 2
行政院國家科學委員會專題研究計畫成果報告 
遊戲 AI 之團隊策略訓練與研究 
計畫編號：NSC 96-2221-E-029-026 
執行期限：96 年 8 月 1 日至 97 年 7 月 31 日 
主持人：蔡清欉   東海大學資訊工程學系 
共同主持人：袁賢銘   國立交通大學資訊工程學系 
            廖啟賢   東海大學資訊工程學系 
 
Abstract 
 
Computer team games have attracted many 
players in recent years. Most of them are rule-based 
systems because they are simple and easy to 
implement. However, they usually cause a non-player 
character (NPC) to be inflexible, and it may repeat a 
failure. Some studies investigated the learning of a 
single NPC, and its learning capability has been 
improved. However, each NPC in a team is 
independent and it does not cooperate with others in a 
multiplayer game. This paper explores an evolution 
strategy for a computer team game based on Quake III 
Arena. The Particle Swarm Optimization (PSO) 
algorithm will be applied to evolve an NPC-team in 
Quake III to be more efficient and intelligent. The 
evolution of a single NPC, which accommodates to its 
team and, moreover, the team has learning and 
cooperating abilities, will be discussed. An efficient 
team is composed of various members with their own 
specialties, and the leader is capable of evaluating the 
performance of a member and assigning it a proper 
job. Furthermore, the leader of an intelligent team will 
adapt a strategy appropriate for various circumstances 
and obtain the team’s best performance. Instead of 
considering the tactic of an individual bot, this project 
takes the strategy of a team into account. 
 
Keywords: non-player character; team game; 
multiplayer game; Quake III; Particle 
Swarm Optimization; First Person 
Shooter; Capture the Flag 
 
摘 要 
 
電腦遊戲具有高度的互動性以及整合各式媒體的
能力，電腦遊戲擬真的影像與具備臨場感的聲音效
果，能帶給玩家身歷其境的遊戲體驗，使得電腦遊
戲已經成為現代人們重要的一項休閒娛樂活動，也
因此帶動遊戲產業更加蓬勃發展。近幾年遊戲的發
展，由於電腦繪圖技術的突飛猛進，因此遊戲的開
發團隊大多把重心投入電腦繪圖的領域，而忽略了
遊戲的本質，這個問題漸漸地受到遊戲公司的重
視，所以許多的遊戲公司已將注意力放在提升遊戲
的人工智慧上，期望具有思考且多變的人工智慧，
可以製作出更具遊戲性的電腦遊戲。  
針對電腦遊戲人工智慧的應用，較適合運算速度較
快且較為穩定的演算法，粒子群最佳化演算法
（Particle Swarm Optimization）在人工智慧領域中
擁有相當不錯評價，其特點為運算與收斂速度較快
且穩定，是一項新興的最佳化機器學習技術。 在
計畫中我們提出一種團隊策略的學習方法，可以很
容易訓練出具有效率的團隊，訓練過程中不需要大
量的訓練資料以及費時的運算，更適合遊戲環境的
應用，並且可以輔助遊戲的人工智慧設計師調整行
為參數，節省測試不同參數組合的時間，增加遊戲
開發的效率。 最後，我們將所提出的演算法實際
套入雷神之鎚3遊戲中之單旗搶旗模式中，結果顯
示我們所創造出來的團隊，與原始的人工智慧相比
較。 
 
關鍵詞：遊戲人工智慧、粒子群最佳化（Particle 
Swarm Optimization）、第一人稱射擊遊戲（First 
Person Shooter Game）、團隊學習 
 
1. Introduction 
 
The rapid progress of hardware has brought computer 
games with bounteous contents and enhanced 
multimedia effects. In addition, the interactivity of 
games is the key factor of its attraction. Players 
interact with virtual characters or other human players 
and enjoy the plot of games. This kind of situation 
could not be achieved via many traditional media. 
Thus, playing interactive multimedia games has 
become an important entertainment. 
In recent years, game developing companies have 
increased their investments in order to attract more 
players with variety of games. Most of the games are 
rule-based systems because they are simple and easy 
to implement. However, their chief defect is that they 
cannot stop being overwhelmed by players if a 
weakness is found, and as a result, games are no 
longer challenging. In order to solve this problem, 
many game designers have focused on the 
improvement of AI in Non-Player Characters (NPC) 
such that the best strategy will be adopted based on 
the situation in a game. Therefore, the game designers 
have focused on the training of an NPC that is capable 
of thinking like a human does, learning from 
experience, and choosing a proper strategy in various 
circumstances (Bourg, 2004, Fairclough et al., 2000; 
 4
earlier games. 
The second title in the Quake series, Quake II, 
was developed in 1997. It introduced many new 
weapons and enemies. The graphic engine is more 
advanced and the bots appear more intelligent than 
those in previous games. Quake II uses different states 
for characters’ behaviors such as standing, walking, 
running, dodging, attacking, melee, seeing by the 
enemy, idle, and searching. 
 
2.1. Quake III Arena 
In 1999, Quake III Arena, an FPS game 
focusing upon multiplayer action, was released by the 
same company. A player in the game views from a 
first person perspective and moves around to 
eliminate opponents and score points based on the 
objective of the game mode in a real-time 3D virtual 
world. The game modes include deathmatch, Team 
deathmatch, Capture the flag (CTF), and tournament. 
A series of maps, varying from easy to difficult, that 
consist of combat against different characters, are 
available in the game. The game ends when the time 
limit has been reached, or a player or team gets a 
specified score. 
Quake III attracts game players by its gorgeous 
scenes and exciting fighting modes with NPCs that 
have various characters. In order to increase the 
diversity of the game, the Quake III source code was 
released in 2005 for modification. Like most 
multiplayer FPSs, the aim of the game is to move 
throughout the arena killing opponents and scoring 
points according to the objective of the mode. 
 
2.2. The mode of One-Flag CTF 
In the mode of One-Flag CTF, one team must 
reach the central position to grab the white flag before 
the other team, which consists of other players or 
computer-controlled bots, and bring it to the enemy’s 
base, touching the flag to score points. In general, 
teams partition themselves into squads such as offense 
and defense. Defense defends the base and prevents 
the enemy’s arrival; contrarily, offense tries to obtain 
the flag and invade to the opponent’s base. In addition, 
some may serve as floaters to attack the enemies near 
the flag. Each team will try their best to retrieve the 
white flag which is similar to a basketball or soccer 
game. 
The victory in a One-Flag CTF game relies on 
the cooperation of all members in a team. In addition, 
the team leader should be able to deliberate and issue 
right commands to its members upon various 
situations, like the Team leader AI [19]. The team 
leader is a commander and its members are being told 
to assault the enemy, seize the flag, defend the base, 
or lurk and strike the opponent in a game. The role of 
a leader is critical and its strategic decision is the key 
to winning a game. The leader may issue one of the 
six commands to its members, including attack, 
defend, camp, patrol, retrieve, and escort [18]. In 
order to get points, various strategies may be applied 
according to the situation of the flag. Flexible tactics 
and mutual supports are important in this kind of team 
game. 
 
2.3. Behavior control 
All the FPS games preceding Quake are 
rule-base architecture, including a cheating perception 
system, which knows all the information of a map; for 
instance, a bot knows the position of its opponent who 
hides behind a wall. As a result, the behaviors of bots 
are simple and the games are not fair and very 
interesting. The perception system of a bot in Quake 
III is like a human’s perception; it sees and hears only 
objects located in a limited distance without 
obstruction. 
The behaviors of a bot are composed of some 
basic actions; for example, the behavior of attacking 
an enemy far behind includes return, walking, and 
shooting. Based on the rules of a game, QuakeRule, 
and the perceptional information it collects from the 
environment, such as seeing an object and hearing 
sound, its AI system then decides the action, as 
depicted in Fig. 1. The Interface Dynamic Link 
Libraries (DLL) collects all the perceptional 
information and then transmits it to the Quakebot AI 
system through the Socket I/O. Quakebot AI figures 
out the most suitable response action upon QuakeRule 
and, via the Socket I/O, sends it back to the Interface 
DLL; finally, the Interface DLL presents the action. 
Actions between bots influence each other through the 
Interface DLL. 
The capability of the Quakebot AI is restricted; 
however, the source code is open for modification. 
The PSO-evolution system, which is based on the 
proposed strategy, enhances a team’s learning 
capability and adaptivity by examining the past 
command values of each bot and the performance of 
its team. Based on the proposed method, a set of new 
command values will be obtained and transferred back 
to the Quakebot AI as illustrated in Fig. 1. The 
Quakebot AI will determine a proper action of a bot 
according to the command values obtained from the 
PSO-evolution system and the perceptional 
information collected from the environment according 
to the game rules. The new set of command values is 
then transferred back to the Interface DLL, through 
the Socket I/O, to affect a bot’s behavior. 
 
3. The team evolution method 
 
The purpose of this research is to discover a 
PSO-evolution method that makes a team in Quake III 
Arena, henceforth called the PSO-team, which is 
capable of learning and uses an effective strategy to 
win in the mode of One-Flag CTF. In a multiplayer 
game, a bot’s capability is not so important; instead, 
the cooperation in a team becomes crucial. The leader 
of a team is selected to guide a whole team. In any 
circumstance, it will assign a proper task to its 
members and each member will try its best to do the 
assigned job. 
 
bot j in team i and in the kth iteration. 
In PSO, the particles in a swarm learn from each 
other to find an optimized solution. In a 
PSO-evolution team, it evolves according to the 
team’s knowledge and the other team’s experience. 
All individuals in the team learn not only from their 
own knowledge but also from the most successful 
team. Finally, the team will behave like the best one 
after development. We apply this approach to evolve a 
team by letting several teams in turn play against a 
computer-controlled team. The leaders of these teams 
learn from each other and consequently, all the teams 
will perform like the best one after convergence. Thus, 
a variety of optimized strategies for different 
situations can be discovered. 
 6
 
3.3. State transitions and weights 
The progress of a multiplayer game is affected 
by many factors. In order to simplify the problem, it 
can be represented as an FSM. According to the 
situations of the flag in the mode of One-Flag CTF of 
Quake III Arena, there are five states, including Flag 
at the center, Flag captured by the PSO-team, Flag 
captured by the opponent, Flag dropped, and Scored, 
as depicted in Fig. 2. Various state transitions are 
given different weights based on whether the change 
is beneficial or harmful. The most important 
beneficial transition is the change from Flag captured 
by the PSO-team to Scored and thus, it has the highest 
weight. The change from Flag captured by the 
opponent to Flag dropped has to return to Flag 
captured by the PSO-team as soon as possible so the 
transition has the second highest weight. The 
alternation from Flag captured by the PSO-team to 
Flag dropped then back to Flag captured by the 
PSO-team has less weight. Finally, the conversion of 
Flag at the center to Flag captured by the PSO-team is 
given the least weight. On the contrary, 
disadvantageous transitions in the descending order 
are: Flag captured by the opponent to Scored, Flag 
captured by the PSO-team to Flag dropped then Flag 
captured by the opponent, Flag captured by the 
opponent to Flag dropped then Flag captured by the 
opponent, Flag at the center to Flag captured by the 
opponent. All the undesirable transitions are given the 
same weights as the corresponding beneficial 
transitions, except those weights are negative. 
The performance of a team is evaluated based 
on the total number of flags captured in an interval of 
time. That is, if the progress in a game is desirable, the 
faster the movement to a better state, the more chance 
to win; and in an undesirable situation, the longer the 
time letting the opponent move to a worse state, the 
less threatening. Thus, a fitness function, f(t), is used 
to assess the performance of a team in a game. 
                                                
1
1
2)( −+= −e ti
tf α  (7) 
where t is the time between the transition states,   
is the weight of the transition, and i is the state 
transition path, which is 1, 2, 3, and 4. The fitness 
value of a team is assessed after a match is over; the 
higher the fitness value, the better tactic the team has. 
 
3.4. Evolution 
During the evolution, the PSO-team learns the 
strategy from the team performs best, which has the 
highest fittest value; meanwhile, team members also 
evolve from their own experience. Several PSO-teams 
are arranged to fight against a computer-controlled 
team to obtain an optimized strategy in each state 
transition. The computer-controlled team is controlled 
by the Quake III AI and the PSO-teams are directed 
by the PSO-evolution system. A fitness value is 
evaluated when a match is over. The current command 
parameter is set as pbest if the fitness value is larger 
than all its previous fitness values. The pbest of those 
teams are then compared and the largest value is set as 
gbest. The command variance of each bot of the next 
iteration can be obtained according to eq. (3). The 
command parameter of every bot in a team in the next 
iteration can be obtained from eq. (5). The procedure 
repeats until a convergent solution is found. 
 
4. Experimental results 
 
The proposed strategy was examined in the 
mode of One-Flag CTF in Quake III Arena. In order 
to observe the performance of the method, mpteam1, 
with the Central Plaza at the center of the map, was 
chosen because the map is identical on both sides. 
There is no trap in the map to ensure the command 
received by bots can be executed. The Quake III 
source code version is 1.32 and the presented 
PSO-evolution system was written in Virtual Basic. 
We arranged three teams in turn playing against 
a computer-controlled team. Initially, each team 
includes five identical members and the strategies of 
the PSO-teams at each state are randomly selected. 
The performance of a team is assessed based on the 
fitness value obtained from eq. (7) during progress. 
We first let an unevolved team match against a 
computer-controlled team for 20 and 40 matches. 
The results, shown in Figs. 4(a) and 5(a), 
demonstrated that the score of the unevolved team is 
compatible to that of the computer-controlled team. 
Then the computer-controlled team fought against the 
PSO-team, in which various state transitions are given 
different weights based on whether the change is 
beneficial to the PSO-team. The most important 
transition is the change from Flag captured by the 
PSO-team to Scored and thus, it has the highest 
weight 0.12. The change from Flag captured by the 
opponent to Flag dropped desires returning to Flag 
captured by the PSO-team as soon as possible, so the 
transition has the second highest weight of 0.08. The 
alternation from Flag captured by the PSO-team to 
Flag dropped then to Flag captured by the PSO-team 
has a weight of 0.06. Finally, the weight for the 
conversion of Flag at the center to Flag captured by 
the PSO-team is set as 0.04. Similarly, the 
corresponding disadvantageous transitions on the 
