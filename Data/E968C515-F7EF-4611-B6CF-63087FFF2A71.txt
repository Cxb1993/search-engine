1A Content-Adaptive Distortion-Quantization Model
for H.264/AVC And Its Application in Constant
Quality Video Coding
Ching-Yu Wu and Po-Chyi Su, Member, IEEE
Abstract—Estimating the distortion associated with the Quan-
tization Parameter (QP ) is very helpful in video encoding. In
this research, a content-adaptive Distortion-Quantization (D-
Q) model for H.264/AVC is proposed to predict the degree
of distortion in each frame before the exact encoding process
is applied. The single parameter in the proposed model is
determined by the content complexity of a macroblock (MB). To
be more specific, the Sum of Absolute Transformed Difference
(SATD) is employed to find the relationship between QP and the
corresponding distortion so that a suitable QP can be assigned.
To show the feasibility of the proposed model, we incorporate
the model in the mechanism of constant quality video coding
so that the quality of each encoded frame will be close to
the targeted one. Such applications of video surveillance or
archiving will be significantly benefited by this coding strategy.
The experimental results show that the proposed scheme can
effectively and efficiently achieve the targeted frame quality in
H.264/AVC.
Index Terms—H.264/AVC, Distortion-Quantization, Constant
Quality Video Coding, Quality Control, SATD.
I. INTRODUCTION
H.264/AVC is becoming widely used in various kinds of
video communications [1]. In such a lossy video codec,
the Rate-Distortion (R-D) performance is mainly controlled
by the quantization of Discrete Cosine Transform (DCT)
coefficients. How to reasonably determine the quantization
parameter (QP ) for each coding unit (usually a frame) has
thus become an important issue. The assignment of suitable
QP can be termed as rate control or quality control, depending
on different constraints that are considered [2]. The major
task of the rate control algorithms is to assign appropriate
QP value to each frame for allocating the limited bit budget.
The distortion performance will be affected by different bit
allocation strategies. On the other hand, the quality control
is designed to reduce the distortion variation among frames.
Such method may benefit the users’ viewing experience more,
when compared with the mechanisms that consider the bit-rate
issues only [3], [4].
Most of the existing algorithms focus on solving the so-
called MINAVE optimization problem. To be more specific,
the algorithms try to minimize the overall (average) distortion
subject to the bit constraint. However, the main drawback of
The authors are with the Department of Computer Science and Information
Engineering, National Central University, Jhongli, Taiwan, (e-mail: pochy-
isu@csie.ncu.edu.tw).
This research was supported by the National Science Council in Taiwan,
R.O.C., under Grant NSC 100-2221-E-008-120.
MINAVE-based rate control is that significant quality fluctu-
ations among frames may be observed, due to the varying
contents in frames or shots, and severely affect the subjective
video quality [5]. To avoid quality variations among frames,
many schemes limit the QP difference between adjacent
frames. However, this limitation may not perform well in
such cases as high camera-motions or abrupt scene-changes.
Therefore, some researchers attempt to design Distortion-
Quantization (D-Q) models to acquire the prior knowledge of
the distortion performances so that the appropriate QP value
can be chosen for a given frame. Ma et al. [6] found that
there exists a linear relationship between the Peak Signal to
Noise Ratio (PSNR) and QP value. A Rate-Distortion model
is then developed to efficiently allocate the bit budget of each
frame. Kamaci et al. [7] utilized Cauchy-density function to
fit the distribution of the DCT coefficients. An approximated
D-Q model was then proposed to derive the R-D model for
solving the frame-level bit allocation problem. Guo et al.
[8] proposed a D-Q curve estimation, which first chooses a
probing QP to encode the video once and then analyzes the
distribution of DCT coefficients to construct the D-Q curve
of a given frame. Wang et al. [9] observed that there is a
simple linear relationship between the MSE (Mean Squared
Error) and quantization step size (Qstep), which can be used
to determine the QP value for encoding a Macroblock (MB).
An initial QP determination algorithm was then proposed to
improve the coding efficiency of H.264/AVC. Nevertheless,
most of these D-Q models contain multiple model parameters
but lack a proper method for estimating them. This implies
that encoding the video several times may be required to ob-
tain the suitable model parameters. The resulting inefficiency
hinders the existing algorithms from being used in practical
applications.
In this research, we propose a content-adaptive D-Q model
for H.264/AVC, with the distortion measured in Sum of
Squared Error (SSE), which is directly related to PSNR. An
approximated D-Q curve, which maps the QP values to the
corresponding distortions, will be formed before the exact en-
coding process is applied. The model has only one parameter,
which can be well estimated by the computationally-efficient
feature extracted from the frame content. That is, given the
raw data of frames to be encoded, some spatial/temporal
features are extracted to help estimate this model parameter.
The proposed model is built at the MB level so that we
can employ the D-Q curves of all the MB’s in a frame
to form a frame-level D-Q curve such that an appropriate
3may not hold in all of the coded blocks with distinct and/or
unique textures. In order to improve the model accuracy, we
incorporate the power function of QP to adjust the curvature
instead of utilizing Qstep only. Figs. 2 (a) and (b) illustrate
the predicted errors from Eq. (2) and the actual errors in I-
frames and P-frames, respectively. The test sequence for I-
frame is the same as the one used in Fig. 1, and the well-
known sequence “Coastguard” is used to evaluate the proposed
model in P-frames. We can observe that the proposed model
performs well with high R2 value in both cases. Compared
with the fitting result of Eq. (1) shown in Fig. 1, the proposed
model performs even higher R2 value in the I-frame test.
Moreover, by adopting the power function of QP , a solid
linear relationship between ln(α) and β is clearly shown in
Figs. 2 (c) and (d), which represents that α and β can be
combined into one single parameter. Thus, Eq. (2) can then
be rewritten as
D = e(l×β+r) ×QP β ×Qstep, (3)
where l and r are constants depending on the frame type
and can be obtained by applying the linear regression in a
model training process. According to our experiences, this
expression performs reasonably well with the commonly used
QP range from 20 to 45. The D-Q curve can then be derived
before the encoding process if we can determine β in Eq. (3)
appropriately. It should be noted that we employ MB’s to train
the model, instead of full frames, so that the performance will
be less affected by the characteristics of different areas in a
frame. We can then, as mentioned before, accumulate the D-Q
curves of MB’s to form a complete D-Q curve at the frame-
level and the model bias caused by the model outliers can also
be alleviated.
The single parameter β in Eq. (3) is supposed to be related
to the content complexity of a MB. We employ the Sum
of Absolute Transformed Difference (SATD) to evaluate the
complexity of the block content. The Hadamard transform of
the residual block is computed by
C = H(B−P)HT, (4)
where B and P are the original source block and the predicted
block, respectively. The Hadamard matrix, H, is defined as
H = HT =

1 1 1 1
1 1 −1 −1
1 −1 −1 1
1 −1 1 −1
 . (5)
The SATD value is then calculated by summing up the
absolute value of every element in C. In general, SATD
can be viewed as a distortion measurement in the frequency
domain. It is applied on the residual blocks to accelerate the
coding mode selection and rate-distortion optimization (RDO)
in the low-complexity mode, and has a better representation
than the traditional Sum of Absolute Difference. Moreover,
the calculation of SATD is efficient since only additions and
subtractions are required.
In our case, since the actual coding mode is not available
before the encoding process, we compute different prediction
signals according to the frame type to form the needed residual
signal. For the spatial predictions, we choose to imitate the
intra4× 4 DC mode by simply averaging the pixel values of
each 4 × 4 subblock in the current MB. Our spatial feature,
i.e., the SATD between the spatially predicted block and the
block to be encoded, is calculated and denoted by satds.
For the temporal predictions, a rough motion estimation is
employed to find the prediction block, where the block size
is set as 16 × 16 and the search range is ±8 to decrease the
computational complexity. Again, the temporal SATD feature
between the temporally predicted block and the raw input
block is calculated and denoted by satdt. It should be noted
that the motion estimation for the feature extraction uses the
original frame data, instead of the reconstructed frame, as the
reference to maintain the uniformity of feature extraction in
different QP setting. In other words, the temporal feature
represents the motion intensity and variation of the current
MB only. The distortion caused by the quantization of DCT
coefficients is not taken into account.
Because all of the MBs are intra-coded in an I-frame,
only the spatial feature satds will be needed to estimate the
model parameter β. On the other hand, the performance of
an inter-coded MB might be affected by both of the spatial
and temporal content variations. Therefore, in a P-frame, both
satds and satdt are considered in β estimation. We observe
that the model parameter, β, can be well-estimated by
β =
{
c1 + c2 × ln(satds) in I-frame
c3 + c4 × ln(satds) + c5 × ln(satdt) in P-frame
,
(6)
where c1, c2, c3, c4, and c5 are constants obtained by the linear
regression in the model training process.
III. THE PROPOSED CONSTANT QUALITY CONTROL
METHOD
The proposed D-Q model is then applied on the constant
quality video coding by assigning an appropriate QP value to
each frame. This coding methodology shows the feasibility of
the proposed model and is also very useful in such applications
as video archiving and video surveillance. The detailed imple-
mentation includes three steps. First, a variable GOP structure
is employed to decide suitable positions or timing for inserting
I-frames. After the frame type is decided, the content-adaptive
D-Q model described in Sec. II is utilized to determine the
QP value of each frame. A model refinement process is then
adopted to make the D-Q curve prediction more accurate.
A. I-Frame Assignment Based on Scene Change Detection
Natural videos are composed of shots with different contents
and lengths. When using the traditional fixed GOP structure, a
scene change frame may be encoded as a P-frame and a contin-
uous frame may be encoded as an I frame. These cases are con-
sidered to be less appropriate. In the variable GOP structure, an
I-frame will only be assigned at the shot boundary to improve
the coding performances. Several spatial/temporal complexity
measurements, e.g., the entropy of block histograms [19],
the difference of accumulated MAD’s (Maximum Absolute
Difference) [20], and difference of histograms [21], etc., have
50 100 200 300 400 500 600 700
0
1
2
3
4
5
6
7
x 106
Bus City Crew Football Foreman Mobile Tempete
frame number
a
cc
u
m
u
la
te
d 
SA
TD
Fig. 3: The shot change detection using the proposed accumu-
lated SATD value. The circles represent that the accumulated
SATD values are larger than T and the abrupt shot changes
are thus recognized.
database and concatenate them into a sequence. We set three
target PSNR values, i.e., 35, 40, and 45 dB, to encode this
sequence, all with the intra coding. The proposed D-Q model
is then employed to determine the QP value for each frame.
It can be observed from Fig. 4 that most of the resultant
PSNR values are close to the targets so our model performs
reasonably well in average. However, the performance may
not be that satisfactory when we check the individual frames
since some larger deviations appear, especially in the relatively
lower quality cases. After carefully analyzing the results, we
found that the variations of these three cases (with varying QP
values) tend to be similar. In other words, we may overestimate
or underestimate the D-Q relationship of a certain frame at
different target PSNR settings. This phenomenon can also be
observed in the P-frame encoding.
0 20 40 60 80 100 120
30
32
34
36
38
40
42
44
46
48
frame number
PS
NR
 (d
B)
Fig. 4: The PSNR values of the Corel image sequence by using
the proposed D-Q model.
Fig. 5 demonstrates two examples of distortion prediction
of one P-frame in Coastguard and one in Stefan. The circles
represent the distortion predicted by our model. These two
frames are chosen because of their worst accuracy in our
tests. It can be seen that the distortion is underestimated
in Coastguard and overestimated in Stefan. However, the
prediction results show a straight line through the origin, which
implies that we can use a simple refining factor to correct the
prediction deviation. A refining factor θ can thus be multiplied
by Eq. (3) to adjust the model accuracy according to
D = θ × e(l×β+r) ×QP β ×Qstep, (8)
where θ is calculated by
θ =
Dactual
Destimated
, (9)
with Dactual and Destimated being the actual and the model-
estimated SSE at the frame-level, respectively.
0 10 20 30 40 50 60 70 80
0
10
20
30
40
50
60
70
80
Actual MSE
Pr
ed
ict
ed
 M
SE
 
 
Without updating
With updating
(a)
0 10 20 30 40 50 60 70 80
0
10
20
30
40
50
60
70
80
Actual MSE
Pr
ed
ict
ed
 M
SE
 
 
Without updating
With updating
(b)
Fig. 5: The updating of the proposed content-adaptive D-Q
model in a P frame of (a) Coastguard and (b) Stefan.
Nevertheless, since we would like to predict the distortion
performance before the encoding process, the exact refining
factor is not available, unless we encode the frame twice.
Since it is a reasonable assumption that the spatial/temporal
characteristics in a continuous shot are similar, the refining
factors of adjacent frames should be quite close. Therefore, a
720 25 30 35 40
0
50
100
150
QP value
M
SE
 
 
Proposed
Actual
[6]
[7]
(a)
20 25 30 35 40
0
20
40
60
80
100
120
QP value
M
SE
 
 
Proposed
Actual
[6]
[7]
(b)
20 25 30 35 40
0
20
40
60
80
100
120
QP value
M
SE
 
 
Proposed
Actual
[6]
[7]
(c)
20 25 30 35 40
0
5
10
15
20
25
30
35
40
45
50
QP value
M
SE
 
 
Proposed
Actual
[6]
[7]
(d)
20 25 30 35 40
0
50
100
150
QP value
M
SE
 
 
Proposed
Actual
[6]
[7]
(e)
20 25 30 35 40
0
10
20
30
40
50
60
QP value
M
SE
 
 
Proposed
Actual
[6]
[7]
(f)
20 25 30 35 40
0
20
40
60
80
100
120
140
160
180
200
QP value
M
SE
 
 
Proposed
Actual
[6]
[7]
(g)
20 25 30 35 40
0
20
40
60
80
100
120
QP value
M
SE
 
 
Proposed
Actual
[6]
[7]
(h)
20 25 30 35 40
0
20
40
60
80
100
120
140
160
QP value
M
SE
 
 
Proposed
Actual
[6]
[7]
(i)
Fig. 7: D-Q curve predictions of intra-coded frame. (a) Bus, (b) City, (c) Coastguard, (d) Crew, (e) Football, (f) Foreman, (g)
Mobile, (h) Stefan, and (i) Tempete.
proposed quality control scheme. Since one may argue that
selecting a fixed QP may work well in the test videos of
the same kind, we compare our scheme with the fixed QP
encoding. Four QP values, 24, 28, 32 and 36, are tested in this
experiment. The target PSNR values for the proposed method
are then set according to the results of the fixed QP encoding.
We also list the results of a single-pass quality control method
by D. Vito et al. [4] for comparison. In Table II, we calculate
both the average absolute deviation (AAD) from the target
PSNR, measured in dB per frame, and the variance of PSNR
(VAR) to evaluate the performances. It can be observed from
Table II that our approach outperforms the other two coding
strategies as these two indicators are much smaller. The values
of AAD and VAR in our scheme are just 0.175 and 0.049,
respectively. It is worth noting that the proposed single-pass
quality control method can also perform well at relatively low
bit-rate conditions, especially in the videos with high motions,
such as Football and Foreman.
Then, the merged sequence, which consists of several com-
mon test videos, is used to imitate the natural video with
shot changes. Figs. 9 (a) and (b) show the frame PSNR
values and QP values respectively by using the fixed QP
encoding (QP = 32), D. Vito’s scheme [4] and our proposed
method. We can see that the fixedQP encoding cannot provide
consistent quality since the contents of scenes are not taken
into account. The large quality fluctuations happen not only
at the shot boundaries but also in the continuous high-motion
scenes. Significant quality variations can also be found in the
results of [4]. For example, the PSNR difference can be as
large as 8 dB in Football. In contrast, the proposed method
considers the scene complexity to decide a suitable QP value
for a frame so that the PSNR value of each frame is very
close to the target and the quality variation is minimized.
Moreover, the encoding result of each frame will be used
to modify the up-coming prediction for increasing the model
accuracy. Therefore, when a sensible quality deviation appears,
9TABLE I: The MSE of the frame level D-Q curve estimation.
Video I-frame coding P-frame codingProposed [6] [7] Proposed [6] [7]
Bus 4.845 32.568 44.581 2.880 24.356 35.5106
City 8.250 30.126 40.003 1.412 2.883 5.235
Coastguard 8.948 42.779 52.552 16.134 53.026 63.517
Crew 0.073 0.229 0.316 0.201 0.259 0.160
Football 8.029 51.061 64.358 0.323 37.561 49.518
Foreman 0.158 0.691 1.407 0.153 0.073 0.419
Mobile 1.539 49.300 72.763 28.104 128.202 165.178
Stefan 0.995 5.942 11.547 1.693 17.434 26.407
Tempete 0.531 19.377 31.048 5.333 27.338 39.617
Average 3.708 25.786 35.397 6.248 32.348 42.840
Circuits and Systems for Video Technology, vol. 15, no. 12, pp. 1533–
1544, Dec. 2005.
[7] N. Kamaci, Y. Altunbasak, and R. M. Mersereau, “Frame bit allocation
for the H.264/AVC video coder via Cauchy-density-based rate and
distortion models,” IEEE Transactions on Circuits and Systems for Video
Technology, vol. 15, no. 8, pp. 994–1006, August 2005.
[8] L. Guo, O. C. Au, M. Ma, Z. Liang, and P. H. W. Wong, “A novel
analytic quantization-distortion model for hybrid video coding,” IEEE
Transactions on Circuits and Systems for Video Technology, vol. 19,
no. 5, pp. 627–641, May 2009.
[9] H. Wang and S. Kwong, “Rate-distortion optimization of rate control for
H.264 with adaptive initial quantization parameter determination,” IEEE
Transactions on Circuits and Systems for Video Technology, vol. 18,
no. 1, pp. 140–144, Jan. 2008.
[10] N. Cherniavsky, G. Shavit, M. F. Ringenburg, R. E. Ladner, and
E. A. Riskin, “Multistage: A minmax bit allocation algorithm for
video coders,” IEEE Transactions on Circuits and Systems for Video
Technology, vol. 17, no. 1, pp. 59–67, Jan. 2007.
[11] K.-L. Huang and H.-M. Hang, “Consistent picture quality control
strategy for dependent video coding,” IEEE Transactions on Image
Processing, vol. 18, no. 5, pp. 1004–1014, May 2009.
[12] D. Zhang, K. N. Ngan, and Z. Chen, “A two-pass rate control algorithm
for H.264/AVC high definition video coding,” Signal Processing:Image
Communication, vol. 24, no. 5, pp. 357–367, May 2009.
[13] B. Han and B. Zhou, “VBR rate control for perceptually consistent video
quality,” IEEE Transactions on Consumer Electronics, vol. 54, no. 4, pp.
1912–1919, Nov. 2008.
[14] J. Huang, J. Sun, , and W. Gao, “A novel two-pass VBR coding algorithm
for the H.264/AVC video coder based on a new analytical R-D model,”
in Proceedings of Picture Coding Symposium, 2007.
[15] X. M. Zhang, A. Vetro, Y. Q. Shi, and H. Sun, “Constant quality
constrained rate allocation for FGS-coded video,” IEEE Transactions on
Circuits and Systems for Video Technology, vol. 13, no. 2, pp. 121–130,
Feb. 2003.
[16] M. Dai, D. Loguinov, and H. M. Radha, “Rate-distortion analysis and
quality control in scalable internet streaming,” IEEE Transactions on
Multimedia, vol. 8, no. 6, pp. 1135–1146, Dec. 2006.
[17] C.-W. Seo, J.-K. Han, and T. Q. Nguyen, “Rate control scheme for
consistent video quality in scalable video codec,” IEEE Transactions on
Image Processing, vol. 20, no. 8, pp. 2166–2176, Aug. 2011.
[18] D.-K. Kwon, M.-Y. Shen, and C.-C. J. Kuo, “Rate control for H.264
video with enhanced rate and distortion models,” IEEE Transactions on
Circuits and Systems for Video Technology, vol. 17, no. 5, pp. 517–529,
May 2007.
[19] J. Lee, I. Shin, and H. Park, “Adaptive intra-frame assignment and bit-
rate estimation for variable gop length in H.264,” IEEE Transactions on
Circuits and Systems for Video Technology, vol. 16, no. 10, pp. 1271–
1279, Oct. 2006.
[20] S.-C. Hsia and S.-H. Wang, “Adaptive video coding control for real-
time H.264/AVC encoder,” Journal of Visual Communication and Image
Representation, vol. 20, no. 7, pp. 463–477, Oct. 2009.
[21] K. Chang, B. Yang, and J. Luo, “A novel rate control scheme for
H.264/AVC based on scene change,” in IEEE/ACIS International Con-
ference on Computer and Information Science, Los Alamitos, CA, USA,
2009, pp. 1085–1089.
11
0 500 1000 1500 2000
26
28
30
32
34
36
38
40
42
Bus City Coastguard Crew Football Foreman Mobile Stefan Tempete
frame number
PS
NR
 (d
B)
 
 
Fixed QP
[4]
Proposed method
(a)
0 500 1000 1500 2000
25
30
35
40
45
50
55
Bus City Coastguard Crew Football Foreman Mobile Stefan Tempete
frame number
QP
 
 
[4]
Proposed Method
(b)
Fig. 9: The (a) PSNR and (b) QP variations in frames of the merged sequence encoded with the target PSNR set as 32 dB.
 2
是探討在網際網路以資料隱藏進行匿名傳輸的應用。該應用以影像為例探討匿名傳輸的方法。我
提問該方法是否適用於壓縮域資料，作者回答這篇論文目前是個構想，在壓縮域如 JPEG 影像上會
遭遇到若干困難，我個人認為實用性將因此受到影響。最後一篇論文則是探討網路匿名問題，與
資料隱藏的關係較不大。 
 
我們的論文則被排在第四篇，我們提出適用於數位影像上的數位浮水印方式，主要是解決數位浮
水印受到影像幾何破壞的問題，例如縮放、裁切、旋轉，甚至是所謂的隨機幾何破壞，這是在影
像浮水印領域一個不容易被解決的問題。我們提出的方式是在數位影像上，根據 SIFT 特徵擷取選
擇位置，再利用樣板嵌入方式徹底解決此問題。我被問及這樣的方式是否有點疊床架屋的感覺。
我回答，若要兼顧實用性與抵抗惡意破壞的攻擊，這種樣板嵌入方式是不得不施行的方法。由於
我們所提出的方法極具實用性，我們的論文也因此得到了最佳論文獎。 
 
 
在接下來兩天的時間，我參加了幾個關於 Collaborative Networks, Systems and Applications 以及
Advances in Vehicular Systems, Technologies and Applications 研討會，雖然我對一些論文內容不甚熟
悉，但跨領域地參與可讓我見識更多知識，相關研究方法，以及實驗結果呈現的方式。參加完晚
宴後，我於 28 日由威尼斯返台。 
 
      
        IARIA 研討會研討室外            會議旅館(Hotel Novotel Venezia Mestre Castellana)外 
A Geometrically Resilient Digital Image
Watermarking Scheme Based on SIFT and
Extended Template Embedding
Po-Chyi Su
Dept. of Computer Science and Information Engineering
National Central University, Jhongli, Taiwan
Email: pochyisu@csie.ncu.edu.tw
Yu-Chuan Chang
Dept. of Computer Science and Information Engineering
National Central University, Jhongli, Taiwan
Email: 995202026@cc.ncu.edu.tw
Abstract—This research presents a feature-based still im-
age watermarking approach. Scale-Invariant Feature Transform
(SIFT) is first applied to locate the interest points, from which
we form the invariant regions for watermark embedding. To
resist geometrical transformations, the extended synchronization
templates, which help to ensure that reasonably large invariant
regions will be available for carrying the watermark payload
and/or for increasing the confidence of watermark detection,
will also be embedded. In the detection phase, after SIFT, the
template is first determined locally by adjusting the related affine
parameters of the grid to match with the possible hidden template
signal so that the watermark can be retrieved afterwards.
Experimental results show the feasibility of the proposed method.
Keywords—digital watermark; geometrical transformations;
SIFT; StirMark.
I. INTRODUCTION
Digital watermarking has been considered as a potential
solution to providing further protection of digital content. The
close integration of the hidden signal, i.e., digital watermark,
with the host media can be used for declaring/verifying the
ownership of the content, controlling the software/hardware
operations or for the trailer tracking purposes. In most of
the related applications, the digital watermark signal has to
be robust against the “watermark attacks,” including lossy
compression, signal processing procedures and even malicious
watermark-removal operations, etc. For still images, the water-
mark surviving geometrical transformations is always required
since such manipulations as cropping, rotation and scaling are
so common. Nevertheless, these procedures cause challenging
synchronization problems for watermark detection and special
care must be taken such that the watermark can resist such
attacks to meet the requirements of applications.
Existing methods to resist geometrical transformations can
be classified into four types, i.e., the exhaustive search [1],
embedding the watermark in invariant domains [2], [3], em-
bedding synchronization templates [4], [5], [6] and employing
feature detections for locating the watermark [7]. In our
opinions, exhaustive search has to be coupled with certain side
information to reduce its computational load. The algorithms
of watermarking in invariant domains seem elegant but they
may not perform well under all kinds of possible geometrical
attacks. Employing synchronization templates may be a more
flexible method to deal with attacks. However, the so-called
“template attack” [8] may detect and remove the template
if it is used repeatedly. The feature-based approaches thus
gain more and more attention. Kutter et al. [7] first claimed
that the feature-based approaches are the second-generation
watermarking schemes. They illustrated this concept by ap-
plying the Mexican-hat wavelet to extract features and the
Voronoi diagram to define the local characteristic regions for
watermarking. Several methods have been proposed in recent
years [9], [10], [11], [12], [13]. The basic idea of these
approaches is applying such feature-point extraction as Harris
corner detection [14] or Scale-Invariant Feature Transform
(SIFT) [15], etc. to determine the interest areas, which are
then transformed into the regions with known shape, size
and orientation for the subsequent watermark embedding and
detection. Since the interest points are extracted according to
the content, the process of locating the embedded areas can
be facilitated. Nevertheless, according to our observation, the
feature-based watermarking may encountered some problems.
First, in order to detect one watermark in a small invariant area,
such transforms as Discrete Cosine Transform (DCT), Discrete
Wavelet Transform (DWT) or Discrete Fourier Transform
(DFT) are usually applied and the middle-frequency compo-
nents may be modified considerably to achieve the robustness
and/or payload. Compared with the neighboring areas without
watermarks, the quality of embedded area may be affected a
lot, especially when the perceptual model is not employed.
Second, deviations of the position, scale and/or orientation
of the watermarked regions may appear under attacks, or
even right after the watermark embedding without any attack.
Therefore, almost all the schemes have to try a few different
shapes of their invariant regions for the watermark detection.
Then, the watermark here looks more like a template or pilot
signal, instead of a watermark signal carrying the necessary
hidden information. The false positive rate of watermark
detection may also become higher. Furthermore, if image
transforms are used, they have to be applied several times
and the computational load will be increased. Third, there are
102Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-204-2
INTERNET 2012 : The Fourth International Conference on Evolving Internet
disperse the feature points. To be more specific, the location of
interest points can be seen as a set of 2-dimensional vectors,
V = {vi}. We calculate the centroid of the |V| vectors and
choose the one closest to the centroid as our first feature
point, fp1. From the |V| − 1 vectors, choose the one that has
the largest distance from the first feature point as the second
feature point, fp2. To find the third feature point from the
remaining |V| − 2 vectors, we calculate the distance Disti
of each vector, vi, and the already chosen feature points,
(i.e.,Disti = Min(Dist(fp1,vi), Dist(fp2,vi))). Again, we
will find the one with the largest distance as fp3. Repeat
the process to increase the number of feature points until
the additional feature point will yield the distance from the
selected points smaller than a distance threshold, TDist. These
feature points will then be used to form the grids for the signal
embedding. Fig. 2(c) illustrates the chosen invariant regions.
Some existing schemes may only employ these regions for the
watermark embedding/detection. However, if the regions are
small, the payload or the detection confidence will be affected.
On the other hand, if a larger size is used, the embedded
watermark may be affected by the local distortion easily. In our
scheme, the invariant areas or grids will be extended to cover
a larger area for signal embedding. The major advantages of
expanding are enhancing the detection confidence, even though
weak signals are embedded, and the increased robustness
against the random bending by StirMark. Before expanding,
we use Voronoi diagram to set up the boundaries for separating
the image into subregions and each subregion belongs to one
selected feature point. The expansion of invariant area can
then be applied. For an invariant grid, four extended grids are
generated. The grids associated with the same feature point
will thus have the same size and orientation. The same process
will be applied on each extended grid too and the expansion
from one initial grid will be limited in the Voronoi subregion.
A few grids can still be added on the boundaries of Voronoi
subregions as long as the added ones will not overlap others.
The grids for signal embedding can then be generated almost
all over the host image as illustrated in Fig. 2(d).
B. Signal Embedding
It should be noted that the signals will be embedded into
almost all the grids but the grids that cover the initially
selected feature points as shown in Fig. 2(c). The reason
of such omission is to avoid the embedded signals from
modifying the descriptors of interest points and from making
these points undetectable. According to our observation, the
signal embedding will change the image data more or less and
we cannot fully prevent the feature-point extraction from being
affected. Although we may apply SIFT on the embedded area
right after the embedding to see whether the selected feature
point is reliable or not, we choose not to take this risk so that
the embedding process can be simplified. In other words, the
signals will only be embedded into the extended grids. There-
fore, each extended grid will be embedded with the watermark
first and then with the template signal. For the watermark
(a) (b)
(c) (d)
Fig. 2. (a) The extracted feature points by SIFT on Lena and (b) the
associated invariant areas. (c) The chosen invariant regions and (d) the
extended grids.
embedding, we employ a pretty traditional spread spectrum
approach in DCT as an example. A pseudo-random sequence
W taking two values, i.e., ±1, is generated as the watermark
signal and embedded into the middle-low frequency DCT
coefficients. To be more specific, we randomize Hadamard
sequences to generate the watermark sequences since they
are mutually orthogonal. This bipolar watermark signal is
then weighted according to Watson’s perceptual model [17].
Although Watson’s model should be able to be applied on
blocks with variable sizes, some questioned that it can only
ensure the invisibility of the noises within blocks. We thus
choose small blocks for the watermark embedding. A grid
with its side length equal to λ× τ will be rotated and scaled
into a 32 × 32 block, which will be divided into 8 × 8
subblocks for the watermark embedding. According to the
zig-zag scan, the lowest three DCT coefficients are skipped
and the next three coefficients are chosen as an illustration
for watermark embedding/detection. The lowest frequency
components are excluded to maintain the high image quality
while the high-frequency components are not chosen to reduce
the interference from the template signals. It should be noted
that other methods, such as quantization index modulation,
may also work under our framework since the synchronization
issue will be settled. The Watson’s model basically takes
two masking effects into account, i.e., the luminance masking
104Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-204-2
INTERNET 2012 : The Fourth International Conference on Evolving Internet
subsequent watermark detection. However, a lower threshold
may introduce a higher false alarm rate. Our strategy is to set
up another adaptive threshold, T3. From our observations, the
responses in grids of a subregion are usually related as they
are similarly large or small, T3 is designed as ρm − 2 × ρσ ,
where ρm is the mean of detected template responses in a
subregion so far, and ρσ is the standard deviation. The rule of
template detection is thus as follows. If ρmaxu of a grid is larger
than T1, the template is ruled as being detected. If ρmaxu is
smaller than T1 but larger than max{T2, T3}, the responses of
its neighbors will be checked. If there is at least one neighbor
with its ρmaxu larger than T1 or there are at least two neighbors
with their ρmaxu larger than T2, the template will also be ruled
as being detected. For the grids around the selected feature
point, T1 = 0.1425 and T2 = 0.1089. For other extended
grids, T1 = 0.1233 and T2 = 0.0937. The corresponding false
alarm rates of T1 and T2 are 0.001 and 0.033 respectively.
Basically, the expansion is applied in a recursive way but we
always check whether a response of a grid has been computed
before to speed up the process.
After the template detection helps to achieve the synchro-
nization, the detection of watermark can be executed in a
straightforward manner. The 32 × 32 affine-transformed grid
will be divided into sixteen 8 × 8 subblocks for calculating
DCT. The same coefficients will be considered for the water-
mark detection. Similarly, the response, ρb, is calculated by
ρb =
∑
h
∑
(i,j)∈B c
∗
i,j,h × wbi,j,h√∑
h
∑
(i,j)∈B
(c∗i,j,h)2
√∑
h
∑
(i,j)∈B
(wbi,j,h)
2
, (9)
where c∗i,j,h is the DCT coefficient and B is the set of selected
DCT coefficients and wbi,j,h = ±1 is the component of
tested watermark sequence. After all the grids are detected,
the existence of watermark is claimed if ρb is larger than a
threshold, Tnc, which is also set according to a pre-determined
false positive rate by Eq. (7) with N equal to the number of
considered DCT coefficients. Large ρb indicates the existence
of a certain watermark signal. To embed more information,
we may simply divide the DCT coefficients into m parts and
each part is embedded with one of 2n watermark sequences
so that m× n bits are embedded.
III. EXPERIMENTAL RESULTS
We demonstrate some results to show the feasibility of
the proposed approach by using 512 × 512 Lena image. The
size of a template pattern is set to be 32 × 32 as mentioned
before. The marked image is shown in Fig. 3 with Peak
Signal to Noise Ratio (PSNR) equal to 36.43 dB. Then we
test several kinds of attacks on the proposed watermarking
scheme. We use StirMark benchmark 4.0 to generate the
attacked images, including rotating 1◦, 2◦, 5◦, 10◦, 15◦, 30◦,
45◦, 90◦, scaling to 50%, 60%, 70%, 75%, 80%, 90%, 110%,
120% of the size, cropping off 15%, 25%, 50%, 75% of the
size, horizontally/vertically shearing by 5%, JPEG with quality
factor equal to 90, 70, 50, Gaussian filtering, sharpening,
and rotating with cropping by 15◦ and 45◦. Table I shows
the results of the attacked Lena images. The second column
shows the number of detected SIFT interest points. The two
values shown in the third column are the numbers of correctly
determined interest points for watermarking and those should
be detected. The fourth column lists the numbers of detected
grids, followed by the numbers of false positive detections.
The fifth column shows the responses of watermark detections.
The sixth column shows the threshold Tnc, which corresponds
to the false alarm rate equal to 10−8. The execution time
evaluated in seconds is listed in the seventh column as the
reference. The first row shows the results of marked image
without any attack for comparison. All the embedded grids
can be correctly determined. As we can see, the watermarks
are detected in almost all the cases except the attack of scaling
by 50% and cropping by 75%. Cropping by a large scale may
result in fewer feature points left and the number of DCT
coefficients may not be large enough to generate a higher
response than the adaptive threshold.
Compared with the existing works, such as [9] and [10],
our method demonstrates more consistent performances under
various attacks. We can compare the numbers of detected
regions, as shown in the third column of Table I, with those
listed in the tables of [10]. The values in their five methods
vary in different cases. If we use the ratio, i.e., the number of
detected areas divided by the number of embedded areas, as an
indication, the ratios of five approaches in [10] are 71/168 =
0.42, 20/132 = 0.15, 84/276 = 0.30, 53/324 = 0.16 and
52/156 = 0.33. The ratio of [9] is 73/468 = 0.16 and
ours is 68/132 = 0.52, which is the highest. In addition, our
approach has reasonable performances in all the attack cases
except the aspect-ratio changes, from which no template can be
determined. This problem may be solved by employing more
flexible templates, instead of squares only. It is worth noting
that our scheme may outperform others under the random
bending attack by StirMark. Since this attack is applied in a
random way, each time a different outcome appears. Table II
illustrates the performances of our scheme against such attack.
We employ two parameters, weaker (1.0) and stronger (2.0)
geometrical modifications. The tests are run six times in each
case. We can see that our scheme has no problem dealing with
the random bending on the Lena image. In fact, according
to our experiments, strong attacks sometimes cause misses
of detections in other images. For stronger random bending
attacks, two challenges may appear. The first challenge is the
stability of SIFT interest points. It seems that either the de-
scriptors or positions of interest points are changed. The same
problem may happen when sharpening is applied. We think
that developing a more suitable feature extraction method for
digital watermarking may be an interesting research topic. The
second challenge is that using only square grids for matching
may not be enough, as mentioned before. Further adjusting the
parameters of grids, such as modifying the positions of four
corners in different ways, may provide more diverse forms of
grids for detection. However, heavier computational load will
be expected and may hinder the feasibility.
106Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-204-2
INTERNET 2012 : The Fourth International Conference on Evolving Internet
International Academy, Research, and Industry Association
Best Paper Award
A Geometrically Resilient Digital Image Watermarking Scheme Based
on SIFT and Extended Template Embedding
By
Po-Chyi Su, Yu-Chuan Chang
Presented during INTERNET 2012, The Fourth International Conference on Evolving Internet,
held in Venice, Italy - June 24-29, 2012
100年度專題研究計畫研究成果彙整表 
計畫主持人：蘇柏齊 計畫編號：100-2221-E-008-120- 
計畫名稱：基於內容適應性與編碼效果預測之 H.264/AVC 視訊壓縮技術 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 1 100% 
備註:期刊論文剛
投 稿 於 IEEE 
Transactions on 
Circuits and 
Systems for 
Video Technology
以 及 APSIPA 
Transactions on 
Signal and 
Information 
Processing(會議
邀請投稿)，另有
一篇編碼相關論
文 投 稿 於 IEEE 
Transactions on 
Multimedia，目前
正 在 進 行
second-run 的審
稿。 
研究報告/技術報告 0 0 100%  
研討會論文 3 3 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  
國外 
專利 已獲得件數 0 0 100% 件  
