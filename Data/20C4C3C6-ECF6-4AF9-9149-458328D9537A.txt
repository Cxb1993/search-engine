行政院國家科學委員會專題研究計畫年度報告  
3D音視訊的內容製作、處理與互動式呈現  
子計畫二:多視域視訊的影像擷取與表示法建構  
 
計畫編號：NSC-96-2221-E-194-016-MY2 
執行期限： 97 年 8 月 1日至 98 年 7 月 31 日  
主持人：林惠勇 國立中正大學電機工程學系  
Email: hylin@ee.ccu.edu.tw, URL: http://vision.ee.ccu.edu.tw/hueiyung 
研究人員：劉庭宏、陳鼎文、賈孟軒、蕭郁樺、林正祐、謝家豪 
一、 中文摘要  
本計畫之目的在於建構三維視訊之內
容，三維電視是一個可能發展成為人類未來
家庭用之重要影音設備，它是需要結合多個
不同領域之研究成果，如電腦視覺、音訊、
影音壓縮、以及電腦圖學視覺化等等。本計
畫為整合型計畫第二年之之執行，其目的在
於建構一個三維電視的前端訊號擷取與模
型建構之模組，此三維模型的取得包含了多
架相機之影像擷取系統、多相機校正程序、
前景移動物體分割、剪影、以及利用視殼演
算法(Visual Hull)之三維物體重建與相機模
擬系統。本年度不僅完成了場景中人物之重
建，亦實現了三維資料表視法的建構。 
 
關鍵詞：三維重建、電腦視覺、三維電視、
相機校正  
Keywords: 3D reconstruction, computer 
vision, 3D TV, camera calibration 
二、計畫緣由及目的 
實體三維電腦模型的重建為電腦視覺
研究領域的重要課題之一，它在傳統上的應
用範圍包含了逆向工程、圖形識別以及工業
上的檢驗等。 
由於電視為人類的重要日常消遣之
一，因此許多電視相關之影音設備不斷的更
新，這個社會的現象使得許多研究人員投入
多媒體的相關研究。人類所生活的地方是一
個三維立體的世界，而目前的電視皆以二維
平面的連續影像所結合產生的影片，來作為
影音多媒體的播放資訊，所以研究人員投入
相當多的努力在三維的影音設備與模型建
構上。例如在聲音上Rendu[1] 提出一個即
時的音訊擷取與擬真之方法在一個真實的
三維環境上即時擷取一般麥克風的音訊。而
在三維物體模型的取得與建構上，則有
Ming[2]等人利用多視角剪影資訊得到一個
以多面體(polyhedral)表示的可見外殼
(visual hull)，並以此可見外殼為初始模型去
限制立體視覺(stereo)演算法的搜尋範圍，
改善立體視覺演算法的深度圖，並能使得重
建物的凹陷區域獲得改善。 
 Esteban[3]等人提出先以多相機視角之
間的體積交集產生以八分樹(octree-based)
的方式來表示粗略幾何模型，再用匹配立方
體網格三角化轉為網格(mesh)資訊，然後利
用多相機之間的色彩資訊，針對網格資訊中
的頂點(vertex)進行改善的動作。此方式並
無法使用在紋理不明顯的物體上，並且必須
額外執行立方體網格三角化法才能建立出
標準的多邊形模型。 
 本計劃所提出來的方式是在多架相機
環場的架設場景下進行影像的擷取，我們先
透過相機校正藉以獲得多架相機之間的內
外部參數資訊，接著利用影像色階降維之擷
 圖：使用八張「貓的虛擬影像」重建之結果，
右側為頭部的特寫。 
 
視覺外殼演算法 Visual Hull 
使用 CheungError! Reference source 
not found.]的 visual hull 方法，取得三維點
資訊，然而，光有這些離散的資訊是不夠
的。Power CrustError! Reference source not 
found.]可用於將空間中一群三維點連接成
連續的多邊形，並會包覆住整個 3D 模型，
在將多邊形化為多個三角形。如此一來，這
樣便有了網格(mesh)資訊了。藉由網格，便
能透過不同的計算獲得更多資訊，如法向
量、平面方程式等。 
首先，我們先實作 view-independent 
texture mapping。此方法是以多台相機的顏
色依權重混合後繪製在 3D 模型表面上，也
就是網格。而權重根據網格之法向量與各來
源相機(reference camera)視線間的夾角而
定。若夾角越接近 180 度，也就是說此網格
越近似以正面被該相機擷取到，其權重值越
大。然而，此方法產出的影像品質卻不盡理
想。由於網格與網格之間並不一定平滑，造
成影像上會有不連續的狀況發生。 
View-dependent rendering 便可以解決
這問題。除了多相機影像、相機參數與網格
資訊外，更將新視角(Novel View)相機的相
機參數加入計算。以 View-dependent 
vertex-based texture mappingError! 
Reference source not found.]為例。該作者
取代以網格法向量與各相機視線間的夾角
而定的權重值，改以新視角視線與各來源相
機視線的夾角作為權重 Wc，c 為來源相機
編號。夾角越小表示該來源相機視線越靠近
新視角視線，所佔權重值越大。如此一來，
影像上不連續的狀況便改善不少。 
然而，該篇是將空間中的三維點填上顏
色。若網格不夠細緻的話，會讓新視角輸出
影像較為模糊。因此，這裡我們用一種以像
素(pixel)為基底的方法做新視角影像輸出。
如下圖，假設已知新視角相機的相機參數，
那麼我們便可以定出相機中心與每個像素
在空間中的座標。利用類似光追蹤(ray 
tracing)的方式，空間中的相機中心與像素
Pij 的射線便會與 3D 模型交於一點或多點，
取最近的點即是該像素看到的點 V。將點 V
投影回各來源相機的影像平面上抓取顏色
資訊，並乘以前述之權重值 Wc 混合形成 Pij
的顏色。這樣便同時解決影像上不連續的問
題且提高了影像品質。 
 
上述有個很大的問題在於速度。因為每
條射線都必須與每個網格計算是否有相
交，這是非常耗時的。因此我們利用深度圖
(depth map)來製作網格對照表(mesh lookup 
table)，用意在迅速知道每條射線會相交於
哪一最近的網格。由於我們已有 3D 模型，
可輕易建立新視線的深度圖。接著，要建立
網格對照表：對於各個網格，先投影回新視
角影像平面，如此可以知道哪幾條射線會相
交於該網格。接著判定該網格相對於新視線
的深度是否等同深度圖。是的話表示該網格
會與其上射線交於最近的點 V，反之則表示
該網格被遮蔽。 
最後，在 Error! Reference source not 
found.]中提到，即使像機再多，顏色混合時
只需要最靠近的 2~3 台相機即可，過多反而
造成錯誤的顏色被混入其中。因此，在計算
權重值時多給予一 m 次方，大大使得越靠
近的相機權重佔越大。 
中圖：拉拉棒經過三維重建後之完整模型 
右圖：針對三維點座標與 RGB 彩色資訊重
新取樣後之完整模型 
除了真實模型的重新取樣，模型資訊也
會一併建立 RGB map。概念如同 data map
示意圖，將方向角與仰角各角度所對應在模
型上的彩色資訊，利用一張 2D 的 map 加以
表示，使得此 map 可以簡單的表示出該模
型的樣子。
 
圖：RGB map。map 的取樣原點為拉拉棒的
模型正中心，然後以方向角 360度(x軸方向)
與仰角 90 度(y 軸方向)的色彩資訊取樣。 
 
整個系統流程如下圖所示： 
將經過重建的三維場景模型，經過
unified representation 的方式加以重新取樣
後，會得到一個比原本重建模型還要精簡的
新模型。但由於 unified representation 有其本
身必然的缺點，所以為了彌補此項缺點，我
們再利用 non-unified representation 的方法
依照需要的精細程度再度做重新取樣的動
作，進而得到我們所希望的細緻模型。 
藉由重新取樣後模型距離 r的histogram
統計結果，來取得角度射線每一層的距離門
檻值，其實此方法還有待精進。此乃因為若
門檻值取得不好，會使得重新取樣後的結
果，其取樣點的分佈呈現如下圖所示一般一
段鬆散一段緊密的結果。 
 
 
四、結論與成果 
3DAV 實驗計劃實行期間，多個實做階
段皆已略有不錯的成果，各項目介紹重點如
下： 
 
1. 同步攝影系統架設 
目前我們已順利架設一套同步攝影系
統，以個人電腦搭載 4 張影像擷取卡予以並
外接 13 台類比擷取攝影機；在這個攝影機
環場擺設的環境下，我們的系統能夠在各相
機滿足同步擷取的情況下達到每秒 6 張影
像的儲存(減少攝影機數量時可再提高每秒
所能儲存影像的數目)。 
 
圖：攝影機環場擺設示意圖。攝影機為環形
擺設，分三層，共 13 架。第一層 8 架，每
45 度 1 架。第二層 4 架，俯視角 60 度，每
90 度 1 架。第三層 1 架，垂直往下看。 
 
圖：用於小物體的攝影機環場擺設 
 
2. 攝錄影像前景擷取 
在取得多個相機攝錄的影像之後，為銜
接後續各階段所需的圖像資訊，我們必須濾
除隨著物體一併被錄製下的背景畫面；我們
從預先拍攝的背景影像作為參考依據，透過
一些已發展成熟的相關演算法來達成這個
目的。 
從虛擬實驗看來，我們提出的方法可讓
產出的新視線影像幾乎等同於原始拍攝影
像。依新視線視角與各相機視角間的角度作
為權重比例讓輸出影像不會有邊緣不連續
的情況。此外，以像素為基底更可讓影像品
質更精確，邊緣相當清晰。然而，從 view0
與 view1 之間的新視線看的出一個問題:當
交點 V 反投影回用以混合的原始影像，卻
可能因為遮蔽造成抓取到的顏色不正確。 
接著以實際拍攝的影像作為實驗輸
入。下圖為八張拍攝木偶人的影像。6810
個 point，8694 個 mesh。 
 
相機擺設位置如下圖
 
其結果如下: 
a.將 novel view 設置在同於 view0 的位置 
  
圖左：Reference view 0。圖右：Novel view 
 
b.將 novel view 設置在 view1 與 view2 之間 
 
圖左到右分別為：Reference view 0、Novel 
view、Reference view 1。 
 
以實際拍攝機器人的影像當做實驗輸
入，相機架設方式與虛擬實驗相同。環形擺
設分三層，下層八架、中層四架、上層一架，
共十三架。拍攝到之影像依下中上順序如下
圖所示。13303 個 point，30543 個 mesh。 
 
其結果如下: 
a.將 novel view 設置在同於 view0 的位置 
 
圖左：Reference view 0。圖右：Novel view 
b.將 novel view 設置在 view0 與 view1 之
間，且恰好在 view8 的斜右下方 
 
使得距離(r)可用 0~255 之間的灰階值加以
表現。圖中所表示之灰階值，距離射線原點
越近之距離(r)值，其代表之灰階值越明亮，
反之離射線原點越遠之交點的 r 值，代表的
灰階值越暗。 
 
圖：將 map 的取樣原點移動至史賓機器人
的模型正中心，然後做方向角 360 度(x 軸方
向)與仰角 90度(y軸方向)的色彩資訊(RGB)
取樣，然後直接依照其方向角與仰角的相對
交點位置，將色彩資訊顯示在此二維 map
上。 
 
 
左圖：真人第 1 架攝影機所拍攝之影像 
中圖：經過三維重建後之完整模型 
右圖：重建後之三維模型(中圖)再經過三維
點資訊與色彩資訊重新取樣之結果 
 
 
圖：將經過重新取樣的真人模型，依照重新
取樣後所得之交點距射線原點之距離(r)，透
過量化後，將距離(r)依照 0~255 之灰階值建
立深度圖。 
 
 
圖：將 map 的取樣原點移動至真人的模型
正中心，然後做方向角 360 度(x 軸方向)與
仰角 90 度(y 軸方向)的色彩資訊(RGB)取
樣，再直接依照其方向角與仰角的相對交點
位置，將色彩資訊顯示在此二維 map 上。 
五、參考文獻 
[1] Emmanuel Gallo, Nicolas Tsingos, and 
Guillaume Lemaitre, “3D-AudioMatting, 
Postediting, and Rerendering from Field 
Recordings”, EURASIP Journal on Advances 
in Signal Processing, Volume 2007, Article ID 
47970, 16 pages. 
[2] M. Li, H. Schirmacher, M. Magnor, H.-P. 
Seidel, “Combining Stereo and Visual Hull 
Information for On-Line Reconstruction and 
Rendering of Dynamic Scenes,” IEEE 
Workshop on Multimedia Signal Processing, 
pp.9-12, 2002. 
[3] C. Hernandez Esteban, F. Schmitt., 
“Multi-Stereo 3D Object Reconstruction,” 
Proceedings of International Symposium on 
3D Data Processing, Visualization and 
Transmission, pp. 159-166, 2002. 
[4] R. Tsai. “A versatile camera calibration 
technique for high-accuracy 3dmachine 
visionmetrology using off-the-shelf tv cameras 
and lenses”. IEEE Trans. Robotics and 
Automation, 3(4):323–344, 1987. 
[5] E. Trucco and A. Verri, “Introductory 
Techniques for 3-D Computer Vision”, 
chapter 10, Prentice Hall, Upper Saddle 
River, N.J., 1998 
[6] R. Hartley and A. Zisserman, “Multiple 
View Geometry in Computer Vision, 2nd 
3D Reconstruction by Combining Shape from Silhouette with Stereo
Huei-Yung Lin and Jing-Ren Wu
Department of Electrical Engineering
National Chung Cheng University
Chia-Yi 621, Taiwan, R.O.C.
lin@ee.ccu.edu.tw, jr@image.ee.ccu.edu.tw
Abstract
In this paper we propose a 3D reconstruction algo-
rithm by combining shape from silhouette with stereo.
Visual hull of the object is first derived from multi-view
silhouette images. Pairwise stereo matching for shape
refinement is then accomplished using the best viewable
images. Based on the reduced correspondence search-
ing range constrained by contact points and bounding
edges, significant improvement of visual hull is possible
even if the number of cameras is limited. Experimental
results are presented for both synthetic data and real
scene images.
1 Introduction
3D model reconstruction of a real scene is an im-
portant and active research topic in computer vision. It
has many applications ranging from industrial inspec-
tion and reverse engineering to computer graphics and
multimedia. The objective is usually to recover the ge-
ometric (and possibly the photometric) information of
the scene using intensity images recorded by a camera.
The acquired 3Dmeasurements can then be used to gen-
erate a computer model of the scene.
In the past few decades many 3D reconstruction al-
gorithms based on different visual cues, such as stereo,
motion, shading, silhouette, texture, have been pro-
posed [12]. For a given real object, however, not all of
the above methods are capable of creating a complete
3D model without acquisition, registration and data fu-
sion of multiple range images [14]. For example, stereo
vision or shape from shading can only provide the so-
called 2.5D range data from a single viewpoint. Mul-
tiple image captures from different viewing directions
are mandatory for the reconstruction of a complete 3D
model. In addition to the 3D alignment between dif-
ferent range data sets, the reconstruction of a dynamic
scene is also not possible without simultaneous multi-
view 3D recovery.
Shape from silhouette, on the other hand, reconstruct
a 3D model using the object silhouettes acquired from
the surrounding cameras [7]. Since the complete 3D
model is recovered by the intersection of all silhouette
cones back-projected from the camera centers, it can be
implemented very easily and is also suitable for the re-
construction of moving objects. One major drawback
of this approach is that the resulting visual hull is gen-
erally not a good geometric approximation of the ob-
served shape. It might be even worse if the number
of cameras is reduced or the object consists of appar-
ent concave surface shape. Thus, a number of methods
have been proposed to improve shape from silhouette
with additional constraints [6, 2, 1, 11].
Our goal is to refine shape from silhouette with
stereo to extract more precise geometric models. Sev-
eral approaches for 3D reconstruction by exploiting the
advantages of combining these two techniques have
been investigated in recent research. Li et al. con-
struct a polyhedral visual hull from silhouettes as an ini-
tial estimate, and it is then used to restrict the disparity
searching range for stereo [8]. Esteban et al. generate
an octree-based coarse model from visual hull followed
by a multi-stereo carving technique for refinement [5].
Cheung et al. assume the object is under rigid motion
and improve shape from silhouette by registering and
refining the visual hulls across time [2].
In this paper, we present a 3D reconstruction algo-
rithm using the visual hull derived from the object sil-
houettes with the refinement of pairwise stereo match-
ing. Different from the previous approaches, our 3D
surface is refined based on the best viewable stereo im-
age pair. Given the cameras with fixed positions and
orientations, the 3D reconstruction result is more pre-
cise for certain viewpoints over others. It is therefore
suitable for stereoscopic image synthesis with predeter-
mined primary viewpoints.
978-1-4244-2175-6/08/$25.00 ©2008 IEEE
Figure 2. Simulation results of synthetic
data set used for error analysis. From the
left to the right: one of the eight rendered
images, 3D reconstruction by shape from
silhouette, stereo refinement with the pro-
posed algorithm.
to be identified. Consider the bounding edge Ejm origi-
nated from them-th image, the corresponding image for
stereo matching is selected by the one with the smallest
angle between its optical axis and the viewing edge rjm.
Mathematically, the stereo image pair (Im, In) is deter-
mined by
argmax
m,n
cos(rjm, on)
where on represents the optical axis of the camera Cn.
Since the viewing edge rjm is not fixed for different
boundary point ujm of the silhouette Sm, the corre-
sponding image for stereo matching varies with the po-
sition of ujm.
Once the best viewable stereo image pair is obtained,
the contact point on any bounding edge can be identified
by template matching and the epipolar constraint. A
sphere can then be created based on the length of the
bounding edge, and the depth range is estimated within
the sphere radius. Note that, since the contact points are
the points contained in the visual hull and closest to the
true object surface, the depth range for correspondence
searching can be further restricted to the same side with
respect to the bounding edge.
The presented algorithm for stereo-based visual hull
refinement can be summarized as follows. The bound-
ing edges generated by shape from silhouette are used
to create the spheres centered at the containing contact
points. The circle projections of the spheres onto the
best viewable stereo image pairs are then used to restrict
the correspondence searching range. Thus, based on the
robust but sparse bounding edges derived from silhou-
ettes, a dense 3D model is reconstructed by stereo with
highly constrained correspondence searching regions.
Figure 3. 3D reconstruction of a pawn and
a cat object from synthetic data set.
4 Experimental Results
Simulation with synthetic data sets is carried out first
to demonstrate the validity of our algorithm. Two tex-
tured 3D computer models, a cone and a sphere, are
generated and rendered using VTK (The Visualization
Toolkit) with eight surrounding virtual cameras. Figure
2 shows one of the captured images, the results of shape
from silhouette and the proposed method, respectively,
for both objects. Different color in the results repre-
sents 3D reconstruction from different camera. The er-
ror analysis in terms of mean absolute deviation (MAD)
is tabulated in Table 1. The base radius and height of the
cone are 4 mm and 8 mm, respectively. The radius of
the sphere is 5 mm. Figure 3 shows the visual hulls and
stereo refinements of more complex objects.
Our experimental setup for 3D model reconstruction
includes eight cameras surrounding the object and fac-
ing downward about 45 degrees. The intrinsic and ex-
trinsic camera parameters are calibrated using Tsai’s
method [13]. Figure 4 shows the foreground images
acquired from all viewpoints. The 3D models recon-
structed using shape from silhouette and the proposed
method are illustrated in Figures 5(a) and 5(b), respec-
tively. The head and other body parts of the object ap-
pear more smooth by the stereo refinement. Depend-
ing on the object size, the processing time with the pro-
posed stereo refinement generally increases about 20%
– 60% compared to the computation of shape from sil-
houette alone.
Table 1. MAD of the virtual objects.
Object model Cone Sphere
Shape from silhouette 0.13 (mm) 0.10 (mm)
The proposed method 0.08 (mm) 0.09 (mm)
Depth Recovery Using Defocus Blur at Infinity
Huei-Yung Lin and Kai-Da Gu
Department of Electrical Engineering
National Chung Cheng University
Chia-Yi 621, Taiwan, R.O.C.
lin@ee.ccu.edu.tw, da0326@yahoo.com.tw
Abstract
This paper presents a depth recovery technique based on
the maximum defocus blur associated with a camera focus
setting. The depth-blur relation is formulated by a math-
ematical model and verified by defocus calibration. Im-
age intensity histogram analysis is used to identify the blur
extent. Different from the existing depth from defocus ap-
proaches, our method is capable of depth recovery using
a single image, possibly up to scale. Experiments on real
scene images have demonstrated the feasibility of the pro-
posed method for depth recovery.
1. Introduction
Depth recovery is one of the essential problems in com-
puter vision research. The objective is to obtain the dis-
tance of an object in a scene using the intensity images
captured by the cameras. Depending on the number of
camera viewpoints required for 3D reconstruction, the most
commonly used techniques can be classified as shape from
stereo/motion and depth from focus/defocus [2]. The for-
mer requires the images captured from multiple viewpoints,
while the latter usually invloves changing the internal pa-
rameter settings of the camera system [8, 6]. Nevertheless,
both methods require multiple image captures for depth re-
covery.
Although stereo or motion based approaches are capa-
ble of generating dense depth map of the scene, they suffer
from the corresponding problem associated with different
viewing positions. The computational cost is also relatively
high due to the correspondence searching. In the methods
of conventional depth from focus or defocus, multiple im-
ages are captued from the same viewpoint with different fo-
cus settings of the camera [1]. Some focus measures are
applied to derive the best focused image or the amount of
defocus, which are then used to compute the distance of the
object [9, 4]. Recently, depth recovery from a single image
captured using a coded aperture is also proposed in Compu-
tational Photography community [7, 3]. However, it usually
requires a good design of the coded pattern and modifica-
tion of the camera system.
In this paper, we present a theory of depth recovery us-
ing the defocus blur obtained from an object placed at an
infinite distance from the camera. The relationship between
depth and defocus blur of the object is derived as a func-
tion of the camera focus range and the observed blur extent
at infinity. Through a calibration stage for determining the
blur at infinity of a given camera focus range, the depth of
an object located at any position can be calculated using the
derived depth-blur relation function. Thus, our approach is
capable of depth recovery from a single defocus image cap-
tured by a calibrated camera, which is not currently avail-
able in any algorithms.
Different from the existing techniques for blur identifica-
tion (e.g. [10, 5]), a novel method based on image intensity
histogram analysis is proposed for defocus calibration. In
case of the illumination condition change and the blur ex-
tent varies, the scale factor is identified by a groundtruth
depth-blur relation. It is then used to derive the blur at in-
finity for other depth computations in the environment. The
experimental results have demonstrated the robustness of
our blur identification method as well as the feasibility of
the proposed depth recovery technique.
2. Image Formation for Defocus Blur
For a thin lens model with focal length f , the relation-
ship between the position of a point P in the scene and the
corresponding focused position P ′ in the image is given by
the well known lens formula
1
p
+
1
q
=
1
f
(1)
where p is the distance of the object from the lens on one
side, and q is the distance of the image plane from the lens
on the other side.
978-1-4244-2175-6/08/$25.00 ©2008 IEEE
ters on both sides of the histogram represent the black and
white regions in the image. Furthermore, the blur pixels in
the images correspond to the histogram areas with inten-
sity values between the two clusters, since the optical de-
focus process introduces a gradual intensity transition for
the black and white image. Let T1 and T2 be the upper and
lower bounds of the left and right clusters in the histogram,
respectively. Then the number of pixels with intensity val-
ues below T1 and T2 represent the non-white and black re-
gions in the image, respectively. Ideally, these two regions
should be bounded by an inner and an outer circle with radii,
say R1 and R2, respectively. Thus, the blur extent defined
by the intensity values between T1 and T2 can be derived
from
b = ΔR = R2 −R1 = 1√
π
(
√
A2 −
√
A1) (9)
whereA1 and A2 are the number of pixels below thresholds
T1 and T2, respectively.
In the above algorithm for blur extent estimation, T1 and
T2 in the histogram are the only parameters to be identified.
These two parameters can be obtained by finding the abrupt
pixel count changes for two consecutive graylevels with a
given threshold. Since T1 and T2 correspond to the upper
and lower bounds of the blur regions, they are identified by
searching from the middle of the histogram to the left and
right, respectively. In practice, due to the acquisition noise
and digitization, the image pixels with the intensities T1 and
T2 might not be perfect circles, as illustrated in Figure 1(b).
Thus, a circle fitting algorithm based on Hough transform is
used to derive the radii R1 and R2 for blur extent computa-
tion. It should be noted that the proposed method does not
need to identify the centers of the circles explicitly, which
is usually not possible for defocused images.
Figure 2 shows the blur extent (in pixel) versus object
distance obtained from real images. It shows that the blur
extent approaches to an upper limit as the object moves be-
yond a certain range. The blur extent at infinity and the
distance with minimum defocus blur represent the camera
constant c and the focus range p as suggested by Eq. (2),
respectively. These two parameters can be derived from a
least-squares fitting using the equation and the calibrated
blur-distance pairs. The red and green marks illustrated in
Figure 2 represent the identified blur extents using the above
algorithm and the curve fitting results, respectively.
It should be noted that the blur extent also depends on the
intensities of the captured image pattern. Thus, there might
exist a scale factor λ for the blur extent obtained from a
different scene or a different illumination condition. Since
the scaling is constant in terms of the scene distance, it can
be easily identified by the ratio of the blur extents at infinity
for different scenes. The resulting blur-depth relation curve
can then be established and used to recover the distance of
any blur extent.
0 50 100 150 200 250 300 350 400 450 500 550 600 650 700
0
10
20
30
40
50
60
70
80
90
100
distance (cm)
bl
ur
 e
xt
en
t (p
ixe
l)
blur extent
curve fitting
Figure 2. Blur extent vs. object distance. The
circles represent the curve fitting result.
Except for the defocus calibration process, it might not
always be able to obtain the blur at infinity. In this case, a
known groundtruth depth z and the observed blur extent d
can be used to derive the constant c by
c =
zd
z − p (10)
where p is the focus range of the camera. This constant can
then be plugged into Eq. (8) for depth computation. One
should notice, however, even the groundtruth distance or the
blur at infinity are not available, it is still possible to recover
the relative depth between two object locations based on the
initial depth-blur calibration curve.
4. Experimental Results
A planar object pattern placed in front of the camera is
used to demonstrate the feasibility of the proposed depth re-
covery technique. Figure 3 shows the images of the object
captured at 700, 800, 900, 1000, 1100, 1200 mm with the
camera’s focus range set as 300 mm. The blur extents of the
images are identified by the variations of intensity profiles
on the pattern boundaries. The calibrated blur-distance re-
lation given in Figure 2 is used to estimate the depth of the
planar object based on the observed blur extent.
Due to the possible illumination change between the cal-
ibration stage and depth recovery experiments, the blur at
infinity might differ by a scale factor between these two
cases, as described in Section 2. Thus, the observed blur-
distance relation at 1200 mm is selected as a groundtruth
to derive the blur at infinity c using Eq. (8). The result-
ing constant c is then used in the same equation to calculate
the depths corresponding to all other images based on the
observed blur extents.
出席國際學術會議心得報告 
                                                             
計畫編號 NSC 96-2221-E-194-016-MY2 
計畫名稱 3D音視訊的內容製作、處理與互動式呈現--子計畫二：多視域視訊的影像擷取與表示法建構（第 2年） 
出國人員姓名 
服務機關及職稱 
林惠勇 
國立中正大學電機工程學系 
副教授 
會議時間地點 
Doc. 8 – 11, 2008 
Tampa, Florida, USA 
會議名稱 The 19th International Conference on Pattern Recognition 
發表論文題目 
1. 3D Reconstruction by Combining Shape from Silhouette with Stereo 
2. Depth Recovery Using Defocus Blur at Infinity 
 
一、參加會議經過 
 
本次出席的國際會議 International Conference on Pattern Recognition (ICPR) 為電腦視
覺、圖形識別等相關研究領中相當重要且知名之國際會議，此一會議為 International 
Association for Pattern (IAPR) 主要贊助之國際會議並依循慣例每兩年舉辦一次。本年度“第十
九屆 IAPR 圖形識別國際研討會” (ICPR 2008) 安排於十二月七日至十二月十一日共五天，在
美國佛羅里達州 Tampa 的國際會議中心舉行，並由南佛羅里達大學負責會議的籌備。歷年
來 ICPR 皆為相當大規模且相當高品質的研討會，今年更吸引了約一千六百篇的論文投稿。
由於地區的便利性其與會人士可能約兩三千人，這些人員大多來自世界各先進國家在各個領
域的工程師、專家、學者以及部分的軍方人士，其中亦不乏相當知名的國際學者。由於會議
的規模龐大且涵蓋的領域相當廣，因此除了 poster section 與 oral section 同時進行之外，oral 
section 亦分為七個同時段不同地點的 parallel section。 
本屆會議在美國佛羅里達州舉辦，適逢十二月份的冬季參與會議與前往觀光的人相當踴
躍。研討會的第一天為 Tutorial sections及 workshops，其中有相當有趣的 topics 及知名演講
者，但因旅程的規劃無法參與。筆者到達佛羅里達 Tamp時為主要研討會的第一天，由於到
達時間為清晨，因此就直接搭車前往會場。此後數天幾乎整天在會場聆聽 oral section 的 
presentation，觀看 poster presentation 的海報以及與與會學者互動討論。在研討會場上也遇到
了不少清大、交大、政大、中研院過去的學者，用餐及行動上增加了不少便利性。這一次參
與的國際會議雖為期五天，但因行程的安排僅能停留四天三夜實為此行最大缺憾。 
 
二、與會心得 
 
ICPR為 IAPR 所直接贊助兩年一度的研討會，其論文的品質一向相當優良，而眾多的
大師級學者也必親自與會。本屆本會議中的幾個 plenary speeches 涵蓋的範圍央當廣泛，從
3D Reconstruction by Combining Shape from Silhouette with Stereo
Huei-Yung Lin and Jing-Ren Wu
Department of Electrical Engineering
National Chung Cheng University
Chia-Yi 621, Taiwan, R.O.C.
lin@ee.ccu.edu.tw, jr@image.ee.ccu.edu.tw
Abstract
In this paper we propose a 3D reconstruction algo-
rithm by combining shape from silhouette with stereo.
Visual hull of the object is first derived from multi-view
silhouette images. Pairwise stereo matching for shape
refinement is then accomplished using the best viewable
images. Based on the reduced correspondence search-
ing range constrained by contact points and bounding
edges, significant improvement of visual hull is possible
even if the number of cameras is limited. Experimental
results are presented for both synthetic data and real
scene images.
1 Introduction
3D model reconstruction of a real scene is an im-
portant and active research topic in computer vision. It
has many applications ranging from industrial inspec-
tion and reverse engineering to computer graphics and
multimedia. The objective is usually to recover the ge-
ometric (and possibly the photometric) information of
the scene using intensity images recorded by a camera.
The acquired 3Dmeasurements can then be used to gen-
erate a computer model of the scene.
In the past few decades many 3D reconstruction al-
gorithms based on different visual cues, such as stereo,
motion, shading, silhouette, texture, have been pro-
posed [12]. For a given real object, however, not all of
the above methods are capable of creating a complete
3D model without acquisition, registration and data fu-
sion of multiple range images [14]. For example, stereo
vision or shape from shading can only provide the so-
called 2.5D range data from a single viewpoint. Mul-
tiple image captures from different viewing directions
are mandatory for the reconstruction of a complete 3D
model. In addition to the 3D alignment between dif-
ferent range data sets, the reconstruction of a dynamic
scene is also not possible without simultaneous multi-
view 3D recovery.
Shape from silhouette, on the other hand, reconstruct
a 3D model using the object silhouettes acquired from
the surrounding cameras [7]. Since the complete 3D
model is recovered by the intersection of all silhouette
cones back-projected from the camera centers, it can be
implemented very easily and is also suitable for the re-
construction of moving objects. One major drawback
of this approach is that the resulting visual hull is gen-
erally not a good geometric approximation of the ob-
served shape. It might be even worse if the number
of cameras is reduced or the object consists of appar-
ent concave surface shape. Thus, a number of methods
have been proposed to improve shape from silhouette
with additional constraints [6, 2, 1, 11].
Our goal is to refine shape from silhouette with
stereo to extract more precise geometric models. Sev-
eral approaches for 3D reconstruction by exploiting the
advantages of combining these two techniques have
been investigated in recent research. Li et al. con-
struct a polyhedral visual hull from silhouettes as an ini-
tial estimate, and it is then used to restrict the disparity
searching range for stereo [8]. Esteban et al. generate
an octree-based coarse model from visual hull followed
by a multi-stereo carving technique for refinement [5].
Cheung et al. assume the object is under rigid motion
and improve shape from silhouette by registering and
refining the visual hulls across time [2].
In this paper, we present a 3D reconstruction algo-
rithm using the visual hull derived from the object sil-
houettes with the refinement of pairwise stereo match-
ing. Different from the previous approaches, our 3D
surface is refined based on the best viewable stereo im-
age pair. Given the cameras with fixed positions and
orientations, the 3D reconstruction result is more pre-
cise for certain viewpoints over others. It is therefore
suitable for stereoscopic image synthesis with predeter-
mined primary viewpoints.
978-1-4244-2175-6/08/$25.00 ©2008 IEEE
Figure 2. Simulation results of synthetic
data set used for error analysis. From the
left to the right: one of the eight rendered
images, 3D reconstruction by shape from
silhouette, stereo refinement with the pro-
posed algorithm.
to be identified. Consider the bounding edge Ejm origi-
nated from them-th image, the corresponding image for
stereo matching is selected by the one with the smallest
angle between its optical axis and the viewing edge rjm.
Mathematically, the stereo image pair (Im, In) is deter-
mined by
argmax
m,n
cos(rjm, on)
where on represents the optical axis of the camera Cn.
Since the viewing edge rjm is not fixed for different
boundary point ujm of the silhouette Sm, the corre-
sponding image for stereo matching varies with the po-
sition of ujm.
Once the best viewable stereo image pair is obtained,
the contact point on any bounding edge can be identified
by template matching and the epipolar constraint. A
sphere can then be created based on the length of the
bounding edge, and the depth range is estimated within
the sphere radius. Note that, since the contact points are
the points contained in the visual hull and closest to the
true object surface, the depth range for correspondence
searching can be further restricted to the same side with
respect to the bounding edge.
The presented algorithm for stereo-based visual hull
refinement can be summarized as follows. The bound-
ing edges generated by shape from silhouette are used
to create the spheres centered at the containing contact
points. The circle projections of the spheres onto the
best viewable stereo image pairs are then used to restrict
the correspondence searching range. Thus, based on the
robust but sparse bounding edges derived from silhou-
ettes, a dense 3D model is reconstructed by stereo with
highly constrained correspondence searching regions.
Figure 3. 3D reconstruction of a pawn and
a cat object from synthetic data set.
4 Experimental Results
Simulation with synthetic data sets is carried out first
to demonstrate the validity of our algorithm. Two tex-
tured 3D computer models, a cone and a sphere, are
generated and rendered using VTK (The Visualization
Toolkit) with eight surrounding virtual cameras. Figure
2 shows one of the captured images, the results of shape
from silhouette and the proposed method, respectively,
for both objects. Different color in the results repre-
sents 3D reconstruction from different camera. The er-
ror analysis in terms of mean absolute deviation (MAD)
is tabulated in Table 1. The base radius and height of the
cone are 4 mm and 8 mm, respectively. The radius of
the sphere is 5 mm. Figure 3 shows the visual hulls and
stereo refinements of more complex objects.
Our experimental setup for 3D model reconstruction
includes eight cameras surrounding the object and fac-
ing downward about 45 degrees. The intrinsic and ex-
trinsic camera parameters are calibrated using Tsai’s
method [13]. Figure 4 shows the foreground images
acquired from all viewpoints. The 3D models recon-
structed using shape from silhouette and the proposed
method are illustrated in Figures 5(a) and 5(b), respec-
tively. The head and other body parts of the object ap-
pear more smooth by the stereo refinement. Depend-
ing on the object size, the processing time with the pro-
posed stereo refinement generally increases about 20%
– 60% compared to the computation of shape from sil-
houette alone.
Table 1. MAD of the virtual objects.
Object model Cone Sphere
Shape from silhouette 0.13 (mm) 0.10 (mm)
The proposed method 0.08 (mm) 0.09 (mm)
Depth Recovery Using Defocus Blur at Infinity
Huei-Yung Lin and Kai-Da Gu
Department of Electrical Engineering
National Chung Cheng University
Chia-Yi 621, Taiwan, R.O.C.
lin@ee.ccu.edu.tw, da0326@yahoo.com.tw
Abstract
This paper presents a depth recovery technique based on
the maximum defocus blur associated with a camera focus
setting. The depth-blur relation is formulated by a math-
ematical model and verified by defocus calibration. Im-
age intensity histogram analysis is used to identify the blur
extent. Different from the existing depth from defocus ap-
proaches, our method is capable of depth recovery using
a single image, possibly up to scale. Experiments on real
scene images have demonstrated the feasibility of the pro-
posed method for depth recovery.
1. Introduction
Depth recovery is one of the essential problems in com-
puter vision research. The objective is to obtain the dis-
tance of an object in a scene using the intensity images
captured by the cameras. Depending on the number of
camera viewpoints required for 3D reconstruction, the most
commonly used techniques can be classified as shape from
stereo/motion and depth from focus/defocus [2]. The for-
mer requires the images captured from multiple viewpoints,
while the latter usually invloves changing the internal pa-
rameter settings of the camera system [8, 6]. Nevertheless,
both methods require multiple image captures for depth re-
covery.
Although stereo or motion based approaches are capa-
ble of generating dense depth map of the scene, they suffer
from the corresponding problem associated with different
viewing positions. The computational cost is also relatively
high due to the correspondence searching. In the methods
of conventional depth from focus or defocus, multiple im-
ages are captued from the same viewpoint with different fo-
cus settings of the camera [1]. Some focus measures are
applied to derive the best focused image or the amount of
defocus, which are then used to compute the distance of the
object [9, 4]. Recently, depth recovery from a single image
captured using a coded aperture is also proposed in Compu-
tational Photography community [7, 3]. However, it usually
requires a good design of the coded pattern and modifica-
tion of the camera system.
In this paper, we present a theory of depth recovery us-
ing the defocus blur obtained from an object placed at an
infinite distance from the camera. The relationship between
depth and defocus blur of the object is derived as a func-
tion of the camera focus range and the observed blur extent
at infinity. Through a calibration stage for determining the
blur at infinity of a given camera focus range, the depth of
an object located at any position can be calculated using the
derived depth-blur relation function. Thus, our approach is
capable of depth recovery from a single defocus image cap-
tured by a calibrated camera, which is not currently avail-
able in any algorithms.
Different from the existing techniques for blur identifica-
tion (e.g. [10, 5]), a novel method based on image intensity
histogram analysis is proposed for defocus calibration. In
case of the illumination condition change and the blur ex-
tent varies, the scale factor is identified by a groundtruth
depth-blur relation. It is then used to derive the blur at in-
finity for other depth computations in the environment. The
experimental results have demonstrated the robustness of
our blur identification method as well as the feasibility of
the proposed depth recovery technique.
2. Image Formation for Defocus Blur
For a thin lens model with focal length f , the relation-
ship between the position of a point P in the scene and the
corresponding focused position P ′ in the image is given by
the well known lens formula
1
p
+
1
q
=
1
f
(1)
where p is the distance of the object from the lens on one
side, and q is the distance of the image plane from the lens
on the other side.
978-1-4244-2175-6/08/$25.00 ©2008 IEEE
ters on both sides of the histogram represent the black and
white regions in the image. Furthermore, the blur pixels in
the images correspond to the histogram areas with inten-
sity values between the two clusters, since the optical de-
focus process introduces a gradual intensity transition for
the black and white image. Let T1 and T2 be the upper and
lower bounds of the left and right clusters in the histogram,
respectively. Then the number of pixels with intensity val-
ues below T1 and T2 represent the non-white and black re-
gions in the image, respectively. Ideally, these two regions
should be bounded by an inner and an outer circle with radii,
say R1 and R2, respectively. Thus, the blur extent defined
by the intensity values between T1 and T2 can be derived
from
b = ΔR = R2 −R1 = 1√
π
(
√
A2 −
√
A1) (9)
whereA1 and A2 are the number of pixels below thresholds
T1 and T2, respectively.
In the above algorithm for blur extent estimation, T1 and
T2 in the histogram are the only parameters to be identified.
These two parameters can be obtained by finding the abrupt
pixel count changes for two consecutive graylevels with a
given threshold. Since T1 and T2 correspond to the upper
and lower bounds of the blur regions, they are identified by
searching from the middle of the histogram to the left and
right, respectively. In practice, due to the acquisition noise
and digitization, the image pixels with the intensities T1 and
T2 might not be perfect circles, as illustrated in Figure 1(b).
Thus, a circle fitting algorithm based on Hough transform is
used to derive the radii R1 and R2 for blur extent computa-
tion. It should be noted that the proposed method does not
need to identify the centers of the circles explicitly, which
is usually not possible for defocused images.
Figure 2 shows the blur extent (in pixel) versus object
distance obtained from real images. It shows that the blur
extent approaches to an upper limit as the object moves be-
yond a certain range. The blur extent at infinity and the
distance with minimum defocus blur represent the camera
constant c and the focus range p as suggested by Eq. (2),
respectively. These two parameters can be derived from a
least-squares fitting using the equation and the calibrated
blur-distance pairs. The red and green marks illustrated in
Figure 2 represent the identified blur extents using the above
algorithm and the curve fitting results, respectively.
It should be noted that the blur extent also depends on the
intensities of the captured image pattern. Thus, there might
exist a scale factor λ for the blur extent obtained from a
different scene or a different illumination condition. Since
the scaling is constant in terms of the scene distance, it can
be easily identified by the ratio of the blur extents at infinity
for different scenes. The resulting blur-depth relation curve
can then be established and used to recover the distance of
any blur extent.
0 50 100 150 200 250 300 350 400 450 500 550 600 650 700
0
10
20
30
40
50
60
70
80
90
100
distance (cm)
bl
ur
 e
xt
en
t (p
ixe
l)
blur extent
curve fitting
Figure 2. Blur extent vs. object distance. The
circles represent the curve fitting result.
Except for the defocus calibration process, it might not
always be able to obtain the blur at infinity. In this case, a
known groundtruth depth z and the observed blur extent d
can be used to derive the constant c by
c =
zd
z − p (10)
where p is the focus range of the camera. This constant can
then be plugged into Eq. (8) for depth computation. One
should notice, however, even the groundtruth distance or the
blur at infinity are not available, it is still possible to recover
the relative depth between two object locations based on the
initial depth-blur calibration curve.
4. Experimental Results
A planar object pattern placed in front of the camera is
used to demonstrate the feasibility of the proposed depth re-
covery technique. Figure 3 shows the images of the object
captured at 700, 800, 900, 1000, 1100, 1200 mm with the
camera’s focus range set as 300 mm. The blur extents of the
images are identified by the variations of intensity profiles
on the pattern boundaries. The calibrated blur-distance re-
lation given in Figure 2 is used to estimate the depth of the
planar object based on the observed blur extent.
Due to the possible illumination change between the cal-
ibration stage and depth recovery experiments, the blur at
infinity might differ by a scale factor between these two
cases, as described in Section 2. Thus, the observed blur-
distance relation at 1200 mm is selected as a groundtruth
to derive the blur at infinity c using Eq. (8). The result-
ing constant c is then used in the same equation to calculate
the depths corresponding to all other images based on the
observed blur extents.
