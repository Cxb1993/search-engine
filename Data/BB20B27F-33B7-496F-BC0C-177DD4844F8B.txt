I 
 
摘要 
傳統監視系統多以分割畫面方式，將數個縮小之監視影像顯示於同一螢幕，標示場景名
稱於各分割畫面，供事件發生時辨識場景之用，並允許使用者觀察不同部分或相同部分不同
視角之場景。然而隨著監視系統逐漸普及且監視器設置數量及密度不斷上升，考量空間與顯
示器之數量、成本，欲配置數以百計監控攝影機於校園、城市路口、建築物各樓層走道等大
範圍監控區域，傳統分割畫面之監控系統已不敷使用，並有分割畫面相關性低、突發事件追
蹤困難、監視影像解析度低、全域監控困難等四大缺失。 
為解決上述傳統監視系統之缺失，全狀況掌控之沉浸式監視系統（Immersive Surveillance 
for total situational awareness）於二維衛星影像上以電腦圖學技術建構三維立體建築模型，
使用者可自行定義各樓層平面資訊與監視器相對應位置以建構樓層平面圖，將建築物外觀及
監控場景利用二維視訊與物件模型之對應以動態更新方式貼上二維靜態材質，完成三維場景
建置，使用者亦可以固定頻率依自行設定之路徑動態選擇最佳攝影視角進行巡邏。本系統提
供一直觀方式讓使用者於立體場景中循移動物件追蹤，可由監視器位置或監視器間鳥瞰場
景，藉由搖桿、滑鼠、變焦控制器操作視野選擇器，使用者可任意更改視角，設定欲偵測場
景之區域，透過操作介面拖曳出一塊矩形區域，並針對特殊物體選擇最佳觀點，藉以傳送場
景變動時間、偵測區代號、物件特徵給予主控台且同時儲存相關訊息於資料庫供日後檢索；
全域軌跡追蹤器包含監視器位置、視野與移動物件軌跡，連續軌跡由不同監視器融合而成，
當被追蹤物件移出某一監視器視野，軌跡隨即遞交至至下一場景並轉換監視器，使用者僅藉
由一大型融合之監視器監控場景，達到物件無縫行動監控之目標；虛擬巡邏路徑提供使用者
循軌跡行走監控全域場景變化，並結合物件大小、移動模式、動態背景更新等技術降低因為
光源、落葉、水波…等等環境突發事件而造成誤判的狀況發生。 
關鍵字：沉浸式監視系統、虛擬巡邏、三維監控場景 
 
 
 
 
III 
 
cameras. Combined with the analysis of target size and pattern of movement, and dynamic 
background updating, the false alarms due to changes attributable to environmental lighting 
condition, fallen leaves, water ripples, etc., can be significantly reduced. The activities can be 
analyzed automatically by event recognition techniques. 
Keyword：immersive surveillance; Virtual Patrol; 3D surveillance scene  
2 
 
第二章、文獻探討 
影像接合（Scene Stitching）[1]：假設監視器分佈於不同區域中，則第 i個區域所擷取之
二維影像可表示為： ( ){ },i i iZone x y= ，經影像接合後，可獲得單一全景影像：
ii
Scene Zone=

；無接縫之影像接合其挑戰在於二維影像同一時間從不同二維空間擷取引發
之視差錯誤補償。 
視訊融合（Video Fusion）[2]：將數個具有重疊部份之二維視訊結合為一全景視訊，於不
同時間點展現二維影像結合技術；令 { }( ) ,i iCamera t t Zone= 代表第 i 個區域所擷取之二維視
訊 ， 經 過 視 訊 融 合 技 術 可 在 一 固 定 時 間 區 間 獲 得 二 維 全 景 視 訊 ：
( , )i iiVideoFusion Camera t Zone= ；除視差錯誤之補償外，必須取得同步之視訊影像。 
增強虛擬實境（Augmented Virtual Environment）[3-4]：將三維物件模型、二維材質影
像與連續監控視訊等各種資訊，依不同時間及二維空間關係同步整合至單一二維場景，提供
擬 真 、 完 整 之 直 觀 全 域 視 野 ， AVE VirtualEnvironment VideoFusion= ∪ ， 其 中
( ){ }, ,VirtualEnvironment x y z= ，其技術關鍵在於二維材質投影於三維模型必須完整融合。 
多攝影機間之協同偵測：將每個攝影機各自所提供之區域資訊[5]，利用[6]加以識別及分
析，整合成為具有涵蓋完整空間之全域監控資訊，除能將全域視訊同步顯示之外，亦能快速
進行大範圍之事件檢索、事件分析與預警，提供整合資訊給監控人員。移動物體偵測方面，
因視訊監控影片皆由一連串影像序列所構成，而每一張影像包含前景與背景兩部份，移動物
體為影像中之前景，故以背景模型（Background Modeling）為基礎並適當去除背景雜訊，針
對後續輸入監視影像進行前景偵測，擷取具有監控意義之移動物件，建立背景模型之方法眾
多，如使用顏色和方向資訊[7]，或利用像素及區塊資訊[8]等描述背景之特徵，多以高斯混
合模型（Gaussian Mixtures）[9]、無參數模型[10]及Codebook[11]背景模型進行背景學習
與前景偵測。其中Takeo Kanade[12]等人利用多攝影機間之協同偵測進行跨區域間之移動行人
與車輛之偵測與追蹤、分類、辨識、動態分析、進行三維座標判斷，自動即時收集與傳遞即
時影像資訊，創立出一套具有協同合作性之多攝影機監控系統，得以對多個運動目標進行無
縫檢測和跟蹤的協同感知系統提供連續即時監控，並利用多攝影機間之資訊建立地點模型
（Site Model）之三維場景；Nam T. Nguyen[13] 等人描述一適用於室內場合對多個目標進行
跟蹤之分佈式監控系統，該系統由多個鏡頭攝影機構成，具有多個攝像機處理模組和一個中
央模組用於協調攝像機間的跟蹤任務。由於每個運動目標有可能被多個攝影機同時跟蹤，因
此如何選擇最合適之攝影機對某一目標跟蹤，特別是在系統資源不足時，成為一個問題。提
出支新演算法能根據目標與攝影機間距離，並考慮遮蔽(occlusion)情況，把目標分配予對應攝
影機，因此出現遮蔽時，能將出現目標指定予能觀測到目標並距離最近之攝影機。實驗結果
顯示該系統能協調多個攝影機進行目標跟蹤，並妥善處理遮蔽狀；Chueh-Wei Chang[14] 等人
利用圖形空間觀念，於共同時間點對不同空間狀態加以識別、提升物件偵測與追蹤效能、單
一中控主機統籌管理多支攝影機、由使用者自行定義監控場景範圍、與即時且有效之異常事
件通報等功能。藉由空間關係建立圖形拓撲，使監控系統具備擴充彈性及環境適應性。透過
局部區域偵測以減少全畫面影像內容大量比對之計算量，可加快偵測及追蹤物件速度。經過
條件路徑與警戒區域之搭配偵測，可掌握第一時間發送警報之時機，減少監控人力負擔。透
4 
 
壁三部分以貼材質方式貼進場景[25,26,27]，擷取出夜間移動物體將有利提昇整體監控安全
性。 
擷取特徵與fusion：Jing Li 等人[26]利用夜晚影像之餘白天影像之Illumination與
Motion變化擷取特徵，最後轉換至頻率域進行Fusion；首先針對夜晚影像進行背景預測，先
決條件假設前景變動程度較背景劇烈，若該pixel與上一張frame相對位置絕對差值小於臨界
值則視為微距變動並將counter減1，反之則為劇烈變動，訓練一段時間之後若counter遞減至
0則該pixel可視為背景。有了背景即可分割前景，首先將夜晚影像轉換成灰階影像，有別於
傳統Histogram Equalization 分割成256 levels，此部份以illumination為基礎將所有pixel
分割成M 個level，量化之後再進行Histogram Equalization，以此結果做為臨界值分別取夜
晚影像與預測背景之RGB絕對差值，大於臨界值則視為夜間移動物，將該pixel分割出來以M 
Map表示之，夜晚影像中唯有人造光部份(室內透出光、路燈等)其亮度較白天影像同一位置
高，將RGB轉至HSV 色彩空間後判斷夜晚影像V值較白天影像高之pixel，過濾夜晚影像亮度較
白天影像高之部，以 L M map表示之，最後以L 與M 兩張map 為判斷依據和參考測試結果[27]，
搭配Shift Invariant Discrete Wavelet Transform(SiDWT1)[28]小波轉換方式將原始白天
與夜晚影像轉換至頻率域進行Fusion，反轉回來即為結果。 
多重資訊混合式偵測器:著重於資訊融合，色彩、動態位移量、五官特徵與幾何資訊等均
為融合時之良好素材，再依多數決以決定最佳組合，如Rowley [29]以類神經網路為基礎之人
臉偵測系統（圖十一）、Hsu 等人[30]利用顏色與雙眼、嘴巴間三角幾何關係進行人臉偵測
等。 
    遞交: Jiman Kim 等人[31] 利用前景區塊數與物件與各監視器之角度資訊來判斷何時應
該進行遞交以及應該遞交至哪隻監視器，首先使用SKDA(Sequential Kernel Density 
pproximation)背景相減法估測背景，接下來依據brightness-distortion 與
chromaticity-distortion 過濾相減後之前景以移除陰影；計算前景向量 F(Rf,Gf,Bf)與背景
向量 B(Rb,Gb,Bb)之夾角θ 與F投影至B之向量長度，任何介於|F-B|與θ 所圍成之圓錐區域
之(R,G,B)區塊皆可視為陰影，再針對這些前景以首腳位置及RGB 顏色資訊與其跟前一張
frame之相對關係做分類，隨著物件在場景中移動，勢必跨越許多監視器，然而監視器有各自
Field Of View(FOV)，各FOV彼此亦有重疊與不重疊之分，當人由監視器1之FOV移至監視器2
之FOV時，系統勢必要跟著轉換才能繼續監控，所以如何讓系統知人由監視器1移動至監視器2
之FOV成為一個重大關鍵，因此 Jiman Kim 等人提出以前景區塊數與物件與各監視器之角度
資訊來判斷遞交發生情況，系統一計算第i個人與第j個監視器之前景區塊數與角度，若人漸
往某一監視器靠近，則此時前景區塊數將越來越多，相對與該監視器角度將越來越小，利用
此概念計算每一人下一個最有可能進入之FOV，以Homography轉換在World Map中勾勒出每一
人之行走軌跡路線。 
    多攝影機人物追蹤:藉前景擷取結果以 connected component labeling 過濾前景破碎小
區塊並記錄各區塊相關資訊，接著針對各攝影機所擷取出之前景以RGB分量百分比與其區塊內
對應位置資訊進行人物辨識[32]，並將各人物區分為5種state(Enter、Leave、Match、
Occlusion、Fraction)進行追蹤[33]，於自行建置之三維動態場景勾勒出每個人於此攝影機
6 
 
第三章、研究方法 
1. 貼上建築物外部材質 
首先藉由衛星影像空照圖取得欲監控區域之二維影像，接著由使用者操作建築建立工
具，選取欲監測地區，輸入建築物名稱、形式與樓層數等資訊。 
  
圖二、左：欲監控區域之衛星空照圖，右：輸入待監控建築物之名稱、形式與樓層數。 
以電腦圖學技術依其輸入之地點與建築資訊，快速產生建築物之立體建築模型，再利用
材質對應工具，可將二維靜態建築物材質經修正透視變形後，於立體場景之對應地點貼上對
應之二維靜態材質。 
  
圖三、左：立體建築模型，右：左上圖: 三維場景模型，右上圖: 欲貼上之建築材質， 
左下圖: 材質與模型對應，右下圖: 將材質貼上模型結果。 
 
 
8 
 
 
圖六、左：裝置座標系統投影至立體監控場景之世界座標系統之示意圖，右：攝影機與移動
車輛位置關係示意圖。 
其中點 B於視野座標系統（Viewing Coordinate System）之座標為： 
V
2 tan
2 2
2 tanB ,2 2
                       
                            1
d
d
width fovyX zFar
width
height fovyY zFar
height
zFar
    ⋅ − ⋅ ⋅        
 
    − ⋅ − ⋅ ⋅ =    
    
 
 
− 
  
                (1) 
其中Xd、Yd為特徵點在裝置上之二維座標位置，width、height為輸出畫面之寬與高，zFar為A、
B兩點之距離、fovy為攝影機垂直視野之夾角。 
將世界座標系統轉換至視野座標系統之視野轉換矩陣（View Transformation Matrix）可
表示如下： 
[0]
[1][ ] ,
[2]
0 0 1
y z w
y z w
y z w
u u A u
v v A vVT
n n A n
 −
 
− =
 −
 
  






                          
(2) 
其中，u、 v、 n為視野座標系統之三個單位元素向量於世界座標系統表示法。則點 B於世
界座標系統之值為： 
[ ] 1 ,w vB VT B
−
= ⋅                              (3) 
由攝影機向特徵點所射出之向量為： 
,w wv B A= −
                               (4) 
因此，裝置座標系統之特徵點位置對應至世界座標體系之特徵點位置即為： 
10 
 
5. 將區域資訊整合為全域資訊 
藉影像辨識技術辨認三維立體場景中不同三維物件，並給予不同編號，再透過上述區域
資訊以 Homography全域座標轉換方式於 world map 勾勒出不同移動物件之軌跡，同時標誌
警戒區域提醒使用者，能夠分別就各區域之入侵物體進行即時無縫追蹤，並與傳統監視系統
功能作結合，各監視器於監控同時，亦儲存該視野之高解析度影像，點擊 world map 中標示
監視器處即可顯示對應之前景明顯移動物體，將鏡頭拍攝畫面之入侵物體所屬區域分割儲
存，以減少影像畫面間冗餘，較習知儲存全部影像方式節省大量硬體空間，同時兼顧大區域
監控及高目標影像解析能力，如圖九、十所示。 
 
圖九、World Map 與 Camera View。 
   
圖十、(左) FOV1，(中) FOV2及(右) FOV3。 
 
 
 
  
 
 
12 
 
參考文獻 
[1]  Eden A., Uyttendaele M., and Szeliski R., “Seamless Image Stitching of Scenes with Large 
Motions and Exposure Differences,” Proc. of CVPR, Vol. 3, Pages 2498-2505, 2006. 
[2]  Ranjan Rahul, Singh Harpreet, Meitzler Thomas, Sohn E. J., Singh, and Kuldip, “Video Image 
Fusion Process using Fuzzy Technique,” Signal Processing, Sensor Fusion, and Target 
Recognition XV, Vol. 6235, Page 62350Y, June, 2006. 
[3]  Neumann U., You S., Hu J., Jiang B., and Lee J. “Augmented Virtual Environments (AVE): 
Dynamic Fusion of Imagery and 3D Models,” VR03, March, 2003. 
[4]  Ismail Oner Sebe , Jinhui Hu , Suya You , and Ulrich Neumann, “3D Video Surveillance with 
Augmented Virtual Environments,” 1st ACM SIGMM Intl. Workshop on Video Surveillance, 
Berkeley, California, Nov. 02-08, 2003. 
[5]  Francois Fleuret, Jerome Berclaz, Richard Lengagne, and Pascal Fua, “Multi-camera People 
Tracking with a Probabilistic Occupancy Map,” IEEE Trans. Pattern Analysis and Machine 
Intelligence, Vol. 30, No. 2, Pages 267-282, 2008. 
[6]  Wei Jyh Heng, King N Ngan, “Digital Video Transition Analysis and Detection,” 
Publisher:World Scientific Co. Pte. Ltd., ISBN:978-9812381859, Jan. 1, 2003.      
[7]  A. Elgammal, D. Harwood, and L. Davis, “Non-parametric Model for Background 
Subtraction,” Proc. of ICCV '99 FRAME-RATE Workshop, 1999.    
[8]  C. Stauffer and E. Grimson, “Adaptive Background Mixture Models for Real-time Tracking,” 
Proc. of ICCV and Pattern Recog., pp. 246-252, , 1999.       
[9]  K. Kim, T. H. Chalidabhongse, D. Harwood, and L. S. Davis, “Real-Time 
Foreground-Background Segmentation using Codebook Model,” Real-Time Imaging, 
pp.172-185, 2005.   
[10] M. Heikkila and M. Pietikainen, “A Texture-Based Method for Modeling the Background and 
Detecting Moving Objects,” IEEE Trans. Pattern Anal. Mach. Intell., Vol. 28, No. 4, pp. 
657-652, 2006.   
[11] O. Javed, K. Shafique, and M. Shan, “A Hierarchical Approach to Robust Background 
Subtraction Using Color and Gradient Information,” Proc. of IEEE Workshop on Motion and 
Video Computing (MOTION’ 02), 2002.    
[12] T. Kanade, R. Collins, A. Lipton, P. Burt and L. Wixson, “Advances in cooperative 
multi-sensor video surveillance”, Proc. of DARPA Image Understanding Workshop, Vol. 1, pp. 
3-24, 1998.    
[13] Nam T. Nguyen, Dinh Q. Phung, Svetha Venkatesh, Hung Hai Bui, “Learning and Detecting 
Activities from Movement Trajectories Using the Hierarchical Hidden Markov Models,” Proc. 
of CVPR, pp. 955-960, 2005.   
[14] C. W. Chang, J. J. Tang and Z. Lee,“An Adaptive Surveillance System Platform for 
Cooperative Object Detection and Tracking in Multiple Cameras,” ICOS 2007 International 
Conference on Open Source, Taipei Taiwan, 2007.    
” 
[15] P. L. Rosin, “Thesholding for Change Detection,” Proc. of ICCV, 1998.   
[16] I. Haritaoglu, D. Harwood, and L. S. Davis, “W4: Real-time surveillance of people and their 
activities,” IEEE Trans. Pattern Anal. Mach. Intell., Vol. 22, pp. 809–830, Aug. 2000.   
[17] Gian Luca Foresti, “Object Recognition and Tracking for Remote Video Surveillance,” IEEE 
Trans. on Circuits and Systems for Video Technology, Vol. 9, Pages 1045-1062, Oct., 1999.   
[18] Weidong Zhang, Feng Chen, Wenli Xu, and Enwei Zhang, “Real-time Video Intelligent 
Surveillance System,” Intl. Conf. on Multimedia and Expo, Pages 1021-1024, July 9-12, 2006.   
[19] C. R. Wren, A. Azarbayejani, T. Darrell, and A. Pentland, “Pfinder: Real-time Tracking of the 
14 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用
價值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是
否適合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評
估。 
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■  達成目標 
□ 未達成目標（請說明，以 100字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明： 
 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：□已發表 □未發表之文稿 □撰寫中 ■無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100字為限） 
 
3. 依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價值
（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以 500
字為限） 
我國安全產業更在國內其他產業成長趨緩之情況下，近年來產值仍快速提升，預估國內
安全產業產值至 2015 可達到 4000 億新台幣。分析指出，由於人口快速增加及伴隨經濟發展
所帶來的社會犯罪問題，衍生人類對安全需求提升，因而全球各區域對電子安全器材需求皆
逐年增加。以英國為例，目前全球有百分之十的閉路電視攝影機設置在英國，2007年時，英
國閉路攝影機的數量將達到兩千五百萬台，平均每兩名成人即有一台攝影機在監視他們，在
英國無論火車、巴士、大街、體育場皆有監視器盯著人瞧，在購物中心設置監視器可能讓購
物者與老板均感安心。不過如此大量的監視攝影機在監控仍有許多困難。因此本計畫透過電
腦圖學技術建構三維立體建築模型技術，建立全狀況掌控之沉浸式監視系統（Immersive 
Surveillance for total situational awareness），以改善傳統監控系統於區域及鏡頭數量之大量增
加將導致監控畫面無法與實際場景迅速對應之缺失，結合虛擬巡邏與融合多攝影機進行協同
偵測技術可即時掌控突發事件以強化全域監控的能力，此外近年來智慧型手機蓬勃發展，可
以將智慧型手機內建的高解析度攝影機、GPS 與 3G 網路功能納入監控系統內預計可大幅提
升監控系統之效能。 
 1 
出席國際學術會議心得報告 
 
計畫編號 NSC99-2221-E-110-075 
計畫名稱 Immersive Surveillance for Total Situational Awareness 
  (全狀況掌控之沉浸式監視系統  ) 
出國人員姓名 蔣依吾 
服務機關及職稱 國立中山大學資訊工程學系 副教授 
會議時間地點 100 年 8 月 22 日至 8 月 25 日 歐洲 比利時 根特 
會議名稱 
Advanced Concepts for Intelligent Vision Systems 
(ACIVS 2011) 
發表論文題目 
Underwater Image Enhancement: Using Wavelength 
Compensation and Image Dehazing (WCID) 
   
一、參加會議經過 
ACIVS 為歐洲智慧視覺領域之每年度固定會議，主要著重於構建適應性，智能化，安
全可靠影像系統之相關技術，論文接受率相當低、極為競爭，接受之論文亦於Lecture 
Notes on Computer Science (LNCS)刊登，此次會議於兼具古典與學術氣息之寧靜小
城：比利時根特(Ghent, Belgium)舉行，美麗浪漫之水道穿梭其中，城中之葛雷芬斯
丁城堡(Gravensteen)更是中世紀古堡代表。 
 
ACIVS 2011會場：University of Ghent 
我為第一次與會，論文發表安排於第一天下午Image Processing session第一場，受
國科會補助計畫衍生研發成果推廣資料表
日期:2011/09/20
國科會補助計畫
計畫名稱: 全狀況掌控之沉浸式監視系統
計畫主持人: 蔣依吾
計畫編號: 99-2221-E-110-075- 學門領域: 影像處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
