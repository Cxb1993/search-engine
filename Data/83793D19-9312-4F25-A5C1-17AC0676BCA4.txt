分散式智慧型汽車網路監控系統的設計與實作 (I) 
Distributed Network Monitoring and Control for Intelligent Automobile: Design and 
Implementation (I) 
 
Abstract—In this report a new MOST (Media Oriented System Transport) for the first year of 
this project is explored. By using the program built in NetServices API (Application Program 
Interface) provided by Oasis, we can control the DVD player, the audio amplifier, the radio tuner, 
and the MOST network card under Microsoft Windows. The application of transporting Mpeg-2 
stream from the DVD player on the MOST network to the PC under Windows is developed in 
this report. The final outcome is to decode the Mpeg-2 stream and play DVD movie on the PC 
Windows environment. This application can be immediately applied to other similar automotive 
applications. For example, a GPS navigation system can work in conjunction with car 
microphone system for navigation by recognizing user’s voice. Also, the car telephone needs to 
mute the stereo system when a call is requested or received. Based on the MOST technology, the 
car vendors can develop many multimedia networking applications in an intelligent automobile. 
The CAN/LIN bus is also briefly studied for the potential integration of MOST bus. 
Index Terms—Media Oriented System Transport (MOST), application program interface (API), 
Global Positioning System (GPS), CAN bus, Lin Bus. 
I. INTRODUCTION 
Automobiles have evolved from having a simple radio with perhaps a cassette or CD player to 
having a variety of sophisticated information systems [1]. With the fast advance of automobile 
electronics, the car is not only a simple vehicle for people but also a place for entertainment. 
People can ease their drive with the help of electronic devices like GPS navigation system, car 
telephone system, night vision system and anti-crack radar. And riders may even enjoy a DVD 
film with stereo audio in a long journey instead sitting bored. 
 
Car today may include Car telephone, DVD Players, Amplifiers, Radio Tuner, LCD monitor, 
GPS, PDA, Bluetooth, …, etc. These devices need to communicate and interact with each other 
with HMI (Human Machine Interface). For example, a GPS navigation system can work in 
conjunction with a security system to locate a stolen car. The car telephone needs to interact with 
the stereo system to mute it when a call is requested or received. More and more electronics 
devices will be added to the car. Therefore the control, transportation and communication 
between theses devices become more and more important issues.  
 
The traditional vehicle bus technology like LIN Bus, CAN Bus, FlexRay [2] can not afford the 
high bandwidth request of real-time multimedia information. Between these technologies, the 
bandwidth of LIN Bus is only 20kbps，the CAN bus is 1Mbps and the FlexRay can be 10 Mbps.    
Due these reasons, a new technology named MOST (Media Oriented System Transportation) is 
proposed. MOST is a standard for building in-vehicle multimedia systems. The MOST 
 The MOST system supports a variety of data types such as control data, packet data and 
synchronous stream data. 
 
1. Control data transfer is for device specific transfers and system management. For example, a 
remote control sends “Eject CD” control data to a CD player to eject a CD as shown in Fig 6. 
This kind of data is transported in parallel to the real - time and asynchronous data. 
 
Fig. 6  Control data 
 
Fig. 7  Asynchronous packet data 
 
2. Bulk data / burst data transfer in asynchronous packets with variable bandwidth requirement 
is shown in Fig 7. It is often used for PDA or network TCP/IP packets. 
3. Real-time data transfer, which requires a guaranteed bandwidth to maintain data quality, is 
shown in Fig2-8. The microphone or CD audio/video data is such kind of data. If real-time 
data is sent to the MOST network, all nodes have access to it. The number of nodes 
“listening“ to real-time data is only limited by the maximum number of nodes. 
 
Fig. 8  Synchronous stream data 
The MOST system services provide all the basic functionality to operate a MOST system. They 
have four parts which are application socket, basic layer system services, low level system 
services, and stream services, as shown in Fig 9. 
 
Fig. 9 Block diagram of the MOST System Services[4] 
3. The Application Message Service is based on CMS (Control Message Service). AMS 
organizes incoming and outgoing messages on a higher level. The CMS limits the message to 
17 bytes. AMS extends the length of messages limited by the CMS. 
4. The Remote Control Service provides the sending of Remote System Message. 
5. The Synchronous Channel Allocation Service is based upon embedded Channel Allocation 
which is part of the Low Level Bus management. It allocates or de-allocates resources for real 
time data streaming on the MOST network and routes this data to the desired destination.  
6. The Transparent Channel Allocation Service provides allocating and de-allocating as well as 
routing of transparent data. 
7. The Asynchronous Data Transmission Service is responsible for sending and receiving of 
asynchronous data. Errors are detected and notified. 
8. The Transceiver Control Service provides access to configuration and address registers of the 
MOST transceiver. 
IV. Multimedia Applications 
In order to demonstrate the understanding of the MOST protocol, a multimedia application on the 
MOST network will be built. The PC running under Windows 2000 with MOST PCI card will 
request the real-time video and audio data stream from the MOST DVD player to be displayed on 
the PC screen. By the same token, it is also easy to request the real-time audio data stream from 
the MOS Radio Tune, so that we can listen to the radio using the speakers in the PC. Figure 12, 
shows the Radio Tuner 4 MOST, DVD Player 4 MOST, MOST PCI Board, and Amplifiers 4 
MOST on the MOST network [10] ~ [12]. Fig 13 shows the connection of the MOST devices in 
the MOST network. 
 
Fig. 12 MOST network topology 
  
Fig. 13 The MOST network 
To transport the Mpeg-2 [14, 15] stream from DVD Player 4 MOST to MOST PCI Board, the 
sixteen synchronous channels should be allocated by the DVD Player. After the channels are 
allocated, the program should connect the output channels to MOST PCI board. This can be done 
by SCS NetServices API. After these steps, the Mpeg-2 stream is transported from the DVD 
Player to MOST PCI Board. Fig 14 shows the diagram of Mpeg-2 transportation. Fig 15 ~ Fig 18 
show the settings of each step in the control program. 
 
Fig. 19 Decode and Play Mpeg-2 Streaming[13] 
 
Figure 20 shows the starting picture of Mpeg-2 Video. 
 
Fig. 20 Play of Mpeg-2 video 
V. The Potential Link with CAN/LIN Buses 
The Controller Area Network (CAN) is a serial communication protocol designed for providing 
reliable data transmission in harsh environment. CAN bus is a distributed control system. Control 
system can be implemented using either a centralized or distributed architecture. The sensors, 
actuators and motors in a centralized system require point to point wiring in order to exchange 
information among the ECUs. Also, each control unit in a distributed system, like CAN bus 
network system, can be implemented with a low cost microcontroller, as shown in Figure 21. 
 
Fig. 21 CAN bus system Distributed Control System 
 
The CAN network is a serial communication protocol initially developed to connect sensors and 
function and connects together. Figure 25 shows that a node includes 3 parts, Microcontroller, 
CAN Controller and CAN Transceiver. 
MAIN SYSTEM 
CONTROLLER
CAN 
CONTROLLER
CAN 
TRANSCEIVER
CAN BUS
CAN 
TRANSCEIVER
CAN 
CONTROLLER
NODE 
CONTROLLER
CAN 
TRANSCEIVER
CAN 
CONTROLLER
NODE 
CONTROLLER
CAN 
TRANSCEIVER
CAN 
CONTROLLER
NODE 
CONTROLLER
 
Fig. 25 CAN node configuration 
 
Lin bus was designed by the LIN consortium and the first specification has been published in 
1999. The consortium members are mainly European cars makers: Audi AG, BMW AG, Daimler 
Chrysler AG, Volkswagen AG, Volvo Cars Corporation AB, Motorola and Volcano 
Communications Technologies. Many car makers are currently implementing LIN in their 
vehicles like Parking Assist System (PAS). 
A Lin network consists of a LIN master and one or several LIN slaves. The LIN bus is connected 
between smart sensors or actuators and an ECU which is often a gateway with CAN bus. As 
shown in Figure 26. 
 
Fig. 26 Lin bus organization 
 
The link between MOST and CAN/LIN will be based on the understanding of all buses. The 
design of a gateway between MOST and CAN/LIN buses will be the major task in the second and 
third years of this project. 
 VII. References 
[1] Website of MOST Cooperation: http://www.mostcooperation.com/technology/index.php 
[2] 郭長祐, “車用電子之多媒體傳控網路：MOST 技術,” April 04, 2004 
Website:http://tech.digitimes.com.tw/ShowNews.aspx?zCatId=135&zNotesDocId=682  
[3] MOST Cooperation, “MOST Specification Framework Rev. 1.1,” 1999. 
[4] MOST Cooperation, “MOST Specification Rev 2.2,” Nov. 2002. 
[5] MOST Cooperation, “MOST NetServices: Application Socket, User Manual and 
Specification Rev1.10,” Jan. 23, 2004. 
[6] MOST Cooperation, “MOST Function Catalog, V2.0,” Oct. 2000. 
[7] MOST Cooperation, “OS 8104 MOST Network Transceiver, Final Product Data Sheet,” Sep. 
2006 
[8] Oasis SiliconSystems, “MOST NetServices Licensing Policy,” May 17, 2005. 
http://www.smsc-ais.com/files/mostnetservices/PFL-MOSTNetServicesLicensing_V01_00_X
X-1.pdf 
[9] Oasis SiliconSystems, “MOST NetServices Layer 1 User Manual/Specification Rev. 1.10.x,” 
Jan 23, 2004. 
[10] Oasis SiliconSytems, “DVDPlayer 4 MOST User Manual, V1.2.0,” Oct 13, 2003. 
[11] Oasis SiliconSystems, “RadioTuner 4 MOST User Manual, V1.0.0,” August 22, 2003. 
[12] Oasis SiliconSystems, “Amplifier Controller 4 MOST Advanced Product Data Sheet,“ Jun. 
2003.  
[13] Microsoft, “DirectX 9.0 Documentation for C++,“ 2002. 
[14] ISO-13818, “Information Technology - generating coding of moving pictures and associated 
audio information - Systems, Second edition,” Dec. 1, 2000. 
[15] ISO-11898-1, “Road vehicles – Controller Area Network (CAN) – Part 1: Data link layer 
and physical signaling,” 2003. 
[16] ISO-11898-2, “Road vehicles – Controller Area Network (CAN) – Part 2: High speed 
medium access unit,” 2003. 
[17] ISO-15765-1, “Road Vehicles – Diagnostic on Controller Area Network (CAN) – Part1: 
General information,” 2004. 
[18] ISO-14229-1, “Road Vehicles – Unified diagnostic services (UDS) – Part1: Specification 
and requirements,” 2006. 
 
Toward a New Three Layer Neural Network with Dynamical Optimal 
Training Performance 
 
Chi-Hsu Wang & Shu-Fan Lin 
Department of Electrical and Control Engineering, National Chiao-Tung University, Hsin-Chu, Taiwan 
 
Keywords: Neural Network, Optimal Training, Iris data 
 
1Abstract --- This paper proposes a revised dynamic 
optimal training algorithm for a three layer neural 
network with sigmoid activation function in the hidden 
layer and linear activation function in the output layer. 
This three layer neural network can be used for 
classification problems, such as the classification of Iris 
data. This revised dynamic optimal training finds optimal 
learning rate with its upper-bound for next iteration to 
guarantee optimal convergence of training result. With 
modification of initial weighting factors and activation 
functions, revised dynamic optimal training algorithm is 
more stable and faster than dynamic optimal training 
algorithm. Excellent improvements of computing time 
and robustness have been obtained for Iris data set. 
 
 
1. Introduction 
Artificial neural network (ANN) is the science of 
investigating and analyzing the algorithms of the human 
brain, and using similar algorithm to build up a powerful 
computational system to do the tasks like pattern 
recognition [1], [2], identification [3], [4] and control of 
dynamical systems [5], [6], system modeling [7], [8] and 
nonlinear prediction of time series [9]. The first model of 
artificial neural network was proposed for simulating 
human brain by F. Rosenblatt in 1958, 1962 [10], [11]. 
So the most attractive character of artificial neural 
network is that it can be taught to achieve the complex 
                                                 
1 This paper was supported by National Science Council 
of Taiwan, under the contract: NSC 95-2221-E-009-102-. 
tasks we had experienced before by using some learning 
algorithms and training examples. Among most popular 
training algorithms of artificial neural network, Error 
Back-Propagation Algorithm [12], [13] is widely used 
for classification problems. The well-known error 
back-propagation algorithm or simply the 
back-propagation algorithm (BPA), for training 
multi-layer perceptrons was proposed by Rumelhart et al. 
in 1986 [14]. Although the popular back-propagation 
algorithm is easier to understand and to implement for 
most applications, it has the problems of converging to 
local minimum and a slow convergence rate. It is well 
known that performance of BPA is significantly affected 
by several parameters, i.e., learning rate, initial 
weighting factors and activation functions. In this thesis, 
the effect of each adjustable parameter will be analyzed, 
in order to find a new training algorithm with better 
performance compared with BPA. 
 
There are many research works on the learning of 
multilayer NN with saturated nonlinear activation 
functions, e.g., sigmoid function. Some researches work 
on the learning process of multilayer NN with linear 
activation functions [15]. However, the authors in [16] 
state that if activation functions of each layer of 
multilayer NN are all sigmoid, the back-propagated error 
signal may be seriously discounted by the saturation 
region of sigmoid function. Hence, convergence rate of 
learning will slow down. Although, multilayer linear 
neural network does not have the problem of saturation, 
its learning capability is not as good as multilayer 
So we have 
1
( ) ( )
1H O x
f x f x
e−
= = +  
Let Dpn represent the desired output for the p-th output 
neuron due to n-th input pattern. Ypn represents the actual 
output for the p-th output neuron due to n-th input 
pattern. VO(p,n) is the output for p-th output-layer 
activation function due to n-th input pattern.. VH(r,n) is the 
output for r-th hidden-layer activation function due to 
n-th input pattern. SO(p,n) is the input for p-th output-layer 
activation function due to n-th input pattern.. SH(r,n) is the 
input for r-th hidden-layer activation function due to n-th 
input pattern. Let WH(r ,m) be the weight between m-th 
input neuron and r-th hidden neuron, and WO(p ,r) be the 
weight between r-th hidden neuron and p-th output 
neuron. So we define the output of r-th hidden neuron as  
( )( , ) ( , ) ( , )1Mr n r m m nH H HmV f W X== ∑  
 
,1Hf
,H rf
,H Rf
,1Of
,O Pf
,O pf
HVX HS OS
OV Y
1X
mX
MX
1Y
pY
PY
YWHW
 
Fig. 1 Three Layer Neural Network 
 
The output of p-th output neuron can be defined as  
( )( , ) ( , ) ( , )1Rp n p r r nO O O HrV f W V== ∑  
The normalized total square error can be defined as 
( )( , ) ( , )
1 1
1
2
P N
p n p n
O
p n
J V D
PN = =
= −∑∑  
The update rule of weighting factors for BP algorithm is 
based on the method of steepest descent, the successive 
adjustments applied to the weight matrix W are in the 
direction opposite to the gradient matrix /J W∂ ∂ . The 
adjustment of weight matrix W can be expressed as: 
( 1) ( ) ( ) ( )t t t t
t
J
W W W W
W
η+ ∂= − Δ = − ∂  
The gradient matrix of J related to WO(p ,r) is: 
( , ) ( , )
( , ) ( , ) ( , ) ( , )
p n p n
O O
p r p n p n p r
O O O O
V SJ J
W V S W
∂ ∂∂ ∂=∂ ∂ ∂ ∂
 
where ( , ) ( , )
( , )
p n p n
Op n
O
J
V D PN
V
∂ = −∂
    
 
( , )
( , ) ( , ) ( , )
( , )
(1 )
p n
p n p n r nO
O O Hp r
O
V
V V V
W
∂ = −∂
  
The gradient matrix of J related to WH(r, m) is: 
( , ) ( , ) ( , )
( , ) ( , ) ( , ) ( , ) ( , )1
p n r n r n
O H H
r m p n r n r n r m
H O H H H
p
P V V SJ J
W V V S W=
∂ ∂ ∂∂ ∂=∂ ∂ ∂ ∂ ∂∑  
where 
( , )
( , ) ( , ) ( , )
( , )
(1 )
p n
p n p n p rO
O O Or n
H
V
V V W
V
∂ = −∂
 
 
( , )
( , ) ( , ) ( , )
( , )
(1 )
r n
r n r n m nH
H Hr m
H
V
V V X
W
∂ = −∂
 
The change of weighting factors for the next iteration can 
be expressed as 
( )
( , ) ( , ) 1 ( , )
( 1) ( ) 1
( , ) ( , ) ( , ) ( , ) ( , ) ( , )
( )               (1)
              (2)(1 )
Np r p r p r
Y t Y t On
p r p n p n p n p n r n
O O O O H
W W PN
V D V V V
η δ
δ
−
+ == −
= − −
⎧⎪⎨⎪⎩
∑
 
( )
( , ) ( , ) ( , )
( 1) ( ) 1
( , ) ( , ) ( , ) ( , ) ( , ) ( , )
1
( , ) 1 ( , ) ( , ) ( , ) ( , )
(3)
(1 ) (4)
( ) (1 )
       
       
Nr m r m r m
H t H t Hn
Pr m p r p r r n r n m n
H H O H Hp
p r p n p n p n p n
H O O O
W W
W V V X
PN V D V V
η δ
δ δ
δ
+ =
=
−
= −
= −
= − −
⎧⎪⎪ ⎡ ⎤⎨ ⎣ ⎦⎪⎪⎩
∑
∑
 
 
3. Revised Dynamic Optimal Training 
Algorithm 
3.1 Problems in the Training of Three Layer NN 
There are two layers of sigmoid activation functions in 
BPA, i.e., VH and VO for hidden and output layers. 
Considering the defects of sigmoid function, whose 
graph is s-shaped, as shown below (Figure 2) with 
1
1 exp( )
y
ax
=
+ −
             (5) 
where y is the output signal of the neuron and x is the 
input signal. And the parameter a is the slope parameter 
of the sigmoid function. We can get different sigmoid 
functions by varying the slope parameter a, but we 
usually choose the sigmoid function with a =1.  
When x is far away from 0, we will get a saturated output 
value which is very close to extreme value, i.e., 0 or 1. 
the saturation problem. When VH(r,n) closes to 0 or 1, the 
term VH(r,n)(1-VH(r,n)) will discount back-propagated real 
error signal (9). In (10), the term VH(r,n) is the output of 
sigmoid activation function, and it is a function of initial 
weighting factors. If we can confine VH(r,n) in the range of, 
say [0.2, 0.8], then the whole term VH(r,n)(1-VH(r,n)) in (10) 
will not close to zero if proper initial weighting factors 
are selected. This will prevent the error signal of hidden 
layer in (10) be discounted by the whole term 
VH(r,n)(1-VH(r,n)) in the beginning phase of training. The 
VH(r,n) can be expressed as 
( , ) ( , ) ( ,:)1 1 expr n j i iH h h
i
V W X
⎛ ⎞⎛ ⎞= + −⎜ ⎟⎜ ⎟⎝ ⎠⎝ ⎠∑      (11) 
where Xhi is the fixed input data. It is a normal practice to 
use random number generator to select the initial 
weighting factors. Let 
max
min
( )
( )
w Max W
w Min W
=
=  
Where W is the weighting matrix. We let wmin < 0 < wmax 
and -wmin = wmax. We use a sigmoid activation function 
with parameter a = 1 in hidden layer. Then we must 
select wmin and wmax to satisfy the following inequality: 
 
min max0.2 ( , ) ( , ) ( , ) 0.8s s sy w y w y wλ λ λ= ≤ ≤ =     (12) 
with 
s
1y ( , )
1 exp( )
w
w
λ λ= + −          (13) 
From (12) and (13), we can have 
min max
max max
ln 4 ln 4w w w
x x
− = ≤ ≤ =     (14) 
So if w satisfies (14), then the output of sigmoid function 
in (13), i.e., ys(w,|xmax|) , will be in the range of [0.2, 0.8]. 
Final conclusion is to select proper weighting factors to 
let 
10.2 ( , ) 0.8
1 exp( )s i i
y w x
wx
≤ = ≤+ −  
so that the error signal (10) in hidden layer will not be 
discounted in the beginning of training process. This will 
also reduce the probability of falling into in local 
minimum on the error surface during the back 
propagation process. 
 
3.4  Determine the upper-bound of learning rate in 
each iteration 
In the dynamic optimal training algorithm [19], it uses 
matlab function “fminbnd” [23] to find optimal learning 
rate. Matlab function “fminbnd” can find a global 
minima value in a given range. Usually, we give a fixed 
interval to find the optimal learning rate. But it take too 
mach time to search because of fixed interval. From the 
experimental result, we can see that the optimal learning 
rate may go from 0.001 to 1000. It is a very large interval. 
Further, it can be discovered that most learning rate in 
each iteration does not exceed 100. So a fixed range will 
waste a lot of time to find an optimal learning rate during 
the learning process. Besides, most of other 
mathematical methods also need an interval to find a 
global minima value. Indeed, the upper bound of stable 
learning rate of the three layer NN shown in Figure 3 can 
be found in the following Theorem. This upper-bound 
will be updated in each iteration and changed according 
to the maxima value of input matrix and output 
weighting matrix. 
 
Theorem: 
The upper-bound of the stable learning rate of the 
three-layer NN shown in Figure 3 can be shown as 
follows: 
( ) 2 2 2max max
32
16u p Y
P N
a N R P R M W X
η ⋅= + ⋅ ⋅ ⋅ ⋅  
where a  is the slope of the output layer activation 
function, WYmax and Xmax are the acceptable maximum 
absolute value for the second (output) layer weighting 
matrix and for input matrix, respectively. 
,1Hϕ
,H rϕ
,H Rϕ
,1Oϕ
,O Pϕ
,O pϕ
HVX HS OS
OV Y
1X
mX
MX
1Y
pY
PY
YWHW
 
Fig. 3 Revised Three Layer Neural Network 
the value of J is smaller than 0.0078125. We assume that 
when ek=0.125, it is enough to say that the network can 
classify IRIS data set. So it means that when 
J=0.0078125, the network can achieve our requirement. 
ST is actual settling time and it can be calculate in the 
form of [ST = T × SI ÷ (total iteration)]. C is times of 
successful convergence for training NN in 20 trials. 
Iris 
4-4-3 BPA DOA RDOA 
J  
(Final error) 
0.079515
(diverged)  0.000626 0.003836
T 
(Training Time, sec) Diverged 41.09 22.87 
SI 
(Conv. Iteration) Diverged 654 529 
ST 
(Conv. Time, sec) Diverged 2.68 1.21 
C 
(% of Conv.) 0% 20% 95% 
 
From row J in Table 1, DOA has a better normalized 
total square error (.000626) than RDOA (.003836) if 
both algorithms have correct results. But the Js in both 
algorithms are small enough to classify IRIS data set. 
From row C, we see that RDOA has a greater probability 
(95%) than DOA (20%) to have a correct result. It means 
that RDOA has a high capability of jumping out local 
minimum. From row ST, we can know that RDOA only 
spends 1.21 seconds to get normalized total square error 
J smaller than 0.0078125 or get into a steady state. 
RDOA is 2.21 times faster than DOA (2.68sec). 
The comparison of the results of three algorithms is 
shown in Figure 5 and Figure 6 The result of BP 
algorithm uses a fixed learning rate 0.1 to train the IRIS. 
The result of DO algorithm uses the initial weights 
drawn from a uniform distribution over interval [-1, 1] to 
train the IRIS. 
In Figure 5, DOA has a slightly better performance than 
that RDOA has at the end of training process. Both of 
them have acceptable performance at the end of training 
process. In Figure 6, it is obvious that RDOA and DOA 
both achieve the minimum acceptable square error 
0.0078125 around the 800th iteration. RDOA has a faster 
convergent rate at very beginning of training process and 
the convergent rate of RDOA slow down when network 
gets into a steady state. After 10000 training iterations, 
the resulting weighting factors and total square error J 
are shown below. Normalized total square error J = 
0.043 
H
-2.9855 -2.5472 3.5321 4.9206
0.7139 0.8761 -1.7383 -1.4481
W =
-0.2861 0.4348 -0.1717 -0.5248
0.6269 0.1752 0.5665 0.1341
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦  
 
Y
0.0874 1.1824 -0.1976 -0.0696
W -1.1929 -1.6309 1.3376 1.0304
1.1153 0.3748 -0.8871 0.0136
⎡ ⎤⎢ ⎥= ⎢ ⎥⎢ ⎥⎣ ⎦  
 
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
Normalized square error J of the RDOA, DOA, and BPA
Iteration
Th
e 
to
ta
l s
qu
ar
e 
er
ro
r J
RDOA
BPA
DOA
 
Fig. 5 Training error of RDOA, DOA, and BPA 
 
After we substitute the trained weighting matrices into 
the network to perform real testing, we find that there is 
no classification error by using training set (the first 75 
data set). However there are 4 classification errors by 
using testing set (the later 75 data set). This is better than 
that of using DO [19], which generates 5 classification 
errors. 
0   100 200 300 400 500 600 700 800 900 
0.008
0.02
0.04
0.06
0.08
0.1
0.12
Normalized square error J of the RDOA and DOA
Iteration
Th
e 
to
ta
l s
qu
ar
e 
er
ro
r J
RDOA
DOA
 
Fig. 6 The close look of training error for Iris problem 
 
5. Conclusion 
In this thesis, a revised dynamic optimal training 
