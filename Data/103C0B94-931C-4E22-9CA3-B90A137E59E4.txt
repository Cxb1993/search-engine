 
Keywords: Artificial Neural Network, 
Back-Propagation Algorithm, Leven-
berg-Marquardt, Block_Cyclic, Parallel 
Processing 
二、 緣由與目的 
類神經網路發展至今約有50年，
最早出現的類神經網路模式為感知器
(Perceptron)，是由Rosenblatt於1958年
所提出[6]。到目前為止已有許多成熟
的模式被提出，其中以監督式學習的
倒傳遞類神經網路使用最為普遍[7]。
倒傳遞類神經網路具有學習準確度
高、回想速度快等優點，以1992到1998
年之間為例，約有78％的類神經網路
分析是採用倒傳遞類神經網路作為其
學習模式[9]。倒傳遞類神經網路的基
礎為多層前饋式網路，其網路架構包
含了輸入層、一至多層的隱藏層以及
輸出層，亦屬於多層感知器之一類。 
傳統的倒傳遞類神經網路通常採
用最陡坡降法的觀念，將目標值與實
際輸出值之間的誤差函數最小化，藉
由調整網路的連結加權值，達到網路
學習的目的。但是使用這樣的方法會
有以下幾個主要的缺點：(1)容易收斂
至局部最小值，不易獲得全域最小值
(Global Minimum)；(2) 當搜尋結果愈
靠近最小值時，由於梯度變小，權重
值更新速率變慢，因此疊代次數變
多，學習時間變長；(3)當靠近最小值
時，若提高學習速率值，可能會使收
斂速率變快，若增加太多則可能造成
發散結果。 
為了改善以上缺點，本研究提出
LM-HLP演算法，希望將傳統的倒傳遞
演算法加以改良。我們期望將類神經
網路的學習過程變得更有效率，能快
速而精確地收斂到全域最小值，並且
提升其預測精確度。 
三、 結果與討論 
本研究提出LM-HLP倒傳遞類神
經網路，以Levenberg Marquardt方法
[3,4,5]替代傳統倒傳遞類神經網路所
採用之最陡坡降法，並且使用批次學
習方式，再採用Block_Cyclic分佈方法
將輸入的大量資料平均分配到各處理
器上，以平行處理的方式來執行倒傳
遞的學習演算法。實驗部分針對
LM-HLP與HP類神經網路的學習時
間、預測精確度與資料交換時間進行
比較與分析。 
以下將說明LM-HLP之架構、學習
流程以及演算法內容；並且介紹實驗
環境、資料來源與處理，針對LM-HLP
與HP類神經網路進行實驗比較與分
析。 
(1) LM-HLP 架構 
在對LM-HLP倒傳遞類神經網路
進行學習訓練之前，必須先對範例資
料進行處理，然後將範例資料輸入到
類 神 經 網 路 中 ， 範 例 資 料 是 以
Block_Cyclic方法平均分配到各處理
器上進行平行運算。假設現在有30個
範例資料輸入到類神經網路中，以
Block_Cyclic方法來分配範例資料，我
們若以5個資料為一個區塊單位進行
分割，可以分割成6個區塊，然後平均
分配到3個處理器中，所以每個處理器
可以分到兩個區塊的資料量，如此透
過Block_Cyclic實現矩陣區塊在不同
2 
(1)初始化靈敏度。 
)( mq
mm
q nfs −=  
其中，S 為靈敏度；n 為淨輸出值(沒
有帶入轉換函數前的值)。 
(2)計算靈敏度。 
11 ))(( ++= mqTmmqmmq swnfs  
(3)計算 Jacobian Matrix。 
[J]  11 −+ ×= mmq as
步驟 6： 
(1)計算權重修正量 ∆ w。 
qkk
T
kk
T ewJIkwJwJ ,
1 )(])()()([ ××+−= −μ
其中，I 為單位向量。 
(2)再計算新的誤差平方和 ： newe
a.先計算新的權重值 。 neww
+= oldnew ww  ∆ w 
b.再計算新的輸出值。 
1,0),( 111 =×= +++ mawfa mmmm  
c.計算新的誤差平方和。 
eee Tnew ×=  
步驟 7：進行判斷，判斷新的誤差平
方和是否小於舊的誤差平
方和？ 
步驟 8：若新的誤差平方和 小於
原本的誤差平方和 ，則
更新權重值，並且以
neww
oldw
ϕ
μk 取
代 kμ ；若新的誤差平方和
沒有小於原本的誤差
平方和 ，則用
neww
oldw kμ ×φ 取
代 kμ ，並且重新計算權重修
正量∆ w。 
步驟 9：判斷誤差平方和是否小於
0.00025？ 
步驟 10：若誤差平方和小於 0.00025，
則類神經網路訓練完成，網
路停止訓練；若誤差平方和
沒有小於 0.00025，則繼續
判斷處理群組中是否還有
其他訓練範例？ 
步驟 11：若處理群組中還有其他訓練
範例，則繼續下一個範例的
訓練；若處理群組中已無任
何訓練範例時，則類神經網
路停止訓練。 
∆w 接下來說明LM-HLP演算法之內
容。圖2為LM-HLP演算法之內容，演
算法一開始先將訓練資料全部輸入：
[0] = b；再使用Block_Cyclic資料分
佈方法將範例資料分割，然後平均分
配到各處理群組中： a [b] = ((b-1)/k) 
Mod P，之後各處理群組再進行平行運
算的動作，其中包括：(1)計算輸出值；
(2)計算誤差平方和；(3)初始化靈敏
度；(4)計算靈敏度及Jacobian矩陣；(4)
計算權重修正量； (5)計算新的權重
值；(6)判斷新的權重值所計算出來的
誤差平方和是否有小於原本的誤差平
方和；(7)若有，則更新權重值，(8)判
斷新的誤差平方和是否小於誤差包容
值，(9)若有，則停止訓練；沒有，則
繼續下一個訓練範例；(10)若是新的誤
差平方和沒有小於原本的誤差平方和
時，則重新計算權重修正量，再計算
出新的誤差平方和與原本的誤差平方
和作比較，直到新的誤差平方和小於
原本的誤差平方和才去更新權重值。
而訓練結束條件為誤差平方和小於
a
4 
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 
1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 
1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 
1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 
1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 
1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 
1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 
1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 
1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 
1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 
1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 
1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 
1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 
1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 
1 1 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 
1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 
 
圖 3 數字 0 之矩陣 
 
HP類神經網路的學習方式採用每
次讀入一個範例資料，一個範例資料
為400×1的向量，然後分割範例資料與
權重初始值到各處理群組中，進行平
行計算，資料交換方式採用GAAB 
(Group All-to-All Broadcast)方式來進
行資料的交換；輸入層有 400個輸入
點，輸出層為一個值，即一個預測數
字，網路有一層隱藏層，轉換函數採
用對數雙彎曲作用函數。 
而LM-HLP類神經網路有400個輸
入點，一開始先將所有的範例資料讀
入，以Block_Cyclic資料分佈方式平均
分配範例資料到各處理群組中，一個
輸出值，即為所預測之數字，我們採
用一層隱藏層，隱藏層所採用的轉換
函數為對數雙彎曲作用函數，輸出層
採用的是線性作用函數。LM-HLP類神
經網路與HP類神經網路詳細的參數設
定可見表1與表2說明。 
 
表 1  HP 類神經網路參數表 
 
參數 初始設定值 
網路層數 3 
隱藏層數 1 
輸入層神經元個
數 
400 
隱藏層神經元個
數 
400 
輸出層神經元個
數 
1 
學習率 0.1 
轉換函數 對數雙彎曲作用函
數 
可容忍誤差值 0.001 
權重初始值 -0.5～0.5 之間以亂
數產生 
 
在HP類神經網路部分，學習率η
是用來控制權重值調整的速度，一般
取值會在0.01～1之間，而本研究在實
驗時採用0.1作為學習率之設定值。 
在LM-HLP類神經網路部分，μ值
的初始值通常不要設太大，因此本實
驗採0.1作為初始值，ϕ值一般介於1～
10之間，本實驗ϕ值採用3為初始值。 
 
 
 
 
 
6 
   %100×−=
i
ii
t
atε            (8) 
其中 為目標輸出值； 為計算輸出
值。 
it ia
另外在 HP 與 LM-HLP 類神經網
路訓練完成後，我們以 100 張數字圖
片(0～9 每個數字各 10 張)作為測試
之用；分別測試 HP 與 LM-HLP 類神
經網路之預測精確度，預測精確度計
算為 %100)1( ×− 測試總個數
預測錯誤個數 ，預測
結果得知 HP 倒傳遞類神經網路預測
精確度為 (1-
100
28 ) =72%，而
LM-HLP 倒傳遞類神經網路預測精確
度為(1-
%100×
100
13 ) =87%。 %100×
 
HP與LM-HLP精確度比較圖
0
2
4
6
8
10
12
1 2 3 4 5 6 7 8
處理群組數
誤
差
指
數
(
%
)
HP誤差指數(%)
LM-HLP誤差指數(%)
 
圖5  HP與LM-HLP預測精確度比較 
 
3、 就HP與LM-HLP平行處理之資料
交換時間做比較： 
因為LM-HLP類神經網路一開始
就載入所有的範例資料，再採用
Block_Cyclic 資料分佈方式平均分配
範例資料到各處理群組中，因此
LM-HLP 類神經網路在進行平行處理
時，並不需要花費到資料交換時間，
比起 HP 類神經網路採用每次輸入一
筆範例資料，然後將資料分割到各處
理群組中進行平行處理而言，HP 類
神經網路就必須在計算時進行資料交
換的動作，因此會多花費一些資料交
換的時間。圖 6 顯示 HP 類神經網路
在平行處理時所花費的資料交換時
間。 
 
HP資料交換時間
0
200
400
600
800
1 2 3 4 5 6 7 8
處理群組數
資
料
交
換
時
間
(
秒
)
HP資料交
換時間(秒)
 
圖 6  HP 資料交換時間 
 
綜合上述比較得知，LM-HLP 演
算法以平行處理的方式來執行倒傳遞
類神經網路，使類神經網路的學習過
程更有效率，且預測精確度提升。 
四、 計畫成果自評 
1. 成果與創建 
類神經網路具有優異的表現而
被廣泛的應用在各領域中，然而類神
經網路一直以來卻面臨相當大的問題
－網路訓練時間耗時。儘管有許多可
以快速學習的網路學習演算法相繼被
8 
10 
1994, pp. 989-993. 
4. Levenberg, K., “A Method for the 
Solution of Certain Problems in 
Least Squares,” Quarterly of Ap-
plied Mathematics, Vol. 2, 1944, 
pp. 164-168. 
5. Marquardt, D., “An Algorithm for 
Least Squares Estimation of 
Nonlinear Parameters,” SIAM 
Journal on Applied Mathematics, 
Vol. 11, 1963, pp. 431-441. 
6. Rosenblatt, F., “The Perceptron: A 
Probabilistic Model for Informa-
tion Storage and Organization in 
the Brain,” Psychological Review, 
Vol. 65, 1958, pp. 388-408. 
7. Rumelhart, D.E., Hinton, G.E., 
and Williams, R.J., “Parallel Dis-
tributed Processing: Explorations 
in the Microstructure of Cogni-
tion,” MIT Press Computational 
Models Of Cognition and Percep-
tion Series, Vol. 1, 1986, pp. 
318-362. 
8. Suresh, S., Omkar, S.N., and Mani, 
V., “Parallel Implementation of 
Back-Propagation Algorithm in 
Networks of Workstations,” IEEE 
Transactions on Parallel and Dis-
tributed Systems, Vol. 16, No.1, 
January 2005, pp. 24-34. 
9. Vellido, A., Lisboa, P.J.G., and 
Vaughan, J., “Neural Networks in 
Business: A Survey of Applica-
tions (1992-1998),” Expert Sys-
tems with Applications, Vol. 17, 
1999, pp. 51-70. 
 
An Object-Oriented Approach to Storage and Retrieval of RDF/XML Documents 
 
Ching-Ming Chao 
Department of Computer and Information Science, Soochow University 
 
 
Abstract-The Resource Description Framework (RDF) is a 
foundation for processing metadata, which provides interopera-
bility between applications exchanging machine-understandable 
information on the World Wide Web. Due to its highly flexible 
hierarchical structure and machine-independent characteristic, 
XML has become the formal way to store and transmit RDF 
models. We refer such documents as RDF/XML documents, or 
RDF documents for short. Efficient storage and retrieval of RDF 
documents in persistent data stores is an important issue in 
computer technology today. In this paper, therefore, we propose 
an object-oriented approach to this issue. Firstly, we propose an 
object-oriented data model, called the RDF Data Storage Model 
(RDSM), for storage of data extracted from RDF documents, as 
well as an RDF document decomposition algorithm for extrac-
tion of data from RDF documents. Secondly, we propose a ge-
neric RDF API that supports fundamental RDF data accessing 
and querying operations, and utilize the W3C’s SPARQL lan-
guage as the high-level query language for retrieval of RDF data. 
Finally, an experimental system is implemented to demonstrate 
the efficiency and effectiveness of the proposed approach. 
 
 
I.  Introduction 
The Resource Description Framework (RDF) is a W3C 
Recommendation for the notation of metadata on the World 
Wide Web (WWW) [1,2]. The RDF Schema (RDFS) extends 
this standard by providing developers with the means to spec-
ify vocabularies and model object structures [3]. These tech-
niques enable enrichment of the Web with ma-
chine-understandable semantics, thus helping to construct the 
Semantic Web described by Tim Berners-Lee et al. in [4]. 
It should to be noticed that RDF is a model for expressing 
metadata, which implies that RDF users can use any possible 
syntax to encode data described in RDF. Due to its highly 
flexible hierarchical structure and machine-independent 
characteristic, XML has become the formal way to store and 
transmit RDF models. We refer such documents as RDF/XML 
documents, or RDF documents for short. How to efficiently 
store and retrieve RDF documents in persistent data stores is 
an important issue in computer technology today. 
In this paper, therefore, we propose an object-oriented ap-
proach to this issue. Firstly, we propose an object-oriented 
data model for storage of data extracted from RDF documents, 
as well as an RDF document decomposition algorithm for 
extraction of data from RDF documents. Secondly, we pro-
pose a generic RDF API that supports fundamental RDF data 
accessing and querying operations, and utilize the emerging 
W3C’s SPARQL language as the high-level query language 
for retrieval of RDF data. Finally, we implement an experi-
mental system to demonstrate the efficiency and effectiveness 
of the proposed approach. 
The rest of this paper is organized as follows. Section II re-
views related work. Section III presents the data model and 
the decomposition algorithm. Section IV describes the generic 
RDF API and the query-answering algorithm. Section V illus-
trates the results of performance evaluation. Section VI con-
cludes this paper and suggests directions for future research. 
II.  Related Work 
Several methods for storing XML documents in relational 
databases have been proposed. These methods can be roughly 
classified into two categories: the structure-mapping approach 
and the model-mapping approach [5]. In the former approach, 
a database schema represents the logical structure of the target 
XML document, and the logical structure can be obtained by 
analyzing the Document Type Definition (DTD) or the XML 
Schema Definition (XSD) accompanying the document. In 
other words, a set of tables is defined for every single XML 
document structure or, more precisely, for the DTD or XSD of 
the XML document. Examples of this approach can be found 
in [6,7]. In the latter approach, a set of consolidated tables is 
created to store all XML documents. Examples of this ap-
proach can be found in [5,8]. The strength of the struc-
ture-mapping approach lies in that the precise information 
represented in the DTD or XSD can be preserved. Since this 
approach creates a set of tables for every DTD or XSD, how-
ever, management of these tables raises a challenging issue to 
the database administrator (DBA). On the other hand, the 
model-mapping approach only maintains one set of consoli-
dated tables and therefore greatly reduces the burden of DBA. 
Besides classification according to the mapping approach, 
these methods can also be distinguished into using schema 
information or using no schema information. By using 
schema information such as DTD or XSD, an XML document 
can be decomposed without losing its precise structure and 
data type information. By using no schema information, on 
the other hand, it is possible to lose the precise structure and 
data type information of a tree-like semistructured data set (to 
which an XML document conforms). Examples of the former 
case can be found in [5,6,7], and examples of the latter case 
can be found in [8]. 
The database management system (DBMS) chosen to store 
incoming XML documents is also an important factor affect-
ing the development of these methods. Traditionally, most of 
an RDF document. Note that the URI attribute in an instance 
of the Literal class is used to store the data type URI of this 
instance, and the actual value in string literal form is stored in 
the literal attribute of the Literal instance. 
To present a statement that consists of a specific subject, 
predicate and object, we use the Statement class as the re-
pository to store the information needed to construct an RDF 
statement. In an instance of the Statement class, we use a List 
object to keep track of the resources and literals that are used 
to form this statement. To record the type of a container that 
might be used in a statement, we simply store the information 
about the type of the container by putting one of the values 
“Bag,” “Seq” and “Alt” in the containerType attribute. 
The Model class can be used to create the model that would 
contain several interconnected statements. We use its opera-
tions to create a new model, duplicate an existing model, cre-
ate a resource or literal, or create a new statement. 
B.  Decomposing RDF/XML Documents 
While receiving an RDF document, we need to decompose 
it into components that our RDF data store can accept and 
store. Before introducing the process of decomposing an RDF 
document, we first consider the following cases: 
 Typed Literals: These specify type information of the 
corresponding literals. Fig. 2(a) shows an example of a typed 
literal. When we decompose an RDF document, type infor-
mation will be preserved for the convenience of query proc-
essing. For a literal that is provided without type information, 
we will try to analyze its structure and determine a reasonable 
type for it. Since the XML Schema standard provides many 
built-in data types, we preload these data types as fundamen-
tal RDF resources for quotation purpose. 
 Nesting Statements: As the example shows in Fig. 2(b), 
the inner rdf:about attribute plays a double role. It is the 
value of the s:Composer property in the first statement, and 
is also the subject of a further statement. When we encounter 
this case, we first flatten nesting statements into several sim-
ple RDF triples, and then we internally group the sharing re-
sources to improve storage performance. 
 Containers: A container structure can be used to indi-
cate a group of things. Fig. 2(c) shows a typical rdf:Bag con-
tainer structure. A container structure can be processed simi-
larly to the way for processing nesting statements, except for 
the rdf:Seq structure. The order information of the involving 
members needs to be recorded to preserve the semantics of an 
rdf:Seq container instance. 
Taking into account of the three cases listed above, we de-
velop an algorithm for decomposing an RDF document. We 
adopt the Document Object Model (DOM) in [9] for process-
ing RDF documents and assume that the namespace URIs 
used in a given document are all declared as attributes of the 
rdf:RDF element for performance consideration. To further 
improve the performance of the algorithm, we will not proc-
ess nodes such as instruction and comment nodes. The algo-
rithm, which is shown in Fig. 3, executes the following steps: 
<rdf:Description rdf:about="http://www.imdb.com/name/nm0515908"> 
  <s:age rdf:datatype="http://www.w3.org/2001/XMLSchema# 
decimal">58</s:age> 
</rdf:Description> 
(a) 
<rdf:Description rdf:about="http://www.imdb.com/name/nm0515908"> 
  <s:Composer> 
    <rdf:Description rdf:about="http://www.imdb.com/title/tt0293508">
      <s:Name>The Phantom of the Opera</s:Name> 
      <s:publishedIn>2004</s:publishedIn> 
    </rdf:Description> 
  </s:Composer> 
</rdf:Description> 
(b) 
<rdf:Description rdf:about="http://www.imdb.com/name/nm0515908"> 
  <s:Composer> 
    <rdf:Bag> 
      <rdf:li rdf:resource="http://www.imdb.com/title/tt0293508"/> 
      <rdf:li rdf:resource="http://www.imdb.com/title/tt0173714/"/> 
      <rdf:li rdf:resource="http://www.imdb.com/title/tt0070239/"/> 
    </rdf:Bag> 
  </s:Composer> 
</rdf:Description> 
(c) 
Fig. 2. Three special RDF/XML serializations. 
1) Record the namespace prefixes and the associated URIs 
used in the whole RDF document. 
2) Create a Model instance for organizing and grouping all 
of the resources and literals extracted from the document. 
3) Extract a subject by retrieving the URI representing this 
resource; use the URI_GENERATOR() function to generate a 
URI for an anonymous resource; record the value of the 
rdf:ID attribute for future use. 
4) Extract the localname of a found predicate and combine 
it with the corresponding URI prefix to form the full URI of 
the predicate. 
5) Check whether the object of the predicate is a resource 
or not. If the object is a resource, acquire the URI of a named 
resource or call the URI_GENERATOR() function to generate 
a URI for an anonymous resource. If the object is a string 
literal, get the value in string form, and so does the corre-
sponding data type if it is explicitly specified in the document 
(or analyzed by the decomposition process). 
6) Use the CONSTRUCT_STATEMENT() function to cre-
ate a new statement if no further object can be extracted, else 
continue to extract the objects belonging to a nesting state-
ment or a container statement. 
IV.  Retrieving RDF/XML Documents 
A.  Generic RDF API 
To provide the ability to access and query RDF data stored 
in our RDF data store, we introduce a generic RDF API for 
accessing and querying RDF data. The set of operations is 
generic RDF Model operations and can be implemented by 
SPARQL is built on the triple pattern, which is written as 
subject, predicate, and object and is terminated with a full 
stop (i.e., “.”). URIs are written inside angle brackets (i.e., 
“<” and “>”). String literals are denoted with either double 
quotes (i.e., “"”) or single quotes (i.e., “'”). The keyword 
PREFIX binds a prefix to a namespace URI. A prefix binding 
applies to any QNames in the query with that prefix. Variables 
are indicated by “?”; however, “?” does not form part of the 
variable. “$” is an alternative to “?”. In a query, $author 
and ?author denote the same variable. The SELECT clause is 
used to define the data items that will be returned by a query. 
The WHERE clause uses braces (i.e., “{” and “}”) to group a 
collection of triple patterns, and this collection is called a 
graph pattern. Each of the triple patterns must match for the 
graph pattern to match. Matching a triple pattern to a graph 
gives bindings between variables and building blocks in a 
RDF graph so that the triple pattern, with variables replaced 
by corresponding RDF data, is a triple of the graph being 
matched. The result of a query is a sequence of results that 
form a table or result set. Each row in the table corresponds to 
one query solution, and each column corresponds to a vari-
able declared in the SELECT clause. 
By combining with the API introduced in the previous 
subsection, we can create a system for querying RDF data 
stored in our data store. Firstly, we iteratively retrieve each of 
the triple solutions to a specified triple pattern and connect it 
to other triple solutions from other triple patterns to form a 
candidate graph. Then, we look up the bindings between the 
variables specified in the SELECT clause and the candidate 
graph to acquire the query result. The algorithm for acquiring 
query results is shown in Fig. 4. 
The application's window is divided into two parts. The 
upper part of the window is configured for loading the RDF 
document. The user can specify the document by entering the 
document's full path or invoking a file selection dialog box by 
clicking the “…” button. The specified document is then 
shown in the display area and the user can load it into the data 
store by clicking the “Store it!!” button. The lower part of the 
window is arranged for querying RDF data. The user issues a 
query statement in the entering area and clicks the “Go!” but-
ton to execute the query statement. The query result is then 
displayed in the display area in a tabular format. 
Fig. 5. GUI of the experimental system. 
V.  Performance Evaluation 
To evaluate the performance of our storing process, we 
conducted the test by taking test files under the categories of 
“Examples From the RDF Model and Syntax Specification”, 
“Automatically Generated RDF Files”, and “Miscellaneous 
Examples” listed on the web page named “RDF Examples 
and Miscellaneous Tests” 
(http://www.w3.org/2000/10/rdf-tests/) as the input files of 
our experimental system. The results of the evaluation are 
shown in Table 1. 
We have implemented an experimental system for storing 
and retrieving RDF documents. In the experimental system, 
databases are built on the Computer Associates' Jasmine II 
object-oriented database management system and programs 
are written in the C# object-oriented programming language 
under the Microsoft Visual Studio 2005 development envi-
ronment. The hardware platform is with the equipments of 
Pentium III 800MHz CPU, 512Mbytes of RAM, and 
80Gbytes of hard disk storage capacity. The graphical user 
interface (GUI) of the experimental system is shown in Fig. 5. Because the original contents of several test files were not 
fully complied with the RDF/XML syntax defined in the RDF 
standard suite, we modified their original contents to make 
these files testable for our system. From the results shown in 
Table 1(a) and 1(c), we can see that most of the test files can 
be processed in less than 3 seconds. Even though our system 
gets the worst performance shown in Table 1(b), since the file 
size in this category is up to 5 kilo bytes, the average proc-
essing time is still less than 4 seconds. From the results of the 
evaluation, we can say that our storing process is acceptably 
efficient for decomposing and storing RDF documents we 
selected, and it is reasonable to presume that our storing 
process can process almost all of the RDF documents. 
 
Algorithm RDF Query Answering Algorithm 
¾ Query all of the possible triple pattern solutions for every triple 
pattern involved in a graph pattern.  
¾ For each graph pattern solution constructed by picking up one of the 
triple pattern solutions from each triple pattern do 
⇒ If the bindings between the variables in the SELECT clause and 
the graph pattern solution exist then 
⇒ Acquire the bindings as one solution of the query result 
set. 
End Algorithm  
Fig. 4. RDF query answering algorithm. 
