 1
行政院國家科學委員會補助專題研究計畫 ■成果報告   □期中進度報告 
 
一個利用跨階層資訊最佳化的 buffer cache 管理機制 
 
 
計畫類別：■個別型計畫   □整合型計畫 
計畫編號：NSC 100-2221-E-005-064- 
執行期間：2011 年 08 月 01 日至 2012 年 07 月 31 日 
 
執行機構及系所：國立中興大學資訊科學與工程學系 
 
計畫主持人：張軒彬 
共同主持人： 
計畫參與人員：陳國瑋、葉焄慧、羅志誠、劉家銘 
 
 
 
成果報告類型(依經費核定清單規定繳交)：■精簡報告  □完整報告 
 
本計畫除繳交成果報告外，另須繳交以下出國心得報告： 
□赴國外出差或研習心得報告 
□赴大陸地區出差或研習心得報告 
■出席國際學術會議心得報告 
□國際合作研究計畫國外研究報告 
 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
            □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
  
 3
 
圖 2. Looping Access Pattern 
如圖 2 所示，Access Sequence 為存取 block 的順序，
而 A、B、C 與 D 表示為 looping 的資料。假設 buffer 
cache的大小可以存放 5個 block且能夠滿足此 looping
的大小，但當迴圈存取 A、B、C 與 D block 時也有存
取其它的block E與F，會使得之後每一次的替換動作，
都會逐出下一個即將被存取的資料，造成 hit ratio 為
零。 
 
2.2 結合其它資訊的替換演算法 
應用程式存取行為多變，已無法單純依據時間區
域性來決定 block 的去留。本節我們將介紹利用其它
資訊所設計的替換演算法。 
 
2.2.1 利用 Buffer Cache 本身的資訊 ─ LIRS 
LIRS 替換演算法利用 IRR 值作為替換選擇的依
據。IRR 的定義為對於同一個 block，最近一次與前一
次的存取間隔中有多少個不同的 block 在這期間內被
存取到。根據 IRR 值的大小，LIRS 將 buffer cache 中
的 blocks 分為 HIR (High Inter-Reference)與 LIR(Low 
Inter-Reference)兩種不同型態，表示 block 的重要性。
當標示為 HIR 的 block 被帶進 buffer cache 時，根據以
往的經驗，此 block 下回再被存取之前會先存取較多
不同的 block，因此，相較於 LIR 的 block，HIR 的 block
會經過較久的時間才會再次的被存取。當 buffer cache
空間不足時，HIR 的 block 就會優先被逐出去，換言
之，LIRS 會保留型態為 LIR 的 block 在 buffer cache
內。 
 
圖 3. LIRS 
LIRS 將 buffer cache 分為 S 與 Q 兩個 stack 來管理，
如圖3所示，stack S與Q都透過時間區域性來做排序，
stack S內存放LIR 的block與HIR block的metadata，
而 stack Q 則存放 HIR 的 block。當 buffer cache 空間
滿了，會將 stack Q 底部的 block 作為逐出的選擇，若
某個 block 再次被存取，而此 block 的 metadata 有在
stack S 中，表示目前 buffer cache 的大小能夠容納此
block 的存取間隔，此 block 就會升級為 LIR 並放入
stack S。 
 
2.2.2 利用檔案存取的資訊 ─ UBM 
對於不同的應用程式，存取同一檔案時，其顯示
出來的存取模式可能不同。例如：對於同一個檔案，
某些應用程式可能只需要讀取此檔案的某一些部分，
而其它應用程式則可能是完整的存取。基於這樣的觀
察，UBM(unified buffer management scheme)替換演算
法[2]利用存取檔案(I-node)時的角度來分類存取過的
block，並將之分類為 SEQ(Sequential)、LOOP(Looping)
與 OTHER 三種不同的類型，SEQ 表示此檔案只會存
取一次後就不再使用，LOOP表示此檔案被存取過後，
經過一段時間後會再次存取，其它則為OTHER類型。
UBM 將 buffer cache 分為三個部分來管理，用來存放
判斷為 SEQ、LOOP 與 OTHER 類型的檔案，在 SEQ
內的檔案都不需要保留，而 LOOP 內的檔案則是透過
檔案的 period 來排序，因 OTHER 類型的檔案無法判
斷其存取行為，故透過 LRU 演算法來做管理。 
 
2.2.3 利用 Program Counter 的資訊 ─PCC 
PCC(Program-Counter-Based Pattern 
Classification)替換演算法將所有執行 I/O 的 Program 
Counter(以下簡稱 PC)做分類[1]，透過 PC 的唯一性，
預測將來執行相同的 I/O instruction 時(相同的 PC)，會
和之前有相同的存取行為。 
下圖4為兩個不同PC所存取過的block分佈圖，
左圖的 PC 都為循序存取，曾經存取過的 block 再也不
會使用，而右圖表示此 PC 會重複的存取曾經存取過
的 block。 
 
圖 4 Program Counter 存取行為 
 
圖 5 PCC - PC Table 與 Block Table 
 
SEQ Program Counter LOOP Program Counter 
 5
 
圖 8 PCC 分類 Block 的問題 
PC1 一開始存取新的 block，所以前 3 個 block 皆判斷
為 OTHER 的類型，當存取到第 4 個 block 時，會判
斷PC1為SEQ的PC並將第 4和 5的 block判斷為SEQ
的類型，接下來當 PC2 也存取相同的 block 時，會得
知這些 block 之前有被存取過，故會將 PC1 的 Seq 減
1 與 Loop 加 1，但這些 block 對於 PC2 而言為第一次
存取，所以一開始的 block 都判斷為 OTHER 的類型，
而當存取完第 4 個 block 之後也會判斷 PC2 為 SEQ 的
PC，也將第 4 和 5 的 block 判斷為 SEQ 的類型，之後
PC2 存取新的 block，都會認定這些 block 都為 SEQ
的類型，而當 PC3 存取與 PC2 同樣的 block 時，也會
將 PC2 的 Seq 減 1 與 Loop 加 1，但這些 block 對於
PC3 而言都為第一次存取，所以一開始判斷這些 block
為 OTHER 的類型，因 PC3 存取的這些 block 數並未
超過 3，所以目前 PC3 為 OTHER 的 PC。接下來 PC2
會存取到之前自己所存取過的資料時，但目前 PC2 的
Seq值為 5仍然比Threshold要來的大且Loop值為 2，
故接下來存取的第一個 block 會視為 SEQ 的類型，當
PC2 的 Loop 值大於 Seq 值時，後來存取的 block 會判
斷為 LOOP 的類型，最後，PC3 存取到 PC2 存取過的
block，但這些 block 對於 PC3 而言都為第一次存取，
而之前 PC3 的 Seq 值為 2，所以第一個 block 判斷為
OTHER 的類型，而之後存取 block 視為 SEQ 的類型。  
從圖 8 很明顯的看得出來 PC1、PC2 與 PC3 都
會互相存取曾經存取過的 block，這些被存取的 block
應該都為 LOOP 的類型，但按照於原先 PCC 的方式，
會使著這些 block 不斷的在 SEQ、OTHER 與 LOOP
三種類型間不斷徘徊而無法正確的判斷，而判斷為
SEQ 的 block 並不會保留在 Buffer Cache 中，最糟的
情況會導致 block 在 buffer cache 內被逐出後又被存
取。 
 
2.4.2 沒有考量 Program Counter 異常的存取行為 
 PCC透過Seq與Loop值來判斷PC的存取行為，
但並不是所有的 PC 都會按照判斷出的結果來存取資
料，如下圖 9 所示，此為 gcc 應用程式中兩個不同 PC
所存取過的block分佈，左圖為會被判斷為SEQ的PC，
而右圖則是會被判斷為 LOOP 的 PC。從圖 9 中我們
看得出判斷為 SEQ 的 PC 雖然會循序的存取 block，
但仍有一小部分的 looping(圓圈所框部分)，而右圖
LOOP 的 PC 在一段時間內重複存取資料後，這些資
料就不會再使用到(圓圈所框部分)。左圖若按照 PCC
管理資料的方式，會導致由 SEQ 的 PC 存取的 block
被丟棄後又馬上存取；右圖為對於 LOOP 的 block，
PCC 可能會因當時存取這些 block 時的 period 較小而
一直保留在 buffer cache 而無法被逐出。 
 
 
 圖 9 不尋常的 PC 存取行為 
3. 系統架構與設計 
3.1 系統架構 
 本系統基於 LIRS 與 PCC 兩種替換演算法，利用
PC 的存取行為預測存取的 block 再被使用的機會，讓
LIRS 在替換上能有更多選擇，而不是只單純依照
block 的型態與時間區域性。 
如下圖 10 所示，本系統藉由 PC Table 與 Block 
Table 來記錄會執行 I/O 的 PC 與 block 的資訊，步驟
(1)(2)(3)(4)皆與原本 LIRS 相同，而(5)(6)為我們新的
機制。 
 
圖 10  系統架構 
(1)一開始所有新存取的 block 都會先放入 stack S，直
到 stack S 滿了；(2)接下來新存取的 block 會放入 stack 
Q，同時也會將此 block 的 metadata 放入 stack S 以表
示當時存取的時間點；(3)當 block 再次被存取，不論
是否存在於 buffer cache 中；只要此 block 的 metadata
有存在於 stack S，則此block會被提升為LIR的block；
(4)當LIR的個數超過 stack S的大小時，最底部的block
會降級為 HIR 並放入 stack Q，同時的，我們也會將此
block 的 metadata 從 stack Q 中移除。 
我們新加入的機制(5)會從 stack S 中挑選暫時重
複存取的 block 提早降級到 stack Q；(6)當 buffer cache
空間不足時，會從 stack Q 中挑選最適合的 block 給予
Block TablePC Table 
SEQ Program Counter LOOP Program Counter 
 7
 
 圖 13 解決 PCs 分享存取的問題 
如圖 13 所示，假設目前 Threshold 也是 3，PC1 一開
始存取新的 block，所以前 3 個 block 皆判斷為 OTHER
的類型，當存取到第 4 個 block 時，會判斷 PC1 為 SEQ
的 PC 並將第 4 和 5 的 block 判斷為 SEQ 的類型，接
下來當 PC2 也存取相同的 block 時，會得知這些 block
之前有被存取過，我們就會將目前 PC 的 Loop 值加 1
及 Seq值減 1，並同時將 PC1的 Seq減 1與Loop加 1，
因 PC2 的 Loop 值大於 Seq 值，所以之後由 PC2 存取
的 block 都會認為是 LOOP 的類型，而當 PC3 存取到
PC2 存取過的 block 時，也會將自己與 PC2 的 Loop
值加 1 與 Seq 值減 1，以此類推，故我們能夠馬上的
判斷出這些 block 都為 LOOP 的類型。 
 
4. 實驗與結果 
4.1 實驗環境 
本實驗我們使用 PCC 所提供的 AccuSim 模擬器
與 Trace 檔案。表 1 為執行的應用程式及其 Trace 來
源。 
表 1 應用程式的 Trace 來源 
應用程式 Trace 來源 
gcc 編譯 linux kernel 2.4.20 
viewperf 執行此 benchmark 實記錄的 Trace 
glimpse 搜尋比對 550MB 資料量的 Trace 
cscope 編輯 linux kernel 2.4.20 
tpc-h 執行此 benchmark 實記錄的 Trace 
   
4.1.1 環境參數 
我們所改良的演算法中ࢼ值為判斷 stack S 中是
否有暫時重複存取的 block，故我們先實驗選擇最適合
的ࢼ值。 
在這個實驗中，我們分別給予 gcc、viewperf 與
glimpse為2MB、32MB和512MB的buffer cache大小，
而 cscope、tpc-h、multi1、multi2 與 multi3 都為 128MB
的buffer cache大小。如圖14，我們能看到當ࢼ值較小，
會使我們在判斷暫時重複存取的 block時太過於積極，
導致 hit ratio 下降，而當ࢼ值太大，會讓我們的方法幾
乎沒有效用，使得 hit ratio 與原本的 LIRS 並無差別，
而從圖中看得出當ࢼ值等於2.5時能夠有較好的效能，
故我們選擇 2.5 為我們的ࢼ值。判斷 PC 是否為循序存
取的 Seq_threshold 值我們採用與 PCC 相同的值，而
計算 PC 與 block 的 period 的 α值我們則設為 0.6，最
後我們每 50 次的存取就會偵測目前的情況來作 stack 
S 與 Q 大小的調整。 
 
圖 14 選擇適當的ࢼ值 
4.2 實驗結果 
 實驗分為三個部分，第一部份會先介紹實驗使用
的 Trace 存取 block 的分佈，第二部份比較在使用的
Trace 下， PCC 與我們的方法在 PC 分類上的差異，
最後部分則與其它不同的替換演算法做比較。 
 
4.2.1 Trace Access Pattern 
一開始我們先查看應用程式的存取行為，圖 15
為每個應用程式在執行時會存取的資料，縱軸 Logical 
Block Number(LBN)，而橫軸為時間點。 
 
圖 15 (a)gcc (b)viewperf (c)glimpse (d)cscope(e)tpc-h 
trace 
如圖 15 所示，圖 15(a)gcc 與圖 15(b)viewperf 包
含循序與迴圈兩種存取行為，而有些資料暫時重覆存
取後便不再使用，其中 viewperf 暫時重覆存取的行為
相當明顯；glimpse 與 cscope 的存取行為類似，glimpse
大部分都為迴圈存取，但包含了一些循序存取，而
cscope 在一開始會先不斷的重複存取曾經存取過的
block，之後皆為迴圈存取，且某些迴圈存取的 period
並不一致，我們也能看出 cscope 也有些只是重複存取
的 block。tpc-h 為隨機的存取行為，從圖中能夠看出
每個被存取 block 位置與時間非常分散。 
 
4.2.2 Program Counter Classification 
圖 16 為我們與 PCC 對於每個 Trace 所有存取過
H
it 
ra
tio
(%
) 
ࢼ 
 9
 
圖 20 gcc hit ratio(%) 
我們觀察到圖 15(a)中，gcc 有暫時重複存取的 block，
因原本 LIRS 無法發現暫時重複存取的 block，故在替
換選擇時只能將 stack Q 底部的 block 逐出，而我們的
方法能夠提早一步的將這些 block 給移動到 stack Q 成
為替換的候選，從實驗結果看出我們改良過後的演算
法，表現能夠較原本的 LIRS 要來的好；因 PCC 迴圈
存取的 block 都由 period 來管理，會使得 period 較小
且只是暫時重複存取的 block 無法被逐出，所以從實
驗數據顯示我們的方法也會比 PCC 要來的好。 
 
圖 21 viewperf hit ratio(%) 
圖 21 為我們的方法與其它不同演算法跑 viewperf 的
Trace 的 hit ratio。由之前圖 15(b)我們能夠知道
viewperf有著許多的暫時重覆存取與循序存取的block，
LIRS 演算法會將暫時重複存取的 block 擺放在 stack S
一段時間而無法提早將這些 block 替換出去，我們的
方法能夠提早得知這些 block 不會再被使用，並將這
些 block 提早移動到 stack Q，相較於原本的 LIRS，我
們提供了更多的替換選擇，改善了 LIRS 在替換時的
草率。由圖 16(b)所示，我們判斷 LOOP 的數量比 PCC
要來的少，但 SEQ 數量要來的多，但從圖 15(b)顯示，
循序存取的 block 數量約占了整體存取的 26%左右，
這表示 PCC 在分類 PC 時沒有考量到分享存取的問題，
使得許多 block 徘徊在 SEQ 與 LOOP 之間而無法正確
的分類。此實驗中有著不少暫時重複存取的 block，按
照 PCC 的管理方式，對於 LOOP 資料的管理則是採取
period 的大小來決定，會使得這些 block 保留在 LOOP
的區域中無法被丟棄出去，故在此實驗中我們的演算
法較 PCC 要來的好。 
 
圖 22 multi1 hit ratio(%) 
圖 22 為我們的方法與其它不同演算法跑 multi1 的
Trace 的 hit ratio。此實驗為同時執行 gcc 與 cscope 兩
個不同的應用程式，由上圖的實驗數據所示，我們的
方法與原本的 LIRS 相較並無多大的差異，這是因為
gcc 只需 2MB 左右的 buffer cache 大小就能滿足其效
能，而從圖 20 也顯示出 gcc 只需 1MB 的大小 hit ratio
就能夠達到 80%，這表示 gcc 對於自己曾經存取過的
block 會再次的存取，而且很少會存取新的 block，由
此原因，故 LIRS 的替換錯誤行為較不明顯，所以由
上面數據顯示出，我們的方法只些微好於原本的
LIRS。 
 
圖 23 multi2 hit ratio(%) 
圖 23 為我們的方法與其它不同演算法跑 multi2 的
Trace 的 hit ratio。此實驗為同時執行 gcc、cscope 與
viewperf 三個不同的應用程式，由此實驗就能夠看出
當同時執行多個應用程式時，LIRS 所受到的影響。
viewperf 有著不少的循序與暫時重複存取的 block，當
與 cscope 和 gcc 同時執行時，block 的交錯存取使得
應該為 LIR 的 block 遭遇循序存取而被替換出去，而
暫時存取的 block 會在原本 LIRS 的 stack S 中滯留一
段時間而無法提早被替換出去；而 PCC 方面，因 gcc
與 viewperf 皆有暫時重複存取的 block，這些 block 可
能會因當時存取的 period 太小而一直保留在 buffer 
cache 內無法被替換掉，總結以上問題點的處理，我們
偵測暫時重複存取的 block 早一步移動到 stack Q 成為
替換的候選，並不是根據 block 的 period 來管理；在
stack Q 中選擇最合適的 block 替換，避免原本應該為
LIR 的 block 被丟棄出去。從此實驗結果顯示出，我
(MB) 
(MB) 
(MB) 
(MB) 
H
it 
ra
tio
(%
) 
H
it 
ra
tio
(%
) 
H
it 
ra
tio
(%
) 
H
it 
ra
tio
(%
) 
出席國際學術會議心得報告 
 
計畫編號 NSC 100-2221-E-005-064- 
計畫名稱 一個利用跨階層資訊最佳化的 buffer cache 管理機制 
出國人員姓名 
服務機關及職稱 張軒彬，國立中興大學資訊科學系副教授 
會議時間地點 101.7.3 ~ 101.7.4 大連 
會議名稱 (中文) 第七屆無線普及計畫國際研討會 
(英文) 7th International Symposium on Wireless Pervasive Computing 
發表論文題目 
(中文 ) 在不具備 MMU 的無線感測網路上設計與實作 incremental
checkpoint 機制 
(英文 ) Design and Implementation of MMU-less Incremental Checkpoint
Schemes in Wireless Sensor Networks 
 
 
一、參加會議經過 
 
本屆無線普及計畫國際研討會是於大陸大連舉行，本人於先由桃園搭乘中華航空飛往北京，
再由北京轉搭南方航空到大連，會議地點會於大連昱聖苑國際酒店。大連正在蓋地鐵，因此從機
場到達會議地點必須搭乘計程車。 
本屆無線普及計畫國際研討會共由 47 papers 投稿，大會選出 22 full papers。因此，會議也
從原本的三天改為兩天，並且為 single track。共有 Air Interfaces、Ad-Hoc Networks、Wireless 
Networks、Ad-hoc Networks & Routing 、General Wireless Mobile Computing 等五個 session。我的
報告時間原本在第一天下午的 Wireless Networks session，但是，早上的議程有三位報到者缺席，
而下午議程的報告者恰好都有出席。因此，會議主席決定將第一天的議程全部濃縮在早上報告完
畢。我的論文題目為：在不具備 MMU 的無線感測網路上設計與實作 incremental checkpoint 機制。
整個報告的過程尚稱順利。或許因為區域性的關係，報告者多來自於台灣和中國大陸，只有少數
美國的學者。也認識了同樣來自台灣的學者，例如海洋大學資工系的趙志民教授、工研院的陳昱
丞博士..等。另外，一些學者也詢問我的研究內容，也在會場與他們交換了一些意見。午餐時
session chair 和參與者一起合照，摘錄於下。 
最後，在與會過程中也相當程度的增廣了自己的見聞，同時也和與會的他國學者交換意見，
對於個人未來的研究有很大的幫助。也謝謝國科會的支持，使得本人有機會參與此次研討會，聆
聽許多一流的研究成果，也認知到不同的研究觀點與研究精神，並從當中獲取到許多有用的資訊。 
 
  
Design and Implementation of MMU-less 
Incremental Checkpoint Schemes in Wireless Sensor 
Networks 
 
Hsung-Pin Chang 
Department of Computer Science and Engineering 
National Chung Hsing University 
Taichung, Taiwan, R.O.C 
hpchang@cs.nchu.edu.tw 
Yen-Ting Liu   Shang-Sheng Yang 
Institute of Networking and Multimedia 
National Chung Hsing University 
Taichung, Taiwan, R.O.C. 
hylswind@gmail.com,       b3c4d5e6f7@hotmail.com
 
 
Abstract—Wireless Sensor Networks promise a new instrument 
for monitoring and collecting data about some outside world 
phenomenon. Since sensor nodes would embed valuable 
information, the failure of these nodes would cause the false 
alarms or missed detections. One solution is to use space-
redundant technique along with the incremental checkpointing to 
save the sensor nodes states. Nevertheless, previous incremental 
checkpoint relies on the page protection hardware. Since sensor 
nodes do not have the page protection hardware, previous 
incremental checkpointing cannot be applied.  In this paper, we 
thus propose three incremental checkpoint methods in MMU-less 
sensor networks. These three methods differ in the granularity of 
checkpoint unit and the module execution overhead. We have 
implemented the three incremental checkpointings on SOS in the 
mica2 mote sensor nodes. According to the experimental results, 
we checkpoint and restore the application state successfully. 
Besides, depending on the characteristics of a module, each 
application modules can select its own incremental checkpoint 
scheme to minimize the checkpoint overhead. 
Keywords-component; formatting; style; styling; insert (key 
words) 
I.  INTRODUCTION 
Wireless Sensor Networks (WSNs) have been used in many 
application domains such as habitat monitoring [14], theft 
detection [6], and ecosystem sampling [5]. In these applications, 
sensor nodes are expected to be long-lived. However, hardware 
reliability, e.g., aging or wear-out, infant mortality, design 
defects etc., is a major obstacle to this expectation [4]. This 
problem is getting more serious since some sensor nodes are 
deployed in harsh environments such as the forest canopy, 
volcano, or the ocean, that would cause the hardware further 
prone to fail [1, 9, 13]. Nevertheless, for some applications, 
node failure is unsustainable and unacceptable. Furthermore, 
some sensor nodes play an important role and failure of these 
nodes could at worst render a senor network useless. For 
example, the failure of a cluster head in cluster-based routings 
would cause the forwarding path to the sink broken. 
Responding to a hardware failure of a node would require 
physical access to the node. Unfortunately, WSNs are usually 
deployed in inaccessible and inhospitable environments. Once 
sensor nodes are deployed, it is impractical and impossible to 
physical access each individual node. Consequently, a failure 
resilient sensor network that can automatically detect and 
recover from hardware failures is important and in fact, 
necessary. 
One possible solution is to use space-redundant technique 
so that the system can continue to operate when a hardware 
component fails. In this paper, on the basis of mica2 motes [2], 
we propose a dual-motes sensor node architecture that connects 
two motes, one is primary and the other is backup node, by an 
UART link. Ordinary, the primary node is active and the 
backup node lies dormant, drawing just few current. Once the 
primary node failed, the backup node takes over the job of 
primary node. Nevertheless, the primary node may embed 
important information. Along with the failure of the primary 
node, the information, which is crucial in some applications, 
e.g., the surveillance systems, is also lost. 
One promising solution to this kind of applications is 
utilizing a checkpoint scheme in our dual-motes sensor nodes. 
The primary node periodically checkpoints its state to the 
backup node. Once the backup node detects that the primary 
node is failed [7], the backup node takes over the job of the 
primary node. However, checkpointing is an expensive 
operation. Thus, lots of improvements have been proposed to 
reduce the checkpoint overhead [11]. One of the well-known 
optimization schemes is the incremental checkpointing. It 
utilizes the page protection hardware to identify the unchanged 
portion of a checkpoint and saves only the changed portion to 
reduce the amount of data written to the storage. Unfortunately, 
sensor nodes do not have the page protection unit and the 
legacy incremental checkpoint cannot be applied in WSNs. 
Consequently, in this paper, we design and implement three 
software-based incremental checkpoint schemes in the MMU-
less sensor nodes. These three incremental checkpoint schemes 
differ in the checkpoint data size and module execution 
overhead. We have implemented these three incremental 
checkpoint schemes in the mica2 motes on the SOS kernel [8]. 
According to the experimental results, our proposed three 
incremental checkpoint schemes can be applied for different 
application modules and reduce the checkpoint overhead 
significantly. 
The remainder of this paper is organized as follows. Section 
II describes the previous checkpoint schemes and introduces 
Consequently, we first parse the mod_bin data
save all the modules’ states. Then, we parse the m
data structure, which is used by SOS to maintain th
to saves the dynamically allocated memory. Howev
the heap memory content, but also the malloc
structure is saved during checkpointing. This is 
malloc_heap at the primary node is changed wh
dynamically allocate and free memory at run tim
malloc_heap at the backup node is still in its init
contrast, we do not have to save the module_bin da
since if a new module is dynamically added into th
will be uploaded to both the primary and backup n
the two module_bin structures in primary and back
always the same. 
Finally, messages that are queued in the messag
timers that have not yet been expired are 
Consequently, the malloc_heap, the messages an
are the only kernel data structures that have to be s
checkpointing. 
(a)  (b) 
Figure 2. (a) surge state structure (b) SurgeMsg struc
IV. INCREMENTAL CHECKPOINT 
In this subsection, we present our prop
incremental checkpointings for MMU-less WSNs. 
A. Method 1: Scheduler-Based Scheme 
During a checkpointing period, some of the m
not be executed. Nevertheless, as stated abov
checkpointing saves the memory area of all of the 
matter they are executed or not. Consequentl
incremental checkpointing intercepts the message 
keep track of the modules that are executed. We
module_bit_map data structure with each bit corres
module. If a module is selected to run by t
scheduler, the bit that is corresponding to the m
When checkpointing, only those modules whose b
the module_bit_map are checkpointed. 
Nevertheless, in addition to the message passin
can also be invoked when another module calls 
functions. In SOS, functions that are provided and
by a module are all maintained in the module’s mo
Consequently, when a module is selected to run by 
scheduler, we also check the module header of thi
see if it also calls other modules’ provided function
we also set the bits of these modules called by this
the process is continued until the bits of modules th
either directly or indirectly by this module are all se
B. Method 2: iCALL-Based Scheme 
Nevertheless, not all subscribed functions will
during execution. For example, although module A
 structure to 
alloc_heap 
e heap area,
er, not only 
_heap data 
because the
en modules
e while the
ial state. By 
ta structure, 
e systems, it
odes. Thus,
up nodes are 
e queue and
also saved.
d the timers 
aved during 
 
ture. 
osed three
odules may 
e, the base
modules, no
y, the first
scheduler to
 maintain a
ponding to a
he message
odule is set.
its are set in
g, a module
its exported
 subscribed
dule header.
the message
s module to 
s. If it does,
 module and 
at are called
t. 
 be invoked
 subscribes 
the exported function F of module B, 
may not invoke function F during its 
method 1 would also checkpoint the m
To address this problem, we interc
insert a new kernel fun
module_called(unsigned module_id) in
of all modules. The module_called
module’s identifier as the parameter. W
function is invoked, we set the bi
module’s identifier in the module_bit_
this module during the checkpointing. 
Compared with method 1, me
checkpoint the modules that are indeed
method 2 pays the extra execution ove
module_called() statement in all provid
C. Method 3: Store-Based Scheme 
The checkpoint unit in both method
address space used by a module, i.e
occupied by a module is checkpointe
variables in either the module state stru
allocated memory are modified during
other words, the checkpoint unit in bot
coarse. 
Consequently, we proposed t
checkpoint scheme that traces the mem
keep track of the part of memory that a
checkpoint period. Firstly, we divid
number of micro blocks and each m
number of bytes. Then, we maintain 
data structure with each bit correspon
Finally, as shown in Figure 3, we inser
each store instruction. The inserted p
new kernel function called mem_ac
finds the micro block that the target 
then sets the corresponding bit in th
structure. Besides, we also modify th
clears the bits of the micro block whi
releases. 
Figure 3. modifications of the sto
From above discussions, method 3
checkpoint methodology since only
memory that are modified are chec
nevertheless, module A 
execution. However, the
emory area of module B. 
ept all module calls and
ction call called 
 all provided functions
() is passed with the 
hen the module_called() 
t corresponding to the
map and also checkpoint 
thod 2 can correctly 
 executed. Nevertheless, 
rhead due to the inserted
ed functions. 
 1 and 2 is the complete 
, the all memory area 
d. Nevertheless, not all
cture or the dynamically 
 a checkpoint period. In
h method 1 and 2 is too
he third incremental
ory write instructions to
re modified during each
e the memory into a
icro block consists of a 
a micro_block_bit_map 
ding to a micro block. 
t a program block before
rogram block invokes a
cess_recorder(), which 
address belongs to and
e micro_block_bit_map 
e free() system call that 
ch the free() system call 
 
re instructions. 
 provides a fine-grained 
 the micro blocks of 
kpointed. Nevertheless, 
message handlers which are invoked at module start up and 
termination, photo_temp module does not embed any message 
handler. Consequently, Table 3 does not show the execution 
time of the photo_temp module. In fact, since photo_temp is 
called by the surge module, the execution time of surge module 
thus includes the execution time of functions exported by the 
photo_temp module. 
TABLE I.  CODE SIZE AND MEMORY OVERHEAD COMPARISON (BYTE S) 
 Code Size Data Size 
Original SOS 48282 2948 
Method 1 55028(+13%) 3512(+19%) 
Method 2 54452(+12%) 3512(+19%) 
Method 3 (2 bytes)  
55804(+15%) 
3640(23%) 
Method 3 (4bytes) 3576(21%) 
Method 3 (8 bytes) 3544(20%) 
TABLE II.  CODE SIZE INCREASE OF APPLICATIONS MODULES (BYTES) 
 Original Method 1 Method 2 Method 3 
Blink 158 158 158 196 
Surge 530 530 530 720 
Neighbor 1222 1222 1222 1748 
Tree 
routing 
920 920 926 1152 
Photo 
temp 
420 420 464 450 
DVM 10976 10976 10976 13740 
From Table 3, similar to that shown in Table 2, method 1 
does not incur any extra execution overhead. Besides, in 
method 2, modules that do not invoke other modules’ exported 
functions have the same execution time as that of the original 
modules. This includes the blink module, neighbor module, 
tree routing module and DVM module. By contrast, since surge 
module calls the exported functions of photo_temp and tree 
routing modules, the execution time is thus increased due to the 
inserted module_called() function. Finally, as expected, for all 
modules, method 3 increases the most significant execution 
overhead. 
Finally, we compare the energy consumption of each 
module under each checkpoint scheme and the results are 
shown in Table 4. The result is similar to the Table 3. Method 
3 consumes the most significant energy consumption. 
TABLE III.  COMPARISONS OF RUN TIME OVERHEADS (MS) 
 Original Method 1 Method 2 Method 3 
Blink 0.27137 0.27137 0.27137 0.407056 
Surge 2.161872 2.225237 2.587056 2.786024 
Neighbor 2.496608 2.686567 2.618725 4.966079 
Tree routing 1.139756 1.166893 1.248304 1.940299 
DVM 13.30841 13.31967 13.37626 24.72863 
TABLE IV.  COMPARISONS OF ENERGY CONSUMPTION (M ICROJ) 
 Original Method 1 Method 2 Method 3 
Blink 6 6 6 9 
Surge 47.7999 52.2 57.20001 61.5999 
Neighbor 55.2 57.9 62.4 109.8 
Tree routing 25.2 25.8 27.6 42.9 
DVM 294.5 294.24999 295.75 546.75 
C. Checkpoint Performance 
Finally, we measure the checkpoint performance. Firstly, 
we measure the checkpoint sizes. At startup, we perform the 
base checkpointing and then perform five incremental 
checkpointing with each checkpoint is invoked every 5 minutes, 
and the average value is used for performance comparison. The 
experimental result is shown in Table 5. Firstly, for blink 
module, since the module state of blink is only 2 bytes, the 
checkpoint size by method 1 and 2 is thus 2 bytes. Nevertheless, 
in method 3, the checkpoint size is 4 or 8 bytes when the micro 
block size is 4 or 8 bytes, which is larger than that in method 1 
and 2. This is because the module state of blink is too small 
(only 2 bytes) that the size of a micro block may be larger than 
that of the blink module state. 
TABLE V.  COMPARISONS OF CHECKPOINT SIZES (BYTE S) 
 Blink modules Surge + 
Photo_temp + 
Tree_routing + 
Neighbor 
modules 
DVM + 
CntToLed scripts 
 Base Inc. Base Inc. Base Inc. 
Method 1 2 2 26 26 552 552 
Method 2 2 2 26 26 552 552 
Method 3 
(2 bytes) 
2 2 26 16 552 38 
Method 3 
(4 bytes) 
2 4 26 24 552 44 
Method 3 
(8 bytes) 
2 8 26 40 552 48 
Secondly, for photo_temp, tree routing, and neighbor 
modules, the checkpoint sizes of method 1 and method 2 are 
the same since tree routing and photo_temp modules are called 
each time when the surge module executes. In method 3, when 
the micro block size is 2 or 4 bytes, the checkpoint size is 
smaller than that of method 1 and 2. Nevertheless, similar to 
the blink module, when the micro block size is 8 bytes, the 
checkpoint size is larger than that of both method 1 and 2. In 
other words, the size of a micro block is too big such that it 
covers the memory areas that are not used by the checkpointed 
modules. 
Finally, for CntToLed script running on the DVM virtual 
machine, since DVM does not invoke any exported functions, 
the checkpoint sizes of both method 1 and 2 are the same. 
Beside, compared with blink and surge modules, the module 
state size of DVM is very large. Consequently, no matter the 
micro block size is set to 2, 4, or 8 bytes, method 3 always 
drives the highest performance. 
Then, we measure the checkpoint time by each incremental 
checkpoint scheme. Table 6 shows the experimental result. 
Compared with Table 5, for all three benchmarks, although the 
checkpoint sizes of base checkpointing, method 1 and method 
2 are the same, the checkpoint time of method 1 and method 2 
is larger than that of base checkpointing since both methods 
have to search the module_bit_map data structure to find 
modules that was executed during the past checkpoint period. 
Besides, for method 3, when the micro block size is 2 bytes, 
the checkpoint size is the smallest. Nevertheless, as shown in 
Table 6, the checkpoint time is the largest when the micro 
block size is 2 bytes. This is because the smaller the micro 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/30
國科會補助計畫
計畫名稱: 一個利用跨階層資訊最佳化的buffer cache管理機制
計畫主持人: 張軒彬
計畫編號: 100-2221-E-005-064- 學門領域: LINUX推動計劃 
研發成果名稱
(中文) Buffer Cache替換演算法
(英文)
成果歸屬機構
國立中興大學 發明人
(創作人)
張軒彬,姜承邦
技術說明
(中文) 記憶體替換演算法(Buffer Cache Replacement)為整體系統運作的重要環節，有
效的利用記憶體空間能減少硬碟的存取次數並提升系統效能。傳統的演算法無法
應付循序與迴圈兩種存取行為(Access Pattern)，故國內外許多學者陸續提出新
的演算法來解決傳統替換演算法上的不足。LIRS演算法在處理這些問題上表現相
當出色，並有效的解決傳統演算法的問題，但是，當多個應用程式同時執行時，
會造成LIRS演算法管理與替換選擇上不夠妥善。我們利用程式計數器(Program 
Counter)的資訊，將LIRS在資料管理與替換選擇上做改善，根據實驗結果所示，
使用新的管理與替換機制後，能夠確實的提升資料的命中率。
(英文) Buffer cache replacement algorithm is an important part of the operation system. A well-
behaved replacement algorithm must efficient utilize the buffer cache space to reduce the 
number of disk accesses so as to increase the system performance. Traditional buffer 
cache replacement, like LRU scheme, cannot handle sequential and loop access pattern, 
and thus many researchers have been proposed new buffer cache replacement schemes to 
solve the problem by properly handling these access patterns. One of the well-known 
solutions is the LIRS buffer cache replacement. However, when many applications 
execute concurrently, LIRS cannot manage the buffer cache properly and would replace 
the wrong blocks. We exploit the program counter information to enhance LIRS buffer 
cache replacement. The experimental results show that our new management mechanism 
can effectively enhance the LIRS buffer cache replacement scheme and improve the hit 
ratios.
產業別 資訊服務業
技術/產品應用範圍
電腦系統產業 
技術移轉可行性及
預期效益
經由本技術，我們可以buffer cache資料命中率，提升硬碟 I/O的存取效能。
註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
2012數位生活科技研討會最佳論文獎 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
