1 
 
行政院國家科學委員會專題研究計畫成果報告 
模糊調控支持向量機-類別標籤模糊化 
(Fuzzy Regularized Support Vector Machines - Class Label Fuzzification) 
 
計畫編號：NSC 95-2221-E-149-016 
執行期限：95 年 8 月 1 日至 96 年 7 月 31 日 
主 持 人：楊 棧 雲      北台灣科學技術學院 機械工程學系 
共同主持人：楊 智 旭      淡江大學 機械工程學系 
計畫參與人員：戴信良、周峰毅、林立夫 
 
 
摘要 
本研究從學習樣本的類別標籤軟化的角度，導入一個基本的模糊概念，用以改變學
習樣本在組態支持向量機的決策函數時的二元化對峙態度。模糊化的類別標籤可兼容一
個足以表達學習樣本在群體中因各種原因而導致的學習態度不一致。本研究同時提出一
個二階段的樣本態度評估方法，用以指定類別標籤模糊值，其大小正相關於其對於支持
向量機的決策函數的影響，是為一類別標籤模糊化支持向量機。經第一階段檢視的學習
樣本，依計算所得的樣本間隔大小，在第二階段重新賦予大小不一的樣本態度，以助於
支持向量機在訓練時因而獲取一較大間隔的決策邊界。也因這個大間隔的決策邊界，本
研究進而獲得最值得注意的結果，一個較佳的泛化性能的決策函數。 
關鍵詞：支持向量機, 模糊, 模式辨識 
 
Abstract 
The paper attempts to introduce a fundamental fuzzy concept to break the equivalent at-
titude of the input training set of SVM, and tries to give individual example in the set a dif-
ferent attitude. The attitude can stand for the influence that the example takes into account in 
the classification. In the paper, we present a method to refresh the attitude by assigning proper 
fuzzy value to the class label of each example. Based on the benefit of individualized fuzzifi-
cation, we re-examine the formulation of SVM, and try to find out the corresponding effects. 
Although it costs a little more computation, the introduction of fuzzy class label is eventually 
worth for the better generalization performance with a large margin. The effect is especially 
both crucial and subtle in worse confused training data set with many hard examples. 
Keywords：Support vector machine; Fuzzy; Pattern Recognition 
3 
 
如前述，在最佳化的過程中，損失函數著重於懲罰負值間隔的樣本，並以所得到的
分類正確程度來決定懲罰的大小，以一個要求訓練正確性高的分類器來說，可加重損失
函數對負值間隔樣本的懲罰，懲罰代價高迫使分類器減少誤差分類正確性自然提高。相
對而言，在一般的情形下，只會要求一個足夠的懲罰程度以追求能產生最大的分類間
隔。所以在上述的條件下，以一個參數 C 來調整訓練精度與間隔之間的平衡。同時透過
這個概念，提出了以個別調整懲罰來減少具干擾樣本影響的方法，因為具干擾樣本通常
具有或多或少的干擾雜訊，所以如何減低具干擾樣本的影響是最直接能強化分類器的方
式，換言之，增進了分類器的泛化能力。 
2. 研究方法 
2.1. 類別標籤模糊化模型 
為了從新計算對具干擾樣本的懲罰，我們建立了標籤模糊化支持向量機以達到這樣
的效果，這個計算從監督式機器學習開始，首先在樣本空間中取得一個訓練集 T，T = {(xi, 
yi)}, i = 1, 2, …, n，在此訓練集中，各樣本賦予一個標籤 yi，yi∈{-1, +1}，以進行二元分
類(Binary Classification)，將樣本分類到兩個不同的類別。過去我們曾探討過如何將受限
制的類別標間給予一較寬鬆的範圍值[6-7]，這樣的方式能讓訓練點具有更多關於自身所
代表的資訊，透過此種模糊化的方式，樣本的類別標籤將實際的從原本限制的固定值轉
換成一模糊集合，訓練集合因此改變為： 
. ..., ,3 ,2 ,1     )}, ,{(' nizT ii == x  (4) 
在(4)中 zi代表了樣本 i 的影響力，為一權重，將其權重表逹為經模糊化後的糢糊標籤，
這樣的目的是為了能以此減少具干擾樣本的權重來減低其影響力。並將 zi以結合一個模
糊係數 si來表示： 
.iii ysz =  (5) 
模糊數 si 可經由許多不同的比例縮減函數來獲得。最經常的狀況是由一個隸屬函數
(Membership Function, MF)用以推估樣本(xi, yi)與其母族群之間的關係來獲得： 
)).(,( iii fyfzs x=  (6) 
為了表逹一個適切的與母族群之間關係的連繫，一個用以表逹其關係強弱的量測必
須先給定。我們期望這個量測可以線性地將關係轉換成一個連續的指標值。在一般的認
定裡，間隔 θi = yif(xi)是這個量測一個相當不錯的選擇。不只是因為 θi可以鑑別其值是
否為負，而且它尚可反應樣本在空間中與分類超平面的距離，一個樣本若其 θi負值越大
5 
 
成： 
. ..., 2, 1,     ,1),( nibsy iii =≥+xw  (11) 
在標籤 yi只能以+1 或-1 表示的條件下，我們也可將不等式分開表示為下面兩個式子 
.1for      /1,
and ,1for      /1,
−=−≤+
+=+≥+
iii
iii
ysb
ysb
xw
xw
 (12) 
對照原本標準的 SVM ： 
,1for      1,
and ,1for      1,
−=−≤+
+=+≥+
ii
ii
yb
yb
xw
xw
 (13) 
在(12)式中，因為導入了 si 所以我們可以將限制式實數化，si 為一模糊值，故其倒數必
定會大於 1。而在實際的應用上不等式(12)中的也改變了原始的式子中的限制式，概括
而言，以 zi來代替 yi能影響原始限制的邊界條件，使其從“±1”延伸為“±1/si”，比較原始
的 SVM，si在最佳化的過程中所具有一個最重要的價值即在於讓我們能以係數變化的方
式來調整對應每一限制式的可行區域(Feasible Region)。 
仿上述模糊化標籤 zi的影響，我們重新修正了軟間隔支持向量機，具ξi的限制式改
變如下： 
     ibz iii ∀−≥+     1),( ,ξxw  (14) 
透過結合限制式(14)與目標函數(1)，在經過 yi與 zi轉換及仿的原始 SVM 推演過程，我
們可以得到一個與原始 SVM 類似的式子，最後經由類似的步驟轉換成一個以對偶變數
表逹的的二次最佳化問題： 
,,
2
1)( max
11 1
∑∑∑
== =
+〉〈−=
n
i
i
n
i
n
j
jijijid zzJ ααα xxα  (15) 
限制式 
.0 Ci ≤≤ α  (16) 
觀察(15-16)，這裡提出的模糊化類別標籤支持向量機與原始的支持向量機[1]幾近
相同，差別只在於將 yi以 zi做取代。同時也與先前 Lin 與 Wang 所提出的模糊支持向量
機(FVM)不同[8-9]，FSVM 引用了外部的模糊值來推演其關係，在最佳化的過程中以外
部的模糊值做為調整鬆弛變數ξi的權重因子。 
7 
 
  
(a) 不具有隸屬函數原始 SVM 模型       (b) 使用常數隸屬函數模糊化轉換後模型 
 
(c) 使用線性隸屬函數模糊化轉換後模型 
圖 3. 比較分由帶隸屬函數或不帶隸屬函數不同模型產生的決策邊界與分類間隔 
此外，我們使用一個 k 摺交叉驗證(k-fold cross-validation)更詳細的測試模糊化轉換
後模型的性能。設 k = 10，對等地設定參數 θ、τ，並以格點{1x102, 1x103, 1x104}來設定
C，RBF 核函數中的參數 σ 則統一採用 1，對 Twonorm 資料集進行格點搜尋配合交叉驗
證的程序，更廣泛的了解使用不同隸屬函數分類器的行為。圖 4 繪出實驗後其性能比較。 
圖 4 每一子圖中，實線及其上的實心符號點與虛線及其上的空心符號點分別表示具
隸屬函數的模糊化轉換後模型與不具隸屬函數的原始模型的性能趨勢比較，比較原始模
型，雖然模糊化轉換後模型在上述對等設定下以較低的 C 值可逹到較低的交叉驗證誤
差。一般而言，採用常數隸屬函數的模型，其交叉驗證誤差的降低並不明顯(圖 4a)，但
採用線性隸屬函數的模型，就有可觀而明顯的降低(圖 4c)。這個較低的交叉驗證誤差證
明模糊化轉換後模型具有大間隔分類器的優越特性。而最低的交叉驗證誤差並非出現在
變化率係數 θ 及 τ 採用極端的設定值，這表示溫和的減低具干擾樣本的影響力較好，太
激烈的減低影響力反而使得分類器學習不足(Under-fitting)，導致泛化性能降低。圖 4b
及圖 4d 二子圖則繪出隨變化率係數 θ 及 τ 增加的間隔變化趨勢，相對於交叉驗證誤差
或多或少的隨變化率係數增加而減少，圖 4b 及圖 4d 表逹一個一致的間隔增加趨勢，間
隔持續地增加直到其到逹一個高界，雖然此高界在使用常數隸屬函數的模型中因參數值
尚未到逹飽和而無從觀察(圖 4b)，但在使用線性隸屬函數的模型就可明顯觀察到(圖
4d)。然而事實上，在此高界所有的具干擾樣本的影響力被消滅殆盡，反而如前所述因
9 
 
[2]  C. Cortes and V.N. Vanik, “Support vector networks,” Machine Learning, Vol. 20, 1995, 
pp 273-297. 
[3]  T. Hastie, R. Tibshirani, and J. Friedman, The Eelements of Statistical Learning, 
Springer-Verlag, New York, 2001. 
[4]  J.D. McAuliffe, M.I. Jordan and P.L. Bartlett, “Convexity, classification, and risk bounds,” 
Tech. Report 638, Dept. of Statistics, UC Berkeley, 2003, CA. 
[5]  J. Shawe-Taylor, M. Anthony, P.L. Bartlett, and R.C. Williamson, “Structural risk minimi-
zation over data-dependent hierarchies,” IEEE Transactions on Information Theory, Vol. 
44, No. 5, 1998, pp 1926-1940. 
[6]  C.-Y. Yang, “Support vector classifier with a fuzzy-value class label,” Lecture Notes in 
Computer Science, Vol. 3173, 2004, Springer-Verlag, Berlin, pp 506-511. 
[7]  C.-Y. Yang, “Generalization Ability in SVM with Fuzzy Class Labels.” Proceedings of In-
ternational Conference on Computational Intellignece and Security 2006, Guangzhou, 
China, Nov. 2006, pp 97-100. 
[8]  C.-F. Lin, and S.-D. Wang, “Fuzzy support vector machines,” IEEE Transactions on Neu-
ral Network, Vol. 13, No. 2, 2002, pp 464-471. 
[9]  C.-F. Lin, and S.-D. Wang, “Training algorithms for fuzzy support vector machines with 
noisy data,” Pattern Recognition Letters, Vol. 25, No. 14, 2004, pp 1647-1656. 
[10]  L. Breiman, “Bias, variance and arcing classifiers,” Tech. Report 460, Dept. of Statistics, 
UC Berkeley, 1996, CA. 
[11]  B. Schölkopf, D. Schuurmans, P.L. Bartlett, and A.J. Smola, Advances in Large Margin 
Classifiers, MIT press, Cambridge, MA, 2000. 
執行本計畫相關之著作 
[1]  Chan-Yun Yang, Che-Chang Hsu, Jr-Syu Yang. Learning SVM with Varied Example Cost: 
A kNN Evaluating Approach. Lecture Notes in Artificial Intelligence. (To appear) 
[2]  Chan-Yun Yang. Generalization Ability in SVM with Fuzzy Class Labels. In Proceedings 
of International Conference on Computational Intellignece and Security 2006 (CIS 2006), 
Guangzhou, China, pp.97-100, 2006,11. (EI) 
[3]  Chan-Yun Yang, Che-Chang Hsu, Jr-Syu Yang. A Novel SVM to Improve Classification 
for Heterogeneous Learning Samples. In Proceedings of International Conference on 
Computational Intellignece and Security 2006 (CIS 2006), Guangzhou, China, pp.172-175, 
2006,11. (EI) 
[4]  Che-Chang Hsu, Chan-Yun Yang, Jr-Syu Yang. Associating kNN and SVM for higher 
classification accuracy. Lecture Notes in Artificial Intelligence, 3801 (2005) 550-555. 
(SCIE) 
[5]  Chan-Yun Yang. Support vector classifier with a fuzzy-value class label. Lecture Notes in 
Computer Science, 3173 (2004) 506-511. (SCIE) 
[6]  Chan-Yun Yang. Highlighting heterogeneous samples in support vector machines' training. 
Submitted to Neurocomputing. (Revised) 
[7]  Chan-Yun Yang, Che-Chang Hsu, Jr-Syu Yang. Emphasizing heterogeneities in SVM clas-
sification by an embedded kNN preprocessor. Submitted to Pattern Recognition Letters. 
(Under revision) 
[8]  Chan-Yun Yang. Learning support vector machines with fuzzified class labels. Submitted 
to Fuzzy Sets and Systems. (Under review) 
計畫成果自評 
出席國際學術會議心得報告 
                                                             
計畫編號 NSC 95-2221-E-149-016 
計畫名稱 模糊調控支持向量機 - 類別標籤模糊化 
出國人員姓名 
服務機關及職稱 
楊棧雲, 北台灣科學技術學院, 副教授 
會議時間地點 自 95年 11月 03日至年 11月 06日, 中國大陸廣東省廣州市 
會議名稱 2006 International Conference on Computational Intelligence and Security 
發表論文題目 
1. Generalization Ability in SVM with Fuzzy Class Labels 
2. A Novel SVM to Improve Classification for Heterogeneous Learning 
Samples. 
 
一、參加會議經過 
去年我們參加過 CIS2005的會議，並與與會的許多學者討論交換意見，今年 CIS2005
再度來函邀稿，我們欣然同意的投稿，在經過冗長的審稿過程後，我們的文章儌倖獲得
錄取，不期國科會計畫通過，原編列有出席國際會議的經費，使得出國的經費有著落，
謝謝國科會的支持．難能可貴的是，我們有兩篇文章獲得此研討會刊錄發表，收錄到 IEEE 
Explore的資料庫中，是 EI收錄的論文集， 並且在會中受邀擔任“Session Chair”，這是
大會給予的榮譽，也對個人研究受到重視有一點的感到光榮，或許這可用以表徵我們長
期的耕耘受到了肯定。當然我們也在文章的誌謝中標上了國科會的計畫編號
「NSC95-2221-E-149-016」，以彰顯國科會給我們的支持． 
二、與會心得 
支持向量機是機器學習及資料探勘領域較新的一個理論，在理論或應用都與時俱進
每每都有不斷的創新，參加如此國際性研討會一來可以更接近的了解國際上一些新發展
的前沿，二來發表個人的研究心得，參與這個國際性的競爭，以表達我們的研究，建立
信心。此次會議的專家多為世界各國此領域之專家，因此在這裡可以充分接收到有關的
最新研究及其目前發展的訊息。我們很高興三年來的研究正陸續在開花結果中，有了國
科會的支持，我們比較能有專注投注在研究上．謝謝國科會！目前我們正在努力擴大我
們研究，包括其所站立的基礎與延伸應用，也試圖在我校建立一個相關的研究團隊，當
然這需要持續的關注．另外，智慧型控制在本校之機械系及機電所是已行之有年的課程，
由於這方面的課程內容屢屢創新，必須經常吸收各種最新技術以維持教學品質與內容上
的跟上潮流。此次研討會廣泛吸收到的一些看法及研究正好可以用以補充這方面的課程
需求，藉以豐富課程內容，畢竟這些豐富的資料都是當今最新的發展。將來或可表現在
機電整合研究所智慧型控制課程或其他課程的相關主題中。 
In general, the surrogate of loss function always 
focuses on the examples with negative margin and tries 
to penalize these examples under the optimization. The 
degree of penalization depends on the goal of accuracy 
that the classification ties to. For good classification 
accuracy, the surrogate, forcing the optimizer to focus 
on the misclassified examples, should penalize the 
examples with negative margin more heavily than the 
ones with positive margin. In contrast to the goal of 
generalization performance, small but sufficient 
penalties for the examples with negative margin to 
grant the margin of a classifier as large as possible are 
usually demanded. The regularization between the 
classification accuracy and the generalization 
performance depends on the penalty parameter C in (1). 
Based on the idea, a method try to reduce the influence 
of such hard examples by adjusting the penalties was 
proposed. Since most of hard examples contained quite 
amount of noisy information, reduction of their 
influence is an efficient way to enhance the robustness 
of the classifier, in other words, to give the classifier 
good generalization ability with large margin.  
2. Relaxation of hard-limited class label 
In order to re-evaluate the penalties of hard 
examples, a theory of fuzzy class label for SVM was 
developed. The fuzzification can start with the problem 
in supervised machine learning. A training set T, T = 
{(xi, yi)}, i = 1, 2, …, n, should first be drawn from the 
sample space. In the training set, the class label yi
given as a crisp value, yi{-1, +1} for a binary 
classification, represents the class belonging of 
example i. Motivated by the hard-limited crisp values 
for the class label, a proposition to allow some 
relaxation from the crisp value to a fuzzy value has 
been studied in the author’s previous work [6]. The 
relaxation urges the training example to carry more 
information itself about the organization of the training 
set. Based on the fuzzification, the class label is 
changed from the crisp label set to a fuzzy set 
essentially. The training set is therefore changed as 
 ....,,3,2,1)},,{(' nizT ii   x  
In (4), zi denotes a fuzzified class label, a scaling 
weight to represent the exact influence of example i.
The intention of this fuzzification is to weaken the 
influence by reducing the weights of the hard examples. 
Hence, zi can be given as a reduction version of yi by a 
scaling fuzzy number si:
 .iii ysz   
The fuzzy number si can be obtained using many 
scaling algorithms. Usually, it is used to be assigned by 
a membership function (MF) to assess the relationship 
between the example (xi, yi) and the exact class the 
example belongs to: 
 )).(,( iii fyfzs x  
In order to present a proper modification of 
relationship to the native class, a metric expressing the 
relationship strength of the hard examples should be 
given in advance. The metric should be capable to 
linearly convert the relationship into continuous 
indices. As generally adopted, the margin yif(xi) was an 
excellent choice of the metric. For the reason that not 
only it can identify the hard examples with negative 
value, but also it can reflect the distance measuring 
from example location to the separating hyperplane. 
The larger the negative margin of an example, the 
weaker the relationship to its native class. The choice 
of the membership function depends on the problem 
that the application wants to solve and the capability 
that the classifier wants to achieve. For example, a 
typical linear membership function is chosen as a 
demonstration: 

°¯
°
®
­
!

t
 
,0)(for,1
,/1)(for,
,0)(/1for,0),(1
))(,(
ii
ii
iiii
ii
fy
fy
fyfy
fyfz
x
x
xx
x
WH
WWW  
where Ĳ denotes a rate constant, and İ denotes a 
sufficient small constant to avoid singular 
configurations in the matrix operation. With the 
parameterized zi, the given label of each example in the 
training set is no longer a crisp value. Instead, it 
became a positive or a negative fuzzy value: 
 11 dd iz  
Due to the change from yi to zi, the corresponding 
canonical constraints in the separable case of SVM is 
also changed from yi(<w, xi>+b)  1 to  
 ....,2,1,,1),( nibz ii  txw  
where w and b resulting from zi is denoted the weights 
and bias as they have been formerly characterized in 
SVM, and the expression of <·, ·> denotes a kernel 
function. The inequalities of (9) can be recovered by 
replacing zi with yi and si:
1-4244-0605-6/06/$20.00 ©2006 IEEE. 98
Furthermore, a set of k-fold cross-validation 
procedures setting k=10 was set up to investigate 
further details for the performance of the fuz-SVM. 
The procedures were starting with a grid-search of 
parameter Ĳ in the membership function. In order to 
widely inspect the behavior of the membership 
functions, a neat change of C in the set of {1x102,
1x103, 1x104} was issued. Beside that, the parameter ı
involving in the validation process was identically set 
to 1. Panels in Fig. 3 show the results of the validations 
on the Twonorm dataset. In Fig. 3, both of the panels, 
consisting of three pairs of solid and dashed lines with 
the respective solid and hollow legends, illustrate the 
tendency of the performance issue. Comparing to the 
standard SVM, the model has the ability to reach the 
maximal error reduction with a small value of C. With 
the error reduction, the fuz-SVM presents its 
excellence for being a large margin classifier. The 
maximal error reduction does not occur at the 
extremely setting of the parameter Ĳ. Only the 
intension to moderately weaken the influence of the 
hard examples is a good policy. The adequate value for 
the parameter Ĳ depends on the complexity of dataset. 
In spite of the error more or less reduced with the 
increasing parameter setting, Fig. 3 shows an 
increasing trend towards an agreement of the large 
margin. The margin continuously grows until it 
reaches an upper bound. The scale of growth is very 
significant. As illustrated [1, 4], a large margin reduces 
the complexity in VC dimension, and makes the 
induced classifier has a higher capacity of 
generalization performance. Hence, we can conclude 
that the evidence of margin growth is a crucial point 
for the success of the fuz-SVM. 
 13
 14
 22
 24
 32
 38
 41
 43
 64
 84
101
114
Non-Fuzzy
Point in Class A              
Point in Class B              
Misclassified Point in Class A
Misclassified Point in Class B
  7
 14
 16
 20
 22
 24
 32
 36
 37
 38
 52
 61
 64
 65
 83
100
101
113
114
Linear MF, W = 32
Point in Class A              
Point in Class B              
Misclassified Point in Class A
Misclassified Point in Class B
(a) Without MF                         (b) With MF
Figure 2. Decision boundaries generated by 
comparative models 
0 2 4 6 8 10 12
18
20
22
24
26
28
30
C
la
s
s
if
ic
a
ti
o
n
 E
rr
o
r 
(%
)
log
2
(W)
Fuz-SVM (C = 1e2)
Standard SVM (C = 1e2)
Fuz-SVM (C = 1e3)
Standard SVM (C = 1e3)
Fuz-SVM (C = 1e4)
Standard SVM (C = 1e4)
0 2 4 6 8 10 12
0
0.05
0.1
0.15
0.2
0.25
M
a
rg
in
log
2
(W)
Fuz-SVM (C = 1e2)
Standard SVM (C = 1e2)
Fuz-SVM (C = 1e3)
Standard SVM (C = 1e3)
Fuz-SVM (C = 1e4)
Standard SVM (C = 1e4)
(a) Classification error                 (b) Margin
Figure 3. Generalization performance by 
cross-validation 
5. Conclusion 
With the most crucial property of the potential for 
generalization, the method of the fuz-SVM, 
introducing an idea of fuzzy class membership to the 
examples of training set, was successfully developed. 
Both the classification error and margin were 
examined by a k-fold cross-validation to confirm its 
generalization performance. The model of the fuz-
SVM stepped into the reconfiguration of the decision 
function by the involving fuzzy membership among 
the training examples and reformed a more 
generalization hyperplane. Since change of zi is 
substantially intrinsic and elementary, the idea may 
broadly extend to different prototypes of SVM if it is 
not conflict with the original idea of the prototype. The 
beautiful formulation of subjected minimization with 
the ordered replacements of yi with zi renders the class 
membership fuzzification advantageous for further 
developments or modifications. 
Acknowledgment 
This work was supported by the National Science 
Council, Taiwan, ROC, under Contract NSC 95-2221-
E-149-016. 
References 
 [1] Vapnik, V.N., The nature of statistical learning theory,
Springer-Verlag, New York, 1995. 
[2] C. Cortes, and V.N. Vapnik, “Support vector networks”, 
Machine Learning, vol. 20, 1995, pp. 273-297. 
[3] Hastie, T., R. Tibshirani, and J. Friedman, The elements 
of statistical learning, Springer-Verlag, New York, 2001. 
[4] P.L. Bartlett, M.I. Jordan, and J.D. McAuliffe, 
“Convexity, classification, and risk bounds”, Tech. Report 
638, Dept. of Statistics, UC Berkeley, CA, 2003. 
[5] J. Shawe-Taylor, P.L. Bartlett, R.C. Williamson, and M. 
Anthony, “Structural risk minimization over data-dependent 
hierarchies”, IEEE Transactions on Information Theory, vol. 
44, no. 5, 1998, pp. 1926-1940. 
[6] C.-Y. Yang, “Support vector classifier with a fuzzy-value 
class label”, Lecture Notes in Computer Science, vol. 3173, 
Springer-Verlag, Berlin, 2004, pp. 506-511. 
[7] C.-F. Lin, and S.-D. Wang, “Fuzzy support vector 
machines”, IEEE Transactions on Neural Networks, vol. 13, 
no. 2, 2002, pp. 464-471. 
[8] C.-F. Lin, and S.-D. Wang, “Training algorithms for 
fuzzy support vector machines with noisy data”, Pattern 
Recognition Letters, vol. 25, no. 14, 2004, pp. 1647-1656. 
[9] L. Breiman, “Bias, variance and arcing classifiers”, Tech. 
Report 460, Dept. of Statistics, UC Berkeley, CA, 1996. 
[10] Smola, A.J., P.L. Bartlett, B. Schölkopf, and D. 
Schuurmans, Advances in Large Margin Classifiers, MIT 
press, Cambridge, MA, 2000. 
1-4244-0605-6/06/$20.00 ©2006 IEEE. 100
learning technique, which is learning directly from a 
set of available examples, and interpreting the results 
statistically. Instead of trying to create rules, the kNN 
approaches work directly from examples themselves. 
Suppose an unlabeled example x is placed among a set 
of n training examples in a region of volume V, and it 
captures k neighboring examples. Counting the 
neighboring examples, a portion of kj examples turns 
out to be in class labeled Ȧj. The joint density function 
of x and Ȧj can be approximated  
 .
/
),(
V
nk
p jj  Zx  
From Bayes rule, 
 ),,()()( jj ppP ZZ xxx   
The posteriori probability of P(Ȧj|x) can be 
obtained by  
 ,
)/(
)/(
),(
),(
)(
11
k
k
V
nk
V
nk
p
p
P jc
t
t
j
c
t
t
j
j    
¦¦
  
Z
Z
Z
x
x
x  
where c denotes the number of classes. With (3), one 
can estimate P(Ȧj|x) by the fraction of examples 
captured in a local region labeled Ȧj. Since it does not 
need to build a classifier actually, the set of training 
examples should be very representative for inference. 
The term of “prototypes” is used for the crucial set of 
examples. In the kNN classification, the kNN rules 
only require: an ordinary odd integer k, a family of 
prototypes, and a metric to measure “closeness” to 
collect the closest patterns for decision-making. Using 
the prototypes, the class belonging of a new arrival 
query example is determined locally by the k nearest 
neighboring prototypes around the query example. It is 
a simple and intuitive method to classify unlabeled 
examples based on the similarity to the neighbors in 
the feature space. 
In general, difficult examples may lie distant from 
the gathering area of the examples with the same class 
label, or may locate close to the border of adjacent 
overlapped region in which the examples come from 
different class are resided together. However, the non-
parametric method which captures the local structure 
of a little part of the underlying prototypes is quite 
suitable to employ as a preprocessor for filtering the 
individual difficult examples. 
2.2. SVMs with weighted class labels 
The support vector machines were developed based 
on the structural risk minimization principle of 
statistical learning theory [1-5] by learning from the 
training samples, the decision function therefore can be 
obtained. The basic form of decision function in the 
SVM is given as f(x) = sign(<w, x> + b) which is 
described by a vector of orientation weights w and a 
bias term b. The goal of the SVM is to find a 
separating hyperplane with maximal margin while the 
classification error can be minimized by the training 
samples. With the notation of the input training set, S =
{(xi, yi)}, a proposition starts with the change of S [12]: 
 ....,2,,1)},~,{(~ niyS ii   x  
In the expression, Ϳi, having its sign identical to yi in 
the training set S, denotes a relaxed real-valued class 
label to represent the potential weights that sample i
should be taken. The expression of S˜ tries to carry 
more information about the training set, regardless 
both S and S˜ contains a set of the same patterns xi. The 
change of S in S˜ involves a remapping regarding to the 
idea of assigning various weights for the samples in 
different situations. In (4), class label Ϳi is no longer a 
discrete value; instead it becomes a real-value to 
represent an implicit relationship to the sample’s native 
class. Incorporating the idea of kNN, the value of Ϳi
can be obtained from the idea of kNN 
 ,
)|(
~
ii
i
i P
yy
xZ
K  
where P(Ȧi|xi) is the posteriori probability denoted in 
(3). The method of (5), called kNN emphasizer, adopts 
an inverted scheme to scale the value of Ϳi. The 
essential in the expression of Ϳi can be discovered by 
the ratio of magnification 1/P(Ȧi|xi). Generally, the 
value of the ratio will be greater than 1. Our intention 
is to use the ratio to magnify yi. Parameter Ș in the 
expressions, called an acceleration factor, should be a 
positive real and greater than 1 to ensure that | Ϳi |  1. 
The scaled-up real-valued class label Ϳi provides a 
stricter penalty in the optimization, and is able to 
conduct the classification more accurate. Especially, 
the improvement will be more significant in a much 
more confused dataset with many difficult examples. 
By the magnification, difficult examples are designed 
to carry heavier weight in order to receive more 
penalties in the optimization. However, a set of 
canonical constraints is setup with the primal objective 
1-4244-0605-6/06/$20.00 ©2006 IEEE. 173
