The classical LS and TLS approximation me-
thods minimize, respectively, the latency and misfit for a 
linear static model class. However, for the linear time-
invariant dynamic model class, these two methods cannot 
be used directly. Since only one single time epoch is con-
sidered in both the LS and TLS algorithm (snapshot algo-
rithm), the correlation between consecutive time epochs is 
not able to be included in both algorithms. In this paper, 
to overcome this difficulty, we adopt a behavioral ap-
proach to model a dynamical system. The behavioral ap-
proach, introduced in a series of papers by J. C. Wil-
lems [15][16][17], is able to provide a rigorous frame-
work for deriving mathematical models that is suitable for 
dynamic model identification (or model approximation). 
In this approach, a dynamical system can be modeled in a 
kernel representation and then the kernel representation 
can be transformed to an equivalent structured TLS prob-
lem, which can then be solved by a structured TLS algo-
rithm. In this paper, a system identification problem for a 
flight dynamical trajectory is served as an example to 
illustrate the idea. After a dynamical system (in kernel 
representation) is identified by the structured TLS method, 
we can then use the derived model to predict the trajecto-
ry in arbitrary time epoch. 
,
 
On the other hand, for a classical GNSS receiver, 
the DOP (dilution of precision) values can only be calcu-
lated at a single time epoch even for a dynamical system 
that is highly time correlated[8][9]. As we shall see in this 
paper, the behavioral framework provides a natural way 
to derive the geometric matrix for multiple time epochs so 
that the geometric DOP of a kinematic positioning prob-
lem can be determined. In this way, the geometric diversi-
ty of the positioning problem can be increased so that the 
DOP of the corresponding problem can be reduced. 
 
The rest of the paper is organized as follows: In 
Section 2, we will briefly review the concept of behavior-
al framework as a tool for the optimal approximate mod-
eling of dynamical systems as well as some of the useful 
results and theorem. In Section 3, we shall use the concept 
of DOP to give a brief analysis on different optimization 
algorithms. In Section 4, some experimental results will 
be given to illustrate the usefulness of the proposed me-
thod. Finally, some concluding remarks will be given in 
Section 5. 
 
 
2. BEHAVIORAL FRAMEWORK 
 
Dynamical systems describe variables that are functions 
of one independent variable, usually referred to as “time”. 
In the behavioral setting, a system was defined as a subset 
B of a universum set U. In the context of dynamical sys-
tem, U is a set of functions  denoted by WT. 
The sets W and  are called signal space and time 
axis, respectively. The signal space is the set where the 
system variables take on their values and the time axis is 
the set where the time variable takes on its values. We use 
the following definition of a dynamical system 
:w →T W
⊆T R
T
TW
B
[15]. 
 
Definition 1. A dynamical system Σ is a 3-tuple 
Σ = (T, W, B) with  the time axis, W the signal 
space, and B  the behavior.   ■ 
⊆R
⊆
 
The behavior  is the set of all legitimate 
functions, according to the system Σ, from the universum 
set U = WT. The behavior can be described in many ways, 
while in the context of dynamical systems, the most often 
used are represented by the mapping 
⊆ TW
: gf →TW R , i.e., 
 
{ }| ( ) 0=B w f w= ∈ TW
σB,
2 1 1( )t t− +Rw
1 2[ , ]
| t t∈B
w∈
. 
 
The equations f(w) = 0 are called annihilating behavioral 
equations. A dynamical system Σ = (T, W, B) is linear 
when the signal space W is a vector space and B is a li-
near subspace of WT (viewed as a vector space in the nat-
ural way). In this paper, we restrict ourselves to the case 
when the time axis is either T = N or T = Z, namely the 
discrete-time case. A system Σ = (T, W, B) is time-
invariant if  where σ is the backward shift oper-
ator (σw) := w(t+1) and . In the case 
T = Z, a system Σ = (T, W, B) is time-invariant if B = 
σB. Time-invariance requires that if a time series w is a 
trajectory of a time-invariant system, then all its backward 
shifts σtw, t > 0, are also trajectories of that system. 
⊆B
: { | }w wσ σ= ∈B B
.
 
The restriction of the behavior  to 
the time interval [t1, t2], where  and t1 < t2, is 
denoted by 
( )⊆ TRB w
1 2,t t ∈T
 
1 2[ , ]
| : { | , , ( , , ) }t t w w w w w w− + − += ∈ ∃ ∋ ∈B Bcol
 
 
A system Σ = (T, W, B) is complete if 
 
0 1[ , ]
| t tw  for all   0 1 0 1, ,t t t t∈ ≤T
implies that  ;B
 
2 
The constraint of (5) enforces the structured matrix  
to be rank deficient, with rank at most rowdim(X). The 
cost function measures the distance of the given data w to 
its approximation . Thus the STLS problem aims at 
optimal structured low rank approximation of S  by 
. To express (4) as an STLS problem (5), we need to 
ensure that the constraint 
ˆ( )wS
( )w
wˆ
ˆ( )wS
class. Let B be defined as above and let w be an observed 
time series such that . It is assumed that . 
The problem of global total least-squares aims to find the 
model  that best fits the data according to the misfit 
criterion 
w∈B ∈MB
Bˆ
 
ˆ : arg min ( , )M w
∈
=
MB
B B  
  
ˆ( ) 0
X
w
I
⎡ ⎤⎢ ⎥ =⎢ ⎥−⎣ ⎦
S  with 
 
 
2
2
ˆ
ˆ( , ) : min
w
M w w
∈
= − ABB  w . is equivalent to . The following results are 
taken from 
,ˆ m lw∈ ∈LB
[7]. 
 
For the solution of the GTLS problem, on the one hand, 
the approach taken by Roorda and Heij [11][12] is based 
on solving the inner minimization problem, the misfit 
computation, by using isometric state representation and 
subsequently used alternating least-squares or Gauss-
Newton type algorithm for the outer minimization prob-
lem. Also a state-space representation with driving input 
is used in [11][12]. On the other hand, in [6], Markovsky 
et al. use a different approach to solve the GTLS problem. 
They have related the identification problem to the struc-
tured total least-squares (STLS) problem and subsequent-
ly used solution methods developed for the STLS problem. 
Also a kernel representation of the system was used in 
their approach. 
 
Lemma 3. Consider a time series w := 
(w(1),…,w(T)), , and natural numbers m  
and . Assume that  for certain 
matrix R := [R0 R1 … Rl], , where p := 
w – m, with Rl being full row rank. Then the system B, 
defined by the kernel representation R(σ)w = 0 with 
, is such that w , and 
the order of B is n(B) = pl.   ■ 
( )w t ∈Rw
i
iR z
≤ w
,m l∈
1l T≤ −
0
( ) l
i==∑
1( ) 0+ =Hl wR
1 1,...,
p
lR
×∈R w
[1, ]| ,T∈B B
,R R
R z L
 
Lemma 4. Consider a time series 
  
The optimization problem considered in this pa-
per is defined as follows. w := (w(1),…,w(T)), , ( )w t ∈R
w
  
and natural numbers  and . Assume that 
there is a system B  with order n(B) = pl, such that 
. Let R(σ)w = 0, where , be 
a shortest lag kernel representation of B. Then Rl is full-
row rank and the matrix R := [R0 R1 … Rl] annihilates the 
Hankel matrix Hl+1(w), i.e., .  ■ 
m≤ w
,m lL
R
1l T≤ −
( )R z =
1( ) 0=w
∈
[1, ]| Tw∈B 0
l i
ii
R z=∑
+Hl
Problem 1(GTLS): For a given time series w and a com-
plexity specification (m, l), where m is the number of in-
puts and l is the lag of the identified system, solve the 
optimization problem 
 
 ( )
2,
2
ˆ
ˆ ˆ: arg min min .
m l w
w w
∈ ∈
= −
L AB B
B   (4) 
 
 The optimal approximating time series is , correspond-
ing to a global minimum point of (4), and the optimal 
approximating system is . 
wˆ
Bˆ
Theorem 5. Assume that  is a system 
that admits a kernel representation R(σ)w = 0, 
 with Rl =: [Ql  -Pl], 
,m l∈LB
0
( ) l iiiR z R z==∑ p p×lP ∈R
[1, ]| TB
 of full-
row rank. Then the constraint w  is equivalent to 
the constraint 
∈
 
Problem 1 is a GTLS problem for the model 
class M = Lm,l. The first step here to solve the GTLS 
problem is to express (3) as an STLS problem. The STLS 
problem is defined as follows [1][4]: Given a time series 
w and a structure specification S, find the global mini-
mum point of the optimization problem 
 
1( ) 0,
T
l
X
w
I+
⎡ ⎤⎢ ⎥ =⎢ ⎥−⎣ ⎦
H    (6) 
  
2
2
ˆ
ˆ ˆmin min  subject to ( ) 0 .
X w
X
w w w
I
⎛ ⎞⎡ ⎤ ⎟⎜ ⎢ ⎥ ⎟−⎜ ⎟⎜ ⎢ ⎥ ⎟⎜ −⎝ ⎠⎣ ⎦
SA =  (5) 
where 1 0 1[   ]
T
l l .lX P R R Q
−
−= "    ■ 
 
 Before the end of this section, we will present a 
simple way to model a dynamic positioning problem in  
4 
  
Clearly, the observation matrix H’ is subject to uncertain-
ty mainly due to ephemeris error in this more realistic 
model. A positioning method that accounts for the error 
associated with observation matrix is worth investigating. 
2 -1 2 -1
5 5GDOP ( ' ' ) ' '( ' ' )
T T T
TLS trace H H I H H H H Iσ σ= − −
 
To calculate GDOP for the STLS algorithm, again we 
have to firstly derive a solution of the STLS optimization 
problem, which can be obtained by the subspace identifi-
cation method. We will not present the detail of the pro-
cedure here. Only the results will be presented here, 
which can be obtained by using the method shown 
in [7][13]. By using the subspace identification method, 
the solution to the STLS problem (or equivalently, solu-
tion to (6)) can be shown t
 
 
Let H’ = H + HΔ , where HΔ  is the perturba-
tion matrix, the resulting matrix equation becomes 
 
( )H H+Δ = +p q e.    (11) 
 
This model is termed as the unstructured error-in-variable 
model as the matrix HΔ  represents the errors associated 
with the observation matrix and no a priori structure on 
HΔ  is being imposed. The traditional least-squares solu-
tion of Equation (10) can be derived as: 
o be 
;( )
0
1
1 1 1 1 1
:
   ( ) ( )( ( ) ( )) ( )
STLS
T T
l l l l l
X
Y
I
w I w w w w−+ + + + +
⎡ ⎤⎢ ⎥ =⎢ ⎥⎣ ⎦
= −H H H H H
 
 
  -1ˆ ( ' ') ' ;T TLS H H H=p q  
 Note that the similarity between the solution of the TLS 
problem and STLS problem; however, in the case of the 
STLS optimization, the dimension of Hl+1(w) is 
 times (l – T), therefore the solution Y0 would be 
a n× es n matrix rather than a single vector as 
in the case of the TLS problem. With the above identity in 
hand, the GDOP for the STLS problem can be easily 
computed as follows: 
( 1)n l× +
( 1l+ )
and the geometric dilution of precision (GDOP) for the 
LS algorithm is given by 
 
 -1GDOP ( ' ')TLS trace H H=   tim
 
On the other hand, for the TLS algorithm, the 
GDOP can be derived as follows. The TLS optimization 
has a solution if and only if the matrix [H’ q] is rank defi-
cient. A general procedure for solving the TLS problem 
can be found in [13] and we shall only give a brief outline 
here. Let  
 
( )( )11 1 1 1 1 1
GDOP
( ) ( )( ( ) ( )) ( ) ( ) .
STLS
T T T
l l l l l ltrace w I w w w w w
−
+ + + + + += −H H H H H H
 
   [ '    ] TTLS TLS TLSH U S V=q   
4. EXPERIMENTAL AND SIMULA-
TION RESULTS 
 
be a singular value decomposition of the augmentated 
matrix [H’  q], where UTLS  is an n × n orthogonal matrix, 
  
In order to verify the performance of proposed methods, 
we shall conduct several experiments in this section. 
Firstly, a static positioning problem is investigated. We 
will compare the results of different optimization algo-
rithms. Then a kinematic positioning problem is investi-
gated. To do this, we first construct a flight trajectory and 
then analyze the resulting error statistics. Finally, we will 
consider a model identification problem which may be 
solved by using the STLS algorithm in the behavioral 
setting. 
  
  
1
2
3
4
5
5
0 0 0 0
0 0 0 0
0 0 0 0
0 0 0 0
0 0 0 0
0 0 0 0 0
TLS
n
S
σ
σ
σ
σ
σ
×
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥= ⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
# # # # #
   For the first experiment, the results shown in 
Figures 1-6 are based on real collected data. In those fig-
ures, DLS, MTLS, and STLS stand for data least-squares, 
mix least-squares-total least squares, and structured total 
least squares, respectively. DLS is a modified version of 
the TLS algorithm, and MTLS can be viewed as a special 
case for the STLS algorithm. Here we shall compare the 
results of different algorithm. For detailed discussion of 
those methods, please see [1][2][11][14]. Figure 1 shows 
and VTLS is a 5 × 5 orthogonal matrix. Then the optimal 
TLS estimator is given by 
 
  2 -15ˆ ( ' ' ) ' ,T TTLS H H I Hσ= −p q
 
and the GDOP in this case is given by 
 
6 
[12] B. Roorda and C. Heij, “Global Total Least Squares 
Modeling of Multivariate Time Series,” IEEE Trans. 
Automatic Control, Vol. 40, No. 1, pp. 50-63, 1995. 
[15] J.C. Willems, “From Time Series to Linear System—
Part I. Finite Dimensional Linear Time Invariant Sys-
tems,” Automatica, Vol. 22, pp. 561-580, 1986. 
[13] P. Van Overschee and B. De Moor, Subspace Identi-
fication for Linear Systems: Theory,Implementation, 
Applications, Kluwer, Boston, 1996. 
[16] J.C. Willems, “From Time Series to Linear System—
Part II. Exact Modeling,” Automatica, Vol. 22, pp. 
675-694, 1986. 
[14] S. Van Huffel and J. Vandewalle, The Total Least 
Squares Problem : Computational Aspects and Anal-
ysis. Philadelphia: Society for Industrial and Applied 
Mathematics, 1991. 
[17] J.C. Willems, “From Time Series to Linear System—
Part III. Approximate Modeling,” Automatica, Vol. 
23, pp. 87-115, 1987. 
 
 
 
 
 
 
 
STLS 
STLS 
STLS 
Figure 1. Estimated x-, y-, z-positions in ECEF coordinate 
 
 
8 
 
 
 
STLS 
Figure 4. Mean Estimation Errors for Different Algorithms 
 
 
 
 
STLS 
Figure 5. Error Covariances for Different Algorithms 
 
10 
STLS 
STLS 
 
Figure 8. Estimated Errors in x-, y-, z-directions 
 
STLS 
STLS 
Figure 9.  Expectation Values of Estimated Errors 
 
 
 
12 
  
Figure 12. Flight Trajectory in ECEF Coordinate 
 
 
 
Figure 13. Initial Trajectory and Predicted Trajectory in ECEF Coordinate Using WTLS Method 
14 
