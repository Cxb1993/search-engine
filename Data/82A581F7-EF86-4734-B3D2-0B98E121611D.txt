instructions are also transmitted to the smart phone 
and control the robot using the bluetooth 
transmission. In addition, the video data are 
transmitted to the high performance servers or PCs 
for the further processing. Pedestrians are detected 
for the intelligent functions. Using the proposed 
system, we wish to be able to provide a low-cost 
robot and the intelligent functions for the 
home/office surveillance. 
英文關鍵詞： autonomous robot, pedestrian detection, video 
surveillance, WiFi location, smart phone 
 
 
2 
功能，擷取、記錄、通報監控物件以提高辦公或居家安全。 
綜觀現有保全自走車，大多僅能偵測與追蹤移動物件，但其美中不足的之處，有下列幾項：(1)電
力問題：自走車大多採用單機作業，上面配備個人電腦以便可以直接攫取影像，直接進行分析與辨識，
然其需要大量電力，以維持其順利運作。(2) 定位問題：其驅動方式使用步進馬達方式，計算其行走距
離以估測其概略定位，若輔以利用周遭的地形地貌進行精確定位，不但需要大量計算，並導致行進速
度緩慢。(3) 體型龐大且造價昂貴：每一台自走車將所有設備集於一身，其造價超過 40 萬元以上，不
是一般小型企業能夠負擔的系統。本研究計畫建構一個行動監控環境，系統架構概略說明如下(如圖一
所示)：在室內環境中，藉由行動自走車監控系統，可擴大監控範圍，將採用一般直流馬達驅動的小型，
且價格便宜的自走車上，裝載小型攝影機，並藉由無線網路方式，將影像訊號傳至後台的主機上進行
處理與判斷，再將控制指令透過藍芽傳輸傳給自走車，如此一來電力與計算問題可以同時解決。此外，
定位問題則先利用 WiFi 訊號進行初步定位，由於 WiFi 定位系統傳輸資料很少，於極短時間內傳輸並
概略計算出自走車的位置，雖然 WiFi 的定位精準度有限，其誤差往往超過一公尺以上，但能紀錄事件
發生的大概地點，將巡邏時所遇到的人、事與物，後傳到伺服器上加以存證，同時進行發現可疑人物
時發出警訊，此一設計理念，發展小型保全自走車，建立出符合人性、實用性的協同式監控系統，完
成室內環境人物視覺偵測、追蹤與辨識與無線網路定位/通報之目標。 
三、相關研究 
本計畫兩核心技術為「自走車導航」與「物件偵測」，以下探討此兩個核心技術相關研究文獻： 
1. 物件偵測 
近年來，圖型識別相關研究的發展，讓目標物偵測研究上，已發表許多成功的偵測文獻，人臉偵
測可使用 Adaboost 演算法，取得快速且不錯的效果，在人形偵測上[1-5]，遇到的困難有很多，像
是行人有各種不同的姿態、所穿著衣服顏色變化大、光線照射形成的陰影變化、行人遮蔽、行人
大小比例變化等，而行人偵測現行的演算法有下列幾種： 
(1) 使用 Haar 小波來描述行人的特徵，並使用多項式支援向量機器來學習行人分類器 
Constantine Papageorgiou[6]等人採用2D小波轉換(Wavelet transform)的方式取得簡單的人行輪廓
邊緣資訊，結合 3 種不同方向的濾波器：水平、垂直和對角線進行物件邊緣特徵的擷取，將擷
取到的特徵結合 SVM(Support Vector Machine)演算法做訓練及分類針對街道上的行人進行偵
測。Viola 與 Jones[9]提出利用矩形 Haar 特徵搭配上 Adaboost 演算法偵測人臉，此特徵擷取的
方法及分類器也被應用於行人偵測，由於行人的形狀變異程度大且多樣性，使得此方法偵測的
正確率並無偵測人臉時來的高，針對此一問題，他們加上了行人運動(motion)的資訊[10]，先計
算前後張影像在對不同方向做位移後的差值影像，再用距形特徵來抽取出行人運動的資訊，提
升偵測的正確率。 
(2) 利用邊緣(Edge)來做人型偵測 
Broggi 等人[7]在 ARGO 汽車上，建立一個人形偵測器，為加快其效能，其利用行人之外形(頭
形)比對，可以快速找到人形位置。Gavrila 與 Philomin [8]等人相同的利用邊緣資訊來做行人偵
測，其首先找出輸入影像上的邊緣位置，再計算出該影像上所有像素與影像上邊緣的距離轉換
(Distance Transform)影像，其轉換方法為計算每一個像素與最近影像邊緣像素的距離，接下來藉
由比對資料庫中行人的邊緣圖與經距離轉換後的影像算出其斜面距離(Chamfer Distance)，最後
 
4 
域巡邏時隨時偵測、追蹤與紀錄來往的行人資料，並與人臉辨識技術進行整合，確保監控環境之安全。 
四、成果 
本計畫開發以視覺為基礎，以自走車為載台的行動監控系統，其環境設備設計如下：首先系統架
設實驗平台，將智慧型手架設自走車上(如圖二)，智慧型手機可經由WiFi無線網路回傳影像作為攝影機
及定位的功能，且實驗場景為台北科技大學綜合科館四樓大廳(如圖三)，我們希望此實驗能在自走車巡
邏時從手機上回傳行進中的影像及位置資訊到後端PC上供程式處理，進行可疑人物偵測與定位，伺服
器中的PHP程式，儲存管理各項資訊，建立網頁式介面，供日後查詢用。 
(A) WiFi 無線定位模組 
智慧型手機上的WiFi定位是利用設備中的WiFi Sensor取得附近無線基地台訊號強度資訊做為特徵
向量並且與已訓練的定位訊號模組進行比對，如圖三所示，因人持有手機所在的位置不同所取得附近
無線基地台訊號強度會而有所改變，利用訊號的改變來判別自走車與智慧型手機的位置。其運作模式
可以分為訓練階段與判斷階段。此一技術是延續工研院資通所功能，針對位置定位之精準度進行改善。 
定位模組的訓練階段，先針對可無線網路訊號涵蓋的範圍內設定幾個固定位置的地標點，事先在
不同參考點(reference points)利用感測器接收附近不同無線基地台的訊號，因為感測器與基地台距離遠
近的關係，可以蒐集到不同強弱的訊號，感測器越接近某個基地台，則訊號資料數據上呈現較強的反
應；反之，則得到較弱的訊號資料數據。因此透過不同參考點(如圖三紅點所示)位置可以得到與各個無
線基地台之間的訊號資料差異性，以此當做訓練樣本的特徵，我們再利用高斯混合模型(Gaussian 
Mixture Model，簡稱GMM)演算法進行訊號特徵樣本的訓練及分群。 
使用GMM演算法目的為，若在相同收集點位置，附近的AP訊號強度並不是一直呈現穩定狀態，
所以造成收集的訓練資料分佈不是一個完整的高斯分佈狀態，因此利用GMM可以用許多Component來
Model不同資料分佈，最後保留個個參考點的GMM資料分佈資訊。在WiFi定位比對部分，當年智慧型
手機傳回所在位置附近AP訊號資訊後，伺服器即可與訓練好的GMM模組比對，比對後可以知道自走
車位於哪個參考點附近。 
使用高斯混合模型於每個參考點辨識，對不同參考點之資料需分別建立其高斯混合模型，一般傳
統上高斯混合模型之參數預測是使用EM (Expectation Maximization)演算法。此演算法主要是用來預測
高斯混合模型中多變數機率分佈函數之參數值，其目的是要找到最佳之參數Θ使得 )|X( Θp 最大，其
中 { }Nttx ,...,2,1),(X == 為 訓 練 資 料 之 特 徵 向 量 集 合 ， N 為 訓 練 資 料 之 數 目 ；
{ }Mrp rrr ,...,2,1|,),( =∑≡Θ μθ ，  )( rp θ 為在高斯混合模型中第 r 個高斯分佈之事前機率 (Prior 
Probability)， rμ  為平均值向量， r∑ 為共變異數矩陣(Covariance matrix)，M 為高斯混合模型中高斯分
佈之群數。EM演算法之詳細步驟如下： 
步驟1: 執行K-Means演算法 
首先，依據高斯混合模型中所指定之高斯分佈群數執行K-Means演算法分群，以每群之平均值向量
作為每個高斯分佈之平均值向量之初始值，且將共變異數矩陣之初始值設為單位矩陣。 
步驟2: Expectation-Step 
對所有資料(特徵向量之維度為( d )計算其屬於高斯混合模型中每一高斯分佈之機率比值作為預測
值，其公式如下： 
 
6 
cell 內所有像素分別對其所屬的方向 bin 做投票，這九個方向 bin 的資訊可用九維的向量來代表，將四
個相鄰 cell 視為一個 block，不同 block 間可相互重疊，block 用其內 4 個 cell 的方向 bin 來描述訓練影
像在該位置的局部邊緣資訊，可以 36 維向量來代表，該 36 維向量經正規化(Normalize)使向量長度為 1，
將所有 7x15 個 block 的向量(36 維)組合起來可得到 3780 維的向量，該向量包含了行人整體與局部的資
訊，供訓練的正樣本與負樣本影像大小若為 64×128，利用 HOG 特徵對所有行人訓練影像與非行人訓
練影像做特徵抽取後，可得到許多正樣本(行人)與負樣本(非行人)的資料(均為 3780 維的向量)，至於在
偵測器的學習上，採用 LSVM(linear support vector machine)的分類器，在資料維度(3780 維)上學習將正
例與反例分最開的超平面(hyperplane)做為判斷是否為人型的依據，人形偵測結果如圖五所示。 
 
(C) 自動避障模組 
自走車利用燒錄在車內的控制程式，藉由藍芽傳輸，接收由PC端所傳送的控制指令，達到自動行
駛的效果，而PC端之控制介面所寫入自走車路線控制的功能，則由WiFi定位系統，隨時監控自走車位
置，再由控制介面之程式決定自走車行進路線，達到自走車路線規劃、自動避障等功能。自走車以WiFi
定位座標為依據，依照輸入的目的地座標規劃路線，從起點開始與目的地畫一條直線，一開始假定自
走車可以直接抵達目的地，先行進五秒再由WiFi定位確定其座標，判斷自走車是否有依照直線槽目標
物前進，若為是，則繼續前進等待下個五秒再行判斷，若為否，則判斷其前進方向之夾角與原本的直
線角度為何，在將自走車車頭依其角度轉為對準目的地，再繼續前進直到自走車抵達目的地，如圖六
所示。 
自走車的紅外線感測器可以感測前方是否有障礙物？透過紅外線感測器，自走車可以探測到前方
和左右兩側 10 ~ 80 mm以內的障礙，就像是自走車上的一對「眼睛」，如何有效使用自走車的「眼睛」
達到閃避障礙物的效果？當自走車前方沒有障礙物時讓自走車前進，當自走車前方有障礙物時，自走
車停止動作，且調整角度，直到紅外線感測器未發現障礙物，然後繼續前進。 
另一感測器為碰撞感測器，是一種讓自走車由感知碰撞環上的碰撞資訊能力。在自走車的前、左
前、右前設置有三個碰撞開關，碰撞環與底盤柔性連接，它們與碰撞環共同構成了碰撞感測器，受力
後與底盤產生相對位移，觸發底盤上相應碰撞開關，使之閉合，自走車的下方設計了一個碰撞系統，
保證自走車的正常活動，且若障礙物低於紅外線感測器所能偵測之有效高度時，碰撞感測器就能發揮
偵測障礙物的功能。自走車的碰撞機構能夠檢測到來自前後各 120° 範圍內物體的碰撞，使自走車遭
遇到來自不同方向的碰撞後，能夠轉彎避開並保持正常活動，前後的碰撞系統分別由一個被彈性固定
到自走車主體上的半環狀金屬片和三個碰撞開關組成，來自不同方向的碰撞將使不同的碰撞開關閉
合，從而可以判斷出障礙物的方向。 
五、研究成果與後續工作 
本次計畫藉由低成本自走車上架設智慧型手機，取代了原本造價昂貴的步計馬達自走車，透過定
位可以達到類似的效果，本計畫在自走車行進時傳回影像，由後端偵測是否有可疑人物，並且記錄可
疑人物出現時的座標，進一步達成室內環境的安全監控，對於安全的品質與保全的效果可以說是有顯
著的幫助。未來可以增加目標物的追蹤，以達到降低可能發生的危害，使其得到為安全的監控環境，
並且要預防可能會被直接攻擊的情況使得監控環境有更好的保護。 
 
8 
年六月 
[19] 林東儀，臉孔偵測與追蹤應用於居家醫療自走車，國立中央大學電機工程研究所碩士論文，中華
民國九十五年七月 
圖一:小型自走車保全系統 
 
 
圖二:自動車設備 
 
 
10 
 
(a) (b) 
 
(c) (d) 
圖五: 人形偵測結果 
 
 1
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                    日期：100 年 10 月 27 日 
一、參加會議經過 
2011 ICCST國際學術研討會於 2011年 10月 18日至 10月 21日在西班牙巴塞隆納的Tecnocampus 
Mataró Maresme University 舉行，本人於該會議中除發表論文外，並聆聽多場專題演講，獲益匪淺。
此次參加會議歷程如下； 
2011/10/16：搭乘中華航空 CI 65 班機前往阿姆斯特丹轉搭荷蘭航空 KL 1673 班機前往巴塞隆納機
場。並於當地時間 10/17 號下午 15:00 抵達巴塞隆納機場，隨後前往預定飯店 check in。 
2011/10/18：至會場報到並參與開幕晚會，大會主席除了歡迎與會長官及學者的蒞臨，並介紹各國
及西班牙與國土安全相關的政府官員，希望在這為期四天的研討會中，能夠相互交流
學習。 
2011/10/19：上午 9:00 開幕儀式吸引相當多學者到場，首先由大會主席 Gordon Thomas 介紹該研討
會的歷史及其宗旨。本日共有 6 場 Session，本人選擇參加 Biometrics 和 Biometrics II
兩場，此兩場 session 講者主要介紹如何將生物認證機制應用於機場保安及大城市安全
監控系統。 
2011/10/20： Keynote speech 由西班牙國家關鍵基礎設施保護(CNPIC)主任 Fernando J. Sánchez 
Gómez 主講西班牙國土安全防護相關議題 The Spanish model” Chairperson: Marcos 
Faundez-Zanuy。CNPIC 參與超過 80~100 個國家重要基礎設施，做為官方與地方政府
之間的溝通橋梁，並以建構國家安全為最終目標。因此該部門的主要宗旨為預防任何蓄
意攻擊恐怖事件活動。 
2011/10/21： Session 12: Applications IV 發表論文，題目為 The color identification of automobiles for 
計畫編
號 
NSC－100－2221－E－239－035 
計畫名
稱 
室內環境安全監控雙網 
出國人
員姓名 韓欽銓 
服務機
構及職
稱 
國立聯合大學資訊工程學系教授 
會 議 時
間 
自民國 100 年 10
月 18 日至 100 年
10 月 21 日 
會 議 地
點 西班牙 巴塞隆納 
會議名
稱 
(中文) 2011 年國際卡拉漢安全技術研討會 
(英文) 45th IEEE INTERNATIONAL CARNAHAN CONFERENCE 
ON SECURITY TECHNOLOGY(ICCST) 2011 
發表論
文題目 
(中文)視訊監控系統中，車輛顏色分類 
(英文) The color identification of automobiles for video surveillance 
 3
六、其他 
1. 活動照片 
 
 
 
 
 The Color Identification of Automobiles for Video Surveillance 
 
Yu-Chen Wang1, Chen-Ta Hsieh1, and Chin-Chuan Han2, Kuo-Chin Fan1 
1Department of Computer Science and Information Engineering, 
National Central University, Taoyuan, Taiwan 
2Department of Computer Science and Information Engineering, 
National United University, Miaoli, Taiwan 
 
 
Abstract – Color identification of automobiles plays a significant 
in intelligent transportation systems (ITS). In this paper, a novel 
scheme for color identification of automobile is proposed using the 
taillight detection and a template matching module. The taillights of 
cars are detected to find the valid regions of interested (ROIs) for 
color identification. The color feature vectors generated by 3 by 3 
neighboring pixels are classified by a template matching strategy. 
Seven classes, red, yellow, blue, green, black, white, and gray, are 
identified in this work. Experimental results have been conducted to 
show the validity of the proposed method. The averaged accuracy 
rate 81.71% is achieved and the performance of this scheme is up to 
20 frames per second.  
 
 
Index Terms — Intelligent transportation system, region of 
interest, color identification of automobiles, taillight detection 
algorithm, color template matching algorithm. 
 
 
I.  INTRODUCTION 
 
Recently, the automobile color identification is increasingly 
demanded on urban roads for video surveillance. When an accident 
occurs, license plate (LP) numbers are the directly cues of escape 
cars. However, they are not the effective cues due to the missed and 
small LPs which are caused from the view angle and distance of 
witnesses. Witnesses only remember the automobile colors. The 
color identification from video data can assist police agencies in the 
crime prevention and the later investigation of crime events. 
Moreover, the government widely set up cameras on the road for 
traffic monitor or crime prevention. How to extract color of 
automobile effectively and also taking color as a metadata of other 
feature recognition is a hot issue of current research. 
Early, color feature descriptors have widely used in content 
based image retrieval (CBIR)[1-3]. High quality images with less 
illumination impact in their works. However, the color identification 
of automobiles in outdoor is susceptible to the camera design and 
environmental factors. First, the sequential images captured from the 
outdoor cameras are distorted by chromatic polarization and white 
balance. Second, the performance of identification is impacted by the 
illumination and weather in outdoor. Cameras capture the color 
features from the front, side, or back views of automobiles. 
Whichever views are captured, the over-exposure regions in images 
are always generated due to the sun light. As illustrated in Fig. 1, the 
classification errors always occur. Baek et al. [4] extracted the two-
dimensional histogram on the hue and saturation features. A multi-
class support vector machine (SVM)-based classifier was trained to 
identify color of automobiles. Tsia et al.[5] proposed a color model 
to reduce the illumination impact in outdoors. After the 
transformation on color spaces, a Bayesian classifier is used to 
identify the vehicle color and background. Chen et al. [6] applied the 
PCA-based technique to calculate the eigenvectors and the 
corresponding eigenvalues from the training samples represented in 
the RGB color space. The color pixels in space RGB are transformed 
to the new color space. The multiple instance learning is used to 
classify color. Brown[7] has evaluated four color feature descriptors, 
including standard color histograms, weighted color histograms, 
variable bin size color histograms and color correlograms, to identify 
vehicle colors for surveillance. Stokman et al.[8] proposed a fusion 
scheme to combine several color models. The selected and weighted 
color model is constructed for the detection of discriminatory and 
robust color features. Dule et al.[9] manually assigned the regions of 
interested (ROIs) on car hoods from the detected foreground objects. 
The color features in the half ROI are classified to determine the 
vehicle’s color. Practically, automatically finding the ROI is the 
critical issue in surveillance systems. Wu et al.[10] found the ROIs 
by integrating the results of background subtraction and those of 
color segmentation. An SVM-based classifier in a two-layer 
structure is then applied to classify the pixels in ROI. Li et al.[11] 
proposed a classification method using the template matching 
strategy. Before identification process, a color calibration procedure 
is executed to adjust the image colors [12]. The HSI color space is 
selected and the relative error distances are calculated to identify the 
automobile colors.  
 
 
(a) (b) 
 
(c) (d) 
Figure 1: The over-exposure regions (red regions). (a)-(c): the frontal 
views, and (d)-(f) the back views. 
 
In this paper, a novel scheme is proposed to identify the 
automobile colors in outdoors as shown in Fig. 2. The identification 
process consists of two modules: The location of a valid ROI and the 
classification using color features. Unlike the traditional background 
subtraction methods which are very sensitive to illumination changes, 
a taillight detection algorithm is proposed to obtain the valid ROI 
  
(a) (b) 
 
(c) (d) 
Figure 4: (a) an original image, (b) the detected red regions, (c) a pair 
of taillights, and (d)the valid ROI. 
 
Many color spaces (e.g. RGB, HSV, YCrCb, CIELab...etc) are 
explored for object segmentation or classification using color 
features. In this study, color space CIELab model is adopted for 
representing car colors. From the previous studies, the colors in 
space CIELab are very close to human visual aspects. Due to the 
illumination of sun light, robust features should be extracted to 
reduce the noises and lighting impacts. The feature vectors of length 
27 are generated from the 3 by 3 neighboring pixels in space CIELab. 
They are classified with the trained prototypes by the template 
matching rule. Each feature vector is represented as  
nkikikikiki
vvvvv ,........,,,
321
 , that subscript i denotes the ith 
template sample, subscript k represents the kth color index, and n is 
the vector dimension, 27 in this work. As to the parameters of 
template matching, k nearest neighbor (K-NN) strategy is performed 
to find the best matched prototype. In addition, the metric of 
Euclidean distance is adopted to assess the similarity of color 
features as defined as:  
 
  21
1
2
, 



 
n
m
jkijki
ij
k mm
vvvvd
.
 (4)
 
Here symbol kiv  denotes the color features of the kth trained 
samples, and symbol jv is the feature vectors in ROI. Using the 
results in section 3.A, the valid ROI is obtained by using taillight 
detection algorithm. The feature vector for each pixel in ROI is 
extracted and matching with the trained prototypes. The similarity is 
defined as the inversed Euclidean distance. The color of each pixel in 
ROI is classified by EQ. (5):  
  jkiijkkk vvdC ,minarg , (5)
 
where Ck denotes the color prototype index. Finally, the distances 
from ROI to seven prototypes are calculated. The statistics data 
determine vehicle in which the color of the valid ROI area with 
highest percentage, and defines this color for the automobile. In this 
study, seven colors’ prototypes, e.g., black, gray, white, red, yellow, 
green and blue, are collected which frequently appear in city roads. 
 
III.  EXPERIMENTAL RESULTS 
 
In this section, some experiments are conducted to show the 
effectiveness of the proposed method. A stationary CCD camera was 
installed in outdoors. Six video clips are captured in 30 f/s in various 
sunny, cloudy, and rainy days. 8319 image frames of size 320 by 240 
were extracted from the video clips, and there are 3654 image frames 
possess the automobiles. Several illustrated images used in the 
experiments are shown in Fig. 5. The identification system is 
implemented in a PC-based platform and is developed by the 
Microsoft visual C++ 2008 and Opencv 2.1 tool kits.  
The taillights are first detected and a valid ROI is determined. The 
color features in ROI are extracted and matched with the trained 
prototypes by using the template matching strategy. The detection 
rates as shown in Table I are very high. In addition, 3579 taillight 
pairs are correctly paired from 3654 pair of taillights whose ground-
truths are manually assigned. The averaged detection rate for 
taillights is more than 98%. The proposed method could correctly 
detect the taillight regions from videos. The taillight pairs are 
verified by the geometric rules to generate the valid ROI, and the 
color features in ROIs are extracted to classify the vehicle colors. 
The automobile colors are determined by the dominative colors in a 
valid ROI area. The template matching algorithm considers the 
neighboring pixels to reduce the noise distortion. The window pixels 
of 3 by 3 are fused to generate a color feature of length 27 in CIELab 
color space. Since the cameras are installed in outdoors, the image 
quality of automobile colors is poor with much variation due to the 
illumination.  
In the experiments, the colors of ROIs are classified into seven 
classes using the template matching algorithm, e.g. color red, yellow, 
blue, green, black, white and gray. Several correct classified results 
are shown in Fig. 6. Each illustration correctly finds the valid ROI 
and identifies the correct color. The classification rates for six video 
clips are tabulated in Table II. The averaged accuracy rate is 81.71%. 
In addition, the identification performance in the proposed method is 
up to 20 frames per second. On the contrary, two mis-classification 
results are shown in Fig. 7. The errors are resulted from the invalid 
ROI. Invalid ROIs will generate the incorrect features. The locations 
of taillights for a van and a truck are different from those of a car. 
The truck taillights are located at the lower parts, and the windshield 
of a van is located between two taillights.  
 
TABLE I 
The detection rates of taillight regions. 
Video clips Match taillight pairs 
Number of total 
pair taillights detection rates 
1 450 455 0.989 
2 530 557 0.952 
3 432 457 0.945 
4 166 167 0.994 
5 981 996 0.985 
6 1020 1022 0.998 
Average 
Accuracy rate   0.98 
 
 
 consists of two modules: The location of a valid ROI and the 
classification using color features. The taillights are detected and a 
valid ROI is generated. The pixels in ROI are classified to determine 
the color of cars using color features. The neighboring pixels are 
considered to effectively reduce the illumination impacts and noises. 
From the conducted experimental results, the high performance of 
the proposed method is achieved. In the future works, the shapes of 
cars will be identified to correctly locate the valid ROI. The ROIs 
from the frontal view or the side view will be extracted for car color 
identification. In addition, more image processing algorithms will be 
designed to overcome the over-exposure problem. 
 
 
V.  ACKNOWLEDGEMENTS 
 
The work was supported by the National Science Council 
under grant no. NSC 99-2221-E-239 -030, and by the 
Technology Development Program for Academia of DOIT, 
MOEA, Taiwan under grant no. 99-EC-17-A-02-S1-032. 
 
 
VI.  REFERENCES  
 
[1]. I. K. Park, I. D. Yun and S.U. Lee “Color image retrieval 
using hybrid graph representation,” Image and Vision 
Computing, vol. 17, pp. 465-474, 1999. 
[2]. L. Cinque, G. Ciocca, S. Levialdi, A. Pellicanò and R. 
Schettini, “Color based image retrieval using spatial 
chromatic histograms,” Image and Vision Computing, vol. 19, 
pp. 979-986, 2001. 
[3]. G. Qiu, “Embedded colour image coding for content-based 
retrieval,” Journal of Visual Communication and Image 
Representation, vol. 15, pp. 507-521, 2004. 
[4]. N. Baek, S. M. Park, K. J. Kim, and S. B. Park, “Vehicle 
color classification based on the support vector machine 
method,” Communications in Computer and Information 
Science, vol. 2, pp.1133-1139, 2007. 
[5]. L. W. Tsai, J. W. Hsieh and K. C. Fan, “Vehicle detection 
using normalized color and edge map,” IEEE Transactions on 
Image Processing, vol. 16, pp.850-864, 2007. 
[6]. S. Y. Chen, J.W. Hsieh, J. C. Wu and Y. S. Chen, “Vehicle 
retrieval using eigen color and multiple instance learning,” 
International Conference on Intelligent Information Hiding 
and Multimedia Signal Processing, pp.657-660, 2009. 
[7]. L. M. Brown, “Example-based color vehicle retrieval for 
surveillance,” IEEE International Conference on Advanced 
Video and Signal Based Surveillance, pp.91-96, 2010. 
[8]. H. Stokman and T. Gevers, "Selection and fusion of color 
models for image feature detection," IEEE Transactions on 
Pattern Analysis and Machine Intelligence, vol. 29, pp. 371-
381, 2007. 
[9]. E. Dule, M. Gokmen and M.S. Beratoglu, “A convenient 
feature vector construction for vehicle color recognition,” 
Proceedings of the 11th WSEAS international conference on 
nural networks, evolutionary computing and Fuzzy systems, 
2010. 
[10]. Y. T. Wu, J. H. Kao, and M. Y. Shih, “A vehicle color 
classification method for video surveillance system 
concerning model-based background subtraction,” Pacific-
Rim Conference on Multimedia, vol. 6297, pp. 369-380, 2010. 
[11]. X. Li, G. Zhang, J. Fang, J. Wu and Z. Cui, “Vehicle color 
recognition using vector matching of template,” International 
Symposium on Electronic Commerce and Security, pp.189-
193, 2010. 
[12]. G. D. Finlayson, B. Schiele, J.L. Crowley, “Comprehensive 
color image normalization,” Proceedings of 5th European 
Conference on Computer Vision, vol. 1, pp. 475-490, June 
1998. 
[13]. Y. Y. Lu, C. C. Han, M. C. Lu and K. C. Fan, “A vision-
based system for the prevention of car collisions at night,” 
Machine Vision and Applications, vol. 22, pp. 117-127, 2011. 
 
 
VIII.  VITA 
 
Yu-Chen Wang was born in Taichung, Taiwan, R.O.C., in 1984. 
He received the B. S. degree in Department of Computer Science 
and Information Engineering from Min-Dao University, Changhua, 
Taiwan, in 2005. He received M. S. degree in the Department of 
Computer Science and Information Engine from Chung Hua 
University, Hsinchu, Taiwan, in 2008. He is currently pursuing the 
Ph.D degree in computer science at the National Central University. 
His research interests include pattern recognition, computer vision, 
and machine learning. 
Cheng-Ta Hsieh was born in Taipei, Taiwan, R.O.C., in 1983. 
He received the B. S. and M. S. degree in Department of Computer 
Science and Information Engineering from Chung Hua University, 
Hsinchu, Taiwan, Republic of China in 2005. He is currently 
pursuing the Ph. D degree in computer science at the National 
Central University. His research interests include pattern recognition, 
computer vision, and machine learning. 
Chin-Chuan Han received the BS degree in computer 
engineering from National Chiao-Tung University in 1989, and the 
MS and PhD degrees in computer science and electronic engineering 
from National Central University in 1991 and 1994, respectively. 
From 1995 to 1998, he was a postdoctoral fellow in the Institute of 
Information Science, Academia Sinica, Taipei, Taiwan. He was an 
assistant research fellow in the Telecommunication Laboratories, 
Chunghwa Telecom Co. in 1999. From 2000 to 2004, he worked 
with the Department of Computer Science and Information 
Engineering, Chung Hua University, Taiwan. In 2004, he joined the 
Department of Computer Science and Information Engineering, 
National United University, Taiwan, where he became a professor in 
2007. He is a member of the IEEE, the SPIE, and the IPPR in 
Taiwan. His research interests are in the areas of face recognition, 
biometrics authentication, video surveillance, image analysis, 
computer vision, and pattern recognition. 
Kuo-Chin Fan received the BS degree in electrical engineering 
from the National Tsing-Hua University, Hsinchu, in 1981, and the 
MS and PhD degrees from the University of Florida (UF), 
Gainesville, in 1985 and 1989, respectively. In 1983, he joined the 
Electronic Research and Service Organization (ERSO), Taiwan, as a 
computer engineer. From 1984 to 1989, he was a research assistant 
with the Center for Information Research at UF. In 1989, he joined 
the Institute of Computer Science and Information Engineering, 
National Central University, Chung-Li, Taiwan, where he became a 
professor in 1994, and from 1994 to 1997, he was the chairman of 
the department. Currently, he is the director of the Computer Center. 
His current research interests include image analysis, optical 
character recognition, and document analysis. He is a member of the 
SPIE. 
 
100年度專題研究計畫研究成果彙整表 
計畫主持人：韓欽銓 計畫編號：100-2221-E-239-035- 
計畫名稱：室內環境安全監控雙網 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 2 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
