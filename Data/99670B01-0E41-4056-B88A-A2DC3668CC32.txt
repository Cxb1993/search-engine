一、中文摘要 
本報告提供在室內環境中的機器人一
套完整且低成本的定位方法，其整合了聲場
特徵定位法以及光流影像定位法。聲場特徵
定位法是由個人電腦端的雙聲道麥克風收
錄機器人在不同位置所發出的聲音資訊，並
依據各位置上收錄聲音的相位差以及大小
比分佈，預先建立高斯混合聲場地標模型；
使機器人在模型環境中移動並發出聲音
時，可藉由將新的聲音資訊代入模型中運
算，以偵測機器人所在的絕對位置。而光流
影像定位法是由兩個光流感測器對地面偵
測各別的位移資訊，並加以演算合併以求得
機器人的相對位移資訊。在此更進一步地利
用機率格方法整合以上兩種定位資訊，以提
升絕對位置定位的可靠度。 
 
Abstract 
This report proposes an integrated and 
low-cost method of localization, which can 
estimate the absolute location of a mobile 
robot in a complex indoor environment by 
using sound field characteristics and optical 
flow sensing. The first method, Localization 
using sound field characteristics, record the 
sound information by two channel 
microphone of personal computer, and 
establish Gaussian mixture-sound field 
landmark model according the phase 
difference and magnitude ratio distributions 
between two microphone at each landmark. 
When the robot moves and generates sound in 
the modeled environment, we can bring the 
new sound information into the landmark 
models to detect it’s location. The second 
method, Localization using optical flow 
sensing, use two optical flow sensors to 
estimate the relative movement information of 
the mobile robot. Moreover, we utilize the 
position probability grids method to integrate 
these two methods for improving the 
reliability of estimating the absolute location. 
Key Word: Robot, Microphone Array, 
Optical flow sensing. 
二、研究目的與參考文獻 
Localizing a mobile robot in an indoor 
environment is an important issue in the field 
of robotics. To provide reliable solutions, 
various equipments and sensors, including 
camera, radio frequency identification (RFID), 
infrared red (IR), ultra sonic sensor, laser 
range scanner, wireless LAN and inertial 
navigation sensor, have been adopted [1-8]. 
Among these methods, optical flow sensing 
based method, which can perform relative 
localization, was proposed for robot 
localization applications [9-10]. Because the 
optical flow sensor is placed underneath the 
robot and faces the ground, the measurement 
would be less influenced by the change of 
environmental illumination. Although it is not 
influenced by wheel slip like the rotary 
encoder, it still suffers from errors such as 
sudden change of ground height. On the other 
hand, some researchers utilize audible ranged 
sound devices, like microphones and speakers, 
to perform robot localization [11-12]. One of 
the major reasons is that these devices are 
becoming general equipment on the indoor 
robots to provide a more nature 
communication interface between robots and 
human. 
The methods of using multiple microphones 
to localize sound sources have been developed 
for a long time. Various methods, such as 
steer-beamformer based methods, 
eigenstructure based methods, and generalized 
cross-correlation (GCC) based methods were 
proposed [13]. Among these methods, GCC 
based methods [14-18] were used for robot 
localization application [11]. The major 
difficulty of using sound wave to localize 
 Fig. 1. Overall system architecture. 
The proposed algorithm is as follows: 
A. Sound Field Characteristics Based 
Localization Method 
I. Description of the Proposed Robot 
Localization Model Using Sound Field 
Characteristics 
To establish the GM-SFLM, the SFCBLA 
needs to construct models for the sound 
source at different locations. ( )bxP ω  and 
( )bxM ω  denote the phase difference and 
magnitude ratio respectively for constructing 
GM-SFLM  at frequency bω , { }Bb ,...,1∈ . The 
GMMs are defined as the weighted sum of 1N  
and 2N  mixtures of Gaussian component 
densities shown below, 
( ) ( )xi
N
i
iPPx gG PP ∑
=
= 1
1
,| ρλ           (1) 
( ) ( )xiN
i
iMMx gG MM ∑
=
= 2
1
,| ρλ          (2) 
where 
( ) ( )[ ]TBxxx PP ωω L1=P , ( ) ( )[ ]TBxxx MM ωω L1=M , 
iP,ρ  and iM ,ρ  are the i-th mixture weights, 
and ( )xig P  and ( )xig M  are the Gaussian 
density function. Notably, the mixture weights 
must satisfy the constraints: 
1
1
1
, =∑
=
N
i
iSPρ  and  1
2
1
=∑
=
N
i
iM ,ρ         (3) 
The terms Pλ  and Mλ  represent the 
parameters of 1N  and 2N  component 
densities. 
{ }PPPP Σμλ ,,ρ=                  (4) 
{ }MMMM Σμλ ,,ρ=         (5) 
where  
1,1 ,P P P N
ρ ρ⎡ ⎤= ⎣ ⎦Lρ  denotes the phase 
difference mixture weight vector with 
dimensions 11 N× .  
2,1 ,M M M N
ρ ρ⎡ ⎤= ⎣ ⎦Lρ  denotes the magnitude 
ratio mixture weight vector with dimensions 
21 N× . 
1,1 ,P P P N
⎡ ⎤= ⎣ ⎦μ Lμ μ  denotes the phase 
difference mean matrix with dimensions 
1NB×  and 
( ) ( )[ ]TBiPiPiP ωμωμ ,1,, L=μ . 
2,1 ,M M M N
⎡ ⎤= ⎣ ⎦μ Lμ μ  denotes the magnitude 
ratio mean matrix with dimensions 2NB×  and 
( ) ( )[ ]TBiMiMiM ωμωμ ,1,, L=μ . 
1,1 ,P P P N
⎡ ⎤= ⎣ ⎦Σ Σ ΣL  denotes the phase 
difference covariance matrix with dimensions 
1BNB×  and 
( )
( )⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
=
BiP
iP
iP
ωσ
ωσ
2
,
1
2
,
,
00
00
00
OΣ  
for a blind search. Since the probability 
densities ( )Yp P  are the same for all location 
models, the detection rule can be recast as: 
( )( ) ( )( )∏∏
==
+=
K
k
M
k
YM
K
k
P
k
YPtotal lGlGlG
1
)(
1
)( ||)( λλ MP αα  
                                  (16) 
)(maxargˆ
1
lGl totalLl≤≤=                      (17)      
B. The Theory of Localization Using Optical 
Flow Sensing 
Two optical sensors are located at different 
locations to estimate the movement 
information of the robot [10]. The velocity 
equations can be written as 
xryyr
yrxxr
xryyr
yrxxr
dVV
dVV
dVV
dVV
,2,2,
,2,2,
,1,1,
,1,1,
⋅−=
⋅+=
⋅−=
⋅+=
ω
ω
ω
ω
                     (18) 
where 
xrV , and yrV ,  are the velocities of the 
robot center, rω is the angular velocity of the 
robot, 
xiV , and yiV ,  are the velocities of the i-th 
optical sensor and 
xid , and yid ,  are the 
projection of the distances from the sensor to 
the robot center in x-axis and y-axis, as shown 
in Fig. 2.  
 
Fig. 2. The configuration of the optical 
sensors and the robot. 
The least-squares method can be used to solve 
Eq.(18) and the result is shown in Eq.(19).  
1
1,1, 1, 1,
,
1,1, 1, 1,
,
2,2, 2, 2,
2,2, 2, 2,
1 0 1 0 1 0
0 1 0 1 0 1
1 0 1 0 1 0
0 1 0 1 0 1
xy y y
r x
yx x x
r y
xy y y
r
yx x x
Vd d d
V
Vd d d
V
Vd d d
Vd d d
ω
−Τ Τ⎧ ⎫⎡ ⎤− − − ⎡ ⎤⎡ ⎤ ⎡ ⎤ ⎡ ⎤⎪ ⎪⎡ ⎤ ⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎪ ⎪⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥=⎨ ⎬⎢ ⎥ ⎢ ⎥ ⎢⎢ ⎥ ⎢ ⎥ ⎢ ⎥− − −⎪ ⎪⎢ ⎥ ⎢ ⎥ ⎢⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎪ ⎪⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢⎢ ⎥⎣ ⎦ ⎣ ⎦ ⎣ ⎦ ⎣ ⎦⎣ ⎦⎩ ⎭
⎥⎥⎥    
                            (19) 
Hence, the location and orientation of the 
robot can be derived from Eq.(20). 
( )
( )
( )∫
∫
∫
+=
−=
=
dtVVY
dtVVX
dt
robotyrrobotxrrobot
robotyrrobotxrrobot
rrobot
)cos()sin(
)sin()cos(
,,
,,
θθ
θθ
ωθ
      (20) 
C. The Theory of Location Probability Grid 
A location probability grid is built to 
estimate the absolute location of the robot 
given the environment model and sensor 
readings [20]. Each time the sensors receive 
the new data, two steps are executed. 
(i). Update the location probability grid 
according to the movement of the robot. 
(ii). Combine the latest location probability 
grid with the probability stored last time. 
Two assumptions are made: 
(i). The environment model must be 
distributed in square form. 
(ii). The robot can not leave the 
environment model. 
(1) Integrate Multiple Sensor Readings 
To achieve a reliable location detection, 
information of consecutive sensor readings 
need to be integrated. Assume  
)|( 111 −− ∧∧ nn rrlp K is the posteriori probability 
where l  is the location and nr  is the n-th 
sensor reading, including 
nYnY ,, ,MP , and 
nrobotnrobotnrobot YX ,,, ,,θ . The following formula is 
used to update the posteriori probability: 
In this experiment, the robot is placed 
initially in an unknown location and is given 
an open-loop motion command (suppose to 
move in a straight line). During the motion, 
the system runs 14 times to find its absolute 
location in the grid map. Those results of the 
proposed system are compared with the results 
that use only SFCBLA. Fig. 5 shows the 
measured probability values for the first, 
second, seventh, and fourteenth step. The 
probability values measured by using only 
SFCBLA are depicted in the left side of Fig. 5 
and the probability values measured by LPGA 
are shown at the right side of Fig. 5. Since 180 
orientations are considered, the right side in 
Fig. 5 only depicts the measured probability 
values of the most possible orientation after 
14 steps. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
As illustrated in the left side of Fig. 5, the 
maximum measured probability values is not 
obvious in each step and the detection results 
may be confused with the locations with 
similar sound field characteristics when only 
SFCBLA is utilized. On the other hand, by 
observing the detection results from the LPGA, 
the localization results are becoming more 
focused with the increasing of the step number 
and the measured probability values of the 
locations far from the actual robot’s location 
are reduced. This is because the LPGA can 
employ the detection results from prior steps 
by the information of OFSBLA to emphasize 
the latest detection result.  
The detection results obtained at every step 
are depicted in Fig. 6. Note that the initial 
conditions of OFSBLA are assumed known 
for the sake of sketching the figure. Further, 
the robot does not move in a straight line due 
to wheel slip. The localization results using 
only SFCBLA, like S10, S11, S13, may be far 
away form actual location. If the robot is 
moving to a target, this kind of ambiguity will 
cause the robot to change its direction and 
path substantially. Nevertheless, this kind of 
ambiguity can be avoided in the localization 
results of LPGA, and the localization results 
are much more close to actual location at each 
step.  
 
 
 
 
 
 
 
    
    
    
    
Fig. 5. Location probability after 1, 2, 7 and 14 step. 
 
Fig. 6. Detection results. 
transform,” IEEE Signal Processing 
Letters, vol. 61, pp. 1497-1498, Oct. 1973. 
[17] Brandstein, M. S. and Silverman, H. F., 
“A robust method for speech signal 
time-delay estimation in reverberant 
rooms,” IEEE Int. Conf. on Acoustics, 
Speech, and Signal Processing, vol. 1, pp. 
375-378, April 1997. 
[18] Nikas C.L. and Shao M., Signal 
Processing with Alpha-Stable 
Distributions and Applications. New York: 
Wiley, 1995. 
[19] L. W. Wu, W. H. Liu, C. C. Cheng, and J. 
Hu, “Gaussian mixture sound field 
landmark model for robot localization,” 
Advanced Robotics,   to be published. 
[20] W. Burgard, D. Fox, D, Henning, and T. 
Schmidt, “Estimating the absolute position 
of a mobile robot using position 
probability grid,” Proc. 14th National 
Conf. on Artificial Intelligence, 1996. 
[21] D. A. Reynolds and R. C. Rose, “Robust 
text-independent speaker independent 
using Gaussian mixture speaker models,” 
IEEE Trans. Speech Audio Processing, vol. 
3, no. 1, pp. 72-83, Jan. 1995. 
[22] G. Xuan, W. Zhang, P. Chai, “EM 
algorithms of Gaussian mixture model and 
hidden Markov model,” in 2001 IEEE Int. 
Conf. Image Processing, pp. 145-148. 
[23] J. B. MacQueen, "Some methods for 
classification and analysis of multivariate 
observations”, in 1967 Proceedings of 5-th 
Berkeley Symposium on Mathematical 
Statistics and Probability, pp. 281-297. 
[24] C. Elkan, “Using the triangle inequality 
to accelerate k-means,” in 2003 Proc. of 
the Twentieth Int. Conf.  Machine 
Learning, pp. 147-153. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位
研發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
表 Y04 
報告內容應包括下列各項： 
一、參加會議經過 
 
本次大會的主題為 Ubiquitous Robotics，總共有來自 57 個國家，1818 篇論文投稿，其中
包含 1737 篇 contributed papers，50 篇 invited sessions 論文，以及 31 篇 long video 
contributions。其中平均每 6 篇投稿就有一篇有附 video。最後共有 787 篇技術論文做口
頭發表(錄取率約 44%)，以及 20 個 long videos。其中 146 篇附 video。這些論文共安排
168 個 session 發表，包含 2 個 video sessions。同時亦安排了 19 個 workshop 及 3 個
tutorials。此外，大會安排 3 個大會演講(plenary speech)： 
1. Plenary Talk I, Simplifying principles for perception, action, locomotion and 
navigation. A common problem for brains and robots, Prof. Alain Berthoz, Collège de 
France, Paris, FRANCE, Wednesday, 11 April 2007 - 8:20-9:10 
2. Plenary Talk II, Putting the Turing into Manufacturing: Automation Science and Recent 
Developments in Feeding and Fixturing, Prof. Ken Goldberg, University of California
Berkeley, USA, Thursday, 12 April 2007 - 9:40-10:30. 
3. Plenary Talk III, Programming-by-demonstration: From assembly-plan through 
dancing humanoid, Prof. Katsushi Ikeuchi, University of Tokyo, Tokyo, JAPAN, 
Friday, 13 April 2007 - 9:40-10:30 
 
最精采的莫過大會安排了一個 Open Forum，主題為 The future of Robot Operating System。
Industrial and publisher exhibits 則包含下列廠商： 
1. KUKA Roboter GmbH (GOLD PATRON), www.kuka.com  
2. Microsoft Research - Microsoft Robotics Studio (GOLD PATRON), 
research.microsoft.com, microsoft.com/robotics  
3. Sintesi ScpA (GOLD PATRON), www.sintesi-scpa.com  
4. BARRETT Technology Inc., www.barrett.com  
5. Bluebotics, www.bluebotics.com  
6. BRILL VSP Publishing, www.brill.nl  
7. Cambridge University Press, www.cambridge.org  
8. COMAU SpA, www.comau.com/robotics  
9. Cyberbotics, www.cyberbotics.com  
10. IEEE Robotics and Automation Society, www.ieee.org/ras  
11. IEEE, The Institute of Electrical Engineers, Inc., www.ieee.org  
12. KINEOCAM, www.kineocam.com  
13. K-TEAM SA, www.k-team.com  
14. MetraLabs GmbH, www.metralabs.com  
15. The MIT PRESS, mitpress.mit.edu  
16. Mobile Robots Inc., www.mobilerobots.com  
17. Museo di Rovereto, www.museocivico.rovereto.tn.it  
18. Nespresso, www.nespresso.com  
19. PAL Technology, www.pal-robotics.com  
20. QUANSER, www.quanser.com  
21. ROBOSOFT, www.robosoft.fr  
22. R-STATION, www.r-station.co.kr  
23. SAGE Publications, www.sagepub.co.uk  
24. SAGEM Defénse Securité, www.robotics-platform.eu.com  
25. Scuola di Robotica, www.scuoladirobotica.it  
26. Springer-Verlag GmbH, www.springer.com  
27. TECHNAID, isens.technaid.com  
28. TEORESI Srl, www.teoresi.it  
表 Y04 
 
平均準度可達 2cm。 
3. Compact Tactile Display for Fingertips with Multiple Vibrotactile Actuator and
Thermoelectric Module 
韓國針對觸覺發展的 sensor module 如下圖。 
 
4. Development of Multi-fingered Hand for Life-size Humanoid Robots 
日本 AIST 的 HRP-3 的手指系統如下圖， 
 
5. 955-fps Real-time Shape Measurement of a Moving/Deforming Object using 
High-speed Vision for Numerous-point Analysis 
東京大學石川正俊教授 (Professor Masatoshi Ishikawa, Tokyo University) Vision 
Chip for target tracking at 1 KHz 如下圖之說明： 
 
6. On the Evaluation of Emotion Expressing Robots 
 
 
