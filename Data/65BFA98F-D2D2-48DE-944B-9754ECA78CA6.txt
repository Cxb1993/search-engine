I 
 
目錄                                           
目錄…………………………………………………………………………………..I  
研究計畫中文摘要…………………………………………………………..……II 
研究計畫英文摘要…………………………………………………………..……II 
第一年報告……………………………………………………………………..……1 
第二年報告… … . … … … … … … … … … … … … … … … … … … … … … … 1 5 
第三年報告… … … … … … … … … … … … … … … … … … … … … … … … 2 8 
附錄 … … … … … … … … … … … … … … … … … … … … … … … … … … 6 0 
C G I  2 0 0 8  發 表 論 文 … … … … … … … … … … … … … … … … … 
C G I  2 0 0 9  發 表 論 文 … … … … … … … … … … … … … … … … … 
 
行政院國家科學委員會補助專題研究計畫 □成果報告 ■精簡報告 
 
角色動畫之自動摘要與可視化之研究 
 
 
計畫類別：■ 個別型計畫  □ 整合型計畫 
計畫編號：NSC 96－2221－E－033－073－MY3 
執行期間： 96 年 08 月 01 日至 99 年 07 月 31 日 
 
計畫主持人：楊熙年 
共同主持人： 
計畫參與人員： 黃科森、陳英祺、林威成 
 
 
成果報告類型(依經費核定清單規定繳交)：■精簡報告  □完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
□出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
          
執行單位：中原大學 
 
中   華   民   國  96 年  05 月   19 日
1
Recently, due to the success of motion 
capture technologies, large motion capture 
data become available. Therefore, generating 
a synopsis of motion data [7], [8], [9] has 
attracted a great deal of attention. Assa et al. 
[8] pointed out that the need to display human 
activity in still images is commonplace in 
many fields of modern life such in a 
newspaper, scientific journal, the printed 
media or digital libraries. 
 
2. Contents (研究報告內容) 
 
 
Figure1: The proposed approach consists of symbolization, 
identifying latent structure and grammar generation. 
 
We first introduce three components of the 
proposed approach. They are symbolization, 
latent structure identification and grammar 
generation (Figure 1). 
 
Symbolization of Motion Data 
1) Posture Extraction: A motion capture 
data is usually composed of global and local 
motion. The first includes the root joint 
translation and rotation, representing the 
relation between the character and the 
environment. The second, also referred as 
posture, includes the remaining joints degrees 
of freedom. In this project, we use the 
postures (a set of localized 3D positions) as 
our motion representation to perform the 
symbolization process. In other words, we 
ensure each posture within a symbol is 
similar. 
2) Motion Clustering: It is difficult to 
detect complex patterns in high dimensional 
data directly [10], even in cases with the use 
of other dimensionality reduction methods 
such as PCA or MDS. However, these 
methods mix the original feature components 
and this situation implies that we can not 
handle them directly. Therefore, we adopt 
k-means clustering [11] to lower the 
dimensionality of the input sequence. The 
feasibility of applying k-means clustering to 
motion capture data has been verified and had 
reasonable results [12], [13]. 
In our implementation, the users can give 
an error threshold to indicate the distance 
between the posture and the centers. And we 
can use the following three subsequent 
processes to perform kmeans clustering: (1) 
Given an error threshold R, we can obtain n 
clusters (symbols) using the k-means 
algorithm. We set the initial n = 2; (2) We 
compute the n distances from these clusters. 
If they are smaller than R, the clustering 
process is complete. (3) Otherwise, we set n = 
n + 1 and perform k-means clustering again. 
 
 
Figure 2: Symbolization of Motion Data. (a) shows the given 
sequence (top), the clustered and labeled sequence (middle) and the 
final motion string (bottom). (b) presents an example of symbolized 
walking sequence. 
 
3) Transformation of Motion Sequence into 
Label String: Fig. 2(a) shows the process of 
motion symbolization. Given a motion 
sequence (top of Fig. 2(a)), we first use 
k-means clustering to group similar postures 
and each posture is labeled by its cluster label 
(middle of Fig. 2(a)). Finally, we scan the 
labeled string and group them based on their 
label to obtain the final motion string (bottom 
of Fig. 2(a)). Fig. 2(b) presents an example of 
a walking sequence. 
 
 
3
quantify the meaningfulness of discovered 
patterns. In our case, the captured motions 
usually start with a special state and end 
with another one. These states are 
anticipatory action of the given motion 
[21]. That is, if all the used patterns start 
with a reasonable posture (frame) or 
sub-action, it suggests NRPs whose initial 
or final symbol is an anticipatory action 
are more reasonable. 
3) Detecting the Anticipatory Action: The 
main characteristic of anticipatory action is 
that there is a tendency that action usually 
occurs before (after) another action. And we 
also assume the input motion sequence is an 
action. Therefore, we define two rules to 
assist us in finding the anticipatory actions of 
a transformed motion string. 
z The first and last symbols of a 
transformed motion string are its 
anticipatory action. We name these two 
symbol as initial and final symbol 
respectively. 
z The preceding (subsequent) symbols of 
initial (final) symbol are another final 
(initial) symbol. We name both initial and 
final symbols as anticipatory symbols. 
4) Generating the Hierarchy and Order of 
Discovered Nontrivial Repeating Patterns: In 
order to use discovered patterns to generate 
hidden grammar, we have to construct the 
hierarchy and order of discovered NRPs. To 
do this, we first use the method presented by 
Hsu et al. [14] to obtain NRPs. Then we 
remove some unsuitable NRPs by checking 
for overlapping patterns according to the 
grammar production principle. The hierarchy 
construction is based on the length of NRP 
and the order is sorted with both the 
frequency principle and anticipatory action 
principle. More precisely, the result can be 
obtained through the following steps: 
z Hierarchy construction. The length of 
NRP is used as acriterion to obtain the 
layered non-trivial repeating pattern. 
Those patterns with equal length belong 
to the same level. 
z Sorting by pattern’s frequency. For each 
level, a stable sorting method1 is applied 
to the NRPs according to their frequencies. 
This is straightforward; we can reduce the 
size of the generated grammar if the 
patterns with high frequency are used to 
construct the grammar. 
z Sorting by anticipatory action principle. 
For each level, we sort the NRPs 
according to the property of its first and 
last symbol. There are three conditions, 
both first and last symbol are anticipatory 
symbols, one of them is an anticipatory 
symbol, and neither of them are 
anticipatory symbols. The first condition 
has the highest priority.  
To discriminate the patterns which have 
been evaluated by interestingness measures, 
we modify the definition of NRP to match 
four principles listed above and entitled 
constrained nontrivial repeating pattern 
(CNRP). 
Definition 3: X is a constrained non-trivial 
repeating pattern (CNRP) if and only if it 
satisfies the following two conditions: (a) It is 
still a non-trivial repeating pattern after the 
counts of overlap positions have been 
removed. (b) It has been sorted with length, 
frequency and anticipatory action principles. 
 
Grammar Generation 
In this project, we select the appropriate 
grammar by examining cost function and 
selecting the minimal cost grammar to avoid 
the ambiguity grammar [20], since given a 
motion string we can deduce several different 
grammars for it. We argue that the grammar 
size cost function of a grammar G, named S(G) 
should consider the total number of terminal 
occurrences to obtain a compact form of the 
original motion string. Therefore, we define 
the following principle for grammar 
extending. 
5
absolute spatial location. However, a given 
motion usually contains the motion 
information only when is there is no 
background or auxiliary scene. Therefore, the 
generated results of spatially expanded layout 
will be similar to the list of the selected 
keyframes directly (Figure 5). Using motion 
skimming, this problem can be handled well. 
 
Figure 6: A visual comparison of summary sequence generated by 
uniform picking (left) and our method (right). The bold-enhanced 
frames are the selected frames which used to form summary 
sequence. 
 
2) Motion Skimming: Analogous to the 
concept of video skimming [1], we also 
define two variations for motion skimming, 
summary sequence and highlight. Based on 
the definition of motion skimming (section 
I-B), we distinguish summary sequence and 
highlight according to its evaluating target 
(”posture” or ”subaction”).  
Summary sequence. In fact, motion 
summary sequence can be regarded as an 
extension of motion summary because the 
main difference between them is their 
presentation form. The goal of summary is to 
generate the representative images and that of 
summary sequence is to construct 
fast-forward video. To generate summary 
sequence, our approach also uses the 
discovered structure (grammar) to assist us to 
select some important frames. Given the 
desired compression ratio ”ϕ ” (this can also 
mean “ ϕ ” times fast-forward), we can 
calculate that how many frames are needed in 
each symbol of the interesting parts. As in 
Type II of Fig. 4, we apply the ”key probe” 
method [9] to each symbol for obtaining its 
keyframes. Fig. 6 shows the visual 
comparison of uniform sampling and our 
method for two fragments of a pugilism 
sequence. For the first fragment (Fig. 6(a)), 
uniform sampling will miss the secondary left 
hand straight” even though it has five selected 
keyframes. For the second fragment (Fig. 
6(b)), the uniform picking can not catch the 
second ”left hand raise” event.  
Motion highlight. For selecting motion 
highlights, we consider the statistical 
information to select sub-actions as highlights. 
Using such information, an important 
sub-action could be defined as the frequent or 
infrequent patterns in a given motion. For 
example, the frequent patterns in a dialogue 
usually imply that maybe it is someone’s 
nonverbal behavior. In contrast to frequent 
patterns, a sub-action should be an 
unexpected action due to its infrequency in 
some situations. In this project, we utilize the 
following two principles to determine which 
sub-sequences are highlights of the give 
sequence.  
z Unexpectedness. A pattern is interesting 
if it is ”surprising” to the user. Recently, 
this principle has been applied to time 
series data [25]. In our case, recall the two 
mentioned observations, the common 
characteristic is that “something” is 
repeating. In other words, the 
non-repeating sub-sequences may expose 
some important information. For example, 
it will be interesting that a cyclic motion 
has some non-repeating sub-sequences. 
z Constitutive ratio. This means the 
relationship between length of CNRPs 
and the input motion string. Let α  be 
the length of CNRP and β  be its 
frequency. ω  is the length of the input 
motion string. θ  is a given threshold for 
determining its priority. 
( * )ω α β θ− ≤  
The reason for adopting this principle is 
that if a sub-action appears many times in the 
7
z To evaluate more symbolization methods 
for motion data. 
z To use it as a basic component of motion 
analysis for various applications such as 
the study of non-verbal behavior in 
dialogue [30] or verbal behavior in daily 
life [28]. 
z To investigate motion synthesis method 
via generated grammar. This idea is 
similar to motion patches [29], we can try 
to build the transitions between patterns 
(sub-actions) instead of frames (postures) 
[12], [13] or motion primitives (symbols) 
[29]. 
z Inspired by the pervious work [26], the 
audio information such as background 
music may assist us to automatically 
detect and emphasize more semantic 
information in the future. 
 
3. Reference (參考文獻) 
1. J. Oh, Q. Wen, J. Lee, and S. Hwang, Video Data 
Management and Information Retrieval. IRM Press, 
2004, ch. Video Abstraction, pp. 321–346. 
2. B. T. Truong and S. Venkatesh, “Video abstraction: A 
systematic review and classification,” ACM 
Transactions on Multimedia Computing, 
Communications, and Applications, vol. 3, no. 1, p. 
3, 2007. 
3. P. Shilane and T. Funkhouser, “Distinctive regions of 
3d surfaces,” ACM Transactions on Graphics, vol. 
26, no. 2, p. 7, 2007. 
4. C. H. Lee, A. Varshney, and D. W. Jacobs, “Mesh 
saliency,” ACM Transactions on Graphics, vol. 24, 
no. 3, pp. 659–666, 2005. 
5. R. Gal, A. Shamir, and D. Cohen-Or, 
“Pose-oblivious shape signature,” IEEE Transactions 
on Visualization and Computer Graphics, vol. 13, no. 
2, pp. 261–271, 2007.  
6. C. Rother, L. Bordeaux, Y. Hamadi, and A. Blake, 
“Autocollage,” ACM Transactions on Graphics, vol. 
25, no. 3, pp. 847–852, 2006.  
7. I. S. Lim and D. Thalmann, “Key-posture extraction 
out of human motion data by curve simplification,” 
in Proceedings of 23rd Annual International 
Conference of the IEEE Engineering in Medicine 
and Biology Society (EMBC 2001), 2001, pp. 
1167–1169. 
8. J. Assa, Y. Caspi, and D. Cohen-Or, “Action 
synopsis: pose selection and illustration,” ACM 
Transactions on Graphics, vol. 24, no. 3, pp. 
667–676, 2005. 
9. K.-S. Huang, C.-F. Chang, Y.-Y. Hsu, and S.-N. Yang, 
“Key probe: A technique for animation keyframe 
extraction,” The Visual Computer, vol. 21, no. 8-10, 
pp. 532–541, 2005. 
10. J. F. Roddick and M. Spiliopoulou, “A survey of 
temporal knowledge discovery paradigms and 
methods,” IEEE Transactions on Knowledge and 
Data Engineering, vol. 14, no. 4, pp. 750–767, 2002. 
11. B. Mirkin, Clustering for Data Mining: A Data 
Recovery Approach. Chapman & Hall/CRC, 2005. 
12. O. Arikan and D. A. Forsyth, “Interactive motion 
generation from examples,” ACM Transactions on 
Graphics, vol. 21, no. 3, pp. 483–490, 2002. 
13. J. Lee, J. Chai, P. S. A. Reitsma, J. K. Hodgins, 
and N. S. Pollard, “Interactive control of avatars 
animated with human motion data,” ACM 
Transactions on Graphics, vol. 21, no. 3, pp. 
491–500, 2002. 
14. J.-L. Hsu, C.-C. Liu, and A. L. P. Chen, 
“Discovering nontrivial repeating patterns in music 
data,” IEEE Transactions on Multimedia, vol. 3, no. 
3, pp. 311–325, 2001.  
15. C.-C. Yu and Y.-L. Chen, “Mining sequential 
patterns from multidimensional sequence data,” 
IEEE Transactions on Knowledge and Data 
Engineering, vol. 17, no. 1, pp. 136–140, 2005. 
16. G. Cormode and S. Muthukrishnan, “What’s hot 
and what’s not: tracking most frequent items 
dynamically,” ACM Transactions on Database 
Systems, vol. 30, no. 1, pp. 249–278, 2005. 
17. G. Piatetsky-Shapiro and C. J. Matheus, “The 
interestingness of deviations,”in Proceedings of 
KDD-94 Workshop Knowledge Dzscovery in 
Databases. AAAI Press, 1994, pp. 25–36. 
18. A. Silberschatz and A. Tuzhilin, “What makes 
patterns interesting in knowledge discovery 
systems,” IEEE Transactions on Knowledge and 
Data Engineering, vol. 8, no. 6, pp. 970–974, 1996. 
19. K. McGarry, “A survey of interestingness 
measures for knowledge discovery,” Knowledge 
Engineering Review, vol. 20, no. 1, pp. 39–61, 2005. 
20. J. E. Hopcroft, R. Motwani, and J. D. Ullman, 
Introduction to Automata Theory, Languages, and 
Computation. Addison Wesley, 2000. 
21. R. Metoyer, V. Zordan, B. Hermens, C.-C. Wu, 
and M. Soriano, “Psychologically inspired 
anticipation and dynamic response for impacts to the 
head and upper body,” IEEE Transactions on 
Visualization and Computer Graphics, vol.14 no.1 
pp.173-185, 2008. 
22. J. J. V. Wijk, “Views on visualization,” IEEE 
Transactions on Visualization and Computer 
Graphics, vol. 12, no. 4, pp. 421–433, 2006. 
9
可供推廣之研發成果資料表 
□ 可申請專利  ■ 可技術移轉                                      日期：97 年 05 月 19 日 
國科會補助計畫 
計畫名稱：角色動畫之自動摘要與可視化之研究 
計畫主持人：楊熙年 
計畫編號：NSC 96-2221-E-033-073-MY3 學門領域：資訊工程 
技術/創作名稱 動畫內容摘要之自動產生技術 
發明人/創作人 楊熙年、黃科森 
中文：龐大的資料量是動畫的一大特徵。如何有效管理、搜尋以及
視覺化是一大課題。本研究提出一種新的動畫內容摘要之自動產生
技術，可以對動畫資料進行分析進而萃取其重要特徵與隱含的結
構，進而產生動作之靜態摘要(summary)或動態濃縮動畫
（skimming）。 
 
 
技術說明 英文：It is known that large volume size is an inherent 
characteristic of animations. To explore or search a 
specified target (e.g. a clip for motion data) in such data 
is time-consuming and expensive process. In this study, the 
proposed method can find the latent structure of a given 
motion data. Based on the discovered latent structure, our 
method can produce both its summarization and skimming
automatically. 
可利用之產業 
及 
可開發之產品 
數位內容產業。 
技術特點 
本技術可以對一段動畫有效地產生各種形式的摘要。其精簡表示法
將有利於動畫資料之管理、展示和網路傳輸。 
推廣及運用的價值 
提升動畫資料之管理，進而改善現有動畫資料及內容之重複使用率
以提高動畫製作之生產力。 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位
研發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 11
INTERMEDIA 的特別場次。本大會所宣讀的論文主要在計算機圖學中之
塑形、成像及動畫等之原理與技術。除了論文發表者外大會還邀請多位
知名的研究學者一起探討上述主題，使得討論變得十分熱絡。 
此次會議我們有一篇宣讀論文(Oral Presentation) :Automatic Lighting 
Design for Motion Data被接受，並安排在大會第一天(六月九日)上午
Motion Analysis/Synthesis 的場次發表。 
 
二、與會心得 
大會於6/9上午9時開始，第一場為Keynote Speech，其內容主要探討
手機上之視覺介面設計。由Nokia Research Center的工程師 Kari Pulli 主
講，重點在如何設計有趣的手機遊戲，除了遊戲本身之專業外，還涉及
手機上之計算與繪圖能力。第二堂則是三維物體的塑形與形變，其中包
括中原大學鍾斌賢教授及其研究團隊的作品。下午的主題包括「實體成
像」及電腦動畫。其中包括我們有關動畫中自動照明設計的論文。 
第二天上午由國際圖學學會主席Nadia Magnenat-Thalmann 主講
Real-Time Virtual Humans 從靜態之人物塑技術形到動態的自然反應動
作之合成原理並介紹群眾行為之模擬技巧。這雖是Tutorial場次但對未來
有關動畫教學之內容預備實穫益良多。當天上午同時進行的還有幾何塑
形(Geometry Modeling)與及時成像(Real-Time Rendering)等兩場次。接著
是Keynote Speech由奧地利維也納科技大學的教授Werner Purgathofer介
紹醫學影像技術中實體成像(Volume Rendering)的最新發展。其重點在如
何從實體資料中求得具描述性的摘要。這也是當前視覺資訊(Visual 
Information) 摘要化的熱門研究課題之一。其基本精神可作為動畫摘要技
術之參考。雖然方法不一樣，但是研究如何淬取視覺元素的目的是一樣
的，聽完他的作法後，認為如何利用視覺化技術來協助動作元素的淬取
是個有趣的研究課題。當天下午的主要課題是Mesh Deformation，我國有
成功大學李同益教授之團對發表其研究成果:Example-Driven Animation 
Synthesis。因同屬動畫領域，故有許多足供參考的演算法。例如利用降
維技術將動作資料映射到低維空間，然後在低維空間合成想要的動作行
為。 
13
行政院國家科學委員會補助專題研究計畫 □成果報告 ■精簡報告 
 
角色動畫之自動摘要與可視化之研究 
 
 
計畫類別：■ 個別型計畫  □ 整合型計畫 
計畫編號：NSC 96－2221－E－033－073－MY3 
執行期間： 96 年 08 月 01 日至 99 年 07 月 31 日 
 
計畫主持人：楊熙年 
共同主持人： 
計畫參與人員： 黃科森、陳鴻源 
 
 
成果報告類型(依經費核定清單規定繳交)：■精簡報告  □完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
□出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
          
執行單位：中原大學 
 
中   華   民   國  98 年  05 月   14 日
15
Shi-Nine Yang, Ke-Sen Huang & Hao-Ming Chen / Automatic Summarization and Visualization of Character Animations
Figure 1: Five stages of our system: (a) data preparation,
(b) camera placement, (c) motion cut (segmentation), (d)
view selection, and (e) view refinement.
have to prepare the motion data for incorporating it into im-
plementation platform. The next step, we compute the can-
didate views of the entire motion based on two cinemato-
graphic rules, the line of interest and the triangle principle.
After segmentation procedure, the entire motion is divided
into shots (segments). Now every shot has its own candidate
views, we furthermore choose the appropriate view among
those views by shot visibility. When the appropriate view of
each shot is determined, we can make some further refine-
ment in order to provide more variations. Finally, the result-
ing well-edited movie is produced.
2. Camera Placement for Motion Data
2.1. The Line of Interest
The director usually considers the scene or the moving path
which is surrounded by the Circle of Action [Kat91] and an
invisible line which divides the circle into two parts. This in-
visible line is the line of interest (LOI). The director usually
places the cameras on one side of the line to avoid confusing
audiences. In this project, we describe a computation model
to obtain LOI for a given motion data as follows:
1. We use a plane equation ax+by+cz+d = 0 to represent
the LOI in the scene. This plane is perpendicular to the
stage. The vector N (a,b,c) is the normal of this plane.
2. To consider the scene of the given motion, we compute
the center (cx,cy,cz) of the bounding box according to
the root positions of the sequence. We assume the center
is a point on the LOI.
3. For the distribution of body orientations, we apply prin-
cipal components analysis [Jol02] to all root orientations
(only using the x and z components) of the given motion.
(a) (b)
Figure 2: (a) The LOI plane. (b) The selected half LOV is
used to place three cameras.
Then we can obtain the second eigenvector (ex,0,ez) as
the direction of LOI.
4. Now, we have the normal to the LOI ((a,b,c)=(ex,0,ez))
and a point (cx,cy,cz) that lies on the LOI. Hence, we can
obtain the LOI.
After above procedures, the Line-of-Vision (LOV) sphere†
was divided into two halves (Figure 2(a)). The possible cam-
era positions are limited to half sphere relative to LOI.
2.2. The Triangle Principle
To construct a triangle arrangement of camera, we define the
notion of motion viewpoint entropy to determine which side
of LOI is suitable for placing cameras. Then we also use
proposed entropy to place three cameras at this side. Similar
to previous works [TFN05, BS05, JS06], we also follow the
concept of viewpoint entropy introduced in [VFSH01]. We
formulate viewpoint entropy as the following equation:
H( f ) =−
n
∑
i=1
pi( f ) log2 pi( f ) f ∈ { fa, fb} (1)
where n is the frame number of the given motion sequence.
f is the viewing direction and fa and fb are the two fields
divided by the LOI. Let pi( f ) be the probability of viewpoint
i appearing in view direction f . We can determine the proper
viewing fields by the following equation:
arg max
f∈{ fa, fb}
H( f ) (2)
Figure 2(b) shows the selected half LOV sphere. To place the
first camera (apex viewing vector), we parameterize the se-
lected view field ( fa or fb) into 12 sub-fields every 30 degree
in azimuth and every 90 degree in elevation (Figure 3(a)).
† The viewing sphere of a scene (motion) is referred to an imag-
inary sphere which encompasses the scene and the viewpoints are
moving on the surface of the sphere.
17
Shi-Nine Yang, Ke-Sen Huang & Hao-Ming Chen / Automatic Summarization and Visualization of Character Animations
Figure 5: (a) Disobeying the rule of thirds; (b) the rule of
thirds of human figure.
and lower torso movement. That is, if a segment’s movement
ratio is larger than a threshold, the shot is taken as medium
shot; otherwise it is treated as a long shot.
4.2. Common Visual Axis
Common visual axis, where the camera moves forward or
backward, has a key characteristic-emphasizing the Number
Contrast [Ari76]. We implements the common visual prop-
erty by the aforementioned movement ratio. Using motion
ratio, we can segment a motion cut by checking its shot
type. If a segment of motion can be divided into two parts
by the movement ratio, then this property is applied and the
segment is divided into two subsegments. Consequently, our
shot change may not be exactly the same as that of motion
cut, and therefore results in more variations.
4.3. Artistic Composition
The Rule of Thirds. Golden ratio came from the asymmet-
ric balance, and it consists of the rule of thirds and the rule
of fifths [GRMS01] in artistic composition. Here, current
study only takes the rule of thirds into consideration because
the rule of fifths, which is the derivative rule of the rule of
thirds, is the more complicated method. That is, the rule of
thirds is a better choice than the rule of fifths, in terms of
performance and computation. The way that photographers
implement the rule of thirds is usually to imagine vertical
and horizontal lines dividing the image into 9 equal-shaped
blocks. To confirm the rule and create a good composition,
photographers usually position the objects at these the 4 in-
tersection points. For example, if the subject is a human be-
ing, photographers will position the person covering two left
or right intersection points, which follows the asymmetric
balance and provides larger looking room (Figure 5).
Maintaining Sectors. Maintaining sectors [Ari76] means
to divide the frame into even parts and the character of the
succeeding shot should be approximately in the same sec-
tor within the frame from shot to shot. Maintaining sectors
can be easily achieved by composition and bounding box
method.
5. Results and Discussions
It is known that the aesthetic of movies is subjective, and the
cinematographic rules are usually hard to formulate. How-
ever, by following these rules, we can usually produce ac-
ceptable results. Here we present three different motions’
cutting results. We first demonstrated the motion’s resulting
camera placements (represented by look-at vectors) and an-
imation snapshots, the number of shots was determined by
the number of motion segments. Then, we illustrated the re-
sulting shots of the motion by comparing with the fixed shot
of the motion.
Slam Dunk Motion. Figure 6(a) shows the resulting cam-
era placements of a slam dunk motion (348 frames). The pur-
ple lines indicates the triangle cameras’ look-at vectors (the
default sets), and the two green lines (slam dunk motion was
segmented into two segments) indicates our resulting cam-
eras’ look-at vectors. Note that the two green lines are differ-
ent from the left and right external look-at (purple lines) re-
spectively, because we made some further adjustments. Fur-
ther, in order to demonstrate the resulting movie on paper,
the resulting shots is illustrated by keyframes selected by
hand as shown in Figure 6(c), and its frame numbers corre-
sponds to fix shot as shown in Figure 6(e). The fixed camera
was inspired by three-quarter view, and the camera’s look-at
vector was the center of motion bounding box minus the top
left vertex of motion bounding box.
Somersault Motion. Figure 6(b) depicts the resulting
camera placements of somersault motion (149 frames) and
animation snapshots, which is the same representation as in-
dicated in Figure 6(a). Notice that the shot 1’s look-at vec-
tor is short, because its camera was constrained to avoid be-
low the floor. Next, Figure 6(d) presents the resulting shots
of somersault motion, the keyframes was selected by hand,
compared to fixed shot’s result as shown in Figure 6(f).
Playground Motion. The final example is more complex
than preceding examples. Playground motion’s duration is
1636 frames, and it was segmented into ten segments (shots).
Note when shot 2 cuts to shot 3 (see Figure 7(a), row 1 col-
umn 4 to row 2 cloumn 1), there is a common visual axis
used to emphasize the movement from long shot to medium
shot. Figure 7(a) and Figure 7(b) show a comparison be-
tween our results and fixed shot’s results.
6. Conclusion
In this project, we have presented a cinematography-based
automatic view selection system about human motion data.
The cinematographic rules not only can be considered as fil-
ters to classify the unit sphere, but as constraints to maintain
continuity for generating the good candidate views. After
the shot selection from candidate views which is achieved
by occlusion property (we prefer the shot with minimum oc-
clusion of a player), we imposed the artistic compositional
19
Shi-Nine Yang, Ke-Sen Huang & Hao-Ming Chen / Automatic Summarization and Visualization of Character Animations
(a) The resulting shot of playground motion. (b) The fixed shot of playground motion.
Figure 7: The resulting snapshots of playground sequence.
rules; thus a human motion can be represented as a well-
edited movie, and the motion from shot to shot looks smooth.
Limitations. In addressing the cinematic scene, our
method had certain limitations. First, instead of exploring
the overall continuum of possible viewpoints, we only search
over a finite set of candidate views; in other words, we might
miss the best view. Second, our method can not film dia-
logue shot because of the non-narrative motion without the
information of dialogue. Finally, for dynamic motion data,
we have to consider an additional dimension of time; thus,
it might be needed another optimization method, such as ab-
straction of model or hardware acceleration.
References
[Abl02] ABLAN D.: Digital Cinematography & Direct-
ing. New Riders Press, 2002.
[ACOYL08] ASSA J., COHEN-OR D., YEH I.-C., LEE
T.-Y.: Motion overview of human actions. ACM Trans-
actions on Graphics 27, 5 (2008), 115:1–115:10.
[Ari76] ARIJON D.: Grammar of the Film Language.
Hastings House Publishers, 1976.
[BB98] BINDIGANAVALE R., BADLER N. I.: Motion ab-
straction and mapping with spatial constraints. In Pro-
ceedings of the International Workshop on Modelling
and Motion Capture Techniques for Virtual Environments
(1998), pp. 70–82.
[BS05] BORDOLOI U. D., SHEN H.-W.: View selection
for volume rendering. In Proceedings of the IEEE Visual-
ization 2005 (2005), pp. 487–494.
[CN05] CHRISTIE M., NORMAND J.-M.: A semantic
space partitioning approach to virtual camera composi-
tion. Computer Graphics Forum 24, 3 (2005), 247– 256.
[CO06] CHRISTIE M., OLIVIER P.: Camera control for
computer graphics. Eurographics 2006 State of The Art
Report (2006), 89–113.
[EMC02] ÉRIC MARCHAND, COURTY N.: Controlling a
camera in a virtual environment. The Visual Computer 18,
1 (2002), 1–19.
[GRMS01] GOOCH B., REINHARD E., MOULDING C.,
SHIRLEY P.: Artistic composition for image creation. In
Proceedings of the 12th Eurographics Workshop on Ren-
dering Techniques (2001), pp. 83–88.
[Haw05] HAWKINS B.: Real-time cinematography for
games. Charles River Media, 2005.
[HCS96] HE L.-W., COHEN M. F., SALESIN D. H.: The
virtual cinematographer: a paradigm for automatic real-
time camera control and directing. In Proceedings of SIG-
GRAPH 1996 (1996), pp. 217–224.
[Jol02] JOLLIFFE I. T.: Principal Component Analysis.
Springer, 2002.
[JS06] JI G., SHEN H.-W.: Dynamic view selection for
time-varying volumes. IEEE Transactions on Visualiza-
tion and Computer Graphics 12, 5 (2006), 1109 – 1116.
[Kat91] KATZ S. D.: Film Directing Shot by Shot: Visual-
izing from Concept to Screen. Michael Wiese Productions,
1991.
[KL08] KWON J.-Y., LEE I.-K.: Determination of cam-
21
Self assessment(成果自評) 
本計畫是延續過去多年在 3D 動畫之研究成果，請參看國科會個人資料中本人之
publication list。本計畫所提出之動作抽象化之方法，可以有效地達成下列項目: 
一、鏡頭配置目標函數之訂定 
二、電影攝影術相關規則之實作 
三、靜態相機主要準則歸納和演算法設計 
四、動態相機主要準則歸納和演算法設計 
五、演算法與雛型系統實作 
六、鏡頭配置演算法與可視化效果之效率評估 
本計劃之各項產出物，未來可以進一步於各種應用中使用。例如計算後的鏡頭資訊可以
用於動作合成系統(motion system)中，提供自動化的鏡頭配置；或是在視覺化(Visualization)
系統中提供資訊量蘊含最為豐富的視角讓使用者觀看。 
本計畫之結果計畫將投稿至知名之國際會議及學術期刊。 
 
 
23
出席國際會議報告 
                                     
姓  名 楊熙年 會議期間 2009/5/26 ~ 2009/5/29 
會議名稱 中文：第二十七屆國際計算機圖學研討會(2009 年) 
 英文：Computer Graphics International (2009 ) 
  
出國報告： 一、會議介紹 
此國際會議是國際計算機圖學學會(the Computer Graphics Society)每
年舉辦一次的電腦圖學研討會，本年度研討會除了由國際圖學學會贊助
外，協辦單位還包括美國計算機圖學學會(ACM Siggraph)及歐洲圖學學
會(Eurographics)並由加拿大 Victoria 大學主辦。本大會創於 1982 年，到
目前已有 27 年歷史。過去曾在英國、美國、加拿大及歐洲各國舉行。今
年會議地點是在加拿大英屬哥倫比亞省的維多利亞城(Victoria, British 
Columbia, Canada)舉行。在四天中來自近 10 多個國家的研究人員聚集一
堂，共同研究計算機圖學之最新趨勢。本次大會與計算機美學國際會議
(Computational Aesthetics 2009)共計有六十多篇論文以論文宣讀來呈現當
前最具前瞻性的研究成果。主要議程可分為下列四大類： 
1. Tutorial Courses 
2. Keynote Speeches 
3. Technical Papers(包括 Oral Presentations 與 Short Papers) 
4.   Art Show 
會議議程分成多個主題，各個主題分別進行論文宣讀。在論文宣讀
的部分，每個主題有四至六篇論文進行宣讀，每篇論文有十八分鐘的時
間。每個場次都吸引許多學者參與討論。此外，每天上午及中午餐會大
會安排一場 keynote Speech，邀請知名學者針對目前最新的研究課題進行
演說，聽眾反應熱烈。 
大會第一天進行的主題包括 Mesh， Motion Analysis/Synthesis，
Simulation，Computer Vision，Surface Modeling 等，。第二天進行的主題
25
理複雜度的技術上是有相通之處，例如大家都會使用PCA以達到降維效
果的技巧有些相像。當天最後一場包括變形( Deformation )與 臉部動畫
( Face Animations) ，基本上它們都是研究曲面變形的技術，不過本次臉
部動畫的論文加入了頭骨模型的限制條件，使得所合成的效果較佳。 
第三天上午的演講，是由Microsoft Cambridge的Andrew Fitzgibbon
主講，他的專長基本上是Computer Vision 中 Complex Visual Effect，雖
然分析的成份較多，不過其內容可做為電腦圖學中影像合成技術之參
考。其它場次以物理模擬( Physically Based Simulation)的論文較為有趣，
其中浙江大學提出了基於梯度擴散模式(Gradient-based Spreading Model) 
的薄殼燃燒模擬，效果良好。 
這次會雖受國際流感N1H1的影響，但有來自世界10多個國家之學
者專家近100人參加，經三天多的討論切磋，相關問題包括動作合成
(Motion Synthesis)，及時成像(Real-Time Rendering)，網格變形(Mesh 
Deformation)，物理模擬 (Physically Based Simulation)等。參加本次會議
除了在圖學知識上獲益良多外，更在群眾模擬(Crowd Simulation)領域中
有關群眾合成與分析的技巧上有更新的認識。 
三、攜回資料 
CGI 2009 Conference Proceedings 以及相關之磁碟片。 
 
  
 
27
 
 
人體動作效果之互動式圖示產生法 
(Interactive Motion Cue Generation for Motion Data) 
中文摘要                                       
現今動畫及遊戲產業的製作過程中，動作資料(Motion Capture Data) 已經被
廣泛地應用。隨著動作資料的普及動作資料庫的資料量也與日俱增變得十分龐
大，因此有效的動作查詢功能成為當前動作資料庫的重要研究課題之一，而查詢
功能的核心技術就是關鍵影格(Key Frame)的找尋與表示法。近來已有許多關鍵
影格找尋及表示法的研究，但這些關鍵影格通常所能提供的往往只有靜態的姿勢
資訊，未能將連續的運動行為表現出來。所謂動作暗示(Motion Cue)是一種能將
物體連續的運動行為在單張圖片(影像)中呈現出來的技巧，此技巧常見於漫畫繪
製上，本研究將探討此技術以及它在關鍵影格上提升視覺化效果的應用。首先，
我們整理了各種漫畫家所使用的動作暗示，接著進一步說明這些動作暗示使用的
時機與方法。然後我們實作一個系統，讓使用者能夠從動作資料中選取想要表現
的關鍵影格，再透過各種動作暗示參數之設定，便可在關鍵影格中產生想要的動
作暗示以提升其視覺化的效果。 
關鍵字：動作資料、動作暗示、動作理解、動作圖示 
 
英文摘要                                        
Recently, motion capture data have been wildly used in both animation and game 
industries. Accordingly, the size of motion database grows extensively and an 
effective motion query function becomes inevitable for the application of motion 
database. It is known that the key frame representation plays an essential role in 
designing an effective motion query and many studies have been proposed on 
selecting key frames from the given motion data. However, the selected key frames 
only provide static gesture without motion information. In this study, we will discuss 
the idea of motion cue and explore how it is applied to the key frame representation to 
enhance its motion information. First, we study the most existing motion cue styles 
used by cartoonists, and investigate the relations between different kinds of motions 
and their corresponding motion cues. Then we implement a motion cue generation 
system to provide motion information of key frames. The system provides several 
motion cues and can generate desired motion cues automatically according to the user 
specified motion parameters. 
Keywords： motion data、motion cue、motion understanding、motion illustration
29
 
 
第二章 相關研究成果                            
如何在一張靜態的圖片(影像)中描述出物體的運動行為是一件很複雜的問
題，因為圖片(影像)這種媒體本身無法提供物體連續運動的資訊。Tufte[1997]中
也提到了這些 2D 媒體的缺陷，因為在感知上及時瞬間的影像無法精確的呈現出
人類眼睛所看見的景象。而隨著藝術歷史的演進，藝術家透過觀察日常事物漸漸
的發展出一套在圖片中描述物體運動行為的繪畫技巧；並且由於相機的發明，利
用各種攝影技巧拍攝動態物體所得到照片中的一些效果也被藝術家運用在繪畫
當中，我們將這些能在圖片中描述物體運動狀態的技巧稱為動作暗示。 
Cutting [2002]詳細的整理了在藝術、科學和流行文化中各種在靜態的圖片
(影像)中描述物體運動的技術。他將這些技術分為五種類型: photo-graphic blur、
speed lines、multiple stroboscopic images、shearing 和 dynamic balance。而在漫畫
和動畫中，漫畫家或動畫師會利用更誇張的手法去呈現出各種動作暗示。
McCloud[1993]描述了在漫畫中利用 action lines、stroboscopic effects 和 streaking 
effects 等手法讓讀者可以直覺的感覺到物體正在運動。而王庸聲[2005]則將漫畫
中繪製物體運動狀態的效果分成六類:氣流線、多重曝光、瞬間圖像、模糊圖像、
模糊背景和氣氛烘托。 
由於動作暗示這種技術可以改善 2D 媒體本身沒有辦法提供物體連續運動資
訊的缺陷，所以開始有許多針對在圖片(影像)上合成動作暗示的研究。而利用各
種影像處理的手法在影像中產生動作暗示的研究很早就被提出，Potmesil et 
al.[1983]和 Nelson et al.[1985]提出了利用各種影像處理的手法在圖片(影像)中產
生 motion blur 這種動作暗示的方法。Potmesil et al.[1983] 利用一個通用的相機模
型(Camera Model)去累積影像平面上物體移動軌跡的點資訊，利用這些採樣點資
訊的色彩值進行有時間限制的曝光處理來得到這種效果。Nelson et al.[1985]的方
法則是利用排序(Sorting)各影格中物體在影片中的景深，然後將這些排序過的影
格利用他們提出的公式重新組成在同一張圖片(影像)上產生這種效果。 
接下來，其他在圖片(影像)上合成各種不同動作暗示的方法和系統也陸陸續
續被提出。Hsu et al.[1994]提出了一個系統，這個系統可以模擬出不同風格的藝
術家在繪畫上不同筆觸的線條，而他們的系統也可以應用在繪製不同筆觸效果的
速度線這種動作暗示上。Masuch et al.[1999]利用動畫中的關鍵影格資訊和 3D 
mesh model 做為輸入資料，提出各種演算法去產生能讓 model 呈現出速度感的
動作暗示:speed lines、arrows 和 repeated contours。Brostow et al.[2001]提出了產
生 motion blur 這種動作暗示的方法，首先為了要計算兩張連續的影格中像素的
對應關係必須將相鄰的像素分群，利用仿射轉換(Affine Transforms)追蹤和記錄
每個像素的運動向量(Motion Vector)，最後每個像素的色彩值根據像素在影片中
移動的距離重新分配產生這種效果。Kawagishi et al.[2003]將動畫中的非擬真動
作效果(Non-photorealistic Motion Blur)分為三種類型 :Lines、After-images 和
Deformation，然後進一步分析這三種類型效果，找出能夠控制這些效果呈現的
31
 
 
等非擬真的動作暗示將此動作資料呈現的運動型態在單張圖片中摘要出來。 
另外，也有不僅僅將動作暗示合成在圖片(影像)這類 2D 媒體上而是直接合
成在動畫影片中的研究。Schmid et al.[2010]提出五種演算法: instant render、speed 
line、motion blur、stroboscopic images 和 shift，直接將這些動作暗示直接合成在
動畫中，利用這些卡通形態的動作暗示加強了動畫中物體運動的效果。 
本研究中整理了各種不同的動作暗示效果，進一步的去分析這些效果應用的
情形和控制效果的各種因素，歸納成表格讓讀者容易的了解各種動作暗示。而我
們希望能夠利用動作暗示這種手法透過單一影格便能將動作資料中影格人物的
運動行為描述出來，加強動作資料中影格能表達的資訊。所以我們也實做了一個
系統，系統使用了動作資料當作輸入資料，使用者根據我們提出的五種動作暗示
合成演算法(平行直線、軌跡線、集中效果線、透明度漸層和碰撞效果線)透過參
數的設定在指定的影格上合成想要的動作暗示效果。 
我們的系統與前述的各種研究相比較，在動作暗示種類的選擇方面 Potmesil 
et al.[1983]、Nelson et al.[1985]、Brostow et al.[2001]、Collomosse et al.[2005]和
Schmid et al.[2010] 這些研究所提出的方法需要利用運動物體中的像素(Pixel)色
彩資訊去做重新的分配來產生這些效果，而我們系統所有的資訊只有動作資料中
關節點的座標資訊，所以我們不會採用這類的方法。Kawagishi et al.[2003] 和 Lai 
[2005]分類中所提出的 deformation 類型的動作暗示效果，一般而言漫畫家會將這
種類型的效果利用在產生物體運動速度的情況上較少使用在漫畫(動畫)人物身
上，所以我們也沒有使用這類的動作暗示。而我們的系統提供了上述研究中所沒
有提到能描述物體發生碰撞情形時使用的碰撞效果線這種動作暗示的合成。 
若以動作暗示應用效果好壞的角度去比較，Goldman et al.[2006]利用 motion 
arrow 去描述物體運動的手法只能呈現物體在影片中運動的軌跡沒有辦法有效的
表現物體運動的速度感。Bouvier-Zappa et al.[2007]的論文中，利用 motion arrow
上顏色的深淺來呈現人物運動速度上的不同在視覺上不夠直觀，若是利用速度線
和集中效果線這種漫畫中常使用的手法，透過參數的控制來產生不同速度感的效
果能夠讓使用者更直接的了解物體的運動狀態。而在 Kato et al.[2004]的系統中各
種線條，我們覺得呈現物體運動速度效果的感覺不夠好，讓使用者調整的參數也
不夠完整，我們的研究中針對漫畫家使用各種線條的手法提供完整的分析，並針
對這些去分析設計我們的系統，能夠讓使用者方便的調整參數去控制動作暗示效
果的表現。Kawagishi et al.[2003] 和 Lai [2005]的線條這種類型動作暗示合成演
算法必須沿著物體在影片中的運動軌跡去合成，我們的系統可以根據使用者分析
動作資料中人物的實際運動情或是使用者隨意設定去產生。 
第三章 研究方法                                
本研究使用的輸入資料是動作資料所有的資訊為各關節點的座標位置，而模
糊背景、模糊物體、瞬間圖像和變形是屬於對圖片(影像)資訊做處理的動作暗示
效果所以我們沒有採用這類的手法。我們希望使用者能透夠設定簡單的參數便能
33
 
 
3D 座標資訊轉換成 2D 座標資訊，所以我們也可以藉由上個小節的概念直接從
骨架上拉出平行直線。下列為參數介紹: 
 W:線條與線條之間的間隔範圍，我們訂在 0~1 之間。 
 Llength: 平行直線長度的範圍，每繪製一條直線會隨機從這個範圍內選擇一個
值。 
 Lcolor_range: 線條的顏色範圍，每繪製一條直線會隨機設定這個範圍。
(0,0,0)~(255,255,255) 
 繪製由骨架延伸出的平行直線演算法 
SELECT SKELETAL (XNode1 , YNode1)  (XNode2, YNode2) 
FOR(i=0  i< 1  i+=W) 
{ 
     SET_START_POINT 
IF XNode1＞XNode2 
X=X+W* SKELETALlength *COS(θ) 
Y=Y-W* SKELETALlength *SIN(θ) 
ELSE 
X=X-W* SKELETALlength *COS(θ) 
Y=Y-W* SKELETALlength *SIN(θ) 
 
SET_END_POINT 
IF XNode1＜XNode2 
X=X+W* SKELETALlength *COS(θ)+ Llength 
Y=Y-W* SKELETALlength *SIN(θ) 
ELSE 
X=X-W* SKELETALlength *COS(θ)- Llength 
Y=Y-W* SKELETALlength *SIN(θ) 
 
     DRAW_LINE(); 
} 
表 3.2 繪製由骨架延伸出的平行直線演算法。 
3.2 軌跡線 
在這個小節中，我們將介紹如何繪製軌跡線。軌跡線是用來呈現人物四肢或
軀幹運動軌跡的效果。從第三章我們對軌跡線的分析可以得到軌跡線的長度和密
集度是影響軌跡線用來描述人物運動行為的因素。而在運動資料中，人物運動通
常是針對一個關節點為中心骨架長度為半徑去運行。因此，我們針對這個特性和
上述影響軌跡線效果的兩個因素設計可以調整的參數去繪製軌跡線。下列為參數
介紹: 
 θmin 和 θmax:軌跡線要繪製的範圍。這兩個參數會決定軌跡線的長度。 
 W:兩條軌跡線間距大小的範圍，用來決定軌跡線的密集度。 
 Rmin:軌跡線的內徑長度。 
 Rmax:軌跡線的外徑長度。 
 W、Rmin 和 Rmax 這三個參數會決定軌跡線的線條數。 
 θchange:調整每條軌跡線長短的變化參數範圍。 
 繪製軌跡線的演算法 
35
 
 
(0,0,0)~(255,255,255) 
 繪製以線條為基礎的集中效果線演算法 
SET(X,Y)  //TRNSLATE(X,Y) 
SELECT(θmin、θmax) 
 For (θ =θmin  θ<=θmax  θ+=W)   
 { 
     SELECT{ Lcolor_range =random()} 
     SELECT{W=random()} 
SELECT{ Rmin =random()} 
SELECT{ Rmax =random()} 
 
        SET START POINT 
    X=( Rmin) * SIN(θ)  
           Y=( Rmin) * COS(θ)  
     SET END POINT 
    X=( Rmax) * SIN(θ)  
Y= (Rmax ) *COS(θ)  
 
  DRAW_LINE(); 
   } 
表 3.4 繪製以線條為基礎的集中效果線演算法。 
3.3.2 以線條束為基礎的集中效果線-集中效果線(2) 
在這個小節中，我們將介紹如何利用以線條為基礎的集中效果線透過增加一
個步驟繪製以線條束為基礎的集中效果線，讓集中效果線呈現一束一束的效果 
。下列為參數介紹: 
 θmin 和 θmax: 以線條束為基礎的集中效果線要繪製的範圍。 
 R:內徑的範圍。決定以線條束為基礎的集中效果線之中心區域的大小。 
 W1: 每次旋轉的弧度角範圍，每次選擇要轉的弧度角都隨機從這個範圍內選
擇一個值。 
 θRADIATIVE RAY-1: 以線條為基礎的集中效果線要繪製的範圍，由於想呈現一束
一束的集中效果線通常都設定很小的值。 
 Rmin: 以線條為基礎的集中效果線之內徑範圍，每繪製一條集中效果線都隨
機從這個範圍內選擇一個值。 
 Rmax: 以線條為基礎的集中效果線之外徑範圍，每繪製一條集中效果線都隨
機從這個範圍內選擇一個值。 
 W2: 以線條為基礎的集中效果線每次旋轉的弧度角範圍，每次選擇要轉的弧
度角都隨機從這個範圍內選擇一個值。由於要呈現一束一束線條的感覺，這
個值通常都設很小。 
37
 
 
FOR(i=1 i<=N i++) 
{ 
    Fi = Fi-1 -W; 
SET{α = (N – i) / (N + 1) ) 
DRAE_SKELETAL(Fi); 
} 
表 3.6  繪製透明漸層效果演算法。 
34.5 碰撞效果線 
影響碰撞效果線效果的因素有碰撞效果線的範圍、凸起效果的數目和凸起效
果的銳利程度等因素。因此，我們根據這些要素去設計可以調整這些效果的參
數。下列為參數介紹: 
 θmin 和 θmax: 碰撞效果線要繪製的範圍。 
 Rmin: 凹下點的半徑範圍，每次都隨機從這個範圍內選一個值。 
 Rmax: 凸起點的半徑範圍，每次都隨機從這個範圍內選一個值。 
 Rmin 和 Rmax 範圍會決定凸起形狀的尖銳程度和變化程度。 
 W: 每次旋轉的弧度角範圍，每次選擇要轉的弧度角都隨機從這個範圍內選
一個值。W 的大小會決定凸起形狀的尖銳程度。 
 繪製碰撞效果線演算法 
SET(X,Y) 
SELECT{θmin、θmax } 
SELECT{W=random()} 
    FOR(θ=θmin  θ<=θmax  θ+=W) 
    { 
        SELECT{Rmin=random()} 
        SELECT{Rmax=random()} 
 
        SET START POINT 
           X= Rmin * SIN(θ-W) 
           Y= Rmin* COS(θ-W) 
 
        SET END POINT 
           X= Rmax * SIN(θ)  
           Y= Rmax * COS(θ )  
        DRAW_LINE(); 
 
        SET START POINT 
           X= Rmax * SIN(θ) 
           Y= Rmax * COS(θ) 
 
        SELECT{W=random()} 
        θ=θ+W 
 
SET END POINT 
           X= Rmin * SIN(θ) 
           Y= Rmin * COS(θ ) 
        DRAW_LINE(); 
        SELECT{W=random()} 
      } 
表 3.7 繪製碰撞效果線的演算法。 
39
 
 
勢沒有運動的連續狀態，而圖片(c)則將人物奔跑的速度感呈現出來。 
 
(a)模仿範例[章魚桔丸子的漫畫
教室 2010] 
(b)選出的關鍵影格 (c)加上動作暗示的關鍵影格
圖 4.3 平行直線-跑(3)。 
   
(a)模仿範例[井上雄彥 2009] (b)選出的關鍵影格 (c)加上動作暗示的關鍵影格 
圖 4.4 平行直線-跳投(1)。 
   
(a)模仿範例[井上雄彥 2009] (b)選出的關鍵影格 (c)加上動作暗示的關鍵影格 
圖 4.5 平行直線-跳投。 
圖 4.4 中，圖片(a)範例圖片中人物呈現向上跳起投籃的狀態。圖片(c)是加
上垂直的平行直線這種動作暗示後的結果圖，讀者可以藉由與圖片(b)尚未加上
動作暗示的關鍵影格比較，圖片(b)只能呈現出人物投籃的姿勢，而圖片(c)則能
將人物跳起投籃的整體的運動感覺表現出來。 
圖 4.5 與圖 4.4 類似，圖片(a)範例圖片中人物呈現向上跳起投籃的狀態。
圖片(c)是加上垂直的平行直線這種動作暗示後的結果圖，讀者可以藉由與圖片(b)
尚未加上動作暗示的關鍵影格比較，圖片(b)只能呈現出人物投籃的姿勢，而圖
片(c)則能將人物跳起投籃的整體感覺表現出來。 
41
 
 
圖片(c)是加上軌跡線這種動作暗示後的結果圖，讀者可以藉由與圖片(b)尚未加
上動作暗示的關鍵影格比較，圖片(b)只能呈現出人物投籃的姿勢，而圖片(c)則
能將人物手腕出手投籃的整體運動感覺表現出來。 
4.3 集中效果線 
  
(a)模仿範例[井上雄彥 2009] (b)選出的關鍵影格 (c)加上動作暗示的關鍵影格 
圖 4.9 集中效果線-灌籃。 
 
(a)模仿範例[Lai 2005] (b)選出的關鍵影格 (c)加上動作暗示的關鍵影格 
圖 4.10 集中效果線-掉落。 
 
 
(a)模仿範例[Scott McCloud 
1994] 
(b)選出的關鍵影格 (c)加上動作暗示的關鍵影格 
圖 4.11 集中效果線-跑。 
圖 4.9 中，圖片(a)範例圖片利用了集中效果線來加強人物灌籃的速度感和
氣氛。圖片(c)是加上集中效果線(1)的結果圖，讀者可以藉由與圖片(b)尚未加上
動作暗示的關鍵影格比較，圖片(b)只能呈現出人物灌籃的姿勢，圖片(c)可以明
顯的感覺到加上集中效果線讓讀者視線聚焦於人物灌籃的動作上。 
43
 
 
 
(a)模仿範例[井上雄彥 2009] (b)選出的關鍵影格 (c)加上動作暗示的關鍵影格 
圖 4.14 集中效果線-灌籃。 
 
(a)模仿範例[井上雄彥 2009] (b)選出的關鍵影格 (c)加上動作暗示的關鍵影格 
圖 4.15 透明度-投籃。 
   
(a)模仿範例[Scott McCloud 
1994] 
(b)選出的關鍵影格 (c)加上動作暗示的關鍵影格 
圖 4.16 碰撞效果線-出拳。 
4.4 透明度漸層 
圖 4.15 中，圖片(a)範例圖片利用了透明度漸層效果來讓人物上肢呈現出手
投籃的連續動作。圖片(c)是加上透明度漸層效果的結果圖，讀者可以藉由與圖
片(b)尚未加上動作暗示的關鍵影格比較，圖片(b)呈現的只是一個標準投籃的姿
勢，圖片(c)加上透明度漸層效果後可以明顯的感覺到人物上肢出手投籃前後的
連續動作。 
4.5 碰撞效果線 
45
 
 
圖 4.17 中，圖片(a)範例圖片利用了軌跡線和碰撞效果線讓人物呈現倒下撞
到地面的感覺。圖片(c)是加上碰撞效果線的結果圖，讀者可以藉由與圖片(b)尚
未加上動作暗示的關鍵影格比較，圖片(b)呈現的只是一個人物平躺的狀態，圖
片(c)加上碰撞效果線後可以明顯的感覺到人物撞擊到地面的感覺，但沒辦法知
道人物是如何倒下。圖片(d)加上軌跡線後，便能完整的表現人物從何處開始倒
下碰撞到地面。 
4.6.2 碰撞效果線和平行直線 
 
(a)模仿範例[井上雄彥 2009] 
 
(b)選出的關鍵影格 
 
(c)加上一種動作暗示的關鍵影格 
 
(d)加上第二種動作暗示的關鍵影格 
圖 4.18 平行直線和碰撞效果線-掉落踏地。 
圖 4.18 中，圖片(a)範例圖片利用了碰撞效果線和平行直線讓人物呈現向下
掉落單腳踏地的感覺。圖片(c)是加上碰撞效果線的結果圖，讀者可以藉由與圖
片(b)尚未加上動作暗示的關鍵影格比較，圖片(b)呈現的只是一個單腳站在地面
的姿勢，圖片(c)加上碰撞效果線後可以明顯的感覺到人物腳步踏到地面的感
覺，但沒辦法呈現掉落的速度感。圖片(d)加上垂直的平行直線效果後，便能完
整的表現人物掉落的速度感和最後踏到地面碰撞的感覺。 
4.6.3 碰撞效果線和平行直線 
圖 4.19 中，圖片(a)範例圖片利用了碰撞效果線和平行直線讓人物呈現向前
奔跑踏地的感覺。圖片(c)是加上碰撞效果線的結果圖，讀者可以藉由與圖片(b)
47
 
 
 
(a)模仿範例[Scott McCloud 1994] (b)選出的關鍵影格 
 
(c)加上一種動作暗示的關鍵影格 
  
(d)加上第二種動作暗示的關鍵影格 
圖 4.20 平行直線和透明度-跑。 
49
 
 
Symposium on Non-photorealistic Animation and Rendering, 133- 140. 
BROSTOW, G. J., ESSA, I. 2001. Image-based Motion Blur for Stop Motion 
Animation. In Proc. The 28th Annual Conference on Computer Graphics and 
Interactive Techniques, 561–566. 
CMU. 2010. Carnegie Mellon University, Graphics Lab Motion Capture Database. 
http://mocap.cs.cmu.edu/. 
COLLOMOSSE, J., ROWNTREE, D., AND HALL, P. M. 2005. Rendering 
Cartoon-style Motion Cues in Post-production Video. Graphical Models 67, 6, 
549-564. 
CUTTING, J. E. 2002. Representing Motion in a Static Image: Constraints and 
Parallels in Art, Science, and Popular Culture. Perception 31, 1165–1193. 
GOLDMAN, D. B., CURLESS, B., SALESIN, D., SEITZ, S. M. 2006. Schematic 
Storyboarding for Video Visualization and Editing. ACM Transactions on 
Graphics 25, 3, 862-871. 
HSU, S. C., LEE, I. H. H. 1994. Drawing and Animation Using Skeletal Strokes. In 
Proc. The 21st Annual Conference on Computer Graphics and Interactive 
Techniques, 109-118.  
HWANG, W. I., LEE, P. J., CHUN, B. K., RYU, D. S., CHO, H. G. 2006. Cinema 
Comic: Cartoon Generation from Video Stream. In Proc. GRAPP ’06, 299-304. 
JHU, S. J. 2004. Study of the Application of the Comics on Animation-Using the 
Cartoon style and Comics System as the Key.  Master’s thesis, Multimedia and 
Animation Arts Department, National Taiwan University of Arts, Taiwan. 
KATO, Y., SHIBAYAMA, E., TAKAHASHI, S. 2004. Effect Lines for Specifying 
Animation Effects. In Proc. IEEE Symposium on Visual Languages and 
Human-Centric Computing, 27-34. 
KAWAGISHI, Y., HATSUYAMA, K., KONDO, K. 2003. Cartoon Blur :  
Non-photorealistic Motion Blur. In Proc. Computer Graphics International ’03, 
276-281. 
LAI, Y. H. 2005. The Synthesis of Non-Photorealistic Motion Effects for Cartoon. 
Master’s thesis, Computer Science Department, National Chiao Tung University, 
Taiwan. 
LASSETER, J. 1987. Principles of Traditional Animation Applied to 3D Computer 
Animation. In Proc. The 14th Annual Conference on Computer Graphics and 
Interactive Techniques, 35-44. 
MASUCH, M., SCHLECHTWEG, S., SCHULZ, R. 1999. Speedlines: Depicting 
Motion in Motionless Pictures. In ACM SIGGRAPH’99 Conference Abstracts and 
Applications, ACM Press, New York, USA, 277. 
MCCLOUD, S. 1994. Understanding Comics, Harper Paperbacks Press.  
51
 
 
計畫成果自評 (Self assessment) 
 
1. 本計畫是延續過去多年在 3D 動畫之研究成果，請參看國科會個人資料中本
人之著作目錄。本計畫所提出之動作圖示產生之方法，完成項目包含下列: 
一、動作暗示 (motion cue) 種類之調查 
二、動作暗示適用情境分析 
三、動作暗示繪製之演算法設計 
四、互動式介面設計 
五、演算法與雛型系統實作 
六、產生之動作圖示結果評估 
 
2. 針對本研究，以下列舉了幾項目前研究結果可以再加強的方向: 
 目前線條的樣式太過於單調，漫畫家所繪製的線條有著各種不同的筆
觸；然而現今也有許多研究是針對去模擬不同的筆觸，若能利用這些研究，
將不同的筆觸效果應用在我們的系統中，將可以提升系統繪製動作暗示的效
果。 
 目前各種動作暗示繪製為通用型，未來可以針對某種特定應用或是資料
(例如不同類型的人物運動)來模組化，使得產出流程更有效率。 
 未來希望能夠透過分析關鍵影格在動作資料中所表現的動作行為，讓系
統自動化判斷要繪製何種動暗示在影格上；最後，透過這些分析將動作行為
的表現程度(移動速度快慢和轉動角度大小等等)量化，並根據這些資訊自動
的設定好動作暗示繪製的參數。 
 
3. 本計畫之結果計畫將投稿至知名之國際會議及學術期刊。 
53
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
 
現今動畫及遊戲產業的製作過程中，動作資料(Motion Capture Data)已經被
廣泛地應用。隨著動作資料的普及動作資料庫的資料量也與日俱增變得十分
龐大，因此在本計劃的幾個核心技術適合應用於下列幾個重要課題上： 
(1)有效的動作查詢功能成為當前動作資料庫的重要研究課題之一，而查詢功
能的核心技術就是關鍵影格(Key Frame)的找尋與表示法。 
(2)資料量大小一直是多媒體資料(如動畫、視訊與聲訊資料等)的基本問題，
藉由摘要技術，可以將動畫內容作一個彈性的安排，不論是多精度(level of 
detail)的表示或是做為動畫內容轉換(animation transcoding)機制的方
法，皆可提升動畫內容的使用率。 
(3)動畫角色鏡頭規劃：因此目前不論在遊戲產業、數位內容導覽甚至延伸擴
展到醫學應用瀏覽上(如物理復健與運動訓練)，若可以加以利用好的鏡頭規
劃，將可提高各種應用實施之效率。 
(4)動畫角色照明規劃：照明規劃可以應用在舞台設計、電影內容的成像效果
與科學資料中重要資訊的凸顯。 
(5)產生動畫角色之動作暗示：本技術採用目前廣為年輕族群所熟悉的一種內
容呈現方式-漫畫效果，提供了一個新的視覺呈現方式可用於搭配創意構思來
產生具易讀性與簡潔性的數位內容。 
 
55
國科會補助專題研究計畫項下出席國際學術會議心得報告 
一、參加會議經過 
此國際會議是繼續過去三屆在歐美各國舉行的國際影像與訊號處理研討會，本年度研討會除
了由國際圖形識別學會 International Association for Pattern Recognition (IAPR)及魁北克省主要
幾所大學聯合贊助外，協辦單位還包括 International Journal on Graphics, Vision and Image 
Processing(GVIP)。本研討會主要是探討近年來在影像與訊號處理，多媒體（Multimedia）與計算
機圖學等領域之最新成果與未來發展。今年會議地點是在加拿大魁北克省的三叉河城(Trois- 
Rivieres, Quebec, Canada)舉行。在三天中來自各國的研究人員聚集一堂，共同分享討論各研究單
位在相關議題之研發成果。本次大會共計錄取六十五篇論文以論文宣讀來呈現當前最具前瞻性的
研究成果。主要議程可分為下列三大類： 
1. Keynote Speeches 
2. Technical Papers 
3.   Demonstration(系統展示) 
會議議程分成多個主題，各個主題分別進行論文宣讀。在論文宣讀的部分，每個主題有六至
七篇論文進行宣讀，每篇論文有十七分鐘的時間。每個場次都吸引許多學者參與討論。此外，每
天上午大會安排一場 keynote Speech，邀請知名學者針對目前最新的研究課題進行演說，聽眾反
應熱烈。 
大會第一天進行的主題包括 Image Filtering, Image Coding, Pattern Recognition, Biometry, Posters 
等。第二天進行的主題包括 Signal Processing, Video coding and Processing, Watermarking and Document 
Processing 等，而第三天除了 Computer Vision, Biomedical Application 外還有一些演釋的特別場次。本
大會所宣讀的論文除了在影像處理，訊號處理，醫學影像等之原理與技術外還加了一些電腦視覺與圖
形識別的論文。除了論文發表者外大會還邀請多位知名的 Keynote Speakers 一起探討上述主題，使得
計畫編號 NSC 96－2221－E－033－073－MY3 
計畫名稱 角色動畫之自動摘要與可視化之研究 
出國人員
姓名 楊熙年 
服務機構
及職稱 中原大學 資訊工程系教授 
會議時間 2010 年 6 月 30 日至2010 年 7 月 02 日 會議地點 Trois- Rivieres, Quebec, Canada 
會議名稱 
(中文)第四屆國際影像與訊號處理研討會(2010 年) 
(英文)International Conference on Image and Signal Processing (ICISP 2010 ) 
57
(Université de Montréal, Canada)的 Caroline Rougier 所提出的單一相機人頭追蹤演算法(3D Head 
trajectory Using a Single Camera) 較為有趣。下午場次為醫學影像處理與應用，因其內容較偏應
用，有些又需要醫學專業知識，幸好過去曾研究過 CT 影像，故學習的重點均集中在影像處理技
術上。 
這次會議離加拿大大城 Montreal 需兩個小時車程，交通並不算方便，但有公車接駁還算可以。
來自世界各國家之學者專家，經三天多的討論切磋，相關問題包括影像處理(Image Processing)，
訊號處理(Signal Processing)，醫學影像(Medical Image)，圖形識別(Pattern Recognition)及電腦視覺 
(Computer Vision)等。參加本次會議除了在影像處理與電腦視覺之知識上獲益良多外，更在視訊
編碼(Video Coding)領域中有關事件偵測(Event Detection)與內容分析(Content Analysis)的技巧上有
更新的認識。 
三、考察參觀活動(無是項活動者略) 
  無該項活動 
四、建議 
  國內有 CVGIP 之年會，行之多年，亦建立良好口碑，建議參考加拿大之作法，請有關單位
如國科會等予以補助成為國際會議。 
五、攜回資料名稱及內容 
ICISP Conference Proceedings 以及相關之磁碟片。 
 
59
2 Ke-Sen Huang et al.
vey the characteristics of the given data?”. To cope with
this problem, there are two approaches, namely, ”user-
specified” and ”automatic”.
The main feature of user-specified approaches [18,21,
19,16,6,12] is the provision of an interactive mechanism
for specifying user requirements. The common concept
of Poulin and Fournier [18], Schoeneman et al. [21] and
Poulin et al. [19] is providing a user-interface for sketch-
ing (painting) the shadows and highlights, and then their
systems will compute or adjust the parameters and po-
sitions of light sources.
Marks et al. [16] proposed a system ”Design Gal-
leries” which allows users to create their lighting design
by specifying a pair of input and output vectors. The
input vector includes a light position, a light type, and
a light direction. The output vector contains the values
of desired pixel luminances. Using these initial settings
(input vectors) and desired result (output vectors), they
compute the final arrangement of light sources. Similar
to the concept presented by Marks et al. [16], Costa et
al. [6] also proposed a system which accepts a pair of
design conditions and its design goal for their lighting
design. The proposed framework also allows users to pro-
vide their cost function for adjusting the desired results.
Halle and Meng [12] proposed a system based on
three-point lighting which is a well-known lighting tech-
nique used in both still photography and film [2,1,4].
This system also allows the user to adjust the color and
warmth of lighting.
The automatic methods usually construct their light-
ing algorithm based on the characteristics of given data
[22,11,15,20]. For example, curvature is an important
metric for the surface detail of a 3D model. Shacked
and Lischinski [22] presented an image-based approach
to measure a lighting design. There are six terms for
measuring the local luminance, pixel luminance statis-
tics, and illumination direction of the rendered image.
Gumhold [11] adopted the concept of entropy to eval-
uate a light design. He presented a concept of ”illumi-
nation entropy” which can be calculated based on the
brightness information of the rendered image. Intuitively,
larger entropy means the rendered image can reveal more
information about the scene.
Lee et al. [15] first segmented the input model by
using a curvature-based-watershed method. Once a set
of patches is obtained, the light-placement function sets
appropriate light directions for illuminating the model
according to the diffuse and specular properties at every
vertex. Then lights are also assigned to patches based on
the light-placement function. In order to enhance fea-
tures of the model, silhouette lighting and proximity
shadows are also added. Rusinkiewicz et al. [20] adopted
a shaded relief and local lighting model for depicting
both overall shape and fine-scale detail. Instead of search-
ing for where to put the lights, they used a deterministic
criterion for light positioning that allows users to com-
pute an effective light direction per vertex.
3 Stage Lighting
3.1 Building the stage
In this paper, our scenario assumes that the given motion
is performed on a virtual stage (Figure 1(g) or 1(h)).
Therefore, we first design global stage lighting for the
given motion data. To build a virtual stage, we consider
how to determine the size of the stage and the stage
orientation (which faces the audience).
Computing acting area. Acting areas are ”those
spaces on the stage where specific scenes are played” [9].
In this paper, the shape and size of the acting area is
determined by the bounding box of the given motion
data1. The floor of Figure 1(a) is an example of an acting
area.
Determining the stage orientation. This step
consists of line of interest (LOI) construction and view
entropy.
1. The line of interest (LOI) (the purple plane of Fig-
ure 1(b)) is an imaginary line which divides the scene
into two parts and guides the performance of actors
[3,1]. In general, for a cinematographer, the LOI is
drawn by the director and all shots are taken on one
side of the LOI to maintain the consistency of vi-
sual flow. This characteristic matches our scenario
because we assume that the viewer is the audience
in front of the stage. Therefore, we follow this rule
to determine the direction of the stage. To construct
LOI for a given motion data, we propose the following
procedure:
(a) We use a plane equation ax + by + cz + d = 0
to represent the LOI in the scene. The vector N
(a, b, c) is the normal of this plane.
(b) To consider the scene of the given motion, we com-
pute the center (cx, cy, cz) of the acting area (the
bounding box) according to the root positions of
the sequence. We assume the center is a point on
the LOI.
(c) For the distribution of body orientations, we ap-
ply PCA [14] to all root orientations (only us-
ing the x and z components) of the given mo-
tion. Then we can obtain the second eigenvector
(ex, 0, ez) as the direction of LOI.
(d) Now, we have the normal to the LOI ((a, b, c) =
(ex, 0, ez)) and a point (cx, cy, cz) that lies on the
LOI. Hence, we can obtain the LOI.
2. To determine which side of the LOI is the stage ori-
entation, we define the concept of viewpoint entropy,
similar to previous works [23,5,13]. In contrast to the
original version [24] which evaluates the viewpoint
quality under perspective projections, we formulate
viewpoint entropy according to the face direction of
1 If there are some auxiliaries such as the ladder in Fig-
ure 1(a), we will take them into account to compute the
bounding box.
4 Ke-Sen Huang et al.
(a) (b) (c) (d)
Fig. 3 The representation of sampling space used in our method. (a) The sampling space (left) and its front view (middle)
and top view (right). (b) Sampling space of key light. (c) Sampling space of fill light. (d) Sampling space of rim light. The
brown plane of (b)-(d) presents the Y-Z plane of sampling sphere (a).
Spoty = hbdbox +
d ∗ tan(β − α)
1− tan(Π2 − α− β) ∗ tan(β − α)
Spotz = Cz + (
d
2
+BD) ∗ Vdiaz
where BD = (Spoty-hbdbox) * tan(Π2 - α - β). Once the
positions of global lights are obtained, we will adjust the
height of the stage and decorate it with global lights
(Figure 1(e)).
4 Character Lighting
Although there is no absolute rule for us to determine the
position of a light source, the light source could always
be placed according to previous experience. In general,
the primary light source could be placed based on the
relative positions of the subject and camera [2,4]. Ob-
viously, different light source positions have different ef-
fects. Roughly speaking, these are known as”front light”,
”back light”, ”side light”, ”top light”, ”bottom light” ac-
cording to the relative position between light and sub-
ject. In this paper, we allocate various light sources ac-
cording to their functions. They are key light, fill light,
rim light and kicker light.
4.1 Light Source Placement
Key light. This is the primary light source and usually
shoots the subject with higher intensity directly. It also
plays an important role in presenting the personality of
the subject and the mood of the scene. A lighting de-
signer could control what he or she wants to accentuate
or not by loosely controlling the key light. If there is no
particular need, three-quarters-back key light (placed 30
degree to 45 degree to the side relative to the camera
axes) is a common placement. We introduce an auto-
matic method to place a key light based on the following
equation:
Lkey = arg max
posi∈Ωkey
Lik (3)
where Lik = w
1
kE(pi) + w
2
kN(pi) +w
3
kI(pi). i = 1,2,. . . n
and w1k, w
2
k and w
3
k are constant.
Fig. 4 The STD block examples for Armadillo, Greek and
WoodDoll.
– posi is the position of the ith sampled light source.
pi is the image rendered by the ith sampled light.
– Sampling space (Ωkey): The sampling space is (zenith
of 30◦ ∼ 60◦, azimuth of −60◦ ∼ 60◦). Figure 3(b)
shows the sampling space of the key light.
– Entropy measurement E for maximum information:
Here we follow the entropy proposed by Gumhold
[11].
– Luminance measurement N and I for essential illu-
mination:
– N : We calculate the number of pixels whose lu-
minance is larger than a user-specified constant.
– I: We calculate the summation of the intensity2
of all pixels of the subject rendered in an image.
Fill light. After a key light has been placed, the high
intensity light will be accompanied by some sharp shad-
ows. To fill in the sharp shadow areas, a diffuse light
with lower intensity works well. A fill light should neither
result in noticeable shadows, nor a noticeable or sharp
specular highlight. According to three-point lighting, the
position of the fill light is usually on the opposite side of
the key light relative to the camera axes.
Lfill = arg min
posi∈Ωfill
Lif (4)
where Lif = w
1
fΣ
m
j=1STD(b
j
i ) + w
2
fGRAD(pi). i= 1,2,. . .
n and w1f , w
2
f are constant.
– posi is the position of ith sampled light source. pi is
the image rendered by ith sampled light.
2 The color space used in this paper is sRGB. We trans-
form color space from RGB to CIS sRGB (Y = 0.21262R +
0.71514G+ 0.07215B).
6 Ke-Sen Huang et al.
Fig. 7 The motion data Somersault is segmented into three
sub-sequences (green, yellow and green) depending on the
acting area of the root. The midpoint of each sub-sequence
is a keyframe (frame 11, 44 and 108). So, the total keyframe
number of Somersault is five if considering the first and the
last frame.
4. We add a kicker light based on equation 6.
Besides presenting the method to place various light sources,
we also handle their parameter settings including color,
distance and intensity.
– In this paper, we only use white light to illuminate
data.
– For the light distance, we construct a lighting sphere
on which various lights are placed. In our case, we
choose the neck as the center of the lighting sphere.
Then the radius of the lighting sphere is the distance
which makes the lighting sphere just cover the whole
character.
– For light intensity, we follow the idea used by LightKit
[12], where each light intensity is expressed as a ra-
tio relative to the key light. The used ratio is ”key :
fill : rim : kicker” = ”12 : 6 : 4 : 2”. And the ratio
of lights of the same type is 2 : 1. For example, if
there are two fill lights then ”fill1:fill2” is ”6:3”.
5 Lighting Interpolation
Once the configurations of global lighting and the local
lighting are obtained, a possible solution is to calculate
all parameters of all frames. However, we still need to
consider the flickering effect. As pointed out by El-Nasr
et al. [8], this is because the parameters we obtain in
each frame are not continuous in the spatial domain. In
other words, the calculated positions of the light sources
are unpredictable and can even hop from frame to frame.
To handle flickering effects, we propose a lighting inter-
polation scheme as follows:
1. To maintain the consistency of parameters in both
temporal and spatial domains, an acting-area is di-
vided into some sub-acting areas. In the current im-
plementation, a sub-acting area is a square and its
edge length is determined by the longest distance be-
tween two joints of a character.
2. Given a motion sequence with n frames, we segment
it into m sub-sequences according to the scene plan-
ning and the position of a character’s root (Figure 7).
3. In order to facilitate interpolation, we unify the num-
ber of lights (γ) for each frame of the sequence as
follows:
Fig. 8 Light blue circles: view spheres of the k-th and the
k+1-th keyframe. Purple circle: the view sphere whose char-
acter lighting needs to be interpolated. Given the information
of the view spheres whose frame numbers are fk, j and fk+1,
we calculate the light source position (lqj ) by linear interpola-
tion of the vectors, vqfk and v
q
fk+1
whose directions are from
their view sphere center to their character light source. After
the linear interpolation (Equation 7), we can easily find the
resampled vector vqj and the light source position l
q
j .
(a) We pick m+2 keyframes (the first frame, the last
frame and the middle frame of m sub-sequences)
and compute their configurations {Φk| k = 1, 2,
. . ., m+1, m+2 }. Each lighting configuration con-
sists of (1) the number of lights, (2) the type of
each light and (3) its position. And lqk represents
the qth light position in Φk.
(b) We divide these configurations (m+2) into groups
based on their number of lights. Then γ is set to
the number of lights of the majority group, that
is, the group with maximum number of configu-
rations. For example, m+2 configurations are di-
vided into two groups such that one has n1 mem-
bers and needs five lights and the other contains
n2 (n1+n2 = m+2, n2 > n1) members and needs
six lights, then γ is set to six.
(c) We recompute those configurations whose number
of lights is unequal to γ to keep all configurations
with the same number of lights.
4. As Figure 8 shows, we use the lighting configurations
of two consecutive keyframes to interpolate the light-
ing configuration of those frames which lie between
them. The following procedure shows how to compute
the jth frame’s lighting configuration (Φj) based on
two lighting configurations Φk and Φk+1.
Φj = {lqj |q = 1, 2, . . . , γ} fk < j < fk+1 (7)
lqj = rj ∗
vqj
|vqj |
+ cj
vqj =
(fk+1 − j) ∗ vqk + (j − fk) ∗ vqk+1
fk+1 − fk
fk is the frame number of lighting configuration Φk.
rj is the radius of lighting sphere of jth frame. cj
8 Ke-Sen Huang et al.
(a) Greek model (b) Armadillo model (c) WoodDoll model
Fig. 9 The position of various light sources and the corresponding results for Greek, Armadillo and WoodDoll.
(a)
(b)
Fig. 10 We show the selected frames of somersault data rendered by (a) somersault and (b) playground.
(a) Maya default setting without auxiliary scene.
(b) Our method without auxiliary scene.
(c) Maya default setting with auxiliary scene.
(d) Our method with auxiliary scene.
Fig. 11 We show the selected frames of rendered slam-dunk data and the order of legibility is (d) > (c),(b) > (a).
(a) (b)
Figure 2: (a) The LOI plane. (b) The selected half LOV is used to
place three cameras.
2 Camera Placement for Motion Data
2.1 The Line of Interest
The director usually considers the scene or the moving path which
is surrounded by the Circle of Action [Katz 1991] and an invisible
line which divides the circle into two parts. This invisible line is
the line of interest (LOI). The director usually places the cameras
on one side of the line to avoid confusing audiences. In this paper,
we describe a computation model to obtain LOI for a given motion
data as follows:
1. We use a plane equation ax + by + cz + d = 0 to represent
the LOI in the scene. This plane is perpendicular to the stage.
The vector N (a, b, c) is the normal of this plane.
2. To consider the scene of the given motion, we compute the
center (cx, cy, cz) of the bounding box according to the root
positions of the sequence. We assume the center is a point on
the LOI.
3. For the distribution of body orientations, we apply principal
components analysis [Jolliffe 2002] to all root orientations
(only using the x and z components) of the given motion.
Then we can obtain the second eigenvector (ex, 0, ez) as the
direction of LOI.
4. Now, we have the normal to the LOI ((a, b, c)=(ex, 0, ez))
and a point (cx, cy, cz) that lies on the LOI. Hence, we can
obtain the LOI.
After above procedures, the Line-of-Vision (LOV) sphere1 was di-
vided into two halves (Figure 2(a)). The possible camera positions
are limited to half sphere relative to LOI.
2.2 The Triangle Principle
To construct a triangle arrangement of camera, we define the no-
tion of motion viewpoint entropy to determine which side of LOI
is suitable for placing cameras. Then we also use proposed en-
tropy to place three cameras at this side. Similar to previous works
[Takahashi et al. 2005; Bordoloi and Shen 2005; Ji and Shen 2006],
we also follow the concept of viewpoint entropy introduced in
[Va´zquez et al. 2001]. We formulate viewpoint entropy as the fol-
1The viewing sphere of a scene (motion) is referred to an imaginary
sphere which encompasses the scene and the viewpoints are moving on the
surface of the sphere.
(a) (b)
(c) (d)
Figure 3: (a) Divide the half sphere into 12 fields every 30 degree in
azimuth and every 90 degree in elevation; (b) the camera placement
type I; (c) the camera placement type II; (d) the green transparent
triangular areas represent the angle of reflection.
lowing equation:
H(f) = −
n∑
i=1
pi(f) log2 pi(f) f ∈ {fa, fb} (1)
where n is the frame number of the given motion sequence. f is
the viewing direction and fa and fb are the two fields divided by
the LOI. Let pi(f) be the probability of viewpoint i appearing in
view direction f . We can determine the proper viewing fields by
the following equation:
arg max
f∈{fa,fb}
H(f) (2)
Figure 2(b) shows the selected half LOV sphere. To place the first
camera (apex viewing vector), we parameterize the selected view
field (fa or fb) into 12 sub-fields every 30 degree in azimuth and
every 90 degree in elevation (Figure 3(a)). Then we compute the
entropy of sub-view fields by the following equation:
arg max
f∈{f1s ,f2s ,...,f12s }
H(f) (3)
where fs is the selected view field. Then we can obtain the apex
viewing vector by averaging the vectors in the chosen viewing field.
Consequently, we need to compute another two vectors by using the
apex viewing vector. Our approach is to set the two external cam-
eras on the two ends of LOI (0◦ and 180◦) as Figure 3(b) shown.
However, if the included angle between the apex viewing vector and
LOI is small than 30 degrees, we set (1) apex camera as an external
camera, (2) another external camera is located based on its angle
of reflection (Figure 3(d)), and (3) the real apex camera is perpen-
dicular to it, that is we take the right angle positions of the triangle
principle into consideration. Figure 3(c) shows this condition.
3 Motion Cut and View Selection
3.1 Motion Cut
The feasibility of segmenting motion data by using zero-crossing of
the second derivative has been shown by the previous works [Bindi-
ganavale and Badler 1998]. And it is also useful in our situation
(a) The resulting shot of slam dunk motion. (b) The fixed shot of slam dunk motion.
Figure 6: The resulting camera placements and snapshots of slam dunk and somersault sequences.
determined by the number of motion segments. Then, we illustrated
the resulting shots of the motion by comparing with the fixed shot
of the motion.
Figure 5 shows the resulting camera placements of a slam dunk mo-
tion (348 frames). The purple lines indicates the triangle cameras’
look-at vectors (the default sets), and the two green lines (slam dunk
motion was segmented into two segments) indicates our resulting
cameras’ look-at vectors. Note that the two green lines are different
from the left and right external look-at (purple lines) respectively,
because we made some further adjustments. Further, in order to
demonstrate the resulting movie on paper, the resulting shots is il-
lustrated by keyframes selected by hand as shown in Figure 6(a),
and its frame numbers corresponds to fix shot as shown in Figure
6(b). The fixed camera was inspired by three-quarter view, and the
camera’s look-at vector was the center of motion bounding box mi-
nus the top left vertex of motion bounding box. For more generated
results, we refer the reader to the supplementary video.
6 Conclusion
In this paper, we have presented a cinematography-based automatic
view selection system about human motion data. The cinemato-
graphic rules not only can be considered as filters to classify the
unit sphere, but as constraints to maintain continuity for generat-
ing the good candidate views. After the shot selection from candi-
date views which is achieved by occlusion property (we prefer the
shot with minimum occlusion of a player), we imposed the artistic
compositional rules; thus a human motion can be represented as a
well-edited movie, and the motion from shot to shot looks smooth.
Limitations. In addressing the cinematic scene, our method had
certain limitations. First, instead of exploring the overall contin-
uum of possible viewpoints, we only search over a finite set of can-
didate views; in other words, we might miss the best view. Second,
our method can not film dialogue shot because of the non-narrative
motion without the information of dialogue. Finally, for dynamic
motion data, we have to consider an additional dimension of time;
thus, it might be needed another optimization method, such as ab-
straction of model or hardware acceleration.
Acknowledgements
This work was supported by the National Science Council of Tai-
wan under the numbers NSC96-2221-E-033-073-MY3.
References
ABLAN, D. 2002. Digital Cinematography & Directing. New
Riders Press.
ARIJON, D. 1976. Grammar of the Film Language. Hastings
House Publishers.
ASSA, J., COHEN-OR, D., YEH, I.-C., AND LEE, T.-Y. 2008.
Motion overview of human actions. ACM Transactions on
Graphics 27, 5, 115:1–115:10.
BINDIGANAVALE, R., AND BADLER, N. I. 1998. Motion ab-
straction and mapping with spatial constraints. In Proceedings
of the International Workshop on Modelling and Motion Capture
Techniques for Virtual Environments, 70–82.
BORDOLOI, U. D., AND SHEN, H.-W. 2005. View selection
for volume rendering. In Proceedings of the IEEE Visualization
2005, 487–494.
CHRISTIE, M., AND NORMAND, J.-M. 2005. A semantic space
partitioning approach to virtual camera composition. Computer
Graphics Forum 24, 3, 247– 256.
CHRISTIE, M., AND OLIVIER, P. 2006. Camera control for com-
puter graphics. Eurographics 2006 State of The Art Report, 89–
113.
E´RIC MARCHAND, AND COURTY, N. 2002. Controlling a camera
in a virtual environment. The Visual Computer 18, 1, 1–19.
GOOCH, B., REINHARD, E., MOULDING, C., AND SHIRLEY, P.
2001. Artistic composition for image creation. In Proceedings
of the 12th Eurographics Workshop on Rendering Techniques,
83–88.
HAWKINS, B. 2005. Real-time cinematography for games. Charles
River Media.
 1
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                              日期：2010 年 08 月 01 日 
一、參加會議經過 
此國際會議是繼續過去三屆在歐美各國舉行的國際影像與訊號處理研討會，本年度研
討會除了由國際圖形識別學會 International Association for Pattern Recognition (IAPR)及
魁北克省主要幾所大學聯合贊助外，協辦單位還包括 International Journal on Graphics, 
Vision and Image Processing(GVIP)。本研討會主要是探討近年來在影像與訊號處理，多媒
體（Multimedia）與計算機圖學等領域之最新成果與未來發展。今年會議地點是在加拿大
魁北克省的三叉河城(Trois- Rivieres, Quebec, Canada)舉行。在三天中來自各國的研究人員
聚集一堂，共同分享討論各研究單位在相關議題之研發成果。本次大會共計錄取六十五篇
論文以論文宣讀來呈現當前最具前瞻性的研究成果。主要議程可分為下列三大類： 
1. Keynote Speeches 
計畫編
號 
NSC  96－2221－E－033－073－MY3 
計畫名
稱 
角色動畫之自動摘要與可視化之研究 
出國人
員姓名 楊熙年 
服務機
構及職
稱 
中原大學 資訊工程系教授 
會議時
間 
2010 年 6 月 30
至 
2010 年 7 月 2
日 
會議地
點 
Trois- Rivieres, Quebec, Canada 
會議名
稱 
(中文)第四屆國際影像與訊號處理研討會(2010 年) 
(英文) International Conference on Image and Signal 
Processing (ICISP 2010 ) 
附件四 
 3
Overview 是許多入門者常閱讀的文獻。其內容對目前我在Video Content Analysis 之研究有
所助益。當天上午同時進行的還有訊號處理之場次，包括(Sampling Filter，Tracking，Dynamic 
Systems)等內容。當天下午第一場的主要課題是Video Coding and Processing，其內容包括
有火焰特徵偵測(Fire Detection)，容錯視頻編碼(Error Resilient Video Coding)，視訊品質預
估(Video Quality Prediction)，失衡偵測(Fall Detection)等有關視訊內容之特徵偵測演算法。
雖然這些論文大多是涉及動態的物件，與靜態影像在資料結構上十分不同，但在影像處理
的技術上是有相通之處，尤其與動畫資訊更有類似之處，其處理技巧可作為Motion Data 
Content Analysis 之借鏡。當天最後一場是有關浮水印( Watermarking )與  文件處理
( Document Processing) ，基本上它們都是研究文件處理與資訊隱藏(Information Hiding)的
技術，其中以來自法國Institut de Recherche en Informatique de Toulouse 所發表的多結構文
件分析較為有趣。 
第三天上午的演講，是由Siemens Corporate Research的Leo Grady主講，他的專長基本
上是Computer Vision 中 離散數學結構，雖然數學分析的成份較多，不過其內容可做為電
腦動畫中動作分析技術之參考。上午場次以電腦視覺( Computer Vision)的論文為主，其中
以來自蒙特羅大學(Université de Montréal, Canada)的Caroline Rougier 所提出的單一相機人
頭追蹤演算法(3D Head trajectory Using a Single Camera) 較為有趣。下午場次為醫學影像處
理與應用，因其內容較偏應用，有些又需要醫學專業知識，幸好過去曾研究過CT影像，故
學習的重點均集中在影像處理技術上。 
這次會議離加拿大大城 Montreal 需兩個小時車程，交通並不算方便，但有公車接駁還
算可以。來自世界各國家之學者專家，經三天多的討論切磋，相關問題包括影像處理
(Image Processing)，訊號處理(Signal Processing)，醫學影像(Medical Image)，圖形識別
(Pattern Recognition)及電腦視覺 (Computer Vision)等。參加本次會議除了在影像處理與
電腦視覺之知識上獲益良多外，更在視訊編碼(Video Coding)領域中有關事件偵測
(Event Detection)與內容分析(Content Analysis)的技巧上有更新的認識。 
三、考察參觀活動(無是項活動者略) 
  無該項活動 
四、建議 
  國內有 CVGIP 之年會，行之多年，亦建立良好口碑，建議參考加拿大之作法，請有
關單位如國科會等予以補助成為國際會議。 
五、攜回資料名稱及內容 
ICISP Conference Proceedings 以及相關之磁碟片。 
 
96年度專題研究計畫研究成果彙整表 
計畫主持人：楊熙年 計畫編號：96-2221-E-033-073-MY3 
計畫名稱：角色動畫之自動摘要與可視化之研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 0%  
研究報告/技術報告 3 3 100%  
研討會論文 4 4 100% 
篇 
 
論文著作 
專書 0 0 0%   
申請中件數 0 0 0%  專利 已獲得件數 0 0 0% 件  
件數 0 0 0% 件  
技術移轉 
權利金 0 0 0% 千元  
碩士生 4 4 100%  
博士生 1 1 100%  
博士後研究員 0 0 0%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 0% 
人次 
 
期刊論文 0 0 0%  
研究報告/技術報告 0 0 0%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 0% 章/本  
申請中件數 0 0 0%  專利 已獲得件數 0 0 0% 件  
件數 0 0 0% 件  
技術移轉 
權利金 0 0 0% 千元  
碩士生 4 4 100%  
博士生 1 1 100%  
博士後研究員 0 0 0%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 0% 
人次 
 
 
