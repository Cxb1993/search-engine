In feature ranking, we adopted the new idea of false 
feature to rank features based on their importance, 
and applied SFS to features that are less important 
and of lower rank. By doing so, we not only overcame 
issues with the original SFS but also extracted more 
critical feature subsets. 
 
英文關鍵詞： feature selection, feature ranking, , false feature, 
machine Learning, k-NN 
 
 1 
結合特徵排序的改良式浮動序列特徵擷取演算法 
Modified Sequential Floating Search Algorithm with a Novel Ranking Method 
 
計畫編號：NSC 100-2221-E-32 -069 
執行期限：100 年 08 月 01 日 至 101 年 09 月 31 日 
主持人：  周建興 助理教授  淡江大學電機工程系 
計畫參與人員：  戴賢榜     淡江大學電機工程系 
    簡義翔     淡江大學電機工程系 
    陳克威     淡江大學電機工程系 
e-mail：chchou@mail.tku.edu.tw 
 
一、摘要 
模式識別的研究領域中，特徵擷取演算
法是一個十分重要的研究主題。例如在 DNA
序列的分析研究中，透過特徵擷取可以找到
序列中可能導致疾病的段落位置或是氨基酸
種類；在 microarray 的資料中選擇可能導致
某種疾病的基因；又譬如在文件分類的問題
中，找出真正有助於分類的關鍵字詞為何。
此外，透過特徵擷取，不但可以選出使用者
感興趣的特徵，還可以減少分類器訓練與測
試所花費的時間與運算量，以及資料儲存時
所需的容量。 
在眾多的特徵擷取方法中，Sequential 
floating search (SFS) 可說是相當知名且被廣
泛使用的方法。在此計畫中，我們提出一個
結合特徵排序(feature ranking)與 SFS 的特徵
擷取方法。在特徵排序階段，我們使用假特
徵(false feature)的觀念，來排序出(rank)特徵
的重要度，然後挑選出排名後重要性較低的
特徵，進行 SFS 演算法。這樣的作法，一方
面可以克服傳統 SFS 演算法可能遭遇到的問
題，另一方面還能夠擷取出更關鍵性的子特
徵集合。 
關鍵詞：特徵選取、特徵排序、假特徵、機
器學習、k 最近鄰居法 
 
Abstract 
In issues of pattern classification, choice 
of a suitable feature selection method is often 
the key to success. A successful feature 
selection not only raises the classification 
accuracy but also extracts the critical features 
that users are concerned with. For instance, in 
the analysis research of DNA sequence, 
feature selection enables us to locate the 
segments on the sequence that may lead to 
certain diseases and types of amino acids; and 
to select the gene that may lead to certain 
diseases from the data of microarray. As 
another example in text categorization issues, 
feature selection enables us to extract the 
keywords that contributed to classification. In 
addition, using feature selection we can not 
only select the features that users are most 
interested in but also save time while training 
and testing of the classifier, as well as memory 
space for data storage. 
Of the various feature selection methods, 
sequential floating search (SFS) is well 
known and widely adopted. In this project, 
we propose a feature selection method 
combining feature ranking and SFS. In 
feature ranking, we adopted the new idea of 
false feature to rank features based on their 
importance, and applied SFS to features that 
are less important and of lower rank. By 
doing so, we not only overcame issues with 
the original SFS but also extracted more 
critical feature subsets. 
 
Keywords: feature selection, feature ranking, , 
false feature, machine Learning, k-NN  
 
二、計畫緣由目的與文獻探討 
 
在模式識別的研究領域中，特徵擷取演
 選 取 演 算 法 (hybrid sequential backward 
floating search, HSBFS)的演算法。HSBFS 總
共包括兩個階段，分別是特徵排序與特徵選
取。首先我們使用假特徵(false feature)的新
概念，將特徵按照其重要程度予以排序
(ranking)，經由排序後的結果，挑選出部分
排名後較不重要的特徵，進行 SBFS 演算
法。HSBFS 透過反覆地進行特徵排序與特徵
選取的步驟，以逐步地擷取出關鍵的子特徵
集合(feature subset)。在此計畫中，我們將採
用最近鄰居法(Nearest Neighbor, NN)當分類
器，並以分類後所得到的辨識率作為評估函
數(criterion function)。接下來，我們介紹在
HSBFS 演算法的實行步驟。 
 
A. 使用假特徵的特徵排序方法： 
 
在此計畫中，我們提出一個使用假特徵
(false feature)概念的特徵排序方法。此特徵
排序方法的目的是排序出所有特徵的重要
性，並挑選出排名後重要性較低的特徵，進
行 SBFS。此方法的實行步驟如下： 
Step 1: 首先在 D 個原始的特徵集合中，人
工地加入 m 個假特徵；在此，假特徵就
是代表特徵數值全部設定為 0 的特徵。
使得每個原始的特徵集合包含了 D+m
個特徵。 
Step 2: 在 D+m 個的特徵集合中，隨機抽取
特徵以建立 K 個子特徵集合，每個子特
徵集合都包含 n 個被隨機選取的特徵
（此 n 個特徵可被重複選取）。  
Step 3: 用上述包含 n 個特徵的子特徵集合, 
S, 訓練分類器，並得到其所對應的辨識
率 Acc(S)。 
Step 4: 在計算出不同子特徵集合的辨識率
Acc(S)之後，我們透過下列公式評估每
一個特徵, f, 的重要度(importance)，其
中也包含 m 個假特徵.  
 
|f
S
fimprotance fS )(|
)Acc(
)( )(
Λ
=
∑ Λ∈
 (1) 
 
其中 )( fΛ 代表某一組子特徵集合包含
了特徵 f， |)(| fΛ 是包含特徵 f 的子特
徵集合之特徵總數。importance(f)的數
值越大則代表此特徵 f 越重要。 
Step 5:計算此 m 個假特徵的平均 importance
數值, T。 
Step 6: 在 D 個原始特徵中，挑選出
importance 小於 T 的子特徵集合, U, 接
下來執行 SBFS 進行特徵選取。 
 
B. 使用 SBFS 進行特徵選取 
 
    在特徵選取階段中，大致上的概念與原
本的 SBFS 一樣。不過執行時，有一些部分
不同。我們將這些不同點整理如下： 
1. 我們在執行 SBFS 演算法時，可被移除
(removable)的特徵，僅限於子特徵集合 U
裡面的特徵。 
2. 在搜尋可移除特徵的階段時，若移除某一
特徵 f 後，辨識率(or criterion function)能
提升或是維持不變，則標示此特徵 f 為可
被移除的候選特徵(removable candidate).  
3. 在所有被標示為可被移除的候選特徵
中，移除可得到最高辨識率之特徵。若有
數個特徵在移除(remove)後，皆可得到相
同的辨識率；則以 importance 最低的特
徵，做為最優先移除的對象。 
4. 在執行完 SBFS 演算法後，假若沒有任何
一個的特徵被刪除或新增，則結束
HSBFS 演算法。反之，則重複執行特徵
排序與特徵選取。 
 
此研究方法的優點，是在每一次疊代
(iteration)時，透過特徵排序提供一組較不重
要的子特徵集合。若某些特徵的 importance
小於假特徵的平均 importance，則我們可以
合理的假設這些特徵蘊含的關鍵資訊不足。
HSBFS 演算法透過多次的疊代，逐步地刪除
與新增特徵；我們認為這樣的作法，能夠比
傳統 SBFS 呈現出更好的效果。 
 
C. 針對多類模式分類問題 (Multi-Class 
Classification Problem) 的特徵擷取策略 
 
一般的特徵擷取方法，並不會特別去區
分，此分類問題是屬於兩類別的分類問題
(binary classification)或多類別的分類問題
(multi-class classification)。根據我們實驗模
 Dataset 1:  
在 Dataset 1 中，包含了中文字元 ‘太’ 
and ‘大’。表 2 整理出 Dataset 1 的實驗模擬
結果。我們可以發現兩種特徵選取演算法都
能提升辨識率。雖然 SBFS 選出較少的特
徵，可是 HSBFS 得到較高的辨識率。接著
我們將兩種方法各自選擇的特徵，呈現在圖
2 中。在圖 2 中，如果某個特徵被選取了，
則我們將該特徵在字元中，所對應的影像區
塊(block)用粉紅色的色塊標示出來。圖 2(b)
中，我們可以明顯的看出，HSBFS 所選取子
特徵集合之區域，大部分集中在下方中央的
區域。顯而易見地，這個區域確實是分類字
元‘太’ and ‘大’的關鍵區域。 
 
表 2. Dataset 1 的模擬結果。 
Dataset 1 
Without 
Feature 
Selection 
SBFS HSBFS 
Number of Selected 
Features 256 21 35 
Accuracy of Test 
Data (%) 68.66 82.09 91.04 
Computational 
Time for Feature 
Selection 
 
0.8 hour 0.4 hour 
 
 
(a)     (b) 
圖 2. (a)SBFS; (b)HSBFS選出的子特徵集合。 
 
Dataset 2:  
在 Dataset 2 中，除了字元 ‘太’ and ‘大’
之外，還多加了‘犬’這類字元。表 3 整理出
Dataset 2 的實驗模擬結果。同樣地，兩種特
徵選取演算法都能提升辨識率，但是 SBFS
提升的效果較為有限。圖 3 顯示兩種方法各
自選擇的 feature subset。由於類別增加為三
類，所以我們也發現兩種方法所擷取的特徵
數量變多了。在此，我們搭配 one-against-all
的特徵擷取策略。表 4 為 HSBFS 使用
one-against-all strategy 後 的 結 果 。
One-against-all strategy 針對三個字元‘太’、
‘大’ and ‘犬’，各自選出 34, 59 and 39 個特
徵。圖 4 呈現此三個字元類別各自選擇的子
特徵集合。令人感興趣的是，字元‘太’選擇
的子特徵集合，主要集中在下方中央的區域
(見圖 4(a))；字元‘犬’選擇的子特徵集合主要
集中在右上方的區域(見圖 4(c)) ；而字元
‘大’ 選擇的子特徵集合，則涵蓋了前面所提
的兩個區域(見圖 4(b))；這樣的結果十分符
合我們的直覺與期待。此外測試辨識率也更
進一步由 79.5%提升到 88%。 
 
表 3. Dataset 2 的模擬結果。 
Dataset 1 
Without 
Feature 
Selection 
SBFS HSBFS 
Number of Selected 
Features 256 44 67 
Accuracy of Test 
Data (%) 63.5 69.5 79.5 
Computational 
Time for Feature 
Selection 
 1.8 hours 0.8 hour 
 
 
(a)     (b) 
圖 3. (a)SBFS; (b)HSBFS選出的子特徵集合。 
 
表 4. HSBFS 使用 one-against-all 策略的模擬
結果。 
Dataset 2 HSBFS with one-against-all strategy 
Class 太 Class 大 Class 犬 
Number of Selected 
Features 
34 59 39 
Accuracy of Test Data (%) 88 
Computational Time for 
Feature Selection 1.4 hours 
 1989. 
[17] I. Dhillon, S. Mallela, R. Kumar, “A divisive 
information-theoretic feature clustering algorithmfor 
text classification,” Journal of Machine Learning 
Research, vol. 3, pp.1265–1287, 2003. 
[18] K. Torkkola, “Feature extraction by non-parametric 
mutual information maximization,” Journal of 
Machine Learning Research, vol. 3, pp. 1415–1438, 
2003. 
[19] S. Das, “Filters, Wrappers and a Boosting-Based 
Hybrid for Feature Selection,” Proc. 18th Int’l Conf. 
Machine Learning, pp. 74-81, 2001. 
[20] E. Xing, M. Jordan, and R. Karp, “Feature Selection 
for High-Dimensional Genomic Microarray Data,” 
Proc. 15th Int’l Conf. Machine Learning, pp. 601-608, 
2001. 
[21] V. Vapnik, The Nature of Statistical Learning 
Theory, Springer Verlag, New York, 1995. 
[22] C. Cortes and V. Vapnik, “Support vector 
machines,” Machine Learning, vol. 20, pp. 1-25, 
1995. 
[23] K. Torkkola, “Feature extraction by non-parametric 
mutual information maximization,” Journal of 
Machine Learning Research, vol. 3, pp. 1415–1438, 
2003. 
 
 
International Journal of Innovative
Computing, Information and Control ICIC International c©2012 ISSN 1349-4198
Volume 8, Number 3(B), March 2012 pp. 2089–2100
MODIFIED SEQUENTIAL FLOATING SEARCH ALGORITHM
WITH A NOVEL RANKING METHOD
Chien-Hsing Chou1,∗, Yi-Zeng Hsieh2 and Chi-Yi Tsai1
1Department of Electrical Engineering
Tamkang University
No. 151, Yingjhuan Rd., Danshuei, Taipei 25137, Taiwan
∗Corresponding author: chchou@mail.tku.edu.tw
2Department of Computer Science and Information Engineering
National Central University
No. 300, Jhongda Rd., Jhongli City, Taoyuan 32001, Taiwan
Received December 2010; revised April 2011
Abstract. Feature selection plays a critical role in pattern classification. Of the various
feature selection methods, the sequential floating search (SFS) method is perhaps the most
well-known and widely adopted. This paper proposes a feature selection method combining
feature ranking and SFS. The proposed feature ranking approach adopts the new idea
of false features to rank features based on their importance, and then applies SFS to
features that are less important or of lower rank. This approach overcomes issues with
the original SFS and extracts more critical features. In addition, most feature selection
methods do not consider the problem of multi-class classification. As a result, these
methods have difficulty achieving good performance when dealing with a greater variety
of classes. Therefore, this study adopts a one-against-all strategy to address this issue.
The proposed approach divides multi-class classification into several binary classifications
and adopts feature selection to derive individual feature subsets. This strategy achieves
satisfactory performance in experimental simulations.
Keywords: Feature selection, Sequential floating search, False feature, One-against-all,
Pattern classification
1. Introduction. The choice of a suitable classification algorithm and feature selection
is often the key to success in pattern classification. There are currently a variety of pat-
tern classification methods, including nearest neighbors (NN) [1,2], k-nearest neighbors
(KNN) [3,4], condensed nearest neighbor (CNN) [5], multilayer perception (MLP) [6] and
support vector machines (SVM) [7,8]. The process of selecting an appropriate classifica-
tion algorithm should consider not only the accuracy of the classifier, but also the equally
important considerations of the time required for training and testing. For more discus-
sions on classification algorithms, refer to [2]. This study focuses on the process of feature
selection. A successful feature selection improves classification accuracy and extracts the
critical features that users are concerned with. For instance, in the analysis research of
DNA sequence, feature selection makes it possible to locate the segments on the sequence
or the types of amino acids that may lead to certain diseases [9,10], or select the genes
that may lead to certain diseases from the data of microarray [11]. Another example is
text categorization, in which feature selection makes it possible to extract the keywords
contributing to text classification [12-15]. In addition, feature selection not only selects
the features that users are most interested in, but also saves time in training and testing
the classifier, and reduces the memory space required for data storage.
2089
MODIFIED SEQUENTIAL FLOATING SEARCH ALGORITHM 2091
Figure 2. Removing either one of the features in the character area of a,
b or c obtains the same accuracy rate
addition, the feature selection stage chooses those less important features to process the
SBFS algorithm. As a result, this method can accurately extract critical features and
increase the accuracy rate at the same time.
This study attempts to solve another problem occurring in feature selection algorithms.
For most feature selection algorithms, the classification problem, which is either binary
or multi-class classification, is not considered prior. When the classes of classification in-
crease, it is difficult to achieve satisfactory performance for many feature selection meth-
ods. Consider the following example to illustrate this issue. Figure 3 shows four similar
classes of Chinese characters to be classified. The critical area of character in Figure 3(a)
on the right upper part (the block in pink) is for identifying the character “ ”. For char-
acter “ ”, (Figure 3(b)) the critical area is the lower middle part. However, if we want to
deal with these four characters at the same time, most feature selection algorithms cannot
classify them by selecting just a few critical areas. Our simulations indicate that when
there are a greater variety of character classes, most feature selection algorithms select
more features as the feature subset for classification. Therefore, the meaning of feature
selection is lost and the expected results cannot be achieved.
(a) (b) (c) (d)
Figure 3. Four similar Chinese characters. The pink block represents the
critical area for classifying the corresponding character.
This study proposes a one-against-all strategy for feature selection to overcome this
problem. The key point of this strategy is to partition the multi-class classification into
several binary sub-classifications. Assuming that we have N classes, it is possible to
generate N binary sub-classifications from the training dataset. An individual feature
subset can be extracted for each binary sub-classification. During the result validation
stage, a more appropriate feature subset is used for the corresponding sub-classification
to improve the testing performance.
2. The Proposed Hybrid Sequential Backward Floating Search. This section
presents a hybrid sequential backward floating search (HSBFS) algorithm. This algorithm
includes two stages: feature ranking and feature selection. HSBFS gradually extracts a
critical feature subset through the iterative process of feature ranking and feature selec-
tion. This study uses the nearest neighbor (NN) method as the classification method,
MODIFIED SEQUENTIAL FLOATING SEARCH ALGORITHM 2093
we can reasonably assume that information for that feature is lacking. Without examin-
ing all the features, only a subset consisting of less important features is given to execute
SBFS.
In addition, because the removable features are already extracted from the less impor-
tant ones in feature selection stage, we only add a feature for sure when we know that
the accuracy rate will increase. This kind of requirement is much stricter than that for
removing features. Compared with the original SBFS, HSBFS achieves better accuracy
rates and selects more critical features in our simulations.
3. The Feature Selection Strategy to Address the Multi-Class Classification.
Most feature selection methods do not distinguish whether issues are caused by binary
classification or multi-class classification. Experimental simulations indicate that the
greater the number of classes for multi-class classification, the greater the difficulty in
feature selection. Therefore, this study adopts a “one-against-all” strategy to overcome
this issue. This strategy has been successfully adopted for pattern classification techniques
such as SVM [7,8]. Therefore, we used the same concept to design a feature selection
strategy to address the multi-class classification problem.
Assuming that there are N classes in the dataset, generate N binary sub-classifications,
where each binary sub-classification (class i for instance) involves the dataset of class i
and class non-i. Class non-i represents all other data patterns not belonging to class i.
Next, use HSBFS and NN to extract individual feature subset to each sub-classification.
Because the selected feature subsets of each sub-classification are different from the fea-
ture amounts, this study proposes a process to classify data pattern in the testing stage
(Figure 4). We sent the data pattern x to each independent sub-classifier to calculate the
corresponding membership(i). The formula is listed below:
membership(i) =
dnon-i
di + dnon-i
(2)
where di = min
xj∈Class i
||x− xj||, (3)
dnon-i = min
xk∈Class non-i
||x− xk|| (4)
where di is the shortest distance between x and the data pattern belongs to class i in
the ith sub-classifier, and dnon-i is the shortest distance to class non-i. The bigger the
membership, the higher the possibility that x belongs to class i. Select the class with
the largest membership as the classified class to data pattern x. The formula is shown as
below:
i∗ = Arg max
i=1,...,N
membership(i) (5)
where i∗ is the classified class to data pattern x.
4. Experimental Results. The experimental simulations in this study compared the
SBFS and HSBFS algorithms. The parameters of the HSBFS algorithm were M = 5,
n = 64 and K = 640. The datasets for the experiment consisted of combinations of
ten similar Chinese characters selected from the ETL9b [34] handwritten Chinese char-
acters database, where each class includes 200 handwritten Chinese characters. Figure 5
shows some examples of these characters, while Table 1 shows the four different datasets
generated for simulation. The experimental simulations in this study partitioned each
dataset into a training dataset and testing dataset. To extract feature subsets using fea-
ture selection methods, we used half of the data patterns in the training dataset and the
nearest neighbor (NN) method [1,2] as the classification method to build a classifier. The
MODIFIED SEQUENTIAL FLOATING SEARCH ALGORITHM 2095
Table 1. Four datasets generated for experimental simulations
Number
Character Label
Number of Number of
of Class Training Data Testing Data
Dataset 1 2 , 267 133
Dataset 2 3 , , 400 200
Dataset 3 5 , , , , 667 333
Dataset 4 10 , , , , , , , , , 1,333 667
For each character image, we adopted the non-linear normalization technique [35] to
normalize it to a size of 64×64. Each character image was then divided into 16×16 blocks
(Figure 6). Each block consisted of 4×4 pixels, and compiled the numbers of the black
pixels in each blocks to be the features [36]. If a block contains more black pixels, then
the corresponding feature value is large. For examples, Figure 6 shows that the feature
value of block ‘a’ is 10 because more than half of pixels are black pixels within block ‘a’.
And the feature value of block ‘b’ is 0, because there are no black pixels within block
‘b.’ A character image finally consists of 256 features (i.e., 16×16 blocks), whose values
range from 0 to 16; each feature represents the information of a certain area (block) in
a character image. Therefore, if a certain block is the critical area for classifying (e.g.,
block ‘a’), the feature selection method should extract the corresponding feature. On
other hand, the corresponding feature would not be extracted if the block is not the
critical area for classifying (e.g., block ‘b’). The following discussion presents the results
of the experimental simulation.
Figure 6. Character image divided into 16×16 blocks
Dataset 1: Dataset 1 included the Chinese characters ‘ ’ and ‘ ’. Figure 5 shows some
examples of two Chinese characters from the ETL9b database. For further analysis, the
bottom of the central area of character image is the critical area to classify the characters
‘ ’ and ‘ ’ (Figure 7). If a feature selection method extracted more critical features from
this area, the recognition result should be more accurate. Table 2 shows the simulation
results of Dataset 1. Each of the two feature selection algorithms improved the accuracy
rate after selecting its feature subset. SBFS extracted fewer features, but HSBFS obtained
a higher accuracy rate than SBFS. Besides, HSBFS also spent less time than SBFS in
seeking the corresponding feature subset.
Figure 8 displays the corresponding feature subsets selected by the two algorithms to
verify the simulation results. If an algorithm selects a feature in the feature subset, the
corresponding character area to this feature is labeled in pink. Examining the pink areas in
the character image reveals which areas (i.e., features) are selected by the feature selection
MODIFIED SEQUENTIAL FLOATING SEARCH ALGORITHM 2097
(a) (b)
Figure 9. Feature subset selected by (a) SBFS; (b) HSBFS
Table 4. Simulation result of Dataset 2 by applying one-against-all strategy
Dataset 2 HSBFS with one-against-all strategy
Number of Selected Features
Class Class Class
34 59 39
Accuracy of Test Data (%) 88
Computational Time for Feature Selection 1.4 hours
(a) (b) (c)
Figure 10. Feature subset selected for (a) Class ‘ ’; (b) Class ‘ ’; (c) Class ‘ ’
‘ ’ from another two classes ‘ ’ and ‘ ’ is mainly located in the lower central area (see
Figure 10(a)); the features mainly located in the upper right part are selected for the
feature subset to discriminate class ‘ ’ from ‘ ’ and ‘ ’ (see Figure 10(c)). The feature
subset selected to discriminate class ‘ ’ from the other two classes covers the two above-
mentioned areas (see Figure 10(b)). These results perfectly matched our intuition and
expectations. In addition, the accuracy rate increased from 79.5% to 88%.
Dataset 3: Dataset 3 includes the characters ‘ ’, ‘ ’, ‘ ’, ‘ ’ and ‘ ’. Table 5 shows the
simulation results of Dataset 3. Figure 11 shows the selected feature subset from SBFS
and HSBFS, respectively. The number of features increased, while the disparity between
HSBFS and SBFS in terms of the accuracy rate decreased. Table 6 lists the results of
implementing the one-against-all strategy with HSBFS. There are 62, 67, 61, 16 and 18
features to be extracted from the five sub-classifications, respectively. Figure 12 shows
the selected feature subsets of the five classes. When using the one-against-all strategy,
the accuracy rate increased from 81.74% to 89.22%. At the same time, comparing the
results of Figures 11(b) and 12(a)-12(e) shows that adopting the one-against-all strategy
can really help users identify the critical features they are interested in. This method is
also perfectly suited to overcoming the multi-class classification problem.
Dataset 4: Dataset 4 includes 10 similar characters. Table 7 shows the simulation
results from Dataset 4. There was a greater number of selected features and not such
a significant increase in the accuracy rate of SBFS or HSBFS. After adopting the one-
against-all strategy (see Table 8), the accuracy rate increased from 80.51% to 88.61% and
there were less selected features. For classes ‘ ’ and ‘ ’, only 15 and 9 features were
selected to deal with the corresponding sub-classifications, respectively.
MODIFIED SEQUENTIAL FLOATING SEARCH ALGORITHM 2099
Table 8. Simulation result of Dataset 4 by applying one-against-all strategy
Dataset 4 HSBFS with one-against-all strategy
Number of Selected Features
80 115 62 29 69 32 15 67 57 9
Accuracy of Test Data (%) 88.61
Computational Time
80 hours
for Feature Selection
5. Conclusion. This study is the first to introduce the HSBFS algorithm. This algo-
rithm is able to overcome the problems involved in sequential floating search and can
extract the critical feature subset more accurately and effectively. This study also adopts
a one-against-all strategy to improve the effectiveness of feature selection methods and
address the multi-class classification problem. The accuracy rates in three experimental
simulations increased by at least 8%. Although these simulations show that the pro-
posed one-against-all strategy provides satisfactory performance, the computational cost
is expensive if the class number is large. To overcome this limitation, the one-against-all
strategy makes it possible to accomplish training on decomposed classes in parallel pro-
cessing. Another way to decrease computational cost is to reduce the amount of data
in the training data set. Therefore, for training a binary sub-classification (Class i for
instance), it is possible to select part of relative classes as Class non-i, instead of using
all other data patterns that do not belong to Class i.
Acknowledgement. This work was supported by the National Science Council, Taiwan,
under the Grant NSC 99-2221-E-238-017, NSC 99-2622-E-032-004-CC3 and NSC 100-
2221-E-032-069.
REFERENCES
[1] T. Cover and P. E. Hart, The nearest neighbor rule for small samples drawn from uniform distribu-
tions, IEEE Trans. Information Theory, vol.13, no.1, pp.21-27, 1967.
[2] R. O. Duda, P. E. Hart and D. G. Stork, Pattern Classification, Wiley, New York, 2001.
[3] A. Levine, L. Lustick and B. Saltzberg, The nearest neighbor rule for small samples drawn from
uniform distributions, IEEE Trans. Information Theory, vol.19, no.5, pp.697-699, 1973.
[4] J. F. O’Callaghan, An alternative definition for “neighborhood of a point”, IEEE Trans. Computers,
vol.24, no.11, pp.1121-1125, 1975.
[5] P. Hart, The condensed nearest neighbor rule, IEEE Trans. Information Theory, vol.14, pp.515-516,
1968.
[6] S. Haykin, Neural Network, 2nd Edition, Pretince Hall, 1999.
[7] V. Vapnik, The Nature of Statistical Learning Theory, Springer Verlag, New York, 1995.
[8] C. Cortes and V. Vapnik, Support vector machines, Machine Learning, vol.20, pp.1-25, 1995.
[9] H. Liu, J. Li and L. Wong, A comparative study on feature selection and classification methods using
gene expression profiles and proteomic patterns, Genome Informatics, vol.13, pp.51-60, 2002.
[10] T. Li, C. Zhang and M. Ogihara, A comparative study of feature selection and multiclass classification
methods for tissue classification based on gene expression, Bioinformatics, vol.20, no.15, pp.2429-
2437, 2004.
[11] E. P. Xing, M. I. Jordan and R. M. Karp, Feature selection for high-dimensional genomic microarry
data, International Conference on Machine Learning, pp.601-608, 2001.
[12] Y. Yang and J. O. Pedersen, A comparative study on feature selection in text categorization, Inter-
national Conference on Machine Learning, pp.412-420, 1997.
[13] T. Liu, S. Liu, Z. Chen and W. Y. Ma, An evaluation of feature selection for text categorization,
International Conference on Machine Learning, 2003.
[14] G. Forman, An extensive empirical study of feature selection metrics for text classification, Journal
of Machine Learning Research, vol.3, pp.1289-1305, 2003.
  
附件二 
 
出席國際會議心得報告與發表論文 
 
刊登至 2012 International Conference on Automatic Control and 
Artificial Intelligence (ACAI 2012)國際會議 
 
A New Validity Measure and Fuzzy Clustering algorithm for 
Vanishing-point Detection 
 
 
Yu-Xiang Zhao1, Hsien-Pang Tai2 , Syue-Jyun Fang2 , Chien-Hsing Chou2 † 
 
 
Department of Computer Science & Information Engineering, National Quemoy 
University, Taiwan1 
 
 
Department of Electrical Engineering, Tamkang University, Taiwan2† 
    
(a)      (b)             (c)          (d) 
Fig. 2. (a) Image after blurring operation; (b) Clustering results achieved by FCM algorithm; (c) Reproduced image; (d) Vanishing 
lines and vanishing point retrieved after applying Canny edge detection and Hough transform. 
 
 
2 The Proposed Preprocessing Method for Detecting 
Vanishing Lines 
The proposed preprocessing method consists of five steps.  
 
Step 1: First, blur the image using the mean filter with 5×5 
mask size. Figure 2(a) shows the image resulting from 
blurring processing. 
Step 2: After blurring, transfer the image pixel from the RGB 
color space to the YCbCr color space. Then the original 
image is divided into several non-overlapping image blocks 
of 8×8 pixels. Calculate the mean values of Cb and Cr from 
the pixels within the image block; and use the Cb and Cr 
mean values to typify color information of this image block.  
Step 3: Consider each image block as a data pattern, and use 
the Cb and Cr mean values as features of the data pattern. 
Generate the dataset which contains the data pattern of each 
image block. Then group the dataset into two clusters by 
FCM [9] algorithm. Figure 2(b) shows that the blue data 
patterns and the red data patterns represent two different 
clusters, and position 'X'  represents cluster center C. 
Step 4: For each pixel of the original image, find cluster center 
C* with the shortest distance based on Euclidean distance; 
then use the Cb and Cr values of corresponding C* to 
replace the Cb and Cr values of each pixel. For each pixel, 
only the Cb and Cr values are changed, while the Y value 
remains unchanged. Then transfer the image from the 
YCbCr color space to the RGB color space. Figure 2(c) 
shows the reproduced image. This step can be considered a 
type of color quantization processing. 
Step 5: Finally, locate vanishing lines and the vanishing point 
using Canny edge detection and Hough transform on the 
reproduced image.  
In Fig. 2(d), the image’s vanishing point lies at the 
intersection of the vanishing lines. Only critical edge 
information is extracted to locate vanishing lines and the 
vanishing point. Comparing Figs. 2(d) and 1(b), the proposed 
preprocessing method can locate the correct vanishing point 
more easily.  
 
3 The Proposed New Cluster Validity Measure 
Results of the proposed preprocessing method were 
satisfactory, but clustering using the FCM [9] algorithm 
required setting up the initial cluster number in advance. The 
initial cluster number for Fig. 2 is set at two. However, the 
optimal initial cluster number may be different for different 
images. For example, Fig. 3 contains three main color regions: 
the red toy car (region A), the ivory floor (region B), and the 
blue wall (region C). The cluster number should reasonably be 
set to three.  
Evaluating the appropriate cluster number from the 
dataset is named as cluster validity. Many studies present 
different cluster validity measures to assess the appropriate 
cluster number [10-12]. However, the design concept in most 
cluster validity measures, the most reasonable cluster number 
for datasets in Fig. 1(b) should be one, meaning that most 
cluster validity measures are not suitable for this problem. This 
paper proposes a new cluster validity measure to solve this 
problem. 
 
 
Fig. 3. Image containing three main color regions. 
 
1683
  
(a) 
 
(b) 
 
(c) 
Fig. 5. Some experimental results of the proposed method. 
 
 
(a) 
 
(b) 
 
(c) 
Fig.6. Some experimental results of the other comparison 
method. 
 
5 Conclusion 
In combination with the clustering algorithm and the 
new validity measure, the proposed preprocessing can easily 
locate the position of vanishing lines and the vanishing point in 
an image (especially outdoor images). In addition, the proposed 
area measure can provide a suitable initial cluster number to 
this research.  
 
Acknowledgement This work was supported by the National 
Science Council, Taiwan, R.O.C., under the Grant NSC 100-
2221-E-032-069. 
 
Reference 
[1] A. Saxena, S. H. Chung, and A. Y. Ng, “3-D Depth 
Reconstruction from a Single Still Image,” International 
Journal of Computer Vision, vol. 76, no. 1, pp. 53-69, 
(2008). 
[2] A. Saxena, S. H. Chung, and A. Y. Ng, “Make3D: Learning 
3D Scene Structure from a Single Still Image, “ IEEE Trans. 
on Pattern Analysis and Machine Intelligence, vol. 31, no. 5, 
pp. 824-840, (2009). 
[3] A. Saxena, S. H. Chung, and A. Y. Ng, “Learning depth 
from single monocular images,” Neural information 
processing system, vol. 18, (2005). 
[4] J. Michels, A. Saxena, and A.Y. Ng, “High speed obstacle 
avoidance using monocular vision and reinforcement 
learning,” Proceeding of 22nd Internal conference of 
Machine Learning, (2005). 
[5] J.M. Loomis, “Looking down is looking up,” Nature News 
and Views, vol. 414, pp. 155–156, (2001). 
[6] S. Battiato, A. Capra, S. Curti, and M. L. Cascia, "3D 
Stereoscopic Image Pairs by Depth-Map Generation," 
Proceedings of the 3D Data Processing, Visualization, and 
Transmission, 2nd International Symposium, (2004). 
[7] Yi-Min Tsai, Yu-Lin Chang, and Liang-Gee Chen, "Block-
based Vanishing Line and Vanishing Point Detection for 3D 
Scene Reconstruction," International Symposium on 
Intelligent Signal Processing and Communications, pp. 586-
589, (2006). 
[8] V. Cantoni, L. Lombardi, M. Porta, N. Sicari,“Vanishing 
Point Detection: Representation Analysis and New 
Approaches,” Proceeding of the 11th International 
Conference on Image Analysis and Processing, pp. 90-94, 
(2001). 
[9] J.C. Bezdek, Pattern Recognition with Fuzzy Objective 
Function Algorithms, Plenum, New York, (1981). 
[10] J. C. Dunn, “Well Separated Clusters and Optimal Fuzzy 
Partitions,” Journal Cybern., vol. 4, pp. 95-104, (1974). 
[11] J. C. Bezdek, “Numerical Taxonomy with Fuzzy Sets,” J. 
Math. Biol., vol. 1, pp. 57-71, (1974). 
[12] C. H. Chou, M. C. Su and E. Lai, “A New Cluster Validity 
Measure and Its Application to Image Compression,” 
Pattern Analysis and Applications, vol. 7, no. 2, pp. 205-
220, (2004). 
1685
?????????????????? 
Development of a somatosensory game-based home rehabilitation 
system for stoke patients 
??? 1*???? 1???? 1???? 2 
?????????? 1 
??????? 2 
Chien-Hsing Chou1*, Syue-Jyun Fang1, Ke-Wei Chen1, Ya-Chun Hsiao2 
Department of Electrical Engineering, Tamkang University, Taiwan 1 
Smart Care Rehabilitation Clinic2 
Email: chchou@mail.tku.edu.tw 
 
 
?? 
  ??????????????????
????????????????????
????????????????????
Kinect????????????????
????????????????????
????????????????????
???????????????????
????????????????????
????????????????????
????????????????????
????????????????????
????????????????????
????????????????????
?????????? 
?? ?? 
  ?????????????????
????????????????????
????????????????????
????????????????????
????????????????????
????????????????????
????????????????????
?????????????????? 
  ??????????????????
???????????????????
?????? 6 ?????????????
??????????? 1 ??? 3 ????
???????????????????
????????????????????
????????????????????
??????????????????
????????????????(4 ? 6
?)???????????????????
????????????????????
???????????????????? 
  ??????????????????
????????????????????
????????????????????
????????????????????
????????????????????
????????????????????
???????????????????
???????????????????
????????????????????
??????????????????? 
 
(AR)?????????????????
????????????????????
??????????? augmented reality ?
????????????????????
?????????????? augmented 
reality ?????????????????
?? 
 
?? ??????? 
2.3 ????????????? 
  ??????????????????
????????????????? 
(1) ??? 
  ????????Server???????
????????????????????
??????????????????? 
?? ????????? 
??????????? 
????Server??????
???? 
??? 
????
?? ?????????????
? 
 
(2) ??? 
  ??????????????????
????????????????????
????????????????????
???????????????????
??????? Server ??????????
????????????? 
?? ????????? 
???????????? 
?????????????
? 
?????????? 
??? 
????
?? 
?????????????
? 
 
?? ???? 
  ??????????????? Kinect
??? 2 ????????????????
????????????????????
???????????????????
????????????????????
???? 
????(1)????? 
  ?????????????????
?????????[6]??????????
????????????????????
????????????????????
????????????????????
????????????????????
????????????????????
????????????? 
 
?? ????????????????? 
health records," 2011 IEEE International 
Conference on e-Health Networking 
Applications and Services , pp.197-200, 2011. 
[5] N. Hocine,; A. Gouaich, Di Loreto, I.; M. 
Joab, M, "Motivation based difficulty adaptation 
for therapeutic games," 2011 IEEE International 
Conference on Serious Games and Applications 
for Health, pp.1-8, 2011 
[6]???????? ????? pp38-45,2009 
available:http://www.tzuchi.com.tw/file/tcmed/2
00902-62/38-45.pdf. 
 
JOURNAL OF INFORMATION SCIENCE AND ENGINEERING XX, XXX-XXX (201X) 
1  
A New Measure of Cluster Validity Using Line Sym-
metry 
http://www.software995.com/ 
CHIEN-HSING CHOU1, YI-ZENG HSIEH2 AND MU-CHUN SU2* 
1Department of Electrical Engineering,  
Tamkang University, Taiwan 
2Department of Computer Science & Information Engineering,  
National Central University, Taiwan 
Email: muchun@csie.ncu.edu.tw 
 
Many real-world and man-made objects are symmetry, therefore, it is reasonable to as-
sume that some kind of symmetry may exist in data clusters. In this paper a new cluster va-
lidity measure which adopts a non-metric distance measure based on the idea of "line sym-
metry" is presented. The proposed validity measure can be applied in finding the number of 
clusters of different geometrical structures. Several data sets are used to illustrate the per-
formance of the proposed measure. 
 
Keywords: cluster validity, clustering algorithm, line symmetry, cluster analysis, similarity 
measure, unsupervised learning 
 
1. INTRODUCTION 
 
Cluster analysis is one of the basic tools for exploring the underlying structure of a 
given data set and plays an important role in many applications [1]-[6]. In cluster analysis, 
two crucial problems required to be solved are (1) the determining of the similarity 
measure based on which patterns are assigned to the corresponding clusters and (2) the 
determining of the optimal number of clusters. While the determining of the similarity 
measure is the so-called data clustering problem, the estimation of the number of clusters 
in the data set is the cluster validity problem. In order to mathematically identify clusters 
in a data set, it is usually necessary to first define a measure of similarity or proximity 
which will establish a rule for assigning patterns to the domain of a particular cluster 
center. Recently, several different clustering algorithms have been proposed to deal with 
clusters with various geometric shapes. These algorithms can detect compact clusters [7], 
straight lines [8], shells [9]-[11], contours with polygonal boundaries [12] or 
well-separated non-convex clusters [13]. One thing that should be emphasized is that 
there is no cluster algorithm which can tackle all kinds of clusters. Some comprehensive 
overview of clustering algorithms can be found in the literature [1]-[4]. 
In fact, if cluster analysis is to make a significant contribution to engineering appli-
cations, much more attention must be paid to cluster validity issues that are concerned 
with determining the optimal number of clusters and checking the quality of clustering 
results. For the partition-based clustering algorithm [7]-[13], the cluster number should be 
decided in prior. Basically, there are three different approaches to the determination of 
the cluster number of a data set. The first approach is to use a certain global validity 
measure to validate clustering results for a range of cluster numbers [14]-[19]. The sec-
ond approach is based on the idea of performing progressive clustering [20]-[22]. The 
third approach is the projection-based approach [23]-[29]. Projection algorithms allow us 
to visualize high-dimensional data as a two-dimensional or three-dimensional scatter plot. 
A New Measure of Cluster Validity Using Line Symmetry 
 
3 
 
strate the effectiveness of the new validity measure. Section 4 presents the simulation 
results. Finally, Section 5 presents the conclusion. 
 
2. CLUSTER VALIDITY MEASURES 
 
Cluster validation refers to procedures that evaluate the clustering results in a quan-
titative and objective fashion. Some kinds of validity measures are usually adopted to 
measure the adequacy of a structure recovered through cluster analysis. Determining the 
correct number of clusters in a data set has been, by far, the most common application of 
cluster validity. Bensaid et al. [44] grouped validity measures into three categories. The 
first category consists of validity measures that evaluate the properties of the crisp struc-
ture imposed on the data by the clustering algorithm [14], [17]. The second category con-
sists of measures that use the membership degrees produced by the corresponding fuzzy 
clustering algorithms (e.g. the FCM algorithm) [3], [15], where the membership degree 
represents the possibility of a data pattern belonging to the specified cluster. The third 
category consists of validity measures that take into account not only the membership 
degrees but also the data themselves [16], [18], [31]. Each has its own considerations and 
limitations. A comparative examination of thirty validity measures is presented in [33] 
and an overview of the various measures can be found in [45]. Since it is not feasible to 
attempt a comprehensive comparison of our proposed validity measure with many others, 
we just chose three of the popular measures for comparisons. These three measures have 
different rationales and properties. Consider a partition of the data set { }NjxX j ,,2,1; K==  
and the center of each cluster iv ),,2,1( ci K= , where N is the data number and c repre-
sents the cluster number. 
Partition coefficient (PC) [15]: 
   Bezdek designs the partition coefficient (PC) to measure the amount of “over-
lap” between clusters. He defines the partition coefficient (PC) as follows: 
åå
= =
=
c
i
N
j
ijuN
cPC
1 1
2)(
1
)(                 (1) 
where iju ),,2,1;,,2,1( Njci KK ==  is the membership of data pattern j in cluster i. 
The closer this value is to unity the better the data are classified. In the case of a hard 
partition, we obtain the maximum value 1)( =cPC . If we are looking for a good partition, 
we aim at a partition with a maximum partition coefficient. This kind of partition yields 
the "most unambiguous" assignment. The disadvantages of the partition coefficient are its 
monotonic decreasing with c and the lack of direct connection to some properties of the 
data themselves.  
Classification entropy (CE) [3]: 
   The classification entropy measure strongly resembles the partition coefficient; 
however, it is related on Shannon's information theory. The classification entropy is de-
fined as follows: 
 
åå
= =
-=
c
i
N
j
ijij uuN
cCE
1 1
)log(
1
)(
                (2) 
If we have a crisp partition we have the most information (i.e. the minimum entropy). 
Consequently, a partition with the minimum entropy is regarded as a good partition. Al-
A New Measure of Cluster Validity Using Line Symmetry 
 
5 
 
the case of 2 clusters) to 0.909 (for the case of 3 clusters); the membership degree, 
11u , of the data pattern, 1x , decreases from 0.991 (for the case of 2 clusters) to 0.544 
(for the case of 3 clusters). For data patterns located at the upper region of cluster 1 
(e.g. 2x ) or the right region of cluster 3, the increment of their membership degrees 
induced by partitioning the data set into 3 clusters indicates that it is appropriate to 
partition the data set into 3 clusters. On the contrary, the decrement of the member-
ship degrees for data patterns located at the lower region of cluster 1 (e.g. 1x ) or the 
left region of cluster 3 indicates that the partition of 3 clusters is not a good choice. 
Because the total decrement of membership degrees is larger than the total increment 
of membership degrees due to the increases of the number of clusters, the value of 
the PC measure decreases from 0.923 to 0.877. Understandably, the PC measure fa-
vors the partition of two clusters in this example. A similar reason can also be ap-
plied to explain why the classification entropy measure CE can not find the right 
number of clusters for this data set either.  
 
2. Not only dose the value of the numerator in Eq. (3) changes from 86.59 to 46.87 
when the number of clusters changes from 2 to 3, but also the minimum Euclidean 
distance between cluster centers, mind , changes from 4.51 to 2.17. While the decre-
ment of the numerator indicates that the degree of compactness increases, the decre-
ment of the denominator indicates the degree of separation decreases. Finally, the 
value of S(c) changes from 0.048 to 0.054. Therefore, the S(c) measure prefers the 
partition of 2 clusters.  
 
3. The two measures, PC and CE, use the basic heuristic rule that good clusters are not 
fuzzy. Consequently, they rely solely upon the memberships to determine the fuzzi-
ness of the partition. This example confirms what one already knows: validity meas-
ures that use only the fuzzy membership grades usually lack direct connection to 
some geometrical properties of the data themselves.  
 
4. The separation measure S uses åå= =
-
c
i
N
j
ijij vxu
1 1
22
to measure the compactness. For compact 
spherical clusters, this kind of measure may be effective; however, for some clusters 
with different geometrical structures (e.g. linear clusters, shells, etc), it may not work 
well. In this example, the data set includes not only a spherical cluster but also two 
linear clusters so that it is not very surprising when we find out that the separation 
measure S can not find the correct partition.  
5. If we move two linear clusters closer to the spherical cluster (e.g. the distances be-
tween each other do not vary too much), the measures, PC, CE, and S, can find the 
correct number of clusters. 
A New Measure of Cluster Validity Using Line Symmetry 
 
7 
 
nearest neighbor search to reduce the complexity of finding the closest symmetric point. 
Although object with point symmetry is very widespread, line symmetry is the most 
common type of symmetry around us. Based on the point symmetry distance, a new line 
symmetry distance is developed in our another work [46]. Before we present the defini-
tion of the proposed line symmetry distance, we briefly review the definition of the point 
symmetry distance.  
A 2-dimensional figure is with point symmetry if it can be rotated 180 degrees about a 
point onto itself. To generalize this idea of point symmetry to measure the degree of point 
symmetry for a set of high-dimensional data patterns, a point symmetry distance is de-
fined as follows [51]. Given a data set X containing N patterns, Njx j ,,1, L= , and a ref-
erence vector c  (e.g. a cluster center), the “point symmetry distance” between a pattern 
jx  and the reference vector c  is defined as 
||)||||(||
||)()(||
min),(
,,1 cxcx
cxcx
cxd
ij
ij
jiand
Nijps -+-
-+-
=
¹
= L
             (4) 
Note that Eq. (4) is minimized (i.e. 0),( =cxd jps ) if there is a pattern )2(* j
ps
j xcx -=  ex-
ists in the data set (see Fig. 2). The pattern 
ps
jx *  is then denoted as the point symmetrical 
data pattern relative to jx  with respect to c  as shown in Fig. 2.  
 
 
Fig. 2. An example for illustrating the bias problem incurred by the original version 
of the point symmetry distance. 
 
By further analyzing the point symmetry distance defined in Eq. (4), we find that the 
distance measure has a bias for data patterns with a larger distance from the center c  
even if they are with the same distance to the point symmetrical data pattern psjx * . For 
example, two data patterns, 1x  and 2x , are with the same distance to the symmetrical 
data pattern psjx *  as shown in Fig. 2. According to the definition of Eq. (4), we find that 
||)||||(||
||)()(||
||)||||(||
||)()(||
2
2
1
1
cxcx
cxcx
cxcx
cxcx
j
j
j
j
-+-
-+-
>
-+-
-+-
 even if these two patterns are with the same distances to the 
point symmetrical data pattern psjx * . To amend this flaw, we modify the point symmetry 
distance as follows: 
||)||||||||(||
||)()(||
min),(
**
,,1
jijj
ij
jiand
Nijps xxcxcx
cxcx
cxd
-+-+-
-+-
=
¹
= L
           (5) 
Figure 3 shows examples of the distance functions defined in Eq. (4) and Eq. (5) for 
the case of 
T
jx )0,1(-= , Tc )0,0(= , and 
Tps
jx )0,1(* =  (corresponding to three small circle points 
in the figure). For each point in the picture plane, the distance is used for the intensity. 
A New Measure of Cluster Validity Using Line Symmetry 
 
9 
 
 
Fig. 4. A geometrical explanation about the definitions of point symmetry and line sym-
metry. 
 
Since the covariance matrix Cov  is real and symmetric, we can find a set of n or-
thonormal eigenvectors from the matrix. One of the n orthonormal eigenvector will be 
chosen to be the unit direction vector e . The vector e  is then regarded as the symmetric 
line of the data set. For each cluster, the eigenvector with the smallest amount of line 
symmetry distances (e.g., the i*th eigenvector in this case) is chosen to be the symmetri-
cal line of that cluster based on the following minimum-value criterion: 
å
Î
=
=
kSj
k
ikjlsni
ecxdArgi ),,(min*
,1 L
                 (9) 
where kS  represents the data set consisting of data points belonging to cluster k and 
k
ie  represents the ith eigenvector computed from the covariance matrix corresponding to 
the data set kS . The normal projected data pattern p  can be computed by 
  
( )[ ] eecxcepccp Tj ´-+=´-+=              (10) 
After we have computed the normal projected data pattern p , we can find the line 
symmetrical data pattern, 
ls
jx * , relative to jx  with respect to the center c  and the unit 
direction vector e  by the following equation: 
[ ] )()(2
)(2
)(2
22
22
*
j
T
jjj
jjj
jjj
ls
j
xpecxcxx
xpcpcxx
xpxpxx
-´---+=
-´---+=
-´-+=
              (11)  
 
3.2 The Validity Measure Using Line Symmetry 
 
The proposed validity measure is referred to as LS measure and is computed as fol-
lows. Consider a partition of the data set { }NjxX j ,,2,1; K==  and each data pattern jx  is 
assigned to its corresponding cluster by a particular clustering algorithm. In order to cal-
culate line symmetry distance, we need re-compute the cluster center iv  (i.e. mean vec-
A New Measure of Cluster Validity Using Line Symmetry 
 
11 
 
4. EXPERIMENTAL RESULTS 
We illustrate the effectiveness of the proposed validity measure by testing some data 
sets with different geometrical structures. For the comparison purpose, these data sets 
were also tested by the three popular validity measures— the partition coefficient (PC), 
the classification entropy (CE) and the Xie-Beni’s separation measure (S). The FCM al-
gorithm or the Gustafson-Kessel (GK) algorithm [7] is applied to cluster these data sets at 
each cluster number c from c=2 to c=10. According the experimental results, we notice 
that the FCM algorithm is not suitable to be applied for ellipsoidal clusters, but the GK 
algorithm can be used to cluster spherical and ellipsoidal clusters. Therefore, in examples 
2 to 5, the GK algorithm is applied to cluster the data patterns. A value of the fuzzy ex-
ponent m=2 was chosen for the FCM algorithm and the GK algorithm. The parameter   
d0 was chosen to be 0.005 for the composite line symmetry distance. 
 
Example 1: The data set shown in Fig. 1(a) consists of a combination of a spherical clus-
ter and two linear clusters. We generated the data patterns randomly with uniform distri-
bution. The total number of data patterns is 400. In this example, the FCM algorithm is 
applied to cluster the data patterns. The total number of data patterns is 400. The per-
formance of each validity measure is tabulated in Table I. In Table I and others to follow, 
the highlighted (bold and shaded) entries correspond to optimal values of the measures. 
Note that only the LS validity measure finds the optimal cluster is three, but the PC, CE 
and S validity measures choose two clusters as the optimal partition. The clustering result 
achieved by the FCM algorithm at c=3 is shown in Fig. 1(c). 
Table I. Numerical values of the validity measures for example 1. 
C 2 3 4 5 6 7 8 9 10 
PC 0.923 0.877 0.790 0.829 0.792 0.723 0.738 0.674 0.661 
CE 0.153 0.242 0.397 0.358 0.440 0.546 0.573 0.666 0.709 
S 0.048 0.054 1.372 0.075 0.142 0.297 0.363 0.257 0.218 
LS 0.050 0.017 0.037 0.096 0.238 0.170 0.160 0.184 0.158 
 
Example 2: We randomly generated a mixture of spherical and ellipsoidal clusters with 
Gaussian distribution. This data set consists of 850 data patterns distributed on five clus-
ters, as shown in Fig. 5(a). The clustering results achieved by the GK and FCM algo-
rithms at c=5 are shown in Fig. 5(b) and Fig. 5(c). We notice that the FCM algorithm is 
not suitable to be applied for ellipsoidal clusters; the two non-overlapped compact clus-
ters are not clustered correctly. Therefore, in this example, the GK algorithm is applied to 
cluster the data patterns. The performance of each validity measure is given in Table II. 
Both the S and LS validity measures find that the optimal cluster number c is at c=5, but 
both the PC and CE validity measures indicate two clusters as the optimal partition.  
Table II. Numerical values of the validity measures for example 2. 
C 2 3 4 5 6 7 8 9 10 
PC 0.836 0.743 0.738 0.780 0.731 0.681 0.654 0.634 0.591 
CE 0.287 0.469 0.524 0.484 0.592 0.698 0.771 0.840 0.911 
S 0.398 0.180 0.186 0.082 0.383 0.392 0.296 0.274 0.796 
LS 0.143 0.083 0.045 0.041 0.091 0.152 0.126 0.180 0.137 
A New Measure of Cluster Validity Using Line Symmetry 
 
13 
 
Example 4: This example demonstrates an application of the LS validity measure to de-
tect the number of objects in an image. In image processing, it is very important to find 
objects in images. In this example, these objects have different geometric shapes. Fig. 7(a) 
shows a real image consisting of a mobile phone, a doll, and an object of crescent. First, 
we apply the thresholding technique to extract the objects from the original image (see 
Fig. 7(b)). Then we transfer the object pixels to be the data patterns. The GK algorithm is 
used to cluster the data set. Table IV shows the performance of each validity measure. 
The LS validity measure finds that the optimal cluster number c is at c=3. However, the 
PC, CE and S validity measures find the optimal cluster number at c=2. Once again, this 
example demonstrates that the proposed LS validity measure can work well for a set of 
clusters of different geometrical shapes. The clustering results achieved by the GK and 
FCM algorithms at c=3 are shown in Fig. 7(c) and Fig. 7(d). 
 
Table IV. Numerical values of the validity measures for example 4. 
C 2 3 4 5 6 7 8 9 10 
PC 0.956 0.846 0.786 0.728 0.682 0.638 0.605 0.588 0.570 
CE 0.101 0.307 0.422 0.553 0.657 0.724 0.815 0.854 0.956 
S 0.071 0.111 0.136 0.244 0.323 0.375 0.321 0.436 0.363 
LS 0.034 0.018 0.027 0.043 0.052 0.039 0.064 0.062 0.056 
 
  
(a)                   (b) 
  
(c)                (d) 
Fig. 7. (a) The original image containing a mobile phone, a toll, and an object of crescent; 
(b) the binary image by applying threshold to the original image; (c) the clustering result 
achieved by the GK algorithm at c=3; (d) the clustering result achieved by the FCM algo-
rithm at c=3. 
A New Measure of Cluster Validity Using Line Symmetry 
 
15 
 
 
5. DISCUSSION AND CONCLUSION 
Based on the line symmetry distance, a new measure LS is then proposed for cluster 
validation. The simulation results reveal the interesting observations about the validity 
measures discussed in this paper. The S validity measure fails if not in one example, at 
least in the other, and the PC and CE validity measures fail in all the examples. The pro-
posed LS validity measure shows that consistency for these and several other examples 
that are tried. Because the line symmetry distance is extended from the point symmetry 
distance, one may interest the performance if the cluster validity measure is based on 
point symmetry. According our simulations, if the clusters with point symmetry structure 
(e.g. examples 2 and 3), the validity measure based on point symmetry should also per-
form well for these cases. However, if the geometrical structure of clusters is line symme-
try (e.g. example 4), it not a good choice to use point symmetry as validity measure. 
 Although these simulations show that the new measure outperforms the other three 
measures, we want to emphasize that there are also limitations associated with this new 
measure. First, we need to assume that clusters are line symmetrical structures. If the data 
set does not follow the assumption, the measure may not work well. Second, for large 
data sets, the determination of the measure is very computationally expensive. The price 
paid for the flexibility in detecting cluster number is the increase of computational com-
plexity. The computing complexity is O(N2), where N indicates the data number. In fact, a 
lot of future work can be done to improve not only the line symmetry distance but also 
the LS measure. 
 
Acknowledgement This work was supported by the National Science Council, Taiwan, 
R.O.C., under the Grant NSC 101-2221-E-032-055, NSC 100-2221-E-032-069, NSC 
101-2221-E-008-124-MY3, the NSC 100-2911-I-008-001-, and the NSC 
101-2631-S-008-001-. 
REFERENCES 
[1] A. K. Jain and R. C. Dubes, Algorithms for Clustering Data. Englewood Cliffs, NJ: 
Prentice Hall, New Jersey, 1988.  
[2] R. O. Duda, P. E. Hart, D. G. Stork, Pattern Classification, Wiley, New York, 2001. 
[3] J. Bezdek, Pattern Recognition with Fuzzy Objective Function Algorithms. New York: 
Plenum, 1981. 
[4] F. Höppner, F. Klawonn, R. Kruse, and T. Runkler, Fuzzy Cluster Analysis-Methods 
for Classification, Data Analysis and Image Recognition. John Wiley & Sons, LTD, 
1999. 
[5] C.-S. Fahn and Y.-C. Lo, ”On the Clustering of Head-Related Transfer Functions 
Used for 3-D Sound Localization,” Journal of Information Science and Engineering, 
Vol. 19, No. 1, 2003, pp. 141-157. 
[6] P.-C. Wang and J.-J. Leou, “New Fuzzy Hierarchical Clustering Algorithms,” Journal 
of Information Science and Engineering, Vol. 9, No. 3, 1993, pp. 461-489. 
A New Measure of Cluster Validity Using Line Symmetry 
 
17 
 
[26] M. C. Su, N. DeClaris, and T. K. Liu, “Application of neural networks in cluster 
analysis,” in IEEE International conference on systems, Man, and Cybernetics, 1997, 
pp. 1-6. 
[27] M. C. Su and H. C. Chang, “A new model of self-organizing neural networks and its 
application in data projection,” IEEE Trans. on Neural Networks, Vol. 12, 2001, pp. 
153-158. 
[28] H. Yin, “Data visualization and manifold mapping using the ViSOM,” Neural Net-
works, Vol. 15, 2002, pp. 1005-1016. 
[29] M. C. Su, S. Y. Su, and Y. X. Zhao, “A Swarm-Inspired Projection Algorithm,” 
Pattern Recognition, Vol. 42, No. 11, 2009, pp.2764-2786. 
[30] R. N. Dave, “New Measures for Evaluating Fuzzy Partitions Induced Through 
c-Shells Clustering,” Proc. SPIE Conf. Intell. Robot Computer Vision X, Vol. 1670, 
Boston, 1991, pp. 406-414. 
[31] R. N. Dave, “Validating Fuzzy Partitions Obtained Through C-Shells Clustering,” 
Pattern Recognition Letters, Vol. 17, 1996, pp. 613-623. 
[32] R. Krishnapuram, H. Frogui, and O. Nasraoui, “Fuzzy and Possibilistic Shell Clus-
tering Algorithms and Their Application to Boundary Detection and Surface Ap-
proximation – Part 1&2,” IEEE Trans. Fuzzy Systems, Vol. 3, No. 1, 1995, pp. 
29-61. 
[33] M. P. Windham, “Cluster Validity for Fuzzy Clustering Algorithms,” Fuzzy Sets and 
Systems, Vol. 5, 1981, pp. 177-185. 
[34] J. C. Bezdek and N. R. Pal, “Some New Indexes of Cluster Validity,” IEEE Trans. 
on System, Man, and Cybernetics, Vol. 28, No. 3, 1998, pp. 301-315. 
[35] V. S. Tseng and C.-P. Kao, “An Efficient Approach to Identifying and Validating 
Clusters in Multivariate Datasets with Applications in Gene Expression Analysis,” 
Journal of Information Science and Engineering, Vol. 20, No. 4, 2004, pp. 665-677. 
[36] J.-S. Wang and J.-C. Chiang, "A Cluster Validity Measure with Outlier Detection for 
Support Vector Clustering," IEEE Trans. on Systems, Man, and Cybernetics, Part B, 
Vol. 38, No. 1, 2008, pp. 78-89.  
[37] R. Jain and A. Koronios, “Innovation in the cluster validating techniques,” Fuzzy 
Optimization and Decision Making, Vol. 7, No. 3, 2008, pp. 233-241. 
[38] A. K. Jain, M. N. Murthy and P. J. Flynn, "Data Clustering: A Review," ACM Com-
puting Surveys, Vol. 31, No. 3, 1999, pp. 265-323.  
[39] D. Reisfeld, H. Wolfsow, and Y. Yeshurun, “Context-Free Attentional Operators: 
the Generalized Symmetry Transform,” International Journal of Computer Vision, 
Vol. 14, 1995, pp. 119-130. 
[40] D. Reisfeld and Y. Yeshurun, “Preprocessing of Face Images: Detection of Features 
and Pose Normalisation,” Computer Vision and Image Understanding, Vol. 71, No. 
3, 1998, pp. 413-430. 
[41] R. K. K. Yip, “A Hough transform technique for the detection of reflectional sym-
metry and skew-symmetry,” Pattern Recognition Letters, Vol. 21, No. 2, 2000, pp. 
117-130. 
[42] G. Loy and A. Zelinsky, “Fast radial symmetry for detecting points of interest,” 
IEEE Transactions on pattern analysis and machine, Vol. 25, No. 8, 2003, pp. 
959-973. 
[43] G. Loy and J. Eklundh, “Detecting symmetry and symmetric constellations of fea-
A New Measure of Cluster Validity Using Line Symmetry 
 
19 
 
Mu-Chun Su received the B. S. degree in electronics engineering 
from National Chiao Tung University, Taiwan, in 1986, and the M. S. 
and Ph.D. degrees in electrical engineering from University of 
Maryland, College Park, in 1990 and 1993, respectively. He was the 
IEEE Franklin V. Taylor Award recipient for the most outstanding 
paper co-authored with Dr. N. DeClaris and presented to the 1991 
IEEE SMC Conference. He is currently a professor of computer 
science and information engineering at National Central University, Taiwan. He is a sen-
ior member of the IEEE Computational Intelligence Society and Systems, Man, and Cy-
bernetics Society. His current research interests include neural networks, fuzzy systems, 
assistive technologies, swarm intelligence, effective computing, pattern recognition, 
physiological signal processing, and image processing. 
 
 
 
 1 
Implementation of Parallel Computing FAST Algorithm on Mobile GPU 
Chien-Hsing Chou1*, Peter Liu1, Yu-Xiang Zhao2, Tai-Yi Wu1, Yi-Hsiang Chien 1 
Department of Electrical Engineering, Tamkang University, Taiwan1 
Department of Computer Science & Information Engineering, National Quemoy University, Taiwan2 
Email: chchou@mail.tku.edu.tw 
 
Abstract 
Corner detection is an extremely important technique in image 
recognition, which is widely employed in various applications for 
image recognition. With the widespread use of mobile devices, 
image recognition techniques are frequently applied in such 
devices. However, the hardware resource of smartphones are 
lacked and restricted, it is a difficult task to apply the techniques 
of corner detection smoothly in these devices. To enhance the 
computational speed, the FAST corner detection algorithm is 
implemented with parallel computing of GPU in mobile devices. 
In the experiments, the computational speed of the FAST corner 
detection algorithm increases 24 times after using GPU parallel 
computing. Compared with the widely known SURF algorithm, 
which is computed with mobile CPU only, the proposed 
technique in this study is 468 times faster than SURF algorithm. 
Keywords: FAST algorithm, GPU, corner detection, mobile 
device, SURF 
 
1. Introduction 
Corner detection is an extremely important technique in 
image recognition, which is widely employed in various 
applications for image recognition. Corner detection is applied to 
applications including 3D reconstruction [1], object recognition 
[2], image mosaicing [3], and motion estimation [4], etc. Since 
the debut of the iPhone in 2008, smartphones have become 
necessary products in the lives of many people. Because 
smartphone characteristic are portable and practical, more and 
more image recognition applications are appearing in these 
devices. Examples include using facial recognition to unlock the 
smartphone; combining augmented reality with image 
recognition in applications [5], using auto-stitching to merge 
multiple images into one [6]-[8], and using seam carving to 
adjust image sizes [9]-[11]. However, the hardware resource of 
smartphones are lacked and restricted, it is a difficult task to 
apply these recognition techniques smoothly in these devices. 
To solve the above problem, the most straightforward 
solution is to employ GPU to assist the computation of the image 
recognition applications. In the latest version of the well-known 
open source computer vision library (OpenCV), part of the 
program already uses GPU to perform computations. Most 
methods employ CUDA[12] to execute numerous computations 
in GPUs[13]-[14]. However, these CUDA accelerated methods 
are only suitable for the latest NVIDIA graphics chips, can only 
run on standard PC platforms, and have no support for current 
mobile devices. In addition, the GPU inside smartphones may not 
use NVIDIA graphics chip, this also hinders researchers to 
transfer their research techniques to smartphones.  
In order to enhance the computational speed during 
detecting corners, the FAST algorithm [15] is chosen as the 
corner detection method to detect corners in an image, because it 
requires fewer computations. Then this study implements the 
FAST algorithm in mobile devices by employing GPGPU 
technique to control GPUs. In the Section 2 of this study, we 
introduce GPUs and the FAST algorithm. In Section 3, we 
explain how to use the parallel processing characteristics of 
GPUs to implement the FAST algorithm. During the experiment 
in Section 4, we compare the method proposed in this study with 
other available methods. In Section 5, we present our conclusions. 
 
2. Introduction to GPUs and the FAST Algorithm 
 
2.1 Graphics Processing Units  
In 1990, graphics processing units (GPUs) were employed 
to assist in the drawing of 2D images, such as for line drawing, 
tile rendering, and VCD/DVD playing acceleration. By the end 
of the 1990s, they were used in 3D accelerations. Around 2000, 
many affordable 3D acceleration display cards emerged in the 
market for 3D rendering. These 3D display cards reduce the 
computational loading for the central processing unit (CPU) 
while rendering 3D graphics. Before the existence of GPUs, all 
rendering computations were processed by the CPU. However, 
with GPU support, much of the computational load is now shared, 
increasing the efficiency of 2D and 3D image rendering for users.  
The hardware architecture of GPUs is a multi-core CPU, 
with up to hundreds of cores. Because most of the pixels in the 
image are rendered independently, no sequential relationships 
typically exist between pixels. Thus, if a GPU contains 32 
processing units, theoretically, the speed of rendering a single 
image might be 32 times faster than that of a single core CPU. 
Figure 1 is the architecture of CPU and GPU [16].  
 
Fig. 1. The architecture of CPU and GPU. 
 
Originally, GPUs were designed to render 3D graphics 
rather than to assist the computational tasks of variety image 
processing. However, in recent years, many researchers have 
used GPUs on PC platforms to perform computations involving 
convolution, RGB-grayscale conversion, and Gaussian blurring 
 3 
algorithm after GPU acceleration. Table 1 shows the hardware 
specifications of the iPhone 3GS and iPhone 4. The iPhone 4 has a 
faster CPU and more memory; however, both devices use the same 
graphics chip. For this experiment, we used four sizes of test images. 
The computational times are shown in Table 2. Based on the data in 
Table 2, we summarized the following two points: 
1. Although the iPhone 4 is superior to the iPhone 3GS for 
certain hardware specifications, the computational time of 
FAST algorithm conducted by GPUs was the same because 
of the identical graphics chip used.  
2. The required computation time did not increase linearly with 
increases in image size. For example, when the image size 
increased from 1024×1024 to 2048×2048, although four 
times as many pixels were used, the computational time was 
only 1.8 times longer. These result further validated another 
benefits of using GPU parallel computing. 
3.  
Table 1. The hardware specifications of the iPhone 3GS and iPhone 4.  
 iPhone 3GS iPhone 4 
CPU ARM Cortex-A8 
833 MHz 
Apple A4 
1 GHz 
Graphics PowerVR SGX535 GPU 
Memory 256 MB DRAM 512 MB DRAM 
Table 2. Experimental results of the iPhone 3GS and iPhone 4. 
Image Size iPhone 3GS (ms) iPhone 4 (ms) 
512x512 64 64 
1024x1024 81 81 
1536x1536 113 114 
2048x2048 147 146 
 
Experiment 2:  
 During this experiment, we compared the time required to 
compute the FAST algorithm using a GPU (FAST-GPU) or without 
(FAST-CPU). In addition, another well-known corner detection 
algorithm SURF (using CPU) was also compared. The experimental 
data are shown in Table 3. Figure 5 shows the experimental results of 
the three corner detection methods. From the data in Table 3, we 
summarized the following three points:  
1. When the image size is 512×512, computational speed of 
the FAST algorithm using a GPU is three times faster than 
that without using a GPU. Compared with SURF, it is 54 
times faster. When the image size is increased to 2048×2048, 
computational speed of the FAST algorithm increases by 24 
times after using a GPU. Compared with SURF, the 
computational speed differ by as much as 468 times.  
2. With the two CPU only methods, increases in image size are 
accompanied by linear increases in the required computation 
time. This indicates that parallel computing using GPUs is 
more effective for increasing speed when using larger 
images.  
3. Two main reasons lead to the long computation time for 
SURF algorithm. One is that the SURF algorithm is 
complex. The other is that when SURF algorithm uses the 
CPU for computations, the OS must continuously reallocate 
memory because of the limited memory of mobile devices, 
resulting in additional computation time.  
Table 3. Experimental results of three corner detection algorithms. 
Image-Size FAST-GPU (ms) 
FAST-CPU 
(ms) 
SURF-CPU 
(ms) 
512x512 64 201 3,452 
1024x1024 81 848 15,128 
1536x1536 114 1,877 36,301 
2048x2048 146 3,509 68,271 
  
(a)  (b)  
 
(c) 
Figure 5. The results of corner detection after using (a) FAST-
GPU; (b) FAST-CPU; (c) SURF-CPU。 
 
Experiment 3: 
For this experiment, we used various images to assess our 
proposed methods. Figure 6 shows the five test images used in 
this experiment [17]. Table 4 lists the average time required to 
process these images using FAST-GPU. By comparing the 
experimental data in Table 3, we found that no significant 
differences in computation times existed. This also indicates that 
our method is not affected by various test images. 
   
  
Figure 6. Five test images using in experiment 3. 
RDT04 
  
 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                          101  年   03   月   28   日   
  報    告    人 
  姓          名  
周建興 
 服 務 機 關 
 及   職  稱  
淡江大學電機系助理教授 
           時  間  
  會   議 
           地  點            
自 2012 年 03 月 24 日至
2012 年 03 月 26 日 
中國廈門
 本 會 核 定 
            
 補 助 文 號 
NSC 100-2221-E-032-069 
                     
會  議  名   稱 （ 中文 ）2012 自動控制與人工智慧國際研討會 
（ 英文 ）2012 International Conference on Automatic Control 
and Artificial Intelligence (ACAI 2012) 
                    
 發 表 論 文 題 目 （ 中文 ）使用群聚量測與模糊群聚演算法於消失點之偵測 
（ 英文 ）A New Validity Measure and Fuzzy Clustering algorithm for 
Vanishing-point Detection 
報告內容應包括下列各項： 
 
一、參加會議經過   
 
 
二、與會心得 
 
 
三、考察參觀活動(無是項活動者省略) 
 
 
四、建議 
 
 
五、攜回資料名稱及內容 
 
 
六、其他 
 
 
 
 
 
 
RDT04 
  
四、攜回資料名稱及內容 
    與會後攜回的主要資料，除了本次會議的詳細議程外，以及論文光碟 CD 一片及一本大
會論文摘要集。以及國際工程與科技學會(IET)的相關介紹資料。 
 
五、誌謝 
    本次經費支出係由國科會專題計劃(NSC 100-2221-E-032-069)出國經費補助 
100年度專題研究計畫研究成果彙整表 
計畫主持人：周建興 計畫編號：100-2221-E-032-069- 
計畫名稱：結合特徵排序的改良式浮動序列特徵擷取演算法 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100% 共發表兩篇國內的研討會論文 
研究報告/技術報告 0 0 100%  
研討會論文 2 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 3 100% 共三名碩士生參與此計畫 
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 1 100% 
共發表1篇國外期
刊論文與1篇國外
研討會論文 
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
