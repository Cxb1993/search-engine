中文摘要
關鍵字: 一般化霍福轉換、視覺物件檢索、平行處理、內嵌式系統、即時影像資料庫系統。
本計畫提出一結合修改式的一般化霍福轉換(Generalized Hough Transformation, GHT)及
視覺樣本之快速影像物件檢索方法，並據以研發大型影像資料庫的平行處理檢索方法及硬
體製作，提供內嵌式系統即時影像檢索平台。
快速地從大型多媒體資料庫系統檢索出符合使用者語意的物件不是一件容易的事，許多
技術和理論都還有研究及改進的空間，例如一個影像物件在檢索時，該物件可能在影像中
經過移動、旋轉或縮放大小，如此就增加了檢索的困難度。多媒體資料檢索包含資料偏移
(Data Skew)及執行偏移(Execution Skew)雙重問題，平行處理架構必須適當，才能克服輸
出入系統太慢或網路太窄，影響資料載入速度的問題；再者，使用一般化的循序執行電腦
系統比對資料，無法善用媒體資料處理本身隱含平行處理的因子，為了解決這個問題，本
計畫提出一平行物件比對的演算法，以利達到大型多媒體資料庫系統檢索即時處理的目標。
報告內容
1. Introduction
Although content-based image retrieval (CBIR) is
extremely desirable in many applications, it must overcome
several challenges including image segmentation, extracting
features from the images that capture the perceptual and
semantic meanings, and matching the images in a database
with a query image based on the extracted features [1-3].
Instead of segmentation and detailed object representation,
Iqbal and Aggarwal proposed an interesting way to extract the
image structure from large manmade objects for image
retrieval by perceptual grouping [4,5]. In [5], the isotropic and
anisotropic mappings, which are invariant to the translation,
rotation and reflection of image data, were also defined to
extract image structure without using image segmentation for
image retrieval. The use of perceptual grouping to extract
structure gives the system an edge over content-based image
retrieval systems that retrieve images containing structural
objects based purely upon low-level features. However, the
low-level features are sensitive to lighting change in an
unconstrained environment. Some methods are proposed in
the literature to solve the problem. Drew et. al. proposed an
illumination color covariant method for normalizing
illumination change [6]. In addition, making use of
feature-consistent locales in an image, they develop a scene
partition by localization, rather than by image segmentation.
While the locale-based object representation seems to
perform well in constructing an object-based image retrieval
system, the localization is not always perfect, locales can be
inaccurate or missing.
Orthogonal transforms (i.e. Fourier transform), which
offer invariance to geometric transformations, provide other
rotation, translation, and scale-invariant approaches to
content-based image retrieval [7]. This makes sense for
whole-image comparisons, but here we wish to seek instances
of a model within an image –a search by object model, and
hence, orthogonal transforms are ruled out. Jain et al. [8] used
deformable templates to retrieve images. Deformation
transformations with associated distribution were defined for
prototype templates, and using Bayesian probability a
multi-resolution parameter search was used to minimize the
objective function. While such search method is object-based,
however, the database has to be either consist of manually
extracted object boundaries, or there is an exhaustive search
for the template in an image; this is infeasible for an image
and video database in real-time.
Edge features, which are recognized as an important
aspect of human visual perception, are commonly used in
shape analysis. Decomposition of images into low-frequency
blocks and blocks containing visually important features
(such as edges or lines) suggest visual continuity and visual
discontinuity constraints. A block is visually continuous if the
values of all the pixels in the block are almost the same. In
contrast, if the variations of the pixel values in the block are
noticeable, it is a visually discontinuous block. The mean of a
visually continuous block is enough to represent the block. If
a block is visually discontinuous and if a strong orientation is
associated with it, then it should be coded as a kind of
visually important feature. Using coded edges, we can
represent the structure of an image without explicitly
extracting visual features.
This paper proposes an object-based image retrieval using
a method based on visual pattern matching. A visual pattern is
obtained by detecting the line edge from a square block using
the moment-preserving edge detector. Instead of segmentation
and detailed object representation, the objective of this
research is to develop and apply computer vision methods
that explore the structure of an image object by visual pattern
detection to retrieve images from a database. The visual
pattern of a square block can be rapidly extracted using an
analytical moment-preserving edge detector [3]. In contrast to
the integrated color- and shape-based methods proposed in
the literature, in this paper we propose the usage of
block-based visual patterns of an image as the features for
identification. A simple approach for image retrieval on the
basis of visual patterns is to encode both a database image
and a query image and then compute the difference between
the visual-pattern codes of the two images [3]. Unfortunately,
the retrieval results are sensitive to geometric transformations.
A voting scheme based on generalized Hough transform
(GHT) and visual patterns is proposed in this work to provide
object search method, which is invariant to the translation,
rotation, reflection of image data, and hence, invariant to
orientation and position.
2. Structure Analysis Using Visual Pattern
At the lowest level of computer vision, potentially useful
visual patterns such as edges and line segments can be
extracted from an image without any knowledge of the image
content. The low-level visual patterns can then hierarchically
grouped together to obtain meaningful high-level structure. In
the context of vision modeling for perception and recognition,
as opposed to recognition by analysis of discrete primitive
image features, the visual pattern grouping by proximity,
similarity, continuation, closure, and symmetry, embodies
such concepts [5,6].
In our approach, segmentation and detailed object
representation are not required to capture the structural
information of an image. A given image is partitioned into a
set of non-overlapping square blocks. Each block is coded as
either a uniform block or an edge block. The edge in each
block is detected by the moment-preserving edge detection
technique which was proposed in our previous work [3], and
the image can be reconstructed according to the parameters of
these blocks. The continuous two-dimensional edge model
specified by four parameters, two representative gray (color)
values h1 and h2, an edge translation l, and an orientation
angle for an edge in square block B is shown in Fig.1. The
edge translation l is defined as the length from the center of
the edge model to the transition, and is confined within the
range of 2 to 2 . The parameter  specified the
direction of the edge and is confined within the range of 0 to
180 degrees. The solution to the edge detection problem in a
given block is analytic and this means that the edge detection
process can be performed very fast for large-database
applications with no need for special hardware.
We approach the problem of object-based image retrieval
and the edge block Bˆ = )ˆ,ˆ,ˆ,ˆ( 21 lhh  centered at ),( ˆˆ BB yx be the
corresponding block of B in the target image. For each edge
block B in the model, we scan the edge blocks of the target
image one-by-one in the fashion of left-to-right and
up-to-down to find its possible matches. Suppose that B’is the
scanned block in the target image currently, ),,,( 21  lhhB
is the starting point to find the best
match )ˆ,ˆ,ˆ,ˆ(ˆ 21 lhhB nearing B
’, shown in Fig. 3. Obviously,
the orientation of Bˆ equals to that of B’, that is .ˆ  
Furthermore, we should have ll ˆ for Bˆ to be one of the
possible matches of B. The actual problem is: where should
the center ),( ˆˆ BB yx of Bˆ be? Obviously, the location of Bˆ can
be obtained by solving the following linear system
)(tan
sincos
ˆˆ
ˆˆ
BBBB
BBBB
xxyy
l
N
yy
N
xx
l









 . (2)
That is    .sin)(,cos)(, ˆˆ    llNyllNxyx BBBB The
remainder of the edge parameters of Bˆ can be approximated
as
  
  





otherwisehhhh
llifhhhh
1221
2121
,ˆ,ˆ
0,ˆ,ˆ . (3)
User Object Model Selection
R-table construction
Visual patterns detection using moment-preserving principle
Perform block matching to find block matches from the target image
Extract an edge block from the R-table
Is every block in the R-table checked?
Object centroid determination and scaling factor and rotation angle calculation
Voting on spaceyxs 
Peak detection and parameters verification
Matches
Yes
No
Fig. 2. Flowchart of object matching steps.
The selected object might be scaled in the target image.
Given a scaling factor s, the size of Bˆ should be changed
from NN into NsNs  . If s > 1, Bˆ is enlarged;
otherwise, Bˆ is downsized. In this case, the edge
parameters )ˆ,ˆ( l can also be set equal to ),( l ; however, the
method defined in (15) to determine the values
of )ˆ,ˆ( 21 hh should be modified. Once the location and the edge
parameters of Bˆ are determined, the line that separates the
pixels within Bˆ into two classes can be defined as
l
sN
yy
sN
xx
L BB ˆˆsinˆcos: ˆˆ 


  . (4)
The values of )ˆ,ˆ( 21 hh can then be determined as
])(,)([]ˆ,ˆ[
21
21 


ClasspClassp
pI
m
qm
pI
m
q
hh (5)
where m is the total number of pixels in Bˆ , q is the number
of pixels that are classified into Class 1, and I(p) returns the
pixel value of p. A pixel point at (x,y) in Bˆ is classified into
Class 1 if l
sN
yy
sN
xx
BB ˆˆsin
)(ˆcos
)( ˆˆ   , otherwise it is
classified into Class 2.
Object model
Target image
B Bˆ
Coded
blocks
B
Fig. 3. The boundary of the matches for a given edge block in
the selected object model might not be coincided with that of
coded blocks in the target image.
Given a scaling value s, for each block match
pair )ˆ,( BBP , we compute the support value as follows:
),(exp1
2
),(
Pa
Pah 


 (6)
where the parameter is used to control the speed that the
support h achieves one of its two extremes 0 and 1 according
to the value of ),( Pa
 (defined as the difference between B
and Bˆ ). The value of h is about 1 if the value of ),( Pa nears
zero. On the contrary, the value of h is about 0 if the
transformed block difference is large. The value ofis set to
be 0.01 in this study. Once the value of ),( Pah  is larger than a
pre-defined threshold (i.e. 0.5), P has a vote on the
coordinates  RR yxs ,,ˆ,  using (1); otherwise, P is not
used in the voting process.
The time to locate the peak corresponding to model in the
yxs  parameter space can be reduced by first
codes also makes the retrieval process robust and accurate,
and the method using the modified generalized Hough
transform to find the correct geometry transformation
parameters in object searches without the main disadvantage
of high-computational complexity for traditional generalized
Hough transform. Furthermore, the proposed method does not
suffer from the problems of object segmentation used in
conventional object search approaches.
The speed of retrievals can also be increased without
affecting the robustness of the system by clustering the visual
patterns of a database image in advance. Future work will
deal with linking edge patterns into objects to the proposed
system, and increasing the database size.
References
[1] P. John Eakins, “Towards Inteligent Image Retrieval,” 
Pattern Recognition, vol. 1 no. 35, pp3-14, 2002.
[2] A. W. Smeulders, M. M. Worring, A. Gupta, and R. Jain,
“Content-Based Image Retrieval at the End of the Early
Years,”IEEE Trans. Pattern Anal. Machine Intell., vol. 22
no. 12, pp1349-1380, 2000.
[3] S. C. Cheng, “Content-based Image Retrieval Using
Moment-preserving Edge Detection,” Image and Vision
Computing, vol. 21 no 9, pp. 809-826, Sep. 2003.
[4] Q. Iqbal and J. K. Aggarwal, “Retrieval by
Classification of Images Containing Large Manmade
Objects Using Perceptual Grouping,” Pattern
Recognition, vol. 35 pp. 1463-1479, 2002.
[5] Q. Iqbal and J. K. Aggarwal, “Image Retrieval Via
Isotropic and Anistropic Mappings,”Pattern Recognition,
vol. 35 pp. 2673-2686, 2002.
[6] M. S. Drew, Z.-N. Li, and Z. Tauber,“Illumination Color
Covariant Locale-Based Visual Object Retrieval,”Pattern
Recognition, vol. 35, pp. 1687-1704, 2002.
[7] R. Milanese, “A Rotation, Translation, and
Scale-Invariant Approach to Content-Based Image
Retrieval,”Journal of Visual Communication and Image
Representation, Vol. 10, pp. 186-196, 1999.
[8] A. K. Jain, Y. Zhong, and S. Lakshmanan, “Object
Matching Using Deformable Templates,” IEEE Trans.
Pattern Anal. Mach. Intell. Vol 18, no. 3, pp. 267-278,
1996.
[9] D. Ballard, “Generalizing the Hough Transform to
Detect Arbitrary Shapes,”Pattern Recognition, Vol. 13 No.
2, pp. 111-122, 1981.
[10] K. Jain and A. Vailaya, “Image Retrieval Using Color
and Shape,”Pattern Recognition, Vol. 29 pp.1233–1244,
1996.
可供推廣之研發成果資料表
■ 可申請專利 □ 可技術移轉 日期： 年 月 日
國科會補助計畫
計畫名稱：即時內容導向影像檢索的平行處理演算法設計及其硬體
設計
計畫主持人：鄭錫齊
計畫編號：NSC 95－2221－E－327－093－學門領域：資訊二
技術/創作名稱 視覺物件影像檢索
發明人/創作人 鄭錫齊
中文：
快速地從大型多媒體資料庫系統檢索出符合使用者語意的物件
不是一件容易的事，許多技術和理論都還有研究及改進的空間，例
如一個影像物件在檢索時，該物件可能在影像中經過移動、旋轉或
縮放大小，如此就增加了檢索的困難度。多媒體資料檢索包含資料
偏移(Data Skew)及執行偏移(Execution Skew)雙重問題，平行處
理架構必須適當，才能克服輸出入系統太慢或網路太窄，影響資料
載入速度的問題；再者，使用一般化的循序執行電腦系統比對資
料，無法善用媒體資料處理本身隱含平行處理的因子，為了解決這
個問題，本計畫提出一平行物件比對的演算法，以利達到大型多媒
體資料庫系統檢索即時處理的目標。
技術說明
英文：
In this project, a fast content-based image retrieval method on the
basis of the modified generalized Hough transform (GHT) and visual
pattern matching is proposed. A parallel algorithm is also proposed to
incorporate the implicit fine-gram parallelism of GHT and
visual-pattern matching in the visual object retrieval method. Finally,
the hardware implementation of the proposed parallel algorithm is
included in this project to aim at constructing a real-time image
database retrieval system.
It is obviously not a trivial work to quickly retrieve a semantic
object from a large multimedia database. Many methods proposed in
the literature need modified to satisfy the real-time requirement of
image retrieval. Many factors affect the retrieval accuracy and speed.
For example, given a query object, the visual object might be
translated, rotated, and scaling from the database images to increase the
difficulty for answering the query. The problems to retrieval visual
objects from a large multimedia database are twofold: data skew and
execution skew. The parallel architecture of the media server should be
designed properly in order to overcome the I/O bound problem.
Furthermore, the sequential execution model of general CPUs cannot
efficiently explore the fine-gram parallelism of media computation in
general. In this project, we design a parallel algorithm to speed up the
execution of the proposed visual object retrieval method.
行政院國家科學委員會補助國內專家學者出席國際學術會議報告
95 年 10 月 16 日
報告人姓名 鄭錫齊 服務機構
及職稱
國立台灣海洋大學教授
會議期間及地點 2006/10/8~10/11
美國亞特蘭大
本會核定
補助文號
NSC 95－2221－E－327－
093－
會議名稱 (中文)2006 年 IEEE 國際影像處理研討會
(英文)IEEE International Conference of Image Processing 2006
發表論文題目 (中文)非變型的圖形擷取使用以區塊為基礎比對方法
(英文) Invariant image retrieval using block-based visual pattern
matching
報告內容：
一、參加會議經過
2006年IEEE國際影像處理研討會於95年10月8日至11日在美國亞特蘭大巿舉行，IEEE
國際影像處理研討會已經舉行多年，前幾屆分別在全世界各地舉辦，舉凡美國、澳大利
亞、日本、新加坡、西班牙、義大利等世界重要國家均曾舉辦過，本屆由美國亞特蘭大
喬智亞理工舉辦，會議大會收到來自超過60個不同國家接近3000篇投稿論文，其中只有
47%的投稿論文被接受參與勝會，討論最新的技術發展趨勢，來自全世界各知名大學超
過600個學者，齊聚一堂共同討論影像處理的最新技術。
今年該研討會著重在電腦影像及視訊的六大領域，分別為 1.電腦影像/視訊的編碼
技術及傳輸(Image/video coding and transmission)，2.電腦影像/視訊的處理(Image/
video processing)，3.影像的格式(Image formation)，4.影像的掃描、顯示及輸出
(Image scanning, display, and printing)，5. 電腦影像/視訊的儲存、擷掫及驗證
(Image/video storage, retrieval, and authentication) 及 6. 影 像 的 應 用
(Applications)。這是個人第二次參加此類型的國際研討會，在 4月 17 日收到論文接
受信後，即開始規劃參加研討會事宜。本人與博士班學生郭振宗，於 10 月 7 日晚上自
桃園國際機場塔乘中華航空公司隊機出發，經由美國洛杉磯改塔達美航空(Delta)轉
機，於當地 10 月 8 日早上八點到達亞特蘭大巿的會場。隨即展開為期四天的研討會議
程。
二、與會心得
第 13 屆 2006 年國際影像處理研討會(ICIP 2006)是美國電機及電子工程師學會
(Institute of Electrical and Electronics Engineers; IEEE)所主辦，自 1994 年起
舉辦在電腦影像及視訊領域中就理論發展及應用最具聲望的研討會之一。
第一天(10 月 8 日)是 Tutorials 課程，分別為
INVARIANT IMAGE RETRIEVAL USING BLOCK-BASED VISUAL PATTERN MATCHING
Shyi-Chyi Cheng1, Chen-Tsung Kuo2, and Hong-Jay Chen2
1Department of Computer Science, National Taiwan Ocean University, Taiwan
2Department of Computer and Communication Engineering, National Kaohsiung First University of
Science and Technology, Taiwan
ABSTRACT
This paper proposes an object-based image retrieval using a
method based on visual pattern matching. A visual pattern is
obtained by detecting the line edge from a square block using the
moment-preserving edge detector. It is desirable and yet remains as
a challenge for querying multimedia data by finding an object
inside a target image. Given an object model, an added difficulty is
that the object might be translated, rotated, and scaled inside a
target image. Instead of segmentation and detailed object
representation, the objective of this research is to develop and
apply computer vision methods that explore the structure of an
image object by visual pattern detection to retrieve images from a
database. A voting scheme based on generalized Hough transform
is proposed to provide object search method, which is invariant to
the translation, rotation, scaling of image data. Computer
simulation results show that the proposed method gives good
performance in terms of retrieval accuracy and robustness.
Index Terms— Hough transforms, object detection, information
retrieval
1. INTRODUCTION
Although content-based image retrieval (CBIR) is extremely
desirable in many applications, it must overcome several
challenges including image segmentation, extracting features from
the images that capture the perceptual and semantic meanings, and
matching the images in a database with a query image based on the
extracted features [1-3].
Instead of segmentation and detailed object representation,
Iqbal and Aggarwal proposed an interesting way to extract the
image structure from large manmade objects for image retrieval by
perceptual grouping [4,5]. In [5], the isotropic and anisotropic
mappings, which are invariant to the translation, rotation and
reflection of image data, were also defined to extract image
structure without using image segmentation for image retrieval.
The use of perceptual grouping to extract structure gives the
system an edge over content-based image retrieval systems that
retrieve images containing structural objects based purely upon
low-level features. However, the low-level features are sensitive to
lighting change in an unconstrained environment. Some methods
are proposed in the literature to solve the problem. Drew et. al.
proposed an illumination color covariant method for normalizing
illumination change [6]. In addition, making use of feature-
consistent locales in an image, they develop a scene partition by
localization, rather than by image segmentation. While the locale-
based object representation seems to perform well in constructing
an object-based image retrieval system, the localization is not
always perfect, locales can be inaccurate or missing.
Orthogonal transforms (i.e. Fourier transform), which offer
invariance to geometric transformations, provide other rotation,
translation, and scale-invariant approaches to content-based image
retrieval [7]. This makes sense for whole-image comparisons, but
here we wish to seek instances of a model within an image –a
search by object model, and hence, orthogonal transforms are ruled
out. Jain et al. [8] used deformable templates to retrieve images.
Deformation transformations with associated distribution were
defined for prototype templates, and using Bayesian probability a
multi-resolution parameter search was used to minimize the
objective function. While such search method is object-based,
however, the database has to be either consist of manually
extracted object boundaries, or there is an exhaustive search for the
template in an image; this is infeasible for an image and video
database in real-time.
Edge features, which are recognized as an important aspect of
human visual perception, are commonly used in shape analysis.
Decomposition of images into low-frequency blocks and blocks
containing visually important features (such as edges or lines)
suggest visual continuity and visual discontinuity constraints. A
block is visually continuous if the values of all the pixels in the
block are almost the same. In contrast, if the variations of the pixel
values in the block are noticeable, it is a visually discontinuous
block. The mean of a visually continuous block is enough to
represent the block. If a block is visually discontinuous and if a
strong orientation is associated with it, then it should be coded as a
kind of visually important feature. Using coded edges, we can
represent the structure of an image without explicitly extracting
visual features.
This paper proposes an object-based image retrieval using a
method based on visual pattern matching. A visual pattern is
obtained by detecting the line edge from a square block using the
moment-preserving edge detector. Instead of segmentation and
detailed object representation, the objective of this research is to
develop and apply computer vision methods that explore the
structure of an image object by visual pattern detection to retrieve
images from a database. The visual pattern of a square block can
be rapidly extracted using an analytical moment-preserving edge
detector [3]. In contrast to the integrated color- and shape-based
methods proposed in the literature, in this paper we propose the
usage of block-based visual patterns of an image as the features for
identification. A simple approach for image retrieval on the basis
of visual patterns is to encode both a database image and a query
image and then compute the difference between the visual-pattern
codes of the two images [3]. Unfortunately, the retrieval results are
Let the edge block B = ),,,( 21 lhh  be part of boundary of an
object C centered at ),( 00 yx in the selected object model, and the
edge block Bˆ = )ˆ,ˆ,ˆ,ˆ( 21 lhh  centered at ),( ˆˆ BB yx be the corresponding
block of B in the target image. For each edge block B in the model,
we scan the edge blocks of the target image one-by-one in the
fashion of left-to-right and up-to-down to find its possible matches.
Suppose that B’is the scanned block in the target image currently,
),,,( 21  lhhB is the starting point to find the best
match )ˆ,ˆ,ˆ,ˆ(ˆ 21 lhhB nearing B
’, shown in Fig. 3. Obviously, the
orientation of Bˆ equals to that of B’, that is .ˆ   Furthermore, we
should have ll ˆ for Bˆ to be one of the possible matches of B. The
actual problem is: where should the center ),( ˆˆ BB yx of Bˆ be?
Obviously, the location of Bˆ can be obtained by solving the
following linear system
)(tan
sincos
ˆˆ
ˆˆ
BBBB
BBBB
xxyy
l
N
yy
N
xx
l









 . (2)
That is    .sin)(,cos)(, ˆˆ    llNyllNxyx BBBB The
remainder of the edge parameters of Bˆ can be approximated as
  
  




otherwisehhhh
llifhhhh
1221
2121
,ˆ,ˆ
0,ˆ,ˆ . (3)
User Object Model Selection
R-table construction
Visual patterns detection using moment-preserving principle
Perform block matching to find block matches from the target image
Extract an edge block from the R-table
Is every block in the R-table checked?
Object centroid determination and scaling factor and rotation angle calculation
Voting on spaceyxs 
Peak detection and parameters verification
Matches
Yes
No
Fig. 2. Flowchart of object matching steps.
The selected object might be scaled in the target image. Given a
scaling factor s, the size of Bˆ should be changed
from NN into NsNs  . If s > 1, Bˆ is enlarged; otherwise, Bˆ is
downsized. In this case, the edge parameters )ˆ,ˆ( l can also be set
equal to ),( l ; however, the method defined in (15) to determine
the values of )ˆ,ˆ( 21 hh should be modified. Once the location and the
edge parameters of Bˆ are determined, the line that separates the
pixels within Bˆ into two classes can be defined as
l
sN
yy
sN
xx
L BB ˆˆsinˆcos: ˆˆ 


  . (4)
The values of )ˆ,ˆ( 21 hh can then be determined as
])(,)([]ˆ,ˆ[
21
21 


ClasspClassp
pI
m
qm
pI
m
q
hh (5)
where m is the total number of pixels in Bˆ , q is the number of
pixels that are classified into Class 1, and I(p) returns the pixel
value of p. A pixel point at (x,y) in Bˆ is classified into Class 1 if
l
sN
yy
sN
xx
BB ˆˆsin
)(ˆcos
)( ˆˆ   , otherwise it is classified into Class 2.
Object model
Target image
B Bˆ
Coded
blocks
B
Fig. 3. The boundary of the matches for a given edge block in the
selected object model might not be coincided with that of coded
blocks in the target image.
Given a scaling value s, for each block match pair )ˆ,( BBP , we
compute the support value as follows:
),(exp1
2
),( PaPah 


 (6)
where the parameter is used to control the speed that the support
h achieves one of its two extremes 0 and 1 according to the value
of ),( Pa (defined as the difference between B and Bˆ ). The value
of h is about 1 if the value of ),( Pa nears zero. On the contrary,
the value of h is about 0 if the transformed block difference is large.
The value ofis set to be 0.01 in this study. Once the value of
),( Pah
 is larger than a pre-defined threshold (i.e. 0.5), P has a vote
on the coordinates  RR yxs ,,ˆ,  using (1); otherwise, P is not
used in the voting process.
The time to locate the peak corresponding to model in the
yxs  parameter space can be reduced by first approximating
s*,*, x*, and y* and then refining the search in a small area. In
general, most blocks in an image are uniform, and so the number
of edge blocks is not large. Hence, the execution speed of the
proposed approach is fast.
4. EXPERIMENTAL RESULTS
In order to evaluate the proposed approach, a series of
experiments was conducted on an Intel PENTIUM-IV 1.5GHz PC
and a color image database consisted of 20,000 scenery images,
coming from Corel’s CorelPhoto image databases, was used. The
size of each image in the database is 384 x 256. Query images of
different sizes were extracted from these images.
For retrieval on the basis of visual patterns, the experiments
were conducted on the 100 color images randomly selected from
our database as the query images. The retrieval technique based on
edge and color histograms proposed by Jain and Vailaya [10] was
also implemented for performance comparison. Table 1 presents
the retrieval results of the proposed method and Jain’s method,
