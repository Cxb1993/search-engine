 2
guarantee the good utilization of limited 
memory bandwidth. Therefore we develop 
novel memory subsystem, which integrates a 
new interconnection network, a 
scheduling/arbitration mechanism, a unified 
access buffer, and a multiple channel 
memory, to provide a sufficient memory 
bandwidth for the multimedia processors 
with high bandwidth requirements. The 
performance bottleneck of the memory 
access in the modern multimedia SOC chips 
also can be solved.  
To make our memory subsystem 
become completely, we will focus on 
modeling the basic simulation environment 
by using SystemC and Electronic System 
Level (ESL) design. Then we can rapidly 
build the virtual platform and the model of 
our high-performance memory controller in 
a complete system platform.  
The execution of this project not only 
for designing a novel high-performance 
memory controller but also can be the 
reference of a modern memory subsystem 
for other researchers who intend to develop 
new MPSoC (Multi-Processor System-on-a- 
Chip) architectures. 
 
Keywords: High-Performance Memory 
Controller, Memory Access Scheduling, 
Multimedia System-on-a-Chip, 
Interconnection Network, SystemC, 
Electronic System Level Design. 
 
二、緣由與目的 
  在消費型電子產品的蓬勃發展下，為
了提供消費者更多的影音視訊整合服務，
通常以一顆複雜而功能強大的多媒體系統
晶 片 (Multimedia System-on-a-Chip; 
Multimedia SOC)，來負責主要多媒體計
算。此種多媒體系統晶片，內部整合多個
多媒體運算處理器(Multimedia Processor)/
矽智產(IP) 於在單一晶片內，以便滿足各
項 不 同 的 多 媒 體 服 務 ， 如 MP3 
Encode/Decode, MPEG4 Encode/Decode…
等等。晶片的功能越來越多，每個
Multimedia Processor 所需的資料吞吐量
(Data Throughput)也越來越高，但是整個
系統晶片的效能往往不能完全滿足使用者
的期待。分析其原因，主要是因為記憶體
子系統的頻寬不足，而無法充分發揮所有
Multimedia Processor 的計算效能。 
  有鑑於此，本計劃著眼於記憶體子系
統之效能與資料吞吐量之提升，以解決在
多媒體系統晶片下，記憶體頻寬嚴重不足
的問題。此一新設計之記憶體子系統，簡
而言之，即是擔任各 Multimedia Processor
和記憶體兩者之間溝通的橋樑，任何
Multimedia Processor 資料的存取皆經過此
子系統來達到讀取和寫入的動作。在先前
的初期研究探索後，我們發現：當系統晶
片上有許多個高頻寬需求的 Multimedia 
Processor 時，頻寬的滿足除了取決於記憶
體的效能外，更需要動態的隨著不同的
Multimedia Processor 記憶體存取的需求分
配，才能確保有限頻寬的最佳利用。因此
在本計劃裡，我們發展一個全新的記憶體
子系統，透過新型互連網路、排程/仲裁機
制、存取緩衝區、與多通道記憶體的配合，
提供多個高頻寬需求的多媒體處理器，所
需要的記憶體存取能力，以解決多媒體
SOC 系統中，記憶體存取的效能瓶頸。 
  一個多媒體 SOC 系統主要的效能除
了受到製程技術的影響之外，主要取決於
架構設計上；記憶體由於製程的的不同，
使得其在 SOC 晶片製作過程中，必須放置
於晶片外，所以系統架構上以 Shared 
Memory 為基礎架構。在此架構下，所有
存取記憶體之行為都必須透過系統匯流排
（System Bus）才能完成，造成一個時脈
週期內只能有一個 Multimedia Processor
能 對 記 憶 體 作 存 取 ， 所 以 當 多 個
Multimedia Processor 在同一時間內都要對
記憶體作存取時，就會造成衝突而影響系
統效能，因此降低記憶體子系統之存取衝
突，充分提升記憶體的使用效率，將可大
幅度增加整體效能。當 SOC 為了提高效
能，朝多處理器系統發展，多個處理器對
記憶體存取的行為，決定了整體系統的效
能，所以在一個多處理器系統下，如何有
效地存取記憶體，就變的格外的重要。 
 4
提供的 AHB Bus Functional Model，以
求與真實系統相近，亦有真實系統匯流
排之頻寬限制，而非傳統以 C/C++設計
之模擬器的假設性的參數設定。除此之
外，我們自行研發 Memory Controller
及 Multimedia Processor 之可參數化
SystemC 模組，此平台之設計，預期可
以找出一般多媒體應用之 SOC 系統，可
能之效能瓶頸，並作為此一研究後續階
段之高效能記憶體控制器效能比較之基
本平台。 
 
階段三：System Specification Definition： 
 
在此一階段，我們需針對設計高效能
記憶體控制器，所要解決之問題與系統
規格加以釐清，並決定該以何種演算法
與機制，進行更有效率的排程規劃，並
能充分發揮晶片外記憶體 (Off-Chip 
Memory)的最大頻寬，以減少各種潛在
的延遲與等待時間。首先，我們先定義
各 個 所 需 的 功 能 模 組 （ Function 
Block），規劃其需要具備的功能、介面
（Interface）以及需要達到的效能要求。 
為了解決晶片上 Multimedia Processor
大量存取記憶體所造成的效能瓶頸，在
本研究中在高效能記憶體控制器內提供
資料排程緩衝區（ Data Scheduling 
Buffer）。在 Data Scheduling Buffer 與
Multimedia Processor 間，需要提供快速
的資料傳遞方法，所以需要一個簡單且
有效的 Interconnect Network與一個有效
的排程機制來達成此一目標。並希望能
在 Data Scheduling Buffer 與系統記憶體
間，提供針對各個存取記憶體之行為，
設計適合的排程機制，使記憶體之存取
更有效率。 
圖三為本研究之系統架構圖，此一
SOC 晶 片 包 含 了 四 個 Multimedia 
Processor 與基本所需要的週邊 IP；本研
究之主題：高效能記憶體控制器，亦置
於 SOC 晶片內，並與晶片外記憶體連
接，以作為主要的記憶體管理模組。其
中的高效能記憶體控制器包含下列數個
重要模組： 
 
(1) DRAM Controller：此模組為是存取
外部 DRAM Bank 的記憶體控制器。 
(2) Scheduler Block：此模組有兩個主要
任務，一是決定各緩衝區存取系統
記憶體資料之順序，一是決定各個
Multimedia Processor 與 Data 
Scheduling Buffer 之間之資料傳
輸，處理器可透過專屬控制網路，
設定並控制其排程演算法與排程策
略。 
(3) Data Scheduling Buffer：此模組是
Multimedia Processor 與 DRAM 的資
料交換緩衝區。 
(4) Interconnect Network：則是各個高頻
寬需求之 Multimedia Processor 與
Data Scheduling Buffer 之間的連接
骨幹。 
 
以下簡述兩個排程器之初步排程演算
法： 
 
【演算法一】Multimedia Processor 與
Data Scheduling Buffer 間排程演算法： 
 
為 了 簡 化 設 計 問 題 ， 在 此 先
Multimedia Processor 與 Data 
Scheduling Buffer 之間的關係設為多
對一存取，提供以下初步的演算法，
如演算法一所示： 
 
 
【演算法二】Data Scheduling Buffer 與
DRAM System 間排程演算法： 
 
此 部 分 傳 輸 效 能 主 要 受 限 於
DRAM 之存取時間較長，所以我們將
DRAM System 之頻寬加大，以滿足
Multimedia Processor 之高頻寬需求，
在搭配初期簡單的排程機制，以期望
系統效能有顯著的提升，如演算法二
所示： 
 
 
階段四：SystemC Modeling： 
 6
185-193 (July 2010) (NSC 
98-2221-E-033 -042) 
[4] Advanced RISC Machine Ltd., “AMBA 
Specification Rev 2.0”, 1999, 
http://www.arm.com. 
[5] Synopsys Inc., http://www.synopsys.com. 
[6] C. A. Waldspurger and W.E. Weihl, 
“Lottery Scheduling: Flexible 
Proportional-Share Resource 
Management,” in: Proc. of symposim on 
Operating System Design and 
Implementation, pp.1-11, Nov. 1994. 
[7] C. H. Chen, G. W. Lee, J. D. Huang, and 
J. Y. Jou, “Real-time and bandwidth 
guaranteed arbitration algorithm for SoC 
bus communication”,  in: Proc. of 11th 
Asia and South Pacific Design 
Automation Conference 
(ASP-DAC2006) ,pp.600-605, Jan. 2006 . 
[8] CoWare Inc., http://www.coware.com. 
[9] G. Kornaros , I. Papaefstathiou,A. 
Nikologiannis , and N. Zervos, “A 
Fully-Programmable Memory 
Management System Optimization Queue 
Handling at Multi Gigabit Rate” in: Proc. 
of Design Automation Conference,  pp. 
54- 59, June 2003.  
[10] IBM Corporation, “CoreConnect 
Web-Page”, http://www-03.ibm.com/ 
technology/ges/semiconductor/power/lice
nsing/coreconnect/. 
[11] Intel Corporation, “8237A Datasheet”; 
http://www.intel.com. 
[12] Nieh, M. S. Lam, “The Design, 
Implementation and Evaluation of 
SMART: A Scheduler for Multimedia 
Applications”, in: Proc. of the Sixteenth 
ACM Symposium on Operating Systems 
Principles, pp.184-197, Oct. 1997.  
[13] K. Vlachos , N. Nikolaou , T. 
Orphanoudakis , S. Perissakis , D. 
Pnevmatikatos , G. Kornaros , J. A. 
Sanchez , G. Konstantoulakis , 
“Processing and Scheduling Components 
in an Innovative Network Processor 
Architecture” in: Proc. of the 16th VLSI 
Conference, pp.195-201, Jan. 2003.    
[14] M. Conti, M. Caldari, G. B. Vece , S. 
Orcioni, and C. Turchetti, “Performance 
Analysis of Different Arbitration 
Algorithms of the AMBA AHB Bus,” in 
Proc. of the 41st Design Automation 
Conference, pp.618-621, 2004. 
[15] Open Core Protocol International 
Partnership, http://www.ocpip.org. 
[16] Open SystemC Initiative, 
http://www.systemc.org. 
[17] S. Rixner, W. J. Dally, U. J. Kapasi, P. 
Mattson, J. D. Owens. ”Memory Access 
Scheduling”, in: Proc. of the 27th Annual 
International Symposium on Computer 
Architecture, Vancouver, Canada, pp. 
128-138, Jun. 2000. 
[18] Sonics Inc, http://www.sonicsinc.com/.
 8
 
 
演算法二： Data Scheduling Buffer 與 DRAM System 間
排程演算法： 
 
Step 1：由排程器檢視 Data Scheduling Buffer 內各 Multimedia 
Processor 之資料緩衝區的狀態，以決定哪些 Multimedia 
Processor 之存取要加入排程。 
Step 2：算出各個加入排程的 Multimedia Processor 存取之權重（實際傳輸
與需求傳輸之比值，經 Hash Function 處理），允許權重最高的進行
傳輸。 
Step 3：當最高權重的 Multimedia Processor 存取有數個時，以 Round Robin
的方式排程各個加入排程的 Multimedia Processor 存取，time 
slice 則是 DRAM 之存取時間。 
Step 4：控制 DRAM System 與 Data Scheduling Buffer 內各 Multimedia 
Processor 之資料緩衝區之傳輸進行。 
Step 5：結束此次傳輸，返回 Step 1。 
 
 
 
 
 10
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
0
100000
200000
300000
400000
500000
600000
700000
800000
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49
Clock Cycle (million)
Tr
an
sfe
r N
um
be
r (
By
tes
)
Require Transfer
Number
Scheduling with
Fixed Boundary
Scheduling with
Variable
Boundary
MPEG4 Encoder @1080p
 
(a) 
0
100000
200000
300000
400000
500000
600000
700000
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49
Clock Cycle (million)
Tr
an
sf
er
 N
um
be
r (
B
yt
es
)
Require Transfer
Number
Scheduling With
Fixed Boundary
Scheduling With
Variable Boundary
MPEG4 Encoder 720p
 
(b) 
0
100000
200000
300000
400000
500000
600000
700000
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49
Clock Cycle (million)
Tr
an
sf
er
 N
um
be
r (
B
yt
es
) Require TransferNumber
Scheduling With
Fixed Boundary
Scheduling With
Variable Boundary
MPEG4 Encoder 720x480
 
(c) 
0
100000
200000
300000
400000
500000
600000
700000
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49
Clock Cycle (million)
Tr
an
sf
er
 N
um
be
r (
B
yt
es
)
Require Transfer
Number
Scheduling With
Fixed Boundary
Scheduling With
Variabled
Boundary
MPEG4 Encoder 320x200
 
(d) 
圖三：啟動高效能記憶體控制器動態排程機制對四項高頻寬IP之影響。 
註：(a) MPEG4 encoder in 1080p, (b) MPEG4 encoder in 720p,  
(c)MPEG4 encoder in 720x480, (d) MPEG4 encoder in 320x200. 
 2
我從中學到了許多新觀念及想法。 
  附報告相片乙張 
 
 
三、考察參觀活動(無是項活動者略) 
四、建議 
  參加各種國際會議，對我來說可以是一個非常寶貴且收穫良多的經驗。故建議國科會能多鼓
勵學生出國參加專長領域之國際研討會上報告自已的研究成果。這樣不但能夠開擴學生的視野、
培養學生的國際觀、增進學生的語文能力、此外也能提升學校在國際舞台上的地位。 
 
五、攜回資料名稱及內容 
  HPCC 會議論文集光碟：收錄本次研討會所接受之論文電子全文。 
六、其他 
  發表論文請詳見隨附資料。 
 
results will be discussed and the conclusion will be 
addressed in Section 5. 
II. BACKGROUND 
In this section, the hardware platforms used in this paper 
will be reviewed. The architectural summary of architectures 
studied is listed in Table 1. 
TABLE I.  THE SUMMARY OF ARCHITECTURES USED IN THIS STUDY. 
20881062.7220445.2811.2GFLOPS (single)
418891545.2811.2GFLOPS (double)
14402401+648Number of PE
40nm
151W
334mm2
2.15B
725MHz
ATi Radeon 
HD5850
55nm90nm40nm65nmFabrication Process
183W110W80W95WTDP
576mm2235mm2107mm2342mm2Chip area
1.4B241M825M503MTransistor count
1.476GHz3.2GHz2.83GHz1.4GHzClock rate of PE
nVidia 
GeForce
GTX285
IBM Cell in 
PlayStation3
Intel Xeon 
E5440
SUN 
UltraSPARC
T2
 
A. ATi Radeon HD5800 series 
Radeon HD5800 series[4] are the latest GPU family 
announced by ATi in 2009, with over 2 tera FLOPs 
computing capability. The architecture of HD5800 is shown 
in Figure 2. The main computing power come from the 20 
SIMD engines; each engine consists of 16 thread processors, 
which contains 5 stream cores as fundamental processing 
elements,. Totally, this chip contains 1600 stream cores. 
Each stream core can perform IEEE754 compatible floating 
point operations, even fused-multiply-add operations. This 
architecture is designed by mixing SIMD and VLIW 
techniques. Accordingly, the device drivers of these GPU 
have to pack instructions into VLIW format and arrange 16 
thread processors to feed into a SIMD engine. In order to 
fully utilize all SIMD engines, the given workloads need to 
be scheduled and dispatched by Ultra-Threaded Dispatch 
Processor, according to their ages, requirements and 
availabilities. Then each SIMD engine will keep multiple 
threads alive and switching, while encounters long latency 
operations. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
B. nVidia GeForce GT200 series 
nVidia has announced its GT200 series[2] GPU in early 
2009, with over 1 tera FLOPs computing capability. The 
architecture of nVidia GT200 was shown in Figure 3. It 
consists of 10 TPCs (Texture/Processor Clusters); each TPC 
consists of 3 SM (Streaming Multiprocessors) and 1 Texture 
unit. The main computing elements in GT200 are the SPs 
(Streaming Processors), where the 8 SPs work in SIMD 
(Single Instruction Multiple Data) fashion. Both SPs and 
SFUs (Special Function Units) execute the same instruction, 
share L1 data/instruction cache, In a GT200 GPU, there will 
be 240 SPs in total. Accordingly, the computing threads will 
be arranged into warps and blocks, where 32 threads formed 
a warp and multiple warps formed a block. Since each warp 
consists of 32 computing threads, the 8 SPs need to use four 
cycles to execute one instruction of all 32 threads in a warp. 
 
 
 
 
 
 
 
 
 
 
 
C. IBM Cell Processor 
The architecture of IBM Cell processor[8] is illustrated in 
Figure 4. PPE (Power Processor Element) is the main control 
unit of Cell processor, and SPEs (Synergistic Processor 
Elements) are specifically designed for data-intensive and 
streaming processing computations. PPE is an in-order, 2-
way multi-threading superscalar processor, supports 64-bit 
Power ISA and VMX extension. SPU is a dual-issue 128-bit 
SIMD architecture with specialized ISA. All of these 
processor elements, memory controller, and I/O are attached 
onto EIB (Element Interconnect Bus). Accordingly, SPEs 
only can rely on DMA to move data from/to memory 
controller. Meanwhile, the memory controller is shared by all 
processor elements. So it limits the memory bounded 
applications and programming paradigm. Additionally, the 
Cell processor adopted in Sony PlayStation3 only enables six 
SPEs. 
 
 
 
 
 
 
 
 
 
 
 
 
D. SUN UltraSPARC T2 
In this study, it adopts Sun UltraSPARC T2 to be one of 
the reference machines for demonstrating OpenMP, as 
shown in Figure 7. This chip integrates 8 SPARC cores, 
which is designed by 2-way superscalar with 8 fine-grained 
SIMD Engine
 
Figure 2.  The organization of ATi Radeon HD5800 series. 
 
Figure 3.  The organization of nVidia GeForce GT200 series. 
 
Figure 4.  The organization of IBM Cell processor. 
475557
Memory for execution, and a Local Memory for sharing data 
within work-group. The Local Data Shares and Shared 
Memory in ATi and nVidias GPUs are designed to map this 
memory hierarchy. Finally, Global/Constant Memory is 
provided to store large data structures and frequently 
referenced constant data for all work-items. All of memory 
data transfers by work-items need to be copied explicitly 
through Host Memory, Global Memory, Local Memory, then 
be sent back. It means that programmers need to handle 
memory management explicitly. 
Accordingly, OpenCL requires programmer to manage 
memory movement and execution carefully. Write a program 
with good performance in OpenCL, programmer need to 
familiar with underlying architectures for adjusting 
appropriate parameters for execution. 
C. Example for migrating from OpenMP to OpenCL 
In this subsection, we adopt a simple example, “pi 
calculation”, to demonstrate the difference between OpenMP 
and OpenCL. The OpenMP version of pi calculation is 
shown in Figure 10. Since variable x is independent in each 
parallel section and not accessible to other threads, it needs 
to be declared as “private” by OpenMP directive. A parallel 
reduction is performed on variable sum, which needs to be 
protected by OpenMP when multi-threaded execution with 
“reduction” declaration in directive. OpenMP runtime will 
divide the loop into multiple threads with appropriate 
iterations when execution. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
In order to migrate this OpenMP program to OpenCL, 
there are two things need to be considered. First, the number 
of iterations in each work-item is specified by programmers, 
and need to be take care when writing. Second, the parallel 
reductions need to be hand-coded. During this migration, we 
will leave the parallel reduction operations for host processor 
to finish in order to simply the OpenCL kernel. The OpenCL 
kernel program is written in Figure 11. 
Each OpenCL kernel must begin with __kernel 
declaration and attributes of input data need to be indicated. 
Input argument partial_sum was used to return partial result 
of each work-item to host for parallel reduction. Another 
argument chunk specifies number of global work-items in 
execution. The floating point operations is changed from 
double precision to single precision since it is an optional 
feature in OpenCL 1.0 and not all of OpenCL runtime 
support this. In this kernel, each work-item is calculating a 
small portion of 1,000,000,000 iterations which specified by 
NUMSTEPS. The NUMSTEPS is calculated from number 
of total iterations and number of global work-items in 
execution. The global ID gid, retrieved by get_global_id call, 
will be used to calculate initial value in each work-item. 
When the result has been calculated, it will be stored into 
partial_sum with indexing by gid, and wait to copy back to 
host for parallel reduction. The OpenCL kernels usually save 
as a text file with .cl file extension. The kernel file can be 
easily migrated to different architecture with appropriate 
NDRange execution specified by OpenCL API of host 
without modifications. The host program is needed to setup 
the whole environment for OpenCL execution. This part is 
omitted due to the page limitation. 
 
 
 
 
 
 
 
 
 
 
 
Compare to OpenMP parallel programming, write an 
OpenCL program is relatively hard and complex than C or 
C++ with OpenMP directives since programmers need to 
configure parameters and manage execution themselves. 
However, the advantage of OpenCL is the portability of 
kernel program which utilizes computing capabilities of 
heterogeneous platforms. The performance difference of 
OpenCL and OpenMP will be discussed in next section. 
IV. EXPERIMENTAL RESULTS 
In this section, we adopt several benchmarks with 
different characteristics to observe performance differences 
on five platforms and two parallel programming paradigms. 
The experiment environments are listed in Table 2. 
TABLE II.  THE ARCHITECTURAL SUMMARY OF EXPERIMENTAL 
ENVIRONMENT. 
Catalyst 
10.2
Forceware
190.89N/AN/AN/ADriver
Visual 
Studio 2008
Visual 
Studio 2008GCC 4.1.2
GCC 4.1.1
Intel C 
Compiler 11.0
Sun C 5.9Compiler
StreamSDK
2.0.1
GPU 
Computing 
SDK 2.3A
Cell SDK 3.1.0 
+ OpenCL 
SDK 0.1.1
N/AN/AOpenCL runtime
Intel Core-i5 
750
Intel Core-i5 
750
Power 
Processor 
Element
Intel Xeon 
E5440 × 2
UltraSPARC
T2Host
USD$350
Windows7
(32-bit)
ATi Radeon 
HD5850
USD$400USD$400USD$5,000USD$30,000Price
Windows7
(32-bit)
Linux 2.6.21 
(64-bit)
Linux 2.6.9
(32-bit)SunOS 5.10OS
nVidia 
GeForce
GTX285
IBM Cell in 
PlayStation3
Intel Xeon 
E5440
SUN 
UltraSPARC
T2
 
A. Pi Calculation 
The first benchmark is a highly compute-intensive one 
obtained from section 3.3, the calculation of pi. In OpenCL 
test, different number of work-items will be used to observe 
 int num_steps = 1000000000; 
double step; 
 
int main(int argc, char * argv[]) 
{ 
  double x, pi, sum=0.0; 
  int i; 
  step = 1.0/(double)num_steps; 
   
#pragma omp parallel for private(x) reduction(+:sum)   
  for( i=0; i<num_steps; i++)   {  
    x = (i + 0.5)*step; 
    sum = sum + 4.0/(1.0 + x*x); 
  } 
  pi = sum*step; 
  printf("The value of PI is %15.12f\n",pi); 
  return 0; 
}  
Figure 10.  The OpenMP parallelized program of calculation of pi. 
 __kernel void pi_cal( __global float *partial_sum, const uint chunk) 
{                                                      
  uint gid = get_global_id(0); 
  uint i, NUMSTEPS = 1000000000/chuck; 
  float x, sum = 0.0f, tmp = gid*NUMSTEPS; 
  float gStep = 1.0f/1000000000; 
 
  for( i = 0 ; i < NUMSTEPS; i++)   { 
    x    = ( tmp + 0.5f ) * gStep; 
    sum += 4.0f / (1.0f + x*x );  
    tmp += 1.0f; 
  } 
  partial_sum[gid] = sum*gStep; 
}  
Figure 11.  The OpenCL parallelized kernel program of calculation of pi. 
477559
compilation and memory data movement between host and 
GPU. If comparing net execution time only, ATi GPU 
performs near six times faster than Xeon server and 35 times 
faster than UltraSPARC T2. The performance of 
UltraSPARC T2 is limited by its working frequency in such 
compute-intensive application. Also, PS3 has relatively low 
performance than GPUs since the number of compute units 
is much less than GPUs but it still outperforms UltraSPARC 
T2. 
B. Matrix Transpose and Matrix Multiplication 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
The next two benchmarks are matrix transpose and 
matrix multiplication with various data sizes which will 
achieve maximum capabilities of these platforms. It adopts 8 
threads and 16 threads for Xeon server and UltraSPARC T2 
respectively. In matrix transpose, the number of work-items 
is equal to size of matrix in OpenCL kernel. In matrix 
multiplication with tiling of size 4 by 4, such that the number 
of work-items equals (matrix width/4)×(matrix height/4). The 
number of work-items per work-group is set to 8x8 for both 
benchmarks. The results are shown in Figure 15 and 16. 
In these benchmarks, the performance of GPUs have 
significantly outperforms commodity multi-core processors 
even expensive servers. Since the execution time in GPU 
spent at least one third of time in compiling kernel and data 
transfer, if the data size becomes larger and operation 
become more complex, the performance of GPUs will be 
overwhelming compare to commodity CPUs. Today, GPUs 
have large amount of processing elements and comparative 
performance to supercomputers with the help of OpenCL. 
Although, this parallel programming paradigm is harder than 
OpenMP, the performance extracting from GPU is 
inexpensive and efficiency than traditional parallel 
programming paradigm with homogeneous platforms. Also, 
the OpenCL kernels are used across different architectures 
without any modifications which achieve portability on 
different computing devices. 
V. CONCLUSION 
The continuous growing of semiconductor technology 
makes the dramatic improvement of GPUs in low price. But 
it still not widely adopted in scientific computing due to its 
difficult programming model. Therefore, an open standard, 
OpenCL, is proposed to utilize heterogeneous CPUs and 
GPUs. In this study, it provides several benchmarks, with 
various computation characteristics, to demonstrate the 
capabilities of OpenCL with several platforms. An example 
to illustrate the migration of the given program, from 
OpenMP to OpenCL is also discussed in Section 3. 
According to our experiments, as long as the kernel code is 
properly developed, the computing capabilities can be 
released from these device and achieve high performance 
with little budget compare to use a supercomputer. Compare 
to conventional OpenMP parallelized programs that targeted 
on expensive servers, the OpenCL parallelized applications 
that targeted on relative cheep GPUs will obtains from 20 to 
100 times speedup. So it will be the first step of ubiquitous 
supercomputing if we adopt GPUs with cross-platform 
OpenCL paradigm. 
ACKNOWLEDGMENT 
This work is supported in part by the National Science 
Council of Republic of China, Taiwan under Grant NSC 98-
2221-E-033-042, Sun Microsystems Ltd, and Intel 
Corporation. The authors also thank the anonymous 
reviewers for their valuable comments. 
REFERENCES 
[1] I. Buck, T. Foley, D. Horn, J. Sugerman, K. Fatahalian, M. Houston, 
and P. Hanrahan, “Brook for GPUs: Stream computing on graphics 
hardware,” ACM Tran. on Graphics, vol. 23, no. 3, Aug., 2004. 
[2] Real World Technologies. NVIDIA's GT200: Inside a Parallel 
Processor. [Online]. Available: http://www.realworldtech.com/ 
[3] The OpenCL Specification 1.0 rev.48, Khronos OpenCL Working 
Group, Oct., 2009. [Online]. Available : http://www.khronos.org/ 
[4] AMD corp. AMD Graphics for Desktop PCs. [Online]. Available : 
http://www.amd.com/us/ 
[5] NVIDIA corp. NVIDIA GeForce Family. [Online]. Available: 
http://www.nvidia.com/object/geforce_family.html 
[6] Intel corp. Intel® microprocessor export compliance metrics. [Online]. 
Available:http://www.intel.com/support/processors/sb/CS017346.htm 
[7] OpenMP specification version 3.0, OpenMP Architecture Review 
Board, 2008. [Online]. Available : http://openmp.org/wp/ 
[8] IBM corp. Cell Broadband Engine resource center. [Online]. 
Available: http://www.ibm.com/developerworks/power/cell/ 
[9] ATi Stream Computing “OpenCLTM”, Programming Guide, ATi, 
March, 2010. 
[10] OpenCL Programming for the CUDA Architecture, nVidia, Aug., 
2009. 
Matrix Transpose Performance Comparison
0
1
2
3
4
5
6
7
8
9
10
256x256 512x512 1024x1024 2048x2048 4096x4096Size
se
c
Xeon E5440x2 with icc
T2 with suncc -fast
ATi Radeon HD5850
nVidia GeForce GTX285
PS3_Static_Compiling
 
Figure 15.  The execution time of matrix transposes with various sizes 
with OpenMP and OpenCL paradigms on various platforms. 
Matrix Multiplication Performance Comparison
0
10
20
30
40
50
60
70
80
90
100
256x256 512x512 1024x1024 2048x2048size
Xeon E5440x2 with icc
T2 with suncc -fast
ATi HD5850
nVidia GeForce GTX285
PS3_Staic_Compile
 
Figure 16.  The execution time of matrix multiplications with various 
sizes with OpenMP and OpenCL paradigms on various platforms. 
479561
98年度專題研究計畫研究成果彙整表 
計畫主持人：朱守禮 計畫編號：98-2221-E-033-042- 
計畫名稱：以電子系統層方法設計適合多媒體系統晶片之高效能記憶體控制器 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 2 2 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 1 100%  
博士生 4 4 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 1 100%  
博士生 4 4 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
