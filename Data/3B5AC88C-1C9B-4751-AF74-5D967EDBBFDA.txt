一、前言
近年來，隨著科技的發展與經濟的突飛猛進，多數的國家都面臨社會的治安
日趨惡化的問題，同時歹徒犯罪的手法也愈來愈多元化，如何在事前有效防止犯
罪發生或在犯罪的同時立即給予適當的處理，是目前各界必須深入研究的重要課
題。美國 911 事件發生至今已經滿五年了，當時引起各界極大的震撼並且積極投
入相當龐大的人力與資源，研究如何防止恐怖事件再次發生，例如美國在機場海
關入關時的指紋建檔與人臉影像辨識技術，並配合大量的監視攝影機進行環境的
監控，都是在防止恐怖份子或歹徒的滲透與破壞。雖然如此，但是每一年在全球
各地都還是會有許多類似的恐怖事件不斷地發生，事實證明未來在安全監控這一
方面的需求量與重要性將會與日俱增，積極投入相關的研究能量絕對有其必要。
另一方面，許多個人自身安全和財產保障的問題也隨著這股潮流逐漸受到重視，
因此許多公司與家庭或者公共區域都開始裝設監視錄影設備以保護生命財產的
安全。
隨著數位影像設備技術的進步，以及製造成本日益低廉，影像式安全監控系
統近年來已被廣泛地應用在許多不同的場合，例如：公司、住家、商店、百貨賣
場、停車場和馬路交通，甚至於軍事國防等用途上。在安全監控的作法上，現今
設備與技術多半還是要仰賴現場或事後的人工操作與監控，因此會有容易發生疏
失，並且無法針對某特定目標、地點、事件或時間做出自發性的反應，例如異常
事件偵測 (event detection) 、事件分析 (event analysis) 和資訊融合
(information integration)等，甚至於常常是問題發生後才開始調閱錄影帶來
找尋事證，不但為時已晚，更有可能沒拍到重要的畫面。隨著電腦運算速度與監
視器材的快速發展，因此近年來有大量的研究人員投入在運用電腦視覺技術來解
決這類問題上，其中包含發展物件偵測(object detection)、物件追蹤(object
tracking)、物件辨識(object recognition)和行為分析(behavior analysis)
等技術。這些研究部分已經成功地應用在虛擬實境介面、視訊多媒體遊戲、自動
導航系統、交通流量管制系統和安全監控系統等等。但是目前的安全監控系統還
停留在獨立運作的階段，如何藉由強大的網路功能進行整合，也是非常重要的工
作，這也是本計畫想要進行的工作。
二、研究目的
在本計畫中的取像設備會隨著安全監控平台移動，以加強安全監控功能，但
此時影像將面臨隨時更新的複雜背景，以及相機本身會移動時受到監控的物件會
存在有兩種運動因素，現存大多數的物件偵測、物件追蹤、物件辨識和行為分析
等技術並不能完全適用，因此本計畫預計在移動式安全監控平台上，開發以電腦
視覺為基礎的安全監控技術，該安全監控平台將以無線網路的方式和運算功能強
大的伺服器進行資料傳輸，未來安全防護的功能將不會僅受限於單一系統而已，
透過無線網路與無線感測網路結合本計畫擬開發的移動式安全監控平台，並且利
用主從式架構，將大量的計算有效地分攤在移動式平台與伺服器上，達到即時安
全監控的目的。本計畫的開發重點在於設計可承受各種不同光線變化下的演算
法，在真實環境下光線的變化將造成影像資料的大幅改變，如何有效地克服這個
問題將有助於達成移動式安全監控平台的技術；另一方面必須可以在複雜背景下
運作，此時很難擁有一個固定的背景模型，許多利用背景相減方法的技術將無法

後再對其做巴納德檢測。而彩色影像必需先分成 R、G、B三張灰階影像，分別對
灰階的影像做處理，最後才用巴納德檢測去選出特徵點。
巴納德偵測器本身就是計算像素點的臨近點強度的運算，如果照著一般的
處理方式，很容易就讓特徵點仍然太多，並且會集中在一些較強烈特徵的區域，
讓不明顯的資訊無法表達，忽略整張完整描述，反而讓我們取得的資訊過多而
雜，無法達到描述影像的目的。
我們著重於影像檢索的特徵擷取，而且想利用特徵點完整表示影像內容的
物件，且考量到資料庫比對的迅速與確實，要以最佳的資訊量來描述圖像，方便
做比對，我們就先前的方法歸納以下幾點，通常特徵點的數量必須加以限制，以
符合影像檢索的效能，如果影像內容範圍太大，部分物件的資訊會喪失，如果範
圍太小，產生的特徵點過多，好處是特徵點會集中在邊緣處系數較高的部分，壞
處是在點數受限制時無法分散，還是會集中在值高的部分。
偵測出來的特徵點都有一個特徵值。由實驗得知，這些點有強弱之分。這
些強弱決定一般處理的方式，不外乎依照特徵值的強弱來定立門檻值，或用來淘
汰臨近的點，但在較少特徵點的限制下，卻不能完整的描述一張圖像。在本文實
驗人像的部分，特徵點集中在頭髮的部份，因為這個部份顏色明暗較強烈，強度
也較大，如果選點機制是照著強弱來分的話，分配的極為不均，且集中在一些相
鄰的地方，這樣就不能有效的利用偵測出來的特徵點描述整張影像內容的意義。
偵測出來特徵點的強度可以用一曲線圖來看出變化，為這些特徵點分布整張影像
各個區域上。我們在選取特徵點的時候，除了考慮特徵點強度表達的訊息外，更
要考慮一張圖像整體的描述，故要考量區域分布的各個特徵點，在此我們用區域
劃分的方式，來達到我們分散選點的目地。
我們統一處理灰階的圖像，首先用巴納德偵測去偵測整張圖像所有的顯著
的點，考慮到相鄰過於密集的點的確不能表示圖像，故先照 Song 等學者所提出
的方法去對特徵點做 8方向的篩選，即每個特徵點去做 8個方向的偵測，如果同
時也有特徵點的存在，則留下此 8個方向中的最大值，如此消除臨近連續的特徵
點後，接下來即是我們的方法，四分統計量權重分配法。我們將特徵點取得的數
量加以限制，以固定數量來選取所要的特徵點，這樣在比對資料時，等於強制將
圖像的圖像正規化，達到比對的迅速與確實，一般的選點方法就不適用。在限制
固定數量特徵點的條件下，反而會強調影像某一變化強烈的區域。
我們的選點方法較可對應出原始影像的形狀，較不會如圖對影像內容資訊
過於複雜造成失焦，一般方法的特徵點趨向集中在特徵較明顯的地方，在比對的
時候即增加許多距離過大的量，我們提出的方法卻較不會受此影響，即使複雜也
能準確的分辨出強弱的地方，其他衣服及上面的花紋，也不失焦的去描述它。直
方圖能夠看出我們的方法線較一般方法的差異性。我們的方法在一些圖片上的結
果可以較明顯，表達相像或是不像，這可能是我們的方法較能不受影像內容變化
巨烈的地方影響的效果。
Robust Face Detection” IEEE Transactions on pattern analysis and machine intelligence ,
VOL.26, NO.11 , 2004.
[13] H. A. Rowley , S. Baluja , and T. Kanade, “ Rotational Invariant Neural Network –Based
Face Detection,” in Proceedings, IEEE Conference on Computer Vision and Patern 
Recognition, pp. 38-44, 1998.
[14] A. Yulie , P. Halinan, and D. Cohen,” Feature Exaction from Faces using deformable
templates,” International Journal of Computing Vision, VOL.8, NO.2, pp.99-111, 1992.
[15] Alessandro Colombo, Claudio Cusano, Raimondo Schettini, “3D face detection using
curvature analysis,” Pattern Recognition, VOL.39, pp. 444–455, 2006.
[16] Y. Li, S. Gong, Jamie Sherah, Heather Liddel, “Support vector machine based multi-view
face detection and recognition,” Image and Vision Computing, VOL.22, pp. 413–427, 2004.
[17] B. Wu, H. Ai, C. Huang and S. Lao, “Fast Rotation Invariant Multi-View Face Detection
Based on Real Adaboost,”Proceedings of the Sixth IEEE International Conference on
Automatic Face and Gesture Recognition (FGR’04).
[18] C. Huang, H. Ai, Y. Li and S. Lao, “Learning Sparse Features in Granular Space for
Multi-View Face Detection,” Proceedings of the 7th International Conference on Automatic 
Face and Gesture Recognition (FGR’06)
UAE, Jan, 2009.
[8] Y. H. Tsai,”A New Window Selection for local Image Thresholding under Uneven
illuminations,”International Journal of Innovative Computing, Information and
Control, vol. 6, no. 3, March 2010 (SCI).
detector and the wavelet saliency value. In Section 3, we describe
the proposed the reduction technique for salient points.
Experimental results are presented in Section 4. Section 5
concludes this paper.
II. SALIENT POINT DETECTION
First, we introduce the Barnard detector [15] for extracting
the salient points from the original image. For a given image f(m,n)
and the non-overlapped processing window W, the operator t(m,n)
is defined to measure the strength of the center point from the
difference in horizontal, vertical and diagonal directions. For
each pixel in the processing window, we calculate the operator
t(m,n) by the following formula.
 ( , ) min , , ,t m n H V L R ,
where
       2 2, 1, , 1,H f m n f m n f m n f m n           
       2 2, , 1 , , 1V f m n f m n f m n f m n           
       2 2, 1, 1 , 1, 1L f m n f m n f m n f m n             
       2 2, 1, 1 , 1, 1R f m n f m n f m n f m n             .
Then, find a pixel f(m', n') to make t(m',n') maximum in window W
by the following equation.
)},({max)','(
,
nmtnmt
Wnm 
 .
Note that the differences in gray value of f(m',n') and its neighbors
in window W are large.
Secondly, two-dimensional wavelet transform need three
different wavelet functions, which reflect three spatial
orientations (horizontal, vertical and diagonal). The wavelet
representation of an image of size MN is the set of wavelet
coefficients. The wavelet coefficient can be expressed as the form,
),(
2
nmfW j at scale 2j, where jZ and j-1. The children set of
wavelet coefficient is as follows:
)},({)),(( 122 lkfWnmfWC jj  ,
where 2mk2m+2p-1, 2nl2n+2p-1, p is the wavelet regularity.
Loupias et al. [7] select salient points with the highest gradient
from 2p points. The saliency value is as the sum of the absolute
value of the wavelet coefficients.




j
k
k nmfWCS j
1
2
)( )),((
For a point in an image, its corresponding saliency value is
computed for every wavelet coefficient. We then have to
threshold the saliency value, in relation to the desired number of
salient points.
Fig. 1. (a) the original image (b) the points obtained from Barnard
detector
Fig. 1 (b) shows the result of Barnard detector from the input
image in Fig. 1(a). There are total 2669 potential salient points in
Fig. 1 (b). It is obvious that this result contains many continuous
point sets. There are still some problems about the point
extraction above. The number of points is usually too large and
there exists some continuous point sets. Since it is believed that
not all of them are needed, we try to reduce the set of points to
extract the local most salient ones.
III. SALIENT POINTS REDUCTION
In Song’s algorithm [14], there are two main approaches to
improve the previous algorithm [7]. First, they choose a threshold
varies according to the content of the input image. Calculate the
sum of the saliency value of all the potential salient points, and
the threshold is set to a given percentage threshold p of the sum.
Extract the most salient points continuously until the sum of their
saliency values reaches the threshold. Second, for a chosen point
f(m,n), check the eight neighbors of it, if there are still other
salient points, decide whether the saliency value of f(m,n) is the
maximum, if not, get rid of it. After applying this method to all the
salient points, there are no two salient points join together.
In the proposed algorithm, we subdivide the original image
into four equal sized ones recursively and select salient points
from the above generated set to describe different parts of the
image. Suppose the total sum of the saliency value is S, then Si,
where i=1,2,3,4, represents the corresponding saliency value for
each subimage. The recursive relation among saliency values is





 

4
1i
iSS
, 




 

4,3,2,1,
4
1
jSS
i
jij
,… .
For the efficiency of CBIR, the number of salient points is
fixed, since it takes a long time to perform the point matching.
Let N be the total number of salient points that we specify for
further image retrieval. The proposed algorithm controls the
(a) (b)
Fig. 4. Results of the CBIR using salient points generated by the
proposed algorithm.
. It is obviously that the results of the proposed algorithm are
very similar to the query image. On the other hand, we can see
that the green curve is almost under the blue one, i.e., the selected
top 100 images is more similar to the query one. From the
experimental results, the proposed method has better
performance than Song’s method.
V. CONCLUSION
Salient points are frequently used to represent local properties
of the image in content-based image retrieval. In this paper, we
present a reduction algorithm that extracts the local most salient
points such that they can give a satisfying representation of an
image for applications in computer vision. The resulting salient
points are evaluated by image retrieval with a retrieval system
using the measure of Hausdoff distance. In this experiment, our
method provides better retrieval performance comparing with
other point detectors.
REFERENCES
[1] M. Fiala, “Using normalized interest point trajectories over scale for image
search,”Proceedings of the 3rd Canadian Conference on Computer and
Robot Vision, CRV 2006. pp. 58–58.
[2] Z.H. Zhang Y. Quan, W.H. Li, W. Guo, “A New Content-Based Image
Retrieval”, Proceedings of the Fifth International Conference on
Machine Learning and Cybernetics, Dalian, 13–16 August 2006. pp.
385–388.
[3] R. Datta, D. Joshi, J. Li, and J. Z. Wang,“Image retrieval: ideas, influences,
and trends of the new age,”ACM Computing Surveys, vol. 40, no. 2,
Article 5, 2008.
[4] H. J. Lin, Y. T. Kao, S. H. Yen, andC. J. Wang, “A study of shape-based
image retrieval,”Proceedings of the 24th International Conference on
Distributed Computing Systems Workshops, ICDCSW 2004. pp. 118–
123.
[5] L. Birgale, M. Kokare, and D. Doye, “Colour and texture features for
content based image retrieval,” Proceedings of the International
Conference on Computer Graphics, Imaging and Visualisation, CGIV
2006. pp. 146–149.
[6] J. W. Han and L. Guo, "New image retrieval approach based on interest
points", Proceedings of SPIE, vol. 4862, 2002, pp. 197–197.
[7] E. Loupias, N. Sebe, S. Bres, and J.-M. Jolion, “Wavelet-based salient
points for image retrieval,”Proceedings of International Conference on
Image Processing, vol. 2, 2000, pp. 518–521.
[8] H. Ling, and D. W. Jacobs, “Deformation invariant image matching,”
Proceedings of the Tenth IEEE International Conference on Computer
Vision, ICCV 2005, vol. 2, pp. 1466–1473.
[9] J. Wang, H. Zha, and R. Cipolla,“Combining interest points and edges for
content-based image retrieval,”Processings of the ICIP in IEEE
International Conference, 2005. pp. III - 1256-9.
[10] Md. M. Rahman, B. C. Desai, and P. Bhatacharya, “Visual 
keyword-based image retrieval using latent semantic indexing,
correlation-enhanced similarity matching and query expansion in inverted
index,”10th International Database Engineering and Applications
Symposium, IDEAS 2006, pp. 201–208.
[11] G. Ding, Q. Dai, W. Xu, and F. Yang, “Afine-invariant image retrieval
based on Wavelet interest points,”Processings of the IEEE Multimedia
Signal, 2005, pp. 1–4.
[12] C. Schmid and R. Mohr, “Local gray value invariants for image
retrieval,”IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 19, no. 5, pp. 530–535, 1997.
[13] P. B. Albee and G. C. Stockman, “Interest points from the radial mass
transform,”IEEE International Conference on Acoustics, Speech, and
Signal Processing, vol. 2, pp. 261–264, 2005.
[14] H. Song, B. Li and L. Zhang, “Color salient points detection using
wavelet,”Proceedings of the 6th World Congress on Intelligent Control
and Automation, Dalian, China, June 21–23, 2006, pp. 10298–10301.
[15] S. T. Barnard, and W. B. Thompson,“Disparity analysis of images,”IEEE
Transactions on Pattern Analysis and Machine Intelligence, pp. 333–340,
1980.
[16] D. P. Huttenlocher, G. A. Klanderman, and W. J. Rucklidge,“Comparing
images using the Hausdorff distance,”IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 15, no. 9, pp. 850–863, 1993.
[17] B. Takacs, “Comparing face images using the modified hausdorf 
distance,”Pattern Recognition, vol. 31, no. 12, pp. 1873–1881, 1998.
[18] Y. Wang, and C. Chua,“Robust face recognition from 2D and 3D images 
using structural Hausdorf distance”, Proceedings of the ICARCV in
International Conference, 2006. pp. 502–507.
[19] H. Tan, Y. Zhang, “A novel weighted Hausdorf distance for face 
localization,”Image and Vision Computing, vol. 24, no. 7, 2006. pp.
656-662.
出席國際學術會議心得報告
計畫編號 NSC 96-2221-E-364 -001 -MY2
計畫名稱 以電腦視覺為基礎之移動式安全監控技術
出國人員姓名
服務機關及職
稱
蔡耀弘，玄奘大學資訊科學學系，助理教授
會議時間地點 2009/01/28~30，Dubai, United Arab Emirates
會議名稱 ICCSE 2009 : "International Conference on Computer Scienceand Engineering"
發表論文題目 Salient Points Reduction for Content-Based Image Retrieval
一、參加會議經過
1. 於一月二十七日搭乘國泰航空班機由中正機場出發飛往 Dubai, United
Arab Emirates。因為國內直飛的航空公司在價格上比國泰航空高出許
多，所以只好選擇國泰航空，但中間必須經過香港轉機與等待，比較不
方便。
2. Dubai 當地時間一月二十八日上午 5:05 到達 Dubai 國際機場，出關手續
還算順利，接著搭乘飯店接駁巴士前往會議舉辦旅館，杜拜馬可波羅飯
店。
3. 一月二十八日當天因時間尚早，故先行進行市區動線與周邊交通的了解
工作，隨後探訪開會地點並完成報到手續。
4. 一月二十八日一大早參加 ICCSE 2009 的開幕式由 進行開場。隨後進
行專題演講。
