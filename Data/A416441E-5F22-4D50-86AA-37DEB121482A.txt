  2
contrast to the “black box” scenario, CBR being an inductive machine learning technique could substitute for ANN. 
Literature has described that CBR became widely used in diagnosis domain and enhanced some of the deficiencies in 
statistical models and ANN (Weiss & Kapouleaas, 1989; Barletta, 1991; Park & Han, 2000).  
Considering the attributes extraction, too many attributes might result in lower accuracy and effectiveness, 
whereas key attributes enable CBR to retrieve most similar cases correctly. In data mining techniques, Rough Set 
Theory (RST) has successfully applied to classification problems. RST can reduce unnecessary variables (attributes) 
and identify subsets of potentially important explanatory variables in a multiattribute information system. Some 
researchers (Slowinski & Zopounidis, 1995; McKee, 1998, 2000, 2003; McKee & Lensberg, 2001; Jelonek et al., 
1995; Hashemil et al., 1998; Ahn et al., 2000) used RST to construct a prediction model or combine with other 
methods as a data preprocessor in order to identify key attributes, initiating the sub-process of investigating the 
important attributes crucial for identifying analogous cases and predicting the value of the target variables.  
The approach of attribute weighting is also being concerned in CBR. The attributes with weights of zero are 
effectively ignored during similarity computation, otherwise the attributes whose weights are high having the most 
impact on determining similarity.  There are many weighting approaches such as experts’ experience, Analytic 
Hierarchy Process (AHP) and Grey Relational Analysis (GRA). For example, Park and Han (2002) Analytic 
Hierarchy Process (AHP), and Yip (2003) employed statistical evaluation for automatically assigning attribute 
weights.  
To strengthen the effectiveness of CBR predicting capability, a proposed hybrid model called Hybrid Failure 
Prediction model (HFP) is classified into three phases. First, applying RST to extract key attributes, and then using 
GRA to obtain the key attribute weights while data samples are insufficient. Finally, the retrieved key attributes and 
weights are fed into CBR to construct a new business failure prediction model. 
 
三、 文獻探討 
3.1 Rough Set Theory (RST) 
RST, a powerful mathematical tool introduced by Pawlak (1982), is a formal framework for discovering facts 
from imperfect data (Walczak & Massart, 1999). RST has been successfully applied to real-world classification 
problems in a variety of areas but only recently to business failure prediction (Slowinski, 1995; Dimitras et al., 1999; 
McKee, 1998; 2000;2003; McKee & Lensberg, 2001; Ahn et al., 2000). The following section will present some 
basic concepts of RST. 
3.1.1 Information system (IS) 
Consider an information system (IS) can be seen as a system ),( AUIS = , where U is a finite set of objects 
and A is a finite set of attributes. Each attribute Aa∈  (attribute a belonging to the considered set of attributes A) 
defines an information function, aa VUf →: , where aV  is the set of values of a, called the domain of attribute a. 
For every set of attributes AB∈ , an indiscernibility relation )(BInd  is defined in the following way: two objects, 
ix  and jx  , are indiscernible by the set B in A, if b( ix ) = b( jx ) for every Bb⊂ . The equivalence class of 
)(BInd  is called elementary set in B because it represents the smallest discernible groups of objects. For any 
element ix  of U, the equivalence class of ix  in relation )(BInd is represented as [ ] )( Bindix . 
3.1.2 Reduction of attributes 
Assume the set of attributes AR ⊆ depends on the set of attributes AB ⊆  in IS (denotation BÆR) 
iff )(    )( RIndBInd ⊆ . The minimal subset ABR ⊆⊆  like FBμ = FRμ is called F-reduct of B, which means no 
ambiguity in the understanding of F and denoted by )(FREDB . In addition, the IS might have more than one 
F-reduct. Intersection of all F-reducts is called F-core of B, i.e. )(FCOREB = )(FREDB∩ . The core is a 
collection of most significant attributes in the system. Note that the core can be empty as well. 
3.1.3 Decision tables  
Consider an IS can be seen as a decision table. Assuming that DCA ∪=  and ∅≠∩DC , C is a set of 
condition attributes, and D is a set of decision attributes. Decision table ( )DCUS ∪= ,  is deterministic if DC→ ; 
otherwise, it is non-deterministic. The deterministic decision table uniquely describes the decisions to be made when 
some conditions are satisfied. In the case of a non-deterministic table, decisions are not determined by the conditions. 
Instead, a subset of decisions is defined which could be taken under circumstances determined by conditions. From 
the decision table, a set of decision rules can be derived. Let )(CIndU be a family of all C-elementary sets called 
condition classes, denoted by ),...,2,1( kiX i = , where k is the number of )(CIndU . Let )(DIndU  be the 
family of all D-elementary sets called decision classes, denoted by ),...,2,1( njX j = , where n is the number of 
)(DIndU . )()( jDiC XDesXDes ⇒ which is called the (C,D)-decision rule. The rules are logical statements 
  4
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
=
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
+−
+
+
1
1
4
1
3
1
2
)1()1(
)1()1(
2
)1()1(
2
)1(
1
)1(
1
)1(
1
)1(
1
)1(
1
)1(
1
)()(
)3()3(
)2()2(
)(5.0)1(5.0
)3(5.0)2(5.0
)2(5.0)1(5.0
a
b
a
b
a
b
a
b
nxnx
xx
xx
nxnx
xx
xx
N
NN
N
N
MK
MKM
K
K
M
 
Set m
j b
a
b ˆ
1
=  , where Nm ,...,4,3,2= . It becomes as follows: 
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
=
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
+−
+
+
N
NN
N
N
b
b
b
b
nxnx
xx
xx
nxnx
xx
xx
ˆ
ˆ
ˆ
ˆ
)()(
)3()3(
)2()2(
)(5.0)1(5.0
)3(5.0)2(5.0
)2(5.0)1(5.0
4
3
2
)1()1(
)1()1(
2
)1()1(
2
)1(
1
)1(
1
)1(
1
)1(
1
)1(
1
)1(
1
MK
MKM
K
K
M
   
Step 5: Apply the equation, Bˆ = ( XYYY TT 1)− to solve the values of mbˆ  where 
X =
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
+−
+
+
)(5.0)1(5.0
)3(5.0)2(5.0
)2(5.0)1(5.0
)1(
1
)1(
1
)1(
1
)1(
1
)1(
1
)1(
1
nxnx
xx
xx
M
, Y =
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
)()(
)3()3(
)2()2(
)1()1(
)1()1(
2
)1()1(
2
nxnx
xx
xx
NN
N
N
K
MKM
K
K
, 
and Bˆ =
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
Nb
b
b
b
ˆ
ˆ
ˆ
ˆ
4
3
2
M
 
Application of the proceeding five steps leads to the value of “grey relational ordinal” ( mbˆ ) which means 
)()1(1 kX to )(
)1( kX j . mbˆ , however, is only a sequence of ranking that does not make any sense original, we could 
normalize it to acquire the GRA-weights by following equation: 
Nj
bbb
bw
N
i
i ,...,3,2,
...
^^
3
^
2
^
=
+++
=  
3.3 Case-Based Reasoning (CBR)   
CBR, developed by Schank and Abelson in the 1970s, uses “similar” cases stored in a case base to build a 
solution to a new case based on the adaptation of solutions to past similar cases (Kolodner, 1991). Financial 
applications of CBR typically focus on the classification and forecasting, and cases matching. Complete reviews of 
CBR (López de Mántaras & Plaza, 1997; Watson & Marir, 1994) provided comprehensive details and application for 
further works. In general, the use of CBR leads to the retrieval time increasing linearly with the number of cases. 
Therefore, CBR approach is more effective when the case base is relatively small. 
The establishment of CBR involves the following steps (Aamodt & Plaza, 1994): (a) retrieving relevant cases from 
the case memory (this requires indexing the cases by appropriate features), (b) electing a set of best cases, (c) 
deriving a solution, (d) evaluating the solution (in order to make sure that poor solutions are not repeated), and (e) 
storing the newly solved case in the case memory. It retrieves relevant cases from the database and adapts a similar 
solution to fit the new problem by computing the values of “similarity” which is divided into the qualitative and the 
quantification according to the attributes type. The qualitative similarity and the quantitative computation are defined 
as  
  6
Table1 Set of all attributes 
Attributes       Description 
A1 Return on assets (%) 
A2 Return on equity (%) 
A3 Net income (%)-except disposed 
A4 Gross margin (%) 
A5 Net income (%) 
A6 Current ratio (%) 
A7 Acid test ratio (%) 
A8 Liabilities ratio (%) 
A9 TCRI credit ranking 
A10 Cash flow operation to current liabilities (%) 
A11 Total equity growth ratio (%) 
A12 Return on total assets growth ratio (%) 
A13 Days- accounts receivable turnover 
A14 Inventory turnover (%) 
A15 Earning per share 
A16 Added value per person 
A17 Manager-director 
A18 Director and supervisor shareholding 
A19 Inflation rate (%) 
A20 Business cycle 
A21 Rediscount Rate (%) 
 
3.2 RST attribute reduction 
U is all of the healthy and ill-management firms and C is the set of financial and non-financial attributes 
representative in prior research, D is the status of U which means that would be the most attribute to examinee the 
business whether it will be fail or not, and A is the total number of attributes which selected in the study. RST 
reduces attributes, which implies that some redundant attributes will be eliminated without any information loss, 
to induct the minimal variables subsets named “reduct.” It is possible that there will be quite a few reducts after 
applying the RST software, ROSETTA, and we could not decide the critical attributes in all of the reducts based 
on a theoretical model or on individual variable strength. Hence, this study applied a “manually adjoined” 
stepwise approach which steps are as follows: 
Step1. Extract and remove the most frequently appeared attributes from all of the reducts. 
Step2. Apply ROSSETA again to obtain the new sets of reducts. 
Step3. Repeat Step1 until the remaining attributes would no longer reduce the numbers of the reducts.  
Since the remaining attributes cannot determine the decision attribute—business status, the set of removed 
attributes is saved while the other attributes are disregarded. 
3.3  GRA attribute weighting 
  In this study, the )(0 kX  is the objective sequence of the status of ill-management and )(kX i is the reference 
sequence of other attributes. This part applies GRA to derive mbˆ , the grey ordinal, which have been mentioned in the 
steps 1-5 of section 2.2. Bˆ can be normalized as the GRA-weights if the attributes with weights of zero, they can be 
effectively ignored during similarity computation. Whereas, the attributes have the most impact on determining 
similarity if they with high weights.  
3.4  CBR failure diagnosis 
CBR produce the solution for diagnosing firms. After assigning the key attributes form RST and with 
GRA-weights into CBR, we apply the CBR-works 4.0 to compute the attribute similarities. CBR will search for the 
best match of the target case in the case base and adapt retrieved cases to classify the business status whether it 
becomes failed or not. The resulting model with the additional validation cases is evaluated as well in next section. 
 
五、資料分析與討論 
  The data sampling followed Beaver’s (1966) first proposed “pairwised sampling method”. The sample data sets 
consist of the equal number of healthy and failed companies: 31 healthy companies and 31 ill-managed companies. 
Each data set is split into two subsets: a training set of 90% the data and a validation set of 10% the data. The training 
  8
 
Table 4 Key attributes with GRA-weights  
Attributes Description Weight 
A6 Current ratio 0.032 
A7 Acid ratio 0.048 
A8 Liability ratio 0.333 
A9 TCRI credit ranking 0.529 
A16 Added value per person 0.005 
A18 Director and supervisor shareholding 0.053 
total  1.00 
 
     
  
Fig.4 HFP model prediction 
 
Table 5 Diagnosis results of HFP  
Original data Testing data 
 0 (healthy firms) 1 (ill-management firms) 
9 1 0 (healthy firms) 
1 (ill-management firms) 4 16 
Remark: The average accuracy is 83.3%. 
 
HFP which combined RST, CBR, with GRA is applied to diagnose the business status. RST has reduced the 
initial 21 attributes and then the key attributes weights are developed by GRA in Excel which described in Table 4. 
The first two important attributes are TCRI crediting ranking and Liability ratio. TCRI credit ranking is an integrated 
index while viewing the business finance. The second is Liability which is the ratio of total liability to the total assets 
mainly measures the capital structure in the company. Others, though, are not significant but still remaining for they 
seemed to be relative more important than those attributes which RST removed. In the experiment, HFP model only 
misclassified 5 cases in the testing data and the predictive accuracy is about 83.3%. In addition, there is only one 
case in type 1 error in HFP shown in Table 5 and it is superior accuracy to CBR and RST-CBR.   
Comparing to prior prediction model, Table 6 presents comparison of the prior studies and the current study. As 
shown in the Table 6, the reason the study has improved more on prior research are included the following: (a) the 
attributes retrieved and assigned different weights in mathematical methods (RST with GRA) are more objective 
than those in experts’ subjective experience or frequent application; (b) the research data acquired from Taiwan are 
not consistent in those studies; and (c) the non-financial variables, such as intelligent capital and accounting 
information, are combined in the study which conforms to the current tendency of predicting business crisis. 
 
Table 6 Comparison of business failure prediction models 
Methods Authors  Sample size Variable used Accuracy (%) 
RST Slowinski & Zopounidis(1995) 39 4 N/A 
RST Dimitras et al. (1999) 80 4 60.53 
RST McKee (2000)  100 2 76.3 
RST-GP McKee & Lensberg (2001) 291 4 80 
HFP Proposed by Lin and Huang 62  6 83.3 
Remark: Roughly compared on different data 
 
 
  10
Research, 167-179. 
13. Deng, J. (1982). Control problems of grey systems. System and Control Letters, 5, 288–294. 
14. Dimitras, A. I., Zankis, S. H.,& Zopounidis, C. (1996). “A survey of business failures with an emphasis on 
prediction methods and industrial applications.” European Journal of Operational Research, 90,487-513. 
15. Dimitras, A., Slowinski, R., Susmaga, R., & Zopounidis, C. (1999). Business failure prediction using rough 
sets. European Journal of Operational Research, 114(2), 263–280.  
16. Hashemi, R., Le Blanc, L. A., Rucks, C. T., & Rajaratnam, A. (1998). A hybrid intelligent system for 
predicting bank holding structures. European Journal of Operational Research, 109(2), 390–402. 
17. Jelonek, J., Krawiec, K., & Slowinski, R. (1995). Rough set reduction of attributes and their domains for neural 
networks. Computational Intelligence, 11(2), 339–347. 
18. Kolodner, J. L. (1991). Improving human decision making through case-based  decision aiding. AI 
Magazine, 12(2), 52–68. 
19. Kumar, P. R., & Ravi, V. (2006). Bankruptcy prediction in banks and firms via statistical and intelligent 
techniques – A review. European Journal of Operational Research, 180(1), 1-28. 
20. Lai, H. H., Lin, Y. C., & Yeh, C.H. (2005). Form design of product image using grey relational analysis and 
neural network models. Computer & Operation Research, 32, 2689-2711.  
21. Lee, C.Y. (1999). Predicting corporate bankruptcy and financial distress by gray forecasting theory and 
artificial neural networks. Unpublished doctoral dissertation, I-Shou University, Kaohsiung. [Text in Chinese] 
22. McKee, T.E. (1998). A mathematically derived rough set model for bankruptcy prediction. In Brown, C.E. 
(Ed.), Collected Papers of the Seventh Annual Research Workshop on Artificial Intelligence and Emerging 
Technologies in Accounting, Auditing and Tax, Artificial Intelligence/Emerging Technologies Section of the 
American Accounting Association. 
23. McKee, T. E. (2000). Developing a bankruptcy prediction model via rough set theory. International Journal of 
Intelligent Systems in Accounting, Finance and  Management, 9, 159-173. 
24. McKee, T. E., & Lensberg, T. (2001). Genetic programming and rough sets: A hybridapproach to bankruptcy 
classification. Journal of Operational Research, 138,436–451. 
25. McKee, T.E., (2003). Rough sets bankruptcy prediction models versus auditor signaling rates. Journal of 
Forecasting, 22, 569-586. 
26. Meyer, P.A., & Pifer, H.W. (1970). Prediction of bank failures. Journal of Finance, 25, 853-868. 
27. Odom, M., & Sharda, R. (1990). A neural network model for bankruptcy prediction. Proceedings of the IEEE 
International Conference on Neural Network, 2, 163–168. 
28. Ohlson, J.A. (1980). Financial ratios and the probabilistic prediction of bankruptcy. Journal of Accounting 
Research, 18(1)109-131. 
29. Park, C. S., & Han, I. (2002). A case-based reasoning with the feature weights derived by analytic hierarchy 
process for bankruptcy prediction. Expert Systems with Applications, 23(3), 255-264. 
30. Pawlak, Z (1982). Rough sets. International journal of information and computer sciences, 11, 341-356.  
31. Salchenberger, L. M., Cinar, E. M., & Lash, N. A. (1992). Neural networks: a new tool for predicting thrift 
failures. Decision Sciences, 23, 899–916. 
32. Shih, B. J., & Tang, L. L. (2001). Predict the financial crisis by using grey relation analysis, neural network, 
and case-based reasoning. Web Journal of Chinese Management Review, 4(2), 25-37. [Text in Chinese] 
33. Simpson, W.G., & Gleason, A.E. (1999). Board structure, ownership, and financial distress in banking firms. 
International Review of Economics and Finance, 8, 281–292. 
34. Slowinski, R. (1995). Rough set approach to decision analysis. AI Expert, March: 19-25. 
35. Slowinski, R., & Zopounidis, C. (1995). Application of the rough set approach to evaluation of bankruptcy risk. 
International Journal of Intelligent Systems in Accounting, Finance, and Management, 4(1), 27–41.  
36. Walczak, B., & Massart, D. L. (1999). Rough sets heory. Chemometrics and Intelligent Laboratory Systems, 47, 
1–16. 
37. Watson, I., & Marir, F. (1994). Case-based reasoning: A review. The Knowledge Engineering Review 9(4), 
327–354. 
38. Weiss, S.M., & Kapouleas, I. (1989). An empirical comparison of pattern recognition, neural nets, and machine 
learning classification methods. In proceedings of the 11th IJCAI. 
39. Yip, A. (2003). A hybrid case-based reasoning approach to business failure prediction. Amsterdam, Netherlands: 
IOS Press. 
40. Council for economic planning and development. Retrieved from http://www.cepd.gov.tw/index.jsp on 
November 24, 2006 
41. Taiwan stock exchange corporation. Retrieved from http://www.tse.com.tw/ch/index.php on October 18, 2006 
 
八、 致謝 
This research is sponsored by National Science Foundation in NSC-96-2221-E-027-014. 
分組研討 465 篇論文於 53 個 Sections 進行發表。研討主題包括：Accounting；Business 
Administration；Business Policy and Strategy；Economics；Electronic Commerce 
Entrepreneurship；Financial and Banking；Health Care Administration；Human 
Resource；Information System and Technology；International Business；Management 
and Organization Behavior；Management Education；Management Information System；
Managerial Consultation ； Marketing ； Operations Management ； Organizational 
Development and Change；Non-Profit Sector Management；Research Methods；Social 
Issues in Management；Technology and Innovation；Web Technology and Management；
and Other Relevant Topics。 
4. 作者於 7 月08 日參與 Session F3 (Financial and Banking) 發表論文，本組共 7 篇
論文發表。本人研究主題主要在決策管理科學相關的數量模式求解方法，發表混合式資料
探勘技術(類神經網路與案例基礎推理方法)應用於應用於增加信用卡徵信評分模型建立
之研究結果，” USING DATA MINING APPROACHES TO ENHANCE THE 
CREDIT SCORING MODEL ”，在此研究中，我們提出一個嶄新的整合性資料探勘
技術(類神經網路與案例基礎推理方法)應用於增加信用卡徵信評分模型建立，對信用卡徵
信評分問題從事研究，使其能建立增加信用卡徵信評分模型。因此，本研究欲提升信用評
等的準確率並降低分類時可能產生的誤差 (型一與型二誤差) 建立一增加型信用評等最
佳化 (Enhanced Credit Scoring, ECS) 模型，來減少人為因素、節省作業成本及加強信用卡
授信風險管理，使其客觀、快速的信用卡授信與開發潛在客戶。報告後，得到與會的許多
學者專家的興趣與討論，與會專家學者的提出寶貴意見相信對於此領域的未來研究會有許
多貢獻。 
二、與會心得 
1.本次研討會之研討內容不但包括決策理論與數值分析之研究與實務上各種決策分析與管理
的應用，更包含了目前有關數位時代、網際網路、電子商務等的決策管理與應用，可見全
世界各國的專家學者在決策科學領域的關注包含了理論研究與最新的實務應用，此次與會
之心得除了參與論文發表中學習最新的研究方法與方向外，更重要的是會議期間與會後能
2. 2008 年 7 月在馬來西亞吉隆坡市舉辦之徵求論文摺頁。 
六、其他 
投稿本次會議之論文如附件。 
strategy, credit scoring and other related fields. 
The objective of credit scoring models is to help the banks to find good credit applications 
who are likely to observe obligation according to their age, credit limit, income and martial 
condition. Many different credit scoring models have been developed by the banks and researchers 
in order to solve the classification problems, such as linear discrimintant analysis (LDA), logistic 
regression (LR), multivariate adaptive regression splines (MARS), classification and regression tree 
(CART), case based reasoning (CBR), and artificial neural networks (ANNs).  
LDA, LR and ANNs are generally used as methods to construct credit scoring models. LDA 
is the earliest one used for the credit scoring model. However, the utilization of LDA has often been 
criticized due to the assumptions of linear relationship between input and output variables, which 
seldom holds, and it is sensitive to deviations from the multivariate normality assumption (West, 
2000). In addition to LDA, LR is another common alternative to conduct credit scoring tasks. 
Basically, the LR model is emerged as the technique in predicting dichotomous outcomes and does 
not require the multivariate normality assumption. However, both LDA and LR are designed for the 
relationships between variables are linear, which caused them less accurate in credit scoring. 
In handling credit scoring tasks, ANNs became a new alternative and proved more accurate 
than LDA and LR. However, ANNs is also being criticized for its long training process in obtaining 
the optimal network and not easy to identify the relative importance of potential input variables, in 
addition, it had certain interpretive difficulties. Therefore, ANNs has limitation of applicability in 
handling general classification and credit scoring problems (Piramuthu, 1999). 
In addition to the above-mentioned techniques, MARS, a common used classification 
technique, is proved to be a good supporting tool for neural networks as the advantages of MARS 
can overcome the shortcomings of neural networks (Lee, 2005). Except for being used in 
classification models, CBR is also being widely applied in credit scoring models. CBR can use past 
experiences to find a solution, update database to increase the ability of solving complex and 
unstructured decision making problems (Shin, 1999; Wheeler, 2000). 
Many studies have contributed to increasing the accuracy of the classification model with 
various methods (Ong, 2005; Lee, 2006; Huang, 2006). However, most of the previous studies have 
only concentrated in building a more accurate credit scoring or behavioral scoring model. Even 
such scoring models are accurate, some misclassification patterns could be emerged, such as Type I 
error or Type II error. The purpose of this paper is to explore the performance of ANNs-based credit 
scoring model and reassign the rejected good credit applicants to the preferable accepted class by 
using the CBR-based classification technique. The increased accepted credit applicants of reassign 
classification systems in credit card can be significant benefit to decision makers, which contributes 
to increase business revenue and reduce Type I error of scoring system. 
 
2. LITERATURE REVIEW 
In this section, common techniques in building credit scoring models will be discussed.  
LDA was first proposed by Fisher (1936) as a classification technique. It has been reported so 
far as the most commonly used technique in handling classification problems (Lee et al., 1999). In 
the simplest type of LDA, two-group LDA, a linear discriminant function (LDF) that passes 
is split in two exclusive daughter groups. In the next step, every daughter group becomes a mother 
group. Every split is described by one value of one descriptor, chosen in such a way that all objects 
in a daughter group have more similar response variable values. The split for continuous variables 
is defined by “ Xi <A j ” where Xi is the selected explanatory variable and Aj its split value (shown 
as Figure 1). CART has been widely discussed in science, weather forecasting, social research, 
medical research, (Davis et al., 1999; Fu, 2004; Kurt et al., 2006; Li, 2006). CART has also been 
used by Li (2004) and Lee et al. (2006) in building credit scoring models. 
 
3. RESEARCH METHODOLOGY 
The purpose of this study is to present a Reassign Credit Scoring Model (RCSM). RCSM 
model is divided into two phases. In the first phase, MARS is used to obtain significant input 
variables of the ANNs model to reduce the number of input nodes, simplify the network structure 
and shorten the model building time. And then, ANNs model is used to classify credit applicants to 
either good or bad credit group which are represented as “ 1 ” and “ 0 ”, respectively. The rejected 
credit applicants by ANNs will be re-evaluated in reassigning credit scoring model which is applied 
by CBR to compare similarities between rejected credit applicants and CBR database which is 
divided into good and bad databases. If the value of SG (similarity retrieved from good applicants 
database) is higher than that of SB (similarity retrieved from bad applicants database), the rejected 
credit applicant will be reassigned to the conditional accepted class to reduce the Type I error and 
increase the banking portfolio; otherwise, this applicants will be assigned to rejected group. The 
processes of reassign credit scoring model is shown in Figure 2. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2 The processes of reassign credit scoring model 
Credit applicants 
dataset
MARS 
ANNs 
CBR 
Accept credit Conditional accepted Reject credit 
SG≦SB SG＞SB 
Credit 
Scoring 
Stage 
Reassign 
Credit 
Scoring 
Stage 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Simple backpropagation network illustration 
 
Since Vellido et al. (1999) pointed out that near 80% of business applications using ANNs 
will adopt the BPN training algorithm. This study will also use the popular BPN to build the credit 
scoring model. As recommended by Zhang et al. (1998), the single hidden layer network is 
sufficient to model any complex system. The designed network will have only one hidden layer. 
From a user’s viewpoint, as illustrated in Figure 3, a simple BPN consists of three layers: the input 
layer, the hidden layer and the output layer. The input-layer processes the input variables and 
provides the processed values to the hidden layer. The hidden layer further processes the 
intermediate values and transmits the processed values to the output layer. The output layer 
corresponds to the output variables of the BPN. The BPN is trained with a training sample. The 
training involves repeatedly presenting the input layer with the training sample until the network is 
able to remember the output of most of the sample items. The BPN develops memory by identifying 
the relationship between the input variables and the output variables. If the network commits a 
mistake, the BPN algorithm starts at the output layer and propagates the error backward through 
hidden layers. Therefore, besides the input variables and output variables, an optimal BPN also 
includes the optimal number of hidden neurons. For example, to evaluate credit card applicants, a 
BPN with the following features was developed: 
(a) Eight input variables: Status of existing checking account, duration in month, credit history, 
purpose, credit amount, saving account, present employment since, Other debtors/guarantors are 
applied in this paper. 
(b) Hidden nodes may be set to 2n, 2n ± 1, 2n ± 2, (n denotes input nodes), varying as try and error 
approach in section 4.4. 
(c) One output variable that takes two values: 1 for good credit and 0 for bad credit. 
 
3.2 The reassign credit scoring stage of RCSM 
 
Output Neurons 
Hidden Neurons 
Input Neurons 
Output Layer 
Hidden Layer 
Input Layer 
Connections 
Connections 
Good credit applicants / Bad credit 
Other debtors or Credit 
Present employment 
Purpose Saving Credit 
Duration in Status of existing checking 
applicants will be used to test the model, and the remaining 138 (68 good and 70 bad) applicants 
will be retained for validation. There are 14 independent variables in the dataset such as the credit 
card applicant’s age, credit amount, credit history, employment, housing and so on. The dependent 
variable is the credit status of the customer, that is, good or bad credit. 
 
4.1.1 Results of Credit Scoring Stage in RCSM 
The MARS-BPN credit scoring model is implemented by using MARS version 2.0 and 
NeuroShell version 2.0. The single hidden layer BPN model will again be adopted in building the 
two-stage hybrid model. MARS obtains 7 significant variables form 14 original variables as shows 
in Table 1. The input layer of the proposed model contains the obtained significant independent 
variables of the MARS credit scoring model as the input nodes. The trial and error approach will 
again be used to determine the appropriate number of hidden nodes for the desired networks (2n, 2n 
± 1, 2n ± 2, n denotes input nodes). The training of the network is also implemented with various 
learning rates and training lengths ranging from 1,000 to 10,000 iterations until the network 
convergence. The network weights are also reset for each combination of the network parameters 
such as learning rates (from 0.01 to 0.4) and momentum (from 0.8 to 0.99). Several options of the 
NN architectures for the testing data are evaluated, in which the 7-14-1 is found to obtain the better 
results in Table 2 and the learning rate, momentum and training epochs are set to 0.1, 0.9 and 3000, 
respectively. Accuracy rate of the validation data with various learning parameters for the 7-14-1 
architecture are summarized in Table 3. 
Table 1 Variable selection results of MARS 
Attribute Type Importance% 
A8 Categorical 100 
A3 Continuous 32.931 
A14 Continuous 27.282 
A10 Continuous 26.862 
A5 Categorical 24.925 
A7 Continuous 11.913 
A4 Categorical 7.481 
 
Table 2 Result of testing data with various network architectures 
Architecture Accuracy rate% 
7-12-1 93.50 
7-13-1 93.50 
7-14-1 93.50 
7-15-1 92.00 
7-16-1 93.50 
 
The credit scoring results of the validation sample are summarized in Table 4. From the 
results in Table 4, it is observed that the MARS-BPN model accurately classified 86.96% of the 
138 applications. Groupwise, the model correctly classified 63% of the 70 applications of bad 
credit card applications and 83.82% of the 68 applications of good credit card applications. 
 
124 87.73 94.84 No 485 92.43 93.82 No 
130 93.18 95.52 No 497 94.94 95.51 No 
150 86.16 91.87 No 518 89.64 93.32 No 
163 92.92 95.86 No 521 92.30 95.96 No 
172 91.70 95.07 No 536 93.92 97.50 No 
183 89.01 93.06 No 540 90.40 96.75 No 
203 91.34 94.14 No 582 90.32 93.33 No 
205 90.64 96.07 No 583 88.74 94.34 No 
216 91.41 96.60 No 584 96.56 96.58 No 
234 81.97 87.60 No 588 94.07 94.87 No 
241 88.88 91.23 No 589 95.98 96.48 No 
253 89.11 94.73 No 598 96.79 97.11 No 
256 94.59 97.71 No 604 89.69 96.08 No 
299 91.58 96.58 No 611 90.48 96.45 No 
314 91.61 96.58 No 623 93.17 97.32 No 
325 92.53 95.65 No 640 93.94 93.83 Yes 
332 94.07 95.31 No 643 88.90 93.49 No 
333 95.94 96.28 No 650 91.05 95.93 No 
337 91.91 96.34 No 651 95.64 96.39 No 
339 91.86 93.22 No 663 92.75 95.41 No 
344 93.43 96.01 No    
 
Table 7 Results of reassign credit scoring stage in RCSM 
Classified class Actual class 
Bad credit Good credit 
Bad credit 62(88.57%) 8(11.43%) 
Good credit 6(8.82%) 62(91.18%) 
Overall % Correct: 89.86% 
 
According to Table 6, one original bad credit applicants will be reassigned after CBR-based 
reassigning. However, according to Table 5, five original good credit applicants will be reassigned, 
as a result, Type I error will almost be eliminated (Table 7). Also, with six reassigned applicants, 
the approval rate will be increased from 46.38% (64/138) to 50.72% (70/138). 
 
4.1.3 Discussion 
In order to evaluate the classification capabilities of the five built credit scoring models, the 
credit scoring results of the validation samples are summarized in Table 8. 
 
Table 8 Credit scoring results of the constructed models (Australian credit data) 
Classified class Methods 
Bad-Bad Good-Good 
Type I 
error% 
Type II 
error% 
Accuracy 
rate% 
LDA 65 53 22.06 7.14 85.51 
LR 63 56 17.65 10 86.23 
CART 65 53 22.06 7.14 85.51 
ANNs 62 61 10.29 11.43 89.13 
Credit scoring stage of RCSM 63 57 16.18 10 86.96 
Reassign credit scoring stage of RCSM 62 62 8.82 11.43 89.86 
 
The result shows that the proposed model, RSCM, not only outperforms the commonly 
utilized linear discriminant analysis, logistic regression, and neural networks credit scoring models, 
but also provides efficient alternatives in conducting credit scoring tasks. The attribute names and 
values have been changed to symbolic data to protect the confidentiality of this data, as a result, the 
increases approval rate of credit card application.  
Most companies are starting better strategies with the help of credit scoring models. Hence, 
various modeling techniques have been developed in different credit evaluation processes for better 
credit approval schemes. This study helps to find conditional accepted applicants. The appropriate 
management strategies for these conditional accepted applicants are approved of lower credit limit 
and usage under monitoring on probation, for example, more frequently to remind and monitor their 
credit amounts to prevent them from overusing their credit. The creditor can consider increasing 
their credit limit according to credit behavior after monitoring for a certain time. The conditional 
accepted applicants will contribute to increase revenue. 
 
REFERENCES 
1. Arminger, G., Enache, D. & Bonne, T. 1997. Analyzing credit risk data: a comparison of 
logistic discriminant classification tree analysis and feedforward networks. Computational 
Statistics, 12, 293–310. 
2. Bardos, M. 1998. Detecting the risk of company failure at the banque de france. Journal of 
Banking & Finance, 22, 1405–1419. 
3. Barney, D. K., Graves, O. F. & Johnson, J. D. 1999. The farmers home administration and farm 
debt failure prediction. Journal of Accounting and Public Policy, 18, 99–139. 
4. Bradley, P. 1994 Case-based reasoning: Business applications. Communication of the ACM, 37, 
40–43. 
5. Breiman, L., Friedman, J. H., Olshen, R. A. & Stone, C. J. 1984. Classification and Regression 
Trees, Wadsworth, Pacific Grove, CA. 
6. Chen, M. C. & Huang, S. H. 2003. Credit scoring and rejected instances reassigning through 
evolutionary computation techniques. Expert Systems with Applications, 24, 433–441. 
7. Chou S. M., Lee T. S., Shao Y. E. & Chen I. F. 2004. Mining the breast cancer pattern using 
artificial neural networks and multivariate adaptive regression splines. Expert Systems with 
Applications, 27, 133-142. 
8. Davis, R. E., Elder K., Howlett D. & Bouzaglou E. 1999. Relating storm and weather factors to 
dry slab avalanche activity at Alta, Utah, and Mammoth Mountain, California, using 
classification and regression trees. Cold Regions Science and Technology, 30, 79-89. 
9. Fisher, R. A. 1936. The use of multiple measurements in taxonomic problems. Annual Eugenics, 
7, 179–188. 
10. Freeman, J. A. & Skapura, D. M. 1992. Neural networks algorithm, application and 
programming techniques. MI, USA: Addison-Wesley. 
11. Fu, C. Y. 2004. Combining loglinear model with classification and regression tree (CART): an 
application to birth data. Computational Statistics & Data Analysis. 45, 865-874. 
