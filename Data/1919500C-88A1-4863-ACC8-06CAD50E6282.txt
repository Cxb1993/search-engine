 2
無法正確偵測外力，即可能造成人類或機械
手臂的傳動機構損傷；反之被動式順應性，
通常是藉由機構上的順應機制來達到防護的
效果，因此不會有上述的問題。 
被動式順應性致動器在發展上，可分為
順應性固定式致動器 (Constant Compliance 
Actuator)與順應性可調整式致動器(Variable 
Compliance Actuator)兩種機制。在順應性固定
式致動器方面，1995 年，MIT 的 Williamson
利用一個具十字斷面的板型彈簧與馬達串
接，建構串列彈性致動器 (Series Elastic 
Actuators；SEA)做為機械手臂的關節致動裝
置[7]，以達到順應性的效果；在 2003 年，韓
國技術學院的 Seung 等人[5]，以數個線性彈
簧 配 合 磁 流 變 液 旋 轉 阻 尼 器
(Magneto-rheological Rotary Damper)來建構
順應性致動器，由於其順應性致動器具有阻
尼器，因此具有吸震的優點。另外，在 2004
年，史丹佛大學的 Zinn 等人利用大扭力馬達
與具有小扭力馬達的 SEA 配合，且透過鋼索
驅動連桿，建構出分散式大小  (Distributed 
Marco-mini；DM2)致動器[8]，可使機械手臂
除了具有順應性之外，亦具有高頻工作的能
力。 
對於順應性可調整式致動器，大致上可
以分成兩個種類[6]，第一種為利用兩對抗式
非線性彈簧之裝置(An Antagonistic Setup of 2 
Non-linear Springs)達到可調式功能，其主要
的概念為採用兩個具有非線性順應的致動
器，且採用互相耦合的對稱式設置，故可藉
由控制兩個致動器，即可改變關節的順應性
[1][3]。第二種為利用結構式的彈性常數控制
(Structural Stiffness Control)的方式，其主要的
概念為利用改變彈簧的幾何特徵，如長度或
慣性矩，即可達到改變彈性常數之目的[2][4]。 
計畫目的 
機器人逐漸地走入人群已是一種趨勢，
相信在不久的將來，人類與機器人共同生活
的場景一定會呈現在我們眼前。由於當機器
人與人共存時，相互間接觸與碰撞的狀況是
不可避免的，因此安全性是必須考量的問
題。本研究計畫第一年的目的在於建構一個
能與人類共同相處在同一生活空間之類人形
機械手臂，以做為後續服務用機器人發展之
基礎。擬研發較簡單、較易實現的技術，使
得機器人除了能完成期望的任務外，亦能擁
有安全機制。 
三、研究方法 
3.1 具被動式順應性關節機械手臂之設計 
為了使機械手臂具有順應性的效果，因
此本研究設計了一個肩部三關節具有被動式
順應性關節(Passive Compliant Joint；PCJ)的七
軸機械手臂，在機械手臂整體的設計上，主
要考慮以下幾點要求: 
a) 因為空間有限，機械手臂整體體積儘
量保持最小，並作最佳機構排列方式
的設計，使空間有效利用。  
b) 機械手臂機構的材質亦盡可能輕，以
降低機構的慣性，達到降低馬達負載
並增加系統的操控性。 
c) 成本控制最小(包括材料、保養、消耗
的能源)，所以應該以一般工業用材
料、標準零件與低磨耗元件為主。  
d) 必須考慮 PCJ 與機械手臂主體的連接
性，且撓性元件如彈簧都應符合拆裝
容易。  
e) PCJ 的可撓曲角度與剛性大小應適當
選擇，使系統具有可控性。  
f) 設計時須考慮手臂末端希望的負載要
求，以達到結構強度與系統性能的平
衡。  
g) 機構設計時，應同時規劃將來機械手
臂的致動裝置(馬達)及其他感測元件
的配線方式。  
h) 為了使機構運動的精度提高，其傳動
機構的背隙(Backlash)要越小越好。 
3.1.1 機械手臂之機構設計 
在手臂機構上，如圖一所示，建立了肩
(Shoulder)、肘 (Elbow)及腕 (Wrist)之三大關
節；其中，肩部包含滾動(Roll)、搖動(Pitch)
及擺動(Yaw)三旋轉軸，並使其相交於一點，
且於三個旋轉軸分別加入 PCJ，使得機械手臂
具有順應性，肘部具有一自由度，腕部也包
 4
桿角度誤差的增益值，此增益值的大小可決
定馬達對彈簧壓縮量 eq 產生變化時的靈敏
度。因此，當 1FL  時，即可降低馬達對彈簧
壓縮量變化的反應，並將此變化量累加後，
即可得到馬達實際上應到達的角度 1n+q ，反之
當 1FL  時，即代表馬達直接反應出彈簧的壓
縮量變化。 
 
圖三 機械手臂的控制方塊圖 
為了達到以手教導(Teach-by-hand)的功
能，即透過人們的手以抓取機械手臂並拖曳
其移動的方式，來產生機械手臂的期望運動
路徑，通常須要加裝力量感測器，以量測機
械手臂是否受到外力的依據，才得以完成。
但當吾人在發展具被動式順應性關節之機械
手臂時，為了量測彈簧的變形量，故在 PCJ
上安裝外部編碼器，也因為如此，吾人發現
若在拖曳過程中，將機械手臂受到外力產生
的彈簧變形量視作關節的輸入命令，則被動
式順應性關節機械手臂即可使執行教導式工
作，其控制方塊圖如圖四所示。利用此種方
式來達到教導式工作最大的優點在於無須使
用昂貴的力量感測器。 
 
圖四 以手教導的控制方塊圖 
3.4 機械手臂的性能測試 
3.4.1 軌跡追蹤 
本實驗測試之主要目的在於驗證機械手
臂軌跡追蹤的性能，利用分散式的控制架
構，以執行一個圓形路徑之追蹤，全程共需
17 秒，機械手臂末端在工作空間的運動軌跡
如圖五所示。 
 
圖五 機械手臂末端的運動軌跡 
3.4.2 順應性測試 
在本實驗中，藉由碰撞測試來驗證所建
構機械手臂的順應性效果。當機械手臂到達
定點後，有一外力作用在其末端，如圖六所
示；而碰撞時，肩部三關節的連桿角度變化
情形，則如圖七所示。 
 
圖六 碰撞點示意圖 
 
圖七 肩部三關節於碰撞時之連桿角度 
表 Y04 1
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                            97 年 06 月 18 日 
報告人姓名 
 
蔡清元 服務機構
及職稱 
 
成功大學  機械工程學系 
副教授 
     時間 
會議 
     地點 
 
自 97 年 01 月 31 日至 97 
年 02 月 02 日；日本‧別府
本會核定
補助文號
 
NSC 96-2221-E-006-336 
會議 
名稱 
 (中文) 第十三屆「人工生活與機器人學」國際會議 
 (英文) The Thirteenth International Symposium on Artificial Life and Robotics 
發表 
論文 
題目 
 (中文) 結合聲音感知系統之雙眼機械頭強健追蹤控制 
 (英文) Robust Binocular Tracking with an Auditory Perception System 
一、參加會議經過 
第十三屆「人工生活與機器人學」國際會議 (The Thirteenth International 
Symposium on Artificial Life and Robotics 2008; AROB 13th ‘08)係由日本學術振興會
(Japan Society for the Promotion of Science)所主辦。於 2008 年 01 月 31 日至 02 月 02
日，在日本別府市舉行。本次會議在日本別府市 B-Con Plaza 的國際多功能會議中
心舉行，有相當好的會場安排，參加會議的人士均能享有十分融洽的場面。此次議
程由 3 個 Plenary Talks、4 個 Invited Talks、Technical Sessions 及 Poster Sessions 所
組成。 
本次會議地點在別府，由於該地沒有國際機場，必須先搭乘國際航班至福岡，
再轉搭火車至別府。因此，筆者於 1 月 29 日先搭本國飛機至福岡住宿一晚，並於
當天前往福岡的機器人廣場(ROBOSQUARE)，參觀其推廣機器人教育與產業的成
果，對於其針對機器人技術所規劃的多元推廣方式留下深刻的印象。隔天，即搭火
車前往別府，傍晚報到後，晚上則參加歡迎餐會，而論文發表則排在 1 月 31 日至 2
月 2 日三天;每天分別排定一場大會演講(Plenary Talk)，分別為“Haptics for Medical 
Applications＂、“Membrane Computing and Brain Modeling＂與“A Simple Adaptive 
Control of Electrically Driven Flexible-Joint Robots Using Function Approximation 
Techniques＂，並於第一及第三天各穿插有一場邀請演講(Invited Talk)，而於第二天
則穿插有二場邀請演講。筆者被安排於 2 月 1 日主持『Artificial Life and Embodied 
Robotics: Current Issues and Future Challenges』之邀請演講，並被安排於 2月 2 日主
持『Robotics-II』場次之論文發表會，而筆者的論文“Robust Binocular Tracking with 
an Auditory Perception System＂則被安排於 2 月 2 日的『Robotics-I』場次中發表，
The Thirteenth International Symposium on Artificial Life and Robotics 2008(AROB 13th ’08), 
B-Con Plaza, Beppu, Oita, Japan, Jan. 31- Feb.2, 2008  
©ISAROB 2008  799 
Robust Binocular Tracking with an Auditory Perception System 
 
 T. I. James Tsay and Yuan-Yu Li 
Department of Mechanical Engineering, National Cheng Kung University 
Tainan, Taiwan 701, R.O.C 
 tijtsay@mail.ncku.edu.tw 
 
 
Abstract: This paper uses an auditory perception system to assist visual tracking. Using auditory perception system to 
track the source of the sound from a target and then moving the robotic head to face the target help the CCD cameras of 
the head locate the target’s image, ensuring that the visual tracking works. Combining the visual tracking system with 
an auditory perception system increases the overall efficiency of the visual tracking system. In auditory perception, an 
equilateral triangular array of microphones is used to receive sound signals, and the direction to the sound source is 
determined using the cross-spectrum method. Digital signal filters and a cepstrum lifter can be used to recognize the 
digital signal to determine whether the sound matches the target’s sound model. Then, the target’s sound source is 
recognized and its direction is determined. In image tracking, three-step hierarchal search method is replaced with the 
edge-blobs pattern matching method proposed herein to increase the tracking speed. It is combined with edge-blobs 
contour matching and YCbCr space image recognition to improve the recognition of the target image and to filter out 
background noise. The target is then tracked regardless it is out of shape or passes through a work space with complex 
background. A robust binocular tracking system with an auditory perception system is developed to enhance target 
tracking. One experiment is conducted to verify the effectiveness of functions for the developed system. 
 
Keywords: auditory perception, binocular, cepstrum lifter, edge-blobs pattern matching, edge-blobs contour matching. 
 
 
I. INTRODUCTION 
With advances in robotic vision, visual target 
tracking remains constrained by difficulties in tracking 
invisible targets: designing an image tracking algorithm 
to search for a target that is not in the image plane is 
difficult. If, when the tracking system starts, the target is 
not in the image plane, or it is covered by objects in the 
working space, “target disappearance” may occur and 
much time will be spent searching for the target in the 
work space, causing the visual tracking to become 
unstable. 
Some recent studies about visual tracking, such as 
those of Chen [1] and Wang [2], have combined pattern 
matching and contour matching to increase the 
robustness of visual tracking systems. In 1995, 
Yanagisawa et al. [3] constructed an equilateral triangle 
microphone array to track a whistle. In 2002, Chen [4] 
realized such a sound tracking system by designing IC 
circuits and using digital signal processors. He placed 
this sound source tracking system on an autonomous 
guided vehicle to enable it to track a whistle.  
While combining auditory perception with visual 
tracking, Hu et al. [5] developed a self-calibrated 
speaker tracking system based on both audio and video 
information. They set an array of equilateral triangle 
microphones on a pan-tile active camera to track a 
speaker. The Japanese company, NEC, researched and 
designed a small, intelligent robot-R100 with a height of 
44cm and a weight of 7.9Kg. It has two cameras, three 
microphones, many force sensors and ultrasonic sensors. 
NEC began to plan the R100 in 1997. The robot-R100 
combines pronunciation recognition, image recognition 
and an electromechanical system to track and recognize 
specific people. R100 can understand hundred Japanese 
characters. This robot responds appropriately, enabling 
simple interactions with humans. 
In this paper, a simple but fast image-tracking 
algorithm – edge-blobs pattern and contour matching is 
proposed for visual tracking. A color space target model 
is constructed and color space patterns and contours 
matched to filter out background noise. In auditory 
perception, an equilateral triangular array of 
microphones is used to track the sound source and 
perform simple voice recognition. The sound source-
tracking algorithm separates the two-dimensional-pan 
space into 12 directions. The goal of sound source 
tracking is to control the robotic head to face the target 
to track the image. In summary, a robust binocular 
tracking system with auditory perception is developed 
to enhance target tracking. This control architecture can 
be used not only in a security system but also in home 
robots to improve their interaction with humans. 
 
The Thirteenth International Symposium on Artificial Life and Robotics 2008(AROB 13th ’08), 
B-Con Plaza, Beppu, Oita, Japan, Jan. 31- Feb.2, 2008  
©ISAROB 2008  801 
then the second to fifth axes of the motors maintain the 
image target in the center of the image plane; otherwise, 
the sound source is tracked until the target appears in 
one or both of the CCD image planes. Fig. 4 shows the 
overall tracking algorithm. 
 
Fig. 3 The overall target-tracking system 
 
Fig. 4 Block diagram of overall tracking system 
 
4.2. Experiments 
The experiment concerns binocular tracking with the 
auditory perception system under complicated and 
semi-covered circumstances. Fig. 5 depicts the 
experimental setup. A cover plate is placed to cover the 
left-half path of the target. The goal of this experiment 
is to test the robustness of the overall tracking system, 
as the speed and the direction of the covered target 
changes. The target is covered from 0 cm to 20 cm 
along the sliding rail, so a tracking error of around 10cm 
exists from time 0sec to 15sec. From 15sec to 20sec, the 
target accelerates. The experimental results in Figs. 6 to 
8 display the robotic binocular head tracking 
performance. 
 
Fig. 5 Experimental setup 
 
Fig. 6 Neck pan-direction trajectory 
