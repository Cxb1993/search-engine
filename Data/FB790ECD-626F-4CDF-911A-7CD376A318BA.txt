執行單位：國立中正大學電機工程學系暨研究所 
 
中   華   民   國 96 年  2 月  25  日 
行政院國家科學委員會專題研究計畫成果報告 
智慧型看護與居家照顧機器人系統之設計與研製--子計畫四：以家庭閘道器為基礎之群組機
器人學習與合作策略 
計畫編號：NSC 95-2218-E-194-009  
執行期限： 95 年 10 月 1 日至 96 年 10 月 31 日 
主持人：黃國勝 國立中正大學電機工程學系 
Email: hwang@ee.ccu.edu.tw, URL: http://www.eis.ee.ccu.edu.tw/~hwang/ 
 
 
中文摘要 
    一般機器人的動作設計，是依據特定的目的，由設
計者站在機器人的角度上，觀測感測訊號，來撰寫控制
的法則，這樣需要縝密的考量以及豐富的機器人學知
識，並且在做最佳化的時候，得以嘗試錯誤的方式，來
做動作的調整與修正。Reinforcement Learning-based 
Decision Tree Algorithm 的方法則是從機器人的角度來
看設計者，藉由觀察設計者的操控，使用決策樹來歸納
並分割動作，把設計者的動作模仿起來。既降低了設計
動作的困難度，在最佳化方面，也可以讓機器人自主地
學習。 
    本論文所提出的融合行為學習演算法(FBQL)，仍然
保持著設計動作的簡單性，同時能夠考量多個行為的動
作，並且針對整體的行為做最佳化。設計者只需針對多
個單一行為各自做操控示範，分別讓機器人學著模仿，
然後直觀地設計整體行為的回饋值，利用 FBQL 系統來
將多個單一行為的動作以權重值 W 值來做融合，並由
學習法則來不斷地調整權重值 W 值，找出各行為間的
最恰當比重。 
I.前言 
A. 動機 
    一般機器人的動作，都是由程式設計者依據各種特
定的目的，制定各種不同的行為，並且把複雜的動作轉
化成程式碼後，才能夠讓機器人實現。但是設計者要能
夠把心中想像的動作，分區規劃成各個子系統的控制
值，需要很縝密的考量以及豐富的機器人學的知識，並
且得經由實測結果，不斷地來做調整修正，才能夠讓機
器人有比較好的表現。而且，設計者雖然是依據機器人
所搭載感測器的值來做判斷，但是在動態的情況中，往
往不容易去觀察感測器的值，因此設計者所看到的通常
都是延遲過後的值，或者是情況相對靜態的值。這樣的
設計方式，設計者既不容易設計，也不容易準確地操控
機器人。 
     Reinforcement Learning-based Decision Tree 
Algorithm (RL-based Decision Tree) [1][2]以設計者遙
控機器人的方式，讓機器人去紀錄感測值與輸出動作的
組合，經由系統性的整理與歸納，讓機器人懂得去模仿
設計者的輸出動作。以遙控的方式，來把設計者的理念
灌輸給機器人，雖然設計者的遙控不會是最完美的動
作，但是可以讓機器人用加強式學習來自主地訓練、調
整與進化，這樣出來的動作，便是機器人即時地對環境
變化作出的適當反應。Reinforcement Learning-based 
Decision Tree Algorithm 一次要同時模仿多個行為的動
作時，便會產生幾點問題： 
1.設計者不容易同時兼顧多個行為，以操控的   
  方式讓機器人模仿。 
2.當感測訊號種類太多時，很難以決策樹做歸 
  類，並且需要大量的時間。 
3.機器人很難對整體行為來做最佳化。         
    本論文所提出的架構，便是利用 RL-based Decision 
Tree Algorithm 可以模仿單一行為的特點，各自模仿出多
個單一行為，以融合的方式來決定各行為的重要性，並
且不斷地學習融合的比例。藉由機器人多次與環境互動
的經驗，讓機器人自主地學習與調整，既能提升調整的
效率，又能節省設計者的時間。本論文的比較對象是以
Subsumption Architecture[3][4][5]做三個單一行為的融
合，經過多次實驗後，分別從多項整體目標來比較兩個
演算法的效果。 
B. 論文架構 
    第二章針對了解本論文所需要的背景知識做介紹，
包含加強式學習(Reinforcement Learning)、Q-Learning、
決策樹歸納法(Decision Tree Induction)、以及
Subsumption Architecture。 
第三章介紹本實驗室提出的 Reinforcement 
Learning-based Decision Tree Algorithm，如何利用決策樹
來模仿單一行為，並且以 Decision Tree-based Q-Learning 
(DT-based Q-Learning)來做行為的最佳化。第四章介紹本
論文提出的融合行為學習演算法(Fused Behavior 
Q-Learning Algorithm, FBQL)，包括整個架構、各項參數
的意義和用法、以及演算法的流程。第五章介紹實驗設
計與討論。包含三個單一行為的設計、以 Subsumption 
Architecture 融合行為的結果、以融合行為學習演算法
 4
表格，方便搜尋與求值。 
      ∑
=
⎟⎠
⎞⎜⎝
⎛ ∗=
n
i
ii
T
S
T
SE
1
2 )(log)()(φ          (3) 
 T 為內部節點φ 內所有的資料總數， iS 則是第 i 種類
的資料所佔的個數，n 是不同種類資料的種類數。 
樹根節點
樹葉
內部節點
 
圖 3  Decision Tree Induction 的架構圖 
 
5≤x
5≤x
18≤y
5>x
18≤y 18>y
1fz = 2fz =
3fz =
y
x
3f
2f
1f
5=x
18=y
 
 
圖 4  Decision Tree Induction 的範例 
 
樹枝生長
樹葉生長
樹枝生長程序：
IF 符合任一終止條件
指定樹葉的值
ELSE
以切割標準來決定切割位置
把內部節點切割成兩部分
兩部分都各自執行樹枝生長程序
END IF  
圖 5  Decision Tree Induction 的演算法 
D. Subsumption 架構 
     Subsumption Architecture [3][4][5] 是 Rodney 
Brooks 所提出的一個多階層的架構。他認為建構世界模
型，然後再以推理的方式來規劃並控制機器人，無法及
時對現實做出適當的反應。因此把機器人依據各種不同
的功能，來設計個別的子系統，每個子系統都具有足夠
能力來完成機器人的操控。如此一來，每個子系統並不
需要考慮到所有複雜的問題，只需要處理特定的資訊，
便可以驅動機器人了。 
 
感測訊號 致動器感知器建構模型 路徑規劃執行任務 馬達控制
 
圖 6  傳統的機器人控制模型 
    圖 6 是一般傳統的機器人控制模型，藉由幾個主要
的子系統，來處理各式不同的資料。感知器用來接收感
測元件所傳出來的訊號；建構模型是把收到的這些訊
號，轉化成對機器人有用的資訊；執行任務是決策應該
達成的目標為何；路徑規劃是決定如何達成目標的步
驟；馬達控制則是輸出命令給馬達，讓機器人能夠沿著
我們所預想的路徑移動。  
    圖 7 是 Rodney Brooks 所 提 的 Subsumption 
Architecture，每一階層都是個獨立的行為，專門處理特
定的動作，可以達成特定的任務。因此每個階層，只需
要感測少量的訊號，分析必須的資訊，便可以控制機器
人了，大大加快了運算的時間。然而，如果同時有好幾
個階層的行為被觸發的時候，勢必會產生行為的衝突，
因此 Rodney Brooks 在每個階層裡面加上了抑止訊號，
藉由此訊號，可以針對其他的階層作約束，甚至是整個
壓抑掉。因此同時被觸發的這些階層，在經過彼此壓抑
的過程之後，最後剩下的階層才會去啟動致動器。而且
這種系統還有一個好處，每個階層的設計都可以獨立出
來，就算使用不同的記憶體，彼此的時序不同，甚至是
用不同的處理器都沒有關係，大大增加了系統的擴充性
以及設計方便性。 
 
感測訊號 致動器
探索新區域
避開障礙物
移動
撿起罐子
回家
放下罐子
 
圖 7  Subsumption Architecture 
III. 先前的研究 
     RL-based Decision Tree [1][2]是本實驗室的研
究團隊所提出的演算法。這個演算法的出發點，是人類
在跟環境互動的過程中，憑藉的是直覺，但是設計機器
人動作的時候，卻要算出機器人的動態模型以及定義出
感測訊號與動作間的關係，非常地不容易。因此由設計
者依據直覺來遙控機器人，讓機器人記錄下感測訊號與
動作間的關係，然後利用決策樹來做整理與歸納，把設
計者的經驗內化成機器人的知識。利用加強式學習的法
則，來學習如何使用決策樹整理，能夠把動作歸類的最
整齊。 
    接下來以多個小節介紹RL-based Decision Tree的整
套演算法，以及建構在這演算法之上的 Decision 
Tree-based Q-Learning (DT-based Q-Learning)，用來改善
機器人的效能，並且展示研究成果。 
A. RL-based Decision Tree  
     Decision Tree Inductio是一個 top-down的貪婪歸類
法，雖然這樣能夠把局部的資料分類的很清楚，然而以
宏觀的角度來看，最理想的分類情形還要考慮到之後繼
續歸類下去的結果，因此有論文研究非貪婪的歸類法[18] 
[19]。一般的決策樹在做分割的時候，會依據當時的內
部節點，從最能夠把資料分成兩個種類的地方做切割。
然而這樣的方式太注重當下分割的結果，每次都從最有
 6
 
C. 研究結果 
    這裡以移動式機器人追尋目標的實驗來驗證
RL-based Decision Tree Algorithm 與 DT-based 
Q-Learning。由設計者示範尋標的動作，讓機器人紀錄
感測訊號與輸出動作的關係，共 1000 筆資料，如圖 11
所示。圖中的 X 軸為機器人與目標點的距離、Y 軸為機
器人與目標點的角度、Z 軸為機器人所記錄下來的類別
(包含左輪與右輪的輪速)。 
Z(attribute)
X(distance)
Y(angle)
 
圖 11  機器人記錄下來的資料(1000 筆) 
經由 RL-based Decision Tree Algorithm 的歸納之後，我
們可以從圖 12 的訓練結果來看出，大約在訓練第 50 次
左右的時候，整棵樹的成長大約就收斂了。若以總純度
來看的話，如圖 13，大約也是訓練到地 50 次左右便收
斂在-0.4 左右。 
0
50
100
150
200
0 200 400 600 800 1000
trials
number of leaf nodes
 
圖 12 RL-based Decision Tree 的歸類結果(1/2) 
-1
-0.8
-0.6
-0.4
-0.2
0
0 500 1000
trials
impurity
 
圖 13 RL-based Decision Tree 的歸類結果(2/2) 
Z(attribute)
X(distance)
Y(angle)
 
圖 14 RL-based Decision Tree 的歸類結果 
     經過這樣的歸類之後，原本圖 11 的資料便會很有
次序地如圖 14 排列，可以明顯地看出同類型的資料會被
分成同一區塊。機器人依據這樣的歸類去控制，便可模
仿出設計者當初所操控的動作。圖 15 與圖 16 則是把移
動式機器人的兩輪輪速分別獨立出來表示，輪速為正值
表示輪子正轉，負值代表輪子要反轉。 
 
3 2
6
14
5
 
Fig.17 The soccer field 
 
Z(speed)
X(distance)
Y(angle)  
圖 15 左輪輪速的歸類結果 
Z(speed)
X(distance)
Y(angle)  
圖 16 右輪輪速的歸類結果 
    我們以圖 17 的情況來讓機器人做 DT-based 
Q-Learning 訓練。圖中的六個點分別都可以當作機器人
的起點以及終點，總共可能的路徑有 15 條，我們挑選其
中的 10 條來做學習，每次學習 30000 步。經過學習之後，
左輪與右輪的輪速從原本的圖 15與圖 16變成圖 18與圖
19，可以明顯地看出學習的效果讓輪速有了較多的變
化，同時又不失去模仿動作的基礎。 
 8
Behavior
reward
Fused
r
FB
sen
w(s,1)
w(s,2)
w(s,|B|)
Behavior(|B|)
FBQL
Behavior(2)
Behavior(1)
sensory input
agent
environment
 
圖 21  FBQL 的架構圖(2) 
B. 單一行為的模仿 
    單一行為經過了 RL-based Decision Tree Algorithm
的分割之後，會被分類成幾區極具代表性的輸出動作，
可以利用邊界範圍來表示同一種類的輸出情況。因此，
當要模仿此單一行為時，只需根據這兩個變數的值去查
表，看是被歸類到哪個分類之中，便可以知道該做怎樣
的輸出動作來模仿。 
 
Algorithm: Fused Behavior Q-Learning Algorithm  (FBQL) 
Input:  sensory input sen , reward r  
Output:  Fused Behavior FB  
1. Use RL-based Decision Tree Algorithm to classify each behavior. 
2. Initialize ( for all Ss∈ , Bi∈  ) 
    ( )isQ ,  arbitrarily , 
    weight ( )isw ,  arbitrarily , 
    the cardinality of the behavior set B  , 
    each output of the single behavior iB  , 
    learning rates α  )10( ≤< α  , 
    temporal discount factor γ  )10( ≤< γ  , 
    offset f  to ensure that the value to update ),( isw  is positive, 
    learning rates δ  )
),(max
10(
fisQ +≤< δ  . 
3. Demonstrate the Example Policy to the robot. 
4. Repeat for each step of episode: 
(a)  observe sensory input sen  to check class for each behavior, and then decide the state 
{ }Bclassclassclasss ,,, 21 ⋅⋅⋅⋅⋅⋅=  
(b)  execute Fused Behavior FB  
∑
=
⎟⎠
⎞⎜⎝
⎛ ∗←
B
i
iBiswFB
1
),(
 
(c)  observe reward r  and next state 's   
(d)  update each ),( isQ  for all Bi∈  
⎟⎟⎠
⎞
⎜⎜⎝
⎛ −⎟⎠
⎞⎜⎝
⎛ ∗∗+∗∗+← ∑
=
),(),'(),'(),(),(),(
1
isQjsQjswriswisQisQ
B
j
γα  
(e)  update each ),( isw  for all Bi∈  
 10
也要不斷地參考評估值 Q 值，以此來做更新。更新公式
如(15)所示： 
( )
( ) ( )⎪⎪⎩
⎪⎪⎨
⎧
⎟⎠
⎞⎜⎝
⎛ +∗∗−
=⎟⎠
⎞⎜⎝
⎛ +∗∗−
+←
otherwisefisQisw
isQiiffisQisw
iswisw
i
,,),(0
),(maxarg,),(),(1
),(),(
δ
δ  (15) 
    權重值 W 值在經過更新之後，整個權重的和可能大
於 1，會讓機器人的輸出動作超過單一行為的能力限
制，因此權重值在更新過後，還要做正規劃的步驟，如
公式(16)： 
∑
=
←
B
j
jsw
iswisw
1
),(
),(),(   for all Bi∈       (16) 
H. 示範動作 
   示範動作的架構如圖 25 所示，基本上依然是原始的
FBQL 系統，但是本來應該由系統決定權重比例的地
方，改由設計者所示範的策略來取代，由設計者來決定
機器人的權重比例，來引領機器人去接觸到特定的回饋
值。由於 FBQL 系統剛開始對於環境是完全陌生的，因
此讓系統自主控制機器人的動作的話，反而是盲目地行
動；而依據設計者的示範動作來行動的話，則是有目標
性的行動。目標包括獎勵以及懲罰的回饋值，因此設計
者可以很容易地帶領機器人接觸這些目標。 
Behavior
reward
Fused
r
FB
sen
FBQL
Behavior(|B|)
Behavior(2)
Behavior(1)
sensory input
Example
Policy
 
圖 25 FBQL 的架構圖(3) 
V. 實驗設計與討論 
 
A. 模擬環境設計 
    在這個實驗中，所採用的模擬環境為 FIRA 聯盟
(Federation of International Robot-soccer Association)所使
用的模擬組比賽平台。這個平台包含了比賽場地、邊牆、
雙方球門、球、以及雙方各五隻機器人，雙方可以寫 C
語言或是 Lingo 語言來控制場地上的機器人。圖 26 是這
個模擬環境的顯示介面。 
 
圖 26 FIRA SimuroSot 模擬器 
    雖然這個模擬器裡面的機器人都是兩輪驅動，但是
為了將來如果要硬體實驗的話，機器人的硬體結構可能
不會一模一樣，因此這裡定義所有行為的輸出，都是以
從機器人出發的座標點來表示，如圖 27 所示。 
  Robot_Y
Robot_X
 
圖 27 行為輸出的座標系 
    這裡設計了三個單一行為，分別是追球行為、避牆
行為、跑位行為，而最終的實驗目標便是融合這三種獨
立的單一行為，把球踢進球門，完成一整套機器人的進
攻策略。以下分別來描述這些單一行為。 
  (1) 追球行為 
    追球行為是讓機器人能盡速碰到球的行為，雖然這
裡不考慮球被撞到以後會往哪個方向滾動，但是基本上
會往機器人與球連線的延伸方向上。球相對於機器人的
距離與機器人車頭角度(-180˚~+180˚)的定義，如圖 28 所
示。 
            
Ball_distance
Ball_angle
 
圖 28 追球行為的示意圖 
 
  (2) 避牆行為 
    避牆行為是為了避免實體機器人碰撞牆壁時，瞬間
的大電流會對馬達及電路的造成負擔，而且經常遭受大
力撞擊的話，對於外部的機構以及內部的精密零件，都
可能造成損壞。這裡雖然是用模擬器當做實驗平台，但
是避牆行為仍然能夠避免機器人卡在牆邊，而造成實驗
效果不彰。藉由場地上距離機器人最近牆壁的距離與角
度，來判斷機器人應該使用多少的輪速來驅動，才能避
免撞到牆壁。最近的牆壁相對於機器人的距離與機器人
車頭角度(-180˚~+180˚)的定義，如圖 29 所示。 
 
          
Wall_distance
Wall_angle
 
圖 29 避牆行為的示意圖 
  (3) 跑位行為 
跑位行為是一個讓機器人策略性地跑到目標點的行為。
在足球比賽中，跑位常常可以讓自己能夠有更好的機會
可以攻擊或是阻擋敵方球員的傳球。這裡設計的跑位行
為，主要的概念便是以進攻機會作為考量，因此跑位的
方式，先是以弧線繞到目標點(星號)的後方，才從後方
 12
Algorithm 來模仿單一行為是否各個行為仍然具有足夠
的代表性、以及說明示範動作是如何地設計、並且說明
所提出的融合行為學習演算法(FBQL)，如何學習把球踢
進黃門內，並且展現實驗結果。 
  (1) 以 RL-based Decision Tree 模仿單一行為 
    首先分別執行這三種單一行為的動作，然後把當時
的輸入資料和輸出座標存起來，然後放入 RL-based 
Decision Tree 去做輸入資料的切割，希望在經過切割之
後，能把輸出座標類似的切在同一個範圍之內。系統針
對輸入資料做切割的時候，會持續切割直到符合中止條
件為止，終止條件有三點： 
1.這個範圍內的資料純度大於-0.5，已經很統一了。 
2.這個範圍內的資料總數在 6 個以下，資料數太少了。 
3.這個範圍的斜邊不足 6，表示切割範圍太小了。 
    圖 34 便顯示以追球行為的兩個輸入資料做分類的
結果，每個不同的顏色表示不同的分類區塊，可以觀察
出當與球的距離與角度都很小的時候，分類的特別細，
表示這裡的動作比較複雜。 
 
 
圖 34  追球行為的分類結果 
 
圖 35 則顯示以避牆行為的兩個輸入資料做分類的結
果，大致上是距離牆壁近的話，比較需要考量動作。    
        
             圖 35 避牆行為的分類結果 
圖 36 則顯示以跑位行為的兩個輸入資料做分類的結
果，大部分的分類都集中在機器人面對目標點的角度上。 
 
 
圖 36  跑位行為的分類結果 
  (2) 示範動作的設計 
    示範動作是讓機器人能夠更快地達成特定的目的，
藉由設計者的操控來行動，讓機器人在對環境還沒有一
定的認識之前，可以省去大量盲目搜索的時間。利用這
套示範動作，不論動作的結果是成功或失敗，總共操控
機器人 20 次，讓機器人在這 20 次的過程中，學習所碰
到特定情況的回饋值。 
 
  (3) 以 FBQL 融合的學習結果 
    經過了 20 次示範的動作，之後機器人便完全靠自我
學習所累積到的評估值 Q 值以及權重值 W 值來自主控
制了。如圖 37 所示，而回饋值 r 值則會依據各個行為出
力比例的不同，來做評估值 Q 值的調整，讓下次在做決
策的時候，能夠有更可信賴的參考經驗。 
ApproachApproach_angleRobot_angle
GoBallBall_distanceBall_angle
Robot_X
Robot_Y
w2
w1
w3
OffWallWall_distanceWall_angle
 
圖 37 以融合行為學習演算法(FBQL)融合行為的架構圖 
以融合行為學習演算法(FBQL)來做實驗，總失敗次數只
有 4 次，最後累積到 50 次的成功結果，總成功率為
92.59%。圖 38 是以移動平均法來計算成功率，移動期
數為 17 期，計算每 17 次裡面成功的機率值。成功率最
低在 82.35%，最高可到達 100%，可見經過融合學習演
算法(FBQL)的學習之後，成功率有顯著的提升，而且逐
漸趨於穩定。 
 
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45
實驗次數
成
功
率
 
圖 38 以融合行為學習演算法(FBQL)融合行為的成功率 
D. 實驗結果討論 
    將兩個實驗結果做比較。首先以成功比率來做比
 14
Stanford University, pp.159-174, 1997. 
[8]  W. D. Smart, “Making Reinforcement Learning Work 
on Real Robots,” PhD thesis, Brown University, 2002. 
[9]  M. Bowling and M. Velso, “Rational and Convergent 
Learning in Stochastic Games,” In Proceedings of the 
Seventeenth International Joint Conference on Artificial 
Intelligence, Seattle, WA, pp.121-1026, 2001. 
[10]  J. P. Brusey, “Learning Behaviors for Robor Soccer,” 
RMIT University, Melbourne, Victoria, Australia, 2002. 
[11]  R. Sun and G. Zhao, “An Efficient Multi-Agent 
Q-learning Method Based on Observing the Adversary 
Agent State Change,” IEEE International Conference on 
Systems, Man, and Cybernetics, pp.4169-4174, 2006. 
[12]  R. C. Arkin, “Moving Up the Food Chain: 
Motivation and Emotion in Behavior-Based Robots,” In: J. 
Fellous, M. Arbib, eds. , Who Needs Emotions: The Brain 
Meets the Robot, Oxford University Press, New York, 
2005. 
[13]  Z. Liu, M. H., Jr. Ang, and W. K. G. Seah, 
“Reinforcement Learning of Cooperative Behaviors for 
Multi-Robot Tracking of Multiple Moving Targets,” 
IEEE/RSJ International Conference on Intelligent Robots 
and Systems, pp.1289-1294, 2005. 
[14]  C. J. C. H. Watkins, “Learning from Delayed 
Rewards,” PhD thesis, Cambridge University, 1989. 
[15]  S. R. Safavian and D. Landgrebe, “A Survey of 
Decision Tree Classifier Methodology,” IEEE Transactions 
on Systems, Man, and Cybernetics, vol.21, pp.660-674, 
1990. 
[16]  S. K. Murthy, “Automatic Construction of Decision 
Trees form Data: A Multi-Disciplinary Survey,” Data 
Mining and Knowledge Discovery, vol.2, pp.345-389, 
1998. 
[17]  L. Hyafil and R. L. Rivest, “Constructing Optimal 
Binary Decision Trees is NP-Complete,” Information 
Processing Letter, vol.5, 1976. 
[18]  M. Dong and R. Kothari, “Look-Ahead Based Fuzzy 
Decision Tree Induction,” IEEE Trans. Fuzzy Syst, vol.9, 
2001. 
[19]  S. K. Murthy and S. Salzberg, “Lookahead and 
Pathology in Decision Tree Induction,” in Proceedings of 
the Seventeenth International Joint Conference on Artificial 
Intelligence, San Mateo, CA, pp.1025-1031, 1995. 
 
