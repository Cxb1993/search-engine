 1
行政院國家科學委員會專題研究計畫成果報告 
遠距居家照護系統設計－子計畫三：醫療照護診斷系統(II) 
計畫編號：NSC 94－2218－E－003－001 
執行期限：94 年 10 月 01 日至 95 年 09 月 30 日 
主 持 人：洪欽銘  教授 
參與人員：黃昭諺、張淑芳、黃加孟、陳信嘉、林益民、張家成、方冠勛 
 
摘    要 
針對模糊醫學診斷規則之可解釋性，本計畫今年度發展一系統化之規則簡化策略以達成此一目標。本
簡化策略係以模糊集之集合論式相似度測量技術為基礎，分別藉由模糊集、前件特徵以及模糊規則數目
之三步驟簡化程序，逐步提昇系統之模糊規則可讀性。相較於其他方法而言，本計畫發展之相似度量測
技術之特點在於其將模糊集之重疊部份視為一凸多邊形區域，只要吾人得知其頂點座標後即可利用常見
之面積行列式公式輕易算出此重疊區域面積，並進而算出模糊集之相似度。因此，本方法最大優點是其
可計算一般非對稱型模糊集之相似度，且其概念簡單、毋須針對各種重疊情形推導個別之面積算法。此
外，對於模糊醫學診斷系統之分類能力以及模糊規則簡化方法之有效性，本計畫採用一目前常見之標準
測試資料庫來加以驗證之。 
關鍵詞：模糊醫療診斷系統、可解釋性、模糊規則簡化、集合論式相似度測量、凸多邊形、行列式面積
公式、非對稱型模糊集 
 
Abstract 
In this year, we have developed a strategy to simplify the fuzzy rules of fuzzy medical diagnosis system in 
order to enhance the interpretability of fuzzy rules. Based on the set-theoretic similarity measurement derived 
by the well-known determinant formula of area, the strategy consists of three steps: 1) fuzzy set mergence, 2) 
antecedent feature pruning, and 3) rule number simplification. According to these steps, we simplify the fuzzy 
medical rule base step by step, and meanwhile enhance its interpretability and readability. It is worth noting that 
the set-theoretic similarity measurement proposed in the project is different from the conventional ones since it 
unifies the procedure for computing intersection area between two fuzzy sets. Moreover, we employ a 
bench-mark classification database to verify the effectiveness of the medical diagnosis system and the process 
of fuzzy rule simplification. 
Keywords: fuzzy medical diagnosis system, interpretability, fuzzy rule simplification, set-theoretic similarity 
measurement, convex polygon, determinant formula of area, asymmetric fuzzy set.  
 
 
一、前    言(Introduction) 
本計畫於前一年之醫療照護診斷系統(I)中已完成模糊-類神經醫學診斷系統之一般設計工
作，本年度之研究重點在於改善模糊診斷系統之規則可解釋性(interpretability)，使得所發展之
模糊診斷規則能更加簡單清晰，以方便使用者能清楚地了解診斷機制並快速地進行診斷結果之
確認。根據 Mikut 等人整理的定義[1]，為確保模糊規則之可解釋性一般須符合四項條件：1) 規
則數須愈少愈好；2) 規則之前件部特徵愈少愈好；3) 語意名詞(即模糊集)須愈少愈好；4) 推
論方法須愈簡單愈好。本計畫第一年的研究已發展出以簡單之 Mamdani-type 模糊規則為主體
之醫學診斷系統，且在推論過程中僅須使用最簡單之取大或取小運算，因此已確立條件 4 所要
求之推論方法單純與簡易性。延續第一年的研究成果，本計畫今年之執行重點係著重於滿足其
他 3 條件之要求，以確實達成使模糊規則具備高度可解釋性之理想目標。為達成此一目的，本
計畫將改良目前現有之集合論式(set-theoretic)相似度量測(similarity measurement)方法，使其適
 3
Rule j:  IF 1x  is 
( )
1
jA  and 2x  is 
( )
2
jA  and…and Nx  is 
( )j
NA                   (1) 
THEN the input feature pattern [ ]1 2, ,..., Nx x x  belongs to class i. 
其中 IF 部份稱為規則之前件部 (antecedent)且 THEN 部份稱為規則之後件或結論部
(consequent)。在醫學診斷領域中， kx ， 1,2,...,k N= 為病患之各式記錄(patient records)，其可
以是：性別、年齡、身高、體重、體溫、血壓、脈搏、體質指數(body mass index, BMI)…等基
本資料，也可以是其他臨床參數(clinical parameters)、實驗室參數(laboratory parameters)或病患
主訴(chief complaint)等相關資訊；類別標記 i則表示系統對病人所患疾病類型之診斷(diagnosis)
或預後(prognosis)。此類由 Mamdani-type 模糊規則所描述的模型可用四層式模糊-類神經網路
(fuzzy-neuro network, FNN)架構表示(如圖一所示)，其透過如式(2)所示之截斷式模糊交、聯集
算子(truncation operators)進行模糊推論以及式(3)所示之 winner-take-all 解模糊方式以獲得明確
診斷結果。 
{ }( ) ( )
1,2,...,1
min
N
j j
j k kk Nk
ar μ μ=== =T  ； { }( ) ( )1,2,...,1 max
i
i
R
i i
i j jj Rj
ac ar ar
==
= =⊥     (2) 
{ }
{ }
1,2,...,
arg max i
i C
i ac∗
∈
=           (3) 
其中 jar 代表規則激發強度； iac 代表類別激發強度；而 i∗ 則表示模糊-類神經醫療診斷系統所
輸出之診斷結果。值得注意的是，本計畫所發展之醫療診斷系統本質上可視為一分類器
(classifier)，因此本計畫所發展之模糊規則建立技術實際上可適用於其他領域之分類問題上。 
MMF
μ1Mμ 2Mμ11μ 11Fμ12μ kkfμ
1x Nxkx
jar1ar Rar
iac Cac
Class 1
1ac
Class i Class C
Rule 1 Rule j Rule R
Input 1 Input NInput k
2ar
Rule 2
 
圖一  四層式模糊-類神經網路架構 
 
 
 5
屬函數參數進行再修正。茲將本計畫目前已發展完成之最速下降型(steepest descent)參數學習
演算法簡列如表一所示且其演算法流程如圖三所示。 
表一 歸屬函數參數學習(精鍊)演算法 
Parameter Learning (Refining) Algorithm 
1. Feed a new pattern ( , )=p x d  into fuzzy medical diagnosis system (FMDS). 
2. For each rule neuron, save the index associated to the winner fuzzy set neuron 
kkf
μ  that originated the maximum in the fuzzification layer. 
3. For each class neuron, save the index associated to the winner rule neuron 
jar  that originated the minimum in the rule layer. 
4. Compute the output class vector c by means of Eq. (2), and then compute the 
classification error vector = −e d c . 
5. Considering the winner rule neuron for each class neuron, find out its winner 
fuzzy set neuron. Compute  
( )
( )2 ,  if  [ , )
       0 ,            otherwise
k
k k
k
k k
k
k kf
k kf kfkf
kf kf
kf
x b
x a b
b a
a
μ
⎧ −⎪ ∈∂ ⎪ −= ⎨∂ ⎪⎪⎩
 ; 
( )
( )
( )
( )
2
2
,  if  [ , )
,  if  [ , ]
k
k k
k kk
k k
k k
k k
kf k
k kf kf
kf kfkf
kf kf k
k kf kf
kf kf
a x
x a b
b a
b c x
x b c
c b
μ
⎧ −⎪ ∈⎪ −∂ ⎪= ⎨∂ + −⎪ ∈⎪ −⎪⎩
 
and  
( )
( )2 ,  if  [ , ]
       0 ,            otherwise
k
k k
k
k k
k
k kf
k kf kfkf
kf kf
kf
x b
x b c
c b
c
μ
⎧ −⎪ ∈∂ ⎪ −= ⎨∂ ⎪⎪⎩
 
6. Define the parameter vector , ,
k k k k
T
kf kf kf kfa b c⎡ ⎤= ⎣ ⎦θ , and obtain 
,   if  
      0,            otherwise
k
k
kk
kf
i i j kf
kfkf
e ac ar
μη μ∂⎧ = =⎪ ∂Δ = ⎨⎪⎩
θθ  
7 Update the parameter vector of 
kkf
μ  by 
k k kkf kf kf
= +Δθ θ θ , and go to step 1. 
 
 
四、模糊集相似度測量演算法(Similarity Measurement) 
為能確實反應模糊集之間的相容性(compatibility)或相等性(equality)，本計畫將發展基於集
合論(set-theoretic)之相似度測量方法。假設現有 A、B 兩模糊集位於 U 論域中且 ( )A xμ 與 ( )B xμ
分別為其歸屬函數，根據 Setnes 等人的定義[1]，一理想之集合論式相似度測量方法應能滿足
下列四條件： 
1) 若模糊集 A 與 B 之間無重疊情形，則兩者相似度為 0，即 
( , ) 0S A B =   if and only if  ( ) ( ) 0A Bx xμ μ⋅ = , x U∀ ∈            (6) 
2) 若模糊集 A 與 B 之間有重疊區域，則兩者相似度應大於 0，即 
( , ) 0S A B >   if and only if  x U∃ ∈  such that ( ) ( ) 0A Bx xμ μ⋅ >        (7) 
 7
於兩歸屬函數各邊所成直線之交點上，藉由簡單之求直線交點公式，吾人便可找出此多邊形之
全部頂點。至於求解交點順序，本計畫係採由左至右之求解順序，例如：吾人欲計算 A、B 兩
模糊集之交集面積，則先計算歸屬函數 A 之左邊與歸屬函數 B 之左右兩邊交點，其次再計算
歸屬函數 A 之右邊與歸屬函數 B 之左右兩邊交點。由上述方式可知，兩模糊集若皆以三角型
歸屬函數描述的話，先扣除在 UoD 軸之頂點不算，則最多可得到四個交點。然而由圖五可看
出，並非所有直線交點都是重疊區域之頂點，亦即當交點之 y 座標大於 1 或小於 0 時，吾人稱
此類交點為無效交點(如空心點所示)；反之，當交點之 y 座標介於 0 與 1 之間時，吾人稱此類
交點為有效交點(如實心點所示)，而重疊區域之凸多邊形頂點即是有效交點。 
Aμ Bμ
Aa
Ba Ac
Bc UoD
1
0
 
圖五 直線交點與重疊區域頂點之關係 
接著考慮落於 UoD 軸之凸多邊形頂點，由圖四可看出，此類頂點必定是兩歸屬函數四個上/
下界點當中的二個，且必為第二與第三上/下界點，例如：圖一(b)之點 ( ,0)Ba 與 ( ,0)Ac ；圖一(c)
之點 ( ,0)Aa 與 ( ,0)Ac ；圖一(d)之點 ( ,0)Aa 與 ( ,0)Bc 。 
 
B. 三角型歸屬函數之交集面積計算 
令不在 UoD 軸之凸多邊形頂點座標為 ( , )i ix y ， 2,3,..., 1i N= + ，且令落於 UoD 軸之凸多邊
形左邊頂點座標為 1 1( , )x y 以及右邊頂點為 2 2( , )N Nx y+ + ，則依行列式面積計算公式，可得兩模糊
集之交集(重疊)區域 I 之面積為 
2 3 1 2 2 11 2
2 3 1 2 2 11 2
1 abs
2
N N N
N N N
x x x x x xx x
I
y y y y y yy y
+ + +
+ + +
⎛ ⎞= ⋅ + + + +⎜ ⎟⎝ ⎠
L           (11) 
其中 abs(·)表示取絕對值運算，其目的係為使依順時針順序排列之多邊形頂點 ( , )i ix y ，
1,2,...,i n= 面積為正值。此外，由於交集區域之凸多邊形位於 UoD 軸上之頂點期 y 座標必為 0，
即 1 2 0Ny y += = ，故 
2 1 2 1
2 1
0
0 0
N N
N
x x x x
y y
+ +
+
= =                            (12) 
因此可將面積計算公式簡化為 
 9
五、模糊規則之簡化策略(Fuzzy Rule Simplification) 
本計畫所提出之模糊規則簡化步驟包含三大方面：模糊集合數目簡化、前提特徵數目簡化
與模糊規則數目簡化，值得注意的是，後兩者之簡化與否常視前者之簡化結果而定。茲將三者
之進行方式說明如下： 
 
A. 模糊集合之數目簡化 
模糊集之合併工作進行與否係取決於論域 U 中的所有模糊集兩兩成對之相似度高低而定，
當兩模糊集相似度大於 0.5 時，則執行兩者之合併工作以產生一新模糊集。此處定義新模糊集
之 a、b、c 參數即為兩模糊集個別 a、b、c 參數之平均值。例如：若現有模糊集 A 與 B，且
( , ) 0.5S A B > ，則新模糊集 C 之參數為 
2
A B
C
a aa += ，  
2
A B
C
b bb += ，  
2
A B
C
c cc +=  
待新模糊集 C 產生後，接著檢查規則表中所有模糊規則之前件部(antecedent)模糊集，只要規
則中出現模糊集 A 與 B 者全都皆以模糊集 C 取代。 
 
B. 前件特徵之數目簡化 
各規則前件部之特徵(feature)簡化基本上牽涉到模糊集之刪除，而模糊集之刪除工作進行與
否係取決於各模糊集與其所在論域 U 之相似度高低而定，當某模糊集與其所在論域之相似度
大於 0.7 時，即 ( , ) 0.7S A U > ，即表示使用到模糊集 A 之子前提(subpremise)為不重要子前提，
故可將模糊集 A 連同其所在子前提一併由規則中刪除。換言之，當吾人確定模糊集 A 該刪除
時，則檢查所有規則之前件部且刪除所有有模糊集 A 出現之子前提。 
值得注意的是，此處稱模糊集 A 為「不重要」之主要原因是由於吾人採用 min{·}算子進行
模糊 AND 運算，因模糊集 A 所在特徵其歸屬度在取 min 後幾乎不會被當成規則激發強度(因 A
近似 U，不管該特徵輸入值為何，其恆有趨近於 1 之歸屬度)，此意味在模糊推論過程中吾人
可忽略模糊集 A 與其所在子前提特徵之影響，因此可將其連同其所在特徵一併自規則中刪除。 
 
C. 模糊規則之數目簡化 
關於模糊規則數目之簡化與否端視模糊集合併/刪除結果或前提特徵簡化結果而定，因前兩
者之進行結果常會使不同規則最後擁有相同前提部，若這些具有相同前提部之規則其後件部亦
相同的話，則吾人將刪除冗餘規則，僅保留其中一條規則即可。反之，若這些具有相同前提部
之規則其後件部卻不盡相同的話，則牽涉到規則矛盾(contradiction)問題，此處的解決方式是統
計哪一條規則可分對「最多」樣本，吾人即以該規則之後件部為主並刪除其他規則。 
 
圖六所示為本計畫提出之模糊規則簡化之演算法流程，其中須注意的是在合併(merging)/
刪除(pruning)模糊集之後吾人必須接著更改對應之模糊規則表內容才可再進行下一步驟。此
外，當吾人簡化模糊規則後一般須再調整一次模糊規則參數以克服因規則簡化而造成分類效果
降低之缺點，因此，如圖七所示，吾人須針對簡化後之模糊規則再進行一次參數精鍊處理，以
獲得最終具高可解釋性分類精確度之模糊醫學診斷分類器系統。 
 11
六、模擬與實驗結果(Simulations and Experiments) 
為驗證上述之模糊規則簡化策略是否能確實提昇醫學診斷規則之可解釋性以及其是否能針
對分類問題提供可靠之分類規則，本計畫利用 UCI 網頁提供之 Wine 資料庫[10]進行測試。茲
將其測試內容與結果分述如下： 
Wine 分類資料庫總共包含 178 筆葡萄酒樣本之化學分析數據，這些葡萄酒皆由義大利某一
地區內 3 種葡萄品種釀製而成，每一品種樣本數分別為 59、71 與 48。每筆樣本包含兩類資料：
第一類是以連續資料型態呈現之 13 項特徵(feature/attribute)；第二類是該樣本所屬品種類別資
料。此 13 項特徵分別為：alcohol(x1)、malic acid(x2)、ash(x3)、alkalinity of ash(x4)、magnesium(x5)、
total phenols(x6)、flavanoids(x7)、nonflavanoid phenols(x8)、proanthocyanins(x9)、color 
intensity(x10)、hue(x11)、OD280/OD315(x12)與 praline(x13)。 
為方便 K-means 聚類，本計畫將所有特徵量測值予以正規化至[0,1]區間內，接著在樣本空
間中隨機選擇 3 點初始群聚中心，經聚類演算法以及群聚投影處理後，吾人可得圖八所示之 3
條初始模糊規則前件部模糊集。若以 K-means 聚類完成之分割矩陣(partition martix)進行 Wine
分類，則其精確度約為 92.3%，即共有 13 筆樣本分類錯誤。緊接著繼續以精鍊演算法調整歸
屬函數參數，經 100 次迭代後可得圖九之模糊規則前件部模糊集，此時可得精確度 100%之最
佳分類效果。然而由圖九亦可看出，某些特徵論域上的模糊集因重疊區域過大而具有極高的相
似度，此情形不利吾人對所得模糊集賦予適當之語意名詞(linguistic term)，亦即此時所得之 3
條模糊規則之可解釋性不高，因此適當之規則簡化在此是必要的。圖十所示為吾人將高相似度
(S(A,B)>0.5)模糊集合併後之規則前件部模糊集，可看出共 7 項特徵(x2、x3、x4、x5、x6、x8 與
x9)其論域上之模糊集已被吾人大幅簡化成僅剩一個，此時之分類精確度為 90.45%。由於此 7
項特徵之模糊集與其論域間之相似度過高(S(A,U)>0.7)，因此接著進行模糊集與規徵特徵項之
刪除動作，圖十一所示為完成刪除與再精鍊後之前件部模糊集，可看出每個模糊集重疊現象適
中有利於語意名詞的標註，且此時之分類精確度仍可高達 99.44%，即僅有一樣本分類錯誤而
已。 
表二與表三所示為最後所得之模糊規則表與其前件部模糊集歸屬函數參數，其中 S 代表語
意項小(small)，M 代表語意項中(medium)，L 代表語意項大(large)，可清楚看出原本 13 項前件
特徵已被簡化為只需 6 項 (x1、x7、x10、x11、x12、x13)，且原本各特徵共 39 個語意名詞(模糊集)
亦被簡化成僅需 14 個，而仍能保持相當高之分類精確度(99.44%)。由實驗結果可看出，此分
類效果相較於其他現有方法而言[3],[5],[11]-[13]，不管在規則數目、語意名詞數目、前件特徵
數目以及分類精確度等各方面均可有效與之比擬。 
 13
0 0.5 1
0
0.5
1
Alcohol, x1              
0 0.5 1
0
0.5
1
Malic Acid, x2           
0 0.5 1
0
0.5
1
Ash, x3                  
0 0.5 1
0
0.5
1
Alcalinity of Ash, x4    
0 0.5 1
0
0.5
1
Magnesium, x5            
0 0.5 1
0
0.5
1
Total Penols, x6         
0 0.5 1
0
0.5
1
Flavanoids, x7           
0 0.5 1
0
0.5
1
NonFlavanoids Phenols, x8
0 0.5 1
0
0.5
1
Proanthocyaninsm, x9     
0 0.5 1
0
0.5
1
Color Intensity, x10    
0 0.5 1
0
0.5
1
Hue, x11                
0 0.5 1
0
0.5
1
OD280/OD315, x12        
0 0.5 1
0
0.5
1
Praline, x13            
 
0 0.5 1
0
0.5
1
Alcohol, x1              
0 0.5 1
0
0.5
1
Malic Acid, x2           
0 0.5 1
0
0.5
1
Ash, x3                  
0 0.5 1
0
0.5
1
Alcalinity of Ash, x4    
0 0.5 1
0
0.5
1
Magnesium, x5            
0 0.5 1
0
0.5
1
Total Penols, x6         
0 0.5 1
0
0.5
1
Flavanoids, x7           
0 0.5 1
0
0.5
1
NonFlavanoids Phenols, x8
0 0.5 1
0
0.5
1
Proanthocyaninsm, x9     
0 0.5 1
0
0.5
1
Color Intensity, x10    
0 0.5 1
0
0.5
1
Hue, x11                
0 0.5 1
0
0.5
1
OD280/OD315, x12        
0 0.5 1
0
0.5
1
Praline, x13            
 
圖十 Wine 分類器經模糊集合併簡化後之前件部模糊集 
Accuracy: 90.45% 
圖十一 Wine分類器經模糊集刪除簡化與參數再精鍊
後之前件部模糊集 Accuracy: 99.44% 
 15
performance,” IEEE Trans. Fuzzy Sys. Vol. 8, no. 5, pp. 509-522, 2000. 
[4] H. Roubos and M. Setnes, “Compact and transparent fuzzy models and classifiers through 
iterative complexity reduction,” IEEE Trans. Fuzzy Sys. vol. 9, no. 4, pp. 516-524, 2001. 
[5] J. A. Roubos, M. Setnes, and J. Abonyi, “Learning fuzzy classification rules from labeled 
data,” Inform. Sciences, vol. 150, pp. 77-93, 2003. 
[6] M. Y. Chen and D. A. Linkens, “Rule-base self-generation and simplification for data-driven 
fuzzy models,” Fuzzy Sets and Sys., vol. 142, pp. 243-265, 2004. 
[7] C. T. Chao, Y. J. Chen, and C. C. Teng, “Simplification of fuzzy-neural systems using 
similarity analysis,” IEEE Trans. Sys. Man, Cyber. Part B, vol. 26, no. 2, pp. 344-354, 1996. 
[8] C. T. Lin, “A neural fuzzy control system with structure and parameter learning,” Fuzzy Sets 
and Sys., vol. 70, pp. 183-212, 1995. 
[9] Y. Jin, “Fuzzy modeling of high-dimensional systems: complexity reduction and 
interpretability improvement,” IEEE Trans. Fuzzy Sys. vol. 8, no. 2, pp. 212-221, 2000. 
[10] UCI machine learning repository content Summary (http://www.ics.uci.edu/~mlearn/MLSum 
mary.html) 
[11] X. Chang and J. H. Lilly, “evolutionary design of a fuzzy classifier from data,” IEEE Trans. 
Sys., Man, Cyber.: Part B, vol. 34, no. 4, pp. 1894-1906, 2004. 
[12] J. S. Wang and G. C. S. Lee, “Self-adaptive neuro-fuzzy inference system for classification 
application,” IEEE Trans. Fuzy Sys., vol. 10, pp. 790-802, 2002. 
[13] H. Ishibuchi, T. Nakashima, and T. Murata, “Three-objective genetic-based machine learning 
for linguistic rule extraction,” Inf. Sci., vol. 136, pp. 109-133, 2001. 
 
 
 
WCCI 國際會議報告書 
一、參加會議經過 
此次國際會議係委託台北市柏達旅行社代為安排一切事宜，於七月十七日下
午 14：05 於中正國際機場搭乘華航 CI-032 班機，飛行時間約十小時三十分，
於當地時間 09：30 抵達加拿大溫哥華後，即搭車至 Travelodge Vancouver 
Airport 飯店，稍事休息後，接下來即進行為期將近一週會議行程；大會事前
準備非常嚴謹，提供每位與會者一本精緻論文摘要集與論文全文光碟片。為
期一週的議程中，大會密集安排了許多場次。會議結束後，於七月二十四日
下午搭乘華航 CI-031 班機至台北，此行收穫良多，一切都很順利，圓滿地完
成整個會議之相關行程。 
二、與會心得 
1. 國際間人工智慧領域之研究發展 
此屆WCII國際研討會包含Neural network(IJCNN)、Fuzzy system(FUZZ)、
Evolutionary computation(CEC)等三個重要研究領域。其中Neural network與
Fuzzy system雖然已是相當成熟之研究，隨著更多不同的應用層面，包括醫學
診斷、財經預測、影像處理等都展現其多元的發展性；由於各國不斷致力的投
入研究與發展，在思維上了激起更多的火花，其成果令人激賞 。參與國際會議
之收穫，除了增進本身相關的知識，更大的收穫是學習到其他國家研究者對研
究的心態及觀摩到國際研討會所給予的新視界。 
每日會議安排如下： 
ROOM Junior Ballroom  Junior Ballroom AB Pavilion Ballroom  Pavilion Ballroom 
8:30am-11:00am 
Principles and Applications 
of Neural Networks  
Fuzzy Sets and Pattern Recognition 
 
Evolutionary Computation: 
A Unified Approach  
 
Towards an 
Autonomous 
Computationally 
Intelligent System 
12:30pm-3:00pm 
Predictive Learning and 
Philosophy of Science  
Evolvable Neural-, Fuzzy-, and Hybrid 
Systems: Methods and Applications 
Evolutionary Robotics  
 
Biologically 
Motivated Mental 
Architectures  
四、攜帶回資料 
此次參加會議，大會準備了大會議程表、論文集及許多明年相關國際研討會的
Call for Paper 資料，個人將其攜回台灣師範大學應電所作為同事及研究生明年
參加會議之參考： 
1. 一本大會編輯之摘要論文集與全文光碟片。 
2. 國際會議之邀稿資料(已公佈於台師大應電所公布欄)： 
（1） 2007 年 5th Atlantic Web Intelligence Conference 
（2） 2007 年 4th International Symposium on Neural Networks 
（3） 2007 年 IEEE Congress on Evolutionary Computation 
  
A. The Learning Structure of Neuro-Fuzzy Network
Based on the study proposed in [8], the learning
architecture of the employed neuro-fuzzy network can be
distinguished as four layers and shown as Fig. 1. The first
layer is the input layer which consists of input neurons and
each neuron corresponds to an input feature. The second
layer is the fuzzification layer which consists of fuzzy set
neurons and each neuron corresponds to a linguistic term,
such as low, moderate, and high, and so on. The third layer is
the rule layer which consists of rule neurons and each neuron
corresponds to a fuzzy rule. The fourth layer is the class layer
which consists of class neurons and each neuron corresponds
to a class label. To explain the detailed operation procedures
of the neuro-fuzzy classifier, the notations used in Fig. 1 are
explained as follows: kx is the
thk input feature, kl
denotes the thl membership function of the thk input feature,
Rule j is the thj fuzzy rule, and Class i is the thi output
class. In addition, jar denotes the activation strength of the
thj neuron in the rule layer, and iac denotes the activation
strength of the thi neuron in the class layer. Suppose that the
triangular membership functions kl with three parameters
are adopted herein for the employed neuro-fuzzy network,
and formulated as follows:
, if [ , )
( ) , if [ , ]
0 , otherwise
k kl
k kl kl
kl kl
kl k
kl k k kl kl
kl kl
x a
x a b
b a
c x
x x b c
c b

  
   



(1)
where
kla , klb , and klc represent respectively the left, center,
and right vertices of the thl membership function associated
with the thk input feature. For simplifying the notations, a
compact notation  Tklkl,klkl c,ba , which is the parameter
vector of kl , is adopted herein.
1ac iac cac
iClass cClass
1ar 2ar jar rar
1Rule 2Rule jRule rRule
11 12 m1 kl 1n 2n nm
1Input kInput nInput
1x kx nx
1Class
Fig. 1. Architecture of four-layer neuro-fuzzy network
B. Generation of Fuzzy If-Then Rules
In this study, the employed neuro-fuzzy classifier follows
six steps to generate fuzzy rules for mining the medical
diagnosis rule, and summarized as follows:
Step 1. Computing the activation strength of each fuzzy
neuron in the rule layer by the minimum operator,
and formulated as follows:
 )(
,...,2,1
min jknkj uar 
(2)
where jar is the activation strength of the
antecedent part of the thj rule neuron, )j(ku
represents the membership degree of the thj rule
fired by the thk input feature, which can be
computed as  kmkkjk uuuu ,...,,max 21)(  , and m is
the number of the defined linguistic terms of each
input feature.
Step 2. Computing the activation strength of each class
neuron in the class layer by the maximum operator,
and formulated as follows:
 )(
,...,2,1
max ijzji arac i
 (3)
where iac is the activation strength of the
consequent part of the thi class neuron, )i(
jar
represents the activation strength of the thi class
neuron fired by the thj rule which is the set member
of the activation strengths of the rule neurons
represented as  rjij arararar ,...,,...,1)(  , and iz is
the number of the fired fuzzy rules of the thi class
neuron, and rz i  .
Step 3. Computing the corresponding class register values
by respectively calculating the summation of the
activation strengths fired by all training patterns for
each fuzzy rule, and formulated as follows:



y
p
p
j
ii Xacjt
1
)( )()( (4)
where rjci ,...,2,1,,...,2,1  and )( jti is the
class register value of the thi class for the thj rule,
)X(ac p
)j(
i is the activation strength of the
thi
class neuron fired by the thp training pattern for the
thj rule, and y is the total number of the training
patterns.
Step 4. Finding the class label with the largest class register
value for each fuzzy rule, and formulated as follows:
)}({maxarg)(
,...,2,1
jtj i
ci
 (5)
where )( j represents the new class label with
largest class register value for the thj rule.
Step 5. Evaluating the performance index for each fuzzy
rule, and formulated as follows:
1,2,..., ,
( ) ( ) ( )i
i c i
p j t j t j
 
   (6)
736
IV. UNCONSTRAINED PARAMETER LEARNING FOR THE
EMPLOYED NEURO-FUZZY NETWORK
A gradient-based learning algorithm which can adjust the
parameters of fuzzy sets used in the employed neuro-fuzzy
network based on the given training patterns is presented
herein. In this work, the idea of back-propagation is adopted
to derive the supervised learning algorithm. The training
rules for the employed neuro-fuzzy classifier is derived from
defining a cost function and minimize the cost function as
follows:
eeecdE T2
1
2
1
2
1 2  (12)
where the vector  1 2, , ..., Tcac ac acc is the output class of
the classifier, the vector  1 2, ,..., ce e ee , is viewed as the
classification error.
The goal of the learning process is to adjust the parameters
klθ of the neuro-fuzzy classifier to minimize the cost function
E, and the optimized procedure is processed iteratively by the
steepest descent method, and formulated as follows:
( )
( 1) ( )
( )kl kl kl
E t
t t
t
  

θ θ
θ
(13)
where t denotes discrete time, i.e. the time step of the iterative
learning process, the parameter is a learning rate.
The updating value of the parameter
klθ is defined as
follows:
( )
( )
( )kl kl
E t
t
t
 

θ
θ
(14)
Typically, the derivative of the cost function E with respect to
klθ is calculated by the chain rule of calculus, and calculated
as follows:
T
kl kl kl kl
T
ji kl
i j kl kl
E E E E
a b c
aracE
ac ar


         
                   
θ
e
e θ
(15)
Differentiating both sides of Eq. (12) with respect to e, we
have
E 

e
e
(16)
Since  e d c , it leads to
i
iac
 

e
e (17)
where
ie represents a basis column vector whose the
thi
element is 1 and the others are 0’s. To consider the
discontinuity of intersection operators, we must use some
tricks while calculating the derivative of maximum operator.
In this case, we define the derivative of
iac with respect to
jar as
1, if
0, otherwise
i ji
j
ac arac
ar
  
. (18)
Thus, the derivative regarding the winner neuron is equal to 1,
and is equal to 0 for the other non-winner neurons. Similarly,
the same trick used in Eq. (18) is performed for minimum
operator while calculating the derivative of
jar with respect
to
kl , as follows
1, if
0, otherwise
j klj
kl
arar 

  
. (19)
Considering Eq. (1), we get
T
kl kl kl kl
kl kl kl kla b c
            θ
(20)
where each element in Eq. (20) can be respectively derived as
follows:
 
 2
, if [ , )
0 , otherwise
kl k
k kl kl
kl
kl kl
kl
b x
x a b
b a
a

    

(21 a)
 
 
 
 
2
2
, if [ , )
, if [ , ]
kl k
k kl kl
kl klkl
kl kl k
k kl kl
kl kl
a x
x a b
b a
b c x
x b c
c b

        
(21 b)
 
 2 , if [ , ]
0 , otherwise
k kl
k kl kl
kl
kl kl
kl
x b
x b c
c b
c

    

(21 c)
Therefore, to integrate Eqs. (15) to (19) yields
, if
0, otherwise
kl
i i j kl
kl
kl
e ac arE
      
θ
θ
. (22)
According to Eqs. (14) and (22), we have
, if
0, otherwise
kl
i i j kl
klkl
kl
e ac arE
 

       
θθ
θ
. (23)
Here, we can conclude that only a parameter vector
associated with indexed membership function should be
adjusted to reduce nonzero classification error ie in each
time step t.
V. FEATURE REDUCTION BY GREY RELATIONAL
ANALYSIS
Feature reduction aims to filter out the irrelevant factors
with the medical diagnosis from the medical data to promote
the efficiency of medical diagnosis. Generally, doctors could
not make good use of a decision support system if it has too
many considered rules and features, i.e. a qualified classifier
in medical domain must have the simplest and deliberate
features [15]. Professor Deng Julong proposed the grey
relational analysis (GRA) which can rank the relational
degree for numerous sub-factors with the main factor
according to the grey relational grade [13]. The importance
of single data sequence could be explored by the grey
738
terms of initially determining the fuzzy membership
functions, adaptively tuning the fuzzy membership functions
initially determined by the refined K-means algorithm, and
simplifying the fuzzy rules for promoting the inference
performance and interpretable semantics of medical
diagnosis rules.
CT
2 4 8 100
0.5
1
small
large
6
UCSZ
2 4 8 100
0.5
1
small
large
6
UCSP
2 4 8 100
0.5
1
small
large
6
MA
2 4 8 100
0.5
1
small
large
6
SECS
2 4 8 100
0.5
1
small
large
6
BN
2 4 8 100
0.5
1
small
large
6
BC
2 4 8 100
0.5
1
small
large
6
NN
2 4 8 100
0.5
1
small
large
6
MI
2 4 8 100
0.5
1
small
large
6
Fig. 3. The optimized membership functions for nine input features used in
Wisconsin Breast Cancer (WBC) dataset
VII. CONCLUSION
To employ an excellent neuro-fuzzy network for
developing a decision support system to medical diagnosis
must contain advantages in terms of simple learning structure,
highly interpretable semantics and easy implementation. The
proposed neuro-fuzzy classifier with feature reduction can
satisfy the conditions mentioned-above. In the proposed
scheme, the refined K-means clustering algorithm and
gradient-based learning algorithm is first used to optimize the
fuzzy membership functions for each input feature under only
using few training epochs. Additionally, the grey relational
analysis is applied to filter out some irrelevant features for
simplifying the obtained fuzzy rules, thus promoting the
inference performance and interpretable semantics while
applying for medical diagnosis. The experimental results
confirmed that the proposed scheme can help doctors to
develop an efficient and practicable decision support system
for medical diagnosis.
ACKNOWLEDGEMENTS
The authors would like to thank the National Science
Council of the Republic of China, Taiwan for financially
supporting this research under Contract No. NSC
94-2218-E-003 -002.
REFERENCES
[1] L. A. Zadeh, “Fuzzy sets,” Inform. and Contr., vol. 8, pp. 338-353,
1965.
[2] L. I. Kuncheva and F. Stimann, “Fuzzy diagnosis,” Atrif. Intel. in
Medic., vol. 16, pp. 121-128, 1999.
[3] P. R. Innocent and R. I. John, “Computer aided fuzzy medical 
diagnosis,” Artif. Intell. Med., vol. 162, pp.81-104, 2004.
[4] J. F. F. Yao and J. S. Yao, “Fuzzy decision making for medical 
diagnosis based on fuzzy number and compositional rule of
inference,”Fuzzy Sets and Sys., vol.120, pp.351-366, 2001.
[5] D. R. Cundell, R. S. Silibovsky, R. Sanders, and L. M. Sztandera,
“Generation of an inteligentmedical system, using a real database, to
diagnose bacterial infection in hospitalized patients,” Inter. Jour. of
Med. Inform. vol. 63, pp.31-40, 2001.
[6] D. L. Luigi, G. Antonio, A. Antonio, G. Giuseppe, and M. Franco, “A 
fuzzy-based methodology for the analysis of diabetic neuropathy,” 
Fuzzy Sets and Sys., vol. 129, pp.203-228, 2002.
[7] X. G. Chang and J. H. Lily, “Evolutionary design of a fuzzy classifier 
from data,” IEEE Trans. Sys. Man, Cyber.-B, vol. 34, no. 4, pp.
1894-1906, 2004.
[8] C. T. Lin, “A neural fuzzy control system with structure and parameter
learning,” Fuzzy sets and Sys., vol. 70, pp.183-212, 1995.
[9] R. P. Paiva and A. Dourado, “Interpretability and learning in 
neuro-fuzzy systems,” Fuzzy Sets Sys., vol. 147, pp. 17-38, 2004.
[10] R. P. Li, M. Mukaidono and I.B. Turksen, “A fuzzy neural network for 
patern classification and feature selection,” Fuzzy Sets Sys., vol. 130,
pp. 101-108, 2002
[11] J. S. Wang and C.S. George Lee “Self-Adaptive Neuro-Fuzzy
Inference Systems for Classification Applicaions,” IEEE Trans. Fuzzy
Sys., vol. 10, no. 6, pp. 790-802, 2002
[12] D. Nauck and R. Kruse, “Obtaining interpretable fuzzy classification 
rules from medical data,” Artif. Intell. in Medicine, vol. 16,
pp.149-169, 1999.
[13] J. L. Deng, “Introduction to Grey System Theory,” The Journal of
Grey System, vol. 1, Issue 1, pp. 1-24, 1989.
[14] R. Xu, and D. Wunsch I, “Survey of clustering algorithms,” IEEE
Trans. Neural Net., vol. 16, no. 3, pp. 645-678, 2005.
[15] R. Mikut, J. Jakel and L. Grol, “Interpretability issues in data-based
learning of fuzzy systems,” Fuzzy Sets and Systems, pp.179–197,
2005
[16] N. Lavrac, “Selected techniques for data mining in medicine,” Artif.
Intell. in Medic., vol. 16, pp. 3-23, 1999.
TABLE I
THE CENTER OF EACH CLUSTER DETERMINED BY THE REFINED K-MEANS ALGORITHM
CT UCSZ UCSP MA SECS BN BC NN MIT
Center 1 2.149 0.397 0.508 0.388 1.153 0.594 1.209 2.316 0.094
Center 2 8.351 8.148 8.231 6.916 6.444 8.74 7.462 7.518 3.611
TABLEⅡ
THE TOP FOUR DISCOVERED FUZZY RULES RANKED BY THE ACTIVATION STRENGTH
If CT UCSZ UCSP MA SECS BN BC NN MIT Then
Rule 1： S S S S S S S S S benign
Rule 2： L L L L L L L L L malign
Rule 3： S L L L L L L L S malign
Rule 4： L L L L L L L L S malign
740
