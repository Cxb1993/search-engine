成果報告：
1. Shih-Hsuan Chiu, Chuan-Pin Lu, Dien-Chi Wu, Che-Yen Wen, 2008,
October, “A Histogram based Data-reducing Algorithm for the
Fixed-point Independent Component Analysis”, Patern Recognition 
Letters Vol. 29, pp. 370-376.
2. Shih-Hsuan Chiu, Chuan-Pin Lu, Dien-Chi Wu, Che-Yen Wen, 2008,
June, “Geometric Transformation Based Independent Component 
Analysis for Mixed Image Separation”, Journal of the Chinese Institute of 
Engineers Vol. 31, pp. 497-502.
498 Journal of the Chinese Institute of Engineers, Vol. 31, No. 3 (2008)
we propose another geometric transformation based
method.  Different from Yamaguchi’s method, our
method includes four  s teps:  data  correct ion,
whitening, geometric rotation and slant compensation.
Firstly, correction data points are added for evaluat-
ing the slant angle.  Then the whitening process
decorrelates the mixed signals, and a geometric rota-
tion procedure is used to align the scatter diagram
with orthogonal axes.  Finally, the slant compensa-
tion process reshapes the scatter diagram to a square.
In experiments, several synthetic mixed image pairs
were used to evaluate the performance of this method.
It is shown that the proposed method provides good
results in mixed image separation.
II. PROPOSED METHOD
The basic idea of the ICA model is to find the
mutual orthogonal space for given mixed signals.
Vectors s = [s1(k), ..., sn(k)]T and x = [x1(k), ...,
xn(k)]T, where integer k denotes the kth data (k = 1 ~
m), represent the source signal and observed signal,
respectively.  The observed signal can be described
as a linear mixture of source signal (Hyvärinen and
Oja, 1997)
x = As, (1)
where A is a matrix with unknown elements.  When
applying ICA , one needs the following assumptions
(Yamaguchi et al., 2000): (1) all source signals are
statistically independent of each other; (2) all inde-
pendent components must be non-Gaussian; (3) the
number of the observed linear mixed signals must be
greater than (or equal) the number of independent
signals.  In this paper, we focus on the case n = 2
(two source signals and two observed signals), which
is enough for general image applications.
1. Data Correction
In general cases, mixed signals will not have zero
means.  The scatter distribution is usually non-uni-
form and the scatter diagram will not align with the
orthogonal axes, even after the whitening process and
geometric rotation.  It is the so-called “slant phenom-
enon” (see Fig. 1(c)).  Since the slant phenomenon
will reduce the accuracy of independent component
extraction, we do slant compensation (the last step in
our method) to obviate this phenomenon.  We add
two correction points into the observed signals as the
(m + 1)th and (m + 2)th data points, then the observed
signals become [xi(1), ..., xi(m), xi(m + 1), xi(m + 2)].
The gray scales of the (m + 1)th and (m + 2)th points
are 0 and 255, respectively (i.e. xi(m + 1) = 0 and
xi(m + 2) = 255).
2. Whitening
The whitening process is used to decorrelate the
X2
X2
X1
′
X1*
X2
PA
BLL
PB
C2
C1
* X2*
X1′
θ
(X1, X2)
(a) (b)
(c) (d)
° °
θ
Rθ
BR
~
X1*
~
Fig. 1 (a) Scatter diagram of two observed images, (b) scatter diagram after whitening process, (c) scatter diagram after geometric rotation,
(d) scatter diagram after slant compensation.
500 Journal of the Chinese Institute of Engineers, Vol. 31, No. 3 (2008)
255
125
0 125
(e)
255
255
125
0 125
(f)
255
(a)
(c)
255
125
0 125
(g)
255
255
125
0 125
(h)
255
(b)
(d)
Fig. 3 Experimental results of the image pair B, ‘Landscape’ and ‘Seacoast’: (a) two mixed images, (b) FastICA, (c) Yamaguchi’s method,
(d) proposed method, respectively, (e) ~ (h) scatter diagrams of (a) ~ (d), respectively
clockwise rotation of angle (θL + θR)/2 about the vir-
tual rotation center on X1*
x1
*(k) = x1*(k) – x2*(k)tan(–
θ R + θ L
2 ) ,
x2
*(k) = x2*(k)sec(–
θ R + θ L
2 ) ,  k = 1, 2, ..., m.   (5)
After slant compensation, all points on the scatter
diagram are converted to the estimated source sig-
nals s1*(k) and s2*(k).
III. EXPERIMENTAL RESULTS
In experiments, the proposed method, FastICA and
Yamaguchi’s method were compared.  A personal com-
puter (Intel Pentium III / 1G Hz CPU) was used to imple-
ment ICA methods.  To evaluate the performance, peak
signal-to-noise ratio (PSNR) is defined as the follow-
ing equations,
PSNRi = 10 Log10(2552/MSEi),
MSEi = 1m (si(k) – si*(k))2Σk = 1
m
,  (i = 1, 2) (6)
The larger the PSNR means the better the image sepa-
ration result.  Three synthetic image pairs A ~ C are
used to compare the performance of these methods.
The experimental results are shown in the Figs. 2 ~
4.  Among these results, it is obvious that the images
separated by our proposed method have no remnant.
The computation time and PSNR value are listed in
Table 1.  Because the whitening process requires a
longer computation time, our proposed method is a
little slower than Yamaguchi’s method.  The proposed
method and Yamaguchi’s method are both suitable
for real time applications.  Also, it shows that the
proposed method has the largest PSNR value in all
experiments.  It is said that the proposed method has
the best accuracy in image separation.
IV. CONCLUSIONS
In this study, an independent component analy-
sis method for images separation by geometric trans-
formation of a scatter diagram is proposed.  This
method has no mathematical convergence issue as in
FastICA.  It can be considered an improved version
of the Yamaguchi’s method, especially in geometric
502 Journal of the Chinese Institute of Engineers, Vol. 31, No. 3 (2008)
Vol. 16, No. 3-4, pp. 469-478.
Haritopoulos, M., Yin, H., and Allinson, N. M., 2002,
“Image Denoising Using Self-Organizing Map-
Based Nonlinear  Independent  Component
Analysis,” Neural Networks, Vol. 15, No. 8-9, pp.
1085-1098.
Hyvärinen, A., and Oja, E., 1997, “A Fast Fixed-Point
Algorithm for Independent Component Analysis,”
Neural Computation, Vol. 9, No. 7, pp. 1483-1492.
Hyvärinen, A., and Oja, E., 2000, “Independent Com-
ponent Analysis: Algorithms and Applications,”
Neural Networks, Vol. 13, No. 4-5, pp. 411-430.
Plumbley, M. D., 2003, “Algorithms for Nonnega-
tive Independent Component Analysis,” IEEE
Transactions on Neural Networks, Vol. 14, No.
3, pp. 534-543.
Polder, G., van der Heijden, G. W. A. M., and Young,
I. T., 2003, “Tomato Sorting Using Independent
Component Analysis on Spectral Images,” Real-
Time Imaging, Vol. 9, No. 4, pp. 253-259.
Yamaguchi, T., Hirokawa, K., and Itoh, K., 2000,
“Independent Component Analysis by Transform-
ing a Scatter Diagram of Mixtures of Signals,”
Optics Communications, Vol. 173, No. 1-6, pp.
107-114.
Manuscript Received: May 10, 2006
Revision Received: June 09, 2007
and Accepted: July 09, 2007
amounts of data (e.g. 1024 · 1024 images).
estimated source signal s^.
vector after decorrelation, and it can be obtained by a whit-
ening process,
z ¼ Vx; ð5Þ
where V is a linear transformation for decorrelating x. The
sum of one-unit contrast functions, C ¼Pni¼1JðwiÞ, is max-
imized (with respect to wi, i = 1, . . . ,n) to reduce the mutual
information between the observed signals. The basic ﬁxed-
point iteration formulation with the learning steps can be
simpliﬁed as (Hyva¨rinen, 1999)
gnition Letters 29 (2008) 370–376 3712.2. Fast ﬁxed-point independent component analysis
Hyva¨rinen proposed the ﬁxed-point ICA (FastICA)
(Hyva¨rinen, 1999). This method uses the negentropy to
measure the non-Gaussianity of a histogram. Negentropy
uses a contrast function for estimating the FastICA trans-
formation matrix W. The contrast function J(Æ) is deﬁned
as
JðwiÞ ¼ ðEfGðwTi zÞg  EfGðtÞgÞ2; ð4Þ
where E is the expectation, G is practically any non-qua-
dratic function (e.g. the tanh function), wi is a m-dimension2. Background of independent component analysis
2.1. ICA theory
The main concept of ICA is to ﬁnd a mutual orthogonal
space for observed mixed signals (which are statistically
independent). Let s = [s1(k), . . . , sn(k)]
T and x = [x1(k), . . . ,
xn(k)]
T denote a source signal vector and an observed sig-
nal vector, respectively, where k = 1  m. xi(k) can be
described as linear mixtures of sj(k) (Hyva¨rinen and Oja,
2000),
xiðkÞ ¼
Xn
j¼1
aijsjðkÞ; i ¼ 1; 2; . . . ; n; ð1Þ
where the mixing weights aij are constant coeﬃcients. It is
convenient to use the matrix notation,
x ¼ As; ð2Þ
where A is an unknown matrix with the elements aij. The
goal of ICA is to obtain an approximation s^ of s by esti-
mating a transformation matrix W, such as
s^ ¼Wx: ð3Þ
That is, it maximizes the non-Gaussianity and obtain theobserved data for lower dimensions. This paper proposes a
histogram based data-reducing algorithm (DR-FastICA)
for estimating independent components. The proposed
algorithm reduces the observed data number for FastICA.
Based upon two statistical criteria, in order to keep the his-
togram contour of processed data, the proposed algorithm
uses two steps: a coarse step for data sampling and a ﬁne
one for data tuning. Two statistical criteria are used to
keep the histogram contour with a smaller sample number
from the observed data. The proposed algorithm reduces
the computation time and the amount of memory needed
for executing any ICA algorithm, especially for large
S.-H. Chiu et al. / Pattern Recoweight vector (the ith row of W), t is a Gaussian variable
with zero mean and unit variance, z is an observed randomwi  EfzG0ðwTi zÞg  EfG00ðwTi zÞgwi; ð6Þ
where G 0 and G00 are the ﬁrst and second derivatives of G,
respectively. A comprehensive description of ICA and Fas-
tICA can be found in the literature (Hyva¨rinen et al.,
2001).
3. The data-reducing algorithm
The proposed data-reducing algorithm reduces tedious
computation by transforming the histogram of observed
mixed signals. In this paper, gray scale images are used
to explain the concept. Let si(k) and si ðkÞ be observed
and data-reduced (processed with the proposed algorithm)
signals (si(k) and si ðkÞ 2 ½0; 255Þ. We use Hi = (Hi[0], . . . ,
Hi[255])
T and Hi ¼ ðH i ½0; . . . ;H i ½255ÞT to denote origi-
nal and data-reduced histograms, respectively. The pro-
posed data-reducing algorithm is to obtain the
relationship,
H i ½g ¼ rHi½g; g 2 ½0; 255; ð7Þ
where g is the gray scale, r is a down-sampling coeﬃcient
(0 < r < 1). A lower r means more data is reduced. Two cri-
teria are used to decide r:
ð1Þ bli  li c ¼ 0; ð8Þ
ð2Þ bvari  vari c ¼ 0; ð9Þ
where li = E{si(k)}, li ¼ Efsi ðkÞg, vari = E{(si(k)  li)2}
and vari ¼ Efðsi ðkÞ  li Þ2g, bÆc is the ﬂoor function
(rounding a number to the nearest integer less than or
equal to it). Fig. 1 shows the ﬂowchart for obtaining r. ra
means the r value in the ath iterative computation. The rule
of thumb for the choice of the initial value r is 0.5 and the
decreasing step d is 0.1. We set d = 0.1 when the image size
1
r =0.5
1+α
r = δα −r
Does r satisfy Eqs.(8) 
and (9) ?
r =
α
r
Yes 
NoFig. 1. Flowchart for computing r.
pix
10 70 20 70
10 70 60 70 10 20 30 40 60 70 
 
*1
gnitA B  C  D  
A B  C  D  
A  B  C  D  3. ‘‘
suc
If the
of N
increa
redun
will c
W
p2:(6,
(1)
Fig. 4.
NH2[2A B  C  D  
A B  C  D   03 0 3
][21 gH  tb  M = 4,  
A B  C  D  
A  B  C  D   
A  B  C  D  
A B  C  D  
A B  C  D  
A B  C  D  
A  B  C  D  
03 0 0
][11 gH  t
3]A[]A[ *111 == HH
a
S.-H. Chiu et al. / Pattern Reco’’ can only appear once in each component column,
h as the three pixels
pa, pb and pc (a5 b5 c)
pa! (NH1[],NH2[•],NH3[•], . . . ,NHn[]),
pb! (NH1[•],NH2[],NH3[•], . . . ,NHn[•]),
pc! (NH1[•],NH2[•],NH3[], . . . ,NHn[•]).
chosen q pixels are added to P, it will adjust the value
Hi[] toward zero. Because these q pixels will also
se the NHi[•] value by one, it must remove (q  1)
dant pixels (with NHi[g] > 0) from P. The ﬁne step
ontinue until all Hi are satisﬁed.
ith the example of Fig. 2, two points p1:(5, 4) and
2) are picked from P as follows (see Fig. 5):
Adding a point p1:(5, 4) with gray scale ‘‘D’’ and
‘‘70’’, then NH1[D] = 0 and NH2[70] = +1.
A B  C  D  
A B  C  D  
B},A{
][][ *121
∈
=
y
yHyH
A B C  D  
A B C  D   
A B C  D  
A B  C  D  
A B  C  D  
A B  C  D  
A  B  C  D  
3 0 33
][31 gHt
C}B,A,{
][][ *131
∈
=
y
yHyH
A B C D
A B C  D   
A B C  D  
A B  C  D
A B  C  D  
A B  C  D  
A  B  C  D  
3 233
][41 gH  t
1]D[1 =NH  
C}B,A,{
][][ *141
∈
=
y
yHyH
c
d
An example of the coarse step with M = 4: (a) ﬁrst sampling; (b) sec
0] = 1).10 20 20 30 3]10[]10[ 22 == HH
10 70 40 30
10 70 20 40
10 60 40 40
10 70 20 70
10 20 30 40 60 70 
3 0 1 20 0 
][22 gHtel p: ),( t
10 70 40 30
10 70 20 40
10 60 40 40
3 0 0 00 0 
][12 gHt
ion Letters 29 (2008) 370–376 373(2) Adding a point p2:(6, 2) with gray scale ‘‘B’’ and
‘‘20’’, then NH1[B] = +1 and NH2[20] = 0.
We remove one point (2,2) with gray scale ‘‘B’’ and ‘‘70’’
from P to let NH1[B] = 0 and NH2[70] = 0. After the
coarse and ﬁne steps, the original histogram Hi is trans-
formed to the new one Hi
4. Experimental results and comparisons
Two experiments were used to evaluate the proposed
algorithm’s performance. In the ﬁrst experiment, we con-
sidered mixed image separation with diﬀerent down-sam-
pling coeﬃcients. In the second one, we study the
amount of data and computation time with diﬀerent image
sizes. The mean-square error (MSE) is used to evaluate the
10 70 60 70
10 20 20 30 }60,10{
][][ *222
∈
=
y
yHyH
10 70 40 30
10 70 20 40
10 60 40 40
10 70 20 70
10 70 60 70
10 20 20 30
10 20 30 40 60 70 
3 1 1 20 2 
][32 gHt
}60,40,10{
][][ *232
∈
=
y
yHyH
10 70 40 30
10 70 20 40
10 60 40 40
10 70 20 70
10 70 60 70
10 20 20 30
10 20 30 40 60 70 
3 1 1 31 2 
][42 gHt
1]20[2 =NH
}70,60,40,30,10{
][][ *242
∈
=
y
yHyH
ond sampling; (c) 3rd sampling; (d) fourth sampling (NH1[D] = 1 and
ast
Image size FastICA DR-FastICA
0
054
521
524
199
960
zes
ime
.72
.20
.81
.53
.42
gnitthe amount of computation memory, we use the amount
of data to represent the memory requirement. Tables 1
and 2 present the performance of the FastICA and
DR-FastICA for the amount of data, computation time,
comparative ratios (Rm, Rt as deﬁned in Eq. (12)) and
MSE.
Comparative ratio of memory requirement Rm ¼ NN 0 : 1;diﬀerent size images (i.e. 64 · 64  1024 · 1024 pixels).
Since the amount of data is directly proportional to
N = L · L TimeFastICA r N
L = 1024 1,048,576 131.71 0.003 3
L = 512 262,144 41.50 0.010 2
L = 256 65,536 9.12 0.040 2
L = 128 16,384 2.18 0.200 3
L = 64 4096 0.83 0.250
N: data number; N0: data number after reducing; computation time (s).
Table 2
The ratios and MSE of FastICA and DR-FastICA with diﬀerent image si
Image size FastICA DR-FastICA
N Time N 0 T
L = 1024 1048576 131.71 3054 2
L = 512 262144 41.50 2521 1
L = 256 65536 9.12 2524 0
L = 128 16384 2.18 3199 0
L = 64 4096 0.83 960 0Table 1
Amount of data (image size) and computation time of FastICA and DR-F
S.-H. Chiu et al. / Pattern RecoComparative ratio of computation time
Rt ¼ TimeFastICA
TimeDR-FastICA
: 1 ð12Þ
where N is the original data number, N 0 is the data num-
ber after data-reducing. In the FastICA optimal algo-
rithm, one needs O(N2) (N = m) operations to calculate
the term ðEfzG0ðwTi zÞgÞ of Eq. (6) at each iteration. For
DR-FastICA, a smaller rwill need less computation time,
i.e. O(N2),N  rm. The computation time for obtaining r
is included in the coarse step time (Table 1). With DR-
FastICA, it can reduce a large amount of data, and save
computation time. Especially when the image size is large
(e.g. 1024 · 1024), DR-FastICA requires less memory
and uses less computation time than FastICA does (see
the comparative ratios Rm and Rt). However, the data-
reducing algorithm is not necessary for FastICA when
the image size is smaller than 64 · 64.
5. Conclusions
In this paper, we propose a histogram based data-reduc-
ing algorithm for improving the performance of the ﬁxed-point independent component analysis. The main concept
of the proposed method is to keep the histogram contour
with a small number of samples from the observed data
of FastICA by two statistical criteria (mean and variance,
i.e. Eqs. (8) and (9)). From the experimental results, our
algorithm reduces the amount of data and saves computa-
tion time over FastICA, especially for cases with large
amounts of data (e.g. an image with 1024 · 1024 pixels).
The method also can be applied to other kinds of data,
such as 1-D signal data. Since FastICA estimates indepen-
dent components by iterative computation with the ﬁxed-
Rm ¼ NN 0 : 1 Rt ¼ TimeFastICATimeDR-FastICA : 1 MSE * 10
3
343:1 48:1 0.05
104:1 35:1 0.04
26:1 11:1 0.09
5:1 4:1 0.06
4:1 2:1 0.61ICA with diﬀerent image sizes
TimeData-reducing Algorithm TimeFastICA Total
Coarse step Fine step
0.26 1.35 1.11 2.72
0.19 0.43 0.58 1.20
0.17 0.24 0.4 0.81
0.15 0.18 0.2 0.53
0.05 0.15 0.22 0.42
ion Letters 29 (2008) 370–376 375point algorithm, the larger the Rm, the more time and
memory space are saved. However, our data-reducing
algorithm may become redundant for FastICA when the
image size is smaller than 64 · 64. For other ICA algo-
rithms (to ﬁnd the independent components by measuring
the non-Gaussianity of estimated components), our
method is also useful.
References
Bell, A.J., Sejnowski, T.J., 1995. An information-maximization approach
to blind separation and blind deconvolution. Neural Comput. 7, 1129–
1159.
Burel, G., 1992. Blind separation of sources: A nonlinear neural
algorithm. Neural Network 5, 937–947.
Cardoso, J.F., 1998. Blind signal separation: Statistical principles. Proc.
IEEE 86, 2009–2025.
Comon, P., 1994. Independent component analysis, A new concept?
Signal Process. 36 (3), 287–314.
Cichocki, A., Rutkowski, T., Siwek, K., 2002. Blind signal extraction of
signals with speciﬁed frequency band. In: Proc. Neural Networks for
Signal Process., the 2002 12th IEEE Workshop on 4–6 September,
pp. 515–524.
Girolami,M., Fyfe, C., 1997. Generalized independent component analysis
through unsupervised learning with emergent Bussgang properties. In:
Internat. Conf. on Neural Networks, vol. 3, pp. 1788–1791.
