 
????? 
 
??????????????????? BTB ??????????
?????????? BTB ?????????????????????
?????????? BTB ?????????????????????
??????????taken branch?????? BTB???????????
BTB ???????????????????????????????
?????????? taken trace?????????????? taken trace
??????????????????? taken trace ??????????
?????????????????? BTB ?????????????
???????????????????????????????? 77% 
BTB ???????? 
???: ????????BTB???????????? 
 
In this project, we propose an energy-efficient lookup scheme to reduce the BTB 
energy consumption by filtering out the redundant lookups. Unlike the traditional scheme 
in which the BTB has to be looked up every instruction fetch, the key idea behind our 
scheme is to look up the BTB only when the instruction is likely to be a taken branch. In 
the proposed scheme, the conventional BTB organization would be augmented with an 
additional field to store the instruction number between the predicted target and the next 
taken branch, referred to as a taken trace. We develop a dynamic taken trace profiling 
technique which can collect the sufficient taken trace information during program 
execution. According to the profiled data from the previous runs, our scheme can 
conditionally skip the BTB lookup to reduce the energy consumption. The experimental 
results show that with a negligible performance degradation our design can reduce the 
BTB energy consumption by about 77% on average. 
Keywords: BTB, lookup scheme, energy efficiency.   
 
 
                                                 
 
typical instruction fetch integrated with the BTB lookup. 
During IF stage, the instruction address, i.e., program 
counter (PC) value, is concurrently issued to the instruction 
cache and BTB. If a valid BTB entry is found for that 
address, then the instruction is a branch. According to the 
cached prediction information, if the branch is predicted 
taken, the BTB would output the corresponding target 
address to be used as the next PC. If the branch is predicted 
not taken, the processor continues fetching sequentially after 
the branch. 
After the processor finishes executing the branch, it 
checks to see if the BTB correctly predicted the branch. If it 
has, all is well, and the processor can continue sequentially. 
If the branch was predicted incorrectly, the processor must 
flush the pipeline and begin fetching from the correct branch 
path. Then, the branch prediction information and branch 
target address (if changed) must be updated. 
A. Characteristics of the BTB Lookups 
Note that the BTB only caches the information 
regarding the recently executed branch instructions. Thus the 
BTB lookup is necessary only for the branch instructions. In 
the traditional BTB lookup mechanism, because the fetch 
engine has no sufficient information to distinguish the 
branch instructions, the BTB has to be looked up every 
instruction fetch, such that an overwhelming majority of the 
BTB lookups are redundant (or unnecessary). As indicated in 
[9], the branch instructions account for about 20% of the 
total executed instructions. It means that at least 80% of the 
BTB lookups are redundant. Fig. 2 shows the proportion of 
the non-branch instructions to the total executed instructions 
(referred to as redundant rate) measured from the execution 
traces of MediaBench benchmarks [7]. From this figure, the 
BTB lookup redundant rate is around 83% on average. 
Unlike the conventional design where the BTB is 
always looked up every instruction fetch, motivated by most 
BTB lookups are redundant, we propose an alternative BTB 
design, called lazy BTB. The lazy BTB can dynamically 
profile sufficient information during program execution, and 
then use these profiled data to skip the BTB lookup 
conditionally. The goal is to look up the BTB only when the 
lookup is necessary. By filtering out most redundant BTB 
lookups, our design can effectively reduce the total energy 
consumption of the BTB. 
B. Related Work 
As described previously, we only survey the related 
work which target on reducing lookups to save energy 
dissipated in BTB. Petrov and Orailoglu [4] proposed 
application customizable branch target buffer (ACBTB), 
which is a software profiling technique. By utilizing the 
precise control-flow information of the application, the 
ACBTB is accessed only when a branch instruction is to be 
executed. Because the control-flow information must be 
extracted during compile/link time, their method is static and 
not applicable to the existing executable programs. In 
addition, a large hardware modification is necessary. 
We can use predecode technique to test if the 
instruction is a branch, but the drawback is that the 
predecode bits only become available at the end of the 
instruction fetch stage. This would result in a significant 
performance penalty. In [5], Parikh et al. proposed a small 
hardware table, called prediction probe detector (PPD), to 
reduce unnecessary predictor and BTB accesses. The PPD 
can use compiler hints and predecode bits to recognize when 
lookups to the direction-predictor and BTB can be avoided. 
The drawback of this approach is that the PPD lookup must 
be performed before accessing the predictor and BTB. That 
would result in the extra power consumption and possible 
performance penalty. 
III. Lazy BTB 
This section gives the detailed description of the 
proposed lazy BTB design. We first discuss the BTB 
management, and then develop a dynamic profiling 
technique, which is critical to the lazy BTB. In addition, the 
necessary hardware augmentations are also provided. 
A. BTB Management 
The BTB management is concerned with the issue of 
entry allocation and replacement. For most microprocessors, 
the BTB is a valuable resource with limited size. Thus, 
instead of allocating entry for each branch, we only cache 
the branches which have the potential for improving 
performance. Because caching the untaken branches does not 
improve the performance and they are unlikely to be taken in 
the future [10], the allocation policy used in our lazy BTB is 
that we only allocate a new entry for a branch on its first 
taken execution. If no entry is available, then the 
replacement is necessary. As indicated in [10], LRU is good 
enough. It achieves the similar performance gain to their 
proposed MPP algorithm which is an elaborate replacement 
policy. Thus, the entry replacement used in the lazy BTB is 
the simple LRU. 
PC 
instruction 
cache 
Branch Target Buffer 
v BT TA PI 
v: Valid Bit 
BA: Branch Address 
TA: Target Address 
PI: Prediction Information
branch target address 
Fig. 1. A typical instruction fetch integrated with the BTB
lookup. 
BTB Lookup Redundant Rate
50%
60%
70%
80%
90%
100%
ad
pc
m
epi
c
g7
21 gsm jpe
g
mp
eg
2
gh
ost
sci
rpt
Av
era
ge
Encoder
Decoder
Fig. 2. BTB lookup redundant rate measured from MediaBench.
BTB lookup is performed (or skipped) during IF stage, and 
the actual branch result, i.e., the path and target address, 
would be determined in ID stage. If the prediction is correct, 
the execution continues with no stall. Otherwise, the 
recovery procedure for misprediction would be executed in 
the EX stage, which costs the performance penalty. From 
this figure, we can break the entire dynamic profiling scheme 
into seven possible paths. Their characteristics, including 
penalty cycles incurred by misprediction, are summarized in 
Table 1, and the detailed descriptions are provided as follow. 
Path 1: In this path, because the instruction is found in 
the BTB and predicted taken, we can retrieve the 
corresponding TTS from the hit entry and set RTL to it 
during the IF stage. Next, in the ID stage, the branch is 
resolved and actually not taken. It is a misprediction case. 
The RTL value has to be reset to 0, and the TSA continues to 
accumulate the taken trace size. Note that the ID stage would 
be overlapped with the IF stage in the pipeline. In order to 
avoid hardware conflict the RTL value changes only in the 
second phase of IF stage, and the first phase of ID stage. In 
the EX stage, due to the misprediction, we have to kill the 
fetched instruction, delete the BTB entry, and restart to fetch 
the instruction from the correct path. The penalty cycles are 
2 for this path. 
Path 2: Unlike the path 1 which is a misprediction, the 
BTB prediction is correct in this path. As shown in Fig. 5, 
the RTL is set to the retrieved TTS value during the IF stage. 
Next, in the ID stage, the TSA value has to be restored to the 
previous taken branch entry indexed by TE, and then be reset 
to 0 to accumulate the following taken trace size. Finally, the 
TE value is set to the index of the hit entry in the EX stage. 
Due to the correct prediction, the penalty cycle is 0. 
Path 3: This path is the execution flow of the non-
branch instructions. Thus, we only increase the TSA value 
by 1 to accumulate the taken trace size during the ID stage. 
Of course, the penalty cycle is 0. 
Path 4: Due to the BTB miss, the instruction is 
predicted as non-branch (or not taken), but it is resolved as a 
taken branch in the ID stage. Consequently, the TSA value 
has to be restored to the previous taken branch entry indexed 
by TE, and then be reset to 0 to accumulate the next taken 
trace size. Next, in the EX stage, we allocate a BTB entry for 
this taken branch. After storing the branch address and its 
target addresses, the TE value is set to the index of the 
allocated entry. Finally, we have to kill the fetched 
send PC to
BTB RTL=0?
TSA++
RTL<=0
N
Hit?
Taken
branch?
Y
BTB[TE].TTS<=TSA
TSA<=0
N
Y
Y
TSA++
Taken
branch?
BTB[TE].TTS<=TSA
TSA<=0
N
Y
RTL<=BTB[index].TTS
send out the
predicted PC
TE=index
correct
prediction
(1) delete entry
(2) kill the fetched instr.
(3) restart to fetch another
instr.
misprediction
(1) store branch & target
addresses into BTB[index]
(2) TE=index
(3) kill the fetched instr.
and restart to fetch another
instr.
normal
instruction
execution
Taken
branch?
BTB[TE].TTS<=TSA
TSA<=0
N
Y
N
RTL--
TSA++
normal
instruction
execution
Hit in BTB?
Y
N
(1) RTL<=BTB[index].TTS
(2) TE=index
(1) store branch & target
addresses into BTB[index]
(2) TE=index & RTL=0
(3) kill the fetched instr.
and restart to fetch another
instr.
Path 2
Path 1
Path 3
Path 4
Path 5
Path 6
Path 7
IF
ID
EX
Fig. 5. The dynamic taken trace profiling technique developed for the lazy BTB design. 
Table 1. The seven possible paths in the lazy BTB scheme. 
Possible
Paths
BTB
Lookup Hit/Miss Prediction
Actual
Branch
BTB Looup
in EX
Penalty
Cycles
Path 1 Y Hit taken not taken - 2
Path 2 Y Hit taken taken - 0
Path 3 Y Miss - not taken - 0
Path 4 Y Miss - taken - 2
Path 5 - - - not taken - 0
Path 6 - - - taken Y/Hit 3/4
Path 7 - - - taken Y/Miss 1/2
consumption per lookup is about 0.484 nJ and 0.492 nJ for 
the conventional and lazy BTBs, respectively.  Because in 
our design the BTB is augmented with an extra TTS (6-bit) 
field for each entry, the energy consumption per BTB lookup 
is slightly larger than that of the conventional BTB. 
The metric used to evaluate the energy efficiency is the 
simple total energy consumption of BTB lookups. Table 4 
shows the total energy consumption number in mJ for both 
the conventional and lazy BTB designs. Compared to the 
conventional BTB, one can immediately notice that the 
energy reduction would be an order of magnitudes. By 
filtering out most redundant BTB lookups, the lazy BTB can 
reduce the total energy consumption of BTB lookups by 
56%~88% for MediaBench.
Performance Impact: The unit of performance 
measurement we use is instructions per cycle (IPC), 
calculated as the total number of execution instructions 
divided by execution cycles. Given that both the number of 
execution instructions and processor cycle time are constant, 
IPC is a direct measure of performance. Compared to the 
conventional BTB, from Fig. 5 we can see that only the 
paths 6 and 7 result in the extra penalty cycles. In this case, 
the BTB lookup is skipped, but the instruction is a taken 
branch actually. Thus, one BTB lookup has to be paid during 
the EX stage, that would decrease the overall performance. 
The paths 6 and 7 are, therefore, referred to as unfavorable 
path.
From previous discussion, we conclude that the 
negative impact of our design on the performance depends 
on the occurrence of unfavorable path. If most instructions 
follow the unfavorable path, then the lazy BTB would result 
in a significant decrease in IPC. Fortunately, the occurrence 
of unfavorable path is small enough. As shown in Table 3, 
the percentage of path 6~7 is about 1.05% on average, 
Therefore, the lazy BTB has a negligible degradation in 
performance. It can be seen from Fig. 6, which shows the 
IPC value for the conventional and lazy BTBs. Our design 
results in roughly 1.7% IPC degradation on average. 
V. Conclusions 
In this paper, we have proposed a low power BTB 
design, called lazy BTB. By using the developed dynamic 
taken trace profiling technique, the lazy BTB can achieve the 
goal of one BTB lookup per taken trace instead of one BTB 
lookup per basic block. The results show that without 
noticeable performance difference from the conventional 
BTB, our design can reduce the total energy dissipated in 
BTB lookups up to 88% for the MediaBench applications. 
References 
[1] S. Manne, A. Klauser, and D. Grunwald, “Pipeline Gating: 
Speculation Control for Energy Reduction,” in Proc. of 
International Symposium on Computer Architecture, 1998, pp. 
132-141. 
[2] B. Fagin, “Partial Resolution in Branch Target Buffers,” IEEE 
Transactions on Computers, Vol. 46, No. 10, 1997, pp. 1142-
1145. 
[3] D. H. Albonesi, “Selective Cache Ways: On-Demand Cache 
Resource Allocation,” in Proc. of International Symposium on 
Microarchitecture, 1999, pp. 248-259. 
[4] P. Petrov and A. Orailoglu, “Low-Power Branch Target Buffer 
for Application-Specific Embedded Processors,” in Proc. of 
Euromicro Symposium on Digital System Design, 2003, pp. 
158-165. 
[5] D. Parikh, K. Shadron, Y. Zhang, and M. Stan, “Power-Aware 
Branch Prediction: Characterization and Design,” IEEE 
Transactions on Computers, Vol. 53, No. 2, 2004, pp. 168-186. 
[6] D.C. Burger and T. M. Austin, “The SimpleScalar Tool Set, 
Version 2.0,” Computer Architecture News, 25 (3), pp. 13-25, 
June, 1997. Extended version appears as UW Computer 
Sciences Technical Report #1342, June 1997. 
[7] C. Lee, M. Potkonjak and W. H. Mangione-Smith, 
“MediaBench: A Tool for Evaluating and Synthesizing 
Multimedia and Communications Systems,” in Proc. of 
International Symposium on Microarchitecture, Dec. 1997, pp. 
330-335. 
[8] G. Reinman and N. P. Jouppi, “CACTI 2.0: An Integrated 
Cache Timing and Power Model,” COMPAQ WRL Research 
Report, 2000. 
[9] J. L. Hennessy and D. A. Patterson, “Computer Architecture: 
A Quantitative Approach,” 3rd Ed., Morgan Kaufmann 
Publishers, Inc., 2003. 
[10] C. H. Perleberg and A. J. Smith, “Branch Target Buffer Design 
and Optimization,” IEEE Transactions on Computers, Vol. 42, 
No. 4, 1993, pp. 396-412. 
[11] R. Witek and J. Montanaro, “StrongARM: A High-
Performance ARM Processor,” in Proc. of COMPCON, 1996, 
pp. 188-191. 
IPC for both the conventional and lazy BTBs
0.0
0.2
0.4
0.6
0.8
1.0
adp
cm
_en
adp
cm
_d
e
epi
c_e
n
epi
c_d
e
g7
21
_en
g7
21
_d
e
gsm
_en
gsm
_d
e
jpe
g_
en
jpe
g_
de
mp
eg2
_e
n
mp
eg2
_d
e
gh
ost
sci
rpt
Av
era
ge
IPCConv
IPCLazy
onv
azy
Fig. 6. The IPC value for the conventional and lazy BTBs. 
Table 4. Total energy consumption (measured in mJ) for both
the conventional and lazy BTB designs. 
BTBConv BTBLazy Reduction
adpcm_en 290.8 126.9 56.35%
adpcm_de 239.2 90.7 62.09%
epic_en 25.3 3.7 85.25%
epic_de 3.2 0.6 82.73%
g721_en 131.9 26.1 80.22%
g721_de 128.5 25.0 80.56%
gsm_en 896.6 144.4 83.90%
gsm_de 305.7 35.6 88.35%
jpeg_en 48.6 7.6 84.41%
jpeg_de 12.3 1.9 84.58%
mpeg2_en 544.4 192.8 64.59%
mpeg2_de 82.2 15.6 80.99%
ghostscirpt 557.1 77.3 86.13%
Average 251.2 57.6 77.09%
Improve CAM Power Efficiency Using 
Decoupled Match Line Scheme 
 
Yen-Jen Chang 
Dept. of Computer Science 
National ChungHsing University 
Taichung, Taiwan 
ychang@cs.nchu.edu.tw 
Yuan-Hong Liao 
Dept. of Computer Science 
National ChungHsing University 
Taichung, Taiwan 
s9356042@cs.nchu.edu.tw 
Shanq-Jang Ruan 
Dept. of Electronic Engineering 
NTUST, Taipei, Taiwan 
sjruan@mail.ntust.edu.tw 
 
 
Abstract 
Content addressable memory (CAM) is widely used in 
many applications that require fast table lookup. Due to the 
parallel comparison feature and high frequency of lookup, 
however, the power consumption of CAM is usually significant. 
In this paper we propose a decoupled match line scheme 
which combines the performance advantage of the traditional 
NOR-type CAM and the power efficiency of the traditional 
NAND-type CAM. In our design, a CAM word is divided into 
two segments, and then all the CAM cells are decoupled from 
the match line. By minimizing both the match line 
capacitances and switching activities, our design can largely 
reduce the CAM power dissipated in search operations. The 
results measured from the fabricated chip show that without 
any performance penalty our design can reduce the search 
energy consumption of the CAM by 89% compared to the 
traditional NOR-type CAM design. 
1. Introduction  
Content addressable memory (CAM) is a storage, which 
is addressed by the content (or data) rather than the memory 
address. Because CAM is a hardware table and can compare 
the search data with all the stored data in parallel, the speed of 
CAM lookup is much faster than the software lookup. 
Therefore, CAM is widely used in TLB, high associative 
cache, image processing, database and network routers, etc., 
all require fast table lookup. However, the power consumption 
of CAM is usually considerable due to the parallel comparison 
feature in which a large amount of transistors are active on 
each lookup. For example, in StrongARM [1] embedded 
processors, the fully-associative TLBs consume about 17% of 
the total chip power. 
There are two traditional CAM designs; one is the NOR-
type CAM, and the other is the NAND-type CAM. The NOR-
type CAM provides the best performance in search operation, 
but it costs a large power consumption. In contrast, the 
NAND-type CAM trades the search performance for low 
power feature. As indicated in the previous researches, the 
main sources of power dissipation in CAM are the match lines 
and search lines (or bitlines). The power consumption of 
                                                             
  This work was supported by the National Science Council of Taiwan under 
grant No. NSC95-2221-E-005-049. 
match lines can be reduced by reducing the voltage swing on 
the match lines [2][3], or by segmenting the match line [4][5]. 
Because the match lines are precharged conditionally in the 
segmentation techniques [4][5], the performance degradation 
is a vital and unavoidable disadvantage. 
Combing the performance advantage of the traditional 
NOR-type CAM and power efficiency of the traditional 
NAND-type CAM, this paper presents a decoupled match line 
scheme, which can largely reduce the power consumption of 
CAM without any performance penalty. In the proposed CAM 
design, only the matched words can discharge the match line 
from 1 to 0. Similar to the match line segmentation methods, 
our design also divides a CAM word into two segments. The 
most distinct features of the proposed decoupled match line 
scheme are summarized as follows. (1) In our design, because 
all the pull-down paths are disconnected from the ground 
during the precharge phase, it is unnecessary to discharge all 
the bitlines to prevent the unexpected short-circuit power 
consumption. Thus, the power dissipated in bitline switching 
activities can be effectively reduced. (2) By using 
segmentation method, our design can largely reduce the match 
line switching activities, and thus the power consumption of 
match lines. (3) Because we decouple all CAM cells from the 
match line, the match line is lightweight that accelerates the 
discharge speed. This ensures our design has the same search 
performance as the traditional NOR-type CAM. (4) Because 
we provide a level restore path on the match line, our design 
has the immunity from the false match incurred by the 
possible race condition problem. 
The proposed CAM design was fabricated with the 
TSMC 0.18µm technology. With the size of 128×32, the 
measurement results show that if a CAM word is divided into 
4 and 28 bits, without any performance loss our design can 
improve the search energy efficiency up to roughly 89% 
compared to the traditional NOR-type CAM design, and the 
total area overhead is less than 6.1%. 
The rest of this paper is organized as follows. Section 2 
reviews the CAM organization and the previous work on 
CAM power reduction. Section 3 describes the circuitry 
developed for the decoupled match line scheme in detail. 
Besides the discussions on the importance issues, the 
comparison between our design and the related work is also 
segmentation. In [4], Zukowski et al. introduced a selective 
precharge technique to reduce the match line power 
consumption by breaking a CAM word into two stages. A 
small subset of CAM cells can be used to do a precalculation, 
and the results are used to do a conditional (selective) 
precharge. As indicated in [4], separating 7 out of 128 bits 
would reduce the CAM power consumption by roughly 85% 
with a modest delay penalty. 
A similar CAM word structure was proposed in [5]. 
Besides the segmented match line, they also introduce a new 
CAM cell with single bitline. The single bitline design 
requires only one heavy loading bitline, and prevents the 
frequent switching. Therefore, their method can further reduce 
the power consumption of CAM, but the performance 
degradation is still inevitable. 
An adaptive serial-parallel CAM [6] is another low 
power CAM structure, which can operate either in parallel or 
in serial mode. In serial mode the energy consumption is 
almost a quarter of the conventional parallel CAM, but the 
cycle time is 25% slower than the original CAM. In parallel 
mode, the energy consumption is still 33% better than the 
conventional parallel CAM without any performance penalty. 
3. Low Power CAM Design Using Decoupled 
Match Line Scheme 
The key idea behind our design is to combine the 
performance advantage of NOR-type CAM with the power 
efficiency of NAND-type CAM. As shown in Fig. 4, we 
divide a CAM word into two segments, i.e., SEG_1 and 
SEG_2, and the necessary control circuitry. In the SEG_1, the 
CAM cell is implemented as XNOR-type and their pull-down 
transistors are arranged in the NAND type, denoted as NAND-
type block in Fig. 4. The NAND-type block is connected to 
the ground only when all the CAM cells of SEG_1 are 
matched. In contrast to SEG_1, we use the XOR-type CAM 
cell to implement the SEG_2, and their pull-down transistors 
are placed in the NOR type, denoted as NOR-type block in 
Fig. 4. The NOR-type block is disconnected from the ground 
only when all the CAM cells of SEG_2 are matched. 
3.1 Search Operation 
Similar to the traditional CAM, in our design there are 
two phases during a search. They are precharge and match 
evaluation phases, respectively. In the precharge phase, all the 
match lines are first precharged to high, and then in the match 
evaluation phase only the matched words would change the 
logic level of the corresponding match line from 1 to 0. 
 Precharge Phase 
In this phase, the control signal PRE is low. Thus, the 
match line (ML) is initially precharged to high. Because the 
pull-down path T1 and T2 are disconnected by N1 and N2 
transistors, respectively, both M1 and M2 nodes are 
precharged to high via P1 and P2. Due to no paths to the 
ground, it is unnecessary to discharge all the bitlines to 0 to 
prevent the unexpected short-circuit during the precharge 
phase. Compared to the traditional CAM implementation, 
therefore, our design is more efficient in bitline power saving. 
In addition, in our design the match lines are precharged 
unconditionally. It is different from other segmentation 
techniques [4] [5], in which the match lines are precharged 
conditionally that would result in a performance penalty. 
 Match Evaluation Phase 
After the precharge phase, the control signal PRE is 
asserted high and the search data have to be loaded on the 
bitlines to start the matching process. This phase is called 
match evaluation phase. Because we divide a CAM word into 
two segments, i.e., SEG_1 and SEG_2 as shown in Fig. 4, 
depending on the match results of each segment there are four 
possible cases in the match evaluation phase. It is a real match 
only when both the SEG_1 and SEG_2 are matched. These 
cases are described in detail as follows. 
      
XNOR 
CAM cell 
PRE 
XOR 
CAM cell
PRE
PRE 
ML 
N1 
N2 
N3 P1 P2 
P4 P3 
M1
M2
SEG_1 SEG_2Control
T1 T2
NOR-type block 
NAND-type block 
 
Fig. 4. The CAM word structure of the decoupled match line scheme. 
length of SEG_1 is constrained within 4 bits throughout this 
paper. 
 SEG_1 Length vs. Power Saving 
As described above, short SEG_1 can prevent the charge 
sharing problem, but it increases the probability of M1 
discharge. Suppose, for example, that the length of SEG_1 is 
one bit. For a random pattern, the probability of M1 discharge 
would be 50% on average, i.e., the probability of tail transistor 
N2 turned on is also 50%. Because there are n-1 pull-down 
transistors in the NOR-type block, the probability of T2 path 
connected to the ground would increase largely. It results in a 
significant power dissipated in the discharge of the M2 node 
with large drain capacitances. Ideally, the probability of M2 
discharge is the product of the probability of T1 path 
conducting, i.e., p(T1 conducting), and the probability of T2 
path conducting, i.e., p(T2 conducting), as shown in the 
following equation: 
p(T1 conducting) × p(T2 conducting) 
= (1/2)x × (1-(1/2)n-x) 
= (1/2)x - (1/2)n 
, where n and x are the lengths of the entire word and SEG_1, 
respectively. In this equation, we assume that the match 
probability is 1/2 for each CAM cell. In the SEG_2, because 
all the pull-down transistors are arranged in the NOR type, the 
T2 path is disconnected only when they are all turned off. 
Thus, p(T2 conducting) is equal to (1-(1/2)n-x). Fig. 8 shows 
the probability of M2 discharge for various SEG_1 lengths, in 
which n=32 is assumed. Clearly, the probability of M2 
discharge decreases sharply as the length of SEG_1 is 
increased. This implies that the search operation would 
consume more power when we decrease the length of SEG_1. 
4. Results 
 For a solid result, in this paper we use TSMC 0.18µm 
CMOS technology to implement the proposed CAM with 
decoupled match line scheme, and a conventional NOR-type 
CAM used for comparison. Both they are with size of 128×32, 
i.e., 128 words by 32 bits. The core was broken into four 
blocks for both the performance and power efficiency. 
 Performance 
In this paper, the metric used to evaluate the CAM 
performance is the match delay, which is defined as the 
elapsed time from signal PRE is asserted high to the match 
line discharged to 0 in case of a match. Fig. 9 shows the match 
delay for SEG_1 length from 1 to 6 bits. Due to no 
segmentation, the match delay of the conventional CAM 
design is fixed at 0.641ns, as the dash line shown in this 
figure. As revealed in Section 3.2, if the length of SEG_1 is 
larger than 4, then there is a possible false match due to charge 
sharing problem. From this figure, we summarize the most 
important aspects as follows. (1) In our design the match delay 
increases with the length of SEG_1. It is expected, because the 
match line discharge relies on the M1 discharge that connects 
T1 and T2 paths to the ground, and M1 discharge speed 
depends on the number of transistors in the NAND-type block. 
Therefore, the length of SEG_1 is critical to the match 
performance of our design. 
(2) One interesting observation from this result is that 
when the length of SEG_1 is less than 4 bits, the match delay 
of our design is even shorter than that of the conventional 
CAM. This is because our design decouples all CAM cells 
from the match line, such that the match line is lightweight. 
Once the path T1 is conducting, it can discharge the 
lightweight match line quickly. Although the small NAND-
type block would degrade the match performance slightly, the 
lightweight match line can compensate for the loss of match 
performance. From Fig. 9, when SEG_1 is 4 bits, both designs 
have almost the same match delay. 
 Power and Energy 
Fig. 10 shows the power consumption during a search for 
various SEG_1 lengths. As analyzed in Section 3.2, clearly, 
the search power consumption can be reduced sharply as we 
increase the length of SEG_1. When the SEG_1 length is 6 
 
0
0 .5
1
1 .5
2
1 2 3 4 5 6 7 8 9 10 11 12
Length  o f SEG_1  (bits)
Vo
lta
ge
 L
ev
el 
of
 M
atc
h L
ine
 
Fig. 7. The voltage level of match line under the consideration 
to the worst charge sharing problem for various SEG_1 
lengths. 
 
0
0 .1
0 .2
0 .3
0 .4
0 .5
0 .6
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Length  of SEG_1  (bits)
Pr
ob
ab
ilit
y o
f M
2 d
isc
ha
rg
e
 
Fig. 8. The probability of M2 discharge for various SEG_1 
lengths. 
 
Match  Delay  (ns)
0
0 .2
0 .4
0 .6
0 .8
1
1 2 3 4 5 6
Leng th o f SEG_1 (bits)
Our
Conv.
 
Fig. 9. The match delay for various SEG_1 lengths. 
