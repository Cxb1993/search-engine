共 1 頁，第 1 頁 
行政院國家科學委員會補助專題研究計畫
 成 果 報 告   
□期中進度報告 
子計畫三：結合多媒體互動與生理回饋技術建立智慧型太極拳
輔助學習環境－太極拳輔助學習系統與師生姿態同步及動作
分析技術研發 
 
計畫類別：□ 個別型計畫   整合型計畫 
計畫編號：NSC 95－2221－E－260－028－MY3 
執行期間： 2006 年 8 月 1 日至 2009 年 7 月 31 日 
 
計畫主持人：石勝文 
共同主持人： 
計畫參與人員：沈政達、梁祐銘、許瑞文、吳翎慈、許迺赫、周家德、朱
峰頡、王蔚強、劉秀雯、蔡孟傑、鍾宛聿、盧珮騏、陳侃如 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、列
管計畫及下列情形者外，得立即公開查詢 
涉及專利或其他智慧財產權，□一年二年後可公開查詢 
 
執行單位：國立暨南國際大學資訊工程學系 
中   華   民   國   98   年   7   月   31   日 
2.4 系統校正成果與討論 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.4.1 以單擺作攝影機校正 . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.4.2 固定式投影機校正 . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.4.3 可旋轉式投影機校正 . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.4.4 身形參數量測程序 . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3 姿態差異分析、 師生動作同步, 以及動作分析 19
3.1 前言 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
3.2 文獻探討 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3.2.1 利用微機電感測器 (MEMS Sensor) 做動作分析 . . . . . . . . . . 20
3.2.2 師生姿態同步 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
3.2.3 人類行為分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
3.3 研究方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3.3.1 使用微機電感測器 (MEMS Sensor) 估測姿態差異 . . . . . . . . . 23
3.3.2 Metric Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
3.3.3 同步影像序列 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
3.4 研究成果及討論 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.4.1 可穿戴式感應器系統 . . . . . . . . . . . . . . . . . . . . . . . . . 35
2
圖形目錄
1.1 Motion Master 使用情形,(圖左) 學員穿戴 Motion Capture 追蹤球及頭盔
式顯示器。 (圖右) 學員由頭盔式顯示器看到的場景, 圖中穿紅衣者是虛擬教練。 4
1.2 用於 Motion Capture 之光學追蹤球 . . . . . . . . . . . . . . . . . . . . 5
2.1 簡易單擺示意圖 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2 單擺擺盪時間和角度關係圖 . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.3 旋轉投影機的兩種基本型 . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.4 本國勞工身體尺寸統計值 . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.5 人體姿態之人形輪廓 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.6 藍色曲線為擺錘於影像中位置和擺盪時間的關係, 紅色曲線則為擺動角度。 . 14
2.7 實際實驗中取得的在五個不同方向觀測到的單擺軌跡。 . . . . . . . . . . . . 14
2.8 實驗中取到的垂直與水平的第五個位元 gray code 的影像 . . . . . . . . . . 15
2.9 攝影中所取得之投影機垂直與水平 gray code 交點座標圖 . . . . . . . . . . 15
4
3.14 正方形點代表使用 MEMS Sensor 實際量測的值, 虛線為使用 MicroScribe
量測的理論值。 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.15 (a) 四十五維訊號的歐氏距離相似群 (藍色) 及不相似群之結果 (紅色)。 (b)
馬氏距離相似群及不相似群之結果。 . . . . . . . . . . . . . . . . . . . . . 39
3.16 雲手之相似群訊號及不相似群訊號 . . . . . . . . . . . . . . . . . . . . . . 40
3.17 棚捋擠按之相似群訊號及不相似群訊號動作 . . . . . . . . . . . . . . . . . 41
3.18 (a) 學員在 t=200 時刻, 回溯 T=70 步的匹配結果, (b) 學員在 t=300 時刻,
回溯 T=70 步的匹配結果。 . . . . . . . . . . . . . . . . . . . . . . . . . . 43
3.19 (a) 學員在 t=200 時刻, 回溯 T=70 步的匹配結果。 (b) 學員在 t=300 時
刻, 回溯 T=70 步的匹配結果。 . . . . . . . . . . . . . . . . . . . . . . . . 43
3.20 關鍵動作時, 歐氏距離及馬氏距離師生影像同步比對結果。 . . . . . . . . . . 44
3.21 師生影像非同步結果 (“雲手”一) . . . . . . . . . . . . . . . . . . . . . . . 44
3.22 師生影像非同步結果 (“雲手”二) . . . . . . . . . . . . . . . . . . . . . . . 45
3.23 師生影像同步結果 (“雲手”一) . . . . . . . . . . . . . . . . . . . . . . . . 46
3.24 師生影像同步結果 (“雲手”二) . . . . . . . . . . . . . . . . . . . . . . . . 47
3.25 師生影像非同步結果 (“棚捋擠按”一) . . . . . . . . . . . . . . . . . . . . 48
3.26 師生影像非同步結果 (“棚捋擠按”二) . . . . . . . . . . . . . . . . . . . . 48
3.27 師生影像同步結果 (“棚捋擠按”一) . . . . . . . . . . . . . . . . . . . . . 49
3.28 師生影像同步結果 (“棚捋擠按”二) . . . . . . . . . . . . . . . . . . . . . 50
6
4.14 追蹤人體姿態流程圖 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
4.15 雙手前舉不使用感測器追蹤結果 . . . . . . . . . . . . . . . . . . . . . . . 74
4.16 雙手前舉使用感測器追蹤結果 . . . . . . . . . . . . . . . . . . . . . . . . 75
4.17 三維人體姿態追蹤流程圖 . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
4.18 簡單動作的追蹤結果 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
4.19 “掤、 捋、 擠、 按”的追蹤結果。 . . . . . . . . . . . . . . . . . . . . . . . . 81
4.20 由左至右分別是原始影像、 剪影、 追蹤結果以及未配戴感測器的追蹤結果。 . 81
8
摘要
本三年期計畫的第一年主要任務是發展系統校正法, 主要成果包括一個以單擺作為校正目標
的多重攝影機校正法, 使用格雷碼 (gray code) 的固定式投影機至固定攝影機的校正法, 以
及利用 PTZ 攝影機回授作旋轉式投影機的自我校正法。 另外我們也發展了一套身形量測的
程序, 可建立師生之身形參數。 同時也將介紹將身形輪廓依動作與身裁參數調變的結果。 這
些方法經實驗證實皆可有效解決攝影機與投影機之校正問題。
第二年的研究重心在於姿態差異估測、 師生同步技術與行為分析等三個項目。 在姿態差
異的估測方面, 我們利用穿戴在身上的加速度感應器訊號來估算姿態差異度。 為了調整感應
器訊號貢獻至姿態量的權重, 我們使用了 Metric Learning 的技術。 藉由人工評斷的相似或
不相似的姿態集, 可以自動學習在抑制無關訊號的同時, 也強調重要的訊號差異, 讓學到的距
離函數與人的感覺較一致。 而後我們使用學習到的姿態距離作為師生同步計算的基礎。 實驗
顯示不管是姿態差異的估測或是動作同步的計算結果皆可以達到預期的效果。 另外在人類行
為分析方面, 我們提出使用 VLMM 來學習人類動作模式, 並將 VLMM 轉為 HMM 的方
法。 這個方法的優點是 VLMM 可以學習出狀態的拓樸結構, 而 VLMM 的參數也可以轉換
為 HMM 參數。 實驗結果顯示我們所提出的方法在效能上皆優於直接使用 HMM 的作法。
第三年的研究重心在於結合微機電感測器與視覺感測器來追蹤三維的人體姿態以及與
子計畫一的技術整合。 我們使用階層式的貝式濾波器結合剪影及穿戴在身上的感測器來追蹤
人體姿態, 藉由樹狀結構去縮小尋找範圍找尋最佳的解。 我們也整合了子計畫一提供的粒子
濾波器技術, 使用攝影機與微機電感測器去估測人體姿態。 粒子濾波器主要的好處在於能描
述非線性及多形式事後機率分佈, 這些都是追蹤人體動作會遇到的問題。 我們證明了使用微
機電感測器可以有效的降低粒子濾波器技術的搜尋維度。 實驗結果顯示微機電感測器可以提
供視覺感測器所難以提供的重要姿態線索, 因此, 使用微機電感測器估測姿態的成效會比單
單只用視覺感測器來得更強健。
Abstract
In the first year of the three-years project, our main task is to develop system cali-
bration methods assisting people in the main project to construct an intelligent space
for learning Tai Chi Chuan. Our main results include a multi-camera calibration sys-
tem using a pendulum, a projector calibration method that uses gray code patterns,
and a steerable projector self-calibration method that uses a PTZ camera for estimat-
ing unknown parameters. Furthermore, we have also developed both a measurement
procedure for acquiring the length of each body parts, and a human silhouette defor-
mation method. Experimental results shown in this report con rm that the proposed
method can effectively solved both the camera calibration problem and the projector
calibration problem.
The focuses of the second year include posture difference estimation, motion syn-
chronization, and human behavior analysis. The signals of the wearable accelerometers
are employed to estimate the posture difference using the metric learning technique.
In order to learn the sense of posture difference observed by a person, posture images
were manually matched to form a similar-posture set and a dissimilar posture set,
respectively. The supervised learning approach will try to suppress signal differences
that are irrelevant to the posture difference while emphasizing those signal differences
closely related to the posture difference. The learnt posture distance function is used
to align two motion sequences with dynamic programming. Experiments show that
both the proposed methods of posture difference estimation and motion synchroniza-
tion can provide satisfactory results. Additionally, we proposed to use VLMM for
learning atomic human actions, and developed a method converting a learnt VLMM
into an HMM. The main advantage of this approach is that the VLMM technique
can effectively learn the topology of a Markov model characterizing an atomic action.
Keyword: Multi-camera Calibration, MEMS, Behavior Analysis, Tracking
1
員必須時時注意其重心、 呼吸、 身形與步伐, 由入門至熟悉整個動作通常需要十數年光陰。 在
學習過程中更經常需要教練親身指導, 否則易有事倍功半之憾。 而在本計畫中欲研究的智慧
型太極拳輔助學習環境, 即是要利用多媒體技術加上生理回饋機制提供虛擬教練, 以輔助學
員調整其身心狀態。 現行的多媒體太極拳教學多是以影片方式進行教學, 因此缺少互動機制。
何況在太極拳練習過程中, 經常需要變化方位, 所以侷限在螢幕前的學習方式並不適用。
國立體育學院 [2] 利用 Motion Capture 技術將教練演練國術忠義拳的姿態擷取下來
以 Web3D 方式在網頁上呈現作為教學用途, 但是 Motion Capture 的設備以及創造高擬
真度的 VR 環境其成本都非常高, 而且單純將錄下來的運動資料以三維繪圖方式呈現雖然比
2-D 之視訊稍好, 但是缺少學員的姿態擷取/分析的步驟其互動性仍嫌不足。 卡內基美農大學
的 Entertainment Technology Center 在 Motion Master 計畫中發展了一套太極拳教學系
統(如圖 1.1 所示)[2], 這套系統加入了學員的動作擷取及顯示的功能。 學員與虛擬教練的姿
態皆由 VR 系統呈現, 在 Motion Master 一共實現了五種學習模式:
1. One on One: 一位虛擬教練位於學員的前方。
2. Four Teachers: 四位虛擬教練分別位於學員的前後左右四個方位。
3. Side by Side: 與第二種模式類似, 但為展示學員與教練的動作差異, 在四位虛擬教練
旁加繪學員的虛擬身影。
4. Superimposed 1: 將虛擬教練以線框 (Wire Frame) 方式畫出, 並重疊於學員的虛擬
身影上。
5. Superimposed 2: 虛擬教練改以骨架 (Skeleton) 方式畫出, 同時也將學員的虛擬身影
變為半透明再加上線框, 以突顯出學員與虛擬教練姿態之差異處。
這五種學習模式的優缺點則未見其作進一步的探討。姑且不論 Motion Capture 系統的
建置成本, 在作動作擷取時需穿著之緊身衣及十幾顆光學追蹤球(如圖 1.2 所示), 以及頭盔
式顯示器 (Head-Mounted Display) 皆會讓使用者望之卻步, 而且由電腦圖學系統所繪製出
來的圖形品質也還有許多的改善空間。
3
圖 1.2: 用於 Motion Capture 之光學追蹤球
5
§ 2.2 文獻探討
系統校正工作包含了多重攝影機校正、 投影機校正、 投影機導向裝置校正等三項工作。
其中有關於攝影機校正的研究成果數量非常龐大, 最早期的攝影機校正方法大部份是使用精
準校正目標物方法 [3, 4], 這種方法可靠但成本較高。 故大約在90年代初期即有人開始發展
不需校正目標物的攝影機自我校正 (Self-Calibration) 方法 [5, 6, 7, 8, 9, 10, 11], 這類
方法大部份需假設多張影像中的點對應關係為已知, 依影像中的特殊結構而可求得不同層次
的 (Projective, Affine, Metric) 攝影機參數。 攝影機自我校正的方法雖然看似較無限制, 但
是相對的使用這些方法必須要有較專業的知識否則若點對應計算不當則容易得到不穩定的計
算結果。 Zhang 整合了這兩類方法的優缺點, 發展了一個只需要一個平板校正目標物的方法
[12]。 Zhang 的校正法不需要昂貴的儀器, 只需要手持著平面校正板在不同方法取像, 即可求
得攝影機參數。 Zhang [12] 的方法在用於傳統的單眼或雙眼視覺系統的校正時效能不錯, 所
以也被收入 Intel OpenCV [13]中。 由於有共享的程式碼可用, 所以更加速其獲廣泛採用的
程度。
必須注意的是傳統的攝影機校正法在用於多攝影機校正時通常會遭遇許多困難, 主要的
原因是校正目標物通常無法同時出現在不同方向攝影機的視野內。 所以有些研究者便引入雷
射量測裝置來測量校正板的位置 [14], 如此一來即可得到精準的校正目標物, 而可以傳統的
攝影機校正法 [3, 4] 來估測攝影機參數。 Barreto and Daniilidis 也提出一個概念上類似的
方法 [15], 他們使用一對經過校正的攝影機取代雷射作為量測裝置, 利用這對經過校正的立
體攝影機可即時對視野中的校正目標物進行三維量測, 而後再以量測所得座標值來校正其餘
攝影機。 另外也有人預先校正好攝影機的內部參數 (Intrinsic Parameter) 再將之架設到預
定位置, 如此只剩攝影機的外在參數 (Extrinsic Parameter) 需要估測 [16], 因此可大幅簡化
多重攝影機的校正問題。 Baker and Aloimonos 則提出一個使用刻有精確校正圖樣之平板校
正物輔助, 來校正多重攝影機 [17]。 有趣的是攝影機自我校正技術雖然用在不含校正物的影
像中易因對應點誤差大, 致使校正效果較不可預期, 但是若使用校正物, 則可大幅簡化校正程
序。 Baker and Aloimonos [18] 與 Svoboda [19] 提出藉由舞動一個 LED, 以校正多重攝影
機。 但是攝影機自我校正的結果通常只能提供 projective 重建的結果, 若要還原 metric 重
建, 則必須提供額外的限制。 在 [18] 中, 是使用一個長度已知之校正棒。 而在 [19] 中, 則是
使用攝影機之各座標軸正交之條件, 利用八台攝影機即可解出所需參數。 另外 Chen 等人則
7
圖 2.1: 簡易單擺示意圖
單擺的運動方程式如下所示:
d2θ
dt2
= λ1
dθ
dt
+ λ2 sin θ, (2.1)
在單擺軸承上, 我們裝設了一個旋轉編碼器, 以便記錄單擺的角度隨著時間而變化的狀態(如
圖 2.2 所示)。 所測得的資料可用來估測方程式 (2.1) 中的單擺摩擦係數與擺長-重力加速度
比。
圖 2.2: 單擺擺盪時間和角度關係圖
使用單擺校正攝影機必須求得單擺影像位置與角度的關係, 才能推得擺錘的 3-D 位置:
R cos θ
R sin θ
1
 , (2.2)
9
因為使用 Gray code 作為校正圖形, 所以這些校正用的對應點可取得相當密集, 以至於
我們可以利用這些對應點更進一步建構出非線性的對應關係。 在本研究中, 我們以 NURBS
曲面描述由攝影機至投影機影像平面, 包含了透鏡失真的非線性對應關係。 由投影機至攝影
機影像平面亦可經由同樣的程序求出。
§ 2.3.3 旋轉式投影機校正
旋轉式投影機有兩種主要的型式(如圖 2.3 所示), 一種是直接利用馬達旋轉整個投影機,
另一種則是利用可旋轉的鏡面將投影方向導向欲投影的位置。 這樣的機構基本上有兩個自由
度, 在校正上我們必須處理機構的運動參數校正以及投影參數的校正。 因為要估測機構的運
動參數, 所以現有的校正法都頗為複雜。
圖 2.3: 旋轉投影機的兩種基本型
在絕大多數的應用中, 旋轉投影機與投影幕的關係是固定的, 所以進行複雜的三維校正
不一定是必須的。 在本年度的研究中, 我們發展了一套簡便的校正法。 這個方法的主要核心
有下列幾點:
1. 旋轉投影機投影範圍自動定位系統: 以 PTZ camera 為取像系統, 自動在投影幕中計
算出投影機的投影範圍。 在偵測時 PTZ 攝影機會先回到預設位置, 並 Zoom out 到最
大視野, 以便快速定出投影範圍的中心位置, 然後調整 PTZ 的數值使得投影範圍正好
落在攝影機的取像範圍。 因為在校正過程中投射的圖案可隨需求變化,所以可以精準的
完成此一任務。
11
圖 2.4: 本國勞工身體尺寸統計值
2. 使用 PCA 方法分析各個人體姿態輪廓之特徵。 分析後可以取得資料庫中, 標準人體姿
態之輪廓, 以及各人體姿態與標準姿態之差異量。 {4Si|i = 1, 2, ..., n}, 每個 4Si 各
代表標準身裁至另一種身裁 (如胖、 壯、 高、 瘦、 矮) 在不同姿態下的身形差異向量。
3. 透過標準姿態及標準姿態之差異量, 加上調變參數 , 即可合成標準姿態形變至目標姿
態之人體輪廓圖形。
Smodel = S + λ4˙Si (2.6)
圖 2.5: 人體姿態之人形輪廓
13
§ 2.4.2 固定式投影機校正
校正所得的固定式投影機至固定攝影機之影像座標對應圖則顯示於圖 2.9 之中。 值得注
意的是這對應關係中包含了攝影機與投影機的非線性光學透鏡失真。 所以其對應值比單純用
Homograpy 來還精準。
圖 2.8: 實驗中取到的垂直與水平的第五個位元 gray code 的影像
圖 2.9: 攝影中所取得之投影機垂直與水平 gray code 交點座標圖
§ 2.4.3 可旋轉式投影機校正
在可旋轉式投影機的校正研究方面, 我們先以電腦模擬校正之過程, 用以驗証我們所提
的方法是否可行。 圖 2.10 為電腦模擬過程中, 所建構出的 {pi(α, β)|i = 1, 2, 3, 4} 四個函
數表。 圖中的每一條線 (iso-curve) 對應到每一個固定的 α 或 β 角度值。 由這個模擬結果顯
15
表 2.1: 實驗室成員中九人的身體各部份長度數據表 (單位為公分)
身高 頭 身軀 上手臂 下手臂 手掌 上腿 下腿 腳掌
1 172 20 56 31 26 19 42 37 7
2 181 24 56 33 26 20 46 36 7
3 147 21 46 26 21 16 43 32 6
4 161 20 50 30 23 18 43 33 6
5 172 18 54 33 25 18 44 35 7
6 168 22 53 33 24 19 43 36 7
7 177 20 56 34 25 18 46 35 7
8 174 20 54 35 26 19 45 36 7
9 164 21 51 33 24 18 42 35 7
手側展站立姿態調變過程。 注意在第二列中最右側為變化至較為矮小身裁不同動作的結果。
圖 2.11: 標準姿態形變至單腳站立姿態調變過程
17
第 三 章
姿態差異分析、 師生動作同步, 以及動作分
析
§ 3.1 前言
在本計畫中, 我們建構了一個智慧型太極拳輔助學習環境, 教練在此一環境中錄製其演
練太極拳的身影, 經編輯處理後, 以虛擬教練方式呈現教導學員。 學員在這個智慧型空間中可
以透過同步技術跟著虛擬教練學習太極拳。為了降低計算量, 達到即時比對的功效, 我們採用
了 MEMS 感測器協助計算人體姿態之差異。 在這一部分, 研究成果包含以下三點:
1. 估測師生姿態差異: 發展利用 MEMS 訊號計算師生姿態差異估測技術, 作為同步計算
之依據。
2. 師生影像序列之同步技術: 師生影像同步技術包含兩項工作, 第一項是對兩個 MEMS
訊號序列找出對應的關係, 第二個工作則是利用此一對應關係估測出播放的速度。 我
們假設學員一開始已指定好要演練那一式的太極拳, 再由訊號序列所提供的相對應訊
號找出對應的關係。
3. 行為分析: 發展行為分析模組, 方便感知學員在這個智慧型學習空間中的意圖, 以提供
更妥適的操作介面。
19
的方法去估測在小提琴演奏時琴弓的速度。
3. 結合姿態重建的動作分析: Mayagoitia 等人 [42] 結合了加速規及 rate-gyro, 發展了
一套光學的動作分析系統去估測在 Sagittal plane 上的關節角度、 速度及前臂的加速
度。 Zhu 及 Zhou [43] 提出了一套使用感測器模組即時的追蹤人類動作的方法。 每一
個感測器模組都包含一個三軸加速規、 一個三軸陀螺儀及一個三軸的磁力感測器。 十
五個感測器穿戴在身體的十五個部位上。 因此, 磁力感測器可以計算使用者的位置及
走向, 使用者的姿態可以透過一個線性的卡曼濾波器來估測。 在計畫的第二年度裡, 我
們的感測器配置便是參考 [43] 的穿戴方式。
§ 3.2.2 師生姿態同步
太極拳運動的過程中, 教練常會隨心神氣合的變化而改變在打拳的速度, 反之學員因不
熟練的關係, 也容易會有打的速度太快或太慢的情形發生, 造成學員在學習太極拳運動時速
度和教練的差異。 姿態比對的前題在於必須將欲相互比對的訊號組作同步, 近年來動作同步
(motion synchronization)的相關研究突破相當有限,最常見的方法就是使用Dynamic Pro-
gramming。 Dynamic Programming是一個很常用解決最佳化問題的方法,其精神在於將原
始的問題拆解成一個個小問題去求解, 透過解決每一個小問題所得到的答案去找出原始問題
的最佳解。 其中 Dynamic Time Warping演算法很常用來做同步的應用,如音樂的同步或是
動作的同步等等。 Veeraraghavan 等人 [44] 針對兩個相同的連續動作使用 Dynamic Time
Warping 的方法去解決其同步的問題, 而 Jang 等人 [45] 及 Nishimura 等人 [46] 則是
應用在音樂的同步研究上。 其不適用於本計畫需求之處在於該方法需要給予開始及結束的動
作, 藉此在該段運動訊號內作 Time Warping 之運算。 因此在本計畫中直接使用 Incremen-
tal Dynamic Programming 的方法對於受測者輸入的訊號作動態的同步, 透過每一筆訊號
的輸入及時的執行其同步運算, 期待達到及時 (Real Time) 給予學員在執行太極拳運動時的
一些協助資訊。
§ 3.2.3 人類行為分析
人類行為分析在近年來成為廣受重視的研究題目,此一研究主題可細分為人物偵測 (Hu-
21
§ 3.3 研究方法
§ 3.3.1 使用微機電感測器 (MEMS Sensor) 估測姿態差異
我們設計了一套可穿戴的微機電感測器系統, 利用 MEMS 訊號來學習姿態的差異量,
並用於同步技術上。 整個系統會預先取得太極拳老師穿戴著感測器演練一套太極拳時的感測
器訊號及影像序列, 這些訓練樣本經過機器學習的機制後可以得到一個距離函數, 當學員在
演練太極拳運動時, 身上穿戴的感應器所取得加速度及先前學習的距離函數, 經由動態規劃
演算法便可以使虛擬教練 (預先取得的老師演練影像) 與學員的動作同步。 本系統包含下列
幾個部分:
1. 加速規模組: 我們在人體十五個部位裝設十五個加速規模組, 每個加速規模組都可以
求得三軸對地的加速度, 因此每個姿態都可以得到一個四十五維的向量供系統分析。
2. Metric Learning: Metric Learning 是一種機器學習的方法, 主要的目的是拉近兩個
相似資料的距離, 將兩個不相似的資料距離拉遠。 在訓練過程中, 請專家先對於訓練樣
本做分類, 分出哪些是相似的資料, 哪些是不相似的資料, 接著透過 Metric Learning
演算法學習出一個 metric, 也就是一個距離函數 (Distance Function), 餵給馬氏距離
(Mahalanobis Distance) 當做 Covariance Matrix 使用。 從此以後, 執行漸增動態規
劃演算法 (Incremental Dynamic Programming Alogrithm) 時, 便使用馬式距離來
找出最佳路徑。
3. 漸增動態規劃演算法(Incremental Dynamic Programming Algorithm): 由於虛擬教
練的訊號會事先取得, 當學員在演練太極拳時, 第一步我們會先計算出虛擬教練與學員
的價值表, 第二步則追溯 T 步最佳路徑。 每次在計算價值表時, 只需要計算學員新增
的資料比對部分, 即可算出漸增的價值表。 每次都固定往回溯 T 步, 利用這 T 步計算
學員相對於虛擬教練的速率平均值, 作為控制虛擬教練影像播放速率的參數。
穿戴式系統架構圖如圖 3.1 所示, 三軸加速規模組的設計概念主要是由一顆三軸加速規
感應器所組成的, 此感應器會同時送出三個軸向的類比電壓 (分別為 X, Y, Z) 及一個參考
電壓 (V ref)。 X, Y, Z 會隨著三軸加速規模組擺設方式或擺動方式而送出相對應的電壓值,
23
圖 3.2: (a) H34C 功能方塊圖,(b) H34C 腳位定義。
如圖 3.2 (a), H48C 可以送出 (X, Y, Z, V ref) 類比電壓, X, Y, Z 會隨著擺動或震動
而送出相對的類比電壓, 在實做中要將 H48C 及時運作便將 STBYB 腳位設定為 high, 其
腳位功能說明如圖 3.2 (b)。 圖 3.3 展示了 H34C 三軸加速規的座標系統, 圖中可知 H34C
三軸加速規是屬於左手座標系, 藉此座標系統我們即可從計算出的加速度評估感應器擺動的
方式。 H34C是一顆極為敏感的感應器,隨著感應器震動、移動或擺動方式而會送出不同的電
壓。 在 圖 3.4 展示了感應器靜止不動時, 根據不同擺設方式而送出相對的電壓。
圖 3.3: H34C 三軸加速規晶片座標系統
我們發展了一套可穿戴式的感應器系統, 此套系統可以偵測每個肢體部位運動過程中改
變的加速度訊號, , 讓我們得以從收到的數據中進一步解析穿戴者的運動狀態。 我們將三軸
加速規模組裝設在人體的十五個肢體上面, 見圖 3.5 (a), 每相鄰一對的三軸加速規模組都
會對應到一個關節角度, 如圖 圖 3.5 (b) 所示。 透過一個微處理器 (Microprocessor) 收集
訊號並整理成一個四十五維度的向量, 再透過 RS-232 傳輸受測者運動訊號至電腦端進一步
25
§ 3.3.2 Metric Learning
我們安排了六位受測者穿著感測器分別做出兩套太極拳動作的關鍵動作 (Key Posture),
得到經由穿戴式感應器系統得到相對的訊號,及從錄製的影像中擷取相對關鍵動作之影像,再
由專家在這些姿態影像中指出相似及不相似的姿態, 作為訓練樣本提供給 Metric Learning
演算法來估測馬氏距離函數 (Mahalanobis Distance Function)。
馬氏距離 (Mahalanobis Distance) 於 1936 年, 由一位印度統計學者馬哈拉諾比斯
(P.C. Mahalanobis) 所提出。 該方法特性為與尺度無相關 (Scale invariant), 也就是可以單
獨量測尺度 (Scale)。此方法可以有效的分辨出一個未知樣本與已知樣本之間的相似度。一般
而言, 馬氏距離很容易辨識出樣本間是否為同一類型的。 如果樣本之間相似度很高, 則所算出
來的馬氏距離將非常小; 如果樣本之間相似度非常低, 則計算出來的馬氏距離將放大其效果。
因此, 馬氏距離是一個很適合作為判斷樣本間的相似度, 可當成分類重要特性的基礎。馬氏距
離考慮到各種特性之間的相關性, 在統計學中, 相關的意義是用來衡量兩個樣本相對於其間
相互獨立的距離。 馬氏距離為考慮多種樣本協方差矩陣 (Convariance Matrix) 的一種統計
距離。 如果只有兩個樣本, 樣本一及樣本二, 則觀測值 i 和 j 的馬氏距離公式表示如下:
dM(xi, xj) = (xi − xj)TM(xi − xj), (3.1)
其中,xi =
 xi1
xi2
, xj =
 xj1
xj2
, M 是 i 和 j 的協方差矩陣。 上式的公式如果多種樣本
有 k個時, 則 xi 和 xj 各為一個 k×1 的矩陣, M 是一個 k×k 的協方差矩陣。 由此公式可以
得知, 如果樣本一和樣本二沒有任何相關性時 (表示無任何線性關係), 則 M 就會變為一個
對角線為 1 的單位矩陣, 此時馬氏距離就可以簡化為歐氏距離 (Euclidean Distance), 歐氏
距離公式表示如下:
dE(xi, xj) =
√
‖xi − xj‖2. (3.2)
在執行 Metric Learning [55] 前, 需要先收集充分的樣本數量, 該樣本集合分類為相似
集合以及不相似集合, 樣本之間的關係可透過馬氏距離公式求得, 並遵守其限制條件, 限制條
件表示如下:
dM(xi, xj) ≤ u (i, j) ∈ S
dM(xi, xj) ≥ l (i, j) ∈ D
, (3.3)
27
Algorithm 1 Metric Learning Algorithm
Require:
X: 輸入一個d× n的資料集合
S: 成對相似的資料集
D: 成對不相似的資料集
u, l: 馬氏距離的臨界值 (thresholds)
M0: 輸入馬氏矩陣
γ: 懲罰參數。 當分類結果與預設不一樣時, 即給相對的懲罰
c: 紀錄懲罰索引
Ensure: M : 馬氏矩陣
1: M ←M0
2: λi,j ← 0 ∀ i, j
3: if ξc(i,j) ≤ u then
4: (i, j) ∈ S
5: else if ξc(i,j) ≥ l then
6: (i, j) ∈ D
7: end if
8: while divergence do
9: 選擇群組 (i, j) ∈ S 或者 (i, j) ∈ D
10: p← (xi − xj)TM(xi − xj)
11: if (i, j) ∈ S then
12: δ ← 1
13: else
14: δ ← −1
15: end if
16: α← min(λi,j, δ2( 1p − γξc(i,j) ))
17: β ← δα
1−δαp
18: ξc(i,j) ← γξc(i,j)γ+δαξc(i,j)
19: λi,j ← λi,j − α
20: M ←M + βM(xi − xj)(xi, xj)TM
21: end while
22: return M
29
Sj,j = 1, 2, ..., t, 其中 t 值為表示時間的整數, 因此隨著時間的增加而 t 值會越來越大。 每
次在執行比對時, 我們假設學員速度不會大於教練的 k 倍。 所以在每一個時刻時, 必須要對
Tkt 與 St 做一次動態規劃的匹配, 其中
Tkt = {Ti|i = 1, 2, ..., kt}
St = {Sj|j = 1, 2, ..., t}
, (3.8)
動態規劃的匹配包含兩個主要的步驟, 第一步是要計算一個二維的動態路徑價值表, 第
二步則是要追溯最佳的路徑。一開始會先將虛擬教練的訊號 Ti 存放在比對資料庫裡, 當學員
開始演練太極拳時, 根據學員演練速度快慢決定建立的價值表的大小, 這樣的改進方式可以
節省掉大部分的計算時間, 只是記憶體的需求會越來越大。 在 k 變得很大時, 就需要較長的
時間計算價值表, 但是使用此方法的優點是, 每當時間演進一步時, 我們不需要重新計算所有
序列的比對, 只需要計算目前這時刻新增的資料比對部分, 用空間換取時間的方式, 即可算出
漸增的動態規劃匹配結果。 但是在速率變化範圍受限的條件下, 我們可以很容易的去掉不可
能的對應區, 因此記憶體的需求也會降低。
由圖 3.6所示, 假設存放在資料庫裡, 教練訊號為 T1, T2, ..., T25, 學員此時刻進來的訊號
為 S7, 我們計算這時刻相對應的累計價值值, 在計算價值時同時需要紀錄相對應的路徑, 利
用 IDP 去控制虛擬教練演練太極拳的速度, 因此我們不用每次都回溯至原點的地方, 這樣
由終點 {S7, T14} 往回追溯 T 步 (此範例我們設定 T=3), 就可以得到最佳路徑相對應節點
{S7, T14;S6, T12;S5, T11}, 而所得到的最佳解便是在此時刻裡所找出的最佳路徑, 之後會隨
著學員訊號一直增長而一直找出不同的最佳路徑表。 每次我們都固定回溯 T 步, 透過這條最
佳路徑可以計算出教練影像的播放速率資訊, 並計算播放速率的平均值 (Relative Framer-
ate, 簡稱 RF) 作為控制下個時刻點需新增計算的價值表範圍 (δt), 同時作為控制教練影像
速率的參數。
圖 3.6: 動態規劃演算法例子說明, 當 T = 7 時。
31
圖 3.9: 動態規劃演算法例子說明, 當 T = 10 時。
在比對兩個不同訊號波形時, 會使用訊號波形間彼此的差異量及相似度來找出相對應的
訊號組合。 教練與學員的訊號透過 Metric Learning 學習出一個距離函數 (Distance Func-
tion), 將之代入馬氏距離, 即可讓越相近的訊號計算出來的累積價值 (Accumulated Merit,
簡稱為AM) 越大, 反之, 則越小。 透過最佳化 AM 的方式即可幫助執行 IDP 時, 更容易找
出最佳路徑。
計算 AM 時, 每個節點都會有相對應的三條路徑選擇 (↘, ↓,→), 為了考慮節點的最佳
路徑, 必須給予每條路徑不同的價值 (Merit), 才可以透過 AM 找出兩訊號最適合的對應點。
如圖 3.10所示, 為了走到目前時刻點 Ai,j 即有三種路徑選擇方式。
圖 3.10: 計算每個節點數的價值
我們輸入教練訊號 {T1, T2, ..., Ti, ...Tn}及學員訊號 {S1, S2, ..., Sj, ...St},使用 IDP演
算法比對訊號, 當 1 ≤ i ≤ n 且 1 ≤ j ≤ t 時, 我們即可計算出 Ai,j 每個節點的 AM, 當
一個資料被判斷為沒有對應點時, 其價值為 0, 反之則以 e
−dis2
2σ2 做為其對應價值, 這個對應價
值與被判定為對應點之資料距離成反比, 其中 σ 為一個設定參數。 AM 的計算公式如下式所
33
• RF= 1時, 表示虛擬教練影片播放速度與學員演練速度一樣快, 因此就根據目前速度
播放即可。
• RF< 1時, 表示虛擬教練影片播放速度太慢, 必須加速影片播放速度, 追上學員演練速
度才可。
以下為整個方法的流程:
• 在 traning 過程中, 固定幾組關鍵動作 (Key Posture), 從 MEMS Sensor 中擷取相
對的訊號及由錄製的影片中擷取出相對的關鍵動作之影像樣本, 經由專家在這些影像
姿態中指出相似及不相似的姿態, 透過這些姿態訊號代入 Metric Learning 中來估測
馬氏距離的距離函數 (Distance Function)。
• 在測試過程中, 受測者穿戴感應器系統演練太極拳動作, 將學員每筆測試的訊號代入漸
增動態規劃演算法中, 與虛擬教練訊號做同步及比對。
• 比對時我們使用 Metric Learning 訓練出距離函數, 配合漸增動態規劃演算法, 計算出
相對應的價值, 並以固定回溯 T 步找出最佳路徑解, 則可分析出學員演練的速度。
• 經由上一步得知學員的演練速度, 則虛擬教練影像播放速度則根據學員演練速度做適
當的調整。
流程圖程序:
§ 3.4 研究成果及討論
§ 3.4.1 可穿戴式感應器系統
我們設計的可穿戴式感應器系統,主要是由十五顆加速規模組所組合成的 (如圖3.12 (a)
所示), 穿戴至受測者身上各部分 (如圖 3.12 (c) 所示)。 每顆加速規模組分別送出三軸之訊
號, 經由微處理器模組集結成一個四十五維之訊號 (如3.12 (b) 所示), 透過 RS-232 傳送受
35
圖 3.12: (a) 三軸加速規模組 (b) 微處理器模組 (c) 加速規穿戴於受測者身上情形。
量測數據時, 我們將加速規模組分別做八個不同角度的取樣,圖 3.14展示其結果,圖中
正方形的點代表我們實際計算出來相對的八對由 MicroScribe 與 MEMS Sensor 分
別的量測結果, 虛線則代表理想的角度對應關係 (斜率為1, 且穿過原點), 在此圖中, 可
以驗證此系統的精確度相當高。
37
• 姿態差異實驗結果
實驗中,我們安排了六位受測者,分別做出兩套太極拳運動的關鍵動作 (Key Posture),
從中擷取相對的訊號及由錄製的影片中擷取出相對的關鍵動作之影像。 將這些影像集
結起來, 經由專家評估影像中姿態是屬於相似姿態或者不相似姿態。 將這些分類過後
姿態的訊號代入 Metric Learning 中, 學習出一個 Distance Function, 作為之後評估
訊號相似度的標準, 圖 3.16 及圖 3.17 展示了兩套動作的相似及不相似的姿態影像。
為了展示經過 Metric Learning 後的成果, 我們將每個姿態分別使用歐氏距離及馬氏
距離作為比較,如圖 3.15所示,其中橫軸為兩訊號的距離差異,縱軸為使用 Histogram
畫出每個距離的機率值。 相似族群由藍色線條表示, 而不相似則以紅色線條表示。 由圖
3.15 (a) 中可知, 使用歐氏距離不管相似群或者不相似群都會堆疊在一起, 而圖 3.15
(b)中可知,訓練過後的馬氏距離則可區分出兩個族群。所以學員在演練太極拳動作時,
便可透過與教練的每個姿態訊號做比較, 找出相似訊號, 以便控制影像播放速率。
圖 3.15: (a) 四十五維訊號的歐氏距離相似群 (藍色) 及不相似群之結果 (紅色)。 (b) 馬氏距
離相似群及不相似群之結果。
39
圖 3.17: 棚捋擠按之相似群訊號及不相似群訊號動作
41
圖 3.18: (a) 學員在 t=200 時刻, 回溯 T=70 步的匹配結果, (b) 學員在 t=300 時刻, 回溯
T=70 步的匹配結果。
圖 3.19: (a) 學員在 t=200 時刻, 回溯 T=70 步的匹配結果。 (b) 學員在 t=300 時刻, 回溯
T=70 步的匹配結果。
43
圖 3.22: 師生影像非同步結果 (“雲手”二)
45
圖 3.24: 師生影像同步結果 (“雲手”二)
47
圖 3.27: 師生影像同步結果 (“棚捋擠按”一)
49
§ 3.4.2 行為分析研究方法
使用 VLMM 做行為辨識包含了兩個主要的步驟, 第一個步驟是要由訓練資料中建構出
VLMM, 而第二個步驟則是要將 VLMM 轉為 HMM。 每一個 VLMM 皆可表為以列之參數
, 其中
• S 代表一組有限的狀態集合, 每一個狀態皆有一個符號字串作為其狀態轉換的條件記
憶。
• V 代表一個有限的觀測字符集 (Alphabet)。 在本研究中, 我們使用人物剪影作為輸
入, 並利用以下之流程圖 (圖 3.29) 來建構 V。 其中剪影形狀比對的部份是以 Shape
Context [58] 為基礎, 加以改造為 CSCD (Convex-hull Shape Contexts Distance)
[54] 以加快運算速度。
• τ : S × V → S 觀測到新字符後的狀態轉換函數,τ(sj, v)→Sj+1。
• γ : S × V → [0, 1] 是輸出機率函數, 滿足 ∀s ∈ S,∑v∈V γ(s, v) = 1
• pi : S → [0.1] 則是初始狀態的機率函數, 滿足 ∑s∈S pi(s) = 1
圖 3.29: 觀測字符集的建構流程
這些參數皆可由訓練資料所建構出來的 PST (Prediction Suffix Tree) 中求得 [59]。
VLMM 的最大好處是其記憶體的長度是可隨資料的特質來調變, 為了調整 VLMM 的記憶
體, 加權過的 Kullback-Lievler (KL) Divergence 被引用來分辨不同記憶長度的機率值有沒
有明顯的差異。 若長短不一的記憶長度所產生的機率分佈類似的, 則系統會自行捨棄較長的
51
si|Xt−1 = si) = 0)。 所以在最後一個步驟中, 我們由未剔除重覆姿態的原始輸入序列學習
P (Xt = si|Xt−1 = si) 的機率, 再將之代入模型中。 如此一來, 即可將 VLMM 轉換為
HMM。 而行為辨識的結果可以下列方程式計算之:
i∗ = arg max
i
log [P (O|Λi)] (3.15)
• 行為分析研究成果
我們使用兩組方法來測試我們提出來的行為分析方法。 第一組資料如圖 3.31 所示, 共
包含十種動作序列, 共有九位體型不同的人參與實驗(參見圖 3.30)。 辨識實驗結果如
Table 3.2 所示。 為了比較, 我們也利用 HMM 技術來辨識相同的資料序列。 但是因為
HMM 最佳的拓樸結構為未知, 故我們使用常見的 Left-Right Model, 搭配由 5 至 30
等不同數量的狀態, 經過參數訓練之後用來辨識這十個序列。
圖 3.30: 九位參與實驗者的體型
這一個實驗中, 我們觀測到在辨識不同動作序列時, 最佳的 HMM 拓樸並不相同, 為了
呈現這一點, 我們將 Table 3.2 中的最佳 HMM 辨識結果標以灰色的底色。 這個實驗
結果顯示在使用 HMM時必須嚐試不同的拓樸, 以求得最佳成果的困難。相反的, 使用
我們所提出來的方法時並不會遇到這種困擾。 VLMM 會自行決定好的的拓樸結構, 而
其辨識結果在這個實驗中也是表現最佳的。
第二組實驗數據是使用公開的資料庫 (見圖 3.31, 含九個人, 十種動作) 的測試結果。
在 Table 3.3中, 我們使用兩種學習的方法, 第一種是僅使用一個人的輸入序列來訓練,
而用其它人的資料來測試 (記為 leave-eight-out)。 另一個學習的方法是使用八個人來
訓練, 而用另一個人的資料來測試 (記為 leave-one-out)。 其中第二種訓練的方法與文
53
表 3.2: 使用第一組資料之辨識結果
表 3.3: 使用公用資料庫測試結果
55
第 四 章
結合微機電系統感測器與視覺感測器追蹤三
維人體姿態
§ 4.1 前言
人體動作資訊在電腦視覺、 醫學、 電影與遊戲電玩等領域裡扮演很重要的角色, 例如在
醫學領域中醫生藉由擷取病人的動作來評估病人復健的狀態, 遊戲電玩領域透過擷取人體動
作來創造更你真的角色動作, 在電影領域中更經由大量的虛擬角色擷取演員的動作來創造驚
人的動作特效。 此外還有許多領域常藉由擷取人體動作來進行人類動作分析, 應用於虛擬實
境、 視覺化人機介面、 智慧型監視系統等用途, 故動作追蹤 (Motion Tracking) 研究一直是
很受重視的研究主題之一。 但目前動作擷取 (Motion Capture) 的設備昂貴, 且使用場地需
受限於動作擷取所採用的技術、 安裝麻煩及所搭配軟體學習不易等因素, 故發展一套低成本、
容易操作的動作擷取系統是必然的趨勢。
§ 4.2 困難點
追蹤人體姿態有許多因素會提高這類研究的困難度, 下述項次為在追蹤人體姿態時常遇
到也是很有挑戰性的問題。
• 複雜的背景: 人體姿態追蹤常常需要先做前景背景切割, 在這一步若能做得好, 後續的
57
§ 4.3 研究方法
由第二年的文獻探討 3.2.1 及研究成果 3.3.1 可以了解微機電感測器所提供的訊號數據
是個很值得參考的資訊, 也在許多領域的應用上有一定程度的貢獻。 在第三年裡, 我們利用微
機電感測器所提供的訊號資訊, 結合視覺感測器來追蹤三維人體姿態。 由於人體屬於一個高
自由度的形體, 高維度的運算讓系統整體效能難以往上提升, 是否有什麼辦法能讓我們所建
立的運動學模型 (Kinematic Model) 在追蹤人體姿態時, 能夠有效的降低搜尋空間 (Search
Space) 的維度又同時能維持追蹤的精確度呢? 在這一年我們也針對了這個問題做了些探討。
在姿態追蹤方面,首先我們發展了一套使用階層式的貝氏濾波器 (Bayesian Filter)結合微機
電感測器來追蹤三維人體姿態的追蹤系統, 該套系統能夠解決單由視訊無法解決的“遮蔽”問
題。 接著我們由子計畫一獲得粒子濾波器的追蹤技術, 以此為基礎再結合微機電感測器, 發
展了一套三維人體姿態追蹤系統。 該系統的論文將發表在 National Computer Symposium
2009 全國計算機會議上。
§ 4.3.1 運動學模型限制 (Kinematic model constraint)
追蹤三維人體姿態通常需要建構一個三維模型, 一旦可以建構出一個極相似人體的三維
模型 [64], 追蹤的正確率便可以大大提升, 同時研究的困難度也就會降低不少。 人體是由一百
多個關節所組成的一個高自由度軀體, 一個看似普通的姿態, 背後常常包含著繁複的關節組
成。 建構一個完整符合人類肢體自由度的三維模形並不符合經濟效益, 該思考的問題應該是
如何讓所建構出的簡易三維模型能更真實的反應出人類的動作。 我們發現人體有許多部位倚
靠著某種線性或非線性的關係連接著, 這些關係可以提供給三維模型更真實的反應出真實的
人體姿態。
• 硬體配置
不同於第二年計畫所述之參考 [43] 的感測器佩戴方式及數量, 我們只在使用者身上佩
戴了九個加速規, 分別裝在身體、 左上臂、 右上臂、 左前臂、 右前臂、 左大腿、 右大腿、
左小腿及右小腿上, 位置如圖 4.2, 圖 4.3 則為使用者真實配戴微機電感測器的情況。
• 人體運動學模型
59
圖 4.3: 真實配戴情況
轉換矩陣可以拆解成:
iTi+1 =
i Ti′
i′Ti+1, (4.1)
其中
iTi′ = Qi = Rotz(qi), (4.2)
i′Ti+1 = Vi, (4.3)
Rotz(θ) =

cos θ − sin θ 0 0
sin θ cos θ 0 0
0 0 1 0
0 0 0 1
 , (4.4)
Trans(
[
x y z
]t
=

1 0 0 x
0 1 0 y
0 0 1 z
0 0 0 1
 , (4.5)
qi = si q
′
i, (4.6)
61
表 4.2: 各部位的運動範圍
Body part DOFs Terms of movements
Abduction/Adduction
LUA/RUA 3 Extension/Flextion
Lateral rotation/Medial rotation
LFA/RFA 2 Extension/Flexion
Supination/Pronation
Abduction/Adduction
LT/RT 3 Extension/Flexion
Lateral rotation/Medial rotation
LS/RS 2 Extension/Flexion
Circumduction
個人的肌肉生長不同, 所旋轉的角度都不一樣, 但在三維模型裡, 前手臂跟上手臂會猶如一個
剛性結構, 完整的旋轉相同角度。此時的感測器所感測出的數據已不能相信, 因為三維模型無
法反映出這類的非線性情況, 這類的情況便依賴粒子濾波器來尋找出一個最佳解。此外, 當手
臂在做 Abduction/Adduction 時, 由圖 4.6 可以觀察出, 配戴在上手臂的感測器位置會因
為皮膚滑動而產生移動的狀況。
我們試著要找出人體部位間的連接關係, 以手臂為例, 將感測器佩戴在手臂上便會發生
上述的非線性問題, 我們透過 CPC 運動學模型來探討上手臂跟前手臂之間的關係, 假設上
手臂的位置固定了, 很直覺的我們可以想像前手臂的搜尋維度似乎會受到某種限制, 其關係
式依照 (4.10) 可寫成: 
u
v
w
 = WT3V3Q4V4Q5Vacc2

x
y
z
 , (4.11)
經過化簡, 前手臂的部分可得:
sin θ1 cos θ1 cos θ2 − cos θ1 sin θ2
0 sin θ2 cos θ2
− cos θ1 sin θ1 cos θ2 − sin θ1 sin θ2


x
y
z
 =

u
v
w
 , (4.12)
63
0 20 40 60 80 100 120
0
20
40
60
80
100
120
The included angle between forearm and ground
Th
e 
in
cl
ud
ed
 a
ng
le
 b
et
we
en
 u
pp
er
ea
rm
 a
nd
 g
ro
un
d
圖 4.7: 上手臂在做 Lateral/Medical rotation 時與前手臂的非線性關係
65
接下來我們利用貝式法則, 將 xt−1 移至已知 zt−1 後發生 xt 的條件機率項如 (4.16)
式。
p(xt|z1:t−1) =
∫
p(xt|xt−1, zt−1)p(xt−1|zt−1)dxt−1 (4.16)
化簡為察普曼-科莫高洛夫方程式 (Chapman-Kolmogorov Equation) 如 (4.17) 式,
其中 p(xt|xt−1) 為狀態轉移機率 (state-transition probability)。
p(xt|z1:t−1) =
∫
p(xt|xt−1)p(xt−1|z1:t−1)dxt−1 (4.17)
在狀態轉移機率中, 在本研究中主要關聯到動作模型 (Motion Model) 為主, 而動作模
型主要可分為以下三種:
– 等速度(Constant Velocity)
– 等加速度 (Constent Acceleration Model)
– 隨機遊走 (Random Walk)
如果已知狀態變化模式可使用等速度、 等加速度模型等來計算狀態轉移機率。 由於我
們事先無法得知姿態的變化模型, 因此我們採用 Random Walk 模型來計算我們的狀
態轉移機率, 在本研究中使用高斯模型 (Gaussian Model) 來計算狀態轉移機率。 公式
(4.18) 為更新方程式, 其中 ct 為正規化 (Normalization) 方程式。
p(xt|z1:t) = c−1t p(zt|xt)p(t|z1:t−1) (4.18)
ct =
∫
p(zt|xt)p(t|z1:t−1)dxt (4.19)
• 相似度機率
由於無法得知受測者姿態連動的模式, 因此我們假設受測者的運動模式為一個高斯分
佈隨機遊走模式。我們定義貝式分析的狀態為人體關節角度, 如表 4.3我們根據關節自
由度將人體定義為 12 個關節, 其中關節自由度的定義我們參考 [68] 一書所制定。而觀
測值在本研究中主要使用拍攝受測者的剪影與擷取受測者的 MEMS 感測器資料, 並
且其計算其相似度機率值作為貝式分析之觀測值。
– 影像相似度機率計算
我們使用高斯函數來求取影像相似度機率值, 其中誤差函數我們使用受測者剪影
與剪影模型產生之剪影像素進行 XOR計算取得。 如公式 (4.20)所示, 其中 f(x)
67
我們同樣使用高斯函數來計算 MEMS 相似度機率。 在此之前, 我們已事先虛擬
了三維人體模型上的虛擬感測器 (在下一節會有詳細介紹), 模擬出虛擬感測器於
某姿態下所對應的虛擬三軸的重力向量, 虛擬感測器的資料將與實際感測器代入
高斯函數作為 MEMS 的相似度機率計算的誤差函數。
我們參考 [43] 將人體定義為 15 個片段, 每個片段使用一個 MEMS 感測器來感
測人體部位的移動變化, 每個感測器有三個軸的輸出值, 因此每個姿態的 MEMS
相似度機率計算需計算 45 維的資料量。 假設虛擬感測器 X 軸分量為 US, 實際
感測器 X 軸分量為 UG, 我們計算 US 與 UG 的向量夾角作為高斯函數的誤差
函數如公式 (4.22), 公式中 i 表示第 i 個 MEMS 感測器。 同樣的, 我們藉由調
整 λ 值來調整誤差函數的容忍度。
p(ZSensor,XSensor) = exp(−λ(a cos−1(uGi )− cos−1(uSi ))2) (4.22)
• 階層式貝式濾波器 (Hierarchical Bayesian Filter)
由於貝式狀態向量是一個維度相當高的高維度空間, 因此如何有效率的找出與受測者
近似的姿態是一個很重要的課題。 Stenger 等人 [69] 的研究中設計了一個有效率的樹
狀搜尋演算法, 並且使用網格濾波器 (Grid-Based Filter) 找出一個近似的貝式最佳
解。 雖然我們窮舉了所有的狀態, 但是實際上不可能獲得所有的狀態。 因此我們對這些
狀態取樣, 使用離散的狀態 R 來表示某狀態, 在追蹤人體姿態的問題中,R 為關節可轉
動的區間, 並且將貝式狀態向量為表示為 x ∈ Rn。
首先我們先將 R 區分為 Nl 個分割 {Si,l} 在 l 層的階層樹如公式 (4.23), 圖 4.10 為
階層式分割後的狀態空間示意圖。
Nl⋃
i=1
Si,l = R for l = 1, . . . ,L. (4.23)
一開始我們先將狀態空間粗分為四個區域, 而第二層灰色區域為原先第一層區域之子
分割, 由此圖可以看出當分層越多時其狀態空間則切割的越細。 因此我們定義一個不
連續的機率分布來表示 Si,l 這些區域如公式 (4.24)。
p(xˆi,lt |z1:t) =
∫
xt∈Si,l
p(xt|z1:t)dxt (4.24)
圖4.11為一個人體姿態之狀態空間範例, 在第一層中我們將姿態粗分為 4 個姿態, 第二
層為上一層節點再細分其雙親節點之狀態, 依據這個概念我們可以建構出一個由粗略
姿態至個較為精細之人體姿態樹。
69
圖 4.11: 人體姿態之狀態空間範例
圖 4.12: 離散的方式計算連續狀態空間之相似度機率值
當 l > 1:
p(xˆj,l1 |z1) =
 p(z1|xˆ
j,l
1 ), if p(xˆ
par(j),l−1
1 |z1) > np(xˆl−11 |z1)
p(xˆ
par(j),l−1
1 |z1), otherwise,
(4.26)
當計算出l的所有節點後, 將此層進行正規化
∑Nl
j=1 p(xˆ
j,;
1 |x1) = 1
建立第一層樹節點後, 我們將該節點狀態代入剪影模型產生虛擬人體剪影與虛擬
感測器資料, 與輸入的資料來計算該節點之相似度機率值。 根據 (4.21) 和 (4.22)
求得影像與 MEMS 的相似度機率值後, 將兩機率值相乘作為該節點之相似度機
率值。由於描述人體姿態是一個高維度的空間, 如果計算所有的節點, 階層樹每一
層節點數量將以指數成長, 將耗費過多的時間與計算機資源。 因此計算該層樹節
71
圖 4.14: 追蹤人體姿態流程圖
階層 l = 1:
p(xˆj,11 |z1:t) = p(zt|xˆj,1t )p(xˆj,1t |z1:t−1), (4.27)
p(xˆj,11 |z1:t−1) =
NL∑
i=1
p(xˆi,Lt |z1:t−1)p(xˆi,Lt−1|z1:t−1), (4.28)
階層 l > 1:
p(xˆj,l1 |z1:t) =
 p(zt|xˆ
j,l
t )p(xˆ
j,l
t |z1:t−1), if p(xˆpar(j),l−1t |z1:t) > 30p(xˆl−11 |z1)
p(xˆ
par(j),l−1
t |z1:t), otherwise,
(4.29)
p(xˆj,1t |z1:t−1) =
NL∑
i=1
p(xˆi,lt |xˆi,Lt−1p(xˆj,Lt−1|z1 : t) = 1. (4.30)
輸入時間為 t 的影像與感測器資料。 與初始化不同的是我們使用 t− 1 時間點經
由階層式貝氏濾波器找到的姿態狀態, 使用其關節角度正負10度為範圍來產生階
層樹,其中需計算 t−1至 t的事前機率與 t時間點的相似度機率值如公式 (4.27)
與 (4.28)。 其中事前機率 p(xˆj,1t |z1:t−1) 需要先求出公式 (4.28) 中的狀態轉移機
率 p(xˆi,Lt |z1:t−1)¿¿。 我們使用高斯模型來計算其狀態轉移機率, 利用 t − 1 之機
率樹最底層 L 層節點與 t − 1 找到的最大機率值狀態, 將兩狀態之關節角度取
73
圖 4.16: 雙手前舉使用感測器追蹤結果
75
蔽現象, 尤其是攝影機 (視覺感測器) 的數量很少時。 攝影機數量多的確可以達到不錯
的成效, 不過用越少的攝影機達到越好的成效, 是我們做這項計畫最終的目標, 因此我
們嘗試以單攝影機結合微機電感測器及子計畫一的技術, 來追蹤三維的人體姿態, 整個
系統的流程圖可見圖 4.17。
Human motion
Monocular image
Accelerometer
signals
Silhouettes
Particle filter
Previous pose
constraints
Dynamic
圖 4.17: 三維人體姿態追蹤流程圖
– 人體姿態追蹤
人體姿態追蹤有著許多非線性及難以估側狀態等問題, 要處理這種非線性以及局
步最佳化的問題, 我們參考子一提供的粒子濾波器 (Particlle Filter) 技術。 粒子
濾波器特性在於能夠描述非線性及多型態的事後機率分佈。 其對於每一個時間的
狀態向量的事後機率分佈更新則是由 Chapman-Komogorov 方程式來決定。
p(xt|zt) ∝ p(zt|xt)
∫
p(xt|xt−1)p(xt−1|zt−1)dxt−1, (4.31)
其中 zt 是由受測者的剪影影像 s 及 三軸加速規所量測到的那九個加速度 ai ∈
R3, i = 1, 2, ..., 9所組成的 為了簡化問題, 我們假設每個部位的計算都是互相
獨立, 因此相似度函式 (Likelihood Function) p(zt|xt) 可以定義成:
p(zt|xt) = p(s|xt)
9∏
i=1
p(ai|xt), (4.32)
在這裡我們用一個高斯來描述加速規的雜訊, 而加速規的相似度則是:
p(ai|x) ∝ exp−‖aM − ai‖
2
2σ2a
, (4.33)
77
其中 i∗ = arg maxi(piit)。
注意, 所有的追蹤步驟都是先由身體開始, 依序朝著肢體的末端部位去追蹤。
• 實驗結果 在實驗裡我們使用羅技 QuickcamPro 攝影機為視覺感測器, 並使用 [70] 校
正之。 每秒鐘可以取得十張解析度為 320 × 240 的影像, 加速規與攝影機取得的資料
會先經過同步才拿來使用, 每張影像的剪影則是透過事前訓練的背景模型所擷取出。
首先, 在第一個實驗我們使用一些簡單的動作 (例如手插腰、 看手錶、 手畫圓以及雙手
抱胸) 來測試我們的系統方法, 追蹤的結果可見圖4.18, 紅色框框表示我們估測的狀態,
由圖可知我們的系統可以在簡單的動作上成功的追蹤人體姿態。
接著我們要追蹤本計畫的重點,也就是追蹤演練太極拳的影像序列。我們要追蹤的太極
拳拳法是 “掤、 捋、 擠、 按”。 雖然太極拳運動過程有著許多的遮蔽情況, 追蹤結果 (見
圖4.19) 還算是差強人意。 為了驗證感測器在這個系統扮演角色的重要性, 我們拿掉了
感測器, 單單使用視覺感測器來追蹤, 藉以比較兩者之間的差異。 比較結果如圖 4.20,
可以看出沒有配置感測器的追蹤成效整個下降許多, 因為單憑一個二維的剪影是沒有
辦法求出在三維空間下的人體姿態的。 此結果也驗證了配戴慣性感測器的確可以改善
三維人體追蹤的成效。
79
圖 4.19: “掤、 捋、 擠、 按”的追蹤結果。
圖 4.20: 由左至右分別是原始影像、 剪影、 追蹤結果以及未配戴感測器的追蹤結果。
81
的正確率, 原則上完成項目與第三年計畫目標相去不遠, 唯一需要檢討得是我們並未完成動
作評分系統。 人體是個複雜的結構, 在追蹤、 分析人體姿態時, 更常常遇到許多瓶頸造成進度
上的落後, 動作評分這個環節並非全然的工程問題, 有些學員的姿態與教練不同, 但是評分仍
然很高, 反之, 動作與教練相仿的卻又因為一些小細節而不受青睞。 我們會持續去做, 希望能
盡快找出評分的準則。 另一方面, 我們在第三年中將研究重心置於結合 MEMS 感測器與攝
影機的姿態追蹤系統。 在研究過程中我們用 MEMS 感測器讀值來限縮解空間的方程式。 這
個研究成果對於人體姿態追蹤將有極大的功效, 相關論文正在整理準備發表中。整體而言, 在
第三年中我們也盡力達成了與計畫相關的研究成果。
§ 5.2 研究成果的學術價值
由研究創新的角度來看本年度研究成果的學術價值, 可找到以下貢獻。
我們在第一年發展了多重攝影機校正技術, 成本低廉、 架設容易且攝影機之間不需同步
是它的特點。 這類的技術進來已相當受到重視, 尤其是對於戶外的異常事件監控系統上的應
用有很大的幫助。 由於照像機視野的局限性, 需使用多種形式攝影機的戶外監控系統。 多重
攝影機校正技術能夠降低相機建置的困難度, 在應用層面更可以自動化偵測異常事件發生地
點及追蹤, 對於社會有很大的貢獻。
在智慧型環境中做動作教學在目前越來越受到重視。 但是因為動作分析有相當的困難
度, 所以動作較慢且較流暢的太極拳學習便成為相當合適的研究題目。 世界上的一些學術重
鎮如卡內基美濃大學以及瑞士 ETH 都在從事太極拳學習的研究。 在本年度的研究中, 我們
著眼於太極拳的師生姿態差異度的估測與師生姿態的同步估測, 在類似的研究計畫中都是比
較創新的題材。
接著是人類行為分析方法中的創新, 在這個研究中我們提出使用 VLMM 來模式化人類
行為。 VLMM可以有效地學出人類行為的隨機有限狀態機 PFSA,然後再提出將 VLMM轉
為 HMM 的系統化程序。 這樣做的好處是 VLMM 的學習程序可以提供 HMM 的拓樸結構,
而且 VLMM 的參數可以直接轉成 HMM 的參數。 實驗的結果顯示我們的方法遠較直接使
用的 HMM 作法效果優異許多 [54]。 這項研究成果也可以應用於其它需要使用 HMM 的場
83
參考文獻
[1] Jiayong Zhang. Statistical modeling and localization of nonrigid and articulated
shapes. Technical report, Robotics Institute, Carnegie Mellon University, Pitts-
burgh, PA, March 2006.
[2] Philo Tan Chua, Rebecca Crivella, Bo Daly, Ning Hu, Russ Schaaf, David Ventura,
Todd Camill, Jessica Hodgins, and Randy Pausch. Mastermotion: Full body
wireless virtual reality for tai chi. In SIGGRAPH ’02: ACM SIGGRAPH 2002
conference abstracts and applications, pages 214–214, New York, NY, USA, 2002.
ACM.
[3] R. Y. Tsai. A versatile camera calibration technique for high-accurancy 3d machine
vision metrology using off-the-shelf cameras and lenses. IEEE Journal of Robotics
and Automation RA-3, pages 323–344, August 1987.
[4] J. Weng, P. Cohen, and M. Herniou. Camera calibration with distortion models
and accuracy evaluation. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 14(10):965–980, 1992.
[5] R. Hartley, R. Gupta, and T. Chang. Stereo from uncalibrated cameras. In
Computer Vision and Pattern Recognition, pages 761–764, 1992.
[6] R. Hartley and A. Zisserman. Multiple View Geometry. Cambridge University
Press, 1999.
[7] R. I. Hartley. In defense of the eight-point algorithm. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 19(6):580–593, 1997.
85
[20] Y. P. Hung K. W. Chen and Y. S. Chen. A new method for camera calibration with
a bouncing ball. In Proceedings of 2005 IPPR Conference on Computer Vision,
Graphics, and Image Processing, Taipei, Taiwan, August 2005.
[21] Andreas Butz, Michael Schneider, and Mira Spassova. Searchlight - a lightweight
search function for pervasive environments. In Pervasive, pages 351–356, 2004.
[22] Han Chen, Rahul Sukthankar, Grant Wallace, and Kai Li. Scalable alignment of
large-format multi-projector displays using camera homography trees. In IEEE
Visualization, pages 339–346, 2002.
[23] Takayuki Okatani and Koichiro Deguchi. Autocalibration of a projector-screen-
camera system: Theory and algorithm for screen-to-camera homography estima-
tion. In International Conference on Computer Vision, pages 774–781, 2003.
[24] Andrew Raij and Marc Pollefeys. Auto-calibration of multi-projector display walls.
In ICPR, pages 14–17, 2004.
[25] R. Raskar and P. A. Beardsley. A self-correcting projector. In Computer Vision
and Pattern Recognition, December 2001.
[26] R. Raskar, P. Beardsley, J. V. Baar, Y. Wang, P. Dietz, J. Lee, D. Leigh, and
T. Willwacher. Rfig lamps: Interacting with a self-describing world via photosens-
ing wireless tags and projectors. ACM Trans. Graph., 23(3):406–415, 2004.
[27] R. Sukthankar, R. G. Stockton, and M. D. Mullin. Smarter presentations: Ex-
ploiting homography in camera-projector systems. In International Conference on
Computer Vision, pages 247–253, 2001.
[28] J. Ehnes, K. Hirota, and M. Hirose. Projected augmentation - augmented re-
ality using rotatable video projectors. In ISMAR ’04: Proceedings of the 3rd
IEEE/ACM International Symposium on Mixed and Augmented Reality, pages
26–35, 2004.
[29] G. Pingali, C. Pinhanez, T. Levas, R. Kjeldsen, and M. Podlaseck. User-following
displays. In Multimedia and Expo, 2002. ICME ’02. Proceedings. 2002 IEEE
International Conference on, volume 1, pages 845–848, 2002.
87
[40] Daniel Spelmezan and Jan Borchers. Real-time snowboard training system. In
CHI Extended Abstracts, pages 3327–3332, 2008.
[41] Erwin Schoonderwaldt, Nicolas H. Rasamimanana, and Fre´de´ric Bevilacqua. Com-
bining accelerometer and video camera: Reconstruction of bow velocity profiles.
In Proceedings of the 2006 International Conference on New Interfaces for Musical
Expression, pages 200–203, 2006.
[42] Ruth E. Mayagoitia, Anand V. Neneb, and Peter H. Veltink. Accelerometer and
rate gyroscope measurement of kinematics: an inexpensive alternative to optical
motion analysis systems. Journal of Biomechanics, 35:537–542, 2002.
[43] R. Zhu and Z. Zhou. A real-time articulated human motion tracking using tri-axis
inertial/magnetic sensors package. IEEE Transactions on Neural Systems and
Rehabilitation Engineering, 12(2):295–302, 2004.
[44] Ashok Veeraraghavan and Amit K. Roy Chowdhury. The function space of an
activity. In Computer Vision and Pattern Recognition, pages 959–968, 2006.
[45] Jyh-Shing Roger Jang and Hong-Ru Lee. Hierarchical filtering method for content-
based music retrieval via acoustic input. In ACM Multimedia, pages 401–410, 2001.
[46] Takuichi Nishimura. Music signal spotting retrieval by a humming query using
start frame feature dependent continuous dynamic programming. In ISMIR, pages
211–218, 2001.
[47] Liang Wang, Weiming Hu, and Tieniu Tan. Recent developments in human motion
analysis. Pattern Recognition, 36(3):585–601, 2003.
[48] Aaron F. Bobick and Yuri A. Ivanov. Action recognition using probabilistic pars-
ing. In CVPR, pages 196–202, 1998.
[49] S. Ogale, A. Karapurkar, and Y. Aloimonos. View-invariant modeling and recog-
nition of human actions using grammars. In Workshop on Dynamical Vision at
ICCV, pages 115–126, 2005.
89
[61] Jian Sun, Weiwei Zhang, Xiaoou Tang, and Heung-Yeung Shum. Background cut.
In European Conference on Computer Vision, volume 2, pages 628–641, 2006.
[62] Antonio Criminisi, Geoffrey Cross, Andrew Blake, and Vladimir Kolmogorov. Bi-
layer segmentation of live video. In Computer Vision and Pattern Recognition,
pages 53–60, 2006.
[63] Chris Stauffer and W. Eric L. Grimson. Adaptive background mixture models for
real-time tracking. In Computer Vision and Pattern Recognition, pages 2246–2252,
1999.
[64] L. Mu¨ndermann, S. Corazza, and T. P. Andriacchi. Accurately measuring human
movement using articulated ICP with soft-joint constraints and a repository of
articulated models. In Computer Vision and Pattern Recognition, 2007.
[65] Keith L. Moore and Arthur F. Dalley. Clinically Oriented Anatomy. Lippincott
Williams and Wilkins, 4 edition edition, April 1999.
[66] H. Zhuang, Z. S. Roth, and F. Hamano. A complete and parametrically continuous
kinematic model for robot manipulators. IEEE Transactions on Robotics and
Automation, 8(4):451–463, 1992.
[67] Sheng-Wen Shih, Yi-Ping Hung, and Wei-Song Lin. Kinematicparameter iden-
tification of an binocular head using stereo vision. Technical Report TR-94-006,
Institute of Information Science, Academia Sinica, Nankang, Taipei, Taiwan, 1994.
[68] Arthur F Dalley by Keith L Moore. Clinically Oriented Anatomy. Lippincott
Williams & Wilkins, April 15, 1999.
[69] Bjorn Stenger, Arasanathan Thayananthan, Philip H.S. Torr, and Roberto
Cipolla. Model-based hand tracking using a hierarchical bayesian filter. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 28(9):1372–1384,
2006.
[70] Zhengyou Zhang. Flexible camera claibration by viewing a plane from unknown
orientations. In International Conference on Computer Vision, pages 666–673,
1999.
91
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
LIANG et al.: LEARNING ATOMIC HUMAN ACTIONS USING VARIABLE-LENGTH MARKOV MODELS 269
can be categorized into the following two classes: deterministic
algorithms [5], [16] [17] and stochastic algorithms [9], [27],
[28]. Since the latter have higher degrees of freedom than the
former, they are suitable for a wider range of applications.
Currently, the HMM is the most popular stochastic algorithm
for language modeling because of its versatility and mathemat-
ical simplicity. However, since the states of an HMM are not
observable, encoding high-order temporal dependences with
this model is a challenging task. There is no systematic way to
determine the topology of an HMM or even the number of its
states. Moreover, the training process only guarantees a local
optimal solution; thus, the training result is very sensitive to
the initial values of the parameters. On the other hand, since
the states of a VLMM are observable, its parameters can be
estimated easily, given sufficient training data. Consequently,
a VLMM can capture both long- and short-term dependences
efficiently because the amount of memory required for predic-
tion is optimized during the training process. However, thus far,
the VLMM technique has not been applied to human behavior
recognition directly because of the following two limitations:
1) It cannot handle the dynamic time warping problem, and
2) it lacks a model for handling the noise observation.
In this paper, we propose a hybrid framework of VLMM and
HMM that retains the models’ advantages while avoiding their
drawbacks. The framework is composed of the following two
modules: a posture labeling module and a VLMM atomic action
learning and recognition module. First, a posture template
selection algorithm is developed based on a modified shape
context technique. The selected posture templates constitute a
codebook, which is used to convert input posture sequences into
discrete symbol sequences for subsequent processing. Then, the
VLMM technique is applied to learn the symbol sequences
that correspond to atomic actions. This avoids the problem
of learning the parameters of an HMM. Finally, the learned
VLMMs are transformed into HMMs for atomic action recog-
nition. Thus, an input posture sequence can be classified with
the fault tolerance property of an HMM.
The remainder of this paper is organized as follows. In
Section II, we introduce the theory of VLMM. The proposed
approach is described in Section III, and the experimental
results are detailed in Section IV. Then, in Section V, we
present our conclusions.
II. VLMM
A VLMM technique [9], [12], [19] is frequently applied to
language modeling problems because of its powerful ability to
encode temporal dependences. As shown in Fig. 1, a VLMM
can be regarded as a probabilistic finite-state automaton (PFSA)
Λ = (S, V, τ, γ, π) [19], where the variables are described as
follows.
1) S denotes a finite set of model states, each of which
is uniquely labeled by a symbol string representing the
memory of a conditional transition of the VLMM,
2) V denotes a finite observation alphabet.
3) τ : S × V → S is a state transition function such that
τ(sj , ν) → sj+1.
Fig. 1. Example of a VLMM.
Fig. 2. PST for constructing the VLMM shown in Fig. 1.
4) γ : S × V → [0, 1] represents the output probability
function with ∀s ∈ S,∑ν∈V γ(s, ν) = 1.
5) π : S → [0, 1] is the probability function of the initial
state satisfying
∑
s∈S π(s) = 1.
In the following sections, we consider the VLMM learn-
ing in Section II-A, followed by the VLMM recognition in
Section II-B.
A. VLMM Learning
The topology and the parameters of a VLMM can be learned
from training sequences by optimizing the amount of memory
required to predict the next symbol. Usually, the first step of
training a VLMM involves the construction of a prediction
suffix tree (PST) [19]. A PST contains the information of the
prefix of a symbol learned from the training data. Therefore,
this prefix/suffix relationship helps one to determine the amount
of memory required to predict the next symbol. After the PST is
constructed from the training sequences, the PST is converted to
a PFSA representing the trained VLMM. Fig. 2 shows the PST
constructed from a training sequence for converting the VLMM
shown in Fig. 1. Except for the root node, each node of the
PST represents a nonempty symbol string, and each parent node
represents the longest suffix tree of its child nodes. In addition,
P (ν|s) is the output probability distribution of the next symbol
ν of each node s that satisfies
∑
ν∈V P (ν|s) = 1. The output
and prior probabilities can be derived from the training symbol
sequences as follows:
P (ν|s) = N(sν)
N(s)
(1)
P (s) =
N(s)
N0
(2)
Authorized licensed use limited to: National Chi-Nan University Library. Downloaded on December 10, 2008 at 22:31 from IEEE Xplore.  Restrictions apply.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
LIANG et al.: LEARNING ATOMIC HUMAN ACTIONS USING VARIABLE-LENGTH MARKOV MODELS 271
Fig. 4. Shape context computation and matching. (a) and (b) Sampled points of two shapes. (c)–(e) Local shape contexts corresponding to different reference
points. (f) Diagram of the log-polar space. (g) Correspondence between points computed using a bipartite graph matching method.
Assume that pi and qj are points of the first and second
shapes, respectively. The shape context approach defines the
cost of matching the two points as follows:
C(pi, qj) =
1
2
K∑
k=1
[hi(k)− hj(k)]2
hi(k) + hj(k)
(6)
where hi(k) and hj(k) denote the K-bin normalized his-
tograms of pi and qj , respectively. Shape matching is accom-
plished by minimizing the following total matching cost:
H(π) =
∑
i
C
(
pi, qπ(i)
) (7)
where π is a permutation of 1, 2, . . ., n. Due to the constraint
of one-to-one matching, shape matching can be considered
as an assignment problem that can be solved by a bipartite
graph matching method. A bipartite graph is a graph G = (V =
{pi} ∪ {qj}, E), where {pi} and {qj} are two disjoint sets
of vertices and E is a set of edges connecting vertices from
{pi} to {qj}. The matching of a bipartite graph is to assign
the edge connection. There are many matching algorithms for
bipartite graphs described in [2]. Here, the resulting correspon-
dence points are denoted by {(pi, qπ(i))|, i = 1, 2, . . . , n} or
{(qi, pπ(i))|, i = 1, 2, . . . ,m}, where n and m are the numbers
of sample points on shapes P and Q, respectively. Therefore,
the shape context distance between two shapes P and Q can be
computed as follows:
Dsc(P,Q) =
1
n
∑
i
C
(
pi, qπ(i)
)
+
1
m
∑
j
C
(
qj , pπ(j)
)
.
(8)
Although the shape context matching algorithm usually pro-
vides satisfactory results, the computational cost of applying it
to a large database of posture templates is so high that is not
feasible. To reduce the computation time, we only compute the
local shape contexts at certain critical reference points, which
should be easily and efficiently computable, robust against
segmentation error, and critical to defining the shape of the
silhouette. Note that the last requirement is very important
because it helps preserve the informative local shape context. In
this work, the critical reference points are selected as the ver-
tices of the convex hull of a human silhouette. Matching based
on this modified shape context technique can be accomplished
by minimizing a modified version of (7) as follows:
H ′(π) =
∑
p∈A
C
(
p, qπ(p)
) (9)
where A is the set of convex hull vertices and H ′ is the adapted
total matching cost. However, reducing the number of local
shape contexts to be matched will also increase the influence of
false matching results. To minimize the false matching rate, the
ordering constraint of the vertices has to be imposed. However,
since traditional bipartite graph matching algorithms [2] do not
consider the order of all sample points, they are not suitable for
our algorithm. Therefore, dynamic programming is adopted in
the shape matching process. Suppose a shape P includes a set
of convex hull vertices A and another shape Q includes a set of
convex hull vertices B. The convex-hull shape context (CSC)
distance can be calculated as follows:
Dcsc(P,Q) =
1
|A|
∑
p∈A
C
(
p, qπ(p)
)
+
1
|B|
∑
q∈B
C
(
q, pπ(q)
)
.
(10)
Authorized licensed use limited to: National Chi-Nan University Library. Downloaded on December 10, 2008 at 22:31 from IEEE Xplore.  Restrictions apply.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
LIANG et al.: LEARNING ATOMIC HUMAN ACTIONS USING VARIABLE-LENGTH MARKOV MODELS 273
Fig. 6. (a) VLMM constructed with the original input training sequence. (b) Original VLMM constructed with the preprocessed training sequence. (c) Modified
VLMM, which includes the possibility of self-transition.
transition probability must also be updated as anewij = aoldij (1−
anewii ). For example, if the input training symbol sequence
is “AAABBAAACCAAABB,” the preprocessed training sym-
bol sequence becomes “ABACAB.” The VLMM constructed
with the original input training sequence is shown in Fig. 6(a),
while the original VLMM and the modified VLMM constructed
with the preprocessed training sequence are shown in Fig. 6(b)
and (c), respectively.
Next, a noise observation model is introduced to convert
a VLMM into an HMM. Note that the output of a VLMM
determines its state transition and vice versa because the state
of a VLMM is observable. In general, the possible output is
restricted to several discrete symbols. However, due to the
noise caused by image preprocessing, the symbol sequence
corresponding to an atomic action includes some randomness.
Such randomness will cause the action sequence not recog-
nizable by the VLMMs. Therefore, we propose to modify
the symbol observation model as described in the following.
Suppose that the output symbol of a VLMM is qt at time t
and that its posture template retrieved from the codebook is
aqt . If the VLMM is the right model, the extracted silhouette
image ot will not deviate too much from its corresponding
posture template aqt , provided that the segmentation result does
not contain any major errors. Due to noise observation, the
silhouette image ot is a random variable, and so is the CSC
distance Dcsc(ot, aqt). It is possible to learn the distribution
of the CSC distance Dcsc(ot, aqt) using the training data. An
example is shown in Fig. 7. In this example, it is clear that a
Gaussian distribution can be applied to model the CSC distance,
i.e., P (ot|qt,Λ) = (1/
√
2πσ)e−Dcsc(ot,at)/2σ
2
. The standard
deviation σ of this distribution is estimated using the maximum-
likelihood technique.
Note that the VLMM has now been converted into a first-
order Markov chain. If the VLMM’s observation model is
detached from the symbol of a state, then the VLMM be-
comes a standard HMM. The probability of the observed sil-
houette image sequence O = o1o2 . . . oT for a given model
Λ can be evaluated by the HMM forward/backward proce-
dure with proper scaling [18]. Finally, category i∗ obtained
with the following equation is deemed to be the recognition
result:
i∗ = argmax
i
log [P (O|Λi)] . (13)
Fig. 7. Distribution of observation error, obtained using the training data.
IV. EXPERIMENTS
We conducted a series of experiments to evaluate the effec-
tiveness of the proposed method. A powerful scalable recog-
nition system would only use the data extracted from one
person for training but would still be capable of recognizing
data collected from other people. Accordingly, the training data
used in our experiments were a real video sequence comprised
of approximately 900 frames [30]. The training data contained
ten categories of action sequences that were performed by a
single person. Some typical image frames are shown in Fig. 8.
Using the posture template selection algorithm, a codebook of
95 posture templates (see Fig. 9) was constructed from the
training data. The data were then used to build ten VLMMs,
each of which was associated with one of the atomic actions
shown in Fig. 8.
To demonstrate the effectiveness and efficiency of the pro-
posed CSC matching process, we compared the posture la-
beling results and computation time of the proposed method
with those of the original shape context matching approach.
In the first experiment, the codebook of posture templates
(see Fig. 9) was used to label the training data. The experimen-
tal results show that 85% of the labeling results done by the
proposed method were the same as those obtained by applying
the original method, but our approach could save 95% of the
computation time. Since the computation time is very important
in the procedure of human action recognition, a significant
Authorized licensed use limited to: National Chi-Nan University Library. Downloaded on December 10, 2008 at 22:31 from IEEE Xplore.  Restrictions apply.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
LIANG et al.: LEARNING ATOMIC HUMAN ACTIONS USING VARIABLE-LENGTH MARKOV MODELS 275
Fig. 9. Posture templates extracted from the training data.
considered in this experiment. Because the initial parameters
and the number of HMM states would affect recognition results,
the HMM implementation was evaluated using a variety of
HMMs, each of which had a different number of hidden states.
Furthermore, the HMM was trained ten times, and the average
results were used to reduce the effect of the initial random
parameters. Table II compares our method’s recognition rate
with that of the HMM method, for test data from nine different
human subjects. Our method clearly outperforms the HMM
method, no matter how many states were selected. In Table II,
the shaded cells denote the best recognition results of the HMM
approach for a particular action. It is clear that the selection of
the number of states is a critical issue for the HMM method.
Note that the number of HMM states that could be set for
deriving the best performance was varying in different actions,
which makes the selection of the number of states even more
difficult. In contrast to the difficulty in determining the topology
of an HMM, our method is simple and effective because the
topology of a VLMM can be determined automatically with a
robust algorithm. Note that the recognition rates for action 1
were the worst across all actions. Fig. 12(a) shows some typ-
ical input postures for a human subject performing action 1.
The retrieved corresponding closest posture templates in the
database are shown in Fig. 12(b). When comparing the cor-
responding posture templates shown in Fig. 12(b) with the
training posture sequences shown in Fig. 8, it is clear that
Authorized licensed use limited to: National Chi-Nan University Library. Downloaded on December 10, 2008 at 22:31 from IEEE Xplore.  Restrictions apply.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
LIANG et al.: LEARNING ATOMIC HUMAN ACTIONS USING VARIABLE-LENGTH MARKOV MODELS 277
TABLE II
COMPARISON OF OUR METHOD’S RECOGNITION RATE WITH THAT OF THE HMM COMPUTED
WITH THE TEST DATA OBTAINED FROM NINE DIFFERENT HUMAN SUBJECTS
Fig. 12. Some typical postures of a human subject exercising action 1. (a) Input posture sequence. (b) Corresponding minimum-CSC-distance posture templates.
Fig. 13. Recognition rates with respect to different τc’s.
database [4], [25]. This database consists of 90 low-resolution
(180 × 144) action sequences from nine different people,
each performing ten natural actions. These actions include
bending (bend), jumping jacks (jack), jumping forward on
two legs (jump), jumping in place on two legs (pjump), run-
ning (run), galloping sideways (side), skipping (skip), walking
(walk), waving one hand (wave1), and waving two hands
(wave2). Sample images of each type of action sequence are
shown in Fig. 14. In [25], a sequence of human silhouettes
derived from each action sequence was converted into two
representations, namely, average motion energy (AME) and
mean motion shape (MMS). Subsequently, a nearest neighbor
classifier (NN) was used for recognition, and the leave-one-out
cross-validation rule was adopted to compute the recognition
rate. Recognition results for these two representations, shown
in the top two rows of Table III, are compared against our
method.
In order to compare our method with the two competing
methods in a fairer fashion, we also applied the leave-one-out
rule to our method. In this case, eight sets of data grabbed from
eight distinct human subjects were used to train the VLMMs,
resulting in eight VLMMs for each action. Finally, the category
with the maximum likelihood was deemed to be the recognition
result. Results using this methodology are shown in the last row
Authorized licensed use limited to: National Chi-Nan University Library. Downloaded on December 10, 2008 at 22:31 from IEEE Xplore.  Restrictions apply.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
LIANG et al.: LEARNING ATOMIC HUMAN ACTIONS USING VARIABLE-LENGTH MARKOV MODELS 279
of Table III. It is clear that our method outperforms the other
two methods for this public database.
V. CONCLUSION
We have proposed a framework for understanding human
atomic actions using VLMMs. The framework comprises the
following two modules: a posture labeling module and a
VLMM atomic action learning and recognition module. We
have developed a simple and efficient posture template selection
algorithm based on a modified shape context matching method.
A codebook of posture templates is created to convert the input
posture sequences into discrete symbols so that the language
modeling approach can be applied. The VLMM technique is
then used to learn human action sequences. To handle the
dynamic time warping problem and the lack of noise obser-
vation model problem of applying the VLMM technique to
behavior analysis, we have also developed a systematic method
to convert the learned VLMMs into HMMs. The contribution
of our approach is that the topology of the HMMs can be
automatically determined and that the recognition accuracy is
better than the traditional HMM approach. Experimental results
demonstrate the efficacy of the proposed method.
REFERENCES
[1] J. K. Aggarwal and Q. Cai, “Human motion analysis: A review,” Comput.
Vis. Image Understanding, vol. 73, no. 3, pp. 428–440, Mar. 1999.
[2] H. A. Baler Saip and C. L. Lucchesi, “Matching algorithm for bipartite
graph,” Departamento de Cincia da Computao, Universidade Estudal de
Campinas, Campinas, Brazil, Tech. Rep. DCC-03/93, 1993.
[3] S. Belongie, J. Malik, and J. Puzicha, “Shape matching and object recog-
nition using shape contexts,” IEEE Trans. Pattern Anal. Mach. Intell.,
vol. 24, no. 4, pp. 509–522, Apr. 2002.
[4] M. Blank, L. Gorelick, E. Shechtman, M. Irani, and R. Basri, “Actions as
space-time shapes,” in Proc. IEEE Int. Conf. Comput. Vis., 2005, vol. 2,
pp. 1395–1402.
[5] A. F. Bobick and Y. A. Ivanov, “Action recognition using probabilistic
parsing,” in Proc. IEEE Conf. Comput. Vis. Pattern Recog., Santa Barbara,
CA, 1998, pp. 196–202.
[6] D. Y. Chen, S. W. Shih, and H.-Y. M. Liao, “Atomic human action
segmentation using a spatio-temporal probabilistic framework,” in Proc.
IEEE Int. Conf. Intell. Inf. Hiding Multimedia Signal Process., Pasadena,
CA, 2006, pp. 327–330.
[7] R. T. Collins, A. J. Lipton, and T. Kanade, “Introduction to the special
section on video surveillance,” IEEE Trans. Pattern Anal. Mach. Intell.,
vol. 22, no. 8, pp. 745–746, Aug. 2000.
[8] W. B. Croft and J. Lafferty, Language Modeling for Information Retrieval.
Norwell, MA: Kluwer, 2003.
[9] A. Galata, N. Johnson, and D. Hogg, “Learning variable-length Markov
models of behavior,” Comput. Vis. Image Understanding, vol. 81, no. 3,
pp. 398–413, Mar. 2001.
[10] D. M. Gavrila, “The visual analysis of human movement: A survey,”
Comput. Vis. Image Understanding, vol. 73, no. 1, pp. 82–98, Jan. 1999.
[11] P. Guttorp, Stochastic Modeling of Scientific Data. London, U.K.:
Chapman & Hall, 1995.
[12] I. Guyon and F. Pereira, “Design of a linguistic postprocessor using
variable memory length Markov models,” in Proc. Int. Conf. Document
Anal. Recog., Montréal, QC, Canada, 1995, pp. 454–457.
[13] J. W. Hsieh, Y. T. Hsu, H.-Y. M. Liao, and C. C. Chen, “Video-based
human movement analysis and its application to surveillance systems,”
IEEE Trans. Multimedia, vol. 10, no. 3, pp. 372–384, Apr. 2008.
[14] F. Jelinek, Statistical Methods for Speech Recognition. Cambridge, MA:
MIT Press, 1998.
[15] H. Miyamori and S. Iisaku, “Video annotation for content-based
retrieval using human behavior analysis and domain knowledge,” in Proc.
IEEE Int. Conf. Autom. Face Gesture Recog., Grenoble, France, 2000,
pp. 320–325.
[16] A. S. Ogale, A. Karapurkar, and Y. Aloimonos, “View-invariant modeling
and recognition of human actions using grammars,” in Proc. Workshop
Dynamical Vis. ICCV, Beijing, China, 2005, pp. 115–126.
[17] J. Park, S. Park, and J. K. Aggarwal, “Model-based human motion track-
ing and behavior recognition using hierarchical finite state automata,” in
Proc. Int. Conf. Comput. Sci. Appl., Assisi, Italy, 2004, pp. 311–320.
[18] L. R. Rabiner, “A tutorial on hidden Markov models and selected appli-
cations in speech recognition,” Proc. IEEE, vol. 77, no. 2, pp. 257–286,
Feb. 1989.
[19] D. Ron, Y. Singer, and N. Tishby, “The power of amnesia,” in Advances in
Neural Information Processing Systems. New York: Morgan Kaufmann,
1994, pp. 176–183.
[20] R. Rosenfeld, “Two decades of statistical language modeling: Where do
we go from here?” Proc. IEEE, vol. 88, no. 8, pp. 1270–1278, Aug. 2000.
[21] R. Sharma, V. I. Pavlovic´, and T. S. Huang, “Toward multimodal
human–computer interface,” Proc. IEEE, vol. 86, no. 5, pp. 853–869,
May 1998.
[22] C. W. Su, H.-Y. M. Liao, H. R. Tyan, C. W. Lin, D. Y. Chen, and
K. C. Fan, “Motion flow-based video retrieval,” IEEE Trans. Multimedia,
vol. 9, no. 6, pp. 1193–1201, Oct. 2007.
[23] A. Vinciarelli, S. Bengio, and H. Bunke, “Offline recognition of uncon-
strained handwritten texts using HMMs and statistical language models,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 26, no. 6, pp. 709–720,
Jun. 2004.
[24] L. Wang, W. Hu, and T. Tan, “Recent developments in human motion
analysis,” Pattern Recognit., vol. 36, no. 3, pp. 585–601, Mar. 2003.
[25] L. Wang and D. Suter, “Informative shape representations for human
action recognition,” in Proc. IEEE Int. Conf. Pattern Recog., 2006, vol. 2,
pp. 1266–1269.
[26] C. R. Wren, A. Azarbayejani, T. Darrell, and A. P. Pentland, “Pfinder:
Real-time tracking of the human body,” IEEE Trans. Pattern Anal. Mach.
Intell., vol. 19, no. 7, pp. 780–785, Jul. 1997.
[27] J. Yamato, J. Ohya, and K. Ishii, “Recognizing human action in time-
sequential images using hidden Markov model,” in Proc. IEEE Conf.
Comput. Vis. Pattern Recog., 1992, pp. 379–385.
[28] J. Yang, Y. Xu, and C. S. Chen, “Human action learning via hidden
Markov model,” IEEE Trans. Syst., Man, Cybern. A, Syst., Humans,
vol. 27, no. 1, pp. 34–44, Jan. 1997.
[29] [Online]. Available: http://en.wikipedia.org/wiki/Statistical_significance
[30] [Online]. Available:http://www.iis.sinica.edu.tw/~ulin/Behavior
Yu-Ming Liang received the B.S. and M.S. de-
grees in information and computer education from
National Taiwan Normal University, Taipei, Taiwan,
in 1999 and 2002, respectively. He has been work-
ing toward the Ph.D. degree in the Department of
Computer Science, National Chiao Tung University,
Hsinchu, Taiwan, since 2004.
His research interests include computer vi-
sion, pattern recognition, and multimedia signal
processing.
Sheng-Wen Shih received the M.S. and Ph.D.
degrees in electrical engineering from National
Taiwan University, Taipei, Taiwan, in 1990 and 1996,
respectively.
From January to July 1997, he was a Postdoctoral
Associate with the Image Formation and Processing
Laboratory, Beckman Institute, University of Illinois,
Urbana. He has been an Associate Professor with the
Department of Computer Science and Information
Engineering, National Chi Nan University, Nantou,
Taiwan, since 2003, where he was an Assistant
Professor in August 1997. His research interests include computer vision,
biometrics, and human–computer interaction.
Authorized licensed use limited to: National Chi-Nan University Library. Downloaded on December 10, 2008 at 22:31 from IEEE Xplore.  Restrictions apply.
Tai-Chi Chuan Motion Tracking by Integrating Inertial and Monocular
Visual Sensors
Cheng-Ta Shen
CSIE, NCNU
chengtashen@gmail.com
Nai-He Hsu
CSIE, NCNU
ksbcboy@gmail.com
Chih-Yi Chiu
CSIE, NCYU
cychiu@iis.sinica.edu.tw
Sheng-Wen Shih
CSIE, NCNU
swshih@ncnu.edu.tw
Abstract—In this paper, we study the 3-D human pos-
ture tracking problem for computer-aided Tai-Chi Chuan
motion learning. A model-based human motion tracking
method is proposed which uses a monocular visual sensor
and multiple accelerometers attached to different body
parts of a subject. In order to track human postures
robustly, a generic complete and parametrically continu-
ous (CPC) kinematic model is constructed. Twelve CPC
parameters are adapted according to the lengthes of the
upper limbs and the joint positions of the shoulders and
the thighs so that the generic CPC model can be used
to track different subjects. Equations for predicting the
acceleration measurements are derived using the kinematic
model. A method for assessing the difference between the
measured and the predicted silhouette images is provided.
The particle filter is adopt to track human posture from
torso toward the ends of the limbs. Real experiments
have been conducted to test the proposed method with
various human motions including five simple actions and
a fundamental Tai-Chi Chuan actions (i.e., “ward off”,
“roll back”, “press”, and “push”). The results show that
the proposed method is very promising.
Index Terms—Human Motion Tracking, Accelerometer,
Particle Filter.
I. INTRODUCTION
Tracking human motion has been an important
topic in computer vision because many practical
applications can benefit from the human motion
information. In particular, tracking human motion
is an integral part to developing a computer-aided
learning system for sports. In this work, we will
study the motion tracking problem of computer-
aided learning for Tai-Chi Chuan. Tai-Chi is a popu-
lar exercise which is good for health and is suitable
for people of all ages including postoperative and/or
asthma patients. Although learning Tai-Chi looks
easy, every Tai-Chi action is composed of complex
movements and it might take decades to get familiar
with the whole action. Beginners often need an
experienced coach to assist learning. Chua et al. de-
veloped an immersion Tai-Chi learning environment
[1] in their MasterMotion project using a motion-
capture (mocap) device. The mocap data of the
student and the Tai-Chi teacher are compared and
the student error is displayed in their virtual reality
(VR) system. Through this visual feedback, the stu-
dent can correct their Tai-Chi motion accordingly.
However, the cost of the mocap device and the VR
environment is too high to popularize their learning
system, not to mention that users have to wear a
head-mounted display and a bodysuit with dozens
of optical tracking balls. Therefore, a computer-
aided Tai-Chi learning system is demanding a less-
restrictive, low cost and efficient motion tracking
method. The purpose of human motion tracking is
to use different kinds of sensors, such as inertial
sensors, cameras, magnetics and ultrasound systems
[2–4], to estimate the orientations and positions
of human body parts. In recent years, the vision-
based motion capture has become a popular research
topic. A visual sensor can provide shape, color, and
location information for posture tracking. However,
cluttered background, variation of the subject body
shape, different clothing, and self-occlusion will all
make visual tracking of the human posture a very
challenging problem. Additionally, the high degrees
of freedom (DOFs) of a human body constitute a
high dimensional state space which makes searching
the optimal state a very time-consuming problem.
In a computer-aided learning system, the cluttered
background problem can be alleviated by training a
background model. Moreover, the clothing can be
designed to improve the tracking accuracy. While
the self-occlusion problem can be alleviated by
using multiple cameras [5–10], the cost of setting
up a multiple camera network is proportional to
Fong et al. [13] develop an acceleration-based
wireless upper limb motion sensing system for
recording the hand motion of a running sub-
ject in a sports science application. Kunze et
al. use body-worn gyroscopes and accelerom-
eters to analyze the smoothness of Tai-Chi
Chuan movements [22]. They proposed to
assess the energy consumption of a Tai-Chi
practicer using the energy of the measured
inertial signals. Since a Tai-Chi expert usually
consumes less energy than a beginner when
playing the same Tai-Chi action, their system
was trained to classify the grade of a Tai-
Chi practicer using the estimated energy con-
sumption. Spelmezan and Borchers [23] pro-
posed a wireless computer-aided system for
real-time snowboard training. The proposed
system consists of two Bluetooth Shake SK6
inertial sensor packs, two bend sensors, and
force-sensitive resistors. The sensors are used
to detect common mistakes of beginner snow-
boarders, such as insufficient knee bending,
incorrect weight distribution, and incorrect
rotation of the upper body, and are fed back
to the snowboarders immediately. Kwon and
Gross [24] proposed a martial art training sys-
tem using a visual sensor and a wrist-mounted
wireless accelerometer. Their system monitors
and evaluates the motion of a trainee’s single
arm and then generates a visual feedback for
gesture training. Schoonderwaldt et al. [25]
develop a cost-effective method combining a
two-axis accelerometer and a video camera
for estimating the bow velocity in violin play-
ing.
3) Motion analysis with posture reconstruction:
Luinge and Veltink described a preliminary
study of using the Kalman filter technique
to estimate the inclination of a human torso
by considering the dynamics of the torso
[12]. Mayagoitia et al. [26] developed an
optical motion analysis system combining ac-
celerometers and rate-gyros to estimate the
joint angles, velocities and accelerations of
lower limbs in the sagittal plane. The accuracy
of the proposed method is verified using a
mocap system. Zhu and Zhou [15] propose
a real time tracking method using sensor
modules to track human motion. Each sensor
module consists of a tri-axis accelerometer,
a tri-axis rate-gyro and a tri-axis magnetic
sensor. Fifteen sensor modules are attached to
different body parts (12 modules for the four
limbs and three modules for the torso). Since
the magnetic sensors can provide the position
and orientation measurement, the body pos-
ture can be readily estimated using a linear
Kalman filter.
Among the aforementioned methods, the one
proposed by Zhu and Zhuo [15] is the most similar
to ours in terms of using multiple body sensors
to track the whole body posture. However, metals
in the environment may distort the measurements
of a magnetic sensor. Also, the orientation angle
obtained by integrating a rate-gyro signal tend to
drift over time. Using magnetic sensors and rate-
gyros will not only increase the system cost but
also introduce more uncontrollable distortions and
measurement biases. Therefore, in our system, only
tri-axis accelerometers are included as the body sen-
sors. Additionally, a monocular visual sensor is used
to provide supplemental information for estimating
the whole body posture. The measurements from
both the visual sensor and the body sensors are
provided to particle filters for estimating the human
motion parameters. Fig. 1 shows an overview of our
method, which will be described in depth in the
following sections.
Human motion
Monocular image
Accelerometer
signals
Silhouettes
Particle filter
Previous pose
constraints
Dynamic
Fig. 1. Overview of the proposed method.
III. HUMAN MOTION TRACKING
A. Body Sensors Schematic
In this work, nine tri-axis MEMS accelerometers
are attached to nine body parts, which are denoted
pivot position between the ith joint and the (i+1)st
joint are coincided, the translation vector in equation
(7) can be set to zero.
Figure 3 shows the kinematic chain of an upper
limb, where {W} denotes the world coordinate
system and and {base} is the base (i.e., the torso)
coordinate system. The shoulder and the elbow are
formulated as three (Q1, Q2, and Q3) and two (Q4,
and Q5) revolute joints, respectively. The coordinate
systems of the accelerometers attached to the upper
arm and the forearm are defined with two extra
6-DOF transformation matrices Vacc1 and Vacc2,
respectively. Therefore, the coordinate transforma-
tion matrix from the accelerometers to the world
coordinate system are respectively given by
WTacc1 =
WT0V0Q1V1Q2V2Q3Vacc1, (9)
and
WTacc1 =
WT0V0Q1V1Q2V2Q3V3Q4V4Q5Vacc2.
(10)
To use the CPC kinematic model in video tracking,
it is assumed that the monocular camera is cali-
brated such that its intrinsic parameters and its posi-
tion and orientation with respect to {W} are known.
In order to fit the generic CPC model to different
subjects, three translation parameters, i.e., l0,x, l0,y,
and l3,z, are treated as unknowns to be estimated
in the posture reconstruction process. Notably, l0,x
and l0,y determine the position of the shoulder joint
while l3,z determines the length of the upper arm.
The kinematic chain of a lower limb is defined in a
similar way and, thus, the kinematic chains of the
four limbs share the same world coordinate system
and the same base coordinate system. Each limb
has five joint angles and three translation parameters
to be estimated. Additionally, the torso has six
DOFs to be determined. Therefore, at each time
instance t, the human posture estimation problem
has 38-D unknown vector denoted as xt to be
estimated. Since searching an optimal solution in
the 38-D space is intractable, we will assume that a
reliable initial solution x0 is known and the posture
reconstruction problem reduces to a state tracking
problem which will be detailed in the next section.
C. Human Posture Tracking
The human posture tracking problem is an ill-
posed problem which possesses high nonlinearity in
TABLE II
TERMS OF MOVEMENTS
Body part DOFs Terms of movements
LUA/RUA
Abduction/Adduction
3 Extension/Flextion
Lateral rotation/Medial rotation
LFA/RFA 2 Extension/FlexionSupination/Pronation
LT/RT
Abduction/Adduction
3 Extension/Flexion
Lateral rotation/Medial rotation
LS/RS 2 Extension/FlexionCircumduction
both measurement functions and the state transition
function and it also has multiple local optima. To
handle the nonlinearity and the local optima, the
particle filtering technique is adopted in this work.
The particle filtering is capable of modeling non-
linear and multi-modal posterior distributions. The
update of the posterior probability distribution of the
state vector is governed by the following Chapman-
Komogorov equation.
p(xt|zt) ∝ p(zt|xt)
∫
p(xt|xt−1)p(xt−1|zt−1)dxt−1,
(11)
where zt is the measurement vector consisting of the
silhouette image s of the subject and the tri-axis ac-
celeration measurements of the nine accelerometers
ai ∈ R3, i = 1, 2, ..., 9. For simplicity, we assume
that the measurements are mutually independent.
Therefore, the likelihood function p(zt|xt) can be
formulated as follows.
p(zt|xt) = p(s|xt)
9∏
i=1
p(ai|xt), (12)
Assume that the measure noises of the accelerome-
ters are Gaussain, the likelihood of the accelerations
is given by
p(ai|x) ∝ exp−‖aM − ai‖
2
2σ2a
, (13)
where aM = g+ab is composed of the acceleration
of gravity g and the acceleration of body part
motion ab. Both the two acceleration sources can be
computed using the kinematic model. Returning to
synchronously. Silhouettes of the target subject are
segmented using a pre-trained background model.
In the first experiment, we tested the propose
method using simple actions such as hands on the
hips, checking watch, circling, hands fold across
the chest, and extension. The posture reconstruction
results are shown in Fig. 4, where the wire-frame
of the recovered human posture are overlayed on
the input image. The results shows that our method
successfully tracks the human posture in simple
video sequences.
(a) Hands on the hips.
(b) Checking watch.
(c) Circling.
(d) Hands fold across the chest.
(e) Extension.
Fig. 4. Tracking results for simple actions.
In the second experiment, the proposed method is
applied to track a fundamental Tai-Chi action which
is termed “ward off”, “roll back”, “press”, and
“push” (i.e., 掤、捋、擠、按 ). Although this Tai-Chi
action possesses high order self-occlusion, the track-
ing results shown in Fig. 5 are satisfactory. In order
to show the importance of using the accelerometers,
we also track this Tai-Chi action without using any
inertial sensors for comparison. As shown in Fig. 6,
the tracking results degraded severely because the
monocular silhouette sequences is ambiguous to
recover the 3-D human posture. The results demon-
strate that inertial sensors can improve 3-D tracking
results.
Fig. 5. Tracking results of “ward off”, “roll back”, “press”, and
“push”
Fig. 6. The observed image, segmented silhouette, tracking result
with the proposed method, and tracking result without using inertial
sensors are shown from left to right.
V. CONCLUSIONS AND FUTURE WORKS
In this work, a human posture tracking method
is proposed which uses a monocular camera and
multiple accelerometers attached to different body
parts of a subject. To simplify the 3-D posture
tracking problem in a monocular video sequence,
a generic CPC kinematic model [19] of the human
body is created. Twelve CPC parameters are adapted
according to the lengthes of the upper limbs and the
joint positions of the shoulders and the thighs so that
the generic CPC model can be used to track different
subjects. The particle filter technique is used to track
the body parts in a top-down sequence from the
torso toward the ends of the limbs. The proposed
system is cost-effective which can be used to track
human motion for assisting the learning of Tai-
Chi actions. Real experiments have been conducted,
actions on Neural Systems and Rehabilitation
Engineering, 12(2):295–302, 2004.
[16] Jamie A. Ward, Paul Lukowicz, Gerhard
Tro¨ster, and Thad Starner. Activity recognition
of assembly tasks using body-worn micro-
phones and accelerometers. IEEE Transactions
on Pattern Analysis and Machine Intelligence,
28(10):1553–1567, 2006.
[17] Alexa Hauck, Stefan Lanser, and Christoph
Zierl. Hierarchical recognition of articulated
objects from single perspective views. In Com-
puter Vision and Pattern Recognition, pages
870–876, 1997.
[18] M. Isard and A. Blake. Condensation – condi-
tional density propagation for visual tracking.
International Journal of Computer Vision, 29
(1):5–28, 1998.
[19] H. Zhuang, Z. S. Roth, and F. Hamano. A
complete and parametrically continuous kine-
matic model for robot manipulators. IEEE
Transactions on Robotics and Automation, 8
(4):451–463, 1992.
[20] Ed Huai hsin Chi, Jin Song, and Greg Corbin.
”killer app” of wearable computing: wireless
force sensing body protectors for martial arts.
In UIST, pages 277–285, 2004.
[21] Masami Takahata, Kensuke Shiraki, Yutaka
Sakane, and Yoichi Takebayashi. Sound feed-
back for powerful karate training. In NIME,
pages 13–18, 2004.
[22] Kai Kunze, Michae Barry, Ernst A. Heinz,
Paul Lukowicz, Dennis Majoe, and Ju¨rg
Gutknecht. Towards recognizing tai chi - an
initial experiment using wearable sensors. In
Proceedings of IFAWC, pages 380–389, 2006.
[23] Daniel Spelmezan and Jan Borchers. Real-time
snowboard training system. In CHI Extended
Abstracts, pages 3327–3332, 2008.
[24] Doo Young Kwon and Markus H. Gross. Com-
bining body sensors and visual sensors for
motion training. In Advances in Computer En-
tertainment Technology, pages 94–101, 2005.
[25] Erwin Schoonderwaldt, Nicolas H. Rasami-
manana, and Fre´de´ric Bevilacqua. Combining
accelerometer and video camera: Reconstruc-
tion of bow velocity profiles. In Proceedings
of the 2006 International Conference on New
Interfaces for Musical Expression, pages 200–
203, 2006.
[26] Ruth E. Mayagoitia, Anand V. Neneb, and
Peter H. Veltink. Accelerometer and rate
gyroscope measurement of kinematics: an in-
expensive alternative to optical motion analysis
systems. Journal of Biomechanics, 35:537–
542, 2002.
[27] Keith L. Moore and Arthur F. Dalley. Clini-
cally Oriented Anatomy. Lippincott Williams
and Wilkins, 4 edition edition, April 1999.
[28] Sheng-Wen Shih, Yi-Ping Hung, and Wei-
Song Lin. New closed-form solution for kine-
matic parameter identification of a binocular
head using point measurements. IEEE Trans-
actions on Systems Man, and Cybernetics,
Part-B, 28(2), 1998.
[29] Zhengyou Zhang. Flexible camera claibration
by viewing a plane from unknown orientations.
In International Conference on Computer Vi-
sion, pages 666–673, 1999.
