2results were sensitive to the fuzzifier. A proper fuzzifier has to be determined before
clustering reference data using PCM for achieving a feasible solution. In this paper, a
distance-based FCM algorithm has been proposed. A boundary distance is derived for each
cluster, and an outlier can be identified when its data point to all cluster centers are greater
than the respective boundary distances. The outlier should be discarded from the reference
dataset.
The rest of this paper is organized as follows. Section 2 briefly describes the PCA
method and derives the recursive formulas of the means, standard deviations and covariances
with additional dataset, whereas the adaptive PCA method is proposed. The FCM, adaptive
FCM and distance-based FCM (DFCM) algorithms are presented in Section 3. The method
of transferring the cluster parameters to the new subspace is detailed in the section. In
section 4, the advantages of the proposed method are assessed through an industrial
application. Finally, the conclusions are given in the last section.
2. Variable Dimension Reduction
2.1. Principal Component Analysis
Consider the data matrix ×m nRW with m rows of observations and n columns of
variables. Each column is normalized to zero means and unit variances:   1 X W 1W S
where W is a mean vector, 1 is a column vector in which all elements are one, and S is a
diagonal matrix of standard deviations. The eigenvectors (P) of the covariance matrix can
be obtained from the normalized dataset. The score vectors are the projection of the data
matrix X to each eigenvector. The data matrix X can be decomposed as:
T T
1 1
k n
i i i i
i i k
ˆ
 
    X t p t p X E (1)
in which ti is the ith score vector that the data project onto the ith eigenvector, pi, and Xˆ
being the projection of the data matrix X onto the subspace formed by the first k eigenvectors
and E being remainder of X that is orthogonal to the subspace.
The statistic Q is defined in order to examine if the new data belong to the PCA subspace
or not.
     TT T Tk kQ     ee x x x x x I P P x (2)
The loading vectors ×n kk RP are the first k terms of eigenvectors of covariance matrix.
Statistic Q is a measure of approximation error of the new data with PCA subspace. Another
measure of the difference between new data and the PCA subspace is the statistic T2.
2 1 T T
k kT
xPΛ Px (3)
The diagonal matrix Λ is the first k terms of eigenvalues, where
 1 2 kdiag ...   Λ . The confidence limits of Q and T2 can be found in Jackson8.
The new data belong to the PCA subspace within  1 confidence limits only when
Q Q and 2 2T T .
2.2. Adaptive PCA
Assuming the data of new events with m' rows of observations, the data matrix is
denoted as ×m' n' RW . The mean vector ( 'W ) and the diagonal matrix of standard
43. Fuzzy Clustering
3.1. Fuzzy C-Means
The objective of fuzzy clustering is to partition the dataset T, which are the compressed
data through PCA, into c clusters with vague boundaries. Gustafson and Kessel7 proposed
each cluster has its own covariance matrix in order to reflect clusters of different geometrical
shapes in one dataset. The object function is defined as:
    2
1 1
i
c m q
ij ij ,, ,
i j
min J ; , , u D
 
 ΣUμΣ T UμΣ ,    
T2 1
iij , j i i j i
D   Σ t μ Σ t μ (11)
where iμ and iΣ are the i
th cluster center and covariance respectively, subject to the
constraints:
 0 1 1 1iju , , i c, j m    ,
1
1 1
c
ij
i
u , j m,

  
1
0 1
m
ij
j
u m, i c

   (12)
where iju is the membership value of the j
th data point sharing with the ith cluster, the score
vectors T are used in fuzzy clustering instead of the original data in this paper. The fuzzifier
q determines the fuzziness of the resulting clusters; when it is closer to 1 the boundaries of
clusters are crisper. Using the Lagrange multiplier method, the Gustafson-Kessel algorithm
can be derived and utilized to iterate the cluster centers and covariances with the reference
data.
3.2. Distance-based FCM
The FCM used the constraints that the memberships of an observation in all clusters
must sum to 1, so that the outliers significantly affect the clustering results. However,
outliers are inevitable in a real-plant dataset. In this paper, a boundary distance for the
cluster has been proposed in order to identify outliers of the reference dataset. The boundary
distance of the ith cluster can be defined as:
3b,i ave,i std ,iD D D  (13)
2
1 1
i
m m
q q
ave,i ij ij , ij
j j
D u D u
 
 Σ ,  
0 5
22
1 1
1
i
.
m m
q q
std ,i ij ij , ave,i ij
j j
D u D D u
 
         
 Σ
where ave,iD and std ,iD respectively are the average and standard deviation of the
Mahalanobis distances that the data points belong to the ith cluster. Based on the assumption
that the data spread following a Gaussian distribution within a steady-state operation, the
boundary distance covers 99.87% data points, which belong to the ith cluster.
The distances of an outlier to each cluster center should be larger than the respective
boundary distances. Therefore, before performing FCM iterations, the outliers should be
trimmed according to their distances. The proposed DFCM algorithm is as follows:
1. The initial clustering results of FCM are used to calculate the boundary distance for each
cluster  1kb,iD , i ...c . Set the number of retained observations equaling to the number
of the reference data, i.e. kretm m , and 0k  .
2. When the observations are inliers, at least one of the Mahalanobis distances should be less
than the corresponding boundary distances. Otherwise, the outliers are discarded.
3. Perform FCM iterations by using the retained observations, in which the numbers of the
retained data points are  1kretm
 . After converging, the boundary distances,
6be derived as follows:
1 1/ k* / k* *
b,i i i b,iD D
Σ Σ (21)
where the  11 T*i k ,k* i k ,k* Σ C ΣC are used.
4. Industrial Application
The proposed method is applied to monitor a polyethylene plant, which is located in
Kaohsiung, Taiwan. In Figure 1, the process flow diagram has been shown. A detailed
description can be found in the previous work9. In this paper, 14 measurement variables,
which are listed in Table 1, were chosen according to the plant experts’advices. The
reference data were collected every 5 minutes for 5 days. There were 1440 observations in
the dataset, including three different production grades, labeled G1, G2 and G3. Figure 2
shows these measured values, which are reactor top temperature and pressure, in arbitrary
units for confidentiality. In the production grade G1 and G2, the specifications of melting
index, the major quality, are the same. The difference of the specifications between the two
production grades is the concentration of the copolymer in the final products. Therefore, the
reactor temperature profile and pressure were regulated at the same ranges, while the
concentrations of the modifiers were maintained at different levels.
There were 1366 observations that could be explained by the PCA subspace, in which
the captured variances were 93% with 4 PCs, within 99% confidence limits. The clustering
results of the projection of the first two score vectors are shown in Figure 3, in which the data
of grade G1 and G2 were clustered as C1 and C2 respectively and the data of grade G3
chronologically shared with clusters C3 and C4. It should be noted that clusters C3 and C4
could be well separated in the third score vector. In the figure, the gray and black lines
represent the boundary distances of the clusters by using FCM and DFCM. It is obvious that
the shapes of the FCM clusters were stretched due to the outliers from the grade changeover.
The situation would be significantly improved by using the proposed method. The outliers
were symbolized as white points in the figure.
The data after the reference dataset were collected in one day. Figure 4 shows the
statistic Q and T2 of the test dataset, in which the solid and dashed lines represent the 99% and
95% confidence limits respectively and the Q chart suggests the new events had emerged after
the 60th observation. The data within the PCA subspace were classified into cluster C4
according to their membership values. The centers, covariances and boundary distances of
cluster C4 were finely adapted by using the additional information. After that, the PCA
subspace was accommodated by using the new event data and the proposed adaptive PCA
method. The adaptive PCA captured 92% variances with 4 PCs, in which the Q and T2
statistics for the test data has been shown in Figure 5. In the figure, almost all of the
statistics of the test data were under their control limits. It shows the proposed method is
capable of updating the subspace to accommodate new events without recalculating the
trained data.
Only the new event data need to be clustered on the new subspace, which was labeled as
C5 in Figure 6, in which the solid line represents the boundary distances of the group. The
parameters of the known groups could be directly transferred from the previous subspace by
using the proposed method. Figure 6 shows the reference data and the cluster boundary
distances were transferred from the previous subspace and symbolized with gray points and
8TIC-003
Primary
Compressor
Secondary
Compressor
Fresh C2
FI-001
Reactor
High
Pressure
Separator
Extrusion
Hopper
Modifier
FIC-001
FIC-002
FIC-004 Purge
FIC-003
FI-002
PIC-001
TIC-002
TIC-004
FI-003
FI-004
FI-005
TIC-001
Figure 1. Process flow diagram of polyethylene autoclave process.
Table 1. Variable descriptions in the polyethylene process.
Abbreviation Description
FI-001 High-purity ethylene flow rate
FIC-001 Modifier (M1) flow rate
FIC-002 Modifier (M2) flow rate
FI-002 The reactor feeding flow rate
TIC-001 The reactor feeding temperature
TIC-002 Reactor top temperature
TIC-003 Reactor middle temperature
TIC-004 Reactor bottom temperature
PIC-001 Reactor pressure
FI-003 Initiator flow rate for regulating TIC-002
FI-004 Initiator flow rate for regulating TIC-003
FI-005 Initiator flow rate for regulating TIC-004
FIC-003 The low-pressure recycle flow rate
FIC-004 The purge flow rate
10
0 20 40 60 80 100 120 140 160 180 200 220 240 260 280
0
1
2
3
4
5
6
Q
Observations
100
101
102
103
T2
Figure 5. Q and T2 statistics for the test data by using the adaptive PCA model.
-6 -4 -2 0 2 4
-4
-2
0
2
4
6
Additional data for C4
Data of new events
Known clusters
New cluster
C5
C4
C2
C1
C3
t 2
t1
Figure 6. The clustering results of the new event data and known groups.
 Applications to Aerospace, Automotive, Electrical, Chemical and
Mechanical Systems.
此外，更安排三個主題演講：
1. Fault Detection and Isolation in Nonlinear Systems
這個演講主要探討動態系統的異常偵測及辨識，所使用的方法是藉由映射
(Affine)、雙線性(Bilinear)以及線性時變系統(Linear Parameter Varying Systems)，
平滑化非線性微分方程組。同時提出兩種解決方案，分別是微分代數(Differential
Algebraic)和微分幾何方法(Differential Geometric Tools)。在微分代數方面，必須
先推衍描述非線性系統的狀態方程組，並且利用此一方程組重建製程異常訊號。
同樣的問題利用微分幾何方法重現，此時必須考量偵測過濾器以及量測輸入值的
設計。
2. Airbus State of the Art and Practices on FDI and FTC
這個演講主要針對 Airbus 飛機中的電子飛行控制系統(Electrical Flight
Control System)異常容忍控制(Fault Tolerant Control，FTC)，以及異常偵測及辨識
(Fault Detection and Isolation，FDI)的策略進行探討。電子飛行控制系統是對於飛
行安全極為重要的設備，當它出現故障時，後果往往是不堪設想。因此在設計過
程中必須嚴格地考量飛行安全、適用性以及可靠性。異常偵測的設計理念已被涵
蓋於異常容忍控制的策略當中，此一設計理念目前仍在 Airbus 離線測試中，尚
未實際應用於真實的飛行案例。
3. Data-Driven Fault Detection and Diagnosis for Complex Industrial Processes
這個演講主要著重於利用資料導向方法應用於化工複雜製程的異常偵及既
診斷，由於化工製程的複雜程度，建議採用多層級的監控方法，也就是依照製程
區的相關特性，切割成多個監控區域，每個區域各自有其監控模式，在區域層級
之上，再建立監控各個區域的全域監控模式，以達到分層監控的目的。此外，也
提出失誤診斷的方法，並且證明該方法能正確地辨識異常變數，相較於利用傳統
統計量貢獻圖會有變數污染的問題，該方法的確能有效地提供正確診斷失誤源的
線索。
     
Monitoring an Air Separation Process with Multiple Operating Modes 
 
Jialin Liu*, Ding-Sou Chen** 
 
* Department of Information Management, Fortune Institute of Technology, 1-10, Nwongchang Rd., Neighborhood 28, 
Lyouciyou Village, Daliao Township, Kaohsiung Country, Taiwan, Republic of China  
(Tel: 886-7-7889888 ext. 6122; e-mail: jialin@center.fotech.edu.tw). 
**Chemical Process & Water Treatment Section, New Materials Research & Development Department, China Steel 
Corporation, 1, Chung Kang Rd., Hsiao Kang, Kaohsiung, Taiwan, Republic of China (e-mail: t6410@mail.csc.com.tw) 
Abstract: In general, an industrial process has to be capable of operating at different modes for fulfilling 
demands of throughputs, product grade changes, and so forth.  The conventional principal component 
analysis (PCA) monitoring methods cannot be applied to this kind of processes, since the historical process 
data would not follow a normal distribution.  In this paper, we assumed that process data follow a Gaussian 
distribution within an operating mode.  The number of operating conditions and the locations, which are 
used as the initial guesses of cluster analysis, are roughly evaluated by using kernel density estimation 
(KDE).  After data clustering, a fault identification index based on cluster centers and covariances from 
Bayesian classification has been proposed.  The proposed approach has been applied to monitor an air 
separation process with multiple operating modes.  Results show that the abnormal situations can be early 
detected in the multimode process and the faulty variables also have been located. 
 
1. INTRODUCTION 
In chemical processes, there are a large number of measured 
and controlled variables which are highly correlated.  
Principal component analysis (PCA) is a popular method of 
compressing high-dimensional inputs into a few orthonormal 
variables.  Past researchers, for example, Wise and Gallagher 
(1996) have constructed PCA subspaces from “healthy” plant 
data for the purpose of fault detection.  The reality is that the 
historical data from a real plant often contain multiple 
operating states, including normal and abnormal situations, 
which makes isolating the “healthy” data a difficulty before 
performing PCA for fault detection.  Another aspect of 
applying PCA on historical data is to reduce variable 
dimensions for cluster analysis.  The score vectors are used to 
extract different operating states from the reference dataset, 
instead of the raw data being used.  For example, Teppola et 
al. (1999) used fuzzy c-means (FCM) for monitoring a 
wastewater treatment plant with two operating conditions.  
Choi et al. (2003) applied credibilistic fuzzy c-means (CFCM) 
for continuous monitoring emissions of a power plant.  He et 
al (2005) isolated different classes of polyester film 
manufacturing data by using k-means clustering.  However, 
with this kind of approach, which reduces variable 
dimensions in order to alleviate the complexity of data 
clustering, the risk is that scores cannot represent the new 
measurements in the on-line classification phase.  Therefore, 
the statistic Q of new data needs to be validated with its 
confidence limits before classifying new data into known 
events. 
The mixture model is a linear combination of priori 
probabilities and conditional probability density functions.  It 
is used for evaluating the unknown probabilities in Bayesian 
classification; the details can be found in Theodoridis and 
Koutroumbas (1999).  Generally, the maximum likelihood 
(ML) method is used to estimate the parameters of the 
mixture model, the means and covariances of each density 
function and the priori probabilities.  The solution of the ML 
problem, usually known as the expectation-maximization 
(EM) procedure, suffers three well-known difficulties.  First, 
there are many local maxima that cause inconsistent solutions 
from different initial conditions for local maximum seeking 
algorithms.  Second, the numbers of clusters have to be 
predefined.  Lastly, outliers are inevitable in a real plant 
dataset.  They will affect the parameters of Gaussian density 
functions and cause instability in the convergence of the 
parameter optimization.  The first two difficulties will be 
alleviated as data density is estimated from the observations 
and then multiple initial values are selected over the density 
function.  The initial values will be attracted into several 
local maxima of the density estimator through a non-smooth 
function optimization.  The numbers of maxima and the 
locations can be used for the numbers of clusters and the 
initial conditions of the EM procedure. 
Density estimation is a non-parameter statistical tool to 
construct a density function from the observed data 
(Silverman, 1986).  In this paper, the estimated density 
function is used to locate cluster centers.  For each cluster, 
the local statistic T2 is validated because that conditional 
probability density function is a Gaussian distribution in the 
mixture model.  In addition, the third difficulty of EM can be 
relieved if the initial conditions are properly found through 
kernel density estimation.  Since outliers are away from any 
cluster center, an α-cut threshold of conditional probability 
density function is set to trim the outliers during EM iteration.  
In this paper, a fault identification index is derived based on 
cluster centers and covariances.  It combines two effects, the 
distance between two centers and rotating directions by 
Preprints of the 7th IFAC Symposium on
Fault Detection, Supervision and Safety of Technical Processes
Barcelona, Spain, June 30 - July 3, 2009
1252
 
 
     
 
3. ILLUSTRATIVE EXAMPLE 
Five columns are used in series to separate air into three 
products, N2, O2 and Ar, as Fig. 1 shows. The purified, high 
pressure and low temperature air is fed into the pressurized 
column (COL1), and then the low pressure column (COL2).  
In the two columns, nitrogen and oxygen are properly 
separated.  A side stream is withdrawn from the low pressure 
column, and then fed into the crude argon columns in order to 
purify argon from oxygen.  Since the boiling points of argon 
and oxygen are very close, it is difficult to separate the two 
components with limited trays in a column.  In practical, the 
column has to be split into two columns (COL3 and COL4).  
The argon composition reaches 99% at the distillate of COL4.  
Then, the distillate is fed into the pure argon column in order 
to further purify the concentration up to 99.9999%.  In the air 
separation process, operators often complained the second 
crude argon column is too sensitive to reject disturbances.  
The column dumping frequently happened when the 
operators were aware of the abnormal situations from the 
measured variables.  It results in the distillate of COL4 highly 
fluctuated, so that the pure argon column has to be shutdown.  
Therefore, an early detection tool is requested to prevent the 
column dumping. 
 
Fig. 1. Air separation process flow diagram. 
In this paper, 21 measured variables were chosen in order to 
monitor the operations of COL4, which have been listed in 
Table 1.  The training data were collected every 5 minutes for 
7 days.  There were 2016 observations in the training dataset.  
There were 1874 observations that could be explained by the 
PCA subspace, in which the captured variances were 92% 
with 3 PCs, within 99% confidence limits.  Figure 2 shows 
the statistic Q and T2 for the training dataset, in which the 
dash and solid lines respectively represent the 95% and 99% 
control limits.  It indicates that there were two abnormal 
situations during the data collection period, which have been 
confirmed with the field operators.  However, the statistic T2 
only detected the first abnormality and skipped the second 
one.  It is because that the control limits of statistic T2 were 
derived based on the assumption that the scores follow a 
normal distribution on the PCA subspace.  The first two 
scores of the training dataset with the control limits are 
plotted in Fig. 3.  Obviously, it shows the scores would not 
distribute following a normal distribution so that the tests of 
T2 in Fig. 2 are invalid.  The control limits of statistic T2 need 
to be revised, especially for a process with multiple operating 
conditions. 
Table 1.  Measured variables for the air separation process. 
No. Variable Description 
1 FIC001.PV Feed stream flowrate 
2 PIC001.PV Feed stream pressure 
3 TI001.PV Feed stream temperature 
4 AIC002.PV O2 concentration of bottom flow at COL1 
5 TIC002.PV Bottom flow temperature at COL1 
6 PIC002.PV Bottom flow pressure at COL1 
7 AIC003.PV O2 concentration of side-draw stream at COL2 
8 TIC003.PV Side-draw stream temperature at COL2 
9 PIC003.PV Distillate stream pressure at COL2 
10 PI004.PV Distillate stream pressure at COL3 
11 FIC004.PV Recycle flowrate at COL4 
12 TI005.PV Distillate stream temperature at COL4 
13 AIC005.PV O2 concentration of distillate stream at COL4 
14 FIC005.PV Distillate stream flowrate at COL4 
15 PIC005.PV Distillate stream pressure at COL4 
16 PI006.PV Condenser pressure at COL4 
17 TI006.PV Condenser temperature at COL4 
18 PDI001.PV Pressure differences between PIC003 and PI004 
19 PDI002.PV Pressure differences between PI004 and PIC005 
20 PDI003.PV Total pressure drop of COL3 and COL4 
21 PDI004.PV Pressure drop of condenser 
0 500 1000 1500 2000
0
10
20
30
40
 
Q
Observations
0
20
40
60
80
T2
 
Fig. 2. The statistic Q and T2 for the training dataset. 
In the training dataset, we assumed the scores distribute 
following a Gaussian distribution within an operating mode.  
Therefore, each operating condition needs to be extracted 
from the training data.  The local maxima and contour plot 
from kernel density estimation with the score vectors have 
been sown in Fig. 4(a), from which three clusters were 
chosen and the initial conditions of EM procedures also were 
located.  The convergence of EM has been shown in Fig. 4(b), 
1254
 
 
     
 
5 10 15 20
0.0
0.5
1.0
1.5
2.0
 Covariance Effect
 Center Effect
C
on
tri
bu
tio
n
Variables  
Fig. 6. Fault identification indices for C1 and C2 classes. 
0 250 500 750 1000 1250
 C1  C2
TI
00
6.
P
V
 
TI
00
5.
P
V
Observations  
Fig. 7. Faulty variables for C1 and C2 classes. 
5 10 15 20
0.0
0.5
1.0
1.5
2.0
 Covariance Effect
 Center Effect
C
on
tri
bu
tio
n
Variables
 
Fig. 8. Fault identification indices for C2 and C3 classes. 
1000 1250 1500 1750 2000
Observations
 
TI
00
1.
PV
 C2 C3
PD
I0
04
.P
V
 
P
D
I0
01
.P
V
 
Fig. 9. Faulty variables for C2 and C3 classes. 
The data after the training dataset were collected every five 
minutes for 7 days.  The statistic Q and T2 of the test dataset 
have been shown in Fig. 10(a), in which the statistic T2 show 
all data were within its control limits, and data around the 
3500th samples were out of the PCA subspace.  It illustrates 
the conventional T2 test of PCA is not suitable for a process 
with multiple operating conditions.  On the contrary, the 
statistic T2 for each class is more reasonable, as Fig. 10(b) 
shows, in which all test data were far away from C1 and C2 
classes and the data before the 2318th sample belonged to C3 
class.  The first two scores of the test data have been plotted 
in Fig. 11.  It shows the data, after the 2318th sample, 
gradually went away from C3 class and grouped into a new 
class.  Therefore, a new cluster needs to be generated for the 
test dataset.  The data leaving C3 class were collected every 
five minutes for 4 hours to generate C4 class, as Fig. 11 
shows.  After that, the newer data were tested with the control 
limits of T2 for C4 class.  If it belonged to C4 class, the 
cluster center, covariance, and control limits were updated 
via the newer data.  Eventually, C4 class was stabilized by 
using all test data in Fig. 11.  The data denoted with triangle 
were trimmed because that the data violated the control limits 
of T2 for C4 at that time.  It could be observed from the third 
score vectors, although some of them seemed to be within C4 
class in Fig. 11. 
2500 3000 3500 4000
0
5
10
15
20
 
Q
Observations
0
5
10
15
20
T2
 
(a) 
1256
