100 年度國科會計畫結案報告 
題目：語音特徵各域之強化技術於強健性語音辨識之研究 
計畫編號：100-2221-E-260-032- 
 
洪志偉 
暨南大學電機系 
 
摘要 
本計畫主要是針對雜訊干擾環境下的語音處理系統所
做之改進其特徵強健性的研究。根據語音訊號在不同
領域 (domain)下的表示式，探討受到雜訊干擾的現
象，進而提供不同的強化技術，藉此提升用以訓練與
辨識之語音特徵的強健性。此計畫中，我們共提出了
強度頻譜強化法、分頻功率正規化法、低通濾波補零
法、離散餘弦轉換之頻譜強度替代法等。經由在國際
知名之Aurora-2連續數字資料庫的實驗環境，驗證上
述所提出的各種新方法皆能有效地提昇語音特徵係數
在雜訊環境下的強健性，以提昇辨識精確度。本計畫
之研究成果已發表在二篇國際期刊論文[1,2]、與二篇
國際會議論文[3,4]及二篇國內會議論文[5,6]中，顯示
研究成果頗受肯定。 
 
I.簡介 
本研究計畫的目的，在於針對語音處理系統其實際應
用環境與系統發展環境不匹配之現象之雜訊干擾的效
應，開發相關的語音特徵強健性技術，來提升語音辨
認系統在實際應用環境下的辨識精確度，增加語音辨
認技術的實用。 
由於梅爾倒頻譜特徵(mel-frequency cepstral coefficients, 
MFCC)為當前語音辨認系統中的主流語音特徵。我們
針對求取MFCC之過程中、語音訊號所在之一連串不
同的領域(domain)表示法上，研發其對應的強健性演
算法。例如： 
1. 頻域(spectral domain)：強度頻譜強化法(magnitude 
spectrum enhancement)，此方法將在章節 II作介
紹。 
2. 時間序列域(temporal domain)：分頻功率正規化
法、低通濾波補零法，這些方法將在章節III作介
紹。 
3. 調變頻域(modulation spectral domain)：離散餘弦
轉換之頻譜強度替代法，此方法將在章節IV作介
紹。 
上述各種方法的共通優點，在於執行上複雜度低，且
有良好的雜訊強健性改善效能，對於當前數位多媒體
通訊與訊號處理上，極具運用價值。 
 
 
 
II. 頻域之強健技術：強度頻譜強化法 
這裡所述的頻域，是指單一音框（短時間範圍內的語
音訊號）中訊號呈現變化快慢的空間。 
II.1 方法介紹 
在訊號處理領域處理雜訊干擾問題時，其主要目的在
於還原原始未受干擾之訊號或其變形，但這裡我們所
發展之技術：強度頻譜強化法 (magnitude spectrum 
enhancement, 簡稱 MSE)，則是將一段訊號中，語音的
片段其頻譜加以放大，而非語音之片段其頻譜加以衰
減，藉以凸顯語音跟非語音的差異。而並非求取近似
乾淨的頻譜強度。MSE 法雖然無法得到人耳清晰可辨
之語音訊號，但是其得到的新頻譜轉換成語音特徵
MFCC 後，卻可以得到良好的辨識結果、大幅降低雜
訊的影響。 
MSE 法之步驟如下： 
1. 將一段訊號之語音音框與非語音音框作區隔。 
2. 將「語音音框」的頻譜強度加以放大，而令「非
語音音框」的頻譜強度設為極小值。 
 
在上述步驟 1 中，我們使用一個端點偵測 (voice 
activity detector, VAD)法： 
 將訊號之音框之所有的對數頻譜強度(logarithmic 
magnitude spectrum) 序 列 ， 以 及 對 數 能 量
(logarithmic energy)序列，皆通過一高通濾波
器。 
 將所有高通的對數頻譜強度序列相加，成一新序
列（以 { }
m
z 表示），而高通之對數能量序列以
{ }
m
e 表示，然後利用此兩序列之值對於每一音
框加以分類為「語音音框」與「非語音音框」： 
1 2m m
z e
m
q qìï > >ïíïïî
         
     
語音音框 若 或第 個音框為 非語音音框 其他
                                                                                    (5) 
其中
1
q 與
2
q 為門檻值，分別設為序列 { }
m
z 與
{ }
m
e 的平均。 
上述之經由高通濾波器處理的對數能量與對數頻譜強
度，經驗證可比原始對數能量或頻譜強度在 VAD 上
的精確度的表現上更好。在區分語音與非語音區段之
除了辨識率之外，圖三繪製了經過一語句在兩種環境
（乾淨無干擾、SNR 為 5 dB ）下藉由 MSE 處理之時
頻圖(spectrogram)，由這兩圖之比較，我們看到 MSE
處理下之乾淨語音仍完整保有原始之頻譜架構，語句
中的靜音片段被突顯出來，而雜訊語音經 MSE 處理
後，頻譜架構也未被明顯破壞，與乾淨環境下之語音
仍有不錯的匹配度，也因此能有尚佳的辨識率。 
 
 
圖 二 ： 經 過 MSE 法 處 理 後 之 語 句 的 時 頻 圖
(spectrogram)：(a)為乾淨環境(b)為5 dB 之SNR的環境 
 
上述提出的 MSE 法，已投稿於 Eurasip Journal of 
Signal Processing 國際期刊中，並已被接受與刊登，詳
細內容請參照文獻[1,3]或此檔案後附的文件。 
 
III. 時間序列域之強健技術 
這裡所述的時間序列域(temporal domain)，是指具有時
間先後順序的音框(frame)，其特徵值隨著時間軸變化
特性所在的空間。 
 
III.1 分頻功率正規化法 
在此新方法：分頻功率正規化法 (sub-band power 
normalization, SBPN)中，藉由將 MFCC 特徵其功率正
規化，來降低雜訊產生的失真效應。具體作法是藉由
離散小波轉換(discrete wavelet transform, DWT)，將語
音特徵時間序列切割成不同的子頻帶特徵序列，接著
將各子頻帶序列之功率值正規化為一事先設定的功率
值（此預設的功率值可由原始乾淨訓練語音之特徵求
得）。值得注意的是，當使用 DWT 進行分頻時，其
將整體頻帶作不一致(non-uniform)的切割，低頻區域
切割地較細，而高頻區域則切割地較寬，換言之，頻
率解析度隨著頻率增加而降低。此 DWT 的特性，運
用於語音特徵時間序列的處理時，恰為其用，因為根
據諸多文獻，語音特徵時間序列其不同的頻率成分對
於語音辨識有不一致的重要性，低頻（約 1 Hz-20 
Hz）最為重要，若此頻帶受到干擾，則對辨識率的影
響比其他頻帶的干擾明顯較大。因此，上述 DWT 的
分頻功率正規化法，可以對於低頻帶較細緻的切割，
進而對各子頻帶作功率上的補償，而比等切子頻帶的
處理方式達到更好的強健化效果。 
SBPN 整體演算法的流程圖如圖三所示，詳細步驟
為： 
1. 將各語音特徵時間序列 { [ ];0 1}c n n N£ £ - 藉由
L-級的 DWT 切割成 L 個子頻帶訊號，第 l 個子頻
帶訊號 [ ]m
l
c n , 約在以下所述的頻率範圍之間：:  
 
             
1
2 1
1 1
1
[0 ( )] 1
22
2 2
[ ( ) ( )] 2
2 22 2
s
L
l l
s s
L L
F
l
F F
l L
-
- -
- -
ìïï , , =ïïï ,íïï , , £ £ïïïî
 (8) 
 
其中， 
s
F  (Hz) 是音框的取樣頻率，故全頻帶範圍是
[0, 2
s
F  Hz] 。.  
2. 接著，將各子頻帶的特徵序列作以下的功率正規
化處理： 
 
                ,
,
[ ] [ ]
m
target lm m
l l m
single l
P
n c nc
P
= ´ ,  (9) 
 
其中  [ ]ml nc  是新子頻帶語音特徵序列，  .mtarget lP  與 
,
m
single l
P  分別為參考子頻帶功率值與當下處理之子頻帶
序列 [ ]m
l
c n 原始功率值。 
3. 將更新後的各子頻帶特徵序列藉由反離散小波轉
換，合成為全頻帶的新語音特徵序列。  
 
 實驗結果與討論 
我們沿用前一節所述之 Aurora-2 資料庫的實驗環境，
來執行 SBPN 的實驗，跟之前設定唯一不同之處，在
於我們將每個數字 HMM 模型各狀態(state)所包含的高
斯混合(Gaussian mixture)增加至 20 個，以提升整體辨 
識率。所得實驗結果列在表二之中。 
我們沿用第 I 節所述之 Aurora-2 資料庫的實驗環境，
來執行 LFZI 的實驗，類似 SBPN 的實驗，我們將每
個數字 HMM 模型各狀態 (state)所包含的高斯混合
(Gaussian mixture)增加至 20 個，以提升整體辨識率。
所得實驗結果列在表三之中。 
 
表三：LFZI 之辨識率(%) 
 Set A Set B Set C  Average
Approximate part of DWT-
processed MFCC 45.78 46.71 46.10 46.20 
DWT lowpass-filtered MFCC 73.02 70.03 79.90 74.32 
LFZI 78.34 80.21 77.15 78.57 
MFCC baseline 71.13 67.55 78.53 72.40 
 
從表三中，我們看出 LFZI 此一簡易的運算，即可使
辨識率從基礎實驗的 72.40％提升至 78.57%，而當我
們只使用前述之 DWT 的低頻帶特徵序列但不予以補
零（如表三的第二列），辨識效果變的十分低落，此
顯示音框取樣率不宜變低，而若直接將 DWT 使用的
低通濾波器施加於原始 MFCC 特徵、不作降取樣的處
理時（如表三的第二列），其辨識率可提升至
74.32%，但仍不及我們所提出之 LFZI 法的表現。 
 
上述提出的 LFZI 法，已投稿於 International Journal of 
Innovation, Management and Technology (IJIMT) 國際期
刊中，並已被接受與刊登，詳細內容請參照文獻[2,4]
或此檔案後附的文件。 
 
IV. 調變頻譜域之強健性技術 
語音特徵之調變頻譜，是指語音特徵時間序列，經由
傅立葉轉換（或其變形）所得之頻譜，其通常更能反
映語音特徵在時間序列上的特性。在這裡，我們提出
了離散餘弦轉換(discrete cosine transform, DCT)所得
之調變頻譜的補償技術：DCT 強度頻譜替換法。 
 
 在數位訊號處理的領域上，離散餘弦轉換(discrete 
cosine transform, DCT)為一個很有用的分析工具，且
已被成功的應用在許多領域上，像是資料壓縮、資料
編碼、特徵擷取等。DCT 類似於離散傅立葉轉換
DFT，皆是利用一系列無限長度的週期性訊號做為基
底(basis)來分析一任意訊號。但 DCT 和 DFT 最大的不
同在於，前者只使用餘弦函數(cosine functions)，而後
者則同時使用正弦和餘弦函數(sine functions)。ㄧ實數
訊號其 DFT 值（通常稱為頻譜）一般而言皆是複數
（相角可為任意值），其 DCT 值也為實數（相角只可
能為 0 或 p）。DCT 共有八類變型，但由於第二類的
DCT(DCT-II)具備較佳的能量緊縮性質，最被廣為使
用，在很多影像與視訊壓縮的國際標準中 (例如
JPEG、MPG 即 H.261)也皆採用 DCT-II，因此一般提
到的 DCT 通常就是指 DCT-II。 
  對一有限長度的實數序列 { [ ];0 1}x n n N£ £ - 而
言，其 DFT 和 DCT 可分別表示為下式(10)與(11)： 
21
0
[ ] [ ] ,
N j
NX k x n e
p- -
=
= å kn
n
0 1, ,k N= -                      (10) 
1 2
2
1
0
[ ] [ ]cos ( 1) ,
N
k
C k x n n k
NN
pm
-
=
æ ö÷ç ÷= +ç ÷ç ÷çè øån    
0 1, ,k N= -                                                          (11) 
其中 0 1m = ， 2m =k 當 1,2,...,k N= 。 
此外 [ ]X k 和 [ ]C k 彼此關係可表示為： 
2
2
[ ] 2 [ ] , 0, , 1
[2 ] 2 [ ], 0, , 1
k
j
N
k
j
N
X k e C k k N
X N k e C k k N
p
p-
ìïï = = ¼ -ïïíïï - = = ¼ -ïïî
           (12) 
從式(12)可以看出，由於其中 DFT 之 [ ]X k 成左右對
稱，有效長度約為原始長度的一半，但 DCT 之有效長
度則為原始長度，此表示 DCT 相對於 DFT 有較高的
頻率解析度。另外，式(12)也顯示了 DCT 可以藉由具
有快速演算法的 DFT 間接求得。在 DCT 係數中，
[0]C 通常稱為直流(direct current, DC)成分，而其他的
[ ]C k 值則稱為交流(alternating current, DC)成分。 
 
以下，我們介紹所提出之基於 DCT 之特徵調變頻譜補
償技術。假設 { }, 0 1x n n Né ù £ £ -ê úë û 為單一語句之某
一維待處理的語音特徵時間序列，我們希望其處理過
後所得的新語音特徵序列{ }, 0 1x n n Né ù £ £ -ê úë û 能更具
雜訊強健性，以下我們即描述所新提出，基於 DCT 的
特徵序列處理演算法：DCT 強度替換法 (DCT-MS)。 
此方法的發展背景在於，當語音特徵序列受到雜訊
干擾後，其 DCT 係數的強度(magnitude)成分會明顯失
真，但除非在雜訊非常大的情況之下，其 DCT 係數的
性質符號(即正負號)才會被明顯改變，文獻指出，欲
從受干擾的語音特徵估測乾淨語音特徵的相位成分，
最佳的估測值就是其原始受干擾之特徵其相位成分。
因此本篇論文中，我們著重於更新語音特徵其 DCT 的
強度成分，而保留相位成分不作改變。在講述步驟
前，我們先以簡單的試驗，來觀察雜訊對於語音特徵
之 DCT 的影響。 
表 四 列 出 了 一 語 音 檔 (Aurora-2 資 料 庫 中 的
MBM_7Z69616A.08 檔)其第一維 MFCC 特徵(c1)序列
之 DCT 係數，在各種訊雜比(signal-to-noise ratio, SNR)
下，與原始乾淨特徵之 DCT 係數在正負號上的相似度
（即相角的相同比例），以下式(13)所示： 
後 ， 我 們 保 留 Cˆ ké ùê úë û 的 性 質 符 號 （ 正 負 號 ）
( )ˆsgn C ké ùê úë û ，而將Cˆ ké ùê úë û的強度值 Cˆ ké ùê úë û 以步驟一所得之
參考強度取代，換言之，新語音特徵序列之 DCT 以下
式表示： 
( )ˆsgnrefC k A k C ké ù é ù é ù=ê ú ê ú ê úë û ë û ë û ,     0 1k M£ £ -              (17) 
3.使用反離散餘弦轉換(inverse discrete cosine transform, 
IDCT)求取新特徵序列 
我們將(7)式所得之新 DCT 序列作M 點的 IDCT，進
而求得時間序列域上的新語音特徵，值得注意的是，
為了使新語音特徵的點數（音框數）與原始語音特徵
點數L相同，我們只保留上述步驟所得的M 點新語音
特徵之前 L 點，根據經驗顯示，此步驟中所捨棄的
M L- 點，其值都很小，因此其所造成的資訊損失或
誤差可以忽略不計。 
以上所述之 DCT 強度替換法，其特點大致歸納如下： 
I. 由於語音特徵序列之 DCT 的強度替代為參考強
度，此相當於捨棄原始特徵序列之 DCT 強度的資訊，
其優點是可將雜訊造成的強度失真完全忽略不計，而
可能的缺點則是原始強度所包含的語音辨識資訊也因
此喪失。 
II. 一般而言，DCT 具有資料壓縮的良好性質，我
們發現，其運用在語音特徵時間序列的處理上，同樣
顯示了此優點，即強度較大的值都集中在其 DCT 序列
前端少數幾項（即低頻成分），此代表了我們在儲存
特徵值時，可用較少的空間，而仍幾乎保有原始特徵
的資訊。 
 
 實驗結果與討論 
我們採用之前所述之 Aurora-2 資料庫的實驗環境，
作相關的實驗與討論，表五列出了原始 MFCC 特徵及
我們新提出之 DCT-MS 法作用於原始特徵所得之平均
辨識率，而表六則列出 DCT-MS 法作用於許多正規化
法(如 CMN [9]、CMVN [10]、HEQ [11]、CGN [12] 與
MVA [13])處理之改良特徵的辨識率。 
從表 II 和表 III 的數據，我們可以觀察到： 
1. DCT-MS 法處理在 MFCC 特徵上，即可使平均辨識
率從 71.58%大幅提昇至 83.59%，相對錯誤率降低率
約 42%，這顯示了此新方法對於強化語音特徵上有十
分顯著的效果。 
2. DCT-MS 法與各種特徵正規化法結合時，相較於單
一特徵正規化法，可產生更佳的辨識率，代表 DCT-
MS 對這些特徵正規化法具有良好的加成性，例如
DCT-MS 與 CMN 法結合後，平均辨識率可高達
89.68%，高於單一 CMN 法之 80.71%。 
 
 
表五：DCT-MS 作用於原始語音特徵之辨識率 
 Set A Set B Set C Avg RR 
MFCC  71.92 68.22 77.61 71.58 － 
DCT-MS 82.73 84.55 83.39 83.59 42.26%
 
表六：DCT-MS 與特徵正規化法結合所得之平均辨識
率 
 Set A Set B Set C Avg RR 
CMN 79.37 82.47  79.90  80.71 － 
DCT-MS+CMN 89.15 90.45 89.23 89.68 46.50
CMVN 85.03 85.56 85.6 85.35 － 
DCT-MS + 
CMVN 89.29 90.55 89.28 89.79 30.29
HEQ 87.59 88.84 87.64 88.1 － 
DCT-MS + 
HEQ 88.5 90 89.04 89.21 9.31%
CGN 87.64 88.55 87.73 88.02 － 
DCT-MS + 
CGN 89.25 90.58 89.27 89.79 14.73
MVA 88.12 88.81 88.50 88.47 － 
DCT_MS+ 
MVA 88.93 90.20 88.88 89.42 8.24
 
 
除了辨識率以外，我們觀察提出的 DCT-MS 是否能
有效降低語音特徵在調變頻譜上的失真。我們採用
Aurora-2 中的 MBM_7Z69616A.08 語音檔，在不同訊
雜比的地下鐵雜訊下，經特徵擷取後所得的語音特徵
畫出其功率頻譜密度(power spectral density, PSD)圖，
其中圖六(a)和(b)分別為原始未經處理和經 DCT-MS
法處理之倒頻譜特徵 c1 的功率頻譜曲線圖。 
根據圖六的功率頻譜密度曲線圖的結果，我們可以
發現，從圖六(a)得知，在不同訊雜比下(clean, 10 dB 
與 0dB)，未經處理過的倒頻譜 c1 特徵序列，其功率
頻譜密度曲線受到加成性雜訊(additive noise)的影響有
明顯失真的情形。而經過我們所提出的 DCT-MS 法處
理後，其結果為圖六(b)所示。功率頻譜曲線在全部的
調變頻率範圍(0 Hz~50 Hz)的失真的情況已有很明顯
的降低，由此可見此新方法在提升語音強健性的成
效。  
這裡所提出的 DCT-MS 法，已發表於兩篇會議論文
[5,6]中，目前正進行投稿至國際期刊的準備。 
 
UN
CO
RR
EC
TE
D
PR
OO
F
Hung et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:189
http://asp.eurasipjournals.com/content/2012/1/189
RESEARCH Open Access1
Enhancing the magnitude spectrum of speech
features for robust speech recognition
2
3
Jeih-weih Hung*, Hao-teng Fan and Wen-hsiang Tu4
Abstract5
In this article, we present an eﬀective compensation scheme to improve noise robustness for the spectra of speech
signals. In this compensation scheme, called magnitude spectrum enhancement (MSE), a voice activity detection
(VAD) process is performed on the frame sequence of the utterance. The magnitude spectra of non-speech frames are
then reduced while those of speech frames are ampliﬁed. In experiments conducted on the Aurora-2 noisy digits
database, MSE achieves an error reduction rate of nearly 42% relative to baseline processing. This method
outperforms well-known spectral-domain speech enhancement techniques, including spectral subtraction (SS) and
Wiener ﬁltering (WF). In addition, the proposed MSE can be integrated with cepstral-domain robustness methods,
such as mean and variance normalization (MVN) and histogram normalization (HEQ), to achieve further improvements
in recognition accuracy under noise-corrupted environments.
6
7
8
9
10
11
12
13
14
Keywords: Voice activity detection, Robust speech recognition, Speech enhancement15
Introduction16
The environmental mismatch caused by additive noise17
and/or channel distortion often seriously degrades the18
performance of speech recognition systems. Various19
robustness techniques have been proposed to reduce this20
mismatch, which can be roughly divided into two classes:21
model-based and feature-based approaches. In model-22
based approaches, compensation is performed on the23
pre-trained recognition model parameters so that the24
modiﬁed recognition models can more eﬀectively clas-25
sify the mismatched test speech features collected in the26
application environment. Typical examples of this class27
include noise masking [1-3], speech and noise decompo-28
sition (SND) [4], vector Taylor series (VTS) [5], maximum29
likelihood linear regression (MLLR) [6], model-based30
stochastic matching [7,8], model compensation based on31
non-uniform spectral compression (MC-SNSC) [9], sta-32
tistical re-estimation (STAR) [10], and parallel model33
combination (PMC) [11-13] methods. In the feature-34
based approaches, a noise-robust feature representation35
is developed to reduce the sensitivity to various acous-36
tic conditions and thereby alleviate the mismatch between37
*Correspondence: jwhung@ncnu.edu.tw
Department of Electrical Engineering, National Chi Nan University, Taiwan,
Republic of China
those features used for training and testing. Examples of 38
this class include spectral subtraction (SS) [14-17],Weiner 39
ﬁltering [18,19], short-time spectral amplitude estima- 40
tion based on minimum mean-squared error criteria 41
(MMSE-STSA) [20], MMSE-based log-spectral amplitude 42
estimation (MMSE log-STSA) [21], codeword-dependent 43
cepstral normalization (CDCN) [22], SNR-dependent 44
non-uniform spectral compression scheme (SNSC) [23], 45
feature-based stochastic matching [7,8], multivariate 46
Gaussian-based cepstral normalization (RATZ) [10], 47
stereo-based piecewise linear compensation for environ- 48
ments (SPLICE) [24,25] methods, and a series of cepstral- 49
feature statistics normalization techniques such as 50
cepstral mean subtraction (CMS) [26], cepstral mean and 51
variance normalization (MVN) [27], MVN plus ARMA 52
ﬁltering (MVA) [28], cepstral gain normalization (CGN) 53
[29], histogram equalization (HEQ) [30,31], and cepstral 54
shape normalization (CSN) [32]. A common advantage of 55
the feature-based methods is their relative simplicity of 56
implementation. This simplicity arises because all of these 57
methods focus on front-end speech feature processing 58
without any need to change the back-end model training 59
and recognition schemes. Despite their simplicity, these 60
methods usually improve recognition performance signif- 61
icantly in noise-corrupted application environments. 62
© 2012 Hung et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction
in any medium, provided the original work is properly cited.
UN
CO
RR
EC
TE
D
PR
OO
F
Hung et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:189 Page 3 of 20
http://asp.eurasipjournals.com/content/2012/1/189
random variables with Rician and Rayleigh distributions162
[38], respectively. The parameter MSR in Equation (3) is163
then164
γ [ k] = π2 exp
(
−|Sp[ k] |
2
4σ 2
)((
1 + |Sp[ k] |
2
2σ 2
)
I0
(
|Sp[ k] |2
4σ 2
)
+ |Sp[ k] |
2
2σ 2 I1
(
|Sp[ k] |2
4σ 2
))
,
(4)
where σ 2 is the variance of the real- and imaginary- parts165
of the noise Dm[ k], m = p, q , and I0(.) and I1(.) are the166
modiﬁed Bessel functions of the ﬁrst kind with orders zero167
and one, respectively. Furthermore, γ [ k] in Equation (4)168
is, in fact, monotonically decreasing with respect to the169
noise variance σ 2 (see Appendix 1 for a detailed analysis of170
the above results), indicating that speech frames become171
increasingly indistinguishable from non-speech frames172
based on their magnitude spectra as the signal-to-noise173
ratio (SNR) decreases.174
Eﬀect of additive noise on the logarithmic magnitude175
spectrum in the frame sequences176
First, we investigate the eﬀect of noise on the logarith-177
mic magnitude spectrum in an arbitrary frame within an178
utterance. According to Equation (2), we have179
X(l)m [ k] = log(|Xm[ k] |)
= 0.5 log(|Xm[ k] |2)
= 0.5 log(|Sm[ k]+Dm[ k] |2)
≈ 0.5 log(|Sm[ k] |2 + |Dm[ k] |2)
= 0.5 log(exp(2S(l)m [ k] ) + exp(2D(l)m [ k] )), (5)
where X(l)m [ k], S(l)m [ k], and D(l)m [ k] are the logarithmic180
magnitude spectra of xm[ n], sm[ n], and dm[ n], respec-181
tively, from Equation (1). Thus, the diﬀerence between182
X(l)m [ k] (for the noise-corrupted speech) and S(l)m [ k] (for183
the embedded clean speech) is184
[ k] = X(l)m [ k]−S(l)m [ k]
≈ 0.5 log
(
1 + exp(2D
(l)
m [ k] )
exp(2S(l)m [ k] )
)
= 0.5 log
(
1 + |Dm[ k] |
2
|Sm[ k] |2
)
, (6)
From Equation (6), it is obvious that under the same185
noise magnitude level |Dm[ k] |, the diﬀerence [ k]186
decreases as the speech magnitude |Sm[ k] | increases.187
Therefore, for a noise-corrupted utterance, the logarith-188
mic magnitude spectrum of the speech frame is often less189
vulnerable to noise than that of the non-speech (noise-190
only) frame. However, this condition does not hold for the191
(linear) magnitude spectrum.192
Next, let us consider the eﬀect of noise on the frame 193
sequence of logarithmic magnitude spectra, denoted 194
by {X(l)m [ k] }M−1m=0 , for the utterance. Taking the Tay- 195
lor series approximation of Equation (5) with respect to 196
(S(l)m [ k] ,D(l)m [ k] ) = (0, 0) up to order 2, we have 197
X(l)m [ k]=0.5 log(exp(2S(l)m [ k] ) + exp(2D(l)m [ k] ))
≈0.5 log 2+0.5(S(l)m [ k]+D(l)m [ k] )+0.25((S(l)m [ k] )2
+ (D(l)m [ k] )2 − 2S(l)m [ k]D(l)m [ k] ). (7)
Thus the modulation spectrumMX(jω) of the sequence 198
{X(l)m [ k] }M−1m=0 , computed by 199
MX(jω) =
M−1∑
m=0
X(l)m [ k] e−jωm, (8)
can be approximated as 200
MX(jω) ≈ (π log 2)δ(ω) + 0.5(MS(jω) + MD(jω))
+ 18π (MS(jω)∗MS(jω) + MD(jω)∗MD(jω)
− 2MS(jω)∗MD(jω)), (9)
where MX(jω), MS(jω), and MD(jω) are discrete- 201
time Fourier transforms (DTFTs) of {X(l)m [ k] }M−1m=0 , 202
{S(l)m [ k] }M−1m=0 , and {D(l)m [ k] }M−1m=0 (along the frame axis 203
with the index m, as in Equation (8)), respectively, and 204
the symbol “∗” denotes the convolution operation. If the 205
two sequences, {S(l)m [ k] }M−1m=0 and {D(l)m [ k] }M−1m=0 , are both 206
low-pass and their bandwidths are Bs and Bd , respectively, 207
then the terms MD(jω) ∗MD(jω) and MS(jω) ∗MD(jω) 208
in Equation (9) have bandwidths of 2Bd and Bs + Bd, 209
respectively. This ﬁnding implies that {X(l)m [ k] }M−1m=0 has a 210
wider bandwidth than {D(l)m [ k] }M−1m=0 . In other words, the 211
logarithmic magnitude spectrum of the noise-corrupted 212
speech segment possesses higher modulation frequency 213
components than that of the noise-only segment in a 214
noisy utterance. Again, this condition does not hold for 215
the (linear) magnitude spectrum. 216
Note: it is easy to demonstrate that the above analysis of 217
the logarithmicmagnitude spectrum can be performed on 218
the logarithmic energy (log E) sequence in an utterance, 219
obtaining the same conclusions [35]. That is, 220
1. The logarithmic energy is less distorted in a speech 221
frame than in a non-speech frame. 222
2. For the logarithmic energy sequence of a noisy 223
utterance, the speech segment possesses components 224
of even-higher frequency than the non-speech 225
segment. 226
UN
CO
RR
EC
TE
D
PR
OO
F
Hung et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:189 Page 5 of 20
http://asp.eurasipjournals.com/content/2012/1/189
Finally, the result of the VAD process is obtained from290
the two parameters dm,1 in Equation (14) and dm,2 in291
Equation (15):292
dm =
{
1 if dm,1 = 1 or dm,2 = 1
0 otherwise , 0 ≤ m ≤ M − 1,
(16)
where dm is the VAD indicator ﬁnally used. That is, the293
mth frame is classiﬁed as speech if either dm,1 or dm,2 is294
equal to unity. The main reason for using the “or” opera-295
tion in Equation (16) is that the speech frames are likely to296
be misclassiﬁed as non-speech frames (i.e., a higher false-297
rejection rate) when we simply depend on either decision298
parameter dm,1 or dm,2 alone, especially when the SNR299
degrades.300
Step II. Obtain the enhanced magnitude spectrum301
This step ampliﬁes the magnitude spectrum for the302
speech frames while diminishing it for the non-speech303
frames. The main purpose of this step is to enlarge the304
ratio of speech frames to non-speech frames in magnitude305
spectra to reduce the noise eﬀect, as discussed in Section306
‘Eﬀect of additive noise to the linear and logarithmic307
magnitude spectrum of a speech signal’. The magnitude308
spectra for the non-speech frames detected in Step I are309
ﬁrst collected and then averaged to obtain the estimated310
noise (magnitude) spectrum for the utterance:311
N[ k]=
M−1∑
m=0
(1 − dm)|Xm[ k] |
M−1∑
m=0
(1 − dm)
, 0 ≤ k ≤ K2 . (17)
Note that here, N[ k] is independent of the frame index312
m. Thus, the noise spectrum is estimated once for the313
utterance.314
Next, a weighting factor for each magnitude spectral315
value Xm[ k] is deﬁned as follows:316
wm[ k]=
⎧⎨
⎩
( |Xm[ k] |
N[ k]+δ
)α
if dm=1
ε if dm=0
, 0 ≤ k ≤ K2 , 0≤m≤M−1, (18)
where α is a parameter within the range [ 0, 1] that deter-317
mines the degree of ampliﬁcation, δ is a small positive con-318
stant that avoids the weighting factor becoming inﬁnitely319
large as N[ k]−→ 0, and ε is a very small positive ran-320
dom variable such that the magnitude spectra of detected321
non-speech frames are signiﬁcantly reduced.322
Thus, the weighting factor for a speech frame (dm = 1)323
in Equation (18) is related to the SNR as follows:324
wm[ k]≈ (
√
SNRm[ k] + 1)α , (19)
where SNRm[ k]=
( |Xm[ k] |2
N2[ k]
)
−1 is the (estimated) SNR325
for the kth frequency bin of themth frame.326
Finally, the enhanced magnitude spectrum is obtained 327
by multiplying the original magnitude spectrum with the 328
weighting factor wm[ k] in Equation (18): 329
|X˜m[ k] | = wm[ k] |Xm[ k] |, 0 ≤ k ≤ K2 , 0 ≤ m ≤ M−1,
(20)
The proposed MSE has the following properties: 330
1. In MSE, the embedded VAD process uses the 331
logarithmic magnitude spectrum rather than the 332
linear magnitude spectrum. According to the 333
discussions in Section ‘Eﬀect of additive noise to the 334
linear and logarithmic magnitude spectrum of a 335
speech signal’, the logarithmic magnitude spectrum 336
is less vulnerable to noise in speech frames, and its 337
temporal-domain sequence exhibits a wider 338
(modulation) spectral bandwidth in speech portions 339
than in non-speech portions. Based on these two 340
characteristics, the logarithmic magnitude spectrum 341
is a more appropriate VAD indicator than the linear 342
magnitude spectrum. The experimental results 343
shown later will reveal that the logarithmic 344
magnitude spectrum outperforms the linear 345
magnitude spectrum for providing MSE with better 346
recognition accuracy. 347
2. By assigning diﬀerent weights to the magnitude 348
spectra of speech and non-speech frames, the speech 349
portions of an utterance are highlighted and the 350
diﬀerence between the speech and non-speech 351
portions in magnitude spectrum is strongly 352
emphasized. This eﬀect leads to a large magnitude 353
spectral ratio (MSR) as deﬁned in Equation (2) and 354
implies that the eﬀect of noise has been eﬀectively 355
reduced. 356
3. The idea of MSE is partially motivated by the 357
matched ﬁlter theory in the ﬁeld of communications 358
[38]. For an observed signal denoted by 359
x[ n]= s[ n]+d[ n], where s[ n] and d[ n] are the 360
desired signal and additive noise, respectively, the 361
magnitude (frequency) response of the matched ﬁlter 362
which maximizes the output SNR is [39]: 363
|H(jω)| = |S(jω)|Pd(ω) , (21)
where |S(jω)| and Pd(ω) are the magnitude spectrum 364
of s[ n] and the power spectral density of the noise 365
d[ n], respectively. From Equation (21), we ﬁnd 366
|H(jω)| is proportional to the input frequency 367
domain SNR (deﬁned by |S(jω)|
2
Pd(ω)
) provided the 368
signal level |S(jω)|2 or the noise level Pd(ω) is ﬁxed. 369
Thus, MSE shares the idea of the matched ﬁlter and 370
uses a spectral weighting factor wm[ k] in 371
UN
CO
RR
EC
TE
D
PR
OO
F
Hung et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:189 Page 7 of 20
http://asp.eurasipjournals.com/content/2012/1/189
evaluate the degree of robust capability of the speech fea-464
tures (associated with the robustness algorithm) against465
noise. As for the second mode, “multi-condition training”,466
the corresponding results can reveal the impact of a dif-467
ferent type of noise or a diﬀerent SNR than seen during468
training [37]. In our following experiments and discus-469
sions, we will primarily focus on the ﬁrst mode in order470
to observe the presented MSE in the reduction of noise471
eﬀect. However, we will also provide the experimental472
results for the second mode together with relatively brief473
discussions.474
Results for the task of clean-condition training and475
multi-condition testing476
With the Aurora-2 database under the mode of “clean-477
condition training”, we perform the MSE method and478
a series of robustness methods to compare the recog-479
nition accuracy. As for the cepstral-domain methods,480
each utterance in the clean training set and three test-481
ing sets is directly converted to 13-dimensional MFCC482
(c1–c12, c0) sequence according to the feature settings483
in [37]. Next, the MFCC features are processed using484
MVN, MVA or HEQ. The spectral-domain methods used485
here include our MSE, spectral subtraction (SS), Wiener486
ﬁltering (WF) and MMSE-based log-spectral amplitude487
estimation (MMSE log-STSA). Each utterance is ﬁrst pro-488
cessed in the linear spectral domain. The updated spectra489
are converted to a sequence of 13-dimensional MFCC490
((c1–c12, c0)). The resulting 13 new features, plus their491
ﬁrst- and second-order derivatives, are the components of492
the ﬁnal 39-dimensional feature vector. With the new fea-493
ture vectors in the clean training set, the hidden Markov494
models (HMMs) for each digit and silence are trained with495
the demo scripts provided by the Aurora-2 CD set [42].496
Each digit HMM has 16 states, with 3 Gaussian mixtures497
per state.498
Detailed information about some of the methods used499
follows:500
1. We apply three versions of spectral subtraction (SS)501
proposed in [14-16]. For the purposes of clarity, they502
are denoted by SSBoll, SSBerouti, and SSKamath,503
respectively, in which the author names are504
represented by the subscripts.505
2. As with spectral subtraction, three versions of the506
Wiener ﬁltering (WF) methods proposed in [18,19]507
are tested here. The ﬁrst method is based on a priori508
signal to noise ratio (PSNR) estimation, and the509
latter two WF methods apply a two-step noise510
reduction (TSNR) procedure and a harmonic511
regeneration noise reduction (HRNR) scheme,512
respectively. Thus, these methods are abbreviated as513
WFPSNR, WFTSNR, and WFHRNR for later514
discussions.515
3. For the proposed MSE, the parameters δ in Equation 516
(18) is set to 0.001, and the positive random number 517
ε in Equation (18) is uniformly distributed within the 518
range (0, 10−5). In order to obtain a proper selection 519
of the ﬁlter coeﬃcient λ in Equation (12) and the 520
weight parameter α in Equation (18), we use the 8440 521
noise-corrupted training utterances for the mode of 522
“multi-condition training” in the Aurora-2 database 523
as the development set. The averaged recognition 524
accuracy rates with respect to diﬀerent assignments 525
of λ and α (both from 0.1 to 0.9 with an interval of 526
0.2) are shown in Table 1. As a result, we set λ and α T1527
to 0.7 and 0.5, respectively, since such a setting gets 528
the optimal accuracy rate for the development set. 529
4. For MVA, the order of the ARMA ﬁlter is set to 3. 530
5. For HEQ, each feature stream in the utterance is 531
normalized to approach a Gaussian distribution with 532
zero mean and unity variance. 533
Comparison of various noise robustness approaches 534
Table 2 presents the individual set recognition accuracy T2535
rates averaged over ﬁve SNR conditions (0–20 dB at 5 dB 536
intervals) for Test Sets A, B, and C, achieved using various 537
approaches.Figure 2 shows the accuracy rates for spectral- F2538
domain methods under diﬀerent SNR conditions, which 539
are obtained by averaging over all ten noise types con- 540
tained in the three Test Sets. Based on Table 2 and 541
Figure 2, we make the following observations: 542
1. Compared to baseline processing, most approaches 543
provide signiﬁcant recognition accuracy 544
improvement in almost all cases. All three SS 545
methods give better results than the baseline for Test 546
Sets A and B, while the improvement for Test Set C 547
is relatively insigniﬁcant. A possible explanation of 548
this ﬁnding is that SS is particularly designed to 549
alleviate additive noise and thus does not handle the 550
channel mismatch in the utterances of Test Set C 551
Table 1 The averaged recognition accuracy rates (%) of
the development set (themulti-condition training data in
the AURORA-2 database) achieved byMSE with diﬀerent
ﬁlter coeﬃcients λ in Equation (12) and exponents α in
Equation (18).
t1.1
t1.2
t1.3
t1.4
t1.5
t1.6The exponent α
in Equation (18)
The ﬁlter coeﬃcient λ in Equation (12)
t1.70.1 0.3 0.5 0.7 0.9
t1.80.1 88.22 88.44 88.73 88.96 88.90
t1.90.3 89.21 89.49 89.71 90.03 89.54
t1.100.5 89.53 89.66 89.86 90.38 89.78
t1.110.7 89.98 89.94 90.21 90.28 89.54
t1.120.9 89.93 89.80 90.06 90.27 89.31
UN
CO
RR
EC
TE
D
PR
OO
F
Hung et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:189 Page 9 of 20
http://asp.eurasipjournals.com/content/2012/1/189
Figure 3 The SSBoll–processed spectrogram of an utterance under two SNR levels: (a) clean, (b) 5dB.
∫∞
2.326
1√
2π e
− u22 du ≈ 1% = 1 − 99%). According to602
the obtained test statistic z in Equation (25), we ﬁnd603
that the improvement brought by MSE relative to the604
other spectral-domain methods is statistically605
signiﬁcant. For example, when the method for606
comparison is MMSE log-STSA, the corresponding607
test statistic z in Equation (25) is 41.99, far larger608
than the threshold 2.326.609
In addition to the recognition accuracy, we also exam-610
ine the various spectral-domain methods’ capabilities of611
reducing the spectrogram mismatch caused by additive612
noise. Figures 3, 4, 5, 6, 7, 8, 9, and 10 show the spec-F3-F10 613
trograms of a digit utterance (“FLJ 97159A.08” in the614
Aurora-2 database) for two SNR levels, clean and 5 dB615
(with babble noise), obtained by SSBoll, SSBerouti, SSKamath,616
WFPSNR, WFTSNR, WFHRNR, MMSE log-STSA and the617
proposed MSE, respectively. First, the ﬁgures show that618
for the clean case, the voiced portions and the short 619
pauses between any two consecutive digits or syllables 620
are clearly revealed using almost all approaches. Second, 621
for the noise-corrupted case, WFPSNR, MMSE log-STSA, 622
and MSE highlight the short pauses more than the other 623
approaches, and they preserve the voiced segments bet- 624
ter with less distortion (especially in the region [0.7 s, 625
1.3 s]). Thus, the similar treatment of these short pauses 626
under clean and noise-corrupted conditions using the 627
three methods may result in a relatively insigniﬁcant 628
mismatch between the two SNR conditions, causing the 629
higher recognition accuracy shown previously. Finally, the 630
detected speech segments are quite obviously separated in 631
the MSE-processed spectrogram, and this fact may be one 632
reason why MSE performs very well. 633
Integration of MSE with cepstral feature processing 634
techniques MSE, which is performed on the spectral 635
Figure 4 The SSBerouti–processed spectrogram of an utterance under two SNR levels: (a) clean, (b) 5dB.
UN
CO
RR
EC
TE
D
PR
OO
F
Hung et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:189 Page 11 of 20
http://asp.eurasipjournals.com/content/2012/1/189
Figure 7 TheWFTSNR–processed spectrogram of an utterance under two SNR levels: (a) clean, (b) 5dB.
log-STSA, with any cepstral-domain method673
performs worse than the component674
cepstral-domain method alone. For example, MMSE675
log-STSA plus HEQ achieves an averaged accuracy676
of 79.58%, less than 82.21% obtained by the single677
HEQ. These results again imply that the presented678
MSE outperforms the other three spectral-domain679
methods used here.680
The inﬂuence of the VAD error for MSE in speech681
recognition In this section, we ﬁrst investigate the eﬀect682
of the VAD error on recognition performance in MSE.683
For this purpose, we perform MSE under the “oracle684
condition”. That is, the VAD results for each clean utter-685
ance are directly applied to its various noise-corrupted686
counterparts to implement the magnitude spectrum687
enhancement. This process is referred to as “MSE(o)”688
here. Assuming that the VAD error of MSE for a clean689
utterance is small and negligible, the recognition accuracy690
diﬀerence between MSE(o) and MSE for noise-corrupted 691
utterances can be viewed as a consequence of the VAD 692
error due to noise. 693
The recognition accuracy rates for MSE(o) and MSE are 694
listed in Table 4. As expected, MSE(o) always performs T4695
better thanMSE because it contains no VAD errors. How- 696
ever, the diﬀerence in accuracy is not very signiﬁcant. In 697
the worst case (SNR = 0 dB), the performance degrada- 698
tion is 4.96% (1.64% for Set A, 8.77% for Set B, and 4.00% 699
for Set C), and on average, it is 2.90% (1.57% for Set A, 700
3.92% for Set B, and 3.49% for Set C). These results indi- 701
cate that the performance of MSE is somewhat inﬂuenced 702
by the the error of the embedded VAD process. 703
Next, we select diﬀerent VAD indicators for MSE to 704
see the corresponding eﬀect. According to the analysis in 705
Section ‘Eﬀect of additive noise on the logarithmic mag- 706
nitude spectrum in the frame sequences’, the high-pass 707
ﬁltered logarithmic magnitude spectrum (logMS) and the 708
logarithmic energy (log E) can emphasize the diﬀerence of 709
Figure 8 TheWFHRNR–processed spectrogram of an utterance under two SNR levels: (a) clean, (b) 5dB.
UN
CO
RR
EC
TE
D
PR
OO
F
Hung et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:189 Page 13 of 20
http://asp.eurasipjournals.com/content/2012/1/189
Table 3 Recognition accuracy (%) achieved by various approaches for Aurora-2 clean-condition training task averaged
across the SNRs between 0 and 20dB, where AVG (%) and RR (%) are the averaged accuracy rate and the relative error
rate reduction over the baseline
t3.1
t3.2
t3.3
t3.4Method Set A Set B Set C AVG RR
t3.5MFCC baseline 59.24 56.37 67.53 59.75 –
t3.6SSBerouti 69.76 70.47 69.39 69.97 25.40
t3.7WFPSNR 71.78 73.66 70.37 72.25 31.05
t3.8MMSE log-STSA 72.71 73.58 71.99 72.91 32.71
t3.9MSE 77.76 79.89 69.42 76.94 42.72
t3.10MVN 73.81 75.02 75.08 74.55 36.77
t3.11SSBerouti+MVN 65.71 70.39 66.94 67.83 20.07
t3.12WFPSNR+MVN 67.33 70.26 67.35 68.50 21.75
t3.13MMSE log-STSA+MVN 73.55 75.67 73.33 74.35 36.28
t3.14MSE+MVN 81.85 82.15 76.23 80.85 52.42
t3.15HEQ 81.42 83.34 81.51 82.21 55.80
t3.16SSBerouti+HEQ 73.04 76.99 73.52 74.71 37.18
t3.17WFPSNR+HEQ 74.95 77.30 74.92 75.88 40.08
t3.18MMSE log-STSA+HEQ 79.13 80.81 77.99 79.58 49.26
t3.19MSE+HEQ 84.19 83.20 78.20 83.80 59.75
t3.20MVA 78.15 79.17 79.12 78.75 47.21
t3.21SSBerouti+MVA 71.07 75.05 72.05 72.86 32.56
t3.22WFPSNR+MVA 69.02 71.70 69.01 70.09 25.69
t3.23MMSE log-STSA+MVA 74.39 76.79 74.50 75.37 38.81
t3.24MSE+MVA 83.58 85.02 80.58 82.37 56.20
Examining Equation (26), the exponent value α con-741
trols the degree of ampliﬁcation. Increasing the value of742
α enlarges the diﬀerence between the speech and non-743
speech frames in magnitude spectrum and may also lead744
to a greater mismatch among the speech frames for the745
same syllable or phoneme under diﬀerent SNR conditions.746
As a result, a larger α in MSE does not always bring about747
improved recognition accuracy, even if the VAD contains748
no errors. Here, we assign the exponent α to diﬀerent val-749
ues within the range [0, 1] and then proceed with MSE to750
investigate the corresponding recognition accuracy.751
Figure 12 shows the recognition results averaged overF12 752
ﬁve SNR conditions (0 ∼ 20 dB) and all ten noise types in753
the three Test Sets for diﬀerent values of α forMSE (the ﬁl- 754
ter coeﬃcient λ in Equation (12) is ﬁxed as 0.7). As shown 755
in Figure 12, we ﬁnd that 756
1. The case α = 0, where the magnitude spectrum is 757
kept unchanged in MSE, yields an averaged 758
recognition accuracy of 72.26%, signiﬁcantly better 759
than the MFCC baseline result (59.75%). This result 760
shows that simply setting the magnitude spectrum of 761
the detected non-speech frames to be nearly zero is 762
beneﬁcial to the recognition performance. 763
2. The recognition accuracy improves as the value α is 764
increased from 0 to 0.6, and the additional 765
Table 4 Recognition accuracy (%) achieved byMSE(o) andMSE for Aurora-2 clean-condition training task, whereMSE(o) is
MSE employing nearly error-free VAD results (MSE in the oracle condition)
t4.1
t4.2
t4.3
SNR4
Set A Set B Set C average
t4.4MSE(o) MSE MSE(o) MSE MSE(o) MSE MSE(o) MSE
t4.520 dB 97.70 97.31 98.19 97.55 96.91 96.19 97.74 97.18
t4.615 dB 95.76 94.47 96.78 95.54 93.60 90.98 95.73 94.20
t4.710 dB 89.36 87.25 92.76 89.54 82.86 78.67 89.42 86.45
t4.85 dB 72.43 69.97 79.23 73.47 60.47 54.53 72.76 68.28
t4.90 dB 41.44 39.80 52.10 43.33 30.73 26.73 43.56 38.60
t4.10average 79.33 77.76 83.81 79.89 72.91 69.42 79.84 76.94
UN
CO
RR
EC
TE
D
PR
OO
F
Hung et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:189 Page 15 of 20
http://asp.eurasipjournals.com/content/2012/1/189
Figure 13 Recognition accuracy (%) (averaged over ﬁve SNR values and all ten noise types in three Test Sets) versus diﬀerent
assignments of the ﬁlter coeﬃcient λ in MSE.
that emphasizing the higher modulation frequency com-798
ponents enhances the VAD of MSE. Next, setting λ to 0.8799
yields the optimal accuracy rate (77.04%), 0.10% better800
than the accuracy obtained by setting λ = 0.7 (76.94%).801
Finally, when the value of λ is within the range [0.1, 0.9],802
the diﬀerences among the accuracy rates obtained with803
diﬀerent values of λ are relatively small, and the decrease804
in maximum accuracy is just 1.82%. This result implies805
that nearly optimal performance can be obtained without806
meticulous adjustment of the parameter λ.807
The eﬀect of processing the short pauses within the utter-808
ance in MSE809
In the VAD procedure of MSE, each frame in an utter-810
ance is always classiﬁed as either speech or non-speech.811
Therefore, no frame will be classiﬁed as a “transient812
frame”, as is the case for some more delicate VAD pro-813
cesses. In fact, the transient frames that exist in the short814
region between two connected acoustic units (which are815
often called “short pauses”) are quite often classiﬁed as816
non-speech in MSE, and thus their magnitude spectrums817
are assigned as very small. For this reason, the VAD in 818
MSE is unlike some conventional end-point detectors, in 819
which only the onset and oﬀset frames of an utterance 820
are decided, while the inter-word or inter-syllable frames 821
that often possess lower energy are not processed. How- 822
ever, we ﬁnd that further processing of these detected 823
short pauses between the onset and oﬀset times for utter- 824
ances is quite helpful in speech recognition, especially 825
when the SNR is low. To demonstrate this phenomenon, 826
a simpler form of MSE is designed, in which we only pro- 827
cess the ﬁrst and the last detected non-speech segments 828
(the corresponding frames are assigned to have very small 829
magnitude spectra) and treat the remaining non-speech 830
segments as speech (the magnitude spectra of the corre- 831
sponding frames are weighted as in Equation (24)). This 832
method is called “MSE(s)” here for simplicity, and we com- 833
pare it with the original MSE with respect to speech 834
recognition performance. 835
Figure 14 shows the recognition accuracy rates for F14836
MSE(s) under diﬀerent SNR conditions for the three Test 837
Sets. In this ﬁgure, we see that almost no performance 838
Figure 14 Recognition accuracy (%) (averaged over all the ten noise types in three Test Sets) achieved by MSE andMSE(s) for diﬀerent
SNR conditions.
UN
CO
RR
EC
TE
D
PR
OO
F
Hung et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:189 Page 17 of 20
http://asp.eurasipjournals.com/content/2012/1/189
2. In contrast with the spectral-domain methods, the885
three cepstral-domain methods can give signiﬁcant886
performance improvement over the MFCC baseline.887
MVA behaves the best, followed by MVN and then888
HEQ. We ﬁnd that MVN outperforms HEQ slightly,889
which is not the case for the mode of clean-condition890
training as shown in Table 2. This phenomenon is891
probably because the mismatch between the training892
data and the testing data is relatively small in the893
mode of multi-condition training, and the894
over-normalization problem may occur in HEQ,895
which results in worse accuracy relative to MVN.896
3. None of the three spectral-domain methods,897
SSBerouti, WFPSNR and MSE, can help the subsequent898
cepstral-domain method to provide better899
recognition accuracy rates in comparison with the900
single cepstral-domain method. These results again901
imply these spectral-domain methods very probably902
diminish the helpful speech components in the noisy903
training data and are inappropriate for the task of904
multi-condition training.905
Experiments for the Num-100A database906
Besides the Aurora-2 database, here we adopt another907
database, called NUM-100A [40], to test the performance908
of the presented MSE. The NUM-100A database con-909
sists of 8,000 Mandarin digit strings produced by 50 male910
and 50 female speakers, recorded in a normal labora-911
tory environment at an 8 kHz sampling rate. These 8000912
digit strings include 1000 each of two-, three-, four-, ﬁve-,913
six-, and seven-digit strings, respectively, plus 2000 sin-914
gle digit utterances. Among the 8000 Mandarin digital915
strings, 7520 with diﬀerent lengths are selected for train-916
ing, while the other 480 are for testing. In particular, the917
480 clean testing strings are added with four types of noise918
(white, babble, pink and f16) taken from the NOISEX-919
92 database [44] at four diﬀerent SNRs (20, 15, 10, and920
5 dB) to produce the noise-corrupted testing data. The921
speech features used here are the same as those in the922
Aurora-2 task, which contain 13 MFCCs (c1–c12, c0) and923
their delta and delta-delta. With the feature vectors in924
the training set, the HMMs for each of the 10 digits and925
silence were trained with the HTK toolkit [45]. Each digit926
HMM contains ﬁve states and eight mixtures per state,927
and the silence HMM has three states and eight mixtures928
per state.929
For simplicity, we use the MSE with the same parame-930
ter settings in Aurora-2 task to process the training and931
testing signals and to create the correspondingMFCC fea-932
tures. In addition, since we just intend to investigate if933
MSE is also helpful to improve the noisy speech recogni-934
tion for another database besides Aurora-2, we do not per-935
form the other spectral-domain methods like SS and WF,936
and simply choose one cepstral-domain method, MVN, 937
for processing the MFCC features. 938
Figures 15 and 16a–d show the recognition accuracy F15
F16
939
rates for the four methods, MFCC baseline, MSE, MVN 940
and the pairing of MSE and MVN, under the clean and 941
four noise-corrupted situations with diﬀerent SNRs. From 942
these ﬁgures, we have the following ﬁndings: 943
1. Under the clean and matched condition, both MSE 944
and MVN degrades the recognition rate of the 945
MFCC slightly, and the combination of MSE and 946
MVN gets the worst results. These results imply that 947
the robustness methods can probably reduce the 948
discriminability of the original features when the 949
environment is noise-free. 950
2. The recognition accuracy of the original MFCC gets 951
apparently worse at mismatched noisy situations. 952
However, the presented MSE can enhance the MFCC 953
and bring about signiﬁcant accuracy improvement 954
irrespective of the type of noise. For example, at the 955
SNR of 10 dB, MSE provides the MFCC with the 956
accuracy rate improvements of 20.09%, 53.42%, 957
26.77%, and 41.74% for the noise being white, 958
babble, pink and f16, respectively. Therefore, we 959
show that MSE works well as a noise-robustness 960
approach for this Mandarin digit database in addition 961
to Aurora-2. 962
3. MVN promotes the recognition accuracy very well 963
relative to the MFCC baseline when the environment 964
is noisy, and it outperforms MSE in most cases. 965
However, the cascade of MSE and MVN performs 966
better than MVN alone (except for the babble 967
and f16 noises at the SNR of 20 dB), showing again 968
that MSE is well additive to the cepstral-domain 969
method, MVN. 970
Figure 15 Recognition accuracy (%) achieved by various
approaches for the NUM-100A database under the noise-free
environment.
UN
CO
RR
EC
TE
D
PR
OO
F
Hung et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:189 Page 19 of 20
http://asp.eurasipjournals.com/content/2012/1/189
respectively. Furthermore, assuming Dp[ k] and Dq[ k]1002
are statistically independent (since they correspond to1003
diﬀerent frames) and identically distributed, we have1004
Equation (3) as1005
γ [ k] = E
( |Sp[ k]+Dp[ k] |
Dq[ k]
)
= E
( 1
|Dq[ k] |
)
E(|Sp[ k]+Dp[ k] |)
=
(∫ ∞
0
1
x
( x
σ 2
exp
(−x2
2σ 2
))
dx
)
×
(
σ
√
π
2 1F1
(
−12 ; 1;−
|Sp[ k] |2
2σ 2
))
=
(√
π
2
1
σ
)(
σ
√
π
2 1F1(−
1
2 ; 1;−
|Sp[ k] |2
2σ 2 )
)
= π2 1F1
(
−12 ; 1;−
|Sp[ k] |2
2σ 2
)
, (29)
where 1F1(. , . , .) is the conﬂuent hypergeometric1006
function [38]:1007
1F1
(
−12 ; 1;−x
)
= exp
(
−x2
) (
(1 + x)I0
(x
2
)
+ xI1
(x
2
))
,
(30)
in which I1(.) is the modiﬁed Bessel function of the ﬁrst1008
kind with order one.1009
1010
It can be shown that1011
1F1
(
−12 ; 1;−x
)
> 0, forx > 0 (31)
and since ddx I0(x) = I1(x) and ddx (xI1(x)) = xI0(x), we1012
have1013
d
dx
(
1F1
(
−12 ; 1;−x
))
= exp
(
−x2
)(1
2 I0
(x
2
)
+ 32 I1
(x
2
))
>0, for x>0.
(32)
Therefore, 1F1(− 12 ; 1;−x) is a positive and monoton-1014
ically increasing function for x > 0, and we con-1015
clude that the parameter γ [ k]= π2 1F1(−
1
2 ; 1;−
|Sp[ k] |2
2σ 2 )1016
in Equation (29) decreases as the noise variance σ 21017
increases (with decreasing 1
σ 2
) , with two limiting cases1018
lim
σ 2→0
γ [ k]= ∞ and lim
σ 2→∞
γ [ k]= π2 .1019
Competing interests 1020
The authors declare that they have no competing interests. 1021
Received: 15 February 2012 Accepted: 13 August 2012 1022
Published: 30 August 2012 1023
References 1024
1. JN Holmes, NC Sedgwick, Noise compensation for speech recognition 1025
using probabilistic models. in 1986 International Conference on Acoustics, 1026
Speech and Signal Processing (ICASSP’86), vol. 11 (Tokyo, Japan, 1986), pp. 1027
741–744 1028
2. DH Klatt, A digital ﬁlter bank for spectral matching. in 1979 International 1029
Conference on Acoustics, Speech and Signal Processing (ICASSP’76), vol. 1 1030
(Philadelphia, USA, 1976), pp. 573–576 1031
3. A Nadas, D Nahamoo, M Picheny, Speech recognition using 1032
noise-adaptive prototypes. in 1988 International Conference on Acoustics, 1033
Speech and Signal Processing (ICASSP’88), vol. 1 (New York, USA, 1988), 1034
pp. 517–520 1035
4. AP Varga, RK Moore, Hidden Markov model decomposition of speech and 1036
noise. in 1990 International Conference on Acoustics, Speech and Signal 1037
Processing (ICASSP’90), vol. 2 (Albuquerque, USA, 1990), pp. 845–848 1038
5. A Acero, L Deng, T Kristjansson, J Zhang, HMM adaptation using vector 1039
Taylor series for noisy speech recognition. in 2000 International Conference 1040
on Spoken Language Processing (ICSLP’00), vol. 3 (Beijing, China, 2000), 1041
pp. 869–872 1042
6. CJ Leggester, PC Woodland, Maximum likelihood linear regression for 1043
speaker adaptation of continuous density HMMs. Comput. Speech Lang. 1044
9, 171–186 (1995) 1045
7. A Sankar, C-H Lee, A maximum-likelihood approach to stochastic 1046
matching for robust speech recognition. IEEE Trans. Speech Audio 1047
Process. 4, 190–202 (1996) 1048
8. C-H Lee, On stochastic feature and model compensation approaches to 1049
robust speech recognition. Speech Commun. 25, 29–47 (1998) 1050
9. G-X Ning, G Wei, K-K Chu, Model compensation approach based on 1051
nonuniform spectral compression features for noisy speech recognition. 1052
EURASIP J. Adv. Signal Process. 2007 (2007) 1053
10. PJ Moreno, B Raj, RM Stern, Data-driven environmental compensation for 1054
speech recognition: a uniﬁed approach. Speech Commun. 24, 267–285 1055
(1998) 1056
11. MJF Gales, SJ Young, Cepstral parameter compensation for HMM 1057
recognition in noise. Speech Commun. 12, 231–239 (1993) 1058
12. MJF Gales, SJ Young, Robust speech recognition in additive and 1059
convolutional noise using parallel model combination. Comput. Speech 1060
Lang. 9, 289–307 (1995) 1061
13. MJF Gales, SJ Young, A fast and ﬂexible implementation of parallel model 1062
combination. in 1995 International Conference on Acoustics, Speech and 1063
Signal Processing (ICASSP’95), vol. 1 (Detroit, USA, 1995), pp. 133–136 1064
14. SF Boll, Suppression of acoustic noise in speech using spectral 1065
subtraction. IEEE Trans. Acoust. Speech Signal Process. 27, 1066
113–120 (1979) 1067
15. M Berouti, R Schwartz, J Makhoul, Enhancement of speech corrupted by 1068
acoustic noise. in 1979 International Conference on Acoustics, Speech and 1069
Signal Processing (ICASSP’79), vol. 4 (Washington, USA, 1979), 1070
pp. 208–211 1071
16. S Kamath, P Loizou, A multi-band spectral subtraction method for 1072
enhancing speech corrupted by colored noise. in 2002 International 1073
Conference on Acoustics, Speech and Signal Processing (ICASSP’02), vol. 4 1074
(Orlando, USA, 2002), pp. IV–4164 1075
17. B BabaAli, H Sameti, M Safayani, Likelihood maximizing based multi-band 1076
spectral subtraction for robust speech recognition. EURASIP J. Adv. Signal 1077
Process. 2009 (2009) 1078
18. P Scalart, JV Filho, Speech enhancement based on a priori signal to noise 1079
estimation. in 1996 International Conference on Acoustics, Speech and 1080
Signal Processing (ICASSP’96), vol. 2 (Atlanta, USA, 1996), pp. 629–632 1081
19. C Plapous, C Marro, P Scalart, Improved signal-to-noise ratio estimation 1082
for speech enhancement. IEEE Trans. Audio Speech Lang. Process. 14, 1083
2098–2108 (2006) 1084
20. Y Ephraim, D Malah, Speech enhancement using a minimum 1085
mean-square error short-time spectral amplitude estimator. IEEE Trans. 1086
Acoust. Speech Signal Process. 32, 1109–1121 (1984) 1087
  
 
Abstract—This paper proposes three novel noise robustness 
techniques for speech recognition based on discrete wavelet 
transform (DWT), which are wavelet filter cepstral coefficients 
(WFCCs), sub-band power normalization (SBPN), and lowpass 
filtering plus zero interpolation (LFZI). According to our 
experiments, the proposed WFCC is found to provide a more 
robust c0 (the zeroth ceptral coefficient) for speech recognition, 
and with the proper integration of WFCCs and the 
conventional MFCCs, the resulting compound features can 
enhance the recognition accuracy. Second, the SBPN procedure 
is found to reduce the power mismatch within each modulation 
spectral sub-band, and thus to improve the recognition 
accuracy significantly. Finally, the third technique, LFZI, can 
reduce the storage space for speech features, while it is still 
helpful in speech recognition under noisy conditions. 
 
 
I. INTRODUCTION 
The conventional mel-frequency cepstral coefficients 
(MFCC) [1] have been one of the most widely used speech 
features for speech recognition over many years. In deriving 
the MFCC, the short-time Fourier transform (STFT) is 
applied. However, due to its time-frequency properties, 
STFT is actually not very suitable for analyzing a 
non-stationary signal like speech [2], which implies the 
resulting MFCC is not always optimal for representing the 
speech signal and possibly provides less recognition 
accuracy. One way to partially solve the problem is to apply 
the multi-resolution property in time and frequency domain, 
and the multi-resolution goal is achieved by replacing STFT 
with wavelet transform [3]. Unlike the Fourier transform, the 
finite-length basis functions [4] help wavelet transform 
analyze the non-stationary signal with better transformable 
ability. Although wavelet transform has a better performance 
in analyzing the nonperiodic signal, STFT performs better for 
presenting the periodic signal [5]. Thus in this paper we 
create the compound feature to integrate them both and find it 
is helpful for speech recognition.  
The environmental mismatch caused by additive noise 
and/or channel distortions often degrades the performance of 
a speech recognition system. To overcome this problem, 
researchers have proposed many speech enhancement or 
 
Manuscript received August 10, 2012; revised September 24,2012. 
Jeih-Weih Hung, Hao
Department of electrical engineering, national Chi Nan University, Taiwan 
(e-mail: jwhung@ncnu.edu.tw, s99323904@ncnu.edu.tw).  
 
robustness techniques to enhance the speech or alleviate the 
effect of noise. We find that the wavelet analysis can be also 
applied to constructing the noise-robust speech features. In 
the past research of our lab, the wavelet transform was used 
in the temporal speech feature stream and good recognition 
performance can be achieved [6]. Therefore, in this paper we 
follow this direction to provide more noise-robust features 
with wavelet transform, and come up with two novel 
robustness methods, sub-band power normalization (SBPN) 
and lowpass filtering plus zero interpolation (LFZI).  
The remainder of this paper is organized as follows: 
Section II briefly introduces the discrete wavelet transform 
(DWT). Then we present three DWT-based noise robustness 
methods in Section III. Section V contains the experimental 
results together with the discussions. Finally, a brief 
concluding remark is given in Section VI. 
 
II. DISCRETE WAVELET TRANSFORM 
Here, we make a brief introduction of discrete wavelet 
transform. Consider a signal f[n] that is decomposed by a 
discrete wavelet transform with the scaling ( [ ]j k n  ) and 
wavelet ( [ ]j k n  ) basis functions [7]:  
 
       , , , ,[ ] [ ] [ ]j k j k j k j k
j k j k
f n a n d n     (1) 
 
Based on eq. (1), f[n] is decomposed into aj,k and dj,k, 
which are considered as the approximation (low-pass) part 
and the detail (high-pass) part, respectively.  
In practice, the implementation of the discrete wavelet 
transform is sometimes accomplished with sequential 
filtering and down-sampling, as presented in Fig. 1. In this 
figure, h[-n] and g[-n] are the low-pass and high-pass filters 
respectively, followed by a down-sampling process. 
According to Fig. 1, the signal aj+l,k is first decomposed into 
the approximation and detail parts with filtering and 
down-sampling procedures, and then the approximation 
sequence is decomposed again with the same process.  
  In this paper, we viewed the discrete wavelet transform as 
a filtering process and develop several noise-robustness 
methods. 
 
III. SEVERAL NOVEL PROPOSED TECHNIQUES 
Based on DWT, we present three novel methods to 
improve the original MFCC in its recognition accuracy under 
noisy environments.  
Several New DWT-Based Methods for Noise-Robust 
Speech Recognition 
Jeih-Weih Hung, Hao-Teng Fan, and Syu-Siang Wang 
International Journal of Innovation, Management and Technology, Vol. 3, No. 5, October 2012
547
Index Terms—Discrete wavelet tranSform, wavelet filter 
cepstral coefficients, sub-band power normalization, lowpass 
filtering and zero interpolation, speech recognition.
DOI: 10.7763/IJIMT.2012.V3.295 
-Teng Fan and Syu-Siang Wang are with the 
  
by { [ ]
m
lc n }, is roughly within the following modulation 
frequency range:  
 
             
1
2 1
1 1
1
[0 ( )] 1
22
2 2
[ ( ) ( )] 2
2 22 2
s
L
l l
s s
L L
F
l
F F
l L

 
 

  

    

 (4) 
 
where 
sF  (Hz) is the frame sampling rate. Therefore, through 
DWT, we split the entire frequency band [0, 
2
sF  Hz] into L 
sub-bands with unequal bandwidth.  
Next, each sub-band sequence is updated for power 
normalization according to the following equation:  
 
                [ ] [ ]
m
target_lmm
l l m
single_l
P
n c nc
P
    (5) 
 
where the [ ]
m
l nc  is the new speech feature, and 
m
target lP   and 
m
single lP   are the target power (obtained from the clean 
sub-band features in the training set) and the power of the 
currently processed [ ]
m
lc n , respectively.  
Finally, all the updated sub-band sequences are used 
together to reconstruct the new full-band (temporal) 
sequence with an L-level inverse discrete wavelet transform 
(IDWT).  
C. Lowpass Filtering and Zero Interpolation (LFZI) 
According to the original DWT decomposition process, 
the low-pass and high-pass filters are first applied to the input 
data, and a down-sampling procedure is applied to the two 
filter outputs. As stated in [9], the main speech component (1 
Hz 16 Hz) is just within the front half modulation spectrum. 
Therefore, if the original speech frame rate is 100 Hz, then 
after processing the feature frame sequence with a one-level 
DWT, the low-pass filter output (before down-sampling) is 
roughly within [0, 25 Hz], preserves the speech information, 
while the high-pass filter output seems less helpful in speech 
recognition. In addition, the down-sampling process doubles 
the bandwidth of filtered sequence. 
 
 
 
Fig. 5. The SBPN process with four sub-bands 
 
Accordingly, the approximation feature sequence via 
DWT (the lowpass-filtered and down-sampled sequence) 
may perform better than the original sequence since the 
irrelative components (high frequency parts) are removed. 
However, due to down-sampling, the approximation 
sequence is a half of the original feature sequence in length. 
The feature length reduction is found to be defective since 
there will be insufficient data for training accurate acoustic 
models.  
 
 
 
Fig. 6. The detailed procedure of the LFZI technique 
 
In order to solve or alleviate the above problem of data 
insufficiency, we apply a zero interpolation (adding one zero 
between each sample) in the approximation sequence to 
make the resulting new sequence roughly equal the original 
sequence in length. We name the above process as “lowpass 
filtering and zero interpolation”, abbreviated as “LFZI”. The 
procedure of the LFZI method is depicted in Fig. 6.  
 
IV. EXPERIMENTAL SETUP 
We use the AURORA-2 database [10], which is widely 
used for evaluating robustness algorithms under noisy 
conditions. For the recognition environment, three different 
subsets are defined: Test Sets A, B and C. Speech signals in 
Test Sets A and B are affected by additive noise (in Set A, the 
noise types are subway, babble, car and exhibition; and in Set 
B, they are restaurant, street, airport and train station), and 
speech signals in Test Set C is affected by additive noise and 
channel effects (subway or street noise together with an 
MIRS channel mismatch). Each noise instance is added to the 
clean speech at six SNR levels (ranging from 20 dB to -5 dB). 
Each utterance in the clean training set and three 
noise-corrupted testing sets is first converted into a sequence 
of 13-dimensional MFCCs (c0 c12) and the same 
dimensional WFCCs. The frame length and frame shift are 
set to 32 ms and 10 ms, respectively.  
The Hidden Markov Model Tool kit (HTK) [11] is used for 
the training and recognition process. The resulting acoustic 
models include 11 digit models (zero, one, two, three, four, 
five, six, seven, eight, nine and oh) and a silence model. Each 
digit model contains 16 states and 20 Gaussian mixtures per 
state. 
 
V. EXPERIMENT RESULTS AND ANALYSES 
In this subsection, we will separately present the 
recognition performance achieved by our three novel 
methods and give the corresponding discussions. 
Since WFCC alone do not perform very well (as will be 
shown as Feature Set iiiv in Table 2), we partition the original 
13 cepstral features into two sets: { c0 }  and { c1, c2, ..., 
c12 } , and then we have seven compound feature sets as 
listed in Table 1, and they are depicted in Fig. 7 for a clearer 
International Journal of Innovation, Management and Technology, Vol. 3, No. 5, October 2012
549
  
1) Our proposed LFZI provides 6.17% accuracy 
improvement over the baseline. In addition, LFZI 
performs the best among the three methods listed in this 
table.  
2) The method that directly uses the approximate part of 
DWT-processed MFCCs has the worst performance, 
which is possibly due to the insufficient training data, 
causing the inaccurate acoustic models.  
3) Although the low-pass filter-processed sequence has the 
same length as the original MFCC sequence and gets 
better accuracy rates than the MFCC baseline, it is worse 
than our proposed LFZI method. The results reveal that 
in addition to low-pass filtering, the down-sampling and 
zero-insertion processes in LFZI indeed help improve 
the noise robustness of MFCC features.  
 
VI. CONCLUSION 
In this paper, we propose three noise-robustness 
techniques. First, the new WFCC construction process gives 
a better c0 feature while MFCCs have superior c1 c12 
features, which makes the compound features behave better 
than WFCC alone and MFCC alone. Second, the sub-band 
power normalization (SBPN) attempts to normalize the 
power of each sub-band in the temporal domain. Finally, 
LFZI reserves the more important modulation spectral 
portions in the feature sequence and reduces the storage 
space for speech features simultaneously. Despite the 
simplicity in implementation, the proposed SBPN and LFZI 
significantly improve the recognition accuracy under noisy 
situations.  
REFERENCES 
[1] X. Huang, A. Acero， and H. W. Hon, Spoken language processing: A 
guide to theory, algorithm, and system development, Prentice Hall PTR, 
2001.  
[2] R. Modic, B. Lindberg, and B. Petek, “Comparative wavelet and 
MFCC speech recognition experiments on the slovenian and English 
speechDat2,” ISCA Non-Linear Speech Processing (NOLISP), vol.16, 
2003.  
[3] R. Sarikaya and J. H. L. Hansen, “High resolution speech feature 
parametrization for monophone-based stressed speech recognition,” 
IEEE Signal Processing Letters, vol. 7, no. 7, pp. 182-185, 2000.  
[4] F. B. Tuteur, “Wavelet transformations in signal detection,” IEEE 
International Conference on Acoustics, Speech, and Signal Processing 
(ICASSP), vol. 3, pp. 1435-1438, 1988.  
[5] O. Farooq and S. Datta, “Mel filter-like admissible wavelet packet 
structure for speech recognition,” IEEE Signal Processing Letters, vol. 
8, no. 7, pp. 196-198, 2001.  
[6] H. Fan and J. Hung, “Sub-band feature statistics normaliztion 
techniques based on discrete wavelet transform for robust speech 
recognition,” IEEE Signal Processing Letters, vol. 16, no. 9, pp. 
806-809, 2009.  
[7] M. Vetterli and J. Kovačević, Wavelets and Subband Coding, 
Prentice-Hall PTR, 1995.  
[8] W. Chong and J. Kim, “Speech and image compressions by DCT, 
wavelet, and wavelet packet,” International Conference on 
Information, Communications and Signal Processing (ICICS), vol. 3, 
pp. 1353-1357, 1997.  
[9] N. Kanedera, T. Arai, H. Hermansky, and M. Pavel, “On the 
importance of various modulation frequencies for speech recognition,” 
European Conference on Speech Communication and Technology 
(EUROSPEECH), pp. 1079-1082, 1997.  
[10] D. Pearce and H. G. Hirsch, “The AURORA experimental framework 
for the performance evaluation of speech recognition systems under 
noisy conditions," ICSA ITRW ASR2000, 1999.  
[11] HTK. [Online]. Available: http://htk.eng.cam.ac.uk/ 
 
 
International Journal of Innovation, Management and Technology, Vol. 3, No. 5, October 2012
551
取的論文、並聽取其他各領域與會者的論文發表，並與其交換心得。 
此次參加FSKD會議，如同去年所參加之FSKD會議一樣，跟以往參加單一領域（語
音處理）的會議最大的不同，是能接觸涉獵其他領域、使用不同技術（如模糊理論
Fuzzy theory）用以處理各種生物資訊的理論與技術，由於語音本身也是生物資訊之
一，吾人發現，處理其他生物資訊（例如分類、鑑別、辨識等）的技術許多跟處理
語音訊號相重疊，但另外個人覺得最大的收穫，是發現Fuzzy theory目前也逐步運用
於資訊分類的主題上，而Fuzzy theory正是我比較不熟悉的領域，因此它對我未來的
研究，極具參考價值，極可能對於現行語音強健性技術提供嶄新的發展方向，至少
可以與現行技術相互補償、達到相輔相成的結果。這些發現拓展了我個人未來的研
究廣度，回國之後，我與研究生將研讀此次會議的諸多論文，藉此吸取新知並觸發
新的想法，以發展語音相關的新方法，或延伸語音相關的技術至其他不同生物資訊
的處理上，使研究領域更廣泛。 
本研討會提供了不同領域之學者彼此溝通交流新知的機會，本人著實感謝國科會提
供補助，使我們能出國參加此會議，除了吸收新知、擴展視野以外，發表個人論文
也意味著所從事的研究受到國際的肯定，因此，期待國科會或政府相關機構未來能
繼續補助支持我們參加此類學術會議。 
 
附註：攜回資料名稱及內容 
1. The Hardcopy of FSKD 2012 Program Guide：內容為本次會議中所有論文發表 
時間及論文標題，其為一紙本。 
2. The CDROM of the Proceedings for FSKD 2012：內容為本次會議所發表所有論 
文之電子檔，其為光碟片一張。 
                                                                                                                                          1422
 
 
Figure 1. The procedures of the proposed sub-band gain normalization (SB-GN) approach, where the 4-level discrete wavelet 
transform (DWT) is used for the octave-band filter-bank analysis and synthesis. 
 
 (QMF) bank, which spreads over the entire frequency 
band and does not introduce aliasing distortion. Thus, 
DWT does not further contaminate the original feature 
stream or cause any information loss. Experiments 
conducted on the Aurora-2 digit database show that, the 
presented sub-band GN operation enhances the speech 
features by reducing the distortion and thus significantly 
achieves better recognition accuracy. Furthermore, the 
sub-band GN outperforms the conventional full-band GN 
in recognition performance.  
The remainder of the paper is organized as follows: 
Section II introduces the proposed sub-band GN 
compensation methods. The experimental setup is 
described in Section III, and the experiment results are 
given and discussed in Section IV. Finally, Section V 
contains a brief concluding remark and future works. 
 
II. THE SUB-BAND GAIN NORMAIZATION 
METHODS  
Let { [ ],  0 1}x n n N£ £ -  be a specific channel (say, 
the 2nd mel-frequency cepstral coefficient, c2) of a 
speech feature vector stream associated with an 
utterance, where N  is the number of frames of the 
utterance. We update the feature stream { [ ]}x n  to obtain 
a new stream { [ ]}x n , which is expected to be more 
noise-robust than { [ ]}x n . In this study, we focus on 
refining the method of gain normalization [3] in the 
process of feature stream update. In the (full-band) gain 
normalization (termed FB-GN for simplicity) process, 
the relationship between { [ ]}x n  and { [ ]}x n  is 
[ ]
[ ] x
x
x n
x n
d
m-= ,                                                           (1) 
where  
x
m  and 
x
d  are, respectively, the mean and 
dynamic range of { [ ]}x n , and are obtained by  
1
0
1
[ ]
N
x
n
x n
N
m
-
=
= å ,                                                            (2) 
and  
max{ [ ]} min{ [ ]}
x n n
d x n x n= - ,                                        (3) 
respectively. Thus, the new stream { [ ]}x n  has a zero 
mean and unity dynamic range. 
In the presented sub-band gain normalization process 
(which is named SB-GN hereafter) as shown in Figure 1, 
the new stream { [ ]}x n  is obtained from { [ ]}x n  through 
three steps: 
 
Step I: Split the original stream { [ ]}x n  via discrete 
wavelet transform (DWT) 
 
We use an L-level discrete wavelet transform (DWT) 
on the stream { [ ]}x n  to produce (L+1) sub-band streams 
{ [ ]}x n , 0 L£ £ . Thus, given that the frame rate of 
{ [ ]}x n  is 
s
F  in Hz and thus { [ ]}x n  is within the 
modulation spectral band [0, 2]
s
F , the band range of the 
th  sub-band stream is nearly 
( )
( ) ( )1
1
0, 2                 if 0
2
2 2
2 , 2     if 1,2, ,
2 2
sL
s sL L
F
F F L
-
ìé ùïïê ú =ïïê úïë ûíé ùïïê ú =ïê úïïê úë ûïî
 

 
.                    (4) 
Therefore, more sub-band streams with a narrower 
bandwidth are at low frequencies while fewer sub-band 
streams with a broader bandwidth are at high 
Gain normalization 
                                                                                                                                          1424
 
Table 1. Recognition accuracy (%) achieved by various methods for the Aurora-2 clean condition training task averaged across 
the SNRs between 0 and 20dB. RR1 (%) and RR2 (%) are the relative error rate reductions over the baseline and the full-band 
method, respectively 
 Set A Set B Set C － 
noise type subway babble car exhibition restaurant street airport station MIRS subway MIRS street Avg. RR1 RR2 
baseline 77.14 61.67 70.62 78.09 66.40 75.05 64.72 66.85 76.90 78.41 71.59 － － 
FB-CGN 87.88 87.74 87.20 87.72 89.53 87.65 89.45 87.54 87.99 87.50 88.02 57.84 － 
SB-CGN(1) 88.30 87.98 89.15 87.58 89.73 88.50 90.09 89.46 88.50 88.45 88.79 60.54 6.42 
SB-CGN(1,2) 89.39 89.42 91.17 89.41 90.56 90.15 91.41 90.83 89.67 89.94 90.20 65.51 18.20
SB-CGN(1,2,3) 89.79 89.23 90.70 88.93 90.71 89.85 91.32 90.60 89.73 89.65 90.05 64.98 16.95
SB-CGN(1,2,3,4) 89.46 88.86 89.96 88.42 90.30 89.43 90.62 89.98 89.41 89.39 89.58 63.34 13.02
  
cutoff frequency is varied, with the main reason that the 
lower modulation frequencies are usually more 
important than the higher ones in speech recognition. 
From Table 1, we have the following findings. 
1. Compared to baseline processing, FB-GN and all 
modes of SB-GN achieve very significant 
improvement (more than 55% in relative error 
reduction) in averaged recognition accuracy. 
2. All the four modes of SB-GN outperform FB-GN. In 
particular, the best possible SB-GN provides 16.95% 
in relative error reduction compared with FB-GN. 
These results show that the presented sub-band 
processing approach behaves better than the 
conventional full-band one, and they support our 
statements in the previous sections that performing 
the normalization process on each sub-band 
separately rather than on the entire band is indeed 
beneficial. 
3. As for the different modes of SB-GN, we find that, 
even if only the lowest sub-band ([0, 6.25 Hz]) 
features are processed while the other frequency 
components are kept unchanged (i.e., SB-GN(1)), we 
still can achieve significant accuracy improvement 
with respect to baseline processing and FB-GN. As 
we increase the number of sub-bands for 
normalization, the accuracy rate is also raised 
accordingly in most cases. In particular, processing 
the lowest two sub-bands ([0, 6.25Hz] and [6.25Hz, 
12.5Hz])  as in the method "SB-CGN(1,2)" already 
achieves the nearly optimal performance. 
 
IV. CONCLUDING REMARKS AND FUTURE WORKS 
In this paper, we propose to perform gain 
normalization on the sub-band feature streams, and 
show that the resulting SB-GN outperforms the original 
GN that is performed in a full-band manner. In 
particular, the discrete wavelet transform (DWT) is used 
for splitting the full-band stream into sub-band streams. 
In the future, we will apply other types of wavelet 
functions in the DWT and IDWT processes of our 
approach to investigate if a different analysis/synthesis 
operation will influence the recognition accuracy. 
Besides, we will use the wavelet packet transform (WPT) 
to perform a more flexible sub-band division and 
examine the corresponding effect in SB-GN. 
V. ACKNOWLEDGEMENT 
This work was sponsored in part by "Aim for the 
Top University Plan" of National Taiwan Normal 
University and Ministry of Education, Taiwan, and the 
National Science Council, Taiwan, under Grants NSC 
100-2221-E-260-032 
REFERENCES 
[1]  B.S. Atal, "Effectiveness of linear prediction characteristics of 
the speech wave for automatic speaker identification and 
verification", Journal of the Acoustic Society of America, 1974  
[2]  S. Tibrewala and H. Hermansky, "Multiband and adaptation 
approaches to robust speech recognition", Eurospeech 1997 
[3]  S. Yoshizawa et al., "Cepstral gain normalization for noise 
robust speech recognition," ICASSP 2004 
[4]  C.-W. Hsu and L.-S. Lee, "Higher order cepstral moment 
normalization  (HOCMN) for robust speech recognition", 
ICASSP 2004 
[5]  F. Hilger and H. Ney, "Quantile based histogram equalization 
for noise robust large vocabulary speech recognition," IEEE 
Trans. on Audio, Speech and Language Processing, 2006 
[6]  Jeih-weih Hung; Hao-Teng Fan, "Subband feature statistics 
normalization techniques based on a discrete wavelet transform 
for robust speech recognition", IEEE Signal Processing Letters, 
Vol. 16,  2009 
[7]  N. Kanedera, T. Arai, H. Hermansky, and M. Pavel, "On the 
importance of various modulation frequencies for speech 
recognition", Eurospeech 1997 
[8] Guan-min He and Jeih-weih Hung, "Integrating Codebook and 
Utterance Information in Cepstral Statistics Normalization 
Techniques for Robust Speech Recognition", Interspeech 2009 
[9]  Sanjit K. Mitra, "Digital Signal Processing, a computer-based 
approach", 3rd version, McGraw-Hill 
[10]  H. G. Hirsch and D. Pearce, "The AURORA experimental 
framework for the performance evaluations of speech recognition 
systems under noisy conditions," ISCA ITRW ASR 2000 
[11]  J. Droppo, L. Deng, and A. Acero, "Evaluation of SPLICE on 
the AURORA 2 and 3 tasks", ICSLP 2002 
[12] The hidden Markov model toolkit. Available from: 
http://htk.eng.cam.ac.uk.  
请注意，如果通过银行汇款，请您在银行转账单据的备注栏填写 FSKD'12 和所录用的论文编号P****（未填则无法确
认该论文已交费）。汇出注册费后，请把转账单据扫描后上传到最终稿系统。如果没有收到扫描的单据，我们将视为
没有收到注册费。 
6. We would greatly appreciate it if you could complete the Registration Form (download at conference registration website) 
and upload it to us latest by 25 April 2012, together with a scanned copy of the bank transfer (showing your Paper_IDs) if you 
pay by bank transfer or a copy of your credit-card payment confirmation. This is very important to help us sort out which 
payment is for which paper. 
If any of the above requirements are not met by the deadline 25 April 2012, your paper cannot be included in the 
conference proceedings or the conference program. Your kind cooperation will be greatly appreciated.  
This notification serves as our Acceptance Letter. If you require a hardcopy of our Acceptance Letter or have any queries, 
please send an email to the Conference Secretariat icnc-fskd@cqupt.edu.cn. 
Thank you for choosing ICNC-FSKD conferences to present your research results and we look forward to seeing you in May 
2012, Chongqing, China. We also hope that you will submit your excellent work to future ICNC-FSKD conferences (further 
details will be announced later) 
Yours sincerely,  
Program Chairs, FSKD'12  
http://icnc-fskd.cqupt.edu.cn/ 
Some points to note when you format your paper:  
1. Do not use double-line-spacing (with spacing between lines wide enough to fit another line). Use single-line-spacing (there 
should be no spaces between lines).  
2. Make sure that the margins at the 4 sides are not too wide or too narrow. 
3. For color figures, please make sure that the figures are legible when they are printed in black-and-white (printed proceedings 
will be in black-and-white).  
4. In your reference list, do not add things like [J] and [C]  
5. Avoid using undefined acronyms, i.e., if you wish to use an acronym, you must first define it in your paper, e.g., fuzzy neural 
network (FNN).  
6. Please do a thorough spelling check, e.g., by using the spelling tool in Word.  
  
Comments from Reviewer 1 :  
--------------------------------------------------------------------------------  
The paper is very interesting and technically sound. Unfortuntaely it is out of the scope of the conference which is focusing on 
computational intelligence. 
--------------------------------------------------------------------------------  
  
Comments from Reviewer 2 :  
--------------------------------------------------------------------------------  
I think that the authors have their own innovation and what they write is substantial,so this is a good article! 
--------------------------------------------------------------------------------  
  
学校税务登记号 50010845040189-9
组织机构代码 500000523
Page 2 of 3
2012/6/14cid:20120614135927.CD63.A5351E19@ncnu.edu.tw
國科會補助計畫衍生研發成果推廣資料表
日期:2013/01/31
國科會補助計畫
計畫名稱: 語音特徵各域之強化技術於強健性語音辨識之研究
計畫主持人: 洪志偉
計畫編號: 100-2221-E-260-032- 學門領域: 訊號處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
1. 於暨南大學主辦 2011 年系統科學與工程研討會(National Symposium on 
System Science and Engineering)，擔任註冊主席與財務主席（六月 17 日，
18 日） 
2. 於暨南大學主辦 2010 年自然語言與語音處理研討會（ Conference on 
Computational Linguistics and Speech Processing），擔任議程主席（九月
一日，二日） 
3. 指導學生王緒翔獲得暨南大學電機系 99年碩士畢業成果展優等獎 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
