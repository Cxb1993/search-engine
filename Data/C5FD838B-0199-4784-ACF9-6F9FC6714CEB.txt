i 
 
行政院國家科學委員會補助專題研究計畫 ▓ 成 果 報 告   
□ 期中進度報告 
 
Criti-core：超越多核心之高可靠度晶片系統平台技術開發－
子計畫一：以超低延遲環形晶片網路實現多核
心無一致性快取記憶體系統 
 
計畫類別：□ 個別型計畫  ▓ 整合型計畫 
計畫編號：NSC 98－2220－E－194－012－ 
執行期間：2009 年 08 月 01 日至 2011 年 07 月 31 日 
執行機構及系所：交通大學/中正大學 資訊工程所 
計畫主持人：陳添福 
共同主持人： 
計畫參與人員： 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  ▓完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
□出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
處 理 方 式 ： 除 列 管 計 畫 及 下 列 情 形 者 外 ， 得 立 即 公 開 查 詢          
□涉及專利或其他智慧財產權，□一年▓二年後可公開
查詢 
中   華   民   國      100 年     10 月    31 日 
iii 
 
十一 Conclusion - Snoop2-NUCA ··············································· 52 
十二 Conclusion - NUDA Debugging ··········································· 53 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
i 
 
List of Tables 
Table 7- 1: Target System Simulation Parameters. ............................................... 41 
Table 7- 2: Benchmark Workloads and Inputs. ..................................................... 42 
Table 9- 1: Target System Parameters. .................................................................. 47 
Table 9- 2: Benchmarks’ Statistics, Resource Requirements, Debugging Event 
Rate, And Performance ......................................................................................... 47 
Table 10- 1: Comparisons of Race Detection Methods (64-Core) ........................ 50 
Table 10- 2: Race Detection by Renaming Locks. ................................................ 51 
 
 
 
 
 
 
 
 
2 
 
each NUDA node is used to keep track of the access footprints. The union of all the 
caches can serve as a race detection probe without disturbing execution ordering. 
Using the proposed approach, we show that parallel race bugs can be precisely 
captured, and that most false-positive alerts can be efficiently eliminated at an average 
slowdown cost of only 1.4%~3.6%. The net hardware cost is relatively low, so that 
the NUDA can readily scale to increasingly complex many-core systems. 
 
中文摘要 
雖然增加單一晶片中的核心數是未來的趨勢，但其成長的速度會遇到瓶頸，
其原因在於晶片外溝通的頻寬成長的速度遠小於晶片內核心成長。另外，傳統的
快取(cache)架構設計會產生不必要的能量消耗和限制效能表現。因此，許多關於
記憶體系統以及晶片內溝通的研究想要解決以上問題。此篇論文中，我們提出階
層性的快取分享概念，以達到覆載平衡(load-balance)和有效的溝通機制。藉由超
低延遲環型晶片網路(Single-Cycle Ring)提供延遲及能量的減少，我們提出一個稱
為 Snoop2的無一致性快取記憶體系統並用在第一層快取上，以達到第一層快取
分享的目的。Snoop2 無一致性快取記憶體系統的目的在於提供有效的最佳化方
法，讓核心使用最大的快取使用量，除此之外，提供一個有效率的快取資料搜尋
策略。另外，我們也針對 Snoop2無一致性快取記憶體系統的幾個實現上可能的
問題提出探討和解決方法。最後，實驗結果呈現出 Snoop2無一致性快取記憶體
系統在減少晶片外的頻寬和快取失誤的比率能有不錯的表現。 
傳統的 debug methodology 無法用在多核心架構上的平行程式，因為
synchronization問題或是 race condition很難被偵測。而我們提出的 Non-Uniform 
Debugging Architecture (NUDA)多核心處理器之除錯設計是基於 ring 
interconnection，在 debug many-core processing 時更 feasible 和 scalable。因為
debugging command 將不是像以往以訊號線來傳遞，這些 command 會被拆成封
包傳到各個 core上，而每個 core上將有對應的 decoder以及 encoder來解封包，
部分 ICE以及 TAP controller功能將會拆解到各個 core上面。使用我們的方法，
parallel race bug可被偵測到，且有效降低 false-positive alert。 
 
 
 
 
4 
 
L1 data and instruction caches, and the L2 caches can either be shared or private. 
In this work, we propose a “Single-Copy Snoop2 Non-Uniform Cache 
Architecture” (Single-Copy Snoop2-NUCA) on level-1 cache to solve the above 
challenges in the context of CMPs cache architecture. Snoop2-NUCA utilizes on-chip 
capacity with single-copy data to reduce the off-chip memory traffic for large shared 
working set solving off-chip bandwidth bottleneck. In addition to, for read-write 
shared data, Snoop2-NUCA will save the energy consumption without using 
coherence broadcast lookups. At those points, single-copy cache utilizes cache 
capacity and reduces congestion of off-chip memory demand. 
二.2 Main Contribution (Snoop2-NUCA) 
In this paper, we explore Single-copy Snoop2-NUCA on L1 cache. In summary, 
the contributions of this work are as following: 
1. A hierarchical sharing way, as Snoop2-NUCA for local sharing and 
L2-NUCA for global sharing, to alleviate the increasing bandwidth pressure. 
2. Maximizing utilization of cache system by no redundant cache lines, it 
means cache system is less unused cached data, and less conflict penalty (i.e. more 
associative ways compared to conventional L1 cache). 
3. Reduce the snoopy broadcast overhead, especially in energy saving. It 
means the power consumption of snoopy broadcast costs in snoopy bus switching and 
L1 tag bank polling can be eliminated. 
二.3 Introduction (NUDA Debugging) 
Multitasking concurrency in the context of parallel programs is essential to the 
future development of MPSOC. However, nondeterministic parallel programs are 
hard to debug with traditional methodologies because subsequent executions with 
identical inputs are not guaranteed to result in the same behavior. Using a software 
instrument or debugger to insert probe code into the target program can help to 
identify potential problems, but it may cause a probe effect.  
Many-core debugging poses several challenges.  
First, the debugging or monitoring cannot disturb the sequential consistency of 
the target program. This kind of disturbance is called a “probe effect,”[1] and will 
tend to mask certain synchronization errors after an unknown delay.  
Second, in the context of parallel sequencing, on-chip trace modules are usually 
employed to record program execution or memory/register state changes and to allow 
for reliable offline re-execution using this information. A tracer requires a large 
6 
 
synchronization problems using the proposed NUDA mechanism. 
二.4 Main Contribution (NUDA Debugging) 
In summary, the contributions of this work are as following: 
1. An isolated non-intrusive debugging architecture (NUDA) based on a 
non-uniform memory map-ping. 
2. A distributed and simultaneous notify mechanism (Sync-token) upon 
NUDA for synchronous debugging-control. 
3. Software programming model (RunAssert) for many-core debugging with 
NUDA.  
4. A lockset based data race detection design on NUDA 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
8 
 
The most closely related proposals to Snoop2-NUCA are the Cooperative 
Caching [8][16][17] proposals. Chang and Sohi [8] present a unified framework to 
create a globally-shared aggregate on-chip cache with private caches. Figure 3- 1 
demonstrates that main idea is optimizing the average latency of a memory request so 
as to improve performance.  
 
Central Coherence Engine
L2 A Tags
L1 A Tags
L2 B Tags
L1 B Tags
L2 C Tags
L1 C Tags
L2 D Tags
L1 D Tags
L2 A
L1 A
CoreA
L2 C
L1 C
CoreC
L2 B
L1 B
CoreB
L2 D
L1 D
CoreD
 
Figure 3- 1: CC Memory Structure. 
 
Base on the Cooperative Caching (CC) framework, Herrero et al. present the 
Distributed Cooperative Caching (DCC), a scalable architecture which managing 
CMP cache resources. DCC redesign the coherence engine of CC by distributing it 
among all the nodes. In other words, the coherence engine is partitioned into 
Distributed Coherence Engines (DCE) that responds to a part of the address space, as 
shown in Figure 3- 2. DCC gets an average performance improvement and shows an 
increase in the MIPS3/W over the CC framework. 
 
Tags
L2 A
L1 A
CoreA
L2 C
L1 C
CoreC
L2 B
L1 B
CoreB
L2 D
L1 D
CoreD
Distributed Coherence 
Engine
Distributed Coherence 
Engine
Tags
 
10 
 
schemes: a snoop based protocol and a directory based protocol. Directory based 
coherences need an extra level of indirection, that is, use a directory to record the 
location and state of cached lines. With a large number of cores configuration, 
directory based coherences have more scalability for multiprocessors. 
Another is a snoopy protocol with bus to broadcast coherence operations. Snoop 
requests may suffer long latencies and induce many snoop messages. A drawback of 
snoopy bus is its limited scalability. In addition to, the power consumption of 
broadcast is unnecessary if no shared data-copy in other cores. 
    Therefore, much previous works on snoop energy reduction have been 
developed. JETTY [21] proposes a method that is a filter for reducing the energy 
consumed by snoop requests in snoopy bus-based symmetric multiprocessor (SMP) 
systems. Cantin [24] and Moshovos et al. [23] define a region to be a continuous, 
aligned memory area and observe that many requests miss in all remote nodes in 
shared memory multiprocessors. They propose RegionScout, a simple filter 
mechanisms that dynamically detect most non-shared regions. Using RegionScout 
filter can determine that a snoop request will miss in all remote nodes. The key 
difference between JETTY and RegionScout is that every node must snoops all 
request in JETTY (i.e. only avoid probing the L2 tag array), but in RegionScout, a 
requesting node can determine that a request would miss in all other nodes.  
Cantin et al. [24] shows that on average 67% to 94% of broadcasts are 
unnecessary in commercial, scientific and multiprogrammed workloads. Based on the 
observation, they propose a technique which be called as Coarse-Grain Coherence 
Tracking, monitors the coherence status of large regions of memory, and uses the 
information to avoid unnecessary broadcasts. 
Strauss, Shen, and Torrellas focus their attention on the study of using snoopy 
cache coherence with a unidirectional ring in the network of a multiprocessor [25]. 
Although logically-embedded rings in the network is a good choice for snoopy 
protocols (i.e. simple and low cost), it induce some problem. For instance, coherence 
may suffer long snoop request latencies and many snoop messages. Flexible Snooping 
[25] introduces Flexible Snooping algorithms and shows different trade-offs in 
number of snoop operations and messages, snoop response time, and energy 
consumption. 
Agarwal et al. [26] discuss the snoop protocol in mesh network. Just like above 
literature, they also want to solve the broadcast overhead of snoop protocol. In [26], 
they propose “In-Network Coherence Filters (INCFs)” inside on-chip routers that 
dynamically analyze sharing data among various cores. Using the sharing information, 
INCFs can filter away redundant snoop requests that are traveling towards unshared 
cores. Because these useless messages be filtered by INCFs, system can save network 
12 
 
Reference 
[1] B. Rogers, A. Krishna, G. Bell, K. Vu, X. Jiang, and Y. Solihin. Scaling the Bandwidth 
Wall: Challenges in and Avenues for CMP Scaling. In Proceedings of the Intl. 
Symposium on Computer Architecture (ISCA), 2009. 
[2] P. Kongetira, K. Aingaran, and K. Olukotun. Niagara: A 32-Way Multithreaded SPARC 
Processor. IEEE Micro, vol. 25, no. 2, 2005. 
[3] J. M. Tendler, J. S. Dodson, J. S. Fields, H. Le, and B. Sinharoy. POWER4 System 
Microarchitecture. IBM Journal of Research and Development, vol. 46, no. 1, 2002. 
[4] L. A. Barroso et al. Piranha: A Scalable Architecture Based on Single-Chip 
Multiprocessing. In Proceedings of the Intl. Symposium on Computer Architecture (ISCA), 
2000. 
[5] C. Kim, D. Burger, and S. W. Keckler. An Adaptive, Non-Uniform Cache Structure for 
Wire-Delay Dominated On-Chip Caches. In Proceedings of the International 
Conference on Architectural Support for Programming Languages and Operating 
Systems, 2002. 
[6] B. M. Beckmann and D. A. Wood. Managing Wire Delay in Large Chip-Multiprocessor 
Caches. In Proceedings of the annual International Symposium on Microarchitecture, 
2004. 
[7] J. Huh, C. Kim, H. Shafi, L. Zhang, D. Burger, and S. W. Keckler. A NUCA Substrate for 
Flexible CMP Cache Sharing. In Proceedings of the Annual International Conference on 
Supercomputing (ICS), 2005. 
[8] J. Chang and G. S. Sohi. Cooperative Caching for Chip Multiprocessors. In International 
Symposium on Computer Architecture (ISCA), 2006. 
[9] Z. Chishti, M. D. Powell, and T. N. Vijaykumar. Distance Associativity for 
High-Performance Energy-Efficient Non-Uniform Cache Architectures. In Proceedings 
of the Annual International Symposium on Microarchitecture, 2003. 
[10] C. Kim, D. Burger, and S. W. Keckler. Nonuniform Cache Architectures for Wire-Delay 
Dominated On-Chip Caches. IEEE Micro, vol. 23, no. 6, 2003. 
14 
 
[24] J. F. Cantin, M. H. Lipasti, and J. E. Smith. Improving Multiprocessor Performance with 
Coarse-Grain Coherence Tracking. In Proceedings of the International Symposium on 
Computer Architecture (ISCA), 2005. 
[25] K. Strauss, X. Shen, and J. Torrellas. Flexible Snooping: Adaptive Forwarding and 
Filtering of Snoops in Embedded-Ring Multiprocessors. In Proceedings of the 
International Symposium on Computer Architecture (ISCA), 2006. 
[26] N. Agarwal, L.-S. Peh, and N. K. Jha. In-Network Coherence Filtering: Snoopy 
Coherence without Broadcasts. In Proceedings of the Annual International Symposium on 
Microarchitecture, 2009. 
[27] M. M. K. Martin, M. D. Hill, and D. A. Wood. Token Coherence: Decoupling 
Performance and Correctness. In Proceedings of the Intl. Symposium on Computer 
Architecture (ISCA), 2003. 
[28] M. R. Marty, J. D. Bingham, M. D. Hill, A. J. Hu, M. M. K. Martin, and D. A. Wood. 
Improving Multiple-CMP Systems Using Token Coherence. In Procs. of the Intl. Symp. 
on High Performance Computer Architecture (HPCA), 2005. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
16 
 
四.2 Race Detection and Related HW Approaches 
There are two algorithms to detect race conditions, the “happened-before” [9, 10] 
and “lockset-based” [11-13] algorithms. The happened-before algorithm works by 
comparing the timestamps or vector clocks [14] of particular synchronization 
operations, such as access to shared objects, between threads. The benefit of the 
happened-before algorithm is the fact that it is easily for hardware implementation. 
However, the drawbacks are that it only discovers those races that are manifested 
during the monitored execution. On the other hand, the lockset-based algorithm 
provides more precise information to overcome the limitations of the happened-before 
algorithm. The Eraser [11] probe all of the lock/unlock operations and memory access 
events, in order to detect data races efficiently. 
There are several hardware approaches for debugging. FDR [15] and Rerun [16] 
are hardware approaches that use a low-overhead hardware recorder in the context of 
caches or cores, essentially to log the minimum thread ordering information that is 
necessary to play back the multiprocessor execution faithfully after the event. It’s 
truly helps programmers to debug, but a lengthy trace may be required. HARD [7] 
provides a novel hardware-assisted lockset-based race detection approach, and it 
employs additional fields (such as a Bloom filter vector) in the cache and detects the 
race condition. The HARD is the first hardware feasible by efficiently storing and 
maintaining the candidate set, which is a set of locks protecting a variable in hardware, 
and radically simplifying the set operations in the lockset algorithm. But, the design is 
subject to the imperfections of degraded race detection capability and high memory 
cost with several improvemet spaces. 
 
 
 
 
 
 
 
 
 
 
 
 
18 
 
[12] J. Voung, R. Jhala, and S. Lerner, "RELAY: static race detection on 
millions of lines of code," In Proceedings of the the 6th joint meeting of the European 
software engineering conference, pp. 205-214, 2007. 
[13] Y. Yu, T. Rodeheffer, and W. Chen, “Racetrack: efficient detection of data 
race conditions via adaptive tracking,” ACM SIGOPS Operating Systems Review, vol. 
39, no. 5, pp. 221-234, 2005. 
[14] M. Singhal, and A. Kshemkalyani, “An efficient implementation of vector 
clocks,” Information Processing Letters, vol. 43, no. 1, pp. 47-52, 1992. 
[15] M. Xu, R. Bodik, and M. Hill, "A" flight data recorder" for enabling 
full-system multiprocessor deterministic replay,” In Proceedings of the 30th annual 
international symposium on Computer architecture (ISCA'2003), pp. 122-133, 2003 
[16] D. Hower, and M. Hill, "Rerun: Exploiting Episodes for Lightweight 
Memory Race Recording," ACM SIGARCH Computer Architecture News, pp. 
265-276, 2008 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
20 
 
Core 2
Shadow-
Tag
Tag Data
L1 $
BF
MMU
Core 1
Shadow-
Tag
Tag Data
L1 $
BF
Ring Interconnect
Core 0
Shadow-
Tag
Tag Data
L1 $
BF
MMU MMU
M
em
o
ry
1
2
34
① Local L1 cache access ② Neighbor L1 cache access
③ Remote L1 cache access ④ L1 cache miss
MMU
Core 3
Shadow-
Tag
Tag Data
L1 $
BF
U
Core 4
Shadow-
Tag
Tag Data
L1 $
BF
MMU
Core 5
Shadow-
Tag
Tag Data
L1 $
BF
MMU
Core 6
Shadow-
Tag
Tag Data
L1 $
BF
MMU
Core 7
Shadow-
Tag
Tag Data
L1 $
BF
MMU
 
Figure 5- 1: System architecture of single-copy Snoop2-NUCA. 
 
Figure 5- 1 (a) demonstrates the data access in local cache slice. In snoop 
protocol, cache-to-cache transfer (1-hop) provides fast data communication. However, 
the main limitation of snoop protocol is unnecessary broadcast, this means snoop 
protocol can increase average request latency and lead to large amounts of 
interconnect traffic in network on chip. Vast literature has been investigated on the 
problem 
To utilize on-chip network bandwidth, we use another two kinds of non-uniform 
shared cache access without broadcasting shown in Figure 5- 1 (b and c). First, the 
core can access a shared cache slice of its neighbor by hitting the replicated tag 
(shadow-tag) which is discussed in our shadow-tag mechanism for efficient location 
search as the best case, and another one is to access remote shared cache slice with 
longer search latency and transfer time. As shown in Figure 5- 1 (d), external memory 
access is caused by cache miss occurring in total cache slices of ring. Based on, we 
adopt circuit-switched ring interconnections for benefiting ultra-low latency and low 
power under an acceptable frequency. 
Unlike broadcast-based coherence caches, single-copy Snoop2-NUCA does not 
generate extra traffic on searching neighbor cache slices, and it does not broadcast to 
access remote cache slices. With parallel execution and non-broadcasting features, L1 
shared cache slices are interconnected by parallel transaction interconnections. The 
conventional banked caches are connected via a 2D mesh interconnection network. 
22 
 
(a) Cache hit on local cache
(b) Searching Remote shared data (core4)
(c) Fetch data from neighbor core (core2) on the right side
(d) Fetch data from neighbor core (core0) on the left side
Tag-copy
0xab 0x81
Load/Store shared address region from core (ex. Core1)
   Tag           Data
L1 Data 
Cache Slice
=? =? =?
Index
Tag
Physical address
I/O
 
W
ra
p
p
e
r
b
c
d
To CPU
a
Cache slice 1
 
Figure 5- 2: Using shadow-tag to find block location. 
 
On-chip interconnection bandwidth can be saved by hiding neighbor cache slices 
queries in local cache slice. Consequently, on-chip bandwidth would not be hurt by 
the alternative local multicast, which does not resemble previous multicast search 
mechanism. The example in Figure 5- 2 illustrates that the local cache slices can 
decide that the data is mapped to the local cache slice, the two nearest cache slices, or 
none of them. When a data is requested by a processor (Core 1), it first checks three 
tag arrays reside in local slice (slice 1).  
If the tag of requested address is hit in the local L1 data cache, data responses 
directly to the querying core shown in Figure 5- 2 (a). If a request misses in the cache 
slice of core 1 by comparing the level-1 tag and shadow-tag arrays, the block is 
guaranteed not to be in core 1 and neighbor cores, that is, core 0 and core 2. As well 
as, the request would be transferred to other remainder slices as shown in Figure 5- 2 
(b). To utilize on-chip network bandwidth and avoid unnecessary search overhead, 
neighbor cache slices would be checked by snoop2 shadow-tag arrays hiding in the 
nearest cache slice without broadcasting shown in Figure 5- 2 (c) (d). 
五.4 Parallel Search Policy on Snoop2 
24 
 
Figure 5- 4 illustrates how the energy saving strategy with Bloom Filter works. If 
a load/ store address is absent from the local cache slice that indicated by Bloom 
Filter arrays, the requested cache line is not in the cache slice. Otherwise, the 
requested line is probable in the cache, but it is not guaranteed to be. It is known as 
“false positive”. 
Figure 5- 4 (a) shows the false positive effect. When a hit in Bloom Filter, the 
request is only checked the local tag to lower redundant energy consumptions at the 
first time, but the prediction may be failed. Therefore, Figure 5- 4 (b) shows the next 
operation. If a filter error is encountered, the request would be checked on shadow-tag 
arrays again with one cycle latency. Besides the examples given above, Figure 5- 4 (c) 
illustrates that if the requested data is not located in local cache slice and the miss is 
detected early enough through Bloom Filter. 
 
L1 D-cache Controller 
Bloom Filter
L1 D-cache Slice 
Shadow-Tags Tags Data
a
b
c
(a) A cache hit in local cache slice. (b) Bloom 
F ilter causes a false positive, and it needs to 
check s h a d o w - t a g  arrays with one cycle 
overhead. (c) For cache miss in local cache slice, 
directly check Shadow-tag arrays.
Maybe hit
Miss
Miss
 
Figure 5- 4: Using bloom filter for energy saving. 
 
 
 
 
 
 
26 
 
Inter-cluster 
debugging
0
NUDA
Node 3
NUDA
Node 2
NUDA
Node 1
NUDA
Node 5
63
NUDA
Node 0
NUDA
Node 7
NUDA-cluster
Directory
Many-core ICE Debugging host
Intra-cluster 
debugging
1
NUDA
Node 4
N
o
n
-U
n
ifo
rm
 la
te
n
c
y
Core 0
NUDA
Node 6
Monitor 
histogram
2
 
Figure 6- 1: The NUDA system architecture. 
 
The NUDA architecture consists of a cluster with a NUDA node, the NUDA 
interconnection to transfer information between NUDA nodes, and the non-uniform 
memory shared with NUDA nodes. Due to the distributed coordinate architecture, the 
philosophy of the NUDA defines the cluster as a collaborative unit. 
 
¤
S
yn
c-
T
o
ke
n
Core 0
DCP
Core 1
DCP
Core 2
DCP
Core 3
DCP
Core 4
DCP
Core 5
DCP
Core 6
DCP
Dbus
Filter & Send/Receive 
debugging events
¶ Mem, Thread events
Core 7
DCP
NUDA
Node N+1
NUDA
node N
NUDA
Node N-1
¶
 M
e
m
 e
ve
n
t
¤ Core control
 
Figure 6- 2: The concept of the debugging cluster (NUDA cluster). 
 
Fig. 6-2 shows the concept of a NUDA cluster that features two to eight 
neighboring cores and allows communication with the related NUDA nodes (Node 
28 
 
六.1 The Structure of The NUDA Node 
As shown in Fig. 6-3, the NUDA node structure consists of three major parts: 
computing, memory, and communication. The specific router handles a NUDA node’s 
communication with its neighboring NUDA nodes, local debugging bus (Dbus), and 
the internal computing part. The specific router uses a packet-switched design for 
node-to-node communication, but a circuit-switched design to connect Dbus and the 
computing part directly with intra-cluster core control signals. 
 
R
o
u
te
r
Cluster N
(Dbus)
Node N-1
(Ring-links)
Node N+1
(Ring-links)
I-MEM
CAM
Data
Bank
CAM
Switch
Data
Bank
Data
Bank
Data
Bank
(SRAM)
Nano-P (np)TAPc
Que
Event 
handler
Sync
Instruction
Memory Part
Computing Part
Event trigger
Config. Data-paths
Comunication 
Part
 
Figure 6- 3: The structure of the NUDA node. 
 
The key component in the computing part is a programmable nano-processor (nP) 
that is triggered by user-defined debugging events and activates the corresponding 
debugging processes. The nP replaces dedicated hardware functions and also handles 
events by predefined software routines. The particular routines are stored in a small 
instruction memory within each node. The event handler is composed of an interface 
and a queue to receive events and stimulate the corresponding behaviors. The TAP 
controller (TAPc) is responsible for intra-core control. The synchronization module 
(Sync) is used to manipulate the inter-core communications, such as cross-trigger 
events and synchronizations. However, it is still a challenge for the nP to provide a 
quick response or to record frequent events. An alternative solution is to employ a 
small accelerator (embedded FPGA or configured data-paths) for an extended 
instruction set addition. The configurable accelerator is implemented as a wide 
communication interface between computing and memory parts to accelerate the 
30 
 
TAPc.
Control signals (Dbus)
Core
DCPTranslate
Cmd.
Nano-p
Event 
handler
  Event
  Trace
Event 
handler CAM
Condition match
Ring-
links
Data banks
Hit
  Rule
  Check
Event 
handler
Data banks
Nano-p
I-MEM
Check 
routine
  Core
  Manage
  Non-   
  uniform 
  access
  SYNC Sync
Node 0
Sync
Node 1
Sync
Node 2
T
T
Node 0
access
Miss
  Core
  Control
Dbus
Ring-
links
Sync-TokenT
Core
DCP
Core
DCP
Remote
debug
CAM
Node 1
Router Directory
Many-core ICE
Router
Node N
CAM Data banks
Hit
Node N+1
Remote 
event
CAM
Tri
gge
r
DCP
Monitor 
core 
status
Data banks
Nano-p
I-MEM
Manage
routine
CAM
Event 
handler
Dbus
Ring-
links Core 
management
DbusDCP
Monitor 
core 
status
 
Figure 6- 4: Functionalities of a NUDA node. 
 
Furthermore, non-uniform debugging memory is organized with non-uniform 
access for greater scalability, and it creates larger monitor capacity for runtime 
debugging on many-core systems. The distributed event synchronization mechanism 
by "Sync-Tokens" helps with global synchronization in debugging. Overall, each 
NUDA node includes a programmable design implementation by software to satisfy 
various requirements. 
 
六.2 Non-Uniform Debugging Memory Design Space 
The architecture-dependent construction for debugging purposes may suffer from 
mutual influences between normal and debugging operations. For example, a 
debugging storage aligned with cache hierarchies must inevitably be polluted by 
useless debugging events that are used for core execution, and the useful debugging 
histogram can be lost or swapped out during a cache miss. On the contrary, an 
independent debugging memory can be well utilized for runtime monitoring without 
intruding unnecessary events. Therefore, our design strategy use the independent 
memory for runtime monitoring, but it is distributed, rather than centralized, as a 
non-uniform cache architecture (NUCA), as shown in Fig. 6-5(b). 
The conventional NUCA design features larger shared memory for the benefit of 
multi-core communication, especially in some multithreading programs. It usually has 
32 
 
 
Fig. 6-5(a) depicts the page-based memory design consisting of content 
addressable memory (CAM) and SRAM memory. The CAM memory is used for 
parallel searching for the location of the monitor page, and then accessing the monitor 
histogram in SRAM memory. It acts as a fully associative cache to swap out data only 
when meeting an overflow for minimizing the probability of cache miss. The 
page-grain design strategy is not only because of the cost of CAM memory usage, but 
also for easy management. Fig. 6-5(b) shows the page-grain NUCA distributing in 
multiple NUDA nodes for scalable monitor capacity. Setting the DCP by user-defined 
#pragma or compiler aids, interesting address spaces are filtered out, and then 
monitored by the page-grain NUCA. As shown in Fig. 6-5(b), those interesting 
regions correspond to a monitor page in a certain NUDA node. For example, 
interesting region I is mapped to a monitor page in NUDA node 3, and region I/III are 
mapped to NUDA node 0. 
In general programs, access to shared variables is governed by data locality. For 
example, two threads may access the same shared variables, but at different time 
slices, by the protected locks, which means that a quantity of debugging events are 
generated from a thread at a given time slice. If the thread and the monitor page are in 
the same NUDA cluster, the debugging event triggers an intra-cluster histogram 
access. On the other hand, three steps are required for an inter-cluster histogram 
access, with longer processing latency. In order to eliminate the global stall caused by 
too much inter-cluster histogram access, a good page migration mechanism is 
necessary, and the information of lock/unlock is a possible reference for prediction. 
Fig. 6-5(1-3) describes the inter-cluster debugging histogram access flow. At first, 
the debugging event accesses the local NUDA node to search for the matching 
monitor region. If the search fails, the debugging event goes to many-core ICE for a 
central location directory checking. Then, the many-core ICE passes the event to the 
remote NUDA node for the inter-cluster debugging histogram access. Monitor page 
migration is necessary when different threads are accessing the same shared variables 
at different time slices. In our design strategy, a page queue in the many-core ICE 
dynamically records inter-cluster access counts of current frequent-used pages and 
makes a decision for migration if the count is over a defined ratio. If a monitor page 
migration is triggered in a NUDA node with no available storage, it selects a monitor 
page to replace, and the replaced page is swapped to neighbor NUDA nodes or 
exchanged with the migration page. 
 
34 
 
-Debugging exception 
-Tolerate the latency of runtime debugging process  
2) Update all cores 
-Updating NUDA nodes’ configuration 
-Globally examining debugging events 
 
It is sometimes necessary to start/stop/update all cores simultaneously. While 
meeting a debugging exception, our strategy relies on synchronously stopping all 
cores and activating the related handling routine. The debugging exception includes 
breakpoints, race detection, user defined assertions alert, histogram overflow, and so 
on. Sometimes, too many debugging events concurrently happening in close 
succession or a debugging event which has long processing time may cause the event 
queue (in DCP or NUDA node) to become full. In order to avoid losing debugging 
events, our strategy consists of synchronously stopping all cores to digest them as a 
tolerance without disturbing the original execution order. In other words, it is also 
necessary to support synchronously updating all NUDA nodes or globally examining 
debugging events.  
In this work, we propose a "Sync-Token" mechanism for synchronous 
debugging-control. The key idea behind our “Sync-Token" is to pass a specific and 
related countdown number to each NUDA node, as shown in Fig. 6-7. This 
countdown number decreases incrementally with time. When the token passes 
through all of the NUDA nodes, the countdown numbers across the nodes are allowed 
to reach zero, whereupon they can be synchronized for start/stop/update operations. 
Fig. 6-7(a) shows an example of how the sync-token works. Assume that 
Sync-event A happens in NUDA node N3 and is associated with token number 4. One 
cycle later, Token A broadcasts to NUDA node N2 and NUDA node N4 and transmits 
token number 3. At the same time, the token number at NUDA node N3 itself 
decreases to 3. Therefore, NUDA nodes N2, N3, N4 are synchronized, as shown in 
Fig. 6-7(b). However, there is another Sync-event B that is being concurrently 
processed by NUDA node N7. The tokens broadcast conflict, as shown in Fig. 6-7(c). 
NUDA nodes N1, N2, N3, N4, and N5 are synchronized by token number 2 and 
NUDA nodes N0, N6, N7 are synchronized by token number 3. Under our mechanism, 
lower-numbered tokens can inherit larger numbers, so Sync-event B will hold and 
wait to be resent at NUDA node N7. When all the tokens across the NUDA nodes 
count down to zero, all the NUDA nodes can be synchronized for non-intrusive 
control. 
Non-intrusive debugging control is one of the main debugging targets of the 
NUDA system for greater scalability in future many-core systems. The proposed 
36 
 
Configurable 
Filter (CF) Unit
Debugging Co-processor (DCP)
    FSM
I-b
u
ffe
r
Status reg
O
-b
u
ffe
r
D
b
u
s
_
in
Control 
signals
Data
Debugging 
Events
Control
Config.
Monitor core status
(PC, load/store, thread 
info, and so on)
D
b
u
s
_
o
u
t
Core
Registers
(64 ~ 256-bit)
Read
Update
Set config.
C
o
n
fi
g
. 
re
g
Path
Sel
Selectable Data-paths
(includes AND, OR, XOR, Shifter, 
Compare, One-Hot encoding)
(1) Vector filter
(2) Bloom filter
Monitor core status
Debugging 
Events
Configurable Filter (CF) Unit
Cmd
 
Figure 6- 8: Configurable filter (CF) unit in DCP. 
 
Fig. 6-8 also depicts the composition of the CF unit, which mainly includes a set 
of registers and selectable data-paths. The set of registers can be assumed to be the 
temporal storage of some candidates that are compressed for filtering, and those 
registers are totally reusable in the proposed multiple functions. Data paths are built 
out of multiple selectable paths with several basic operations, including "AND", "OR", 
"XOR", "Shifter", "Compare", "One-Hot encoding", and so on, by selecting a certain 
data-path for the desired operation and combing registers for reading and updating. 
We plan to configure two filtering mechanisms ("Vector-filter" and "Bloom-filter"). 
The cost of the CF unit depends on the size of registers and the operation bandwidth 
of the data paths. 
An adaptive re-range "Vector filter" is shown in Fig. 6-9, which features a high 
filter-rate and low complexity for multiple non-continuous monitor-spaces. Usually, 
the interesting regions for monitoring or observing will be user-defined, and those 
regions may be distributed (different cores or threads in different regions of interest).. 
In the debugging setting, a chunk region is defined for including multiple fragmental 
monitor spaces. Therefore, there is a rough filter to detect whether or not events hit 
the chunk region. 
 
38 
 
Monitor Event Set C(v)
Address of M1
Address of M2
Address of M3
Address of M4
Address of M5
Address of Mn
0 0 0 1 0 0 1 1 1 B
F
V
e
cto
r 
re
g
iste
r
1 0 1 1 1 0 1 1 1
0 1 0 1 0 0 1 0 0
Compress monitor events into Bloom filter vector (static time)
0 1 1 1 0 0 1 1 1
O
n
e
-H
o
t 
e
n
co
d
in
g
In
clu
sio
n
 
(O
R
)
0 0 0 1 0 0 1 1 1 B
F
V
e
cto
r 
re
g
iste
r
1 0 1 1 1 0 1 1 1
0 1 0 1 0 0 1 0 0
0 1 1 1 0 0 1 1 1
E
v
e
n
t's a
d
r
O
n
e
-H
o
t 
e
n
co
d
in
g
Intersection (AND)
Non-
empty
Empty
Monitor 
event
(M1, M2, ...)
 
Figure 6- 10: "Bloom-filter" for sparse fragmental monitor spaces. 
 
In order to address the problem of the low filter-rate caused by sparse fragmental 
monitor spaces, another alternative is to use a "Bloom-filter". As shown in Fig. 6-10, 
the basic idea is to compress monitor events into a Bloom filter vector (BFVector), 
which is used to filter out the desired monitor events in runtime. The compressed 
BFVector is stored in the set of registers in the CF unit at a static time. Therefore, in 
runtime debugging, a monitor event just intersects with the BFVector register to 
decide whether or not it is a candidate. The method shows a hardware-feasible 
solution for sparse fragmental monitor spaces, and it has very low miss judgments to 
allow unnecessary events to pass though. In our experimental results, either of the two 
filtering mechanisms can perform well, depending on the specific debugging 
requirements. 
 
六.5 Software Assistance: RunAssert 
The NUDA framework includes the software solution for a better experience 
with parallel program debugging. We used directive #pragma in C language as a 
programming model, which is composed of directives, macro functions and assertion 
expressions in a library. Because it has the characteristics of runtime assertion, we call 
this tool RunAssert. Users can specify the assertion type, range, scope and target of 
directives and then define particular rules by macro functions. Finally, assertion 
expression is an option used to indicate the particular conditions of the debugging 
rules. The concept of a non-intrusive programming model for parallel program 
debugging is to decouple the debugging and guarding operations from the ordinary 
40 
 
reconfigurable logic on NUDA node. The race condition is one of the hardest and 
most significant problems in parallel programs, as a huge number of debugging events 
spill from each core into the debugging channel. The key issue in detecting races 
non-intrusively is how to filter the massive suspected memory accesses. With 
RunAssert directives, users can easily define race conditions to locate precisely the 
critical section that requires monitoring. At the beginning, programmer uses the 
RunAssert directives to identify the target. In this case, we use the directive mdb_lock 
to indicate the locks and use the directive mdb_shared to indicate the shared objects. 
Second, the programmer needs to identify the checking rules with objects within the 
RunAssert global configuration directive. The macro ESR(void (*fp)(void*)) assists 
users to indicate the exception service routine that is executed as the rules are 
established. Moreover, the macro LOCK assists users to identify a guarding lock, and 
the following macro MONITOR assists users to list the monitoring shared objects. In 
order to express the lock hierarchy in the source code, we use the horizontal 
expression to identify nested locks. In Figure 6-11, the shared variable X is under the 
nested locks S1 and S2, so the RunAssert representative is LOCK(t1.S1 && t4.S2). 
The main benefit of using the horizontal expression is that users can narrow the 
monitor scope in the inner locks. For instance, we only need to use LOCK(t1.S2) to 
describe the process of monitoring S2. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
42 
 
七.2 Benchmark Workloads 
Table 7- 2: Benchmark Workloads and Inputs. 
Shared Memory Applications 
SPLASH-2 PARSEC 2.1 
Program Input Program 
barnes 65,536 particles blackscholes fluidanimate 
cholesky tk29.O bodytrack freqmine 
fft 4,194,304 data points canneal streamcluster 
fmm 65,536 particles dedup swaptions 
lucontig 1024×1024 matrix, 64×64 blocks facesim vips 
lunoncontig 1024×1024 matrix, 64×64 blocks ferret x264 
oceancontig 514×514 grid Input 
oceannoncontig 514×514 grid Medium set 
raytrace teapot  
radix 8,388,608 integers  
waternsquared 4096 molecules  
waterspatial 4096 molecules  
 
The study focuses on parallel workloads, we use shared memory programs from 
the PARSEC and SPLASH-2 benchmark suits. All of the PARSEC benchmarks use 
the medium dataset. Table 7- 2 also shows the detail of the input data of SPLASH-2 
workload used in our experiments. The analysis is carried out to study the region of 
interest (ROI) of SPLASH-2 and PARSEC, respectively. Following the approach 
supported by M5 simulator with creating a checkpoint file to forward fast to the 
beginning of ROI. Results are collected over the end of ROI. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
44 
 
L1-NUCA is lowest than other benchmark by amount of shared data. Therefore, the reason for 
the L1 miss rate of FMM in ideal L1-NUCA is higher than private L1 cache architecture with 
coherence is that there are not good replacement and migration policies in ideal L1-NUCA. 
Without smart replacement and migration policies, the ideal L1-NUCA induces cache 
pollution in FMM benchmark. In other word, one core could evict some cache data which is 
used by other cores. 
 
 
Figure 8- 2: Comparisons of L1 miss ratio in PARSEC 2.1. 
 
八.2 Off-Chip Memory Bandwidth 
The Figure 8- 3 and Figure 8- 4 show the results of last level cache (LLC) 
bandwidth reduction in SPLASH2 and PARSEC 2.1. 
Because there is only one cache level hierarchy, we estimate L1 as last level 
cache for PL1 and ideal L1-NUCA architectures. The LLC bandwidth stands for 
requesting data from off-chip memory. We expect there is lower bandwidth in cache 
architecture, because of bandwidth wall issue. As shown in Figure 8- 3 and Figure 8- 
4, ideal L1-NUCA has smallest off-chip bandwidth than other cache architecture. 
Average bandwidth reduction is 25% in SPLASH2 and 19% in PARSEC 2.1 in ideal 
L1-NUCA compared with private L1 cache. 
 
0.7% 3.7% 12% 1.8% 1.8% 2% 0.4% 3.6% 4.6% 1.3% 1.8% 0.4% 0.1% 2.6% 
0.0
1.0
2.0
3.0
4.0
5.0
6.0
N
o
rm
a
li
ze
d
 t
o
 P
L
1
L1 Miss Rate
PL1
L1L2
Ideal_L1NUCA
46 
 
to Figure 8- 3 and Figure 8- 4. The ideal L1-NUCA is a good architecture for reducing 
off-chip bandwidth. 
 
 
Figure 8- 5: Comparisons of LLC bandwidth reduction (MB/s) in SPLASH2. 
 
 
Figure 8- 6: Comparisons of LLC bandwidth reduction (MB/s) in PARSEC 2.1. 
 
 
 
 
672 239 222 334 2159 766 181 3045 3779 436 675 918 1119 
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
N
o
rm
a
li
ze
d
 
to
 P
L
1
 M
B
/s
Average Last Level Cache Bandwidth
PL1
L1L2
Ideal_L1NUCA
 
588 1301 2845 998 1332 639 353 2043 1795 994 943 107 148 1083 
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
N
o
rm
a
li
ze
d
 
to
 P
L
1
 M
B
/s
Average Last Level Cache Bandwidth
PL1
L1L2
Ideal_L1NUCA
48 
 
 
Race detection by the NUDA is essentially real-time and non-intrusive to the 
original execution. However, in the case that the monitoring buffer is full, the whole 
system has to be stopped. Table 9-2 illustrates the NUDA overhead in SPLASH-2 for 
runtime race detection. We found the overhead to be inversely proportional to the 
buffer size; more DCP buffers can tolerate more concurrent memory access events. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
50 
 
too high and is infeasible in most many-core environments. Our proposed NUDA 
features non-intrusive truly race detection without false positives. The system offers 
negligible slowdown (0.51~3.06%↓), and supports user defined assertions. Table 
10-1 also shows the related hardware cost estimates for several popular many-core 
processors, interconnection units, and memory, by CACTI 5.3. We assume that the 
many-core environment is composed of 64 Intel Atom processors, with 16KB I/D 
cache for each core, and 16MB L2-NUCA for sharing. The total estimated area is 564 
mm2 under 65nm technology. Comparing HARD and NUDA, the main factors in our 
estimates were memory-usage and interconnections. Other elements (logic, FSM, etc.) 
do not impact the chip area. In HARD, the BFvector (2B/per line) in the L1 cache is 
estimated from SRAM usage, while the BFvector (2B/per line) in the L2 cache is 
estimated from DRAM usage. There are 8 NUDA nodes in our proposed system. In 
addition, the width of the ring interconnection that links all 9 nodes (include the 
many-core ICE) is 1B, and we also include the ring routers. As shown in Table III, the 
HARD has 0.98% (5.53/564) of the area cost compared with the proposed many-core 
system, and this work has 0.37%. 
 
Table 10- 1: Comparisons of Race Detection Methods (64-Core) 
* Valgrind works on Intel Core2 Quad CPU 2.5G RAM 2G workstation with Linux kernel 2.6.24
† The SPEC of HARD [11]  (normalize to 65nm): 2B BFVector/L1 $line (1.94mm2), 2B BFVector/L2
   $line (3.58mm2), the estimated area: 5.53 mm2.
‡ The proposed SPEC of NUDA (normalize to 65nm): 8 NUDA clusters, 8KB SRAM (8*0.19mm2), 
   32entries CAM (8*0.041mm2) in each NUDA node (C:32/P:1KB/B:32B), 2B-width local Dbus in each 
   NUDA cluster (0.1mm2), ring interconnection (9 nodes, 2 rings, 1B-width/per-ring) (0.135mm2), the 
   estimated area: 2.08 mm2.
§ Memory ref (cacti 5.3v, 65nm): 32B/line 2048-entry DRAM: 0.34mm2, 2B/line 2048-entry DRAM: 
   0.014mm2, 2B/line 128-entry SRAM: 0.0076mm2, 8B/line 1024-entry SRAM: 0.19mm2, 4B/line 32-
   entry CAM: 0.041mm2
¶ The assumed SPEC of many-core SOC (normalize to 65nm): 64 Atom cores with 16KB 4-way 32B/
   line I/D-cache (6.1mm2 per core), 16MB 8-way 32B/line 256 banks L2-NUCA  with Mesh NoC 
   (174.4mm2), the estimated area: 564 mm2.
\\ Many-core chip ref: Intel’s 80-Core (80 tiles, 65nm): 275mm2, 0.5mm2 (per router), The CELL 
   processor (12 tiles, 90nm): 235mm2, The UltraSPARC T1 (14 tiles, 90nmm,): 378mm2,The Atom (1 
   tiles, 45nm): 25mm2, EIB in CELL (12 nodes, 4 rings, 16B-width/per-ring, 90nm): 5.98mm2.
Slowdown
Valgrind* [8] HARD† [11] NUDA‡ (This Work)
60-400X 0.1-2.6% 0.1-3.06%
Non-intrusive
False positive/negative 
User-defined assertions
Cache coherency
Memory-bit usage§
Area estimation¶ (65nm)
No Probably Yes Yes
Positive Both None
No No Yes
No Required No
No
64KB extra in L1$,
1MB extra in L2$
64KB extra SRAM,
1KB extra CAM
0.0% 5.53mm2 (0.98%\\) 2.08mm2 (0.37%\\)
 
 
Table 10-2 compares the race detection results. We modified the SPLASH2 
macro to create delta locks, which involve assigning different locks every time a lock 
is acquired. The software, produced too many false positives and the execution speed 
52 
 
十一 Conclusion - Snoop2-NUCA 
We proposed a Snoop2-NUCA framework and a shadow-tag mechanism with 
efficient location-searching. The method avoids the high bandwidth being produced 
by the snooping coherence broadcast. Previous NUCA researches often used 
broadcast to search requested data. Although with small latency, the method produce 
high power consumption, and our proposed Snoop2-NUCA architecture may reduce 
power consumption and access latency. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
100 年 6 月 20 日 
 
報告人姓名 陳添福 服務機構 國立交通大學資工系 職稱 教授 
  會   議 
 時間地點 
2011/06/05~ 09   
美國加州聖地牙哥 
本會核定補助文號  
會 議 名 稱  中文: 2011 IEEE/ACM 國際設計自動化會議 
  英文: 2011 IEEE/ACM Design Auatomation Conference (DAC) 
 
一、參加會議經過 
 筆者此次蒙國科會補助出席在美國加州聖地牙哥舉辦之"2011 IEEE/ACM 國際設計
自動化會議", 會期自 100年6月5日至100年6月9日為期五天。 
 
 每年的國際設計自動化會議（DAC）一直是國際晶片系統學界與業界最重要且規模
最大的會議，今年會議更創紀錄有來自世界各國將近一萬多位學者專家與會參加，除學
術界學者外，亦有業界等主要電腦公司研究人員的研究人員參加。會中主要活動計有: 
一般論文發表、專題演講、短期課程、tutorials及poster等，更有200家廠商的產品展覽. 
論文發表討論熱烈，與會者均感收穫不少，並能凝聚出學術界與工業界對未來設計自動
化走向的共同看法。國際設計自動化會議（DAC）一直是國際晶片系統學界與業界中最
重要的學術會議之一，往往均能顯示未來的研究方向。 
 
 整個DAC 大會共分為超過35 technical sessions涵蓋最近發展的design methodologies 
與 EDA tool developments。同時展國際重要的出有超過200家EDA, silicon and IP 
Providers。會議論文發表區分幾項領域Power Analysis and Low-Power Design(5 
sessions)、Physical Design and Manufacturability(4 sessions)、System-Level Design and 
Analysis(4 sessions)，主題包括：Circuit Simulation and Timing Analysis, Electrical-level 
circuit and timing, Design Tools, research and development of design tools and supporting 
algorithms. innovative methodologies for the design of electronic circuits and systems, design 
automation in state-of-the-art design projects. Embedded Systems等。 
 
二、與會心得 
Single-copy L1-NUCA for cache capacity maximiza-
tion in Multicore SoC 
Chien-Chih Chen †, Yin-Chi Peng ‡, Chi-Neng Wen‡, Shu-Hsuan Chou ‡ and Tien-Fu Chen† 
ABSTRACT 
The external memory bandwidth requirement is one of key factors in 
terms of real-time performance and power envelope. An efficient 
optimization way is to maximize the utilization of cache subsystem 
and provide sufficient controllability for programming. This work 
proposes a level-1 single-copy non-uniform cache architecture (L1-
NUCA) to well utilize the capacity of on-chip SRAM and save 
external memory bandwidth. In preliminary analysis results, L1-
NUCA averagely reduces 22% external memory bandwidth 
compared with the cache coherence method. 
1. INTRODUCTION 
Multicore systems with an increasing number of cores provide 
the potential for throughputs and performance gains. However, off-
chip bandwidth bottleneck is inevitable; since the additional cores 
produce additional cache misses which should be serviced by off-
chip memory [1]. For this reason, the first challenge of reducing off-
chip access is to maximize the utilization in size-limited on-chip 
caches. Since an application splits multiple tasks for multicore col-
laborative execution, having on-chip communication and data coher-
ence will be necessary to improve system performance and reduce the 
software complexity. The second challenge is to provide effective 
data organization in distributed shared caches. 
There are two basic schemes to maintain the on-chip cache in 
multicore memory systems. One is that each sliced on-chip cache is 
used as private cache for the local core. It provides low latency by 
keeping read-only shared data in local cache slice. In order to keep 
data consistency, additional cache miss, coherence miss, occurs. As 
well as, it causes unnecessary lookups and power consumptions, and 
that multiple copies of shared data will degrade cache capacity, in-
creases the miss ratio and cause data pollution. The other is that the 
distributed slices are coordinated to form a high capacity shared on-
chip cache with non-uniform access latency [3, 4]. This scheme uti-
lizes on-chip capacity with single-copy data to reduce the off-chip 
memory traffic for large shared working set. For read-write shared 
data, it will save the energy consumption without using broadcast 
lookups. At that point, single-copy cache utilizes cache capacity and 
reduces congestion of off-chip memory demand.  
In this work, we target on above problems and propose single-
copy level-1 non-uniform cache architecture (single-copy L1-NUCA) 
to utilize cache capacity solving off-chip bandwidth bottleneck and 
keep energy consumption low estimating broadcast overhead. 
2. PROPOSED SINGLE-COPY L1-NUCA 
Single-copy L1-NUCA is a technique which is proposed for well-
utilizing the capacity of level-1 shared cache. The design space in-
cludes cache architecture and distributed shared location searching 
mechanism. 
2.1 System Architecture 
As shown in Figure 1, the single-copy L1-NUCA consists of L1 
cache architecture with the neighbor tag copies, the particular cache 
controller for local core to manipulate distributed shared location 
searching mechanism and the communication channel between cach-
es. We adopt the circuit-switched ring interconnections for benefiting 
ultra-low latency and low power under an acceptable frequency [2]. 
Multiple transactions can be simultaneously transferred in a single 
cycle to solve potential on-chip bandwidth problem. In addition, the 
distributed location search mechanism is designed to take advantages 
of ring topology for optimized search time. 
2.2 Tag-copy Mechanism for Distributed Location 
Search 
With multiple banks in single-copy L1-NUCA, we have choice of 
allowing a cache block to reside in one of multiple banks. In order to 
search distributed shared memory locations in single-copy L1-NUCA 
more quickly, this work proposes an efficient location-searching 
mechanism by duplicating neighbor cores’ tag array as shown in 
Figure (a). Figure (b) illustrates the efficient single-copy L1-NUCA 
search mechanism.  
These duplicated tags of each core are connected to each other us-
ing point-to-point interconnection, separate from the network con-
necting peer caches for data transfers. When a miss event occurs, the 
core of the missed request will issue an update message to the left 
and right neighbors. On the other hand, the neighbor cores will up-
date the corresponding block in the duplicated tags after they receive 
the update massage.  
3. PRELIMINARY RESULTS 
In the proposed single-copy L1-NUCA experiment, we modified 
the full-system simulator M5 [5]. The experiment constructs 8 cores 
two kinds of configuration as shown in Table 1. 
Figure 3 shows the average bandwidth (normalized by L1 cache 
coherency) of each configuration. The L1 cache coherency configura-
tion broadcasts information to each core and off-chip memory 
through shared bus. It caused a lot of unnecessary off-chip traffic. 
Therefore, the Ideal L1 NUCA effectively reduces off-chip band-
width at almost PARSEC benchmarks. In facesim, the result shows 
that there are no extra benefits in Ideal L1 NUCA, because facesim 
showed it almost does not have shared data. 
4. CONCLUSION AND FUTURE WORK 
We proposed a single-copy L1-NUCA framework and a tag-copy 
mechanism with efficient location-searching. The method avoids the 
high bandwidth being produced by the snooping coherence broadcast.  
Previous NUCA researches often used broadcast to search re-
quested data. Although with small latency, the method produce high 
power consumption, and we expect our proposed tag-copy technolo-
gy may reduce power consumption. 
‡
Dept. of CSIE, National Chung Cheng University, 
Taiwan, R.O.C. 
{pyc98m, wcn93, csh93}@cs.ccu.edu.tw 
†
Dept. of CS, National Chiao Tung University, 
Taiwan, R.O.C. 
john740207.cs99g@nctu.edu.tw, tfchen@cs.nctu.edu.tw 
CONFERENCE PROGRAM
TRAVEL & STAY
REGISTRATION
EXHIBITS
FOR EXHIBITORS
RELATED EVENTS
DAC USER TRACK
48TH CALL FOR 
CONTRIBUTIONS
DAC VOLUNTEERS
STUDENTS & 
SCHOLARSHIPS
DEADLINES
PRESENTING YOUR WORK 
AT DAC
Filter By Topic General Interest
Filter By Type -- All Event Types --   GO 
WORK-IN-PROGRESS  
Work In Progress (WIP) Poster Session
Topic Area: General Interest 
Wednesday, June 8, 2011 
Time: 6:00 PM — 7:00 PM
Location: Sails Pavilion
Summary: New this year to the DAC program is the Work in Progress (WIP) poster session. This session is 
designed to provide authors an opportunity to present their ideas to industry peers in an effort to initiate 
discussion and gain feedback in the early stages of the project.
 
Organizers: Nikil Dutt - Univ. of California, Irvine, CA
 Soha Hassoun - Tufts Univ., Medford, MA
 
Speaker: Suman Kaylan Mandal - Texas A&M Univ., College Station, TX
Authors: Nikhil Gupta - Texas A&M Univ., College Station, TX
 Suman Kaylan Mandal - Texas A&M Univ., College Station, TX
 
 
Hybrid CMOS-MQCA Logic Architecture Using Multi-Layer Spintronic Devices 
Speaker: Jayita Das - Univ. of South Florida, Tampa, FL
Authors: Jayita Das - Univ. of South Florida, Tampa, FL
 Syed M. Alam - Everspin Tech., Inc., Austin, TX
 Srinath Rajaram - Univ. of South Florida, Tampa, FL
 Sanjukta Bhanja - Univ. of South Florida, Tampa, FL
A Parametric Model Order Reduction Approach for Weakly Nonlinear Systems 
Speaker: Ehsan Rasekh - The Univ. of Western Ontario, London, ON, Canada
Authors: Ehsan Rasekh - The Univ. of Western Ontario, London, ON, Canada
 Anestis Dounavis - The Univ. of Western Ontario, London, ON, Canada
Statistical Model of TiO2 Memristor 
Speaker: Hai Li - Polytechnic Institute of New York Univ., Brooklyn, NY
Authors: Miao Hu - Polytechnic Institute of New York Univ., Brooklyn, NY
 Hai Li - Polytechnic Institute of New York Univ., Brooklyn, NY
 Yiran Chen - Univ. of Pittsburgh, Pittsburgh, PA
 Robinson E. Pino - Air Force Research Laboratory/RITC, Rome, NY
Unequal RF Interconnected Wireless Network-On-Chip to Improving On-Chip Communication 
Performance 
Speaker: Ruizhe Wu - Univ. of Louisiana at Lafayette, LA
Authors: Ruizhe Wu - Univ. of Louisiana at Lafayette, LA
 Danella Zhao - Univ. of Louisiana at Lafayette, LA
An Efficient Methodology of Topology Selection and Sizing in Geometric Programming-Based Design 
Environment 
Speaker: Supriyo Maji - Indian Institute of Technology, Kharagpur, India
Authors: Supriyo Maji - Indian Institute of Technology, Kharagpur, India
 Pradip Mandal - Indian Institute of Technology, Kharagpur, India
A Heterogeneous On-Demand Load-Balanced Cloud-Ground System with Special Emphasis on 
Functional Verification Applications 
Speaker: Swapnajit Mitra - PLX Technology Inc., Sunnyvale, CA
Authors: Swapnajit Mitra - PLX Technology Inc., Sunnyvale, CA
 Kiran Maiya - Synopsys, Inc., Mountain View, CA
Post-Manufacturing Instruction Synthesis for Adaptive Embedded Processor System 
Speaker: Yong Kyu Jung - Adaptmicrosys, LLC and Gannon Univ., Erie, PA
Author: Yong Kyu Jung - Adaptmicrosys, LLC and Gannon Univ., Erie, PA
Learning Digital Circuits: Initial Results 
Speaker: Bo Marr - Raytheon Company, Manhattan Beach, CA
Author: Bo Marr - Raytheon Company, Manhattan Beach, CA
A Highly Energy-Efficient Accelerator Enabling Post-Silicon Engineering Changes and Its Patch 
Compilation Method 
At a Glance Search
Keynotes
Technical Program
Panels
Tutorials
User Track
Management Day
ESS Executive Day
Work-In-Progress (WIP)
Workshops & Colocated 
Events
Adjunct Events
Additional Meetings
頁 3 / 8DAC - DAC 2011 - Conference Program - Work-In-Progress (WIP)
2011/5/24http://www.dac.com/work_in_progress+_wip_.aspx?event=144&topic=13
國科會補助計畫衍生研發成果推廣資料表
日期:2011/12/13
國科會補助計畫
計畫名稱: 子計畫一：前瞻多核心系統架構設計與系統環境建置(2/2)
計畫主持人: 陳添福
計畫編號: 99-2220-E-009-069- 學門領域: 晶片科技計畫--整合型學術研究
計畫
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
