components of some of these devices. 
However, existing techniques that compute 
static speeds for non-preemptive 
real-time tasks either are based on pessimistic 
feasibility tests or overestimate preemption 
times on non-preemptive tasks, leaving 
much room for improvement. In this project, 
we propose energy-efficient scheduling techniques 
which overcome the limitations of 
two representative energy-aware algorithms 
for scheduling non-preemptive tasks on variable- 
speed processor systems. The DS* algorithm, 
which computes two speeds for a 
given task set, is found to excel at low system 
utilization； the ISA algorithm, which 
computes one speed for each individual task, 
shows its strength as the system load gets 
higher. Our preliminary experimental results 
show that both DS* and ISA obtain significant 
energy gains, compared to the existing 
techniques from which they evolve. 
In this proposed research, we will first 
investigate a selective frequency inheritance 
policy, which aims to efficiently determine if 
processor speedup can be disabled without 
jeopardizing any task deadlines whenever a 
task is blocked. We expect it to be a useful 
policy in terms of saving energy consumption, 
when combined with ISA. We will 
also consider the potential benefits of 
non-preemptive scheduling algorithms for 
saving energy on multiprocessor environments, 
particularly in those systems where 
job migration is too expensive. In addition, 
we will discuss how our approach must 
change if static power consumption is taken 
into consideration. 
英文關鍵詞： Real-time computing,dynamic voltage scaling, fixed-
priority scheduling, dynamic and static power 
consumption, selective frequency inheritance. 
 
I 
 I 
目錄 
一、 摘要………………………………………………………………………………..II 
(一) 中文摘要……………………………………………………………………...II 
(二) Abstract……………………………………………………………………….II 
二、 報告內容…………………………………………………………………………..1 
I. INTRODUCTION……………………………………………………………..1 
II. RELATED WORK…………………………………………………………….1 
III. MODEL AND ASSUMPTIONS……………………………………………....2 
IV. INDIVIDUAL-SPEED ALGORITHM………………………………………..3 
V. PERFORMANCE EVALUATION……………………………………………8 
VI. CONCLUSION………………………………………………………………..10 
三、 參考文獻…………………………………………………………………………..10 
  
1 
 
二、報告內容 
I. INTRODUCTION 
Power consumption has become one of the most 
important issues when it comes to designing many 
mobile and embedded real-time applications. When it 
is necessary to trade system performance for reduced 
power consumption, the system can exploit Dynamic 
Voltage Scaling (DVS) to change the supply voltage 
dynamically or Dynamic Threshold Voltage Scaling 
(DTVS) by controlling the body bias voltage to 
change the threshold voltage dynamically. Different 
supply voltages or threshold voltages result in differ-
ent processor speeds. Therefore, how to choose the 
proper speeds, along with the supply voltages and 
threshold voltages, is of importance for energy reduc-
tion and meeting system timing requirements.  
In this work, we consider energy-efficient 
scheduling of non-preemptive tasks on uniprocessor 
systems that support dynamic speed adjustment. Al-
though a large body of past research has focused on 
preemptive scheduling [4], [20], [24], non-preemptive 
scheduling is still attractive in different application 
domains where properties of device hardware and 
software make preemption either prohibitively expen-
sive or just impossible [9]. Despite their inherent li-
mitations, non-preemptive scheduling algorithms are 
easier to implement, have lower run-time overhead, 
require less memory, and eliminate the need for syn-
chronization and its associated overhead 
[9],[11].Non-preemption also helps preserve program 
locality, which in turn makes programs more amena-
ble to worst-time execution time analysis [22]. As a 
result, non-preemptive scheduling has been adopted in 
many avionics applications [17] as well as in embed-
ded systems, particularly in small embedded devices 
with limited memory capacity [2], [15], [18], [30].  
To the best of our knowledge, no specific algo-
rithm has been developed for scheduling of fully 
non-preemptive fixed priority tasks for energy mini-
mization under real-time constraints in the past. The 
most relevant ones are the dual-speed (DS) algorithm 
due to Zhang and Chanson [32] and the uniform 
slowdown algorithm with frequency inheritance 
(USFI) proposed by Jejurikar and Gupta [12]. Both 
algorithms are designed for tasks that need synchro-
nization and hence have critical sections. In this paper, 
we propose ISA (Individual Speed Algorithm), a nov-
el scheme that computes one speed for each individual 
task in a non-preemptive task set. We address the 
challenging issue of figuring out the exact workload of 
higher-priority tasks on each non-preemptive task. 
The exact workload enables us to calculate optimal 
processor speeds which give optimal energy con-
sumption while satisfying the given timing constraint. 
ISA achieves considerable energy savings compared 
to existing methods with comparable quality, particu-
larly at high processor utilization. Another scheme we 
devise to co-work with ISA is selective frequency 
inheritance (SFI) which efficiently determines if pro-
cessor speedup can be disabled without jeopardizing 
any task deadlines whenever a task is blocked. We 
show that further improvement (20% to 30% reduc-
tion in energy consumption) can be obtained when 
ISA is combined with the SFI policy, at the cost of 
slight runtime overhead. Considering that the task 
actual execution time is usually less than its worst 
case execution time (WCET), we further propose a 
dynamic slack reclamation scheme based on ISA, 
which is named ISA-DR, for more energy savings. 
Experimental study shows that up to 30% energy sav-
ings can be achieved by ISA-DR compared to ISA. 
Besides their primary role in reducing processor 
energy consumption for real-time applications, we 
also demonstrate an additional use of our algorithms 
to trade off energy consumption against transmission 
delay in saving communication energy.  
The remainder of this paper is organized as fol-
lows: Section II briefly discusses related work. Sec-
tion III describes task and power models along with 
some assumptions we make. Section IV details the 
motivation and design of the ISA algorithm. In the 
same section, the SFI policy applied to ISA is also 
introduced. The experimental results are discussed in 
Section V. Finally, Section VI concludes the paper 
with some remarks. 
II. RELATED WORK 
Power consumption has become a critical prob-
lem in embedded and mobile systems with real-time 
constraint in recent years. Researchers have tackled 
energy reduction on a processor with variable speeds 
for both periodic and aperiodic real-time tasks [4], 
[20], [24], [29], as well as tasks with critical sections 
[12], [32], scheduling using a hybrid of the slowdown 
and shutdown strategies [13], [21], energy reduction 
based on slack reclamation [4], [10], [19], ener-
gy-aware scheduling with reliability requirements 
[33]–[35] (or on fault-tolerance systems [14]), ener-
gy-efficient real-time scheduling on wireless networks 
[6], [23], multiprocessor energy-efficient scheduling 
[1], [5], and energy-aware scheduling at a broader 
system level [3], [36]. Note that all the aforemen-
tioned studies focused on preemptive scheduling. 
Hong et al. [8] first reported superior results 
yielded by non-preemptive scheduling policies on 
power minimization of variable-voltage core-based 
systems. The authors proposed a synthesis approach 
that explores static scheduling algorithms, determines 
the cache size and configuration, and selects the pro-
cessor core. The effectiveness of the approach was 
demonstrated on a variety of industrial-strength mul-
timedia and communication applications. Zhang and 
3 
 
case Pd (f)/f is a convex function where fee has the 
minimum Pd (f)/f in the range of [fmin, fmax], then we 
have to set fmin as fee to ensure that only energyeffi-
cient operation frequencies are selected. 
 
IV. INDIVIDUAL-SPEED ALGORITHM 
Section IV-A introduces the motivation of ISA. 
We describe the design detail of ISA in Section IV-B. 
Section IV-C considers a policy, namely SFI, to re-
duce the frequency of processor speedup at task 
blocking times with ISA. Remarks for practical sys-
tems and a note on DS are presented in Section IV-D. 
 
A. Motivation 
Algorithm DS [32] computes two speeds for the 
task system consisting of periodic tasks with 
non-preemptive critical sections that are executed on a 
variable-speed uniprocessor, while algorithm USFI 
[12] determines a uniform slowdown for each task in 
the task set. Both DS and USFI can be applied to 
non-preemptive scheduling by treating each task as 
one critical section. However, both approaches would 
overestimate preemption times on non-preemptive 
tasks, and hence, result in task slowdown factors 
higher than necessary. We will give an example later 
to illustrate this problem. Since DS only computes two 
static speeds for the entire task set and the improve-
ment on it is trivial, a short discussion of this algo-
rithm is postponed to Section IV-D. In the following, 
we first review how USFI works, and then present an 
example to illustrate why USFI is not energy-efficient 
for fully nonpreemptive task set. 
USFI iteratively computes slowdown factors for 
all tasks, from highest to lowest priority. An index q, 
initialized to 1, is used to record the fact that at any 
instant in time the slowdown factors for tasks τ1 to τq-1, 
denoted η1, η2,…, ηq-1 ,have been determined. During 
each iteration, it first finds candidate slowdown fac-
tors for tasks from τq to τn, i.e., ηq , ηq+1 ,…, ηn. Sup-
pose ηm(q ≦ m ≦ n) is the maximum of ηq , ηq+1 ,…, 
ηn. USFI selects ηm as the slowdown factor for all 
tasks from τq to τm. It then updates q to be m+1 and 
continues the next iteration until all n tasks have 
slowdown factors determined. When determining 
candidate slowdown factor ηi for each task in 
{τq ,τq+1,…,τn} at each iteration, USFI uses the fol-
lowing time-demand-based feasibility test (a direct 
extension of the schedulability test in [26]): 
� �
Cr
ηr1≤r<𝑞
�
Si,jTr �� + 1ηi,j �Bi + � Cpq≤p<𝑖 �Si,jTp�� = Si,j 
(1) 
Si,j is a scheduling point of task τi and τi,j is the can-
didate slowdown factor corresponding to Si,j . Si,j is a 
member of Si  defined as:  S𝑖 = {(t ∈ S) ∧ (t < D𝑖) ∪ (D𝑖) ,where S = {𝑘T𝑗|𝑗 =
1, … , 𝑖; 𝑘 = 1, … , [Ti/Tj]}. B𝑖 = 𝑚𝑎𝑥𝑗∈𝑙𝑝(𝑖){𝐶𝑗} is the 
blocking factor due to nonpreemptability, where lp(i) 
is the set of tasks having lower priority than τi’s. 
When there are more than one scheduling point for 
task τi, i.e., more than one candidate slowdown factor, 
to achieve the best energy savings, USFI chooses the 
minimum candidate slowdown factor as the slowdown 
factor for τi, i.e, ηi = minj(ηi,j). 
Whenever blocking occurs, USFI adopts the 
frequency inheritance (FI) policy to change the pro-
cessor speed immediately by employing the slowdown 
factor of the blocked job in order to ensure task dead-
lines. In all the other cases, each job will execute at its 
slowdown factor to save energy. Note that the slow-
down factors computed by USFI are in a nonincreas-
ing order. 
The following example explains why USFI has 
room for improvement when applied to fully 
non-preemptive task scheduling, and motivates us to 
push forward. 
Example 1. Consider the following three periodic 
tasks τ1 =(5, 5, 1), τ2 = (10, 10, 2), τ3 = (20, 20, 1) 
specified with their periods, relative deadlines, and 
worst-case execution times. The slowdown factors 
computed by USFI are η1 = 0.6, η2 = 0.45 and η3 = 
0.225, respectively. 
Now we look into how USFI works. Suppose 
we have finished the first iteration of the algorithm 
and determined η1 to be 0:6, and we continue to de-
termine η2  at the second iteration. Consider how 
USFI determines η22 for S22 (one of the scheduling 
points of τ2) by applying Equation (1) to get the fol-
lowing equation: 10.6 �10T1 � + 1η22 (1 + � Cp[10Tp ]) = 102≤p≤2       (2) 
In (2), the workload from τ1 has been counted 
for 2 times(i.e.,[10
5
]), which means that in the worst 
case τ1 can execute twice before τ2 starts its execution. 
But actually, this is an overestimation. To see this, we 
assume τ1,1 and τ2,1  arrive at time t = 0 while τ3,1 
arrives just a little earlier. It is not hard to see that this 
is a case when τ2 has its longest response time. Initial-
ly, USFI schedules τ3,1 to execute. τ1,1 and τ2,1 arrive 
immediately while τ3,1 is executing. Since τ1,1 (as well 
as τ2,1) is blocked, the processor speed is increased to 
η1 =0.6. At t = 53 , τ3,1  is finished, and τ1,1  begins 
execution still at speed η1 =0.6. At t = 103  , τ1,1 finishes 
and τ2,1 finally gets its chance to execute at speed η2. 
Fig. 1 shows the schedule during the first 10 time 
units (the lifetime of τ2,1). 
 
 
 
ENERGY EFFICIENT SCHEDULING IN NON-PREEMPTIVE SYSTEMS WITH REAL-TIME CONSTRAINTS 5
replace η˜i by ηi,k and Si,j by Si,k in (5). Hence, the new slow-
down for τi can be calculated simply as MinSi,k∈Si(ηi,k).
Since Si ⊆ Si, this new slowdown factor for τi is certainly no
smaller than η˜i, which is an intermediate slowdown computed
by USFI based on Si. On the other hand, by comparing
Equation (3) and (6), it is obvious that the computed value
ηi by using our approach is not larger than η˜i. Combining
these two facts, the lemma follows.
Note that using ηi,k in (6) as τi’s slowdown factor can
guarantee τi’s deadline, but Inequality (5) may no longer hold
if we replace η˜i by ηi,k and Si,j by Si,k in (5), which means
τi may not begin execution some time before Si,k. This is
not permissible since in non-preemptive scheduling, once the
task instance has seized the control of processor, it will not
release it until it finishes its execution. So if τi does not start its
execution before Si,k, the slowdown factor calculated by (6),
which uses Si,k to bound the span of higher priority tasks,
cannot guarantee that τi will meet its deadline. To address
this problem, consider the following ratio:
Bi +
∑
q≤p<i Cp
⌈
Si,k
Tp
⌉
Si,k −
∑
1≤r<q
Cr
ηr
⌈
Si,k
Tr
⌉ (7)
Based on our definitions of Si and Inequality (5), η˜i is larger
than the value in (7). We choose a value that is a little larger
than (7), but is smaller than η˜i. We call this value ηi,k. Clearly
Inequality (5) still holds when we replace η˜i by ηi,k in (5). We
let η′i,k = Max(ηi,k, ηi,k). Now we are sure that using η
′
i,k
as τi’s slowdown factor can not only meet τi’s deadline, but
also ensure τi’s kick-off before Si,k. In order to achieve better
energy savings we let τi’s new candidate slowdown factor ηi =
MinSi,k∈Si(η
′
i,k). Finally, Lines 12-15 of Algorithm 1 further
choose the maximum candidate slowdown factor from among
ηq, . . . , ηn as the final slowdown factor for tasks τq, . . . , τm.
We present an example to illustrate our approach.
Example 2. We compute new slowdown factors for the task
set given in Example 1. The same as USFI, ISA involves
three iterations of computation, and the calculation details are
shown in Table I. Since the slowdown factor for τ1 computed
by ISA is the same as that of USFI, i.e., η1 = 0.6, we omit to
show the detail of computing η1. Now we come to the second
iteration, with η1 = 0.6, we continue to find η2 and η3. By
using Equations (3) and (4) with q = 2, we obtain η˜2 =
0.45 and η˜3 = 0.375. For task τ2, Inequality (5) holds at
S2,1 = 5 and S2,2 = 10, hence S2 = {S2,1(5), S2,2(10)}.
We go on to find η2,1 = 0.36, η2,1 = 0.3001, η2,2 = 0.45,
and η2,2 = 0.15001. Hence, η2 = Min(η
′
2,1, η
′
2,2) = 0.36.
Following the same steps, we get η3 = 0.3001. Since η2 > η3,
the second iteration is concluded with η2 = 0.36. Finally,
our third iteration determines η3 to be 0.09. In summary, the
new slowdown factors computed by our method are η1 = 0.6,
η2 = 0.36, and η3 = 0.09, respectively.
Now we need to be sure that all task deadlines can still be
met with our new slowdown factors. We first introduce two
important facts. From the computation process of ISA, it is
not difficult to derive the following fact.
TABLE I
CALCULATION OF OUR NEW SLOWDOWN FACTORS FOR THE TASK SET
GIVEN IN EXAMPLE 1 IN THREE ITERATIONS.
Iter. τ1 τ2 τ3
η˜1 = 0.6 η˜2 = 0.5 η˜3 = 0.45
S1 = {S1,1}1st
η1 = 0.6
η1 = 0.6
η˜2 = 0.45 η˜3 = 0.375
S2 ={S2,1, S2,2} S3={S3,2, S3,4}2nd
η2 = 0.36 η3 = 0.3001
η1 = 0.6 η2 = 0.36
η˜3 = 0.45
S3 ={S3,2, S3,4}3rd
η3 = 0.09
η1 = 0.6 η2 = 0.36 η3 = 0.09
Lemma 2. Given the tasks are ordered in a non-decreasing
order of their periods, the new slowdown factors computed by
ISA are in a non-increasing order.
Proof: This property is a direct result of how USFI
determines slowdown factors for unassigned tasks during each
iteration: find the task with the maximum candidate slowdown
factor among all unassigned tasks, and assign its slowdown
factor value, say ηm, to the unassigned tasks up to index m.
This process is repeated until all tasks have their slowdown
factors assigned. Because this process of determining slow-
down factors is also employed in our approach, the lemma
immediately follows.
With our new slowdown factors computed, the run-time
scheduler will use them to control the processor speed at
different points in time so as to reduce energy consumption,
while still guaranteeing all task deadlines. In this paper, we
consider two policies to control the processor speed: (1)
Frequency inheritance (FI) [12]: When blocking occurs, if
the blocked job has a higher slowdown factor, this slowdown
factor is inherited by the blocking job. In all other cases, each
job will execute at its corresponding slowdown factor to save
energy. (2) Selective frequency inheritance (SFI). We consider
FI in this section, and will discuss SFI in Section IV-C. By
Lemma 2 and the FI policy, we have the following fact.
Lemma 3. During the time period when τi is executing, the
minimum slowdown of the processor is ηi.
With the FI policy, we prove that the slowdown factors
computed by ISA can guarantee all task deadlines, as stated
below.
Theorem 1. Based on the DM scheduling policy, the slow-
down factors computed by ISA along with the FI policy of
controlling the processor speed guarantee all task deadlines.
Proof: Our approach assigns every task a slowdown factor
that is larger than or equal to the slowdown values given in
(6) and (7). Based on this, we know that for every task τi,
i = 1, . . . , n, there exists a scheduling point Si,k such that the
following two inequalities hold:
1
ηi
Bi +
∑
1≤j<i
Cj
ηj
⌈
Sik
Tj
⌉
+
1
ηi
Ci ≤ Di (8)
7 
 
 
SFI policy at time t2, we need to consider the follow-
ing two situations at time t1: 
 τb inherits ?̅?𝑖  at time t1: During [t1,t2], the 
processor speed is certainly no less than ?̅?𝑖. 
No matter whether the SFI policy promotes 
the processor speed at time t2 or not, the pro-
cessor speed is still no less than ?̅?𝑖  during 
[t2,t3]. Hence, t3 - t1 is upper bounded by 
𝐵𝑖
𝜂�𝑖
. 
 Frequency inheritance did not occur at time t1: 
This means that at time t1, Rb ≤  𝐵𝑖𝜂�𝑖 . During 
[t1,t2], the processor speed is certainly no less 
than the speed at time t1. Again, no matter 
whether the SFI policy promotes the proces-
sor speed at time t2 or not, the processor 
speed during [t2,t3] is no less than the speed at 
time t1. Hence,we know 𝑡3 − 𝑡1 ≤  𝐵𝑖𝜂�𝑖. 
Based on the above facts and our arguments given 
in the proof of Theorem 1, τi can still finish its execu-
tion before t1 + Si,k, and hence its deadline. 
As for τa, i.e., the newly blocked task when the SFI 
policy is applied, it is not hard to see that its maxi-
mum blocking time is bounded by 𝐵𝑎 𝜂𝑎� . The work-
load on τa from higher priority tasks remains the same. 
By this fact, Lemma 4, and Theorem 1, we thus have 
the following theorem. 
Theorem 2. Based on the DM scheduling policy, the 
slowdown factors computed by ISA along with the 
SFI policy can guarantee all task deadlines. 
Fig. 2(c) shows the schedule produced by the 
SFI policy for the task set given in Example 1. At 
time 3, τ1,1 arrives while τ3,1 is running at speed ?̅?2 = 
0.36. At this time instant, τ3,1 needs at most 0.53 time 
units to complete at its current speed, whereas τ1,1 can 
tolerate a maximum blocking of 𝐵1 ?̅?1� = 2 0.6� . 
Based on Theorem 2, τ3,1 can keep running at speed 
0.36 without jeopardizing the deadline of τ1,1. Like-
wise, τ2,1 can continue execution at speed ?̅?2 at time 8. 
In this case, the energy consumed by our SFI policy is 
2.48 in the first 20 time units, which is 13.8% im-
provement over USFI. Our experimental results in 
Section VI show that the SFI policy gives us a large 
gain in energy savings under a variety of parameter 
settings. Note that the overhead involved with SFI is 
small — the scheduler only needs to record the pro-
cessor time spent on executing each active job, and 
whenever blocking occurs it performs a simple com-
putation and test, as described in Lemma 4, to deter-
mine if the processor speed must be increased. 
D. Remarks for Practical Systems and DS 
Processors with discrete speeds: For commer-
cial processors, only discrete speeds might be availa-
ble. To address this problem, we can first derive a 
solution by assuming continuously available speeds as 
the approaches in Section IV-B, and then use some 
available speeds to approximate the speeds determined 
by ISA. For example, we first compute a speed for τ1 
by ISA. If this speed is unavailable, we then choose 
the next higher speed (also called inflated speed) 
which is available and assign it to τ1. After that, we 
utilize the inflated speed to compute slowdown factors 
for the remaining tasks τ2,…,τn and choose the next 
higher available speeds for them, in a similar way. 
Note here our method is similar to the one adopted by 
the PM-Clock algorithm proposed in [24], which fo-
cuses on fixed priority preemptive scheduing. 
Non-negligible frequency transition overhead: It 
is now a common knowledge that changing from one 
frequency level to another takes a fixed amount of 
time (denoted ∆t, ranges from tens of microseconds to 
tens of milliseconds), referred to as the transition (or 
switch) overhead [16]. When frequency transition 
overhead is large enough that they cannot be ignored, 
we need to recompute the slowdown factors by consi-
dering such overhead. From the computation process 
of ISA, it can be observed that for task τi, the number 
of frequency switches, say λ, is bounded by the num-
ber of jobs with higher priorities than τi, plus two ad-
ditional possible switches, one for τi itself (when the 
system state is changed from idle to executing τi, or 
conversely), and the other for a lower priority block-
ing job. Specifically, we have λ =  ∑ [Si,j Tr� ]1≤r<q +2 
for formulas (3) and (5), and λ =  ∑ [Si,k Tr� ]1≤r<q +2 for 
(6). Hence, if the transition workload is non-negligible, 
we should add λ∆𝑡 into the left-half-part of (3), (5) 
and (6) to compute ?̅?𝑖 . Note here our method for 
counting transition workload is a conservative one 
since it considers the worst case scenario, but it is also 
a safe one as it suffices to guarantee schedulability. 
Besides time overhead, transition also incurs energy 
overhead, hence, it is important to guarantee that the 
energy saved by slowdown policy must be no less 
than the increased energy consumption resulted from 
the transition overhead. Otherwise, the slowdown 
policy is meaningless. However, since the actual 
number of frequency transition may change during 
run-time and depends on the actual execution cycles 
of each job, it is hard to decide a situation where the 
energy saved by slowdown is less than the energy 
consumption by frequency transition. Because the 
focus of this paper is handling time overhead while 
meeting deadlines, we leave the problem of tackling 
the energy overhead for future work. 
Refinement on DS: Even though we only 
present algorithm ISA for deciding one individual 
speed for each task, it is also applicable to refine algo-
rithm DS [32]. A simple yet efficient refinement 
would be choosing the highest speed by ISA as the 
high speed in DS. This certainly will guarantee the 
schedulability, while energy reduction can also be 
achieved comparing to DS. Since the refinement is 
trivial and straightforward, we do not detail it in this 
work. 
9 
 
 
 
in a non-increasing order, since the highest-priority 
task in a task set in these two approaches has the larg-
est slowdown factor and executes and invokes speed 
promotion most frequently, it is natural to conjecture 
that energy savings would be more significant as the 
difference between the maximum slowdown factors 
computed by USFI and ISA-FI gets larger. This trend 
is indeed reflectedin Figs. 5 and 6. Both figures show 
that the performance gap between USFI and ISA-FI is 
wider at a larger value of SFr. On average, USFI 
consumes 8% to 15% more energy than ISA-FI. The 
energy reduction achieved by ISA-FI, relative to USFI, 
mainly comes from its lower slowdown factors, since 
both ISA-FI and USFI use the same speed-control 
policy. 
(3) ISA-SFI performs better than all other ap-
proaches in nearly all cases. Like ISA-FI, as the pro-
cessor utilization becomes larger, the energy reduction 
achieved by ISA-SFI becomes more. When the pro-
cessor utilization reaches 60%, ISA-SFI consumes, on 
average, 32% and 27% less energy than 
USFI when η𝑒𝑒 = 0.29 and 0.37, respectively. This 
is because at high processor utilization, tasks tend to 
experience more blocking, and the benefit of SFI be-
comes obvious. Unlike ISA-FI, the change of SFr 
does not affect the magnitude of energy reduction by 
ISA-SFI, as shown in both Figs. 5 and 6. This phe-
nomenon also explains ISA-SFI’s primary energy 
savings come from the SFI policy, rather than its 
smaller slowdown factors. 
(4) The effect of increasing the proportion of fre-
quency independent power in our energy consumption 
formula is clearly seen from Figure 5 to Figure 6. 
When the frequency independent power increases in 
the formula, our approach becomes less energy effi-
cient. However, we can still achieve 5% to 10% more 
energy savings, compared to USFI, when η𝑒𝑒 
changes from 0.29 to 0.37. 
2) Task Granularity: In the second set of experi-
ment, we generate two types of task set, namely 
fine-grained task set and coarse-grained task set, to 
examine how different approaches fare in the energy 
consumption under different task granularity. In the 
fine-grained task set, one task is generated from the 
long period and one is selected from the medium pe- 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
riod, while the others are randomly generated from the 
short period. In the coarse-grained task set, one task is 
selected from the short period and one is from the 
medium period, while the others are randomly from 
the long period. In this simulation, all tasks are also 
assumed to execute up to their WCETs and η𝑒𝑒  is 
fixed to be 0.29. The system utilization is varied from 
10% to 60%. It can be observed from Figure 7.(a) that 
for coarse-grained task set, ISA-SFI consistently out-
performs the other three schemes on energy consump-
tion performance, except when the utilization is below 
15%, all the four schemes have the same energy con-
sumption, due to that the slowdown factors computed 
by every algorithm are all less than the threshold fre-
quency η𝑒𝑒  and they are set to be η𝑒𝑒. Moreover, it 
can be seen that the gap between ISA-SFI and the 
other three approaches is increasing with the growth 
of the utilization. This is because at high utilization, 
tasks tend to experience more blocking, and the bene-
fit of SFI becomes obvious. For the fine-grained task 
set, ISA-SFI also outperforms the other three schemes, 
as shown in Figure 7.(b). However, the difference 
between ISA-SFI and ISA-FI is not that large as in the 
coarse-gained task set. The reason is that for 
fine-grained task set, the slowdown factors computed 
are more close to each other than in the coarse-grained 
case. In fact, in our experiment we observe that most 
of the task slowdown factors computed are the same 
for the fine-grained task set, hence the energy savings 
from the SFI policy is not that significant as in the 
coarse-grained task set. 
3) Slack Factor: In this set of simulations, we ex-
amine the additional energy gains derived through the 
dynamic slack reclamation policy. For simplicity, we 
only compare the performance of the following algo-
rithms: ISA-FI, ISA-SFI and ISA-DR. We define a 
notation namely slack factor (sf ) to specify the dif-
ference between the actual case execution time 
(ACET) and the task’s WCET. Specifically, the 
ACET of the tasks are uniformly distributed 
tween(1 −  𝑠𝑓) × WCET and the task’s WCET. η𝑒𝑒 
is fixed to be 0.29. To facilitate comparison, the ener-
gy consumed by ISA-FI is used as the baseline, and 
the energy consumed by each other solution is norma-
lized against the baseline. Figure 8 shows the simula-
11 
 
ing for low-power embedded operating systems. In Proc. of 
SOSP, pages 89–102, 2001. 
[20] G. Quan and X. Hu. Minimum energy fixed-priority scheduling  
for variable voltageprocessors. In Proc. of DATE, pages 
782–787, 2002. 
[21] G. Quan, L. Niu, X. Hu, and B. Mochocki. Fixed priority  
scheduling for reducing overall energy on variable voltage 
processors. In Proc. Of IEEE Real-Time Systems Sympo-
sium, pages 309–318, 2004. 
[22] H. Ramaprasad and F. Mueller. Tightening the bounds on  
feasible preemption points. In Proc. of IEEE Real-Time 
Systems Symposium, pages 212–224, 2006. 
[23] X. Ruan, S. Yin, A. Manzanares, M. Alghamdi, and X. Qin. A  
messagescheduling scheme for energy conservation in mul-
timedia wireless systems. IEEE Transactions on Systems, 
Man and Cybernetics, Part A: Systems and Humans, 
(99):1–12, 2011. 
[24] S. Saewong and R. Rajkumar. Practical voltage-scaling for  
fixed-priority rt-systems. In Proc. of IEEE Real-Time and 
embedded Technology and Applications Symposium, pages 
106–115, 2003. 
[25] C. Schurgers, V. Raghunathan, and M. Srivastava. Power  
management for energy-aware communication systems. 
ACM Transactions on Embedded Computing Systems 
(TECS), 2(3):431–447, 2003. 
[26] L. Sha, R. Rajkumar, and J. Lehoczky. Priority inheritance  
protocols: an approach to real-time synchronization. IEEE 
Transactions on Computers, 39(9):1175–1185, 1990. 
[27] V. Swaminathan and K. Chakrabarty. Real-time task schedule 
ing for energy-aware embedded systems. Journal of the 
Franklin Institute, 338(6):729–750, 2001. 
[28] V. Swaminathan, C. Schweizer, K. Chakrabarty, and A. Patel.  
Experiences in implementing an energy-driven task schedu-
ler in RT-linux. In Proc. of IEEE RTAS, page 229, 2002. 
[29] F. Yao, A. Demers, and S. Shenker. A scheduling model for  
reduced CPU energy. In Proc. of the Annual Symposium on 
Foundations of Computer Science, pages 374–382, 1995. 
[30] G. Yao, G. Buttazzo, and M. Bertogna. Bounding the maxi 
mum length of non-preemptive regions under fixed priority 
scheduling. In Proc. of RTCSA, 2009. 
[31] F. Zhang and S. Chanson. Processor voltage scheduling for  
real-time tasks with non-preemptible sections. In Proc. of 
the 22nd IEEE Real- Time Systems Symposium, pages 
235–245, 2002. 
[32] F. Zhang and S. Chanson. Blocking-aware processor voltage  
scheduling 
for real-time tasks. ACM Transactions on Embedded Com-
putting Systems, 3(2):307–335, 2004. 
[33] Y. Zhang and K. Chakrabarty. Dynamic adaptation for fault  
Tolerance and power management in embedded real-time 
systems. ACM Transactions on Embedded Computing Sys-
tems, 3(2):336–360, 2004. 
[34] D. Zhu and H. Aydin. Reliability-aware energy management  
for periodic real-time tasks. In Proc. of IEEE RTAS, pages 
225–235, 2007. 
[35] D. Zhu, R. Melhem, and D. Mossé. The effects of energy  
management 
on reliability in real-time embedded systems. In Proc. of 
ICCAD, pages 35–40, 2004. 
[36] J. Zhuo and C. Chakrabarti. System-level energy-efficient  
dynamic task scheduling. In Proc. of DAC, pages 628–631, 
2005. 
processing 之論文“CkNN Query Processing over Moving Objects with Uncertain 
Speeds in Road Networks＂，得到一些同行之精闢建議。另外，當然也聆聽其他
作者之大作，藉以了解最新的研究趨勢。 
本次會議有 3 場 keynotes 令我獲益良多。第一場由伊利諾大學教授 Philip S. 
Yu 主講 “Information Networks Mining and Analysis.” 主要內容是: With the 
ubiquity of information networks and their broad applications, there have been 
numerous studies on the construction, online analytical processing, and mining of 
information networks in multiple disciplines, including social network analysis, 
world-wide web, database systems, data mining, machine learning, and networked 
communication and information systems. Moreover, with a great demand of research 
in this direction, there is a need to understand methods for analysis of information 
networks from multiple disciplines. In this talk, the speaker presented various issues 
and solutions on scalable mining and analysis of information networks. These include 
data integration, data cleaning and data validation in information networks, 
summarization, OLAP and multi-dimensional analysis in information networks. 
第二場由武漢大學教授 Deyi Li 主講 “iMiner: From passive searching to active 
pushing.” 主要內容是 : The success of a search engine in cloud computing 
environment relies on the numbers of users and their click-through. If we take the 
previous search key words as tags of users to study and differentiate the user 
interaction behaviors, the search engine is able to actively push related and useful 
information to users based on their previous actions instead of passively waiting for 
users’ queries. The talk tries to answer the following questions: Is there any statistical 
property almost independent to search key words? How to push recommendation 
based on the queried key words? And how to extract user behavior models of 
searching actions in order to recommend the information to meet users’ real needs 
more timely and precisely? The speaker takes the do-the-best strategy with uncertainty. 
Statistics play a great rule here. From the statistical point of view there are two very 
important distributions: the Gauss distribution and the power-law distribution. The 
speaker has studied an interesting cloud model to bridge the gap between the two by 
using 3 mathematical characteristics: expectation, entropy and hyper entropy. 
第三場由 Aarhus 大學教授 Christian S. Jensen 主講 “On the querying for 
places on the mobile web.” 主要內容是: The web is undergoing a fundamental 
transformation: it is becoming mobile and is acquiring a spatial dimension. Thus, the 
web is increasingly being used from mobile devices, notably smart phones, that can 
be geo-positioned using GPS or technologies that exploit wireless communication 
networks. In addition, web content is being geo-tagged. This information calls for new, 
spatio-textual query functionality. The talk touches on the following questions: How 
 計畫編號 NSC 99-2221-E-006 -124 
計畫名稱 在可變動速度處理器上之非搶佔式即時排程 
出國人員姓名 
服務機關及職稱
徐立群 
國立成功大學會計學系 教授 
會議時間地點 
2011 年 9 月 14 日至 2011 年 9 月 16 日 
中國西安 
會議名稱 International Conference on Signal Processing, Communications, and 
Computing, ICSPCC 2011 
發表論文題目 僅出席會議 
 
一、目的 
ICSPCC 2011 是由國際著名資訊電子協會 IEEE 主辦，其目標是提供全世界從事
資訊、通訊與信號處理研發工作之產學研界一個討論創新想法(包括理論與實務)
的一個平台。此次會議在西安舉辦，有許多單位協辦，包括 IEEE Xian section, 
IEEE Hong Kong Section, Northwestern Polytechnical University, Xidian 
University, Xian Jiaotong University, University of Hong Kong, 
Polytechnical University of Hong Kong, City University of Hong Kong, 
Chinese University of Hong Kong 等。 
本次共有 486 篇論文投稿至 ICSPCC 2011 會議，來自於 12 國 (包括成功大學電
機系的王駿發教授等)，最後有 275 篇論文收錄至會議論文集，接受率約 56%; 每
篇會議論文至少有 2 位 technical program committee 之委員 review，以確保論文品
質。因為會議集由 IEEE 出版，故可以用 IEEE Explore 方便閱讀。會議共分為 40 
個 regular sessions，以及數個special sessions。Special sessions 包括 computational 
biology, underwater acoustic communication, speech signal processing, cognitive radio 
and networks, intelligent life, MIMO communication and signal processing, 內容可說
非常豐富。 
   本次會議共有 4 個 keynote speech: (1) Subcarrier coding: a fast coordination 
研究課題，如多媒體訊號呈現、處理、編碼資料探勘與知識發現等。多媒體的內
容一般皆是 unstructured 的，因此探勘的問題特別複雜且富挑戰性。雖然個人在資
料探勘方面的研究，主要是專注在 structured data 上，然而這場 keynote speech
帶給我許多的啟發，講解者介紹幾項針對 unstructured data mining 的技術上，初
步看起來也可以用到 structured data mining 上，個人將持續此方向的探索，期
望能得到良好的成果。 
 另一場論文報告「Optimizing the battery energy efficiency in wireless sensor」討論
如何針對由 nonrenewable 電池推動的感測網路節點之電池節能效率。此篇論文對
於電池能耗提供很好的 overview，它對於我在即時系統節能排程的研究上，應能
提供一些創新想法。 
 另一項對於個人有相當程度的啟發的演講是由成功大學王駿發教授主講「Smart 
Living, Orange Computing & Cloud Computing」。由於世界人口增加與年老化，地
球天然資源逐漸耗盡，科技研發必須探索如何讓我們的生活環境與地球生態維繫
下去。王教授提出的 Orange Computing 著眼於人文照顧，專注於為人類身心靈平
衡設計演算法與系統。這項演講相當提升我的研究眼界，對未來研究方向之制定
很有助益。此次參加會議遇到台灣光寶科技研發部許經理。因他在業界有相當豐
富的經歷，本身亦具有博士學位，故相當熟悉科技產業發展趨勢。個人希望能跟
他有更多的機會進行研發方向上的討論，未來能進行一些具有商品化潛力的研究。 
 「Network survivability against region failure」由 Xiaohong Jiang 演講，他提到通
訊網路目前面臨愈來愈多大範圍的威脅，如自然災難與實體攻擊。當這樣的區域
問題發生時，許多地理相關區域的網路元素可能同時被毀，造成整體網路連接都
可能受影響甚至完全停擺。Prof. Jiang 探討 network survivability against region 
failure，將問題 model 成網路上的 circular cut. 他檢視在 region-disjoint 與
node-disjoint 之 region failure 情形下，network throughput performance degrade 的情
形。 
除了聆聽許多精彩的演講，我也與同行交流彼此的研究，包括大會主席 Prof. 
Dear LihChyun Shu, 
Thank you for your submission to APWeb 2011. On behalf of the Programme Committee, we are 
delighted to inform you that your paper 109 - CkNN Query Processing over Moving Objects with 
Uncertain Speeds in road networks has been accepted as a full research paper and for presentation at the 
conference. Congratulations!
You can view the reviews by logging in at http://cmt.research.microsoft.com/APWEB2011.
When preparing the final version of your paper, please address issues raised in the reviews. Instructions 
for preparing the camera-ready copy can be found at 
http://deke.ruc.edu.cn/apweb2011/page/papersubmission.htm. The deadline for camera-ready copy is Jan. 
22th, 2011.
This year we received 104 submissions. The Programme Committee has worked very hard to thoroughly 
review all the submitted papers, and has accepted 26 full papers and 11 short papers. The acceptance ratio 
for full papers is only 25%, which is very competitive for a regional conference. Congratulations for the 
acceptance of your paper!
You are also invited to give a demo of your results/system/tool at the conference. If you decide to accept 
the invitation, you may find details about preparing the demo from our website soon.
We look forward to seeing you in Beijing in April 2011.
Best regards,
Xiaoyong, Wenfei and Jianmin
頁 1 / 1
2012/1/30mhtml:file://C:\Users\Lin\Downloads\APWeb 2011_ Paper 109 Decision. .mht
of candidate moving objects.
Based on the two proposed distance functions, we present
a continuous UkNN (CUkNN ) query method, named
CUkNNMean, to continuously obtain the UkNN result of
query q during a given time period. In particular, the UkNN
result consists of k UkNN objects, each of which has a larger
probability of being one of the kNNs of query q than that of
any other objects not in UkNN result set. Our CUkNNMean
method consists of three stages, the pruning phase, the refining
phase, and the probability evaluation and ranking phase. In
the pruning phase, by calculating the pruning distance we
efficiently prune the objects that are impossible to be the kNN
result of query q and form the candidate set. Then in the
refining phase, by constructing the kNN-MaxD-PolyLine of
query q, we decide the possible kNN objects of query q at
time period [ts, te], depending on the candidate set determined
at the pruning phase. Then in the probability evaluation and
ranking phase, by introducing the MeanDq,o(t), the Mean
value of the distance between object o and query q at time t,
to represent the distance of each UkNN object, the probability
calculation is greatly simplified.
When updates of object speeds and directions occur, which
implicitly means that some objects change their moving
directions or their speeds go beyond the speed limits, a
straightforward way to derive the valid query result is to
re-execute the CUkNNMean algorithm which may greatly
degrade the performance of our method. Instead, we modify
the query result according to the speed and direction updates
of the candidate objects so as to keep the query result valid.
Moreover, by organizing the data structures in a novel and
neat way, we extend our method to handle CUkNN queries in
large networks with high efficiency. Especially, we can greatly
reduce disk access costs.
In summary, the main contributions of the paper are as
follows:
1) An uncertain network distance model is proposed to
represent the distance between a moving object and a
query, both of which move with variable speeds in a
road network.
2) A continuous UkNN query monitoring method, consist-
ing of three stages, i.e., the pruning phase, the refining
phase, and the probability evaluation and ranking phase,
is introduced to obtain the UkNN results at each
monitored time instant, where all moving objects and
queries move continuously in the road network with
uncertain speeds.
3) In the probability evaluation and ranking phase, by
introducing the mean distance of q and o within the
monitored time period, the calculation and ranking of
the probability of each candidate object being a kNN
of q are greatly simplified.
4) We address the issues of processing object updates,
instead of re-executing the CUkNNMean algorithm
when speed and direction updates of candidate objects
occur.
5) By organizing the data structures in a novel and neat
way, we extend our method to handle CUkNN queries
in large road networks with high efficiency and low disk
access costs.
6) We evaluate the performance of the CUkNNMean
method through a set of simulation experiments both
in a real road network and a synthetic 2-dimensional
grid network, and the performance result shows that our
method is efficient, precise and scalable.
The rest of the paper is organized as follows. Section II
reviews related work. Section III gives problem descriptions,
data structures used, and the network distance model proposed.
The algorithms of our CUkNNMean method are described in
Section IV and the extension of the proposed method to large
road networks is shown in Section V. Section VI gives the
experimental study. Finally, Section VII concludes the paper.
II. RELATED WORK
Jensen et al. [7] first addressed the problem of kNN
search in road networks in the year 2003. Since then, kNN
query processing in road networks has received widespread
attentions and a variety of algorithms are proposed. Section
II-A reviews the existing methods for processing kNN queries
in road networks, and Section II-B discusses the related work
that deals with CkNN queries in road networks. Finally,
Section II-C discusses the related works on kNN and other
similarity query methods in uncertain databases.
A. kNN Query Methods in Road Networks
Papadias et al. [8] presented an architecture that integrates
network and Euclidean information to process network-based
queries. They utilized the principle that, for any two objects
in the network, the network distance is the same as or larger
than the Euclidean distance between them. They proposed
two methods to process static kNN queries in network
spaces, which are incremental Euclidean restriction (IER) and
incremental network expansion (INE). We omit the discussion
about IER since INE algorithm outperforms IER. Starting
from the query point q, INE expands the network to search
for kNNs and examines the objects in the order they are
encountered. When the expansion radius is larger than the
distance between the kth-nearest neighbor object and query
point q, the expansion is terminated. The efficiency of INE
depends on the density of objects. If the objects are sparsely
distributed on a road network, the network expansion for
searching kNNs is expected to pass through many nodes in
the road network and the efficiency can be low.
To avoid on-line distance computation, Kolahdouzan et
al. proposed the Voronoi Network Nearest Neighbor (VN3)
method [9], which is based on network Voronoi diagrams.
In this method, the network Voronoi polygons (NVP) and
some network distances are pre-computed and the entire road
network is divided into a number of small regions which are
called cells. The first NN of any query q is the generator
of the NVP that contains q. If additional NNs are required,
the adjacent Voronoi polygons will be considered iteratively.
The VN3 can be efficiently used for networks that have a low
density. However, in high density networks, its efficiency can
be low since it includes constructing, storing and inquiring a
large number of Voronoi polygons to answer kNN queries.
2
n
14 9
n
13
n
1
n
2
n
3
n
4
n
7
n
4
8
10
12.5
19
12
n
11
n
10
n
8
n
9
5
8.5
15
6
18
4.5
10.5
18
4.5 10
6
6
n
6.55
(b) t=0.5
o6
o
3
o2
q
o
4
o5
o1
12
n
(a) t=0
13
n
6
q
1
n 2
n
3
n
4
n
7
n
6
4
9
12
11
20
11
n
10
n
8
n
8
5
8
16
6
18
4
12
18
4 10
8
e3
6
n
n14
o6
o1
o
2
o
5
o
3
o
4
9
n
5n5n
e1 e2
e4
e5 e3
e1 e2
e4
e5
(c) (d)
V.minobject V.max
O1 2         4
O2 -1  -2
O3 2         4
O5 -1  -2
O4 1         2
O6 1         3
q -1  -2
e
5
5
edge V.max
e
1
4
e
2
3
e3    5
e4 4
 Fig. 1. An example of a CUkNN query q and objects o1 thru o6 in a road network. (a) and (b) show the positions of the query and the objects at time = 0and 0.5 respectively. (c) shows the maximum and minimum speeds of each object. (d) shows the maximum speed of each relevant edge in the road network.
B. Data Structures
In this study, we use an undirected weighted graph to
represent a road network which includes an edge set and a
node set. The graph nodes and edges represent road nodes and
road segments respectively. A road node is the intersection
point of two or more road segments or the dead end of a
road segment. Each edge has a non-negative weight which
represents the cost of the road segment, e.g., the length of
a road segment or the time required to travel through the
road segment. We maintain a set of moving CUkNN queries
and moving objects in the road network. When we process
CUkNN queries in a road network, the distance we consider
is the distance in the road network, which we will describe
shortly below.
Four in-memory tables are constructed to store the infor-
mation about the edges and nodes of a road network, moving
objects and queries. The first is the edge table Tedge which
stores for each edge e: (1)its id (e.id), (2) its starting node
(e.snode), (3) its ending node (e.enode); (4) its weight (e.w);
(5) a set of identities for objects currently moving on edge
e (e.obj set); (6) the maximum speed limit of the edge
(e.vlimit). For the sake of ease in processing queries, we
assign an id for each node and assume that for the two nodes
of an edge e, the node with the smaller id is e.snode, and the
node with the larger id is e.enode. Henceforth, whenever we
refer to the position of a query or an object we mean the edge
e on which it travels and its distance from e.snode.
We use the node table Tnode to store for each node n: (1)
its id (n.id); (2) the set of edges connecting to n (n.eadj). The
third table we use is Tobj , which stores for each object obj:
(1) its id (obj.id); (2) the edge on which the object obj travels
(obj.e); (3) the distance from obj to the starting node of obj.e
(obj.dist); (4) the minimum speed of obj (obj.vmin); (5) the
maximum speed of obj (obj.vmax). We assume if obj moves
along the direction from the starting node to the ending node
of obj.e, its speed is positive; otherwise it is negative.
Lastly, we use the query table Tqry to store for
each query q: (1) its id (q.id); (2) its UkNN re-
sult within a certain time period (q.UkNN Final Set).
In particular, q.UkNN Final Set is a set of tuples <
[ti, tj ], {o1, o2, o3, ...} >, where o1, o2, and o3 are the objects
that are the probable kNNs of query q during the time period
[ti, tj], listed in descending order of probability values.
As object o and query q move with uncertain speeds in
the road network, their positions are not exactly determined
and this causes difficulties in calculating the distance between
q and o, especially when they move on two different edges.
However, based on the notion of distance determinate pro-
posed below, even if o and q move on two different edges, it
becomes much easier to calculate their distance in most cases.
Definition 1: Given two different edges ei and ej , if the
shortest distance between every pair of two points, one from
ei and another from ej , is always determined by the same
path, then we say ei and ej are distance determinate.
For example, e2 and e4 in Figure 1(a) are distance determi-
nate with a special path that starts and ends at the same node
n2, whereas e4 and e5 are distance indeterminate.
To determine whether a pair of edges ei and ej in a road
network is distance determinate, we use the following method.
Suppose that ei has two nodes ei.snode and ei.enode, and
ej also has two nodes ej .snode and ej .enode. The distance
between any pair of nodes ni and nj , d(ni,nj), can be pre-
computed and stored in the system. So d(ei.snode, ej .snode),
d(ei.snode, ej .enode), d(ei.enode, ej .snode) and d(ei.enode,
ej .enode) are available when determining whether ei and ej
are distance determinate. These four distance values can be
sorted in ascending order. Suppose these four distance values
after sorting are denoted as d1, d2, d3 and d4 respectively. It is
not hard to see that if the following three predicates all hold,
then ei and ej are distance determinate.
d2=d1 +min(ei.w, ej .w)
d3=d1 +max(ei.w, ej .w)
d4=d1 + ei.w + ej .w
To efficiently evaluate these predicates, we construct a
matrixDD, with each element having one of the following five
values: the value Ø means that ei and ej are distance indeter-
minate; otherwise, ei and ej are distance determinate and there
is a shortest path connecting ei and ej . Henceforth, we call
the two endpoints of the shortest path the connecting nodes.
There are four subcases: the value <0,0 > means these two
connecting nodes are ei.snode and ej .snode respectively; <-
1,-1> indicates that these two connecting nodes are ei.enode
and ej .enode respectively; <0,-1> indicates that these two
4
(b) The two edges q and o reside are
     distance indeterminate
nk nl
nm
nn
the shortest
path connecting
ei and ej
ei
ej
q
o Lo(t)lo(t)
lq(t) Lq(t)
(a) The two edges q and o reside are
     distance determinate
nk nl
nm nn
ei
ej
q
o
Lo(t)lo(t)
lq(t) Lq(t)
 
 Fig. 4. Illustration of MaxDq,o(t) and MinDq,o(t) when q and o move
on two different edges
Assume q is moving on the edge ei (nk, nl) and o is moving
on the edge ej (nm, nn). There are two subcases to consider:
(1) ei and ej are distance determinate. By Definition 1,
there is a shortest path connecting ei and ej . Without loss of
generality, we assume the two nodes connecting this shortest
path are nl and nn.
From Figure 4(a) we can see that no matter o and q
move toward or away from the shortest path connecting ei
and ej , MinDq,o(t) is equal to the sum of min(d(lq(t), nl),
d(Lq(t), nl)), DNl,n, and min(d(lo(t), nn), d(Lo(t), nn)),
where d(lq(t), nl) is the distance from lq(t) to nl,DNl,n is the
distance between nl and nn. Recall our use of the matrix DD
in which the entry DDij with < value0, value1 > is used to
represent different relations between ei and ej in terms of dis-
tance determinate. Note that when nl is ei.snode (the value0
of DDij is 0), d(lq(t), nl) = q.dist+q.vmin×(t−t0); when nl
is ei.enode (the value0 of DDij is -1), d(lq(t), nl) = ei.w−
(q.dist+q.vmin×(t−t0)). Thus d(lq(t), nl) can be uniformly
represented by |q.dist+ q.vmin × (t− t0) +DDij .value0 ×
ei.w|, no matter whether nl is ei.snode or ei.enode.
The distance functions of d(Lq(t), nl), d(lo(t), nn) and
d(Lo(t), nn) can be defined similarly. Moreover, MaxDq,o(t)
is equal to the sum ofmax(d(lq(t), nl), d(Lq(t), nl)), DNl,n,
max(d(lo(t), nn), d(Lo(t), nn)).
(2) ei and ej are distance indeterminate. Note that
the possible locations of q and o lie in two segments, i.e.,
(lq(t), Lq(t)) and (lo(t), Lo(t)), respectively. Thus as shown
in Figure 4(b), the shortest path from any point in these
two segments to the outside network should pass through
their terminals. Therefore, MinDq,o(t) must be the distance
between one terminal of (lq(t), Lq(t)) and one terminal of
(lo(t), Lo(t)). As both (lq(t), Lq(t)) and (lo(t), Lo(t)) have
two terminals, we need to take into account four network
distances, i.e., d(lq(t), lo(t)), d(lq(t), Lo(t)), d(Lq(t), lo(t)),
and d(Lq(t), Lo(t)). MinDq,o(t) is equal to the minimum of
these four distances.
When ei and ej are distance indeterminate, it is difficult to
calculate the exact value of MaxDq,o(t) due to the following
reasons: (1) it is time-consuming to find the two points, one
on the position segment of q and the other on the position
segment of o, where the distance between these two points
equals MaxDq,o(t); (2) these two points are changing as
time goes by. Thus we use an approximate value, denoted
app MaxDq,o(t), as a substitute for MaxDq,o(t).
We consider two requirements in our calculation of
app MaxDq,o(t). Firstly, existing CkNN methods in the
pruning phase usually use MaxDq,o(t) of kNN objects
(evaluated at the starting time of each time period) to calculate
the pruning distance, which is in turn used to filter out objects
that can not be query q’s kNNs. Thus app MaxDq,o(t)
must be no smaller than the exact value of MaxDq,o(t)
at any time instant, so as not to leave out any potential
kNN objects. Second, app MaxDq,o(t) should be as close
as possible to the exact value of MaxDq,o(t). As a result,
we set app MaxDq,o(t) to be the sum of MinDq,o(t), the
length of the position segment of q, and the length of the
position segment of o. Our app MaxDq,o(t) meets these two
requirements aforesaid since it is no smaller than MaxDq,o(t)
at any time instant, and it is the smallest possible value we
can choose for app MaxDq,o(t), hence closest to the exact
value of MaxDq,o(t).
Now we use the objects in Figure 1 as an example to
illustrate the calculation of MaxDq,o(t) and MinDq,o(t).
Here we consider three different cases: object o1 and query
q that are on the same edge; object o2 and query q that are
on two different edges which are distance determinate; object
o5 and query q that are on two different edges which are
distance indeterminate. We obtain the following functions of
MaxDq,o(t) and MinDq,o(t) with respect to different pairs
of objects and query in different time intervals:
0≤t≤2
MinDq,o1(t) = 11 + 3× t MaxDq,o1(t) = 11 + 6× t
MinDq,o2(t) = 15− 4× t MaxDq,o2(t) = 15− 2× t
0≤t≤0.333
MinDq,o5(t) = 39 + 2× t MaxDq,o5(t) = 39 + 4× t
0.333≤t≤2
MinDq,o5(t) = 41− 4× t MaxDq,o5(t) = 41− 2× t
Observe that MaxDq,o(t) and MinDq,o(t) are linear func-
tions of time t, except that (1) when q and o move on the same
edge and they move toward each other, meet and then move
away from each other within the same time period being moni-
tored,MaxDq,o(t) is a polyline which consists of one segment
with a downward slope and another segment with an upward
slope, and MinDq,o(t) is a polyline which consists of one
segment with a downward slope, a horizontal segment, and one
segment with an upward slope; (2) when q and o move on two
edges which are distance indeterminate, bothMaxDq,o(t) and
MinDq,o(t) could be polylines which consist of one segment
with an upward slope and another segment with a downward
slope. The CUkNN algorithm presented below will divide
the time horizon being monitored into consecutive windows by
the turning points of MaxDq,o(t) and/or MinDq,o(t) of the
candidate objects of query q, hence we are sure that these two
functions are always linear in time within each time window.
IV. CUkNNMean ALGORITHM
In this section, we discuss how to continuously monitor a
UkNN query in a road network. To begin with, we use an
efficient snapshot kNN method, such as INE[7], NV3[8], to
calculate the kNN result at the starting time ts = 0. The
main issue is how to continuously monitor the kNN result
after ts. As the objects and the query in question may travel on
different edges in the road network and change their directions
6
Algorithm 2: Refining Phase
input : a set of moving objects O of previous kNNs at time
instant ts, the Cand Set reserved from the
pruning phase, and the time period [ts, te]
output: the UkNN Interm Set which is a set of tuples
< [ti, tj ], {o1, o2, ..., on} >, where
o1, o2, ..., on(n ≥ k) are the UkNNs of q from ti to
tj , and ti, tj are time instants within [ts, te]
1 begin
2 /∗Stage 1:determining the kNN-MaxD-Polyline ∗/ ;
3 let ok to be the kth-object at time instant ts;
4 list kNN-MaxD-Polyline=Ø; list L = Ø; ta = ts; tb = ts ;
5 while tb < te do
6 for each object o ∈ Cand Set and o <> ok do
7 determine the time instant t by solving the
equation MaxDq,o(t) =MaxDq,ok (t) ;
8 select the earliest time instant t ∈ [tb, te] ;
9 if t exists then
10 insert the tuple < t, o > into list L in
ascending order of time;
11 if L.head exists then
12 ta = tb; tb = L.head.t; ok = L.head.o;
13 insert < [ta, tb],MaxDq,ok(t) > into
kNN-MaxD-Polyline ;
14 L = Ø;
15 insert < [tb, te],MaxDq,ok (t) > into kNN-MaxD-Polyline
;
16 /∗Stage 2: determining the UkNNs within [ts, te] ∗/ ;
17 let UkNN Set to be objects whose MinDq,o(t) are
below kNN-MaxD-Polyline at ts;
18 list L = Ø; set UkNN Interm Set = Ø; ta = ts;
tb = ts ;
19 for each object o ∈ Cand Set do
20 if MinDq,o(t) intersects kNN-MaxD-Polyline at a time
instant t & t ∈ [ta, tb] then
21 insert the tuple < t, o > into list L in ascending
order of time;
22 while L is not empty do
23 < t, o >=L.pop-front; ta = tb; tb = t;
24 insert the tuple < [ta, tb], {UkNN Set} > into
UkNN Interm Set ;
25 if o is not in UkNN Set then
26 insert o into UkNN Set;
27 else
28 delete o from UkNN Set;
29 insert the tuple < [tb, te], {UkNN Set} > into
UkNN Interm Set ;
30 return UkNN Interm Set ;
the list kNN-MaxD-Polyline, which is initialized to empty,
to organize the line segments of the polyline we want to
construct; 2) sets the list L, which is initialized to empty,
to organize the intersection points of the Maximum distance
functions of objects o ∈ Cand Set and ok within [ts, te]; and
3) sets two variables ta and tb, which are both initialized to
ts, to record the beginning and the end of the sub-periods
being processed within [ts, te], respectively. Then, we repeat
the following steps when tb is smaller than te (lines 4-14):
1) for each object o ∈ Cand Set, if the Maximum distance
function of o intersects with that of ok at time t ∈ [tb, te],
insert the tuple < t, o > into list L in ascending order of
time t (line 10). If there is more than one such time instant,
the earliest one will be selected (line 8). After all the objects
in candidate set have been processed, list L has recorded
all the objects whose Maximum distance functions intersect
with that of ok within [tb, te] in ascending order of time.
Thus, the head element L.head includes the object which will
first replace ok; 2) if L.head exists (line 11), L.head.o will
work as new ok after the corresponding time point L.head.t.
Thus, we replace ta with tb and replace tb with L.head.t
which means a new sub-period has been formed (line 12).
Then, we insert the tuple < [ta, tb],MaxDq,ok(t) >, which
represents the newly formed line segment, into kNN-MaxD-
Polyline (line 13). Below we use query q in Figure 1 as an
example to illustrate how we construct kNN-MaxD-Polyline
within [ts, te].
As shown in Figure 5, the kth object at time ts is o2. Then
we should make sure that at which time instant o2 will not be
the kth object any more. Note that we don’t know in advance
which object in Cand Set will replace o2 as the kth object
first, thus we should consider each other object o in Cand Set
by solving the equation MaxDq,o2(t) = MaxDq,o(t) (line
7). In this way we determine that o1 replaces o2 as the
kth object at time t=0.5, and obtain the first tuple of kNN-
MaxD-Polyline, i.e., ([0, 0.5],MaxDq,o2(t)). Then we use o1
as the kth object at time t=0.5 and continue the construction
of kNN-MaxD-Polyline of q until time period [ts, te] is all pro-
cessed. Finally, kNN-MaxD-Polyline consists of three tuples:
< [0, 0.5],MaxDq,o2(t) >, < [0.5, 1.111],MaxDq,o1(t) >
and < [1.111, 2.0],MaxDq,o3(t) >.
Note for an object o, if MinDq,o is larger than
MaxDq,kth object at a time instant t, o can not be a kNN
of q at time t; otherwise, o can possibly be a kNN of
q at time t. Moreover, kNN-MaxD-Polyline characterizes
MaxDq,kth object within the time period [ts, te]. Hence, for an
object o whose Minimum distance line intersects kNN-MaxD-
Polyline at time t, 1) if o is not in current UkNN Set, which
means the Minimum distance function of o is above kNN-
MaxD-Polyline before t, intersects with it at t, and then comes
below it after t, thus o should be included into UkNN Set;
2) if o is already in current UkNN Set, which means the
Minimum distance function of o is below kNN-MaxD-Polyline
before t, intersects with it at t, and then comes above it after t,
thus o should be deleted from UkNN Set. That is, whenever
the Minimum distance line of an object intersects kNN-MaxD-
Polyline, UkNN Set will change. We call this time instant
Result Change Time instant (RCT).
In the second stage of Algorithm 2, lines 17-18 1) set
UkNN Set to include all the objects whose MinDq,o(t)
are below kNN-MaxD-Polyline at ts; 2) set two variables,
ta and tb, which are both initialized to ts, to record the
beginning and the end of the sub-periods being processed
within [tb, te], respectively; and 3) set the list L to organize
the time points t ∈ [ts, te], when the Minimum distance
functions of candidate objects intersect with kNN-MaxD-
Polyline; and 4) use the set UkNN Interm Set to organize
the intermediate UkNN result which includes a set of tuples:
< sub − period, the corresopoing UkNN set >. Both L
8
02
4
6
8
10
12
14
16
18
20
t=0.833 1.667
MeanD(q,o1)
MeanD(q,o2)
MeanD(q,o3)
eanDq,o1(t
ea D ,o2(t)
eanDq,o3(t
t=0.111
 
Fig. 7. Divide the time period into sub-periods by the intersections of
MeanDq,o(t) of objects in UkNN Set.
within this period. This means that, for any two objects o1
and o2 in UkNN Set, if Po1(t) > Po2(t) at a time instant
t within a divided time period, Po1(t) is always larger than
Po2(t) at any time instant within this divided time period.
As shown in Figure 7, MeanDq,o1(t) intersects with
MeanDq,o3(t) at time t=1.111, thus the time period
[0.833, 1.667] is divided into 2 sub-periods: [0.833, 1.111]
and [1.111, 1.667]. Then, for each divided time period, we
calculate the mean distance of each object o in UkNN Set
from q at a time point within [ti, tj ]. Without loss of
generality, we choose the central time point of the time
period [ti, tj ], i.e., t=(ti + tj)/2 (line 17). Next, we in-
sert the objects in UkNN set into UkNN List in as-
cending order of mean distances at time (ti + tj)/2
(line 18). Thus the objects o in UkNN List are in
descending order of Po([ti, tj ]) and the first k objects
are the kNNs we want. All sub-periods together with
their ordered UkNN Lists form the UkNN Final Set
of query q (line 19), which consists of four tuples: <
[0, 0.833], {o1, o2} >, < [0.833, 1.111], {o2, o1, o3} >, <
[1.111, 1.667], {o2, o3, o1} >, and < [1.667, 2.0], {o2, o3} >
D. Handling the Moving Speed and Direction Update of
Candidate Objects
For an object o, if it changes its moving direction or
its moving speed goes beyond its speed limit due to vary-
ing traffic conditions, the object o may move out of its
position segment. Hereinafter, we call this situation, that an
monitored object moves out of its position segment at time
t ∈ [ts, te], object update. For a query q, an object update
of its candidate objects can potentially invalidate the previous
query result so that the result needs to be updated accordingly.
Note the object updates of non-candidate objects will not affect
the query result of q. A straightforward approach to updating
the query result is re-executing the CUkNN algorithm when
object updates of candidate objects occur. However, this ap-
proach will greatly degrade the efficiency and flexibility of our
method. Instead, we propose the following strategy to handle
these object updates so as to maintain the UkNN query result.
Assume that when an object moves out of its position
segment at time tu ∈ [ts, te], it sends its new location,
minimum speed, maximum speed, and moving direction to
the server. Once the server receives an update message for
moving object o, it first determines if object o belongs to
Algorithm 3: Probability evaluation and ranking phase
input : the UkNN Interm Set gotten in refining phase
output: the UkNN Final Set
1 begin
2 /∗Step1:further divide [ti, tj ] into shorter time periods ∗/;
3 set UkNN Final Set = Ø; list UkNN List=Ø ;
4 for each tuple < [ti, tj ], UkNN Set > of
UkNN Interm Set, whose size of UkNN Set > k do
5 if the MaxDq,o(t) or MinDq,o(t) function of any
o ∈ UkNN Set has a turning point tu within [ti, tj ]
then
6 Divide it into two tuples: < [ti, tu], UkNN Set >
and < [tu, tj ], UkNN Set > ;
7 Construct the MeanDq,o(t) of each object in
UkNN Set ;
8 if the MeanDq,o(t) of any two objects intersect each
other at tu within [ti, tj ] then
9 Divide it into two tuples: < [ti, tu], UkNN Set >
and < [tu, tj ], UkNN Set > ;
10 /∗Step 2: arrange the objects in UkNN Set in descending
order of their probabilities being a kNN object of q ∗/ ;
11 for each tuple < [ti, tj ], UkNN Set > of
UkNN Interm Set do
12 if the size of UkNN Set ≤ k then
13 Insert the objects in UkNN Set into
UkNN List sequentially;
14 Insert the tuple < [ti, tj ], UkNN List > into
UkNN Final Set ;
15 else
16 for each object o in UkNN Set do
17 Calculate the mean distance of o at time point
(ti + tj)/2, i.e., MeanDq,o((ti + tj)/2) ;
18 Insert o into UkNN List in ascending order
of mean distances at time (ti + tj)/2;
19 Insert the tuple < [ti, tj ], UkNN List > into
UkNN Final Set ;
20 UkNN List=Ø ;
21 output UkNN Final Set;
the candidate set of query q. If not, no processing is needed.
Otherwise, we re-calculate the Minimum and the Maximum
distance functions of object o after tu, i.e., MinDq,o(t) and
MaxDq,o(t). Then, there are two cases regarding whether
object o is a UkNN object of query q within [tu, te].
Case 1: o is a UkNN object of query q within [tu, te].
Remember that in the pruning phase, we have calculated the
pruning distance, i.e,Dpruning , and determined the Cand Set
which consists of all the objects within Dpruning of query
q at time ts. Cand Set includes at least k objects. If
Cand Set includes more than k objects or the largest value
of MaxDq,o(t) within the time period [tu, te] is no more than
Dpruning , there are also at least k objects within Dpruning
of query q within [tu, te]. Otherwise, Dpruning should be
replaced by the largest value of MaxDq,o(t) within [tu, te],
and some new objects within this new Dpruning of query q
may be added into the new candidate set.
Next, we re-calculate kNN-MaxD-Polyline if MaxDq,o(t)
is part of original kNN-MaxD-Polyline within [tu, te] or
MaxDq,o(t) intersects with original kNN-MaxD-Polyline at
10
  Fig. 10. The subdivision rules of HC.
1
2
3
4
5 6
7
8
9 10
11
12
13 
 (a)                (b)                  (c) 
 
Fig. 11. Getting the filling curve and the number of each cell.
each point on the curve we got sequentially. Thus, we can get
the number of each cell. To continue the example in Figure 8,
we can get the space-filling curve and the number of each cell
which are shown in Figure 11. Figure 11(a) shows the first
level curve. Since the first level cells at the upper-left side
and at the upper-right side, which are corresponding to the
upper-left and the upper-right points of the first level filling
curve respectively, have sub-cells, thus these two points are
replaced by two small bottom cups and a right join (Figure
11(b); refer to the first line of Figure 10). The final filling
curve and the number of each cell are shown in Figure 11(c).
Similarly, we can get the second part number of a node,
i.e., the number of a node in the cell it resides. However, the
second part number is rather easy to get since the number of
nodes within a cell is small.
Up to now, we have discussed how to number the cells and
the nodes of the network. For example, the grey node in Figure
8 is within Cell 1, thus its first part number is 1. Assume
its number in the cell it resides is 3, we can get the serial
number of the grey node which is 1-3. Then we discuss how
to organize and store the matrix DD. In general, we consider
the nodes within a cell together and get the construction of
matrix DD as follows. Assume that we totally get m cells,
thus we have
DD =

DDCell1,Cell1 · · · DDCell1,Cellm
DDCell2,Cell1 · · · DDCell2,Cellm
...
. . .
...
DDCellm,Cell1 · · · DDCellm,Cellm
 (2)
DDCelli,Cellj =

dCell1
i
,Cell1
j
· · · dCell1
i
,Cellδ
j
dCell2
i
,Cell1
j
· · · dCell2
i
,Cellδ
j
...
. . .
...
dCellδ
i
,Cell1
j
· · · dCellδ
i
,Cellδ
j
 (3)
In particular, dCellk
i
,Celll
j
is the distance between the node k
in celli and the node l in cellj and eachDDCelli,Cellj includes
δ × δ such distance values. Remember that we choose the
value δ especially to make sure that δ × δ distance values
just takes a whole disk block. Consider that the matrix DD
and each sub matrix DDCelli,Cellj are both symmetric with
respect to the main diagonals, thus we only need to keep the
upper (or lower) triangular matrix. Here the lower triangular
matrix is chosen. As a result, we can modify the value δ to
further save the space as follows:
δ =
√
block size
the size of an element of matrix DN
∗ 2 (4)
Thus DDCelli,Cellj , which is an element of matrix DD,
takes just a disk block. Then, we store the lower triangular
matrix of matrix DD in (m × m + m)/2 continuous disk
blocks in a row-major order: DDCell1,Cell1 , DDCell2,Cell1 ,
DDCell2,Cell2 , DDCell3,Cell1 , DDCell3,Cell2 , DDCell3,Cell3 ,
· · · , DDCellm,Cell1 , DDCellm,Cell2 , · · · , DDCellm,Cellm .
Considering that the nodes within the same cell are very
close to each other and the distances between the nodes within
the same cell are frequently used in processing CUkNN
queries, we store m special elements DDCelli,Celli (i=1,2,· · · ,
m), which keeps the distances between the nodes within the
cell i, in main memory. In this way, we can greatly reduce
disk access for matrix DD.
Finally, we conclude this subsection with the techniques we
used to make the disk access as less as possible:
1. we partition the network space into cells according to the
nodes on the space. Moreover, we consider the adjacent nodes
in the same cell as a whole and store the distances between
nodes in disk blocks according to the cells they belong to.
2. we number the cells using a method similar to HC
order, thus the cells which are near to each other in space
are near in their numbers. Furthermore, our space-filling curve
is more efficient than HC, since we can get continuous cell
numbers even for the network space whose nodes are unevenly
distributed.
3. the number of a node consists of two part: the number of
the cell it resides and its number in the cell it resides. Thus,
given a set of nodes, we can easily get the cells they belong
to. Then, we can easily determine which blocks are needed
to access. Furthermore, these blocks are usually in sequential
order which will further short disk access time.
4. we store m special elements DDCelli,Celli (i=1,2,· · · , m)
in main memory to further reduce disk access.
VI. PERFORMANCE EVALUATION
In this section, we evaluate the performance of the pro-
posed CUkNNMean algorithm and the techniques we use to
12
01
2
3
4
10 30 50 70 100
Query interval length(time units)
C
P
U
 
t
i
m
e
(
s
)
CUkNN_Mean CUkNN_PLS
CUkNN_Mean_Imp CUkNN_PLS_Imp
75
80
85
90
95
100
10 30 50 70 100
Query interval length(time units)
P
r
e
c
i
s
i
o
n
(
%
)
CUkNN_Mean CUkNN_PLS
CUkNN_Mean_Imp CUkNN_PLS_Imp
0
10
20
30
40
50
60
70
10 30 50 70 100
Query interval length(time units)
 
D
i
s
k
 
b
l
o
c
k
s
(
K
)
CUkNN_Mean CUkNN_PLS
CUkNN_Mean_Imp CUkNN_PLS_Imp
 
                  (a)                                         (b)                                         (c) 
 
Fig. 12. Effect of query interval length
In the second set of experiments, we compare the perfor-
mance of the algorithms by varying the value of k from 1 to
20.
As shown in Figure 13(a), the CPU overhead for these four
algorithms grows when k increases. This is because that as the
value of k increases, the number of qualified objects increases,
thus more distance comparisons among these qualified objects
are needed. In general, CUkNNMean Imp and CUkNNMean
outperform their competitors in all cases.
Figure 13(b) shows that the precision of these four
algorithms increases slightly as k increases. The preci-
sion of CUkNNMean and CUkNNMean Imp is above
86% in all cases, and the precision of CUkNNPLS and
CUkNNPLS Imp is slightly less than that of CUkNNMean
and CUkNNMean Imp.
Figure 13(c) shows that the value k has great effect
on the number of accessed disk blocks for CUkNNMean
and CUkNNPLS . While, for CUkNNMean Imp, and
CUkNNPLS Imp, the varying value k has little effect on their
performance.
We then study the effect of varying the number of query
points on the performance of these four algorithms.
As shown in Figure 14(a), the running time of these four
methods increases almost linearly as the number of query
points increases. It is because the total running time is the
sum of the processing time of each query in the system.
Figure 14(b) shows that the query cardinality has little effect
on the precision of these four methods.
Figure 14(c) tells us that the query cardinality has very
different effect on accessed disk blocks of these four methods.
For CUkNNMean and CUkNNPLS , the number of accessed
disk blocks increases almost linearly as we increase the queries
on the system. While the increasing trend is rather gentle
for CUkNNMean Imp and CUkNNPLS Imp, contrast to
CUkNNMean and CUkNNPLS .
Next, we examine the effect of changing the number of
moving objects on the performance of the algorithms.
Figure 15(a) shows that the running time of these four meth-
ods decreases as the number of moving objects increases. This
is because that, as the number of objects increases, the object
density increases correspondingly, and the monitoring range of
a query decreases as well, thus the distance computation and
comparison needed decrease. As a result, the running time
decreases.
As shown in Figure 15(b), the precision of these methods
decreases slightly as the number of moving objects increases.
The reason lies in that as the object density increases, there
are more objects whose position segments overlap with each
other, thus some fault results may be caused.
Figure 15(c) shows that the object cardinality also has very
different effect on accessed disk blocks of these four methods.
For CUkNNMean and CUkNNPLS , the number of accessed
disk blocks decreases obviously as object cardinality increases.
On the contrary, the decreasing trend is rather gentle for
CUkNNMean Imp and CUkNNPLS Imp.
Finally, we compare the performance of these four algo-
rithms with respect to the number of vertices in the network.
We use a synthetic 2-dimensional grid network as the basic
network and 300*300 is the default grid size of the network,
which includes 5k queries and 500k objects following random
distributions. We consider the network with different grid sizes
that vary from 100*100 to 500*500, and the number of objects
(queries) included in the network changes correspondingly in
order to maintain a constant ratio of object (query) cardinality
and edge cardinality.
As shown in Figure 16(a), when the number of cells in-
creases, the CPU time of these algorithms increases obviously.
This is due to the fact that the number of queries in the system
increases as the network size increases.
Figure 16(b) shows that the precision of these four methods
remains almost unchanged as the network size varies.
As shown in Figure 16(c), the number of accessed disk
blocks of these algorithms increases greatly as the grid size
increases. The reason mainly lies in that the number of queries
need to be processed increases greatly when we increase the
the network size.
In summary, the CUkNNMean Imp algorithm has better
performance in CPU time, precision, and the number of
accessed disk blocks, and it is robust with respect to different
values of k, query cardinality, object cardinality, and network
size.
VII. CONCLUSION
This paper addresses the issue of processing CkNN queries
over moving objects with uncertain speeds (CUkNN ) in
road networks. We present an uncertain network distance
model to calculate the distances between moving objects and
a submitted query, both of which move with uncertain speeds
in the road network. Based on the distance model, we propose
a continuous UkNN query monitoring method, which is
14
[3] K. Mouratidis, M.L. Yiu, D. Papadias, and N.Mamoulis,“Continuous
Nearest Neighbor Monitoring in Road Networks”, in VLDB, 2006, pp.
43-54.
[4] Y.K. Huang, Z.W. Chen, and C. Lee, “Continuous k-Nearest Neighbor
Query over Moving Objects in Road Network”, APWeb-WAIM 2009,
LNCS 5446, pp.27-38.
[5] W. Liao, X.P. Wu, C.H Yan ,and Z.N. Zhong, “Processing of Continuous
k Nearest Neighbor Queries in Road Networks”, Software Engineering,
Artificial Intelligence, SCI 209, pp. 31-42.
[6] Y.K. Huang, S.J. Liao, and C. Lee, “Evaluating continuous K-nearest
neighbor query on moving objects with uncertainty”, Information Sys-
tems(June 2009), Volume 34 Issue 4-5, pp. 415–437.
[7] C.S. Jensen, J. Kolar, T.B. Pedersen, and I. Timko, “Nearest neighbor
queries in road networks”, in ACM GIS(2003), pp. 1 - 8.
[8] D. Papadias, J. Zhang, N. Mamoulis, and Y. Tao, “Query Processing in
Spatial Network Databases”, in VLDB, 2003, pp. 802-813.
[9] M.R. Kolahdouzan and C. Shahabi, “Voronoi-Based K Nearest Neighbor
Search for Spatial Network Databases”, in VLDB, 2004, pp. 840-851.
[10] X. Huang, C. S. Jensen, and S. Saltenis, “The Islands Approach to
Nearest Neighbor Querying in Spatial Networks”, in SSTD(2005), pp.
73-90.
[11] H. Wang and R. Zimmermann, “Location-based Query Processing on
Moving Objects in Road Networks”, in VLDB, 2007, pp. 321-332.
[12] A. Guttman, “R-Trees: A Dynamic Index Structure for Spatial Search-
ing”, in SIGMOD Conference, 1984, pp. 47-57.
[13] Maytham Safar, “Enhanced Continuous KNN Queries Using PINE on
Road Networks”, in ICDIM, 2006, pp. 248-256.
[14] C. R, N. Dalvi, and D. Suciu, “Efficient top-k query evaluation on
probabilistic data”, in ICDE, 2007, pp. 886-895.
[15] K. Yi, F. Li, G. Kollios, and D. Srivastava, “Efficient processing of top-k
queries in uncertain databases”, in ICDE, 2008, pp. 1669-1682.
[16] M.A. Soliman, I.F. Ilyas, and K.C.C. Chang, “Top-k query processing
in uncertain databases”, in ICDE, 2007, pp. 896–905.
[17] G. Beskales, M. Soliman, I.F. lyas, “Efficient search for the top-k
probable nearest neighbors in uncertain databases”, in VLDB, 2008, pp.
326-339.
[18] N. Augsten, D. Barbosa, M. Bo¨hlen, and T. Palpanas, “TASM: Top-k
Approximate Subtree Matching”, in ICDE, 2010, pp. 353-364.
[19] L.F. Lin, Y.K. Huang, “Scalable Processing of Continuous K-Nearest
Neighbor Queries with Uncertainty in Spatio-Temporal Databases”, in
ICRCCS, 2009, pp. 210-213.
[20] Goce Trajcevski, Roberto Tamassia, Hui Ding, Peter Scheuermann,
and F.Cruz Isabel, “Continuous probabilistic nearest-neighbor queries for
uncertain trajectories”, in EDBT, 2009, pp. 874-885 .
[21] J.C. Chen, R. Cheng, M. Mokbel,and C.Y. Chow, “Scalable processing of
snapshot and continuous nearest-neighbor queries over one-dimensional
uncertain data”, The VLDB Journal(2009) 18, pp. 1219-1240.
[22] Y.K. Huang and C. Lee, “Efficient evaluation of continuous spatio-
temporal queries on moving objects with uncertain speed”, Geoinformat-
ica(April 2010), Volume 14 Issue 2, pp. 163-200.
[23] Gotsman, C., Lindenbaum, M.: On the metric properties of discrete
space-filling curves. IEEE Trans. Image Process. 5(5), 794C797 (1996)
[24] Guohui Li, Yanhong Li, and LihChyun Shu, “CkNN Query Processing
over Moving Objects with Uncertain Speeds in road networks”, in APweb
2011.
[25] http://www.fh-oow.de/institute/iapg/personen/brinkhoff/generator/
16
99 年度專題研究計畫研究成果彙整表 
計畫主持人：徐立群 計畫編號：99-2221-E-006-124- 
計畫名稱：在可變動速度處理器上之非搶佔式即時排程 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 3 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100 字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：□已發表 □未發表之文稿 ■撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100 字為限） 
已經撰文並投稿至國際期刊,目前審查中. 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
The past decade has seen signicant growth in the volume and sophistication of 
research on energy-efficient scheduling for real-time systems on machines with 
DVS capabilities. Although most of such research performed to date assumes 
preemptive scheduling, it is our belief that novel research is needed for 
non-preemptive workloads in a number of embedded applications, particularly those 
found in small embedded devices with limited memory capacity as well as those built 
on multiprocessor platforms. To the extent the proposed research is successful, 
it will provide a useful scheduling technique for designers of energy-constrained
applications in which 
task preemption is not an option to reduce task response times. Our technique in 
characterizing the workload on a non-preemptive task from higher-priority tasks 
may also be applicable in other non-preemptive scheduling research. It is our hope 
that the results obtained from this research may serve as enabling technologies 
for some of the recent research focuses promoted in the National Science Council's 
Computer Science & Information Engineering program that are supervised by the 
Department of Engineering and Applied Science. Some of these focused areas include 
life caring systems, smart living spaces, digital life technologies and 
