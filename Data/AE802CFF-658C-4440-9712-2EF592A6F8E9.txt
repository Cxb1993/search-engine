摘要 
        本研究提出一個結合倒傳遞網路(Back-Propagation Network, BPN)與 ARIMA 優點的
ARIMA‐BPN 神經網路，它是以 BPN 為模型，將 ARIMA 模式的輸入，包括前時 p 個時刻
的數列值與前 q 個時刻的數列殘差值做為輸入值，組成非線性函數，以建立更準確的時
間數列預測模型。因為數列殘差值在 BPN 的訓練過程中會因網路連結權值的調整而改
變，因此必須修改 BPN 的演算法來適應此需求，即藉由不斷更新每次預測所得之殘差值
做為網路的輸入值。本研究以六個人為設計的例題，及現實世界的各國股市時間數列預
測來比較 ARIMA、BPN 和 ARIMA‐BPN 三者的效能。研究結果顯示，ARIMA‐BPN 神經網
路演算法在部份例題比 ARIMA 與 BPN 方法更準確。 
關鍵字：時間數列、ARIMA、倒傳遞神經網路。 
 
Abstract 
    In this paper we proposed an ARIMA-BPN algorithm combining advantages of 
Back-propagation networks (BPN) and ARIMA. The algorithm is based on BPN and its 
inputs are the same as ARIMA. It can generate a non-linear function to create an accurate 
model to predict time series. The BPN algorithm must be modified because the residuals 
would be changed when the weights were changed during continuously training BPN. 
Therefore, the continuously updated residuals are used as the inputs of ARIMA-BPN. This 
study examined 6 artificial designed cases and stock index prediction in real world to evaluate 
the abilities of the ARIMA, BPN, and ARIMA-BPN. The results showed that ARIMA-BPN is 
the most accurate methods in the three methods. 
Key words: time series, ARIMA, Back-Propagation Neural Network. 
 
壹、 前言 
時間序列預測是一個具有廣泛應用的研究領域。ARIMA 模式，又稱 Box-Jenkins 模
式，是一種用來處理複雜的時間序列的一種重要的建模方法(Pankratz, 1983)，其優點在
於能精確掌握複雜的時間序列。使用 ARIMA 模式所處理的時間數列必須是穩態的時間
序列。所謂穩態是指其統計參數如平均值與標準差不隨時間而變，因此有傾向或季節性
的數列便不符合穩態的條件，但可先將數列去除傾向性與季節性後，得到穩態數列再加
以處理。ARIMA 在建立時間數列預測模型時，以前 p 個時刻的數列值與前 q 個時刻的數
列殘差值做為輸入值，組成線性函數： 
ptpttt yyyy −−− ++++= φφφφ ...22110  2211 −− −−+ ttt εθεθε qtq −−− εθ...  (1) 
其中 tyt = 時刻數列值； tt =ε 時刻殘差值； =0φ 常數項； =− ptφ 第 pt − 時刻數列值的係
數； =− ptθ 第 pt − 時刻殘差值的係數。 
以 ARIMA 建立模型，此模型的預測值與殘差交由 ANN 建立預測模型。例如，Tseng 
et al. (2001)以此法處理臺灣的機械產業產值與軟性飲料產值的時間數列，研究結果發
現，此法優於單獨使用 ARIMA 與 ANN。 
第 3 類：單模型串列整合法 ─ ARIMA-ANN 
以 ANN 建立模型，此模型的預測值與殘差交由 ARIMA 建立預測模型。此法為第 2
類方法的反向方法，即先用 ANN 再用 ARIMA。 
第 4 類：雙模型平行加權法 
以 ARIMA 與 ANN 各自建立一個模型，再以加權平均法等將二個模型的預測值整
合成最終預測值。加權平均的權值一般可用模型的殘差平方和的倒數。例如，張百棧
(2004)建立多元迴歸分析、倒傳遞類神經網路、以及自我迴歸移動平均整合模式，再將
這三種預測模型所得到之結果透過一動態權重值加以結合。並以台灣地區股價指數之預
測為例，進行驗證。研究結果發現，結合模型的預測誤差比其他三種預測模型有明顯改
善。Inoue et al. (2001)則用此法整合 ANN 與最近鄰居法(kNN)於時間數列預測，研究結
果發現，整合屬於 Global Model 的 ANN 與 Local Model 的 kNN 可以獲得更準確的預測。 
第 5 類：雙模型串列疊加法 ─ ANN-ARIMA 
以 ANN 建立第一個模型，此模型的殘差交由 ARIMA 建立第二個模型，最後以二
個模型的預測值疊加成最終預測值。例如，王成財(2002)結合 BPN 與 ARIMA 發展「複
合模式」。此模式先以 BPN 做預測，求出預測值與實際值的殘差，再利用 ARIMA 針對
殘差值加以預測，最後將二者所得之預測值疊加後產生最終預測值。研究結果顯示，此
複合模式優於 BPN 模式，而 ARIMA 最差。 
第 6 類：雙模型串列疊加法 ─ ARIMA-ANN 
以 ARIMA 建立第一個模型，此模型的殘差交由 ANN 建立第二個模型，最後以二
個模型的預測值疊加成最終預測值。例如，Zhang (2003)用此法處理三個時間數列的經
典例題：太陽黑子(sunspot)問題(1700~1987，288 筆年資料)、加拿大山貓(lynx)問題
(1821~1934，114 筆年資料)、英磅美金匯率問題(1980~1993，731 筆週資料)。研究結果
顯示，此法優於單獨使用 ARIMA 與 ANN。此外，Bo et al. (2007)擴充 Zhang 的方法，
整合了 ARCH 族的方法(包括 ARCH, GARCH, EGARCH 等)與 ANN。而 He et al. (2006) 
也擴充 Zhang 的方法，整合了 ARIMA 與支持向量機(support vector machine)。這些研究
都顯示複合模式優於單一模式。 
 
貳、 ARIMA-BPN 神經網路 
本研究所提出的 ARIMA-BPN 演算法是以 BPN 為基礎，並加入了殘差向量來改良
預測效果的演算法。BPN 的理論已有大量的文獻(葉怡成，2005；葉怡成 2006)，在此不
再贅述。 
傳統的 BPN 在處理時間數列時是以前 p 個時刻的數據來預測第 1+p 個數值，也就
是輸入層有 p 個。ARIMA-BPN 則另外加入了 q 個殘差做為輸入，組成非線性函數
),...,,,,...,,( 2121 ptttptttt yyyfy −−−−−−= εεε ，以建立更準確的時間數列預測模型。因為數列
殘差值在 BPN 的訓練過程中會因網路連結權值的調整而改變，因此必須修改 BPN 的演
算法來適應此需求，即藉由不斷更新每次預測所得之殘差值做為網路的輸入值。圖 2 是
ARIMA‐BPN 神經網路架構圖。 
 
Yt-1
Yt-p
Yt-p+1
p 個
ε t-1
ε t-q
ε t-q+1
q 個
Input Layer
Hidden Layer
Output Layer
Yt
 
圖 2 ARIMA‐BPN 神經網路架構圖 
 
    在第一次訓練時，由於還沒有預測值，所以輸入層的 q 個殘差皆為 0。另外輸入層
還有 1 至 p 等 p 個時刻的數據作為輸入層。所以演算法用了 qp + 個輸入層，透過隱藏
層的運算(此時殘差皆為 0)得到第 1+p 個時刻的預測數據。此時得到預測數據後，再將
第 1+p 個時刻的實際數據與預測數據相減，即可得到殘差。這時我們需要更新初始的 q
個殘差。假設有 q~1 個殘差，以第 1−q 個取代第 q 個殘差，第 2−q 個取代第 1−q 個殘
差，以此類推直到第 1 個取代第 2 個殘差。最後以本次的殘差取代第 1 個殘差，即再開
始下個訓練。 
    在第二次訓練時，在輸入層的第 1 個殘差為前一時刻的殘差，其餘 1−q 個殘差還是
( ) ( )1−Δ⋅+=Δ nWhnW kkk αηδ                                (13) 
( ) ( )1−Δ⋅+−=Δ nn θαηδθ                                   (14) 
(b) 計算隱藏層加權值矩陣修正量，及門限值向量修正量 
( ) ( )1−Δ⋅+=Δ nWxnW ikikik αηδ                               (15) 
( ) ( )1−Δ⋅+−=Δ nn kkk θαηδθ                                (16) 
9. 更新加權值矩陣，及門限值向量 
(a) 更新輸出層加權值矩陣，及門限值向量 
kkk WWW Δ+=                                            (17) 
θθθ Δ+=                                               (18) 
(b) 更新隱藏層加權值矩陣，及門限值向量 
ikikik WWW Δ+=                                            (19) 
kkk θθθ Δ+=                                              (20) 
(c) 計算殘差值 
   yydesiret −= _ε                                           (21) 
10. 設 t=t+1 
11. 檢查是否已到最後一個訓練範例，完成一個學習循環：如果 t≤ n 回到步驟 5，否則
到步驟 12。 
12. 重覆步驟 3 至步驟 11，直到預設的學習循環數目。 
 
        以上介紹中，值得注意的是由於開始時尚未做預測，因此殘差向量設為 0。在預測
第 1+p 個時刻時，其 qp + 個輸入變數中，前 p 個輸入為前 p 個數列值，後 q 個輸入為
前 p 個時刻的殘差值。而學習過程中在更新殘差向量時，以第 1−q 個取代第 q 個殘差，
第 2−q 個取代第 1−q 個殘差，以此類推直到第 1 個取代第 2 個殘差，最後以本次的殘
差取代第 1 個殘差。另外每次在開始一個新的學習循環時，要記得將殘差向量設為 0，
因為循環開始時尚未有預測數據，殘差是未知的，故設為 0。 
舉例來說，有 5 筆數據，以前 p 個時刻及前 q 個時刻的殘差值為神經網路之輸入，
進而預測下個時刻之數據。本例子的 p 與 q 為 2。圖 3 為此範例之流程圖。在步驟一，
欲預測值為第 3 個時刻，因此我們以第 1 與第 2 個時刻，也就是前 2 個數列值為輸入。
另外，ARIMA‐BPN 神經網路還需要前 q 個時刻之殘差值做為輸入，但是由於步驟一之前
並未有任何的預測，所以我們將第 1 個與第 2 個時刻之殘差值 q1 與 q2 設為 0。輸入完
成後 ARIMA‐BPN 神經網路即可得到第 3 個時刻的預測值，並以倒傳遞演算法的通用差
距法則(general delta rule)，即(13)~(16)式，修正網路的連結權值。 
        到了步驟二時，欲預測值為第 4 個時刻，因此我們以第 2 與第 3 個時刻，也就是前
2 個數列值為輸入。而在殘差值部分，將第 3 個時刻之實際值減去步驟一的第 3 個時刻
的預測值得到第 3 個時刻的殘差值，然後以以第 2 個與第 3 個時刻的殘差值為輸入。輸
 例題二 
tttty εεε += −− 212                                                  (23) 
 
例題三 
tttty εεε +−= −− 2 22 1 6.03.1                                             (24) 
 
例題四 
tttttty εεεεε +++−= −−−− 2121 109.08.1                                   (25) 
 
例題五 
tttty γεγ += −− 113                                                   (26) 
tttt εεγγ ++= −− 11 9.09.0                                              (27) 
 
例題六 
tttty γεγ ++= −2 1                                                   (28) 
ttt εεγ += −2 18                                                       (29) 
每一個公式產生一個由 800 個數列值構成的時間序列，其中前 400 個將作為訓練範
例，後 400 作為測試範例，以避免過度配適的問題。對所有例題，BPN 一律取前二個時
刻的數列值為輸入，而 ARIMA-BPN 則除了取前二個時刻的數列值為輸入外，還取前二
個時刻的殘差值為輸入，即取 2=p ， 2=q 。而 ARIMA 則以典型的平穩化、模型鑑別、
參數估計、殘差診斷等四個步驟建立最適模型。 
 
3.2 結果  
表 1 與表 2 為實驗結果，表中的結果都以測試範例為準。由表可得下列結論： 
1. 誤差均方根的比較 
由表 1 可知，除了數列二之外，ARIMA‐BPN 的誤差均方根(RMS)都是三個方法中最
小的，且在六個例題中，ARIMA‐BPN 的誤差都低於 ARIMA。 
2. 判定係數 2R 的比較 
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
1 2 3 4 5 6
非線性時間序列
判
定
係
數
ARIMA
BP
ARIMA-BPN
 
圖 4 各方法之 2R 比較 
 
R2 = 0.2243
實際值
預
測
值
圖 5(a) BPN 測試範例之散佈圖 
R2 = 0.3821
實際值
預
測
值
圖 5(b) ARIMA-BPN 測試範例之散佈圖 
 
R2 = 0.0893
實際值
預
測
值
圖 6(a) BPN 測試範例之散佈圖 
R2 = 0.1056
實際值
預
測
值
 
圖 6(b) ARIMA-BPN 測試範例之散佈圖 
 
R2 = 0.2243
實際值
預
測
值
圖 10(a) BPN 測試範例之散佈圖 
R2 = 0.2656
實際值
預
測
值
 
圖 10(b) ARIMA-BPN 測試範例之散佈圖 
 
肆、 各國股市時間數列預測 
4.1 簡介 
本節以包括美國、巴西、英國、香港、印尼、日本、韓國、馬來西亞、墨西哥、台
灣等十個股票市場(表 3)，各約 2000 多日的收盤假時間數列，來評估 ARIMA‐BP 的建模
能力。 
 
表 3 本研究選取的股價指數 
國家 股市名稱 股票代號 
日本 NIKKEI 225 N225 
印尼 Composite Index  JKSE 
巴西 IBOVESPA SAO PAULO BVSP 
馬來西亞 KLSE Composite KLSE 
墨西哥 IPC MXX 
韓國 KOSPI Composite Index KS11 
美國 DOW JONES COMPOSITE INDEX DJA 
英國 FTSE 100 FTSE 
香港 HANG SENG INDEX HSI 
台灣 TSEC weighted index TWII 
 
05000
10000
15000
20000
25000
圖 11(i) Mexico 原始數據折線圖 
0
2000
4000
6000
8000
10000
12000
 
圖 11(j) Taiwan 原始數據折線圖 
  
4.2 ARIMA 分析 
在本例題的 ARIMA 分析實驗中，使用了 CPRE 軟體且一般差分設為 1。經過 ARIMA
分析後，可以得到四種模型的 2R 值，選擇 2R 值最高者為最佳模型。但是可以發現有不
只一個最大 2R 的狀況，也就是 ARIMA 無法精確建模的情形，如 America 與 Mexico。
觀察 RMS 值，發現還是無法判定 America 在何種模型中表現較佳。其他可發現 Indonesia
與 Mexico 為 MA1 模型，其餘全部為 AR2 模型。表 4 為經過 ARIMA 分析後所得的 2R
值與 RMS 值。圖 12 與圖 13 分別為 ACF 圖與 PACF 圖。 
 
 
圖 12(a) America 股市 ACF 圖: 一次差分 
 
圖 13(a) America 股市 PACF 圖: 一次差分 
  
 
圖 12(g) Korea 股市 ACF 圖: 一次差分 圖 13(g) Korea 股市 PACF 圖: 一次差分 
 
圖 12(h) Malaysia 股市 ACF 圖: 一次差分 
 
圖 13(h) Malaysia 股市 PACF 圖: 一次差分 
圖 12(i) Mexico 股市 ACF 圖: 一次差分 
 
圖 13(i) Mexico 股市 PACF 圖: 一次差分 
圖 12(j) Taiwan 股市 ACF 圖: 一次差分 
 
圖 13(j) Taiwan 股市 PACF 圖: 一次差分 
表 4 ARIMA 分析各國股市之 2R 值與 RMS 值(續) 
  AR1 AR2 MA1 MA2 
全部範例判定係數 0.005  0.007  0.005  0.001 
全部範例 RMS 15.38 15.36 15.37 15.41 
訓練範例 RMS (1060) 17.596 17.575 17.59 17.65 
韓國 
測試範例 RM S (1057) 12.765 12.75 12.763 12.77 
全部範例判定係數  0.013  0.050  0.013  0.035 
全部範例 RMS 10.60 10.39 10.59 10.47 
訓練範例 RMS (1060) 14.02 13.72 14.02 13.82 
馬來西亞 
測試範例 RM S (1057) 5.29 5.271 5.29 5.272 
全部範例判定係數 0.013 0.014  0.014  0.001 
全部範例 RMS 110.81 110.79 110.78 111.50 
訓練範例 RMS (1060) 105.55 105.40 105.44 106.58 
墨西哥 
測試範例 RM S (1057) 115.573 115.63 115.576 116.05 
全部範例判定係數  0.004  0.006  0.004  0.004 
全部範例 RMS 105.79 105.69 105.80 105.77 
訓練範例 RMS (1060) 129.13 128.94 129.15 129.06 
台灣 
測試範例 RM S (1057) 75.48 75.470 75.472 75.55 
 
將各國人均 GDP(美元，2003)( United Nations Statistics Division, 2003)與 AR(1)模型
的判定係數繪成散佈圖(圖 14)，可發現，除了巴西之外，人均 GDP 的對數值與 AR(1)
模型的判定係數成反比，可見高所得(高度經濟發展)國家的股票市場的市場效率比低所
得(低度經濟發展)者高，這與一般對效率市場的看法是吻合的。 
 
 
 
 
 
 
 
 
 
表 6 倒傳遞類神經網路預測十國股市之 RMS 值 
 所有範例 訓練範例 測試範例  BP 
美國  30.09 (2117) 31.96 (1060) 28.08 (1057) (1,0,2) 
巴西  322.22 (2117) 297.16 (1060) 345.53 (1057) (1,0,2) 
英國  60.82 (2117) 71.24 (1060) 48.15 (1057) (1,0,2) 
香港  205.38 (2117) 264.94 (1060) 118.67 (1057) (1,0,2) 
印尼  10.112 (2117) 10.71 (1060) 9.48 (1057) (1,0,2) 
日本  203.37 (2117) 247.00 (1060) 147.16 (1057) (1,0,2) 
韓國  15.42 (2117) 17.64 (1060) 12.81 (1057) (1,0,2) 
馬來西亞  10.61 (2117) 14.03 (1060) 5.31 (1057) (1,0,2) 
墨西哥  110.86 (2117) 105.57 (1060) 115.93 (1057) (1,0,2) 
台灣 106.31 (2117) 129.64 (1060) 76.03 (1057) (1,0,2) 
 
4.6.4 ARIMA-BP 分析 
    表 7 為 ARIMA-BP 神經網路預測十國股市的結果。比較表 6 與表 7 中的測試範例
之 RMS 值，可以發現 ARIMA-BP 的表現與倒傳遞神經網路不相上下，但是並未優於
ARIMA。 
 
表 7 ARIMA-BP 預測十國股市之 RMS 值 
 所有範例 訓練範例 測試範例 ARIMA-BP 
美國  30.10 (2117) 31.96 (1060) 28.10 (1058) (1,0,2,2) 
巴西  322.26 (2117) 297.14 (1060) 345.63 (1058) (1,0,2,2) 
英國  60.82 (2117) 71.25 (1060) 48.15 (1057) (1,0,2,2) 
香港  205.26 (2117) 264.78 (1060) 118.65 (1057) (1,0,2,2) 
印尼  10.111 (2117) 10.70 (1060) 9.48 (1057) (1,0,2,2) 
日本  203.32 (2117) 246.97 (1060) 147.07 (1057) (1,0,2,2) 
韓國  15.41 (2117) 17.63 (1060) 12.80 (1057) (1,0,2,2) 
馬來西亞  10.58 (2117) 13.99 (1060) 5.30 (1057) (1,0,2,2) 
墨西哥  110.88 (2117) 105.48 (1060) 116.04 (1057) (1,0,2,2) 
台灣 106.23 (2117) 129.55 (1060) 75.96 (1058) (1,0,2,2) 
 
Jain, A. and Kumar, A.M. (2007) “Hybrid neural network models for hydrologic time series 
forecasting,” Applied Soft Computing, 7, 585–592. 
Law, R. (2000). “Back-propagation learning in improving the accuracy of neural 
network-based tourism demand forecasting,” Tourism Management, 21(4) 331-340. 
Law, R. and Au, N. (1999). “A neural network model to forecast Japanese demand for travel 
to Hong Kong,” Tourism Management, 20(1), 89-97. 
Pankratz, A. (1983). Forecasting with Univariate Box-Jenkins Models: Concepts and Cases. 
New York: John Wiley. 
Tseng, F.M., Yu, H.C., and Tzeng, G.H. (2001) “Combining neural network model with 
seasonal time series ARIMA model,” Technological Forecasting and Social Change, 
69(1), 71–87. 
Zhang, G. P. (2003) “Time series forecasting using a hybrid ARIMA and neural network 
model,” Neurocomputing, 50 , 159–175. 
王成財(2000)，林陳彥，洪水鴻，游珮瑛，蕭佩玉，莊峻瑜，潘淯民，「結合類神經網路
與時間序列預測台北地區臭氧濃度之研究」，第七屆人工智慧與應用研討會
(TAAI2002)論文集，53-58。 
李振民(2001)，「以類神經網路為基礎之預測系統之研究」，中華大學土木工程研究所，
碩士論文。 
呂志峰(1999)，品質成本與品質管理之關聯性研究-關聯性研究-類神經網路類神經網路
預測模式之運用，義守大學管理科學研究所，碩士論文。 
張百棧，王彥文，楊雯寧(2004)，「混合式預測模式之發展—預測台灣股價指數之波動」，
Journal of the Chinese Institute of Industrial Engineers, 21(4), 358-368. 
湯健文(2003)，「類神經網路於因果關係模型與時間數列模型之應用」，中華大學土木工
程研究所，碩士論文。 
葉怡成(2005)，應用類神經網路，台北：儒林書局。 
葉怡成(2006)，應用類神經網路—應用與實作，台北：儒林書局。 
蔡裕春(2001)，台灣地區營造工程物價指數預測之研究—以類神經網路與 ARIMA 模
式，輔仁大學應用統計學研究所，碩士論文。 
 
 
Elliptical Probabilistic Neural Networks 
 
I-Cheng Yeh / Department of Information Management, 
Chung Hua University 
Hsin Chu, Taiwan 30067, R.O.C. 
E-MAIL: icyeh@chu.edu.tw 
Kuan-Cheng Lin / Department of Information 
Management, Chung Hua University 
Hsin Chu, Taiwan 30067, R.O.C. 
E-MAIL: m09410024@chu.edu.tw 
Kuan-Chieh Huang / Department of Computer Science 
and Information Engineering,  Cheng-Kung University 
Tainan, Taiwan 30067, R.O.C. 
E-MAIL: m9104041@chu.edu.tw 
Xinying Zhang / Department of Management Science 
and Engineering in School of Economy and 
Management, Harbin Institute of Technology 
Harbin, China 
E-MAIL: alpha3068@sina.com 
Chong Wu / Department of Management Science  
and Engineering in School of Economy and Management,  
Harbin Institute of Technology 
Harbin, China 
E-MAIL: wuchong@hit.edu.cn
 
Abstract—The traditional Probabilistic Neural Networks (PNN) 
believes that all the variables have the same status, making the 
contour of probabilistic density function round. In this study, 
variable weights are added into the probabilistic density 
function of Elliptical Probabilistic Neural Network (EPNN), so 
that the kernel function can be adjusted into arbitrary 
hyper-ellipse to match the various shapes of classification 
boundaries. Although there are three kinds of network 
parameters in EPNN, including variable weights representing 
the importance of input variables, the core-width-reciprocal 
representing the effective range of data, and data weights 
representing the data reliability, in this study the principle of 
minimizing error sum of squares is used to derive the 
supervised learning rules for all the parameters with a unified 
mathematic theoretical framework. The performance of EPNN 
is testified and compared with MLP and PNN with 15 real 
classification applications. The results show that EPNN is more 
accurate than MLP and PNN. 
Keywords- probabilistic neural network, variable importance, 
classification, learning rule 
THEORY 
Probabilistic Neural Network (PNN) [1-6] has a wide 
range of applications in model identification, time series 
prediction, as well as fault diagnosis and other fields [7-11] 
and the algorithm also has a number of different variations 
[3, 8-14]. For example, Specht [3] pointed out that PNN 
learned quickly from examples in one pass and 
asymptotically achieved the Bayes-optimal decision 
boundaries. The major disadvantage of a PNN was that it 
required one node or neuron for each training pattern. 
Various clustering techniques were proposed to change this 
requirement into one node per cluster center.  
Berthold and Diamond [13] proposed a constructive 
training algorithm for probabilistic neural networks, a 
special type of radial basis function networks. In contrast to 
other algorithms, predefinition of the network topology was 
not required. The proposed algorithm introduced new 
hidden units whenever necessary and adjusted the shape of 
existing units individually to minimize the risk of 
misclassification. This led to smaller networks compared to 
classical PNNs and therefore enabled the use of large data 
sets. Song, et al. [8] proposed a modified probabilistic 
neural network (PNN) for brain tissue segmentation with 
magnetic resonance imaging (MRI). In this approach, 
covariance matrices were used to replace the singular 
smoothing factor in the PNN's kernel function, and 
weighting factors were added in the pattern of summation 
layer. Rutkowski [10] proposed a new class of PNN 
working in nonstationary environment. He formulated the 
problem of pattern classification in nonstationary 
environment as the prediction problem and designed a 
probabilistic neural network to classify patterns with 
time-varying probability distributions. Rutkowski [11] also 
proposed a new class of generalized regression neural 
networks working in nonstationary environment, and proved 
convergence of the General Regression Neural Network 
(GRNN) based on general learning theorems. 
The proposed EPNN has three kinds of network 
parameters: 
(1) The variable weights decide the shape of 
probabilistic density function so that the contour lines 
are not round but elliptical. The larger the variable 
weight of the variable, the more important the variable, 
and the smaller the radius of the ellipse in the direction 
of the variable; 
(2) The core-width-reciprocal is equivalent to the 
reciprocal of the smooth parameter in the traditional 
probabilistic density function, that is, the 
width-reciprocal of probabilistic density function. The 
larger core-width (the smaller core-width-reciprocal), 
the larger the effective range of the sample; 
(3) The data weight is equivalent to the height of 
probabilistic density function. The larger the height, 
the higher the credibility of the sample. 
In this study, the probabilistic density function is 
modified as follows, 
)
2
)(
exp( 2
1
22
2
p
m
i
p
iiip
pp
xxW
hf σ
∑
=
−
−=                            
(9) 
=ix  the i-th input variable of the unknown samples; 
=pix  the i-th input variable of the p-th sample in the 
sample base; =ipW  the i-th input variable weight of the 
p-th sample in the sample base, which decides the shape of 
the probabilistic density function so that the contour lines 
are not round but elliptical; =ph  the data weight of the 
p-th sample in the sample base, which is equivalent to the 
height of the probabilistic density function; =pσ  the 
smooth parameter of the p-th sample in the sample base, 
which is equivalent to the width of the probabilistic density 
function. 
 
∑
∑
=
=
⋅
= n
p
p
n
p
pp
f
tf
y
1
1
 
 
nf1f  2f  
1
1x  
2
1x  
1
2x  nx1  
nx2
2
2x  
•••••••  nn hσ22 hσ  11 hσ  
11W  
1t  2t  nt  
Output 
Unit
nW1 nW221W  12W  22W  
1x  2x  
Figure 1. Elliptical Probabilistic Neural Network 
The smooth parameter may be approaching to 0 in the 
learning process so as to make the denominator be zero. In 
order to avoid the problem, the core-width-reciprocal is used 
instead of the smooth parameter.  
 
Let  
p
pV σ2
1=                                                
(10) 
where =pV  the core-width-reciprocal of the p-th sample 
in the sample base, which is equivalent to the 
width-reciprocal of the probabilistic density function,  
 
then  
))(exp(
1
2222 ∑
=
−−=
m
i
p
iiipppp xxWVhf                   
(11) 
Let  
∑
=
−≡
m
i
p
iiippp xxWVD
1
222 )( ,          
(12) 
then  
)exp(2 ppp Dhf −=                                          
(13) 
The output of the network is the weighted summation 
of the known value of output variable of the samples in the 
training set, as shown in Figure 1. 
∑
∑ ⋅
=
p
p
p
pjp
j f
tf
y                                            
(14) 
where =pjt  the known value of the j-th output variable of 
the p-th sample of the training set. 
 
In order to derive the supervised learning rules to 
adjust the parameters in the equation above, let the error 
function be 
∑
=
−= n
j
jj ytE
1
2)(
2
1
                                       
(15) 
where n = the number of output variables; =jt  the 
known value of the j-th output variable of the training data; 
=jy  the inference value of the j-th output variable of the 
training data. 
The goal to adjust the network parameters is to 
minimize the error function. Therefore, the supervised 
learning rules of network parameters can be derived with the 
steepest descent method as follows. 
(1) Variable weights 
a set is chosen as the testing data while the remaining nine 
sets are used as the training data. The integrated 
performance of all the test data is used to assess the 
accuracy of each network model.  
 
Table 1. The descriptions of the data sets 
Data set Input Output Data
Iris 4 3 150 
Insurance Company 7 2 700 
Statlog (Shuttle) 9 3 5000
Glass Identification 9 6 214 
Vowel Recognition 10 11 990 
Wine 13 3 178 
Forest Cover Type 14 7 4000
Letter Recognition 16 26 2000
Image Segmentation 18 7 2310
Statlog (Vehicle Silhouettes) 18 4 846 
Statlog (German Credit) 19 2 1000
Statlog (Heart) 20 2 270 
Thyroid Disease 21 3 7200
Statlog (Landsat Satellite) 36 6 6000
SPAMBASE 57 2 4500
 
Table 2. The comparison of the error rates in the 
application examples 
Data set MLP EPNN PNN SVM
Iris 2.7 4 4 2.7
Insurance Company 36.49 34.48 35.63 33.65
Statlog (Shuttle) 0.35 0.3 0.33 0.4
Glass Identification 26.56 25.0 29.69 33.6
Vowel Recognition 41.13 42.64 48.48 40.52
Wine 1.72 0.0 3.45 0.56
Forest Cover Type 23.2 19.4 25.4 19.5
Letter Recognition 34.6 17.4 20.0 19.2
Image Segmentation 4.2 4.07 6.91 3.98
Statlog (Vehicle) 12.77 21.63 26.6 19.4
Statlog (German Credit) 25.67 25.1 27.0 23.65
Statlog (Heart) 15.71 12.86 15.71 15.87
Thyroid Disease 1.95 4.08 6.48 2.15
Statlog (Landsat Satellite) 5.09 7.17 10.04 8.7
SPAMBASE 3.63 5.11 12.33 6.6
Average 15.72 14.88 18.13 15.37 
 
Table 2 shows the results of the models established by 
EPNN, PNN, MLP [19], and SVM (support vector machine) 
[16-18], illustrating that EPNN is more accurate than MLP 
and SVM, and much more accurate than PNN.  
CONCLUSIONS 
This study proposes Elliptical Probabilistic Neural 
Networks (EPNN) which has three kinds of network 
parameters: variable weights representing the importance of 
input variables, the core-width-reciprocal representing the 
effective width, and data weights representing the data 
reliability. These three kinds of parameters can be adjusted 
through training. We tested 15 real applications, and 
compare EPNN with MLP, PNN, and SVM. The results 
showed that EPNN is more accurate than MLP and SVM, 
and much more accurate than PNN for the real classification 
applications.  
 
Acknowledgments  
This study is sponsored by the National Science Council 
(Project No. 99-2221-E-216-040). 
References 
1. Specht, D. F. “Probabilistic Neural Networks for 
Classification, or Associative Memory,” Proceedings of 
the 1988 IEEE International Conference on Neural 
Networks, San Diego, Vol. 1, pp. 525-535 (1988). 
2. Specht, D. F. “Probabilistic Neural Networks,” Neural 
Networks, Vol. 3, No.1, pp. 109-118 (1990). 
3. Specht, D.F.  “Enhancements to probabilistic neural 
networks,” 1992 International Joint Conference on 
Neural Networks, Baltimore, Vol. 1, pp. 761-768 
(1992). 
4. Patra, P. K., Nayak, M., Nayak, S. K., and Gobbak, N. 
K., “Probabilistic Neural Network for Pattern 
Classification,” Proceedings of the 2002 International 
Joint Conference on Neural Networks, Honolulu, Vol. 2, 
pp. 1200-1205 (2002). 
5. Parzen, E. “On Estimation of a Probability Density 
Function and Mode,” Annals of Mathematical Statistics, 
Vol. 33, No.3, pp. 1065-1076 (1962). 
6. Specht, D. F. “A General Regression Neural Network,” 
IEEE Transactions on Neural Networks, Vol. 2, No.6, 
pp. 568-576 (1991). 
7. Adeli, H. and Panakkat, A. “A probabilistic neural 
network for earthquake magnitude prediction,” Neural 
Networks, Vol. 22, No. 7, pp. 1018-1024 (2009).  
8. Song, T., Jamshidi, M.M., Lee, R.R., and Huang, M. “A 
Modified Probabilistic Neural Network for Partial 
Volume Segmentation in Brain MR Image,” IEEE 
Transactions on Neural Networks, Vol. 18, No. 5, pp. 
1424-1432 (2007). 
9. Yu, S. N. and Chen, Y. H. “Electrocardiogram beat 
classification based on wavelet transformation and 
probabilistic neural network," Pattern Recognition 
Letters, Vol. 28, No. 10, pp. 1142-1150 (2007). 
10. Rutkowski, L. “Adaptive probabilistic neural networks 
for pattern classification in time-varying environment,” 
IEEE Transactions on Neural Networks, Vol. 15, No. 4, 
pp. 811-827 (2004). 
11. Rutkowski, L. “Generalized Regression Neural 
Networks in Time-Varying Environment,” IEEE 
Transactions on Neural Networks, Vol. 15, No. 3, pp. 
576-596 (2004). 
12. Chen, N., Sun, F., Ding, L., and Wang, H. “An adaptive 
PNN-DS approach to classification using multi-sensor 
information fusion ,” Neural Computing & 
Applications, Vol. 18, No. 5, pp. 455-467 (2009). 
13. Berthold, M. R. and Diamond, J. “Constructive training 
of probabilistic neural networks,” Neurocomputing, Vol. 
19, No. 1, pp. 167-183 (1998). 
表 Y04 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：葉怡成 計畫編號：99-2221-E-216-040- 
計畫名稱：類神經網路演算法之改良(II) 
量化 
成果項目 
實際
已達
成數
（被
接受
或已
發
表） 
預期總
達成數
(含實
際已達
成數) 
本
計
畫
實
際
貢
獻
百
分
比 
單
位 
備註（質化說明：如數個計畫共同成果、成果列為該
期刊之封面故事...等） 
期刊論文 2 1 200% 
1. I-Cheng Yeh and Chung-Chih Chen (2011). ＇Hybrid 
Tranfer Function Networks,＇ Journal of Information 
Technology and Applications, Vol.5, No.1, 37-45.  
2. I-Cheng Yeh, Kuan-Cheng Lin, Kuan-Chieh Huang, 
Xinying Zhang, and Chong Wu (2011), ＇Elliptical 
Probabilistic Neural Networks,＇ Journal of Information 
Technology and Applications. (accepted) 
研究報告/技
術報告 1 1 
10
0%  
研 討 會 論
文 0 0 
10
0% 
篇 
 
論文著
作 
專書 0 0 100%   
申 請 中 件
數 0 0 
10
0%  專利 已 獲 得 件
數 0 0 
10
0% 
件 
 
件數 0 0 100% 件  技術移
轉 權利金 0 0 100% 
千
元  
碩士生 2 0 100%  
博士生 0 0 100%  
博 士 後 研
究員 0 0 
10
0%  
國
內 
參與計
畫人力 
（本國
籍） 
專任助理 0 0 100% 
人
次 
 
其他成果 
(無法以量
化表達之
成果如辦
理學術活
動、獲得獎
項、重要國
際合作、研
究成果國
際影響力
及其他協
助產業技
術發展之
具體效益
事項等，請
以文字敘
述填列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量
性) 0  
課程/模組 0  
電腦及網路系統或工
具 0 
 
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 
計畫成果推廣之參與
（閱聽）人數 0 
 
