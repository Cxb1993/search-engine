 
evaluated by simulations in Section 4. And 
Section 5 summarizes our results and 
outlines future directions. 
 
2. RELATED WORK 
Remote file systems have been actively 
developed for more than two decades, 
during which solutions to deal with 
communication overhead, file consistency, 
data duplication, etc. were proposed [4, 5, 
12, 15]. 
Early NFS [13] allow users to mount file 
systems across the network and use them as 
if they were local directories. NFS’s data 
write-back strategy closely mimics 
traditional file system’s write() procedure. 
The client repeatedly sends write() 
operations to the server when user updates a 
file. Since those write operations are 
synchronous requests, the client cannot 
consistently perform well if it does not 
receive acknowledgements. Especially 
when the network has low bandwidth or 
suffers high latency, the system 
performance deteriorates drastically. 
The AFS [6] uses the write-back-on-close 
approach to improve remote file accessing 
performance. When a file is being edited, all 
write operations only update the cached 
local copy. The changes are buffered at the 
client side until the file is closed, then the 
client transfers the new file to the server. To 
ensure that a client’s cache does not contain 
stale data, the file server records the 
location of all file replicas. If the file has not 
been modified on the server on file opening, 
the client does not need to fetch the file 
from the server. Instead, it only has to verify 
the file recency and directly operate on the 
local copy. 
To support operations in weaker network 
connections, Coda [7] explores 
asynchronous write-back [9] to update files. 
Coda records changes in a local log instead 
of propagating updates to the server and 
immediately responding to update requests. 
Updates are asynchronously sent to the 
server whenever available bandwidth exists. 
To reduce the amount of data that must be 
flushed, a Coda client tracks the log to 
eliminate reversed file operation. Although 
asynchronous write-back does optimize the 
amount of transporting, the status recorded 
at the server is stale, thus lagging the status 
at clients. Stale server status results in 
consistency problems that complicate data 
sharing. When an update conflict occurs, 
user intervention is needed. Also, servers 
often do not record complete update history 
in systems with log optimizations, and 
support for properties like version control 
are difficult [10]. [8] extends Coda to 
support operation-based updates. The 
clients move operation-based updates to 
network surrogates maintaining strong 
connections to the server. The network 
surrogates perform file updates and relay 
acknowledgements back to weakly 
connected clients. Using this approach, data 
transmission mostly occurs between 
strongly connected devices. Network 
surrogates usually are trusted components. 
The file operation types that can be 
performed are limited by the need of data 
encryption at clients if network surrogates 
belong to an untrusted infrastructure. 
LBFS [11] is a network file system 
designed for updating files over slow or 
wide-area networks. It uses a persistent 
cache to provide AFS-like close-to-open 
consistency. Once a user has written and 
closed a file, other users can always read the 
new version during simultaneously 
accesses.   
popular connection options for residential 
users and some small businesses. Although 
wired networking technologies provide 
higher bandwidth and lower latency, today’s  
users in fact rely heavily on wireless 
technologies to access computing resources. 
Table 2 lists some popular network 
technologies and their networking 
capabilities in reality. Recent WiMAX 
technology supports bandwidth of 144Mb/s. 
Newer standards of Wi-Fi promises 
bandwidth from 54Mb/s to a maximum of 
600Mb/s. Nonetheless, actual service levels 
[3,14] dramatically deteriorate in practical 
wireless networking environments when the 
population of users increase in the same 
area. [8,11] studied networking issues in the 
case of remote storages. 
 
Table 2: Common networking technologies 
Network 
types 
Bandwidth 
(Kb/s) 
Latenc
y (ms) 
100Gbase-X 10
8
 1 
DOCSISv3.0 
160,000/120,00
0 
30 
ADSL2+ 24,576/3,584 10 
802.11n 288,900 10 
802.16e 144,000/35,000 10 
UMTS/SGS
M 
384/64 100 
 
Network file systems are usually 
deployed in local area networks (LAN) with 
better and stable connecting conditions, 
where users in the same LAN can 
efficiently manipulate files on remote 
storage. In wide area networks, the last-mile 
connection is usually the bottleneck of data 
transmission. Users performing updates to 
remote files often suffer from unstable and 
slow connections. 
This paper proposes a remote file 
accessing method for mobile users to 
securely and efficiently update files on the 
server. Figure 1 illustrates our targeted 
application scenario, where users usually 
access files via relatively weak connections.  
To reduce the updating latency of file 
operations in a remote file access system, 
LBFS uses the AFS-like 
write-back-on-close strategy. A file is 
updated to the server after a client finishes 
editing and closes the file. Rabin hashing 
helps reduce the amount of data to be 
transmitted to complete an update. However, 
the time needed to perform the update may 
still be too long in the case of collaborative 
file editing. 
To further reduce the latency of the 
completion of a file update, our proposed 
method considers users’/applications’ 
editing patterns and starts updating 
continuous file segments to server while the 
file is still being edited. Rabin hashing is 
used on modified file blocks to determine 
the actual data to be transmitted. 
 
 Figure 1: Cloud storage file accessing 
scenario  
While using Rabin hashing, if a user 
modifies data located inside the sliding 
window that hashes to a block boundary, the 
boundary fingerprint is changed. The 
server-side. 
4.1 Environment Configuration 
In our simulations, a client performs a 
sequence of random editing to a remote text 
file. A client edits contiguous file segments 
of random lengths. Each segment has a 
random starting position in the file. The 
above editing behavior creates the most 
communication and storage overheads, 
while a user often edits files more rationally 
in reality. 
The block size is set between 4KB and 
16KB when applying Rabin hashing. The 
simulations further assume a CBC-mode 
128-bit block cipher is employed to encrypt 
file content. The connection is configured to 
simulate W-CDMA’s bandwidth with the 
upload and download speeds at 64Kb/s and 
384Kb/s, respectively. 
4.2 Total Data Transmission 
This batch of simulations compares the 
amount of transmitted data of AFS, LBFS, 
and EDAP, given the same editing sequence. 
In AFS, data is sent to the server only after 
the client issues the close() request. In 
LBFS, although data is updated upon 
closing the file, only edited file segments 
are transmitted. In EDAP, contiguous 
blocks are sent whenever the client moves 
on to edit another disjoint file segment. 
Files of different sizes are tested. For 
each file size, 50 different random editing 
sequences are used. The simulations record 
the average amount of transmitted data of 
the three approaches. Figure 2 shows the 
simulation results. The x-axis shows the file 
sizes used in the simulation, and the y-axis 
indicates the average transmitted data 
amount for each remote file accessing 
approach. 
In EDAP, when random editing is 
performed, a block can be updated to the 
server multiple times since the editing steps 
may move among disjoint file segments 
repeatedly. Therefore, the amount of 
transmitted data increases. As shown in 
Figure 2, when EDAP and LBFS perform 
much better with increased file size, EDAP 
produces slightly more communication 
overhead than LBFS. However, when the 
block updating order is more organized, the 
overhead is expected to decrease. Also, the 
slight overhead allows the server to receive 
the revised file much earlier, which implies 
that a collaborative software system using 
EDAP may be more responsive for users to 
exchange documents. 
 
Figure 2:Total data transmission 
4.3 File Closing Latency 
The refresh time after closing a file is 
compared between EDAP and LBFS. The 
simulations are conducted using various 
number of editing operations and file sizes. 
All data needed be updated is uploaded after 
close() is issued in LBFS. In EDAP, a 
segment of blocks is uploaded whenever the 
current position of editing is located in a 
block disconnected from this segment. For 
each file size and number of editing 
operations in the simulations, 50 different 
of blocks in a file is maximized) and data 
blocks overlap in the maximum size, the 
storage overhead satisfies the following 
inequality:  
MinBS
BSBC
MaxPO
12 
               (2) 
In Equation 2, MaxPO is the maximum 
ratio of redundant data in the storage usage 
of a file, BSBC is the block size used by the 
block cipher, and MinBS is the minimum 
allowable file block size of the 
fingerprinting scheme. When a 128-bit (16 
bytes) block cipher is employed and the 
minimum file block size is set as 4KB (4096 
bytes), which was used in the simulations in 
this section, the amount of redundant data is 
no more than 0.76%. 
Using the same file size and number of 
editing operations in the previous 
simulation, the amount of redundant data is 
estimated and shown in Figure 4. Figure 4 
indicates that with a fixed number of editing 
operations, the amount of redundant data 
initially increases with the file size. This 
because the size of a cipher block becomes 
less significant as the number of blocks 
grows larger. Therefore, when the file takes 
a certain size of storage, the amount of 
redundant data begins to decrease. 
Figure 4: Storage overheads at the server. 
 
It can be observed from Figure 4 that the 
amount of redundant data, as expected, 
never exceeds the worst estimation, 0.76%, 
in all cases of the simulation. In fact, when 
the same file block is updated at different 
times, data is truncated at the client to 
remove the redundant portion. Therefore, 
the amount of redundant data does not 
accumulate endlessly over time. 
5. CONCLUSION AND FUTURE 
WORK 
This study proposes a remote data 
accessing scheme called EDAP that aims at 
improving the responsiveness of remote 
storage services. Similar to other file 
patching systems, EDAP updates only 
changed file blocks to the server. However, 
EDAP balances the pure synchronous 
updating and the write-back-on-close 
updating methods by synchronizing 
segments of consecutive file blocks ahead 
of closing a file. Through the simulations, it 
can be observed that EDAP greatly 
improves the responsiveness of remote file 
access. Although the improvement comes 
with a communication overhead, the 
overhead is only slightly over the previous 
write-back-on-close approach and is 
acceptable. 
EDAP also provides a solution to the case 
of untrusted network infrastructure, for 
example, cloud storage services, by 
allowing the server stores only file content 
encrypted by client to protect client privacy. 
To maintain data integrity, there is an 
induced storage overhead which can be 
bounded depending on the block size and 
the employed block cipher. Using the 
parameter settings in the simulations, the 
storage overhead is bounded within 0.76%. 
Moreover, due to the fact that redundant 
storage are used only at the beginning of file 
implementation of the sun network filesystem,” 
Proceedings of the Summer USENIX 
Conference, 1985, pp. 119–130. 
[14] K. Xu, S. Bae, S. Lee, and M. Gerla, “Tcp 
behavior across multihop wireless networks and 
the wired internet,” Proceedings of the 5th 
ACM International Workshop on Wireless 
Mobile Multimedia, 2002, pp. 41–48. 
[15] H. Yu and A. Vahdat, “Design and 
evaluation of a continuous consistency model 
for replicated services,” Proceedings of the 4th 
Symposium on Operating System Design and 
Implementation, 2000. 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：項天瑞 計畫編號：99-2221-E-011-060- 
計畫名稱：適用於行動裝置的遠端檔案儲存與分享系統 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 4 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
