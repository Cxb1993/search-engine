多張影像間幾何投射分析與其運用於強健式特徵點抽取及相機投
射矩陣估測 
Multiple-View Image Projection Analysis and Its Use for Robust Feature Point Extraction and 
Camera Projection Matrix Estimation 
計畫編號：NSC-94-2213-E-009-127- 
執行期限：94 年 8 月 1 日至 95 年 7 月 31 日 
  主持人：陳稔教授 / 國立交通大學資工系 
E-mail: zchen@csie.nctu.edu.tw Tel: 03-5731875 
1. 摘要 
在電腦視覺研究中，我們要針對二張或更多張
不同視度下所拍攝的景物影像，進行三維景物幾何
形狀以及相機相互之間的幾何位置的推測典型的
視覺重建步驟包括：  
(1) 抽取影像中具有區辨能力的特徵點或特徵
線。 
(2) 尋找不同影像間的對應特徵點或線。 
(3) 估算影像的 epipole 及該相機的等效投影
矩陣。 
(4) 估算景物的三維投影幾何結構(projective 
structure)。 
(5) 另一方面也可以運用多於二張的影像中的
對應像點進行像點矩陣(data matrix D)的
矩陣分解(matrix factorization)以獲得三維
景物的幾何結構及相機內外參數資料。 
我們將先建立影像間幾何轉換矩陣 H 。其次，
我 們 將 矩 陣 H 分 解 為 三 個 基 本 型 矩 陣
H = tH aH pH ，並推導 pH , aH , tH 對影像作用
的幾何特性，以瞭解
pH , aH , tH 對影像所產生的
影響。我們再針對矩陣H 的幾何影響，考慮如何設
計影像上特徵點(feature points) 的抽取，以使得部
份特徵點在矩陣 H 的作用下，仍然能維持大致不
變，以做為兩張影像間的點對應之用。我們將分析
採用 Gabor filtering 技術做特徵點擷取 (feature 
extraction)的適當性及濾波器的參數如何設計，以
便所抽取的特徵點向量在前述 pH , aH 及 tH 的轉
換下不致全部被破壞掉，再用這些對應像點估算
Fundamental matrix 及相機的等效投影矩陣，並求
三維幾何結構。若能拍攝到足夠張的影像及獲得足
夠的對應點數，用對應點的 data matrix 做 matrix 
factorization，求取相機投射矩陣及三維幾何結構，
以觀察估測的準確度是否可提高？我們將做電腦
模擬，以印證本計劃所發展的理論及瞭解其的實用
性。 
關鍵詞：相機投射矩陣，影像轉換，投射變形，
旋轉，位移，多尺寸，多角位，嘉寶濾波器，強
健特徵點，基本矩陣，矩陣分解 
Abstract: 
 In computer vision, two or more images taken 
under different viewpoints are used to estimate 3D 
projective structure and camera models. Typical steps 
in such a reconstruction task include: 
(1) Extraction of feature points with discriminative 
power 
(2) Point matching between image pairs 
(3) Computation of eipoles and canonical projection 
matrices 
(4) 3D projective structure determination 
(5) Factorization method for obtaining projective 
structure and motion from multiple views 
 We start with the computation of transformation 
matrix between the two given images. The 
transformation matrix is then decomposed into three 
elementary projection matrices: pH , aH , tH . We 
[33]-[35]。我們也將利用所獲得的精確對應點群，
做此種方式之重建，看是否能獲得更好的結果？ 
本計畫主要工作內容包括對特徵點抽取及比對兩
項問題提出更好辦法，再用這些對應點來估算
Fundamental matrix、相機投影矩陣及三維重建，最
後評估所得結果。工作將由影像間轉換矩陣的分析
下手。轉換矩陣可依其幾何複雜度分為 projection 
matrix -> affine matrix -> similarity matrix 三層
[30]。我們將轉換矩陣分解為上述三種基本型矩陣
（elementary matrices）的乘積。然後，分析各個基
本型矩陣對影像圖形變形的影響。再由這些分析去
設計特徵點抽取及比對方法。我們將用實際影像做
電腦模擬，以印證所發展的理論及實用性。 
 
3. Geometric properties of projective matrix 
我們將先建立影像間之幾何轉換矩陣，若
scene 距離相機遠大於景物本身尺寸時，兩張景物
的投射可近似為 weak-perspective projection，則 
{ [ ]
⎪⎩
⎪⎨
⎧
+=
+=⎥⎦
⎤⎢⎣
⎡=
×
'''
1
 |
3
3
3
12
bPAu
bPA
P
bAu
vvv
vvvvv  
令 ( ) 30330 QAPPAuuq vvvvvv =−=−= , ; 03P ,v is a specified point 
 ( ) '''''' 30,330 QAPPAuuq vvvvvv =−=−=  
 ( ) qAAAQ TT vv 13 −=  
 ( ) {
12
1''
×
−= qAAAAq TT vv  
 
{
( ) ( )
{
13
33
11
13
110
0'
1
'
1
'
××
−−
×
⎟⎠
⎞⎜⎝
⎛
⎟⎟⎠
⎞
⎜⎜⎝
⎛=⎟⎟⎠
⎞
⎜⎜⎝
⎛=⎟⎠
⎞⎜⎝
⎛ qAAAAqAAAAq
H
T
TTTT v
444 344 21
v
rvv  
或 
 uHu vv 33' ×=  
所以上述兩張影像之間的轉換 H 是一個 affine 
matrix。若再考慮 perspective 投影因素，上述H 矩
陣的 31h 及 32h 不再為零，唯值很小，故成為一個通
常的 projective matrix。 
當有了兩張圖存在一個 projective 轉換矩陣 
uHu vv =' 時，我們將探討如何尋找強健的特徵點？我
們探討的方法是將 projective matrix H 分解成三個
矩陣： H = Ht Ha Hp 
假設 
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
=
13231
232221
131211
hh
hhh
hhh
H
.  
定義 
⎥⎥⎦
⎤
⎢⎢⎣
⎡
⎥⎥⎦
⎤
⎢⎢⎣
⎡
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
= ×
1
0
0
10
01
100
0
0
100
10
01
3231
22
23
13
hh
Ah
h
H  
then  
[ ]3231
23
13
2221
1211 hh
h
h
hh
hh
A ⎥⎦
⎤⎢⎣
⎡−⎥⎦
⎤⎢⎣
⎡=  
⎥⎦
⎤⎢⎣
⎡
−−
−−=
322322312321
321312311311
hhhhhh
hhhhhh  
因此，可獲得此 3 個基本 projective matrices 如
下： 
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
=
100
10
01
23
13
h
h
Ht
, 
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
−−
−−
=
100
0
0
322322312321
321312311311
hhhhhh
hhhhhh
H a , and  
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
=
1
010
001
3231 hh
H p
.  
以下我們逐一分析此 3 個基本 projective 
matrices 對影像變形所產生之幾何影響。 
 
3.1 Ht對影像變形所產生之幾何影響 
Ht: ⎥⎦
⎤⎢⎣
⎡+⎥⎦
⎤⎢⎣
⎡=⎥⎦
⎤⎢⎣
⎡
32
31
'
'
h
h
v
u
v
u . 
矩陣 tH 的影響則是平移性的（translation），
故不會造成影像尺寸或形狀的改變。 
 
3.2 Ha對影像變形所產生之幾何影響 
⎥⎦
⎤⎢⎣
⎡⎥⎦
⎤⎢⎣
⎡
−−
−−=⎥⎦
⎤⎢⎣
⎡
v
u
hhhhhh
hhhhhh
v
u
322322312321
321312311311
'
'  
transformed into a line of the 
form ( ) ( ) c'vchb'ucha =+++ 3231 . 
 
Lemma 2: Under the projective transformation 
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
=
1
010
001
3231 hh
H p
 the vanishing point of a set of 
parallel lines: {au + bv = ci; i = 1, 2, 3,..}is given by 
⎥⎦
⎤⎢⎣
⎡
−−
−=⎥⎥⎦
⎤
⎢⎢⎣
⎡
∞
∞
ahbha
ahbhb
v
u
'
'
3231
3231  
 
Lemma 3: Under the projective transformation 
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
=
1
010
001
3231 hh
H p
the set of parallel lines of the form 
{h31u + h32v = ci; i = 1, 2, 3,..} is transformed into a 
set of parallel lines of the form {h31u’ + h32v’ = ci 
/(1+ci) ; i = 1, 2, 3,..}. This is the only set of parallel 
lines that remains parallel after the transformation. 
 
Lemma 4:  
The set of parallel lines perpendicular to a set of 
parallel lines: {
icv
hh
hu
hh
h =
+
+
+ 232231
32
2
32
2
31
31 ; i = 1, 
2, 3,..}, where ci is the inter-distance between two 
neighboring lines, is of the form 
{
idv
hh
hu
hh
h =
+
−
+ 232231
31
2
32
2
31
32 ; i = 1, 2, 3,..}.  
 
Lemma 5:  
  Define a new orthogonal coordinate system (a, b) 
in the (u, v) orthogonal coordinate system according 
to Hp: 
 
( )
( )
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎣
⎡
++−
++
=
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎣
⎡
++−
++
=
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
1100
0//
0//
1
/
/
1
2
32
2
3131
2
32
2
3132
2
32
2
3132
2
32
2
3131
2
32
2
313132
2
32
2
313231
v
u
hhhhhh
hhhhhh
hhvhuh
hhvhuh
b
a
 
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎣
⎡
++
+−+
=
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
1100
0//
0//
1
 then,
2
32
2
3131
2
32
2
3132
2
32
2
3132
2
32
2
3131
b
a
hhhhhh
hhhhhh
v
u  
And 
=
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
++
++
=
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
=
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
1
1
1
11
3231
3231
vhuh/v
vhuh/u
  v
u
H  'v
'u
p
( )
( )
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
⎟⎠
⎞⎜⎝
⎛ ++++
⎟⎠
⎞⎜⎝
⎛ +++−
1
1/
1/
2
32
2
31
2
32
2
313132
2
32
2
31
2
32
2
313231
hhahhbhah
hhahhbhah
 
Property 3: 對於任何一個
pH , reference image 上
平行於消失線方向 31
32
h
h
⎛ ⎞⎜ ⎟⎝ ⎠
的線，在 transformed image 
上仍然會與消失線平行, 而原先垂直於消失線方向
32
31
h
h
−⎛ ⎞⎜ ⎟⎝ ⎠
的線，在 transformed image 上則會造成 fore- 
shortening 現象, 而交於一個消失點。 
圖 2(a)中之草綠色為以原 image 邊界做正方形
切割之 grid，其經 pH 轉換後如 Fig. 2(b) 之草綠
色，此轉換之 grid，變化沒有規則性，並不利於分
析。依 Property 3，我們依消失線方向 31
32
h
h
⎛ ⎞⎜ ⎟⎝ ⎠
、 32
31
h
h
−⎛ ⎞⎜ ⎟⎝ ⎠
之座標軸重新切割 grid 上，如圖 2(a) 紅色虛線所
示，此 grid 經 pH 轉換後如圖 2(b) 之紅色虛線。 
 
(a) (b) 
圖 2 pH 對影像變形所產生之幾何影響 
我們可以很明顯的觀察到如 Property 3 中說明
之 pH 幾何特性： 
(1) 垂直於消失線主軸方向之平行線間距會持續
像特徵點，則所要的特徵點必須是由局部性的結構
（local structure pattern）上出發。其次，全域中有
不同的尺寸變化，故特徵點要不受 scaling 的影響。
除此之外，還要考慮在同一張影像先天具有大小不
同的特徵結構，故特徵點的抽取不能只針對固定一
種 視 窗 做 運 算 。 它 必 須 是 multi-scale 、 
multi-orientation 的。 
(1) 為抓出各種不同尺寸物件結構之特徵，及因
應上述 projective transformation 下 scaling 
及 rotation 變化會因位置不同而不同之現
象，我們提出一個 scale/orientation-adaptive 
Gabor filtering technique, 此方法係基於
multi-scale, multi-orientation 之觀念，且每一
local pattern 具 有 dominant scaling 及
dominant orientation 的特性。 
4.1 Important Properties of the Gabor-filtered 
Image 
Gabor function： 
)exp(]})'()'[(exp{
2
1),( 222
,, xjwyxyxg s
sss
lNs ′+−= ασσπασ
  
where 
     ⎥⎦
⎤⎢⎣
⎡⎥⎦
⎤⎢⎣
⎡
−=⎥⎦
⎤⎢⎣
⎡
y
x
y
x
ll
ll
θθ
θθ
cossin
sincos
'
'  
with lθ being the orientation parameter; sσ  and 
sασ are the Gaussian window size parameters (α = 1 
is assumed here); sω  is the spatial frequency 
parameter. A normalization condition is usually 
imposed on the parameters sσ  and sω  such that 
4/πσ Nwss = (Here 4 sσ  is used to approximate the 
Gaussian window size, which is also called the filter 
size) for all scale indices s. 
 
(a) 
 
(b) 
Fig. 4 (a) A set of even-symmetric Gabor filters. (b) 
The plot of a typical even-symmetric Gabor filter 
with N = 3. 
Let I(x,y) be the input image function. For 
multiple scales s, s ∈ {1, 2, 3, …, S}, and multiple 
orientations lθ , θθ Δ×= ll , l = 1, 2,….., L (π is a 
multiple of θΔ ), the filter responses or outputs are 
given by the convolution operations:  
),(*),(),( ,,,, yxgyxIyxR lNseven
lNs =   
 
In the following, we give the important properties of 
Gabor-filtered image that lead to the robust feature 
points. 
Property 6: Let ),(ˆ ,, yxR lNs  and ),(,, yxR lNs  be the 
filter responses to the images ),(ˆ yxI  and ),( yxI . If 
),(ˆ yxI  is obtained from ),( yxI by a rotation through 
an angle φ in the counter-clock direction, 
i.e., ),(),(ˆ yxIyxI ′′= for points ),( yx and 
),( yx ′′ that are related by 
⎟⎟⎠
⎞
⎜⎜⎝
⎛
⎟⎟⎠
⎞
⎜⎜⎝
⎛
−=⎟⎟⎠
⎞
⎜⎜⎝
⎛
y
x
y
x
φφ
φφ
cossin
sincos
'
'  
each candidate dominant point obtained so far is 
also a local maximum in a neighborhood of size 
m×m centered at the point. 
4.3 The feature vector representation of 
Gabor-based feature point 
We use a feature vector 
ip
V
r
consisting of L filter 
responses to a set of Gabor filters with an incremental 
orientation step of π/L to characterize the local pattern 
around the feature point ( , )i i ip x y . The principal scale 
i
d
ps  and the principal orientation i
d
pθ  are iteratively 
tuned to a high accuracy. The mathematical form of a 
Gabor-based feature point is given as 
1 1( , ) ( , )( , )[ ( , ), ( , ), ( , )]
d d d dd d p p p pi i i ip pi i
i
Ls ss TL L
p i i i i i iV R x y R x y R x y
θ π θ πθ −+ +=r K
The similarity ( , )
i jp q
S V V
r r  between feature points 
ip
V
r
and
jq
V
r
 can be measured by the normalized cross 
correlation: 
)()(
))(())((
),(
jjii
jjii
ji
qqpp
qqpp
qp
VmeanVVmeanV
VmeanVVmeanV
VVS rrrr
rrrr
rr
−⋅−
−⋅−=  
5. 實驗結果 
首先，我們使用 synthetic data 來分析 image 上
任一位置對變形之抵抗性，實驗是將圖 6(b) 之
pattern 經過特定角度旋轉後放在圖6(a)background 
image 上之特定位置，產生一張 reference image(如
圖 7(a))，此 reference image 經 H 轉換後產生一張
sensed image(如圖 7(b))，在此二張 image 上分別去
抓特徵點，產生 feature vector 作比對。 
 
      
(a)               (b) 
圖 6 (a)Background, (b)pattern 
 
(a)                 (b) 
圖 7(a)Reference image, (b)sensed image 
圖 8 為分別在各種位置上將 pattern 於 0~360 度
間每隔 15 度作一次旋轉，reference 及 sensed 影像
上每組對應的特徵點作特徵比對，對於比對係數大
於 0.9 者，我們顯示其 dominant orientation，以觀
察各種不同位置之影像變形對特徵點存活率之影
響，實驗結果可歸納如下： 
(1) 對於趨近於 isotropic scaling 變形的地方(如左
上角點編號 1)，所有方向特徵點均能存活。 
(2) anisotropic scaling 變形越嚴重可存活的方向越
少，特徵點存活的方向與被壓縮變形的方向垂
直。 
1 2 3 4 5
6 7 8 9 10
11 12 13 14 15
16 17 18 19 20
21 22 23 24 25
1
2
3
4
5
6 7 8 9 10
11 12 13 14 15
16 17
18 19 20
21
22 23
24 25
 
(a)                 (b) 
圖 8(a)Reference image, (b)sensed image，顯示比對
係數大於 0.9 者之特徵點方向 
圖 9 展示特徵點編號 1(影像中之左上角點)及
編號 25(影像中之右下角點)各種不同方向之特徵
向量比對係數。 
  0.2
  0.4
  0.6
  0.8
  1
30
60
90
120
150
180 0  
  0.2
  0.4
  0.6
  0.8
  1
30
60
90
120
150
180 0  
Trans. PAMI，vol. 14，no.4 (1992)，pp.430-449. 
2. A. Garrido，P. D. L. Blanca and M. Garcia 
Silvente ， “Boundary simplification using a 
multiscale dominant-point detection 
algorithm，” Pattern Recognition，vol.31，No.6 
(1998)，pp.791-804. 
3. J. Fayolle，L. Riou and C. Ducottet，Robustness 
of a multi-scale scheme of feature points 
detection，Pattern Recognition，vol.33(2000)，
pp1437-1453. 
4. L. Kitchen and A. Rosenfeld. Gray-level corner 
detection. Pattern Recognition Letter. (1982)，
pp.95-102.  
5. J. A. Noble，Finding corners，Image Vision 
Comput. 6(1988)，pp.121-128. 
6. C. H. Chen，J. S. Lee and Y. N. Sun，Wavelet 
transform for gray-level corner detection ，
Pattern Recognition， vol.28， no.6(1995)，
pp.853-861 
7. Q. Zheng and R. Chellappa，Automatic feature 
point extraction and tracking in image 
sequences for arbitrary camera motion. 
International Journal of Computer Vision. 
Vol.15(1995)，pp.31-76. 
8. B. S. Manjunath ， C. Shekhar and R. 
Chellappa，A new approach to image feature 
detection with application ， Pattern 
Recognition，vol.29，no.4 (1996)，pp627-640. 
9. J. W. Hsieh，H. Y. Liao，K. C. Fan，M. T. Ko 
and Y. P. Hung， Image Registration Using a 
New Edge-based Approach，Computer Vision 
and Image Understanding，Vol.67，No.2(1997) 
pp.112-130. 
10. B. Robbins and R. Owens，2D feature detection 
via local energy. Image Vision and Computing 
15(1997)，pp 353-368. 
11. M. Trajković and M. Hedley，Fast corner 
detection，Image and Vision Computing，vol.16 
(1998)，pp.75-87. 
12. J. Chen，Y. Sato，and S. Tamura，Orientation 
space filtering for multiple orientation line 
segmentation. IEEE Trans，PAMI，vol. 22，no. 
5 (2000)，pp. 417-429. 
13. A.P. Witkin，Scale-space filtering. In proc 8th 
Int. Joint Conf. Artificial Intell.，1983，pp. 
1019-1021. 
14. R. Mehrotra ， K.R. Namuuduri ， and N. 
Ranganathsn ， “Gabor filter –based edge 
detection，” Pattern Recognition，Vol. 25，pp. 
1479-1494，1992. 
15. A.K. Jain，N.K. Ratha，and S. Lakshmanan，
“Object detection using Gabor Filters，” Pattern 
Recognition，Vol. 30，pp. 295-309，1997. 
16. Z. Wang and M. Jenkin，Using complex Gabor 
filters to detect and localize edges and bars. In: 
C. Archibald and E. Petriu，(eds.): Advanced in 
Machine Vision: Strategies and Applications，
vol. 32，River Edge，NJ: World Scientific ，
1992，pp. 151-170. 
17. R.P. Wurtz and T. Lourens，Corner detection in 
color images through a multiscale combination 
of end-stopped cortical sells. Image Vision and 
Computing 18(2000)，pp.531-541. 
18. B.S. Mahjunath and W.Y. Ma ， “Texture 
features for browsing and retrieval of image 
data”，IEEE TPAMI，pp. 837-842. 1996. 
19. J.G. Daugman ， Two-dimensional spectral 
analysis of cortical receptive field profile. 
Vision Research，vol. 20(1980)，pp. 847-856. 
20. J.G. Daugman ， Uncertainty relation for 
resolution in space， spatial frequency，and 
orientation optimized by two-dimensional 
visual cortical filters. J. Optical Soc. Amer.，vol. 
2，no.7(1988)，pp. 1169-1179. 
  
   
   
   
   
   
Figure 12(a). The multiple images od a human head with extracted feature points. 
 
 
 
 
   
