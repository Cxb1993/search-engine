1 
 
整合網路封包安全檢測系統及多核心封包擷取模組 
(An Integrated NIDS with Multi-Core Aware Packet Capture for Multi-Gigabits 
Networks) 
 
國科會產學合作計畫 
合作企業：文佳科技股份有限公司 
 
主持人：王勝德 教授 
國立台灣大學 電機系 
研究生：許嘉豪  周暉豪   林顥宗 
2011. 07. 28. 
摘要 
 
隨著網路頻寬速度與網路惡意入侵攻擊的倍增，網路安全已經成為一嚴重的議題，使用
網路入侵檢測系統(NIDS)也成為了在資料伺服器前端不可或缺的設備。然而，單一核心的處
理器的效能並沒有持續成長，其已無法因應目前的網路速度與持續增加的規則比對庫(rule set)。
本報告主要提出一個嵌入式多核心網路安全入侵檢測系統，根據不同的乙太網路卡硬體架構
特性，提出了 Flow Ring與 MCA Ring兩種不同的架構，並包含了三項特點： 1) 整合了高速
網路封包擷取模組與網路入侵檢測系統； 2) 運用了封包零拷貝技術來減少在收集網路封包時
所造成的額外負擔； 3) 使用了中斷與程序的處理器偏好設定來提高系統處理效能。實驗數據
中呈現了網路入侵檢測系統在不同架構下的封包檢測系統中的效能差異，實驗的結果呈現了
本論文所提出的設計方法於高頻寬網路系統下明顯地提升系統檢測整體的效能。 
 
Abstract 
Network security has been a serious problem in the Internet. To face this issue, network intrusion detection tools have 
become indispensable for computer systems and network gateways. In this paper we propose an embedded, multi-core 
aware network intrusion detection system (NIDS), which has the following features: 1) It integrates a novel multi-core 
aware packet capture module, called the MCA ring, and an NIDS. 2) It exploits a zero-copy mechanism to remove the 
overheads of packet copy processing from the network interface driver to the NIDS application.  3) It uses the concept of 
process and IRQ affinity to enhance the processing speed. The performance of NIDS under different packet capture 
modules in multi-gigabits networks has also been analyzed and presented in this paper. The results show that our 
integrated multi-core aware MCA ring and NIDS is effective for detecting network intrusion attacks in multi-gigabits 
networks. 
 
 
1. Introduction 
Security concern has been emerged as a main issue in Internet applications. Network attacks and threats are 
dominating much of network traffic over the ever increasing communication bandwidth, which has increased from 
100Mbps to 10Gbps during the last decade. Snort, a well-known network intrusion detection system (NIDS), is developed 
for identifying possible network attacks by using large rule sets, which are also known as attacking signatures that 
describe some malicious network traffic. However, the rule sets are continuously updated and become more and more 
sophisticated; on the contrary, the performance of a single CPU is not growing enough to meet the requirements of 
3 
 
library. The advantages of PF_Ring are that packets are still sent to the kernel network stack and it is driver-independent. 
However, PF_Ring does not take advantages of the new features of multi-core processors and multi-RX queues.  
To improve the performance of PF_Ring, it is necessary to modify the network device drivers. TNAPI   just makes use 
of this concept. As shown in Figure 4, TNAPI uses many kernel threads to handle the incoming packets. In this approach, 
it needs to modify the NIC device driver to generate the kernel threads for polling the incoming packets, and then the 
packets will be sent to PF_Ring. TNAPI improves scalability by avoiding using locks, but has the following drawbacks.  
1) It uses kernel threads, which are handled by the system scheduler, to fetch packets, resulting in system overheads.  
2) To co-working with PF_Ring, it needs at least one memory copy from the socket buffer to PF_Ring’s ring buffer.  
We primarily concentrate our approach on developing an efficient and embedded NIDS with multi-core aware packet 
capture for network security of a real-time multi-gigabit network. The integrated NIDS will be designed as an embedded 
device with multi-core aware packet capture, packet acquisition module, and packet reassembly plugin to an application 
level NIDS tool, Snort [3]. To the end of speeding up packet capture operations, we can customize the systems and 
remove some redundant processes, such as the protocol handler and the kernel threads. The incoming packets are directly 
sent to our ring buffers from NIC by DMA, and the ring buffers are directly mapped from kernel space to user space. 
Finally, the overheads of the resultant NIDS are greatly reduced. 
We will demonstrate the performance of the proposed embedded NIDS through experiments. The results show that the 
effectiveness and the applicability of the proposed approach in multi-gigabits network environments.  
 
 
Figure 3: PF_Ring with NAPI architecture 
 
 
Figure 4: PF_Ring with TNAPI architecture 
 
3. Multi-Core Aware Ring Architecture 
In the proposed method, several pairs of multiple receiving (RX) queues and multi-core aware ring buffers, called the 
MCA Ring, are used to receive packets, depending on how many cores are involved. As shown in Figure 5, packets in 
NIC’s RXs are directly transferred to the MCA Ring buffers by DMA. All the MCA Ring buffers are mapped and 
5 
 
interrupts are assigned to core0 and processes are scheduled by system scheduler. As a result, core0 will be overloaded by 
handling all the RX queues and will become a bottleneck of packet capture.  
In our approach, the IRQ affinity mechanism together with RSS is used to balance the heavy loading caused by high 
speed networks. MSI-X (Message Signaled Interrupt-Extended) is an alternative way to interrupt the host CPU in parallel 
with several interrupt requests. In multi-core systems, an MSI-X enabled NIC allows a network device to support two or 
more interrupt vectors. Furthermore, the interrupt vectors can be distributed to different cores by setting IRQ affinity.  
In order to achieve the best performance, we proposed an MCA affinity setting as shown in Figure 7. The MCA 
affinity setting consists of both process affinity and IRQ affinity. For the process affinity, we start a separate Snort process 
on each core to handle each MCA Ring buffer. For the IRQ affinity, we assign each RX interrupt to a CPU core. The 
affinity setting is shown in below: 
 
Process    {P0, P1, ….., Pn } 
RX           {RX0, RX1, ….., RXn} 
 
MCA Affinity Setting:  
 
         P0 & RX0 are assigned to Core0 
         P1 & RX1 are assigned to Core1  
         … 
         Pn & RXn are assigned to CoreN 
 
Because of using the IRQ affinity setting, the kernel threads used for polling the incoming packets are removed in our 
approach. Besides this, each core has its own private ring structure to manage the ingress traffic, meaning that MCA Ring 
module can avoid using locks to protect the ring structure, and therefore increasing the performance.  
 
 
Figure 7: The setting of MCA Ring with NIDS 
 
4. Experimental Setup 
Table 1 gives the details of the various system settings that have been tested in this paper. The tests were performed on 
an embedded system with 6-core Intel CPU (3400 MHz), Ubuntu 10.04 OS and a PCI Express 10 Gigabit Ethernet card.  
The experimental setup is given in Figure 8, where we used two client PCs with a 4-port Gigabit Ethernet card and one 
client PC with a 2-port Gigabit Ethernet to generate packets by pktgen[4]. Pktgen can generate different flows of UDP (IP, 
Port) for the testing of the system performance. The maximum number of packets that can be generated from the three 
clients is about 14000Kpps. A 10-Gigabit switch is used to connect the test target and three clients. 
 
7 
 
 
Figure 9: Single core’s utilization (Mono RX Queue) 
(soft: softirq    sys: system level) 
 
5.1.2. Multiple RX Queues with Affinity 
As shown in Table 3, the multi-RX queues and multi-core processors with the affinity setting mentioned in section 3.3 
are used to capture the network packets. PF_Ring with TNPAI and NAPI have the packet capture rates of 3800Kpps and 
3100Kpps, respectively. The result shows that the performance indeed increases when using more processor cores to deal 
with the incoming packets. However, MCA Ring can totally handle the all incoming packets and its performance is more 
than 3 times larger than PF_Ring.  
The result of the average utilization of multi-core CPU is presented in Figure 10. PF_Ring with NAPI consumes 
approximately 62% of each CPU’s utilization at 3100Kpps and TNAPI has about 70% of each CPU’s utilization at 
3800Kpps. In MCA Ring, it only uses 5% of utilization at 14000Kpps which means that the system will have more time to 
run the NIDS applications or to handle more packets.  
In order to know the difference between using affinity and without using the affinity setting, we carry out more 
experiments and present the difference of packet capturing rate and CPU core utilization in the next section.  
 
Table 3: Maximal packet capture rate under different capture libraries with Multi RX Queues & with affinity setting 
Capture Library Queue Kpps 
MCA Ring Multi RX Queues 14000 
PF_Ring+TNAPI Multi RX Queues 3800 
PF_Ring+NAPI Multi RX Queues 3100 
 
 
Figure 10: CPUs’ average utilization (Multi RX Queue & with affinity setting) 
 
5.1.3. Multiple RX Queues without Affinity 
In this test, multi-RX queues and multi-core processors without the affinity setting are used to capture the network 
packets. Table 4 shows that PF_Ring with NAPI has only 1200Kpps and it can be explained by Figure 11. Because of 
without using the affinity setting, the IRQ interrupts are all handled by core0. As a result, the performance of PF_Ring 
with NAPI is the worst one. Moreover, compared to the results in section 5.1.2, the performance of TNAPI has also 
9 
 
 
Figure 12: Utilization of each core (PF_Ring + NAPI) 
(soft: softirq    sys: system level    usr: user level) 
 
 
Figure 13: Utilization of each core (PF_Ring + TNAPI) 
 
5.2.2. MCA Ring 
Figure 14 and Figure 15 show the performance of MCA Ring. Figure 14 indicates that the packet drop begins at 
7800Kpps (1300Kpps per core).  As mentioned in Section 5.1.2, MCA Ring only consumes a small amount of CPU time 
to receive packets because of DMA mechanism. On account of this reason, Snort applications have more CPU time to 
focus on the intrusion detection. In addition, each core is able to monitor more about 1200Kpps without dropping any 
packets while executing packet capture on the same core.  
According to the results of Section 5.1.2., MCA Ring only has 10% CPU consumption on each core which means that 
there still remains 90% free power for the usage of Snort. It is evident that the results can correctly verify that our IRQ and 
process affinity settings are the efficient way to detect intrusion attacks and receive packets executed on the same core. 
 
 
Figure 14: Packet drop rate with Snort applications 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/01/01
國科會補助計畫
計畫名稱: 正規樣式比對引擎開發
計畫主持人: 王勝德
計畫編號: 99-2622-E-002-028-CC3 學門領域: 平行與分散處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
本計畫研究產出使用多核心的封包擷取方法與比對技術，並完成其程式實作，
目前移轉合作廠商使用中。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
