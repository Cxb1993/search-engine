行政院國家科學委員會專題研究計畫成果報告 
 
在多核心平台上實現以視覺為基礎之嵌入式夜間駕駛輔助暨監控系統之研究 
  
The Study of Implementing a Vision-based Embedded Nighttime Driver Assistance and 
Surveillance System on a Heterogeneous Multi-Processor Platform 
 
計畫編號： NSC 98-2220-E-027-010 
執行期限：98 年 08 月01日至 99 年 10 月31日 
主  持  人：陳彥霖 助理教授  
國立台北科技大學資訊工程學系 
 
摘要 
本計畫主要在於發展一套以電腦視覺技術為基礎的嵌
入式夜間駕駛安全輔助與監控系統，藉由架設於汽車擋風玻
璃後之視訊擷取裝置，即時獲取前方路況以及車輛影像，並
且以一系列電腦視覺技術加以分析處理，以提供對於夜間行
車輔助所需的車輛偵測、辨識、前方路況分析與判斷、事件
偵測、行車視訊監控記錄、與即時行車事件視訊傳輸等各項
技術模組，並整合實現於一套異質ARM-DSP多處理器核心
之嵌入式系統平台上。在系統實現上，首先將各項需求進行
模組化，分為：行車事件偵測與分類模組、行車影像壓縮與
記錄模組、無線網路與RTP傳輸模組，最後將夜間車輛偵測
與距離估測系統 (Nighttime Vehicle Detection and Range 
Estimation System, NVDRES)系統[1][2][3]整合各項模組並
實現於一套嵌入式系統平台上，實作一套應用電腦視覺技術
的嵌入式夜間駕駛輔助與監控系統，這套系統的主要功能與
特點如下： 
 利用影像切割、物件辨識與相對運動分析技術，有效
分析與判斷前方路況。 
 藉由影像壓縮、無線傳輸，達到即時遠端記錄功能 
 軟體系統模組化設計，達到往後擴充功能的便利性與
加強功能的擴充性。 
 整合實現於異質ARM-DSP多處理器核心嵌入式系統
平台上，達到低成本、高效能的夜間駕駛輔助系統。 
關鍵詞：嵌入式系統、異質多核心系統、影像處理、夜間駕
駛、駕駛輔助系統 
 
 
英文摘要 
This project develops an intelligent vision-based nighttime 
driver assistance and surveillance system (VIDASS system), 
release them as embedded software components, and as well as 
implement this component-based system on an embedded 
multi-core platform. For this purpose, we will develop and 
implement the computer vision techniques for the road-scene 
frames in front of the host car, which are grabbed from the 
CCD cameras mounted in the vehicle. These vision-based 
technologies will be integrated and implemented on an 
ARM-DSP multi-core embedded platform, as well as the 
peripheral devices, including image grabbing devices, 3G/3.5G 
mobile communication module, and other in-vehicle control 
devices, will be also integrated to accomplish an in-vehicle 
embedded vision-based nighttime driver assistance and 
surveillance system.  
Accordingly, our system is expected to provide the 
following functional features:  
1) Effectively detection and tracking of oncoming and 
preceding vehicles based on image segmentation and pattern 
analysis techniques. 
2) Robust and adaptive vehicle detection under various 
illuminated conditions at nighttime urban environments 
benefited by a novel automatic object segmentation scheme. 
3) Providing beneficial information for assisting the driver to 
perceive surrounding traffic conditions outside the car during 
nighttime driving. 
4) Providing versatile control machineries for in-vehicle 
facilities of the autonomous vehicles, such as: 
 Intelligent control of vehicle high-beam and low-beam 
states of headlights 
 Pre-crash safety system for collision avoidance 
 Automatic Cruise Speed Control 
5) Offering a real-time event-driven traffic video surveillance 
machinery for recording evidences of possible traffic accidents. 
 
Keywords: Embedded System, heterogeneous multi-core 
systems, image processing 
 
圖 1、VIDASS 系統架構圖 
 
2.1 NVDRES System 
本計畫針沿用過去的系統並重新命名為車輛偵測與距
離估測系統，並將其作改良。 
2.1.1 BOS（Bright Object Segmentation Subsystem, 夜間光
源物件影像切割子系統） 
本子系統之主要目的在於擷取影像後，依照影像
進行適應性臨界值分析，找出最佳明亮光源物件，並進
行影像之明亮光源切割，並將結果傳送給予 SACPV 子
系統使用。依功能可細分為兩個模組： 
 適應性臨界值分析模組：以適應性統計分析模型
加以分析影像之最佳明亮光源之臨界值。  
 夜間光源物件切割模組：在找出最佳明亮光源之
臨界值，將影像進行明亮光源物件之切割。  
 
2.1.2 SACPV（Spatial Analysis and Clustering of Potential 
Vehicle-Light Components Subsystem, 光源物件之標
定與分類子系統） 
本子系統主要目的在於，根據所切割出之明亮物
件，進行影像物件的標籤化處理，將相同標籤之光源物
件的像素區域進行聚合，歸類為同一物件。在標籤化物
件後，進行車燈特徵之比對，將相同之物件進行聚合為
同一車燈物件。在功能細分上，將有下列兩個模組：  
 連通物件標定模組：將影像進行快速的連通物件
分析標定程序。  
 車燈物件分類與聚合模組：將各個物件進行汽車
車燈特徵的比對動作，將具有相同特性之光源物
件，進行聚合為同一汽車車燈物件。 
2.1.3 VIL（Rule-Based Vehicle Identification and Location 
Subsystem,車輛車燈物件辨識與標定子系統） 
本子系統之主要目的在於針對汽車車燈物件進行
更進一步之確認，是否完全符合汽車車燈之特徵。完成
再次比對後，進 行辨識車燈之種類，為車輛頭燈或是後
尾燈。。在功能上，將細分為下列兩模組：  
 車輛物件辨識模組：將 SACPV 子系統所聚合出
之車輛車燈物件，更進一步辨識是否完全符合車
輛車燈之特徵。  
 車輛車燈種類判別標定模組：進行車輛車燈種類
的判別與標定，將車輛車燈物件細分為：車輛頭
燈以及車輛後尾燈。 
2.1.4 VDPMA（ Vehicle Related Distance Position and 
Motion Analysis,車輛空間距離及位置與相對運動分
析判定子系統） 
本子系統之主要目的在於將視訊擷取裝置所擷取
之影像，進 行二維數位影像空間轉換為真實三維空間模
式，以此三維空間模式計算與前方 車輛之距離以及更進
一步推算車輛相對速度等資料。在功能細分上，將可分
為下列四個模組：  
 三維空間模型模組：將透過視訊擷取裝置所擷取
的影像進行二維空間轉換為三維空間模式。  
 前方車輛距離偵測模組：推算前車與本車之間的
相對距離。  
 車輛橫向距離偵測模組：透過三維空間模型可在
畫面中進行汽車之橫向距離的計算。 
 車輛相對速度偵測模組：對本身車輛與前方偵測
目標車輛進行相對的速度偵測計算。 
 
2.2 RETVRS System & EVDAS System 
本計畫針對 VIDASS系統發展即時夜間行車安全影像
監控與紀錄系統與車用嵌入式即時視覺子系統。 
2.2.1 即時夜間行車安全影像監控與紀錄系統（RETVRS）: 
1. Event-driven Traffic Detection Subsystem, (ETDS), 行
車事件偵測 
2. Event-driven Traffic Video Compression Subsystem, 
(ETVC), 行車影像壓縮子系統 
3. Event-driven Traffic Video Transmission Subsystem, 
(ETVTS), 行車影像傳輸子系統 
 圖 5、行車事件偵測架構圖 
圖6所示為正常行車情況，故在此行車狀況下並不需記
錄其行車狀況以節省資源的消耗。 
當前方行車狀況過於靠近，分類時將會歸類為緊急狀
況，並將啟動行車影像壓縮之功能。由於當前行車狀況已達
到警示情況故啟動行車影像紀錄，將可能肇事之狀況加以記
錄，以協助日後對於肇事現場重現，與責任歸屬之釐清等工
作。如圖6.(b)所示當前行車情況為可能發生意外之警訊，將
啟動行車影像紀錄加以記錄儲存與傳輸即時行車視訊。 
  
6.(a) 6.(b) 
圖 6、(a)正常行車情況下之行車影像, (b)可能肇事之行車影
像 
3.2. 行車影像壓縮子系統(Event-driven Traffic 
Video Transmission Subsystem ,ETVCS) 
本子系統架構圖如圖7所示，主要目的在於在當行車狀
況含有發生危險可能性時進行影像錄影功能。為了擁有快速
且高品質之視訊壓縮技術，將使用H.264壓縮技術，藉由該
行車影像與壓縮模組的結合，大提昇路況監控視訊壓縮編碼
之速度，同時還能保有優良的影像視覺品質之壓縮。如果在
系統判斷出本車前方有發生危險之可能性時，在功能上，將
可細分為下列兩個模組： 
1. 行車影像壓縮模組 
接收本車與前方車輛有發生危險的可能性的訊息之
後，將開始對行車影像進行高品質且快速的壓縮。在此會使
用TI(Texas Instruments)所以提供的H.264之CODEC，對影像
進行高品質的壓縮。 
2. 行車影像記錄模組 
行車影像即時壓縮之後，對行車壓縮後之影像以時間
將其命名之，接著將行車事件紀錄在一個系統內部已設定好
的行車事件紀錄日誌裡，並標記發生時間。 
 
圖 7、行車影像壓縮架構圖 
 
3.3. 行車影像傳輸子系統(Event-driven Traffic 
Video Transmission Subsystem, ETVTS) 
本子系統如圖8所示，主要目的在於為了確保視訊傳送
的即時性，我們將實作一套應用RTP（Real-Time Transport 
Protocol）標準的即時遠端視訊傳輸技術。如此我們可以藉
由車上之無線網路設備，透過一個預先架構的RTP伺服器，
將行車監控的視訊串流（Video Stream）加以傳遞至監控中
心，以備即時對於可能發生的 行車意外做出反應與最佳救援
處置。在功能細分上，可分為以下兩個模組： 
1. 無線網路模組 
將壓縮記錄後的影像，轉為視訊串流，然後使用RTP
傳輸模組將其封裝成RTP協定所制定的格式之封包，接著由
OMAP-3530之WLAN無線網路傳輸模組將其透過無線網路
傳輸出去。由於目前還只是學術用途，所以無線網路傳輸尚
未使用任何複雜的加密技術。 
2. RTP傳輸模組 
以RTP/UDP方式來建構監控視訊串流伺服器，以確保
視訊封包傳送的即時性。在轉完視訊串流後，將bitstream以
固定的位元數(payload)，各別封包之後再發送的方法，來降
低影像張數損失率，遠端接收端在收到封包後，再把屬於同
一張影像的bitstream重組起來。 
分，藉此發揮異質雙核心的效益。 
首先，利用 TI的軟體 GenCodecPkg[16]，如圖 11所示，
產生一個於 DSP 端可執行的 (name)MODULE，由於
GenCodecPkg 提供多種類型的 MODULE 可以讓開發者選
擇，在此我們選擇 IUNIVERSAL MODULE，因為
IUNIVERSAL MODULE是 TI專門為想開發屬於自己的演
算法的人而設計的。接著我們為 NVDRES裡所有子系統的
演算法以及 RETVRS的 ETDS子系統演算法分別創造四個
IUNIVERSAL MODULE。以下為本次系統所需創的四種
IUNIVERSAL MODULE： 
1. 車燈找尋模組(Light Search Module, LSM)：將
NVDRES裡的 BOS子系統的演算法與 SACPV子
系統的演算法整合成一個可以尋找車燈的模組，
並適當改寫成符合 TI 所要求的格式後放入
IUNIVERSAL MODULE後，將其命名為 LSM。 
2. 車尾燈與車頭燈辨識模組 (Headlights and 
Taillights Identification Module, HTIM)：將
NVDRES裡的VIL子系統的演算法適當改寫成符
合 TI 所要求的格式後放入另外建立的
IUNIVERSAL MODULE 裡 ， 並 將 VIL 的
IUNIVERSAL MODULE命名為 HTIM 
3. 行車間距測量模組(Driving Distance Detection 
Module, DDDM)：將 NVDRES裡的 VDPMA子
系統的演算法適當改寫成符合 TI 所要求的格式
後放入另外建立的 IUNIVERSAL MODULE裡，
並將 V VDPMA的 IUNIVERSAL MODULE命名
為 DDDM。 
4. 行車事件偵測模組(Event-driven Traffic 
Detection Module, ETDM)：將 RETVRS的 ETDS
子系統的演算法適當改寫成符合 TI所要求的格
式後放入 IUNIVERSAL MODULE裡，並將 ETDS
的 IUNIVERSAL MODULE命名為 ETDM。 
再來，利用 TI提供的軟體 Genserver[17]，如圖 12所
示，將這四個 IUNIVERSAL MOUDLE 整合至一個.x64P
裡。.x64P檔是一種 DSP端的執行檔。我們在 ARM端的程
式碼可以藉由 TI提供的 CODEC ENGINE來使用我們先前
所做的能夠在 DSP端執行的 IUNIVERSAL Module，進而達
到異質雙核心的分工。在此，也整合 H.264(或者 MPEG4)
等 TI提供的 CODEC演算法模組，並將其運用在 RETVRS
的 ETVCS子系統中，提高壓縮後影片的品質，且也能使用
此 CODEC讀取經由 DVD PLAYER傳過來的測試影片，以
利模擬真實行車狀況。 
 
圖 11、GenCodecPkg軟體 
 
圖 12、Genserver軟體 
 
ARM-CORE
UI
I / O
     
圖 13、ARM端架構 
最後我們使用一台DVD PLAYER將之前拍好的行車影
像紀錄影片，透過傳輸線傳入 OMAP3530，藉此模擬真實
的行車狀況。其架構圖如圖 15所示。圖 13及圖 14則為的
ARM端與 DSP端的解說圖。在 ARM端接收到影片後，將
其透過 CODEC ENGINE傳送影片(圖片)至 DSP端作處理。
行車距離以策安全。再者，當系統判斷出本車前方有發生危
險之可能性，如與前車之車距突然快速的縮短、或本車偏離
車道以致於有進入來車之車道之危險，此時系統便會啟動影
像監控紀錄功能，並透過即時無線網路視訊傳輸，將當下之
狀況影像與資料傳送至控制中心。如此，可以將可能肇事之
狀況加以記錄，以協助日後對於肇事現場重現，與責任歸屬
之釐清等工作，更可以加速道路救援之相關工作的進行。在
自由軟體元件上，成功結合產出了明亮物件切割軟體元件、
光源物件標定與分類軟體元件、車輛車燈辨識與標定軟體元
件、車輛距離判定軟體元件、車輛定位與分析軟體元件、即
時夜間行車安全影像監控與紀錄系統、行車影像壓縮子系
統、以及行車影像傳輸子系統，並提供一套 ARM-DSP異質
雙核心嵌入式系統實現之技術方案，藉由各項軟體元件之整
合與產出，協助產業界對於嵌入式夜間駕駛輔助系統有著更
深入的了解，並可藉由研究以及改良本軟體元件已達到更良
好的效果。 
本研究計畫之成果並已發表兩篇論文於知名 SCI與 EI
等級國際期刊論文上，該兩篇期刊論文分別發表於 IEEE 
Transactions on Industrial Electronics[18] 、 Journal of 
Convergence Information Technology [19]等知名 SCI/EI國際
期刊，足可證明本計畫所產出之研究成果具有國際水準。 
8. 參考文獻 
[1] Yen-Lin Chen and Chuan-Yen Chiang, "Embedded 
On-road Nighttime Vehicle Detection and Tracking 
System for Driver Assistance", in Proceedings of the 
2010 IEEE Conference on System, Man and 
Cybernetics(SMC 2010), pp. 1555 –1562, Istanbul, 
Turkey, Oct. 2010. 
[2] 陳彥霖, 江川彥, 陳庚延, 林敬竣, 林信佑, “結合電腦
視覺與資料壓縮技術之嵌入式夜間駕駛輔助系統之研
究”, in Proceedings of 2010 National Symposium on 
System Science and Engineering, CD-ROM, Taipei, 
Taiwan, Jul. 2010. 
[3] Yen-Lin Chen and Chuan-Yen Chiang, "Embedded 
Vision-based Nighttime Driver Assistance System", in 
Proceedings of the 2010 IEEE International Symposium 
on Computer, Communication, Control and Automation 
(3CA 2010), Vol. 2, pp.199-203, Tainan, Taiwan, May 
2010. 
[4] M. Antonini, M. Barlaud, P. Mathieu, and I. Daubechies, 
“Image coding using wavelet transform,” IEEE Trans. 
Image Processing, vol. 1, pp. 205-220, April 1992. 
[5] S. Li, W. Li, “Shape-adaptive discrete wavelet 
transforms for arbitrarily shaped visual objectcoding,” 
IEEE Trans. Circuits and Systems for Video Technology, 
vol. 10, pp.725-743, Aug.2000. 
[6] J. M. Shapiro, “Embedded image coding using zerotrees 
of wavelets coefficients,” IEEE Trans. Signal Processing, 
vol. 41, pp. 3445-3462, Dec. 1993. 
[7] A. Said and W. A. Pearlman, “A new, fast, and efficient 
image codec based on set partitioning in hierarchical 
trees,” IEEE Trans. Circuits and Systems for Video 
Technology, vol. 6, no. 3, pp. 243-250, June 1996. 
[8] ISO/IEC, “ISO/IEC 15444-1: Information 
Technology-JPEG2000 image coding system”, 2000. 
[9] C. Christopoulos, A. Skodras, and T. Ebrahimi, “The 
JPEG2000 still image coding system: an overview,” 
IEEE Trans. Consumer Electronics, vol. 46, pp.1103-127, 
Nov. 2000. 
[10] D. Taubman, “High performance image scalable image 
compression with ebcot,” IEEE Trans. Image Processing, 
vol. 9, pp. 1158-1170, July 2000. 
[11] C.-Y. Su and B.-F. Wu, “ A low memory embedded 
zerotree coding, ”IEEE Trans. on Image Processing, Vol. 
12, No.3, pp. 271-282, March 2003. 
[12] D. Zhao, Y. K. Chan, and W. Gao, “Low-Complexity 
and Low-Memory Entropy Coder for Image 
Compression”, IEEE Trans. on Circuits & Systems for 
Video Technology, Vol.11, No.10, October 2001. 
[13] I. Hontsch, and L.J. Karam, “Locally adaptive perceptual 
image coding,” IEEE Trans. ImageProcess., vol. 9, no. 9, 
pp. 1472-1483, 2000. 
[14] B.-F. Wu, Y.-L. Chen, C.-J. Chen, C.-C. Chiu, and C.-Y. 
Su, “A high-speed real-time video compression 
technique and its application on intelligent video 
surveillance system”, R.O.C. Patent No. I 241074, 2005. 
[15] B.-F. Wu, Y.-L. Chen, C.-J. Chen, and C.-C. Chiu, and 
C.-Y. Su, “A real-time wavelet-based video compression 
approach to intelligent video surveillance systems”, Int’l 
J. of Computers Appl. in Technol., Vol. 25, No. 1, pp. 
50-64, 2006. 
[16] Codec Engine GenCodecPkg Wizard FAQ – 
“http://processors.wiki.ti.com/index.php/Codec_Engine_
GenCodecPkg_Wizard_FAQ” 
[17] Codec Engine GenServer Wizard FAQ – 
http://processors.wiki.ti.com/index.php/Codec_Engine_G
enServer_Wizard_FAQ 
[18] Y.-L. Chen, B.-F. Wu, H.-Y. Huang, and C.-J. Fan, “A 
Real-time Vision System for Nighttime Vehicle 
Detection and Traffic Surveillance”, accepted for 
publication in IEEE Transactions on Industrial 
Electronics, DOI: 10.1109/TIE.2010.2055771, 2010. 
[19] Y.-L. Chen, C.-Y. Chiang, W.-Y. Liang, and C.-H. 
Chuang, “Embedded Vision-based Nighttime Driver 
Assistance System”, accepted for publication in Journal 
of Convergence Information Technology, 2010. 
 
二、與會心得 
本次研討會匯集了來自全世界各地智慧型系統研究領域的菁英學者，在這個研討會中互
相討論交流，激盪出更高層次的研究理念與見解，來自世界各地的教授及研究生們藉由此機
會發表了數百篇最新最先進的相關研究論文，對於各領域之互相交流，激盪出更先進的研究
構想，以及提高體學術風氣有十分正面的影響。本次研討會邀請了多個領域知名學者發表演
說，對於所有參與此次研討會的專家學者們是一次難得的學習機會。本人也藉此交流獲得了
對於本計畫之後續研究發展之構想。 
三、考察參觀活動 
在伊斯坦堡參與會議的 5 日行程期間，於無議程安排的晚上時間，於伊斯坦堡市區參觀
訪問，主要拜訪了伊斯坦堡之歷史文化遺產、博斯普魯斯海峽、及伊斯蘭清真寺等文化古蹟，
以瞭解當地特有的風俗民情與歷史文化遺產，這是一次非常難得的文化體驗。 
四、建議 
 建議國內各個學術單位能積極爭取主辦大型國際研討會，增進與國際學者學術交流，此
外政府相關單位亦能充分支持國內的學術研究與交流機會，如此不但可以增進國際能見度，
亦能給予國內學者及學生一個學習、討論的機會。 
五、攜回資料名稱及內容 
    本次帶回了一片 CD 會議論文集，除了大會所邀請的專題演講講義之外，亦收集了各個
分組討論所發表的論文，十分具有參考價值，其對於從事智慧型系統之相關研究上亦有極大
的助益。 
techniques on an embedded real-time system mounted in 
the host car. The proposed system are integrated and 
implemented on an ARM-Linux embedded platform, as 
well as the peripheral devices, including image grabbing 
devices, mobile communication module, voice reporting 
module, and other in-vehicle control devices, will be also 
integrated to accomplish an in-vehicle embedded vision-
based nighttime driver assistance system. 
II. THE PROPOSED NIGHTTIME VISION SYSTEM 
In the proposed system, we adopt an effective 
nighttime vehicle detection method for identifying vehicles 
by locating and analyzing their headlights and taillights. 
This proposed method comprises of the following 
processing stages include of bright object segmentation, 
spatial clustering process, rule-based vehicle identification, 
vehicles distance estimation and traffic event warning and 
control signaling machinery. Figure 1 sketches the flow 
diagram of the proposed nighttime driver assistance system. 
Bright Object Segmentation
Bright Object Plane 
Spatial Clustering Process
Input Road-scene 
Frames
Vehicle Identification 
Located Potential Vehicle-
Light Components
Approaching Vehicles 
with Pairing Headlights
Preceding Vehicles 
with Pairing Rear-lights
Nighttime Vehicle 
Detection
Tracking 
Vehicle - 1
Vehicle Tracking Process Coordinator
Find vehicle-light set 
matched track i
Update track i
successful
Tracking 
Vehicle - i
Tracking 
Vehicle - n
Vehicle Trackers
unsuccessful
Nighttime Vehicle 
TrackingVehicle Distance Estimation
... ...
Potential Moving Vehicles in 
Nighttime Road Scene
Traffic Event Warning and 
Control Signaling Machinery
 
Figure 1. System block diagram 
A. Bright Object Segmentation 
The input image sequences grabbed from the vision 
system, these grabbed frames reflect nighttime road 
environments appeared in front of the car. Figure 2 shows 
a sample of nighttime road scene taken from the vision 
system. In this sample scene, there are two vehicles 
appeared on the road, where the left one is approaching in 
the opposite direction on the neighboring lane, and the 
right one is moving in the same direction with the camera-
assisted car. 
Hence, the first task is to extract these bright objects 
from the road scene image to facilitate further rule-based 
analysis. To save the computation cost on extracting bright 
objects, we firstly extracted the grayscale image, which is 
shown as Figure 3, i.e. the Y-channel, of the grabbed image 
by performing a RGB to Y transformation. For extracting 
these bright objects from a given transformed gray-
intensity image, pixels of bright objects must be separated 
from other object pixels of different illuminations. Thus, 
an effective multilevel thresholding technique is needed 
for automatically determining the appropriate number of 
thresholds for segmenting bright object regions from the 
road-scene image. For this purpose, we have already 
proposed an automatic multilevel thresholding technique 
for image segmentation [6]. This technique can 
automatically decompose a grabbed road-scene image into 
a set of homogeneous thresholded images. 
  
Figure 2. A Sample of nighttime 
road scene 
Figure 3. A Sample of grayscale 
nighttime road scene 
  
Figure 4. The detection region 
and virtual horizon for bright 
object extraction in Figure 2 
Figure 5. Results of performing 
the bright object segmentation 
process on the road scene 
images in Figure 2 
To preliminarily screen out non-vehicle illuminant 
objects such as street lamps and traffic lights located above 
the half of the vertical y-axis, i.e. the "horizon", and save 
the computation cost, the bright object extraction process 
will be only performed on the bright components located 
under the virtual horizon, as shown in Figure 4. 
Accordingly, as shown in Figure 5, after performing the 
bright object segmentation process, we can find that pixels 
of bright objects in Figure 2 are successfully separated into 
thresholded object planes under illumination conditions. 
1556
clustering process, several groups of bright components 
are obtained, and they are called candidate vehicle light 
groups. 
 
Figure 6. The spatial clustering process of the bright components of 
interest 
C. Vehicle Tracking and Identification 
The light groups of the potential vehicles are obtained 
in each image frame by the aforementioned methods. 
However, since complete features of some potential 
vehicles may not be immediately detected from single 
image frames, a tracking procedure of these vehicle light 
groups will be applied to evaluate the information of 
potential vehicles from more consecutive image frames. 
The tracking information can then be used to refine the 
detection results and correct the errors due to occlusions 
caused by noise and errors during the bright object 
segmentation process and spatial clustering process. 
Moreover, the tracking information can also be applied to 
determine useful information like the direction of 
movement, positions, and relative velocities of potential 
vehicles entering the area under surveillance. 
Our proposed vehicle tracking and identification 
process includes two phases. First, the phase of potential 
vehicle tracking process associates the motion relation of 
potential vehicles in succeeding frames by analyzing their 
spatial and temporal features. Then the phase of vehicle 
recognition process identifies actual vehicles among the 
tracked potential vehicles. 
1) Potential Vehicle Tracking Process  
When only considering the detected object regions of 
potential vehicles in single frames, the problems caused 
by the segmentation process and the spatial clustering 
process might be: 1) A preceding vehicle may have been 
moving too close to other parallel moving vehicles or 
street lamps so that they may be occluded during the 
segmentation process, and thus are detected as one 
connected region. 2) An oncoming vehicle may have been 
passing so close to the host car that it may be occluded by 
the reflected beams on the road, and hence be merged as 
one large connected region. 3) The headlight set or 
taillight set of a vehicle may be composed of multiple 
light pairs and they may not be immediately merged into a 
single group by the spatial clustering process. 
When the motion of object regions of potential vehicles 
is being considered, we can progressively refine and 
correct the detection results of the potential vehicles by 
associating them in sequential frames. Therefore, we 
present a tracking process for potential vehicles which can 
effectively handle the above-mentioned problems. 
Moreover, the tracking information of the potential 
vehicles can be provided to the following vehicle 
identification and motion analysis processes to determine 
appropriate relative motion information of the target 
vehicles ahead of the host car. 
When a potential vehicle is firstly detected in the field 
of view in front of the host car, a tracker will be created to 
associate this potential vehicle with those in subsequent 
frames by applying their spatial-temporal features. The 
features used in the tracking process are described and 
defined as follows: 
a) tiP  denotes the i
th potential vehicle appearing in 
front of the host car in frame t; and the bounding box 
which encloses all the bright components contained in tiP  
is denoted as ( )tiB P . 
b) The location of tiP  employed in the tracking 
process is represented by its central position, and can be 
expressed by, 
( ) ( ) ( ) ( )( ) ( ) ( ) ( )
,
2 2
t t t t
i i i it
i
l B P r B P t B P b B P
P
⎛ ⎞+ +⎜ ⎟= ⎜ ⎟⎝ ⎠
     (8) 
c) The overlapping score of the two potential 
vehicles tiP  and 
t
jP
′ , detected at two different time t and 
t’, can be computed by using their area of intersection, 
( )
( )( , ) ( ), ( )
t t
i jt t
o i j t t
i j
A P P
S P P
Max A P A P
′
′
′
∩
=                  (9) 
d) The size-ratio feature of the enclosing bounding 
box of tiP  is defined by, 
( ) ( ) ( )
( )
( )
t
it
s i t
i
W B P
R P
H B P
=                  (10) 
e) Then the symmetry score of the two potential 
vehicles tiP  and 
t
jP
′  can be obtained by, 
( )
( )( , )t t s Ss i j s L
R PS P P R P
′
=                        (11) 
1558
be in reasonable proportion to the size of the size-ratio 
feature of its enclosing bounding box, thus, the following 
alignment condition must be satisfied, 
( )1 2( ) ( )( ) ( )j ja cc j aj j
W TP W TP
N TP
H TP H TP
τ τ
⎛ ⎞ ⎛ ⎞
≤ ≤⎜ ⎟ ⎜ ⎟⎜ ⎟ ⎜ ⎟⎝ ⎠ ⎝ ⎠
     (15) 
where the thresholds 1aτ  and  2aτ  are determined as 0.4 
and 2.0, respectively, according to our analysis of typical 
visual characteristics of most vehicles during nighttime 
driving. 
c) The oncoming vehicles are usually on the left 
side of the road, so the jTP  containing a non-red-light 
pair at the right side of the red-light pairs should be 
ignored. Hence, the headlight-pair locating condition is 
defined as, 
( ) ( )head j tail jl TP l TP<                       (16) 
where ( ) jl TP  is denoted as a left coordinate. 
The above-mentioned discriminating rules are obtained 
by analyzing many experimental videos of real nighttime 
road environments in which vehicle lights appear in 
different shapes, sizes, directions and distances. The values 
of thresholds utilized for these discriminating rules are 
determined to yield good performance in most general 
cases of nighttime road environments. 
Accordingly, a tracked potential vehicle will be 
recognized as an actual vehicle when the above-mentioned 
vehicle identification rules are satisfied. A tracked vehicle 
is no longer recognized as an actual vehicle when it cannot 
satisfy the vehicle identification criteria over a number of 
frames (three frames in general) or when it disappears 
from the field of view. 
D. Vehicle Distance Estimation 
To estimate the distance between the camera-assisted 
car and detected vehicles, we apply the perspective range 
estimation model of the CCD camera as introduced in [8]. 
The origin of the virtual vehicle coordinate system is 
placed at the central point of the camera lens. The X and Y-
coordinate axes of the virtual vehicle coordinate are 
parallel to the x and y-coordinates of the grabbed images, 
and the Z-axis is placed along the optical axis and 
perpendicular to the plane formed by the X and Y axes. A 
vehicle on the road at a distance Z in front of the camera-
assisted car will project to the image at a vertical 
coordinate y. Thus a perspective range estimation model as 
presented in [8] can be utilized for estimating the Z-
distance in meters between the camera-assisted car and one 
detected vehicle by using the equation, 
                        ( )( )Z k f H y= ⋅ ⋅                               (17) 
where k is a given factor for converting from pixels to 
millimeters for the CCD camera which is mounted on the 
car at the height H, and f is focal length in meters. 
E. Traffic Event Warning and Control Signaling 
Machinery 
The traffic event warning and control signaling 
machinery is an automatic control process in the proposed 
system. This automatic control process is comprised of the 
vehicle headlight control process and the vehicle speed 
control process. When any oncoming vehicles being 
detected, the headlight control process switch the 
headlights to low beams, and then turn back to high beams 
after the detected vehicles leave the detection zone. The 
warning voice will be activated for noticing drivers to 
throttling slow down when the distance to the detected 
preceding vehicles being too close to collide. Figure 7 and 
Figure 8 illustrate the flowcharts of the headlight control 
process and the vehicle speed control process, 
respectively. 
 
Figure 7. Headlight control process work flow 
 
Figure 8. Vehicle speed warning process work flow 
III. IMPLEMENTATIONS OF EVNDAS 
In this section, the implementation of EVNDAS will 
be presented in detail.  Subsection A depicts the software 
implementation of the proposed system, including the 
integration of the ARM-based embedded platform with 
embedded Linux OS and Qt graphical user interface for 
user friendly control machinery. Then, we also apply a 
component-based framework to implement the software 
framework of the proposed system. Subsection B 
introduces the hardware architecture of the proposed 
system. 
A. Software Development 
In this section, an component-based framework is 
adopted for developing the software framework of the 
1560
up on our experimental camera-assisted car. The frame rate 
of the vision system is 10 frames per second and the size 
of each frame of grabbed image sequences is 320 pixels by 
240 pixels per frame. The proposed system has been tested 
on several videos of real nighttime road-scenes under 
various conditions. Figure 16 illustrates our embedded 
system development platform. 
 
Figure 16. Result of embedded system development platform 
Figure 17 shows the main screen of proposed system, 
which consists of three buttons, including the functions of 
the system configuration, system starting and system 
stopping. The system configuration is for setting the 
system value such as the voice volume, control signaling, 
while system starting and system stopping buttons are 
applied for starting and stopping the proposed system, 
respectively. 
Figure 18 shows the results of detecting the preceding 
vehicles, and the locations and estimated distances of the 
detected vehicles are illustrated on the LCD. Here the 
locations of the detected preceding vehicles are drawn by 
green rectangles. When detecting any oncoming vehicles, 
the traffic event warning and control signaling machinery 
process will transfer the head light status into low beams. 
Figure 19 is the situation of both oncoming and preceding 
vehicles being detected, and the detected oncoming vehicle 
is sketched by red rectangle. After the oncoming vehicle 
passing the host car, the traffic event warning and control 
signaling machinery process will change the headlight 
status into high beams. Figure 20 depicts a sample that the 
system determines the detected preceding vehicle being 
too close to the host car. In this situation, the traffic event 
warning and control signaling machinery process will 
display the warning message on the LCD and activate the 
warning message voice for notifying the driver to slow 
down to avoid collision dangers. 
 
Figure 17. The user interface of the 
proposed system 
Figure 18. Result of detecting 
preceding vehicle and vehicle light 
status is low beams
 
Figure 19. Result of detection 
preceding vehicle, oncoming 
vehicle and vehicle light status is 
switched from high beams to low 
beams
Figure 20. Result of detecting 
preceding vehicle, vehicle light 
status is high beams and warning 
message is activated when too 
close to the host car
ACKNOWLEDGEMENTS 
This paper was supported by the National Science 
Council of R.O.C. under Contract No. 98-2220-E-027-010, 
and NSC-98-2622-E-027-041-CC3. 
REFERENCES 
[1] I. Masaki (Ed.), Vision-based Vehicle Guidance, Springer-Verlag, 
New York, 1992. 
[2] A. Broggi, M. Bertozzi, A. Fascioli, G. Conte, Automatic Vehicle 
Guidance: The Experience of the ARGO Autonomous Vehicle, 
World Scientific, Singapore, 1999. 
[3] A. Broggi, M. Bertozzi, A. Fascioli, C.G.L. Bianco, A. Piazzi, 
“Visual perception of obstacles and vehicles for platooning”, IEEE 
Trans. Intell. Transport. Syst., vol. 1, pp. 164-176, 2000. 
[4] S. Nedevschi, R. Danescu, D. Frentiu, T. Marita, F. Oniga, C. 
Pocol, R. Schmidt, T. Graf, “High accuracy stereo vision for far 
distance obstacle detection”, in Proc. IEEE Intell. Vehicle Symp., 
pp. 292-297, 2004. 
[5] M. Betke, E. Haritaoglu, and L. S. Davis, “Real-time multiple 
vehicle detection and tracking from a moving vehicle”, Mach. 
Vision Appl., vol. 12, pp. 69-83, 2000. 
[6] B.-F. Wu, Y.-L. Chen, and C.-C. Chiu, “A discriminant analysis 
based recursive automatic thresholding approach for image 
segmentation,” IEICE Trans. Info. Systems, vol. E88-D, no.7, 
pp.1716-1723, 2005. 
[7] K. Suzuki, I. Horiba, and N. Sugie, “Linear-time connected-
component labeling based on sequential local operations”, 
Computer Vision & Image Understand., vol. 89, pp. 1-23, 2003. 
[8] G. P. Stein, O. Mano and A. Shashua, “Vision-based ACC with a 
Single Camera: Bounds on Range and Range Rate Accuracy”, in 
Proc. IEEE Intell. Vehicle Symp., pp. 120-125, 2003. 
 
 
1562
contrast, illumination, and texture can be handled easily and 
well. Then a knowledge-based text extraction and 
identification procedure is applied on the resultant planes to 
detect and extract and identify text-lines with various 
characteristics in each respective plane. This method is 
composed of two processing phases: the text region extraction 
and text-line identification phases. The knowledge rules which 
encode the geometric and statistical features of text-lines 
having different illuminations, sizes, font styles, are 
constructed as two rule sets, text region extraction rules and 
text-line identification rules, according to the phase where they 
are performed. The inference engine of the proposed system is 
also based on the hierarchically structured control and strategy 
rules for efficiently performing the processes of text-line 
extraction and identification from real-life complex document 
images. Furthermore, the proposed system can offer high 
flexibility and expandability by just updating new rules for 
coping with more various types of real-life and future complex 
document images. Experimental results demonstrate that the 
proposed knowledge-based approach can provide satisfactory 
extraction of text-lines with different illuminations, sizes and 
font styles from various complex document images.  
II. MULTI-PLANE SEGMENTATION APPROACH 
For complex document images with text-lines in different 
illuminations, sizes, and font styles, and printed on varying or 
inhomogeneous background objects with uneven, gradational, 
and sharp variations in contrast, illumination, and texture, such 
as illustrations, photographs, pictures or other background 
patterns, a critical difficulty arises that no global segmentation 
techniques could work well for such kinds of document 
images. This is because when the regions of interesting text-
lines consisted of multiple colors or gray intensities are 
undersized as compared with those of the touched pictorial 
objects and complex backgrounds with indistinct contrasts, 
these text-lines cannot be discriminated in a global view of 
statistical features. A typical example with these 
characteristics is shown in Fig. 1(a). This sample image 
consists of three different colored textual regions printed on a 
varying and shaded background. Moreover, the black 
characters are superimposed on the white characters. By 
observing some localized regions, the statistical features of the 
textual objects, pictorial objects, and backgrounds could be 
much more distinguishable.  
Therefore, we have presented a regional and adaptive 
technique – the multi-plane segmentation approach [12] to 
segment textual objects from complex document images. This 
approach decomposes the document image into separate object 
planes by applying two processing stages: automatic localized 
histogram multilevel thresholding, and multi-plane region 
matching and assembling. In the first stage, distinct objects 
embedded in block regions are decomposed into separate 
“sub-block regions (SRs)” by applying the localized histogram 
multilevel thresholding process. Afterward, in the second 
stage, the multi-plane region matching and assembling process 
is applied to these obtained sub-block regions to classify and 
arrange them into homogeneous object planes. Consequently, 
homogeneous objects including textual regions of interest, 
non-text objects such as graphics and pictures, and background 
textures are extracted and separated into distinct object planes. 
The text extraction procedure is then performed on the 
resultant planes to detect and extract the textual objects with 
different characteristics in the respective planes. 
A. Localized multilevel thresholding 
The multi-plane segmentation process, if necessary, begins 
by applying a color-to-grayscale transformation on the RGB 
components of image pixels in a color document image, to 
obtain its illumination image Y . Then the obtained 
illumination image Y  will be sectored into non-overlapping 
rectangular block regions with dimension MH×MV. To 
facilitate analysis in the following stage, the objects of interest 
must be extracted from these localized block regions into 
separate “sub-block regions”, each of which contains objects 
with homogeneous features. Toward this goal, an efficient 
multilevel thresholding technique is needed to automatically 
determine the suitable number of thresholds to segment the 
block region into different decomposed object regions. By 
using the properties of discriminant analysis, we have 
proposed an automatic multilevel global thresholding 
technique for image segmentation [13]. This technique extends 
and applies the concept of discriminant criterion on analyzing 
the separability among the gray levels in the image. It can 
automatically determine the suitable number of thresholds, and 
utilizes a fast recursive selection strategy to select the optimal 
thresholds to segment the image into separate objects with 
similar features in a computationally frugal way. By applying 
this localized histogram multilevel thresholding process, we 
can decompose distinct objects with homogeneous features in 
localized regions into separate sub-block regions. 
B. Multi-plane region matching and assembling process 
Having decomposed all localized block regions into 
several separate classes of pixels by the localized multilevel 
thresholding procedure, various objects embedded or 
superimposed in different background objects and textures are 
respectively separated into relevant SRs. Then we need a 
methodology for grouping them into meaningful objects, 
especially textual objects of interest, for further extraction 
process. Therefore, we have presented an effective 
segmentation approach, multi-plane region matching and 
assembling process [12], which adopts both the localized 
spatial dissimilarity relation and the global feature 
information, to perceptually classify and assemble these 
obtained SRs to compose a set of “object planes” of 
homogeneous features, especially textual regions of interest. 
This proposed multi-plane region matching and assembling 
process is conducted by recursively performing the following 
three phases – the initial plane selection phase, the matching 
phase and the plane construction phase.  
For assembling the object planes of interest, the initial 
plane selection phase is firstly performed on these unclassified 
SRs to determine a representative set of seed SRs, and then 
initially setting up N initial object planes using these selected 
seed SRs. Afterward, the matching phase will be subsequently 
performed on the rest of unclassified SRs in the Pool and these 
initial planes, to determine the association and belongingness 
3271
Multi-plane Segmentation 
Process
Input Document 
Images
Object Planes
Domain Data Partition
Control Data Partition
Global Data Structure
Strategy Rules
Inference Engine
Focus-of-Attention Rules
Control Rules
Meta Rules
Knowledge Base
Text Region Extraction Rules
Knowledge Rules
Text-line Identification Rules
Knowledge-based Text-line Extraction 
and Identification System
 
Fig. 2. The proposed knowledge-based system 
The control rules, which composed of an inference engine, 
are conducted for determining which connected-components 
and connected-component groups to be evaluated and the 
subsequent process to be performed. The control rules are 
further divided into two categories: the focus-of-attention rules, 
and the meta-rules. The focus-of-attention rules is utilized for 
determine the connected-component and connected-
component group to be evaluated and performed by the 
knowledge rules next; while the meta-rules are utilized for 
determining the processing phases and feature configurations 
to specify which set of the knowledge rules to be performed 
next. The strategy rules are applied for deciding the invocation 
process of a given set of control rules and determining their 
execution order on the connected-components and connected-
component groups. 
To facilitate the rule-based reasoning process on 
extracting and identification of text-lines, the global data 
structure, which contains domain and control data partitions, is 
adopted to keep the critical processing information of the 
connected-components and connected-component groups 
being processed and their immediate control statuses. The 
domain data partition comprises of the features and 
information about the connected-components and connected-
component groups from binarized planes to be processed by 
the text-line extraction and identification modules; while the 
control data partition consists of the control information about 
the statuses of the extraction and identification processes, and 
the detailed records about any results being kept in the global 
data structure. 
B. Knowledge-based Text Region Extraction 
First, the domain data of the textual components used in 
the knowledge-based text extraction and identification process 
are given as follows: 
(a). Ci denotes a connected-component of the current qBP , 
and the bounding box which encloses Ci is denoted as ( )iB C .  
(b). jCS denotes a group of connected-components, 
{ }: 0,1,2,..., ( )j i cc jCS C i N CS= = , where the number of 
connected-components contained in jCS  is denoted as 
( )cc jN CS , and the bounding box which enclose all the 
connected-components belonging to jCS  is denoted as 
( )jB CS . A group jCS  represents a preliminary general text 
line. Note that the simplified denotation B represents a 
bounding box of a connected-component Ci, or a group of 
connected-components jCS , respectively in the following 
descriptions for the spatial clustering process.  
(c). The location of the bounding boxes of Cs or CSs 
employed in the spatial clustering process are their top and 
left coordinates, and they are denoted as ( )t B  and ( )l B ,  
respectively.  
(d). The width and height of the bounding boxes are denoted 
as ( )W B  and ( )H B , respectively. 
(e). The horizontal and vertical distances between two 
bounding boxes are defined as 
 ( , ) max ( ), ( ) min ( ), ( )h i j i j i jD B B l B l B r B r B⎡ ⎤ ⎡ ⎤= −⎣ ⎦ ⎣ ⎦   (2) 
, and 
( , ) min ( ), ( ) max ( ), ( )v i j i j i jD B B b B b B t B t B⎡ ⎤ ⎡ ⎤= −⎣ ⎦ ⎣ ⎦   (3) 
If the two bounding boxes are overlapping in the horizontal 
or vertical direction, then the value of 
( , ) or ( , )h i j v i jD B B D B B  will be a negative value.  
(f). The measures of overlap between the horizontal and 
vertical projections of the two bounding boxes are defined as,  
  ( , )( , )
min ( ), ( )
h i j
h i j
i j
D B BP B B
W B W B
−
= ⎡ ⎤⎣ ⎦
        (4) 
, and ( , )( , )
min ( ), ( )
v i j
v i j
i j
D B BP B B
H B H B
−
= ⎡ ⎤⎣ ⎦
        (5) 
Based on the domain data of the textual components 
defined above, the proposed knowledge-based textual region 
segmentation process is detailed as follows. This proposed 
knowledge-based textual region extraction process is 
conducted by the following knowledge rules, control rules, 
and strategy rules, to recursively perform the horizontal and 
vertical clustering procedures on the bounding boxes of the 
contained connected-components of the current processing 
binary plane qBP .  
The text region extraction process is conducted by 
recursively performing the horizontal and vertical clustering 
procedures on the bounding boxes of the contained connected-
components of each binary plane qBP , and is comprised by the 
following knowledge, control, and strategy rules.  
First, the horizontal clustering procedure - H-cluster( inCS ) 
(where the subscript “in” refers to “the original input group of 
connected-components”) is performed by using the following 
rules.  
Focus-of-Attention rule (F-1-1): 
IF:  
(a). The current status is “text region extraction”. 
(b). The current process corresponds to the “horizontal 
clustering procedure”.  
THEN:  
3273
Knowledge rule (K-1-4): 
IF:  
(a). There exists partial determined groups of connected-
components jCS . 
(b). The horizontal space between two adjacent groups 
1kCS  and 2kCS  is sufficiently small to reflect they should 
be belong to the same textual object group. This can be 
determined by the following horizontal space condition 
among the two adjacent groups 1kCS  and 2kCS :  
( ) 1 21 2
1 2
( ( )) ( ( ))( ), ( ) max ,
( ) ( )
k k
h k k
cc k cc k
W B W BD B B
N N
⎛ ⎞
< ⎜ ⎟⎜ ⎟⎝ ⎠
CS CSCS CS
CS CS
    
 (8) 
where the term ( ( )) ( )ccW B CS N CS  reflects the average 
width of all connected-components that belong to the 
group CS . 
THEN:  
(1). These two adjacent groups 1kCS  and 2kCS  are merged 
into the same group. 
Strategy rule (S-1-1): 
IF:  
(a). There is any more resultant groups of connected-
components can still be incrementally obtained.  
THEN:  
(1). Apply all the control rules of the text region 
segmentation on each connected-component groups kCS , 
until no more connected-component groups can be 
obtained.  
Initially, the initial group inCS  comprises all the connected-
components of the current processing binary plane qBP . 
Accordingly, based on the above-mentioned two procedures, 
the connected-component clustering process is started by 
performing the horizontal clustering rules on the initial group 
inCS . If the first recursion of performing the horizontal 
clustering rules cannot divide the initial group inCS  into more 
than one group, then perform the vertical clustering rules on 
inCS . The clustering process is performed by recursively 
applying the horizontal and vertical clustering rules until the 
resultant connected-component groups cannot be divided into 
more sub-groups.  
Fig. 3 depicts the text extraction process. As shown in Fig. 
3(a), for the corresponding connected-components of 
characters in the binary plane 4BP  (corresponding to the plane 
4P  in Fig. 1(f)), there are five resultant CSs obtained after the 
horizontal clustering rules is performed. The vertical 
clustering rules is in turn performed on these five CSs. For 
instance, as shown in Fig. 3(b), the vertical clustering rules is 
performed on the CS at the top of the five CSs obtained from 
the horizontal clustering rules, and then one resultant CS is 
obtained. This is because the connected-components in Fig. 
3(b) are all close to each other, and hence are clustered into a 
single resultant CS. After the knowledge-based text region 
extraction process on a binary plane is completed, several final 
CSs are obtained, representing candidates of actual text-lines, 
as shown in Fig. 3(c). Accordingly, the CSs associated with 
the remaining binary planes are also obtained after the 
knowledge-based text region extraction process is in turn 
performed on all binary planes.  
C. Knowledge-based Text-line Identification 
The knowledge-based text identification phase is then 
conducted to distinguish whether each one of these obtained 
CSs comprises actual text-lines or non-textual objects. Before 
distinguishing and extracting text-lines, we first identify 
halftone pictorial objects and background regions using 
normalized correlation features [16]. For each one of these 
CSs, its associated normalized correlation features are 
computed on the bounding box region covered by its contained 
components. If the normalized correlation features of one CS 
meet the discrimination rules of halftone pictorial objects as 
suggested in [16], then it is determined to be a pictorial object 
or a background region.  
After pictorial objects and background regions are 
identified and eliminated, the text identification is then 
performed on the rest of CSs. If a CS actually comprises a 
text-line, it may have the following distinguishing 
characteristics: 1) its contained connected-components should 
be respectively aligned, and the number of them should also be 
in proportion to the width of the CS; 2) the contained object 
pixels in the enclosing region of this CS show distinctive 
spatial variation. This characteristic of the contained object 
pixels of the CS can be determined by the statistical features as: 
considering that “0” represents object pixels and “1” 
background pixels, the number of transition pixels pT  in the 
enclosing box of the CS is determined by computing the 
number of “0” to “1” and “1” to “0” transitions. Accordingly, 
a CS can be identified as an actual text-line if it satisfies both 
of the above characteristics.  
Based on the above-mentioned concepts, the text 
identification process employs the following knowledge-based 
processing rules for identifying whether one CS consists of a 
text-line or non-text objects. A CS is identified as a real text-
line if the following knowledge rule is satisfied.  
Focus-of-Attention rule (F-2-1): 
IF:  
(a). The current status is “text-line identification”. 
(b). A CS has been selected.  
THEN:  
(1). Select an adjacent CS which is not identified as a text-
line or a non-textual object.  
Meta rule (M-2-1): 
IF:  
(a). The text-line identification is performing. 
(b). There are any CSs unidentified. 
THEN:  
(1). Select one unidentified CS. 
(2). Apply the text-line identification rules on the selected 
CS.  
Knowledge rule (K-2-1): 
IF:  
(a). The ratio of the width W and the height H of the 
bounding box of a CS satisfy the size-ratio condition, 
3275
 
(a). Test image 1 
(size: 1600 × 2304) 
(b). Test image 2 
(size: 2309 × 2829) 
Fig. 4. Original images of the test images 1 and 2 
 
(a). Binarized extraction results by 
our approach 
(b). Binarized extraction results by 
Jain & Yu’s 
Fig. 5. Text extraction results of Fig. 4(a) by our approach, and Jain and 
Yu’s method. 
Table 1. Experimental data of Jain and Yu’s method and our approach 
Method Recall Rate Precision Rate 
Jain and Yu’s 79.5% 95.2% 
The proposed 99.3 % 99.1% 
 
(a). Text extraction results by our 
approach 
(b). Text extraction results by Jain 
and Yu’s method 
Fig. 6. Text extraction results of Fig. 4(b) by our approach and Jain and 
Yu’s method. 
For the quantitative evaluation of text-line extraction and 
identification performance, two measures, the recall rate and 
the precision rate, which are commonly used for evaluating 
performance in information retrieval, are adopted. They are 
respectively defined as, 
.
.
No of correctly extracted text linesrecall rate
No of actual text lines
−
=
−
  (14) 
 .
.
precision rate
No of correctly extracted text lines
No of extracted text like component groups
−
=
−
(15) 
We compute the recall and precision rates for text-line 
extraction and identification results of test images by manually 
counting the number of actual text-lines printed on the 
document images, total extracted text-like connected-
component groups, and the correctly extracted text-lines, 
respectively. The experiments of quantitative evaluation were 
performed on our test database of 46 complex document 
images with totaling 1286 readable text-lines. Table 1 depicts 
the results of quantitative evaluation of Jain and Yu’s method 
and the proposed approach. By observing Table 1, we can see 
that the proposed approach provides better text-line extraction 
performance as compared to that of Jain and Yu’s method.  
 
ACKNOWLEDGEMENTS 
This paper was supported by the National Science Council 
of R.O.C. under Contract No. 98-2220-E-027-010, and NSC-
98-2622-E-027-041-CC3. 
REFERENCES 
[1] D. Doermann, “The indexing and retrieval of document images: a survey”, 
Comput. Vision Image Understand., vol. 70, pp. 287-298, 1998.  
[2] L. O’ Gorman, and R. Kasturi, “Document Image Analysis”, IEEE 
Computer Society Press, Silver Spring, MD, 1995.  
[3] L. A. Fletcher and R. Kasturi, “A robust algorithm for text string separation 
from mixed text/graphics images”, IEEE Trans. Pattern Anal. Mach. Intell., 
vol. 10, no. 6, pp. 910-918, 1988.  
[4] J. L. Fisher, S. C. Hinds and D. P. D’Amato, “Rule-based system for 
document image segmentation”, in Proc. 10th Int. Conf. Pattern Recognit., pp. 
567-572, 1990.  
[5] F. Y. Shih, S. S. Chen, D. C. D. Hung and P. A. Ng, “Document 
segmentation, classification and recognition system”, in Proc. IEEE Int. Conf. 
Syst. Integr., pp. 258-267, 1992.  
[6] D. Niyogi and S.N. Srihari, "An Integrated Approach to Document 
Decomposition and Structural Analysis," Int'l J. Imaging Sys. & Tech., vol. 7, 
pp. 330-342, 1996. 
[7] K.-H. Lee, Y.-C. Choy, and S.-B. Cho, “Geometric structure analysis of 
document images: a knowledge-based approach,” IEEE Trans. Pattern Anal.  
Machine Intell., vol. 22, pp.1224-1240, 2000. 
[8] A. K. Jain, and B. Yu, “Automatic text location in images and video 
frames”, Pattern Recognit., vol. 31 no. 12, pp. 2055-2076, 1998.  
[9] C. Strouthopoulos, N. Papamarkos, and A. E. Atsalakis, “Text extraction in 
complex color documents”, Pattern Recognit., vol.  35, pp. 1743-1758, 2002.  
[10] H. Yang, and S. Ozawa, “Extraction of Bibliography Information Based 
on the Image of Book Cover,” IEICE Trans. Info. Syst., vol. E82-D, no. 7, pp. 
1109-1116, 1999.  
[11] H. Hase, M. Yoneda, S. Tokai, J. Kato, and C. Y. Suen, “Color 
segmentation for text extraction”, Int’l. J. Doc. Anal. Recognit., vol. 6, no. 4, 
pp. 271-284, 2004.  
[12] Y.-L. Chen, and B.-F. Wu, “Text Extraction from Complex Document 
Images Using the Multi-plane Segmentation Technique”, in Proc. 2006 IEEE 
Conf. Syst. Man Cybern., pp. 3540 –  3547, Taiwan, 2006.  
[13] B.-F. Wu, Y.-L. Chen, and C.-C. Chiu, “A discriminant analysis based 
recursive automatic thresholding approach for image segmentation”, IEICE 
Trans. Info. Syst., vol. E88-D, no. 7, pp. 1716-1723, 2005.  
[14] K. Suzuki, I. Horiba, and N. Sugie, “Linear-time connected-component 
labeling based on sequential local operations”, Comput. Vision Image 
Understand., vol. 89, pp. 1-23, 2003.  
[15] M.D. Levine and A.M. Nazif, “Dynamic measurement of computer 
generated image segmentation”, IEEE Trans. Pattern Anal.  Machine Intell., 
vol.7, pp.155-164, 1985. 
[16] T. Pavlidis and J. Zhou, “Page segmentation and classification”, Comput. 
Vis. Graph. Image Process., vol. 54, no. 6, pp. 484-496, 1992.  
3277
98年度專題研究計畫研究成果彙整表 
計畫主持人：陳彥霖 計畫編號：98-2220-E-027-010- 
計畫名稱：在多核心平台上實現以視覺為基礎之嵌入式夜間駕駛輔助暨監控系統之研究(2.嵌入式系統
軟體技術開發分項) 
量化 
成果項目 
實際已達
成數（被接
受或已發
表） 
預期總達成
數(含實際
已達成數)
本計畫
實際貢
獻百分
比 
單位
備註（質化說明：如數個
計畫共同成果、成果列為
該 期 刊 之 封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報
告 1 1 100%  
研討會論文 1 1 100% 
篇 
陳彥霖, 江川彥, 陳庚延, 
林敬竣, 林信佑, ＇結合電
腦視覺與資料壓縮技術之嵌
入式夜間駕駛輔助系統之研
究＇, in Proceedings of 
2010 National Symposium on 
System Science and 
Engineering, CD-ROM, 
Taipei, Taiwan, Jul. 2010.
論文著作 
專書 0 0 100%   
申請中件數 1 1 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 3 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次
 
研討會論文 3 2 100% 
1. Yen-Lin Chen and 
Chuan-Yen 
Chiang, ＇＇＇＇Embedded 
On-road Nighttime Vehicle 
Detection and Tracking 
System for Driver 
Assistance＇＇＇＇, in 
Proceedings of the 2010 
IEEE Conference on System, 
Man and Cybernetics(SMC 
2010), pp. 1555 –1562, 
Istanbul, Turkey, Oct. 
2010.  
2. Yen-Lin Chen, ＇＇＇＇A 
knowledge-based approach 
for Textual Information 
Extraction from Mixed 
Text/Graphics Complex 
Document Images＇＇＇＇, 
in Proceedings of the 2010 
IEEE Conference on System, 
Man and Cybernetics(SMC 
2010), pp. 3270 - 3277, 
Istanbul, Turkey, Oct. 
2010.  
3. Yen-Lin Chen and 
Chuan-Yen 
Chiang, ＇＇＇＇Embedded 
Vision-based Nighttime 
Driver Assistance 
System＇＇＇＇, in 
Proceedings of the 2010 
IEEE International 
Symposium on Computer, 
Communication, Control and 
Automation (3CA 2010), 
Vol. 2, pp.199-203, 
Tainan, Taiwan, May 2010.
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次
 
 
