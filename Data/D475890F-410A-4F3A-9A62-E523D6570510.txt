classification accuracy. Experiments conducted on the 
music database employed in the ISMIR2004 Audio 
Description Contest (ISMIR2004) have shown that the 
proposed approach can achieve a classification 
accuracy of 89.44%, which is better than the winner 
of the contest by 5.37%. 
英文關鍵詞： music genre classification, modulation spectrum, 
principal component analysis (PCA), linear 
discriminant analysis (LDA), nonparametric 
discriminant analysis (NDA) 
 
2 
摘要 
本計畫我們提出新的調變頻譜特徵並將其應用於音樂曲風之自動分類，我們分別對
八度音程頻譜對比值(OSC)聲譜圖、MPEG-7之正規化聲音頻譜封包(NASE)聲譜圖及梅
爾倒頻譜(MFCC)聲譜圖做調變頻譜分析，分別產生 OSC 調變聲譜圖(modulated OSC 
spectrogram)、NASE 調變聲譜圖(modulated NASE spectrogram)及 MFCC 調變聲譜圖
(modulated MFCC cepstrogram)，然後自每一個調變頻譜圖中擷取新的調變頻譜特徵值，
包含調變頻譜能量值、調變頻譜平滑度、調變頻譜質心、調變頻譜波谷值及調變頻譜對
比值，接著以主軸分析(principal component analysis, PCA)演算法來選取適當之調變頻譜
特徵值並降低特徵向量維度，再以多重特徵向量來表示同一種音樂曲風，最後再以線性
區別分析(linear discriminant analysis, LDA)演算法或非參數區別分析(nonparametric 
discriminant analysis, NDA)演算法來提升辨識率。實驗結果比較我們所提出之方法及
2004年音樂曲風分類競賽之前五名參賽者，還有其他具備相同實驗設定之論文，我們所
提出之方法得到最佳之辨識率 (89.44%)，比 2004年音樂曲風分類競賽之優勝者(84.07%)
還高 5.37%。 
 
一. 報告內容 
1. 前言 
對於音樂曲風之分類，其擷取之特徵向量可分為短時距特徵(short-term features)和長
時距特徵(long-term feature)兩類。短時距特徵是從一段較短時間(通常是一個音框)之音
樂訊號中所擷取之特徵向量，一般而言是屬於較低階之音樂特徵。最常用來做為音樂曲
風分類之音樂特徵可分為三類：音色(timbre)、節奏(rhythm)及音高(pitch)。 
音色特徵通常呈現演奏之樂器或聲音來源之特性，譬如音樂、語音、及環境聲音等。
通常較常使用之音色特徵有以下幾種： 
(1) 低能量特徵 (low-energy feature, LEF) 
此特徵之定義是將連續數個音框看成一個紋理視窗(texture window)，首先計算每個
音框之能量值： 
2/1
1
0
2 )])[(1()( ∑
−
=
+×=
M
m
RMS mMnxM
nE  
其中，M表示每個音框的樣本數目，然後計算紋理視窗中所有音框能量的平均值： 
∑
−
=
=
1
0
)(1
N
n
RMSRMS nEN
E  
其中，N表示每個紋理視窗中的音框數目，此低能量特徵是計算在此紋理視窗中有
4 
∑ ∑
= =
×=
nR
m
N
m
nn mMTHmM
1 1
][][  
其中 Rn為頻譜滑動率，TH = 0.85為此公式常用的數值 
(6) 頻譜變遷度 (spectral flux)  
用來描述前後連續兩個音框間的頻譜差異性，可用來分割聲音片段。 
∑
=
−−=
N
m
nnn mNmNF
1
2
1 ])[][(  
其中 Nn[m]為將 Mn[m]正規化的值，其計算公式如下： 
∑
=
= N
m
n
n
n
mM
mMmN
1
][
][][  
(7) 梅爾倒頻譜係數 (Mel-frequency cepstral coefficients, MFCC) 
梅爾倒頻譜係數之概念是用一組模擬人類聽覺系統之梅爾濾波器來過濾每一臨界
頻帶之聲音訊號，並對每一頻帶之對數能量頻譜值(logrithmic spectra)做離散餘弦轉
換(discrete cosine transform, DCT)，即可求得每一音框之梅爾倒頻譜係數。 
(8) 八度音程頻譜對比值 (octave-based spectral contrast, OSC) 
八度音程頻譜對比值[1]是用來描述每一個八度音程的子頻帶中，頻譜波峰和波谷
的強度值之差異值，如此可以大略的反映聲音訊號之泛音(harmonic)和非泛音
(non-harminic)的分佈狀況。 
 
節奏特徵主要是描述一首音樂之節奏特性，通常是由一段音樂中的節拍統計圖(beat 
histogram)中擷取其節奏特徵，包括所有節拍的強度、主節拍的速度及強度、主節拍和
次節拍之速度間距，以及主節拍和次節拍的相對強度值。預估主節拍速度和其對應強度
的方法可參考[2, 3]。Tzanetakis提出從一首音樂之音高統計圖(pitch histogram)中擷取音
高特徵的方法[4]，其特徵包括頻率、音高強度值和音高間距。此一音高統計圖可以使用
各種音高偵測演算法來統計得到[5, 6]，而旋律與泛音也廣泛地由音樂家用來研究音樂的
結構，因此，Scaringella等人提出藉由描述每一小段音樂片段之音高分佈來擷取旋律與
泛音的方法[7]，此一方法類似旋律或泛音分析器，但不用事先決定較高階之音樂特性，
如基頻、和弦或音樂調性。 
通常要表示一整首音樂之特性時，必需將短時距的特徵向量整合起來構成長時距之
特徵向量，整合的方式包括計算所有短時距特徵向量之平均值及標準差，或者以自我回
6 
本計畫應用調變聲譜圖於音樂曲風之自動辨識分類，調變聲譜圖主要是對八度音程
頻譜對比值(octave spectral contrast, OSC)、MPEG-7之正規化聲音頻譜封包(NASE)及梅
爾倒頻譜(MFCC)聲譜圖做調變頻譜分析，分別產生 OSC調變聲譜圖、NASE調變聲譜
圖及 MFCC 調變聲譜圖，然後我們將每一個調變頻譜分解成對數間距之調變頻帶，接
著自每一調變頻帶中擷取五種調變特徵值：調變頻譜能量值、調變頻譜平滑度、調變頻
譜質心、調變頻譜波谷值及調變頻譜對比值。然後以主軸向量分析演算法來選取適當之
調變頻譜特徵值並降低特徵向量維度，再以多重特徵向量來表示同一種音樂曲風，最後
再以線性區別分析演算法或非參數區別分析演算法來辨識輸入之音樂檔案是屬於何種
類別之音樂曲風。 
本計劃之音樂曲風自動分類辨識系統包含訓練階段和辨識階段兩部分，訓練階段是
由四個主要模組所組成: 調變頻譜特徵擷取、主軸向量分析演算法、多重特徵向量分群
演算法、及線性區別分析演算法或非參數區別分析演算法。辨識階段是由四個主要模組
所組成: 調變頻譜特徵擷取、主軸向量分析轉換、線性區別分析轉換或非參數區別分析
轉換、和分類。 
 
2.1 調變頻譜特徵擷取 
首先，我們先建構聲音訊號之聲譜圖，包括 OSC 聲譜圖、NASE聲譜圖及 MFCC
聲譜圖，然後對各式各樣聲譜圖以調變頻譜分析來描述其隨時間之變化趨勢以擷取辨識
特徵。 
 
2.1.1 OSC聲譜圖 
OSC 是用來描述一音樂訊號之頻譜特性[1]，首先將音樂訊號依據八度音程之觀念
將其分解為 B個(本計劃中 B = 9)子頻帶，每一子頻帶之頻率範圍請參考表一，然後分別
計算每一子頻帶之頻譜波峰和波谷的強度值，一般而言，頻譜波峰主要反映聲音訊號之
泛音(harmonic)成份，而波谷相當於非泛音(non-harminic)或雜訊成份，因此頻譜波峰值
和波谷值的之差異值可以大略的反映聲音頻譜的對比分佈狀況。對於一音樂訊號，我們
先將其切割成一個個音框，然後以傅立葉轉換得到每一音框之聲音頻譜，接下來以八度
音程之帶通濾波器(octave scale band-pass filter) 將一音框之聲音頻譜分解為 B 個子頻
帶，然後再對每一子頻帶計算其頻譜對比特徵。假設(xb,1, xb,2, …, xb,Nb)代表第 b個子頻
帶之強度頻譜，Nb代表所有位於第 b個子頻帶中之傅立葉轉換係數之數目，假設此一子
頻帶之強度頻譜已經依據其強度值由大至小排序過，也就是說 xb,1 ≥ xb,2 ≥ … ≥ xb,Nb，第
8 
個為hiEdge到取樣頻率一半的頻帶能量總合，因此整個頻譜範圍可分解為 (B+2) 個子頻
帶，圖一為一個八度音解析度之邊界頻率fedge分隔圖。 
 
 
62.5 
125 250 500 1K 2K 4K 8K 
16K 
loEdge hiEdge 
88.4 176.8 353.6 707.1 1414 2828 5656 11313 
1 coefficient 16 coefficients 1 coefficient 
 
圖一. 八度音頻帶濾波器(頻譜解析度 r = 1/2) 
 
NASE在MPEG-7標準中是針對每一個音框之ASE係數轉換至分貝之刻度單位後做正規
化之動作，然而對一段音樂訊號而言，可能包含了許多音框，所以我們將所有音框之
NASE係數沿著時間軸串接起來構成二維之影像圖，稱為NASE聲譜圖。我們取所有音框
之NASE係數值(NASE(b), 0 ≤ b ≤ B+1)及RMS值(R(b))沿著時間軸串接起來構成二維之影
像圖，稱為NASE聲譜圖。 
 
2.1.3 MFCC聲譜圖 
梅爾倒頻譜係數已經廣泛應用於語音辨識上[15-17]，事實上，梅爾 (mel)是用以表示
人類聽覺系統對一個音調(tone)感覺上的音高或頻率的計算單位，在人類聽覺系統中，
對於一個音調的實際頻率(physical frequency)之反應並不是完全呈線性變化，而實際頻
率(f)和梅爾頻率(mel)之間的對應關係在頻率低於 1 KHz 時是呈線性變化，但在高頻的
部份則是呈現對數變化，兩者間之對應關係式如下： 
  )7001(log2595 10
fmel +=  
)110(700 2595 −=
mel
f  
人類之聽覺系統可將聲音之頻率分為一個個臨界頻帶(critical band)，位於同一臨界頻帶
內之頻率聲音對人耳聽起來是相似的，因此我們可以用一組梅爾濾波器來過濾每一臨界
頻帶之音樂訊號，並對每一頻帶之對數能量頻譜值(logrithmic spectra)做離散餘弦轉換
(discrete cosine transform, DCT)，即可求得每一音框之MFCC係數，然後將所有音框之
MFCC係數沿著時間軸串接起來構成二維之影像圖，稱為MFCC聲譜圖。 
對於每一種聲譜圖(OSC 聲譜圖、NASE 聲譜圖及 MFCC 聲譜圖)，我們將分別對
其做調變頻譜分析來描述各聲譜圖中之頻譜(或倒頻譜)係數隨時間變化之辨識特徵。 
 
10 
其中 Mt(m, d)表示第 t個分析視窗之調變頻譜，m代表調變頻率索引值。接著我們將調
變頻譜分為 J個對數間距之調變子頻帶(modulation subband)，每個調變子頻帶之調變頻
率分佈範圍可參考表二(J = 8)，然後我們自每一個調變子頻帶中擷取五種調變頻譜特徵
值：調變頻譜能量值(modulation subband energy, MSE)、調變頻譜平滑度(modulation 
spectral flatness, MSF)、調變頻譜質心(modulation spectral centroid, MSCEN)、調變頻譜
波谷值(modulation spectral valley, MSV)及調變頻譜對比值(modulation spectral contrast, 
MSC)。 
(1) 調變頻譜能量值之定義如下： 
∑
=
=
hj
lj
Φ
Φm
t dmMdjMSE
,
,
|),(|),(  
其中 Φj,l及 Φj,h分別表示第 j個調變子頻帶之下界頻率索引值和上界頻率索引值。 
(2) 調變頻譜平滑度用來表示每一調變子頻帶中各個調變頻率之能量分布是否很平均，
其定義如下： 
   
∑
∏
=
+−
=
+−
=
hj
lj
ljhj
hj
lj
Φ
Φmljhj
ΦΦ
Φ
Φm
dmM
ΦΦ
dmM
djMSF
,
,
,,
,
,
|),(|
1
1
|),(|
),(
,,
1
 
(3) 調變頻譜質心表示每一調變子頻帶之能量質心，其定義如下： 
 
   ))),((1(log10),( 210
,
,
∑
=
+=
hj
lj
Φ
Φm
dmMdjMSE  
(4) 調變頻譜波谷值(modulation spectral valley, MSV)及調變頻譜波峰值(modulation 
spectral peak, MSP)之定義如下： 
|),(|max),(
,,
dmMdjMSP
hjlj ΦmΦ ≤≤
=  
|),(|min),(
,,
dmMdjMSV
hjlj ΦmΦ ≤≤
=  
(5) 調變頻譜對比值則定義為調變頻譜波峰值與調變頻譜波谷值之差異值： 
),(  ),(),( djMSVdjMSPdjMSC −=  
因此每一特徵向量之所有調變頻譜對比值、調變頻譜波谷值、調變頻譜平滑度、調變頻
譜質心及調變頻譜能量值可構成五個 D×J之矩陣，每一矩陣可視為二維之影像圖，統稱
為調變聲譜圖。然後以主軸向量分析演算法來選取適當之調變頻譜特徵值並降低特徵向
量維度，再以多重特徵向量來表示同一種音樂曲風，最後再以線性區別分析演算法或非
參數區別分析演算法來辨識輸入之音樂檔案是屬於何種類別之音樂曲風。 
 
2.2主軸向量分析演算法(principal component analysis, PCA) 
12 
轉換成低維度向量，透過這樣的轉換我們能夠增強不同類別之間的差異性。最常使用的
轉換矩陣主要依據 Fisher criterion JF來求得： 
 ))()(()( T1T ASAASAtrAJ BWF
−=  
其中，SW和 SB分別代表的是相同類別之散佈矩陣(within-class scatter matrix)和不同類別
之散佈矩陣(between-class scatter matrix)，而相同類別之散佈矩陣的定義如下： 
 ∑∑
= =
−−=
C
j
N
i
j
j
ij
j
iW
i
S
1 1
T))(( μxμx  
而 jix 代表在類別 j中的第 i個特徵向量，μj為第 j類的平均向量(mean vector)，C為類別
的數目，Nj為類別 j裡的特徵向量個數。而不同類別之散佈矩陣之定義如下： 
  ∑
=
−−=
C
j
jjjB pS
1
T))(( μμμμ ,     
μ為所有類別的平均向量，pj為第 j類的事先機率。LDA演算法的目的是要去求出能夠
使不同類別之散佈矩陣和相同類別之散佈矩陣的比值為最大值之轉換矩陣 ALDA，而其
維度大小為 n×d： 
 
.
)(
)(maxarg T
T
LDA ASAtr
ASAtr
W
B
A
=A
 
此一轉換矩陣，可經由求出 B
1
W SS
− 的 eigenvectors來得到，而 ALDA之(C-1)個行向量為前
(C-1)個最大 eigenvalue值所對應之 eigenvector。 
在我們決定出最佳的轉換矩陣 ALDA後，我們以 ALDA將每一 n維的特徵向量轉換為
(C-1)維之向量。令 xPCA為 PCA轉換後之特徵向量，則 LDA轉換後之特徵向量如下: 
PCA
T
LDALDA xAx =  
 
2.5非參數區別分析演算法(nonparametric discriminant analysis, NDA) 
LDA 演算法假設每一類別之特徵向量是呈現高斯分佈，若是資料之分佈非高斯分
佈，其辨識率會隨之下降。因此 Fukunaga提出 NDA演算法來解決此一問題[23]，其原
始方法僅適用於兩種類別之分類上。在 NDA中，相同類別之散佈矩陣的定義同 LDA，
而不同類別之散佈矩陣之定義如下： 
( )( ) ( )( )Tll
N
l
ll
N
l
T
llll
NDA
b mmlwmmlwS )()(),2()()(),1(
2
1
2
1
2
1
2
1
1
2
11
2
1
21
xxxxxxxx −−+−−= ∑∑
==
 
14 
≤ k ≤ C, 1 ≤ d ≤ Nk, 其中 C為資料庫中音樂之種類數目，Nk為第 k種音樂類別之代表特
徵向量數目)之間的距離，在這裡的距離公式是歐幾里德距離，假設輸入之音樂訊號總
計可切割為 T個分析視窗，令 dt,k表示 xt和第 k種音樂類別之所有 Nk個代表特徵向量之
間的最小距離： 
   CkTtdd dktNdkt k
≤≤≤≤=
≤≤
1  ,1  ),,(min ,1, xx  
最終辨認之音樂種類代表編碼 s可由下列公式來決定： 
   ∏
=
≤≤
=
T
t
ktNk
ds
1
,1
minarg  
 
3. 實驗結果與討論 
在實驗中所使用之音樂資料庫為 2004年音樂曲風分類競賽(ISMIR2004 Music Genre 
Classification Contest)所使用之音樂資料庫[25]，此資料庫中有 1458 首音樂檔案，其中
有一半 729首音樂檔案用於訓練，另外一半 729首音樂檔案用於辨識，這些音樂檔案之
取樣頻率為 44100 Hz，壓縮之位元率為 128 kbps，音訊範圍大小為 16 bits且為立體聲之
MP3檔案，在本實驗中，我們先將每一壓縮檔案轉換為 44100 Hz、16 bits之單聲道音
樂檔案。這些音樂檔案總共分為六種類別：古典音樂(Classical)、電子音樂(Electronic) 、
爵士/藍調音樂(Jazz/Blue)、重金屬/龐克音樂(Metal/Punk)、搖滾/流行音樂(Rock/Pop)、及
世界音樂(World)，總計用於訓練及辨識之古典音樂檔案分別有 320/320首，電子音樂檔
案分別有 115/114首，爵士/藍調音樂檔案分別有 26/26首，重金屬 /龐克音樂檔案分別有
45/45首，搖滾/流行音樂檔案分別有 101/102首，世界音樂檔案分別有 122/122首。 
為了與 2004年音樂曲風分類競賽之參賽者之實驗結果比較，我們實驗中也是採用相
同 50:50 之訓練檔案及辨識檔案比例，但是因為每一音樂類別之檔案數目不盡相同，因
此其整體之辨識率定義如下： 
  ,
1
∑
≤≤
×=
Cc
cc CAPCA  
其中 Pc為第 c種音樂類別之出現機率，CAc為第 c種音樂類別之辨識率。 
 表格三比較各種調變特徵向量之辨識率，由此表格可以看出當 αPCA = 0.99時將所
有特徵向量整合來計算距離時以 LDA/NDA辨識可以得到最佳之辨識率(88.07%)。 
 表格四與表格五比較以多重特徵向量和LDA或NDA來辨識各種調變特徵之辨識率
之辨識率，由此表格可以看出當以 LDA 來辨識可以得到之最佳辨識率為 89.30%，以
NDA來辨識可以得到之最佳辨識率為 89.44%。 
16 
其他具備相同實驗設定之論文，由此表格中我們可以發現我們所提出之方法得到最佳之
辨識率(89.44%)，比 2004年音樂曲風分類競賽之優勝者(84.07%)還高 5.37%。 
 
表六、對於 2004年音樂曲風分類競賽之音樂資料庫之辨識率比較 
References CA 
Our proposed approach 89.44% 
Our previous approach [26] 86.83% 
Y. Song et al. [27] 84.77% 
T. Lidy & A. Rauber [28] 79.70% 
E. Pampalk (winner) 84.07% 
K. West (2nd rank) 78.33% 
G. Tzanetakis (3rd rank) 71.33% 
T. Lidy & A. Rauber (4th rank) 70.37% 
D. Ellis & B. Whitman (5th rank) 64.00% 
 
二. 參考文獻 
[1] D. N. Jiang, L. Lu, H. J. Zhang, J. H. Tao, and L. H. Cai, “Music type classification by 
spectral contrast feature”, in Proc. of IEEE Int. Conf. on Multimedia & Expo, Vol. 1, pp. 
113-116, 2002. 
[2] M. E. P. Davies and M. D. Plumbley, “Beat tracking with a two state model”, in Proc. 
ICASSP, Vol. 3, pp. 241-244, 2005. 
[3] W. A. Sethares, R. D. Robin, and J. C. Sethares, “Beat tracking of musical performance 
using low-level audio feature”, IEEE Trans. Speech and Audio Processing, Vol. 13, No. 
12, Mar. 2005, pp. 275-285. 
[4] G. Tzanetakis, A. Ermolinskyi, and P. Cook, “Pitch Histogram in Audio and Symbolic 
Music Information Retrieval”, in Proc. IRCAM, 2002. 
[5] T. Tolonen and M. Karjalainen, “A computationally efficient multipitch analysis model”, 
IEEE Trans. Speech and Audio Processing, Vol. 8, No. 6, pp. 708-716, Nov. 2000. 
[6] R. Meddis and L. O’Mard, “A unitary model of pitch perception”, Journal of the 
Acoustical Society of America, Vol. 102, No. 3, pp. 1811-1820, Sep. 1997. 
[7] N. Scaringella, G. Zoia and D. Mlynek, "Automatic genre classification of music content: 
a survey", IEEE Signal Processing Magazine, Vol. 23, Issue 2, pp.133 - 141, Mar 2006. 
[8] A. Meng, P. Ahrendt, J. Larsen, and L. K. Hansen, ”Temporal feature integration for 
music genre classification”, IEEE Trans. Audio, Speech and Language Processing, Vol. 
15,  No. 5, pp.1654-1664, July 2007. 
[9] B. Kingsbury, N. Morgan, and S. Greenberg, “Robust speech recognition using the 
modulation spectrogram“, Speech Communication, Vol. 25, No. 1, pp.117-132, 1998. 
18 
pp. 755-761, Apr. 2009. 
[25] http://ismir2004.ismir.net/ISMIR_Contest.html. 
[26] C. H. Lee, J. L. Shih, K. M. Yu, and H. S. Lin, “Automatic Music Genre Classification 
Based on Modulation Spectral Analysis of Spectral and Cepstral Features”, IEEE Trans. 
on Multimedia, vol. 11, no. 4, pp. 670-682, June 2009. 
[27] Y. Song and C. Zhang, “Content-based information fusion for semi-supervised music 
genre classification”, IEEE Trans. on Multimedia, vol. 10, no. 1, pp. 145-152, Jan. 2008. 
[28] T. Lidy and A. Rauber, “Evaluation of feature extractors and psycho-acoustic 
transformations for music genre classification”, in Proc. 6th Int. Conf. on Music 
Information Retrieval, 2005, pp. 34-41. 
 
三. 計畫成果自評 
本計劃完成音樂曲風之自動分類系統，能夠根據音樂的性質事先將音樂曲目分類為
不同的曲風類型，有效率的管理龐大的音樂資料庫，此外也可做為音樂推薦系統使用，
當使用者在選取一首喜愛的音樂時，可以將曲風相似之音樂曲目推薦給使用者，減少使
用者搜尋性質相似之音樂所花的時間。當初提計劃書時預計以二年期之計劃來完成音樂
曲風及樂器音色之自動分類辨識之系統，但是計劃只通過一年期，因此我們先完成音樂
曲風自動分類系統，有關樂器音色之自動分類辨識系統則預計在後續之計劃中執行完
成。目前我們已發表之相關論文如下： 
 
期刊論文 (Journal Papers)： 
[1] C. H. Lee, C. H. Chou, and J. C. Fang, “Automatic Music Genre Classification Using 
Modulation Spectral Features and Nonparametric Discriminant Analysis”, Journal of 
Information Technology and Applications, Vol. 5, No. 2, June 2011, pp. 75-82. 
[2] C. H. Lee, J. L. Shih, K. M. Yu, and H. S. Lin, “Automatic Music Genre Classification 
Based on Modulation Spectral Analysis of Spectral and Cepstral Features”, IEEE Trans. 
on Multimedia, Vol. 11, No. 4, June 2009, pp. 670-682. (SCI, EI) 
[3] C. H. Lee, C. C. Han, and C. C. Chuang, “Automatic Classification of Bird Species by 
Their Sounds Using Two Dimensional Cepstral Coefficients”, IEEE Trans. on Audio, 
Speech, and Language Processing, Vol. 16, No. 8, Nov. 2008, pp. 1541-1550. (SCI, EI) 
[4] C. H. Lee, C. H. Chou, C. H. Han, and R. Z. Huang, “Automatic Recognition of Animal 
Vocalizations Using Averaged MFCC and Linear Discriminant Analysis”, Pattern 
Recognition Letters, Vol. 27, Issue 2, Jan. 2006, pp. 93-101. (SCI, EI) 
[5] C. H. Lee, Y. K. Lee and R. Z. Huang, “Automatic recognition of bird songs using 
cepstral coefficients”, Journal of Information Technology and Applications, Vol. 1, No. 1, 
表 Y04 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                           101年 1 月 10 日 
報告人姓名  
李建興 
 
服務機構 
及職稱 
 
中華大學資訊工程系 
 
     時間 
會議 
     地點 
15-17 October, 2011 
Shanghai, China 
本會核定 
補助文號 
計劃編號： 
NSC 99-2221-E-216-048- 
會議 
名稱 
 (中文) 
 (英文) The 4th International Congress on Image and Signal Processing (CISP 
2011) 
發表 
論文 
題目 
 (中文) 
 (英文) Music Genre Classification Using Modulation Spectral Features and 
Multiple Prototype Vectors Representation 
 
一、參加會議經過 
10/14搭乘 16:30長榮航空班機前往中國上海，下榻於 Radisson上海新世界酒店。10/15
早上搭乘地鐵前往會議地點—光大會展中心國際大酒店，先註冊報到，中午用餐後，參
加下午的 Session 1B(Image Processing Applications) 及 Session 2B(Feature extraction and 
machine vision in Images) 的演講。10/16早上聆聽了 Session 3B (Speech and Language 
Processing) 及 Session 4B (Image enhancement and noise filtering)的演講。我們的論文則是
安排於下午之 poster session，由於此一研討會是與 The 4th International Conference on 
BioMedical Engineering and Informatics合辦，因此會場中除了有影像及訊號處理相關論
文發表，還有許多和生物醫學有關之論文發表，相當熱絡。10/17 早上參加 Session 
7B(Pattern Recognition in images) 及 Session 8B(signal processing applications) 的演講
後，即返回飯店整理行李，準備搭機返回台灣。 
 
 
會場-上海光大會展中心 會場報到處 
附件三
 
表 Y04 
 
 
CISP論文光碟 CISP論文輯 
 
 
六、其他 
非常感謝國科會之補助得以參加該研討會。 
 
                                                                                                                                          2763
The training phase is composed of four main modules: 
modulation spectral feature extraction, principal component 
analysis (PCA) [18, 19], multiple prototype vectors generation, 
and linear discriminant analysis (LDA) [18, 19]. The 
classification phase consists of four modules: modulation 
sepctral feature extraction, PCA transformation, LDA 
transformation, and classification. A detailed description of 
each module will be described below. 
A. Modulation Spectral Feature Extraction 
1) Frame-based feature extraction: In this paper, the frame 
based feature vectors used to describe an audio frame include 
MFCC, OSC, and NASE. The feature vectors used to represent 
the t-th audio frame can be summarized as follows: 
T1)]-( , (1), (0),[  LMFCCMFCCMFCC ttt
MFCC
t "=x  (1) 
T])12(,),1(),0([  -BOSCOSCOSC Ottt
OSC
t "=x  (2) 
T]1)( ,),1( ),0( ,[ += Nttt
NASE
t BNASENASENASER "x (3) 
where L is the length of MFCC feature vector, BO is the 
number of octave scale filters, BN is the number of logarithmic 
subbands and R is the RMS-norm gain value computed from 
the audio spectral envelope ASEdb(b) of all subbands: 
∑+
=
=
1
0
2))((
NB
b
dB bASER     (4) 
2) Modulation spectral analysis: The frame-based features 
can’t characterize the variations of a sound within a long-time 
analysis window. In this study, we will apply long-term 
modulation spectral analysis to MFCC, OSC, and NASE to 
capture the time-varying behavior of the music signals. 
Without loss of generality, let xn = [xn(1), xn(2), …, xn(D)]T 
denote the feature vector extracted from the n-th audio frame of 
a music signal, where D is the length of the feature vector. The 
feature vector xn can be the frame based MFCC/OSC/NASE 
feature vector, or a combination of these feature vectors by 
concatenating them together. By applying FFT on each feature 
value along the time trajectory within a texture window of 
length W, we can get the modulation spectrogram: 
DdWmedxdmM
W
n
m
W
nj
nWtt <≤<≤=∑−
=
−
+× 0,0 ,)(),(
1
0
2
)2/(
π  (5) 
where Mt(m, d) is the modulation spectrogram for the t-th 
texture window, m is the modulation frequency index. In this 
study, the window length W is 512 with 50% overlapping 
between two neighboring texture windows. By time averaging 
the magnitude modulation spectrograms of all texture 
windows, the representative modulation spectrogram of a 
music track can be derived as follows: 
DdWmdmM
T
dmM
T
t
t <≤<≤= ∑
=
0,0   ,),(1),(
1
 (6) 
where T is the total number of texture windows in the music 
track. 
3) Modulation spectral feature extraction: The averaged 
modulation spectrum of each feature value will be decomposed 
into J logarithmically spaced modulation subbands (in this 
paper, J = 8), Table I shows the frequency interval of each 
modulation subband. For each feature value, modulation 
spectral contrast (MSC) [9], modulation spectral valley (MSV) 
[9], as well as modulation spectral energy (MSE), modulation 
spectral centroid (MSCEN), and modulation spectral flatness 
(MSF) within each modulation subband are then evaluated: 
( )),(max),( 
,,
dmMdjMSP
hjlj ΦmΦ <≤
=
   (7) 
( )),(min),(
,,
dmMdjMSV
hjlj ΦmΦ <≤
=
   (8) 
))),((1(log10),( 210
,
,
∑
=
+=
hj
lj
Φ
Φm
dmMdjMSE   (9) 
∑
∑
=
=
×
=
hj
lj
hj
lj
Φ
Φm
Φ
Φm
dmM
mdmM
djMSCEN
,
,
,
,
),(
),(
),(
  (10) 
∑
∏
=
+−
=
+−
=
hj
lj
ljhj
hj
lj
Φ
Φmljhj
ΦΦ
Φ
Φm
dmM
ΦΦ
dmM
djMSF
,
,
,,
,
,
),(
1
1
),(
),(
,,
1
  (11) 
where Φj,l and Φj,h are respectively the low modulation 
frequency index and high modulation frequency index of the j-
th modulation subband, 0 ≤ j < J. The MSPs correspond to the 
dominant rhythmic components, MSVs the non-rhythmic 
components, MSEs express the power of each modulation 
subband, MSCENs indicate the mass center of each modulation 
subband, and MSFs represent the modulation frequency 
distribution within a modulation subband. Further, the 
difference between MSP and MSV will reflect the modulation 
spectral contrast distribution: 
),(  ),(),( djMSVdjMSPdjMSC −=   (12) 
TABLE I. FREQUENCY RANGE OF EACH MODULATION SUBBAND. 
Modulation 
subband index 
Modulation 
frequency index range 
Modulation 
frequency range (Hz) 
0 [0, 3) [0, 0.5)
1 [3, 6) [0.5, 0.1)
2 [6, 12) [1, 2)
3 [12, 24) [2, 4)
4 [24 48) [4, 8)
5 [48, 96) [8, 16)
6 [96 192) [16, 32)
7 [192, 256) [32, 42.24)
As a result, all MSCs (MSVs, MSEs, MSCENs or MSFs) 
will form a D×J matrix. To derive a compact feature vector, the 
mean and standard deviation along each row (and each 
column) of the MSC, MSV, MSE, MSCEN, and MSF matrices 
will be computed. Let the modulation spectral feature values 
derived from the d-th (0 ≤ d < D) row of the MSC matrix be 
notated )(du rowMSC  and )(d
row
MSCσ . Thus, for a music track the 
modulation spectral feature vector derived from the D rows of 
                                                                                                                                          2765
subclasses, and Nc is the number of training vectors labeled as 
subclasses c. The between-class scatter matrix is given by: 
,))((
1
T∑
=
−−=
scN
c
cccN xxxxSB    (27) 
where x  is the mean vector of all training vectors. The most 
widely used transformation matrix is a linear mapping that 
maximizes the so-called Fisher criterion JF defined as the ratio 
of between-class scatter to within-class scatter: 
)).()(()( T1T ASAASAA BW
−
= trJ F   (28) 
From the above equation, we can see that LDA tries to find a 
transformation matrix that maximizes the ratio of between-
class scatter to within-class scatter in a lower-dimensional 
space. In this study, a whitening procedure is integrated with 
LDA transformation such that the multivariate normal 
distribution of the set of training vectors becomes a spherical 
one [19]. First, the eigenvalues and corresponding 
eigenvectors of SW are calculated. Let Φ denote the matrix 
whose columns are the orthonormal eigenvectors of SW, and Λ 
the diagonal matrix formed by the corresponding eigenvalues. 
Thus, SWΦ = ΦΛ. Each training vector x is then whitening 
transformed by ΦΛ-1/2: 
xΦΛx T2/1 )( −=w     (29) 
It can be shown that the whitened within-class scatter 
matrix )()( 2/1T2/1 −−= ΦΛSΦΛS WW
w  derived from all the 
whitened training vectors will become an identity matrix I. 
Thus, the whitened between-class scatter matrix 
)()( 2/1T2/1 −−= ΦΛSΦΛS BB
w  contains all the discriminative 
information. A transformation matrix Ψ can be determined by 
finding the eigenvectors of wBS . Assuming that the eigenvalues 
are sorted in a decreasing order, the eigenvectors 
corresponding to the (Nsc–1) largest eigenvalues will form the 
column vectors of the transformation matrix Ψ. Finally, the 
optimal whitened LDA transformation matrix AWLDA is defined 
as: 
ΨΦΛA 2/1−=WLDA     (30) 
AWLDA will be employed to transform each H-dimensional 
feature vector to be a lower h-dimensional vector. Let x denote 
the H-dimensional feature vector, the reduced h-dimensional 
feature vector can be computed by: 
PCAWLDAxAy
T
=     (31) 
E. Music Genre Classification 
In the classification phase, let yMMFCC, yMOSC, yMNASE 
respectively denote the modulation spectral feature vectors 
extracted from MFCC, OSC, and NASE modulation 
spectrograms. At the stage of feature level fusion, a new 
combined feature vector yMCOMB is obtained by concatenating 
yMMFCC, yMOSC, and yMNASE together: 
TT
MNASE
T
MOSC
T
MMFCCMCOMB ] [ yyyy , ,  =   (32) 
The same linear normalization using (20) is applied to 
each feature value. Each type of normalized feature vector is 
then transformed to be a lower-dimensional feature vector by 
using PCA transformation matrix APCA, and LDA 
transformation matrix AWLDA. The classifier is then employed 
to compute the distances between the transformed feature 
vector and the representative feature vectors of all music 
classes. The distance between the input music track and the c-
th music genre in terms of modulation MFCC feature is 
defined as follows: 
)),(,(min)(
1 1
kidcd MMFCCMMFCCKkC,iMMFCC yy≤≤≤≤=
 (33) 
where MMFCCy  and ),( kiMMFCCy  are the modulation MFCC 
feature vectors of the input music track and the k-th prototype 
vector of the i-th music genre, respectively. The distance 
between the input music track and every music genre in terms 
of modulation OSC, NASE, and combined feature (denoted by 
dMOSC(c), dMNASE(c), and dMCOMB(c)) can be computed in a 
similar way. The overall distance evaluated for the c-th (1 ≤ c 
≤ C) music genre is defined as the sum of each individual 
distance [20]: 
)()()()()( cdcdcdc d cd MCOMBMNASEMOSCMMFCC +++=  (34) 
Thus, the subject code s that denotes the identified music 
genre is determined by finding the music class that has the 
minimum overall distance: 
)(min arg
1
cds
Cc≤≤
=     (35) 
III. EXPERIMENTAL RESULTS 
A. Datasets 
The dataset used in the ISMIR2004 Music Genre 
Classification Contest [21] will be employed for performance 
comparison. This dataset consists of 1458 music tracks in 
which 729 music tracks are used for training and the other 729 
tracks for testing. The audio file format is 44.1 kHz, 128 kbps, 
16-bit, stereo MP3 files. In this study, each stereo MP3 audio 
file was first converted into a 44.1 kHz, 16-bit, mono audio 
file before classification. These music tracks are classified into 
six classes: Classical, Electronic, Jazz/Blue, Metal/Punk, 
Rock/Pop, and World. In summary, the music tracks used for 
training/testing include 320/320 tracks of Classical, 115/114 
tracks of Electronic, 26/26 tracks of Jazz/Blue, 45/45 tracks of 
Metal/Punk, 101/102 tracks of Rock/Pop, and 122/122 tracks 
of World music genre. Since the music tracks per class are not 
equally distributed, the overall classification accuracy is 
defined as follows: 
,
1
∑
≤≤
×=
Cc
cc CAPCA     (36) 
where Pc is the probability of appearance of the c-th music 
genre, CAc is the classification accuracy for the c-th music 
genre. 
B. Classification Results 
Table II compares the classification accuracy of different 
modulation spectral feature vectors derived from modulation 
spectral analysis of MFCC, OSC, and NASE: MMFCC, 
MOSC, MNASE, and their combination MCOMB using 
Date: Thu, 25 Aug 2011 13:34:59 +0000 
From: PC Chair <cisp-bmei@dhu.edu.cn>   
To: chlee@chu.edu.tw 
Subject: CISP'11-BMEI'11 P2543 Acceptance Notification 
 
 
Dear Chang-Hsing Lee,  
Paper ID : P2543 
Paper Title : Music Genre Classification Using Modulation Spectral Features and Multiple Prototype Vectors Representation 
 
(All Chinese characters in this email are intended for authors from China's mainland only. 请浏览会议网站上的中文注册和终稿上传信息。) 
Congratulations! We are pleased to inform you that your above paper has been accepted for presentation at the 4th International 
Conference on Image and Signal Processing (CISP'11) to be held from 15-17 October 2011, in Shanghai, China. After you complete the 
requirements below, your paper will appear in conference proceedings and will be indexed by both EI Compendex and ISTP, as well as the 
IEEE Xplore (IEEE Conference Record Number for CISP'11: 18205; IEEE Conference Record Number for BMEI'11: 18206. CISP IEEE 
Catalog Number: CFP1194D-CDR, ISBN: 978-1-4244-9305-0; BMEI IEEE Catalog Number: CFP1193D-CDR, ISBN: 978-1-4244-9350-0. 
CISP-BMEI 2008-2010 papers have already been indexed in EI Compendex). Substantially extended versions of best papers will be 
considered for publication in a CISP'11-BMEI'11 special issue of the Computers and Electrical Engineering journal (SCI-indexed). Only 
registered papers will be considered for the SCI journal special issue and only the selected authors will be notified by 30 October 2011.  
The conference will feature world-renowned keynote speakers: Thanos Stouraitis, President-Elect of IEEE Circuits and Systems Society; 
Seong-Whan Lee, Hyundai-Kia Motor Chair Professor; Metin Akay, Fellow of the Institute of Physics (IOP), the American Institute of 
Medical Biological Engineering (AIMBE) and the American Association for the Advancement of Science (AAAS); Yuan-Ting Zhang, Editor-
in-Chief for IEEE Trans. on Information Technology in Biomedicine (all Fellow of the IEEE).  
In order for your paper to be included in the proceedings indexed by Ei Compendex/ISTP, it is important that you closely follow each and 
every instruction below, as the acceptance is conditional on your accurate and timely reactions :  
1. Revise your paper, appropriately addressing the reviewer comments (at the end of this email, if any) which are intended to help you 
improve your paper for final publication. If any review comments seem vague, please revise your paper according to your best 
understanding.  
2. Strictly follow the IEEE format requirements; incorrectly formatted papers cannot be included in the proceedings. Please refer to the 
conference website disable_http://cisp-bmei.dhu.edu.cn/ for detailed formatting instructions and templates. Some of the formatting 
instructions are given below. Closely follow the instructions at the conference website (Final Submission page) to convert your paper to 
IEEE Xplore-compliant pdf file using PDF eXpress and upload your final camera-ready full paper as soon as possible, but latest by 8 
September 2011. Please ensure that all formulas, figures and embedded objects in your file are error-free. It is crucial to make sure your 
pdf file is IEEE Xplore-compliant using PDF eXpress. Otherwise your paper may not be included in the IEEE Xplore or indexed in EI/ISTP. 
Please submit your final paper, IEEE Copyright Form, Registration Form, and Payment Confirmation by clicking "Upload Final Paper" at 
the conference submission system. In addition, please click "Edit Submission" at the conference submission system to ensure that all 
paper information are accurate, including the paper title and all author names, emails, and affiliations. This step is very important, since the 
same author and paper information will appear in the proceedings and indexing. 
3. Please download IEEE Copyright Form at the conference website, complete the form, sign it, and upload a scanned form to us. In the 
Copyright Form, you will need to enter the conference name. Please note that your paper above is accepted in CISP'11, not BMEI'11. If 
you have more than one paper accepted, each paper may be accepted by a different conference. In addition, some papers were originally 
submitted to one of the two conferences (i.e., CISP'11 and BMEI'11), but were later transferred to the other conference by the Program 
Committee for better matches in topics.  
4. Each paper must have 1 dedicated registration with full payment received by 8 September 2011 for the paper to be included in the 
proceedings. The registration fee is US$400 or RMB 2600 for each paper of maximum 5 pages. These payments must also be received by 
8 September 2011 for the paper to be included in the proceedings. 
5. You may pay with a credit card through the secure link provided by Paypal.com available at the conference website registration page. 
You may also pay by telegraphic transfer to the following bank account: 
您也可以从邮局汇款： 
汇款地址：上海市松江区人民北路2999号东华大学信息科学与技术学院 
收款人：刘肖燕 
邮政编码：201620 
For authors outside of China's mainland:
Details of Beneficiary's Bank: BANK OF CHINA SHANGHAI CHANG NING SUB-BRANCH 
Address of Beneficiary's Bank: 2067 YAN'AN ROAD (WEST) SHANGHAI
Swift Code: BKCHCNBJ300
Beneficiary's Name: DONG HUA UNIVERSITY
Beneficiary's A/C No: 044175-8300-04360818091001
  
For authors within China's mainland (内地作者银行信息):
收款单位： 东华大学
开户行 工行上海市松江支行
账号 1001739619000026626
地址 上海市松江区人民北路2999号东华大学信息科学与技术学院
邮编: 201620
Page 1 of 2Inbox: 1150 messages unread - chlee@chu.edu.tw - 1463.8MB (71.4%)...
2012/1/10https://webmail.chu.edu.tw/cgi-bin/openwebmail/openwebmail-read.pl?s...
表 Y04 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                           101年 1 月 6 日 
報告人姓名  
李建興 
 
服務機構 
及職稱 
 
中華大學資訊工程系 
 
     時間 
會議 
     地點 
2010/11/4~2010/11/6 
日本-福岡 
本會核定 
補助文號 
計劃編號： 
NSC 99-2221-E-216-048- 
會議 
名稱 
 (中文) 
 (英文) The Fifth International Conference on Broadband and Wireless 
Computing, Communication (International Workshop on Intelligent 
Sensors and Smart Environments, ISSE)  
發表 
論文 
題目 
 (中文) 
 (英文) A 3D Model Retrieval System Based On The Cylindrical Projection Descriptor 
 
一、參加會議經過 
此次會議地點位於日本九州北部的福岡，我是在會議前一天來到福岡。會議地點位
於福岡西北方的福岡工業大學，必須搭 JR線火車前往，還好學校在車站附近，沒有找尋
上的困擾。此次會議同時有研討會 (BWCCA和 3PGCIC)合辦，因此有相當多的 workshops
同時進行，但 session rooms又分布於不同的建築物，感覺上人數比較稀疏。我所屬的
session(International Workshop on Intelligent Sensors and Smart Environments, ISSE)是在
會議的第二天中午，這個 workshop是由雲林科技大學張傳育教授所主持，雖然聆聽的人
數並不多，但發問討論卻極為踴躍情。 
 
  
會場-福岡工業大學 會場報到處 
 
附件三
 
表 Y04 
五、攜回資料名稱及內容 
BWCCA’2010論文光碟。 
 
 
六、其他 
1. 日本的 JR 火車真準時。日本人民有禮貌，街道乾淨。想要發展服務業，提昇觀光產
業的台灣，這些是值得學習的。 
2. 非常感謝國科會之補助得以參加該研討會。 
 
Novotni and Klein proposed a 3D shape retrieval 
method using 3D Zernike moments, which is naturally 
an extension of spherical harmonics based descriptors 
[13]. Ricard et al. [14] presented a 3D shape descriptor, 
the 3D Angular Radial Transform (3D-ART) for 3D 
model retrieval. First, the 3D models are represented in 
spherical coordinates. Next, a Principal Components 
Analysis (PCA) is applied to align the 3D models along 
the z-axis. Then, the 3D extension of MPEG-7’s ART 
[15] is applied to extract feature vectors. 
Mademlis et al. [16] decomposed 3D models into 
meaningful parts and an attributed graph was 
constructed based on the connectivity of the parts. Then, 
the 3D Distance Field Descriptor (3D-DFD) was 
computed and associated to the corresponding graph 
nodes for partial and global 3D model retrieval. 
Papadakis et al. [17] proposed two shape descriptors 
for 3D model retrieval. The 3D model was first aligned 
by continuous PCA (CPCA) or normal PCA (NPCA). 
In CPCA, the traditional one, the principal component 
is analyzed based on the covariance matrix computed 
from the coordinate vectors of the vertices, whereas in 
NPCA the covariance matrix is computed from the unit 
normal vectors of the mesh surfaces. The spherical 
harmonics was then applied on the filled 3D model to 
extract two feature vectors from the CPCA and NPCA 
aligned models separately. Vranic and Saupe proposed 
a modified PCA which used the corresponding triangle 
areas as weighting factors for covariance matrix 
computation [18]. The directions of 20 vertices on 
dodecahedron and the distances computed from the 
center point to the farthest intersections were used as 
features to index similar 3D models. 
Zarpalas et al. [19] proposed a 3D model retrieval 
method using 240 (12×20) 2D gray-level projection 
images, which are obtained by projecting a 3D model 
onto the 240 planes rendered from the 12 vertices of 20 
icosahedrons with different radii. Features were 
extracted from these gray-level images and combined 
to improve the performance. Another 3D model 
retrieval system used 20 depth images rendered from 
the 20 vertices of a dodecahedron [20]. The depth 
information of a pixel in each depth image was 
encoded as a 5-level character. Each row (depth line) in 
the depth image is then represented as a sequence of 
depth information. Dynamic programming was then 
used to compute the distance between two depth line 
descriptors. 
In this paper, the cylindrical projection descriptor 
(CPD) will be proposed for 3D model retrieval. To 
derive better retrieval results, the CPD will be 
combined with the radial distance descriptor (RDD) 
[21]. The rest of the paper is organized as follows. In 
Section 2, the proposed 3D model retrieval method will 
be described. In Section 3, gives the experimental 
results to show the effectiveness of the proposed 
features. Finally, conclusions are given in Section 4. 
2. THE PROPOSED 3D MODEL RETRIEVAL 
METHOD 
 
In this study, two descriptors, including the radial 
distance descriptor (RDD) [21] and the cylindrical 
projection descriptor (CPD) are used for 3D model 
retrieval. Before extracting the feature vectors, the 3D 
model is aligned according to the principal plane [12].  
 
2.1 Radial Distance Descriptor(RDD) 
 
The main steps for computing the radial distance 
descriptor [21] are described as follows: 
(1) 3D model is aligned by it’s the principal plane [12]. 
The principal plane is defined as the symmetric 
plane on which the sum of distance of all points 
projected is minimal. 
(2) The bounding cube is then decomposed into a voxel 
grid of size 100×100×100 (see Fig. 1). A voxel 
located at coordinates (x, y, z) will be defined as an 
opaque voxel, notated as Voxel(x, y, z) = 1, if there 
is a mesh located within this voxel; otherwise, the 
voxel is defined as a transparent voxel, notated as 
Voxel(x, y, z) = 0. To normalize for translation and 
scale, the object’s mass center, is moved to the 
point (0, 0, 0) and the average distance from 
non-zero voxels to the mass center is scaled to 25. 
(3) Six projection planes (see Fig. 1), which describe 
the radial distance from the 3D model surface to the 
mass center (see Fig. 2), are derived to represent a 
3D model. Each projection plane is represented by a 
gray level image in which the gray value denotes 
the distance from an opaque voxel to the mass 
center (see Fig. 3). Let the six projection planes be 
notated as Ik, k = 1, 2,…,6. Then ,the gray value of 
each pixel on these images is defined as follows:                 
 
,50,05for                     
)),,,(Voxel),,(R(max),(
5011
≤≤−
=
≤≤
zx
zyxzyxzxI
y  
 
,50,05for                     
)),,,(Voxel),,(R(max),(
5012
≤≤−
=
≤≤
yx
zyxzyxyxI
z  
 
,50,05for                     
)),,,(Voxel),,(R(max),(
5013
≤≤−
=
≤≤
zy
zyxzyxzyI
x  
 
,50,05for                      
)),,,(Voxel),,(R(max),(
1504
≤≤−
=
−≤≤−
zx
zyxzyxzxI
y  
 
,50,05for                     
)),,,(Voxel),,(R(max),(
1505
≤≤−
=
−≤≤−
yx
zyxzyxyxI
z  
 
,50,05for                     
)),,,(Voxel),,(R(max),(
1506
≤≤−
=
−≤≤−
zy
zyxzyxzyI
x  
where .),,(R 222 zyxzyx ++=  
 
551
 
 
 
 
 
Fig. 5 The cylindrical projection descriptor. PO  
represent the distance from the 3D model surface to the 
mass. 
 
   
  
   
  
Fig. 6 The three gray-level image F1, F2, and F3, are 
obtained by the cylindrial projection. 
 
2.3 Distance Computation 
 
Let TT6
T
2
T
1 ])( , ... ,)( ,)[( rddrddrddrdd = and
TT
6
T
2
T
1 ])( , ... ,)( ,)[(
bbbb rddrddrddrdd = denote the 
RDD of a query model and the b-th matching model in 
the database, respectively. The distance between the 
query model and the b-th matching model is defined as 
follows: 
∑∑
∑
= =
=
−=
−=
6
1
36
1RDD
1
6
1RDD
RDD
)(rdd)(rdd1            
1Dis
k i
b
kk
k
b
kk
b
ii
N
N
rddrdd
 
where 366RDD ×=N . CPD is defined as: 
Let TT3
T
2
T
1 ])( ,)( ,)[( cpdcpdcpdcpd = and
TT
3
T
2
T
1 ])( ,)( ,)[(
bbb cpdcpdcpdcpd =  denote the CPD 
of a query model and the b-th matching model in the 
database, respectively. The distance between the query 
model and the b-th matching model is defined as 
follows: 
∑∑
∑
= =
=
−=
−=
3
1
1024
1CPD
1
3
1CPD
CPD
)(cpd)(cpd1            
1Dis
k i
b
kk
k
b
kk
b
ii
N
N
cpdcpd
 
where 10243CPD ×=N . Finall we use three kinds of 
similarity measure methods to combine RDD and 
CPD: 
1) 
bb
b
CPDRDD
1 DisDis
1Sim
+
=  
2) Use the Borda Count Algorithm [34] to combine 
the RDD and CPD:  
bb
b
CPDRDD
2 RankRank
1Sim
+
=  
where. bRankRDD  and 
bRankCPD  are the retrieval 
rank values of the b-th matching model for the 
RDD and CPD, respectively. 
 
3. EXPERIMENTAL RESULTS 
 
To demonstrate the effectiveness of the proposed 
method for different 3D models, some experiments 
have been conducted on the Princeton Shape 
Benchmark (PSB) database [23]. The PSB database 
contains 1814 models (161 classes) which are divided 
into 907 training models (90 classes) and 907 test 
models (92 classes). Note that in this database the 
number of models is different for each class. Since the 
number of models in each class is different in the PSB 
database, the recall value ( jiRe ) for the j-th query 
model in the i-th class is defined as follows: 
,/ i
j
i
j
i NNRe =  
where jiN  denotes in the retrieval list the number of 
models labeled as class i and Ni is the total number of 
models in class i. The average recall values is defined 
as follows: 
92
1 1
1 iT j
i
i jS
Re Re
T
= =
= ∑∑
 
where Ts = T1 + T2 + … + T92. The Discounted 
Cumulative Gain (DCG) [28], will also be employed to 
compare the performance of different approaches. DCG 
at the k-th rank is recursively defined as follows: 
,
1 ,                           
2 ,
)(log
DCG
=DCG
1
2
1
⎪⎩
⎪⎨
⎧
=
≥+
−
kL
k
k
Lk
k
k  
where Lk=1 if the k-th retrieval model and the query 
one belong to the same class; otherwise, Lk=0. The 
overall DCG score for a query model q is defined as 
,DCG
maxk
where kmax is the total number of models in 
the database. DCG is clear that if the top-ranked 
models and the query one are of the same class,
 
max
DCG k will be larger than the retrieval result with 
similar models appearing in the bottom of the retrieval 
list. 
In our experimental, each model in database is 
presented as a query one. Table 1 compares the 
y x 
z 
P P’ 
O 
z 
x y 
z 
x y 
z 
y x 
F1 F3 F2 
553
Descriptor”, Pattern Recognition, pp. 283-295, 
2007. 
[11] C.T. Kuo and S.C. Cheng, “3D model retrieval 
using principal plane analysis and dynamic 
programming”, Pattern Recognition, pp. 742-755, 
2007. 
[12] J.L. Shih and W.C. Wang, “A 3D Model Retrieval 
Approach based on The Principal Plane 
Descriptor”, Proceedings of The Second 
International Conference on Innovative Computing, 
Information and Control (ICICIC), pp. 59-62, 
2007. 
[13] M. Ankerst, G. Kastenmuller, H.P. Kriegel, and T. 
Seidl, “3D shape histograms for similarity search 
and classification in spatial databases”, 
Proceedings of 6th International Symposium on 
Spatial Databases (SSD’99), pp. 207-226, 1999. 
[14] J. Ricard, D. Coeurjolly and A. Baskurt, 
“Generalizations of angular radial transform for 2D 
and 3D shape retrieval”, Pattern Recognition 
Letters,  pp. 2174-2186, 2005. 
[15] MPEG Video Group, “MPEG-7 Visual part of 
experimentation Model Version 9.0“, 2001. 
[16] A. Mademlis, P. Daras, A. Axenopoulos, D. 
Tzovaras, and M. G. Strintzis, “Combining      
Topological and Geometrical Features for Global 
and Partial 3D Shape Retrieval”, IEEE Tran. on 
Multimedia, pp. 819-831, 2008.  
[17] Panagiotis Papadakisa, Ioannis Pratikakisa, Stavros 
Perantonisa, Theoharis Theoharis,  “Efficient 3D 
shape matching and retrieval using a concrete 
radialized spherical projection representation”, 
Pattern Recognition, pp. 2437-2452, 2007.  
[18] D. V. Vranic and D. Saupe, “3D Model Retrieval”, 
Proceedings of the Spring Conference on 
Computer Graphics and its Applications 
(SCCG2000), pp. 89-93, 2000. 
[19] Dimitrios Zarpalas, Petros Daras, Apostolos 
Axenopoulos, Dimitrios Tzovaras, and Michael G. 
Strintzis, “3D Model Search and Retrieval Using 
the Spherical Trace Transform,” EURASIP Journal 
on Advances in Signal Processing, 2007. 
[20] Mohamed Chaouch, Anne Verroust-Blondet, “ A 
New Descriptor for 2D Depth Image Indexing and 
3D Model Retrieval”, IEEE International 
Conference on Image Processing, pp. 373-376, 
2007. 
[21] J.L. Shih, C.H. Lee and C.H Chuang, “A 3D Model 
Retrieval System Based On The Derivative Radial 
Distance”, Proceedings of The 22th IPPR 
Conference On Computer Vision, Graphics and 
Image Processing (CVGIP) 2009. 
[22] J.L. Shih, T.Y Huang, and Y.C. Wang, “A 3D 
Model Retrieval System Using the Derivative 
Elevation and 3D-ART”, Proceedings of the IEEE 
Asia-Pacific Services Computing Conference, 
(APSCC), pp. 739-744, 2008.  
[23] P. Shilane, P. Min, M. Kazhdan, T. Funkhouser, 
“The Princeton shape benchmark”, Proceedings of 
Shape Modeling Applications, pp. 167-178, 2004. 
[24] J. L. Shih and H. Y. Chen, “A 3D model retrieval 
approach using the interior and exterior 3D shape 
information”, Multimedia Tools Applicaion., vol. 
43, no. 1, pp. 45-62, May 2009. 
[25] B. K. P. Horn, “Extended Gaussian images”, in 
Proceedings of IEEE, vol. 72, no. 12, pp. 
1671-1686, Dec. 1984. 
[26] M. Kazhdan, T. Funkhouser, and S. Rusinkiewicz, 
“Rotation invariant spherical harmonic 
representation of 3D shape descriptors”, in 
Proceedings of Eurographics/ACM SIGGRAPH 
Symposium on Geometry processing, pp. 156-164, 
2003. 
[27] D. V. Vranic, “3D model retrieval”, Ph.D. 
Dissertation, University of Leipzig, Department of 
Computer Science, 2004. 
[28] C. B. Akgul, B. Sankur, Y. Yemez, and F. Schmitt, 
“3D model retrieval using probability 
density-based shape descriptors”, IEEE 
Transaction on Pattern Analysis and Machine 
Intelligence, vol. 31, no. 6, pp. 1117-1133, June, 
2009. 
[29] H. Laga, H. Takahashi, and M. Nakajima, 
“Spherical wavelet descriptors for content-based 
3D model retrieval,” in Proceedings of IEEE 
International Conference on Shape Modeling and 
Application (SMI‘06), 2006. 
[30] T. Zaharia and F. J. Preteux, “Shape-based retrieval 
of 3D mesh models”, in Proceedings of the IEEE 
International Conference on Multimedia and Expo, 
vol. 1, pp. 437-440, 2002. 
[31] E. Paquet and M. Rioux, “Nefertiti: A Query by 
Content Software for Three-Dimensional Models 
Databases Management”, in Proceedings of 
International Conference on Recent Advances in 
3D Digital Imaging and Modeling, pp. 345-352, 
1997. 
[32] D. V. Vranic, “An Improvement of Rotation 
Invariant 3D Shape Descriptor Based on Functions 
on Concentric Spheres”, in Proceedings of IEEE 
International Conference on Image Processing, pp. 
757-760, Sept. 2003. 
[33] T. F. Ansary, M. Daoudi, and J.-P. 
Vandeborre, ”3D Model Retrieval Based on 
Adaptive Views Clustering”, LNCS 3687, pp. 
473–483, 2005. 
[34] M. Jovic, Y. Hatakeyana, F. Dong, and K. Hirota, 
“Image Retrieval Based on Similarity Score Fusion 
from Feature Similarity Ranking Lists”, LNAI 
4223, pp. 461-470, 2006. 
555
國科會補助計畫衍生研發成果推廣資料表
日期:2012/01/04
國科會補助計畫
計畫名稱: 調變頻譜分析於音樂曲風及樂器音色之自動分類辨識之研究
計畫主持人: 李建興
計畫編號: 99-2221-E-216-048- 學門領域: 圖形辨識
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
