 ii 
目錄 
一、 計畫中文摘要 1 
ABSTRACT 2 
二、 報告內容 3 
(一) 前言 3 
(二) 研究目的 3 
(三) 文獻探討 4 
I. 基因序列比對 4 
II. Kolmogorov複雜度 5 
III. 機器學習 6 
(四) 研究方法 7 
(五) 結果與討論 9 
三、 計畫成果自評 12 
參考文獻： 13 
 
 1 
一、計畫中文摘要 
 
近年來生物技術的進步造成基因圖譜的大量解碼，也造成相關基因體研究的突飛猛進。我
們利用對基因圖譜的了解來推測序列所代表的功能意義和推測序列所代表物種之間的關
係。譬如研究基因和物種外在表現型之間的關係，和推測兩序列所代表物種的演化過程。
生物學上定義兩序列為 orthologs 如果它們演化自同一祖先，且在生物上表現出相同的功
能。如果 ortholog 的辨別可以量化，我們可以解決基因識別的問題，也可推得一些基因序
列與表現型的關係。雖然一定比例同源關係可以由序列上的相似性推得，但是也有極大比
例的同源序列對不能光靠簡單的序列排比辨別出來。部分同源序列彼此擁有很低的序列間
相似度或相似度幾乎無法和隨機序列間排比有很大的區別。我們需要比簡單序列排比更優
異的方法來發掘序列之間的同源關係。 
我們提出機器學習的模型架構來探討序列之間的同源關係。特別來說，我們採用決策樹的
分類方式來作為辨別序列同源性的一套方法。對於特徵指標的建立，除了常見序列排比產
生的相似度等以外，我們嘗試納入 Kolmogorov 複雜度的估計與計算。 
對於資料的選擇方面，我們挑選三種物種：人、家鼠和雞來作為計畫的試驗資料。計畫的
內容在於透過我們對人和家鼠基因序列的了解，來找尋雞的序列當中相應、演化來自共同
祖先與負責相同生物功能的序列。 
 
關鍵詞：同源性, Kolmogorov 複雜度, 決策樹, 序列比對, 資料壓縮, 推進術, 支援向量機 
 
 3 
二、報告內容 
 
(一)前言 
近年來生物技術的進步造成基因圖譜的大量解碼，也造成相關基因體研究的突飛猛
進。我們利用對基因圖譜的了解來推測序列所代表的功能意義和推測序列所代表物種之間
的關係。譬如研究基因和物種外在表現型之間的關係，和推測兩序列所代表物種的演化過
程。生物學上定義兩序列稱為 orthologs如果它們演化自同一祖先，且具有相同的生物功能。
如果 ortholog 的辨別可以用一些可具體言說的法則來敘述，我們可以藉此建立自動化的預
測流程，因而有效率的推得一些基因序列與表現型的關係。文獻顯示一定比例的同源關係
不能完全靠簡單的序列比對辨別出來 [5]。某些同源序列可能彼此擁有很低的序列間相似度
或相似度幾乎和隨機序列間相似度差不多。我們提出機器學習上決策樹的分類架構來探討
序列之間的同源關係。對於特徵指標的建立，除了常見序列比對產生的相似度等以外，我
們特別納入 Kolmogorov 複雜度的估計與計算。實驗顯示，Kolmogorov 複雜度的推估對於
兩序列的同源關係有相當程度的掌握。我們挑選三種物種：人、家鼠和雞來作為計畫的試
驗資料：透過我們對人和家鼠基因序列的了解，來找尋雞的序列當中相對應、演化來自共
同祖先與負責類似生物功能的序列。詳細內容已投稿於 [27]。除此之外，我們也研究序列
二級結構對於同源性預測的影響。初步的實驗顯示，二級結構對於同源性預測的增進有限 
[18][19]。這樣的結果和一般的期待有些距離。 
對於機器學習的分類問題本身，我們也因為此問題的研究，間接發現利用決策樹和
支援向量機(SVM)的整合，我們可以解決一個一般性的分類問題：對於連續型和離散型的
資料，我們可以利用決策樹擅長處理離散型資料，而支援向量機善於處理連續型資料分別
的特性，嘗試整合兩個模型，進而對一般混和型的資料作出不錯的分類結果。詳細內容已
投稿於 [28]，請參照。除此之外，我們也將相關對於 SVM 及核化 (Kernelization) 的研究，
及一些數據視覺化的經驗寫成書本章節，發表於 [38]。 
 
(二)研究目的 
大量基因圖譜的建立造成基因研究的突飛猛進 [37][14][7][8]。利用對基因圖譜的觀
察和了解，我們可以推測序列和物種外在表現型之間的聯繫，也可以說明物種之間的關係，
進而快速的掌握大量未解讀的基因資料。基因或蛋白質序列絕大部分並非憑空產生的，而
是循著某條演化歷史逐漸演變而來。這樣的原則告訴我們要對大量基因資料進行了解並無
想像中的毫無機會。生物學上定義兩條序列為 orthologs 如果：(一)它們均從一個共同的祖
先演化而來，(二)它們在生物上表現出相同的功能。對於一對 orthologs，我們可以由一個序
列的了解推得對另一序列的認識。所以如果一對序列是否彼此為 ortholog 的辨別可以經由
一定的規則或電腦演算法自動推測得知，那我們便可以獲得一些基因序列與表現型的關
係，也可以解決很多未知基因識別的問題。但是 ortholog 的識別並不簡單。雖然一定比例
的同源關係可以由一級序列 (primary sequence) 上的相似性推得，但是也有極大比例擁有
同源關係 (或互為 ortholog) 的序列對不能光靠簡單的序列比對 (alignment) 演算法或軟體
辨別出來；亦即經由一些常用序列比對演算法的計算，有些同源的序列彼此只擁有很低的
序列間吻合度 [5]。 
針對序列同源性預測的問題，我們提出機器學習的架構。給定一組含有 m個資料的
 5 
備。 
在全域的比對的演算法方面，一般都是運用動態規劃的方式來求取最佳解。
Needleman-Wunsch algorithm [24] 即是運用此方式來完成最佳解的計算。全域比對的意義在
於輸入序列自頭至尾的比對都在考慮的範圍之內。一般此種方式較多運用於長度比較相近
的序列之間的比對，或序列彼此相似情形位於兩序列絕大部分區域時。另一個全域比對的
演算法來自 Gotoh 提出的方法 [12]。它提供更有效率的演算方式，雖然仍較局部序列比對
演算法所花的時間長，但是結果似乎更為可靠，時間也在可容許的範圍之內。其它的全域
序列比對工具包含 CLUSTALW [34]。對於多序列的比對，CLUSTALW提供一般公認不錯的
比對結果。 
一般來說，基因的比對可以以全域的方式考慮，或是只注重彼此局部序列的相似性。
兩種方式各有其優劣。全域的方式考慮整條的序列，綜合的求取最佳解，所以所有的序列
內容都會在被考慮的範圍之內。有時這樣的方式並不適合處理彼此長度差距太大的序列，
或是只有少數局部範圍有相似性的序列。對於這樣的序列，一般我們可以考慮局部比對的
演算法來解決問題。但是局部比對的方式有時一般並不考慮序列的整體性，提供的結果有
時也只注重在一小部分的序列內容，提供幾條相似度高的小序列對，卻沒有描述到序列的
其它部分。段落對段落 (segment-to-segment) 的比對 (或稱作 gap-free 的比對) 提供一套折
衷的方式。比對的序列間，它只著重局部小序列的相似性，然後它將這些小序列比對且鏈
結在一起。對於兩兩小序列之間、彼此相似度低的部分、或長度上有不小差異的部分，演
算法並沒有作任何的考慮。以一般全域演算法的語言來說，它不會對沒有被比對好區域的
排比間隙 (gap) 作太多的減分。另一方面，因為它還是全序列一起作排比，所以還是有一
套方式對整體的排比結果有一套客觀可比較的評比方式。舉例來說，對於兩同源序列 ABCD
和 ABD的排比 (‘A’ ‘B’ ‘C’ or ‘D’ 代表的都是一段核苷酸或氨基酸)，如果‘C’的長度過大，
傳統的全域演算法預料不能找到理想的排比方式，因為長的間隙會獲得很大的減分而不會
成為最佳解。對於這類的排比方式，我們採用 Morgenstern et al.發展的 DIALIGN [21][22] 來
作為我們序列排比的工具。另外可以預期的，因為不需像一般全域比對的情形定義一些重
要參數 gap penalties (penalties for gap opening & gap extension)，此法可以省去之前決定最佳
參數的繁複過程，也可省去一些在參數決定過程中可能產生的偏失。 
 
II. Kolmogorov 複雜度 
Kolmogorov複雜度 (K複雜度或Kolmogorov entropy 或 Kolmogorov complexity) 的
分析較早運用於對一般字串的描述 [17]。在此，我們借用來作為基因序列的複雜度分析。
一個字串 (一般字串或只含有特別字元符號的基因序列) 的 Kolmogorov 複雜度被定義成描
述 (或輸出) 此字串的最小電腦程式的長度。這樣的定義非常抽象，一般無法為電腦計算出
來。我們可以利用壓縮的方法來嘗試逼近這樣的解答。我們可以量度一個字串壓縮後的長
度來決定此字串的複雜度。一般來說，壓縮程度愈不好的字串，表示字串的複雜度愈高、
愈無規則可言。生物序列並非亂數所構成，一般相信有某種程度的規則存在於序列裡 (或
序列與序列之間) [6]。我們利用兩種壓縮的方法來作為 Kolmogorov 複雜度的近似計算。第
一種是 Lempel 和 Ziv發展的 LZ77演算法 [15][39]。我們使用以 LZ77寫成的 gzip (在 UNIX 
system 的架構) 來作為實際的壓縮工具。另一種採用的壓縮方式是 Chen et al.發展出來的
GenCompress [6]。兩種壓縮方式都是採用不固定長度到固定長度 (variable-to-block) 的編碼
方式來作壓縮。兩者的差異在於 LZ77的編碼考慮完全相同子字串且將後發現的子字串用簡
 7 
問題。對此，C5.0 (或 C4.5) 提供可以接受的不錯結果來供我們參考。C5.0 等有相當豐富
的參數群可以針對不同的情形來作特別的調整，最後提供適當的決策樹。對於樹與資料過
分密合的問題，我們必須小心來處理。一類廣泛流行的模型建構哲學建議當兩組模型同時
都可以描述一組資料時 (有同樣的預測正確率)，我們應當選取比較簡單的那一組模型 
(Occam’s razor) [9][20]。這樣的說法不一定是正確的，雖然簡易的模型一般提供較高的效
率。一般的做法是在對未知資料的預測率降低的同時，我們就應該停止模型朝已知資料更
進一步的密合[20][10][11]。一個用於分類工作的方法，最小描述長度原則討論類似的議題 
[20]。對於決策樹來說，如果兩個樹都可以同樣正確的描述一組資料，我們猜測選擇比較小
的樹會增加對未來資料預測的正確率。這樣的猜想實踐在於對樹的裁減或是在適當的時間
停止樹的繼續分歧 [20][29]。 
推進術 (boosting) 是一個可以提高機器學習模型效能的重要方法。最著名的一種是
Freund 和 Schapire 發展的 AdaBoost [10][11]。對於一個效能未臻完美的學習模型，我們可
以透過推進術來進一步改進它。以 AdaBoost為例，這樣的改善可以無窮盡的朝向完美的方
向來前進。它的作法是針對初步分類有錯的資料點給予較大的權重，再依此作另一次的模
型建構。之後的分類模型可以再進一步的被用來檢驗資料的正確率。對於分類模型仍然不
能正確預測的資料點，我們再給予比原先更大的權重，之後依此再建立一個新的學習模型，
以此類推。C5.0 和 C4.5 的一項最大差異就在於 C5.0 提供推進術的功能，雖然它所使用的
推進術是 AdaBoost 的一種變形 [31]。C5.0 運用推進術的技巧建立一連串的決策樹。每一
個樹都是針對之前的樹預測不好的資料點作權重的重分配作出來的新樹。最後的資料分類
預測取決於這一連串決策樹對此資料的分類 (結果不盡相同)，之後再作一個有權重的平均 
(voting) 來決定最後的分類結果。 
決策樹並非唯一一套建立機器學習模型的方法。另外的幾種機器學習方法包含支撐
向量機 (support vector machine) [35][36]、類神經網路 (neural network)、基因演算法等。決
策樹的模型和這幾類機器學習模型最大的差別在於決策樹提供相當透明的結果。依此相關
領域的專家可以利用產生的決策樹模型作更進一步的修正或改良。這在其它的模型架構下
並不容易辦到。一些生物上的問題曾以決策樹獲得不錯的結果，決策樹的模型簡化了其它
做法可能帶來分析上的困難。譬如 Morgan system [32] 利用決策樹解決 intron-exon 的識別
問題。另外 Arikawa et al. [3] 也成功運用決策樹找尋到蛋白質負責特殊功能的區域。 
對於決策樹本身，除了 C5.0 (或 C4.5) 也有以外的幾種演算法。譬如在決策樹中，
決策節點可以以非二元的方式進行分割。其它的選擇是在資料點的空間中用非垂直的平面 
(仍為線性) 來作切割，或運用不同的標準 (不用 entropy) 來作空間的切割，或採取貪進法
以外的方式來建立決策樹等，各方法的優劣及相關的討論可以參考 [29]。 
 
(四)研究方法 
我們挑選三種物種：人、家鼠和雞來作為計畫的試驗資料。我們企圖透過一組已知
互為 orthologs 的人和家鼠的序列，來找尋雞的基因圖譜中和這組序列互為 ortholog 的序列。 
挑選人和家鼠的原因主要在於人和家鼠是現在為止，大型生物中基因圖譜比較完整的兩個
物種，同時相關的資料較多也較正確。依此我們可以獲得更多的序列與序列相關的資訊來
作為機器學習的輸入資料。至於挑選雞的原因在於雞在演化歷史中和人與家鼠的演化距離
約略相等，我們可以依此運用分子時鐘假說 (molecular clock hypothesis) [23] 來假設突變發
生在人與雞之間和人與家鼠之間的頻率是相近的，如此情形將讓我們的研究工作更為單純
 9 
關係愈遠，反之亦然。我們給出函數 D 和 D’如下 
 
 
 
和 Li et al. [16]提出的函數 d比較則可以如下的關係式來說明 
 
 
 
所以可以看出來以作為分類工具的特徵值的觀點來看，函數 d和函數 D’將具有
一樣的功效。而函數 D’和函數 D的差距在於函數 D有利用到條件 Kolmogorov
複雜度的估計。S 和 T 定義為序列 s 或 t 所屬物種「所有可獲得基因序列所成
的集合」。Conditioned 複雜度如 K(s | S)的計算在於對於某序列 s的估計，如果
假設此序列物種的所有序列樣式 (pattern) S 為已知，我們需要多少額外的
coding 來描述這個序列。 
(C) 對未知類別資料作預測：(B)的結果是一個或多個決策樹 (依照是否有採用推進
術而定)。我們可以依此來對未曾見過的資料作是否為同源序列的預測。對於新
得到的序列，我們可以依照類似(B)的第一步作特徵指標的計算。得到的特徵數
列就可以運用決策樹來求得估計的類別 (是否為同源的序列)，譬如預測 
(CX, Hi, Mi),   ∀ 1 ≤ i ≤ K 
的結果來說明新得到的 CX和哪一對的人-鼠序列是同源的。值得一提的是 C5.0
可以運用推進術來增進分類的效果，結果也有類似支援向量機將正負兩類資料
中的間隔 (margin) 推開的效果。C5.0 的結果除了作出序列類別的估計以外，
也會提供數值來描述此估計的可靠性 [29]。 
以這個架構為前提，我們得到一些正面的結果，我們提出的 Kolmogorov 複雜度對於
序列同源關係判斷有很大的幫助，詳下節的討論。 
 
(五)結果與討論 
在說明 Kolmogorov 複雜度對於序列同源判斷有很大的幫助之前，我們需要針對
Kolmogorov 複雜度做一測試，看何種關於 Kolmogorov 複雜度的估計最適合我們分類的工
作。我們可以利用不同的壓縮演算法做為 Kolmogorov 複雜度的估計方式。在這裡我們比較
GenCompress 和 gzip 兩種壓縮演算法。對於將 Kolmogorov 複雜度估計轉化為距離，我們
也有一些選擇，我們可以用較簡單的函數 D’(s, t)或較複雜的函數 D(s, t)，另外我們也可以
用其它不同於 S 或 T 譬如亂數產生的一序列做為背景序列來做計算。在圖表一中，我們針
對一些不同形式關於 Kolmogorov 複雜度的估計分別依序加入分類器的訓練來觀察哪一種
估計得到最好的結果。測試的項目為(1)以 GenCompress 做為壓縮的演算法、採用複雜公式
D(s, t)，(2)以 gzip 做為壓縮的演算法、採用複雜公式 D(s, t)，(3)計算較簡單的公式 D’(s, t)、
利用 GenCompress 做為壓縮的演算法，(4)利用亂數產生的序列來當做長序列 S或 T、利用
GenCompress 做為壓縮的演算法。結果顯示，利用 GenCompress 演算法來做壓縮與採用複
雜公式 D(s, t)是最好的對 Kolmogorov 複雜度估計的組合。此組合不論在正或負資料中均有
較低的判斷錯誤率。 
 
)|()|(
)(
),(
TtKSsK
stK
tsD
+
=
)(
)()()(2
)(
)|()(
1),(
stK
tKsKstK
stK
tsKsK
tsd
−−
≈
−
−=
)()(
)(
),('
tKsK
stK
tsD
+
=
 11 
圖表二、10-FOLD cross-validation的結果，比較五類關於序列相似度的特徵值集合 
Attribute Set  Missed in Testing: +/– (%) 
  A: only one in (& class)  B: at most one out 
w. all attr.    41.9 / 15.6 (0.87%) 
Matcher  (w.) 71.7 / 53.7 (1.89%)  (w/o) 41.3 / 16.3 (0.87%) 
Gotoh  (w.) 70.8 / 67.7 (2.09%)  (w/o) 43.2 / 15.0 (0.88%) 
ClustalW  (w.) 45.3 / 46.1 (1.38%)  (w/o) 41.9 / 15.5 (0.87%) 
DIALIGN  (w.) 51.7 / 51.8 (1.56%)  (w/o) 40.1 / 16.1 (0.85%) 
GenCompress  (w.) 101.6 / 12.6 (1.72%)  (w/o) 47.9 / 50.3 (1.48%) 
 
 13
參考文獻： 
[1] Altschul, S. F., Gish, W., Miller, W., Myers, E. W., & Lipman, D. J. (1990). “Basic local 
alignment search tool”, J. Mol. Biol., 215: 403–410. 
 
[2] Altschul, S. F., Madden, T. L., Schäffer, A. A., Zhang, J., Zhang, Z., Miller, W., Lipman, D. 
J. (1997). “Gapped BLAST and PSI-BLAST: A new generation of protein database search 
programs”, Nucleic Acids Research, 25(17): 3389–3402. 
 
[3] Arikawa, S., Miyano, S., Shinohara, A., Kuhara, S., Mukouchi, Y., and Shinohara, T. (1993). 
“A machine discovery from amino-acid-sequences by decision trees over regular patterns”, 
New Generation Computing, 11: 361–375. 
 
[4] Benedetto, D., Caglioti, E., & Loreto, V. (2002). “Language trees and zipping”, Physical 
Review Letters, 88: 048702. 
 
[5] Brenner, S. E., Chothia, C. and Hubbard T. J. P. (1998). “Assessing sequence comparison 
methods with reliable structurally identified distant evolutionary relationships”, Proc. Natl. 
Acad. Sci. USA, 95(11): 6073–6078. 
 
[6] Chen, X., Kwong, S., & Li, M. (2000). “A compression algorithm for dna sequences and its 
applications in genome comparison”, RECOMB, p. 107. 
 
[7] Collins, F. S. and Galas, D. (1993). “A new 5-year plan for the U.S. Human Genome 
Project”, Science, 262: 43–50. 
 
[8] Collins, F. S. et al. (1998). “New goals for the U.S. Human Genome Project: 1998-2003”, 
Science, 282: 682–689. 
 
[9] Cover, T. and Thomas, J. (1991). Elements of Information Theory, Wiley & Sons, NY. 
 
[10] Freund, Y., & Schapire, R. E. (1997). “A decision-theoretic generalization of on-line 
learning and an application to boosting”, Journal of Computer and System Science, 55, 
119–139. 
 
[11] Freund, Y., & Schapire, R. E. (1999). “A short introduction to boosting”, Journal of 
Japanese Society for Artificial Intelligence, 14(5): 771–780. 
 
[12] Gotoh, O. (1982). “An improved algorithm for matching biological sequences”, J. Mol. Biol., 
162: 705–708. 
 
[13] Huang, X., & Miller, W. (1991). “A time efficient, linear space local similarity algorithm”, 
Adv. Appl. Math., 12, 337–357. 
 
[14] International Human Genome Sequencing Consortium. (2001). “Initial sequencing and 
analysis of the human genome”, Nature, 409: 860–921. 
 
[15] Lempel, A., & Ziv, J. (1977). “A universal algorithm for sequential data compression”, 
IEEE Trans. Inf. Theory, 23, 337–343. 
 
[16] Li, M., Badger, J. H., Chen, X., Kwong, S., Kearney, P., & Zhang, H. (2001). “An 
information-based sequence distance and its application to whole mitochondrial genome 
phylogeny”, Bioinformatics, 17, 149–154. 
 15
[33] Smith, T. F., & Waterman, M. S. (1981). “Comparison of biosequences”, Adv. Appl. Math., 
2, 482–489. 
 
[34] Thompson, J. D., Higgins, D. G., & Gibson, T. J. (1994). “Clustal w: improving the 
sensitivity of progressive multiple sequence alignment through sequence weighting 
position-specific gap penalties and weight matrix choice”, Nucleic Acids Res., 22, 
4673–4680. 
 
[35] Vapnik, V. (1995). The Nature of Statistical Learning Theory, Springer Verlag, New York. 
 
[36] Vapnik, V. (1998). Statistical Learning Theory, John Wiley and Sons, New York, 1998. 
 
[37] Venter, J. C. et al. (2001). “The sequence of the human genome”, Science, 291: 1304–1351. 
 
[38] Chang, Y.-C., Lee, Y.-J., Pao, H.-K., Lee, M.-H. and Huang, S.-Y. (2006). “Data 
Visualization via Kernel Machines” in Chen, C.-H., Hardle, W. and Unwin, A. (editors), 
Handbook of Computational Statistics (Volume III), Data Visualization, Springer Verlag. 
 
[39] Ziv, J. and Lempel, A. (1978). “Compression of individual sequences via variable-rate 
coding”, IEEE Trans. Inf. Theory, 24(5): 530–536. 
Review on Compstat 2006 Satellite Workshop on  
Data and Information Visualization 2006 (DIV 2006)
*
 
(出席國際會議心得報告) 
 
會議地點： Humboldt-Universität zu Berlin, 
School of Business and Economics,  
Institute for Statistics and Econometrics,  
Berlin, Germany 
會議日期： 2006 年 8 月 23-25 日 
參與人員： 台灣科技大學資工系鮑興國助理教授 
 
會議簡介 
DIV 為經常舉辦，統計領域裡亟負盛名的研討會，歷年來均有不錯的口碑，此次
由德國知名大學 Humboldt-Universität zu Berlin 主辦。會議的主要議題有以下幾
項：一、Machine Learning，二、Data Mining，三、Data Visualization，四、Machine 
Learning 或 Data Mining 相關演算工具。我以參與一、Machine Learning，二、Data 
Mining，三、Data Visualization 等的議題討論為主。至於我所發表論文的報告則
被安排在八月 25 日。報告題目為“Data Visualization via Kernel Machines” [1]。 
 
心得報告 
本年度依照計畫「利用機器學習理論分析及判別基因同源性」†的研究架構針對
一組的生物序列作同源性的分析。分析的方式是依據機器學習的模型，利用決策
樹來作序列同源性的推測。研究的方向可以概略分為兩個部分。第一、針對一基
因序列集合，我們利用決策樹 C5.0 [5][1]來作對此集合同源性關係的推斷。不同
於一般特徵值的搜尋，我們強調 Kolmogorov 複雜度的估計以求得有用的特徵
值。實驗顯示，這類特徵值可以有效的提升我們對於同源性關係的預測。此結果
已有不錯的結果，發表於 [3]。第二、方法學上，我們針對機器學習的分類問題
作更進一步的探討。我們考慮較一般的問題架構，對於一含有連續型特徵 (譬如
人的身高、體重等) 和離散型特徵 (譬如人的居住地、性別等) 的資料集，我們
企圖給出較具有分類效力的分類樹 (叫做模型樹 [4])。此分類問題的研究當然可
以運用於各類同時具有連續型和離散型特徵的資料集。對於序列同源性關係的預
測，也會有顯著的影響。我們參考的訓練資料集 (training data) 本身即同時具有
連續型 (譬如一序列比對所得出的相似度) 和離散型 (譬如一序列組歸類為具有
某類生物功能) 的兩類特徵。依照 [4]的結果，我們所建構的模型樹可以更正確
                                                 
*
 Taiwan National Science Council Grant # NSC 94-2213-E-011-006. 
 
