II
中文摘要：
為了在分散式記憶體電腦系統上更有效率的執行資料平行程式，將陣列資料重分配
通常是被需要的。重分配一個陣列的資料所需要的代價主要包括陣列資料索引的計算，
訊息的包裹與解包裹，訊息的傳輸，以及訊息傳輸的排程。處理器對應技術 (processor
mapping technique) 是一個試圖減少被重分配的陣列資料之數量的技術，以期降低資料
重分配的訊息傳輸代價。另一方面，程式設計人員可能希望在重分配陣列的資料時，某
些陣列資料不要被重分配出去 (也就是被區域化)，以滿足計算的需求並使得整體的系
統效能可以被改善。在這個研究中，我們提出一個適用於 Block-Cyclic 資料重分配的
彈性的處理器對應技術 (flexible processor mapping technique)，此技術讓程式設計人員可
以自行決定那些陣列資料在資料重分配時要被區域化。此外，在使用同步訊息傳輸的方
式來施行陣列資料的重分配時，電腦間的訊息傳輸必須被妥善的安排，以避免引發較高
的資料傳輸代價。在這個研究中，我們針對使用彈性的處理器對應技術的 Block-Cyclic
資料重分配，提出一個有效率的最佳的訊息傳輸排程的方法。
關鍵詞：平行編譯器、資料平行程式設計、分散式記憶體電腦系統、資料重分配、處理
器對應、訊息傳送介面
英文摘要：
Array redistribution is usually required for more efficiently executing a data-parallel
program on the distributed memory multicomputers. The cost for redistributing an array
mainly includes the overheads for index computation, message packing/unpacking, data
transfer and communication scheduling. The processor mapping technique strives to reduce
the amount of data elements to be redistributed for minimizing the data transfer cost in the
redistribution. On the other hand, the programmers may want certain data elements to be
un-redistributed (localized) in the redistribution for meeting the computing requirement such
that the overall system performance could be improved. In this research, we propose a flexible
processor mapping technique for Block-Cyclic redistribution, which can allow the
programmers to determine which data elements to be localized in the redistribution. Besides,
in performing an array redistribution using synchronous communication mode, the data
communications among processors should be properly arranged for not incurring higher data
transfer cost. In this research, we propose an efficient optimal communication scheduling
method for the flexible processor mapping technique being applied Block-Cyclic
redistribution.
Keywords: Parallel Compiler, Data-Parallel Programming, Distributed Memory
Multicomputers, Data Redistribution, Processor Mapping, MPI
2況下並非一對一 (one-to-one) 或映成的 (onto)，因而在應用上可能有所限制。
研究方法：
我們先對 Kalns 等人的處理器對應技術做一簡單的描述。
定義一：對於一個 Block to Cyclic(r) 的資料重分配，我們稱 Block 為來源分配 (source
distribution)，而稱 Cyclic(r) 為目的分配 (destination distribution)。r 是一資料區塊所包含
的資料元素數量，而資料區塊是資料重分配的基本單位。
定義二：在程式的初始化階段，每一個處理器節點會被指定一個實體的處理器編碼
(physical processor id，記作 ppid)，表示該節點的實體順序。一個節點的實體順序在整個程
式的執行期間不會被變更。假設資料重分配是在具有 m 個節點的分散式記憶體電腦系統
上執行，而此 m 個節點分別被指定 0、1、 及 m-1 的實體順序。
定義三：在資料(重)分配時，每一個處理器節點會被指定一個邏輯的處理器編碼 (logic
processor id，記作 lpid)，表示該節點的邏輯順序。一般來說，在起初的資料分配時，指定
給一個節點的邏輯順序是與其實體順序相同的，但該節點的邏輯順序可能會在資料重分配
時變更。對於一個資料重分配，我們以 slpid (source lpid) 來表示節點在來源分配狀態下
的邏輯順序，並以 dlpid來表示節點在目的分配狀態下的邏輯順序；我們以 Pi 來表示 slpid
為 i 的節點，以 Pj 來表示 dlpid 為 j 的節點；我們稱在來源 (目的) 分配狀態下的節點
為來源 (目的) 節點。
定義四：在 m 節點上對一個一維陣列 A[0:n-1] 施行資料重分配時，我們以 SLAi[0:n/m-1]
表示分佈在來源節點 Pi 的所有陣列資料，0im-1，而以 DLAj[0:n/m-1] 表示分佈於目的
節點 Pj 的所有陣列資料，0jm-1。
(a) (b)
(圖一) 在 7 個節點上施行一般的 Block(4×r) to Cyclic(r) 的資料重分配
(a) (b)
(圖二) 在 7 個節點上施行有使用處理器對應技術的 Block(4×r) to Cyclic(r) 的資料重分
配
如前所述，處理器對應技術是在陣列資料再分配時，將節點的邏輯順序重新排列，並
Source:Block(4r)
block index
slpid ppid
0 1 2 3
0 0 0 1 2 3
1 1 4 5 6 7
2 2 8 9 10 11
3 3 12 13 14 15
4 4 16 17 18 19
5 5 20 21 22 23
6 6 24 25 26 27
Destination:Cyclic(r)
block index
dlpid ppid
0 1 2 3
0 0 0 7 14 21
1 1 1 8 15 22
2 2 2 9 16 23
3 3 3 10 17 24
4 4 4 11 18 25
5 5 5 12 19 26
6 6 6 13 20 27
Source:Block(4r)
block index
slpid ppid
0 1 2 3
0 0 0 1 2 3
1 1 4 5 6 7
2 2 8 9 10 11
3 3 12 13 14 15
4 4 16 17 18 19
5 5 20 21 22 23
6 6 24 25 26 27
Destination:Cyclic(r)
block index
dlpid ppid
0 1 2 3
0 0 0 7 14 21
4 1 4 11 18 25
1 2 1 8 15 22
5 3 5 12 19 26
2 4 2 9 16 23
6 5 6 13 20 27
3 6 3 10 17 24
4其中假如 k<m，那麼 0uk-1，假如 km，那麼 0um-1。其實 (Eq1) 函數是 Kalns 等
人所提出的處理器對應函數的延伸 (他們直接視 u 為 0)。然而 (Eq1) 在應用上要注意的
是，此對應關係有可能不是一對一的對應，也就是說某些來源節點以上述的對應關係來重
新排列，會對應到相同的目的節點(會具有相同的目的節點邏輯順序 (dlpid))。根據 [13] 的
研究，假如 gcd(k,m)=1，那麼 (Eq1) 就是一對一的對應，否則就不是。
例如，在 5 個節點上對一個一維陣列施行 9rr 的資料重分配。在此例中，k=9，
m=5，gcd(k,m)=1。在資料重分配時，如果使用 (Eq1) 對應函數來重新排列節點的邏輯順
序 (假設被給定的 u 值為 2)，那麼來源節點 P0，P1，P2，P3 和 P4 會分別一對一的對
應到目的節點 P2，P1，P0，P4 和 P3，如(圖四)所示。這個對應也使得所有來源節點的
資料區塊 2，成為在資料重分配時第一個被被區域化的資料區塊。
(圖
四 )
在
5
個
節
點上施行有使用 (Eq1) 處理器對應技術的 9×rr 的資料重分配
然而，對於 gcd(k,m)1 的情況，我們必須要用另外的對應技術來作為節點重新排列的
依據。根據 [13] 的研究，在 gcd(k,m)=g1 的情況下使用 (Eq1) 作為節點的對應，來源節
點會被分成 m/g 個節點群組，每一節點群組包含有 g 個節點。同一群組的 g 個節點會
對應到相同的 dlpid，並且對應到某一群組的 dlpid 和對應到另一群組的 dlpid 之間的差值
為 g。要為此種情況建立一對一的對應函數，我們可以考慮每一節點群組有一個相對應的
base dlpid，而在相同群組的每一節點被給予一個順序 ，0g-1，此 被用來決定相對
於 base dlpid 的位移值 (offset)，而一個來源節點在重新排列後真正對應的 dlpid 則是由其
所屬的節點群組的 base dlpid 及其被給予的 所得的位移值來決定。實務上被給予一個
來源節點的 代表著該來源節點真正對應的 dlpid 與其原先(利用 (Eq1) 函數) 對應的
dlpid 的差值。因為不同節點群組的 base dlpid 之差值為 g，我們可以將一節點群組的 base
dlpid 與 (Eq1) 相關聯，並且以 mod(ik+u,m)/gg 來計算其值。同樣的，因為每一個節
點群組包含有 g 個節點而有 g 個順序要被指定，我們可以用 mod(mod(ik+u,m)+,g) 來
計算出相對一個順序 的位移值。也就是說在，對於一個 slpid 為 i 的節點，其所對應
的 dlpid (假設為 j) 可以用下列的對應關係來表示：
j=f(i)=mod(ik+u,m)/gg+mod(mod(ik+u,m)+,g) (Eq2)
很明顯的，當 g1 時，(Eq2) 是一對一且映成的對應。
另一方面，此種對節點 slpid 與 dlpid 的新的一對一對應，也會導致節點上被區域化
的資料區塊被重新定義。就來源節點而言，其上每一 lcc 資料片段的資料元素，可以被分
成 k/g 個資料群 (data cluster)，而每一資料群包含有 g 個資料區塊。在 g1 的情況下，
若使用 (Eq1) 作為節點的對應，那麼對每一 lcc 資料片段來說，資料區塊 u 是第一個被
區域化的資料區塊，此資料區塊是屬於索引值為 u/g的資料群，此資料群具有 u/gg
的基值 (base value)。根據以上討論，一個節點群組的每一節點被給予一個順序 ，來決定
相對於其 base dlpid 的位移值，以得到它真正的 dlpid。此 同時也可用來為一來源節點
的每一 lcc 資料片段，決定相對於其原來資料群基值的位移值，以得到它真正的被區域化
的資料區塊。簡言之，在 g1 的情況下，使用 (Eq2) 作為節點的對應，對來源節點 i 的
Source:Block-Cyclic(9r)
lcc1
block index
slpid ppid
0 1 2 3 4 5 6 7 8
0 0 0 1 2 3 4 5 6 7 8
1 1 9 10 11 12 13 14 15 16 17
2 2 18 19 20 21 22 23 24 25 26
3 3 27 28 29 30 31 32 33 34 35
4 4 36 37 38 39 40 41 42 43 44
Destination:Block-Cyclic(r)
lcc1
block index
dlpid ppid
0 1 2 3 4 5 6 7 8
2 0 2 7 12 17 22 27 32 37 42
1 1 1 6 11 16 21 26 31 36 41
0 2 0 5 10 15 20 25 30 35 40
4 3 4 9 14 19 24 29 34 39 44
3 4 3 8 13 18 23 28 33 38 43
6度是一樣的。例如：在第一個傳輸步驟中，目的節點 P2 的資料區塊 0、1，目的節點
P1 的資料區塊 2、3，目的節點 P0 的資料區塊 4、5，目的節點 P4 的資料區塊 5、
6 以及目的節點 P3 的資料區塊 7、8，分別被安排是從來源節點 P0、P1、P2、P3 以
及 P4 所接收的。值得注意的是，在每一個傳輸步驟中，所有被傳輸的訊息存放在它
們來源節點的相同位置上。
(圖
六)
使
用
(1)
中
的訊息傳送排程方法於(圖四)的資料重分配之情形
(2) 在 m 個節點上施行 krr 的資料重分配時，若 gcd(k,m)=g1 且 (Eq2) 被用作節點的
對應，在把目的節點的每一個 lcc 資料片段的資料元素看作是一個環圈的情況下，安
排每一個目的節點 Pj (0jm-1) 的每一個 lcc 資料片段，從資料區塊 (ik+)/m(其中
=u/gg+ mod(mod(ik+u,m)+,g)) 放置所接收的第一個訊息，並延沿著環圈放置所接
收的其它訊息，這樣的訊息傳輸排程是一個最佳化的訊息傳輸排程。
(圖
七)
使
用
(2)
中
的
訊息傳送排程方法於(圖五)的資料重分配之情形
如(圖七)所示，將上述的訊息傳輸排程方法應用於(圖五)中的資料重分配時，在每一
個傳輸步驟中，每一目的節點被安排從不同的來源節點接收資料，且所接收的訊息長
度是一樣的。例如：在第一個傳輸步驟中，目的節點 P2 的資料區塊 0、1，目的節點
P4 的資料區塊 1、2，目的節點 P1 的資料區塊 3、4，目的節點 P3 的資料區塊 4、
5，目的節點 P0 的資料區塊 6、7 以及目的節點 P5 的資料區塊 7、8，分別被安排是
從來源節點 P0、P1、P2、P3、P4 以及 P5 所接收的。值得注意的是，在每一個傳輸步
驟中，所有被傳輸的訊息存放在它們來源節點的相同資料群上。
從(圖六)與(圖七)中可很明顯的看出，將前述的最佳訊息傳輸排程方法應用在區域化
的資料重分配時，被安排在第一傳輸步驟所傳送的訊息，事實上就是那些要被區域化的資
料，它們不需要被真正的傳送出去，只需在原節點上調整其存放位置即可。
結果與討論：
我們在安裝有 MPI (message passing interface) [26] 平行程式環境的 PC-Cluster 分散
式記憶體電腦系統上，實作陣列資料區域化的資料重分配。我們以 C 程式語言以及 MPI
Source:Block-Cyclic(9r)
lcc1
block index
slpid ppid
0 1 2 3 4 5 6 7 8
0 0 0 1 2 3 4 5 6 7 8
1 1 9 10 11 12 13 14 15 16 17
2 2 18 19 20 21 22 23 24 25 26
3 3 27 28 29 30 31 32 33 34 35
4 4 36 37 38 39 40 41 42 43 44
Destination:Block-Cyclic(r)
lcc1
block index
dlpid ppid
0 1 2 3 4 5 6 7 8
2 0 2 7 12 17 22 27 32 37 42
1 1 1 6 11 16 21 26 31 36 41
0 2 0 5 10 15 20 25 30 35 40
4 3 4 9 14 19 24 29 34 39 44
3 4 3 8 13 18 23 28 33 38 43
Source:Block-Cyclic(9r)
lcc1
block index
slpid ppid
0 1 2 3 4 5 6 7 8
0 0 0 1 2 3 4 5 6 7 8
1 1 9 10 11 12 13 14 15 16 17
2 2 18 19 20 21 22 23 24 25 26
3 3 27 28 29 30 31 32 33 34 35
4 4 36 37 38 39 40 41 42 43 44
5 5 45 46 47 48 49 50 51 52 53
Destination:Block-Cyclic(r)
lcc1
block index
dlpid ppid
0 1 2 3 4 5 6 7 8
2 0 2 8 14 20 26 32 38 44 50
4 1 4 10 16 22 28 34 40 46 52
1 2 1 7 13 19 25 31 37 43 49
3 3 3 9 15 21 27 33 39 45 51
0 4 0 6 12 18 24 30 36 42 48
5 5 5 11 17 23 29 35 41 47 53
816. A. W. Lam and M. S. Lam. Maximizing Parallelism and Minimizing Synchronization with
Affine Partitions. Parallel Computing, Vol. 24, No. 3-4, pp. 445-475, May 1998.
17. N. Park, V. K. Prasanna, and C. S. Raghavendra. Efficient Algorithms for Block-Cyclic Array
Redistribution Between Processor Sets. IEEE Transactions on Parallel and Distributed
Systems, Vol. 10, No. 12, pp. 1217-1240, December 1999.
18. L. Prylli and B. Tourancheau. Fast Runtime Block Cyclic Data Redistribution on
Multiprocessors. Journal of Parallel and Distributed Computing, Vol. 45, pp. 63-72, 1997.
19. J. Ramanujam and P. Sadayappan. Compile-Time Techniques for Data Distribution in
Distributed Memory Machines. IEEE Transactions on Parallel and Distributed Systems, Vol.
2, No. 4, pp. 472482, October 1991.
20. S. Ramaswamy and P. Banerjee. Automatic Generation of Efficient Array Redistribution
Routines for Distributed Memory Multicomputers. In Frontiers '95: The Fifth Symposium on
the Frontiers of Massively Parallel Computation, pp. 342-349, February 1995.
21. S. Ramaswamy, B. Simons, and P. Banerjee. Optimization for Efficient Array Redistribution
on Distributed Memory Multicomputers. Journal of Parallel and Distributed Computing, Vol.
38, pp. 217-228, 1996.
22. R. Thakur, A. Choudhary, and G. Fox. Runtime Array Redistribution in HPF Programs. In
Proceedings of Scalable High Performance Computing Conference, pp. 309-316, May 1994.
23. R. Thakur, A. Choudhary, and J. Ramanujam. Efficient Algorithm for Array Redistribution,
IEEE Transactions on Parallel and Distributed Systems, Vol. 7, No. 6, pp. 587-594, June
1996.
24. A. Wakatani and M. Wolfe. Optimization of Array Redistribution for Distributed Memory
Multicomputers. Parallel Computing, Vol. 21, No. 9, pp. 1485-1490, 1995.
25. D. W. Walker and S. W. Otto. Redistribution of Block-Cyclic Data Distributions Using MPI.
Concurrency: Practice and Experience, Vol. 8, No 9, pp. 707-728, 1996.
26. The Message Passing Interface (MPI) standard, http//www-unix.mcs.anl.gov/mpi.
計畫成果自評：
我們的研究按原計畫內容發展出一個彈性的處理器對應技術，以有助於資料重分配時
的資料區域化需求，同時也依原計畫內容發展出一個最佳的訊息傳輸排程的方法，可以很
有效率的應用在資料區域化的資料重分配上。我們的研究結果已在相關會議上發表，並將
進行相關論述與實作的加強，以期在學術期刊上發表。
1. Jih-Woei Huang and Chih-Ping Chu "Scheduling for Data Localized Block-Cyclic Data
Redistribution" In Proceedings of the 12th Workshop on Compiler Techniques for High
Performance Computing, pp.184-192, 17 March 2006.
10
mapping technique (i.e., the data localization technique)
is applied. The proposed method is founded on the data
localization technique and can more efficiently construct
the required communication schedules than other
optimal scheduling methods.
The rest of this paper is organized as follows: In
Section 2, the data localization technique for the
Block-Cyclic(kr) to Block-Cyclic(r) redistribution is
introduced. In Section 3, our communication scheduling
method and algorithms for the redistribution with data
localization are introduced. In Section 4, we give the
related works of the communication scheduling. In
Section 5, the analysis of scheduling overheads and data
transfer costs using the proposed method and Walker
and Oto’s method is ofered.Finally, a brief conclusion
is drawn in Section 6.
2. The Data Localization Technique
In this section we briefly introduce the data
localization technique applied to a Block-Cyclic(kr) to
Block-Cyclic(r) redistribution over m processors. To
facilitate our illustration, we give the terms used in this
paper. Definitions 1, 4 and 5 also appear in [11]:
Definition 1: Given a Block-Cyclic(kr) to
Block-Cyclic(r) data redistribution, denoted as krr
redistribution for simplification, Block-Cyclic(kr) and
Block-Cyclic(r) are called the source distribution and
the destination distribution.
Definition 2: A physical processor ID (ppid) is the ID
assigned to a physical processor node at program
initialization. The ppid of a processor will not be altered
across the whole program. Let m be the number of the
processors involved in the redistribution, and these m
processors are assigned ppid 0, 1, , and m-1,
respectively.
Definition 3: A logical processor ID (lpid) is the ID
assigned to a physical processor node in data
(re)distribution. Usually, the lpid of a processor is equal
to its ppid in the initial data distribution and may be
changed in the data redistribution. In a redistribution, the
lpid of a processor in the source distribution is called
source logical processor ID (slpid); the lpid of a
processor in the destination distribution is called
destination logical processor ID (dlpid); the processor
with slpid i is denoted as Pi; the processor with dlpid j is
denoted as Pj.
Definition 4: Given a krr redistribution on a
one-dimensional array A[0:N-1] over the same processor
set with m processors. The global complete cycle (gcc)
is the number of the array elements which can be
distributed/redistributed to all processors in the source
/destination distribution exactly once, i.e., gcc=lcm(m
kr,mr)=mkr (the least common multiple of mkr and
mr)=mkr. For simplification, N is assumed to be a
multiple of the gcc in our discussion. Therefore, the
array elements in A[0:N-1] can be divided into N/gcc
slices, and a gcc slice of array A contains mk array
blocks.
Definition 5: Given a krr redistribution on A[0:N-1]
over m processors. The local complete cycle (lcc) is the
number of the data elements distributed/redistributed to
a processor in the source/destination distribution from a
gcc slice, i.e., lcc=gcc/m=kr, and a lcc slice of array A
on a processor contains k array blocks.
Since for a krr redistribution on an array over m
processors, all lcc slices of a processor possess the same
redistribution pattern [17], we will, for simplicity, focus
on the data blocks of the first lcc slice in the following
illustrative examples.
Before illustrating the data localization technique, we
first discuss the amount of the data blocks able to be
remained on-processor in redistribution to facilitate our
later analysis of data transfer cost. Given a krr
redistribution on an array over m processors, we can
assert, according to [25], that for each lcc slice of a
processor, the maximum amount of the un-redistributed
data blocks isk/m, and we have the following for each
lcc slice of a processor:
1. If k<m, there is at most one (k/m=1 in this case)
data block to be un-redistributed in redistribution.
Enabling any data block u, 0uk-1, to be
un-redistributed in redistribution can cause the
maximum amount data block (i.e. one data block) to
be remained on-processor.
2. If k=cm (c is an integer and c1), there are always c
(k/m=c in this case) data blocks to be
un-redistributed in redistribution. That is, enabling
any data block u, 0um-1, to be un-redistributed in
redistribution can always retain the maximum amount
data blocks on-processor.
3. If k>m and kcm, there are at most k/m+1 (k/m=
k/m+1 in this case) data blocks to be
un-redistributed in redistribution. Enabling any data
block u, 0umod(k,m)-1, to be un-redistributed in
redistribution can cause the maximum amount (i.e.,
k/m+1) data blocks to be remained on-processor;
however, enabling any data block u, mod(k,m)-1<u
k-1, to be un-redistributed in redistribution can only
make lesser amount (i.e., k/m) data blocks to be
remained on-processor.
According to [25], in performing a krr
redistribution over m processors with gcd(k,m)=1,
enabling the data block u in each lcc slice of a source
processor Pi, 0im-1, to be localized in redistribution
can be achieved by reordering the processors sequence
using the mapping (reordering) function in (Eq1)
(suppose after the reordering the logical order of the
processor Pi is j):
j=f(i)= mod(ik+u,m) (Eq1),
here, u is a programmer defined value to allow the
programmer to localize the required data blocks.
For example, given a 9rr redistribution on a
one-dimensional array over 5 processors. In this
example, k=9, m=5, and gcd(k,m)=1. If the processor
mapping function (Eq1) is used to reorder the processors
sequence in redistribution with programmer defined
value u being set 2, then the source processors P0, P1,
P2, P3, and P4 (i.e., the processors with slpids 0, 1, 2, 3,
and 4) are respectively mapped to the destination
processors P2, P1, P0, P4, and P3 (i.e., the processors
with dlpids 2, 1, 0, 4, and 3), as shown below in Fig 1.
This mapping enables the data block 2 of all source
12
lcc slice of a processor in each communication step. If
k>m and kcm, the data communications are all-to-all
communications with different message lengths, and
there are k/mor k/mdata blocks to be sent
from/received by each lcc slice of a processor in each
communication step. Besides, an optimal
communication scheduling enables all messages in a
communication step to be respectively sent/received by
distinct source/destination processors and have the same
length.
Given a krr redistribution over m processors with
(Eq1) and (Eq2) being data localization techniques for
the case of gcd(k,m)=1 and the case of gcd(k,m)>1. Let
us consider that during the redistribution each processor
sends/receives its first message to/from a processor
according to these mappings. Then, all messages in the
first communication step are each sent/received by
distinct source/destination processors because both these
mappings are one-to-one mappings. Our communication
scheduling for the given redistribution is based on
imaging that the data elements in each lcc slice of a
destination processor form a ring, and that each
destination processor locates its messages starting from
a local location determined using the above mappings.
Lemma 3-1: Given a krr redistribution over m
processors with gcd(k,m)=1, and suppose (Eq1) is used
for source-to-destination processor mapping in redistri-
bution with a given u. Let the data elements in each lcc
slice of a destination processor be a ring. An optimal
communication scheduling for this redistribution can be
achieved by enabling a destination processor Pj,
0jm-1, to locate the received messages starting from
the data block(ik+u)/m.
Proof: Omitted.

(a)
(b)
Fig.3. The communication scheduling for the redistri-
bution in Fig 1 using the method of Lemma 3-1
Obviously, using the communication scheduling
method of Lemma 3-1 for a krr redistribution with
processor mapping technique (Eq1) applied, the
messages scheduled to be received in the first
communication step are indeed composed of the
localized data elements that need not be really received
but only relocated in redistribution. For instance, if this
scheduling method is applied to the data redistribution in
Fig 1, then for the first communication step each
destination processor is scheduled to receive 2 data
blocks for each lcc slice from a distinct source processor,
e.g., the data blocks 0 and 1 of P2, the data blocks 2 and
3 of P1, the data blocks 4 and 5 of P0, the data blocks 5
and 6 of P4, and the data blocks 7 and 8 of P3 are
respectively scheduled to be received from P0, P1, P2,
P3, and P4 in the first communication step, and these
data blocks are in fact the localized data blocks in
redistribution, as shown in Fig 3. Besides, each
destination processor is scheduled to receive 2 data
block for each lcc slice from a distinct source processor
in the second, the fourth and the last communication
steps, and each destination processor is scheduled to
receive 1 data blocks for each lcc slice from a distinct
source processor in the third communication step. Note
that all messages received in a communication step are
located on their source processors starting from the same
location, e.g., the data blocks 3, 4, 0, and 1 on each
source processor are respectively the starting locations
to send the messages in the second, the third, the fourth
and the last communication steps.
Lemma 3-2: Given a krr redistribution over m
processors with g=gcd(k,m)>1, and suppose (Eq2) is
used for source-to-destination processor mapping in
redistribution with a given u. Let the data elements in
each lcc slice of a destination processor be a ring. An
optimal communication scheduling for this redistri-
bution can be achieved by enabling a destination
processor Pj, 0jm-1, to locate the received messages
starting from the data block(ik+)/m. Here,=u/gg
+mod(mod(ik+u,m)+,g), and is the order defined for
the source processor Pi.
Proof: Omitted.

(a)
(b)
Fig.4. The communication scheduling for the redistri-
bution in Fig 2 using the method of Lemma 3-2
Source:Block-Cyclic(9r)
lcc1
block index
slpid ppid
0 1 2 3 4 5 6 7 8
0 0 0 1 2 3 4 5 6 7 8
1 1 9 10 11 12 13 14 15 16 17
2 2 18 19 20 21 22 23 24 25 26
3 3 27 28 29 30 31 32 33 34 35
4 4 36 37 38 39 40 41 42 43 44
Destination:Block-Cyclic(r)
lcc1
block index
dlpid ppid
0 1 2 3 4 5 6 7 8
2 0 2 7 12 17 22 27 32 37 42
1 1 1 6 11 16 21 26 31 36 41
0 2 0 5 10 15 20 25 30 35 40
4 3 4 9 14 19 24 29 34 39 44
3 4 3 8 13 18 23 28 33 38 43
Source:Block-Cyclic(9r)
lcc1
block index
slpid ppid
0 1 2 3 4 5 6 7 8
0 0 0 1 2 3 4 5 6 7 8
1 1 9 10 11 12 13 14 15 16 17
2 2 18 19 20 21 22 23 24 25 26
3 3 27 28 29 30 31 32 33 34 35
4 4 36 37 38 39 40 41 42 43 44
5 5 45 46 47 48 49 50 51 52 53
Destination:Block-Cyclic(r)
lcc1
block index
dlpid ppid
0 1 2 3 4 5 6 7 8
2 0 2 8 14 20 26 32 38 44 50
4 1 4 10 16 22 28 34 40 46 52
1 2 1 7 13 19 25 31 37 43 49
3 3 3 9 15 21 27 33 39 45 51
0 4 0 6 12 18 24 30 36 42 48
5 5 5 11 17 23 29 35 41 47 53
14
block n in the nth communication step. Here,
dn=dfroms(Ps), s=mod(i-(m 10ns sl +u)/k,m), and
n=(bng+on), 1nm; bn=mod(u+m10ns sl ,k)/g,
and on=mod((ik+u)+s,g),s is the order defined for Ps;
l0=0 and ln is the same as above, 1nm.
b. Receive scheduling algorithm
Using (Eq2) for processor mapping, the destination
processor Pj is scheduled to receive lnN/gcc data
blocks from the source processor Psn in the nth
communication step, and the destination processor Pj
will locate the data elements received in the nth
communication step starting from the data block n.
Here, n=mod((ik+)/m+10ns sl ,k) and sn= (j+n
m)/k, 1nm; l0=0 and ln is the same as above, 1nm.
In our given algorithms, each processor only
computes the items needed in its send and receive
communication schedules. Hence, the cost of computing
the schedule in terms of the communication steps using
the proposed method is O(min(k,m)). Additionally, we
only need O(min(k,m))-1 communication steps to
transfer all messages because the messages scheduled in
the first communication step are composed of the
localized data elements.
4. Related Works
Many communication scheduling methods for normal
Block-Cyclic data redistribution (i.e., the Block-Cyclic
redistribution without data localization) have been
proposed (the scheduling methods for rkr
redistribution can be reversely performed on krr
redistribution [10]).In [22], Walker and Otto presented a
communication scheduling scheme to eliminate node
contention for rkr redistribution over the same
processor set. In [23], Desprez et al. extended Walker
and Otto’s work and proposed the communication 
scheduling methods for arbitrary Block-Cyclic
redistribution by formulating the redistribution as
bipartite matching between pairs of processors. In [10],
Park et al. presented the communi- cation scheduling
methods for rkr redistribution over different processor
sets by using generalized circulant matrix formalism. In
[24], Guo et al. solved the communication scheduling of
data redistribution based on a developed LDD notation.
This notation was utilized to determine which pairs of
the source-target processors to communicate in
redistribution.
For an rkr data redistribution over the same
processor set with m processors, [23] described a
method which can only handle the simple case (i.e., the
case of gcd(k,m)=1) in the case of k>m. The generalized
circulant matrix in [10] may not be built to work in the
case of k>m, as all of the entries in a certain row in the
constructed matrix are the same. The scheduling work in
[24] will cause different length messages to be sent in
the same communication step in the k>m case. In
addition, the Caterpillar algorithm described in [16] only
considers the k>m case and will schedule different
length messages to be sent in the same communication
step. In [20], the communication scheduling methods for
a krr redistribution were described. Technically, in the
case of k<m, our scheduling method and theirs are
similar. However, in the case of k>m, their
straightforward scheduling will cause different length
messages to be sent in the same communication step. A
scheduling method described in [13] has a situation
similar to that proposed in [16].
On the whole, Walker and Oto’s (denoted as WO)
scheduling method can be reversely performed to
construct optimal communication schedules for most
cases of the normal krr redistribution over the same
processor set. However, in the case of k>m, their method
may result in larger data transfer cost, and the
scheduling overhead should be much improved.
5. Experimental Results
Since PC cluster is a very popular high performance
computing platform, we have experimented with our
communication scheduling method on some data
redistributions in a PC-cluster environment. Our
PC-cluster includes a master, a PC with one P4 (Pentium
4) 1.8 GHz CPU and 256 MB main memory, and nine
slaves, each a PC with one P4 1.5 GHz CPU and 128
MB main memory, which are connected with Ethernet.
The operation environment was the RedHat Linux 7.1
with the installed parallel software package
MPI-1.2.5.2. We hand-coded the data redistributions in
MPI (Message Passing Interface) with C language.
In this section we analyze scheduling overheads and
the data transfer costs of the redistributions with data
localization technique applied using WO’s method and
ours. Each communication scheduling and each data
redistribution were performed 20 times, and the mean
scheduling time and the mean data transfer time of the
20 tests were used to be its scheduling overhead and its
data transfer cost. The data redistributions were
executed in nonblocking mode [26]. Generally, in
distributed memory model, the data transfer cost for
transferring a message includes the cost for message
start-up and the cost for message transmission. The
start-up cost is incurred once for each communication
event and is independent of the communicated message
size. The unit transmission cost is the cost of
transferring a message of unit length over the network
and the cost for transmitting a message is proportional to
the length of the message [10].
A. Scheduling overhead
m=64
4 8 16 32 64 128 256 512 1024
WO’s 5 7 9 15 26 49 95 186 369
Our 4 5 6 8 15 15 15 15 15
Table 1. The overheads in scheduling the
communications for the krr redistribution over 64
processors with various k’susing WO’s method and ours
In a krr data redistribution over m processors with
data localization technique applied, the costs for
constructing communication schedule in terms of the
communication steps using WO’s method and ours are 
O(k) and O(min(k,m)). In the case of k<m, our
kOverhead
Method
(us)
16
implementation of the redistributions with data
localization, we took u to be 2 and 7, respectively, and
the orderdefined for a PC node with slpid i wasi/3.
As shown above in Table 4, the data transfer costs for
u=2 are less than those for u=7.
6. Conclusion
Array redistribution is usually required for more
efficiently executing a data-parallel program on the
distributed memory multi-computers. In performing
array redistribution using synchronous communication
mode, an efficient optimal communication scheduling is
important for improving the overall system performance.
On the other hand, the flexible processor mapping
technique enables the programmers to localize the
required data elements in redistribution for satisfying
computation need. In this paper, we present an efficient
optimal communication scheduling method for the
Block-Cyclic(kr) to Block-Cyclic(r) redistribution over
the same processor set in which the flexible processor
mapping technique is applied. Our method is founded on
the mapping feature of this technique, and it can more
efficiently construct the required communication
schedules for most cases of such redistribution than
other optimal scheduling methods.
References
27. E. T. Kalns and L. M. Ni. Processor Mapping
Techniques Toward Efficient Data Redistribution.
IEEE Transactions on Parallel and Distributed
Systems, Vol. 6, No. 12, pp. 1234-1247, December
1995.
28. David Bau, Induprakas Kodukula, Vladimir Kotlyar,
Keshav Pingali, and Paul Stodghill. Solving
alignment using elementary linear algebra. In:
Conference Record of the 7th Workshop on
Languages and Compilers for Parallel Computing,
pp. 4660, August 1994.
29. J. Ramanujam and P. Sadayappan. Compile-Time
Techniques for Data Distribution in Distributed
Memory Machines. IEEE Transactions on Parallel
and Distributed Systems, Vol. 2, No. 4, pp. 472482,
October 1991.
30. Michel Dion and Yves Robert. Mapping Affine
Loop Nests: New Results. Parallel Computing, Vol.
22, No. 10, pp. 1373-1397, December 1996.
31. A. W. Lam and M. S. Lam. Maximizing Parallelism
and Minimizing Synchronization with Affine
Partitions. Parallel Computing, Vol. 24, No. 3-4, pp.
445-475, May 1998.
32. W.-L. Chang, J.-W. Huang, and C.-P. Chu. Using
Elementary Linear Algebra to Solve Data
Alignment for Arrays with Linear or Quadratic
References. IEEE Transactions on Parallel and
Distributed Systems, Vol. 15, No. 1, pp. 28-39,
January 2004.
33. S. Hiranandani, K. Kennedy, J. Mellor-Crummey
and A. Sethi. Compilation Techniques for
Block-Cyclic Distributions. ACM International
Conference on Supercomputing, pp. 392-403, July
1994.
34. S. Chatterjee, J. R. Gilbert, F. J. E. Long, R.
Schreiber, and S.-H. Teng. Generating Local
Address and Communication Sets for Data Parallel
Programs. Journal of Parallel and Distributed
Computing, Vol. 26, pp. 72-84, 1995.
35. S. K. S. Gupta, S. D. Kaushik, C.-H. Huang, and P.
Sadayappan. On Compiling Array Expressions for
Efficient Execution on Distributed-Memory
Machines. Journal of Parallel and Distributed
Computing, Vol. 32, pp. 155-172, 1996.
36. N. Park, V. K. Prasanna, and C. S. Raghavendra.
Efficient Algorithms for Block-Cyclic Array
Redistribution Between Processor Sets. IEEE
Transactions on Parallel and Distributed Systems,
Vol. 10, No. 12, pp. 1217-1240, December 1999.
37. C.-H. Hsu and Y.-H. Chung. Efficient Methods for
kr r and r kr Array Redistribution. The Journal
of Supercomputing, Vol. 12, pp. 253-276, 1998.
38. S. Ramaswamy and P. Banerjee. Automatic
Generation of Efficient Array Redistribution
Routines for Distributed Memory Multicomputers.
In Frontiers '95: The Fifth Symposium on the
Frontiers of Massively Parallel Computation, pp.
342-349, February 1995.
39. S. Ramaswamy, B. Simons, and P. Banerjee.
Optimization for Efficient Array Redistribution on
Distributed Memory Multicomputers. Journal of
Parallel and Distributed Computing, Vol. 38, pp.
217-228, 1996
40. R. Thakur, A. Choudhary, and G. Fox. Runtime
Array Redistribution in HPF Programs. In
Proceedings of Scalable High Performance
Computing Conference, pp. 309-316, May 1994.
41. R. Thakur, A. Choudhary, and J. Ramanujam.
Efficient Algorithm for Array Redistribution, IEEE
Transactions on Parallel and Distributed Systems,
Vol. 7, No. 6, pp. 587-594, June 1996
42. L. Prylli and B. Tourancheau. Fast Runtime Block
Cyclic Data Redistribution on Multiprocessors.
Journal of Parallel and Distributed Computing, Vol.
45, pp. 63-72, 1997.
43. C.-H. Hsu, S.-W. Bai, Y.-C. Chung, C.-S. Yang. A
Generalized Basic-Cycle Calculation Method for
Efficient Array Redistribution. IEEE Transactions
on Parallel and Distributed Systems, Vol. 11, No. 12,
pp. 1201-1216, December 2000.
44. A. Wakatani and M. Wolfe. Optimization of Array
Redistribution for Distributed Memory
Multicomputers. Parallel Computing, Vol. 21, No. 9,
pp. 1485-1490, 1995.
45. S. D. Kaushik, C.-H. Huang, R. W. Johnson, and P.
Sadayappan. An Approach to
Communication-Efficient Data Redistribution. In
Proceedings of International Conference on
Supercomputing, pp. 364-373, July 1994.
46. S. D. Kaushik, C.-H. Huang, J. Ramanujam, and P.
Sadayappan. Multi-Phase Array Redistribution:
Modeling and Evaluation. In Proceedings of
International Parallel Processing Symposium, pp.
441-445, April 1995.
47. C.-H. Hsu, Y.-C. Chung, D.-L.Yang, C.-R. Dow. A
Generalized Processor Mapping Technique for
Array Redistribution. IEEE Transactions on Parallel
and Distributed Systems, Vol. 12, No. 7, pp.
743-757, July 2001.
48. D. W. Walker and S. W. Otto. Redistribution of
1出席國際學術會議心得報告
計畫編號 NSC 94-2213-E-006-070
計畫名稱 資料區域化的陣列資料動態重分配技術之研究
出國人員姓名
服務機關及職稱
朱治平
國立成功大學資訊工程系教授
會議時間地點
(1). 95 年 1 月 16 日至 1 月 19 日 (2). 95 年 8 月 30 日至 9 月 1 日
日本仙台 中國北京
會議名稱
(1). The International Conference on Information Networking 2006 (ICOIN 2006)
(2). 2006 International Conference on Innovative Computing, Information and Control (ICICIC
2006)
發表論文題目
(1). Service Discovery in Mobile Ad Hoc Networks
(2). Design of a Scalable Computer Cluster for MPEG-4 Parallel Encoder
一、 參加會議經過
(1) 2006 資訊網路國際會議(ICOIN 2006)於元月十六日在日本仙台舉辦，此會議是由日本資訊處理學會
與韓國資訊科學學會舉辦，是每年舉辦一次之國際研討會。本年度會議討論之主題包括隨意與感應器
(sensor)網路、高速網路、有線與無線網路整合、應用與服務、網路安全、下一代網際網路、無線多媒體
系統、P2P 網路等約 13 個議程，大會共發表 115 篇論文，來自中華民國之專家學者約有七位。
此次會議本人及本人學生合著有一篇屬正規論文被接收；題目為「Service Discovery in
Mobile Ad Hoc Networks」。此次會議過程與多位在美國、日本、大陸、香港及韓國等地之學者或專家做
經驗交流，特別是在分享在隨意網路、P2P 網路之研究心得上，收穫頗豐。
(2) 2006 年創新計算、資訊與控制國際會議，於八月三十日至九月一日在中國北京交通大學舉行，此國
際會議今年是第一屆，由北京交通大學與 ICIC International 共同主辦，國際電機電子工程學會(IEEE)
贊助，是每年舉辦一次之國際研討會。本會議討論之主題有二: (一)創新計算與資訊，(二)智慧型系統與
控制。內容包括: 能量系統管理與控制、資訊網路與應用、流程監督與控制、進化計算、計算機網路、智
慧資料分析與應用、影像處理、內嵌式計算、型的識別與分類、資訊隱藏與安全等約 53 個議程，大會共
發表 403 篇投稿接受論文(論文接受率約僅 33%)及 125 篇邀稿論文，另有 3 場 Keynote 演講，領域涵蓋範
圍廣泛。會議一共有來自世界各國約 500 人參加，其中來自中華民國之專家學者約有 30 位。
此次會議論文發表全部均以口頭報告方式呈現，本人有一篇論文發表；題目為「Design of a
Scalable Computer Cluster for MPEG-4 Parallel Encoder」，被安排於九月一日下午發表。此次會議
3Service Discovery in Mobile Ad Hoc Networks*
Hua-Wen Tsai1, Tzung-Shi Chen2, Chih-Ping Chu1
1Department of Computer Science and Information Engineering
National Cheng Kung University, Tainan 701, Taiwan
Tel: 886-6-2757575 ext. 62527, Fax: 886-6-2747076
chucp@csie.ncku.edu.tw
2Department of Information and Learning Technology
National University of Tainan, Tainan 700, Taiwan
chents@mail.nutn.edu.tw
Abstract. Discovery of service is expected to be a crucial feature for the usability of mobile ad hoc networks
(MANETs). A number of existing service discovery protocols is mainly aimed at infrastructure-based and/or 1-
hop ad hoc wireless networks. In this paper, we propose a novel mechanism using hierarchical grid architectures
for service discovery in MANETs. The geographic area of a MANET is partitioned into 2D logically hierarchical
grid. In each grid, we select a node as directory to cache the available service descriptions for replying the inquiry
of requestors. To enhance registration and discovery efficiency, we propose a transmission pattern for service
registration and discovery. The description of shared service is registered into the backbone grids. When a seeker
needs a specific service, it obtains the descriptions from the directory in backbone grids. Finally, the performance
shows that our addressed protocol can work efficiently in different scales of networks with any densities.
1 Introduction
Recently, the work has focused on service discovery from an application’s point of view.Service/location discovery
[5][6] is one of applications in MANET to achieve stand-alone and self-configurable communication networks. The
wild range of applications is used in providing host’s location, printing documents, processing database queries, 
lending of computational power, delivering documents, using technical equipment [5], and so forth. A service
discovery protocol lets service providers to advertise their services, and allows users to discover such services
automatically. Another application of service discovery is of location discovery for offering the node’s geographical
position information.
The major structural difference between existing service discovery architectures is the utilization or not of a central
directory. Sun’s Jini [1] and UDDI [7] rely on a central directory that stores information about available services in
the network. The central directory is responsible for managing the registration of service and replying the service
information to the seeker. It can reduce the cost of search efficiently. But it is more suited for wireless infrastructure-
based networks. Decentralized resource discovery protocols appear better suited to MANETs, such as UPnP [8] and
SLP [3]. When a client wants to access a service or a provider wants to register a service, it broadcasts a query
packet or registration packet to the network. This method is known as pull-based and push-based discovery. But the
broadcast strategy incurs more traffic overhead.
Recently, an approach using geographic information is proposed to register service descriptions. Geography-based
Content Location Protocol (GCLP) [6] uses physical location information to reduce proactive traffic. When a
provider wants to register a service, it designates four farthest neighbors as the forwarding nodes that are located at
its east, west, south and north respectively. And then the registration packet is propagated along four geographic
trajectories. When a node receives the registration packet, it must cache the advertisements. If it is the designated
forwarding node at the trajectory, it continues to forward the packet along the trajectory. When a client seeks a
service, it sends a discovery packet along the geographic trajectories that are similar to registration process. The
packet eventually is intersected the advertisement route. This method efficiently reduces the cost of discovery and
registration and is suitable for scalability network. However, there are two major drawbacks in this solution. First, it
is suitable for the high density MANETs. When the density of whole or part network is sparse, this method can not
guarantee to implement very well. Another drawback is that it leads a large number of nodes to cache service
53 Service Discovery Protocol
In this section, we describe our proposed protocol SGrid with the hierarchical grid architecture, directory selection,
service registration, and service discovery phase.
3.1 Hierarchical Grid Architecture
In this subsection, we discuss a hierarchical grid architecture used for service registration and discovery. Two terms,
grid level (L) and zone size (Z), are used to form hierarchical grid architecture. The level of grid is from 0 to L. A
grid in level 0 is also named zone and the side length of zone is Z grids. The network is in NN grids where N is set
to 12)1(  LZ . Fig. 1 illustrates an example with different zone sizes, where the range of Z is from 1 to 2 and the
value of L is set to 2. This hierarchical grid architecture can easily adapt to the different network sizes. The size of
MANET can be well-known and predefined, because MANET, in usual, is used in a specific area or purpose.
Assume that each node knows the values of L, Z and the origin of the grid coordinates. When a node receives a
physical location of its neighbors or itself, it can obtain the grid coordinate via a predefined mapping. When a node
knows the coordinate of grid, it can compute the level of grid by itself. Algorithm 1 is shown in Fig. 2 and each node
utilizes this algorithm to obtain the grid level. For example, we have L = 2 and Z = 2. When node n knows that it is in
grid(6, 4), it has by Algorithm 1 Gn.xlv = 2, Gn.ylv = 0 and Gn.lv = 2. That is, the level of grid(6, 4) is 2.
Fig. 1. An example of
different zone size (Z) of
hierarchical grid architecture. Fig. 2. Algorithm 1 - Computing node n’s grid level.
3.2 Directory Selection Phase
In each grid, we select a head as directory to record the descriptions of shared service within the grid. Each node
needs to know the information about its neighbors. In order to keep the latest information about its neighbors, each
node periodically broadcasts Hello message to its neighbors. The Hello message includes the node location and its
state. The format of Hello message is Hello(id, x, y, state, <NSL>), which id is unique ID of broadcasting node; x, y
are xy-coordinate; state is node state that are two states: Head (H) and normal (N); NSL indicates a node state list
with eight node states in its neighbor grids. When a node receives a Hello message from its neighbor, it records the
information of Hello message into its neighbor table.
The eight node states in NSL are got from the neighbor table and they are arranged in NSL according to the positions
(directions) of grid as <northwest, north, northeast, west, east, southwest, south, and southeast>. If there are more
than two nodes in a neighbor grid, we select the state of head as the top priority. If no node is in a neighbor grid, the
node state is set as NULL. An example shown in Fig. 3(a), node A broadcast Hello(A, x, y, H, NSL<NULL, H, N, H,
N, NULL, H, N>) to its neighbors. We use this information to detect whether a gap grid exists and to connect with
neighbor grids. Gap grid means that there is no node in a grid. So we can not forward any packets to the gap grid.
When a head do communication with another head in its neighbor grid but their distance is greater than 1 hop, the
communication between them must pass a relay node (gateway). The information of the gateway is obtained from
7Fig. 5. A provider n at region A or C of grid level 1 transmits the service descriptions toward horizontal direction
(Regn.td = 0). If the provider is at region B or D of grid level 1, it transmits the service descriptions toward vertical
direction (Regn.td = 1). When a head receives a registration packet and it is the designated node but its grid level is
not 0, it caches the descriptions in its service table and continue to forward this packet. If a non-directory or non-
designated node receives this registration packet, it just drops out the packet. The transmission direction of
registration packet is kept constant at the same level. When the packet is transmitted from grid level lv to grid level
lv+1, the head at level lv+1 varies the transmission direction. This process is repeated until the service is registered
to the maximal level. In Fig. 5(a), grid(1, 7) is at region A of grid level 1, so the first transmission direction of grid(1, 7)
is the horizontal direction. When a designated head in grid level 1 (e.g. grid(2, 7)) receives the service registration
packet, it varies the transmission direction from the horizontal to vertical direction. When the registration packet is
transmitted to grid(2, 4) that is grid level 2, the transmission direction is varied from vertical to horizontal direction.
The grid at the registration pattern is named backbone grid that caches service descriptions for discovery.
As mentioned above, we use the different registration patterns for each region at the same level on the Grid. This is
because we distribute the registration to different backbone nodes for reducing the overhead for each head as well as
for dispersing the registration loads to the different backbone. Based on these merits, we can reduce the registration
overheads and provide the high service discovery chances as more as possible.
The first registration direction of other level grid except grid level 0 is along the direction of same level. For example,
a provider is in grid(2, 1) where the first transmission direction is the vertical direction. When a node in grid(2, 4)
receives the registration packet, it varies the transmission direction from the vertical to horizontal direction. When
the situation is grid(x, y).xlv equal to grid(x, y).ylv (e.g. grid(4, 4) or grid(2, 6)), the transmission direction of provider can
be selected the horizontal or vertical direction. We define the transmission direction as vertical direction in this paper,
when a provider is in a grid that grid(x, y).xlv equal to grid(x, y).ylv.
When a directory receives a registration packet that has same service description by other provider, it first checks
whether it has the same service description in its service table or not. If the directory has the same service description,
it caches the service description in its service table but it does not forward the packet to its neighbors. It is because
that the same service description transmission will incur the redundant overhead. In other words, when a directory in
backbone has the same service description and receives the same service description from other provider, it does not
need to transmit the same service description to its neighbors.
Fig. 6. An example of service discovery. Fig. 7. An example of improved SGrid.
3.4 Service Discovery Phase
When a requestor can not find out a special service in its service table, it has to do discover from MANETs. We also
name this kind of requestor as seeker that issues a discovery packet to query the directories along discovery pattern.
The seeker designates the heads in its neighbor grids to receive and convey the discovery packet along the
transmission pattern. When a head receives this query, it checks whether it has the special service description in its
service table or not. If yes, it replies the information that includes the provider’s address and position to the seeker. If
no, it checks whether it is a designated node. If no, it just drops the packet. If yes, it forwards the discovery packet
along the discovery pattern to the backbone. Here assume that each node knows the size of network; so, it can
compute and know the position of the highest level grid. When the discovery packet is forwarded to the backbone
grids at the highest level grid, it will stop to forward this packet. The range of discovery is confined to a region of the
highest grid level. An example of discovery is shown in Fig. 6. There are two providers and two seekers in this
network.
9SGrid can reduce the consumption of resources effectively. The performance of discovery overhead is illustrated in
Fig. 8(b). SGrid needs a little more discovery overheads than that for GCLP. This is because that SGrid is in using
fewer directories to cache the service descriptions. When a requestor can not find out the cached service descriptions
in its service table, it must issue discovery packets to seek the directories. This kind of requestor is named seeker.
The seeker/requestor ratio of SGrid is higher than GLCP’s. This is because that SGrid utilizes the fewer directories
at the higher density network. E.g. in 500 nodes and 10 m/s, GCLP and SGrid respectively have 51.81% and 69.40%
requestors that need to seek the services.
(a) Number of Service Directories
0
50
100
150
200
250
100 200 300 400 500
Nodes
#
of
D
ire
ct
or
ie
s
GCLP-10
SGrid-10
GCLP-50
SGrid-50
(b) Search Overhead
0
20000
40000
60000
80000
100000
120000
100 200 300 400 500
Nodes
#
of
Pa
ck
et
s
GCLP-10
SGrid-10
GCLP-50
SGrid-50
(c) Success Ratio
0
0.2
0.4
0.6
0.8
1
100 200 300 400 500
Nodes
Su
cc
es
s
R
at
io
GCLP-10
SGrid-10
GCLP-50
SGrid-50
(d) Avg. Delay Time
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
100 200 300 400 500
Nodes
T
im
e
(s
ec
)
GCLP-10
SGrid-10
GCLP-50
SGrid-50
Fig. 8. The performance of simulation.
The performance of success ratio is shown in Fig. 8(c). The success ratio represents the ratio of requestor that can
discover the desirous of service. The success ratio of SGrid is higher than GCLP’s, e.g. in 400 nodes and 10 m/s, the
success ratio of GCLP and SGrid are 72.42% and 88.67% respectively. This cause is that the hollow regions exist in
this network though the network connects each other. If the geographic trajectory in GCLP meets a hollow region, it
will break down and the transmission unable to continue along the predefined direction. SGisd can overcome this
shortcoming in the low node density, because the alternate grid is used to prevent the hollow region problem. And
then we compare the performance of delay time in Fig. 8(d). The delay time is shown the transmission time for
discovery. The average delay time of GCLP and our algorithm’s are similar.
From the experimental results, we can observe that SGrid has better efficiency than GCLP in any density network. In
the middle or low density network, the efficiency of GCLP is not good. This is because that the hollow regions have
existed in network though the networks have connection. The alternate grid in SGrid is used for solving the hollow
region problem. For reducing the overhead for each head and dispersing the registration loads to each backbone, we
use the different registration patterns for different regions. That is, we distribute the registration to different
backbone nodes. In addition, the grid structure can reduce the number of directories that is used to cache the service
descriptions. Although the service descriptions are registered into the backbone grids that are in the centerline of
network, the same service description will not be registered to the centerline grids repeatedly. This way will not
make the centerline grids increase too many burdens. Moreover, SGrid can reduce the number of directories but
increase the success ratio of service discovery.
5 Conclusions
We proposed a protocol for service discovery in MANETs. The objective was to increase the performance but
reduce the service directories for the registration. Our algorithm was designed based on a hierarchical grid structure.
When a provider wants to register its service into MANETs, it transmits the registration packet along a predefined
transmission pattern to the backbone grids. The discovery process is confined in a quarter of network area, so we can
11
Design of a Scalable Computer Cluster for MPEG-4 Parallel Encoder
Po-Kai Chiu, Wen-Yu Su, Chih-Ping Chu
Department of Computer Science and Information Engineering,
National Cheng Kung University, Tainan, Taiwan
chucp@csie.ncku.edu.tw
Abstract
This paper describes a architecture of homogeneous
computer cluster for the parallel encoder of the natural
video section of the MPEG-4 specification. The
parallel encoder on this architecture can lower the
negative impact on the achieved speed-up under high
resolution input video data that has large
communication overhead. Experimental results show
that the proposed architecture is featured with
scalability and has higher CPU utilization and
speedup than general architecture of homogeneous
computer cluster.
1. Introduction
In 1999, the MPEG-4 specifications for the
encoding of audio and video sequences were completed
[3][4][11]. A system that is much more complex and
requires much more computational power for the
encoding than the former MPEG specifications is
defined [10]. This work led to the need for efficient
tools and mechanisms that help the implementation of
systems based on the new specification.
A computer cluster [2][8] is a type of parallel and
distributed processing system, which consists of a
collection of interconnected stand-alone computers
working together as a single, integrated computing
resource. A computer node can be a single or
multiprocessor system (PCs, workstations, or SMPs)
with memory, I/O facilities, and an operating system. A
computer cluster generally refers to two or more
computers (nodes) connected together. The nodes can
exist in a single cabinet or be physical separated and
connected via a LAN. An interconnected (LAN-based)
cluster of computers can appear as a single system to
users and applications. Such a system can provide a
cost-effective way to gain features and benefits (fast
and reliable services) that have historically been found
only on more expensive proprietary shared memory
systems. In this paper, we describe a computer cluster
with 11 personal computers as the nodes connected by
Fast-Ethernet, running Linux operating system.
Miguel Ribeiro et al. [6] described a parallel
encoder which exploits data parallelism, the pseudo-
GOV partitioning, exhibited by coding framework and
uses QCIF and CIF as its input video data for the
natural video section of the MPEG-4 specification to
speed up the encoding process. It can work because
there has no any data dependence between I-VOPs in
the process of encoding (Figure 1).
Figure 1. VOP encoding modes.
Figure 2 shows the data dependencies for each VOP
type and the data distribution for the pseudo-GOV
partitioning data distribution. In this batch contains a
single pseudo-GOV, but the model is valid for any
integer number of pseudo-GOVs.
Figure 2. A VOP sequence and its distribution.
There still exist other data parallelisms described in
[6], to which the scalability, load balancing, and
performance aren’t good enough. In this paper, we
adopt data parallelism, the pseudo-GOV partitioning
described in [6], to implement a parallel encoder and
propose a cluster architecture with scalability which
has high CPU utilization and speedup obtained in the
experimental results.
This paper is organized as follows. In section 2, the
proposed cluster architecture is presented. Section 3
13
b
b
n N
D
G  (1) and
b
s
t N
F
N  (the nth slave’s
  tw NnN  1 )
In order to ensure that the CPU of each slave in
each sub-system has no idle time （ Nw≒0 ） , the
number of slaves of each sub-system should be:
t
n
tntwt
N
E
S
NSNnNNE


(2)
Using Equation (1) and Equation (2), the cluster
which has high CPU utilization can be constructed.
Table 1. Parameters required for designing
cluster architecture.
Parameter
Nt Network transfer time for receiving of each slave（sec）
Nw Network wait time for receiving of each slave（sec）
Nb Network bandwidth（Mb/sec）
E Average encoding time of each frame（sec）
Fs VOP size（KB）
Db Effective disk bandwidth（MB/sec）of the Master
Gn Number of sub-systems
Sn Number of slaves of each sub-system
Pn Number of processors（Pn＝Sn × Gn＋1）
3. Implementation and experimental
results
In this experiment, two different cluster
architectures were used. One is general（Figure 5）
and the other is scalable（Figure 6）. The computer
characteristics of the two architectures are the same,
shows as Table 2. The connecting network was based
on Fast-Ethernet.
Table 2. Cluster characteristics.
Type Processor Memory Disk
Maste
r
P4 1.8
GHz
256MB
ATA 33
（33MB/s）
Slave
P4 1.5
GHz
128MB
ATA 33
（33MB/s）
Several standard MPEG-4 testing sequences were
used. Table 3 shows the results obtained with the
original serial implementation of the encoder, in a
single 1.5GHz computer, like the slave.
In order to construct the scalable cluster
architecture we proposed, according to Table 3, the
values of some parameters for constructing the scalable
cluster architecture are obtained. Number of sub-
systems can be obtained:
Mb/s100
MB/s33


b
b
N
D
nG
 64.2
100
833 (3)
The number of slave of each sub-system can also be
obtained（For example, 640×480）：
21.0
0439.0
102480
8450
KB450
Mb/s80






E
N
F
N
t
s
b
nS 78.40439.0
21.0
(4)
Table 3. Input sequences characteristics and
serial encoding time.
Resolutio
n
Numbe
r of
VOPs
Sequenc
e
length(s)
Serial
encodin
g time
Encode
d VOPs
per
second
VOPs
Size(KB
)
640×480 960 32 198.315 4.84 450
960×528 300 10 96.243 3.12 742.5
1024×768 900 30 486.85 1.85 1152
According to Eqs. (3) and (4), we can get the
maximum values of Gn=2 and Sn=4 (Pn=9) for the
cluster with the characteristics shown in Table 2 when
the resolution of the input video data is 640 × 480.
Consequently, the parallel cluster architecture is
constructed with two sub-systems, 4 slaves each, and 1
master. This architecture will have better speedup and
performance than general cluster architecture with 8
slaves and 1 master such as the on shown in Figure 5,
because Nw≒0 during encoding process.
We use a MPI（Message Passing Interface）[7] [9]
compliant library （ MPICH ） [5] to handle the
communications （ not data transfer ） between the
different processors. In order to enhance the
performance, we adopt multi-thread [1] architecture to
implement. We separate encoding and data transferring
into two threads on each slave. And we adopt client-
server architecture to transfer video data by socket
programming because the master should transfer video
data concurrently for several sub-systems. Figure 7
shows the detailed flow when encoding process runs on
the parallel cluster architecture.
Figure 8 shows the detailed flow when encoding
process runs on the general cluster architecture. The
master send the input video data in sequence to each
slave (not sub-system)
Figure 9 shows the evolution of the speedup for the
parallel encoding of the different resolution sequences
and Figure 10 shows the average efficiency of cluster
during encoding
Although the scalable cluster architecture make the
efficiency of parallel encoder achieve to 70%~80%,
this is not good enough. If the master can run the
encoding process, the efficiency can achieve to
80%~90%. In order to make it, we should make the
CPU of the master run the encoding process instead of
