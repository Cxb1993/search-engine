 2 
I. INTRODUCTION 
It is recently seen that the sensor resolution and the number of sensor bands are significantly enhanced 
for satellite images. With IKONOS, QuickBird, GeoEye-1 and WorldView-2, optical images with a ground 
sampling distance (GSD) of 0.5m are available. In addition, for WorldView-2, the number of spectral bands 
has been increased to eight. For remote sensing applications, both high spatial and spectral resolutions are 
often simultaneously required. It means that both ground and spectrum details must be accurately presented in 
a single image. To meet this goal, image fusion methods are frequently used to enhance the spatial resolution 
of the multispectral (MS) images with the aid of higher spatial resolution panchromatic (Pan) ones. The two 
most important quality aspects of pan-sharpened images are the spatial resolution enhancement and the 
spectral feature preservation [1-8]. A successful image fusion algorithm must not distort the spectral content 
of a multispectral image during increasing its spatial resolution. 
More recently, Padwick et al. presented a new pan-sharpening algorithm called Hyperspherical Color 
Sharpening (HCS) [9]. This fusion method, specifically developed for WorldView-2 data, is capable of 
transforming the high-quality multispectral image to increase the spatial resolution. Meanwhile, the 
commercial software ERDAS IMAGINE 2011 has included the HCS technique. Instead of employing a 
physical model, the HCS algorithm is reported to statistically model the data which involves the exact spectral 
bands of the sensor in question. Thus, the spectral overlap of the bands may not be a strong determining factor 
in the quality of the pan-sharpened image. In this work, however, we will indicate that the naïve mode of HCS 
can be viewed as a Brovey transform (BT) method [7] except a different definition of the intensity component, 
and that the smart mode of HCS is equivalent to the smoothing-filter-based intensity modulation (SFIM) 
technique [10]. In fact, it will be shown that the quality of the pan-sharpened image is still highly associated 
with the spectral overlap of the sensor bands. We design an adjustable IHS-BT-SFIM image fusion algorithm 
which possesses five different modules for the IKONOS, QuickBird, GeoEye-1, and WorldView-2 imagery. 
These five modules are called IHS, IHS-BT, BT, BT-SFIM, and SFIM, respectively. Furthermore, an 
analytical framework is proposed to design new image fusion methods. According to the spectral responses of 
WorldView-2 sensors, we propose five different synthetic intensity images to explore the problem of color 
distortion. To validate the proposed technique, experiments are conducted for evaluating real WorldView-2 
imagery covering different areas. 
 4 
2 2
, 1 2 2 2
sin cosHCS n
P R G G PG P I G
I I IR G
θ θ += = ⋅ ⋅ ⋅ = ⋅
+
      (5-2) 
2 2
, 2 2HCS n
R G R PR P R
I IR G
+
= = ⋅
+
               (5-3) 
and 
, 1cosHCS s
L L L
P P B PB I I B
P P I P
θ= = = ⋅         (6-1) 
2 2
, 1 2 2 2
sin cosHCS s
L L L
P P R G G PG I I G
P P I PR G
θ θ += = = ⋅
+
   (6-2) 
2 2
, 2 2HCS s
L L
P R G R PR I R
P I PR G
+
= = ⋅
+
       (6-3) 
These two equations can be further rewritten as follows 
,
,
,
HCS n
HCS n
HCS n
R R
PG G
I
B B
   
   = ⋅   
     
  and  
,
,
,
HCS s
HCS s
L
HCS s
R R
PG G
P
B B
   
   = ⋅   
     
    (7) 
Equation (7) states that the fused image can be easily obtained from the resize of the 
original [ ], , TR G B by simple multiplication. Accordingly, the HCS method can be efficiently implemented in 
this manner. It is noted that the naïve mode of HCS is an n-band version of the Brovey transform (BT) [7], 
while the smart mode is related to the smoothing-filter-based intensity modulation (SFIM) technique [10]. The 
BT and SFIM can be respectively expressed as 
BT
BT
BT
R R
PG G
I
B B
   
   = ⋅   
      
 with  [ ] / 3I R G B= + +      (8) 
and 
 
SFIM
SFIM
L
SFIM
R R
PG G
P
B B
   
   = ⋅   
      
          (9) 
Without loss generality, we introdece a BT-like form as follows 
 6 
the BT-like image fusion. To further investigate the visually perceived color properties, the linear RGB-IHS 
model is explored. The conversion formula can be defined by [11] 
1
2
1/ 3 1/ 3 1/ 3
2 / 6 2 / 6 2 2 / 6
1/ 2 1/ 2 0
I R
v G
v B
    
    = − − =    
    −    
 and 1
2
1 1/ 2 1/ 2 I
1 1/ 2 1/ 2
1 2 0
R
G v
B v
 −       = − −             
 (13) 
where the intensity component is defined by ( )    / 3I R G B= + + . For the chromatic information, H and S 
are respectively defined by the internal variables v1 and v2, and represented by 
1 2
1
tan vH
v
−  =  
 
 and 
2 2
1 2S v v= +      (14) 
Equation (13) can be further partitioned as 
1
1
2
2
1 1/ 2 1/ 2 1/ 2 1/ 2
1 1/ 2 1/ 2 1/ 2 1/ 2
1 2 0 2 0
R I I
v
G v I
v
B v I
   − −               = − − = + − −                                   
(15)
 
Thus, (10) can be rewritten as 
1
2
1/ 2 1/ 2
1/ 2 1/ 2
2 0
ii
i
i i i
i
i i
IR R
v
G G I
v
B B I
γ
γ
γ
 −′′        ⋅     ′ ′= ⋅ = + − −        ⋅    ′    ′        
      (16) 
From (14), it can be obtained that the hue value of the fused image is the same as its corresponding one of the 
original RGB image, but the intensity and saturation values will be altered. It is showed as follows 
[ ]1 1/ 3
PI R G B I I P
I
γ′ ′ ′ ′= + + = ⋅ = ⋅ =
        
(17-1) 
[ ]2 2/ 3
L L
P II R G B I I P
P P
γ′ ′ ′ ′= + + = ⋅ = ⋅ = ⋅
       
(17-2) 
1 2
1
tan ii
i
vH H
v
γ
γ
−  ⋅′ = = ⋅ 
           (17-3) 
2 2
1 2i i iS v v Sγ γ′ = ⋅ + = ⋅            (17-4) 
with a generalized form 
1 2( )i i
L
S I S SP Pand
S I S I S P
′ ′ ′ ′
= = =
  
 (17-5) 
 8 
hyperspectral image fusion. In summary, the BT, HCSn, and IHS methods preserve the complete spatial 
information but suffer more spectral information loss. In the SFIM, HCSs, and WT methods, there is a trade-off 
between the spatial and spectral information. Additionally, for SFIM, HCSs, and WT, the success of 
spatial-detail injection mainly relies on the designed low-pass filter. Table I can be readily employed to evaluate 
various image fusion approaches for remote sensing applications. It can be seen that the saturation value is the 
main difference in color distortion between IHS-like and BT-like methods. The S value is compressed in 
IHS-like approaches since I ′  is larger than I. In BT-like approaches, on the contrary, it can be shown that the S 
value is stretched. 
IV. INTENSITY MATCHING PROBLEM 
To reduce color distortion, a common strategy is using the intensity matching technique to match the 
spectrum of MS to that of Pan. According to the operation of HCSs (or SFIM) in (6) and Table I, the intensity 
component I is simply an internal variable in the fusion. The color distortion is dominantly determined by 
whether the value 
L
P
P  is close to one. This means that the LP  decides not only the spatial details of the fused 
image but also the degree of color distortion. Hence, the pre-processing to match both P and LP to I may not be a 
must for the SFIM-like approaches. 
For the IHS, BT, and HCSn, on the other hand, the degree of color distortion depends on the similarity of P 
and I. Therefore, a matching technique to modify the MS intensity to fit the Pan image is required. For the 
relative spectral responses of the QuickBird satellite, fig. 2(a) shows that the MS coverage of the blue and green 
bands is partially matched to that of the Pan. Obviously, the color distortion in the above-mentioned fusion 
approaches results from this spectral mismatch. Furthermore, vegetation is likely to appear relatively high 
reflectance in NIR and Pan bands, and low reflectance in RGB bands. Accordingly, the digital number (DN) 
values in I are much smaller than those in P since the effect of the NIR band is not included in the determination 
of I for the vegetation areas. This leads to significant color distortion on green vegetation regions in the fused 
image. To reduce the color distortion during the fusion process, the response of NIR band must be included into 
the calculation of I [3, 6]. Although the theoretical spatial resolution of the fused images is not equal to that of 
the Pan image, satisfactory resolution can be achieved in practice. 
A generalized approach for R-G-B-NIR multispectral imagery can be represented by [3, 6, 12] 
 10 
correlation coefficients (CCs) between Pan and these five intensity images are listed in Table II. It can be seen 
that the I6_avg is more similar to the Pan than the I8_dist. The reason is that the spectral characteristic of I6_avg is 
more similar to that of Pan than I8_dist since I8_dist includes both NIR1 and NIR2 while both Pan and I6_avg include 
only NIR1. The I8_dist is the most different from the Pan, which can be obviously seen in vegetated areas where 
the reflectance of infrared radiation is the strongest. For IKONOS/QuickBird imagery [7], it is found that the 
I4_adj image can be designed to be the most similar to Pan if the calculated spectral characteristic can be exactly 
overlapped with that of the Pan. Theoretically, I6_adj is possible to be the most similar to the Pan. Table II, 
however, shows that the I6_adj is not as expected. The reason is that the NIR1 band 
is not fully covered by the 
Pan spectrum. In the following experiment, we will investigate the real WorldView-2 data to validate the 
findings. 
V. AN ADJUSTABLE IHS-BT-SFIM APPROACH 
It is known that SFIM (HCSs) fusion is among the easily approaches in practice to control the tradeoff 
between the spatial and spectral information. However, the SFIM method is not efficient for the fusion of 
high-resolution images. In addition, it preserves more spectral information but suffer more spatial information 
loss. Its effectiveness heavily depends on the filter design. On the contrary, the IHS, BT and HCSn methods can 
be capable of quickly merging the massive volume of data and simultaneously preserving most of the spatial 
information. However, the problem of color distortion must be solved during the application of the IHS and BT 
approaches to the fusion of high-resolution images. 
It is found that the IHS technique generates significant saturation compression, and that the BT method 
suffers from saturation stretch. Thus, the color distortion over vegetated areas in the IHS fusion is contrary to 
that in the BT fusion. Motivated by this idea, an adjustable IHS-BT approach has been proposed in [7]. SFIM 
 12 
BT
BT
BT
R R R
PG G G
I
B B B
′′     
     ′′ = = ⋅     
′′            
with PI I P
I
′′ = ⋅ =  and 
S P I P
S I I I
′′
= ⋅ =  
(iv) if k1= k2=1 , and ˆ LP P=  
( )
( )
( )
BT SFIM L
BT SFIM L
L
BT SFIM L
R R R P I
PG G G P I
P
B B B P I
−
−
−
′′ + −     
     ′′ = = ⋅ + −     
′′     + −       
with 
( )
( )
L
L
I P II P P
I P I
+ −′′ = ⋅ =
+ −  and 0.5 0.5L
S P I
S P P I
′′
= ⋅
⋅ + ⋅  
(v) if k1= 1, k2=0 , and ˆ LP P=  
SFIM
SFIM
L
SFIM
R R R
PG G G
P
B B B
′′     
     ′′ = = ⋅     
′′            
with 
L L
P II I P
P P
′′ = ⋅ = ⋅  and 
L L
S P I P
S P I P
′′
= ⋅ =  
 
Two most important modules of the propose approaches are IHS-BT and BT-SFIM. The IHS-BT approach 
which combines IHS with BT to balance various saturation changes can reduce the saturation distortion 
problems. For the BT-SFIM module, however, the most attractive feature is that the theoretical spatial 
resolution of the fused images is equal to that of the Pan image. This means that we can use a low-pass filter 
with a lower order. For example, a simple 3x3 mask low-pass mean filter is sufficient to generate an adequate 
value for PL. This will greatly reduce the dependence on the filter design. The color distortion can be effectively 
mitigated if a proper intensity in (25) is chosen to replace I. 
In our experiment, the IHS-BT-SFIM approach is implemented into a Windows-based program consisting 
of two sub-windows which are called the Scroll window and the Main Image window, respectively. The input 
RGB image is entirely displayed in the Scroll window with subsampled resolution. A box shown in the Scroll 
window indicates the area that is displayed in the Main Image window with full resolution. The program 
initially reads and fuses only the images within the Main Image window. After k1 and k2 are determined, the 
 14 
The fused images of I8_dist, I6_dist, I8_avg, I6_avg, and I6_adj are sequentially displayed from the third row of 
Figs. 5 and 6. For these images, the first, second and third columns are fused by IHS, IHS-BT, and BT 
approaches, respectively. Compared to the original RGB image in Figs. 5(b) and 6(b), it is found that the color 
distortion in the fused images generated by IHS, BT, and IHS-BT has been efficiently mitigated since the NIR 
information has been introduced into I. Visual inspection can reveal that the IHS approach provides more 
spatial details, while the BT preserves better spectral information. It is noted that the IHS-BT approach can 
effectively balance the information changes between IHS and BT by an adjustable parameter. 
As shown in Table III, the fused images produced by I8_dist and I6_dist (shown in Figs. 5(g)-5(l) and 6(g)-6(l)) 
have the worst spatial details, especially in Fig. 5(g) produced by IHS with I8_dist. Compared to the spatial 
details of the Pan image in Figs. 5(a) and 6(a), the spatial details of the fused images generated by IHS-I8_dist 
and IHS- I6_dist are clearly strengthened. It is especially obvious in the area of vegetation, as shown in Figs. 3(b) 
and 3(c). The reason is that I8_dist includes both NIR1 and NIR2, and I6_dist includes NIR1 while Pan only 
partially includes NIR1. However, this phenomenon is not found in the images generated by I8_avg and I6_avg, as 
shown in Figs. 5(m)-5(r) and 6(m)-6(r). This is due to the fact that I8_dist and I6_dist are the real distance images 
while I8_avg and I6_avg are the mean images. The DN values of the I8_dist and I6_dist are much larger than I8_avg and 
I6_avg even if a linear stretching technique is used to rescale I8_dist and I6_dist. Thus, to use I8_dist and I6_dist to match 
Pan often improperly sharpens the intensity fluctuation on vegetation areas in the fused image. Therefore, it is 
not recommended to use I8_dist and I6_dist since more calculation is also required.  
Generally speaking, the critical issue of image fusion focuses on how to produce a fused imagery of high 
quality. For classification-oriented and visualization-oriented fusions, however, the required techniques are 
quite different. For visualization, the fused imagery should offer the highest possible spatial information content 
and still preserve good spectral information quality. In this respect, IHS-BT with I6_adj or I6_avg is thus 
recommended. For the I6_adj, its fused image has the best spatial details, but suffers compressed saturation 
which can be seen in Fig. 5(s). By visually inspecting Figs. 5 and 6, and comparing the data in Table III, it is 
found that IHS-BT with I6_avg or I8_avg is the relatively best choice for classification-oriented fusion because the 
spectral information and spatial details are best balanced. In our early experiments [6], however, the I4_adj image 
outperforms the I4_avg in IKONOS/QuickBird imagery with regard to spectral information preserving. The main 
reason can be attributed to that the NIR1 band does not fully covered by the Pan spectrum of the Worldview-2 
 16 
2005. 
[8] K. A. Kalpoma and J.-i. Kudoh, "Image Fusion Processing for IKONOS 1-m Color Imagery," IEEE 
Trans. Geosci. Remote Sens., vol. 45, no. 10, pp. 3075–3086, Oct. 2007. 
[9] C. Padwick, M. Deskevich, F. Pacifici and S. Smallwood, “WorldView-2 pan-sharpening,” ASPRS 2010 
Annual Conf., San Diego, California, Apr. 26–30, 2010. 
[10] J. G. Liu, "Smoothing filter-based intensity modulation: a spectral preserve image fusion technique for 
improving spatial details," Int. Journal of Remote Sens., vol. 21, no. 18, pp. 3461–3472, 2000. 
[11] R. S. Ledley, M. Buas, and T. J. Golab, "Fundamentals of true-color image processing," IEEE 
Proceedings, 10th Int. Conf., Pattern Recognition, vol. 1, pp. 791–795, 1990. 
[12] M. González-Audícana, X. Otazu, O. Fors, and J. Alvarez-Mozos, "A low computational-cost method to 
fuse IKONOS images using the spectral response function of its sensors," IEEE Trans. Geosci. Remote 
Sens., vol. 44, no. 6, pp. 1683–1691, June 2006. 
 
 18 
Table I 
    Algorithms 
Results 
BT-like in the linear model IHS-like in the nonlinear model 
BT/HCSn SFIM/HCSs IHS WT 
I ′  P 
L
IP P⋅  
P ( )LP I P+ −  
H ′  H H H H 
S
S
′
 
I
I
′
 
I
I ′  
P
I
 L
P
P  
I
P
 ( )L
I
I P P+ −  
 
Table II 
CCs. Pan I8_dist I6_dist I8_avg I6_avg I6_adj 
Pan 1.0000 0.9224 0.9298 0.9302 0.9304 0.9245 
 
Table III 
Cc R G B Pan 
IHS-I8_dist 0.9435 0.9266 0.9254 0.8886 
IHS-I6_ dist 0.9449 0.9301 0.9279 0.9067 
IHS-I8_ avg 0.9507 0.9371 0.9347 0.9383 
IHS-I6_ avg 0.9470 0.9331 0.9304 0.9496 
IHS-I6_adj 0.9416 0.9262 0.9233 0.9670 
IHS-BT-I8_dist 0.9428 0.9318 0.9287 0.8976 
IHS-BT-I6_ dist 0.9443 0.9349 0.9313 0.9108 
IHS-BT-I8_ avg 0.9496 0.9408 0.9372 0.9362 
IHS-BT-I6_ avg 0.9465 0.9375 0.9340 0.9444 
IHS-BT-I6_ adj 0.9425 0.9318 0.9291 0.9608 
BT-I8_dist 0.9389 0.9345 0.9274 0.9018 
BT-I6_ dist 0.9422 0.9384 0.9314 0.9119 
 20 
 
 
(a) 
 
(b) 
Figure 2 
 
 22 
 
(a) 
 
(b) 
Figure 4 
 24 
   
(m)       (n)        (o ) 
   
(p)       (q)        (r ) 
   
(s)       (t)        (u ) 
         
Figure 5 
 
 
 
 
 26 
   
(m)       (n)        (o ) 
   
(p)       (q)        (r ) 
   
(s)       (t)        (u ) 
 
Figure 6 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/08/12
國科會補助計畫
計畫名稱: 建立新一代衛星影像GeoEye-1/RapidEye/WorldViewII分析植被,土壤及水的
相關技術
計畫主持人: 杜德銘
計畫編號: 99-2221-E-233-001- 學門領域: 訊號處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
此次研究亦同時指正現用於 EDARS2011 版(萊卡公司所出的全球業界使用量最
大之遙測軟體)專為WorldView-2所推出之影像融合演算法的嚴重錯誤(由DG公
司 兩 位 首 席 科 學 家 及 兩 位 資 深 程 式 設 計 師 所 研 發 ) ， DG 公 司
(QuicikBrd,WorldView-2母公司)原為全世界第二大衛星公司,目前挾其優勢已
逐漸有取代 GeoEYE 公司(IKONOS,GeoEye-1 母公司)，這些影像我國軍方均有接
收，過去我們即藉由兩家公司彼此之間的矛盾與影像的缺陷，成功爭取減價的
空間以減少公帑. 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
