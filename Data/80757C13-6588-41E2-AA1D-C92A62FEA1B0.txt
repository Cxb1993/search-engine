2.1 Grey Model 
 
The following steps briefly describe GM(1,1|α) [1] 
modeling. 
 nkjxkx
k
j
,...,2,1,)()(
1
)0()1( ==∑
=
 (1) 
)()0( kx : sampled nonnegative data sequence 
 nkbkazkx ,...,3,2,)()( )1()0( ==+  (2) 
 10),1()1()()( )1()1()1( ≤≤−−+= ααα kxkxkz  (3) 
)()1( kz : the background value 
 
(1)(0)
(0) (1)
(0) (1)
(0) (1)
(2) 1(2)
(3) (3) 1
ˆ, ,(4) 1(4)
1( ) ( )
N
zx
x z
a
Y B ax z b
x n z n
⎡ ⎤−⎡ ⎤ ⎢ ⎥⎢ ⎥ −⎢ ⎥⎢ ⎥ ⎡ ⎤⎢ ⎥⎢ ⎥= = =− ⎢ ⎥⎢ ⎥⎢ ⎥ ⎣ ⎦⎢ ⎥⎢ ⎥ ⎢ ⎥⎢ ⎥ −⎢ ⎥⎣ ⎦ ⎣ ⎦
 (4) 
 ˆNY Ba=  (5) 
 bkxa
dk
kxd =+ )(ˆ)(ˆ )1(
)1(
 (6) 
 
,...3,2),)()1((
)1(ˆ)(ˆ)(ˆ
)2()1()0(
)1()1()0(
=−−=
−−=
−−−− kee
a
bx
kxkxkx
kaka
 (7) 
 
2.2 Cumulated 3-point Least Square Prediction 
Model 
 
A cumulated 3-point least squared prediction 
(C3LSP) [4] is constructed as follows: 
 3,2,1,)()( )0()1( =+⋅= koffsetksscaleks  (8) 
 )()0( ks : given data. 
scale : an user-defined scaling factor. 
offset : an user-defined shift factor or offset. 
 3,2,1,)()(
1
)1()2( ==∑
=
kjsks
k
j
 (9) 
)()1( js : representing three successive sampled data 
denoted as )3()1( −ks , )2()1( −ks , and )1()1( −ks  before 
predicting the single step-ahead output )(ˆ )1( ks . 
 01)2( )( ckcks += , 3,2,1=k  (10) 
This turns out to be a normal equation, 
 KCS = , (11) 
and its approximate optimal parameter estimation is  
 SKKKC TT 1)( −= . (12) 
Tsss )]3(),2(,)1([ )2()2()2(=S ,
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
=
13
12
11
K , and Tcc ],[ 01=C . 
 01)2( )1()1(ˆ ckcks ++=+ , 3=k  (13) 
 )()1(ˆ)1(ˆ )2()2()1( ksksks −+=+ , 3=k  (14) 
 ))1(ˆ(1)1(ˆ )1()0( offsetks
scale
ks −+=+  (15) 
 
2.3 BPNN Tuning Grey and C3LSP 
 
A well-known intelligent computing machine, 
three-layer back-propagation neural net (BPNN) [6] as 
shown in Fig. 1, is used in this hybrid Grey-C3LSP  
prediction [4] for tuning the appropriate weights for 
the forecasts from Grey and C3LSP models. By 
crossing examination among the performance criteria 
of the forecast outputs in the experimental results 
section, the prior validation (prediction) has showed 
that the forecasts are acceptable from the accuracy 
point of view. For this three-layer BPNN, a structure 
of 5×14×2 multilayer-perceptron is used that the input 
layer has 5 input neurons to catch the input patterns, 
the hidden layer has 14 neurons to propagate the 
intermediate signals, and the output layer has 2 
neurons to display the computed results (weights). We 
arrange the input pattern in the following: a 
single-step-ahead predicted output from grey model 
)1(ˆ +kygrey , a single-step-ahead predicted output from 
C3LSP model )1(ˆ 3 +ky lspc , and three most recent 
differential values of the true observations denoted by 
)(kyΔ , )1( −Δ ky , and )2( −Δ ky . Two appropriate weights 
wgrey and wc3lsp, applied to hybrid Grey-C3LSP 
prediction, are designed as the outputs. For more 
training assignments in this three-layer BPNN, the 
log-sigmoid transfer function is applied as the 
activations in the hidden layer, the pure-line transfer 
function is employed to the output layer as the 
activations, and Bayesian regulation involved 
Levenberg-Marquardt training method is set as the 
learning algorithm for this three-layer BPNN. 
)1(ˆ +kygrey
)1(ˆ 3 +ky lspc
)(kyΔ
)1( −Δ ky
)2( −Δ ky
 
Fig. 1. A typical three-layer BPNN architecture and 
Sigmoid transfer function as the output of activations 
in the hidden layer. 
 
3. ARMAX/NGARCH Composite Model 
 
ARMAX/NGARCH composite model allows you 
to deal with the presence of conditional 
heteroscedasticity for time series prediction, especially 
in financial time series applications like asset return 
problem.  
The ARMAX [11] encompass autoregressive (AR), 
moving-average (MA), and regression (X) models, in 
any combinations as expressed below. 
 ∑ ∑∑
= ==
+−++−+=
m
j
N
k
kj
r
i
i
x
ktjtMtityRCty
1 11
),()()()()( Xβεε ,(16) 
where 1K = a constant coefficient, iR = autoregressive 
coefficients, jM = moving average coefficients, )(te = 
residuals, )(ty = responses, kβ = regression 
coefficients, X = an explanatory regression matrix in 
which each column is a time series, and ),( ktX = a 
element at the t th row and k th column. 
 )()(
*
1
i0 xw φαα i
l
i
i −=∑
=
, (24) 
and an optimal bias 
 ⎟⎟
⎟
⎠
⎞
⎜⎜
⎜
⎝
⎛
−= ∑
=
l
i
T
l
1
))((1 0ii0 wxyb φ . (25) 
A fast algorithm applied to constraint optimization 
for support vector regression (SVR) is called adaptive 
support vector regression (ASVR) [19]. It is designed 
for exploring three free parameters C, ε  and rbkfσ  
[18] such that the computation time of quadratic 
programming (QP) is significantly reduced and 
achieved rapid convergence to the near-optimal 
solution. In the ASVR algorithm, two scale factors, v  
and υ  refer to [19], are evaluated in advance and 
then applied these evaluated values for calculating the 
free parameters ε  in Eq. (26) and rbkfσ  in Eq. (29), 
respectively, where the vector X  stands for an input 
vector. An order n  ,see [19], is also predetermined 
and used in computing the free parameter C in Eq. (27) 
and Eq. (28). In this manner, a straightforward 
parameter-seeking is done rather than using a heuristic 
method due to a long time for searching. Note that Eq. 
(27) and Eq. (28) are based on the modified Bessel 
function of second kind with the order n  [20] as 
follows: 
 2
)min()max( XX −×= vε  (26) 
 
{ }
{ }∑
∑
∞
=
+
−
=
−
+
+Φ+Φ+
−+
−−−+
+−==
0
2
1
0
2
1
)()(
)!(!
)2/(
2
)1(
)2/()!1()1(
2
1
)()2/ln()1()(
k
knn
n
k
nkk
n
n
n
knk
knk
kn
IKC
ε
ε
εγεε
 (27) 
 ∑∞
=
+
++Γ=
0
2
)1(!
)2/()(
k
kn
n knk
I εε  (28) 
where ...5772156.0=γ  is Euler’s constant and 
0)0(,1...
3
1
2
11)( =Φ++++=Φ
p
p  [20]. 
Eq. (29) determines a free parameter rbkfσ  of radial 
basis kernel function for quadratic programming in 
SVR. 
 l
x
x
l
xx
υσ
l
j
j
l
i
i
rbkf
∑∑
== =
⎟⎟
⎟⎟
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜⎜
⎜⎜
⎜
⎝
⎛
−
−
⋅= 1
2/1
1
2
,
1
)(
 (29) 
In short, adaptive support vector regression firstly 
finds three tunable free parameters C, ε , and rbkfσ  
and subsequently utilizes these parameters in SVR 
optimization to search for the optimal weights and 
bias mentioned above. 
 
5.2 Neuromorphic Quantum-Based ASVR 
(NQASVR) 
 
The synaptic weights ijklw  are given in a Hopfield 
network [21]. 
 ∑∑∑ −−=
ij
ijij
ij kl
klijijklHN ohoowE 2
1 , (30) 
where ijh  is the external bias for a neuron. The 
synaptic weights are obtained as 
 
 
)1(2)1(2
)1(2)1(2
,,,,
,,,,
kilkjikilkji
ljkikiljijkl
dc
baw
δδδδ
δδδδ
−−−−
−−−−=
−−++
, (31) 
where ijδ  is the Kronecker delta. Let us consider that 
each qubit corresponds to each neuron of a Hopfield 
network. The state vector ψ  of the whole system is 
given by the product of all qubit states. 
The time evolution of the system is given by the 
following chr¨odinger equation. 
 )()()1()1(
)(
tetUt
tiH
ψψψ h−==+ , (32) 
Here, the operator )1(U  is given by the Pad´e 
approximation [22]. 
The quantum computation algorithm utilizing 
adiabatic Hamiltonian evolution has been proposed by 
Farhi et al. [23]. Adiabatic Hamiltonian evolution is 
given as 
 FI HT
tH
T
ttH +⎟⎠
⎞⎜⎝
⎛ −= 1)( , (33) 
where IH  and FH  are the initial and final 
Hamiltonians, respectively. We assume that the 
quantum system starts at t = 0 in the ground state of 
IH , so that all possible candidates are set in the initial 
state )0(ψ . T  denotes the period in which the 
Hamiltonian evolves and the quantum state changes, 
and we can control the speed of such changes to be 
suitable for finding the optimal solution among all 
candidates set in )0(ψ . If a sufficiently large T  is 
chosen, the evolution becomes adiabatic. The 
adiabatic theorem says that the quantum state will 
remain close to each ground state [24]. Therefore, the 
optimal solution can be found as the final state )(Tψ . 
However, successful operation is not guaranteed in the 
case that there exists any degeneracy in energy levels 
or any energy crossing during the evolution [24]. The 
initial Hamiltonian IH  is chosen so that its ground 
state is given by the superposition of all states as 
 ∑−
=
=
12
02
1)0(
n
i
n
iψ , (34) 
where n  is the number of qubits and i  is the n-th 
eigenvector. The initial Hamiltonian IH  is given as 
 
)
(
)12()1()0( 16
x
xx
xxxI
II
IIII
H
σ
σσ
σσσ
⊗⋅⋅⋅⊗⊗+⋅⋅⋅+
⊗⋅⋅⋅⊗⊗+⊗⋅⋅⋅⊗⊗=
⎟⎠
⎞⎜⎝
⎛ +⋅⋅⋅++= −
, (35) 
where xσ  is the x-component of the Pauli spin matrix. 
One can choose any other IH  which satisfies that its 
ground state is expressed by a linear combination of 
all states. For example, xσ  can be replaced by yσ . It 
may be possible to solve optimization problems by 
composing a new Hamiltonian FH  considering the 
 
1)()(..
)(ˆ)()(ˆ)()(ˆ
=+
⋅+⋅=
kwkwts
kkwkykwky
ngarchbwgc
ngarchngarchbwgcbwgcbwgcngarch σδδ
(42) 
where the weights, )(kwbwgc  and )(kwngarch , are 
determined by applying neuromorphic quantum-based 
ASVR as shown in Fig. 3. A predicted result is 
determined by adding a predicted variation at next 
period into the current true value as expressed below. 
 )(ˆ)1()(ˆ kykyky bwgcngarchbwgcnagrch δ+−=  (43) 
A digital cost-function (DCF) [28] defined as 
 ∑
=
−=
l
k
bwgcngarch kykyDCF
1
2
)(ˆ)( , (44) 
which can be used for measuring the accuracy for 
each trial of quantum computing. 
 
 
Fig. 3. A diagram depicts the composite model, 
BWGC/NGARCH, regularized by neuromorphic 
quantum-based adaptive support vector regression 
(NQASVR). 
 
6.2 Adaptation by NQASVR for Seeking Weights 
 
A promising quantum computing for the process of 
optimization search in an unsorted weight-space is 
used to look for the appropriate weights in Eq. (42) to 
minimize DCF on Eq. (44) so that the predicted results 
on non-periodic short-term time series forecast can get 
best accuracy. Neuromorphic quantum-based ASVR 
mentioned above is employed in this hybrid prediction 
for tuning the appropriate weights, )(kwbwgc  and 
)(kwngarch , for the forecast )(ˆ kybwgcδ  and )(ˆ kngarchσ  as 
per Eq. (42), respectively. In short, to use quantum 
computing for the optimization search in weight-space 
first requires one to express hybrid model in a digital 
representation including both the weights and the 
cost-function computation those which are coded as 
8-bit values varied in the range [-100,100]. The set of 
digital weights to be optimized are stored in as many 
associated qbits, that are prepared in an initial, 
equiprobable superposition. Secondly, feeding the 
initial state to the cost-function supports an exhaustive 
scanning of the weight-space. In order to highlight the 
specific advantages of the quantum approach, a 
comparative method, real-coded genetic algorithm 
(RGA) [29], applied to the random-search process is 
also served in the optimization on Eq. (42). Generally, 
genetic algorithm as a search engine represents the 
useful tool for NP-complete problems in the lack of 
effective optimization techniques. The comparison, 
involving quantum-based minimization algorithm and 
real-value genetic algorithm, is fulfilled by the 
following two experiments in the next section. 
 
三、 結果與討論 
7. Experimental Results and Discussions 
 
As shown in Fig. 4 to Fig. 7 or Fig. 8 to Fig. 9, the 
predicted sequences indicate the predicted results for 
the following competing methods: grey model (GM), 
auto-regressive moving-average (ARMA), radial basis 
function neural network (RBFNN), adaptive 
neuro-fuzzy inference system (ANFIS), support vector 
regression (SVR), real-coded genetic algorithm tuning 
BWGC/NGARCH model (RGA-BWGC/NGARCH), 
and neuromorphic quantum-based adaptive support 
vector regression tuning BWGC/NGARCH 
model(NQASVR-BWGC/NGARCH). In the 
experiments, the most recent four actual values is 
considered as a set of input data used for modeling to 
predict the next desired output. As the next desired 
value is obtained, the first value in the current input 
data set is discarded and joins the latest desired 
(observed) value to form a new input data set for the 
use of next prediction. The international stock price 
indexes prediction for four markets (New York Dow 
Jones Industrials Index, London Financial Time Index 
(FTSE-100), Tokyo Nikkei Index, and Hong Kong 
Hang Seng Index) [30] have been experimented as 
shown in Fig. 4 to Fig. 7. The accuracy of predicted 
result, including GM, ARMA, RBFNN, ANFIS, SVR, 
RGA-ASVR/NGARCH, and the proposed one, is also 
compared and the summary of first experiment is 
listed in Table 1. Criterion of mean square error for 
measuring the predictive accuracy is expressed below. 
 
( )
l
yy
MSE
l
t
tttt cc∑
=
++ −
= 1
2ˆ
 (45) 
where l  = the number of periods in forecasting, ct  
= the current period, ttcy +  = a desired value at the 
ttc + th period and ttcy +ˆ  = a predicted value at the 
ttc + th period. 
The goodness of fit on the first experiment is 
tested by Q-test successfully due to all of p-values 
greater than level of significance (0.05) [31]. London 
International Financial Futures and Options Exchange 
(LIFFE) [32] provides the indices of volumes of 
equity products on futures and options, and further 
their indices forecast are shown in Fig. 8 and Fig. 9. 
Table 2 has listed the summary of forecasting 
accuracy for the comparison between methods. This is 
also tested by Q-test successfully due to all of 
p-values greater than level of significance (0.05). 
 
四、 計畫成果自評 
8. Conclusions 
 
BWGC cannot avoid volatility clustering effect 
and thus worst its generalization capability. NGARCH 
is utilized for dealing with the problem of volatility 
clustering or fat-tail effect to best fit the modeling. 
Therefore, NGARCH provides nonlinear time-varying 
conditional variance of residual to combine BWGC 
for dealing with volatility clustering phenomenon 
where neuromorphic quantum-based adaptive support 
[30]  International Federation of Stock Exchanges, 
FIBV FOCUS MONTHLY STATISTICS, 2005.
 Available: 
http://www.fibv.com/WFE/home.Asp?nav=ie 
[31] G. M. Ljung, and G. E. P. Box, “On a Measure of 
Lack of Fit in Time Series Models,” Biometrika, 
Vol. 65, pp. 67-72, 1978. 
[32] London International Financial Futures and 
Options Exchange (LIFFE), 2002. Available: 
http://www.liffe.com/. 
[33] Bao Rong Chang, Shiou Fen Tsai, and Shi-Huang 
Chen, “Neuromorphic Quantum-Based Adaptive 
Support Vector Regression for Tuning 
BWGC/NGARCH Forecast Model,” Proc. of 
11th Conference on Artificial Intelligence and 
Applications (TAAI2006), Special Section 3, Oral 
Track, National Kaohsiung University of Applied 
Science, Dec. 15-16, 2006. (Accepted) 
[34] Bao Rong Chang, “Novel Prediction Approach - 
Quantum-Minimum Adaptation to ANFIS 
Outputs and Nonlinear Generalized 
Autoregressive Conditional Heteroscedasticity,” 
Lecture Notes in Artificial Intelligence, Vol. 4223, 
pp. 908-918, Sept. 2006. (SCI, EI) (NSC 
94-2218-E-143-001) 
[35] Bao Rong Chang, “Applying Nonlinear 
Generalized Autoregressive Conditional 
Heteroscedasticity to Compensate ANFIS 
Outputs Tuned by Adaptive Support Vector 
Regression,” Fuzzy Sets and Systems, Vol. 157, 
No. 13, pp.1832-1850, July 2006. (SCI, EI) (NSC 
93-2218-E-143-001) 
[36] Bao Rong Chang, “A Hybrid System 
ASVR/NGARCH Tuned by Quantum-Based 
Minimization to Improve Forecasting Accuracy,” 
Journal of Advanced Computational Intelligence 
& Intelligent Informatics, Vol. 10, No. 4, 2006. 
(In Press) (Special Issue on Selected Papers from 
TAAI2005) (EI) (NSC 93-2218-E-143-001) 
[37] Bao Rong Chang, Shi-Huang Chen, and Hsiu Fen 
Tsai, “Forecasting the Flow of Data Packets for 
Website Traffic Analysis - ASVR-tuned 
ANFIS/NGARCH Approach,” Lecture Notes in 
Computer Science, Vol. 4233, pp. 925-933, Oct. 
2006. (SCI, EI) (NSC 93-2218-E-143-001) 
[38] Hsiu Fen Tsai and Bao Rong Chang, 
“Intelligence-Based Model to Timing Problem of 
Resources Exploration in the Behavior of Firm,” 
Lecture Notes in Computer Science, Vol. 4234, 
pp. 478-487, Oct. 2006. (SCI, EI) (NSC 
93-2218-E-143-001) 
[39] Shiou Fen Tsai and Bao Rong Chang, “Adaptive 
Support Vector Regression Tuning Composite 
Model of BWCG and NGARCH for Applications 
of Time-Series Prediction,” Journal of Grey 
System (Published by Chinese Grey System 
Association, Taiwan), Vol. 9, No. 1, pp. 1-7, June 
2006. (NSC 92-2213-E-143-001) 
 
 
 
 
 
 
 
Table 1. 
The comparison between different prediction models 
based on Mean Squared Error (MSE) on international 
stock price monthly indices for 48 months from Jan. 
2002 to Dec. 2005. (unit=105) 
Methods
NY-D.J. 
Industrials
Index 
London 
FTSE-
100 
Index 
Tokyo 
Nikkei 
Index 
HK 
Hang 
Seng 
Index 
Averag
e MSE
GM 1.9582 0.4006 3.2209 5.1384 2.6795
ARMA 1.8230 0.3883 2.9384 4.3407 2.3726
RBFNN 1.8319 0.3062 3.8234 3.1976 2.2898
ANFIS 1.4267 0.3974 3.5435 4.0279 2.3489
SVR 1.2744 0.3206 2.6498 3.1275 1.8431
RGA- 
BWGCNG 1.2642 0.2630 2.0175 3.1271 1.6680
NQASVR-
BWGCNG 1.1694 0.2168 1.8671 3.0362 1.5724
Note: method abbreviation 
1. GM: Grey Model 
2. ARMA: Autoregressive Moving-Average 
3. RBFNN: Radial Basis Function Neural Network 
4. ANFIS: Adaptive Neuro-Fuzzy Inference System 
5. SVR: Support Vector Regression 
6. RGA-BWGCNG: Real-Code GA tuning BWGC/NGARCH 
7. NQASVR-BWGCNG: NQASVR-BWGC/NGARCH 
 
Table 2. 
The comparison between different prediction models 
based on Mean Squared Error (MSE) on futures and 
options volumes monthly indices of equity products 
for 24 months from Jan. 2001 to Dec. 2002. 
Methods
Futures Index 
of 
Equity 
Products 
Options Index 
of 
Equity 
Products 
Average 
MSE 
GM 0.0945 0.0138 0.0542 
ARMA 0.0547 0.0114 0.0331 
RBFNN 0.0196 0.0087 0.0142 
ANFIS 0.0112 0.0092 0.0102 
SVR 0.0191 0.0085 0.0138 
RGA- 
BWGCNG 0.0110 0.0082 0.0096 
NQASVR-
BWGCNG 0.0101 0.0070 0.0086 
Note: the same abbreviation as Table 1. 
0 5 10 15 20 25 30 35 40 45
7000
8000
9000
10000
11000
12000
sample number
fu
nc
tio
n 
va
lu
e
djindex
desired sequence
predicted sequence by grey
predicted sequence by arma
predicted sequence by nqasvr-bwgc/ngarch
 
Fig. 4. Forecasts of N. Y. -D. J. Industrilas monthly 
index for 48 months from Jan. 2002 to Dec. 2005. 
