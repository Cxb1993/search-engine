行政院國家科學委員會補助專題研究計畫 
□期中進度報告 
期末報告 
 
投影攝影系統互動之研究 
Interactive with a Projector-camera System 
 
計畫類別：個別型計畫   □整合型計畫 
計畫編號：NSC100-2221-E-126 -014- 
執行期間：   100年 8 月 1  日至 101  年 7 月 31  日 
 
執行機構及系所：靜宜大學資訊傳播工程學系 
計畫主持人： 吳賦哲 
共同主持人： 
計畫參與人員：廖婉婷、張嘉宸 
 
 
 
 
本計畫除繳交成果報告外，另含下列出國報告，共 ___ 份： 
□移地研究心得報告 
□出席國際學術會議心得報告 
□國際合作研究計畫國外研究報告 
 
 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
            □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
 
中   華   民   國   101年  10  月 30  日
2 
 
前言 
Art is a creative process. Can it possibly be supported by a computational tool? Flagg et 
al. [Flagg et al. 2009] argue that capture and access technology can provide a key form of 
computational support for the creative process. They have designed an interactive system 
for guiding artists to paint and sculpt using traditional materials and tools.  
 
In their system, a camera is used to capture the current process and a projector is used to 
guide the user how to progress. With their inspiration, we construct such a system to 
evaluate how useful the guidance of this system is for a novice sculptor. It is a 
challenging for a beginner to sculpt from scratch. In the general process, as a guideline, a 
sculptor should use a pen to draw a shape on the target. To produce a realistic statue from 
a different viewpoint, the ratio scale and the position should stay as correct as possible. 
However, this is not easy. The purpose of our system is to fill the gap. Through the 
projector, the system renders the guiding information on the surface to let the user know 
where to carve. 
 
研究目的 
 
To implement such a system, the key technology will be 3D scanning and projector-based 
rendering. Both technologies rely on the well calibration process. Nowadays, the 
technology of 3D scanning is more developed, both in terms of accuracy and affordability. 
A projector and a camera are enough to construct a 3D scanner system. The price will be 
about $2000 dollars or less. The projector based rendering also received significant notice 
recently. It is not only suitable for large areas of display, but also for rendering on 
arbitrary surfaces. Thus, such a combination is ideal for constructing a Procams for 
sculpture assistance. 
 
The system is constructed to help a novice sculptor learn how to sculpt. When compared 
with Flagg’s system, we find two issues are very important for using such a system. 
 
 User needs stable and consistent information during the entire process. 
 User needs coarse to fine information in different stages. 
 
The system is as shown in Figure 1. Initially, the users used clay to sculpt a statue. For a 
novice user, it is not easy to do this job well. The system with a projector will render a 
difference in color for the user working on the model. The blue color indicates that there 
is too little clay in this position. The user should add more clay at this point. Likewise, 
the red color indicates that there is too much clay in this position. The user should 
remove extra clay at this point.
 
 
 
4 
 
calibration method [Okatani and Deguchi 2009], which assumes that the internal 
parameters of the projector, except the focal length, are known. Then, a camera captures 
the same pattern image that is projected from four projectors. All the parameters can then 
be calculated. Drareni et al [Drareni et al. ] made a similar approach. The projection 
surface is assumed as a plane, then the point on the projection surface with the pixels of 
the projector can form a homograph relationship. All the parameters can be estimated 
through the different positions of the projectors. Bhasker et al [Bhasker et al. 2007] used 
a low-resolution camera to calibrate a multi-projector projection plane. This method can 
also calibrate the deformation of the projection surface. Similarly, Kimura et al [Kimura 
et al. ] used a calibrated camera and projection plane to calibrate the parameters of a 
projector. 
 
Griesser et al [Griesser and Van Gool ] used a handheld whiteboard in front of the camera. 
By changing its position and angle, different image patterns could be captured. After the 
corresponding relationship among these patterns is recovered, the parameters of the 
projector can be calibrated. 
 
In general, the projection surface is a plane. Thus, the relationship between projection 
surface and the surface of the projector is a linear relationship. It is easy to calculate; 
however, Johnson [Johnson et al. 2006] obtains the optimal solution by using the genetic 
algorithm to solve the spherical projection surface problem. 
 
If the parameters of the projector came out in the estimation, then its position can no 
longer be changed. In order to change the position, its parameters need to be re-estimated. 
To solve this problem, Zhou et al [Zhou et al. 2008] combine the projector and camera 
together. Certain feature points in frames will be discovered. Based on these feature 
points the change of the location of the projector can be estimated. In this manner, the 
purpose of continuous calibration can be achieved. Ishihara et al [Ishihara and Ishihara 
2007] proposed another method in which no feature point is needed to continuously 
calibrate the projector. In this method, by measuring the changes in the projection 
position and intensity, the location changes can be estimated. Dao et al [Dao et al. 2007] 
used a 3D tracker to continuously track the projector in the space of change, which, in 
turn, can make a handheld projector maintain a fixed image in the projection plane. By 
using an angle-changing mirror, Choi et al [Choi et al. 2007] made the system respond to 
the changes of angle with corresponding calibration. 
 
研究方法 
System Configuration 
Figure 2 shows the system configuration. The system consists of a projector and a camera. 
The camera is the Point Grey Chameleon USB camera with a resolution of 1280x960. 
The projector is the NEC M300x which a resolution of 1024x768. To keep a fixed 
relationship between the projector and the camera, the camera is mounted on the 
projector. The input image sequences are captured with the FlyCapture2 SDK. 
6 
 
problem is solved with epipolar 
geometry. 
 
As Figure 3 shows, a pixel of a 
projector is received from a pixel 
in the camera. These locations, 
combined with the projection 
location will form a plane, 
proceeded by x0 = Hx, where H 
is the fundamental matrix, and x0, 
x are the corresponding points 
among two images. 
 
To estimate the camera’s 
parameters, two cameras are 
prepared. The camera’s intrinsic 
and extrinsic parameters are 
estimated by the Matlab camera 
calibration Toolbox. A physical 
checkerboard pattern is provided 
in front of two cameras in 
different positions and poses. 
Figure 4 shows the calibration 
process, namely a physical 
chessboard in front of the two 
cameras.  
 
During the calibration process, 
each step is based on certain 
known information in order to 
discover the unknown. First, we 
know the size of the physical chessboard, and from the corresponding pair, two cameras 
appear. Thus, we can get determine the intrinsic and 
extrinsic parameters of the two cameras. To find the projector’s parameters we project a 
sequence of patterns on the plane. With the already calibrated cameras, we can recovery 
the position and size of the pattern in this plane. Since the pattern is projected by the 
projector, we can know each point of its coordinates in the physical and image spaces. 
Thus, we can calculate the intrinsic and extrinsic parameters of the projector. For easy 
recovery of the relationship of each feature point between the camera and the projector, 
the Gray Code pattern is used. 
 
3D scanning 
 
Lanman and Taubin have presented “Build Your Own 3D Scanner: Optical Triangulation 
for Beginners” [Lanman and Taubin 2009]. We built the system based on the source code 
they provided. During the capture process, the system will project the fringe pattern on 
8 
 
of the camera and projector. Since the fringe pattern is both vertical and horizontal, the 
position should avoid the vertical and horizontal planes. Otherwise, it is not easy to 
determine the correct depth. 
 
When the Gray Code is projected from the projector and the image is acquired from the 
camera, the corresponding relationship between the camera and the projector is 
determined. From the view of the projector, each line of Gray Code is a plane. Each pixel 
of an image is a ray line. To estimate the intersection point of the ray line and the plane, 
we can obtain the 3D position. Thus, we can get the cloud point the set of target model 
and the current state of the working model. Figure 6 shows the scanning result of the 
target model.
 
 
Projector rendering  
 
To make sure 
everything is fine. We 
could check it by 
shading the model with 
a projector. If anything 
is wrong, such as 
camera parameter or 
projector parameter are 
wrong, the 3d model or 
shading position will be 
wrong also. That is, if 
any pixel of the lighting 
is not on the correct 
position, it is not 
possible only the model 
be lighted. 
 
To begin sculpture, we 
should prepare a 
scaffolding to support 
clay weight. After 
constructing the 
scaffolding, the 3D 
structure can be scanned 
and be rendered with a 
projector. The result is 
as figure 7 shown. . 
 
How application 
works 
10 
 
In the original implementation, two models are compared based on the shortest distance. 
To enhance the speed and calculate the difference of the two models, the sample data are 
divided into sample grids first. Each sample point will belong to a center point inside a 
grid. Each scanned data will locate at a grid first, thus it is easy to refine the shortest path 
inside this grid cell. However, there are two cases that should be cared for. Some parts in 
the target model are without sample points. Therefore, if two models are compared with 
the shortest path, this portion may reference a place far from the portion. In another case, 
the same effect will appear out of the silhouette of the target model from the viewpoint of 
the projector. In both cases, the system will get the wrong distance and cannot provide 
the correct information for the user. To avoid this situation, we need to construct a mesh 
for the target model first. MeshLab is used to remove outlying points and reconstruct a 
Poission mesh. Then, each ray line from the projector to the sample point is used to find 
the intersection with the target mesh. Therefore, if the intersection is in front of the 
working model, that means there is already too much clay in this location. If the 
intersection is behind the working model that means the clay in this location is not 
sufficient. If there is no intersection between the ray line and the target mesh, that means 
that the point is out of the silhouette region. This portion should be removed completely. 
 
Information rendering  
 
To guide a user as to how to sculpt 
a model, the system will show the 
difference in different colors. If 
there is not enough clay, the 
portion will be rendered as blue 
color. Vice versa the portion will 
be rendered as red. If this portion 
is out of the silhouette, it will be 
rendered as green. To discriminate 
coarse to fine in a different scale, 
the system provides a dynamic 
adjustment capability. In the 
coarse scale, more saturation of 
blue or red color is represented as 
a large difference, indicating that 
this area should be corrected first. 
At the fine-tuned scale, most of 
the portion will not be easy to 
discriminate. Thus, we can scale 
down the threshold to a lower 
value. Then the greatest saturation 
color can be represented as only 
one or two millimeters. Figure 10 
shows the difference between the 
coarse and fine rendering scales. 
The system also provides a 
12 
 
following instance will perhaps consist of too little. With the help of the fiducial marker, 
the registration problem is solved. Consistent progress is a big issue for users of such a 
system. 
 
During the sculpture process, users may focus on different types of information. In the 
initial stage, the user only pays attention to the silhouette of the target model. At the 
coarse modeling stage, the user is only concerned with the biggest difference. Thus, a 
varied scale of difference rendering is provided.  
 
For rendering the difference of color, the paper clay is the best choice. However, the 
paper clay can easily become hard. The moisture needs to be kept inside the clay in order 
to keep it soft. Otherwise, after hardening, changing the shape of the clay will be more 
difficult. But if there is too much moisture inside the paper clay, it will become too soft to 
sculpt its shape. In general, sculpting a statue is a time-consuming process. Even an 
experienced sculptor may need weeks or months. Oil-based clay may be a better choice 
because it can remain malleable even when left for long periods in dry environments. 
However, the color of the oil-based clay is too dark. Not only is it hard to render the 
difference of color, but it also is not easy to scan its 3D shape. To solve this problem, we 
change the color of the structured light to green to let its reflection become easier to 
discriminate. 
 
Art is a creation process. The system cannot replace the artist’s work. However, the 
system can act as a guide to teach a novice user how to sculpt. With the system’s help, 
even a novice user can construct a prototype in a few hours. However, there are also 
certain limitations under current implementation. 
 
The shadow area or black area cannot estimate the correct depth. For the human statue, 
the nose, chin and hair cannot be modeling well. And the color of the modeling material 
cannot be too dark. Otherwise, the system cannot capture its 3D structure. To reduce the 
effect of the problem, a suitable model and material will be easier. 
 
The system’s behavior is a discrete guide. When a user sculpts the model for a while, he 
should ask the system to create a new image and show the difference in the current state. 
The user should stop and wait for the system. The scanning process and computation time 
is about three minutes. If the user does not need to wait, the system can provide scanning 
and show the difference in a continual mode. In each modification of the model, the user 
can see the change immediately. It will be more productive. It is important to note that 
during the sculpture period, the hand of the user is usually dirty. It is not convenient to 
manipulate the system with a keyboard or mouse. If the system could capture the range 
data in a continual mode and recognize the position of the hand as a control event, the 
user would be very happy with this feature. Currently, the range data are scanned only for 
modeling. However, range data based applications are developed as a new user interface 
as of late. For example, the Kinect for Xbox 360 brings games and entertainment to life 
in extraordinary new ways without using a controller. However, if we have a high-speed 
camera, it will be easier to add the new feature to our system. 
 
14 
 
References 
BHASKER, E., JUANG, R., AND MAJUMDER, A. 2007. Registration techniques for 
using imperfect and par tially calibrated devices in planar multi-projector displays. IEEE 
Transactions on Visualization and Computer Graphics 13, 6, 1368–1375. 
 
CHOI, H., KYOUNG, D., AND JUNG, K., 2007. Real-time image correction for 
interactive environment.  
 
DAO, V. N., HOSOI, K., AND SUGIMOTO, M., 2007. A semiautomatic realtime 
calibration technique for a handheld projector. 
 
DRARENI, J., ROY, S., AND STURM, P. Geometric video projector auto-calibration. In 
Computer Vision and Pattern Recognition Workshops, 2009. CVPR Workshops 2009. 
IEEE Computer Society Conference on, 39–46. 
 
FLAGG, M., AND REHG, J. M. 2006. Projector-guided painting. In Proceedings of the 
19th annual ACM symposium on User interface software and technology, ACM, New 
York, NY, USA, UIST ’06, 235–244. 
 
FLAGG, M., SKEELS, C., AND REHG, J. M. 2009. Computational support for 
creativity via capture and access.  
 
GRIESSER, A., AND VAN GOOL, L. Automatic interactive calibration of multi-
projector-camera systems. In Computer Vision and Pattern Recognition Workshop, 2006. 
CVPRW ’06. Conference on, 8–8. 
 
ISHIHARA, Y., AND ISHIHARA, M., 2007. Locating a projector using the strength of 
beams reflected on a screen. 
 
JOHNSON, C. M., BHAT, A., AND THIBAULT, W. C., 2006. An evolutionary 
approach to camera-based projector calibration. 
 
KIMURA, M., MOCHIMARU, M., AND KANADE, T. Projector calibration using 
arbitrary planes and calibrated camera. In Computer Vision and Pattern Recognition, 
2007. CVPR ’07. IEEE Conference on, 1–2. 
 
LANMAN, D., AND TAUBIN, G., 2009. Build your own 3d scanner: Optical 
triangulation for beginners @ONLINE. 
 
L O¨ CHTEFELD, M., GEHRING, S., JUNG, R., AND KR U¨GER, A. 2011. guitar: 
supporting guitar learning through mobile projection. In Proceedings of the 2011 annual 
conference extended abstracts on Human factors in computing systems, ACM, New York, 
NY, USA, CHI EA ’11, 1447–1452. 
 
OKATANI, T., AND DEGUCHI, K. 2009. Easy calibration of a multi-projector display 
system. Int. J. Comput. Vision 85, 1, 1– 18. 
 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/28
國科會補助計畫
計畫名稱: 投影攝影系統互動之研究
計畫主持人: 吳賦哲
計畫編號: 100-2221-E-126-014- 學門領域: 計算機圖學
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
