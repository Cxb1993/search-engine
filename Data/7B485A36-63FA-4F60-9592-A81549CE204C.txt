I 
 
 
行政院國家科學委員會補助專題研究計畫 成果報告   
□期中進度報告 
 
大學雲：分散式雲端運算中介軟體 
子計畫Ｃ3：分散式資料庫管理系統 
 
計畫類別：□個別型計畫   整合型計畫 
計畫編號：NSC 99-2218-E-001-009 
執行期間：99年 8月 1日 至 100年 7月 31日 
 
 
執行機構及系所：中央研究院資訊科學研究所 
 
計畫主持人：吳真貞教授 
 
 
 
成果報告類型(依經費核定清單規定繳交)：精簡報告  □完整報告 
 
本計畫除繳交成果報告外，另須繳交以下出國心得報告： 
□ 赴國外出差或研習心得報告 
□ 赴大陸地區出差或研習心得報告 
□ 出席國際學術會議心得報告 
□ 國際合作研究計畫國外研究報告 
 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
            □涉及專利或其他智慧財產權，□一年二年後可公開查詢 
 
中   華   民   國  100  年  10  月  28  日
II 
 
Abstract 
As the size of data set in cloud increases rapidly, how to process large amount of data efficiently has 
become a critical issue. MapReduce provides a framework for large data processing and is shown to be 
scalable and fault-tolerant on commodity machines. However, it has higher learning curve than SQL-like 
language and the codes are hard to maintain and reuse. On the other hand, traditional SQL-based data 
processing is familiar to user but is limited in scalability. In this project, we propose a hybrid approach to fill 
the gap between SQL-based and MapReduce data processing.  
We develop a data management system for cloud, named SQLMR. SQLMR complies SQL-like queries 
to a sequence of MapReduce jobs. Existing SQL-based applications are compatible seamlessly with SQLMR 
and users can manage Tera to Pata Byte scale of data with SQL-like queries instead of writing MapReduce 
codes. We also devise a number of optimization techniques to improve the performance of SQLMR. The 
experiment results demonstrate both performance and scalability advantage of SQLMR compared to MySQL 
and two NoSQL data processing systems, Hive and HadoopDB. 
 
Keywords: Cloud Computing, Storage Cloud, cloud data management, NoSQL framework, SQL to NoSQL 
translation and optimization, MapReduce 
1 
 
一、 前言 
The advent of cloud computing and hosted software as a service is creating a novel market for data 
management. Cloud-based DB services are starting to appear, and have the potential to attract customers from 
very diverse sectors of the market, from small businesses aiming at reducing the total cost of ownership, to 
very large enterprises seeking high-profile solutions spanning on potentially thousands of machines. At the 
same time, due to the ever increasing size of data sets, traditional parallel database solution can be 
prohibitively expensive. To be able to perform this type of analysis in a cost-effective manner, several 
companies have developed distributed data storage and processing systems on large clusters of shared-nothing 
commodity servers, including Google File System [1], BigTable [2], MapReduce [3], Hadoop [4], Amazon’s 
Simple Storage Service (S3) [5], SimpleDB [6], Microsoft’s SDS Cloud database [7]. There are also various 
NoSQL databases used to manage large amounts of data, including MongoDB [8], Apache CouchDB [9], 
Cassandra [10] and Dynamo [11]. 
 
二、 研究目的 
Many of these cloud databases are designed to run on a cluster of hundreds to thousands of nodes, and 
are capable of serving data ranging from hundreds of terabytes to petabytes. Compared with traditional 
relational database servers, such cloud databases may offer less querying capability and often weaker 
consistency guarantees, but scale much better by providing built-in support on availability, elasticity, and load 
balancing. 
On the other hand, data management tools are an important part of relational and analytical data 
management business since business analysts are often not technically advanced and do not feel comfortable 
interfacing with low-level database software directly. These tools typically interface with the database using 
ODBC or JDBC, so database software that want to work these products must accept SQL queries. Therefore, a 
novel technology to combine DBMS capability with Cloud-scale scalability is highly desirable. 
In this project, we propose a hybrid solution, called SQLMR that combines the programming advantage 
of SQL with the fault tolerant, heterogeneous cluster, scalable capabilities of MapReduce. Users of SQLMR 
can write data management programs with familiar query language or to run existing programs without 
modification. SQLMR provides a compiler to translate a SQL program to a MapReduce program, and execute 
it in a MapReduce system. To achieve high performance in data processing, we also devise a number of 
optimization techniques. 
 
三、 文獻探討 
Current commercial Cloud DB products such as Amazon’s Simple Storage Service (S3) [5], SimpleDB 
[6] and Relational Database Service (RDS) [12], Microsoft’s SQL server [13] and SDS [7] Cloud database, all 
claim to support SQL. However, most of them do not satisfy many of the desirable properties of Cloud DBMS. 
For example, Amazon’s SimpleDB only supports a small subset of SQL queries. It does not support 
aggregation and joins types of complex queries. Microsoft’s SDS, although supports full SQL functionality, is 
far more inferior to traditional SQL servers in both performance and scalability. 
3 
 
codes are not compliant with Pig Latin, which limits the portability of existing applications to Pig. 
Similar to Pig, Hive [18] also provides a SQL-like query language named HiveQL, which makes it easy 
to be compatible with existing applications using SQL queries. Hive also compiles HiveQL to map-reduce 
code. Compared with Pig, Hive produces more efficient map-reduce code. 
HadoopDB system [19] is a data management system that combines DBMS capability and MapReduce 
techniques. It targets analytical workloads on structured data and is designed to run on commodity machines. 
HadoopDB inherits the scalability of Hadoop and the authors claim it achieves superior performance 
compared with current parallel DBMS. 
The major differences between these hybrid data management systems (SQLMR, Pig, Hive and 
HadoopDB) can be summarized as follows. We devise a number of novel optimization techniques to improve 
performance of query processing in SQLMR: (1) a low-overhead data file construction technique that enables 
fast dynamic conversion of SQL database files to HDFS (Hadoop distributed file system) files that can be 
accepted as input files by the MapReduce runtime engine. This technique significantly reduces data 
conversion time between SQL and MapReduce, (2) a set of effective database partitioning and indexing 
techniques for fast locating of queried data in HDFS and reducing disk I/O for range queries, (3) a query 
result caching mechanism that can avoid re-processing of redundant queries, and (4) optimization techniques 
for Hadoop’s MapReduce runtime system to further reduce query processing time. Our experiment results in 
later section demonstrate the performance and scalability advantage of SQLMR against these three systems. 
 
四、 研究方法 
A. SYSTEM ARCHITECTURE  
In this section, we describe the SQLMR system architecture. Since most users are familiar with 
SQL-like languages, the goal of the SQLMR system is to design a framework that combines the 
programming advantage of SQL and the scalability and fault tolerance of MapReduce. Figure 1 
illustrates the concept of SQLMR. The system accepts SQL queries as input and translates them to a 
sequence of MapReduce jobs. When the MapReduce jobs are completed, the system returns the query 
results to the user in SQL form. 
 
5 
 
2. Query Result Manager 
The query result runtime system caches the result for each query. When a new query enters 
SQLMR, the compiler first passes the query to Query Result Manager to compare the query with 
previous ones in the log. If there are valid cached results for that query, the result is returned to user 
without re-processing the query. Otherwise, the compiler will parse the query and generates 
optimized MapReduce code. The cached results will be invalid when a user updates or deletes data 
from the database. 
3. Database Partitioning and Indexing Manager (DPIM) 
This system component manages data files and indexing. When new data is added into the 
system, DPIM partitions the new data and creates index for the new data. With smart partitioning 
and indexing, SQLMR can do fast locating of queried data blocks as well as identifying exact data 
blocks that need be accessed in range query in order to reduce disk I/O. The partitioning and 
indexing techniques are what distinguish our work from other related efforts, which typically export 
the entire data file to the MapReduce runtime system. 
4. Optimized Hadoop 
Hadoop system is a software framework for distributed processing of large data sets on 
compute clusters. Our Compiler generates optimized MapReduce jobs and execute the jobs on the 
Hadoop system. We devise a set of optimizations, such as cross-rack communication optimization, 
to improve the performance of the Hadoop system. 
 
B. PERFORMANCE OPTIMIZATION 
1. Data Partitioning and Pre-processing 
In this section, we describe SQLMR’s approch to transferring data from traditional RDBMS to 
the Hadoop MapReduce system. We also give an overview of HadoopDB’s approach, which will be 
used as a basis for comparison in our experiments. Figure 2 is the flowchart of HadoopDB. At the 
beginning, the user needs to export all data from PostgreSQL to a comma-separated value (CSV) 
text file and put the file in HDFS for subsequent hash-partitioning pre-processing. The purpose of 
hash-partitioning is to push more query logic into databases (e.g. joins). This can be done in two 
phases. First, the data file needs to be loaded into HDFS. Then, a HadoopDB custom-made Hadoop 
job, named Global Hasher, re-partitions data into a specified number of partitions (e.g. number of 
nodes in a cluster). The next step is to download all partitioned data from HDFS to the local disk 
and import the split data to local PostgreSQL server on each node. In contrast, in our SQLMR 
framework, all database files are stored in HDFS directly without having to pre-process them. This 
design can reduce the pre-processing time significantly.  
Although the hybrid design of HadoopDB allows users to access PostgreSQL database directly, 
there are two problems that need be overcome. First, in HadoopDB each PostgreSQL server only 
stores a partition of the database. Therefore, the user has to merge all partial query results returned 
from each database partition to get the final result. Second, users need to recovery the system 
manually should a PostgreSQL Server crashes. In comparison with HadoopDB, our SQLMR 
framework stores all data in HDFS, which saves the user the burden of gathering the partial query 
results. The process of recovery can also be done by Hadoop MapReduce System automatically. 
7 
 
two indexing techniques to accelerate data searching. SQLMR chooses a suitable index technique 
depending on the characteristics of the database. We next introduce the two index techniques. 
(1). Partition Index 
In this approach, the files storing database tables are split into fixed length files. The size 
can be determined by the size of block in HDFS, such that a file would be completely 
contained in one block. Each file contains the records with a range of series keys. The range of 
keys is decided by the schema of table and the size of a file. For example, in Figure 5, there are 
4 columns in a table. After table schema analysis, a record is 2KB and a file is 64MB. Then, 
SQLMR will partition one table data file into multiple partitioned table data files. A file will 
contain a series of data rows in which the keys range from 1 to 32,768. The next file will 
contain data rows with keys ranging from 32,769 to 65,537, and so on. The insertion, deletion 
and search operation for a key are all constant time O(1), because the file contains the 
corresponding record and the offset in a file can be calculated by the key of the target record. 
This approach is suitable for data with dense key space, since we pre-allocate the space for a 
record in a file. For general cases, we use B+ tree index. 
  
(2). B+ tree Index 
B+ tree index are extensively and commonly used in database indexing. Many 
open-source and commercial database product, like Oracle [20] and MySQL, apply B+ tree to 
index the data. It is a general approach and applicable to various applications. In SQLMR, the 
B+ tree structure is maintained by the HDFS master, and we modify the DFS Client module in 
Hadoop such that we can query the block location by a key through the tree. The search, 
deletion, and insertion are all logarithmic amortized time O(logN) where N is the number of 
nodes in the tree. In order to build the index tree, we need to query the master node for the 
block information. The master node returns all the locations of a block, including the master 
copy and its replicas. Our B+ tree node stores all the replicated blocks information to maintain 
high availability of data blocks information. 
The query process is as follows. SQLMR receives a query of a search key and finds the 
corresponding block through the tree. The internal node of the tree only stores the information 
of key for searching, and the leaf node of the tree stores the data of block information, 
including the block ID and location. Figure 6 illustrates the structure of the B+ tree. To 
facilitate block mergence and split, the keys in a block have to be sorted. We sort the key in a 
block when the blocks is merged or split to reduce the extra overhead of sorting and since the 
number of keys in a block is bound by a constant C, where C = ( the size of block ) / ( the size 
9 
 
We choose five popular and representative applications as our benchmark suite and 
conduct the experiment in a four-rack cluster. The evaluation metric is the speedup rate 
compared with un-optimized Hadoop. Table II shows the speedup of each application. The best 
improvement is PageRank which achieves 32% and FPM does not have significant 
improvement. This is because FPM has a much skewed distribution in intermediate data size 
and our traffic model assumes the size of intermediate data is a constant. In contrast, other 
applications have approximately the same size of intermediate data. 
 
 
五、 實際效能 
A. Effect of Data Size 
This set of experiments compares the scalability w.r.t. increase in data size. The number of nodes is 
fixed at 10, and the data size varies from 512MB to 1TB. Figure 7 shows the execution time of MySQL, 
MySQL cluster, Hive, HadoopDB and SQLMR on SELECT operation with different data sizes. Figure 8 
and Figure 9 give graphical illustrations of the performance comparison. The SQL query is as follows. 
 
For small data size (Figure 8), we found that MySQL and MySQL cluster outperform 
MapReduce-based systems when the data size is smaller than 4GB. When the data size increases beyond 
8GB, both of them are outperformed by MapReduce based systems. The reason is that MySQL does not 
parallelize processing of single query. MySQL cluster employs in memory database technique, which 
writes all data to memory before starting database operation and thus is limited by the size of the 
physical memory. In our experiment environment, MySQL cluster would crash due to out of memory 
when the data size reaches 64GB. 
In Figure 9, MySQL crashes when the data size reaches 772GB. The reason is that a MySQL data 
table can only accommodate 232 data records. 772GB is the maximum data size that can be generated by 
11 
 
  
B. Effect of System Size 
This set of experiments compares the scalability of different database systems w.r.t. increase in 
system size. The number of physical nodes varies from 1 to 16. Each physical node runs four virtual 
machines (i.e., virtual nodes). The data size per virtual node is fixed at 10GB. Figure 13 compares the 
result of SELECT (range sum) query, and Figure 14 shows the result of JOIN query. 
As shown in Figure 13, HadoopDB exhibits unstable system scalability while SQLMR and Hive 
behave similarly and both exhibit more stable scalability than HadoopDB. All of the systems perform 
worst when the number of virtual nodes (i.e. virtual machines) is four. This is because the four virtual 
machines residing on the same physical node saturate source utilization on that node. When the number 
of virtual machines increases from 4 to 16, the workload is shared between multiple physical nodes, 
which increases parallelism and results in decrease of execution time. When the number of virtual 
machines increases to 32, the network becomes the bottleneck and causes increase in the execution time. 
From Figure 13, we can see that the performance improvement ratio of SQLMR against HadoopDB 
ranges from 4.16 (1 node) to 4.95 (64 nodes). The performance improvement ratio of SQLMR against 
Hive ranges from 1.67 to 2.05.  
Figure 14 also shows that HadoopDB exhibits unstable system scalability while SQLMR and Hive 
behave similarly and both exhibit more stable scalability than HadoopDB. The main reason for 
HadoopDB’s poor system scalability is that, HadoopDB uses PostgreSQL server on each local node as 
the database storage without the support of distributed storage system. HadoopDB uses the Map function 
to collect all the queried data and send the whole set of data to the reducer for computation of the final 
result. From Figure 13, we can see that the performance improvement ratio of SQLMR against 
HadoopDB ranges from 6.03 (2 nodes) to 10.57 (64 nodes). The performance improvement ratio of 
SQLMR against Hive ranges from 1.65 to 2.24. 
C. Comparison of Data Preprocessing 
Figure 15 compares the breakdown of data pre-processing overhead between SQLMR and 
HadoopDB. Recall that HadoopDB requires four phases of data pre-processing (Section IV-A), while 
SQLMR only requires two phases. For the partition phase, SQLMR uses the split function in the Linux 
OS to partition database table data into small files.  
As shown in Figure 16, HadoopDB requires 140.44 times more partitioning time than SQLMR with 
512MB data size. This is because HadoopDB relies on the Map function in the MapReduce system for 
the data partitioning phase. This approach benefits from large parallelism provided by the MapReduce 
system. However, it suffers large overhead when partitioning small data file. On the other hand, the 
performance advantage of SQLMR decreases when the data size increases. At 1TB data size, HadoopDB 
requires 70% partitioning time of SQLMR (Figure 17). The reason is that the split function is not 
13 
 
參考文獻 
[1] S. Ghemawat, H. Gobioff, and S.-T. Leung, “The Google file system,” in Proceedings of the nineteenth 
ACM symposium on Operating systems principles, ser. SOSP ’03. New York, NY, USA: ACM, 2003, pp. 
29–43. [Online]. Available: http://doi.acm.org/10.1145/945445.945450  
[2] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Burrows, T. Chandra, A. Fikes, and R. 
E. Gruber, “BigTable: A distributed storage system for structured data,” ACM Trans. Comput. Syst., vol. 
26, pp. 4:1–4:26, June 2008. [Online]. Available: http://doi.acm.org/10.1145/1365815.1365816  
[3] J. Dean and S. Ghemawat, “MapReduce: simplified data processing on large clusters,” in Proceedings of 
the 6th conference on Symposium on Operating Systems Design and Implementation, ser. OSDI 04, vol. 
6. Berkeley, CA, USA: USENIX Association, 2004, pp. 10–10.  
[4] “Hadoop,” http://hadoop.apache.org/.  
[5] “Amazon simple storage service,” http://aws.amazon.com/s3/.  
[6] “Amazon SimpleDB,” http://aws.amazon.com/simpledb/.  
[7] “Microsoft SQL azure,” http://msdn.microsoft.com/enus/windowsazure/sqlazure/default.aspx. 
[8] “MongoDB,” http://www.mongodb.org/.  
[9] “CouchDB,” http://couchdb.apache.org/.  
[10] “Cassandra,” http://cassandra.apache.org/. 
[11] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman, A. Pilchin, S. Sivasubramanian, P. 
Vosshall, and W. Vogels, “Dynamo: amazon’s highly available key-value store,” in Proceedings of 
twentyfirst ACM SIGOPS symposium on Operating systems principles, ser. SOSP ’07. New York, NY, 
USA: ACM, 2007, pp. 205–220. [Online]. Available: http://doi.acm.org/10.1145/1294261.1294281  
[12] “Amazon relational database service,” http://aws.amazon.com/rds/.  
[13] “Microsoft sql server,” http://www.microsoft.com/sqlserver/en/us/default.aspx.  
[14] “Nchc BigTable,” http://trac.nchc.org.tw/cloud/wiki/BigTable.  
[15] “Mysql database,” http://www.mysql.com/.  
[16] “Mysql cluster,” http://www.mysql.com/products/cluster/.  
[17] C. Olston, B. Reed, U. Srivastava, R. Kumar, and A. Tomkins, “Pig latin: a not-so-foreign language for 
data processing,” in Proceedings of the 2008 ACM SIGMOD international conference on Management of 
data, ser. SIGMOD ’08. New York, NY, USA: ACM, 2008, pp. 1099–1110. [Online]. Available: 
http://doi.acm.org/10.1145/1376616.1376726  
[18] A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka, S. Anthony, H. Liu, P. Wyckoff, and R. Murthy, 
“Hive: a warehousing solution over a map-reduce framework,” Proc. VLDB Endow., vol. 2, pp. 
1626–1629, August 2009. [Online]. Available: http://portal.acm.org/citation.cfm?id=1687553.1687609  
[19] A. Abouzeid, K. Bajda-Pawlikowski, D. Abadi, A. Silberschatz, and A. Rasin, “HadoopDB: an 
architectural hybrid of MapReduce and DBMS technologies for analytical workloads,” Proc. VLDB 
Endow., vol. 2, pp. 922–933, August 2009. [Online]. Available: 
http://portal.acm.org/citation.cfm?id=1687627.1687731  
[20] “Oracle database,” http://www.oracle.com/us/products/database/index.html 
[21] L.-Y. Ho, J.-J. Wu, and P. Liu, “Optimal algorithms for cross-rack communication optimization in 
MapReduce framework,” IEEE International Conference in Cloud Computing, 2011. 
15 
 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500字為限） 
雲端運算是這一兩年來最火熱也最具有影響力的 IT技術(或是概念)之一，本
研究計畫研究了雲端運算中各階層的關鍵技術，此計畫主要研究項目是雲端
資料庫技術。其關鍵性在於資料庫對於產業界來說，是相當普遍且需求量大
的一項軟體技術。面對儲存資料量越趨於龐大，傳統的資料庫已逐漸無法滿
足產業界的需求。我們的雲端資料庫不僅掌握具備雲端的重要技術(如大量運
算、資料儲存等等)，還具備了雲端的重要特性(如資料儲存之安全性、效能
擴展的特性)，在此兩項基礎上，我們建置了一套完整的資料庫系統供產業界
使用。讓使用者能夠無痛升級、避免資料/技術的移轉所產生的龐大成本。我
們的研究算是在台灣發展雲端運算中相當重要的研究計畫。 
本年度我們建置了一套雲端資料庫管理系統，並提供許多雲端中重要的服務
如資料儲存索引、語言翻譯轉換、即時管理系統等，這些系統都有相當重要
的研究與商品化之價值。在未來展望方面，由於我們的設計特別針對分散式
的資源以及考慮資源的異質性，因此更可以將此系統套用到整合多個分散式
的資料中心，或擴展到整合跨國的資源來建置大型分散式的雲端資料庫系統。 
 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：吳真貞 計畫編號：99-2218-E-001-009- 
計畫名稱：大學雲：分散式雲端運算中介軟體--子計畫六：分散式資料管理系統 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 4 100%  
博士生 2 2 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
