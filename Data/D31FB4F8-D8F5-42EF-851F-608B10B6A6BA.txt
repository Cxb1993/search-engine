 2
行政院國家科學委員會專題研究計畫成果報告  
3D 音訊的內容製作、處理與互動式呈現 
子計畫五 : 基於單視域來源之立體視訊轉換技術  
 計畫編號：NSC 96-2221-E-212-003-MY2 
執行期限：96 年 8 月 1 日至 98 年 7 月 31 日 
主持人：林國祥助理教授   大葉大學資訊工程學系 
計畫參與人員：陳怡菁、林一昌、趙翊傑、黃鈞凱  大葉大學資工所 
 
一、 中文摘要 
 
近來三維影像已經於許多應用領域中獲得相當程
度的注意，例如，多媒體系統、電玩遊戲、立體電
視(3DTV)廣播系統和虛擬實境等等。其主要的原
因是，相較於二維影像，三維影像可以提供比較真
實的感覺。隨著三維視訊擷取裝置和三維電視顯示
裝置相關技術的逐漸成熟，三維影像/視訊的重要
性將會隨之增加。然而，由於三維影像/視訊擷取
裝置不易攜帶與架設，三維影像/視訊是不容易取
得的。此外，目前三維影像/視訊技術也僅設計與
提供專業使用者。因此，我們針對二維視訊轉三維
立體視訊技術 (2D/3D video conversion) 進行研
究。此項研究成果可以使得使用者在家享受 3D 顯
示的生活。 
到目前為止，本計畫已經分析探討四種不同類型的
特徵用於深度資訊之估計。為了整合這四種特徵，
本計畫使用最小平方誤差法求出相對應之權重。透
過這些權重可以整合這四種不同類型的特徵，估計
出深度資訊。配合背景深度指派，則可以獲得完整
之深度資訊，進而轉換獲得視差圖。此外，主觀實
驗評估顯示，基於 DIBR 技術下，本計畫產生 3D
視訊可以具有接近真實 3D 視訊的視覺品質。 
 
關鍵詞：2D/3D video converter、depth estimation、
stereo image synthesis  
  
Abstract 
 
Recently three-dimensional (3D) images are 
attracting a considerable amount of attention from 
various fields of application, such as multimedia 
systems, games, 3D-TV broadcasting, and virtual 
reality. This is because 3D images provide higher 
realism than the conventional two-dimensional (2D) 
images. With the technologies of 3D video capture 
device and 3D-TV display device getting more 
mature, the importance of the 3D images/videos will 
increase in the future. However, it is still difficult to 
get 3D image/videos in the real world because a 3D 
image/video capture device is heavy to carry around 
and to be installed and adjusted. In addition, current 
3D image/video technologies are only developed and 
provided in the professional fields. Therefore, we 
focus on the research of converting 2D videos into 
3D videos. This research result can make people 
enjoy the 3D display at home. 
Until now, four features for depth estimation are 
investigated in this project. In order to integrate those 
features, we first use the least square method to 
calculate the weights and then combine those 
features for depth estimation based on the computed 
weights. After assigning the background depth, the 
final depth can be obtained and then a disparity map 
can be derived. In addition, subjective experiments 
show that the 3D stereo video generated by using our 
depth assignment scheme and the Depth Image 
Based Rendering (DIBR) technique presents little 
difference to that created based on the depth ground 
truths. 
 
二、 緣由與目的 
 
由於遺失深度之資訊，要發展一套全自動的
2D to 3D video conversion 演算法是非常困難的。
此外，影像內容可能千變萬化，不太可能用簡單的
方法即可估計出深度資訊進而將 2D 視訊轉換成
3D 視訊。可以預期地，要將一張影像轉換成立體
影像，即左、右眼影像，其最重要的步驟為影像中
的場景深度估測。若可以獲得正確的深度資訊，將
原影像配合相對應的深度圖，即可利用 DIBR 
(depth image based rendering) 技術，合成出左、右
眼影像。 
經由文獻[1]~[9]，我們可以將估測的方法分
為(1)單一深度線索估測法、(2)影像分割 (image 
segmentation) 深度指定法和(3)混合法三類。 
(1) 單一深度線索估測法  
 在文獻[1][2][3]中，使用單一深度線索做為深
度估測的依據，其中文獻  [1] 使用影像中的
motion vector 資訊來估測深度。意即 motion 愈大，
可能距離觀察者愈近，反之，距離觀察者愈遠。文
獻 [2] 使用影像中的 edge 資訊，edge 愈大，距離
觀察者愈近，反之，距離觀察者愈遠。文獻 [3] 藉
由小波轉換分析影像中高頻成份資訊，高頻成分愈
多，可能距離觀察者愈近，反之，距離觀察者愈遠。
以上三者皆只使用單一種深度線索來做估測，轉換
的效果相當有限。 
 4
⎥⎦
⎤⎢⎣
⎡+⎥⎦
⎤⎢⎣
⎡⎥⎦
⎤⎢⎣
⎡
−=⎥⎦
⎤⎢⎣
⎡
′
′
tilt
pan
y
x
zoomrotate
rotatezoom
y
x
 
其中，[ ]tyx 、[ ]tyx ′′ 分別代表攝影機運動前
後的影像點座標(對應到相同物體點)。pan 與 tilt
則代表小角度 panning 或 tilting 時，在影像平面上
產生的位移量 (分別於 x 及 y 方向上)。根據上面
的假設，本計畫選取多數之移動向量視為攝影機運
動產生之影響，另外，此 4-parameter model 與 affine 
model 非常相似，因此可以採用類似 least square 
estimation 的方法來求出攝影機運動參數 zoom、
rotate、pan 和 tilt。可以預期地，使用 least square 
estimation 求解方式是非常簡單，但是，假若選取
之移動向量也包含移動物體部分，則計算出之攝影
機運動參數誤差較大。為了解決此問題，本計畫參
照文獻[13]提出之方法，使用疊代修正的方式去除
移動物體之部分，進而求出較精確的攝影機運動參
數，如圖 3 所示。 
Parameter 
estimation
Outliers exist?
Motion vectors
Camera motion 
parameters
Yes
No
 
圖 3、攝影機運動參數疊代修正示意圖 
 
3.2 移動向量修正 
移動向量對於精確地進行 depth information 
estimation 的非常重要。雖然已經可以估計攝影機
移動參數，但是壓縮視訊中之移動向量，通常會有
不一致或不正確的現象，因此，本計畫針對壓縮視
訊之移動向量進行修正的處理。 
基本上，基於局部剛性物體的假設下，除了
屬於物體邊緣的 boundary MB 外，物體內每個 
MB 的 MV 應該是與其周圍相鄰 MB 的 MV 
相似。即使是 boundary MB，其 MV 亦會與其中
一半相鄰 MB 者 (即物體或背景)相似。在此前提
下，比較每個 MB 與周遭 MBs 其 MV 的差異
性。其差異性大者，則我們可以判定這個 MB 的 
MV 是有問題的。 
紅色雙線段即為兩 macroblock 
之 motion vector 向量差 
箭頭表示為每個 macroblock 的 motion vector 
數字為 macroblock 的編號 
1 2 3
4 5 6
7 8 9
5
1
  
圖 4、3×3 區塊內的移動向量表示 
 
本計畫採用的方法是統計一個 3×3 區域內 
(每一個 element 代表一個 Macroblock )，兩兩個 
Macroblock 間移動向量的相似性。例如圖 4 中，
編號 1~9 的 Macroblock ，其中編號 5 代表目前
欲進行移動向量修正的  Macroblock ，記為 
MB#5 。在此，我們先利用一些演算法來判定 
MB#5 的移動向量是否出了問題，若判定移動向
量有錯 (如圖 5 的左圖)，才須加以修正。 
本計畫所採用之修正的方法是使用多數決的
方式。當發現某一 Macroblock 的移動向量出現問
題時，我們將企圖找出周遭正確的 Macroblock 移
動向量，取這些向量的平均值來修正替代其原始的
移動向量。為什麼說是周遭正確的 Macroblock 移
動向量呢？因為周遭的 Macroblock移動向量也有
可能是錯的，錯誤的 Macroblock 移動向量會導致
我們的修正產生誤差，所以我們要找出周遭正確的 
Macroblock 移動向量。 
 
   
 
  
   
   
 
  
   
中央 Macroblock 有錯誤 中央 Macroblock 正確 
 
圖 5、Macroblock 移動向量出現錯誤的例子 
 
例如圖 6 中，MB #5 的移動向量有錯誤，所
以利用週遭 MBs 的移動向量做修正，但是 MB 
#7 所擁有的移動向量是錯誤的，所以我們只取 
MBs #1, #2, #3, #4, #6, #8, #9 的移動向量作平
均，來取代之前錯誤的 MB 移動向量值。當然也
可能 MB #5 周遭 MB 移動向量的關係很雜亂而
無法估測，在此情況下，我們不針對 MB #5 進行
移動向量估測。 
 
 6
度下的特徵值，最後經過適當的線性加乘得到最終
的深度圖如圖 12 所示。 
 
圖 8、材質漸層示意圖 
 
 
圖 9、 Law’s masks 
 
 
圖 10、大氣透視示意圖 (摘自文獻[16]) 
 
 
圖 11、multi-resolution feature extraction flow chart 
 
  
(a)                (b) 
  
(c)                (d) 
圖 12、(a)原解析度 (b)1/2 解析度 (c)1/4 解析度 
(d)最終深度圖 
 
4.2 特徵融合之參數估測 
 為了對上述四種計算出的特徵值進行融合，
我們希望能以真實的深度圖為依據，找到一組適當
的線性參數解。在本計畫中，我們使用 least square 
estimation 方法求出最佳的參數解 ω1 到 ω4，其做
法如下： 
 
motion Variance Edge density contrast
Initail depth map
*? 1 *? 2 *? 3 *? 4
 
圖 13、特徵融合流程圖 
 
 
 
 
 
                                       (5) 
對於影像中任一位置，我們可以求得 a、b、
c、d 四種特徵值，而圖 7(a)所用的測試影像為
Microsoft 所提供的 break dancer 影像，並提供相
對應的深度圖，我們以此深度值做為真實深度值 
(ground truth depth) ，來估測出我們的線性解 ω1
到 ω4。實驗結果顯示，以真實的深度圖為依據去
估測線性解，可以找出較為適當的參數，使得結果
較趨近於真實深度值。 
 
五、 前景深度修正與背景深度指派 
 
5.1 視訊內容分析與分類 
根據視訊內容與深度線索特徵之性質，我們
將視訊分成下列三種類型： 
(1) 移動量大且背景單純 
(2) 移動量小 
(3) 移動量大且背景複雜 
針對上述三種類型之視訊，本計畫分別設計出合適
之前景物體切割法則，用以輔助深度值之修正與指
派。為了分類此三種視訊，我們以一個 video shot 
為處理單元  (假設已進行 shot segmentation 處
理)，配合移動量和背景複雜度分析來達成。後續，
我們將分別針對移動量和背景複雜度分析進行說
明。 
(1) 移動量分析 
對於一個 video shot 之移動量分析，先計算
此 shot 內之平均動量。每張畫面之平均動量之定
義如下： 
,))1,,(),,((1 ∑∑ −−−
x y
d
WH
TtyxItyxIU
NN  (6) 
其中，NH 為畫面高度，NW 為畫面寬度，I(x,y,t)
表示第 t 張影像中在(x,y)處之灰階值，Td表示前後
影像間絕對差值之門檻值， )(⋅U 代表  unit step 
1 1 1 1   1
1
1 1 1 1   2
2
                        
3
                        
4
     n n n n n
a b c d ground truth depth
a b c d ground truth depth
a b c d ground truth depth
ω
ω
ω
ω
⎡ ⎤ ⎡ ⎤⎡ ⎤⎢ ⎥ ⎢ ⎥⎢ ⎥⎢ ⎥ ⎢ ⎥⎢ ⎥⎢ ⎥ ⎢ ⎥⋅ = ⋅⎢ ⎥⎢ ⎥ ⎢ ⎥⋅ ⋅⎢ ⎥⎢ ⎥ ⎢ ⎥⎣ ⎦⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣ ⎦
 8
之深度初始值較小。經過二值化處理後容易被誤判
為背景區域。此情況會造成物件內部產生空洞現
象。為了消除物件內空洞現象，本計畫將針對任一
個標記之背景區域被前景區域包圍和任一個標記
之背景區域只與影像其中一個外框邊緣相鄰者做
處理，將該背景區域修正為前景區域。 
 
• Delaunay 三角化修正 
經過以上步驟，我們可以得到 block-based 
的前景物體區域圖，雖然切割出的前景位置與實際
前景位置大致上符合，但因為此前景圖是以區塊為
單位，切割出的輪廓無法與實際物體輪廓吻合。在
此步驟中，我們將使用 Delaunay 三角化做前景廓
輪修正。建構 Delaunay 三角化最簡易的演算法為 
Bowyer-Watson [17]。藉由 Delaunay 三角化圖和 
block-based 前景圖做比對，得到較符合實際物體
輪廓的前景圖。其步驟如下： 
(1.) 對於 Delaunay 三角化圖，我們對每個三角形
區域做相鄰單元標記，得到各個區域的面積。 
(2.) 統計各區域中， block-based 前景區域佔任一
區域的比例，若大於一個門檻值，則此區域設為前
景區域。 
 
• 前景物體深度值指派 
我們將對上述切割出的前景物體指派深度
值。其方法為對前景區域，給定初步深度值，再對
前景區域中，深度值過小的位置，以左、右鄰近的
深度峰值進行內插。 
 
5.2.2 法二：累積畫面差值+ Delaunay 三角化修正
之前景切割技術與深度值指派 
對於移動量較小的視訊內容而言，例如新聞
畫面、面談畫面等，不論背景複雜度大或小，要切
割出前景移動物體相較於移動量大的視訊容易的
多。在此，我們將利用畫面差值做為前景移動物體
偵測的方式，但由於移動量較小，單單取前後張畫
面的差值並不足夠，因此以下將使用累積多張畫面
的差值來做為偵測的依據，選取的畫面張數將跟移
動量分析有關，移動量愈小，取的畫面張數要愈
多，資訊才足夠，最後再配合前述的 Delaunay 三
角化做修正，即可將前景移動物體切割出。流程如
圖 15 所示。 
 
單視域視訊 
累積畫面張數
決定 
移動物體偵測 
靜止狀態判斷 
否 
Delaunay 三
角化修正 
靜止狀態修正 
是 
前景深度圖 
空洞填補 
雜訊消除 
區域移除 
輪廓平滑化 
前景移動物體
深度值指派 
 
圖 15 法二流程圖 
 
• 累積畫面張數決定 
對於使用累積畫面張數進行移動物體偵測而
言，累積畫面張數的多寡對於偵測的結果有很大的
影響。在此我們將延用之前步驟中的移動量分析，
藉由 video shot 的平均動量大小來決定要累積的
畫面張數，平均動量愈大，需累積的畫面張數愈
少；反之，愈多，彼此呈反比。平均動量和累積畫
面張數間的對應函式將由實驗中分析得到，如此系
統能夠依內容做調適性的改變。 
 
• 移動物體偵測 
對於畫面中的任一點，我們將累積前數張畫
面差值的絕對值總和，若此數值大於一個門檻值，
則此點判定為移動像素，設為 255，若否，則設為
0，藉此初步偵測出移動物體。 
 
• 空洞填補 
為了防止移動物體輪廓中間因為太平滑或移
動量太小，以致於累積像素差值太小，我們對偵測
出的移動物體做補洞的動作。 
 
• 雜訊消除 
為了避免雜訊干擾造成的偵測錯誤，我們使
用形態學中的斷開 (opening) 處理，消除了雜訊的
影響，並且讓輪廓較平滑。 
 10
如公式(12)所示。深度愈遠，視差愈大，它跟負視
差剛好相反，能夠使畫面呈現在螢幕後方。 
,
255
1 ⎟⎠
⎞⎜⎝
⎛ −×= Dtd x
    (12) 
其中，變數定義如同(11)式。輸入影像上座標位置
將右移視差量的一半產生左影像，左移產生右影
像。 
(3)正、負視差繪圖法： 
,
128
1 ⎟⎠
⎞⎜⎝
⎛ −×= Dtd x
    (13) 
其中，變數定義如同(13)式。其中分母項代表零視
差平面的位置，深度值大於 128 將產生負視差，小
於 128 將產生正視差，如此可將深度值較大的前景
呈現在螢幕前方，背景則呈現在螢幕後方。 
恰當的調整視差方式，能夠產生出不同的立體
效果。但若使用不當，將可能產生突兀、失真，甚
至無立體效果。為了讓觀賞者享受較舒服的立體視
覺效果，本計畫將暫時採取正視差繪圖法作為合成
左、右眼影像的轉換公式。 
 
七、 實驗結果 
 
由於立體影像的品質好壞，目前尚未有較為客
觀的評估方法，因此目前將以附有深度圖的影像來
做測試，希望能估測出近似於真實深度圖的結果，
並且產生出和使用真實深度圖產生的左、右眼影像
相似的立體效果和品質。目前測試影像包括
Microsoft 提供的 break dancer 和 ballet 影像，如圖
16、17，與測試沒有深度圖的影像如 weather 影像。 
本計畫使用 SHARP 立體液晶顯示器做為觀看
立體影像的方式，使用裸眼立體顯示器的好處是可
以避免傳統使用偏光式立體眼鏡可能造成鬼影產
生，使用交錯式立體眼鏡的成本又太高，容易損
壞，且在觀賞時容易產生閃爍的情況。 
 在估測出場景深度後，本計畫將以 DIBR 技
術合成左、右眼影像。接著以主觀人眼評估來分析
各種立體影像的立體效果。我們對 10 至 12 個人進
行測試，對立體視訊打 0 至 10 的分數，並且不事
先告知立體影像的產生方式，以維持公平性。深度
影像與合成左、右眼影像如圖 18、圖 19、圖 20
和圖 21 所示。 
 
  
(a)                (b) 
圖 16、(a) break dancer 測試影像，(b)相對應的真
實深度圖 
 
  
(a)                (b) 
圖 17、(a) ballet 測試影像，(b)相對應的真實深度 
 
  
(a)                 (b) 
  
(c)                 (d) 
圖 18、(a) break dancer 測試影像， (b) 估測深度，
圖 (c) 合成出的左眼影像，(d) 合成出的右眼影像 
 
  
            (a)                (b) 
  
            (c)                (d) 
圖 19、(a) Flamenco 測試影像， (b) 估測深度圖， 
(c) 合成出的左眼影像，(d) 合成出的右眼影像 
 
表 1 顯示人眼主觀評估結果， breakdancer1 
為使用我們系統所得到深度圖而合成出的立體視
訊，其與配合真實深度圖合成的立體視訊 
breakdancer2 分數差不多，顯示出我們的方法效果
已相當不錯。Flamenco 影像由於畫面中光影的影
響，造成地面部分有少許深度錯置造成的凹凸不
平，造成人眼觀看的不適，分數稍低，但還在可接
受的範圍。Salesman 影像尺寸較小，造成人眼不
容易感受到立體感，分數也稍低。而 Manege1 影
像移動量大且背景複雜，只用移動向量做深度估
測，雖然造成深度感較沒有真實的立體影像 
Manege2 好，但已有不錯的接受度。總括來說，
 12
[5] Joohwan Kim, Yunhee Kim, Junghyun Park, 
Jin-mo Kang, and Byoungho Lee, “Stereoscopic 
conversion of two-dimensional movie encoded in 
MPEG-2,” in Proc. SPIE, vol. 6311, 2006. 
[6] Yu-Lin Chang, Chih-Ying Fang, Li-Fu Ding, 
Shao-Yi Chen, and Liang-Gee Chen, “Depth 
Map Generation for 2D-to-3D Conversion by 
Short-Term Motion Assisted Color 
Segmentation,” in Proc. IEEE International 
Conference on Multimedia and Expo, pp.1958 –
1961, 2007. 
[7] S. Battiato, A. capra, S. Curti, and M. L. Cascia, 
“3D stereoscopic pairs by depth-map 
generation,” Proceeding of the 2nd International
Symposium on 3D Data Processing, 
Visualization, and Transmission, Jun 19-21, 
2004. 
[8] S. Battiato, S. Curti, M. La Cascia, E. Scordato, 
and M. Tortora, “Depth Map Generation By 
Image Classification,” SPIE IS&T/SPIE's 16th 
Annual Symposium on Electronic Imaging, 2004.
[9] Yu-Lin Chang, Wei-Yin Chen, Jing-Ying Chang, 
Yi-Min Tsai, Chia-Lin Lee, and Liang-Gee 
Chen, “Priority depth fusion for the 2D to 3D 
conversion system,” in Proc. SPIE, vol. 6805, 
2008.    
[10] Bela Julesz, Foundations of Cyclopean 
Perception, 2006. 
[11] 
 
 
Y. Wang, J. Ostermann, and Y.-Q. Zhang, 
Video Processing and Communication, Prentice 
Hall, 2002. 
[12] Y.-P.Vtn, S. R. Kulkarni, and P. J. Ramadge, “A 
new method for camera motion parameter 
estimation,” in Proc. IEEE Int’l Conf. Image 
Processing, vol. 1, pp. 406-409, 1995. 
[13] Y. Su, M.-T. Sun, and V. Hsu, “Global motion 
estimation from coarsely sampled motion vector 
field and the applications,” IEEE Trans. on 
Circuits and Systems for Video Technology, vol. 
15, no. 2, Feb. 2005. 
[14] Shih-Hsuan Yang, Fu-Min Jheng, “An Adaptive 
Image Stabilization Technique,” IEEE 
International Conference on Systems, Man and 
Cybernetics, vol. 3, pp. 1968-1973, 2006. 
[15] E.R. Davies. Laws’ texture energy in TEXTURE. 
In Machine Vision: Theory, Algorithms, 
Practicalities 2nd Edition. Academic Press, San 
Diego, 1997. 
[16] 陳明民，數位立體影像之理論探討與創作實驗
之研究，國立嘉義大學碩士論文，2005 年。
[17] 吳振君, 施斌, 祁長青, “任意多介質區域變
尺寸有限元網格自動剖分,＂ 工程地質電腦應
用，2002 年 4 期 
 
 
九、 計畫成果自評 
根據目前之研究結果而言，與原計畫內容之符
合度很高，已經達成本計畫預期的進度與成果。由
於 2D/3D video conversion 之技術，將有助於數位
內容 3D 影像/視訊相關產業技術層面的豐富與擴
展，也有助於智慧生活空間科技的厚實，因此，本
計畫的研究成果非常值得申請專利。此外，本計畫
之研究成果具有相當之學術價值，也將陸續發表於
國際會議與國際期刊中。 
對於學生思考能力、英文論文閱讀能力、實作
能力、系統整合之能力之等各方面的訓練，均有著
非常大的助益。 
表 Y04 
表一、本人於MVA2009之報告時程與摘要 
Session 3  
Title Image Forgery Detection Based on Quantization Table Estimation 
Author 
*Guo-Shiang Lin (Da-Yeh University, Taiwan), Min-Kuan Chang (National Chung Hsing 
University, Taiwan), You-lin Chen (National Chung Hsing University, Taiwan) 
 
Page pp. 66 - 69 
Keyword Forgery detection, quantization table estimation 
Abstract 
In this paper, we proposed a passive scheme to achieve image forgery. The inconsistent 
measure of quantization table is characterized to develop the proposed scheme. The proposed 
scheme is composed of candidate region selection, quantization table estimation, and forgery 
detection. To select candidate regions for estimating quantization table, a split-and-merge 
algorithm based on quad-tree decomposition is devised. To estimate the quantization table, 
we classify the type of PSD and then adjust the estimation algorithm. After quantization table 
estimation, the variation resulting from the inconsistent of quantization table is utilized to 
detect tampered regions. The experimental results show that our proposed scheme can not 
only estimates quantization table correctly but also detect tampered regions well. 
 
 
 
圖一、本人出席MVA2009國際會議之照片 
 
