I 
 
（一）計畫中文摘要 
隨著電腦與網路技術的日新月異及大量普及，人們產生與獲取資料越來越快
速便捷，因而對於分析處理大量資料的技術更加重視，以至帶動了各式資料探勘
相關技術的研究發展，其中頻繁項目集的探勘可以應用在很多不同的領域上，也
是常常被使用於交易資料庫分析的技術。但是隨著技術日漸進步，所帶來隱私的
議題也越來越受到關注。過去保護交易資料庫內敏感資訊的技術，都是以單一的
最小支持度或是隱私係數來作為保護的基準，並且嘗試取得「機密資訊保護」和
「非機密資訊保存」之間的平衡點。但是在實際生活中，不同的項目集可能需要
運用不同的最小支持度去判斷他是否為頻繁的，而在靈活的最小支持度設定下，
過去針對單一門檻值下頻繁項目集特性而設計的方法都不再適用，因此在本計畫
中，我們加入多門檻值的設定，考量在多門檻值下頻繁項目集的特性，提出一個
新的淨除演算法來保護機密的資訊，使得對敏感資訊的保護能更切合於它本身的
特性，並藉此達到在修改過後的資料庫內保留更多的非機密資訊，亦即降低資訊
遺失的程度。最後會對所提出之方法進行效率、擴充性和對資料庫影響最小化等
各方面作最佳化的研究。 
 
（二） 計畫英文摘要 
Frequent pattern mining is one of popular research topics in the data mining area. 
With the advance of these techniques, privacy issues attract more and more attentions 
in recent years. In this field, previous works hide sensitive information based on a 
uniform support threshold or using a privacy disclosure parameter. The challenge is 
how to achieve a balance between the information preservation and the sensitivity 
protection. However, in practical applications, we probably need to apply different 
support thresholds on different itemsets to reflect their significance respectively. In 
this project, we design new hiding strategies to hide sensitive patterns with multiple 
support thresholds. With flexible user-defined multiple support thresholds, the hiding 
result is expected to be closer to user requirements and real applications. Besides, 
after hiding sensitive patterns and rules, the revised dataset is expected to preserve 
most information of the original dataset. Furthermore, in this project, we also try to 
extend our method to optimize the efficiency, the scalability, and the preservation of 
information while hiding sensitive patterns. 
 
 
 
 
 
1 
 
一、前言 
近年來隨著網際網路的普及以及資料庫技術的純熟，資料的產生與收集都更
加便利，除了組織和公司團體都各自擁有了大量的資料累積，個人用戶也成為各
式資料來源的貢獻者，造就了資料量快速且大量成長的現象，但是這一堆未經過
處理的資料通常不適合直接地運用於一些商業決策或醫學、股票、環境等應用上
面，因此，資料擁有者對於進一步之資料分析處理技術的需求日漸增加，也使得
資料探勘(data mining)相關技術更加受到重視[1]。過去有許多學者對資料探勘下
了很多不同的定義，又可以把它稱為資料庫的知識挖掘，簡稱為 KDD (Knowledge 
Discovery in Databases)。一般而言，資料探勘指得是從大量資料當中萃取出合適
的資料，進行資料處理、挖掘，以找出專家未知的或是有趣的、有用的、潛在的
資訊，作為決策的參考依據。 
隨著資料探勘技術日漸提昇，我們可以從雜亂無章的資料當中探索出更多有
用的資訊，但是同時也面臨到一個值得重視的問題⎯⎯隱私(Privacy)。從資料庫
當中分析萃取出來的資訊有時候可能是一些公司組織的機密，或是牽涉到個人的
隱私權，所以在探尋隱含的有用資訊之際，同時也要能確保不會洩漏出機密或敏
感的資訊。然而，保護機密資訊的方式，若只是單純地將機密資訊從資料庫裡刪
除，再釋放出修改過的資料庫，很有可能經由推論或探勘的方法，從非機密的資
訊中推演得知機密資訊，因此，在設計資料保護機制時，不但要考慮到機密的資
訊必須避免經由資料探勘的技術被獲取，還要更進一步考慮，從所有的非機密資
訊當中應該都不可以推得機密的資訊。除此之外，資料探勘分析的目的是希望取
得有用的資訊來幫助制定決策或是其他運用方面，所以在保護機密資訊的同時，
也必須注意到原本資訊保存的最大化，並且盡量不要產生額外的非真實資訊，以
確保原本資料庫本身的價值。 
二、研究目的 
計劃中，我們針對從交易資料庫中隱藏敏感項目集的問題上，在[2]中已經
完整的解釋隱藏敏感項目集的動機及重要性。正如以下所述，假設大部份的顧客
買牛奶通常也會買 Green Paper 這家造紙公司的紙。如果另一家造紙公司 Dedtrees 
Paper 從一個超級市場的資料庫中探勘出這樣的一個法則，然後發布一個促銷活
動「如果你買 Dedtrees Paper 的紙，再買牛奶，你的牛奶將會得到 50 元的折扣」
接著 Green Paper 造紙公司的銷售量就會因為這樣的一個商業策略而降低。出於
這個原因 Green Paper 造紙公司不會想要提供較低的價格給超級市場。另一方
面，Dedtrees Paper 造紙公司已經達到他的目的也再也不會有意願提供較低的價
格給超級市場。那超級市場就會遭遇到相當嚴重的損失，因此在公佈資料庫之前
必須先對敏感資料做淨除動作。 
3 
 
擇方式，使得資料庫的品質及相關聯的高頻項目集能夠被完好的維護。在[4]中，
作者提出一個演算法可以用來抵擋向前推論攻擊(forward inference attack)。藉由
乘上原資料庫矩陣，然後一起作一次的淨除。這個方法擁有更好的效能。最近的
研究大部份都是注重在最小化對資料庫的副作用[7][8]。這些方法分別減少了對
交易資料及項目的修改來限制對資料庫所產生的副作用。 
 資料重建：這一類演算法的動機在於先前基於資料庫作修改的方法花太多時
間在掃描資料庫上而且也不能直接控制公佈資料的資訊。因此資料重建這一類演
算法，使用基於資訊考量的方法，直接重建包含資料庫擁有者所要保留的資訊的
新資料庫，再將其公佈出去。一般的情，在原資料庫中的高頻項目集是被視為一
種知識。反向高頻項目集探勘(inverse frequent set mining)問題在[9]中第一次被提
出，也被證明為是一個 NP-hard 的問題。因而，大部份的學者應用這樣的概念到
隱私的議題和演算法的基準[10]。在[11]中，作者提出一個 constraint inverse 
itemset lattice mining 技術來自動產生簡單的、可公佈的且可分享的資料集。他指
出如果存在一個可行的支持度集合，他們可以藉由一對一的對映產生包含一些高
頻項目集的新資料庫。在[12]中，作者提出 FP-tree-based 的方法用來做 inverse 
frequent set 探勘。且新產生的資料庫完全滿足全部所給的限制。然而，此方法沒
有提供完整和良好的隱藏。他只有控制非敏感項目集的支持度數量跟原資料庫中
的一模一樣，但出現頻率並沒有滿足原來的限制。針對此議題，現在最重要的問
題是要如何找出可行的支持度集並且對映到適合的資料集。 
 此外，多重支持度門檻值的概念在[13]第一次被提出。由於觀察到的現象，
發現不同項目集的支持度在實際情況下不會完全相同。我們使用一些已有的多重
支持度門檻值的規範來當作衡量我們演算法的基準，會在下一章介紹。 
四、研究方法 
淨除演算法 
 
圖表 1：淨除演算法架構 
我們的淨除演算法架構如圖表 1所示，主要包含了三個元件：敏感項目集表
5 
 
是刪除有包含{2, 3}的交易資料中的{3}來隱藏{2, 3}這個敏感項目集。SPC 是代
表可被此樣式所淨除的敏感項目集數量。舉例來說，像在表格 1(b)中的 TP3 可以
同時隱藏兩個敏感項目集{2, 3}、{3}，所以此樣式的 SPC 為 2。接著 MC 代表
當要隱藏此樣式所對應的敏感項目集時，所需要降低支持度的最少數量。因此，
樣式的 MC 為所有此樣式所對應的敏感項目集中的最大修改數量。例如在表格
1(b)中，TP3 的 MC 為 max{3, 5}=5。 
不只是以上所介紹的樣式，另外我們藉由產生接合樣式(joint template)來包
含更多的敏感項目集。若任兩個樣式，它們擁有相同的移除項目且各自的 UCP
沒有互相包含的話，我們可以將其接合起來成為一個新的樣式。而此新樣式的
SPC 及 MC 則會依據對應的敏感項目集計算得出。如表格 1 所示，TP3 和 TP5 可
以結合產生 TP7，其 UCP 為{1, 2, 3 ,4}是{2, 3}與{1, 3, 4}的聯集。而 TP7 的 SPC
將會是 3，因為從包含{1, 2, 3, 4}的交易資料中移除{3}這個項目可以同時讓 3 個
敏感項目集的支持度下降，分別是{3}、{2, 3}及{1, 3, 4}。而 TP7 的 MC 為 max{3, 
5, 6} = 6。因此，TP7 成為比 TP3、TP5 還要好的選擇，因為 TP7 的 SPC 大於 TP3
和 TP5 能夠同時隱藏更多的敏感項目集。 
選擇策略及更新步驟 
我們隱藏策略的本質為「使副作用達到最小」，我們在每一回合選擇 SPC 為
最大的樣式，如果有超過一個以上的樣式擁有相同的 SPC，就會去選擇擁有最小
MC 的樣式。若仍然還有超過一個以上有相同的 MC，那就選擇一個其移除項目
的支持度為所有資料庫中最小的樣式，最後若上述的所有情況無法解決的話就使
用亂數的機制來挑選。在選完樣式之後，藉由一開始所建立的交易資料索引找出
所有相對應的敏感交易資料，如果找出的敏感交易資料數量大於所選樣式 MC，
就選擇前 MC 個較短的交易資料移至淨除動作表中進行淨除。反之所有相對應的
交易資料將會被淨除。接著根據所淨除的交易資料數量，更新 Count，MC 會被
重新計算。若有些敏感項目集被此樣式給隱藏起來，則 SPC 和 UCP 也會改變。
當一個樣式的 SPC 變成零的時候，我們從樣式表裡移除此樣式。 
為了要提升演算法的效能，我們提出修改邊緣項目集(border itemests)的概
念，以減少不必要的工作。由於頻繁項目集的單調特性，隱藏一個敏感項目集也
會同時隱藏其擴展項目集(superset itemsets)，因此在淨除過程中，我們僅僅只需
刪除沒有敏感子項目集(subset itemsets)的敏感項目集。這樣的一個項目集也被稱
為邊緣項目集。例如，如果{1, 2, 3}、{1, 3}和{1}為三個敏感項目集，其中{1}就
為邊緣項目集。只要敏感項目集中的邊緣項目集都被隱藏，我們就能保護所有的
敏感項目集的資訊。然而這樣的技術不能應用在隱藏敏感項目集在多重最小門檻
值的情況下，當一個敏感項目集的擴展集所設定的門檻值小於他自己的門檻值的
話，只隱藏邊緣項目集並不能保證能保護所有的敏感頻繁項目集。以上面的例子
來看，如果最小門檻值對應到{1, 2, 3}、{1, 3}和{1}，分別為 2、4 和 3。那麼{1}
7 
 
 我們採用兩種衡量指標，資訊遺失(information loss, IL)和隱藏失敗(hiding 
failure)來評估我們刪除策略的效能。資訊遺失(IL)是指在淨除過程中被隱藏的非
敏感項目集所占的百分比，如同下列式子所示： 
|)||(|
|))||(||)||((|
s
ss
PFP
PPFPFP
IL −
′−′−−=  
其中 || FP 為在原資料庫 D 中頻繁項目集的數量， || sP 為在原資料庫 D 中敏感項
目集的數量。另外 || PF ′ 和 || sP′ 分別為在新資料庫 D’中頻繁項目集的數量以及在
新資料庫 D’中敏感項目集的數量。隱藏失敗(HF)是仍然在新資料庫 D’出現的敏
感項目集所占的百分比，以下列方式表示： 
||
||
s
s
P
P
HF
′=  
在單一最小門檻值的情況下， sP 包含任何一個敏感項目集的擴展集。然而在多重
最小門檻值的情況下，一個頻繁項目的子項目集不一定是高頻的。 
 首先，我們在單一最小門檻值的情況下，同時評估我們的演算法以及 IGA
效能。我們分別設定透露門檻值為 0.2 及 0.002 來隱藏在 accidents 及 kosarak 資
料集中的敏感項目集。敏感項目集是由高頻項目集中隨機挑選出來，挑選出來的
項目集其支持度分別大於 20%及 0.2%，隨後我們對新的資料庫分別以 10% 到
90%及 0.1% 到 0.28%的支持度門檻值探勘高頻項目集。資訊遺失的結果如圖表
2 所示。資訊遺失的趨勢與測試的資料集特性有很高的關聯性。我們可以觀察到
我們的方法有達到較好的資料保護。大部份隱藏失敗都為零，除了將 accidents
的最小支持度門檻值設在 10%時，IGA 的隱藏失敗為 0.0057%而我們的方法則為
0.325%。 
 
 
圖表 2：資訊遺失與透露門檻值 
我們在相同的資料庫以及相同的敏感項目集下與 IGA 比較執行時間以及可
擴展性，。且兩個方法的透露門檻值參數α都設為零，當此參數設為零時，代表
要完全隱藏。我們隱藏 6 個長度為 2~6 且彼此互斥的敏感項目集，同時也改變
資料集的大小由 10 萬筆資料到 90 萬筆。結果如圖表 3(a)所示。接著我們再改變
所隱藏的敏感項目集數量由 1 個敏感項目集到 10 個敏感項目集，項目集都是隨
機挑選的。其結果如果圖表 3(b)所示。我們可以觀察到執行時間與資料庫的資料
筆數以及敏感項目集數量呈線性關系。可得知我們的方法如同 IGA 達到了很好
的可擴展性。並且實現了更好的資訊保護及提供了能夠隱藏敏感項目集在多重最
9 
 
Exchange Workshop, pp. 45–52 (1999) 
[5] Verykios, V.S., Elmagarmid, A., Bertino, E., Saygin, Y., Dasseni, E.: Association 
Rule Hiding. IEEE Transactions on Knowledge and Data Engineering 16(4), 
434–447 (2004) 
[6] Xingzhi, S., Yu, P.S.:  A Border-Based Approach for Hiding Sensitive Frequent 
Itemsets. In: Proc. of 5th IEEE Int. Conf. on Data Mining, pp. 426–433 (2005) 
[7] Wu, Y.H., Chiang, C.M., Chen, A.L.P.: Hiding Sensitive Association Rules with 
Limited Side Effects.  IEEE Transactions on Knowledge and Data Engineering 
19(1), 29–42 (2007) 
[8]Gkoulalas-Divanis, A., Verykios, V.S.: An Integer Programming Approach for 
Frequent Itemset Hiding. In: Proc. of Int. Conf. on Information and Knowledge 
Management, pp. 748–757 (2006) 
[9] Mielikainen, T.: On Inverse Frequent Set Mining. Proc. of the 2nd IEEE ICDM 
Workshop on Privacy Preserving Data Mining (2003) 
[10] Wu, X., Wu, Y., Wang, Y., Li, Y.: Privacy-Aware Market Basket Data Set 
Generation: A Feasible Approach for Inverse Frequent Set Mining. Proc. 5th 
SIAM Int. Conf. on Data Mining (2005) 
[11] Chen, X., Orlowska, M., Li, X.: A New Framework of Privacy Preserving Data 
Sharing. Proc. of IEEE 4th Int. Workshop on Privacy and Security Aspects of Data 
Mining (2004) 47–56 
[12]Guo, Y.: Reconstruction-Based Association Rule Hiding. Proc. of SIGMOD 2007 
Ph.D. Workshop on Innovative Database Research (2007) 
[13] Liu, B., Hsu, W., Ma, Y.: Mining Association Rules with Multiple Minimum 
Supports. Proc. of the 5th ACM SIGKDD Int. Conf. on Knowledge Discovery and 
Data Mining (1999) 337–341 
[14]Wang, K., He, Y., Han, J.: Pushing Support Constraints into Association Rules 
Mining. IEEE Transactions on Knowledge and Data Engineering 15(3) (2003) 
642–658 
[15] Lee, Y.C., Hong, T.P., Lin, W.Y.: Mining Association Rules with Multiple 
Minimum Supports Using Maximum Constraints. Int. Journal of Approximate 
Reasoning on Data Mining and Granular Computing 40(1–2) (2005) 44–54 
[16] Blake, C.L., Merz, C.J.:UCIRepository of machine learning databases 
[http://www.ics.uci.edu/  mlearn/MLRepository.html]. Irvine, CA:University of 
附件一
Hiding Frequent Patterns under Multiple
Sensitive Thresholds
Ya-Ping Kuo, Pai-Yu Lin, and Bi-Ru Dai
Department of Computer Science and Information Engineering, National Taiwan
University of Science and Technology, Taipei, Taiwan. R.O.C.
{m9515063,m9615082}@mail.ntust.edu.tw, brdai@csie.ntust.edu.tw
Abstract. Frequent pattern mining is a popular topic in data mining.
With the advance of this technique, privacy issues attract more and more
attention in recent years. In this field, previous works based hiding sensi-
tive information on a uniform support threshold or a disclosure threshold.
However, in practical applications, we probably need to apply different
support thresholds to different itemsets for reflecting their significance.
In this paper, we propose a new hiding strategy to protect sensitive
frequent patterns with multiple sensitive thresholds. Based on different
sensitive thresholds, the sanitized dataset is able to highly fulfill user re-
quirements in real applications, while preserving more information of the
original dataset. Empirical studies show that our approach can protect
sensitive knowledge well not only under multiple thresholds, but also un-
der a uniform threshold. Moreover, the quality of the sanitized dataset
can be maintained.
Keywords: privacy, frequent pattern hiding, multiple threshold, sensi-
tive knowledge, security, data sanitization.
1 Introduction
Frequent pattern and association rule mining play the important roles in data
mining [1]. By this technique, we can discover interesting but hidden information
from database. This technique has been applied to many application domains,
such as the analysis of market basket, medical management, stock, environment,
business, etc., and brings great advantages. However, most database owners are
unwilling to supply their datasets to analysis, since some sensitive information
or private commercial strategies are at the risk of being disclosed from the min-
ing result. Therefore, although many benefits can be provided by this technique,
it causes new threats to privacy and security. For above reason, the database
should be processed before releasing so that it can contain the most of original
non-sensitive knowledge and the least of sensitive information for the owner.
Intuitively, the database owner can permit only partial access of dataset for
analysis or directly remove all sensitive information from the mining result of
database. However, it is possible that the adversary still can infer sensitive item-
sets or high-level items from non-sensitive patterns or low-level items. For exam-
ple, suppose that {1} is the sensitive pattern and the set of all frequent patterns
31.2 Contributions
In this paper, we propose a new strategy, which combines the sanitization algo-
rithm and the concept of multiple support thresholds [14], to solve the problem
which is mentioned above. Before sanitizing, the database owner can specify the
support threshold, called sensitive threshold, for each sensitive pattern based on
his/her domain knowledge. Then our algorithm will decrease the support of each
sensitive pattern to be below its sensitive threshold, respectively. Under multi-
ple thresholds, the database owner can directly decide the sanitization degree
of different patterns hence the protected database can much more satisfy the
demand of the database owner. Consequently, the proposed strategy is able to
reduce the probability of privacy breach and preserve as much information as
possible.
The main contributions of this paper are as follows: (1) a new hiding strat-
egy with multiple sensitive thresholds, which is more applicable in reality, is
suggested; (2) the proposed algorithm can achieve better privacy protection and
information preservation; (3) The new metrics are presented to measure perfor-
mance for hiding frequent patterns under multiple sensitive thresholds because
of the difference between under multiple thresholds and a uniform one, while the
sets of the patterns which need to be hidden by user-predefined are the same.
The rest of this paper is organized as follows. The preliminary knowledge is
stated in Section 2. In Section 3, we introduce our sanitizaion framework, the
whole sanitization process, and some techniques which improve performance and
efficiency. Some related hiding algorithms are reviewed in Section 4. The new
metrics, experimental results and discussion are presented in Section 5. In the
last part, Section 6 presents our conclusions.
2 Preliminaries
Before presenting our hiding strategy and framework, we introduce the prelim-
inaries of frequent patterns, the transaction database, and the related concepts
of privacy and multiple thresholds briefly.
Frequent pattern and Transaction database. Let I = {1, . . . , n} be a
non-empty set of items. Each non-empty subset X ⊆ I is called a pattern or an
itemset. A transaction is a pair of itemset t ⊆ I with a unique identifier Ti, called
the transaction identifier or TID. A transaction database D = {T1, . . . ,TN} is
a set of transactions, and its size is |D| = N . We assume that the itemsets
and the transactions are ordered in lexicographic order. A transaction t sup-
ports X, if X ⊆ t. Given a database D, the support of an itemset X, denoted
sup(X), is the number of transactions that support X in D. The frequency of X
is sup(X)/|D|. An itemset X is said to be frequent if sup(X)/|D| is larger than
the user-predefined minimum support, denoted as minsup, 0 ≤ minsup ≤ 1.
Sensitive itemset, sensitive threshold, and sensitive transaction. Let
D be a transaction database, FP be a set of frequent patterns, and {sp1, . . . , spi}
∈ SP be a set of patterns that need to be hidden based on some security require-
ments. A set of frequent patterns which are able to infer any patterns in SP,
5Sensitive
Pattern
Transaction
Index
Original
Database
New
Database
Template
Table
Action
Table
Threshold
Sensitive
Calculate
Inverted file
Mining
Build/
Update
Retrieve
Modify & Output
Fig. 1: The sanitization framework
In this section, our framework and the whole sanitization process of hiding
frequent patterns are presented. We propose a template-based framework which
is similar as [5] but different strategy on choosing an optimal hiding action
with minimal side effect. The proposed method hides the sensitive itemset by
decreasing its support. We apply the template to evaluate the impact of choosing
different items to be victims and different hiding order of sensitive itemsets.
In order to reach minimum side effect, we would like to choose the optimal
modification of a template which can hide most sensitive itemsets and sanitize
least sensitive transactions at the same time. In addition to promote efficiency,
we suggest a revised border-based method to reduce the redundant work on
hiding and rely on the inverted file and pattern index to speed up the renovation
of each component in our sanitization process. The summarization of notations
used in this paper is shown as Table 1.
3.1 The Sanitization Framework
The framework of our sanitization process is illustrated as Fig. 1. It mainly
consists of three components: sensitive pattern table, template table, and ac-
tion table. At first, the database is scanned to find all supports and sensitive
transactions of sensitive itemsets, and then the sensitive pattern table is built
that stores the number of supports should be decreased based on the sensitive
threshold for each sensitive itemsets. Secondly, we generate the corresponding
templates for each sensitive itemset that contains all probable choices of vic-
tim items for hiding this itemset. Next, a template is selected from template
table according to the hiding strategy of minimizing side effects for the original
database. Then we search out the corresponding sensitive transactions enough
to be modified for hiding all sensitive patterns covered by this template and
then put all pairs,(victimitem, TID), to action table. Then, the information of
all components is updated. The choosing and updating process will repeat until
all sensitive itemsets are hidden. Finally, we remove each victim item from its
pair transaction in the action table. Note that the whole sanitization framework
only needs to scan the database twice.
7Table 2, TP1 for hiding {3} is to delete {3} from the transactions containing {3};
TP3 for hiding {2, 3} and {3} is to delete {3} from the transaction containing
{2, 3}, etc. The SPC, stands for the number of the sensitive patterns which
can be sanitized by this template. For example, the TP3 in Table 2 can hide
two sensitive patterns {3} and {2, 3} at the same time, so its SPC is 2. The
MC indicates the minimal number of the support should be decreased, such
that all corresponding sensitive patterns of this template are hidden. Hence the
MC is the maximum Count among all corresponding sensitive patterns of this
template. For instance, the MC of TP3 in Table 2 is max{3, 5} = 5.
Not only are those templates introduced above, but also we generate joint
templates to cover more sensitive patterns. If any two templates have the same
victim, and their UCP do not contain each other, we can join them to be a
new template. The UCP of the new joint template is the union of the UCPs
of all combined templates. Then the SPC and the MC are computed according
all corresponding sensitive patterns. As shown in Table 2, TP3 and TP5 can be
combined to generate TP7. The UCP , the union of {2, 3} and {1, 3, 4}, is {1,
2, 3, 4}. Then the SPC of TP7 will be 3 because removing {3} from transaction
containing {1, 2, 3, 4} can decrease the supports of three sensitive patterns, {3},
{2, 3}, and {1, 3, 4}. The MC of TP7 is max{3, 5, 6} = 6. Consequently, TP7
becomes a better choice than TP3 and TP5 because the SPC in it is larger than
the others, thus TP7 can hide more patterns at the same time. We use the hash
table to avoid generating the same template with existing ones, and transfer the
pattern index from binary to decimal to be the hash key. We can compute the
SPC and the MC of all templates refer to the sensitive pattern table, and the
computation algorithm is similar in [5].
3.4 Choosing Strategy and Updating Process
Based on the essence of our hiding strategies - “minimizing side effect”, we
choose the template having the largest SPC at each round. If there exists more
than one template having the same SPC, the template with the smallest MC
is selected. If there still exists more than one, we choose the template which has
the victim with the lowest support in the database. Finally, if the tie is still
not solved, a random choice will be picked. After choosing the template, the
corresponding sensitive transactions are found out by the transaction index. If
the number of the sensitive transactions is larger than the MC of the chosen
template, we choose the first MC shortest transactions to move to action table
for sanitizing; otherwise all corresponding sensitive transactions will be sanitized.
By the number of sanitized transactions, the Count and theMC are recomputed.
If some patterns are hidden by this template, the SPC and the UCP should
be changed. As the SPC of a template becomes zero, we remove this template
from template table. Lastly, the TID of sanitized transactions are removed from
the transaction index of the corresponding sensitive patterns of this template.
In order to achieve better hiding performance, the number of the victim items
in one transaction is not restricted.
9an algorithm which can be secure against forward inference attack. By multi-
plying the original database matrix and a sanitization one together, the method
raises more efficiency. Most of recent works focus on the minimal side effects on
database [5][10]. These methods minimize the number of sanitized transactions
or items to limit the side effects on database respectively.
Data reconstruction. The motivation of this group is that the previous
database-based modification methods spend more time on scanning the database
and we can not directly control the information on the released database. Hence
the data-reconstruction group uses knowledge-based method to directly recon-
struct the released dataset containing the knowledge that the database owner
wants to preserve. In general, the set of frequent patterns in original dataset is
regarded as knowledge. The concept of “inverse frequent set mining problem”
was first proposed in [11], and was proved to be an NP-hard problem. Con-
sequently, most researches apply this concept on privacy issues and algorithm
benchmarks [12]. In [3], the authors proposed a constraint inverse itemset lat-
tice mining technique to automatically generate a sample dataset which can be
released for sharing. It indicates that if there exists a feasible support set of
all itemsets, they can generate the new database containing the same frequent
itemsets by one-to-one mapping. In [13], the authors proposed the FP-tree-based
method for inverse frequent set mining, and the new database exactly satisfies
the whole given constraints. However, this method does not provide complete
and well hiding. It only controls the support counts of the non-sensitive patterns
to be the same as before, but the frequencies do not satisfy the original con-
straint. For this issue, the major problem at present is how to find the feasible
support set which has compatible dataset.
In addition, the concept of multiple support thresholds is first proposed in [14]
owing to the observed phenomenon that support thresholds of different itemsets
are not always uniform in the reality. We take some of existing specification of
multiple support thresholds to be the benchmark of our sanitization framework,
and they will be introduced in next section.
5 Experiments
In this section, the performance, efficiency and scalability of our proposed saniti-
zation process are shown. We use the support constraint [18] and the maximum
constraint [8] to assign different sensitive threshold to each sensitive pattern. In
addition, in order to compare our sanitization process with the Item Grouping
Algorithm [20], IGA for short, the same disclosure threshold of IGA is used
by our framework to show performance for the situation of uniform sensitive
threshold. These constraints are described as follow:
Disclosure thresholds: It is devised to compare our method with IGA
under uniform support thresholds. Therefore, the sensitive thresholds are set to
be the same with IGA as follows:
st(X) = sup(X)× α
11
(a) accident dataset (b) kosarak dataset
Fig. 2: IL with disclosure thresholds
Fig. 3: IL with support con-
straints on chess dataset
trans. Items
Accidents 340,184 572
Kosarak 990,002 41,270
Chess 3,196 75
mushroom 8,124 119
Table 3: The characteristics
of each dataset
non-sensitive patterns which are hidden in the sanitization process, as shown in
the following equation:
IL =
((|FP | − |Ps|)− (|FP ′| − |P ′s|))
(|FP | − |Ps|)
where |FP | and |Ps| are the number of frequent patterns in original database,
D, and the number of sensitive patterns in D, respectively, and |FP ′| and |P ′s|
are the number of frequent patterns in new database, D′, and the number of
sensitive patterns in D′, respectively. Hiding failure (HF ) is the percentage of
sensitive patterns remaining in D′ after sanitization, represented as follows:
HF =
|P ′s|
|Ps|
Under the uniform support threshold, Ps contains all the supersets of any pat-
terns in SP. However under the multiple support thresholds, the subset of a
frequent pattern may not be frequent. Hence Ps should only contain the super-
sets which have larger sensitive thresholds than those of its sensitive subsets of
any pattern in SP.
5.2 Performance
Firstly, our framework is compared with IGA to evaluate its performance under
the uniform support threshold. We use the disclosure thresholds 0.2 and 0.002 to
13
patterns of chess dataset are higher and more items are deleted to hide such
patterns, the IL is higher. All HFs are zero. We can observe that the IL will
increase along with the decrease of γ and the increase of the number of hidden
sensitive patterns.
5.3 Efficiency and Scalability
We estimate the efficiency and scalability of our method compared with IGA on
the size of the database and the number of the sensitive patterns. The disclosure
parameter of IGA is set to be zero, and the same is α of our method. The zero
value means hiding completely.
We vary the size of dataset from 100K to 900K on hiding six mutually ex-
clusive frequent patterns with length 2-7. The result is illustrated in Fig. 5(a).
Next, the number of the hidden sensitive patterns is varied from 1 pattern to
10 patterns. All the patterns are chosen randomly. The result is illustrated in
Fig. 5(b). We can observe that the execution time is linear with the size of the
database and the number of sensitive patterns. Note that our method achieves
good scalability as IGA while attaining better information preservation and pro-
viding additional capability of hiding with multiple sensitive thresholds.
6 Conclusions
In this paper, we introduce the concept of frequent pattern hiding under multi-
ple sensitive thresholds. A new hiding strategy of multiple sensitive thresholds
is proposed. The hiding strategy is more applicable in the practical applica-
tions. Considering the properties of the frequent patterns under multiple sen-
sitive thresholds, we suggest the revised border-based method to reduce the
redundant work on hiding, and used the inverted file and the pattern index to
speed up the update in our framework.
We empirically validated the performance, efficiency and scalability of our
method by using a series of experiments. In all of these experiments, we took
into account the uniform support threshold, multiple support thresholds with
support constraints, and multiple support thresholds under maximal support
constraints. The results of our experiments reveal that our method is effective
and achieves significant improvement over the IGA with the uniform support
thresholds. Furthermore, we can hide sensitive knowledge of the dataset with
multiple sensitive thresholds.
References
1. Agrawal, R., Imielin´ski, T., Swami, A.: Mining Associations Rule Between Sets of
Items in Massive Database. Proc. of the ACM SIGMOD Int. Conf. on Management
of Data. (1993) 207–216
2. Clifton, C., Marks, D.: Security and Privacy Implication of Data Mining. ACM
SIGMOD Workshop on Data Mining and Knowledge Discovery (1996) 15–19
可供推廣之研發成果資料表 
□ 可申請專利  □ 可技術移轉                                      日期： 年 
月 日 
國科會補助計畫 
計畫名稱：資料探勘之敏感資料保護技術研發 
計畫主持人： 戴碧如        
計畫編號：NSC  97-2221-E-011-092- 學門領域：人工智慧 
技術/創作名稱 Hiding Frequent Patterns under Multiple Sensitive Thresholds 
發明人/創作人 Ya-Ping Kuo, Pai-Yu Lin, and Bi-Ru Dai 
技術說明 
中文： 
根據敏感項目表裡所提供的資訊來產生樣式表(template table)，並
利用樣式表中的資訊以及接合樣式表的方式，選擇出能夠對資料庫
產生最小副作用的移除項目。 
英文： 
According to the information provided by Sensitive Pattern Table, a 
Template Table is generated. The information in Template Table and 
corresponding joint templates allow us to select victims, which have 
the minimum side effect to the database, to sanitize the datasets.  
可利用之產業 
及 
可開發之產品 
擁有交易資料之單位，及有與他人分享自身資料庫的對象，像是超
級市場、百貨公司和便利超商等。 
可將此方法實作在資料庫上，在公佈資料庫時應用此方法，來產生
具隱私保護的資料庫。 
技術特點 
由於大部份的敏感項目集淨除演算法都是將所有的敏感項目集降
低到單一的門檻值下，但這樣子情況並不適合運用在實際的應用
上。然而，此敏感項目集淨除演算法，能夠依照敏感項目集在資料
庫中的分布，將其各自的支持降低到所指定的不同門檻值底下。 
推廣及運用的價值 
此方法針對現實生活中所觀察到的現象，發現以往的淨除演算法，
使用單一門檻值並不夠實際，為了解決這樣的問題。提出一個符合
實際應用需求的演算法並使用不同的門檻值，且擁有較好的效能。
 
附件二 
