摘要
本計畫有三個主要的研究成果。第一個成果是中文輸入法的研究，特別是基於混
淆度對於簡化中文輸入法作可行性評估。第二個成果是中文文法的自動學習。因為我
們採用最小描述長度的方法，因此也達到文本資料庫的壓縮。值得一提的是這些研究
都用到中央研究院平衡語料庫。此外，在自動語音辨識部份，我們建立了一個以隱藏
式馬可夫模型為基礎的系統，用 TCC300 語料庫作訓練，作為音譯英文名字辨識的前
端系統。此系統之後端利用全球資訊網及搜尋引擎來提升辨識度。
關鍵詞:中文輸入法，自動文法學習，語音辨識，搜尋引擎
Abstract
There are three main research results in this project. The first is on the study of Chinese
input methods. We evaluate the feasibility of simplifying the input methods based on the
perplexity. The second is automatic learning of grammar rules of Chinese. As we use the
principle of minimum description length, an effect of text corpus compression is achieved.
Note that these researches use the Academia Sinica Balanced Corpus (ASBC). In addition,
in automatic speech recognition, we build a system based on the hidden Markov models,
which are trained by the TCC300 corpus. It is used as the front end for a spoken
transliterated name recognition system. The back end of this system uses the worldwide
web (WWW) and search engines to enhance the recognition results.
Keywords: Chinese input methods, context-free grammar, automatic speech recognition,
search engine
I
研究目的
The goal of this research is to investigate using simplified methods to use with mobile
devices. This has to deal with the limited size and computing power of such device. With
more powerful handsets and faster data communication speeds, mobile electronic devices
appear to be the converging points for new information technologies, looming to replace the
immobile counter-parts. However, for that to happen, the user interfaces on these devices do
need significant overhauls. Take the instant message (IM) service for example. Being used
to run on desktops and laptops, it is now running on the mobile phones since the advent of
3G wireless network. In order to input a text message, the users can only use the keypads
limited in size and the number of distinct keys. Since the set of potential text is large, this
constraint in size posts a severe challenge for a convenient and healthy interface.
From the perspective of source coding, we can view the Chinese input problem as
representing each Chinese sentence (source) by a codeword of input symbols. Ideally, a
source code has a high probability of being decodable and a low expected code length.
Here, in addition, we require that the number of code symbols (the size of the alphabet set)
should be as low as possible.
The scenario of our input scheme is as follows. When a user wants to input a sentence,
he inputs the sequence of first Mandarin phonetic symbols of the characters in the sentence.
Given the input sequence, the system outputs the most-likely candidate sentences for the
user to choose from.
Whether this is a feasible approach or not depends on the entropy of the text (source)
and the entropy of symbol sequence. It is certainly feasible if these entropies are similar in
magnitude. Otherwise, there will be many sentences (exponential in the input size) for given
input symbol sequence. If this is the case, the system must be able to search efficiently for
potential sentences and list the top candidates in the order of probability for the user to
choose.
We also study the problem of learning context-free grammar (CFG) from a corpus of
part-of-speech tags. The framework of CFG, although not complex enough to enclose all
human languages, is an approximation good enough for many purposes. For a natural
language, a decent CFG can derive most sentences in the language. Put differently, with
high probability, a sentence can be parsed by a parser based on the CFG. A set of
grammatical rules may be used in place of the n-gram language models. They may be small
in size and can be stored in a mobile device.
2
研究方法
We use bigram language models for the study of Chinese input methods. To estimate the
parameters in the bigram language model, we use a maximum-likelihood-based estimator
modified by smoothing and backing-off. The maximum-likelihood estimate (MLE) is
simply the relative frequency in the training set. To cope with bigrams unseen in the training
set, we use the add-one smoothing scheme. On top of smoothing, we also incorporate
backoff scheme into our bigram language model.
Once we have a language model P, we compute the perplexity of a test set and relate the
perplexity to the number of typical sequences in the domain of the test data. The numbers of
typical sequences are used to quantify the feasibility of three input modes: the first-symbol
based, syllable based and character based. The data we use are the standard ASBC and a
self-collected CHAT text sets.
In the automatic learning of context-free grammar, we use the minimum description
length criteria. A cost of specifying the grammatical rules and deriving the sentences in a
corpus (ASBC) is defined for a CFG.
We first analyze two special cases: the recursive CFG and the exhaustive CFG.. Both
cover the corpus in an apparent way, but the costs are quite different. With the cost function,
we then investigate the relationship between cost and the number of rules. We also use the
learned grammars to parse the original set of sentences and we have found some parse trees
similar to known grammatical structures.
4
