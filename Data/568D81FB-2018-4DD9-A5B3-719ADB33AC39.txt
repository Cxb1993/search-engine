 
 
 
1 
 
 
中文摘要 
 
傳統睡眠檢測使用多頻道睡眠生理檢測儀以接觸方式量測生理訊號，從而判斷睡眠狀態
與睡眠病理，其缺點為成本高昂且不便利。本報告提出以視覺為基礎之睡眠姿勢識別，以非
接觸方式量測睡眠體位，幫助睡眠狀態與睡眠病理的判斷 。本方法從夜間紅外線影像擷取特
徵點進行特徵點偵測，估測睡眠影像中睡眠者之關節點位置，再透過貝氏網路辨識身體姿勢。
為解決睡眠體位變化造成關節位置仿射轉換的變動，我們採取尺度不變特徵轉換演算法來完
成特徵點比對，得到睡眠者關節點位置之機率模型。由於關節點位置可能會有錯誤偵測並有
遮蔽情形，因此我們以貝氏網路辨認器推論各種睡眠姿勢發生機率。實驗結果證實本方法有
良好之準確度。 
 
關鍵詞：多頻道睡眠生理檢測儀、尺度不變特徵轉換、特徵點比對、睡眠姿勢識別、貝氏網
路辨認器 
  
 
 
3 
 
1. 簡介  
  
睡眠障礙(Sleep Disorder)在睡眠醫學中已成為最常見而重要的一種病徵。研究指出一天需
睡眠時間約 6 到 8 小時最適宜。睡眠時間佔一天約四分之一甚至更多的時間，因此睡眠的好
壞對於生理及心理方面有著極為重要的影響。醫學研究指出不良的睡眠品質對白天工作有著
負面的影響，例如專注力不佳、嗜睡等。睡眠障礙是發生於人類睡眠期間，進入無意識狀態
時所產生干擾睡眠的行為，無法由患者本身明確地自我檢測。睡眠障礙輕者影響睡眠品質，
嚴重者甚至威脅生命。然而醫生無法待在病患整夜睡眠期間，進行診斷評估病患睡眠障礙種
類及嚴重度。因此，睡眠檢測及治療已經變成愈來愈重要的議題。 
 
 為了達到真實記錄檢測者平時的睡眠情況，以供醫師做正確的診斷，一個理想的睡眠檢
測系統應詃具有非接觸、低干擾及高準確度等條件。現今臨床睡眠問題診使用之標準檢測系
統為多頻道睡眠生理檢查(Polysomnography, PSG) [1]，PSG 能記錄數十種睡眠期間產生的生理
訊號，例如：腦電圖(Electroencephalography,  EEG)、心電圖(Electrocardiography, ECG)、肌
電圖(Electromyography, EMG)和眼電圖(Electrooculography, EOG)、鼻氣流(airflow)、血氧飽和
濃度(SaO2)等等。醫生可透過分析病患睡眠期間發生睡眠障礙時產生的多種生理訊號反應，
進行睡眠障礙種類及嚴重度的診斷。因此 PSG 對於睡眠品質評估及睡眠障礙的診斷皆具有高
準確度。 
 
然而，PSG 也存在許多嚴重的問題，由於需量測的生理訊號極多，因此需在檢測者身體
黏貼大量的生理訊號感測器，造成高侵入性及高干擾性，往往無法真實呈現檢測者平常的睡
眠情況，而昂貴的檢測儀器及黏貼大量的生理訊號感測器需由專業的睡眠技師進行等問題，
進而無法達成居家化檢測，通常檢測者需前往特別設立之睡眠中心或醫院做檢測，並且分析
師必需待命在檢測者身邊以應付感測器脫落、失眠、上廁所等需求，檢測後高達整夜六小時
的數十種生理訊號資料龐大，需要許多儲存空間，且事後需由分析師人工方式進行睡眠狀態
標示，非常秏費人力成本，進而使 PSG 檢測費用居高不下。 
 
為了減化 PSG 訊號線數量，本報告提出「以視覺為基礎之睡眠姿勢辨識」，研究以紅外線
影像(infrared image)技術與電腦視覺技術來進行睡眠醫學分析。將夜間錄影之紅外線影像，透
過電腦視覺來分析受測者之身體動作與姿勢變化，以降低 PSG 量測生理訊號之個數，從而改
善睡眠分析記錄之方便性。本計畫內容為「紅外線影像分析技術之 Automatic Sleep Pose 
Recognition」，以視訊技術開發睡眠姿勢辨識技術，進行夜間紅外線影像分析，擷取人體關節
點特徵進而計算睡眠姿勢，並自製紅外線睡衣，可讓人體關節點在紅外線影像中得到明顯的
輪廓及對比資訊 。 
 
從紅外線影像中識別睡眠體位有著如下挑戰：紅外線影像近似灰階影像缺乏色彩資訊，
也無法偵測膚色。邊緣資訊也不明顯，無法準確估測人臉位置及擷取邊緣資訊。不易使用色
 
 
5 
 
睡眠障礙 。 
 
 
圖 1、系統目的與架構 
 
 本系統主要分成四大部分，分別為：人體特徵擷取 (HFE)、節點取樣估測 (JSI)、姿勢分
類器訓練 (PCT)、睡眠姿勢分類 (SPC)。系統可從病患睡眠期間擷取睡眠體位資訊，將識別
結果記錄成檔案。HFE 接收紅外線攝影機之串流影像，擷取關節點分佈資訊並傳送至 JSI，JSI
從關節點分佈資訊中估測出關節點位置，PCT 建立睡眠姿勢分類器，並以關節點位置做為訓
練資料，SPC 接收 JSI 之關節點位置 PCT 訓練後之睡眠姿勢分類器進行姿勢預測。 
 
SPR
HFE JSI SPC 睡眠姿勢時序圖
PCT
病患前景影像
 
圖 2、系統流程 
 
 
本報告共分 5 節，第 2 節說明以尺度不變特徵轉換方法為基礎的人體關節擷取方法，
第 3 節提出以貝氏網路辨認器進行人體睡眠姿勢辨識的方法，第 4 節介紹實驗資料及實驗結
果，第 5 節為結論。 
 
 
2. 人體關節偵測  
 
 
 
7 
 
 
圖 4、對於尺度空間的每一個八度層，產生出高斯差影像。 
 
 
 
圖 5、使用不同尺度高斯差影像找局部極值 
 
 
對於偵測到所有特徵點，做濾波剔除不穩定特徵點後，以周圍點計算特徵點周圍影像梯
度進行方向分配。以區塊為基礎統計特徵點周圍像素梯度方向組合成方向直方圖做為特徵描
述資訊。以式(2)將低對比的特徵點，視為不穩定極值點做剔除。 xˆ為取樣點 x 的極值。 
 
 1ˆ ˆ( )
2
TDD D ∂= +
∂
x x
x
 (2) 
 
2 1
2
ˆ D D
−∂ ∂
= −
∂ ∂
x
x x
 (3) 
 
將影像中偵測到所有特徵點以式(4)、(5)計算區域像素梯度，進行梯度方向分配。 ( , )m x y 為梯
度大小、 ( , )x yθ 為梯度方向、L 為高斯模糊影像。 
 
 
9 
 
2.2節點取樣估測找關節點分佈位置 
 
影像中偵測到的特徵點集合 X，與 建立之物件模型特徵點集合 Y 做比對，特徵比對演算
法使用 Best Bin First (BBF) search [17]找出對於物件影像與測試影像中，搜尋到最相似兩個特
徵點，使用 squared Euclidian distance [18]計算兩特徵點距離，做為特徵點之間的相似度指標
(Distance ratio)，並將距離小於臨界值 THDR 記錄為集合。將比對吻合的特徵點集合，以中值
濾波器估測關節點位置，可抵抗估測結果受到極值特徵點影響，如圖 8(a)左上角為特徵影像，
黃色格線將測試影像均分為 25 個區塊，紫色線條代表兩張影像比對成功的特徵點連接線，紫
色圓圈為估測之關節點位置及所在區塊。圖 8(b)為所有關節點估測結果，將所有估測到關節
點位置資訊定義為觀測值集合{X|x1,x2,..., x10}，從 observation 推論睡眠姿勢。 
 
   
(a) (b) 
圖 8、關節點估測結果(a) 單一關節點 (b) 全部關節點 
 
 
3. 貝氏模型推論睡眠姿勢 
 
發生不同睡眠姿勢時，產生的各關節點位置也不盡相同。因此當已知關節點位置，求未
知睡眠體位時，可由各關節點位置的機率分佈圖估測睡眠體位。姿勢分類器使用貝氏網路分
類器[19]中的 Naïve 貝氏模型 (Naïve Bayes model) 架構，將關節點做為已知狀態，睡眠體位
為未知狀態，以條件機率方法估測睡眠體位。本方法以兩級和 10 個節點來建立貝氏網路，第
一級為一個父節點，其節點狀態為睡眠姿勢種類：正躺 (supine)及側躺(lateral)；第二級共有 9
個子節點，對應紅外線睡衣上的關節點種類，將影像均分為 25 個區塊，其節點狀態為關節點
所在區塊位置。 
 
 
 
11 
 
本實驗從實驗影片中擷取 12 張不同姿勢變化之靜態影像進行分析。調整節點取樣估測方
法參數以找出準確率最高之參數組，並以混淆矩陣評估睡眠姿勢分類方法之可信度。 
 
本實驗收集五位個案如下表，使用 Mintron MTV-14G5DHE 近紅外線類比攝影機，搭配
LED 主動式紅外線光源投射器，穿著自行研發之被動式紅外線睡衣錄製實驗影片。實驗影片
共錄製 10 部，其內容為平躺與側躺兩種睡眠體位變化之過程，每種睡眠體位加上 3 種肢體。
每段實驗影片長度為 20 秒，拍攝之紅外線影像解析度為 640*480，frame rate 為 30。 
 
編號  身高(cm) 體重(kg) 年齡  BMI 
1 180 85 26 26.23 
2 175 74 26 24.16 
3 163 66 23 24.84 
4 160 50 21 19.53 
5 163 51 21 19.20 
MEAN 168.2 65.2 23.4 22.79 
STD 7.83 13.44 2.24 2.88 
 
 
4.1 節點取樣估測準確率  
 
對 20 張測試影像，以偵測到正確的關節點數量/總關節點數量做為準確率，調整 SIFT 參
數以找到準確率最高的最佳參數組。將關節點與測試影像中的特徵點進行比對，兩特徵點對
比度需大於臨界值 THDR 才視為特徵點比對成功。如 
 
而對於任一特徵點，其建立描述子時，鄰近像素的向量大小需超過臨界值 SCW 才進行建
立。將 THc 固定為 0.8 條件下，調 SCW 以得到最佳偵測率。如表 2 第一列為 SCW 三種變化值，
第一欄為測試影像編號，最後一列為平均正確率，由此表格可得到 SCW 為 1.5 時可得到最佳偵
測率。 
 
4.2 睡眠姿勢分類準確率  
 
 對 20 張測試影像，分別為 10 張側躺及 10 張正躺影像。以混淆矩陣分析本方法識別結果
之可靠度，並以人眼識別之睡眠姿勢為正確結果。可靠度以下列四種指標進行分析：positive 
predictive value(PPV)、negative predictive value(NPV)、sensitivity(TPR) 及 specificity(SPC)。PPV
代表本方法識別為正躺時的正確率，NPV 代表本方法識別為側躺時的正確率；TPR 代表
groundtruth 中正躺被本方法識別正確的機率，SPC 代表 groundtruth 中側躺被本方法識別正確
的機率。表 3 為正確率分析的實驗結果。 
 
 
 
13 
 
2 40 40 40 
3 30 30 30 
4 40 50 40 
5 30 70 70 
6 30 70 60 
7 40 60 50 
8 50 60 30 
9 40 50 60 
10 50 40 40 
11 40 60 30 
12 30 70 60 
平均值 39.17 54.17 47.5 
 
 
 
表 3、辨識結果 
 Groundtruth 
Supine Lateral 
Proposed 
method 
Supine 6 4 
Lateral 2 8 
 
 PPV NPV TPR SPC 
value 0.6 0.8 0.75 0.67 
 
 
5. 結論  
 
 本報告發展於紅外線影像中以影像處理方式識別睡眠人形姿態的方法。使用特徵點為基
礎方式以影像梯度資訊做特徵比對，估測關節點位置描述人形姿態，可有效在低對比、高雜
訊的紅外線影像中擷取人形姿態，解決睡眠人形物件不易切割之問題。睡眠人形姿態採用貝
氏網路架構之分類器，採用機率概念推論睡眠姿勢，可抵抗肢體變化之影響。但仍受限制於
攝影機架設位置、高度與拍攝視角需與訓練影像維持一致，未來可發展適應於各種拍攝位置、
視角之演算法，以提高走入居家化之便利性。 
 
 
 
 
15 
 
International journal of computer vision, vol. 90, pp. 313-330, Num. 2010. 
[16] D. G. Lowe, "Distinctive image features from scale-invariant keypoints," International 
Journal of Computer Vision, vol. 60, pp. 91-110, Nov. 2004. 
[17] J. S. Beis and D. G. Lowe, "Shape indexing using approximate nearest-neighbor search in 
high-dimensional spaces," in Conference on Computer Vision and Pattern Recognition, 1997, 
p. 1000. 
[18] M. M. Deza and E. Deza, Encyclopedia of Distances, 2009. 
[19] D. Heckerman, "A Tutorial on Learning With Bayesian Networks," pp. 301-354, Mar. 1995. 
[20] C. Huang and A. Darwiche, "Inference in belief networks: A procedural guide," 
International Journal of Approximate Reasoning, vol. 15, pp. 225-263, 1996. 
 
 
表 Y04 
間的交流來激發出新的想法。 
 
本次論文報告題目為 A Large Scale Video Surveillance System with Heterogeneous 
Information Fusion and Visualization for Wide Area Monitoring，該議程有許多位外國學者與
會聆聽。由於報告之主題為一個經濟部學界科專計畫中的系統整合成果，實際建立的一個
適合廣域監控，大型並且新穎視訊監控系統，是一個難得的經驗，因此與會者聆聽專注，
事後且發問踴躍。 
 
另外該會議三天議程都安排有專題演講。第一天早晨邀請美國 Wright State 
University 的 Nikolaos Bourbakis 教授演講，Bourbakis 教授為 IEEE Fellow，講題為
Converting Images into NL Text for Multimedia Retrieval，這是一個跨領域的研究，
題材相當新穎而有趣，將影像訊號處理與人工智慧自然語言處理兩個領域的研究相結合，
產生一個新的應用研究。第一天下午邀請美國 University of Oklahoma 的 Stamatios 
Kartalopoulos 教授演講，該教授為 IEEE 終身院士，講題為: Self-Defending Optical 
Networks: Optical Channel Protection and Countermeasures。第二天早上邀請美國加
州大學的Demetri Terzopoulos教授演講, 該教授亦為IEEE Fellow，講題為Virtual Vision: 
Computer Vision in Virtual Reality。這是這幾年益發流行的主流研究之一，又稱之為
擴增實境(Augmented Reality)，演講中給了許多實際的研究成果，展示了許多前瞻的應用
想法。第三天則邀請英國 University College London 的 George Pavlou 教授，演講主題
為 Information-Centric Networking: Overview, Current State and Key Challenges。 
 
該會議特色為有許多跨領域的研究成果，並且於每日都安排有 1～2個主題式演講，讓
與會者能瞭解近來相關領域的最新研究現況，聆聽這些演講以及其他的論文報告，皆獲益
良多。最後，會議在 7/20 日下午結束，也結束此次的學術探訪之旅。 
 
二、與會心得 
 
在 IIHMSP 會議論文報告完畢後，我們也與在場學者進行討論，得到了很好的迴響。
本年度所出席之國際會議涵蓋了資訊隱藏、訊號處理與生物資訊的多項學術領域，更能藉
由了解其他領域的研究技巧來激發出不同的創意構想。 
在此特別感謝國科會計畫之經費補助，讓本人能參與此類國際研討，發表研究成果並
獲得更多新知，並得到非常豐碩之收穫，對未來之研究計畫構想與執行亦有很大之助益。
能與歐洲等各國學者交流研究心得，可獲得各種新穎的研究想法，收穫甚豐 。 
 
 
 
A Large Scale Video Surveillance System with Heterogeneous Information Fusion 
and Visualization for Wide Area Monitoring 
 
Yuan-Kai Wang 
Department of Electronic Engineering 
Fu Jen Catholic University, Taiwan 
E-mail: ykwang@mails.fju.edu.tw 
 
Ching-Tang Fan 
Graduate Institute of Applied Science 
and Engineering 
Fu Jen Catholic University, Taiwan 
Email: bevis@islab.tw 
 
 
Cai-Ren Huang 
Graduate Institute of Applied Science 
and Engineering 
Fu Jen Catholic University, Taiwan 
Email: baboo@islab.tw
 
Abstract—Wide area monitoring for community and city can 
be a very challenging engineering task due to its scale and 
heterogeneity in sensor, algorithm, and visualization levels. 
Multi-modal cameras and algorithms have to be fused into 
compact presentation for a single operator to actively and 
effectively respond to anomaly events and jeopardy. This 
paper presents a distributed and scalable video surveillance 
system, subcontracted by intelligent surveillance components 
(ISCs) and visualization surveillance components (VSCs) in 
compliance with functional labors. The ISCs are high-level 
algorithms applying computer vision for behavioral analysis of 
human and vehicles. The VSCs constitute a multi-tier 
subsystem to visualize fused results of messages, key frames, 
streaming videos and geographic context information. The 
system helps the operator to focus attention on interested 
events gathered from distrusted ISCs and presented by VSCs 
on map and three-dimensional homographic views. Robustness 
and effectiveness of the system has been demonstrated by a test 
run of real scenarios deployed in a campus. 
Keywords-visual surveillance; third-generation surveillance 
system (3GSS); visualization; information fusion* 
I.  INTRODUCTION 
Intelligent video surveillance has evolved to the third-
generation surveillance system (3GSS)[1] with large-scale 
architecture and distributed fashion for multi-modal camera 
network. Safety and security surveillance, which usually 
installs a huge number of cameras with geographical 
diversity, can be the most critical application of 3GSS. 
However, heterogeneity in sensors and algorithms produces 
diversified information including event messages, key 
frames and recorded videos, which complicate the 
development of the system for system-level considerations 
in hardware architecture and user interface [2]. Another 
impediment to the success of 3GSS is the robustness of 
algorithms [3] because the system is confronted with more 
difficulties from dynamic environmental challenges such as 
illumination variations, weather conditions and unexpected 
occlusion and crowding. 
An extensible framework S3 [4] was proposed to 
integrate diverse image understanding techniques into a 
large-scale deployable system. The S3 adopts a plug-ins 
                                                          
* This work was supported financially by the Ministry of Economic Affairs, Taiwan under project 
no. 100-EC-17-A-02-S1-032 through Technology Development Program for Academia. 
scheme in vertical hierarchy to enable the integration of 
independently developed video analytics. Unfortunately, 
plug-ins can not cooperate with each other due to the lack of 
horizontal communication mechanism. The extensibility 
also makes it difficult to fuse plug-ins' information into one 
user interface, because each plug-in owns its web page. The 
burden of the operator is dramatically increased when the 
system scales up with more plug-ins. 
DOTS [5] had a simplified goal to track people of 
interest at indoor. Its effective user interface enabling 
security personal to monitor views of interest is an 
improvement feature. It has a floor plan map for the 
operator to navigate the 3D scene model of the building and 
track persons. Although it shows the importance of 
interactive visualization for next generation visual 
surveillance [6], it is a pity that lack of multiplicity in 
camera sensors and algorithms makes it incomplete to be a 
3GSS system. 
Based on these premises, we believe that a more 
advanced large-scale system should play the role of active 
assistance to help security work, instead of a new 
mechanism to replace humans. In addition to video 
understanding for immediate safety alert and retrieval issues, 
a well-designed visualization scheme is also critical to help 
the operator semi-automatically filter alerts, focus attention 
on solidating evidence of the event, and make security 
decision. In this paper we address a scalable 3GSS system 
for wide area monitoring. The system is outdoor, real-time, 
built with a cloud infrastructure. A four-tier visualization 
subsystem, integrating text, image, streaming video and 
geographic context information, is proposed. 
The rest of the paper is organized as follows. Section 2 
gives an architectural overview of our system. Section 3 
introduces the video understanding algorithms in the system, 
and the details of the information fusion and visualization 
technique are given in Section 4. Test-run experiences are 
discussed in Section 5. Finally, we summarize the paper in 
Section 6. 
II. SYSTEM OVERVIEW 
As a large-scale video surveillance system, our system 
has complex software and hardware architectures. An 
architectural overview is given in Fig. 1. The main 
constituents of this system are: video understanding 
2012 Eighth International Conference on Intelligent Information Hiding and Multimedia Signal Processing
978-0-7695-4712-1/12 $26.00 © 2012 IEEE
DOI 10.1109/IIH-MSP.2012.49
178
into a short video representation, since video browsing and 
retrieval is time consuming. A synopsis is generated at every 
time slot of endless video streams and indexed into the 
original time of each object is also indexed. 
 
IV. INFORMATION FUSION 
Matrix arrangement of video displays has been widely 
installed in traditional control room. The one-camera-one-
monitor way introduces difficulties for event detection and 
tracking in a complex monitoring environment. We propose 
a comprehensive solution of visual representation in a four-
tier fusion scheme by web service, which involves VSCs to 
assist the presentation at back-end. It helps the operator to 
navigate the monitored space and focus on situated 
awareness. All the essential information is centralized in a 
web-based user interface by the CMS, which can also be 
adopted on portable devices such as smart phones. The 
CMS is not only a middleware as previous section described, 
but also the most critical VSC in the full system. 
The web-based visualization as part of the CMS includes 
two standard views shown in Fig. 2. The two views 
correspond to two modes of the CMS: live mode and 
playback mode. The live mode presents all real-time 
information, and playback mode displays event-based 
recorded videos.  
There are four tiers to complete the fusion and 
visualization of all surveillance information in Fig. 2. Each 
tier subsequently digests diverse information into high-level 
message for next tier. The first tier includes digital video 
processing to acquire and enhance streaming videos. Tier-2 
feeds on enhanced videos, adopts ISCs and VSCs to analyze 
the videos, and produces symbolic texts when events happen. 
The text is the basic element in both two modes. It is not 
only employed for real-time alerts, but also revealed to all 
the operations on the user interface.  
Geoinformation fusion acts as tier-3. The geographic 
metadata contains 2D and 3D information. The dynamic 
messages which are mentioned before are blended with the 
2D map in this tier. On the other hand, the source of 
streaming video and the clicked camera maker will change 
conjointly, when a camera on its geographical position is 
manually selected. In accordance with the relation of 
cameras, we organize cooperated cameras into one group, 
and each group has two windows to play streaming videos. 
 
 
(a)                                                       (b) 
Figure 2. The CMS website with (a) live mode, and (b) playback mode, 
where T1, T2, T3, and T4 denote tier-1, tier-2, tier-3, and tier-4, 
respectively. 
  
                     (a)                                                 (b) 
Figure 3. The smart security centre of (a) panoramic view and (b) frontal 
view. All heterogeneous information are presented by the five monitors 
including CMS web service interface, multi-resolution 3D vision, parking 
space interface, synoptic video, and single-camera video. 
 
When an alert is issued by an ISC, the alert ID, key frames, 
time stamp, geographic position and action are composed 
for a fused presentation. The event video will transfer to the 
second window, and a cooperated camera will be 
automatically selected to substitute to the main window. 
The tier-4 is the aggregator which fuses all the 
information. The event database is also connected to this tier. 
An event-based retrieval is furnished with playback mode. 
The unconfirmed events will be highlighted with their 
cameras on the 2D map. Event investigation is the process 
to inspect further details of an event, and needs intelligent 
video retrieval. Object-based retrieval is connected to the 
2D map in the CMS’s playback mode. We also mark the 
geographical regions of 3D real-time information into the 
2D map. 
The four-tier visualization scheme composing of 
comprehensive VSCs functions is employed to build a smart 
security centre, that false alarms could be filtered 
immediately by human capabilities. Illustrations of the 
smart security centre are shown in Fig. 3. 
V. TEST RUN AND EXPERIENCES 
We have developed and installed the system at a 
campus in Taiwan under the Vision Based Intelligent 
Environment (VBIE) project. More than twenty cameras 
were setup within two critical regions and cooperated as two 
camera cluters. Geographic content is relied on Google map, 
and the 3D visualization is established on Google earth with 
our 3D models. We encode all storage videos into high-
quality H.264 format and stream them by HTML5. 
The system has been tested by operating more than two 
weeks to demonstrate its reliability and robustness. The test 
run undergoes different weather conditions, such as cloudy, 
sunny, rainy, and heavy-rained situations. Few false alarms 
were triggered, and many successful detections of illegal 
events were achieved against variable illuminations and 
cluttered outdoor environment. Some examples are shown in 
Fig. 4. An illegal parking event happened in a rainy day is 
shown in Fig. 4(a), and then dynamic face detection is 
triggered to take the driver's face, even part of the face was 
obscured by umbrella. Figs. 4(b) and (c) show the events 
under a sunny day, which brings complex and dynamic 
umbrae from human, building, and trees. A successful case 
of intrusion detection is shown in Fig. 4(b), and a PTZ 
camera collaborates to track the climbed person in follow-up 
180
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/31
國科會補助計畫
計畫名稱: 子計畫二：以視覺為基礎之睡眠障礙分析(II)
計畫主持人: 王元凱
計畫編號: 100-2218-E-030-004- 學門領域: 推動計畫-開放軟體 
研發成果名稱
(中文) 以視覺為基礎之睡眠姿勢識別
(英文) Vision-based Sleep Pose Recognition 
成果歸屬機構
輔仁大學 發明人
(創作人)
王元凱,陳泓諭,陳建儒
技術說明
(中文) 傳統睡眠檢測使用多頻道睡眠生理檢測儀以接觸方式量測生理訊號，從而判斷睡
眠狀態與睡眠病理，其缺點為成本高昂且不便利。本技術以視覺為基礎之睡眠姿
勢識別，以非接觸方式量測睡眠體位，幫助睡眠狀態與睡眠病理的判斷 。本技
術從夜間紅外線影像擷取特徵點進行特徵點偵測，估測睡眠影像中睡眠者之關節
點位置，再透過機率網路辨識身體姿勢。本技術已解決以下兩項睡眠視訊分析的
挑戰與問題：睡眠體位變化造成關節位置仿射轉換的變動，以及關節的遮蔽。本
方法已經證實有良好之準確度。
(英文) Traditional sleep diagnostic methods adopt contact-based biosignal measurement 
approach, such as polysomnography for sleep state determination and syndrome decision. 
It consumes high cost and is inconvenient for home healthcare. This technique is a vision-
based sleep posture recognition technique for the contactless measurement of sleep body 
position and the syndrome diagnosis of sleep disorder. We first extract the joint positions 
of sleeper from infrared images by keypoint matching, and then recognize sleep postures 
by probabilistic classifier. Two main challenges in night sleep videos have been resolved: 
affine transformation and occlusion. That is, the affine transformation problem happed in 
the variations of sleep postures can be resolved. The joints in the sleep posture model can 
be missing or occluded. This technique has been proven to have good recognition 
accuracy.
產業別 其他專業、科學及技術服務業
技術/產品應用範圍 睡眠醫學居家檢測，智慧家庭，居家照護
技術移轉可行性及
預期效益
增進居家照護便利性，降低睡眠檢測成本
註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
