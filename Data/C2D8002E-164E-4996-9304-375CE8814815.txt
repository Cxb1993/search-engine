video coding technologies and the projective 
transformation from two-dimensional videos to 
stereoscopic videos for the viewers with limited 
resource over the heterogeneous networks, and also 
multiview video synthesis for the purposes of both 
rendering and compression. 
英文關鍵詞： Multiview Video Coding, 3D/ Stereoscopic Video, View 
Synthesis. 
 
2 
 
 
行政院國家科學委員會專題研究計畫 
成果報告-精簡報告 
計畫名稱: 在社群網路上進行2D/3D感知串流之多視角合成與編碼技術 
Multiview Video Synthesis and Coding Technologies of the 2D/3D Perceptual Streaming 
over the Social Networks 
計畫編號：NSC 99-2221-E-009 -108 
執行期限：99年8月1日至100年10月31日 
計畫主持人：蕭旭峰教授 交通大學資訊科學工程學系 
 
 
中文摘要 
諸如自由視角電視及自由視角視訊之可在網路上傳輸的多視角視訊應用，隨著顯示器、
視訊擷取裝置以及編碼技術的演進與成熟而逐漸受到矚目。為了達成這些應用的自由導
覽，在有限的網路資源下通常需要景深圖來合成所需觀看之視角。為了增加傳輸和儲存多
視角視訊大量影像資料的效率，包含於 MPEG-4 AVC Multiview High Profile 裡之多視角視
訊編碼也已標準化。雖然這些壓縮技術雖然提供了比單純做時間軸的預測編碼為優的壓縮
效果，但仍未將多視角視訊的特性完全利用。另外，3D Video 以及 Free Viewpoint Television
在 MPEG 組織裡標準化的準備動作正在活躍進行中。本計畫的目標，著重研究多視角視訊
編碼技術，以及因應異質網路資源所對應到之二維視訊至立體視訊之合成，和多視角視訊
視角合成。 
關鍵詞：多視角視訊、立體視訊、視角合成。 
 
英文摘要 
With the recent progress of display, capture device, and coding technologies, multiview video 
applications over the transmission networks such as free viewpoint TV (FTV), free viewpoint 
video (FVV), and stereoscopic video have been introduced to the public with growing interest. To 
achieve free navigation of such applications, depth information is usually required along with the 
video data to synthesize the desired view under the constraint of available network resource. In 
order to improve the efficiency of transmission and storage of huge amount of Multiview video 
streams, the multiview video coding in MPEG-4 AVC as the Multiview High Profile is 
standardized. Even though these coding techniques can offer improvements over pure temporal 
prediction structure, the essential features of multiview video streams can be further taken 
advantage of. At the same time, there are emergent standardization activities of 3D video and 
Free Viewpoint Television in MPEG organization that also show the great research and 
development interest from the experts in the related fields. The objectives of this project include 
the development and innovation of the key technologies that enable interactive stereoscopic video 
and free viewpoint television services over the peer-to-peer streaming networks, such as 
4 
 
 
一、 簡介 
本計畫的主要目的，在於針對同儕社群網路的特性進行多視角視訊服務所需之關鍵技
術。本計畫原規劃以三年的時間，延續我們目前對多視角合成與同儕網路的研究心得，研
究與發展包含自單視角/多視角進行立體/多視角合成技術、多品質可調性視訊技術、以及在
同儕社群網路下針對可調性多視角串流之路由技術、基於網路編碼之安全性傳輸、與促進
社群競爭且合作之演算法與技術。 
雖然原規劃是三年，獲補助一年，本報告將簡述本年度之各項成果。 
因為多視角視訊之資料量龐大，為了讓這些龐大的視訊資料能更有效率地於網路上傳送
或是有效率地利用儲存媒體，多視角視訊編碼的技術與在 MPEG 標準化下因應而生。利用
不同視角之間所擁有的關連性擷取出來，以增進視訊壓縮的效能，來達到資料減量，加速
傳輸的目的。特別是在異質串流網路上，各個使用者網路/計算能力各異，各個客戶端點也
需要具備在編碼方面延展性之支援，甚至有網路編碼的需要。由於自由視角電視或是立體
視訊之視訊呈現上的需求，也需要進行視角合成。除此之外，立體視覺影像能打破傳統平
面顯影所帶來的視覺感受。為了達到立體顯影的效果，必須有不同視角的視訊資料需要傳
送給使用者，使其兩個視角之視差約略等於使用者左右眼成像的視差。立體視訊合成主要
分為多視角視訊資料和單一視角視訊資料兩種類。前者又特別在自由視角電視的應用上較
為常見：利用多視角的龐大資料量，合成視角之間的過渡影像，使得使用者能夠平順地切
換不同視角；後者的相關研究則較少，並且由於資料量相對於多視角視訊資料不足，所以
在模擬虛擬影像時所遇到的困難較多，誤差也較大，但是在應用層面則較廣，可以不受攝
影器材限制，將傳統的單一視角影像立體化，也可以大大減少傳輸時的資料量。 
以下為我們到目前為止所進行的相關研究，第二部份為研究方法、結果與討論，第三部
份為結論與未來發展，第四部份為所發表的論文，第五部份為參考文獻。 
二、 研究方法、結果與討論 
在這部份我們敘述感官視訊，多視角合成品質估計，與 2D 轉 3D 視訊之研究方法，結
果與相關討論。 
知覺上的品質對於多視角視訊的處理是相當重要的。在第一年之計畫裡，我們首先探討
影響感知品質之視訊特徵，進行文獻研究分析與實作驗證。在 PSPNR 理以 objective quality 
evaluation 的結果來近似 subjective quality，其使用 JND (Just-Noticeable Difference)來扣除
視覺上應當察覺不出之差值。除了 PSPNR 以外，在影像上還有 SSIM (structural similar index 
measurement) [1]。 
SSIM 的概念是在一張圖片當中，彼此相鄰的像素之間有著強烈的關聯性，此種關聯性
記載著場景中物體的結構資訊。而觀察者在觀察圖片時，會習慣性的抽取這種關聯性來當
做圖片好壞的其中一環，因此在衡量圖片的品質對於感官品質來說，SSIM 也是一種測量方
6 
 
的數值越小，MOVIE 的數值也就會越小，表示測試影片的品質越接近參考影片的內容，下
圖則表示說明，測試影片 MOVIE 以及 DMOS 之間關係，其中產生出來的數值與 DMOS 產
生出的數值是否有正相關的關係存在。 
 
圖(一)DMOS數值與 MOVIE之間的關係圖 
由此可知，視角合成的演算法是利用周遭已知的攝影資料來合成出欲觀察的視角的影片
資料。因此，若是周遭已知的攝影資料發生錯誤的話，可能會影響到欲觀察的視角的影片
的品質。因此欲觀察的視角的影片的品質與已知視角的影片的品質可能存在著某種關係。
換句話說，欲觀察的視角的影片的品質可能與已知視角的影片的品質存在著關係，如此一
來，此關係如同下圖所示: 
 
圖(二)欲觀察的視角的影片的品質與已知視角的影片的品質之關係 
為此，演算法如下圖所示: 
Original 
video 
encoder
Reconstructed 
video 
decoder
View 
synthesis
Target view 
data of one 
reference view
Perceptual 
error 
estimation
Neural 
network
Formula 
combination
Error 
prediction of 
target view
Target view 
position
Prediction 
error
 
圖(三) 合成視角品質預估演算法 
8 
 
在這些實驗結果當中，door flower 這段合成影片都是利用 cam 03 的視角以及 cam 14 這
兩邊視角當作已知視角來預估位在它們之間的合成視角的錯誤的比例值，從這些比例值當
中，我們可以發現到彼此比例關係大多小於 8%左右，還算是非常精準。在 newspaper 當中
都是利用 cam 01 的視角以及 cam 07 這兩邊視角當作已知視角來預估位在它們之間的合成
視角的錯誤的比例，而彼此比例關係也大多小於 8%左右。是個非常穩定的數值差距。 
在 2D 轉 3D 視訊方面，我們使用的方法流程如下: 
 
圖(五) 2D 轉 3D 視訊流程 
首先我們進行背景註冊演算法來分離前景移動物體，接著藉由消失點與消失線的偵測來
讓我們所建構之景深轉換模型得以計算整張背景圖之景深，如下式所示。 
,
)(
))
2
)(
2
(( 2
yyf
y
HH
yfh
d
vanish
frameframe
vanishC


  
 
(a)                          (b)  
Figure 7- The vanishing lines determined by the proposed algorithm are in blue of the (a) camera 1 of lovebird1 and (b) hallway. 
圖(六)消失線的偵測: (a) camera 1 of lovebird1 and (b) hallway. 
對於前景的部分則分析物體移動狀況來解析其於背景裡可能之立足點或投影點，再由背
景景深結果推估前景之景深。合成結果截圖請見下圖。 
(a) (b) 
10 
 
“Depth Map Generation for 2D-to-3D Conversion by Short-Term Motion Assisted 
Color Segmentation”, IEEE International Conference on Multimedia and Expo, pp. 
1958-1961, 2007. 
[6] Shao-Yi Chien, Shyh-Yih Ma, and Liang-Gee Chen, “Efficient Moving Object 
Segmentation Algorithm Using Background Registration Technique”, IEEE 
Transactions on Circuits and Systems for Video Technology, Vol. 12, No. 7, July 2002. 
[7] S. Battiato, A. Capra, S. Curti, M. La Cascia, “3D Stereoscopic Image Pairs by 
Depth-Map Generation”, Proceedings of the 2nd International Symposium on 3D Data 
Processing, Visualization, and Transmission, pp. 124-131, 2004. 
 
 
 更多之參考文獻限於篇幅，敬請詳見本子計畫之計畫申請書。 
 2 
Dialog Model: Intent, Knowledge and User Interaction”。分別由 Cisco 之 Susie Wee，U 
Sienna 之 Mauro Barni，與 Microsoft 之 Harry Shum 擔任演講人。Overview 的部分
有”DASH: Enabling Formats for Video Streaming over the Open Internet” ，”Multimodal 
Processing in Speech-based Interactions” ， 以及” A Dictionary Perspective on Multimedia 
Signal Processing” 。分別由 Qualcomm 之 Thomas Stockhammer，Chinese Univ Hong 
Kong 之 Helen Meng，與 EPFL 之 Pascal Frossard 擔任演講人。 
 
此次我們所發表的論文為 ” An Error Resilient Multiple Description Video Coder.” 在發表
的過程裡，蒐集了其他與會研究人員對我們所發表的看法，作為持續的研究參考，以及
讓其他研究團體瞭解我們的研究成果。會場上也與許多同領域傑出研究人員會面，交換
研究近況，收穫良多。 
 
三、攜回資料名稱及內容 
USB 論文檔案集一份。 
上述論文檔案以及所有會議相關之論文資料可以在 IEEE 的 paper 資料庫中 (IEEE Xplore)取得。 
IEEE Xplore 網頁 http://ieeexplore.ieee.org/Xplore/guesthome.jsp。 
 
四、其他 
論文被接受之紀錄請見: 
http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6093819 
 
[1] Y.-J. Huang, Hsu-Feng Hsiao, "An Error Resilient Multiple Description Video Coder," IEEE 
Workshop on Multimedia Signal Processing (IEEE MMSP), 2011. 
 
descriptions. By inserting PD before DCT module 
and adding data partition function, PDMD enhances 
the ability of error-resiliency in spatial domain. 
In this paper, multiple description scalar 
quantizer is modified such that the scalability of 
index assignment table is increased and a novel 
coefficients downsampling method in transform 
domain is integrated to balance the importance and 
quality among different descriptions at better 
coding efficiency. In addition, in the case that a 
decoder cannot receive all the descriptions, an 
estimation mechanism for generating similar 
reference frames at both encoder and decoder is 
introduced to deal with the drifting problem. 
The remainder of this paper is organized as 
follows. In Section II, the proposed MDC encoding 
tools are presented. The simulation results are 
shown in Section III, followed by the conclusion 
remarks in Section IV. 
II. THE PROPOSED ERROR RESILIENT MULTIPLE DESCRIPTION 
VIDEO CODER 
The proposed framework of a four-description 
MDC encoder is shown in Fig. 1. The DCT module 
in Fig. 1 stands for DCT transform coding, and 
IDCT is the corresponding inverse transform. The 
“Q” module performs quantization and “IQ” is the 
inverse quantization. The “EE” is the entropy 
encoding in MPEG-4 AVC. The proposed 
Coefficient Partition, MDSQ, and α component are 
integrated into the MPEG-4 AVC/H.264 framework. 
The Coefficient Partition component is used to split 
the residual DCT coefficients into two partitions. 
Both partitions are sent to a modified MDSQ 
component to assign the indices according to the 
index table of each description. As a result, there 
are four descriptions being sent to the entropy 
coding. Finally, the α component is the estimation 
mechanism to alleviate the drifting problem. They 
will be described in Section II-A, II-B, and II-C, 
respectively. 
 
 
Fig. 1.  The proposed framework of MDC encoder, built upon MPEG-4 AVC 
 
A. The Modified MDSQ 
The redundancy of each encoded streams in the 
MDSQ is usually determined by the used cells. To 
represent a certain scale level, the more central cells 
are used, the fewer the bits are used per level. 
However, the available number of the cells in a 
MDSQ table is decided by the number of diagonal 
lines. In other words, the number of diagonal lines 
has great impact on the redundancy allocation. If 
more diagonal lines are used, a smaller number of 
side indices shall be enough to present the 
quantized levels. On the contrary, if fewer diagonal 
lines are used, it takes more bits to present the same 
scale levels. 
For the MDSQ in the literature, the index 
assignment algorithm scans the cells as if there 
were indices lying on the main diagonal and its 2K 
neighboring diagonals and associates a progressive 
number with each of them.  For a MDSQ with fixed 
K, if the description loss occurs, the average 
distortion of scale levels at different columns of the 
MDSQ table will be the same in despite of the fact 
that the occurrence probability of the levels are 
quite different as shown by an example of video 
Carphone in Fig. 2. 
 
 
Fig. 2.  Level occurrence of residual DCT coefficients in video Carphone 
between [-13,-1] and [1,13] 
 
priorities as shown in Fig. 5 where Partition 1 has 
high priority for the first round at this block. The 
priority assignment of the nonzero coefficient is 
used to avoid the situation where one specific 
partition always gets the more important 
coefficients. 
The following procedures I and II summarize the 
proposed partition algorithm. 
 
 
Procedure I 
zig-zag scan of the block 
Coefficients assigned priority (p1,p2) = (partition 1, partition 2) 
while( unassigned nonzero coefficients in the block){ 
 p1 gets one nonzero coefficient 
 p2 gets one nonzero coefficient 
 (p1, p2) = (p2, p1);    
//Convert the priority 
} 
 
 
 
Procedure II 
if(  (description 1 & odd frame) || 
 (description 2 & even frame)  ){ 
 P = partition 1; 
}else{ 
P = partition 2; 
} 
while(unassigned blocks in a frame){ 
send P to description 
 P = the other partition 
} 
 
 
C. Estimation Mechanism for Error-Drifting Alleviation 
For every MDC coder, error propagation due to 
reference mismatch at encoder and decoder for the 
temporal predicted frames is a serious and difficult 
problem when the description loss occurs. An 
estimation mechanism is proposed to keep the 
similarity of the reference frames as much as 
possible. 
In this paper, a MDC coder encodes a video into 
four descriptions based on the algorithms described 
above. By integrating these results of the encoded 
four descriptions, we could estimate the 
approximate of the decoded frames as the reference 
frame at the encoder. The diagram of the proposed 
estimation mechanism is the α component shown in 
Fig. 1. Without losing generality, suppose that D(f), 
D1(f), D2(f), D3(f), and D4(f) are the directly 
decoded DCT coefficients of frame f from all the 
descriptions, description 1, 2 ,3, and 4, respectively. 
D(f) is the chosen DCT coefficients of the reference 
for the succeeding frames. The α component 
estimates D(f) at the encoder by the following 
equation. 
 
,)()(
4
1
∑
=
=
i
ii fDfD α  (1)
where αi are constants and Σαi = 1. 
In the α component, unlike at the encoder, the 
decoder may not have all the descriptions, however, 
the decoder still uses the results of D1(f), D2(f), 
D3(f), and D4(f) to compute the estimated D(f). If 
some of the descriptions are missing, the lost values 
are set to 0. 
III. SIMULATION RESULTS 
Several AVC-based MDC coder simulations at 
various conditions are performed and compared 
with the methods in the literature in this section. 
The MDC coders are all implemented on JM14.0 
reference software and the αi =0.25 (i=1,2,3,4) are 
used in the simulations. 
A. MDC with Two Descriptions 
The proposed algorithms contain two tools that 
are able to generate four descriptions. Here, two 
MDC coders with two descriptions based on one of 
the proposed tools are constructed and the coding 
efficiency of each coder is compared with the 
PDMD [2] and the MDSQ (K=1) [4] implemented 
over JM 14.0.  The videos are at QCIF resolution 
and the results in Fig. 6 show that the proposed 
tools can generate two descriptions at higher coding 
efficiency. 
 
0 200 400 600 800
30
32
34
36
38
40
42
P
S
N
R
(d
B
)
kbit/sec
Carphone at QCIF
 
 
PDMD(Jia-2006)
MDSQ(Campana 2008)
Proposed-CP+alpha component
Proposed-MDSQ+alpha component
200 400 600 800 1000
28
30
32
34
36
38
40
P
S
N
R
(d
B
)
kbit/sec
Foreman at QCIF
 
 
PDMD(Jia-2006)
MDSQ(Campana 2008)
Proposed-CP+alpha component
Proposed-MDSQ+alpha component
 
Fig. 6.  Coding efficiency of two-description MDC coders 
 
on Circuits and Systems for Video Technology, vol.18, no.2, pp.268-
272, Feb. 2008. 
[5] Boulgouris, N.V.; Zachariadis, K.E.; Leontaris, A.N.; Strintzis, M.G., 
"Drift-free multiple description coding of video," IEEE Fourth 
Workshop on Multimedia Signal Processing, 2001, vol., no., pp.105-
110, 2001. 
[6] Yen-Chi Lee; Altunbasak, Y.; Mersereau, R.M., "A drift-free motion-
compensated predictive encoding technique for multiple description 
coding," International Conference on Multimedia and Expo, 2003. 
ICME '03. Proceedings. 2003 , vol.3, no., pp. III-581-4 vol.3, 6-9 July 
2003. 
[7] Mengyao Ma; Au, O.C.; Liwei Guo; Xiaopeng Fan; Ling Hou; Chan, 
S.-H.G., "A multi-hypothesis decoder for multiple description video 
coding," IEEE International Symposium on Circuits and Systems,  
pp.2050-2053, 18-21 May 2008. 
[8] Servetto, S. D.; Ramchandran, K.; Vaishampayan, V. A.; and 
Nahrstedt, K., "Multiple description wavelet based image coding," 
IEEE Trans. on Image Process., vol. 9, no. 5, pp. 813–826, May 2000. 
[9] Matty, K.R.;   Kondi, L.P., "Balanced multiple description video 
coding using optimal partitioning of the DCT coefficients," IEEE 
Transactions on Circuits and Systems for Video Technology, vol.15, 
no.7, pp.928-934, 2005. 
 
 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：蕭旭 計畫編號：99-2221-E-009-108- 
計畫名稱：在社群網路上進行 2D/3D 感知串流之多視角合成與編碼技術 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
