 1
行政院國家科學委員會專題研究計畫成果報告 
計畫編號：NSC 97-2221-E-150-065 
執行期限：97 年 8 月 1 日至 98 年 7 月 31 日 主持人：黃惠俞   
執行機構及單位名稱：國立虎尾科技大學資訊工程系 
計畫參與人員：蕭俊男(國立虎尾科技大學資工所)、 
戴厚吉、黃智慧(國立中正大學資工所)  
                 
 
一、中文摘要 
 
透過網路的無遠弗屆許多人的數位資
訊正藉由它快速分享，然卻不見有整合的
系統來將這些彼此相關的數位影像資訊加
以處理彙整，再者這些影像間經由不同的
拍攝角度、拍攝時間及攝影器材功能的限
制，雖說同樣表達同件事物，亦增加了整
合上的困難度。然有數位影像的支援及網
際網路的資訊交換與分享，並無將這些相
同資訊加以整合及其應用的有效機制，因
此，我們研究一個能夠將這些對相同環境
或是建築物做描述之數位影像整合系統，
透過環場接圖技術的協助，初步建立其相
對的全景圖像，利用電腦視覺及虛擬實境
上的技術，重現出符合此資料的建築物或
是環境的 3D 全景環境，形成一個十分逼
近實物外觀之 3D 的視覺環境場景。本專
題主要研發一套以在多鏡頭視角下的環場
接圖技術為基礎的 3D 環境重現系統，其
主要步驟分兩大部分 3D 環境重現系統及
物件分析處理。系統具有相當好的延展
性，可應用於如古蹟修復、數位典藏、室
內設計、虛擬實境、3D 遊戲與電影製作等
方面。其中技術包含影像失真回復、影像
分群與關鍵影像決定、影像嵌合、幾何座
標轉換。經過這一系列的處理技術完成建
構 3D 環境或物件外觀的重現系統，實現
影像資訊整合。 
關鍵詞：3D 環境重現， 影像鑲嵌，物件
分析 
Abstract 
A variety of applications in digital 
image field is to improve the quality of 
human life and to extend their enjoyment. 
Some situations did not have to achieve in 
the past leisurely. Many people gain the 
digital image information by different video 
camera; however, it is lacked an integrated 
system to involve image information and 
applications. These images maybe obtained 
from the different angle, time, and the 
equipment limit. Hence, it caused some 
challenges in which how image information 
coordinates. 
In this scheme, we hoped to carry out a 
digital image integrated system to construct 
a panoramic scene of an approximate real 
environment. The procedures of this 
 3
三、結果與討論 
研究方法步驟： 
本計畫主要將針對從架設在環境中之攝影
機所錄影的視訊資料中，利用視訊處理將
受視之立體環境建置。圖一顯示系統的架
構圖。系統本身則分成立體環境呈現(3D 
representation) 與物件分析(object analysis)
兩大部份。在 3D representation 部分其步
驟有視訊影像分割與挑選、物件分離、特
徵點比對影像嵌合處理。在物件分析則有
物件之接合及回復與背景圖的建置等。以
下將說明各步驟。 
3.1 3D representation  
1. Frames 分割及分群處理 
就 frames segment 而言，對於從監控系統
中所取得視訊影像，由於 frames 數量眾
多，若是貿然對所有的 frames 進行接合處
理，將會大幅降低系統效能，因此必須先
進行適當的 frames數量的挑選與分群的處
理，在從分群處理過後的各群中，逐一挑
選出每個群體中較具有代表性的 frame，
然後將此 frame 視為此群體的 key frame，
當影像進行接合處理時，將只使用這些挑
選出來的 key frames，進一步接合。 
對於數量眾多的 frame 分群方式是只
挑選監控系統所取出的視訊影像片段中的
I frame 進行分群，主要的測量為計算各個
I frames 間的相似度高低，利用門檻值的
調整可以分割出不同數量的 groups，相似
度 計 算 主 要 利 用 Sum of squared 
differences (SSD) [7-8]誤差量來評估，當
SSD 值越低時代表兩個 frame 越相似。因
此我們利用此 SSD 的特性計算其 frame 間
誤差值，若其值低於門檻值既代表從開始
計算的 frame 到目前之 frame 為止將必須
歸類為同一 group，經此程序處理後將可
分割出數個 group。 
2. 群組中之 Key frame 的選擇 
當視訊序列分割產生數個 group 後，由於
每個 group 中依舊包含著數個 frames，因
此必須再將這些 frames之中在挑選出一個
frame 當作 key frame，藉以代表此 group。
目前採用由低於門檻值後，產生不同 group
時的第一個 I frame 當做此 group 的 key 
frame，如此相較於對全部的群組之所有的
frames 而言,利用 key frame 進行一系列的
處理將是簡單且節省計算時間和降低其複
雜度，並且同時滿足影像接合上，overlap
的部分不至於因為挑選出來的 key frames
間的 overlap 部份過小，導致不易進行接合
上的特徵點尋找，因而產生接合結果不佳
的現象。 
3. 物件分離 
利用專用軟體之協助藉以取得視訊影像與
攝影器材間所需的參數與附加之資訊，並
進行正規化處理，對於帶有曲度之監控視
訊影像做先置處理，調整出其適當之轉換
關係，藉以消除影像邊緣失真後對於後續
影像嵌合處理時所可能造成的影響。利用
視訊影像序列在時間點上的差異，透過背
 5
五、成果自評 
本次計畫的執行主要預達成的目標成果是
利用多支攝影機不同視角下建構出立體的
環境重現系統。其主要技術有影像失真回
復、影像分群與關鍵影像決定、影像嵌合、
幾何座標轉換等。 
就目前而言，由於多支攝影機所拍攝
而得其存在有建築物的對稱、鏡頭角度、
及光源不均等問題，增加了建置立體環境
場景的困難度。因此，目前僅對單一鏡頭
深入研究。針對單一鏡頭下的影像分群處
理，由於不同的門檻值影響造成數個的
group 數量，過高及過低的門檻值都將影
響接合處理，過少群組個數將產生不易尋
找 overlap 部份接合的情形，而過多群組個
數將產生無法分群的情況。因此，在目前
的實驗中，門檻值大約設定在 50%~60%
之間，如此將可獲得較好的決策值；而特
徵點的擷取部分有相當不錯的效果，亦不
受旋轉角度以及亮度差異的影響，接合結
果則是令人滿意的。對未來將繼續針對多
支鏡頭拍攝相同場景的影像加以接合處理
並克服視角及光源問題，以冀實現系統原
先所設定之目標，最後展示出此從這些鏡
頭所合成的整體環境效果。 
 
六、參考文獻 
[1]. P. Biber, S. Fleck, and T. Duckett, “3D 
modeling of indoor environments for a 
robotic security guard,” in Proc. of the 
IEEE Computer Society Conf. on 
Computer Vision and Pattern 
Recognition, vol. 3, pp. 124-130, 2005. 
[2]. M. Goesele, N. Snavely, B. Curless, H. 
Hoppe, and S. M. Seitz, “Multi-view 
stereo for community photo 
collections,” in Proc. of ICCV 2007, 
Rio de Janeiro, Brasil, 2007. 
[3]. M. Maitre, C. Guillemot, and L. Morin, 
“3-D model-based frame interpolation 
for distributed video coding of static 
scenes,” IEEE Trans. on Image 
Processing, vol. 16, no. 5, pp. 
1246-1257, 2007. 
[4]. Z. Zhigang, T. Hao, G. Wolberg, and J. 
R. Layne, “Content-based 3D mosaic 
representation for video of dynamic 3D 
scenes,” in Proc. of the 34th IEEE 
Applied Imagery and Pattern 
Recognition Workshop, pp. 6-12, 2005. 
[5]. A. Mittal and D. Huttenlocher, “Scene 
modeling for wide area surveillance 
and image synthesis,” in Proc. of Conf. 
on Computer Vision and Pattern 
Recognition, vol.2, pp. 160-167, 2000. 
[6]. N. Snavely, S. M. Seitz, and R. Szeliski, 
“Photo tourism: exploring photo 
collections in 3D,” ACM Trans. on 
Graphics (SIGGRAPH Proceedings), 
25(3), pp. 835-846, 2006. 
[7]. A. Agarwala, M. Agrawala, M. Cohen, 
D. Salesin, and R.Szeliski, 
“Photographing long scenes with 
 7
 
 
 
 
 
圖三 在同一鏡頭下不同角度之拍攝之資料，經由 SIFT 處理後，特徵點 mapping 的結
果圖。 
 
表 Y04 1
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                     九十八年五月二十四日 
報告人姓名 黃惠俞 
Hui-Yu Huang 
 
服務機構
及職稱 
國立虎尾科技大學 
資訊工程學系 
助理教授 
    時間 
會議 
     地點 
2009 年 05 月 20 日至 05 月
22 日  
日本國 橫濱  
本會核定
補助文號
NSC97-2221-E-150-065 
會議 
名稱 
 (中文) 第 11 屆機器視覺及應用研討會 
 (英文) 2009 Machine Vision and Applications, Tokyo, Yokohama 
發表 
論文 
題目 
 (中文) 植基於虛擬 3D DCT 及量化指標模組的一個視訊浮水印演算法 
 (英文) Video watermarking algorithm based on pseudo 3D DCT and quantization 
index modulation 
報告內容應包括下列各項： 
一、參加會議經過 
我的論文被選為以海報方式進行發表，並安排在 5月 21 日下午的 poster section，舉行的
時間從 13：00～14：30。大會本次提供較大海報牆給 poster 報告者使用因而亦可以呈現更
完整的研究主體及內容展示，為此本次亦製作兩大張 A0 大小之海報資料攜至大會會場。於會
議舉行前搭乘華航航空飛抵開會地點日本國東京成田(Narita Airport)國際機場。會議舉行之地
點是位在橫濱區的 Keio University 之協生館內進行大會之全部議程。會議開幕式準時開始於
二十日(三)早上 9:00，主席 Mr. Saitog 僅簡單致歡迎詞並介紹本次研討會特色及本會議從創
立至今已 20 年之發展史之對機器視覺相關應用的貢獻，其中亦發表相當多具有建設性及應用
性有關於物件辨識、醫學、監控系統等領域之論文，並將秉持著本會議的精神繼續延續發展。
隨即進行第一場的 oral presentation 及相關的 poster sections。此次大會議程安排每天有三場
sections、一場 poster section、及每天下午皆有一場 invited talk 的議程進行，整個會議相當充
實且緊湊。 
 
二、與會心得 
對於 IAPR MAV conference 會議已是我第三次參加，地點皆在日本國舉行，此會議為期三
天(五月二十號至二十二日)。我的論文在會議中是以海報方式進行論文及成果展示與報告。
附件三
 
表 Y04 3
  我的 poster section 時間被安排在會議進行的第二天(五月二十一日)下午一點至兩點半之
間，本 section 共有 30 篇的論文分別在兩個展示會場內進行。因正值午餐時間所以餐後既湧
入了相當多與會的學者專家到 poster 會場，展示時間現場一片熱絡的討論聲，每位 poster 有
相當多的時間為其所做的研究做最佳的說明與學者討論，當時間結束時會場內仍就相當多來
賓在其內與之討論，討論聲絡繹不絕久久沒散。由於多次參加 MVA 會議，加上多次參於其
他國際會議之經驗，使自己越來越能得心應手並充裕的準備所需的資料，當然最大的收穫是
可獲得近來的研究趨勢及學術應用在科技方面的發展或產業開發等寶貴的知識，無論是實作
或是理論都受益良多，無疑是給自己又再一次成長及學習的機會，本人秉持著更積極的學習
態度及交換研究心得的心境參與此次會議。在這將近兩小時的 poster representation 時間獲益
良多，因與會來賓有學者、專家或業界等，基於他們的專業知識而提出不同見解及疑問，因
而給了不同角度的思考思緒，對問題或研究有了更新的思考。每一位前來討論來賓做詳盡的
報告及解說之外，同時也交換彼此在這方面的心得及看法，亦同時獲得良好的建議及對我研
究的讚賞等。對我而言，這三天的會期是再一次非常難得既寶貴的學習之旅，往後將更加努
力於研究方面及多參加國際間相關研究舉行之會議，拓展自己的視野，提升自己在研究方面
能更具廣度及深度性。因會議舉行地點在橫濱區-慶應大學內，於參加會議期間順道與其他來
至台灣的學者們一同瀏覽校園裡的環境及學生們的互動情況等，做了一次校園尋訪。 
   
三、考察參觀活動(無是項活動者省略) 
無 
四、建議 
無 
五、攜回資料名稱及內容 
本次會議主要攜回論文集一本 (proceedings)、會議光碟片 1 片及議程資料。議程包含
invited talks(3 篇)、oral papers and poster papers 共 122 篇等全文論文。其主要目錄依日期分列
如附錄一。 
TABLE OF CONTENTS 
 
Wednesday, May 20, 2009 
 
Session 1: Interaction & Virtual Reality 
1-1 Indoor Map Display Method Using Mixed Reality ····················································· 1 
Satoshi Hisanaga and Takaaki Kase, Japan 
1-2 Texture Overlay onto Flexible Object with PCA of Silhouettes and K-Means Method for Search 
into Database ································································································· 5 
Hiroshi Tanaka and Hideo Saito, Japan 
1-3 Real-time Motion-based Gesture Recognition Using the GPU ········································ 9 
Mark Bayazit, Alex Couture-Beil and Greg Mori, Canada 
Session 2: Motion & Multiview 
2-1 Automatic Gigapixel Mosaicing in Large Scale Unstructured Underground Environments ···· 13 
Daniela Craciun, Nicolas Paparoditis and Francis Schmitt, France 
2-2 Image Rectification for Robust Matching of Car-mounted Camera Images ······················· 17 
Naoko Enami, Norimichi Ukita and Masatsugu Kidode, Japan 
2-3 Self Image Rectification for Uncalibrated Stereo Video with Varying Camera Motions and 
Zooming Effects ··························································································· 21 
Chia-Ming Cheng, Shang-Hong Lai and Shyh-Haur Su, Taiwan 
2-4 A New Method for Projector Calibration Based on Visual Servoing ······························· 25 
Jérémie Mosnier, Francois Berry and Omar Ait-Aider, France 
Session 3: Poster Session 1 
3-1 New System Implementation on SIMD Processor for Reliable Fingerprint Singularity Detection 
by Singular Candidate Method ·········································································· 30 
Daisuke Tomizawa, Yuta Hasegawa, Tomohiko Ohtsuka and Hiroyuki Aoki, Japan 
3-2 Detection of Abnormal Objects in a Scene Based on Local Features ······························· 34 
Junya Kobayashi and Keiichi Yamada, Japan 
3-3 Real-Time Uncharacteristic-Part Tracking Based on Points Tracking ······························ 38 
Norimichi Ukita, Akira Makino and Masatsugu Kidode, Japan 
3-4 Automatic Extraction of Layer Structure for Retina in OCT Image Using Morphological 
Operation and Active Net ················································································ 42 
Ai Yamakawa, Shinji Tsuruoka, Hiroharu Kawanaka, Fumio Okuyama and Toshiki Yagi, 
Japan 
3-5 Human Behavior Analysis Using Multiple 2D Features and Multicategory Support Vector 
Machine ···································································································· 46 
Hao-Cheng Mo, Jin-Jang Leou and Cheng-Shian Lin, Taiwan 
3-6 Exploitation of 3D Information for Directing Visual Attention and Object Recognition ········ 50 
Oytun Akman and Pieter Jonker, Netherlands 
ⅰ
3-25 Application of Co-Occurrence Frequency Image ···················································· 126 
Takayuki Fujiwara, Hiroyasu Koshimizu and Manabu Hashimoto, Japan 
3-26 Improved Matching Accuracy in Traffic Sign Recognition by Using Different Feature Subspaces
 ·············································································································· 130 
Arihito Ihara, Hironobu Fujiyoshi, Masanari Takagi, Hiroaki Kumon and Yukimasa Tamatsu, 
Japan 
3-27 Object Pose Estimation Using Patch-Duplet/SIFT Hybrids ········································ 134 
Fredrik Viksten, Sweden 
Session 4: Invited Talk 1 
4-1 Large Scale Image Search ··············································································· 138 
Dr. Cordelia Schmid, INRIA, France 
Session 5: Visual Surveillance 
5-1 Probabilistic BPRRC: Robust Change Detection against Illumination Changes and Background 
Movements ································································································ 148 
Kentaro Yokoi, Japan 
5-2 People Re-Identification by Means of a Camera Network Using a Graph-Based Approach ··· 152 
Dung-Nghi Truong Cong, Louahdi Khoudour, Catherine Achard and Philippe Phothisane, 
France 
5-3 Incident Detection Based on Dynamic Background Modeling and Statistical Learning Using 
Spatio-Temporal Features ··············································································· 156 
Yasuhiro Murai, Hironobu Fujiyoshi and Masato Kazui, Japan 
5-4 Human Head Tracking Based on Inheritance and Evolution Concept ····························· 162 
Yi Hu and Tetsuya Takamori, Japan 
5-5 Unsupervised Abnormal Behavior Detection for Real-Time Surveillance Using Observed 
History ····································································································· 166 
Tsz-Ho Yu and Yiu-Sang Moon, Hong Kong 
5-6 Abnormal Spatial Event Detection and Video Content Searching in a Multi-Camera Surveillance 
System ····································································································· 170 
Chueh-Wei Chang, Ti-Hua Yang and Yu-Yu Tsao, Taiwan 
 
Thursday, May 21, 2009 
 
Session 6: Feature Extraction & Pattern Recognition 
6-1 Statistical Reach Feature Method and Its Application to Template Matching ···················· 174 
Ryushi Ozaki, Yutaka Satoh, Kenji Iwata and Katsuhiko Sakaue, Japan 
6-2 Design of Feature Extraction Operators for Use on Biologically Motivated Hexagonal Image 
Structures ·································································································· 178 
Sonya Coleman, Bryan Scotney and Bryan Gardiner, United Kingdom 
6-3 An Appearance Based Fast Linear Pose Estimation ················································· 182 
Toshiyuki Amano and Toru Tamaki, Japan 
ⅲ
8-12 Face Recognition by Combining Complementary Matchings of Single Image and Sequential 
Images ····································································································· 253 
Yea-Shuan Huang, Wei-Cheng Liu and Fang-Hsuan Cheng, Taiwan 
8-13 Recognition of Lung Cancers on CT Using 3-D Object Models of Different Classes ·········· 257 
Hotaka Takizawa, Shinji Yamamoto and Tsuyoshi Shiina, Japan 
8-14 Model-Based 3D Object Tracking with Online Texture Update ··································· 261 
Keisuke Tateno, Daisuke Kotake and Shinji Uchiyama, Japan 
8-15 On a Technique for Evaluating Performance of Wipers Based on Forward Visibility ··········· 265 
Takashi Kitayama, Masatoshi Sakai, Takashi Kato, Muneo Yamada, Tomoaki Nakano, Shin 
Yamamoto, Osami Yamamoto, Masahiko Yamanishi and Hiroshi Matsumoto, Japan 
8-16 A Multicamera Vision System Supporting the Development of Wide-Area Exertainment 
Applications ······························································································· 269 
Xenophon Zabulis, Thomas Sarmis, Dimitris Grammenos and Antonis A. Argyros, Greece 
8-17 Multiple-Person Tracking for a Mobile Robot Using Stereo ······································· 273 
Junji Satake and Jun Miura, Japan 
8-18 Automatic Analysis of Fish Behaviors and Abnormality Detection ······························· 278 
Myo Thida, How-Lung Eng and Boon Fong Chew, Singapore 
8-19 Video Retrieval System Using Handwriting Sketch ················································· 283 
Akihiro Sekura and Masashi Toda, Japan 
8-20 Linear 3-D Object Pose Estimation with Dense Sample Images –Discussions about Limitation 
of Parameter Estimation Ability by the Linear Regressions– ······································· 287 
Toshiyuki Amano and Toru Tamaki, Japan 
8-21 Predicting Attainable Region of Vehicle Using Trajectory Clustering ····························· 291 
Noritaka Sekiyama, Jien Kato and Toyohide Watanabe, Japan 
8-22 A Sensor Based Navigation Algorithm for Moving Obstacles Assuring Convergence Property
 ·············································································································· 295 
Masaaki Tomita and Motoji Yamamoto, Japan 
8-23 Static Estimation of the Meteorological Visibility Distance in Night Fog with Imagery ······· 300 
Romain Gallen, Nicolas Hautière and Eric Dumont, France 
8-24 Machine Vision for Excess Gluing Inspection in Spindle Motor Assembly ······················ 304 
Varunyou Buddhachan and Pakorn Kaewtrakulpong, Thailand 
8-25 Chromosome Image Recognition with Local Band Patterns ······································· 308 
Toru Abe, Chieko Hamada and Tetsuo Kinoshita, Japan 
8-26 Comparison of Conventional US and Spatial Compound Imaging in Diagnostic Performances of 
Computer Aided Diagnosis System ···································································· 312 
Wei-Chih Shen, Ruey-Feng Chang and Woo Kyung Moon, Taiwan 
8-27 Worker Behavior and Intension Modeling in Production Process ································· 316 
Takuma Funahashi, Takayuki Hoshino, Naoya Tokuda, Takayuki Fujiwara and Hiroyasu 
Koshimizu, Japan 
8-28 Investigation of OCT Images Descriptions on the Base of Representational MDL Principle ·· 320 
Igor Gurov and Alexey Potapov, Russia 
 
ⅴ
12-2 Extracting 3D Polyhedral Building Models from Aerial Images Using a Featureless and Direct 
Approach ·································································································· 378 
Fadi Dornaika and Karim Hammoudi, France 
12-3 AR GIS on a Physical Map Based on Map Image Retrieval Using LLAH Tracking ············ 382 
Hideaki Uchiyama, Hideo Saito, Myriam Servières and Guillaume Moreau, Japan 
Session 13: Poster Session 3 
13-1 Probabilistic-Based Semantic Image Feature Using Visual Words ································ 386 
Cheng-Chieh Chiang, Jia-Wei Wu and Greg C. Lee, Taiwan 
13-2 A Block Matching Technique Using Unit Gradient Vectors ········································ 390 
Toshiaki Kondo and Waree Kongprawechnon, Thailand 
13-3 An Easy Technique for Fisheye Camera Calibration ················································ 394 
Haijiang Zhu and Fuchao Wu, China 
13-4 Robust Vision-Based Target Tracking Control System for an Unmanned Helicopter Using 
Feature Fusion ···························································································· 398 
Feng Lin, Ben M. Chen and Tong H. Lee, Singapore 
13-5 Robust Background Segmentation Using Background Models for Surveillance Application ·· 402 
Tianci Huang, Jingbang Qiu, Takahiro Sakayori and Takeshi Ikenaga, Japan 
13-6 Motion Segmentation Using Divisive Graph Cuts ··················································· 406 
Jong-Sung Kim and Il-Kwon Jeong, South Korea 
13-7 A Novel Transducer: From Lip Motion to Voice Message ·········································· 410 
Takeshi Saitoh, Tomoya Kato and Ryosuke Konishi, Japan 
13-8 Proposal of Exaggeration Method Based on Shape and Positional Relations of Automotive Parts
 ·············································································································· 414 
Hiroshi Fujimoto, Takuma Funahashi, Takayuki Fujiwara and Hiroyasu Koshimizu, Japan 
13-9 Image Processing Architecture for Real-Time Micro- and Nanohandling Applications ········ 418 
Tim Wortmann, Christian Dahmen, Robert Tunnell and Sergej Fatikow, Germany 
13-10 Fitting Curves on Riemannian Manifolds Using Energy Minimization ··························· 422 
Chafik Samir, Pierre-Antoine Absil, Anuj Srivastava and Eric Klassen, Belgium 
13-11 Classification of Spectators' State in Video Sequences by Voting of Facial Expressions and Face 
Directions ································································································· 426 
Tetsu Matsukawa, Akinori Hidaka and Takio Kurita, Japan 
13-12 A Hybrid Model for Multiple Object Category Detection and Localization ······················ 431 
Dipankar Das, Yoshinori Kobayashi and Yoshinori Kuno, Japan 
13-13 Unsupervised Material Classification of Printed Circuit Boards Using Dimension-Reduced 
Spectral Information ····················································································· 435 
Abdelhameed Ibrahim, Shoji Tominaga and Takahiko Horiuchi, Japan 
13-14 An Optical Video Cryptosystem with Adaptive Steganography ···································· 439 
Cheng-Hung Chuang and Guo-Shiang Lin, Taiwan 
 
 
ⅶ
15-2 Free Space Detection for Autonomous Navigation in Daytime Foggy Weather ················· 501 
Nicolas Hautière, Jean-Philippe Tarel and Didier Aubert, France 
15-3 An Intelligent Night Vision System for Automobiles ················································ 505 
Tsz-Ho Yu, Yiu-Sang Moon, Jiansheng Chen, Hung-Kwan Fung, Hoi-Fung Ko and Ran Wang, 
Hong Kong 
15-4 A New Approach for In-Vehicle Camera Traffic Sign Detection and Recognition ·············· 509 
Andrzej Ruta, Yongmin Li, Fatih Porikli, Shintaro Watanabe, Hiroshi Kage and Kazuhiko 
Sumi, United Kingdom 
15-5 Recognition of Road Markings from In-Vehicle Camera Images by a Generative Learning 
Method ····································································································· 514 
Masafumi Noda, Tomokazu Takahashi, Daisuke Deguchi, Ichiro Ide, Hiroshi Murase, 
Yoshiko Kojima and Takashi Naito, Japan 
15-6 Vision Based Tangent Point Detection Algorithm, Evaluation and Validation ··················· 518 
Romain Gallen and Sébastien Glaser, France 
ⅸ
Fig. 1. A pseudo 3D DCT diagram.
transform these DC values to DCT domain again. After
transforming process, we can obtain a new DC value and
three AC values. According to Eq. (1), a sum of these AC
values with corresponding weight values can be computed
and obtained. By repeating above steps until all blocks of
frames with the same group, we will acquire a sequence of
sums of every block. Finally, the embedding information
can be obtained to construct the embedding technique.
2.2. Watermark embedding
In order to embed the watermark bits, the quantization
index modulation (QIM) method is employed. Based on
the QIM algorithm, the embedding domain is divided into
several regions. The interval of every region is the same
value which equals to the threshold T (i), and an index ob-
tained by Q(i, k) is assigned to each region. Every region
represents a value of watermark. According to the Q(i, k)
and the embedded bit streams, we will further modify the
values of Sum(i, k) by means of the QIM method. The
modiﬁcation is expressed as:
Q(i, k) =
⎧⎪⎪⎨
⎪⎪⎩
2× p, if EW is 0, Matched, (a)
2× p, if EW is 1, Unmatched, (b)
(2× p) + 1, if EW is 1, Matched, (c)
(2× p) + 1, if EW is 0, Unmatched. (d)
(2)
where p and EW denote a random non-negative integer and
the embedded watermark bit, respectively. If the relation-
ship of Q(i, k) and the watermark bit conforms to Eq. (2)
(a) or (c), the Sum(i, k) is not modiﬁed. Otherwise, the
Sum(i, k) will be changed to ﬁt this condition. In order to
increase the robustness of our proposed system, Sum(i, k)
value will be changed to the center value corresponding
to this section to gain the distortion tolerance. The inser-
tion processing is illustrated as Fig. 2. After performing all
blocks of frames within the group, we can derive the vari-
ation sequence Diff(i) which is the difference denoted as
D(i, k) between the modiﬁed Sum(i, k) and Sum(i, k) for
each block. It is reasonable that the blocks with the small
variations will be selected to embed the watermark. Thus,
the embedding positions can be determined according to the
amount of D(i, k). Since Sum(i, k) is consisted of sev-
eral AC values, the modiﬁcation of Sum(i, k) equals to the
change of AC values. Note that the low frequency com-
ponent is more robust and visually sensitive than the high
frequency component. That is, if the low frequency com-
ponent is modulated, it will cause the distortions more seri-
ously, but it has higher ability to resist attacks than the high
frequency component does. Therefore, we use the weights
Fig. 2. Watermark insertion by QIM method.
to modulate the Sum(i, k) which is deﬁned as:
Sum′(i, k) =
∑
l
Ws(i, k, l)× |AC(i, k, l)|
+We(i, k, l)×D(i, k), (3)
where the D(i, k) represents the difference between
Sum′(i, k) and Sum(i, k). AC(i, k, l) and We(i, k, l)
denote the AC value and the weights corresponding to
the kth block within the ith group in the frames, respec-
tively. We(i, k, l) can be adjusted by user and the sum of
We(i, k, l) must equal to one. After determining the em-
bedding position, we change the original value in the posi-
tion into the center of the corresponding section by using
Eq. (3). By repeating above procedures until all watermark
bits are inserted, the embedding process will be achieved.
Finally, all embedding positions, the secret seed S, weights
Ws(i, k, l), and the threshold T (i) will be recorded as the
secret embedding key. This embedding key will provide the
important information to exactly extract the embedded wa-
termark.
2.3. Watermark extraction
The extraction process is the inverse of the embedding
process. First of all, the raw video sequence is separated
into several groups of frames and each frame is divided into
blocks. When we determine the embedding blocks, the se-
lected blocks were transformed by using pseudo 3D DCT,
we can further obtain Sume(i, k) by using Eq. (1). Then we
divide Sume(i, k) based on the relative threshold T (i) and
the secret embedding key to calculate the quotient Qe(i, k).
According to Qe(i, k), The embedded bit can be detected
and given by
EW =
{
0, if Qe(i, k) = 2× p,
1, if Qe(i, k) = 2× p + 1, (4)
IfQe(i, k) is odd value, the embedded bit is 1. IfQe(i, k) is
even value, the embedded bit is 0. By repeating above steps,
we can exactly determine the embedded bits gradually until
all watermark bits are extracted. Finally, using the secret
seed S recorded in the secret embedding key, the embedded
watermark can be effectively detected.
3. PERFORMANCE MEASUREMENT
There are two important factors to measure the per-
formance of watermark system:transparency and robust-
ness. For transparency, we use the peak-signal-to-noise ra-
tio (PSNR) to present this characteristic expressed as
PSNR = 10× log S
2
max
MSE
, (5)
208
(a)
(b)
Fig. 5. NC values for the different MPEG-1 compres-
sion compared with our proposed method, Thiemert et al.’s
method, and Kong et al’s. method.
(a)
(b)
NC=1
(c)
NC=0.5903
(d)
NC=0.6361
Fig. 6. (a)Watermarked frame with Wiener ﬁltering. Ex-
traction watermark results. (b)-(d)The proposed method,
Thiemert et al.’s method, and Kong et al’s method, respec-
tively.
(a)
(b)
NC=1
(c)
NC=0.5889
(d)
NC=0.6042
Fig. 7. (a)Watermarked frame with Wiener ﬁltering. Ex-
traction watermark results. (b)-(d)The proposed method,
Thiemert et al.’s method, and Kong et al’s. method, respec-
tively.
(a)
(b)
NC=1
(c)
NC=0.6181
(d)
NC=0.5903
Fig. 8. (a)Watermarked frame with pepper and salt noise.
Extraction watermark results. (b)-(d)The proposed method,
Thiemert et al.’s method, and Kong et al’s method, respec-
tively.
(a)
(b)
NC=0.9986
(c)
NC=0.6278
(d)
NC=0.5833
Fig. 9. (a)Watermarked frame with pepper and salt noise.
Extraction watermark results. (b)-(d)The proposed method,
Thiemert et al.’s method, and Kong et al’s. method, respec-
tively.
210
表 Y04 2
與會的人員有來自世界各地在這機器視覺方面的專家、科技人才及學者等，一起在這會議中
進行交流及經驗分享。 
此次的會議共有 15 場 sections 包含 3 場 invited talks、9 場 oral presentations、3 場 poster 
sections 分別在會議舉行期間進行。此會議固定每兩年在日本國舉行，主要是以機器視覺及人
機介面，圖形化資訊系統等相關應用之論文發表為主的技術交流之國際會議，每兩年以 IAPR 
協會主導由日本相關協會組織負責會議的舉辦及會議論文之發行。因此，在機器視覺領域此
會議是相當知名的一個國際性會議。其涵蓋的論文領域相當廣泛主要有： 
 
 
結合所有相關的機器視覺，保密系統、行為分析、人機互動、醫學系統，地圖資訊系統等及
其他應用之跨領域範疇的研討會。由於這會議是每兩年舉行一次，徵稿文件更是加倍增加，
為了維持高品質的論文，所接受刊登之論文皆具有相當的水平。因此對與會的人員而言，本
會議無疑是提供一個可獲得最新且更具實際應用及瞭解當今熱門研究方針之再學習及分享成
果的殿堂。同時亦可和各與會專家及學者互相進行技術研討及學術交流，是相當有意義的國
際會議。其出席大會的學者相當踴躍且可觀，無論是 oral presentation 或是 poster section 或是
invited talk 皆充分把握在每一位演講者結束後之 Q ＆ A 時間對來賓提出了問題或是建議
等，而在 poster section 時更是討論踴躍及熱烈。 
 
表 Y04 4
六、其他 
  附上會議之舉行地點及出席 poster presentation 的照片如下。 
 
  
大會會址       大會會場 
  
大會會場(場外、場內) 
  
Poster Section 
 
附錄一： 
3-7 Inter Mode Decision Algorithm For Advanced Video Coding ······································ 54 
Fang-Hsuan Cheng and Yea-Shuan Huang, Taiwan 
3-8 Large-Scale Stereo for Improvement of 3D Measurement Accuracy in Gaze-Observation ····· 58 
Masafumi Nakagawa, Yoshihiro Kawai and Fumiaki Tomita, Japan 
3-9 An Active Appearance Model with a Derivative-Free Optimization ······························· 62 
Jixia Zhang, Franck Davoine and Chunhong Pan, China 
3-10 Image Forgery Detection Based on Quantization Table Estimation ································ 66 
Guo-Shiang Lin, Min-Kuan Chang and You-lin Chen, Taiwan 
3-11 A Binarization Method for Crack Detection in a Road Surface Image with the Fractal Dimension
 ··············································································································· 70 
Hiromi Yoshida and Naoki Tanaka, Japan 
3-12 Building a Multi-Touch Display Based on Computer Vision Techniques ·························· 74 
Damien Michel, Antonis Argyros, Dimitris Grammenos, Xenophon Zabulis and Thomas 
Sarmis, Greece 
3-13 Synchronization of Full-View Image and GPS Data for Route Map Building ···················· 78 
Ying Hai, Shigang Li and Yuto Mizuno, Japan 
3-14 Study of Early Screening Method of Dementia and Its Systemization ····························· 82 
Youhei Kondou, Mikiko Kawasumi, Osami Yamamoto, Muneo Yamada, Shin Yamamoto and 
Tomoaki Nakanno, Japan 
3-15 Face Blurring for Privacy in Street-level Geoviewers Combining Face, Body and Skin Detectors
 ··············································································································· 86 
Alexandre Devaux, Nicolas Paparoditis, Frédéric Precioso and Bertrand Cannelle, France 
3-16 Vision-Based Real-Time Monitoring on the Behavior of Fish School ····························· 90 
Boon Fong Chew, How-Lung Eng and Myo Thida, Singapore 
3-17 Design and Evaluation of Features That Best Dene Text in Complex Scene Images ··········· 94 
Navid Mavaddat, Tae-Kyun Kim and Roberto Cipolla, Australia 
3-18 OCR Based Thresholding ················································································ 98 
Yves Rangoni, Faisal Shafait and Thomas M. Breuel, Germany 
3-19 A 3.2 kHz, Stereo Sensing Module Using Two Profile Sensors ···································· 102 
Yoshinori Matsui, Munenori Takumi, Haruyoshi Toyoda, Naohisa Mukozaka, Atsushi Ihori, 
Yukinobu Sugiyama and Seiichiro Mizuno, Japan 
3-20 Video-Based Face Recognition Using a Probabilistic Graphical Model ·························· 106 
Yi-Chia Chan, Cheng-Chieh Chiang, Kai-Ming Wang and Greg C. Lee, Taiwan 
3-21 A Comparative Study of Hierarchical Matching Algorithms for Face Recognition ············· 110 
Hajar Momeni, Mohammad T. Sadeghi and Hamid R. Abutalebi, Iran 
3-22 1,000-fps Visual Feedback Control of an Active Vision System over a High-Load Network ·· 114 
Daisuke Wako, Shingo Kagami and Koichi Hashimoto, Japan 
3-23 Image Based View Localization System Retrieving from a Panorama Database by SURF ···· 118 
Naoyuki Yazawa, Hideaki Uchiyama, Hideo Saito, Myriam Servières and Guillaume Moreau, 
Japan 
3-24 Hand Shape Recognition Based on Kernel Orthogonal Mutual Subspace Method ·············· 122 
Yasuhiro Ohkawa and Kazuhiro Fukui, Japan 
ⅱ
6-4 Semi-Supervised Spectral Mapping for Enhancing Separation between Classes ················ 187 
Weiwei Du and Kiichi Urahama, Japan 
Session 7: Object Recognition 
7-1 Keypoint Extraction and Selection for Object Recognition ········································· 191 
Maja Rudinac, Boris Lenseigne and Pieter Jonker, Netherlands 
7-2 Scene Classification Using Generalized Local Correlation ········································· 195 
Hideki Nakayama, Tatsuya Harada and Yasuo Kuniyoshi, Japan 
7-3 Unsupervised Learning of Characteristic Object Parts from Videos ······························· 199 
Henrik Skibbe, Alexandra Teynor and Hans Burkhardt, Germany 
7-4 Color Image Classification Using Locally Linear Manifolds and Learning ······················ 203 
Kazuki Kondo and Seiji Hotta, Japan 
Session 8: Poster Session 2 
8-1 Video Watermarking Algorithm Based on Pseudo 3D DCT and Quantization Index Modulation
 ·············································································································· 207 
Hui-Yu Huang, Cheng-Han Yang and Wen-Hsing Hsu, Taiwan 
8-2 Overlap Vehicle Detection by Tracking Horizontal Lines ·········································· 211 
Kazunori Onoguchi, Japan 
8-3 Measuring Conicity from Shape Boundaries ························································· 215 
Milos Stojmenovic and Amiya Nayak, Canada 
8-4 Image Based Search System Using Hierarchical Object Category Recognition Technique ···· 219 
Takuya Minagawa and Hideo Saito, Japan 
8-5 Improvement of Particle Filtering for Intersection of Targets with Similar Patterns ············ 223 
Naoki Enda, Shinji Fukui, Yuji Iwahori and Robert J. Woodham, Japan 
8-6 Voting Based Video Classification Using Clustering and Learning ································ 227 
Kei Kikuchi and Seiji Hotta, Japan 
8-7 Algorithmic Considerations for Real-Time Stereo Vision Applications ··························· 231 
Kristian Ambrosch, Christian Zinner and Wilfried Kubinger, Austria 
8-8 Behavior Recognition with HMM and Feature Analysis using a Wide-view Camera and a PTZ 
camera ····································································································· 235 
Norimichi Ukita, Akihito Kotera and Masatsugu Kidode, Japan 
8-9 A Hardware Accelerator with Variable Pixel Representation & Skip Mode Prediction for Feature 
Point Detection Part of SIFT Algorithm ······························································· 239 
Jingbang Qiu, Tianci Huang, Yiqing Huang and Takeshi Ikenaga, Japan 
8-10 Segmentation of Femoral Head from CT after Femoral Neck Fracture ··························· 243 
Jan Horáek, Lukáš Maršálek, Martin Horák, Philipp Slusallek and Josef Pelikán, Czech 
Republic 
8-11 Medial Features for Superpixel Segmentation ························································ 248 
David Engel, Luciano Spinello, Rudolph Triebel, Roland Siegwart, Heinrich H. Bülthoff and 
Cristóbal Curio, Germany 
ⅳ
8-29 Feature Level Clustering of Large Biometric Database ············································· 324 
Hunny Mehrotra, Dakshina R. Kisku, V. Bhawani Radhika, Banshidhar Majhi and Phalguni 
Gupta, India 
8-30 3D Porous Media Liquid-Solid Interaction Simulation Using SPH Modeling and Tomographic 
Images ····································································································· 328 
Alfonso Gastelum Strozzi, Jorge Marquez, Flavio Trujillo, Celine Duwig, Blanca Prado, 
Pavan Gamage and Patrice Delmas, New Zealand 
Session 9: Invited Talk 2 
9-1 Focal Stack Photography: High-Performance Photography with a Conventional Camera ····· 332 
Prof. Kyros Kutulakos, University of Toronto, Canada 
Session 10: Human Sensing 
10-1 Shape Measurement System of Foot Sole Surface from Flatbed Scanner Image ················ 338 
Sandy Martedi, Hideo Saito and Myriam Servières, Japan 
10-2 Extracting Appearance Information inside the Pupil for Cataract Screening ····················· 342 
Retno Supriyanti, Hitoshi Habe, Masatsugu Kidode and Satoru Nagata, Japan 
10-3 Eye Detection Using Intensity and Appearance Information ······································· 346 
M. Hassaballah and Shun Ido, Japan 
10-4 Eye Blink Based Fatigue Detection for Prevention of Computer Vision Syndrome ············· 350 
Matjaž Divjak and Horst Bischof, Austria 
10-5 Detection of Approaching Pedestrians from a Distance Using Temporal Intensity Patterns···· 354 
Siu-Ming Cheung and Yiu-Sang Moon, Hong Kong 
 
Friday, May 22, 2009 
 
Session 11: Industrial Applications 
11-1 A New Flat Pattern Oriented Order Statistic Filter for Impulse Noise Reduction from Highly 
Corrupted Images ························································································ 358 
Tomohiko Ohtsuka, Homare Sasaki, Masato Suzuki and Hiroyuki Aoki, Japan 
11-2 Multiple Object Detection for Pick-and-Place Applications ········································ 362 
Paolo Piccinini, Andrea Prati and Rita Cucchiara, Italy 
11-3 A Cloth Detection Method Based on Image Wrinkle Feature for Daily Assistive Robots ······ 366 
Kimitoshi Yamazaki and Masayuki Inaba, Japan 
11-4 Carbon Nanotube Detection by Scanning Electron Microscopy ··································· 370 
Tim Wortmann and Sergej Fatikow, Germany 
Session 11: Geographic Information Systems 
12-1 Automatic GIS Updating from High Resolution Satellite Images ································· 374 
Nicolas Champion, Georges Stamon and Marc Pierrot-Deseilligny, France 
ⅵ
13-15 Development of System for Comprehensively Measuring Driving Ability for Elderly Safe 
Driving ····································································································· 443 
Toshiaki Kasukabe, Masatake Hiraoka, Osami Yamamoto, Muneo Yamada, Tomoaki Nakano, 
Shin Yamamoto, Katsumi Matsuda and Mikiko Kawasumi, Japan 
13-16 Generation of Overhead View Images by Estimating Intrinsic and Extrinsic Camera Parameters 
of Multiple Fish-Eye Cameras ·········································································· 447 
Ryota Okutsu, Kenji Terabayashi, Yohei Aragaki, Noriko Shimomura and Kazunori Umeda, 
Japan 
13-17 Image Processing in Retinal Angiography: Extracting Angiographical Features without the 
Requirement of Contrast Agents ······································································· 451 
Tati Rajab Mengko, Astri Handayani, Vanya Vabrina Valindria, Samekta Hadi and Iwan 
Sovani, Indonesia 
13-18 A Human Fall Detection System Using an Omni-Directional Camera in Practical Environments 
for Health Care Applications ············································································ 455 
Yi-Chang Huang, Shaou-Gang Miaou and Tsung-Yen Liao, Taiwan 
13-19 Intrinsic Color Acquisition By Active Color Lighting ··············································· 459 
Hisanaga Fujiwara, Japan 
13-20 Large Area Video Surveillance System with Handoff Scheme among Multiple Cameras ······ 463 
Cheng-Chang Lien, Yue-Min Jiang and Lih-Guong Jang, Taiwan 
13-21 Classification of Wet/Dry Area Based on the Mahalanobis Distance of Feature from Time Space 
Image Analysis ··························································································· 467 
Tomoaki Teshima, Hideo Saito, Masayoshi Shimizu and Akinori Taguchi, Japan 
13-22 Classification of In-Vivo Endomicroscopic Images of the Alveolar Respiratory System ······· 471 
Caroline Petitjean, Jonathan Benoist, Luc Thiberville, Mathieu Salaün and Laurent Heutte, 
France 
13-23 On Face Recognition Using Hierarchical Self-Organized Gabor Features ······················· 475 
Saleh Aly, Naoyuki Tsuruta and Rin-ichiro Taniguchi, Japan 
13-24 Scallop Detection from Gravel-Seabed Images for Fishery Investigation ························ 479 
Koichiro Enomoto, Masashi Toda, Yasuhiro Kuwahara, Masaaki Wada and Katsumori 
Hatanaka, Japan 
13-25 Stereo Vision: A Java-Based Online Platform ························································ 483 
Minh Nguyen, Georgy Gimel'farb and Patrice Delmas, New Zealand 
13-26 Lavatube - A Software Framework for Computer Vision Research and Development ·········· 487 
Kenji Iwata, Yutaka Satoh and Katsuhiko Sakaue, Japan 
Session 14: Invited Talk 3 
14-1 Integration of Earth Observation Data: Challenge of GEOSS (Global Earth Observation System 
of Systems) ································································································ 491 
Prof. Ryosuke Shibasaki, The University of Tokyo, Japan 
Session 15: Machine Vision for Transportation 
15-1 Automatic Extraction of Droppers in Catenary Scenes ·············································· 497 
Caroline Petitjean, Laurent Heutte, Régis Kouadio and Vincent Delcourt, France 
ⅷ
VIDEO WATERMARKING ALGORITHM BASED ON PSEUDO 3D
DCT AND QUANTIZATION INDEX MODULATION
Hui-Yu Huang
National Formosa University, Taiwan
hyhuang@nfu.edu.tw
Cheng-Han Yang, Wen-Hsing Hsu
National Tsing Hua University, Taiwan
ABSTRACT
In this paper, we propose an effective watermarking algo-
rithm based on a pseudo 3D DCT and quantization index
modulation for vide against the attacks. The watermark is
principally inserted into the uncompressed domain by ad-
justing the correlation between DCT coefﬁcients of the se-
lected blocks, and the extraction of watermark is blind. This
approach includes a pseudo 3D DCT, watermark embed-
ding, and extraction. A pseudo 3D DCT obtained by taking
twice DCT transformations will be ﬁrstly utilized to calcu-
late the embedding messages. Using the quantization index
modulation, we insert the watermark into the quantization
regions from the successive frames and record the relative
information to create a secret embedding key. This secret
embedding key will further use to the extraction procedure.
Experimental results demonstrate that our proposed method
can gain a good performance in transparency and robustness
against ﬁltering, compression, and noise attacks.
1. INTRODUCTION
Owing to network technology rapidly advance, humans
can arbitrarily and easily access or distribute any multime-
dia data from networks. Hence, the protection of intellec-
tual property becomes more and more attentive and impor-
tant for the society, Based on this scheme, many methods
are developed [1–4]. Digital watermarking is a favorable
method for copyright protection of the multimedia. It is a
digital code embedded in the host data and typically con-
tains information about origin, status, and/or destination of
the data. Applications of watermarking technique include
copyright protection, ﬁngerprinting, authentication, copy
control, tamper detection, and data hiding applications such
as broadcast monitoring [4].
Many watermarking techniques have been proposed
which was worked in the spatial domain [5] and frequency
domain [1, 6, 7], etc. Lancini et al. [5] proposed a video
watermarking technique in the spatial domain. In this ap-
proach, an important notion is mentioned: the compres-
sion algorithm will strongly decrease the chrominance qual-
ity. Kong et al. [8] proposed a video watermarking based
on Singular Value Decomposition. The watermarks in this
method are embedded in speciﬁcally selected singular val-
ues for the luminance channel of the video frames. Many
of techniques based on the frequency domain contain the
discrete cosine transform (DCT), discrete fourier transform
(DFT), discrete wavelet transform (DWT), quantization in-
dex modulation (QIM) [9–11]. Li and Cox [10] proposed a
watermarking system based on Watson’s perceptual model
to select the quantization step in the QIM method. The
Watson’s model can modify the quantization scale and pro-
vide a QIM algorithm that is invariant to valumetric scal-
ing and further to improve ﬁdelity. Thiemert et al. [12]
designed a block based video watermark system which is
robust against several image processing operations. The au-
thors worked on quantized DCT blocks with size of 8×8 for
luminance channel in MEPG1/2 compressed videos. This
scheme enforces the relationships between block averages
in groups of blocks to represent the embedded binary mes-
sage, and chosen coefﬁcients into each block to represent
the message redundantly. In this paper, we propose an video
watermark system based on the DCT domain to achieve
various attacks and copyright protection. In order to avoid
the distortion of the chrominance quality of video data, we
mainly focus on the luminance component to perform our
embedded system.
The rest of the paper is organized as follows. In Sec-
tion 2, we describe our proposed method. The performance
evaluation will be presented in Section 3. Section 4 presents
the experimental results. Finally, Section 5 gives the brief
conclusions.
2. PROPOSED METHOD
Our proposed system is based on DCT domain. Details
of the whole method are described in the following.
2.1. Pseudo 3D DCT transformation
For video data, we take several successive frames as a
group. Each frame within a group will be divided into a
number of blocks which will be transformed into DCT do-
main by pseudo 3D DCT method. By means of pseudo
3D DCT method, our approach can reduce the computa-
tional complexity. First, we take four consecutive frames
as a group, and every frame within a group is divided into
some of blocks. Next, the DC value of each block located in
the same position of successive frames for a group is trans-
formed into the DCT domain again. After transforming the
second DCT process, we will obtain a new DC value and
several AC values. This procedure is called a pseudo 3D
DCT. The pseudo 3D DCT diagram is shown in Fig. 1. And
the sum of all absolute AC values with weights is expressed
as
Sum(i, k) =
∑
l
Ws(i, k, l)× |AC(i, k, l)|, (1)
where Sum(i, k), Ws(i, k, l), and AC(i, k, l) denote the
sum of all AC values, the corresponding weight value, and
the lth AC value corresponding to the kth blocks of suc-
cessive frames within the ith group, respectively. Here, the
initial wight value can be decided by user. Next, we arrange
these sums and then achieve the embedding process.
For example, we take four frames as a group, and each
of frames will be separated into the 8×8 size of blocks, and
further the block will be ﬁrstly transformed to DCT domain
by DCT method. Then we will pick the DC values of ev-
ery block which locate the same position on the frames and
MVA2009 IAPR Conference on Machine Vision Applications, May 20-22, 2009, Yokohama, JAPAN8-1
207
(a) (b)
(c)
Fig. 3. (a) and (b) Original frames. (c) Watermark image.
MSE =
1
h× w
h∑
y
w∑
x
|S1(x, y)− S2(x, y)|2, (6)
where S1 and S2 denote the corrupted and original images,
respectively. Variables h and w denote the height and width
of the image. For a gray level image, Smax represents 255
gray value. For robustness, we use the NC value to repre-
sent this characteristic. The NC value which measures the
similarity between the original watermark W (i, j) and the
extracted watermark Wˆ (i, j) is given by
NC =
∑w
i=0
∑h
j=0 W (i, j)× Wˆ (i, j)∑w
i=0
∑h
j=0[W (i, j)]2
, (7)
where h and w denote the height and width of the water-
mark.
4. EXPERIMENTAL RESULTS
In the experiments, we used the 720×480 raw video se-
quence with 80 frames and made four frames as a group,
and each frame is divided into the number of 8×8 blocks.
The watermark with the size of 36×20 is prepermuted into
a binary pattern by pseudorandom generator. The thresh-
old is obtained by computed the median of the quantiza-
tion region. Figure 3 shows the testing data and water-
mark. Figure 4 shows the PSNR values of 80 frames com-
paring with our proposed method, Kong et al.’s [8] method
and Thiemert et al.’s [12] method. From these results, it
is clearly obvious that the transparency of our proposed
method is superior to Thiemert et al’.’s and Kong et al.’s re-
sults. For robustness, the compared results for the MPEG-
1 compression are shown in Fig. 5. Figures 6-9 illustrate
the results of different attacks. According to above experi-
ments, it is obvious that our proposed method is more robust
than Kong et al.’s method and Thiemert et al.’s method.
5. CONCLUSIONS
In this paper, we have proposed an effective video wa-
termarking algorithm based on 3D pseudo DCT and QIM
method to achieve the copyright protection in DCT domain.
We use twice DCT method to obtain the embedded infor-
mation and the QIM method to decide the embedded bits
of watermark as previously discussed. Experimental results
demonstrate that our proposed approach is feasible and can
obtain the good performance in transparency and robust-
ness.
Fig. 4. The PSNR values compared with our proposed
method, Kong et al.’s method, and Thiemert et al.’s method.
6. ACKNOWLEDGEMENTS
This work was supported in part by the National Science
Council of Republic of China under Grant No. NSC 95-
2221-E-007-187-MY3 and NSC 97-2221-E-150-065.
References
[1] R. B. Wolfgang, C. I. Podilchuk, and E. J. Delp, “Perceptual
watermarks for digital images and video,” Proceedings of
the IEEE, vol. 87, no. 7, pp. 1108–1126, July 1999.
[2] I. J. Cox, F. J. Kilian, and T. Shamoon, “Secure spread spec-
trum watermarking for multimedia,” IEEE Trans. on Image
Processing, vol. 6, no. 12, pp. 1673–1687, Dec. 1997.
[3] C. T. Hsu and J. L. Wu, “Hidden digital watermarks in im-
ages,” IEEE Trans. on Image Processing, vol. 8, no. 1, pp.
58–68, Jan. 1999.
[4] C. I. Podilchuk and E. J. Delp, “Digital watermarking: algo-
rithms and applications,” IEEE Signal Processing Magazine,
vol. 18, no. 4, pp. 33–46, July 2001.
[5] R. Lancini, F. Mapelli, and S. Tubaro, “A robust video water-
marking technique in the spatial domain,” in Proc. of the 8th
IEEE Int. Symposium on Video/Image Processing and Multi-
media Communications, June 2002, vol. 17, pp. 251–256.
[6] A. S. Lewis and G. Knowles, “Image compression using the
2-D wavelet transform,” IEEE Trans. on Image Processing,
vol. 1, no. 2, pp. 244–250, April 1992.
[7] N. M. Charkari and M. A. Z. Chahooki, “A robust high ca-
pacity watermrking based on DCT and spread spectrum,” in
Proc. of IEEE Int. Symposium on Signal Processing and In-
formation Technology, 2007, pp. 194–197.
[8] W. Kong, B. Yang, D. Wu, and X. Niu, “SVD based blind
vido watermarking algorithm,” in Proc. of the ﬁrst Int. Conf.
on Innovative Computing, Information and Control, 2006,
pp. 265–268.
[9] H. Y. Huang, C. H. Fan, and W. H. Hsu, “An effective water-
mark embedding algorithm for high jpeg compression,” in
Proc. of Machine Vision Applications, May. 2007, pp. 256–
259.
[10] Q Li and I. J. Cox, “Using perceptual models to improve ﬁ-
delity and provide resistance to valumetric scaling for quan-
tization index modulation watermarking,” IEEE Trans. on
Infor. Forensics and Security, vol. 2, no. 2, pp. 127–139, June
2007.
[11] B. Chen and G. W. Wornell, “Quantization index modula-
tion: a class of provably good methods for digital water-
marking and information embeeding,” IEEE Trans. on In-
formation Theory, vol. 47, no. 4, pp. 1423–1443, May 2001.
[12] S. Thiemert, T. Vogel, J. Dittmann, and M. Steinebach, “A
high-capacity block based vido watermark,” in Proc. of the
30th Euromicro Conference, 2004, pp. 457–460.
209
