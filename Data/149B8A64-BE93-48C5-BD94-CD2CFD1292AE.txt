 1
中文摘要 
 
本論文提出一個應用在 H.264/AVC 上的高效率的錯誤還原方法，在本論文中利用動作
向量搭配物件的觀念將遺失的區塊像素值以及動作向量還原。在 H.264 解碼的過程中，每
一個 4x4 像素的小區塊(block)都會存入一個動作向量(Motion Vector)，當遇到遺失的區
域時，我們將遺失區域周圍的區塊找出來，並且用 
周圍區塊的動作向量來找出與前一張畫面中對應的區塊，藉由尋找前一張畫面區塊的動作
向量相似且連接的區塊，將動作向量相似且連接在一起的區塊設定為一個物件，利用物件
整體移動的特性將物件像素拿來回填遺失區域的像素，同時連同動作向量一併還原。動作
向量的還原是有必要的，因為下一個巨方塊的動作向量會使用本方塊之動作向量做為預測
值 
實驗數據顯示我們的演算法能提供比目前已提出的演算法更好的解決方案，並且更適
合人眼視覺上感受，而在計算時間方面仍然能保持即時解碼的效率。 
 3
目錄 
 
Chapter 1 Introduction.................................................................................................................. 4 
Chapter 2 Background and Related Work .................................................................................... 5 
2.1 Error Concealment ............................................................................................................. 5 
2.2 Motion vector recovery algorithm using polynomial model.............................................. 7 
Chapter 3 The Proposed Object-based Error Concealment Algorithm....................................... 10 
3.1 Select 4x4 reliable MVs in blocks near damage slice...................................................... 11 
3.2 Search object from previous frame and damage slice recovery....................................... 11 
3.3 Motion vector recovery .................................................................................................... 13 
3.4 Action flow....................................................................................................................... 13 
Chapter 4 Simulation Results ..................................................................................................... 15 
4.1 Test environment .............................................................................................................. 15 
4.2 The results ........................................................................................................................ 17 
4.3 The advantage of object-base method.............................................................................. 23 
4.3.1 Compare with Polynomial Module algorithm....................................................... 23 
4.3.2 The advantage of object-based module for visual effect....................................... 24 
4.3.3 Conceal two continues slices ................................................................................ 25 
Chapter 5 ........................................................................................................................................ 28 
Conclusions .................................................................................................................................... 28 
Chapter 6 Acknowledgment ....................................................................................................... 28 
 5
 
Chapter 2  
Background and Related Work 
2.1 Error Concealment  
Error Concealment is very important for real time video transmission. Fig 2.1 shows concealed 
frame V.S. no concealed frame. The gray region is a damage slice. Error concealment algorithm is 
used to recover this region. 
 
Fig. 2.1 Error concealment V.S no concealment 
Generally, error concealment algorithms are divided into two modes, spatial error concealment 
and temporal error concealment. In spatial error concealment, lost block is recovered by the 
neighboring blocks. It usually uses in intra frame recovery. In temporal error concealment, lost 
block is recovered by motion vector which points to reference frame. Because it is hard to find 
the correct motion vector of lost block, temporal error concealment is more complex than spatial 
error concealment. Normally, the information in temporal domain usually keeps more than in 
spatial domain. Temporal error concealment is usually used in inter frame. 
Many efficient error concealment algorithms have been proposed. The common sense uses zero 
motion, which uses the same position block of the last frame to replace damaged block. In this 
Thesis, we present a motion vector recovery algorithm for H.264 decoding. 
Temporal error concealment uses motion vector to recover the information of lost block which 
shows in Fig. 2.2  
 7
combining temporal replacement with motion estimation, can be used to improve the 
effectiveness of concealment. Here lists some of these algorithm details. 
Although the MVs of neighbor blocks are usually closed to true motion, the choosing methods 
are not good enough. 
2.2 Motion vector recovery algorithm using polynomial model 
Motion vector recovery algorithm using polynomial module is the method used to recover the 
damaged regions that can’t be recovered by our proposed algorithm. The method is shown as 
follows. 
During the encoding procedure, each picture is divided into a group of macroblocks called slices. 
The header of each slice includes synchronization signal. The main performance of slice is to 
prevent the effects of channel error propagation. Normally, the neighboring macroblocks in 
horizontal direction will be assigned to the same slice. If a packet lost, only the neighboring 
macroblock in vertical direction can be used to reconstruct the lost macroblocks, as the 
neighboring macroblocks in horizontal direction are not available. 
In the decoder of H.264, a record will be generated during the decoding procedure. When a slice 
is available for decoding, the macroblocks residing in this slice will be denoted as “correctly 
received”. Otherwise, the macroblock will be labeled as “lost”. By use this record, the location of 
lost macroblock can be found easily. The lost macroblock is denoted as Fi,j. In Fig 2.11, the lost 
macroblock is indicated by the dark square, and the grey squares indicate the macroblocks which 
motion vectors will be used to estimate the lost motion vectors. 
 
Fig. 2.4 Location of lost macroblock and its neighbor macroblocks 
As shown in Fig 2.12, the Mv1,…,Mv4 represent the motion vectors that cover 4 x 4 blocks in 
lost macroblock, and  ,…,  denote the motion vectors in left neighboring macroblock  . As the 
distance between two motion vectors increases, the correlation between them will decrease. The 
motion vectors in left neighboring macroblock are selected to recover the motion vectors that 
cover left part of lost macroblock. For simplicity, we only present how to use ,…,   to recover 
Mv1 and Mv2. The other motion vectors in lost macroblock can be recovered by reiterating the 
same approach. 
 9
suitable for this application. When the movement in neighboring macroblock is linear, the 
coefficient of the second-order item in second-order polynomial will approach to zero, then the 
second-order polynomial will approximate to first-order polynomial. Based on the correlation of 
neighboring motion vectors, we can use the second-order polynomial to estimate the motion 
vectors, Mv1 and Mv2, in lost macroblock. 
 11
MV
Damage Slice
Current frame
4x4 block
Recover object by MV
Reference frame
Moving together 
blocks
 
Fig. 3.1 Idea of this object-based algorithm 
In Fig3.1, left side is the reference frame and the right side is the damage slice in current 
frame. The correctly decoded 4x4 block neighboring the damage slice keeps the correct Motion 
Vector (MV) which points to the reference block in the previous frame. The object which 
includes the reference block should be found and be used to recovery the damage slice. 
3.1 Select 4x4 reliable MVs in blocks near damage slice 
At the first, all 4x4 blocks near the damage slice will be marked up. Information of these blocks 
will be used in this conceal algorithm.  
The test sequence is in QCIF (176 x144) format, each frame includes 13 slices and each slice 
includes eight 16x16 macroblock. Each16x16 macroblock includes sixteen 4x4 sub-blocks. In 
this Thesis, 4x4 sub-block is the base block. 
The best way to find the reference object for the damage slice is to search from the blocks which 
are neighbor of the damage slice.  
Generally, the low-reliable MV is much different from its neighbor MV and usually happens 
in a small local region. The method to check whether the block MV is acceptable is to compare 
with MVs of neighbor blocks. We set a threshold to test whether the MV is much different from 
neighbor MVs. If the difference is lower than the threshold, we set this MV is reliable. 
3.2 Search object from previous frame and damage slice recovery 
Define B(i,j) is the neighbor block of damage slice and its MV is reliable MV. B(i’,j’) is the block 
in previous frame referenced by MV of B(i,j). 
i’=i+MVy(B(i,j)); 
j’=j+MVx(B(i,j))  
If blocks are in the same object, the MV difference between them should be small. If MV 
 13
may be not true motion, neighbor MV is still useful to recover this region. However, we don’t 
know which neighbor MV is the best choice. At last, the Polynomial Module mentioned in 
chapter 2 is chosen to solve this situation. 
3.3 Motion vector recovery  
Besides the lost pixel value, the lost MV also should be recovered for MV Predictive Motion 
Vector (pmv) calculating. Current block MV is used as PMV for next block. The MV of each 4x4 
block are calculated as follow： 
MV = PMV + decoded mv 
MV of every 4x4 block in lost slice will be recovered. Fig 3.4 shows a matching situation. 
Because this algorithm uses the performance frame block which is pointed by block in current 
frame block to recover lost pixel value, so a block would cover 1 to 4 blocks in reference frame. 
For the example in Fig 3.5, the referenced block points to a block across 4 reference frame blocks 
in current frame. 
 
Fig. 3.4 A block point to 4 blocks 
 
Fig. 3.5 MV recovery 
When the reference frame is an Intra frame. Intra frame doesn’t have motion vector to refer to 
previous frame. Since there is no motion information, the object can’t be found using MVs. In 
this situation, we use the Polynomial Model to recover MVs and pixel value. Because Polynomial 
Model is a block base algorithm, it is easy to find motion vector. 
3.4 Action flow 
1. Mark all 4x4 blocks neighbor damage slice 
2. Check for reliable MVs 
3. Find the starting block from previous frame by MV 
 15
 
Chapter 4  
Simulation Results 
4.1 Test environment 
We implement the error concealment method into the Joint Model (JM10.1) of H.264 software 
[15]. Error concealment is often used in real-time video communication. It is noted that the video 
sizes used in mobile equipment is very small. In test environment, QCIF (176 x 144) format 
videos are selected as test video. All are color videos and the color smapling formats are 4:2:0 
YUV video.  
A slice stores 8 macroblocks and each macroblock is in size of 16x16. Each frame includes 10 
columns and 13 slices. The first twelve slices include full 8 macroblock and the 13th slice 
includes only 3 macroblocks. The slice would lose randomly in decoding time. These lost slices 
would be marked up. After the current frame is completely decoded the slices will be concealed. 
In Fig. 4.1, it shows a frame with 13 slice and one slice (slice 5) lose is showed in Fig 4.2. 
 17
4.2 The results 
The simulation results are shown in Table 4.1 
 
A special situation is continues lost slices 16% that couldn’t be implement by Neighbor MV 
Recovery and Polynomial Module. If the lost slices are continues, it would lost more than one 
column in a frame. Because these methods couldn’t solve this situation, it is tested by proposed 
algorithm and compared with zero motion method. The detail of the simulation “Continues lost 
slice16%” would be discussed in 4.3.3.
 19
 
Table 4-2 Simulation results in ‘Mobile’ sequence IPPPIPPP... 
Video 
sequence 
QP Bitrate Original
    (Kbit/s) PSNR
PSNR of Different Macroblock Lost rate 
      (db) Continues 
        Method 5% 8% 16% lost slice16%
Mobile 24 1240.9 36.96 Proposed 34.18 32.71 31.24 31.91 
      Polynomial 33.69 32.16 30.40   
IPPPPIPPP…     Neighbor 33.20 31.41 28.36   
      Zero 32.75 30.67 27.55 27.96 
      None 25.33 20.35 17.84 19.92 
  30 673.68 31.72 Proposed 30.28 29.81 28.87 28.54 
      Polynomial 29.80 29.13 28.13   
      Neighbor 29.12 27.02 25.97   
      Zero 28.93 26.15 24.70 24.57 
      None 22.81 18.49 16.63 16.50 
 
5%  macroblock lost
31.00
32.00
33.00
34.00
35.00
36.00
37.00
24 30 36
QP
PS
NR
Proposed
Polynomial
Neighbor
Zero
 
Fig. 4.3 5% Slice lost 
 21
5%  macroblock lost
28.00
29.00
30.00
31.00
32.00
33.00
34.00
35.00
24 30 36
QP
PS
NR
Proposed
Polynomial
Neighbor
Zero
 
Fig. 4.6 5% Slice lost 
8%  macroblock lost
27.00
28.00
29.00
30.00
31.00
32.00
33.00
34.00
24 30 36
QP
PS
NR
Proposed
Polynomial
Neighbor
Zero
 
Fig. 4.7 8% Slice los 
 23
 
4.3 The advantage of object-base method 
Here are some advantages than other algorithms. Object-base algorithm is good in visual effect 
and it provides some advantages beside high PSNR. 
In Fig4.3－Fig4.5 In frame structure IPPPIPPPI…Shows the simulation results under 5%, 8%, 
16% slice damage rate. 
4.3.1 Compare with Polynomial Module algorithm 
 
Fig. 4.9 First Comparison with Polynomial Module 
In Fig. 4.9, (b) is concealed by polynomial module and (a) is concealed by proposed method. 
Gray lines mean the motion vector of 4x4 blocks and the black point is end of motion vector. The 
motion vectors under white cycle in Fig. 4.9 (b) are disorder. By polynomial module, the disorder 
MVs would recover incorrect MVs in damage slice. Blocks in white cycle are recovered by 
incorrect MV. Each recovered block has no spatial relation with neighboring blocks because the 
MV of each recovered block is different. Fig. 4.9 (a) doesn’t have this discomfort because the 
blocks of this region are set as an object and recovered together. 
In Fig. 4.10, right two pictures show a small region concealed by our proposed algorithm and 
polynomial module in the 14th frame of Foreman sequence. The small region is a part of building 
which is in stable region of the sequence, the true-motion in this region is zero motion. However, 
the MVs in this region are encoded to incorrect motion by H.264 encoder. The motion estimation 
process compares the pixel value in neighboring region in previous frame and chooses the 
location with minimum distortion as the motion vector. Although these MVs can’t be promised as 
the true motion, the pixel values are still similar. In Fig. 4.10 (b), my proposed method uses a 
incorrect MV to recover the damage slice, but it looks like a correct recovered region because of 
the pixel values are similar to each other. In Fig. 4.10(c), polynomial module uses the MVs 
 25
 
Fig. 4.11 A disorder MV region recovered by different method 
The picture in Fig 4.11 (a) is concealed by proposed algorithm and the picture in Fig 4.11 (b) is 
concealed by polynomial module. Because of the disorder MVs, the recovered blocks in Fig. 4.11 
(b) are disorder and the foreman’s eye looks nausea. The eye in Fig. 4.11(a) isn’t recovered 
correctly, but the object in this region is complete. It is good for human vision.  
4.3.3 Conceal two continues slices  
In other error concealment algorithms, it is hard to recover two continues slices because of not 
enough neighboring information provided by blocks. It is possible to conceal two continues slices 
by proposed algorithm. Fig. 4.12 (a) shows a non-concealed image in foreman and Fig.4.12 (b) 
shows the image concealed by proposed method. Fig. 4.12 (c) shows the image concealed by zero 
motion method. 
 27
16%  macroblock lost
29.00
30.00
31.00
32.00
33.00
34.00
35.00
24 30 36
QP
PS
NR
Proposed
Zero
 
Fig. 4.13 Continues slices lost in IPPPIPPPI… frame structure 
16%  macroblock lost
26.00
27.00
28.00
29.00
30.00
31.00
32.00
24 30 36
QP
PS
NR
Proposed
Zero
 
Fig. 4.14 Continues slices lost in IPPPPPPPI… frame structure 
 29
參考文獻 
[1] Y. Wang, Q. F. Zhu, and L. Shaw, “Maximally smooth image recovery in transform coding,” 
IEEE Trans. Commun., vol. 41, no. 10, pp. 1544–1551, Oct. 1993. 
[2] J.W. Park, J.W. Kim, and S. U. Lee, “DCT coefficients recovery-based error concealment 
technique and its application to the MPEG-2 bit stream error,” IEEE Trans. Circuits Syst. Video 
Technol., vol. 7, pp. 845–854, Dec. 1997. 
[3] Y. Wang and Q. F. Zhu, “Error control and concealment for video communication: A 
review,” Proc. IEEE, vol. 86, no. 5, pp. 974–997, May 1998. 
[4] J. W. Suh and Y. S. Ho, “Error concealment techniques for digital TV,” IEEE Trans. 
Broadcast., vol. 48, pp. 299–306, Dec. 2002. 
[5] Z.Wang, Y.Yu, and D. Zhang, “Best neighboring matching: An information loss restoration 
technique for block-based image coding systems,” IEEE Trans. Image Processing, vol. 7, no. 7, 
pp. 1056–1061, Jul. 1998. 
[6] S. Tsekeridou and I. Pitas, “MPEG-2 error concealment based on blockmatching principles,” 
IEEE Trans. Circuits Syst. Video Technol., vol. 10, pp. 646–658, Jun. 2000. 
[7] Yanling Xu; Yuanhua Zhou;” H.264 video communication based refined error concealment 
schemes” Consumer Electronics, IEEE Transactions on Volume 50,  Page(s):1135 – 1141  Nov. 
2004  
[8] Y. Zhang and K.-K. Ma, “Error concealment for video transmission with dual multiscale 
Markov random field modeling,” IEEE Trans. Image Processing, vol. 12, no. 2, pp. 236–242, 
Feb. 2003. 
[9] M. Bystrom, V. Parthasarathy, and J. W. Modestino, “Hybrid error concealment schemes for 
broadcast video transmission over ATM networks,” IEEE Trans. Circuits Syst. Video Technol., 
vol. 9, pp. 868–881, Sep. 1999. 
[10] S. Cen and P. C. Cosman, “Decision trees for error concealment in video decoding,” IEEE 
Trans. Multimedia, vol. 5, no. 1, pp. 1–7, Mar. 2003. 
[11] X. Li and M. T. Orchard, “Novel sequential error-concealment techniques using orientation 
adaptive interpolation,” IEEE Trans. Circuits Syst. Video Technol., vol. 12, pp. 857–864, Oct. 
2002. 
[12] Mei-juan Chen; Che-Shing Chen; Ming-Chieh Chi; “Temporal error concealment algorithm 
by recursive block-matching principle” Circuits and Systems for Video Technology, IEEE 
Transactions on Volume 15,  Page(s):1385 - 1393 Nov. 2005 
[13] Jinghong Zheng; Lap-Pui Chau; “Efficient motion vector recovery algorithm for H.264 
based on a polynomial model” Multimedia, IEEE Transactions on 
Volume 7,  Page(s):507 – 513 June 2005 
[14] Y.Wang, M. M. Hannuksela,V.Varsa, A. Hourunranta, and M. Gabbouj, “The error 
concealment feature in the H.26L test model,” in Proc. Int. Conf. Image Processing, vol. 2, pp. 
729–732. Sep. 2002, 
[15]M. Flierl, T. Wiegand, B. Girod, “Rate-constrained multihtpothesis prediction for 
motion-compensated video compensated video compression”, IEEE Trans. Curcits System Video 
 31
 
計畫成果自評 
一、 研究內容與原計畫相符程度 
【第一年】 
主要了解 H.264 的整個程式碼，並針對 intra 與 inter 的預測編碼(Predictive coding)做探
討，模式(mode)的選擇與運算複雜度的簡化，同時針對 SI 及 SP 進行測點研究以便提出更
有效率之視訊串流技術。 
【第二年】 
開始撰寫並且最佳化功能區塊的軟體程式，將視訊壓縮系統中的各個功能區塊一一對
應到平台上的處理器矽核後，可開始依照子計畫一所定義完成的指令集，來撰寫各個功能
區塊執行所需的軟體程式。將根據原先完成的 VC++程式，改寫成可在子計畫二所提出處
理器矽核上執行，同時與子計畫(一)、(二)密切聯繫，隨時注意並觀察是否需要為了提高視
訊處理效能，再次修正處理器矽核硬體電路(子計畫二)並增加特殊的指令(子計畫一)。 
【第三年】 
主要是整合前兩年的研究成果，進而在模擬平台上完成以 H.264 為基礎之視訊串流系
統。此外並將提供以完成之高效率、高彈性及低功率之視訊編解碼演算技術給其他分項計
劃，完成處理器矽核各功能區塊模組之建構。 
二、 達成預期目標情況 
【第一年】 
(1) 針對目前國內、外的視訊串流技術之效能及運算複雜度進行研究與比較。 
(2) 針對 H.264 之高壓縮率及視訊編碼技術進行研究。  
(3) 針對 H.264 中幾個主要運算步驟所需的高計算複雜度進行評估。 
(4) 開發改進 H.264 高計算複雜度之編碼技術。 
(5) 開發 H.264 應用於視訊串流編碼的有效演算技術。 
【第二年】 
(1) 根據第一年所研發之降低 H.264 計算複雜度之結果，進而完成更具彈性的視訊壓縮技
術及編解碼系統。 
(2) 改良第一年所研發出之視訊串流編碼技術，開發更有效率之視訊位元率控制(bit rate 
control)技術。  
(3) 針對已完成之高效率視訊壓縮各功能區塊進行軟、硬體程式撰寫與模組化。  
(4) 配合其他分項計畫，提供低功率(低計算複雜度、高效率)之矽核研製編譯演算法技術。 
【第三年】 
   
Abstract—This approach proposed a system 
for the recognition of the facial expression, 
which can be using cross-correlation of optical 
flow and mathematical models from the facial 
points. That defined these facial points of 
interest in the first frame of an input face 
sequence image, which utilized manually marker. 
The facial points were automatically tracked by 
using a cross-correlation based on optical flow, 
and extracted feature vectors. The mathematical 
model extracted features from feature vectors. 
We were used to classifying expressions when an 
ELMAN neural network was used. The 
performances of the proposed facial expressions 
recognition were computed using Cohn–Kanade 
facial expressions database. The proposed 
method achieved a high recognition rate. 
 
Keyword—facial expression recognition, optical flow, 
mathematical model, neural network 
I. INTRODUCTION 
he face plays a crucial role in interpersonal 
communication. By seeing a face, we can 
recognize a person’s identity, gender, age, 
expression, etc. This information is irreplaceable for 
the normal conduct of human communications. If 
machines could recognize such information from a 
human face, humans and machines might thereby 
communicate more smoothly, robustly, and 
 
 
harmoniously. 
The face can send many subtle signals. For 
example, an array of facial expressions—a smile of 
happiness, a frown of sadness or disapproval, the 
wide-open eyes of surprise, or a lip curled in 
disgust—all show a wide range of emotions. 
Research by Ekman et al. [1] has identified six 
basic emotions that people can identify from facial 
expressions with high accuracy. 
The Facial Action Coding System (FACS) [2, 3] 
is currently the most widely used method in 
recognizing facial expressions. FACS encodes the 
contraction of each facial muscle (stand alone as 
well as in combination with other muscles) changes 
the appearance of face and has been used widely for 
the measurement of shown emotions.  
Gizatdinova and Surakka [4] used feature-based 
method for detecting landmarks from facial images. 
The method was based on extracting oriented edges 
and constructing edge maps at two resolution levels. 
Edge regions with characteristic edge pattern 
formed landmark candidates. 
An optical-flow based approach [5] is sensitive to 
subtle changes in facial expression. Action unit (AU) 
[6, 7] combinations in the brow and mouth regions 
were selected for analysis if they occurred at a 
minimum of twenty-five times in the database. 
Selected facial features were automatically tracked 
by using a hierarchical algorithm for estimating 
optical flow. Image sequences were randomly 
divided into training and test sets. 
We proposed seven features mathematical models 
Facial Expression Recognition using Neural 
Network 
Shen-Chuan Tai                           Hung-Fu Huang*                      Kuo-Chen Chung                 
sctai@mail.ncku.edu.tw            hhf93d@lily.ee.ncku.edu.tw    zgz@lily.ee.ncku.edu.tw 
 
 
Yu-Yi Liao         Chien-Shiang Hong                     
lyy94d@lily.ee.ncku.edu.tw    hcs95d@dcmc.ee.ncku.edu.tw 
 
Department of Electrical Engineering, National Cheng Kung University  
Tainan, Taiwan, R.O.C. 
T 
附件
 z Philtrum:  
 
Y13-Y16                   (6) 
 
z Eye to Cheek Distance:  
 
((Y11-Y13)+(Y12-Y13))/2          (7) 
 
where X is x-axis and Y is y-axis in the two 
dimension area. 
IV. THE ELMAN NEURAL NETWORK MODEL 
All features were used to classify that expression 
to one of the six basic emotions. We used ELMAN 
neural network system [10-13]. It belongs to 
recurrent networks which differentiate other 
networks, and it is the additional connection from 
the hidden unit to itself. The recurrent connection 
allows detecting and generating time-varying 
patterns. The simplest method of recurrent network 
is ELMAN network. ELMAN network at each time 
step, a copy of the hidden layer units is made to a 
copy layer.  
We used two hidden layers and the number of 
input layer units must be equal to the number of 
extracted features and the output layers correspond 
to six kinds of facial expressions. The highest value 
will be indicated to the corresponding facial 
expression. The network architecture is shown as 
Fig. 3.  
V. SIMULATION RESULT 
1) Data set 
The Cohn-Kanade facial expression database [14] 
consists of 100 university students aged from 18 to 
30 years, of which 65% were female, 15% were 
African-American, and 3% were Asian or Latino. 
Subjects were instructed to perform a series of 23 
facial displays, and six of which were based on 
descriptions of prototypic emotions (i.e., anger, 
disgust, fear, happy, sad, and surprise). For each 
person there are on average twelve frames for each 
expression. Image sequences from neutral to target 
display were digitized into 640x490 pixel arrays. 
 
 
 
2)  Result 
To show the efficiency of our method, extensive 
experiments are done on the Cohn-Kanade facial 
expression database to test, and to train which fifty-
five subjects are available to us. Since several 
subjects are not to contain six facial expressions, 
the choice of training and testing set obeys 
following rules: (1) the subject that has more or 
equal to four expressions is selected as training set 
(forty-five subjects); (2) the subject that has more or 
equal to four expressions is selected as testing set 
(ten subjects); (3) the test was repeated five times; 
(4) each time changed different testing and training 
subjects. Table 1 shows the recognition rate.  
VI. CONCLUSION 
The automatic facial expressions recognition of 
novel expressers is a difficult job for machines. In 
the paper, we developed a method for the estimation 
of the facial features, which used manually marker 
facial points. Then, the cross-correlation of optical 
flow does facial point tracking and mathematical 
models do feature extraction from the facial points.  
 
 
F1  
F4 
F7 
Angry 
Happy 
Surprise 
Disgust 
Fear 
Sad 
Input 
Layer 
Hidden 
Layer1 
Hidden 
Layer2 
Output 
Layer 
Fig. 3 The ELMAN neural network architecture
