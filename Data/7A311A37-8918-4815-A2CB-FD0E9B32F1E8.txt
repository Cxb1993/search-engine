 I
目錄 
中文摘要.......................................................................................................................................... 1 
英文摘要.......................................................................................................................................... 1 
1. 前言............................................................................................................................................. 2 
2. GMDH演算法............................................................................................................................. 3 
2.1 GMDH的學習演算法....................................................................................................... 4 
3. 方法............................................................................................................................................. 5 
4. 實驗成果..................................................................................................................................... 6 
4.1 SOPNN工具箱.................................................................................................................. 7 
4.2 Mackey-Glass時間序列(TP 1) ......................................................................................... 7 
4.3 The Logistic Map時間序列(TP 2) .................................................................................... 8 
4.4 比較................................................................................................................................... 8 
5. 結論............................................................................................................................................. 8 
6. 計畫成果自評............................................................................................................................. 8 
參考文獻.......................................................................................................................................... 9 
 
附錄：已發表的研討會論文 ( 本計畫的部分研究成果已於 2009年 5月 23日發表於「第 20
屆國際資訊管理學術研討會-資訊科技與創新管理」 ) 
 
 
 2
1. 前言 
神經計算(neuro-computing, NC)是計算
智慧(computational intelligence)家族中的一
個成員，其它的成員包括模糊邏輯(fuzzy 
logic)、基因演算法(genetic algorithm)、演化
規劃(evolutionary programming)、昆蟲智慧
(swarm intelligence)與人工生命(artificial life)
等 。 NC 有 許 多 學 習 演 算 法 (learning 
algorithms)。例如：監督式(supervised)、非
監督式(unsupervised)與競爭式(competitive)
等法則，其中監督式法則可以藉由最小均方
演算法(least mean square algorithm)精準地
塑模資料的成對輸入與輸出之關係[5]。本計
畫使用監督式法則。 
資料庫知識發掘(knowledge discovery 
in databases, KDD)的過程可以將低階的資
料轉換成高階的知識。KDD的過程是由資料
準備 (data preparation)、資料選擇 (data 
selection)、資料清理 (data cleaning)、資料
整 合 (incorporation of appropriate prior 
knowledge)、資料探勘(data mining, DM)和
DM結果的適當解釋(proper interpretation of 
the data mining results)等所組成。DM是KDD
過程中的核心技術並且已經被使用於許多
應用上。例如：預測 (forecasting)、摘要
(summarization)、分類(classification)、分群
(clustering)、規則歸納(rule generation)和序
列分析等(sequence analysis)[7]。本計畫著重
發展NC的預測技術。 
商業智慧(business intelligence, BI)是一
個結合資料獲取(data gathering)、資料儲存
(data storage)和資料分析以提供從輸入到決
策過程的一種資料驅動系統 (data-driven 
system)。BI系統藉由系統化的獲得、校對分
析和資訊的解釋與探索，讓企業能瞭解其內
部與外部環境[15]。BI系統所包含的元件
有 ： 線 上 分 析 處 理 (online analytical 
process) 、 知 識 管 理 (knowledge 
management)、顧客關係管理 (customer 
relationship management)/ 資 料 庫 行 銷
(database marketing)、資料庫探勘(database 
mining)、視覺化資料(visualization)、決策支
援系統(decision support system)/高階主管資
訊系統(executive information system)、DM與
地 理 資 訊 系 統 (geographic information 
system) [1]，如圖1所示。因此，BI系統可
視為一群資訊科技的應用。本計畫著重於
DM技術的開發。 
 
地理資訊系統
(GIS)
資料探勘
(DM)決策支援系統
資料庫探勘
線上分析處理
(OLAP) 知識管理
顧客關係管理/
資料庫行銷
視覺化資料
 
圖 1  BI系統的架構圖 
 
應用於時間序列分析 (time series 
analysis)的技術可分成兩類：統計方法與頻
譜分析 (spectral analysis) [2, 13]。時間序列
迴歸  (time series regression)、指數平滑 
(exponential smoothing) 與自我迴歸整合移
動平均  (autoregressive integrated moving 
average, ARIMA) 等傳統的統計方法，是以
線性模式為基礎。這些方法假設：時間序列
的未來值是線性地相關於歷史的觀察值。線
性統計預測技術的優點為 (1)容易使用與執
行及(2)容易瞭解與解釋。不幸的是，使用線
性方法預測具有非線性行為的真實世界的
問題時，它們是失效的。因此，許多非線性
的時間序列技術被發展出來以增進對於預
測非線性系統的性能(performance)。這些技
術有雙線性模式 (bilinear model)、門檻自我
迴歸模式 (threshold autoregressive model)、
平 滑 轉 換 自 我 迴 歸 模 式  (smoothing 
transition autoregressive model)、自我迴歸條
件 異 方 差  (autoregressive conditional 
heteroscedastic model)與一般性自我迴歸條
件 異 方 差 (generalized autoregressive 
 4
es es,0 es,1 es,2 es,3 es,4 es,5[ , , , , , ]
Tβ β β β β β=β 被計算出
來後，在第 k層中第m個神經元之第 j個輸
出 kmjoy , 可由式(2.3)獲得，如下所示： 
2/)1(,,2,1,,,2,1,,,2,1
,
maxtr
5,es
2
4,es
2
3,es2,es1,es0,es,
−===
+++++=
NNmkknj
xxxxxxy wjvjwjvjwjvj
km
jo
………
ββββββ  
(2.3) 
其中 
maxk = 最大的演化層數 
 
…
…
1x 2x 1−Nx Nx
maxk
 
圖 2  GMDH演算法的網路拓樸 
在評估 ,d jy 與 ,kmo jy 的誤差之後，在第 k
層中第m個神經元所獲得的誤差將與在第
k 層中一個事先定義的門檻值( 誤差 )比
較，只有具有比門檻值較小誤差的神經元可
以通過，並成為下一層的輸入變數；然而，
具有比門檻值較大誤差的神經元將從該層
中被剔除。此自組織網路拓樸的形成過程相
似於自然演化(natural evolution)。 
 
 2.1 GMDH的學習演算法 
 
此學習過程之目的為找到 GMDH 演算
法的網路拓樸以及每一個神經元的 esβ ，網
路拓樸的產生主要可以分成三個步驟，說明
如下。 
步驟一： esβ 的估計 
在 GMDH 的學習演算法中，為了避免
過度學習(over-learning)的情形發生，會將資
料集畫分成兩個子資料集，分別是學習資料
集(training data set)與查核資料集(checking 
data set)，其中查核資料集的個數定義為
chn 。學習資料集用於推估 esβ ，而查核資料
集則在步驟二扮演篩選新產生的神經元的
角色。 
本層(current layer)的神經元個數是根
據前一層(preceding layer)的輸入變數的個
數而決定的，可使用式(2.2)計算得到。每一
個神經元只連接兩個輸入變數，並且有一個
輸出( 此輸出即為下一層的輸入變數 )。欲
推估 esβ 可使用式(2.4)的矩陣計算而獲得。 
1
es ( )
T T
d
−=β X X X Y       (2.4) 
其中 
1 2 3[ , , , ]
T
d d d dy y y=Y " ，目標輸出的矩陣 
tr tr tr tr tr tr
2 2
1 1 1 1 1 1
2 2
2 2 2 2 2 2
2 2
1
1
1
v w v w v w
v w v w v w
vn wn vn wn vn wn
x x x x x x
x x x x x x
x x x x x x
⎡ ⎤⎢ ⎥⎢ ⎥= ⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
X # # # # # #  
，設計矩陣 (design matrix) 
在式(2.4)中，如果 TX X為非奇異矩陣
(non-singular matrix)，則它的逆矩陣存在，
esβ 可以順利獲得，並配適一個多項式迴歸
模型如式(2.3)所示，藉由此多項式迴歸模
型，可以產生一個新的神經元，此神經元的
輸出變數是由 ,kmo jy 所組成的行向量。如果
TX X為奇異矩陣(singular matrix)，則它的逆
矩陣將不存在，導致無法獲得 esβ ，進而無
法產生新的神經元。 
步驟二：神經元的擇優 
此步驟之目的是針對在步驟一中所產
生的神經元進行篩選。GMDH 演算法採用
正規準則 (regularity criterion) [3] 執行神經
元的剪除(pruning)，如式(2.5)所示： 
2/)1(,,2,1,,,2,1,
)(
max
1
2
,
1
2
,,
2
ch
tr
chtr
tr −==
−
=
∑
∑
+=
+
+= NNmkk
y
yy
r n
nj
jd
nn
nj
km
jojd
km ……
 
(2.5) 
 6
步驟三：神經元的產生 
在 SOPNN中，本層中的神經元個數可
使用式(2.2)計算而獲得。隨後，利用式(2.4)
的矩陣運算推估每一個神經元的 esβ ，並且
根據式(2.3)產生每一個神經元之輸出。 
步驟四：神經元的擇優  
本計畫所提出的 SOPNN是採用多項式
迴歸模型。因此，SOPNN 採用複判定係數
(coefficient of multiple determination)做為性
能指標(performance index)，如下所示： 
2/)1(,,2,1,,,2,1,
)-(
)-(
max
1
2''
,
1
2'
,
2
ch
ch
−===
∑
∑
=
= NNmkk
yy
yy
R n
j
djd
n
j
d
km
jo
km ……
 
(3.2) 
其中 
2
kmR = 在第 k層中第m個神經元的複判定係
數 
'
dy = ch
' ' '
,1 ,2 ,[ , ]
T
d d d ny y y… 的平均值 
'
dy = 正規化後目標輸出向量 
2
kmR 用於說明神經元所配適之迴歸模型
的適合性。 2kmR 介於 0與 1之間， 2kmR 越接近
1表示，該神經元所配適的迴歸模型對於用
於建立模型的輸入樣式的解釋能力越高。如
果 2kmR 大於 kσ ，則該神經元可以通過，並且
成為下一層的輸入變數。反之，如果 2kmR 小
於或等於 kσ ，則該神經元將在 SOPNN的網
路拓樸中被刪除。 
步驟五：提早終止網路的學習 
SOPNN 使用一個準則提早停止網路學
習的運算元，此運算元可以降低 SOPNN網
路拓樸的複雜度。如果在步驟四中的
2max[ ] 0.999kmR ≥ ，表示此 SOPNN的網路結
構對於輸入樣式具有相當高的解釋能力，因
此，提前終止 SOPNN的演化。 
步驟六：學習準確度的評估 
選 擇 步 驟 四 中 具 有 最 佳 適 合 度
( 2max[ ]kmR )的神經元，並且藉由該神經元所
獲得之多項式迴歸模型取得該層中最佳之
神經元 *,o jy  ( 1,2, , trj n= … )的輸出。正規化
均方根誤差 (normalized root mean square 
error, NRMSE)被用於評估 SOPNN的學習準
確度，如式(3.3)所示： 
tr
' * 2
, ,
1
tr
tr
( )
, 1, 2, ,
n
d j o j
j
y y
NRMSE j n
n
=
−
= =
∑
…  
(3.3) 
學習的 NRMSE越小表示學習的準確度
越高。 
步驟七：門檻值 1kσ + 的更新 
為了避免 GMDH 演算法的限制(1) 對
於相對簡單的系統( 資料 )，它會產生十分
複雜的二次多項式迴歸模式模式；(2) 受限
制於它的二次兩變數多項式  (quadratic 
two-variable polynomial)，當處理高度非線
性系統時，它會產生過度複雜的模式[12]。
在 SOPNN中，門檻值會隨著演化的層數而
逐步增加，下一層的門檻值是本層的 1.05
倍，如式(3.4)所示： 
1 1.05k kσ σ+ = ×        (3.4) 
此運算元可以在 SOPNN之網路架構演
化的過程中，逐步的減少各層中神經元的數
目，並有助於篩選出優良的神經元。 
反覆地執行步驟三到七，直到終止條件
最大的演化層數 maxk 或 2max[ ] 0.999kmR ≥ 被
滿足為止。 
測試階段： 
步驟八：測試準確度的評估 
在學習階段中，SOPNN 演化出最佳的
網路拓樸和獲得在此拓樸中每一個神經元
的 esβ 。SOPNN的回想(recall)過程可藉由將
測試資料的樣式輸入此網路拓樸，隨後得到
此網路的輸出。SOPNN 使用式(3.3)評估測
試的準確度，其中 te1, 2, ,j n= … 。 
 
4. 實驗成果 
本計畫所提出的 SOPNN 將應用於兩
個混沌時間序列(chaotic time series)問題，
分別是Mackey-Glass時間序列 [6] 和 The 
Logistic Map [8] 。本計畫所提出的SOPNN
使用MATLAB進行編碼，並在 Pentium D 
3.0 (GHz) P4的個人電腦上執行執行。對於
每一個測試問題，終止條件為 maxk = 10 或
2max[ ] 0.999kmR ≥ 。 
根據式(4.1)與(4.2)分別產生具有 1100
筆輸入樣式的資料集，此資料集將被畫分成
兩個資料集，分別是具有 1000 筆輸入樣式
 8
4.3 The Logistic Map時間序列(TP 2) 
 
( 1) 4 ( )[1 ( )]y t y t y t+ = −       (4.2) 
 
初始的設定為 (0)y = 0.2與 ( )y t = 0 ( 當
t < 0 時 )。四個輸入變數 ( )y t 、 ( 1)y t − 、
( 2)y t − 與 ( 3)y t − 被用以預測 ( 1)y t + 。 
本計畫所提出的 SOPNN 演化一層即
獲得最佳的實驗結果。圖 9(a)為繪製學習樣
式之個數與期望輸出之時間序列圖。圖 9(b)
顯示學習樣式之個數與塑模誤差的直方
圖，由圖中可知誤差皆趨近於 0。 
圖 10(a)繪製測試樣式之期望輸出與網
路輸出的曲線。由圖中可知，期望輸出與
網路輸出是擬合的。圖 10(b)顯示樣式之個
數與推廣誤差的直方圖，由圖中可知誤差
皆趨近於 0，意指測試的結果理想。 
0 100 200 300 400 500 600 700 800 900 1000
0.2
0.4
0.6
0.8
1
(a) Number of training patterns
D
es
ire
d 
ne
tw
or
k 
ou
tp
ut
0 100 200 300 400 500 600 700 800 900 1000
-0.02
-0.01
0
0.01
0.02
(b) Number of training patterns
E
rro
r
 
圖 9 使用 SOPNN所獲得之 TP 2最佳的學
習結果 
0 10 20 30 40 50 60 70 80 90 100
0.2
0.4
0.6
0.8
1
(a) Number of testing patterns
O
ut
pu
t
desired network output
actual network output
0 10 20 30 40 50 60 70 80 90 100
-0.02
-0.01
0
0.01
0.02
(b) Number of testing patterns
E
rro
r
 
圖10 使用SOPNN所獲得之TP 2最佳的測
試結果 
 
4.4 比較 
本計畫比較 SOPNN 與先前已發表的
小腦連結控制器神經網路(cerebellar model 
articulation controller neural network, CMAC 
NN) 和 倒 傳 遞 網 路 (back-propagation 
network, BPN)的實驗結果[16]。表 1 列示最
佳的 SOPNN、CMAC NN與 BPN之結果。
在 TP1中，儘管 SOPNN的學習的 NRMSE
高於 CMAC NN，但 SOPNN NN之推廣能
力優於 CMAC NN。在 TP2中，SOPNN學
習與推廣的能力明顯地優於 CMAC NN與
BPN。此外，SOPNN 所需的計算的 CPU
時間也遠低於 CMAC NN與 BPN。 
表 1  SOPNN、CMAC NN和 BPN的實驗
結果比較表 
 
 
學習的
NRMSE 
測試的 
NRMSE 
計算的 
CPU時間(秒)
SOPNN 0.0045 0.0051 7.98
CMACNN 0.0034 0.0058 56.51TP1 
BPN 0.0072 0.0060 40.31
SOPNN 3.74E-14 3.71E-14 3.62
CMAC NN 0.0084 0.0098 51.48TP2 
BPN 0.0229 0.0238 44.45
 
5. 結論 
本計畫提出一個 NC 的 DM 技
術—SOPNN，具有可自行演化出最佳的網
路拓樸的特性。因此，具備容易使用之特
性。本計畫所發展的 SOPNN 以 GMDH 演
算法為基礎，它是一種統計的學習網路。本
計畫所提出的 SOPNN之性能，已藉由兩個
混沌序列問題而加以評估。結果顯示
SOPNN 可以有效地處理混沌時間序列問
題。因此，SOPNN 可以用於處理傳統統計
方法或頻譜分析技術不易求解的時間序列
問題。 
 
6. 計畫成果自評 
本計畫所提出的 SOPNN藉由兩個混沌
時間序列而評估。結果顯示，SOPNN 可以
處理混沌時間序列問題，與本計畫預期的貢
獻相符。SOPNN 應可嘗試應用於供應鏈管
理中的需求預測與財務時間序列問題。此
外，本計畫的部分研究成果已於 2009 年 5
月 23日發表於「第 20屆國際資訊管理學術
研討會-資訊科技與創新管理」( 請見附
錄 )。本計畫的完整成果報告加入使用者介
面的說明與實驗成果的比較。本計畫將積極
嘗試把研究成果發表於神經計算與應用之
國際期刊。 
 應用於預測混沌時間序列之神經計算 
的資料探勘方法 
吳瑞煜* 
龍華科技大學企管系 
jywu@mail.lhu.edu.tw 
王世昌 
龍華科技大學企管系 
scwang@mail.lhu.edu.tw 
 
摘要 
 
統計與頻譜分析方法(spectral analysis)
已經被應用於時間序列問題。以線性為基
礎的統計方法，例如 Box-Jenkins 方法
( autoregressive integrated moving average 
approach )，失效於塑模具有非系性行為的
真實系統。因此，許多非線性時間序列模
型被發展出來，例如：雙線性模式 (bilinear 
model)、門檻自我迴歸模式  (threshold 
autoregressive model)、平滑轉換自我迴歸
模式  (smoothing transition autoregressive 
model) 、 自 我 迴 歸 條 件 異 方 差 
(autoregressive conditional heteroscedastic 
model)與一般性自我迴歸條件異方差 
(generalized autoregressive conditional 
heteroscedastic model)。不幸的是，這些非
線性模型是針對特殊問題而設計的。這意
味著，這些方法在應用上缺乏一般的適用
性。此外，頻譜分析方法對於一般的使用
者在執行上是有困難的。這是因為這些方
法必須具備訊號處理( signal processing )的
知識才能去修改各方法所屬的特殊參數。
為了克服統計與頻譜分析方法的缺點，本
研究提出一個神經計算(neuro-computing)
的資料探勘(data mining, DM)方法。此 DM
技術是一個以 GMDH (group method of 
data handling) 演算法為基礎的自組織多項
式神經網路  (self-organizing polynomial 
neural network, SOPNN)。本研究所提出
SOPNN 將藉由應用於兩個混沌時間序列
(chaotic time series)問題而被評估。實驗結
果指出，本研究所提出的 SOPNN可以有效
地應用於混沌序列問題。 
關鍵詞：神經計算、資料探勘、混沌時間
序列、GMDH演算法、神經網路 
 
1. 前言 
神經計算(neuro-computing, NC)是計
算智慧(computational intelligence)家族中的
一個成員，其它的成員包括模糊邏輯(fuzzy 
logic)、基因演算法(genetic algorithm)、演
化規劃(evolutionary programming)、昆蟲智
慧(swarm intelligence)與人工生命(artificial 
life)等。NC有許多學習演算法 (learning 
algorithms)。例如：監督式(supervised)、非
監督式(unsupervised)與競爭式(competitive)
等法則，其中監督式法則可以藉由最小均
方演算法(least mean square algorithm)精準
地塑模資料的成對輸入與輸出之關係[5]。
本研究使用監督式法則。 
資料庫知識發掘(knowledge discovery 
in databases, KDD)的過程可以將低階的資
料轉換成高階的知識。KDD的過程是由資
料準備(data preparation)、資料選擇(data 
selection)、資料清理 (data cleaning)、資料
整 合 (incorporation of appropriate prior 
knowledge)、資料探勘(data mining, DM)和
DM結果的適當解釋(proper interpretation of 
the data mining results)等所組成。DM是
KDD過程中的核心技術並且已經被使用於
許多應用上。例如：預測(forecasting)、摘
要(summarization)、分類(classification)、分
群(clustering)、規則歸納(rule generation)和
序列分析等(sequence analysis)[7]。本研究
著重發展NC的預測技術。 
商業智慧(business intelligence, BI)是
一個結合資料獲取(data gathering)、資料儲
存(data storage)和資料分析以提供從輸入
到 決 策 過 程 的 一 種 資 料 驅 動 系 統
(data-driven system)。BI系統藉由系統化的
獲得、校對分析和資訊的解釋與探索，讓
企業能瞭解其內部與外部環境[15]。BI系
統所包含的元件有：線上分析處理(online 
analytical process)、知識管理 (knowledge 
2035
ICIM2009 ???屆??資????????
 外，以 GMDH演算法為基礎的 DM的工具
已經被發展出來，並且成功地被應用於預
測 ( 如：股票預測、資產負債表預測、銷
售預測與水污染預測等 ) 與分類問題 
( 如：心臟疾病的診斷 )[9]。 
為了克服傳統的統計方法與頻譜分析
技術需依賴人為主觀判斷與不易使用的限
制，本研究提出一個以 GMDH演算法為基
礎 的 自 組 織 多 項 式 神 經 網 路 
(self-organizing polynomial neural network, 
SOPNN)。此 SOPNN將被用於兩個混沌時
間序列問題( 分別是 Mackey-Glass 時間序
列 [6] 和 The Logistic Map時間序列[8] )
的 塑 模 (modelling) 與 歸 納 推 廣
(generalization)，藉以評估它的性能。 
 
2. GMDH演算法 
 
GMDH 演算法的網路拓樸 (network 
topology)是一種多層前饋式神經網路
(feed-forward neural network)。在網路拓樸
中，每一層包含許多神經元(neurons)，每
一個神經元擁有兩個輸入變數，並且使用
二次多項式迴歸模型(quadric polynomial 
regression model)，如式(2.1)所示： 
tr5
2
4
2
3210, ,,2,1, njxxxxxxy wjvjwjvjwjvjjd …=+++++= ββββββ  
(2.1) 
其 中 ,d jy 為 第 j 個 目 標 輸 出 (desired 
output)， vx 與 wx 為神經元的兩個輸入變
數， trn 則為學習資料集的個數。在式(2.1)
中， ,d jy 、 vjx 與 wjx 是已知的，因此，必須
推估係數 0β 、 1β 、 2β 、 3β 、 4β 與 5β 。這
些係數的計算過程將在節 2.1中討論。圖 2
顯示 GMDH演算法的網路拓樸。 
由圖 2可知，在輸入層中有 N 個輸入
向量，因此在第一層中將產生的神經元之
個數，可使用式(2.2)計算，如下所示： 
( 1)
2
N N −            (2.2) 
在 推 估 的 係 數 之 矩 陣
es es,0 es,1 es,2 es,3 es,4 es,5[ , , , , , ]
Tβ β β β β β=β 被計算出
來後，在第 k層中第m個神經元之第 j個輸
出 kmjoy , 可由式(2.3)獲得，如下所示： 
2/)1(,,2,1,,,2,1,,,2,1
,
maxtr
5,es
2
4,es
2
3,es2,es1,es0,es,
−===
+++++=
NNmkknj
xxxxxxy wjvjwjvjwjvj
km
jo
………
ββββββ  
(2.3) 
其中 
maxk = 最大的演化層數 
 
…
…
1x 2x 1−Nx Nx
maxk
 
圖 2:  GMDH演算法的網路拓樸 
在評估 ,d jy 與 ,kmo jy 的誤差之後，在第 k
層中第m個神經元所獲得的誤差將與在第
k層中一個事先定義的門檻值( 誤差 )比
較，只有具有比門檻值較小誤差的神經元
可以通過，並成為下一層的輸入變數；然
而，具有比門檻值較大誤差的神經元將從
該層中被剔除。此自組織網路拓樸的形成
過程相似於自然演化(natural evolution)。 
 
2.1  GMDH的學習演算法 
 
此學習過程之目的為找到 GMDH 演
算法的網路拓樸以及每一個神經元的
esβ ，網路拓樸的產生主要可以分成三個步
驟，說明如下。 
 
步驟一： esβ 的估計 
在 GMDH的學習演算法中，為了避免
過度學習(over-learning)的情形發生，會將
資料集畫分成兩個子資料集，分別是學習
資料集 (training data set)與查核資料集
2037
ICIM2009 ???屆??資????????
 值 
maxE  = 期望輸出的極大值 
min max[ , ]E E 通常分別設定 0.2與 0.8。 
 
Procedure SOPNN
Begin
End
End
While                  domaxkk ≤
endWhile
(  )
1←m
1←k
if                    thenkkmR σ>2
保留神經元
else
剔除神經元
endIF
if                              then999.0]max[ 2 ≥kmR
break
endIF
1+← mm
1+← kk
05.11 ×←+ kk σσ
if                    then0.11 >+kσ
endIF
0.11 =+kσ
kσ
1+kσ
 
圖 3:  SOPNN的虛擬程式碼 
 
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
++++
++++
++++
tetrtetrtetrtetr
trtrtrtr
trtrtrtr
trtrtrtr
,21
2,22221
1,11211
,21
2,22212
1,12111
nndnNnnnnn
ndNnnn
ndNnnn
ndNnnn
dN
dN
yxxx
yxxx
yxxx
yxxx
yxxx
yxxx
"
#####
"
"
"
#####
#####
"
"
trn
chn
ten
totaln
1x 2x " Nx dY
totaln
trn
chn
ten
tetrtotal nnn +=
totaln
 
圖 4:  資料集的分割圖示 
步驟二：門檻值 kσ 的設定 
在 SOPNN中，每一層都設有一個門檻
值，以 max( 1,2, , )k k kσ = … 做為第 k的門檻
值， kσ 介於 0與 1之間。 kσ 越接近 1表示
神經元通過該層的門檻越高。 
步驟三：神經元的產生 
在 SOPNN中，本層中的神經元個數可
使用式(2.2)計算而獲得。隨後，利用式(2.4)
的矩陣運算推估每一個神經元的 esβ ，並且
根據式(2.3)產生每一個神經元之輸出。 
步驟四：神經元的擇優  
本研究所提出的 SOPNN 是採用多項
式迴歸模型。因此，SOPNN採用複判定係
數(coefficient of multiple determination)做
為性能指標(performance index)，如下所示： 
2/)1(,,2,1,,,2,1,
)-(
)-(
max
1
2''
,
1
2'
,
2
ch
ch
−===
∑
∑
=
= NNmkk
yy
yy
R n
j
djd
n
j
d
km
jo
km ……
 
(3.2) 
其中 
2
kmR = 在第 k層中第m個神經元的複判定
係數 
'
dy = ch
' ' '
,1 ,2 ,[ , ]
T
d d d ny y y… 的平均值 
'
dy = 正規化後目標輸出向量 
2
kmR 用於說明神經元所配適之迴歸模
型的適合性。 2kmR 介於 0與 1之間， 2kmR 越
接近 1 表示，該神經元所配適的迴歸模型
對於用於建立模型的輸入樣式的解釋能力
越高。如果 2kmR 大於 kσ ，則該神經元可以通
過，並且成為下一層的輸入變數。反之，
如果 2kmR 小於或等於 kσ ，則該神經元將在
SOPNN的網路拓樸中被刪除。 
步驟五：提早終止網路的學習 
SOPNN 使用一個準則提早停止網路
學習的運算元，此運算元可以降低 SOPNN
網路拓樸的複雜度。如果在步驟四中的
2max[ ] 0.999kmR ≥ ，表示此 SOPNN 的網路
結構對於輸入樣式具有相當高的解釋能
力，因此，提前終止 SOPNN的演化。 
步驟六：學習準確度的評估 
選擇步驟四中具有最佳適合度
( 2max[ ]kmR )的神經元，並且藉由該神經元所
獲得之多項式迴歸模型取得該層中最佳之
2039
ICIM2009 ???屆??資????????
 0 10 20 30 40 50 60 70 80 90 100
0.2
0.4
0.6
0.8
1
(a) Number of testing patterns
O
ut
pu
t
desired network output
actual network output
0 10 20 30 40 50 60 70 80 90 100
-0.02
-0.01
0
0.01
0.02
(b) Number of testing patterns
E
rro
r
 
圖 6: 使用 SOPNN所獲得之 TP 1最佳的
測試結果 
 
圖 6(a)繪製測試樣式之期望輸出與網
路輸出的曲線。由圖中可知，期望輸出與
網路輸出是擬合的。圖 6(b)顯示樣式之個
數與推廣誤差(generalization error)的直方
圖，由圖中可知誤差皆位於[−0.01, 0.01]
之間，意指測試的結果理想。 
 
4.2  The Logistic Map時間序列(TP 2) 
 
( 1) 4 ( )[1 ( )]y t y t y t+ = −       (4.2) 
 
初始的設定為 (0)y = 0.2 與 ( )y t = 0 
( 當 t < 0 時  )。四個輸入變數 ( )y t 、
( 1)y t − 、 ( 1)y t − 與 ( 1)y t − 被用以預測
( 1)y t + 。 
本研究所提出的 SOPNN 演化一層即
獲得最佳的實驗結果。圖 7(a)為繪製學習
樣式之個數與期望輸出之時間序列圖。圖
7(b)顯示學習樣式之個數與塑模誤差的直
方圖，由圖中可知誤差皆趨近於 0。 
圖 8(a)繪製測試樣式之期望輸出與網
路輸出的曲線。由圖中可知，期望輸出與
網路輸出是擬合的。圖 8(b)顯示樣式之個
數與推廣誤差的直方圖，由圖中可知誤差
皆趨近於 0，意指測試的結果理想。 
0 100 200 300 400 500 600 700 800 900 1000
0.2
0.4
0.6
0.8
1
(a) Number of training patterns
D
es
ire
d 
ne
tw
or
k 
ou
tp
ut
0 100 200 300 400 500 600 700 800 900 1000
-0.02
-0.01
0
0.01
0.02
(b) Number of training patterns
E
rro
r
 
圖 7: 使用 SOPNN所獲得之 TP 2最佳的
學習結果 
 
0 10 20 30 40 50 60 70 80 90 100
0.2
0.4
0.6
0.8
1
(a) Number of testing patterns
O
ut
pu
t
desired network output
actual network output
0 10 20 30 40 50 60 70 80 90 100
-0.02
-0.01
0
0.01
0.02
(b) Number of testing patterns
E
rro
r
 
圖 8: 使用 SOPNN所獲得之 TP 2最佳的
測試結果 
 
5. 結論 
本研究提出一個 NC 的 DM 技
術—SOPNN，具有可自行演化出最佳的網
路拓樸的特性。因此，具備容易使用之特
性。本研究所發展的 SOPNN以 GMDH演
算法為基礎，它是一種統計的學習網路。
本研究所提出的 SOPNN之性能，已藉由兩
個混沌序列問題而加以評估。結果顯示
SOPNN 可以有效地處理混沌時間序列問
題。因此，SOPNN可以用於處理傳統統計
方法或頻譜分析技術不易求解的時間序列
問題。 
 
誌謝 
本研究的作者感謝國科會在經費上的
資助，計畫編號為：NSC 97-2218-E-262-002-。 
2041
ICIM2009 ???屆??資????????
