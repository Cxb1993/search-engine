rates when the textures are contaminated by additive noise. To mitigate these problems, a new approach for 
texture classification, based on wavelet transformation and fuzzy support vector machine (FSVM) is proposed 
in this paper.   
The support vector machine (SVM) has been widely used in various application areas like the signal 
processing, classification, feature extraction, and recognition applications [24-28]. The SVM was developed 
based on the idea of structural risk minimization. The basic principle of SVM is to find an optimal separating 
hyperplane (OSH) so as to separate two classes of patterns with the maximal margin between the training set 
and the decision boundary, which can be formulated as a quadratic programming (QP) problem in the feature 
space [1-3]. 
The reason is that SVMs are much more effective than other conventional nonparametric classifiers (e.g. 
the neural networks, nearest neighbor, k-NN classifier) in terms of classification accuracy, computational time, 
stability to parameter setting. The SVM obtains high generalization performance without the need for a priori 
knowledge even when the input dimension space is high. Moreover, this is a model allowing more accurate 
and formal assessment of the generalization performance. Though simple SVM models can handle various 
classification tasks, fuzzy support vector machines have been examined for a period of time [29-32] to make 
the classification become more effective. Lin and Wang [4] first proposed a prototype of fuzzy SVM (FSVM), 
in which one applies a fuzzy membership function to each input data of SVM. 
However, when faced with some particular data types, SVM would suffer from two critical problems: the 
overfitting problem due to outliers, and the multi-cluster distribution problem. Both problems would result in 
high misclassification. Considering the advantages of SVM, this study uses SVM as a base and employs a 
novel classifier called the fuzzy SVM, which not only inherits the merits of SVM, but also overcomes its 
drawbacks. The proposed FSVM approach has two main advantages over SVM: (1) by incorporating a fuzzy 
membership into the objective, the proposed FSVM is less sensitive to outliers and therefore the overfitting 
problem can be solved, (2) instead of assigning membership values to data points in input space like previous 
methods, e.g., fuzzy SVM [30, 33], in the proposed FSVM, the membership values of the target data can be 
assigned in a higher-dimensional feature space automatically. Thus, an FSVM containing several FSVM 
members is formed. By doing so, the multi-cluster distribution problem can be solved. In Section 4, we will 
show by using the proposed FSVM as the texture classifier, not only the performance of SVM in texture 
classification can be greatly improved, but also a high classification accuracies rate can be achieved. 
 
 
2. METHODOLOGY 
 
2.1 Wavelet transform 
 
Wavelet transforms are finding inverse use in fields as diverse as telecommunications and biology. Because 
of their suitability for analyzing non-stationary signals, they have become a powerful alternative to Fourier 
methods in many medical applications, where such signals abound [7]. The main advantages of wavelets is 
they have a varying window size, being wide for slow frequencies and narrow for the fast ones, thus leading to 
an optimal time–frequency resolution in all the frequency ranges. Further, because the windows are adapted to 
the transients of each scale, wavelets lack the requirement of stationarity. A wavelet expansion is Fourier series 
expansion, but is defined by a two-parameter family of functions. It can be defined as follows: 
 
        ∑=
ji
jiji xcxf
,
,, )()( ϕ                                    (1)      
           
where i and j are integers, the functions )(, xjiϕ  are the wavelet expansion functions and the two-parameter 
expansion coefficients  are called the discrete wavelet transform (DWT) coefficients of f(x). The 
coefficients are given by 
jic ,
 
                                                   (2)    ∫= +∞∞− )()( ,, xxfc jiji ϕ
 
The wavelet basis functions can be computed from a function )(xϕ  called the generating or mother wavelet 
through translation and dilation  
                                     (3)  )2(2)( )2/(, jxx
ii
ji −= −− ϕϕ
 2 
 
 
where C is a constant. Note, a smaller i reduces the effect of the parameters iξ in problem (9) so the 
corresponding point is treated as less important.  i
To solve this optimization problem we construct the Largangian  
x
 
),,,,( βαξbwL                                                                   
∑ −+−+⋅∑ −+⋅=
= ==
l
i
l
i
iiiiii
l
i
ii bzwysCww
1 11
)1)(()2/1( ξβξαξ ∑                    (10)            
 
And find the saddle point of ),,,,( βαξbwL . The parameters must satisfy the following conditions: 
 
∑ =−=∂
∂
=
l
i
iii zyww
bwL
1
0),,,,( αβαξ                                    (11)            
∑ =−=∂
∂
=
l
i
ii yb
bwL
1
0),,,,( αβαξ                                       (12)           
0),,,,( =−−=∂
∂
iii
i
CsbwL βαξ
βαξ                                    (13)            
Apply these conditions into the Lagrangian (10), the problem (9) can be transformed into  
                  
Maximize ∑∑∑ −=
= ==
l
i
l
j
jijiji
l
i
i xxKyyW
1 11
),(
2
1)( αααα          
Sub to                          (14) liCsy
l
i
iiii ,....,1,00
1
=∑ ≤≤=
=
αα
 
And the Kuhn-Tucker conditions are defined as  
                          libzwy iii ,....,10)1)(( ==+−+⋅ ξα                            (15)           
liCs iii ,....,10)( ==− ξα                                      (16)           
 
The point i with the corresponding x 0>iα is called a support vector. There are also two types of support 
vectors. The one with corresponding 0 < Csii <α lies on the margin of the hyperplane. The one with 
corresponding Csii =α is misclassified. An important difference between SVM and FSVM is points with the 
same value of a may indicate a different type of support vectors in FSVM due to the factor .    is
      
2.4 Fuzzy membership 
 
2.4.1 Method-1: Membership setting by class center and radius 
Many research results have shown the SVM is very sensitive to noises and outliers [6]. The FSVM can also be 
applied to reduce to the effects of outliers. First, we propose a model by setting the fuzzy membership as a 
function of the distance between the point and its class center. This setting of the membership may not be the 
best way to solve the problem of outliers but it may be better to choose a different model of fuzzy membership 
function in different training sets. Suppose we are given a sequence of training points 
 
                                                    (17) ),,(),....,,,( 111 lll sxysxy
Denote the mean of class “+1” as μ＋ and the mean of class “-1” as μ－. Let the radius of class +1 
 
        +
=
+ −= μiyix xR )1:{max                                           (18) 
and the radius of class -1 
 4 
 
⎪⎪
⎪
⎩
⎪⎪
⎪
⎨
⎧
>⎟⎟⎠
⎞
⎜⎜⎝
⎛
−+×
≤+
+
−
×
=
Rxd
Rxd
Rxd
R
xd
R
xd
xs
i
i
i
i
i
i
)(,
)((1
14.0
)(4.0)(1
)(1
6.0
)(
                        (24) 
 
where axxd ii −=)(  stands for the distance between class center and data.  
 
 
 
2.4.3 Method-3: Membership setting by Fuzzy c-mean  
Third, the membership is set by fuzzy c-mean. The original data should be preprocessed by the related analysis 
method, such as regression analysis or random forest in order to reduce the outliers and noises. The whole data 
set is further divided into training data and testing data set. To find the optimal parameters, degree of fuzziness 
and cluster size, during the training process, we should try and error and use the grid search method to find the 
best match solutions. When we find the optimal parameter combinations (m*, c*), we can also find each 
center such as: . By doing so, we can use FCM to calculate the 
membership for each data. The formula is displayed as follows:    
),...,,,(*)( *,
*
,3
*
,2
*
,1
c
in
c
i
c
i
c
ii xxxxmv =
 
 
 
⎜⎜
⎜⎜
⎝
⎛
⎟⎟
⎟⎟
⎠
⎞
⎜⎜
⎜⎜
⎝
⎛
⎟⎟⎠
⎞
−
−=
−
=
−∑
1
*
1
1*
2
*
,
*
,*
c
j
m
jxk
ixk
ik vx
vx
u                                      (25) 
where, stands for the membership of kth input vector in i cluster. The same formula is also used in 
the testing data set. In this research, using different membership setting makes the different class exist the 
different tolerance. However, the traditional SVM cannot have the different tolerance degree among the 
different classes.  
*
,kiu
 
 
 
 
3. TEXTURE CLASSIFICATION PROCEDURE 
 
3.1 Texture image data 
 
The Brodatz texture database [34] is widely used for evaluating texture classification algorithms. The 
Brodatz database consists of both homogeneous and non-homogeneous textures. The classification of 
non-homogeneous image textures is a challenging task. Therefore, most of the existing approaches have 
considered only subsets of textures from the Brodatz database [34]. To test the performance of our approach in 
a realistic environment, the subset of 10 textures in Brodatz texture database is considered. 5 samples of these 
22 images are shown in Fig. 1, other images can be obtained from Brodatz texture database [34]. Each of these 
640 × 640 images is divided into 100 64 × 64 non-overlapping subimages, resulting in 2200 subimages 
(samples).  
 
3.2 Feature extraction 
 
Feature extraction plays an important role in texture classification. In this study, multi-resolution wavelet 
 6 
 
The results of multi-class texture classification are presented in Table 1. Table 1 shows the classification 
accuracies of SVM using linear, RBF and polynomial kernel functions approximate up to 100%, except when 
using the sigmoid function. The performance of the SVM classification using the RBF kernel function is the 
best kernel function compared with the other three. Noted, the classification accuracies are between 56% and 
86.7% for the sigmoid kernel function, which may not be a suitable kernel function for texture classification.    
 
Table 1. Results of One-against-all SVM classification  
Linear RBF Polynomial Sigmoid  NO. 
Training 
Accuracy 
Testing 
Accuracy 
Training 
Accuracy
Testing 
Accuracy
Training 
Accuracy 
Testing 
Accuracy 
Training 
Accuracy 
Testing 
Accuracy 
1 95.5  95.5  99.4 98.4 95.5  95.5  95.5  95.5  
2 95.5  95.5  98.6 99.3 95.5  95.5  95.5  95.5  
3 95.5  95.5  97.7 97.6 95.5  95.5  95.5  95.5  
4 95.5  95.5  100.0 99.6 95.5  95.5  95.5  95.5  
5 95.5  95.5  99.9 98.2 95.5  95.5  95.5  95.5  
6 95.5  95.5  99.5 99.1 95.5  95.5  95.5  95.5  
7 95.5  95.5  100.0 99.8 95.5  95.5  95.5  95.5  
8 95.5  95.5  100.0 100.0 95.5  95.5  95.5  95.5  
9 95.5  95.5  99.6 99.3 95.5  95.5  95.5  95.5  
10 95.5  95.5  99.7 98.5 95.5  95.5  95.5  95.5  
11 95.5  95.5  100.0 99.5 95.5  95.5  95.5  95.5  
12 95.5  95.5  100.0 99.8 95.5  95.5  95.5  95.5  
13 95.5  95.5  100.0 99.8 95.5  95.5  95.5  95.5  
14 95.5  95.5  99.6 97.8 95.5  95.5  95.5  95.5  
15 95.5  95.5  99.5 99.5 95.5  95.5  95.5  95.5  
16 95.5  95.5  99.9 98.9 95.5  95.5  95.5  95.5  
17 95.5  95.5  99.5 98.7 95.5  95.5  95.5  95.5  
18 95.5  95.5  99.5 98.7 95.5  95.5  95.5  95.5  
19 95.5  95.5  99.3 98.9 95.5  95.5  95.5  95.5  
20 95.5  95.5  99.3 99.1 95.5  95.5  95.5  95.5  
21 95.5  95.5  100 100 95.5  95.5  95.5  95.5  
22 95.5  95.5  100 100 95.5  95.5  95.5  95.5  
 
4.2 Texture classification by FSVM 
 
The results of multi-class texture classification have been presented in Table 2. Table 2 shows the 
classification accuracies of FSVM using linear, RBF, polynomial and sigmoid kernel functions and three 
different kinds of membership setting functions. The performance of FSVM classification using the RBF 
kernel function is the best kernel function compared with the other three. The accuracies of classification for 
both training and testing are 100% for RBF kernel function and three membership setting functions. The 
accuracies of testing for polynomial kernel function using method-1 and method-2 are 94% and 91%, 
respectively. However, one of the main different accuracies using polynomial kernel function and method-3 
membership setting is 55.5%. That is the worst result using three different kinds of membership setting. In 
addition, the other results for testing using linear and sigmoid kernel function are about 40%~60% 
classification accuracies. Therefore, the linear and sigmoid kernel function may not be a suitable kernel 
function for texture classification.   
 
 8 
 
[8] Gonzalez R.C., R.E. Woods, Digital Image Processing, third ed. Addison-Wesley, Reading, 1992. 
[9]  Haralick R. M., K. Shanmugan, I. Dinstein, “Texture features for image classification,” IEEE Trans. Syst., 
Man, Cybern., Vol. SMC-3, No. 6, pp. 610–621, Nov. 1973. 
[10] Ohanian P. P., R. C. Dubes, “Performance evaluation for four classes of textural features,” Pattern 
Recognit., Vol. 25, No. 8, pp. 819–833, 1992. 
[11] Cohen F. S., Z. Fan, S. Attali, “Automated inspection of textile fabrics using textural models,” IEEE 
Trans. Pattern Anal. Mach. Intell., Vol. 13, No. 8, pp. 803–808, Aug. 1991. 
[12] Speis, A., G. Healey, “Feature extraction for texture discrimination via random field models with random 
spatial interaction,” IEEE Trans. Image Process., Vol. 5, No. 4, pp. 635–645, Apr. 1996. 
[13] Hong, T. H., C. R. Dyer, A. Rosenfeld, “Texture primitive extraction using an edge-based approach,” 
IEEE Trans. Syst., Man, Cybern., Vol. SMC-10, No. 10, pp. 659–675, Oct. 1980. 
[14] Garcia-Sevilla, P., M. Petrou, “Classification of binary textures using the 1-D Boolean model,” IEEE 
Trans. Image Process., Vol. 8, No. 10, pp. 1457–1462, Oct. 1999. 
[15] Laws, K. I., “Rapid texture identification,” in Proc. SPIE Image Processing for Missile Guidance, 1980, 
pp. 376–380. 
[16] Arof, H., F. Deravi, “Circular neighbourhood and 1-D DFT features for texture classification and 
segmentation,” IEE Proc. Vis., Image, Signal Process., Vol. 145, No. 3, pp. 167–172, 1998. 
[17] Azencott, R., W. Jia-Ping, L. Younes, “Texture classification using windowed Fourier filters,” IEEE Trans. 
Pattern Anal. Mach. Intell., Vol. 19, No. 2, pp. 148–153, Feb. 1997. 
[18] Daubechies, I., Ten Lectures on Wavelets. Philadelphia, PA: SIAM, 1992. 
[19] Mallat, S. G., “A theory for multiresolution signal decomposition: The wavelet representation,” IEEE 
Trans. Pattern Anal. Mach. Intell., Vol. 11, No. 7, pp. 674–693, Jul. 1989. 
[20] Chang, T., C.-C. J. Kuo, “Texture analysis and classification with tree-structured wavelet transform,” 
IEEE Trans. Image Process., Vol. 2, No. 4, pp. 429–441, Apr. 1993. 
[21] Guoliang, F., X. Xiang-Gen, “Wavelet-based texture analysis and synthesis using hidden Markov 
models,” IEEE Trans. Circuits Syst. Fundam. Theory Appl., Vol. 50, No. 1, pp. 106–120, Jan. 2003. 
[22] Dasgupta, N., L. Carin, “Texture analysis with variational hidden Markov trees,” IEEE Trans. Signal 
Process., Vol. 54, No. 6, pp. 2353–2356, Jun. 2006. 
[23] Randen, T., J. H. Husoy, “Filtering for texture classification: A comparative study,” IEEE Trans. Pattern 
Anal. Mach. Intell., Vol. 21, No. 4, pp. 291–310, Apr. 1999. 
[24] Comak, E., K. Polat, S. Gues, A. Arslan, “A new medical decision making system: Least square support 
vector machine (LSSVM) with Fuzzy Weighting Pre-processing,” Exp. Syst. with Appl, Vol. 32, No.2, pp. 
409–414, 2007. 
[25] Huang, C.L., C.J. Wang, “A GA-based feature selection and parameters optimization for support vector 
machines,” Exp. Syst. with Appl, Vol. 31, No.2, pp. 231–240, 2006. 
[26] Polat, K., S. Gunes, A. Arslan, “A cascade learning system for classification of diabetes disease: 
Generalized discriminant analysis and least square support vector machine,” Exp. Syst. with Appl, Vol. 34, 
No.1, 482–487, 2008. 
 10 
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                日期： 99年 9 月 15 日 
計畫編
號 
NSC 98－2221－E－159－003 
計畫名
稱 
應用小波轉換與模糊支向機進行紋理分類之研究 
出國人
員姓名 李得盛 
服務機
構及職
稱 
明新科技大學工管系 
會議時
間 
99 年 7 月 25 日
至 99 年 7 月 28
日 
會議地
點 
Awaji Island, Japan  
淡路島，淡路夢舞台，日本 
會議名
稱 
(中文)第四十屆電腦與工業工程國際研討會 
(英文)The 40th International Conference on Computers & Industrial 
Engineering 
發表論
文題目 
(中文)應用小波轉換與模糊支向機於紋理分類之研究 
(英文)Texture classification using Wavelet Transform and Fuzzy 
Support Vector Machine  
一、參加會議經過 
1.參加 Keynote speech, 主講人：Mr. Masanori UEDA (Nissan Co.)，題目：
Challenges on Widespread Marketplace Acceptance of Electric Vehicles, towards 
zero emission mobility society. 主講人主要演講內容是全球暖化的現況與未
來、日產汽車發展電動車的歷程與未來的挑戰包括新能源、再生能源、新電
池材料、能源轉換效率等，目標是 2025 年達到 0 排放的電動車社會。 
 
2.聆聽 Quality Management/Engineering/Reliability Section Presentation:其中之
一是元智大學工工所博士生報告的 Min-Hue Lo：A Modified Variable 
Neighborhood Search Algorithm for Orienteering Problem 與另一位元智大學工
工所博士生 Tsui-Chiao Chao 報告的：Resampling-based Variable Selection 
Technique and its Application to Model Semiconductor E-Test Data. 並給予一些
個人的看法與建議。另外，也與在場的韓國 Pusan National University, 工工
系教授 Professor, Won Young Yun 交換意見與討論。 
 
3.參加 Tutorial Talk II 中國北京清華大學電機系 Xinjie Yu 教授：Introduction 
  
  
 
 
 
Texture Classification using Wavelet Transform and  
Fuzzy Support Vector Machine 
 
Te-Sheng Li  
Department of Industrial Engineering and Management, Minghsin University of Science and Technology, 
1, Hsin-Hsin Rd., Hsinchu, 30401, Taiwan (jeff@must.edu.tw) 
 
ABSTRACT 
In recent years, texture analysis has played an important role in many tasks, ranging from 
remote sensing, defect detection, pattern recognition to medical imaging and query by 
content in large image databases. In this paper, we apply a classification approach based 
on wavelet transforms and fuzzy support vector machines (FSVMs), offering the 
classification performance for texture classification. Since one of the main difficulties in 
applying of standard SVM is their sensitivity to outliers and noises in the training 
procedure due to overfitting, the fuzzy support vector machine is proposed to deal with 
the difficulties. In addition, the fuzzy membership setting in FSVM is also a critical 
factor significantly reflecting their relative degrees as meaningful data. In this paper, we 
also introduce mean and radius membership setting functions in texture classification. 
The experimental results show that the FSVM outperforms the SVM in terms of 
classification accuracy and computation time and FSVM has the potential to reduce the 
effects of noises or outliers in the texture classification. 
 
Keywords: texture classification, support vector machine, fuzzy support vector machine 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(DWT) coefficients of f(x). The coefficients are given by 
 5
                         (2)    ∫= +∞∞− )()( ,, xxfc jiji ϕ
The wavelet basis functions can be computed from a function 
)(xϕ  called the generating or mother wavelet through 
translation and dilation  
                    (3)        )2(2)( )2/(, jxx
ii
ji −= −− ϕϕ     
where j is the translation and i the dilation parameter. The 
mother wavelet function is not unique, but it must satisfy a 
small set of conditions. One is the multi-resolution condition 
and related to the two-scale difference equation 
         ∑ −=
k
kxkhx )2()(2)( φφ            (4)                
where )(xφ is the scaling function and h(k) must satisfy several 
conditions to make the basis wavelet functions unique, 
orthonormal and have a certain degree of regularity. The 
mother wavelet is related to the scaling function as follows: 
  ∑ −=
k
kxkgx )2()(2)( φϕ               (5)          
where . At this point, if valid h(x) is 
available, one can obtain g(x). Note that h and g can be viewed 
as filter coefficients of half band low-pass and high-pass filters, 
respectively. J-level wavelet decomposition can be computed 
with Eq. (6) as follows: 
)1()1()( khxg k −−=
∑ ∑ ∑+==
= ++++k k
J
j
kjkjkjkjkk xdxckcxf
0
,1,1,1,1,0,00 ))()(()()( ϕφφ      (6)           
where coefficient c0,k and coefficients cj+1,n and dj+1,n at scale j 
+ 1 are given and they can be obtained if the coefficient at 
scale j is available 
        ∑ −=+
k
kjnj nkhcc )2(,,1
∑ −= nkgcd )2(             (7)                             +
k
kjnj ,,1
 
2.2 SVM 
 
In this section we will not review the basis of the theory of 
SVM in classification, please refer the references [1-3] in the 
case of linearly separately, linearly non-separately and 
non-linear algorithm.  
 
2.3 Fuzzy SVM 
 
SVM is a powerful tool for solving classification problems, 
but there are still some limitations to this theory. From the 
formulations discussed previously, each training point belongs 
to either one class or the other. For each class, we can easily 
check the training points are classified into the correct class or 
not based on the theory of SVM. However, in many real-world 
applications, the effects of the training points are different. 
Note, some training points are more important than others in 
the classification problem. That is, each training point may no 
more exactly belong to one of the two classes. It may 80% 
belong to one class and 20% belong to the other class. 
Therefore, there is a fuzzy membership 1≤≤ isσ li ,,1K, = ，
0>σ  associated with each training point i . In this paper, we 
employ the Lin and Wang [4, 5] fuzzy SVM model, and 
transform all the training data into high dimension feature 
space. The value of i  represents the membership of each 
training point and the parameter of 
x
s
ξ  measures the errors of 
the fuzzy SVM in the classification problem. The formulation 
of fuzzy SVM is illustrated as follows.      
Suppose, we are given a set S of labeled training points 
with associated fuzzy membership 
                            (8) ),,(),....,,,( 111 lll sxysxy
NRx ∈Each training point is given a label i }1,1{−∈iy  and a 
fuzzy membership 1≤≤ isσ with  and sufficient 
small
li ,....,1=
0>σ . Let )(xz ϕ= , denote the corresponding feature 
space vector with a mapping ϕ from to a feature space Z. 
Since the fuzzy membership i is the attitude of the 
corresponding point i toward one class and the parameter i
NR
s
x ξ is 
a measure of error in the SVM, the term iis ξ is a measure of 
error with different weighting. The optimal hyperplane 
problem is then regarded as the solution to  
Minimize                  ∑+⋅
=
l
i
iisCww
1
)2/1( ξ
    Subject to  libzwy iii ,....,11)( =−≥+⋅ ξ                
lii ,....,1,0 =≥ξ                    (9) 
where C is a constant. Note, a smaller i reduces the effect of 
the parameter i
s
ξ in problem (9) so the corresponding point is 
treated as less important.  
ix
To solve this optimization problem we construct the 
Largangian  
),,,,( βαξbwL                                             
∑ ∑−+−+⋅∑ −+⋅=
= ==
l
i
l
i
iiiiii
l
i
ii bzwysCww
1 11
)1)(()2/1( ξβξαξ      
(10)      
And find the saddle point of ),,,,( βαξbwL . The parameters 
must satisfy the following conditions: 
 
∑ =−=∂
∂
=
l
i
iii zyww
bwL
1
0),,,,( αβαξ              (11)      
∑ =−=∂
∂
=
l
i
ii yb
bwL
1
0),,,,( αβαξ                  (12)     
0),,,,( =−−=∂
∂
iii
i
CsbwL βαξ
βαξ              (13)      
Apply these conditions into the Lagrangian (10), the problem 
(9) can be transformed into  
                  
Maximize   ∑∑∑ −=
= ==
l
i
l
j
jijiji
l
i
i xxKyyW
1 11
),(
2
1)( αααα               
Subject  (14)           liCsy
l
i
iiii ,....,1,00
1
=∑ ≤≤=
=
αα
And the Kuhn-Tucker conditions are defined as  
     
libzwy iii ,....,10)1)(( ==+−+⋅ ξα       (15)      
liCs iii ,....,10)( ==− ξα            (16)      
The point with the corresponding ix 0>iα is called a support 
one-against-all approach for the SVM and FSVM 
classifiers. Employ the different kernel functions and 
parameters in SVM and FSVM classification. 
Table 2. Results of One-against-all FSVM classification (%) 
 7
6. Compare and discussion the performance of SVM with 
FSVM.  
 
4. EXPERIMENTAL RESULTS  
 
500 monochrome texture images for each 64×64 subimage, 
obtained from the Brodatz image database shown in Fig. 1. are 
used in this study. The proposed approach was implemented 
using MATLAB on a machine, with a 2.4-GHz Pentium IV 
processor and 1 GB of RAM. We have applied one through 
three levels of wavelet decomposition (generating three to nine 
wavelet subbands). The level one wavelet decomposition has 
the best performance in all the tests. Therefore, to represent 
each texture image using the proposed model, we require only 
four parameters. After several experiments, the parameters 
were set as C =100, γ =0.5, =3 and Coef0=1 in both 
SVM and FSVM classification.  
d
 
4.1 Texture classification by SVM 
 
The results of multi-class texture classification are presented 
in Table 1. Table 1 shows the classification accuracies of SVM 
using linear, RBF and polynomial kernel functions 
approximate up to 100%, except when using the sigmoid 
function. The performance of the SVM classification using the 
RBF kernel function is the best kernel function compared with 
the other three. Noted, the classification accuracies are between 
56% and 86.7% for the sigmoid kernel function, which may 
not be a suitable kernel function for texture classification.    
 
Table 1. Results of One-against-all SVM classification  
Linear RBF Polynomial Sigmoid   
Training 
Accuracy 
Testing 
Accuracy 
Training 
Accuracy 
Testing 
Accuracy 
Training 
Accuracy 
Testing 
Accuracy 
Training 
Accuracy
Testing 
Accuracy
1 99.7 98.7 100 100 100 99.3 73.1 68.7
2 100 99.3 100 100 100 99.3 60.0 56.0
3 100 100 100 100 100 100 84.0 86.7
4 100 100 100 100 100 100 60.6 57.3
5 100 100 100 100 100 99.3 60.9 66.0
 
 
4.2 Texture classification by FSVM 
Linear RBF Polynomial Sigmoid 
Training 
Accuracy
Testing 
Accuracy
Training 
Accuracy
Testing 
Accuracy 
Training 
Accuracy 
Testing 
Accuracy
Training 
Accuracy
Testing 
Accuracy
1 100 99.3 100 100 100 99.3 78.9 78.0 
2 100 100 100 100 100 100 62.6 61.3 
3 100 100 100 100 100 100 86.8 86.7 
4 100 100 100 100 100 100 61.9 66.7 
5 100 100 100 100 100 100 63.1 65.3 
 
The results of the feature extraction and classification 
processing performed using SVM and FSVM classifiers are 
given in Table 1 and 2. Table 1 and 2 show the performance in 
terms of classification accuracy is robust for both SVM and 
FSVM, but the training time of SVM is shorter than that of the 
FSVM and it can always converge in a short time period while 
FSVM cannot. It is also shown the classification accuracies of 
FSVM indeed have improved over a limited range compared 
with those of SVM. However, since the 1000 Brodatz texture 
images are considered without adding noise in this experiment, 
it is more difficult to show the capability of FSVM to detect 
outliers or noises in the training procedure due to overfitting. It 
is worth mentioning the proposed approach uses different 
kernel functions and parameters for promising classification 
accuracies.  
 
5. CONCLUSTION 
In this study, two texture image classification approaches are 
introduced and compared. In the feature extraction phase of 
this proposed approach, the DWT decomposition coefficients 
and Co-occurrence features are used for increasing the 
accuracies rates of texture classification. Then, these effective 
features are given to the inputs of SVM and FSVM classifiers, 
respectively. Table 1 and 2 show the classification performance 
is promising and robust for both SVM and FSVM approach, 
but the training time of SVM is shorter than that of the FSVM 
and it can always converge in a short time while FSVM cannot. 
The classification performance of both SVM and FSVM in this 
study shows the advantages and merits of these classifiers.  
 
ACKNOWLEDGMENT 
The author would like to thank that this work was supported by 
the National Science Council, Taiwan, ROC, under Grant No. 
98-2221-E-159-003. 
  The results of multi-class texture classification have been 
presented in Table 2. Table 2 shows the classification 
accuracies of FSVM using linear, RBF and polynomial kernel 
functions are also approximating up to 100%, except when 
using the sigmoid function. The performance of FSVM 
classification using the RBF kernel function is the best kernel 
function compared with the other three. In addition, the 
accuracies of classification are between 61.3% and 86.7% for 
using the sigmoid kernel function, which may not be a suitable 
kernel function for texture classification.   
REFERENCES 
 
[1] Cortes C., V.N. Vapnik, “Support vector networks,” 
Machine learning, Vol. 20, pp. 273-297, 1995.  
[2] Vapnik V.P., The nature of statistical learning theory. New 
York: Springer-Verlag, 1995. 
[3] Vapnik V.N., Statistical learning theory. New York: Wiley, 
1998.  
[4] C.F. Lin, S.D.Wang S. D., “Fuzzy support vector 
machines,” IEEE Trans. on Neural Networks, Vol. 13, 
No.2, pp. 464-471, 2002.  
無研發成果推廣資料 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
