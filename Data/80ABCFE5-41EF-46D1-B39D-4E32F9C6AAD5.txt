store these data sustainably and reliably. This 
project proposes the use of cloud computing to 
construct a data storage and management information 
system for such purpose. Cloud computing enables the 
access to large computation resources through the 
Internet to maintain sustainable operations and 
reliable storage, and layouts a firm foundation for 
accumulating tunnel inspection and monitoring data in 
the long run. The stored data need to be effectively 
used.  Thus, a visualization system is developed to 
fuse and visualize tunnels＇ 1) inspection and 
monitoring data, 2) basic geometrical data, 3) 
geological condition, and 4) host mountain＇s 
topography. Visualization enables deep understanding 
of the data, facilitates communications between 
users, and help users identify the cause of 
anomalies. 
MongoDB, a cloud-based database management system, is 
used in this work for sustainable data storage. A 
visualization system written in C++ helps utilize the 
inspection and monitoring data. The system has a 
friendly graphical user interface developed using Qt 
class library, and its visualization is done using 
VTK. Furthermore, code generators were created using 
PHP to generate C++ classes for accessing MongoDB. 
 
英文關鍵詞： cloud computing, tunnel inspection, tunnel 
monitoring, information system, visualization, 
computer graphics, virtual realty 
 
2 
 
           圖 1 傳統異狀判釋方式                   圖 2 維護管理資料系統概念圖[4] 
二、文獻探討 
2.1 隧道檢監測作業技術與記錄格式  
目前國內針對隧道結構安全檢測，已採用目視測繪與全斷面掃描等方式進行初步調查，而異狀顯
著處進行襯砌調查或混凝土取樣及強度試驗等調查[5, 6] ，嚴重者進行混凝土鑽孔及強度試驗等二次
檢測或詳細檢測，並且針對試驗結果實施補強措施，並根據不同的階段，將使用不同的作業技術。此
外，監測系統的配置，通常由業主單位委託進行探查與設置監測儀器並進行監控與改善的規劃。常見
的監測項目有地中傾斜管、水壓計、結構物傾斜計、地錨荷重計及雨量計等五項監測儀器，且針對每
個項目在不同地點設置多個監測儀器[7]。然過去鮮少探討適當的資料模型以統合不同監測儀器之資
料與數據。 
2.2 隧道維護管理資訊化現況 
國外在十數年前即已注意到應將隧道之維護管理予以資訊化。美國 FTA (Federal Transit 
Administration) 於 2001 年己提出的隧道管理系統應含有檢測、管理、維護與修復等四大功能，並進
行規劃。針對國內隧道檢監測的資訊化記錄系統，於 2007 年時由聯合大地工程顧問公司開發出的集
集隧道維護管理系統[8]，之後於中興工程顧問社運用 VB.NET 與 X3D 程式語言，發展了隧道營運維
護管理系統[9]。台北捷運則由設施維護管理的角度，建立一套台北捷運之維護管理系統[10]。 
2.3 雲端技術與 NoSQL 資料庫管理系統 
根據美國國家標準局(NIST, National Institute of Standards and Technology)針對雲端的定義[11]，雲
端應該具備五大特徵(5 Characteristics)、可分為四個佈署模型(4 Deployment Models)與三種可能服務模
式(3 Service Models)。而雲端運算本質上即為分散式運算(Distributed Computing)，強調使用可透過網
路存取之運算資源來解決問題或提供服務，如資料雲的概念[12]。 
許多新一代的網站系統如 Facebook、Google 等大型網站，為了解決存取或處理大量資料時的效
能問題，捨棄了關聯式資料庫技術，改用 NoSQL 資料庫來提升效能與擴充彈性[13]。目前已開發出
之 NoSQL 資料庫系統可大致分為鍵值資料庫(Key-Value store)、圖形化資料庫(Graph)與文件型資料庫
(Document store)等，其中以鍵值資料庫是最常看到的，最大特色就是採用 Key-Value 的資料架構，並
且每筆資料各自獨立，以 BigTable、HBase、Dynamo、Cassandry、Hypertable 等屬於 Key-value 這類
的資料庫。而文件型資料庫較適合儲存非結構性的文件，也就是欄位較不像表格般固定，其中的代表
以 MongoDB、Riak、CouchDB 等。而圖形化資料庫專門用來處裡樹狀或網狀結構較適合，像是地理
資訊系統或者複雜關係的系統，其中的代表以 Neo4j、InfoGrid 等為主[14]。而在選擇 NoSQL 資料庫
時，可依據應用之需求，考量分散式系統中 CAP(Consistency、Availability、Partition Tolerance) 等三
特性之比重[15]加以選擇。而本研究認為 MongoDB 相當適合於建置隧道檢監測資料庫。 
2.4 虛擬實境與視覺化技術 
虛擬實境(VR, Virtual Reality)，為利用電腦模擬產生一個三維的虛擬空間，提供使用者自然與即
時地觀察三度空間內的事物，如同身歷其境進行視覺等感官的模擬。於隧道工程方面，中興工程顧問
社應用虛擬實境技術，結合了 VB.NET 與 X3D 程式語言開發三維視覺化模組[9]，能夠提供管理者更
擬真的隧道實況資訊。然由由於其利用 X3D 建置虛擬實境之場景，將會受限於其檢視器(viewer)提供
之功能，可能無法有效提供檢監測資訊。 
本研究針對隧道的專案進行建模與資料呈現的動作，使用者可進行整個隧道專案的概觀預覽，與
走進隧道內部進行資料勘查。而上述的功能皆可使用 VTK 所提供的類別去協助完成，此外 VTK 架
4 
 
境。另外特別的是，本論文將檢測的資料分為三種型態，分別為評等式資料、描述式資料與連續
式資料。 
(5) db.Tunnel_Type: 記錄隧道的型態所用 (ex.公路隧道、鐵路隧道等)。 
(6) db.Event_Type: 記錄事件型態所用 (ex.地震事件、爆炸事件等)。 
(7) db.Monitor_Item: 記錄監測儀器的類型 (ex.地中傾斜儀、雨量計等)。 
(8) db.InspectionType: 記錄檢測的型態 (ex.平時檢測、臨時檢測等)。 
(9) db.Unit: 記錄於隧道檢監測資料中可使用的單位 (ex. mm、cm)。 
(10) db.AdminstrativeRegion: 記錄行政區域所使用 (ex.台北市、新北市等)。 
(11) db.Environment_Image_type: 記錄環境影像類型 (ex. 地形圖、空照圖等)。 
 
圖 4 本系統資料庫模型 
db.Tunnel
_id: string,
Name: string,
StartMileage: string,
EndMileage: string,
Length_M: double,
OpeningDate: Date_t,
Type: string,
Note: string,
BenchmarkCoordinate: [double],
MonitorDBRef: [string],
InspectionDBRef: [string],
TunnelEvent: [{
Name: string,
Date: Date_t,
Type: string,
Location: string,
Who: string,
What: string,
How: string
}],
TunnelFile:[{
Name: string,
Note: string,
Keyword: [string]
}],
TunnelImage:[{
Title: string,
StartMileage: string,
EndMileage: string,
Date: Date_t,
FileName: string,
type: string
}],
EnvironmentInfo:{
AdminstrativeRegion: string,
TerrainOverview: string,
GeologicalOverview: string,
NearEngineeringOverview: string,
Note: string,
EnvironmentImage:[{
Title: string,
Date: Date_t,
Contents: string,
Scale: string,
SourceStartMileage: string,
FileName: string,
Type: string
}]
},
Geometry:{
CrossSectionalShape:[{
Order: int,
Coordinate: [double]
}],
Linear:[{
Order: int,
Coordinate: [double]
}]
},
Abnormal:[{
IsOutside: string,
AbnormalStartMileage: string,
AbnormalEndMileage: string,
Coordinate: [double],
AbnormalType: string,
Description: string,
Observed: Date_t,
Point:[double]
}]
db.MonitorInformation
_id: string,
MonitorItem: string,
BenchmarkElevation: double,
AutoRecode: string,
MonitorCoordinate: [double],
IsOutside: string,
DataName: [string],
DataUnit: [string],
MonitorData: [{
DID: string,
DateTime: Date_t,
Value: [double]
}]
db.DEMCollection
_id: int,
x: double,
y: double,
z: double
db.InspectionInformation
_id: string,
MethodName: string,
InspectionType: string,
DateTime: Date_t,
StartMileage: string,
EndMileage: string,
Degree: double,
Coordinate: [double],
Detector: string,
Note: string,
DescriptionData:[{
DescriptionItem: string,
DescriptionContents: string
}],
ContinuousData:[{
DataName: string,
DataUnit: string,
Value: double
}],
RatingData:[{
RatingName: string,
RatingValue: int
}],
Marks: [{
Type: string,
Name: string,
Note: string
}],
InspectionImage:[{
PictureTitle: string,
Date: Date_t,
Contents: string,
Note: string,
StartMileage: string,
EndMileage: string,
FileName: string,
Item: string
}]
db.Tunnel_Type
_id: string
db.Monitor_Item
_id: string,
NumberOfValue: int
db.Event_Type
_id: string
db.Environment_Image_Type
_id: string
db.InspectionType
_id: string
db.Unit
_id: string
db.AdminstrativeRegion
_id: string
Reference
Reference
6 
 
  
圖 7 檢測資料程式架構圖                      圖 8 監測資料程式架構 
 
  
      圖 9 三維視覺模擬程式架構圖                圖 10 其它類別圖 
另外 WGS84_TWD97TM2_Transform 用於 WGS84 轉成 TWD97 之格式所使用，本論文於視覺環
境中所使用的皆為 TM2 的座標系統，然實務上許多資料乃以 WGS84 座標系統儲存，因此必需進行
座標轉換。本研究使用 Proj[16] 函式庫解決座標轉換之問題。 
3.4 自訂義資料庫存取介面 
本研究針對資料庫的開發與規劃本系統所需要的功能後，發現其中有許多通用的模式去進行資料
的存取與尋找，經過整理後可分為 (1)FindDB(尋找資料)、(2)SaveDB(儲存資料)、(3)UpdateDB(更新
資料)與 (4)針對 Collection 本身所包含的 Object(物件)或 Array Object(陣列裡面包含物件)進行新增與
刪除的功能。所以利用物件導向設計模式，撰寫 PHP 程式，利用其解析以 JSON 格式定義之資料模
Qt 
InspectionInformation
RatingData
ContunuiusData
DescriptionData
InspectionImage
InspectionType
AddInspection
CreateInspectionType
InspectionInfoView
1
0..*
1
0..*
1
0..*
1
0..*
1
0..*
TunnelTunnel3D
1
0..1
WGS84_TWD97TM2_Transform
1
0..*
1 0..*
A B A uses  B
A has B (Composition)A B
A B
1 0..*
A has B (Aggregation)
Marks
1
0..*
Qt 
Monitor_Item
MonitorInformation
MonitorData
AddMonitorData
CreateMonitor
MonitorInfo
MonitorItem
1
0..*
Unitview
Unit
1
0..*
1
0..*
Tunnel
1
0..*
1
0..*
1 0..*
A B A uses  B
A has B (Composition)A B
Qt 
Unit
Login
MainWindow
Unitview
WGS84_TWD97TM2_Transform
dem_erdas
SelectTunnel
AddInspection
AdminstrativeRegionView
CreateEnvironmentImageType
CreateMonitor
CreateTunnel
EnvironmentInfoView
InspectionInfoView MonitorInfo
MonitorItem
Tunnel3D
TunnelEvents TunnelFilesView
TunnelInfo
UploadLinear
UploadCrossSectionalShape
CreateTunnelType
DEMCollection
8 
 
系統先讀取隧道斷面展開圖後，事先建立輔助板的基本網格，之後建立材質座標後繪製出來，如圖 12
所示。並且透過 VTK 所提供的 vtkSeedWidget 建構與使用者之互動，讓使用者可以進行標註的功能。
如此可方便使用者輸入檢監測資料或對於異狀判釋輸入註記，並透過即時之三維呈現可避免資料位置
之錯誤輸入，如圖 13 所示。 
 
圖 12 檢測位置標示板 
 
圖 13 二維平面標示點建構至三維模擬環境成果圖 
4.4 三維互動資料探索系統操作設計 
Qt creator 是一個完整程式開發環境 (IDE)，並額外支援 Qt 的圖形化使用者介面(Graphics User 
Interface, GUI)開發，其中內容包含了編輯模式與 UI 設計模式，更有豐富的套件可以快速開發出所需
要的 GUI。所以本研究運用 Qt 的元件，控制周邊環境顯示的範圍與進行隧道內部移動的操作做為主
軸，使用拉桿(QSlider)讓使用者可針對 DEM 的範圍進行調整，並透過 Qt 所提供的 LCD 顯示元件
(QLCDNumber)可計算使用者所觀看的範圍大小。另外使用工具按鈕(Tool button)讓使用者可進行資料
讀取的開關。其中 Slider 結合 LCD 元件與 DEM 模型之互動關係，使用者拉動拉桿，會將拉桿的數
值傳入 DEM 模型繪製的功能中，透過重新計算 DEM 所需要顯示的範圍，並且將攝影機重新設置後，
後重新繪製 DEM 模型。 
本研究將檢監測資料匯入至視覺化模擬環境中，以圓球體的狀態去標示監測儀器的位置，使用者
可使用 Tool button 進行開關，並且於滑鼠停留在球體上，顯示相關的檢監測資訊，當使用者開啟顯
示監測資料的按鈕，程式在視覺模擬的類別中，呼叫資料庫中存取介面中的隧道資料，取得此專案內
所記錄的監測儀器 ID 後，呼叫資料庫存取介面中的監測儀器資料，並且根據監測儀器所記錄的位置
將資料給繪製出來，且使用者亦可透過拉桿(Slider)進入隧道內部進行觀察。繪製成果展示於圖 14 與
圖 15。 
10 
 
 
圖 17 案例隧道專案基本資料 
 
圖 18 案例隧道地形概述 
 
圖 19 隧道環境檢視成果 
根據監測報告書內所提供的監測儀器座標位置，於本專案中，建立各監測儀器資訊，並將提供的
資料進行輸入的動作，如圖 22，此外，不同的監測儀器所記錄的數據不同，選擇監測儀器編號 GP01
的監測資料觀看，此監測儀器類型為 GPS 測點，記錄的數據為 N、E、Z 的 TM2 座標系統資料與高
程資料，且資料頻率為每年記錄一次。於記錄的資料內可觀察到該點位每年的變化情況。並根據案例
隧道專案報告書所提供的隧道斷面異狀繪製展開圖，將真實影像貼附至隧道斷面上，並結合異狀繪製
展開圖貼放於檢測位置標示板上，並針對圖片上異狀顯著處於異狀檢測板上進行標記，並於視覺化環
境中走入隧道內的位置觀看。並且於查詢資料時，將滑鼠移動至檢測的資料點上方，系統將會自動顯
示出檢測時所輸入的內容，如圖 23。 
  
12 
 
 本研究所發展的隧道異狀輔助判釋系統，亦可應用於隧道檢監測相關的教學上，透過資料庫所儲
存的檢監測資訊，與直觀的視覺環境，未來學生將不需要進入現場，就能夠走入隧道內部與了解
檢監測資料以及模擬隧道異狀判釋，亦能作為隧道檢監測教育訓練之案例教學與異狀診斷平台。 
誌謝 
本研究承蒙國立臺北科技大學王泰典教授的大力協助，除提供寶貴的資料外，並給予多方指教，特此
感謝。 
參考文獻 
1. 馮正一、王天佑、洪世勳、陳錦清, 個人數位助理在大地工程上的應用. 中興工程季刊, 2002. 
2. 張吉佐、侯秉承、劉弘祥, 台灣山岳隧道工程技術之回顧與展望. 地工技術第 100 期, 2004. 
93-106. 
3. Hsieh, Y.M. and Y.S. Lu, A Visualization System for Field Monitoring Data in Construction 
Engineering, in Automation in Constructionin revision. 
4. 鄒昌佑, 應用視覺化技術於隧道異狀輔助判釋之初探, in 營建工程系 2012, 國立台灣科技大
學. 
5. Asakura, T. and Y. Kojima, Tunnel maintenance in Japan. Tunnelling and Underground Space 
Technology, 2003. 18(2-3): p. 161-169. 
6. 蕭牟淵、游本志、王泰典、蕭興臺, 台灣公路隧道安全檢測及評估之研究. 台灣公路工程第 36
卷第 5 期, 2010. 
7. 永碁工程顧問有限公司、交通部公路總局第三區養護工程處, 省道 20 號線嘉寶隧道地區後續
監測計畫工作, 2008. 
8. 聯合大地工程顧問股份有限公司, 隧道維護管理系統使用手冊. 2009. 
9. 李國榮、龍明治、高憲彰, 隧道異狀之繪製及健全性評估方法研究, in 第十三屆大地工程學術
研討會 2009. 
10. 高宗正、陳俊宏、嚴崇一、黃佳駿、李維峰, 台北都會區大眾捷運系統結構物維護管理資訊系
統之建立. 管道工程技術, 2009. 36: p. 73. 
11. NIST, The NIST Definition of Cloud Computing, in Computer Security2011: U.S. Department of 
Commerce. 
12. 黃重憲, 淺談雲端運算. 國立台灣大學計算機及資訊網路中心電子報第 00008 期, 2009. 
13. Muthukkaruppan, K. The Underlying Technology of Messages. 2010; Available from: 
https://www.facebook.com/notes/facebook-engineering/the-underlying-technology-of-messages/454
991608919. 
14. Kovács, K. Cassandra vs MongoDB vs CouchDB vs Redis vs Riak vs HBase vs Membase vs Neo4j 
comparison. Available from: http://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis. 
15. Hurst, N. Visual Guide to NoSQL Systems. 2010; Available from: 
http://blog.nahurst.com/visual-guide-to-nosql-systems. 
16 Edgewall. Proj API Reference. Available from: http://trac.osgeo.org/proj/wiki/ProjAPI. 
 
對不同的稀疏矩陣格式進行效能之評估，而他也順利完成了他首次在國際會議上
的發表。第二篇發表之文章為「Enhancing Performance of Meshfree Methods by 
Hybrid Computing」，亦由潘茂森同學發表，以增進他在國際研討會發表之經驗，
此發表被安排在第二天下午之「Computational Mechanics」session，此文章中介紹
了我們如何透過 GPU 加速無元素分析之計算時間。而第三篇發表之文章為「Ｒ
eliability Assessments on a Distributed-Computing Based Monitoring Information 
System」，被安排在第三天近中午之「Distributed Computing in Engineering」session，
此發表介紹我們在解決大規模監測專案時使用之分散式運算架構，並針對此架構
進行了可靠度分析，以協助系統管理者決定其佈署方式。 
  而除了參與前述之三個議程外，本人也參與了如「Disaster Preparedness, 
Response and Recovery with IT 」、「Computer Simulation for Civil and Building 
Engineering」、「Building Information Modeling, Industry Foundation Classes」、
等議程，並與其它國內外學者進行意見的交流與討論。 
 
   
二、與會心得 
  此次會議中，GPU (圖形處理器) 之應用成果已逐漸顯現，除了本人發表
之文章外，如「Seismic damage simulation for urban buildings based on high 
performance GPU computing」之發表即利用 GPU 技術去作大規模都會地區的
震災評估，並可達到十分良好之加速；「Parallel finite element mesh generator 
using multiple GPUs」則是利用多個 GPU 來加速有限元素網格之產生。 
  此外，BIM (建築資訊模型) 仍然是相當熱門之研究課題，包括如何
利用 BIM 進行設施管理、使用 BIM 之工作流程、BIM 可能造成之法律
訴頌、應用 BIM 進行建築標準之自動檢查、利用 BIM 估算改建或重建
之廢棄物量、BIM 導入之成熟度評估、BIM 應用於高效率節能之建築等
等。 
三、考察參觀活動(無是項活動者略) 
  略 
四、建議 
  本次會議的論文集完全採用電子全文的形式，不再像過去附上一本厚厚的論
文集，僅將各場次發表的論文摘要集結成薄薄的一冊。此作法不但可減輕與會人
員的負擔、並可達到節能減碳的目的，建議國科會未來在補助國際會議舉辦時，
能強烈建議舉辦單位使用相同的作法。 
 
五、攜回資料名稱及內容 
  會議論文集之電子全文。 
六、其他 
  本次會議議程可由官方網站 http://www.icccbe.ru 下載，透過
http://www.icccbe.ru/frontend/index.php?page_id=194 之互動式議程可找到
下載本人此次會議發表之三篇文章，應足以佐證文章已被接受。 
  

   
 
Abstract 
Field monitoring cost has been steadily decreasing due to the advances in wireless sensor network 
(WSN), system-on-a-chip (SoC), and microelectromechanical systems (MEMS).  We are already 
swimming in a sea of sensors (PCAST 2010).  It is thus vital to design an information system 
architecture that effectively store and manage vast amount of monitoring data.  For this purpose, this 
paper introduces service orientated information system architecture.  This design use distributed 
computing to help manage monitoring data.  It is well suited to deploy in cloud-computing based 
facilities.  It can also be used as a reference to build monitoring information systems.  In addition, 
reliability analyses are introduced on vital functions for such systems.  These analyses help developers 
design system processes in the information system.  They can also help system administrators decide 
system deployment schemes to achieve desired data reliability or service levels. 
Keywords: information system, monitoring, distributed computing, cloud computing, reliability 
1 Introduction 
Monitoring data are valuable for civil and construction engineering projects.  Monitoring data are 
used in safety monitoring for early warning and hazard prevention (Aktan et al, 1998; Wong 2004; Li 
et al. 2006;).  These data can verify accuracy of analytical or numerical models (Finno and Calvello 
2005; Hashash et al. 2006).  They can build predictive models through regression, artificial 
intelligence, or data mining (Liang et al. 1999; Malkawi et al. 2000).  Finally, they are basis in 
decision-support systems (Cheng and Ko 2002; Cheng et al. 2002).  Because monitoring data have 
many applications, and they record real responses of civil and infrastructure systems, they are thus 
very valuable.  Therefore, it is vital to store and manage them properly. 
Fig. 1 shows typical automated monitoring information systems.  They have four main 
components: 1) transducers, 2) data loggers, 3) Internet connections, and 4) Internet servers and 
databases.  Transducers measure physical quantities (e.g. water pressure, settlement, etc.) and output 
analog or digital signals (e.g. voltage or duty cycles).  The signals are then read and digitized by data 
loggers.  The digitized data is then sent over the Internet to the Internet server, and they in the end get 
stored into the backend database.  Finally, the Internet server or the application server responds to 
user requests such as charting and querying of monitoring data.  
Scale of automated monitoring will grow rapidly in next few years.  This is owing to the advances 
of wireless communication, MEMS sensors, etc.  As a result, conventional setup shown in Fig. 1 can 
face scaling issues in bandwidth, storage space, processing power, etc.  It has been noted that we are 
Reliability Assessments on a Distributed-Computing Based 
Monitoring Information System 
Y.-M. Hsieh & Y.-H. Chen 
National Taiwan University of Science and Technology, Taiwan, Republic of China 
 
 
   
 
Data service (DS) is the storage service for monitoring data.  In other words, all monitoring data 
are stored and managed by DS.  It must be noted an information system may use many DS.  This 
serves two purposes: 1) to scale up the storage capacity without interruption and 2) to provide data 
redundancy.  First, conventional client-server architecture cannot expand storage capacity without 
shutting down server machines or using special hardware.  The proposed system design can increase 
system storage capacity by adding more DS.  As a result, no system interruption is needed when 
adding capacity.  Second, data redundancy can be easily achieved by duplicating data on two or more 
DS.  This is further discussed next. 
Field service (FS) runs on data loggers or field computers.  It is responsible for getting 
instrumentation data and sending them to the designated DS.  This is a very simple task.  However, 
there are two important designs that must be noted: 1) there may be one or more designated DS, and 2) 
the DS designation is temporal.  When monitoring data are important, one may choose to assign two 
or even more DS to one FS.  This introduces data redundancy and improves data reliability.  Our 
design also introduces temporal DS designation.  The DS designation is provided by coordination 
service to be introduced.  By using temporal DS designation, we can avoid system interruption during 
capacity upgrade or system maintenance.  Before a DS runs out of its disk space, the system 
administrator or some supervising programs can find all field services associated with this particular 
DS and route all further data storage needs to other DS by configuring coordination.  As a result, there 
is no need for service interruption. 
Coordination service (CS) is a very simple yet extremely important.  Its sole purpose is to respond 
to field services about their designated DS.  It other words, the associations between field services and 
data services are managed on CS, or CS couples FS with DS.  As noted previously, this coupling 
helps achieve data reliability, capability scalability, and always-on accessibility.   
Figure 2. A distributed system architecture for automated monitoring systems 
2.2 Deployment Schemes 
It must be noted in previous discussions we mentioned services, not servers.  This is because software 
services are software components deployed on servers, and there is usually no limit on how many 
services can be deployed on one server.  As a result, there are many possible schemes to deploy the 
aforementioned services.   In the following, we show two out of many possible deployments for 
   
 
3.1 Service invocation reliability 
CS, DS, and FS collaborate with each other to build a functional information system for storing and 
managing monitoring data.  The interactions between them are based on web-service invocations.  
Assuming the client computer tries to invoke a service offered by the server computer.  First, the 
client sends a service request over its Internet connection.  The request is received through the Internet 
connection of the server.  The server then processes the request and sends the response over its 
Internet connection.  Finally, the response is received by the client over its Internet connection.  In 
this process, failure can occur on 1) the client when sending requests, 2) the Internet connection of the 
client when sending requests, 3) the Internet connection of the server when receiving requests, 4) the 
server, 5) the Internet connection of the server when sending responses, 6) the Internet connection of 
the client when receiving responses, and 7) the client when receiving responses. 
We assume 1) failures are owing to hardware and networking issues, 2) software are implemented 
perfectly, and 3) all processes involved are independent.  Under such assumptions, the reliability of 
service invocation can be estimated using Eq. 1. 
        2 2 21 1 1 1 1SI C IC IS SF F F F F          (1) 
In Eq. 1, SIF , CF , ICF , ISF , and SF  represent failure probabilities of service invocation, the client 
computer, the Internet connection of the client computer, the Internet connection of the server 
computer, and the server computer.  Let’s assume these probabilities are all 3%, Eq. 1 then suggests 
the SIF  is 19.2%.  In other words, the service invocation is more likely to fail than any individual 
component or process involved.  This result seems to against using distributed computing for building 
information systems. This is true only if the software is implemented poorly. In network programming, 
retry mechanisms are often implemented to fight against unreliable networking.  Eq. 2 and Eq. 3 
estimate the failure probability of service invocation with considerations of the retry mechanism. 
     1
1
1 1 1
NT nretry
SI C retry retry
n
F F F F


     (2) 
      2 21 1 1 1 1retry C S IC ISF F F F F       (3)  
 
In Eq. 2, retrySIF  estimates failure probability of service invocation after NT  attempts. Eq. 3 
computes failure probability of one retrial under all above assumptions.  In addition, Eq. 3 also 
assumes if the client computer spots a failure in the former service invocation, it will retry the 
invocation.  Meanwhile, the client computer will not fail.  Fig. 4 shows the how retrySIF  decreases the 
increasing number of retrials.  It can be seen the retry mechanism is very effective at making service 
invocation reliable.  It also shows retrySIF  ultimately approaches CF .  In other words, by giving 
sufficient number of retrials, the reliability of service invocation is solely determined by the reliability 
of the client computer.  This result also suggests service invocation can be as reliable as local function 
calls.  Both of their failure probabilities are solely determined the computer making function calls or 
service invocations. 
   
 
3.3 Server-level storage reliability 
Data storage is also an important function for monitoring systems.  Typical client-server architecture 
stores all monitoring data on one computer, and data are stored in hard disks.  Hard drives, comparing 
to other components in computer systems, can break easily due to their mechanical nature.  According 
to a large-scale survey done by Google (Pinheiro et al. 2007) in their data centers, the AFR (annual 
failure rate) is 2% for hard drives in their first service year.  AFR becomes 7% if the service life is 
more than two years.  Therefore, if data should be kept long term, it is necessary to evaluate and to 
develop solutions to resolve relatively unreliable storage medium. 
The information system design described in Section 2 can provide two levels of protections.  At 
the first level, one may use RAID (redundant array of independent disks) on servers to keep data safe.  
This is the server-level storage reliability. The distributed information system can additional provide 
system-level data protection.  This is described in section 3.4. 
RAID has been protecting data on servers for many years.  Modern computer mainboards now 
have this capability built in.  There are many RAID levels.  Different levels provide different 
characteristics.  Some RAID levels provide speed, and some provide data safety.  In the following, we 
discuss two commonly used RAID levels and their reliabilities.  
RAID-1, also known as mirroring, mirrors content on one hard drive to others.  In this 
configuration, data is lost only when all drives constituting the disk array fail.  Assuming the failure 
probability of one hard drive is hdF , then the failure probability 1RAIDF   for a RAID-1 array configured 
using hdN  is: 
  1 hdNRAID hdF F   (6) 
If hdF  is 2%, then a RAID-1 array with two drives has a failure probability of 0.04%.  RAID-1 
array with four drives would have a very low failure probability at 51.6 10 % .  Therefore, RAID-1 is 
very effective at reducing the risk of losing data. 
Another commonly used RAID configuration is RAID-5.  This configuration needs at least three 
disks and allows one disk failure without losing data.  The failure probability of RAID-5 arrays is: 
     145 0 11 1 1hd hdhd N NNRAID hd hd hdF C F C F F        (7) 
Thus, a RAID-5 array with four drives has a failure probability of 0.23%, much higher than using 
RAID-1 array.  By using Eq. 6 or Eq. 7, we can assess reliability of data storage on servers running 
DS.  However, it must be noted these equations assume the failure between hard disks are 
independent.  However, disk failures are sometimes related to improper ventilation, power 
fluctuations, etc.  As a result, the assumption is optimistic.  The true failure probability could be 
higher than those calculated using Eqs. 6 and 7. 
3.4 System-level storage reliability 
As suggested early, the system designed can associate many DS to one FS to keep data at different 
servers.  We refer this as the system-level storage reliability.  System-level redundancy is essentially 
the same as the RAID-1 configuration.  In other words, assuming one FS is associated with DSN , then 
the probability of data loss, SYSF : 
   DSNSYS DSF F  (8) 
DSF  is the failure probability of disks hosting DS.  It could be simple disk failure probability.  It 
could also calculate from Eq. 6 or Eq. 7, depending on the disk configuration on the server.   
Eq. 8 also bears the assumption of independence of server disk failures, as in previous discussions.  
It must be emphasized that the independence assumption at system level is more likely to be true than 
   
 
Abstract 
Hybrid computing technique is used in this study to significantly enhance the performance of 
meshfree methods.  These methods are typically slower than finite element methods (FEM) mostly 
because their stiffness matrices are much denser ones formed by FEM.  As a result, both forming 
stiffness matrices and solving equations are much slower.  In this paper, we report our use of NVidia 
based accelerators and CUDA programing techniques.  We also demonstrate that our hybrid 
computing technique is generally applicable to most meshfree methods, and can significantly boost 
their performance.  Further performance improvements are possible by porting more portions of our 
code from host to device. 
Keywords: hybrid computing, meshfree methods, CUDA, GPU, EFG. 
1 Introduction 
Meshfree or meshless methods are new methods for solving partial differential equations. They have 
general applicability just like the finite element method (FEM). Furthermore, they can easily solve 
problems involving crack propagation and large deformation (Li and Liu, 2002; Chen et al., 1997).  
These challenging problems are hard to solve using mesh based methods such as FEM. Although 
meshfree methods have some advantages over FEM, their computing speed is inferior. Their time to 
solution tends to be much longer than FEM (Bathe and De, 2001). The adoption rate of meshfree 
methods can be much greater if their speed was comparable to FEM. Thus, seeking for methods to 
enhance the performance of meshfree methods is important. 
Handling sparse matrices is one area that is often overlooked in meshfree methods. In our other 
paper in this conference proceeding (Pan and Hsieh, 2012), it is revealed that significant portion of 
computation time in meshfree analyses is spent on sparse matrix related operations. Further 
investigations show adding local stiffness matrices (LSM) to the global stiffness matrix (GSM), or 
GSM assemblage, takes the most time in these operations. In this process, the data structure for 
storing the GSM plays an important role. Simple data structures (e.g. dense matrix) allow fast GSM 
assemblage but need huge amount of memory; advanced data structures (e.g. compressed row storage, 
CRS (Sherman, 1975)) need much fewer memory space but incur overhead for accessing elements.  
This overhead is due to the need to find the proper element to be accessed in the stored data. It can be 
as little as a simple addition; it can also be as much as performing a linear search. Consequently, the 
data structure for storing GSM dictates the computing time for meshfree analyses. This is 
demonstrated in our other contributed paper (Pan and Hsieh, 2012). It is also interesting to note that 
Enhancing Performance of Meshfree Methods by Hybrid Computing 
Yo-Ming Hsieh & Mao-Sen Pan   
Department of Construction Engineering, National Taiwan University of Science and Technology, 
Taipei, Taiwan, Republic of China 
 
 
   
 
we need only one sparse matrix data structure and we can control our code to use more memory for 
efficiency or sacrifice performance for solving large problems.   
 
Figure 1. illustrated row-blocks on CRABS. 
3 Strategies to speedup GSM assemblage using hybrid computing technique 
In this work, Intel CPU and NVidia accelerator (Tesla C2050) are used simultaneously to perform 
meshfree analyses.  Intel CPU is a good general purpose processor, and is responsible for most tasks 
in meshfree analyses. These tasks include finding neighbouring nodes, compute shape functions, 
gauss integration, solving linear system of equations, etc. Tesla C2050 is solely responsible for 
assembling GSM using CRABS. This choice is made because we want to utilize the high memory 
bandwidth available on Tesla C2050 device. This high memory bandwidth can only be achieved with 
proper memory access patterns and multiple threads. Such requirement limits our choice on which 
part of meshfree analyses can be ported onto this device. During the GSM assemblage, there is a high 
demand of memory bandwidth in order to find the proper location to store each element in LSM.  
Each element is LSM is independent from others, and therefore this task is highly parallel. For a LSM 
of size 16x16, there are 256 elements that can be searched simultaneously and stored into GSM.  
Typical meshfree methods have LSM much larger than 16x16, and therefore it is suitable to offload 
GSM assemblage from CPU onto Tesla C2050. To make the hybrid computing efficient, it is 
necessary to let CPU and Tesla C2050 do computational work at the same time.  We achieve this 
during GSM assemblage. CPU is used to compute LSM, which is a small and dense matrix. Once 
LSM is calculated, it is transferred asynchronously to Tesla C2050. At the same time, CPU will 
compute the next LSM. Therefore, during GSM assemblage, both CPU and Tesla C2050 are busy. 
One is busy computing LSM, and the other one is busy adding LSM to GSM. This also makes CPU 
cache, which is relatively small, efficient, because it no longer need to store data for GSM. This 
design, as shown later in the paper, is proper and can achieve good speedup over our code 
implemented fully on CPU.  
 
Three strategies on Tesla C2050, or simply called the device, were devised in order to do hybrid 
computation. To use the device, it needs to use some toolkits.  There are two choices: NVidia CUDA 
and OpenCL. NVidia CUDA has reached maturity and mainstream, while OpenCL is relatively young. 
We used NVidia CUDA in our implementation.  Although OpenCL and CUDA are two different 
standards, their concepts are rather similar. We devised three different strategies to use the device for 
80 3 4 6 91 2 75
 
(a) assumed a row vector of sparse matrix 
0 3 4 61 2 75
 
(b) CRABS E=0 
0 1 2 75
Block 1 Block 2Block 0  
(c) CRABS E=1 
   
 
3.3 Strategy three 
Strategy three improves upon strategy two by using a different thread configuration.  This strategy 
organizes threads into three-dimensional thread blocks.  This is illustrated in Figure 4.  Three-
dimensional thread blocks help expose more parallelism.  In the first dimension of a thread block, 
different threads work on different row-blocks in a row.  In the second dimension of a thread block, 
different threads work on different rows of GSM.  Finally, threads in the third dimension of a thread 
block work on different elements within a row-block.  The three-dimensional thread blocks are then 
laid out on a two-dimensional grid to cover all row-blocks in GSM. 
 
Figure 4. Illustrated 3D thread block for strategy 3. 
4 Effectiveness evaluation of different strategies 
Effectiveness of the developed strategies was evaluated using the computing environment 
summarized in Table 2. Before our evaluations, the computer codes were first validated. This was 
done by computing benchmark problems in 2D and 3D.  The benchmark problems were plane-stress 
cantilever beam problem and point-load problem. These problems were chosen because they have 
analytical solutions, which can be used to validate the computation results. The computed solutions all 
agreed very well with analytical solutions.   
Once the computer codes were validated, evaluations were conducted. These evaluations involved 
benchmark problems of various problem sizes. This was done in order to know the effect of problem 
sizes on the archived speedups or slowdowns. Also, pure CPU computations were done so that 
speedups could be computed.  In the following, the speedup is defined by the total computation time 
of pure CPU computation divided by the total computation time using hybrid computation.  The 
following sections show our benchmark problems and achieved speedups. 
 
Table 2. Computational environments. 
CPU Intel Core i7 950 @ 3.07 GHz 
Accelerator NVidia Tesla C2050 
Memory  24 GB 
Operation System Gentoo Linux 64 bit 
Compiler Intel C/C
++
 Compiler 11.1 
Accelerator SDK CUDA SDK 3.2 
Solver Intel MKL, PARDISO 10.3.6 
 
4.1 Evaluations in 2D 
Figure 5 shows the 2D plane-stress cantilever beam benchmark problem. The beam subjects to a 
concentrated load at the free-end with P=1000 N. Depth of the beam, D, is 12m and length of the 
beam, L, is 48 m. The material of the beam is assumed to be linearly elastic, and has Young’s 
modulus E=3x10
7 
N/m
2 and Poisson’s ratio v=0.3. Nodes and integrated cells are uniformly 
X
Y
Z
3D Thread Block
thread block
thread
 
   
 
 
4.2 Evaluations in 3D 
Figure 7 shows a 3D point-load in elastic half-space problem. Roller boundary conditions are applied 
on all sides except the top surface.  The elastic medium has Young’s modulus of 3.0x107 N/m2 and 
Poisson’s ratio of 0.3.  The analysing domain uses dimensions of 1m by 1m by 1m. The point load is 
applied at the corner of the cubic domain assumed with a magnitude of 1N.  Nodes and integration 
cells are uniformly distributed within the problem domain with three selected cases listed in Table 4. 
Each integration cell has 125 quadrature points (5x5x5). 
Figure 8(a) shows speedups using CRABS with E=0.0. Similar to 2D results, strategy 1 and 3 give 
identical speedups, ranging between 3.0 and 3.5, while strategy 2 slows down the analyses.  
Comparing to 2D results, though, that 3D analyses have higher speedups than in 2D.  This is because 
the GSM in 3D tends to be larger and denser than ones in 2D.  There are more non-zero elements to 
be added to GSM for 3D analyses than for 2D analyses. 
Figure 8(b) shows speedups using CRABS with E=1.0. Again, strategy 1 and 3 give favourable 
speedups. It is seen in Figure 8(b) that strategy 1 outperforms strategy 3 in this setting, and a 
significant speedup of 12.0 is achieved using strategy 1. Strategy 3 also managed to achieve a speedup 
of 8 for the largest case. It is also seen in Figure 8 that speedup increases with increasing problem 
sizes.    
Figure 7. Cantilever beam model. 
Table 4. Three selected cases of the nodes and integration cells distributed in 3D model. 
a) b) c) d) e) f) g) h) i) 
1 7 7 7 1029 7 7 7 1715 
2 9 9 9 2187 9 9 9 3645 
3 10 10 10 3000 10 10 10 5000 
* a) cases, b) number of nodes in X direction, c) number of nodes in Y direction, d) number of nodes in Z 
direction, e) number of DOFs, f) number of cells in X direction, g) number of cells in Y direction, h) number of 
cells in Z direction, i) number of quadrature points. 
 
P = 1 N
L = 
1 m
W
 =
 
1 
m
X
Z Y
 
   
 
Abstract 
This paper discusses how memory usage and performance in meshfree analyses are affected by 
various data structures of sparse matrices and algorithms. In meshfree analysis, the global stiffness 
matrix is sparse, and it is necessary making use of this characteristic to make analyses efficient.  This 
means reducing the simulation time and/or the use of memory. Sparse matrices have been extensively 
discussed in the context of the finite element method.  It is rarely studied in meshfree method. This 
paper shows the sparse matrix format is very important in meshfree analyses.  Both memory 
consumption and performance are demonstrated in 2D and 3D meshfree analyses using various sparse 
matrix formats.  The results show that 98% of computation time can be attributed to assembling 
global stiffness matrix in extreme conditions. 
Keywords: Sparse matrix, meshfree methods, computational performance. 
1 Introduction 
The meshfree method is a new class of numerical methods for solving partial differential equations.  It 
is well know that this method is much slower than finite element methods (FEM).  This is believed to 
be relating to 1) additional spatial search (Chu, 2003), 2) more elaborated shape function construction 
(Chen, 2000), 3) high-order Gauss integration (Chen et al., 2002) and 4) more non-zeros in global 
stiffness matrix (GSM) (Chen et al., 2002).  However, no one seemed to pay attention to operations on 
the global stiffness matrix in meshfree analyses.  These operations can in fact dominate the entire 
analysis time, as shown in this paper. 
The GSM formed by both FEM and meshfree method are sparse.  This sparsity needs to be 
exploited to make analyses efficient.  For example, one can avoid storing zero entries in GSM to 
reduce memory consumption.  Mathematical operations can also be skipped on these entries to reduce 
computation time.  To achieve these benefits, however, a sparse matrix storage scheme or format is 
needed.  There were many sparse matrix storage schemes developed in the past.  How different sparse 
matrix formats affect the performance on meshfree analyses, however, was not discussed. 
In this work, we compare both performance and memory consumption of three different sparse 
matrix formats in meshfree analyses.  These comparisons are done for 2D and 3D analyses of various 
problem sizes.   We use element free Galerkin method (EFG, Belytschko 1994) as the meshfree 
method in this paper.  This method is widely used in literatures.   These comparisons can reveal 1) the 
fraction of time spent on sparse matrix operations in meshfree analyses, 2) how different sparse 
matrix formats affect the memory usage for storing GSM. 
The Evaluation of Sparse Matrix Performance in Meshfree Analysis   
Mao-Sen Pan & Yo-Ming Hsieh  
Department of Construction Engineering, National Taiwan University of Science and Technology, 
Taipei, Taiwan, Republic of China 
   
 
2.2 Performance evaluation method 
In order to evaluate relative performances of various sparse matrix formats, we first need to define 
how the time is collected.  Table 1 lists and comments on the steps performing operations related to 
sparse matrices in the EFG procedure.  We thus collect timing information from these steps, except 
step 4.  This exclusion is valid because 1) imposing essential boundary conditions needs some 
calculations unrelated to sparse matrices, and 2) this step uses very little time comparing to other 
steps.  Therefore, we collect time from step 2, 3, and 6.  However, step 3 is complex.  This step 
assembles GSM, and its computing time (in each quadrature point) includes, a) calculating 
approximation functions, b) computing local stiffness matrix, c) doing numerical integration, and d) 
adding local stiffness matrix (LSM) to GSM.  Only the last item (adding LSM to GSM) is related to 
the sparse matrix.  Hence, the time collection for this particular step follows three steps: 1) obtain time 
in step 3 as T1; 2) modify program to skip adding LSM to GSM and collect time as T2; 3) use T1 – 
T2 as the time spent in step 3. 
Table 1. The effect of sparse matrix in computational procedure of EFG analysis 
Operation Step Comment 
Collecting neighbouring 
nodes information 
2 
Some sparse matrix formats need neighbouring node information, 
and some do not.  This information is collected at this step. 
Assembling global 
stiffness matrix 
3 
Each field node first computes its local stiffness matrix (LSM) in 
dense format.  LSM is then summed into the global stiffness 
matrix (GSM) in sparse matrix format. 
Imposing essential 
boundary conditions 
4 This step accesses and updates entries in the GSM.  
Converting data into 
CRS format 
6 
GSM needs to be converted into compress row storage (CRS) 
format to be fed into PARDISO. 
In addition to collect time associated with sparse matrix operations, we also collect memory 
consumption for each evaluated sparse matrix format.  This is done analytically by computing needed 
memory for storing GSM for each sparse matrix format. 
In this work, we benchmark three different sparse matrix formats: Skyline (Jennings, 1996), CRS 
(Sherman, 1975), and CRABS.  CRABS is our home-grown sparse matrix format.  It features a 
parameter E to tune its performance.  Details of this format can be found in our other paper (Hsieh 
and Pan, 2012). 
By having both time and memory usage from various sparse matrix formats, we can understand 
and compare how different sparse matrix formats affect analyses done using EFG. 
2.3 Computational environments 
Table 2 summarizes the computing environment used for the performance evaluated in this study. 
 
Table 2. Computational environments 
CPU Intel Core i7 950 @ 3.07 GHz 
Memory  24 GB 
Operation System Gentoo Linux 64 bit 
Compiler Intel C/C
++
 Compiler 11.1 
Solver MKL PARDISO 10.3.6 
   
 
Figure 3. Total computing time spent on sparse matrices in 2D cases. 
Figure 4. Fraction of time spent on sparse matrices in 2D cases. 
3.3 Memory consumption 
Figure 5 shows the memory consumption for storing GSM using various sparse matrix formats.  From 
the figure, CRS and CRABS (E=1) use less memory; CRABS(E=0) and Skyline use much more 
memory.  The memory consumption grows with increasing DOFs. In case 9, the difference on 
memory consumption between CRABS(E=0) and CRABS(E=1) is about 24 times.  In other words, 
sparse matrix formats affect memory consumptions in EFG analyses significantly.  
Figure 5. Memory consumption evaluates for sparse matrices in 2D cases. 
0
5
10
15
20
25
30
35
40
45
0 1 2 3 4 5 6 7 8 9 10
To
ta
l c
o
m
p
u
ti
n
g 
ti
m
e
o
n
 s
p
ar
se
 
m
at
ri
x 
(s
e
c.
)
Cases
CRS
CRABS E=1
CRABS E=0
Skyline
 
0.0%
10.0%
20.0%
30.0%
40.0%
50.0%
60.0%
70.0%
80.0%
0 1 2 3 4 5 6 7 8 9 10
Fr
ac
ti
o
n
 o
f 
ti
m
e
 s
p
e
n
t 
o
n
 s
p
ar
se
 
m
at
ri
x 
co
m
p
u
ta
ti
o
n
Cases
CRS
CRABS E=1
CRABS E=0
Skyline
 
-500
0
500
1000
1500
2000
2500
3000
3500
0 1 2 3 4 5 6 7 8 9 10
M
e
m
o
ry
 C
o
n
su
m
p
ti
o
n
 (
M
B
)
Cases
CRS
CRABS E=1
CRABS E=0
Skyline
117 MB
2796 MB
2435MB
  
   
 
CRABS(E=1) spent 53837 seconds on sparse matrix operations.  There is an 11.7 time difference 
between these two formats. 
Figure 8 shows the fraction of time spent on sparse matrices relative to the complete EFG analysis 
for analyses listed in Table 4.  Again, we see size invariance in these analyses, meaning the fraction of 
time spent on sparse matrix operations is almost constant.  In these analyses, sparse matrix related 
operations account for 75% to 98% of the total analysis time.  This is much more than ones observed 
in 2D analyses. 
 
Figure 7. Total computing time spent on sparse matrix in 3D cases. 
Figure 8. Fraction of time spent on sparse matrix computation in 3D cases. 
4.3 Memory consumption 
Figure 9 shows memory consumption for sparse matrices. From the figure, CRABS(E=1) uses the 
least amount of memory, while CRABS(E=0) uses the most.  The memory consumption, same as in 
2D cases, grows with increasing DOFs.  In case 9, the difference on memory consumption between 
CRABS (E=0) and CRABS(E=1) is about a factor of 3.18.  This difference is much less than in 2D 
analyses. 
5 Conclusive summary 
Significant impacts of sparse matrix formats on EFG analyses are shown in this study.  These impacts 
include performance and memory consumption.  Results show the fraction of time spent on sparse 
0
10000
20000
30000
40000
50000
60000
0 1 2 3 4 5 6 7 8 9 10
To
ta
l c
o
m
p
u
ti
n
g 
ti
m
e
o
n
 s
p
ar
se
 
m
at
ri
x 
(s
e
c.
)
Case
CRS
CRABS E=1
CRABS E=0
Skyline
 
70.0%
75.0%
80.0%
85.0%
90.0%
95.0%
100.0%
0 1 2 3 4 5 6 7 8 9 10
Fr
ac
ti
o
n
 o
f 
ti
m
e
 s
p
e
n
t 
o
n
 s
p
ar
se
 
m
at
ri
x 
co
m
p
u
ta
ti
o
n
Case
CRS
CRABS E=1
CRABS E=0
Skyline
 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/30
國科會補助計畫
計畫名稱: 應用雲端運算技術於建構隧道檢監測資訊系統之研析
計畫主持人: 謝佑明
計畫編號: 100-2221-E-011-114- 學門領域: 大地工程
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
