視訊修補技術在物件移除與影片重建之研究 
Digital Video Inpainting for Object Removal and Video Restoration 
計畫編號：NSC 95-2221-E-468 -007 - 
執行期間：2006 年 8 月 1 日至 2007 年 7 月 31 日 
計畫主持人：張榮吉 
 
摘    要 
本研究計畫主要利用視訊修補技術，針對視訊中移動物件被擷取後的影片進行重建工作；此研究工作將考慮兩
個類型之實例。在第一類研究實例中，我們在固定式攝影機所捕捉的影片裡，擷取動態的目標物(物件)，並重建物
件擷取後影片之背景(場景)；此實驗是利用動態物件追蹤原理、形態影像學的處理技術與影像修補演算法，有效地
擷取使用者感到興趣的物件或在視訊中移除妨礙視訊畫面之動、靜態物件。第二類的研究類型是將使用者已經設計
好的動態物件，取代目前視訊中的目標物；此機制首先分析物件的特徵，利用以區塊為基礎的移動評估及計算移動
向量等技術，確立目標物運動的路徑。再利用影片中動態物件在空間域(Temporal)與時間域(Spatial)的相互關係，尋
找有用的修補資訊，強化物件移除與取代後，視訊重建的效果。本研究所提出的技術與實驗的結果可以應用在不同
的視訊影片內容，包含視訊的特殊效果、視訊的編修與重建等。 
 
ABSTRACT 
Video inpainting is the technique that can 
automatically restore damaged or partially removed 
image. It is also the unique tool for filling in the 
missing part in a video sequence. Exploration of more 
advanced concepts in video painting, we develop a 
new video algorithm based on temporal continuations 
and exemplar-based image inpainting techniques. This 
proposed algorithm involves the elements of removing 
the moving objects on stationary and non-stationary 
background. Therefore, the related experiments include 
a wide variety of temporal continuations of foreground 
and background. According to the results of our 
experiments, a motion-compensated inpainting 
procedure is successfully developed and it can be 
further extended to implant into the inpainted 
background video. Ultimately, the inpainted video 
backgrounds are visually pleasant with smooth 
transitions.  
Keywords: Video inpainting, Object removal, 
Motion estimation, Image completion, Digital, 
inpainting 
1. INTRODUCTION 
Digital inpainting is an interesting new research 
topic in multimedia computing and image processing 
since 2000. In the literature, the first intention of image 
inpainting was to remove damages portions of an aged 
photo, by completing the area with surrounding or 
global information. The techniques used include the 
analysis and usage of pixel properties in spatial and 
frequency domains. Furthermore, image inpainting 
techniques were used in object removal (or image 
completion) in photos. Several strategic algorithms 
were developed based on confidence values and 
priorities of using pixels. The techniques used in still 
images were then extended to video inpainting, which 
need to consider temporal properties such as motion 
vectors. With a reasonable combination of object 
tracking and image completion, objects in video can be 
removed and possibly replaced. On the other hand, 
aged films contain two types of defects: spikes and 
lone vertical lines. These defects need to be precisely 
detected and removed to restore the original film. In 
addition, based on image completion techniques, 
incompleteness of scanning results of a 3-D scanner 
due to improper location or other reasons of a scanner 
can be solved.  
Unlike image inpainting that user can select a 
target area to be inpainted, the problem in video 
inpainting is to detect damages, such as spikes or lone 
scratch lines. Detection of defects in video frames is a 
must for motion picture restoration. Defects in video 
frames include spikes (usually in a bright or dark 
intensity) [1] and long vertical lines (usually in a dark 
intensity and in a large length) [2-4]. The former is due 
to dust and dirt, which occurs randomly and mostly 
appears in one or two frames. The later is accidentally 
the behavior and characteristics of the moving objects 
differ significantly. The quality of segmentation result 
depends strongly on the background noise, object 
motion, and the contrast between the object and the 
background. While the algorithm, which are based on 
inter-frame change detection, enable automatic 
detection of objects and allow larger non-grid motion 
compared to object tracking methods and object 
boundaries tend to be irregular in some critical image 
areas due to the lack of spatial edge information. 
Instead of trying to obtain more information from the 
moving objects of the video sequence, the focus is on 
the rough background. The long-term behavior of the 
object motion accumulated from the several frames 
instead of relying on frame difference of two 
consecutive frames that causes the final result more 
rough. The block diagram of rough background 
construction is display in Figure 1. The first step is to 
calculate the frame difference mask by thresholding 
frame difference that comes from two consecutive 
frames and ten frame difference masks are generated. 
The information of ten frame difference masks is 
counted in the background buffer. According the frame 
difference masks of past several frames, pixels that are 
not moving for a long time are considered as rough 
background. 
 
 
Figure 1. The block diagram of the rough background 
construction 
 
2.2 Object Region Decision 
We check the difference of mark in the video 
sequence and the difference of image mask of the 
background to decide the object region. Table 1 lists 
the criteria for distinguishing the regions, where "No" 
and "Yes" mean the pixel on frame difference (or 
background subtraction) mask to be the unchanged and 
changed, respectively. Therefore, the initial object 
mask can be obtained, which combines the still region 
mask and the moving region mask. The sample result 
of object region decision shown in Figure 2. 
Table 1. Object region detection rule 
Region type Background 
subtraction mask 
Frame difference 
mask 
Background No No 
Uncovered 
background 
No Yes 
Still region Yes No 
Moving region Yes Yes 
 
(a) (b) 
(c) (d) 
Figure 2. Illustration of object region decision: (a) and 
(b) are the frames from original video sequence. (c) 
demonstrates the frame difference by showing red color 
between two consecutive input frames. (d) presents the 
result of the object region decision. 
 
2.3 Moving-Object Detection 
In our video inpainting algorithm, it is necessary 
to determine whether there is any moving object 
contained in the input image sequence. Thus, an image 
frame will be viewed as to be composed of foreground 
(i.e., moving object) and background as shown in 
Figure 3.  
 Figure 4. The motion vectors estimation tool 
 
The above tool illustrates a very simple 
mechanism on stationary video. The RGB color space 
is used. However, similar technique and other color 
spaces can be used to enhance the results. In fact, for 
non-stationary videos, advanced techniques are 
required (will be investigated with the analysis of 
temporal continuations).   
 
2.4 Video Inpainting on Object Removed 
Our discussion strictly follows the notations 
introduced in [9]. Let I be the original image (or a 
frame in a video) which includes a target area, denoted 
by Ω, to be inpainted and a source area, denoted by Φ, 
where patches are searched and used. Hence, I = Φ  ∪
Ω. A simple region segmentation algorithm based on 
the CIE Lab color space is used to convert I to I’. Let 
pi and pj be pixels and si and sj be segments.  
∀pi ∈ I, ∀pj ∈ I, pi ≠ pj and pi is adjacent to pj,  
SSDCIE Lab (pi , pj) < δc ⇒ make pi , pj in the same 
segment;  
∀si ∈ I, ∀sj ∈ I, si is adjacent to sj,  
pn(si) - pn(sj ) < δpn ⇒ make si , sj in the same segment,  
where pn(s) computes the number of pixels in a 
segment. And, SSDCIE Lab (pi , pj) calculates the SSDs 
using the CIE Lab color space. 
The algorithm for video inpainting is very complicated. 
In general, it can be described as the following: 
Step 1. The target area Ω1 in the first frame is manually 
selected and tracked as Ωt through the entire 
video. Bounding boxes Ωt are computed.  
Step 2. Inpaint Ω1 using the image inpainting algorithm. 
Step 3. For each Ωt, 2 ≦ t ≦ last-frame,  
Step 3-1. Compute the difference, μt = Ωt-1 - Ωt . 
Step 3-2. Compensate and copy patches in μt by 
Δx,y (and χx,y) from frame t-1 to frame t.  
Step 3-3. Inpaint Ωt \μt using the image inpainting 
algorithm 
We use several parameters which are summarized 
below with best values: 
z Color distance for segmentation: δc  = 3 
z Pixel number of groups for color segmentation: δpn 
is between 50 and 100, depends on video 
z Size of patch: |Ψp| = 5¯5 
z Distance to search: r = 15 
z Neighbor distance index for patch template: k=1 
The first two parameters influence the edge map 
Φε. The use of δpn for video games is smaller as 
compared to scenery videos in general. The size of 
patches is set to 5 by 5 for all videos. A patch size 
smaller will make the search less accurate thus the 
confidence term fails to prioritize patch candidates. A 
larger patch size results in a less realistic inpainting 
result. We also found that the distance for searching 
patches does not significantly affect inpainting results. 
Thus, we use r = 15. The index for patch template is 
set to 1. Thus, a patch template is 9 times of its patch in 
size. On the other hand, several interesting 
observations were found due to temporal inpainting. 
Searching for patch templates among different frames 
does not increase the visual quality of image inpainting 
on a particular frame, since we only use short video 
scenes which contain similar background in all frames. 
3. EXPERIMENTS RESULTS 
We use a simple tracking technique to locate 
moving objects in static and non-static background of 
video clips. The objects are removed from frames. The 
temporal and spatial inpainting algorithms we have 
developed for the above two types of restoration are 
used to produce two short sequences in Figures 5-7. In 
the first experiment (Figure 5), we have successfully 
removed the sportsman in the center. Observe that the 
inpainted background is consistent throughout the 
  
  
Figure 7. Video inpainting with non-stationary background. Top row: Five selected frames from the original car 
sequence. Middle row: The correspondent frames obtained by our algorithm, in which the human was removed. 
Bottom row: The human extracted form the original frame. 
 
4. CONCLUSIONS  
Employing the video inpainting technique under a 
full set of camera motions is a challenging task. In this 
paper, we propose a novel algorithm that integrates with 
several inpainting techniques, including background 
replacement, image inpainting and object interpolation for 
video inpainting techniques.  One of our major 
contributions in this study is to develop a motion-
compensated video inpainting procedure based on the 
novel inpainting algorithm and the computation of motion 
stabilizer vectors. We conclude that different temporal 
continuations of foreground and background should be 
treated differently with different inpainting strategies. 
Experimental results prove that our proposed algorithm 
can produce visually pleasant results. 
 
REFERENCES 
[1] Machi, A.; Collura, F.; “Accurate Spatio-Temporal 
Restoration of Compact Single Frame Defects in Aged 
Motion Picture,” The 12th International Conference on 
Image Analysis and Processing, 2003, pp. 454 – 459. 
[2] Joyeux, L.; Buisson, O.; Besserer, B.; Boukir, S. 
(1999): “Detection and removal of line scratches in 
motion picture films,” IEEE Computer Society 
Conference on Computer Vision and Pattern 
Recognition, 1999., vol.1 ,23-25 June 1999 pp. 549-
553 
[3] Boukir, S.; Suter, D., “Application of rigid motion 
geometry to film restoration,” International Conference 
on Pattern Recognition Proceedings 16th ,Vol.1 11-15 
Aug. 2002, pp.360 – 363 
[4] Bruni, V.; Vitulano, D., “A generalized model for 
scratch detection,” IEEE Transactions on Image 
Processing, vol.13 Jan. 2004, pp.44 – 50 
[5] Yamauchi, H.; Haber, J.; Seidel, H.-P."Image 
restoration using multiresolution texture synthesis and 
image inpainting", Computer Graphics International, 
2003, pp.108-113 
[6] Kedar A. Patwardhan, Guillermo Sapiro, and Marcelo 
Bertalmio, “Video Inpainting of Occluding and 
Occluded Objects,” The 2005 IEEE International 
Conference on Image Processing, Genova, 2005, pp. 
II-69-72 
[7] Bartesaghi, A. , Sapiro, G., “Tracking of moving 
objects under severe and total occlusions,” IEEE 
International Conference on Image Processing 
ICIP,Vol.1 11-14 Sept. 2005, pp. I - 301-4 
[8] Zhang, Yunjun; Xiao, Jiangjian; and Shah, Mubarak; 
“Motion Layer Based Object Removal in Videos,” The 
Seventh IEEE Workshops on Application of Computer 
Vision, 2005, pp. 516-521 
[9] A. Criminisi, P. Perez, and K. Toyama, “Object 
Removal by Exemplar-Based Inpainting,” IEEE 
Conference on Computer Vision and Pattern 
Recognition (CVPR), vol. 2, 2003, pp. 721-728 
[10] T.-H. Chen, T.-Y. Chen and Y.-C. Chiou, “An 
Efficient Real-time Video Object Segmentation 
Algorithm Based on Chang Detection and Background 
Updating,” IEEE International Conference on Image 
Processing, Oct. 2006, pp. 1837-1840 
 2
   * Security, Privacy and Trust 
   * Fault-tolerant and Dependable System 
   * Multi-agent System and Applications 
   * Parallel/Distributed Algorithm and Architecture 
   * Distributed Database and Data Mining 
   * Distributed Graphics and VR/AR/MR System 
   * Distributed AI and Soft/Natural Computing 
   * Biological Informatics and Computing 
   * E-Learning, E-Commerce, E-Society, etc. 
   * Grid, Cluster and Internet Computing 
   * Peer-to-Peer (P2P) System 
   * Service-oriented Framework and Middleware 
   * Autonomic Computing and Communication 
   * WWW, Semantic Web and Cyber World 
   * Mobile and Context-aware Computing 
   * Ubiquitous/Pervasive Networks and Computing 
   * Ubiquitous Intelligence and Smart World 
   * Smart Object, Space/Environment and System 
   * Innovative Networking and Applications 
* Social, Ethical & Other Issues of Networked World 
 
此次會議共有 444 篇論文或研究報告等投稿，經過嚴格的審查制度，僅有 200 多篇文章
被錄取為口頭報告，收錄於論文集中。主會議議程與 15 個專題討論議程，分散在三天的各時
段中進行研討（上、下午各兩個時段），有來自世界各地 30 個國家的學者出席會議的學者、
專家與學生近 400 人參與，會議地點在多倫多市郊的尼加拉瓜瀑布旁的 Sheraton Fallsview 飯
店，風景優美，氣候宜人。為期三天的會議，與會學者們在會議進行中廣泛的討論與意見交
換，會議的進行相當熱絡與順利。 
本次的研討會議主要是討論有關網路、多媒體等技術的最新發展與應用，會議中特別邀
請多位在先進網路技術等領域具有相當知名的國際研究學者進行專題演講，三天來共有 3 個
場次的專題演說，分別為： 
1. Public Computing—Challenges and Solutions. Yi Pan, Georgia State University, USA 
2. Filling the Generation Gap—Convergence in Wireless NetworkingVictor C.M. Leung, 
University of British Columbia, Canada 
3. Designing the Future Internet. Arjan Durresi, Louisiana State University, USA 
