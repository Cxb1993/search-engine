 1
行政院國家科學委員會專題研究計畫成果報告 
空間-時間視覺注意模型及其應用(3/3) 
計畫編號：NSC 96-2221-E-194-033-MY3 
執行期限：98 年 8 月 1 日至 99 年 7 月 31 日 
主持人：柳金章   執行機構及單位名稱: 國立中正大學資訊工程學系 
計畫參與人員：陳軒盈，鄭茗徽，胡維哲，簡永元 
 
一、中文摘要 
 
對於許多研究領域，諸如多媒體處理、
影像/視訊內容檢索，一個有效且有意義的影
像/視訊表示法是非常重要的事。表示法的有
效性通常取決於它跟人類認知的接近程度及
外加信號的反應。人類認知通常會對影像中
的重要物體及視訊中的有趣動作優先注意。
視覺注意分析主要是藉著對目標影像/視訊自
動產生其 saliency maps 以模擬人類視覺系
統。因此視覺注意主要是從影像中檢測出觀
察者有興趣的區域，並且從視訊中找到觀察
者有興趣的運動。 
在本研究計畫第三年中，我們提出了使
用空間-時間特徵的視覺注意區域偵測系統。
我們採用基於紋理的顯著物件偵測方法，利
用最小可察覺差異模型計算出動態臨界值，
並產生出空間顯著圖。另外，對移動向量做
反向投影累加以產生時間顯著圖。考慮移動
對比和空間顯著遞減模型，我們提出了一個
適應融合方式，將空間與時間顯著圖結合成
空間-時間顯著圖。最後，我們採用一種動態
視覺注意區域決定策略，以決定視覺注意區
域。依視覺效果及讓實驗者評分的結果，本
研究計畫所提的方法比其他方法有較好的效
能。 
 
關鍵詞：視覺注意區域決定、空間顯著圖、
時間顯著圖、適應融合方式。 
 
Abstract 
 
A meaningful and effective image/video 
representation is very important in various 
research fields, such as multimedia processing 
and content-based image/video retrieval. The 
effectiveness of the representation is determined 
by how well it fits to human perception and 
reaction to external visual signals. Human 
perception tends to firstly picks attended regions 
which correspond to either prominent objects in 
an image or interesting actions in a video 
sequence. Visual attention analysis simulates 
the human visual system behavior by 
automatically producing saliency maps of the 
target image or video sequence. Visual attention 
deals with detecting the regions of interest (ROI) 
in an image and interesting actions in a video 
sequence that are the most attractive to viewers. 
In this study, a visual attention detection 
system using spatiotemporal features is 
proposed.  The spatial features include 
intensity and color.  A texture based salient 
object detecting scheme is employed to generate 
the spatial saliency map, in which the just 
noticeable difference model is applied to 
determine an adaptive threshold.  On the other 
hand, the motion vector field is backward 
accumulated to obtain the temporal saliency 
map.  Both spatial and temporal saliency maps 
are combined to construct spatiotemporal 
saliency map using an adaptive fusion function 
considering the spatial saliency leaking mode 
and motion contrast.  Additionally, a dynamic 
visual attention region determination scheme is 
presented to determine visual attention regions.  
Based on the experimental results obtained in 
this study, the proposed system has better 
performance than the two comparison systems. 
 
Keywords: Visual attention region 
determination, spatial saliency map, temporal 
saliency map, adaptive fusion function. 
 
二、緣由與目的 
 
由於影像/視訊包含的資訊量非常龐雜，
使得電腦系統無法同時完成所有訊號處理所
 3
是 center-on 類型為例，中央部份取解析度較
高的金字塔影像，周圍部分取解析度較低的
金字塔影像，如此即可用計算複雜度較低的
方式來達到中央周圍比對機制，經過
normalization 及 combination 兩個步驟後，再
利用 bottom-up 的方式將之合成為 spatial 
saliency map，最後利用 winner-take-all (WTA) 
competition選出最顯著的影像位置為 attended 
points。Itti et al. [15]的研究建立了一個從
computational attention model 到 biological 
theories 的架構。然而，這類架構中的高運算
複雜度問題，需要一個大量平行處理的演算
法來獲得快速的回應。 
Lemeur et al. [16]提出了一種 coherent 
approach for visual attention modeling，Fig. 4
顯示此方法的架構圖。在 coherent approach for 
visual attention modeling 中，第一步先將原影
像由 RGB 色彩模型轉為 Krauskopf’s 色彩模
型 ACr1Cr2，其中 A 代表亮度，Cr1 與 Cr2
分別代表紅綠與藍黃的對比元素。接著，各
色彩元素分別進入 visibility part:經過 cortex 
transform 後 ， 就 完 成 了 perceptual 
decomposition (如 Fig. 5，亮度 A 會產生 4 個
channels，也就是如 Fig. 5(a)所示有四個同心
圓，共 17 個 psychovisual sub-band images，彩
度 Cr1Cr2 各會產生 2 個 channels，也就是如
Fig. 5(b) 所示有兩個同心圓，共 5 個
psychovisual sub-band images；若以亮度 A 做
cortex transform 為例，轉換後之結果分別如
Figs. 6 及 7 所示)，然後計算每個 sub-band 
image 的 contrast sensitivity function (CSF)作
為權重值， 利用 intra masking 將每個元素之
sub-band images 做加權計算，即可完成
visibility part。第二步，進入 perception part:
首先，將 channel 2 的彩度特徵值加強(合併)
到每一個亮度的 sub-band images (共 17 張)
後，接著利用 2D difference-of-Gaussians (DoG)
找出加強(合併)後的特徵圖，那一些區域是
屬於 classical receptive field (CRF)，也就是影
像中人眼會重視的地方。第三步是 perceptual 
grouping part: 對於有些落於 CRF 之外，但很
靠近 CRF 的特徵值，若其特徵與 CRF 內之特
徵很近似(例如：成一直線)，則將這一類特徵
值給予保留。最後，將 sub-band images 合併，
便可將 spatial saliency map 完成。 
由於目前人腦對外界影像的分析與處理
的機制還未完全清楚，因此，演算法完全複
製 biological structure 來分析視訊內容的作法
並不太精確。所以本研究計畫利用 human 
attention model 相關理論來做一些新的演算法
設計。期望這些新的演算法也可以使用於多
媒體應用方面，諸如 image content analysis、
video skimming、video summarization。 
運用human attention將影像中對於人眼真
正有意義的區域找出來，並可藉由人類視覺
會忽略影像中某些背景影像的特性來降低計
算量，一些已知 attention model 的相關研究以
及應用，例如 scene understanding [17-19]、
object detection [20,21] 、 pattern clustering 
[22,23] 、 neural networks [24-26] 、 image 
segmentation [27]、data compression [28] 、
feature extraction [29,30] 、multimedia system 
[31] 、quantitative biology [32]。 
在本研究計畫第三年中，我們提出了使用
空間-時間特徵的視覺注意區域偵測系統。我
們採用基於紋理的顯著物件偵測方法，利用
最小可察覺差異模型計算出動態臨界值，並
產生出空間顯著圖。另外，對移動向量做反
向投影累加以產生時間顯著圖。考慮移動對
比和空間顯著遞減模型，我們提出了一個適
應融合方式，將空間與時間顯著圖結合成空
間-時間顯著圖。最後，我們採用一種動態視
覺注意區域決定策略，以決定視覺注意區域。 
 
三、研究方法與成果 
 
本 研 究 計 畫 第 三 年 我 們 使 用
saliency-based visual attention model [15]的架
構來實現 computational attention model，為了
實作出較高品質的 saliency map，我們探討如
何有效改善 saliency-based visual attention 
model 的效能，以便能利用 saliency map 作為
重要特徵，來進行不同領域的應用。 
在 saliency-based visual attention model 中
[15]，先計算出 intensity、color、orientation
三方面的 pyramid 特徵資訊。首先，intensity (I)
與 color (r、g、b、y)影像分別由原影像中每
個像素點(pixel)的三原色，紅色(R)、綠色(G)、
與藍色(B)所共同運算得來，其定義的公式如
下： 
 ,3/)( BGRI ++=  (1)
 5
在 Itti et al. [15] 等 人 所 提 出 的
saliency-based visual attention model 中，每一
張  video frame 被分解為 intensity contrast 
image 及 color contrast image。令 r, g, 及 b 分
別代表原 video frame 的 red, green, 及 blue 
channels。intensity contrast 影像定義如下 
 ( ) .3bgrI I ++=  (11) 
而 color contrast image IRG 及 IBY 分別代表 R 
與 G 及 B 與 Y 的 differences，其定義如下 
 ( ) ,2bgrR +−=  (12) 
 ( ) ,2brgG +−=  (13) 
 ( ) ,2grbB +−=  (14) 
 ( ) ,22 bgrgrY −−++=  (15) 
 ,GRI RG −=  (16) 
 .YBI BY −=  (17) 
為了從每一張 contrast image 中獲取更多
的 morphological information，每一張 contrast 
image 利用 multiscaled down-sampling with a 
sampling factor of d來得到multiscaled contrast 
image Id。如 Fig. 9 所示，原 contrast image I of 
size HW × 產生了三張  multiscaled contract 
images: ,0I  ,
1I  及
2I ，其中 WW =0 及
HH =0 。這些 multiscaled contrast image 將成
為接下來介紹的 texture-based model 的 input 
image。 
在 Chen 及 Leou 所提出的 visual attention 
texture model [36]中，在 contrast image Id (大
小為 dd HW × )上的每一個 block jiB , (大小
為 hw× )將被轉換成為 modified standard 
deviation representation，換句話說， 
( ) ⎩⎨
⎧ >−=′
otherwise,0,
, if,
, ,,,,,
jijijiji
ji
THTH
yxB
σσ
(18)
其中 block 的標準差 ji,σ 及平均值 ji,μ 分別定
義為 
( ),])),(([ 1
0
1
0
2
,,, hwyxB
w
x
h
y
jijiji ×−= ∑∑−
=
−
=
μσ (19)
,)(]),([
1
0
1
0
,, hwyxB
w
x
h
y
jiji ×= ∑∑−
=
−
=
μ  (20)
(x,y)是 block 中的 spatial coordinates，(i,j)是 
block index，及 jiTH , 為 saliency threshold 其
由“just noticeable difference” (JND)所決定。
JND 代 表 the ability to discriminate the 
luminance change in human vision, i.e., 
⎪⎩
⎪⎨⎧ +−⋅
≤+−⋅=
otherwise,,8)127(
,127 if,8)1271(
,
,,0
,
ji
jiji
ji
μT
TH μγ
μ
(21) 
其中 0T 及γ  憑經驗設定為 17 及 3/128 [37] 。 
Standard deviation representation jiB ,′  接著正
規化為 
 
( ) ( ) ,255,, ×′=
Max
yxByxOd  (22) 
其中 Max 是所有 ( )yxB ,′ 值中的最大值，而 Od 
為 multiscaled object map of Id。最後，每一個 
multiscaled object map Od 都 up-sampled 為 
original size 再 linearly combined 成為 object 
combination map O, i.e., 
 
,)ˆ(
1
0
∑−
=
= s
d
d sOO  (23) 
其中 dOˆ  是 up-sampled object map，而 s 為 
number of the multiscaled object maps。 
  依照 texture-based object model 的架構，三
張 contrast images, ,II   ,RGI  及 BYI 將分別
產生三張 object saliency maps, ,IO  ,RGO 及
BYO 。 接著，使用 hierarchical combination 
scheme 來做整合，亦即 RGO 及 BYO 整合成為
color object saliency map CO , 然後 CO 及
IO 再整合成 spatial saliency map .SSM  Fig. 
10 顯示一個 spatial saliency model 的例子。 
 
2. Temporal visual attention model 
在 temporal visual attention region 
detection 中， video sequence 裡的 object 
motions 通常能夠吸引 viewers’ attention，所以
是非常重要的特徵值。要表現物體運動，直
覺的方式即利用 motion vector。Motion vector
可由 motion estimation 或是直接從 H.264 
compressed video bitstream 取得。我們使用第
二種方式，因為 H.264 允許 7 種不同大小的
MB 作 interframe motion estimation，故所擷取
出的 motion vector field (註：將每張 frame 中
以 MB 為單位，記錄每個 MB 的 motion vector
的集合，稱為 motion vector field)可以均勻地
重新採樣。例如，一個 16×16 大小的 MB 可
以被切分成 16 個繼承原 motion vector 值的小
MB 。 此 外 ， 還 需 要 再 經 過 temporal 
normalization，換言之，在 current frame Ic 中
的 block Bi，其 normalized motion vector 
 7
注意的是，小船所引起的水波成功地被去
除，且 saliency map 上兩個 salient objects 亦成
功地被捕捉到。 
 
3. Adaptive fusion function 
為了產生最後的 spatiotemporal saliency 
map，上述的 spatial 及 temporal saliency maps
必須適當地 fusing 以符合人視覺系統的視覺
特性。根據心理學的研究可以得知人類視覺
對於會移動的物體會給予較多的注意，即
temporal saliency 的 重 要 性 大 於 spatial 
saliency。然而在運動不顯著的靜態影片中，
temporal saliency 的影響會變得不明顯，此時
需要將 spatial saliency 的重要性適當地提升。
此外，影片中的某塊區域若總是靜止不動，
人類視覺終將漸漸失去對其的注意力。故該
區域的 spatial saliency 需要隨著時間逐漸降
低。 
Frame It 上 block Bi 的 spatiotemporal 
saliency 定義如下， 
),()()( it
T
tit
S
tit BTSMBSSMBSM ⋅+⋅= ωω (34) 
其中 Stω 和 Ttω 分別為 spatial 和 temporal 
saliency maps 的加權係數。在此 Stω 和 Ttω 能夠
根據 motion contrast 和一塊區域持續靜止不
動的時間而動態地調整， 
 ),()1()( itti
S
t BslB ⋅−= δω  (35) 
 ( ),)(1)()( itittiTt BslBslB −+⋅= δω  (36) 
以 tδ 代表 motion contrast 並定義如下， 
⎪⎪
⎪
⎩
⎪⎪
⎪
⎨
⎧
+×⋅
>+×⋅
<+×⋅
=
otherwise,,
,8.0 if,8.0
,2.0if,2.0
βα
βα
βα
δ
HW
sigdiff
HW
sigdiff
HW
sigdiff
t
 
(37) 
其中α和 β 根據經驗法則分別定為 20/3 和
2/15，而 sigdiff 代表在連續兩張 video frames
中，發生明顯改變的 block 總數。 tδ 需介於 0.2
和 0.8 之間，以確保 spatial 及 temporal saliency
同時會被考慮。為了模擬 spatial saliency 
leaking model，block Bi的 spatial saliency value
在經過 1 秒之後並沒有發生明顯變化的情況
下， )( it Bsl 會被逐漸降低，換句話說， 
 ⎩
⎨⎧ ≤×= .otherwise,1.0
,1if,)10(1
)(
ll
Bsl it
  (38)
最後利用 Eq. (34)產生 spatiotemporal saliency 
map。Fig. 13 顯示一個 spatiotemporal saliency 
map 的例子。 
 
4. Visual attention region detection region 
在本計畫中，我們採用 visual attention 
region determination scheme 來決定 “actual” 
視覺注意區域。在 modified saliency map SM
中，百分之κ 的像素將被選取(本計劃中，
κ =10)。這些被選取的像素將被 labeled，並
使 用 8-connected neighborhood region 
identification algorithm [39]。若 Os 為一個
labeled 像素集合，則 Ros 為一個包覆 Os 的最
小 矩 形 ， Ros 在 visual attention region 
determination scheme 中會反覆擴大來形成最
後的“actual” 視覺注意區域。如 Fig. 14 所
示，search window size與 expanding pixel k和
Ros 的長或寬有關，R 是 search window 中
nonzero saliency value 的數目之於 search 
window size的比例。Ros是為依序單方向擴大
的方式來找尋視覺注意區域，擴大的順序為
上、右、下、左。換句話說，Ros 在某一方向
上的擴大將會持續到其 R 小於 predefined 
threshold 後，換另一個方向擴大 Ros。 
visual attention region determination 
scheme 的詳細步驟如下， 
Step 1: For a seed region Ros of size '' hw× , 
determine the expanding size k of the 
search window (here k=5). 
Step 2: The seed region Ros is expanded in the 
“up” direction for the search window 
(sw), if the ratio R between the total 
number of nonzero saliency value in 
the search window Num(sw) and the 
search window size S(sw) is larger 
than or equal to the predefined 
threshold Tr. The above expanding 
process in the “up” direction is 
repeated until R< Tr. 
Step 3: The expanded seed region obtained in 
step 2 is expanded in the “right” 
direction using the similar expanding 
process and the same threshold Tr. 
Step 4: The expanded seed region obtained in 
step 3 is expanded in the “down” 
direction using the similar expanding 
process and the same threshold Tr. 
Step 5: The expanded seed region obtained in 
step 4 is expanded in the “left” 
 9
[13] E. Todt and C. Torras, “Detection of natural 
landmarks through multiscale opponent 
features,” in Proc. of IEEE Int. Conf. on 
Pattern Recognition, Sep. 2000, pp. 976-979. 
[14] C. Peters and C. O’Sullivan, “Bottom-up 
visual attention for virtual human animation,” 
in Proc. IEEE Int. Conf. Computer Animation 
and Social Agents, May. 2003, pp. 111-117. 
[15] L. Itti, C. Koch, and E. Niebur, “A model of 
saliency-based visual attention for rapid scene 
analysis,” IEEE Trans. on Pattern Anal. Mach. 
Intell., vol. 20, no. 11, pp. 1254-1259, Nov. 
1998. 
[16] O. Lemeur, P. Lecallet, D. Barba, D. Thoreau, 
and E. Francois, “From low level perception 
to high level perception, a coherent approach 
for visual attention modeling,” SPIE proc. ser., 
vol. 5292, Jan. 2004, pp. 284-295. 
[17] K. Rapantzikos, Y. Avrithis, and S. Kollias, 
“Handling uncertainty in video analysis with 
spatiotemporal visual attention,” in Proc. of 
IEEE Int. Conf. on Fuzzy Systems, May. 2005, 
pp. 213-217. 
[18] T. Kohonen, “A computational model of 
visual attention,” in Proc. of IEEE Int. Conf. 
on Neural Networks, Jul. 2003, pp. 
3238-3243. 
[19] O. LeMeur, D. Thoreau, P. LeCallet, and D. 
Barba, “A spatio-temporal model of the 
selective human visual attention,” in Proc. of 
IEEE Int. Conf. on Image Processing, Sep. 
2005, pp. 1188-1191. 
[20] A. Oliva, A. Torralba, M. S. Castelhano, and J. 
M. Henderson, “Top-down control of visual 
attention in object detection,” in Proc. of 
IEEE Int. Conf. on Image Processing, Sep. 
2003, pp. 253-256. 
[21] J. Han, K. N. Ngan, M. Li, and H. J. Zhang, 
“Unsupervised extraction of visual attention 
objects in co-lor images,” IEEE Trans. on 
Circuits and Systems for Video Technology, 
vol. 16, no. 1, pp. 141-145, Jan. 2006. 
[22] A. Nguyen, V. Chandran, and S. Sridharan, 
“Visual attention based ROI maps from gaze 
tracking data,” in Proc. of IEEE Int. Conf. on 
Image Processing, Oct. 2004, pp. 3459-3498. 
[23] O. LeMeur, P. LeCallet, D. Barba, and D. 
Thoreau, “A coherent computational approach 
to model bottom-up visual attention,” IEEE 
Trans. on Pattern Analysis and Machine 
Intelligence, vol. 28, no. 5, pp. 802-817, May. 
2006. 
[24] R. I. Chang and P. Y. Hsiao, “Unsupervised 
query-based learning of neural networks using 
selective-attention and self-regulation,” IEEE 
Trans. on Neural Networks, vol. 8, no. 2, pp. 
205-217, Mar. 1997. 
[25] C. Koch and S. Ullman, “Shifts in selective 
visual attention: toward the underlying neural 
circuitry,” Human Neurobiol., vol. 4, pp. 
219-227, 1985. 
[26] S. Ahmad, “VISIT: a neural model of covert 
attention,” in Advances in Neural Information 
Processing Systems. 1991, vol. 4, pp. 
420-427. 
[27] M. Z. Aziz, M. S. Shafik, B. Mertsching, and 
A. Munir, “Color segmentation for visual 
attention of mobile robots,” in Proc. of the 
IEEE Symposium on Emerging Technologies, 
Sep. 2005, pp. 115-120. 
[28] L. Itti, “Automatic foveation for video 
compression using a neurobiological model of 
visual attention,” IEEE Trans. on Image 
Processing, vol. 13, no. 10, pp. 1304-1318, 
Oct. 2004. 
[29] Z. Lu, W. Lin, X. Yang, E. Ong, and S. Yao, 
“Modeling visual attention's modulatory 
aftereffects on visual sensitivity and quality 
evaluation,” IEEE Trans. on Image 
Processing, vol. 14, no. 11, pp. 1928-1942, 
Nov. 2005. 
[30] A. Treisman, “Perception of features and 
objects,” in Visual Attention. New York: 
Oxford Univ. Press, 1998. 
[31] R. Stiefelhagen, J. Yang, and A. Waibel, 
“Modeling focus of attention for meeting 
indexing based on multiple cues,” IEEE Trans. 
on Neural Networks, vol. 13, no. 4, pp. 
928-938, Jul. 2002. 
[32] F. Crik and C. Koch, “Some reflections on 
visual awareness,” in Proc. Cold Spring 
Harbor Symposia on Quantitative Biology, 
vol. LV. Cold Spring Harbor , NY, 1990, pp. 
953-962. 
[33] S. G. Sun, D. M. Kwak, W. B. Jang, and D. J. 
Kim, “Small target detection using 
center-surround difference with locally 
adaptive threshold,” in Proc. of the 
International Symposium on Image and Signal 
Processing and Analysis, Sept. 2005, 
pp.402-407. 
[34] H. Greenspan, S. Belongie, R. Goodman, P. 
Perona, S. Rakshit, and C.H. Anderson, 
“Overcomplete steerable pyramid filters and 
rotation invariance,” in Proc. of IEEE 
Computer Vision and Pattern Recognition, 
June 1994, pp. 222-228. 
[35] M. W. Cannon and S. C. Fullenkamp, “A 
model for inhibitory lateral interaction effects 
in perceived contrast,” Vision Res., vol. 36, no. 
8, pp. 1,115–1,125, Apr. 1996. 
[36] H. Y. Chen and J. J. Leou, “A new visual 
 11
 
Fig. 3. Itti 所提出之 saliency-based visual attention model. 
 
 
Fig. 4. A coherent approach for visual attention modeling. 
 13
 
Fig. 7. Cortex transform (channel = 4). 
 
H.264 compressed 
video frame
Motion Vector 
normalization and 
accumulation
Global motion 
compensation
Intensity 
object map
Color   
object map
Video frame Motion 
vector field
Multiscaled 
intensity 
object map
Multiscaled 
color     
object map
Spatial 
saliency map
Spatiotemporal 
saliency map
Temporal 
saliency map
Temporal visual 
attention model
Spatial visual 
attention model
Adaptive
fusion
Visual attention 
region detection
 
Fig. 8. The framework of the proposed approach. 
 
 15
 
    
(a)                 (b)                  (c)                  (d) 
Fig. 13. An illustrated example of the proposed spatiotemporal saliency model: (a) the original 
frame It; (b) the spatial saliency map SSMt; (c) the temporal saliency map TSMt; (d) the 
spatiotemporal saliency map SMt. 
 
    
  (a)                     (b) 
Fig. 14. The expanding process of a seed region Ros (a) and the final visual attention region 
(b). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 17
 
 
 
 
 
 
(a) (b) (c) (d) 
Fig. 16. Experimental results of the video sequence “Coastguard” (frames 1, 46, 79, 127, and 
190).  (a) The original video frames; (b)–(d) the visual attention region detection results by Ma et 
al.’s system [41], Liu et al.’s system [38], and the proposed approach, respectively. 
 
 
 
 
 
 
 
無研發成果推廣資料 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次
 
其他成果 
(無法以量化表達之
成果如辦理學術活
動、獲得獎項、重
要國際合作、研究
成果國際影響力及
其他協助產業技術
發展之具體效益事
項等，請以文字敘
述填列。) 
無. 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
