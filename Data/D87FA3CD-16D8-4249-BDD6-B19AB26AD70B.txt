foundation of behavioral science and cognitive 
psychology from data stream and proposes a new data 
structure, PI-tree (Pattern’s Interestingness tree) 
for discovering interestingness frequent patterns. 
英文關鍵詞： data mining, sequential association rule mining, 
sensitive itemsets, interestingness-oriented 
knowledge discovery 
 
1 
 
中文摘要 
本計畫預期達成的目標為持續發
展智慧型居家照護服務系統之各個不
同學習雛型，在計畫中，我們延續第
二年的成果，更深入地討論了探勘序
列關聯規則的問題。不同於先前關於
序列式樣探勘的研究，我們的方法是
找出序列關聯規則，這種規則同樣可
以應用在數位家庭中，探勘家庭成員
在一定順序的行為後產生的頻繁式
樣，可以對家庭成員的行為做更準確
的預測。接著，針對安全性的考量，
我們探討在建立家庭成員行為資料庫
時，所面臨的隱私問題，並進一步地
研究隱藏敏感式樣對整個資料庫的影
響。最後，由於在現實生活中探勘出
來的資料的數量往往會超過使用者所
能吸收的，因此，我們提出了知識展
現的問題，此部分的研究著重於在資
料串流的環境中，利用行為科學和認
知心理學的基礎，提出了六個知識分
類原則，主觀地辨別使用者感興趣的
式樣，並且提出一個新的資料結構
PI-tree 來探勘興趣導向關聯規則。 
關鍵字 
資料探勘、序列關聯規則探勘、敏感
項目集、興趣導向知識發現。 
Abstract 
In the third year, we continually 
develop different learning prototypes of 
smart home service system. The 
problem of mining sequential 
association rules is re-studied in the 
project. The concept of sequential 
association rule is proposed and two 
efficient algorithms, GSAR and PSAR, 
are proposed to solve this problem. The 
sequential association rule represents 
that a set of items usually occur after a 
specific order sequence. Moreover, due 
to privacy or security concerns, some 
sensitive itemsets have to be hidden in 
the database. Therefore, in the second 
study, the major categories of sensitive 
knowledge protecting methodologies are 
introduced and discussed. Moreover, it 
aimed to secure sensitive information 
contained in patterns extracted during 
association patterns mining. The 
proposed approach successfully hides 
sensitive itemsets whilst minimizing the 
impact of the sanitization process on 
nonsensitive itemsets. Finally, due to the 
number of the mined association 
patterns often exceeds the capacity of 
human’s mind in the real-world data. 
Therefore, the third work focuses on 
subjective distinguishing interesting 
knowledge by using six principles which 
based on the foundation of behavioral 
science and cognitive psychology from 
data stream and proposes a new data 
structure,  PI-tree (Pattern’s 
Interestingness tree), for discovering 
interestingness frequent patterns. 
Keywords 
data mining, sequential association rule 
mining, sensitive itemsets, 
interestingness-oriented knowledge 
discovery.  
3 
 
Algorithm GSAR 
Input: A sequence DB S, min_sup and min_conf. 
Output: The set of sequential association rules SAR 
 
Method: 
1. generate the set of frequent 1-items, L1 
2. let Ck=  // Ck is the set of candidate k-sequences 
3. let CSAR =  // CSAR is the set of candidate SAR 
4. let Lk=  // Lk is the set of frequent k-sequences 
5. generate C2 from L1 
6. for each 2c C  do 
7.   if (sup(c)≥min_sup && sup(c)≥min_conf) 
8.     L2 = L2   {c} 
9.   end if 
10. end for 
11. k=3 
12. generate Ck from Lk-1  
13. generate CSAR from Lk-1  
14. while ( Ck is not empty && CSAR is not empty) do 
15.   for each kc C  do 
16.     if (sup(c)≥min_sup && sup(c)≥min_conf) 
17.       Lk = Lk   {c} 
18.     end if 
19.   end for 
20.   for each SARc C  do 
21.     if (sup(c)≥min_sup && sup(c)≥min_conf) 
22.       SAR= SAR   {c} 
23.     end if 
24.   end for 
25.   k++ 
26. end while 
27. for each sequential association rule X in SAR do 
28.   for each sequential pattern Y in Lk do 
29.     if (X=Y) 
30.       delete X 
31.     end if 
32.     else 
33.       return X 
34.     end else 
35.   end for 
36. end for  
5 
 
7. 隱私級別(Privacy level)： 針對某
些惡意攻擊，有那些已經被隱藏
的敏感知識可能仍會被洩漏的不
確信程度 
接著我們將之前研究的敏感知識隱藏
方法整理成四類並且比較分析各個方
法的評估效能： 
1. Heuristic Based 方法 
2. 邊界修正(Border based)的方法 
3. CSP(Constraint-Satisfaction 
Problem)方法 
4. 資 料 庫 重 建 (Database 
Reconstruction)方法 
這 項 研 究 的 部 分 成 果 ， 將 發 表
於 WIREs Data Mining and Knowledge 
Discovery 期刊中，詳細內容請參考[2]。 
接著，在這個議題中，我們根據 CSP
方法，提出一個在無解的環境下，有
效能的資訊保護方法，茲簡述於下。
假設在 D 中，頻繁項目集的集合為 F，
敏感項目集的集合為 SI。根據 Apriori
的特性，如果一個敏感項目集 X 要被
隱藏，任何在 F 中，X 的 super itemset
也會被隱藏。因此，非敏感項目集的
集合 F’可以藉由移除每個敏感項目集
和它的 super itemset 而得之。但是這樣
F’的數量是相當龐大的，所以再根據
Apriori 的特性，定義出 F’的正邊界
( ) { |  and , }B F X X F Y F X Y     
。接著，每個在 ( )B F  和 SI 裡的項目
集可以被轉換成相符的不等式來表示
這個項目集在 D’中是頻繁還是不頻
繁。 
在此類的問題中，如果該敏感資料庫
對應之 CSP 是有解的話，則我們可以
找到最佳的轉換方法。然而，在實際
的敏感資料庫中，其對應之 CSP 幾乎
都是無解的，因此，如何放寬 CSP 內
的限制條件，使得得到之結果資料庫
能有最小的邊際效應是ㄧ個很重要的
問題。先前的研究是利用移除在
( )B F  中符合最小支持度的最大項目
集相對應的條件使得 CSP 有解。在計
畫中，我們考慮在 ( )B F  和 SI 裡的項
目集之間的關係，提出一個方法來找
出會造成 CSP 變得無解的不等式的集
合。另外，再提出一個方法來選擇被
犧牲的項目集，使得資訊遺失最小。 
 我們提出的方法主要分成三個部份： 
1. 提出一個簡單的 lemma，用以
找 出 hazardous 項 目 集 。
hazardous 項目集即為可能會
導致 CSP 變成無解的項目結
合。 
2. 對每個 hazardous 項目集去計
算 gaining factor(GF)，GF 的意
義為犧牲此項目整體所能獲
得的最大效益。  
3. 藉由移除有最大 gaining factor
項目集所對應之不等式，重覆
放寬 CSP，直到 CSP 變得可行
為止。 
這項研究的部分成果，我們已於 2011
年 The International Conference on 
Security-enriched Urban Computing and 
Smart Grids 的國際會議中發表。實驗
結果顯示雖然在執行時間上我們會稍
長於現存處理此類問題的演算法，但
是在資訊遺失和修改幅度上，我們的
方法會低於現存的演算法。這代表我
7 
 
得到知識差異的值後(我們以P.PIG表
示，其中 P 代表該新進知識，而 P.PIG
即為該新進知識與既有知識的差異
度)，我們根據 IOKD 的原則 3~5，將
新進知識區分到三個狀態 (unknown, 
static, and evolving)中，如下圖所示。 
 
 
根據我們的定義，我們提出了一個以
FP-tree 和 DS-Tree 為基礎的新資料結
構 PI-tree 來探勘興趣導向的關聯規
則，下表為在 PI-tree 中節點的描述： 
Field Expatiation 
P The pattern, P, which N stands for. 
P.Freq The accumulated frequency of P in 
latest batch. 
P.Age The age for counting how many 
batches P survives. 
P.PIG A value which represents the 
interestingness of P at the moment. 
為了節省節點交換的次數，PI-tree 維
持一個用字母排序的雜湊表做水平的
節點追蹤，並且根據我們提出的量化
方法，有效的更新K 中式樣的累積頻
率和每個式樣的年齡。 
最後，我們提出二個 Lemmas 來幫助提
升我們提出用來探勘興趣導向關聯規
則演算法的效能，這兩個 lemmas 主要
是依據 property of pattern和 age of 
pattern 的特性，與 P.PIG 的值的關
係，探討在那些情況下是不需要計算
一個新進知識與既有知識的差異度，
以減少演算法執行時的計算量，茲將
減少計算量的兩個lemmas與演算法列
於後。 
Lemma 1  ( , )rD P P should be 
calculated only when  
 
2
r rP P P P
 
 
  
     
Lemma 2  ( , )rD P P should be 
calculated only if 
, ( , )s r sP P D P P     
根據實驗結果顯示，利用這兩個
lemmas，將可大量減少知識分類時的
運算量，除此之外，在應用於實際資
料時，我們驗證此方法可以有效地描
述知識的演變。這項研究的部分成
果，我們發表於 2011年的 International 
Conference on Security-enriched Urban 
Computing and Smart Grids 國際會議
中以及即將刊登於 Journal of Internet 
Technology 期刊中，詳細內容請參考
[4][5]。在這項研究中，我們策畫六
個原則來定義有興趣的知識和建構知
識串流，並且提出了二種測量知識成
長的方法和利用二個 lemmas來改善演
算法的效能。也提出了新的資料結構
PI-tree 來探勘興趣導向關聯規則。 
9 
 
Input: the pattern pair P and Pr. 
output: return TRUE if this pair passed the filter 
lemmas; otherwise, return False.  
 
Procedure ShouldContinue (P, Pr) 
{ 
(1)if P equals Pr and P is a new-added node of PI-Tree 
(2)    return FALSE; 
(3)else{ 
(4)    if the pair (P, Pr) passed the Lemma 1 then{ 
(5)        set PIG_Value = α* DP(P, Pr) + β* DA(P, Pr);
(6)        replace P.PIG if PIGValue is less than P.PIG; 
(7)        replace Pr.PIG if PIGValue is less than Pr.PIG;
(8)        if the pair (P,Pr) passed the Lemma 2 then 
(9)            return TRUE; 
(10)      else 
(11)          return FALSE; } 
(12)  else 
(13)      return FALSE; } 
} 
 
呼叫於 CheckThePair 副程式中之副程式:ShouldContinue，利用 lemma1 與 2 的
觀念，用於決定是否需要繼續計算節點與新進知識的差異度。 
Input: a node, N; the result list, L; 
output: null. 
 
Procedure Maturate(N,L) 
{  
(1)let P be the pattern which N stands for;  
(2)if P.freq is less than min_sup then{ // P is infrequent
(3)    prune N and all its children in T;} 
(4)else 
(5)    P.Freq *= δ; 
(6)    P.Age += 1; 
(7)add a copy of P into L; 
(8)reset P.PIG = μ for next batch;} 
} 
 
呼叫於 IOKD 演算法之副程式:Maturate，用於準備現有知識的狀況，並刪除過期
知識，以備下一批新知識進來時使用。 
 
1 
 
Mining Sequential Association Rules Efficiently by Using Prefix Projected 
Databases 
Yi-Chun Chen1, Guanling Lee1 
1 Department of Computer Science and Information Engineering , National Dong Hwa University , 
Hualien, 974 Taiwan, R.O.C. 
{divien, guanlingl}@gmail.com 
Abstract. The mining of sequential patterns has been studied for several years, however, there is little studies 
pay attention to mining of sequential association rules despite such rules also providing valuable knowledge 
about many real applications. The sequential association rule represents the concept that a set of items usually 
occur after a specific order sequence. In this paper, the idea of sequential association rule is introduced and 
two algorithms, GSAR and PSAR  algorithms, are proposed to discover these hidden knowledge. Moreover, 
experiments are performed on both synthetic and real datasets to show the benefit of our approach. 
Keywords: component, sequential association rule, projected database method, data mining 
1   Introduction 
Over the past decade, the technique of data mining is extensively applied to discover useful knowledge in 
various domains. Traditional issues of data mining includes mining association rules [10, 11, 12, 13, 16, 17, 18, 
19, 20, 21, 22, 23], classification, clustering, mining stream, time-series and sequential pattern [1, 2, 3, 4, 5, 6, 7, 
14, 15], etc. Association rules mining has become widely used in both market prediction and customer behavior 
analysis. It was first investigated in [10], and its goal is to discover the relationship between data items in a static 
database of sales transactions. The approaches for finding association rules in the literature can be roughly 
divided into two categories: (1) the apriori-based Apriori algorithm [11] and (2) the tree-based algorithm [12]. In 
[11], Agrawal et al. propose the Apriori algorithm which employs a level-wise search approach and needs to 
generate candidates according an anti-monotonic heuristic to mine out frequent itemsets. That is, it uses k-
itemsets to explore k+1-itemsets iteratively. However, one of the problems with the Apriori algorithm is that it 
needs to scan the database k+1 times, where k is the length of maximum frequent pattern, which generates too 
many candidate patterns. Many approaches have been used to improve the efficiency of the Apriori method, such 
as a hash technique for efficiently generating large itemsets [16], partitioning so as to only scan the database 
twice [17], sampling to randomly extract the data from the original database and utilizing these data to find 
frequent patterns [24], and utilizing simple structures such as matrices and vectors, and minimizing the number 
of candidate sets in the process of generating frequent patterns [18]. 
  Han et al. [12] proposed a compressed technique, named FP-tree, to mine frequent itemsets using a tree-
based structure. This algorithm did not have the problem of scalability and it did not generate candidate patterns 
to find frequent patterns. However, when the database is large, it is sometimes unrealistic to construct the FP-tree 
in main memory. Pei et al. [19][20] proposed a dynamic structure to adjust links dynamically to avoid the 
problems of FP-tree. Since FP-tree did not handle the problem of incremental mining, Leung et al. [21] proposed 
a novel tree structure to capture the transaction database and arrange tree nodes into some sort of canonical order. 
By exploiting ordering properties, this tree structure can be maintained when transactions are inserted or deleted. 
Ghoting et al. [22][23] proposed a cache-conscious FP-tree approach to improve the performance of the FP-tree 
algorithm on modern processors. 
Sequential pattern mining is also an important data mining problem with many applications, such as 
modeling the behavior of customer purchase sequences in the market prediction, DNA sequences analyzing and 
disease diagnosing and treatments prediction. The basic idea of sequential pattern mining was first introduced in 
[2]. Thereafter, many mining sequential pattern methods have been studied extensively. Srikant et al. [7] proposed 
apriori-based GSP algorithm, which generates sequential patterns level by level iteratively. It generates candidate 
k-sequences from frequent (k-1)-sequences in iteration based on the anti-monotone property which means that all 
subsequences of a frequent sequence must be frequent. However, because it could be generated large number of 
candidate sequences and must scan database multiple times to get sequence support, the mining process is costly. 
Subsequently, many approaches have been proposed to improve the efficiency of mining process, such as 
FreeSpan [3], PrefixSpan [4], SPADE [6], and SPAM [1].  
In [3], FreeSpan algorithm which adopted pattern-growth method and divide-and-conquer technique was 
proposed to find sequential patterns efficiently. That is, it constructed corresponding projected database based on 
the current sequential patterns without candidate generation to mine the next length of sequential patterns. 
Example 2.1 Refer to the sequence database listed in Table I. Given min_sup = 0.5. The set of items in the 
sequence database is {a, b, c, d, e, f}. Take sequence <a,c> as an example, ( , )a c   ={1, 2, 3, 4} and 
 . Sequence <a, c> is a sequential pattern. sup( , ) 4a c   min_ sup   4 = 2 
In the past researches on sequential patterns mining, the mining results are usually a set of specific order 
sequences. However, it is worth noting another specific type of sequential patterns. For example, refer to Table I, 
we know that <a,b>, <a, b, c> and <a, b, d> are sequential patterns, but <a, b, c, d> and <a, b, d, c> are not 
frequent sequences in the sequence database. However, the itemset {c, d} usually occurrs after the sequential 
pattern <a, b>. Therefore, in this paper, the concept of sequential association rule ( s A ) is introduced for 
revealing the knowledge of the sequential pattern and the itemsets following it . As mentioned in section 1, obesity 
usually results in hypertriglycemia. Moreover, obesity and hypertriglycemia usually cause a higher incidence of 
complications, such as hypertension, diabetes mellitus and hyperuricemia. Therefore, we say <obesity, 
hypertriglycemia>  {hypertension, diabetes mellitus, hyperuricemia} forms a sequential association rule. Based 
on above discussion, sequential association rule is defined as follows. 
Definition 2.1 (Sequential Association Rule) 
Given a sequential pattern s in S, where , and an itemset , they constitute a sequential 
association rule 
| |  2s  A I
s A ,  if and only if the following conditions hold: 
 
1) Let , ... , ...,s i i i    , it satisfies that { , ... , ..., }i i i A       
2) It satisfies that sup( s A ) ≥ min_sup and conf( s A ) ≥ min_conf. 
3) There does not exist , ...,k ms j  ' mss j   such that in_ sup  and 
'j s
A j
 
   , where ,k mj j I  . 
                                                                                                                                                                       █ 
There are three conditions in the definition of sequential association rules. First condition means that if 
s A  is a sequential association rule, the intersection of the element of the antecedent sequence and the 
consequent itemsets should be empty. And the second condition is used to check whether the sequential 
association rule is strong by using support and confidence thresholds. Moreover, the main purpose of sequential 
association rule is to identify which itemset occurs frequently after a specific order sequence, and the order of the 
itemset is not important. Therefore, the finally condition is used to guarantee that if an itemset A and a sequential 
pattern s can form a sequential association rule, then there must not exist a specific order s’ of the items in A, 
which make ss’ also be a sequential pattern. 
3 
 
 Algorithm GSAR 
Input: A sequence DB S, min_sup and min_conf. 
Output: The set of sequential association rules SAR 
 
Method: 
1. generate the set of frequent 1-items, L1 
2. let Ck=  // Ck is the set of candidate k-sequences 
3. let CSAR =  // CSAR is the set of candidate SAR 
4. let Lk=  // Lk is the set of frequent k-sequences 
5. generate C2 from L1 
6. for each 2c C  do 
7.   if (sup(c)≥min_sup && sup(c)≥min_conf) 
8.     L2 = L2   {c} 
9.   end if 
10. end for 
11. k=3 
12. generate Ck from Lk-1  
13. generate CSAR from Lk-1  
14. while ( Ck is not empty && CSAR is not empty) do 
15.   for each kc C  do 
16.     if (sup(c)≥min_sup && sup(c)≥min_conf) 
17.       Lk = Lk   {c} 
18.     end if 
19.   end for 
20.   for each SARc C  do 
21.     if (sup(c)≥min_sup && sup(c)≥min_conf) 
22.       SAR= SAR   {c} 
23.     end if 
24.   end for 
25.   k++ 
26. end while 
27. for each sequential association rule X in SAR do 
28.   for each sequential pattern Y in Lk do 
29.     if (X=Y) 
30.       delete X 
31.     end if 
32.     else 
33.       return X 
34.     end else 
35.   end for 
36. end for 
Fig. 1. Algorithm GSAR 
5 
 
 Algorithm PSAR 
Input: A sequence DB S, min_sup, min_conf. 
Output: The set SAR of sequential association rules 
Method: 
1. Modify_PrefixSpan(< >, l, S) 
2. for each sequential association rule X in SAR do 
3.   for each sequential pattern Y in Lk do 
4.     if (X=Y) 
5.       delete X 
6.     end if 
7.     else 
8.       return X 
9.     end else 
10.   end for 
11. end for 
Fig. 3.  Algorithm PSAR. 
The subroutine Modify_PrefixSpan is discussed in Fig. 4. According to Definition 2.1, by considering the 
length of sequential pattern, it can be decomposed into two parts (line 2 and line13). We generate all 2-sequential 
patterns in the first part (line2- line 12). Second, for the sequential patterns generated in the first part, by 
constructing projected databases of the sequential patterns, we can mine out sequential patterns and sequential 
association rules level by level, recursively (line 13- line 30). Moreover, the confidence of sequential association 
rule is calculated in this part and used to check whether the rule is strong (line 26). Example 3.2 is used to 
illustrate the main concept of Algorithm PSAR. 
Example 3.2 Following the example above, sequential association rules can be found by using PSAR 
algorithm in the following steps. 
 
1) Scan S once to find all frequent 1-sequential patterns, <a>: 4, <b>: 4, <c>: 4, <d>: 3, <e>: 4, <f>: 2 
2) According to the six prefixes, the corresponding projected databases are constructed and frequent 2-
sequential patterns are generated. For example, sequences in S containing <a> form the a-projected 
database. By scanning a-projected database, the 2-sequential patterns having prefix <a> can be mined, 
<a, b>: 4, <a, c>: 4, <a, d>: 3, <a, e>: 3, <a, f>: 2. 
3) Subsequently, for the sequential patterns generated in the step 2, by constructing projected databases of 
the sequential patterns, we can mine out sequential patterns and sequential association rules level by level, 
recursively. For example, by using 2-sequential patterns having prefix <a, b>-, <a, c>-, <a, d>-, <a, e>-, 
and <a, f>-, five projected database are constructed. Mining <a, b>-projected database returns four 3-
sequential patterns, <a, b, c>: 3, <a, b, d>: 2, <a, b, f>: 2, and <a, b, e>: 2, and two frequent 2-itemsets, 
{c, d}: 2 and {e, f}: 2. Because conf( ,  { ,  }a b c d  )=2/4=0.5  and 
conf( )=0.5 , these two sequential association rules are strong. Therefore, 
the rules  }c d   and ,  }e f  are stored into the set SAR. Similarly, we can find 
sequential patterns and sequential association rules on prefix <b>, <c>, <d>, <e>, and <f>. The 
constructing projected databases and mining process are illustrated in Fig. 5. 
min_ conf
,  { ,  }a b e f 
,  { ,a b
min_ conf
,  {a b 
4) Eventually, similar to step 3 of Example 3.1, <a, b, f, e> and <a, c, f, e> are two sequential patterns. 
Therefore,  and  are not the valid sequential association rules and 
are removed from the results. 
, { ,  }a b e f  ,  { ,  }a c e f 
7 
 
 
---------------------------------------------------------------------------------------------------------- 
 
Fig. 5.  An example of PSAR algorithm
9 
 
 Fig. 6. Execution time with different min_sup for synthetic dataset. 
 The number of sequential association rules generated in different support thresholds is shown in Fig. 7. When 
min_sup is 0.5%, few sequential association rules are reported. This is because that there are many sequential 
patterns as support threshold is low. Therefore, only a small portion of sequential association rules candidates can 
pass condition 3 in Definition 2.1. As support threshold increases from 0.5% to 1.5%, the number of sequential 
association rules increase. This is because the number of sequential patterns and the length of patterns decrease as 
the min_sup increases. As a result, a large portion of sequential association rules candidates can pass condition 3 
in definition 2.1. That is, for a sequential association rule candidate s A , the probability that ss’ where s’ is a 
specific order of items in A contains in another sequential pattern decreases. As the support threshold is between 
1.5% and 3%, the number of sequential association rules decreased. The main reason is that the minimum support 
is high enough, the restriction of sequential association rules become more tightly. Fig 8 shows the distribution of 
sequential association rules with different min_sup. We can see that as min_sup is no less than 1.5%, the length of 
sequential association rules is short. This is because that as support threshold is high enough, the restriction of 
sequential association rules become more tightly, especially long sequences. Moreover, when support threshold is 
low (say 0.5%), a large number of sequential patterns will be generated, including both short and long sequences. 
As a result, according to the restriction of condition 3 of Definition 2.1, many candidate rules are false positive. 
 
Fig. 7. Number of sequential association rules with different min_sup for synthetic dataset. 
11 
 
 Fig. 10. Number of sequential association rules with different average number of transactions per customer for synthetic 
dataset. 
 
Fig. 11. Distribution of sequential association rules with different average number of transactions per customer for synthetic 
dataset. 
 
 
4.2   Efficiency Analysis for Real Dataset 
In the second experiment, Plan dataset [8] is used to show the efficiency and scalability of two approaches. 
In the experiment, the min_sup setting is according to [6]. Fig. 12 shows that algorithm PSAR outperforms 
algorithm GSAR. The main reason for this result is that PSAR algorithm is FP-growth based approach and does 
not require candidate generation. Fig. 13 shows the distribution of sequential association rules of Plan dataset for 
different support thresholds. We can see that it is a symmetric bell shape distribution as support is 60%. This is 
due to Plan dataset is a very dense dataset. Only a small portion of sequential association rules candidates can 
pass condition 3 in Definition 2.1, as the number of short length sequential patterns is large. Therefore, the 
number of short length of sequential association rules is rather small. Moreover, according to the property of 
apriori, the number of long length sequential patterns is less than that of short length sequential patterns. As a 
result, the probability of a long sequential pattern to form a sequential association rule is less than that of a short 
sequential pattern. Therefore, the number of long length sequential association rules is also quite small. Fig. 14 
13 
 
 Fig. 14. Number of sequential association rules for Plan dataset.
 
In summary, the experimental results described above demonstrate that GSAR and PSAR algorithms offer the 
scalability and reports sequential association rules with different support thresholds. Moreover, PSAR algorithm 
offers a notable improvement in efficiency. 
 
 
5   CONCLUSION AND FUTURE WORKS 
In this paper, a new concept of sequential association rules is introduced. Mining sequential association rules 
provide valuable knowledge which cannot get from sequential patterns mining and it can be applied in many 
applications, such as disease analysis and market decision. Moreover, GSAR and PSAR algorithm are proposed 
for mining sequential association rules. The experiments show that the GSAR and PSAR algorithms offer 
execution time of scalability with support threshold and reports sequential association rules. Moreover, PSAR 
algorithm offers a notable improvement in efficiency. 
There remain some problems that are worth studying in the future. First, investigating more efficient algorithms 
to improve the scalability of mining sequential association rules is important. Furthermore, extending to find 
negative sequential association rules will also be worthwhile. 
References 
[1] J. Ayres, J. Gehrke, T. Yiu, and J. Flannick, “Sequential Pattern Mining using a Bitmap Representation”, In Proc. 2002 
ACM Int. Conf. Knowledge Discovery and Data Mining (SIGKDD’02), Edmonton, Alberta, Canada, July 2002, pp. 429-
435. 
[2] R. Agrawal and R. Srikant, “Mining sequential patterns”, In Proc. 1995 Int. Conf. Data Engineering (ICDE’95), Taipei, 
Taiwan, Mar. 1995, pp. 3-14. 
[3] J. Han, J. Pei, B. Mortazavi-Asl, Q. Chen, U. Dayal, and M. C. Hsu, “FreeSpan: Frequent Pattern-Projected Sequential 
Pattern Mining”, In Proc. 2000 Int. Conf. Knowledge Discovery and Data Mining (SIGKDD’00), Boston, Massachusetts, 
USA, Aug. 2000, pp. 355-359. 
15 
 
17 
 
[22] A. Ghoting, G. Buehrer, S. Parthasarathy, D. Kim, A. Nguyen, Y.-K. Chen, and P. Dubey, “Cache-Conscious Frequent 
Pattern Mining on Modern and Emerging Processors,” In the International Journal on Very Large Data Bases (VLDBJ), 
Jan 2007, pp. 77–96. 
[23] L. Liu, E. Li, Y. Zhang, and Z. Tang, “Optimization of Frequent Itemset Mining on Multiple-Core Processor,” In Proc. 
2007 Int. Conf. Very Large Data Bases(VLDB’07), Vienna, Austria, Sep. 2007 pp. 1275-1285. 
[24] H. Toivonen, “Sampling Large Databases for Association Rules”, In Proc. 1996 Int. Conf. Very Large Data 
Bases(VLDB’96), Bombay, India, Sept. 1996, pp. 134-145. 
 
 
 
 
 
 
 
 
 
 
 
 
solve  the problem.  In  this chapter, we will  introduce and discuss  the major categories of sensitive 
knowledge protecting methodologies.  
 
1. Introduction 
The  rapid  growth  of  the  information  technologies  such  as  computer  computation,  networks 
communication,  and  the  Internet  is  accompanied  by  some  challenges.  The  huge  amount  of  data 
would become unmanageable  if  they are not analyzed efficiently, and various  techniques of data 
mining  have  been  presented  to  achieve  this.  Data  mining  techniques  can  be  used  in  various 
applications—including  market  basket  analysis,  business  management,  science  exploration,  and 
medical diagnoses—to  retrieve  valuable  information  from  the huge  amounts of data. Association 
rule  mining,  which  is  an  important  functionality  in  the  research  field  of  data  mining,  was  first 
introduced by Agrawal et al. [1] and extended in [2]. Association rule mining is a kind of method for 
exploring the relationship among data items and a lot of approaches are proposed for solving it [3‐9]. 
Take the example of market‐basket database, with the analysis of association rule, we can find those 
products which are frequently bought together. This rule is written as the following form: “Milk → 
Bread, support = 10%, confidence = 90%”. The support of 10% of the rule means  that 10% of all 
transactions show milk and bread are purchased together. And the confidence means that 90% of 
the customers who purchased milk also bought bread.  
The  techniques  of  association  rule  mining  have  been  widely  used  in  business  applications  to 
understand the behaviour of customers and support decision‐making. Some companies share their 
data in business collaborations in order to increase leverage profitability and market share or reduce 
production  costs. However,  in a  cooperative project,  some of  these  companies may want  certain 
strategic  or  private  data  called  sensitive  knowledge  to  not  be  published  in  the  database.  The 
sensitive  knowledge  can  be  represented  as  a  set  of  association  rules  or  frequent  itemsets  with 
security  implications,  such  as  commercial  secrets.  Therefore,  before  releasing  database,  the 
companies would  like  to  transform  their original databases  into new ones  that hide  the  sensitive 
knowledge according  to  some  specific privacy policies. We use  the example mentioned  in  [10]  to 
motivate the sensitive knowledge hiding problem.  
 Example: Consider a supermarket and two beer suppliers A and B. The database of the supermarket 
is released to the suppliers for exchanging a lower price of goods, and thus, supplier A can mine the 
association rules related to his products for the purpose of sales promotion. However, if supplier A 
finds that most customers who buy diapers also buy B’s beers, he can run a coupon for giving a 10 
percent discount when buying A’s beers together with diapers.  As a result, the amount of sales on 
B’s beers decreases and B can not give a low price to the supermarket as before. Finally, supplier A 
monopolizes the beer market and is unwilling to give a low price to the supermarket any more. From 
this aspect, releasing the database is bad for the supermarket. Therefore, an effective way for hiding 
sensitive association rules before releasing the database is needed for the supermarket. This leads to 
the research of sensitive rules hiding. 
In the context of data mining and particularly association rules, the process of hiding sensitive rules 
is called data sanitization [11][12]. Verykios et al. [13][14] considered the problem of hiding sensitive 
information  to  be  an  important  issue  of  preserving  privacy  when  data  mining.  To  preserve  the 
3. The modification degree, which  is quantified as  the difference between original and  sanitized 
databases, should be minimized. 
Different  from  the  way  that  the  sensitive  knowledge  protecting  approaches  rank  the 
aforementioned goals for having them satisfied, a lot of researches proposed. We will discuss them 
in the following section. 
2.3 Evaluation Metrics  
Because  the  sensitive  knowledge  hiding  approaches  rank  the  goals  in  different  ways,  a  lot  of 
evaluation metrics should be considered when evaluating  the performance of  the algorithms  [15]. 
We list them as follows: 
‐ Execution time: the time need to execute the algorithm. 
‐ Scalability: the efficiency of the algorithm for increasing sizes of the database. 
‐ Hiding failure: the portion of sensitive knowledge that is not hidden after applying the sensitive 
knowledge hiding procedure. 
‐ Information  loss:    can  be  quantified  as  the  number  of  nonsensitive  itemsets  that  become 
infrequent in the sanitization database. 
‐ False rules: can be quantified as the number of ghost rules in the sanitization database. 
‐ Modification  degree:  can  be  quantified  as  the  difference  between  the  original  and  sanitized 
databases. 
‐ Privacy level: estimates the degree of uncertainty, according to which sensitive information, that 
has been hidden, can still be revealed from some malicious attacks. 
3. Sensitive Knowledge Protecting Algorithms 
To hide  the  sensitive  knowledge, we  can  either decrease  the  support of  sensitive  itemset or  the 
confidence  of  sensitive  rule  to  a  value  smaller  than  minimum  support  or  minimum  confidence, 
respectively.  In  other words,  a  set  of  transactions  which  contain  sensitive  knowledge  should  be 
identified  from D and modified to decrease the support or confidence of the sensitive knowledge. 
According to the transaction modification strategies, we categorize the sensitive knowledge hiding 
methodologies  into  four  classes:  “heuristic  based  approaches”,  “border  based  approaches”, 
“constraint‐satisfaction  problem  approaches“  and  “database  reconstruction  approaches”,  and 
discuss the main ideas of them in the following subsections. 
3.1 Heuristic Based Approaches 
The common  idea of hiding sensitive rules  in a database  is as follows: Given a sensitive rule X ‐> Y, 
delete  item  iX∪Y  from  the  transactions  containing  X∪Y  to  decrease  both  the  support  and 
confidence of the rule. Or,  insert  item  iX to the transactions which partially support X but do not 
support  Y, make  the  transactions  support  X  to  increase  the  support  of  X  and  thus  decrease  the 
confidence  of  X‐>Y.  The  approaches  in  this  category  aimed  at  developing  the methodologies  for 
identifying  the set of  transactions and  items  for modification. M. Attallah et al.  [11] proposed  the 
subpatterns of a sensitive pattern with a length of two should be removed to avoid the situation that 
the hiding pattern will be recursively inferred. 
Instead  of  inserting  or  deleting  items  in  the  selected  transactions  to  decrease  the  support  or 
confidence of the sensitive knowledge, Y. Saygin et al. [35‐36] introduced the idea of using unknown 
(“?”)  for  replacing    the  items  which  are  selected  to  be  inserted  or  deleted.  The  goal  of  using 
unknown  instead of “false” value  is to protect the user of the data from  learning “false” rules.   To 
achieve  this,  the  safety margin  is defined  to  show how much below  the minimum  threshold of a 
sensitive  rule  should.  Based  on  the  idea,  an  efficient  approach  was  proposed  in  [37].  However, 
Pontikakis  et  al.  [38]  pointed  out  that  an  adversary  can  discover  the  hidden  sensitive  rules  by 
identifying  those  itemsets  that  contain  unknown  symbols  and  lead  to  rules  with  a  maximum 
confidence that larger than the minimum confidence threshold. 
3.2 Border Based Approaches 
Border based approaches [39‐41] used the notion of borders presented in [42] to track the impact of 
altering  transactions  by  greedy  selecting  those  modifications  whilst  minimizing  the  side  effects. 
These  approaches  focus  on  preserving  the  border  of  nonsensitive  frequent  itemsets  rather  than 
considering all nonsensitive itemsets during the sanitization process. We summarize the approaches 
as  follows:  Let  the  set  of  frequent  itemsets  in  D  be  F  and  the  set  of  sensitive  itemsets  be  SI. 
According  to  the apriori property,  if a sensitive  itemset X  is hidden, any super‐itemset of X  is also 
hidden from F. Therefore, the set of nonsensitive  itemsets, F', can be computed by removing each 
sensitive  itemset  and  its  super‐itemset  from  F.  The  number  of  itemsets  in  F'  is  often  very  large, 
which  makes  it  inefficient  to  consider  them  all.  The  apriori  property  allows  us  to  focus  on  the 
positive border of F', denoted as B+(F'), which consists of all the largest itemsets in F'; formally, 
},''|{)'( YXFYandFXXFB  . 
Again, according to apriori property, if one of the subset of itemset X is infrequent in D, X cannot be 
a  frequent  itemset  in D, either. Therefore,  to hide all  the sensitive  itemsets defined  in SI, we only 
need to focus on hiding the negative border of SI, denoted as B‐(SI), which consists of all the smallest 
itemsets in SI; formally,  
},|{)( XYSIYandSIXXSIB   
Therefore,  the  sanitization  process  is  generalized  to  hide  the  itemsets  in  B‐(SI)  and  protect  the 
border  of  F’,  i.e.,  B+(F').  By  greedy  selecting  the  modification  with  minimal  side  effect  on  B+(F’) 
iteratively,  the sensitive  itemsets are hidden. The degree of vulnerability of a border element B  is 
computed by the following equation: 







sup0,sup
1sup,
sup
1
)(
""
"
"
mSupSupm
mSup
mSup
SupSup
Bw
BB
B
B
BB

 
In the equation, SupB denotes the support of B in D and  is an integer larger than |B+(F’)|. Moreover, 
"
BSup  denotes the support of B  in D”, where D”  is the database during the process of sanitization. 
1  0  0  0 
Table 1： Original database D 
   
3.3 Constraint‐Satisfaction Problem Approaches 
Menon et al. [43] first represented the transactional database as a binary matrix and mapped each 
sensitive  itemset  and  nonsensitive  itemset  into  a  proper  inequality  so  as  to  transform  the 
sanitization  problem  into  a  constraint‐satisfaction  problem  (CSP)  [44].  This  allows  the  sanitized 
database to be obtained by binary  integer programming [45].  In order to ensure the quality of the 
sanitized database, [46] extended [43] by presenting an exact methodology to identify the smallest 
number of candidate  items  for sanitization and minimize  the distance  (i.e.,  the number of variant 
bits) between the original and sanitized databases. Similar to [46], [47] did not decrease the support 
of  the sensitive  itemsets but extended new  transactions  to  the database based on minimizing  the 
effects of the nonsensitive itemsets. The extended transactions were carefully designed to lower the 
importance  of  the  sensitive  itemsets  whilst  minimally  affecting  the  importance  of  nonsensitive 
itemsets in the original database.  
The benefit of this kind of approaches is that the sensitive itemsets can be successfully hidden whilst 
still allowing the nonsensitive itemsets to be mined from the sanitized database and also minimizing 
the difference between the original and sanitized databases when the corresponding CSP is feasible. 
However,  the  approach  may  be  infeasible  for  many  real  datasets.  Therefore,  when  the 
corresponding  CSP  is  infeasible,  [46]  relaxes  the  constraints  by  removing  some  inequalities  that 
correspond to the largest and minimum‐support itemsets.  
Because these approaches play an important role in the research field of sensitive knowledge hiding 
problem, we summarize the approach proposed in [46] to make the readers get a clear view of the 
main idea of these approaches. 
Divanis et al. [46] formulate the sanitization problem as a CSP by using a border revision method [39] 
to identify the revised border of D for hiding sensitive itemsets and removing the inequalities that do 
not affect the solution set. The basic idea of [46] is that an itemset X is frequent in D' if its number of 
occurrences  is  no  smaller  than  msup,  which  can  be  stated  as 
1
'
j
n
ij
i I X
t msup
 
 ,  where 
1,     if 
'
0,     otherwise
j i
ij
I T
t
 
. Equivalently,  it  is  infrequent  if 
1
'
j
n
ij
i I X
t msup
 
 . Therefore, each  itemset 
in  ( ')B F  and SI can be transformed into a corresponding inequality to represent that it should be 
frequent or  infrequent  in D'. For an example, refer  to  the transaction database  in Table 1. Assume 
msup=2, and SI = {AB}. In this case the positive border of F ' is {A, BCD}.  
In the sanitization process, as presented  in Table 2, each entry tij that belongs to the transaction 
supporting  the  sensitive  itemsets  is  replaced by a binary variable uij. The goal  is  to determine  the 
value of uij that satisfies the following inequalities (i.e., sensitive itemsets should be infrequent in D' 
In  these  approaches,  sanitized  database  is  generated  according  to  a  given  collection  of  frequent 
itemsets with some privacy concerns. That  is, they do not focus on how to modify the database to 
hide  the  sensitive  knowledge.  Instead,  they  focus  on  how  to  modify  the  collection  of  frequent 
itemsets mined from the original database to avoid revealing sensitive knowledge. And, by using the 
modified  collection  of  frequent  itemsets,  a  compatible  database  is  generated  as  the  sanitized 
database. As discussed  in  [48‐51], although  there  is a one to one mapping  from an  itemset  lattice 
structure with support  information to a binary dataset,  finding a binary dataset compatible with a 
given collection of frequent itemsets is a NP‐hard problem. Therefore, the computation costs of the 
approaches  in the category are high. Based on the  idea of  inverse  itemset mining, Chen et al. [52] 
introduced  a database  reconstruction  framework  for data  sharing with  privacy preserving.  In  the 
sanitization  procedure,  the  frequent  itemsets  mined  from  the  original  database  are modified  to 
confirm  the  user‐specified  security  constraint  and  the  trade‐off  principle,  so  that  the  sensitive 
itemsets are protected while the side‐effect  is tolerable. Then an  iterative database reconstruction 
procedure  is  performed  to  check  the  verification  for  consistency  among  itemset  supports  in  the 
lattice and modify the itemset support if necessary. Wu et al. [53] proposed a feasible solution to the 
problem  of  inverse  frequent  itemsets  mining  by  applying  graph‐theoretical  results  to  divide  the 
original itemsets into components that preserve maximum likelihood estimation. Moreover, Y. Guo 
[54‐55] proposed an FP‐tree based approach for database reconstructing. 
 
4. Conclusion 
The  techniques  of  association  rule  mining  have  been  widely  used  in  business  applications  to 
understand the behaviour of customers and support decision‐making. However, the serious privacy 
concerns  that  are  raised  when  sharing  the  databases  with  untrusted  third  parties.  Therefore, 
sensitive  rules hiding problem  is  initiated and  focuses on how  to modify  the database  to hide  the 
association  rules with  privacy  concerns whilst minimizing  the  side  effects.  In  1999, Atallah  et  al. 
showed  that  the  sensitive  rule  hiding  problem  is  NP‐hard.  After  that,  a  lot  of  approaches  were 
proposed to solve the problem efficiently and effectively. In general, the research in this area can be 
divided  into  four  categories:  (i)  heuristic  based  approaches,  (ii)  border  based  approaches,  (iii) 
constraint‐satisfaction problem approaches, and  (iv) database  reconstruction approaches. We use 
the following table for comparing the performance metrics of the approaches. 
  Execution 
time 
Scalability Hiding 
Failure 
Information 
loss 
Modification 
degree 
Heuristic 
approaches 
Sensitive 
transactions 
identification 
methods[19‐22] 
moderate low  low  moderate  moderate 
Sensitive 
associations 
clustering 
methods[25‐26] 
moderate moderate none  moderate  moderate 
[3] J. Han and Y. Fu, “Discovery of Multiple‐level Association Rules from Large Databases,” Proc. of 
International Conference Very Large Data Bases, pp. 420‐431, 1995. 
[4]  J.S.  Park, M.S.  Chen,  and  P.S.  Yu,  “An  Efficient  Hash‐based  Algorithm  for Mining  Association 
Rules,” Proc. of ACM SIGMOD  International Conference on Management of Data, pp. 175‐186, San 
Jose, Ca, May, 1995. 
[5] A, Savasere, E. Omiecinski, and S. Navathe, “An Efficient Algorithm for Mining Association Rules 
in Large Databases,” Proc. of International Conference Very Large Data Bases, pp. 432‐443, 1995. 
[6] S. Brin, R. Motwani,  J.D. Ullman, and S. Tsur. “Dynamic  Itemset Counting and  Implication Rules 
for Market Basket Analysis,” Proc. of ACM  SIGMOD  International Conference on Management of 
Data, pp. 255‐264, 1997. 
[7]  J. Han,  J. Pei,  and Y.  Yin,  “Mining  Frequent Patterns Without Candidate Generation,” Proc. of  
ACM SIGMOD International Conference on Management of data, 2000. 
[8]  M.J.  Zaki,  “Scalable  Algorithms  for  Association  Mining,”  IEEE  Transaction  on  Knowledge  and 
Information Engineering, Volume 12, 2000. 
[9]  B.  Chen,  P.  Haas,  and  P.  Scheuermann,  “A  New  Two‐Phase  Sampling  Based  Algorithm  for 
Discovering  Association  Rules,”  Proc.  of  ACM  SIGKDD  International  Conference  on  Knowledge 
Discovery and Data Mining, 2002. 
[10]  C.  Clifton  and  D.  Marks,  “Security  and  Privacy  Implications  of  Data  Mining,”  Proc.  of  ACM 
Workshop Research Issues in Data Mining and Knowledge Discovery, 1996. 
[11] M. Atallah, E. Bertino, A. Elmagarmid, M.  Ibrahim, and V. Verykios,  “Disclosure  Limitation of 
Sensitive Rules,” Proc. IEEE Workshop on Knowledge and Data Engineering Exchange, pp.45‐52, 1999. 
[12]  P.  Wang,  “Research  on  Privacy  Preserving  Association  Rule  Mining  a  Survey,”  Proc.  of  IEEE 
International Conference on Information Management and Engineering, pp. 194‐198, 2010. 
[13] V.S. Verykios, E. Bertino, I.G. Fovino, L.P. Provenza, Y. Saygin, and Y. Theodoridis, “Start‐of‐the‐
art in Privacy Preserving Data Mining,” SIGMOD Record, Volume 33, pp. 50‐57, 2004. 
[14]  V.S.  Verykios  and  A.  Gkoulalas‐Divanis,  “A  survey  of  Association  Rule  Hiding  Methods  for 
privacy,”  Privacy  Preserving Data Mining: Models  and Algorithms,  Springer Berlin Heidelberg, pp. 
267‐289, 2008. 
[15] E. Bertino, I.N. Fovino, and L.P. Provenza, “A Framework for Evaluating Privacy Preserving Data 
Mining Algorithms,” Data Mining and Knowledge Discovery, Volume 11, pp. 121‐154, 2005. 
[16] E. Dasseni, V. S. Verykios,   A.K. Elmagarmid, and E. Bertino, “Hiding Association Rules by Using 
Confidence and Support,” Proc. of the 4th  International Workshop on  Information Hiding, pp. 369‐
383, 2001. 
[17] V.S. Verykios, A.K. Elmagarmid, E. Bertino, Y. Saygin and E. Dasseni,  “Association Rule Hiding,” 
IEEE Trans. on Knowledge and Data Engineering, Volume 16, pp. 434‐447, 2004. 
[33] Z. Wang, W. Wang, B. Shi, and S.H. Boey, “Preserving Private Knowledge  in Frequent Pattern 
Mining,” Proc. of the IEEE International Conference on Data Mining Workshops, pp. 530‐534, 2006. 
[34] Z. Wang, W. Wang, and B. Shi, “Blocking Inference Channels in Frequent Pattern Sharing,“ Proc. 
of the International Conference on Data Engineering , pp. 1425‐1429, 2007. 
[35] Y. Saygin, V.S. Verykios, C. Clifton, “Using Unknowns to Prevent Discovery of Association Rules,” 
SIGMOD Record, Volume 30, pp.45‐54, 2001. 
[36] Y. Saygin, V.S. Verykios, A.K. Elmagarmid, “Privacy Preserving Association Rule Mining,” Proc. of 
IEEE  International Workshop on Research  Issues  in Data Engineering: Engineering e‐Commerce/ e‐
Business Systems, pp.151‐158, 2002. 
[37] S.J. Wang and A. Jafari, “Using Unknowns for Hiding Sensitive Predictive Association Rules,” Proc. 
of IEEE International Conference on Information Reuse and Integration, pp.223‐228, 2005. 
[38] E.D. Pontikakis, Y. Theodoridis, A.A. Tsitsonis, L. Chang, and V. S. Verykios, “A Quantitative and 
Qualitative Analysis of Blocking in Association Rule Hiding,” Proc. of the ACM workshop on Privacy in 
the electronic society, pp.29‐30, 2004. 
[39] X.Sun, and P.S.Yu. “A Border‐Based Approach for Hiding Sensitive Frequent  Itemsets,” Proc. of 
IEEE International Conference on Data mining, pp.426‐433, 2005. 
[40]  X.  Sun  and  P.S.  Yu,  “Hiding  Sensitive  Frequent  Itemsets  by  a  Border‐Based  Approach,” 
Computing Science and Eng., vol. 1, pp. 74‐94, 2007. 
[41] G.V. Moustakidesa and V.S. Verykios, “A MaxMin Approach for Hiding Frequent Itemsets,” Data 
and Knowledge Engineering, Volume 65, pp. 75‐89, 2008. 
[42]  H.  Mannila  and  H.  Toivonen,  “Levelwise  Search  and  Borders  of  Theories  in  Knowledge 
Discovery,” Data Mining and Knowledge Discovery, Volume 1, pp. 241‐258, 1997. 
[43]  S.  Menon,  S.  Sarkar,  and  S.  Mukherjree,  “Maximizing  Accuracy  of  Shared  Databases  When 
Concealing Sensitive Patterns," Information Systems Research, Volume 16, pp.256‐270, 2005. 
[44] S. Russell and P. Norvig, “Artificial Intelligence: A Modern Approach,” Prentice‐Hall, 2nd edition, 
2003. 
[45]  C.  Gueret,  C.  Prins,  and  M.  Sevaux,  “Applications  of  Optimization  with  Xpress‐MP,”  Dash 
Optimization Ltd, 2002. 
[46]  A.G..  Divanis,  and  V.S.  Verykios,  “An  Integer  Programming  Approach  for  Frequent  Itemset 
Hiding,” Proc. of ACM  International Conference on  Information and Knowledge Management, pp. 
748‐757, 2006. 
[47] A.G. Divanis,  and V.S. Verykios,  “Exact Knowledge Hiding Through Database Extension,”  IEEE 
Trans. on Knowledge and Data Engineering, pp.699‐713, Volume21, 2009. 
[48]  T. Mielikainen,  “On  Inverse  Frequent  Set Mining,”  Proc.  of  IEEE  ICDM Workshop  on  Privacy 
Preserving Data Mining, pp. 18‐23, 2003. 
Solving the Sensitive Itemset Hiding Problem
Whilst Minimizing Side Eﬀects on a Sanitized
Database
Guanling Lee, Yi-Chun Chen, Sheng-Lung Peng, and Jyun-Hao Lin
Department of Computer Science and Information Engineering
National Dong Hwa University, Hualien 974, Taiwan
guanling@mail.ndhu.edu.tw
Abstract. Mining frequent itemsets from huge amounts of data is an
important issue in data mining, with the retrieved information often be-
ing commercially valuable. However, some sensitive itemsets have to be
hidden in the database due to privacy or security concerns. This study
aimed to secure sensitive information contained in patterns extracted
during association-rule mining. The proposed approach successfully hides
sensitive itemsets whilst minimizing the impact of the sanitization pro-
cess on nonsensitive itemsets. Our approach ensures that any modiﬁca-
tion to the database is controlled according to its impact on the sanitized
database. The results of simulations demonstrate the beneﬁts of our ap-
proach.
Keywords: data mining, frequent patterns, sensitive itemsets, sanitiza-
tion process.
1 Introduction
The techniques of data mining [2] have been widely used in business applica-
tions to understand the behavior of customers and support decision-making.
Some companies share their data in business collaborations in order to increase
leverage proﬁtability and market share or reduce production costs. However,
in a cooperative project, some of these companies may want certain strategic
or private data called sensitive itemsets to not be published in the database.
Therefore, before sending database to other parties, these companies would like
to transform their original databases into new ones that hide the sensitive item-
sets according to some speciﬁc privacy policies.
The process of hiding sensitive itemsets is called data sanitization [1]. Verykios
et al. considered the problem of hiding sensitive information to be an important is-
sue of preserving privacy when data mining [14]. The sanitization process needs to
ensure that the quality of the database is preserved by minimizing the impact on
nonsensitive frequent itemsets. Therefore, the problem of determining the most
 Corresponding author.
R.-S. Chang, T.-h. Kim, and S.-L. Peng (Eds.): SUComS 2011, CCIS 223, pp. 104–113, 2011.
  Springer-Verlag Berlin Heidelberg 2011
106 G. Lee et al.
and X is a pattern, and is said to contain pattern Y iﬀ Y ⊆ X . A transaction
database TDB is a set of transactions. Let Γ (X) be the set of transactions
containing itemset X . Given a database D, SupX denotes the support of an
itemset X , which is the number of transactions in D containing X (i.e., |Γ (X)|).
An itemset X is called frequent in D iﬀ its frequency in D is at least equal to
a minimum threshold minf . Equivalently, X is frequent in D iﬀ SupX ≥ msup,
where msup = minf × |D|. In general, D can be represented as a binary matrix
D = Tn×m, where n is the number of transactions and m is the number of items
in D. Entry tij in D is 1 if the jth item (i.e., Ij) appears in the ith transaction;
otherwise tij = 0. For convenience, we use Ti to denote the ith transaction of D.
To hide the sensitive frequent itemsets, the original database (D) needs to
be modiﬁed into the sanitized database, D′. Side eﬀects of the process of hiding
sensitive itemsets on the sanitized database need to be minimized, and can be
evaluated from two aspects: (1) the information loss, which is quantiﬁed as
the number of nonsensitive itemsets that become infrequent in D′; and (2) the
modiﬁcation degree, which is quantiﬁed as the number of variant bits between D
and D′. This paper investigates how to hide sensitive itemsets from a database
whilst minimizing the information loss and modiﬁcation degree in the sanitized
database.
2.2 Hiding Methodology
The concept of [3] is summarized as follows. The set of frequent itemsets in D is
F and the set of sensitive itemsets is SI. According to the Apriori property, if
a sensitive itemset X is hidden, any super-itemset of X is also hidden from F .
Therefore, the set of nonsensitive itemsets, F ′, can be computed by removing
each sensitive itemset and its super-itemset from F . The apriori property also
allows us to focus on the positive border of F ′, denoted as B+(F ′), which consists
of all the largest itemsets in F ′, i.e., B+(F ) = {X |X ∈ F and ∀Y ∈ F,X  Y }.
The basic idea of [3] is that an itemset X is frequent in D′ if its number of
occurrences is no smaller than msup, which can be stated as
δX =
n∑
i=0
∏
Ij∈X
t′ij ≥ msup, where t′i,j =
{
1, if Ij ∈ Ti
0, otherwise.
(1)
Equivalently, it is infrequent if δX < msup. Therefore, each itemset in B+(F ′)
and SI can be transformed into a corresponding inequality to represent that it
should be frequent or infrequent in D′.
However, the CSP may be infeasible in many real datasets, in which case a
relaxation procedure is repeated by removing the constraint corresponding to the
largest and minimum-support itemset in B+(F ′) until the CSP becomes feasible.
Removing the constraint indicates that the corresponding itemset and some of its
subsets will be infrequent in D′, and we name such itemsets as victim itemsets.
How to minimize the number of victim itemsets (i.e., the information loss) is an
important issue in the sanitization problem. Therefore, the constraint should be
removed carefully to reduce the number of victim itemsets. This paper considers
108 G. Lee et al.
Lemma 2. Aﬀected group AGX is infeasible free if
SupX − |Γ (X)−
⋃
Y ∈AGX
Γ (Y )| −
∑
Y ∈AGX
|overlapY | < msup,
where overlapY = Γ (X) ∩ Γ (Y )−
⋃
Y ′∈AGX and Y ′ =Y
Γ (Y ′).
Proof. Assume itemsets Y1, . . . , Yn belong to aﬀected group AGX . It is clear that
the set of transactions that can be modiﬁed to reduce the support of X without
aﬀecting the supports of Y1, . . . , Yn is Γ (X)−
⋃
Y ∈AGX
Γ (Y ).
Furthermore, some transactions that overlap between X and each itemset Yi
can also be modiﬁed without aﬀecting the support of Y1, . . . , Yn. We denote this
set of transactions as overlapYi , which can be calculated as Γ (X) ∩ Γ (Yi) −⋃
j=1∼n,j =i
Γ (Yj). That is, overlapYi contains the transactions that support X
and Yi but do not support any other itemsets belonging to aﬀected group AGX .
Therefore, for the transactions belonging to overlapYi , some item x (where x ∈ X
and x /∈ Yi) can be eliminated to reduce SupX without aﬀecting SupYi.
According to the above discussion, the total number of transactions that
can be safely modiﬁed to decrease SupX is Δ = |Γ (X) −
⋃
Y ∈AGX
Γ (Yj)| +∑
Y ∈AGX
|overlapY |. Then, if SupX − Δ < msup (i.e., then X can be success-
fully hidden without aﬀecting any other itemsets belonging to B+(F ′)), aﬀected
group AGX is infeasible free. unionsq
The groups that cannot satisfy Lemma 2 are identiﬁed as hazardous groups, and
the itemsets therein are identiﬁed as hazardous itemsets. For each hazardous
itemset, a gaining factor is computed to determine the net proﬁt of discarding it.
3.2 Measuring the Gaining Factor
The number of hazardous groups to which the itemset belongs is referred to as
the relaxation degree of discarding it in the sanitization process. The relaxation
degree of an itemset Y , denoted as RDY , can be regarded as the largest number
of hazardous groups that may become infeasible free by removing the inequal-
ity corresponding to Y from the CSP. That is, RDY is the greatest beneﬁt of
discarding Y . However, because Y ∈ B+(F ′), discarding Y may result in some
frequent itemsets Y ′ (where Y ′ ⊆ Y ) becoming infrequent after the sanitization
process. We denote the set of itemsets that may become infrequent after remov-
ing the inequality corresponding to Y as Dis(Y ), where Dis(Y ) = {Y ′ | Y ′ ⊆ Y
and ∀P ∈ B+(F ′)−Y, Y ′  P . That is, if Y ′ is not a sub-itemset of any itemset
belonging to B+(F ′)−Y (except for the inequality corresponding to Y ), there are
110 G. Lee et al.
Algorithm 2. Hazardous Groups identiﬁcation( )
1: for each X ∈ aﬀective itemsets do
2: Create aﬀected group AGX ;
3: end for
4: for each Y in current PB do
5: for each X in aﬀective itemsets do
6: if Y ∩X = ∅ then
7: AGX = AGX ∪ {Y };
8: end if
9: end for
10: end for
11: for each aﬀected group AGX do
12: overlap=0;
13: for each Y ∈ AGX do
14: overlapY = Γ (Y ) ∩ Γ (X)−⋃Y ′∈AGX∧Y ′ =Y Γ (Y ′);
15: overlap = overlap + |overlapY |
16: end for
17: if supx − |Γ (X)−⋃Y ′∈AGX | − overlap < msup then
18: aﬀected itemsets = aﬀected itemsets −X ;
19: else
20: Identify AGX as Hazardous groups;
21: end if
22: end for
Table 1. Experimental parameters. The ration of sensitive itemsets is the number of
sensitive itemsets divides the number of frequent itemsets.
Notation Description Default
value
Range
N Total number of transactions 10000 -
I Number of distinct items 100 -
LEN Average length of a transaction 10 -
RS Ratio of sensitive itemsets =
number of sensitive itemsets
number of frequent itemsets
0.0003 0.0001478 ∼ 0.0007382
4 Experimental Results
This section describes tests of the performance of our approach, and compares
it with that of the Inline algorithm proposed in [3]. Information loss which
means that some of the non-sensitive items are hidden in D′ and Modiﬁcation
degree which means that the number of variant bits between the original and the
sanitized databases are used to measure the quality of D′. Both our approach
and the Inline algorithm ensure that the sensitive itemsets will be successfully
112 G. Lee et al.
Ĵıı
ĵıı
Ķıı
ķıı
ıįıııĲĵĸĹ ıįıııĳĺĶķ ıįıııĵĵĴĲ ıįıııĶĺĲĴ ıįıııĸĴĹĳ
ņŹ
ŦŤ
Ŷŵ
ŪŰ
ůġ
ŵŪŮ
Ŧĩ
ŴŦ
ŤĪ
ųŢŵŪŰġŰŧġŴŦůŴŪŵŪŷŦġŪŵŦŮŴŦŵŴ
ŐŶųġŢűűųŰŢŤũ
ŊůŭŪůŦġŢŭŨŰųŪŵũŮ
Fig. 3. Comparison of the variation in the execution time with the ratio of sensitive
itemsets between our approach and the Inline algorithm
Inline algorithm are compared in Fig. 3. The focus of our approach on minimizing
the side eﬀects on the sanitized database increases the time needed to break the
infeasible situation, which slightly increases the execution time.
5 Conclusion
This paper has addressed the hiding of sensitive itemsets in a database whilst
minimizing the information loss and modiﬁcation degree in the sanitized database.
The approach involves transforming the sanitization problem into a CSP, with
the side eﬀects on the sanitized database being minimized when the CSP is in-
feasible. This is achieved by considering only the itemsets in B+(F ′) that may
result in an infeasible CSP and removing the inequality according to the net
proﬁt to be gained in a decreasing order. The experimental results show that
though the execution time of our approach is slightly longer than that of the
Inline algorithm, the information loss and modiﬁcation degree of our approach
are both markedly lower than those of the Inline algorithm. This indicates that
our approach preserves more nonsensitive itemsets in the sanitized database.
References
1. Atallah, M., Bertino, E., Elmagarmid, A., Ibrahim, M., Verykios, V.: Disclosure
Limitation of Sensitive Rules. In: IEEE Workshop on Knowledge and Data Engi-
neering Exchange, pp. 45–52 (1999)
2. Ayubia, S., Muyebab, M.K., Baraania, A., Keanec, J.: An Algorithm to Mine
General Association Rules from Tabular Data. Information Sciences 179, 3520–
3539 (2009)
3. Divanis, A.G., Verykios, V.S.: An Integer Programming Approach for Frequent
Itemset Hiding. In: ACM International Conference on Information and Knowledge
Management, pp. 748–757 (2006)
R.-S. Chang, T.-h. Kim, and S.-L. Peng (Eds.): SUComS 2011, CCIS 223, pp. 74–83, 2011. 
© Springer-Verlag Berlin Heidelberg 2011 
Summarizing Association Itemsets by Pattern 
Interestingness in a Data Stream Environment 
Guanling Lee, Yu-tang Zhu, and Yi-Chun Chen 
Department of Computer Science and Information Engineering 
National Dong Hwa University, Hualien, Taiwan, R.O.C 
guanling@mail.ndhu.edu.tw 
m9821002@ems.ndhu.edu.tw 
divien@gmail.com 
Abstract. In the age of Knowledge economy, people are paying more attention 
to data mining. However, the number of the mined association patterns often 
exceeds the capacity of human’s mind. Therefore, it is necessary for effectively 
present patterns according to their interestingness. This approach focuses on 
continuously differentiating interesting and valuable patterns from data stream 
and proposes a new data structure, Pattern’s Interestingness Tree (PI-Tree) for 
discovering frequent patterns and helping to distinguish interesting knowledge. 
Performance Analysis indicates that the proposed approach is efficient for 
IOKD. 
Keywords: Association itemsets, Knowledge Discovery, Data Stream, Data 
mining. 
1   Introduction 
Data mining has become a progressively important technology in recent decades, 
especially association patterns (rules) mining which was first investigated in [1]. Due 
to the number of association rules often exceeds the capacity of human’s mind, it is 
necessary to effectively present the rules according to the interestingness. In general, 
the interestingness of a rule can be measured for different criteria. Among them are 
confidence and support, gain, variance and chi-squared value, entropy gain, gini, 
laplace, lift (a.k.a. interest or strength), and conviction [2]. Omiecinski also proposed 
the other three kinds of alternative measures, any-confidence, all-confidence, and 
bond [3]. Above mentioned approaches are objective interesting measures, because 
they are all based on the structure of discovered patterns and the statistics underlying 
them. However, “Your honey is my poison.” The difference of users’ knowledge 
should be considered as well. It leads the way for Interestingness Analysis System [4], 
which is the originator of subjective interesting measures. These kinds of measures 
are based on user beliefs in the data. As the length limit, more details and resembling 
approaches can be found on [4][5][6][7][8]. 
In recent years, emerging applications, such as sensor network data analysis, web-
click stream mining, network traffic surveillance and ubiquitous computing, call for a 
study of a new kind of environment, named: data stream. When the data stream 
76 G. Lee, Y.-t. Zhu, and Y.-C. Chen 
Our approach will use the foregoing principles as the guidelines for modeling IOKD 
and take root in the FP-Tree [13] and the DS-Tree [14]. 
3   Problem Definition and Algorithm 
Let a data stream is decomposed for infinite batches, each batch consists of n  
transactions. After preprocessing, amounts of knowledge pattern P  are generated, 
which is in the form of itemset: 1 2 }{ , , ..., kP i ii= . 
3.1   Problem Definition 
In general, the purpose of IOKD is as follows: 
Given a data stream, S ; a pre-existing knowledge pattern base,  ; two user-
specific bounds, μ  and λ . Calculates the dissimilarity of each undifferentiated 
knowledge P B∈ , with 'P ∈ , the most similar pre-existing knowledge of P , 
where B  is the latest batch of S . Then according to Principle 3~5 in section 2, 
differentiates P  into one of three states :{ Unknown, Static, Evolving}, by setting 
such dissimilarity as its interestingness. And finally, update   with B . 
Since the naturally fluctuation of data stream, the knowledge it brings are always 
drifting as well.  
To describe the degree of knowledge-drifting, two major dimensions: (i) Property 
and (ii) Age are considered to measure the dissimilarity among knowledge patterns in 
our approach. 
i) Property of pattern 
The property of an itemset can be represented by the composition of items it contains 
since it’s the co-occurrence of potentially related items. If two itemsets are sharing the 
same common items, they are also sharing the same property for the subset of their 
intersection. 
In behalf of measuring the dissimilarity for the pattern pair: 1P  and 2P  in the 
dimension of property, 1 2( , )DP P P  is defined as follow: 
Definition 1. Dissimilarity of Property, 1 2( , )DP P P : 
1 2
1 2
1 2
( , ) 1 P PDP P P
P P
∩
= −
∪
 (1)
1 2( , )DP P P  is an extended application of Jaccard Distance. It stands for the ratio of 
difference between 1P  and 2P .  
ii) Age of pattern 
The period (age) of validity for a knowledge pattern can be regarded as its subjective 
reliability. While a coming knowledge may be the evolving result of several pre-
existing knowledge, people may subjectively associate it with the most senior one of 
them since it’s more credible than others. Therefore, people also will be more 
interested if their prejudice has been evolved, which brings more unexpectedness. 
78 G. Lee, Y.-t. Zhu, and Y.-C. Chen 
#
 
Fig. 1. The states transition of a pattern in IOKD 
Thus for updating   efficiently and meaningfully, Definition 5~7 are defined: 
Definition 5. Frequency of pattern rP  at time t , . ( )rP Freq t , can be formulated as 
+( ) ( 1) ( ) . . .r r rP Freq t P Freq t P Freq tδ= − ⋅  (5)
Since the effect of past knowledge is fading though the time, Definition 5 is used as 
the accumulated frequency of every rP ∈ to keep both past and present frequency 
of rP . δ  is the decay factor, we let 0.5δ =  for convenient in our approach. 
Definition 6. rP is frequent if . ( )rP Freq t min_sup≥ . 
And the definition for the age of pattern rP ∈ is presented as follow: 
Definition 7. Age of pattern, .rP Age : 
. 1         ,    
.
.               ,  
r r
r
r
P Age if P is frequent
P Age
P Age otherwise
+
=
⎧ ⎫⎨ ⎬⎩ ⎭  (6)
However, the naïve approach of IOKD — to calculate the dissimilarities between all 
coming knowledge with all pre-existing knowledge — may still suffer from a 
tremendous amount of redundant computation. To avoid this predicament, some 
observations of Definition 4 are very useful. 
80 G. Lee, Y.-t. Zhu, and Y.-C. Chen 
(1)  while S has the latest unanalyzed batch do{ 
(2)    let B be the latest batch of S; 
(3)    for each transaction I in B do{ 
(4)      insert I into T;} // update T.HT(header-table of T) if necessary 
(5)    let L be the result list of all patterns in T; 
(6)    for each node-list NL in T.HT do{ // descending order 
(7)       for each node N in NL do{ 
(8)         let P be the pattern which N stands for; 
(9)         let X be the result of P removes those whole-new items; 
(10)        if |X| = 0 then{ 
(11)          set P.PIG = 1;} // unknown pattern 
(12)    else{ 
(13)       for each item R in X do{ 
(14)          let RL be the node-list of R in T.HT; 
(15)  for each node Q in RL do{   
(16)     check P,Q by using lemmas 1&&2;} } } } 
(17)       run Maturate(N,L);} 
(18)    sort L by PIG; 
(19)    output three parts of L: Unknown, Static and Evolving; } 
} 
 
Fig. 2. IOKD algorithm 
Input: a node, N; the result list, L; 
output: null. 
 
Procedure Maturate(N,L) 
{  
(1)  let P be the pattern which N stands for;  
(2)  if P.freq is less than min_sup then{ // P is infrequent 
(3)   prune N and all its children in T;} 
(4)  else 
(5)   P.Freq *= δ; 
(6)   P.Age += 1; 
(7)  add a copy of P into L; 
(8)  reset P.PIG = μ for next batch;} 
} 
 
Fig. 3. Procedure Maturate 
82 G. Lee, Y.-t. Zhu, and Y.-C. Chen 
0
150
300
450
600
750
3 6 9 12 15
Average length of transaction
Ex
ec
u
tio
n
 T
im
e 
(Se
c
Naïve
Filter
  
Fig. 5. Execution times vs. Average length of transaction 
0
10
20
30
40
50
0.2K 0.4K 0.6K 0.8K 1K
Number of distinct items
Ex
ec
u
tio
n
 
Ti
m
e 
(Se
c
Naïve
Filter
  
Fig. 6. Execution times vs. Number of distinct items 
5   Conclusion 
In this paper, a framework is introduced to solve the issue of interestingness-oriented 
knowledge discovery (IOKD) of knowledge stream. On the foundation of behavioral 
science and cognitive psychology, six principles are devised to define the 
interestingness of knowledge and model the knowledge stream. Two interestingness 
measures are proposed for measuring the interestingness growth of knowledge from 
different dimensions. A new data structure, Pattern’s Interestingness Tree (PI-Tree) is 
presented for efficiently maintaining and refining the pre-existing knowledge base. Two 
filtering lemmas are derived for improving the efficiency of our approach. The results of 
performance analysis indicate that the Filtered-IOKD is efficient, effective and capable 
for real-time differentiating interesting knowledge from the knowledge stream. 
Summarizing Association Patterns Efficiently by Using PI Tree in a Data Stream 
Environment 
 
Guanling Lee, Yu-tang Zhu and Yi-Chun Chen 
Department of Computer Science and Information Engineering 
National Dong Hwa University 
 Hualien, Taiwan, R.O.C. 
guanling@mail.ndhu.edu.tw, dodoju@hotmail.com, divien@gmail.com  
 
 
 
Abstract 
 
In the age of Knowledge economy, people are paying more 
attention to data mining; especially association patterns 
mining in owing to valuable information have often been 
buried with irrelevant noise. However, the number of the 
association patterns often exceeds the capacity of human’s 
mind. Therefore, it is necessary to present patterns 
according to their interestingness effectively. On the other 
hand, the interestingness of patterns vary from different 
situation; people are interesting in revealing undiscovered 
truths, but also boring when they are familiar with them. 
Therefore, this approach focuses on continuously 
differentiating interesting and valuable patterns from data 
stream, which is the issue for Interestingness-oriented 
Knowledge Discovery (IOKD). Several principles and 
interestingness measures are proposed for modeling and 
measuring the interestingness of association patterns in a 
data stream. A new data structure, Pattern’s 
Interestingness Tree (PI-Tree) is presented for discovering 
frequent patterns and helping to distinguish interesting 
knowledge. Performance Analysis indicates that the 
proposed approach is efficient for IOKD. 
 
Keywords: Association Patterns, Knowledge Discovery, 
Data Stream, Data Mining 
 
1   Introduction 
Data mining has become a progressively important 
technology in recent decades, especially association 
patterns (rules) mining which was first investigated in [1]. 
The approaches of finding association rules in a static 
database can be roughly categorized into two categories. 
Apriori method [1], proposed by Agrawal et al., is on 
behalf of the first category and there are many researches 
relative to it [23]. On the other side, FP-Tree [9] stands for 
the second category. Similar approaches can be found on 
[25]. 
Due to the number of association patterns often 
exceeds the capacity of human’s mind, it is necessary to 
effectively present the rules according to the 
interestingness. In general, the interestingness of a rule can 
be measured for different criteria. Among them are 
confidence and support, gain, variance and chi-squared 
value, lift, and conviction [21]. Omiecinski also proposed 
the other three kinds of alternative measures, 
any-confidence, all-confidence, and bond [18]. Above 
mentioned approaches are objective interesting measures, 
because they are all based on the structure of the patterns 
and the statistics underlying them. 
However, the difference of users’ knowledge should 
be considered as well. It leads the way for Interestingness 
Analysis System [15], which is the originator of subjective 
interesting measures. These kinds of measures are based 
on user beliefs in the data. As the length limit, more details 
and resembling approaches can be found on [22, 26, 27]. 
In recent years, emerging applications, such as sensor 
network data analysis, web-click stream mining and 
ubiquitous computing, call for a study of a new kind of 
environment, named: data stream. Data of data stream is in 
the form of swiftly continuous growth and its length may 
be potentially infinite. Although the basic ideas of Apriori 
and the FP-Tree methods focus on the static database, their 
spirit inspires lots of excellent research in the data stream 
environment. [4, 13, 16] are based on Apriori, and [12, 14, 
17] are derived from FP-Tree. 
When the data stream growth over time, it is constantly 
changing with a variety of properties and quantities, which 
brings more challenge for analyzing and describing the 
tremendous amount of reformed knowledge. On the other 
hand, just as a Chinese proverb goes: “If you stay in a 
room full of orchids for a long time, their fragrance will 
disappear.” People are interesting in revealing 
undiscovered truths, but also boring when they are familiar 
with them. 
By the above observations, we propose the idea that 
the patterns extracted from a data stream can be modeled 
as a knowledge stream. Moreover, the fluctuation of data 
stream by time triggers the evolution of knowledge and 
their interestingness. In this paper, several critical 
principles are proposed for modeling, describing and 
presenting knowledge stream, in owing to its evolvement 
and sensibility of human. And a new data structure, 
Pattern’s Interestingness Tree (PI-Tree) is presented for 
discovering frequent patterns and helping to distinguish 
interesting knowledge. 
The rest of the paper is organized as follows. The 
foundation of IOKD from behavioral science and 
cognitive psychology is introduced in section 2.  Section 3 
defines the purposes of IOKD, data structure and 
algorithms. Performance analysis is discussed in section 4, 
and section 5 concludes this work. 
 
2   Interestingness-oriented Knowledge 
Discovery 
Consequently, if the frequent pattern 1P  may evolve 
to 2P , its interestingness is proportional to the relative 
distance of age between them. 1 2( , )DA P P  is defined to 
measure such distance: 
Definition 2 Distance of Age, 1 2( , )DA P P : 
1 2
1 2
. .
( , )
( )
P Age P Age
DA P P
max_age

K
 
1.P Age  and 2 .P Age  are the age of 1P  and 2P , 
respectively. ( )max_age K  is the age of the oldest pattern 
in K . The relative distance of age for 1P  and 2P  can be 
reflected in 1 2( , )DA P P . 
In the light of principle 1, above formulas are 
integrated in the form of linear algebra to completely 
measure patterns’ dissimilarity: 
Definition 3 Dissimilarity of pattern-pair 1P  and 2P , 
1 2( , )D P P : 
1 2 1 2 1 2
1 2
( , ) ( , )      , 
                         1                              , 
( , )
DP P P DA P P if  P P  
otherwise
D P P
          
   If 1 2P P   , 1 2( , )D P P  reaches its maximum: 1, 
because 1P  and 2P  do not share any items. If 1 2P P , 
1 2( , )D P P  deservedly reaches its minimum: 0. 
User may also give different weight of each part :   
and   in 1 2( , )D P P , where 1   . 1 2( , )D P P  can be 
extended to more dimension with suitable adjustment if 
necessary. 
To calculate the interestingness of pattern P, P.PIG 
(Pattern’s Interestingness Growth), for all pre-existing 
frequent patterns Pr in K , ( , )rD P P is calculated, and set 
the minimum of it as .P PIG . 
Definition 4 The value of Pattern’s Interestingness 
Growth, .P PIG : 
 . min ( , )r rP PIG D P P P   K  
 
Figure 1. The states transition of a pattern in IOKD 
After getting .P PIG , IOKD differentiates P  into one 
of three states according to principles 3~5. The states 
transition of a frequent pattern in IOKD can be depicted as 
Figure 1. 
If a pattern migrates to another state, it will be 
noteworthy because there must be some reasons which 
trigger this evolvement. Such migration appears in series, 
and composes the “Knowledge Stream”. IOKD 
successfully models this phenomenon. 
Drifts of knowledge stream involve the maintenance 
and refinement of the pre-existing frequent pattern base 
K . On the other hand, even the knowledge of past may be 
valuable and interesting, it fades from peoples’ memory if 
it becomes a standstill. 
Thus for updating K  efficiently and meaningfully, 
Definition 5~7 are defined: 
Definition 5 Frequency of pattern rP  at time t , 
. ( )rP Freq t : 
+( ) ( 1) ( ) . . .r r rP Freq t P Freq t P Freq t    
Since the effect of past knowledge is fading though the 
time, Definition 5 is used as the accumulated frequency of 
every rP  K to keep both past and present frequency of 
rP .   is the decay factor.  
Then we use . ( )rP Freq t  as the criteria of frequent: 
Definition 6 rP is frequent if . ( )rP Freq t min_sup . 
The following definition is used to calculate the age of 
pattern rP  K when a new batch arrives. 
Definition 7  Age of pattern, .rP Age : 
. 1         ,    
.               ,  
. r rr
r
Age if is frequent
Age
P PP Age
P otherwise
     
  
However, the naïve approach of IOKD — to calculate 
the dissimilarities between all coming frequent patterns 
with all pre-existing patterns — may still suffer from a 
tremendous amount of redundant computation. To avoid 
this predicament, some observations of Definition 4 are 
very useful. 
Let   be half of the value of P.PIG calculated from 
now. In the process for finding the minimum of .P PIG , 
  will be replaced again and again when ( , ) 2rD P P  . 
In other words, if ( , ) 2rD P P  , P.PIG will not be 
changed. Thus two filtering lemmas are derived as 
follows: 
Lemma 1  ( , )rD P P should be calculated only when  
 
2
r rP P P P
 
 
  
     
Proof: 
Only when ( , ) 2rD P P  , P.PIG would be changed. 
Thus, we get that only when ( , )rDP P P    
( , ) 2rDA P P   , D(P,Pr) should be calculated to 
replace the current value of P.PIG. Let ( , )rDA P P  be  the 
minimal value zero, and divide the right side of inequation 
by 2. We get the derivation formulas as follow: 
The other procedures called in IOKD are listed as 
follows: 
 
Figure 3. Procedure CheckThePair 
Figure 3 represents the exposition of Procedure 
CheckThePair. Line 1 gets the pattern Pr by unpacking 
node Q. Line 2 calls the procedure: ShouldContinue(P, Pr) 
to check the pair of (P, Pr). If it returns True, lines 3~4 
calls CheckThePair(P, C) recursively for P and each 
child-node C of Q. 
 
Figure 4. Procedure ShouldContinue 
Figure 4 represents the exposition of Procedure 
ShouldContinue. Since self-comparisons are only needed 
for those patterns in K  in our approach, lines 1~2 check 
whether P equals Pr and P is a new-added node of PI-Tree, 
and return FALSE if they satisfied such condition. Line 4 
checks to see whether the pair (P, Pr) passes Lemma 1. If 
this pair passes the lemma, lines 5~7 follow Definition 4 to 
calculate the PIG value for pattern P. Lines 8~10 checks 
whether this pair passes Lemma 2. If this pair has been 
filtered out by anyone of the lemmas, finish this procedure 
and return FALSE; otherwise, return TRUE. 
Figure 5 shows the exposition of Procedure Maturate. 
Line 1 gets the pattern P  by unpacking node N. Lines 2~3 
will prune the branch starts with N in PI-Tree by following 
Definition 6. If P  is survived, Line 5 decays .P Freq  by 
 , which is mentioned in Definition 5, then line 6 weigh 
the age of P  by following Definition 7. Line 7 adds a 
copy of P  into the result list, then line 8 resets .P PIG  to 
  for next batch. 
 
Figure 5. Procedure Maturate 
 
4   Performance analysis 
In this section, a set of simulations was performed to 
present the efficiency and effectiveness of our approach. 
The testing data streams were generated by the IBM 
Synthetic-data Generator. The simulations were 
implemented in JAVA and performed by an Intel Core 2 
Duo 2.53GHz computer with 2GB of memory, running 
windows XP. Table 2 lists the parameters for generating 
the synthetic data streams and evaluating our approach for 
execution time and memory usage. The efficiencies of 
naïve-IOKD and Filtered-IOKD were compared in each 
simulation. 
Parameters Default value Ranges 
Number of 
transactions (per 
batch) 
4K 2K~10K 
Average length of 
transaction 
6 3~15 
Number of distinct 
items 
0.4K 0.2K~1K 
Minimum support 
threshold 
1.5% - 
Upper bound, μ, for 
the “Unknowns” 
0.85 - 
Lower bound, λ, for 
the “Statics” 
0.15 - 
Table 2. The parameters of performance analysis (for 
synthetic data stream) 
Input: a node, N; the result list, L; 
output: null. 
 
Procedure Maturate(N,L) 
{  
(1)let P be the pattern which N stands for;  
(2)if P.freq is less than min_sup then{ // P is infrequent
(3)    prune N and all its children in T;} 
(4)else 
(5)    P.Freq *= δ; 
(6)    P.Age += 1; 
(7)add a copy of P into L; 
(8)reset P.PIG = μ for next batch;} 
} 
Input: the pattern pair P and Pr. 
output: return TRUE if this pair passed the filter 
lemmas; otherwise, return False.  
 
Procedure ShouldContinue (P, Pr) 
{ 
(1)if P equals Pr and P is a new-added node of PI-Tree 
(2)    return FALSE; 
(3)else{ 
(4)    if the pair (P, Pr) passed the Lemma 1 then{ 
(5)        set PIG_Value = α* DP(P, Pr) + β* DA(P, Pr);
(6)        replace P.PIG if PIGValue is less than P.PIG; 
(7)        replace Pr.PIG if PIGValue is less than Pr.PIG;
(8)        if the pair (P,Pr) passed the Lemma 2 then 
(9)            return TRUE; 
(10)      else 
(11)          return FALSE; } 
(12)  else 
(13)      return FALSE; } 
} 
Input: a pattern,P; a node, Q. 
 
Procedure CheckThePair (P, Q) 
{ 
(1)let Pr be the pattern which Q stands for; 
(2)if ShouldContinue(P, Pr) = TRUE then{ 
(3)    for each child-node C of Q do{ 
(4)        run CheckThePair(P, C);} } 
} 
Minimum support threshold 2.0 (times) 
Upper bound, μ, for the “Unknowns” 0.85 
Lower bound, λ, for the “Statics” 0.15 
Table 3. The parameters of performance analysis (for real 
data stream of Google Trend ─ Jun. 26 ~ Jul. 11) 
The parameters in this simulation are listed as Table 3, 
and Table 4~6 exhibits the effectiveness of our approach. 
Symbol “U” means: Unknown; symbol “E” means: 
Evolving; symbol “S” means: Static; and symbol “X” 
means not exist in PI-tree. 
The state evolvement for all length-1 patterns (which 
means the teams) and length-2 patterns (which means the 
matches) are listed in Tables 4 and 5, respectively. As 
Table 4 and 5 shown, our approach successfully models 
and represents such evolvements of interestingness, for 
example: 
On Jul. 7th (the day of semi-final between Spain and 
Germany), the pattern “ESP GER” (Spain V.S. Germany) 
became frequent; however, it was regarded as Evolving 
instead of Unknown since the pattern “ESP PAR” (Spain 
V.S. Paraguay) was Static on Jul. 6th. Then “ESP GER” 
was regarded as Static until Jul. 9th. On Jul. 10th, “GER 
URU” became frequent again, but at the moment it was 
evolved from “ESP GER”, which responses Germany had 
lost the semi-final and the next match they needed to 
compete with Uruguay for the third place of World Cup. 
Finally, we found an interestingly phenomenon which 
extended by previous simulation. For people who live in 
Hong Kong, the pattern “Octopus” may be associated with 
“Octopus Company,” and the pattern “Octopus Paul” may 
be associated with the meaning of “an employee of 
Octopus Company who named Paul,” since the Octopus 
Company [19] is the famous company of smart card 
payment systems in Hong Kong. However, as Table 6 
showed, these two patterns surprisingly became frequent, 
because “Paul, The Octopus,” which has well-known by 
its accurate predictions of match results in 2010 FIFA 
World Cup. Such event changes the association between 
the pattern “Octopus Paul” and its meaning, which brings 
more interestingness. 
Table 4. The states evolvement of length-1 patterns 
which is presented by IOKD 
Table 5. The states evolvement of length-2 patterns which 
is presented by IOKD 
  Dates 
Patterns 6/26 6/27 6/28 6/29 6/30 7/1 7/2 7/3 7/4 7/5 7/6 7/7 7/8 7/9 7/10 7/11
Octopus X X X X X X X X X U S S S S S S
Octopus Paul X X X X X X X X X X E S S S S S
Table 6. The states evolvement of “Octopus” and 
“Octopus Paul” 
 
5   Conclusion 
In this paper, a framework is introduced to solve the issue 
of interestingness-oriented knowledge discovery of 
knowledge stream. On the foundation of behavioral 
science and cognitive psychology, six principles are 
devised to define the interestingness of patterns and model 
the knowledge stream. Two interestingness measures are 
proposed for measuring the interestingness growth of 
knowledge from different dimensions. A new data 
structure, Pattern’s Interestingness Tree (PI-Tree) is 
presented for efficiently maintaining and refining the 
pre-existing frequent patterns base. Two filtering lemmas 
are derived for improving the efficiency of our approach. 
The results of performance analysis for synthetic and real 
data both indicate that the Filtered-IOKD is efficient, 
effective and capable for differentiating interesting 
frequent patterns from the knowledge stream. 
 
References 
 [1] R. Agrawal and R. Srikant. “Fast Algorithms for 
Mining Association Rules in Large Databases”. 
Proceedings of International Conference on Very 
Large Data Base, 1994, pp.487-499. 
[2] J. Bauer,”Warum ich fühle, was du fülst”, Hamburg, 
2005 
[3] D. E. Berlyne, “Structure and Direction in Thinking”, 
John Wiley & Sons, Inc., 1965. 
[4] A. Boukerche and S. Samarah, “A Novel Algorithm for 
Mining Association Rules in Wireless Ad Hoc Sensor 
Networks”, IEEE Transactions on Parallel and 
Distributed Systems, Vol. 19, Issue 7, July 2008. 
[5] 鄭昭明, “認知心理學”, 桂冠, 2004. 
  Dates 
Patterns 6/26 6/27 6/28 6/29 6/30 7/1 7/2 7/3 7/4 7/5 7/6 7/7 7/8 7/9 7/10 7/11
ESP  U S S S S S S S S S S S S S S S 
POR  X X X U S S S S S S S S S X X X
PAR  U S S S S S S S S S S S S X X X
GER  U S S S S S S S S S S S S S S S 
NED  U S S S S S S S S S S S S S S S 
SVK  X X U S S S S S S S S X X X X X
BRA  U S S S S S S S S S S S S X X X
URU  U S S S S S S S S S S S S S S S 
ENG  U S S S S S S S S S S S S S X X
ARG  U S S S S S S S S S S S S S X X
KOR  U S S S S S S S S S S S S X X X
GHA  U S S S S S S S S S S S S S X X
USA  U S S S S S S S S S X X X X X X
MEX  X U S S S S S S S S S S S X X X
CHI  X X U S S S S S S S S S S X X X
JPN  X X X U S S S S S S S S S X X X
is currently pursuing the Ph.D. degree in the same 
department.  His research interests include data mining and 
search in the P2P network. 
97 年度專題研究計畫研究成果彙整表 
計畫主持人：李官陵 計畫編號：97-2221-E-259-015-MY3 
計畫名稱：RFID 數位家庭中網路技術與應用之整合及個人化服務平台之建置--子計畫四:數位家庭資
料探勘服務 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 2 4 100%  
研究報告/技術報告 2 2 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 8 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
