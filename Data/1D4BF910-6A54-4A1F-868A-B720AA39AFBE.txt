             
     The genes screened by SCAD SVM and WEPO will 
be used as the features for SVM [3] and SOM [4]. 
Section 2 introduces the basic concepts of SCAD SVM 
and WEPO while SVM and SOM are reviewed in 
Section 3. Section 4 presents the experimental results 
and Section 5 gives the conclusion. 
 
2. Gene selection methods 
 
2.1. SCAD SVM 
 
     Given the input data 
1{( , ),..., ( , )}1 nx x nS y y=                        
(1) 
 The SCAD SVM [1] is proposed as equation (2). 
     
,
1 1
1
min [1 ( )] (| |)w w x w
n d
b i i j
i j
y b p
n
λ
= =
− + ⋅ +∑ ∑             (2) 
where ix  is the i-th input vector and iy  is the label 
corresponding to ix . 
The penalty function is defined according to the 
following formulas 
( ) ( )
( )
2 2
2
                                   if 
2
       if 
2( 1)
1
                         if 
2
a
p a
a
a
a
λ
λ λ
λ λ
λ λ
λ λ
 ≤

− +
= − < ≤
−
 +
 >

w w
w w
w w
w
      (3) 
 
If some jw  in the calculated weight vector w is 
very close to zero or smaller than a certain threshold, 
then the j-th dimension (feature) is regarded as a 
redundant feature which will not be used 
 
2.2. WEPO 
 
     The concept of WEPO [2] is to use the punishment 
as the score for each gene. 
Define the different categories are 1C  and 2C , 
respectively, and each vector ig  contains the i-th gene 
expression of all the patients. 
 For each element in ig , the punishment 
of the i-th gene is defined according to the 
following equations.  
 
  
1 2
1
z, z > 0( ),     where     W(z)=
0, z 0p C q C
s W p q
∈ ∈

= −  ≤
∑ ∑
                
(4) 
2 1
2
z, z > 0( ),      where     W(z)=
0, z 0q C p C
s W q p
∈ ∈

= −  ≤
∑ ∑
              
(5)  
where p and q are two components in ig  
whose corresponding patients belong to 1C and 
2C , respectively. 
     The score of each gene is acquired from 
1 2min( , )s s  and the genes with lower scores are 
considered more important. 
 
3. Classification methods 
 
     In this section, we review SVM [3] and SOM [4] 
which are used for classification in our experiments. 
 
3.1. SVM 
 
     SVM projects high dimensional vectors to lower 
dimensional space and searches the hyperplane which 
has the maximum distance to the two nodes closest to 
each other between different classes. 
     The training data S is given from equation (1), we 
have to find out the optimal separating hyperplane in 
the following figure. 
 
Figure 1 The maximum margin hyperplane 
    The geometric margin γ  is then defined as 
21/ || ||w . 
We can maximize the geometric margin γ  by 
minimizing 
2|| ||w  as 
,
minimize
w b  ,w w  
operation treatment for hepatocellular carcinoma (HCC) 
at National Taiwan University Hospital.      
In our experiment, the 4 samples without HBV and 
HCV and one sample with both HBV and HCV in 
HCC55 are not used for classification. We will use 20 
of the remaining 50 samples as the training dataset 
consisting of 10 HBV and 10 HCV while other 19 
HBV and 11 HCV will be used for testing. 
Besides this, we used five datasets from the 
websites [6]. They are named ALL-AML_Leukemia 
[7], CentralNervous [8], ColonTumor [9], DLBCL [10], 
and Prostate [11] 
   The dataset ALL-AML_Leukemia contains 72 
samples of two classes, acute lymphoblastic leukemia 
(ALL) and acute myeloid leukemia (AML). Each 
sample has 7129 genes. There are 47 ALL and 25 
AML types respectively. We use 12 ALL and 12 AML 
as a training dataset, the remaining samples are used 
for testing. 
 The dataset CentralNervous contains 60 
samples of two classes, class 1 and  class 0. Samples 
labeled as class 1 are patients who are alive after 
treatment while samples labeled as class 0 are those 
who succumbed to their disease. Each sample contains 
7129 genes. There are 21 class 1 patients and 39 class 
0 patients in this dataset. We use 10 class 1 and 10 
class 0 patients as a training dataset, the remaining 
samples are used for testing. 
 The dataset ColonTumor contains 62 samples 
of two classes, colon-cancer and normal. Each sample 
has 2000 genes. They were computed from 40 colon-
cancer biopsies and 22 normal biopsies from tumors 
and healthy parts of the colons of the same patients, 
respectively. We use 11 cancer and 11 normal types as 
a training dataset, the remaining samples are used for 
testing. 
 The dataset DLBCL contains 47 samples of 
two classes, germinal centre B-like and activated B-like. 
Each sample has 4026 genes. There are 22 germinal 
centre B-like and 25 activated B-like types in this 
dataset. We use 11 germinal centre B-like and 11 
activated B-like types as a training dataset, the 
remaining samples are used for testing. 
 The dataset Prostate contains 136 samples of 
two classes, prostate tumor and normal. Each sample 
contains 12600 genes. There are 77 prostate tumor and 
59 normal samples in this dataset. We use 29 tumor 
and 29 normal as a training dataset, the remaining 
samples are used for testing. 
 
5. Experimental results 
 
      We apply SCAD SVM and WEPO to find out the 
important genes and used these genes as the features 
for SVM and SOM. Our experiments are running on a 
windows based system with Pentium4 3.00GHz CPU. 
SCAD SVM screens 22 features for ALL-AML_ 
Leukemia, 41 features for CentralNervous, 12 features 
for ColonTumor, 22 features for DLBCL, 21 features 
for Prostate, and 46 features for HCC55. Since WEPO 
just ranks features by their scores, we take the features 
which have minimum scores in each dataset, and the 
number of token features of each dataset is the same as 
the number of features selected by SCAD SVM. 
Figure 2 shows the performance of classifying the 
microarray data using SVM and SOM. We use the 
suitable kernel for each dataset in SVM while the 
proper lattices are used in SOM.  
Except for Prostate, SVM performs better than 
SOM when using the genes selected by WEPO as 
features. For all the datasets, SVM outperforms SOM if 
we use the genes selected by SCAD SVM. 
 
Figure 2 The performance of SVM and SOM with the 
two different gene selection methods 
 
6. Conclusion  
 
We can find that SVM has a stable and great 
performance of classifying microarray data. Except for 
Prostate, SOM also has great classification rate. 
For the two datasets CentralNervous and Prostate, 
WEPO seems not able to select the discriminative 
features for classification. We find that the WEPO 
scores of these two datasets are very high. This is the 
probable reason which affects the efficacy of WEPO. 
Although WEPO and SOM have nice performance 
for gene selection and classification, respectively, SVM 
with linear kernel and features selected by SCAD SVM 
is the best composition for most microarray datasets.  
 
7. References 
 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                                 95年 08月 25日 
報告人姓名: 陳朝欽           服務機構: 國立清華大學      職稱: 教授 
會議時間:   95年 08月 21~24日   地點:  Hong Kong, China 
 
本會核定補助編號: 95-2221-E-007-222 
會議名稱: The 18th International Conference on Pattern Recognition 
發表論文題目: A Comparison of Texture Features Based on SVM and SOM 
一 、參加會議經過: 95年 08月 20日 14:00搭乘長榮航空 BR0855班機於 20日
當地 15:40 pm抵 Hong Kong Airport，轉乘機場快捷到九龍車站連接巴士進
住 Kowloon Hotel。8月 21日八點半報到隨即參加開幕式，大會共同主席
Professor Yuan Yan Tang報告本次會議共有 1071人報名參加，含一般學者
專家 620人及學生 451人，打破歷年來的記錄。緊接著，由 K.S. Fu award 
receiver Professor J. Kittler演講 ”On Context, Modelling, Dimensionality, and 
Small Sample Size in Pattern Recognition”。會議共分成五個 Tracks及一個
Poster Session同時進行，每日上午一段，下午兩段，而每天上午 8~9點都
邀請一位專家演講相關子領域的 History, Present, Perspective，其中由密西
根州立大學 Professor A.K. Jain有關指紋認證的應用演說最吸引人。我們的
論文安排於 22日 13:30-15:10貼海報，引起數位學者的好奇與討論。並參
加 Conference Reception and Banquet和其它國內外學者專家討論教學、
Researching Funding、Research Topics, and etc.。本次會議約有超過 15位台
灣來的教授和數位研究生參加，大家都覺得受益良多。由於香港離台灣很
近，一方面沒有時差，另一方面，文化、語言環境也接近，經費能較有效
使用。8月 24日 13:45搭乘 BR0868於 15:30 pm返抵台灣。 
二、與會心得: 本會議約有來自 55個國家近 1100位學者專家參加，是個大型會 
議由 Baptist University of Hong Kong主辦，台灣幾個主要大學及中國許多學 
者都出席了，幾乎所有活動全照議程準時進行，值得國內參考，而且能就近 
和學者專家討論勝過苦讀 10年書。最後，竭誠感謝國科會的經費補助。 
三、考察參觀活動: 無。 
四、建議: 國科會編列經費鼓勵部份台灣的中小型(100~200人)國內會議國際化 
以提升知名度。政府積極編列適當經費鼓勵學者多參加重要國際會議 (縱使 
沒有發表論文) 以提升台灣學術水準與知名度。 
五、攜回資料名稱及內容: DVD for the contents of papers包含近 1000篇論文。 
六、其他: 無。 
 
聯絡電話: (Tel/Fax) (03) 573-1078 / 572-3694      E-mail: cchen@cs.nthu.edu.tw  
簽名蓋章: 陳朝欽                            日期:  95年 8月 25日 
so there are twenty features derived from a 3-scale 
wavelet transform. The matrices corresponding to the 
four wavelet transforms are given as follows. 
Figure 1.  A 3-scale wavelet transform.
2.2.1. Haar Wavelet Transform:  The Haar transform 
is the simplest type of the wavelet transforms. In brief, 
the concept of Haar wavelet can be explained by 
moving average and moving difference whose matrix 
form is given below. 
Haar
1 1 0 0
2 2
1 10 0 0
2 2
1 10 0
2 2
1 1 0 0
2 2
1 10 0 0
2 2
1 10 0
2 2
W
  ¯
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °¢ ±



=
"
"
# % #
"
"
# % #
.                 (4)
2.2.2. Daub4 Wavelet Transform:  As in the Haar 
transform, the Daub4 transform is expressed as 
D aub4
0 1 2 3
0 1 2 3
2 3 0 1
3 2 1 0
3 2 1 0
1 0 3 2
0 0
0 0 0
0 0
0 0
0 0 0
c c c c
c c c c
c c c c
W
c c c c
c c c c
c c c c
  ¯
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °
¡ °¡ °¢ ±
=  
 
 
"
"
%
"
"
"
%
"
(5)
where
1 3 3 3 3 3
0 1 24 2 4 2 4 2
, , ,c c c     and 1 33 4 2c
 .
2.2.3. 5/3 Wavelet Transform: The 5/3 wavelet 
transform is a reversible transform for lossless image 
data compression in JPEG2000 [11] whose matrix 
form is given in (6). 
2.2.4. 9/7 Wavelet Transform: Besides reversible 5/3 
wavelet transform, a floating bi-orthogonal 9/7 wavelet 
transform, is presented to compress images more 
efficiently. The forward 9/7 wavelet transform is given 
in (7). Based on (3), the texture features can be derived 
by using the 9/7 wavelet matrix. 
5 3
3 1 1 1 10 0
4 4 8 8 4
31 1 1 1 0 0
8 4 4 4 8
31 1 1 10 0
8 8 4 4 4
1 11 0 0
2 2
1 10 1 0 0
2 2
1 10 0 1
2 2
W
  ¯ ¡ °
¡ °
¡ ° ¡ °
¡ °
¡ °
¡ °
¡ °
¡ ° ¡ °
¡ ° ¡ °
¡ °
¡ ° ¡ °
¡ °
¡ °
¡ °
¡ °
¡ ° ¡ °¡ °¢ ±
=
" "
# % #
# % #
"
"
# "
# % #
# %
" "
(6)
97
0 00 1 2 3 4 4 3 2 1
0 02 1 0 1 2 3 4 4 3
0 02 3 4 4 3 2 1 0 1
0 00 1 2 3 4 2 1
0 02 1 0 1 2 3 4
0 02 3 4 2 1 0 1
W
D D D D D D D D D
D D D D D D D D D
D D D D D D D D D
E E E E E E E
E E E E E E E
E E E E E E E
   
  
   
 
 
 
ª º
« »
« »
« »
« »
« »
« »
« »
« »
« »
« »
« »
¬ ¼
 
" "
" "
# " % % #
# " % % #
" "
" "
" "
# % % #
# % % #
" "
(7)
where the coefficients for the W97 are given in Table 1. 
Table 1. The coefficients of 9/7 wavelet transform.
2.3. Features derived from Gabor and wavelets 
To implement a Gabor transform on an image X, 
we need to compute inverseFT(FT(X)8FT(G)), where 
8 is a pointwise operation and FT represents Fourier 
transform. For other wavelet transforms, a matrix 
computation WXWt is applied. For either Gabor or 
other four wavelet transforms, the mean and standard 
deviation of corresponding transformed coefficients in 
each subband are computed as features. 
3. Texture classification
3.1. Support Vector Machine (SVM)
Support Vector Machine (SVM) investigated by 
Vapnik has recently been proposed as new machine 
learning system based on statistical learning theory [6]. 
SVM designs the classifier functions by constructing 
hyperplanes in a multidimensional space that separate 
different categories of the training data. The main idea 
is to build the hyperplanes as the decision boundaries 
by using the fitting kernel such as radial basis function 
(RBF), polynomial or linear classifiers [6] [5]. Then, 
the hyperplanes try to split the positive examples from 
the negative examples and maximize the distance of 
the marginal separation between classes. 
In practice, there are various algorithms to train 
SVM. We adopt the Platt’s SMO algorithm to train the 
input data in our experiments [7] [8]. For multi-class 
classification, the one-against-one approach is chosen 
to classify the test data. 
0-7695-2521-0/06/$20.00 (c) 2006 IEEE
