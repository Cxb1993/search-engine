Minimizing Migration on Grid Environments: an Experience
on Sun Grid Engine
Yuan-Jin Wen and Sheng-De Wang
Department of Eletrical Engineering, Division of Computer Science,
National Taiwan University, Taipei, Taiwan
{d95921014,sdwang}@ntu.edu.tw
ABSTRACT
In distributed systems, the techniques of migration have been extensively studied by simulation, implementation,
and successful results were accepted in most schedulers for grid computing. In eﬀect, migration is capable of
improving the load balancing, reliability, and QoS of distributed systems. However, migration can also be
accompanied by negative eﬀects. It not only consumes a considerable amount of resource, but also comes with
migration failure. In this paper, we present some migration decision strategies and experimental results on job
migration. Through diﬀerent policies, it makes decision to select and migrate to the next available node. In
this paper, we propose a lookahead strategy and a Poisson process strategy to improve the performance of the
decision-making. The target node is selected such that it may provide a high availability environment and thus
may ﬁnish the execution with high probability.
Keywords: Migration, Load balancing, reliability, QoS, lookahead strategy, Poisson process.
1. INTRODUCTION
The deﬁnition of migration is the movement of process [1] , job [2], data, method, or service [3] from one node to
another. Nodes can be a computer, a notebook, a work station, or even embedded devices such as PDAs, mobile
phones, PSPs(Play Station Portable). Nowadays, the techniques of process migration and job migration are
very mature. Process migration means transfering one running process from one machine to another. There are
several implementations that were built on diﬀerent operating systems. For example, MOSIX(Barak and Litman,
1985), Sprite(Ousterhout et al., 1988), Mash(Accetta et al., 1986), openMOSIX(Moshe Bar, 2002). There are
some features [4] that make MOSIX(Multicomputer Operating System for UNIX) an important tool. First, a
MOSIX grid is able to be divided into virtual clusters. Second, resources, such as memory usage and availability,
CPU speeds, and system loads can be discovered automatically. Third, the process migration is done by copying
the image of memory of the process. Forth, it checkpoints a process by saving the image of memory of the
process into a ﬁle. In addition to these advantages, it also supports load balancing in the migration mechanism.
On the other hand, openMOSIX [5] is as a branch of MOSIX, which is developed under GPL. openMOSIX also
has a very friendly GUI.
Sun Grid Engine is one of the most famous distributed resource management systems in the recent years.
It has a lot of features. First, Sun Grid Engine provides users to submit batch jobs, interactive jobs, and parallel
jobs to the Sun Grid Engine system. There are two methods to submit jobs, to submit from the command line
and the Graphical User Interface, QMON. When users submit jobs, Sun Grid Engine also provides various kind
of parameters, such as jobs priority, execution time, shell selection, standard output, resource request, parallel
environment, checkpoint mechanism, email the situations of these jobs to users, and assign jobs to the queue of
the execution host. Second, Sun Grid Engine supports checkpointing programs mechanism. Checkpointing [6]
is a process that writing the data and state information of a job on execution to a ﬁle or storage device. Once
if an execution node has failed or crashed, checkpoing Sun Grid Engine jobs on this failed execution node are
aborted and migrated to other available execution nodes in Sun Grid Engine pool. To select the migrated target
Selection. Third, the Schedule generation step involves that the AppLeS agent will generate a set of schedule for
the assigned applications and programs to execute on the selected available resources. Fourth, after generating
a set of suitable schedules, the AppLeS agent must apply some scheduling algorithm to ﬁlter some schedules and
select the best one of all generated schedules. The ﬁfth step is about the application execution. After adopting
the best schedule, we can execute these pending jobs on the target machines. Finally, because of considering
the dynamic characteristics of the availability, priority, and exhaustion of the resources, the targeting reosurces
may encounter variation than used before. It means what we need to adapt the content of the selected schedule
which is the Schedule Adaptation. In the AppLes, our proposed algorithm and strategies may be used in the
step 3, the schedule generation, to promote the performance and reduce migrations.
MARS [17](Michigan Advanced Resource Scheduler) is an implementation that is a metascheduling frame-
work to schedule on-demand tasks and jobs with priority. The goal of MARS is providing the information of
resource and schedules tasks according to the infomation. MARS makes ouse of resource forecasting to estimate
the availability of CPU and the utilization of other resources. In MARS, the migration mechanism may help
jobs to ﬁnd other available resource instead of just pending the job.
2. THE GRID ENVIRONMENT AND THE MIGRATION MECHANISM
2.1. Three diﬀerent environment models
There are three diﬀerent environment models will be discussed in the following experiments. An environment
model characterises the availability of nodes in a grid envrionment.
Chaos environment model
A chaos environment model has the most similarity to the execution nodes in the real world. It is primarily
simulated to model a large grid environment, for example, Asia, Europe, etc. In the chaos environment model, all
available probability generated uniformly. Figure 1(a) illustrates a scenario of chaos environment model, where
it shows the available probabilities of nodes with respect to the time. We can see that the availability of nodes
is quite uniform as time goes.
Figure 1. (a). Chaos environment (b). High available environment (c). Low available environment
High availability environment model
The high availability environment model is used to simulate the execution nodes in the region that is usually
on, which means these nodes are usually available for hosts or users forn other sites in whole day long. It is
3.1. Most available strategy
If one job is executing on node A, suddenly the node A is unavailable for the reason of power failure, shutdown,
or other unexpected factors. After that, we must migrate this job to other available nodes to re-run the job.
Intuitively, we may migrate the job to one of the available nodes that would be available all the time afterward.
According to the past record of these available nodes, we can choose the migrating target that has the property
as shown in Algorithm 1, where array avail_probability[] is used to keep the past record for each nodes.
Algorithm 1 MA-schedule
Migration Phase:
time_now = g_time;
Get avail_nodes set contains available nodes from Available Table;
Select the node i that avail_nodes[i].avail_probability[time_now + 1] is the largest value.
Migrate job A.ckpt to the node i.
Execution Phase:
For each node n
In the period of ckpt_time,
Execute checkpointing job A
return job A.ckpt.
3.2. Lookahead strategy
In the most available strategy, it is insuﬃcient to select only the one with highest available probability in the
next time slice. If the available probability of one node of the next time slice is low, but after the next time slice,
the available probability of the node may maintain high for the serveral time slices. In this case, the job may
suﬀer only one migration in the next time but have an opportunity to enjoy high available environment. On the
contrary, if the available probability of another node of the next time slice is the highest, but after the next time,
Algorithm 2 Lookahead strategy
Migration Phase:
time_now = g_time;
specify lookahead length;
specify an acceptable threshold;
do
do
Get avail_nodes set contains available nodes from Available Table;
Delete the available nodes that do not pass the speciﬁed acceptable threshold
Sort all available nodes incrementally;
Stack the all sorted available nodes and pop the node whose avail_nodes[i].avail_probability[time_now ++]
is the largest value;
until reaching a chain of the speciﬁed lookahead length
calculate the ﬁtness of the chain and record it
compare the ﬁtness whether if the chain is the best choice
util node_stack is empty
Migrate job A.ckpt to the node i that is the ﬁrst node of best chain;
Execution Phase:
For each node n
In the period of ckpt_time,
Execute checkpointing job A
return job A.ckpt.
the execution of the job. In the Poisson process, assume that migration happening is the speciﬁed event, and we
can use the calculation as follow:
Pr (T > t) = Pr (nomigration in time [0, t]) = Pr (X = 0) = e
−λt×(λt)0
0! = e
−λt
According to the result, we can extend to the algortithm 3.
4. EXPERIMENTS
In the section, we have a series of experiments which are based on diﬀerent environment models. The pro-
posed strategies will also be implemented and integrated into the Sun Grid Engine to see its impacts on the
migration times on diﬀerent environment model. There are three parts corresponding to the models, chaos envi-
ronment model experiment, high available environment model experiment, and low available environment model
experiment. Due to the page limit of the paper, we present here only the experimental results for the chaos
environment
Most available strategy v.s. SGE default strategy
Figure 2. Least-load strategy on Chaos Environment
The Sun Grid Engine has implemented a default migration strategy, called least-load strategy, where a migration
target is selected if its load is lightest. In Figure 2, we plot the various amount of jobs executed in the Sun Grid
Engine based on Chaos Environment. Figure 2(a) depicts the range of migration times to the execution jobs.
For example, the range of migration times to 10 execution jobs is from 229 to 303. And the range of migration
times to 100 execution jobs is from 2273 to 3225. Figure 2(b) shows the average migration times to diﬀerent
execution jobs. In the experiment, we are able to see the general performance of the least-load strategy which
is adopted on the Sun Grid Engine. Generally, the range of average migration times to the execution jobs with
least-load strategy is distributed from 22 to 32.
The range of migration times to the execution jobs and the average migration times to diﬀerent execution
Figure 4. Lookahead strategy on Chaos Environment
10 jobs (length, ac_prob) (5, 30%) (7, 30%) (10, 30%)
avg_migration times 181 168 172
migration overhead (secs) 3.97 6.41 6.07
dispatch latency (secs) 7.17 10.65 18.65
running algorithm range (secs) 0.83∼4.16 3.2∼8.31 3.95∼22.63
running algorithm range (secs) 1.1 0.7 0.8
Table 1. Diﬀerence of length of lookahead chain
5. CONCLUSION
In this paper, we show that diﬀerent policies can help the migration mechanism make decision to migrate jobs
to the next available node. Among the policies, we propose the lookahead strategy to improve the performance
of the decision-making. The target node is selected such that it may provide a high availability environment and
thus may ﬁnish the execution with high probability.We have seen that the lookahead strategy is a good choice
for chaos environment. With a longer chain, the prediction accuracy may be higher. However, a longer chain
means a larger computing loads for the underlying scheduler, such as Sun grid engine.
A Dependable Outbound Bandwidth Based
Approach for Peer to Peer Media Streaming
Zheng-Yi Huang and Sheng-De Wang
Department of Electrical Engineering
National Taiwan University, Taipei, TAIWAN
Email: sdwang@ntu.edu.tw
Abstract— A fundamental problem in peer-to-peer streaming
is how to select peers with desired media data so that the best
possible streaming quality can be maintained. In this paper,
we propose an outbound bandwidth based streaming model in
which peers are layered according to their oﬀered outbound
bandwidth and are permitted to request data of peers from upper
layers peers only. Based on the layered approach, a media data
assignment algorithm for the subset of media data is presented
to select qualified sending peers to ensure that they will be
received before their scheduled playback time. We also present
two resolutions for request conflicts, which arise when there are
more than one peer simultaneously requesting data from the
same sending peer that can’t aﬀord outbound bandwidth for all
requests. We evaluated the proposed streaming model through
simulations. Experimental results show that streaming quality of
the proposed streaming model is excellent and the properties of
scalability as well as robustness are obtained even in a highly
dynamic environment where peers join and leave frequently.
1. I
Recently, the peer-to-peer networking paradigm oﬀers the
alternative possibility for streaming media over the network
due to the most important inherent characteristic: resources are
shared among participants. Peers that simultaneously function
as both clients and servers share their resources (e.g., comput-
ing power, bandwidth, storages and contents) with others. This
important characteristic avoids dedicated replication servers
altogether and hence it does not have the bottleneck of
client/server streaming architecture. Unlike peer-to-peer file
sharing systems, a peer-to-peer media streaming system is
operated in a kind of play-while-downloading mode ([1]). It
is implied that the outbound bandwidth (upstream bandwidth)
that a peer can acquire is a crucial factor for streaming quality.
Another element of diversity is that the local storage of each
peer is leveraged as a cache-and-relay mechanism ([4]) in
which requesting peers request media data and cache the
most recently played media data during streaming. The cached
content can then be relayed to later peers that request the
same content. But the most distinct feature of all is that
from peer-to-peer streaming systems, users not only enjoy
the availability of media content but also the high quality of
media streaming. The media streaming quality depends on a
combination of factors, ranging from the characteristics of the
streaming sources (e.g., link capacity, availability, accessible
outbound bandwidth) to the characteristics of the network
paths (e.g., available bandwidth, packet loss rate, and overlap
of paths from multiple sources to receiver [7]). Thus the main
challenge is to design peer selection strategies to realize high
streaming quality. Much research has been done to optimize
solutions for streaming that aim at providing a minimal delay
time for buﬀering media data. We argue that such optimal
solutions are adequate only in some particular situations such
as the number of requesting peers is relatively less than the
number of sending peers. In addition, in order to get an optimal
solution, one trends to exhaust peers with powerful capability,
for example peers with high outbound bandwidth.
In this paper, we study the peers selection problem as
well as the media data assignment problem as in [1]. Given
requesting peers and sending peers (supplying peers) with
heterogeneous outbound bandwidth, we proposed to select
sending peers and assign a subset of media data to them
based on the outbound bandwidth layered hierarchy. Diﬀerent
from other optimal peer selection algorithms which result in
optimal solutions for an individual requesting peer, our goal is
to provide a high streaming quality for whole participants and
to maintain the fairness among peers according to their contri-
bution to the network. Also unlike other elaborate streaming
architectures, the control overhead of the proposed streaming
model is extremely low and it can apply to any existing
peer-to-peer overlay network. Briefly, peers participating in
the streaming network are layered according their oﬀered
outbound bandwidth to form a hierarchy in which they are
permitted to request data from peers in higher layers only. To
avoid peers from requesting data blindly and from accessing
resource greedily, a media data assignment algorithm is pre-
sented such that peers can get their required media data with
limited resource and media data can be received before their
scheduled playback time so that the streaming quality can be
maintained. Since there is limited capacity (hereafter we will
use outbound bandwidth and capacity alternately) of a sending
peer, request conflicts would occur frequently and inevitably. A
request conflict occurs when there are more than one peer that
simultaneously request data from the same sending peer that
can’t aﬀord enough capacity for all requirements. In this paper,
we propose two possible mechanisms for resolving request
conflicts, namely capacity sharing and capacity preemption
which enlarges the accessible capacity of a requesting peer and
gives peers chances to preempt other peers with lower priority,
respectively. Both of these mechanisms aim at maintaining
a best possible streaming quality for all participants in the
network. The features of the proposed outbound bandwidth
TABLE I
N T     Pi
Neighbors (Pi)
Peer Layer Layer Peer
P1 1 1 P1
P2 2 2 P2, P3
P3 2 3 P4
P4 3 - -
P5 4 - -
blind pull(request) or push(send). This can tend to result in
extremely communication overhead and will cause streaming
quality to be degraded significantly.
Each peer has its own layered hierarchy and the establishing
process is distributed with little overheads. In addition, the
layered hierarchy can be found on any existing peer-to-peer
overlay network. In this paper, however, we adopt the gossip-
based approach for the construction of the overlay since its
excellent robustness, scalability and reliability as evaluated in
detailed by Laurent Massouli et al. ([2]). Upon peer Pi setting
up its neighbor table (or partial view [2]), Pi constructs its
layer by information stored in its neighbor table and then
prepares its media streaming. For example, if neighbors of Pi
are as the left-half part of table I and if Pi is of layer four, then
its accessible set will be { (R1), (R2), (R3)}. Therefore, its
outbound bandwidth based layer will be the right-half part
of table I. It is evident that the layer constructing process is
distributed and the additional overhead is to exchange layer
information with the neighbors. And due to duplicated peers
of each layer, the layered streaming model will be reliable
even in the present of peer failures.
B. Media Segments Assignment for Layers
Unlike other segments assignment research, the problem
we study is how to assign media segments to layers such
that segments can be received before their scheduled playback
time, given an admissible buﬀering time of a session and its
size. An admissible buﬀering time is the maximum time a
peer is allowed to request and receive segments for a session
to such an extent that each segments will be cached on time.
To prevent peers from requesting peers from particular
layers arbitrarily and greedily, a maximum accessible peers for
the layers are defined for each session. Algorithm 1 defines
the maximum peers for each layer that a requesting peer
Pi can request, namely (Pi), where σ, called outbound
degree, defines the scope of claimable outbound bandwidth.
It is obvious that if Pi ∈ (R), then | (Pi)| = σ · .
For example, if Pi ∈ (R4) and σ = 1, then | (Pi)|=4,
i.e. (Pi)={ (R1), (R2), (R3), (R3)}. That is to say, Pi
can use 1, 1 and 2 peers from layer 1, layer 2, and layer
3 respectively during a streaming process. And their total
outbound bandwidth is R0, of course.
Algorithm 2, executed by requesting peers, assigns media
segments to layers. Its intrinsic idea is very intuitive and
simple: just make on time segment by segment. Giving the
size of a session, the assignment algorithm selects layers for
1: procedure AP
2: total ← 0;
3: (Pi)← ∅;
4: while total < σ do
5:  ← round robin select a layer in [LPi − 1 , 1];
6: if total + R ≤ σ then
7: let Pc be a peer ∈ (R);
8: (Pi)← (Pi) ∪ Pc;
9: total ← total + R;
10: end if
11: if (total MOD 1)=0 then
12: set selected index in [LPi − 1 , 1] to LPi − 1;
13: end if
14: end while
15: end procedure
Algorithm 1: maximum accessible peers
segments such that they pass for being received before their
scheduled playback time in the view of the requesting peers.
In the algorithm, there are two auxiliary values associated for
a suppling peer. δPs is the time it takes Ps to deliver a segment.
If Ps ∈ (Rr) then δPs = 2rT0. φPs indicates the next free time
of Ps and is maintained by requesting peer locally. By locally,
we mean that it won’t be influenced or considering requesting
statuses of other peers. However, it also means that a local free
status might not match the actual free status of the supplying
peer. We will propose mechanisms to fix this problem in the
next session. Initially, δPs is zero, if a segment is assigned to
sending peer Ps, then the local next free time of Ps will be
updated to current δPs + φPs .
1: procedure A(size)  the size of sessions
2: if size = 1 then
3: S egmentsize−1 ← CANDIDATE(0);
4: else
5: Assign(size − 1);
6: S egmentsize−1 ← CANDIDATE(SIZE-1);
7: end if
8: end procedure
Algorithm 2: Assigning Layers for Session
1: procedure C(seq)  the sequence of segment
2: τ← τseq,s + seq;  the scheduled playback time
3: for all Pc ∈ (Pi) do
4: select a Pc if δPs + φPs ≤ τ
5: end for
6: φPc ← δPc + φPc ;
7: return LPc  return the layer of selected peer
8: end procedure
Algorithm 3: Selecting Candidate Layer
When there are more than one candidate layers for a
segment, a decision must be made to select one for it.
The following are four possible selection strategies for this
situation.
• Lowest Layer First: to select a lowest layer from multiple
candidate layers. The delivering time of this layer is the
to do is to notify sharable peers with little communication
overhead. It won’t lengthen delivering time for sending peers
nor shorten it. The benefit of capacity sharing is the reduction
of opportunities being rejected while peers issue requests. But
the major drawback of this is the reduction of network capacity
implicitly since more are capacities being consumed in order
to share segments with each other.
2) Capacity Preemption: In the capacity preemption mech-
anism, priority is assigned to each peer according to which
layer it locates. Peers in higher layers deserve higher priority
to access resources. Upon receiving a request from Pi, Ps
determines whether there are preemptable peers based on the
following procedure. Preempted peers will lose their required
1: procedure P(Pi)
2: for all Pj ∈ do
3: select one Pj such that LP j < LPi ;
4: end for
5: if such Pj exists then
6: Pi preempts Pj;
7: end if
8: end procedure
Algorithm 6: Capacity preemption
segments requested from Ps and of course their streaming
quality will degrade. However, from experimental results in
section 5, we will see that capacity preemption is the decisive
factor which maintains a best streaming quality for whole
participants in our layered streaming model.
B. On outbound bandwidth based peer-to-peer streaming
When peer Pi joins to a media streaming network, it
establishes its neighbor membership by the underlined peer-
to-peer overlay protocols. In the meantime, Pi exchanges the
information of it’s located layer with its neighbors to build
its own outbound bandwidth based layer. Upon receiving the
layer information of Pi, peers should update their layers to
reflect the eﬀect that Pi will contribute and consume capacity
for the network. Before the playback of the media data, Pi
must determine which segment should be its initial playback
segment ςPi suggested by the following equations, assuming
that LPi = .
max(ςPk ) < ςPi < min(ςPj ) ∀Pk ∈ (R+1),∀Pj ∈ (R−1)
The reason why the initial playback segment of Pi is not as
live as source peers is that since Pi will become a sending peer
who sends segments to others located in lower layers, it also
will be a requesting peer to request segments from peers of
higher layers. Therefore, the initial playback segment should
not be too new otherwise there will be few peers or none can
satisfy its requirement and it should not be too old or it can’t
provide any segments for peers in lower layers since their
desired segments will be newer than cached by Pi. Thus, a
eclectic strategy for determining a initial playback segment is
to choose a segment with sequence less than the least sequence
of segments among it’s neighbors of an immediate upper layer
and greater than the largest sequence of a segment among it’s
6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
1
2
3
Sequence
L
ay
er
s
Fig. 1. The best initial segment of Pi
6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
1  
2  
3  
Sequence
L
ay
er
s
Fig. 2. cached segments after 4 T0
neighbors of an immediate lower layer. For example, if the
new joining peer Pi belongs to (R2), and the largest initial
segment of its immediate low layer is 8, and the least initial
segment of its immediate up layer is 16, then the best initial
segment of Pi will be 12 as shown in Fig 1 assuming that
peers in layer 3 requested a four-size session. Peers in layer
3 who request their sessions initialed from segment 8, won’t
request data from Pi, since when they schedule their request
targets (during play segments from 4 to 7), Pi does not exist in
their neighbor table, and therefore it will avoid quality degrade
since Pi still does not have enough data. But in Fig 2, peers
in layer 3 may and can request segments from Pi since Pi
had cached their desired segment in its cache. Consider the
relations between Pi and its immediate up layer, layer 1. If Pi
requests a session with initial segment larger than 12, then Pi
won’t get its required data since none have cached them.
After initializations described above, Pi can perform stream-
ing processes and will act as both a requesting peer and a
sending peer by performing procedure 7 and 8, respectively.
Pi executes the requesting procedure for buﬀering next session
during playback of current session. As described in the previ-
ous section, Pi issues requests relying on its local free statuses
of sending peers. In case of having the required segments
not being received which are requested from some sending
peer, says Ps′ , Pi will record Ps′ as an ill-reputation sending
peer and it will try to select a peer with a best reputation
for requesting next time. This selection strategy makes our
1 2 3 4 5
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
Layer
Pr
ee
m
pt
ive
 R
at
io
Fig. 4. Preemption ratio of each layer
with Strategy 4, Strategy 2 has an impressive improvement.
The reason is that, in our streaming model, peers are allowed
requesting media segments only from peers in a higher layer,
and there will be a chain of segments lost in the case of
peers in a higher layer that could not acquire their desired
media segments. This situation will cause streaming quality to
degrade significantly if request conflict takes place frequently
and peers in a higher layer does not have higher priority to
access resources. Although preemption will also bring about
streaming quality to degrade in the preempted peers, the
proportion of preemption is slight as show in Fig. 4. To go
a step further in discussion the reason why the proportion of
preemption is slight is that the buﬀering time and delay time of
peers is basing on layers of peers, peers in a lower layer will
have longer buﬀering time and therefore delay time, which
means that they can request media segments from peers in
”lower layer” (but higher than themselves) and it will reduce
opportunities been preempted. For this reason, the preemptive
mechanism improves streaming quality considerably. Strategy
3 also improves streaming quality although its improvement
is not as a good deal as Strategy 2. Since when performing
a sharing process, there are at least three peers involving in
one media segments, say one sending peer and two requesting
peers, it will consume the capacity of the streaming network
indirectly. Moreover, if there are a lot of sharing processes
in a higher layer, it will reduce the ability to service peers
in lower layers. It is evident that ignoring the resource con-
flict phenomenon is not a good idea because of its terrible
streaming quality.
2) The Impact of Outbound Degree: The outbound degree
defines the accessible scope a peer can access for one session.
That is, it is the number of peers involved during streaming
in one session. The larger the outbound degree, the more the
peers will be involved. We would like to know what is the most
suitable outbound degree for a streaming system and how it is
related to the streaming quality. Will a larger outbound degree
imply a higher streaming quality? To understand these ques-
tions, we ran experiments of diﬀerent outbound degrees with
network sizes of 10,000 peers and 20,000 peers respectively.
Fig. 5 gives the answers. If all participants of the network grab
outbound bandwidth greedily, that is, with a large possible
1 1.5 2 2.5 3 3.5 4 4.5
0.65
0.7
0.75
0.8
0.85
0.9
Outbound Bandwidth Degree
St
re
am
in
g
Q
ua
lit
y
 
 
size=10000
size=20000
Fig. 5. The Eﬀect of outbound degree
outbound degree, the result is that there will be lots of request
conflicts and that will exhaust the capacities of peers. Thus the
streaming quality will degrade rapidly. Although peers with
smaller outbound degrees will limit their accessible scope,
it is indeed beneficial to the streaming quality of the whole
streaming network, since it also reduces request conflicts. But
with an exact outbound degree, it will also limit the accessible
scope of peers which leads to limiting the ability of peers to
handle request conflict and ill-reputation peers. That is why
the smallest outbound degree can’t bring the highest streaming
quality.
3) Comparison of Layer Selection Strategies: In this set
of experiments, we investigate the impact of layers selecting
strategies. A layer selecting process will be performed when
there is more than one candidate layer for a requested segment.
Layer selection strategies been compared are Lowest Layer
First(LLF), Highest Layer First(HLF), Random, and Least Re-
cently Assigned(LRA), they all relate to outbound bandwidth
only and we consider neither end-to-end delay nor underlining
topology. As we can see from Fig. 6 the LRA layers selection
strategy has the best streaming quality among all strategies and
the LLF strategy has the worst streaming quality. The reason
is that the LRA strategy distributes the consumed capacity
loading among the accessible layers of a request peers and
hence it avoids exhausting the resource of particular layers.
Both LLF and HLF tend to consume capacity of the highest
layer and lowest layer respectively and therefore they won’t
have a good streaming quality.
B. Scalability and Resilience to Peer Failures
Attractive features of the proposed streaming model are its
scalability to network growing and robustness to peer failures.
It is self-growing since requesting peers joining later will
become sending peers; therefore the streaming network’s total
capacity will be amplified; it is self-adapting because detected
failures of peers will cause requesting peers to switch their
request target to seek more stable sources. These features make
for a high and stable streaming quality in highly dynamic
environments where peers disconnect or connect arbitrarily
and frequently. The goal of this set of experiments is to attest
to scalability and robustness of our streaming model.
