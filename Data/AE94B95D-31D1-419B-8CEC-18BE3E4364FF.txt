 2 
Unfortunately, since no classifiers may be 
perfectly tuned, estimation of DOA values does 
not always be proper. A document that is 
similar to a category does not always get a 
higher DOA value with respect to the category. 
Similarly, a document that is not similar to a 
category does not always get a lower DOA 
value with respect to the category. Improper 
DOA estimations may heavily deteriorate the 
performance of both filtering and classification. 
In this project, we explore how various 
classifiers’ performances in text filtering and 
classification may be improved by selecting and 
integrating more suitable features (keywords) to 
distinguish relevant documents from irrelevant 
documents for each category. This goal differs 
from many previous attempts, which often 
aimed at improving the processes of classifier 
building, threshold tuning, and document 
selection. The research result of the project may 
complement the previous techniques. 
Feature selection was often an 
experimental issue in previous studies, although 
there were many feature selection techniques 
developed. There were also studies that 
maintained an evolvable feature set covering all 
features currently seen, although inappropriate 
features may introduce inefficiency and poorer 
performance in text classification. Therefore, 
there was no standard guideline to construct a 
perfect feature set. A feature set was often 
determined by an experimental tuning process. 
However, even a feature set may be 
perfectly tuned to distinguish the categories, it 
is not necessarily suitable to filter out those 
documents not belonging to the categories. This 
problem is due to the common goal of previous 
feature selection techniques: selecting those 
features that may be used to distinguish a 
category from others. Under such a goal, 
whether a feature may be selected mainly 
depends on the content relatedness among the 
categories, without paying much attention to 
how the contents of a category c and a 
document d over-lap with each other. If d 
contains much information not in c or vice versa, 
d should not be classified into c, even though d 
mentions some content of c. Similar problems 
may also be found in similarity measurement 
between two documents: two documents should 
not be similar to each other if they have a lot of 
different contents, no matter how the feature set 
is tuned using training documents. This problem 
motivates the research in the project. 
To tackle the problem, features should be 
dynamically selected in response to each 
individual input document (rather than training 
documents in the categories). The features may 
help to measure the extent to which the content 
of the document overlaps with that of each 
category. They are helpful when a document of 
a category does not employ a terminology 
different from that employed by training 
documents of the category. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
3. Result and Discussion 
?? Based on the above analysis, we develop a 
dynamic profiling technique DP4FC (Dynamic 
Profiling for Filtering Classification) to promote 
various classifiers’ performances in text filtering 
and classification. DP4FC is also empirically 
evaluated under various environments.  
 
3.1 Technical development 
In training, DP4FC joins the thresholding 
process, while in testing, DP4FC joins the 
process of making filtering and classification 
decisions. Both the underlying classifier and 
DP4FC estimate each document’s DOA with 
respect to each category. The key point is that 
DOA values estimated by DP4FC are based on 
dynamic profiling, which aims to measure the 
extent to which a document’s content overlaps 
that of a category. 
Table 1. DOA estimation by dynamic profiling 
Procedure DOAEstimationByDP(c, d), where  
(1) c is a category, 
(2) d is a document for thresholding or testing 
Return: DOA value of d with respect to c 
Begin 
(1) DOAbyDP = 0; 
(2) For each term t that is positively correlated with 
c but does not appear in d, do 
(2.1) DOAReduction = 2(t,c); 
(2.2) DOAbyDP = DOAbyDP - DOAReduction; 
(3) For each term t that is negatively correlated with 
c but appears in d, do 
(3.1) DOAReduction = (number of occurrences of 
t in d)  2(t,c); 
(3.2) DOAbyDP = DOAbyDP - DOAReduction; 
(4) Return DOAbyDP; 
End. 
 4 
former is used to build the classifier, while the 
latter is used to tune a threshold for each 
category. Therefore, to guarantee that each 
category has at least one document for classifier 
building and one document for threshold tuning, 
we remove those categories that had fewer than 
2 training documents, and hence 95 categories 
remain. Among the 95 categories, 12 categories 
have no test documents. From both theoretical 
and practical standpoints, these categories 
deserve investigation, although they were 
excluded by several previous studies. After 
re-moving those documents to which no 
categories are assigned (i.e. not belonging to 
any of the 95 categories), the training set 
contains 7780 documents. Moreover, since 
previous studies did not suggest the way of 
setting the documents for classifier building and 
threshold tuning, we try different settings to 
conduct thorough investigation: 50%-50% and 
80%-20%, which conduct 2-fold and 5-fold 
cross validation, respectively. In the 2-fold 
cross validation, 50% of the data is used for 
classifier building, and the remaining 50% of the 
data is used for threshold tuning, and the 
process repeats 2 times so that each training 
document is used for threshold tuning exactly 
one time. Similarly, in the 5-fold cross 
validation, 80% of the data is used for classifier 
building, and the remaining 20% of the data is 
used for threshold tuning, and the process 
repeats 5 times. 
Moreover, to test those out-space 
documents that are less related to the categories, 
we randomly sample 370 documents from a text 
hierarchy that was extracted from Yahoo! 
(http://www.yahoo.com) and employed by 
previous studies. The documents are randomly 
extracted from the categories of science, 
computers and Internet, and society and culture, 
and hence are less related to the content of the 
Reuters categories. With the help of the Yahoo 
out-space documents, we may measure the 
system’s text filtering performance in 
processing those out-space documents with 
different degrees of relatedness with respect to 
the categories. 
 
3.2.2 Evaluation criteria 
The classification of in-space test 
documents and the filtering of out-space test 
documents require different evaluation criteria. 
For the former, we employ precision (P) and 
recall (R). Both P and R are common evaluation 
criteria in previous studies (Lewis, 1995; Yang, 
2001). To integrate P and R into a single 
measure, we employ F1 = 2PR / (P+R), which 
places the same emphasis on P and R. 
As in many previous studies, P, R, and F1 
have two versions: the micro-averaged version 
and the macro-averaged version. The 
micro-average version tends to view all 
categories as a system, and hence estimates P by 
[total number of correct classifications / total 
number of classifications made], and R by [total 
number of correct classifications / total number 
of correct classifications that should be made]. 
On the other hand, the macro-averaged version 
tends to view each individual category as a 
system, and hence estimates P, R, and F1 by the 
average of the P, R, and F1 values on individual 
categories, respectively. When computing the 
macro-average values, we exclude incomputable 
values (i.e. those whose denominators are zero). 
On the other hand, to evaluate the filtering 
of out-space test documents, we employ 
misclassification ratio (MR), which is estimated 
by [number of misclassifications for the 
out-space documents / number of the out-space 
documents]. A system should avoid 
misclassifying out-space documents into many 
categories (i.e. lower MR). 
 
3.2.3 Underlying classifiers 
Each category c is associated with a 
classifier. Upon receiving a document d, the 
classifier estimates the similarity between d and 
c (i.e. DOA of d with respect to c) in order to 
make a binary decision for d: accepting d or 
rejecting d. To investigate the contributions of 
DP4FC to different kinds of classification 
methodologies, DP4FC is applied to two kinds 
of classifiers: the Rocchio classifier (RO, which 
is based on vector-based methodology) and the 
Naive Bayes classifier (NB, which is based on 
probability-based methodology). 
All the classifiers require a fixed 
(predefined) feature set, which is built using the 
documents for classifier building. Each term 
that is not a stop word may be a candidate 
