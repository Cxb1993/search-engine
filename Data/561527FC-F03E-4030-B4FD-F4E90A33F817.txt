 2 
parameters to match the training patterns.  
A neural network contains many processing 
neurons such that a higher degree of 
robustness and fault tolerance becomes 
possible. 
A three-layer feedforward neural 
network with sufficient neurons in the 
hidden layer has been shown to have the 
function approximation capabilities to an 
arbitrary degree of accuracy.  The 
backpropagation algorithm has been popular 
in supervised training of multilayer 
feedforward neural network.   The 
algorithm is in essence a steepest descent 
method, which suffers from converging 
slowly and sticking easily in local minima 
of the error function.  Many variations of 
training algorithm have been developed.   
Previous studies are mainly in two 
categories.  The first aims at developing ad 
hoc techniques such as variable the learning 
rate, momentum and rescaling variables to 
improve the training speed, while another 
focuses on the optimization techniques.  In 
these training algorithms, the initial weight 
and bias are known to be critical to network 
training, and hence learning performance.    
An adaptive genetic algorithm is developed 
in this work to find the global minimum of 
the error function of a feedforward neural 
network.  To accelerate the training process, 
however, a modified Newton algorithm is 
also developed for efficient training.   The 
two-stage algorithm is to integrate the 
adaptive genetic algorithm in the first stage 
for better initial weight/bias and the 
modified Newton algorithm with the 
specific initial weight/bias in the second 
stage for efficient training. 
(5)6 76 76 76 7 4
Artificial neural network (ANN) is an 
information processing system with a large 
number of simple connection neurons to 
imitate the biological neural network.  The 
relationship between the input and output 
pattern of an artificial neuron can be 
represented by  
∑ −=
i
jiijj xwn θ  (1) 
 
(2) 
where ix  is the i
th
 input of the processing 
element; jθ is the bias representing the 
threshold of the transfer function; ijw  is the 
connection weight between ith input and jth 
output for imitating the biological synapse 
strength; jn  is the jth linear combination 
output; jy  is the jth output of the 
processing element; and f  is the transfer 
function (or the activity function) for 
converting the weighted summation of the 
input to the output.  The connection weight 
of each connection represents the relative 
strength between two artificial neurons. 
The training process is to adjust 
repeatedly the connection weights of the 
multi-layer neural network for minimizing 
the error function pE  defined by 
           )(  jj nfy =
 4 
( ) ( ) ( ) ( ) ( )( 1) ( ) (1 )( ) ( ) ( ) ( )
( ) ( ) ( ) ( ) ( ) ( )
                    ( ) ( )
T T
T T
T T
T
q k k q k p k p kk k
p k q k p k q k
p k q k k k q k p k
p k q k
+ = + +
+
−
BB B
B B
algorithm is to find the initial weight and 
bias by the adaptive genetic algorithm with 
the chromosomes represented by floating 
number.  The reproduction, crossover, and 
mutation operations are selected adaptively 
in each generation. The multi-directional 
search of genetic algorithm usually takes 
lengthy training time, however, the 
algorithm can be used in the early stage of 
training to detect the region where the 
global minimum is likely to be found for 
good starting point to efficient learning. 
Conventional backpropagation 
algorithm and conjugate gradient algorithm 
are the first-order training algorithm for they 
require only the first derivatives of the error 
function.  Among the commonly seen 
learning algorithms, the so called second 
order algorithm such as the Newton’s 
algorithm and Levenberg-Marquardt 
algorithm are considered to be more 
efficient in simple neural network involving 
a relatively small number of weight and bias.  
The Broyden-Fletcher-Goldfarb-Shanno 
(BFGS) algorithm in numerical analysis can 
be modified as follows for application in 
neural network training.  Start with the 
initial weights, bias and an initial estimate 
of the inverse of the Hessian matrix of the 
error function.  In the absence of additional 
information, initial is taken as the identity 
matrix to compute the gradient vector 
))(( kwE ij∇ . Define the update rule as 
     ))( ()(η)1( kwEkkw ijij ∇−=+∆ B  (4)                                  
where η  is the network learning rate. 
Update the inverse of the Hessian matrix by 
 
Where, 
    ),()1()( kwkwkp ijij −+=       (6)                             
))(( ))1( ( )( kwEkwEkq ijij ∇−+∇=  (7)                      
or if ε≤)()( kqkpT , where ε is a positive 
constant, then B(k+1) is the identity matrix. 
An example of nonlinear system 
identification is applied to illustrate the 
two-stage algorithm.  The plant to be 
identified is governed by the difference 
equation 
). )1( ),(),2( ),1( ),(()1( −−−=+ kukukykykyFky  (8)    
where 
 .
1
)1( ), , , , ( 22
23
435321
54321
xx
xxxxxx
xxxxxF
++
+−
=  (9)                
A [5*5*1] feedforward neural network, 
representing 5 neurons in the input layer, 5 
in the hidden layer, and 1 in the output layer, 
is constructed to train the network.  The 
input to the plant and network model is 
assumed to be the independent and 
identically distributed random signal 
uniformly ranged in the interval [-1, 1].  
System identification by neural network 
contains two phases: the network learning 
(or training) phase and the validation phase.  
The learning phase is to find the appropriate 
connection weights by the two-stage 
algorithm.  If the error between the 
network’s estimated output and the desired 
output is small enough, the connection 
 
(5) 
 6 
clamp-free boundary condition.  One 
piezoelectric actuator driven by an 
independent and identically distributed 
random signal uniformly distributed over 
± 9V is employed to excite the structure, 
and the system state from the attached 
accelerometer (B&K 4375) attached at the 
beam tip and piezoelectric sensor signal are 
sampled at 100 Hz.  The measurement of 
accelerometer is then integrated to yield 
velocity signal by hardware and then 
integrated and filtered to displacement 
signal by software.  The data acquisition is 
based on LABVIEW6.1 (National 
Instruments) and an AT-MIO-16H-9 
interface board.  A [3*3*2] feedforward 
neural network with the two-stage training 
algorithm is constructed for modeling the 
input/output transfer function.  The 
comparison of time response between the 
actual plant and the network model is shown 
in Fig. 3, where the network with the 
two-stage training algorithm can identify 
successfully the dynamics of a composite 
smart structure with embedded piezelectric 
sensor and actuator. 
The neural controller design, similar 
to the indirect Model-Following Control, is 
shown in Fig. 4.  All that required for 
controller design is a neural network model 
of the original system and a reference model 
for desired system performance.  The 
neural controller can be self-organized via 
the output error between the plant and 
reference model.  The desired response of 
the controlled plant with respect to a 
command input is specified in the reference 
model.  The training algorithm should 
adjust the weight and bias of the neural 
controller to meet the design requirement. 
A [3*3*1] feedforward neural 
network with the two-stage training 
algorithm is constructed to train the 
controller by the sampling data, and the 
sampling frequency is at 100 Hz such that 
the neural controller can adequately measure 
the displacement and velocity signals.  The 
reference model is chosen as 
)(240024005 tuxxx =++ &&&   (11) 
in which the un-damped natural 
frequency near 7.8 Hz.  Figure 5(a) shows 
the free vibration response of the composite 
beam under the neural controller.  Figure 
5(b) shows the forced vibration response of 
the composite beam.  The experimental 
results indicate that the neural network 
controller with the two-stage training 
algorithm can reduce the structure vibration 
efficiently.  The identification network and 
the neural controller are in minimal order 
while remaining effective in dynamics 
modeling and vibration suppression. 
()       
A two-stage training algorithm is 
developed for feedforward neural network 
training.  The first stage is to calculate 
better initial weight and bias by using an 
adaptive genetic algorithm for a near 
optimal solution in the early stage of 
training.  The second stage is to train the 
neural network by using a modified Newton 
algorithm with the specific initial weight 
and bias.  This two-stage algorithm 
includes the advantages of genetic algorithm 
in having a critical and better initial 
weight/bias and the modified Newton 
