行政院國家科學委員會補助專題研究計畫
■成果報告
□期中進度報告
在Multi-GPU 架構使用多重片段法設計可分割負載最
佳化分佈之演算法
計畫類別：■個別型計畫　　□整合型計畫
計畫編號：NSC－99－2221－E－006－103－
執行期間：2010年8月1日至　2011年7月31日
計畫主持人：朱治平
共同主持人：
計畫參與人員：陳奇業林侑勢高臆婷李莉俁
成果報告類型(依經費核定清單規定繳交)：■精簡報告 □完整報告
本成果報告包括以下應繳交之附件：
□赴國外出差或研習心得報告一份
□赴大陸地區出差或研習心得報告一份
□出席國際學術會議心得報告及發表之論文各一份
□國際合作研究計畫國外研究報告書一份
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、列管計畫及
下列情形者外，得立即公開查詢
□涉及專利或其他智慧財產權，□一年□二年後可公開查詢
執行單位：國立成功大學資訊工程學系（所）
中 華 民 國 100 年 7 月 31 日
2 The Model
We consider a homogeneous environment for analysis. A node consists of N GPUs, where N is a positive
integer that is greater than or equal to one. Since PCI-E or SLI can provide communication path between
two GPUs, we assume that any two GPUs can send messages immediately. In this paper, we let that our
algorithm is running on a linear array of N GPUs. The processor can send messages to all GPUs simultane-
ously but that share the bandwidth. All the links have the same communication speed and bandwidth. All
the GPU have the same processing capability. A GPU can only start to perform the computation after the
entire load fraction assigned to it is received from processor. We list the notations and terminology used
in this paper in Table 1.
Table 1: Notations and Terminology
N The number of GPUs in a node.
L The total load to be processed.
Tcm The time to transmit a unit of load along a bus.
Tcp Tcp is the time to process a unit of load on a GPU.
β The computation-to-communication ratio of a GPU in the node where β = Tcp/Tcm.
τj Interval j or the time duration of interval j.
ρ ρ = β/N .
m m is the number of multi-installment intervals.
x x is the temporary variable in each algorithm. The value of x is computed based on L
and it is different in all the algorithms studies in this paper.
Lj The amount of the load processed by GPUj for algorithm where
∑N
j=1 Lj = L.
TAm, S
A
m The parallel execution time and speedup of algorithm A with multi-installment in a sys-
tem of a GPU.
TAN,m, S
A
N,m The parallel execution time and speedup of algorithm A with multi-installment in a sys-
tem of N GPUs.
We are particularly interested in TA∞ = limm→∞ TAm , TAN,∞ = limm→∞ TAN,m, SA∞ = limm→∞ SAm and SAN,∞ =
limm→∞ SAN,m, that is, the ultimate parallel processing time and the asymptotic speedup when the number
of installment goes to infinity. Without loss of generality and for notational convenience, we assume in this
paper that a unit load is defined such that Tcm = 1 and Tcp = β.
3 The Existing Method on GPU
The classic divisible load distributionmethods running on a GPU is sequentially transmitting data and pro-
cessing load. Algorithm C on a GPU divides the parallel execution process into 2m time intervals. In each
interval, GPU is receiving data or processing load. The loaddistribution diagramof algorithmC is illustrated
in Fig. 1. In the first time interval, processor transmits x units of load to GPU. In the second time interval,
GPU processes x units of load. We repeat that two time intervals until GPU processes m load fraction.
 
x 
x 
x 
x 
x 
x 
x 
x 
comm. comp. 
Figure 1: Load distribution diagram of algorithm C.
We have to calculate the parallel execution time of algorithm C. Since the load fraction processed by
GPU ismx and the total load is L. We obtain the following equation
L = mx
2
Algorithm F
1: for i← 1 tom do {for each time interval τi}
2: if i = 1 then
3: Thread A sends x amount of load to GPU;
4: else if 2 < i < m then
5: for j ← 1 to 2 do {in parallel}
6: if imod 2 = 0 then
7: if j = 1 then
8: GPU computes x amount of load in Thread A;
9: else
10: Thread B sends x amount of load to GPU;
11: end if
12: else
13: if j = 1 then
14: Thread A sends x amount of load to GPU;
15: else
16: GPU computes x amount of load in Thread B;
17: end if
18: end if
19: end for
20: else
21: GPU computes x amount of load;
22: end if
23: end for
Hence, the parallel execution time is
TFm = (1 + β)x+ (m− 1)xmax{1, β}
=
{
(β +m)L/m, for all β < 1
(1 + βm)L/m, for all β ≥ 1
and the speedup is
SFm =
TCm
TFm
=
{
(β +m)m/(β +m), for all β < 1
(β +m)m/(1 + βm), for all β ≥ 1 . (4.2)
Theorem .1. The speed-up of algorithm F is (4.2) and the asymptotic speed-up is SF∞ = (1 + β)/β, for all
β ≥ 1 or SF∞ = 1 + β, for all β < 1.
4.2 Algorithm P with Flexible Size of Chunk
Since transmitting and processing of load fraction of algorithm F is not finish at same time. Hence, we
propose a method that is using flexible size of chunk. The load distribution diagram of algorithm P is illus-
trated in Fig. 3. In the first interval, the processor transmits x/β6 units of load to GPU. In the second time
interval, GPU processes x/β6 units of load and receive x/β5 units of load simultaneously. The same com-
putation and communication processes repeat until GPU receives a load in themth interval and processes
the received load fraction in the last interval.
We give a complete pseudocode of algorithm P. Since the load fraction processed by GPU is x/βm−i+1
for all 2 ≤ i ≤ m+ 1 and the total load processed is L, we have
L = x
(
1 +
1
β
+
1
β2
+ · · ·+ 1
βm−1
)
which yields
x =
1
β − 1
1
βm − 1
L
4
stage and multi-installment stage. We first summarize the performance results in Table 3 to give an easy
reference for readers to understand the straightforward.
Table 3: Performance Summary
Algorithm Execution time Speedup
C (1 + ρ)L 1
F
{
(ρ+m)L/m, for all ρ < 1
(1 + ρm)L/m, for all ρ ≥ 1
{
(ρ+m)m/(ρ+m), for all ρ < 1
(ρ+m)m/(1 + ρm), for all ρ ≥ 1
P (1− ρm+1)/(1− ρm)L (1 + ρ)(1− ρm)/(1− ρm+1)
M
(1+1/β)N+ ρρ−1 (ρ
m−1−1)
(1+1/β)N+ 1ρ−1 (ρ
m−1−1)−1L
(1+ρ)((1+1/β)N+ 1ρ−1 (ρ
m−1−1)−1)
(1+1/β)N+ ρρ−1 (ρ
m−1−1)
5.1 The Execution Time of Algorithm C with Multiple GPU
The algorithm C running on multi-GPU is sequentially transmitting data and processing load. The load
distribution diagram of algorithm C on multi-GPU is illustrated in Fig. 4. In first time interval, Processor
transmits x units of load to all GPU simultaneously. In the second time interval, GPU processes x units of
load simultaneously. The same computation and communication processes repeat until GPU receives a
load in the (2m− 1)th interval and processes the received load fraction in the last interval.
 
x 
x 
x 
x 
x 
x 
x 
x 
comm. comp. 
x 
x 
x 
x 
x 
x 
x 
x 
x 
x 
x 
x 
x 
x 
x 
x 
GPU3 
GPU2 
GPU1 
Figure 4: Load distribution diagram of algorithm C on Multiple GPU.
We have to calculate the parallel execution time of algorithm C on multi-GPU. Since the load fraction
processed by each GPU is number ofm load fraction. We obtain the following equation
L = Nmx
that is
x =
L
Nm
Hence, the parallel execution time is
TCN,m = (N + β)mx
= (1 + ρ)L. (5.4)
5.2 The Speedup of Algorithm F with Multiple GPU
The algorithm F running on multi-GPU is using overlapping technique and send fixed size of chunk to all
GPU simultaneously. The load distribution diagram of algorithm F on multi-GPU is illustrated in Fig. 5.
6
 comm. comp. 
GPU3 
GPU2 
GPU1 
36xȕ6 
36xȕ
35xȕ5 
35xȕ5 
34xȕ
34xȕ
33xȕ
33xȕ
32xȕ
32xȕ2 
3xȕ 
3xȕ 
x 
x 
36xȕ6 
36xȕ
35xȕ5 
35xȕ5 
34xȕ
34xȕ
33xȕ
33xȕ
32xȕ
32xȕ2 
3xȕ 
3xȕ 
x 
x 
Thread A Thread B 
36xȕ6 
36xȕ
35xȕ5 
35xȕ5 
34xȕ
34xȕ
33xȕ
33xȕ
32xȕ
32xȕ2 
3xȕ 
3xȕ 
x 
x 
Figure 6: Load distribution diagram of algorithm P on Multiple GPU.
which yields
x =
N
β − 1
Nm
βm − 1
L
N
Consequently, the parallel execution time is
TPm =
Nm−1x
βm−1
+ βx
(
1 +
N
β
+
N2
β2
+ · · ·+ N
m−1
βm−1
)
= (1− ρm+1)/(1− ρm)L
and the speedup is
SPN,m =
TCN,m
TPN,m
= (1 + ρ)(1− ρm)/(1− ρm+1). (5.6)
Theorem .5. The speed-up of algorithm P is (5.6) and the asymptotic speed-up is SPN,∞ = (1 + ρ)/ρ, for all
ρ ≥ 1 or SPN,∞ = 1 + ρ, for all ρ < 1.
5.4 AlgorithmMwith Flexible Size of Chunk on Multiple GPU
By carefully inspecting the load distribution in Fig. 6, it can be seen that the waiting time for processor to
receive a load fraction to compute is the main delay, because that share the bandwidth. In this section, we
propose a method that employs pipelined technique to enhance the performance. We split algorithm M
into two stages that are pipelined stage and multi-installment stage.
5.4.1 Pipelined Stage [21]
The algorithm M of pipelined stage running on multi-GPU is using pipelined communication technique.
The load distribution diagram of algorithm M with pipelined stage on multi-GPU is illustrated in Fig. 7. In
the first interval, the processor transmits (1 + 1/β)2x units of load to GPU3. In the second time interval,
GPU3 splits the received load into two parts, (1 + 1/β)x/β and (1 + 1/β)x. The former remains in GPU3 for
processing and the latter is transmitted to GPU2. Notice that the term (1 + 1/β)x, which is the amount of
load transmitted from the processor. The same computation and communication processes repeat until
GPU1 receives a load in the N th interval and the received load fraction in the last interval is using in multi-
installment stage.
8
AlgorithmM
1: for i← 1 to N +m do {for each time interval τi}
2: if i ≤ N then {Denote GPUN+1 by CPU}
3: for j ← N − i+ 1 to N do {in parallel}
4: if j > N − i+ 1 then
5: send (1 + 1/β)N−ix amount of load to GPUj ;
6: receive (1 + 1/β)N−ix amount of load from GPUj+2;
7: compute (1 + 1/β)N−ix/β amount of load;
8: else
9: receive (1 + 1/β)N−ix amount of load from GPUN−i+2;
10: end if
11: end for
12: else if i < N +m then
13: for j ← 1 to N do {in parallel}
14: receive ρi−Nx amount of load from GPUN+1;
15: compute ρi−N−1x amount of load;;
16: end for
17: else
18: for j ← 1 to N do {in parallel}
19: compute x amount of load;
20: end for
21: end if
22: end for
which yields
x =
L
β
(1 + 1/β)N + 1ρ−1 (ρ
m−1 − 1)− 1
and
LN = (1 + 1/β)
N−1x+
ρ
ρ− 1(ρ
m−1 − 1)x
=
(1 + 1/β)N−1 + ρρ−1 (ρ
m−1 − 1)
(1 + 1/β)N + 1ρ−1 (ρ
m−1 − 1)− 1
L
β
Consequently, the parallel execution time is
TMN,m = LNβ + (1 + 1/β)
N−1x
=
(1 + 1/β)N + ρρ−1 (ρ
m−1 − 1)
(1 + 1/β)N + 1ρ−1 (ρ
m−1 − 1)− 1L (5.7)
and the speedup is
SMN,m =
TCN,m
TMN,m
=
(1 + ρ)((1 + 1/β)N + 1ρ−1 (ρ
m−1 − 1)− 1)
(1 + 1/β)N + ρρ−1 (ρ
m−1 − 1) . (5.8)
Theorem .6. The speed-up of algorithmM is (5.8) and the asymptotic speed-up is SMN,∞ = (1+ ρ)/ρ, for all
ρ ≥ 1 or SMN,∞ = 1 + ρ, for all ρ < 1.
In case of ρ = 1, the load fraction Lj of the formula (5.7) will become infinite. In practically, the load
fractions L1, L2, . . . , LN are defined as
Lj = (1 + 1/β)
j−1x+ (m− 1)x, for ρ = 1
for all 1 ≤ j ≤ N . The condition L1 + L2 + . . .+ LN = L gives(
1 + (1 + 1/β) + (1 + 1/β)2 + · · ·+ (1 + 1/β)N−1)x+N(m− 1)x = L
10
[]
[]
Figure 9: Speed-up of algorithms P and F, wherem = 5 and (a) β = 100 (b) β = 0.1.
A POOF OF THEOREM 3
First, we prove that TCm > TFm for all β > 0 onm ≥ 2. The following equation is always true.{
1 + β > β+mm , for all β < 1
1 + β > 1+βmm , for all β ≥ 1
Second, we prove that TFm > TPm for all β > 0 onm ≥ 2. To show
TFm = (β +m)L/m >
1− βm+1
1− βm L = T
P
m , for all β < 1
it suffices to show that
βm + βm−1 + · · ·+ β > mβm, for all β < 1
that is,
β(1− βm)
1− β > mβ
m, for all β < 1
12
or
1− βm +mβ −mβm+1 > m−mβm+1, for all β > 1
Consequently
(1 + βm)(1− βm) > m(1− βm+1), for all β > 1
The last inequality is obvious ifm ≥ 2. In the case of β = 1, TFm = TPm
B POOF OF THEOREM 8
First, we prove that TCN,m > TFN,m for all β > 0 on N,m ≥ 2. The following equation is always true.{
1 + ρ > ρ+mm , for all ρ < 1
1 + ρ > 1+ρmm , for all ρ ≥ 1
Second, we prove that TFN,m > TPN,m for all β > 0 on N,m ≥ 2. To show
TFN,m = (ρ+m)L/m >
1− ρm+1
1− ρm L = T
P
N,m, for all ρ < 1
it suffices to show that
ρm + ρm−1 + · · ·+ ρ > mρm, for all ρ < 1
that is,
ρ(1− ρm)
1− ρ > mρ
m, for all ρ < 1
or
ρ− ρm+1 +m−mρm > m−mρm+1, for all ρ < 1
Consequently
(ρ+m)(1− ρm) > m(1− ρm+1), for all ρ < 1
The last inequality is obvious if N,m ≥ 2. To show
TFN,m = (1 + ρm)L/m >
1− ρm+1
1− ρm L = T
P
N,m, for all ρ > 1
it suffices to show that
ρm−1 + ρm−2 + · · ·+ 1 > m, for all ρ > 1
that is,
1− ρm
1− ρ > m, for all ρ > 1
or
1− ρm +mρ−mρm+1 > m−mρm+1, for all ρ > 1
Consequently
(1 + ρm)(1− ρm) > m(1− ρm+1), for all ρ > 1
The last inequality is obvious if N,m ≥ 2. In the case of ρ = 1, TFN,m = TPN,m.
Third, we prove that TPN,m > TMN,m for all β > 0 on N,m ≥ 2. To show
TPN,m =
1− ρm+1
1− ρm L >
(1 + 1/β)N + (m− 1)
(1 + 1/β)N + 1ρ (m− 1)− 1
L = TMN,m, for ρ ̸= 1
14
[6] V. Bharadwaj and H. M. Wong, ``Scheduling divisible loads on heterogeneous linear daisy chain net-
works with arbitrary processor release times,'' IEEE Transactions on Parallel and Distributed Systems,
vol. 15, pp. 273--288, 2004.
[7] M. Billeter, O. Olsson, and U. Assarsson, ``Efficient stream compaction on wide simd many-core ar-
chitectures,'' in HPG '09: Proceedings of the Conference on High Performance Graphics 2009. New
York, NY, USA: ACM, 2009, pp. 159--166.
[8] J. Blazewicz, M. Drozdowski, F. Guinand, and D. Trystram, ``Scheduling a divisible task in a
two-dimensional toroidal mesh,'' Discrete Applied Mathematics, vol. 94, no. 1-3, pp. 35 -
- 50, 1999, proceedings of the Third International Conference on Graphs and Optimization
GO-III. [Online]. Available: http://www.sciencedirect.com/science/article/B6TYW-3X52GHW-4/2/
e609cc4f555fe2013ee42828c408e707
[9] J. Blazewicz, M. Drozdowski, and M. Markiewicz, ``Divisible task scheduling --- concept and verifica-
tion,'' Parallel Comput., vol. 25, no. 1, pp. 87--98, 1999.
[10] S. K. Chan, V. Bharadwaj, and D. Ghose, ``Large matrix-vector products on distributed bus networks
with communication delays using the divisible load paradigm: performance analysis and simulation,''
Mathematics and Computers in Simulation, vol. 58, no. 1, pp. 71 -- 92, 2001. [Online]. Available: http://
www.sciencedirect.com/science/article/B6V0T-4472MY6-4/2/3471d7972320f1651ce93f15917cf336
[11] Y.-K. Chang, J.-H. Wu, C.-Y. Chen, and C.-P. Chu, ``Improved methods for divisible load distribution
on k-dimensional meshes using multi-installment,'' IEEE Transactions on Parallel and Distributed Sys-
tems, vol. 18, pp. 1618--1629, 2007.
[12] S. Charcranoon, T. Robertazzi, and S. Luryi, ``Parallel processor configuration design with processing/
transmission costs,'' Computers, IEEE Transactions on, vol. 49, no. 9, pp. 987 --991, sep 2000.
[13] C. Y. Chen and C. P. Chu, ``Improved methods for divisible load distribution on d-dimensional hyper-
cube using multi-installment,'' Journal of the Chinese Institute of Engineers, vol. 31, no. 7, pp. 1199--
1206, 2008.
[14] Y. Cheng and T. Robertazzi, ``Distributed computation with communication delay [distributed intel-
ligent sensor networks],'' Aerospace and Electronic Systems, IEEE Transactions on, vol. 24, no. 6, pp.
700 --712, nov 1988.
[15] M. Drozdowski and W. Glazek, ``Scheduling divisible loads in a three-dimensional mesh of
processors,'' Parallel Computing, vol. 25, no. 4, pp. 381 -- 404, 1999. [Online]. Available: http://
www.sciencedirect.com/science/article/B6V12-48CP9S8-G/2/c8af3b36b3c1c869e3ccbfcfeac7778f
[16] M. Drozdowski and P. Wolniewicz, ``Out-of-core divisible load processing,'' IEEE Transactions on Par-
allel and Distributed Systems, vol. 14, pp. 1048--1056, 2003.
[17] N. Fujimoto, ``Fastermatrix-vectormultiplication on geforce 8800gtx,'' in Parallel and Distributed Pro-
cessing, 2008. IPDPS 2008. IEEE International Symposium on, 14-18 2008, pp. 1 --8.
[18] D. Ghose and H. J. Kim, ``Computing blas level-2 operations on workstation clusters using the
divisible load paradigm,,'' Mathematical and Computer Modelling, vol. 41, no. 1, pp. 49 --
70, 2005. [Online]. Available: http://www.sciencedirect.com/science/article/B6V0V-4G3CBNS-5/2/
fe6b2090605f5232e553fa855a087bb0
[19] W. Glazek, ``Distributed computation in a three-dimensional mesh with communication delays,'' in
Parallel and Distributed Processing, 1998. PDP '98. Proceedings of the Sixth Euromicro Workshop on,
21-23 1998, pp. 38 --42.
[20] J. Guo, J. Yao, and L. Bhuyan, ``An efficient packet scheduling algorithm in network processors,'' in
INFOCOM 2005. 24th Annual Joint Conference of the IEEE Computer and Communications Societies.
Proceedings IEEE, vol. 2, 13-17 2005, pp. 807 -- 818 vol. 2.
[21] K. Li, ``Improved methods for divisible load distribution on k-dimensional meshes using pipelined
communications,'' IEEE Transactions on Parallel and Distributed Systems, vol. 14, pp. 1250--1261,
2003.
[22] ------, ``Accelerating divisible load distribution on tree and pyramid networks using pipelined com-
munications,'' Parallel and Distributed Processing Symposium, International, vol. 14, p. 228b, 2004.
16
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/18
國科會補助計畫
計畫名稱: 在Multi-GPU架構使用多重片段法設計可分割負載最佳化分佈之演算法
計畫主持人: 朱治平
計畫編號: 99-2221-E-006-103- 學門領域: 平行與分散處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
「無」。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
