convergence in the PSO. In Section 4, the proposed 
HPSO is compared with some existing evolutionary 
algorithms using 5 benchmark functions with a low 
dimension and a high dimension. Finally, Section 5 
draws conclusions about the proposed HPSO approach 
to solving optimization problems. 
 
2. Particle Swarm Optimization 
PSO is an evolutionary computation technique 
proposed by Kennedy and Eberhart. Its development 
was based on observations of the social behavior of 
animals such as bird flocking, fish schooling, and 
swarm theory. Like the GA, the PSO is initialized with 
a population of random solutions of the fitness 
function. The individuals in the population are called 
as particles. The PSO is an iterative method based on 
the search behavior of a swarm of m particles in a 
multidimensional search space. In each iteration, the 
velocity and the position of each particle are updated. 
According to the fitness values of the updated 
individuals, the personal best position of each particle 
and the global best position among all the particles are 
updated. For the update of the velocities in the PSO, a 
particle is influenced by its personal best position and 
the global best position. Therefore, the PSO searches 
the global optimum solution by adjusting the 
trajectory of each particle toward its personal best 
position and the global best position. 
Consequently, each particle keeps track of its 
own best solution, which is associated with the best 
fitness value, it has achieved so far in a vector. 
Furthermore, the best solution among all the 
individuals obtained so far in the population is kept 
track of as the global best position. The adjustment 
toward its personal best position and the global best 
position is conceptually similar to the crossover 
operator utilized by the GA. However, all the particles 
move toward the global best position and the PSO has 
no the mutation operator. These probably lead to the 
premature convergence so that the obtained solution is 
trapped into the local optimum. In order to illustrating 
the premature convergence in the PSO, an example is 
considered as follows: 
∑
=
∈+−=
D
1i
ii3
2
iD211 ].13,3[x),xsin(xsin)x,,x,x(f L
        (1) 
The results of finding the maximum values of 
the function 1f  for different dimension values of D is 
shown in Table 1, where the target solution is the 
optimal solution of the function ( D*21598.1 ), and 
the best solution, the worse solution, and the mean 
solution are the maximum value, the minimum value, 
and the average value in 30 independent runs, 
respectively. Furthermore, a distance function dist(D) 
for describing the mean distance between the optimal 
solution and the obtained best solution is defined as 
follows [3]: 
D
)D(f)D(f
)D(dist bestopt
−
= ,   (2) 
where )D(fopt  and )D(fbest  is the optimal solution 
and the obtained best solution, respectively. It is useful 
to evaluate the performance of the evolutionary 
learning approach to the optimization problem with a 
large number of parameters. A better evolutionary 
learning maintains the dist(D) value, as the number of 
parameters increases. On the other hand, a bad one 
tends to increase the dist(D) value. In Fig. 1, the 
average dist(D) values obtained by the PSO to the 
function 1f  using 30 independent runs for 
D=10,20,...,100, are shown . According to the results 
of Fig. 1, as the value of the dimension increases, the 
different between the obtained solution by the PSO 
and the target solution increase. That is the reason that 
as the value of the dimension increases, the solution 
space increases hugely. Therefore, the solution of the 
PSO traps easily in a local optimum so that the PSO 
has the premature convergence. In order to prevent the 
premature convergence, the HPSO is proposed in the 
next section. 
 
 
 
3. Hierarchical Particle Swarm 
Optimization 
In this section, the HPSO is proposed to 
improve the premature convergence in the PSO. In the 
HPSO, all particles are arranged in a hierarchy that 
defines a regular tree structure. The structure of the 
regular tree is determined by the height (h) and the 
branching degree (d) of the tree so that m nodes, 
,m,,2,1k,s k L=  are generated for constructing the 
regular tree, where 1d
1dm
h
−
−
=  is the number of 
nodes in the tree. In the initial population, m 
individuals, ,m,,2,1k,p
k
L=  are generated initially 
and arranged orderly in the m nodes of the regular tree. 
That is, the individual },m,,2,1{k,p
k
L∈  is 
arranged in the node }m,,2,1{k,sk L∈ . In each 
iteration, the positions of two individuals in the tree 
are probably exchanged. For a individual 
k
p  in the 
non-root node }m,,3,2{j,s j L∈ , the fitness value of 
the individual 
k
p  is compared with that of the 
individual f
k
p , where the individual f
k
p  is in the 
parent node of js . If )p(fit)p(fit fkk >  then kp  and 
f
k
p  exchange their positions within the tree. 
Therefore, the individual 
k
p  will move up one level 
in the tree. For the update of the velocities in HPSO, 
the velocity vectors of the particles ,m,,2,1k,p
k
L=  
4. Simulations 
In this section, 5 benchmark functions [3] are 
employed to examine the efficiency of the proposed 
HPSO for optimization problems. The test function, 
parameter domain, and global optimum for each 
benchmark function are listed in Table 1. In order to 
examine the proposed HPSO approach to optimization 
problems with a low dimension and a high dimension, 
the dimensions of the 5 benchmark functions are set to 
D=10 and D=100 in the experiments, respectively. The 
initial conditions for the proposed HPSO method to 
each benchmark function are given in the following: 
the height of the tree: h=3, the branching degree of the 
tree: d=4, the mutation rate: 1.0p m = , the constants 
for the HPSO: )95.0,9.0,2,2()r,w,c,c( 21 = . Table 2 
and Table 3 compare our results of the proposed 
HPSO approach with the results obtained by the PSO 
approach for each benchmark function with D=10 and 
D=100, respectively. According to Table 2 and Table 3, 
it reveals that the PSO approach method might lead to 
the earlier convergence so that the result of the PSO 
approach is trapped into the local optimum solution 
owing to the lack of swarm’s diversity. However, the 
proposed HPSO has more diversity such that it is hard 
to be trapped into the local optimum owing to having 
more searching choices for the particle swarm. 
Consequently, the proposed HPSO is an effective tool 
for solving optimization problems. 
 
5. Conclusion 
In this project, a hierarchical particle swarm is 
proposed to solve unconstrained optimization 
problems. In the HPSO, the particles are arranged in a 
regular tree and move up or down in the tree 
according to their fitness values. The velocity update 
of each particle depends on the position of the particle 
in the tree. Furthermore, a mutation operator is added 
in the HPSO approach. Consequently, the diversity of 
the population in the proposed HPSO will increase so 
that the HPSO approach has a much opportunity of 
finding the global optimum solution. According to the 
simulation results for 5 benchmark functions, it is 
clear that the proposed HPSO approach is superior to 
the other evolutionary algorithms in the ability to 
finding the global optimum solution. 
 
References 
[1] D.B. Chen and C.X. Zhao, Particle swarm 
optimization with adaptive population size and 
its application, Applied Soft Computing 9 (1) 
(2009) 39-48. 
[2] M. Clerc and J. Kennedy, The particle swarm－
Explosion, stability, and convergence in a 
multidimensional complex space, IEEE Trans. 
Evolutionary Computation 6 (1) (2002) 58-73. 
[3] S.Y. Ho, L.S. Shu and J.H. Chen, Intelligent 
evolutionary algorithms for large parameter 
optimization problems, IEEE Transactions on 
Evolutionary Computation 8 (6) (2004) 
522-541. 
[4] S.Y. Ho, H.S. Lin, W.H. Liauh, and S.J. Ho, 
OPSO: Orthogonal partical swarm optimization 
and its applications to task assignment problems, 
IEEE Trans. on Systems, Man, and Cybernetics
－part A: Systems and Humans 38 (2) (2008) 
288-298. 
[5] S. Janson and M. Middendorf, A hierarchical 
particle swarm optimizer and its adaptive variant, 
IEEE Transactions on Systems, Man, and 
Cybernetics- Part B: Cybernetics 35 (6) (2005) 
1272-1282. 
[6] W. Jian, Y.C. Xue, J.X. Qian, Improved particles 
swarm optimization algorithms study based on 
the neighborhoods topologies, The 30th Annual 
Conference of the IEEE Industrial Electronics 
Society, Busan Korea, (2004) 2192-2196. 
[7] P.N. Suganthan, Particle swarm optimizer with 
neighborhood operator, Proc. Congr. 
Evolutionary Computation (CEC 1999), (1999) 
1958-1962. 
 
 
Table 1 The test function, parameter domain, and 
global optimum for each benchmark function 
Test function 
ix  domain Optimu
m 
∑
=






+−=
D
1i
i
i1 )3
x2
sin()xsin(f  
[3,13] 1.21598
D (max) 
∑
−
=
+
+ 





++−=
1D
1i
1ii
1ii2 )3
xx2
sin()xxsin(f  
[3,13] D2≈ (
max) 
[ ]∑
=
+pi−=
D
1i
i
2
i3 10)x2cos(10xf  
[-5.12,5.12] 0 (min) 
[ ]∑−
=
+ −+−=
1D
1i
2
i
22
i1i4 )1x()xx(100f  
[-5.12,5.12] 0 (min) 
1)
i
x(cosx
4000
1f i
D
1i
D
1i
2
i5 +−= ∏∑
==
 
[-600,600] 0 (min) 
 
 
 
Table 2 Comparison between our results of the 
proposed HPSO and the results obtained by the PSO 
method for each benchmark function with D=10 
 HPSO PSO 
Test 
function 
Mean function 
value 
Mean 
function 
value 
1f  12.1597 10.2999 
2f  16.8107 9.5543 
3f  510524.9 −×  68.1531 
4f  3.9732 443.85 
5f  0 16.992 
無衍生研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
