ABSTRACT
In this paper, a new semantic learning method to
generate probabilistic semantic information for image
annotation from a given amount of labeling effort is
proposed. In our approach, the database images are
classified into two classes –the labeled class and the
unlabeled class. For each image in the labeled class,
we maintain a list of probabilities, each indicating the
probability of this image having one of the pre-selected
semantic concepts. All the images in the database are
segmented into multiple disjoint regions, where each of
them is represented by three types of low-level visual
features (i.e. color, shape, and texture). With this
representation a probabilistic model based on the
statistical information of low-level visual features is
predicted to analyze semantic concepts hidden in the
database. The proposed semantic learning scheme
provides a way to bridge the gap between the high-level
semantic concept and the low-level features for
content-based image retrieval. Experimental results
show that the performance of the proposed method is
excellent as compared with that of other simulated
semantic learning algorithms proposed in the literature.
1. INTRODUCTION
With the advance of the Internet, the demands for
storing multimedia information have increased. A richer
set of search facilities that include keywords, sounds,
examples, shape, color, texture, spatial structure and
motion is required to facilitate the development of such
databases. In the past decade, textual features such as
filenames, captions, and keywords have been used to
annotate and retrieve images. As they are applied to a
large database, the use of keywords becomes not only
cumbersome but also inadequate to represent the image
content. Many content-based image retrieval systems
have been proposed in the literature [1-5]. To access
images based on their content, low-level features such
as colors, textures, and shapes of objects are widely
used as indexing features for image retrieval. Although
content-based image retrieval (CBIR) is extremely
desirable in many applications, it must overcome
several challenges including image segmentation,
extracting features from the images that capture the
user’s notion, and match the images in a database with
a query image based on the extracted features. Due to
these difficulties, semantic learning approaches
facilitate further image management such as
semantics-intensive image retrieval, image annotation,
and metadata computation.
Deriving high-level semantic concepts from
low-level visual features is typically based on one of
two approaches. These approaches can be classified as:
(1) learning from interactive user relevant feedback and
(2) learning from templates without run-time user
interaction. In the relevance feedback approach [6,7],
one may formulate learning of user perception as an
optimization problem to estimate parameters of the
distance metric to minimize the sum of distances of
relevant examples from the query in the low-level
feature space based on user feedback. However, since
the set of semantically similar objects might be
classified into several clusters in the low-level feature
space, querying an object in one cluster would not be
able to retrieve semantically similar objects in other
clusters by estimating parameters of the distance metric.
Lu et. al. [8] proposed a similar approach to use
relevance feedback to build semantic knowledge inside
the database. During training, the objects in the
database are grouped into small clusters and related the
clusters with semantic weights in their systems. During
retrieval, the clusters and the semantic weights are
updated according to the user’s feedback. Potential
drawbacks of the relevance feedback approach include:
(1) the number of iterative cycles to converge is hard to
predict; (2) sensitivity to user subjectivity; and (3)
short-term knowledge is stored only during the current
query session and does not propagate to later queries
[9].
More recently, several algorithms were introduced to
improve the performance of the relevance feedback
approach. In particular, He et al. [10] proposed a
learning method to discover hidden semantic space
from the user relevance feedback. Tong and Chang [11]
proposed incorporating active learning to search a given
image database with relevance feedback using the
support vector machine. In [12], a neural network
model with back-propagation through structure learning
scheme is proposed to learn the tree-structure
representation of the image content. Another approach
to the above problem is hidden annotation. In [13] and
[14], some experiments on hidden annotation in their
retrieval systems showed positive results. Most existing
systems for learning and propagating labels assigned by
human users using hidden annotation either annotate all
the images (full annotation) in the database, or annotate
a subset of the database manually selected (partial
annotation). Full annotation is cumbersome when the
database becomes larger and larger. Zhang and Chen
[15] presented an active learning framework to guide
hidden annotation by partial annotation in order to trim
down the heavy manual labors and improve the
retrieval performance. A potential problem of partial
annotation is that it is not clear how much annotation is
sufficient for a specific database, and what the best
represented by a semantic vector )(is which is defined
as
 ni ssss ,...,, 21)(  , 1
1


n
j
js (1)
where )/( ijj ISPs  denotes the conditional
probability of semantic description Sj given image Ii,
indicating the importance degree of Sj . Let there be a
fixed though unknown relation ),( sxP  between the input
low-level feature vector x

and the target semantic
vector s . Suppose we have an image database
 ul DDD , where,   Niiil sxD 1)()( ,   consists of the labeled
images drawn independently from ),( sxP  and  Miiu xD 1')'( 
are the unlabeled images drawn independently
from )(xP  such that MN  . When the number of
unlabeled images are 0 (M = 0), the problem becomes a
pure supervised learning problem. On the other hand,
when the number of labeled images are 0 (N = 0) the
problem becomes a pure unsupervised learning problem.
Initially, N is very small. More specifically, one may
explicitly label randomly chosen images. We propose a
process to extract the hidden semantic vectors of
images in the unlabeled dataset based on the estimated
statistics using the original labeled data. Along with the
extracted hidden semantic vectors, the corresponding
unlabeled images are classified based a classifier using
maximum likelihood estimation. Then the most
confusion pattern chosen from Du is selected to label
explicitly, add to Dl, and remove from Du. With each
iteration of active learning, Dl increases in size,
whereas Du decreases in size. Once the size of Dl
achieves a preset size, the semantic vectors of the
remainders of the unlabeled images are automatically
generated using a semantic interpolation process.
The image semantic learning problem has two
challenges in real-world applications. First, an
unlabeled image may have few similar images that have
been labeled in Dl because the size of Dl is small.
Secondly, the probabilities for most elements in the
semantic vectors are 0 due to the fact that the number of
meaningful objects in an image is not large. Thus, the
labeled images in Dl probably contribute an incomplete
set of semantic descriptions as candidates in later
annotation processing. This phenomenon explains why
low-recall queries are easy to appear even in digital
libraries with millions of images.
3. REGION-BASED IMAGE REPRESENTATION
The “hidden semantics discovery” concept requires 
that similarity measures for low-level visual features,
such as color, texture, and shape, to be defined. Ideally,
what we try to measure is the semantic similarity,
which physically is very difficult to define, even to
describe. Humans tend to use high-level concepts in
everyday life. However, what existing computer vision
techniques can automatically extract from image are
mostly low-level features. Object segmentation and
recognition is the primary step of computer vision for
applying to image retrieval of higher-level image
analysis [4]. In constrained applications, such as the
human face and finger print, it is possible to represent
the high-level concepts (faces or finger prints) on the
basis of the low-level features. In a general setting,
however, there is a semantic gap between the object in
the real world and the low-level features. Automatic
segmentation and recognition of objects via object
models is a difficult task without a priori knowledge
about the shape of objects.
Instead of segmentation and detailed object
representation, several image-to-image similarity
measurements that combine information from all of the
regions have been proposed [2,3]. For example, the
SIMPLIcity system [5] uses an integrated region
matching as its image similarity measure. By allowing
many-to-many relationship of the regions, the approach
is robust to inaccurate segmentation.
In this work, the images in a database are first
segmented into homogeneous regions. Then multiple
semantics-related features such as color, texture, and
shape are extracted to represent the content of each
region. The methods for image segmentation and
corresponding feature extraction are similar to those
employed in [5], which are shown to be effective.
Normalization process to each type of feature should be
further performed to keep the variance of each feature
type almost the same. In the preexisting CBIR systems,
the three features are used independently. Users are
required to set the weighting of each feature type in
order to retrieve semantically relevant images.
Unfortunately, the low-level features and the high-level
semantic concepts do not have an intuitive relationship,
and hence, the problem of weighting setting is not a
density estimation [23]. In practice, this model defined
by directly sampling the training data provides a better
explanation of the class conditional density of the
semantic class Si.
Considering all the images in the semantic class Si,
we get an estimate of )|( iSXP based on the given
labeled set using Parzen windows, i.e.,

 










j
i
N
j j
jD
jN
j
j
i
XX
s
s
SXP
1
2
1
2
),(
exp
1
)|(ˆ


(11)
where Xj is an image in Si, Ni is the size of Si, j is the
bandwidth of the kernel function with respect to Xj, sj is
the semantic probability for Xj belonging to the semantic
class Si, and ),( XX jD is the quadratic distance
between the neighboring sample Xj and the center
sample X defined in (6). The estimated a priori
probability of a class, )(ˆ iSP can be obtained from Dl. We
can then classify the labeled images based on (18) in
terms of low-level visual features using the estimated a
priori probabilities and the estimated class conditional
probabilities obtained from (11).
Obviously, the value of )|(ˆ iSXP depends on the choice
of the bandwidth j for each observation ij SX  . If,
for instance, the range of influence of the kernel function
from Xj is very small, then probability density function
estimate will be a very spiky function, while if the range
is large the estimate will fail to detect local variations
in )|( iSXP . The optimal Parzen window size has been
studied extensively in the literature [24]. For example,
we can choose a constant bandwidth i which is set
equal to the variance of distances among pairs of training
samples in Si.
Based on the estimated )|(ˆ iSXP , in this work, we
propose a probabilistic model to estimate the hidden
semantics for images in terms of low-level visual
features. Given an image object X, we can obtain the
estimated posterior probability )|(ˆ XSP i using (8). The
problem is the probability density function P(X)
remains unknown. Instead of deriving the value of P(X)
using some statistical method, as shown in Fig. 1, the
posterior probabilities of the pairwise comparisons are
arranged to form a reciprocal matrix for further
calculations. The main diagonal of the matrix is always
1. Considering a pair of semantic classes Si and Sj,
given an image X, the likelihood
ratio )|()|( ji SXPSXP of posterior probabilities of Si
and Sj plays a role as the preference measurement to
judge which class X belongs to. Let A be
a nn preference matrix constructed through pairwise
comparisons of n semantic classes. For each row of A,
we can define a weighting measurement as
)(ˆ)|(ˆ
)(ˆ)|(ˆ
)|(ˆ
)|(ˆ
22
11
2
1
SPSXP
SPSXP
XSP
XSP 
X
S1
S2
Si
…
Sn
…
S1 S2 … Si Sn
1
)(ˆ)|(ˆ
)(ˆ)|(ˆ 11
ii SPSXP
SPSXP
…
)(ˆ)|(ˆ
)(ˆ)|(ˆ 11
nn SPSXP
SPSXP… …
)(ˆ)|(ˆ
)(ˆ)|(ˆ
11 SPSXP
SPSXP ii
1 …
)(ˆ)|(ˆ
)(ˆ)|(ˆ 22
ii SPSXP
SPSXP
)(ˆ)|(ˆ
)(ˆ)|(ˆ 22
nn SPSXP
SPSXP…
… … … … …
…
)(ˆ)|(ˆ
)(ˆ)|(ˆ
22 SPSXP
SPSXP ii 1 … )(ˆ)|(ˆ
)(ˆ)|(ˆ
nn
ii
SPSXP
SPSXP
… … … …
…
)(ˆ)|(ˆ
)(ˆ)|(ˆ
11 SPSXP
SPSXP nn
)(ˆ)|(ˆ
)(ˆ)|(ˆ
22 SPSXP
SPSXP nn …
)(ˆ)|(ˆ
)(ˆ)|(ˆ
ii
nn
SPSXP
SPSXP … 1
Fig. 1. The reciprocal matrix with each cell to store the
likelihood ratio between the posterior probabilities of a
pair of semantic classes.
ni
a
a
a
a
a
a
A
n
iii
i ,...,1,...
21
 (12)
where )(/)(ˆ)|(ˆ XPSPSXPa iii  is the relatively
important value of the i-th semantic description. Then
the estimates of the posterior probability )|(ˆ XSP i , i =
1, …, n are determined by
ni
A
A
AfXSP
ii
ii
ii ,...,1,)exp(1
)exp(1
)()|(ˆ 



 (13)
where f(.) is a sigmoid function and i >0 is the
regulation factor of f(.). Obviously, from (13), if the
weighting measurement Ai for X is large, the value of
)|(ˆ XSP i achieves 1. On the other hand, if the weighting
measurement Ai for X is small, the value of
)|(ˆ XSP i nears 0. The estimated a posterior probability
of an image X, )|(ˆ XSP i , can be used to annotate X.
More specifically, we can define a cost function )(E to
be minimized as



iN
j
jiji XSPsE
1
2))|(ˆ(
2
1
)( (14)
where
i is the parameters in the estimation process,
i.e. i in (11) and i in (12), sj is the semantic probability
of a labeled image Xj belonging to the semantic class Si.
By labeling the patterns in Dl, or equivalently, through
the adjustment of
i the overall goal is to minimize (14).
Finding a (possibly local) minimum
point   niii ,...,1,, **   that minimizes )( iE  is of
primary concern. Due to complexity of E, we resort to an
iterative algorithm to explore the input space efficiently.
In the gradient-based iterative descent method, the next
point










i
iold
i
next
i
i
iold
i
next
i
E
E




)(
)(
(15)
whereis small positive learning rate. In practice, we set
the value ofto be 0.0015. The first derivatives of E
at iare
5. EXPERIMENTAL RESULTS
In order to evaluate the proposed approach, a series
of experiments was conducted on an Intel
PENTIUM-IV 1.5GHz PC and a real database of 6515
natural scene images. The test database is initially
separated into two parts – a labeled set Dl and
un-labeled set Du. Dl initially consists of 500 images
which are randomly selected from the test database. For
each image in Dl, we use the AHP annotation process,
proposed in our previous work [29], to obtain the
corresponding semantic vector. The hidden semantics
of every image in Du is extracted using the proposed
semantic learning method.
The experimental test has been separated into three
phases: (1) in the first phase, we use our active learning
algorithm to select image objects with the largest
uncertainty measurement one by one from Du until a
pre-specified number varying from 1000 to 6000 with a
step size 500; (2) in the second phase, the semantic
vectors of all the remainders in Du are automatically
generated; (3) finally, we measure the annotation
efficiency by testing the final retrieval performance of
our retrieval system. Initially, the size of Dl is 500.
Given any specific query q and its top R retrieval
results, the average matching error for these results is
measured by
 


qof
resultsRtopj
S
qj
S
q R
e )(
1 
(23)
where )(S
qj is the semantic distance between the query
and the jth retrieval object, which is defined as
 


n
i
q
i
j
i
j
i
q
i
S
qj ssss
1
)()()()()( )1()1( (24)
where n is the total number of semantic descriptions.
Fig. 3. Performance comparison among our method,
random sampling, Zhang and Chen’s method for the
test image database.
(a)
(b)
Fig. 4. The user interface of the system: (a) a query
example for ‘sunset’image; (b) a query example for
‘butterfly’image.
The item )1()1( )()()()( qijijiqi ssws  is actually the
probability of objects j and q disagreeing with each
other on the ith semantic description. The semantic
vector of q again is obtained by human assessment
using AHP and the )(Sqe indicates the average matching
error for the top R retrieved objects with respect to the
query. The smaller the )(Sqje , the better the performance
of the system for the query q. The overall system
performance is evaluated by



N
i
S
ieN
Err
1
)(1 (26)
where N is the number of images in the database, as we
take every image in the database as a query and calculate
the average matching error.
The performance comparison among the proposed
semantic learning method, the random sampling method,
and Zhang and Chen’s method [16] on the test database 
is given in Fig. 3. To start the compared algorithms, 500
images are randomly chosen and annotated. The
horizontal axis of Fig. 3 is the number of images that
have been annotated, including the initial 500 randomly
drawn samples. The vertical axis is the average matching
error for the whole database measured by (25). A curve
closer to the bottom-left corner is considered to have a
better performance. From Fig. 3, it is obvious that our
semantic learning algorithm outperforms the other
methods.
Fig. 4 shows the user interface to perform two
