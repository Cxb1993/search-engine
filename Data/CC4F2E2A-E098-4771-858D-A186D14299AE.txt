 1 
 
行政院國家科學委員會補助專題研究計畫 █成果報告   
□期中進度報告 
 
（計畫名稱） 
 
 
計畫類別：█個別型計畫   □整合型計畫 
計畫編號：NSC99－2221－ E － 001 － 020 
執行期間： 2010 年 8 月 1 日至 2011 年 7 月 31 日 
 
執行機構及系所：中央研究院資訊科技創新研究中心 
 
計畫主持人：王鈺強 
共同主持人： 
計畫參與人員：楊閩淳(博士班研究生-兼任助理人員) 
 
 
 
成果報告類型(依經費核定清單規定繳交)：█精簡報告  □完整報告 
 
本計畫除繳交成果報告外，另須繳交以下出國心得報告： 
□赴國外出差或研習心得報告 
□赴大陸地區出差或研習心得報告 
█出席國際學術會議心得報告 
□國際合作研究計畫國外研究報告 
 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
            □涉及專利或其他智慧財產權，□一年█二年後可公開查詢 
 
中   華   民   國 100 年 10 月 31 日 
附件一 
 3 
attention maps which are constructed by class-specific visual words of sparse-coded SIFT (ScSIFT) 
descriptors. These visual words are learned via combining support vector machines and feature ranking 
techniques. Therefore, spatially discriminative information is learned and embedded in the visual words. We 
further refine the constructed map by Gaussian smoothing and cross bilateral filtering to preserve local 
appearance information of the objects. We call this final map as sparse-coded visual attention map (ScVAM), 
and apply this map to address object localization problems. 
 
A. Sparse coding for image representation and classification 
 
Vector quantization (VQ) has been a popular technique to compress local descriptors by prototypical 
descriptors which can be obtained by applying clustering algorithms such as K-means clustering. This 
compressing process can be posed as the following optimization problem 
    (1) 
, 
 
where  is the total reconstruction error, , ,   is also 
known as codebook or dictionary,  is the i-th cluster centroid,  means that only one element 
of  is nonzero, and  means that all elements of  are non-negative. Each original descriptor is 
approximated by a single centroid. For example,  means that  is approximated by . In brief, the 
dictionary is learned using clustering algorithm, and the each descriptor  is encoded (or approximated) by 
its nearest centroid. 
However, the  may be too restrict and may lose too much information, rendering the total 
reconstruct error  too large. To relax the constraint, one can replace it with a Lq-norm regularization 
penalty on , which enable  to have multiple but few numbers of non-zero elements,  
 
  (2) 
 
 
where  means the Lq-norm regularization, the term  is also known as the sparsity penalty, and 
 is the sparse code (SC) or sparse representation of . The  constraint enforces the basis  
to be a normalized basis. Ideally, L0-norm sparsity is imposed; however, solving the L0-norm penalty 
optimization problem is a NP-hard problem. Therefore, L1-norm and L2-norm penalties are the most popular 
alternatives. We note that, since now  can have multiple non-zero elements, the total reconstruction error in 
 5 
images from all object classes, the codebook D and the sparse code  in Equation (2) can be solved. The 
keywords in D are thus considered as sparse-coded visual words for image representation. For a test image 
represented as a descriptor set, its SC codes are obtained by optimizing Equation (2) with the given D. 
It is worth mentioning that we choose SC to derive image representations because is has a number of 
attractive properties. First, compared with the standard VQ coding, SC coding can achieve a much lower 
recognition error due to the less restrictive constraint. Second, sparsity allows the representation to be 
specialized, and to capture salient properties of images. Finally, research in image statistics clearly reveals 
that image patches (especially those from natural images) are sparse signals. 
 
B.2 Construction of sparse-coded visual attention map (ScVAM) via visual word ranking 
 
The support vector machine (SVM) is an attractive classifier due to its generalization ability. The SVM is 
able to learn an optimal separating hyperplane to distinguish between two different classes. The derivation of 
the SVM can be posed by the following optimization problem, 
 
  (3) 
Inspired by Guyon et al. [10] and Chang et al. [11], who used the squared or the absolute value of wj in 
(3) to rank the feature attributes in x to achieve feature ranking, we propose to train an one-vs-all SVM to 
separate one object class from others using the sparse-coded visual word histograms learned in B.1. As a 
result, different SVMs are designed to rank class-specific visual words, while those extracted from 
background clutter will be suppressed. Unlike prior work, we do not rank the visual words according to |wj|; 
instead, we ignore the negative wj by setting them to zero, and rank the nonnegative wj accordingly.  
With the ranked visual words, we next construct a visual attention map for each object class. We assign 
the ranking scores (between 0 and 1) of the visual words to their corresponding local dense SIFT descriptors, 
and thus a grayscale mask is built as an example shown in Fig. 3.1c. To obtain a mask with preserves both 
appearance and shape information of the objects of interest, we perform Gaussian smoothing and cross 
bilateral filtering to refine the above grayscale map as our final sparse-coded visual attention map (ScVAM). 
An example of the ScVAM of Fig. 3.1a is shown in Fig. 3.1e. 
 
C. ScVAM for object recognition 
 
We now discuss how we apply the proposed ScVAM to produce a fundamental improvement on the 
popular BOF model for object categorization. Prior work using the standard BOF model assigned equal 
weights to all visual words in an image, and thus the number of occurrences of each visual word is considered 
equally important in classifier learning. We choose to weight the sparse-coded BOF model using our ScVAM, 
so that the visual words spatially associated to each object will be weighted by the associated probability 
value, while those extracted from background clutter will be penalized by a smaller value. 
 7 
of our methods, we also compare our approach with prior work on the Caltech 101 dataset. Our method 
outperforms many previously proposed methods, and we believe that this is because our approach is able to 
extract more salient properties of class-specific visual patterns across different image scales from different 
object categories. It is worth repeating that, different from many previous methods, our MSL framework only 
requires linear kernels and thus provides excellent scalability to large-scale image classification problems. 
 
 
 
4. Conclusion 
 
We presented a novel MSL framework that automatically learns the optimal spatial pyramid image 
representation for visual categorization, which is done by solving a MKL problem which determines the 
optimal combination of base kernels constructed by features pooled from different image scales. Our proposed 
method is able to capture class-specific salient properties of visual patterns in different image scales, and thus 
improves the recognition performance. Among different dictionary learning and pooling strategies, our 
proposed framework based on sparse coding and pyramid max pooling strategies outperforms prior methods 
A Multi-Scale Learning Framework
for Visual Categorization
Shao-Chuan Wang and Yu-Chiang Frank Wang
Research Center for Information Technology Innovation
Academia Sinica, Taipei, Taiwan
Abstract. Spatial pyramid matching has recently become a promising
technique for image classiﬁcation. Despite its success and popularity, no
prior work has tackled the problem of learning the optimal spatial pyra-
mid representation for the given image data and the associated object
category. We propose a Multiple Scale Learning (MSL) framework to
learn the best weights for each scale in the pyramid. Our MSL algorithm
would produce class-speciﬁc spatial pyramid image representations and
thus provide improved recognition performance. We approach the MSL
problem as solving a multiple kernel learning (MKL) task, which deﬁnes
the optimal combination of base kernels constructed at diﬀerent pyra-
mid levels. A wide range of experiments on Oxford ﬂower and Caltech-
101 datasets are conducted, including the use of state-of-the-art feature
encoding and pooling strategies. Finally, excellent empirical results re-
ported on both datasets validate the feasibility of our proposed method.
1 Introduction
Among existing methods for image classiﬁcation, the bag-of-features model [1,2]
has become a very popular technique and has demonstrated its success in re-
cent years. It quantizes image descriptors into distinct visual words, and uses a
compact histogram representation to record the numbers of occurrences of each
visual word in an image. One of the major problems of this is the determination
of visual words, since the widely-used strategy is to cluster local image descrip-
tors into a set of disjoint groups, and thus the representative of each group is
considered as a visual word of the given image data [1,2] (the collection of such
visual words is called a dictionary (or a codebook)). However, the major concern
of this technique is that it discards the spatial order of local descriptors. Lazeb-
nik et al. [3] proposed a spatial pyramid matching (SPM) technique to address
this concern by utilizing a spatial pyramid image representation, in which an
image is iteratively divided into grid cells in a top-down way (i.e. from coarse
to ﬁne scales). Instead of constructing a codebook by vector quantization, Yang
et al. [4] further extended the spatial pyramid representation and proposed a
ScSPM framework with sparse coding of image descriptors and max pooling
techniques. Since only linear kernels are required in their work, Yang’s ScSPM is
able to address large-scale classiﬁcation problems with reasonable computation
time.
R. Kimmel, R. Klette, and A. Sugimoto (Eds.): ACCV 2010, Part I, LNCS 6492, pp. 310–322, 2011.
  Springer-Verlag Berlin Heidelberg 2011
312 S.-C. Wang and Y.-C. F. Wang
the vectors pooled by the sum operation) within each grid is calculated. All
histograms from diﬀerent grids and levels are concatenated with a predetermined
factor (e.g. 1 or 1/22). The ﬁnal concatenated vector is thus considered as the
spatial pyramid representation of the given image. We note that if the coarsest
level  = 0 is used, SPM is simply the standard bag-of-features model.
To the best of our knowledge, existing approaches using SPM for image clas-
siﬁcation simply integrate visual word histograms generated at diﬀerent levels of
the pyramid in an ad hoc way, which might not be practical. We thus propose to
construct the optimal spatial pyramid representation for each class by learning
the weighting factors at each level in the pyramid. Our goal is not only to achieve
better recognition performance, but provides an eﬀective visualization of sematic
and scale information for each object class. While it is possible to use cross valida-
tion to determine the optimal weights for each visual word histogram at diﬀerent
levels in the pyramid, it will signiﬁcantly increases the computation complex-
ity, especially if there is a large number of free parameters to be determined in
the entire system. We note that existing work has utilized diﬀerent learning or
optimization strategies to address this type of problem, and the performance
can be improved without sacriﬁcing the computation load. More speciﬁcally, re-
searchers in machine learning communities have proposed boosting techniques to
select the optimal kernel or feature combination for recognition, regression, etc.
problems [8, 9, 10, 11, 12]. Other methods like metric/similarity learning [13, 14],
distance function learning [15, 16, 17, 18], and descriptor learning [19] also ap-
ply the latest optimization strategies to adaptively learn the parameters from
the data. Recently, one of the successful examples in image classiﬁcation and
kernel learning is the fusion of heterogeneous features proposed by Gehler and
Nowozin [10], and also by Bosch et al. [12]. Gehler and Nowozin proposed to
combine heterogeneous features via multiple kernel learning as well as linear
programming boosting methods (LPBoost), while Bosch fused shape and ap-
pearance features via MKL combined with a regions of interest preprocessing.
Both reported attractive results on Caltech datasets.
Inspired by the above work, we propose to use a MKL framework to identify
discriminating image scales, and thus weight the image representations accord-
ingly. We will show in Sect. 4 that the performance of our proposed framework
outperforms state-of-the-art methods using bag-of-features or SPM models using
predetermined weighting schemes. It is worth repeating that, once the optimal
weights for each scale are determined, one can easily extract signiﬁcant scale
information for each image object class. This provides an eﬀective semantic in-
terpretation for the given image data.
3 Multi-Scale Learning for Image Classification
Previously, Gehler et al. [10] associated image features with kernel functions, and
transformed the feature selection/combination problem into a task of kernel selec-
tion. Similarly, Subrahmanya and Shin [20] performed a feature selection proce-
dure by constructing base kernels using diﬀerent group of features. Our proposed
314 S.-C. Wang and Y.-C. F. Wang
max
b,θ
θ (4)
subject to
∑L
=0 b = 1, b  0,
∑L
=0 bS(a) ≥ θ
∀a ∈ RN with 0  a  C and ∑i yiai = 0,
where S(a) ≡ 12
∑N
ij aiajyiyjk

ij −
∑N
i ai.
Note that the above SILP is actually a linear programming problem due to
the fact that θ and b are linearly constrained with infinite constraints, i.e. there
will be a constraint for each a ∈ RN satisfying 0  a  C and ∑i yiai = 0.
To solve this problem, a wrapper algorithm [23] is proposed to alternatively
optimize a and b in each iteration. When b is ﬁxed, SILP turns into a single
kernel SVM problem, which can be eﬃciently solved by many SVM solvers such
as LibSVM [24]. On the other hand, when a is ﬁxed, we need to solve a linear
programming problem with ﬁnite constraints, which can be also eﬃciently solved
by many linear programming solvers. As a result, this wrapper algorithm enjoys
the beneﬁt of easy and eﬃcient implementation.
Algorithm 1. Multi-scale learning for class-speciﬁc spatial pyramid rep-
resentation
{1}. Building the kernels in all scales:
for  = 0 to L do
for all i, j do
kij ← 〈vi , vj〉 {for linear kernel}
end for
end for
k ← k/Tr(k) {trace normalization}
{2}. Learning b by solving a semi-inﬁnite linear program [23]:
(a, b) ← SILP(y, k)
Note that all kernel matrices have been normalized to unit trace in order to
balance the contributions of base kernels. Algorithm 1 shows our proposed algo-
rithm for learning class-speciﬁc spatial pyramid representations. Note that a, b
in Algorithm 1 are the MKL parameters; a represent the Lagrange multipliers
for SVM, and b describe the optimal weights of each base kernel, indicating the
preferable spatial pyramid image representation for each object category. In our
implementation of the SILP solver, we integrate LibSVM [24] and the MATLAB
function linprog for solving the single kernel SVM and the linear programming
problems, respectively.
After solving the above optimization problem for the given image data, we
obtain the estimated optimal weighting factors b and equivalently acquire the
signiﬁcance of concatenated pooled vectors from diﬀerent scales. This weighted
and concatenated feature vector will be the ﬁnal form for our spatial pyramid
image representation.
316 S.-C. Wang and Y.-C. F. Wang
4.2 Dictionary Learning and Local Descriptor Encoding
We choose two dictionary learning scenarios for comparisons: vector quantiza-
tion (VQ) and sparse coding (SC). We select K = 225 and 900 as the sizes of
the dictionary. To perform sparse coding, we use the SPAMS software package
developed by Mairal et al. [27], and the parameter λ, which controls the sparsity
of the encoded coeﬃcient vectors α, is 0.2. We note that only training images
are involved during the phase of dictionary learning.
4.3 Training
In our experiment, we adopt the one-vs-rest scheme to design multi-class MSL
classiﬁers. Each classiﬁer recognizes one class against all others, and thus learns
the optimal weights b of diﬀerent image scales for the corresponding object
category. Fig. 5 shows a visualization example of the learned b, as well as the
predetermined ones used in prior work. We consider only linear kernels for a
major advantage that the computation complexity for training and testing will
be signiﬁcantly reduced compared to the cases using nonlinear kernels. Therefore,
our proposed method is scalable to large-scale classiﬁcation problems. The only
free parameter to be determined is the regularization term C, and we apply a
ﬁve-fold cross validation to search for its optimal value.
4.4 Results of the Oxford Flower Dataset
To compare our proposed MSL method with existing methods for image clas-
siﬁcation, we consider two diﬀerent bag-of-features models as the baselines: the
standard one without pyramid representation (i.e. level L = 0), and the SPM
which concatenates pooled vectors from each scale with constant weights. Sum
Table 1. Mean average precision (MAP) comparison table for Oxford ﬂower dataset.
L: the maximal level in the spatial pyramid.
Encoding
method
L Pooling method MSL MAP
K=225
MAP
K=900
(a) 0 Sum Pooling No 36.76% 40.00%
(b) 2 Pyramid Sum Pooling No 48.09% 49.26%
VQ (c) 2 Pyramid Sum Pooling Yes 53.68% 55.74%
(d) 0 Max Pooling No 19.12% 40.59%
(e) 2 Pyramid Max Pooling No 55.29% 55.59%
(f) 2 Pyramid Max Pooling Yes 53.82% 57.35%
(g) 0 Sum Pooling No 42.94% 47.50%
(h) 2 Pyramid Sum Pooling No 50.74% 55.00%
SC (i) 2 Pyramid Sum Pooling Yes 55.00% 58.68%
(j) 0 Max Pooling No 40.74% 53.38%
(k) 2 Pyramid Max Pooling No 60.30% 62.79%
(m) 2 Pyramid Max Pooling Yes 60.15% 65.29%
318 S.-C. Wang and Y.-C. F. Wang
Sparse Coding Vector Quantization
Fig. 4. Mean average precision comparison table for the Caltech-101 dataset. K: the
size of dictionary. MSL: Our Multiple Scale Learning (L=3). PSP: Pyramid Sum Pool-
ing (L=3). SP: Sum Pooling (L=0). PMP: Pyramid Max Pooling (L=3). MP: Max
Pooling (L=0). Best viewed in color.
We note that the method PMP in the left column of Fig. 4 is our implemen-
tation of ScSPM, which is proposed by Yang et al. [4]. We did not reproduce
exactly the same results as reported in [4], probably due to the SIFT descriptor
extraction, feature normalization process, etc. engineering details. Therefore, we
choose to use the same implementation details for all methods on both datasets
for comparisons.
To show the competitiveness of our methods, we also compare our approach
with prior work on the Caltech 101 dataset. Our method outperforms many pre-
viously proposedmethods, and we believe that this is because our approach is able
to extract more salient properties of class-speciﬁc visual patterns across diﬀerent
image scales from diﬀerent object categories. It is worth repeating that, diﬀerent
frommany previousmethods, ourMSL framework only requires linear kernels and
thus provides excellent scalability to large-scale image classiﬁcation problems.
Fig. 5 illustrates the values of b for all 101 object classes using diﬀerent
methods. The top row is the case of the standard bag-of-features model. Since
320 S.-C. Wang and Y.-C. F. Wang
promising results were reported [10,11,12,33]. Our MSL framework can be easily
combined with these ideas, since multiple feature descriptors can be integrated
into our proposed framework and can still be solved by MKL techniques. In such
cases, we expect a signiﬁcantly greater improvement on the recognition accuracy
over state-of-the-art classiﬁcation methods.
5 Conclusion
We presented a novel MSL framework that automatically learns the optimal
spatial pyramid image representation for visual categorization, which is done
by solving a MKL problem which determines the optimal combination of base
kernels constructed by features pooled from diﬀerent image scales. Our proposed
method is able to capture class-speciﬁc salient properties of visual patterns in
diﬀerent image scales, and thus improves the recognition performance. Among
diﬀerent dictionary learning and pooling strategies, our proposed framework
based on sparse coding and pyramid max pooling strategies outperforms prior
methods on Oxford ﬂower and Caltech 101-datasets. In addition, through the
visualization of the weights learned for each image scale and for each object
category, our MSL framework produces a class-speciﬁc spatial pyramid image
representation, which cannot be achieved by the standard SPM. Finally, since
only linear kernels are required in our proposed learning framework, our method
is computationally feasible for large-scale image classiﬁcation problems.
Acknowledgement. We are grateful for the anonymous reviewers for their help-
ful comments. This work is supported in part by the National Science Council
of Taiwan under NSC98-2218-E-001-004 and NSC99-2631-H-001-018.
References
1. Csurka, G., Bray, C., Dance, C., Fan, L.: Visual categorization with bags of key-
points. In: ECCV Workshop on Statistical Learning in Computer Vision, pp. 1–22
(2004)
2. Jurie, F., Triggs, B.: Creating eﬃcient codebooks for visual recognition. In: ICCV
2005: Proceedings of the Tenth IEEE International Conference on Computer Vision
(ICCV 2005), Washington, DC, USA, vol. 1, pp. 604–610. IEEE Computer Society,
Los Alamitos (2005)
3. Lazebnik, S., Schmid, C., Ponce, J.: Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories. In: CVPR 2006: Proceedings of
the 2006 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition, Washington, DC, USA, pp. 2169–2178. IEEE Computer Society, Los
Alamitos (2006)
4. Yang, J., Yu, K., Gong, Y., Huang, T.S.: Linear spatial pyramid matching using
sparse coding for image classiﬁcation. In: CVPR 2009: Proceedings of the 2009
IEEE Computer Society Conference on Computer Vision and Pattern Recognition,
pp. 1794–1801. IEEE Computer Society, Los Alamitos (2009)
322 S.-C. Wang and Y.-C. F. Wang
21. Bach, F.R., Thibaux, R., Jordan, M.I.: Computing regularization paths for learning
multiple kernels. In: Saul, L.K., Weiss, Y., Bottou, L. (eds.) Advances in Neural
Information Processing Systems 17, pp. 73–80. MIT Press, Cambridge (2005)
22. Rakotomamonjy, A., Bach, F., Canu, S., Grandvalet, Y.: More eﬃciency in multiple
kernel learning. In: ICML 2007: Proceedings of the 24th International Conference
on Machine Learning, pp. 775–782. ACM, New York (2007)
23. Sonnenburg, S., Ra¨tsch, G., Scha¨fer, C., Scho¨lkopf, B.: Large scale multiple kernel
learning. J. Mach. Learn. Res. 7, 1531–1565 (2006)
24. Chang, C.C., Lin, C.J.: LIBSVM: a library for support vector machines (2001),
http://www.csie.ntu.edu.tw/~cjlin/libsvm
25. Nilsback, M.E., Zisserman, A.: A visual vocabulary for ﬂower classiﬁcation. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
vol. 2, pp. 1447–1454 (2006)
26. Fei-Fei, L., Fergus, R., Perona, P.: Learning generative visual models from few
training examples: An incremental bayesian approach tested on 101 object cate-
gories. Comput. Vis. Image Underst. 106, 59–70 (2007)
27. Mairal, J., Bach, F., Ponce, J., Sapiro, G.: Online dictionary learning for sparse
coding. In: ICML 2009: Proceedings of the 26th Annual International Conference
on Machine Learning, pp. 689–696. ACM, New York (2009)
28. Raina, R., Battle, A., Lee, H., Packer, B., Ng, A.Y.: Self-taught learning: transfer
learning from unlabeled data. In: ICML 2007: Proceedings of the 24th International
Conference on Machine Learning, pp. 759–766. ACM, New York (2007)
29. Berg, A.C., Berg, T.L., Malik, J.: Shape matching and object recognition using
low distortion correspondence. In: CVPR 2005: Proceedings of the 2005 IEEE
Computer Society Conference on Computer Vision and Pattern Recognition, vol. 1,
pp. 26–33. IEEE Computer Society, Los Alamitos (2005)
30. Mutch, J., Lowe, D.G.: Multiclass object recognition with sparse, localized features.
In: CVPR 2006: Proceedings of the 2006 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition, Washington, DC, USA, pp. 11–18.
IEEE Computer Society, Los Alamitos (2006)
31. Zhang, H., Berg, A.C., Maire, M., Malik, J.: SVM-KNN: Discriminative nearest
neighbor classiﬁcation for visual category recognition. In: CVPR 2006: Proceedings
of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition, pp. 2126–2136. IEEE Computer Society, Los Alamitos (2006)
32. Lin, Y.Y., Liu, T.L., Fuh, C.S.: Local ensemble kernel learning for object category
recognition. In: CVPR 2007: Proceedings of the 2007 IEEE Computer Society
Conference on Computer Vision and Pattern Recognition, pp. 1–8. IEEE Computer
Society, Los Alamitos (2007)
33. Cao, L., Luo, J., Liang, F., Huang, T.S.: Heterogeneous feature machines for visual
recognition. In: ICCV 2009: Proceedings of the Tenth IEEE International Confer-
ence on Computer Vision (ICCV 2009). IEEE Computer Society, Los Alamitos
(2009)
表 Y04 
絡繹不絕，更有人在聽我講解完並回答問題之後，再回到現場與本人進行第二輪 Q&A。
相信藉由此親身參與國際會議，呈現學術成果並與與會者積極互動之方式，可促進中央
研究院與國際間各大學系所的研究學者們共同交流與合作，期望能為台灣在影像處理領
域有所貢獻，並且提升台灣在國際間的研究實力與能見度。 
 
三、考察參觀活動(無是項活動者省略) 
無。 
 
四、建議 
本次開會期間，本人有機會與國外知名學者如 Prof. Stephane Mallat共同討論，對於國外
在多媒體信號分析、機器學習領域的研究與發展，進行意見交換。同時也提到日後台灣
爭取主辦相關國際會議的可能性，以達到提升台灣研究知名度的目標。總結來說，這次
參加跟我研究領域極為相關的研討會可以說是收穫良多，不僅吸收到許多最新的研究方
法與應用，也感受到主辦單位之用心與規劃，此經驗更可以作為日後協助國內舉辦會議
的參考。
 
 
五、攜回資料名稱及內容 
大會議程與論文集之光碟一份。 
 
六、其他 
無。 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：王鈺強 計畫編號：99-2221-E-001-020- 
計畫名稱：以稀疏編碼之影像與視頻物體擷取技術進行人類動作識別 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 4 4 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
