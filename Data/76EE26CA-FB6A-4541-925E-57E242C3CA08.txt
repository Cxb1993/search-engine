 2
Abstract 
The artificial intelligence aid classification of proteins helps bio-researchers to 
shorten their experiments, for the prediction can exclusive those wrong answers. But 
how to separate the right and the wrong properly becomes an important task.  
In this work, we based on our prior researches and combine SVM with ECOC to 
develop an efficient classifier and apply to the classification of proteins structure. 
The ECOC SVM is an approach for solving multi-class classification problem by 
reducing it into a group of binary classification tasks and combines the binary 
classification results to predict the multi-class target labels. First, the ECOC SVM 
assigns a unique binary string of length n, called ‘codeword’, for every class to 
distinguish each other. Then, the n binary classifiers are trained, one for each bit 
position in the codeword. To improve the performance of each binary classifier, a 
gradient descent method is used to determine the better penalty parameters and kernel 
parameters of SVM adaptively. After the training phase, a set of independent binary 
SVM classifiers, with their parameters, are constructed automatically.  In the testing 
phase, the unlabeled data are predicted by evaluating each output result of each binary 
classifier and finding the closest vector in the ECOC matrix.  
We apply the ECOCSVM to the protein fold classification problem in SCOP 
with HLA, which has proposed by us. Experimental results show that the proposed 
scheme can achieve good classification accuracy. To further enhance the overall 
accuracy, different codeword of different length are also applied to analyze the 
classification performance and to obtain better results.   
 
Key Words: SVM, ECOC, HLA, SCOP, bioinformatics, protein 
 
 
 4
Table 1. The descriptors and feature dimension sizes of each of the six protein attributes 
(protein sequence information --- PSI). 
2020 kinds of am ino acidsC om position (C )
W eak
N egative
Sm all
N egative
L oop
M iddle
N eural
M iddle
N eural 
B eta
125T otal N um ber 
21StrongPolarizability (Z)
21PositivePolarity  (P)
21L argeV olum e (V )
21PositiveH ydrophobicity (H )
21A lphaPredicted Secondary  
Structure (S)
Feature SizeD escrip torsC haracteristics
 
Support Vector Machine (SVM) is a discriminative method based on the 
statistical learning theory and has been widely used in many applications including 
the complex problems of bioinformatics [1,2,5,6]. In this research, a different 
multi-class support vector machine algorithm, called self-tuning error correcting 
output coding (ECOC) SVM is proposed to deal with the multi-class protein fold 
classification problem. The ECOC SVM is an approach for solving multi-class 
classification problem by reducing it into a group of binary classification tasks and 
combines the binary classification results to predict the multi-class target labels.  
 
2. Proteins, Protein databank 
Structure Classification of Protein (SCOP) is a famous protein databank, which 
uses the evolution and similarity of proteins to classify the structure of proteins. The 
data structure of SCOP is found according to the hierarchical structure of proteins. In 
the SCOP, the main classes are divided into several classes. The main classes, with 
most numbers of protein, are all alpha(α), all beta(β), alpha/beta (α/β) and alpha 
and beta (α+β). These four classes are named by the structure of proteins [7, 8, 9, 
10]. The protein classification in SCOP was performed manually or 
semi-automatically, which takes a great amount of time for such a complex task.  
 6
3. SVM, Support Vector Machines 
Support Vector Machine (SVM) is a typical two-class classifier and a kind of 
universal feedforward network. The SVM will construct a hyperplane in a 
high-dimensional features space as the decision surface between positive and negative 
patterns. The structural risk minimization ability makes the SVM a very efficient 
classifier in various applications including biosequences analysis also [11,12,13,14].  
With the further improvements by other researchers recently, the SVM has the 
ability to do multi-class classification directly, which is the model adopted here in our 
HLA as the constituent multi-class classifiers. [15]  
Given the training set S = {(x1, y1), (x2, y2), …, (xl, yl)} with explanatory 
variables di ∈x R  and the corresponding binary class labels { 1, 1}iy ∈ − + , for all 
1, ,i l= " , where l is the number of data, and d is the dimension of the problem, we 
wish to find a separating d-dimensional hyperplane described by 
 0 =0b⋅ +w x , (1) 
where 1 2[ , , , ]lw w w=w "  is the set of linear weights,  
1 2[ , , , ]l=x x x x"  is the input dataset, and  
b0 is the constant.  
The separation problem is to determine the hyperplane such that 0 1b⋅ + ≥ +w x  for 
positive examples and 0 1b⋅ + ≤ −w x  for negative examples. If it is separated 
without error and the distance between the closest vector to the hyperplane is maximal, 
this set of vectors is separated by the optimal hyperplane. In this connection, the SVM 
is used to find a hyperplane to maximize the function M as follow:   
 
2M
W
= & & . (2) 
The solution of Eq. (2) must satisfy the following constraints of inequality type 
 8
In practical applications for real-life data, the two classes are not completely 
separable and the separating plane is always a nonlinear function of the data. So the 
idea of feature space is discovered to solve this problem. The idea in designing a 
nonlinear SVM is to map an input vector d∈x R  into a vector z of a 
higher-dimensional feature space F ( ( )φ=z x , where φ  represents a mapping 
d f→R R ), and to solve a linear classification problem in this feature space: 
      1 1 2 2( ) [ ( ), ( ), , ( )]
d T f
n nR z a a a Rφ φ φ∈ → = ∈x x x x x" , 
where na  is constant. Since the computation of the dot products is prohibitive if the 
dimension of transformed training vectors ( )iφ x  is very large, and since ( )iφ x  is 
not known a priori, the Mercer’s theorem for positive definite functions allows to 
replace ( ) ( )i jφ φ⋅x x  by a positive definite symmetric kernel function ( , )i jk x x , i.e., 
( , ) ( ) ( )i j i jk φ φ= ⋅x x x x . So we need to select a kernel function and then solve the 
following dual quadratic optimization in order to obtain an optimal hyperplane for any 
linear or nonlinear space: 
 maxα   ( ) ( )1 ,2dL y y Kα α α α= −∑ ∑ x x
l l
i i j i j i j
i=1 i, j=1
 
 subject to  0 ,  1,2,...,C i lα≤ ≤ =i  and 0yα =∑l i i
i =1
. (8) 
The indicator function is   
 0 0
1
( ) sign ( )
svN
i i i
i
f y k bα Θ
=
⎛ ⎞= ⋅ +⎜ ⎟⎝ ⎠∑x x x , (9)  
where a kernel kΘ  depends on a set of parameters Θ . 
4. Error-Correcting Output Coding (ECOC) 
Error-correcting output coding (ECOC) is an approach for solving multi-class 
categorization problems [16]. It reduces the multi-class classification problem to a 
 10
other multi-class SVM classifiers. However, it will be affected by some factors such 
as performance of the composing binary classifiers, independence of the binary 
classifiers, and the loss function. In the next section, an ECOC SVM classifier will be 
proposed to improve the classification accuracy and design efficiency of the ECOC 
classifier. 
 
Classes Code words (each column for one binary) ""   
0 -1 -1 -1 -1 ""  +1 
1 -1 +1 -1 +1 "" -1 
2 +1 -1 +1 -1 "" +1 
3 -1 -1 +1 -1 "" -1 
#  #  #  #  #  ""  #  
m -1 +1 -1 -1 ""  -1 
 
Fig. 1. Error Correcting Output Code matrix, where m is the number of classes (rows) 
      and l is the number of classifiers (columns). 
 
5. The ECOCSVM Classifier 
Since the accuracy of an ECOC SVM classifier is highly affected by the 
performance of its composing binary classifiers, the performance of a binary SVM 
classifier is determined by several parameters such as the penalty parameter C and 
kernel parameters. The penalty parameter C controls the tradeoff between margin 
maximization and error minimization. The kernel parameters determine the proper 
and efficient non-linear mapping from pattern space into feature space. We find out 
that the best penalty parameter C and kernel parameters to make every binary SVM 
classifier have the best classification function so that the classification mistakes can 
be eliminated. Besides, the accuracy of the whole ECOC SVM classifier can be 
increased globally. 
 
 12
This estimate is given as follows: 
           
1
1 ( ( ))
N
i i
i
E y f
N =
= Ψ −∑ x , (13) 
where Ψ  is the step function:Ψ (x) = 1 when x > 0 and Ψ (x) = 0, otherwise; and N 
is the size of the data set.  
The goal is to find the values of the parameters α  and Θ  such that margin M is 
maximized and the generalization error E is minimized. With the parameter Θ  fixed, 
we can obtain 0 arg max ( )Wα α=  and then choose Θ  as 
        arg min ( , )E αΘΘ = Θ . (14) 
Because the step function Ψ  is not differentiable, we cannot use a gradient 
descent method to minimize the estimates of generalization errors. To circumvent this 
problem, Platt proposed the following estimate of the posterior distribution 
( 1 )P Y X= = x  of an SVM output: 
       1( 1 )
1 exp( ) )
P Y X
Af B
= = = + +x (x , (15) 
where ( )f x  is the output of the SVM. The constants A and B in the above equation 
are found by minimizing the Kullback-Leibler divergence between P and an empirical 
approximation of P built from a training set S={(x1, y1), (x2, y2), …, (xn, yn)} ∈ X, 
containing n examples: 
        * *
, 1
1 1( , ) arg max ( log( ( )) log(1 ( ))
2 2
n
i i
i iA B i
y yA B p p
=
+ −= × + × −∑ x x . (16) 
The error probability of either target value for a given data example ix is formulated by  
             1( ) (1 )i it ti i i i iE p y z p p
−= ≠ = − , (17) 
where ( )i i iz f f x= =  is the corresponding SVM output value, ip  is the estimated 
posterior probability, and it  is that 1it =  if the input vector ix  belongs to class 
 14
by the gradient descent method 
    ( , )rr
r
E α θθ ε θ
∂∆ = − ∂ , (25) 
where ε  is the amplitude of the step along the search direction.  
 
6. The measurement of Accuracy 
In our HLA classification approach, such confusing conditions will not happen. 
Therefore, the accuracy measurement in our experiments is quite clear and simple. Let 
us use a function A (accuracy) to indicate the classification correctness of a protein 
pattern fed into the HLA. Then the total number of correctly classified proteins can be 
expressed as: 
                
( 2 | 1)
( 2) ( 1 2)
O A level level
A level A level level
=
= ∩                          (26) 
where A is a conditional function whose value is 1 only when a protein pattern is 
correctly classified by the classifiers in both Level 1 and Level 2 of the HLA, and is 0.  
Based on the above concepts, the accuracy measurement of the proposed approach 
is defined as follows. If the number of testing proteins belonging to the thiF  fold is in , 
but the tested classifier only recognizes io  proteins as the
th
iF fold, then the accuracy 
rate of this tested classifier is set as i
i
o
n
 for the thiF fold. The total classification 
accuracy can be briefly calculated as follows. 
      1 2 3
1
... i i
i
N n n n n n
=
= + + + + =∑   (in this case, i=27, N=385)            (27) 
     1 2 3
1
... i i
i
O o o o o o
=
= + + + + =∑    (in this case, i=27)                   (28) 
         OQ
N
=                                                     (29) 
where N is the total number of testing proteins data, O is the total number of correctly 
classified proteins in Eq. (26), and Q is the classification (prediction) accuracy. 
 16
In Table 4, the single-layer approach is used to directly classify each test input 
vector into 27 folds. It can be seen that the performance of proposed method can have 
good accuracy when compared to other methods.  
 
Table 6. Classification accuracies of the ECOC-SVM-based HLA with various 
combinations of global features (C+H+S+P+V+Z) and local features 
(bi-gram coded feature (B) and spaced bi-gram coded feature (SB)). 
    ECOC-Based HLA     
Features Global features(6PSI) PSIs+B PSIs+B+SB 
No. of Features   125 125+441 125+441+441
Accuracy of Level 1   80.26 83.38 84.94 
Group 1 70.49 78.69 83.61 
Group2 54.7 66.67 71.79 
Group 3 40.32 50 56.45 
Accuracy of Level 2 
Group 4 38.71 50 56.45 
Overall Accuracy(%) 56.1 65.45 68.57 
 
Table 7. Classification accuracies of the RBFN-based HLA with various combinations 
of global features (C+H+S+P+V+Z) and local features (bi-gram coded 
feature (B) and spaced bi-gram coded feature (SB)). 
8 3 .68 3 .17 9 .28 1 .6A cc u ra c y  o f L e v e l 1
6 9 .06 2 .86 0 .05 8 .6C la s s  3
C la s s  4
C la s s  2
C la s s  1
5 6 .4
4 8 .4
5 2 .1
6 7 .2
1 2 5
G lo b a l 
fe a tu re s
(6  P S Is )
5 8 .2
5 6 .5
5 6 .4
5 9 .0
4 4 1
L o c a l 
fe a tu re  B
6 3 .7
5 4 .8
6 2 .4
7 7 .0
1 2 5 + 4 4 1
P S Is +  B
6 5 .5
5 3 .2
6 3 .2
7 3 .8
1 2 5 + 4 4 1 + 4 4 1
P S Is +  B  +  S B
O v e ra ll A c cu ra c y  (% )
A cc u ra c y  o f 
L e ve l 2  (% )
N o . o f F e a tu re s
F e a tu re s
R B F N -B a s e d  H L A
 
 
 
 18
9. References 
[1] P. Baldi and S. Brunak, Bioinformatics: the Machine Learning Approach, MIT 
Press, 1998. 
[2] C. H. Wu, Neural networks and genome informatics, Elsevier, U.K., 2000. 
[3]  J. Yang, R. Parehk, V. Honavar, and D. Dobbs, “Data driven theory refinement 
algorithms for bioinformatics,” IJCNN ’99 International Joint Conference, Vol. 6, 
pp. 4064-4068, July 1999. 
[4] C. D. Huang, Hierarchical Learning Architecture for Multi-class Protein Fold 
Classification Based on Neural Networks and Support Vector Machine, Doctoral 
Dissertation, NCTU, HsinChu, Taiwan, 2004. 
[5] I. Dubchak and C. H. Q. Ding, “Multi-class protein fold recognition using support 
vector machines and neural networks,” Bioinformatics, Vol. 17, No. 4, pp. 349-358, 
2001.  
[6] I. Dubchak, I. Muchnik, S. R. Holbrook, and S. H. Kim, “Prediction of protein 
folding class using global description of amino acid sequence,” Proc. Natl. Acad. 
Sci., USA, Vol. 92, pp. 8700-8704, 1995. 
[7] A. G. Murzin, S. E. Brenner, T. Hubbard, and C. Chothia, “SCOP: A structural 
classification of proteins database for the investigation of sequence and structures,” 
Journal of Molecular Biology, Vol. 247, pp. 536-540, 1995. 
[8] I. Dubchak, I. Muchnik, C. Mayor, I. Dralyuk, and S. H. Kim “Recognition of a 
protein fold in the context of the SCOP classification,” PROTEINS: Structure, 
Function, and Genetics, Vol. 35, pp. 401-407, 1999. 
[9] L. L. Conte, B. Ailey, T. J. Hubbard, S. E. Brenner, A. G. Murzin, and C. Chothia, 
“SCOP: a structural classification of proteins database,” Nuclear Acid Research, 
Vol. 28, No. 1, pp. 257-259, 2000. 
