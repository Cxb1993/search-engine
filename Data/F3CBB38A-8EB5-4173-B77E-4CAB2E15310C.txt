 II
中文摘要 
 
我們研究解隨機化相關問題。虛擬亂數產生器(Pseudo-random number generator)在
下列問題中扮演了重要的角色：是否每一個在問題類 BPP 中的問題都有一個多項式時間的
確定式演算法能解決。(稱為 BPP 的解隨機化。) 為了構造偽亂數產生器，通常我們需要經
由一些函數難度上的假設出發去建構此產生器。在本計畫，我們將探討不同的函數難度上
的假設與偽亂數產生器之關係。我們特別探討下列主題： 
(ㄧ) 難度放大器之構造的不可能結果； 
(二) 在 NP 問題類之難度放大問題與構造； 
(三) 建立虛擬亂數產生器與不同函數難度上的假設間之關係； 
(四) 對於更限制一類電路分辦者，構造一虛擬亂數產生器使其無法分辦。 
 
 
關鍵詞：隨機計算(randomized computation)、難度放大(hardness amplification)、
解隨機化(derandomization)、虛擬亂數產生器(Pseudo-random number 
generator) 
 
 
 
 IV
Index 
1 Introduction (前言) 1 
2 Goal of Research (研究目的)                                3 
3 Previous Results (文獻探討) 3 
4 Our Results (研究方法與結果) 5 
4.1 Strongly Black-Box Hardness Amplification 5 
4.2 Mild Hardness and Pseudorandomness 6  
4.3 Weakly Black-Box Hardness Amplification 7 
4.4 Hardness Amplification within NP                       7 
4.5 Jensen-Shannon Divergence and Variational Distance     7 
5 Reference (參考文獻) 8 
6 Appendix: Self-Evaluation (附錄: 自評) 10 
 
 
 
 
 
function which is hard against uniform Turing machines. This is why most results on hardness
amplification are based on non-uniform assumptions (except [IW98, TV02]).
It is also known that from a PRG, one can obtain a worst-case hard function [NW94]. Therefore,
in a high complexity class such as E, the notions of pseudrandomness and various degrees of hardness
are equivalent. However, when we go down to a lower complexity class, such as NP, the picture
among worst-case hardness, average-case hardness, and pseudorandomness is no longer clear. In
fact, all the known transformations from a worst-case hard function to a mildly hard one or to a
PRG require exponential time (or linear space) [IW97, STV01, KM99, ISW00, SU01, Uma03], and
it appears very difficult to bring down the complexity.
1.2 Strongly and Weakly Black-Box Constructions.
According to the prior discussion, we would like to show that some kind of hardness amplification
or PRG construction is indeed impossible. For this, we need to clarify what type of hardness am-
plification we are talking about, especially given the possibility that an average-case hard function
may indeed exist.
One important type of hardness amplification is the (strongly) black-box hardness amplification
from ε-hardness to ε¯-hardness. First, the initial function f is given as a black box to construct the
new function f¯ , in the sense that there is an oracle Turing machine Amp(·) such that f¯ = Ampf .
Furthermore, the ε¯-hardness of the new function f¯ is also guaranteed in a black box way, in the
sense that there is another oracle Turing machine Dec(·) such that given any adversary A which
computes f¯ correctly on at least (1 − ε¯) fraction of inputs, Dec using A as oracle can compute f
correctly on at least (1− ε) fraction of inputs. We call Amp the encoding procedure and Dec the
decoding procedure. In fact, almost all previous constructions of hardness amplification were done
in such a black-box way [Yao82, BFNW93, GNW95, Im95, IW97, STV01, KM99, OD02, HVV04].
As we will see, such type of hardness amplification has its limitation.
One relaxation is the so-called weakly black-box hardness amplification, in which the hardness
proof is no longer required to be done in a black-box way (dropping the requirement of having a
decoding procedure). Precisely, its hardness proof is only to show the following statement: if there
is an efficient adversary A computing f¯ correctly on at least (1− ε¯)-fraction of inputs, then there
exists an efficient adversary B which computes the initial function f on at least (1− ε) fraction of
inputs. Note that the analysis is arbitrary and hence is not necessarily restricted in a black-box way.
In this sense, this weakly model is a natural relaxation of strongly black-box model. The difference
between strongly and weakly black-box models is remarkable especially when an average-case hard
function indeed exists. A hardness proof of the weakly black-box model may just to show that
the resulting function f¯ is close to that average-case hard one. Hence this sufficiently fulfills the
statement of hardness proof. However, this proof approach is not allowed for the strongly black-box
model. Again, as we will see, the weakly black-box hardness amplification also has its limitation
when it is unable to embed any average-case (or mildly) hard function in itself.
Similarly, one can consider black-box construction of a PRG from a hard function, which builds
a PRG from a hard function in a black-box way. Again, almost all previous PRG constructions
were done in such a black-box way [IW97, STV01, KM99, ISW00, SU01, Uma03]. We will also
consider black-box construction of a hard function from a PRG, which builds a hard function from
a PRG in a black-box way. The construction of [NW94] was indeed a black-box one. Now we can
back to the relationship among pseudorandomness and various degrees of hardness within NP (or
PH).
2
a PRG from an average-case hard function [NW94]. In fact, each of them can be done in a strongly
black-box way, with both the encoding and decoding procedures realized in P.
Finally, to complete the circle, one can transform a PRG back to a worst-case hard function
[NW94]. Note that the above transformation can be done in a black-box way, in which the decoding
procedure is realized in P while the encoding procedure needs the complexity of NP. This raises
the following two questions. First, can the complexity of the encoding procedure be reduced?
Next, since the transformation from worst-case hardness to average-case hardness seems to require
high complexity, can we transform a PRG directly into an average-case hard function, using a
low-complexity procedure, say in NP (or even in P)?
3.2 Hardness Amplification within NP
It remains open for lower complexity classes. In fact, there are results showing that the same
techniques used for high complexity classes can not be used for the class NP to obtain average-
case hardness when starting from worst-case hardness [BT03] or even starting slightly below mild
hardness [Vio04, LTW05, Vio05a].
So we focus on the task of transforming mild hardness to average-case hardness for the com-
plexity class NP. One attempt is to use Yao’s XOR lemma [Yao82, GNW95], which trans-
forms a given function f : {0, 1}n → {0, 1} into a function f ′ : ({0, 1}n)k → {0, 1} defined by
f ′(x1, · · · , xk) = f(x1)⊕ · · · ⊕ f(xk). However, we do not know if this works here, since we do not
know if NP is closed under the XOR operation. O’Donnell [OD02] gave the first result along this
line, showing how to convert any balanced function f ∈ NP which is mildly hard for polynomial-size
circuits into another f ′ ∈ NP which is (1/2 − 1/n1/2−α)-hard for polynomial-size circuits, for any
constant α > 0. He considered transformations of the form: f ′(x1, . . . , xk) = C(f(x1), . . . , f(xk)),
where C is a polynomial-time computable monotone function. Then he used the “tribes” function
and the “recursive majority” function, and took their composition as the function C. Recently,
Healy et al. [HVV04] were able to amplify hardness beyond 1/2−1/poly(n), showing how to convert
any balanced function in NP which is mildly hard for circuits of size s(n) into one in NP which is
(1/2− 1/s′(n))-hard for circuits of size s′(n), with s′(n) = s(n1/2)Ω(1). In particular, s′(n) = nω(1)
when s(n) = nω(1), s′(n) = 2nΩ(1) when s(n) = 2nΩ(1) , and s′(n) = 2Ω(n1/2) when s(n) = 2Ω(n).
A key source of their improvement came from derandomizing O’Donnell’s proof (the other source
being nondeterminism). They observed that the function C used by O’Donnell can be computed by
a small-size read-once branching program and thus can be fooled by the pseudorandom generator
of Nisan [Nis91]. Unfortunately, this generator becomes the bottleneck of their approach when
s(n) = 2Ω(n), which prevents them from achieving the goal of having s′(n) = 2Ω(n).
3.3 Pseudorandom Generators fooling Restricted Classes of Circuits
A pseudorandom generator (PRG) is an efficiently computable function which maps a short random
input to a long pseudorandom output which is hard for small boolean circuits to distinguish from
truly uniform distribution. Pseudorandom generators play an important role in the theory of
computation such as derandomization of randomized algorithms. The first pseudorandom generator
for derandomization is constructed by Nisan and Wigderson [NW94] based on a boolean function
which is hard for subexponential circuits to approximate. However it is unknown about the existence
of such a hard function so far.
4
say with (1− δ)/2 = Ω(1), it fails to give a meaningful bound. We can cover this case: our result
implies that no AC0 circuit can realize a black-box hardness amplification, say, from hardness 1/3 to
hardness 1/2+2−nΩ(1) . On the other hand, our result when restricted to worst-case to average-case
hardness amplification is incomparable to those of [BT03] and [Vio05a]. Finally, two interesting
facts follow from our result. First, it is impossible to produce in a black-box way a function which
is (1 − δk)/2–hard against a uniform low complexity class, say DTIME(O(1)), even if we start
from a function which is (1 − δ)/2–hard against a uniform but arbitrarily high complexity class
equipped with an advice of length 2o(n), say DTIME(22
n
)/2o(n). On the other hand, it is easy to
show that hard functions against DTIME(O(1)) do exist. This demonstrates one severe weakness of
black-box hardness amplifications. Second, when amplifying hardness from (1− δ)/2 to (1− δk)/2,
the complexity of such amplification is determined mainly by the parameter k; a larger value of k
results in a higher complexity requirement, for typical values of δ.
Note that our lower bound above vanishes for d = Ω(log k). Our second result deals with this
issue. We show that if the encoding procedure is computed by a nondeterministic circuit of size
o(k/ log k), even with arbitrary depth, the decoding procedure Dec must need an advice of length
2Ω(n). As a result, in nondeterministic polynomial time, one can not amplify hardness from (1−δ)/2
to hardness (1− δk)/2 for any super-polynomial k (for example, from hardness below 1/poly(n) to
hardness Ω(1)).
Our third result shows that even without any complexity constraint on the encoding or decoding
procedure, amplification between certain range of hardness is still inherently non-uniform. One can
derive this for the case of amplifying hardness beyond 1/4, using Plotkin bound [Plo60] from coding
theory. We obtain a quantitative bound on the non-uniformity for a general range of hardness
amplification. We show that to amplify from hardness (1−δ)/2 to hardness (1−ε)/2, the decoding
procedure Dec must need an advice of Ω(log(δ2/ε)) bits. Thus, when ε = δk, an advice of length
Ω(k log(1/δ)) is necessary, and when ε ≤ cδ2 for some constant c, such hardness amplification must
be inherently non-uniform. Our result generalizes that of Trevisan and Vadhan [TV02].
Finally, we derive similar lower bounds on black-box constructions of PRG from any function
with a given hardness.
4.2 The Relationship between Mild Hardness and Pseudorandomness
Our result provides strongly black-box constructions of average-case hard functions from PRGs.
As a result, we build the equivalence between PRG and average-case hardness within NP Then, we
also derive negative results for weakly black-box hardness amplification. Therefore, we widen the
gap between worst-case and mild hardness within NP.
First, we give strongly black-box constructions of average-case hard functions from PRGs. The
first construction has the encoding procedure realized in NP and the decoding procedure realized
in P/poly (or randomized polynomial time). This improves the result of [NW94] which, using an
encoding procedure in NP as well, obtains only a worst-case hard function. A natural question then
is: can we further reduce the complexity of the encoding procedure, or can we prove a complexity
lower bound? We give a partial answer to this by providing another strongly black-box construction
with the encoding procedure realized in P but at the expense of increasing the complexity of the
decoding procedure to NP, which rules out the possibility of proving a complexity lower bound
for the encoding procedure without restricting the complexity of the decoding procedure. This
still leaves open the question of whether or not one can have both the encoding and decoding
procedures realized in P. Our positive results also imply some impossibility results. By combining
6
graph based extractor. Finally we show that the useful parity lemma [Vaz87] in studying pseudo-
randomness does not hold in the new metric.
References
[BFNW93] La´szlo´ Babai, Lance Fortnow, Noam Nisan, and Avi Wigderson. BPP has subexponen-
tial time simulations unless EXPTIME has publishable proofs. Computational Complex-
ity, 3(4), pages 307–318, 1993.
[BM82] Manuel Blum and Silvio Micali. How to generate cryptographically strong sequences of
pseudo random bits. In Proceedings of the 23rd Annual IEEE Symposium on Foundations
of Computer Science, pages 112–117, 1982.
[BT03] Andrej Bogdanov and Luca Trevisan. On worst-case to average-case reductions for NP
problems. In 44th Annual Symposium on Foundations of Computer Science, Cambridge,
Massachusetts, pages 11–14, 2003.
[DS03] D. M. Endres and J. E. Schindelin. A New Metric for Probability Distributions. IEEE
Transaction on Information Theory, vol 49, pp.1858-60. July 2003.
[FSS84] Merrick L. Furst, James B. Saxe, and Michael Sipser. Parity, circuits, and the
polynomial-time hierarchy. Mathematical Systems Theory, 17(1), pages 13–27, 1984.
[GNW95] Oded Goldreich, Noam Nisan, and Avi Wigderson. On Yao’s XOR lemma. Technical
Report TR95–050, Electronic Colloquium on Computational Complexity, 1995.
[GV05] Venkatesan Guruswami and Salil Vadhan. A lower bound on list size for list decoding.
In Proceedings of the 8th International Workshop on Randomization and Computation
(RANDOM ‘05), pages 318–329, 2005.
[Has86] Johan H˚astad. Computational limitations for small depth circuits. PhD thesis, MIT
Press, 1986.
[HVV04] Alexander Healy, Salil P. Vadhan, and Emanuele Viola. Using nondeterminism to am-
plify hardness. In Proceedings of the 36th ACM Symposium on Theory of Computing,
pages 192–201, 2004.
[Im95] Russel Impagliazzo. Hard-core distributions for somewhat hard problems. In Proceedings
of the 36th Annual IEEE Symposium on Foundations of Computer Science, pages 538–
545, 1995.
[ISW00] Russell Impagliazzo, Ronen Shaltiel, and Avi Wigderson. Extractors and pseudo-random
generators with optimal seed length. In Proceedings of the 32nd ACM Symposium on
Theory of Computing, pages 1–10, 2000.
[IW97] Russel Impagliazzo and Avi Wigderson. P=BPP if E requires exponential circuits: de-
randomizing the XOR lemma. In Proceedings of the 29th ACM Symposium on Theory
of Computing, pages 220–229, 1997.
8
