 I 
語意網路本體知識選擇、建立及整合技術之研究
(2/3) 
 
摘要 
 
 如何有效地處理來自不同來源的知識本體，並將他們合併成一個有效、可
再利用與整合的知識本體倉儲，是一個有趣且重要的研究議題。在本研究中，
我們提出一個知識本體倉儲的架構，來整合許多單一的知識本體，以構成一個
更有用的知識本體。在這個架構中包含了分析、選擇、萃取、轉換、整合、知
識本體倉儲、知識本體市集與應用等模組。 
 在本年度計畫中，我們將重點放於知識本體倉儲的轉換模組上。特別是提
出了兩種知識本體轉換。其一為結構轉換，另一個為值轉換。結構轉換是將萃
取出之用不同語言編碼的知識本體轉換成一致的文法。而值轉換則是將具有相
同意義的值，集合成一個等價類別，以簡化接下來的整合程序。此外，整合模
組接著運作藉由漸次地合併，將各個轉換後的知識本體合而為一。而此整合而
成的知識本體倉儲可以進一步地依據不同的應用需求，化分成不同的知識本體
市集。 
 
關鍵字：知識本體、語意網、知識本體倉儲、知識本體市集、辭網。 
 3 
1. Introduction 
Due to the wide usage of internet in this decade, the need of sharing and 
exchanging knowledge is dramatically increasing. It, however, also causes some 
difficulties in correctly using the knowledge since different definitions and 
terminologies may be adopted among different data sources. Considerable time and 
cost may be spent in obtaining knowledge from multiple sources. Recently, the 
concept of semantic webs has been widely used to help make machines read and 
understand the web pages in different domains. Ontology plays an important role on 
performing semantic webs, and is used to describe particular domain knowledge and 
related terms for logical inference. How to effectively deal with ontologies and to 
combine them in different resources for forming an efficient, reusable and integrated 
ontology warehouse is an interesting and challenging task. 
In the first year of the project, we proposed a two-phased ontology selection 
approach. In the first phase, a two-level requirement analysis model was introduced 
for describing users’ requirements. In the second phase, the selection approach was 
then used to help users easily find an appropriate ontology for a new application. The 
targets we had achieved in the first year of the project included the models for 
describing users’ requirements of ontology, the methods of effectively analyzing the 
requirements of users, and the methods of selecting suitable ontology that meet the 
user requirements. 
Continuing the achievements of the first year of the project, we propose a 
framework of ontology warehouse to achieve this purpose in this year. It integrates 
several single ontologies to form a more useful ontology warehouse. Users can not 
only obtain information from each individual ontology, but can also get more 
sophisticated knowledge from the ontology warehouse. Modules consisting of the 
framework are also designed to meet the requirements of building an ontology 
warehouse. These modules include analysis, selection, extraction, transformation, 
integration, ontology warehouse, ontology marts, and applications.  
The extracted ontologies from the proposed two-phased ontology selection 
approach are then transformed into a consistent format before they are integrated into 
the ontology warehouse. Two types of ontology transformation are proposed here. 
One is structure transformation and the other is value transformation. After that, the 
transformed ontologies can be integrated into the ontology warehouse by merging the 
paths extracted previously. The integration proceeds by incrementally merging each 
transformed ontology into the result one. The ontology warehouse can then be divided 
 5 
It integrates and transforms enterprise internal and external data for efficient query 
and analysis. There are three major components in a data warehousing system: the 
data collector, the data warehouse, and the OLAP and query processor. The data 
collector is responsible for collecting necessary information and transaction messages 
from several individual data sources through communication networks with respect to 
the requirements of end users and the views defined in the data warehouse. The data 
warehouse receives data from the data collector, then filters them and stores them in 
its own database. The OLAP and query processor provides all necessary information 
for users’ queries and OLAP requirements. These three components can be integrated 
in a single site or exist individually in different sites, and can be linked together 
through communication networks. The data collector may also be broken down into 
several subparts, each located near a data source. The same consideration works for 
the OLAP and query processors.  
One of the primary goals of a data warehousing system is to support on-line 
analytical processing, called OLAP. OLAP can provide multi-dimensional data and 
on-line response for user queries. For this reason, a data warehouse usually maintains 
a large number of views for speeding up query processing and avoiding large amounts 
of network transmission. Views can be defined by query languages and can provide 
particular formats of query results to users. There are two kinds of views in a data 
warehousing system: materialized view and virtual view. A materialized view 
retrieves all of the necessary information from data sources according to the view 
definition and physically stores the extracted data in a data warehouse. A virtual view 
retrieves the information from other materialized views using the query language 
whenever the view contents are required. Each kind of views has its advantages and 
disadvantages, which can be referred to in [6][7][21][27][28]. OLAP also provides 
five operations: roll-up, drill-down, pivoting, slice and dice for processing diverse 
analysis requirement. 
The star schema [16] is most commonly used for modeling a data warehouse. It is 
designed to provide effective analysis from users’ viewpoint. The star schema consists 
of a fact table and multiple dimension tables. The fact table stores the metrics to be 
analyzed and the dimension tables represent the business dimensions [25]. The fact 
table in the middle and the dimension tables are arranged around the fact table. In this 
arrangement, each of the dimension tables has a direct relationship with the fact table 
in the middle, thus speeding up the analysis with the constraints from the dimension 
tables.  
 
 7 
handling. 
 
3.3 WordNet 
WordNet was developed by the Cognitive Science Laboratory at Princeton 
University for building relations among different terms. It is an online lexical 
reference system, inspired by current psycholinguistic theories of human lexical 
memory. WordNet in English version consists of a database keeping the semantic 
relations of English vocabulary meaning (called synsets). Words with the same 
meaning are grouped and accessed in a way similar to that of a thesaurus. Nouns, 
verbs, adjectives and adverbs are organized into different synonym sets, with each 
representing an underlying lexical concept. Different synonym sets are also linked 
together with labels on the links. Several relations are defined for nouns in WordNet, 
including synonym, hypernym/hyponym (a-kind-of relation), homonym (part-of 
relation) and the meronym (parts-of relation). It also includes more than 42,000 new 
links between nouns and verbs that are morphologically related. 
The current WordNet version is 2.0. It is provided as a free package for usage. In 
addition to the database, WordNet also contains a user-friendly interface. A topical 
organization across many areas classifies synsets by categories, regions, usage, gloss 
and synset fixes, and new terminology. 
 
3.4 XML  
XML (eXtensible Markup Language) is defined by the XML Working Group, 
sponsored by W3C (WWW Consortium), for serving as a standard language for 
exchanging information on the web. It is a markup language, which means some 
markup instructions are put at front of contents to specify the display formats of the 
contents. XML emphasizes on structural description and verifiability of web 
documents for effective transmission and processing of document contents on internet. 
In addition, XML may be used with some other related techniques, listed as follows, 
to enhance its power. 
(1) DTD (Document Type Definition): It is used for defining data types and 
assisting the XML parser in interpreting document contents. 
(2) XML Schema: It is developed by MicroSoft and its function is like DTD. It can 
 9 
A framework of ontology warehouse is proposed, as shown in Figure 1, for 
management of ontologies from different sources. There are six components for the 
proposed ontology warehouse: source ontologies, extraction module, transformation 
and integration module, ontology warehouse, ontology marts, and applications. These 
components are described as follows.  
Ontology Marts
Source
Ontology 1
Source
Ontology 3
Source
Ontology  n
Source 
Ontology 2
Ontology
Warehouse
Ex
tr
ac
tio
n
Ex
tr
ac
tio
n
E
xtraction
Tr
an
sf
o
rm
at
io
n
&
Integration
…
Applications
…
…
E
xtraction
 
Figure 1: The framework of ontology warehouse. 
 
 The Source Ontologies 
Domain ontology can be used to describe particular domain knowledge and 
related terms for effective logical inference. Thus, an ontology warehouse will collect 
and integrate a set of ontologies from related or similar domains for a variety of 
applications. The source ontologies act as the knowledge sources for the ontology 
warehouse. The knowledge contained in different source ontologies may be different. 
When building an ontology warehouse, it is necessary to consider what application 
scope the ontology warehouse will be used for. Thus, appropriate source ontologies 
are chosen for the ontology warehouse according to the defined domain scopes. For 
example, in an ontology warehouse for providing medical information, some 
 11 
 Ontology Warehouse 
An ontology warehouse stores the integrated ontology for different applications. It 
can not only effectively provide integrated ontology knowledge to users, but can also 
divide itself into different particular ontologies, called ontology marts, to help 
particular users increase their efficiency in ontology retrieval. The relationship 
between an ontology warehouse and ontology marts is a little similar to the 
relationship between a data warehouse and data marts. A data warehouse involves all 
the integrated enterprise data and provides appropriate decision support to high-level 
managers, but a data mart contains only a specific scope of data interesting to 
particular users. An ontology warehouse plays the same role in providing appropriate 
ontologies to users. 
 
 Ontology Mart 
An ontology mart is a part of an ontology warehouse and focuses on a particular 
subscope. It is extracted from an ontology warehouse according to the special 
requirements of an application. Its size is much less than the whole ontology 
warehouse, such that it can effectively and quickly respond to users. 
 
 Applications 
Appropriate applications may use ontology marts to increase their intelligent 
behavior. Different applications use different ontology marts. These applications, 
however, have the same scope as the integrated ontology warehouse. Web service is 
an example of the applications. 
 
4.2 The Transformation Module 
 13 
Figure 3: The process of value transformation 
The value transformation groups the values with the same meaning into an 
equivalent class. The attribute values appearing in the relevant parts of all the selected 
ontologies are checked through the assistance of WordNet or by the synonym 
templates assigned by users. The first value encountered is used as the name for the 
equivalent class. A synonym relation is built between the terms in a same equivalent 
class, and only the equivalent class name is used in the integration process performed 
later. The integration process can thus be simplifed. For example, the terms “Man”, 
“Men” and “M” appearing in different ontologies may be grouped into an equivalent 
class. The term “Man” is used to represent the equivalent class and replaces “Men” 
and “M” appearing in the other ontologies. 
 
4.3 The Integration Module 
When the extracted ontologies have been transformed into a consistent structure 
and format, they can then be integrated into the ontology warehouse by merging the 
paths found previously. The purpose of integration is to remove redundant nodes and 
combine similar nodes on the paths. The integration proceeds by incrementally 
merging each transformed ontology into the result one, as shown in Figure 4. 
WordNet
Consistent Represented Ontologies
Value 
Transformation
Synonym
Templates 
Synonym
Grouping
Equivalent
Class 1
Equivalent
Class 2
Equivalent
Class 3
Equivalent
Class n…
…
Vocabulary
Replacement
…
Experts
Editing 
 15 
A
A
Path 1
Path 2
A
Integration
 
Figure 6: The integration of two paths for case 1 
(2) If a node in a path is similar to a node in the other path according to WordNet or 
the synonym templates, form an equivalence class for these two nodes, build a 
synonym relation between the nodes in the same equivalent class, and connect 
the two paths by the node. It is shown in Figure 7.  
A
B
Path 1
Path 2
A
Integration
B
Synonym 
Equivalence Class
Synonym
 
Figure 7: The integration of two paths for case 2. 
 
4.4 Generation of Ontology Marts 
Since an ontology warehouse is formed from multiple relevant source ontologies, 
 17 
into the result one. In each step, only two ontologies are merged into a resulting one. 
Each path on a new transformed ontology is compared with each path in the ontology 
resulting from the previous merge. The integrated ontology warehouse can then be 
divided into ontology marts by different application requirements. 
 
6. Self-evaluation of the Research 
In the project of this year, we propose a framework of ontology warehouse to 
effectively deal with ontologies and to combine them in different resources for 
forming an efficient, reusable and integrated ontology. It integrates several single 
ontologies to form a more useful ontology warehouse. Modules on the framework are 
also designed to meet the requirements of building an ontology warehouse. These 
modules include analysis, selection, extraction, transformation, integration, ontology 
warehouse, ontology marts, and applications. The transformation model is focused in 
this year. We have proposed two types of ontology transformation. One is structure 
transformation and the other is value transformation. The integration model is also 
introduced for incrementally merging each transformed ontology into the result one. 
Additionally, the concept of ontology marts is discussed in this project. The integrated 
ontology warehouse can then be divided into ontology marts by different application 
requirements. Besides, we have also published several journal and conference papers 
due to the project. 
 
References 
[1] W. C. Chen, T. P Hong and W. Y. Lin “View Maintenance in an Object-Oriented 
Data Warehouse,” The Fourth International Conference on Computer Science 
and Informatics, 1998, pp. 353-356. 
[2] A. Farquhar, R. Fikes, W. Pratt and J. Rice, Collaborative Ontology 
Construction for Information Integration, Technical Report KSL-95-63, 
Knowledge Systems Laboratory, Stanford University, 1995. 
[3] C. Fellbaum, “WordNet, an Electronic Lexical Database,” The MIT Press, 1998. 
[4] A. Gómez-Pérez, “Some ideas and examples to evaluate ontologies,” The IEEE 
Eleventh Conference on Artificial Intelligence Applications, Washington, USA, 
1995, pp. 299-305. 
[5] A. Gómez-Pérez and O. Corcho, “Ontology languages for the semantic web,” 
IEEE Intelligent Systems, Vol. 17, 2002, pp. 54-60. 
 19 
[20] G. Miller, R. Beckwith, C. Fellbaum, D. Gross and K.Miller, “Introduction to 
WordNet: an Online Lexical Database,” Five Thesiss on WordNet Princeton 
University, 1993. 
[21] I. Mumick, D. Quass and B. Mumick. "Maintenance of data cubes and summary 
tables in a warehouse," The ACM SIGMOD Conference, Tucson, Arizona, 1997, 
pp. 100-111. 
[22] N. F. Noy and D. L. McGuinness, Ontology Development 101: A Guide to 
Creating Your First Ontology, Standford Knowldege Systems Laboratory 
Technical Report, 2001. 
[23] N. F. Noy and M. A. Musen, “PROMPT: algorithm and tool for automated 
ontology merging and alignment,” The Seventeenth National Conference on 
Artificial Intelligence, Austin, Texas, 2000, pp.450-455.  
[24] R. Neches, R. E. Fikes, T. Finin, T. R. Gruber, T. Senator and W. R. Swartout, 
“Enabling technology for knowledge sharing,” AI Magazine, Vol. 12, 1991, pp. 
36-56. 
[25] P. Ponniah, Data Warehousing Fundamentals : A Comprehensive Guide for IT 
Professionals, Wely-QED, 2001. 
[26] S. Pinto, A. Gómez-Pérez, and J. Martins, “Some issues on ontology 
integration,” The Workshop on Ontologies and Problem-Solving Methods, In 
International Joint Conference on Artificial Intelligence, Stockholm, Sweden, 
1999, pp. 7.1-7.12. 
[27] D. Quass. "Maintenance expressions for views with aggregation," The Workshop 
on Materialized Views, In Association for Computing Machinery, Montreal, 
Canada, 1996, pp. 110-118.  
[28] D. Quass, A. Gupta, I. S. Mumick, and J. Widom "Making views 
self-maintainable for data warehousing," The Conference on Parallel and 
Distributed Information Systems, Miami Beach, USA, 1996, pp. 158-169. 
[29] G. Stumme and A. Maedche, “FCA-MERGE: bottom-up merging of 
ontologies,” The Seventeenth International Conference on Artificial Intelligence, 
Seattle, USA, 2001, pp. 225-234. 
[30] M. Uschold and M. Gruninger, “Ontologies: principles, methods and 
applications,” The Knowledge Engineering Review, Vol. 2, 1996, pp. 93-155. 
[31] M. Uschold and M. King, “Towards a methodology for building ontologies,” 
The Workshop on Basic Ontological Issues in Knowledge Sharing, In 
International Joint Conference on Artificial Intelligence, Montreal, Canada, 
1995. 
  
 
2. 智慧型系統和控制 
此部份較強調實際智慧型系統之控制應
用方面，包含了模糊系統和約略集合，以知識
基準的系統，混合的智慧型系統，智慧型錯誤
檢測和確認，最佳化決策，數學問題和控制，
控制系統和識別，參數確認，無參數確認，模
擬等等，均為相當重要且常見的應用。部份筆
者較感興趣的內容介紹如下: 
系統控制：系統控制為目前人機整合中相
當重要的一個關鍵研究領域。此次發表的文
章，包括了模糊控制、神經網路控制、混合式
控制、及控制應用等。除此之外，也針對系統
自動化和測量方面，做了進一步的研究和討
論。 
 
3.其它相關技術 
此部份探討和智慧型工程系統間接相關
之技術，包括了通訊、訊號處理、多媒體和
Web 的技術、軟體技術及系統模擬，感測器網
路、資訊安全等議題。部份筆者較感興趣的內
容介紹如下: 
網際網路技術與應用：此部份主要是探討
如何有效地促進網際網路之效能及增廣其應
用，並能有效地搜尋網際網路的資料，這對於
網際網路發達的今日資訊社會相當的重要。此
部份的文章包括了使用一些機器學習的方法
去改善並增進網路效能，網路服務品質的探
討，網路的流量問題和安全性相關方面的議題
做研究及相關實際系統應用等。除此之外，更
針對感測器網路和無線網路做進一步的改
良。比如說如何去增加感測器網路的存活時
間，如何去改善無線網路的傳送品質，並透過
一 些機器學習的方法來增加效能和效率，並
改善頻寬方面的問題。 
資訊安全：主要針對於如何去隱藏某些資
訊以達到資訊安全做了進一步的探討，比如數
位浮水印和一些金鑰管理和加解密相關方面
的議題，像是影像方面的加密演算和簽章方面
的結構等等。 
 
(三) 建議 
    1.此次承蒙國科會工程處計劃補助，方得
成行，在此先致上十二萬分的謝意，也希望爾
後國科會及教育部對於參加國際知名會議之
學者均能大力支持。 
    2.國內有不少研究此方面的學者，而每年
也都舉辦相關的國際或國內研討會，藉此國內
外學者能互相切磋交流，甚至彼此合作，這是
相當可喜的現象，也希望國內的研討會能不斷
舉辦下去，並朝世界知名國際會議目標努力。 
     
(四) 攜回資料名稱及內容 
    1.會議論文集三冊。 
    2.會議內容介紹一冊。 
    3.其他相關會議之 call for paper 資料。 
  
2. Review of frequent pattern trees 
 
Han et al. proposed the Frequent-Pattern-tree structure (FP-tree) for efficiently mining association 
rules without generation of candidate itemsets [4]. The FP-tree mining algorithm consists of two 
phases. The first phase focuses on constructing the FP-tree from the database, and the second phase 
focuses on deriving frequent patterns from the FP-tree. 
Three steps are involved in FP-tree construction. The database is first scanned to find all items with 
their frequency. The items with their supports larger than a predefined minimum support are selected 
as large 1-itemsets (items). Next, the large items are sorted in descending frequency. At last, the 
database is scanned again to construct the FP-tree according to the sorted order of large items. The 
construction process is executed tuple by tuple, from the first transaction to the last one. After all 
transactions are processed, the FP-tree is completely constructed.  
 
3. The proposed maintenance approach 
 
Assume an FUFP-tree has been built in advance from the original database before records are 
modified. The FUFP-tree construction algorithm is the same as the FP-tree algorithm [4] except 
that the links between parent nodes and their child nodes are bi-directional. Bi-directional linking 
will help fasten the process of item modification in the maintenance process. Besides, the counts 
of the sorted frequent items are also kept in the Header_Table. 
Considering an original database and some records to be modified, the following four cases 
(illustrated in Figure 1) may arise. 
Positive
Original 
database
Large
items
Small
items
Case 1 Case 2
Case 3          Case 4
Negative (zero) 
differencedifference
Original
database
Item
difference
 
Figure 1. Four cases when records are modified from an existing database 
 
Since items in Case 1 are large in the original database and have positive count difference, they 
will still be large after the database is updated. Similarly, items in Case 4 will still be small after 
the records are modified. Thus, Cases 1 and 4 will not affect the final large items. Items in Case 2 
are large in the original database and have negative (or zero) count difference. Some existing 
large items may be removed after the database is modified. It is easily decided since the counts of 
  
Substep 6-2: Set the new count SU(I) of I in the entire updated database as:  
SU(I)=SD(I)+SM(I). 
Substep 6-3: If SU(I) ≥ d*Sup, item I will be large after the database is updated; add item I in 
the sets of Increase_Items and Rescan_Items, and put the transaction IDs with item I 
from the unchanged records D- into the set of Rescan_Transactions. 
STEP 7: For each updated record before modification (T’) and with an item J existing in the set of 
Decrease_Items, find the corresponding branch of J in the FUFP-tree for the record, and 
subtract 1 from the count of the J node in the branch. 
STEP 8: Sort the items in the Rescan_Items in a descending order of their updated counts.  
STEP 9: Insert the items in the Rescan_Items to the end of the Header_Table according to the 
sorted order.  
STEP 10: For the records in the Rescan_Transactions with an item J existing in the set of 
Rescan_Items, if J has not been at the corresponding branch of the FUFP-tree for the 
record, insert J at the end of the branch and set its count as 1; Otherwise, add 1 to the 
count of the node J. 
STEP 11:For the updated records after modification with an item J existing in the Increase_Items, 
if J has not been at the corresponding branch of the FUFP-tree, insert J at the end of the 
branch and set its count as 1; Otherwise, add 1 to the count of the J node. 
In Step 7, a corresponding branch is the branch generated from only the large items in a 
transaction and corresponding to the order of items appearing in the Header_Table. 
 
4. An example 
 
In this session, an example is given to illustrate the proposed algorithm for maintaining an FUFP 
tree when the records are modified. Table 1 shows a database to be used in the example. The 
database contains 10 transactions and 9 items, denoted a to i. 
 
Table 1. The original database in the example 
Old database 
Transaction 
No. 
Items 
1 a, b, c, d, g, h 
2 b, f, ,g, i 
3 b, d, e, f, g 
4 a, b, f, h 
5 a, b, f 
  
d 1 
e -2 
f -1 
g -1 
h -1 
i -1 
 
All the items in M are divided into four cases and are then processed in four individual ways. The 
final results are shown in Figure 3. 
Based on the FUFP-tree shown in Figure 3, the desired large itemsets can then be found by the 
FP-Growth mining approach as proposed in [4]. 
 
5. Experimental results 
 
The experiments were performed in C++ on an Intel x86 PC with a 2.8G Hz processor and 512 MB 
main memory and running the Microsoft Windows XP operating system. A real dataset called 
BMS-POS [5] were used in the experiments. This dataset was also used in the KDDCUP 2000 
competition. 
Header Table
Item Frequency Head 
b          9
a          8
f           5
d           5
{}
b:9
f:2a:7
Null
Null
a:1
f:3 d:2 Null
Null
d:1
d:1
d:1
 
Figure 3. The Final FUFP-tree after all the modified records are processed 
 
The first 425,000 transactions were extracted from the BMS-POS database to construct an initial 
FP-tree. Each time, 5,000 transactions were randomly chosen from the last updated database for 
modification. The next 5,000 transactions outside the 425,000 ones were used as the new contents in 
the modified records. The minimum support was set at 4%. Figure 4 shows the execution times 
required by the batch FP-tree construction algorithm and by the FUFP-tree maintenance algorithm for 
processing each 5000 modified records. 
 
  
References 
 
[1] R. Agrawal, T. Imielinksi and A. Swami, “Mining association rules between sets of items in large database,“ The ACM SIGMOD 
Conference, pp. 207-216, 1993. 
[2] R. Agrawal and R. Srikant, “Fast algorithm for mining association rules,” The International Conference on Very Large Data Bases, pp. 
487-499, 1994. 
[3] R. Agrawal, R. Srikant and Q. Vu, “Mining association rules with item constraints,” The Third International Conference on 
Knowledge Discovery in Databases and Data Mining, pp. 67-73, 1997. 
[4] J. Han, J. Pei, and Y. Yin, ”Mining frequent patterns without candidate generation” The 2000 ACM SIGMOD International 
Conference on Management of Data, 2000. 
[5] Z. Zheng, R. Kohavi and L. Mason, “Real world performance of association rule algorithms”, The International Conference on 
Knowledge Discovery and Data Mining, pp. 401-406, 2001.  
均是目前相當熱門之主題。部份筆者較感
興趣的內容介紹如下: 
資料挖掘及機器學習：資料挖掘主要
是從大型資料庫中整理分析資料，以求出
高階總結性之規則作為決策依據。此題目
為目前資料庫及知識庫的整合中非常熱門
的一個研究方向，廣義而言，甚至可將機
器學習領域納入。此次會議發表的文章均
是將傳統資料挖掘技術再做更深入的變
化，以符合不同的需求。而在機器學習部
份，則包含加強式學習、遺傳演算法、神
經網路式學習及資料群聚等文章。 
影像處理：此部份主要探討影像處理
及圖形識別等技術，主要是針對影像的編
碼和傳送做進一步的探討和研究。 
 
2. 智慧型系統和控制 
此部份較強調實際智慧型系統之控制
應用方面，包含了模糊系統和約略集合，
以知識基準的系統，混合的智慧型系統，
智慧型錯誤檢測和確認，最佳化決策，數
學問題和控制，控制系統和識別，參數確
認，無參數確認，模擬等等，均為相當重
要且常見的應用。部份筆者較感興趣的內
容介紹如下: 
系統控制：系統控制為目前人機整合
中相當重要的一個關鍵研究領域。此次發
表的文章，包括了模糊控制、神經網路控
制、混合式控制、及控制應用等。除此之
外，也針對系統自動化和測量方面，做了
進一步的研究和討論。 
 
3.其它相關技術 
此部份探討和智慧型工程系統間接相
關之技術，包括了通訊、訊號處理、多媒
體和Web的技術、軟體技術及系統模擬，
感測器網路、資訊安全等議題。部份筆者
較感興趣的內容介紹如下: 
網際網路技術與應用：此部份主要是
探討如何有效地促進網際網路之效能及增
廣其應用，並能有效地搜尋網際網路的資
料，這對於網際網路發達的今日資訊社會
相當的重要。此部份的文章包括了使用一
些機器學習的方法去改善並增進網路效
能，網路服務品質的探討，網路的流量問
題和安全性相關方面的議題做研究及相關
實際系統應用等。除此之外，更針對感測
器網路和無線網路做進一步的改良。比如
說如何去增加感測器網路的存活時間，如
何去改善無線網路的傳送品質，並透過一 
些機器學習的方法來增加效能和效率，並
改善頻寬方面的問題。 
資訊安全：主要針對於如何去隱藏某
些資訊以達到資訊安全做了進一步的探
討，比如數位浮水印和一些金鑰管理和加
解密相關方面的議題，像是影像方面的加
密演算和簽章方面的結構等等。 
 
(三) 建議 
    1.此次承蒙國科會工程處計劃補助，
方得成行，在此先致上十二萬分的謝意，
也希望爾後國科會及教育部對於參加國際
知名會議之學者均能大力支持。 
    2.國內有不少研究此方面的學者，而
每年也都舉辦相關的國際或國內研討會，
藉此國內外學者能互相切磋交流，甚至彼
此合作，這是相當可喜的現象，也希望國
內的研討會能不斷舉辦下去，並朝世界知
名國際會議目標努力。 
     
(四) 攜回資料名稱及內容 
    1.會議論文集三冊。 
    2.會議內容介紹一冊。 
    3.其他相關會議之 call for paper資料。 
