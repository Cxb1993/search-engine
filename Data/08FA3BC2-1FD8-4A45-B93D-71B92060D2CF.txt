 2
 
行政院國家科學委員會專題研究計畫結案報告： 
用於音訊音樂的計算聽覺場景分析 
 
計畫編號：NSC 96-2628-E-007-141-MY3 
執行期限：2009 年 8 月 1 日至 2010 年 7 月 31 日 
主持人：張智星（jang@cs.nthu.edu.tw） 
計畫執行單位：清華大學資訊工程學系 
計畫參與人員：許肇凌、陳致生、陳亮宇 
 
1. 中文摘要 
 
本結案報告說明執行國科會計畫「用
於音訊音樂的計算聽覺場景分析」（第三
年）所衍生之成果，包含研究主題介紹、
原理說明及成果簡介。 
所 謂 「 計 算 聽 覺 場 景 分 析 」
（Computational Auditory Scene Analysis，
簡稱 CASA）是緣至於「聽覺場景分析」
(Auditory Scene Analysis，簡稱 ASA) ，是
探討人類聽覺如何將不同來源的聲音進行
分離並加以分析的研究，而 CASA 就是使
用計算機去模擬人類的聽覺系統來進行聲
音分離的研究。在第一年的計畫中，我們
專注於研究如何將歌曲中的歌聲抽取出
來，尤其是氣音的部份，並取得了初步的
研究成果，且發表了一篇國際研討會論
文。第二年的計畫部份，我們做了更深入
的研究分析，並將成果發表於 IEEE 
Transaction 期刊。在第三年的計畫裡，我
們將大幅改良了用於聲音分離的關鍵技術:
音高追蹤。良好的音高追蹤是將聲音中非
氣音(voiced)的部分分離出來的關鍵，而
在歌聲中有 95%的聲音是屬於非氣音的部
分，也因此我們於第三年的計畫中著重於
改良多音音樂中歌聲非氣音的部分音高，
我們並使用所研發之演算法參加 Music 
Information Retrieval Evaluation 
eXchange (MIREX)國際比賽並在最近兩年
的演算法中得到了第一名的效能，也證實
了我們的演算法對背景音樂的影響較能夠
容忍且能夠非常有效的將歌聲音高正確的
抽取出來。 
 
關鍵詞：  
 
聽覺場景分析、計算聽覺場景分析、語音
訊號處理、音樂資訊檢索 
 
Abstract 
 
This report describes the execution 
results of NSC project “Computational 
auditory scene analysis for music signal 
processing”, including an introduction to the 
research, explaining the principles behind the 
method, and some analysis to the 
experimental results. 
Computational auditory scene analysis, or 
CASA, originates from the field of auditory 
scene analysis (ASA) that discusses how 
human’s auditory system separates and 
analyzes sounds from different sources. 
During the first year of the project, we 
focused on how to extract singing voices, 
particularly the unvoiced parts, from a 
polyphonic music. A preliminary result was 
obtained and was published in an 
international conference. A detailed research 
was carried out during the second year of the 
project and the result was publish in an IEEE 
transaction. During the third year of the 
project, we significantly improved a key 
technology used in singing voice separation: 
pitch tracking. A good pitch tracking result 
leads to the an accurate extraction of the 
voiced parts of a singing voice, and voiced 
sounds comprise 95% of the entire singing 
voice. This improved technology was 
employed in our system for an international 
contest in Music Information Retrieval 
Evaluation eXchange (MIREX) and obtained 
 4
voice is enhanced by considering temporal 
and spectral smoothness. A sinusoidal partial 
extraction algorithm is then applied to extract 
the frequency slopes of the harmonic sounds. 
After that, we extract features from each 
partial and use a classifier to detect and 
prune instrumental partials. Finally, we 
locate vocal fundamental frequencies (F0s) 
in a series of time-frequency (T-F) blocks 
according to the remaining partials after 
pruning. These T-F blocks give pitch ranges 
along time and are much narrower than that 
of the entire voice pitch range. Comparing to 
our previous algorithm in [4], we add the 
vocal component enhancement as 
pre-processing.  We also improve sinusoidal 
partial extraction so that the extracted 
partials are more accurate. In addition, a 
classifier is trained to perform instrumental 
partial detection instead of choosing 
parameter values heuristically. Lastly, we 
now consider the neighbors of selected T-F 
blocks to improve pitch range estimation. 
 
4.1  Vocal Component Enhancement 
 
Our trend estimation starts from a vocal 
enhancement algorithm proposed by 
Tachibana et al. [3]. They used 
harmonic/percussive sound separation (HPSS) 
to enhance singing voice in two stages. In the 
first stage, they attenuate the energy of 
harmonic instruments (e.g., guitar and flute). 
The energy of percussive instruments (e.g. 
drum and cymbal) is further attenuated in the 
second stage. 
In this study, we only apply the first stage of 
HPSS which attenuates the energy of 
harmonic instruments. The reason is that the 
sounds of percussive instruments are 
aperiodic and do not create much difficulty 
in estimating the pitch of singing voice. In 
addition, the second stage usually corrupts 
the singing voice in the spectrogram and 
degrades the performance of our partial 
extraction algorithm. 
Vocal 
Component 
Enhancement
Sinusoidal 
Partial 
Extraction
Instrumental 
Partial 
Pruning
Mixture
Pitch Range 
Estimation
Estimated 
Trend
 
 
Fig. 1.  Schematic diagram of trend estimation. 
 
4.2  Sinusoidal Partial Extraction 
 
This stage extracts sinusoidal partials from a 
mixture. First, we apply the multi-resolution 
fast Fourier transform (MR-FFT) proposed 
by Dressler [5]. It removes the unreliable 
peaks that do not originate from periodic 
sounds by considering the local 
characteristics of phase spectrum, or more 
precisely, the instantaneous frequencies of 
neighboring frequency bins. 
Reliable peaks are then used to form 
sinusoidal partials. Because some peaks in 
the same time frame may correspond to the 
same sinusoidal component, we first check 
the instantaneous frequencies of the peaks in 
each time frame. If their instantaneous 
frequencies are close enough, the one with 
the largest magnitude is selected. 
A grouping algorithm is applied after 
peak selection. The goal of the algorithm is 
to group peaks so that each peak corresponds 
to a partial. It consists of three steps: initial 
grouping, re-grouping, and refining.  
1) Initial grouping: It starts by selecting any 
ungrouped peak in the spectrogram as the 
first peak in the group and recursively 
groups other peaks neighboring to the 
group until every peak belongs to a group. 
To choose a peak from the 
time-overlapping peaks in a group, we 
apply a pitch dynamic prediction 
 6
Fig. 2(a) and Fig. 2(b) show examples 
of partial extraction and instrumental partial 
pruning from a song mixture, respectively. 
As we can see, lots of the instrumental 
partials are pruned while most of the vocal 
F0 partials are retained. 
Fr
eq
ue
nc
y 
(H
z)
(a)
1 2 3 4 5 6 
100
200
400
600
8001000
 
Fr
eq
ue
nc
y 
(H
z)
(b)
1 2 3 4 5 6
100
200
400
600
8001000
 
Fr
eq
ue
nc
y 
(H
z)
(c)
1 2 3 4 5 6
100
200
400
600
8001000
 
T−F block time indexT−
F 
bl
oc
k 
fre
qu
en
cy
 in
de
x (d)
1 2 3 4 5 6 7 8
2
4
6
8
10
 
Time (secs)
Fr
eq
ue
nc
y 
(H
z)
(e)
1 2 3 4 5 6
200
400
600
800
1000
1200
 
 
Fig. 2. An example of trend estimation. (a) Extracted partials. (b) The result of instrumental partial pruning. 
(c) The result after deleting the harmonic partials. (d) Magnitude-downsampled diagram. A color indicates the 
energy strength of a T-F block. The solid line indicates the optimal path found by DP. (e) The spectrogram of 
the song mixture. Dashed lines show the extended boundaries of the estimated trend and solid lines show the 
ground truth pitch. 
 
4.4  Pitch Range Estimation 
 
The last stage of the trend estimation is 
to find a sequence of relatively tight pitch 
ranges where the F0s of the singing voice are 
present. 
 
0 10 20 30 40 50 60 70 80 90 100
0
10
20
30
40
50
60
70
80
90
100 DET Curve of Instrumental Partials Detection
Miss Rate (%)
Fa
ls
e 
A
la
rm
 R
at
e 
(%
)
 
 
0dB
-5dB
5dB
 
Fig. 3. Detection tradeoff curves for instrumental partial detection. 
 
of the modification is based on the 
observation that most of the energy in a song 
in located at the low frequency bins, and the 
energy of the harmonic structures of the 
singing voice seems to decay slower than 
that of instruments [4]. Therefore, when 
more harmonic components are considered, 
energy of the vocal sounds is further 
strengthened. Although some percussive 
instruments (e.g. cymbals) present high 
energy at higher frequency bins, their 
non-harmonic nature does not affect the 
NSHS much. 
 8
]
]
Based on the proposed NSHS, we can 
extract a feature vector of Energy at 
Semitones of Interests (ESI) for each given 
frame. 
For each integer semitone of interests 
within the range , we identify its 
maximum energy as an element of the feature 
vector. Take semitone 69 for example, the 
search range in semitone is , 
corresponding to a frequency bin of 
 in terms of Hertz. Then we 
find the maximum power spectrum within 
this range as the feature associated with 
semitone 69. Since there are 36 elements 
within semitone of interests, the length of the 
feature vector of ESI is also 36. Please refer 
to [1] for more details. 
[ 75,40
]
[ 5.69,5.68
[ 89.452,47.427
 
4.6  DP-based Pitch Extraction 
 
The goal of using the DP technique is to 
find a path [ ]10 ,,,, −⋅⋅⋅⋅⋅⋅= ni ffff  that 
maximizes the score function: 
 
( ) ( )∑ ∑−
=
−
= −
−×−= 1
0
1
1
1,s
n
t
n
t
tttt fffYfcore θθ ,  (7)
          
 
where ( )tt fY  is a feature vector extracted 
from NSHS at the frame t  and frequency 
. The first term in the score function is the 
sum of energy of the pitches along the path, 
while the second term controls the 
smoothness of the path with the use of a 
penalty term 
tf
θ  (which is set to 2 in this 
work). If θ  is larger, then the computed 
path are smoother.  
The bandwidth of a trend after boundary 
extension is one octave (12 semitones). 
 10
Table 1 shows the results of trend 
estimation and singing pitch detection where 
the second and third rows show the 
performance of trend estimation in [4] and 
our proposed algorithm at different SNR 
levels respectively. A pitch range is 
considered as correct if the ground truth of 
that frame is within the trend. It can be seen 
that our trend estimation outperforms the 
previous method, especially at low SNR.  
We perform pitch detection by using the 
dynamic programming algorithm proposed in 
[4]. The fourth and fifth rows of Table 1 
show the performance of pitch detection with 
and without the proposed trend estimation, 
respectively. An estimated pitch is 
considered correct if its difference to the 
ground truth is less than 0.5 semitone. As we 
can see, the performance of pitch detection is 
improved significantly at all three SNR 
levels. 
We also submitted this algorithm to the 
MIREX 2010 audio melody extraction 
competition. Fig. 4 and Table 2 show the 
results of the last two years. Each submission 
has been tested on six datasets.  The first 
bar shows the result of our system and it 
achieves the best raw-pitch accuracy for 
vocal songs.  
Fig. 4. The combined results of MIREX 2009 and 2010 for vocal songs. The first bar indicates the performance of the
proposed algorithm. 
 
 
6. Conclusions 
 
This report proposes a trend estimation 
algorithm for singing pitch detection in 
musical recordings, which has been little 
investigated in previous research. Our 
algorithm decides the pitch range without the 
prior knowledge of input songs and 
substantially improves the performance of 
pitch detection. This algorithm can be 
adopted by existing singing pitch detection 
algorithms to improve their results. The 
evaluation results show the proposed trend 
estimation algorithm is robust at different 
SNRs and perform well on different datasets. 
 
7.  References 
 
[1] G. E. Poliner, D. P. W. Ellis, A. F. 
Ehmann, E. Gomez, S. Streich, and B. 
Ong, "Melody transcription from music 
audio: approaches and evaluation," IEEE 
TASLP, vol. 15, pp. 1247-1256, 2007. 
 12
 
我們的論文報告排在第二天 (3/16) 下午13:30-15:30的
Multimedia Indexing & Retrieval 場次，主持人是 
Columbia University 的 Dan Ellis。由於經過多次演練，
佳民的報告還算順利，但由於他的英文聽力有限，所以無法
立刻順利地回答台下聽眾的問題，還好 Dan Ellis 挺身幫
忙，這部分是佳民還需要加強的地方。在報告完後，佳民還
和 Dan Ellis，做為第一次英文口頭宣讀論文的紀念。  
由於達拉斯和台灣的時差將近12小時，所以我們的作息受到
時差的影響很嚴重。通常我們晚上都大約10點睡到4點（當
成是在台灣的午覺），而下午在會場則會在飯店大廳找個座
位瞇一下（當成在台灣晚上的睡覺），這樣回到台灣後，也比較容易將時差調整
回來，以便立刻回到正常作息。  
在這幾天的研討會中，聽了很多口頭宣讀的論文，比較有印
象的是下列三篇：  
1. Bayesian Compressive Sensing for Phonetic 
Classification (by Tara Sainath at IBM T. J. 
Watson Research Center): 作者以 Compressive 
Sensing 來進行 TIMIT 語料的 phone recognition，
可以達到 80% 的辨識率，這是目前 TIMIT phone 
recognition 最好的結果，因此可見 CS 也可以用到
音樂檢索等其他相關領域。  
2. Cyclic Tempogram - A Mid-level Temp 
Representation for Music Signals (by Meinard 
Mueller, etc): 作者深入淺出地介紹 cyclic 
tempogram，說明非常生動，搭配投影片的動畫展示，
是一個非常成功的口頭宣讀。  
3. Singing Information Processing Based on Singing Voice Modeling (by 
Masataka Goto at AIST, Japan): 作者介紹了在 AIST 所開發的一系列音
樂處理系統，非常有趣，有很多有趣的構想，都可能實現相關的商業應用。  
在回國前一天，我去參觀當地著名的景點：第六號樓博物館
(The Sixth Floor Museum)，此地是美國前總統甘迺迪被槍
手開槍暗殺的地方，因為剛好是在第六樓，所以就以此來命
名並作為紀念。博物館內部蒐集了許多相關的影音資料，讓
我沈醉在濃濃的美國懷舊文化氣息裡，彷彿我也走過那一段
歷史般。  
此次會議共收到 2855 篇論文，1356 篇被接受，接受率為 
48%，共有 70 oral sessions 及 83 poster sessions，論
文分佈如下：  
  
佳民完成口頭宣讀
論文後，和 
session chair 
Dan Ellis 留影。  
  
研討會是在德州的
牛仔秀（Rodeo 
show）舉行。由左
至右依次是我、交
大蔡銘箴教授、香
港理工科技大學區
子廉教授、台大貝
蘇章教授、成大吳
宗憲教授。  
  
大會壁報會場。  
Technical Area TC Code
Submitted 
Papers
Accepted 
Papers
Speech Processing (SLTC) (SPE) 511 249
Spoken Language Processing (SLTC) (SLP) 110 59
Image, Video & Multidimensional (IVMSP) 451 198
頁 2 / 4
2010/12/29
 攜回資料名稱及內容 
研討會論文集電子檔。  
Last updated on 12/29/2010 10:14:10.   
頁 4 / 4
2010/12/29
96年度專題研究計畫研究成果彙整表 
計畫主持人：張智星 計畫編號：96-2628-E-007-141-MY3 
計畫名稱：用於音訊音樂的計算聽覺場景分析 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 2 2 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
