 
I
行政院國家科學委員會專題研究計畫進度報告 
 
若干統計方法應用於適應性語音辨識 
Some Statistical Approaches for Adaptive Speech Recognition 
 
計畫編號：NSC 96-2628-E-006-141-MY3 
執行期限：98 年 8 月 1 日至 99 年 7 月 31 日 
主持人：簡仁宗  執行機構：國立成功大學資訊工程學系 
 
中文摘要 
在本計畫中我們提出一新穎統計法則以有效提升語音辨識效能。在三年的計畫中，我們將
分別從前端之訊號增強、語音與語言模型統一訓練與達成最佳貝氏決策理論之語音辨識器進行
辨識效能之改進。以下分別簡介這三年之計畫內容。 
在第一年的計畫當中，我們針對噪訊語音辨識議題，提出了一個新穎之子空間模型與子空
間選取之方法。在子空間模型方面，我們利用了因素分析(Factor Analysis, FA)來描述雜訊語
音。其中因素分析是一種利用從資料當中萃取出來的共同因子(Common Factors)、因素負荷矩
陣(Factor Loading Matrix)、與獨特因子(Specific Factor)來描述資料生成的模型。在本計畫當
中，我們探討了因素分析與傳統子空間模型(Signal Subspace, SS)之間的關係，其中最大的差異
是因素分析除了把雜訊語音投射到包含主要語音資訊與噪音的主要子空間(Principal Subspace)
外，在次要子空間(Minor Subspace)的定義上也令其包含了細微之殘存語音資訊與噪音。為了
估算出乾淨的語音，我們利用人耳的聽覺效應，在主要空間與次要空間的濾波器皆設計為最小
化語音失真(Speech Distortion)。另外更重要的一點是在於如何選取最佳的子空間維度，在此點
上我們透過統計中假設檢定的問題與方法來處理最佳化子空間分割的問題，除了檢定次要子空
間中的每個特徵值(Eigenvalue)是否相等，以決定次要子空間的維度(同時也決定出了主要子空
間的維度)外，為了能夠滿足因素分析模型中的假設，我們更進一步地提出檢定殘存語音的共
變異數矩陣是否為對角線矩陣，來決定次要子空間的維度。所提出的兩種最佳解皆能夠透過相
似度比值的檢定，以及 chi-square 分佈的近似做為測試統計量而達成。子空間的分割便根據信
賴度是否落在拒絕區域來判定。在實驗中，相較於傳統的訊號子空間方法，本計畫所提出的因
素模型架構與子空間維度的選取方法皆能有效改進雜訊語音的辨識率。 
在第二年的計畫當中，我們針對聲學(acoustic)及語言(linguistic)模型對語音辨識的影響性，
提出了一個整合性最大熵(Maximum Entropy)模型作為語音辨識器的主要架構。透過理論分析
來建立整合性模型與鑑別式訓練準則之間的關聯性。在傳統語音辨識系統中，通常假設語音和
語言兩種資訊來源是相互獨立，分別各自訓練其模型參數。然而，語音辨識所擷取的候選文字
串與輸入之語音信號存有相互影響性。因此，在本計畫中，將語音和語言之間的共變異性特徵
結合在模型中。除此，在鑑別性最大熵模型下，可以有效結合其他較高層級的資訊在模型之
中。在實驗中，我們將所提出的方法實現在自發性廣播新聞語音辨識系統上。相較於傳統以最
佳相似度為主的方法，本計畫所提出的方法皆能有效改進語音的辨識率。 
 
III
Abstract 
In this project, we propose new statistical approaches to improve the performance of speech 
recognition system. During the period of this three-year project, we will deal with the issues from the 
signal enhancement in the front end processing, the joint training of acoustic and language models and 
the speech recognizer fulfilling the optimal Bayes decision to improve the recognition performance. 
The details of these approaches are described as follows. 
To solve the noise environment influence on noisy speech recognition, we propose a new subspace 
modeling and selection approach for dealing with this problem. In subspace modeling, we develop 
factor analysis (FA) for representing noisy speech. FA is known as a data generation model where the 
common factors are extracted with a factor loading matrix and specific factors. Interestingly, FA is 
found to be an extension of signal subspace (SS) approach for modeling noisy speech. This approach 
can partition noisy speech space into a principal subspace containing clean speech and a minor 
subspace containing residual speech and residual noise. To estimate clean speech with residual 
information, we minimize the energies of speech distortion in principal subspace as well as minor 
subspace. More importantly, in subspace selection, we explore optimal subspace partition of 
subspaces via solving hypothesis test problems. We test the equivalence of eigenvalues in minor 
subspace so as to determine subspace dimensions. To fulfill FA spirit, we further examine the 
hypothesis of uncorrelated residual speech. Optimal solutions are realized by a likelihood ratio test 
with the approximated chi-square distributions as test statistics. Subspace partition is performed 
according to the confidence towards rejecting null hypotheses. In the experiments on Aurora2 
database, FA outperforms SS in subspace modeling. New selection algorithms effectively determine 
subspace dimension for noisy speech recognition. 
Traditionally, speech recognition system is established assuming that acoustic and linguistic 
information sources are independent. Parameters of hidden Markov model and n-gram are estimated 
individually and then plugged in a maximum a posteriori classification rule. However, acoustic and 
linguistic features are correlated in essence. Modeling performance is limited accordingly. This 
project aims to relax the independence assumption and achieve sophisticated acoustic and linguistic 
modeling for speech recognition. We propose an integrated approach based on maximum entropy (ME) 
principle where acoustic and linguistic features are optimally merged in a unified framework. The 
correlations between acoustic and linguistic features are explored and properly represented in the 
integrated models. Attractively, different from traditional ML speech recognition using a fixed scaling 
factor to combine HMM and n-gram, ME approach determines scaling factor dynamically and merges 
it into parameters implicitly. In the experiments, we carry out the proposed methods for broadcast 
news transcription using MATBN database. We obtain significant improvement compared to 
conventional speech recognition system using individual maximum likelihood training. 
In the third year, we present a new speech recognition framework towards fulfilling optimal Bayes 
decision theory, which is essential for general pattern classification. Classification procedure is 
developed through minimizing the Bayes risk or the expected loss due to misclassification. Generally, 
loss function measures the cost of choosing a candidate. This function was manually specified or 
nonparametrically calculated. In this project, we exploit a novel Bayes loss function via testing 
whether the classification produces loss or not. A Bayes factor is derived to express parametric loss 
using predictive distributions. The predictive distributions are not only merged in Bayes loss function 
but also the word posterior probability. Robust decision using uncertain system parameters can be 
guaranteed. Interestingly, optimizing this Bayes criterion is equivalent to minimizing the classification 
errors of test data. We bridge the relation between minimum classification error (MCE) classifier and 
 
V
目錄 
中文摘要...................................................................................................................................................I 
Abstract .................................................................................................................................................. III 
目錄......................................................................................................................................................... V 
First Year Project: Robust Noisy Speech Preprocessing ......................................................................... 1 
1. INTRODUCTION ....................................................................................................................... 1 
2. SUBSPACE MODELING........................................................................................................... 1 
3. OPTIMAL SUBSPACE SELECTION........................................................................................ 4 
4. EXPERIMENTS.......................................................................................................................... 5 
5. SUMMARY................................................................................................................................. 7 
Second Year Project: Maximum Entropy Joint Modeling....................................................................... 8 
1. INTRODUCTION ....................................................................................................................... 8 
2. ACOUSTIC AND LINGUISTIC MODELING .......................................................................... 8 
4. EXPERIMENTS........................................................................................................................ 11 
5. SUMMARY............................................................................................................................... 12 
Third Year Project: Optimal Bayes Speech Recognition ...................................................................... 13 
1. INTRODUCTION ..................................................................................................................... 13 
2. OPTIMAL BAYES CLASSIFICATION .................................................................................. 13 
3. EXPERIMENTS........................................................................................................................ 16 
4. SUMMARY............................................................................................................................... 17 
REFERENCES ...................................................................................................................................... 18 
 
2
MK −  zero eigenvalues, respectively. Signal subspace coincides with the subspace span W  for 
constructing clean signal. Clean signal can be estimated by minimizing signal distortion and 
simultaneously limiting permissible level of residual noise [12]. 
More attractively, we present FA modeling of noisy signal. The basic idea of FA is to use a 
MK ×  factor loading matrix Φ , a M-dimensional common factor vector f  and a K-dimensional 
specific factor vector r  to represent signal z  as 
rfz +Φ= .                                                               (1.2) 
Specific factors r  are viewed as residual signal or modeling error. In analysis of noisy speech, these 
factors carry information of residual speech as well as residual noise. FA modeling should possess the 
following properties [4]. Common factors and residual signal are uncorrelated 0][ =TE fr  and 
Gaussian distributed with zero mean vectors 0][][ == rf EE  and diagonal covariance matrices 
M
T IE =][ff  and Ψ=][ TE rr . Then, In general, the dependencies among features can be properly 
modeled by common factors.  The covariance matrix of residual signal Ψ  should be diagonal. 
Namely, residual factors are specific and uncorrelated. Observation vectors are then Gaussian 
distributed with ) ,0(~ Ψ+ΦΦ TNz . 
Different from principal component analysis (PCA) [18] developed for dimension reduction, FA 
aims to extract common factors for data modeling. PCA finds principal components for representing 
majority of data variability while FA characterizes data dependencies using a small number of 
common factors. Using FA, factor loadings Φ  should be estimated. One approach of finding Φ  was 
derived from probabilistic PCA model [9][28] using maximum likelihood estimation. Also, parameter 
Φ  can be estimated via eigendecomposition of covariance matrix [18]. 
TTTT WWWWWWR mmmp
2/1
p
2/1
ppz Λ+ΛΛ=Λ=Ψ+ΦΦ= ,                        (1.3) 
where ] [ mp WWW =  and ] [diag mp ΛΛ=Λ  are partitioned eigenvector matrix and eigenvalue matrix, 
respectively. Factor loadings are obtained by 2/1ppΛ=Φ W  using principal submatrices pp  ,ΛW  
corresponding to the preceding M  eigenvalues. Covariance matrix of specific factors Ψ  is generated 
using minor submatrices mm  ,ΛW  corresponding to the remaining MK −  eigenvalues. To connect the 
relation of FA to PCA in realization of zR , we can formulate noisy signal in PCA form by cz
21Λ=W . 
The whitened sample c  with K
T I=cc  can be obtained by zc TW21−Λ=  and partitioned by 
TTT ] [ mp ccc = . Then, common factors f  and specific factors r  can be obtained from 
rfccz +Φ=Λ+Λ= m2/1mmp2/1pp WW . This is a PCA oriented approach for estimating FA parameters. In 
this FA approximation, we have M
TT IEE == ][][ ppccff  but non-diagonal Ψ . We use this approach to 
realize FA. 
It is interesting that FA can be viewed as subspace approach because parameters ΨΦ  , , f  are 
derived from principal subspace pp span WV =  and complimentary minor subspace mm span WV = . 
Using SS approach, clean signal and noise signal individually constitute the signal subspace and the 
noise subspace, respectively. Differently, common factors f  in FA come from the sources of clean 
signal yf  and noise signal nf , ny fff += . Also, specific factors r  can be expressed as the sum of 
factors associated with residual clean signal yr  and residual noise nr , ny rrr += . Assuming these 
factors are independent, covariance matrix of noisy signal is yielded by 
 
4
3. OPTIMAL SUBSPACE SELECTION 
Previously, we explored model selection of HMMs for speech recognition [8]. In this project, we 
concern selection of common factors for FA subspace approach. In subspace modeling, it is critical to 
determine the partition of principal/signal subspace and minor/noise subspace or equivalently their 
dimensions MV =pdim  and MKV −=mdim . This partition also corresponds to choose number of 
factors for FA. Using SS [12], subspace selection was empirically controlled by the estimated noise 
variance 2nσ . The smaller the variance was, the larger the signal subspace was specified for modeling 
noisy signal. 
 
3.1. Selection via Testing Equivalence of Eigenvalues  
To significantly perform subspace decomposition, we employ hypothesis test principle to determine 
dimension M . With decomposition of covariance matrix zR , the problem of subspace selection turns 
out to evaluate the equivalence of the last MK −  eigenvalues. Conceptually, decision boundary 
between principal and minor subspaces can be determined when the last MK −  eigenvalues are 
relative small. We can test the null hypothesis that the last MK −  eigenvalues are equal against 
alternative hypothesis that at least two of them are different [2]. 
KMMH λλλ === ++ L210 :   
:1H At least two of the last MK −  eigenvalues are different 
Equivalently, we are testing isotropic [28] eigenvalues in minor subspace. Let zR  be calculated using 
training samples }{ 1 NzzZ L= . Eigenvalues of zR  represent the variances of decorrelated samples 
}{ 1 Ndd L  transformed by dz mW= . Assuming that eigenvalues are Gaussian distributed, we can 
represent the likelihood under null hypothesis as 
⎭⎬
⎫
⎩⎨
⎧ ΔΛΔ−⋅Λ= ∑
=
−−
−− N
n
T
nn
NMKN
HL
1
1
m
2
m
2
)(
0 2
1exp)2()( ddπ ,                      (1.10) 
where ddd −=Δ ii  and ],,[diag 1m KM λλ L+=Λ  is eigenvalue matrix associated with minor 
subspace. )( 0HL  can be further arranged as 
⎭⎬
⎫
⎩⎨
⎧ ΛΛ−⋅⎥⎦
⎤⎢⎣
⎡ ⎟⎠
⎞⎜⎝
⎛
−
−
−
+=
−− ∑ ]tr[2exp1)2( 1mm
2
1
2
)( N
MK
N
K
Mk
k
MKN
λπ                       (1.11) 
Also, likelihood under alternative hypothesis is derived by 
⎭⎬
⎫
⎩⎨
⎧ ΛΛ−⋅⎟⎟⎠
⎞⎜⎜⎝
⎛= −
−
+=
−− ∏ ]tr[2exp)2()( 1mm
2
1
2
)(
1
NHL
N
K
Mk
k
MKN
λπ .                      (1.12) 
Optimal solution is carried out by evaluating likelihood ratio )()( 10 HLHLq = . The test statistic 
qlog2−  has the form 
⎟⎠
⎞⎜⎝
⎛
−−+− ∑∏ +=+=
K
Mk
k
K
Mk
k MK
MKNN
11
1log)(log λλ .                               (1.13) 
This statistic can be approximated as a chi-square density 2νχ  with degree of freedom being 
1)1)((5.0 −+−−= MKMKν  [2]. Finally, null hypothesis 0H  is rejected at a significance level α  if 
2
;log2 ανχ≥− q . 
 
6
Speech features consisted of 13 MFCC coefficients and energy along with the delta and 
acceleration coefficients. We estimated continuous-density HMM parameters and built speech 
recognizer using HTK toolkit. We specified 16 states per word and three Gaussian mixture 
components per state. Subspace decomposition and selection were performed frame by frame. In 
signal estimation procedure, we used 40 sampling points as a frame and shifted every 20 points. 
Time-domain filters with 4040×  matrices were estimated. When computing covariance matrix zR , a 
window of nine frames was considered. The control parameters pμ  and mμ  were tuned for different 
environments and SNRs. Larger pμ  is applied to produce smaller residual noise and larger signal 
distortion in principal subspace. On the contrary, smaller mμ  extracts residual speech with larger 
noise. In the experiment, we tuned two multipliers in ranges of 30 p ≤≤ μ , 70 m ≤≤ μ . Significance 
level was set as 95.0=α  in two FA selection methods. 
 
4.2. Experimental Results 
The effectiveness of FA subspace modeling and selection is illustrated in Table 1. We report 
recognition rates averaged by three test sets in clean training for cases of baseline system, SS and FA 
approaches. FA subspace selection methods via testing eigenvalues and variances are labeled by FA I 
and FA II, respectively. We find that subspace denoising procedures do improve baseline speech 
recognition rates. FA I and FA II outperform SS in presence of different SNR conditions. The lower 
the SNR, the better the improvement is obtained. FA II achieved higher recognition rates than FA I. 
We have confirmed the statistical significance of recognition improvement of using FA compared to 
SS via matched-pairs test. Also, similar results are obtained when evaluating different methods in 
multi-condition training as shown in Table 2. Improvement is moderate. From two sets of experiments, 
we assure the effectiveness of proposed subspace model with selection algorithms for noisy speech 
recognition. 
 
Table 1-1. Accuracies (%) of different methods and noise conditions in case of clean training 
 Baseline SS FA I FA II 
Clean 99.1 99.3 99.3 99.3 
20 dB 97.4 97.7 97.7 97.9 
15 dB 93.8 94.5 94.7 94.9 
10 dB 81.7 85.0 86.0 87.2 
5 dB 56.8 64.0 66.3 70.1 
0 dB 30.3 38.3 41.8 44.7 
-5 dB 15.2 19.8 21.3 22.7 
 
Table 1-2. Accuracies (%) of different methods and noise conditions in case of multi-condition 
training 
 Baseline SS FA I FA II 
Clean 98.9 98.9 99.0 99.0 
20 dB 98.3 98.6 98.6 98.6 
15 dB 97.6 97.9 98.0 98.1 
10 dB 95.6 96.3 96.4 96.5 
5 dB 88.3 90.9 91.3 91.5 
0 dB 63.0 75.3 75.8 76.6 
-5 dB 27.5 45.1 45.7 46.5 
 
8
Second Year Project: Maximum Entropy Joint Modeling 
1. INTRODUCTION 
Automatic speech recognition has been increasingly important in many human-machine interaction 
systems. How to build a desirable classification procedure is critical to assure system performance. 
Following Bayesian decision theory, speech recognition endeavors to find the most likely word 
sequence Wˆ  through maximizing a posteriori (MAP) probability given an observed speech sentence X  
)()(maxarg)(maxargˆ WpWXpXWpW
WW ΓΛ
== .                                  (2.1)
 
In (2.1), )( WXpΛ  represents acoustic likelihood of matching signal X  with hidden Markov models 
(HMM’s) for W . The prior probability )(WpΓ  serves as language model characterizing the linguistic 
regularities in natural language. N-gram model is popular to explore local lexical characteristics from 
text documents. Undoubtedly, the estimation of HMM’s Λ  and n-grams Γ  plays an important role in 
speech recognition system. In the literature, maximum likelihood (ML) criterion is widely applied for 
parameter estimation. The estimated parameters can attain the largest likelihood score using training 
data. However, higher likelihood does not guarantee better classification. To improve classification 
performance, it is beneficial to directly enhance model discriminability. The minimum classification 
error (MCE) and maximum mutual information (MMI) criteria were proposed for discriminative 
modeling of acoustic [3][20][26] as well as linguistic features [7][10][23]. Under the assumption of 
independence between acoustic and linguistic events, parameters of HMM and n-gram were optimized 
individually. These parameters were used to calculate acoustic )( WXpΛ  and linguistic )(WpΓ  
likelihoods to determine the optimal word sequence Wˆ  according to plug-in MAP decoding in (2.1). 
Nevertheless, considering the hierarchical structure from phonetic-level matching to sentence-level 
matching, such assumption was unrealistic to estimate truly optimal acoustic and linguistic model 
parameters. In [24], transition weight and language model in finite state decoding graphs were 
simultaneously optimized under MCE criterion. In [16], HMM and unigram features were induced for 
hidden condition random fields (HCRFs) based phone classification. Maximum conditional likelihood 
criterion was used for parameter estimation. Here, we systematically build an integrated model 
combining HMM and n-gram features using the maximum entropy principle for continuous speech 
recognition. More importantly, we present the modularized framework for joint estimation of acoustic 
and linguistic parameters. The dependence between acoustic and linguistic features is properly 
considered. We illustrate the effectiveness of using integrated ME approach compared to conventional 
ML training in plug-in MAP classification system for LVCSR. 
2. ACOUSTIC AND LINGUISTIC MODELING 
No matter using ML, MMI, MCE or ME criterion, acoustic and language models were separately 
estimated for plug-in MAP speech recognition. The dependencies between acoustic and language 
models were neglected in model estimation. Strictly speaking, HMM and n-gram parameters express 
the information sources in different levels conveying important cues for finding reliable word 
candidates underlying input speech signal. These two model sets should be jointly established in a 
consistent way or following the same objective function. To release the independence assumption, one 
statistically attractive approach is to adopt ME principle simultaneously for acoustic and linguistic 
modeling. In what follows, we construct a modularized ME framework integrating acoustic and 
linguistic features for building speech recognition system. 
 
10
∑
=Θ
Θ =
F
i
ii LSXWfXp
XLSWp
1
LALA ),,,(exp
)(
1),,( λ ,                            (2.9) 
with normalization term )(XpΘ . Here, we directly estimate the posterior probability using ME 
approach so that acoustic and linguistic parameters can be optimized simultaneously. The associations 
between acoustic and language models are properly exploited and merged in the ME model. 
 
2.2. Relations of ME Model to HMM and N-Gram 
The proposed ME model can seen as a generalization of standard HMM’s and n-gram models. To 
illustrate the relations, we specify the corresponding parameters. We would like to expand posterior 
distribution to express the specification of parameters for HMM and n-gram features. Using 
continuous-density HMM parameters ),(),({},,{ 11 −= tt sspspBAπ  },),( 2 dlsdlstt ttttslp σμ  with Gaussian 
mixture observation probability and n-gram parameters )}({ 1 1
−
+−
i
nii wwp , the posterior probability is 
calculated with scaling factor by ( )
⎟⎟
⎟
⎠
⎞
⎟⎟⎠
⎞
⎜⎜⎝
⎛ −−
⎜⎜⎝
⎛ ⋅⋅⋅=
≈
∏
∏∏ −− +−
d dls
dlsdt
dls
tt
t
tt
i
i
nii
tt
tt
tt
x
slpsspspwwp
WLSXpWLSpWpXLSWp
2
2
,
2
11
1
1
2
)(
exp
2
1         
)()()()(    
),,(),()(),,(
σ
μ
πσ
α
α
.                                      (2.10) 
After careful arrangement, this posterior distribution can be represented in a consistent form consisting 
of different sets of log linear distributions 
⎟⎟⎠
⎞++
⎜⎜⎝
⎛ +
∑ ∑∑
∑∑
=− −−
+−
+−+−
K
k dls
m
dls
m
dls
ss
a
s
a
s
s
ss
w
ww
tt
k
tt
k
tt
tt
t
t
t
t
i
ni
i
ni
i
ni
LSXWfLSXWf
LSXWfLSXWf
0 ,,
,,,,
,
LL
),,,( ),,,(           
),,,(),,,(exp   
1
11
1
11
1
11
λλ
λλ ππ
.                          (2.11) 
According to the feature functions in (2.3)-(2.6), we show the associations between ME parameters 
and standard parameters as follows: 
)(log 1 1
L
1
−
+−=+− i niiw wwpi ni αλ ,                                                         (2.12) 
)(log 11 sps =πλ  ,                                                               (2.13) 
)(log 1
1
−=− iias sspttλ ,                                                             (2.14) 
⎟⎟⎠
⎞
⎜⎜⎝
⎛ +−= 2
2
22log
2
1)(log0
dls
dls
dlstt
m
dls
tt
tt
tttt
slp σ
μπσλ ,                                       (2.15) 
21
dlsdls
m
dls tttttt
σμλ = ,                                                               (2.16) 
2212 dls
m
dls tttt
σλ −= ,                                                                (2.17) 
,...,Kk,k
tt
m
dls 3     0 ==λ .                                                          (2.18) 
Using these parameter settings, the ME model can completely specify the characteristics of standard 
HMM and n-gram models.  
 
12
top one competing sentence was explored. Using Katz back-off smoothing, baseline ME bigram 
language model was trained using Academic Sinica CKIP balanced corpus constructed by about five 
million words with lexical size of 32,909. 
In the experiments, we focused on three major parameters, including n-gram, state transition 
(Trans.) and first-order observation parameters (Obs1.). To evaluate speech recognition performance, 
we report syllable, character and word error rates. In Table 2-1, we show the performance with 
individually updated parameters. Baseline system adopts conventional HMM and n-gram parameters. 
Interestingly, we obtain desirable syllable error rates via updating observation parameters while good 
word error rates are achieved by updating n-gram parameters. This is reasonable because observation 
parameters play the critical role for modeling acoustic properties, which affects syllable error rate 
significantly. N-gram parameters are used to characterize word association so that word accuracy can 
be improved. 
We also implemented the experiments using integrated model to illustrate the effectiveness of 
different parameter combinations. Error rates using different parameter combinations are shown in 
Table 2. We find that integrated parameters achieve better improvements than using individually 
updated parameters. Updated n-gram parameters have slight improvement of syllable error rate with 
1.7% and combined n-gram and acoustic features can achieve 3.9%. Namely, combining language and 
acoustic models in ME model does improve speech recognition. Furthermore, we obtain the best word 
error rate improvement with 5.9% based on integrated model with all updated parameters. 
 
Table 2-1. Evaluation for different updated parameters 
Maximum Entropy  Baseline N-gram Trans. Obs1. 
SER (%) 29.1 28.6 (1.7) 28.6 (1.7) 28.5 (2.2)
CER (%) 36.8 35.3 (4.1) 36.4 (1.1) 35.9 (2.4)
WER (%) 48.9 46.9 (4.1) 48.3 (1.2) 47.8 (2.2)
 
Table 2-2. Evaluation for combinations of updated parameters 
Maximum Entropy  Baseline 
Trans.+ Obs1. N-gram+Trans. N-gram+Obs1. All 
SER (%)  29.1 28.1(3.4) 28.0(3.9) 28.0(3.9) 27.8(4.5) 
CER (%) 36.8 35.7(3.0) 35.1(4.6) 34.9(5.2) 34.6(6.0) 
WER (%) 48.9 47.6(2.7) 46.5(4.9) 46.5(4.9) 46.0(5.9) 
5. SUMMARY 
We have presented a joint modeling approach to acoustic and linguistic features to effectively 
estimate correlated parameters for speech recognition. This approach released the independence 
assumption which was made by many speech recognition systems. Importantly, we developed a ME 
framework jointly merging HMM and n-gram features and modeling their dependencies in a 
consistent and optimal fashion. Specially, ME model involved a constrained optimization procedure 
and was powerful for knowledge integration. In the experiments on broadcast news transcription, we 
obtained desirable performance compared to conventional recognition system using independent 
HMM and n-gram parameters. 
 
14
Bayesian approach. We are describing the robust and discriminative capabilities of applying proposed 
OBC decision rule. 
 
3.1. Test of Classification Loss 
The specification of loss function is crucial for optimal Bayes decision. Basically, whether the 
classification )(Xd  produces loss or not is referred as a two-class PR problem. To formulate the 
confidence or loss due to a classification action, we describe the mathematical model as a hypothesis 
test problem. According to the outcomes of loss and lossless classification events, null 0H  and 
alternative 1H  hypotheses are naturally defined by 
H0: test data X  is misclassified, or )(Xd  produces loss. 
H1: test data X  is not misclassified, or )(Xd  is lossless. 
From Neyman-Pearson’s Lemma, the optimal solution to hypothesis testing is called likelihood ratio 
test. Having probability distributions of two hypotheses, null hypothesis 0H  is accepted if likelihood 
ratio exceeds a critical threshold 
τ>=
≠=
))(:)(,(
))(:)(,(
LR
1
0
WXdHXdXP
WXdHXdXP
.                 (3.1) 
However, we don’t know true distributions of null hypothesis ))(,( 0HXdXP  and alternative 
hypothesis ))(,( 1HXdXP . Implementation of loss function using likelihood ratio ))(,(LR XdWl  
shall be sensitive to the uncertainties of distribution forms and trained parameters. To setup a loss 
function robust to uncertainties of speech models, we present a novel Bayes loss function for OBC 
speech recognition. This function is built by solving hypothesis test problem using Bayesian approach 
[17][22] where model parameters },{ ΓΛ=Ξ  are random with prior distributions )(ΛP  and )(ΓP . A 
Bayes factor is yielded and expressed using predictive distributions )(~ WXP  and )(~ WP  
∫∫
∑ ∫∫
∑
ΓΛ
ΓΛ
Ω ΓΩ Λ
≠ Ω ΓΩ Λ
≠
ΓΓΛΛ
ΓΓΛΛ
=
===
dPWPdpWXP
dPXdPdPXdXP
WPWXP
XdPXdXP
HXdXP
HXdXP
XdWb
WXd
WXd
)()()()(
)())(()())((
      
)(~)(~
))((~))((~
))(,(~
))(,(~
))(,(BF
)(
)(
1
0
               (3.2) 
In (3.2), the numerator sums up all joint predictive distributions ))(,(~ XdXP  corresponding to 
misclassification actions WXd ≠)(  while the denominator involves only the predictive distribution 
for the case of correct classification WXd =)( . In LVCSR implementation, the word candidate with 
the highest predictive score is referred as true transcription W . The other competing word candidates 
at the same word segment are included in word set of null hypothesis WXd ≠)( . These word 
candidates are found from word lattices produced by word graph generation algorithm. To determine 
an effective Bayes factor, we can empirically merge tuning factors when calculating predictive 
distributions for different competing words. Using this Bayes factor, we are able to develop a new 
parametric loss function better than conventional zero-one, confidence measure and word error rate 
loss functions. The resulting loss function is robust because predictive distributions are calculated to 
tackle randomness of trained model parameters. 
 
 
16
))(,(
)())(,(log)(),(log
))(,(~  log),(~log))(,(log
)(
)(
XdWm
dPXdXPdPWXP
XdXPWXPXdWb
WXd
WXd
=
ΞΞ+ΞΞ−=
+−=
∑ ∫∫
∑
≠ ΩΩ
Ξ
≠
ΞΞ
.                  (3.8) 
Attractively, we can interpret misclassification measure in MCE as a logarithmic Bayes factor or 
confidence measure for testing the hypothesis of misclassification action against that of correct 
classification action. Therefore, considering these properties, it is meaningful to claim that OBC 
decision rule can achieve discriminative classification because minimizing expected loss for OBC is 
comparable to minimizing classification errors or enhancing model discriminability. 
3. EXPERIMENTS 
3.1. Databases and Experimental Setup 
We carried out Bayes decision rules for broadcast news transcription. LVCSR decoder contained 
lexicon tree, acoustic model and language model. In lexicon set, we used 74,868 Chinese words. Each 
word had at most four characters. All words were organized in a tree structure for within-word search. 
Acoustic model set consisted of Initial/Final sub-syllable HMM’s for Mandarin speech recognition. 
Initial and Final HMM’s had three and five states, respectively. Each state had at most 32 Gaussian 
mixture components. We used 7080 utterances from TCC300 speech database to train seed speaker 
independent HMM’s. Then, we performed MAP task adaptation for LVCSR of MATBN broadcast 
news corpus using 680 MATBN utterances (35.8 minutes). In order to estimate the parameters in 
sigmoid function, loss function and language model weighting, we prepared 700 utterances as the 
held-out set. Also, there were 500 test utterances (11 minutes) containing 4105 characters. MATBN 
were shared by the Public Television Service Foundation of Taiwan and collected by Academia Sinica, 
Taiwan. Each speech frame was parameterized as 39-dimensional feature vector of 12 Mel-frequency 
cepstral coefficients (MFCC), one log energy and their first and second derivatives. Sentence-based 
cepstral mean subtraction was performed. Trigram language model was trained using CIRB corpus 
(about 342MB) via SRI language model toolkit. Good-Turing smoothing was applied. Language 
model perplexity was 437. We reported character error rate (CER) performance for different decision 
rules. In this study, we compared MAP, MBR and OBC decision rules. Different loss functions were 
incorporated in word graph rescoring. Word-conditioned tree copy search was performed to build 
word graph. MAP decoding was referred as the baseline system. MBR with Levenshtein loss function 
was implemented for comparison. OBC decoding using Bayes loss function calculated with predictive 
distribution and with different priors were investigated. 
 
3.2. Implementation Issues 
In OBC rule implementation, we only calculated predictive distribution ),(~ WXP μ  considering 
the uncertainty of HMM mean vector μ . The other HMM parameters and trigram parameters were 
assumed to be deterministic. Prior density of HMM mean vector was modeled by a state-level tied 
Gaussian distribution ),()()( Σ==Λ mNPP μμ  with mean vector m  and covariance matrix Σ . Predictive 
distribution was derived in a form of Gaussian distribution [11]. Here, hyperparameters ),( Σm  were 
empirically estimated from training data via taking sample mean and variance of maximum likelihood 
parameters. Similar technique was applied to determine those hyperparameters corresponding to 
 
18
 
REFERENCES 
[1] M. Afify, F. Liu, H. Jiang and O. Siohan, “A new verification-based fast match for large vocabulary continuous speech 
recognition”, IEEE Trans. Speech and Audio Processing, vol.13, no. 4, pp. 546-553, 2005. 
[2] T. W. Anderson, “Asymptotic theory for principal component analysis”, Annals of Mathematical Statistics, vol. 34, pp.122-
148, 1963. 
[3] L. Bahl, P. Brown, P. de Souza and R. Mercer, “Maximum mutual information estimation of hidden Markov model 
parameters for speech recognition”, in Proc. of ICASSP, vol. 1, pp. 49-52, 1986. 
[4] A. Basilevsky, Statistical Factor Analysis and Related Methods - Theory and Applications, John Wiley & Sons, 1994. 
[5] S. F. Boll, “Suppression of acoustic noise in speech using spectral subtraction”, IEEE Trans. Acoustic, Speech and Signal 
Processing, vol. 27, pp. 113–120, 1979. 
[6] G. E. P. Box, “A general distribution theory for a class of likelihood criteria”, Biometrika, vol. 36, pp.317-346, 1949. 
[7] Z. Chen, K.-F. Lee, M.-J. Li, “Discriminative training on language model,” in Proc. of ICSLP, pp. 16-20, 2000. 
[8] J.-T. Chien and S. Furui, “Predictive hidden Markov model selection for speech recognition”, IEEE Trans. Speech and Audio 
Processing, vol. 13, no. 3, pp. 377-387, 2005. 
[9] J.-T. Chien and C.-W. Ting, “Speaker identification using probabilistic PCA model selection”, ICSLP, vol. 3, pp. 1785-1788, 
2004. 
[10]C.-H. Chueh, T.-C. Chien, and J.-T. Chien, “Discriminative maximum entropy language model for speech recognition,” in 
Proc. of INTERSPEECH, pp. 721-724, 2005. 
[11] R. O. Duda and P. E. Hart, Pattern Classification and Scene Analysis, John Wiley & Sons, 1973. 
[12] Y. Ephraim and H. L. Van Trees, “A signal subspace approach for speech enhancement”, IEEE Trans. Speech and Audio 
Processing, vol. 3, no. 4, pp. 251-266, 1995. 
[13] I. T. Jolliffe, Principal Component Analysis, Springer-Verlag, 1986. 
[14] V. Goel and W. Byrne, “Minimum Bayes-risk automatic speech recognition”, Computer Speech and Language, vol. 14, pp. 
115-135, 2000. 
[15]V. Goel and W. Byrne and S. Khudanpur, “LVCSR rescoring with modified loss function: a decision theoretic perspective”, 
in Proc. ICASSP, pp. 425-428, 1998. 
[16] A. Gunawardana, M. Mahajan, A. Acero and J. C. Platt, “Hidden conditional random fields for phone classification”,  
[17] H. Jiang and L. Deng, “A Bayesian approach to the verification problem: applications to speaker verification”, IEEE Trans. 
Speech and Audio Processing, vol. 9, no. 8, pp. 874-884, 2001. 
[18] I. T. Jolliffe, Principal Component Analysis, Springer-Verlag, 1986. 
[19] B.-H. Juang, W. Chou and C.-H. Lee, “Minimum classification error rate methods for speech recognition”, IEEE Trans. 
Speech and Audio Processing, vol. 5, no. 3, pp. 257-265, 1997. 
[20]B.-H. Juang and S. Katagirl, “Discriminative learning for minimum error classification”, IEEE Trans. Signal Processing, vol. 
40, pp. 3043-3054, 1992. 
[21] J. Kaiser, B. Horvat and Z. Kacic, “Overall risk criterion estimation of hidden Markov model parameters”, Speech 
Communication, vol. 38, pp. 383-398, 2002. 
[22] R. E. Kass and A. E. Raftery, “Bayes factors”, Journal of the American Statistical Association, vol. 90, no. 430, pp. 773-795, 
1995. 
[23] H.-K. J. Kuo, E. Fosle-Lussier, H. Jiang and C.-H. Lee, “Discriminative training of language models for speech recognition”, 
in Proc. of ICASSP, vol. 1, pp. 325-328, 2002. 
[24]S.-S. Lin and F. Yvon, “Discriminative training of finite state decoding graphs”, in Proc. of INTERSPEECH, pp. 733-736, 
2005. 
[25] L. Mangu, E. Brill and A. Stolcke, “Finding consensus among words: lattice-based word error minimization”, in Proc. 
EUROSPEECH, pp. 495-498, 1999. 
[26] Y. Normandin, R. Cardin and R. De Mori, “High-performance connected digit recognition using maximum mutual 
information estimation”, IEEE Trans. Speech and Audio Processing, vol. 2, pp. 299-311, 1994. 
[27] A. Stolcke, Y. Konig and M. Weintraub, “Explicit word error minimization in N-best list rescoring”, in Proc. 
EUROSPEECH, pp. 163-165, 1997. 
[28] M. E. Tipping and C. M. Bishop, “Mixtures of probabilistic principal component analyzers”, Neural Computation, vol. 11, 
pp. 443-482, 1999. 
[29] S. Wang, D. Schuurmans, F. Peng and Y. Zhao, “Learning mixture models with the regularized latent maximum entropy 
principle”, IEEE Trans. Neural Networks, vol. 15, no. 4, 2004. 
[30]F. Wessel, R. Schluter and H. Ney, “Explicit word error minimization using word hypothesis posterior probabilities”, in Proc. 
ICASSP, pp. 33-36, 2001. 
[31] F. Wessel, R. Schluter, K. Macherey and H. Ney, “Confidence measures for large vocabulary continuous speech 
recognition”, IEEE Trans. Speech and Audio Processing, vol. 9, no. 3, pp. 288-298, 2001. 
七月十三日 參訪京都大學並發表專題演講「機器學習方法應用於語音辨識」，與 Tatsuya 
Kawahara 教授及 Kawahara 實驗室之副教授、助手及博碩士學生共同討論
及交流，晚上受 Tatsuya Kawahara 教授之邀前往位於京都鴨川邊的傳統日
本式餐廳，享用京都市最有名的京都豆腐，該餐廳十分典雅且建築物已超
過百年歷史，在鴨川邊用餐氣氛相當難忘，晚餐後 Tatsuya Kawahara 教授
還帶我至市中心參觀京都市一年一度的祇園祭，真是一次豐富的學術及文
化之旅。 
七月十四日 參與 Kawahara 實驗室討論並收集計畫相關資料，與實驗室 Shinsuke Mori
副教授進行學術交流。 
七月十五日 參與 NTT Signal Processing Research Group 討論並收集計畫相關資料，與
Erik McDermott 博士進行學術交流。 
七月十六日 參與 NTT Signal Processing Research Group 討論並收集計畫相關資料，與
Shoko Araki 博士進行學術交流。 
七月十七日 參與 NTT Signal Processing Research Group 討論並收集計畫相關資料，與
Shinji Watanabe 博士進行學術交流。 
七月十八日 搭乘新幹線火車到達東京市，晚上下榻於浦田區的旅館。。 
七月十九日 參觀東京市。 
七月二十日 參觀 University of Tsukuba 並收集計畫相關資料。 
七月二十一日 發表專題演講並參與 Shoji Makino 教授實驗室討論及學術交流，專題演講
題目為「獨立成份分析應用於訊號分離及語音辨識」，Shoji Makino 教授是
該研究領域的世界級著名學者。訪問期間 Makino 教授帶我參觀筑波大學博
物館，館中陳列三位筑波大學教授榮獲諾貝爾獎的經過，其中兩位榮獲物
理學獎另外一位榮獲化學獎，陳列他們的研究生涯、手札及實驗器具，令
人印象深刻。另外，筑波大學在體育方面師資學員陣容非常堅強，館中還
陳列多位來自筑波大學的奧林匹克金牌得主的得獎事蹟，十分具有特色，
該館還陳列許多藝術品，藝術科系在筑波大學非常有名。演講結束後
Makino 教授帶我到一家相當道地的日本涮涮鍋餐廳，享用牛肉涮涮鍋並閒
聊一些相關領域的最新研究動態及現況。 
七月二十二日 參與 Dr. Shoji Makino 實驗室討論並收集計畫相關資料。 
七月二十三日 搭乘鐵路至東京成田機場並搭乘日亞航班機飛抵桃園國際機場。 
 
 
二、與會心得 
 
рৢ୯ሞᏢೌ཮᝼Јளൔ֋
ीฝጓဦ NSC 96-2628-E-006-141-MY3 
ीฝӜᆀ ऩυ಍ीБݤᔈҔܭ፾ᔈ܄ᇟॣᒣ᛽ (2/3) 
р୯Γ঩ۉӜ
୍ܺᐒᜢϷᙍᆀ
ᙁϘے ୯ҥԋфεᏢၗૻπำᏢس ௲௤
཮᝼ਔ໔Ӧᗺ ΜΒДΜϖВԿΜΒДΜΖВ GOA, ӑࡋ
཮᝼Ӝᆀ
)ύЎ*!2008ԃ IEEEαॊᇟقמೌ୯ሞࣴ૸཮!
)मЎ*!2008 IEEE Workshop on Spoken Language Technology (SLT) 
ว߄ፕЎᚒҞ
)ύЎ*!ೱុ܄ЬᚒᇟقኳࠠᔈҔܭᇟॣᒣ᛽!
)मЎ*!Continuous Topic Language Modeling for Speech Recognition 
)ύЎ*!ወӧ DirichletᇟقኳࠠᔈҔܭᇟॣᒣ᛽!
)मЎ*!Latent Dirichlet Language Model for Speech Recognition 
΋ǵୖу཮᝼࿶ၸ
р୯໒཮Չำൔ֋Ǻ
ΜΒДΜѤВ வѠࠄрวམًԿλෝᐒ൑ , མ४๮ૐ੤ᐒԿ३ෝ୯ሞᐒ൑ᙯམ JET 
AIRWAYS०ᐒ, ډၲӑࡋۏວᙯᐒԿӑࡋ Goa, ӆҗε཮Ӽ௨ௗᐒЃγԿ
໒཮Ӧᗺ-Holiday Inn Goa, ډၲ཮൑ਔ໔ࣁΜΒДΜϖВԐ΢ΐਔǶ
ΜΒДΜϖВ! ୖу SLTࣴ૸཮Ƕ!
ΜΒДΜϤВ! ୖу SLTࣴ૸཮Ƕ!
ΜΒДΜΎВ!!!!ୖу SLTࣴ૸཮Ϸఁ৏Ƕ٠ܭ᝼ำ(Topics in Speech and Language Modeling)
ว߄ٿጇፕЎȨೱុ܄ЬᚒᇟقኳࠠᔈҔܭᇟॣᒣ᛽ȩϷȨወӧ Dirichlet
ᇟقኳࠠᔈҔܭᇟॣᒣ᛽ȩǶ!
ΜΒДΜΖВ! ୖу SLTࣴ૸཮٠⨪ًԿ Goaᐒ൑, མ४ΠϱΎਔ JET AIRWAYS०ᐒԿ
ۏວᙯᐒԿ३ෝӆᙯᐒԿଯ໢-!མًԿѠࠄςΜΒДΜΐВΠϱΟਔǶ!
೭ԛр୯ୖуӧӑࡋ GoaᖐՉޑ 2008ԃ IEEEαॊᇟقמೌ୯ሞࣴ૸཮(SLT 2008)ӅѤ
ϺǴ೭΋سӈࣴ૸཮ࢂҗ୯ሞႝᐒႝηπำৣᏢ཮ (The Institute of Electrical and 
Electronics Engineers, IEEE)ЬᒤǴ؂ٿԃԃۭۓයܭШࣚӚεࠤѱ፺ࢬᖐՉǴSLTࢂα
ॊᇟقמೌ࣬ᜢሦୱύനڀж߄܄ЪፕЎௗڙ౗եޑ୯ሞ཮᝼Ǵ؂ԃ֎ЇٰԾШࣚӚӦ
CONTINUOUS TOPIC LANGUAGE MODELING FOR SPEECH RECOGNITION 
Chuang-Hua Chueh and Jen-Tzung Chien
Department of Computer Science and Information Engineering 
National Cheng Kung University, Tainan, Taiwan 70101, ROC 
{chchueh, chien}@chien.csie.ncku.edu.tw 
ABSTRACT
Continuous representation of word sequence can effectively 
solve data sparseness problem in n-gram language model, 
where the discrete variables of words are represented and 
the unseen events are prone to happen. This problem is 
increasingly severe when extracting long-distance 
regularities for high-order n-gram model. Rather than 
considering discrete word space, we construct the 
continuous space of word sequence where the latent topic 
information is extracted. The continuous vector is formed 
by the topic posterior probabilities and the least-squares 
projection matrix from discrete word space to continuous 
topic space is estimated accordingly. The unseen words can 
be predicted through the new continuous latent topic 
language model. In the experiments on continuous speech 
recognition, we obtain significant performance 
improvement over the conventional topic-based language 
model. 
Index Terms— Smoothing methods, clustering methods, 
natural languages, speech recognition 
1. INTRODUCTION
Language model plays an important role in automatic 
speech recognition. Using statistical n-gram language model, 
the probability of a word sequence is computed by a product 
of probabilities of individual words conditioned on their 
historical n-1 words. It is simple and efficient to calculate 
the probability of word sequence using n-gram model. 
However, the traditional n-gram model suffers from the 
problems of data sparseness and insufficient long-distance 
information [5]. Model generalization is limited. To deal 
with data sparseness problem, it is usual to adopt the back-
off scheme where the model parameters are smoothed by 
using low-order language models [8]. However, back-off n-
gram model does not consider word similarity. Each n-gram 
is smoothed individually. Some words should share the 
same smoothing parameters if they play similar role in 
lexical representation.
In the literature, the word relationships were employed 
for parameter smoothing in topic-based language models [6].
The words with the same topic share model distribution. 
More recently, the continuous language models were 
presented [1][2][3][10] to solve data sparseness problem. 
These methods performed vector projection using matrix 
extracted by neural network or latent semantic analysis and 
then estimated the probability distribution in low 
dimensional space. Using the continuous representation, the 
sparse lexical space is transformed to a compact continuous 
space. Each dimension of continuous vector is a 
combination of all lexical words so that the word 
relationship can be effectively considered. This paper 
presents a new continuous representation of historical words 
based on the scheme of topic decomposition. In addition to 
the incorporation of topic information, we estimate the 
least-squares spanning set of continuous topic space. 
Accordingly, we explore the continuous representation for 
unseen historical events and calculate the corresponding 
language model probabilities. In the experiments, we 
illustrate the effectiveness of proposed model in speech 
recognition compared to other methods. 
2. RELATED WORKS
Given a predictive word w  and its historical context h, the 
n-gram models exploit the conditional probability 
in a discrete vocabulary space. With a vocabulary of V
words, each history h is represented by using a 
vector h where the entries corresponding to seen words are 
assigned to be one. Those entries of unseen words are set to 
be zero. Such history vectors are too sparse to cover many 
unseen events. In contrast, a continuous space language 
model represents a history by a low dimensional vector with 
continuous values. Similar histories are transformed to the 
neighboring location and shared by the same parameters for 
probability calculation. The continuous space language 
modeling follows two steps; continuous representation and 
probability estimation
)|( hwp
11 unV
[10]. First, the history vector h is 
mapped into a continuous space. The probability 
distribution  is estimated in that space. The 
probability is computed through a projected history vector 
where each dimension is a combination of probabilities of 
vocabulary words. Word relationships are merged into the 
projected vector. This is helpful for generalization to unseen 
events.
)|( hwp
2.1. Neural network language model
In [3], a multi-layer perceptron (MLP) neural network was 
proposed for continuous language modeling. A shared 
projection matrix for each historical word was embedded in 
the first layer of the MLP. Maximum likelihood criterion 
was applied to estimate the synaptic weights of neural 
,((( 6/7
where  is the number of occurrences of h and w in 
the corpus. The EM algorithm is applied for parameter 
estimation with the latent topic variables. In E-step, we 
calculate the auxiliary function by taking expectation over 
hidden variable c by 
),( wc h
¦ ¦
w c
cwpwcpwc
,
)|,(log),|(),(
h
hhh ,               (3) 
where ),|( wcp h  is calculated based on the current estimate:
¦ c cc
 
c cpcwp
cpcwpwcp
)|()|(
)|()|(),|(
h
hh . (4)
In M-step, we take differential of (3) with respect to 
 and  and obtain the following solution)|( cwp )|( hcp
¦ ¦
¦
c cc
 
w wcpwc
wcpwccwp
h
h
hh
hh
),|(),(
),|(),()|( , (5)
¦ ¦
¦
c c
 
c w
w
wcpwc
wcpwc
cp
),|(),(
),|(),(
)|(
hh
hh
h . (6)
The latent topic language model is established by EM 
iterations [6].
3.2. Continuous space construction
Given the latent topic language model, we find the 
coordinate  of history h in topic-based continuous space. 
However, only the histories appearing in training corpus are 
modeled. To generalize the topic space, we estimate the 
projection matrix A that satisfies the constraint 
ph
||,||)1()1( HCpHVnVnC uuu  HHA , (7)
where  is the number of different histories consisting of 
n-1 words in training data. H  and  are the matrices 
with columns constructed by original vectors h and 
projected vectors , respectively. Each row vector  of 
A is extracted by a set of linear equations 
|| H
pH
ph
T
ia
1||,1)1(,)1(|| uuu   HiVni
T
VnH raH , (8)
where  is the ith row of . Typically,  is much 
larger than , which causes the over-determination 
of finding . We employ the least-squares method and 
minimize the sum of square errors 
T
ir pH || H
Vn )1( 
ia
¦
 
  
||
1
2
,
2
)(
H
j
jij
T
iii
T rE haraH , (9)
where  is the j-th entry of . By calculating the gradientjir , ir
)(2)(2
||
1
, ii
T
j
H
j
jij
T
i rE
i
raHHhhaa    ¦
 
, (10)
and setting it to zero, we yield the least-squares solution 
i
T
i rHHHa  
1)( . (11)
The projection matrix A  attaining the least-squares errors 
is estimated and used to build the topic space spanned by 
the bases Cii ,,1,  a . Given an event , we first 
calculate the projected history vector . The 
conditional probability  is then computed. 
Therefore, we determine the probabilities not only for seen 
history events but also for unseen history events.
),( wh
Ahh  p
)|( hwp
4. EXPERIMENTS
The Wall Street Journal (WSJ) corpus [9] was used to 
evaluate the proposed methods for continuous speech 
recognition. We used 5K non-verbalized punctuation closed 
vocabulary in the evaluation. The SI-84 training set was 
adopted to estimate acoustic parameters using feature 
vectors composed of twelve Mel-frequency cepstral 
coefficients, one energy and their first-order and second-
order derivatives. We selected ’87-89 WSJ text corpus with 
30M words to train baseline Katz trigram and topic 
language model [6]. The test set with 330 utterances was 
sampled from November 1992 ARPA CSR benchmark test 
data. The topic language model by Gildea and Hofmann [6]
(denoted by GHLM) and the proposed continuous topic 
language model (denoted by CTLM) were carried out for 
comparison. In the implementation, we first estimated the 
topic model parameters by (5) and (6) for GHLM as well as 
CTLM. Then, GHLM and CTLM calculated the topic
posterior probabilities of test history by the online EM 
approximation [6] and the least-squares projection matrix, 
respectively. The HTK toolkit was used for acoustic model 
training and lattice generation. The 100-best lists were 
extracted using baseline trigram. Different language models 
were then applied for N-best rescoring and finding 
recognition result. 
First of all, we investigate the effect of topic size (C) in 
topic-based language models using GHLM and CTLM and 
show the comparison of perplexities in Figure 2. The topic 
language models were accessed with four historical words 
(i.e. 5 n ). No matter using GHLM or CTLM model, the 
perplexity is consistently reduced by increasing the topic 
size. The proposed CTLM attains lower perplexity than 
GHLM because CTLM can predict the posterior probability 
for seen as well as unseen histories more reliably than 
GHLM.
To characterize local lexical regularities, we 
interpolated the latent topic language model (either GHLM 
or CTLM) with Katz’s back-off trigram model. The baseline 
trigram attains word error rate (WER) of 5.29%. The 
recognition performance is compared in Figure 3. The 
interpolated model using GHLM with 120 topics reduces 
WER to 5.25%. However, the interpolated model using 
CTLM with 240 topics improves word error rate to 5.08%.  
When adopting larger number of topics, the recognition 
performance is degraded due to overfitting problem. These 
results were obtained by considering four historical words 
in GHLM and CTLM. In case of CTLM with 240 topics and 

LATENT DIRICHLET LANGUAGE MODEL FOR SPEECH RECOGNITION 
Jen-Tzung Chien and Chuang-Hua Chueh 
Department of Computer Science and Information Engineering 
National Cheng Kung University, Tainan, Taiwan 70101, ROC 
E-mail:{chien, chchueh}@chien.csie.ncku.edu.tw 
ABSTRACT 
Latent Dirichlet allocation (LDA) has been successfully 
presented for document modeling and classification. LDA 
calculates the document probability based on bag-of-words 
scheme without considering the sequence of words. This 
model discovers the topic structure at document level, 
which is different from the concern of word prediction in 
speech recognition. In this paper, we present a new latent 
Dirichlet language model (LDLM) for modeling of word 
sequence. A new Bayesian framework is introduced by 
merging the Dirichlet priors to characterize the uncertainty 
of latent topics of n-gram events. The robust topic-based 
language model is established accordingly. In the 
experiments, we implement LDLM for continuous speech 
recognition and obtain better performance than probabilistic 
latent semantic analysis (PLSA) based language method. 
Index Terms—Natural languages, Bayes procedures, 
clustering methods, smoothing methods, speech recognition
1. INTRODUCTION 
The statistical n-gram language models have been 
successfully developed for continuous speech recognition 
and many other applications. The n-gram model suffers 
from the insufficiencies of training data and long distance 
information, which limits the model generalization. The 
association pattern language model was presented to explore 
long distance associations of multiple words and merge 
them in n-gram model [3]. This insufficiency was also 
compensated by extracting the latent semantic information 
for topic-based language modeling. The latent semantic 
analysis (LSA) language model [1] was built accordingly. 
LSA performed matrix decomposition and found latent 
semantic information for different words and documents. 
Hofmann [6] proposed the probabilistic LSA (PLSA), 
where the latent topic parameters were estimated by 
maximum likelihood method via the EM algorithm. The 
semantic regularities in n-gram events were exploited  for 
speech recognition [4][8]. In general, PLSA characterized 
the collected documents individually by different 
parameters. Parameter size was increased significantly. The 
unseen documents could not be predicted. Blei et al. [2] 
presented the latent Dirichlet allocation (LDA) and 
improved the PLSA model by merging Dirichlet priors for 
topic mixtures. Seen and unseen documents were 
consistently generated by the LDA parameters, which were 
estimated by variation inference method. The model 
complexity was controlled. The new documents were 
generalized. LDA has been successfully applied in 
document classification [2] and information retrieval [11]. 
More recently, LDA was employed for adaptation of 
language model [5][9]. Nonetheless, PLSA and LDA were 
developed by exploiting topic information at document level. 
The extracted topic statistics are not directly representative 
for speech recognition, where the latent topics of n-gram 
events should be concerned. 
In this work, we endeavor to build the latent Dirichlet 
language model (LDLM), where the topic structure of LDA 
model is merged in generation of language model 
probability. Different from LDA for modeling document 
probability, LDLM characterizes the n-gram regularities 
from different documents. A Bayesian approach is 
presented to calculate the model parameters under the 
assumption of latent topics of n-gram events being Dirichlet 
distributed. The topic characteristics are expressed in 
conditional probability of a word given its history, which is 
consistent to the function of word prediction for speech 
recognition. This LDLM is smoothed by incorporating the 
topic information and estimated by maximizing the marginal 
likelihood of training data. The uncertainty of topic 
variables is compensated for robust language modeling. 
2. LATENT DIRICHLET ALLOCATION 
LDA extends the PLSA model by treating the latent topic of 
each document as a random variable. The number of 
parameters is controlled even through the size of training 
documents is increased significantly. Different from PLSA, 
LDA model is capable of calculating likelihood function of 
unseen documents. Typically, LDA is a generative 
probabilistic model for documents in text corpus. The 
documents are represented by the random latent topics, 
which are characterized by the distributions over words. The 
graphical representation of LDA is shown in Figure 1. The 
LDA parameters consist of },{ ȕĮ  where ],,,[ 21 CDDD  Į
denotes the Dirichlet parameters of C latent topic mixtures, 
and ȕ  is a matrix with multinomial entry )|(, iwi cwp E .
Using LDA, the probability of an N-word document  
],,,[ 21 Nwww  w  is calculated by the following procedure. 
,((( 6/7
uncertainty of topic mixture vector ș  is incorporated for 
robust language modeling. 
ȕ
A
h
ș
Figure 2 Graphical model of LDLM 
3.2. Model estimation 
The parameters },{ ȕA  of LDLM can be estimated by 
maximizing the marginal distribution accumulated from all 
training data 
¦
w
wpwc
,
),,|(log),(
h
ȕAhh ,                    (6)
where ),( wc h  is the number of occurrence of n-gram event 
),( wh  in training corpus. The LDLM ),,|( ȕAhwp  calculates 
the probability of a word w conditioned on its historical 
words h. The integral over ș  in (5) is computed by 
¦
³
 
  C
j
T
j
T
i
ii dcppcp
1
)|(),|(),|(
ha
ha
șșAhșAh ,     (7) 
which is an expectation of Dirichlet distribution of a latent 
topic ic . The variational inference is not needed at this 
circumstance. The probability of n-gram event using LDLM 
is expressed by 
¦
¦
  
 C
j
T
j
T
i
C
i
icwpwp
11
),|(),,|(
ha
ha
ȕȕAh .             (8) 
Notably, the hidden variable ic  is used in marginal 
likelihood function ),,|( ȕAhwp . The problem of 
incomplete data ),( wh  happens in model estimation 
procedure. EM algorithm should be applied by considering 
complete data ),,( cwh . In E-step, the auxiliary function of 
new estimates },{ ȕA cc  given current estimates },{ ȕA  is 
calculated by taking the expectation of marginal likelihood 
function of (6) over the hidden variable ic  by 
¦
¦
¦
¦ ¦
¦
»
»
¼
º
«
«
¬
ª
c
c
c 
cc 
cc cc
  
 
w
C
j
T
j
T
i
i
C
i
i
w
i
C
i
i
w
c
cwpwcpwc
cwpwcpwc
cwpEwcQ
, 11
, 1
,
,),|(log),,,|(),(
),,|,(log),,,|(),(
],),,|,([log),(),|,(
h
h
h
ha
ha
ȕȕAhh
ȕAhȕAhh
ȕAȕAhhȕAȕA
    (9) 
where ),,,|( ȕAh wcp i  is the posterior probability calculated 
by using the current estimate },{ ȕA
¦  

 C
j
T
jj
T
ii
i cwp
cwp
wcp
1 ),|(
),|(
),,,|(
haȕ
haȕ
ȕAh .          (10) 
In M-step, we maximize the auxiliary function with respect 
to the parameters ),|(, ȕc c iwi cwpȕ  and iac  and find the 
updated estimates },{ ȕA cc . The Lagrange multipliers 
],,[ 1 COOO   are applied in estimation of probability 
parameters ȕc  where the extended objective function is 
established by 
¦ ¦
 
»
¼
º
«
¬
ª ccc
C
i w
ii cwpQ
1
1),|(),|,( ȕȕAȕA O .             (11) 
The new parameter wi,E c  is updated by the closed-form 
formula 
¦ ¦
¦
c cc
 c
w i
i
wi wcpwc
wcpwc
ȕ
h
h
ȕAhh
ȕAhh
),,,|(),(
),,,|(),(
, ,             (12) 
But, no closed-form solution exists for parameter Ac . The 
decent algorithm is employed to find 1)1( u Vn  parameter 
vector iac . To do so, the gradient function ),|,( ȕAȕAa cc cQi
is calculated by 
h
haha
ȕAhh
h
11),,,|(),(
, 1
¦
¦ »
»
¼
º
«
«
¬
ª
c

c
 w
C
j
T
j
T
i
i wcpwc ,       (13) 
and the new parameter )1t( cia  at )1t(  iteration is updated by 
),|,()t()t()1t( ȕAȕAaa a ccc c c
 Q
iii
K ,               (14) 
with learning rate K . LDLM parameters are estimated by 
several EM iterations. 
3.3. Comparison of LDA, LDA bigram and LDLM 
It is interesting to investigate the relations of LDA [2], LDA 
bigram [10] and LDLM. The LDA and LDA bigram 
endeavor to characterize the word distributions of 
documents, which contain bags of unigrams and bigrams, 
respectively. Their likelihood functions are calculated 
individually for training documents by using the shared 
Dirichlet parameter Į . New documents are generated from 
the corresponding word collections. In contrast, LDLM 
focuses on exploiting the word distribution given the 
historical words. The observed event consist of the current 
word and its historical words. The current word is predicted 
based on the history-dependent Dirichlet parameter, which 
is controlled by a shared matrix A and the history vector h.
Different from LDA language model adaptation [5][9], 
LDLM directly merges the prior uncertainty of latent topics 
in generation of language model probability. LDLM 
calculates the predictive/marginal distribution over topic 
mixtures for Bayesian language modeling. Using LDA 
bigram, each document is represented as a collection of 
bigram events. When trigram or higher order n-gram are 
considered, the size of parameters ȕ  grows exponentially. 

