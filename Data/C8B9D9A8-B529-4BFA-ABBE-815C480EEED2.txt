data storage and transmission control protocols based 
on a memory-centric on-chip interconnection network 
(OCIN). The memory-centric OCIN provides the micro-
architecture and building blocks, including routers, 
link wires and network interfaces (NIs), for the on-
chip data communication platform. For the memory-
centric OCIN, a two-level FIFO buffer router is 
proposed to enhance the on-chip network performance 
by increasing the utility of the centralized buffer 
and reducing the head-of-line blocking problems. 
Accordingly, an adaptive congestion-aware routing 
algorithm is also proposed to further increase the 
performance of mesh networks by detecting the traffic 
around a routing node. 
英文關鍵詞： On-chip interconnection network, Multi-core, Two-
level FIFO buffer router, Adaptive congestion-aware 
routing algorithm 
 
 2 
以記憶儲存為重心之晶內資料傳輸應用於節能效益之多核心系統 
 
計畫編號：NSC99-2221-E-009-189 執行期間：99年8月1日至100年9月30日 
主持人：黃威教授 國立交通大學電子工程研究所 
 
一、中文摘要 
 
隨著多核心系統的發展，系統晶片運算能力也隨之快速成長。然而晶片內資料
傳遞及資料儲存卻無法跟上多核心的發展腳步，進而成為多核心系統的瓶頸。因此，
晶片上不僅需要放置更多、更快、且低功耗的記憶體來提供儲存資料，也必須針對
不同的系統應用來建構相對應的資料傳輸平台；為此，我們將提出一個以記憶儲存
為中心之晶內資料傳輸平台。此晶內資料傳輸平台將藉由階層式記憶體管理單元的
協助與管理，提供異質多核心系統具節能效益之資料傳輸及資料儲存，已達到系統
效能的最佳化。本計劃所提出的晶內資料傳輸平台專司晶內資料傳遞、資料儲存及
其通訊協定之設計。我們將結合電路與架構的設計，提出管理晶片上動態記憶體分
配、頻寬、與資料傳輸之機制，此外還負責管理晶片內外間資料傳輸之方法，且著
重於低功耗之設計以解決多核心系統資料傳遞及資料同步所將遇到之瓶頸。以記憶
體為重心之晶內連線網路提供微架構及構成要素給晶內資料傳輸平台，構成要素包
括了路由器(router)、連接導線(link wires)及網路介面(network interface)。在晶內路由
器方面，本計畫提出了兩階層先進先出資料暫存器及具堵塞感知之適應性路由演算
法，兩階層設計將資料暫存器分成分散式資料暫存器及集中暫存器，藉由提升集中
式資料暫存器的使用率及降低線頭阻塞來提升晶內網路效能。具堵塞感知之適應性
路由演算法，藉由偵測路由器附近的資料傳輸情況－可以避開雍塞的路線。 
 
關鍵字 
晶內資料傳輸平台、多核心、兩階層先進先出資料暫存器、路由演算法
 4 
二、計畫的緣由與目的 
On-Chip interconnect network (OCIN) or network-on-chip (NoC) architectures for 
on-chip data communication were investigated by [1] as a method for dealing with a 
number of challenges caused by the scale and complexity of next generation SoC designs. 
Additionally, NoC design has been considered an effective solution for integrating a 
multi-core system and process-independent interconnection architecture [2,3]. The 
generic NoC architecture is based on a homogeneous and scalable switch fabric network, 
which considers all requirements associated with on-chip data communication and traffic. 
NoC platforms have a few beneficial characteristics, namely, low communication latency, 
low energy consumption constraints, and design-time specialization [4]. The motivation 
in establishing NoC platforms is to achieve performance using a system communication 
perspective. 
Advanced NoC designs using nano-scale technologies face a number of challenges, 
especially for great amount of energy consumption in switch fabrics (or called router) and 
link wires. A router consists of a set of input buffers, an interconnect matrix, a set of 
output buffers and a routing controller and an arbiter. A router dominates the overall 
performance of the NoC platforms. The average latencies of submitted packets will 
highly rely on the design of buffers and the routing controller. Decreasing the average 
latencies of packets not only increases the performance but also reduces energy 
consumption of NoC platforms. Thus, a framework for application-aware routing was 
proposed to target minimization of latency [5]. Link wires will comprise a limiting factor 
for performance and energy consumption. Three critical issues, delay, power and 
reliability must be addressed. Therefore, a bidirectional channel NoC architecture was 
proposed to enhance the performance of NoC by transmitting flits in either direction 
dynamically [6]. Additionally, some coding schemes were proposed to address signal 
integrity for link wires in NoC [7,8]. In advanced technologies, circuits and interconnects 
degrade further due to noise with decreasing operating voltages. Furthermore, increasing 
coupling noise, the soft-error rate, and bouncing noise also decrease the reliability of 
circuits. Thus, self-calibrated and low-energy circuitry has become essential for 
near-future NoC designs. 
 6 
Self-corrected green coding scheme generates low energy and reliable link wires for 
advanced technologies. The self-corrected green coding scheme is constructed via two 
stages, the green bus coding stage and triplication error correction coding stage. The 
green bus coding has the advantages of shorter delay for error correction coding, greater 
energy reduction and smaller area than other approaches. The green bus coding is 
developed using the joint triplication bus power model to achieve additional energy 
reductions for triplication error correction coding. 
x0
x1
xk-1
x0
x1
xk-1
Triplication
Channels
Majority Gate : 
a
b
c
FM
FM = ab + bc + ca 
Decoder
 
Fig. 2.  Triplication Error Correction Stage. 
Fig. 2 shows the triplication error correction coding stage which is a single error 
correcting code by triplicating each bit. Each bit can be corrected individually when no 
more than two error bits exist in the three triplicated bits, which are defined as a 
triplication set. The error bit can be corrected by a majority gate. Fig. 2 also shows the 
function of the majority gate. Compared with other error correction mechanisms, the 
critical delay of the decoder is a constant delay of a majority gate and significantly 
smaller than that of other approaches [7–8]. Restated, the triplication error correction 
coding has rapid correction ability via self-correction mechanism at the bit level. 
Therefore, triplication error correction coding is more suitable to NoC platforms because 
data can be decoded and encoded in each switch fabrics using the small delay of 
triplication correction coding. 
 8 
Data words Codewords
X3~X0 C4~C0
0  0  0  0
0  0  0  1
0  0  1  0
0  0  1  1
0  1  0  0
0  1  0  1
0  1  1  0
0  1  1  1
1  0  0  0
1  0  0  1
1  0  1  0
1  0  1  1
1  1  0  0
1  1  0  1
1  1  1  0
1  1  1  1
0  0  0  0  0
0  0  0  0  1
0  0  0  1  0
0  0  0  1  1
0  0  1  0  0
1  0  0  0  0
0  0  1  1  0
0  0  1  1  1
0  1  0  0  0
1  1  1  0  0
1  1  1  1  1
1  1  1  1  0
0  1  1  0  0
1  1  0  0  0
0  1  1  1  0
0  1  1  1  1
(a)
0000
1000
1100
1110
1111
0001
0010
0011
0100
0110
0111
0101
1101
1001
1011
1010
Orignial Set Converted Set
if (C4=1) then 
else 
C0 = X0 , C2= X2
C4 = X2X1X0 
     + X3X2X0   
     + X3X2X1
C0 = X0 , C2= X2
(b)
 
Fig. 4.  (a) The mapping table between 4-bit dataword and 5-bit codeword of the green 
bus coding stage (b) The two sets and Boolean expression of the green bus coding stage. 
The proposed self-calibrated voltage scaling technique is applied to reduce the 
operating voltage of link wires for energy reduction and ensure the reliability based on 
the self-corrected green coding scheme. The self-calibrated voltage scaling technique will 
identify the optimal operating voltage to trade off between energy consumption and 
reliability for the self-calibrated circuitry. Fig. 5 presents the architecture of the 
self-calibrated voltage scaling technique. The technique is constructed by comprising low 
swing drivers, level converters, voltage scaling control unit, crosstalk-aware test error 
detection stage and run-time error detection stage. Depending on the detections about the 
two error detection stages, the voltage control unit adjusts voltage swing levels of the link 
wires. The crosstalk-aware test error detection stage detects errors by maximal aggressor 
fault (MAF) test patterns [9] in the test mode. The run-time error detection stage detects 
errors using the double sampling data checking technique and the adaptive delay line. 
Moreover, the self-calibrated voltage scaling technique is tolerant of timing variations by 
the adaptive timing borrowing technique. In response to detected errors, the 
self-calibrated voltage scaling technique can reduce voltage swing to reduce energy 
consumption and guarantee reliability simultaneously. 
 10 
Start
Crosstalk-aware 
test error detection
Raise test 
voltage 
swing
No
Yes
NoYes
No
error free
Run-time error 
detection
V_scale =1 
Voltage 
scaling for 
next window
T_start =1 Yes
Differernt detect Bit error rate: 
Error rate<5%  : (fall)
             5%<Error rate<15%: (stay)
             15%<Error rate : (raise)
             
LV
MV
HV
 
Fig. 6. The control policy and voltage state diagram of self-calibrated voltage scaling 
technique. 
After the crosstalk-aware test error detection stage, the run-time error detection stage 
raises V_scale to trigger a scaling mechanism within every N clock cycles window. 
Based on the bit error rate, the voltage control unit can further increase or decrease the 
signal voltage swing during run-time. The bit error rate is defined as the ratio of total 
transmission data in one window to error data. If the bit error rate is less than 5%, signal 
voltage swing is reduced one level or kept at the lowest safe signal. However, if the bit 
error rate is lager than 5% but less than 15%, the signal voltage swing level is the same as 
that for the previous window. If the bit error rate is larger than 15%, signal voltage swing 
is increased one level or kept at the highest signal swing level. The range of bit error rate 
detection depends on properties of self-corrected green coding scheme. If un- coded input 
data are random, the probability of the forbidden pattern condition (two adjacent lines 
switch in opposite directions, e.g. ↑↓ or ↓↑) of the coding scheme is roughly 15%. 
 12 
neighboring nodes which indicate the traffic around this router. 
Header Decoder & 
Minimal/Non-Minimal 
Node Decision
Adaptive Decision 
Unit
Buffer Information 
Collector (East)
Input Packet
Buffer Information 
Collector (West)
Buffer Information 
Collector (South)
Buffer Information 
Collector (North)
S
c
o
r
e
 
c
a
l
c
u
l
a
t
o
r
score1
score2
score3
Crossbar
Node 
Information
Buffer Information 
Collector (this node)
 
Fig. 8.  The architecture of adaptive congestion-aware routing. 
4
3
4 414
4
8
4 472
3
0
4 451
3
5
4 450
1
7
4 443
Score_N = 3
Score_E = 5.5
Score_S = 6
(No SW turn)
(No EN, ES turn)
Router 
No.1
Router 
No.3
Router 
No.5
Router 
No.2
Router 
No.4
Destination
Minimal 
Paths
Non-minimal 
path
Specific 
switching 
value = 4
P
Column 1
Column 3
 
Fig. 9.  An example of score calculation and adaptive decision. 
The score of the next node is computed by referring the available buffer space of the 
next node and the neighboring nodes of the next node with an Odd-Even turn model [10]. 
Fig. 9 is an example of the score calculation. The light blue square represents the router, 
and the green square represents the buffer. ‘P’ represents the position of the packet. In 
addition, the number in the green square represents the number of available buffer unit. 
 14 
Injection Rate (%)
0 10 20 30 40 50
0
200
400
600
800
A
ve
ra
ge
 L
at
en
cy
 (C
yc
le
) XY routing
Odd-Even
NoP
DyXy
This Work
0 5 10 15 20 25 30
0
200
400
600
800
1000
XY routing
Odd-Even
NoP
DyXy
This Work
A
ve
ra
ge
 L
at
en
cy
 (C
yc
le
)
Injection Rate (%)
(a)
Uniform
Uniform with 6 hot spots(i=[2,3,7], j = [3,4])
(b)  
Fig. 10. The comparisons under the uniform patterns (a) without hotspots (b) with 6 hot 
spots. 
Asynchronous two-level FIFO buffer architecture reduces head-of-line blocking 
problems without increasing buffer size. Figure 11 shows the architecture of the 
asynchronous two-level FIFO buffer. The proposed architecture is modified from the 
synchronous two-level FIFO buffer architecture [13]. Further, the two-level FIFO buffer 
architecture has a shared memory mechanism for output buffers that can share memory 
elements of the centralized FIFO. Hence, the inputs can send data to the same output 
buffer at the same time slot via parallel virtual channels. 
The asynchronous two-level FIFO buffer scheme is constructed by a data-link 
scheduler, distributed level-1 FIFOs, and a data-link-based centralized level-2 FIFO. The 
centralized level-2 FIFO achieves shared buffering. Hence, the proposed data-link 
memory-based centralized level-2 FIFO is utilized in the two-level FIFO buffer 
architecture with the optimized FIFO utility. Each memory element has two stored fields, 
the data field and linker field. In a memory element, the data field stores previous data 
 16 
from different input channels and creates the link between the granted distributed level-1 
FIFO and the data-link scheduler. Therefore, the data-link scheduler transmits the granted 
data to the centralized level-2 FIFO or the output distributed level-1 FIFO depending on 
the buffering condition in the output channel. If no data exist in the centralized level-2 
FIFO and the distributed level-1 FIFO is not full, data are transmitted to the distributed 
level-1 FIFO. Restated, if the distributed level-1 FIFO is full or the centralized 
level-2FIFO for this output channel is occupied, the data are delivered to the centralized 
level-2 FIFO. For single access of the centralized level-2 FIFO, the data-link scheduler 
also requires a read arbiter and write arbiter to arbitrate congestion on the read and write 
ports.  
Wordline 
Encoder
Linker
Field
W
rit
e 
G
en
er
at
or
Read 
Controller
Output packet to the granted 
distributed Level-1 FIFO
Write 
Aribter
Input Packets 
after switching
Pulse 
Generatorr_
re
q
r_req/r_ack 
from/to distrbuted 
Level-1 FIFO
Read
Arbiter
r_pulse
w_req
w_pulse
R/W Enable
Arbiter
r_enable
w_enable
Data
 Field
w_req/w_ack 
from/to distrbuted 
Level-1 FIFO
 
Fig. 12. Data-link scheduler and centralized level-2 FIFO for asynchronous two-level 
FIFO buffer. 
Fig. 12 shows the details of the data-link scheduler and centralized level-2 FIFO for the 
asynchronous two-level FIFO buffer. The write and read arbiters determine access 
permission of the centralized level-2 FIFO for different output channels when more than 
two output channels issue requests. The R/W-enable controller produces R/W-enabled 
signals for the centralized level-2 FIFO based on read_req and write_req, which are 
 18 
decrease in head-of-line problems. In this demonstration, the 8×8 DCT is also 
implemented in a synchronous system with a single voltage and multiple voltages. 
Compared with the synchronous 8×8 DCT system with a single voltage, the synchronous 
8×8 DCT system with multiple voltages can achieve a 12.5% energy reduction at the same 
execution time. Moreover, the asynchronous two-level FIFO buffer with multi-voltage can 
achieve further energy reduction because of the short execution time. 
四、結論與討論 
Energy consumption is one of critical issues for advanced NoC designs using 
nano-scale technologies. In this paper, several energy-efficient design techniques are 
presented for the circuit design in NoC platforms, including adaptive low-power and 
variation-tolerant link wires, adaptive congestion-aware routing and asynchronous 
two-level FIFO buffers. The self-calibrated low-power coding and voltage scaling 
interconnection architecture and congestion-aware routing are designed to realize adaptive 
energy-efficiency for link wires and routers in NoC platforms, respectively. Moreover, the 
proposed asynchronous two-level FIFO buffer architecture is also a powerful approach to 
achieve further energy savings for routers in advanced NoC designs. 
五、參考文獻 
[1] L. Benini and G. De-Micheli, “Networks on Chips: A New SoC Paradigm,” IEEE 
Computer, vol. 35, pp. 70-78, Jan. 2002. 
[2] V. Chandra, et al., “An interconnect channel design methodology for high perfor
 mance integrated circuits,” Design, Automation and Test in Europe Conference 
and Exhibition, Volume 2, pp. 1138-1143, 2004. 
[3] R.I. Bahar, et al., “Architectures for Silicon Nanoelectronics and Beyond,” IEEE 
Computer, vol. 40, No.1, pp.25-33, Jan. 2007. 
[4] P.P. Pande, et al., “Performance Evaluation and Design Trade-Offs for 
Network-on-Chip Interconnect Architectures,” IEEE Transactions on Computers, 
Vol. 54, No. 8, Aug. 2005. 
[5] M. Kinsy, et al., “Application-Aware Deadlock-Free Oblivious Routing", 
International Symposium on Computer Architecture, June 2009. 
 20 
 國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或
應用價值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能
性）、是否適合在學術期刊發表或申請專利、主要發現或其他有關價值等
作一綜合評估。 
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
█   達成目標 
□ 未達成目標（請說明，以 100字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 ■申請中 □無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100字為限） 
 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應
用價值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能
性）（以 500字為限） 
隨著多核心系統的發展，系統晶片運算能力也隨之快速成長。然而晶片內資料傳遞及
資料儲存卻無法跟上多核心的發展腳步，進而成為多核心系統的瓶頸。因此，晶片上
不僅需要放置更多、更快、且低功耗的記憶體來提供儲存資料，也必須針對不同的系
統應用來建構相對應的資料傳輸平台；為此，我們將提出一個以記憶儲存為中心之晶
內資料傳輸平台。此晶內資料傳輸平台將藉由階層式記憶體管理單元的協助與管理，
提供異質多核心系統具節能效益之資料傳輸及資料儲存，已達到系統效能的最佳化。
本計劃所提出的晶內資料傳輸平台專司晶內資料傳遞、資料儲存及其通訊協定之設
計。將可以有效降低多核心系統之功率及提供頻寬管理；此將更進一步促進可攜式電
子產品之發展。 
 
 
 1 
附錄 – 完整技術報告 
Chapter 1: 
Introduction 
With the advancement of the wireless communication and multimedia techniques, 
great amount of digital electronic devices are developed in human life. These modern 
electronic products provide a convenient entertainment environment. Fig. 1.1 presents 
a heterogeneous network environment in our life that provides wireless video 
entertainment systems anytime and anywhere. In recent years, merging different 
networks, electronic appliances and media devices into a heterogeneous integrated 
platform becomes a trend that provides a friendly and energy-efficient digital 
environment for people enjoying their life [1.1]. Therefore, heterogeneous multi-core 
system-on-Chip (SoC) designs provide an integrated solution for merging processor 
elements (PEs) or intellectual properties (IPs) in communications, multimedia and 
consumer electronics. A successful SoC design depends on the availability of 
methodologies that allow designers to meet two major challenges—the 
miniaturization of interconnecting features, and the requirement of memory 
capacity/memory bandwidth. 
 
Fig. 1.1 A heterogeneous network environment for wireless video entertainment systems. 
 3 
As design complexity of multi-core SoC continues to increase, a global approach is 
needed to effectively transport and manage on-chip communication traffic, and to 
optimize wire efficiency. Therefore, process-independent network-on-chip (NoC) has 
been considered an effective solution for integrating a multi-core system. NoC was 
investigated for dealing with the challenges of on-chip data communication caused by 
the increasing scale of next generation SoC designs [1.8], [1.9]. The most important 
characteristics of NoC can be considered as a packet switched approach [1.10] and a 
flexible and user-defined topology [1.11]. Furthermore, on-chip interconnection 
networks (OCINs) provide the micro-architecture and the building blocks for NoCs, 
including network interfaces, routers and link wires [1.12], [1.13]. The generic OCIN 
is based on a scalable network, which considers all requirements associated with 
on-chip data communication and traffic. OCINs have a few beneficial characteristics, 
namely, low communication latency, low energy consumption constraints, and 
design-time specialization. The motivation in establishing OCINs is to achieve 
performance using a system communication perspective. 
Multi-core SoCs have become a major trend of architecture in modern data 
computing systems. The multiple PEs are integrated on a single chip or package to 
exploit the parallelism of applications and achieve superior performance as well as 
energy efficiency. Because these systems are highly integrated, their designs and 
trade-offs are tightly coupled; a single design decision can impose significant impact 
on multiple design layers. Thus, for optimal results, designers have to consider 
multiple design layers (vertical exploration) and multiple architecture options 
(horizontal exploration) when mapping an application to an underlying multi-core 
system as shown in Fig. 1.2 [1.14]. In multi-core SoC designs, the processes of data 
streaming can be divided into three parts, including data computation, data storage 
 5 
space and high memory-access bandwidth for satisfying the video real-time 
requirement. Accordingly, large amounts of high speed and low power memories are 
indispensable for multi-task and multi-system emerging. These memories should be 
able to support diverse memory requirement of different PEs in a wireless video 
entertainment system using a memory sub-system. 
When process technologies shrink to nano-scale, the ever-increasing on-chip 
integrations in recent years have led to a dramatic increase in system performance and 
system scale. Unfortunately, as performance and area are improved, power dissipation 
and heat density are substantially increased [1.16]. Accordingly, power dissipation in 
multi-core SoC designs has become a critical design issue. In multi-core SoC 
implementations of mobile systems, especially for handheld audio and video 
applications, low power considerations dominate the overall performance since the 
Memory 
Bandwidth
Multi-c
ore 
system
s
Communication 
Efficiency
Computation 
Capability  
Fig. 1.3 Comparison between memory bandwidth, computation capability and communication 
efficiency in multi-core SoCs. 
R
el
at
iv
e 
C
om
pl
ex
ity
(M
P
E
G
 Q
C
IF
 3
0f
ps
)
 
Fig. 1.4 Relative complexity of a video system. [1.15] 
 7 
Chapter 2: 
Survey of On-Chip Data Communication 
With development of System-on-Chip (SoC) and multimedia communication 
technologies, a great amount of data computing requirement increases rapidly. In 
addition, the communication bandwidth requirement between processor elements 
(PEs) and the memory bandwidth requirement are also increasing to maintain the 
system performance. Therefore, the aggregate communication bandwidth between the 
processing cores is in the GBytes/s range for many video applications. In the future, 
with the integration of many applications onto a single device and with increased 
processing speed of cores, the bandwidth demands will scale up to much larger values. 
Multi-core SoC architectures are emerging as appealing solutions for embedded 
multimedia applications [2.1]-[2.5]. In general, multi-core SoCs are composed of core 
processors, memories and some application-specific cores. Additionally, data 
communication among PEs is provided by advanced interconnect fabrics, such as high 
performance and efficient networks-on-chip (NoCs) [2.6]. NoC was investigated for 
dealing with the challenges of on-chip data communication caused by the increasing 
scale of next generation SoC designs. Furthermore, on-chip interconnection networks 
(OCINs) provide the micro-architecture and the building blocks for NoCs, including 
network interfaces (NIs), routers and link wires [2.7], [2.8]. In OCINs, PEs (including 
memory modules) communicate by sending packet to one another over the network 
instead of by sending wires over ad-hoc wiring structures [2.9]. In this chapter, the 
related works of on-chip data communication are given, including NoCs, OCINs and 
memory sub-systems. The organization of this chapter is as shown in Fig. 2.1. 
 9 
development factor for integration with increasing PEs. Existing bus architectures and 
techniques are proving to be non-scalable, unable to meet leading edge complexity 
and performance requirements. Second, the interconnect delay across the chip exceeds 
the average clock period of the IP blocks, especially in nano-scale technologies [2.11]. 
The ratio of global interconnect delay to average clock period will continue to grow. 
An interconnect channel design methodology for high performance ICs has proposed 
in [2.11], it devised a methodology to size the FIFOs in an interconnect channel 
containing one or more FIFOs connected in series and shows that the sizing of the 
FIFOs in the channel is a function of system parameters such as data production rate 
and communication rate, number of channel stages etc. 
Third, in nano-scale technologies, increased coupling effect for interconnects not 
only aggravates the power-delay metrics but also deteriorates the signal integrity due 
to capacitive and inductive crosstalk noises [2.11]. Several options were proposed to 
reduce the inter-wire capacitances. The first option is to widen the pitch between bus 
lines. The second option is using P&R (place & route) tools to avoid routing of the 
bus lines side by side. However, the interconnect complexity and the routing time do 
not allow designers trying it to minimize the coupling capacitances. The third option 
is to change the geometrical shape of bus lines. But the disadvantage of this method is 
that the frank area will increase since the cross-sectional area of a bus line is fixed. 
The fourth technique is to add a shielding line (VDD/Ground) between two adjacent 
signal lines. The fifth option reduces the coupling power consumption via bus 
encoding schemes [2.12]-[2.16]. However, on-chip physical interconnections will 
present a limited factor for performance, reliability and energy consumption due to 
advanced technologies [2.17], [2.18]. Therefore, the encoding schemes for low power 
and reliability issues were proposed in [2.19], [2.20]. The designers must overcome 
 11 
conventional on-chip bus platform to reduce the shared-medium channels 
[2.24]-[2.26]. Multi-layer on-chip buses enable parallel access paths between multiple 
masters and slaves by a bus matrix. However, multi-layer bus architectures are 
confused with complex wire routings inducing larger power consumption and 
interconnect delay associated with the increasing number of PEs. 
PE
Memory
link
network 
interface
PE
Router
Memory
PE
Router
network 
interface
Router
network 
network 
link
(switch)
 
Fig. 2.4 On-chip interconnection network, including routers, link wires and network interfaces. [2.9] 
OCIN architecture was proposed based on a scalable switch fabric network, which 
considers all the requirements of on-chip communications and traffic via routing 
packets [2.9]. Moreover, OCINs have a few distinctive characteristics, namely low 
communication latency, energy consumption constraints and design-time 
specialization. Fig. 2.4 presents the OCIN architecture that provides the building 
blocks and backbone for NoC platform. The motivation of establishing NoC platform 
is to achieve performance using a system perspective of communication. The core of 
NoC technology is the active switching fabric that manages multi-purpose data 
packets within complex, IP laden designs. 
2.2 Design Abstraction Levels of NoC 
The design of NoC is vast and complex. Therefore, considering on-chip data 
 13 
and macro-architectural choices aiming to seamlessly merge the interconnection 
backbone with the remaining system modules [2.30]. 
NoC protocols are described bottom-up, starting from the physical layer up to the 
application layer. In the physical layer, link wires are the physical implementation of 
the communication channels. It is important to realize that a well-balanced design 
should not over design wires so that their behavior approaches an ideal one, because 
that the corresponding cost in performance, energy-efficiency and modularity may be 
too high. Physical layer design should find a compromise between competing quality 
metrics and provide a clean and complete abstraction of channel characteristics to 
layers above.  
NoC design entails the specification of network architectures and control protocols. 
The data-link layer abstracts the physical layer as an unreliable digital link, where the 
probability of bit upsets is non null. Furthermore, reliability can be traded off for 
energy. The main purpose of data-link protocols is to increase the reliability of the 
link up to a minimum required level, under the assumption that the physical layer by 
itself is not sufficiently reliable. At the data link layer, error correction can be 
complemented by several packet-based error detection and recovery protocols. 
Several parameters in the protocols can be adjusted depending on the goal to achieve 
maximum performance at a specified residual error probability within given energy 
consumption bounds. 
At the network and transport (transaction) layer, packet data transmission can be 
customized by the choice of switching and routing algorithms. The NoC designers 
establish the type of connection to its final destination. Switching and routing affect 
heavily performance and energy consumption. Robustness and fault tolerance will 
also be highly desirable. Algorithms deal with the decomposition of messages into 
 15 
2.7, which are message, packet, flit and phit (physical transfer unit) [2.7]. Therefore, 
in addition to the reduced design abstraction layers, the spectrum of NoC research is 
also divided into four areas based on the flow of data, including system, network 
adapter, network and link [2.31]. The correspondence between these four areas and 
OSI models is as shown in Fig. 2.8. The network adapter provides a bridge between 
high-level services and communication primitives using core interfaces (CIs) and NIs. 
Category based on design abstraction layers Category based on flow of data abstraction
 
Fig. 2.9 NoC Research category based on design abstraction layers and flow of data abstraction. [2.31] 
According to the design abstraction layers and flow of data, the NoC research 
topics can be categorized as shown in Fig. 2.9 [2.31], [2.32]. In the following sections, 
the research topics associated with OCINs are introduced, including both 
macro-architectural exploration (topology) and micro-architectural exploration 
(building blocks). Moreover, the research related to power analysis, voltage scaling 
and GALS of NoC is also described. 
2.3 Network Topologies of OCINs 
NoC platforms enable designing parallel systems resembling cellular structures 
including thousands of PEs. Such systems combined with multi-threaded computing 
can increase system efficiency for fine-grain parallel programs [2.33], [2.34]. 
Therefore, the OCIN architecture of NoC should be efficient for a huge amount of 
 17 
architecture is basically the same as a regular mesh. The only difference is that the 
switches at the edges are connected to the switches at the opposite edge through 
wrap-around channels. Every switch has five ports, one connected to the local 
resource and the others connected to the closest neighboring switches. The long 
end-around connections can yield excessive delays. However, this can be avoided by 
folding the torus as Fig. 2.10(d) [2.37]. This renders to a more suitable VLSI. The 
OCTAGON MP-SoC architecture was proposed in [2.38]. Fig. 2.10(e) shows a basic 
octagon unit consisting of eight nodes and 12 bidirectional links. Each node is 
associated with a processing element and a switch. Communication between any pair 
of nodes takes at most two hops within the basic octagonal unit. For a system 
consisting of more than eight nodes, the octagon is extended to multidimensional 
space. This type of interconnection mechanism may significantly increase the wiring 
complexity. In a Butterfly Fat-Tree (BFT) architecture which is shown as Fig. 2.10(f), 
PEs are placed at the leaves and switches placed at the vertices [2.39]. A pair of 
coordinates is used to label each node. The number of switches in the butterfly fat tree 
architecture converges to a constant independent of the number of levels. Other 
high-radix topologies were also studied as OCIN architectures [2.40], [2.41]. 
However, the complexity of the switching circuits in high-radix topologies induces 
huge amount of area and power consumption. 
 
Fig. 2. 11 Xipies Architecture. [2.42] 
 19 
than the star. Among the hierarchical topologies excluding the hierarchical 
point-to-point topology, the hierarchical star (locally star globally star or H-star) 
topology shows the lowest energy cost under any kinds of traffic. The network area 
cost including the area of switches, multiplexers/ demultiplexers, and links are also 
analyzed as shown in Fig. 2.13(b). The area of point-to-point topologies is 
skyrocketing as the increases because of their huge link wires interconnecting every 
PU pair. This is the major reason which makes the point-to-point topology impractical 
to implement. The area consumption of the hierarchical topologies is as small as bus 
topologies. Considering the energy and area cost together, the hierarchical star 
topology is the most energy-efficient and cost-effective topology in general. 
 
Fig. 2.13 (a) Energy consumption (b) network area according to a number of PEs. [2.48] 
In order to achieve better performance, functionality and packaging density, 
through-silicon-via (TSV) three-dimensional (3D) ICs were proposed with multiple 
layers of active devices [2.49]. Additionally, TSV 3D-ICs allow for performance 
enhancements in the absence of scaling. The performance improvement arising from 
the architectural advantages of NoCs will be significantly enhanced if TSV 3D-ICs 
are adopted as the basic fabrication methodologies. Therefore, new topologies of TSV 
 21 
and bandwidth to competing packet. In these cases, the flow-control methods must 
perform an arbitration to decide which packet gets the channel it has requested. The 
arbitration method must also decide how to dispose of any packets that did not get 
their requested destination. Circuit switching is a form of bufferless flow control that 
operates by first allocating channels to form a circuit from source to destination and 
then sending one or more packets along this circuit [2.54]. When no further packets 
need to be sent, the circuit is deallocated. Circuit switching differs from dropping 
flow control in that if the request flit is blocked, it is held in place rather than dropped. 
However, circuit switching has two weaknesses that make it less attractive than 
buffered flow control methods: high latency and low throughput.  
 
Fig. 2.14 Buffered flow control methods can be classified based on their granularity of channel 
bandwidth allocation and buffer allocation. [2.54] 
Adding buffers to OCINs results in significantly more efficient flow control, 
allowing the allocation of the second channel to be delayed without complications. 
Storing a flit (or a packet) in a buffer decouples allocation of the input channel to a flit 
from the allocation of the output channel to a flit. Adding a buffer prevents the waste 
of the channel bandwidth caused by dropping or misrouting packets or the idle time 
inherent in circuit switching. As a result, full channel utilization with buffered flow 
control can be realized via buffered flow control. Therefore, the buffered flow control 
mechanisms should allocate buffers considering channel bandwidth. Moreover, the 
buffered flow control mechanisms depend on the granularity of the buffer allocation 
and channel allocation as depicted in Fig. 2.14 [2.54].  
 23 
busy output channel, the complete message is buffered at the node. Thus, at high 
network loads, VCT switching behaves like SAF. 
VCT flow control reduced the latency from the product of the hop count and the 
serialization latency, giving very high channel utilization by using buffers to decouple 
channel allocation. It also achieves very low latency by forwarding packets as soon as 
possible. However, the cut-through method, or any other packet-based method, has 
two serious shortcomings. First, by allocating buffers in units of packets, it makes 
very inefficient use of buffer storage. As we shall see, we can make much more 
effective use of storage by allocating buffers in units of flits. This is particularly 
important when we need multiple, independent buffer sets to reduce blocking or 
provide deadlock avoidance. Second, by allocating channels in units of packets, 
contention latency is increased. 
2.4.2  Flit-Buffer Flow Control 
Wormhole flow control operates like cut-through, but with channel and buffers 
allocated to flits rather than packets [2.8]. In wormhole switching, the buffer 
requirements within the routers are substantially reduced over the requirement for 
VCT switching. The primary difference between wormhole switching and VCT 
switching is that, in the former, the unit of message flow control is a single flit and, as 
a consequence, small buffers can be used. Compared to cut-through flow control, 
wormhole flow control makes far more efficient use of buffer space, as only a small 
number of flit buffers are required per virtual channel. In contrast, cut-through flow 
control requires several packets of buffer space, which is typically at least an order of 
magnitude more storage than wormhole flow control. This savings in buffer space, 
however, comes at the expense of some throughput, since wormhole flow control may 
 25 
throughput than wormhole flow control. 
As in wormhole flow control, an arriving head flit must allocate a virtual channel, 
a downstream flit buffer, and channel bandwidth to advance. Subsequent body flits 
from the packet use the virtual channel allocated by the header and still must allocate 
a flit buffer and channel bandwidth. However, unlike wormhole flow control, these 
flits are not guaranteed access to channel bandwidth because other virtual channels 
may be competing to transmit flits of their packets across the same link. In fact, given 
the same total amount of buffer space, virtual-channel flow control also outperforms 
cut-through flow control because it is more efficient to allocate buffer space as 
multiple short virtual-channel flit buffers than as a single large cut-through packet 
buffer. 
2.4.3  Buffer Management and Backpressure 
All of the flow control methods that use buffering need a means to communicate 
the availability of buffers at the downstream nodes. Then the upstream nodes can 
determine when a buffer is available to hold the next flit (or packet for 
store-and-forward or cut-through) to be transmitted. This type of buffer management 
provides backpressure by informing the upstream nodes when they must stop 
transmitting flits because all of the downstream flit buffers are full. Three types of 
low-level flow control mechanisms are in common use today to provide such 
backpressure[2.57]: credit-based [2.58], on/off, and ack/nack [2.59]. 
With credit-based flow control [2.60], [2.61], the upstream router keeps a count of 
the number of free flit buffers in each virtual channel downstream. Then, each time 
the upstream router forwards a flit, thus consuming a downstream buffer, it 
decrements the appropriate count. If the count reaches zero, all of the downstream 
 27 
employed in most systems that have large numbers of flit buffers. 
 
Fig. 2.16 A robust self-calibrating transmission scheme for OCIN links. [2.62] 
2.5 Link Wires for OCINs 
The interconnection is used to distributed clock and signals, and to provide power 
lines among PEs on a chip. Generally speaking, high performance signaling has two 
modes, which are voltage-mode signaling and current-mode signaling. Voltage-mode 
signaling uses current to charge the capacitive load of the link wire, using the level of 
voltage to decide the logic high or low. Current-mode signaling does not need to the 
charging node to full swing, and can sense the current direction and decide the logic. 
To simplify the design, voltage-mode signaling is usually adopted for link wires in 
OCINs [2.7]. 
The features of OCIN links consists of regularity, point-to-point (no fan-out tree), 
and well-defined current return path [2.32]. Additionally, the OCIN links can be 
optimized for noise/speed/power at bit-level and flit-level both. At flit-level, dynamic 
voltage scaling for link wires is presented to reduce energy consumption [2.62]. A 
variable frequency and swing technique as shown in Fig. 2.16 is proposed to trade off 
speed for energy, which is achieved by dynamic voltage-swing scaling and two 
 29 
modified in such a way that coding/decoding is needed only at the source and 
destination nodes, then there will be no extra power dissipation arising out of the 
codec blocks in the intermediate nodes. Eventually this will help to reduce the overall 
communication energy dissipation. 
Incorporating of different coding schemes have been investigated to increase 
system reliability. CAC algorithms reduce the worse-case switching capacitance of a 
wire by ensuring that a specific codeword transitions doesn’t happen. However, OCIN 
links are sensitive to internal (power supply noise, crosstalk noise, inter-symbol 
interference) and external (electromagnetic interference, thermal noise, noise by alpha 
particles) noise sources due to lower supply voltage, smaller node capacitances, a 
decrease of wire spacing, the increasing role of coupling capacitances, the higher 
clock frequency. Therefore, CAC incorporates with forward error correction coding is 
a solution for a robust NoC system. Jointing CAC and single error correction (SEC) 
codes are proposed as duplicate-add-parity(DAP) [2.71], boundary shift code (BSC) 
[2.72] and modified dual rail (MDR) code [2.73]. 
 
Fig. 2.17 Duplicate-add-parity(DAP) coding (a)Encoder (b)Decoder. [2.71] 
Duplicate-add-parity encoder/decoder is as shown in Fig. 2.17. The encoder 
duplicates data(x0,x1,x2,x3) and generates (y1,y3,y5,y7). y8 is a parity bit generated 
from x0⊕x1⊕x2⊕x3 which means if data has odd “1” y8=1, else (even “1”) y8=0. 
 31 
(1)  CAC needs to be the outermost code. 
(2)  LPC can follow CAC. 
(3)  ECC needs to be systematic. 
(4)  The additional information bits generated by LPC (p) and ECC (m) need to be 
encode through linear crosstalk code (LXC). 
This framework derives a wide variety of joint codes which enables the trade-off 
between delay, power, reliability and area. In addition to coding and voltage scaling 
for link wires, a bi-direction channel design has been proposed to maximize the 
utilization of link wires based on the traffic conditions as shown in Fig. 2.20 [2.74], 
[2.75]. This scheme allows each communication channel to be dynamically 
self-configured to transmit flits in either direction. 
 
Fig. 2.20 A bi-direction channels to optimize the utilization of link wires. [2.74] 
For realizing high-performance and low power links at bit-level, an energy 
recovery technique is adopted for OCIN links [2.76]. Dynamic voltage scaling for link 
wires is also presented to reduce energy consumption at bit-level via lookahead-based 
 33 
 
Fig. 2.22 Micro-architecture of a router for mesh-based OCINs. [2.9] 
2.6.1  Routing Algorithm for Link Control 
The design of link control unit highly relies on the topologies of OCINs. Most 
routing algorithms were developed for regular mesh OCINs. The routing schemes for 
mesh OCINs can be classified into several categories. The routing decision at every 
router can be static or dynamic. In static routing schemes (or called oblivious routing), 
the path is completely determined the source and destination address [2.79], [2.80]. 
The routing scheme does not consider the current load and the situation on the 
network. On the contrary, the dynamic routing scheme decides the path by not only 
source and destination address but also the dynamic network condition. The 
advantage of static routing scheme is its simplicity of design and hardware overhead. 
Dynamic routing schemes can use alternative paths which consider the network traffic 
situation but it may cost much hardware resource [2.81]. 
Furthermore, routing techniques can be classified according to where the routing 
information decisions are determined. In distributed routing, the routing decisions are 
decided in each router. Therefore, each packet carries its source and destination 
addresses, and the router according to the information to lookup the routing table or to 
 35 
The major constraint for any routing algorithm is assuring the freedom from 
deadlock. The definition of deadlock is that a packet is blocked at some intermediate 
resource and cannot reach its destination. Deadlock occurs when one or more packets 
in the network are blocked during an indefinite time, waiting for an event that can’t 
happen. An example as depicted in Fig. 2.24 is a situation where four packets are 
routed in a circle between the routers in a square mesh. The packet in A1 router is 
allocated to west and another packet in A2 router is allocated to south, and so as A3 
and A4. The four packets are already held by another packet and will never be 
released [2.92], [2.93]. 
 
Fig. 2.24 An example of deadlock. [2.92] 
The prominent strategy for dealing with deadlock problems is avoidance and most 
deadlock-free routing algorithms are deduced by the strategy. (1) Choose a particular 
routing algorithm. (2) Check whether this algorithm is deadlock free. (3) If need, add 
hardware resource or restrict routing to guarantee that this algorithm is deadlock free. 
Deadlock free can be analyzed via building a dependency graph, and this dependency 
graph cannot be cyclic. Another constrain for the routing algorithms is livelock. The 
definition of livelock is a packet enters a cyclic path dose not reach its destination. 
Livelock is only induced by dynamic routing algorithms.  
A1 
 
A3 
A2 
 
A4 
 37 
Additionally, (1,3) and (0,2) also send the information of available to (1,2). 
Nevertheless, these two nodes are not on minimal paths with given destination (3,2) 
as shown in Fig 2.4 (c)-(d). In view of this, only two nodes (1,2) and (2,1) send the 
score to(1,1), and node (1,1) decides that the next node is (2,1). 
 
Fig. 2.26 Example for neighbors-on-path algorithm. [2.94] 
Antnet algorithm uses a nature method to route the least latency way [2.95]. At 
first, every packet chooses the next node randomly with a forward ant for recording 
the paths. When the packet reaches the destination, send back a backward ant along 
the same path as the forward ant passed, and update the routing table of every router 
on the path (increase the priorities of directions of the router on the path). After a 
period, the less latency the path is, the more backward ants come back, so the 
directions of routers on the path will get higher priority due to backward ants. 
Therefore, more and more packets will follow the way which has less latency 
dynamically. 
In recent years, research of routing algorithms focuses on routing for irregular 
topologies [2.96], [2.97], routing for hierarchical topologies [2.98], [2.99], fault 
 39 
consumption and delay with large input and output [2.48]. This approach divides the 
switching matrix into multiple segments to reduce the capacitance.  
 
Fig. 2.27 Crossbar partial activation technique. [2.48] 
2.6.3  Arbitration Unit in Routers 
For arbitrating the output conflicts, an arbitration unit is adopted for each output 
channel. The latency of the arbitration unit increases rapidly with the increasing size 
of the switching matrix. TDMA [2.107]-[2.109] and round-robin scheduling algorithm 
[2.110], [2.111] are widely utilized since the characteristics of simple implementation 
and fairness. Moreover, a pseudo-LRU algorithm was proposed for achieving smaller 
area and latency than those of the round-robin algorithm [2.112]. Accordingly, an 
input contention-aware arbitration algorithm was also proposed to achieve high 
performance by considering the traffic of neighbor nodes [2.112]-[2.114].  
2.6.4  Queuing Buffer in Routers 
Queuing buffer is used in routers or NIs to store un-routed data. Buffer allow for 
local storage of data that cannot be immediately routed. Unfortunately, queuing 
 41 
via back-end. Primarily, communication reliability has to be ensured by means of 
proper error-detection strategies and effective error recovery techniques. 
 
Fig. 2.28 Network interfaces connecting cores to the NoC and possible message dependencies 
in (a) shared-memory and (b) message-passing communication paradigms. [2.121] 
 
Fig. 2.29 Block diagram of network interface supporting the CTC protocol. [2.121] 
NI is a major factor in the performance of OCINs, and implements the 
communication protocols of OCINs. NI is implemented based on the message 
dependencies in shared-memory and message-passing communication paradigms as 
shown in Fig. 2.28. Therefore, a NI micro-architecture was proposed for the 
connection-then-credit (CTC) flow control protocol among both communication 
paradigms as presented in Fig. 2.29 [2.121]. CTC is based on the classic end-to-end 
credit-based flow control protocol but differs from it because it uses a network 
 43 
NoC as shown in Fig. 2.30, including system level, transaction level, word level, bit 
level [2.125]. The major difference between system level and transaction level is that 
the system level contains how to connect the resources and routers and the transaction 
level is dealing with the hot traffic and the communications between two routers. The 
bit level model can be divided into four parts as shown in Fig. 2.30, and the difficult 
part for analyzing is wiring delay. Bit-level power models for OCIN have been 
utilized in [2.126]-[2.128]. In word level, using the thermal time constant (TTC) can 
determine the number of flits that are present in the network during this time window 
and the consequent energy consumption of each of these flits. Flit traversal in the 
network can be broken down into a sequence of operations buffer reading, switch 
traversal, external link traversal and buffer writing [2.125], [2.129]. 
 
Fig. 2.30 Four levels for power analysis of OCINs 
By not providing actual placements of the communicating cores, the power models 
for network on chip provide the system designer with that degree of freedom and by 
controlling the bandwidth allocated to the communication and controlling the peak 
power consumption [2.129],[2.130]. The consequential tradeoff was the decrease in 
throughput and increased latency. The power models are currently developing a low 
cost, online power management strategy that would improve the throughput and 
reduce the latency. Therefore, power analysis and power exploitation were proposed 
System Level 
Transaction Level 
Word Level 
Bit Level 
Routing Arbitration Wiring Switching 
NoC Simlator, compiler and synthesis 
OCIN 
 45 
the communication layers so that design and synthesis of each layer is simpler and can 
be done separately. In addition, decoupling enables easier management of power 
consumption and performance at the level of communicating PEs. Network-centric 
power management utilizes interaction with the other system cores regarding the 
power and the quality of service (QoS) needs. Therefore, a methodology for managing 
power consumption of NOCs was proposed in [2.136] and a power management 
problem is formulated for the first time using closed-loop control concepts via an 
estimator and a controller as presented in Fig. 2.33. The estimator is capable of very 
fast and accurate tracking of changes in the system parameters. The main task of the 
estimator is to observe the system behavior and based on that to estimate the 
parameters needed for optimization and control. The quality of power management 
decisions strongly depends on estimator’s ability to track changes of critical 
parameters at runtime. NOC power management requires estimation of workload 
characteristics, core parameters, and buffering behavior. 
 
Fig. 2.33 A methodology for managing power consumption of NOCs via the estimator. [2.136] 
Given the continuing transistor shrinking and component parallelization, run-time 
monitoring of NoC will become essential as always-optimal design paradigm and has 
been acknowledged to achieve better power efficiency among other benefits [2.137], 
[2.138]. It is important to consider NoC power while designing techniques for chip 
level power management is important. Especially monitoring and actively controlling 
 47 
will be rare but unavoidable events. The other main drawback of GALS systems is 
because of the handshake protocol, the high latency issue will degrade the average 
data throughput compared with synchronous based design. 
 
Fig. 2.35 GALS systems based on plausible clocking. [2.143] 
 
Fig. 2.36 GALS systems based on clock gating. [2.143] 
Most of the proposed on-chip clock generators for GALS systems are based on 
plausible clocking which is based on on-chip ring oscillators as presented in Fig. 2.35 
[2.143]. Pausible clock generator is the traditional type of the clock controller. The 
generator generates the local clock for the LS module and the clock will be stretched 
or paused by the control signal. Other methods for generating local clock pulse are 
introduced that is based on clock gating as shown in Fig. 2.36 [2.144]-[2.146]. 
Moreover, synchronizer or MUTEX cannot be absent for clock gating technique for 
mixed clock system [2.147]-[2.150]. A synchronization scheme for clock-based 
GALS wrapper has been proposed, which is called locally delayed latching (LDL) 
synchronization [2.145]. This method allows for GALS inter-modular communication 
 49 
mixed-timing FIFO and self-timed ring. Low latency mixed-timing FIFO interface 
designs for mixed-timing systems have been proposed [2.151]-[2.153]. First, they 
have low latency, which means the delay time from the first sent data to the first 
received data. Second, the depth of the FIFO and the width of data bus are scalable 
with few circuit modifications. A self-timed ring architecture used for GALS 
multi-point communication was proposed as shown in Fig. 2.38 [2.154]. shows the 
bypass architecture of the self-timed ring, the ring transceiver in the gray part of the 
figure is composed of two parts: a router that decides where the incoming data packet 
has to go and an arbiter that decides which request to pass (incoming request from the 
preceding ring transceiver, or request from the host circuitry that wants to feed a data 
packet into a ring).  
 
Fig. 2.38 Bypass architecture of the self-timed ring. [2.154] 
In recent years, GALS NoCs are implemented with voltage islands to achieve 
energy optimization via optimal voltage-frequency island partition [2.155], [2.156]. 
NoC architecture combined with a GALS paradigm is a natural enabler for DVFS 
mechanisms. Therefore, asynchronous power aware and adaptive NoCs have been 
proposed via GALS technique with voltage islands [2.157]-[2.159]. Fig. 2.39 presents 
 51 
memory-centric OCIN and an on-demand memory sub-system. The memory-centric 
OCIN provides the micro-architecture for data communication based on the building 
blocks, including link wires, routers and NIs. In this dissertation, all building blocks 
are analyzed and developed to realize energy-efficient multi-core SoCs.  
 53 
interconnections, three critical issues, delay, power and reliability must be addressed. 
For the delay issue, propagation decreased by coupling capacitances. For long global 
lines, discharging large capacitances takes considerable time. For the power issue, 
power dissipation increases due to both parasitic and coupling capacitances. Finally, 
the reliability issue for on-chip interconnections will be degraded due to noise. In 
advanced technologies, circuits and interconnects degrade further due to noise with 
decreasing operating voltages. Furthermore, increasing coupling noise, the soft-error 
rate, and bouncing noise also decrease the reliability of circuits. Thus, self-calibrated 
circuitry has become essential for near-future interconnection architecture designs. 
To achieve low latency and reliable and low energy on-chip communication, 
energy efficiency is the primary challenge for current OCIN designs with nano-scale 
effects. First, coupling capacitance increases significantly in nano-scale technology. 
Second, decreasing operating voltage makes the interconnection susceptible to noise 
increasingly. Due to crosstalk noise, the coupling effect not only aggravates the 
power-delay metrics but also deteriorates the signal integrity. Many techniques have 
been developed to reduce the coupling capacitance effect using bus encoding schemes 
[3.5]-[3.13]. Bus encoding is an elegant and effective technique for eliminating the 
crosstalk effect, and provides a reliability bound for on-chip interconnects. Moreover, 
in order to provide a reliability bound for on-chip interconnects, forward error 
correction (FEC) and automatic repeat request (ARQ) techniques are widely used in 
NoC [3.1], [3.14]. Additionally, a joint error correction coding and bus coding 
technique is an effective solution to resolve delay, power and reliability. Encoding 
schemes for low power and reliability issues were proposed in [3.15]-[3.20]. The 
designers increased reliability for on-chip interconnections. Moreover, robust 
self-calibrating transmission schemes were proposed in [3.14], [3.21]-[3.23], which 
 55 
effect. The purpose of CAC is to reduce the worst-case switching patterns, which are 
forbidden overlap condition (FOC), forbidden transition condition (FTC) and 
forbidden pattern condition (FPC) [3.15]. FOC represents a codeword transition from 
010 to 101 or from 101 to 010. In addition, FTC represents a codeword transition 
from 01 to 10 or from 10 to 01, and FPC represents a codeword having 010 or 101 
patterns. In order to reduce or avoid the worst-case switching patterns, many coding 
schemes are proposed to be directed against the three conditions [3.20]. Forbidden 
Overlap Code provides a 5-bit codeword for a 4-bit dataword to eliminate FOC. And 
Forbidden Pattern Code is also a 5-bit codeword for a 4-bit dataword to avoid FPC in 
codeword. Additionally, Forbidden Transition Codes provides a 4-bit codeword for a 
3-bit dataword to prevent FTC. However, these three coding schemes do not satisfy 
the forbidden adjacent boundary pattern condition, which is defined as two adjacent 
bit boundaries in the codes cannot both be of 01-type and 10-type. Hence, One 
Lambda Codes is proposed not only to avoid FTC and FPC but also to satisfy the 
forbidden adjacent boundary pattern condition [3.20]. However, it needs an 8-bit 
codeword to transfer a 4-bit dataword. 
Joint coding schemes based on the unified framework as shown in Fig. 1 provide 
better communication performance. However, these schemes just combine different 
kinds of codes directly, since the intrinsic qualities of CACs and ECCs are mutually 
exclusive, except for duplicating codes (DAP, MDR and BSC) [3.15], [3.18]. In DAP 
coding, nevertheless, the critical path of the priority bit is much longer than others. 
Moreover, CAC must be a code that does not modify the parity bits in any way as 
decoding of ECC has to occur before any other decoding in the receiver. In order to 
reduce the coupling effect of the parity bits, the linear crosstalk code could be applied 
without destroying the parity bits. 
 57 
provides low energy and high reliability channels for OCINs. The SCG coding 
scheme is constructed in two stages, the green bus coding stage and the triplication 
error correction coding stage. In routers, an un-decoded code increases the area and 
energy dissipation of switching circuits by large physical transfer unit sizes. Therefore, 
the error correction code should be decoded in routers to reduce power dissipation and 
the area of switching circuits and buffers. The triplication error correction coding 
stage achieves rapid correction to reduce the physical transfer unit size in routers via a 
self-corrected mechanism at the bit level. To efficiently reduce the coupling effect, the 
green bus coding stage is developed using the joint triplication bus power model, 
which depends upon the characteristics of triplication error correction coding. The 
SCG coding can avoid the FOC and FPC, and reduce the FTC to achieve the power 
saving of channels. The bit-width in the self-calibrated low power coding and voltage 
scaling varies. The green bus coding encodes packets in accordance with a 4-to-5 
codec. To increase the reliability of channels, the triplication error correction stage 
increases bit-width from k-bit to 3k-bit. Although the SCG coding increases link wires 
in channels, on-chip wires are cheap and plentiful with the increasing metal layers in 
advanced technologies [29, 30]. 
Designers can tradeoff between power consumption and reliability by reducing the 
operating voltage as the error correction coding increases the reliability of channels. 
Therefore, the operating voltage of the link wires in channels is adjusted according to 
the SCG coding scheme using a self-calibrated voltage-scaling technique. This 
technique detects error conditions of channels in the triplication error correction stage, 
and thus feeds the control signals back to the low swing drivers and adjusts the 
operating voltage of the link wires. The self-calibrated voltage scaling technique 
determines the optimal operating point to trade off between energy consumption and 
 59 
of each bit is 3. Therefore, each bit can be corrected individually when no more than 
one error bit exist in the three triplicated bits, which are defined as a triplication set. 
The error bit can be corrected by a majority gate. Fig. 3.3 also shows the function of 
the majority gate. Compared with other error correction mechanisms, the critical 
delay of the decoder is a constant delay of a majority gate and significantly smaller 
than that of other approaches [3.14]-[3.20]. Restated, the triplication error correction 
coding has rapid correction ability via self-correction mechanism at the bit level. 
Therefore, triplication error correction coding is more suitable to OCINs because data 
can be decoded and encoded in each router using the small delay of triplication 
correction coding. 
Additionally, one advantage of incorporating error correction mechanisms in an 
OCIN data stream is that the supply voltage of channels can be reduced without 
compromising the system reliability. Reducing supply voltage, Vdd, increases bit error 
probability. To simplify error sources, we assume bit error probability, ε, is as Eq. (3.1) 
when a Gaussian distributed noise voltage, VN, with variance σN2 is added to the signal 
waveform. 
2
dd
n
VQε
σ
 
=  
 
                                     (3.1) 
where Q(x) is given as 
( )
2
21
2
y
x
Q x e dy
π
∞
−
= ∫                            (3.2) 
Each triplication set can be error-free if and only if no error transmission exists or just 
1-bit error transmission exists. For each triplication set, P1-bit correct is given as 
( ) ( )3 21
3
1 1
1bit correct
P ε ε ε−
 
= − + − 
 
                     (3.3) 
For k-bits data, transmission is error-free if and only if all k triplication sets are 
 61 
where ε is bit error probability with full swing voltage  of 1.0 V, and εˆ  is bit error 
probability with a lower swing voltage. To obtain the lowest supply voltage for 
specific error correction coding under the same level of reliability of the un-coded 
code, supply voltage can be revised as 
( )
( ) ( ) ( )
1
1
ˆˆ ˆ,dd dd uncode ecc
Q
V V P P
Q
ε
ε ε
ε
−
−= =                     (3.8) 
The inverse function of the Gaussian distributed function is also called a probit 
function Ф(x). The probit function has been proved that the function does not have 
primary primitive. To solve the problems, this work first approximates the bit error 
probability by varying voltage swing. By integrating from 100–Vdd/2, the integral 
range on the x-axis is divided into 0.0001(v) segments, and each segment can produce 
a trapezoid. The areas of all trapezoids are then summed, which is the approximation 
of bit error probability. Therefore, the lowest voltage swing for a specific error 
correction coding that satisfies Eq. (3.8) can be obtained. 
When an un-coded code is operated at full swing supply voltage (1.0v), different 
levels of bit error probability, ε, can be obtained by altering the variance of the 
Gaussian distributed function. Fig. 3.4(a) and Fig. 3.4 (b) show the voltages of 
specific error correction coding versus different un-coded word error rates with k = 8 
and k = 32, respectively. Factor, k, is bit-width. If bit error probability of an un-code 
word, ε, is 10-20, the specific voltage of Hamming code [3.15], 
Duplication-Add-Parity code [3.15], [3.16], joint crosstalk avoidance and 
triple-error-correction code (JTEC) [3.19] and the proposed SCG code are 0.705V, 
0.710V, 0.579V, and 0.696V, respectively. The JTEC code uses a double error 
correction coding stage to enhance error correction and obtains lower voltages. 
However, delay and area overheads of the JTEC are much worse than those of other 
 63 
when a bit pattern does not have a transition from 01 to 10 or from 10 to 01. This 
work modified the RLC cyclic bus model in [3.26] by considering loading 
capacitances and coupling capacitances. Fig. 3.5(a) shows the modified model with a 
four-bit bus, where C11 means the loading capacitance of line 1 and the C12 is the 
coupling capacitance between line 1 and line 2. Moreover, the bus lines are parallel 
and coplanar. Most of the electrical field is trapped between adjacent lines and the 
ground. Fig. 3.5(b) shows an approximate bus power model that ignores the parasitic 
capacitances between nonadjacent lines. 
C12 C23 C34
C13 C24
C14
C1 C2 C3 C4
C12 C23 C34
C1 C2 C3 C4
(a) (b)
 
Fig. 3.5 (a) Bus Model for 4 bits (b) The approximate bus power model. 
We assume all grounded capacitors have the same value without considering the 
fringing effect of boundary lines. Because of fringing capacitors are much smaller 
than loading and coupling capacitors, even for the wide buses. Therefore, this work 
utilized a joint triplication bus model to implement the bus coding stage to further 
reduce energy consumption. For a 4-bit triplication bus, the capacitance matrix Ct can 
be expressed as: 
3 0 0
3 2 0
,
0 3 2
0 0 3
X
L
L
CtC C
C
λ λ
λ λ λ
λ
λ λ λ
λ λ
+ − 
 − + − = =
 − + −
 − + 
            (3.9) 
The parameter, λ, is defined as the ratio of coupling capacitance, Cx, to loading 
capacitance, CL. Therefore, the λ parameter depends on the technology, the specific 
 65 
and power density, respectively; f and V (VDD) are frequency and voltage (voltage 
supply), respectively. Bi is the current voltage level (1 or 0) for line i, and Bi-1 is the 
previous voltage level for the line i. 
( ) ( ){ }2 1 1
( ) ( )f T t f i
t
DD i i j j
i j
E V C V V
P f V C B B B B− −
= −
= ∗ ∗ − ∗ −∑∑           (3.10) 
Power density, P, can be transferred into Eq. (3.11). 
( ) ( ) ( )
( ) ( ) ( )
( ) ( )
( ) ( )
2 2 21 1 1
1 1 2 2 3 3
221 1 1
4 4 1 1 2 2
2
21 1
2 2 3 3
21 1
3 3 4 4
3 3 3
3
L DD
B B B B B B
B B B B B B
P f C V
B B B B
B B B B
λ
λ
λ
− − −
− − −
− −
− −
 − + − + −
 
  + − + − − −   = ∗ ∗ ∗ 
  + − − −  
  + − − −   
          (3.11) 
The items in Eq. (3.11) are defined and identified as follows. 
1 2 1
1 1 2
1 1 1 1
( )
[( ) ( )] 4
i i i i i
i i j j i j ij
ij i i j j i i j j
B B B B r
B B B B r r d
where d B B B B B B B B
− −
− −
− − − −
− = ⊕ =
− − − = ⊕ + ×
= 
                   (3.12) 
2
1 2 3 4 1 2 2 3 3 4
12 23 34
3( ) ( )
4 ( )
L DDP f C V
r r r r r r r r r r
d d d
α
α λ
λ
= × × ×
= + + + + ⊕ + ⊕ + ⊕
+ + +
              (3.13) 
The ri means that a switch of line i exists and is not concerned with the direction of 
change and adjacent lines. This item, ri, only considers loading capacitances. The 
meaning of ri⊕ rj is that only one line is changing between two lines of i and j. 
Additionally, dij indicates that two lines change in opposite directions. Moreover, 
compared with the other two definitions, ri and ri⊕ rj, the voltage difference across the 
coupling capacitance is double and when squared it factor 4 for dij. Using Eq. (3.12), 
the power formula can be obtained as Eq. (3.13) with the parameter of λ. The term α 
 67 
transition patterns are selected with minimal values of α as the codeword to eliminate 
crosstalk. The green bus coding chooses a 4:5 code to minimize α depending on the 
energy saving bound and the latency of codec. In a data bus, the bit-width of a data is 
usually a multiple of 4. Therefore, the energy-saving bound of 4:5 to 4:8 codes are 
between 40% to 55% from the energy-saving bound analysis of [3.28]. However, the 
latency of the codec will increase significantly as the size of a codeword increases. 
Data words Codewords
X3~X0 C4~C0
0  0  0  0
0  0  0  1
0  0  1  0
0  0  1  1
0  1  0  0
0  1  0  1
0  1  1  0
0  1  1  1
1  0  0  0
1  0  0  1
1  0  1  0
1  0  1  1
1  1  0  0
1  1  0  1
1  1  1  0
1  1  1  1
0  0  0  0  0
0  0  0  0  1
0  0  0  1  0
0  0  0  1  1
0  0  1  0  0
1  0  0  0  0
0  0  1  1  0
0  0  1  1  1
0  1  0  0  0
1  1  1  0  0
1  1  1  1  1
1  1  1  1  0
0  1  1  0  0
1  1  0  0  0
0  1  1  1  0
0  1  1  1  1
(a)
0000
1000
1100
1110
1111
0001
0010
0011
0100
0110
0111
0101
1101
1001
1011
1010
Orignial Set Converted Set
if (C4=1) then 
else 
C0 = X0 , C2= X2
C4 = X2X1X0 
     + X3X2X0   
     + X3X2X1
C0 = X0 , C2= X2
(b)
 
Fig. 3.8 (a) The mapping table between 4-bit dataword and 5-bit codeword of the green bus coding 
stage (b) The two sets and Boolean expression of the green bus coding stage. 
Fig. 3.8(a) shows the relationships between the 4-bit dataword and 5-bit codeword. 
According to the relationships, the data-word can be grouped into two sets, the 
original set and converted set as shown in Fig. 3.8(b). When transmitted data are in 
the converted set, the green bus coding stage converts the data into the original set via 
one-on-one mapping. Meanwhile, the converted bit, c4, will be asserted, and c0 and 
c2 will be inverted and mapped to the original set. Notably, X1 and X2 will always not 
be modified. 
Fig. 3.9 shows the circuit implementation of green bus coding, including the 
encoder and decoder. The circuitry of green bus coding is more simple and effective 
 69 
(3) Adding extra shielding lines to reduce the coupling effect between two 
adjacent codeword with increasing coding bits is unnecessary. 
(4) According to the delay model and energy model given by [3.28], the energy 
dissipation and critical delay are reduced from (1+1.5λ)CV2 to 
(1.18+1.17λ)CV2 and (1+4λ)τ0 to (1+2λ)τ0 via the green bus coding, 
respectively. τ0 is defined as the delay of a crosstalk-free wire. 
Low Swing 
Driver clk
Error Flag
Adaptive delay line
data from error 
correction 
encoder
DFF
Triplication 
Error Correction
Decoder
Test Pattern
GeneratorTest Error Detection
data out
Transceiver ReceiverChannel
Run-time error 
detection stage
Crosstalk-aware test error 
detection stage
Level 
Converter
Voltage Scaling 
Control Unit
Run-time 
Error Detection
Test Pattern
Generator
Self-Calibrated Low 
Swing Driver 
Self-Calibrated Voltage Scaling 
( including error correction decoder)  
DFF
Error from ECC
(5-bit)
Voltage 
control Signal
Data
 
Fig. 3.10 The block diagrams of self-calibrated voltage scaling technique with crosstalk-aware test 
error detection stage and run-time error detection stage. 
3.4  Self-Calibrated Voltage Scaling Technique 
The proposed self-calibrated voltage scaling technique is applied to reduce the 
operating voltage of channels for energy reduction and ensure the reliability based on 
the SCG coding scheme. The self-calibrated voltage scaling technique will identify 
the optimal operating voltage to trade off between energy consumption and reliability 
for the self-calibrated circuitry. Fig. 3.10 presents the block diagrams of the 
self-calibrated voltage scaling technique. This technique is constructed by comprising 
low swing drivers, level converters, voltage scaling control unit, crosstalk-aware test 
error detection stage and run-time error detection stage. Depending on the detections 
about the two error detection stages, the voltage control unit adjusts voltage swing 
 71 
(a)
(b) (c)
in out
VDD VDDL
S2 S1 S0
VDDL
VDD
Low Vt S2 S1 S0
10 1
1 0 1
1 01
VDDL
VDDVDD
VDD
VDDL
VDDL
in
out
VDD-2|Vtp|
VDD-|Vtp|
VDD
 
Fig. 3.11 (a) Low swing voltages (b) Low swing driver (c) Level converter. 
Start
Crosstalk-aware 
test error detection
Raise test 
voltage 
swing
No
Yes
NoYes
No
error free
Run-time error 
detection
V_scale =1 
Voltage 
scaling for 
next window
T_start =1 Yes
Differernt detect Bit error rate: 
Error rate<5%  : (fall)
             5%<Error rate<15%: (stay)
             15%<Error rate        : (raise)
             
LV
MV
HV
 
Fig. 3.12 The control policy of self-calibrated voltage scaling technique. 
Fig. 3.12 shows the control policy and voltage state diagram of the self-calibrated 
voltage scaling technique. Therefore, the crosstalk-aware test error detection stage is 
triggered by T_start, and crosstalk-aware test vectors are generated. Test results are 
 73 
control voltages for the low swing driver. The crosstalk-aware test error detection 
stage is triggered by T_start, and then generates crosstalk-aware test vectors. 
Conventional test pattern generators, such as the Liner Feedback Shift Register (LFSR) 
[3.29], [3.30], generate pseudo-random pattern sequences. By changing the feedback 
polynomial of the LFSR, the LFSR generates different subsets of the 
maximum-length LFSR (maximum 2n-1 patterns when the LFSR tests n-bits data with 
primitive polynomials). However, test patterns generated by the LFSR based TPG are 
complicated and require a long test time to achieve high error coverage. Hence, a 
better self-test methodology is needed to achieve low hardware overhead, fast test 
time, and high error coverage. 
0 0
1 1
0 0
1 0
0 1
1 0
1 1
0 1
State
s1
s2
s3
s4
s5
s6
s7
s8
Sr
Sf
Gp
Dr
Df
Gn
VictimAggressor
State Machine
Victim 
Counter
Select 
Decoder
Sel
0
Sel
1
Sel
 n-1
C_reset
C_enable
Victim Aggressor
D0 D1 Dn-1
S0
S1
S2 S8
S4
S5
S6
S3 S7
T_start=1
T_start = 0
C_value = n-1
C_value
(a) (b)
 
Fig. 3.13 MAF based test pattern generator (a) 8 states complete 6 faults test of MAF model (b) 
Hardware implementation. 
Depending on test vectors, therefore, the test error detector can detect error data 
following error correction coding. The crosstalk-aware test vectors are generated by a 
test pattern generator with the maximal aggressor fault (MAF) model as shown in Fig. 
3.13 [3.31]. The MAF-based test patterns are a simple pattern stream that represents 
 75 
master-slave flip-flop (MSFF) [3.32] and double sampling data checking technique 
[3.33] have been proposed to detect timing errors. The MSFF contains a master 
flip-flop and a slave flip-flop, both of which operate at the same frequency. However, 
the slave flip-flop is positively triggered by a delay clock (∆t) which is proportion to 
master flip-flop. We assume the data captured by the slave flip-flop is correct. The 
data captured by the master flip-flop and the slave flip-flop are compared using an 
XOR gate; an error-flag is generated when the two data are not identical. When an 
error occurs, the control circuit stalls pipeline data flow for 1 clock and the slave 
flip-flop resends correct data to the master flip-flop. The principle of the double 
sampling data checking technique is similar to that of the MSFF. 
The timing delay variation of on-chip interconnects affects the design on ∆t. The 
different propagation delay on the on-chip interconnection caused by crosstalk is due 
to different pattern transients. For the increasing timing variation of on-chip 
interconnections, detecting timing error is difficult for various voltage levels. 
However, the MSFF and double sampling data checking technique are limited by the 
clock period and fixed delay line, respectively. Therefore, the run-time error detection 
stage is constructed using the adaptive timing borrowing technique as shown in Fig. 
3.10. The adaptive timing borrowing technique modifies the double sampling data 
checking technique with the adaptive delay line. In addition, the adaptive timing 
borrowing technique also has correction ability via a multiplexer. The modified 
double sampling data checking technique with the adaptive delay line has the adaptive 
timing borrowing ability to borrow timing from the next clock period. 
Fig.3.14 presents analytical results for timing constraints. To ensure that 
functionality of the modified double sampling data checking technique is correct, time 
interval ∆t must be set appropriately, and each pipeline stages must be considered. If 
 77 
datum arrives. Therefore, ∆t should be satisfied as in Eq. (3.15). 
2 3 1 3DFF XOR setup DFF d XOR setupt t t t t t t t+ + < D < + + +                (3.15) 
Additionally, the pipeline stages after the double sampling data checking stage must 
satisfy basic constraints, as in Eq. (3.16), to avoid the excessive timing borrowing. 
3 4DFF MUX Decoder setup clkt t t t t tD + + + + <                   (3.16) 
Eq. (3.14) and (3.15) are the timing conditions that avoid error detections. Eq. (3.16) 
is the timing condition that prevents setup timing violation of the sequential circuitry. 
According to Eqs. (3.14)–(3.16), the upper and lower bounds of time interval ∆t is 
derived by Eq. (3.17). When the time interval ∆t is appropriate, the run-time error 
detection stage corrects error data and provides run-time error rate information, 
allowing the self-calibrated voltage scaling technique to adjust the voltage swing 
levels of link wires. 
2
3
4
Max{( - ),( )}
<  <
Min{( ),
( - )}
DFF1 d XOR setup3 clk DFF XOR setup3
DFF1 d XOR setup
clk DFF3 MUX Decoder setup
t t t t t t t
t
t t t t
t t t t
t
t
+ + + + +
D
+ + +
+ + +
         (3.17) 
If Eq. (3.14) is not be satisfied, a type I statistical error occurs. The double sampling 
data checking technique cannot detect true errors, and suppose that the sampling data 
would be correct. On the other hand, if Eq. (3.15) is not be satisfied, the type II 
statistical error occurs. The double sampling data checking technique then misjudges 
and asserts an error flag when the transferred data is correct. 
Timing delay variation is caused by the crosstalk effect, process variation, width 
variation, voltage variation. In view of increasing timing variation, the adaptive delay 
line is an effective solution that satisfies these conditions. Furthermore, data path 
 79 
coding schemes for 8-bit link wires, which consist of the physical transfer unit size in 
channels and routers. 
Table 3.2 Different combinations of joint coding schemes 
Category Coding Scheme 
Crosstalk 
Avoidance 
Coding 
Error 
Correction 
Coding 
Linear 
Crosstalk 
Coding 
Phit Size 
(wire) 
Phit Size 
(Router) 
ECC Hamming - Hamming - 12 12 
ECCx2 JTEC Duplication Hamming +Parity - 25 25 
CAC 
+ 
ECC 
FTC-HC FTC Hamming Shielding 21 21 
FOC-HC FOC Hamming - 16 16 
OLC-HC OLC Hamming Shielding 34 34 
BSC Duplication Parity - 17 17 
DAP Duplication Parity - 17 8 
DSAP Duplication Parity Shielding 25 8 
SCG 
Green/Triplication Green Triplication - 30 10 
Table 3.3 Summaries of different joint coding schemes for 8-bit link wires. 
Coding Scheme 
Link Wires (8-bit) Codec 
Delay 
(τ0) 
Avg. Energy 
(CVdd2) 
Lowest 
Vdd (V) 
Area 
(μm2) 
Delay 
(ns) 
Power 
(μW) 
Hamming 1+4λ 3.00+5.50λ 0.705 253.3 0.73 190.9 
JTEC 1+2λ 6.25+4.00λ 0.579 512.2 0.93 311.1 
FTC-HC 1+2λ 3.38+4.77λ 0.705 465.5 0.83 253.2 
FOC-HC 1+3λ 3.19+5.14λ 0.705 421.3 0.59 250.1 
OLC-HC 1+λ 6.76+4.91λ 0.710 961.6 0.62 321.3 
BSC 1+2λ 4.13+3.81λ 0.710 488.4 0.73 207.6 
DAP 1+2λ 4.25+4.00λ 0.710 146.3 0.35 68.8 
DSAP 1+λ 4.25+4.00λ 0.710 149.2 0.35 68.9 
SCG 
Green/Triplication 1+2λ 7.05+2.77λ 0.696 266.3 0.28/0.09 103.0/41.5 
The maximum delay and average energy of link wires, the corresponding lowest 
supply voltage of different coding scheme are list in Table 3.3. Table 3.3 also 
summarizes the codec of different approaches, including the corresponding codec area, 
power and latency. The lowest supply voltages are theoretical values from Fig. 3.5 
when ε = 10-20. The JTEC uses double error correction coding to enhance error 
correction. However, codec overhead and energy dissipation are much worse than 
 81 
coupling capacitance of two adjacent lines and loading capacitance. The proposed 
SCG coding achieves the highest EDP reduction regardless of the value of λ. Through 
the tradeoff between reliability and power consumption, the signal swing levels of 
specific codes can be reduced further to the lowest values based on the error 
correction abilities. The lowest signal swing guarantees the same level of word error 
rate as that of the un-coded code. Fig. 3.15(b) shows the energy reduction compared 
to un-coded code under different λ values and the lowest signal swing level. 
Simulation results indicate that the proposed SCG coding realizes more EDP saving 
than other joint coding schemes. When λ equals 4 with a full swing signal (1.0v), the 
 
2 4 6 8 10
-60
-40
-20
0
20
40
A
ve
ra
ge
 E
D
P
 s
av
in
g 
ov
er
 U
nc
od
ed
 w
or
d 
(%
)
Ratio of coupling capacitance to loading capacitance (λ) 
Hamming
FTC-HC
FOC-HC
OLC-HC
BSC
DAP
DSAP
JTEC
Triplication
SCG
2 4 6 8 10
20
30
40
50
60
70
Hamming
FTC-HC
OLC-HC
BSC
DSAP
JTEC
SCG
(a)
SCG Coding Scheme EDP saving 
improved by green 
bus coding stage
(b)
Triplication Error Correction stage
SCG Coding Scheme
A
ve
ra
ge
 E
D
P
 s
av
in
g 
ov
er
 U
nc
od
ed
 w
or
d 
(%
)
Ratio of coupling capacitance to loading capacitance (λ) 
 
Fig. 3.15 The energy-delay product (EDP) reduction to un-coded code under different values of λ with 
(a) full swing signal (b) the lowest swing signal. 
 83 
is as high as 1GHz. Fig. 3.17 illustrates energy reduction with different number of 
routers (N), different lengths (M) under the normal voltage (1.0v) and lowest voltage 
(0.7v). According to some NoC chips [3.34]-[3.36], the length of link wires is set 
from 200μm to 1800μm.  The energy reduction increases while the length of link 
wires increases. Additionally, both reducing coupling effect and supply voltage can 
achieve significant energy saving by the SCG coding scheme. 
200 400 600 800 1000 1200 1400 1600 1800
0
10
20
30
40
N = 8
N = 16
N = 24
N = 32
N = 40
N = 8 (0.7V)
N = 16 (0.7V)
N = 24 (0.7V)
N = 32 (0.7V)
N = 40 (0.7V)
Energy saving 
by reducing 
coupling effects
Energy saving 
by reducing 
voltage
M = Wire Length of Link Wires Between two Routers (μm)
En
er
gy
 R
ed
uc
tio
n 
(%
)
Normal Voltage
(1.0V)
Lowest Voltage
(0.7V)
 
Fig. 3.17 Energy reduction under different lengths of link wires and different number of routers. 
10 15 20 25 30 35 40
0
50
100
150
200
Uncoded
FTC - HC
OLC
BSC
DSAP
JTEC
SCG
Injection Rate (%)
E
ne
rg
y 
D
is
si
pa
tio
n 
(n
J)
8 x 8 Mesh
Uniform traffic pattern
X-Y Routing
Flit size =32 bits
Buffer Depth = 8
Simulation Setup
SCG Coding Scheme
 
Fig. 3.18 Energy dissipation of an 8x8 mesh-NoC with different joint CAC and ECC coding schemes. 
Fig. 3.18 shows the energy dissipation of an 8x8 mesh interconnection network 
with different joint CAC and ECC coding schemes under their lowest supply voltages. 
 85 
and low voltage (0.7v), respectively. The supply voltages have a 15% variation in 3σ 
range and the mean are 1.0V, 0.85v, 0.7v.The maximum value and minimum value of 
td occur in the Dr case and Sf case. The maximum and minimums value under 0.7V, 
0.85V and 1V are 910/485 (ps), 619/333 (ps), and 471/271 (ps), respectively. 
 
250 300 350 400 450
0.00
0.02
0.04
0.06
0.08
0.10
Data path delay(ps)
P
ro
ba
bi
lit
y
Falling Speed-up(Sf) 
Rising Speed-up(Sr) 
Normal falling(Nf)
Falling Delay(Df)
Normal rising(Nr)
Rising Delay(Dr)
271ps
471ps
350 400 450 500 550 600
0.00
0.02
0.04
0.06
0.08
P
ro
ba
bi
lit
y
Data path delay(ps)
Falling Speed-up(Sf) 
Rising
Speed-up(Sr) 
Normal falling(Nf)
Falling Delay(Df)
Rising 
Delay(Dr)
Normal 
rising(Nr)
619ps
333ps
500 600 700 800 900
0.00
0.02
0.04
0.06
0.08
P
ro
ba
bi
lit
y
Data path delay(ps)
Falling Speed-up(Sf) 
Normal falling(Nf)
Falling Delay(Df)
Rising
Speed-up(Sr) Normal 
rising(Nr) Rising 
Delay(Dr)
485ps
910ps
(a)
(b)
(c)
Max Delay
Min 
Delay
 
Fig. 3.19 The data path delay(td) distributions of rising speed-up, falling speed-up, rising delay, falling 
delay, normal rising and normal falling cases under (a) high voltage (1.0v) (b) medium voltage (0.85v) 
(c) low voltage (0.7v). 
 87 
The noise distributions (σv) and timing variations(σd) are distributed in |3σ| range. The 
timing variations may caused by process variation, temperature variation, large 
current density and coupling effect. The control policy of the proposed self-calibrated 
voltage scaling technique is well-described in Section V. The test time of the 
crosstalk-aware test error detection stage is 42 cycles (40 cycles for testing, 2 cycles 
for feedback and adjusting voltage) or 84 cycles. In phase1-4, the initial voltage level 
is the lowest voltage determined by the test stage. Additionally, the initial voltage 
levels in phase5 and phase6 are medium and high, respectively. The voltage in the 
run-time error detection stage cannot be lower than the voltage level determined by 
the crosstalk-aware test error detection stage. Therefore, in phase 6, the voltage level 
is always high in the run-time stage. Based on the error rate, the voltage control unit 
can further increase or decrease the signal voltage swing during run-time. The timing 
overhead of voltage switching is 1 cycle over (256+2) cycles. 
Router
Link wires + low swing driver
Self-calibrated voltage scaling
0.0
0.2
0.4
0.6
0.8
1.0
High 
Voltage
Uncoded Medium 
Voltage
Low 
Voltage
Adaptive
delay line
Test pattern 
generator
66.3%
Double sampling 
data checking
15.1%
8.1%
6.7%
Error detection 
circuits Others (including  
SCG coding codec)
3.8%
-28.3%
-17.6%
-7.2%
+6.9%
(including SCG coding codec)
N
or
m
al
iz
ed
 E
ne
rg
y 
C
on
su
m
pt
io
n
+6.9%
+6.9%
 
Fig. 3.21 Energy analysis of the self-calibrated energy-efficient and reliable interconnection 
architecture. 
In OCINs, link wires in channels dominate the overall power consumption in 
advanced technologies. The proposed SCG coding scheme eliminates most crosstalk 
effects and achieves energy reduction. From Fig. 3.15(b), the EDP reduction of low 
 89 
round-robin arbitration. The router architecture is set as 5input/output ports with 
4-stage pipeline. And the FIFO depth of each output buffer is 8 flits. The size of each 
flit size is 32 bits. The area breakdown of adaptive double sampling data checking, 
MAF-based test generator and voltage control unit in the self-calibrated voltage 
scaling are 71%, 8% and 21%, respectively. 
Table 3.4 Summaries of SCG coding and self-calibrated voltage scaling 
(length=1800μm, low voltage) 
Area overhead 
in a router 
Energy 
Overhead 
Energy Reduction 
(Channels) 
SCG Coding 1.21% 0.26% 14.1% 
Self-calibrated voltage scaling 13.23% 6.62% 21.1% 
3.6  Summary 
The physical effects of crosstalk and PVT variations in nano-scale technologies 
degrade the performance of on-chip interconnection networks (OCINs). This work 
uses a combination of a self-calibrated voltage scaling technique and a self-corrected 
green (SCG) coding scheme to overcome increasing variations and achieve 
energy-efficient on-chip data communication. The SCG coding scheme is used to 
construct reliable and energy-efficient channels. The SCG coding scheme has two 
stages, the triplication error correction coding stage, and the green bus coding stage. 
Triplication error correction coding is a reliable mechanism that achieves rapid 
correction ability to reduce the physical transfer unit (phit) size in routers via 
self-correction at the bit level. Green bus coding reduces energy reduction 
significantly via a joint triplication bus power model that eliminates crosstalk effects. 
The self-calibrated voltage scaling technique is designed with the SCG coding scheme. 
The self-calibrated voltage scaling technique adjusts the voltage swing of link wires 
via two error detection stages, the crosstalk-aware test error detection stage and 
 91 
Chapter 4: 
Two-Level FIFO Buffer Design for Routers 
The on-chip interconnection network (OCIN) is an integrated solution for 
system-on-chip (SoC) designs. The buffer architecture and size, however, dominate 
the performance of OCINs and affect the design of routers. This work analyzes 
different buffer architectures and uses a data-link two-level FIFO (first-in first-out) 
buffer architecture to implement high-performance routers. The concepts of shared 
buffers and multiple accesses for buffers are developed using the two-level FIFO 
buffer architecture. The proposed two-level FIFO buffer architecture increases the 
utilities of the storage elements via the centralized buffer organization and reduces the 
area and power consumption of routers to achieve the same performance achieved by 
other buffer architectures. Depending on a cycle-accurate simulator, the proposed 
data-link two-level FIFO buffer can realize performance similar to that of the 
conventional virtual channels, while using 25% buffers. Consequently, the two-level 
FIFO buffer can achieve about 22% power reduction compared with the similar 
performance of the conventional virtual channels using UMC 65nm CMOS 
technology. 
4.1 Background 
The generic OCIN is based on a scalable network, which considers all 
requirements associated with on-chip data communication and traffic. OCINs have a 
few beneficial characteristics, namely, low communication latency, low energy 
consumption constraints, and design-time specialization. The motivation in 
establishing OCINs is to achieve performance using a system communication 
 93 
conversely, over-provisioning buffers clearly wastes scarce area resources. Thus, in 
this work, buffer utilization is optimized via a centralized buffer in a router as shown 
in Fig. 4.2(a). Compared with a generic router as shown in Fig. 4.2(b), a centralized 
buffer has a shared buffer mechanism allowing channels to share the centralized 
buffer with sufficient buffer space. 
Centralized
Buffer
MUX
M
U
X
MUX
MU
X
M
U
X
(a) (b)
Arbiter
Arbiter
M
U
X
A
rbiter
MUX
Ar
bit
er
Arbiter
M
U
X
A
rb
ite
r
BufferDE-MUX
InputOutputInputOutput
MUXMUX
 
Fig. 4.2 (a) A router with the centralized buffer (b) A generic router. 
A data-link two-level FIFO (first-in first-out) buffer architecture with the 
centralized shared buffer is proposed in this paper. The proposed two-level FIFO 
buffer architecture has a shared buffer mechanism allowing the output channels to 
share the centralized FIFO with sufficient buffer space. Additionally, the proposed 
architecture reduces the area and power consumption to achieve the same 
performance. The reminder of this chapter is organized as follows. Section 4.2 
compares and analyzes different buffer architectures and different circuit 
implementations. The concept of the proposed two-level FIFO buffer architecture is 
presented in Section 4.3. Section 4.4 and 4.5 describe the behavior and circuit 
implementation of two-level FIFO buffer for the synchronous router and 
asynchronous router, respectively. The associated two-level FIFO buffer architectures 
are presented in Section 4.5. Section 4.7 shows simulation results. Finally, summaries 
are given in Section 4.8. 
 95 
due to rapid increasing power consumption and circuit area [4.6], [4.10]. In most 
OCINs, register-based buffers are adopted to provide high bandwidth of on-chip data 
communication. Consequently, register-based buffers can be classified into four 
different implementations — (a) Shift Register, (b) Bus-In Shift-Out Register, (c) 
Bus-In Bus-Out Register, and (d) Bus-In MUX-ut Register [4.1]. 
(a)
(b)
(c) (d)
D Q D Q D Q D QD Q
R_req
enenenenen
packet_in packet_out
First InNew Arrival Intermediated Empty Bubble
D Q D Q D Q D Q
FIFO Cell FIFO Cell FIFO Cell FIFO Cell
Reset
W_reg
Packet_in
D Q D Q D Q D Q
Reset
R_reg
Packet_out
New Arrival First In
D Q D Q D Q D Q
FIFO Cell FIFO Cell FIFO Cell FIFO Cell
Reset
W_reg
MUX
Packet_in
packet_out
1
A
dder
D Q
R_reg
First InNew 
Arrival
packet_out
D Q D Q D QD Q
R_req
enenenen
packet_in
First InNew Arrival
D Q
en
onononoffoff 110
Controller
0
W_reg
 
Fig. 4.4 Different buffer implementation (a) Shift Register (b) Bus-In Shift-Out Register (c) Bus-In 
Bus-Out Register (d) Bus-In MUX-Out Register. 
Fig. 4.4(a) shows a conventional shift register. When a consumer sends a request to 
a buffer, a shift register will enable all registers and shift the data to the output port. 
Indeed, implementing a shift register is less complicated than implementing others. 
However, intermediate empty cells induced by different packet in/out rates temporally 
influence the network performance by adding unnecessary latency. Nevertheless, 
shifting all registers in a buffer consumes a huge amount of power. Implementing a 
shift register on a chip is not desirable due to unnecessary latency and massive power 
consumption. Fig. 4.4(b) shows the Bus-In Shift-Out Register, which only shifts full 
cells to remove intermediate empty bubbles. An arrival packet can be stored in the 
 97 
disadvantage of output buffering is that in one cycle, data from multiple input ports 
may be written to the same output port. Nevertheless, a multiple-access buffer can be 
implemented in parallel at the output to deal with this shortcoming. Both output 
buffers and input buffers can cause the head-of-line blocking problem and stall input 
data. Fig. 4.5 shows the input buffers, middle buffers and output buffers in routers. 
During middle buffering, the buffer placement moves to the middle of switching 
circuits. Middle buffer architectures have O(N2) buffer blocks for an N-port router, 
while input and output buffering architecture only have O(N) buffer blocks. The 
middle buffer architecture, however, can reduce the effects of head-of-line blocking 
via multiple virtual channels during switching. This is a trade-off between traffic 
problems and buffer sizes. 
B
uf
fe
r
Input Buffer Middle Buffer Output Buffer
deMUX
MUX
Virtual 
Channels  
Fig. 4.5 Diagram of input buffer, middle buffer and output buffer. 
Since buffer resources are costly in resource-constrained OCIN environments, 
minimizing buffer size without adversely affecting performance is essential. However, 
based on observed traffic patterns, buffer size and architecture cannot be changed 
dynamically during operation. Therefore, some approaches [4.6], [4.7] optimize 
pre-determined buffer size during the design stage via a detailed analysis of 
application-specific traffic patterns. Additionally, static virtual channel allocation 
techniques were proposed to optimize the performance, area and power for target 
 99 
allocation of each virtual channel via virtual channel control tables and dispensers. 
However, the hardware overhead would increase non-linearly. In view of this, other 
dynamically-allocated virtual channel architectures were proposed by inspecting the 
physical link state and speculating the packet transferring [4.20]-[4.23]. However, 
when the shared buffers of an input port are full, these approaches do not provide a 
mechanism for accessing the buffers of other virtual channels at other input ports. 
Furthermore, the performance of these dynamical virtual channel allocation schemes 
is also limited since the resource-constraints of the pointers and virtual channel 
control tables. 
Fig. 4.6(b) shows the centralized shared buffer architecture that maximizes buffer 
utilization [4.24]-[4.26]. Shared buffer architectures are implemented by centralized 
buffer organizations, which dynamically alter buffer size for different channels. The 
input packets from different ports can access all buffers without any head-of-line 
blocking. This architecture enhances OCIN performance regardless of traffic type. 
Shared buffering, in addition, achieves the best buffer utility with the fewest memory 
elements. The centralized shared buffer architectures enhance the buffer utilization via 
allocation tables [4.25], [4.26]. Nevertheless, the control mechanisms of these shared 
buffer architectures are more complex than those of other buffer architectures and 
increase the pipeline stages. Hence, the new proposed data-link two-level FIFO buffer 
architecture is utilized as the shared buffer architecture to simplify the shared buffer 
architecture and achieve better performance than other buffer architectures while not 
increasing buffer size. 
4.3 Concept of Two-Level FIFO Buffer Scheme 
The proposed two-level FIFO buffer is constructed by a centralized level-2 FIFO 
 101 
FIFO buffer, the arbiter only manages the order of switching packets in output 
channels. 
Initial Point Flit[0] Linker[0]
Flit[1] Linker[1]
Flit[2]
Slot E
Slot A
Slot B
Flit[2]
Flit[0] Slot E
Flit[1] Slot A
Data Field Linker Field
A
B
C
D
E
F
Data-link-based FIFO Empty Slot  
Fig. 4.8 Concept of the data-link-based FIFO. 
The centralized level-2 FIFO achieves shared buffering using data-link-based FIFO. 
Fig. 4.8 presents the concept of the data-link-based FIFO, which takes advantage of 
data continuity in an FIFO queue. Each slot in the data-link-based FIFO has two 
stored fields, the data field and linker field. In a slot, the data field stores a flit and 
linker field stores the address of the next slot, which may not be the adjacent slot in 
the data-link-based FIFO. In the other words, the linker[i] will store the address of the 
flit[i+1] in the same FIFO queue. Therefore, the read controller reads the next datum 
depending on the address stored in the linker field. 
The two-level FIFO buffer scheme can be employed at the flit level or packet level 
depending on flow control techniques, store-and-forward switching, virtual 
cut-through switching or Wormhole switching [4.27]. Wormhole flow control was 
proposed to improve performance at the flit level and relaxes the constraints on buffer 
sizes. Therefore, the wormhole switching technique is the most popular switching 
technique in packet-switching-based OCINs [4.28]-[4.30]. At the flit level, when more 
than one packet are sent to the same output, the links between these packets cannot be 
constructed. Therefore, the two-level FIFO buffer needs an extra linker table to record 
 103 
same time, the read address of output N will be changed to slot 1. Additionally, the 
data flit from the router W will be stored into slot 4 that is linked to slot 9. In this 
example, the packets form E, N and P are routed to the output S, and the order of 
these packets is E–N–P. The header flit of packet N should be linked to the tail flit of 
packet E. However, the tail flit of packet E is not dispensed to this router yet. In view 
of this, the two-level FIFO buffer scheme needs an extra linker table to reconstruct the 
link by recording the linker for the tail flit. 
D
at
a
-lin
k 
S
ch
ed
ul
er
Centralized 
Level-2 FIFO 
D
istributed 
Level-1 FIFO
Output packets
Acknowledge 
signals from 
neighbor nodes
H
eader D
ecoder &
 R
outing Arbiter
Input P
ackets from
 
different Input channels
Read/Write Signals
Traffic
 
Fig. 4.10 Two-level FIFO buffer architecture in routers. 
4.4 Synchronous Two-Level FIFO Buffer Architecture 
The two-level FIFO buffer architecture is implemented using register-based buffer 
and consists of a data-link scheduler, distributed level-1 FIFOs, and a data-link-based 
centralized level-2 FIFO. Fig. 4.10 shows the architecture of the data-link two-level 
FIFO buffer. The operation of the two-level FIFO buffer router is briefly described as 
follows. When input packets arrive to the two-level FIFO buffer architecture, the 
 105 
record the linked addresses. The width of the data fields is m-bits, and depends on the 
physical size of a flit. 
Data
 Field
Wordline 
Encoder
W
rit
e 
G
en
er
at
or
Read 
Controller
R_req from 
distrbuted Level-1 
FIFO
Output packets to distributed Level-1 
FIFOs of each output channel
Aribter
Arbitration 
Information
Input Packets from different 
Input channels
Centralized 
level-2 FIFO
Linker 
Table
Write signals 
for one slot
&
Linker
Field
Data-link Scheduler
 
Fig. 4.11 Data-linked based centralized level-2 FIFO and data-link scheduler. 
The data-link scheduler creates links among output channels using the write 
generator and linker fields. The write generator generates the writing wordlines for the 
data fields to write input flits. While asserting the writing wordlines for the data fields, 
the linked addresses are produced using the wordline encoder. The wordline encoder 
encodes these writing wordlines and feeds the encoded addresses (linked addresses) 
into the linker fields to create links. Therefore, the write generator also latches writing 
wordlines of the data fields for the linker fields to record the addresses of the next 
arrival flits. Thus, the switching circuits of the router are utilized in the write 
generator and data fields based on the link information. Clearly, the read controller 
obtains addresses from the linker fields to read the next flits of the output channels in 
the data fields. Hence, the read controller reads the output flits and linked addresses at 
the same time, and latches the linked addresses for the next transaction. Restated, 
 107 
information to the write generator first. Thus, the write generator switches the Bus-In 
data into appropriate words by writing wordlines and writing MUXs. Further, 
depending on switching conditions, the write generator transfers writing wordlines to 
the wordline encoder and creates links. The read controller and reading MUXs decode 
link addresses and send output data to the distributed level-1 FIFO. 
4.4.3 Distributed level-1 FIFO 
The distributed level-1 FIFOs are designed as output queues located in output 
channels. Hence, the distributed level-1 FIFOs are implemented using Bus-In 
Mux-Out registers for shallow output queues. The purpose of distributed level-1 
FIFOs are to provide a linear increasing of the FIFO sizes to retrieve the fixed sizes of 
the centralized level-2 FIFO. Therefore, the size of the distributed level-1 FIFO is 
usually small, and the Bus-In MUX-Out register is preferred. Moreover, the 
distributed level-1 FIFOs pre-fetch flits from the centralized level-2 FIFO and to keep 
the data flow when other output channels are congested. 
4.4.4 Arbiter 
The arbiter determines the order of multiple accesses in the same cycle. When 
more than one packet at different input ports require the same output port, the arbiter 
prioritizes the packets. The arbitration algorithm, however, relies on buffer sizes. 
When buffer size is insufficient, the complexity of the arbiter algorithm increases to 
eliminate traffic problems. The two-level FIFO buffer architecture provides sufficient 
buffer sizes using the shared buffer mechanism and multiple accesses for output 
buffers. That is, the arbiter only decides the order of packets from different input 
channels when the header flits of these packets are arrival at the same time. 
 109 
[4.32]-[4.34]. In GALS systems, data synchronization and communication across 
different clock domains are extremely challenging. An alternative solution is provided 
by GALS interface and an asynchronous network [4.35]-[4.38]. 
D
at
a
-lin
k 
S
ch
ed
ul
er
Centralized 
Level-2 FIFO 
Read/Write Signals
Distributed 
Level-1 FIFO
A
rbiters for O
utput C
hannels 
R
ea
d_
re
q
A
rbiter
Arbitrate the read and 
write data of centralized 
level-2 FIFO Distributed Level-1 FIFO
Output 
packet
R
ea
d_
ac
k
R
ea
d_
re
q
R
ea
d_
ac
k
write_req
write_ack
input packet
H
eader D
ecoder &
 S
w
itching
 
Fig. 4.14 Asynchronous two-level FIFO buffer architecture. 
The two-level FIFO buffer is also employed for an asynchronous router, and the 
behavior of the asynchronous two-level FIFO buffer is similar to that of the 
synchronous two-level FIFO buffer. Fig. 4.14 shows the architecture of the 
asynchronous two-level FIFO buffer. However, the main difference between the 
synchronous and asynchronous two-level FIFO buffers is that the synchronous 
two-level FIFO has a multiple access mechanism. In terms of the different frequencies 
of each local clock domain, the arrival times of data from different input channels 
cannot be predicted. Thus, the asynchronous two-level FIFO buffer must be 
time-insensitive. Therefore, implementing the level-2 FIFO with multiple access 
 111 
avoid confusion, the grant signal must be free of hazards and not be in other 
meta-stable states. The arbitration for the four-request asynchronous architecture can 
be performed using a MUTEX-based scheme, which is called MUTEX-NET [4.33]. 
When four requests arrive from different senders, the MUTEX-NET guarantees that 
the first request and last request are granted in their original order, but the other two 
requests may not retain their order. 
Wordline 
Encoder
Linker
Field
W
rit
e 
G
en
er
at
or
Read 
Controller
Output packet to the granted 
distributed Level-1 FIFO
Write 
Aribter
Input Packets 
after switching
Pulse 
Generatorr_
re
q
r_req/r_ack 
from/to distrbuted 
Level-1 FIFO
Read
Arbiter
r_pulse
w_req
w_pulse
R/W Enable
Arbiter
r_enable
w_enable
Data
 Field
w_req/w_ack 
from/to distrbuted 
Level-1 FIFO
 
Fig. 4.15 Data-link scheduler and centralized level-2 FIFO for asynchronous two-level FIFO buffer. 
Fig. 4.15 shows the details of the data-link scheduler and centralized level-2 FIFO 
for the asynchronous two-level FIFO buffer. The write and read arbiters determine 
access permission of the centralized level-2 FIFO for different output channels when 
more than two output channels issue requests. The R/W-enable controller produces 
R/W-enabled signals for the centralized level-2 FIFO based on read_req and write_req, 
which are requests from the read port and write port, respectively. Because the write 
generator must identify the written word for the next transaction according to the 
 113 
4.6 Associated Two-Level FIFO Buffer Architecture 
Example : 8 output channels
Full association (8-way)
4-way association2-way association
Hybrid association
Distributed 
Level-1 FIFO
Centralized 
Level-2 FIFO
Distributed 
Level-1 FIFO
Centralized 
Level-2 FIFO
Data-Link-Based FIFO
(Bus-MUX-In MUX-
Out Registers)
 
Fig. 4.17 Different associations between the distributed level-1 FIFO and the centralized level-2 FIFO 
for 8 output channels. 
The two-level FIFO buffer architecture has a unified shared buffer to eliminate 
head-of-line problems by the data-link-based FIFO and multiple-access mechanism. 
However, based on the register-based buffer, the power and area overhead of multiple 
accesses for the centralized level-2 FIFO increases rapidly as the number of output 
channels increases. Therefore, a trade-off exists between buffer utilities and the power 
overhead of multiple accesses. That is, the centralized level-2FIFO can be divided 
into subgroups for specific output channels. Fig. 4.17 shows the different associations 
between the distributed level-1 FIFO and centralized level-2 FIFO for 8 output 
channels. The centralized level-2 FIFO can be deconstructed into different 
subgroups—two-way association, four-way association, full association or hybrid 
association. The higher association between the level-2 FIFO and level-1 FIFO will 
increase buffer utilities of the two-level FIFO buffer. That is, each output channel can 
access an increased number of buffers in the higher association.  Moreover, the 
physical size of the linker field decreases with the increasing association. Assume the 
number of available buffer in the centralized level-2 FIFO is k slots. For the m-way 
 115 
shared buffer to increase performance. However, an extra pipeline stage is added into 
the DSB router that has a five-stage pipeline comprising RC, timestamping (TS), 
conflict resolution (CR) and VA, first switching traversal (ST1) and middle memory 
writing (MM_WR), and middle memory reading (MM_RD) and second switching 
traversal (ST2). The proposed data-link two-level FIFO buffer also provides a 
centralized shared buffer without inserting extra pipeline stage. Therefore, the 4 
pipelining stages include RC, arbitration and write wordlines generating & encoding 
(W_Gen), data buffer writing (Data_W) and data link creating (Link_W), and data 
buffer reading (Data_R) and linker reading (Link_R). Moreover, the switching circuit 
is concealed in write wordlines generating and data link creating as described in 
Section 4.4. 
Arbitration
LT
LTRC VA SA ST
ST1
MM_WR
CR
VATSRC
MM_RE
ST2
RC: Routing computation
VA: Virtual channel allocation (Arbitration)
SA: Switch allocation ST: Switching traversal
output buffer, middle buffer (virtual channel), VichaR
Generic router (4-stages pipeline router)
Distributed shared buffer (DSB) router (5-stages router)
TS: Timestamping CR: Conflict resolution
ST1: First switching traversal ST2: Second switching traversal
MM_WR: Middle memory writing MM_RE: Middle memory reading
LT
Data_W
Link_WW_Gen
RC
Data_R
Link_R
Two-level FIFO buffer router (4-stages router)
W_Gen: Write wordlines generating and enocding
Data_W: Data Buffer writing Data_R: Data buffer reading
Link_W: Data link creating Link_R: Reading address of next data 
LT: Link traversal
 
Fig. 4.18 Pipeline stages of the generic router, DSB router and two-level FIFO buffer router. 
According to the cycle-driven simulation in SystemC, Fig. 4.19 shows the 
performance of output buffer, middle buffer, ViChaR, DSB and two-level FIFO 
buffers (including 2-3 hybrid association and full association) with different buffer 
 117 
In a mesh network, each router has 5 inputs and 5 outputs. In a ViChaR router, each 
input channel has a unified buffer structure with the same buffer size of the 
input/output buffer. For the middle buffer, each input port has 4 virtual channels. 
Therefore, the depth of each virtual channel is from 2-flit to 16-flit in this simulation. 
For the two-level FIFO buffers, the distributed level-1FIFO is set as 2-flit for each 
output channel. The 2-3 hybrid associated two-level FIFO buffer divides the 
centralized level-2 FIFO into two subgroups. One is shared by the east and west ports, 
and the other is shared by the north port, south port and processor element. Figs. 
15(a)–(c) show the simulation results under different injection loads of low (0.15), 
medium (0.25) and high (0.35), respectively. Performance is normalized to the 
throughput of infinite buffers within constant cycles. The two-level FIFO buffer 
architecture performs best with the same size of other buffer architectures regardless 
of injection loads. For the ViChaR, the unified buffer structure shares buffers in 
virtual channels for each input port. Thus, the unified control logic controls 
arriving/departing pointers and virtual channel allocation of each virtual channel 
through virtual channel control tables and dispensers. However, when the shared 
buffer of an input port is full, the ViChaR does not provide a mechanism for accessing 
buffers of other virtual channels in other input ports. For the centralized shared buffer, 
the performance of the DSB buffer is similar to that of the fully associated two-level 
FIFO buffer. Additionally, when the total buffer size is small, the performance of the 
middle buffer is worse than that of the output buffer. The reason of this phenomenon 
is due to shallow virtual channels. In high injection load, the throughput of different 
buffer architectures is quite smaller than that of infinite buffers because the 
performance is dominated the heavy congestion of the network. Compared to the 
traditional router with middle buffers, the total buffer size of the two-level FIFO 
buffer can be reduced to 20%–25% for achieving the same performance. 
 119 
injection load, the average latencies of the DSB buffer are larger than those of 
ViCharR and two-level FIFO buffer because inserting one extra pipelining stage. With 
the increasing injection load, the average latencies of DSB buffer are reaching to 
those of two-level FIFO buffer because the latencies are dominated the heavy 
congestion of the network. Moreover, the fully associated two-level FIFO buffer can 
realize the lowest average latencies compared to other buffers. Nevertheless, the 
boundaries of shared buffers, including ViChaR and two-level FIFO buffer, decrease 
significantly in hotspot patterns. Restated, the shared mechanism cannot lighten the 
traffic efficiently with hotspots. 
(a)
5 10 15 20 25 30
100
200
300
400
500
XY (middle buffer)
DyXY(middle buffer)
Adaptive (middle buffer)
XY (two-level FIFO)
DyXY (two-level FIFO)
Adaptive (two-level FIFO)
Injection Rate  (%, flits/node/cycle)
A
ve
ra
ge
 L
at
en
cy
 (C
yc
le
s)
(b)
Injection Rate  (%, flits/node/cycle)
5 10 15 20 25 30 35
50
100
150
200
250
300
350
400
XY (middle buffer)
DyXY(middle buffer)
Adaptive (middle buffer)
XY (two-level FIFO)
DyXY (two-level FIFO)
Adaptive (two-level FIFO)
A
ve
ra
ge
 L
at
en
cy
 (C
yc
le
s)
 
Fig. 4.21 Average latencies of XY, DyXY and adaptive routing algorithms in (a) uniform patterns (b) 
hotspot patterns. 
 121 
UMC 65nm CMOS technology at 1.0V and 1GHz. Table 4.1 lists the area and power 
consumption of routers with different buffer architectures for similar buffer sizes. 
These buffer architectures include the middle buffer using the static virtual channel 
allocation, a dynamic virtual channel regulator (ViChaR), DSB and two-level FIFO 
buffer architectures. The middle buffer architectures are implemented as the Bus-In 
MUX-Out and Bus-In Bus-Out registers, respectively. The middle buffer is 
implemented as 5 input ports with 4 virtual channels for input port; each virtual 
channel has 8 flits and each flit size is 64 bits. Therefore, the total number of flits for 
each router is 160 flits (5×4×8). The ViChaR has a unified buffer structure that 
dynamically allocates virtual channels and buffer resources according to network 
traffic patterns. The ViChaR is composed of a unified buffer structure and unified 
control logics, which are the arriving/departing pointers and the VC control table. In 
each input port, the size of the unified buffer structure is 32 flits. For fully associated 
two-level FIFO buffer architecture, the centralized level-2 FIFO is implemented as 
Bus-MUX-in MUX-out registers; the centralized level-2 FIFO is 128 flits (words) 
deep and 64 bits wide. For the 2-3 hybrid associated centralized level-2 FIFO, the 
depth is 64 flits (words) and width is 64 bits for each subgroup. In order to obtain the 
same size of two-level FIFO buffers for different buffer architectures, each distributed 
level-1 FIFO has 6 flits with 64 bits that linearly increase FIFO sizes to determine the 
fixed sizes of the centralized level-2 FIFO. Therefore, the total number of flits in the 
fully associated and 2-3 hybrid association is 158 flits (128+6×5, 64×2+6×5). For a 
similar number of flits, the DSB occupies the largest area compared with those of 
other buffer architectures because of two switch circuits, complex arbitration and 
great amount of lookup tables. The proposed two-level FIFO buffer architecture 
induces 38.2% area overhead since multiple accesses of the centralized level-2 FIFO. 
Nevertheless, the two-level FIFO buffer architecture dissipates the smaller power than 
 123 
Consequently, DSB and two-level FIFO buffer achieve the similar performance using 
one-fourth buffers (40 flits), and each flit size is 64 bits. Based on UMC 65nm CMOS 
technology at 1.0V and 1GHz, the ViChaR, DSB and proposed two-level FIFO buffer 
can achieve 9.3%, 19.2% and 22.3% power reduction, respectively. 
Table 4.2 Area and power comparisons between different buffer architectures with similar performance. 
Buffer architecture Area (μm2) Power (mw) 
Middle buffer 
(Bus-in MUX-out, 160 flits) 60524.3(0%) 16.94(0%) 
ViChaR 
(80 flits) 52213.6(-13.7%) 15.37(-9.3%) 
DSB 
(40 flits) 47771.6(-21.1%) 13.69(-19.2%) 
Fully associated two-level 
FIFO buffer 
(40 flits) 
44251.3(-26.9%) 13.17(-22.3%) 
4.7.2 Asynchronous Two-Level FIFO Buffer 
PE2
PE3
Demonstrated System (8x8 DCT)
Adder
Multiplier
Shifter
PE1
Controller &
Memory
Asynchronous
Router
PE4
 
Fig. 4.23 The demonstrated 8x8 DCT system for an asynchronous router. 
To estimate performance of the proposed asynchronous two-level FIFO buffer, a 
fixed-point 8×8 discrete cosine transform (DCT) is demonstrated. This demonstrated 
8x8 DCT is performed with an 8-point 1-dimensional DCT along x-axis and y-axis 
individually. Fig. 4.23 shows the setup of the fixed-point 8x8 DCT with an 
 125 
area and energy are normalized to the output buffer. And thus, the latency, area and 
energy dissipation of the output buffer are 473.2ns, 17624.7μm2 and 3.65nJ, 
respectively. According to the increasing latency, area and energy of the fully 
associated two-level FIFO buffer, this buffer architecture is not adapted to the 
asynchronous router due to shallow level-1 FIFOs and the limited bandwidth of the 
centralized level-2 FIFO. The tow-way associated two-level FIFO buffer reduces total 
latency and energy consumption by 13.1% and 5.2% compared with those of 
traditional output buffers due to the decrease in head-of-line problems. Nevertheless, 
the area overhead of the two-way association is 10.1% occupied by extra arbiters and 
the data-link scheduler. 
4.8 Summary 
On-chip interconnection network (OCIN) designs have been considered as an 
effective solution to integrate process-independent interconnection architectures and 
multi-core systems. Additionally, OCIN performance is directly related to the buffer 
sizes and utilization. In this paper, a data-link two-level FIFO buffer architecture is 
presented as a good solution for routers in OCINs based on a shared buffer 
mechanism and multiple accesses. Additionally, the centralized level-2 FIFO is 
realized via a data-link scheduler. This buffer architecture with a small buffer size 
reduces the magnitude of head-of-line blocking problems and performs well. 
According to the cycle-accurate simulator, the two-level FIFO buffer can realize 
performance similar to that of the conventional virtual channels, while using 
20%-25% buffers. Based on UMC 65nm CMOS technology, the proposed data-link 
two-level FIFO buffer can achieve about 22% power reduction compared with the 
similar performance of the conventional virtual channels. In addition to the 
asynchronous router using UMC 90nm CMOS technology, the two level FIFO buffer 
 127 
Chapter 5: 
Adaptive Congestion-Aware Routing Algorithm 
for Mesh On-Chip Interconnection Networks 
In this chapter, an adaptive congestion-aware routing algorithm is proposed for 
mesh on-chip interconnection networks (OCINs). In mesh OCINs, data of each 
processor elements (PEs) are communicated by packet switch techniques. The 
destinations of the packets are determined by the routing algorithm, which plays a key 
role to determine the performance and power consumption. Depending on the traffic 
around the routed node, the proposed routing algorithm provides not only minimum 
paths but also non-minimum paths for routing packets. Both minimum and 
non-minimum paths are based on the odd-even turn model to avoid deadlock and 
livelock problems. The decision of the minimum paths or non-minimum paths 
depends on the utilities of buffers in neighbor nodes and the specific switching value. 
In this adaptive algorithm, the congestion conditions and distributed hotspots can be 
avoided.  It has the advantages of higher performance and lower latency. From the 
simulation results, it clearly shows that the adaptive congestion-aware routing 
algorithm is superior to other algorithms for the mesh OCINs. 
5.1 Background 
Multi-core system-on-chip (SoC) designs provide the integrated solutions for the 
applications in communications, multimedia and consumer electronics. Due to the 
requirements of high speed and low power on-chip communication are growing 
continuously, OCINs are introduced to migrate challenges caused by the bus 
interconnect technology and the complexity of next generation SoC designs [5.1], 
 129 
balanced, including power consumption, area overhead, performance and robustness. 
For the performance issue, the routing algorithms have to reduce the latency and to 
maximize the traffic utilization. The latency will be affected by the congestion and 
contention problems. Hence, handling resources spatially and temporally can prevent 
the contention, which overloads the resources individually, and the congestion, which 
is caused by packets collection. For the robustness to traffic changes, the routing 
algorithms should behave balanced to a large spectrum of traffic conditions. 
Another major constraint for the routing algorithms is assuring the freedom from 
deadlock and livelock [5.4]. The definition of deadlock is that a packet is blocked at 
some intermediate resource and it does not reach its destination. Deadlock occurs 
when one or more packets in the network are blocked and keep blocked for an 
indefinite time, waiting for an event that can’t happen. The prominent strategy for 
dealing with deadlock is avoidance, and most deadlock-free routing algorithms are 
deduced by the strategy. The designers should check whether this algorithm is 
deadlock free or not. If not, the designers have to add a hardware resource or to 
restrict routing rules for deadlock free. Deadlock free is analyzed by building a 
dependency graph. And thus the dependency graph can’t be cyclic. There are lots of 
algorithms to solve deadlock by prohibiting some specific turns to prevent the 
deadlock, such as West-First Routing Algorithm, North-Last Routing Algorithm and 
Negative-First Routing Algorithm [5.5]. The definition of livelock is a packet enters a 
cyclic path dose not reach its destination. The livelock will be occurred in dynamic 
routing algorithms. 
The routing algorithms can be classified to several categories [5.6]. The routing 
decision at each router can be static or dynamic. In the static routing, the path is 
completely determined the source and destination address. The routing does not 
 131 
to avoid hotspots and to shorten the average latency of transmitted packets through 
adopting minimal and non-minimal path alternately without virtual channels. When a 
hotspot is close to a router, the routing algorithm will choose the non-minimal paths. 
Otherwise, the minimal path will be selected. In addition, a guaranteeing QoS 
arbitration mechanism is also implemented in the router depending on the priority of 
the packets. Fig. 5.1 illustrates the architecture of the router design. We implemented 
an NxN network of interconnected tiles with the mesh topology. Each tile is 
composed of a router and a processing element. Therefore, a router is connected with 
four adjacent routers and its processing element. 
The flow diagram of the routing procedure in a router is shown as Fig. 5.2. And the 
detail of each step will be described as follow. 
Step 1. To begin with the packet injection, the routing controller will calculate the 
scores of each direction by summing the number of available buffer unit of 
adjacent router and the buffer of its neighbors.  
Step 2. Determine the next nodes whether are on minimal paths or not.  
Step 3. If the scores of next nodes which are on minimal paths are higher than a 
specific value we can adjust, select one of the next nodes on minimal paths 
with the highest score. Otherwise, select the score is the highest one from the 
next nodes which are on the minimal and the non-minimal paths.  
Step 4. If there are more than one packet will be routed to the same output channel, 
the arbiter will arbitrate the packets according to the priorities which will 
guarantee the quality-of-service. However, in order to avoid deadlock caused 
from guaranteeing QoS, once any packet is halted for more than 10 cycles, 
the arbitration policy is switched to choose the packet with the longest 
waiting time.  
 133 
5.3 Congestion-Aware Routing Algorithm 
Header Decoder & 
Minimal/Non-Minimal 
Node Decision
Adaptive Decision 
Unit
Buffer Information 
Collector (East)
Input Packet
Buffer Information 
Collector (West)
Buffer Information 
Collector (South)
Buffer Information 
Collector (North)
S
c
o
r
e
 
c
a
l
c
u
l
a
t
o
r
score1
score2
score3
Crossbar
Node 
Information
Buffer Information 
Collector (this node)
 
Fig. 5.3 The architecture of adaptive congestion-aware routing algorithm. 
In this section, the proposed adaptive congestion-aware routing algorithm will be 
described. It is proposed for the mesh topology to avoid hotspots and to shorten the 
average latency of transmitted packets through adopting minimal and non-minimal 
path alternately. Fig. 5.3 shows the architecture of the routing algorithm. Once a 
packet injects a node, the adaptive congestion-aware routing algorithm will decide the 
output of the injected packet depending on the adaptive decision. The adaptive 
decision of the output channels depends on the scores of the probable neighboring 
nodes which indicate the traffic around this router. The detail of each block will be 
described as follow. 
5.3.1 Deadlock Avoidance by the Odd-Even Turn Model 
The proposed congestion-aware adaptive routing algorithm adopts Odd-Even turn 
model to solve the deadlock problem [5.17]. The Odd-Even (OE) turn model prohibit 
some turns for eliminating deadlock. The rules of OE turns are as following. 
Rule 1. Any packet is not allowed to take an EN turn at any nodes located in an 
even column, and it is not allowed to take an NW turn at any nodes located 
in an odd column. 
 135 
packet. In addition, the number in the green square represents the number of available 
buffer unit. Now, the packet is in the East output buffer of No.1 router, and it indicates 
the packet will go east in this cycle. Since the packet does not arrive at the destination 
and cannot go back to west, its next node will only be North, East and South of No.3 
router. 
In order to calculate the north score, the score calculator should get the buffer 
information of the north node from the buffer information collector with the 
Odd-Even turn model. The north score includes the number of available buffer unit of 
the north output buffer and the next probable output buffers, namely the average of 
the number of the light green parts. Beware that the included output buffer should be 
based on Odd-Even turn model. Because the NW turn is prohibited, the west output 
buffer of No.2 router is not included in the north score. The calculation of the east 
score is the same as that of the north score. Since the east score should be based on 
Odd-Even turn model (the EN, ES turn are prohibited.), the score is the average of the 
numbers which are shown as the light green parts. And the South score is calculated in 
the same manner as shown in Fig. 5.4. 
Table 5.1 The modified score calculator. 
The valid buffer of the next node 
(Depending on the Odd-Even Turn Model) Score Calculation 
0 out 
1 (out + next_1) / 2 
2 (2 x out + next_1 + next_2) / 4 
3 (Out + next_1 + next_2 + next_3) / 4 
The output buffer of the routed node is denoted as “out”. And the output buffers of 
the next node are denoted as “next_1”, “next_2” and “next_3”. 
For the hardware implementation, the function of the average will be implemented 
as a divider, as the average number is from 2 to 4 by the Odd-Even turn model. The 
 137 
of the minimal path is higher than the specific switching value, it indicates that the 
next node is not a hot spot and far from hotspots also. Therefore, the non-minimal 
path routing is unnecessary to be considered, even if the score of the non-minimal 
path is much higher than that of the minimal path. 
Fig. 5.5 shows an example while the score of the nominal path is higher than the 
specific switching value. (Assume the specific switching value equals to 4.) Although 
the score of the non-minimal path is higher than the minimal path, the adaptive 
decision unit still selects the minimal path because there are no hot spots on this path. 
5.3.4 Buffer Information Collector 
The score calculator calculates the scores for the adaptive decision unit depending 
on the traffic around the routed node. And thus, the buffer information collectors will 
indicate the buffer utilities of the routed node and neighbor nodes. For each neighbor 
node, the information of three output buffers should be collected for the score 
calculator. Therefore, six extra bits will be added to the link wires to transfer the 
buffer utilities for the buffer information collector. Every two bit indicates the three 
states of each output buffer, which are “increase”, “decrease” and “silence”. The 
buffer information collector will record the buffer utilities by the three states. 
5.4 QoS Guarantee Arbitration Mechanism 
The arbitration mechanism is applied after the routing algorithm. If there are two 
or more packets routed to the same next node, the arbiter must decide which packet 
can pass. A Quality-of-Service (QoS) guarantee arbiter is proposed as a dynamic 
arbitration mechanism. In the proposed arbitration architecture, it considers not only 
the number of waited cycles of packets, but also the QoS of the system. 
 139 
packet has the four levels of priority. This arbitration chooses the highest priority one 
if the waited cycle of each input packets does not exceed the value of WAIT (this 
variable is set the same value for every algorithms). If one of input packets exceed, 
the controller will choose the maximum waited cycle of input packets to prevent the 
deadlock problems. 
0 1 2 3 4 5 6 7
12.1
12.2
12.3
12.4
12.5
12.6
12.7
Specific switching Value
A
ve
ra
ge
 L
at
en
cy
 (C
yc
le
s)
(a)
Uniform with injection rate=30%
0 1 2 3 4 5 6 7
0
50
100
150
200
250
300
350
Specific switching Value
A
ve
ra
ge
 L
at
en
cy
 (C
yc
le
s)
(b)
Uniform with injection 
rate=30% and 6 hotspots
 
Fig. 5.7 The average latencies versus the specific switching values under the uniform patterns (a) 
without hotspots (b) with 6 hotspots. 
5.5 QoS Guarantee Arbitration Mechanism 
Different traffic patterns are applied for the simulation of the routing algorithms, 
such as uniform, uniform with hotspots, transpose and transpose with hotspots. In 
 141 
hotspots, it is obviously that the differences between the different specific switching 
values are not significant. According the analysis of the specific values as shown in 
Fig. 5.7 and Fig. 5.9, the performance will be better when the “specific switching 
value” is from 3 to 5. 
0 1 2 3 4 5 6 7
600
700
800
900
1000
1100
Specific switching Value
(b)
Transpose with injection 
rate=17% and 6 hotspots
A
ve
ra
ge
 L
at
en
cy
 (C
yc
le
s)
0 1 2 3 4 5 6 7
100
200
300
400
500
600
700
Transpose with injection rate=17%
Specific switching Value
(a)
A
ve
ra
ge
 L
at
en
cy
 (C
yc
le
s)
 
Fig. 5.9 The average latencies versus the specific switching values under the transpose patterns (a) 
without hotspots (b) with 6 hotspots. 
Before analysis the performance of the proposed routing algorithm, some 
compared routing algorithms will be introduced as follow first. 
 The XY-routing algorithm is a minimal algorithm. In XY-routing, every node 
chooses the next node concern the direction X of destination first. The packet 
 143 
 DyXY, dynamic XY routing algorithm, is proposed to provide adaptive routing 
by the stress value which is a parameter representing the congestion condition. 
Each packet only travels along the minimal paths. 
The comparisons between the adaptive congestion-aware routing algorithm and 
other approaches are shown in Fig. 5.10(a) and Fig. 5.10(b) under the uniform traffic 
patterns without/with 6 hotspots, respectively. The 6 hotspots have 50% ~80% 
injection rates. No whether the network has hotspots or not, the proposed routing 
algorithm can achieve average latency reduction. The transpose traffic patterns are 
utilized to compare the latency of the given algorithms. In addition, the comparisons 
between the routing algorithms are also simulated under the transpose traffic patterns 
as shown in Fig. 5.11. 
0 5 10 15 20 25 30
0
200
400
600
800
1000
1200
A
ve
ra
ge
 L
at
en
cy
 (C
yc
le
)
Injection Rate (%)
XY routing
Odd-Even
NoP
DyXy
This Work
Transpose
 
Fig. 5.11 The comparisons under the transpose patterns. 
The OE algorithm and NoP algorithm are not outstanding no whether in the 
uniform traffic patterns or the transpose traffic patterns. The DyXY performs better in 
the traffic pattern with hotspots, but not as well as the proposed algorithm, an adaptive 
congestion-aware routing algorithm. The proposed algorithm is effective in the pattern 
with hotspots at the proper injection rates. It is very practical in the real application 
 145 
adaptive routing architecture is superior to other routing algorithm no whether which 
traffic pattern is. If there are hotspots distributed in the mesh, the proposed 
architecture would become more useful. Therefore, the proposed algorithm is 
effective in the pattern with hotspots at the proper injection rates. It is very practical in 
the real applications of mesh OCINs. 
 147 
Reference 
References of Chapter 1 
[1.1] K. Ahmad and A. C. Begen, “IPTV and Video Networks in the 2015 
Timeframe: The Evolution to medianets,” IEEE Communications Magazine, 
Vol. 47, No. 9, Sept. 2009. 
[1.2] (2005-2009) International Technology Roadmap for Semiconductors. 
Semiconductor Industry Assoc. [Online]. Available: http://public.itrs.net 
[1.3] V. Chandra, A. Xu, H. Schmit and L. Pileggi, “An interconnect channel design 
methodology for high performance integrated circuits,” in Proceeding of 
Design Automation Test European Conference Exhibition, Vol. 2, pp. 
1138-1143, Mar. 2004. 
[1.4] P. Wodey, G. Caarroque, F. Baray, R. Hersemeule and J.P. Cousin, “LOTOS 
Code Generation for Model Checking of STBus Based SoC: The STBus 
Interconnect,” in Proceeding of ACM/IEEE International Conference on 
Formal Methods and Model for Co-design, pp. 204-213, 2003. 
[1.5] K.-C. Chang, J.-S. Shen and T.-F. Chen, “Evaluation and Design Trade-Offs 
between Circuit Switched and Packet Switched NOCs for Application-
-Specific SOCs,” in Proceeding of Design Automation Conference, pp. 
143-148, Jun. 2006. 
[1.6] V. Chandra, A. Xu, H. Schmit and L. Pileggi, “An interconnect channel design 
methodology for high performance integrated circuits,” in Proceeding of IEEE 
Design, Automation and Test in Europe Conference and Exhibition, Vol. 2, pp. 
1138 -1143, 2004. 
[1.7] J. Muttersbach, T. Villiger, and W. Fichtner, “Practical design of globally 
asynchronous locally synchronous systems,” in Proceeding of International 
Symposium on Advanced Research of Asynchronous Circuits and System, pp. 
52-59, Apr. 2000. 
[1.8] L. Benini and G. De Micheli, “Networks on Chips: A New SoC Paradigm,” 
IEEE Computer, Vol. 35, pp. 70-78, Jan. 2002. 
[1.9] R.I. Bahar, D. Hammerstrom, J. Harlow, W. H. Joyner Jr., C. Lau, D. 
Marculescu, A. Orailoglu, and M. Pedram, “Architectures for Silicon 
Nanoelectronics and Beyond,” IEEE Computer, Vol. 40, No. 1, pp. 25-33, Jan. 
2007. 
 149 
Proceeding of IEEE International Solid-State Circuits Conference, pp.96-97, 
Feb. 2010. 
[2.3] J. L. Shin, K. Tam, D. Huang, B. Petrick, H. Pham, C.-K. Hwang, H.-P. Li, A. 
Smith, T. Johnson, F. Schumacher, D. Greenhill, A. S. Leon, and A. Strong, “A 
40nm 16-core 128-thread CMT SPARC SoC processor,” in Proceeding of 
IEEE International Solid-State Circuits Conference, pp.98-99, Feb. 2010. 
[2.4] Y. Yuyama, M. Ito, Y. Kiyoshige, Y. Nitta, S. Matsui, O. Nishii, A. Hasegawa, 
M. Ishikawa, T. Yamada, J. Miyakoshi, K. Terada, T. Nojiri, M. Satoh, H. 
Mizuno, K. Uchiyama, Y. Wada, K. Kimura, H. Kasahara, and H. Maejima, “A 
45nm 37.3GOPS/W heterogeneous multi-core SoC,” in Proceeding of IEEE 
International Solid-State Circuits Conference, pp.100-101, Feb. 2010. 
[2.5] C. Johnson, D. H. Allen, J. Brown, S. Vanderwiel, R. Hoover, H. Achilles, 
C.-Y. Cher, G. A. May, H. Franke, J. Xenedis, and C. Basso, “A wire-speed 
powerTM processor: 2.3GHz 45nm SOI with 16 cores and 64 threads,” in 
Proceeding of IEEE International Solid-State Circuits Conference, pp.104-105, 
Feb. 2010. 
[2.6] L. Benini and G. De Micheli, “Networks on Chips: A New SoC Paradigm,” 
IEEE Computer, Vol. 35, pp. 70-78, Jan. 2002. 
[2.7] L. Benini and G. De Micheli, Network on Chips: Technology and Tools, 
Morgan Kaufmann, 2006. 
[2.8] W. J. Dally and B. Towles, Principles and Practices of Interconnection 
Networks, Morgan Kaufmann, 2004. 
[2.9] W. J. Dally and B. Towles, “Route Packets, Not Wires: On-Chip 
Interconnection Networks,” in Proceeding of Design Automation Conference, 
pp.684-689, 2001. 
[2.10] M. Drinic, D. Kirovski, S. Megerian, and M. Potkonjak, “Latency-Guided 
On-Chip Bus-Network Design,” IEEE Transactions on Computer-Aided 
Design of Integrated Circuits and Systems, Vol. 25,  No. 12, pp. 2663-2673, 
Dec. 2006. 
[2.11] V. Chandra, A. Xu, H. Schmit and L. Pileggi, “An interconnect channel design 
methodology for high performance integrated circuits,” in Proceeding of IEEE 
Design, Automation and Test in Europe Conference and Exhibition, Vol. 2, pp. 
1138-1143, 2004. 
[2.12] H. Lekatsas and J. Henkel, “ETAM++: extended transition activity measure 
for low power address bus designs,” in Proceeding of VLSI Design, pp. 
113-120, 2002. 
 151 
[2.25] S. Lee, C. Lee and H.-J. Lee, “A new multi-channel on-chip-bus architecture 
for system-on-chips,” in Proceeding of IEEE International SoC Conference, 
pp. 305-308, Sep. 2004. 
[2.26] M. Ariyamparambath, D. Bussaglia, B. Reinkemeier, T. Kogel and T. Kempf, 
“A highly efficient modeling style for heterogeneous bus architectures,” in 
Proceeding of IEEE International SoC Conference, pp. 83-87, Nov. 2003. 
[2.27] A. Ivanov and G. De-Micheli, “Guest Editors' Introduction: The 
Network-on-Chip Paradigm in Practice and Research,” IEEE Design and Test 
of Computers, Vol. 22, Issue 5,pp. 399-403, Sep. 2005. 
[2.28] E. Rijpkema, K.Goossens, A. Radulescu, J. Dielissen, J. Meerbergen, P. 
Wielage and E. Waterlander, “Trade offs in the design of a router with both 
guaranteed and best-effort services for networks on chip,” in Proceeding of 
IEEE Design, Automation and Test in Europe Conference and Exhibition, pp. 
350-355, 2003. 
[2.29] M. Dehyadgari, M. Nickray, A. Afzali-kusha and Z. Navabi, “A new protocol 
stack model for network on chip,” IEEE Computer Society Annual Symposium 
on Emerging VLSI Technologies and Architectures, 2006. 
[2.30] C. Nicopoulos, V. Narayanan and C.R. Das, Network-on-Chip Architectures, 
Springer, 2009. 
[2.31] T. Bjerregaard and S. Mahadevan, “A Survey of Research and Practices of 
Network-on-Chip,” ACM Computing Surveys, Vol. 38, Article 1, Mar. 2006. 
[2.32] I. Cidon, Tutorial in IEEE Network-on-Chip Symposium, May. 2009. 
[2.33] M. Tudruj and L. Masko, “Dynamic SMP Clusters with Communication on the 
Fly in NoC Technology for Very Fine Grain Computations”, in Proceeding of  
International Symposium on/Algorithms, Models and Tools for Parallel 
Computing on Heterogeneous Networks, pp. 97-104, 2004. 
[2.34] P.G. Paulin,C. Pilkington, E. Bensoudane, M. Langevin and D. Lyonnard, 
“Application of a multi-processor SoC platform to high-speed packet 
forwarding”, in Proceeding of Design, Automation and Test in Europe 
Conference and Exhibition, pp. 58-63, 2004. 
[2.35] A. Adriahantenaina, H. Charlery, A. Greiner, L. Mortiez, and C. A. Zeferino, 
“SPIN: a scalable, packet switched, on-chip micro-network,” in Proceeding of 
Design, Automation and Test in Europe Conference and Exhibition, pp. 70-73, 
2003. 
[2.36] W.J. Dally and B. Towles, “Router Packets, “Route Packets, Not Wires: 
On-Chip Interconnection Networks,” in Proceeding of Design Automation 
Conference, pp. 683-689, 2001. 
 153 
[2.48] K. Lee, S.-J. Lee and H.-J. Yoo, “Low-power network-on-chip for 
high-performance SoC design,” IEEE Transactions on Very Large Scale 
Integration (VLSI) Systems, Vol. 14, pp. 148-160, 2006. 
[2.49] Yole Development. (2007). 3DIC & TSV Report [Online]. 
http://www.yole.fr/pagesan/products/report_sample/3dic.pdf 
[2.50] V. F. Pavlidis and E. G. Friedman, “3-D Topologies for Networks-on-Chip,” in 
Proceeding of IEEE International SOC Conference, pp. 285-288, 2006 
[2.51] C. Addo-Quaye, “Thermal-aware mapping and placement for 3-D NoC 
designs,” in Proceeding of IEEE International SOC Conference, pp. 25-28, 
2005. 
[2.52] A. Y. Weldezion, M. Grange, D. Pamunuwa, Z. Lu, A. Jantsch, R. Weerasekera, 
and H. Tenhunen, “Scalability of Network-on-Chip Communication 
Architecture for 3-D Meshes,” in Proceeding of IEEE International 
Symposium on Networks-on-Chip, pp 114-123, 2009. 
[2.53] F. Li, C. Nicopoulos, T. Richardson, Y. Xie, V. Narayanan and M. Kandemir, 
“Design and Management of 3D Chip Multiprocessors Using 
Network-in-Memory,” in Proceeding of International Symposium on 
Computer Architecture, pp 130-141, 2006. 
[2.54] J. Duato, S. Yalamanchili and L. Ni, Interconnection Networks - An 
Engineering Approach, Morgan Kaufmann, 2003. 
[2.55] Y. Qian, Z. Lu and W. Dou, “Analysis of Worst-Case Delay Bounds for 
On-Chip Packet-Switching Networks,” IEEE Transactions on  
Computer-Aided Design of Integrated Circuits and Systems, Vol.29, No. 5, 
pp.802-815, May 2010. 
[2.56] I. Nousias and T. Arslan, “Wormhole Routing with Virtual Channels using 
Adaptive Rate Control for Network-on-Chip (NoC),” First NASA/ESA 
Conference on Adaptive Hardware and Systems (AHS), pp.420-423, 2006. 
[2.57] L.I. Tabada and P.U. Tagle, “Shared Buffer Approach in Fault Tolerant 
Networks,” International Conference on Computer Technology and 
Development, pp.235-239, 2009. 
[2.58] C. Xiao, M. Zhang, Y. Dou and Z. Zhao, “Dimensional Bubble Flow Control 
and Fully Adaptive Routing in the 2-D Mesh Network on Chip,” in Proceeding 
of International Conference on Embedded and Ubiquitous Computing, 
pp.353-358, 2008. 
[2.59] A. Pullini, F. Angiolini, D. Bertozzi, and L. Benini, “Fault Tolerance Overhead 
in Network-on-Chip Flow Control Schemes,” in Proceeding of Symposium on 
Integrated Circuits and Systems Design, pp.224-229, 2005. 
 155 
[2.71] S.R. Sridhara and Naresh R. Shanbhag, “Coding for System-on-Chip 
Networks: A Unified Framework,” IEEE Transactions on Very Large Scale 
Integration (VLSI) Systems, Vol. 13, No. 6, pp.655-667, Jun. 2005. 
[2.72] S. Chen and X. Liu, “A Low-Latency and Low-Power Hybrid Insertion 
Methodology for Global Interconnects in VDSM Designs,” in Proceeding of 
IEEE International Symposium on Networks-on-Chip, pp.75-82, 2007. 
[2.73] P. P. Pande, H. Zhu, A. Ganguly and C. Grecu, “Design of Low power & 
Reliable Networks on Chip through joint crosstalk avoidance and forward 
error correction coding,” in Proceeding of IEEE International Symposium on 
Defect and Fault-Tolerance in VLSI Systems, pp.466-476, 2006. 
[2.74] Y.-C. Lan, S.-H. Lo, Y.-C. Lin, Y.-H. Hu and S.-J. Chen, “BiNoC: A 
Bidirectional NoC Architecture with Dynamic Self-Reconfigurable Channel,” 
in Proceeding of IEEE International Symposium on Networks-on-Chip, 
pp.256-265, 2009. 
[2.75] S. H. Lo, Y. C. Lan, H. H. Yeh, W. C. Tsai, Y. H. Hu and S. J. Chen, “QoS 
Aware BiNoC Architecture,” in Proceeding of IEEE International Parallel & 
Distributed Processing Symposium, pp.1-10, Apr. 2010. 
[2.76] A. Ejlali and B. M. Al-Hashimi, “SEU-Hardened Energy Recovery Pipelined 
Interconnects for On-Chip Networks,” in Proceeding of IEEE International 
Symposium on Networks-on-Chip, pp 67-76, 2008. 
[2.77] B. Fu, D. Wolpert and P. Ampadu, “Lookahead-Based Adaptive Voltage 
Scheme for Energy-Efficient On-Chip Interconnect Links,” in Proceeding of 
IEEE International Symposium on Networks-on-Chip, pp 54-63, 2009. 
[2.78] S.-J. Lee, K. Kim, H. Kim, N. Cho, and H.-J. Yoo, “Adaptive network-on-chip 
with wave-front train serialization scheme,” in Proceeding of IEEE symposium 
on VLSI Circuits, pp. 104-107, 2005. 
[2.79] M. Kinsy, M. H. Cho, T. Wen, G. E. Suh, M. van Dijk and S. Devadas, 
“Application-Aware Deadlock-Free Oblivious Routing,” in Proceedings of the 
Int'l Symposium on Computer Architecture , pp. 208-219, June 2009. 
[2.80] M. Lis, M. H. Cho, K. S. Shim, and S. Devadas, “Path-Diverse Inorder 
Routing,” in Proceedings of the International Conference on Green Circuits 
and Systems, pp. 311-316, June 2010. 
[2.81] M. Majer, C. Bobda, A. Ahmadinia, and J. Teich, “Packet Routing in 
Dynamically Changing Networks on Chip,” in Proceeding of IEEE 
International Parallel and Distributed Processing Symposium, pp. 154b, 2005. 
[2.82] G. Michelogiannakis, D. Pnevmatikatos, and M. Katevenis, “Approaching 
Ideal NoC Latency with Pre-Configured Routes,” in Proceeding of IEEE 
International Symposium on Networks-on-Chip, pp. 153-162, 2007. 
 157 
[2.93] M. Palesi, K. Shashi, R. Holsmark, and V. Catania, “Exploiting 
Communication Concurrency for Efficient Deadlock Free Routing in 
Reconfigurable NoC Platforms,” IEEE International Parallel and Distributed 
Processing Symposium, pp. 1-8, 2007. 
[2.94] A. Giuseppe, C. Vincenzo, P. Maurizio, and P. Davide, “Neighbors-on-Path: A 
New Selection Strategy for On-Chip Networks,” in Proceeding of 
IEEE/ACM/IFIP Workshop Embedded Systems for Real Time Multimedia, pp. 
79-84, 2006. 
[2.95] M. Daneshtalab, A. Sobhani, A. Afzali-Kusha, O. Fatemi and Z. Navabi, “NoC 
Hot Spot minimization Using AntNet Dynamic Routing Algorithm,” in 
Proceeding of International Conference on Application-specific Systems, 
Architectures and Processors, pp. 33-38, 2006. 
[2.96] J. Flich, S. Rodrigo and J. Fuato, “An Efficient Implementation of Distributed 
Routing Algorithm for NoCs,” in Proceeding of IEEE International 
Symposium on Networks-on-Chip, pp 87-96, 2008. 
[2.97] X. Duan, D. Zhang and X. Sun, “Routing Scheme of an Irregular Mesh-Based 
NoC,” in Proceeding of International Conference on Networks Security, 
Wireless Communications and Trusted Computing, pp. 572-575, 2009. 
[2.98] R. Holsmark, S. Kumar, M. Palesi and A. Mejia, “HiRA: A Methodology for 
Deadlock Free Routing in Hierarchical Networks on Chip,” in Proceeding of 
IEEE International Symposium on Networks-on-Chip, pp 2-11, 2009. 
[2.99] Z. Song, G. Ma and D. Song, “Hierarchical Star: An Optimal NoC Topology 
for High-Performance SoC Design,” in Proceeding of International Multi- 
symposiums on Computer and Computational Sciences, pp 2-11, 2009. 
[2.100] A. Kohler and M. Radetzki, “Fault-Tolerant Architecture and Deflection 
Routing for Degradable NoC Switches,” in Proceeding of IEEE International 
Symposium on Networks-on-Chip, pp 22-31, 2009. 
[2.101] Y.-C. Lan, M.-C. Chen, A.-P. Su, Y.-H. Hu and S.-J. Chen, “Fluidity Concept 
for NoC: A Congestion Avoidance and Relief Routing Scheme,” in 
Proceeding of IEEE International SOC Conference, pp 65-70, Sept. 2008. 
[2.102] W. Song, D. Edwards, J.L. Nunez-Yanez and S. Dasgupta, “Adaptive 
Stochastic Routing in Fault-tolerant On-chip Networks,” in Proceeding of 
IEEE International Symposium on Networks-on-Chip, pp 32-37, 2009. 
[2.103] C.-H. Chao, K.-Y. Jheng, H.-Y. Wang, J.-C. Wu, and A.-Y. Wu, “Traffic- and 
Thermal-Aware Run-Time Thermal Management Scheme for 3D NoC 
Systems,” in Proceeding of IEEE International Symposium on 
Networks-on-Chip, pp 223-230, 2010. 
 159 
[2.115] H. Zimmer, S. Zink, T. Hollstein and M. Glesner, “Buffer-Architecture 
Exploration for Routers in a Hierarchical Network-on-Chip,” in Proceeding of 
IEEE International Parallel and Distributed Processing Symposium, pp.171a, 
2005. 
[2.116] C.A. Nicopoulos, D. Park, J. Kim, N. Vijaykrishnan, M.S. Yousif and C.R. 
Das, “ViChaR: A Dynamic Virtual Channel Regulator for Network-on-Chip 
Routers,” in Proceeding of IEEE/ACM International Proceeding 
Microarchitecture, pp. 333-346, 2006. 
[2.117] M.A.J. Jamali and A. Khademzadeh, “A New Method for Improving the 
Performance of Network on Chip using DAMQ Buffer Schemes,” in 
Proceeding of  International Conference Application Information 
Communication Technology, pp. 1-6, 2009. 
[2.118] J. Liu and J. G. Delgado-Frias, “A Shared Self-Compacting Buffer for 
Network-On-Chip Systems,” in Proceeding of IEEE International Midwest 
Symposium on Circuits and System, pp. 26-30, 2006. 
[2.119] J. Hu, U.Y. Ogras and R. Marculescu, “Application-specific buffer space 
allocation for networks-on-chip router design”, in Proceeding of IEEE/ACM 
International Conference of Computer Aided Design, pp. 354-361, 2004. 
[2.120] T.-C. Huang, U.Y. Ogras and R. Marculescu, “Virtual Channels Planning for 
Network-on-Chip,” in Proceeding of International Symposium on Quality 
Electronic Design, pp. 879-884, 2007. 
[2.121] N. Concer, L. Bononi, M. Soulie, R. Locatelli and L. P. Carloni, “The 
Connection-Then-Credit Flow Control Protocol for Heterogeneous Multicore 
System-on-Chip,” IEEE Transaction on Computer-Aided Design of Integrated 
Circuits and Systems, Vol. 29, No. 6, pp. 869-882, Jun. 2010. 
[2.122] A. Radulescu, J. Dielissen, K. Goossens, E. Rijpkema and P. Wielage, “An 
Efficient On-Chip Network Interface Offering Guaranteed Services, 
Shared-Memory Abstraction, and Flexible Network Configuration,” in 
Proceedings of the Design, Automation and Test in Europe Conference and 
Exhibition, pp. 1-6, Mar. 2004. 
[2.123] F. Clermidy, R. Lemaire, Y. Thonnart and P. Vivet, “A Communication and 
Configuration Controller for NoC based Reconfigurable Data Flow 
Architecture,” in Proceeding of ACM/IEEE International Symposium 
Networks-on-Chip, pp. 153-162, May. 2009. 
[2.124] Y.-L. Lai, S.-W. Yang, M.-H. Sheu, H.-Y. Tang and P.-Z. Huang, “A 
High-Speeed Network Interface Design for Packet-Based NoC,” in Proceeding 
of IEEE International Conference on Communication, Circuits and Systems, 
pp. 2667-2671, 2006. 
 161 
[2.136] T. Simunic, S.P. Boyd and P. Glynn, “Managing power consumption in 
networks on chips,” IEEE Transactions on Very Large Scale Integration (VLSI) 
Systems , Vol. 12, No. 1, pp.96-107, Jan. 2004. 
[2.137] C. Ciordas, A. Hansson, K. Goossens, and T. Basten, “A monitoringaware 
network-on-chip design flow,” in Proceeding of EUROMICRO Conference on 
Digital System Design, pp. 97-106, 2006. 
[2.138] J. M. Rabaey, “Scaling the power wall: Revisiting the low-power design 
rules,” Keynote speech at Symposium on SoC, Nov. 2007. 
[2.139] P. Bhojwani, J.-D. Lee and Rabi Mahapatra, "SAPP: Scalable and Adaptable 
Peak Power Management in NoCs,” in Proceeding of International 
Symposium on Low Power Electronic Devices, pp.340-345, 2007. 
[2.140] G. Liang, P. Liljeberg, E. Nigussie and H. Tenhunen, “A Review of Dynamic 
Power Management Methods in NoC under Emerging Design 
Considerations,” in Proceeding of IEEE NORACHIP, pp. 1-6, Nov. 2009. 
[2.141] G. Liang and A. Jantsch, “Adaptive power management for the on-chip 
communication network,” in Proceeding of EUROMICRO Conference on 
Digital System Design, pp. 649-656, 2006. 
[2.142] S. Moore, G. Taylor, R, Mullins and P. Robinson, “Point to Point GALS 
Interconnect,” in Proceeding of International Symposium on Asynchronous 
Circuits and Systems, pp. 69-75, April 2002. 
[2.143] J. Muttersbach, T. Villiger, and W. Fichtner, “Practical Design of 
Globally-Asynchronous Locally-Synchronous Systems,” in Proceeding of 
International Symposium on Asynchronous Circuits and Systems, pp.52-59,  
2000. 
[2.144] E. Amini, M. Najibi, and H. Pedram, “Globally Asynchronous Locally 
Synchronous Wrapper Circuit based on Clock Gating,” in Proceeding of 
Emerging VLSI Technologies and Architectures, 2006. 
[2.145] D. Rostislav, R. Ginosar, and Christos P. Sotiriou, “High Rate Data 
Synchronization in GALS SoCs,” IEEE Transactions on Very Large Scale 
Integration (VLSI) Systems, Vol. 14, No. 10, pp. 1063-1074, Oct. 2006. 
[2.146] J. Mekie, S. Chakraborty, and D.K. Sharma, “Evaluation of Pausible Clocking 
for Interfacing High Speed IP Cores in GALS Framework,” in Proceeding of 
International Conference on VLSI Design, pp. 559-564, 2004. 
[2.147] D. Kim, K. Kim, J.-H. Kim, S.-J. Lee and H.-J. Yoo, “Solutions for Real Chip 
Implementation Issues of NoC and Their Application to Memory-Centric 
NoC,” in Proceeding of IEEE International Symposium on Networks-on-Chip, 
pp. 30-39, 2007. 
 163 
[2.159] I. Miro-Panades, F. Clermidy, P. Vivet and A. Greiner, “Physical 
Implementation of the DSPIN Network-in-Chip in the FAUST Architecture,” 
in Proceeding of IEEE International Symposium on Networks-on-Chip, 
pp.139-148, 2008. 
References of Chapter 3 
[3.1] L. Benini and G. De Micheli, Network on Chips: Technology and Tools, 
Morgan Kaufmann, 2006. 
[3.2] W. J. Dally and B. Towles, Principles and Practices of Interconnection 
Networks, Morgan Kaufmann, 2004. 
[3.3] V. Raghunathan, M.B. Srivastava, and R.K. Gupta, “A Survey of Techniques 
for Energy Efficient On-Chip Communication,” in Proceeding of IEEE/ACM 
Design and Automation Conference, pp. 900–905, Jun. 2003. 
[3.4] R. Marculescu, U.Y. Ogras, L.-S. Peh, N.E. Jerger, and Y. Hoskote, 
“Outstanding Research Problems in NoC Design: System, Microarchitecture, 
and Circuit Perspectives,” IEEE Transactions on Computer-Aided Design of 
Integrated Circuits and Systems, Vol. 28, No. 1, pp. 3–21, Jan. 2009. 
[3.5] H. Lekatsas and J. Henkel, “ETAM++: Extended Transition Activity Measure 
for Low Power Address Bus Designs,” in Proceeding of VLSI Design 
Conference, pp. 113–120, 2002. 
[3.6] K.-H. Baek, K.-W. Kim, and S.M. Kang, “A Low Energy Encoding Technique 
for Reduction of Coupling Effects in SoC Interconnects,” in Proceeding of 
IEEE International Midwest Symposium on Circuits and System, Vol. 1, pp. 
80–83, 2000. 
[3.7] C.-G. Lyuh and T.-W. Kim, “Low Power Bus Encoding with Crosstalk Delay 
Elimination,” in Proceeding of International ASIC/SoC Conference, pp. 
389–393, 2002. 
[3.8] T. Lv, J. Henkel, H. Lekatsas, and W. Wolf, “An Adaptive Dictionary 
Encoding Scheme for SOC Data Buses,” in Proceedings of the Design, 
Automation and Test in Europe Conference and Exhibition, pp. 1059–1064, 
2002. 
[3.9] K.-M. Lee, S.-J. Lee, and H.-J. Yoo, “Low-Power Network-on-Chip for High 
Performance SoC Design,” IEEE Transactions on Very Large Scale 
Integration (VLSI) Systems, Vol. 14, No. 2, pp. 148–160, Feb. 2006. 
[3.10] J. Yang and R. Gupta, “FV Encoding for Low-Power Data I/O,” in Proceeding 
of IEEE International Symposium on Low Power Electronic and Design, pp. 
84–87, 2001. 
 165 
[3.22] F. Worm, P. Thiran, G. De Micheli, and P. Ienne, “Self-Calibrating 
Networks-On-Chip,” in Proceeding of IEEE International Symposium on 
Circuits and Systems, Vol. 3, pp. 2361–2364, May 2005. 
[3.23] M. Simone, M. Lajolo, and D. Bertozzi, “Variation Tolerant NoC Design by 
Means of Self-Calibrating Links,” in Proceedings of the Design, Automation 
and Test in Europe Conference and Exhibition, pp. 1402–1407, 2008. 
[3.24] R. Ho, “On-Chip Wires: Scaling and Efficiency,” Ph.D. dissertation, Stanford 
University, Aug. 2003. 
[3.25] R. Ho, K. Mai, and M. Horowitz, “Efficient On-Chip Global Interconnects,” in 
Proceedings of IEEE Symposium on VLSI Circuits, pp. 271–274, 2003. 
[3.26] P.P. Sotiriadis and A.P. Chandrakasan, “A Bus Energy Model for Deep 
Submicron Technology,” IEEE Transaction on Very Large Scale Integration 
(VLSI) Systems, Vol. 10, No. 3, pp.341–350, Jun. 2002. 
[3.27] K.-W. Kim, K.-H. Baek, N. Shanbhag, C.-L. Liu, and S.-M. Kang, 
“Coupling-Driven Signal Encoding Scheme for Low-Power Interface Design”, 
in Proceeding of International Conference on Computer-Aided Designs, pp. 
318–321, 2000. 
[3.28] P.P. Sotiriadis, “Interconnect Modeling and Optimization in Deep Submicron 
Technologies,” Ph.D. dissertation, Massachusetts Institute Technology, 
Cambridge, May 2002. 
[3.29] R. Pendurkar, A. Chatterjee, and Y. Zorian, “Switching Activity Generation 
with Automated BIST Synthesis for Performance Testing of Interconnects,” 
IEEE Transactions on Computer-Aided Design of Integrated Circuits and 
Systems, Vol. 20, No. 9, pp. 1143–1158, Sept. 2001. 
[3.30] K. Sekar and S. Dey, “LI-BIST: A Low-Cost Self-Test Scheme for SoC Logic 
Cores and Interconnects,” in Proceeding of IEEE VLSI Test Symposium, 
pp.417–422, 2002. 
[3.31] B. Xiaoliang, et al., “Self-Test Methodology for At-Speed Test of Crosstalk in 
Chip Interconnects,” in Proceeding of IEEE Design and Automation 
Conference, pp.619–624, Jun. 2000. 
[3.32] R. Tamhankar, S. Murali, S. Stergiou, A. Pullini, F. Angiolini, L. Benini, and G. 
De Micheli, “Timing-Error-Tolerant Network-on-Chip Design Methodology,” 
IEEE Transactions on Computer-Aided Design of Integrated Circuits and 
Systems, Vol. 26, No. 7, pp. 1297–1310, July 2007. 
[3.33] Yi Zhao, S. Dey, and Li Chen, “Double Sampling Data Checking Technique: 
An Online Testing Solution for Multi-Source Noise-Induced Errors on 
On-Chip Interconnects and Buses,” IEEE Transactions on Very Large Scale 
Integration (VLSI) Systems, Vol. 12, No.7, pp. 746–755, July 2004. 
 167 
IEEE/ACM International Conference of Computer-Aided Design, pp. 354–361, 
2004. 
[4.9] J. Hu, U.Y. Ogras and R. Marculescu, “System-Level Buffer Allocation for 
Application-Specific Network-on-Chip Router Design,” IEEE Transactions 
Computer-Aided Design of Integrated Circuits and Systems, Vol. 25, No. 12, 
pp. 2919–2933, Dec. 2007. 
[4.10] M. Coenen, S. Murali, A. Radulescu and K. Goossens, “A Buffer-Sizing 
Algorithm for Networks on Chip using TDMA and Credit-Based End-to-End 
Flow Control,” in Proceeding of IEEE International Conference on 
Hardware/Software Codesign and Syst. Synthesis, pp. 130–135, 2006. 
[4.11] W.J. Dally and B. Towles, “Route Packets, not Wires: On-Chip 
Interconnection Network,” in Proceeding of Design Automation Conference, 
pp. 684–689, Jun. 2001. 
[4.12] A. K. Kodi, A. Sarathy and A. Louri, “iDEAL: Inter-Router Dual-Function 
Energy and Area-Efficient Links for Network-on-Chip (NoC),” in Proceeding 
of International Symposium on Computer Architecture, pp. 241–250, 2008. 
[4.13] D. Kim, K. Kim, J.-Y. Kim, S. Lee and H.-J. Yoo, “Memory-Centric 
Network-on-Chip for Power Efficient Execution of Task-Level Pipeline on a 
Multi-Core Processor,” IET Computer & Digital Technology, Vol. 3, iss. 5, pp. 
513–524, 2009. 
[4.14] T.-C. Huang, U.Y. Ogras and R. Marculescu, “Virtual Channels Planning for 
Network-on-Chip,” in Proceeding of International Symposium on Quality 
Electronic Design, pp. 879–884, 2007. 
[4.15] J. Park, B.W. Okrafka, S. Vassiliadis and J. Delgado-Frias, “Deign and 
Evaluation of a DAMQ Multiprocessor Network with Self-Compacting 
Buffers,” in Proceedings of Supercomputing, pp. 713–722, 1994. 
[4.16] N. Ni, M. Pirvu and L. Bhuyan, “Circular Buffered Switch Design with 
Wormhole Routing and Virtual Channels,” in Proceeding of IEEE 
International Conference on Computer-Aided Design, pp. 466–473, 1998. 
[4.17] J. Liu and J. G. Delgado-Frias, “A Shared Self-Compacting Buffer for 
Network-On-Chip Systems,” in Proceeding of IEEE International Midwest 
Symposium on Circuits and Systems, pp. 26–30, 2006. 
[4.18] M.A.J. Jamali and A. Khademzadeh, “A New Method for Improving the 
Performance of Network on Chip using DAMQ Buffer Schemes,” in 
Proceeding of International Conference on Application of Information and 
Communication Technologies, pp. 1–6, 2009. 
[4.19] C.A. Nicopoulos, D. Park, J. Kim, N. Vijaykrishnan, M.S. Yousif and C.R. 
Das, “ViChaR: A Dynamic Virtual Channel Regulator for Network-on-Chip 
 169 
R.V.D. Wijngaart and T. Mattson,, “A 48-Core IA-32 Message-Passing 
Processor with DVFS in 45nm CMOS,” in Proceeding of IEEE International 
Solid-State Circuits Conference, pp. 108–110, Feb. 2010. 
[4.31] M. Made, F. Felicijan, A. Efthymiou, D. Edwards and L. Lavagno, 
“Asynchronous On-Chip Networks,” IET Proceeding of Computer & Digital 
Technology, Vol. 152, No. 2, pp. 273–283, 2005. 
[4.32] A. Lines, “Asynchronous interconnect for synchronous soc design,” IEEE 
Micro, Vol. 24, No. 1, pp.32–41, 2004. 
[4.33] R. Dobkin, R. Ginosar and C. P. Sotiriou, “High Rate Data Synchronization in 
GALS SoCs,” IEEE Transactions on Very Large Scale Integration (VLSI) 
Systems, Vol. 14, No. 10, pp. 1063–1074, Oct. 2006. 
[4.34] T. Chelcea and S. M. Nowick, “Robust Interfaces for Mixed-Timing systems,” 
IEEE Transactions on Very Large Scale Integration (VLSI) Systems, Vol. 12, 
No. 8, pp. 857–873, Aug. 2004. 
[4.35] I. Miro-Panades, F. Clermidy, P. Vivet and A. Greiner, “Physical 
Implementation of the DSPIN Network-on-Chip in the FAUST Architecture,” 
in Proceeding of IEEE International Symposium on Netwrok-on-Chip, pp. 
139–148, Apr. 2008. 
[4.36] P. Vivet, D. Lattard, F. Clermidy, E. Beigne, C. Bernard, Y. Durand, J. Durupt 
and D. Varreau, “FAUST, an Asynchronous Network-on-Chip based 
Architecture for Telecom Applications,” in Proceeding of IEEE International 
Symposium on Asynchronous Circuits and Systems, pp. 172–181, Mar. 2006. 
[4.37] E. Beignre, F. Clermidy, H. Lhermet, S. Miermont, Y. Thonnart, X.-T. Tran, A. 
Valentian, D. Varreau, P. Vivet, X. Popon and H. Lebreton, “An Asynchronous 
Power Aware and Adaptive NoC Based Circuit,” IEEE Journal of Solid-State 
Circuits, Vol. 44, No. 4, pp. 1167–1177, Apr. 2009. 
[4.38] E. Beignre and P. Vivet, “Design of On-chip and Off-chip Interfaces for a 
GALS NoC Architecture,” in Proceeding of IEEE International Symposium on 
Asynchronous Circuits and Systems, pp. 172–181, Mar. 2006. 
[4.39] M. Li, Q.-A. Zeng and W.-B. Jone, “DyXY – A Proximity Congestion-Aware 
Deadlock-Free Dynamic Routing Method for Network on Chip,” in 
Proceeding of Design Automation Conference, pp. 849-852, June, 2006. 
[4.40] P.-T. Huang and W. Hwang, “An Adaptive Congestion-Aware Routing 
Algorithm for Mesh Network-on-Chip Platform,” in Proceeding of IEEE 
System-on-Chip Conference, pp. 375-378, Sept. 2009. 
 171 
Proceeding of IEEE/ACM Design and Automation Conference, pp. 849-852, 
June, 2006. 
[5.13] M. Palesi, G. Longo, S. Signorino, R. Holsmark, S. Kumar and V. Catania, 
“Design of Bandwidth Aware and Congestion Avoiding Efficient Routing 
Algorithm for Network-on-Chip Platforms,” in Proceeding of IEEE/ACM 
International Symposium on Network-on-Chip, pp. 97-106, 2008. 
[5.14] J. Flich, A. Mejia, P. Lopez and J. Duato, “Region-Based Routing: An 
Efficient Routing Mechanism to Tackle Unreliable Hardware in Network on 
Chips,” in Proceeding of IEEE/ACM International Symposium on 
Network-on-Chip, pp. 183-194, 2007. 
[5.15] M. Koibuchi, H. Matsutani, H. Amano and T. M. Pinkston, “A Lightweight 
Fault-Tolerant Mechanism for Network-on-Chip,” in Proceeding of 
IEEE/ACM International Symposium on Network-on-Chip, pp. 13-22, 2008. 
[5.16] M.R. Aliabadi, A. Khademzadeh and A.M. Raiyat, “A Novel Reliable Routing 
Algorithm for Network on Chips,” in Proceeding of International Conference 
on Industrial Engineering and Engineering Management, pp.1375-1379, 2008. 
[5.17] G.M. Chiu, “The odd-even turn model for adaptive routing,” IEEE 
Transactions on Parallel and Distributed Systems, Vol. 11, No. 7, pp. 729-738, 
July 2000. 
[5.18] C. J. Glass and L.-M. Ni, “The turn model for adaptive routing,” Journal of 
the Association for Computing Machinery, pp. 874-902, Sept. 1994. 
  
提出的論文被安排在 8 月 3 日早上 10 點的 SRAM 主題發表。 
 
二、與會心得 
在會議中發表了本實驗室設計之超低電壓靜態記憶體單元，由於其特殊的設計，可
以和傳統 6T 靜態記憶體同樣使用位元交錯結構，此一技術在現在先進奈米製程下可
以有效避免 soft-error 的發生。與會學者也對我們設計的靜態存取記憶體相當注意，
也對我們提出了一些相當有趣的問題，藉由討論讓我們了解還有甚麼部分需要再深
入研究的。 
三、建議 
日本大部分的人都不太會講英文，所以去之前最好可以先學幾句常用會話，這樣會
方便很多。 
四、攜回資料名稱及內容 
[1] 議程表 
[2] 論文集 usb 隨身碟 
[3] 其他相關國際會議文宣 
五、其他 
無。 
 2
 4
二、與會心得 
現今生醫電子當紅時代裡，低電壓是基本要求，但是電路在低電壓情況下更容易受
PVT 的影響，所以本實驗室提出了一個具有 PVT compensation 電路產生準確的
clock，讓 function 能夠正確運作，更可以運用在所謂的生醫電子上，剛好也符合這
個會議的宗旨。而在同一個 section 裡，我也看到了其他方式來改善低電壓下會產生
的影響，對本實驗室是一大收穫! 
三、建議 
可能稍微懂一點點日文會比較好，從會議當中可以明顯感覺到日本人的英文水準不
是那麼高，出去買東西時服務人員懂英文的也比較少。如果是夏天去的話，要記得
戴帽子雨傘小型電風扇或是扇子可以散熱或是隔熱之類的東西，因為福岡實在太
熱，會議的禮物之一就是一把扇子，讓我們在福岡那幾天可以比較涼爽一點。 
四、攜回資料名稱及內容 
[1] 議程表 
[2] 論文集 USB 隨身碟 
[3] 其他相關國際會議文宣 
 
五、其他 
無。 
 
 
 
 
 
 
 
 
 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：黃威 計畫編號：99-2221-E-009-184- 
計畫名稱：應用於多視角立體視訊之多核心節能智慧超微型通訊系統研究--子計畫二：以記憶儲存為
重心之晶內資料傳輸應用於節能效益之多核心系統(I) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 1 0% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 2 2 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 4 75%  
博士生 3 3 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 5 2 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 2 2 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
