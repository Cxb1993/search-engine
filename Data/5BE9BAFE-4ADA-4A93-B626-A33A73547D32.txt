 2
algorithm. Here, three learning algorithms are considered; including the C4.5 decision tree 
(DT) learning, the back propagation artificial neural network (ANN) and the support vector 
machine (SVM). The proposed GA-based learning bias selection algorithm can decide 
optimal learning algorithm while simultaneously determine the optimal subset of system 
features and learning parameters under various performance criteria. With an optimal 
learning bias selection procedure, the experiment indicates that using SVM learning bias 
can achieve best generalization ability of the KBs, in terms of prediction accuracy of 
unseen data in throughput performance criterion. However, in mean cycle time and number 
of tardiness job performance criterion, decision tree learning bias mechanism can fulfill 
best generalization ability of the KBs. Furthermore, experiment results indicate hat the 
proposed approaches to build adaptive scheduling system improves generalization ability of 
the adaptive scheduling KBs than those obtained with classical machine learning-based 
approach, according to various performance criteria. 
 
Keywords: Adaptive Scheduling, Machine Learning, Decision Tree Learning, Artificial 
Neural Network, Support Vector Machine, Genetic Algorithm, Feature Selection. 
 
 
1. Introduction 
In the worldwide competitive market, world-class enterprises survival strongly 
depends on their ability to provide a wide range of low cost, high quality products within 
short cycle times. Hence, many manufacturing enterprises implement computer integrated 
manufacturing (CIM) and flexible manufacturing system (FMS) technology to achieve 
these goals. Many researchers (Cho and Wysk 1993, Liu and MacCarthy 1996, Park et al. 
1997, Su and Shiue 2003) believed that the development of an efficient scheduling 
mechanism is the core of FMS operational control functions. Thus, it becomes necessary to 
use the system resources in the most effective manner when building a sophisticated 
adaptive scheduling mechanism. 
Using the machine learning technologies to develop knowledge bases for adaptive 
 4
system. These additional constraints are called the learning biases (Chen et. al. 1999). 
Mitchell (1997) emphasizes the necessity of learning biases in order that an `inductive leap’ 
happens in learning systems. Without any learning biases, the performance of learning 
systems cannot surpass rote learning, which is the learning without generalization ability. 
Generalization is an important ability specific to DT learning that predicts unseen data with 
high accuracy, according to concepts learned from training examples (Mitchell 1997).  
So far there is little research that focuses on how to select an appropriate learning bias 
in the early developing stage of adaptive scheduling KBs. It usually assumes a set of proper 
features which are known in advance. Moreover, the learning algorithm to be applied for 
developing scheduling KBs is also fixed beforehand. Owing to the fact that the essential 
features and proper learning algorithm are uncertain in manufacturing systems, how to 
select an optimal learning bias to enhance KBs generalization ability is a crucial research 
issue for adaptive scheduling problem domain. 
Basically, there are three approaches to solve supervised classification problems in 
machine learning algorithms: the decision tree learning (Quinlan 1986, 1987, 1993), the 
back propagation neural network (Rumelhart et al. 1986) and the support vector machines 
(Vapnik, 1995). However, there is still little research that focuses on how to select proper 
learning biases in the early developing stage of scheduling KBs to enhance the performance 
of the resulting KBs. 
Based on above mention, developing an appropriate learning biases selection 
algorithm including machine leaning algorithm, and the subset of feature (manufacturing 
attribute) is highly desirable. 
2. Background information about machine learning algorithm 
2.1 Artificial neural networks 
ANNs as learning tools have demonstrated the ability to capture the general 
relationship between variables that are difficult or impossible to relate to each other 
analytically by learning, recalling, and generalizing from training patterns or data. In this 
(∑ ∑
∈
−=
p outputk
pkpk otp
E
layer   
2
2
1 )                                             （4） 
where tpk is the target of the kth output neuron when pth pattern is presented; opk is the 
actual output of the kth output neuron when pth pattern is presented. 
Once the basic network architecture has been established, the details of the training 
process and training regime can be determined. The initial weights of BP network are 
randomly set between [-0.5, +0.5]. For training, the standard back -propagation learning 
algorithm can be used. In this case, the best performance can be achieved by varying the 
learning and momentum coefficients during the training process. Based on the experimental 
experience, the value ofηcan be set  low （about 0.20 to 0.25）at the beginning. But by 
gradually increasing the value ofη, one can find a suitable learning rate. On the other hand, 
the value ofαcan be set high （about 0.90 to 0.95）at the beginning. But by gradually 
increasing the value ofα, one can find a proper momentum coefficient. 
The training set should be divided into two groups: a set for training the network, and 
a set for testing the performance of the trained network. The training and testing sets should 
be of approximately equal size if a sufficient number of samples are available. Otherwise, 
the training set should be given the priority In order to insure good generalization of 
training examples, the value of the root mean square (RMS) error defined in equation (5) 
for all patterns can be used as an index for the performance of the trained network. If RMS 
error reaches a stable condition or is less than some criterion, the network stops training 
and then uses testing data set to verify the trained networks. The network with the lowest 
RMS error can be selected as final neural network. 
( )∑ ∑
∈
−==
p outputk
pkpk otP
ERMS
layer   
2
2
1                                    （5） 
In the neural network approach applied to adaptive scheduling (Cho and Wysk 1993, 
Sun and Yih 1996, Arzi and Iaroslavitz1999), the neural network constructed for a set of 
training samples can suggest the preference indicator of all dispatching rules for given 
system status. A neural network as the rule selector suggests one rule based on current 
 6
In the tree creation phase, C4.5 is a greedy algorithm that grows the tree from the 
top-down, selecting at each node the attribute that best classifies the local training examples. 
This process continues until the tree perfectly classifies the training examples or until all 
attributes have been tested. Each instance attribute is evaluated using a statistical test 
(called information gain) to determine how well it classifies the training examples.  
In order to define information gain precisely, Quinlan (1986) define a measure used in 
information theory called entropy that characterizes the impurity of a collections of 
examples, is defined as: 
)(log)()( 2 cPcPSEntropy
Cc
∑
∈
−=                                       (6) 
where S is a collection of examples, C is the set of class labels and is the categories of 
dispatching rules used in this study, is the proportion of S belonging to class c. )(cP
The information gain is simply the expected reduction in entropy caused by 
partitioning the examples according to their features. Hence, the information gain is defined 
as:   
∑
∈
−=
fVv
vs SEntropyvPSEntropyfSGain )()()(),(                                (7) 
where f is a feature relative to a collection of examples S, Vf is the set of all possible values 
for feature f, and Sv is the subset of S for which feature f has value v (i.e., 
). P})(|{ vsfSsSv =∈= S(v) is the probability that S belong to feature value v. The 
probability is estimated from relative frequency of the training set. 
The best attribute is selected and used as the test at the root node of the tree. A 
descendant of the root node is then created for each possible value of this attribute, 
and the training examples are sorted to the appropriate descendant nodes. The entire 
process is then repeated using the training examples associated with each descendant 
node to select the best attribute to test at that point in the tree. 
The tree creation phase can lead to difficulties when the data includes noise. When DT 
classifies such data, the resulting tree tends to be very large. However, the number of 
branches reflects the probability of occurrence of particular data rather than underlying 
 8
In DTs learning approach, Shaw et al. (1992), Park et al.(1997), and Arzi and 
Iaroslavitz (2000) employed inductive inference algorithm to derive heuristics for selecting 
the appropriate dispatching rules in a manufacturing environment. This approach utilizes a 
set of training examples, each of which consists of a vector of system attribute values and 
corresponding dispatching rules that is generated by simulation to construct the scheduling 
knowledge in the form of a decision tree. By decision-tree learning, the suggested 
procedure provides the scheduler with the capability of selecting dispatching rules 
adaptively in dynamic changing manufacturing environment. 
2.3 Support vector machine 
Besides ANN and DT learning, support vector machine (SVM) is a new technique for 
data classification puts forward by the Vapnik (1995) and the team of AT＆T laboratory in 
1995. Support vector machine main theories comes from the Structural Risk Minimization 
of statistical learning theory (Burges 1998, Schőlkopf et. al. 1999). Support the vector 
machine mainly is using Separating Hyperplane to distinguish the data of two or several 
different Class that deal with the data mining problem of classification. The typical problem 
of classification, we usually will define some fundamental symbols as follow: 
ix ：It is represented as a vector consisting of individual components, which represent the N 
dimensions patterns or attributes. Where i denotes the number of data. 
miRx Ni ,...,1, =∈  
iy ：It is labels value or target value of the data. If the case is the binary classification 
situation with the data, having corresponding labels { 1}iy = ± .One class containing all 
the vectors with 1iy = +  and the other containing all the vector with . 1iy = −
miyi ,...,1},1{ =±∈  
  We will describe the construction of the SVM with linear and non-linear cases as 
follow. 
2.3.1 Linear SVM 
The SVM approach is to minimize the structural risk and then to find the one with the 
 10
transformed to the DL . 
)(
2
1
0,1
jij
m
ji
iji
m
i
iD xxyyL ⋅−= ∑∑
==
ααα                                          (15) 
Subject to: 0≥iα     and    mi ,...,1= 0
1
=∑
=
m
i
ii yα
According to Karush Kuhn-Tucker conditions (KKT) (Fletcher 1987), the constrained 
function satisfies the Karush Kuhn-Tucker conditions and sufficient for maximizing (15). 
So the decision function (16) is found. 
))(sgn()(
1
bxxyxf
m
i
jiii +⋅⋅= ∑
=
α                                            (16) 
When ，indicate that data the same as label of class “+1”；others is the same 
as label of class  “-1”. 
0)( >xf
2.3.2 Non-linear SVM 
It is impossible every data can find linear separating hyperplane in real data. Boser et. 
al. (1992) applied the data in the original input space can transform to higher-dimensional 
feature space via a mapping function Φ . The mapping function Φ  also called kernel 
function. The data in the feature space can use the method of linear SVM to solve 
non-linear problem and get better accuracy. 
In the dual lagrangian (15) the dataset ix  only appear inside an inner product. The 
function form the mapping ( )ixΦ  does not need to be known since it is implicitly defined 
by function (17). The data can become separable in feature space despite being non-linear 
problem in the original space by kernel function. 
))()((:),( jiji xxxxk Φ⋅Φ=                      (17) 
So non-linear SVM dual Lagrangian as follow: 
)(
2
1
0,1
jij
m
ji
iji
m
i
iD xxkyyL ⋅−= ∑∑
==
ααα                                         (18) 
Subject to: Ci ≤≤ α0     and    mi ,...,1= 0
1
=∑
=
m
i
ii yα
There are four kernel function in common use, including Linear kernel、Polynomial 
kernel、Radial Basis Function kernel (RBF) and Sigmoid kernel (Gunn 1998). The kernel 
function is choice by user but different Kernel function have different parameter should be 
 12
measures; is the subset of candidate system features; and  represents the optimal 
control strategy (dispatching rule) under such performance criteria and system status. 
SF *d
Three kinds of performance criteria are typically studied in scheduling problems: 
throughput-based, flow time-based, and due date-based.  
This research seeks to identify essential system attributes under various performance 
criteria. Therefore, all possible system attributes are exhaustively examined. Table 1 lists 
the 30 candidate attributes. 
Table 1 Candidate attributes used in this study 
System attribute 
ID 
Description 
1 Number of the jobs in the system  
2 The mean utilization of machines  
3 The standard deviation of machine utilization  
4 The mean utilization of load/unload stations 
5 The mean utilization of pallet buffers  
6 The mean utilization of AGVs  
7 The minimum imminent operation time of candidate jobs within 
the system  
8 The maximum imminent operation time of candidate jobs within 
the system  
9 The mean imminent operation time of candidate jobs within the 
system  
10 The standard deviation of the imminent operation time of 
candidate jobs within the system  
11 The minimum total processing time of candidate jobs within the 
system  
12 The maximum total processing time of candidate jobs within the 
system  
13 The mean total processing time of candidate jobs within the 
system  
14 The standard deviation of the total processing time of candidate 
jobs within the system  
15 The minimum remaining processing time of candidate jobs 
within the system  
16 The maximum remaining processing time of candidate jobs 
within the system  
 
 14
environments are unnecessary. Table 3 specifies six dispatching rules that were found to be 
effective in terms of the three performance criteria in this study. 
Table 3. Dispatching rules used in this study 
Dispatching Rule Description 
FIFO Select the job according to the rule of first in first out 
CR Select the job with the minimum ratio between time now until 
due-date and its remaining processing time 
    DS Select the job with minimum slack time  
    EDD Select the job with the earliest due-date  
SPT Select the job with the shortest processing time 
SRPT Select the job with the shortest remaining processing time 
4. The hybrid GA/ANN-based MPAS 
4.1. GA-based learning bias selection algorithm 
Figure 1 depicts the proposed approach that consists of two modules: a GA module 
and a machine learning module. A GA module is used to search and representative biases, 
to determine an optimal learning bias according to various performance measures, whereas 
the machine learning module generates training examples that were applied to evaluate the 
fitness of a given search and representative biases determined by the GA module. This 
GA-DT process iterates until a feature subset is obtained with the best performance in 
classifying unseen data, and the corresponding machine learning module is obtained with 
an optimal learning bias to generate the adaptive knowledge base. A GA-based learning 
bias selection algorithm as follows. 
 Step 1 While L ≠ Null do 
    Step 2 Choosing one machine learning algorithm L∈l  into machine learning module, 
and remove l from L.   
    Step 3 Initialize GA and machine learning algorithm l parameters.  
      Set . 0←g
Step 4 Initialize a random population of chromosomes . lgP
Step 5 Decode  into machine learning module, denoted as . lgP ML→lgP
 16
Choosing learning 
algorithm 
Initialized randomly 
generated chromosomes
Chosen learning algorithm 
& feature subset 
Fitness evaluation 
GA evolution 
process 
Updated 
population 
GA evolution 
T i i
No 
Yes 
Optimal feature subset, learning 
algorithm & learning parameter 
More 
learning 
algorithm 
Yes 
No 
Optimal 
learning bias 
Adaptive scheduling 
KBs 
Figure 1 GA-based approach learning bias selection algorithm 
GA Module 
Machine learning 
Module 
  
 
4.2 Chromosomal representation in machine learning algorithm 
(i) C4.5 DT algorithm  
In this study, the GA was utilized to optimize the subset of features and the pruning 
parameters of the C4.5 algorithm (the minimal number of examples represented at any branch of 
any feature-value test denoted as M parameter and the confidence level of pruning denoted as C 
parameter) used for training the adaptive scheduling KBs. Hence, every C4.5 algorithm was 
 18
 20
]
subset of features and the optimal number of hidden layers and neurons) and the parameters 
of the BPN algorithm (learning rate, momentum factor, and the range of initial weights) 
used for training the adaptive scheduling KBs. Hence, every BPN was coded as a binary 
string of finite length. A GA population consists of  binary strings. Each binary 
string , encodes an ANN architecture
N
[ Ncc ,1, ∈ [ ]Ncc ,1, ∈ . The binary string consists of 
 bits. The  bits are divided into seven parts. The first part of length  is used to 
encode what input neurons (system attributes) will be used. The second part of length  
is used to encode the number of hidden layers. The third and fourth parts of length  and 
 are used to encode the number of hidden neurons in hidden layers one and two, 
respectively. The initial learning rate, momentum factor, and connection weights range are 
encoded in the fifth, sixth, and seventh parts of length, ,  and , respectively. 
lc lc 1lc
2lc
3lc
4lc
5lc 6lc 7lc
Given the number of bits 2lc  = 1 in the second part of the string, the number of 
hidden layers can be 1 or 2. Given the number of bits, 643 == lclc , in the third and fourth 
part of the string, the maximum number of hidden neurons in each hidden layer is 
. According to a previous study (Shiue and Su 2003), as the number of hidden 
neurons was increased, the learning results were usually improved. However, once the 
number of hidden neurons exceeded sum of input and output neurons, it usually did not 
improve learning significantly, but it did lengthen the learning time. Hence, the number 63 
was considered an appropriate maximum number of hidden neurons to be set in the current 
investigation. Note that when the network has only one hidden layer (i.e., the second part of 
the string is ‘0’), the number of neurons in hidden layer 2 is zero (i.e., the fourth part of the 
string is ‘000000’). 
63126 =−
Given the number of bits 2765 === lclclc  in the fifth, sixth and seventh parts of the 
string, four different initial learning rates, momentum factors, and connection weights ranges can be 
encoded. Learning rate and momentum factor are both decreased gradually during the training 
process because, according to experience from the preliminary simulations, the network would not 
converge if learning rate and momentum factor were constant during the entire training process. 
Table 2 summarizes the binary encoding of the second to seventh parts of the string.  
L：the length of bit string  
5. Experiment results and discussion 
To guarantee valid results for prediction regarding unseen data sets, the training 
examples was further randomly partitioned into learning data set and unseen data set via a 
k-fold cross validation. In this procedure, the training examples are divided into k 
non-overlapping groups. We train an SVM using the first k-1 groups of training data and 
test the trained the trained SVM on the kth group. We repeat this procedure until each of 
the groups is used as an unseen data set once. In this study, we set k=10. This is a 
reasonable compromise considering the computational complexity of study case. Table 6, 
7, and 8 shows the results of optimal subset of system attributes, DT, BPN, and SVM 
parameters and the corresponding fitness value according to each performance criterion. 
Table 6. The results of selected system attributes and DT parameter for each 
performance criterion 
Performance 
criterion 
Optimal subset of system attributes ID C parameter M parameter Fitness value
TP {2,12,14,23} 25 2 0.7180 
MCT {10,15~16} 20 2 0.6880 
NT {15,21~23,26} 15 2 0.7080 
Table 7. The results of selected system attributes and BPN parameter for each 
performance criterion 
Performance 
criterion 
Optimal subset of 
system attributes 
ID 
Network 
configuration
Initial 
learning rate
Initial 
momentum 
rate 
Initial  
weights range
Fitness 
value 
TP {2,5~7,9~14,16, 
18, 22, 26, 28} 
15-32-6-9 0.5 0.4 [-0.5, +0.5] 0.6970 
MCT {2, 5~7, 9~14,16, 
18,22,26,28} 
14-56-9 0.3 0.9 [-0.5, +0.5] 0.6720 
NT {1~2,3~4,9~11,15
,18,23,28,30} 
12-5-53-9 0.3 0.4 [-1.0, +1.0] 0.6680 
 
 
 22
 24
[5] Burges, C. J. C., 1998, A tutorial on support vector machines for pattern recognition. 
Data Mining and Knowledge Discovery, Vol. 2, 955-974. 
[6] Chen, C. C., and Yih, Y., 1996, Identifying attributes for knowledge-based development 
in dynamic scheduling environments, International Journal of Production Research, 34, 
1739-1755. 
[7] Chen, C. C., Yih, Y. and Wu, Y. C., 1999, Auto-bias selection for learning-based 
scheduling systems, International Journal of Production Research, 37, 1987-2002. 
[8] Cho, H., and Wysk, R. A., 1993, A robust adaptive scheduler for an intelligent 
workstation controller. International Journal of Production Research, 31, 771-789. 
[9] Cho, H., and Wysk, R. A., 1995, An Intelligent workstation controller for 
computer-integrated manufacturing: problems and models. Journal of Manufacturing 
Systems, 14, 252-263. 
[10] Cover, T. M., 1974, The best two independent measurements are not the two best. 
IEEE Transactions on Systems, Man, and Cybernetics, 4, 116-117. 
[11] Fletcher, R., 1987, Practical Methods of Optimization. John Wiley and Sons, Inc., 2nd 
edition. 
[12] Goldberg, D. E., 1989, Genetic Algorithms in search, Optimization and Machine 
learning (Reading MA: Addison-Wesley). 
[13] Gunn, S. R., 1998, Support Vector Machines for Classification and Regression. 
University of Southampton, Technical Report. 
[14] Hsu, C. W., Chang, C. C., and Lin, C. J., 2003, A Practical Guide to Support Vector 
Classification, Available at http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf. 
[15] Kim, C. O., Min, H. S., and Yih, Y., 1998, Integration of inductive learning and neural 
networks for multi-objective FMS scheduling. International Journal of Production 
Research, 36, 2497-2509. 
