uncertain judgments are usually transformed or in-
tegrated into advice provided by the system after
some uncertainty calculation.
In real-world applications, multiple experts are
usually consulted for a specific decision problem.
When uncertainty judgments are elicited from mul-
tiple experts, we usually need to aggregate multi-
ple sources of expertise to arrive at a single rep-
resentation of uncertainty. There are a variety of
combination methods that researchers or practi-
tioners can use to aggregate probability distribu-
tions. These methods are often compartmental-
ized as matheamtical aggregation methods or as
behavioral methods (although, in practice, aggre-
gation might involve some aspects of each). Most
of the previous work in probability aggregation is
reviewed in Clemen (2008).
Linear opinion pool, a probability mixture or a
weighted average of experts’ distribution estima-
tions, is a widely used method for combining judg-
ments. The weighting scheme in opinion pooling
aims to model or reflect the performability of the
experts, because it seems to be desirable to rely
more on those experts who are perceived as better.
Thus, an intuitive approach is to use some questions
to evaluate expertise and determine corresponding
weights of individual experts. In the field of risk
analysis, Cooke (1991) derived a proper scoring rule
to measure the substantive and normative expertise
of experts, based on the Kullback-Leibler (K-L) dis-
tance or relative entropy. This entropy-based scor-
ing rule has been used to aggregate experts judg-
ments in many fields and for many real-world appli-
cations in the past 15 years, and is considered one
of the most widely used and sophisticated methods
for aggregating distribution estimates Cooke and
Goossens (2008).
Intuitively, when the number of seed variables
increases, the performance weighting scheme based
on K-L distance should become more powerful and
robust, because with more calibration variables for
evaluating expertise, better experts can be read-
ily identified. Thus, finding the threshold beyond
which Cooke’s performance weighting scheme will
be stable enough or be consistently outperform
other weighting schemes such as the equal weight
or the best expert approaches should be critical
for real-world applications. However, partially ow-
ing to the asymptotic proper scoring rule (which is
based on the limiting properties of the sampling dis-
tribution) used for model evaluation, and partially
owing to the fact that human experts will never be
perfectly calibrated, the K-L distance based cali-
bration score tends to decrease dramatically as the
number of seed questions increases, and is not a
robust measure for studying the trend or for iden-
tifying the threshold.
While eliciting, sifting, and aggregating experts’
uncertainty judgments is an important issue in ex-
pert and decision support systems, previous re-
search in expert systems has usually assumed that
all the judgments offered by various members in
an expert panel are with the same validity, or that
all experts in a panel have similar level of exper-
tise. Thus, past studies have been focused more
on designing new statistical or artificial intelligent
methods (such as neural network) to combine ex-
perts’ opinion, or on the use of generalized informa-
tion theory (including possibility theory, evidence
theory, fuzzy set theory and others) to represent
the qualitative and subjective nature of uncertain
events. Studies concentrating in evaluating and
sifting better experts are limited.
On the other hand, psychologists who study hu-
man judgment and decision making have been fo-
cused more on the quality of individual’s uncer-
tainty judgments. They defined calibration as the
consistency between a person’s probability judg-
ment and the relative frequency observed from re-
alizations of assessed events. Early studies in judg-
ment calibration revealed the overconfidence phe-
nomenon found in both lay persons’ and experts’
judgment, and some even claimed that overconfi-
dence is the most serious problem in human deci-
sion making. Later studies tried to identify the fac-
tors affecting the quality or calibration level of peo-
ple’s probability judgment. Recently, a few stud-
ies have examined whether overconfidence is a sta-
ble psychological phenomenon or primarily an ar-
tifact of the question selection or experiment de-
sign. These string of research concerns little about
how to combine judgments provided by multiple ex-
perts, not to mentioned the case when the level of
overconfidence or dependence is high.
A few studies in risk and decision analysis have
addressed the issue on identifying the condition un-
der which the performance weight based on K-L
distance or Cooke’s scoring rule can be more de-
sirable. Although Cooke (1991) pointed out that
the entropy-based score will be approximately chi-
square distributed only when the number of seed
questions is large enough, he also indicated that this
issue would not be very crucial because the main
purpose of this chi-square statistic is not for hy-
pothesis testing. Based on Cooke’s opinion, when
using K-L distance scoring rule is to differentiate
better-calibrated experts, 8 to 10 seed variables or
calibrated variables might be sufficient. However,
Clemen (2008) use a cross-validation procedure to
assess out-of-sample performance of various linear
opinion pooling models, no trend can be readily
identified. The relative performance between equal
weight and performance weight linear opinion pool-
ing methods does not significantly affected by num-
ber of seed variables. In a similar follow-up study,
2
bility intervals predicted by multiple experts might
be positively correlated, as discussed in Bier (2004).
2.1.2 Cooke’s Method
A conspicuous shortcoming of an opinion pool is its
inability to adequately model the performability of
the experts. Regarding the aggregate of experts’
judgments, it seems desirable to rely on those ex-
perts who are perceived as “better”. This philos-
ophy is reflected in the performance weights wi in
both linear and logarithmic opinion pools. Cooke
(1991) might be the most widely used and most
sophisticated method of assigning weight wi to dif-
ferent experts in the opinion pool.
For fulfilling the principles of reproducibility, ac-
countability, empirical control, neutrality, and fair-
ness discussed in Cooke (1991), Cooke’s method as-
signs the weight on the basis of the performance of
each expert in assessing seed variables, in which the
true values are now known and are selected to re-
semble as closely as possible the questions of inter-
est that the study was intended to address. Typi-
cally, Cooke’s method elicits the 5th, 50th, and 95th
precentiles. The scoring rule for eliciting expert’s
distribution and for assigning the relative weight
for each expert includes two components, calibra-
tion and informativeness, and the two components
are based on the idea of the Kullback-Leibler di-
vergence (or relative entropy) between the empir-
ical distribution and the ideal distribution (Cover
and Thomas, 2006). This method has been used in
dozens of expert opinion studies for a wide variety
of topics (see, for example, Cooke and Goossens,
2008; O’Hagan et al., 2006).
The Rationale of Cooke’s Classical Model
In Cooke’s model, the weights are determined from
experts’ calibration and information scores on seed
variables – a set of empirical calibration questions
(for which the values are known or will be known
by the decision makers but not by the experts).
Experts are downweighted if their relative per-
formance on these two measures is poor. Basi-
cally, calibration measures how the realization cor-
responds to an expert’s probability distribution as-
sessment, and the information measures how the
assessed distribution concentrated.
2.2 Data
We used a major subset of the database in Cooke
and Goossens (2008) to evaluate the aggregation
models. The original data set was compiled by
Cooke and contains 45 expert-judgment studies
(i.e., 45 different expert panels) conducted by
Cooke and colleagues at the Technical University
of Delft and elsewhere during the past 15 years.
Experts were requested to answer important ques-
tions regarding their domains of expertise (instead
of the almanac or common-knowledge questions
used in many psychological studies in decision mak-
ing). As was summarized in Cooke and Goossens
(2008), these expert studies can be categorized into
nine different sectors, including nuclear, chemical
and gas, occupational, health, banking, and others.
Readers who are interested in exploring more de-
tailed information regarding the individual studies
should refer to Cooke and Goossens (2008).
2.3 Jackknife Re-sampling
Positive results regarding using Kullback-Leibler
distance to evaluate and combine experts’ judg-
ments have been reported in Cooke and Goossens
(2000) and Cooke and Goossens (2008). However, a
thorough out-of-sample analysis on the robustness
of the method has not yet been conducted. The
threshold regarding minimum number of seed vari-
ables required for generating stable results remains
a question needing to be clearly answered.
Jackknifing, a nonparametric and computer-
intense method proposed by Quenouille (1949), is
thus used in our study to estimate the robustness
of Cooke’s weighting scheme derived from K-L dis-
tance. Jackknife is originally used to estimate the
bias or variance of a statistic, when a re-sample
of original observations is used to calculate it. The
basic idea behind the jackknife estimator lies in sys-
tematically recomputing the statistic estimate leav-
ing out some observations at a time. From this
new set of observations, the bias and variance for
the statistic can be estimated. The typical imple-
mentation or algorithm of the jackknife is to omit
the first observation from a sample of size n, com-
pute the statistic, and then repeat the procedure by
omitting the second, third, . . . observation. After n
iterations, the variance of these re-sampled statis-
tics is an estimate of the variance of the original
sample statistic.
The jackknife is a method of estimating sampling
variation of a statistic using a minimum of statis-
tical theory or distribution assumption (i.e., para-
metric assumption). Similar to bootstrap method
(see, for example, (Chermick, 2007)), the jackknife
can help us obtain trustworthy variance estimates
with ease. Furthermore, instead of computing the
statistic for a large collection of bootstrap samples
from original data, the jackknife is less computa-
tional demanding. The algorithm used in our eval-
uation is listed below.
1. For each elicitation study with n seed variables
2. Set i = 1
3. Exclude all judgments relating to ith seed vari-
able
4
Table 1: Mean distance from average weight obtained from jackknife re-sampling procedure
Expert panel # expert # seeds avg. dist.
Sulphur trioxide 4 7 0.340
Radioactive deposition (Delft) 4 22 0.115
Soil transfer 4 31 0.104
Option trading 5 34 0.157
Real estate 5 31 0.003
Radiation dosimetry 5 38 0.002
River dredging 6 8 0.000
Building temperature 6 48 0.207
Composite materials 6 12 0.383
Acrylonitrile 7 10 0.586
Wet deposition 7 19 0.204
Atmospheric dispersion (TNO) 7 36 0.261
Groundwater 7 10 0.449
Space debris 7 26 0.000
Early health effects 7 15 0.342
Movable barriers 8 14 0.262
Crane risk 8 10 0.284
Dry deposition 8 14 0.032
Atmospheric dispersion 8 23 0.252
Montserrat 10 8 0.000
Crane risk (flanges) 10 8 0.400
Atomspheric dispersion (Delft) 11 36 0.208
Water Pollution 11 9 0.190
Gas pipelines 16 14 0.013
Dike ring 17 47 0.002
infosec 13 10 0.499
mvoseeds 77 5 0.613
vesuvio 14 10 0.452
teide 17 10 0.389
volcrisk 45 10 0.255
carma 12 10 0.554
carmaga 6 10 0.451
ladders 7 10 0.221
Eunrcas 7 6 0.629
6
as the interpretation of any inferential results. We
believe nonlinear models are more appropriate for
us.
We don’t have any convincing reasons for choos-
ing one model other the other. We might want
to compare the non-nested models discussed above
(both of which approach the asymptote 0 as x →
∞. One model may provide a superior fit to our
data, and we would like to select that model.
Since the models are not nested, using the extra
sum of squares (due to the extra parameters) and
the corresponding F test (which is not an exact test
as is used in linear models) to conduct the analysis
will not be possible. In this case, the most impor-
tant statistical analysis which can be used for model
selection is the analysis of the residual. The model
with the smallest residual mean squares and the
most random-looking residuals should be chossen.
10 20 30 40
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
number of seed question
m
e
a
n
 d
ist
an
ce
exponential
rational
4 Conclusion and Discussion
In this project, we conducted an out-of-sample
evaluation (i.e., leave-one-out cross validation) and
Jackknife re-sampling technique to thoroughly eval-
uate the opinion pooling method based on cali-
bration questions and Kullback-Leibler distance.
We found that the performance score for Cooke’s
classical model dropped considerably in our out-
of-sample analysis. This part of the results indi-
cates that we might over-estimate the performance
of Cooke’s model if we use the same data to cal-
culate the performance weights of experts and the
performance scores of the aggregation models. It is
possible that removing one seed question each time
might “favor experts who assessed that seed badly
and hurts experts who assessed that seed well,” as
mentioned in Cooke (2008), and it is also possi-
ble that the cumulative adverse effect (on Cooke’s
performance-weight approach) might be so large
that performance weighting would appear in a poor
light; nevertheless, we believe that the previously
reported performance measures might at least be
slightly over-estimated. Researchers who further
explore this issue might do well to consider a bet-
ter procedure.
Even using the out-of-sample evaluation, though,
we found that Cooke’s performance-weight ap-
proach still outperforms both the equal-weight lin-
ear opinion pool and the best-expert approach
within a panel. Even if we consider only cali-
bration score (not the information score), Cooke’s
PW still marginally outperforms other approaches.
This shows that using seed questions (or calibra-
tion questions) to sift out better calibrated experts
can lead to superior aggregated distributions. The
Jackknife re-sampling computation indiciated that
the reliability of the performance weights based on
K-L distance increases exponentially as the number
of seed questions increases, further confirming the
stability of the model.
Overall, this project provides practical guidelines
for combining distribution forecasts, and contrasts
previous studies with guidelines reflect a broader
range of criteria. Furthermore, we verified most
of these methods by referring to a large real-world
data set; in this way, we can guarantee the applica-
bility of these guidelines.
5 研究成果自評
本研究的部份成果, 已投稿並已獲得期刊 Journal of
Modelling in Management (ABI Indexed) 接受刊登
(Lin and Cheng, 2009),另一正投稿於Applied Math-
ematics and Computation (SCI Indexed)。 並透過
國內外研討發與其他學者交換意見 (Lin and Cheng,
2008) (EI Indexed)。 整體而言, 本研究的執行成果已
大致達成當初計畫書中所預定的目標, 未來將持續進行
研究成果的發表,並且以此研究之成果為基礎,引申擴大
現有的模型, 並發展後續的研究。
本研究承蒙國科會經費補助, 謹此致謝。 在這一年的
研究期間, 所有參與計劃人員均已獲得下列之訓練:
1. 學習到決策理論與行為決策偏誤的基本理論與實務
上的實用。
2. 學習到機率擷取與整合之基本方法與理論。
3. 學習到如何使用 S-PLUS 與 R 等程式語言來撰寫
相關演算法的程式碼來處理不同的數學整合模型,
並進行交叉驗證分析與 Jackknife 重抽樣法分析。
4. 學習並瞭解如何將類似模型運用在不同的管理議
題, 並深入了解相關因子對不同整合模型的影響,
並據以研擬適當的決策。
5. 學習貝氏整合模型的建構與精煉。
8
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                           97年 12月 15 日 
報告人姓名  林希偉 
 
服務機構
及職稱 
元智大學 
企業管理學系 
助理教授 
     時間 
會議 
     地點 
2008/12/8-2008/12/11 
Singapore 
計畫編號 
計畫名稱
NSC97 - 2221 - H - 155 - 046  
整合專家機率分配判斷的貝氏校
準模型之設計(I) 
會議 
名稱 
 (中文) 2008年工業工程與工程管理國際研討會 
 (英文) 2008 International Conference on Industrial Engineering and 
Engineering Management (IEEM2008) 
發表 
論文 
題目 
 (中文) 使用古典模型可以篩選與整合出較佳的機率區間估計嗎？ 
 (英文) Can Cooke’s Model Sift Out Better Experts and Produce 
Well-Calibrated Aggregated Probabilities? 
 
一、參加會議經過 
2008年工業工程與工程管理國際研討會(IEEM2008)於新加坡 Furama Riverfront Hotel舉
辦，提供學術單位與產業界一個工業工程與工程管理理論及應用技術的交流平台，此研
討會為當前工業工程學界重要的研討會之一。本次研討會亦有數百名來自世界各國的學
界專家與產業界人士參加。 
 
除了安排三場 Keynote presentations 「The Nexus Between Creativity and Innovation at the 
National Library Board」、「Process Mining and RFID」與「Industrial Engineering and 
Challenges in the Global Economy」之外，從 12/09上午到 12/11下午則為 parallel session
的時段。個人的研究被安排在 12/09 下午屬於 Decision Analysis and Methods的場次中
發表。這個研究主要在於探討在專家不確性判斷（機率判斷）的應用中，當存在複數專
家時，使用 Kullback-Leibler distance 來找出判斷的校準度與資訊度較佳的專家並據以
整合的可行性。報告結束後亦與多位來自他國的與會者針對本研究議題進行一些互動與
意見的交換。 
 
本人同時在這次的研討會中被安排擔任二個 session 的 session chair，除了透過擔任 
session chair 的機會與其他學者互動之外，也在不同的場次中試著吸收工程管理相關領
域一些較新的方法與應用，參與本次會議可謂受益良多。 
 
表 Y04 
