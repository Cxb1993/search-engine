Keywords: video content analysis, video 
abstraction, video summarization, video 
skimming, shot detection, scene generation, 
event detection, video tempo analysis, MFCC, 
audio processing and audio shot detection 
Motivation and Goal 
Due to the advance of modern multimedia 
technologies such as high storage equipment 
and high performance video streaming tech-
niques, more digital video has become avail-
able to serve commercial, entertainment, edu-
cation, and many other different purposes. To 
effectively manage these contents, it is impor-
tant to develop automatic techniques for video 
indexing, searching, browsing, editing, summa-
rizing, skimming, and abstracting. 
 
This project attempts to organize the raw video 
data into systematic structures so that its se-
mantic content can be effectively represented 
for future usage. Although video content un-
derstanding is an easy task for human beings, it 
is difficult for computers to have the same de-
gree of semantic understanding and knowledge 
extraction. As a result, previous work has fo-
cused more on content description based on 
low-level features such as motion, shape and 
color descriptors in MPEG-7. Furthermore, 
techniques such as shot change detection, key-
frame selection, face recognition, object track-
ing, speech recognition, and audio segmenta-
tion have been developed for low-level video 
content analysis. Although the low-level im-
age/video processing methodology is relatively 
mature and many reliable algorithms have been 
proposed, the way to bridge the gap between 
low-level syntactic features and high-level se-
mantic meanings is generally known as an open 
and challenging problem. 
 
Proposed Methods 
1. Fast Shot Change Detection (快速場景切
換偵測演算法) 
 
Shot change detection is a basic work in video 
analysis. In order to find the interesting scenes, 
we should do video segmentation at the first. 
Shot change detection is a critical problem in 
video segmentation.  
 
 
In the traditional methods, shot change detec-
tion can be achieved by histogram difference of 
two successive frames. This method accumu-
lates each bin’s difference of the histogram of 
Y component between two successive frames; 
shot change occurred if the difference is larger 
than the predefine threshold. However, this 
method is time-consuming in the histogram 
calculation ( ) and it 
is impractical for the hardware implementation 
and the real-time applications.  
( )2_Time Complexity O n=
 
In order to reduce time complexity and make it 
possible for hardware implementation, we pro-
posed a multi-step fast shot change detection 
algorithm. Because shot change means two 
shots in different situation, it must have differ-
ent sense of sight. Therefore, it is possible to do 
shot change detection in partial information of 
frames. Hence, we only consider partial infor-
mation of frames in our proposed method. At 
the first, we predefine four 1D patterns shown 
in Fig. 1. In the first step, we extract the pixels 
from two successive frames with predefined 
top-right to bottom-left 1D patterns; continu-
ously extracting the luminance level from 1D 
pattern signal to calculate pixel difference in Eq. 
1 of luminance level signal to represent the 
difference of these frames. Then, we compare 
the difference to determine whether shot 
change occurs when the difference is larger 
than the given threshold values. 
1 , (n n nD Y Y−= − 1)∑          
In this step, we can avoid to pay large compu-
tational complexity on calculating non-shot 
change frames. If shot change detected in the 
first step, we continue to proceed with the sec-
ond step. In the second step, we extract the pix-
els from two successive frames with the prede-
fined top-right to bottom-left and bottom-right 
to top-left 1D patterns; continuously extracting 
the luminance level from 1D pattern signal to 
calculate variation of luminance level signal to 
represent the difference of these frames. Then, 
we compare the difference to determine 
whether shot change occurs when the differ-
ence is larger than the given thresholds. If shot 
change detected in the second step, we continue 
to proceed with the third step. In the third step, 
we extract the pixels from two successive 
frames with the predefined top-right to bot-
tom-left, bottom-right to top-left and horizontal 
1D patterns; continuously extracting the lumi-
 2
3. Audio Shot Detection (音訊場景切換偵測
演算法) 
For shot change detection, only visual feature 
to determine is not enough. If we only use vis-
ual feature, we cannot distinguish the two 
scenes with the similar semantic concept. We 
should combine visual and acoustic features to 
achieve the best result. 
There are often two types of audio shots in 
videos: speech and music environment sound. 
To classify these two shots, we use short-time 
energy, zero-crossing rate, and MFCC 
(Mel-Frequency Cepstral Coefficients) as our 
features. First, we use Gaussian Mixture Model 
(GMM) to train each type of audio shots. Then 
we use GMM to classify each shot. Finally, 
post-processing is introduced to enhance the 
result. After all these steps, we will get an au-
dio shot sequence. Figure 4 shows the flow-
chart of the proposed method. 
 
 
 
Figure 4: Flowchart of the proposed method 
 
Feature Extraction 
The short-time energy and the zero-crossing 
rate are commonly used in audio analysis. In 
order to get more accurate results, we add 
MFCC coefficients into our features, so the to-
tal dimension of our feature space is 
14-Dimensional (14-D). Follows are how we 
extract these features: 
 
 
a. Short-Time Energy 
The Short-Time Energy function is defined as 
follows 
21( ) ( ), (2)
m
stEn n a m
N
= ∑  
where  is the frame number,  is the total 
number of data in one frame,  is the time 
index, and  is the amplitude at time . 
n N
m
a m
 
 
b. Zero-Crossing Rate 
 
The Zero-Crossing Rate function is defined as 
follows 
1 sgn[ ( )] sgn[ ( 1)] , (3)
2 m
zcr a m a m= − −∑  
where 
1, ( ) 0.
sgn[ ( )]
1, ( ) 0.
a m
a m
a m
≥⎧= ⎨− <⎩
 
c. Mel-Frequency Cepstral Coefficients 
 
The MFCC function is defined as follows: 
~
1
1( ) ( )cos( ( 0.5)), (4)
N
s
l
C n Y l l s
N N
π
=
= −∑  
where  is the th coefficients of the 
th frame, and 
~
 is the th filter bank, 
which is the representative of the critical band 
in human auditory system.  is the en-
ergy band and other twelve coefficients s , 
=1,2,⋯,12 are generally adopted in audio 
signal analysis. 
( )sC n s
n ( )Y l l
)(0 nC
( )C n
s
Gaussian Mixture Model Classification 
Because GMM has the advantage which has the 
capability to describe complex distributions, we 
choose it to classify different types of shot. 
Here, we have two parts: one is training part 
and the other is the classification part. 
 
a. GMM Training Model 
 
First, we train models for the two audio shot 
types through a manually classified audio shot 
database in advance, as shown in Fig. 5. This 
figure shows the distribution of features pro-
jected from the 14-D feature space into 2-D 
feature space. From this figure, even though it 
only shows 2-D feature space distribution we 
still can clearly see the distinction of these two 
types of shot. Since we have the training mod-
els, next we can classify shots by using these 
training models.  
 4
ences between two neighboring frames, if the 
difference D is greater than the predefined 
threshold, then the shot change has occurred 
255
1
0
( ) ( )
( ) , (6)
i i
h
H h H h
D i
N
+
=
−
=
∑
 
where Hi represents the luminance histogram of 
frame i and N is the total pixel number in one 
frame. An adaptive threshold via the short-term 
variance of the histogram difference is used to 
detect the hard-cut shot change. After obtaining 
the individual shots, we select one frame as 
keyframe from each shot for later processes in 
order to save computational complexity. 
 
Low-level feature extraction 
Feature selection must be considered carefully; 
more features are not necessary better. Selected 
features should be transformed with regard to 
the specific domain knowledge. Therefore, we 
select features related to tempo concept; 
meanwhile, the computational complexity is 
also considered. As for visual analysis, we cal-
culate the motion vector average M and the lu-
minance histogram difference average L in a 
shot. The M and L are defined as follows: 
1
1 0
, , (7
1
r B
i
n i b
i b
mM m mv
r
−
= =
= =−∑ ∑ )
JJJK
 
1
1
, ( ), (8)
1
r
i
n i
i
lL l D i
r
−
=
= =−∑  
where r, n and mvb represent the frame number 
in a shot, shot index and motion vector of the 
macroblock b, respectively; mi and li represent 
the motion vector sum of all macroblock B at 
frame i and the luminance difference between 
frames i and i+1, respectively. The higher the 
shot stability, the lower its corresponding value 
Ln is.  These two features allow one to under-
stand the meaning of motion activity and shot 
stability.  This is because interesting segments 
also happen when motion activity and shot sta-
bility are both high. As for audio analysis, we 
calculate energy En and zero-crossing rate 
ZCRn of audio signal in the shot n in order to 
understand audio energy level and noise level. 
The En and ZCRn are defined as follows: 
0
,
aN
j j
n
j a
R R
E
N=
×=∑ (9)  
{ } { }1 1
1
1 , (10)
2
aN
s
n j j
ja
fZCR sign R sign R
N
−
+
=
= −∑  
  { }
1, 0
0, 0 , (11)
1,
j
j j
if R
sign R if R
otherwise
>⎧⎪= =⎨⎪−⎩
 
where Rj, Na and fs represent the audio signal, 
total audio signal sample in a shot and sam-
pling rate, respectively. These two features are 
used to detect percussive sound. This is be-
cause interesting shots usually contain percus-
sive sound effect and we use audio energy and 
zero-crossing rate to measure it. These four ba-
sic features related to tempo concept are em-
ployed to measure visual tempo and audio 
tempo of shots in a video. Selected features, in 
general, should correspond to specific domain 
knowledge. Results of low-level feature extrac-
tion are shown in Fig. 8 
 
 
 
Figure 8: Low-level feature extraction (a) audio 
energy (b) shot number (c) motion activity 
 
Automatic threshold setting is usually an im-
portant issue in video content analysis. Here, 
we determine adaptive threshold values for in-
teresting shot selection according to shots 
themselves obtained from the preprocessing 
step, avoiding threshold setting problems for 
different genres of videos. Motion activity and 
shot stability will be affected significantly by 
camera motion, and, therefore, making it tough 
to determine the appropriate thresholds for mo-
tion activity and shot stability for interesting 
shot selection. We consider jointly these two 
 6
 
Figure 9: Tempo curves of the last part of the 
film “Kung Fu Hustle” 
 
5. Multimodal Data Fusion (多型態特徵值融
合分析演算法) 
 
Figure 10 shows the audio tempo, visual tempo, 
and synthesized audiovisual tempo curves of 
the last half of the film “Kung Fu Hustle.”  
Between 62 mins and 75 mins, the volume of 
audio tempo curves is similar to that that visual 
tempo curve. The volume of audio tempo curve, 
however, is much stronger than that of visual 
tempo between 75 mins and 85 mins. The sim-
plest way to fuse visual and audio tempo in-
formation is to synthesize the two tempo curves 
with equal weight. The fusion model can be 
represented simply as 
( ) ( ) (1 ) ( ), (17)s vP t P t P tαα α= + −  
where Pv(t) and Pa(t) represent visual and au-
dio tempo curves, respectively, and α is the 
weighting factor and equal to 0.5 in this case.  
Alternatively, audio energy Fae, shot frequency 
Fsf, motion activity Fma –– results of the 
low-level feature extraction stage shown in Fig. 
5 –– are also helpful to synthesize tempo 
curves.  For example, we can redefine Eq. 17 
as follows 
( ) ( ) ( ), (18)s aP t t P tβ′ =  
{ }
( )( ) , (19)
max
ae
ae
F tt
F
β =  
where β  is the weighting factor determined 
by audio energy information Fae.  Here, Fae 
can be replaced by shot frequency Fsf or motion 
activity Fma for different needs.   
 
Figure 10: Audio tempo, visual tempo, and 
synthesized audiovisual tempo curves of the 
last half of the film “Kung Fu Hustle” 
 
Tempo curve fusion, of course, can be achieved 
with more complex processes of, for instance, 
visual-based constraints. We first process vis-
ual tempo analysis to obtain interesting shots.  
Then, we re-analyze these interesting shots 
with audio-based constraints to screen out ir-
relevant shots. Finally, the remaining shots will 
form a tempo curve, shown in Fig. 10, and this 
tempo curve Ps''(t) can be defined as 
( , ), ,
( ) ( ), (20)
E ZCR
s
t t t E T ZCR T
P t S
τ ττ
τ
∈ +∆ ≥ >
″ ′′= ∑  
where Sv(τ ) is the number of interesting shots 
selected by visual features such as motion ac-
tivity and shot stability around the interval time 
τ .  
 
As Fig. 10 illustrates, a complete scene with the 
cinematic structure of a video can easily be 
found between 56 mins and 75 mins.  The 
way to select automatically fusion models for 
different genres of videos is still an open prob-
lem. Video can be roughly classified into gen-
res such as drama, comedy, action, horror, 
animation, etc.. We can obtain some informa-
tion from the preprocessing step as well as the 
low-level feature extraction step to select the 
best fusion model for the input video.  For 
example, color analysis is used for the general 
classification of movies and sports genres.  
Furthermore, EPG (Electronic Program Guide) 
can provide metadata information regarding the 
genres of recorded videos. 
 
Results and Discussions 
 
In this project, we discussed the characteristics 
 8
essing Magazine (SCI&EI), vol. 23, no. 2, pp. 
79-89, March 2006.  
[8] H. Sundaram, L. Xie, S. F. Chang, “A util-
ity framework for the automatic generation of 
audio-visual skims, in Proceedings of the Tenth 
Association for Computing Machinery Interna-
tional Conference on Multimedia, pp. 189–198, 
2002. 
[9] S. F. Chang, “The Holy Grail of con-
tent-based media analysis,” IEEE Multimedia, 
pp. 6–10, 2002. 
[10] Chia-Hung Yeh, Shih-Hung Lee and C. -C. 
Jay Kuo, “Content-based video analysis for 
knowledge discovery,” Handbook of Pattern 
Recognition and Computer Vision 3th Edition 
Version, Editor by Prof. C. H. Chen and Prof. 
P.S.P. Wang, World Scientific Publishing Co. 
ISBN:  981-256-105-6., 2005.  
[11] B. S. Manjunath, T. Huang, A. M. Teklap 
and H. J. Zhang, “Guest editorial introduction 
to the special issue on image and video proc-
essing for digital libraries,” IEEE Transactions 
on Image Processing, vol. 9, pp. 1-2, 2000. 
[12] A. Hampapur, R. Jain and T. Weymouth, 
“Digital video segmentation,” Association for 
Computing Machinery Multimedia, pp. 
357-364, 1994. 
[13] H. J. Zhang, J. Wu, D. Zhong and S. W. 
Smoliar, “An integrated system for con-
tent-based video retrieval and browsing,” Pat-
tern Recognition, vol. 30, no. 4, pp. 62-72, 
1994. 
[14] S. F. Chang, J. R. Smith, M. Beigi and A. 
Benitez, “Visual information retrieval from 
large distributed on-line repositories,” in Pro-
ceedings of Association for Computing Ma-
chinery Communication, vol. 40, no. 12, pp. 
63–71, 1997. 
[15] R. Lienhart, S. Pfeiffer and W. Effelsberg, 
“Video abstracting,” Communications of the 
Association for Computing Machinery, pp. 
55–62, 1997. 
[16] Y. Li, T. Zhang and DTretter, “An over-
view of video abstraction techniques,” HP 
Laboratories Technical Report, HPL-2001-191, 
2001. 
[17] H. Jiang, T. Lin and H. J. Zhang, “Video 
segmentation with the assistance of audio con-
tent analysis,” in Proceedings of IEEE Interna-
tional Conference on Multimedia and Expo, vol. 
3, pp. 1507-1510, 2000. 
[18] J. Guo and C.-C. Jay Kuo, “Semantic 
video object segmentation for content-based 
multimedia applications,” Kluwer Academic 
Publishers, 2001. 
[19] W. Zhou, S. Dao and C.-C. Jay Kuo, 
“On-line knowledge-based and rule-based clas-
sification system for video indexing and dis-
semination,” Information Systems, vol. 27, pp. 
559-586, 2002. 
[20] A. Rosenfeld, D. Doermann, and D. De-
menthon, “Video mining,” Kluwer Academic 
Publishers, 2003. 
[21] S. W. Smoliar and H. J. Zhang, “Con-
tent-based video indexing and retrieval,” IEEE 
Multimedia, pp. 62-72, 1994. 
[22] M. Fichner, H. Sawhney, W. Niblack, J. 
Ashley, Q. Huang, B. Dom, M. Gorkani, J. 
Hafiner, D. Lee, D. Petkovic, D. Steele and P. 
Yanker “Query by image and video content: the 
QBIC system,” IEEE Computer, vol. 289, pp. 
23-32, 1995. 
[23] C.-W. Ngo, Y.-F. Ma and H.-J. Zhang, 
“Video summarization and scene detection by 
graph modeling ,” IEEE Transactions on Cir-
cuits and Systems for Video Technology, vol. 
15, no. 2, pp. 296-305, 2005. 
[24] M. R. Naphade, I. V. Kozintsev and T. S. 
Huang, “A factor graph framework for seman-
tics video indexing,” IEEE Transactions on 
Circuits and Systems for Video Technology, vol. 
12, no. 1, pp. 40-52, 2002. 
[25] M. Smith and T. Kanade, “Video skim-
ming and characterization through the combi-
nation of image and language understanding 
techniques,” in Proceedings of the IEEE Com-
puter Vision and Pattern Recognition, pp. 
775-781, 1997. 
[26] U. Gargi, R. Kasturi and S. H. Strayer, 
“Performance characterization of 
video-shot-change detection methods,” IEEE 
Transactions on Circuits and Systems for Video 
Technology, vol. 10, no. 1, 2000. 
[27] Y. Li, “Content-based video analysis, in-
dexing and representation using multimodal 
information,” Ph.D. dissertation USC 2002. 
[28] H. J. Zhang, A. Kankanhalli, and S. W. 
Smoliar, “Automatic partitioning of full-motion 
video,” Multimedia Systems, vol. 1, no. 1, 
pp.10-28, 1993. 
[29] A. Hanjalic, “Multimodal approach to 
measuring excitement in video,” in Proceed-
ings of the IEEE International Conferences on 
Multimedia and Expo, pp. 289-292, 2003. 
[30] C. Toklu and S. P. Liou, “Automatic key-
frame selection for content-based video index-
ing and access,” in Proceedings of The Interna-
tional Society for Optical Engineering, vol. 
3972, pp. 554-563, 2000. 
 10
表 Y04 
會議進行方式可以分為專題演講( Keynote Speech )和技術會議( Technical Conference )，
其為第十六屆國際電腦通訊與網路研討會 ( International Conference on Computer 
Communication and Network )之分支會議。該會議當中除了我們所參加的多媒體分析與
處理國際研討會之外，尚有「電腦及通訊網路效能建模與評估國際研討會( International 
Workshop on Performance Modeling and Evaluation in Computer and Telecommunication 
Networks )」以及「分散感測網路國際研討會( International Workshop on Distributed Sensor 
Networks )」同時舉行。本會議中總共接收了 32個國家，超過 550篇論文，平均接受率
大約是 29%，意即大約接受 160篇論文。會議舉辦於烏龜灣渡假村( Turtle Bay Resort )，
其位於歐胡島北邊的海濱。歐胡島為夏威夷群島裡面最大的島，將近八成的人口居住在
此，主要生活區域在南部的威基基( Wakiki )地區，其主要地型為火山地型，群島中的所
有島嶼均為火山島，尤於四面環海及穩定優良的氣候，觀光收入為其主要來源。 
 
會議當天早上 8:30，主辦單位舉辦一場專題演講，演講者為 Cheng-Zhong Xu博士，講
題為「Quality Assurance and adaption: A Key to Stress-Resilient Internet Service」，其主要
內容探討在網頁服務下的自治性品質控制架構。在下午也有一場重要的專題演講，演講
者為來自新加坡南洋理工大學的 WeiSi Li 博士，講題為「Perception Is Reality: Visual 
Quality Evaluation and Beyond」，其主要內容探討視訊主觀品質(Objective quality)的衡量
是否為評斷的唯一準則，如何利用人類視覺系統(Human Visual System) 提出主觀品質衡
量方法(Subjective quality)。在其中有三場會議進行，分別為筆者所參加的「視訊編碼與
分析( Video Coding and Analysis )」、「多媒體品質評估( Multimedia Quality Evaluation )」
及「智慧型視訊監控媒體處理( Media Processing for Intelligent Video Surveillance )」。 
 
筆者在此次會議當中發表「基於使用者專注模型之感興趣區域強健決策法則-使用視覺韻
律分析」。其屬於視訊編碼與分析( Video Coding and Analysis )之領域，會議主持人為來
自美國 QUALCOMM公司的 Gökçe Dane博士。我們於 8月 16日上午發表我們的研究
成果，我們被安排在第四位發表。因在一般研究者認知攝影機移動而造成的小幅動量一
直是視訊編碼當中的難題，筆者發表論文時，主持人 Gökçe Dane 博士對於我們所提出
的演算法能有效地消除攝影機移動而造成的小幅動量感到十分有興趣，並與筆者交換許
多意見。 
 
 
