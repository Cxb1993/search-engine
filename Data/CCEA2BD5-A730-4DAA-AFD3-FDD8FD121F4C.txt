 2
程結束後的量測資料之FDC架構，歸納出製程或機台中的參數資料與製程事故的相關
性，提供事故發生時的診斷、分類及緊急處理。最後，藉由實證研究以檢驗所建立之
理論架構，驗證模式之效度。本研究發展不同資料挖礦的方法，提供TFT-LCD產業製
造管理與良率提昇的決策依據，對TFT-LCD產業將有創新的貢獻，本計畫三年相關研
究成果已發表六篇學術期刊論文，並陸續整理發表其他研究結果中。 
關鍵字：事故診斷、良率提升、資料挖礦、約略集合理論、TFT-LCD 
 
Abstract 
The manufacturing processes of Thin Film Transistor-Liquid Crystal Displays 
(TFT-LCD) are complex in which many factors could cause different defect on panel and 
result in low yield. However, engineers is used to rely on personal experiences for trouble 
shooting in TFT-LCD manufacturing, which could not quickly locate possible fault root 
causes by own domain knowledge or rule of thumb. Owing to the fully automation 
manufacturing environment in TFT-LCD factories, a large amount of raw data has been 
increasingly accumulated from various sources automatically or semi-automatically for fault 
diagnose and process monitoring. This study aims to develop a data mining framework to 
diagnose the possible causes of TFT-LCD defects in factories. The extracted information and 
knowledge is helpful to engineers as a basis for trouble-shooting and defect diagnosis. 
Finally, an empirical study in a TFT-LCD company was validated and the results 
demonstrated the practical viability of this approach. In sum, we have published more than 
six journal papers sponsored by this three-year NSC project and also been preparing other 
manuscripts for publishing related results in the near future.  
 4
This study aims to develop an effective defect diagnosis methodology to assist the 
engineer in identifying equipment excursion that are caused the panel defect. In particular, a 
data mining approach based on Rough Set Theory (RST) [14] is developed to explore 
TFT-LCD manufacturing data and thus derive decision rule between manufacturing process 
and plate defect degree. The extracted information and knowledge is helpful to engineers as 
a basis for trouble-shooting and defect diagnosis. Indeed, data mining has been widely 
applied in many fields, yet its application in TFT-LCD is rare. RST can discover the 
relationships hidden in manufacturing process data and then express them in decision rules 
[7, 15]. Furthermore, this approach was validated with an empirical study in a TFT-LCD 
company in Taiwan and the results demonstrated the practical viability of this approach. 
2 FUNDAMENTAL 
2.1 Data mining 
Data mining is a multidisciplinary approach for discovering potentially useful patterns 
from large amounts of data stored either in databases, data warehouses, or other information 
repositories [1]. Data mining is a data-driven yet assumption-driven. Owing to advances of 
information technologies and new applications, large amounts of data are recorded that can 
provide a rich resource for data mining and knowledge discovery from database.  
The model functions of data mining are very diverse because many types of patterns 
exist in a large database. Data mining has been applied in many areas that can be categorized 
by different problem types including association, clustering, classification, and prediction. 
 6
how the decision was made with simple, understandable, and useful rules in the presence of 
imprecision uncertainty and vagueness. Using RST does not need assumption such as 
independency among the variables and normality of data distribution and data size. The 
“IF-THEN” rule is easy for user to explain and understand. In particular, RST can be applied 
for categorical data directly. RST has been applied in various domains such as business 
failure prediction [9], medical decision making [13], and fault location [15]. Yet, little 
research has been done for yield enhancement for TFT-LCD manufacturing. The concept of 
RST is detailed in the following sections. 
Information system of RST 
Pawlak first instroduced RST as an extension of set theory for the study of intelligent 
systems that are characterized by incomplete information to classify imprecise, uncertain or 
incomplete information, or knowledge, which is expressed in terms of data. Information can 
be associated with objects in the universe and thus can be expressed in the decision table. In 
the decision table each row represents an object and each column represents an attribute. The 
attributes are generally classified into conditions and decisions as shown in Table 1. 
Therefore, knowledge can be described in an information system, containing four 
components as follows: 
 
( )fVQUS ,,,=  (1) 
 
U is called a universe, which is a finite set of objects. A is the finite set of attributes. V is the 
 8
),(),(),( ayfaxfIyx D =⇔∈      Da∈∀  (3) 
 
Considering only with the subset B of the attributes, x and y are D-indiscernible. The 
D-indiscernibility relation will induce the D-elementary set in S. Moreover, the family of all 
equivalence classes defined by the relation DI  is denoted by 
DI
U .  
Based on indiscernibility relations and the universe can be decomposed into blocks of 
indiscernible objects, a partition of the universe can be generated. For example, the attribute 
“Process station A” will induce two elementary sets {1, 5} and {2, 3, 4}. Let 1D ={“Process 
station A”} , then two D–elementary sets can be generated. That is, 
1D
I ={1,5}, given the 
Process station A is “A01.” Objects 1 and 5 are indiscernible, since their previous work 
experiences are identical, that is, “A02.” Similarly, 
1D
I ={2,3,4} given the Process station A 
is “A01.” Therefore, 
1D
I
U ={{1,5},{2,3,4}}. Considering both the attributes “Process station 
B” and “Process station C,” the elementary sets of {1, 5}, {2}, and {3, 4} will be generated. 
In other words, the D-elementary set, where 23D = {“Process station B”, “Process stationC”} 
generates three elementary sets {1, 5}, {2}, and {3, 4}. Hence, 
23D
I
U = {{1, 5}, {2}, {3, 4}}. 
Similarly, any subset of the attributes can generate elementary sets. 
Approximation of sets 
Any union of the P-elementary sets is considered be either a precise set or a rough 
(imprecise) set. Take Table 1 as an example, the condition attribute 1 values of Objects 1 and 
5 are the same but the decision attribute are different as shown in Table I. This situation 
implies that no rule of existing information can be drawn but approximations may be derived. 
 10
Takes Table 1 as example, the objects can be classified into two categories according by the 
values of decision attribute “Defect degree”, 1Y and 2Y , as follows:  
 
}2,1{1 =Y denoting the objects that the yield rate of plate is acceptable (7) 
 
}5,4,3{2 =Y denoting the objects that the yield rate of plate is not acceptable (8) 
 
Let }Cstation  ProcessB,station  ProcessA,station  Process{123 =D , the 
23D
I
U ={{1,5}{2}{3},(4)}. The only element that can be contained by is {2}, the lower 
approximation of the set 1Y  in D, 1YD is {2}. Since 
 
φ≠∩=∩ }2,1{}4,1{}5,1{ 1Y  (9) 
 
φ≠∩=∩ }2,1{}2{}2{ 1Y  (10) 
 
φ=∩=∩ }2,1{}3{}3{ 1Y  (11) 
 
φ=∩=∩ }2,1{}4,1{}4{ 1Y  (12) 
 
As the result, 1YD  is {1, 2, 5}. The boundary region of 1Y in B is as follows: 
 
}5,1{}2{}5,2,1{)( =−=−= YDYDYBND  (13) 
 
Moreover, the accuracy of approximation for set iY  in D is defined as follows: 
 
)(
)()(
i
i
iD YDcard
YDcardY =α  (14) 
 
The index )( iD Yα  is between 0 and 1. If )( iD Yα =1, iY  is an ordinary (exact) set with 
respect to D; on other hands, if )( iD Yα <1, iY  is a rough (vague) set with respect to D. 
 12
 
U
BIX
C XCBPOS
∈
=)(  (18) 
 
Which denotes the set of objects that can be properly classified into B-elementary sets 
generated by BI  employing the knowledge expressed by CI  . Let Cq∈ . 
If )()( }{ BPOSBPOS qCC −= ,Then q is B-indispensable in C; 
Otherwise, q is B-independent in C (19) 
The subset F of C is called the B-reduct of C if and only if F is the B-independent 
subset of C and )()( BPOSBPOS CF =  
For example, let C ={“Process station A”, “Process stationC” } , that generates four 
elementary sets {1, 5},{2}, {3}, and {4}. Hence, }}4{},3{},2{},5,1{{/ =cIU .  
let E={“Defect degree”} and thus }}5,4,3{},2,1{{/ =EIU . Therefore, 
}5,3,2{)( =EPOS C . Considering the removal of the attribute 1q = “Process station 
A” from set C, then }}4{},2{},5,3,1{{/
1
=−qcIU and thus )(}4,2{)(1 EPOSEPOS CqC ≠=− . 
Therefore, the attribute “Process station A” is E-indispensable in C. Furthermore, in RST, 
core is defined to denote the interaction of the reducts as follows: 
 
)()( BREDBCore I=  (20) 
 
RED(B) is the set of all the reducts of B . Core contains the most critical subset of 
attributes. Any of the attributes removed from RED(B) could cause the decrease of the 
classification power. 
 14
a TFT or a CF is good. In addition to improve TFT or CF plate yield directly, the yield losses 
can be reduced by optimization of matching process. Su et al. [16] formulated a linear 
programming for the cassettes matching and plates matching problems of TFT substrate and 
CF plate in the cell process. Furthermore, while the low yield problem is happened, 
engineers have to quick response and identification of root cause of problem as soon as 
possible to reduce the loss caused by excursion. However, little research has been done to 
extract the causal relationship between process and low yield as semiconductor 
manufacturing [6, 8, 11] 
 
3 PROPOSED APPROACH 
 
This study proposed a data mining framework based on RST for TFT-LCD 
manufacturing defect diagnosis. As shown in Figure 1, the proposed framework consists of 
problem definition, data preprocessing, RST analysis, rules validation, and knowledge 
implementation. 
 
(Please insert Figure 1 about here) 
 
3.1 Problem definition and structuring 
The study is to identify the abnormal process step or trouble shooting and defect 
diagnose for specific panel defect in TFT-LCD manufacturing. At the end of cell process, the 
panel surface defects inspection are conducted to ensure the panel quality. There are three 
types of LCD panel defects: (1) visual appearance defects, (2) electricity induced defects, (3) 
 16
(ii) Metrology data: The measurement data collected for a specific lot (for example: Critical 
Dimension, Oxide thickness) including lot id, measure parameter name of the glass 
substrate, measure parameter value of the product, specification of upper and lower 
limits of the product. 
(iii) Non-Lot data: The measurement data collected for a specific machine (for example: the 
number of particles, etching rate) and usually collected from prevention and 
maintenance including operation machine, measurement parameter of the machine, the 
measured value, measurement time and date, the specification of upper and lower limits 
of the machine. 
(iv) Defect data: The data that describes the defects of a product, usually collected from the 
inspection equipments, failure analysis, SEM, and signature analysis including lot id, 
product name, the defect layer, the number of defects in a layer, defect density, the 
number of defects in a substrate. 
However, the collected data often includes noisy, missing and inconsistent data. Data 
preparation can not only improve the quality of the data but also make mining result 
efficiency in the following steps. Data integration merges data from multiple sources into a 
coherent data store. Data cleaning is applied to remove noisy data, identify outliers, fill in 
missing data, and correct inconsistencies. Data transformation is to transform or consolidate 
the data in a form appropriate for mining. Data reduction is to reduce the data dimension to 
 18
Step 9: For generated rule i: 
    Set j= 1 ; 
      Set support i =0. 
Step 10: Check object in the training data set: 
If the object matches both the condition and decision parts of the rule i, then 
support= support+1. If j < the total numbers of objects in the training data set, 
then j= j+1 and go to step 11. 
Step 11: If there are two or more than two decision values for one rule, then choose the most 
frequent outcome given the specific condition to be the decision attribute to maximize 
its strength. 
Step 12: If support < threshold of support , then put generated rule into candidate rules pool. 
Step 13: If i < the total number of the generated rules, then and go to step 9. 
Step 14: Stop and output the candidate rules. Finally, it is also important to present the 
derived rules to domain experts to evaluate the meaningfulness of the mined rules and 
discuss implementation strategies. 
 
3.4 Rules validation and knowledge usage 
Testing data set is used to estimate the validity of the candidate rules derived from 
training data. Accuracy and coverage is the criteria employed to test the rules. In the end, 
domain experts will examine the results to eliminate the rules with unusual pattern or far 
 20
the burn-in test and quality inspection, the product is completed. 
 
(Please insert Figure 2 about here) 
 
Without losing generality, an empirical study has been done for validation by using 
transformed data from a fab in Taiwan. In this case, cloud MURA defects were found on the 
panel and may cause by some causal relationships between the machines of specific process 
and the panel defect at certain time period. Therefore, the manufacturing data including test, 
and relative data was extracted from the engineering database. The data has been done some 
data transformation without losing data quality.  
 
4.2 Data preparation 
The inspection and manufacturing data including defect location of plate, process 
stations, operation machines, operation time, lot id, and panel inspection test time were 
integrated into a single dataset firstly. The process data were 2111 plates including 613 plates 
with cloud defect and 1398 plates without such defect. To capture the root-cause of low yield 
effectively, SOM (Self-Organization Map) clustering is applied to separate yield into high, 
medium and low three groups as shown in Figure 3. In particular, the high and low group 
were selected for further RST analysis. In addition, the original data (2111) was also split 
into training data (1688) and testing data (423) randomly for rule validation. The target 
variable of yield groups and relative process stations information including operation 
equipments and operation time will be used for further analysis. 
 22
 
4.4 Rules validation and knowledge usage 
The rules were validated by testing data. The total coverage is 100% and total accuracy 
of defect rule is 64.3%. The total coverage is 100% means 423 rows of testing data are 
matched the rule. The accuracy information table is showing as Table 3. There are two level 
of decision attribute 1 and 0. The data with decision attribute “1” means the produce with 
cloud defect. The data with decision attribute “0” means the produce without cloud defect. In 
particular, the predicted accuracy of cloud mura defect is 77%. 
 
(Please insert Table 3 about here) 
 
The defect of cloud MURA is mainly caused from the incomplete cleaning of subtract 
that will make the difference of threshold voltage. And result in the electric shock is not 
uniform during the etch process. However, such kind of problem cannot be inspected directly 
by naked eye. It could be only inspected via the visual phenomenon of inconsistent color 
after the threshold voltage is transmitted.  
 
5 CONCLUSION 
The proposed framework integrated rough set theory and other data mining approaches 
to explore the engineering data of TFT-LCD array process for identification of root cause 
and yield enhancement. Based on the empirical results, this study validated that the proposed 
approach has practical viability. It can help domain engineers to find out root causes of 
defects and provide the information to support decision makers to understand how to 
 24
5. Chien, C., and C. Hsu, “A novel method for determining machine subgroups and 
backups with an empirical study for semiconductor manufacturing,” Journal of 
Intelligent Manufacturing, 17, 429-440 (2006). 
6. Chien, C., D. Lin, C. Peng, S. Hsu, “Developing Data Mining Framework and Methods 
for Diagnosing Semiconductor Manufacturing Defects and An Empirical Study of 
Wafer Acceptance Test Data in A Wafer Fab,” Journal of the Chinese Institute of 
Industrial Engineers, 18(4), 37-48 (2001). 
7. Chien, C. and L. Chen, “Using Rough Set Theory to Recruit and Retain High-Potential 
Talents for Semiconductor Manufacturing,” IEEE Transactions on Semiconductor 
Manufacturing, 20(4), 528-541 (2007). 
8. Chien, C., W. Wang, and J. Cheng, “Data mining for yield enhancement in 
semiconductor manufacturing and an empirical study,” Expert Systems with 
Applications, 33, 192-198 (2007). 
9. Dimitras, A. I., R. Slowinski, R. Susmaga, and C. Zopounidia, “Business failure 
prediction using rough sets,” Europe Journal of Operation Research, 114(2), 263-280 
(1999). 
10. Han, J., and M. Kamber, Data Mining: Concepts and Techniques, 2nd ed., Morgan 
Kaufmann Publishers (2006). 
11. Hsu, S., and C. Chien, “Hybrid data mining approach for pattern extraction from wafer 
bin map to improve yield in semiconductor manufacturing,” International Journal of 
Production Economics, 107(1), 88-103 (2007). 
12. Jiang, B. C., C. Wang, and H. Liu, “Liquid crystal display surface uniformity defect 
inspection using analysis of variance and exponentially weighted moving average 
techniques,” International Journal of Production Research, 43(1), 67-80, (2005). 
13. Kusiak, A., J. A. Kern, K. H. Kernstine, and B. Tseng, “Autonomous decision-making: 
a data mining approach,” IEEE Transactions on Information Technology in Biomedicine, 
4(4), 274-284 (2000). 
14. Pawlak, Z., “Rough Sets,” International Journal of Computer and Information Sciences, 
11(5), 341-356 (1982). 
15. Peng, J., C. Chien and B. Tseng, “Rough set theory for data mining for fault diagnosis 
on distribution feeder,” IEE Proceedings-Generation, Transmission, and Distributions, 
151(6), 689-697 (2004). 
16. Su, C., P. Wang, and C. Hsu, “Effective approach for low temperature polysilicon 
TFT-LCD post-mapping yield control problem,” IEEE Transactions on Automation 
Science and Engineering, 2(2), 198-206 (2005). 
17. Su, Y., M. Hung, F. Cheng, and Y. Chen, “A processing quality prognostics scheme for 
plasma sputtering in TFT-LCD manufacturing,” IEEE Transactions on Semiconductor 
Manufacturing, 19(2), 183-194 (2006). 
 26
計畫成果自評 
本三年計畫（2005-2008）係建構一整合架構以結合資料挖礦（Data mining）、先
進製程管制（Advanced Process control）、事故診斷與分類（Fault Detection and 
Classification）以提昇TFT-LCD良率，並進行實證研究，作為提昇TFT-LCD產業營運總
體競爭力之導向性基礎研究。第一年（NSC94-2213-E-007-050）藉著實習與專家訪談
使參與研究人員對問題具有深度的瞭解，定義與架構問題，運用文獻的探討整合相關
理論及個案研究，建立理論基礎，發展TFT-LCD 製程資料挖礦之模式，目前已完成初
步成果整理發表中。第二年（NSC95-2221-E-007-125）建立TFT-LCD 之APC 架構、
藉由回顧理論與相關研究，發展製程式控制制模式，以回饋補償製程參數所產生之漂
移（drift）與位移（shift）。第三年（NSC96-2221-E-007-048）結合資料挖礦技術，進
行資料的定義、蒐集，挖掘整理資料的背後可能隱藏的樣型與資訊，並建構所收集之
製程或機臺端資料與製程結束後的量測資料之FDC 架構，歸納出製程或機台中的參數
資料與製程事故的相關性，提供事故發生時的診斷、分類及緊急處理。最後，藉由實
證研究以檢驗所建立之理論架構，驗證模式之效度。 
透過國科會補助計畫以及與友達光電的合作實證研究，本研究團隊已成功將過去
在半導體產業之資料挖礦研究成果轉換至TFT-LCD 產業領域，並針對TFT-LCD 產業
之特性發展研究方法和資料挖礦工具，未來正逐步發展先進製程控制模式，以提升
TFT-LCD陣列製程之良率。本計畫所發展的研究方法，整合資料挖礦、先進製程管制、
事故診斷與分類以提供TFT-LCD 產業製造管理與良率提昇的決策依據，對TFT-LCD 
產業將有創新的貢獻。 
相關研究成果除參與研討會發表外，並陸續整理發表為學術期刊論文（已發表包
括六篇學術期刊論文） 
Hsu, C., K. Lin, C.-Y. Chien, and Chen-Fu Chien (2008), “Using Rough Set Theory to 
Diagnose TFT-LCD Manufacturing Defect and an Empirical Study,” Proceedings of 
the Chinese Institute of Industrial Engineers National Conference, Taoyuan, Taiwan. 
 28
Chien, Chen-Fu, Y. Lin and J. Cheng (2008), “Construct Fuzzy Decision Tree for Mining 
Interrelated Semiconductor Manufacturing Data for Yield Enhancement,” Journal of 
Quality, 15(3), 193-210. (NSC 95-2221-E-007-125) 
Lin, C., Chen-Fu Chien, and C. Hu (2008), “UNISON Decision Analysis Framework for 
Constructing the Process Validation Model for Liquid Crystal Manufacturing Plant,” 
Journal of Quality, 15(5), 313-326. (NSC94-2213-E-007-050; NSC 
95-2221-E-007-125) 
Wu, J. and Chen-Fu Chien (2008), “Modeling semiconductor testing job scheduling and 
dynamic testing machine configuration,” Expert Systems with Applications, 35, 
485-496. (NSC95-2221-E-007-125) (SCI) 
 
 30
Problem 
definition
Decision table 
construction
Data 
preparation
Testing 
data set
Reducts 
generation
Training    
data set
Decision
Candidate
Decision reles
Defect
data partition
Problem definition 
and 
Data preparation
RST approach 
implementation 
Rules validation
Knowledge
Rules
generation
Rule filter
 
Figure 1 RST framework for TFT-LCD defect diagnosis 
 
 
Array Process Cell Process Module Process
Glass Substrate
Cleaning
Thin Film Deposition
Resist Coating
Exposure
Etching
Resist Remove
Test
Array Substrate
Cleaning
Polyimide Film Printing
Cleaning
Assembly
LCM Injection
TFT-LCD Panel
OLB
PCB Soldering
Backlight Mounting
Burn-in
Test
Color Filter
Spacer Spray Sealant Coating
Transfer Coating
End Sealing / Anneal
Polarizer Attachment
Test
Repeat 5 
or 7 times
 
Figure 2 TFT-LCD manufacturing process 
 
 1
出席國際學術會議心得報告 
                                                             
計畫編號 NSC-96-2221-E-007-048 
計畫名稱 整合資料挖礦、先進製程管制、事故診斷與分類以提昇 TFT-LCD 良率之前瞻研究(3/3) 
出國人員姓名 
服務機關及職稱 
簡禎富 教授  
國立清華大學工業工程與工程管理學系教授 
會議時間地點 September 22-25, 2007, Scottsdale, Arizona, U.S.A. 
會議名稱 IEEE CASE2007 (IEEE Conference on Automation Science and Engineering) 
發表論文題目 Structuring Manufacturing Strategy 
 
一、參加會議經過 
「半導體製造模型與分析研討會」（Modeling and Analysis of Semiconductor 
Manufacturing）是由美國 SEMATECH 與亞利桑那州立大學 (Arizona State University)所
舉辦之重要國際性會議，過去此研討會一直在美國本土舉辦，本次是第一次與 IEEE 
Conference on Automation Science and Engineering 一起舉辦，此次主辦單位為亞利桑那州
立大學，協辦單位為 IEEE Robotics and Automation Society。正式會議由九月二十二日至
二十五日舉行。此外國內學者還包括成功大學鄭芳田教授等學者。 
會議主題著重於半導體製造管理與應用，內容涵蓋 300mm 晶圓製造之挑戰、先進
設備生產力提昇、作業模式與模擬、統計方法之應用及相關應用電腦系統。有來自世界
各國的論文在此研討會發表。 
個人於會中發表論文題目為「Structuring Manufacturing Strategy」，大會並安排我擔
任與談人分享對高科技產業的看法，受到與會各國學者專家的注意，在報告過程與結束
之後得到與會學者不少寶貴的意見和建議。在各場次間筆者皆有參與熱烈之討論與新知
識的獲取，午間用餐時間亦安排多場專業技術之演講，在此行並結識許多工程與學術界
的先進與教授們，並應邀擔任 IEEE Transactions on Automation Science and Engineering
的副主編。 
