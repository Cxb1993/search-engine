Replica Aware Job Scheduling in Grid Environments
Wei-Cheng Liao
Department of Computer Science and Information Engineering
National Taiwan University
Taipei, Taiwan
r95134@csie.ntu.edu.tw
Pangfeng Liu
Department of Computer Science and Information Engineering
Graduate Institute of Networking and Multimedia
National Taiwan University
Taipei, Taiwan
Jan-Jan Wu
Institute of Information Science
Academia Sinica
Nankang, Taiwan
Abstract
We proposes a heuristic algorithm that explores the advantage of file sharing and replication
while executing independent jobs in a grid environment. The proposed scheduling algorithm
not only consider execution time load balancing, but also consider the location of data and
replica these jobs require. The simulation results indicate that the proposed algorithm is more
scalable than a random heuristic algorithm. In addition, in most cases the relative perfor-
mance of our heuristic algorithm is within 50% of a conservative theoretical lower bound. The
simulation results also indicate that the heuristic performs well in our synthesis environment,
as well as in real grid system like Taiwan Unigrid.
1 Introduction
As grid systems become increasingly popular, the vast amount of computing and storage resources
allow us to dispatch large scale resource-demanding problems to remote servers so that the final
solution can be found within a reasonable amount of time. Grid systems are widely adopted by
scientists in various research areas. For example, theWorldwide Large Hadron Collider Computing
Grid [13] build and maintain a data storage and analysis infrastructure for the entire high energy
physics community. Other organizations like the Biomedical Informatics Research Network [3]
provides a platform for biomedical science sharing data and computing resources.
There are many grid applications that solve various problems in different research areas, and
almost all of them require input data to find the final solutions. The computation cannot proceed
without the required data. For example, the AppLeS Parameter Sweep Template [8] provides a
framework for fast deployment and execution of grid applications. The execution model consists
of independent jobs, and each of them requires a different set of data that may be shared by
1
simultaneous will provide a good opportunity for improving system performance.
There have been works that integrate scheduling and replication in order to improve system
performance [14, 9, 17, 10]. Ranganathan and Foster combined scheduling and replication strategy
by using a two-stage scheduling that combines an external scheduler and a dataset scheduler.
However they only address the issue of single file dependency [14]. Chakrabarti et. al. extend
the data sharing model to deal with multiple data requirement [9]. Tang et. al. proposed both
centralized and distributed heuristic algorithms for dynamic replication, and combine them with
job scheduling algorithm to solve on-line scheduling problem [17]. Desprez and Vernois gives a
linear programming technique for shared data scheduling [10].
The integrated strategies mentioned above require explicit replication services to monitoring
the replication status of of the system periodically so as to re-replicate data items whenever
necessary in order to avoid communication contention on a single node. In addition, these works
do not utilize the existing replicas that have been generated for the purpose of executing tasks.
For example, in grid environments like GFS [5], the replication service does not consider
communication contention while deciding where to place replicas, not will it reuse those data files
that have been replicated in the grid to execute jobs that have been submitted and executed
earlier. As a result we need a scheduling strategy that is aware of the existing replicas, no matter
they are actively replicated to improve performance, or by-product of previous job execution, and
make a good use of these replicas so as to improve overall system performance.
3 System Model
We now consider the Grid system model for the replica aware shared-data scheduling problem.
The Grid system consists of computing nodes, and each node has direct links to all the other
nodes in the system. We represent the link connection within a Grid system as a complete graph
G = (V ,E), where a node v ∈ V is a computing node and an edge e = (u, v) ∈ E is a link between
node u and node v. The system is heterogeneous so that both the computing capabilities of nodes
and the latencies of communication links may be different. We denote the computing capability
of a node v as |v|, and the latency of link (u, v) as |(u, v)|.
3.1 Computation
Initially a set of independent jobs are submitted into the system. Our system model assumes that
a computing node executes jobs one at a time and the execution is non-preemptive. Formally we
use Jv to denote the job execution sequence of node v, which initially has all the jobs assigned to
node v for execution. Node v will execute the first job in Jv, then remove it from Jv when the
execution finishes. That is, a job j must wait for the other jobs that are in front of it in Jv to
finish before it can start. Let |j| denote the amount of computation of job j. Recall that |v| is the
computation power of node v, so the time for node v to execute job j can be defined as
|j|
|v|
.
In addition to waiting for other jobs assigned to the same node, a job must also wait for the
data in order to proceed. Let Dj denote the data required by job j, then Dj must be available at
node v before v can start executing j. Therefore we define that a job j is ready at node v if all data
in Dj are available at v and j is now at the head of execution sequence Jv. After the execution
j will be removed from Jv so that the next job in Jv can become ready. In the following we will
describe the communication mechanism that node v acquires all the data it needs, i.e., the set Dj
for all job j in the execution sequence Jv.
3.2 Communication
We adopt the full-duplex one-port communication model in [2]. In the full-duplex one-port model,
each node can simultaneously send/receive one data item to/from one of its neighbors. Note that
3
Figure 1: A data warehouse and computing nodes with local storage.
denotes the computation power of node v, so the time for node v to execute the first job j in Jv
can be defined as |j|
|v|
. The completion time is therefore the sum of ready time and the execution
time of j, and we can determine the ready time and the completion time from the first to the last
job in Jv. The completion time of the last job in Jv is the complete time of node v. The maximum
completion time of all nodes is the total completion time.
We consider the following problem in this paper. Given a Grid system G = (V ,E), a set of
jobs J and a set of data D, find a schedule Ψ = (J,S) that minimizes the total completion time,
where J = {Jv|v ∈ V} and S = {Sv|v ∈ V} are the sets of execution and sending sequences of nodes
in V respectively.
3.4 Notations
We summarize the notations used in the system model in Table 1.
4 Heuristic Algorithm
In this section we propose a heuristic algorithm that finds efficient schedules for the replica aware
shared-data scheduling problem described in Section 3. As mentioned in Section 2, this scheduling
problem is NP-complete even if there are only two computing nodes, but it is still important in
practice to find an efficient schedule for a set of independent jobs to run on a grid system.
The proposed greedy heuristic assigns jobs to computing nodes iteratively until all jobs are
assigned. Each iteration of the heuristic algorithm contains two phases. The first phase is the
selection phase and the second phase is the assignment phase. In the selection phase the heuristic
algorithm evaluates each remaining job, and selects the job that could finish at the earliest time
to run for this iteration. The evaluation uses a Earliest-Completion-First heuristic to collect data
for the current job under consideration in order to estimate a completion time, so that it can
determines a data sending sequence for each computing node. Finally the heuristic algorithm
assigns the chosen job j to a computing node that can complete j at the earliest possible time.
5
4.1.1 Estimated Finish Time
In the selection phase the heuristic algorithm evaluates each job according to its minimum esti-
mated finish time. The estimated finish time of job j at node v is defined as its estimated ready
time at node v plus the execution time |j|
|v|
. The minimum estimated finish time of a job j is the
minimum among all estimated finish times of j at every computing nodes in the grid system.
Estimated ready time of a job j We now define the estimated ready time of a job j on a node
v. Recall that job j is ready at node v, as mentioned in Section 3, when all jobs before j in the
execution sequence of node v have completed, and all data items needed by j (denoted by Dj) are
available at node v.
There are two conditions a job j has to wait in order to become ready at node v – the jobs
before j completes execution and the data set Dj is available at v. Recall that jv is the current
last job at node v, and its completion time is Cj(jv, v). If we could estimate when data items
in Dj will be available at v, then we can estimate when j can start. Formally we use estimated
available time of Dj at v to denote the earliest time all data in Dj will be available at v. It is
obvious that the estimated ready time of a job j at node v is the maximum of Cj(jv, v) and the
estimated available time of Dj. We now formally define the estimated available time for a data
set Dj.
Estimated available time of data set Dj We first observe that some data in Dj could have
already been brought into node v, so we do not need to retrieve them, therefore in order to
estimate the available times we only focus on the data that are in Dj but not current at node v.
For ease of notation we partition Dj into two subsets – those that are already at v is denoted by
Dvj and those that are not is denoted by D
v
j .
There are two cases to considered. First if D
v
j is an empty set then all data in Dj are already
at node v. Recall that Cr(d, v) is the arrival time of data item d on node v. The estimated
available time of Dj at node v is the maximum Cr(d, v) for all data d in Dj in this case. The
second case is that when D
v
j is not empty, i.e., there are some data items that are in Dj but not
in Dv. Now we need to decide an order to receive these data in D
v
j so that the data set D
v
j can
be brought into v at the earliest time. This will reduce the data waiting time should we decide
to assign job j to node v.
We use a Earliest-Available-Time-First heuristic to determine the order for v to receive data in
D
v
j . Let t
v
d denote the earliest time of d ∈ D
v
j at node v. For each data item d ∈ D
i
j, we estimate
the earliest time of d to be transferred into v, and choose the data d that has the minimum tvd to
be received next.
tvd = min
u∈V
{max{Cr(rv, v),Cs(su,u, x)} + |d|× |(u, v)|}. (1)
Note that in Equation 1 we consider all possible source u that could send data d to v. The
transfer from u to v could not begin until the sender u finishes sending its current last data su to
the destination x of su, or the receiver v finishes receiving its current last data rv. The transfer
completes when both parties are ready, plus the transfer time of data d along the link (u, v).
Using the Earliest-Available-Time-First heuristic, we choose the data item d that has the
minimum available time tvd and add d into the receiving sequence of v and the sending sequence
of u. We repeat this selection until all data item in D
v
j are received. The data available time of
data set Dj is the arrival time of the last data items in Dj arrives at node v.
Note that for every d and u we choose, we need to add d to the sending sequence of u and
receiving sequence of v. This will affect the last receiving data rv of v and last sending data su,
7
Algorithm 3: ScheduleUpdate: A schedule updating routine.
begin
Initialize all sequences of computing nodes to ∅.
Add all jobs into the set of remaining jobs Ju.
for i← 1 to |J| do
j← SelectJob(Ju, Ψi−1).
Let v be the computing node in which j has minimum estimated finish time.
Append j to the execution sequence Jv.
Update the sending sequences of sender nodes of Dj.
Remove j from Ju.
Ψi ← (J,S), where J and S are sequences of all computing nodes.
end
end
to v. The pseudo code of the these update operations is listed in Algorithm 3 (ScheduleUpdate).
4.3 Notations
We introduced new notations while describing our heuristic algorithm. These additional notations
are summarized in Table 2.
Notation Description
Dv data items stored in the local storage of node v
Dvj data items required by j and already at v
D
v
j data items required by j but not at v
Ψk schedule at k-th iteration
jv the last job in the execution sequence of node v
rv the last data item that node v received
sv the last data item that node v sent
Cj(j, v) completion time of job j on node v
Cr(d, v) arrival time of data d on node v
Cs(d, v,u) completion time for v to send d to u
tvd estimated data available time at node v
tvj estimated completion time of j at node v
Table 2: Additional notations used in describing the heuristic algorithm
9
may dominate the other in different situations. For example, Figure 2 shows the two estimated
lower bounds for different number of nodes. The first lower bound TL1 decreases and intersects
with the second lower bound as the number of nodes increases. This due to the effect that when
the number of nodes increases, the communication overheads becomes more important than the
job execution time, so the second lower bound will dominate. This will create a “lump” while
computing the relative performance against the lower bound since the maximum of these two
lower bound is not smooth enough, especially when they intersect in the middle, as indicated in
Figure 2.
 0
 200
 400
 600
 800
 1000
 1200
 1400
8 16 32 64 128
M
ak
es
pa
n
Number of Nodes
Estimated Lower Bound
Lower Bound 1
Lower Bound 2
Figure 2: The two theoretical lower bounds.
5.2.2 Random Job Selection
The other method we compared is a random job selection heuristic. This algorithm simply chooses
a job j at random in each scheduling phase. Then the heuristic uses a Earliest-completion-first
heuristic to select a computing node for executing j. Note that while the job selection to be
executed is at random, but the selection of computing node for this job is carefully selected to
minimize its completion time.
5.3 Experimental Results
We conduct experiments in both synthetic simulation parameters and parameters taken from a
real grid system (Taiwan Unigrid).
5.3.1 Synthetic Simulation Parameters
We compare the relative performance of heuristics, using the theoretical lower bound as the
baseline. We test the heuristic with different number of nodes, jobs and data items to see how it
performs in different environments. Each set of performance figures contains three figures (from
top to bottom) for computation-to-communication ratio set to 10:1, 1:1 and 1:10 respectively.
We first test the effects of system scale, i.e., the number of computing nodes. In Figure 3
the X coordinate is the number of computing nodes and Y coordinate is the relative perfor-
mance compared against a theoretical lower bound. The three figures represent three different
computation-to-communication ratios.
11
In Figure 3 we find that our heuristics are closer to the lower bound when the amount of data
communication increases, i.e., when the computation-to-communication ratio decrease from 10:1,
to 1:1, and to 1:10. As mentioned in Section 5.2.1, the unstable behavior in computation-intensive
case is due to the lower bound estimation method.
In Figure 4 we test the heuristics with different job sizes. The X coordinate is the number of
jobs and Y coordinate is the relative performance compared against a theoretical lower bound. As
the number of jobs increases, our heuristic remains relatively stable but the random job selection
heuristic performs poorly when the number of jobs increases. It seems that our heuristic can
distribute most data items in the early stage of job execution, so that jobs start at the later
stage of execution will not need to wait for data. This suggests that even when we use the
earliest-completion-first heuristic in selecting computing nodes for execution, we should still be
very careful about the selection of job for execution.
In Figure 5 we test the heuristics with different different number of data. The X coordinate
is the number of data and the Y coordinate is the relative performance compared against the
theoretical lower bound. As the number of data increases, the amount of data sharing among jobs
decreases since the number of data shared among jobs is randomly selected from 1 to 3. If the data
items are not shared, the data transferring time increases and the makespan will be dominated
by the communication time. In addition, the performance of both our heuristic and the random
job selection heuristic will be very close to the theoretical lower bound when the number of data
is beyond 128. The reason is that when the communication time dominates, the second lower
bound, which is based on the communication time, can accurately reflect the total overhead. As
we can see in Figure 5 the makespans from both heuristics are very close to the lower bound.
On the other hand, this indicates that our algorithm will have performance advantage if jobs do
share data items, as indicated by Figure 5 when the number of data is less than 128.
We observed that when the amount of data communication increases, the data communication
time dominates the job execution time. In such cases our heuristic can benefit from data item
sharing and replication since our heuristic tends to distribute data items required by jobs at the
early stage of execution. These experiments also indicate that the performance of our heuristic is
scalable with increasing number of computing nodes and jobs, therefore our heuristic will perform
well in large scale systems.
5.3.2 Real Grid Parameters
We now use the system parameters gathered from Taiwan Unigrid as our environment settings.
We gathered the performance parameter of 39 nodes from Taiwan Unigrid. We use one node as
data warehouse and the remaining 38 nodes as computing nodes. The computing capacities are
measured from ubench [18], and the values are normalized with respect to the slowest node, so
the range of node computing capacities are from 1 to 2.55.
We measured the link latencies using the average time of transferring a 512 megabyte message
on each link. We normalized the link latencies with respect to the interconnection in the site
that contains fastest link. The link used as baseline is with 11.48 seconds of latency, and the
normalized link latencies are ranging from 1 to 13.95.
As the simulations in the synthetic environment settings, we also experiment our heuris-
tic algorithm in Taiwan Unigrid environment settings with the three different computation-to-
communication ratios. Figure 6 indicates the performance under different numbers of jobs, and
Figure 7 shows the impact of different number of data. We observe from these figures and conclude
that the results are consistent with the performance obtained from the previous synthetic param-
eters configurations. This consistency indicates that that our heuristic algorithm will produce
efficient schedules in a real world system, e.g., Taiwan Unigrid.
13
 1
 1.2
 1.4
 1.6
 1.8
 2
 2.2
 2.4
8 32 128 512 2048
R
at
io
 o
f R
el
at
ive
 P
er
fo
rm
an
ce
Number of Data Items
Computation:Communication 10:1
Lower Bound
Greedy
Random Job Selection
 1
 1.2
 1.4
 1.6
 1.8
 2
 2.2
 2.4
8 32 128 512 2048
R
at
io
 o
f R
el
at
ive
 P
er
fo
rm
an
ce
Number of Data Items
Computation:Communication 1:1
Lower Bound
Greedy
Random Job Selection
 1
 1.2
 1.4
 1.6
 1.8
 2
 2.2
 2.4
8 32 128 512 2048
R
at
io
 o
f R
el
at
ive
 P
er
fo
rm
an
ce
Number of Data Items
Computation:Communication 1:10
Lower Bound
Greedy
Random Job Selection
Figure 5: The relative performance of the heuristic algorithms to the theoretical lower bound.
The number of data items is from 8 to 2048.
15
 1
 1.2
 1.4
 1.6
 1.8
 2
 2.2
 2.4
 2.6
 2.8
8 32 128 512 2048
R
at
io
 o
f R
el
at
ive
 P
er
fo
rm
an
ce
Number of Data Items
Computation:Communication 10:1
Lower Bound
Greedy
Random Job Selection
 1
 1.2
 1.4
 1.6
 1.8
 2
 2.2
 2.4
 2.6
 2.8
8 32 128 512 2048
R
at
io
 o
f R
el
at
ive
 P
er
fo
rm
an
ce
Number of Data Items
Computation:Communication 1:1
Lower Bound
Greedy
Random Job Selection
 1
 1.2
 1.4
 1.6
 1.8
 2
 2.2
 2.4
 2.6
 2.8
8 32 128 512 2048
R
at
io
 o
f R
el
at
ive
 P
er
fo
rm
an
ce
Number of Data Items
Computation:Communication 1:10
Lower Bound
Greedy
Random Job Selection
Figure 7: The relative performance of the heuristic algorithms to the theoretical lower bound
under Taiwan Unigrid environment settings. The number of data items in system is from 8 to
2048. 17
[8] H. Casanova, G. Obertelli, F. Berman, and R. Wolski. The apples parameter sweep tem-
plate: user-level middleware for the grid. In Proceedings of the ACM/IEEE conference on
Supercomputing, pages 75–76, 2000.
[9] A. Chakrabarti, R.A. Dheepak, and S. Sengupta. Integration of scheduling and replication
in data grids. In International Conference on High Performance Computing 2004, pages
375–385, 2004.
[10] F. Desprez and A. Vernois. Simultaneous scheduling of replication and computation for
data-intensive applications on the grid. Journal of Grid Computing, 4(1):19–31, 2006.
[11] M. Garey and D. Johnson. Computers and Intractability: A Guide to the Theory of NP-
Completeness. WH Freeman and Co. New York, NY, USA., 1979.
[12] Globus Toolkit. http://www.globus.org/toolkit/.
[13] LCG: LHC Computing Grid. http://lcg.web.cern.ch/LCG/.
[14] K. Ranganathan and I. Foster. Simulation studies of computation and data scheduling algo-
rithms for data grids. Journal of Grid Computing, 1:53–62(10), 2003.
[15] Storage Resource Broker. http://www.sdsc.edu/srb/.
[16] Taiwan Unigrid. http://www.unigrid.org.tw/.
[17] M. Tang, B. Lee, X. Tang, and C. Yeo. Combining data replication algorithms and job
scheduling heuristics in the data grid. In 11th International Euro-Par Conference, pages
381–390, 2005.
[18] Ubench. http://www.phystech.com/download/ubench.html.
[19] J. D. Ullman. Np-complete scheduling problems. Journal of Computer and System Sciences,
10(3):384–393, 1975.
19
eggs which was delicious and expensive too, 6.99 USD. However, I realized
that they provided breakfast after I arrived at the lobby.
The first day keynote speech was given by Dr. Joel Salts, which was enti-
tled “ “Translation Research Design Templates, Grid Computing, and HPC
”. This speech was mainly about the design pattern about the translation
between Medical Pathology and Bio-informatics. Dr. Salts gave a very vivid
speech in my opinion. However, I could not fully understand what he talked
about since, first, my English capability is not good enough, and second, the
lack of my background knowledge. I had few, if any, background knowledge
about the Pattern Design and the Bio-informatics. But still, I was very ex-
cited about his speech. After speech, we had a coffee break. Since I known
no body in this conference yet, I ate along and planed which session I might
be interesting in.
The morning session I attended was “Algorithm - Scheduling”. The first
speech was given by a Spanish lady, whose accent was kind of difficult for me
to understand. Although I’ve read the related papers about her topic, the
Advanced Reservation, I still got a fuzzy idea about what her talk about. The
second speech was given by a Germany. I think he is a professor because of
his well prepared presentation. I was very impressive. He hided all technical
details(although it is technical session), but the intuitive thought about his
research and results. Presentation was very critical in the conference. You
need to let people know what you are doing and how you did in only 25
minutes. The following speeches were out of my memory...
2
