1 
 
2D 影像之 3D 深度演算法則與數位影像處理 SOC 之可行性研究 
計畫編號：97－2622－E－007－013－CC2－ 
執行期間：97 年 12 月 15 日至 98 年 12 月 15 日 
計畫主持人：陳永昌 
計畫參與人員：吳政機、孫偉智、謝佳純、許宏安、賴師璿、 
郭主引、辜柏榮、林禹辰、王佳琪、黃詩音 
 
1. 中文摘要 
在 3D 影像技術的發展中，如何擷取影像之深度
資訊一直是值得研究探討的問題。若欲以一般攝影機
擷取 2D 影像並轉換為含有場景中深度資訊的 3D 
影像，傳統的方法為使用兩個或兩個以上的攝影機進
行同步攝影，方能建構 3D 場景。但此傳統方法容易
遭受多台攝影機之間的校正精準度的影響，且多台攝
影機之校正與取得不易使得此方法於實用上較不易實
現。因此，我們的目標為設計一個使用單眼快速變焦
攝影機即可獲得深度資訊並利用此深度資訊轉換為 
3D 影像的演算法，藉由此演算法將不同焦距下所拍
攝到的影像轉換為 3D 影像必須的深度資訊，且單眼
鏡頭可避免校正精準度的問題並可以廣泛應用於現有
一般市面上之影像擷取設備，使得 3D 影像擷取將不
再受限於特定設備才能使用。最後將此演算法實現於 
FPGA 平台，並評估其實現於 SOC 的可行性。 
 
關鍵詞：3D 影像、自動變焦、深度圖、FPGA 硬體實
現 
 
Abstract 
In the development of 3D visual technology, how to 
acquire an image along with its depth information is 
always one of the most challenging problems. In order to 
achieve this from the 2D image series captured by an 
ordinary camera equipment, traditional methods use two 
or more cameras capturing video sequence from different 
angles simultaneously to reconstruct the 3D structure of 
the scene. However, this traditional method is sensitive 
to calibration accuracies between cameras and the 
difficulties of calibration and acquisition of multiple 
cameras also make it difficult to realize. Therefore, our 
goal is to design a system that uses one monoscopic 
camera with fast auto-focusing capability to obtain 
various defocused information in the scene and then 
develop an algorithm to transform these defocused 
information to depth information, which completes a 3D 
reconstruction process. In other words, based on our 
algorithm, we could get the depth information required 
for 3D display from the captured image series under 
different focal lengths. As advantages, requiring only one 
monoscopic camera can avoid the challenge of 
calibration and can be widely applied to modern image 
capturing devices. As a result, 3D image capturing is no 
longer limited to specific special equipments. Finally, we 
will implement our algorithm on an FPGA multimedia 
board, and then estimate the feasibility of its realization 
on SOC systems. 
 
Keywords: 3D image、auto-focusing、depth map、FPGA 
hardware implementation 
 
2. 研究目的 
Lots of discussions and methods have been 
developed for estimating the depth of a scene based on 
images, but most of these estimation algorithms are 
experimented on the images that are synthesized or 
captured by high quality lens and super-CCD photo 
sensor, such as handheld digital cameras. However, in 
recent years, mobile phones are ubiquitous, and cell 
3 
 
Edge Detection 
As we know, frequency components are higher on 
edges than plain regions. Thus detecting edges is 
detecting the high frequency components. Here we 
choose Laplacian filter as the edge detection operator.  
 
Figure 3.1: The overall system flow. 
Depth Propagation 
To propagate the depth value on the edges to the 
whole depth map, we incorporate the depth propagation 
process which was inspired by Levin’s work: 
“Colorization Using Optimization”. The idea of 
propagating a few color scribbles to the whole image is 
similar to the process of propagating a few accurate 
depth values to the whole depth map. 
2D to 3D Conversion Method (Version 2) 
Spatial Domain Transform Method (STM) 
In 1994, Subbarao and Surya et al proposed an 
approach which is called the S-Transform Method 
(STM). The method is based on Spatial Domain 
Convolution/Deconvolution Transform. STM requires 
only two images taken with different camera parameters 
such as focal length, lens position or aperture diameter. 
We choose STM method to be implemented on Xilinx 
FPGA because it is more fast and simple in comparison 
with other depth from focus or depth from defocus 
methods. 
Modified Spatial Transform Method 
The original STM directly computes the variance of 
grayscale images, but sometimes it may generate errors 
in some regions. 
Instead of directly computing the variances of 
gray-level images, we apply the Laplacian operator to 
gray-level images before computing the variances. 
The value of 2σ  is inverse proportional to the 
distance between camera and objects. According to this 
characteristic, we apply an inverter to the value of 2σ  
to get the relative distance of objects from the camera. 
Hardware Design for Spatial Transform Method 
Overview of the Hardware Architecture 
The hardware architecture of FPGA includes read 
data, write data and STM process as shown in Figure 3.2: 
 
Figure 3.2: Architecture of Overall System. 
The finite state machine (FSM) is shown in Figure 
3.3 includes contains Start, Read, STM process, Write 
and Done state. 
Start State 
5 
 
block. 
Step2: 
First, we add the values of the same pixel of g1 and 
g2 which were filtered by the Laplacian operator 
respectively. Then the sum is shifted right by 1 bit. 
Finally, we summarize the square of    2
2
2
1
2 gg ∇+∇
 
in the same block. 
Step3: 
We establish the lookup table of n to find the 
appropriate shift bits for calculating the value of w/n.      
Step4: 
Shifts w/n left by 4 bits. 
Step5: 
In this step, we also establish a lookup table of G2. 
According to different regions of G2, we can choose an 
appropriate square root of G2. Then we can decide the 
sign of G2 according to the values of v1, v2.   
Step6: 
We let β=954, 2β=64, and ±G-β2 is shifted 
right by 8 bits to compute σ2 in convenience. 
Step7: 
We apply an inverter toσ2 to estimate the relative 
depth map. 
Write State 
In write state, the per-pixel depth data will be 
written in FIFO. 
Done State 
The done state will be raised if both relative depth 
map of the scene from two different defocused images 
are estimated. 
Hardware Implementation for Virtual View 
Generation 
Overview of the Hardware Architecture 
The hardware architecture includes video input 
interface, video output interface, clock generation, 
memory controller interface and virtual view synthesis 
(VVS) core, as shown in Fig 3.5. 
 
Figure 3.5: Top level of our proposed system. 
The Access of External Memory 
The three-stage pipeline decides the way of the 
memory access for output video. We use two blocks of 
ZBTs for storage of our interlace input video and three 
blocks of ZBTs for storage and display of our interlace 
output video. 
Architecture of Virtual View Synthesis System 
The process of Virtual View Synthesis System 
(VVSS) is based on line-based block. Since the width of 
our synthesized image is 360 pixels, the depth of the 
buffer usually equals to 360 for hardware design. The 
address that is associated with addition operation and 
shift operation is generated by Buffer controller. And 
then it is provided for the interface of Memory controller 
to read the line pixel. 
Original Image Buffer 
We use 1 strip of 360x24bit internal SRAM to store 
our right images of “mixframe”, as the left-eye view 
image. Internal SRAM is used frequently in Virtual View 
Synthesis System in order to reduce the execution time 
for reading data from ZBT MEMORY. 
Depth Image Buffer 
We use 1 strip of 360x24bit internal SRAM to store 
the depth images in our left images of “mixframe”. 
7 
 
4. 結果與討論 
Experiment Result of 2D to 3D Conversion Method 
Version 1 
Complex Scene: The “Wire＂ Images 
 
(a) Far focused image.     (b) Near focused image. 
Figure 4.1: Test image pair: wire 
 
(a) Initial depth map of “Wire” (d) The final depth map. 
 
(b) The edge mask.        (c) The depth on the edge. 
Figure 4.2: The 2D to 3D conversion results of the 
“wire” image pair. 
Experiment Result of 2D to 3D Conversion Method 
The Wire Images 
 
(a) Far focused image.     (b) Near focused image. 
Figure 4.3: Test image pair: wire. 
 
Figure 4.4: The depth map of wire. 
Environment of Xilinx FPGA Verification 
 
Figure 4.5: Obtained relative depth map of wire. 
Virtual View Generation 
   
Fig 4.6: The test sequence “Cg” of mixframe. 
 
9 
 
TV—Concepts and key technologies", Proc. IEEE, 
vol. 96, pp. 523, Mar. 2006. 
5. C.Fehn, "A 3D-TV approach using 
depth-image-based rendering (DIBR)", in 
Proceedings of Visualization, Imaging, and Image 
Processing '93, Benalmadena, Spain, pp. 482-487, 
Sep. 2003. 
6. C.Fehn, "Depth-image-based rendering (DIBR), 
compression and transmission for a new approach 
on 3DTV", in Proc. of SPIE Conference on 
Stereoscopic Displays and Virtual Reality Systems, 
vol. 5291, pp. 93-104, San José, CA, May2004. 
7. C.Vázques, W.Tam, and F.Speranza, "Stereoscopic 
imaging: Filling disoccluded areas in depth 
image-based rendering", in Proc. of SPIE 
Three-Dimensional TV, Video, and Display 
(ITCOM), vol. 6392, pp. 63920D, Boston, MA, Oct. 
2006. 
8. CHEN Xiang-cheng, YANG Sheng and WANG 
Ya-jun, "Research on 3D Shape Reconstruction 
using Uneven Defocusing Model", Mechatronics 
and Automation, 2007. ICMA 2007. International 
Conference on, pp. 2326-2331, Aug. 2007. 
9. D.Tschumperlé and R.Deriche, "Vector-valued 
image regularization with PDEs: A common 
framework for different applications", IEEE 
Transactions On Pattern ANALYSIS AND 
MACHINE INTELLIGENCE, vol. 27, no. 4, pp. 
506-517, Apr. 2005. 
10. Elder J. H. and Zucker S. W., "Local scale control 
for edge detection and blur estimation", IEEE 
Transactions on PAMI, vol. 20, NO. 7, pp. 699-716, 
July 1998. 
11. G.J.Iddan and G.Yahav, "3D Imaging in the Studio 
(and Elsewhere ...)", in Proc. of SPIE Videometrics 
and Optical Methods for 3D Shape Measurements 
'01, pp. 48-55, San José, CA, USA, Jan. 2001. 
12. Ge Guo, Nan Zhang, Longshe Huo and Wen Gao, 
"2D to 3D convertion based on edge defocus and 
segmentation", Acoustics, Speech and Signal 
Processing, pp. 2181-2184, MARCH 2008. 
13. Honig, J. Heit, B. Bremont, J. "Visual depth 
perception based on optical blur", Image Processing, 
1996. Proceedings., International Conference on, pp. 
721-724, Sep 1996. 
14. J. Flack, P. Harman and S.Fox, "Low bandwidth 
stereoscopic image encoding and transmission", in 
Proc. of SPIC Conference on Stereoscopic Displays 
and Virtual Reality Systems X, vol. 5006, pp. 
206-214, CA, U.S.A., Jan. 2003. 
15. L.Lipton, Foundations of the Stereoscopic Cinema - 
A Study in Depth, Van Nostrand Reinhold, New 
York, NY, USA, 1982. 
16. L.Zhang and W.J.Tam, "Stereoscopic image 
generation based on depth images for 3D TV", IEEE 
Transactions On Broadcast, vol. 51, pp. 191-199, 
June 2005. 
17. Levin A., Lischinski D., and Weiss Y., "Colorization 
using optimization", Proceeding of ACM 
SIGGRAPH conference. ACM Transactions on 
Graphics vol. 23, pp. 689-694, Mar 2004. 
18. M. Subbarao and G. Surya, "Depth from Defocus: A 
Spatial Domain Approach", Int'l J. Computer Vision, 
vol. 13, pp. 271-294, 1994. 
19. M. Ziegler, L.Falkenhagen, R.Horst and D.Kalivas, 
"Evolution of stereoscopic and three-dimensional 
video", Signal Processing: Image Communication, 
vol. 14, pp. 173-194, 1998. 
20. Murali Subbarao, Tae Choi and Arman Nikzad, 
"Focusing Techniques", Journal of Optical 
Engineering, vol. 32, pp. 2824-2836, Nov. 1992. 
21. Nayar, S.K., "Shape from Focus System", 
Proceedings of the IEEE Computer Society 
Conference on Computer Vision and Pattern 
