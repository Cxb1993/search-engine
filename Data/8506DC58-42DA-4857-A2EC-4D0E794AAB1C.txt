中文摘要 
本計畫的目的是希望透過演化式計算(evolutionary computation)、可重規劃式晶片
(reconfigurable chip) 與機器學習(machine learning)的整合，讓硬體線路可以依照環境
要求，在有限的計算資源中，自主的規劃出可用的、具適應性的硬體系統，使系統
有效的運作。原設計為三年期計畫，但只通過一年，故部分計畫內容有所調整。本
計畫著重降低染色體長度，提升 EHW 計算的 scalability。因為一個硬體線路可以視
為一種圖形的連結，如果可以進一步以功能或結構來進行圖形的分解，將此圖形分
解成子圖形(subgraph)，則每一圖形可以用較短的染色體形式表現。短的染色體形式
將可以有效的減低 EHW 計算上的負擔，可以有效提升 EHW 計算 scalability 的問題。
因國內外 EHW 的研究還在起步階段，本計畫或可開啟更多學者並培養研究人力投
入 EHW 技術的研發。 
關鍵詞： 進化式硬體、演化式計算、自主式硬體、基因演算法、基因程式規劃 
Abstract 
This project is to bring hardware design the ability of adaptation or self-reconfiguration in 
a changing environment and to make hardware adaptive that can work properly even in its 
faulty conditions. In this project, we target on the efficiency and scalability of EHW. 
Graph-based representation of chromosomes is adopted in our EHW engine. A 
divide-and-conquer approach is then employed to partition long chromosomes of EHW 
structures into smaller ones according to the stability and divergence of fitness. 
Small-sized EHW structures that can be computed more efficiently are assembled for 
completing the final EHW. EHW is widely and internationally studied. However, few 
studies are from Taiwan. This project may introduce advanced techniques into the EHW 
community.  
Keywords: Evolvable hardware, evolutionary computation, adaptive hardware, artificial 
intelligence, genetic programming, genetic algorithms, hardware design 
 
一、 研究計畫之背景 
傳統的硬體設計與應用過程，大致上是先以硬體描述語言進行規格(specification)描述，再轉換成模擬
電路，並經過一連串的模擬測試與修正，直到所設計的硬體雛形滿足規格後，再將實際硬體製造出來，
進行實際應用。在數位線路設計上，有硬體設計的輔助工具 (electronics design automation，EDA)，可
以依據使用者所制訂的規格，自動或半自動的合成硬體結構。所製造出來的硬體可以依照規格，解決
在規格考慮範圍內所發生的問題。這種流程已經行之已久，並在成本效益的考量下十分有效可行。利
用模仿生物的習性與生理構造或行為進行計算系統的改造，一直受到 AI 與資訊領域研究人員相當的注
意。進化式硬體(evolvable hardware，以下簡稱 EHW)或適應性硬體(adaptive hardware)是希望透過演化
式計算(evolutionary computation)、可重規劃式晶片(reconfigurable chip) 與機器學習(machine learning)
的整合，在設計或實用的階段，讓硬體線路可以依照環境要求，在有限的計算資源中，自主的規劃出
可用的、具適應性的硬體系統，使系統可以持續面對外在環境變化並有效的運作。運用 EHW 的設計
概念，可以減低重新製造所需消耗的成本與時間。早期的 EHW 研究礙於計算機功能與演化式演算法
與資料結構上的限制，僅能推導出小型的線路，無法設計較複雜的線路結構，scalability 受到相當限制。
近年來，隨著計算資源成本的降低，CPU 效能的進步與記憶體容量的大幅提昇，加上相關演算法的革
新與晶片系統技術的推波助瀾，以往原本不容易將 AI 實現在 EHW 硬體系統上的問題，逐漸露出曙光。 
 
本方法執行步驟如下： 
Step-1.  在進行 EA 的演算時，需記錄過去 m 代的 FF 值，確定其變動量為一
穩定的值。 
Step-2.  記錄每一輸出點的在過去 m 代的滿足規格的錯誤次數(error counts)。 
Step-3.  在一條 EHW 染色體中取出 k 個 error counts 最小的輸出點，並檢查其
是否有包含的關係。 
Step-4.  利用 entropy 計算每個輸出點到輸入端在 EA 過程的複雜度。去除獨
立的最多輸出點為 PP 的集合。其中 entropy 計算預計考慮染色體片段
的錯誤率(εi)與染色體片段的結構密度(di) 
∑∑ −
=
−
=
××−=−
1
0
1
0
)log()(log
n
i
iiii
n
i
ii ddpp εε  
Step-5.  取出相對 entropy 表現較好的輸出點作為 MEPP 集合。 
如果 EHW 結構已經進入穩定狀態，表示 EHW 中有部分的有效線路結構已經找到，
該部分線路在未來的演化計算中通常不會出現很大的改變。因此，剩下來的 EHW 計
算可以針對剩餘結構進行演化計算即可。剩餘的 EHW 結構也找到穩定且最佳的結構
後，再將所有 EHW 組合起來即可。穩定結構的取出方法，我們定義「穩定結構的子
EHW(stable sub-circuits of EHW, sSCkt)」，設計如下。由 MEPP 往輸入方向取出的所有
線路元件，稱為 sSCkt。從一 MEPP 的集合中，取出每一個元素 pi∈MEPP，依照 pi
往輸入端取出樹狀結構，並做適當的位置取代。初步進行構想如下： 
Step-1.  pi往輸入端取出所有相關元件與線路，並以樹狀結構與其他 EHW 線路
切開。形成一子 sSCkt 稱為 sSCkt(pi)。 
Step-2.  如果 sSCkt(pi)與其他線路相連接，連接點為 sSCkt(pi)的 internal node，
複製並保留該 internal node 到輸入端的節在原 EHW 結構中 
Step-3.  從 EHW 中刪除 sSCkt(pi) 
Step-4.  重複 Step-1~Step 4 直到所有 sSCkt(pi)完全分離。 
將此部分移除後，以 EHW 進行剩下的線路的 EHW 計算。最後，利用合併 EHW 子
結構的方法，將兩個圖形做融合(fusing)的動作，但是要做到最佳化。進行構想如下： 
Step-1.  對齊輸入端與輸出端 
Step-2.  從輸入端開始，若 c’與 c’’’的輸出入與使用元件都相同，融合兩元件
並繼續此一程序 
Step-3.  將融合後的 EHW 線路進行最佳化調整。本階段預計使用 GA 做結構
最佳化的調整。 
最後再將所有線路進行整合與最佳化修正。本構想是一種 divide-and-conquer 的作法。
搭配 EA 演化運算的改善，將可以有效提升 EHW 計算規模(scalability)的問題，可以
進行較大型的線路計算。進行步驟說明如下： 
 
Start EA module
Initialization 
 
Fitness calculation  
Mutation  
 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                         九十八年十月二十六日 
報告人姓名 吳志宏 服務機構 
及職稱 
國立高雄大學 
電機工程學系 
副教授 
     時間 
會議 
     地點 
2009年 08月 20日~24日 
韓國 濟州島 
本會核定 
補助文號 
NSC 97-2221-E-390-020- 
會議 
名稱 
 (中文) 2009 IEEE 模糊系統國際研討會 
 (英文) 2009 IEEE International Conference on Fuzzy Systems 
發表 
論文 
題目 
 (中文) 使用軟性計算技術於全球衛星定位系統幾何稀釋因子逼近回歸模
式的比較探討 
 (英文) A Comparative Study on Regression Models of GPS GDOP Using 
Soft-Computing Techniques 
報告內容： 
一、 參加會議經過 
2009年 IEEE模糊系統國際研討會研討會(2009 IEEE International Conference on 
Fuzzy Systems) 於 98年 8月 20日至 8月 24日於韓國濟州島的「濟州島國際會議中
心」--ICC Jeju (International Convention Center Jeju)舉行。這是 IEEE Fuzzy Systems  
Society 每年一度針對模糊系統的理論與實作所舉辦之研討會，也是國際學術界智慧
型系統、模糊邏輯等領域非常受到重視的會議。本次會議的主要目的是聚集世界各地
大學、產業界、及研究單位從事有關模糊系統的專家學者於一堂，共同切磋、貢獻與
發表個人成就，並交換其在該領域的研究心得和最新知識，以吸引更多的學者加入本
領域的研究行列。與會人士來自世界各地；台灣學者長期投入該領域研究，成果斐然，
其曝光度逐漸受到重視，並成功爭取該會議在 2011年於台灣台北舉辦。 
本年度會議由 IEEE 為主要發起單位，並由 IEEE Computational Intelligence 
Society、Korean Institute of Intelligent Systems Society贊助協辦單位。由韓國先進科技
研究院（Korea Advanced Institute of Science and Technology, KAIST)、中央大學
（Chung-Ang University）、延世大學（Yonsei University）、西江大學（Sogang 
University）、漢陽大學（Hanyang University）與日本、中國、新加坡等亞洲國家相關
領域學者協助辦理。會議接受 383論文(含 302 oral 與 81 poster presentations)。為了
使相關議題能充份討論，共規劃了 63個議題場次，3個場次的獨立演講，與 10場訓
練課程(tutorial)。 
在會議舉行的前一天，本人即從桃園出發到韓國濟州島，並在會議當天當天隨即
前往濟州島國際會議中心（ICC）參加會議。ICC劃了獨立的會議大樓與活動區域，
實屬任何會議或社交盛會的理想之選。ICC設有十數個中、小型會議廳暨宴會廳，會
識庫的整合中非常熱門的一個研究方向，廣義而言，甚至可將機器學習領域納入。此
次會議發表的文章均是將傳統資料挖掘技術再做更深入的變化，以符合不同的需求。
而在機器學習部份，則包含加強式學習、模糊學習、神經網路式學習及資料群聚等文
章。 
2. 模糊系統之應用：此部份較強調模糊系統的實際應用，包括了交通控制、錯
誤診斷、工業控制、醫藥系統、影像處理、機器視覺、財務及市場分析預測、語音處
理、環境監測、電力系統等，均為相當重要且常見的應用。部份筆者較感興趣的內容
為系統控制：系統控制為目前人機整合中相當重要的一個關鍵研究領域。此次發表的
文章，包括了模糊控制、神經網路控制、混合式控制、及控制應用等。另外，機器人
學及影像處理：此部份主要探討和機器人設計相關之各項議題，包括了機器人設計方
法之發展，可移式機器人設計、機器人學習、機器人應用及影像處理及圖形識別等技
術。網際網路技術與應用：此部份主要是探討如何有效地促進網際網路之效能及增廣
其應用，並能有效地搜尋網際網路的資料，這對於網際網路發達的今日資訊社會相當
的重要。此部份的文章包括了遠距教學、Internet 及 Intranet 之整合、網頁搜尋技術、
合作式系統對於網際技術之應用，網站瀏覽技術、模糊理論應用於網站上及相關實際
系統應用等。人機介面：此部份主要是在探討人與機械之間的相互影響及系統的建
構，包括了人機間合作技術、人與電腦之間的關係探討及人機介面之設計。 
從會議中不難發現，在系統日益複雜的趨勢中，跨領域的整合研究是必然的趨勢。
例如利用 Fuzzy Soft Computing 來進行智慧型控制、機器人控制等、將多智慧型控制
系統應用於飛行、航海應用等等，都需要跨領域技術的整合。對未來的研究方向頗有
啟發性。 
 
三、 考察參觀活動 
本次前往韓國濟州島，僅能在會議所在地進行參訪，並未能深入了解韓國大學如
何培養研究人才，及實現高等教育的目標，但從參與會議的學者口中得知，亞洲人在
韓國大學進修人數不在少數，多數是中國大陸與日本人。對於國內大學如何吸引亞洲
國家學生來台接受高等教育，頗有收穫。 
 
四、 建議 
1. 此次承蒙國科會工程處計劃補助，方得成行，在此先致上十二萬分的謝意，也希
望爾後國科會及教育部對於參加國際知名會議之學者均能大力支持。 
2. 國內有不少研究此方面的學者，而每年也都會在相關的國際或國內研討會看到國
內學者的投稿和參與，增加國內外學者互相切磋交流的機會，甚至彼此合作，這
A Comparative Study on Regression Models of GPS GDOP Using
Soft-Computing Techniques
Chih-Hung Wu, Member, IEEE and Wei-Han Su
Abstract— Global Positioning System (GPS) has been used
extensively in various fields. One key to success of using GPS
is the positioning accuracy. Geometric Dilution of Precision
(GDOP) is an indicator showing how well the constellation
of GPS satellites is organized geometrically. It is known that
increasing the number of satellites for positioning reduces
GDOP. However, the calculation of GDOP is a time- and power-
consuming task which can be done by solving measurement
equations with complicated matrix transformation and inver-
sion. Previous studies have partially solved this problem with
artificial neural network(ANN). Though ANN is a powerful
function approximation technique, it needs costly training and
the trained model may not be applicable to data deviating too
much from the training data. Using the technique of support
vector regression (SVR), this paper presents the effectiveness of
SVR for GDOP approximation. The experimental results show
that SVR needs less training time to generate a precise model
for GDOP than ANN does.
Index Terms— GPS, GDOP, Support Vector Regression
I. INTRODUCTION
With the popularity of consumer GPS receivers, GPS has
become a convenient tool for positioning and navigation. In
GPS applications, the dilution of precision (GDOP) is often
used as a criteria to select a subset of n ≥ 4 satellites from
all available ones for computing the receiver’s position based
on pseudoranges. By linearizing the pseudorange equation
with Taylor’s series expansion at the approximate receiver
position, the relationship between pseudorange difference
(∆ρi) and positioning difference (∆xi) can be summarized
as follows[1]:
∆ρ1
∆ρ2
∆ρ3
...
∆ρn
 =

e11 e12 e13 1
e21 e22 e23 1
e31 e32 e33 1
...
en1 en2 en3 1


∆xu
∆yu
∆zu
c∆tb
+

vρ1
vρ2
vρ3
...
vρn
 .
(1)
Eq.(1) can have a general form represented as
z = Dx+ v, (2)
where the geometry matrix D is n × 4, n ≥ 4, since it is
necessary to use at least 4 satellites to determine a position in
a 3-dimension space. The matrix D is formed with direction
cosines ei1, ei2, and ei3 from the receiver to the ith satellite,
Chih-Hung Wu (the corresponding author) is with the Department of Elec-
trical Engineering, National University of Kaohsiung, Kaohsiung, Taiwan;
E-mail: johnw@nuk.edu.tw; Tel: +886-7-5919446; Fax: +886-7-5919374
Wei-Han Su is with POLSTAR Technologies Inc.,Zhubei City, Hsinchu
County, Taiwan
and vρi denotes a random noise with an expected value of
0.
Such an overdetermined system like Eq.(2) usually has
no exact solution. Since the n columns of D are linearly
independent, the linear least squares solution can be obtained
by solving the normal equations
DTz = DTDxˆ+DTv, (3)
where DT denotes the transpose of D. D has full rank and
DTD is invertible, then we can have
xˆ = (DTD)−1DTz. (4)
GDOP becomes a linear least squares solution of a linearized
pseudorange equation by taking the difference between the
estimated and the true positions. Therefore,
x˜ = (DTD)−1DTv. (5)
The quality of this solution is evaluated by cov(·) which
denotes the covariance of a measurement.
cov(x˜) = (DTD)−1DTcov(v)((DTD)−1DT)T. (6)
If all components of v are pairwise uncorrelated, cov(v) can
be normalized to an identity matrix, cov(v) = I, a simplified
expression of Eq.(6) can be obtained as
cov(x˜) = (DTD)−1. (7)
This quantifies the magnification of pseudorange errors onto
the user position errors. Let M=DTD be the measurement
matrix. The GDOP factor is defined as:
GDOP =
√
trace(M−1) =
√
trace[adj(M)]
det(M)
(8)
Existing methods for GDOP calculation include, matrix
inversion, closed-form algorithms, maximum volume of
tetrahedrons[1], etc. In order to obtain the optimal combi-
nation of satellites with lowest GDOP, these methods usu-
ally need considerable computational power for exhaustively
examining all possible combinations of satellites. It could
be inflexible when these methods are employed in mobile
devices.
Instead of directly solving the GDOP equations and matrix
inversion, Simon and El-Sherief [2] rephrase the calculation
of GDOP as regression/approximation problems and employ
the technique of artificial neural networks(ANNs) to solve
such problems. Though ANN is a powerful function approx-
imation technique, it needs costly training and the trained
model may not be applicable to data deviating too much from
• Type 2: The second type of training pattern is from the
measurement matrix M. Since M is well-conditioned
and positive definite, that is, it has full rank, we have
M=HTH by using the Cholesky decomposition [9],
where H is an upper triangular matrix. It gives
H =

h1 h2 h3 h4
h5 h6 h7
h8 h9
symmetrical h10
 , (16)
hk=Hi,j , 1 ≤ i ≤ j ≤ 4, k = 1 . . . , 10. The inputs
to the regression engine are (h1, h2, . . . , h10) and the
outputs are (λ−11 , λ
−1
2 , λ
−1
3 , λ
−1
4 )
T .
• Type 3: The third pattern is derived from type 1, by sub-
stituting its output as GDOP. That is, (f1, f2, f3, f4)T
are inputs and GDOP is the direct output of the regres-
sion engine.
• Type 4: The forth pattern is derived from type 2, by
also substituting its output as GDOP. That is, the inputs
are (h1, h2, . . . , h10) and the output is GDOP.
It is reported that GDOP approximation using ANNs obtains
less accuracy both in the training and testing phases when
data are represented in the form of type 1 and type 2. This
phenomenon can be explained by the fact that ANNs for
type 1 and 2 have complicated structures which increase
the complexity for satisfying the four outputs simultaneously
under the principle of ERM.
In this study, we define the GDOP is approximated by a
function in the form of Eq.(9). Training patterns of SVR are
in the forms of type 3 and 4. Regression with training patterns
in the form of type 1 and type 2 can be done by combining
four SVR’s with the four outputs connected as one. How-
ever, this also increase the computational complexity. GDOP
approximation can then be done by training SVR with type
3 and 4 data as the inputs and GDOP as the direct output.
Here, the kernel function used in SVR is the RBF kernel,
i.e., k(xi, xi)=exp(−γ||xi−xj ||2), with C=125, ε=0.001 and
γ=10.5. RBF kernel is widely used due to its simplicity and
the ability to optimize non-linear problems. The effectiveness
of the SVR model is evaluated by MAE and Mean Square
Error (MSE)= 1n
∑n
i=1(xi−x′i)2, where xi is the GDOP from
direct matrix inversion and x′i is output from the regression
model obtained from SVR, n is the number of data.
IV. EXPERIMENTS
In order to demonstrate the performance of the proposed
method, we have to collect enough training data. For this
purpose, a GPS receiver, GARMIN GPS-35, is connected to
a notebook computer via RS-232 for collecting signals from
GPS satellites. These equipments are installed on the location
of author’s institute. The position is in ECEF coordinate
[5082514.841, -2965783.917, 2452159.094]. GPS signals are
collected for several days. Signals in NMEA-0183 format are
recorded every 30 seconds. Coordinates in WGS-84 ECEF
format are computed. In these settings, more than 2880
data pairs are obtained. For forming the geometry matrix
D, signals from 4 different satellites are randomly selected,
0.4000
0.5000
0.6000
0.7000
0.8000
100 200 300 400 500 600 700 800 900 1000
# of data sampled
MSE
90% 80% 70% 60%
(a) Training SVR-4
0.0092
0.0094
0.0096
0.0098
100 200 300 400 500 600 700 800 900 1000
MSE
90% 80% 70% 60%
(b) Training SVR-10
1
1.5
2
2.5
3
M100 M200 M300 M400 M500 M600 M700 M800 M900 M1000
MSE
10% 20% 30% 40%
(c) Validation of SVR-4
1
1.5
2
2.5
3
M100 M200 M300 M400 M500 M600 M700 M800 M900 M1000
MSE
10% 20% 30% 40%
(d) Validation of SVR-10
Fig. 1. Results of Training and Validation
from which GDOP is computed by Eq.(8). The eignvalues of
M=DTD are extracted accordingly. The proposed method
is implemented in C# and installed on a personal computer
with a Intel Core-2 6300 CPU and 2G memory. The train-
ing/testing data are randomly selected from the database. For
convenience, the regression model obtained from type 3-data
is referred to as SVR-4 and the one from type 4-data as SVR-
10. To avoid the biased results, all experiments are repeated
under the same settings for 10 times. All results are presented
in average.
First, data collected in the first 24 hours are used as the
training data for SVR. Various proportions of data (100–1000
points) are randomly selected from these data sets for training
and validating the regression models. In Figure 1(a) and (b),
90%–60% of such data are randomly selected for training
SVR. The rest 10%–40% of data are used for verifying the
training models. The results are shown in Figure 1(c) and (d),
where Mnnn denotes the model obtained by using nnn points
of training data. The training results show that SVR-10 has
better training convergence than SVR-4. Interestingly, as the
number of training data increases, MSE in SVR-4 converges
up to 0.76%; while MSE in SVR-10 goes down to 0.0093%.
Additionally, SVR-4 outperforms SVR-10 in the phase of
validation. It may conclude that SVR-10 overfits GDOP than
SVR-4 does due to the number of constraints associated with
the input features.
