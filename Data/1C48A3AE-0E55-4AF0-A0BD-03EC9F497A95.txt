  
一、中文摘要 
霍夫曼演算法  (Huffman algorithm) 可以提供一個擁有最小多餘量  (minimum 
redundancy) 的最佳前綴碼 (optimal prefix code)，亦即霍夫曼碼 (Huffman code)，是一
個廣為人知的事實。由 Shannon 的無雜訊之通道編碼理論，我們已知霍夫曼碼的多餘量
是介於 0 和 1 之間。當我們在使用霍夫曼碼時，前提是假設要對一個序列的訊源符號編
碼；然而當我們只需對一個訊源符號編碼時，最佳的編碼方法並非霍夫曼碼，而是一對
一碼。舉例來說，若我們每次傳送一個訊息就必須等待對方確認 (acknowledge) 後方可
繼續傳送下一個訊息，這種情況下就適合使用一對一碼。此外，一對一碼的編碼方式是
將每個訊源符號對映到不同的碼字 (codeword)，所以也稱為非奇異碼 (nonsingular codes)。 
在這個計畫中我們考慮兩種不同的一對一碼，即文獻中分別稱為 {0,1}+-encodings 
和 {0,1}∗-encodings 的兩種一對一碼，並對這兩種編碼方式之下的最佳一對一碼之平均長
度作深入的理論分析與探討。我們研究在訊源符號的最大機率 p1 知道的情形下，推導出
上述兩種最佳一對一碼的平均長度的上界。當訊源符號的最大機率 p1 介於 1/3 和 1 之間
時，我們推導出最佳 {0,1}+-encodings 一對一碼的最佳上界；當訊源符號的最大機率 p1
介於 0.3136 和 1 之間時，我們推導出最佳 {0,1}∗-encodings 一對一碼的最佳上界。此外，
我們也提供一個迭代演算法來計算出更好的上界。我們所得到的最佳一對一碼的上界，
是目前已知文獻中最佳的上界。 
 
關鍵詞：非前綴碼、非奇異碼、一對一碼、多餘量、訊源編碼 
 
二、英文摘要 
One-to-one codes are nonsingular codes that assign a distinct codeword to each source 
symbol. One-to-one codes could be employed when one only needs to transmit a single 
source symbol rather than a sequence of source symbols. For example, such a situation can 
arise when the last message must be acknowledged before the next message can be 
transmitted. In this project, we consider two slightly different types of one-to-one encodings, 
namely, {0,1}+-encodings and {0,1}∗-encodings, depending on whether the empty codeword 
is used or not. Given that the probability p1 of the most likely source symbol is available, we 
obtain new upper bounds on the redundancy of an optimal {0,1}+-encoding which are tight 
for 1/3 ≤ p1 ≤ 1, and new upper bounds on the redundancy of an optimal {0,1}∗-encoding 
which are tight for 0.3136 ≤ p1 ≤ 1. Moreover, an iterative algorithm is given for further 
sharpening the obtained new upper bounds. Our results improve on the best upper bounds 
known in the literature, and are the best known upper bounds in the literature.  
 
Keywords: Non-prefix codes, nonsingular codes, one-to-one codes, redundancy, source 
coding. 
 
 
 
 
 
codewords needs to be transmitted. For example, such a situation can arise when the last
message must be acknowledged before the next message can be transmitted.
In the literature, two different types of one-to-one encodings, namely, {0, 1}+-encodings
and {0, 1}∗-encodings, have been considered depending on whether the empty codeword ǫ
is allowed or not. For {0, 1}+-encodings, the empty codeword ǫ is not allowed and the
set of available codewords (the codebook) is {0, 1, 00, 01, 10, 11, 000, 001, . . .}. On the other
hand, for {0, 1}∗-encodings, the empty codeword ǫ is allowed and hence the codebook is
{ǫ, 0, 1, 00, 01, 10, 11, 000, 001, . . .}. Clearly, in an optimal {0, 1}+-encoding, the source sym-
bol i is assigned the ith shortest codeword of length ⌊log (i+ 1)⌋ in the codebook for all
i = 1, 2, . . .. As such, the average codeword length L(X) of an optimal {0, 1}+-encoding for
X is given by L(X) =
∑∞
i=1 pi⌊log (i+ 1)⌋. Similarly, in the optimal {0, 1}
∗-encodings, the
source symbol i is assigned the ith shortest codeword of length ⌊log i⌋ in the codebook for
all i = 1, 2, . . ., and the average codeword length Lǫ(X) of an optimal {0, 1}
∗-encoding for X
is given by Lǫ(X) =
∑∞
i=1 pi⌊log i⌋. As usual, we call L(X)−H(X) (resp. Lǫ(X)−H(X))
the redundancy of an optimal {0, 1}+-encoding (resp. {0, 1}∗-encoding) for X. We note,
however, that L(X) − H(X) and Lǫ(X) − H(X) are called anti-redundancy in [5] as they
could be negative in some occasions.
B. Review of upper bounds on the redundancy L(X)−H(X) and the redundancy
Lǫ(X)−H(X) in the literature
First note that it is easy to see from L(X) ≤ LUD(X) and (1) that
L(X)−H(X) ≤ 1. (2)
In [6], Wyner showed that
Lǫ(X)−H(X) ≤ 0. (3)
Note that both the upper bounds in (2) and (3) are tight as they are achieved by a constant
random variable.
Given that the probability p1 of the most likely source symbol is available, the best known
upper bound on the redundancy L(X)−H(X) is given in [7] as
L(X)−H(X) ≤
{
(log 3) · p1 +
(
log 4
3
)
·
(
(k − 1)p1 +
1+p1
2k
)
− 1, if 0 < p1 ≤ 0.5,(
log 4
3
)
·
(
1−p1
2
)
−Hb(p1) + 1, if 0.5 ≤ p1 ≤ 1,
(4)
A. New Upper Bounds on the Redundancy L(X)−H(X) When p1 Is Available
In the rest of this report, we denote Y as a discrete random variable with probability
distribution q = (q1, q2, . . .), where qi =
pi+1
1−p1
, i = 1, 2, . . .. It is well known that
H(X) = Hb(p1) + (1− p1)H(Y ). (6)
We first give an upper bound on the redundancy L(X) − H(X) that is tight for 0.5 ≤
p1 ≤ 1.
Lemma 1 For 0 < p1 ≤ 1, we have
L(X)−H(X) ≤
{
0, if 0 < p1 ≤ 0.5,
1−Hb(p1), if 0.5 ≤ p1 ≤ 1.
(7)
Furthermore, the upper bound in (7) is tight for 0.5 ≤ p1 ≤ 1.
Proof. Note that ⌊log(i+ 1)⌋ ≤ ⌊log(i− 1)⌋+ 1 for i ≥ 2. As such, we have
L(X) =
∞∑
i=1
pi⌊log(i+ 1)⌋ ≤ p1 +
∞∑
i=2
pi (⌊log(i− 1)⌋+ 1)
= 1 + (1− p1)
∞∑
i=1
qi⌊log i⌋ = 1 + (1− p1)Lǫ(Y ).
It then follows from (6) and (3) that
L(X)−H(X)≤ 1−Hb(p1) + (1− p1) (Lǫ(Y )−H(Y )) ≤ 1−Hb(p1), (8)
where the above two inequalities hold with equalities if p = (p1, 1 − p1, 0, 0, . . .) and 0.5 ≤
p1 ≤ 1. In other words, the upper bound in (8) is tight for 0.5 ≤ p1 ≤ 1.
In the rest of the proof, we assume that 0 < p1 ≤ 0.5 and derive a sharper upper bound
than that in (8). Note that as 1 =
∑∞
j=1 pj ≥
∑i
j=1 pj ≥
∑i
j=1 pi = ipi, we have pi ≤
1
i
for
In the following, we give an expression of L(X) −H(X) in terms of L(Y ) − H(Y ) and
an expression of L(X) −H(X) in terms of Lǫ(X)− H(X) that will be very useful later in
our derivations of sharper upper bounds on the redundancy L(X)−H(X).
First observe that
L(X) = Lǫ(X) +
∑
j≥1
p2j−1.
In [13, Lemma 3.1], Blundo and De Prisco showed that
∑
j≥1
p2j−1 ≤ (k − 1)p1 +
1 + p1
2k
,
where k = ⌊log (1 + 1/p1)⌋. As such, we have the following expression of L(X) − H(X) in
terms of Lǫ(X)−H(X):
L(X)−H(X) ≤ Lǫ(X)−H(X) + (k − 1)p1 +
1 + p1
2k
, (10)
where k = ⌊log (1 + 1/p1)⌋.
Furthermore, it is not difficult to see that
L(X) =
∞∑
i=1
pi⌊log(i+ 1)⌋ = p1 + (1− p1)
∞∑
i=1
qi⌊log(i+ 2)⌋
= p1 + (1− p1)
(
∞∑
i=1
qi⌊log(i+ 1)⌋+
∑
j≥1
q2j+1−2
)
= p1 + (1− p1)
(
L(Y ) +
∑
j≥1
q2j+1−2
)
.
It then follows from (6) that we have the following expression of L(X)−H(X) in terms of
L(Y )−H(Y ):
L(X)−H(X) = p1 + (1− p1)
(
L(Y )−H(Y ) +
∑
j≥1
q2j+1−2
)
−Hb(p1). (11)
We also need the following lemma to obtain our new upper bound on the redundancy
L(X)−H(X) in Theorem 3 below.
where we used (14), (15), and the fact that p2j+1−2 ≤ p1 for all j ≥ 1, in that order.
Note that it can be easily verified that
(k − 1)p1 +
1 + 2p1
2k+1
< kp1 +
1 + 2p1
2k+2
if and only if log (1 + 1/(2p1)) < k + 1. Similarly,
(k − 1)p1 +
1 + 2p1
2k+1
≤ (k − 2)p1 +
1 + 2p1
2k
if and only if log (1 + 1/(2p1)) ≥ k. As such, we have the upper bound in (16) if
k ≤ log
(
1 +
1
2p1
)
< k + 1
for some k ≥ 1. In other words, if 0 < p1 ≤ 0.5, then we have the upper bound in (16),
where k = ⌊log(1 + 1/2p1)⌋. The proof is completed by combining (13) and (16).
Now we state our new upper bound on the redundancy L(X) − H(X) of an optimal
{0, 1}+-encoding for X.
Theorem 3 For 0 < p1 ≤ 1, we have
L(X)−H(X)
≤


(log 3) · p1 +
(
log 4
3
)
·
(
(k − 1)p1 +
1+p1
2k
)
− 1, if 0 < p1 ≤ 0.3043,
1+5p1
4
−Hb(p1), if 0.3043 ≤ p1 ≤ 1/3,
1+p1
2
−Hb(p1), if 1/3 ≤ p1 ≤ 0.4506,
2(1− 2p1)−Hb(2p1), if 0.4506 ≤ p1 ≤ 0.5,
1−Hb(p1), if 0.5 ≤ p1 ≤ 1,
(17)
where k = ⌊log(1 + 1/p1)⌋. Furthermore, the upper bound in (17) is tight for 1/3 ≤ p1 ≤ 1.
Proof. We only prove the upper bound for 0 < p1 ≤ 0.5 as the upper bound for 0.5 ≤ p1 ≤ 1
has already been given in Lemma 1.
By substituting (5) into (10), we have
L(X)−H(X)
≤
{
p1 log
4kk+1
(k+1)k
+ log k+1
2k
−Hb(p1) + (m− 1)p1 +
1+p1
2m
, if 0 < p1 ≤ 0.5,
3−p1
2
−Hb(p1), if 0.5 ≤ p1 ≤ 1,
(18)
where k = ⌊1/p1⌋ and m = ⌊log (1 + 1/p1)⌋.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
q
 
 
r(q)
r*(q)
q=0.5 q=0.8201
Figure 1: The functions r(q) and r∗(q).
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
p1
 
 
Upper bound (4)
Upper bound (18)
Upper bound (21)
p1=0.3043
min {(4), (18), (21)}
Figure 2: The upper bounds in (4), (18), and (21) for 0 < p1 ≤ 0.5.
where k = ⌊log(1 + 1/q)⌋. It then follows from q1 =
p2
1−p1
≤ p1
1−p1
that
L(Y )−H(Y ) ≤ rǫ(q1) ≤ r
∗
ǫ
(
p1
1− p1
)
. (25)
By substituting (25) into (22) and after some algebra, we obtain
Lǫ(X)−H(X)
≤


(log 6) · p1 +
(
log 4
3
)
·
(
(k − 1)p1 +
1
2k
)
−Hb(p1)− 1, if 0 < p1 ≤ 0.2333,
1−4p1
4
−Hb(2p1), if 0.2333 ≤ p1 ≤ 0.25,
−0.2516(1− p1)−Hb(p1), if 0.25 ≤ p1 ≤ 0.3136,
2− (4 + 3 log 3)p1 −Hb(3p1), if 0.3136 ≤ p1 ≤ 1/3,
1− 3p1 −Hb(2p1), if 1/3 ≤ p1 ≤ 0.5,
(26)
where k = ⌊log(1/p1)⌋.
It can be verified that for 0 < p1 ≤ 0.5, the upper bound in (26) is better than that
in (5). As such, the upper bound in (23) follows by combining (24) and (26). Note that
for 0.3161 ≤ p1 ≤ 0.5 the upper bound in (23) is tight as it is achieved by a source with
probability distribution p = (p1, p1, p1, 1−3p1, 0, 0, . . .) if 0.3161 ≤ p1 ≤ 1/3, and by a source
with probability distribution p = (p1, p1, 1− 2p1, 0, 0, . . .) if 1/3 ≤ p1 ≤ 0.5.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
q
 
 
r
ε
(q)
r
ε
*(q)
q=1/3 q=0.4568
Figure 3: The functions rǫ(q) and r
∗
ǫ (q).
Update R(p1) by executing R(p1)← min {R1(p1), R2(p1)} for 0 < p1 ≤ 1.
Step 4 (Update the auxiliary function r∗ǫ (q)): r
∗
ǫ (q)← maxp1≤q R(p1) for 0 < q ≤ 1.
Step 5 (Update Rǫ(p1)): Let Rǫ,0(p1) = Rǫ(p1) and update Rǫ(p1) by executing
Rǫ(p1)←
{
(1− p1)r
∗
ǫ (
p1
1−p1
)−Hb(p1), if 0 < p1 < 0.3136,
Rǫ(p1), if 0.3136 ≤ p1 ≤ 1.
Step 6 (Termination): If Rǫ,0(p1)−Rǫ(p1) ≤ δ for all 0 < p1 ≤ 1, then stop. Otherwise,
go to Step 2 and a new iteration is started.
We remark that as the upper bound in (17) is already tight for 1/3 < p1 ≤ 1, we only
need to set R1(p1) and R2(p1) as R(p1) for 1/3 < p1 ≤ 1 in Step 3 of Algorithm 5. Similarly,
we only need to set Rǫ(p1) as Rǫ(p1) for 0.3136 < p1 ≤ 1 in Step 5 of Algorithm 5 as the
upper bound in (23) is already tight for 0.3136 < p1 ≤ 1.
Furthermore, in Step 3 of Algorithm 5, we update R(p1) by taking the minimum of R1(p1)
and R2(p1) (instead of taking the minimum of R1(p1), R2(p1), and the upper bound in (4))
as it can be shown that the upper bound R1(p1) is better than that given by (4).
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1−1.5
−1
−0.5
0
0.5
1
p1
 
 
Upper bound (4)
Upper bound (17)
Upper bound (5)
Upper bound (23)
 R(p1) given by Algorithm 5
 R
ε
(p1) given by Algorithm 5
Figure 4: Upper bounds on the redundancy L(X)−H(X) given by (4), (17), and Algorithm 5,
and upper bounds on the redundancy Lǫ(X) − H(X) given by (5), (23), and Algorithm 5
for 0 < p1 ≤ 1.
References
[1] C. E. Shannon, “A mathematical theory of communication,” Bell System Technical
Journal , vol. 27, pp. 379–423 (Part I), 623–656 (Part II), July, October 1948.
[2] T. M. Cover and J. A. Thomas, Elements of Information Theory , New York, NY: John
Wiley & Sons, 1991.
[3] R. G. Gallager, “Variations on a theme by Huffman,” IEEE Transactions on Information
Theory , vol. 24, pp. 668–674, November 1978.
[4] D. Manstetten, “Tight bounds on the redundancy of Huffman codes,” IEEE Transac-
tions on Information Theory , vol. 38, pp. 144–151, January 1992.
[5] W. Szpankowski, “One-to-one codes and its anti-redundancy,” in Proceedings IEEE In-
ternational Symposium on Information Theory (ISIT’05), Adelaide, Australia, Septem-
ber 4–9, 2005.
[6] A. D. Wyner, “An upper bound on the entropy series,” Information and Control , vol. 20,
pp. 176–181, 1972.
[7] J. Cheng, T.-K. Huang, and C. Weidmann, “New bounds on the expected length of op-
timal one-to-one codes,” IEEE Transactions on Information Theory , vol. 53, pp. 1884–
1895, May 2007.
[8] S. K. Leung-Yan-Cheong and T. M. Cover, “Some equivalences between Shannon en-
tropy and Kolmogorov complexity,” IEEE Transactions on Information Theory , vol. 24,
pp. 331–338, May 1978.
[9] J. G. Dunham, “Optimal noiseless coding of random variables,” IEEE Transactions on
Information Theory , vol. 26, p. 345, May 1980.
[10] J. Rissanen, “Tight lower bounds for optimum code length,” IEEE Transactions on
Information Theory , vol. 28, pp. 348–349, March 1982.
[11] E. I. Verriest, “An achievable bound for optimal noiseless coding of a random variable,”
IEEE Transactions on Information Theory , vol. 32, pp. 592–594, July 1986.
[12] N. Alon and A. Orlitsky, “A lower bound on the expected length of one-to-one codes,”
IEEE Transactions on Information Theory , vol. 40, pp. 1670–1672, September 1994.
出席國際會議心得報告 
 
一、 會議名稱： 
(中文) ACM 2007 無線通訊暨行動計算國際會議 
(英文) ACM International Wireless Communications and Mobile Computing 
Conference (ACM IWCMC 2007) 
 
二、 會議時間： 
自 2007 年 8 月 12 日 至 2007 年 8 月 16 日 
  
三、 會議地點： 
美國 夏威夷州 檀香山市 (Honolulu, Hawaii, USA) 
 
四、 發表論文題目： 
     On the Expected Codeword Length Per Symbol of Optimal Prefix Codes for Extended Sources 
 
五、 參加會議經過： 
  此會議每年舉辦一次，宗旨是提供機會讓世界各地在無線通訊暨行動計算相關
研究領域之學者專家交換研究的成果及心得。本次會議由 ACM 協會主辦，於 8 月
12 日至 8 月 16 日在美國夏威夷州檀香山市的 Turtle Bay Resort Hotel 舉行。本人於
8 月 12 日下午 2 點 30 分從桃園國際機場搭乘中華航空班機前往檀香山市，並於當
地時間 8 月 12 日上午 6 點 25 分抵達檀香山國際機場，之後隨即前往下褟的旅館安
置行李，並前往會議地點註冊。在會議結束後，本人於當地時間 8 月 16 日上午 7
點 50 分從檀香山國際機場搭乘中華航空班機離開，並於 8 月 17 日中午 12 點 10 分
抵達桃園國際機場。 
本次會議主題為：The Future is Now -- The New Era of Wireless Communications 
and Mobile Computing/Networking Technologies。會議依研究領域細分為 10 個研討
會 (symposium) 進行，本人擔任其中一個 session (Session T2-A: Communication and 
Information Theory Symposium--Coding) 的主持人，並在這個 session 報告一篇論文 
(論文題目為: On the Expected Codeword Length Per Symbol of Optimal Prefix Codes 
for Extended Sources)。 
 
 
交流，或參加重要的國際會議以尋求國際合作研究的機會。這樣一來，應可快
速提昇國內學術水平。 
2. 本人非常感謝國科會提供補助，使得此次會議得以成行。 
 
Let C be the concatenation of k1 C1’s and k2 C2’s, then
C is a prefix code for the extended source Xk1n1+k2n2 with
expected codeword length L(C) = k1L(C1) + k2L(C2) =
kn1Ln1(X)+k2n2Ln2(X). As C is not necessarily an optimal
prefix code for Xk1n1+k2n2 , Ln2(X) < Ln1(X), and k2n2 ≥
1, it follows that
Lk1n1+k2n2(X) ≤
L(C)
k1n1 + k2n2
=
kn1Ln1(X) + k2n2Ln2(X)
k1n1 + k2n2
<
kn1Ln1(X) + k2n2Ln1(X)
k1n1 + k2n2
= Ln1(X).
The theorem is proved.
It is well known that for positive integers k and n we have
Lkn(X) ≤ Ln(X). The following corollary to Theorem 1
gives more details about the two possible cases Lkn(X) <
Ln(X) and Lkn(X) = Ln(X).
Corollary 2 Let k ≥ 1 and n ≥ 1.
(i) If Lkn(X) < Ln(X), then Lk′n(X) < Ln(X) for all
k′ ≥ k.
(ii) If Lkn(X) = Ln(X), then Lk′n(X) = Ln(X) for all
1 ≤ k′ ≤ k.
Proof. (i) If Lkn(X) < Ln(X), then by taking n1 = n,
n2 = kn, and k2 = 1 in (2), we have L(k1+k)n(X) < Ln(X)
for all k1 ≥ 0. In other words, we have Lk′n(X) < Ln(X)
for all k′ ≥ k.
(ii) If Lkn(X) = Ln(X), then we show that Lk′n(X) =
Ln(X) for all 1 ≤ k′ ≤ k by contradiction. Suppose that
Lk′n(X) < Ln(X) for some 1 < k′ < k, then from part (i)
we have Lkn(X) < Ln(X), a contradiction.
We note that Corollary 2 implies that if Lk(X) = L1(X),
then Lk′(X) = L1(X) for all 1 ≤ k′ ≤ k. In Theorem 3
of [3], a sufficient condition is given such that Lk(X) =
L1(X). Our result shows that such a sufficient condition
indeed guarantees that Lk′(X) = L1(X) for all 1 ≤ k′ ≤ k.
In the following, we first recall a result in [3] relating the
probability of the most likely source symbol and the minimum
codeword length of an optimal prefix code for X . Then we
give sufficient conditions for Lkn(X) < Ln(X) in terms of k,
n, the probability of the most likely source symbol p1 and/or
the minimum codeword length of an optimal code for the
original source.
Lemma 3 [3] Let C be an optimal prefix code for X with
minimum codeword length `1(C), and let m ≥ 1. If
p1 <
1
2m − 1 , (3)
then `1(C) ≥ m, and if
p1 >
2
2m+1 + 1
, (4)
then `1(C) ≤ m. Therefore, if
2
2m+1 + 1
< p1 <
1
2m − 1 , (5)
then `1(C) = m, and if
1
2m+1 − 1 ≤ p1 ≤
2
2m+1 + 1
, (6)
then either `1(C) = m or `1(C) = m+ 1.
In the following theorem, we give a sufficient condition for
Lkn(X) < Ln(X) in terms of k, n, and the probability of the
most likely source symbol p1.
Theorem 4 Let k ≥ 2 and n ≥ 1. If(
2
2k(rn+s+1) + 1
)1/kn
< p1 <
(
1
2k(rn+s)+1 − 1
)1/kn
, (7)
where r ≥ 0 and 0 ≤ s ≤ n− 1, then Lk′n(X) < Ln(X) for
all k′ ≥ k.
Proof. By Corollary 2(i), it suffices to show that Lkn(X) <
Ln(X). Let C1 be an optimal prefix code for the extended
source Xn with minimum codeword length `1(C1). From (7),
we see that
pn1 <
(
1
2k(rn+s)+1 − 1
)1/k
<
1
2rn+s
<
1
2rn+s − 1 ,
and
pn1 >
(
2
2k(rn+s+1) + 1
)1/k
>
1
2rn+s+1
>
2
2rn+s+2 + 1
.
If r = s = 0, then we have from Lemma 3 that `1(C1) ≤ 1,
implying that `1(C1) = 1. On the other hand, if r ≥ 1 or
s ≥ 1, then we have either `1(C1) = rn + s or `1(C1) =
rn+ s+ 1.
Let C = Ck1 be the concatenation of k C1’s, then C is a
prefix code for the extended source Xkn, and the minimum
codeword length `1(C) of C is given by `1(C) = k`1(C1) = k
if r = s = 0, and is given by either `1(C) = k(rn + s) or
`1(C) = k(rn+ s+1) if r ≥ 1 or s ≥ 1. Note that from (7),
we have
2
2k(rn+s+1) + 1
< pkn1 <
1
2k(rn+s)+1 − 1 ,
and it follows from Lemma 3 that the minimum codeword
length, say `1, of an optimal prefix code for the extended
source Xkn must satisfy
k(rn+ s) + 1 ≤ `1 ≤ k(rn+ s+ 1)− 1.
As such, C can not be an optimal prefix code for Xkn. It then
follows from C = Ck1 and C1 is an optimal prefix code for
Xn that
Lkn(X) <
L(C)
kn
=
kL(C1)
kn
=
L(C1)
n
= Ln(X),
and the proof is completed.
The proof is completed.
Remark 10 (i) As k ≥ 2, we have
1
2k(rn+s+1)+1 − 1 <
1
(2rn+s+1 − 1)k <
1
2k(rn+s)+1 − 1
for 0 ≤ s ≤ n− 1. It follows that the intervals[(
1
2rn+s+1 − 1
)1/n
,
(
1
2k(rn+s)+1 − 1
)1/kn)
,
s = 0, 1, . . . , n− 1, are disjoint, and they are all contained in
the open interval[(
1
2n(r+1) − 1
)1/n
,
(
1
2knr+1 − 1
)1/kn)
.
Furthermore, as
1
2r+1
<
(
1
2n(r+1) − 1
)1/n
<
(
1
2knr+1 − 1
)1/kn
≤ 1
2r
,
we can see that none of the intervals[(
1
2n(r+1) − 1
)1/n
,
(
1
2knr+1 − 1
)1/kn)
contains a dyadic probability p1, which is a necessary condi-
tion for Lkn(X) < Ln(X) to hold in general.
(ii) From (11), we have
pn1 <
(
1
2k(rn+s)+1 − 1
)1/k
<
1
2rn+s
<
1
2rn+s − 1 ,
and
pn1 ≥
1
2rn+s+1 − 1 .
It follows from Lemma 3 that the minimum codeword length
of an optimal prefix code for the extended source Xn must
be equal to either rn + s or rn + s + 1. As such, the range
of p1 in (11) is consistent with the minimum codeword length
assumption that `1(C1) = rn+ s.
However, note that
(
1
2k(rn+s)+1−1
)1/k
is strictly increasing
in k and approaches 12rn+s as k tends to infinity. As 12rn+s >
2
2rn+s+1+1 , we can define
k∗ = min
{
k ≥ 2 :
(
1
2k(rn+s)+1 − 1
)1/k
>
2
2rn+s+1 + 1
}
.
It follows from Lemma 3 that when k ≥ k∗ and(
2
2rn+s+1 + 1
)1/n
< p1 <
(
1
2k(rn+s)+1 − 1
)1/kn
,
the minimum codeword length of an optimal prefix code for
the extended source Xn must be equal to rn+ s, and in this
case the assumption that `1(C1) = rn+ s is redundant
(iii) Observe that
(
2
2k(rn+s+1)+1
)1/kn
is strictly decreasing
in k and approaches
(
1
2rn+s+1
)1/n
as k tends to infinity. Since(
1
2rn+s+1
)1/n
<
(
1
2rn+s+1−1
)1/n
, we can define
k∗ = max
{
k ≥ 2 :
(
2
2k(rn+s+1) + 1
)1/kn
>
(
1
2rn+s+1 − 1
)1/n}
.
As such, we can see that when k ≤ k∗, the result in Theorem 9
is stronger than that in Theorem 4.
The following corollary is obtained by taking n = 1 and
hence s = 0 in Theorem 9.
Corollary 11 Let k ≥ 2. Suppose that C1 is an optimal prefix
code for X with minimum codeword length `1(C1) = r, where
r ≥ 1. If
1
2r+1 − 1 ≤ p1 <
(
1
2kr+1 − 1
)1/k
, (12)
then Lk′(X) < L1(X) for all k′ ≥ k.
We note that if the statement in Corollary 11 holds for k =
2, then Theorem 2 of [3] shows that L2(X) < L1(X), but
Corollary 11 guarantees that Lk′(X) < L1(X) for all k′ ≥ 2.
Therefore, Corollary 11 (and Theorem 9) is an extension and
generalization of Theorem 2 of [3].
III. CONCLUSIONS
In this paper, we derived some results on the behavior of the
expected codeword length per symbol Ln(X) of an optimal
prefix code for a discrete memoryless source X . We also gave
sufficient conditions for Lkn(X) < Ln(X) in terms of k, n,
the probability of the most likely source symbol p1 and/or the
minimum codeword length of an optimal code for the original
source. Based on these sufficient conditions, for any given
n ≥ 1 and non-dyadic pn1 , we obtained an integer k∗ ≥ 2
such that Lk′n(X) < Ln(X) for all k′ ≥ k∗. Our results
could be regarded as extensions and generalizations of those
in [3]. Finally, we would like to point out that the results in
this paper could be easily extended to the D-ary case by using
a result in [5] relating the probability of the most likely source
symbol p1 and the minimum codeword length of an optimal
D-ary prefix code for X .
ACKNOWLEDGMENTS
This research was supported in part by the National Science
Council, Taiwan, R.O.C., under Contract NSC-94-2213-E-
007-046, Contract NSC-95-2221-E-007-039, and the Program
for Promoting Academic Excellence of Universities NSC 94-
2752-E-007-002-PAE.
