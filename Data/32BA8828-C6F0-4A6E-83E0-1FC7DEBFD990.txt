11. Introduction
In this project researchers design,
implement and control the top-level
algorithms to compose the culture
independent music pattern that can be
parametrically controlled in real time. Fractal
algorithms, Chaos procedures, and Granular
implementations will be the basis for
computational generation of real-time,
parametrically controllable, to drive the
low-level sonic data streams. EEG data will
be collected from test subjects receiving the
data streams, along with a log of the
parametric determinants of the generated
audio streams. For the objective algorithmic
music composition for EEG data observation,
the parameterized MIDI data can be useful to
compose with fractal algorithm, and the EEG
shows the positive data with proper computer
music parameter settings.
2. Research Purpose
Musical Parameters for Music
Effect Cognition
Composers use the term ”parameter”to
refer to any quantifiable dimension of a
musical structure which can be isolated and
scaled according to a range of discrete
values.
The lower level parameters are similar to
what have been called “the elements of
music” for centuries : pitch,
rhythm ,articulation, dynamics, and timbre…
Figure 1. Music Parameters
Pitch Determination
In music, the technical term used to
describe how high or low a note is. It
depends on the frequency (number of
vibrations per second) of the sound, which is
measured in hertz (Hz). Pitch also refers to
the standard to which instruments are tuned.
Nowadays the internationally agreed-upon
pitch is the A above middle C (A4 or a'),
which has a frequency of 440 Hz (vibrations
per second). This is often known as concert
pitch. Pitch can now be measured accurately
by electronic tuning devices. These are
beginning to replace the traditional tuning
fork, but it is still normal practice for
orchestras to tune to an oboe playing A4.
Pitches may be described in various ways,
including high or low, as discrete or
indiscrete, pitch that changes with time and
the manners in which this change with time
occurs: gliding, portamento; or vibrato, and
as determinate or indeterminate.
3 legato - Legato is the opposite of
staccato. The notes are very
connected; there is no space between
the notes at all. There is, however,
still some sort of articulation that
causes a slight but definite break
between the notes (for example, the
violin player's bow changes direction,
the guitar player plucks the string
again, or the wind player uses the
tongue to interrupt the stream of air).
 slur - When notes are slurred, only the
first note under each slur marking has
a definite articulation at the beginning.
The rest of the notes are played (or
sung) so connectedly that there is no
break between the notes.
 tie - A tie looks like a slur, but it is
between two notes that are the same
pitch. A tie is not really an
articulation marking. It is included
here because it looks like one, which
can cause confusion for beginners.
When notes are tied together, they are
played as if they are one single note
that is the length of all the notes that
are tied together. The tie is used to
make music easier to read.
 Accents - An accent is usually
performed by making the accented
note, or the beginning of the note,
louder than the rest of the music.
Although this is mostly a quick
change in dynamics, it usually affects
the articulation of the note, too. The
extra loudness of the note usually
requires a stronger, more definite
attack at the beginning of the
accented note, and it is emphasized
by putting some space before and
after the accented notes. The effect of
a lot of accented notes in a row often
sounds marcato. Some accents may
even be played by using articulation
alone (not dynamics) to emphasize
the note.
Figure 3. The Example for Articulation
Velocity Determination
The force in which a note is pressed or
played (attacked) and released. (i.e., the
harder the note is pressed, the louder it will
sound.) The range of velocities is 0-127.
Some keyboards feature 'velocity sensitive
keys' and some drum machines and
percussion controllers may feature 'velocity
sensitive pads'. Velocity on wind controllers
may be controlled by the amount of air
stream produced into the instrument.
Melody Patterns
Melody is a musical and successive line
of single tones or pitches perceived as a unity.
Its characteristics include range, shape, and
movement. Each of these will be discussed
separately.
Range -- The range of a piece is the distance
between the lowest and highest tones.
Singers refer to an arrangement being in a
low, medium, or high range, meaning that the
5Parameterization
The In 1960’s, the electronic musical
instruments use oscillators, filters, and
amplifiers with electronic circuits to generate
sound and music. Later on not only the
analog electronic instruments, but the digital
instruments were invented based on the
Fourier Transform sound synthesis as the
Synthesizer. American and Japanese then
discussed the protocol for the common
standard communication standard interface
among synthesizers in1980’s, called MIDI
(Musical Instrument Digital Interface).
The following table defines the major
MIDI messages in binary with the“Channel
Voice Messages”for data communication.
Channel Voice Messages
Status
D7----D0
Data Byte(s)
D7----D0
Description
1000cccc 0nnnnnnn
0vvvvvvv
Note Off event.
This message is sent when a note is released (ended).
(nnnnnnn) is the note number. (vvvvvvv) is the velocity.
1001cccc 0nnnnnnn
0vvvvvvv
Note On event.
This message is sent when a note is depressed (start).
(nnnnnnn) is the note number.
1010cccc 0nnnnnnn
0vvvvvvv
Polyphonic Key Pressure (Aftertouch).
This message is sent when the pressure (velocity) of a
previously triggered note changes.
(nnnnnnn) is the note number.
(vvvvvvv) is the new velocity.
1011cccc 0nnnnnnn
0vvvvvvv
Control Change.
This message is sent when a controller value changes.
Controllers include devices such as pedals and levers. Certain
controller numbers are reserved for specific purposes. See
Channel Mode Messages.
(ccccccc) is the controller number.
(vvvvvvv) is the new value.
1100cccc 0ppppppp Program Change.
This message sent when the patch number changes.
(ppppppp) is the new program number.
1101nnnn 0ccccccc Channel Pressure (After-touch).
This message is sent when the channel pressure changes.
Some velocity-sensing keyboards do not support polyphonic
after-touch. Use this message to send the single greatest
velocity (of all the current depressed keys).
(ccccccc) is the channel number.
1110nnnn 0lllllll
0mmmmmmm
Pitch Wheel Change.
This message is sent to indicate a change in the pitch wheel.
The pitch wheel is measured by a fourteen-bit value. Center
(no pitch change) is 2000H.
Sensitivity is a function of the transmitter.
(llllll) are the least significant 7 bits.
(mmmmmm) are the most significant 7 bits.
Figure 4. The Channel Voice Message
Theory and Methodology of
Granulation
There are some methods to implement
the Granulation, however the fundamental is
using the concept of sound grain based on the
ADSR (Attack-Decay-Sustain-Release) curve.
However the most important thing is to
understand that Granulated sound is an
integration of grains, of elementary sonic
particles. The sonic stream can be divided
within a mili-second level, and very short
time window that ear is capable of registering
one event at a specific frequency. Granular
synthesis of sound is the generation of
thousands of short sonic grains which are
combined linearly to form large scale audio
events. A Sound Grain may be represented as
follows:
Attack
Decay
Sustain
Release
Time
Amplitude
Figure 5. The ADSR Curve for a Typical
Sound Wave
The sound grain can be shaped and
controlled by the Grain Envelope, which is a
critical parameter to design Granulated
sound.
Figure 6. Sound Grain Envelope Design with
Non-Linear Curve (a)
7sophisticated software permitted greater
accuracy and control of grains. When
performing AGS, the granular structure of
each "Cloud" is determined probabilistically
in terms of the following parameters:
1. Start time and duration of the cloud
2. Grain duration (Variable for the duration
of the cloud)
3. Density of grains per second (Also
variable)
4. Frequency band of the cloud (Usually high
and low limits)
5. Amplitude envelope of the cloud
6. Waveforms within the grains
7. Spatial dispersion of the cloud
Obviously, AGS abandons the use of specific
algorithms and streams to determine grain
placement with regard to pitch, amplitude,
density and duration. The dynamic nature of
parameter specification in AGS results in
extremely organic and complex timbres.
Fractal Musical Forms
Fractal concepts result in varied ways to
create musical ideas. A simple way of doing
it is to use Mandelbrot equation for
generating musical material would be to
iterate the equation and associate the number
of iterations required to make the point reach
the infinity threshold with a particular pitch
of classes of pitch. This method will hardly
generate interesting music.
Music is a time based art form and as
such it requires different strategies than a
static figure requires to be fully appreciated.
A good way to generate fractal music
ideas is to ignore the beauty of the fractals
image and concentrate on the fundamental
mechanisms that generate self-similarity and
scaling invariance. The concept of self
similarity has been used in music for many
centuries. A clear example is the chorale of
Johann Sebastian Bach included in his Art of
Fuge in which self similarity appears from
the repeated use of the same motifs within
one large section of the piece.
3. Result and Discussion
MIDI Algorithmic Composition
Environment Setup
Even though we can use C language to
perform the MIDI algorithmic music
composition, it is more convenient to use
MAX program as the MIDI algorithmic
composition environment.
Figure 8. The Max Patch Example for
Algorithmic Composition
The Integration between the Virtual
Instrument and the Algorithmic Music
Pattern
The Virtual Instrument (Subproject 1) is
a generalized sound synthesizers, which can
be driven by the algorithmic music pattern.
The music pattern can be generated either
manually or automatically.
9blue lines show the ERPs induced by minor
second stimuli and the red line show the
ERPs induced by perfect fifth stimuli. We
can find that the characteristic of P200 makes
no significant difference between the
musicians and non-musician. However, the
musician’s N400 is larger than the
non-musician’s N400 when listening to
minor second and perfect fifth. In addition,
the minor second stimuli will induce larger
N400 then the perfect fifth for the musician.
Figure 11a. ERPs of a musician at Fz. The
blue line and red line show the ERPs induced
by minor second stimuli and perfect fifth
stimuli, respectively.
Figure 11b. ERPs of a non-musician at Fz.
The blue line and red line show the ERPs
induced by minor second stimuli and perfect
fifth stimuli, respectively.
4. Reference
1. S. L. Siedlecki, “The efect of music on 
power, pain, depression, and disability: a
clinical trial” Ph. D dissertation, Frances 
Payne Bolton School of Nursing, Case
Western Reserve University, January
2005.
2. E.A. M. Baret, “Investigation of the 
principle of helicy: The relationship of
human field and power. In V. M.
Malinski (Ed.), Explorations of Rogers’ 
science of unitary human beings (pp.
177-188). Norwalk:
Appleton-Century-Crofts.
3. Exploring the sound-space of synthesis
algorithms using interactive genetic
algorithms. Colin G. Johnson.
Department of Computer
Science,University of Exeter.
4. Chaosynth, A Cellular Automata-based
Granular Synthesiser. Eduardo Reck
Miranda (3)
5. Helmuth, M., Timbral Composition with
Granular Synthesis,
6. Hunt, A., Kirk, R. and Orton, R.,
Graphical control of Granular Synthesis
using Cellular Automata, Proceedings of
the International Computer Music
Conference, 1991, pp. 416
7. Jones, D. and Parks, T., Generation and
Combination of Grains for Music
Synthesis, Computer Music Journal,
Vol.12, No. 2, summer 1988.
11
Taiwan, July, 2006, pp126-142.
[2] Wu-Jong Yu, Chih-Fang Huang,
and Wei-Hua Chieng,“Workspace
and Dexterity Analyses of the Delta
HexaglidePlatform”, Journal of
Robotics and Mechatronics, Vol.20
No.1, 2007. (Accepted) (SCI)
[3] Chong-Liang Cheng, Yu-Siang Cao,
Zih-Yi Chuang, Sheng-Fu Liang,
Shih-Han Lin, Jenny Ren,
Chih-Fang Huang, Phil Winsor, “A 
Pilot Study in Brain
Electrophysiological Response to
Controlled Auditory Stimuli as
Musical Interval”, The 3rd
International Conference
WOCMAT, Workshop for
Computer Music and Audio
Technology, Hsinchu, Taiwan,
March 24-25, 2007.
[4] Yi-Zuo Chen, Chih-Fang Huang,
“Integrated Methodology and 
Research on Interactive Intermedia
Art for Using the Sensor to Create
Sound and Vision”, The 3rd
International Conference
WOCMAT, Workshop for
Computer Music and Audio
Technology, Hsinchu, Taiwan,
March 24-25, 2007.
[5] Jose Duarte, Chih-Fang Huang, and
Shih-Heng Pan, “Sound Synthesis 
with Dynamic Change of Timbre”,
The 3rd International Conference
WOCMAT, Workshop for
Computer Music and Audio
Technology, Hsinchu, Taiwan,
March 24-25, 2007.
[6] Chih-Fang Huang, Panel Four-
“new perspectives on the studies of
qin music: Computerized sound
analysis for the Qin: The timbre
and articulation”, International
Traditional Music Conference, 39th
World Conference ICTM 2007,
Vienna, Austria, July 06, 2007.
[7] Yi-Tso Chen, Chih-Fang Huang,
“THE METHODOLOGY AND 
RESEARCH OF THE
INTERACTIVE INTERMEDIA
ART USING THE GYROSCOPE
SENSOR FOR SOUND AND
VISION”, International Computer
Music Conference ICMC 2007,
Copenhagen, Denmark, August 31,
2007.
[8] Jenny Ren, Phil Winsor, Chih-Fang
Huang,“DEVELOPING 
CHINESE STYLE
ALGORITHMIC COMPOSITION
USING MARKOV CHAINS:
FROM THE CLASSICAL
CHINESE-POETRY
PERSPECTIVE”, International
Computer Music Conference
ICMC 2007, Copenhagen,
出席國際學術會議心得報告
計畫編號 NSC 95－2221－E－009－219
計畫名稱 電腦音樂創作準則於音樂效應認知之研究
出國人員姓名
服務機關及職稱
黃志方 國立交通大學音樂研究所
會議時間地點 July 06, 2007 / Vienna, Austria
會議名稱 International Traditional Music Conference, 39th World Conference ICTM 2007
發表論文題目 new perspectives on the studies of qin music: Computerized sound analysis forthe Qin: The timbre and articulation
一、參加會議經過
1. Arrived at Vienna, Austria on July 01, 2007, and registered in University of Music and
Performing Arts Vienna on July 04.
2. I attended the conference on July 06, 2007, and make the presentation in Panel Four-“new
perspectives on the studies of qin music: Computerized sound analysis for the Qin: The
timbre and articulation”.
二、與會心得
1. The Purpose to use the Methodology of the Computerized Sound Analysis for Qin.
2. The Main Tools of the Empirical Music Analysis for Qin is the time-frequency
spectrograph to depict Timbre, Structural Form, Voices, Pitch, and Rhythm of Qin.
3. The Cross Cultural Comparison is interesting for Qin and Other Plucked Instruments.
4. The Computer Synthesis Sound techniques can be obtained for Qin via this kind of
analysis.
5. Qin Music can be analyzed with computerized sound tools, to transfer the music meaning
with the proper interpretation from sound parameters.
6. The computerized sound Analysis is very useful for Qin music:
 The Auxiliary Methodology of Education and Practice
 Empirical Musicology Research (Eric Clarke and Nicholas Cook: empirical
musicology: Aims, Methods, and Prospects, Oxford Press, 2004.)
 The computerized sound synthesis techniques of Qin.
7. However it can not be characterized by its score, therefore this paper presents the
methodology of sound analysis for discussing the relation between articulation and timbre
