-1- 
 
 
ABSTRACT 
 
在本計劃中，我們研發了一套新穎的關鍵影格
的偵測方法，其利用視覺上的顯著為基礎的關注徵
及文意上賽程狀態資訊，成功應用於運動節目上。
在此兩個重要的議題被深入探討，第一、人類視覺
上的關注特性：當一個使用者在觀看視訊片段時的
視覺特性；第二、賽程的比分狀態：擷取與賽程精
采事件相關的感興趣指數。我們研發了一個以物件
為導向的視覺關注地圖及用來解釋語意上之感興趣
指曲線。語意上的推演可以用來模擬分析哪一種的
視訊內容會引起觀眾的興趣。再者，我們提出一個
融合的機制來對視覺及語意特徵執行分析及推演人
類感興趣特性，最後，實驗階段利用棒球節目來說
明了系統的效率及強健性。對於未來的視訊內容分
析技術及即時適應性的播放機制有相當的助益。 
 
 
Index Terms— key-frame detection, content analysis, 
contextual modeling, visual attention model, content-based 
video retrieval 
 
1. INTRODUCTION 
 
The need to access the most representative information 
and reduce its transmission cost, have made the development 
of video indexing and a suitable retrieval mechanism 
popular research topics [1]-[3]. One effective approach is 
the use of key-frames, i.e. a very small subset of frames 
representing the whole video. Many multimedia retrieval 
systems have been proposed [4], [5]. Most of them are based 
on key selections (e.q., key-frame and key shot) as an index 
for users to select and browse. From a content semantics 
point of view, a video content can be divided into four 
categories based on their semantic significance, including 
video clip, object, action, and conclusion.  
A key-frame not only represents the entire video content, 
but also reflects the amount of human attention the video 
attracts. There are two clues that help us to understand the 
viewer’s attention, visual cue and contextual information. 
Visual information is the most intuitive feature for the 
human perception system. Modeling the visual attention 
when watching a video [6] is provides a good understanding 
of the video content. In intelligent video applications, visual 
attention modeling [7] combines several representative 
feature models into a single saliency map which is then 
allocated to those regions that are of interest to the user. The 
saliency feature map can be used as an indication of the 
attention level given to the key-frame. On the other hand, in 
sports videos, the game status is the information that 
concerns the subscriber the most. The embedded captions on 
the screen represent condensed key contextual information 
of the video content. Taking advantage of prior implicit 
knowledge about sports videos, Shih et. al [8] proposed an 
automatic system that can understand and extract context 
that can then be used to monitor an event.  
The number of key-frames in a shot is one of the most 
important issues in the key-frame extraction scheme. 
Whether it is predefined or chosen dynamically, either way 
it is affected by the video content. In fact, frames in a shot 
that undergo a strong visual and temporal uncertainty 
complicate the problem even more. To balance this, we let 
the number of key-frames be depending on the level of 
excitement of the context. This means that when the 
excitement in the on-going game is high, more key-frames 
should be extracted from that video shot. 
We proposed a novel attention-based keyframe 
detection system by integrating the object-oriented visual 
attention maps and the contextual on-going game outcomes. 
Using an object-based visual attention model combined with 
contextual attention model not only precisely determines the 
human perceptual characteristics but also effectively 
determines the type of video content that attracts the 
attention of the user. 
 
2. VISUAL ATTENTION 
 
In human-attention-driven application, video object (VO) 
extraction becomes necessary. Visual attention 
characteristics of every VO can be computed by using the 
saliency-based feature map extraction. We modified the 
visual attention model [7] which is frame-based manner to 
object-based approach, called the object-based visual 
attention model which provides more accurate information 
to the viewer. Based on the types of features, the object-
based visual attention model can be determined by four 
types of feature maps, i.e., spatial, temporal (local), facial, 
and (global) camera motion. 
2.1. Spatial Feature Maps 
 
We selected three spatial features to be extracted and to be 
used as the spatial feature map: intensity, color contrast (red-
green, blue-yellow), and orientation. The intensity image I 
was obtained as I = (r + g + b)/3 where r, g, and b indicate 
the red, green, and blue channels of the input image, 
respectively. Four broadly-tuned color channels were 
obtained, R = r - (g + b)/2 for red, G = g - (r + b)/2 for green, 
B = b - (r + g)/2 for blue, and Y = min{[(r + g)/2 - (|r - g|/2 - 
b], 0} for yellow. The local orientation feature was acquired 
-3- 
 
        [ ],t, text/digi,i CBSVPosCT ,=                     
where Pos indicates the relative location in the SCB. The 
text/digit bit indicates that the caption is either text or digit, 
SV indicates a set of support vectors for this character, and 
CB is the corresponding character block. The caption 
template can be used to identify the characters in the 
succeeding videos. 
 
Table 1. The reachable contextual information of the SCB 
Context Annotation Description #States 
INNS λ1 The current innings 9 
RUNNERS λ2 The base that are occupied 
by  runners 
8 
RUNS λ3 The  score difference nsn Ζ∈  
OUTS λ4 The number of outs 3 
BALLS λ5 The number of balls 4 
STRIKES λ6 The number of strikes 3 
 
3.3. Modeling the Contextual Attention 
 
Generally speaking, contextual attention is defined as the 
probability of a user’s interest in a specific game status 
which can be formulated with a context vector. However, 
different domains contain different linguistic information. 
Thus, it is hard to use a generic model for all types of video 
data, since a great deal of domain-specific features are 
involved. Different from visual attention which varies frame-
by-frame, contextual attention varies depending if it is shot-
based or event-based. 
Unfortunately, we were unable to obtain all of the 
statistical information from video the frames. Therefore, in 
this report, we not only adopted the available information 
from the SCB, but also employed the historical statistics data. 
Normally, the contextual information embedded in the SCB 
consists of six classes (Λ={λi|i=1,2,…,6}) for the baseball 
game, as listed in Table 1. 
 
3.3.1. Implicit Factors 
 
In this report, the contextual information is divided into 
three classes. These three classes are based on the 
relationship between the value of the context and the degree 
it excites the viewer, proportionally, specifically, or 
inversely. After classifying the contextual description, a 
group of implicit factors {Fl |l=1,2,…, l’,…,L-1, L} are 
used to model the human excitement characteristics. Then, 
each of these implicit factors can be classified as one of the 
three classes as follows 
Ω(Fl’) ∈{ωp, ωs, ωi} 
where Ω(.) denotes the classifier, and ωp, ωs, and ωi  
represent the corresponding factor as a proportional type, 
specific type, or an inverse type respectively.  It not only 
uses the implicit factor in the current moment (i.e., Ft), but it 
also takes into account the implicit factor from the historic 
statistics (i.e., F0:t-1). Let ψc(fi) indicate the viewer’s 
contextual attention score of frame f, which can be 
contributed to the implicit factors Ft={Fk|k=1,2,…,K} which 
consist of all the probable annotations of the SCB at that 
moment. Also, the historical statistics are taken into 
consideration, in which the implicit factors from the 
historical statistics are represented by F0:t-1={Fq|q=1,2,…,Q}. 
Thus 
ψc
t(fi)= F t+ F 0:t-1. 
Four implicit factors from the current observation are 
considered in determining the contextual attention score, 
which include 
 
• t
1F : The score difference 
The scoring gap between two teams greatly attracts the 
attention of the viewer. When the runs scored are the same 
or are very close, it indicates that the game is very intense. 
Therefore, the game can be formulated as 
t
1F =exp(-α1λ3), 
where α1 denotes the normalization term 
 
• t
2F : The number of BALLS and STRIKES pairs 
The ratio between BALLS and STRIKES can be applied 
to model user attention. In a baseball game that is being 
broadcasted, the number of balls is repeatedly updated from 
0 to 3 and the number of strikes and outs are updated from 0 
to 2. When λ5 or λ6 reaches 3 or 2, it indicates that the game 
is getting a high level of attention, because the current player 
will soon be struck out or get to walk soon. Thus, 
t
2F = exp(λ5-3)*exp(λ6-2). 
• t
3F : The number of the inning being played 
Generally speaking, the less the number of remaining 
innings, the more attention the game will attract. Therefore, 
we can design an implicit factor from the number of the 
inning λ1. In general, the maximum number of innings is 9. 
Thus, this factor can be expressed as 
t
3F =exp(λ1-9). 
• t
4F : The number of outs 
(4) 
(5) 
(6) 
(7) 
(8) 
(9) 
-5- 
 
representative. It is obvious that combining all attention 
feature maps can meet the first rule. In the present study we 
adopted the camera motion characteristics and treated them 
as the balancing coefficient to support the second rule. A 
numerical value was then derived from the visual 
characteristics of all segmented objects within the frame. 
The denominator of each part is the normalization term, 
which is the sum of the attention maps for all frames 
belonging to the same video shot. Based on the predefined 
number of key-frames, the Ri key-frames {Fk*} with the 
largest visual attention score ψvt, is calculated as follows 
[ ]∪i
i
R
i
icamerai
t
vfk
fMfF
1
*
,)()(ψmaxarg
=





 ×=  
where 
,
)(
)(
)(
)(
)(
)()(ψ
1
3
1
2
1
1
∑∑∑
===
×+×+×=
sss N
j
jfacial
ifacial
N
j
jtemporal
itemporal
N
j
jspatial
ispatial
i
t
v
fFM
fFM
fFM
fFM
fFM
fFMf γγγ
 
where γ1, γ2, and γ3 denote the weighting coefficients among 
the visual feature maps.  
 
5. EXPERIMENTAL RESULTS 
 
Our proposed object-based attention model is used as an 
effective scheme to measure the attention score. Table 3 
shows the attention scores in different modules for six 
representative frames shown in Fig. 1. 
The values of each column indicate the attention score, 
and accurately reflect the attention to the content via spatial, 
temporal, facial and global motion. Frames #650 and #1716 
are globally static, so they have low temporal attention 
scores and low camera motions resulting in decreased visual 
attention. Frame #931 zooms in for a close-up, but the object 
itself is stable, resulting in a high visual attention. Frame 
#4001 is a mid-distance view with local motion and the 
camera zooming. However, the face it zooms into is clear 
and near the center of the frame, resulting in a high attention 
score. Frame #3450 is a mid-distance view with middle face 
attention, and with the camera panning, which also increases 
attention. Frame #5763 has high attention due to the rapid 
panning to capture the pitcher throwing the ball.  
Figs. 2 and 3 show the results of key-frame detection of 
the sample. It is evident that the proposed scheme is capable 
of extracting a suitable number of key-frames. Based on the 
visual attention score, the extracted key-frames are highly 
correlated with human visual attention. However, in the third 
and fourth rows of Fig. 2, there are two redundant key-
frames extracted. This is because the shot boundaries were 
incorrectly determined as a result of the fast panning or 
tilting by the camera. In addition, all the frames within the 
shot had near-identical attention scores. Furthermore, the 
method we proposed can also be applied to determine the 
key-frame for slow-motion replay clips as the last selected 
key-frames in Fig. 2. We assume that from each of these 
shots at least one key-frame was selected based on the 
frames’ attention scores. That is why a few of the key-frames 
look like normal average play. Nevertheless, Fig. 3 shows 
that an exciting moment can be properly preserved by means 
of our proposed method. 
Table 3. The examples with different Attention Scores 
#Frame 
Integral 
Visual 
Attention 
Spatial Temporal Facial 
Camera 
Motion 
DVv DVH 
573 0.243 0.053 0.688 0.107 144 241 
650 0.093 0.051 0.110 0.039 0 11 
931 0.691 0.506 0.672 0.562 82 242 
1716 0.051 0.069 0.041 0.091 0 11 
3450 0.186 0.038 0.253 0.038 24 129 
4001 0.286 0.056 0.394 0.047 26 41 
 
#573                        #650                        #931 
 
#1716                       #3450                      #4001 
 
Fig. 1. The representative frames for testing. 
 
 
 
 
 
(13) 
 !"#$%&'()*+,-. /01"234567
7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 789:;<; = <; > ;? 87
@"34A7
BCDE 'F"GHIJKLMNOP /"2QRST=UP"2VWXYGZ
[\]^_`SaWbcG defghijfklk7mlnoe7pqrqq sZ[\'(HIJtuK
LvwKxXcPN0QRyT=z{|}R~y
P'(0@@y{c /P0PSH
c '(¡¢Df£iojo¤k7mhjhli£¥¦P'(§S¨Kx /P'(©ª«¬­"
2® S¯°±s{u ²feh£k7jhejoi Ptu ³´ µ¶·¸¹ºSs»¼'(N½¾y
­¿À Áojkhi PÂÃÄÅyÆ4RÇPsS­¿"2P Áojkhi7jh£kfoe È
ÉÊËÌÍÈS{tÎH /'(0Ï¼ÇÐSÑÉÒPÓÔÕ¨ÖÑ×y
ØT=P"2ÙÑÚÛPsÜÝÞßSàRáP ih£hÁkfoeSàáP âleãrhk
P{äåSæçÇÏ5Sèé5êëìíîï{ ih£hÁkfoe ð âleãrhk P
)*ñò7 óôCõõö÷÷;øöEö;ùùö<;³7
)*ú7 ûüýþ°ÿG !P"#$%&'¹ÿIJ()*+Q'(7
- §
,7
-./7
0123
ðÀú7
45N07 µ2670*89:7
"2ÈÉ7
7 ;<<=?>;;8;
;<< = ? > ;ù 87 "2XY7 Z[\]^_`77
"2ú7
¢¦÷<;;  /HIJ<=>'F"7
¢?¦2011 IEEE International Conference on Multimedia & Expo(ICME2011)7
ÄÅ
&@7
¢¦AºûuBCDQuEFGHIðuEFGJKL7
¢?¦ Key-frame extraction and key-frame rate determination using human attention modeling7
	


 !"#$$ $! 
%&'()$$! $!
*+,-./.01.1.2.3,2-.-,+
.	0./.+./.+0-45..
2+-%+.6+1
78*91./.1..2.-
+..9/.:-.1..-,.8
+,+.	.,20,,.,.,3:,;.00,.1.
8$.,,	<+1./+,93:,;.00,.
.:	1./+8
!)$ $ (!)$! 
'.0
-10,2+.--.,20,,.,21+
8
,<-0,2.0-:/++
	,.8+,/,1.	,.2/.+=8.,:./+
.	,21--+,9		0.3-/.00.	08
=%,.9./:2,./,0./.0.+-	
3.00.-.,-3,<0,:++2.,-	++1.,
,>0./.0.+-	.0++.	0:+++,.2
3+2+,..8
?/,5++-..	0::.+-+,,&1.-+1
9,-/.+,8*.	1../++	./
+.	0.9-3	./..//	8
	.	+.,>:+..9/.:-.:+.0	1.%+.6+1@
-,+.	0.00
.+./.+0-45.
 !
	

 ! "# $! # $$ $! % & 
 !"#$%&'()*+,-. /01"234567
7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 789:;<; = <; > ;? 87
@"34A7
BCDEF)GFHIJKL'M"NOPQRSTUVWDXSU0YZ4[
\D]Y^=D'M"_`aR b?< cYdefghijk ;blYmn]opqrD
stYdughDefvuwxy zz{|7}~7| F {CD][
cRDD Y¡¢R£¤¥¦§¨©56Y
@"ª«v¬­k®¯P\-°H±&Y²]³mKLy´µ¶·¸¹
¡¢\º»°H¼ªD½27
)*¾¿7 À|zÁÁÂÃÃ;ÄÂÂ;bbÂ<;Å7
)*ÆÇ7 ÈÉÊËÌÍRÎÏDÐÑmÒÓÔKÍIJÕÖ×ØÙ'(7
- Ú
ÛÆ7
ÜÝÞ7
ßàáâ
FãÇ7
äåU07 ·áæ0×çè7
"2éê7
7 ;<<=ë>;Å8ì
;<< = ë > ;b 87 "2PQ7 STUVWíXSU077
"2ÆÇ7
¥îf©ïðñEF)GFHIJKL'M"7
¥òf©7óô7Ãõ7{ö§ö¦§¨7z¦÷ø7ù}úûö¦ûü7z¦ýûö7
§õ7þû¨öýõ§7ÿ¨ø§ö¦ü7
ef
&7
¥îf©Ù !D7
¥òf©ÿööö¦7þ¦õ¨7¦÷7"§ý7#øöýö7÷¦7|¦öü7$õ¦ü7
Attention Modeling of Game Excitement for Sports Videos 
Huang-Chia Shih 
Department of Electrical Engineering, Yuan Ze University,  
135 Yuandong Rd., Chungli, Taiwan, R.O.C. 
hcshih@saturn.yzu.edu.tw
Abstract. In this paper, we proposed a novel key-frame detection method by integrating 
the object-oriented visual attention model and the contextual game-status information. We 
have illustrated an approximate distribution of a viewer’s level of excitement through 
contextual annotations using the semantic knowledge and visual characteristics. The 
number of key-frames was successfully determined by applying the contextual attention 
score, while the key-frame selection depends on the combination of all the visual attention 
scores with bias. Employing the object-based visual attention model integrates with the 
contextual attention model not only produces precise human perceptual characteristics, but 
it will also effectively determine the type of video content that will attract the attention of 
viewers. The proposed algorithm was evaluated using commercial baseball game 
sequences and showed promising results. 
Keywords: attention modeling, highlight detection, content analysis, contextual modeling, 
excitement curve, content-based video retrieval
99 年度專題研究計畫研究成果彙整表 
計畫主持人：施皇嘉 計畫編號：99-2218-E-155-013- 
計畫名稱：以人類生理感知特性為基礎的內容可伸縮適應性媒體播放系統之研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 1 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
