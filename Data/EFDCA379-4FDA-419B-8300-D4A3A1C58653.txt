I專題研究計畫名稱:
以互助訓練法為基礎之特定領域專用詞採擷最佳化訓練模式
Optimization of Domain Specific Word Mining Based on Co-Training
摘要:
特定領域專用詞 (Domain Specific Words, DSW’s) 為一群時常出現在特定領域，且
在該領域具有特定意義的詞彙。這樣的詞彙，在許多自然語言處理的應用中，如文
件分類、排除字義的歧義性等，都可以提供極為有用的資訊。 要擷取這樣的專用
詞彙，可以量測詞的“領域專有性”(例如採用傳統 IR 的 TF-IDF 分數來衡量),
而排除比較不可能的專用詞彙。但是，過濾的門檻通常都不容易決定。更重要的是，
這樣的“專有性”分數可能與任何應用的執行績效，都沒有直接的關聯。因此，這
樣找出來的專有詞彙用在真實的應用時可能並不適當。所以，為了擷取專用詞彙時
能達到最高的效能，有必要使用其他的知識作為輔助.。由於文件分類器是以詞為本
位的應用中，最典型的一種應用，因此，可以根據文件分類器使用特定詞集作分類
所得到的績效，來評斷一個特定領域詞集的好壞。而使用較好的詞集，反過來也可
以幫忙文件分類器作較好的分類。因此，如果能透過良好的互相學習機制，兩者互
相提昇彼此的績效是有可能的。
雖然兩者在許多自然語言處理的應用系統中，都扮演著重要的角色，但特定領域專
用詞的擷取，及文件分類器的模型參數訓練，往往是獨立進行，並沒有互相參照，
同步進行最佳化的處理。 因此，領域專用詞的擷取模組，可能不知道到底個別詞
彙在文件分類器這種典型的應用中，能有多大的效能。而分類器據以執行文件分類
的特定領域專用詞集，也可能並不是最理想的一組。為了讓這兩者能互相幫助，本
計劃除採用一種稱為“跨領域亂度”(CDE, Cross-Domain Entropy) 的“專有性”
評量分數之外，並採用“互助訓練法” (Co-Training)，讓文件分類器參數估測能充
分運用最具鑑別力的特定領域專用詞集；同時，分類器參數也將回饋個別詞彙的專
用程度訊息給領域專用詞採擷模組，使兩者可以逐回改善各自的效能。利用這種遞
迴訓練的互助學習方式，系統擷取特定領域專用詞的效能，確能有效提昇。實驗顯
示，若輔以互助學習法，特定領域專用詞的擷取效能，在某些環境下可改善高達
30.7%。
關鍵詞:
特定領域專用詞, 詞彙抽取, 互助學習法, 文件分類, 跨領域亂度, 知識本體,
排除詞義歧義.
11 Motivation & Method
1.1 Domain Specific Words as Important Knowledge Source
Domain Specific Words (DSW’s) are words that appear frequently in some particular
domains with dedicated meanings. Since Domain Specific Words are closely related words
with strong word association in a particular domain, they are useful information sources for
natural language processing (NLP). Such word association information can be used in many
natural language processing applications, such as lexicon acquisition, ontology construction,
document classification, anti-spamming, text summarization, word sense disambiguation
(WSD), machine translation (MT), and much more.
1.2 DSW Acquisition and Optimization
DSW’s can be detected with the conventional TF-IDF term weighting approach [Baeza-Yates
99] or with a cross-domain entropy (CDE)-based measure [Chang 02, 04, 05]. Since the latter
had shown some performance gains in several previous works over the conventional TF-IDF
measure, we will use it throughout the current work.
Such base detection algorithms, however, are based on some domain-specificity
measures and ad hoc thresholds for filtering out unlikely candidates. However, one rarely
knows the best threshold for filtering. More importantly, the “domain specificity”measure
may not be related to any performance criteria of any real world applications. The acquired
DSW’s may therefore be inadequate for intended applications. For this reason, it is necessary
to use other knowledge sources to maximize the mining quality of DSW’s.
Since text classification is a major word-based application, it is possible to use a text
classifier to justify any set of domain specific words in terms of its performance for text
classification. A better set of DSW’s will in turn help the classifier for making better
classification. Hence, mutual promotion is possible through some guided co-training.
While both DSW acquisition and document classification play important roles in many
NLP applications, the acquisition of domain specific words and the model parameter training
of the document classifiers are usually conducted independently, instead of being optimized
jointly. In this manner, the DSW acquisition module may not know exactly how effective the
individual words are for real and typical applications, like document classification, and the
classifiers may not really use the best discriminative set of DSW’s for document classification.
To make both help each other, a co-training approach will be used to make the document
classifier best utilizes the most discriminative information provided by the DSW’s; on the
other hand, the classifier will provide domain specificity information back to the DSW
acquisition module so that iterative improvement of both can be achieved. With such an
iterative co-training process, a better acquisition performance of the domain specific words as
well as a better document classification accuracy would be expected.
In this work, we will therefore address the possibility to enhance the mining quality of
the domain specific words by using a base DSW extraction algorithm and the co-training
approach for refining the base algorithm iteratively. To optimize the detected set of DSW’s,
the co-training process co-trains the base acquisition module with a DSW-based document
classification model, which conduct the classification task based on some probabilities of the
form Pr(word|domain) x Pr(domain), indicative of the probability that a word is specific to a
particular domain. The details are outlined in the following sections.
3distribution of the terms. It has the same function as the DF (document frequency) factor in
the TF-IDF weighting scheme. However, the expected DF is not acquired by simply counting
the number of domains a term may appear. Intuitively, a term that distributes evenly in all
domains is likely to be independent of any domain. One can therefore reject to recognize such
terms as domain specific terms.
The above method, as outlined in [Chang 02, 04, 05], is re-cited in the following
algorithm:
Domain-Specific Word Extraction Algorithm:
Step1 (Data Collection): Acquire a large collection of web documents using a web spider while
preserving the directory hierarchy of the documents. Strip unused markup tags from the web
pages.
Step2 (Word Segmentation or Chunking): Identify word (or compound word) boundaries in the
documents by applying a word segmentation process, such as [Chiang 92; Lin 93], to
Chinese-like documents (where word boundaries are not explicit) or applying a compound word
chunking algorithms to English-like documents in order to identify interested word entities.
Step3 (Acquiring Normalized Term Frequencies for all Words in Various Domains): For each
subdirectory d j , find the number of occurrences nij of each term wi in all the documents,
and derive the normalized term frequency f n Nij ij j / by normalizing nij with the total
document size, N nj ij
i
 , in that directory. The directory is then associated with a set of
 w d fi j ij, , tuples, where wi is the i-th words of the complete word list for all documents,
d j is the j-th directory name (refer to as the domain hereafter), and f n Nij ij j / is the
normalized relative frequency of occurrence of wi in domain d j .
Step4 (Removing Domain-Independent Terms): Domain-independent terms are identified as those
terms which distributed evenly in all domains. That is, terms with large Cross-Domain Entropy
(CDE) defined as follows:
H H w P P
P
f
f
i i ij
j
ij
ij
ij
ij
j
* * log 



b g
Terms whose CDE is above a threshold is unlikely to be specific since such terms are evenly
distributed in many domains. Terms with a low CDE, on the other hand, will be retained in a few
domains with the highest normalized frequencies (e.g., top-1 and top-2).
To appreciate the fact that a high frequency term will be more important in a domain, the CDE is
further weighted by the term frequency in the particular domain when deciding which terms are
important DSW’s. Currently, the weighting method mimics the conventional TF-IDF method
[Baeza-Yates 1999; Jurafsky 2000] in information retrieval. In brief, a word with entropy Hi can
be think of as a term that spreads in 2**Hi domains on average. The equivalent number of
domains a term could be found then can be equated to 2 iHiNd  . The term weight for wi in
the j-th domain can then be estimated as:
5the more a DSW in a document matches the right domain, the higher score one will have for
that domain. It is this probability factor that provides the domain specificity measure. With
this factor included, one can also retrieve“interested documents”better, since domain specific
words will be disambiguated by the domain where they appear.
Note that the log-likelihood ratio (LLR), defined as
    
   
|
; log
|
| |
i
i
i
i i
P w d P d
LLR w d
P w d P d
P d w P w d P d


can be used as an alternative measure of“domain specificity”of the word iw in the domain
d against all the “other domains”d . By selecting words that have higher  ;iLLR w d or
higher  |iP w d P d as the DSW’s that represent the domain d , we are likely to have
better Naïve Bayesian classifier performance as well. Therefore, this probability factor can
also be used as an application-dependent “domain specificity”measure, in contrast to the
application independent domain specificity measure like the conventional TF-IDF term
weights or the TF-CDE weights used in the current work. Shortly we will see how this
correlation with an application can be used to co-train the classifier for better performance.
DSW’s acquired in this application-dependent way will then have a better chance to play
better roles in real applications since classification is the fundamental to many natural
language applications.
There are other better classification models to take advantages of the DSW’s. For the
present, we will assume the simplest case using the Naïve Baysian assumption since this base
model requires the least concern about the data sparseness problems. It is also the easiest to
use this model as an example to show how application-dependent CDW collection can be
acquired by co-training the model parameters with the baseline DSW acquisition model. All
reasonable classifier models, however, can be tried as well.
2.3 Co-Training for Joint Optimization
Although the cross-domain entropy as outlined in early sections can detect many domain
specific words in various domains [Chang 02, 04, 05], the single CDE metric for term
weighting is not totally perfect. In particular, it is difficult to determine the top-k% threshold
without using other knowledge sources. There are error sources that result in mis-recognition.
Two types of errors are frequently observed in previous CDE-based works (and probably in
other entropy-based term weighting works as well.)
On one hand, “Type I”errors were observed, in which some multiple sense words may
have too many senses and thus are mis-recognized as non-specific in each domain (although
the senses are unique in respect domains most of time).
On the other hand, “Type II”errors are observed in which some general (non-specific)
words may have low entropy simply because they appear only in one domain (with CDE=0).
This type of error may occur due to the low occurrence counts of a word. In a sense, this is a
kind of estimation error with an under scored CDE.
The main problem is that a single CDE metric may not be sufficient to capture all
characteristics of “domain-specificity”. For instance, the CDE metric does not suggest how
likely a low CDE word will appear in each domain where it appears to be specific. But this
7Iteration:
Step3 (Estimate Classifier Model Parameters): Estimate the classifier parameters associated with the
current set of DSW’s. The parameters can be estimated from nij , the number of occurrences of
each term wi in each domain d j .
To appreciate the term weights discovered in the Baseline DSW Acquisition model, terms that
fall in the Top-KH% of the whole word list, sorted by ijW , are considered as being more likely
to contribute to the classifier parameters. Therefore, their event counts nij will not be
discounted in estimating the classifier parameters. On the other hand, terms that fall in the
Bottom-KL% of the whole word list are considered non-representative for the domain. Therefore,
they will not be used for estimating the classifier parameters. This is done by setting their nij to
zero. Those other words with ijW between the Top-KH% and the Bottom-KL% will have an
event count nij that is weighted by a fractional weight which linearly drops from 1.0 to 0.0
from the boundary of the Top-KH% to the boundary of the Bottom-KL%. KH and KL will be
two adjustable parameters of the co-training process.
Step4 (Adjust DSW Collection): Adjust the current set of DSW collection by removing DSW’s that
have low probability of belonging to their respective domains according to  | iP d w , or
 |iP w d P d or  ;iLLR w d . (We will refer all these measures with the general term LLR
if no specific distinction is necessary.)
Actually, to have consistent non-DSW labels, the Bottm-K2% of words in the word list sorted by
ijW and the Bottm-K4% of words in the word list sorted by LLR will jointly determine which
words are to be excluded from the DSW collection. Those words belonging to the join of the
above two subsets will not be considered as DSW’s in the next co-training iteration.
Four thresholds, K1~K4, are introduced here to control the co-training process. K1 is the
percentage of words that are likely to be DSW’s in the word list sorted by the term weights, ijW ;
K2 is the percentage of words that are unlikely to be DSW’s according to term weights. On the
other hands, in the word list sorted by LLR, the Top-K3% of words are potential candidates of
DSW’s according to the LLR criterion, and those Bottom-K4% will be non-DSW candidates.
Therefore, a word that have low term weight (in the Bottm-K2%) and low LLR (in the
Bottom-K4%) will be truncated from the DSW collection. On the other hand, the words that have
the highest term weights (in the Top-K1%) and the highest LLR (in the Top-K3%) will be
considered as the DSW collection for the co-training acquired DSW collection for the current
iteration.
Step5 (Adjust CDE Measure): Remove words that are not in the current DSW set from the documents,
then re-compute the cross-domain entropies to further remove highly unlikely candidates.
Step6 (Repeat Co-training Iterations): Repeat the above iterations until some stopping criterion is
reached. And output the co-training acquired DSW collection at the final iteration.
In summary, we first sort the word list in each domain by term weight, ijW . The event counts
nij are then adjusted, based on ijW , to appreciate the term weight knowledge when
estimating the classifier parameters and LLR(w;d). The word list sorted by LLR is then used
together with the word list sorted by term weight to consistently label some terms as
non-DSW’s. The new set is then used to re-estimate new term weights. When the iterative
93. Experiment Results and Discussion
To evaluate the performance of the proposed co-training process, Chinese news web pages of
10 domains are extracted from a subset of the news corpus used in [Chang 05]. The 10
domains include the CULTURE, ENTERTAINMENT, FINANCE, INFOTECH,
INTERNATIONAL, LIFESTYLE, MEDICAL, SPORT, STOCK, and TRAVEL domains.
This subset has about 700 documents.
Precision Rates of DSWAcquisition
0
0.2
0.4
0.6
0.8
1
1.2
0 10 20 30 40
Iteration
%Prec
%Terms
Figure 1. Performance of the Co-Training Process for Acquiring Domain Specific Words.
(KH=10%, KL=0%, K1=K3=10%, K2=K4=20%)
Figure 1 shows a typical performance curve, which indicates the successive
improvement of the precision rate (%Prec) of DSW acquisition during 30 co-training
iterations. It also shows the percentage of terms (%Terms) that is kept in the DSW collection
at each co-training iteration. The training process selects the worst 20% of candidates, based
on term weights and LLR respectively, and deletes their join from the current DSW
candidates in each iteration. Furthermore, 10% (=K1=K3) of the words with the best term
weights and LLRs jointly determine the current set of DSWs. During the co-training process,
the event counts nij associated with the Top-10% wij are kept unchanged while the other event
counts are linearly weighted from 1.0 to 0.0 according to their term weights.
The figure shows that the initial precision, 43.7%, is improved almost monotonically to
the maximum of 57.1% at the 23rd iteration. The improvement proceeds until the DSW
collection is too small to be further truncated at a big step size; the trend is indicated by the
drops after the 23rd iteration. This figure shows that the co-training process improves the
Baseline DSW acquisition model by almost 30.7%. And, in the end of the 23rd iteration, we
have about 4 DSWs in every 7 candidates (4/7=57.1%). The co-training process thus is able to
improve the baseline acquisition model significantly at substantially high precision.
11
Bibliography
[Abney 99] Abney, S., Schapire, R.E. and Singer, Y., “Boosting Applied to Tagging and 
PP--atachment.” In Proceedings of the Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Corpora, EMNLP-VLC, pages 38--45,
College Park, MD, 1999.
[Abney 02] Abney, Steven, “Bootstrapping.” Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics, ACL-2002, 2002.
[Avancini 03] Avancini, F. Henri, Alberto Lavelli, Bernardo Magnini, Fabrizio Sebastiani,
Roberto Zanoli. 2003. "Expanding Domain-Specific Lexicons by Term Categorization."
Proceedings of the 2003 ACM symposium on Applied computing.
[Baeza-Yates 99] Baeza-Yates, Ricardo and Berthier Ribeiro-Neto. 1999. Modern
Information Retrieval, Addison Wesley, New York.
[Blum 98] Blum, A., & Mitchel, T., “Combining labeled and unlabeled data with 
co-training.” In Proceedings of the 11th Annual Conference on Computational Learning
Theory (COLT '98), pp. 92-100, 1998.
[Chang 97a] Chang, J.-S, Automatic Lexicon Acquisition and Precision-Recall Maximization
for Untagged Text Corpora, Ph.D. dissertation, Department of Electrical Engineering,
National Tsing-Hua University, Hsinchu, Taiwan, R.O.C., July, 1997.
[Chang 97b] Jing-Shin Chang and Keh-Yih Su, "An Unsupervised Iterative Method for
Chinese New Lexicon Extraction", International Journal of Computational Linguistics and
Chinese Language Processing (CLCLP), vol. 2, no. 2, pp. 97-148, August, 1997.
[Chang 02] Jing-Shin Chang, “Exploring Domain Specific Word Association from Web
Documents By Using Web Directory Hierarchy,”Technical Report, National Science
Council, under the contract NSC 90-2213-E-260-015-.
[Chang 04] Jing-Shin Chang, "Mining Domain Specific Words from Web Documents,"
(abstract, invited) Abstracts of the 4-th China-Japan Joint Conference to Promote
Cooperation in Natural Language Processing (CJNLP-04), p. 1, CJNLP-2004, City
University of Hong Kong, HK, China, 10-15 Nov., 2004.
[Chang 05] Jing-Shin Chang, "Domain Specific Word Extraction from Hierarchical Web
Documents: A First Step Toward Building Lexicon Trees from Web Corpora," Proceedings
of the Fourth SIGHAN Workshop on Chinese Language Learning, IJCNLP-05
(International Joint Conference on Natural Language Processing), pp. 64-71, Jeju Island,
Korea, October 14-15, 2005.
[Chiang 92] Tung-Hui Chiang, Jing-Shin Chang, Ming-Yu Lin and Keh-Yih Su, "Statistical
Models for Word Segmentation and Unknown Word Resolution," Proceedings of
ROCLING-V, pp. 123-146, Taipei, Taiwan, R.O.C., 1992.
[Collins 99] Colins, M.  and Y. Singer. “Unsupervised models for named entity 
classification.” In Proceedings of EMNLP/WVLC, pp. 100-110, 1999.
[Dasgupta 01] Dasgupta, Sanjoy, Michael Littman and David McAllester, “PAC 
Generalization Bounds for Co-Training”, Neural Information Processing Systems, NIPS 01, 
Vancouver, British Columbia, Canada, 2001.
[Huang 04] Chu-Ren Huang, Xiang-Bing Li, Jia-Fei Hong. 2004. "Domain Lexico-Taxonomy:
An Approach Towards Multi-domain Language Processing". The 1st International Joint
Conference on Language Language Processing (IJCNLP-04) Asian Symposium on Natural
