 行政院國家科學委員會補助專題研究計畫期末報告 
 
顆粒計算之邏輯特徵 
 
計畫類別：■ 個別型計畫  □ 整合型計畫 
計畫編號：NSC 98-2221-E-001 -013 -MY3 
執行期間：2009年 8月 1日至 2012年 12月 31日 
 
計畫主持人：廖純中 
共同主持人： 
計畫參與人員：  
 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  ■完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
■出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
          
執行單位：中央研究院 
 
中華民國 101年 10月 1日 
 
 
Logic, Probability, and Privacy: A Framework
for Specifying Privacy Requirements
Tsan-sheng Hsu, Churn-Jung Liau and Da-Wei Wang
Institute of Information Science, Academia Sinica, Taipei 115, Taiwan
tshsu@iis.sinica.edu.tw, liaucj@iis.sinica.edu.tw, wdw@iis.sinica.edu.tw
Abstract
In this paper, we propose a probabilistic hybrid logic for the specification of data pri-
vacy requirements. The proposed logic is a combination of quantitative uncertainty logic
and basic hybrid logic with a satisfaction operator. We show that it is expressive enough
for the specification of many well-known data privacy requirements, such as k-anonymity,
l-diversity and its precursor logical safety, t-closeness, and δ-disclosure privacy. The main
contribution of the work is twofold. On one hand, the logic provides a common ground to
express and compare existing privacy criteria. On the other hand, the uniform framework
can meet the specification needs of combining new criteria as well as existing ones.
Key words: Data privacy, information systems, probabilistic logic, hybrid logic, k-
anonymity, logical safety, l-diversity, t-closeness, δ-disclosure privacy.
1 Introduction
To address the privacy concerns about the release of microdata, data is often sanitized before
it is released to the public. For example, generalization and suppression of the values of quasi-
identifiers are widely used sanitization methods. To assess the effect of sanitization methods,
several data privacy criteria have been proposed. One of the earliest criteria was the notion
of k-anonymity [13, 12, 14, 15]. Although k-anonymity is an effective way to prevent identity
disclosure, it was soon realized that it was insufficient to ensure protection of sensitive attributes.
To address the attribute disclosure problem, a logical safety criterion was proposed in [6]. The
criterion was later expanded to the epistemic model in [17] and the well-known l-diversity
criterion in [8, 9]. More recently, a variety of privacy criteria have been proposed[7, 2]. Due to
the diversity of the privacy criteria, it is useful to have a flexible language for the specification
of different privacy policies. The purpose of the paper is to provide such a formal specification
language based on probabilistic hybrid logic.
Probabilistic hybrid logic is a fusion of a hybrid logic with a satisfaction operator[1] and
a logic for reasoning about quantitative uncertainty[5]. The syntax of the proposed logic is
comprised of well-formed formulas of both logics, and its semantics is based on epistemic prob-
ability structures with the additional interpretation of nominals. We show that the proposed
probabilistic hybrid logic is expressive enough for the specification of data privacy requirements,
such as k-anonymity, l-diversity and its precursor logical safety, t-closeness, and δ-disclosure
privacy. Furthermore, the language is quite flexible so that we can specify personalized privacy
requirements.
The remainder of this paper is organized as follows. In Section 2, we introduce the syntax
and semantics of probabilistic hybrid logic. In Section 3, we define data representation and
formulate the information systems as models of probabilistic hybrid logic. In Section 4, we
explain how privacy requirements can be precisely specified with the proposed logic language.
Finally, Section 5 contains some concluding remarks.
A.Voronkov (ed.), Turing-100 (EPiC Series, vol.10), pp. 157–167 157
Logic, Probability, and Privacy T.-s. Hsu, C.J. Liau, and D.W. Wang
Definition 4. Let M = (W, (Ra)a∈AGT, (PRa)a∈AGT, pi) be a PH(@) model and w ∈ W be a
possible world. Then, the satisfaction relation is defined as follows:
1. M, w |= >
2. M, w |= p iff w ∈ pi(p) for p ∈ PROP ∪ NOM
3. M, w |= ¬ϕ iff M, w 6|= ϕ
4. M, w |= ϕ ∧ ψ iff M, w |= ϕ and M, w |= ψ
5. M, w |= 〈a〉ϕ iff there is a w′ such that (w,w′) ∈ Ra and M, w′ |= ϕ
6. M, w |= @iϕ iff M, pi(i) |= ϕ
7. M, w |= r1la1(ϕ1) + · · · + rklak(ϕk) > s iff r1µw,a1(|ϕ1| ∩Ww,a1) + · · · + rkµw,ak(|ϕk| ∩
Ww,ak) > s, where |ϕ| = {u |M, u |= ϕ} is the truth set of ϕ in the model M.
A wff ϕ is said to be true in a model, denoted by M |= ϕ, if M, w |= ϕ for all w ∈W .
3 PH(@) Models of Information Systems
3.1 Information systems
In database applications, microdata, such as medical records, financial transaction records, and
employee data, are typically stored in information systems. Am information system or data
table is formally defined as follows[10]:
Definition 5. An information system or a data table1 is a tuple T = (U,A, {Vf | f ∈ A}),
where U is a nonempty finite set, called the universe, and A is a nonempty finite set of attributes
such that each f ∈ A is a total function f : U → Vf , where Vf is the domain of values for f .
Let B ⊆ A be a subset of attributes. Then, the indiscernibility relation with respect to B
is defined on U as follows:
indT (B) = {(x, y) | ∀f ∈ Bf(x) = f(y)}. (1)
Usually, we omit the symbol T in the indiscernibility relation when the underlying information
system is clear from the context. We also abbreviate an equivalence class of the indiscernibility
relation [x]ind(B) as [x]B .
The attributes of an information system can be partitioned into three subsets [4, 11]. First,
we have a subset of quasi-identifiers, the values of which are known to the public. For example,
in [14, 15], it is noted that certain attributes like birth-date, gender, and ethnicity are included
in some public databases, such as census data or voter registration lists. These attributes, if
not appropriately generalized, may be used to re-identify an individual’s record in a medical
data table, thereby causing a violation of privacy. The second kind is the subset of confidential
attributes, the values of which we have to protect. It is often the case that an asymmetry exists
between the values of a confidential attribute. For example, if the attribute is a HIV test result,
then the revelation of a ’+’ value may cause a serious invasion of privacy, whereas it does not
matter to know that an individual has a ’−’ status. Note that confidential attributes can also
serve as quasi-identifiers in some cases. However, since the values of confidential attributes
1Also called knowledge representation systems or attribute-value systems in [10].
159
Logic, Probability, and Privacy T.-s. Hsu, C.J. Liau, and D.W. Wang
Example 2. In privacy research, generalization is a widely-used sanitization operation. For
example, the date of birth may only be given as the year and month, or only the first two digits
of the ZIP code may be given. A concrete generalization of the information system in Table 1
is presented in Table 2. The first column of the table shows the pseudonyms of the individuals.
Note that the sanitization is truthful and proper.
d1 09/56 24*** 160 100K 0
d2 09/56 24*** 160 70K 1
d3 03/56 10*** 160 100K 0
d4 03/56 10*** 165 50K 2
d5 04/55 26*** 170 30K 2
d6 04/55 26*** 170 70K 0
d7 10/52 26*** 175 30K 1
d8 10/52 26*** 175 50K 0
Table 2: A sanitized information system
When a sanitized information system is released, the sanitizing mappings are usually known
to the public, but the de-identifying mapping must be kept secret. In fact, when a sanitization
is truthful and the adversary knows the values of the quasi-identifiers, the adversary can easily
infer the sanitizing mappings. For example, in the previous sanitized information system, it is
easy to see how “ZIP” and “Date of Birth” are generalized.
3.2 Models of sanitized information systems
To specify an information system and its sanitization, we have to use a fixed language. Let us
consider an information system T = (U,A, {Vf | f ∈ A}), where A = Q∪N ∪C and a truthful
sanitization operation σ = (ι, (sf )f∈A). In addition, let σT = (U ′, A′, {V ′f | f ∈ A}) be defined
as above. We assume that U = {i1, · · · , in} and U ′ = {d1, · · · , dn}. Then, the signature of our
language comprises
• PROP = {(f, v) | f ∈ N ∪ C, v ∈ Vf},
• AGT = {a0, a1}, and
• NOM = U ∪ U ′.
In Pawlak’s decision logic[10], a propositional symbol (f, v) is called a descriptor , which means
that the value of attribute f of an individual is v. Here, we only specify neutral and confidential
attributes with the language. We consider two agents a0 and a1; and we assume that agent a0
only receives the trivially sanitized information system, and a1 receives the system σT . The set
of nominals is partitioned into two subsets such that each ij denotes an individual’s identifier
and each dj represents the individual’s pseudonym. The PH(@) models compatible with the
sanitization of an information system are then defined as follows.
Definition 7. Let T = (U,A, {Vf | f ∈ A}) be an information system, σ = (ι, (sf )f∈A)
be a truthful sanitization, and σT = (U ′, A′, {V ′f | f ∈ A}) be the sanitized system, where
A = Q ∪ N ∪ C, U = {i1, · · · , in} and U ′ = {d1, · · · , dn}. Then, a PH(@) model M =
(W,R0, R1,PR0,PR1, pi) with the above-mentioned signature is a model of σT if it satisfies the
following conditions:
161
Logic, Probability, and Privacy T.-s. Hsu, C.J. Liau, and D.W. Wang
4.1 Specification of k-anonymity
According to [13, 12, 14, 15], σT satisfies the k-anonymity criterion if |[d]Q| ≥ k for any d ∈ U ′.
This is easily expressed in PH(@) language by the following formula:
la1(i) ≤
1
k
for i ∈ NOM. Formally, we have the following theorem.
Theorem 1. A sanitized information system σT satisfies the k-anonymity criterion iff σT
(la1(i) ≤ 1k ) for i ∈ U .
The formal specification means that an individual can be identified with probability at most
1
k . In particular, it can be derived that @d(la1(i) ≤ 1k ) is valid in σT for any d ∈ [ι(i)]Q, which
means that, given any record whose quasi-identifiers are indiscernible from i’s quasi-identifiers,
the adversary will be able to recognize i with probability at most 1k .
4.2 Specification of logical safety
The logical safety criterion was proposed in [6] to prevent homogeneity attacks. Subsequently,
it was articulated into an epistemic model for privacy protection in the database linking context
[17]. Here, we consider a simplified version of the logical safety criterion. Recall that, in modal
logic, the modality-free formulas are called objective formulas. Let Γ denote the set of all
nominal-free objective formulas, i.e., the set of descriptors closed under Boolean combinations.
The logical safety criterion allows a flexible personalized privacy requirements, so each individual
can specify the information that he/she wants to keep confidential. More precisely, Sec : U → 2Γ
is such a specification function. According to the semantics of decision logic[10], a pseudonym
d satisfies a descriptor (f, v) with respect to σT , denoted by d |=σT ϕ, if f ′(d) = v, and the
satisfaction relation is extended to all formulas in Γ as usual. We normally omit the subscript
σT . It is said that the adversary knows the individual i has property ϕ, denoted by i |= Kϕ if, for
d ∈ [ι(i)]Q, d |=σT ϕ. Then, σT satisfies the logical safety criterion if Sec(i)∩{ϕ | i |= Kϕ} = ∅
for i ∈ U .
Theorem 2. A sanitized information system σT satisfies the logical safety criterion iff σT
@i¬[a1]ϕ (or equivalently σT @ila1(ϕ) < 1) for i ∈ U and ϕ ∈ Sec(i).
4.3 Specification of l-diversity
In the same spirit of logical safety, the principle of l-diversity is formulated in [8, 9].
Definition 8. Let f be a fixed confidential attribute. Then, an equivalence class E of indσT (Q)
is l-diverse if f ′(E) = {f ′(dj) | d ∈ E} contains at least l “well-represented” values, and σT is
l-diverse if each of its equivalence classes is l-diverse.
We consider two instances of l-diversity that are proposed in [8, 9] to articulate the notion
of “well-represented” values:
1. Distinct l-diversity. This is the simplest instance of l-diversity. It requires that there are
at least l distinct values in f ′(E), i.e., |f ′(E)| ≥ l, for each equivalence class E.
163
Logic, Probability, and Privacy T.-s. Hsu, C.J. Liau, and D.W. Wang
To implement the t-closeness criterion, the distance between two probability distributions
must be specified precisely. Let α = (α1, . . . , αm) and β = (β1, . . . , βm) denote two probability
distributions over a sample space with m outcomes. The variational distance is defined as
follows([7]):
Dvar(α, β) =
m∑
j=1
1
2
|αj − βj | =
∑
αj>βj
(αj − βj) = −
∑
αj<βj
(αj − βj),
where the second and third equations hold because
∑m
j=1 αj =
∑m
j=1 βj = 1.
Theorem 5. A sanitized information system σT satisfies the t-closeness criterion based on the
variational distance iff for any f -clause
∨m
j=1 ϕj and 0 ≤ k ≤ m,
σT (ψ1 ∧ ψ2 ∧ ψ3) ⊃
k∑
j=1
(la0(ϕj)− la1(ϕj)) ≤ t,
where ψ1 = [a0]
∨m
j=1 ϕj, ψ2 =
∧k
j=1 la0(ϕj) > la1(ϕj), and ψ3 =
∧m
j=k+1 la0(ϕj) ≤ la1(ϕj).
The difference between syntactic and semantic privacy criteria is easily observed by compar-
ing the above theorem with the preceding ones, since the baseline agent a0 with the trivial sani-
tization information does not appear in the logical specification of k-anonymity and l-diversity;
however, it plays a crucial role in the formulation of the t-closeness criterion.
The δ-disclosure criterion proposed in [2] is another semantic privacy criterion. It is similar
to the average benefit criterion in [3, 16], although the latter is only defined for two-valued
attributes. Given a set of records E and a confidential attribute value v, let p(E, v) denote the
fraction of records in E whose confidential attribute value is v. Then, an equivalence class E
of indσT (Q) is δ-disclosure-private with regard to the confidential attribute f if, for all v ∈ Vf ,
| log p(E, v)
p(U ′, v)
| < δ,
and σT is δ-disclosure-private if each equivalence class of indσT (Q) is δ-disclosure-private.
Theorem 6. A sanitized information system σT is δ-disclosure-private iff
σT (la1(ϕ) < 2δla0(ϕ)) ∧ (la1(ϕ) > 2−δla0(ϕ))
for all f -atom ϕ.
5 Concluding Remarks
In this paper, we propose a probabilistic hybrid logic for the specification of data privacy
policies. The logic is expressive and flexible enough to represent many existing privacy criteria,
such as k-anonymity, logical safety, l-diversity, t-closeness, and δ-disclosure.
The main contribution of the logic is twofold. On one hand, the uniformity of the framework
explicates the common principle behind a variety of privacy requirements and highlights their
differences. For example, as mentioned in Section 4.4, the difference between syntactic and
semantic privacy criteria is easily observed by using the logical specifications. On the other
hand, the generality of the framework extends the scope of privacy specifications. In particular,
165
Logic, Probability, and Privacy T.-s. Hsu, C.J. Liau, and D.W. Wang
[14] L. Sweeney. Achieving k-anonymity privacy protection using generalization and suppression. In-
ternational Journal on Uncertainty, Fuzziness and Knowledge-based Systems, 10(5):571–588, 2002.
[15] L. Sweeney. k-anonymity: a model for protecting privacy. International Journal on Uncertainty,
Fuzziness and Knowledge-based Systems, 10(5):557–570, 2002.
[16] D.W. Wang, C.J. Liau, and T.-s. Hsu. Medical privacy protection based on granular computing.
Artificial Intelligence in Medicine, 32(2):137–149, 2004.
[17] D.W. Wang, C.J. Liau, and T.-s. Hsu. An epistemic framework for privacy protection in database
linking. Data and Knowledge Engineering, 61(1):176–205, 2007.
167
A Probabilistic Hybrid Logic for Sanitized Information Systems 501
To address the privacy concerns about the release of microdata, data is often
sanitized before it is released to the public. For example, generalization and sup-
pression of the values of quasi-identiﬁers are widely used sanitization methods.
To assess the eﬀect of sanitization methods, several data privacy criteria have
been proposed. One of the earliest criteria was the notion of k-anonymity[12,13].
Although k-anonymity is an eﬀective way to prevent identity disclosure, it was
soon realized that it was insuﬃcient to ensure protection of sensitive attributes.
To address the attribute disclosure problem, a logical safety criterion was pro-
posed in [6]. The criterion was later expanded to the epistemic model in [14]
and the well-known l-diversity criterion in [8,9]. Diﬀerent privacy criteria can
be employed by a data manager to prevent diﬀerent attacks, since it is unlikely
that a single criterion can meet the challenges posed by all possible attacks.
Thus, a natural requirement of data management is to have a ﬂexible language
for expressing diﬀerent privacy constraints. Furthermore, the purpose of data
analysis is to discover general knowledge from the data. Hence, we also need a
formalism to represent the discovered knowledge. The purpose of the paper is
to provide such a formal language based on probabilistic hybrid logic that can
represent discovered knowledge as well as data security constraints. Probabilis-
tic hybrid logic is a fusion of a hybrid logic with a satisfaction operator[1] and
a logic for reasoning about quantitative uncertainty[4]. The syntax of the pro-
posed logic is comprised of well-formed formulas of both logics, and its semantics
is based on epistemic probability structures with the additional interpretation
of nominals. To express privacy requirements, we have to represent knowledge,
uncertainty, and individuals. While existing probabilistic logic[5,7,4] can only
represent knowledge and uncertainty, the incorporation of hybrid logic into the
framework facilitates the representation of individuals.
The remainder of this paper is organized as follows. In Section 2, we review the
deﬁnitions of microdata and sanitized information systems for privacy-preserving
data publication. In Section 3, we introduce the syntax and semantics of proba-
bilistic hybrid logic. An axiomatization of the logic is also presented. While the
axioms are valid for general probabilistic hybrid logic models, we need additional
speciﬁc axioms for released data. In Section 4, and formulate the additional ax-
ioms based on sanitized information systems. We also use examples to explain
how privacy constraints and discovered knowledge can be expressed with the
proposed logic. Finally, Section 5 contains some concluding remarks.
2 Information Systems
In database applications, microdata, such as medical records, ﬁnancial trans-
action records, and employee data, are typically stored in information systems.
Am information system or data table [10]1 is formally deﬁned as a tuple T =
(U,A, {Vf | f ∈ A}), where U is a nonempty ﬁnite set, called the universe, and
A is a nonempty ﬁnite set of attributes such that each f ∈ A is a total function
f : U → Vf , where Vf is the domain of values for f . In an information system,
1 Also called knowledge representation systems or attribute-value systems in [10].
A Probabilistic Hybrid Logic for Sanitized Information Systems 503
Table 1. An information system in a data center
U Date of Birth ZIP Height Income Health Status
i1 24/09/56 24126 160 100K 0
i2 06/09/56 24129 160 70K 1
i3 23/03/56 10427 160 100K 0
i4 18/03/56 10431 165 50K 2
i5 20/04/55 26015 170 30K 2
i6 18/04/55 26032 170 70K 0
i7 12/10/52 26617 175 30K 1
i8 25/10/52 26628 175 50K 0
The application of σ on T results in a sanitized information system σT =
(U ′, A′, {V ′f | f ∈ A}) such that A′ = {f ′ | f ∈ A}; and for each f ∈ A,
f ′ = sf ◦ f ◦ ι−1, where ◦ denotes the functional composition. Note that the
de-identifying mapping ι is invertible because it is a bijection.
The universe U ′ in a sanitized information system is regarded as the set of
pseudonyms of the individuals. We assume that V ′f is a superset of the original
domain Vf , so a sanitizing mapping may be an identity function. A sanitization
operation σ = (ι, (sf )f∈A) is truthful if for each f 	∈ Q, sf = id is the identity
function; and it is proper if ι(indT (Q)) = {(ι(x), ι(y)) | (x, y) ∈ indT (Q)} is a
proper subset of indσT (Q). That is, indT (Q) corresponds to a strictly ﬁner par-
tition2 than indσT (Q) does. In this paper, we only consider truthful sanitization
operations. Moreover, in most cases, proper sanitization is necessary for the pro-
tection of privacy. A special sanitization, called trivial sanitization, is commonly
used as the baseline of privacy assessment[2]. Formally, a sanitization operation
is trivial if, for all f ∈ Q, |sf (Vf )| = 1. The suppression of all quasi-identiﬁers
can achieve the eﬀect of trivial sanitization.
Example 2. In privacy research, generalization is a widely-used sanitization
operation. For example, the date of birth may only be given as the year and
month, or only the ﬁrst two digits of the ZIP code may be given. A concrete
generalization of the information system in Table 1 is presented in Table 2. The
ﬁrst column of the table shows the pseudonyms of the individuals. Note that the
sanitization is truthful and proper.
When a sanitized information system is released, the sanitizing mappings are
usually known to the public, but the de-identifying mapping must be kept secret.
In fact, when a sanitization is truthful and the adversary knows the values of
the quasi-identiﬁers, the adversary can easily infer the sanitizing mappings. For
example, in the previous sanitized information system, it is easy to see how
“ZIP” and “Date of Birth” are generalized.
2 Recall that each equivalence relation corresponds to a partition.
A Probabilistic Hybrid Logic for Sanitized Information Systems 505
A formula is pure if it does not contain any propositional symbols, and nominal-
free if it does not contain any nominals. In a likelihood formula, la(ϕ) is called a
likelihood term. A formula whose outermost likelihood terms only involve agent
a is called an a-likelihood formula; that is, an a-likelihood formula is of the form
r1la(ϕ1) + · · ·+ rkla(ϕk) > s. A set of wﬀs Σ is called a theory in PH(@).
3.2 Semantics
The semantics of PH(@) is based on the epistemic probability frame introduced
in [4]. Before stating the deﬁnition, we review the notion of probability space.
Definition 3. A probability space is a triple (W,X , μ), where X is an algebra3
over W and μ : X → [0, 1] satisﬁes the following two properties:
– P1. μ(W ) = 1,
– P2. μ(U ∪ V ) = μ(U) + μ(V ) if U and V are disjoint elements of X .
The subsets in the algebra X are called the measurable subsets of W . In general,
not all subsets of W are measurable; however, for our application, it suﬃces to
consider the case of X = 2W . Thus, hereafter, we assume all probability spaces
are measurable, i.e., X = 2W ; consequently, a probability space is simply written
as a pair (W,μ).
Definition 4. An epistemic probability frame is a tuple
F = (W, (Ra)a∈AGT, (PRa)a∈AGT), where W is a set of possible worlds (states)
and for each a ∈ AGT
– Ra ⊆ W ×W is a binary relation (the accessibility relation) on W , and
– PRa is probability assignment, i.e., a function that associates a probability
space (Ww,a, μw,a) with each world w.
Definition 5. Let F = (W, (Ra)a∈AGT, (PRa)a∈AGT) be an epistemic probability
frame. Then, an epistemic probability structure (or PH(@) model) based on F
is a pair M = (F, π), where π : PROP ∪ NOM → 2W is an interpretation such that
for all nominals i ∈ NOM, π(i) is a singleton. In this case, we also say that F is
the underlying frame of M.
By slightly abusing the notation, we can identify a singleton and its element.
Thus, when π(i) = {w}, we use π(i) to denote both {w} and w.
Definition 6. Let M = (W, (Ra)a∈AGT, (PRa)a∈AGT, π) be a PH(@) model and
w ∈ W be a possible world. Then, the satisfaction relation is deﬁned as follows:
1. M, w |= 
2. M, w |= p iﬀ w ∈ π(p) for p ∈ PROP∪ NOM
3. M, w |= ¬ϕ iﬀ M, w 	|= ϕ
3 That is, X satisﬁes the following conditions (i) X ⊆ 2W , (ii) W ∈ X , and (iii)X is
closed under union and complementation.
A Probabilistic Hybrid Logic for Sanitized Information Systems 507
1. Axiom schemas:
(CT) All substitution instances of tautologies of the propositional calculus
(Ineq) All substitution instances of valid linear inequality formulas
(K) [a](ϕ ⊃ ψ) ⊃ ([a]ϕ ⊃ [a]ψ)
(K@) @i(ϕ ⊃ ψ) ⊃ (@iϕ ⊃ @iψ)
(Selfdual@) @iϕ ≡ ¬@i¬ϕ
(Ref@) @ii
(Agree) @i@jϕ ≡ @jϕ
(Intro) i ⊃ (ϕ ≡ @iϕ)
(Back) 〈a〉@iϕ ⊃ @iϕ
(QU1) la(ϕ) ≥ 0
(QU2) la(	) = 1
(QU3) la(ϕ ∧ ψ) + la(ϕ ∧ ¬ψ) = la(ψ)
2. Inference rules:
(Modus ponens, MP):
ϕ ϕ ⊃ ψ
ψ
(Gen):
ϕ
[a]ϕ
(Gen@):
ϕ
@iϕ
(GenQU):
ϕ ≡ ψ
la(ϕ) = la(ψ)
(Name): if i does not occur in ϕ
@iϕ
ϕ
(BG): if j = i and j does not occur in ϕ
@i〈a〉j ⊃ @jϕ
@i[a]ϕ
Fig. 1. The axiomatization of PH(@)
A Probabilistic Hybrid Logic for Sanitized Information Systems 509
1. General axiom schemas of information systems:
(Ref) @i〈a〉i
(Sym) @i〈a〉j ⊃ @j〈a〉i
(Tran) (@i〈a〉j ∧@j〈a〉k) ⊃ @i〈a〉k
(Uni) @i〈a0〉j
(Cons) @i〈a〉j ≡ @i(la(j) > 0)
(Unif) @i〈a〉j ⊃ @i(la(i) = la(j))
(SVA) (f, v1) ⊃ ¬(f, v2) if v1 = v2
2. Speciﬁc axioms for σT :
(DC) (
∨n
j=1 dj) ∧ (
∨n
j=1 ij)
(UI) ¬@ij ik for 1 ≤ j = k ≤ n
(UP) ¬@djdk for 1 ≤ j = k ≤ n
(Rec) @dj
∧
f∈A(f, f
′(dj)) for 1 ≤ j ≤ n
(RelP) @d(([a1]
∨
dj∈[d]Q dj) ∧
∧
dj∈[d]Q〈a1〉dj) for d ∈ U
′/Q
(Connect) @i[(
∧
dj∈[ι(i)]Q〈a1〉dj) ∧ [a1](
∨
dj∈[ι(i)]Q dj)] for i ∈ U
Fig. 2. The PH(@) theory of a sanitized information system
4.2 Theories of Sanitized Information Systems
While the axiomatization for general PH(@) models is presented in Figure 1,
the models for information systems are special kinds of PH(@) models. Thus,
additional axioms are needed to characterize the special constraints imposed on
general PH(@) models. The axioms can be separated into two groups. The ﬁrst
group consists of axiom schemas valid for all information systems, while the
second group contains speciﬁc axioms for the given information system and its
sanitization. The additional axioms are shown in Figure 2, where we use U ′/Q
to denote the quotient set of U ′ with respect to indσT (Q). That is, we select a
unique representative from each equivalence class of indσT (Q) arbitrarily and
let U ′/Q denote the set of all such representatives. In addition, we use [d]Q to
denote the equivalence class of indσT (Q) that contains d.
Axioms (Ref), (Sym), and (Tran) reﬂect the fact that each Ra in the
models of σT is an equivalence relation. Thus, [a] is an epistemic modality or
S5 modality in terms of conventional modal logic systems. Indeed, the intended
meaning of [a]ϕ is that agent a knows ϕ. In traditional modal logic, the S5
modalities are typically characterized by the following three axioms:
T: [a]ϕ ⊃ ϕ
4: [a]ϕ ⊃ [a][a]ϕ
5: ¬[a]ϕ ⊃ [a]¬[a]ϕ.
However, due to the extra expressivity of nominals in hybrid logic, we can repre-
sent these constraints by pure formulas. Furthermore, because the agent a0 only
A Probabilistic Hybrid Logic for Sanitized Information Systems 511
corresponding attribute values. The axiom (RelP) encodes the indiscernibility
relation indσT (Q). Because of the reﬂexivity, symmetry, and transitivity axioms
in the ﬁrst group, we only need to specify an axiom for each equivalence class of
indσT (Q) (via each element in the quotient set U
′/Q). The last axiom connects
identiﬁers to pseudonyms. Due to the uncertainty caused by the sanitization, an
identiﬁer is exactly connected to all pseudonyms that are indiscernible with its
own pseudonym.
Let Σ(σT ) denote the set of all instances of the axioms in Figure 2. Then, it
is straightforward to verify the following theorem.
Theorem 1. A PH(@) model M with the above-mentioned signature is a model
of σT iﬀ M |= Σ(σT ).
The following is a direct corollary of the theorem:
Corollary 1. Let ϕ be a wﬀ. Then, σT ϕ iﬀ Σ(σT ) |=glo ϕ.
4.3 Applications
We have shown that the language of PH(@) is expressive enough to describe
an information system and its sanitization. In the following, we use examples
to explain how it can be used to express privacy constraints and knowledge
discovered from information systems.
Example 3. According to [12,13], σT satisﬁes the k-anonymity criterion if
|[d]Q| ≥ k for any d ∈ U ′. This is easily expressed in PH(@) language. For-
mally, a sanitized information system σT satisﬁes the k-anonymity criterion
iﬀ σT (la1(i) ≤ 1k ) for i ∈ U . The formula means that an individual can
be identiﬁed with probability at most 1k . In particular, it can be derived that
@d(la1(i) ≤ 1k ) is valid in σT for any d ∈ [ι(i)]Q, which means that, given
any record whose quasi-identiﬁers are indiscernible from i’s quasi-identiﬁers, the
adversary will be able to recognize i with probability at most 1k .
Example 4. The logical safety criterion was proposed in [6] to prevent homo-
geneity attacks. Subsequently, it was articulated into an epistemic model for
privacy protection in the database linking context [14]. Recall that, in modal
logic, the modality-free formulas are called objective formulas. Let Γ denote the
set of all nominal-free objective formulas, i.e., the set of descriptors closed under
Boolean combinations. The logical safety criterion allows a ﬂexible personalized
privacy policy, so each individual can specify the information that he/she wants
to keep conﬁdential. More precisely, Sec : U → 2Γ is such a speciﬁcation func-
tion. According to the semantics of decision logic[10], a pseudonym d satisﬁes
a descriptor (f, v) with respect to σT , denoted by d |=σT (f, v), if f ′(d) = v,
and the satisfaction relation is extended to all formulas in Γ as usual. We nor-
mally omit the subscript σT . It is said that the adversary knows the individual
i has property ϕ, denoted by i |= Kϕ if, for d ∈ [ι(i)]Q, d |=σT ϕ. Then, σT
satisﬁes the logical safety criterion if Sec(i) ∩ {ϕ | i |= Kϕ} = ∅ for i ∈ U .
Thus, a sanitized information system σT satisﬁes the logical safety criterion iﬀ
σT @i¬[a1]ϕ (or equivalently σT @ila1(ϕ) < 1) for i ∈ U and ϕ ∈ Sec(i).
A Probabilistic Hybrid Logic for Sanitized Information Systems 513
References
1. Areces, C., ten Cate, B.: Hybrid logics. In: Blackburn, P., van Benthem, J., Wolter,
F. (eds.) Handbook of Modal Logic, pp. 821–868. Elsevier (2007)
2. Brickell, J., Shmatikov, V.: The cost of privacy: destruction of data-mining utility
in anonymized data publishing. In: Proceedings of the 14th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data Mining (KDD), pp. 70–78
(2008)
3. Domingo-Ferrer, J.: Microdata. In: Liu, L., Tamer O¨zsu, M. (eds.) Encyclopedia
of Database Systems, pp. 1735–1736. Springer, US (2009)
4. Halpern, J.: Reasoning about Uncertainty. The MIT Press (2003)
5. Heifetz, A., Mongin, P.: Probability logic for type spaces. Games and Economic
Behavior 35(1-2), 31–53 (2001)
6. Hsu, T.-s., Liau, C.-J., Wang, D.-W.: A Logical Model for Privacy Protection. In:
Davida, G.I., Frankel, Y. (eds.) ISC 2001. LNCS, vol. 2200, pp. 110–124. Springer,
Heidelberg (2001)
7. Larsen, K.G., Skou, A.: Bisimulation through probabilistic testing. Information
and Computation 94(1), 1–28 (1991)
8. Machanavajjhala, A., Gehrke, J., Kifer, D., Venkitasubramaniam, M.: l-diversity:
Privacy beyond k-anonymity. In: Proc. of the 22nd IEEE International Conference
on Data Engineering (ICDE), p. 24 (2006)
9. Machanavajjhala, A., Gehrke, J., Kifer, D., Venkitasubramaniam, M.: l-diversity:
Privacy beyond k-anonymity. ACM Transactions on Knowledge Discovery from
Data 1(1) (2007)
10. Pawlak, Z.: Rough Sets–Theoretical Aspects of Reasoning about Data. Kluwer
Academic Publishers (1991)
11. Samarati, P.: Protecting respondents’ identities in microdata release. IEEE Trans-
actions on Knowledge and Data Engineering 13(6), 1010–1027 (2001)
12. Sweeney, L.: Achieving k-anonymity privacy protection using generalization and
suppression. International Journal on Uncertainty, Fuzziness and Knowledge-based
Systems 10(5), 571–588 (2002)
13. Sweeney, L.: k-anonymity: a model for protecting privacy. International Journal on
Uncertainty, Fuzziness and Knowledge-based Systems 10(5), 557–570 (2002)
14. Wang, D.-W., Liau, C.-J., Hsu, T.-s.: An epistemic framework for privacy protec-
tion in database linking. Data and Knowledge Engineering 61(1), 176–205 (2007)
analysis of complex human systems. Many computer programs have been designed and implemented to
aid SNA tasks. With the help of these programs, social scientists can induce useful knowledge from social
networks.
Although the analysis of social network data is of value to researchers and policy makers, publishing
the data may cause privacy invasion. The individuals in published social networks are typically anonymous;
however, previous works on tabulated data have shown that simply maintaining the individuals’ anonymity
may not be sufficient to protect their privacy. The major threat to privacy is the re-identification of the
individuals by linking the anonymous data to some external databases[19, 23]. Although identifiers, such as
names and social security numbers, are typically removed from released data sets, it has long been recognized
that several quasi-identifiers (e.g., ZIP codes, age, and sex) can be used to re-identify individual records.
The main reason is that the quasi-identifiers may appear with an individual’s identifiers in another public
database. Therefore, the problem is how to prevent adversaries inferring sensitive information about an
individual by linking the released data set to some public databases1.
An analogous situation may arise with social network data as an increasing amount of non-sensitive
information about personal social networks becomes publicly available. An adversary may combine the
released anonymous social network with the publicly available non-sensitive information to re-identify the
individuals in the social network. In this paper, we address the issue of privacy preservation in publishing
social networks in the context that an adversary may be able to query publicly available databases. We
assume that the external information is represented by description logic(DL) formalisms[17]. The assumption
seems reasonable because the semantic web community recognizes that DL formalisms are appropriate for
representing social network data[12].
To preserve the privacy of published tabulated data, individuals with the same combination of quasi-
identifier values are grouped in a bin or an information granule. Some qualitative or quantitative safety
criteria are then defined based on the distribution of the confidential attribute values of individuals in
the same information granule[7, 11, 24]. However, in a social network, two individuals with same quasi-
identifier values may still be distinguishable by their relationships with other individuals. Thus, to formulate
information granules for social networks, we have to consider the attributes of the individuals as well as the
relationships between the individuals.
The formulation of information granules for social networks depends on the indiscernibility of indi-
viduals according to the available information. We utilize social position analysis techniques to determine
the indiscernibility of individuals in a social network. Social position analysis attempts to find individuals
that occupy the same position in a social network based on the patterns of their relationships with other
actors. Different notions of positional equivalence have been proposed for different relationship patterns[13].
In this paper, we use the notions of regular equivalence and exact equivalence to define information gran-
ules. Recently, it was shown that social positions based on such equivalences can be characterized by modal
logics[14, 16]; thus, individuals occupying the same social position will satisfy the same set of modal formulas.
Since DL is a variant of modal logic, individuals occupying the same social position can not be distinguished
by the knowledge expressed in DL formalisms. Hence, we can define information granules as social positions
in a social network. Then, by generalizing the definition of information granules, we can extend the analytical
techniques used for tabulated data to social network data.
The remainder of the paper is organized as follows. We review the basic notions of social position
analysis and description logic in Sections 2 and 3 respectively. In Section 4, we use positional analysis
1In this paper, “adversary” or “adversaries” refers to anyone receiving data and having the potential to breach the privacy
of individuals; while an individual or an actor refers to a person whose privacy must be protected.
of the definitions here. The first is based on Boyd and Everett’s characterization [4], which states that an
equivalence relation ρ is a regular equivalence with respect to a binary relation α if it commutes with α, i.e.
αρ = ρα.
By this definition, if ρ is a regular equivalence with respect to α and (a, b) ∈ ρ, then, for each c ∈
N+α (a)(resp. N
−
α (a)), there exists c
′ ∈ N+α (b)(resp. N−α (b)) such that (c, c′) ∈ ρ. This property naturally
leads to an alternative definition of regular equivalence[13], which states that an equivalence relation ρ is a
regular equivalence with respect to a binary relation α if for a, b ∈ U ,
(a, b) ∈ ρ⇒ N+α (a)/ρ = N+α (b)/ρ and N−α (a)/ρ = N−α (b)/ρ,
where for any X ⊆ U , X/ρ is the quotient set of X with respect to ρ, i.e., X/ρ = {[x]ρ | x ∈ X}. According
to this definition, if a and b are regularly equivalent, then they are connected to equivalent neighborhoods.
Since the above definitions are obviously equivalent, we have the following definition.
Definition 2 Let N = (U, (pi)i∈I , (αj)j∈J) be a social network, and let ρ be an equivalence relation on U .
Then, ρ is a regular equivalence with respect to N if
1. (a, b) ∈ ρ implies that a ∈ pi iff b ∈ pi for all i ∈ I; and
2. ρ is a regular equivalence with respect to αj for all j ∈ J .
Similar to the case of structural equivalences, the join of two regular equivalences is still a regular equivalence,
even though their meet is not necessarily a regular equivalence. Consequently, we can find the coarsest regular
equivalence of a network. Then, two actors, x and y, are regularly equivalent, denoted by x ∼=r y, if (x, y) is
in the coarsest regular equivalence of the network.
For regular equivalence, only the occurrence or non-occurrence of a position in the neighborhood of
an actor is of interest. However, the number of occurrences is sometimes an important factor in positional
analysis. In such cases, a restriction on the number can be added to the definition of regular equivalences.
An equivalence relation ρ is an exact equivalence with respect to a binary relation α if for a, b ∈ U ,
(a, b) ∈ ρ⇒ N+α (a)//ρ = N+α (b)//ρ and N−α (a)//ρ = |N−α (b)//ρ,
where for any X ⊆ U , X//ρ is the “quotient multiset” of X with respect to ρ; that is, X//ρ = {|[x]ρ | x ∈ X|},
where {|·|} denotes a multiset. Thus, by replacing the set equality in the definition of regular equivalence with
the multiset equality, the number of equivalent neighbors must be the same for two actors to be considered
exactly equivalent.
Definition 3 Let N = (U, (pi)i∈I , (αj)j∈J) be a social network, and let ρ be an equivalence relation on U .
Then, ρ is an exact equivalence with respect to N if
1. (a, b) ∈ ρ implies that a ∈ pi iff b ∈ pi for all i ∈ I; and
2. ρ is an exact equivalence with respect to αj for all j ∈ J .
The set of all exact equivalences also forms a lattice, but it is not a sublattice of the set of all equivalence
relations[9]. Consequently, the coarsest exact equivalence for a network exists and we can define two actors
x and y as exactly equivalent, denoted by x ∼=e y, if they belong to the coarsest exact equivalence. The
partition produced by an exact equivalence is called an equitable partition or a divisor of a graph[13].
If I satisfies an axiom ϕ, it is written as I |= ϕ. A set of axioms Σ is satisfied by I, written as I |= Σ, if I
satisfies each axiom of Σ. Furthermore, Σ is satisfiable if it is satisfied by some I. An axiom ϕ is the logical
consequence of a TBox Σ, denoted by Σ |= ϕ, if each interpretation that satisfies Σ also satisfies ϕ.
An ABox contains the description of the world in the form of assertional axioms
C(a) or R(a, b),
where C is a concept term , R is a role term, and a, b are individual names. The assertion C(a) is called a
concept assertion, whereas R(a, b) is a role assertion. An interpretation I = 〈∆, [| · |]〉 satisfies an assertion
C(a)⇔ [|a|] ∈ [|C|],
R(a, b)⇔ ([|a|], [|b|]) ∈ [|R|].
I satisfies the ABox Φ if it satisfies each assertion in Φ. It is also said that I is a model of the assertion or
the ABox. Finally, I satisfies an assertion or an ABox Φ with respect to a TBox Σ if it satisfies both Σ and
the assertion or the ABox. An assertion ϕ is a logical consequence of an ABox Φ, written as Φ |= ϕ, if for
every interpretation I, I |= Φ implies that I |= ϕ.
Let L be a DL language, and let I = 〈∆, [| · |]〉 be an interpretation of L. Then, two elements u, v ∈ ∆
are indiscernible with respect to the L-concepts (or simply indiscernible), written as u ≡L v, if for any
concept term C of L, u ∈ [|C|] iff v ∈ [|C|]. Two individual names a and b are indiscernible with respect to the
interpretation I (or simply indiscernible), written as a ≡IL b, if [|a|] and [|b|] are indiscernible. Let ρ ⊆ ∆×∆
be an equivalence relation on the domain of I. Then, we say that the L-concepts are preserved under ρ if
u, v are indiscernible for any (u, v) ∈ ρ. Obviously, a social network (U, (pi)i∈I , (αj)j∈J) can be regarded as
an interpretation of a DL language if each pi corresponds to an atomic concept and each αj corresponds to
an atomic role. Thus, all definitions of interpretations can be applied to social networks. Next, we introduce
two DL languages, called ALCI and ALCQI, whose concepts are preserved under regular equivalence and
exact equivalence respectively.
The ALCI and ALCQI languages belong to the well-known AL (attributive language) family, which
was first presented in[21]. The constructors for the two languages and their semantics are shown in Table 1,
where ](·) denotes the cardinality of a set. Thus, the concept terms of ALCI are formed according to the
following syntax rules:
A | > | ⊥ | ¬C | C uD | ∀R : C | ∀R : C.
The syntax rules for ALCQI consist of those for ALCI and ≥ nR : C and ≤ nR : C, where A is an atomic
concept, C and D are concept terms, R is an atomic role or an inverse role, and n is a natural number.
4 Privacy-Preserving Social Network Publishing
4.1 Problem formulation
We consider a scenario where a data owner wants to release a social network to the public. The basic
requirement is that any identifying information must be removed from the data to be published; therefore,
we can assume that the actors in the social network are anonymous. Formally, we consider an anonymous
social network N = (U, (pi)i∈I , (αj)j∈J), where U is a set of anonymous actors. The name of each actor
in U can be regarded as a pseudonym. We use a DL language L0 characterized by (X, (Ai)i∈I , (Rj)j∈J)
individuals by using the external database alone. This aspect is not related to the publishing of the social
network, and it is obviously beyond the scope of the current problem. In fact, the assumption can be further
relaxed if the full contents of the Tbox are hidden from the public so that an adversary can only retrieve
information from a public database through a limited query language. For simplicity, we assume that access
to public databases is limited in this way.
Let Σ and Φ denote the publicly available TBox and ABox respectively. We assume that the data
owner and the adversary can retrieve non-sensitive information about individuals by using L-concept terms
to query the public databases. The answer to a query C is defined as Ans(C|Σ,Φ) = {a ∈ X | Σ∪Φ |= C(a)}.
We also assume that the public databases are truthful with respect to the given social network (the verity
assumption) and complete (the completeness assumption). The former means that I |= Σ and I |= Φ, and the
latter means that Ans(C|Σ,Φ)∪Ans(¬C|Σ,Φ) = X for any L-concept term C. These two assumptions seem
quite unrealistic because most databases are incomplete and/or contain incorrect information. However, we
make the assumptions so that we can conduct the worst-case analysis of the privacy preservation issue. In
other words, we consider the case where an adversary can obtain as much non-sensitive information about
individuals as possible. If the individuals’ privacy is not breached, even though the adversary can retrieve
such truthful and complete information, then the privacy can still be preserved when only less reliable
databases are available to the adversary.
In summary, the data owner possesses the following information:
1. the anonymous social network: N;
2. the vocabulary of the DL language L0: (X, (Ai)i∈I , (Rj)j∈J);
3. the partition of the vocabulary into sensitive and non-sensitive parts: I = Is ∪ In and J = Js ∪ Jn;
4. the information retrieved from the public databases: Ans(C|Σ,Φ) for any L-concept term C; and
5. the social network as the interpretation of L0: I.
If the anonymous social network is published, the adversary can obtain almost the same information as
that of the data owner, but the information about I is only partially known by the adversary. Because the
released social network is anonymous, the adversary does not have [| · |]  X, although he knows [|Ai|] for i ∈ I
and [|Rj |] for j ∈ J . Thus, we can formulate the issue of privacy-preserving social network publication as the
following problem:
• The data owner must decide if the privacy requirement would be violated if an adversary could access
the above-mentioned information.
Because we do not give a precise specification of the privacy requirement in the formulation, it actually
represents a family of decision problems for the data owner. In the family, the identity disclosure problem
and the information disclosure problem are particularly interesting. The privacy requirement for the former
is that the identities of the individuals must be hidden from the adversary, whereas the requirement for the
latter is that a predefined set of sensitive facts about the individuals must be kept confidential. The two
problems are formulated more precisely as follows:
• Identity disclosure problem: Could the adversary infer [| · |]  X or [|a|] for some a ∈ X?
• Information disclosure problem: Could the adversary infer C(a) or R(a, b) for some a, b ∈ X, where C
is a sensitive concept and R is a sensitive role?
Definition 5 The released social network N = (U, (pi)i∈I , (αj)j∈J) is logically safe with respect to a L0-
concept C (resp. role R) if, for all u ∈ U , there exists v ∈ [u]≡L such that v 6∈ [|C|] (resp. v 6∈ [|∃R : >|] and
v 6∈ [|∃R− : >|]). The social network is simply logically safe if it is logically safe with respect to each concept
in SC and each role in SR.
Once the logical safety requirement has been violated for some [|a|] ∈ U , the adversary can infer some
sensitive information about a without identifying the node in the network that corresponds to a. The
criterion guarantees the heterogeneity of the pseudonyms that the adversary can not distinguish from [|a|]
with respect to the sensitive concepts and roles; hence, the adversary cannot infer any sensitive information
about a with certainty.
4.3 Sanitization of Social Networks
The formulation of information granules allows the data owner to determine whether releasing a social
network might violate the privacy requirement. The owner might decide against releasing the data if doing
so could compromise individual privacy. However, a less radical approach is to simply sanitize the social
network before it is released to the public so that the sanitized network satisfies the privacy requirement.
The most popular techniques for sanitizing traditional data tables are attribute suppression and gen-
eralization. The former deletes all the values in a column, and the latter replaces the values of a given
attribute with less specific values. In the current context, these techniques can be seen as ways of replacing
several attributes and relations in the network with a derived attribute or relation.
Let I ′ ⊆ In and J ′ ⊆ Jn be two subsets of indices. Then, a sanitization strategy can be formally
defined as a triplet σ = (E; I ′, J ′), where E is an L-concept term or an L-role term comprised of only
atomic concepts from {Ai | i ∈ I ′} and atomic roles from {Rj | j ∈ J ′}. When E is a concept term,
σ is called a concept sanitization strategy; otherwise, it is called a role sanitization strategy. Applying a
sanitization strategy (E; I ′, J ′) to a social network removes all attributes in {pi | i ∈ I ′} and all binary
relations in {αj | j ∈ J ′} from the network. Then, a new attribute or binary relation is added based on the
interpretation of E. Specifically, let N = (U, (pi)i∈I , (αj)j∈J) be the social network. Then, after applying
σ to N, the resulting social network is San(N, σ) = (U, (pi)i∈I−I′ , (αj)j∈J−J′ , [|E|]). When E is a concept
term, we denote [|E|] as pE and add it to the set of attributes; otherwise, we denote it as αE and add it to
the set of binary relations. Then, the generalization and suppression operations are particular instances of
the sanitization strategies. The operations are defined as follows:
1. attribute suppression: ASi = (⊥; {i}, ∅),
2. relation suppression: RSj = (⊥; ∅, {j}),
3. attribute generalization: AGi1,i2 = (Ai1 unionsqAi2 ; {i1, i2}, ∅),
4. relation generalization: RGj1,j2 = (Rj1 unionsqRj2 ; ∅, {j1, j2}),
where we use the logical constant ⊥ to denote both the null concept and the null role. In the case of
suppression, no new attribute or relation is added to the network because [|E|] = ∅. Generally, if [|E|] is equal
to some remaining attribute or relation in the network, the sanitization operation simply suppresses all the
attributes in {pi | i ∈ I ′} and all the binary relations in {αj | j ∈ J ′}.
are exactly the DL variants of multi-modal logic and graded modal logic respectively, we can use existing
algorithms for social position analysis to compute the information granules. More precisely, the computation
is based on the following theorem.
Theorem 1 Let Nn = (U, (pi)i∈In , (αj)j∈Jn) be the non-sensitive sub-network of N. Then, for x, y ∈ U ,
1. x ∼=r y in Nn iff x ≡L y when L is formed by using only ALCI constructors and the role constructors
introduced in Table 1;
2. x ∼=e y in Nn iff x ≡L y when L is formed by using only ALCQI constructors.
According to [13], there exist O(m log2 n)-time algorithms for computing the coarsest regular equivalence∼=r and the coarsest equitable ∼=e, where m and n are, respectively, the number of links and the number of
nodes in a network[5, 18]. Thus, an implication of the above theorem is that the computation of information
granules based on the ALCI and ALCQI languages can be achieved with the same time complexity.
4.5 An illustrative example
Let us consider a social network N = (U, (p1, p2), (α1, α2, α3)), where
• U = {1, 2, · · · , 11},
• p1 = {1, 2, 3, 4, 5},
• p2 = {4, 6, 7},
• α1 = {(1, 4), (2, 4), (2, 5), (3, 5), (6, 10), (6, 11), (7, 8), (7, 9), (7, 10)},
• α2 = {(1, 6), (2, 7), (3, 6), (3, 7), (4, 9), (4, 10), (5, 8), (5, 11)}, and
• α3 = {(2, 9), (4, 8), (9, 5)}.
Furthermore, p1, α1, and α2 are non-sensitive, while p2 and α3 are sensitive. The non-sensitive part of
the network is shown in Figure 1(a). We assume the query language for the external database is an ALCI
language extended with the role constructors introduced in Table 1. The signatures of L0 and L are therefore
(X,A1, A2, R1, R2, R3) and (X,A1, R1, R2) respectively, where X = {a1, a2, · · · , a11}. By viewing the social
network as an interpretation of the language I = (U, [| · |]), we can define [|ai|] = i(1 ≤ i ≤ 11), [|Ai|] = pi(i =
1, 2), and [|Ri|] = αi(i = 1, 2, 3). According to our problem formulation, the adversary does not know the
evaluation [|ai|], but he can retrieve the information about the individuals from the database. For ease of
presentation, we assume that all concepts and roles are primitive. Thus, the TBox is empty and the ABox
contains the full description of the graph in Figure 1(a).
Suppose the adversary retrieves the information by using two queries A1 and ∃R1 : > (and perhaps
their negations). Then, he can find that a1, a2, and a3 satisfy both queries; a4 and a5 satisfy A1, but not
∃R1 : >; a6 and a7 satisfy ∃R1 : >, but not A1; and a8, a9, a10, and a11 do not satisfy either of the queries.
Thus, the adversary can construct the following mapping:
{a1, a2, a3} 7−→ {1, 2, 3}
{a4, a5} 7−→ {4, 5}
{a6, a7} 7−→ {6, 7}
{a8, a9, a10, a11} 7−→ {8, 9, 10, 11},
110
3
4
5
6
7
8
9
2
11
1
1
: p1
: ﹁ p1
: α1
: α2
(a) The non-sensitive part of a social network
1
10
3
4
5
6
7
8
9
2
11
: α1
: α2
(b) Attribute suppression AS1
1
10
3
4
5
6
7
8
9
2
11
1
1
: p1
: ﹁ p1
: α2
(c) Relation suppression RS1
1
10
3
4
5
6
7
8
9
2
11
1
1
: p1
: ﹁ p1
: α1
(d) Relation suppression RS2
1
10
3
4
5
6
7
8
9
2
11
: α1
(e) Attribute suppression AS1+ relation suppression RS2
1
10
3
4
5
6
7
8
9
2
11
: α4 =α1 ∪α2
(f) Attribute suppression AS1+ relation generalization RG1,2
Figure 1: A social network N and several sanitization operations
[7] Y.T. Chiang, Y.C. Chiang, T.-s. Hsu, C.-J. Liau, and D.-W. Wang. How much privacy? - a system to
safe guard personal privacy while releasing database. In Proceedings of the 3rd International Conference
on Rough Sets and Current Trends in Computing, LNCS. Springer-Verlag, 2002.
[8] M. de Rijke. Description logics and modal logics. In Proceedings of the 1998 International Workshop
on Description Logics (DL’98), 1998.
[9] M.G. Everett and S.P. Borgatti. Regular equivalences: general theory. Journal of Mathematical Sociol-
ogy, 18(1):29–52, 1994.
[10] R.A. Hanneman and M. Riddle. Introduction to Social Network Methods. University of California,
Riverside, 2005.
[11] T.-s. Hsu, C.-J. Liau, and D.-W. Wang. A logical model for privacy protection. In Proceedings of the 4th
International Conference on Information Security, LNCS 2200, pages 110–124. Springer-Verlag, 2001.
[12] J. Kamps and M. Marx. Notions of indistinguishability for semantic web languages. In Proceedings of
the First International Semantic Web Conference, LNCS 2342, pages 30–38. Springer-Verlag, 2002.
[13] J. Lerner. Role assignments. In U. Brandes and T. Erlebach, editors, Network Analysis, LNCS 3418,
pages 216–252. Springer-Verlag, 2005.
[14] C.J. Liau. Social networks and granular computing. In R.A. Meyers, editor, Encyclopedia of Complexity
and Systems Science, pages 8333–8345. Springer-Verlag, 2009.
[15] F. Lorrain and H.C. White. Structural equivalence of individuals in social networks. Journal of Math-
ematical Sociology, 1:49–80, 1971.
[16] M. Marx and M. Masuch. Regular equivalence and dynamic logic. Social Networks, 25(1):51–65, 2003.
[17] D. Nardi and R. J. Brachman. “An introduction to description logics”. In F. Baader, D. Calvanese,
D.L. McGuinness, D. Nardi, and P.F. Patel-Schneider, editors, Description Logic Handbook, pages 5–44.
Cambridge University Press, 2002.
[18] R. Paige and R.E. Tarjan. Three partition refinement algorithms. SIAM Journal on Computing,
16(6):973–989, 1987.
[19] P. Samarati. “Protecting respondents’ identities in microdata release”. IEEE Transactions on Knowledge
and Data Engineering, 13(6):1010–1027, 2001.
[20] K. Schild. A correspondence theory for terminological logics: Preliminary report. In Proceedings of the
12th International Joint Conference on Artificial Intelligence, pages 466–471, 1991.
[21] M. Schmidt-Schauß and G. Smolka. “Attributive concept descriptions with complements”. Artificial
Intelligence, 48(1):1–26, 1991.
[22] J. Scott. Social Network Analysis: A Handbook. SAGE Publications, 2 edition, 2000.
[23] L. Sweeney. Achieving k-anonymity privacy protection using generalization and suppression. Interna-
tional Journal on Uncertainty, Fuzziness and Knowledge-based Systems, 10(5):571–588, 2002.
[24] L. Sweeney. k-anonymity: a model for protecting privacy. International Journal on Uncertainty,
Fuzziness and Knowledge-based Systems, 10(5):557–570, 2002.
 1 
 
國科會補助專題研究計畫出席國際學術會議心得報告 
                                     日期：2012年 10月 1日 
                               
一、參加會議經過 
眾所周知，A.M.Turing 為現代計算機科學的奠基者，其所發展出來的計算模式至今仍
被沿用著。他在短暫的四十二年生命中，留下了不可磨滅的科學遺產。距離其生於
1912 年 6 月 23日，今年恰逢其百年誕辰，因此全球計算機科學界紛紛舉辦各項活動
紀念其貢獻，並探討目前計算機科學領域的各種尖端議題。這些活動的總主持人為
英國 Leeds大學的 Barry Cooper教授，而其中最特殊的紀念會議之一，就是跨越 Turing
生日當天，並在他最後工作之處，英國曼徹斯特大學所舉行的紀念會議。 
 
本次會議共邀請了九位 Turing Award 得主，著名的物理學家 Roger Penrose，以及前
計畫編號 
NSC 98-2221-E-001-013-MY3 
計畫名稱 顆粒計算之邏輯特徵 
出國人員
姓名 
廖純中 
服務機構
及職稱 
中研院資訊所研究員 
會議時間 
2012年 6月 22日
至 
2012年 6月 25日 
會議地點 
英國曼徹斯特 
會議名稱 
(中文) Turing百年紀念會議 
(英文) The Turing Centenary Conference 
發表題目 
(中文) 邏輯，機率，與隱私: 一個用於表述隱私需求的架構 
(英文)  Logic, Probability, and Privacy: A Framework for Specifying 
Privacy Requirements 
附件五 
 3 
此次會議除了這些邀請的演講者外，也另外向外徵稿，其中獲得接受的論文安排在
24 日當天中午的壁報亦程中發表，發表者在輕鬆的氣氛中與聽眾分享其成果。我們
所發表的論文，題目是 「邏輯，機率，與隱私: 一個用於表述隱私需求的架構」（Logic, 
Probability, and Privacy: A Framework for Specifying Privacy Requirements）。由於資訊
公開的要求，許多資訊系統必須允許公眾查閱，但這也衍生出隱私保護的問題。屏
蔽資訊系統(Sanitized Information Systems)的概念就是對要被公開的資訊系統先作一
些屏蔽處理，以避免侵犯到個人隱私，但如何能夠確知屏蔽處理的效果是否足夠滿
足個人隱私的需求，便需要一種能準確表達個人隱私需求的機制。我們所使用的方
法是以嚴謹的邏輯語言來達到此一目的。由於個人隱私涉及到對於個體，知識，以
及機率的推理，這正好是混合邏輯(hybrid logic)，知態邏輯(epistemic logic)，與機率
邏輯(probabilistic logic)所處理的範疇，因此我們結合這三種邏輯，提出機率混合邏輯
的概念，以之作為表達個人隱私需求與資訊系統知識表徵之工具。我們探討了此一
邏輯的語法，語意，及公理化系統等，希望為隱私強化技術立下一個堅實的邏輯基
礎。 
二、與會心得 
此次會議聆聽了許多知名大師的演講，了解到許多計算機科學研究的尖端課題，雖
然限於個人能力，不可能全部深入，但對於與自身興趣相關的研究課題，可以掌握
最新的方向，深覺受益良多。 
三、發表論文全文或摘要: 詳附件。 
四、攜回資料名稱及內容: 會議論文集詳見
http://www.easychair.org/publications/?page=1900403647 
Logic, Probability, and Privacy T.-s. Hsu, C.J. Liau, and D.W. Wang
2 Probabilistic Hybrid Logic
2.1 Syntax
Hybrid logics are extensions of standard modal logics with nominals that name individual states
in possible world models[1]. The simplest hybrid language is the extension of the basic modal
language with nominals only. More expressive variants can include the existential modality E,
the satisfaction operator @, and the binder ↓. The simplest hybrid language is denoted by H
and its extensions are named by listing the additional operators. For example, H(@) is the
simplest hybrid language extended with the satisfaction operator @. On the other hand, the
probabilistic logic LQUn proposed in [5] consists of (linear) likelihood formulas of the form
r1la1(ϕ1) + · · ·+ rklak(ϕk) > s,
where r1, . . . , rk, s are real numbers, a1, . . . , ak are (not necessarily distinct) agents, and
ϕ1, . . . , ϕk are well-formed formulas of the probabilistic language. The proposed probabilis-
tic hybrid logic is a straightforward fusion of H(@) and LQUn . The following definition gives the
syntax of the resultant language.
Definition 1. Let PROP = {p1, p2, . . .} (the propositional symbols), AGT = {a1, a2, . . .} (the
agent symbols), and NOM = {i1, i2, . . .} (the nominals) be pairwise disjoint, countably infinite sets
of symbols. The well-formed formulas of the probabilistic hybrid logic PH(@) in the signature
〈PROP, AGT, NOM〉 are given by the following recursive definition:
WFF ::= > | p | i | ¬ϕ | ϕ ∧ ψ | 〈a〉ϕ | @iϕ | r1la1(ϕ1) + · · ·+ rklak(ϕk) > s,
where p ∈ PROP; i ∈ NOM; a, a1, . . . , ak ∈ AGT; ϕ,ϕ1, . . . , ϕk ∈ WFF; and r1, . . . , rk, s are real
numbers.
As usual, we abbreviate ¬(¬ϕ ∧ ¬ψ), ¬(ϕ ∧ ¬ψ), and ¬〈a〉ϕ as ϕ ∨ ψ, ϕ ⊃ ψ, and [a]ϕ
respectively. In addition, (ϕ ⊃ ψ) ∧ (ψ ⊃ ϕ) is abbreviated as (ϕ ≡ ψ); and several obvious
abbreviations can be applied to likelihood formulas, e.g., r1la1(ϕ1)+ · · ·+rklak(ϕk) < s denotes
(−r1)la1(ϕ1) + · · ·+ (−rk)lak(ϕk) > −s).
2.2 Semantics
The semantics of PH(@) is based on the epistemic probability frame introduced in [5].
Definition 2. An epistemic probability frame is a tuple F = (W, (Ra)a∈AGT, (PRa)a∈AGT), where
W is a set of possible worlds (states) and for each a ∈ AGT
• Ra ⊆W ×W is a binary relation (the accessibility relation) on W , and
• PRa is probability assignment, i.e., a function that associates a probability space
(Ww,a, µw,a) with each world w.
Definition 3. Let F = (W, (Ra)a∈AGT, (PRa)a∈AGT) be an epistemic probability frame. Then,
an epistemic probability structure (or PH(@) model) based on F is a pair M = (F, pi), where
pi : PROP∪NOM→ 2W is an interpretation such that for all nominals i ∈ NOM, pi(i) is a singleton.
In this case, we also say that F is the underlying frame of M.
By slightly abusing the notation, we can identify a singleton and its element. Thus, when
pi(i) = {w}, we use pi(i) to denote both {w} and w.
158
Logic, Probability, and Privacy T.-s. Hsu, C.J. Liau, and D.W. Wang
are not easily accessible by the public, in this paper, we simply assume that the set of quasi-
identifiers is disjoint with the set of confidential attributes. The remaining attributes are neutral
attributes that are neither quasi-identifying, nor confidential. Hereafter, we assume that the set
of attributes A = Q∪C∪N , where Q, C, N are pairwise disjoint, Q is the set of quasi-identifiers,
C is the set of confidential attributes, and N is the set of neutral attributes. Sometimes, the set
of attributes is defined such that it contains identifiers that can be used to identify a person’s
data record. However, for simplicity, we equate each individual with his/her identifier, so the
universe U can be considered as the set of identifiers. Furthermore, since identifiers are always
removed in a released data table, U simply denotes a set of serial numbers for a de-identified
information system.
Example 1. Table 1 is a simple example of an information system. The quasi-identifiers of the
U Date of Birth ZIP Height Income Health Status
i1 24/09/56 24126 160 100K 0
i2 06/09/56 24129 160 70K 1
i3 23/03/56 10427 160 100K 0
i4 18/03/56 10431 165 50K 2
i5 20/04/55 26015 170 30K 2
i6 18/04/55 26032 170 70K 0
i7 12/10/52 26617 175 30K 1
i8 25/10/52 26628 175 50K 0
Table 1: An information system in a data center
information systems are “Date of Birth” and “ZIP”. The confidential attributes are “Income”
and “Health Status”. The values of “Health Status” indicate“normal”(0), “slightly ill”(1), and
“seriously ill”(2). ”Height” is a neutral attribute.
A common technique for protecting privacy is to release the information system in a sanitized
form. Formally, we define sanitization as an operation on information systems.
Definition 6. Let T = (U,A, {Vf | f ∈ A}) be an information system. Then, a sanitization
operation σ = (ι, (sf )f∈A) is a tuple of mappings such that
• ι : U → U ′ is a 1-1 de-identifying mapping, where |U ′| = |U |, and
• for each f ∈ A, sf : Vf → V ′f is a sanitizing mapping, where V ′f is the domain of sanitized
values for f .
The application of σ on T results in a sanitized information system σT = (U ′, A′, {V ′f | f ∈ A})
such that A′ = {f ′ | f ∈ A}; and for each f ∈ A, f ′ = sf ◦f ◦ι−1, where ◦ denotes the functional
composition. Note that the de-identifying mapping ι is invertible because it is a bijection.
The universe U ′ in a sanitized information system is regarded as the set of pseudonyms of
the individuals. A sanitization operation σ = (ι, (sf )f∈A) is truthful if for each f 6∈ Q, sf = id
is the identity function; and it is proper if ι(indT (Q)) = {(ι(x), ι(y)) | (x, y) ∈ indT (Q)} is a
proper subset of indσT (Q). In this paper, we only consider truthful sanitization operations.
Moreover, in most cases, proper sanitization is necessary for the protection of privacy. A
special sanitization, called trivial sanitization, is commonly used as the baseline of privacy
assessment[2]. Formally, a sanitization operation is trivial if, for all f ∈ Q, |V ′f | = 1. The
suppression of all quasi-identifiers can achieve the effect of trivial sanitization.
160
Logic, Probability, and Privacy T.-s. Hsu, C.J. Liau, and D.W. Wang
• W = {w1, · · · , wn};
• for R0 and R1:
– R0 = W ×W ,
– R1 = {(wj , wk) | (dj , dk) ∈ indσT (Q), 1 ≤ j, k ≤ n};
• for the probability assignments:
– PR0 associates a probability space (W,µ0) with each world w such that µ0({w}) = 1n
for each w ∈W ,
– PR1 associates a probability space (pi([dj ]Q), µwj ,1) with each world wj such that
µwj ,1({w}) = 1|pi([dj ]Q)| for each w ∈ pi([dj ]Q), where [dj ]Q is the equivalence class of
dj with respect to indσT (Q);
• and for the interpretation pi:
– pi(dj) = wj for dj ∈ U ′,
– pi(ij) ∈ pi([dj ]Q) for ij ∈ U and pi(ij) 6= pi(ik) if j 6= k for 1 ≤ j, k ≤ n,
– pi((f, v)) = {wj | f ′(dj) = v} for f ∈ N ∪ C and v ∈ Vf .
The models of σT reflect the adversary’s uncertainty about the identities of the individuals.
The possible worlds stand for the individuals. Although, the pseudonym of each individual is
fixed, as specified by the interpretation pi, the adversary is uncertain about the identifiers of the
individuals. The information that an adversary can obtain is determined by the values of the
individuals’ quasi-identifiers, so an identifier may refer to any individual in a class of individuals
that are indiscernible with respect to the quasi-identifiers. This is specified by the second
clause of the interpretation pi. With trivial sanitization, all individuals are indiscernible, so the
accessibility relation R0 is the universal relation. On the other hand, the sanitization operation
σ results in the indiscernibility relation indσT (Q), so the relation R1 is its isomorphic copy over
the domain of possible worlds. Furthermore, we assume that the indifference principle applies to
individuals, so both probability assignments associate a unform distribution with each possible
world. Since the two probability assignments are characterized completely by the accessibility
relations and R0 is simply the universal relation, we can omit these three components from a
model of σT and write it as a simple hybrid model (W,R1, pi). By the definition of pi, there
may be more than one PH(@) model for a given σT . Hence, a wff ϕ is valid in σT , denoted
by σT ϕ, if it is true in all models of σT .
4 The Specification of Data Privacy Requirements
In this section, we explain how the language of PH(@) can be used to specify different data
privacy policies such as k-anonymity, l-diversity, and t-closeness. As in the preceding section, let
T = (U,A, {Vf | f ∈ A}) denote an information system, where A = Q∪N ∪C. In addition, let
U = {i1, · · · , in}, σ = (ι, (sf )f∈A) be a truthful sanitization, and σT = (U ′, A′, {V ′f | f ∈ A})
be the sanitized information system, where U ′ = {d1, · · · , dn}.
162
Logic, Probability, and Privacy T.-s. Hsu, C.J. Liau, and D.W. Wang
2. Recursive (c, l)-diversity. Let |f ′(E)| = m and let kj(1 ≤ j ≤ m) be the number of times
the jth most frequent confidential value appears in the records of E. Then, E satisfies
(c, l)-diversity if k1 < c(kl + kl+1 + · · · + km), and σT satisfies (c, l)-diversity if every
equivalence class of indσT (Q) satisfies it.
For the specification of distinct l-diversity, let us define an (positive) f -clause of length m as
a disjunctive formula
∨m
j=1(f, vj) such that vj 6= vk for any j 6= k. An f -clause of length 1 is
also called an f -atom. Then, we have the following result.
Theorem 3. A sanitized information system σT satisfies the distinct l-diversity iff σT ¬[a1]ϕ
for any f -clause ϕ of length less than l.
A direct corollary of the theorem shows that distinct l-diversity can be seen as a special
case of logical safety.
Corollary 1. A sanitized information system satisfies the distinct l-diversity iff it satisfies the
logical safety criterion with Sec(i) being the set of all f -clauses of length less than l.
Example 3. This example shows that logical safety is more general and flexible than distinct
l-diversity. Let us consider the sanitized information system in Example 2. We assume that the
average income of individuals in the community is between 50K and 70K, so any income above
this range is considered confidential by an individual. On the other hand, for the health status
attribute, an individual may consider serious illness as confidential. Now, the system obviously
satisfies distinct 2-diversity for each confidential attribute. However, it may cause problems for
an individual if it is known that his income is 100K or he is seriously ill. In such cases, the
system would violate the logical safety criterion if Sec(i) includes the wff (fic, 100K) ∨ (fhs, 2)
because it would be known that both i3 and i4 have this disjunctive property if the system is
released to the public.
Our logic can also specify recursive (c, l)-diversity, although the specification is a little
complicated.
Theorem 4. A sanitized information system σT satisfies recursive (c, l)-diversity iff for any
f -clause
∨m
j=1 ϕj,
σT (ψ1 ∧ ψ2 ∧ ψ3) ⊃ cla1(ϕl) + · · ·+ cla1(ϕm) > la1(ϕ1),
where ψ1 = [a1]
∨m
j=1 ϕj, ψ2 =
∧m−1
j=1 la1(ϕj) ≥ la1(ϕj+1), and ψ3 = la1(ϕm) > 0.
4.4 Specification of t-closeness and δ-disclosure privacy
It is recognized that criteria like k-anonymity and l-diversity are purely syntactic in the sense
that they only consider the distribution of attribute values in a sanitized system, without mea-
suring how much information an adversary may learn from the publication of the system[2]. On
the other hand, several semantic criteria, such as the average benefit model[3, 16], t-closeness[7],
and δ-disclosure privacy[2] have been proposed to capture the incremental gain in the adver-
sary’s knowledge. The common feature of these criteria is that they compare the distribution
of attribute values in the sanitized system with that in the trivially sanitized system. The
semantic criteria are formulated as the t-closeness principle in [7].
Definition 9. An equivalence class of indσT (Q) is said to exhibit t-closeness if the distance
between the distribution of a sensitive attribute in that class and the distribution of the attribute
in the whole table is no more than a threshold t, and σT satisfies t-closeness if each of its
equivalence classes exhibits t-closeness.
164
Logic, Probability, and Privacy T.-s. Hsu, C.J. Liau, and D.W. Wang
we can specify heterogeneous requirements between different individuals, so it is possible to
achieve personalized privacy specification. For example, we can use @i¬[a1]ϕ ∧ @j¬[a1]ψ to
express different privacy requirements of individuals i and j.
Moreover, the logic allows arbitrary combinations of existing privacy requirements, so we
can express compound privacy criteria. For example, we can use @i¬[a1]ϕ∧la1(i) ≤ 1k to express
that both logical safety and k-anonymity are required for the individual i. Since unexpected
attacks may occur occasionally, existing criteria may be inadequate; hence, it may be necessary
to specify new criteria. For example, the logical safety criterion may be combined with δ-
disclosure to require formulas in Sec(i), instead of simply f -atoms, to satisfy the δ-disclosure
privacy criterion. In addition, it is possible to consider the weight of a secret in order to
measures the seriousness of revealing the secret. Thus, Wsec : U × Γ→ [0, 1] is defined as the
weight function for each individual and secret. Then, we can combine the weight with existing
privacy criteria to obtain new privacy protection models. This may facilitate a more effective
tradeoff between privacy protection and data utility. Our logic language provides a uniform
framework to meet the specification needs of such new criteria as well as existing ones.
References
[1] C. Areces and B. ten Cate. Hybrid logics. In P. Blackburn, J. van Benthem, and F. Wolter,
editors, Handbook of Modal Logic, pages 821–868. Elsevier, 2007.
[2] J. Brickell and V. Shmatikov. The cost of privacy: destruction of data-mining utility in anonymized
data publishing. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD), pages 70–78, 2008.
[3] Y.T. Chiang, Y.C. Chiang, T.-s. Hsu, C.J. Liau, and D.W. Wang. How much privacy? - a system
to safe guard personal privacy while releasing database. In Proceedings of the 3rd International
Conference on Rough Sets and Current Trends in Computing, LNCS 2475, pages 226–233. Springer-
Verlag, 2002.
[4] T. Dalenius. Finding a needle in a haystack - or identifying anonymous census records. Journal
of Official Statistics, 2(3):329–336, 1986.
[5] J. Halpern. Reasoning about Uncertainty. The MIT Press, 2003.
[6] T.-s. Hsu, C.J. Liau, and D.W. Wang. A logical model for privacy protection. In Proceedings of
the 4th International Conference on Information Security, LNCS 2200, pages 110–124. Springer-
Verlag, 2001.
[7] N. Li, T. Li, and S. Venkatasubramanian. t-closeness: Privacy beyond k-anonymity and l-diversity.
In Proc. of the 23rd International Conference on Data Engineering (ICDE), pages 106–115, 2007.
[8] A. Machanavajjhala, J. Gehrke, D. Kifer, and M. Venkitasubramaniam. l-diversity: Privacy
beyond k-anonymity. In Proc. of the 22nd IEEE International Conference on Data Engineering
(ICDE), page 24, 2006.
[9] A. Machanavajjhala, J. Gehrke, D. Kifer, and M. Venkitasubramaniam. l-diversity: Privacy
beyond k-anonymity. ACM Transactions on Knowledge Discovery from Data, 1(1), 2007.
[10] Z. Pawlak. Rough Sets–Theoretical Aspects of Reasoning about Data. Kluwer Academic Publishers,
1991.
[11] P. Samarati. Protecting respondents’ identities in microdata release. IEEE Transactions on
Knowledge and Data Engineering, 13(6):1010–1027, 2001.
[12] P. Samarati and L. Sweeney. Protecting privacy when disclosing information: k-anonymity and its
enforcement through generalization and suppression. Technical report SRI-CSL-98-04, Computer
Science Laboratory, SRI International, 1998.
[13] L. Sweeney. Guaranteeing anonymity when sharing medical data, the datafly system. A.I. Working
Paper AIWP-WP344, MIT AI Lab., 1997.
166
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/01
國科會補助計畫
計畫名稱: 顆粒計算之邏輯特徵
計畫主持人: 廖純中
計畫編號: 98-2221-E-001-013-MY3 學門領域: 人工智慧
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
