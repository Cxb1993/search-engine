 2
行政院國家科學委員會專題研究計畫成果報告 
             階層式微分法非線性最佳化 
Stagewise differentiation for nonlinear optimization 
計畫編號：NSC  99－2221－E－011－097－ 
執行期限：99 年 8 月 1 日至 100 年 7 月 31 日 
主持人：水谷英二 (Eiji Mizutani)   國立台灣科技大學工業管理系 
 
中文摘要 
這是一件主要是加州大學柏克萊分校之名譽教授 Stuart Dreyfus, 部分是加州大學柏克萊分
校之教授 James Demmel, 共同主持的研究計畫。我們發展實際的階層式微分法，基於最佳
控制理論與數值線性代數。因為我的中文表達比較不順，詳細的內文細節會用英文在下面
敘述。 
 
 
 
關鍵詞：最佳化控制，動態規劃，類神經網路，數值線性代數，機器學習法。 
 
 4
一、 研究目的 
 
Our objective is to investigate the mechanisms of the so-called plateau phenomena, in which 
first-order gradient-based methods typically causes a long plateau during neural-network learning 
with very little change in the objective function value E (hence, a flat region).  
 
Another goal is to extend our recently-developed stagewise back-propagation procedure to a 
CANFIS neuro-fuzzy modular network model that consists of multiple local-expert neural 
networks mediated by fuzzy membership functions. 
 
Those goals are built up on our previous project, where we have shown experimentally that our 
stagewise back-propagation procedure, derived from the second-order optimal control theory, can 
evaluate the full Hessian matrix H of E faster than ``standard'' rank-update methods that obtain 
only the Gauss-Newton Hessian matrix G (that is still used in Matlab Neural-Network Toolbox); 
this is truly significant in the nonlinear least squares sense. Here is summarized as our 2008 
theorem: 
 
Theorem (Mizutani & Dreyfus, 2008): 
The full Hessian H = G + S can be evaluated at the same cost as the Gauss-Newton Hessian G. 
 
In practice, the full Hessian matrix may not be positive (semi-)definite during the parameter 
updating (i.e., learning) phase, but the widely-employed trust-region nonlinear optimization 
method can deal with the indefinite Hessian since the underlying theory has thrived on the 
``negative curvatures'' over the last two decades. The trust-region approach based on the full 
Hessian matrix is of immense value in solving real-world ``large-residual'' nonlinear least squares 
problems because the matrix of second derivatives is important to efficiency. We thus analyze the 
learning behaviors using the curvature information of the Hessian in conjunction with our 
stagewise procedure in the framework of trust-region methods.  
 
 
 
 
 
 6
三、結果與討論 
 
Our eigenvalue analysis of the Hessian matrix and its numerical investigation lead to a main 
theorem below we have newly proved for multi-layer neural-network learning: 
 
A Main Theorem (Mizutani & Dreyfus, 2010): 
In rank-deficient nonlinear least squares neural-network learning, the full Hessian matrix of the 
sum of squared residuals tends to be indefinite (of nearly full rank) when the gradient vector has 
non-zero components.  
 
It follows from the above theorem that negative curvature often arises in multi-layer neural 
network learning in the vicinity of singularity region, where the Gauss-Newton Hessian matrix 
becomes singular; hence, the name, ``rank-deficient nonlinear least squares.’’ 
 
We also introduced the posed idea into the well-known variable projection method of Golub and 
Pereyra for the separable nonlinear least squares learning. The key ingredient is that we include 
the full Hessian information in conjunction with a block-least squares algorithm we developed in 
2004. This formulation allows us to exploit negative curvature efficiently if it exists. This 
approach, a joint work with Jim Demmel, has been reported in a recent IJCNN 2011 conference 
paper (see the list of publications later). 
 
We have further endeavored to emphasize the usefulness of our stagewise procedure in evaluating 
the Hessian matrix. More specifically, we extend the procedure for a CANFIS neuro-fuzzy 
modular network model that consists of multiple local-expert neural networks. This work is 
reported in a recent FSKD-2011 conference paper. Without our stagewise back-propagation, the 
numerical evidence is hard to obtain for the Hessian analysis especially in such a complex model. 
Hence, the computational convenience is enormously immense of the stagewise procedure. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 8
五、本案研究所衍生之成果 
學術期刊論文 
1. Eiji Mizutani and Stuart Dreyfus. An analysis on negative curvature induced by singularity in 
multi-layer neural-network learning.  Advances in Neural Information Processing Systems, 
vol. 23, J. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R.S. Zemel and A. Culotta, 
pp. 1669--1677,  MIT Press, Cambridge, 2010. 
 
國際學術會議 論文 
1. Eiji Mizutani and Jing-Yun Carey Fan. ``Online Neuro-Fuzzy CANFIS Hidden-Node 
Teaching,'' pp. 2559-2565 (EI Compendex). In Proc. of the 2011 IEEE International 
Conference on Fuzzy Systems, June 27-30, 2011, Taipei, Taiwan. 
2. Eiji Mizutani. ``On exploiting sparsity in CANFIS neuro-fuzzy modular network learning by 
second-order stagewise backpropagation,'' pp.202-206 (EI Compendex). In Proc. of the 2011 
Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD), July 
26-28, Shanghai, China, 2011. 
3. Eiji Mizutani and James Demmel. ``On Improving Trust-Region Variable Projection 
Algorithms for Separable Nonlinear Least Squares Learning,'' pp.397-404 (EI Compendex). 
In Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN 2011), 
San Jose CA, USA, July 31 -- August 5, 2011. 
 
國際學術會議 (abstract only) 
4. Eiji Mizutani and Stuart Dreyfus. ``Second-order stagewise procedures for nonlinear 
optimization applications." Abstract appeared in the booklet of the 2010 international 
conference: MOPTA (Modeling and Optimization: Theory and Applications), page 31, 
Bethlehem, PA, USA, August 18--20, 2010. 
 
Other research activities: 
1. Invited one-week lecture at the Dept. of System Design, Tokyo Metropolitan University on 
``Dynamic Programming and Optimal Control.’’ Hino Campus, Tokyo, Sept. 6 to 10, 2010. 
2. Invited one-week lecture at the Dept. of System Design, Tokyo Metropolitan University on 
``Nonlinear numerical optimization and multi-stage neural-network learning.’’ Hino Campus, 
Tokyo, Aug. 29 to Sept. 2, 2011. 
3. Plenary talk: ``Stagewise back-propagation --- from the Kelley-Bryson optimal control 
gradient formula to recent advancements for the Hessian-matrix evaluations’’ (for one hour), 
confirmed for the 2011 International Workshop on Advanced Computational Intelligence and 
Intelligence Informatics (IWACIII 2011), Suzhou, China, November 19-23, 2011.  
 
碩士論文 
1. 簡逸倫，民國 100 年，碩士論文：Knapsack Problems Revisited，國立台灣科技大學工業
管理研究所。 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                     日期：100 年10月20 日 
計畫編號 NSC 99－2221－E－011－097－ 
計畫名稱 階層式微分法 非線性最佳化 
出國人員
姓名 水谷英二 
服務機構
及職稱 
國立臺灣科技大學工業管理系  
    助理教授 
會議時間 99年 12月 6日至 99 年 12 月 11 日 會議地點 Vancouver, CANADA 
會議名稱 (英文) Twenty-Fourth Annual Conference on Neural Information 
       Processing Systems 
發表論文
題目 
(英文) An analysis on negative curvature induced by singularity in multi-layer  
             neural-network learning 
一、參加會議經過 
This conference, NIPS, is the most prestigious conference in the field of Machine Learning.  
All accepted papers were scheduled in one of the poster sessions (for 7PM – 11:59PM).  
After that, two-day workshops were held.   
二、與會心得 
The conference covers broad aspects of Machine Learning. Since all accepted papers were scheduled for 
poster presentations, we had plenty of time for face-to-face discussions at the conference site. This is one of 
the most attractive point of this NIPS conference. 
三、考察參觀活動(無是項活動者略) 
During the two-day workshops, I mainly attended the sessions concerning optimization and 
decision-making problems as well as the Sam Roweis Symposium.  
四、建議 
Since NIPS conference is a top-notch conference in Machine Learning (with only about 24% acceptance 
rate), it is absolutely worth while attending it.  
五、攜回資料名稱及內容 
Program booklet; all papers are included in electronic proceedings and posted on the web. 
 
 1
99 年度專題研究計畫研究成果彙整表 
計畫主持人：水谷英二 計畫編號：99-2221-E-011-097- 
計畫名稱：階層式微分法 非線性最佳化 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 4 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 1 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
