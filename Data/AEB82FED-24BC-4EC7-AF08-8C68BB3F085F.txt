following optimization problem:
f(x) = min{f(z) : z ∈ H}.
One of the most efficient and en-
forceable methods for solving 0 ∈
T (x) is the proximal point algo-
rithm which, staring with any vec-
tor x0 ∈ H, iteratively updates
xn+1 conforming to the following
recursion:
xn ∈ xn+1 + cnT (xn+1) (1)
where {cn}∞n=0 ⊂ [c,∞) , c > 0, is
a sequence of scalars. However, as
pointed out in [13], the ideal form
of the method is often impractical,
since in many cases solving prob-
lem (1) exactly is either impossi-
ble or as difficult as solving the
original problem 0 ∈ T (x). On
the other hand, there seems to be
little justification of the effort re-
quired to solve the problem accu-
rately when the iterate is far away
from the solution point. In [20],
Rockafellar gave an inexact variant
of the method:
xn+ en+1 ∈ xn+1+ cnT (xn+1) (2)
where {en+1} is regarded as an
error sequence. This method is
called an inexact proximal point
algorithm. Rockafellar [20] proved
that if en → 0 quickly enough
such that
∑∞
n=1 ‖en‖ < ∞, then
xn → z ∈ Rn with 0 ∈ T (z).
Because of its relaxed accuracy
requirement, the inexact proximal
point algorithm is more practical
than the exact one. Thus it has
been studied widely and various
forms of the method have been de-
veloped; see, e.g., [3,4,8,10-12,17-
19, 25]. In most of these papers,
the condition that the error term
being summable is essential for the
convergence of the method. In [20]
and some sequel papers (e.g., [5])
the accuracy criterion is
‖en+1‖ ≤ ηn‖xn+1 − xn‖ (3)
with
∑∞
n=0 ηn <∞.
Recently, Eckstein [13] extended
the method to Bregman-function-
based inexact proximal methods
and proved that the sequence {xn}
generated by the algorithm con-
verges to a root of T under the
conditions
∞∑
n=1
‖en‖ <∞,
∞∑
n=1
〈en, xn〉 <∞
(4)
(see Eqs. (18) and (19) in [13]).
Condition (4) is an assumption
on the whole generated sequence
{xn} and the error term sequence
{en}, and thus seems to be slightly
stronger, but it can be checked
and enforced in practice more eas-
ily than those that existed earlier.
On the other hand, as in He [15],
2
Algorithm 3.2
(i) Select x0 ∈ H arbitrarily.
(ii) Choose a regularization pa-
rameter cn > 0 with error en+1 ∈
H and compute
x˜n+1 := (I + cnT )
−1(xn + en+1).
(iii) Select a relaxation param-
eter αn ∈ [0, 1] and compute the
(n+ 1)th iterate:
xn+1 := αnx0+ (1−αn)x˜n+1. (8)
Then we have the following con-
vergence results for Algorithms 3.1
and 3.2. In order to achieve this,
we need to impose the following
condition:
‖en+1‖ ≤ ηn‖x˜n+1 − xn‖ (9)
with
∑∞
n=0 η
2
n <∞.
Theorem 3.1. Let {xn}∞n=0 be
the sequence generated by Algo-
rithm 3.1.Assume that condition
(9) is satisfied and that
(i) {αn} is bounded away form
1, namely 0 ≤ αn ≤ 1− δ for some
δ ∈ (0, 1) ;
(ii) {cn}∞n=0 ⊂ [c,∞) for some
c > 0.
Then the following statements are
valid:
(a) there exists an integer N0 ≥
0 such that for all n ≥ N0
‖xn+1 − x∗‖2 ≤ β‖xn − x∗‖2
− δ
2
‖x˜n+1 − xn‖2
for all x∗ ∈ S where β = 1+ 2η2n
1−2η2n .
(b) limn→∞ ‖x˜n+1 − xn‖ = 0;
(c) {xn} converges weakly to a
point in S.
Theorem 3.2. Let {xn}∞n=0 be
the sequence generated by Algo-
rithm 3.2. Assume that condition
(9) is satisfied and that
(i) limn→∞ αn = 0,
∑∞
n=0 αn =
∞;
(ii)limn→∞ cn =∞.
Then {xn} converges strongly to
Ps(x0).
4. Project Evaluation
In this project, we introduced
two new algorithms for finding
approximate solutions of zeros of
a maximal monotone operators.
Some new existence convergence
results for these algorithms were
obtained. We believe the results
obtained in this project makes sig-
nificant contribution to the lit-
erature. Part of the results of
this project has been written as a
full paper and has been submitted
to some well known international
journal for possible publication.
4
[12] J. Eckstein and D. P.
Bertsekas, On the Douglas-
Rachford splitting method
and the proximal points algo-
rithm for maximal monotone
operators, Math. Program-
ming, 55(1992), 293-318.
[13] J. Eckstein, Approx-
imate iterations in Bregman-
function-based proximal algo-
rithms, Math. Programming,
83(1998), 113-123.
[14] D. R. Han and B. S. He, A
new accuracy criterion for ap-
proximate proximal point al-
gorithms, J. Math. Anal.
Appl., 263(2001), 343-354.
[15] B. S. He, Inexact implicit
methods for monotone general
variational inequalities,Math.
Programming, 86(1999), 199-
217.
[16] I. Ekeland and R. Temam,
”Convex Analysis and Vari-
ational Problems”, North-
Holland, Amsterdam, 1973.
[17] K. C. Kiwiel, Proximal min-
imization methods with gen-
eralized Bregman functions,
SIAM J. Control Optim.,
35(1997), 1142-1168.
[18]
M. Kyono and M. Fukushima,
Nonlinear proximal decompo-
sition method for convex pro-
gramming, J. Optim. theory
Appl., 106(2000), 357-372.
[19] J. S. Pang, Inexact New-
ton methods for the non-
linear complementarity prob-
lem, Math. Programming,
36(1986), 54-71.
[20] R. T. Rockafellar, Mono-
tone operators and the prox-
imal point algorithm, SIAM
J. Control Optim., 14(1976),
877-898.
[21] M. V. Solodov and B. F.
Svaiter, A hybrid projection-
proximal point algorithm, J.
Convex Anal., 6(1999), 59-70.
[22] M. V. Solodov and B. F.
Svaiter, A hybrid approxi-
mate extragradient-
proximal point algorithm us-
ing the enlargement of a max-
imal monotone operator, Set-
valued Anal., 7(1999), 323-
345.
[23] M. V. Solodov and B. F.
Svaiter, An inexact hybrid
generalized proximal point al-
gorithm and some new re-
sult on the theory of Bregman
functions, Math. Oper. Res.,
25(2000), 214-230.
6
