1 研究背景
電腦圖學已經成為現代電影中不可或缺的技術，特別是用在特效及視覺效果上，而照片擬真的生
成渲染 (photorealistic rendering) 技術更是電影視覺效果的核心，因為不好的生成效果將會使觀
眾失去真實感，而讓電影不賣座。除了電影以外，產品設計師、建築師以及照明設計師也都需要照片
擬真的生成渲染 (photorealistic rendering) 演算法來幫他們預測其產品在各種光線下的外觀，其
中包括建築師需要知道室內設計在不同燈光下的感覺以及建築物在不同天氣狀況下的外觀；舞台設
計師需要瞭解在不同燈光下演唱會舞台的效果為何；汽車設計師想要看到汽車在不同照明環境下會
有怎樣的外觀等。這些都說明了基於物理的生成渲染技術的主要優點在於：即使沒有建立實際的場
景，也能產生影像去準確的預測在真實世界的外觀。但是，傳統基於 CPU 的全局照明 (CPU-based
global illumination) 演算法需要很大的計算量，其結果是不可能即時生成複雜的場景，亦即不
可能讓使用全局照明互動地編輯動畫場景。雖然，目前也已經有基於 GPU 的全局照明 (GPU-based
global illumination) 演算法可以使用，即使 GPU 平行運算核數大增，但是仍無法達到全局照明的
即時編輯效果。另外，在互動圖學及電腦遊戲兩大領域中，基於 GPU 的局部照明 (GPU-based local
illumination) 演算法提供使用者可以與即時生成結果互動的能力，此類方法的主要目的只是在得到
近似真實世界以及偽造出真實感覺的效果，但是在設計之初並沒有考慮到所有可能的光影效果以及物
體間的相互關係，而造成生成的影像會缺乏部分真實的燈光效果。雖然隨著電腦及演算法的改進，基
於 GPU 的局部照明 (GPU-based local illumination) 演算法、基於 CPU 的全局照明 (CPU-based
global illumination) 演算法以及基於 GPU 的全局照明 (GPU-based global illumination) 演算
法都不斷的在進步，但是三種都是獨立設計執行，無法綜合使用。因此，計畫中整合這三個系統來建
構一個綜合系統，該系統會使用局部照明 (local illumination) 演算法來提供場景轉換時所需的中
間訊息，局部照明演算法可以提供編輯動態場景 (animated scenes) 時所需的互動性及場景最終擬
真生成結果的預測性，用以節省使用全局生成法時，設定參數及場景所需的試驗時間，讓他們可以用
更簡單、更方便的設定所需的場景；同時基於 CPU 的全局照明 (CPU-based global illumination)
演算法以及基於 GPU 的全局照明 (GPU-based global illumination) 演算法則提供了最後物理真實
的模擬結果，讓設計者可以得到真實的感受。在計畫中，使用電腦遊戲引擎 OGRE3D 所提供的大量的
技術來做近似即時渲染的效果，這讓我們能夠有彈性的編輯欲得到的場景，並用更快的版本來檢查結
果，這提供了基於 CPU 以及基於 GPU 的生成渲染演算法所無法達到的互動性。而後端的真實生成技
術則將使用基於 CPU 的生成渲染函式庫－ PBRT，以及基於 GPU 的生成渲染函式庫－ Optix 來生
成擬真動畫。最終，此專案將產生一個整合基於 GPU 的局部照明 (GPU-based local illumination)
演算法、基於 CPU 的全局照明 (CPU-based global illumination) 演算法以及基於 GPU 的全局照
明 (GPU-based global illumination) 演算法來編輯動畫場景、預知動畫生成結果以及用以生成擬
真照片的動畫系統。除了建構一個整合系統外，此計畫亦研究如何提生動畫生成的效率，產生更佳的
擬真生成效果演算法。而擬真演算法在未來互動 3D 電視中可能成為一個重要技術，計畫中，我們對
3D 電視技術做一 SURVEY，來研究生成技術在未來 3D 電視上的可能性。
文獻回顧與探討
在互動圖學及電腦遊戲的領域中，大部份互動式的編輯工具及遊戲都是使用局部照明生成渲染
(local illumination) 演算法，特別是以 OpenGL 和 DirectX 為生成渲染的基本工具，雖然電腦硬
體大幅進步，運用硬體來生成的相關研究也大量被發表，但是這類研究大部分強調的是互動性而非正
確性，因此其生成的影像只能模擬部分燈光效果而非真實全部效果，以下會回顧部分的局部照明相關
研究。首先是反射，反射在全局照明 (global illumination) 是很重要的效果，Diefenbach [9, 10]
以及 Wojdala 等人 [39] 提出使用 GPU 的多通演算法來生成渲染平面鏡及曲面鏡的反射結果；多位
學者提出了使用環境映射來處理曲面反射的生成演算影像 [21, 23, 24, 6]；另有學者提出在空間
中，在所有可能表面上取樣點，在每個樣本點上，根據所有可能的視線方向以及光的方向，計算所有
可能的燈光效果並儲存。接著在生成渲染的階段，預先計算的資料會在動態、局部、以及低頻的光線
下，用來快速生成渲染物體 [28, 2, 19, 30, 3, 1, 5]；學者們也提出利用紋理 (texture) 技術，
來計算照明以及產生陰影 [17, 34, 35, 20, 22]。在點陣化硬體 (rasterization hardware) 上，
通常考慮到場景的漫射用以加速互動式顯示，有學者 [40, 41, 42, 12] 提出使用基於影像的散焦映
射 (image-based caustics map) 演算來加入玻璃以及水等等的聚光效果；這些基於 GPU 的局部生
結案報告 共 10 頁第 1 頁
Figure 1: 此圖顯示本計畫所欲建構的系統圖。在圖中，大致可以由三個不同系統所構成，由
OGRE3D 所構成的互動編輯子系統、由 PBRT 所構成的 CPU 全局生成渲染演算子系統以及由 OPTIX
所建構的 GPU 全局生成渲染演算子系統。
結案報告 共 10 頁第 3 頁
Figure 2: 左圖乃在 OGRE 系統中，所可以互動控制場景所生成的影像，而在中圖則是相同場景用
PBRT 所生成的影像成果，而在右圖中則是使用 OPTIX 所生成的影像成果。
2.3 3D電視技術 [37]
自從阿凡達 3D 立體電影上市風迷世界以來，3D 立體顯示己成近來媒體的寵兒，更成為在
CSE2010 展覽會中最引人注目的科技。幾乎所有的電視大廠都己經或將在 2010 中，推出他們的 3D
電視產品，屆時 3D 內容也可能隨著上市，其可能內容包括 2010 下半有多款藍光 3D 電影即將出現
在市場上，外加全球多家著名電視台亦將推出 3D 立體頻道。當然這波熱潮也同時掃向了個人電腦、
個人手機及電腦遊戲市場。但是目前市面上，最新款式的 3D 立體視覺系統即所謂的二眼式立體視覺
系統，仍在初期發展階段，這類系統通常要求使用者配戴特別的眼鐘或是頭載式設備觀看固定角度之
影片。雖然市場上預期下一代的 3D 立體感受系統將於 3 到 5 年內上市，新一代的系統不需要配
戴特別裝備，且可偵測使用者動作用於播放內容互動。但是目前技術仍需克服許多的挑戰包括了光學
系統的設計、顯示的技術、使用者互動介面、3D 訊號的傳輸、3D 資料的表示及編碼以及 3D 的通訊
系統等。此論文 [37] 呈現了 3D 影像通訊系統中主要技術的縱覽概述，特別是 3D 影片數位內容從
製作端到透過通訊系統傳輸到使用者端的生命週期為何？一般使用者對 3D 數位內容的感受為何？有
那些因素是可以影響使用者觀看的感受？如何表現及壓縮此類 3D 數位內容或虛擬實境？如何將資料
有效率地透過網路傳輸到使用者端？從生成者端、網路傳輸端及接收端的觀點上，如何設計新的技術
以提昇 3D 立體數位內容體驗的品質？在同時本論文也另外強調數個研究的可能方向給讀者，包括了
如何調整未來的網路架構才得以满足 3D 影片創新數位內容的傳輸服務要求？在 3D 影片傳輸系統
中，如何針對有效隱藏傳輸誤差以滿足使用者對擬真的 3D 影片內容之期待，而設計適當的 Quality
of Experience (QoE)？如何有效地藉由 3D 數位內容呈現及編碼中不同重要性及內容重覆性去使用
unequal error protection (UEP) 以及動態資源配置來改良系統的表現？何種編碼系統可以被使用
來增加編碼後的訊息流產生錯誤的抵抗力？如何設計在 4G 網路上傳輸 3D 影片數位內容的時程規畫
者及資料配置方式？如何有效地使用 4G 網路中 QoS 來支授 3D 影像內容傳輸？如何利用新的計算
概念像是綠色運算、雲端運算以及分散／合作運算等來設計傳輸源／傳輸者／接受者的點對點 3D 影
片傳輸系統？如何應用易擴縮性的概念來處理 3D 立體影片在異質混合網路通訊系統上的傳輸？
2.4 Voxelization [8]
在做實體的建模編輯跟生成前，一般需要演算方法將物體的表面資料 (surface representation)
轉換成以實體資訊 (volume representation)，此一類的演算法即是所謂的 Voxelization。由於
GPU(Graphic Processing Unit) 的快速進步，目前使用 GPU 為基礎的 voxelization 演算法，幾乎
己經可達成即時的運算得到結果。在論文 [8] 中提出一種新型以 GPU 演算為基礎的 voxelization
演算法。本演算法是利用設定 clipping plane 的方式將表面資料切片，再從中算取實體資料的演算
方法 [?]，在原方法中需要很多次 GPU 的轉移操作或是使用特別的編碼方式，因而限制其有效演算
速度及變換的複雜度。論文提出的演算法則是運用 GPU 新增之硬體功能包括 geometry shader 跟可
結案報告 共 10 頁第 5 頁
Figure 4: 這乃是使用 PMC-IP 演算法所生成的房間動畫。第一排是使用了 PMC-IP 演算法迭代演
算 64 次後所生成的動畫中編號 1、31、61 和 91 的影像。第二排則是使用每個像素 1024 個抽樣
子的傳統 path tracing 演算法所生成的動畫中編號 1、31、61 和 91 的影像。觀察生成的動畫，
PMC-IP 所生成的結果可以大量減少傳統 path tracing 中突出現雜訊，增加結果的可看度。
[4] P. Bekaert, M. Sbert, and J. Halton. Accelerating path tracing by re-using paths.
In Rendering Techniques ’02 (Proceedings of the Thirteenth Eurographics Workshop on Rendering),
pages 125–134, 2002.
[5] A. Ben-Artzi, R. Overbeck, and R. Ramamoorthi. Real-time brdf editing in complex
lighting. ACM Trans. Graph., 25(3):945–954, 2006.
[6] B. Cabral, M. Olano, and P. Nemec. Reflection space image based rendering. In In
SIGGRAPH 99, pages 165–170, 1999.
[7] M. Cammarano and H. W. Jensen. Time dependent photon mapping. In In Proceedings of the
13th Eurographics Workshop on Rendering, pages 135–144, 2002.
[8] S. Chang, Y. Lai, Y. Niu, F. Liu, and K. Hua. Real-time realistic voxel-based
rendering. In Pacific Graphics 2011, 2011.
[9] C. Dachsbacher and M. Stamminger. Reflective shadow maps. In I3D ’05: Proceedings of
the 2005 symposium on Interactive 3D graphics and games, pages 203–231, New York, NY, USA,
2005. ACM.
[10] C. Dachsbacher and M. Stamminger. Splatting indirect illumination. In I3D ’06:
Proceedings of the 2006 symposium on Interactive 3D graphics and games, pages 93–100, New
York, NY, USA, 2006. ACM.
[11] C. Damez, K. Dmitriev, and K. Myszkowski. State of the art in global illumination for
interactive applications and high-quality animations. Computer Graphics Forum, 22(1):
55–77, march 2003.
[12] S. T. Davis and C. Wyman. Interactive refractions with total internal reflection. In
Proceedings of Graphics Interface 2007, GI ’07, pages 185–190, New York, NY, USA, 2007.
ACM.
[13] K. Dmitriev, S. Brabec, K. Myszkowski, and H.-P. Seidel. Interactive global illumi-
nation using selective photon tracing. In Rendering Techniques ’02 (Proceedings of the 13th
Eurographics Workshop on Rendering), pages 25–36, 2002.
結案報告 共 10 頁第 7 頁
[23] J. Kautz and M. D. Mccool. Interactive rendering with arbitrary brdfs using separable
approximations. In In Eurographics Rendering Workshop, pages 281–292, 1999.
[24] J. Kautz, P. pau Vazquez, W. Heidrich, H. peter Seidel, and A. Udg. A unified approach
to prefiltered environment maps. pages 185–196. Springer, 2000.
[25] Y. Lai, S. Chenney, F. Liu, and S. Niu, Y.and Fan. Animation rendering with population
monte carlo image-plane sampler. The Visual Computer, pages 543–553, 2010.
[26] W. Martin, W. Martin, S. Parker, S. Parker, E. Reinhard, E. Reinhard, P. Shirley,
P. Shirley, W. Thompson, and W. Thompson. Temporally coherent interactive ray tracing.
Journal of Graphics Tools, 2:41–48, 2002.
[27] M. Meyer and J. Anderson. Statistical acceleration for animated global illumination.
ACM Trans. Graph., 25:1075–1080, July 2006.
[28] R. Ng, R. Ramamoorthi, and P. Hanrahan. All-frequency shadows using non-linear wavelet
lighting approximation. ACM Trans. Graph., 22(3):376–381, 2003.
[29] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kr ￿ ger, A. E. Lefohn, and
T. J. Purcell. A survey of general-purpose computation on graphics hardware. Computer
Graphics Forum, 26(1):80–113, 2007.
[30] F. Pellacini, K. Vidimcˇe, A. Lefohn, A. Mohr, M. Leone, and J. Warren. Lpics: a hybrid
hardware-accelerated relighting engine for computer cinematography. In SIGGRAPH
’05: ACM SIGGRAPH 2005 Papers, pages 464–470, New York, NY, USA, 2005. ACM.
[31] M. Pharr and G. Humphreys. Physically Based Rendering from Theory to Implementation. Morgan
Kaufmann, 2004.
[32] T. J. Purcell, C. Donner, M. Cammarano, H. W. Jensen, and P. Hanrahan. Photon mapping
on programmable graphics hardware. In Proceedings of the ACM SIGGRAPH/EUROGRAPHICS
Conference on Graphics Hardware, pages 41–50. Eurographics Association, 2003.
[33] M. Sbert and F. Castro. Reuse of paths in final gathering step with moving light
sources. In International Conference on Computational Science 2004, pages 189–196, 2004.
[34] M. Segal, C. Korobkin, R. van Widenfelt, J. Foran, and P. Haeberli. Fast shadows
and lighting effects using texture mapping. SIGGRAPH Comput. Graph., 26(2):249–252,
1992.
[35] J. Shade, D. Lischinski, D. H. Salesin, T. DeRose, and J. Snyder. Hierarchical
image caching for accelerated walkthroughs of complex environments. In SIGGRAPH
’96: Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages
75–82, New York, NY, USA, 1996. ACM.
[36] M. Smky, S.-i. Kinuwaki, R. Durikovic, and K. Myszkowski. Temporally coherent
irradiance caching for high quality animation rendering. In The European Association
for Computer Graphics 26th Annual Conference EUROGRAPHICS 2005, volume 24 of Computer
Graphics Forum, pages xx–xx, Dublin, Ireland, 2005. Blackwell.
[37] G.-M. Su, Y.-C. Lai, A. Kwasinski, and H. Wang. 3d video communications: Challenges
and opportunities. International Journal of Communication Systems, pages n/a–n/a, 2010.
[38] T. Tawara, K. Myszkowski, K. Dmitriev, V. Havran, C. Damez, and H.-P. Seidel.
Exploiting temporal coherence in global illumination. In SCCG ’04: Proceedings of the
20th spring conference on Computer graphics, pages 23–33. ACM Press, 2004. Invited Talk.
結案報告 共 10 頁第 9 頁
Noname manuscript No.
(will be inserted by the editor)
Animation Rendering With Population Monte Carlo Image-Plane
Sampler
Yu-Chi Lai · Stephen Chenney · Feng Liu ·
Yuzhen Niu · ShaoHua Fan
Received: date / Accepted: date
Abstract Except for the first frame, population Monte Carlo
image plane (PMC-IP) sampler renders with a start-up ker-
nel function learned from previous results by using motion
analysis techniques in the vision community to explore the
temporal coherence existing among kernel functions. Pre-
dicted kernel function can shift part of the uniformly dis-
tributed samples from regions with low visual variance to
regions with high visual variance at the start-up iteration and
reduce the temporal noise by considering the temporal rela-
tion of sample distributions among frames. In the following
iterations, the PMC-IP sampler adapts the kernel function
to select pixels for refinement according to a perceptually-
weighted variance criterion. Our results improve rendering
efficiency by a factor between 2 to 5 over existing techniques
in single frame rendering. The rendered animations are per-
ceptually more pleasant.
Keywords Ray-tracing · PMC · Monte Carlo · Global
Illumination
1 Introduction
Global illumination based on Monte Carlo integration pro-
vides the most general solution for photorealistic render-
Yu-Chi Lai
National Taiwan University of Science and Technology, R.O.C. E-
mail: yu-chi@mail.ntust.edu.tw
Stephen Chenney
Emergent Game Technology, U.S.A. E-mail: schenney@gmail.com
Feng Liu
University of Wisconsin-Madison, U.S.A. E-mail: fliu@cs.wisc.edu
Yuzhen Niu
Shandong University, China E-mail: yuzhen@cs.wisc.edu
ShaoHua fan
Financial Engineering Research Center at Suzhou University, China
and NYU, U.S.A E-mail: shaohua@gmail.com
ing problems. To reduce image noise (variance) at practi-
cal computation times is the core of research in the global
illumination community. To render an animation the tempo-
ral variance at each pixel among consecutive frames must
also be considered because our eyes are good at noticing
temporal inconsistency among consecutive frames for sur-
vival purpose; our algorithm is derived from the Population
Monte Carlo (PMC) sampling framework, which is a tech-
nique that adapts sampling distributions over iterations, all
with theoretical guarantees on error and little computational
overhead. Our algorithm generates a start-up kernel function
from previous rendered frames by considering the temporal
correlation among kernel functions. In addition, our algo-
rithm evenly distributes the variance over the image plane
in each frame to remove noisy spikes on the image and in
turn, reduce the temporal inconsistency generally existing
in a frame-by-frame ray-tracing based algorithm.
PMC algorithms iterate on a population of samples. For
our sampler, image-plane sampling (PMC-IP), the popula-
tion is a set of image-plane locations. To render each frame,
the population is initialized with a sample distribution whose
predictions are based on the previous rendering results, and
then PMC-IP generates an intermediate image to start the it-
eration. Any information available at this stage can then be
used to adapt a kernel function that produces a new popu-
lation. The initial prediction of the kernel function is based
on the result of the current frame rendered with a few sam-
ples in each pixel, the result of previous frames, and the
sample distribution of previous frames. We can explore the
temporal coherence of sample distributions among consec-
utive frames by using computer vision techniques to gener-
ate a good start-up kernel function. This prediction prevents
the redundant probed samples on smooth regions of the im-
age. In addition, the prediction takes the temporal variance
into account: perceptually high variance regions in previ-
ous frames have higher probability to be perceptually high
3seen through u in the direction−ω at t-th frame, determined
by the projection function of the camera. We are ignoring,
for discussion purposes, depth of field effects, which would
necessitate integration over directions out of the pixel, and
motion blur, which would require integration over time. In
the following discussion, we will neglect the denotation of t
for the simplification of description. And all the adaptation
of the sample distribution happens in the process of render-
ing a single frame.
An image-plane sampler selects the image-plane loca-
tions, x in Equation 1 for a specific frame. For simplicity,
assume we are working with a ray-tracing style algorithm
that shoots from the eye out into the scene. Adaptive sam-
pling aims to send more rays through image locations that
have high noise, while avoiding bias in the final result.
Taking an importance sampling view, given a set of sam-
ples, {X1, . . . ,XN} from an importance function p(x) for a
single frame, each pixel is estimated using
Iˆi, j =
1
n
N
∑
k=1
Wi, j(Xk)L(Xk,ω)
p(Xk)
(2)
The source of bias in most existing adaptive image-plane
samplers is revealed here. Adaptive sampling without bias
must avoid decisions to terminate sampling at an individual
pixel, and instead look at the entire image plane to decide
where a certain number of new samples will be cast. Every
pixel with non-zero brightness must have non-zero probabil-
ity of being chosen for a sample, regardless of its estimated
error. This guarantee that all pixels will have chance to re-
ceive samples to achieve unbiasedness.
We also note the Equation 2 can be broken into many
integrals, one for the support of each pixel. Provided p(x) is
known in each sub-domain, the global nature of p(x) is not
important.
Figure 1 summarizes the final PMC-IP algorithm for a
single frame:
1 Estimate the start-up kernel function α(0)k from previous frames
2 for s= 0, · · · ,S
4 Use DMS to allocate samples according to α(s)k
5 Generate samples from K(s)IP (x) and accumulate to image
6 Compute the perceptually-weighted variance image
7 Compute α(s+1)k for each pixel k
Fig. 1 The PMC-IP Algorithm for rendering a single frame.
3.1 Integrating the Sampler into a Global Rendering
System
The PMC-IP is easily incorporated into a single rendering
pipeline and allows us to improve the rendering efficiency
for ray-tracing based algorithms such as path tracing, pho-
tonmapping. Figure 2 shows a modern plug-in style Monte
Carlo rendering framework. The only core framework modi-
fication required to support adaptive sampling is the addition
of a feedback path from the output image generator back to
the samplers, required to pass information from one sam-
pling iteration back to the samplers for the next iteration.
The PMC-IP sampler also contains a tracking algorithm, de-
scribed in Section 3.2, to guess the sample distribution from
the previous rendering frames and sample distributions. k
The kernel function is the starting point in creating a
PMC algorithm for adaptive image-plane sampling. We need
a function that has adaptable parameters, is cheap to sample
from, and supports stratification. This can be achieved with
a mixture model of component distributions, hIP,(i, j)(x), one
for each pixel:
K(s)IP (x) = ∑
(i, j)∈P
α(s)
(i, j)hIP,(i, j)(x), ∑
(i, j)∈P
α(s)
(i, j) = 1.
Where (i, j) is the pixel coordinate and P is the set of all
pixels in this image. Each component is uniform over the
domain of a single pixel integral. The parameters to the dis-
tribution are all the α(s)
(i, j) values, and these change for each
iteration, s. We achieve an unbiased result if every α(s)
(i, j) ≥ ε ,
where ε is a small positive constant (we use 0.01). We en-
force this through the adaptive process, and the use of ε ,
rather than 0, provides some assurance that we will not over-
look important contributions (referred to as defensive sam-
pling [11]).
The use of a mixture as the kernel results in a D-kernel
PMC [5] algorithm. Sampling from such a distribution is
achieved by choosing a pixel, (i, j) according to the α(s)
(i, j),
and then sampling from hIP,(i, j)(x). The latter can be done
with a low-discrepancy sampler within each pixel, giving
sub-pixel stratification. Stratification across the entire image
plane can be achieved through deterministic mixture sam-
pling, which we describe shortly. The importance function
p(x) in Equation 2 for a given pixel is p(x) = hIP,(i, j)(x).
This can be derived by considering each pixel as an individ-
ual integral and observing that only one mixture component
has non-zero probability of contributing to each integral.
Notice that this kernel function is not conditional:KIP(x(s)|x(s−1))=
KIP(x(s)). Hence, for image-plane sampling we do not in-
clude a resampling step in the PMC algorithm because no
samples are re-used. The knowledge gained from prior sam-
ples is instead used to adapt the kernel function.
3.2 Predict a Good Initial Start-up Kernel
When observing the kernel function for each frame by us-
ing PMC-IP algorithm, we realize that the high-probability
regions should be temporally correlated among consecutive
5Fig. 3 This is the prediction of the kernel function at frame 4. The images from left are result at frame 3, rough result at frame 4, the final kernel
function at frame 3, the final predicted kernel function at frame 4, and the final kernel function at frame 4. We analyze the deviation by using the
root mean squqre error which is 9.02×10−07
pixel, with pixels that require more samples having higher
high α(s)
(i, j) for the component that covers the pixel.
An appropriate criteria assigns α(s)
(i, j) proportional to the
perceptually-weighted variance at each pixel. The algorithm
tracks the sample variance in power seen among samples
that contribute to each pixel. To account for perception, the
result is divided by the threshold-versus-intensity function
tvi(L) introduced by Ferweda et al. [7]. Normalization also
accounts for ε .
α ′i, j =
σ2(i, j)
tvi(L(i, j))
α(s)i, j = ε+
(1− ε)α ′(i, j)
∑(i′, j′)∈P α ′(i′, j′)
The first iteration for the first frame of the algorithm
samples uniformly over the image plane, so this criteria can
always be computed. The first iteration for the following
frames predict the kernel function according to the result of
previous frame and the current rendering algorithm. The left
images in Figure 4 show an example of an α(0)
(i, j) map for a
given initial image. The perceptual term in the error image
prevents very high errors in both bright regions (a problem
with unweighted variance) and dark areas (a problem with
luminance-weighted variance).
Note that α(s)
(i, j) ≥ ε , so there is a non-zero probability
of generating a sample at any given image plane location.
This meets the requirement for importance sampling that the
importance function is non-zero everywhere where the inte-
grand is non-zero. Furthermore, as the total sample count ap-
proaches infinity, the count at any pixel also approaches in-
finity. Hence, with the correctly computed importance weights
(Equation 2), the algorithm is unbiased.
3.4 Deterministic Mixture Sampling
Randomly sampling from the discrete distribution defined
by the α(s)
(i, j) produces excess noise — some pixels get far
more or fewer samples than their expected value. This prob-
lem is solved with deterministic mixture sampling, DMS,
which is designed to give each component (pixel) a num-
ber of samples roughly proportional to its α(s)
(i, j). Determin-
Image Method # SPP T(s) Err P-Eff
Buddha Uniform 10 58.1 0.625 0.027
PMC-IP 2+4+4 62.4 0.116 0.138
Box Uniform 16 163 0.545 0.011
Uniform 32 328 0.255 0.012
PMC-IP 4+6+6 169 0.182 0.033
Table 1 Measurements comparing PMC-IP and uniform image-plane
sampling, for equal total sample counts. The P-Eff is perceptual effi-
ciency used in [13]. The Buddha image computed direct lighting with
the MIS method, with a total of 8 lighting samples for each pixel sam-
ple. PMC-IP sampling improves the perceptual-based RMS error by a
factor of 5.4 over uniform sampling with only 7.5% more computa-
tion time. It corresponds to an improvement in efficiency of 5.01. The
Cornell Box images use path tracing to compute global illumination
including caustics. Comparing with images of 16 SPPs, PMC-IP im-
proves the efficiency by a factor of 2.65.
istic mixture sampling is unbiased and always gives lower
variance when compared to random mixture sampling, as
proven by Hesterberg [11].
The number of samples per iteration, N, (the population
size) is fixed at a small multiple of the number of pixels. We
typically use 4 samples per pixel, which balances between
spending too much effort on any one iteration and the over-
head of computing a new set of kernel parameters. For each
pixel, the deterministic sampler computes n′(i, j) = Nα(i, j),
the target number of samples for that pixel. It takes bn′(i, j)c
samples from each pixel (i, j)’s component. The remaining
un-allocated samples are sampled from the residual distri-
bution with probability n′(i, j)−bn′(i, j)c at each pixel (suitably
normalized).
4 Results
This section presents the rendering results when we apply
our PMC-IP algorithm to render a single frame of scenes
and animation scenes by plugging in our sampler into the
modern global illumination framework to demonstrate the
usefulness of our algorithm
7introduce the perceptually-based mean squared efficiency
(P-Eff) metric for comparing algorithms, computed as:
Err =
∑pixels e2
tvi(L)
, P-Eff=
1
T ×Err
where e is the difference in intensity between a pixel and
the ground truth value and T is the running time of the algo-
rithm on that image. P-Eff is a measure of how much longer
(or less) you would need to run one algorithm to reach the
perceptual quality of another [21].
The final adaptive image shown is the unweighted aver-
age of three sub-images (initial and two iterations). While
weighting each sub-image may be helpful, in this context it
is not clear that the samples from one iteration are any bet-
ter than those from another because they all used the same
per-sample parameters. We obtained more samples in places
that needed it, but not better samples.
The path tracing algorithm differs from a standard ver-
sion only in how pixel locations are chosen. The improve-
ment due to PMC-IP sampling is more pronounced in this
situation because some areas of the image (the caustic, for
instance) have a much higher variance than others due to the
difficulty of sampling such paths. We compare the results
in two aspects. First, we compare them visually. Working
toward a target image quality, we would continue iterating
the PMC-IP sampler until we were satisfied with the over-
all variance. In Fig. 5, we show the final result of the Cor-
nell box and the comparison between a set of snapshots of
the caustic region between the general PT algorithm and our
adaptive algorithm. We can see that the result of 16th (equiv-
alent to 64 SPPs) is even better than the result of 256 SPPs.
We also notice that even at diffuse regions, our method con-
verges more quickly than the general PT algorithm.
Second, we compare the efficiencies of our algorithm
and the PT algorithm. In this Table 1, we see that PMC-IP
sampling with a total of 16 SPPs improves the efficiency by
a factor of 3 to the uniform sampling with 16 SPPs and 32
SPPs. In this result, we ran our examples for a fixed num-
ber of iterations (bounded by computation time). Note that
because the PMC-IP sampler evenly spreads variance over
the image, an overall image error bound is very unlikely to
leave any high-error pixels.
Photon mapping is an industry standard method for global
illumination, and we implemented the above method for the
gather portion of a photon mapping implementation. Fig-
ure 6 shows a room scene computed with the system. Look-
ing at the blown-up images of right wall by the lamp, in Fig-
ure 6, we can see that our algorithm converges more rapidly
to a smooth image. This is because PMC-IP puts more sam-
ples in this region because of its high variance nature. Our
algorithm improves the efficiency of the final result.
4.2 Animation Rendered with the Adaptive Image Plane
Sampler
We apply our frame-based PMC-IP animation rendering al-
gorithm to two animation scenes: Cornell Box and Room.
Each contains the movement of the camera, objects, and
lights. For the Cornell Box scene, we render each frame with
16 iterations and with each iteration averaging 16 samples
per pixel by plugging in PMC-IP into general path tracing
algorithm. To do the comparison, we also render the anima-
tion with 256 samples per pixel by plugging in uniform sam-
ple distribution into the path tracing algorithm. For the room
scene, we render each frame with 64 iterations and each iter-
ation with averaging 16 samples per pixel and with uniform
1024 samples per pixel. The overhead of prediction of the
kernel function is roughly 5s. The overhead of adjusting the
kernel function at each iteration is about 10s. The cost of
tracing a view ray through a scene is the same because the
only difference is the start-up position of a view ray. Figure 7
shows the final results of 4 frames taken from a sequence
of animation in a Cornell Box scene. When visually check-
ing the prediction and final estimation, the position of the
high-probability region and the strength are similar. When
numerically analyzing the deviation between the prediction
and final estimation of the kernel function, α(i, j), with root
mean square error for each frame, the values are small with
a maximum of 1.26× 10−6. The predictor does a good job
in tracking the high-sample region roughly corresponding
to the caustics lighting on the ground. As a result, our al-
gorithm consistently puts more effort at this region to get
a smoother caustics lighting region. When comparing the
animation result, we can see that the variance of the caus-
tics region when using the uniform 256 SPPs is roughly the
same as the variance of caustics regions when plugging in
our algorithm with 4 iterations, with averaging 16 SPPs per
iteration. Our algorithm with 16 iterations, with averaging
16 SPPs per iteration can generate a much smoother caus-
tics region.
Figure 8 shows the final results of 4 frames taken from
a sequence of animation in a room scene when plugging in
PMC-IP and plugging in uniform sample distribution with
the path tracing. We notice that our algorithm distributes
more samples to high variance regions and therefore renders
images with less noise spikes. In addition to even distribu-
tion of variance on the image plane, our algorithm uses the
prediction of the kernel function to incorporate the temporal
information of the sample distribution among consecutive
frames. As a result, the inconsistency among consecutive
frames is lower. We can observe this from how the chance of
a noise spike popping up in the animation rendered by using
general path tracing algorithm is higher than the chance in
the animation rendered with our algorithm. The result is a
more perceptually pleasant animation.
9Fig. 7 A sequence of images animation of Cornell Box scene are rendered using the PMC-IP algorithm. The top row of images are the final
results of the 1st, 31st, 61st, and 91st frame using PMC-IP algorithm with 16 iterations, with each iteration averaging 16 SPPs. The second row
are the α(i, j) estimated after rendering at the 1st, 31st, 61st, and 91st frame. The third row are the prediction of α(i, j) using prediction algorithm in
Section 3.2 for the 1st, 31st, 61st, and 91st frame. The corresponding root mean square error between the prediction and final value of α(i, j) are
9.28×10−7, 1.26×10−6, 8.81×10−7, and 1.16×10−6.
2. Bolin MR, Meyer GW (1998) A perceptually based
adaptive sampling algorithm. In: SIGGRAPH ’98, pp
299–309
3. Dayal A, Woolley C, Watson B, Luebke D (2005) Adap-
tive frameless rendering. In: Proc. of the 16th Euro-
graphics Symposium on Rendering, pp 265–275
4. Dippe´ MAZ, Wold EH (1985) Antialiasing through
stochastic sampling. In: SIGGRAPH ’85, pp 69–78
5. Douc R, Guillin A, Marin JM, Robert CP (2005)
Convergence of adaptive sampling schemes. Techni-
cal Report 2005-6, University Paris Dauphine, URL
http://www.cmap.polytechnique.fr/ douc/Page/Research/dgmr.pdf
6. Farrugia JP, Pe´roche B (2004) A progressive rendering
algorithm using an adaptive pereptually based image
metric. Computer Graphics Forum (Proc of Eurograph-
ics 2004) 23(3):605–614
7. Ferwerda JA, Pattanaik SN, Shirley P, Greenberg DP
(1996) A model of visual adaptation for realistic image
synthesis. In: SIGGRAPH ’96, pp 249–258
8. Fischler MA, Bolles RC (1981) Random sample
consensus: a paradigm for model fitting with ap-
plications to image analysis and automated car-
tography. Commun ACM 24(6):381–395, DOI
http://doi.acm.org/10.1145/358669.358692
9. Ghosh A, Doucet A, Heidrich W (2006) Sequential
sampling for dynamic environment map illumination.
In: Proc. Eurographics Symposium on Rendering, pp
115–126
10. Glassner A (1995) Principles of Digital Image Synthe-
sis. Morgan Kaufmann
11. Hesterberg T (1995) Weighted average importance sam-
pling and defensive mixture distributions. Technomet-
rics 37:185–194
12. Kirk D, Arvo J (1991) Unbiased sampling techniques
for image synthesis. In: SIGGRAPH ’91, pp 153–156
13. Lai Y, Fan S, Chenney S, Dyer C (2007) Photorealis-
tic image rendering with population monte carlo energy
redistribution. In: Eurographics Symposium on Render-
ing, pp 287–296
14. Lai YC, Liu F, Zhang L, Dyer C (2008) Efficient
schemes for monte carlo markov chain algorithms in
global illumination. In: ISVC ’08: Proceedings of the
4th International Symposium on Advances in Visual
Computing, pp 614–623
15. Lee ME, Redner RA, Uselton SP (1985) Statistically
optimized sampling for distributed ray tracing. In: SIG-
GRAPH ’85, pp 61–68
16. Lowe DG (2004) Distinctive image features from scale-
invariant keypoints. International Journal of Computer
Pacific Graphics (2011) Short Papers
Bing-Yu Chen, Jan Kautz, Tong-Yee Lee, and Ming C. Lin (Editors)
Real-time Realistic Voxel-based Rendering
S.-H. Chang1 and Y.-C. Lai1† and Y. Niu2 and F. Liu2 and K.-L. Hua1
1National Taiwan University of Science and Technology, Taiwan
2 Portland State University, U.S.A.
Abstract
With the advance of graphics hardware, setting 3D texture as render target is newly available to allow voxelization
algorithms to record the existence, color and normal information in a voxel directly without specific encoding and
decoding mechanism. In this paper two new voxel-based applications are proposed to take advantage of this new
functionality for interactively rendering realistic lighting effects including shadow of objects with complex occlu-
sion and refraction and transmission of transparent objects. An absorption coefficient is computed according to
the number of surface drawing in each voxel during voxelization and used to compute the amount of light passing
through partial occluded complex objects. The refraction and transmission of light passing through transparent
objects is simulated by our multiple refraction algorithm using surface normal, transmission coefficient and re-
fraction index in each voxel. All these applications can generate the result in real-time without any preprocessing
step. Additionally, we also found that the newly available geometry shader can be used to transform a highly com-
plex surface-represented scene into a set of high-resolution voxels in only one GPU pass. This possibly improve
the efficiency of the voxelization process.
Categories and Subject Descriptors (according to ACM CCS): I.3.5 [Computer Graphics]: Computational Geometry
and Object Modeling—Curve, surface, solid, and object representations
1. Introduction
Realism is important for human perception but interactiv-
ity is more important for many applications. The refrac-
tion and transmission of light as it passes through dif-
ferent materials result in many beautiful and intriguing
effects. The requirement of interactivity motivates many
research to look for hardware-accelerated approximation
to simulate refraction effects [DB97, HLFpS99, Ohb03,
Oli, LKM01, Ade03, GS04,WD06, OB07, IZT∗07, SZS∗08,
WZHB09,CO,LES09]. Among these, Eikonal rendering al-
gorithms [IZT∗07, SZS∗08] which transforms edge bound-
aries to the gradient of refraction indices for simulating re-
fraction and multiple refraction methods [WD06,OB07,CO,
LES09] which use image-based algoirhtms to approximate
the light transport through a transparent object are popu-
lar because they can provide realistic results in a highly in-
teractive frame rate. However, there are still limitations in
Eikonal methods including complex computation in tranfor-
† Corresponding author, NSC 99-2218-E-011-005-, Taiwan
mation and difficulties in adding absorption and in image-
based methods including the requirement of an extra image
map per object for estimating traversal distance, the assump-
tion of a none-self-occluded object and disability to simulate
multiple refraction and total internal reflection effects. In this
paper we proposed to use volumetric representation to over-
come these limitations. GPU-based voxelization first slices
the surface models into a set of unit-sized voxels stored in
a 3D volume texture. Then, the multiple-refraction method
uses the voxelization results to render the refraction and
transmission effect. Our algorithm takes advantage of the
flexibility and adjustability of the 3D volume texture to com-
pute and store the surface and volume information including
normal, refraction index and transmission coefficient for bet-
ter approximation of refraction and transmission. The infor-
mation allows us to trace a view ray from the camera into
the scene and when intersecting with the surface boundary
voxels, the normal and refraction index is used to compute
the new propagation direction. The process continues until
the ray shoots out of the scene and the color indexed by the
ray direction and the attenuation accumulated along the path
c© The Eurographics Association 2011.
Shu-Huai Chang & Yu-Chi Lai & Yuzhen Niu & Feng Liu & Kai-Lung Hua / Real-time Realistic Voxel-based Rendering
Detecting the voxels intersected with triangle
1 For each triangle, Tri
2 z0 = Z(Tri.V0), z1 = Z(Tri.V1),z2 = Z(Tri.V2)
3 maxslice =max(z0,z1,z2)/thickness
4 minslice =min(z0,z1,z2)/thickness
5 For i= minslice to maxslice
6 Planenear = i∗ thickness
7 Plane f ar = Planenear+ thickness
8 Set Planenear and Plane f ar to projection matrix
9 If (Intersect(Tri))
10 Rasterize Tri into the slice
Figure 1: This is the pseudo code for computing the bound-
ary voxels of a triangle, Tri. V denotes a vertex of a triangle,
thickness is the voxel size which is a user specified value,Z()
is a function to extract the depth value of a vertex after trans-
forming the position of the vertex into the camera coordi-
nate,max() /min() computes the maximum/minimum value
among the set of input values and Intersect() is a function
to test whether the triangle is valid after being culled by the
clipping planes.
1. The voxelization camera is set at the position of the light
source and aligned with the light direction.
2. Our algorithm voxelizes the scene and computes and
stores the absorption of each voxel. The absorption coef-
ficient is computed by accumulating the number of writ-
ing in each voxel and then this number is multiplied by a
user-defined constant to get the absorption coefficient be-
cause the number of drawing reflects the degree of partial
occlusion in the voxel.
3. During the rendering process, the voxel position, (x,y,s),
of the first intersection point from the view is computed.
4. The amount of occlusion can be computed using the fol-
lowing equation, ∑si=0α(x,y, i)×E(x,y, i) where α() de-
scribes the light absorption in this voxel and E() is an
occupation flag which 1 represents that the voxel is occu-
pied by some object.
4. Refraction and Transmission
Refraction is the change in propagation direction of a light
ray when it transports from one medium to another and the
light propagation direction change can be described by the
Snell’s Law. But single refraction is not enough to describe
the light transport through a transparent object because gen-
erally a light ray enters and exits an object in a pair and it is
a multiple refraction phenomenon.
4.1. Two-surface refraction
Wyman et al. [Wym05] proposed that multiple refraction
may be simplified to a two-surface-refraction effect: one
happens when light enters the object and the other happens
when light exits. The first refraction can use the normal of
the intersection point and the incident direction to compute
the first refraction direction, T⃗1. If the traversal distance, d,
between the first and the second refraction point can be esti-
mated, the second refraction position can be estimated with
P2 = P1+ dT⃗1 where P1 and P2 are the first and second re-
fraction position, and T⃗1 is the refraction direction after the
first refraction. Wyman et al. [Wym05] proposed an image-
based method to estimate d without considering the first re-
fraction direction. We realized that our voxelization result
can find a better estimate of d with a similar manner de-
scribed in Sec. 3, P2 can be computed as described previ-
ously and projected into the voxel space to extract the sur-
face normal, N⃗2 and refraction index and T⃗2 can be computed
with T⃗1, N⃗2 and refraction index.
4.1.1. Multiple-surface refraction
The two-surface-refraction method cannot render all trans-
mittance lighting effects when light hits a transparent ob-
ject in a scene. In addition it also has some limit in the al-
lowable models and transmittance. Thus, a multiple-surface-
refraction algorithm is proposed to simulate the refractions
and reflections inside a scene. The same voxelization pro-
cess described in the two-surface-refraction method is used.
When rendering the scene, the view initiates a view ray pass-
ing through the center of a pixel. Then, the ray is propa-
gated inside the voxel space and every time when the ray
hits a boundary voxel, the ray is refracted according to the
Snell’s law. In order to properly locate the boundary voxel
for refraction, the propagating distance must be set prop-
erly to prevent missing the boundary voxel during the traver-
sal procedure and wasting efforts in extra propagation. Our
implementation chooses the physical distance to propagate
through a voxel as the propagation step distance. Then, the
position where the next refraction event happens can be com-
puted with Pi = Pi−1 + thickness× Tvoxel(T⃗i)/cosθ where
Pi−1 is the current position, thickness represents the physical
size of the voxel, T⃗i−1 is the current ray propagation direc-
tion, Tvoxel() is a function to transform the ray into the voxel
coordinate for locating the voxel which records the normal
and transmittance information and θ is the angle between the
ray and the dominant component axis of the ray. The next
step computes the refracted view ray direction according to
the following equation:
T⃗i =
{
re f (T⃗i−1,Nv(Pi),Tv(Pi)) i f (E(Pi)) = 1
T⃗i−1 otherwise
(1)
where re f () computes the new ray direction according to the
Snell’s law, E(Pi) is a flag which indicates whether the voxel
at Pi is a boundary voxel or not, Nv(Pi) extracts the normal
c© The Eurographics Association 2011.
Shu-Huai Chang & Yu-Chi Lai & Yuzhen Niu & Feng Liu & Kai-Lung Hua / Real-time Realistic Voxel-based Rendering
Name Alg. # Tris 1283ms 2563ms 5123ms
Torus Ours 800 1.66 3.73 10.91
Grid 2.67 7.29 641.03
Venusm Ours 43357 3.19 5.04 12.41
Grid 12.41 73.02 746.27
Horse Ours 96966 4.28 7.65 15.09
Grid 15.09 156.99 925.93
Hand Ours 654666 11.5 20.00 27.68
Grid 27.68 632.91 2500
Dragon2 Ours 871414 16.54 23.85 31.06
Grid 31.06 1282.05 3448.28
Table 1: This shows the time needed to voxelize models with
different triangle counts with different resolution settings us-
ing our voxelization algorithm marked with Ours and the
voxelization algorithm proposed by Ignacio [Lla07] et al.
marked with Grid. The performance is measure in ms.
Figure 2: Rendering the shadow of a surface-represented
tree model using ray tracing is too time consuming and thus
transparent shadow map is a better choice. The right is a
tree rendered with a uniform absorption coefficient for the
entire tree. The right is a tree rendered with different voxel
absorption coefficients computed according to the material
and the degree of occlusion. The trunk shadow is dark and
the shadow of the leaves varies according the degree of oc-
clusion.
will also increase. It is even worse that the cost to access the
texture data in the GPU memory space is much higher than
the cost to access CPU memory . Thus, general practice is
to reduce the number of slices with the increase in the slice
resolution to reduce the number of processed slices with ac-
ceptable rendering quality.
5.2. Transparent Shadow
A tree rendered with its transparent shadow is shown in
Fig. 2. The trunk is opaque and the leaves are partially oc-
cluded and we model these partial occlusions using different
absorption coefficients in each voxel. As shown in the right
picture of the figure, Eisemann’s method renders the shadow
of the tree with a uniform absorption coefficient and thus the
shadow of the trunk is not dark enough to show the solid-
ness and the leaf shadow does not show the degree of the
Figure 3: This demonstrates the strength of recording sur-
face normal and transmittance. Our algorithm can render
an object with multiple different materials.
Figure 4: The left, middle and right are rendered using the
two-refraction [Wym05], multiple-refraction and ray tracing
methods.
occlusion along the light paths. Our algorithm computes ab-
sorption coefficients according to the material and the degree
of occlusion. Thus, this gives large absorption coefficients to
the trunk voxels and larger absorption coefficients to highly
occluded leaf voxels and smaller absorption coefficients to
lowly occluded leaf voxels. This makes the trunk shadow
dark and the leaf shadow in the blow-up image in the left
of the figure demonstrates different degree of darkness ac-
cording to the degree of occlusion. Because our absorption
coefficient takes the density of occlusion into account, the
rendered shadow looks more realistic. This realism comes
with almost negligible because the number of drawing in
each pixel is the byproduct of voxelization.
5.3. Refraction and Transmission
The multiple-surface-refraction algorithm can simulate the
multiple refraction and total internal reflection effects to gen-
erate realistic refraction in real time. It is generally more ef-
ficient than traditional ray-tracing. This is because a fixed
number of voxels can be used to represent a complex sur-
face model. Thus, the traversal cost can be limited in a
controllable amount and so is the efficiency of rendering.
Fig. 4 shows the comparison among Wyman’s image-based
2-refraction method [Wym05], multiple-refraction and ray
tracing methods when rendering the refraction of a glass
vase. The image-based method renders the glass vase as light
passing through two planar surfaces. Failure to render ob-
jects with self-occlusion is one of the major problems ex-
isting in the image-based refraction method. In addition, it
cannot simulate the multiple refraction and total internal re-
flection effects, either. The results generated by our multiple-
refraction method are more closed to the one generated by
ray tracing. These results demonstrate that our method can
c© The Eurographics Association 2011.
出國心得 
 
此次出國主要是參加於新加坡所主辦的 CGW2010。發表個人論文。在發表過程中，與
其它學者充分溝通，對自己研究的未來方向很有助益，再加上認識多位學者，其中包括
本國最著名的圖學研究學者李同益教授及林照宏教授等，對本人未來的人際關係及圖學
領域研究，十分有助益。同時也參觀了新加坡這個亞洲四小龍，去認識一個不同的國度
及制度，是一個十分難得的經驗。 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：賴祐吉 計畫編號：99-2218-E-011-005- 
計畫名稱：使用全局照明方法有效生成電腦動畫之研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 2 0 100%  
研究報告/技術報告 2 0 100%  
研討會論文 1 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 3 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
