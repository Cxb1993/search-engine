英 文 摘 要 ： An embedded sensor network is a network of sensor 
nodes deployed in the physical world that interacts 
with the environment. Each sensor node is a 
physically small and relatively inexpensive computer 
that has one or more sensors. These sensor nodes are 
often networked, allowing them to communicate and 
cooperate with each other to monitor the environment.
Typically, an embedded sensor network is controlled 
by its own applications that can access the sensor 
nodes within the network. On the other hand, the 
sensor nodes cannot be easily accessed by 
applications outside of the network. Moreover, even 
within the same network, different applications might 
encounter a race condition when they are trying to 
access a sensor node simultaneously. The issue is 
related to system management. However, not much 
research has been done with a focus on the management 
of sensor nodes. 
In the past few years,  Cloud computing has emerged 
as a new computing paradigm to provide reliable 
resources, software, and data on demand. As for 
resources, essentially, Cloud computing services 
provide users with virtual servers. Users can utilize 
virtual servers without concerning about their 
locations and specifications. With such an 
inspiration, this project proposes a system, Sensor 
Agent Cloud, where users can access the sensor nodes 
without worrying about their locations and detailed 
specifications. Sensor Agent Cloud virtualizes a 
physical sensor node as a virtual ＇sensor agent＇. 
Users can use and control sensor agents with standard 
functions. Each sensor agent operates on behalf of 
its user. The mandatory coordination of these sensor 
agents is related to the system management. 
Therefore, Sensor Agent Cloud must be an autonomic 
system that manages itself with minimum human 
interference. In addition, Sensor Agent Cloud 
supports international standard technologies 
regarding programming and agent communication (C and 
IEEE FIPA standard). Thus, it is expected that the 
proposed Sensor Agent Cloud, enabling users to easily 
access various kinds of sensor nodes residing in 
1  Introduction 
An embedded sensor network is a network of embedded computers deployed in 
the physical world that interacts with the environment. Each embedded computer, 
commonly referred to as a sensor node, is a physically small and relatively 
inexpensive computer that has one or more sensors. These sensor nodes are often 
networked, allowing them to communicate and cooperate with each other to monitor 
the environment[1]. Typically, an embedded sensor network is controlled by its own 
applications that can access the sensor nodes within the network. On the other hand, 
the sensor nodes cannot be easily accessed by applications outside of the network. 
Moreover, even within the same network, different applications might encounter a 
race condition when they are trying to access a sensor node simultaneously. Existing 
research works of embedded sensor networks focus on data processing[2], routing[3, 
4], power management[5], clock synchronization[6-8], localization[9, 10], operating 
system[11, 12], and programming[13, 14]. However, not much research has been 
done with a focus on the management of sensor nodes. 
In the past few years, Cloud computing[15] has emerged as a new computing 
paradigm to provide reliable resources, software, and data on demand. As for 
resources, essentially, Cloud computing services provide users with virtual servers. 
Users can utilize virtual servers without concerning about their locations and 
specifications. With such an inspiration, this project proposes a system, Sensor Agent 
Cloud, where users can access the sensor nodes without worrying about their 
locations and detailed specifications. 
Sensor Agent Cloud virtualizes a physical sensor node as a virtual “sensor agent”. 
Users can use and control sensor agents with standard functions. Dynamically 
grouped sensor agents are provisioned in response to the users’ requests. Users can 
destroy their sensor agents when they are not needed. Monitoring sensor agents is 
used to maintain the quality of service. Sensor Agent Cloud also provides a user 
interface for registering / deleting sensor nodes, requesting for provisioning / 
destroying sensor agents, controlling / monitoring sensor agents, and registering / 
deleting users. 
 Each sensor agent operates on behalf of its user. The mandatory coordination of 
 Figure 3: System architecture of Sensor Agent Cloud. 
(1) Portal server: When a user logs into the portal from a Web browser, the user’s role 
- sensor node owner, Sensor Agent Cloud administrator, or end-user, determines the 
operations available to the user. For an end-user, the portal server shows menus for 
logging in, logging out, requesting for provisioning or destroying sensor agent groups, 
monitoring sensor agents, controlling sensor agents, and creating templates for sensor 
agent groups. For a sensor node owner, the portal server gives menus for logging in, 
logging out, registering sensor nodes, and deleting sensor nodes. For a Sensor Agent 
Cloud administrator, the portal server gives menus for creating, modifying, and 
deleting templates for sensor agents and sensor agent groups. Other menus shown to a 
Sensor Agent Cloud are used to register or delete virtual servers, to manage end-users 
and sensor node owners, and to check the status of virtual servers. All of the menus 
for end-users and sensor node owners are available to a Sensor Agent Cloud 
administrator. 
(2) Autonomic Manager: The autonomic manager provisions sensor agent groups for 
requests from the portal server. It contains a workflow engine and predefined 
workflows. It executes the workflows in a proper order. First, it checks and reserves 
the computing resource pool when it receives a request for provisioning. It retrieves 
the templates for sensor agents and sensor agent groups, and then provisions the 
requested sensor agent groups including sensor agents on the existing or a new virtual 
server. It also provides virtual servers with monitoring agents. After provisioning, the 
autonomic manager updates the information of the sensor agent groups. 
(3) Sensor Agent Group: A sensor agent group is automatically provisioned on a 
virtual server by the autonomic manager. Each sensor agent group is possessed by an 
end-user and contains one or more sensor agents. End-users can control the sensor 
agents. For instance, they can activate or inactivate their sensor agents, set their 
frequency of data collection, and check their status. Sensor agent groups can be 
controlled directly or via a Web browser. 
A mobile agent has an XML structure where a C/C++ sensor agent code is 
embedded[25]. As shown in Figure 4, after a mobile agent is created at a client agency, 
it will migrate to a specified sensor node agency. When a mobile agent arrives at a 
sensor node agency, an Agent Execution Engine (AEE) will be automatically created 
to run the sensor agent code. Once the execution of the sensor agent code is finished, 
the result will be carried by the original mobile agent back to its home client agency. 
Similarly, when such a mobile agent arrives at its home client agency, an AEE will be 
created to run the sensor agent code, which typically displays the result required by a 
user. Ch[26], a C/C++ interpreter, is the AEE of Mobile-C. 
With regard to hardware, the prototype sensor node consists of a VIA ARTiGo 
A1100[27] and a FLAG Programmable System on Chip (PSoC) development 
system[28], as shown in Figures 5 and 6. The VIA ARTiGO A1100 has a size of 14.6 
cm x 9.9 cm x 5.2 cm, and has key features including a 1.2GHz VIA Nano processor, 
hardware accelerated video decoding, multiple I/O ports, and Gigabit Ethernet and 
wireless networking. The FLAG PSoC development system is composed of a PSoC 
development board (FLAG-1605A) with a Cypress programmable chip 
(CY8C29466-24PXI), a TCP/IP network module (FLAG-N001), an LCD module, and 
a temperature and humidity sensor module (FLAG-1613A). 
  
Figure 5: VIA ARTiGO A1100 computer.          Figure 6: FLAG PSoC development system. 
5  Dynamic Sensor Node Data Retrieval through Mobile Agent Technology 
 This section uses an application of dynamic sensor node data retrieval to validate 
the fundamental building block of Sensor Agent Cloud, the sensor node, in terms of 
its ability to execute sensor agent codes to obtain user-desired information. 
 Figure 7 shows the diagram for the dynamic sensor node data retrieval 
application. The communication between the PSoC development board and TCP/IP 
Start
Reads the temperature 
and humidity data
Displays the data on 
the LCD module
Waits for 3 seconds
Sends the data through 
the network module
 
Figure 8: Flowchart of the sensor data 
transmitter program. 
Start
Receives the 
temperature and 
humidity data
Saves the data in the 
database
 
Figure 9: Simplified flowchart of the sensor data 
receiver program. 
start
Obtains the 
temperature and 
humidity data from the 
database
stop
Calculates the average 
for the temperature and 
humidity
 
Figure 10: Simplified flowchart of the 1
st
 sensor 
agent code carried by the mobile agent. 
 
start
stop
Displays the average 
for the temperature and 
humidity
 
Figure 11: Simplified flowchart of the 2
nd
 sensor 
agent code carried by the mobile agent.
 
Figure 12: Screenshot of the sensor node agency, server.exe. 
 
Figure 13: Screenshot of the client agency, client.exe. 
[7] C. Lenzen, P. Sommer, and R. Wattenhofer, "Optimal Clock Synchronization in 
Networks," in The 7th ACM Conference on Embedded Networked Sensor Systems, 
2009. 
[8] A. Rowe, V. Gupta, and R. Rajkumar, "Low-Power Clock Synchronization using 
Electromagnetic Energy Radiating from AC Power Lines," in The 7th ACM 
Conference on Embedded Networked Sensor Systems, 2009. 
[9] K. Matsumoto, R. Katsuma, N. Shibata, K. Yasumoto, and M. Ito, "Extended Abstract: 
Minimizing Localization Cost with Mobile Anchor in Underwater Sensor Networks," 
in The Fourth ACM International Workshop on UnderWater Networks, 2009. 
[10] Z. Zhong and T. He, "Achieving Range-Free Localization Beyond Connectivity," in 
The 7th ACM Conference on Embedded Networked Sensor Systems, 2009. 
[11] J. Hill, R. Szewczyk, A. Woo, S. Hollar, D. Culler, and K. Pister, "System 
Architecture Directions for Networked Sensors," in International Conference on 
Architectural Support for Programming Languages and Operating Systems, 2000. 
[12] K. Klues, C. Liang, J. Paek, R. Musaloiu-E, P. Levis, A. Terzis, and R. Govindan, 
"TOSThreads: Thread-Safe and Non-Invasive Preemption in TinyOS," in The 7th 
ACM Conference on Embedded Networked Sensor Systems, 2009. 
[13] J. S. Miller, P. Dinda, and R. Dick, "Evaluating a BASIC Approach to Sensor 
Network Node Programming," in The 7th ACM Conference on Embedded Networked 
Sensor Systems, 2009. 
[14] T. I. Sookoor, T. W. Hnat, P. Hooimeijer, W. Weimer, and K. Whitehouse, 
"Macrodebugging: Global Views of Distributed Program Execution," in The 7th ACM 
Conference on Embedded Networked Sensor Systems, 2009. 
[15] A. Weiss, "Computing in the Clouds," netWorker, vol. 11, pp. 16-25, 2007. 
[16] IBM, "IBM Autonomic Computing White Paper - An Architectural Blueprint for 
Autonomic Computing," 2005. 
[17] J. O. Kephart and D. M. Chess, "The vision of autonomic computing," IEEE 
Computer, vol. 36, pp. 41-50, 2003. 
[18] "OGC: Open Geospatial Consortium," http://www.opengeospatial.org/. 
[19] "SensorML: Sensor Modeling Language," http://vast.uah.edu/SensorML/. 
[20] M. Gaynor, M. Welsh, S. Moulton, A. Rowan, E. LaCombe, and J. Wynne, 
"Integrating Wireless Sensor Networks with the Grid," IEEE Internet Computing, 
2004. 
[21] J. Shneidman, P. Pietzuch, J. Ledlie, M. Roussopoulos, M. Seltzer, and M. Welsh, 
"Hourglass: An Infrastructure for Connecting Sensor Networks and Applications," 
Harvard University, 2004. 
[22] S. Guo, Z. Zhong, and T. He, "FIND: Faulty Node Detection for Wireless Sensor 
Networks," in Proceedings in the 7th ACM Conference on Embedded Networked 
 1 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                     日期：101年 07月 16日 
                                 
 
 
 
 
 
 
 
 
 
計畫編號 NSC 100－2221－E－033－080－ 
計畫名稱 
Sensor Agent Cloud: 一個以虛擬感測器代理人管理實體感測器節點的基於雲端概
念之自主系統 
出國人員
姓名 
周佑誠 
服務機構
及職稱 
中原大學機械系 助理教授 
會議時間 
101 年 7 月 8 日至
101 年 7 月 10 日 會議地點 
Suzhou Conference Center 
100 Daoqian Road 
Suzhou, 215002, China 
會議名稱 
(中文)第八屆 IEEE/ASME 機電及嵌入式系統與應用國際研討會 
(英文)8th IEEE/ASME International Conference on Mechatronics and Embedded 
Systems and Applications 
發表論文
題目 
(中文)基於單眼視覺粒子濾波之移動機器人目標追蹤 
(英文) Mono Vision Particle Filter based Object Tracking with a Mobile Robot 
 3 
Overall, I gained sufficient knowledge and inspirations that can be beneficial to my on-going and future 
research projects. 
三、考察參觀活動(無是項活動者略) 
None 
四、建議 
I think it would be a good idea to host the IEEE/ASME MESA international conference in Chung Yuan 
Christian University in the future. By doing that, international experts and researchers would have a chance to 
know (or know more about) Chung Yuan Christian University and Taiwan, More importantly, domestic 
researchers would have a great opportunity to interact with researchers from the world and gain valuable 
experiences and ideas from them. 
五、攜回資料名稱及內容 
(1) A compact disc: that contains all the papers published in the 8th IEEE/ASME International Conference 
on Mechatronics and Embedded Systems and Applications. 
(2) A conference program pamphlet: that contains all the information regarding the 8th IEEE/ASME 
International Conference on Mechatronics and Embedded Systems and Applications. 
六、其他 
None 
Yu-Cheng Chou* Bo-Shiun Huang 
Department of Mechanical Engineering, Department of Mechanical Engineering, 
Chung Yuan Chirsitan University Chung Yuan Chirsitan University 
Zhongli, Taiwan Zhongli, Taiwan  
e-mail: ycchou@cycu.edu.tw e-mail: mbjgkl@hotmail.com 
 
Abstract—This paper proposes a new mono vision, particle 
filter based object tracking method that predicts a moving 
object’s position within an image, and computes the 
corresponding real-world position relative to a mobile robot. 
Accordingly, the mobile robot moves toward the moving object to 
keep it at the central field of view. Experimental results show that 
the proposed method allows our mobile robot to track a target 
moving at a moderate speed.  
Keywords- object tracking; image processing; mono vision 
based object tracking; single image based depth detection; particle 
filter; particle filter based object tracking; mobot robot. 
 
I. INTRODUCTION 
Object tracking problems can be modeled by dynamic 
systems. For linear Gaussian problems, the Kalman filter [1] 
can be applied to obtain optimal solutions. However, in 
practice, non-linearity and non-Gaussianity usually exist in 
target tracking problems. The sequential Monte Carlo method, 
also known as the particle filter [2], is able to handle cases 
where the dynamic and observation models are non-linear 
and/or non-Gaussian. In recent years, particle filter based 
object trackers have proven to be effective for visual object 
tracking. In addition, a variety of research projects have also 
been reported concerning the theoretical and practical aspects 
of the particle filter approach [3, 4].  
Conceptually, a particle filter based tracker maintains a 
probability distribution over the state of the object being 
tracked. Particle filters represent this distribution as a set of 
weighted samples, or particles. Each particle represents a 
possible instantiation of the state of the object. In other words, 
each particle is a guess representing one possible location of 
the object being tracked. A portion of particles contain larger 
weights at locations where the object is more likely to be. This 
weighted distribution is propagated through time using a set of 
equations known as the Bayesian filtering equations. The 
trajectory of the tracked object can be determined by taking the 
particle with the highest weight at each time step. 
Computing an object’s distance through image processing 
techniques is an important research area in the field of 
computer vision [5] and robot navigation [6]. Image based 
distance computation techniques can be classified into stereo-
vision and mono-vision based methods. A stereo-vision method 
uses two well-positioned cameras to take two images at the 
same time and obtains the depth map using a complex method 
[7]. Although more accurate, such a method is time consuming 
because multiple images need to be processed, and is costly 
since two cameras are required. 
Mono-vision based methods are comparatively cost-
effective because they require only one camera. An object’s 
depth is computed based on the amount of image resizing 
proportional to the camera movement [8]. This technique 
requires the object size and camera parameters. In addition, this 
technique requires multiple images taken by moving the 
camera, thereby making it computationally expensive. The 
mono-vision based method proposed in [9] includes two steps: 
(a) obtaining an interpolation function based on the height and 
horizontal angle of the camera; (b) using the obtained 
interpolation function to calculate an object’s distance from the 
camera. This method’s limitation is its dependency on the 
camera’s height and horizontal angle. A method uses image 
sequences to find an object’s distance [10], thereby making it 
time-consuming and limiting its applicability in real-time 
situations. A camera along with other auxiliary devices, such as 
lasers and LEDs, are used to find an object’s distance [11]. A 
single image-based method that computes an object’s distance 
with high accuracy was proposed [12]. This method is simple, 
accurate, requires only a single camera, and has a low 
computational cost. It is limited only a few basic shapes 
including rectangular, cylindrical, triangular, and circular 
objects. 
This paper proposes a new object tracking method that 
integrates the particle filter technique and the mono-vision 
based approach. The proposed method predicts a moving 
target’s position within an image, and calculates the “virtual” 
position’s corresponding real-world position relative to a 
mobile robot. According to the target’s relative position, the 
mobile robot moves towards the target to keep it at the 
camera’s central field of view. Compared to the algorithm in 
[12], the proposed method allows for tracking a non-regular 
shaped object, such as the mobile robot in this paper. The rest 
of the paper is organized as follows: the proposed object 
Mono Vision Particle Filter based Object 
Tracking with a Mobile Robot 
This research is supported by the National Science Council, Taiwan, under 
grant NSC 100-2221-E-033-080. 
100年度專題研究計畫研究成果彙整表 
計畫主持人：周佑誠 計畫編號：100-2221-E-033-080- 
計畫名稱：Sensor Agent Cloud: 一個以虛擬感測器代理人管理實體感測器節點的基於雲端概念之自
主系統 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 1 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 3 100% 
篇 
 
論文著作 
專書 1 1 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100字為限） 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500字為限） 
在一個嵌入式感測器網路當中，通常具有為數眾多且散布於各處的感測器節點正與其所處
的環境相互溝通。一般而言，感測器節點是一種小體積、便宜、並具有一個或多個感測器
的電腦。這些感測器節點通常藉由網路來達到相互溝通、合作、並監測其所處的外在環境。
近年來，無論是在學術界或工業界，嵌入式感測器網路都已獲得越來越廣泛的關注，因為
嵌入式感測器網路在工業自動化，資產管理，環境監測，交通運輸業務，以及醫療保健等
眾多領域當中，都具有成為一種嶄新且實際之解決方案的潛力。 
一般而言，嵌入式感測器網路只受控於在該網路內所運行的應用程式。這些應用程式可以
訪問該網路中的任何感測器節點。另一方面，不在該網路中所運行的應用程式無法輕易地
訪問該網路內的任何感測器節點。此外，即使在同一網路中，當不同的應用程式試圖同時
訪問同一個感測器節點時，可能會發生彼此爭相搶用相同資源的不協調狀態。這些問題均
與系統管理相關。然而，過去並無太多研究是著重於嵌入式感測器網路之管理。 
在過去的一兩年裡，雲端運算儼然已成為一個嶄新的計算模式，用以提供用戶即時且可靠
的資源、軟體與數據。針對資源方面，從本質上而言，雲端運算為用戶提供虛擬伺服器。
用戶可以在無需知道實體伺服器的位置和規格的情形下，透過虛擬伺服器來利用實體伺服
器的資源。藉由來自雲端運算的啟發，在這個研究計畫中，我們提出了一個名叫 Sensor 
Agent Cloud 的系統。藉由此系統，用戶可以訪問實體感測器節點，而無須擔心它們的位
置和詳細規格。Sensor Agent Cloud將每一個實體感測器節點虛擬為一個Sensor Agent (感
測器代理人)。用戶可以藉由標準函數來使用並控制 Sensor Agent。每個 Sensor Agent
