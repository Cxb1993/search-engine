instead of obtaining a global optimization. 
This research plan will apply Bees Algorithm (BA) to 
train agents in a game. BA is a new optimization 
algorithm, with the advantage of easy convergence, 
communication among particles, and easy to find 
global optimization. The source code of Quake III 
will be modified as a platform of this research. 
Multi-Objective Bees Algorithm (MOBA) will be 
implemented in the research at the last stage, since 
the complex gaming environment is too difficult to 
solve with a single-objective algorithm. Using a 
multi-objective algorithm to find an optimal solution 
is suitable for gaming environments such as first-
person shooters. 
The character training methods proposed by this plan 
would rapidly and accurately determine the best 
available action, increasing the capabilities of the 
trained characters. Multi-objective algorithms would 
assist AI designers in dealing with two or more 
conflicting objectives and shorten the amount of time 
required to find the best solution in designing an 
agent in a game. 
英文關鍵詞： Artificial Intelligence、Bees Algorithm、Multi-
Objective Optimization、Particle Swarm Optimization、
Interactive Evolution 
 
行政院國家科學委員會專題研究計畫成果報告 
利用多目標蜜蜂演算法於遊戲戰略角色演練 
Training a game role based on a multi-objective bees algorithm 
計畫編號：NSC 100-2221-E-029-030- 
執行期限：100 年 8 月 1 日至 101 年 7 月 31 日 
                          主持人：蔡清欉      東海大學資訊工程學系 
                      共同主持人：廖啟賢      東海大學資訊工程學系 
 
一、中文摘要 
近幾年來最佳化演算法在遊戲人工智慧
(Artificial Intelligence,AI) 產業領域的應
用與探討，已經是國內、外研究所關注的重要議
題。不少常用的多粒子型最佳化演算法如基因演
算法（Genetic Algorithm,GA）、粒子群最佳化演
算法(Particle Swarm Optimization, PSO)等已
被廣泛的應用，但由於遊戲 AI設計太過死板、缺
乏 智 慧 ， 導 致 電 腦 所 控 制 角 色 (Non-Play 
Character，NPC)無法滿足玩家挑戰的慾望，造成
遊戲不耐玩，玩家就會流失。顯最佳化演算法在
遊戲 AI 之中對電腦遊戲角色進行訓練與學習策
略的重要性。 
常用的最佳化演算法於改良遊戲 AI 的效能
的缺點讓不少遊戲開發者所垢病，例如基因演算
法的計算量大、編碼困難，粒子群最佳化演算法
(PSO)易收斂速度過快常陷入局部最佳解而非全
域最佳解的問題。本研究提出以蜜蜂最佳化演算
法(Bees Algorithm，BA)演算法收斂速度快、不
易只有局部最佳解的優點，並經由基於遊戲平台
＜雷神之鎚三：競技場＞來實作演算法之間對電
腦遊戲角色進行訓練與比較。由於單目標的最佳
解已不能滿足遊戲的多樣性及玩家多變的玩法，
所以本研究也應用多目標蜜蜂最佳化演算法
(Multi-Objective Bees Algorithm，MOBA)對於
在單人遊戲角色 AI 與團隊策略遊戲 AI，藉由多
目標最佳化探討遊戲對於多個目標函數來判斷問
題解的優劣，更有助提升遊戲裡 NPC 的平衡性及
面對不同環境、人物的穩定性。 
藉由本研究所提出的學習方法來訓練角色行
為策略，相較於其他最佳化演算法更能快速與確
實找尋到更佳的行為參數組合以提升所訓練的電
腦遊戲角色能力，並藉由多目標最佳化的探討幫
助遊戲 AI 設計師對於相互衝突的兩個以上遊戲
目標函數縮短找尋最佳解集合，進而增加遊戲者
開發效率。 
 
關鍵詞：遊戲人工智慧、蜜蜂演算法、粒子群最
佳化、互動式演化、多目標最佳化。 
 
Abstract 
Gamers enjoy interacting with the characters 
within a game, and optimization algorithms such as 
Genetic Algorithm (GA), Particle Swarm 
Optimization (PSO), etc., are applied in gaming to 
increase the intelligence of a character and variety 
of a game. However, more effort would be required 
to design a game character by utilizing an 
optimization algorithm. For example, the calculation 
of GA is heavy, and it is usually difficult to encode; 
for PSO, it is easy to find a local optimal solution 
instead of obtaining a global optimization. 
This research plan will apply Bees Algorithm 
(BA) to train agents in a game. BA is a new 
optimization algorithm, with the advantage of easy 
convergence, communication among particles, and 
easy to find global optimization. The source code of 
Quake III will be modified as a platform of this 
research. Multi-Objective Bees Algorithm (MOBA) 
will be implemented in the research at the last stage, 
since the complex gaming environment is too 
difficult to solve with a single-objective algorithm. 
Using a multi-objective algorithm to find an optimal 
solution is suitable for gaming environments such as 
【表 1】為近年來學者針對此議題提出不少
關於演算法效能比較的論文，藉由各演算法於不
同平台或實驗上相較其效能，進而讓更多人了解
到針對什麼問題適合用何種演算法來解決問題。 
 
表1  最佳化演算法優缺點比較圖 
 粒子群演算
(PSO) 
基因演算
法(GA) 
蜜蜂演算
(BEES) 
特點 (1)收斂速度快 (1)搜尋空
間大較趨
近整體最
佳解 
(1) 收斂
穩定快速 
(2)適合多變數
複雜問題 
(2)可以處
理的資
料型範
圍極大 
(2) 沒陷
入局部
解問題
的優點 
(3)運算成本低  (3) 蜜蜂
之間會
相 
互溝通
訊息 
缺點 (1) 粒子之間
本身並不
會 
相互溝通訊
息 
 
(1)運算成
本高 
(1) 參數
不是固定 
(2) 收斂過快
而陷入局部
最佳解的情
形 
(2)收斂速
度較慢 
 
 (3)有些問
題有編
碼上的
困難 
 
 (4)它的解
不保證
最適化 
 
 
在雷神之鎚 3（Quake III：Arena）這款遊
戲之中提供每個 BOT 九種不同的行為模式: 
＊跳躍（Jump）：BOT 的基本行為，藉由跳躍來攻
擊或是躲避對手攻擊。 
＊蹲伏（Crouch）：BOT 的基本行為，藉由蹲下來
縮小被攻擊範圍與閃躲敵方或攻擊對方。 
＊警覺（Alerthness）：在一定的範圍區域內來回
巡視，巡視是否有敵方的 NPC。 
＊躲藏（Camp）：藉由躲藏於敵人不易發現的地
形，進行偷襲對手，戰術上常稱守點或稱龜點。 
＊侵略（Aggression）：當敵方之間相互攻擊時，
BOT 本身會積極的向前攻擊敵方，達到更高殺敵
效果。 
＊復仇心（Vengeful）：當被同一敵方擊殺過多
時，下一次重生便會以此目標做為第一攻擊目標。 
＊武器切換（Weapon Jump）：擁有不同武器時，
會針對對手遠近與環境使用適合的武器種類來攻
擊對方。 
＊自我保護（Self preservation）：當血量低於
某個狀態時便會以存活為第一目標，找尋可以補
血或加護盾的東西。 
＊擊火目標（Frag easy target）：選擇對本身有
利的攻擊，當多數 NPC 同時攻擊弱勢一方，便會
加入進行先清除弱勢的一方，以達到有利的攻勢。 
 由上述行為組合成角色 BOT 在遊戲中的
行為模式，並藉由其不同的行為組合出多樣性的
行為策略，提升遊戲之間精彩性與耐玩度。 
 
3.1  蜜蜂最佳化演算法 
 蜜蜂演算法主要是摹仿蜜蜂行為群體智慧的
一種演算法，是一種新型態的智能最佳化演算
法。演算法就像蜜蜂在外採蜜時，當發現花蜜時
便會通知鄰近的蜜蜂，花蜜越多的花叢便聚集的
越多蜜蜂，反之則越少。由此聚集地往鄰近的其
他花叢再做搜尋的動作，彼此間靠著訊息的傳遞
而來決定移動的方向與距離。藉著不斷的向該區
域花叢作搜尋花蜜的動作，進而最終找出花蜜最
多的花叢，這也是蜜蜂演算法搜尋主軸，通過各
蜜蜂各體的局部搜尋行為，有著較快的收斂速度。 
蜜蜂演算法跟一般既定的最佳化演算法不太
一樣，沒有所謂標準的數學公式與未固定的參數
配置，主要經由演算法搜尋過程加上參數間的配
置藉由各蜜蜂區域間與區域間的搜尋，【圖 1】顯
示搜尋的過程與參數配置。 
 
 圖 3  一個蜜蜂代表一個 BOT 之行為 
 
圖 4  BOT 的搜尋的流程圖 
 
3.2  多目標蜜蜂最佳演算法 
3.1 所提到的都是針對單目標最佳化的問
題，但再遊戲裡很多情況並不是單一目標即可解
決，如對戰射擊遊戲中「殺人」與「被殺」就是
一個最基本的例子，如何兼具強大的殺敵技巧能
力又不失適時的閃躲與退後降低被殺，而不是一
股腦向前殺敵，是我們所研究的重點。再多個目
標函數考慮之下，我們使用多目標最佳化來找出
彼此衝突的目標函數中一組最佳解集合供決策者
挑選。其核心思想是尋找滿意解,而不是最優解。
正因為有這樣的特性，在多目標最佳化問題中，
我們所要找的最佳解並不是所有子目標的最佳
解，而是所謂「Preto 最佳（Pareto-optimal 
solution ）」， 由 一 群 未 被 支 配 解 集 合
(non-dominated)所構成，而「未被支配」換句話
說這些解必能支配此可行解空間的其他解。 
根據 Deb(2001)的定義，「一個解 X1 若支配
另一個解 X2」，定義如下： 
(1) X1 在所有目標下皆不比 X2 差。 
(2) X1 至少在一個目標下優於 X2。 
 
以一個兩個衝突目標的多目標最佳化問題為
例，如【圖 5】，針對五個可行解中找出最佳解集
合， af 為極小化目標函數， bf 為極大化目標函數。 
 
圖 5  兩衝突多目標可行解散布圖 
 
如【圖 5】，針對五個可行解中找出最佳解集
合， af 為極小化目標函數， bf 為極大化目標函
數。由圖可知，假如以點 1跟點 5做比較，在 bf
之中點 1值大於點 5，而在 af 之中點 1比點 5大，
如此滿足上敘的兩點定義，可以說「點 1 為點 5
的支配解」，故點 1可以取代點 5。再以點 2與點
3 相作比較，在 bf 之中點 2 大於點 3，但在 af 之
中點 2 小於點 3，彼此間並未支配誰，這種情況
視為「點 3與 點 2 互為非被支配解」。 
經由【圖 6】我們可以找到支配解與非支配
解的所有解集合，根據定義的兩個條件沒有同時
符合的解，歸類為非被支配解，而這些非支配解
之 集 合 便 定 義 為 柏 拉 圖 最 佳 解 集 合
(Pareto-optimal set)。 
圖 6  兩衝突多目標函數的 Pareto-optimal 
front 
 
如【圖 6】所示，藉由多目標最佳化所求，
□程式版本：Virtual basic 6.0 
□開放程式碼：Open source version 1.32 
□實驗地圖：Q3DM13、Q3DM15、Q3DM18 
□搜尋範圍 e：2 
□搜尋範圍 m：4 
□派遣到搜尋範圍 e的數量(nep) ：3 
□派遣到搜尋範圍 m的數量(nsp) ：2 
□初始搜尋半徑(ngh)：0.25 
 
 首先針對蜜蜂演算法與粒子群最佳化演算法
所訓練的 BOT 兩者在 Quake III 平台分別與平台
本身的 BOT 進行對戰，並針對權重比為(2,1)、
(1,2)、(3,1)來比較之間權重的影響，藉由所求
得適應值高低來演化角色行為 AI，且在三個不同
環境的地圖中分別執行，並經由實作實驗來比較
其結果。蜜蜂演算法初始的參數是可任意制定
的，參數 n在此我們設為 8，參數 m設為 4、參數
e 設為 2、參數 nep (n2)設為 3、參數 nsp (n1)
設為 2、參數 ngh 設為 0.25。 
 
首先為在簡單地圖的部份，以兩演算法使用
權重比為(2,1)分別在三個地圖所訓練 20 場取得
最佳的適應值，找到最佳的策略組合，再分別進
行實戰 20 場相比較，【圖 10】為綠色線為當代
疊代數所求得到的平均適應值，紅色線為此疊代
目前的最佳值。 
 
圖 10  PSO 訓練 20 場的適應函數值 
 
藉由 PSO 演算法訓練我們的 BOT（Harley）
在找到局部最佳解後容易陷入局部解，不易跳脫
進而找尋到全域最佳解，此訓練得到最佳值為
0.48 之行為參數。【圖 11】為蜜蜂演算法訓練 20
場的適應函數值，由蜜蜂演算法不只取得更佳全
域最佳解為 0.58 之行為參數，更快收斂更有效避
免陷入局部最佳解。 
 
圖 11  BA 訓練 20 場的適應函數值 
 
【圖 12】 (a)與(b)為 BOT (Harley)(紅色線段
為我們 BOT)以 BA 演算法藉由訓練 20 場取得最
佳的適應值的策略組合分別對其他 5 個 BOT 實戰
20 場的對戰結果，一回合結束設定為一方擊殺數
達到 15 人為條件。 
BA 20場
0
2
4
6
8
10
12
14
16
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
第幾場
殺
敵
數
Harley
Anarki
Angel
Biker
Bitter
Bones
 
     (a) 
BA 20場
0
2
4
6
8
10
12
14
16
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
第幾場
被
殺
數
Anarki
Angel
Biker
Bitter
Bones
Harley
 
     (b) 
圖12  BA實際對戰20場的殺敵數成積(a)與被殺
數成積(b) 
【圖 13】 (a)與(b)為 BOT (Harley)(紅色線
段)以 PSO 藉由訓練 20場取得最佳的適應值的策
略組合分別對其他 5個 BOT 實戰 20 場的對戰結
果。 
善解決 PSO 最佳化演算法易陷入局部最佳解的缺
點，實驗結果所示不僅收斂快速且沒局部解的困
擾，進而求取全域最佳解。本研究提出以多目標
蜜蜂演算法對於 FPS 射擊遊戲之間「殺人」與「被
殺」兩種遊戲本身最原始的衝突函數，經由我們
的多目標蜜蜂演算法得到最佳的行為參數策略最
佳解集合，進而對於不同環境與策略，提供更適
合的行為模式，藉由此論文研究，讓遊戲人工智
慧開發工程師能夠更方便的創造智慧、多樣、擬
真與具有學習能力的角色行為。對於遊戲開發者
縮短遊戲中找尋衝突的多目標函數理想解的時
間，給予更快速、效率的開發過程與價值。 
 
六、參考文獻 
[1]. C.Browne and F.Maire, Member, IEEE, 
“Evolutionary Game Design,” IEEE 
transactions on computaional intelligence and 
AI in games, pp.1 – 16, 2010. 
[2]. M.Preuss, Member, IEEE, Nicola Beume, 
Holger Danielsiek, Tobias Hein, Boris 
Naujoks,Nico Piatkowski, Raphael Stüer, 
Andreas Thom, and Simon Wessing, 
“Towards Intelligent Team Composition and 
Maneuvering in Real-Time Strategy Games,” 
IEEE transactions on computaional 
intelligence and AI in games, pp.82 – 
98,2010. 
[3]. H.Wang, Y.Gao, and X.Chen, “RL-DOT: A 
Reinforcement Learning NPC Team for 
Playing Domination Games,” IEEE 
transactions on computaional intelligence and 
AI in games, pp.17 – 26,2010. 
[4]. S. Woodcock, The Game AI Page Building 
Artificial Intelligence into Games, 
http://www.gameai.com 
[5]. J. Funge, X. Tu, and D. Terzopoulos, 
“Cognitive Modeling: Knowledge, Reasoning 
and Planning for Intelligent Characters,” Proc. 
ACM SIGGRAPH’95, pp 47-54, 1995. 
[6]. S. Rabin, “AI Game Programming Wisdom: 
Preface,” AI Game Programming Wisdom, 
Charles River Media Press, pp xi-xiv, 2002. 
[7]. D. T. Pham, A. Ghanbarzadeh, E. Koc, S. Otri, 
S. Rahim and M. Zaidi, “The Bees 
Algorithm,” Technical Note, Manufacturing 
Engineering Centre, Cardiff University, UK, 
2005. 
[8]. J. Mao, K. Hirasawa, J. Hu & J. Murata, 
“Genetic symbiosis algorithm for 
multiobjective optimization 
problem,”Transactions of the Society of 
Instrument and Control Engineering, 37(9), pp. 
894-901. 
[9]. D. T. Pham, Ghanbarzadeh A. , 
“Multi-Objective Optimisation using the Bees 
Algorithm,” in 3rd International Virtual 
Conference on Intelligent Production 
Machines and Systems (IPROMS 2007): 
Whittles, Dunbeath, Scotland, 2007. 
[10]. J.E. Laird, “It knows what you’re going to do: 
adding anticipation to a 
quakeBot,”Proceedings of the Fifth 
International Conference on Autonomous A 
gents, ACM Press ,Montreal,Canad, pp 
385-392, 2001. 
[11]. J.Poropudas and K.Virtanen, "Game-Theoretic 
Validation and Analysis of Air Combat 
Simulation Models",IEEE transactions on 
systems, Man, And cybernetics —part A: 
systems and humans, pp.1057 – 1070, 2010. 
[12]. C.Chambers, W.C. Feng, Member, IEEE, 
Sambit Sahu, Member, IEEE, Debanjan Saha, 
Member, IEEE, and David Brandt, 
"Characterizing Online Games", IEEE/ACM 
transactions on networking, pp.899 – 
910,2010. 
[13]. J.F. Weng, S.S. Tseng, Member, IEEE, and 
Tsung-Ju Lee, "Teaching Boolean Logic 
through Game Rule Tuning",IEEE 
transactions on learning technologies, 
pp.319 – 328,2010. 
[14]. R. C. Eberhart and J. Kennedy, “A New 
Optimizer Using Particle Swarm Theory,” 
proc. Sixth International Symposium on Micro 
Machine and Human Science, Nagoya, pp. 
39-43,1995. 
[15]. 黃奐禎, 蔡清欉, 廖啟賢, 廖元勳, 利用
粒子群演算法在電腦遊戲訓練團隊策略, 
銘傳大學2007國際學術研討會, March. 
2007. 
[16]. 郝國平、蔡清欉、廖啟賢, “訓練電腦遊戲
團隊的策略,” 2010 資訊與管理應用研討
會, March 2010. 
[17]. 陳柏仲,蔡清欉,廖啟賢,林灶生, “以粒子
群最佳化演算法為基礎之第一人稱射擊遊
戲角色設計 , ” 智慧生活科技研討
會,2006. 
100年度專題研究計畫研究成果彙整表 
計畫主持人：蔡清欉 計畫編號：100-2221-E-029-030- 
計畫名稱：利用多目標蜜蜂演算法於遊戲戰略角色演練 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 1 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
