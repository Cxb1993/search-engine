(MARS) and support vector machine (SVM), the proposed 
model has a 92.9% accuracy rate in predicting 
customer types, is less impacted by prior 
probabilities, and has a significantly low Type I 
errors in comparison with the other five approaches. 
英文關鍵詞： behavior scoring, credit scoring, Bayesian, latent 
variable model, classification and regression tree 
 
2行政院國家科學委員會專題研究計畫成果報告
信用卡持有人顧客品質估計暨刷卡行為異質性分析
Hierarchical Bayesian Approach and CART
Algorithm in Measuring Credit Card holder's
Customer Quality
計 畫 編 號：NSC 99-2221-E-027 -024 -MY2
執 行 期 限：99 年 8 月 1 日至 101 年 7 月 31 日
主 持 人：邱志洲 國立臺北科技大學商業自動化與管理研究所
計畫參與人員：李泓緯、許素娜、李偉銘、顏菁蓉、呂正欽、林于琪、
蔡明松、溫蕙菱 臺北科技大學商管所
摘 要
本研究嘗試藉由應用貝氏統計的潛在變項模式與分類迴歸樹建
構出一套整合模式以解決目前金融機構面對授信業務時的三項難
題：銀行無法明確得知目前的授信決策對未來的風險影響程度；其次
則為當金融機構欲針對申請者的歷史交易和還款情況授予適當的產
品申請時，苦無適當的資訊與易於理解的說法；最後的難題則因人工
授信作業的機制導致授信成本增加且突顯效率不彰的窘境。為佐以實
證研究模型，採用某大銀行的信用卡顧客歷史交易與還款記錄及人口
統計變數等變數於提出的模式中。經由研究證實，藉由持卡人交易行
為評估的顧客品質於建構評分模型時具顯著的價值，並且證實整合模
型的有效性優於鑑別分析與羅吉斯迴歸方法。
關鍵字：行為評分，信用評分，貝氏統計，潛在變項模型，分類迴歸
樹
4便曾應用羅吉斯迴歸及層級貝式的模型，針對貸款額度和相關資訊判
斷貸款人是否會延遲還款的機率予以探討。然而，其研究構面未將貸
款人的還款頻率和金額納入考量，此外還納入Fair–Isaac公司的信用分
數於評分模型中。Min et al. (2008)則利用DEA方法進行個人信用評分
的計算。Huang et al., (2007)等學者則應用支援向量機(Support Vector
Machine, SVM)技術針對個人信用評等進行信用評分模式的建構。雖
然，這些研究可成功地提供不同的信用評分架構，但對金融機構而言
仍舊無法解決「影響顧客品質的因素」與「如何調整授信額度和循環
利率才可確保風險最小化」等問題面臨無明確解釋能力的窘境。
在本研究中，為了解決上述金融產業在執行信用評分下的三項難
題，並明確地了解信用額度與循環利率對持卡人價值的關係，我們發
展出一套整合行為評分(behavior scoring)與信用評分(credit scoring)功
能的模式。在該模式中，我們先針對信用卡持卡人之刷卡付費資料與
利息繳款紀錄進行層級貝氏模行為評分模式之建構，以分析估計出持
卡人的顧客價值異質性；第二階段則根據所預測出的顧客品質，進一
步應用分類迴歸樹(Classification and Regression Tree，CART)的技術
在納入人口統計變數與持卡人活動紀錄資訊的情況下進行信用評分
模型的建立。而藉由研究中所提整合模型的應用，我們發現授信額度
與循環利率對顧客價值具有顯著的影響，而決策樹規則的產生對授信
作業的自動化及準確度皆有實質的助“力”。
為說明所提出模式的有效性，我們針對台灣某銀行所提供的信用
卡持卡人交易資料與繳款記錄進行分析，該資料包含了 2948 位持卡
人於 2005 年 9 月至 2006 年 7 月持卡人連續 11 個月間的交易紀錄、
人口統計資料與金融活動紀錄。根據分析結果顯示，單純應用人口統
計變數來建構顧客行為評分或信用評分模型時容易產生顯著的型 I與
型 II 誤差。因此，對信用評分模式而言，根據持卡人交易資料與還款
記錄所預測出之顧客價值變數，其重要性遠高於人口統計變數；最
後，經由實驗證實所提出的整合模式不僅在辨識顧客好壞的能力可達
6Sahami學者於1998年應用貝氏方法過濾垃圾郵件；Jen等人(2003)更針
對消費者購買頻率的資料，在假設其為possion 與gamma分配的條件
下，建構出可預測消費者價值的層級貝氏模型。
而有關隱藏變數的研究，曾有經濟學領域的學者將其應用在勞動
參與程度及消費者品牌選擇的解釋等問題上 (Greene, 2000)；
Bartholomew 學者於 1998 年針對工作場所勞資關係調查資料，運用
潛在變量分析出 1005 家公司的創新性；Bartholomew 等人(1999)則提
出模擬潛在變量來描述金融機構的顧客價值。
在本研究中，我們將根據信用卡持卡人的還款日距資料及其信用
額度與循環利率進行一層級貝氏行為評分模式的建構。模式中除了將
以估計出的隱藏變數值說明顧客價值外，亦將進一步討論持卡人對信
用額度與循環利率的偏好差異。
2.2 分類迴歸樹
所謂的分類迴歸樹(CART)是 Breiman(1984)所發展出來的一種演
算法，其基本原理乃是使用二元分割過程來分析龐大的資料集，並透
過遞迴(recursive)的程序，依照預測變數與相關劃分條件將 N 個訓練
樣本加以劃分成 M 個已知類別，並產生一組劃分好的規則(rule)，最
後再透過此規則對新樣本進行預測的過程。基本上，CART 在建構模
式時，必須先提供其兩組預先切割好的資料(訓練樣本與測試樣本)，
之後它會針對訓練樣本運用劃分條件建構出具有最多結點數目的決
策樹。在進行決策樹建構的過程中，CART 會根據資料的分散度來決
定分隔變數的先後順序。而所謂的「分散度」是用來評估分隔變數重
要程度的一個衡量指標：當分散度很高時，表示決策樹終結點的資料
組合包含許多個不同類別的資料，而分散度很低時則表示終結點的資
料類別較單純。
近年來，CAR已經被廣泛的應用在不同的領域中。如：醫療診斷、
8不足的情形發生時，其顧客價值相對較低。
根據上述的說明，為能同時觀察刷卡金額與應付利息之繳款情
形，我們將信用卡使用者之繳款能力定義如方程式 (2)。其中，wpij
代表第 i 位顧客於第 j 期的當期本金還款金額；pij表示為第 i 位顧客
在第 j 期的應繳本金總額；Qsij 代表當期繳交利息金額；sij 則為應繳
交利息總額。藉由評估機制，銀行將可了解持卡人在各階段的還款能
力。
 2 1ij ijij
ij ij
wp Qs
x
p s
 
   
           
(2)
在方程式(2)中，等號右邊的第一項 ij
ij
wp
p
代表一個顧客的基本繳款能
力；第二項 ij
ij
Qs
s
則是用以衡量顧客對利息的支付能力；係數( 0 1  )
則是授予兩者權重值。至於兩者間的關係，我們可以利用下面的例子
進行說明。若有一位信用卡持有人，每期皆繳交其刷卡總額
(i.e. ij
ij
wp
=1
p
, ij
ij
Qs
0
s
 , X2ij=0.5)，則這樣的顧客對銀行而言其價值並非最
大。這是因為銀行只能根據該顧客的交易項目酬收簽約商店的部份交
易費。但若有另一位顧客其交易行為動用循環利率時，其除了繳交刷
卡金額外，同時也繳納應付的利息款項(i.e. ij
ij
wp
=1
p
, ij
ij
Qs
1
s
 ,X2ij=1)時，
對銀行而言其價值將會最大。在本研究中，我們將值設為 0.5 的理
由繫基於假設利息繳款能力與本金繳款能力具有相同的重要性。
最後在令 Zi=(zi1,zi2,..ziJ)’， Xi=(xi1,xi2,…,xiJ)’ 的情況下配合下列
各參數的先驗分配的假設:
10
狀結構的工作。CART 依據每個結點的異質性高低來評估的準則，而
在許多的異質性的評估指標中，Gini 是最常用的指標之一。由於每個
結點均可能為末端結點，因此 CART 會進一步去計算每個結點誤判成
本的高低，以作為該結點是否再往下細分的依據。最後當樹的建構程
序完成後，我們將此時的樹狀結構稱之為最大樹。
二、修剪樹狀結構，產生許多子樹群
當最大樹狀結構完成後，CART 便會開始進行樹狀結構的修剪，
以找出最適當大小的決策樹。一般來説，決策樹修剪準則為誤判率
(error rate )或誤判成本(error cost) 。而在整個決策樹修剪的過程中，
CART 會計算最大樹與所有子樹的誤判率，並挑選具有最低誤判成本
或最低誤判率的樹狀結構。通常，在決策樹修剪的過程中所使用的準
則為重代估計值(resubstitution estimate)。而原則上，當重代估計值愈
小時，代表該數狀結構的末端結點個數愈多，亦即其樹狀結構中的切
割點愈多，換言之其樹狀結構也愈大。
三、挑選最佳樹狀結構
待所有可能的樹狀結構被尋找出來後，使用者便可運用測試樣本
來進行交叉之驗證。換言之，即將測試樣本代入所有可能的樹狀結構
中並計算每個結構的誤判率，最後再選取最小誤判率的樹狀結構為最
佳樹。綜合而言，CART 的分割準則是先選擇具備有最大變異下降量
的變數和分割點來進行分割，並使用反應值平均數作為預測值；而
CART 的停止法則是先製造出一棵大樹，再使用交叉確認的方法，由
底層往上修剪，最後再以具有最小交叉確認均方差的樹為最佳的樹狀
結構；至於 CART 的預測法則是使用末端結點內的反應值平均數來作
為預測值。
在本研究中，我們將根據層級貝氏模式的顧客價值預測結果( iˆjz )
12
11 期的交易期間內，有 8 期的信譽良好(yhj=1)則認定該顧客在交易期
間內為好顧客。除持卡人交易的紀錄外，銀行更提供持卡人 7 項人口
統計、個人授信程度資訊與 4 項金融活動的紀錄以建構評分模式。藉
由觀察資料的筆數，考慮資料的分佈性將資料先區分 2248 筆(76.3%)
做為訓練資料及 700 筆(23.7%)測試資料。
4.1 層級貝氏信用評分估計結果
經由整合模型第一階段所提的層級貝氏信用評分模式後，藉由設
定參數的先驗分配值與透過1000次的MCMC技術以模擬與估計參數
後，由圖 1 可觀察出參數值在模擬 500 次後即可達收斂的狀態。因此，
我們針對 500 至 1000 次的參數模擬值進行平均值的運算，以做為第
一階段模式的參數估計值，至於參數估計值則分別整理於表 1 與表 2。
圖 1 整體顧客的係數估計值之收斂圖
經由層級貝氏評分模式所估計出的顧客價值( iz )經由直方圖統計
如圖 2 所示。此外，透過模式中的 0所提供的資訊，我們可將顧客類
別以條件門檻值 0.4606 為識別條件，以評定顧客好壞。換句話說，
當顧客價值低於 0.4606 時其應被判定為壞顧客且在本研究中將顧客
歸屬於群 I，反之則將顧客分至群 II。
14
0 10 20 30 40 50 60
0
500
1,000
1,500
2,000
次
數
Mean = 8.487
Std. Dev. = 17.57513
N = 2,948
Zhat
圖 2 顧客品質分配圖
4.2 分類迴歸樹之變數篩選
藉由 CART 技術的遞迴劃分程序，使得每個末端結點比其父節點
的內的資料集更具均質性，透過劃分規則可判別出重要變數。茲將重
要變數與簡略分類規則整理於表 3 與表 4。藉由觀察，顧客的信用活
動的資訊包含短中期無擔授信次數(JCIC1)、信用卡曾經最長持有月
份數(JCIC2) 、最近 12 個月使用預借現金銀行家數(JCIC3)與最近 1
個月新業務查詢銀行家數(JCIC4)與信用額度(CL)年利率(APR)在建
構信用評分模型時為重要的影響因素。至於人口統計變數僅能提供有
限的資料於信用評分模型。
表 3 CART 分析變數重要性
JCIC3 JCIC4 JCIC1 APR JCIC2 CL
相對重要性(%) 100 82.54 80.41 79.49 57.98 26.94
此外，經由變數篩選後的信用評分模型可藉由 21 條規則，即可
預測顧客價值，其效率較未篩選前 101 條規則顯得簡略且效率較佳。
理論上，決策規則是不易透過羅吉斯迴歸或鑑別分析等分類技術中取
得，根究其原因在於上述技術受限難以尋求判別變數與求算門檻值的
16
10 JCIC3>3.5 , JCIC4>1.5, JCIC1>3.5 , APR>0.135 -0.515 0
4.3 預測顧客品質暨比較
為嚴謹的確認整合模型的有效性，本研究藉由於過去實用於信用
評分模型建構的技術中，具有顯著成效的鑑別分析與羅吉斯迴歸的判
別結果予以比較識別能力。除模型的識別能力外，在此我們也引用學
者Johnson和Wichern(1998)曾提出評量分類模型可從鑑別能力、先驗
機率與誤判成本三項構面予以考量模型的有效性。概括來說，判別模
型的有效性多針對誤判率(type I error 和type II error)的能力加以論
述，但有時卻忽略因資料比例差距而導致建構模型時可能衍生的模型
偏誤。換言之，建構分類模型時除需準確度高的鑑別力外也需考量模
型受先驗機率影響的程度。實務上，更應針對各領域因誤判所面臨的
風險或成本損失的考量角度以斟酌誤判成本對研究目標的影響程
度。簡言之，分類模型需著重於不受先驗機率與考量誤判成本較低的
影響因素下，仍具有顯著預測能力。
在實務上，銀行普遍使用二分法以識別顧客的好壞。在本研究
中，將顧客區為群I(信譽不佳)與群II，至於分類的程序則如下兩個階
段：第一階段，有別過去文獻研究多個人特徵資料以識別顧客類型的
方法，本研究採用透過貝氏方法針對持卡人的顧客歷史的交易行為，
萃取並預測顧客”真正”的顧客品質。至於第二階段則模擬當申請者
對金融機構提出授信申請時，授信單位僅仰賴提供的人口統計資訊與
其金融市場活動的紀錄透過評分模型以預測其顧客品質與類型。茲將
訓練資料經由信用評分模式後識別能力彙整如表5所列。由表5可察
覺，鑑別分析與羅吉斯迴歸各平均辨別率達88.405%與87.335%皆低於
整合模型的平均識別力91.425%。
表 5 各分類技術比較
Actual Discriminant Logistic 整合模型
18
Logistic 24.30% 1.03% 25.32787 8.009884 17.3180
整合模型 12.62% 4.53% 17.14357 6.953771 10.1898
經由表 6 的整理，針對誤差成本實際運算於二種鑑別方法與本研
究模型中可突顯示出整合系統的型 II 誤差雖然無法優於鑑別分析與
羅吉斯迴歸法的辨識力，但是卻可顯著地看出型 I 誤差較其餘兩種分
類技術低，顯著的降低因樣本數而影響到型 I 誤差。總括而言，無論
使用何種誤判成本比例，誤判成本皆低於鑑別分析與羅吉斯，換言
之，整合模型具有較佳的有效性。
4. 結論與建議
近年來，隨著授信決策不當所造成呆帳損失升高的現象，促使金
融機構對信用風險的評估作業越加的重視；尤其在雷曼兄弟破產的事
件發生後，如何有效且正確地評估顧客風險衍然成為各金融機構急於
解決的重要問題之一。根據過去經驗，銀行執行授信業務時常面臨三
項難題：銀行無法明確得知目前的授信決策對未來的風險影響程度為
何；其次則為當金融機構欲針對金融商品申請者的歷史交易資料與還
款情況授予適當的信用額度或循環利率時，常缺乏有效的相關資訊可
供決策；最後的難題則因人工授信作業的機制，衍生出作業成本增加
且突顯效率不彰的情形。
在本研究中，為了解決上述金融產業在執行信用評分下的三項難
題，並有效的了解信用額度與循環利率對持卡人價值的關係，我們發
展出一套整合行為評分(behavior scoring)與信用評分(credit scoring)功
能的模式。在研究中藉由兩階段程序的整合，應用顧客在交易期間內
的顧客品質的以描述持卡人的持卡行為，其有別於過去文獻中經常使
用的分類式的授信決策機制，在本研究中冀以連續型的資料型態以描
述顧客品質，顧客品質不僅可充分地表達持卡人行為，並提供銀行更
20
Breiman, L., Friedman, J.H., Olshen, R.A., Stone, C.J. (1984),
Classification and Regression Trees, Monterey, Ca: Wadsworth.
Breiman, L.(2001), "Random forests", Machine Learning , Vol. 45, No. 1,
pp 5-32.
Burrows, W.R.(1997), “CART Regression Models for Predicting UV 
Radiation at the Ground in the Presence of Cloud and Other
Environmental Factors”, Journal of Applied Meteorology, Vol. 36, pp.
531-544.
Garbe, C., Buttner, P., Bertz, J., Burg, G., d'Hoedt, B., Drepper, H.,
Guggenmoos- Holzmann, I., (...), Stroebel,W. (1995),”Primary cutaneous
melanoma: Identification of prognostic groups and estimation of
individual prognosis for 5093 patients”, Cancer, Vol. 75, No. 10, pp.
2484-2491.
Gelfand, A.E., and Smith, A.F.M. (1990),”Sampling-based approaches to
calculating marginal densities”, J. the American Statistical Association,
Vol. 85, pp.398-409.
Hand, D. J., Crowder, M. J. ( 2005), ”Measuring customer quality in
retail banking”, Statistical Modeling, Vol. 5, No. 2, pp.145-158.
Jen, L., Chou, C.H., Allenby, G.M. (2003), ”A Bayesian Approach to
Customer Scoring in Direct Marketing”, Marketing Letters, Vol. 14 , No.
1, pp.5-20.
Johnson, R. A., Wichern, D. W.(1998), Applied Multivariate Statistical
Analysis, Prentice-Hall Inc.: New York.
Min, J.H., Lee, Y.C.(2008), “A Practical approach to credit scoring”,
Expert Systems with Applications, Vol. 35, No. 4, pp. 1762-1770.
Huang, C.L., Chen, M.C., Wang, C.J.(2007), “Credit scoring with a data
mining approach based on support vector machines”, Expert Systems with
Applications, Vol. 33, No. 4, pp. 847–856.
Rossi, P.E., McCulloch, R.E., Allenby, G.M. (1996), “The value of 
參加 2012年 ICIS第五屆互動科學, IT,人因與數位內容之國際研討會
邱志洲
國際資訊科技學會（Advanced Institute of Convergence Information Technology, AICIT)）與 IEEE 學會韓國分會 於
2012年 6月 26日至 28日於風光美麗的韓國濟州島（Jeju Island, Republic of Korea）第五屆互動科學, IT,人因與數位內容
之國際研討會(5th International Conference on Interaction Sciences: IT, Human and Digital Content)。本人受邀發表論文，深
感榮幸！但本人由於個人健康因素於二月份開刀，因此無法親自參與該研討會。考量再三，最後委託計畫參與之研究生
李偉銘同學前往韓國參加該會議並代替報告論文。以下相關之報告內容乃根據李同學之描述轉寫而成。
而此次研討會參加之成員除了來自美國、加拿大及西太平洋國家地區的專家學者外；亦包含了來自許多亞太地區國
家如澳洲、紐西蘭、日本、韓國、中國大陸與中華民國等之研究者，由於這許多學者的積極參與對促進學術交流，特別
是跨越國界及地區的學術合作有正面的幫助。
而在將近三天的研討會期間，主辦單位共安排了約一百五十篇的論文發表，其內容涵蓋了五大領域 Culture, Human
and Information Technologies, Advances in Information Technology, Convergent Approaches: Future, Sciences and Environment,
Industrial, Service, Management Issues and Applications, Transportation Sciences (Electromobility for Human Development)的
論文。藉著多位學者、專家於論文發表時的意見交流，使得李同學獲得了許多寶貴的經驗，在李同學發表論文場次中的
議程主席（Section Chair）有針對個人之論文提出許多改善之建議及未來進一步深入研究的方向，這對個人論文在將來
發表於一流國際期刊及未來的研究工作上有莫大的助益。此外，主辦單位亦於會場邀請亞洲各大出版商介紹最新的教科
書及數位內容之教學軟體，可供與會者參考並選購。
由於研討會期間適逢各大學學期結束期間，因此台灣各大學院校參與此次研討會之學者亦相當踴躍，此無非也提供
了台灣各校教師們，在學術交流外另一個經驗分享的機會，李同學亦經由與他校老師的交談中，得知各校學者在研究上
所面臨的困難與挑戰，對於其日後往學術界方向發展的規劃有相當大的助益。
整體而言，這是一個相當成功的研討會，經由許多頂尖學者的論文研討，心得與經驗的交流對李同學在未來研究的
學習上有很大的助益。最後感謝國科會慷慨的補助機票費，使得本人與李同學在研究工作上無後顧之憂，而能更上一層
樓。
Applying a hybrid data envelopment analysis
approach to construct an intelligent stock trading
System
Ling-Jing Kao, Chih-Chou Chiu, Wei-Ming Li
Department of Business Management
National Taipei University of Technology
Taipei, Taiwan, R.O.C.
lingjingkao@ntut.edu.tw
Shu-Yu Tiffany Chiu
Department of Business Administration,
Fu Jen Catholic University
Taipei, Taiwan, R.O.C.
Abstract—The predictability of stock market returns has been
investigated in the financial literature for over four decades.
Various prediction or stock classification tools, such as
conventional statistical methods and non-parametric methods,
have been successfully developed in the literature to enhance the
existing financial analysis. Among these tools, the DEA (data
envelopment analysis)-based stock classification method is the
one received most discussion lately. However, literature has
shown that the efficiency estimates of a stock obtained from the
slack variable analysis can be biased if the inputs/outputs of this
stock are strongly correlated. In this paper, a two-stage approach
of integrating independent component analysis (ICA) and data
envelopment analysis (DEA) is proposed to overcome this issue.
We suggest using ICA first to extract the variables for generating
independent components, then selecting the ICs to represent the
independent sources of variables, and finally, inputting the
selected ICs as new variables in the DEA model. A financial data
on the selected integrated circuit companies collected from the
Taiwan Economic Journal and the Taiwan Stock Exchange
Corporation (TSEC) during 2004 to 2006 is used to demonstrate
the validity of the proposed two-stage approach. The results show
that the proposed method has not only the highest classification
accuracy but also the lowest potential loss. Hence the proposed
approach can reduce the possible high risks in stock selection.
Keywords- Independent component analysis, Data envelopment
analysis, Malmquist productivity index, Stock classification.
I. INTRODUCTION
Selecting stocks to maximize the return or minimize the
risk has been a challenging task for most investors. Investors
often obtain public financial statements of listed companies and
use the key financial indices for evaluating company financial
performance [1]. Among those key financial indices, price-
earnings ratio (P/E) and price-book value ratio (P/B) are the
most widespread stock price evaluation that is used to compare
the relative stock performance of companies in the same
industry [2,3,4,5]. For example, Bae et al. [6] demonstrated that
a trading strategy based on a combination of both earnings and
book value generates considerably higher returns than the
trading strategy simply based on earnings or book value.
Schadler and Eakins [7] also incorporated P/E and P/B ratios
into a new index for classifying stock attributes.
Because reviewing financial indices for all investment
targets is extremely tedious, literature has proposed several
solutions that can facilitate the stock selection decisions. For
example, Tetlock [8] investigates whether the occurrence of
negative words in firm-specific news articles can help investor
predict firms’ cash flows and whether firms’ stock market 
prices incorporate linguistic information efficiently.
Mittermayer and G. F. Knolmayer[9] used text mining
techniques to quantify linguistic information and get the
predefined set of features of the collected data. Then by
applying machine learning algorithms, such as neural networks,
various stock selecting models can be built. Hwang, Chuang,
and Chen [10] proposed a DEA-based stock classification
method to enhance the existing financial ratio analysis. They
demonstrate that, the financial return can be improved if the
stock trading strategy of their DEA-based stock classification
method is employed.
DEA (Data Envelopment Analysis) is a methodology that
can effectively measure the relative efficiencies of multiple
decision making units (DMUs) with similar goals and
objectives. However, the efficiency estimates of DMU obtained
from the slack variable analysis can be biased if the
inputs/outputs of a DMU are strongly correlated. This occurs
because DEA techniques mainly use the method of weighting
to calculate the ratio between the inputs and outputs of each
DMU.
Independent component analysis (ICA) is the other solution
to the problem of input/output correlation. Essentially, ICA is a
novel statistical signal processing technique used to extract
independent sources from observed multivariate statistical data
where no relevant data mixture mechanisms are available
[11,12]. It is a methodology for capturing both second and
higher order statistics, and it projects the input data onto the
basis vectors that are as statistically independent as possible
[13,14]. These characteristics of ICA distinguish ICA from
PCA that is used to find a set of the most representative
projection vectors such that the projected samples retain the
most information about the original samples [15]. The literature
has applied ICA in human face recognition on FERET database
[13,16] and the Olivetti and Yale databases[17].
DEA is also known as the efficient frontier approach [22,
23]. The term “envelopment” refers to the idea that inefficient 
DMUs are located inside an area enveloped by the efficient
DMUs. DEA is constructed based on the concept of relative
efficiency, which is defined as the ratio of the weighted sum of
outputs to the weighted sum of inputs [23]. The solution of
DEA requires that the weights for inputs and outputs of each
unit are selected to maximize its efficiency under certain
constraints. Thus, the mathematical programming form of the
CCR model is formulated as follows [23]:
Maximize
1 1 2 2
1 1 2 2
...
...
k k m mk
k k n nk
u y u y u y
v x v x v x
   
  
Subject to
1 1 2 2
1 1 2 2
...
1
...
j j m mj
j j n nj
u y u y u y
v x v x v x
  

  
(5)
j=1,2,…J
vi>0, i=1,2,…,n
ur>0,r=1,2,…,m
where 1 2, , ...,j j njx x x are the n inputs, 1 2, y , ..., yj j mjy are the
moutputs of the unit j, 1 2, , ..., nv v v are weights for the inputs,
and 1 2, , ..., mu u u are the weights for the outputs, k is the
designated unit for an optimization run, and Jis the total
number of units in the study. Because each decision unit has
its own preferences in input and output variables, the
implementation of the model in Eq.(5) can derive the values of
iv (i=1,2,.., n) and ru (r=1,2,.., m) for each decision unit.
Basically, the DEA-CCR model cannot deal with negative
data. To solve this problem, the Range Adjusted Measure
(RAM) model [22] with the property of translation
invariance[24] is used. In this study, by assumingXij equal unity
and all the ratios are considered as Yrj, the basic RAM model
can be modified as follows:
Maximize
1
1 m j
k
r j
s
F
m R



 
Subject to
1
J
j rj r rk
j
Y s Y 

  , r=1 to m
1
1
J
j
j



(6)
, 0j rs  ,
r=1,2,…,m; j=1,2,..,J
where s+ and s- are vectors of slack variables for the inputs and
outputs respectively; is the range from the lowest to highest
values observed for output j (j=1….m);  is the range from the 
lowest to the highest values observed for input i (i = 1…n); J 
denotes the number of units.
III. RESEARCH SCHEME AND EMPIRICAL APPLICATION
The proposed ICA-DEA method is a two-stage approach.
In the first stage, the independent components (ICs) are
obtained from ICA after independent variables are selected. In
the second stages, ICs obtained in the first stage are used to
compute the efficiency scores of each DMU in the DEA model.
A. Data
Financial data on the selected integrated circuit companies
collected from the Taiwan Economic Journal and the Taiwan
Stock Exchange Corporation (TSEC) during 2004 to 2006 is
used in this study to illustrate the proposed ICA–DEA
approach and to compare the performance of the proposed
method with alternative approach.The integrated circuit
companies in Taiwan’s semiconductor industry can be 
categorized into threedistinct sectors according to their
functions: (1) integrated circuit design; (2) integrated circuit
manufacturing (including integrated circuit foundry); and (3)
integrated circuit packaging and testing. Their main products
include: integrated circuit materials, memory (DRAM, SRAM),
logic integrated circuit, and analog integrated circuit, lead
frame, and foundry.To control the difference in company scales
and make the data sample more homogeneous, in this study, we
selected 46 companies (i.e., 46 DMUs)in integrated circuit
design sector for analysis.We chooseP/E and P/B ratios as our
variables because they are the most widespread measures of
firms’ relative stock performances.
As discussed above, the correlation among variables will
cause biased estimates in DEA. The resultof correlation
analysis of our variables (P/E and P/B ratios) shows that there
isnegative correlation (-0.2221)exists. Practically, the subsets
of the inputs or outputs are always correlated. The correlations
among variablescould cause issues with the distribution of the
weights (ur and vi in CCR model). Dropping some from the
assessment sometimes could not only reduce the efficiency
ratings for some DMUs [25] but also occasionally lead to
significant changes in efficiencies [26]. Therefore,removing the
correlation before conducting DEA can release the biased
estimation problem mentioned above.
B. Efficient Score Computation
After the ICs are determined, efficiency scores are
computed by the DEA-RAM model, proposed by [22], in the
LINGO software. Note that the extracted IC might have
negative values which violate the semi-positive assumption for
the DEA-CCR model, i.e., all inputs and all outputs are non
negative, and at least one input and one output are positive. To
solve this problem, we simply use the modified DEA-RAM
model (shown as Eq. (6)) with the property of translation
invariance [24] to calculate the efficiency score of each DMU.
Because we assume Xij equal unity, the used DEA-RAM
models will not have any input measures.
To demonstrate the validity of the proposed model, the
performance of the proposed ICA–DEA method is compared to
the single DEA model. The single DEA model simply applies
the DEA-RAM model to P/E and P/B ratios to measure the
efficiency of stocks without using ICA or PCA.
In the single DEA model, the overall efficiency results of
selected companies (DMUs) are summarized in Table 1. From
[14] Draper, B., Baek, K., Bartlett, M.S., and Beveridge, J.R. (2003),
“Recognizing faces with PCA and ICA”, Computer Vision and Image 
Understanding, 91(1–2), 115–137.
[15] Turk, M. and Pentland, A. (1991), “Eigenfaces for recognition”, Journal 
of Cognitive Neuroscience, 3(1): 71–86.
[16] Liu, C. and Wechsler, H. (1999), “Comparative assessment of 
independent component analysis (ICA) for face recognition”, 
Proceedings of the Second International Conference on Audio- and
Video-based Biometric Person Authentication (AVBPA’99), 
Washington DC.
[17] Yuen, P.C. and Lai, J.H. (2002), “Face representation using independent 
component analysis”, Patern Recognition, 35(6):1247-1257.
[18] Comon, P. (1994), “Independent component analysis: a new concept?”, 
Signal Processing, 36: 287-314.
[19] Lee, T. W. (1998), Independent Component Analysis: Theory and
Application, Kluwer Academic Publishers, Boston, MA.
[20] David, V. and Sanchez, A. (2002), “Frontiers of research in BSS/ICA”, 
Neruocomputing, 49: 7-23.
[21] Hyvärinen, A. (1999), “The fixed-point algorithm and maximum
likelihood estimation for independent component analysis”, Neural 
Processing Letters, 10: 1-5.
[22] Cooper, W.W., Park, K. S., and Pastor, J. T. (1999), “A range adjusted 
measure of inefficiency for use with additive models, and relations to
other models and measures in DEA”, Journal of Productivity Analysis, 
11: 5–42.
[23] Cooper, W.W., Seiford, L.M., and Zhu, J. (2004), Handbook on Data
Envelopment Analysis, Kluwer Academic, Boston, MA.
[24] Ali, I. and Seiford, L. (1990), “Translation invariance in data 
envelopment analysis”, Operations Research Leters, 9: 403–405.
[25] Nunamaker, T.R. (1985), “Using data envelopment analysis to measure 
the efficiency of non-profit organization: A critical evaluation”, 
Managerial and Decision Economics, 26(1): 50–58.
[26] Dyson, R., Allen, R., Camanho, A.S., Podinovski, V.V., Sarrico, C.S.,
and Shale, E.A. (2001), “Pitfals and protocols in DEA”, European 
Journal of Operational Research, 132 (2): 245–259
99年度專題研究計畫研究成果彙整表 
計畫主持人：邱志洲 計畫編號：99-2221-E-027-024-MY2 
計畫名稱：信用卡持有人顧客品質估計暨刷卡行為異質性分析 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100字為限） 
已於 2012年發表刊登於 SCI 期刊--Knowledge-Based Systems 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500字為限） 
In this paper, a new approach is proposed to combine the Bayesian behavior scoring 
model and the CART-based credit scoring model to overcome these three challenges 
facing by credit card issuing banks. The Bayesian behavior scoring model and the 
CART-based credit scoring model are combined by the average latent customer 
quality. The average latent customer quality represents the expected quality 
performance of a customer. Different from the binary credit rating, the continuous 
latent customer quality score provides the magnitude of customer quality and allows 
considerable latitude for producing finer rules for credit-granting decisions. 
The empirical study shows that the demographic variables used in most credit 
scoring models have little explanatory ability with respect to a cardholder＇s 
credit usage and repayment behavior. A cardholder＇s credit history provides the 
most important information in credit scoring. Compared to the performance of 
discriminant analysis, logistic regression, neural network, multivariate adaptive 
regression splines (MARS) and support vector machine (SVM), the proposed approach 
has a 92.9% accuracy rate in predicting customer types, is less impacted by prior 
probabilities, and has the lowest misclassification cost.  
 
