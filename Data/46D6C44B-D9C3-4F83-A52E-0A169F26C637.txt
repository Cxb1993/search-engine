 2
新聞影片字幕自動偵測之研究 
The Research of Automatic Caption Detection from News Video 
計畫編號：NSC 97-2221-E-155 -064 
執行期限：97 年 8 月 1 日 至 98 年 7 月 31 日 
主持人：李建誠    元智大學通訊工程學系 
 
中文摘要 
在影片檢索的領域中，因為新聞反映
時勢，且報導也是較為重要並與人們日常生
活較為相干的事件，所以新聞影片是資訊檢
索裡面最為重要的內容來源。在新聞影片
中，包含了各式訊息，如：聲音、影像、及
文字。而文字又包括了新聞字幕、氣象預
報、股市資訊、新聞跑馬燈等文字，及播報
影片本身的內嵌文字等。其中，因字幕通常
是該段新聞的總結說明，故又以新聞字幕最
能代表該段新聞之內容含義；本計畫「新聞
影片字幕自動偵測之研究」主要目的在於擷
取新聞影片之字幕，並濾除其他跟該段播報
內容無關之文字，以避免該無關文字干擾後
續的索引製作；並且能根據新聞影片的時間
冗餘(temporal redundancy)特性，降低處理的
資訊量，以避免重複的字幕被偵測出。 
 
關鍵詞: 影片檢索, 新聞字幕, 字幕擷取, 
頻率分析, 時間冗餘 
 
Abstract 
In news video, the video frame always 
contains many kinds of text, such as caption, 
scrolling text, weather forecasts, and stock 
quotes. Captions describe and summarize the 
news story. They can be regarded as indexes 
for the story. The indexes should be as concise 
as possible. Other miscellaneous texts will 
disturb the representative of the index. 
Therefore, to describe the story accurately, 
only the captions need to be extracted, and 
other miscellaneous texts should be ignored. 
In this report, we propose a novel caption 
localization and detection approach for news 
video. The proposed approach is able to not 
only localize and detect captions, but also 
filter out the undesired types of text. In 
addition, the approach takes the advantage of 
temporal redundancy property of caption to 
identify the caption beginning frame from 
news video. Hence, it can reduce the 
processing data for the subsequent optical 
character recognition (OCR) process.  
 
Keywords: video retrieval, news caption, 
caption extraction, frequency analysis, 
temporal redundancy 
 
一. 前言與目的 
在資訊傳遞發達的現今，新聞影片通常
同時夾雜了大量不同的訊息，包括主播所播
報的新聞、氣象預報、股市資訊、即時新聞
跑馬燈、重大事件即時訊息等；其中，新聞
字幕(News Caption)是做為總結播報者之新
聞內容，也常做為播報的新聞標題之用，故
其十分適合當作新聞檢索系統中所需的新
聞索引(News Index)資訊。然而，在新聞影
片之中，又充滿了與所播報的新聞不相干的
文字訊息，如：氣象預報、股市資訊、即時
新聞跑馬燈及影片中出現的文字等；因此，
如何成功地從新聞影片裡正確地擷取出新
聞字幕以做為索引，同時濾除掉其他文字，
以避免干擾索引的正確性，便成為新聞影片
檢索系統中十分困難的關鍵技術。 
 1. 前處理 (Preprocessing) 
由於新聞影片中包含了大量的影像與文
字訊息，因此我們希望在前處理步驟中，可
以先抓取出新聞字幕可能出現的區塊，再進
行處理，如此可大量降低後續特徵擷取時所
需的處理量。此程序首先進行播放率的降
低，以減少所需處理的影格數，接著再將這
些影格轉為梯度影像(gradient image)，再以
此梯度影像做後續處理。 
在一則新聞中，新聞字幕會暫留一段時
間，而背景影像畫面，其變動極為頻繁，因
此，我們可透過這個特性將所需的處理區域
縮減。參考 Tang et al. [1]的做法，首先，求
取 影 片 內 的 前 後 兩 張 影 像 的 差 量 
(Inter-frame different)，在新聞字幕區域中，
由於字幕暫留特性，故其差值應維持一個極
小的量，我們將此差值定義為 average pixel 
difference (APD) 
∑∑
= =
+−×=
M
j
N
k
ititit kjgkjgNM
APD
1 1
,1,, ),(),(
1
 (1) 
其中，M x N 為 block size，gt,i(j, k) 為
block i in frame t 位置為(j, k) 的灰階值。 
另外，由於文字本身的區塊較為複雜，
因此可計算一區塊的灰階變異數，若此區塊
的灰階變異數夠大，則可視為文字區塊，我
們將此灰階變異數定義為 block pixel 
variance (BPV) 
∑∑
= =
−×=
M
j
it
N
k
itit kjgNM
BPV
1
2
,
1
,, )),((
1 μ
  (2) 
其中， t,i 　 為 block i in frame t 的灰階平
均值。 
藉由求取 APD 與 BPV，並經由臨界值的
過濾，前後兩張影格內變動太大的區塊，以
及單一影格內區塊灰階值變異太小的，均可
視為非文字區塊，藉此便可用於找出可能的
字幕區塊，我們定義為候選字幕區塊 
(Candidate Caption Block, CCB)。透過個別
影像的 CCB 標記後，找出整段影片中，CCB
出現最頻繁的地方，表示這些區塊最常出現
文字，再將這些區塊合併並擴大，且稱之為
候選字幕條帶  (Candidate Caption Strip, 
CCS)，如圖四所示。 
 
圖四，候選字幕條帶 
2. 字幕面板偵測 (Caption Panel Detection) 
經過前處理程序後，我們可以將整個新
聞影片縮減成為候選字幕條帶影片，並對這
個候選條帶影片進行分割，圖五為一個候選
字幕條帶影片內的其中一張影像，此類影像
內包含的訊息不僅有新聞字幕，依據新聞台
的不同，尚可能包含有股市資訊、氣象預
報、時間與背景等。這些訊息在新聞片段內
出現的頻率並不相同。因此，我們可以透過
更新頻率的不同，以頻率分析方式，來進行
訊息面板與字幕面板的分割。 
 
圖五，候選字幕條帶結構圖 
為了將不同訊息的頻率特徵表現出來，
我們計算前後兩張候選字幕條帶影像的差
異量(difference)，再利用區塊掃描 (Block 
Scan) 的方式，求取每個區塊內的平均差異
量，而每一個區塊在候選字幕條帶影片上構
成的數值序列，便成為時間差異序列 (Time 
 4
 6
MILCOM 2004,  vol. 3,  2004, pp. 
1470-1476. 
[5] B. Le, T. W. Rondeau, D. Maldonado, 
C. W. Bostian, “Modulation 
identification using neural network for 
cognitive radios,” SDR Forum 
Technical Conference, Anaheim, CA, 
2005. 
[6] D. Chen, J. M. Odobez, and J. P. 
Thiran, “A localization/ verification 
scheme for finding text in images and 
video frames based on contrast 
independent features and machine 
learning methods,” Signal processing: 
Image Communication, vol. 19, 
pp.205-217, 2004. 
[7] P. Pudil, F.J. Ferri, J. Novovičová, and 
J. Kittler, “Floating Search Methods 
for Feature Selection with 
Nonmonotonic Criterion Functions,” 
Proc. IEEE Int. Conf. on Pattern 
Recognition, pp. 279-283, 1994. 
[8] Chien-Cheng Lee, Yu-Chun Chiang, 
and Cheng-Yuan Shih, “Subject 
Caption Detection and Localization in 
News Video with Complex Picture,” in 
Proceeding of 2009 World Congress 
on Computer Science and Information 
Engineering (CSIE 2009), March 
31-April 2, 2009, Los Angeles, USA.  
[9] Chien-Cheng Lee, Cheng-Yuan Shih, 
and Hao-Ming Huang, “A Story 
Related Caption Detection and 
Localization in News Video,” Optical 
Engineering, Vol. 48, No. 3, 037005, 
2009.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
圖一，CSIE 2009 會議會場 
 
圖二，海報發表 
 
二、與會心得 
綜觀這幾天的會議，最大的收穫在於瞭解各國在資訊、通訊等科技的水準，及
該科技對其他應用的一些技術，以及 keynote speech 精采演講中的前瞻研究方向。本
人現在正在進行的研究計劃是與影像視訊等智慧型運算相關的議題，在 CSIE 2009
的會議中，有好幾個 Section 的 Papers 都討論到這些 issues.我分別參加幾個 sections，
對我的研究上有著很好的收穫與經驗。CSIE 2009 是一個比較廣泛的研討會，在會議
表 Y04 
Subject Caption Detection in News Video with Complex Picture 
 
 
Chien-Cheng Lee1, Yu-Chun Chiang2, and Cheng-Yuan Shih1 
1Department of Communications Engineering, Yuan Ze University 
2Department of Mechanical Engineering, Yuan Ze University 
Chungli, Taoyuan 320, Taiwan  
 
 
Abstract 
 
We propose a method to detect story related subject 
captions in news video. This paper addresses two 
issues of caption detection problem. One is the time 
consuming in the feature computation, the other is the 
clutter of caption detection results. We first identify the 
subject caption region based on the frequency of text 
occurrence. After that, we detect the subject caption 
beginning frames. In this way, the computation time 
can be reduced significantly. Meanwhile, the 
uncorrected types of text are also filtered out, and only 
the subject caption is detected. Experimental results 
show that the proposed method can fast and robustly 
detect subject captions from news video. 
 
1. Introduction 
 
Video is the most popular media type. News videos 
bring us everyday stories and more meaningful 
information than other videos. Hence, news video 
retrieval has become an essential issue in information 
retrieval. Captions describe video pictures, outline the 
speeches, and summarize the news stories. Therefore, 
caption text can be regarded as keywords in news 
video retrieval. For this reason, caption detection in 
news video is a critical task for constructing the news 
video indexing. 
Most researchers treat each video frame as an 
independent image, and then perform the text detection 
frame by frame. These methods can be roughly divided 
into two categories: texture-based and region-based. 
The region-based methods are based on the assumption 
that text is represented with a uniform color. When 
characters are merged or not well separated from the 
background, the methods do not perform well. The 
texture-based methods treat the text region as a special 
type of texture. These methods extract various features, 
and then feed into a classifier to discriminate the text 
and non-text regions. Numerous features are employed 
in texture representation, such as Gabor filter [1], 
graylevel [2-4], and gradient [5]. Ye et al. [6] use 
wavelet moment features, wavelet histogram features, 
wavelet co-occurrence features, and crossing count 
histogram features to identify text lines. Chen et al. [7] 
utilize features including spatial derivatives, distance 
map, constant gradient variance, and discrete cosine 
transform (DCT) coefficients to find text in images and 
video frames. Zhong et al. [8] propose a method for 
localizing captions in JPEG images and I-frames of 
MPEG compressed videos. They use DCT coefficients 
to capture textural properties, such as the directionality 
and periodicity of local image blocks. 
In contrast to the text detection from independent 
images, captions in news video have an important 
natural property of temporal redundancy. In news 
video, to let audiences have enough time to understand 
captions, captions always stay for several seconds. 
Therefore, extracting captions for each frame will 
result in duplicates of the same captions, so it is not 
necessary. For this reason, temporal redundancy 
property can be used to improve the computation 
efficiency and avoid the duplicate captions. In recent 
literature, many methods of caption detection and 
localization in video have been proposed [9].  
In this paper, we propose a novel subject caption 
detection method for news video with complex 
background. Our proposal differs from many of the 
previous methods for caption detection and localization 
in video. It only detects subject captions, and also 
filters out the unrelated types of text. Accordingly, the 
proposed method can significantly reduce the number 
of processing frames by estimating caption beginning 
frames. The feature scanning is only performed on the 
subject caption region, not entire video frame. 
Consequently, the computation time can be reduced 
significantly, and the detected text is more appropriate 
for news video retrieval. 
The paper is structured as follows. In Session 2, the 
properties of news video with complex background are 
discussed. In Session 3, we describe the proposed 
method. Section 4 gives experiment results of caption 
3. Methodology 
3.1. Likely text block detection 
 
First, the source video sequence is down-converted 
the frame rate by removing the redundant frames to 
reduce the processing frames and duplicate captions. 
After the frame rate down-converting, a Sobel 
operator is used to calculate the gradient image for 
each Y component frame. The gradient images are 
regarded as input images in this step. Each frame is 
divided into several non-overlapping blocks. Both the 
block variance (BV) and different block mean (DBM) 
between consecutive fames for each block are 
calculated. The BV for block i in frame t is defined as 
∑∑
= =
−
×
=
M
j
it
N
k
itit kjgNM
BV
1
2
,
1
,, )),((
1 μ                     (1) 
where M x N is the block size, μt,i is the mean value of 
block i in frame t, and gt,i(j, k) is the pixel value at 
location (j, k) of block i in frame t. In this paper M and 
N are set to 16 and 8, respectively. 
The DBM between consecutive fames for block i in 
frame t is defined as 
∑∑
= =
+−
×
=
M
j
N
k
ititit kjgkjgNM
DBM
1 1
,1,, ),(),(
1
 .          (2) 
After the BVs and DBMs are calculated for each 
block, the LTBt,i is labeled as follows: 
⎩⎨
⎧ <>
=
otherwise
TDBMandTBVif
LTB dbmitvaritit ,0
,1 ,,
,
      (3) 
where Tvar and Tdbm are predefined thresholds.  
 
3.2. Caption strip mask localization 
 
The idea behind the localization of caption strip 
mask is that the LTBs in the caption strip must occur 
most frequently than other blocks because the strip is 
composed of message panel and caption panel. 
Therefore, to find out the LTBs of the more frequent 
occurrence, the probability of LTB occurrence PLOi in 
the whole video sequence is defined as 
∑
=
=
T
t
iti LTBT
PLO
1
,
1    (4) 
where T is the total number of frames in the news 
video.  
Afterward, we find the histogram of PLOi. The peak 
of the histogram is the block of the most frequent 
occurrence at the corresponding block location of the 
video. We extract the more frequent occurrence blocks 
by using the Otsu thresholding. These blocks are 
regarded as caption blocks. After the morphological 
opening operation, the maximum connected 
component of the caption blocks is obtained. A 
horizontal projection profile of caption block is defined 
as the sums of the caption blocks over rows. If the 
profile value is smaller than the half of the total 
number of the blocks for one row, the caption block on 
the row would be removed. The remaining region of 
the component is regarded as the caption strip mask. 
Because the boundaries of the caption text may lie 
outside the caption strip mask, to ensure the mask can 
completely cover the subject caption, the mask 
enlargement is necessary. The width of the mask is 
extended as wide as the image width so that it can 
cover the subject caption panel of any length. The top 
and bottom sides of the mask are enlarged with a half 
of the height of the block.  
 
3.3. Subject caption panel mask localization 
 
Because different types of text have different 
refresh rates in the caption strip, a frequency analysis 
technique can be employed to separate the subject 
caption panel from the caption strip. To achieve this 
purpose, an image sequence consisting of the 
differences of the caption strips between the 
consecutive frames in the video sequence is obtained 
first. These images are binarized into values of zero 
and one by using Otsu thresholding. For each pixel in 
the binarized image, we compute the mean value of all 
the pixels over a moving window centered at the pixel. 
The window is moved along in both the row and 
column dimensions one pixel at a time, until the entire 
image has been covered except the boundary pixels. 
The window size is set to the same size as LTB, i.e., M 
x N. Let Sk be a time series consisting of the mean 
values at the corresponding pixel location k for the 
image sequence.  
The time series Sk is further transformed to the 
frequency domain with zero-mean by the DCT 
transform. Then, the power spectrum of Sk is obtained, 
and it is used to discriminate the types of text. To 
discriminate these types of text from frequency 
domain, we use four features to analyze the power 
spectra [10], including the maximum value of the kth 
power spectrum for series, the variance of the power 
spectrum of Sk, the kurtosis of the power spectrum of 
Sk, and the skewness of the power spectrum of Sk. After 
the frequency features extraction, we choose SVM as 
the classifier to localize the message panel mask. 
To obtain a more complete area in the message 
panel mask, a morphological erosion operation with a 
5 x 5 structure element is performed. Then, a vertical 
projection profile of the label image is defined as the 
sums of the pixels over columns. If the profile value is 
smaller than one third of the total number of the pixels 
for one column, the vertical line on the column would 
be removed. After that, the maximum connected 
component is obtained. The region of the message 
panel mask can be found by the rectangle defined as 
