 行政院國家科學委員會補助專題研究計畫 
█ 成 果 報 告   
□期中進度報告 
 
 
應用 P2P 與 Web 技術發展以 SOA 為基礎的格網中介軟體與經濟模型 
(第三年) 
 
計畫類別：□個別型計畫  整合型計畫 
計畫編號：NSC 97-2628-E-216-006-MY3 
執行期間：99 年 08 月 01 日至 100 年 07 月 31 日 
 
計畫主持人：許慶賢   中華大學資訊工程學系教授 
共同主持人： 
計畫參與人員： 陳世璋、陳泰龍 (中華大學工程科學研究所博士生) 
     許志貴、陳柏宇、李志純、游景涵、黃安婷  
(中華大學資訊工程學系研究生) 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、列
管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年二年後可公開查詢 
          
執行單位：中華大學資訊工程學系 
 
中 華 民 國    100   年  10    月   28   日 
 2 
行政院國家科學委員會專題研究計畫成果報告 
   應用 P2P 與 Web 技術發展以 SOA 為基礎的格網中介
軟體與經濟模型 
 
計畫編號：NSC 97-2628-E-216-006-MY3 
執行期限：99 年 8 月 1 日至 100 年 7 月 31 日 
主持人：許慶賢   中華大學資訊工程學系教授 
 
計畫參與人員： 陳世璋、陳泰龍 (中華大學工程科學研究所博士生) 
     許志貴、陳柏宇、李志純、游景涵、黃安婷  
(中華大學資訊工程學系研究生) 
 
 
一、中文摘要 
 
本計畫整合 P2P 技術於格網系統，目標是提供大量分散式計算與資料之傳
輸。由於 P2P 具有自我調適、擴充性與容錯等特性，而格網著重於異質環境的
整合與資源的管理，格網技術與 P2P 技術的整合已經成為格網系統開發之趨
勢。這樣的結合可形成具擴充性、容錯、自我調適、高效能的格網平台。此外
藉由服務導向架構(SOA, Service-oriented Architecture)，將格網中成員所提供的
功能、參與的資源與交付的工作服務化，使得所有包裝的格網服務彼此間可以
透過標準的協定進行溝通與整合。因此本計畫以 P2P 與服務導向架構為基礎，
建置輕量化的格網系統。 
本計畫執行三年，第一年，我們發展完整的中介資料處理、儲存、與快取
機制、資料儲存機制、P2P 索引機制、安全性機制，並且建置虛擬儲存空間、
作業系統的虛擬檔案系統連結、針對 SRB 進行效能與系統功能測試。第二年，
我們改良 MapReduce 這個資料處理模型，並且配合原本的資料格網系統應用觀
念，發展能夠大量處理資料的應用程式介面，以及能夠針對特定領域來進行資
料處理的領域特定語言與各種平台函式庫開發。第三年，我們利用開發三套資
料格網的管理與分析工具，可以監控資料的分佈、網路狀態、運算資源的狀態，
並整理出資料的分佈模式。讓資料能夠運用其存放節點的運算資源，讓每筆資
料不需要透過集中式的運算，達成輕鬆地移動或複寫自己而提升整體資料的容
錯率。整體而言，本計畫預計完成的研究項目，皆已經實作出來，並在相關期
刊與研討會發表。 
 
 
關鍵詞：P2P、格網計算、Web 技術、中介軟體、經濟模型、服務導向架構、資源管理、
資料格網、網際服務。 
 4 
的研究課題： 
 
 發展以 P2P 及 W3C 各種 Web 技術為核心的格網中介軟體，具有輕量化，高速
傳輸，容錯及大量處理中介資料的能力，並擁有完整的開發環境 API。並且實際
建置於學術網路環境之上，使其成為接下來研究所依賴的開發環境。 
 利用 MapReduce 這個資料處理模型，發展大量資料操作，處理的應用程式介面
(Application Programming Interface)，以及能夠處理資料的領域特定語言(Domain 
Specific Language)，使得在此資料格網上開發應用軟體成為一件簡單的事情。 
 利用前項成果，發展資料自我感測式的複寫管理技術，並針對運行中的平台進行
容錯與效能測試。 
 發展大量部署及自我管理的智慧型資料格網中介軟體平台，使得未來管理的人力
及時間成本大量降低，並且實際地在既有的學術網路環境中建置該平台。 
 發展以服務為導向的格網經濟模型，應用在資料保全、工作排程、與各種 Web
服務，進而滿足不同使用者與系統管理的需求。 
 
 
三、研究方法與成果 
 
第一年，我們以 W3C 的 Web 相關技術，及現有的 P2P 演算法為基礎，研究資
料格網的中介資料，虛擬儲存，大量部署管理等核心技術的開發。而與現有的資料
格網中介軟體-Storage Resource Broker(SRB)，進行效能的比較。此外在校園學術網
路上進行系統平台的大量部署，利用校園內的分散硬碟空間，成功建置大型的資料
格網系統。 
第二年，我們進行資料領域特定語言的設計與各種平台函式庫開發，並且在學
術網路上進行第一年成果的建置與部署並且測試其效能。在這一個階段，我們也與
其他學校進行緊密的整合，並進行許多細節的修正。 
第三年，我們利用前項所發展的資料格網中介軟體 API，以及 P2P 分散式排程
的技術，進行資料自我感測式複寫技術的研究，並且開發完整的使用者介面，以及
發展成該平台複寫技術的核心。另外，建構以服務為導向的格網經濟模型，應用於
各種 Web 服務，進而滿足不同使用者的需求。格網經濟模型研究的重點在於同時考
量供給者的維運成本與滿足消費者的不同需求(QoS)，發展一套未來可以導入企業網
路的格網經濟架構。 
 6 
資料，他可以定期地統計所有 Domain MP 的狀態，並且透過 Grid Information System
的規格對外服務其資料；而對於使用者的部分，則是被用來當作 Web Portal 的負載
平衡進入點。為了避免發生問題，Network MP 的目的就不是用來服務，而是連線
重導，實際上真正進行服務的還是下層的 Domain Overlay。也就是說 Network MP
的功用有點像是 Web 負載平衡主機及 Proxy，但從實做的角度，就是簡單地開啟了
Portal 功能的 MP。 
 
Network Overlay
Domain Overlay
Domain MP
Storage Overlay 
SP
Network MP
 
圖二 P2P 資料格網架構圖 
 
 
如果系統中有單一進入點，通常也會造成單點錯誤(Single Point Failure)的來源。我
們將 Network MP 設計成很簡單的錯誤備援機制，一個 Network MP 只能夠有一個備
援，而透過其他網路上的通訊機制如 Mail 或簡訊來通知管理者已經發生錯誤備援的警
告，如圖三所示。 
 8 
資料中立的概念。這個概念描述說，任何中介資料索引或是儲存的機制，不能夠限制
在同一種儲存媒體上，如資料庫系統。DBMS 的優點是搜尋關連式資料或是聚合同性
質資料有一定的效率，可是對於要求高輸出的中介資料，不見得是好的實做。因此在
這裡我們選擇透過系統函式庫提供 DBMS Abstraction Layer，來支援不同的 DBMS 連
線，藉此達成中立概念。另一方面我們只利用 DBMS 來做中介資料索引及統計的動作。
除非是這兩個動作，否則都是得讓任何要讀取中介資料的節點將整個中介資料的 XML
讀取回去。然而利用 HTTP 的特性，節點在讀取的時候可以檢查其 ETag Header，發現
檔案沒有變動，即可直接利用本地快取。 
 
2. 通訊、Cluster Peer、與資料儲存 
 
HTTP 是一個非常完善設計的檔案通訊協定，而透過一些擴充如 WebDAV，使
得 HTTP 比 FTP 還要能夠有檔案的中介資料及實體資料傳輸的能力。儘管 HTTP 並
不是 Stateful，但我們依然能夠加上 Cookie 及 Session 來完成一些需要狀態的動作，
例如我們自行定義的通訊流程。此外 HTTP 很多核心的觀念都建立在其 Server 裡，
最有名的例子為 Apache HTTP Server，使得檔案的部分傳輸相當地簡單，因此平行
傳輸的問題只剩一半。另一個好處是，如果需要加密的傳輸，只需要把 URL 從 http
改為 https 即可。 
在資料格式定址(Data Format Addressing)的方法上，近來也有一個針對 HTTP 通
訊及 Web 應用相當熱門的名詞稱做 REST。REST 原意為 Representational State 
Transfer，表示同樣的 URL(Resource)，利用 HTTP Accept Header，能夠做到讓 Server
傳回不同型態的資料。例如 http://www.someone.com/login，對於 HTML 要求，便傳
回人看得懂的網頁；而對於程式的 XML 要求，便傳回 XML 結果。我們將利用這種
特性，將一系列的 URL 設計為簡易的 Web API，使得整個程式框架更容易實做。所
以，我們就可以將 URL 視作整個網路內資料的定址及選擇通訊資料格式的方式。 
對於 P2P 連線及平行傳輸，在前述的中介資料索引小節中，我們提到了整個 P2P
機制是透過 Chord 這個分散式雜湊表來完成。透過這機制，取得了任何資料的來源
清單後，便可以透過 HTTP 的資料分斷機制，進行平行化的傳輸。任何資料進行平
行傳輸後，會再次計算 checksum，確保資料傳輸沒有問題。 
針對大量部署及管理的問題，管理者不僅要從虛擬檔案系統看整個資料格網內
的資料，也必頇隨時掌握實體機器的狀態。但以往管理者遇到的問題是，很難快速
 10 
Storage Pool 2Storage Pool 1
Cluster Peer
Clustered Command Clustered Command
 
圖四 Cluster Peer 提供自動部署機制 
 
將大量的 SP 視作為磁碟快取的作法，對提高資料格網效率與擴充性有很大幫
助。這正符合一些用到大量硬碟空間的計畫需求。 
在資料儲存的策略方面，我們選擇的主流的 OS，其 Native IO API 作為預設儲
存實體檔案的驅動方式。也因此目前只支援 Linux 與 Windows。我們保留了可以方
便撰寫 Extension 的框架，期待更多使用者能夠參與開發。 
透過前述定址的方式，任何在資料格網上的檔案將會照著邏輯位置(Logical 
Location)來服務，也就是在虛擬儲存空間上的位置。而其虛擬的儲存位置，僅會以
簡單的雜湊來區隔檔案，而儲存在使用者或是系統指定的目錄裡。雜湊的目的，只
需要確認一個目錄的檔案個數上限不會超過檔案系統所能承受的即可。 
我們針對現行的檔案系統如 NTFS 及 EXT3，透過 Windows 或是 Unix 都會有的
原生 IO 指令來使用。並且設計儲存驅動的程式介面，讓任何人都可以透過撰寫外掛
的方式，來驅動自己所需要的儲存設備，如特殊的階層式檔案系統。此外，針對常
見的備份式儲存，如磁帶，以及可移除式儲存如 USB 碟與光碟，也都有支援。這些
非傳統的儲存介面帶來的挑戰是，利用檔案快取來讓使用者還是能夠存取到。也因
此後面會有提到利用 Cluster Peer(CP)及 SP 在近端網路建立大量磁碟快取(Disk Pool)
的作法，圖五所示。而另外一點令人關切的問題是，不同 Domain 的資料，或是不同
使用者擁有的資料，是不會能夠被其他人看見。 
針對很多學術計畫所需要的大量硬碟空間，透過上述部署機制，減少很多不必
要的管理時間。然而，同性質的機器，除了能夠當作儲存，使用其 CPU 的資源也是
相當重要。舉例如果有個計畫的成果會產生大量的感測器資料，不僅容量大，而且
 12 
與 Globus 整合後，所有節點間的通訊都必頇經過 GSI SSL，免不了加重了 CPU
負擔，也因此上述 CP 與 SP 的硬體要求就相當重要。然而好處是，除了確保計畫資料
絕對可以儲存在透過憑證授權的機器，資料格網的資料也都可以透過原先 Globus 支援
的方式來傳送。例如在沒有 Globus 的情況下，我們是使用未加密的 HTTP 傳送檔案，
而使用了 Globus 之後，任何檔案傳送就變成使用 GridFTP。而叢集指令也會使用
globus-job-run 的方式處理。對於原本在 Globus 上的應用方式，例如執行平行程式之
前，原本都是使用 GridFTP 每個節點來散佈，在我們的設計架構下，也可以呼叫資料
格網 API，或是叢集指令來進行快速的散佈。 
 
 
M
ap
p
in
g
R
ed
u
cin
g
Grouped SP Metadata
CP 
Metadata  
圖六 MapReduce 資料平行處理框架 
 
 
對於資料格網管理者而言，資料格網往往都具備一個重要的元件稱為虛擬檔案
系統，有了這個元件，任何使用者可以以管理單台電腦同樣的觀念來管理資料格網
理的檔案。以往針對虛擬檔案系統的管理方式，一直都是相當繁重而浪費時間的，
絕大部分都是因為浪費在使用者介面與系統通訊來來回回地。SRB 在 3 版後，不管
是 Web 介面，還是視窗介面，都實做出來了，但卻還是很難讓使用者感受到資料
格網是可以管理大量檔案的。問題也是在於操作順暢度，以及系統針對中介資料如
何處理。透過前述的中介資料快取，解決了很多順暢度的問題，因此要達成如檔案
總管般拖拉檔案相當地簡單。基本的檔案管理動作，如複製，更名，移動，會直接
對應到資料格網的 API。而比較複雜的動作，像是讓檔案在節點間移動，我們也會
採取較視覺化的作法。而另一些基本元素的管理，如使用者，也是將其模擬成如同
檔案般的觀念，使得管理者可以以同樣的觀念處理所有的工作。 
 14 
越大。但如果想要系統進行權限檢查時更快速，就必頇捨棄很多驗證的項目。而我
們的格網系統，主要是根據 Unix 的檔案權限系統，只有提供「擁有者」，「群組」，
「其他」等的權限等級，而驗證功能為「讀取 Metadata」，「完全讀取」，「寫入
Metadata」，「完全寫入」等，由於我們的群組功能支援子群組，也因此不需要考
慮一個檔案是不是要屬於很多群組這樣的功能。在預設的情況下，檔案是被擁有人
完全控制，而群組是完全讀取，而非擁有者或群組的帳號是無法讀取或寫入。 
在安全性方面，目前的資料格網系統，如 SRB，使用的是自行撰寫的認證系統。
而任何單一簽入的動作都是傳遞給中央控管的 MCAT 伺服器，這可以想像，每個
檔案都需要進行如此繁雜的動作，需要多少的負載。另一方面，SRB也支援與Globus 
GSI 整合，GSI 的方式幾乎是透過憑證，加上與本機的使用者進行對應。但 GSI 的
缺點，便是安裝，申請憑證的程序需要透過人工。如果要很快速地讓使用的儲存節
點增加，是相當地困難。 
而我們的格網系統採取上述優點的部分，並利用了 HTTP 協定，分做兩種情況。
一般的情況下，任何連線要進行認證前，會採取 SSL 進行通訊，使用的是該節點
安裝的時候便會產生的主機憑證。而實際進行 SSL 連線的時候，考量到該 SP 可能
會換 IP，便不會進行憑證驗證。SSL 連線中途，便向該 SP 傳送帳號密碼，而此 SP
會再透過 MP 進行認證。認證完畢，會傳回由 MP 產生的 Session Key。如此只要
連接在同一個 MP 下的 SP，便不需要重新認證。當 Client 確定離線後，Session Key
便會被清除。圖七展示了上述的認證過程。 
這個機制的優點是：第一，認證的單位，也就是想要單一簽入的範圍可以擴充。
要進行認證的帳號，可以在 MP 上設定將使用者中介資料移往更上層的 MP，這樣
因為發 session key 的單位為更上層，因此單一簽入的範圍就可以跨許多 MP。另一
點是，採取加密的連線，只會發生在進行認證連線的時候，因此對大多數的使用者，
儘管沒那樣安全，卻降低不必要的 CPU 負載。 
 
 16 
 
 
圖八 資料格網經濟模型架構圖 
 
對於最快完成與最小成本的方法，演算法的設計其實並不難。當使用者限制時
間內要完成檔案下載，或限制成本完成檔案下載，我們暫時考慮採用以下的方法。 
限制時間內要完成檔案下載 - 先計算出每個 Replica 在限制時間內最多可下載
的檔案大小 Capabilityi，再依照成本來排序，由最小成本開始選擇 Replica，直到整
個檔案大小可在所選的 Replica 下載完成，則所選的 Replica 即是可完成限制時間
內最小成本可完成下載檔案的 Replica 組合。 
限制成本完成檔案下載 -我們所提出的演算法是將限制的成本完全使用以求最
快的時間能夠完成下載檔案，演算法的概念是假設 Client 從每個 Replica 下載不同
大小的部份檔案，可以使下載的成本剛好等於限制的成本，因每個 Replica 下載的
部份檔案大小都不一樣，每個 replica 都會有一個變數。為了演算法複雜度，我們
將每個 Replica 產生一個分數，而成本愈低分數會愈高，所以演算法會先排序，讓
成本愈低的 Replica 得到愈高的分數，而這些分數會都會乘上一個變數，我們只要
調整此乘冪變數即可讓成本等於限制成本，再進行最佳化的部份。 
上述的劇情，基本上可以導入不同的應用領域。我們將提出若干個最佳化的演
算法，作為服務式計算的核心。並且，實作此一經濟模型與使用者介面，提供適應
於各種使用者需求的 Web 服務。 
 
四、結論與討論 
 
我們以 W3C 的 Web 相關技術，及現有的 P2P 演算法為基礎，研究資料格網的
中介資料，虛擬儲存，大量部署管理等核心技術的開發。而與現有的資料格網中介
 18 
 完成資料自我感測複寫機制技術與實作 
 完成格網經濟模型，並開發使用者介面，提供 Web 服務 
 發表 4 篇 SCI 國際期刊 
 
 Ching-Hsien Hsu, Hai Jin and Franck Cappello, “Peer-to-Peer Grid Technologies”, Future 
Generation Computer Systems (FGCS), Vol. 26, No. 5, pp. 701-703, 2010. 
 Ching-Hsien Hsu, Yun-Chiu Ching, Laurence T. Yang and Frode Eika Sandnes, “An Efficient Peer 
Collaboration Strategy for Optimizing P2P Services in BitTorrent-Like File Sharing Networks”, 
Journal of Internet Technology (JIT), Vol. 11, Issue 1, January 2010, pp. 79-88. (SCIE, EI) 
 Ching-Hsien Hsu and Shih Chang Chen, “A Two-Level Scheduling Strategy for Optimizing 
Communications of Data Parallel Programs in Clusters”, Accepted, International Journal of 
Ad-Hoc and Ubiquitous Computing (IJAHUC), 2010. (SCIE, EI, IF=0.66) 
 Ching-Hsien Hsu and Bing-Ru Tsai, “Scheduling for Atomic Broadcast Operation in 
Heterogeneous Networks with One Port Model,” The Journal of Supercomputing (TJS), Springer, 
Vol. 50, Issue 3, pp. 269-288, December 2009. (SCI, EI, IF=0.615) 
 
 發表 6 篇國際研討會論文 
 
 Ching-Hsien Hsu, Alfredo Cuzzocrea and Shih-Chang Chen, "CAD: Efficient Transmission 
Schemes across Clouds for Data-Intensive Scientific Applications", Proceedings of the 4th 
International Conference on Data Management in Grid and P2P Systems, LNCS, Toulouse, 
France, August 29-September 2, 2011.  
 Tai-Lung Chen, Ching-Hsien Hsu and Shih-Chang Chen, “Scheduling of Job Combination and 
Dispatching Strategy for Grid and Cloud System,” Proceedings of the 5th International Grid and 
Pervasive Computing (GPC 2010), LNCS 6104, pp. 612-621, 2010. 
 Shih-Chang Chen, Tai-Lung Chen and Ching-Hsien Hsu, “Message Clustering Techniques 
towards Efficient Communication Scheduling in Clusters and Grids,” Proceedings of the 10th 
International Conference on Algorithms and Architectures for Parallel Processing (ICA3PP 2010), 
LNCS 6081, pp. 283-291, 2010. 
 Shih-Chang Chen, Ching-Hsien Hsu, Tai-Lung Chen, Kun-Ming Yu, Hsi-Ya Chang and Chih-Hsun 
Chou, “A Compound Scheduling Strategy for Irregular Array Redistribution in Cluster Based 
Parallel System,” Proceedings of the 2nd Russia-Taiwan Symposium on Methods and Tools for 
Parallel Programming (MTPP 2010), LNCS 6083, 2010. 
 Ching-Hsien Hsuand Tai-Lung Chen, “Adaptive Scheduling based on Quality of Services in 
Heterogeneous Environments”, IEEE Proceedings of the 4th International Conference on 
Multimedia and Ubiquitous Engineering (MUE), Cebu, Philippines, Aug. 2010.  
 Ching-Hsien Hsu, Yen-Jun Chen, Kuan-Ching Li, Hsi-Ya Chang and Shuen-Tai Wang, "Power 
Consumption Optimization of MPI Programs on Multi-Core Clusters" Proceedings of the 4th ICST 
International Conference on Scalable Information Systems (InfoScale 2009), Hong Kong, June, 
2009, Lecture Notes of the Institute for Computer Science, Social Informatics and 
Telecommunications Engineering, (ISBN: 978-3-642-10484-8) Vol. 18, pp. 108-120, (DOI: 
10.1007/978-3-642-10485-5_8) (EI) 
 
五、計畫成果自評 
 
本計畫完成了大量中介資料處理機制，以及 CP 的儲存緩衝區與大量部署機
制。此外，我們也在多所大學進行部署、分析資料格網運作紀錄，建立資料運作模
型。本計畫相關論文產出共計發表四篇期刊論文與六篇研討會論文。期刊論文部
 20 
System for the SRB Data Grid,” Proceedings. 20th IEEE/11th NASA Goddard Conference on 
Mass Storage Systems and Technologies, (MSST 2003), 2003. 
[16] [16] Mike Burrows “The Chubby lock service for loosely-coupled distributed systems,” 7th 
USENIX Symposium on Operating Systems Design and Implementation (OSDI), 2006. 
[17] [17] N. Santos and B. Koblitz “Distributed Metadata with the AMGA Metadata Catalog” 
Workshop on Next-Generation Distributed Data Management 
[18] [18] N. Santos and B. Koblitz “Metadata Services on the Grid,” Proceedings of the X 
International Workshop on Advanced Computing and Analysis Techniques in Physics Research. 
[19] [19] Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung “The Google File System,” 
Proceedings of the 19th ACM Symposium on Operating Systems Principles, 2003, pp. 20-43. 
[20] [20] Tim Oreilly “What is Web 2.0: Design Patterns and Business Models for the Next 
Generation of Software,” Communications & Strategies, No. 1, p. 17, First Quarter 2007 
[21] [21] Ching-Hsien Hsu, Yun-Chiu Ching, Laurence T. Yang and Frode Eika Sandnes, “An Efficient Peer 
Collaboration Strategy for Optimizing P2P Services in BitTorrent-Like File Sharing Networks”, Journal of 
Internet Technology (JIT), Vol. 11, Issue 1, January 2010, pp. 79-88.  
[22] [22] Ching-Hsien Hsu and Shih Chang Chen, “A Two-Level Scheduling Strategy for Optimizing 
Communications of Data Parallel Programs in Clusters”, Accepted, International Journal of Ad-Hoc and 
Ubiquitous Computing (IJAHUC), 2010.  
[23] [23] Ching-Hsien Hsu and Bing-Ru Tsai, “Scheduling for Atomic Broadcast Operation in Heterogeneous 
Networks with One Port Model,” The Journal of Supercomputing (TJS), Springer, Vol. 50, Issue 3, pp. 
269-288, December 2009.  
[24] [24] Tai-Lung Chen, Ching-Hsien Hsu and Shih-Chang Chen, “Scheduling of Job Combination and 
Dispatching Strategy for Grid and Cloud System,” Proceedings of the 5th International Grid and Pervasive 
Computing (GPC 2010), LNCS 6104, pp. 612-621, 2010.  
[25] [25] Shih-Chang Chen, Tai-Lung Chen and Ching-Hsien Hsu, “Message Clustering Techniques towards 
Efficient Communication Scheduling in Clusters and Grids,” Proceedings of the 10th International 
Conference on Algorithms and Architectures for Parallel Processing (ICA3PP 2010), LNCS 6081, pp. 
283-291, 2010.  
[26] [26] Shih-Chang Chen, Ching-Hsien Hsu, Tai-Lung Chen, Kun-Ming Yu, Hsi-Ya Chang and Chih-Hsun Chou, 
“A Compound Scheduling Strategy for Irregular Array Redistribution in Cluster Based Parallel System,” 
Proceedings of the 2nd Russia-Taiwan Symposium on Methods and Tools for Parallel Programming (MTPP 
2010), LNCS 6083, 2010.  
 22 
2010/05/18 (上午) 
9:00  發表論文 
11:00 聽取 System Algorithm 相關論文發表 
 
(下午) 
2:00 聽取 Numerical simulation 相關論文發表 
4:00 參訪 Far East National University 
(晚上) 
7:00 參加晚宴 
2010/05/19 (上午) 
9:00 聽取 Simulation 相關論文發表 
 
 
 MTPP-10 是台俄雙邊在平行計算研究領域主要的研討會。這一次參與 MTPP-10 ，本人
擔任會議議程主席，除了發表相關研究成果以外，也在會場與多位國外教授交換研究心得，
並且討論未來可能的合作。 
 這一次參與 MTPP-10 除了發表我們最新的研究成果以外，也在會場中，向多位國內外
學者解釋我們的研究內容，彼此交換研究心得。除了讓別的團隊知道我們的研究方向與成
果，藉此，我們也學習他人的研究經驗。經過兩次的雙邊研討會交流，雙方已經找到共同研
究的題目，兩邊的團隊也將於今年(2010年)8月開始撰寫研究計畫書，進行更密切的合作。 
 這一次在 Vladivostok, Russia 所舉行的國際學術研討會議議程共計四天。開幕當天
由俄羅斯方面的 General Co-Chair，RSA 的 Victor E. Malyshkin 教授，與敝人分別致詞
歡迎大家參加這次的第二屆 MTPP 2010 國際研討會。接著全程參與整個會議的流程，也聽
取不同論文發表，休息時間與俄羅斯的學者教授交換意見和資訊。本人發表的論文在會議第
三天的議程九點三十分發表（A Compound Scheduling Strategy for Irregular Array 
Redistribution in Cluster Based Parallel System）。本人主要聽取 Parallel and 
Distributed 、Grid、Cloud 與 Multicore 相關研究，同時獲悉許多新興起的研究主題，
並了解目前國外學者主要的研究方向。最後一天，我們把握機會與國外的教授認識，希望能
夠讓他們加深對台灣研究的印象。這是一次非常成功的學術研討會。 
 主辦第一、二屆台俄雙邊學術研討會，感受良多。論文篇數從第一屆的 30 篇到第二屆
的 50篇，也讓本人感受到這個研討會的進步成長。台方參與的教授學生超過 15 個學研單位，
包括台大、清華、交大、中研院、成大、中山、等等。俄方也有超過 10 個學研單位的參與。
值得一提的是，這一次的論文集我們爭取到 Springer LNCS 的出版，並且在 EI 索引。這一
個研討會與發表的論文，其影響力已達到國際的水準。 
 
 24 
A Compound Scheduling Strategy for Irregular Array Redistribution in Cluster 
Based Parallel System 
Shih-Chang Chen
1
, Ching-Hsien Hsu
2
, Tai-Lung Chen
1
, Kun-Ming Yu
2
,  
Hsi-Ya Chang
3
 and Chih-Hsun Chou
2* 
 
1 College of Engineering  
2 Department of Computer Science and Information Engineering 
Chung Hua University, Hsinchu, Taiwan 300, R.O.C. 
3 National Center for High-Performance Computing, Hsinchu 30076, Taiwan 
{scc, robert, tai}@grid.chu.edu.tw, yu@chu.edu.tw, jerry@nchc.org.tw, chc@chu.edu.tw 
Abstract. With the advancement of network and techniques of clusters, joining clusters to 
construct a wide parallel system becomes a trend.  Irregular array redistribution employs 
generalized blocks to help utilize the resource while executing scientific application on such 
platforms.  Research for irregular array redistribution is focused on scheduling heuristics 
because communication cost could be saved if this operation follows an efficient schedule.  In 
this paper, a two-step communication cost modification (T2CM) and a synchronization 
delay-aware scheduling heuristic (SDSH) are proposed to normalize the communication cost and 
reduce transmission delay in algorithm level.  The performance evaluations show the 
contributions of proposed method for irregular array redistribution. 
1   Introduction 
Scientific application executing on parallel systems with multiple phases requires appropriate data 
distribution schemes.  Each scheme describes the data quantity for every node in each phase.  Therefore, 
performing data redistribution operations among nodes help enhance the data locality. 
Generally, data redistribution is classified into regular and irregular redistributions.  BLOCK, CYCLIC 
and BLOCK-CYCLIC(c) are used to specify array decomposition for the former while user-defined function, 
such as GEN_BLOCK, is used to specify array decomposition for the latter.  High Performance Fortran 
version 2 provides GEN_BLOCK directive to facilitate the data redistribution for user-defined function.  To 
perform array redistribution efficiently, it is important to follow a schedule with low communication cost. 
With the advancement of network and the popularizing of cluster computing research in campus, it is a 
trend to join clusters in different regions to construct a complex parallel system.  To performing array 
redistribution on this platform, new techniques are required instead of existing methods.   
Schedules illustrate time steps for data segments (messages) to be transmitted in appropriate time.  The 
cost of schedules given by scheduling heuristics is the summation of cost of every time steps while cost of 
each time step is dominated by the message with largest cost.  A phenomenon is observed that most local 
transmissions, which are happened in a node, do not dominate the cost of each step although they are in 
algorithm level for existing methods.  In other words, they are overestimated.  Since a node can send and 
receive only one message in the same time step [5], the arranged position of each message becomes 
important.  Therefore, a two-step communication cost modification (T2CM) and a synchronization 
delay-aware scheduling heuristic (SDSH) are proposed to deal with the overestimate problems, reduce 
overall communication cost and avoid synchronization of schedules in algorithm level. 
 26 
are given, where the array size is 100 units.  These two strings provide necessary information for nodes to 
generate messages to be transmitted among them.  Fig. 1 shows these messages marked from m1 to m11 and 
are with information such as data size, source node and destination node in the relative rows. 
Scheduling heuristics are developed for providing solutions of time steps to reduce total communication cost 
for a GEN_BLOCK redistribution operation.  In each step, there are several messages which are suggested to 
be transmitted in the same time step.  To help perform an efficient redistribution, scheduling methods 
should avoid node contention, synchronization delay and redundant transmission cost.  It is also important 
to follow policies of messages arrangement, i.e. with the same source nodes, messages should not be in the 
same step; with the same destination nodes, messages should be in different step; a node can only deal with 
one message while playing whether source node or destination node.  These messages that cannot be 
scheduled together called conflict tuples, for example, a conflict tuple is formed with messages m1 and m2.  
Note that if a node can only deal with a message while it is a source/destination node, the number of steps for 
a schedule must be the equal to or more than the number of messages from/to these nodes.  In other words, 
the minimal number of time steps is equal to the maximal number of messages in a conflict tuple, CTmax. 
 
Information of messages 
No. of 
message 
Data 
size 
Source 
node 
Destination 
node 
m1 13 0 0 
m2 3 1 0 
m3 17 1 1 
m4 1 2 1 
m5 13 2 2 
m6 3 2 3 
m7 13 3 3 
m8 4 3 4 
m9 12 4 4 
m10 13 5 4 
m11 8 5 5 
  
Fig. 1. Information of messages generated from given schemes to be transmitted on six nodes which are indexed from 0 to 5 
Fig. 2 gives a schedule with low communication cost and arranges messages in the number of minimal steps.  
In this result, there are three time steps with messages sent/received to/from different nodes.  The values 
beside m1~11 are data size, the cost of each step is dominated by the largest one.  Thus, m3, m1 and m8 
dominate step 1, 2 and 3, and the estimated cost are 17, 13 and 4, respectively.  To avoid node contentions, 
messages m1 and m2 are in separate steps due to destination nodes of both messages are the same.  Based on 
same argument, m2 and m3 are in separate steps due to both messages are members of a conflict tuple.  The 
total cost which represents the performance of a schedule is the summation of all cost of steps.  In other 
words, a schedule with lower cost is better than another one with higher cost in terms of performance. 
 28 
Fig. 4, they should be 2.125, 1.625, 1.625 and 13, respectively.  Node 1, 2 and 3 must wait for node 4 and 5 
to proceed next step because when the transmissions of m3, m5 and m7 are finished, the transmission of m10 is 
still on the way. 
 
Information of messages 
No. of 
message 
Data 
size 
Source 
node 
Destination 
node 
m1 1.625 0 0 
m2 3 1 0 
m3 2.125 1 1 
m4 1 2 1 
m5 1.625 2 2 
m6 15 2 3 
m7 1.625 3 3 
m8 4 3 4 
m9 1.5 4 4 
m10 13 5 4 
m11 1 5 5 
  
Fig. 3. The local reduction and inter amplification operations derive new data size for messages m1~11 
 
A result of scheduling heuristics 
No. of  
step 
No. of  
message 
Cost of  
step 
Step 1 m3(2.125), m5(1.625), m7(1.625), m10(13) 13 
Step 2 m1(1.625), m6(15), m9(1.5), m11(1) 15 
Step 3 m2(3), m4(1), m8(4) 4 
Total cost 32 
  
Fig. 4. The results with new dominators and cost 
The proposed synchronization delay-aware scheduling heuristic is a novel and efficient method to avoid 
delay among clusters and shorten communication cost while performing GEN_BLOCK redistribution.  To 
avoid synchronization delay, the transmissions happened in local memory are scheduled together in one 
single step instead of separating them among time steps like the results in Fig. 4.  Other messages are 
pre-proceeded by inter amplification and then scheduled by a low cost scheduling method which selects 
messages with smaller cost to shorten the cost of a step and avoid the node contentions.  Fig. 5 shows the 
results of SDSH which is with low synchronization delay and is contention free.  There are two reasons 
making the results in Fig. 5 better than the results in Fig. 4.  First, SDSH successfully avoids 
synchronization delay by congregating m1, m3, m5, m7, m9 and m11 in step 3.  It also helps reduce the cost of 
a step.  Second, messages m6 and m10 are the most important transmissions in the schedule due to their 
communication cost can dominate any steps.  It is a pity that they are separated in two steps in Fig. 4 due to 
the node contentions.  For example, it is impossible to move m6 to step 1 to shorten the cost of step 2 due to 
 30 
 
Results of evaluations 
Num. of nodes SDSH TPDR Same 
8 813 76 111 
16 946 43 11 
32 950 48 2 
64 914 79 7 
128 903 96 1 
Percentage 90.52% 6.84% 2.64% 
Total 4526 342 132 
 
Fig. 6. The results of both methods on five sets of nodes with 5,000 cases in total 
The attributes of generated cases dependents on the number of nodes, for example, higher CTmax and lower 
communication cost are with higher number of nodes.  It is hard to find the same schedules for two 
scheduling heuristics with larger number of nodes.  Fig. 7 shows the information of cases which are used to 
evaluate the SDSH and TPDR. 
 
Attributes of given cases 
Num. of 
nodes 
 
CTmax 
Average 
CTmax 
Cost of 1,000 cases 
SDSH TPDR 
8 6 3.271 6733580 7953932 
16 8 3.762 5733523 6983753 
32 10 4.246 3564076 4354899 
64 10 4.661 2412444 2781670 
128 11 5.009 1282008 1520884 
 
Fig. 7. Attributes of given cases for five set of nodes 
CTmax of results with 128 nodes is 11 which is almost two times larger than the CTmax of results with 8 nodes.  
The average CTmax also grows with higher number of nodes.  The total cost of schedules given by both 
methods for 1000 cases with different number of nodes explains the contribution of SDSH in Fig. 6.  The 
proposed method provides better schedules and the improves the communication cost about 15% while 
comparing to TPDR.  It also explains how SDSH outperforms its competitor.  Overall speaking, SDSH is a 
novel, efficient and simple method to provide solutions for scheduling communications of GEN_BLOCK 
redistribution. 
6   Conclusions 
To perform GEN_BLOCK redistribution efficiently, research focused on developing scheduling heuristic to 
shorten communication cost in algorithm level.  In this paper, a two-step communication cost modification 
(T2CM) and a synchronization delay-aware scheduling heuristic (SDSH) are proposed to normalize the 
transmission cost and reduce synchronization delay.  The two-step communication cost modification 
 32 
出席國際學術會議心得報告 
 
計 畫 名 稱 應用 P2P 與 Web 技術發展以 SOA 為基礎的格網中介軟體與經濟模型 
計 畫 編 號 NSC 97-2628-E-216-006-MY3 
報 告 人 姓 名 許慶賢 
服 務 機 構 
及 職 稱 
中華大學資訊工程學系教授 
會 議 名 稱 
The 12
th
 IEEE International Conference on Computational Science and 
Engineering (CSE-09) 
會議 /訪問時間地點 Vancouver, Canada / 2009.08.29-31 
發 表 論 文 題 目 
Data Distribution Methods for Communication Localization in Multi-Clusters with 
Heterogeneous Network 
 
參加會議經過 
 
會議時間 行程敘述 
2009/08/29 (上午) 
8:00 會場報到、聆聽 Keynote Speech 
     Privacy, Security, Risk and Trust in Service-Oriented 
Environments by Stephen S. Yau 
9:00 發表論文 
10:30 聽取 Parallel Algorithm 相關論文發表 
 
(下午) 
1:00 聆聽 Keynote Speech 
     Elections with Practical Privacy and Transparent Integrity by David  
Chaum  
2:00 聽取 Grid Computing相關論文發表 
3:30 主持 Session 
 
(晚上) 
7:30 參加歡迎茶會 
 34 
Computing Applications and Trends、White Space Networking - Is it Wi-Fi on Steroids?、
Computational Science and Engineering in Emerging Cyber-Ecosystems 五個不同的題目給予精
闢的演講。本人在這次研討會擔任兩個場次的 Chair 並發表了一篇論文，論文題目為 Data 
Distribution Methods for Communication Localization in Multi-Clusters with Heterogeneous 
Network ，擔任 Chair 分別為 CSE-09 (Session A16) 和 SEC-09(Session A27)，本人參與了三
天全程會議，也選擇了一些相關領域之場次來聆聽論文發表。 
 36 
communications of data parallel program on cluster grid.  The key idea is that of distributing data to 
grid/cluster nodes according to a mapping function at data distribution phase initially instead of in 
numerical-ascending order. 
In this paper, we consider the issue of real communication cost among a number of geographically grid 
nodes which belong to different clusters.  Method proposed previously has less communication cost by 
reordering logic id of processors.  Base on this idea, two new processor reorder techniques are proposed to 
adapt the heterogeneous network environment.  
This paper is organized as follows.  Section 2 presents related work.  In section 3, we provide 
background and review previously proposed processor reorder techniques.  In section 4, the Global 
Reordering technique is proposed for processor reordering in section 4.1.  The Divide and Conquer 
Reordering technique is proposed in section 4.2.  In section 5, we present the results of the evaluation of the 
new schemes.  Finally we have the conclusions and future work in section 6. 
2. Related Work 
Research work on computing grid have been broadly discussed on different aspects, such as security, fault 
tolerance, resource management [4, 6], job scheduling [1, 20, 21, 22], and communication optimizations [2].  
Commutating grid is characterized by a large number of interactive data exchanges among multiple 
distributed clusters over a network.  Thus, providing a reliable response in reasonable time with limited 
communication and computation resources for reducing the interactive data exchanges is required.  Jong 
Sik Lee [16] presented a design and development of a data distribution management modeling in 
computational grid. 
For the issue of communication optimizations, Dawson et al. [2] addressed the problems of optimizations 
of user-level communication patterns in the local space domain for cluster-based parallel computing.  Plaat 
et al. analyzed the behavior of different applications on wide-area multi-clusters [3, 19].  Similar research 
works were studied in the past years over traditional supercomputing architectures [11, 14].  Guo et al. [7] 
eliminated node contention in communication step and reduced communication steps with the schedule table.  
Y. W. Lim et al. [18] presented an efficient algorithm for Block-Cyclic data realignments.  Jih-Woei Huang 
and Chih-Ping Chu [8] presented a unified approach to construct optimal communication schedules for the 
processor mapping technique applying Block-Cyclic redistribution.  The proposed method is founded on 
the processor mapping technique and can more efficiently construct the required communication schedules 
than other optimal scheduling methods.  A processor mapping technique presented by Kalns and Ni [15] 
can minimize the total amount of communicating data.  Namely, the mapping technique minimizes the size 
of data that need to be transmitted between two algorithm phases.  Lee et al. [17] proposed similar method 
to reduce data communication cost by reordering the logical processors‟ id.  They proposed four algorithms 
for logical processor reordering.  They also compared the four reordering algorithms under various 
conditions of communication patterns.  There is significant improvement of the above research for parallel 
applications on distributed memory multi-computers.  However, most techniques are applicable for 
applications running on local space domain, like single cluster or parallel machine.  Ching-Hsien Hsu et al. 
 38 
table.  Messages {a1, a2, a3}, {e1, e2, e3} and {i1, i2, i3} are presented interior communications (|I| = 9); all 
the others are external communications (|E| = 18). 
 
(a) 
 
 
(b) 
 
Figure 1. Communication tables of data reallocation over the cluster grid. (a) Without data mapping. (b) With data mapping. 
The idea of changing logical processor mapping [15, 16] is employed to minimize data transmission time 
of runtime array redistribution in the previous research works.  In the cluster grid, we can derive a mapping 
function to produce a realigned sequence of logical processors‟ id for grouping communications into the 
local cluster.  Given an identical cluster grid with C clusters, a new logical id for replacing processor Pi can 
be determined by New(Pi) = (i mod C) * K + (i / C), where K is the degree of data refinement.  Figure 1(b) 
shows the communication table of the same example after applying the above reordering scheme.  The 
source data is distributed according to the reordered sequence of processors‟ id, i.e., <P0, P3, P6, P1, P4, P7, 
P2, P5, P8> which is computed by mapping function.  Therefore, we have |I| = 27 and |E| = 0. 
For the case of K (degree of refinement) is not equal to n (the number of grid nodes in each cluster), the 
mapping function becomes impracticable.  In this subsection, the previous work proposes a grid node 
replacement algorithm for optimizing distribution localities of data reallocation.  According to the relative 
position of the first of consecutive sub-blocks that produced by each processor, we can determine the best 
target cluster as candidate for node replacement.  Combining with a load balance policy among clusters, 
this algorithm can effectively improve data localities.  Figure 2 gives an example of data reallocation on the 
cluster grid, which has four clusters.  Each cluster provides three processors.  The degree of data 
refinement is set to four (K = 4).  Figure 2(a) demonstrates an original reallocation communication patterns.  
We observe that |I| = 12 and |E| = 36. 
 40 
algorithm, the derived sequence of logical grid nodes is <P2, P5, P9, P3, P6, P10, P4, P11, P0, P7, P12, P1, P8, 
P13>.  Figure 3(b) gives the communication tables when applying data to logical grid nodes mapping 
technique.  This data to grid nodes mapping produces 46 interior communications and 24 external 
communications.  This result reflects the effectiveness of the node replacement algorithm in term of 
minimizing inter-cluster communication overheads. 
 
(a) 
 
 
(b) 
 
Figure 3. Communication tables of data reallocation on non-identical cluster grid. (a) Without data mapping. 
(b) With data mapping. 
3.3 Communication Cost of Multi-Clusters with Heterogeneous Network 
Examples in the above section do not consider the real communication status for multi-clusters over 
heterogeneous network communication.  Figure 4(a) shows an example of four clusters with various 
inter-cluster communication costs.  Each unit‟s block data must spend 20 units time from the cluster-1 
transmission to cluster-2, but each unit‟s block data must spend 30 units time from the cluster-1 transmission 
to cluster-3.  Figure 4(b) shows the table of inter-cluster communication costs.  Therefore, we can 
calculate communication cost of data distribution for each processor over inter-cluster by this 
communication matrix.  After calculating, the communication cost are 1865 and 885 according to 
 42 
 
 
Figure 5. The total communication cost of grid model (C = 4, K = 5, < G (4): {2, 3, 4, 5}> ) 
4. Processor Mapping Methods 
According to communication cost, a candidate processor‟s id can be chosen according to minimum 
distribution cost.  Therefore, the first processor mapping method is proposed called Processor Mapping 
using Global Reordering (GR).  Another method rests on the cluster base, after all data redistribution costs 
of one cluster are arranged in an order, choosing a candidate processor‟s id according to the number of 
processor of its cluster  This method is called Processor Mapping using Divide and Conquer Reordering 
(DCR). 
4.1 Global Reordering Algorithm 
We propose a processor mapping scheme which requires the communication information of inter-cluster.  
First, the minimum cost is selected using Greedy algorithm.  This algorithm, Processor Mapping using 
Global Reordering (GR), is without the complex logic procedures of operation.  To achieve the result of 
processor mapping that has the least communication cost, the key idea is to choose the minimum 
communication cost from global candidates.  The transmission rate between each site over the internet is 
different because of the various network devices.  The cluster can easily measure the transmission rate by 
the present technology and keep it in each cluster.  The system can obtain transmission rates and produce an 
n*n cost matrix.  The combination of communication costs can be calculated using the cost matrix and data 
redistribution pattern.  According to the costs, the data block with minimum cost can be chosen to be 
assigned a processor id first.  In the choice process, two kinds of situations occur possibly.  To assign a 
processor id to a data block for distributing:  (1) the data block with the chosen minimum cost would be 
ignored if this data block has already been assigned to another candidate (processor id) previously.  (2) if no 
more processor id can be offered from the selected cluster, the selecting process will continue to find the 
 44 
P11, P0, P7 P12, P4, P8, P13 >.  Accordingly, the necessary communication cost is 740 units. 
 
According to the method described above, the code of algorithm is shown as follows: 
 
For P=0 to n-1 
    Determine how many cost matrix t on every cluster  
EndFor 
For t=1 to C  
    Order by cost from t 
EndFor 
While (Replacement s not complete) 
    For P=0 to n-1 
        If not (two or more cluster have the candidate) 
            select the target cluster processor id P that has the minimum 
cost 
        EndIf 
    EndFor 
    reorder the remaining cost list from t 
    select the target cluster processor id P by GR Algorithm 
EndWhile 
 
Figure 7. Processor Mapping using Divide and Conquer Reordering Algorithm. 
5. Performance Evaluation 
In this section, proposed techniques and methods without considering actual communication cost are 
implemented to simulate with different communication cost matrixes.  The network bandwidth is different 
from 10Mb to 1Gb for heterogeneous network environment.  Since 10Mb network equipments are almost 
eliminated, the value of transmission ratio is set from 10 to 30.  The value is randomly produced to simulate 
patterns of communication cost matrix.  The variance is set from 150 to 450 in simulations representing of 
network heterogeneity.  The larger number of variance represents the larger network heterogeneity.  
Besides, C is set from 8 to 16, K is set from 16 to 64 for simulations.  100 difference communication cost 
matrix patterns are used to calculate communication costs for each variance case and average of the costs is 
the results of the theoretical value.  The following figures show the results of each method. 
Figure 8 shows the results on a grid consisted of 8 cluster, <G(8):{4, 4, 4, 6, 6, 6, 8, 8}> and K is equal to 
16.  Figure 8 illustrates the comparing results of four different methods.  Original one does not consider 
the actual cost of reordering communications technology, GR is Processor Mapping using Global Reordering 
technology, DCR for the Processor Mapping using Divide and Conquer Reordering technology.  Obviously, 
GR and DCR have less communication cost compared with the other two models.  When the difference in 
the number of 150, GR and DCR can reduce about 33% cost compared with the traditional one which is 
without processor reordering.  Both of them also reduce 6% communications cost while comparing with the 
Original one.  While the variance is 450, the improvement slightly increases about 33% ~ 36%. 
 46 
REFERENCES 
[13] O. Beaumont, A. Legrand and Y. Robert, ”Optimal algorithms for scheduling divisible workloads on heterogeneous 
systems,” Proceedings of the 12
th
 IEEE Heterogeneous Computing Workshop, 2003.  
[14] J. Dawson and P. Strazdins, “Optimizing User-Level Communication Patterns on the Fujitsu AP3000,” Proceedings of the 
1st IEEE International Workshop on Cluster Computing, pp. 105-111, 1999. 
[15] Henri E. Bal, Aske Plaat, Mirjam G. Bakker, Peter Dozy, and Rutger F.H. Hofman, “Optimizing Parallel Applications for 
Wide-Area Clusters,” Proceedings of the 12th International Parallel Processing Symposium IPPS'98, pp 784-790, 1998. 
[16] M. Faerman, A. Birnbaum, H. Casanova and F. Berman, “Resource Allocation for Steerable Parallel Parameter Searches,” 
Proceedings of GRID’02, 2002. 
[17] I. Foster and C. Kessclman, “The Grid: Blueprint for a New Computing Infrastructure,” Morgan Kaufmann, ISBN 
1-55860-475-8, 1999. 
[18] James Frey, Todd Tannenbaum, M. Livny, I. Foster and S. Tuccke, “Condor-G: A Computation Management Agent for 
Multi-Institutional Grids,” Journal of Cluster Computing, vol. 5, pp. 237 – 246, 2002. 
[19] M. Guo and I. Nakata, “A Framework for Efficient Data Redistribution on Distributed Memory Multicomputers,” The 
Journal of Supercomputing, vol.20, no.3, pp. 243-265, 2001. 
[20] Jih-Woei Huang and Chih-Ping Chu, “An Efficient Communication Scheduling Method for the Processor Mapping 
Technique Applied Data Redistribution,” The Journal of Supercomputing, vol. 37, no. 3, pp. 297-318, 2006 
[21] Ching-Hsien Hsu, Guan-Hao Lin, Kuan-Ching Li and Chao-Tung Yang, “Localization Techniques for Cluster-Based Data 
Grid,” Proceedings of the 6th ICA3PP, Melbourne, Australia, 2005 
[22] Ching-Hsien Hsu, Tzu-Tai Lo and Kun-Ming Yu “Localized Communications of Data Parallel Programs on Multi-cluster 
Grid Systems,” European Grid Conference, LNCS 3470, pp. 900 – 910, 2005. 
[23] Florin Isaila and Walter F. Tichy, “Mapping Functions and Data Redistribution for Parallel Files,” Proceedings of IPDPS 
2002 Workshop on Parallel and Distributed Scientific and Engineering Computing with Applications, Fort Lauderdale, 
April 2002. 
[24] Bahman Javadi, Mohammad K. Akbari and Jemal H. Abawajy, "Performance Analysis of Heterogeneous Multi-Cluster 
Systems,"  Proceedings of ICPP, 2005 
[25] Bahman Javadi, J.H. Abawajy and Mohammad K. Akbari “Performance Analysis of Interconnection Networks for 
Multi-cluster Systems” Proceedings of the 6th ICCS, LNCS 3516, pp. 205 – 212, 2005. 
[26] Jens Koonp and Eduard Mehofer, “Distribution assignment placement: Effective optimization of redistribution costs,” IEEE 
TPDS, vol. 13, no. 6, June 2002. 
[27] E. T. Kalns and L. M. Ni, “Processor mapping techniques toward efficient data redistribution,” IEEE TPDS, vol. 6, no. 12, 
pp. 1234-1247, 1995. 
[28] Jong Sik Lee, “Data Distribution Management Modeling and Implementation on Computational Grid,” Proceedings of the 
4th GCC, Beijing, China, 2005. 
[29] Saeri Lee, Hyun-Gyoo Yook, Mi-Soon Koo and Myong-Soon Park, “Processor reordering algorithms toward efficient 
GEN_BLOCK redistribution,” Proceedings of the 2001 ACM symposium on Applied computing, 2001. 
[30] Y. W. Lim, P. B. Bhat and V. K. Parsanna, “Efficient algorithm for block-cyclic redistribution of arrays,” Algorithmica, vol. 
24, no. 3-4, pp. 298-330, 1999. 
[31] Aske Plaat, Henri E. Bal, and Rutger F.H. Hofman, “Sensitivity of Parallel Applications to Large Differences in Bandwidth 
and Latency in Two-Layer Interconnects,” Proceedings of the 5th IEEE High Performance Computer Architecture HPCA'99, 
pp. 244-253, 1999. 
[32] Xiao Qin and Hong Jiang, “Dynamic, Reliability-driven Scheduling of Parallel Real-time Jobs in Heterogeneous Systems,” 
Proceedings of the 30th ICPP, Valencia, Spain, 2001. 
[33] S. Ranaweera and Dharma P. Agrawal, “Scheduling of Periodic Time Critical Applications for Pipelined Execution on 
Heterogeneous Systems,” Proceedings of the 30th ICPP, Valencia, Spain, 2001. 
[34] D.P. Spooner, S.A. Jarvis, J. Caoy, S. Saini and G.R. Nudd, “Local Grid Scheduling Techniques using Performance 
Prediction,” IEE Proc. Computers and Digital Techniques, 150(2): 87-96, 2003. 
 48 
2009/06/11 (上午) 
9:00 聆聽Keynote Speech 
Autonomic Cloud Systems Management: Challenge and  
Opportunities 
Cheng-Zhong Xu, Wayne State University, USA 
10:30聽取 Information Security相關論文發表 
(下午) 
1:30聽取 Parallel and Distributed Computing相關論文發表 
3:30聽取 RFID / Sensor Network 相關論文發表 
 
這一次在香港所舉行的國際學術研討會議共計兩天。第一天上午由 Dr. Xian-He Sun (Illinois 
Institute of Technology, China) 針對 The current Multi-core architecture and memory-wall 
problem，作為此次研討會的開始，下午由 Dr. Minglu Li （Shanghai Jiao Tong University, 
China）針對 The application of mobile communication technology 給予專題演講。下午接著是
一個場次進行。本人聽取 session 3 的相關論文發表，也擔任主持第一天 session 3 的論文發
表。晚上在大會的地點與幾位國外學者及中國、香港教授交換心得意見。第二天，專題演
講是由 Dr. Cheng-Zhong Xu (Central Michigan University, USA) 針對 “Embedded Software 
Development with MDA”發表演說。本人也參與的第二天全部的大會議程，這天由
Cheng-Zhong Xu （Wayne State University, USA）。這一天，也發表了這一次的論文。本人
主要聽取 Multi-Core 等相關研究，同時獲悉許多新興起的研究主題，並了解目前國外大多
數學者主要的研究方向，並且把握最後一天的機會與國外的教授認識，希望能夠讓他們加
深對台灣研究的印象。二天下來，本人聽了許多優秀的論文發表。這些研究所涵蓋的主題
包含有：無線網路技術、網路安全、Multi-Core、資料庫以及普及運算等等熱門的研究課題。
此次的國際學術研討會議有許多知名學者的參與，讓每一位參加這個會議的人士都能夠得
到國際上最新的技術與資訊。是一次非常成功的學術研討會。 
 
 50 
1. Introduction 
Reduction on power consumption of computer systems is a hot issue recently, since many CPUs and 
computer-related hardware has been produced and under operation everywhere. As the number of single-core 
CPU has reached to physical limitation on current semi-conductor technology, the computing performance 
has met the bottleneck. Multi-core CPUs become a simple yet efficient solution to increase performance and 
speed since that concept SMP in a single chip, that is, making up a small cluster to be executed inside a host. 
Additionally, it reduces the amount of context switching while in single-core CPUs, increases straight 
forwardly the overall performance. Some CPU technologies and our target will be introduced in below.  
 
 
Figure 1: Intel Quad-Core CPU system structure [11] 
 
Figure 1 illustrates the architecture of Intel quad-core CPU, which looks like a combination of two 
dual-core CPUs. It has four individual execution engines, where each two cores share one set of L2 cache 
and system bus interface, and connect to the fixed system bus. The advantages of this architecture are 
twofold. The former one is that each core can fully utilize L2 cache as each core needs larger memory, while 
the latter is that each core accesses L2 cache through individual hub [7] simplifying system bus and cache 
memory structures. Intel CPU provides “SpeedStep” [3] technology that helps to control CPU frequency and 
voltage, and it needs to change all cores‟ frequency at the same. 
 52 
 
Figure 3: Multi-core based cluster structure [13] 
 
Developed from 1999, InfiniBand [16] is a point-to-point structure, original design concept that focused 
on high-performance computing support, so bidirectional serial fiber interface, failover mechanism and 
scalable ability are the necessary functions. InfiniBand supports at least 2.5Gbit/s bandwidth in each 
direction in single data rate (SDR) mode, the transmitted information includes 2 Gbit useful data and 
500Mbit control commands. Besides, InfiniBand supports DDR (Double Data Rate) and QDR (Quad Data 
Rate) transmission mode, and each mode supports 3 different speed (1x, 4x and 12x) configurations, so the 
maximum bandwidth is 96Gbit/s. The detail specification is as Table 1. 
 
Table 2: Infiniband transmission mode list 
 Single (SDR) Double (DDR) Quad (QDR) 
1X 2 Gbit/s 4 Gbit/s 8 Gbit/s 
4X 8 Gbit/s 16 Gbit/s 32 Gbit/s 
12X 24 Gbit/s 48 Gbit/s 96 Gbit/s 
 
Infiniband networking technology is a good and fast enough solution to connect all computing nodes of a 
cluster platform, but expensive. Gigabit Ethernet is cheaper solution and widely built in general network 
environment, though slower in transmission speed and definitely drop down data exchange performance. To 
 54 
Lower loading core is indicated higher priority to receive data frame, the method increases core 
working efficiency. 
2. Related Work 
Based on the concept about reducing computing time, the job scheduling methodology as introduced in [8] 
and [21] was designed targeting for a faster complete data transmission; otherwise, adjust cache block size to 
find the fastest speed that transmits data using MPI between MPI nodes in situations as listed in [13] was 
studied, and similar implementation of the method using OpenMP was also observed in [14]. Another 
investigation focused on compiler that analyze program‟s semantics, and insert special hardware control 
command that automatically adjusts simulation board‟s working frequency and voltage, [10] research needs 
to be combined both hardware and software resources. 
Some kinds of paper designed their methodologies or solutions under simulation board, or called NoC 
system, as shown in its structure is as below: 
 
 
Figure 4: General NoC system structure 
 
 56 
operation method, the approach applied in enterprise-scale commercial deployments and saved 74% power 
consumption. 
Besides, some researchers studied OS resource management and power consumption evaluation and task 
scheduling method as [23] and [25], this kind of study provides a direction to optimize computer operation 
tuning and reduces system idle time that brings by resource waiting. 
3. Challenges of Power Saving in Multi-Core Based Cluster 
In the previous single-core CPU based cluster environment, data distribution with CPU energy control are 
easier to implement by isolated CPU frequency control of each host. In multi-core based cluster, CPU 
internal bus architecture, bandwidth and power control structure bring differnet challanges in this issue. 
When we built a cluster platform that combines some key technologies as listed in Chapter 1 for experiment 
purposes, their advantages bring higher speed for data transmission peformance, yet only between cores 
inside a CPU, a CPU core is maintained with high load means the CPU speed cannot be decreased. Analysis 
and reasoning on these situations are discussed next. 
The “SpeedStep” and “PowerNow!” were not show in Figure 1 and 2. The “SpeedStep” provides solely 
full CPU frequency and voltage adjustment. The design makes power control easier, though consumes extra 
energy. If only one core works with high load, power control mechanism cannot reduce other cores‟ 
frequency / voltage, nor dropping down the performance of a busy core. Inefficient energy consumption 
brings temperature increasing, since low loading core generates the same heat as high load one, and brings 
the CPU‟s temperature up at the same time. 
AMD “PowerNow!” shows advantage in this issue, since we can reduce frequency when core works in 
lower loading without need to consider other cores‟ situation, and heat reduction is also another benefit. 
 58 
in core reduces data flow interference. Different CPU structure provides their advantages, and weakness 
appears while they are compared to each other. 
In a general situation, each computing node executed under a given core / host randomly indicated by 
cluster software, signifies that programmer cannot obtain additional core loading from node's code section. 
Following our purpose, finding system information about thread / node location works, but it is a hard 
method since the program would spend large amount of time in device I/O, includes open system state file, 
analysis information and obtaining node‟s location. Another alternative method is easier, where we make 
cluster platform that fixes node location in indicated core or host, and the function helps to get core loading 
from node‟s code. OpenMPI is selected for this issue. 
4. The Proposed Approach 
Upon with CPU specification, CPU power control interface and network structure, we provide a novel 
data dispatching strategy to solve the previous challanges in Chapter 3, it combines data flow limitation, core 
frequency controlling, and accords core working load to transmit data frame, detail operation is as below. 
It is not a good method to keep performance. In fact, we add 1µs delay between two packets, in a real 
environment, and the total transmission time is added as: 
T = N × D (2) 
where T is total time, N is total number of packets and D is delay time between packets. We found that the 
total time has just been added less than one to four seconds in average, when is transmitted 100K data frames 
across two hosts that are connected via Gigabit Ethernet. Additionally, the advantage is that the loading of a 
central node that sends data to other nodes is decreased by almost 50%. On the other hand, data receiving 
core load is decreased by 15% in average when we added 10µs delay in these nodes, follow Function 2, the 
amoung of increased delay time should be 1s, yet in experiment result, total transmission time is increased by 
 60 
 
Figure 5: LAD Algorithm structure diagram 
 
Still in LAD algorithm, as indicated in Figure 4, data frames are sent sequentially from Host 1-Core 0 to 
other cores. This method is often used to distribute wait-for-calculate data blocks in complex math parallel 
calculations. MPI provides broadcast command to distribute data and reduce command to receive result. In 
order to changing data frame transmission path dynamically, we use point-to-point command to switch data, 
since this type of command can indicate sending and receiving node. 
The detail of operation flow is as below: 
 Step 1: Detect core loading 
 Step 2: Find lowest loading core 
 Step 3: Send several data frames to the lowest loading core 
 Step 4: Repeat previous two step until all data frames are transmitted over 
The data distribution algorithm is given as below. 
Loading-Aware Dispatching (LAD)Algorithm 
1. generating wait-for-send data frame 
2. if (node 0) 
3. { 
4.    //send data follow sorting result 
5.    while(!DataSendingFinish) 
6.    { 
7.       //detect nodes‟ loading from system information and save in TargetNode 
8.       OpenCPUState; 
9.       CalculateCPULoading; 
 62 
 
Figure 6: Test environment 
 
Data frame size 
Three different sizes of data frames are transmitted between nodes: one byte, 1460 bytes and 8000 bytes. 
One byte frame is not only the smallest one in MPI data frame, but also in network, for complete data 
transmission in shortest time, source node generates huge amount of one byte frame, these packets congest 
CPU internal bus and network. 
1518 bytes frame is the largest one in network, but considering that network header should be inserted 
into network packet, we select 1460 bytes frame for testing, and then, this size of packet brings largest 
amount of data in a single network packet, and trigger fewest network driver interrupt to CPU. Finally 8000 
bytes frame is set for large data frame testing, since it needs to be separated to several other packets by 
network driver for transmission, but not necessary to be separated in intra-node, and thus need the longest 
time for data transmission. 
While the experiment is executed, we send 100K data frames between two nodes, and calculate the power 
consumption. 
CPU frequency and packet delay 
Each experiment result figures and tables that follows next has four blocks. The first one is executed in 
Performance Mode (PM, CPU works in 2.3GHz), the second one is PowerSave Mode (PS, 1.15GHz), the 
 64 
        Rank 
Number 
 
Mode & TD 
2 3 4 5 6 7 8 
PM 
mode 
0 0.160 0.333 0.501 6.262 10.646 15.072 18.630 
5 0.928 1.152 1.286 6.813 11.276 15.867 19.384 
10 2.292 2.271 1.775 6.655 11.076 15.419 19.238 
20 3.251 3.229 3.216 6.924 11.083 15.603 19.032 
PS 
mode 
0 0.285 0.576 0.909 9.984 16.537 23.151 28.976 
5 1.326 1.689 1.935 10.599 17.311 23.679 29.518 
10 2.637 2.174 2.429 10.580 17.848 24.157 29.598 
20 3.612 6.651 3.850 10.767 17.470 24.165 29.651 
OD 
mode 
0 0.216 0.372 0.531 6.625 11.388 16.025 18.664 
5 1.330 1.625 1.824 7.143 11.973 16.863 19.503 
10 2.630 2.126 2.256 6.898 11.693 16.456 19.456 
20 3.489 3.683 3.756 7.343 11.723 16.615 19.161 
LAD 
0 0.288 0.577 0.918 7.182 12.018 16.699 19.524 
5 1.367 1.423 1.623 8.716 14.587 20.181 20.704 
10 2.659 1.960 2.028 8.718 14.508 20.221 21.355 
20 3.598 3.813 3.716 9.254 15.129 20.253 22.943 
 
 
Figure 7: Time effect of TD on TT (Frame = 1 Byte) 
 66 
 
Figure 8: Power effect of TD on PC (Frame = one Byte) 
 
1460 byte frame 
Table 5 and Figure 7 show 1460 bytes frame TT. By comparing PM mode and OD mode, the completed 
time is longer than one byte frame in all situations. In Figure 8, OD mode uses in average over 200J less than 
PM mode. Our LAD algorithm made uses of 24~25s to complete data transmission as OD mode, yet 
consumes less than OD mode 200~600J in 8 ranks. In other situations, LAD keeps nearly the same 
performance, spending 3s longer than OD mode and consuming 200~600J less than OD mode. 
 
 68 
Figure 9: Time effect of TD on TT (Frame = 1460 Byte) 
 
Table 7: Detail results of power effect of TD on PC (Frame = 1460 Byte) 
       Rank 
Number 
 
Mode & TD 
2 3 4 5 6 7 8 
PM 
mode 
0 56.039 83.345 114.460 1423.847 1971.859 2781.018 4014.203 
5 158.117 188.597 209.711 1855.335 2082.032 2908.813 3872.596 
10 393.864 359.891 288.611 1557.516 2046.631 2818.166 3803.698 
20 528.644 520.865 515.150 2227.449 2025.676 2831.342 3891.170 
PS 
mode 
0 44.339 65.188 89.536 813.317 1351.245 1851.616 2266.093 
5 103.387 130.305 143.086 756.769 1383.661 1886.888 2315.502 
10 210.416 160.793 174.359 863.940 1413.934 1895.313 2342.063 
20 264.751 267.679 281.387 848.946 1385.517 1902.167 2343.848 
OD 
mode 
0 33.586 39.127 52.693 945.259 1645.667 2710.100 3839.306 
5 110.451 132.799 158.958 1032.840 1849.698 2663.291 3900.304 
10 223.043 182.830 195.905 1049.544 1827.601 2729.659 3861.452 
20 306.473 306.226 309.360 1022.164 1827.937 2739.376 3834.217 
LAD 
0 44.982 87.644 123.505 1028.737 1705.181 2431.432 3650.212 
5 104.575 118.019 124.578 1044.754 1715.120 2455.331 3608.556 
10 235.515 153.219 171.224 976.402 1847.987 2431.883 3525.145 
20 308.037 307.738 290.581 890.359 1654.873 2355.386 3209.088 
 
 70 
PS 
mode 
0 2.333 2.619 2.812 16.480 27.429 34.139 38.755 
5 2.240 2.684 2.964 16.716 27.728 35.219 39.884 
10 2.774 3.088 3.245 17.336 19.803 35.387 41.127 
20 4.685 4.678 4.244 16.700 27.613 35.219 39.930 
OD 
mode 
0 1.274 1.464 1.610 10.648 22.022 27.752 31.210 
5 2.045 2.407 2.226 14.377 21.778 27.739 34.744 
10 2.546 2.810 2.769 13.856 22.079 27.932 34.379 
20 4.338 4.380 4.448 14.037 21.957 27.603 34.878 
LAD 
0 1.917 2.125 2.200 12.242 23.169 28.553 33.991 
5 2.234 2.399 2.579 13.487 22.152 29.627 34.864 
10 2.672 2.779 2.740 13.018 24.838 30.845 34.518 
20 4.456 4.663 4.163 12.754 22.897 29.118 36.666 
 
 
 
Figure 11: Time effect of TD on TT (Frame = 8000 Byte)
 72 
 
Figure 12: Power effect of TD on PC (Frame = 8000 Byte) 
 
Remarks 
In this proposed research, LAD algorithm keeps in average 4s TT increasing, yet saves 200~600J that 
compares with OD mode in cross-node situation. Limited by only 2 steps experimental cases of CPU 
frequencies (2.3GHz and 1.15GHz), we cannot keep CPU loading in a smooth curve. In desktop and server 
CPU, they do not keep in high loading work longer time, while they complete a concurrent job and next one 
does not be started. Power saving technology helps to decrease host energy consumption, and decreasing 
energy cost and carbon dioxide emissions can be reduced. 
6. Conclusions 
One byte data frame is the smallest one, and it has 5 seconds transmission time shorter than 1460 bytes 
frame and 14 seconds shorter than 8000 bytes frame in cross node situation. That means two kinds of 
application which have no huge data need to be transmitted are suitable to use small data frame. 
 Mathematical calculation 
 74 
6. “AMD Family 10h Desktop Processor Power and Thermal Data Sheet”, http://www.amd.com/us-en
/assets/content_type/white_papers_and_tech_docs/GH_43375_10h_DT_PTDS_PUB_3.14.pdf 
7. “AMD Opteron Processor with Direct Connect Architecture”, http://enterprise.amd.com/downloads/4
P_Power_PID_4149 8.pdf 
8. Chao-Yang Lan, Ching-Hsien Hsu and Shih-Chang Chen, “Scheduling Contention-Free Irregular R
edistributions in Parallelizing Compilers”, The Journal of Supercomputing, Volume 40, Issue 3, (J
une 2007), Pages: 229-247 
9. Dongkun Shin and Jihong Kim, “Power-Aware Communication Optimization for Networks-on-Chi
ps with Voltage Scalable Links”, Proceeding of the International Conference on Hardware/Softwar
e Codesign and System Synthesis, 2004, Pages: 170-175 
10. Guangyu Chen, Feihui Li and Mahmut Kandemir, “Reducing Energy Consumption of On-Chip 
Networks Through a Hybrid Compiler-Runtime Approach”, 16th International Conference on Par
allel Architecture and Compilation Techniques (PA CT 2007), Pages: 163-174 
11. “Intel 64 And IA-32 Architectures Software Developers Manual, Volume 1”, http://download.in t
el.com/design/processor/manuals/253665.pdf 
12. “Key Architectural Features of AMD Phenom X4 Quad-Core Processors”, http://www.amd.com/u
s-en/Processors/ProductInformation/0,,30_118_15331_15332%5E15334,00.html 
13. Lei Chia, Albert Hartono, Dhabaleswar K. Panda, “Designing High Performance and Scalable M
PI Inter-node Communication Support for Clusters”, 2006 IEEE International Conference on Clu
ster Computing, 25-28 Sept. 2006, Pages: 1-10 
14. Ranjit Noronha and D.K. Panda, “Improving Scalability of OpenMP Applications on Multi-core 
Systems Using Large Page Support”, 2007 IEEE International Parallel and Distributed Processin
g Symposium, 26-30 March 2007, Pages: 1-8 
15. Umit Y. Ogras, Radu Marculescu, Hyung Gyu Lee and Na Ehyuck Chang, “Communication Ar
chitecture Optimization: Making the Shortest Path Shorter in Regular Networks-on-Chip”, 2006 
Proceedings of the conference on Design, Automation and Test in Europe, Munich, Germany, 
March 2006, Volume 1, Pages: 712-717  
 76 
25. Radha Guha, Nader Bagherzadeh and Pai Chou, “Resource Management and Task Partitioning a
nd Scheduling on a Run-Time Reconfigurable Embedded System”, Computers and Electrical En
gineering, March 2009, Volume 35, Issue 2, Pages: 258-285 
 78 
 
Towards Improving QoS-Guided Scheduling in Grids
 
 
Ching-Hsien Hsu
1
, Justin Zhan
2
, Wai-Chi Fang
3
 and Jianhua Ma
4 
 
1
Department of Computer Science and Information Engineering, Chung Hua University, Taiwan 
chh@chu.edu.tw 
2
Heinz School, Carnegie Mellon University, USA 
justinzh@andrew.cmu.edu 
3
Department of Electronics Engineering, National Chiao Tung University, Taiwan 
wfang@mail.nctu.edu.tw 
4
Digital Media Department, Hosei University, Japan 
jianhua@hosei.ac.jp 
 
 
Abstract 
 
With the emergence of grid technologies, the 
problem of scheduling tasks in heterogeneous systems has 
been arousing attention. In this paper, we present two 
optimization schemes, Makespan Optimization 
Rescheduling (MOR) and Resource Optimization 
Rescheduling (ROR), which are based on the QoS 
Min-Min scheduling technique, for reducing the 
makespan of a schedule and the need of total resource 
amount. The main idea of the proposed techniques is to 
reduce overall execution time without increasing 
resource need; or reduce resource need without 
increasing overall execution time. To evaluate the 
effectiveness of the proposed techniques, we have 
implemented both techniques along with the QoS 
Min-Min scheduling algorithm. The experimental results 
show that the MOR and ROR optimization schemes 
provide noticeable improvements.  
 
1. Introduction 
 
With the emergence of IT technologies, the need of 
computing and storage are rapidly increased.  To invest 
more and more equipments is not an economic method for 
an organization to satisfy the even growing computational 
and storage need. As a result, grid has become a widely 
accepted paradigm for high performance computing.   
To realize the concept virtual organization, in [13], 
the grid is also defined as “A type of parallel and 
distributed system that enables the sharing, selection, and 
aggregation of geographically distributed autonomous and 
heterogeneous resources dynamically at runtime 
depending on their availability, capability, performance, 
cost, and users' quality-of-service requirements”.  As the 
grid system aims to satisfy users‟ requirements with limit 
resources, scheduling grid resources plays an important 
factor to improve the overall performance of a grid.   
In general, grid scheduling can be classified in two 
categories: the performance guided schedulers and the 
economy guided schedulers [16]. Objective of the 
performance guided scheduling is to minimize turnaround 
time (or makespan) of grid applications. On the other 
hand, in economy guided scheduling, to minimize the cost 
of resource is the main objective.  However, both of the 
scheduling problems are NP-complete, which has also 
instigated many heuristic solutions [1, 6, 10, 14] to 
resolve. As mentioned in [23], a complete grid scheduling 
framework comprises application model, resource model, 
performance model, and scheduling policy. The 
scheduling policy can further decomposed into three 
phases, the resource discovery and selection phase, the 
job scheduling phase and the job monitoring and 
migration phase, where the second phase is the focus of 
this study.  
Although many research works have been devoted 
in scheduling grid applications on heterogeneous system, 
to deal with QOS scheduling in grid is quite complicated 
due to more constrain factors in job scheduling, such as 
the need of large storage, big size memory, specific I/O 
devices or real-time services, requested by the tasks to be 
completed. In this paper, we present two QoS based 
rescheduling schemes aim to improve the makespan of 
scheduling batch jobs in grid.  In addition, based on the 
QoS guided scheduling scheme, the proposed 
rescheduling technique can also reduce the amount of 
resource need without increasing the makespan of grid 
jobs.  The main contribution of this work are twofold, 
one can shorten the turnaround time of grid applications 
without increasing the need of grid resources; the other 
one can minimize the need of grid resources without 
increasing the turnaround time of grid applications, 
 80 
 A chunk of tasks will be scheduled to run completion 
based on all available machines in a batch system. 
 A task will be executed from the beginning to 
completion without interrupt. 
 The completion time of task ti to be executed on 
machine mj is defined as  
 
CTij = dtij + etij              (1) 
 
Where etij denotes the estimated execution time of task ti 
executed on machine mj; dtij is the delay time of task ti on 
machine mj.   
 
The Min-Min algorithm is shown in Figure 1. 
 
Algorithm_Min-Min() 
{ 
while there are jobs to schedule 
for all job i to schedule 
for all machine j 
Compute CTi,j = CT(job i, machine j) 
end for 
Compute minimum CTi,j 
end for 
Select best metric match m 
Compute minimum CTm,n 
Schedule job m on machine n 
end while 
} End_of_ Min-Min  
 
Figure 1. The Min-Min Algorithm 
 
Analysis: If there are m jobs to be scheduled in n 
machines, the time complexity of Min-Min algorithm is 
O(m
2
n). The Min-Min algorithm does not take into 
account the QoS issue in the scheduling.  In some 
situation, it is possible that normal tasks occupied 
machine that has special services (referred as QoS 
machine).  This may increase the delay of QoS tasks or 
result idle of normal machines. 
 
The QoS guided scheduling is proposed to resolve 
the above defect in the Min-Min algorithm.  In QoS 
guided model, the scheduling is divided into two classes, 
the QoS class and the non-QoS class.  In each class, the 
Min-Min algorithm is employed.  As the QoS tasks have 
higher priority than normal tasks in QoS guided 
scheduling, the QoS tasks are prior to be allocated on 
QoS machines.  The normal tasks are then scheduled to 
all machines in Min-Min manner.  Figure 2 outlines the 
method of QoS guided scheduling model with the 
Min-Min scheme.   
Analysis: If there are m jobs to be scheduled in n 
machines, the time complexity of QoS guided scheduling 
algorithm is O(m
2
n).  
Figure 3 shows an example demonstrating the 
Min-Min and QoS Min-Min scheduling schemes.  The 
asterisk * means that tasks/machines with QoS 
demand/ability, and the X means that QoS tasks couldn‟t 
be executed on that machine.  Obviously, the QoS 
guided scheduling algorithm gets the better performance 
than the Min-Min algorithm in term of makespan.  
Nevertheless, the QoS guided model is not optimal in 
both makespan and resource cost. We will describe the 
rescheduling optimization in next section. 
 
Algorithm_QOS-Min-Min() 
{ 
for all tasks ti in meta-task Mv (in an arbitrary order) 
for all hosts mj (in a fixed arbitrary order) 
       CTij = etij + dtj 
end for 
end for 
do until all tasks with QoS request in Mv are mapped 
for each task with high QoS in Mv,  
find a host in the QoS qualified host set that obtains 
the earliest completion time 
end for 
find task tk with the minimum earliest completion time 
assign task tk to host ml that gives the earliest completion 
time 
delete task tk from Mv 
update dtl 
update CTil for all i 
end do 
do until all tasks with non-QoS request in Mv are mapped 
for each task in Mv 
find the earliest completion time and the 
corresponding host 
       end for 
find the task tk with the minimum earliest completion time 
assign task tk to host ml that gives the earliest completion 
time 
delete task tk from Mv 
update dtl 
    update CTil for all i 
end do 
} End_of_ QOS-Min-Min 
 
Figure 2. The QoS Guided Algorithm 
4. Rescheduling Optimization 
Grid scheduling works as the mapping of individual 
tasks to computer resources, with respecting service level 
agreements (SLAs) [2].  In order to achieve the 
optimized performance, how to mapping heterogeneous 
tasks to the best fit resource is an important factor.  The 
Min-Min algorithm and the QoS guided method aims at 
scheduling jobs to achieve better makespan.  However, 
there are still having rooms to make improvements.  In 
this section, we present two optimization schemes based 
on the QoS guided Min-Min approach.  
 82 
Algorithm_MOR() 
{ 
for CTj in all machines 
find out the machine with maximum makespan CTmax and 
set it to be the standard 
end for 
do until no job can be rescheduled 
for job i in the found machine with CTmax  
            for all machine j 
  according to the job‟s QOS demand, find the 
adaptive machine j  
if (the execute time of job i in machine j + the 
CTj < makespan) 
           rescheduling the job i to machine j   
           update the CTj and CTmax 
       exit for 
end if 
            next for 
            if the job i can be reschedule 
find out the new machine with maximum CTmax 
            exit for 
end if 
next for 
end do  
} End_of_ MOR  
Figure 5. The MOR Algorithm 
4.2 Resource Optimization Rescheduling (ROR) 
Following the assumptions described in MOR, the main 
idea of the ROR scheme is to re-dispatch tasks from the 
machine with minimum number of tasks to other machines, 
expecting a decrease of resource need.  Consequently, if 
we can dispatch all tasks from machine Mx to other 
machines, the total amount of resource need will be 
decreased.  
Figure 6 gives another example of QoS scheduling, 
where the QoS guided scheduling presents makespan = 13. 
According to the clarification of ROR, machine „M1‟ has 
the fewest amount of tasks.  We can dispatch the task 
„T4‟ to machine „M3‟ with the following constraint 
 
CTij + CTj <= CTmax             (2) 
 
The above constraint means that the rescheduling can be 
performed only if the movement of tasks does not 
increase the overall makespan.  In this example, CT43 = 2, 
CT3=7 and CTmax=CT2=13.  Because the makespan of 
M3 (CT3) will be increased from 7 to 9, which is smaller 
than the CTmax, therefore, the task migration can be 
performed.  As the only task in M1 is moved to M3, the 
amount of resource need is also decreased comparing with 
the QoS guided scheduling.   
  
 M1 *M2 
T1 3 4 
T2 6 6 
*T3 X 7 
T4 4 6 
B. The Resource Optimization Rescheduling 
(ROR) Algorithm 
M3 
2 
3 
X 
2 
T5 5 7 2 
*T6 X 6 X 
Machine 
0 
M1 *M2 
*T6 
T1 
4 
8 
13 
M3 
*T3 
T5 
T2 
Makespan 
Machine 
T4 
M1 
A. The QOS guided scheduling 
algorithm 
0 
*M2 
T4 
*T6 
T1 
4 
8 
13 
M3 
*T3 
T5 
T2 
Makespan 
Figure 6. Example of ROR 
 
The ROR is an optimization scheme which aims to 
minimize resource cost. If there are m tasks to be 
scheduled in n machines, the time complexity of ROR is 
also O(m
2
n).  Figure 7 depicts a high level description of 
the ROR optimization scheme. 
 
Algorithm_MOR() 
{ 
for m in all machines 
        find out the machine m with minimum count of jobs 
end for 
do until no job can be rescheduled 
for job i in the found machine with minimum count of jobs 
            for all machine j 
according to the job‟s QOS demand, find the 
adaptive machine j  
if (the execute time of job i in machine j + the 
CTj <= makespan CTmax) 
           rescheduling the job i to machine j   
           update the CTj  
           update the count of jobs in machine m and 
machine j  
       exit for 
end if 
            next for         
next for 
end do 
} End_of_ MOR 
 
Figure 7. The ROR Algorithm  
5. Performance Evaluation 
5.1 Parameters and Metrics 
 
To evaluate the performance of the proposed 
techniques, we have implemented the Min-Min 
scheduling algorithm and the QoS guided Min-Min 
 84 
 
HQ 3 5 7 9 11 
Min-Min 1392.4 1553.9 1724.9 1871.7 2037.8 
QOS Guided Min-Min 867.5 1007.8 1148.2 1273.2 1423.1 
MOR 822.4 936.2 1056.7 1174.3 1316.7 
Improved Ratio 5.20% 7.11% 7.97% 7.77% 7.48% 
  
5.3 Experimental Results of ROR 
Table 3 analyzes the effectiveness of the ROR technique 
under different circumstances.   
 
Table 3: Comparison of Resource Used 
 
(a) (NR=100, QR=30%, QT=20%, HT=1, HQ=1) 
 
Task Number (NT) 200 300 400 500 600 
QOS Guided Min-Min 100 100 100 100 100 
ROR 39.81 44.18 46.97 49.59 51.17 
Improved Ratio 60.19% 55.82% 53.03% 50.41% 48.83% 
 
(b) (NT=500, QR=30%, QT=20%, HT=1, HQ=1) 
 
Resource Number (NR) 50 70 90 110 130 
QOS Guided Min-Min 50 70 90 110 130 
ROR 26.04 35.21 43.65 50.79 58.15 
Improved Ratio 47.92% 49.70% 51.50% 53.83% 55.27% 
 
(c) (NT=500, NR=50, QT=20%, HT=1, HQ=1) 
 
QR% 15% 30% 45% 60% 75% 
QOS Guided Min-Min 50 50 50 50 50 
ROR 14.61 25.94 35.12 40.18 46.5 
Improved Ratio 70.78% 48.12% 29.76% 19.64% 7.00% 
 
(d) (NT=500, NR=100, QR=40%, HT=1, HQ=1) 
 
QT% 15% 30% 45% 60% 75% 
QOS Guided Min-Min 100 100 100 100 100 
ROR 57.74 52.9 48.54 44.71 41.49 
Improved Ratio 42.26% 47.10% 51.46% 55.29% 58.51% 
 
(e) (NT=500, NR=100, QR=30%, QT=20%, HQ=1) 
 
HT 1 3 5 7 9 
QOS Guided Min-Min 100 100 100 100 100 
ROR 47.86 47.51 47.62 47.61 47.28 
Improved Ratio 52.14% 52.49% 52.38% 52.39% 52.72% 
 
(f) (NT=500, NR=100, QR=30%, QT=20%, HT=1) 
 
HQ 3 5 7 9 11 
QOS Guided Min-Min 100 100 100 100 100 
ROR 54.61 52.01 50.64 48.18 46.53 
Improved Ratio 45.39% 47.99% 49.36% 51.82% 53.47% 
 
 
Similar to those of Table 2, Table (a) changes the 
number of tasks to verify the reduction of resource that 
needs to be achieved by the ROR technique.  We noticed 
that the ROR has significant improvement in minimizing 
grid resources.  Comparing with the QoS guided 
Min-Min scheduling algorithm, the ROR achieves 50% ~ 
60% improvements without increasing overall makespan 
of a chunk of grid tasks.  Table (b) changes the number 
of machines.  The ROR retains 50% improvement ratio.  
Table (c) adjusts percentages of QoS machine.  Because 
this test has 20% QoS tasks, the ROR performs best at 
15% QoS machines.  This observation implies that the 
ROR has significant improvement when QoS tasks and 
QoS machines are with the same percentage.  Table (d) 
sets 40% QoS machine and changes the percentages of 
QoS tasks.  Following the above analysis, the ROR 
technique achieves more than 50% improvements when 
QoS tasks are with 45%, 60% and 75%.  Tables (e) and 
(f) change the heterogeneity of tasks.  Similar to the 
results of section 5.2, the heterogeneity of tasks is not 
critical to the improvement rate of the ROR technique.  
Overall speaking, the ROR technique presents 50% 
improvements in minimizing total resource need compare 
with the QoS guided Min-Min scheduling algorithm. 
 
6. Conclusions 
In this paper we have presented two optimization 
schemes aiming to reduce the overall completion time 
(makespan) of a chunk of grid tasks and minimize the 
total resource cost.  The proposed techniques are based 
on the QoS guided Min-Min scheduling algorithm. The 
optimization achieved by this work is twofold; firstly, 
without increasing resource costs, the overall task 
execution time could be reduced by the MOR scheme with 
7%~15% improvements. Second, without increasing task 
completion time, the overall resource cost could be 
reduced by the ROR scheme with 50% reduction on 
average, which is a significant improvement to the state of 
the art scheduling technique. The proposed MOR and 
ROR techniques have characteristics of low complexity, 
high effectiveness in large-scale grid systems with QoS 
services.  
 
References 
 
[1] A. Abraham, R. Buyya, and B. Nath, "Nature‟s Heuristics for 
Scheduling Jobs on Computational Grids", Proc. 8th IEEE 
International Conference on Advanced Computing and 
Communications (ADCOM-2000), pp.45-52, 2000. 
[2] A. Andrieux, D. Berry, J. Garibaldi, S. Jarvis, J. MacLaren, D. 
Ouelhadj, D. Snelling, "Open Issues in Grid Scheduling", 
National e-Science Centre and the Inter-disciplinary Scheduling 
Network Technical Paper, UKeS-2004-03. 
[3] R. Buyya, D. Abramson, Jonathan Giddy, Heinz Stockinger, 
“Economic Models for Resource Management and Scheduling 
 86 
 
 
 行政院國家科學委員會補助專題研究計畫 
█ 成 果 報 告   
□期中進度報告 
 
 
應用 P2P 與 Web 技術發展以 SOA 為基礎的格網中介軟體與經濟模型 
(第三年) 
 
計畫類別：□個別型計畫  整合型計畫 
計畫編號：NSC 97-2628-E-216-006-MY3 
執行期間：99 年 08 月 01 日至 100 年 07 月 31 日 
 
計畫主持人：許慶賢   中華大學資訊工程學系教授 
共同主持人： 
計畫參與人員： 陳世璋、陳泰龍 (中華大學工程科學研究所博士生) 
     許志貴、陳柏宇、李志純、游景涵、黃安婷  
(中華大學資訊工程學系研究生) 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、列
管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年二年後可公開查詢 
          
執行單位：中華大學資訊工程學系 
 
中 華 民 國    100   年  10    月   28   日 
 2 
行政院國家科學委員會專題研究計畫成果報告 
   應用 P2P 與 Web 技術發展以 SOA 為基礎的格網中介
軟體與經濟模型 
 
計畫編號：NSC 97-2628-E-216-006-MY3 
執行期限：99 年 8 月 1 日至 100 年 7 月 31 日 
主持人：許慶賢   中華大學資訊工程學系教授 
 
計畫參與人員： 陳世璋、陳泰龍 (中華大學工程科學研究所博士生) 
     許志貴、陳柏宇、李志純、游景涵、黃安婷  
(中華大學資訊工程學系研究生) 
 
 
一、中文摘要 
 
本計畫整合 P2P 技術於格網系統，目標是提供大量分散式計算與資料之傳
輸。由於 P2P 具有自我調適、擴充性與容錯等特性，而格網著重於異質環境的
整合與資源的管理，格網技術與 P2P 技術的整合已經成為格網系統開發之趨
勢。這樣的結合可形成具擴充性、容錯、自我調適、高效能的格網平台。此外
藉由服務導向架構(SOA, Service-oriented Architecture)，將格網中成員所提供的
功能、參與的資源與交付的工作服務化，使得所有包裝的格網服務彼此間可以
透過標準的協定進行溝通與整合。因此本計畫以 P2P 與服務導向架構為基礎，
建置輕量化的格網系統。 
本計畫執行三年，第一年，我們發展完整的中介資料處理、儲存、與快取
機制、資料儲存機制、P2P 索引機制、安全性機制，並且建置虛擬儲存空間、
作業系統的虛擬檔案系統連結、針對 SRB 進行效能與系統功能測試。第二年，
我們改良 MapReduce 這個資料處理模型，並且配合原本的資料格網系統應用觀
念，發展能夠大量處理資料的應用程式介面，以及能夠針對特定領域來進行資
料處理的領域特定語言與各種平台函式庫開發。第三年，我們利用開發三套資
料格網的管理與分析工具，可以監控資料的分佈、網路狀態、運算資源的狀態，
並整理出資料的分佈模式。讓資料能夠運用其存放節點的運算資源，讓每筆資
料不需要透過集中式的運算，達成輕鬆地移動或複寫自己而提升整體資料的容
錯率。整體而言，本計畫預計完成的研究項目，皆已經實作出來，並在相關期
刊與研討會發表。 
 
 
關鍵詞：P2P、格網計算、Web 技術、中介軟體、經濟模型、服務導向架構、資源管理、
資料格網、網際服務。 
 4 
的研究課題： 
 
 發展以 P2P 及 W3C 各種 Web 技術為核心的格網中介軟體，具有輕量化，高速
傳輸，容錯及大量處理中介資料的能力，並擁有完整的開發環境 API。並且實際
建置於學術網路環境之上，使其成為接下來研究所依賴的開發環境。 
 利用 MapReduce 這個資料處理模型，發展大量資料操作，處理的應用程式介面
(Application Programming Interface)，以及能夠處理資料的領域特定語言(Domain 
Specific Language)，使得在此資料格網上開發應用軟體成為一件簡單的事情。 
 利用前項成果，發展資料自我感測式的複寫管理技術，並針對運行中的平台進行
容錯與效能測試。 
 發展大量部署及自我管理的智慧型資料格網中介軟體平台，使得未來管理的人力
及時間成本大量降低，並且實際地在既有的學術網路環境中建置該平台。 
 發展以服務為導向的格網經濟模型，應用在資料保全、工作排程、與各種 Web
服務，進而滿足不同使用者與系統管理的需求。 
 
 
三、研究方法與成果 
 
第一年，我們以 W3C 的 Web 相關技術，及現有的 P2P 演算法為基礎，研究資
料格網的中介資料，虛擬儲存，大量部署管理等核心技術的開發。而與現有的資料
格網中介軟體-Storage Resource Broker(SRB)，進行效能的比較。此外在校園學術網
路上進行系統平台的大量部署，利用校園內的分散硬碟空間，成功建置大型的資料
格網系統。 
第二年，我們進行資料領域特定語言的設計與各種平台函式庫開發，並且在學
術網路上進行第一年成果的建置與部署並且測試其效能。在這一個階段，我們也與
其他學校進行緊密的整合，並進行許多細節的修正。 
第三年，我們利用前項所發展的資料格網中介軟體 API，以及 P2P 分散式排程
的技術，進行資料自我感測式複寫技術的研究，並且開發完整的使用者介面，以及
發展成該平台複寫技術的核心。另外，建構以服務為導向的格網經濟模型，應用於
各種 Web 服務，進而滿足不同使用者的需求。格網經濟模型研究的重點在於同時考
量供給者的維運成本與滿足消費者的不同需求(QoS)，發展一套未來可以導入企業網
路的格網經濟架構。 
 6 
資料，他可以定期地統計所有 Domain MP 的狀態，並且透過 Grid Information System
的規格對外服務其資料；而對於使用者的部分，則是被用來當作 Web Portal 的負載
平衡進入點。為了避免發生問題，Network MP 的目的就不是用來服務，而是連線
重導，實際上真正進行服務的還是下層的 Domain Overlay。也就是說 Network MP
的功用有點像是 Web 負載平衡主機及 Proxy，但從實做的角度，就是簡單地開啟了
Portal 功能的 MP。 
 
Network Overlay
Domain Overlay
Domain MP
Storage Overlay 
SP
Network MP
 
圖二 P2P 資料格網架構圖 
 
 
如果系統中有單一進入點，通常也會造成單點錯誤(Single Point Failure)的來源。我
們將 Network MP 設計成很簡單的錯誤備援機制，一個 Network MP 只能夠有一個備
援，而透過其他網路上的通訊機制如 Mail 或簡訊來通知管理者已經發生錯誤備援的警
告，如圖三所示。 
 8 
資料中立的概念。這個概念描述說，任何中介資料索引或是儲存的機制，不能夠限制
在同一種儲存媒體上，如資料庫系統。DBMS 的優點是搜尋關連式資料或是聚合同性
質資料有一定的效率，可是對於要求高輸出的中介資料，不見得是好的實做。因此在
這裡我們選擇透過系統函式庫提供 DBMS Abstraction Layer，來支援不同的 DBMS 連
線，藉此達成中立概念。另一方面我們只利用 DBMS 來做中介資料索引及統計的動作。
除非是這兩個動作，否則都是得讓任何要讀取中介資料的節點將整個中介資料的 XML
讀取回去。然而利用 HTTP 的特性，節點在讀取的時候可以檢查其 ETag Header，發現
檔案沒有變動，即可直接利用本地快取。 
 
2. 通訊、Cluster Peer、與資料儲存 
 
HTTP 是一個非常完善設計的檔案通訊協定，而透過一些擴充如 WebDAV，使
得 HTTP 比 FTP 還要能夠有檔案的中介資料及實體資料傳輸的能力。儘管 HTTP 並
不是 Stateful，但我們依然能夠加上 Cookie 及 Session 來完成一些需要狀態的動作，
例如我們自行定義的通訊流程。此外 HTTP 很多核心的觀念都建立在其 Server 裡，
最有名的例子為 Apache HTTP Server，使得檔案的部分傳輸相當地簡單，因此平行
傳輸的問題只剩一半。另一個好處是，如果需要加密的傳輸，只需要把 URL 從 http
改為 https 即可。 
在資料格式定址(Data Format Addressing)的方法上，近來也有一個針對 HTTP 通
訊及 Web 應用相當熱門的名詞稱做 REST。REST 原意為 Representational State 
Transfer，表示同樣的 URL(Resource)，利用 HTTP Accept Header，能夠做到讓 Server
傳回不同型態的資料。例如 http://www.someone.com/login，對於 HTML 要求，便傳
回人看得懂的網頁；而對於程式的 XML 要求，便傳回 XML 結果。我們將利用這種
特性，將一系列的 URL 設計為簡易的 Web API，使得整個程式框架更容易實做。所
以，我們就可以將 URL 視作整個網路內資料的定址及選擇通訊資料格式的方式。 
對於 P2P 連線及平行傳輸，在前述的中介資料索引小節中，我們提到了整個 P2P
機制是透過 Chord 這個分散式雜湊表來完成。透過這機制，取得了任何資料的來源
清單後，便可以透過 HTTP 的資料分斷機制，進行平行化的傳輸。任何資料進行平
行傳輸後，會再次計算 checksum，確保資料傳輸沒有問題。 
針對大量部署及管理的問題，管理者不僅要從虛擬檔案系統看整個資料格網內
的資料，也必頇隨時掌握實體機器的狀態。但以往管理者遇到的問題是，很難快速
 10 
Storage Pool 2Storage Pool 1
Cluster Peer
Clustered Command Clustered Command
 
圖四 Cluster Peer 提供自動部署機制 
 
將大量的 SP 視作為磁碟快取的作法，對提高資料格網效率與擴充性有很大幫
助。這正符合一些用到大量硬碟空間的計畫需求。 
在資料儲存的策略方面，我們選擇的主流的 OS，其 Native IO API 作為預設儲
存實體檔案的驅動方式。也因此目前只支援 Linux 與 Windows。我們保留了可以方
便撰寫 Extension 的框架，期待更多使用者能夠參與開發。 
透過前述定址的方式，任何在資料格網上的檔案將會照著邏輯位置(Logical 
Location)來服務，也就是在虛擬儲存空間上的位置。而其虛擬的儲存位置，僅會以
簡單的雜湊來區隔檔案，而儲存在使用者或是系統指定的目錄裡。雜湊的目的，只
需要確認一個目錄的檔案個數上限不會超過檔案系統所能承受的即可。 
我們針對現行的檔案系統如 NTFS 及 EXT3，透過 Windows 或是 Unix 都會有的
原生 IO 指令來使用。並且設計儲存驅動的程式介面，讓任何人都可以透過撰寫外掛
的方式，來驅動自己所需要的儲存設備，如特殊的階層式檔案系統。此外，針對常
見的備份式儲存，如磁帶，以及可移除式儲存如 USB 碟與光碟，也都有支援。這些
非傳統的儲存介面帶來的挑戰是，利用檔案快取來讓使用者還是能夠存取到。也因
此後面會有提到利用 Cluster Peer(CP)及 SP 在近端網路建立大量磁碟快取(Disk Pool)
的作法，圖五所示。而另外一點令人關切的問題是，不同 Domain 的資料，或是不同
使用者擁有的資料，是不會能夠被其他人看見。 
針對很多學術計畫所需要的大量硬碟空間，透過上述部署機制，減少很多不必
要的管理時間。然而，同性質的機器，除了能夠當作儲存，使用其 CPU 的資源也是
相當重要。舉例如果有個計畫的成果會產生大量的感測器資料，不僅容量大，而且
 12 
與 Globus 整合後，所有節點間的通訊都必頇經過 GSI SSL，免不了加重了 CPU
負擔，也因此上述 CP 與 SP 的硬體要求就相當重要。然而好處是，除了確保計畫資料
絕對可以儲存在透過憑證授權的機器，資料格網的資料也都可以透過原先 Globus 支援
的方式來傳送。例如在沒有 Globus 的情況下，我們是使用未加密的 HTTP 傳送檔案，
而使用了 Globus 之後，任何檔案傳送就變成使用 GridFTP。而叢集指令也會使用
globus-job-run 的方式處理。對於原本在 Globus 上的應用方式，例如執行平行程式之
前，原本都是使用 GridFTP 每個節點來散佈，在我們的設計架構下，也可以呼叫資料
格網 API，或是叢集指令來進行快速的散佈。 
 
 
M
ap
p
in
g
R
ed
u
cin
g
Grouped SP Metadata
CP 
Metadata  
圖六 MapReduce 資料平行處理框架 
 
 
對於資料格網管理者而言，資料格網往往都具備一個重要的元件稱為虛擬檔案
系統，有了這個元件，任何使用者可以以管理單台電腦同樣的觀念來管理資料格網
理的檔案。以往針對虛擬檔案系統的管理方式，一直都是相當繁重而浪費時間的，
絕大部分都是因為浪費在使用者介面與系統通訊來來回回地。SRB 在 3 版後，不管
是 Web 介面，還是視窗介面，都實做出來了，但卻還是很難讓使用者感受到資料
格網是可以管理大量檔案的。問題也是在於操作順暢度，以及系統針對中介資料如
何處理。透過前述的中介資料快取，解決了很多順暢度的問題，因此要達成如檔案
總管般拖拉檔案相當地簡單。基本的檔案管理動作，如複製，更名，移動，會直接
對應到資料格網的 API。而比較複雜的動作，像是讓檔案在節點間移動，我們也會
採取較視覺化的作法。而另一些基本元素的管理，如使用者，也是將其模擬成如同
檔案般的觀念，使得管理者可以以同樣的觀念處理所有的工作。 
 14 
越大。但如果想要系統進行權限檢查時更快速，就必頇捨棄很多驗證的項目。而我
們的格網系統，主要是根據 Unix 的檔案權限系統，只有提供「擁有者」，「群組」，
「其他」等的權限等級，而驗證功能為「讀取 Metadata」，「完全讀取」，「寫入
Metadata」，「完全寫入」等，由於我們的群組功能支援子群組，也因此不需要考
慮一個檔案是不是要屬於很多群組這樣的功能。在預設的情況下，檔案是被擁有人
完全控制，而群組是完全讀取，而非擁有者或群組的帳號是無法讀取或寫入。 
在安全性方面，目前的資料格網系統，如 SRB，使用的是自行撰寫的認證系統。
而任何單一簽入的動作都是傳遞給中央控管的 MCAT 伺服器，這可以想像，每個
檔案都需要進行如此繁雜的動作，需要多少的負載。另一方面，SRB也支援與Globus 
GSI 整合，GSI 的方式幾乎是透過憑證，加上與本機的使用者進行對應。但 GSI 的
缺點，便是安裝，申請憑證的程序需要透過人工。如果要很快速地讓使用的儲存節
點增加，是相當地困難。 
而我們的格網系統採取上述優點的部分，並利用了 HTTP 協定，分做兩種情況。
一般的情況下，任何連線要進行認證前，會採取 SSL 進行通訊，使用的是該節點
安裝的時候便會產生的主機憑證。而實際進行 SSL 連線的時候，考量到該 SP 可能
會換 IP，便不會進行憑證驗證。SSL 連線中途，便向該 SP 傳送帳號密碼，而此 SP
會再透過 MP 進行認證。認證完畢，會傳回由 MP 產生的 Session Key。如此只要
連接在同一個 MP 下的 SP，便不需要重新認證。當 Client 確定離線後，Session Key
便會被清除。圖七展示了上述的認證過程。 
這個機制的優點是：第一，認證的單位，也就是想要單一簽入的範圍可以擴充。
要進行認證的帳號，可以在 MP 上設定將使用者中介資料移往更上層的 MP，這樣
因為發 session key 的單位為更上層，因此單一簽入的範圍就可以跨許多 MP。另一
點是，採取加密的連線，只會發生在進行認證連線的時候，因此對大多數的使用者，
儘管沒那樣安全，卻降低不必要的 CPU 負載。 
 
 16 
 
 
圖八 資料格網經濟模型架構圖 
 
對於最快完成與最小成本的方法，演算法的設計其實並不難。當使用者限制時
間內要完成檔案下載，或限制成本完成檔案下載，我們暫時考慮採用以下的方法。 
限制時間內要完成檔案下載 - 先計算出每個 Replica 在限制時間內最多可下載
的檔案大小 Capabilityi，再依照成本來排序，由最小成本開始選擇 Replica，直到整
個檔案大小可在所選的 Replica 下載完成，則所選的 Replica 即是可完成限制時間
內最小成本可完成下載檔案的 Replica 組合。 
限制成本完成檔案下載 -我們所提出的演算法是將限制的成本完全使用以求最
快的時間能夠完成下載檔案，演算法的概念是假設 Client 從每個 Replica 下載不同
大小的部份檔案，可以使下載的成本剛好等於限制的成本，因每個 Replica 下載的
部份檔案大小都不一樣，每個 replica 都會有一個變數。為了演算法複雜度，我們
將每個 Replica 產生一個分數，而成本愈低分數會愈高，所以演算法會先排序，讓
成本愈低的 Replica 得到愈高的分數，而這些分數會都會乘上一個變數，我們只要
調整此乘冪變數即可讓成本等於限制成本，再進行最佳化的部份。 
上述的劇情，基本上可以導入不同的應用領域。我們將提出若干個最佳化的演
算法，作為服務式計算的核心。並且，實作此一經濟模型與使用者介面，提供適應
於各種使用者需求的 Web 服務。 
 
四、結論與討論 
 
我們以 W3C 的 Web 相關技術，及現有的 P2P 演算法為基礎，研究資料格網的
中介資料，虛擬儲存，大量部署管理等核心技術的開發。而與現有的資料格網中介
 18 
 完成資料自我感測複寫機制技術與實作 
 完成格網經濟模型，並開發使用者介面，提供 Web 服務 
 發表 4 篇 SCI 國際期刊 
 
 Ching-Hsien Hsu, Hai Jin and Franck Cappello, “Peer-to-Peer Grid Technologies”, Future 
Generation Computer Systems (FGCS), Vol. 26, No. 5, pp. 701-703, 2010. 
 Ching-Hsien Hsu, Yun-Chiu Ching, Laurence T. Yang and Frode Eika Sandnes, “An Efficient Peer 
Collaboration Strategy for Optimizing P2P Services in BitTorrent-Like File Sharing Networks”, 
Journal of Internet Technology (JIT), Vol. 11, Issue 1, January 2010, pp. 79-88. (SCIE, EI) 
 Ching-Hsien Hsu and Shih Chang Chen, “A Two-Level Scheduling Strategy for Optimizing 
Communications of Data Parallel Programs in Clusters”, Accepted, International Journal of 
Ad-Hoc and Ubiquitous Computing (IJAHUC), 2010. (SCIE, EI, IF=0.66) 
 Ching-Hsien Hsu and Bing-Ru Tsai, “Scheduling for Atomic Broadcast Operation in 
Heterogeneous Networks with One Port Model,” The Journal of Supercomputing (TJS), Springer, 
Vol. 50, Issue 3, pp. 269-288, December 2009. (SCI, EI, IF=0.615) 
 
 發表 6 篇國際研討會論文 
 
 Ching-Hsien Hsu, Alfredo Cuzzocrea and Shih-Chang Chen, "CAD: Efficient Transmission 
Schemes across Clouds for Data-Intensive Scientific Applications", Proceedings of the 4th 
International Conference on Data Management in Grid and P2P Systems, LNCS, Toulouse, 
France, August 29-September 2, 2011.  
 Tai-Lung Chen, Ching-Hsien Hsu and Shih-Chang Chen, “Scheduling of Job Combination and 
Dispatching Strategy for Grid and Cloud System,” Proceedings of the 5th International Grid and 
Pervasive Computing (GPC 2010), LNCS 6104, pp. 612-621, 2010. 
 Shih-Chang Chen, Tai-Lung Chen and Ching-Hsien Hsu, “Message Clustering Techniques 
towards Efficient Communication Scheduling in Clusters and Grids,” Proceedings of the 10th 
International Conference on Algorithms and Architectures for Parallel Processing (ICA3PP 2010), 
LNCS 6081, pp. 283-291, 2010. 
 Shih-Chang Chen, Ching-Hsien Hsu, Tai-Lung Chen, Kun-Ming Yu, Hsi-Ya Chang and Chih-Hsun 
Chou, “A Compound Scheduling Strategy for Irregular Array Redistribution in Cluster Based 
Parallel System,” Proceedings of the 2nd Russia-Taiwan Symposium on Methods and Tools for 
Parallel Programming (MTPP 2010), LNCS 6083, 2010. 
 Ching-Hsien Hsuand Tai-Lung Chen, “Adaptive Scheduling based on Quality of Services in 
Heterogeneous Environments”, IEEE Proceedings of the 4th International Conference on 
Multimedia and Ubiquitous Engineering (MUE), Cebu, Philippines, Aug. 2010.  
 Ching-Hsien Hsu, Yen-Jun Chen, Kuan-Ching Li, Hsi-Ya Chang and Shuen-Tai Wang, "Power 
Consumption Optimization of MPI Programs on Multi-Core Clusters" Proceedings of the 4th ICST 
International Conference on Scalable Information Systems (InfoScale 2009), Hong Kong, June, 
2009, Lecture Notes of the Institute for Computer Science, Social Informatics and 
Telecommunications Engineering, (ISBN: 978-3-642-10484-8) Vol. 18, pp. 108-120, (DOI: 
10.1007/978-3-642-10485-5_8) (EI) 
 
五、計畫成果自評 
 
本計畫完成了大量中介資料處理機制，以及 CP 的儲存緩衝區與大量部署機
制。此外，我們也在多所大學進行部署、分析資料格網運作紀錄，建立資料運作模
型。本計畫相關論文產出共計發表四篇期刊論文與六篇研討會論文。期刊論文部
 20 
System for the SRB Data Grid,” Proceedings. 20th IEEE/11th NASA Goddard Conference on 
Mass Storage Systems and Technologies, (MSST 2003), 2003. 
[16] [16] Mike Burrows “The Chubby lock service for loosely-coupled distributed systems,” 7th 
USENIX Symposium on Operating Systems Design and Implementation (OSDI), 2006. 
[17] [17] N. Santos and B. Koblitz “Distributed Metadata with the AMGA Metadata Catalog” 
Workshop on Next-Generation Distributed Data Management 
[18] [18] N. Santos and B. Koblitz “Metadata Services on the Grid,” Proceedings of the X 
International Workshop on Advanced Computing and Analysis Techniques in Physics Research. 
[19] [19] Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung “The Google File System,” 
Proceedings of the 19th ACM Symposium on Operating Systems Principles, 2003, pp. 20-43. 
[20] [20] Tim Oreilly “What is Web 2.0: Design Patterns and Business Models for the Next 
Generation of Software,” Communications & Strategies, No. 1, p. 17, First Quarter 2007 
[21] [21] Ching-Hsien Hsu, Yun-Chiu Ching, Laurence T. Yang and Frode Eika Sandnes, “An Efficient Peer 
Collaboration Strategy for Optimizing P2P Services in BitTorrent-Like File Sharing Networks”, Journal of 
Internet Technology (JIT), Vol. 11, Issue 1, January 2010, pp. 79-88.  
[22] [22] Ching-Hsien Hsu and Shih Chang Chen, “A Two-Level Scheduling Strategy for Optimizing 
Communications of Data Parallel Programs in Clusters”, Accepted, International Journal of Ad-Hoc and 
Ubiquitous Computing (IJAHUC), 2010.  
[23] [23] Ching-Hsien Hsu and Bing-Ru Tsai, “Scheduling for Atomic Broadcast Operation in Heterogeneous 
Networks with One Port Model,” The Journal of Supercomputing (TJS), Springer, Vol. 50, Issue 3, pp. 
269-288, December 2009.  
[24] [24] Tai-Lung Chen, Ching-Hsien Hsu and Shih-Chang Chen, “Scheduling of Job Combination and 
Dispatching Strategy for Grid and Cloud System,” Proceedings of the 5th International Grid and Pervasive 
Computing (GPC 2010), LNCS 6104, pp. 612-621, 2010.  
[25] [25] Shih-Chang Chen, Tai-Lung Chen and Ching-Hsien Hsu, “Message Clustering Techniques towards 
Efficient Communication Scheduling in Clusters and Grids,” Proceedings of the 10th International 
Conference on Algorithms and Architectures for Parallel Processing (ICA3PP 2010), LNCS 6081, pp. 
283-291, 2010.  
[26] [26] Shih-Chang Chen, Ching-Hsien Hsu, Tai-Lung Chen, Kun-Ming Yu, Hsi-Ya Chang and Chih-Hsun Chou, 
“A Compound Scheduling Strategy for Irregular Array Redistribution in Cluster Based Parallel System,” 
Proceedings of the 2nd Russia-Taiwan Symposium on Methods and Tools for Parallel Programming (MTPP 
2010), LNCS 6083, 2010.  
 22 
2010/05/18 (上午) 
9:00  發表論文 
11:00 聽取 System Algorithm 相關論文發表 
 
(下午) 
2:00 聽取 Numerical simulation 相關論文發表 
4:00 參訪 Far East National University 
(晚上) 
7:00 參加晚宴 
2010/05/19 (上午) 
9:00 聽取 Simulation 相關論文發表 
 
 
 MTPP-10 是台俄雙邊在平行計算研究領域主要的研討會。這一次參與 MTPP-10 ，本人
擔任會議議程主席，除了發表相關研究成果以外，也在會場與多位國外教授交換研究心得，
並且討論未來可能的合作。 
 這一次參與 MTPP-10 除了發表我們最新的研究成果以外，也在會場中，向多位國內外
學者解釋我們的研究內容，彼此交換研究心得。除了讓別的團隊知道我們的研究方向與成
果，藉此，我們也學習他人的研究經驗。經過兩次的雙邊研討會交流，雙方已經找到共同研
究的題目，兩邊的團隊也將於今年(2010年)8月開始撰寫研究計畫書，進行更密切的合作。 
 這一次在 Vladivostok, Russia 所舉行的國際學術研討會議議程共計四天。開幕當天
由俄羅斯方面的 General Co-Chair，RSA 的 Victor E. Malyshkin 教授，與敝人分別致詞
歡迎大家參加這次的第二屆 MTPP 2010 國際研討會。接著全程參與整個會議的流程，也聽
取不同論文發表，休息時間與俄羅斯的學者教授交換意見和資訊。本人發表的論文在會議第
三天的議程九點三十分發表（A Compound Scheduling Strategy for Irregular Array 
Redistribution in Cluster Based Parallel System）。本人主要聽取 Parallel and 
Distributed 、Grid、Cloud 與 Multicore 相關研究，同時獲悉許多新興起的研究主題，
並了解目前國外學者主要的研究方向。最後一天，我們把握機會與國外的教授認識，希望能
夠讓他們加深對台灣研究的印象。這是一次非常成功的學術研討會。 
 主辦第一、二屆台俄雙邊學術研討會，感受良多。論文篇數從第一屆的 30 篇到第二屆
的 50篇，也讓本人感受到這個研討會的進步成長。台方參與的教授學生超過 15 個學研單位，
包括台大、清華、交大、中研院、成大、中山、等等。俄方也有超過 10 個學研單位的參與。
值得一提的是，這一次的論文集我們爭取到 Springer LNCS 的出版，並且在 EI 索引。這一
個研討會與發表的論文，其影響力已達到國際的水準。 
 
 24 
A Compound Scheduling Strategy for Irregular Array Redistribution in Cluster 
Based Parallel System 
Shih-Chang Chen
1
, Ching-Hsien Hsu
2
, Tai-Lung Chen
1
, Kun-Ming Yu
2
,  
Hsi-Ya Chang
3
 and Chih-Hsun Chou
2* 
 
1 College of Engineering  
2 Department of Computer Science and Information Engineering 
Chung Hua University, Hsinchu, Taiwan 300, R.O.C. 
3 National Center for High-Performance Computing, Hsinchu 30076, Taiwan 
{scc, robert, tai}@grid.chu.edu.tw, yu@chu.edu.tw, jerry@nchc.org.tw, chc@chu.edu.tw 
Abstract. With the advancement of network and techniques of clusters, joining clusters to 
construct a wide parallel system becomes a trend.  Irregular array redistribution employs 
generalized blocks to help utilize the resource while executing scientific application on such 
platforms.  Research for irregular array redistribution is focused on scheduling heuristics 
because communication cost could be saved if this operation follows an efficient schedule.  In 
this paper, a two-step communication cost modification (T2CM) and a synchronization 
delay-aware scheduling heuristic (SDSH) are proposed to normalize the communication cost and 
reduce transmission delay in algorithm level.  The performance evaluations show the 
contributions of proposed method for irregular array redistribution. 
1   Introduction 
Scientific application executing on parallel systems with multiple phases requires appropriate data 
distribution schemes.  Each scheme describes the data quantity for every node in each phase.  Therefore, 
performing data redistribution operations among nodes help enhance the data locality. 
Generally, data redistribution is classified into regular and irregular redistributions.  BLOCK, CYCLIC 
and BLOCK-CYCLIC(c) are used to specify array decomposition for the former while user-defined function, 
such as GEN_BLOCK, is used to specify array decomposition for the latter.  High Performance Fortran 
version 2 provides GEN_BLOCK directive to facilitate the data redistribution for user-defined function.  To 
perform array redistribution efficiently, it is important to follow a schedule with low communication cost. 
With the advancement of network and the popularizing of cluster computing research in campus, it is a 
trend to join clusters in different regions to construct a complex parallel system.  To performing array 
redistribution on this platform, new techniques are required instead of existing methods.   
Schedules illustrate time steps for data segments (messages) to be transmitted in appropriate time.  The 
cost of schedules given by scheduling heuristics is the summation of cost of every time steps while cost of 
each time step is dominated by the message with largest cost.  A phenomenon is observed that most local 
transmissions, which are happened in a node, do not dominate the cost of each step although they are in 
algorithm level for existing methods.  In other words, they are overestimated.  Since a node can send and 
receive only one message in the same time step [5], the arranged position of each message becomes 
important.  Therefore, a two-step communication cost modification (T2CM) and a synchronization 
delay-aware scheduling heuristic (SDSH) are proposed to deal with the overestimate problems, reduce 
overall communication cost and avoid synchronization of schedules in algorithm level. 
 26 
are given, where the array size is 100 units.  These two strings provide necessary information for nodes to 
generate messages to be transmitted among them.  Fig. 1 shows these messages marked from m1 to m11 and 
are with information such as data size, source node and destination node in the relative rows. 
Scheduling heuristics are developed for providing solutions of time steps to reduce total communication cost 
for a GEN_BLOCK redistribution operation.  In each step, there are several messages which are suggested to 
be transmitted in the same time step.  To help perform an efficient redistribution, scheduling methods 
should avoid node contention, synchronization delay and redundant transmission cost.  It is also important 
to follow policies of messages arrangement, i.e. with the same source nodes, messages should not be in the 
same step; with the same destination nodes, messages should be in different step; a node can only deal with 
one message while playing whether source node or destination node.  These messages that cannot be 
scheduled together called conflict tuples, for example, a conflict tuple is formed with messages m1 and m2.  
Note that if a node can only deal with a message while it is a source/destination node, the number of steps for 
a schedule must be the equal to or more than the number of messages from/to these nodes.  In other words, 
the minimal number of time steps is equal to the maximal number of messages in a conflict tuple, CTmax. 
 
Information of messages 
No. of 
message 
Data 
size 
Source 
node 
Destination 
node 
m1 13 0 0 
m2 3 1 0 
m3 17 1 1 
m4 1 2 1 
m5 13 2 2 
m6 3 2 3 
m7 13 3 3 
m8 4 3 4 
m9 12 4 4 
m10 13 5 4 
m11 8 5 5 
  
Fig. 1. Information of messages generated from given schemes to be transmitted on six nodes which are indexed from 0 to 5 
Fig. 2 gives a schedule with low communication cost and arranges messages in the number of minimal steps.  
In this result, there are three time steps with messages sent/received to/from different nodes.  The values 
beside m1~11 are data size, the cost of each step is dominated by the largest one.  Thus, m3, m1 and m8 
dominate step 1, 2 and 3, and the estimated cost are 17, 13 and 4, respectively.  To avoid node contentions, 
messages m1 and m2 are in separate steps due to destination nodes of both messages are the same.  Based on 
same argument, m2 and m3 are in separate steps due to both messages are members of a conflict tuple.  The 
total cost which represents the performance of a schedule is the summation of all cost of steps.  In other 
words, a schedule with lower cost is better than another one with higher cost in terms of performance. 
 28 
Fig. 4, they should be 2.125, 1.625, 1.625 and 13, respectively.  Node 1, 2 and 3 must wait for node 4 and 5 
to proceed next step because when the transmissions of m3, m5 and m7 are finished, the transmission of m10 is 
still on the way. 
 
Information of messages 
No. of 
message 
Data 
size 
Source 
node 
Destination 
node 
m1 1.625 0 0 
m2 3 1 0 
m3 2.125 1 1 
m4 1 2 1 
m5 1.625 2 2 
m6 15 2 3 
m7 1.625 3 3 
m8 4 3 4 
m9 1.5 4 4 
m10 13 5 4 
m11 1 5 5 
  
Fig. 3. The local reduction and inter amplification operations derive new data size for messages m1~11 
 
A result of scheduling heuristics 
No. of  
step 
No. of  
message 
Cost of  
step 
Step 1 m3(2.125), m5(1.625), m7(1.625), m10(13) 13 
Step 2 m1(1.625), m6(15), m9(1.5), m11(1) 15 
Step 3 m2(3), m4(1), m8(4) 4 
Total cost 32 
  
Fig. 4. The results with new dominators and cost 
The proposed synchronization delay-aware scheduling heuristic is a novel and efficient method to avoid 
delay among clusters and shorten communication cost while performing GEN_BLOCK redistribution.  To 
avoid synchronization delay, the transmissions happened in local memory are scheduled together in one 
single step instead of separating them among time steps like the results in Fig. 4.  Other messages are 
pre-proceeded by inter amplification and then scheduled by a low cost scheduling method which selects 
messages with smaller cost to shorten the cost of a step and avoid the node contentions.  Fig. 5 shows the 
results of SDSH which is with low synchronization delay and is contention free.  There are two reasons 
making the results in Fig. 5 better than the results in Fig. 4.  First, SDSH successfully avoids 
synchronization delay by congregating m1, m3, m5, m7, m9 and m11 in step 3.  It also helps reduce the cost of 
a step.  Second, messages m6 and m10 are the most important transmissions in the schedule due to their 
communication cost can dominate any steps.  It is a pity that they are separated in two steps in Fig. 4 due to 
the node contentions.  For example, it is impossible to move m6 to step 1 to shorten the cost of step 2 due to 
 30 
 
Results of evaluations 
Num. of nodes SDSH TPDR Same 
8 813 76 111 
16 946 43 11 
32 950 48 2 
64 914 79 7 
128 903 96 1 
Percentage 90.52% 6.84% 2.64% 
Total 4526 342 132 
 
Fig. 6. The results of both methods on five sets of nodes with 5,000 cases in total 
The attributes of generated cases dependents on the number of nodes, for example, higher CTmax and lower 
communication cost are with higher number of nodes.  It is hard to find the same schedules for two 
scheduling heuristics with larger number of nodes.  Fig. 7 shows the information of cases which are used to 
evaluate the SDSH and TPDR. 
 
Attributes of given cases 
Num. of 
nodes 
 
CTmax 
Average 
CTmax 
Cost of 1,000 cases 
SDSH TPDR 
8 6 3.271 6733580 7953932 
16 8 3.762 5733523 6983753 
32 10 4.246 3564076 4354899 
64 10 4.661 2412444 2781670 
128 11 5.009 1282008 1520884 
 
Fig. 7. Attributes of given cases for five set of nodes 
CTmax of results with 128 nodes is 11 which is almost two times larger than the CTmax of results with 8 nodes.  
The average CTmax also grows with higher number of nodes.  The total cost of schedules given by both 
methods for 1000 cases with different number of nodes explains the contribution of SDSH in Fig. 6.  The 
proposed method provides better schedules and the improves the communication cost about 15% while 
comparing to TPDR.  It also explains how SDSH outperforms its competitor.  Overall speaking, SDSH is a 
novel, efficient and simple method to provide solutions for scheduling communications of GEN_BLOCK 
redistribution. 
6   Conclusions 
To perform GEN_BLOCK redistribution efficiently, research focused on developing scheduling heuristic to 
shorten communication cost in algorithm level.  In this paper, a two-step communication cost modification 
(T2CM) and a synchronization delay-aware scheduling heuristic (SDSH) are proposed to normalize the 
transmission cost and reduce synchronization delay.  The two-step communication cost modification 
 32 
出席國際學術會議心得報告 
 
計 畫 名 稱 應用 P2P 與 Web 技術發展以 SOA 為基礎的格網中介軟體與經濟模型 
計 畫 編 號 NSC 97-2628-E-216-006-MY3 
報 告 人 姓 名 許慶賢 
服 務 機 構 
及 職 稱 
中華大學資訊工程學系教授 
會 議 名 稱 
The 12
th
 IEEE International Conference on Computational Science and 
Engineering (CSE-09) 
會議 /訪問時間地點 Vancouver, Canada / 2009.08.29-31 
發 表 論 文 題 目 
Data Distribution Methods for Communication Localization in Multi-Clusters with 
Heterogeneous Network 
 
參加會議經過 
 
會議時間 行程敘述 
2009/08/29 (上午) 
8:00 會場報到、聆聽 Keynote Speech 
     Privacy, Security, Risk and Trust in Service-Oriented 
Environments by Stephen S. Yau 
9:00 發表論文 
10:30 聽取 Parallel Algorithm 相關論文發表 
 
(下午) 
1:00 聆聽 Keynote Speech 
     Elections with Practical Privacy and Transparent Integrity by David  
Chaum  
2:00 聽取 Grid Computing相關論文發表 
3:30 主持 Session 
 
(晚上) 
7:30 參加歡迎茶會 
 34 
Computing Applications and Trends、White Space Networking - Is it Wi-Fi on Steroids?、
Computational Science and Engineering in Emerging Cyber-Ecosystems 五個不同的題目給予精
闢的演講。本人在這次研討會擔任兩個場次的 Chair 並發表了一篇論文，論文題目為 Data 
Distribution Methods for Communication Localization in Multi-Clusters with Heterogeneous 
Network ，擔任 Chair 分別為 CSE-09 (Session A16) 和 SEC-09(Session A27)，本人參與了三
天全程會議，也選擇了一些相關領域之場次來聆聽論文發表。 
 36 
communications of data parallel program on cluster grid.  The key idea is that of distributing data to 
grid/cluster nodes according to a mapping function at data distribution phase initially instead of in 
numerical-ascending order. 
In this paper, we consider the issue of real communication cost among a number of geographically grid 
nodes which belong to different clusters.  Method proposed previously has less communication cost by 
reordering logic id of processors.  Base on this idea, two new processor reorder techniques are proposed to 
adapt the heterogeneous network environment.  
This paper is organized as follows.  Section 2 presents related work.  In section 3, we provide 
background and review previously proposed processor reorder techniques.  In section 4, the Global 
Reordering technique is proposed for processor reordering in section 4.1.  The Divide and Conquer 
Reordering technique is proposed in section 4.2.  In section 5, we present the results of the evaluation of the 
new schemes.  Finally we have the conclusions and future work in section 6. 
2. Related Work 
Research work on computing grid have been broadly discussed on different aspects, such as security, fault 
tolerance, resource management [4, 6], job scheduling [1, 20, 21, 22], and communication optimizations [2].  
Commutating grid is characterized by a large number of interactive data exchanges among multiple 
distributed clusters over a network.  Thus, providing a reliable response in reasonable time with limited 
communication and computation resources for reducing the interactive data exchanges is required.  Jong 
Sik Lee [16] presented a design and development of a data distribution management modeling in 
computational grid. 
For the issue of communication optimizations, Dawson et al. [2] addressed the problems of optimizations 
of user-level communication patterns in the local space domain for cluster-based parallel computing.  Plaat 
et al. analyzed the behavior of different applications on wide-area multi-clusters [3, 19].  Similar research 
works were studied in the past years over traditional supercomputing architectures [11, 14].  Guo et al. [7] 
eliminated node contention in communication step and reduced communication steps with the schedule table.  
Y. W. Lim et al. [18] presented an efficient algorithm for Block-Cyclic data realignments.  Jih-Woei Huang 
and Chih-Ping Chu [8] presented a unified approach to construct optimal communication schedules for the 
processor mapping technique applying Block-Cyclic redistribution.  The proposed method is founded on 
the processor mapping technique and can more efficiently construct the required communication schedules 
than other optimal scheduling methods.  A processor mapping technique presented by Kalns and Ni [15] 
can minimize the total amount of communicating data.  Namely, the mapping technique minimizes the size 
of data that need to be transmitted between two algorithm phases.  Lee et al. [17] proposed similar method 
to reduce data communication cost by reordering the logical processors‟ id.  They proposed four algorithms 
for logical processor reordering.  They also compared the four reordering algorithms under various 
conditions of communication patterns.  There is significant improvement of the above research for parallel 
applications on distributed memory multi-computers.  However, most techniques are applicable for 
applications running on local space domain, like single cluster or parallel machine.  Ching-Hsien Hsu et al. 
 38 
table.  Messages {a1, a2, a3}, {e1, e2, e3} and {i1, i2, i3} are presented interior communications (|I| = 9); all 
the others are external communications (|E| = 18). 
 
(a) 
 
 
(b) 
 
Figure 1. Communication tables of data reallocation over the cluster grid. (a) Without data mapping. (b) With data mapping. 
The idea of changing logical processor mapping [15, 16] is employed to minimize data transmission time 
of runtime array redistribution in the previous research works.  In the cluster grid, we can derive a mapping 
function to produce a realigned sequence of logical processors‟ id for grouping communications into the 
local cluster.  Given an identical cluster grid with C clusters, a new logical id for replacing processor Pi can 
be determined by New(Pi) = (i mod C) * K + (i / C), where K is the degree of data refinement.  Figure 1(b) 
shows the communication table of the same example after applying the above reordering scheme.  The 
source data is distributed according to the reordered sequence of processors‟ id, i.e., <P0, P3, P6, P1, P4, P7, 
P2, P5, P8> which is computed by mapping function.  Therefore, we have |I| = 27 and |E| = 0. 
For the case of K (degree of refinement) is not equal to n (the number of grid nodes in each cluster), the 
mapping function becomes impracticable.  In this subsection, the previous work proposes a grid node 
replacement algorithm for optimizing distribution localities of data reallocation.  According to the relative 
position of the first of consecutive sub-blocks that produced by each processor, we can determine the best 
target cluster as candidate for node replacement.  Combining with a load balance policy among clusters, 
this algorithm can effectively improve data localities.  Figure 2 gives an example of data reallocation on the 
cluster grid, which has four clusters.  Each cluster provides three processors.  The degree of data 
refinement is set to four (K = 4).  Figure 2(a) demonstrates an original reallocation communication patterns.  
We observe that |I| = 12 and |E| = 36. 
 40 
algorithm, the derived sequence of logical grid nodes is <P2, P5, P9, P3, P6, P10, P4, P11, P0, P7, P12, P1, P8, 
P13>.  Figure 3(b) gives the communication tables when applying data to logical grid nodes mapping 
technique.  This data to grid nodes mapping produces 46 interior communications and 24 external 
communications.  This result reflects the effectiveness of the node replacement algorithm in term of 
minimizing inter-cluster communication overheads. 
 
(a) 
 
 
(b) 
 
Figure 3. Communication tables of data reallocation on non-identical cluster grid. (a) Without data mapping. 
(b) With data mapping. 
3.3 Communication Cost of Multi-Clusters with Heterogeneous Network 
Examples in the above section do not consider the real communication status for multi-clusters over 
heterogeneous network communication.  Figure 4(a) shows an example of four clusters with various 
inter-cluster communication costs.  Each unit‟s block data must spend 20 units time from the cluster-1 
transmission to cluster-2, but each unit‟s block data must spend 30 units time from the cluster-1 transmission 
to cluster-3.  Figure 4(b) shows the table of inter-cluster communication costs.  Therefore, we can 
calculate communication cost of data distribution for each processor over inter-cluster by this 
communication matrix.  After calculating, the communication cost are 1865 and 885 according to 
 42 
 
 
Figure 5. The total communication cost of grid model (C = 4, K = 5, < G (4): {2, 3, 4, 5}> ) 
4. Processor Mapping Methods 
According to communication cost, a candidate processor‟s id can be chosen according to minimum 
distribution cost.  Therefore, the first processor mapping method is proposed called Processor Mapping 
using Global Reordering (GR).  Another method rests on the cluster base, after all data redistribution costs 
of one cluster are arranged in an order, choosing a candidate processor‟s id according to the number of 
processor of its cluster  This method is called Processor Mapping using Divide and Conquer Reordering 
(DCR). 
4.1 Global Reordering Algorithm 
We propose a processor mapping scheme which requires the communication information of inter-cluster.  
First, the minimum cost is selected using Greedy algorithm.  This algorithm, Processor Mapping using 
Global Reordering (GR), is without the complex logic procedures of operation.  To achieve the result of 
processor mapping that has the least communication cost, the key idea is to choose the minimum 
communication cost from global candidates.  The transmission rate between each site over the internet is 
different because of the various network devices.  The cluster can easily measure the transmission rate by 
the present technology and keep it in each cluster.  The system can obtain transmission rates and produce an 
n*n cost matrix.  The combination of communication costs can be calculated using the cost matrix and data 
redistribution pattern.  According to the costs, the data block with minimum cost can be chosen to be 
assigned a processor id first.  In the choice process, two kinds of situations occur possibly.  To assign a 
processor id to a data block for distributing:  (1) the data block with the chosen minimum cost would be 
ignored if this data block has already been assigned to another candidate (processor id) previously.  (2) if no 
more processor id can be offered from the selected cluster, the selecting process will continue to find the 
 44 
P11, P0, P7 P12, P4, P8, P13 >.  Accordingly, the necessary communication cost is 740 units. 
 
According to the method described above, the code of algorithm is shown as follows: 
 
For P=0 to n-1 
    Determine how many cost matrix t on every cluster  
EndFor 
For t=1 to C  
    Order by cost from t 
EndFor 
While (Replacement s not complete) 
    For P=0 to n-1 
        If not (two or more cluster have the candidate) 
            select the target cluster processor id P that has the minimum 
cost 
        EndIf 
    EndFor 
    reorder the remaining cost list from t 
    select the target cluster processor id P by GR Algorithm 
EndWhile 
 
Figure 7. Processor Mapping using Divide and Conquer Reordering Algorithm. 
5. Performance Evaluation 
In this section, proposed techniques and methods without considering actual communication cost are 
implemented to simulate with different communication cost matrixes.  The network bandwidth is different 
from 10Mb to 1Gb for heterogeneous network environment.  Since 10Mb network equipments are almost 
eliminated, the value of transmission ratio is set from 10 to 30.  The value is randomly produced to simulate 
patterns of communication cost matrix.  The variance is set from 150 to 450 in simulations representing of 
network heterogeneity.  The larger number of variance represents the larger network heterogeneity.  
Besides, C is set from 8 to 16, K is set from 16 to 64 for simulations.  100 difference communication cost 
matrix patterns are used to calculate communication costs for each variance case and average of the costs is 
the results of the theoretical value.  The following figures show the results of each method. 
Figure 8 shows the results on a grid consisted of 8 cluster, <G(8):{4, 4, 4, 6, 6, 6, 8, 8}> and K is equal to 
16.  Figure 8 illustrates the comparing results of four different methods.  Original one does not consider 
the actual cost of reordering communications technology, GR is Processor Mapping using Global Reordering 
technology, DCR for the Processor Mapping using Divide and Conquer Reordering technology.  Obviously, 
GR and DCR have less communication cost compared with the other two models.  When the difference in 
the number of 150, GR and DCR can reduce about 33% cost compared with the traditional one which is 
without processor reordering.  Both of them also reduce 6% communications cost while comparing with the 
Original one.  While the variance is 450, the improvement slightly increases about 33% ~ 36%. 
 46 
REFERENCES 
[13] O. Beaumont, A. Legrand and Y. Robert, ”Optimal algorithms for scheduling divisible workloads on heterogeneous 
systems,” Proceedings of the 12
th
 IEEE Heterogeneous Computing Workshop, 2003.  
[14] J. Dawson and P. Strazdins, “Optimizing User-Level Communication Patterns on the Fujitsu AP3000,” Proceedings of the 
1st IEEE International Workshop on Cluster Computing, pp. 105-111, 1999. 
[15] Henri E. Bal, Aske Plaat, Mirjam G. Bakker, Peter Dozy, and Rutger F.H. Hofman, “Optimizing Parallel Applications for 
Wide-Area Clusters,” Proceedings of the 12th International Parallel Processing Symposium IPPS'98, pp 784-790, 1998. 
[16] M. Faerman, A. Birnbaum, H. Casanova and F. Berman, “Resource Allocation for Steerable Parallel Parameter Searches,” 
Proceedings of GRID’02, 2002. 
[17] I. Foster and C. Kessclman, “The Grid: Blueprint for a New Computing Infrastructure,” Morgan Kaufmann, ISBN 
1-55860-475-8, 1999. 
[18] James Frey, Todd Tannenbaum, M. Livny, I. Foster and S. Tuccke, “Condor-G: A Computation Management Agent for 
Multi-Institutional Grids,” Journal of Cluster Computing, vol. 5, pp. 237 – 246, 2002. 
[19] M. Guo and I. Nakata, “A Framework for Efficient Data Redistribution on Distributed Memory Multicomputers,” The 
Journal of Supercomputing, vol.20, no.3, pp. 243-265, 2001. 
[20] Jih-Woei Huang and Chih-Ping Chu, “An Efficient Communication Scheduling Method for the Processor Mapping 
Technique Applied Data Redistribution,” The Journal of Supercomputing, vol. 37, no. 3, pp. 297-318, 2006 
[21] Ching-Hsien Hsu, Guan-Hao Lin, Kuan-Ching Li and Chao-Tung Yang, “Localization Techniques for Cluster-Based Data 
Grid,” Proceedings of the 6th ICA3PP, Melbourne, Australia, 2005 
[22] Ching-Hsien Hsu, Tzu-Tai Lo and Kun-Ming Yu “Localized Communications of Data Parallel Programs on Multi-cluster 
Grid Systems,” European Grid Conference, LNCS 3470, pp. 900 – 910, 2005. 
[23] Florin Isaila and Walter F. Tichy, “Mapping Functions and Data Redistribution for Parallel Files,” Proceedings of IPDPS 
2002 Workshop on Parallel and Distributed Scientific and Engineering Computing with Applications, Fort Lauderdale, 
April 2002. 
[24] Bahman Javadi, Mohammad K. Akbari and Jemal H. Abawajy, "Performance Analysis of Heterogeneous Multi-Cluster 
Systems,"  Proceedings of ICPP, 2005 
[25] Bahman Javadi, J.H. Abawajy and Mohammad K. Akbari “Performance Analysis of Interconnection Networks for 
Multi-cluster Systems” Proceedings of the 6th ICCS, LNCS 3516, pp. 205 – 212, 2005. 
[26] Jens Koonp and Eduard Mehofer, “Distribution assignment placement: Effective optimization of redistribution costs,” IEEE 
TPDS, vol. 13, no. 6, June 2002. 
[27] E. T. Kalns and L. M. Ni, “Processor mapping techniques toward efficient data redistribution,” IEEE TPDS, vol. 6, no. 12, 
pp. 1234-1247, 1995. 
[28] Jong Sik Lee, “Data Distribution Management Modeling and Implementation on Computational Grid,” Proceedings of the 
4th GCC, Beijing, China, 2005. 
[29] Saeri Lee, Hyun-Gyoo Yook, Mi-Soon Koo and Myong-Soon Park, “Processor reordering algorithms toward efficient 
GEN_BLOCK redistribution,” Proceedings of the 2001 ACM symposium on Applied computing, 2001. 
[30] Y. W. Lim, P. B. Bhat and V. K. Parsanna, “Efficient algorithm for block-cyclic redistribution of arrays,” Algorithmica, vol. 
24, no. 3-4, pp. 298-330, 1999. 
[31] Aske Plaat, Henri E. Bal, and Rutger F.H. Hofman, “Sensitivity of Parallel Applications to Large Differences in Bandwidth 
and Latency in Two-Layer Interconnects,” Proceedings of the 5th IEEE High Performance Computer Architecture HPCA'99, 
pp. 244-253, 1999. 
[32] Xiao Qin and Hong Jiang, “Dynamic, Reliability-driven Scheduling of Parallel Real-time Jobs in Heterogeneous Systems,” 
Proceedings of the 30th ICPP, Valencia, Spain, 2001. 
[33] S. Ranaweera and Dharma P. Agrawal, “Scheduling of Periodic Time Critical Applications for Pipelined Execution on 
Heterogeneous Systems,” Proceedings of the 30th ICPP, Valencia, Spain, 2001. 
[34] D.P. Spooner, S.A. Jarvis, J. Caoy, S. Saini and G.R. Nudd, “Local Grid Scheduling Techniques using Performance 
Prediction,” IEE Proc. Computers and Digital Techniques, 150(2): 87-96, 2003. 
 48 
2009/06/11 (上午) 
9:00 聆聽Keynote Speech 
Autonomic Cloud Systems Management: Challenge and  
Opportunities 
Cheng-Zhong Xu, Wayne State University, USA 
10:30聽取 Information Security相關論文發表 
(下午) 
1:30聽取 Parallel and Distributed Computing相關論文發表 
3:30聽取 RFID / Sensor Network 相關論文發表 
 
這一次在香港所舉行的國際學術研討會議共計兩天。第一天上午由 Dr. Xian-He Sun (Illinois 
Institute of Technology, China) 針對 The current Multi-core architecture and memory-wall 
problem，作為此次研討會的開始，下午由 Dr. Minglu Li （Shanghai Jiao Tong University, 
China）針對 The application of mobile communication technology 給予專題演講。下午接著是
一個場次進行。本人聽取 session 3 的相關論文發表，也擔任主持第一天 session 3 的論文發
表。晚上在大會的地點與幾位國外學者及中國、香港教授交換心得意見。第二天，專題演
講是由 Dr. Cheng-Zhong Xu (Central Michigan University, USA) 針對 “Embedded Software 
Development with MDA”發表演說。本人也參與的第二天全部的大會議程，這天由
Cheng-Zhong Xu （Wayne State University, USA）。這一天，也發表了這一次的論文。本人
主要聽取 Multi-Core 等相關研究，同時獲悉許多新興起的研究主題，並了解目前國外大多
數學者主要的研究方向，並且把握最後一天的機會與國外的教授認識，希望能夠讓他們加
深對台灣研究的印象。二天下來，本人聽了許多優秀的論文發表。這些研究所涵蓋的主題
包含有：無線網路技術、網路安全、Multi-Core、資料庫以及普及運算等等熱門的研究課題。
此次的國際學術研討會議有許多知名學者的參與，讓每一位參加這個會議的人士都能夠得
到國際上最新的技術與資訊。是一次非常成功的學術研討會。 
 
 50 
1. Introduction 
Reduction on power consumption of computer systems is a hot issue recently, since many CPUs and 
computer-related hardware has been produced and under operation everywhere. As the number of single-core 
CPU has reached to physical limitation on current semi-conductor technology, the computing performance 
has met the bottleneck. Multi-core CPUs become a simple yet efficient solution to increase performance and 
speed since that concept SMP in a single chip, that is, making up a small cluster to be executed inside a host. 
Additionally, it reduces the amount of context switching while in single-core CPUs, increases straight 
forwardly the overall performance. Some CPU technologies and our target will be introduced in below.  
 
 
Figure 1: Intel Quad-Core CPU system structure [11] 
 
Figure 1 illustrates the architecture of Intel quad-core CPU, which looks like a combination of two 
dual-core CPUs. It has four individual execution engines, where each two cores share one set of L2 cache 
and system bus interface, and connect to the fixed system bus. The advantages of this architecture are 
twofold. The former one is that each core can fully utilize L2 cache as each core needs larger memory, while 
the latter is that each core accesses L2 cache through individual hub [7] simplifying system bus and cache 
memory structures. Intel CPU provides “SpeedStep” [3] technology that helps to control CPU frequency and 
voltage, and it needs to change all cores‟ frequency at the same. 
 52 
 
Figure 3: Multi-core based cluster structure [13] 
 
Developed from 1999, InfiniBand [16] is a point-to-point structure, original design concept that focused 
on high-performance computing support, so bidirectional serial fiber interface, failover mechanism and 
scalable ability are the necessary functions. InfiniBand supports at least 2.5Gbit/s bandwidth in each 
direction in single data rate (SDR) mode, the transmitted information includes 2 Gbit useful data and 
500Mbit control commands. Besides, InfiniBand supports DDR (Double Data Rate) and QDR (Quad Data 
Rate) transmission mode, and each mode supports 3 different speed (1x, 4x and 12x) configurations, so the 
maximum bandwidth is 96Gbit/s. The detail specification is as Table 1. 
 
Table 2: Infiniband transmission mode list 
 Single (SDR) Double (DDR) Quad (QDR) 
1X 2 Gbit/s 4 Gbit/s 8 Gbit/s 
4X 8 Gbit/s 16 Gbit/s 32 Gbit/s 
12X 24 Gbit/s 48 Gbit/s 96 Gbit/s 
 
Infiniband networking technology is a good and fast enough solution to connect all computing nodes of a 
cluster platform, but expensive. Gigabit Ethernet is cheaper solution and widely built in general network 
environment, though slower in transmission speed and definitely drop down data exchange performance. To 
 54 
Lower loading core is indicated higher priority to receive data frame, the method increases core 
working efficiency. 
2. Related Work 
Based on the concept about reducing computing time, the job scheduling methodology as introduced in [8] 
and [21] was designed targeting for a faster complete data transmission; otherwise, adjust cache block size to 
find the fastest speed that transmits data using MPI between MPI nodes in situations as listed in [13] was 
studied, and similar implementation of the method using OpenMP was also observed in [14]. Another 
investigation focused on compiler that analyze program‟s semantics, and insert special hardware control 
command that automatically adjusts simulation board‟s working frequency and voltage, [10] research needs 
to be combined both hardware and software resources. 
Some kinds of paper designed their methodologies or solutions under simulation board, or called NoC 
system, as shown in its structure is as below: 
 
 
Figure 4: General NoC system structure 
 
 56 
operation method, the approach applied in enterprise-scale commercial deployments and saved 74% power 
consumption. 
Besides, some researchers studied OS resource management and power consumption evaluation and task 
scheduling method as [23] and [25], this kind of study provides a direction to optimize computer operation 
tuning and reduces system idle time that brings by resource waiting. 
3. Challenges of Power Saving in Multi-Core Based Cluster 
In the previous single-core CPU based cluster environment, data distribution with CPU energy control are 
easier to implement by isolated CPU frequency control of each host. In multi-core based cluster, CPU 
internal bus architecture, bandwidth and power control structure bring differnet challanges in this issue. 
When we built a cluster platform that combines some key technologies as listed in Chapter 1 for experiment 
purposes, their advantages bring higher speed for data transmission peformance, yet only between cores 
inside a CPU, a CPU core is maintained with high load means the CPU speed cannot be decreased. Analysis 
and reasoning on these situations are discussed next. 
The “SpeedStep” and “PowerNow!” were not show in Figure 1 and 2. The “SpeedStep” provides solely 
full CPU frequency and voltage adjustment. The design makes power control easier, though consumes extra 
energy. If only one core works with high load, power control mechanism cannot reduce other cores‟ 
frequency / voltage, nor dropping down the performance of a busy core. Inefficient energy consumption 
brings temperature increasing, since low loading core generates the same heat as high load one, and brings 
the CPU‟s temperature up at the same time. 
AMD “PowerNow!” shows advantage in this issue, since we can reduce frequency when core works in 
lower loading without need to consider other cores‟ situation, and heat reduction is also another benefit. 
 58 
in core reduces data flow interference. Different CPU structure provides their advantages, and weakness 
appears while they are compared to each other. 
In a general situation, each computing node executed under a given core / host randomly indicated by 
cluster software, signifies that programmer cannot obtain additional core loading from node's code section. 
Following our purpose, finding system information about thread / node location works, but it is a hard 
method since the program would spend large amount of time in device I/O, includes open system state file, 
analysis information and obtaining node‟s location. Another alternative method is easier, where we make 
cluster platform that fixes node location in indicated core or host, and the function helps to get core loading 
from node‟s code. OpenMPI is selected for this issue. 
4. The Proposed Approach 
Upon with CPU specification, CPU power control interface and network structure, we provide a novel 
data dispatching strategy to solve the previous challanges in Chapter 3, it combines data flow limitation, core 
frequency controlling, and accords core working load to transmit data frame, detail operation is as below. 
It is not a good method to keep performance. In fact, we add 1µs delay between two packets, in a real 
environment, and the total transmission time is added as: 
T = N × D (2) 
where T is total time, N is total number of packets and D is delay time between packets. We found that the 
total time has just been added less than one to four seconds in average, when is transmitted 100K data frames 
across two hosts that are connected via Gigabit Ethernet. Additionally, the advantage is that the loading of a 
central node that sends data to other nodes is decreased by almost 50%. On the other hand, data receiving 
core load is decreased by 15% in average when we added 10µs delay in these nodes, follow Function 2, the 
amoung of increased delay time should be 1s, yet in experiment result, total transmission time is increased by 
 60 
 
Figure 5: LAD Algorithm structure diagram 
 
Still in LAD algorithm, as indicated in Figure 4, data frames are sent sequentially from Host 1-Core 0 to 
other cores. This method is often used to distribute wait-for-calculate data blocks in complex math parallel 
calculations. MPI provides broadcast command to distribute data and reduce command to receive result. In 
order to changing data frame transmission path dynamically, we use point-to-point command to switch data, 
since this type of command can indicate sending and receiving node. 
The detail of operation flow is as below: 
 Step 1: Detect core loading 
 Step 2: Find lowest loading core 
 Step 3: Send several data frames to the lowest loading core 
 Step 4: Repeat previous two step until all data frames are transmitted over 
The data distribution algorithm is given as below. 
Loading-Aware Dispatching (LAD)Algorithm 
1. generating wait-for-send data frame 
2. if (node 0) 
3. { 
4.    //send data follow sorting result 
5.    while(!DataSendingFinish) 
6.    { 
7.       //detect nodes‟ loading from system information and save in TargetNode 
8.       OpenCPUState; 
9.       CalculateCPULoading; 
 62 
 
Figure 6: Test environment 
 
Data frame size 
Three different sizes of data frames are transmitted between nodes: one byte, 1460 bytes and 8000 bytes. 
One byte frame is not only the smallest one in MPI data frame, but also in network, for complete data 
transmission in shortest time, source node generates huge amount of one byte frame, these packets congest 
CPU internal bus and network. 
1518 bytes frame is the largest one in network, but considering that network header should be inserted 
into network packet, we select 1460 bytes frame for testing, and then, this size of packet brings largest 
amount of data in a single network packet, and trigger fewest network driver interrupt to CPU. Finally 8000 
bytes frame is set for large data frame testing, since it needs to be separated to several other packets by 
network driver for transmission, but not necessary to be separated in intra-node, and thus need the longest 
time for data transmission. 
While the experiment is executed, we send 100K data frames between two nodes, and calculate the power 
consumption. 
CPU frequency and packet delay 
Each experiment result figures and tables that follows next has four blocks. The first one is executed in 
Performance Mode (PM, CPU works in 2.3GHz), the second one is PowerSave Mode (PS, 1.15GHz), the 
 64 
        Rank 
Number 
 
Mode & TD 
2 3 4 5 6 7 8 
PM 
mode 
0 0.160 0.333 0.501 6.262 10.646 15.072 18.630 
5 0.928 1.152 1.286 6.813 11.276 15.867 19.384 
10 2.292 2.271 1.775 6.655 11.076 15.419 19.238 
20 3.251 3.229 3.216 6.924 11.083 15.603 19.032 
PS 
mode 
0 0.285 0.576 0.909 9.984 16.537 23.151 28.976 
5 1.326 1.689 1.935 10.599 17.311 23.679 29.518 
10 2.637 2.174 2.429 10.580 17.848 24.157 29.598 
20 3.612 6.651 3.850 10.767 17.470 24.165 29.651 
OD 
mode 
0 0.216 0.372 0.531 6.625 11.388 16.025 18.664 
5 1.330 1.625 1.824 7.143 11.973 16.863 19.503 
10 2.630 2.126 2.256 6.898 11.693 16.456 19.456 
20 3.489 3.683 3.756 7.343 11.723 16.615 19.161 
LAD 
0 0.288 0.577 0.918 7.182 12.018 16.699 19.524 
5 1.367 1.423 1.623 8.716 14.587 20.181 20.704 
10 2.659 1.960 2.028 8.718 14.508 20.221 21.355 
20 3.598 3.813 3.716 9.254 15.129 20.253 22.943 
 
 
Figure 7: Time effect of TD on TT (Frame = 1 Byte) 
 66 
 
Figure 8: Power effect of TD on PC (Frame = one Byte) 
 
1460 byte frame 
Table 5 and Figure 7 show 1460 bytes frame TT. By comparing PM mode and OD mode, the completed 
time is longer than one byte frame in all situations. In Figure 8, OD mode uses in average over 200J less than 
PM mode. Our LAD algorithm made uses of 24~25s to complete data transmission as OD mode, yet 
consumes less than OD mode 200~600J in 8 ranks. In other situations, LAD keeps nearly the same 
performance, spending 3s longer than OD mode and consuming 200~600J less than OD mode. 
 
 68 
Figure 9: Time effect of TD on TT (Frame = 1460 Byte) 
 
Table 7: Detail results of power effect of TD on PC (Frame = 1460 Byte) 
       Rank 
Number 
 
Mode & TD 
2 3 4 5 6 7 8 
PM 
mode 
0 56.039 83.345 114.460 1423.847 1971.859 2781.018 4014.203 
5 158.117 188.597 209.711 1855.335 2082.032 2908.813 3872.596 
10 393.864 359.891 288.611 1557.516 2046.631 2818.166 3803.698 
20 528.644 520.865 515.150 2227.449 2025.676 2831.342 3891.170 
PS 
mode 
0 44.339 65.188 89.536 813.317 1351.245 1851.616 2266.093 
5 103.387 130.305 143.086 756.769 1383.661 1886.888 2315.502 
10 210.416 160.793 174.359 863.940 1413.934 1895.313 2342.063 
20 264.751 267.679 281.387 848.946 1385.517 1902.167 2343.848 
OD 
mode 
0 33.586 39.127 52.693 945.259 1645.667 2710.100 3839.306 
5 110.451 132.799 158.958 1032.840 1849.698 2663.291 3900.304 
10 223.043 182.830 195.905 1049.544 1827.601 2729.659 3861.452 
20 306.473 306.226 309.360 1022.164 1827.937 2739.376 3834.217 
LAD 
0 44.982 87.644 123.505 1028.737 1705.181 2431.432 3650.212 
5 104.575 118.019 124.578 1044.754 1715.120 2455.331 3608.556 
10 235.515 153.219 171.224 976.402 1847.987 2431.883 3525.145 
20 308.037 307.738 290.581 890.359 1654.873 2355.386 3209.088 
 
 70 
PS 
mode 
0 2.333 2.619 2.812 16.480 27.429 34.139 38.755 
5 2.240 2.684 2.964 16.716 27.728 35.219 39.884 
10 2.774 3.088 3.245 17.336 19.803 35.387 41.127 
20 4.685 4.678 4.244 16.700 27.613 35.219 39.930 
OD 
mode 
0 1.274 1.464 1.610 10.648 22.022 27.752 31.210 
5 2.045 2.407 2.226 14.377 21.778 27.739 34.744 
10 2.546 2.810 2.769 13.856 22.079 27.932 34.379 
20 4.338 4.380 4.448 14.037 21.957 27.603 34.878 
LAD 
0 1.917 2.125 2.200 12.242 23.169 28.553 33.991 
5 2.234 2.399 2.579 13.487 22.152 29.627 34.864 
10 2.672 2.779 2.740 13.018 24.838 30.845 34.518 
20 4.456 4.663 4.163 12.754 22.897 29.118 36.666 
 
 
 
Figure 11: Time effect of TD on TT (Frame = 8000 Byte)
 72 
 
Figure 12: Power effect of TD on PC (Frame = 8000 Byte) 
 
Remarks 
In this proposed research, LAD algorithm keeps in average 4s TT increasing, yet saves 200~600J that 
compares with OD mode in cross-node situation. Limited by only 2 steps experimental cases of CPU 
frequencies (2.3GHz and 1.15GHz), we cannot keep CPU loading in a smooth curve. In desktop and server 
CPU, they do not keep in high loading work longer time, while they complete a concurrent job and next one 
does not be started. Power saving technology helps to decrease host energy consumption, and decreasing 
energy cost and carbon dioxide emissions can be reduced. 
6. Conclusions 
One byte data frame is the smallest one, and it has 5 seconds transmission time shorter than 1460 bytes 
frame and 14 seconds shorter than 8000 bytes frame in cross node situation. That means two kinds of 
application which have no huge data need to be transmitted are suitable to use small data frame. 
 Mathematical calculation 
 74 
6. “AMD Family 10h Desktop Processor Power and Thermal Data Sheet”, http://www.amd.com/us-en
/assets/content_type/white_papers_and_tech_docs/GH_43375_10h_DT_PTDS_PUB_3.14.pdf 
7. “AMD Opteron Processor with Direct Connect Architecture”, http://enterprise.amd.com/downloads/4
P_Power_PID_4149 8.pdf 
8. Chao-Yang Lan, Ching-Hsien Hsu and Shih-Chang Chen, “Scheduling Contention-Free Irregular R
edistributions in Parallelizing Compilers”, The Journal of Supercomputing, Volume 40, Issue 3, (J
une 2007), Pages: 229-247 
9. Dongkun Shin and Jihong Kim, “Power-Aware Communication Optimization for Networks-on-Chi
ps with Voltage Scalable Links”, Proceeding of the International Conference on Hardware/Softwar
e Codesign and System Synthesis, 2004, Pages: 170-175 
10. Guangyu Chen, Feihui Li and Mahmut Kandemir, “Reducing Energy Consumption of On-Chip 
Networks Through a Hybrid Compiler-Runtime Approach”, 16th International Conference on Par
allel Architecture and Compilation Techniques (PA CT 2007), Pages: 163-174 
11. “Intel 64 And IA-32 Architectures Software Developers Manual, Volume 1”, http://download.in t
el.com/design/processor/manuals/253665.pdf 
12. “Key Architectural Features of AMD Phenom X4 Quad-Core Processors”, http://www.amd.com/u
s-en/Processors/ProductInformation/0,,30_118_15331_15332%5E15334,00.html 
13. Lei Chia, Albert Hartono, Dhabaleswar K. Panda, “Designing High Performance and Scalable M
PI Inter-node Communication Support for Clusters”, 2006 IEEE International Conference on Clu
ster Computing, 25-28 Sept. 2006, Pages: 1-10 
14. Ranjit Noronha and D.K. Panda, “Improving Scalability of OpenMP Applications on Multi-core 
Systems Using Large Page Support”, 2007 IEEE International Parallel and Distributed Processin
g Symposium, 26-30 March 2007, Pages: 1-8 
15. Umit Y. Ogras, Radu Marculescu, Hyung Gyu Lee and Na Ehyuck Chang, “Communication Ar
chitecture Optimization: Making the Shortest Path Shorter in Regular Networks-on-Chip”, 2006 
Proceedings of the conference on Design, Automation and Test in Europe, Munich, Germany, 
March 2006, Volume 1, Pages: 712-717  
 76 
25. Radha Guha, Nader Bagherzadeh and Pai Chou, “Resource Management and Task Partitioning a
nd Scheduling on a Run-Time Reconfigurable Embedded System”, Computers and Electrical En
gineering, March 2009, Volume 35, Issue 2, Pages: 258-285 
 78 
 
Towards Improving QoS-Guided Scheduling in Grids
 
 
Ching-Hsien Hsu
1
, Justin Zhan
2
, Wai-Chi Fang
3
 and Jianhua Ma
4 
 
1
Department of Computer Science and Information Engineering, Chung Hua University, Taiwan 
chh@chu.edu.tw 
2
Heinz School, Carnegie Mellon University, USA 
justinzh@andrew.cmu.edu 
3
Department of Electronics Engineering, National Chiao Tung University, Taiwan 
wfang@mail.nctu.edu.tw 
4
Digital Media Department, Hosei University, Japan 
jianhua@hosei.ac.jp 
 
 
Abstract 
 
With the emergence of grid technologies, the 
problem of scheduling tasks in heterogeneous systems has 
been arousing attention. In this paper, we present two 
optimization schemes, Makespan Optimization 
Rescheduling (MOR) and Resource Optimization 
Rescheduling (ROR), which are based on the QoS 
Min-Min scheduling technique, for reducing the 
makespan of a schedule and the need of total resource 
amount. The main idea of the proposed techniques is to 
reduce overall execution time without increasing 
resource need; or reduce resource need without 
increasing overall execution time. To evaluate the 
effectiveness of the proposed techniques, we have 
implemented both techniques along with the QoS 
Min-Min scheduling algorithm. The experimental results 
show that the MOR and ROR optimization schemes 
provide noticeable improvements.  
 
1. Introduction 
 
With the emergence of IT technologies, the need of 
computing and storage are rapidly increased.  To invest 
more and more equipments is not an economic method for 
an organization to satisfy the even growing computational 
and storage need. As a result, grid has become a widely 
accepted paradigm for high performance computing.   
To realize the concept virtual organization, in [13], 
the grid is also defined as “A type of parallel and 
distributed system that enables the sharing, selection, and 
aggregation of geographically distributed autonomous and 
heterogeneous resources dynamically at runtime 
depending on their availability, capability, performance, 
cost, and users' quality-of-service requirements”.  As the 
grid system aims to satisfy users‟ requirements with limit 
resources, scheduling grid resources plays an important 
factor to improve the overall performance of a grid.   
In general, grid scheduling can be classified in two 
categories: the performance guided schedulers and the 
economy guided schedulers [16]. Objective of the 
performance guided scheduling is to minimize turnaround 
time (or makespan) of grid applications. On the other 
hand, in economy guided scheduling, to minimize the cost 
of resource is the main objective.  However, both of the 
scheduling problems are NP-complete, which has also 
instigated many heuristic solutions [1, 6, 10, 14] to 
resolve. As mentioned in [23], a complete grid scheduling 
framework comprises application model, resource model, 
performance model, and scheduling policy. The 
scheduling policy can further decomposed into three 
phases, the resource discovery and selection phase, the 
job scheduling phase and the job monitoring and 
migration phase, where the second phase is the focus of 
this study.  
Although many research works have been devoted 
in scheduling grid applications on heterogeneous system, 
to deal with QOS scheduling in grid is quite complicated 
due to more constrain factors in job scheduling, such as 
the need of large storage, big size memory, specific I/O 
devices or real-time services, requested by the tasks to be 
completed. In this paper, we present two QoS based 
rescheduling schemes aim to improve the makespan of 
scheduling batch jobs in grid.  In addition, based on the 
QoS guided scheduling scheme, the proposed 
rescheduling technique can also reduce the amount of 
resource need without increasing the makespan of grid 
jobs.  The main contribution of this work are twofold, 
one can shorten the turnaround time of grid applications 
without increasing the need of grid resources; the other 
one can minimize the need of grid resources without 
increasing the turnaround time of grid applications, 
 80 
 A chunk of tasks will be scheduled to run completion 
based on all available machines in a batch system. 
 A task will be executed from the beginning to 
completion without interrupt. 
 The completion time of task ti to be executed on 
machine mj is defined as  
 
CTij = dtij + etij              (1) 
 
Where etij denotes the estimated execution time of task ti 
executed on machine mj; dtij is the delay time of task ti on 
machine mj.   
 
The Min-Min algorithm is shown in Figure 1. 
 
Algorithm_Min-Min() 
{ 
while there are jobs to schedule 
for all job i to schedule 
for all machine j 
Compute CTi,j = CT(job i, machine j) 
end for 
Compute minimum CTi,j 
end for 
Select best metric match m 
Compute minimum CTm,n 
Schedule job m on machine n 
end while 
} End_of_ Min-Min  
 
Figure 1. The Min-Min Algorithm 
 
Analysis: If there are m jobs to be scheduled in n 
machines, the time complexity of Min-Min algorithm is 
O(m
2
n). The Min-Min algorithm does not take into 
account the QoS issue in the scheduling.  In some 
situation, it is possible that normal tasks occupied 
machine that has special services (referred as QoS 
machine).  This may increase the delay of QoS tasks or 
result idle of normal machines. 
 
The QoS guided scheduling is proposed to resolve 
the above defect in the Min-Min algorithm.  In QoS 
guided model, the scheduling is divided into two classes, 
the QoS class and the non-QoS class.  In each class, the 
Min-Min algorithm is employed.  As the QoS tasks have 
higher priority than normal tasks in QoS guided 
scheduling, the QoS tasks are prior to be allocated on 
QoS machines.  The normal tasks are then scheduled to 
all machines in Min-Min manner.  Figure 2 outlines the 
method of QoS guided scheduling model with the 
Min-Min scheme.   
Analysis: If there are m jobs to be scheduled in n 
machines, the time complexity of QoS guided scheduling 
algorithm is O(m
2
n).  
Figure 3 shows an example demonstrating the 
Min-Min and QoS Min-Min scheduling schemes.  The 
asterisk * means that tasks/machines with QoS 
demand/ability, and the X means that QoS tasks couldn‟t 
be executed on that machine.  Obviously, the QoS 
guided scheduling algorithm gets the better performance 
than the Min-Min algorithm in term of makespan.  
Nevertheless, the QoS guided model is not optimal in 
both makespan and resource cost. We will describe the 
rescheduling optimization in next section. 
 
Algorithm_QOS-Min-Min() 
{ 
for all tasks ti in meta-task Mv (in an arbitrary order) 
for all hosts mj (in a fixed arbitrary order) 
       CTij = etij + dtj 
end for 
end for 
do until all tasks with QoS request in Mv are mapped 
for each task with high QoS in Mv,  
find a host in the QoS qualified host set that obtains 
the earliest completion time 
end for 
find task tk with the minimum earliest completion time 
assign task tk to host ml that gives the earliest completion 
time 
delete task tk from Mv 
update dtl 
update CTil for all i 
end do 
do until all tasks with non-QoS request in Mv are mapped 
for each task in Mv 
find the earliest completion time and the 
corresponding host 
       end for 
find the task tk with the minimum earliest completion time 
assign task tk to host ml that gives the earliest completion 
time 
delete task tk from Mv 
update dtl 
    update CTil for all i 
end do 
} End_of_ QOS-Min-Min 
 
Figure 2. The QoS Guided Algorithm 
4. Rescheduling Optimization 
Grid scheduling works as the mapping of individual 
tasks to computer resources, with respecting service level 
agreements (SLAs) [2].  In order to achieve the 
optimized performance, how to mapping heterogeneous 
tasks to the best fit resource is an important factor.  The 
Min-Min algorithm and the QoS guided method aims at 
scheduling jobs to achieve better makespan.  However, 
there are still having rooms to make improvements.  In 
this section, we present two optimization schemes based 
on the QoS guided Min-Min approach.  
 82 
Algorithm_MOR() 
{ 
for CTj in all machines 
find out the machine with maximum makespan CTmax and 
set it to be the standard 
end for 
do until no job can be rescheduled 
for job i in the found machine with CTmax  
            for all machine j 
  according to the job‟s QOS demand, find the 
adaptive machine j  
if (the execute time of job i in machine j + the 
CTj < makespan) 
           rescheduling the job i to machine j   
           update the CTj and CTmax 
       exit for 
end if 
            next for 
            if the job i can be reschedule 
find out the new machine with maximum CTmax 
            exit for 
end if 
next for 
end do  
} End_of_ MOR  
Figure 5. The MOR Algorithm 
4.2 Resource Optimization Rescheduling (ROR) 
Following the assumptions described in MOR, the main 
idea of the ROR scheme is to re-dispatch tasks from the 
machine with minimum number of tasks to other machines, 
expecting a decrease of resource need.  Consequently, if 
we can dispatch all tasks from machine Mx to other 
machines, the total amount of resource need will be 
decreased.  
Figure 6 gives another example of QoS scheduling, 
where the QoS guided scheduling presents makespan = 13. 
According to the clarification of ROR, machine „M1‟ has 
the fewest amount of tasks.  We can dispatch the task 
„T4‟ to machine „M3‟ with the following constraint 
 
CTij + CTj <= CTmax             (2) 
 
The above constraint means that the rescheduling can be 
performed only if the movement of tasks does not 
increase the overall makespan.  In this example, CT43 = 2, 
CT3=7 and CTmax=CT2=13.  Because the makespan of 
M3 (CT3) will be increased from 7 to 9, which is smaller 
than the CTmax, therefore, the task migration can be 
performed.  As the only task in M1 is moved to M3, the 
amount of resource need is also decreased comparing with 
the QoS guided scheduling.   
  
 M1 *M2 
T1 3 4 
T2 6 6 
*T3 X 7 
T4 4 6 
B. The Resource Optimization Rescheduling 
(ROR) Algorithm 
M3 
2 
3 
X 
2 
T5 5 7 2 
*T6 X 6 X 
Machine 
0 
M1 *M2 
*T6 
T1 
4 
8 
13 
M3 
*T3 
T5 
T2 
Makespan 
Machine 
T4 
M1 
A. The QOS guided scheduling 
algorithm 
0 
*M2 
T4 
*T6 
T1 
4 
8 
13 
M3 
*T3 
T5 
T2 
Makespan 
Figure 6. Example of ROR 
 
The ROR is an optimization scheme which aims to 
minimize resource cost. If there are m tasks to be 
scheduled in n machines, the time complexity of ROR is 
also O(m
2
n).  Figure 7 depicts a high level description of 
the ROR optimization scheme. 
 
Algorithm_MOR() 
{ 
for m in all machines 
        find out the machine m with minimum count of jobs 
end for 
do until no job can be rescheduled 
for job i in the found machine with minimum count of jobs 
            for all machine j 
according to the job‟s QOS demand, find the 
adaptive machine j  
if (the execute time of job i in machine j + the 
CTj <= makespan CTmax) 
           rescheduling the job i to machine j   
           update the CTj  
           update the count of jobs in machine m and 
machine j  
       exit for 
end if 
            next for         
next for 
end do 
} End_of_ MOR 
 
Figure 7. The ROR Algorithm  
5. Performance Evaluation 
5.1 Parameters and Metrics 
 
To evaluate the performance of the proposed 
techniques, we have implemented the Min-Min 
scheduling algorithm and the QoS guided Min-Min 
 84 
 
HQ 3 5 7 9 11 
Min-Min 1392.4 1553.9 1724.9 1871.7 2037.8 
QOS Guided Min-Min 867.5 1007.8 1148.2 1273.2 1423.1 
MOR 822.4 936.2 1056.7 1174.3 1316.7 
Improved Ratio 5.20% 7.11% 7.97% 7.77% 7.48% 
  
5.3 Experimental Results of ROR 
Table 3 analyzes the effectiveness of the ROR technique 
under different circumstances.   
 
Table 3: Comparison of Resource Used 
 
(a) (NR=100, QR=30%, QT=20%, HT=1, HQ=1) 
 
Task Number (NT) 200 300 400 500 600 
QOS Guided Min-Min 100 100 100 100 100 
ROR 39.81 44.18 46.97 49.59 51.17 
Improved Ratio 60.19% 55.82% 53.03% 50.41% 48.83% 
 
(b) (NT=500, QR=30%, QT=20%, HT=1, HQ=1) 
 
Resource Number (NR) 50 70 90 110 130 
QOS Guided Min-Min 50 70 90 110 130 
ROR 26.04 35.21 43.65 50.79 58.15 
Improved Ratio 47.92% 49.70% 51.50% 53.83% 55.27% 
 
(c) (NT=500, NR=50, QT=20%, HT=1, HQ=1) 
 
QR% 15% 30% 45% 60% 75% 
QOS Guided Min-Min 50 50 50 50 50 
ROR 14.61 25.94 35.12 40.18 46.5 
Improved Ratio 70.78% 48.12% 29.76% 19.64% 7.00% 
 
(d) (NT=500, NR=100, QR=40%, HT=1, HQ=1) 
 
QT% 15% 30% 45% 60% 75% 
QOS Guided Min-Min 100 100 100 100 100 
ROR 57.74 52.9 48.54 44.71 41.49 
Improved Ratio 42.26% 47.10% 51.46% 55.29% 58.51% 
 
(e) (NT=500, NR=100, QR=30%, QT=20%, HQ=1) 
 
HT 1 3 5 7 9 
QOS Guided Min-Min 100 100 100 100 100 
ROR 47.86 47.51 47.62 47.61 47.28 
Improved Ratio 52.14% 52.49% 52.38% 52.39% 52.72% 
 
(f) (NT=500, NR=100, QR=30%, QT=20%, HT=1) 
 
HQ 3 5 7 9 11 
QOS Guided Min-Min 100 100 100 100 100 
ROR 54.61 52.01 50.64 48.18 46.53 
Improved Ratio 45.39% 47.99% 49.36% 51.82% 53.47% 
 
 
Similar to those of Table 2, Table (a) changes the 
number of tasks to verify the reduction of resource that 
needs to be achieved by the ROR technique.  We noticed 
that the ROR has significant improvement in minimizing 
grid resources.  Comparing with the QoS guided 
Min-Min scheduling algorithm, the ROR achieves 50% ~ 
60% improvements without increasing overall makespan 
of a chunk of grid tasks.  Table (b) changes the number 
of machines.  The ROR retains 50% improvement ratio.  
Table (c) adjusts percentages of QoS machine.  Because 
this test has 20% QoS tasks, the ROR performs best at 
15% QoS machines.  This observation implies that the 
ROR has significant improvement when QoS tasks and 
QoS machines are with the same percentage.  Table (d) 
sets 40% QoS machine and changes the percentages of 
QoS tasks.  Following the above analysis, the ROR 
technique achieves more than 50% improvements when 
QoS tasks are with 45%, 60% and 75%.  Tables (e) and 
(f) change the heterogeneity of tasks.  Similar to the 
results of section 5.2, the heterogeneity of tasks is not 
critical to the improvement rate of the ROR technique.  
Overall speaking, the ROR technique presents 50% 
improvements in minimizing total resource need compare 
with the QoS guided Min-Min scheduling algorithm. 
 
6. Conclusions 
In this paper we have presented two optimization 
schemes aiming to reduce the overall completion time 
(makespan) of a chunk of grid tasks and minimize the 
total resource cost.  The proposed techniques are based 
on the QoS guided Min-Min scheduling algorithm. The 
optimization achieved by this work is twofold; firstly, 
without increasing resource costs, the overall task 
execution time could be reduced by the MOR scheme with 
7%~15% improvements. Second, without increasing task 
completion time, the overall resource cost could be 
reduced by the ROR scheme with 50% reduction on 
average, which is a significant improvement to the state of 
the art scheduling technique. The proposed MOR and 
ROR techniques have characteristics of low complexity, 
high effectiveness in large-scale grid systems with QoS 
services.  
 
References 
 
[1] A. Abraham, R. Buyya, and B. Nath, "Nature‟s Heuristics for 
Scheduling Jobs on Computational Grids", Proc. 8th IEEE 
International Conference on Advanced Computing and 
Communications (ADCOM-2000), pp.45-52, 2000. 
[2] A. Andrieux, D. Berry, J. Garibaldi, S. Jarvis, J. MacLaren, D. 
Ouelhadj, D. Snelling, "Open Issues in Grid Scheduling", 
National e-Science Centre and the Inter-disciplinary Scheduling 
Network Technical Paper, UKeS-2004-03. 
[3] R. Buyya, D. Abramson, Jonathan Giddy, Heinz Stockinger, 
“Economic Models for Resource Management and Scheduling 
 86 
 
 
97 年度專題研究計畫研究成果彙整表 
計畫主持人：許慶賢 計畫編號：97-2628-E-216-006-MY3 
計畫名稱：利用 P2P 技術建構以服務導向架構(SOA)為基礎之輕量化格網系統--子計畫二:應用 P2P 與
Web 技術發展以 SOA 為基礎的格網中介軟體與經濟模型 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 5 5 100%  
博士生 2 2 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 4 3 100% 所發表之期刊皆為 SCI 等級 
研究報告/技術報告 5 5 100%  
研討會論文 6 6 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
