應用互助式類神經網路整合一類具有環場影像 
與PTZ攝影機雙架構的監視系統 
 
彭守道
1
、蔡佳宏
2
、蔡明宏
2 
1
南台科技大學機械工程學系副教授  
2
南台科技大學機械工程學系研究生  
國科會計畫編號：NSC 962221E218038  
  
                 
摘要  
本 文 旨 在 發 展 一 套 具 有 環 場 影 像 與
PTZ(Pan/Tilt/Room)攝影機雙架構的監視系統。研究
中採用徑向基底函數類神經網路(RBFNN)以及自我
組織映射神經網路(SOM)兩種網路架構，以互助學習
方式建立環場攝影系統與PTZ攝影機間的『影像-角度
映射關係』。系統運作時，RBFNN網路以環場影像
資訊為輸入，關連出內部記憶的PTZ攝影機相關角
度，以驅動PTZ攝影機指向目標物。當目標物影像落
在PTZ攝影機畫面後，緊接著，系統再應用SOM網路
的主動式視覺技術，將目標物影像定位在拍攝畫面中
心附近。執行後的相關結果，分別回授調整RBFNN
與SOM網路中的學習參數，並重覆此互助學習模式。
實驗結果顯示，類神經網路經過充分的學習之後，能
以環場影像資訊引導PTZ攝影機拍攝目標物，準確地
將目標物影像定位在畫面中心鄰域。 
 
關鍵字：環場攝影機，非校正式互助學習系統，自
我組織映射網路，徑向基底函數神經網路。 
  
1. 前言  
 現今裝設監視系統已越來越普遍，常見的例子
有十字路口與巷道等戶外環境的監視，住家與銀行等
室內環境的監視。這類監視系統隨著裝設環境的不
同，攝影機的架設方式也有所差異，比較常見的架設
方式是，將多台攝影機分別架設在各個需要監控的位
置，但這樣的架設方式較無法掌握整個全場動態情
況，只能以數個監視器畫面拼湊後判斷事件。 
 為解決上述的問題，有文獻[1-2]提出全方位視
覺系統的應用。全方位視覺系統最大的優點在於：使
用單一環場攝影機就能監視非常廣闊的空間，將此空
間內的人、事、物拍攝在單一監視畫面中，不需要合
併多台攝影機畫面，就能顯示出此空間內的事件跟幾
何位置。全方位視覺系統在智慧型機器人、自主式航
空飛行器等方面有許多應用[3-4]，一般是以單一環場
攝影機進行拍攝，且為了方便使用者觀察，需將360
度的『極座標型視野』展開並轉成一般的『歐氏空間
視野』[5-10]，但在使用上會有兩個缺點：(1) 大範圍
的監視使環場攝影機畫面解析度不足，監視人員在辨
識單一物體或人物時較為困難，(2) 視野轉換後，其
物體與人物會有所扭曲，隨著環場攝影機鏡面曲度的
不同，扭曲程度也會有所差異，使以辨識外型為主的
影像處理方法難以實現。 
 為解決全方位視覺系統應用上的缺點，近年來
已有文獻在探討『整合移動式攝影機與全方位視覺系
統』的雙攝影機架構方案[11]，在此方案中，由一反
射式360度的環場攝影機對全場環境作全方位的監
視，同時引導移動式攝影機針對目標物作高解析度的
拍攝。文獻[11]中採用人工校正的方式，來建立環場
攝影系統與移動式攝影機間的『影像-角度映射關
係』；藉由環場攝影機中目標物的影像資訊、兩攝影
機架設的位置、以及『大地-影像』間的座標轉換等
資訊，先計算出移動式攝影機所需的角度，接著驅動
攝影機拍攝目標物。但此種校正方式在『非線性的環
場影像幾何光學』中非常的複雜。除此之外，當架設
環境有變動時，就需要重新校正。 
為了避開人工校正時複雜且困難的數學推導，
本研究採用非校正的方式，藉由逐步修正、學習、並
記憶的類神經網路來建立環場攝影系統與移動式攝
影機間的『影像-角度映射關係』。在研究中，採用了
徑向基底函數類神經網路 (Radial Basis Function 
Neural Network, RBFNN)以及自我組織映射神經網路
(Self-Organization Maps, SOM)兩種網路架構，並以互
助學習方式進行學習。系統運作時，RBFNN網路以
環場影像資訊為輸入，關連出內部記憶的PTZ攝影機
相關角度，以驅動PTZ攝影機指向目標物。當目標物
影像落在PTZ攝影機畫面後，緊接著，系統再應用
SOM網路的主動式視覺技術[12-14]，將目標物影像再
次移近拍攝畫面中心附近。執行後的相關結果，分別
回授調整RBFNN與SOM網路中的學習參數，並重覆
此學習模式。實驗結果顯示，互助式的類神經網路經
過充分次數的學習後，能藉由環場攝影資訊正確地引
導PTZ攝影機將目標物影像定位在畫面中心鄰域。 
 本文第二章節會先敘述系統整體的架構與流
程，第三章節介紹移動式攝影系統，第四章節介紹環
標 iv 。 
e. 再計算一次移動攝影機移動角度 f∆θ ： 
( )f s c i∆ = −θ A u v        (2) 
其中 cu 為攝影機畫面中心座標值。接著攝影機移
動 f∆θ 後，擷取目標物新的影像座標 fv 。 
f. 計算修正參數 *s∆θ 與 *sA 如下 
( )* 1 Tolds s s f iδ= + −A A E v v       (3) 
( )* 2old olds s s c iδ∆ =∆ + −θ θ A u v      (4) 
其中 2
1 1 f iδ = −v v ， ( )olds f s f i= ∆ − −E θ A v v ，
2 1δ = 。 
g. 修正每個神經元中所存的內容 
( )new old oldr r rs T rhε= + −p p u p      (5) 
( )*new old oldr r rs s rhε∆ = ∆ + ∆ − ∆θ θ θ θ     (6) 
( )*new old oldr r rs s rhε= + −A A A A      (7) 
其中ε 為修正參數，其特性隨著學習次數的增加
而降低；而 rsh 為權重函數，此函數性質如下： 
1,
0,rs
if r s
h
if r s
=⎧⎪= ⎨ − →∞⎪⎩
      (8) 
h. 完成了所有神經元資料的修正則回到步驟b。 
經上述步驟重複不斷學習後，除了可引導目標點影像
至移動式攝影機畫面中心鄰域外，移動式攝影系統的
『影像-角度映射關係』也能逐漸地建立起來。 
 
4. 環場攝影架構  
 
環場攝影系統採用監督式學習的RBF類神經網路
建立『影像-角度映射關係』，其中，學習用的輸出
與輸入訓練對，分別是 
 
學習用的輸入 = 環場攝影機上目標物的影像座 
標(x, y)。 
學習用的輸出 = 經過SOM網路修正後的移動式 
攝影機的Pan/Tilt的角度值。 
 
如圖4所示，環場攝影機的架構中我們採用內含
數個RBFNN的網路架構(Multi- RBFNN)，當取得目標
物的影像座標之後，系統會先找出相對應的RBFNN
子網路，藉由這個子網路計算出移動式攝影機拍攝目
標物所需的 Pan/Tilt 角度值，接著再以SOM網路繼
續修正攝影機角度，當取得更佳資訊後，修正這組子
網路。 
 
4.1 建立RBFNN與學習步驟 
RBFNN的建立與訓練方式如圖4所示，學習步驟
說明如下: 
a. 建立內含m個RBFNN的子網路。每個RBFNN內
含有各自的神經元數量 A，位移參數 2pj ∈ℜc ，
變異量 ( ) 2 21 2,pj pj pjdiag σ σ ×= ∈ℜσ ，與權重矩陣
2
p
×∈ℜw A；其中 1, ,p m= " ; 1, ,j = " A。 
b. 給定一目標點T，此目標點在環場攝影機所拍攝
的的影像座標為 2k ∈ℜx 。 
c. 經由影像座標 2k ∈ℜx 計算出屬於該輸入的優勝
RBFNN p ，計算輸出角度 2( )p k ∈ℜθ  如下: 
( ) ( ) ( )p p pk k k=θ w u           (9) 
其中 2( )p k ×∈ℜw A 為權重矩陣， ( )p k ∈ℜu A 則定義
為 
1 2( ) [ ( ) ( ) .... ( )]
T
p p p pk u k u k u k=u    A  
( ) ( )21( ) exp , 1, ,
2
T
pj k pj jp k pju k j
−−⎛ ⎞= − − =⎜ ⎟⎝ ⎠x c σ x c    " A
  (10) 
d. 驅動移動式攝影機到式(10)所指定的角度 ( )p kθ  。 
e. 以SOM網路繼續修正攝影機角度，執行3.1節中
的步驟b-步驟g。 
f. 取得SOM網路執行後的角度修正訊號 ( )a k∆θ ： 
( ) ( ) ( )a i fk k k∆ = ∆ + ∆θ θ θ      (11) 
g. 修正優勝RBFNN p 中的權重矩陣如下式所示 
( 1) ( ) ( ) ( )Tp p p a pk k k kη+ = + ∆w w θ u        (12) 
其中 pη 為權重學習速率。 
h. 完成修正後回到步驟b，重新學習。 
經過上述步驟重覆多次學習後，互助式系統在引用環
場攝影機資訊時，便能在單一次驅動攝影機時將目標
物影像移到畫面中心附近。 
 
 
 
 
 
 
 
 
 
 
 
5. 實現方法與結果 
5.1 實驗設備 
如圖5所示，實驗中的環場攝影機是採用360度反
Multi-RBFNN 固定式攝影畫面
 RBFNN
selection
( 1) ( ) ( ) ( )Tp p p pk k k kη+ = +w w e u   
p
kx
Updating rule
目標點影像
1x
2x
1φ
2φ
4φ
∑
∑
1u
2u
4u
w
1θ
2θ3φ 3u
winning
RBFNN
Change  position of  the camera
( ) ( )ak k= ∆e θ
From SOM network
 
圖 4： RBFNN 線上學習流程圖 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
6. 結論 
 
 本文提出一類具有環場影像與移動式攝影機雙
架構的非校正式互助學習的監視系統。此系統藉由一
反射式360度環場攝影機監視全場，大範圍的監視天
花板、兩側牆壁與環場攝影機前方空間，也負責對目
標物的監控，引導移動式攝影機進行高解析度的拍
攝。系統整合了監督與非監督式兩種型式的類神經網
路，其中，環場攝影機採用監督式的RBFNN神經網
路架構，而移動式攝影機則是採用非監督式的SOM
神經網路架構。實驗驗證了環場攝影機的RBFNN網
路能夠學習環場攝影機的『影像-角度映射關係』，
即使在不同平面空間也能確實的引導移動式攝影機
拍攝，而移動式攝影機的SOM網路則能逐步的將目標
物準確的拍攝在畫面中心鄰域(2 pixel範圍內)，且能
不斷的以更準確的『輸出-輸入』訓練對再度訓練此
兩類型的類神經網路，如此便能提高類網路的準確
性，即使在系統有些微變動的狀況下，也能藉由學習
加以修正。 
 
參考文獻 
 
[1] Bonarini, P. Aliverti, and M. Lucioni, “An 
omnidirectionalvision sensor for fast tracking for 
mobile robots,” IEEE Trans. On Instrumentation 
and Measurement, Vol. 49, No. 3, pp. 509-512, 
2000. 
[2] G.Scotti, A. Cuocolo, C.Coelho and 
L.Marchesotti, “A novel pedestrian classification 
algorithm for a high definition dual camera 360 
degrees surveillance system,” IEEE International 
Conference on Image Processing, Vol. 3, Sept, pp. 
III - 880-3, 2005.  
[3] Y. Yagi, S. Kawato, and S. Tsuji, “Real-time 
omnidirectional image sensor (COPIS) for 
vision-guided navigation,” IEEE Trans. on 
Robotics and Automation, Vol. 10, No. 1, pp. 
11-22, 1994. 
[4] Z. Zhu, K. D. Rajasekar, E. Riseman, and A. 
Hanson, “Panoramic virtual stereovision of 
cooperative mobile robots for localizing 3-D 
moving objects,” IEEE Workshop 
Omnidirectional Vision, pp. 29-36, 2000 
[5] G. Scotti, L. Marcenaro, C. Coelho, F. Selvaggi, 
and C.S. Regazzoni, “Dual camera intelligent 
sensor for high definition 360 degrees 
surveillance,” IEE Conf. on Vision, Image and 
Signal Processing, Vol. 152, No. 2, 8 April, pp. 
250-257, 2005. 
[6] K. Yamazawa, Y. Yagi and M. Yachida, 
“Obstacle avoidance with omnidirectional image 
sensor hyperomni vision,” Proc. Of IEEE 
International Conference on Robotics and Automation, 
pp. 1062-1067, May 1995. 
[7] Hong LIU, Wenkai PI, Hongbin ZHA, “Motion 
Detection for Multiple Moving Targets by Using 
an Omnidirectional Camera, ”Proc. IEEE Int. 
 
(a) 環場影像 (b) PTZ 移動前拍攝的畫面
目標物(綠色雷射光點)出現
在環場影像正面的位置 
 
(c) RBF網路引導後的畫面 (d)SOM 網路引導後的畫面
圖9：目標物出現在環場攝影機正面時的追蹤情形。
 
(c) RBF 網路引導後的畫面 (d)SOM 網路引導後的畫面
圖8：目標物出現在左側牆壁時的追蹤情形。
 
(a) 環場影像 (b) PTZ 移動前拍攝的畫面
目標物(綠色雷射光點)出現
在環場影像左側牆壁的位置 
