中文摘要 
本研究描述多重解析度特徵選擇的可行性及其應用於對超音波肝臟影像的分類。 本研
究利用基因演算法，針對醫學應用上診斷正確性是最重要的考慮因素，定義出一合適的適
應函數。並經由計算類別間的分離性，我們可以選擇出唯一的特徵向量。最後經由五種不
同的分類器來測試此特徵向量的效能。從實驗的結果中，我們所提的方法是非常可靠的。 
 
關鍵詞：多重解析度分析、小波轉換、基因演算法 
 
Abstract—This work describes the feasibility of multiresolution feature selection and its 
application to classify ultrasonic liver images. The proposed approach uses genetic algorithm and 
defines a novel fitness function for medical applications since the diagnosis correctness is the 
most important consideration. Via the measurement of class seperability, we can uniquely select 
the feature vector. The effectiveness of the proposed approach is tested by using five different 
classifiers. From the experimental results, the proposed approach is trustworthy. 
Keywords-Multiresolution analysis; wavelet transform; genetic algorithm. 
 
I.  INTRODUCTION  
Ultrasonic imaging technique is one of the most widely used diagnostic tools in modern 
medicine. It is relatively inexpensive compared to other modalities, such as computed tomography 
(CT) or magnetic resonance imaging (MRI). The technology is an effective diagnostic tool to 
visualize soft tissues and organs. 
Among liver diseases, primary malignancy of the liver (hepatocellular carcinoma, also called 
hepatoma) is one of the major cancers in regions of Asia. Early detection of hepatoma is an 
important issue since late detection of hepatoma is typically fatal. Cirrhosis of the liver, which is 
also very prevalent, is a potential risk factor for the development of hepatoma. The differentiation 
between hepatoma and cirrhosis from normal liver is an important task since their treatments differ 
significantly.  
B-mode ultrasound images present various granular structures as texture. The image texture 
represents the acoustic properties of human tissues. Therefore, the tissue differentiation of 
ultrasound image can be modeled as a texture classification problem. 
In texture analysis, the most difficult aspect is to define a set of meaningful features. Recently, 
one of the major developments in texture classification has been the use of multiresolution 
analysis. Multiresolution analysis provides information about the image contained in 
time-frequency domain, and thus provides a powerful tool for the description of similar textures. 
Several multiresolution transform algorithms have been used for texture classification, such as the 
wavelet transform and Gabor filters [1]-[3]. Hence, a texture image obtains several subimages 
through multiresolution analysis. However, some of the feature may not provide discrimination. 
Therefore, to select useful features to obtain an efficient and improved solution to a given problem 
is an important task. 
The main purpose of feature selection is to reduce the number of features used in classification 
while maintain an acceptable classification accuracy. A large number of algorithms have been 
III. MULTIRESOLUTION FEATURE SELECTION ALGORITHM BASED ON 
GENETIC ALGORITHMS  
GA is a stochastic algorithm that mimics natural evolution. The basic iterative model of the 
GA is an evaluation-selection-reproduction loop. The initial population pool consists of a set of 
individuals. Each individual is a potential solution, represented by a binary string. The length of 
the binary string is the number of candidate features. The binary string can be viewed as a mask 
vector of the feature vector.  
Each bit of the binary string represents the decision of the corresponding feature. The feature 
is selected if the corresponding bit value of the binary string is one. On the other hand, the feature 
will be discarded if the corresponding bit value equals zero. 
At the population initialization stage, a population of M individuals is generated randomly. A 
checking process for uniqueness is applied to accelerate the convergence of the evolution. 
Duplicate individuals are removed and new individuals are generated randomly to fill the pool.   
Given the population of the ith generation, the (i+1)th generation can be produced after 
(passing through) three stages: evaluation, selection and reproduction. 
In the evaluation stage, the fitness of each individual is evaluated with a fitness function. 
The individual with a higher fitness value will have a higher chance of survival and reproduction. 
In this paper, a novel fitness function is defined as  
 
                       (1) 
          
where CR is the classification rate and  is the number of feature selected.  and  are the 
corresponding weighting for CR and . 
The survival rule favors the individual with high classification rate and fewer features 
selected. The weighting coefficients of the classification rate and the number of feature selected 
are defined as 
                       (2) 
,           
where  denotes the number of total samples used for training and L is the length of the 
binary string, i.e., the number of elements in the feature vector. 
The proposed fitness function guarantees that individuals with higher classification rate are 
superior to the individuals with lower classification rate, regardless of their number of selected 
features. 
The population can be rearranged with respect to the corresponding fitness value of each 
individual. Let , where  is the highest classification rate achieved in 
the current generation.  denotes the set of individuals with classification rate equals to . 
The whole population can be listed as the following manner: 
                                (3) 
In each set , elements are sorted according to the number of feature selected, where 
elements with small  are listed on the top. 
2) Bayes Classifier [13]: One of the ways to represent a pattern classifier is in terms of a set of 
discriminate functions , where K is the total number of classes. The classifier is 
used to assign a feature vector X to class  if  for all . Let us assume that 
the distribution of the feature vector X within the ith class  is a multivariate normal 
distribution with mean vector  and covariance matrix  and that the a priori probabilities are 
equal for all classes. Under such an assumption, the discriminate functions can be defined as 
             (7) 
3) Fuzzy k-Nearest Neighbor (Fuzzy kNN) Classifier: The fuzzy k-NN is defined as a function 
of the number of neighborhoods (k), class membership degrees, and distances between a pattern to 
be classified and patterns for which the class membership degrees were previously determined 
[14]. A class membership degree between 0 and 1 is computed using the first minimum distances 
and the known class membership degrees of the patterns.  
The fuzzy k-NN algorithm has the similar computation procedure as in a classical nearest 
neighbor algorithm. They enable distances between an unlabeled pattern (x), whose class has to be 
determined, and patterns (  whose classes  are previously known, to be ordered in an 
ascending fashion. 
The fuzzy k-NN not only gives a class to which the pattern is assigned, but also the class 
membership degree that provides information about the certainty of the classification decision.  
B. Neural Network Classifiers 
Neural network classifiers have shown promise in pattern classification and are considered to 
be a potential alternative approach to statistical pattern classification. 
1) Back-propagation Neural Network (BPNN) [15]: Back-propagation is currently one of the 
most popular methods for performing a supervised learning task. In supervised learning, we try to 
adapt an artificial neural network whose output comes close to the target output for a training set. 
The goal is to adapt the parameters of the network so that it performs well for patterns outside the 
training set. In this chapter, the BPNN uses a 2-layer structure, one hidden layer with a hyperbolic 
tangent transfer function and one 3-neutron output layer with a linear transfer function, and utilizes 
the Levenberg-Marquardt algorithm as a learning rule to speed up the convergence. 
2) Probabilistic Neural Network (PNN) 
The network structure of a PNN is similar to back-propagation; the primary difference is that the 
transfer function is replaced by the exponential function and all of the training samples are stored 
as weight vectors [16]. 
 
V. EXPERIMENTAL RESULTS 
In this study, the liver diseases cases images include 176 cirrhosis, 166 hepatoma, and 99 
normal. Ten datasets were generated by randomly partitioned the whole samples into disjoint 
training and test sets. In each dataset, one-third of the samples for each class were randomly 
selected as the test set. The remainders are used to train the parameters of each classifier. For each 
trial, the proposed GA-based feature selection algorithm is applied using the training set of each 
TABLE 2.   CLASSIFICATION PERFORMANCE USING FIVE DIVERSITY CLASSIFIERS (%) 
   Feature 
Vector
 
Classifier  
Feature 
Selection 
1 
Feature 
Selection 
2 
Feature 
Selection 
3 
KNN 94.93 95.00 94.93 
Bayes 90.35 91.11 92.15 
F-KNN 94.10 94.17 94.24 
PNN 93.68 93.68 93.75 
BPNN 95.56 95.56 95.83 
Averaged 
accuracy 
93.72 93.90 94.18 
 
 
References 
[1] M. Unser, “Texture Classification and segmentation using wavelet frames,” IEEE Trans. Image 
Processing, vol. 4, no. 11, pp. 1549-1560, Nov. 1995. 
[2] C. M. Chen, H. H. Lu, and K. Han, “A textural approach based on Gabor functions for texture edge 
detection in ultrasound images,” Ultrasound in Med. & Biol., vol. 27, no. 4, pp. 515–534, 2001. 
[3] S. Li and J. S. Taylor, “Comparison and fusion of multiresolution features for texture classification,” 
Pattern Recognition Letters, 26, pp. 633-638, 2005. 
[4] A. K. Jain and D. Zongker, “Feature selection: Evaluation, application, and small sample 
performance,” IEEE Trans. Pattern Anal. Machine Intell., vol. 19, pp. 153-158, Feb. 1997. 
[5] M. Srinvias and L. M. Patnaik, “Genetic algorithms: a survey,” Computer, vol. 27, pp 17-26, June 
1994. 
[6] M. L. Raymer, W. F. Punch, E. D. Goodman, L. A. Kuhn, and A. K. Jain, “Dimensionality reduction 
using genetic algorithms,” IEEE Trans. Evolutionary Computation, vol. 4, issue 2, pp. 164-171, July  
2000. 
[7] E. Zio, P. Baraldi, and N. Pedroni, “Selecting  features for nuclear transients classification by means 
of genetic algorithms,” IEEE Trans. Nuclear Science, vol. 53, issue 3, pp. 1479-1493, June, 2006. 
[8] D. P. Muni, N. R. Pal, and J. Das, “Genetic programming for simultaneous feature selection and 
classifier design,” IEEE Trans. Syst., Man, Cybern., Part B, vol. 36, issue 1, pp. 106-117,  Feb. 2006. 
[9] Z. Sun, G. Bebis, and R. Miller, “Object detection using feature subset selection,” Pattern Recognition, 
37, pp. 2165-2176, 2004. 
[10] W. L. Lee, Y. C. Chen, and K. S. Hsieh, “Ultrasonic liver tissues classification by fractal feature 
vector based on M-band wavelet transform,” IEEE Trans. Medical Imaging, vol. 22, no.3, pp. 382-392, 
March 2003. 
[11] B. B. Mandelbrot, Fractal Geometry of Nature. Freeman Press, San Francisco, 1982. 
[12] K. Fukunaga, Introduction to Statistical Pattern Recognition. San Diego, CA: Academic, 1990. 
[13] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classification. 2nd Edition New York: Wiley, 2001. 
