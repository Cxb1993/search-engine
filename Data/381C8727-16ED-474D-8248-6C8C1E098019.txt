1 
 
前言 
    蛋白質序列或結構的重複特徵，對分析蛋白質摺疊、功能、結構穩定性及基
因演化過程扮演著極為重要的關鍵因素。蛋白質內部重複的主要類型可分為
DOMAIN，SOLENOID和 FIBROUS這三種樣式，這些重複特徵可能會參與蛋白質間
的相互作用或細胞週期調控，進而調控基因及蛋白質摺疊等生化功能。為了自動
偵測不同長度或不同組合類型的蛋白質序列或結構的內部重複特徵，本系統結合
蛋白質一級序列胺基酸內容和二級拓撲結構資訊，經由已知重複序列或結構的資
料訓練及蛋白質功能的註解，設計一套完整的蛋白質結構特徵自動預測分析系統。
訓練資料集的來源包括 SCOP，PFAM及 PROSITE資料庫中所有具重複特性的蛋
白質序列和結構資訊，透過自相關演算法及結構排比驗證，建置一個完整的基本
重複單元資料庫。再藉由相同驗證技術和基本重複單元序列資料的比對，期待能
對未知蛋白質序列或結構的重複特徵進行自動判別及分類，以提供生物學者確認
該蛋白質是否具有內部重複結構的特徵。系統使用 30個已知具有內部重複結構
的資料集進行預測分析，結果顯示本系統可以達到 76.33%的正確辨識率及基本
重複單元的自動切割分析。 
研究目的 
 近年來在研究蛋白質結構的相關議題，蛋白質內部重複特徵常受到相當程度
的注目。過去研究證明蛋白質結構是影響生物功能表現最重要的因素之一，而蛋
白質若具有內部重複的結構特徵，對蛋白質結構的摺疊、生物的功能和基因的演
化過程會扮演著重要的角色[1]。研究亦指出約有 14%的蛋白質具有內部序列或
結構的重複特徵[2]，且依據特徵長度的不同大致可分為三類，第一類是 Fibrous 
Repeat，也是三類之中重複基本單元長度最短的類別，每一基本單元是由 1個到
4個殘基所組成，經由單一胺基酸或短胜肽片段基本單元的重複來維持結構的穩
定；第二類是 Solenoid Protein，每一基本單元的長度分布約為 5個到 50個殘基，
其基本重複單元內應含有二級結構元件的組合，週而復始的重複組合可以形成較
穩定的超螺旋結構；最後一類的重複特性是 Domain Repeat，基本重複單元的長
度大於 50 個殘基，其基本重複單元可以構成一個具有功能的蛋白質結構域，具
有較為獨立特性的完整結構也具備特殊的功能特性。 
 這些不同類型的蛋白質重複特徵不僅影響蛋白質結構的穩定性，也參與蛋白
質與蛋白質間的相互作用或細胞週期的調控，對生物功能的表現具有相當的影響
力。舉例來說，WD40 的重複結構關係著訊號傳遞和細胞週期調控 [3]，
TATA-binding protein (TBP)則是一種轉錄因子負責與 DNA序列間的結合[4]等。
這些蛋白質重複特徵不僅會影響生物功能的正常運作，也可能會造成蛋白質結構
異常產生病變，例如舞蹈症疾病(Huntington’s Disease，HD）的就是因為基因的
3 
 
想法本研究設計了一套蛋白質序列或結構重複特徵的自動辨識系統，並建立一個
完整的蛋白質重複結構基本單元的模式資料庫，同時結合蛋白質序列與結構資訊，
運用自相關演算法以及序列/結構排比工具，從序列或結構階層式地偵測不同長
度的重複特徵，期待在生物學和蛋白質體學上可以提供生物學家更實質的幫助。 
研究方法 
 本研究主要的目的是開發一套完整的辨識系統可以自動偵測具不同重複類
型的蛋白質序列或結構，並將該系統命名為 IRIS (Internal Repeat Identification 
System)。 該系統可以藉由網頁操作介面即時進行自動辨識及分析 domain，
solenoid和 fibrous三種不同類別的重複特徵。如圖一的系統架構圖所示，使用者
輸入蛋白質序列或結構 PDB 編碼，透過蛋白質重複胺基酸序列分析或是與系統
所建立的基本重複單元資料庫進行序列/結構比對，偵測出不同長度及類型的蛋
白質內部重複特徵。 
 
 
圖一：系統架構圖 
 
第一種類型的 fibrous repeat，本系統透過自相關演算法分析使用者輸入的蛋白
質序列，將序列透過不同長度的位移並進行比對，若結果可以發現具有高度自相
關片段且長度是由 1 個到 4 個胺基酸所組成的重複片段，則代表序列中具有
fibrous的重複特徵。第二類型的 solenoid protein和第三類的 domain repeat則是
採用序列排比工具，BLAST (Basic Local Alignment Search Tool)[12]，將輸入的
序列與事先建立的基本重複單元資料庫進行區域性的序列排比，透過 BLASTP
5 
 
域重複的基本測試比對資料集。最後一項基本重複單元的資料來源是 PROSITE
資料庫[16]，該資料庫同樣是基於多重序列排比而得到單一保留性序列模式，提
供各種蛋白質功能域資訊、蛋白質家族序列和重要功能結合區位置的詳細訊息，
本系統從 2338筆序列模式資料中共收集 473種具有重複單元的序列資料，進一
步完成辨識分析具 solenoid 重複特徵的基本重複單元資料集。最後將上述包含
domain 和 solenoid 兩大類重複資料集合成為一套完整的基本重複單元資料庫，
作為後續自動比對、預測、辨識及切割據重複特徵結構的基本參考比對資料。 
 
實驗結果 
  為了驗證 IRIS對蛋白質重複特徵的自動辨識效能，本研究從 REPETITA[18]
中隨機選取 30筆具有重複特徵的蛋白質資料作為測試資料集。REPETITA測試
資料集所提供的蛋白質結構資料大部分是含有 solenoid 重複結構的特徵，且重
複單元內大多是由連續的超螺旋結構所組成。經由 IRIS 自動辨識之後的結果如
表 I 所示，結果顯示這些測試的蛋白質結構中，有 73.33%可以正確的透過 IRIS
分析流程完成自動辨識，並依重複特徵自動切割出結構內部所有的基本重複單元
子結構片段。 
表 I ： 測試結果 
 REPETITA 測試資料集
測試資料集蛋白質結構
數量 30 
正確預測及切割重複結
構數量 22 (73.33%) 
錯誤預測重複結構數量 8 (26.67%) 
  從辨識結果上看來 IRIS仍有 26.67%的錯誤辨識率，為瞭解造成系統無法正
確辨識的原因，我們對這些偵測錯誤的蛋白質資料逐一進行分析。結果可以發現
這些資料的蛋白質結構大部分都含有 β 二級結構元素且在空間結構上含有漸變
的特性，例如同樣的二級結構元素所組成的重複單元但是大小卻不盡相同，如圖
二-1 的範例是來自蛋白質結構 2mev:2，兩個原本應該在結構中屬於同一類別的
重複基本單元，但因為高重複次數後使得前後的基本單元大小有明顯差距，經結
構排比後無法確認為同一基本單元而造成錯誤辨識；另一種現象是蛋白質結構的
複雜度較高並同時形成多種重複結構的特徵，如圖二-2 範例所示，蛋白質結構
1go8:P，該結構同時具有 ββαβ的基本重複結構單元及 αβ的基本重複單元，所以
造成重複單元的不連續狀態，因而造成系統錯誤辨識。這些錯誤辨識的結構基本
上都是因為基本重複單元結構的大小不同或不連續性，所以才會造成 IRIS 無法
準確辨識蛋白質的內部重複特徵，降低 IRIS 的正確自動切割分析及系統辨識
率。 
7 
 
比上所產生的問題。 
  依辨識結果顯示目前 IRIS 的確可以提供一個高效率的自動分析工具。透過
蛋白質的一級胺基酸序列和二級拓樸結構資訊，搭配自相關演算法及序列/結構
排比工具，可以幫助使用者正確辨識重要的內部重複特徵，更可以有效的輔助生
物學者發現重複蛋白質結構與生物功能表現的關係，甚至為基因疾病的研究提供
更多預防醫學的相關訊息，不僅在生物學和蛋白質體學甚或是疾病醫學上都能發
揮更實質的幫助。 
參考文獻 
楊晴涵、施燦煌、葉春超、白敦文、許輝煌, "蛋白質重複結構之自動偵測系統", 
Proceedings of 2012 National Symposium on System Science and Engineering 
(NSSSE2012), pp. 45-48, Keelung, Taiwan, June 16~17, 2012. 
 
計畫成果自評 
本計畫已完成一個辨識蛋白質內部重複結構的系統 (IRIS)，本系統之設計使用
Web 介面，使用者可以輕易透過瀏覽器連結使用本系統。本計畫並已發表研討
會論文一篇，目前正在準備一篇期刊論文投稿。 
2011 IEEE International Conference on Intelligent Computing  
and Integrated Systems (ICISS2011) 
October 24-26, 2011, Guilin, China 
http://www.icismeeting.org/  
 
   
 
Dear Author,  
 
Thanks for your submission to the ICISS 2011. We are pleased to inform you that your paper:  
 
PaperID: 294 
TITLE: People Tracking in a Multi-Camera Environment 
AUTHORS: Hui-Huang Hsu, Wei-Min Yang, Timothy K. Shih 
 
has been accepted as a full paper for the final program. Congratulations!  
The review process was extremely selective and many good papers could not be included in the final program.  
 
The registration deadline is on July 30, 2011. You should finish the three  
steps below before the deadline or you will be deemed to withdraw your paper:  
Step 1: Uploading Your Paper to the website (http://www.icismeeting.org/papersubmission.html).  
Step 2: Making Registration Payment  
(http://www.icismeeting.org/registration.html).  
Step 3: Providing Registration Information.  
 
 
We are looking forward to seeing you in Guilin,China !  
 
Best Regards!  
 
 
 
 
Organizing Committee of ICISS 2011        
 
 
analysis of an indoor, real-time, multi-camera surveillance 
system for monitoring an office building, where one goal is 
to track people of interest [13]. In our work, we also want to 
track people of interest, but we focus on an outdoor 
environment where lighting conditions change all the time.  
III. TARGET TRACKING ACROSS MULTIPLE CAMERAS
In this section, we will present the techniques used to 
track a target object in a multi-camera environment. When a 
target is determined, three stages are needed for the tracking: 
object segmentation, object tracking in a single camera, and 
cross-camera tracking.  
A. Object Segmentation  
The object segmentation method used in this system 
follows the method in [9]. The theory is a Bayesian decision 
framework for classification of the background and the 
foreground. Meanwhile, two types of features are employed 
to model the background containing both stationary and 
moving objects. The statistics of most significant colors are 
used to describe the stationary parts of the background, and 
that of most significant color co-occurrences are used to 
describe the moving objects of the background. Foreground 
objects are extracted by fusing the detection results from 
both stationary and moving points.  
B. Object Tracking in a Single Camera 
As soon as the foreground pixels have been segmented, 
we apply the Connect Component Labeling [14] operation 
to each frame to identify moving objects (shown in Fig.1 
(a)). Then we choose the object blobs with height over 70 
pixels and width over 25 pixels for tracking. This is to 
disregard objects that are too small. In Fig. 1(b) the small 
moving object at the upper-half of Fig. 1(a) is not selected.  
(a)                                                         (b) 
Figure 1. Connect component labeling operation for extracted objects: (a) 
Foreground image, (b) Extracted result. 
In the tracking process, there are three steps. The first 
step is to initialize the target histogram with a bin size of 32 
in the RGB color space. The second one is to calculate the 
similarity of each blob with the target using the 
Bhattacharyya distance.  
 
i
iHiHHHd )()(),( 2121                      (1) 
where H1 and H2 are histogram and i is the index of bins.  
Only the blobs in the next frame, which are close to the 
tracking target of the current frame, are considered as 
possible candidates. The threshold is set to 70 of the 
Euclidean distance between blob centers. The most similar 
blob is then picked from the candidate blobs.  
),(maxarg* iti OOdO 
                            (2) 
where O* is the target object in the current frame, Ot is the 
target in the previous frame, and Oi is the set of candidates 
with an Euclidean distance to Ot of 70 or less. 
The extracted objects using Connect Component 
Labeling might be broken due to color similarity between 
the foreground objects and the background (see Fig. 2), the 
final step is to use the mean shift algorithm [10] to help 
track the target object. Mean shift is an iterative method for 
creating a confidence map in the current frame to find a 
peak near the object’s old position in the previous frame.  
(a)                                                   (b) 
Figure 2. Broken extracted objects caused by color similarity between the 
foreground objects and the background: (a) Segmented moving objects, (b) 
Broken extracted objects.  
C. Cross-Camera Tracking  
Once object tracking is finished in the current camera, 
we can have the information about the object blob, leaving 
time, and leaving location. Using this information to track 
the target in other cameras, we need to know the relative 
location of all cameras and the distance between them. In 
our system, the user can set this information when 
initializing the system. Fig. 3 shows the configuration 
interface for setting the corresponding regions on a camera 
and their distances to other neighboring cameras.  
   
(a)                                                    (b) 
Figure 3. Camera configuration interface: (a) Setting corresponding regions 
on a camera, (b) Setting the neighboring camera and its distance to a 
specified region.  
If a target object leaves a specified region on a camera, 
we would expect it to appear in the preset neighboring 
camera in a short time.  The time is related to the distance of 
the neighboring camera to this specified region. Thus we 
can start searching for the target object in the neighboring 
camera some time after it leaves the current camera. The 
delay time is defined as follows.  
179
(a) 
(b) 
(c) 
Figure 7. Tracking results of three examples. 
1) In this research, we mainly use color information to 
decide if a segmented foreground object is the target or 
not. The color information might change when the 
object image is captured from different viewing angles. 
For example, the object’s cloth has different colors in 
the front and the back. Also, the object carries things 
like a bag or an umbrella.  
2) The color information of the target in the next camera 
might be too little for the system to identify it. When 
the target appears in the next camera, it can be quite 
small. Although the system searches the target in the 
subsequent frames to obtain bigger blobs and apply a 
back-tracking technique to decide if the blobs are the 
target, the system still misses the target is some extreme 
cases.   
3) It is possible that a mistake occurs during the tracking 
or the back-tracking process. If this happens, the 
subsequent tracking is wrong. This can happen when 
multiple foreground objects appear in the same frame 
and there are occlusions.  
From the discussions, it shows that our system still have 
constrains. It might not perform well in some cases. To 
solve the problem, more information would be needed. 
Adding face information is a possibility if high-resolution 
images can be obtained. Speed of the object is another one. 
Assuming that the object moves at a constant pace, speed 
information can give us more accurate tracking.  
V. CONCLUSION
In this paper, we present a video processing system that 
can track interested target in an outdoor multi-camera 
environment. This is a very useful tool in helping the 
detective find the moving trajectory of a crime suspect 
quickly. Moreover, the selected frames images can provide 
more visual information of the suspect. Thus the system is 
suitable to an outdoor surveillance system.  
This paper gives the results of our first attempt in solving 
this problem. The target object is tracked across multiple 
cameras using mainly the color information due to its 
simplicity. But this is sometimes not sufficient and mistakes 
might occur. Therefore, more information is needed. Face 
and speed of the target object might be helpful for further 
improvement of the system.  
REFERENCES
[1] T. Wiegand, G. J. Sullivan, G. Bjontegaard, and A. Luthra. 
“Overview of the H.264/AVC Video Coding Standard,” IEEE 
Transactions on Circuits and Systems for Video Technology, Vol. 13, 
No. 7, July 2003, pp. 560–576. 
[2] I. Haritaoglu, D. Harwood, and L. S. Davis. “W4: Real-Time 
Surveillance of People and Their Activities,” IEEE Transactions on 
Pattern Analysis and Machine Intelligence, Vol. 22, Issue 8, August 
2000, pp.809–830. 
[3] C. Stauffer and W. Grimson. “Learning Patterns of Activity using 
Real-Time Tracking,” IEEE Transactions on Pattern Analysis and 
Machine Intelligence, Vol. 22, Issue 8, August 2000, pp.747–757. 
[4] K. Karmann and A. V. Brandt. “Moving Object Recognition Using an 
Adaptive Background Memory,” in Time-Varing Image Processing 
and Moving Object Recognition, V. Cappellini, Ed., Elsevier Science 
Publishers, Vol. 2, 1990, pp.289–296. 
[5] D. Koller, J. Weber, T. Huang, J. Malik, G. Ogasawara, B. Rao, and S. 
Russel. “Toward Robust Automatic Traffic Scene Analysis in Real 
Time,” Proc. IEEE Conference on Pattern Recognition, Vol. 1, Oct. 
1994, pp.126–131. 
[6] C. Wren, A. Azarbaygaui, T. Darrell, and A. P. Pentland. “Real-Time 
Tracking of the Human Body,” IEEE Transactions on Pattern 
Analysis and Machine Intelligence, Vol. 19, Issue 7, July 1997, 
pp.780–785. 
[7] C. Stauffer and W. E. L Grimson. “Adaptive Background Mixture 
Models for Real-Time Tracking,” Proc. IEEE Computer Society 
Conference on Computer Vision and Pattern Recognition, Vol. 2, 
1999, p.205. 
[8] J.-H. Hsu, Moving Object Detection Using Gaussian Mixture Models, 
MS Thesis, Department of Electrical Engineering, National Taiwan 
University of Science and Technology, January 2007. 
[9] L. Li, W. Huang, I. Y. H. Gu, and Q. Tian, “Foreground Object 
Detection from Videos Containing Complex Background,” Proc. the 
Eleventh ACM International Conference on Multimedia, Nov. 2003, 
pp.2-10. 
[10] D. Comaniciu, V. Ramesh, and P. Meer, “Real-Time Tracking of 
Non-Rigid Objects Using Mean Shift,” Proc. IEEE Conference on 
Computer Vision and Pattern Recognition, Vol. 2, June 2000, pp.13-
15. 
[11] D. Chen, A. J. Bharucha, and H. D. Wactlar, “Intelligent Video 
Monitoring to Improve Safety of Older Persons,” Proc. IEEE 
Conference on Engineering in Medicine and Biology Society, August 
2007, pp.22-26. 
[12] Y.-F. Wang, E. Y. Chang, and K. P. Cheng, “A Video Analysis 
Framework for Soft Biometry Security Surveillance,” Proc. the Third 
ACM International Workshop on Video Surveillance and Sensor 
Networks, Nov. 2005, pp.71-78. 
[13] T. Yang, F. Chen, D. Kimber, and J. Vaughan, “Robust People 
Detection and Tracking in a Multi-Camera Indoor Visual Surveillance 
System,” Proc. IEEE International Conference on Multimedia and 
Expo, July 2007, pp.675-678. 
[14] C. G Rafael and E. W. Richard, Digital Image Processing, Pearson 
Prentice Hall, 2001. 
181
100 年度專題研究計畫研究成果彙整表 
計畫主持人：許輝煌 計畫編號：100-2221-E-032-068- 
計畫名稱：考慮拓樸資訊的蛋白質內部重複結構辨識系統 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 75% 
篇 本研究中使用之
蛋白質內部重複
結構資料庫為先
前研究所開發。 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 1 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 1 75% 
期 刊 論 文 準 備
中，將投稿國外期
刊。本研究中使用
之蛋白質內部重
複結構資料庫為
先前研究所開發。
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
