categorization. However, to the best of our 
knowledge, most of them assumed that the number of 
classes is fixed and none of them are concerned with 
how to categorize image with efficient class 
extendibility yet preserving discriminative ability. 
This is a crucial issue for an effective image 
categorization system. Thus, the goal of our project 
is to propose an image categorization with efficient 
class extendibility and effective discriminative 
ability. 
Scene categorization with category-specific visual-
word construction and image representation is 
proposed in this project. The proposed scene 
categorization has effective discriminative ability 
and class extendibility. The reasons are listed as 
follows. First, since the visual-word construction 
and image representation are category-specific, the 
corresponding learning model for classification has 
substantial discriminating power. Second, since the 
visual-word construction and image representation are 
category-specific, image features related to the 
original classes need not be recreated when new 
classes are added, which minimizes reconstruction 
overhead. Experimental results confirm that the 
accuracy of the proposed method is superior to 
existing methods with single-type feature both in 
single-scale and multi-scale version. 
 
英文關鍵詞： Scene categorization, classification, category-
specific, class extendibility, image retrieval, 
visual words, codebook 
 
2 
 
Accompanied by the advanced technologies of computer and internet and the decreased 
cost of media storage devices, the digital image collections have grown explosively. How to 
effective and efficient categorize images so as to retrieve and organize image collations in a 
large scale database become an active research issue. On the other hand, image categorization 
could be used to facilitate a range of applications such as intelligent agent, object recognition, 
and video summarization. 
There are tremendous literatures related to image categorization, scene categorization, 
and object categorization. However, to the best of our knowledge, most of them assumed that 
the number of classes is fixed and none of them are concerned with how to categorize image 
with efficient class extendibility yet preserving discriminative ability. This is a crucial issue 
for an effective image categorization system. Thus, the goal of our project is to propose an 
image categorization with efficient class extendibility and effective discriminative ability. 
Scene categorization with category-specific visual-word construction and image 
representation is proposed in this project. The proposed scene categorization has effective 
discriminative ability and class extendibility. The reasons are listed as follows. First, since the 
visual-word construction and image representation are category-specific, the corresponding 
learning model for classification has substantial discriminating power. Second, since the 
visual-word construction and image representation are category-specific, image features 
related to the original classes need not be recreated when new classes are added, which 
minimizes reconstruction overhead. Experimental results confirm that the accuracy of the 
proposed method is superior to existing methods with single-type feature both in single-scale 
and multi-scale version. 
Keywords: Scene categorization, classification, category-specific, class extendibility, 
image retrieval, visual words, codebook 
 
二、 計劃目的 
由於視訊媒介及網際網路技術高度發展及媒體儲存設備成本降低，促使數位影像的
數量急速增長，因而延伸出一高挑戰性的問題：如何有效地分類數位影像，以利建檔及
後續檢索。除此之外，影像分類之應用甚廣。例如場景影像分類(Scene Categorization)
資訊除可直接應用於影像檢索外[1–7]，還可協助智慧代理人(Intelligent Agent)瞭解所在
周遭環境，進而做出正確互動決策[8,9]；也可提供環境相關線索(Contextual Cue)提升物
件辨識(Object Recognition)的正確率[10–18]，甚至可根據環境影像分類資訊協助影像/視
訊註記(Image/Video Annotation) [19–24]。 
但影像分類是一高難度的問題，這是因為以電腦視覺(Computer Vision)表示的影像
低階特徵(Low-level Features)與使用者對影像資訊的理解(High-level Semantic)存在著不
一致性，這就是所謂的「語義鴻溝」(Semantic Gap)，正是這個因素的存在限制了影像
4 
 
在此值得一提的是，有許多研究，其區塊切割方式並非採固定式，而是事先偵測興
趣點，且僅在興趣點提取分塊特徵。但已有研究報告[31,35]，其結論是固定式區塊切割，
其分類能力並不下於興趣點，所以本計畫擬採固定分塊切割法。另外雖然目前文獻大都
採視覺詞彙的值方圖(Histogram)來描述一張影像[31–39]，再根據此一特徵，進行影像相
似度比對，進而完成影像分類的工做。但文獻[40]採以視覺詞彙歸類後其區域特徵平均
值（如 SIFT），描述一張影像，進而根據此一特徵，進行影像相似度比對，且實驗結果
顯示其分類正確率有明顯的提升，所以本計畫擬就此兩種影像表示特徵：視覺詞彙的值
方圖(Histogram)與以視覺詞彙歸類後其區域特徵平均值（如 SIFT），加以探討，找出在
加影像特徵。 
但在建構詞彙字典時，大部分的文獻是採將全部類別之訓練影像分塊，一起進行相
似性群聚[31–35, 39,40]。這種方式有兩大缺點[36–38]，一是分塊個數過多，相似性群聚
工作費時，所以效率不彰。另一是將全部類別之訓練影像分塊，一起進行相似性群聚，
所得之詞彙較不具類別代表性[36–38]。所以文獻[36–38]提出，針對各類別分別群聚並
建置詞彙字典。唯各類別詞彙字典，仍須整合成一的詞彙字典，簡稱分聚合建。皆下來，
再將各分塊根據其區域特徵，歸類為適當的詞彙，如此即可得每張訓練影像之詞彙值方
圖，後續作法，則與傳統方法同[31–35, 39,40]。文獻[36–38]證實此一方式，確可得具較
高分辨能力詞彙字典，相對的其分類正確率也較高。 
但是此一針對各類別分別群聚並建置詞彙字典的方法（分聚合建）仍有其缺點。首
先，雖然分聚合建的方式，已改為針對不同類別分別進行詞彙群聚，但是在建置特徵資
料庫時，仍是採用所有類別詞彙字典之整合，當作建置資料庫之依據，所以當有新的類
別加入時，雖然就此類別的詞彙字典可以獨立建置，但因全體詞彙字典仍須更新，所以
所有訓練影像之資料庫，亦需相對更新，導致重建負擔(Overhead)。 
因為根據文獻[36–38]之推論，針對不同類別分別進行詞彙群聚，其建置詞彙的值方
圖(Histogram)特徵，在進行影像分類時，有較高的分辨能力(Discriminative Power)，因
此也有較高的分類正確率。所以本計畫即根基於此邏輯，推論出針對不同類別不同的詞
彙字典，分別建構其不同類別詞彙之影像特徵，應有更高的分辨能力(Discriminative 
Power)，因此也有更高的分類正確率。本計畫將根植於此中心思想，提出一兼具有效率
類別擴充性及有效分類能力之影像分類法。 
 
 
 
 
 
 
 
 
6 
 
似性比對值(Similarity Measure)，將其歸類為詞彙字典(Dictionary)  Mkk ,,1w 中適
當的詞彙 kw ，如此即可得此張測試影像 I 之詞彙值方圖h，最後再輸入整合 C 個 SVM
的影像分類器，即可得 I 影像的類別。 
但本計劃採用的影像特徵未採傳統的詞彙值方圖，而改採向量特徵[40]。也就是一
張影像 csI 是直接以詞彙特徵向量    Mcscscs ffF ,,1  來表示，其中  kcsf 定義如式(2) 
 
 
 
  
      MkSCcNnknLn
n
k
c
s
c
s
NnknL
c
s
c
s
c
s ,1,,,1,s ,,,1
,,1,
,,1, 

 


xx
x
f x  (2)
 
在此值得一提的是 SVM 的影像分類器在以值方圖及特徵向量為特徵時，其維度分別是
M 及 d×M 。文獻[40]的實驗証實後者之分類正確性遠高於前者，故本計劃直接採用詞
彙特徵向量表示一張影像。 
 
圖二、傳統的合聚合建的場景分類方法。 
 
8 
 
 
圖三、基於分聚合建的場景分類方法。 
 
4. 分聚分建 
前述分聚合建策略仍有其缺點，所以本計畫提出如圖四之分類法(簡稱分聚分建)。分
聚分建的策略其建置詞彙字典的方式與分聚合建相同，亦即由  ,,,1 Mkik w  
,,,1 Ci  所組成，但此 C 個字典並不合併為一共用字典，反而是各別獨立使用，得到
針對各類別之特徵。詳而論之，每一具有特徵  ncsx 之訓練影像分塊，會依據字典詞彙
 Mkik ,,1w ，根據相似性比對值   ikcs n wx  歸類於一詞彙   i nL csi xw 。亦即每張具有特
徵  ncsx 之訓練影像 csI 以特徵向量     ,,1,,1,1 Mcscscs ffF     MCC cscs ,,,1, ff  來表
示，其中  kics ,f 定義如式(4) 
 
 
  
     
MkCiSCc
NnknLn
n
ki
c
s
ic
s
NnknL
c
s
c
s
c
s
i
,,1,,,1,,1,s ,,,1
,,1,
, ,,1,







xx
x
f x  (4)
 
10 
 
 
圖四、基於分聚分建的場景分類方法。 
四、 結果與討論 
本研究採用的資料為 Scene-13[2]和 Scene-15[3]，是目前場景分類中較為常用的公共
測試集，分別包含 13 類和 15 類場景，每類場景中的圖片有 200~400 張。資料庫中圖片
的平均大小為 384×256，圖五為 Scene-15 的部分圖片示意圖。實驗中的分類器採用
libsvm[68]，核函數採用徑向基函數（Radial-basis function, RBF）。每次取 100 張訓練圖
片和 100 張測試圖片，做 10 次試驗，取平均值作為最終的分類性能。 
 
圖四、Scene-15 中的部分示例圖片，摘自[3] 。 
12 
 
表一、分聚分建(所提方法)在 Scene-13 資料庫的分類結果。 
Features HOG SIFT 
M 35 50 80 35 50 80 
C-C/W-R 79.88±0.12 80.31±0.12 81.45±0.06 83.44±0.08 85.25±0.12 85.47±0.14 
C-C/C-R 83.25±0.06 83.58±0.06 84.03±0.09 85.42±0.04 86.35±0.03 86.65±0.48 
 
表二、 分聚分建(所提方法)在 Scene-15 資料庫的分類結果。 
Features HOG SIFT 
M 35 50 80 35 50 80 
C-C/W-R 78.49±0.11 79.45±0.19 79.81±0.13 79.45±0.16 79.95±0.14 80.21±0.06 
C-C/C-R 81.45±0.08 81.99±0.09 82.63±0.10 83.02±0.09 83.42±0.08 84.06±0.07 
 
表三、多尺度特徵 SIFT 在 Scene-13 及 Scene-15 資料庫的分類結果。 
Features 13-scene 15-scene 
M 2,15,30 2,25,50 2,40,80 2,15,30 2,25,50 2,40,80 
Accuracy 88.19±1.50 88.46±2.20 88.72±0.19 84.3±0.07 85.52±0.08 85.98±0.07 
 
表四、Scene-15 資料庫的混淆矩陣（塊特徵描述：SIFT）. 
(a) 分聚合建  
 S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 S11 S12 S13 S14 S15
S1 65.1 0 0 9.1 18.2 0 0 0 3.0 0 0 0 0 0 4.5 
S2 0 100 0 0 0 0 0 0 0 0 0 0 0 0 0 
S3 1.8 3.1 63.3 1.2 3.7 0.6 0.6 0.6 3.7 0.6 0.6 0 6.2 0 13.6
S4 6.7 0 1.6 71.6 6.7 0 0 0 1.6 0 0 0 0 6.7 5 
S5 18.7 0 2.1 8.6 56.8 0 0 0 2.9 0 0 0 0 4.3 6.5 
S6 0 0 0 0 0 88.1 0.4 3.3 0 0.4 7.6 0 0 0 0 
S7 0 0 0 0 0 0 95.5 0 0 2.2 2.2 0 0 0 0 
S8 0.9 0 2.7 0 0 3.6 0 91.8 0 0.9 0 0 0 0 0 
S9 0.6 0 4.4 1.9 0.6 0 0 0.6 77.2 0 0.6 4.4 3.8 0 5.7 
S10 0 0 0 0 0 0.4 2.2 0.4 0 90.2 6.25 0 0 0 0.4 
S11 0 0.8 0 0 0 13.4 3..5 0.8 0 6.1 75.4 0 0 0 0 
S12 0 0 3.5 0 0.7 0 0 3.5 2.1 0.7 0.7 85.9 0.7 0 2.1 
S13 0 0.4 4.3 0 1.3 0 0.4 0 1.3 0.9 0 0 87.4 0 3.4 
S14 0.15 0 0 0.15 0.3 0 0 0 0.15 0 0 0 0 92.3 0 
S15 1.2 0 4.2 4.8 6 0 1.2 0 8.5 2.4 0 0.6 0 1.2 69.7
 
14 
 
S12：MITstreet, S13：MITtallbuilding, S14：PARoffice, S15：Store. 
 
表五、Scene-15 資料庫中，基於分聚分建方法的類別可擴展性分析 
Features HOG SIFT 
M 35 50 80 35 50 80 
15 78.49±0.11 79.45±0.19 79.81±0.13 79.45±0.16 79.95±0.14 80.21±0.06 
10 77.68±0.85 78.02±0.79 78.92±0.95 77.86±0.75 78.14±0.58 78.86±0.79 
5 76.43±1.24 76.94±1.01 77.45±1.21 77.12±0.86 77.67±0.90 77.86±0.67 
2 71.45±1.42 72.06±1.72 72.34±1.85 72.47±1.20 72.84±1.31 73.50±0.92 
 
五、 結論 
本計畫提出一各類別分別建置詞彙字典與影像特徵的影像分類法，設計相關區域特
徵擷取、詞彙字典建置、影像特徵表示及分類模式訓練，以達到建置兼具有效率類別擴
充性及有效分類能力之影像分類系統之目標。所提方法具有效分類能力及有效率類別擴
充性，其原因分別說明如后。因為詞彙字典的建置與影像特徵的表示皆是依各影像分類
法是因為各類別分別建置詞彙字典與影像特徵，同時因為分類模式是依各類別分別建置
詞彙字典與影像特徵加以訓練，故有更高的分類能力；而當類別新增時，原類別影像特
徵皆不受影響，所以原資料庫不需重建，亦即原影像資料庫，不需相對更新，如此可免
除重建負擔，而達有效率擴增類別之目標。實驗結果證實所提分類方法不論是採單尺度
(Single-scale)或多重尺度(Multi-scale)特徵之正確率皆高於現有方法。本計畫研究成果已
發表成一篇國際會議論文[69]，二篇投稿國際期刊論文[70,71]，及一篇已線上刊登國際
期刊論文[72]。 
 
六、 計畫成果自評 
研究內容與原計畫相符，達成預期目標。研究成果方面，所提場景分類方法不論是採單
尺度(Single-scale)或多重尺度(Multi-scale) 特徵之正確率皆高於現有方法，研究成果具有
學術以及實務應用價值，本計畫研究成果已發表成一篇國際會議論文[69]，投稿二篇國
際期刊論文[70,71]，及一篇已線上刊登之國際期刊論文[72]。另參與學此生確實在參與
研究過程中獲得有關影像處理、圖形識別、資料庫、影像檢索、影像分類、影像註記、
影像特徵、詞彙字典建置及分類模式等方面之學術經驗與訓練。 
 
七、 參考文獻 
[1] A. Vailaya, M.A.T. Figueiredo, A.K. Jain and H.J. Zhang, “Image classification for 
content-based indexing,” IEEE Trans. Image Processing, vol. 10, no. 1, pp. 117–130, 
2001. 
[2] E. Chang, G. Kingshy, G. Sychay, W. Gang, “CBSA: content-based soft annotation for 
16 
 
retrieval with multiple-instance active learning,” Pattern Recognition, vol. 43, no. 2, pp. 
478–484, 2010. 
[17] L. Wu, S. C. H. Hoi, and N. Yu, “Semantics preserving bag-of-words models and 
applications,” IEEE Trans. Image Processing, vol. 19, no. 7, pp. 1908–1920, 2010. 
[18] D. Larlus and F. Jurie, “Latent mixture vocabularies for object categorization and 
segmentation,” Image and Vision Computing, vol. 27, no. 5, pp. 523–534, 2009. 
[19] G. Carneiro, A. B. Chan, P. J. Moreno, and N. Vasconcelos, “Supervised learning of 
semantic classes for image annotation and retrieval,” IEEE Trans. Pattern Recognition 
and Machine Intelligence, vol. 29, no. 3, pp. 394–410, 2007. 
[20] J. Tang, H. Li, G.-J. Qi, and T.-S. Chua, “Image annotation by graph-based inference 
with integrated multiple/single instance representations,” IEEE Trans. Multimedia, vol. 
12, no. 2, pp. 134–141, 2010. 
[21] S. Barrat and S. Tabbone, “Modeling, classifying and annotating weakly annotated 
images using Bayesian networks,” J. Vision Communication and Image Representation, 
vol. 21, no. 4, pp. 355–363, 2010. 
[22] H. Fu, Z. Chi, and D. Feng, “Recognition of attentive objects with a concept association 
network for image annotation,” Pattern Recognition, vol. 43, no. 10, pp. 3539–3547, 
2010. 
[23] E. Moxley, T. Mei, and B. S. Manjunath, “Video annotation through search and graph 
reinforcement mining,” IEEE Trans. Multimedia, vol. 12, no. 3, pp. 184–193, 2010. 
[24] W.-L. Zhao, X. Wu, and C.-W. Ngo,, “Video annotation through search and graph 
reinforcement mining,” IEEE Trans. Multimedia, vol. 12, no. 5, pp. 448–461, 2010. 
[25] C. Galleguillos and S. Belongie, “Context-based object categorization: A critical 
survey,” Computer Vision and Image Understanding, vol. 114, no. 1, pp. 712–722, 2010.  
[26] A. Bosch, X. Munoz, and R. Marti, “Which is the best way to organize/classify images 
by content?,” Image Vision Computing, vol. 25, no.6, pp. 778–791, 2007. 
[27] A. Oliva and A. Torralba, “Modeling the shape of the scene: a holistic representation of 
the spatial envelope,” Int. J. Computer Vision, vol. 42, no. 3, pp. 145–175, 2001. 
[28] J. Luo, A. Savakis, “Indoor vs outdoor classification of consumer photographs using 
low-level and semantic features,” in Proc. Int. Conf. Image Processing, vol. 2, 
pp.745–748, 2001. 
[29] J. Vogel, B. Schiele, “A semantic typicality measure for natural scene categorization,” in 
Proc. Int. Conf. DAGM Annual Pattern Recognition Symposium, pp.195–203, 2004. 
[30] H. Cheng and R. Wang, “Semantic modeling of natural scenes based on contextual 
Bayesian networks,” Pattern Recognition, vol. 43, no. 12, pp. 4042–4054, 2010. 
[31] F.-F. Li and P. Perona, “A Bayesian hierarchical model for learning natural scene 
18 
 
pp. 2971–2979, 2010. 
[47] A. Abdullah, R. C. Veltkamp, and M. A. Wiering, “Fixed partitioning salient points with 
MPEG-7 cluster correlograms for image categorization,” Pattern Recognition, vol. 43, 
no. 3, pp. 650–662, 2010. 
[48] J. Amores, N. Sebe, and P. Padeva, “Context-based object class recognition and retrieval 
by generalized correlograms,” IEEE Trans. Pattern Recognition and Machine 
Intelligence, vol. 29, no. 10, pp. 1818–1833, 2010. 
[49] T. Xia, D. Tao, T. Mei, Y. Zhang, “Multiple spectral embedding,” IEEE Trans. System 
Man and Cybernetics, 2011. 
[50] T.-C. Lin, R.-S. Liu, C.-Y. Chen, Y.-T. Chao, and S.-Y. Chen, “Pattern classification in 
DNA microarray data of multiple tumor types,” Pattern Recognition, vol. 39, no. 12, pp. 
2426-2438, 2006.  
[51] D. Comaniciu and P. Meer, “Mean-shift: a robust approach toward feature space 
analysis,” IEEE Trans. Pattern Recognition and Machine Intelligence, vol. 24, no. 5, pp. 
603–619, 2002. 
[52] B. J. Brendan and D. Dueck, “Aggolmerative mean-shift clustering,” IEEE Trans. 
Knowledge and Data Engineering, 2011. 
[53] F. Jurie and B. Trigges, “Creating efficient codebooks for visual recognition,” in Proc. 
Int. Conf. Computer Vision, 2005. 
[54] X. Bai, X. Wang, L. J. Latecki, W. Liu, Z. Tu, “Learning context-sensitive shape 
similarity by graph transduction,” IEEE Trans. Pattern Recognition and Machine 
Intelligence, vol. 32, no. 5, pp. 861–874, 2010. 
[55] X. Wang, T. X. Han, and S. Yan, “An HOG-LBP human detector with partial occlusion 
handling,” in Proc. Int. Conf. Computer Vision, pp. 32–39, 2009. 
[56] W. Zhou, Q. Tian, Y. Lu, L. Yang, and H. Li, “Latent visual context learning for web 
image applications,” Pattern Recognition, 2011. 
[57] M.-T. Chang and S.-Y. Chen, “Deformed trademark retrieval based on 2D pseudo-hidden 
Markov model,” Pattern Recognition, vol. 34, no. 5, pp.953-967, 2001.  
[58] C. Galleguillos, A. Rabinovich, and S. Belongie, “Object categorization using 
co-occurrnece, location and appearance,” in Proc. Int. Conf. Computer Vision and 
Pattern Recognition, pp. 1–8, 2008. 
[59] Y. Kamiya, T. Takahashi, I. Ide, and H. Murase, “A multimodal constellation model for 
object category recognition,” in Proc. Int. Multimedia Modeling Conference, pp. 
310–321, 2009. 
[60] Z. Lu and H. H. S. Ip, “Combining context, consistency, and diversity cues for 
interactive image categorization,” IEEE Trans. Multimedia, vol. 12, no. 3, pp. 194–203, 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/08/31
國科會補助計畫
計畫名稱: 兼具有效率類別擴充性及有效分類能力之影像分類法
計畫主持人: 陳淑媛
計畫編號: 100-2221-E-155-086- 學門領域: 影像處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
