without emotion. Hence, in this subproject, the TTS 
SOC module with warming emotion is provided to 
increase the chip value and application areas. 
 
英文關鍵詞： SoC, Hidden Markov Model (HMM), speech synthesis, 
warming emotion speech feedback 
 
一、 計畫內容 
(一) 前言 
為了滿足現代人生活的需求及便利性，更便捷的人機互動方式及技術不斷的被發展。但人與機器
的互動間，始終存在著人性化的議題，像是死板機械化的操作方式、未考慮使用者的情緒反應或親近
感等。因此，有越來越多的研究者紛紛投入情感運算及情緒認知與回饋的領域。無論如何，目前大部
分的人機互動裝置只具有單調朗讀語氣之語音合成回饋功能，而不具有人性化的語音合成回饋，因此
無法讓使用者達到更人性化的互動體驗。倘若系統能根據使用者的情緒改變回饋語音之語氣，則可以
使系統更貼近使用者的需求。 
語音合成晶片系統設計，國外如 Nuance、Sensory，國內如華邦電子、凌陽公司等，在目前相關的
語音合成 SoC 設計中，大多仍採用單音串接語音合成技術，該技術利用語音編碼技術、波形串接及波
形修改技術達到語音合成之目的。另外有部份語音合成 SoC 則採用共振峰合成技術，如 Seiko Epson
之 S1V30120。這些類型之合成語音較單調無變化，僅能達到有限之應用。除此之外，目前尚無情緒語
音合成晶片的問世。然而，生動的語音是人機互動研究之目標。系統若能提供不同情緒的合成語音，
必能增加語音合成晶片的使用價值及應用的領域。因此，本子計畫將完成一溫馨情緒語音回饋技術之
晶片設計，是一項能提升人類生活品質的前瞻性語音 SoC 技術。 
 
(二) 研究目的 
本計畫「溫馨情緒語音合成回饋 SOC 模組設計與實現」為總計畫「整合語者、語氣、N-Best 情
緒關鍵詞辨識之情緒感知與回饋之 SOC 系統單晶片設計與實現」之子計畫一，目的在於利用語音之
情緒語境相關建模技術、基於隱藏式馬可夫模型之語音合成技術並利用總計畫所提供的「共享可重組
式硬體」來完成運算，產生具情緒、生動且自然的語音回饋，並做為總計畫 SOC 之情緒語音回饋功
能。預計能根據總計畫所能辨識之十一種情緒，以不同之溫馨情緒語音回饋，包含關懷、溫柔、感性、
興奮、鼓勵、驚訝以及平靜等情緒。本晶片之溫馨情緒語音合成所回饋的語音在 MOS (Mean Opinion 
Score) 評估下，預計理解度達 4.7、自然度達 4.5、音質達 4.5、溫馨度達 4 以上。 
 
(三) 文獻探討 
目前所查之相關文獻中，情緒語音合成技術主要分為 1)基於語音合成技術與 2)聲音轉換技術。前
者之情緒語音合成隨著語音合成的基本方法而不同，後者通常與語音合成之方法無關，其評述如下： 
 
1. 基於波形串接之單元選取語音合成系統 [5-9] 
此方法先根據文脈資訊預測目標單元之各項參數，再考慮目標單元與語料庫中的候選單元之間的
相似度成本以及單元之間的串接成本，透過 Dynamic Programming 找出最佳的合成語音單元序列，最
後進行波形串接和修改 [5-7]。此技術為了達到多情緒語音合成，必須使用多情緒語音合成語料庫。最
具代表性的文獻有 [8], [9]，其中 Iida 實作出一套日本語的情緒語音合成系統，並應用於聽障殘障者的
發音與情感表達輔助，陳俊甫則根據句法結構的特性，提出一情緒語音單元的挑選方法。由於此方法
需要龐大的語料庫才能達到自然的情緒語音，因此並不適用於嵌入式系統及晶片設計上。 
 
2. HMM-based 統計參數語音合成技術 [10-25] 
此方法利用 HMM 訓練和狀態綑綁 (state-tying) 技術建立每個語音單元之 HMM 狀態在不同發音
情況的聲學參數，合成時根據語句文脈資訊挑選適當的狀態序列 (Sentence HMM)，並透過參數產生演
算法產生語音參數，最後透過 source-filter 技術將參數反轉為語音訊號。由於語音被參數化，因此具有
語音特性易修改之優點。此技術為達到情緒語音合成，可透過下列幾種方式。 
分為低中高三種，如下表。 
 
表一、arousal、溫馨情緒類別與語音特性對照表 
arousal 溫馨情緒類別 語速 語調 音量 
低 關懷、溫柔、感性 較慢、穩定 較低、穩定 平穩 
中 平靜 平穩 平穩 平穩 
高 有趣、鼓舞、讚嘆 較快、不穩定 較高、不穩定 較大、不穩定 
 
語音合成系統將能夠合成三種不同程度 arousal 的語音。這三種類別的語音再透過特殊情境線索插
入技術，可將合成語音擴展為七種溫馨情緒，如下 
 
表二、特殊情境線索插入 
arousal 溫馨情緒 情境線索 
低 
關懷 無 
溫柔 (撒嬌聲) 
感性 (親吻聲) 
中 平靜 無 
高 
興奮 (笑聲) 
鼓勵 (掌聲) 
驚訝 (驚呼聲) 
 
■ 情緒語料庫與聲學模型訓練 
為了建立不同 arousal 的情緒語音模型，我們已錄製了三種不同 arousal 的語音，每種各一千句。
錄音人員根據不同 arousal 的情緒語音在語速、語調和音量上之特性錄製，錄製內容為從北京清華大學
語音合成語料庫 (TH-CoSS) 中篩選而得。語料庫的標記工作是使用訓練好的 HMM 和強制對位 (force 
alignment) 技術對三千句語料進行初次標記，再透過人工檢查調整邊界位置。 
我們將語音的 arousal 表現視為語境分群因素，如同發音和語言因素。模型訓練時會根據其語音相
似性自動建立模型分群決策樹。因此可有效的共用不同 arousal 程度的語音單元之間相似的資料，降低
HMM 模型決策樹的容量。經過 HTS 工具的訓練，分別產生出 mel-cepstrum coefficients models、
state-duration models 和 log fundamental frequency models 並存放於系統中。 
 
■ 統計式文字分析模組 (Statistical Text Analysis Module) 
文字分析用於取得句子中的詞項序列。首先，每個句子都可以從字典中尋得數個可能的候選詞，
透過這些詞項可組出語句的詞彙連接網路，如下圖 
 
圖一、詞彙連接網路 
程度相符的參數模型。 
 
■ 語音參數產生模組 (Speech Parameter Generation Module) 
語音參數產生模組會根據語句中的每個發音單元建構出語句 HMM。發音單元的是由數個 (通常為
3~5 個) 狀態所建構，而各個狀態皆會持續一段時間，其是由持續時間模型所決定。接下來，再透過頻
譜與音高模型的決策樹走訪，可得到每個發音單元的各個狀態下的音框之觀察向量，其包含了頻譜與
音高的靜態與動態參數。最後，再以語音參數產生演算法產生此觀察向量之最大輸出機率的梅爾倒頻
譜係數和 log F0 值的序列。最後，所產生的語音參數將會透過語氣轉換函數轉換至特定之目標語氣。 
 
■ 整合特殊情境線索插入之語音波形重建模組 (Emotional Speech Waveform Reconstruction 
Module and Cue Insertion for Specific Scenario Technique) 
語音波形重建是使用語音合成濾波器 (Mel Log Spectrum Approximation, MLSA) 以二元脈波和雜
訊激發，利用語音參數產生模組所產生的梅爾倒頻譜係數和 F0 值合成語音波形。 
聲源濾波器模型一般是用來表示取樣的語音訊號，如圖二所示。轉移函數 H(z)建立發聲腔道結構
的模型。激發源是透過一個控制語音的有聲/無聲特性的開關來選擇。激發訊號的模型是以一個周期的
脈衝序列做為有聲語音，或是一個隨機雜訊序列做為無聲語音。要產生語音訊號 x(n)，模型的參數必
須隨著時間改變。激發訊號 e(n)經過時變線性系統 H(z)濾波來產生語音訊號 x(n)。 
 
 
圖二、具有脈衝/雜訊激勵的梅爾倒頻譜聲音編碼器。 
 
由於所產生的語音僅能粗略的區分為三種不同的 arousal 程度，因此必須透過特殊情境線索插入，
將語氣擴展至七種不同情緒語音，如表二。情境線索插入技術透過將語音中的副語言資訊插入至語音
的起始、韻律短語邊界、結尾等，目的是為了使聆聽者可明確的感知到合成語音所表達的情緒。為了
達到此目的，我們除了蒐集語者的語音作為聲學模型訓練外，還蒐集了數種不同的非語言人聲。合成
時，根據目標情緒種類以及合成語音，從資料庫中挑選適合用於串接的人聲單元進行串接。 
 
(五) 結果與討論 
本子計畫之最主要之貢獻在於提供一具有不同情緒風格之語音合成晶片。在第一年我們已完成溫
馨情緒語音合成演算法的設計與實現。在實驗上，可解度、自然度和音質 MOS 皆達到 4.5 以上，符合
預期之成果。此外，基於聆聽者的感知，合成語音的溫馨度 MOS 也可達 4 以上。相較於僅使用朗讀語
音之人機互動介面，使用者更偏好於具有溫馨情緒之合成語音。在第二年，我們將實現演算法的模組
加速設計，並於 ARM/FPGA 上功能驗證。 
 
  
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 ■未發表之文稿 □撰寫中 □無 
專利：□已獲得 ■申請中 □無 
技轉：□已技轉 ■洽談中 □無 
其他：（以 100 字為限） 
 
 已投稿一篇國際期刊論文 
“Synthesis of Spontaneous Speech with Syllable Contraction using State-Based 
Context-Dependent Voice Transformation” Journal of the Acoustical Society of America 
(submitted) 
"Personalized Spectral and Prosody Conversion using Frame-Based Codeword Distribution 
and Adaptive CRF," IEEE Transactions on Audio, Speech and Language Processing, Vol. 21, 
Issue. 1, Aug 2012, pp.51-62 (accepted) 
 已投稿兩篇國際會議論文 
"Cross-Lingual Frame Selection Method For Polyglot Speech Synthesis," Proceedings of The 
37th International Conference on Acoustics, Speech, and Signal Processing, 2012 
"Voice Conversion using Precise Speech Alignment based on Spectral Property and 
Eigen-Codeword Distribution," Proceedings of 7th ISCA Speech Synthesis Workshop, 2010 
 
三、 參考文獻 
[1] A. Black and N. Campbell, "Optimising Selection of Units from Speech Databases for Concatenative Synthesis," in Proc. of 
EUROSPEECH, pp.581–584, Sep. 1995. 
[2] A. Hunt and A.Black, "Unit Selection in a Concatenative Speech System using a Large Speech Database," in Proc. of ICASSP, 
1996, pp. 373-376 
[3] A. Iida, F. Higuchi, and N. Campbell, "A Corpus-based Speech Synthesis with Emotion," in Speech Communication, 2001, pp. 
161-187. 
[4] K. Tokuda, H. Zen, J. Yamagishi, T. Masuko, S. Sako, A. W. Black, and T. Nose, "The HMM-based Speech Synthesis System 
(HTS)," http://hts.sp.nitech.ac.jp/  
[5] K. Tokuda, T. Kobayashi, and S. Imai, "Speech Parameter Generation from HMM Using Dynamic Features," in Proc. of 
ICASSP, 1995, pp. 660-663. 
[6] T. Masuko, K. Tokuda, T. Kobayashi, and S. Imai, "Speech Synthesis from HMMs Using Dynamic Features," in Proc. of 
ICASSP, 1996, pp. 389-392. 
[7] T. Yoshimura, K. Tokuda, T. Masuko, T. Kobayashi, and T. Kitamura, "Duration Modeling in HMMbased Speech Synthesis 
System," in Proc. of ICSLP, 1998, pp. 29-32. 
[8] T. Yoshimura, K. Tokuda, T. Masuko, T. Kobayashi, and T. Kitamura, "Simultaneous Modeling of Spectrum, Pitch and 
Duration in HMM-based Speech Synthesis," in Proc. of EUROSPEECH, 1999, pp. 2347-2350. 
[9] K. Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi, and T. Kitamura, "Speech Parameter Generation Algorithms for 
HMM-based Speech Synthesis," in Proc. of ICASSP, pp. 1315-1318, 2000 
[10] K. Tokuda, H. Zen, and A. W. Black, "An HMM-based Speech Synthesis System Applied to English," in IEEE Speech 
Synthesis Workshop, 2002. 
[11] T. Masuko, "HMM-Based speech synthesis and its applications", Doctoral dissertation, Tokyo Institute of Technology, pp. 
34-35, November 2002, http://www.kbys.ip.titech.ac.jp/masuko/masukodoctor.pdf 
[12] H. Zen, T. Toda, "An overview of Nitech HMM-based speech synthesis system for Blizzard Challenge 2005," in Proc. of 
Interspeech, 2005. 
[13] Alan W Black, Heiga Zen, Keiichi Tokuda, "Statistical parameter speech synthesis," in Proc. of ICASSP, 2007 
[14] H. Zen, T. Nose, J. Yamagishi, S. Sako, T. Masuko, A. W. Black, and K. Tokuda, "The HMM-based Speech Synthesis System 
Version 2.0," in Proc. of ISCA, vol. SSW6, 2007, pp. 294-299. 
[15] S. Krstulović, A. Hunecke, and M. Schröeder, "An HMM-based Speech Synthesis System Applied to German and its 
Adaptation to a Limited Set of Expressive Football Announcements," in Proc. of Interspeech, 2007. 
[16] X. Gonzalvo, I. Iriondo, J. C. Socoró, F. Alías, and C. Monzo, "HMM-based Spanish Speech Synthesis using CBR as F0 
Estimator," in ITRW on NOLISP, 2007. 
[17] J. Yamagishi and S. King, "Simple methods for improving speaker-similarity of HMM-based speech synthesis," in Proc. of 
ICASSP, Dallas, TX, pp. 4610-4613, March, 2010 
[18] S. Tiomkin, D. Malah, and S. Shechtman, "Statistical Text-to-Speech Synthesis Based on Segment-Wise Representation With a 
Norm Constraint," in IEEE Tran. on Audio, Speech, and Language Processing, vol. 18, pp. 1077-1082, July, 2010 
[19] A. Iida, N. Campbell, F. Higuchi and M. Yasumura, "A corpus-based speech synthesis with emotion," in Speech 
Communication, 40(1-2): 161-187, 2003. 
[20] 陳俊甫, "應用機率式句法結構與隱含式語意索引於情緒語音合成之單元選取," 國立成功大學資訊工程研究所碩士論
文, 2004. 
[21] M. Abe, S. Nakamura, K. Shikano and H. Kuwabara, "Voice conversion through vector quantization," in Proc. of ICASP, New 
Youk, NY, USA, pp. 655-658, Apr. 1988. 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/31
國科會補助計畫
計畫名稱: 子計畫一：溫馨情緒語音合成回饋SOC模組設計與實現(I)
計畫主持人: 王駿發
計畫編號: 100-2221-E-006-089- 學門領域: 積體電路及系統設計
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
