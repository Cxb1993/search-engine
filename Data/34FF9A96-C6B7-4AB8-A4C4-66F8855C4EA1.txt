 1
行政院國家科學委員會專題研究計畫成果報告 
應用於分散式視訊監控之智慧型視訊處理與網路傳輸技術- 
子計畫二：計算可調性視訊編碼及分散式視訊編碼(3/3) 
計畫編號：NSC 95-2221-E-194-020-MY3 
執行期限：97 年 8 月 1 日至 98 年 7 月 31 日 
主持人：柳金章   執行機構及單位名稱: 國立中正大學資訊工程學系 
計畫參與人員：鄭茗徽，林承賢，楊博羽，辛恆豐 
 
一、 中文摘要 
 
現今的視訊編碼標準無論是 MPEG 或
H.26x 系統，其大部分的複雜運算皆集中在編
碼端，編碼端所需的運算複雜度約為解碼端
的數倍，而且運動估測(motion estimation, ME)
更佔了約 80％的運算量，此一不對稱編解碼
架構較適合於只需壓縮編碼一次而需解碼數
次的應用上，諸如 broadcasting 或 streaming 
video-on-demand。但隨著多元的網路環境以
及低運算能力之視訊擷取裝備的普及，在新
的網路視訊環境中，視訊編碼端可由多個具
有較低運算能力之視訊擷取裝置組成，編碼
後將資料傳送給解碼端進行較複雜之運算將
成為新趨勢。本研究提出一種新興的視訊編
解碼架構，稱為分散式視訊編碼 Distributed 
Video Coding (DVC)，DVC 是根據 Slepian 與 
Wolf 及 Wyzer 與  Ziv 等 4 位所導證的
information-theoretic 結果架構的。對於未來的
低運算能力之裝置，當可供運用的運算量低
於一定程度後，傳統視訊編碼標準之視訊運
動估測的調節方法將因運算量不足，而仍無
法有效完成編碼。在 DVC 架構中，編碼複雜
度將大幅減化，並將大部分的運算從編碼端
轉至解碼端進行成為一個可行的方法。在本
研究中，我們將提供一種建立旁解碼資訊 
(side information)的方法，並研究相關的技術。 
 
關鍵詞：H.264/AVC 視訊編碼、分散式視訊
編碼、旁解碼資訊。 
 
Abstract 
 
Within existing video compression 
standards, such as MPEG-2, MPEG-4, H.263, 
and H.264, the computation complexity of the 
encoder is higher than that of the decoder. The 
full search ME (motion estimation) procedure 
may consume over 80% of the computation 
power of the encoder. The asymmetric structure 
of the encoder and the decoder is more suitable 
for broadcasting or streaming video-on-demand. 
However, some applications may require the 
dual system, i.e., the low-complexity encoder 
and the high-complexity decoder. Illustrated 
examples include wireless video sensors for 
surveillance, wireless IP cameras, mobile 
camera phones, and networked camcorders. In 
this study, a new video coding scheme, namely, 
distributed video coding (DVC) is proposed. 
Distributed coding is a new paradigm for video 
coding (compression), based on Slepian and 
Wolf’s and Wyner and Ziv’s 
information-theoretic results. Wyner-Ziv coding 
(one practical distributed video coding scheme) 
having lossy compression with side information, 
enables lower-complexity video coding where 
the bulk of the computation at the encoder is 
shifted to the decoder. In this study, a new side 
information generation scheme for distributed 
video coding (DVC) is proposed. 
 
Keywords: H.264/AVC video coding, 
distributed video coding, side information. 
 
二、緣由與目的 
 
在網路傳送的多媒體資訊中，由於視訊
的資料量很大，但網路傳輸的頻寬非常有
 3
其他影像屬之，相當於傳統壓縮標準中的 P-
影像與 B-影像，但是其編碼時並無法參考 key 
frames 進 行 預 測 性 編 碼 ， 而 是 單 獨 對
Wyner-Ziv frame 進行量化後，使用無失真的
Slepian-Wolf 編碼對量化後的影像進一步壓
縮，此一量化與進行 Slepian-Wolf 無失真編碼
的一聯串運作即成為 Wyner-Ziv 失真編碼架
構，當完成一連串之編碼動作後，即將壓縮
碼獨立傳送給解碼端。 
解碼端使用傳統 Intraframe coding獨立對
key frames 進行解碼處理，而 Wyner-Ziv 影像
則使用 Slepian-Wolf 編解碼方式解碼，解碼時
會配合參考獨立解碼後的 key frames、先前已
經完成Slepian-Wolf解碼之重建Wyner-Ziv影
像或使用前後張 key frames 以較複雜的時間
軸內插方式產生一張預測影像，進行聯合解
碼的動作，這些協助 Slepian-Wolf 解碼的影像
即稱為 side information。整個參考協助解碼的
過程就類似於傳統視訊編碼時的 ME 搜尋處
理[29, 30]，只是在 DVC 解碼端整個參考預測
的過程並無法使用原始的影像，故其效率約
介於傳統 Intraframe coding (Motion JPEG)與
傳統視訊編碼 Interframe coding 之間，品質上
也還比不上現今視訊壓縮標準如 MPEG-2 與
H.264 等，因此如何有效改善此部分的效率成
為一有趣的課題。本研究在 Distributed Video 
Coding 架構下，提出一種可改善 side 
information 品質的方法，以改善 DVC 最終編
解碼後的影像品質。 
 
三、研究方法與成果 
 
3.1 Overview 
今日的視訊編碼標準無論是 ISO 的
MPEG 或 ITU-T 的 H.26x，其大部分的複雜運
算皆集中在編碼端，編碼所需的運算複雜度
約為解碼端的 5~10 倍，其中 motion estimation
（ME）更佔了約 80％的運算量，此一不對稱
編解碼架構較適合於只需壓縮編碼一次而需
解碼數次的應用上，如 broadcasting 或
streaming video-on-demand。有鑑於多元的網
路環境以及低運算能力之視訊擷取裝備的普
及，未來一個新的視訊網路環境，在視訊擷
取端可由多個低運算能力之視訊擷取裝置組
成，當擷取視訊後，各視訊擷取裝置可先利
用本身的處理器對視訊進行編碼，編碼後透
過網路將資訊傳送給解碼端，而解碼端則由
擁有較強運算能力的電腦進行視訊解碼。
Distributed Video Coding (DVC)為近年來在此
低編碼運算高解碼運算複雜度的條件下新興
的視訊壓縮研究課題。Distributed coding 主要
是由根據 Slepian 與 Wolf’s 理論及 Wyner 與
Ziv’s 理論所發展而來的編碼方式所組成
[18-27]。Slepian-Wolf 理論將引領出一種無失
真的編解碼方式，而 Wyner-Ziv 理論則為失真
編解碼方式的基礎。 
對 於 兩 筆 具 independent identically 
distributed (i.i.d.)特性的資料 X 與Y ，分別單
獨進行編解碼，則 )(XHRX ≥ 且 )(YHRY ≥ ，
其 中 )(XH 與 )(YH 分 別 代 表 X 與 Y 的
entropy。依 Slepian-Wolf 理論，如果資料 X 與
Y 在編解碼時兩者分別單獨進行，其中資料
X 必須以無失真的編解碼方式進行編碼，而
資料Y 可以採用任何可行的編解碼方式進行
編解碼，但在解碼時， X 必須與重建後 Yˆ 的
進行聯合解碼，如此 X 解碼後所得的重建 Xˆ
品質可以比無 Yˆ 進行聯合解碼的情況下有更
好的效果(如 Fig. 1)，對於提供給 X 進行聯合
解碼的 Yˆ 稱之為 side information。 
目前 Distributed source coding 技術大多
使用已驗證過的 channel coding 技術，並用來
實現 Slepian-Wolf 理論，常見的編碼方法有
turbo code 與 low-density parity check code 等
[19, 23-26]，其對於具相關性的非二元訊號與
高斯分佈訊號具有很不錯的編碼效果，而在
解碼時通常需要具迴復式的解碼架構支援，
如 Bayesian network 與 Viterbi 解碼器都是可
運用的解碼架構。 
Wyner 與 Ziv 兩位學者延伸 Slepian-Wolf
 5
頻率域方法在編碼端會有較高的編碼複雜
度，為了簡化編碼端的運算量，本計畫在空
間域 DVC[18, 30]架構下，發展一可有效改善
解碼端 side information 品質的技術，藉由此
較佳的 side information 來協助最後解碼的重
建影像品質。空間域 DVC 處理時會將視訊
frames 區分為兩類：”key frame”與”Wyner-Ziv 
frames” (如 Fig. 5)，key frame 12 −iX 或 12 +iX 就
相當於傳統視訊編碼系統中的 Intra-Prediction 
(I)-影像，其處理時採用傳統 Intraframe coding
單獨進行編解碼，解碼後的重建影像除了輸
出外，亦會成為 Wyner-Ziv frames 解碼時的參
考影像，即所謂的 side information。視訊影像
中除了 key frame以外，其他的都是Wyner-Ziv 
frame iX 2 ，其特性類似傳統視訊編碼中的
Prediction (P)-影像或 Bidirectionally predicted 
(B)-影像。在 DVC 架構中，於編碼端每一
Wyner-Ziv frame 並不會參考其他影像進行預
測，而是直接對每一 Wyner-Ziv frame 進行量
化後，利用 turbo code 對量化後的數值 iq2 進
行編碼，編碼後僅會產生少量的冗餘碼 
(parity bits) ip2 作為傳送資料。當解碼端收到
這些冗餘碼後，會以獨立編解碼後重建的影
像 12' −iX 與 12' +iX 對 side information 進行預
測，預測得到的 side information iY2 ，則可利
用編碼端送來冗餘碼對最後的重建影像 iX 2'
進行修正與改善，此一編解碼架構可大幅降
低編碼端的運算複雜度，讓大部分的運算移
至解碼端。 
對於解碼端 side information 的建立，通
常是運用前後兩張 key frames，以時間軸內插
(temporal interpolation)方法預測而來。時間軸
內插方法在預測 side information 時，會使用
BMAs 進行 ME，找尋前後兩張 key frames 相
對應 blocks 的移動資訊，而 block distortion 
measure 通常是使用傳統視訊編碼 ME 中常用
的 SAD 或 MAD 來作為找尋最相似 block 的
依據。只是，傳統視訊編碼中的 ME 與時間
軸內插方法中的 ME，兩者的目的並不盡相
同。傳統視訊編碼使用最小 SAD 進行 ME 的
目的，是為了找到相減後餘值最小者，以期
經過後續的 entropy coding 編碼處理可以得到
最佳的壓縮率。而時間軸內插方法主要的目
標則是找到視覺上最相似 block，以精確估算
出該 block 真正的移動向量 motion vector 
(MV)，進而內插出正確的影像。採用 SAD 作
為時間軸內插方法找尋最相似 block 的依據，
在 block 含有複雜 edges 與 textures 時，通常
無法找到 block 的真正移動向量。因此，本研
究提出一個以分類為基礎的 ME 方法，來找
尋較為可靠的移動向量，最後在以所提的
multiple block motion interpolation 來建立出一
張視覺品質較佳的 side information，並以此協
助最後解碼出品質較好的重建影像。 
 
3.2 Classification-Based Side Information 
Generation 
使用時間軸內插方法時，對於每一個位
於 key frame 12' +iX 的 block 會到 key frame 
12' −iX 利用 ME 的方式找尋各 block 的移動向
量，而 ME 時通常是採用最小 SAD 值作為
BMAs 找尋最相似 block 的依據。 side 
information iY2 的建立，則是把每個相對應的
block 以 1/2 的移動向量值，貼在兩個 key 
frames 時間軸 1/2 的位置。只是採用最小 SAD
值，在 block 含有複雜 edges 與 textures 時，
通常無法找到 block 的真正移動向量。在本方
法中，會對每個要進行 ME 的 block 進行分
類，在 ME 時，則會在相同類別之 blocks 中，
找尋最相似的 block，藉此找到視覺上相似度
最高的 block。在 side information 建立時，為
了對部分未貼補的空隙填補，本研究中則採
用 multiple block motion interpolation 方法填
補，以提升視覺影像品質。 
本研究所提的架構如 Fig. 6 所示，對於
key frame中所要處理的block，會先用Discrete 
Cosine Transform (DCT)將其轉換到頻率域
中。 
 7
本研究會以簡單的前後 key frame 相對應位置
pixels 取平均後的值，來進行貼補。經由以上
步驟，期望能達到一張視覺影像較佳且對最
後解碼之重建影像有幫助的 side information。 
 
四、結論與討論 
 
根據實驗結果 Figs. 7-12 顯示，本計畫所
提方法在各種情況下均優於比較的現有方法
[30]，這證明所提方法的可適性。 
 
六、計畫成果自評 
 
本研究內容與原計畫書內容相符，預期
成果及目標均已達成，本研究成果計有兩篇
碩士論文[36, 37]、一篇已發表的會議論文
[38]。 
 
七、參考文獻 
 
[1] K. N. Ngan, C. W. Yap, and K. T. Tan, Video 
Coding for Wireless Communications. New 
Jersey: Prentice Hall, 2002. 
[2] Y. Wang, J. Ostermann, and Y. Q. Zhang, Video 
Processing and Communications. New Jersey: 
Prentice Hall, 2002. 
[3] A. M. Tekalp, Digital Video Processing. New 
Jersey: Prentice Hall PTR, 1995. 
[4] M. T. Sun and A. R. Reibman, Compressed 
Video over Networks. New York: Marcel Dekker, 
2001. 
[5] J. L. Mitchell, W. B. Pennebaker, C. E. Fogg, 
and D. J. LeGall, MPEG Video Compression 
Standard. New York: Chapman & Hall, 1997. 
[6] Video codec for audiovisual services at p×64 
kbit/s, CCITT Recommendation H.261, 1990. 
[7] Video coding for low bit rate communication, 
ITU-T Recommendation H.263, May 1998. 
[8] ITU-T Q.6/SG16, “H.26L Test Model Long 
Term Number 9 (TML-9) draft0,” Dec. 2001. 
[9] V. Lappalainen, A. Hallapuro, and T. D. 
Hamadinen, “Optimization of emerging H.26L 
video encoder,” in Proc. of IEEE Workshop on 
Signal Processing, Sept. 2001, pp. 406-415. 
[10] A. Hallapuro, V. Lappalainen, and T. D. 
Hamadinen, “Performance analysis of low bit 
rate H.26L video encoder,” in Proc. of IEEE 
Int. Conf. on Acoustics, Speech, and Signal 
Processing, May 2001, vol. 2, pp. 1129-1132. 
[11] H.264, Draft ITU-T Recommendation and 
Final Draft International Standard, Pattaya, 
Thailand, 2003. 
[12] A. N. Netravali and J. D. Robbins, “Motion 
compensated television coding: Part I,” Bell 
Syst. Tech. J., vol. 58, pp. 631-670, Mar. 1979. 
[13] ISO/IEC JTC1/SC29/WG11 N3908, “MPEG-4 
video verification model V18.0,” Jan. 2001. 
[14] ISO/IEC JTC1/SC29/WG11 N4668, “MPEG-4 
overview,” March 2002. 
[15] E. G. Richardson, H.264 and MPEG-4 Video 
Compression. 1st Edition, Aberdeen:Wiley, 
2003. 
[16] S. Zhu and K. K. Ma, “A new diamond search 
algorithm for fast block-matching motion 
estimation,” IEEE Trans. on Image Processing, 
vol. 9, no. 2, pp. 287-290, Feb. 2000. 
[17] P. I. Hosur and K. K. Ma, “Report on 
performance of fast motion using motion vector 
field adaptive search technique,” ISO/IEC 
JTC1/SC29/WG11 M5453, Dec. 1999. 
[18] B. Girod, A. Aaron, S. Rane and D. Rebollo- 
Monedero, "Distributed video 
coding,"  Proceedings of the IEEE, Special 
Issue on Video Coding and Delivery, vol. 93, 
no. 1, pp. 71-83, January 2005. 
[19] A. Aaron and B. Girod, “Compression with 
side information using turbo codes,” in 
Proc. IEEE Int. Conf. on Data 
Compression, Snowbird, UT, Apr. 2002, pp. 
252-261. 
[20] A. Aaron, E. Setton, and B. Girod, 
“Towards practical Wyner-Ziv coding of 
video,” in Proc. IEEE Int. Conf. on Image 
Processing, Barcelona, Spain, Sept. 2003, 
pp. 869-872. 
[21] A. Aaron, S. Rane, E. Setton, and B. Girod, 
“Transform-domain Wyner-Ziv codec for 
video,” in Proc. SPIE Visual 
Communications and Image Processing, 
vol. 5308, San Jose, CA, Jan. 2004, pp. 
520-528. 
[22] J. D. Slepian and J. K. Wolf, “Noiseless coding 
of correlated information sources,” IEEE Trans. 
Inf. Theory, vol. IT-19, pp.471-480, Jul. 1973. 
[23] J. Garcia-Frias, “Compression of correlated 
binary sources using turbo codes,” IEEE 
Commun. Lett., vol. 5, no. 10, pp. 417-419, Oct. 
2001. 
[24] R. Gallager, “Low-density parity-check 
codes,” IEEE Trans. on Information 
Theory, vol. 8, pp. 21-28, Jan. 1962. 
[25] D. Varodayan, A. Aaron, and B. Girod, 
“Rate-adaptive distributed source coding 
using low-density parity-check codes,” 
in Proc. Asilomar Conf. on Signals and 
Systems, Pacific Grove, CA, Nov. 2005, pp. 
 
Fig. 1. Distributed compression of two statistically dependent data. The decoder jointly decodes X and Y. 
 
 
 
 
 
Fig. 2. Practical Wyner-Ziv coder is obtained by cascading a quantizer and a Slepian-Wolf encoder. 
 
 
 
 
 
 
Fig. 3. Low-complexity pixel domain distributed video encoder and corresponding decoder. 
 
 
 
 11
32
33
34
35
36
200 300 400 500 600 700 800 900
rate (kbps)
PSNR (dB) Intraframe coding SMV Proposed scheme
 
Fig. 7. The average PSNR of the “Foreman” sequence for three comparison schemes, namely, intraframe coding, SMV 
coding, and the proposed scheme. 
 
 
 
(b) (c) 
 
(a) 
 
 (d) (e) 
Fig. 8. The original, interpolated, and reconstructed frames of the 6th frame of the “Foreman” sequence with rate = 500 
kbps: the original frame (a); the side information by SMV and the proposed scheme (b), (c); the reconstructed frames 
by SMV and the proposed scheme (d), (e), respectively. 
 
 
 
(b) (c) 
 
(a) 
 
 (d) (e) 
Fig. 9. The original, interpolated, and reconstructed frames of the 74th frame of the “Foreman” sequence with rate = 
480 kbps: the original frame (a); the side information by SMV and the proposed scheme (b), (c); the reconstructed 
frames by SMV and the proposed scheme (d), (e), respectively. 
 13
Table 1. The encoding time (ms) for the first 100 H.264 video frames of the “Akiyo,” “Miss_AM,” ” Foreman,” 
“Carphone,” and “Coastguard” sequences. 
 
 
 Akiyo Miss_AM Foreman Carphone Coastguard average 
ME time (ms) 26771 27235 33154 31451 36748 31071.8 
Total time (ms) 32876 34891 41811 39188 46939 39141 
Ratio (%) 81.43 78.06 79.29 80.26 78.29 79.38 
 2 
本次會議討論的議題眾多，其中與本人研究較相關的主題(Sessions)，包
括： 
(1) Bio-multimedia Signal Processing 
(2) Joint Source & Channel Coding 
(3) Video Streaming 
(4) Medical Imaging & Graphics 
(5) Multimedia Content Management 
(6) Multimedia Networking and Secuity 
(7) Multimedia Systems 
(8) Multimedia HCI 
(9) Hybrid Multimedia Processing 
(10) Global Motion Estimation 
(11) Mosaicing for Video Analysis and Coding 
(12) Online Video Applications 
 
此次學術會議除 regular sessions 外, 另外找到數位專家組成 3個 special 
sessions: (1) Search and Beyond Search: Online Video Applications, (2) Immer-
sive Tele-Collaboration, and (3) Global Motion Estimation and Mosaicing for Ap-
plications in Video Analysis and Coding , 提供在 Multimedia Signal Processing
新的研究方向及研究題材, 參與之後對未來的研究方向及研究題材有很大的
參考價值。参加這次會議的學者.專家.研究人員超過400人, 包含全世界有關 
Multimedia Signal Processing許多重要的重量級學者專家. 這次參加的我國學
者、專家，除本人外,尚有中研院資訊所廖弘源教授, 清大電機系林嘉文教授, 
清大資工系許秋婷教授, 交大資工系蕭旭峰教授, 中山大學葉家宏教授,  
 
 4 
未來能爭取更多的國際學術會議在國內舉辦, 再一次提升我國在此領域之國
際地位及影響力. 
國際標準組織 (ISO) 所屬關於通訊、電腦系統、多媒體音/視訊壓縮標準
的技術會議，我國學者常由於非聯合國會員國身份而無法參加，這對我國在
這一方面的研究有很大的影響。因此參加這些相關的年會及國際會議除了擷
取最新技術外，另一個重要任務即是與國外著名學者和技術核心人物相互交
流，進而瞭解未來多媒體及相關技術的趨勢，搜集目前最新的相關技術資料
或訊息。 
三、考察參觀活動 
乘著這一次參加MMSP2008會議之便，本人與中研院資訊所廖弘源教授, 
清大電機系林嘉文教授, 清大資工系許秋婷教授, 交大資工系蕭旭峰教授, 中
山大學葉家宏教授,及幾位同行的研究生一同參觀了Cairns外海之名勝: 大堡
礁, 讓我們在參加會議之餘, 可以飽覽 Cairns的海岸風光。 
 
四、建議 
我國每年均補助大批學者參與國際學術會議，教育部也大力鼓吹博士班
學生出席國際學術會議，因此將原用於補助教師出席國際學術會議的經費刪
除，改為博士生出國開會的經費，這雖可以改善博士生國際學術交流的質量，
開拓他們的視野，但也間接地影響到教師出席國際會議經費的申請，希望教
育部能考慮恢復教師出席國際會議的補助申請。這次本人出席MMSP2008 
(2008年IEEE國際多媒體訊號處理學術會議)，承蒙國科會透過專題研究計畫
的經費補助，使本人能藉此機會與全球各國音訊/視訊/影像/圖學及多媒體信
號處理相關的專家、學者、工業界的相關主管齊聚一堂交換學術研發心得，  
從中得到最新音訊/視訊/影像/圖學及多媒體信號處理關的最新知識及訊息，
在此表達謝意。 
 
Image Interpolation Using Visual Attention Model 
and Particle Swarm Optimization 
Hsuan-Ying Chen 1, Jin-Jang Leou 2 
 Department of Computer Science and Information Engineering 
National Chung Cheng University  
Chiayi, Taiwan 621, Republic of China 
1 
chenhy@cs.ccu.edu.tw 
2 jjleou@cs.ccu.edu.tw 
 
Abstract—In this study, a new edge-directed image 
interpolation approach using visual attention model and particle 
swarm optimization (PSO) is proposed. First, a high-quality 
saliency map of an image to be interpolated is generated by the 
proposed visual attention model in an effective manner. Then, 
based on the saliency map, bilinear interpolation and the 
proposed PSO interpolation are employed for non-saliency 
blocks (non-ROIs) and saliency blocks (ROIs), respectively, to 
obtain the final interpolation results. The proposed approach is 
applicable for image interpolation with arbitrary magnification 
factors (MFs). Based on the experimental results obtained in this 
study, the interpolation results of the proposed approach are 
better than those of three comparison methods. 
I. INTRODUCTION 
Image interpolation plays an important role in many 
applications, such as digital zooming and high definition TV. 
In general, there are two kinds of image interpolation methods, 
namely, non-edge-directed and edge-directed methods. Non-
edge-directed image interpolation methods, such as bilinear 
and bicubic interpolations, have low computational 
complexity, whereas they will produce artifacts and image 
blurring in edge regions. To cope with edge blurring, many 
edge-directed interpolation methods are proposed. Li and 
Orchard [1] proposed an edge-directed interpolation algorithm 
to estimate local covariance coefficients from a low-resolution 
image and used the covariance coefficients to adapt each 
higher resolution interpolation based on the geometric duality 
between low-resolution and high-resolution covariances. 
Magnification factors (MFs) of edge-directed interpolation 
methods [1] are usually limited to the power of 2. Recently, 
edge-directed interpolation algorithms with arbitrary MFs are 
proposed. Hwang and Lee [2] proposed adaptive interpolation 
methods by applying an inverse gradient to bilinear and 
bicubic interpolations. Cha and Kim [3] proposed the error-
amended sharp edge (EASE) scheme to amend the 
interpolation error by using the classical interpolation error 
theorem in an edge-adaptive fashion. Although edge-directed 
interpolation methods have better interpolation results, they 
need higher computational complexity to detect image edges 
and perform different interpolations for different types of 
edges. 
Human perception tends to firstly pick attended regions, 
which correspond to prominent objects in an image. Visual 
attention analysis simulates the behavior of the human visual 
system by automatically producing saliency maps of the 
image and detects regions of interest (ROIs) in the image. Itti 
et al. [4] proposed a visual attention model using intensity, 
color, and orientation features, whereas Zhai and Shah [5] 
used the color histogram of an image to create its saliency 
map. 
In this study, a new edge-directed image interpolation 
approach using visual attention model and particle swarm 
optimization (PSO) is proposed. First, a high-quality saliency 
map of an image to be interpolated is generated by the 
proposed visual attention model in an effective manner. Then, 
based on the saliency map, bilinear interpolation and the 
proposed PSO interpolation are employed for non-saliency 
blocks (non-ROIs) and saliency blocks (ROIs), respectively, 
to obtain the final interpolation results. The proposed 
approach is applicable for image interpolation with arbitrary 
MFs. 
II. VISUAL ATTENTION MODEL 
A contrast image I of size HW × can be obtained from a 
corresponding contrast visual signal, which can be partitioned 
into image blocks jiB , of size hw× . Then for the standard 
deviation representation map I ′ , each block can be expressed 
by its standard deviation: 
,)/())),(((
1
0
1
0
2
,,, hwyxB
w
x
h
y
jijiji ×−= ∑∑−
=
−
=
μσ  (1) 
where ji,μ  is defined as 
,)/()],([
1
0
1
0
,, ∑∑−
=
−
=
×=
w
x
h
y
jiji hwyxBμ  (2) 
(x, y) is the coordinate of the map and (i, j) is the block index. 
A block having higher standard deviation will contain 
distinctive features, which will attract more attention, and vice 
versa. I ′  can be normalized as follows: 
,/255),(),( MaxyxSyxS ×′′=′  (3) 
where 
⎩⎨
⎧ >′−′
=′′
        otherwise,                    ,0 
,),( if   ,),(
),( ll
TyxITyxI
yxS  (4) 
+ This work was supported in part by National Science Council, Taiwan, 
Republic of China under Grants NSC 95-2221-E-194-020-MY3 and NSC 96-
2221-E-194-033-MY3. 
978-1-4244-2295-1/08/$25.00 © 2008 IEEE MMSP 2008218
B. PSO Training 
In this study, bilinear interpolation and the proposed PSO 
interpolation are employed for non-saliency blocks (non-ROIs) 
and saliency blocks (ROIs), respectively. Some subsampling 
images with MF=2×2 are used to determine the interpolation 
filtering masks by PSO training, which can be used for 
arbitrary MFs. 
Here, each 55×  image pattern is transformed into a 25-
dimensional (25-D) training data and 25-D PSO training will 
be applied on each edge category. Suppose there are N image 
patterns (Ps) in a category and the population size of PSO is S. 
Each particle (x) initially has 25-D random integer values 
identically within the range (0, 5) as the candidate weights in 
each interpolation mask. The fitness function FV is defined as 
,|)/)((|
1
0
24
0
∑ ∑−
= =
⋅−=
N
m d
i
d
i
d
mmi SVxPOVFV  (10) 
where dix  is the position of particle i at dimension d, i=0, 1, 
2, …, S-1, OV is the corresponding pixel value to be 
interpolated (H or V) of pattern m from the original image, and 
SVi is the sum of 25-D values of xi. The global best is 
determined as the particle having the minimum FV in the 
population. Eqs. (6) and (7) are iterated until the global bests 
are the same in the last ten iterations. Then the 25-D integer 
values of the global best particle at the final iteration is treated 
as the candidate interpolation filtering weights in the category. 
In this study, ten training experiments are done for each 
category. For example, the PSO training results for the 25 
interpolation filtering weights of horizontal o45  interpolation 
are shown in Table I. Finally, an interpolation filtering weight 
is set to 1 if the sum of interpolation filtering weights for the 
ten training experiments is conspicuously greater than the 
others. Based on PSO training results, the eight filtering 
masks for horizontal interpolation are: 
,
00000
00000
01100
00000
00000
 ,
00000
00000
01100
00000
00000
,
00000
01000
00000
00100
00000
 ,
00000
00100
00000
01000
00000
,
00000
00000
00100
00100
00000
 ,
00000
00100
01100
01000
00000
 ,
00000
01100
01100
01100
00000
 ,
00000
00000
01100
00000
00000
5.1575.22
5.1125.67
13545
900
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
=
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
=
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
=
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
=
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
=
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
=
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
=
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
=
oo
oo
oo
oo
HH
HH
HH
HH
II
II
II
II
 
(11) 
 
 
 
and the eight filtering masks for vertical interpolation are: 
.
00000
01100
00110
00000
00000
 ,
00000
00110
01100
00000
00000
,
00000
00100
00100
00000
00000
 ,
00000
00100
00100
00000
00000
,
00000
00100
00100
00000
00000
 ,
00010
00100
00100
01000
00000
 ,
00000
00100
00100
00000
00000
 ,
00000
00100
00100
00000
00000
5.1575.22
5.1125.67
13545
900
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
=
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
=
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
=
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
=
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
=
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
=
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
=
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
=
oo
oo
oo
oo
VV
VV
VV
VV
II
II
II
II
 
(12) 
C. PSO Interpolation 
In this study, bilinear interpolation is employed for non-
saliency blocks and the proposed PSO interpolation is 
employed for saliency blocks. If the pixel to be interpolated 
indeed locates within an edge region, it will be interpolated 
according to its edge type. 
Let g, E, and G be the interpolated image, the original low-
resolution image, and the interpolation function, respectively. 
 ),/,/(),( mymxGyxg = where m is the magnification factor 
(MF). Suppose that 55 ×  original (known) neighboring pixels 
centered at the known pixel (i,j) (called pattern A) are chosen 
in O, where ⎣ ⎦ ⎣ ⎦ ,2/  ,2/ +=+= myjmxi O is obtained from 
E with two boundary padding pixels, 
,
2,22,12,2,12,2
1,21,11,1,11,2
,2,1,,1,2
1,21,11,1,11,2
2,22,12,2,12,2
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
=
++++++−+−
++++++−+−
++−−
−+−+−−−−−
−+−+−−−−−
jijijijiji
jijijijiji
jijijijiji
jijijijiji
jijijijiji
ooooo
ooooo
ooooo
ooooo
ooooo
A  (13) 
and ⎣ ⎦z  denotes the floor of z. Let s=(x mod m)/m and t=(y 
mod m)/m. The PSO interpolation is realized as: 
⎪⎪⎩
⎪⎪⎨
⎧
≠=
≠=
==
=
             
       
       
      
otherwise.),,(
,0  and  0 if),(
,0  and  0 if),(
0,  and  0 if,
)/,/(
,
tsG
tstG
stsG
tso
mymxG
D
V
H
ji
 
(14) 
GH(s), GV(t), and GD(s,t) are filtering outputs for horizontal, 
vertical, and diagonal interpolations. 
For horizontal interpolation, as shown in Fig. 3(a), let e 
denote the direction having the maximum normalized filtering 
output for horizontal edge detection. Let L be a 5×5 matrix 
with the components 
⎩⎨
⎧
⋅
≤⋅−
=
,otherwise       ,
 ,  if ,)1(
 
,
,
,
qp
qp
qp hs
iphs
l  (15) 
220
