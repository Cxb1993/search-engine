 
行政院國家科學委員會補助專題研究計畫 ▓成果報告   
□期中進度報告 
 
網路環境下之嵌入式軟體的自動化測試理論與技術 
 
計畫類別：▓個別型計畫   □整合型計畫 
計畫編號：NSC   97-2221-E-002-129-MY3 
執行期間：  2008 年 8 月 1  日至 2011  年 7 月 31  日 
 
執行機構及系所：台灣大學電機系 
 
計畫主持人：王凡 
共同主持人： 
計畫參與人員：吳榮軒、姚力瑋 
 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  ▓完整報告 
 
本計畫除繳交成果報告外，另須繳交以下出國心得報告： 
▓赴國外出差或研習心得報告 
□赴大陸地區出差或研習心得報告 
□出席國際學術會議心得報告 
□國際合作研究計畫國外研究報告 
 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
        ▓涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
 
中   華   民   國  100  年  12  月  20  日 
 
 
 
 
Intelligent Test Oracle Construction for Reactive Systems 
              without Explicit Speciﬁcations 
Farn Wang1,2 Li-Wei Yao1 Jung-Hsuan Wu1 
  Dept. of Electrical Engineering, National Taiwan University1 
Grad. Inst. of Electronic Engineering, National Taiwan University2 
                  farn@cc.ee.ntu.edu.tw 
Library InTOL is available at https://sourceforge.net/projects/intol/. 
   Abstract—A test oracle is a mechanism that decides whether 
an SUT (software under test) fails or passes a test case. 
We investigate how to use machine learning techniques to 
automatically construct test oracles for such reactive programs 
without reliance on explicit speciﬁcations. Firstly, we present a 
library, called InTOL (Intelligent Test Oracle Library), for the 
convenient and ﬂexible collection of test traces. We can ﬂexibly 
use either user guidance or program assertions to collect 
verdicts to test traces. Such verdicts are used as supervisory 
signals to the supervised learning algorithm (SLA) for a test 
oracle. Secondly, we present several sets of feature variables for 
the temporal relation among events in test traces of unbounded 
lengths. Then we present procedures that convert test traces 
into feature vectors, train an SLA with the feature vectors 
and their verdicts, and use the trained SLA as a test oracle. 
We report the implementation of InTOL on top of SVM 
(support vector machine). We experiment with two open-source 
benchmark SUTs from the internet to check the performance of 
our techniques. Our experiment data shows that high-accuracy 
test verdicts can be achieved with our test oracles for the 
benchmark SUTs. 
   Keywords-test oracle, software testing, AI, SVM, machine 
learning 
The work is partially supported by NSC (Grant NSC 97-2221-E- 
002-129-MY3, Taiwan, ROC) and by Research Center for Information 
Technology Innovation, Academia Sinica, Taiwan, ROC. 
I. I NTRODUCTION 
   In software testing, a test oracle is a mechanism that 
tells whether a software under test (SUT) passes or fails 
a test case. Traditionally, human engineers serve the role 
of test oracles in the industry. Although supposedly human 
engineers have all the common senses and intelligence nec- 
essary for playing the role, they may issue contradicting and 
even incorrect test verdicts due to inconsistent interpretation 
of the same requirement documents and exhaustion from 
repetitive mechanical interaction with SUTs. Also human 
test oracles incur a lot of intervention to test execution and 
can signiﬁcantly slow down test execution. As a result, much 
effort has been dedicated to use programs as test oracles 
for consistent interpretation of the speciﬁcation and efﬁcient 
decision on the test verdicts. 
   For convenience of discussion, from now on, we use 
test oracles for test oracle programs. Early approaches of 
implementing test oracles include manual encoding of the 
knowledge of system speciﬁcations in the oracle programs 
[12]. Although, there are plenty library supports, e.g., CUnit 
[9] and JUnit [15], manually constructed test oracles are still 
time-consuming and difﬁcult to reuse for similar SUTs. 
   In the last few decades, there were projects using formal 
models for SUT speciﬁcations and deriving test oracles from 
the models [10], [14]. Such models may range from au- 
tomata [7], temporal logics [2], [13], to industrial standards, 
e.g., SDL TTCN-3 and UML. The advantages include the 
high abstraction level and conciseness of formal models and 
the possibility for mechanical construction of test oracles 
from formal models. However, considering the high cost of 
a complete speciﬁcation and the ever-changing operating en- 
vironments of modern software, such an assumption may not 
be plausible in practice. From the perspectives of software 
users, programs are naturally expected to provide consistent 
and reasonable services even in scenarios and environments 
not expected in the original speciﬁcations. This suggests that 
test oracles need to be adaptive by learning from the test 
executions and any correcting suggestions from the users. 
Indeed, there have been proposals for utilizing artiﬁcial 
intelligence (AI) techniques for test oracles [1], [16], [18], 
[20]. The proposals generally ﬁt in the framework of su- 
pervised learning [11]. Examples of SLA include support 
vector machines (SVM) [18] and neural networks [1], [16], 
[20]. However, if we check the frameworks and experiments 
in [1], [16], [20], the authors adopted simple terminating 
functions with well-deﬁned input/output relation as the target 
SUTs of their research. They also directly fed the raw input 
data of the SUTs to the SLAs as training data. This is 
incompatible with the common practice of machine learning 
which relies on the careful selection of features in raw input 
data for the success of machine learning [6]. Although in 
[18], the authors presented techniques to adaptively and 
automatically generate feature variables for test oracles, 
their target SUTs are still terminating programs for list 
processing. At present, a major challenge in software testing 
     only checks the test traces generated with those inserted 
     InTOL procedure calls. This separation of concerns sort 
     of blurs the boundary between white-box testing and 
     black-box testing. 
    • Business security: Software companies usually con- 
     sider software artifacts, especially source code, as their 
     core competence and are reluctant to reveal them to 
     software testing companies unless security of those 
     artifacts can be guaranteed. This barrier however can 
     be harmful to the communication and experiment of 
     innovative ideas from researchers both in the academia 
     and the industry. We feel that CUnit and JUnit are suc- 
     cessful example technologies in encouraging exchange 
     of ideas between academia and industry. We thus follow 
     their philosophy to design InTOL. Software compa- 
     nies can directly annotate their SUT programs with 
     InTOL APIs and construct test oracles for their projects. 
     Researchers in academia can focus on improving the 
     machine learning technologies for software testing and 
     communicate with the industry by providing technical 
     support and rolling out new versions of their SLAs. 
To address the issues in the above, our InTOL are designed 
as follows. 
A. Initialization of the trace collection. 
  The design of the initialization function has the following 
concerns. 
   • Direction for either the training phase or the testing 
    phase. 
   • Accumulation of knowledge learned across training 
    and testing sessions. The knowledge includes dictionary 
    of codes for the events and classiﬁcation information, 
    speciﬁcally separating vectors for SVM in our imple- 
    mentation. 
The procedure call is of the following format. 
intol_begin(f, p, d) 
All our procedure names start with “intol_” standing 
for InTOL. Here ‘f’ is the name of the output ﬁle for 
collected test traces. ‘p’ speciﬁes whether the phase is for 
training or testing. Two possible values are macro constants 
“InTOL_TRAIN” and “InTOL_TEST”. 
   The third argument ‘d’ to the procedure is another ﬁle 
name for the accumulated knowledge for the supervised 
test oracle across several sessions of our tool. With this 
argument, we know where to ﬁnd the knowledge base that 
we have accumulated so far for this test oracle. 
B. Restarting a new trace. 
   For reactive systems, the whole test log from the be- 
ginning of a test execution can be viewed as a single test 
trace. However, this is impractical and does not yield sufﬁ- 
cient number of training test traces for supervised learning. 
Moreover, users may reset their test execution for a new test 
case. Users need the ﬂexibility to break a test execution into 
multiple test traces. Thus, we need the following procedure 
in InTOL. 
intol_new() 
This procedure closes the present test trace and restarts a 
new test trace. 
C. Generation of an event 
   To support a wide range of testing tasks, we need to 
ﬂexibly allow the users to generate the kind of events that 
they want to observe. For example, users may want to 
observe the reception of an input event, the sending of an 
output event, the execution of a statement, the values of 
objects, the value changes of a variable, etc. To support such 
diversity in event formats, we adopt the style of printf() 
statement in C language. The procedure accepts a variable 
number of arguments with the following format. 
intol_event(f, ...) 
Argument ‘f’ is a format string used to interpret the ar- 
guments following the format string. For example, we may 
have the following two statements at a program state with 
x = 3 and y = 10.5. 
intol_event("a:receiving(x=%1d) ", x); 
intol_event("entering square(%f)", y); 
The former generates an event ”a:receiving(x=3)” 
which can be interpreted as the receiving of value three 
for variable x through port a. The latter generates an event 
that signals the the entering of procedure square() with 
argument 10.5. How to interpret the text of an event in a test 
trace is up to the users of these traces. In our implementation, 
we use a dictionary to translate the test traces of such 
event texts to traces of numbers, then encode each trace 
of numbers as a feature vector, and then feed the vectors to 
our SLA. 
D. Query and issue of verdicts 
   After recording a long enough test trace, in the training 
phase, the users may want to instruct the SLA of the desired 
verdict of the trace. In the testing phase, the users may want 
to query for the trained verdict of the trace. We need to use 
the same API for these two operations respectively in the two 
phases. Otherwise, veriﬁcation cost can increase since we 
need insert different InTOL verdict statements respectively 
for the training and the testing phases. Moreover, we may 
run into verdict integrity problem since the verdicts for the 
training phase and for the testing phase can be for different 
test traces. With such concerns, we use one verdict procedure 
for both phases. In the training phase, the procedure solicits 
a desired verdict value as a supervisory signal from the 
users. In the testing phase, the procedure issues a verdict 
plane is deﬁned by the sum of the shortest distances from 
the closest vectors in different classes to the hyper-plane. 
Due to page limit, we refer the interested readers to [17], 
[19] for more details. 
  In this work, we train an SVM in the supervised learning 
paradigm for a test oracle in software engineering. Thus 
there are two classes of test traces respectively with verdict 
values pass and fail. For each test trace, we extract a feature 
vector with components that reﬂect the success or failure of 
an SUT while generating the trace. 
V. T EST ORACLES AS SLA S WITH SELECTED FEATURES 
   With the library explained in section III, we can collect 
a set of test traces with verdicts. In general, collecting such 
traces is much easier and less expensive than constructing 
the explicit models and compiling the detailed speciﬁcations. 
   We assume that a test trace is a sequence of events (in fact, 
text tokens) separated by spaces. In general, there may be no 
bound to the lengths of the test traces of a reactive software 
system. An event may repeatedly occur more than one times 
in a trace. As mentioned in the veriﬁcation ﬂow in ﬁgure 1, 
we need to use feature vectors extracted from test traces 
of unbounded lengths as training data. In the following, we 
discuss the considerations in selecting features and present 
techniques to extract features from test traces. 
   For convenience, we adopt the following notations. Given 
two integers i and j, we use [i, j] to represent the interval 
from i to j, i.e., the integer set of {i, i + 1, . . . , j − 1, j}. 
If i > j, then [i, j] is empty. (i, j], [i, j), and (i, j) are 
shorthands for [i+1, j], [i, j−1], and [i+1, j−1] respectively. 
   We let Σ represent a set of events. A test trace can be 
deﬁned as a sequence in Σ∗ , i.e., the set of ﬁnite sequences 
with elements in Σ. The length of a sequence ρ = e0 e1 . . . en 
is denoted |ρ| = n + 1. Given an integer i ∈ [0, n], we use 
ρ[i] to denote ei . For example, if ρ = abca, then |ρ| = 4, 
ρ[0] = ρ[3] = a, ρ[1] = b, and ρ[2] = c. Given a non-empty 
interval [i, j] ⊆ [0, n], we also use ρ[i, j] to denote test trace 
ei . . . e j . 
   For each e ∈ Σ, we assign a distinct positive index to 
e, denoted index(e). For convenience, we let index(e) ∈ 
[1, |Σ|]. In testing an SUT, we are given a trace collected 
in the test execution and query our trained test oracle of the 
verdict of the trace. For each trace ρ, we use a ﬁnite number 
of feature values (deﬁned later in this section) extracted from 
ρ to decide whether ρ is a failed trace or a passed trace. 
   In this work, we present ﬁve feature sets X0 , X1 , X2 , X3 , 
and X4 of test traces of unbounded lengths. A feature is an 
observation to the past at the verdict-issuing time-point in 
a test trace. For convenience, we use φ(ρ) to represent the 
value of a feature φ extracted from trace ρ. This design 
is plausible since a test verdict depends on the relation of 
events that happen before the verdict. 
   These ﬁve feature sets are explained in the following. 
A. X0 feature variables 
   In making a verdict, the recent events naturally exhibit 
more signiﬁcance than those happened long ago. We thus 
propose to use a parameter W prescribed by the users 
to record all the last W events observed at the verdict- 
issuing time-point. We use variables in X0 to record the 
sufﬁx of a trace ρ up to a window size W . Speciﬁcally, 
X0 = {w1 , . . . , wW } with element variables to record the 
indices of the last |W | events observed in ρ. For each 
i ∈ [1, W ], the value of feature variable wi from ρ is 
deﬁned as follows. If i > |ρ|, then wi (ρ) = 0 and means 
that there is no event occurrence corresponding to wi in 
ρ. If i ≤ |ρ|, then wi (ρ) = index(ρ[|ρ| − i]), i.e., the 
event index of the last i’th events of ρ. For example, given 
index(a) = 1, index(b) = 2, and index(c) = 3, if W = 4 
and ρ = abcabb, then w1 (ρ) = index(ρ[5]) = 2, w2 (ρ) = 
2, w3 (ρ) = index(ρ[3]) = 1, w4 (ρ) = 3. 
B. X1 feature variables 
   The window size of the last W events in a trace may not 
be precise enough to issue a correct verdict for ρ, especially 
when the length of ρ is greater than W . So we really want 
to extract some features about events of more than W steps 
to the past along ρ. According to the characteristics of the 
SUTs, there can be many features that may affect the value 
of a correct verdict. However, we believe that there are some 
common generic features which are important for many 
SUTs. Suppose that in addition to X0 , we have m generic 
event features important for issuing correct verdicts. Then 
we can deﬁne sets X1 , X2 , . . . , Xm of feature variables 
for the ﬂags of the generic features. In our experiment, 
we present X1 , X2 , X3 , and X4 for the temporal relation 
features between a pair of events observed at a verdict- 
issuing time-point to the past. 
   We now deﬁne our X1 as follows. We use feature vari- 
ables in X1 to count the last unanswered event occurrences 
between each pair of events. The intuition of such features 
can be explained with the following example. Suppose that 
we have a client that keeps on sending request messages 
to a server. Upon receiving a complete message from the 
server, the client issues a PASS verdict. If the client does not 
receive a complete message in a predeﬁned number of steps 
after the request message, it may resend another request 
message. It may resend the request message 3 times until 
it becomes disappointed and issues a FAIL verdict. Such an 
event sequence pattern of retransmission is very common in 
telecommunication protocols and interactions in distributed 
systems. We thus deﬁne X1 to extract such event patterns, 
i.e., the count of unanswered requests since the last com- 
plete. Speciﬁcally, for each two events e, f ∈ Σ, we use one 
variable xe,f to record the number of e event occurrences 
that we see without any f events from the end of a trace. 
Formally, given a trace ρ and an event e, we let laste (ρ) 
denote the index of the position of the last occurrence of 
Table I 
C ONFIGURATIONS OF THE EXPERIMENTS 
|Σ| 
22 
152 
   # test traces 
training / testing 
   1250 / 1250 
    750 / 750 
 # inserted 
InTOL calls 
     56 
     41 
SUT bug 
1 
Tubularix 2 3 
4 
5 
1 
2 
3 
4 
5 
Table II 
T HE IMPLANTED BUGS 
                          description 
display error when the dropping tiles hit the left or the 
right boundaries of the screen 
no effect of depressing on the right arrow key 
detection error on tile collision on the bottom boundary 
error in the direction of rotation of the dropping tiles 
failure in eliminating a full row in the screen 
insertion to list head mistaken for insertion to the list tail 
insertion to list tail mistaken for insertion to the list head 
insertion before a list node mistaken for insertion 
behind the node 
deletion of list head mistaken for deletion of the list tail 
deletion of list tail mistaken for deletion of the list head 
SUT 
Tubularix 
STL 
# LOCs 
7915 
3434 
# bugs 
5 
5 
VI. I MPLEMENTATION AND EXPERIMENT 
STL 
   We have endeavored to implement InTOL on top of the 
SVM library libsvm by Fan, Chen, and Lin [5]. InTOL 
itself is implemented in C/C++. We have also carried out 
experiment to check how our techniques perform against 
real-world software. We use the following two open-source 
SUTs from the web. 
    • Tubularix: This is a free and open-source game similar 
      to the popular Tetris seen from a tubular perspective. 
      The program is written in C++ with the Qt4 library [4]. 
      A student played Tubularix, compiled from annotated 
      source code, continuously through keyboard to create 
      sets of test traces. 
    • Standard Template Library2 (STL): This is a C++ 
      library of container classes, algorithms, and iterators. 
      The container classes include basic data structures of 
      computer science, e.g., sequences, sets, strings, etc. 
      In our experiment, we inserted InTOL statements to 
      the STL modules for processing lists. We wrote a 
      small program the keeps on randomly calling STL for 
      processing a list and creates test traces of unbounded 
      lengths. 
The characteristics of and experiment conﬁgurations of the 
two SUTs are in table I. We use the following approaches 
to insert InTOL statements to the SUTs. 
    • Other than the implicit starting of new test traces by 
      intol_assert(), we also inserted intol_new() 
      statements at the end of major program modules and 
      function blocks. 
    • At the beginning of a function, we inserted an 
      intol_event() statement for events that mark what 
      are expected to happen in the function. 
    • At the end of a function, we inserted an 
      intol_event() statement for events that mark 
      what should have completed in the function. 
    • For the intermediate statements of a function, we 
      also inserted several intol_event() statements to 
      record intermediate values and control ﬂow branching. 
    • At the end of each function, we inserted 
      intol_assert() or intol_query_verdict() 
      statements. The ratio of inserted intol_assert() 
      statements to inserted intol_query_verdict() 
      statements is kept at 4 to 1 for both benchmarks. 
           check http://www.sgi.com/tech/stl/index.html 
for more details. 
2 Please 
                       Table III 
P ERFORMANCE DATA OF TRAINING SVM AND TEST EXECUTION 
SUT 
Tubularix 
|W | 
 25 
 50 
100 
200 
 25 
 50 
100 
200 
training time 
     5sec. 
     5sec. 
     5sec. 
     5sec. 
    39sec. 
    39sec. 
    40sec. 
    42sec. 
testing time 
    2sec. 
    2sec. 
    2sec. 
    2sec. 
   27sec. 
   28sec. 
   29sec. 
   29sec. 
correct verdicts 
    95.98% 
    96.24% 
    96.52% 
    96.22% 
    92.88% 
    93.36% 
    93.34% 
    93.18% 
STL 
      The reason for more intol_assert() statements 
      than intol_query_verdict() statements is that 
      human intervention caused by verdict queries can 
      be very time-consuming and slow down the testing 
      processes. In our experiments, only failures that 
      are recognized with special event sequences need 
      human intervention in the training process. Such a 
      combination of assertions and verdict queries should 
      provide an acceptable and viable solution to automated 
      test oracle construction. 
As can be seen, for the experiment, the numbers of InTOL 
statements respectively inserted in the two SUTs are all less 
than 1.3% of the total numbers of statement lines of the two 
SUTs. This may imply that our techniques only incur very 
little burden and overhead in software testing projects. 
    To check the performance of our techniques, we implanted 
ﬁve bugs respectively in the two SUTs as described in 
table II. Implanting more than one bugs seems a more 
realistic setting since most software may exhibit behaviors 
affected by several bugs in the software. 
    The performance data for the two SUTs can be found 
in tables III. The experiment was carried out on Intel Core 
i3 CPU M380 of 2.53GHz and 1G memory running Ubuntu 
10.10 Linux. As can be seen from the table, the trained SVM 
exhibits high accuracy in its verdicts. This implies that with 
proper designs and selection of the features, machine learn- 
ing technology shows good promise and may worth further 
investigation. This part of the work actually complements 
the research in [1], [16], [20] that uses raw input data to 
FASE ＆ ETAPS 2011 國際會議 參加報告 1 
 
 
 
 
出國報告（出國類別：國際會議） 
 
 
 
 
 
參加 FASE 與 ETAPS 2011  
國際會議報告 
 
 
 
 
 
 
服務機關：電機工程學系 
姓名職稱：王凡 教授 
派赴國家：德國 
出國期間：民國 100 年 3 月 27 - 4 月 4 日 
報告日期：民國 100 年 4 月 11 日 
FASE ＆ ETAPS 2011 國際會議 參加報告 3 
 
FESCA    8th International Workshop on Formal Engineering Approaches  
to Software Components and Architectures 
GaLoP VI   Games for Logic and Programming Languages  
GT-VMT    10th International Workshop on Graph Transformation and  
Visual Modeling Techniques 
HAS    Hybrid Autonomous Systems  
iWIGP2011   International Workshop on Interactions, Games and Protocols  
LDTA    Eleventh Workshop on Language Descriptions, Tools and  
Applications 
PLACES   Programming Language Approaches to Concurrency and  
Communication-cEntric Software 
QAPL    9th Workshop on Quantitative Aspects of Programming  
Languages and Systems 
ROCKS    Rigorous dependability analysis using model checking  
techniques for stochastic systems 
SVARM    Workshop on Synthesis, Verification, and Analysis of Rich  
    Models 
TERMGRAPH  6th International Workshop on Computing with Terms and  
Graphs 
WGT    3rd Workshop on Generative Technologies 
 
由於規模盛大，吸引了歐美、亞洲各地的許多學者參加，並進行交流。 
    除此之外，ETAPS 還安排了十餘場特邀演講，講員多是國際會議上的熟面
孔。演講內容大致如下： 
FASE ＆ ETAPS 2011 國際會議 參加報告 5 
 
Michael Backes (Saarbrücken, Germany) 
Automated Design and Verification of Security Protocols based on 
Zero-Knowledge Proofs 
Security 的系統驗證，是最近很紅的研究題目。這場演講，給了我一些新的概念。
我還特地上網去下載了一些相關資料，去瞭解 zero-knowledge security 
protocols 的一些原理。非常有趣。 
Marta Kwiatkowska (Oxford, UK) 
Automated Learning of Probabilistic Assumptions for Compositional 
Reasoning 
Kwiatkowska 教授的研究，主要是在 probabilistic 系統的 model checking。他最
大的貢獻，在於將理論完成在 Prism 系統中，並且運用在許多工業界 projects
上。 
    ETASPS 的另一大好處，是讓參加人員，繳一次報名費，可以參加所有
的活動（晚宴除外）。另外，像是 coffee breaks 幾乎是不間斷的。也就是 Coffee 
與點心，全天不斷服務，方便與會人員交流，建立合作關係。這樣的安排，需
要許多經費支持，但成效良好，希望國內主管機關多多效法。 
ETAPS 2011 今年在德國 Saarland 州的首府 Saarbruken 之 Saarland 大學舉
辦。Saarland 是德國最小一州，鄰近法國，歷史上曾數度在德、法間易手。
Saarbruken 早年以產煤著稱，並且有大煉鋼廠。近年煤礦、鋼鐵業沒落，該市
人口逐漸減少。但在德國政府挹注下，近年該事積極擴充 Saarland 大學的資訊
科學方面的研發計畫能量，並且 MPI（Max Planck Institute）的資訊研究中心，
也將設在該校。我們在會議期間，屢屢聽到該校與相關人員對 MPI 在未來該
校的發展，都表示十分樂觀，前途大好。 
而 Saarbrucken 雖然並不是以觀光著名的城市，但有著典型歐洲小鎮的風
光，Saar 河蜿蜒流經市區，周圍市容整潔，規劃有緻，古典近代相諧和，是
個宜人居住的小鎮。 
 
 
二、過程 
 
    這次會議，我於前一日飛抵法蘭克福國際機場，然後搭乘火車，到達
Saarbrucken。然後旅館 check in 後，就搭公車、tram 轉赴會場。一切堪稱順利。
國科會補助計畫衍生研發成果推廣資料表
日期:2012/01/09
國科會補助計畫
計畫名稱: 網路環境下之嵌入式軟體的自動化測試理論與技術
計畫主持人: 王凡
計畫編號: 97-2221-E-002-129-MY3 學門領域: 程式語言與軟體工程
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
