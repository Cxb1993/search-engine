 2
前言 
Virtual reality (VR) is a powerful tool for a wide range of applications. Some well-known examples and 
domains include simulators, games, medicine, architecture, tourisms, and all sorts of visualizations. In 
particular, web-based VR has been successfully applied to Google Street View, which allows users to 
navigate a city through a collection of panoramic street-level photographs of the streets. 國內外許多著
名大學或研究機構都設有虛擬實境實驗室，每個實驗室的重點發展特色都不同，因為虛擬實境系
統技術涵蓋合幾個不同研究領域，它是屬於跨領域的技術整合，其中本實驗室是以計算機圖學與
電腦視覺兩大研究領域為主。  
大部分的虛擬實境系統都採用較為傳統的作法，也就是幾何式建模搭配頭盔HMD或3D數據
手套等硬體介面來與虛擬場景互動。我們選擇的特色非常不同於傳統的作法，是採用真實影像來
呈現立體場景，而且無須穿著或握持任何硬體設備來與場景互動。 
 
研究目的 
隨著數位相機與攝影機的日漸普及，環場影像製作已經是一些數位相機的內建功能，目前立體顯
示系統正在蓬勃發展，利用立體環場影像呈現照片很可能會成為一種新的趨勢，本計畫目標為融
合立體環場影像與動態影像資訊於一體，在呈現立體環場影像的同時，部分動態區域的影片經過
處理亦可同時以立體的方式呈現，同時達到立體、環場與動態等三項視覺效果。為落實宜大資工
所的教育理念，本計畫將結合理論與實作達到相輔相成，最終目的仍是要為參與計畫的幾位研究
生奠定良好研究基礎，也同時藉由實作經驗為其未來工作鋪路。 
 
我們將發展的虛擬實境系統具有高挑戰性的學術價值，它包含以下特色目標- 
*  A single video sequence of a scene is the only input date. 
*  A fully or highly automatic process to produce the final stereo panorama. 
*  A stereo panorama based virtual reality contains animated scenery and/or objects. 
*  The animated sceneries, such as streams or leaves, will be played endlessly and seamlessly to  
   enhance the “natural” appearance of the virtual environment. 
*  The image disparities will be auto-adjusted according to viewer’s field of view to ensure both the  
   stereo fusion ability and correct depth perceptions. 
 
此外學術性目標有: 
*  Algorithms for auto-generation of stereo panoramas and seamless animated sequences from a   
   single video sequence. 
*  Newly developed stereo panorama viewer for dealing with interactivable objects and animated  
   scenery. 
 
研究方法 
Image Stitching and Undesired Object Removal 
    The goal is to generate a cylindrical animated panorama, which is embedded with user-specified 
video textures, from a finite sequence of video frames captured by a single panning camera. When a 
special designed rotating camera is used to produce a panoramic image, every captured image 
contributes same amount of image data (i.e., equal numbers of image columns) to the resulting 
panorama. However, if a sequence of video frames is used to generate the panoramic image, the rotation 
speed varies during recording, and as a result, each frame should contribute different numbers of image 
columns to compose the final panorama. Let ci denote the number of image columns contributed from 
the ith frame to generate the panorama. Further, let c denote the expected average number of image 
columns contributed from all frames. Ideally, 
1
1 N
ii
c N == c∑ , where N is the total number of frames the 
camera captures in a 360 degree rotation.  
    If the camera’s intrinsic parameters are known, then it is possible to estimate c by equation: 
 
where  and  denote the focal length of the camera and the pixel size (assuming squared pixels), 
respectively. The value of  is to be rounded to the neatest integer for later calculations. For each 
frame, only partial image region is used to perform video registration. Let  denote the width of an 
image frame in pixels. Image region bounded by columns  through  in each frame are 
gathered by our program to perform calculations for video registration. 
     The value of  for each  is determined by the result of video registration. There 
have been many algorithms developed for video registration for different purposes. We define the 
similarity measure of two frames as the average pixel intensity difference within the overlapping region. 
Frames are registered based on the obtained minimum similarity value. For instance, if the th frame 
and the th frame have been determined having  columns overlapping, then . The 
resulting panoramic image is composed by stitching together  image columns from the th frame, for 
all . The current pixel’s color value is replaced by the average color value of three 
pixels at the same position in frames , i, and .  
    An image refinement feature, that is the ability to remove the undesired foreground moving objects, 
has been implemented. This makes our developed program more advance than other existing panoramic 
image makers or stitchers. In case that there were some moving objects in the scene during video 
recording and users would like to remove them from the resulting panoramic image. The program 
provides an interface which allows users to specify regions to be refined which contain foreground 
objects, and the specified regions are replaced with back-ground scene by exemplar-based image 
inpainting technique. 
 4
 
 
Video Texture Generation 
Texture synthesis is a process of algorithmically constructing a larger image based on a (small) sample 
image in a way that the resulting image pre-serves structural appearance of the sample image. Graph cut 
technique has demonstrated quite a few satisfactory texture synthesis results and this technique can 
easily be extended to 3D case. However a pure graph cut method is very time consuming especially 
when applying on 3D data. Our approach is somewhat a simplified version of graph cut method, but we 
claim that the quality of the video textures generated by our program is comparable to results of a pure 
graph cut method and the computation time can be greatly reduced. 
      Our goal is, given an image sequence of some moving objects, produce a video texture (often a 
shorter sequence) such that if playback this video texture repeatedly then the objects would appear to 
move continuously and infinitely. Our approach is to first overlap the starting and ending portions of the 
input image sequence, and then obtain a cutting surface within the overlap region. After performing this 
cut, the color difference between the neighborhood-pixels (i.e., pixels in the starting and ending portions 
of video frames respectively) along the cut-surface should be a minimum. Thus, the starting and ending 
portions of the image sequence can be stitched together seamlessly. We convert this problem into a 
finding minimum cut problem in a 3D graph, but instead of finding an absolute minimum cut, we intend 
to look for an approximated solution. 
 
Figure 3: The procedures of video texture generation. The whole pipeline consists of three major tasks: 
(a) overlapping region determination, (b) minimum cut calculation, and (c) minimum cut approximation 
of 3D graph. 
 
    In graph theory, a cut is a partition of the vertices of a graph into two sets. Any edge crossing the 
cut is a cut edge. In the case of a weighted graph, a minimum cut following some specified cutting 
criteria is the cut that minimizes the sum of the cut-edge weights. The procedures and concepts of our 
video texture generation approach is illustrated in Fig. 3. The whole pipeline consists of three major 
tasks: (a) determination of overlap-ping region, (b) finding minimum cuts for each horizontal slice, and 
(c) finding an approximated minimum cut of the 3D graph (i.e., video data) based on the results from (b). 
The details of above-mentioned procedures can be referred to our paper below: 
F. Huang, K.-M. Yang,, Z.-J. Wei, A. Tsai, and J.-Y. Tsai. "Animated Panorama from a Panning Video 
 6
 
        
        Figure 4 shows four results of the generated video textures. The original fountain sequence extracted 
from the input video consists of 115 images of size 247 × 362 pixels. The resulting video texture has 23 frames. 
The video texture computation time for this example is 45 seconds. Every 4th frame of the resulting fountain 
texture sequence is shown in Fig. 4(left). Another three examples are another fountain, waving straws, and 
ocean wave shown in the second, third, and forth columns, respectively. Among them, fountain and straws 
were extracted from the same panorama video. The video information of all these examples and the 
computation times are summarized in Table 1. We aim to produce video textures consisting of small number 
of image frames unless user has specified a preferred minimum video texture length. For the wave example, 
since a complete periodic motion of the ocean wave takes about six seconds in this case, it is reasonable to 
constraint the length of the resulting video texture to be greater than 180 frames.  
 
Table 1: Information of the input and output videos. 
 
        A new software program has been developed, which takes a single video sequence captured by panning 
the camera, and generates an animated panorama. The concept is to create a cylindrical panoramic image 
embedded with multiple video textures. The animated visual effects of our approach are comparable to 
results of panoramic video texture, which has been known a time-consuming task. The contributions of this 
work are as follows: (1) an approach of finding an approximated optimal cut-surface in a video sequence (i.e., 
a 3D space-time volume) has been proposed and implemented, thus the animated panorama can be generated 
within a practically acceptable time; (2) an image inpainting method has been developed which allows users 
to eliminate the undesired moving objects in the scene; and (3) a virtual reality player has been established 
which is capable of playing the resulting animated panorama. 
 
參考文獻 
[1]  A. Agarwala, K.C. Zheng, C. Pal, M. Agrawala, M. Cohen, B. Curless, D. Salesin and R. Szeliski," 
Panoramic video textures," ACM SIGGRAPH 2005, 24(3), pp. 821-827.  
[2]  D. Aliaga and I. Carlbom. Plenoptic Stitching: A Scalable Method for Reconstructing 3D 
Interactive Walkthroughs. Proceedings of SIGGRAPH 2001, pp. 443-450.  
[3]  M. Brown and D. Lowe. Automatic Panoramic Image Stitching Using Invariant Features. in 
International Journal of Computer Vision, 74(1), pages 59-73, 2007.  
[4]  S. E. Chen. QuickTimeVR - an image-based approach to virtual environment navigation. In Proc. 
SIGGRAPH’95, pages 29–38, Los Angeles, California, USA, August 1995.  
 8
 10
Dynamic Range. Proc. ICPR'2002, Quebec, Canada, Aug. 2002, Vol. III, pp 635~638.  
 
計畫成果自評 
 
本研究計畫相關 近年發表之論文 
《Journal》    F. Huang, K.-K. Fehrs, G. Hartmann, and R. Klette. "Wide-angle Vision for Road Views", to 
appear in Opto-Electronics Review, (2012). [SCI] 
《Journal》   F. Huang, A. Torii, and R. Klette. "Geometries of Panoramic Images and 3D Vision", 
Machine GRAPHICS & VISION, Volume. 19, No. 4 (2010), pp. 463~477. [EI] 
《Journal》    F. Huang and R. Klette. "Stereo Panorama Acquisition and Automatic Image Disparity 
Adjustment for Stereoscopic Visualization", Multimedia Tools and Applications, Volume 
47, Issue 3 (2010), pp 353~377. [SCI] 
《Conference》F. Huang, K.-M. Yang,, Z.-J. Wei, A. Tsai, and J.-Y. Tsai. "Animated Panorama from a 
Panning Video Sequence", to appear in IVCNZ’10, Queenstown, New Zealand, November, 
2010. [EI] 
《Conference》 K.-M. Yang, F. Huang, and S.-H. Lin. "Generation of Animated Panorama from Single 
Video Sequence", in CISP’10, Yantai, China, October, 2010, Vol. 1, pp 477~481. [EI] 
《Conference》 K. Scheibe, F. Huang, and R. Klette. "Calibration of Rotating Sensors", in CAIP’09, 
Münster, Germany, September, 2009, LNCS 5702, Springer, pp 157~164. [EI] 
《Conference》N. C. Tang, H.-Y. M. Liao, C.-W. Su, F. Huang, and T. K. Shih. "Video Inpainting on 
Digitized Old Films", in KES 2009, Santiago, Chile, September, 2009, Part II, LNAI 5712, 
pp 421~430. [EI] 
《Thesis》    楊坤銘. "Animated Panorama from a Rotating Camera", CSIE, NIU, 2010. 
《Thesis》    田汝琪. "A Comparative Study of Panoramic Image Stitching Software", CSIE, NIU, 2010. 
《Thesis》    張均薇. "3D Animated Panorama from a Rotating Camera", CSIE, NIU, 2011. 
 
 
本研究計畫之貢獻  
大部分現有的虛擬實境系統，包括市面銷售的軟硬體，都以幾何式為主，由於要達到即時互動所
以幾何式虛擬實境最大的缺點便是真實度不夠，以至於被稱之為虛擬“虛”境。此計畫所發展的影
像式系統將可以完全表現真實的場影畫面，配合立體環場影像技術與動態影片合成技術，可以達
到虛擬“實”境之身歷其境的境界。  
    所研發的影像技術可以應用於現有的手機、數位相機或攝影機，為數位內容加值，讓使用者
可以輕鬆建構立體虛擬實境，也可直接播放與場景互動。如果採用立體螢幕，則無須配戴立體眼
鏡即可享受立體視覺效果。  
    期望為宜蘭大學創造一間生動、親切、科技化之虛擬實境展示室，結合立體顯示與音響系統，
提供模擬導覽、數位創作展示、教育訓練、互動遊戲開發等應用。將來努力朝向數位內容的製作，
強調科技、藝術與文化的融合，推廣當地特色，配合政府的發展方向積極推動數位創意產業。  
 
November 08 and 09 (Monday and Tuesday) 
 
 
Above photos: (Left) Registration desk. (Right) The notice board besides the desk.  
 
The main task on the first two day was to help organizing the workshops. On Monday, there were 
four workshops running full days.  
1. Computer Vision in Vehicle Technology: From Earth to Mars 
2. e-Heritage (Electronic Cultural Heritage) 
3. Gaze Sensing and Interactions 
4. Visual Surveillance 2010 
 
On Tuesday, there were another four workshops also running full days.  
1. Computational Photography and Aesthetics 
2. Subspace 2010 
3. Event Categorization, Tagging and Retrieval (VECTaR 2010) 
4. Application of Computer Vision for Mixed and Augmented Reality 
 
My duty there was to make sure everything runs smoothly according to plans. 
 
   
Above photos: (Left) VECTaR 2010. (Right) Gaze Sensing and Interactions.  
 2
    One of the subjects that people concerns most was “What are the trends in research of computer 
vision?”. Some researchers thought computer vision and computer graphics are gradually converging 
and should include computer graphics into the themes of future conferences. Some thought due to the 
rapidly developed new hardware technology, many computer vision tasks could easily be achieved by 
utilizing affordable hardware devices; this fact causes thrills to computer vision researchers. However, 
on the other hands, some also thought new hardware technology has created many new vision tasks 
and has brought computer vision solutions to a much wider users. 
    During the conference periods, I had to come back and forth between ICCV workshops and 
IVCNZ, also among ACCV seminars, poster sessions, and demo sessions, in order to fulfill my duties, 
present paper, and to catch the interested presentations. Luckily all the events were held in the same 
building only at different floors. However, on the other hand, many events were held in parallel in the 
hotel and the corridor areas (where refreshments were served) were quite small. As a result, people 
might feel very crowded inside the building. 
  
三、建議 
    我個人認為研究人員參加國際學術研討會應是件值得鼓勵與推動的事，很意外獲知註冊費
也必須由國科會補助的出席國際會議差旅費支付。往年大家都明白就算排除註冊費，計畫所補
助的差旅費金額通常不夠支付機票加上基本住宿費用，研究人員通常必須另外申請補助或自
費，如果連註冊費也加進去，以一般品質較高的國際學術研討會而言，註冊費在 2 萬到 3 萬 5
千之間，有可能已經佔用掉一半的補助金額，如果一年有超過一次的出席國外的國際學術研討
會機會，可以想見經費補助額度是占實際基本需求金額的很小百分比。個人感覺這樣的作法其
鼓勵性並不高，反而衍生許多困擾，因此以通常補助出席國際會議額度不足之現況下，也就是“部
分”補助而非全額補助，個人偏向研討會註冊費能由業務費支應的作法。 
    縱使提出上述的想法，而且這次出席 ACCV 國際會議自付額也頗高，但是仍然很感激有這
筆差旅費補助，這次會議讓我充實很多在學術上的寶貴經驗。 
 
四、攜回資料名稱及內容 
 ACCV & IVCNZ conference name badges 
 
 4
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/12
國科會補助計畫
計畫名稱: 自動化立體動態環場影像製作
計畫主持人: 黃于飛
計畫編號: 99-2221-E-197-024- 學門領域: 計算機圖學
無研發成果推廣資料
其他成果 
(無法以量化表達
之成果如辦理學
術活動、獲得獎
項、重要國際合
作、研究成果國際
影響力及其他協
助產業技術發展
之具體效益事項
等，請以文字敘述
填列。) 
100 年的新研究成果將會撰寫成論文投稿至國際學術研討會. 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
