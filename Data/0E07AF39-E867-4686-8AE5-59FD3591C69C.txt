1I. Introduction
For a new hybrid (sequential/batch) training mode, we will study the learning
rates of sequential training for the two-layer NN first. For each epoch, the adjusted
weights will be represented in a polynomial of the learning rate St with order P and
its coefficients can be expressed in recursive form. Meanwhile, the dynamic stable
and optimal learning rate ,
S
t opt , which leads to a minimum total squared error, can be
systematically computed through the change of two adjacent total squared errors. It
has been proven in this paper that the dynamic stable learning rate must be greater
than zero. In the meantime, the dynamic stable learning rate ,
B
t opt of batch training
for the two-layer NN can be obtained by [1]. Hence, the hybrid training mode based
on minimum total squared error is chosen from the sequential training mode and the
batch mode for the current epoch training process. To construct a complete fuzzy
neural network, the two-layer NN being FNN’s consequent part is cascaded to its
premise part (or linguistic part). Feeding with the fuzzfication values of the input
vectors of the premise part, the dynamic optimal learning rates of the two-layer NN
are then found. Because the training error is mainly affected by the consequent part of
FNN, we let the parameters of FNN’s premise part be reasonably pre-selected and be
fixed in later simulation. Hence, there exists a fixed input data matrix for the
two-layer NN. Backing up a Truck-and-Trailer into a loading dock is explored with
excellent results.
II. DYNAMIC OPTIMAL LEARNING RATES FOR A TWO-LAYER NN
Consider the following simple two-layer NN in [1], [2] which will perform the
consequent part of the FNN adopted in this paper where
the training data vector:  1 2 ... T LLr r r r  (1)
the weighting matrix :  1 2 ... L ZZW w w w   (2)
the zth weighting vector: 1 2 ... L Lz z z zw w w w    (3)
the actual output vector:  1 2 ... T ZZy y y y  (4)
and the desired output vector:  1 2 ... T ZZd d d d  (5)
Given a set of training vectors, which forms the training matrix R in (7), it is
desired to use the back propagation technique to train the NN so that the actual
3“S”denotes sequential training approach. ,
S
t zw is renamed as 0,z for the first
iteration at the tth epoch.
The weighting vectors 1,z can be updated as
1
1 0
0
,
, ,
,
S
zS
z z t
z
j   
 
 0 1 0 1 1, , ,
( )S Tz t z zr d r     (13)
where St is learning rate of the sequential training approach.
Similarly, 2 0 1 1 2 0 1 1 2 2, , , , , ,( ( ) )
S S S T S S
z z t z t z t z ze r r e r d r         
0 1 1 2 2, , ,
S S S S
z t z t ze r e r     . (14)
By reiterating the updating procedure described above, the updated weighting vector
,P z (= 1,St zw  ) can be derived as a polynomial function of learning rate with order P
1,
S
t zw 
1
, ,
P
S S S
t z t p z p
p
w e r

   1 1 1 0, , , ,( ) ( ) ...S P S P St P z t P z t z zC C C C        (15)
where 0 0, , ,( )
S
z t z zC w   , 1
1
, , ,( )
P
T S
z p t z p z p
p
C r w d r

  , 1 1, ,( ) ( )Tk z p k z p
p k
C r C p r

 

  for k
=2,…,P1, and 1 1 1
2
1, , ,( ) [( ) ]
P
P T T S
P z p t z z p
p
C r r w d r r

  .
The optimal dynamic learning rates ,
S
t opt of the sequential training will be obtained as
follows. TheV is defined as


 
Z
z
S
zt
S
zt JJV
1
,,1 )(
Z
1 1
z=1 1
1
2
2 , , , , ,
[ ( )( )]
P
T S S T S T S
p t z t z p t z p t z p z
p
r w w r w r w d 

    (16)
By substituting 1,
S
t zw  in (15) into (16),V can be rewritten as
( )StV  2 2 12 2 1 1
1
2
[( ) ( ) ... ]S P S P St p t p tM M M       (17)
where
2
2
1 1
0,( )
Z P
T
P p P z
z p
M r C
 
  , 2 1 1
1 1
2 , ,
Z P
T T
P p P z p P z
z p
M r C r C 
 
 ,…,
1 1
1 1
2 0, , ,( )
Z P
T T S
p z p t z p z
z p
M r C r w d
 
   .
5Step4:Compute 1
S
tW  and 1
B
tW  in reference [1] by using ,
S
t opt and ,Bt opt ,
respectively. And Then find 1
S
tJ  and 1
B
tJ  .
Step5: If 1
S
tJ < 1
B
tJ  , then set 1tW  1StW  , 1tJ  1StJ  otherwise set
1tW  1BtW  , 1tJ  1BtJ  .
Step6: If 1tJ < then stop else set tt+1, Goto Step2.
For further exploration, the structure of a classical fuzzy neural network is described
in the next section.
III. STRUCTURE OF A CLASSICAL FUZZY NEURAL NETWORK
A classical fuzzy neural network (FNN) [1], [2] in Fig. 1 is shown in this section.
The consequent part is two-layer NN (in dashed line box). Gaussian fuzzy
membership functions consisting of the premise part are fixed, whereas the weighting
vectors in the consequent part is to be adjusted once the learning rate is computed.
Layer I Layer II
W
 
rL
y1
y2
yZ
d1
d2
dZ




r1
r2

x1
x2
xN



Layer III Layer IV

Fig. 1 A classical FNN
Rule l: If x1 is 1
lF and ,…, and xN is lNF then y1 is 1
lw and ,…, and yZ is lZw
(19)
where l =1,2,…,L (the number of fuzzy rules). Further, using defuzzification method,
the fuzzy basis function rl , which is for the input data of the NN, can be determined
as
1 2 1 1
1
( ... ) ( ) ( ( ))
L
N Nl l
l N n n n nn n
l
r x x x F x F x 

       
  . (20)
Then
1 1
1 1
,( ( )) ( ( ))
L L
N Nl l
z n n l z n nn n
l l
y F x w F x 
 
           . (21)
For simulation, there is an example to drive a Truck-and-Trailer into a loading
7following tables. Table 1 shows the fuzzy sets of x, which are represented by the
centers and widths of Gaussian functions. Similarly, Table 2 and 3 show the fixed
fuzzy sets fort andc, respectively.
Table 1 Fuzzy Sets of x
mean Deviation
LE 1.5 2.1
LC 7 1
CE 10 0.3
RC 13 1
RI 18.5 2.1
Table 2 Fuzzy Sets oft
mean Deviation
RB -65 17
RU 0 15
RV 52.5 14
VE 90 3
LV 127.5 14
LU 180 15
LB 245 17
Table 3 Fuzzy Sets ofc
mean Deviation
NE -65 30
ZR 0 15
PO 65 30
Therefore, there are 105(=573) fuzzy rules are used in this simulation. After 100
epochs, the values of J for entire batch training, entire sequential training and hybrid
training drop from 17.7877 to .4447, 0.4453, and 0.4443, respectively. The optimal
learning rates of hybrid training via epoch t are shown in Fig. 3.
0 20 40 60 80 100
0
0.5
1
1.5
0 5 10 15
0
0.5
1
(a)
(b)
Fig. 3. (a): The optimal learning rates of chosen batch training of hybrid training,
(b): The optimal learning rates of chosen sequential training of hybrid training.
The tolerated ranges of x and t are set as [9.7, 10.3], [87, 93], respectively. Six
