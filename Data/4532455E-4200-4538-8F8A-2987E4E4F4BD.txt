行政院國家科學委員會補助專題研究計畫成果報告 
※※※※※※※※※※※※※※※※※※※※※※※※※※
※  雲端運算研究平台建置技術與相關資訊科技應用  ※ 
※         之整合型研究         ※ 
※    應用雲端運算的行動服務平台之研究    ※ 
※※※※※※※※※※※※※※※※※※※※※※※※※※ 
 
計畫類別：□個別型計畫  ■整合型計畫 
計畫編號：NSC 99-2632-E-126-001-MY3 
執行期間：101年 8月 1日 至 102年 7月 31日 
 
計畫主持人：林耀鈴 
共同主持人：周文光、林家禎、蔡英德、王孝熙、胡育誠、葉介山、 
胡學誠、李冠憬、溫嘉憲 
計畫參與人員：洪哲倫、鄭介勛、陳紫淇、陳俞伶 
各組計畫參與人員： 
『生物資訊分析組』：滑冠傑、張修華、徐培升、謝承恩、陳文培、
郭士煒、林祺鈞、張雅雯、曹永忠、徐柏棻 
『資料探勘組』：吳勃璋、翁清富、高嘉鴻、曹翔富、柯亘芸、葉貞
麟、林彩雯、文鈞、秦鼎、陳仲桓、張家寧、李螢柳、秦伯瑋、陳婷
婷、周婉霖、林柏均 
『影像處理與辨識組』：許俊傑、鄭鴻毅、黃掀芸、黃英軒、賴嘉緯、
胡怡如、梁峻綸、盧琬瑜 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
■出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
執行單位：靜宜大學 
 
中 華 民 國 102年 10 月 14 日 
 1 
一、摘要 
 本整合型計畫強化雲端運算平台技術
及拓展各類潛在應用為主軸，整合雲端運
算架構與應用兩大範疇，包括架構層以及
應用層。本計畫針對架構層之「雲端運算
平台建置技術」以及應用層之「生物資訊
分析」、「資料探勘」以及「影像處理與
辨識」等主題進行探討。本年度報告中說
明總計畫與各組計畫的緣由與目的、結果
與討論、計畫自評以及研究成果發表與專
利申請等四大部分。 
 
關鍵詞：雲端運算、生物資訊分析、資料
探勘、蛋白質結構分析、利潤探勘、竄改
偵測、區塊截短碼 
 
Abstract 
 This integrated project will explore and 
integrate four main subjects on both 
architecture layer and application layer of the 
cloud computing environment. Four main 
subjects are “the platform implementation 
technology of the cloud computing”, 
bioinformation analysis”, and “image 
processing and recognition on the cloud”. 
The report of this year describes the 
motivation, goal, status, self-assessment, 
research and patent applications published of 
the agglomerate project and sub-subject. 
 
Keywords: Cloud computing, 
Bioinformation analysis, Data mining, 
Protein structure comparison, Utility mining, 
Tamper detection, Block truncation, Hadoop, 
MapReduce 
 
二、緣由與目的 
 
2-1 架構層 
雲端運算主要區分為基礎設施即服務
（IaaS），平台即服務（PaaS）和軟體即服
務（SaaS）。其中，基礎設施即服務（IaaS）
主要探討雲端服務底層平台之建置技術，
也可視為雲端平台的作業系統。其功能可
以聚合和全面管理大型基礎架構資源池，
包括處理器、儲存和網路等，使其成為一
種無縫、靈活、動態的操作環境，能以更
為經濟有效的方式提供雲端服務，甚至建
構出可跨越多個資料中心以及整合公私有
雲之平台環境。 
在本整合型計畫中，應用層之「生物
資訊分析」、「資料探勘」以及「影像處
理與辨識」，各組將分別提出相關之雲端
服務應用，並且透過MapReduce架構下之
雲端運算系統做為其後端之資料處理平
台。 
 基於這樣的需求與理念，我們透過
OpenStack打造專屬的基礎設施即服務
（IaaS），提供一個可快速的佈署與管理的
雲端運算系統平台，以快速滿足新一代應
用的複雜規模、擴充彈性、海量資料等難
題，並且結合上述應用層之雲端服務應用，
提供一個永續發展之研究平台。 
 
2-2 應用層 
本應用層包含「生物資訊分析組」「資
料探勘組」及「影像處理與辨識組」三大
組別，各組計畫本年度之緣由與目的如以
下說明： 
 
2-2-1 「生物資訊分析組」 
隨著生物技術的快速發展，基因序列
資料可以更快速的產生，因此資料量增長
速度更快也更為龐大。要處理這些大量的
生物資料，需要高效能與穩定的運算架構。
本計劃透過雲端運算平台發展出四個生物
雲端運算服務，分別是tag SNP選擇、蛋白
質結構排列、蛋白質配體比對與開放閱讀
框架演化分析。在本計劃中我們透過虛擬
化技術開發出雲端平台，並透過這個平台
以及Hadoop Map/Reduce框架來發展這些
服務，實驗結果顯示這些服務透過雲端平
台可以有效地提升執行效率，並且透過
Hadoop特性，這些服務具有高可用性。透
過本計劃結果，可以提供一個移植生物資
訊工具成為雲端服務的範本。 
 
2-2-2 「資料探勘組」 
網站使用度探勘(Web Usage Mining)
主要是藉由網頁記錄檔做分析，在有用的
網頁中找出隱而未覺的瀏覽樣式，透過探
勘的結果，網頁伺服器可以預測接下來被
存取的網頁，並且提供網頁設計師了解更
多有意義的資訊。 
本研究提出一個新的網頁瀏覽樣式利潤
偏好度探勘模型，接著提出不同時間區段
 3 
件以及用途整理。 
表二、平台環境硬體規格表 
服務 元件 用途 備註 
Control 
Service 
Nova 
虛擬機器
運作管理 
 
Compute 
Service 
Compute 
虛擬機器
資源提供 
KVM 
Network 
Service 
Network 
虛擬網路
管理 
 
Storage 
Service 
Swift 
物件儲存
管理 
Moose
FS 
Block 
Storage 
Volume 
區塊儲存
管理 
iSCSI 
NAS 
File 
Storage 
--- 
檔案儲存
管理 
NAS 
UI 
Service 
Horizon 管理介面  
Image 
Service 
Glance 
虛擬機映
像 
 
Identity 
Service 
Keystone 
認證存取
控管 
 
表三為雲端平台主要添購之硬體設備，
我們透過OpenStack將所有硬體資源進行
整合，虛擬化為一個總體資源池。並替計
畫成員建立專屬的帳號以及分配可執行之
虛擬機器數量、可使用之CPU個數與記憶
體。計劃成員本身不需透過管理者便可自
行配置適合自己的實驗環境與應用服務的
部屬。 
表三、整體資源池 
伺服器 數量 vCPU vRAM vDISK 
IBM 
HS22 
4 32 96G 1,000G 
Dell 
R515 
3 36 48G 1,800G 
Dell 
R515 
2 32 32G 600G 
NAS 1 0 0 8T 
iSCSI 
NAS 
1 0 0 12T 
Total  100 176G 23.4T 
 
 
圖二、靜宜大學雲端平台架構圖 
靜宜大學雲端平台整體主要架構，如
圖二所示，主要為檔案系統的優化，以添
購的伺服器來說，其本身之硬碟容量並無
法支撐過多的虛擬機器數量，雖然使用伺
服器本身的硬碟，可以得到較好的效能與
反應速度，但可建立的虛擬機器數量卻也
因此受限。為了解決這一部分的問題，我
們計畫平台在IBM Server的部分，使用的是
分散式檔案系統 (DFS)，其中我們挑選
Moose File System(MFS)來做為我們的網
路檔案系統。 
Moose File System(MFS)是一個高容
錯的分散式檔案系統，提供高可靠性之服
務，可將資料自動同步到不同的資料主機
(可設定目標值)；擴增容量方便，可透過新
增加電腦/硬碟/記憶體，容量即可動態擴展，
並可設定刪除檔案的保留時間；Dell Server
的部分我們透過網路附接儲存 NAS 
(Network-attached Storage)的方式做為我們
的網路檔案系統，也因此我們所的有虛擬
機器資料實際存的位置並非是在提供虛擬
化的Compute Node上面，而是存在另外的
網路檔案系統，不但擁有較高全性的資料
保護，如此一來即使，Compute Node發生
硬體損毀，也不會影響到虛擬機器上的資
料，只需添購新的機器將故障的置換掉後
重啟虛擬機器即可復原環境。 
此外，IBM中Control Node其Glance元
件負責註冊與派送之虛擬機器的範本，此
部分我們也透過NAS方式，將製作好的範
本放置於網路檔案系統，而為了避免資料
傳送所造成的干擾。我們利用不同的交換
機來控制資料流量，透過高速網路分別連
結於兩台不同的交換器，分別用來做伺服
器與虛擬機器控制傳輸處理線路，以及專
門提供NAS與伺服器或虛擬機器之資料傳
輸處理線路。 
以下為目前雲端平台所提供使用之虛
 5 
估方式。這是一個相對性客觀的評估方法，
可以避免人的主觀視覺判斷、影像品質評
估的不一致性，此方法是將以一張目標影
像與另一張要比較的影像去做比較，當
PSNR的值越大時，代表重建影像品質越
好。 
從實驗所列的竄改範例我們發現，所
提出的方法能夠將影像被竄改的區域標示
出來，而且當藏入認證位元len分別設為2、
3、4時竄改偵測正確率均超過九成。尤其
當len=3、區塊大小4×4時，平均影像品質
為30.576 dB，竄改偵測正確率高達96.42%。
因此我們建議將藏入認證位元len設為3，不
但可以保有還不錯的影像品質，而且竄改
偵測正確率也很高。 
 
四、計畫結果自評 
 各組計畫本年度之研究目標均已達成，
詳細自評結果如下所示： 
 
4-1 架構層 
 架構層目前已透過OpenStack並結合
不同類型之網路檔案系統，建構出合適屬
於自己的雲端服務平台。虛擬機器範本方
面，我們針對了Ubuntu系列的作業系統進
行客製的映像檔的修正，計畫成員只需建
立好預架設的Hadoop Cluster之虛擬機器
數量，只需填入IP相關的資訊，執行簡單
的指令，便能夠完成Hadoop Cluster架設在
我們雲端平台上面。目前佈署五十台節點，
約略只需花費15分鐘，便可提供一個可快
速的佈署與管理的雲端運算系統平台，以
快速滿足複雜規模、擴充彈性、海量資料
等問題，架構層之研究成果與原修正計畫
相符並達成預期目標。 
 
4-2 應用層 
本應用層包含「生物資訊分析組」「資
料探勘組」及「影像處理與辨識組」三大
組別，各組計畫本年度之計畫結果與自評
如以下說明：  
 
4-2-1 「生物資訊分析組」 
 本小組已透過雲端運算平台發展出四
個生物雲端運算服務，分別是tag SNP選擇、
蛋白質結構排列、蛋白質配體比對與開放
閱讀框架演化分析。這些服務的開發皆是
基於Hadoop的Map/Reduce框架，且可以並
行的處理數據的相關運算。目前這些作品
皆已發表在SCI-indexed期刊、國際期刊與
國際研討會，而且這些作品都可以提供線
上服務。基於這些發展經驗，我們希望透
過本計畫的進行過程與發表成果可以讓其
他研究可以快速的進入雲端運算領域，並
且可以吸引更多學者投入雲端生物資訊服
務的領域。本小組研究成果與原修正計畫
相符並達成預期目標。 
 
4-2-2 「資料探勘組」 
 本小組進一步提出不同時間區段的網
頁瀏覽樣式之偏好度探勘的模型與演算法，
加強時間變化的概念去找出更精確的網頁
項目集。本研究並採用雲端 Hadoop 平台環
境，以 MapReduce 技術提升網頁瀏覽樣式
利潤偏好度探勘的效能。 
實驗結果顯示，基於 MapReduce 技術的
網頁瀏覽樣式利潤偏好度探勘的演算法，
能產生更好的執行效能；不同時間區段的
網頁瀏覽樣式之偏好度探勘能找出更精確
的網頁項目集。根據研究分析的結果，可
以讓網頁設計師依照不同時間區段而改變
網頁架構，使得網站的瀏覽更加便利。 
研究成果除了發表多篇期刊與國內外會
議論文外，其中又以網頁資料探勘的大量
資料處理最需要高效能的運算。並可運用
於雲端計算環境，研究的內容與原計畫規
劃內容相符，且其成果並能推廣雲端計算
與GPU多核心計算的運用於資料探勘領域
與相關應用之參考。 
 
4-2-3 「影像處理與辨識組」 
本小組提出了一個創新的影像完整性
保障技術。所提出的方法可用於保障區塊
截短碼壓縮後影像的內容完整性。區塊截
短碼是一個以區塊為基礎的壓縮技術，一
個影像區塊被壓縮後的壓縮碼為分群的位
元圖與兩個重建階。在所提出的技術中我
們利用亂數產生器搭配選定的亂數種子所
產生的亂數序列來產生認證碼，並將認證
碼藏入到壓縮碼的位元圖中。在所提出的
技術中認證碼的長度可以根據使用者的需
 7 
Library in Taiwan", 2012 International 
Conference on e-Commerce, 
e-Administration, e-Society, e-Education, 
and e-Technology (e-CASE & e-Tech 
2012), Hong Kong, China, March 30- 
April 1, 2012. 
11. 曹永忠,許智誠,蔡英德, "A Study of the 
Innovation Usability Model for the 
Heterogeneous Mobile Product 
Development(運用創新可用性模式解決
行動產品開發之研究 )", Scientific 
Journal of Information Engineering(信息
工程期刊), vol. 2, no. 4, 2012. 
12. C. L. Hung, Y. L. Lin, G. J. Hua, and Y. 
C. Hu*, "CloudTSS: A TagSNP 
Selection Approach on Cloud 
Computing", International Conference on 
Grid and Distributed Computing (GDC 
2011), International Convention Center 
Jeju, Jeju Island, Korea, December 8-10, 
2011. 
13. C. L. Hung, C. Y. Lin, and Y. C. Hu, 
"UNION: An Efficient Mapping Tool 
Using UniMark with Non-overlapping 
Interval Indexing Strategy", International 
Conference on Bio-Science and 
Bio-Technology (BSBT 2011), 
International Convention Center Jeju, 
Jeju Island, Korea, December 8-10, 2011. 
14. Yung-Chung Tsao, Yin-Te Tsai, Kevin 
Chihcheng Hsu, Hsin-Kuang Hsuen, 
"The Exploratory Study of the Mobile 
Device Development Strategies Based on 
Cognitive Usability Model Based 
Extended from TAM", 2011 International 
Conference Interaction Design, Hong 
Kong Polytechnic University, China, Nov. 
9-12, 2011. 
15. Chun-Yuan Lin, Chuan Yi Tang, 
Sheng-Ta Li, Yaw-Ling Lin, Che-Lun 
Hung, "CUDA-FRESCO: 
Frequency-Based RE-Sequencing Tool 
Based on CO-clustering Segmentation by 
GPU", High Performance Computing and 
Communications (HPCC), 2011 IEEE 
13th International Conference, pp. 
857-862, Sept. 2-4, 2011. 
16. Yung-Chung Tsao, Yin-Te Tsai,Kevin 
Chihcheng Hsu,Hsin-Kuang Hsuen, 
"Integrated Usability Model for the 
Multimedia Mobile Device 
Development", The 2nd International 
Conference on Multimedia Technology 
(ICMT2011), Hangzhou Dianzi 
University, China , July 26-28, 2011. 
17. Che-Lun Hung*, Chun-Yuan Lin, 
Shih-Cheng Chang, Yeh-Ching Chung, 
Shu-Ju Hsieh, Chuan-Yi Tang, and 
Yaw-Ling Lin, "Multiple genome 
sequences alignment algorithm based on 
coding regions", International Journal of 
Computational Biology and Drug Design, 
Vol. 4, No. 2, pp. 165-178, 2011,06. 
18. Che-Lun Hung*, Yaw-Ling Lin, 
Guan-Jie Hua, and Kuan-Ching Li, 
"Haplotype Block Partitioning and 
TagSNP Selection with MapReduce 
framework", The 28th Workshop on 
Combinatorial Mathematics and 
Computation Theory (CMCT 2011), pp. 
51-58, 
2011,05.(NSC99-2632-E-126-001-MY3) 
19. Chen-En Hsieh, Guan-Jie Hua, Yaw-Ling 
Lin, and Che-Lun Hung, "Performances 
of Protein Structure Alignment 
Algorithms under the MapReduce 
Framework", The 29th Workshop on 
Combinatorial Mathematics and 
Computation Theory (CMCT 2012), 
(ISBN:978-9-86032-359-7) pp. 146-153, 
Taipei, Taiwan, April 27-28, 2011. 
20. Che-Lun Hung, Chihan Lee, Chun-Yuan 
Lin, Chih-Hung Chang, Yeh-Ching 
Chung*, Chuan Yi Tang, "Feature 
Amplified Voting Algorithm for 
Functional Analysis of Protein 
Superfamily, " BMC Genomics[SCI], 
(Suppl 3):S14, 2010,11. 
21. Che-Lun Hung, Chun-Yuan Lin, 
Yeh-Ching Chung, Shu Ju Hsieh, Chuan 
Yi Tang, Shih-Cheng Chang, and 
Yaw-Ling Lin, “CORAL-M: Heuristic 
COding Region ALignment Method for 
Multiple Genome Sequences,” IEEE 
International Conference on 
Bioinformatics & Biomedicine Workshop 
on Integrative Data Analysis in Systems 
Biology, DEC 2010. (EI) 
 
5-2-2 「資料探勘組」 
 
已經接受/發表的論文 
1. 賴鈺芬 , 葉介山 , 林彩雯 , "基於
MapReduce技術之時間區段性的網頁
 9 
縮與可回復隱藏的向量量化編碼技術,” 
第八屆實務教學暨資訊實務研討會
(The E-Learning and Information 
Technology Symposium 2013, EITS 
2013)，南台科技大學資訊系主辦，台
南市，March 27, 2013. 
3. 蔡孟靜、胡育誠、黃詩茹、簡苑倩, “植
基於重建階修改的壓縮領域影像認證
技術,” 第八屆實務教學暨資訊實務研
討會 (The E-Learning and Information 
Technology Symposium 2013, EITS 
2013)，南台科技大學資訊系主辦，台
南市，March 27, 2013. 
4. Chin-Chen Chang , Chia-Chen Lin* , 
Li-Ting Liao, “A DCT-based Dual 
Watermark Scheme for Image 
Authentication and Recovery, ” 
Advances in Information Sciences and 
Service Sciences, accepted by March 7, 
2013. (EI) 
5. Y. C. Hu, W. L. Chen, C. C. Lo, C. M. 
Wu, “A Novel Tamper Detection Scheme 
for BTC Compressed Images,” 
Opto-Electronics Review, Vol. 21, No. 1, 
pp. 137-146, March 2013. 
6. Chang, C. C., Nguyen, T. S., Lin, C. C. 
“A Novel VQ-based Reversible Data 
Hiding Scheme by Using Hybrid 
Encoding Strategies,” Journal of Systems 
and Software, (JSS) Vol. 86, No. 2, pp. 
389-402, Feb. 2013. (SCI, Impact 
Factor:1.117) 
7. 蔡孟靜、胡育誠、陳武林, “適用於區塊
截短碼壓縮後影像的竄改偵測技術 ,” 
第十八屆資訊管理暨實務研討會(IMP 
2012)，台北科技大學資訊與運籌管理
研究所主辦，台北市大安區，Dec. 08, 
2012. 
8. 李侑潾, 葉介山*, "運用資料探勘於大
學生的圖書借閱行為與課業學習表現
之研究 ," Taiwan Academic Network 
Conference 2012 (TANET2012), 銘傳大
學, October 23-25, 2012. 
9. 簡暐哲與周文光, "可用於雲端的車牌
定位系方法 (A clouding approach for 
license plate localization) ", submitted to 
2012數位科技與創新管理研討會, 2012
年6月, 華梵大學. 
10. 徐忠禎與周文光,車牌辨識系統之雲端
應用(A Clouding Approach for Vehicle 
License Plate Character Recognition), 
submitted to 2012數位科技與創新管理
研討會, 2012年6月, 華梵大學. 
11. Chia-Chen Lin and Kuan-Yu Chen, "An 
Improvement of Yeh et al.’s RFID 
Scheme", in Proceedings of 2012 
International Conference on e-Commerce, 
e-Administration, e-Society, and 
e-Technology, March 30-April 1, Hong 
Kong. 
12. Chia-Chen Lin and Song-Lin Tsai, "An 
XML-based Access Control Mechanism 
for Cloud-based Enterprise", in 
Proceedings of 2012 International 
Conference on e-Commerce, 
e-Administration, e-Society, and 
e-Technology, March 30-April 1, Hong 
Kong. 
董文豪、胡育誠、梁峻綸、周文光, "適
用於影像解碼賽克的完整性驗證架構", 
第七屆智慧生活研討會(ILT 2011)，勤
益科技大學主辦，台中市太平區，June 
1, 2 
 
 
 
 11 
二、緣由與目的 
 
Recently, an Internet service concept 
known as cloud computing has become 
popular for providing various services to 
users. The cloud computing environment is a 
distributed system with extremely scalable 
IT-related capabilities, providing multiple 
external customers with numerous services. 
Cloud computing also enables the copying of 
vast datasets to many users  with high fault 
tolerance. Another popular open-source 
software framework designed for 
data-intensive distribution is Hadoop. This 
framework processes petabytes of data 
intercepting thousands of nodes. Hadoop 
provides the MapReduce programming 
model, by which parallel computing of large 
data sets can be implemented in the Cloud 
Computing environment. MapReduce 
enables distributed computing of the mappers 
and reducers. Each mapper performs an 
independent map operation which is 
parallelized with the tasks of other mappers. 
Similarly, a set of reducers can perform a set 
of reduce operations. All outputs of the map 
operations possessing the same key are 
presented to the same reducer at the same 
time. Two additional important benefits of 
Hadoop are scalability and fault tolerance. 
Hadoop can guide jobs toward successful 
completion even when individual nodes or 
network components experience high failure 
rates. Meanwhile, a machine can be readily 
attached as a mapper and reducer in the 
Hadoop cluster. The Hadoop platform, 
therefore, is regarded as a superior solution 
to real-world data-distribution problems. To 
date, Hadoop has been applied in a range of 
bioinformatics domains 
Cloud computing platforms are usually 
based on virtualization technology. 
Computing resources are combined or 
divided into one or more operating 
environments using methodologies such as 
hardware and software partitioning or 
aggregation, partial or complete machine 
simulation, emulation and time-sharing. A 
virtual machine (VM) is a machine 
simulation created by virtualization 
technology, which resides in a physical 
machine and shares its physical resources. 
The web service Amazon Elastic Compute 
Cloud (Amazon EC2) uses virtualization 
technology to generate resizable computing 
capacity in the cloud. The service provides a 
true virtual computing environment, allowing 
users to launch VMs with a variety of 
operating systems. Users can construct their 
own elastic cluster systems by attaching or 
removing VMs. 
In this project, we have developed a cloud 
computing platform based on Hadoop and 
VM technologies to copy with bio-data. We 
proposed four bioinformatics cloud services 
based on the developed platform: Haplotype 
Block Partitioning and Tag SNP Selection, 
Protein Structure Alignment, Protein-Ligand 
Binding Site Comparison and Open Reading 
Frame Phylogenetic Analysis. All of these 
services are available, and users can utilize 
these services on-line. All of proposed 
methods are listed as followings, 
 
1. Haplotype Block Partitioning and Tag 
SNP Selection 
A SNP (Single Nucleotide 
Polymorphisms) is defined as a position in a 
chromosome where each one of two (or more) 
specific nucleotides is observed in at least 
10% of the population. The nucleotides 
 13 
others. 
We are mainly interested in the common 
haplotypes. Therefore we require that, in the 
final block partition, a significant fraction of 
the haplotypes in each block is common 
haplotypes.  Patil et al. require that at least 
α = 70%, 80%, and 90%, respectively, of the 
unambiguous haplotypes appear more than 
once. The parameter, α, is also referred to as 
the coverage of common haplotypes in a 
block. Ambiguous haplotypes are not 
included in calculating percent coverage. The 
coverage of block B can be mathematically 
formulated as a form of diversity: 
S
U
C
U
BS  1)(
 1
 
Here U denotes the number of 
unambiguous haplotypes, C denotes the 
number of common haplotypes, and S 
denotes the number of singleton haplotypes.  
For example, the submatrix M(7, 11) of 
Figure. 1 can be viewed as a sample S = 
{0001, 0011, 0100, 1000, 1011}.  It follows 
that δ(S) = δ(M(8, 11)) =1 － 4/7 = 3/7. 
 
1.3 Haplotype Blocks Partitioning 
Before partitioning the haplotype blocks, 
the farthest site (good partner) for each SNP 
marker need to be found. For each SNP 
marker j, if a left farthest marker i exists, so 
that [i, j] is the longest haplotype block 
among all feasible blocks that terminated at 
site j. The intervals [i',  j]  [i, j] are not 
feasible blocks. The set of left good partners 
Li for each SNP marker i, Li = {x|[x, i] is a 
feasible haplotype block}. A candidate 
haplotype block used for selecting the 
tagSNP is decided by that the diversity value 
of this black calculated by function 1 must 
smaller than the diversity limit D. Denote the 
farthest site j = L[i] for each site i as the 
largest site j with the diversity δ(i, j)  D. 
Given a haplotype matrix A and a diversity 
upper limit D, let S = {B1, B2, …, Bk} be a 
segmentation of A with δ(B)  D for each B 
 S. The length of S is the total length of all 
blocks in S; i.e., |S| = |B1| + |B2|+…+|Bk|. A 
block length evaluation function is defined as  
 
f(k, i, j) = max{ |S|   S a feasible 
segmentation of A(i, j) with k blocks}. 
 
The block length evaluation function for 
the k-longest-blocks is f(k, 1, n). To include 
left farthest sites to the k-longest-blocks, the 
function can be verified that  
 
f(k, 1, j) = max{ f(k, 1, j－1), f(k－1, 1, L[j]
－1) + j－L[j] + 1} 
 
That is, the k-th block of the maximal 
segment S in [1, j] either does not include site 
j; otherwise, the block [L[j], j] must be the 
last block of S. 
 
1.4  TagSNPs Selection 
For each block, we want to minimize 
the number of SNPs that uniquely distinguish 
at least 80% (the α parameter) of the 
unambiguous haplotypes in the block. Those 
SNPs can be interpreted as a signature of the 
haplotype block partition. They are referred 
to as tagSNPs that are able to capture most of 
the haplotype diversity, and therefore, could 
potentially capture most of the information 
for association between a trait and the marker 
loci. 
Our strategy for selecting the tagSNPs in 
haplotype blocks is as the following. First, 
 15 
{δ(i •N/L, i•N/L), δ(i•N/L, i•N/L+1), …,  
δ(i•N/L+N/L, i•N/L+N/L)}. 
 
 
Figure 2. Haplotype block partitioning and 
selection on MapReduce framework. 
 
Therefore, each map has (N/L)
2 
diversity 
scores.  Reduce stage performs haplotype 
block selection algorithm.  In our algorithm, 
just one reduce operation is needed in the 
reduce stage. Since the selection is a linear 
time algorithm, it is not necessary to perform 
the computation in parallel.  The reduce 
operation finds the longest block by merging 
blocks with the interesting diversity scores. 
 
2. Protein Structure Alignment 
 
A. Protein structure alignment and 
refinement 
Protein structure alignment detects 
homologous polymer structures based on 
shape and three-dimensional conformation. 
Protein structural alignment tools detect the 
evolutionary relationships between proteins 
by comparing proteins with low sequence 
similarity. In general, the outputs of a 
structural alignment tool are a superposition 
of the atomic coordinate sets and the 
minimum root mean square deviation (RMSD) 
between the structures. The RMSD of two 
aligned structures indicates their divergence 
from each other. Therefore, RMSD measures 
the accuracy of the structural alignments. The 
smaller the RMSD value the more accurate 
the structural alignment. The RMSD is 
defined below.  
Let T = (t1,…,tn) and L = (l1,…,ln) be two 
sequences of points. The i-th  ZYX ,,  
coordinate value of a point in x is denoted xi, 
and |x| denotes the length of x. Let d be the 
RMSD function which produces the RMSD 
value, then 
 
2
1
1,,, 


n
k
kkn
lRtRLTd   
, where R is a rotation matrix and  is a 
translation vector. The minimum RMSD 
value d(T, L) between T and L is deﬁned as 
d(T, L) = min {d(T, L, R, )}. 
 
The proposed cloud computing service for 
protein structure alignment comprises two 
main stages; structural alignment and 
alignment refinement. The refinement 
strategy adopts two approaches, 
mini-bipartite and parametric adjustment. 
The proposed protein structure alignment is 
operated as follows: 
 
 
Figure 3. Procedure of the refinement 
stage. 
 17 
the RMSD value, is calculated after three self 
rotations in the above example. Since the 
number of rotations is unlimited, many 
RMSD values can be computed from Rs 
calculated by various sets of unit vectors. 
 
 Minimum RMSD Finding 
Since smaller RMSD value implies higher 
structural alignment accuracy, the proposed 
refinement algorithm seeks an alignment that 
minimizes the RMSD. The minimum 
bipartite matching algorithm identifies the 
two sets of unit vectors with the smallest 
RMSD value. We adopt the Munkres 
algorithm in this step. Let P' and Q' be 
translated from P and Q, respectively. The 
mass centers of P' and Q' remain at their 
respective original locations P and Q. Giving 
a weighed graph G = (V, E), V is labeled with 
points of P' and Q', and each (p, q) in E is 
weighted by the squared Euclidean distance. 
The RMSD of the final alignment is reduced 
by pair matching. 
 
 Angle triplet adjustment  
The RMSD values and unit vectors are 
related through the isometric rotation 
transformation formula. Although minimum 
bipartite matching identified the smallest 
RMSD values from various rotations, the 
RMSD is reduced further by adjusting unit 
vectors with angle triplets. In this step, angle 
triplets are adjusted by trigonometric series 
to form different unit vectors. 
 
Figure 4. Implementation of Hadoop 
map/reduce model. 
Trigonometric series can approximate the 
angle triplets with smaller RMSD values. 
The angles ,  and  are sequentially 
adjusted, and the evaluation function 
f()corresponds to the RMSD values altered 
by the adjustments. The f() is defined as 
follows: 
  ,sincos
2
11 





 


k
i
ii iCiCCf 
 
where k is the number of local maximum 
vectors and  = (, , ). The adjusted angles 
evaluated by f() constitute the new 
parameters in the isometric rotation 
transformation. The refinement step is 
performed iteratively until degree k is 
reached.  
Most existing alignment tools are 
computationally time-consuming, and are 
best implemented under powerful parallel 
processing. Moreover, the user expects that 
the computational alignment process never 
fails. Therefore, fault tolerance and high 
availability are important issues in current 
computational services. 
 
B. Cloud computing platform 
The proposed cloud computing platform 
combines two technologies; the Hadoop 
framework and virtualization. The protein 
structure alignment and the proposed 
refinement algorithm are implemented in 
Hadoop and are deployed on a virtualized 
computing environment. 
Hadoop is a distribution computation 
framework that coordinates computing nodes 
for parallelized data distribution. It adopts the 
two-layer Map/Reduce parallel programming 
model. Many cloud computing vendors, such 
as such as Yahoo, Amazon EC2, IBM and 
 19 
proteins, nucleic acids, and complex 
assemblies. The PDB ID identifies a specific 
protein structure. The submitted PDB ID pair 
is stored in a job queue file. Assuming that N 
task-trackers must distribute P PDB ID pairs 
in the job queue file, the ith line in the queue 
file will as assigned as the ith map task and 
sent to Hadoop by streaming operation. Each 
task-tracker node receives a map task which 
aligns the protein structure and executes the 
refinement algorithm. The refined alignment 
is converted to a 3D protein structure image 
using the PDB2VRML tool. When a 
task-tracker node has completed a map task, 
it passes the score to a Reducer and executes 
a new map task. Computation continues until 
all map tasks are complete. Generally, each 
task-tracker node is assigned P/N map tasks. 
In the proposed cloud computing service, the 
reduce task that collects the RMSD value of 
each PBD ID pair is performed solely by the 
Reducer. Finally, the reducer stores the 
RMSD values in a file by HDFS. 
 
 
Figure 6. Cloud service portal. 
 
The proposed cloud computing service for 
protein structure alignment can be regarded 
as a BaaS (Bioinformatics as a Service). The 
proposed service, accessible through the 
Internet, enables molecular biologists to 
efficiently execute 3D protein structure 
alignment. Supplied with two user-input 
PDB IDs, the service searches protein 
structure data archived in the wwPDB and 
compares the protein structures using 
Hadoop.  
The proposed service provides users with a 
hyperlink for accessing the alignment result 
before the computation is complete. In this 
way, the user can repeatedly view and 
download the result. This hyperlink is 
accessible either from the web site or by 
email. The portal of the proposed service is 
illustrated in Fig 6. Figure 7 shows the 
submitted job information, including the 
hyperlink enabling result download. Figure 8 
shows the output of the proposed service, 
including a 3D structural image of the protein 
and RMSD values.  
 
 
Figure 7. The webpage indicates a submitted 
request of protein structure alignment. 
 
 
Figure 8. Protein structure alignment 
and 3D structural image produced by the 
cloud service. 
 21 
Hadoop framework. Each input file is split 
into data blocks that are distributed to 
data-nodes. Hadoop also creates multiple 
replicas of data blocks and distributes them 
to data-nodes throughout a cluster, ensuring 
reliable, extremely rapid computations. The 
name-node serves as both a directory 
namespace manager and a node metadata 
manager for the HDFS. The HDFS 
architecture operates on a single name-node.  
Resource capacity permitting, 
virtualization technology can host several 
virtual machines within a physical machine. 
The proposed cloud service platform 
combines Hadoop and virtualization 
technology, such that all nodes of the 
Hadoop cluster reside in VMs. The cloud 
computing architecture of Cloud-PLBS is 
illustrated in Fig. 11. As shown in that figure, 
master node (name-node) and slave node 
(data-node) constitute the master VM and 
slave VM, respectively. Submitted SMAP 
jobs are recorded in a job queue. The master 
node periodically obtains SMAP jobs from 
the job queue and assigns them to slave 
nodes; a slave node (or mapper) performs the 
task. Once all of the SMAP jobs are complete, 
the reducer collects the comparison results 
from all mappers and stores them in the 
Network File System storage (NFS). A single 
comparison result is stored in a single file in 
NFS. This architecture imbues Cloud-PLBS 
with three desirable characteristics: high 
performance, scalability and availability. 
 
 
Figure 9. Map/Reduce framework of 
Hadoop. 
 
 
Figure 10. The architecture of Hadoop 
cluster. 
 High performance 
In Cloud-PLBS, the SMAP jobs are 
performed in parallel by the map/reduce 
framework. The number of SMAP jobs that 
can be performed simultaneously is the 
number of data nodes. If the number of 
SMAP jobs exceeds the number of data 
nodes, the number node assigns the 
remaining jobs as soon as a data node 
becomes available. 
 
 Availability 
In the event of system failure, 
Cloud-PLBS continues performing SMAP 
jobs via the Hadoop fault tolerance 
mechanism. When a data node (mapper) fails 
during SMAP computation, name-node 
reassigns its job to another slave node 
(mapper). Therefore, in Cloud-PLBS, all of 
the submitted SMAP jobs are executed in the 
event of data-node failure. A hardware 
failure on the physical server will terminate 
all virtual machines running on it. In this 
more catastrophic event, SMAP jobs can be 
reassigned to several new virtual machines 
created on available hosts. As a result of this 
operation, Cloud-SMAP has high 
availability. 
 23 
 
 
(c) Upload file 
 
 
Figure 13. The result produced by 
Cloud-PLBS. The protein IDs are 101M and 
1O0D. 
 
4. Open reading 
Phylogenetic analysis 
The proposed cloud service provides a 
completed phylogenetic analysis by 
integrating ORF finding process, 
phylogenetic tree contractions and ORF 
diversity analysis. The procedure of this 
phylogenetic analysis is shown in figure 14. 
The steps are listed as followings: 
 
Step 1. Detecting open reading frames 
This step is to detect the ORFs that have 
functions from sequences. A protein 
sequence has many ORFs, however only few 
ORFs are significant. The ORF finder is a 
graphical analysis tool that finds all open 
reading frames of a selectable minimum size 
in a sequence. The tool we used is ORF 
Finder that is common-used on NCBI tools 
website. This tool identifies all open reading 
frames using the standard or alternative 
genetic codes. 
 
 
Figure 14. The procedure of phylogenetic 
analysis. 
 
Step 2. Constructing phylogenetic tree based 
on open reading frames 
A phylogenetic tree or evolutionary tree is 
a branching diagram, also named tree, 
showing the inferred evolutionary 
relationships among various biological 
species or other entities according to the 
similarities and differences in their physical 
and/or genetic characteristics. The taxa 
joined together in the tree are implied to have 
descended from a common ancestor. Usually, 
the phylogenetic analysis aligns all sequences 
with whole lengths.  However, the 
phylogenetic tree might be different 
 25 
the final result. The steps to achieve this 
service are listed as followings:  
 
Step 1. Job submission: 
Users can submit their job online 
through the web portal of the proposed cloud 
service. Users can enter the comparative 
DNA/RNA sequences on the web portal or 
upload file that records comparative RNA 
sequences from web portal. 
 
Step 2. Sequence translation:  
To detect the ORF regions, all input 
RNA sequences should be translated to 
protein sequences according to the genetic 
code. The genetic code is the set of rules by 
which information encoded within 
RNA sequences is translated into proteins. 
Every three consecutive nucleotides, called 
codon, in RNA sequence usually represent a 
single amino acid according to a 
corresponding genetic code. The code defines 
how sequences of codons, specify which 
amino acid will be added next during protein 
synthesis. Usually, a codon in a nucleic acid 
sequence specifies a single amino acid. Figure 
16 shows these genetic codes.  
 
Step 3. Phylogenetic analysis:  
This step is to find the ORFs that have 
functions. A protein sequence has many 
ORFs, however only few ORFs are 
significant. In our service, user can provide 
the length of ORFs that is considered as 
meaningful. Then, the service can find the 
significant ORFs. Figure 17 shows the 
example of ORFs. In this example, the first 
ORF is from position 3 to 5099 in the 
sequence AB447445, and this ORF is 
denoted as AB447445_1. In this step, two 
Figure 15. Flowchart of cloud-based ORF phylogenetic analysis service. 
 27 
calculate diversity scores of blocks. 
To assess the performance of the proposed 
Hadoop MapReduce algorithm, we compare 
the computational time between various 
sequence data and various number of 
map/reduce operations.  Two factors, 
number of patterns and the length of patterns, 
affect the performance of sequential 
algorithm and the proposed algorithm.  Patil 
et al. proposed that the haplotype block can 
be found within 300bp and 500bp.  
Therefore, the block size can be 300bp and 
500bp.  The diversity scores are calculated 
according to their corresponding block sizes; 
these scores are {δ(1, 1), δ(1, 2), …, δ(1, 
500), δ(2, 2), …, δ(2, 501), δ(3, 3), …, δ(L, 
L)}.  Figures 18 and 19 illustrate the 
comparisons between sequential algorithm 
and our proposed algorithm under the 
MapReduce framework.  In Fig. 6 and Fig.7, 
the block sizes are 300bp and 500bp, 
respectively. It is observed that 
computational time increases corresponding 
to number of pattern and sequence length.  
The computational time for our algorithm 
with block size 300bp is less than that with 
block size 500bp.  More patterns and longer 
sequence length lead to higher computational 
cost.  These experimental results are 
corresponding to the algorithm analysis in 
previous section. 
The experimental results reveal that the 
computational time is effectively reduced 
when more map operations are deployed.  
Two and four map operations almost 
improve the computation time by factors of 
two and four times accordingly, comparing to 
the original sequential algorithm, 
respectively. Moderate enhancements 
between 8 and 16 map operations are 
observed in all experiments, since the size of 
data set split by 8 is similar to that by 16.  
These figures illustrate the computation 
efficiency can be effectively improved 
proportional to the number of processors 
being used. 
 
 
 
 29 
approximately 7% and 6%, respectively. 
Clearly, the proposed refinement algorithm 
can significantly improve the RMSD values 
produced by standard protein structure 
alignment methods. 
 
Table 1: RMSD computed by our 
proposed algorithm, DALI, and VAST. 
 
Protein 
structure  
alignment 
methods 
Original 
RMSD 
value 
The 
average 
RMSD 
value 
refined by 
bipartite 
matching 
The average 
RMSD value 
after the 
proposed 
refinement 
algorithm 
 
DALI 1.5767 1.5166 1.4713 
VAST 1.5114 1.4664 1.4228 
 
 
To assess the performance of the proposed 
cloud service based on the Hadoop 
framework, the execution time of the service 
was compared for varying structural data size 
and number of Map/Reduce operations. 
Figure 20 illustrates the performance of the 
proposed service under the MapReduce 
framework. The execution time is effectively 
reduced when more map operations are 
deployed. 
Compared to the sequential algorithm 
(implemented in the proposed service with a 
single mapper), introduction of two, four, and 
eight mappers improved the execution time 
by approximate factors of two, four and eight. 
The computation efficiency is improved by 
an amount proportional to the number of 
mappers, although the execution time 
increases as the number of protein pairs and 
protein atoms increases (see Figure 20). We 
infer that the Hadoop framework 
significantly reduces the computational cost. 
 
 
 
 
Figure 20: RMSD values produced by 
various numbers of Mappers. 
 
3. Protein-Ligand Binding Site 
Comparison 
Cloud-PLBS is a software (SaaS) as a 
service service operating under the Hadoop 
framework and virtualization technology. 
The cloud computing platform is composed 
of an NFS server and four IBM blade servers 
in the Providence University Cloud 
Computation Laboratory. Each server is 
equipped with two Quad-Core Intel Xeon 
2.26 GHz CPUs, 24 G RAMs, and 296 G 
disks. Each server can accommodate 8 virtual 
machines; each virtual machine is set to one 
core CPU, 2 G RAM, and 30 G disk running 
 31 
 
 
Figure 22: Execution speed of sequential 
SMAP program and Cloud-PLBS using 2, 4, 
and 6 mappers. 
 
4. Open Reading Frame Phylogenetic 
Analysis 
The proposed cloud service for virus 
analysis was performed on four IBM blade 
servers. Each server was equipped with two 
Quad-Core Intel Xeon 2.26 GHz CPUs, 24 
GB RAM, and 296 GB hard disk, running 
under the Ubuntu operating system version 
10.4, with 8 virtual machines on each server. 
Hadoop version 0.2 MapReduce platform 
was installed on each server. One VM 
constituted the job tracker and name node; 
the others are task trackers and data nodes. 
The job tracker is also the portal of our cloud 
service. The portal is depicted in Figure 23. 
Our current cloud environment permits 
eight virtual machines. Two of these VMs 
are name node and data node running the 
Reducer; the remaining six are responsible 
for map operation. For the experiment, we 
randomly produced three datasets, each 
containing 20 sequences of different lengths 
(300, 400, and 600 nucleotides). All 
sequences in each dataset were compared by 
phylogenetic analysis methods. ClustalW and 
the proposed service were applied three times, 
for simulating three ORF phylogenetic 
analyses. 
The computation time of the proposed 
service illustrated in Figure 24 is 
proportional to the number of mappers. The 
execution time is considerably reduced when 
six mappers are used, relative to two mappers. 
Figure 25 compares the performance between 
sequential phylogenetic analysis methods 
such as ClustalW and the proposed service 
with six mappers, for different sequence 
lengths. Clearly, the proposed service in the 
Hadoop framework achieves better 
performance than standard sequential 
phylogenetic analysis. 
 
 
Figure 23: Portal of cloud-based ORF 
phylogenetic analysis service. 
 
 
Figure 24: Computation time of cloud-based 
ORF phylogenetic analysis with different 
 33 
9. European Molecular Biology 
Laboratory (EMBL), 
http://www.ebi.ac.uk/embl/. 
10. M. Sanjay Ram and V. Vijayaraj, 
“Analysis of the characteristics and 
trusted security of cloud computing,” 
International Journal on Cloud 
Computing, vol. 1, pp. 61–69, 2011. 
11. Amazon EC2, 
http://aws.amazon.com/ec2/. 
12. Google app Engine, 
http://code.google.com/appengine/. 
13. WindowsAzure, 
http://www.microsoft.com/windowsazur
e/windowsazure/. 
14. D. Nurmi, R. Wolski, C. Grzegorczyk et 
al., “The eucalyptus open-source 
cloud-computing system,” in 
Proceedings of the Cloud Computing 
and Its Applications (CCA '08), pp. 
124–131, May 2009. View at Publisher · 
View at Google Scholar · View at 
Scopus 
15. P. Watson, P. Lord, F. Gibson, P. 
Periorellis, and G. Pitsilis, “Cloud 
computing for e-science with carmen,” 
in Proceedings of the 2nd Iberian Grid 
Infrastructure Conference Proceedings 
(IBERGRID '08), pp. 1–5, 2008. 
16. B. Rochwerger, D. Breitgand, E. Levy et 
al., “The Reservoir model and 
architecture for open federated cloud 
computing,” IBM Journal of Research 
and Development, vol. 53, no. 4, pp. 
535–545, 2009. View at Scopus 
17. C. Jin and R. Buyya, “MapReduce 
programming model for. NET-based 
cloud computing,” Lecture Notes in 
Computer Science, vol. 5704, pp. 
417–428, 2009. View at Publisher · 
View at Google Scholar · View at 
Scopus 
18. Hadoop, http://hadoop.apache.org/. 
19. D. Borthakur, The Hadoop Distributed 
File System: Architecture and Design, 
2007. 
20. A. Matsunaga, M. Tsugawa, and J. 
Fortes, “CloudBLAST: combining 
MapReduce and virtualization on 
distributed resources for bioinformatics 
applications,” in Proceedings of the 4th 
IEEE International Conference on 
eScience (eScience '08), pp. 222–229, 
December 2008. View at Publisher · 
View at Google Scholar · View at 
Scopus 
21. B. Langmead, M. C. Schatz, J. Lin, M. 
Pop, and S. L. Salzberg, “Searching for 
SNPs with cloud computing,” Genome 
Biology, vol. 10, no. 11, article R134, 
2009. View at Publisher · View at 
Google Scholar · View at Scopus 
22. K. Motomura, T. Oka, M. Yokoyama et 
al., “Identification of monomorphic and 
divergent haplotypes in the 2006-2007 
norovirus GII/4 epidemic population by 
genomewide tracing of evolutionary 
history,” Journal of Virology, vol. 82, no. 
22, pp. 11247–11262, 2008. View at 
Publisher · View at Google Scholar · 
View at Scopus 
23. J. D. Thompson, D. G. Higgins, and T. J. 
Gibson, “CLUSTAL W: improving the 
sensitivity of progressive multiple 
sequence alignment through sequence 
weighting, position-specific gap 
penalties and weight matrix choice,” 
Nucleic Acids Research, vol. 22, no. 22, 
pp. 4673–4680, 1994. View at Scopus 
24. A. Bessani, V. V. Cogo, M. Correia, et 
al., “Making Hadoop MapReduce 
Byzantine Fault-Tolerant,” 
http://www.gsd.inesc-id.pt/~mpc/pubs/b
ft-mapreduce-fa-dsn10.pdf. 
25. G. Eason, B. Noble, and I. N. Sneddon, 
“On certain integrals of 
Lipschitz-Hankel type involving 
products of Bessel functions,” 
Philosophical Transactions of the Royal 
Society London A, vol. 247, pp. 
529–551, 1955. 
26. J. N. Xi, D. Y. Graham, K. Wang, and M. 
K. Estes, “Norwalk virus genome 
cloning and characterization,” Science, 
vol. 250, no. 4987, pp. 1580–1583, 1990. 
View at Scopus 
27. G. Belliot, S. V. Sosnovtsev, T. Mitra, C. 
Hammer, M. Garfield, and K. Y. Green, 
“In vitro proteolytic processing of the 
MD145 Norovirus ORF1 nonstructural 
polyprotein yields stable precursors and 
products similar to those detected in 
calicivirus-infected cells,” Journal of 
Virology, vol. 77, no. 20, pp. 
10957–10974, 2003. View at Publisher · 
View at Google Scholar · View at 
 35 
Shu-Ju Hsieh, Chuan-Yi Tang, and 
Yaw-Ling Lin, "Multiple genome 
sequences alignment algorithm based on 
coding regions," International Journal of 
Computational Biology and Drug 
Design, Vol. 4, No. 2, pp. 165-178, June 
28, 2011. 
7. Sheng-Ta Lee, Chun-Yuan Lin, 
Che-Lun Hung and Hsuan Ying Huang, 
"Using frequency distance filteration for 
reducing database search workload on 
GPU-based cloud service," CloudCom 
2012, pp. 735-740. 
8. Che-Lun Hung and Chun-Yuan Lin, 
"Cloud service of analyzing virus data: 
A case study for Norovirus," CloudCom 
2012, pp. 747-752. 
9. Che-Lun Hung, Yaw-Ling Lin, Chen-En 
Hsieh and Guan-Jie Hua, "Efficient 
protein structure alignment algorithms 
under the MapReduce framework," 
CloudCom 2012, pp. 753-758. 
10. Yu-Rong Chen, Che-Lun Hung, 
Yu-Shiang Lin, Chun-Yuan Lin, 
Tien-Lin Lee and Kual-Zheng Lee, 
“Parallel UPGMA Algorithm on 
Graphics Processing Units Using 
CUDA,” The 14th IEEE International 
Conference on High Performance 
Computing and Communications, pp. 
849-854, 2012. 
11. Che-Lun Hung, Yu-Chen Hu and 
Kuan-Ching Li, "Dynamic 
Power-Saving Model for Cloud 
Computing System", The 2012 
International Conference on Information 
Science and Technology (IST 2012), 
April 28-30, Shanghai, China. 
12. Che-Lun Hung, Hsiao-Hsi Wang, 
Yu-Chen Hu, " Efficient Load Balancing 
Algorithm for Cloud Computing 
Network ", The 2012 International 
Conference on Information Science and 
Technology (IST 2012), April 28-30, 
Shanghai, China. 
13. Che-Lun Hung, Yaw-Ling Lin, 
Kuan-Ching Li, and Guan-Jie Hua, 
"CloudTSS: A TagSNP Selection 
approach on Cloud Computing," 
InCoB/ISCB-Asia Joint Conference 
2011, Kuala Lumpur, Malaysia, 2011. 
14. Chun Yuan Lin, Chuan Yi Tang, 
Sheng-Ta Li, Yaw-Ling Lin, Che-Lun 
Hung, "CUDA-FRESCO: 
Frequency-Based RE-Sequencing Tool 
Based on CO-clustering Segmentation 
by GPU", IEEE 13th International 
Conference on High Performance 
Computing and Communications 
(HPCC), Sept. 2011, pp. 857-862. 
15. Che-Lun Hung, Chun-Yuan Lin, 
Yeh-Ching Chung, Shu Ju Hsieh, Chuan 
Yi Tang, Shih-Cheng Chang, and 
Yaw-Ling Lin, “CORAL-M: Heuristic 
COding Region ALignment Method for 
Multiple Genome Sequences,” IEEE 
International Conference on 
Bioinformatics & Biomedicine 
Workshop on Integrative Data Analysis 
in Systems Biology, DEC 2010. (EI) 
16. Che-Lun Hung, Chun-Yuan Lin, 
Guan-Jie Hua, Yu-Chen Hu, “CloudTSS: 
A TagSNP Selection Approach on Cloud 
Computing,” Grid and Distributed 
Computing, Communications in 
Computer and Information Science, (T. 
H. Kim et. Al., Eds.), Springer-Verlag, 
Vol. 261, pp. 525-534, 2011. [EI] 
17. Che-Lun Hung, Chun-Yuan Lin, 
Yu-Chen Hu, “UNION: An Efficient 
Mapping Tool Using UniMark with 
Non-overlapping Interval Indexing 
Strategy,” Database Theory and 
Application, Bio-Science and 
Bio-Technology, Communications in 
Computer and Information Science, Vol. 
258, pp. 187-196, 2011. 
 37 
二、緣由與目的 
 
(一) 前言 
網站使用度探勘(Web Usage Mining)主
要是藉由網頁記錄檔做分析，在有用的網
頁中找出隱而未覺的瀏覽樣式。2007年，
Zhou等學者提出把利潤探勘演算法從商品
的計算改變成網頁記錄檔的分析，透過兩
階段利潤探勘方式，可以找尋通過門檻值
的高利潤路徑瀏覽模式。2010年，Chen和
Yeh學者提出網頁瀏覽樣式之利潤偏好度
探勘(Utility Preference Mining)，做法上與
傳統不同的是加入選擇偏好度與時間偏好
度來定義網頁的利潤價值。透過探勘的結
果，網頁伺服器可以預測接下來被存取的
網頁，並且提供網頁設計師了解更多有意
義的資訊。 
2011年Lan等學者提出以時區性的利潤
探勘找出有超過門檻值的高利潤項目，該
兩階段演算法，於第一階段找出不同時區
超過門檻值的候選項目集，於第二階段以
第一階段有過門檻值的項目集計算實際利
潤，如果有超過門檻值就屬於高利潤商品
組合。相同地，這樣的現象也常發生在網
站上，網頁瀏覽次數會因為季節或是月份
而有所變化，例如：購物網站中，在5-9月
夏季的商品瀏覽次數的頻率是比較高的。
所以Supriya和Indira所提出以二階段演算
法使用分時區的概念計算網頁紀錄，這樣
可以提供網頁設計師以分時區性的結果去
針對網頁加以改善。 
本研究提出一個新的網頁瀏覽樣式利潤
偏好度探勘模型，接著提出不同時間區段
的網頁瀏覽樣式之偏好度探勘的演算法能
更精準的找出項目集。此外針對資料量越
來越來大的環境下，本研究將網頁探勘結
合雲端Hadoop平台環境撰寫MapReduce的
程式架構，去執行分散式運算，並且使演
算法達到最好的效能。 
在實驗部分，分為真實資料與人工資料，
真實資料採用靜宜大學招生組的網頁紀錄
檔，經過多次的參數設定，可以明顯看出
執行時間的差異。實驗結果顯示本研究所
提出的演算法能更精確找出有用的項目集
並且降低執行時間。 
 
(二) 研究目的 
本計畫主要的目的與重要性有以下： 
 
1.不同時間區段的網頁瀏覽樣式之偏好
度探勘的演算法能更精準的找出項目集。 
網頁瀏覽次數會因為季節或是月份而有
所變化，例如：購物網站中，在5-9月夏季
的商品瀏覽次數的頻率是比較高的。本研
究提出一個新的網頁瀏覽樣式利潤偏好度
探勘模型，接著提出不同時間區段的網頁
瀏覽樣式之偏好度探勘的演算法能更精準
的找出項目集。這樣可以提供網頁設計師
以分時區性的結果去針對網頁加以改善。 
 
2. 以 雲 端 Hadoop 平 台 環 境 之
MapReduce技術改善網頁瀏覽樣式利潤偏
好度探勘之效能問題。 
網頁瀏覽樣式利潤偏好度探勘中，從web 
log之資料預處理到利潤偏好度的計算，皆
非符合MapReduce技術的平行化運算，而
透過雲端Hadoop平台環境亦可儲存與處理
大量的web log之資料。 
 
三、結果與討論 
 
3.1 實驗環境 
本研究所使用的實驗環境為 Dell 伺服
器，雙 AMD Opteron(tm) Processor 4180 
2.6 GHz、六核心 CPU、16G RAM。Hadoop
平台是以Dell伺服器切割成 8台虛擬機器，
每一台虛擬機器的規格有 1 個 CPU 、1 G 
memory。實驗分別運行在 1台、2台、4
台以及 8台虛擬機器下，以觀測各別的執
行效能。本研究採用之 Hadoop 版本為
 39 
 
表 5 實驗一資料量為 705MB(單位：秒) 
1~10天  705MB 
門檻值 1-node 2-node 4-node 8-node 
0.1 417.53 324.39 282.76 274.49 
0.075 456.19 353.07 315.44 306.53 
0.05 503.42 381.17 339.53 331.65 
0.025 706.35 502.51 457.38 448.29 
 
表 6 實驗一資料量為 1.39GB(單位：秒) 
1~20天 1.39GB 
門檻值 1-node 2-node 4-node 8-node 
0.1 617.49 454.06 386.16 344.69 
0.075 706.56 526.01 452.79 411.34 
0.05 835.6 607.34 520.63 477.25 
0.025 1494.16 955.82 838.73 800.57 
 
表 7 實驗一資料量為 2.26GB(單位：秒) 
1~31天 2.26GB 
門檻值 1-node 2-node 4-node 8-node 
0.1 817.92 575.94 455.76 398.93 
0.075 928.95 650.13 542.22 483.05 
0.05 1134.87 740.29 642.22 571.48 
0.025 2369.52 1369.01 1278.19 1230.05 
 
錯誤! 找不到參照來源。顯示不同資料量在
不同虛擬環境的執行時間。當資料量為 705MB
的時候，可以看出來其實 8台虛擬機器的時間
很接近 4台虛擬機器，這個結果是因為資料量
比較小，所以效能上的比較沒有太大的差異。 
 
 
圖 1不同資料量在不同虛擬機器的執行
時間 
 
為了凸顯 Hadoop的特性，本研究擴大資
料量，將資料量分成 1個月、2個月以及 3
個月， 圖 2顯示當資料量越大的時候在不
同虛擬機器之間執行的時間差異越大，凸
顯 Hadoop適合在大量的資料下運行。 
 
 
圖 2 一、二、三個月的資料量分別在不
同虛擬機器的執行時間 
 
圖 3 顯示虛擬機器的執行時間，門檻值
0.05-0.025的時間會聚增是當門檻值到
0.025 的時候有很多網頁會超過此門檻值，
在排列候選項目集的時候會有比較多的網
頁組合，所以計算上需要較久的時間。 
 
 
圖 3 顯示不同虛擬機器的執行時間 
 
圖 4 顯示當門檻值為 0.05 以及 0.1的情
況下，資料在不同虛擬機器個數的執行時
間，由門檻值 0.1的圖表可以看出 705MB、
1.39GB以及 2.26GB因為資料量不夠大的
原因所以斜率慢慢趨近於平緩，而 4.49GB
以及 5.74GB就有可能當 node數超過 8的
時候也會有良好的效果。 
 41 
0.1
5 
51 16 49 12 43 10 
0.1 97 43 181 37 239 34 
0.0
75 
159 66 382 58 382 58 
0.0
5 
424 122 818 175 808 174 
0.0
25 
979 225 
225
2 
587 
263
7 
588 
 
表 10在不同虛擬環境下，以不同的門檻
值所執行的時間(單位：秒) 
(a)705MB 
1~10天 705MB 
 
2-node 4-node 8-node 
0.25 284.2 206.87 194.63 
0.2 287.34 264 248.8 
0.15 312.22 288.85 274.51 
0.1 341.98 316.37 302.2 
0.075 350.32 322.78 310.62 
0.05 381.19 357.87 335.49 
0.025 517.23 472.23 453.14 
 
(b)1.39GB 
1~20天 1.39G 
 
2-node 4-node 8-node 
0.25 388.13 339.99 308.88 
0.2 419.59 364.43 337.62 
0.15 416.79 366.66 339.51 
0.1 475.33 421.91 391.04 
0.075 502.62 455.23 425.41 
0.05 571.08 527.98 498.14 
0.025 1070.4 994.26 941.28 
 
(c)2.26GB 
1~31天 2.26GB 
 2-node 4-node 8-node 
0.25 429.22 350.02 305.05 
0.2 487.41 406.73 361.23 
0.15 517.92 435.91 387.88 
0.1 589.15 509.81 468.98 
0.075 614.14 531.67 487.64 
0.05 720.33 633.41 575.36 
0.025 1457.75 1403.09 1327.38 
 
實驗一與實驗二的差別在於網頁利潤值，
實驗一每個網頁的利潤值都設為 1，實驗二
是以 PageRank 演算法收斂而成，並且兩者
實驗門檻值皆設為 0.04。表 36 Pangrank 計
算過後利潤值前 10名的網頁。然而這兩個
實驗的結果可以看出來利潤值的第四名碩
士班網頁、第五名大學甄選申請網頁以及
第七名暑假轉學考的網頁在實驗二就有超
過門檻值，但因為實驗一門檻值設定 1的
情況下，這三個網頁點選進去過後裡面有
更多詳細內容，所以都只是瀏覽的必經過
程，例如查詢暑假轉學考簡章，就會點選
暑假轉學考的網頁在進入暑假轉學考簡章。
停留的時間也不長，最終就沒有過門檻
值。 
表 11 Pangrank 計算過後利潤值為前 10
名的網頁名稱以及利潤值 
 
 
URL 網頁內容 
1 /~pu1470/ 招生組首頁 
2 
/~pu1470/cgi-bin/bbs/cpsub/s
ub.php 
Q&A 
3 
/~pu1470/admissions/index.ph
p 
招生組首頁 
4 
/~pu1470/admissions/examb2.
php 
碩士班相關公
告 
5 
/~pu1470/admissions/exama1.
php 
大學推甄申請 
6 
/~pu1470/cgi-bin/ap/active/su
bmini.php 
活動排程與記
錄 
7 
/~pu1470/admissions/exama3.
php 
暑假轉學考 
8 
/~pu1470/freshman2011/index
.php 
2011新鮮人 
9 
/~pu1470/freshman2009/cnt.p
hp 
2009新鮮人計
數 
1
0 
/~pu1470/freshman2011/onlin
e.php 
2011新鮮人線
上人數 
 
跟以往研究不同的地方，本研究透過計算
網頁權重值分析結果發現，可以準確的找出網
頁重要性的高利潤項目集。 
 
 
 
 43 
17. E. Poovammal and P. Cigith, “Mining 
web path traversals based on generation 
of FP tree with utility” CCSEIT 2011, 
CCIS 204, pp. 94-100, 2011. 
18. Jyotsna Supriya .P and D.N.V.S.L.S. 
Indira, “A modern two–phased system 
for on-shelf utility mining on web 
transactions” International Journal of 
Engineering Science and 
Technology(IJEST) vol. 3 no.7, pp. 
5796-5801, 2011. 
19. S. S. Tseng, “Data mining,” ISBN 
10-9-574-42236-4, Flag Publishing Co., 
Taipei, 2005. 
20. T. White, “Hadoop: The Deﬁnitive 
Guide.”, ISBN 0-596-52197-9, O’Reilly 
Press, 2009. 
21. X. Xu, J. Jager, and H. P. Kriegel, “A 
fast parallel clustering algorithm for 
large spatial databases,” Databases. 
Data Mining and Knowledge Discovery, 
vol. 3, no. 3, pp. 263–290 ,1999. 
22. H. Yao, H. J. Hamilton, and C. J. Butz, 
“A foundational approach to mining 
itemset utilities from databases,” 
Proceedings of the 3rd SIAM 
International Conference on Data 
Mining, pp. 482-486, 2004. 
23. H. Yao, H. J. Hamilton, and C. J. Butz, 
“Web mining: information and pattern 
discovery on the world wide web,” 
Proceedings of the 3rd SIAM 
International Conference on Data 
Mining, pp. 482-486, 2004. 
24. L. Zhou, Y. Liu, J. Wang, and Y. Shi, 
“Utility-based web path traversal pattern 
mining,”  Proceedings of the 7th IEEE 
International conference on Data 
Mining Workshops, pp. 373–378, 2007. 
25. W. Zhao, H. Ma, and Q. He, “Parallel 
K-means clustering based on 
MapReduce,” Proceedings of the 1st 
International Conference on Cloud 
Computing (CloudCom), pp. 674-679, 
2009. 
 
六、研究團隊的成果 
『影像處理與辨識組』的研究成員包含
周文光博士、林家禎博士與胡育誠博士以
及所指導的碩士班研究生與大學部同學。
截至目前為止，除了影像完整性保障技術
的開發之外，整個研究團隊也在數位內容
安全控管與車牌辨識系統方面有一些研究
成果。 
 
(a) 投稿中的論文 
 
(b) 已經接受/發表的論文 
1. Jieh-Shan Yeh, Szu-Chen Lin, and 
Shueh-Cheng Hu, "OEOP: A Novel 
Algorithm for Periodic Pattern Mining," 
International Journal of Hybrid 
Information Technology (IJHIT), vol. 5, 
no. 2, pp. 187-192, April, 2012. 
2. Che-Lun Hung, Chun-Yuan Lin, 
Hsiao-Hsi Wang, Jieh-Shan Yeh, 
Yu-Chen Hu and Yaw-Ling Lin, 
“Preference Utility Algorithm Using 
GPGPU Architecture,” 12th IEEE/ACIS 
International Conference on Computer 
and Information Science (ICIS 2013), 
Niigata, Japan, June 16-20, 2013. 
3. Che-Lun Hung 、 Hsiao-Hsi Wang 、
Chin-Yuan Chang、Chun-Yuan Lin,” 
Efficient Packet Pattern Matching for 
Gigabit Network Intrusion Detection 
using GPUs,” ICESS-2012. 
4. 洪哲倫, 張金源, 王孝熙, “在 GPGPU
平行架構下封包內容過濾與分析之研
究 ” , International Conference of 
information Management, ICIM2012, 
Kaohsiung, Taiwan, May-19, 2012. 
5. 賴鈺芬 , 葉介山 , 林彩雯 , "基於
MapReduce 技術之時間區段性的網頁
瀏 覽 樣 式 偏 好 度 探 勘 ," Taiwan 
Academic Network Conference 2013 
(TANET2013), 中 興 大 學 , October 
23-25, 2013. 
6. 李侑潾, 葉介山*, "運用資料探勘於大
學生的圖書借閱行為與課業學習表現
之研究 ," Taiwan Academic Network 
 45 
附件三 
 
行政院國家科學委員會補助專題研究計畫期中報告 
 
雲端運算研究平台建置技術與相關資訊科技應用之整合型研究 
計畫編號：NSC-99-2632-E-126-001-MY3 
執行期限：101年 8月 1日至 102年 7月 31日 
影像完整性保障技術之研發 
『影像處理與辨識組』 
共同主持人：周文光   靜宜大學資訊管理學系 
共同主持人：林家禎   靜宜大學資訊管理學系 
共同主持人：胡育誠   靜宜大學資訊管理學系  
 
一、中文摘要 
 
本計畫提出了一個創新的影像完整性保
障技術。所提出的方法可用於保障區塊截
短碼壓縮後影像的內容完整性。區塊截短
碼是一個以區塊為基礎的壓縮技術，一個
影像區塊被壓縮後的壓縮碼為分群的位元
圖與兩個重建階。在所提出的技術中我們
利用亂數產生器搭配選定的亂數種子所產
生的亂數序列來產生認證碼，並將認證碼
藏入到壓縮碼的位元圖中。在所提出的技
術中認證碼的長度可以根據使用者的需求
來決定。從實驗結果我們發現利用所提出
的方法能夠將影像竄改的區域清楚地偵測
出來，而且所提出方法藏入認證碼後的影
像仍具有良好的重建影像品質。 
 
關鍵詞：影像認證、竄改偵測、區塊截短
碼、影像壓縮 
 
Abstract 
 
A novel image integrity protection scheme 
is proposed in this plan. In this scheme, the 
tampered areas in the compressed images of 
block truncation coding can be detected. As 
we know, block truncation coding is a simple 
and low cost image coding scheme for digital 
image. Each image to be compressed by 
block truncation coding is first divided into 
non-overlapped image blocks. Each image 
block is sequentially compressed, and the 
compressed codes consists of the bit map and 
two quantization levels. In the proposed 
scheme, the authenticaiton codes are 
generated by using the radom values induced 
by the specific random seed. The 
authentication codes are then embedded into 
the bit maps of the image blocks. The length 
of the authentication codes can be adaptively 
determined according to the user’s 
requirement. From the experimental results, 
it is shown that the tampered objects can be 
clearly detected. Meanwhile, the embedded 
images still have good visual qualities. 
 
Keywords: Image authentication, tamper 
detection, block truncation, image 
coding 
 
二、緣由與目的 
 
近年來由於網際網路與數位多媒體的蓬
勃發展，使得數位資訊受到廣泛的應用。
由於數位資訊的傳播非常容易且快速，一
般人可以輕易的從網路上取得文字、聲音、
影像、視訊等數位化的資料。在不同的多
 47 
界值，則位元圖(Bit Map)上該像素的位元
值設為 1；小於臨界值的位元值就設為 0，
依照這樣的方式就能將影像區塊的位元圖
建立。分成的兩群分別再利用公式(4)及公
式(5)計算得到重建階 a 與重建階 b。每一
個影像區塊經過壓縮後會以 (a, b, BM) 來
表示，並將此資料傳送給接收端，在這裡
BM代表位元圖。 
在解壓縮的程序中，所有位元圖上位元
值為0的像素將會以重建階a來取代；所有
位元圖上位元值為1的像素將會以重建階b
來取代。 
 
x  = 


k
i
i
x
k 1
1
 
(1) 
x
2
= 


k
i
i
x
k 1
21
 
(2) 
  = 
2 2
x x ( )
 
(3) 
a = x  －  qk
q

 
(4) 
b = x  ＋  q
qk 
 
(5) 
 
在這裡，變數q代表大於等於量化臨界值
的像素總數。 
 
158 152 149 98 
130 88 85 90 
150 148 135 101 
128 146 97 90 
圖1 原始影像區塊 
 
1 1 1 0 
1 0 0 0 
1 1 1 0 
1 1 0 0 
(a) 位元圖 
 
 
145 145 145 91 
145 91 91 91 
145 145 145 91 
145 145 91 91 
(b) MPBTC重建區塊 
圖2 MPBTC影像區塊壓縮結果 
 
在這裡我們介紹利用 MPBTC 來壓縮
一個 4×4 影像區塊的例子，圖 1 為原始影
像區塊。在這個例子中，影像區塊的平均
值與變異數經過計算得到的結果分別為
122 與 27，圖 2(a)列出了這個 4×4 影像區
塊每個像素值對應的位元圖，為了儲存這
個 4×4 的位元圖，我們必須提供 16位元的
空間來儲存，透過上列 MPBTC 的公式我
們可以算得兩個重建階 a 與重建階 b 分別
為 91 與 145。我們利用二進制的表示法來
編碼對應的位元圖，所以這個影像區塊經
過 壓 縮 的 編 碼 資 料 為  (91, 145, 
(1110100011101100)2)。 
在解壓縮的程序中，接收端會依照所
收到的資料 (a, b, BM) 將壓縮過的影像區
塊加以重建。首先會依照位元圖的位元值
找出其對應的重建階，若位元圖中位元值
為 1就以重建階 b來取代；若位元圖中位
元值為 0的像素將會以重建階 a來取代。
每個像素依照這樣的方式就可以將壓縮過
的影像區塊重建回來，而每個區塊都經由
這個方法就能將整張影像重建回來。在圖
2(b)中列出了將圖 1利用MPBTC壓縮後所
解碼回來的影像區塊。 
（二） 動量絕對值區塊截短碼 
接下來介紹的是動量絕對值區塊截短碼
(AMBTC)的做法，基本上區塊截短碼主要
的概念都相同，其量化臨界值亦設定為區
塊平均值 x，只有重建階a 與重建階b是利
用公式(6)及公式(7)計算得到的。公式如
下： 



 x<
i
x
ix
qk
a
1
 
(6) 

  x
i
x
ix
q
b
1
 
(7) 
 49 
    
    
    
    
(b) 分割成2群 
 
    
    
    
    
(c) 分割成4群 
 
個別群將分別藏入一個位元的認證資料
ac。接著計算每一群的(nn)/len個位元資料
經過同位運算後的結果。如果經過同位值
(parity)運算的結果跟ac相等，我們不需針
對該群的位元資料進行任何的修改，因為
認證資料ac已經隱含在該群的位元資料中；
如果該群的(nn)/len個位元資料經過同位
運算後的結果與ac不同，我們必須修改位
元圖中的一個位元值來達成藏入認證碼的
動作。 
 
1 2 4 1 
1 7 4 2 
1 3 6 1 
0 1 3 1 
圖 5位元圖中位置不同群的鄰居個數 
 
由於我們只有這個區塊的兩個重建階與
位元圖的資料，也就是(a, b, BM)，我們紀
錄每個位元圖中的位置不同群鄰居的個數，
找出個別群中可以當成犧牲者的，將位元
圖中那個位置的位元值加以修改，值得一
提的是，如果挑出的犧牲者是屬於大於等
於的那一群，必須保不是唯一的一個，因
為如果是唯一的一個，把這個犧牲者修改
之後整個位元圖就不同了。每個區塊中個
別群的位元資料依照上述的作法逐一處理
藏入認證碼後，就會產生修改後的位元圖
BM，圖5為以圖3(a)位元圖為例，位元圖中
的位置不同群的鄰居個數。 
 
 
圖 6 區塊竄改偵測的流程圖 
 
（三） 竄改偵測程序 
圖 6為我們提出的影像認證程序針對
個別影像區塊處理的流程圖。要驗證個別
區塊是否有被竄改，需要的資訊包含影像
大小、影像區塊大小、個別認證碼的長度
len與亂數種子 seed。首先我們會利用第參
章第(一)節的認證碼產生程序來產生個別
區塊長度為 len的認證位元，因為認證碼是
用亂數產生器產生得到的，所以只要利用
相同的亂數種子 seed 就能產生跟藏入的認
證碼一模一樣的認證資料。 
接著我們必須取出藏入在個別區塊的
認證碼，所以我們要將大小為 nn的位元
圖 BM’切割成 len群，透過計算個別群位元
資料的同位值(parity)取出一個位元的認證
碼 ac。然後收集各群的一位元認證碼來產
Retrieve (a,b,BM)  
of each block 
Divide BM into len 
groups, compute 
the parity ac for 
each group 
Seed 
Generate random 
value rv  
Convert each rv to q  
Is p = q? 
flag = 0 flag
 
= 1 
Yes No 
Generated the 
tampered block 
Collect the parity 
values to generate p 
 51 
影像品質分別為41.208 dB、33.787 dB、
30.347 dB，當區塊大小分別設定為2×2、4×4
與8×8。由於在AMBTC使用的影像區塊8×8
之重建影像品質較差，一般影像處理的應
用均希望能夠將處理後影像的品質維持在
30 dB以上，使得人類視覺系統無法明顯地
察覺處理後影像與原圖的差異；且影像區
塊大小為4×4的最為常見，因此我們在實驗
的部分針對所提出的方法僅進行大小為
4×4的實驗模擬。 
表2列出了所提出的方法藏入不同數量
的機密資料到大小為4×4的位元圖的影像
品質。從表2的數據我們發現隨著藏入機密
資料位元長度len的增加，所提出方法的重
建影像品質大幅度地降低，影像的平均品
質分別為32.645 dB、31.521 dB、30.576 dB、
29.658 dB，當len設定為1、2、3與4。 
 
  
(a) Airplane (b) Girl 
  
(c) Goldhill (d) Lenna 
  
(e) Pepper (f) Toys 
圖 8 大小為 512512 的實驗影像 
表 1 AMBTC 壓縮技術的重建影像品質 
  區塊大小 
Images 
2×2 4×4 8×8 
Airplane 40.747 33.266 30.141 
Girl 41.921 34.763 31.057 
Goldhill 41.316 33.694 30.293 
Lenna 40.657 33.692 30.247 
Pepper 41.456 34.069 30.247 
Toys 41.152 33.235 30.098 
Average 41.208 33.787 30.347 
 
表 2 所提出方法藏入機密資料後的影像品
質，當區塊大小設定為 4×4 
len  
Images 
1 2 3 4 
Airplane 31.942 30.710 29.650 28.686 
Girl 33.667 32.498 31.536 30.590 
Goldhill 32.773 31.809 30.981 30.042 
Lenna 33.001 32.193 31.432 30.737 
Pepper 33.239 32.200 31.402 30.473 
Toys 31.248 29.714 28.454 27.422 
Average 32.645 31.521 30.576 29.658 
 
  
(a) 位元長度len=1 (b) 位元長度len=2 
  
(c) 位元長度len=3 (d) 位元長度len=4 
圖 9 藏入認證碼後的影像，當區塊大小設
定為 4×4 
 
 53 
表3 偵測竄改正確率 
len  
Factors 
len=1 len=2 len=3 len=4 
未被改&未被偵測 15237 15238 15238 15237 
被改&未被偵測 126 65 41 42 
未被改&被偵測 3 2 2 3 
被改&被偵測 1018 1079 1103 1102 
正確率(%) 88.99 94.32 96.42 96.33 
 
四、計畫成果自評 
 
我們提出了一個影像竄改偵測的技術，
所提出的技術是以動量絕對值區塊截短碼
(AMBTC)為基礎進行方法的設計。從實驗
所列的竄改範例我們發現，所提出的方法
能夠將影像被竄改的區域標示出來，而且
當藏入認證位元 len分別設為 2、3、4時竄
改偵測正確率均超過九成。尤其當 len=3、
區塊大小 4×4 時，平均影像品質為 30.576 
dB，竄改偵測正確率高達 96.42%。因此我
們建議將藏入認證位元 len設為 3，不但可
以保有還不錯的影像品質，而且竄改偵測
正確率也很高。 
 
五、參考文獻 
 
1. 蔡孟靜、胡育誠、陳武林，“兼具影像
壓縮與竄改偵測的技術”，第六屆智慧
生活研討會 (ILT 2011)，2011，pp. 
325-338。 
2. Delp, E. J., and Mitcell, O. R. “Image 
Compression Using Block Truncation 
Coding,” IEEE Transactions on 
Communications (27:9), 1979, pp. 
1335-1342. 
3. Dhara, B. C., Chanda, B. “Color Image 
Compression Based on Block 
Truncation Coding Using Pattern Fitting 
Principle,” Pattern Recognition (40:9), 
2007, pp. 2408-2417. 
4. Fawad Ahmed, M.Y. Siyal, Vali Uddin 
Abbas. “A Secure and Robust 
Hash-Based Scheme for Image 
Authentication,” Signal Processing 
(90:5), May 2010, pp. 1456-1470. 
5. Franti, P., Nevalainen, P., and 
Kaudoranta, T. “Compression of Digital 
Image by Block Truncation Coding: A 
Survey,” Computer Journal (37:4), 
1994, pp. 308-332. 
6. Han, S. H., Chu, C. H. “Content-based 
Image Authentication: Current Status, 
Issues, and Challenges,” International 
Journal of Information Security (9:1), 
2010, pp. 19-32. 
7. Hu, Y. C. “Predictive Moment 
Preserving Block Truncation Coding for 
Gray-level Image Compression,” 
Journal of Electronic Imaging (13:4), 
2004, pp. 871-874. 
8. Chuang, J. C., and Hu, Y. C. “An 
Adaptive Image Authentication Scheme 
for Vector Quantization Compressed 
Image,” Journal of Visual 
Communication and Image 
Representation (22:5), July 2011, pp. 
440-449. 
9. Lema, M. D., and Mitchell, O. R. 
“Absolute Moment Block Truncation 
Coding and Its Application to Color 
Image,” IEEE Transactions on 
Communications (32:10), 1984, pp. 
1148-1157. 
10. Lin, P. L., Huang, P. W. Huang., Peng, A. 
W. “A Fragile Watermarking Scheme 
for Image Authentication with 
Localization and Recovery,” 
Proceedings of IEEE Sixth International 
Symposium on Multimedia Software 
Engineering, Dec. 2004, pp. 146-153. 
11. Lin, S. D., Huang, Y. H. “An Integrated 
Watermarking Technique with Tamper 
Detection and Recovery,” International 
Journal of Innovative Computing, 
Information and Control (5:11), 2009, 
pp. 4309-4316. 
12. Lou, D. C. and Liu, J. L. “Fault 
Resilient and Compression Tolerant 
Digital Signature for Image 
Authentication,” IEEE Transactions on 
Consumer Electronics, (46:1), February 
2000, pp. 31-39.  
13. Phan, R. C. W. “Tampering with a 
Watermarking-based Image 
Authentication,” Pattern Recognition 
(41:11), 2008, pp. 2493-3496. 
 1 
出席國際學術會議報告 
計畫編號 NSC99－2632－E－126－001－MY3 
計畫名稱 雲端運算研究平台建置技術與相關資訊科技應用之整合型研究 
出國人員
姓名 
洪哲倫 
服務機構及
職稱 
靜宜大學資訊傳播工程學系 
助理教授 
會議時間 
une 16 -20, 2013, 
Toki  
ICIS 2013 
Messe, Niigata, 
Japan 
會議地點 
日本 新瀉 
會議名稱 
(中文)第十二屆 IEEE/ACIS 電腦與資訊科學國際會議 
(英文) 12th IEEE/ACIS International Conference on Computer and Information 
Science (ICIS 2013) 
發表論文
題目 
1.Preference Utility Algorithm Using GPGPU Architecture 
  
一、參加會議經過 
第十二屆 IEEE/ACIS電腦與資訊科學國際會議 ICIS 2013於 2013年 6月 16日至 20日在Messe, 
Niigata, Japan 舉辦。 
本人於 6 月 17 日下午 High Performance Computing in GPU Clouds Session，發表一篇論文
Preference Utility Algorithm Using GPGPU Architecture。本人出席了會議 session (Computer Science I 
& Software Engineering, Smart Enterprise System and Multi-Agent System Room, Intelligent Services 
and Applications, Technology Innovation and Online Behavior, Data Engineering and Applications) 及
keynote speech，以期瞭解目前世界上各個研究單位在雲端運算、GPU 運算、資訊應用系統與資料
工程等最新的研究方向以及成果。 
二、與會心得 
IEEE/ACIS 電腦與資訊科學國際會議的規模年年增長，雲端運算與軟體應用領域為會議其中
相當重要的議題。這顯示出了雲端運算與高效能運算以及軟體系統整合研究的重要性，目前世界
各地學者在這些方面的研究仍然持續地在創新以及進展中。 
由於電腦硬體不斷的快速發展，以及新興高速網路架構的出現，雲端運算是目前很熱門的研
 1 
出席國際學術會議報告 
                                  日期： 102  年 07 月 23  日 
                                 
一、參加會議經過 
UCAWSN-13 由韓國科技資訊研究所（KiSTi）與韓國資訊科技學會（KITCS）共同主辦，未來科
技研究國際學會（FTRA）協辦，會議論文由 Springer-LNEE 出版。 此會議是自 2012 年開始舉辦，主
要是提供一個平台讓全世界的研究人員，工程師，學術人員及工業界專家來發表在普及內容知覺
（Ubiquitous Context-Awareness）與無線感測網路（Wireless Sensor Network）兩領域的研究成果，促進
研究交流及產學合作機會。今年的會議在韓國濟州的 Shineville Luxury Resort 酒店舉行，自 07/15 至
07/17 共三天。為配合復興航空直飛濟州班機時間，敝人 07/14 出發，07/17 返國。 
二、與會心得 
敝人的論文於 07/16 的 16:20~18:20 時段作口頭報告。我們研究群最近研究手機旅遊觀光 APP，已
發表若干篇相關論文。本論文的主要內容為提出一個適合 APP 的社交分享內容機制，同時也介紹利用
此架構完成的可分享資料的旅遊觀光 APP。此外，使用者的手勢也可用來操作手機間的資料分享。 
本次會議特別邀請 Jason Hung 教授介紹以人為中心(Human-centered)的智慧旅遊系統，有先進的使
用者介面設計及旅遊時方便使用的功能，敝人受益匪淺。敝人特別對下列論文感興趣： 
  A Novel Ranking Technique based on Page Queries 
  Frequent Graph Mining based on Multiple Minimum Support Constraints 
  Design of Automatic Paper Identification System with QR code for Digital Forensics 
  Network based intelligent agent for Ubiquitous environments 
  Approach of Secure Authentication System for Hybrid Cloud Service 
  Smart-Contents Visualization of Vehicle Big Data Using Vehicle Navigation Status Information 
  Efficient Image Matching on Android using SIFT with Improved RANSAC 
  A Study on a human emotion recognition for a better context awareness on smart space 
計畫編號 NSC99－2632－E－126－001－MY3 
計畫名稱 雲端運算研究平台建置技術與相關資訊科技應用之整合型研究 
出國人員
姓名 
蔡英德 
服務機構
及職稱 
靜宜大學資訊傳播工程系教授 
會議時間 
2013 年 7 月 15 日至 
2013 年 7 月 17 日 會議地點 韓國濟州 
會議名稱 
(中文)2013 年第二屆普及內容知覺與無線感測網路國際學術研討會議 
(英文)2013 The 2nd FTRA International Conference on Ubiquitous Context-Awareness 
and Wireless Sensor Network (UCAWSN-13)  
發表論文
題目 
(中文) 社交分享內容行動應用 
(英文) Toward a Mobile Application for Social Sharing Context 
 1 
出席國際學術會議報告 
                                     日期：102年 7月 30日 
計畫編號 NSC99-2632-E-126-001-MY3 
計畫名稱 雲端運算研究平台建置技術與相關資訊科技應用之整合型研究 
出國人員姓名 胡學誠 
服務機構及
職稱 
靜宜大學資訊傳播工程系 
副教授 
會議時間 
2013年 7月
23日至
2013年 7月
26日 
會議地點 
捷克 布拉格 
會議名稱 
(中文) 2013年國際資訊社會發展協會數位學習研討會 
(英文) IADIS International Conference e-Learning 2013 
發表論文題目 
(中文) Restful 風格之學習歷程即服務 
(英文) Learning portfolio as a service – A Restful style 
                              
一、參加會議經過 
7月 21日晚間搭機啟程前往荷蘭阿姆斯特丹，當地時間 7月 22日早上抵達後隨即轉機前往捷克布拉
格，隨後住宿旅館休息。 
7月 23日上午前往會議地點布拉格經濟大學 (University of Economics, Prague)，完成註冊手續並
領取相關文件，隨後聽取專題演講(keynote speeches) 。並利用午餐時間與來自其他國家的學者互動
交流。下午繼續聽取其他作者的報告。 
7月 24日赴會議地點聽取專題演講(keynote speeches)及各國學者的研究論文發表，並於午餐時間與
來自其他國家的學者互動交流。下午繼續聽取其他作者的報告。 
7月 25日上午參加分組會議聽取各國學者的研究論文發表，之後並就此次所發表的研究主題和與會學
者做更進一步的討論。下午繼續參加分組會議聽取各國學者的研究論文發表，並參與討論。晚間準備
隔日與會要做的口頭報告。 
7月 26日當天上午進行論文口頭發表，接著參加分組會議聽取各國學者的研究論文發表，下午參加會
議的閉幕儀式。 
7月 27日全日參觀布拉格市區重要景點。 
7月 28日早上赴布拉格機場搭乘班機經荷蘭阿姆斯特丹轉機返國。 
7月 29日下午 2:40 抵達桃園機場，隨後返回台中。 
 
二、與會心得 
此次所參與的會議以 e-learning為主，重點在於瞭解各國學者對數位學習領域的研究現況，特別是 7
月 23日上午的 Keynote Speech 主題是探討關於目前最熱門的社群網路對學習的正面以及負面的影
響。此外，會議中亦有許多學者報告如何透過最新的資訊科技與網路技術，以促進有效的教與學活動。
國科會補助計畫衍生研發成果推廣資料表
日期:2013/10/29
國科會補助計畫
計畫名稱: 雲端運算研究平台建置技術與相關資訊科技應用之整合型研究
計畫主持人: 林耀鈴
計畫編號: 99-2632-E-126-001-MY3 學門領域: 平行與分散處理
無研發成果推廣資料
submitted to 
Opto-Electronics Review. 
3. Y. C. Hu and W. L. 
Chen, ＇＇A Novel Tamper 
Detection Scheme for BTC 
compressed images＇＇, 
submitted to 
Opto-Electronics Review. 
研究報告/技術報
告 0 0 100%  
研討會論文 0 0 100%  
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 1 1 100% 
解 決 動 態 路邊停車資訊即
時存取裝置之研究(A Study on 
Instant Information 
Retrieving Device for the 
Dynamic Road Parking Quota 
Informatio nProblems), 2010
資訊管理與傳播科技研討會, 
資訊管理與傳播科技研討會論
文集. 
2010 , 主辦: 高苑科技大學
資訊管理系， 資訊學院，資訊
傳播系，資訊科技系，資訊科
技 應 用 研 究
所,ISBN:9789866755330(2010 
年 10 月 27 日) 
 
參與計畫人力 
（外國籍） 
專任助理 3 3 100% 
人次
1.簡暐哲與周文光, ＇＇可用
於雲端的車牌定位系方法(A 
clouding approach for 
license plate 
localization) ＇ ＇ , 
submitted to 2012 數位科技
與創新管理研討會, 2012年 6
月, 華梵大學. 
2.徐忠禎與周文光, ＇＇車牌
辨 識 系 統 之 雲 端 應 用 (A 
Clouding Approach for 
Vehicle License Plate 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：■已獲得 □申請中 □無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100字為限） 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500字為限） 
一、生物資訊分析組 
本小組已透過雲端運算平台發展出四個生物雲端運算服務，分別是 tag SNP 選擇、蛋白質
結構排列、蛋白質配體比對與開放閱讀框架演化分析。這些服務的開發皆是基於 Hadoop
的 Map/Reduce 框架，且可以並行的處理數據的相關運算。目前這些作品皆已發表在
SCI-indexed 期刊、國際期刊與國際研討會，而且這些作品都可以提供線上服務。基於這
些發展經驗，我們希望透過本計畫的進行過程與發表成果可以讓其他研究可以快速的進入
雲端運算領域，並且可以吸引更多學者投入雲端生物資訊服務的領域。本小組研究成果與
原修正計畫相符並達成預期目標。 
二、資料探勘組 
本資料探勘小組利用雲端計算與 GPU 平行處理等技術提升與改善傳統資料探勘之效能。而
本年度本小組採用雲端 Hadoop 平台環境之 MapReduce 技術，針對網頁瀏覽樣式利潤偏好
度探勘問題提出良好的執行效能。研究成果除了發表多篇期刊與國內外會議論文外，本研
究成果並能推廣雲端計算與 GPU 多核心計算的運用於資料探勘領域，其中又以網頁資料探
勘的大量資料處理最需要高效能的運算。 
本小組之研究成果可以提供 Data Mining, Cloud Computing, GPU Computing 等學術研究
領域與相關應用之參考。 
三、影像處理與辨識組 
我們提出了一個影像竄改偵測的技術，所提出的技術是以動量絕對值區塊截短碼(AMBTC)
