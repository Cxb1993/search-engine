  I 
中文摘要 
本研究案主要著力於學生學習模型、電腦輔助試題翻譯、電腦輔助語文教學和中文訴訟文
書分類四個研究主題。在學生學習模型方面，我們以貝氏網路來表示學生學習模型，並且
提出了一個方法來學習學生的學習模型的方法。在電腦輔助試題翻譯這一項工作中，我們
建構了一個實際的系統，用以輔助專家翻譯 TIMSS 試題。在電腦輔助語文教學這一項工作
中，我們建構了一個真實的系統，可以輔助國語科教師編輯試題。中文訴訟文書分類並不
是這一次研究案的主角，是我們結束前一國科會研究案的工作。本次研究計畫執行期間，
合計發表 17 篇論文（兩篇國際期刊論文、三篇國際學術研討會論文國內學術會議方面，則
有六篇 ROCLING 論文、四篇 TAAI 論文、一篇 NCS 和一篇 TANET 論文），總頁數達到 133
頁；其中包含一篇人工智慧與電腦輔助教學跨領域研究的優質期刊論文(IJAIED)和一篇計
算語言學優質研討會(ACL)的研討會論文。 
關鍵詞：貝氏網路、學生學習歷程、建模技術、資訊檢索、電腦輔助語文學習、機器翻譯 
Abstract 
In this report, we summarize the results of this research project on several fronts. 
For student modeling, we proposed a simulation-based approach to learn the struc-
tures of Bayesian networks that contain unobservable variables. We have built three 
functioning systems for practical applications of natural language processing tech-
niques. We built an environment for computer-assisted translation of TIMSS test 
items, an environment for assisting teachers to compose test items for elementary 
Chinese, and an environment for searching Chinese indictment documents.   
Keywords: Bayesian networks, structure learning, learning processes of composite 
concepts, information retrieval, computer assisted language learning, machine 
translation 
  1 
報告內容 
前言 
本研究案雖然不是一個整合型計畫，但是一開始即訂定多項目標，因此實際上很難以
一份報告來總結所有子項計畫的研究成果。因此，在這一份報告中，我們為個別子項計畫
撰寫簡要的文字資料，然後請有興趣深入研究的讀者繼續研讀已經發表的期刊論文或者學
術會議論文。 
這一個研究案從事兩大類但相互關連的研究工作：一個是認知歷程的建模技術，另一
個則是以自然語言處理為基礎的實際軟體系統的建置。認知歷程的建模技術方面，我們以
學生學習綜合觀念的問題作為研究的主題。實際軟體系統方面，我們建立了三個不同的系
統：中文訴訟文書檢索系統、電腦輔助 TIMSS 試題翻譯環境和電腦輔助國語科試題出題環
境。 
在實際的環境中，如果我們想要利用人工智慧技術來讓軟體系統提供使用者最好的服
務，瞭解使用者真實的需求是必要的基礎。實務上我們很難經常性地詢問使用者的需求和
回饋，因此從間接的資訊來推測使用者的興趣或者意向是重要的基礎技術。所以，以上兩
大類的研究工作，以長遠的角度來說是有密切關連的。現階段的工作是一個逐漸打底的工
作；我們期待繼續朝綜合人工智慧技術、機器學習技術和自然語言處理技術來建構有用的
資訊檢索環境和電腦輔助語文學習的環境。 
在研究進度方面，計畫主持人全力從事認知歷程的建模技術，因此這一部分的成果比
較能夠掌握。應用自然語言處理技術來建立實際系統的部分，則全部是以碩士班研究生執
行，雖然能夠維持一些進度，但是計畫推展的速度並不能令人完全滿意。 
在研究成果方面，我們發表了 17 篇學術論文，總頁數達到 133 頁。在研究成果小節中
我們將分析所達成的成果。我們把各項主要子項工作比較具有代表性的論文附在本份報告
的附錄中。就如前面所說明，這一份報告的本身其實只能是我們所進行的所有工作的大摘
要而已，所有工作的真正成果已經反映在所發表的論文之中，因此雖然我們必須把論文放
在附錄，但是其實論文本身才應該是這一個研究案的成果的真正主角。 
附錄包含了四篇論文：IJAIED 的期刊論文一篇（建模技術相關論文，這是一篇出版商
有版權的文章，不宜在網路上公開），ACL 國際學術研討會論文一篇（電腦輔助國語科試
題出題輔助系統）， ROCLING 國內學術研討會論文一篇（電腦輔助 TIMSS 試題翻譯環境）
和 TAAI 國內學術研討會論文一篇（中文訴訟文書檢索系統）。 
研究目的 
我們分四個段落簡述四個不同的子項目的研究目的。詳細資料請參閱相關論文。 
在建立使用者模型方面，我們希望能夠找到一個好的辦法，讓我們可以在不能夠直接
觀測模型中所有相關變數的狀態的情形之下，仍然能夠以貝氏網路來表示所有相關變數的
直接和間接機率關係。在所進行的研究中，學生的答題的反應（目前僅以「對」和「錯」
表示）是可以直接觀測的變數，而我們所建立的模型包含了學生對於個別觀念的能力。能
力與答題的對錯雖然有密切關係，但是關係卻不是邏輯式的，因為有人會因為運氣好答對
  3 
來導引建議檢索檔案。跟我們以詞組為基礎來做檔案分群的理念相似，以同現的分數高低
來建議檢索資料，也可能因為比較能夠捕捉到檢索者的意圖而提高檢索效率。 
電腦輔助 TIMSS 試題翻譯環境的建置是一個典型的機器翻譯(machine translation)的研
究。對於機器翻譯這個研究議題來說，兩年的計畫時程只能建立基礎而已。我們應用語言
模型(language models)、雙語對譯資料(parallel corpora)、範例式學習技術(example-based 
learning)三個主要技術，結合現在受到學界普遍使用的 Moses 和 Lucene 開放式軟體工具建
立了一個翻譯輔助環境。本研究案，受到國立台灣師範大學科學教育中心的張主任的協助，
因此得以獲得相關的 TIMSS 中英文試題。 
電腦輔助國語科試題出題環境提供五大類型試題的編輯：漢語語音辨識、改錯字試題、
中文克漏詞、中文量詞和句子重組。因此我們須要利用到語音、漢字構形、漢語詞彙和漢
語語法等數個不同層次的語文資訊。我們利用自然語言處理技術，依照試題編輯者（通常
是教師）所要求的試題條件，從所蒐集的語文資料找到相關的語料，並且依照所編輯的試
題的特性提出有用的建言。試題編輯者可以利用我們的介面建立基本的題庫，進而建立試
卷資料庫，爾後學生也可以透過網路作答。學生作答的結果可以立即得到回饋，教師也可
以分析所任課的學生群的測驗結果，檢討其教學策略。 
研究成果與討論 
我們分別簡述四個不同的子項目的研究成果。詳細資料（特別是個別研究的學術意義）
請參閱相關論文中比較詳細的討論。 
在建立使用者模型方面，我們在 International Journal of Artificial Intelligence in 
Education (IJAIED) 發表了一篇 49 頁的長篇論文[1]，在這之前，我們在全國計算機會議發
表了一篇中文論文[12]為國內學者介紹這一個研究的縮影 。IJAIED 是一個優質的期刊，
是 International AIED Society 的正式期刊，由 University of Edinburgh 的教授擔任主編，一
年一般只收錄十餘篇論文，其中部分還是兩年一次的 AIED 學術研討會的最佳論文才能獲
得推薦。因此研究成果能夠在 IJAIED 刊登，應該算是相當不容易的一項成就。 
在中文訴訟文書檢索系統方面，我們在 2007 年和 2008 年的人工智慧學會年會(TAAI)
發表了三篇論文[6, 13, 14]。 
電腦輔助 TIMSS 試題翻譯環境的建置方面，我們在 Journal of Advanced Computational 
Intelligence and Intelligent Informatics (JACIII)發表了一篇簡短的期刊論文[2]，在 RANLP 國
際學術研討會中發表了一篇論文[5], 在 2007 年和 2008 年的計算語言學研討會(ROCLING)
上各發表了一篇論文[10, 17]。 
因為所牽涉的問題，不僅僅是資訊科學的技術，同時還有關於教學的可能成效，因此
電腦輔助國語科試題出題環境的研究成果，部分是發表在比較接近教育領域的會議中，去
接受第一線的使用者的挑戰。這一方面的部分成果發表於 JACIII 期刊論文[2]，2008 年的
ACL 國際學術研討會[3]，2008 年的 CAERDA 學術研討會[4]，2007 年的 RANLP 國際學術
研討會[5]，兩篇 2008 年的計算語言學研討會(ROCLING)[8, 9]和一篇 2007 年的網際網路研
討會(TANET)[15]。ACL 是國際間計算語言學界最著名的國際學術研討會之一，研究成果
能夠獲得 ACL 年會收錄，是一項不錯的成就。 
  5 
10. 張智傑及劉昭麟。以範例為基礎之英漢 TIMSS 試題輔助翻譯，第二十屆自然語言與語
音處理研討會論文集 (ROCLING XX)，308‒322。2008 年。 
11. 陳禹勳及劉昭麟。電腦輔助推薦學術會議論文評審委員之初探，第二十屆自然語言與
語音處理研討會論文集 (ROCLING XX)，323‒337。2008 年。 
12. 劉昭麟。利用試題反應建立學生學習歷程模型的一些經驗，中華民國九十六年全國計
算機會議論文集 (NCS’07)，第一冊(下)，359‒366。2007 年。 
13. 何君豪、鄭人豪及劉昭麟。階層式分群法在民事裁判要旨分群上之應用，第十二屆人
工智慧與應用研討會論文集 (TAAI’07)，794‒800。2007 年。 
14. 鄭人豪及劉昭麟。中文詞彙集的來源與權重對中文裁判書分類成效的影響，第十二屆
人工智慧與應用研討會論文集 (TAAI’07)，801‒808。2007 年。 
15. 林仁祥及劉昭麟。國小國語科測驗卷出題輔助系統，2007 臺灣網際網路研討會論文集 
(TANET’07)，論文光碟。2007 年。 
16. 陳俊達、王台平及劉昭麟。以文件分類技術預測股價趨勢，第十九屆自然語言與語音
處理研討會論文集 (ROCLING XIX)，347‒361。2007 年。 
17. 呂明欣、高照明、劉昭麟及張俊彥。針對數學與科學教育領域之電腦輔助英中試題翻
譯系統，第十九屆自然語言與語音處理研討會論文集 (ROCLING XIX)，407‒421。2007
年。 
  7 
附錄 
本附錄依序包含下列四篇論文。 
1. Chao-Lin Liu. A simulation-based experience in learning structures of Bayesian networks to 
represent how students learn composite concepts, International Journal of Artificial Intelli-
gence in Education, 18(3), 237‒285. IOS Press, The Netherlands, September 2008. 
2. Chao-Lin Liu and Jen-Hsiang Lin(林仁祥). Using structural information for identifying 
similar Chinese characters, Proceedings of the Forty Sixth Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language Technologies (ACL’08), short paper, 
93‒96. Columbus, Ohio, USA, 2008. 
3. 張智傑及劉昭麟。以範例為基礎之英漢 TIMSS 試題輔助翻譯，第二十屆自然語言與語
音處理研討會論文集 (ROCLING XX)，308‒322。2008 年。 
4. 藍家良、賴敏華、田侃文及劉昭麟。訴訟文書檢索系統，第十三屆人工智慧與應用研
討會論文集 (TAAI’08)，305‒312。2008 年。 
 
The Target Question and Assumptions 
Our target problem is to learn how students learn composite concepts by observing students’ fuzzy (Bi-
renbaum et al., 1994) item-response patterns that have only an indirect relationship with their compe-
tence patterns. Students’ item responses are fuzzy because they do not necessarily indicate students’ 
actual competence. 
A composite concept is a concept that requires the knowledge of two or more basic concepts. For 
instance, Mislevy and Gitomer (1996) used “Mechanical Knowledge”, “Hydraulics Knowledge”, 
“Canopy Knowledge”, and “Serial Elimination” as the prerequisites for “Canopy Scenario Requisites--
No Split Possible”, and Vomlel (2004) included “Subtraction”, “Cancelling Out”, and “Multiplication” 
as the basic capabilities that are necessary for finding the solution for 8
1
6
5
4
3 )( −× . 
Although it is convenient to use the nodes for all the prerequisites as the parent nodes of the node 
for the composite concept, we anticipate that constructing a more precise model that reflects the proc-
ess of the learning of the composite concept may improve the performance of CATs and other com-
puter-assisted learning tasks. This anticipation is related to the study of cognitive diagnostic assess-
ment (Nichols et al., 1995; Leighton & Gierl, 2007). Indeed, Carmona et al. (2005) report that intro-
ducing prerequisite relationships into their multi-layered Bayesian student models enables their CAT 
system to diagnose students with a fewer number of test items. Furthermore, if teachers know how 
students normally learn a composite concept, the teachers will have more information as to how to 
provide appropriate and specific help for students who fail to demonstrate competency in the concept 
(Naveh-Benjamin et al., 1995). For instance, if students normally learn dABC by integrating cA and 
dBC and if a student shows a lack of competence in dABC, a teacher may have to consider the stu-
dent’s ability in learning dBC from cB and cC in addition to providing the student with information 
about the three basic concepts. Using Vomlel’s arithmetic problem as an example, we are wondering 
how computational techniques can help us compare the merit of the (partial) Bayesian networks shown 
in Figure 2. 
Therefore, we consider the problem of how the use of computational techniques can help us iden-
tify students’ learning patterns. To facilitate the discussion about the ways in which a composite con-
cept may be learned, we define the notation that we will use to represent how students learn a compos-
ite concept. Let τ denote the composite concept which we would like to know how students learn. As-
sume that there are α basic concepts included in τ. Based on our non-overlapping assumption that 
we present below, τ can have at most α parent concepts. If some of τ’s parent concepts are composite, 
τ will have less than α parent concepts. We denote a way of learning τ by a computational form of τ . 
A computational form of τ may have one or more parts, the parts are connected by underscores, and 
each part of the computational form represents a parent concept of τ. 
Definition 1. Assume that learning κ requires the knowledge of α basic concepts. Let {π1,π2,…,πμ} 
(3/4 * 5/6) - 1/8
MultiplicationCancelling outSubtraction
(3/4 * 5/6) - 1/8
MultiplicationCancelling out
Subtraction C & M
 
Fig. 2. Which model is better? 
denote the set of parent concepts of κ, where μ≤α. The computational form of the way to learn κ is 
π1_π2_…_πμ. Each computational form of a composite concept represents a learning pattern for stu-
dents to learn the composite concept. 
Definition 2. (The non-overlapping assumption) We assume that any two parent concepts defined in 
Definition 1 do not have common basic concepts. 
The non-overlapping assumption presumes that students must learn composite concepts from 
non-overlapping components. Specifically, the parent concepts of the composite concepts do not in-
clude common basic concepts. Hence, there are only four possible ways to learn dABC: (1) integrating 
cA, cB, and cC directly (denoted by A_B_C); (2) integrating dAB and cC (denoted by AB_C); (3) inte-
grating dBC and cA (denoted by BC_A); and (4) integrating dAC and cB (denoted by AC_B). The 
structure shown in Figure 1 is A_B_C. Figure 3 shows three other ways to learn dABC, and, from the 
left to right, they are AB_C, BC_A, and AC_B (Nodes for test items are not included for readability of 
the networks in Figure 3 and other Bayesian networks that we will discuss later in this paper.) 
The non-overlapping assumption simplifies the space of the possible answers. Without excluding 
the possibility of overlapping ingredient concepts, we would have to consider AB_BC, AB_AC, and 
BC_AC if we minimise the number of overlapping basic concepts. We would also have to consider 
cases like AB_BC_A and even AB_BC_AC_A if we do not minimise the number of overlapping basic 
concepts. It is certainly possible that a student can learn dABC with these alternative methods. How-
ever, we leave these more challenging possibilities for future studies. 
As we present more details about the designs of our experiments, it will become clear that the 
methods we propose do not require the application of the non-overlapping assumption. However, mak-
ing use of the assumption simplifies the space of the possible solutions, while the proposed methods 
can still be applied without the assumptions. 
We do not assume further limitations on the ways that students might integrate the candidate par-
ent concepts. For instance, under some circumstances, one might believe that a student cannot inte-
grate cA and cB unless cA is already a part of another relevant concept, say cAC. In this case, one 
might learn dABC from dAC and cB but not from dAB and cC. We did not consider such special con-
straints in our study. 
Definition 3. (The common assumption) All students learn a composite concept with the same learn-
ing pattern. 
The common assumption presumes that all students use the same strategy to learn a composite 
concept. The purpose of using this assumption is just to simplify the presentation of our discussion. It 
is understood that there is no clear support for this rather controversial assumption. However, the cur-
rent goal of our methods is to select exactly one best candidate from the many possible ways of learn-
ing the composite concept. It will become clear, as we present our methods in the rest of this paper, 
that we can easily modify our methods to select the top k candidate solutions for human experts to 
group
cA cB cC
dAB dBC dAC dABC
group
cA cB cC
dAB dBC dAC dABC
group
cA cB cC
dAB dBC dAC dABC
 
Fig. 3. Three other ways to learn dABC (from left to right): AB_C, BC_A, AC_B 
make the final judgment about how students may learn the composite concept. We simply have to pre-
sent the k highest-scored candidate structures to the experts to relax the common assumption. There-
fore, we hope this assumption is not as provocative as it might appear. 
In summary, we would like to find ways to tell which of the candidate networks, e.g., those in 
Figure 3, was used to generate the simulated students records. 
Generating Student Records 
The contents of the conditional probability tables (CPTs) of the Bayesian networks were generated 
based on a Q-matrix (e.g., those contained in Table 1), a given network structure (e.g., those shown in 
Figures 1 and 3), and simulation parameters according to the methods described in (Liu, 2005). When 
generating the CPTs, we considered not only the chances of slip and guess but also the chances of stu-
dents’ abnormal behaviours that deviated from the typical competence patterns of the subgroups to 
which they belonged. To capture the uncertainty of this latter type, we inherited the concepts of group 
guess and group slip discussed in (Liu, 2005), but set both group guess and group slip to groupInflu-
ence. More precisely, when 1, =kjq , we assigned a high probability for the jth subgroup being compe-
tent in the kth concept (if kC  is basic), and this probability is sampled uniformly from [1-
groupInfluence, 1], where groupInfluence is a simulation parameter selected for individual experi-
ments. Hence, even if 1, =kjq , )|competentPr( jk ggroupC ==  might not be equal to 1, and students of 
the jth subgroup might not be competent in the kth concept. Similarly, when 0, =kjq , we assigned a low 
probability for the jth subgroup being competent in the kth concept (if kC  is basic), and this probability 
is sampled uniformly from [0, groupInfluence]. Hence, even if 0, =kjq , students of the jth subgroup 
might be competent in the kth concept. 
The conditional probabilities of correctly responding to test items given different competence 
levels were specified with a standard procedure that has been commonly employed in the literature, 
e.g., (Martin & VanLehn, 1995; Mayo & Mitrovic, 2001; Conati et al., 2002; Millán & Pérez-de-la-
Cruz, 2002). Instead of using two simulation parameters for slip and guess, we set these two parame-
ters to the same value and called it fuzziness. Hence the probabilities 
)competent|correctPr( , == jkj CI and )tincompeten|correctPr( , == jkj CI were, respectively, sampled 
uniformly from [1- fuzziness, 1] and [0, fuzziness]. Notice, again, that the value of fuzziness functioned 
as the bounds of the actual values of guess and slip but not their values. 
Similar to what has been reported in the literature, e.g., (DiBello et al., 1995; Mayo & Mitrovic, 
2001; Conati et al., 2002), we employed the concept of noisy-and (Pearl, 1988) for setting the condi-
tional probabilities for the composite concepts which have multiple parent nodes. Noisy-and nodes 
reflect a probabilistic version of the “AND” relationship in traditional logics. The degree of noise is 
controlled by the simulation parameter groupInfluence. Readers are referred to (Liu, 2005) for more 
details. 
We controlled the percentages of the subgroups in the entire simulated student population by ma-
nipulating the prior distribution over the node group. We could use any prior distribution for group in 
the simulator. In the reported experiments, the node group took the uniform distribution as its prior 
distribution. Hence, if we were simulating a population of 10000 students that consisted of eight sub-
groups, each subgroup might have approximately 1250 students. 
In summary, we created Bayesian networks with the procedure reported in (Liu, 2005), and we 
controlled the degree of uncertainty by two parameters, i.e., groupInfluence and fuzziness. Given the 
network structure and the CPTs, we had a functioning Bayesian network, and could apply this network 
to simulate item responses of different types of students. We employed a uniform random number 
generator in simulating students’ behaviours with a typical Monte Carlo simulation procedure. For 
instance, we randomly sampled a number, δ, from a uniform distribution [0, 1]. If the conditional 
probability of correctly responding to iA2 was 0.3 for a particular subgroup of students and if δ >0.3, 
we would assume that this student responded to iA2 incorrectly. Students of the same subgroup may 
have different item responses to the same item because we independently drew a random number for 
each test item and each simulated student. 
Example 2. Table 2 shows the data for certain students that we generated with the Bayesian network 
shown in Figure 1 and the left Q-matrix shown in Table 1, when setting groupInfluence and fuzziness 
to 0.05 and 0.10, respectively. Each row in Table 2 contains a record for a simulated student, e.g., the 
first simulated student correctly responds to all of the test items while the second simulated student 
fails iA3 and iABC3. Although we always simulate item responses for students of all of the subgroups, 
we cannot show all of the data here. Notice that, due to the degree of uncertainty which was simulated 
and which was controlled by groupInfluence and fuzziness, a student who should be competent in a 
concept might not respond correctly to a test item for that concept. For instance, the second student of 
g1 fails to respond correctly to iA3, although all the members of g1 are supposed to be competent in cA 
as indicated by the Q-matrix.■ 
Computational Complexity 
Assume that there are β basic concepts in C) . The computational complexity of our target problem 
comes from both the number of different ways that students can learn the composite concept which, 
directly or indirectly, integrates all β basic concepts and the number of different Q-matrices. 
Table 2. A sample of simulated students’ item responses for the Bayesian network shown in Figure 
1 and the left Q-matrix in Table 1 (1 and 0 denoting correct and incorrect, respectively) 
Test Items group 
iA1 iA2 iA3 iB1 iB2 … iAB1 iAB2 iAB3 iBC1 … iABC1 iABC2 iABC3 
g1 1 1 1 1 1 … 1 1 1 1 … 1 1 1 
g1 1 1 0 1 1 … 1 1 1 1 … 1 1 0 
g1 1 1 1 1 1 … 1 0 1 1 … 1 1 1 
… … 
g2 1 1 0 1 1 … 0 1 0 1 … 1 1 0 
g2 1 1 1 1 0 … 1 0 0 1 … 1 1 1 
… … 
g5 0 0 0 1 1 … 0 0 0 1 … 0 0 1 
g5 0 1 0 1 1 … 1 0 0 1 … 0 1 0 
… … 
g8 1 1 1 0 1 … 0 0 0 0 … 1 1 1 
g8 1 1 1 1 1 … 0 1 0 0 … 0 1 1 
… … 
Given the non-overlapping assumption and the common assumption, the number of different 
ways that students can learn the composite concept which integrates all β basic concepts is related to 
the Stirling number of the second kind (Knuth, 1973). Formula (1) shows the number of ways to parti-
tion t different objects in exactly i nonempty sets. 
∑−
=
−⎟⎟⎠
⎞
⎜⎜⎝
⎛−= 1
0
)()1(
!
1),(
i
j
tj ji
j
i
i
itS     (1) 
Formula (2) shows the number of ways to partition β different objects in more than two nonempty 
sets, and Table 3 illustrates how the number of possible learning patterns grows with β. )(βS)  is the 
number of possible ways to learn a composite concept from β basic concepts. 
∑ ∑∑
=
−
== ⎟
⎟
⎠
⎞
⎜⎜⎝
⎛ −⎟⎟⎠
⎞
⎜⎜⎝
⎛−== β ββ ββ
2
1
02
)()1(
!
1),()(
i
i
j
j
i
ji
j
i
i
iSS
)
        (2) 
The choice of the Q-matrix influences the prior distribution for the students being simulated, and 
is an important issue for studies that employ simulated students (VanLehn et al., 1998). There can be a 
myriad number of different Q-matrices, cf. (DiBello et al., 1995), and clearly the chosen Q-matrix af-
fects the difficulty of identifying the learning pattern of interest. When there are β basic concepts in C) , 
there can be as many as n=2β-1 different concepts in C
)
, and there can be as many as 2n different com-
petence patterns, as we have explained in Example 1. In principle, a student can belong to any of these 
2n patterns. Because each of these 2n patterns can be either included or not included in the Q-matrix, 
there are 
n22  different Q-matrices. Note that such quantities occur only in the worst-case scenario as 
not all of these 2n patterns and not all of the 2β-1 concepts are practical.  
We can choose to include all possible competence patterns in a Q-matrix, or, alternatively, we can 
make the Q-matrix include only those patterns that appear to be helpful for identifying the learning 
patterns. In the former case, there is only one possible Q-matrix, but the size of this Q-matrix will be 
quite large. For β =3 and β =4, the Q-matrices will include, respectively, 128 and 32768 competence 
patterns. In the latter case, the selection of Q-matrices is equivalent to choosing a certain population of 
students to participate in our studies in order that we can achieve our goals. For instance, all of the 
values in the dABC columns of the Q-matrices in Table 1 are set to 1. As explained in Generating Stu-
dent Records, such a setting makes the simulated students very likely to be able to integrate the parent 
concepts of dABC to learn dABC, and, if we want to learn how students learn dABC, it should be rea-
sonable to recruit students who appear to be competent in dABC in our studies. Hence the choice for 
the settings of the dABC columns of the Q-matrices in Table 1 is not groundless. We will discuss the 
influence of Q-matrices in more detail in Influences of the Q-Matrices and More Realistic Evalua-
tions when we present the experimental results. 
Example 3. Based on this discussion, we choose to report results for interesting Q-matrices in which 
there are only three or four basic concepts. For the C
)
 used in Table 1, β=3 and n=7. There are four 
different ways to learn the composite concept dABC, 128(=27) different competence patterns, and 2128 
possible Q-matrices, so there are 2130 (=4×2128) problem instances. For the problem in which we con-
Table 3. Results of computing Formula (2) grow exponentially with β 
β 3 4 5 6 7 8 9 10 
)(βS) 4 14 51 202 876 4139 21146 115974 
sider four basic concepts (i.e., β=4), there will be 14 different ways to learn dABCD based on Formula 
(2). A complete enumeration of the subsets of {A, B, C, D}, without considering the empty subset, in-
cludes 15 configurations, which makes n =15 in C
)
 (cf. Table 7). Hence, for this case, we have 
32768(=215) competence patterns and 14×232768 different problem instances. ■ 
Impacts of Latent Variables 
In addition to the large search space that was discussed in Computational Complexity, another major 
difficulty in learning the learning patterns comes from the fact we cannot directly observe the levels of 
competence of the students. What we have at hand are students’ responses to test items that are indi-
rectly and probabilistically related to the actual competence levels. The literature, e.g., (Heckerman, 
1999), has addressed common issues in learning network structures with hidden variables, and some, 
e.g., (Desmarais et al., 2006), have discussed issues that are specific to learning network structures for 
educational applications. In this subsection, we look into problems that are directly related to our tar-
get problems. 
If we could directly observe the states of competence levels of concepts, we would be able to ap-
ply theoretical inference tools. Let CI(X, Y, Z) denote the situation that variables in X and Z become 
independent when we obtain information about the variables in Y. For simplicity, we say X and Z are 
conditionally independent given Y when CI(X, Y, Z) holds. (Note that X, Y, and Z may contain one or 
more variables.) Take the case for learning dABC as an example. If we can directly observe the states 
of group, cA, cB, cC, dAC, and dABC, we will find that CI(dAC, {group, cA, cB, cC}, dABC) if the 
actual structure is the network shown in Figure 1. We can tell whether the conditional independence 
holds based on the criteria for judging whether d-separation (Pearl, 1988) holds in Bayesian networks, 
and the data generated with this network are expected to reflect the independent relationship. Hence, 
we would be able to tell the learning pattern for dABC by checking whether CI(dAC, {group, cA, cB, 
cC}, dABC) and other relevant conditional independence relationships hold. 
In reality, we cannot directly observe the states of group, cA, cB, cC, dAC, and dABC, and can 
observe only the states of the test items for cA, cB, cC, dAC, and dABC, i.e., the states of iAj, iBj, iCj, 
iACj, and iABCj, where j=1, 2, 3. This information is helpful but does not allow us to determine the 
answer to the problem for sure, because CI({iACj|j=1,2,3},{iAj, iBj, iCj|j=1,2,3}, {iABCj|j=1,2,3}) 
fails to hold for any structure shown in Figures 1 and 3 now. In Figure 1, even if we further assume the 
availability of information about group either because of students’ records or because of the help of 
student assessment software, nodes iACj and nodes iABCj, j=1,2,3, remain probabilistically dependent. 
In this network, only direct information about the competence levels, i.e., cA and cC, or either of dAC 
and dABC, can d-separate nodes iACj and nodes iABCj, j=1,2,3. As a consequence, if we can observe 
only the states of the nodes for test items, we cannot tell the difference among different ways of learn-
ing dABC based on the concept of d-separation. 
The research into learning Bayesian networks from data has made significant progress in recent 
years (Heckerman, 1999; Neapolitan, 2004). Yet, the problem of learning Bayesian networks with 
hidden variables is relatively more difficult. Based on our limited knowledge, existing algorithms can 
tackle problem instances that consider a limited number of hidden variables but such algorithms do not 
explicitly attempt to learn the relationships among a set of hidden variables, which is the focus of this 
paper. In addition to the consideration of hidden variables, a further major technical challenge in learn-
ing Bayesian networks is missing values in some of the training data. We disregard this consideration 
at this moment, though it is possible for a real student not to answer all the questions in a test. We as-
Computationally, using Search4Pattern is more efficient than directly computing the scores 
for all candidate structures, which is illustrated by the data in Table 5. We duplicate the first row and 
the second row of Table 5 from Table 3. Except for the trivial case when β is 2 at step 1, we must run 
at least the iteration for κ=2. It is easy to verify that when κ=2, there will be 2β -1-1 elements in Ωκ. 
This is the number of different ways to split β different objects into two nonempty sets, and is equal to 
S(β,2) as defined in Formula (1). Ωκ can have at most β elements for the following iterations in which 
κ=3, κ=4, …, κ=β -1. There are only β basic concepts in τ, so we can split any Ωj, where j=2, 3,…, β-
2, in at most β different ways. Hence, during these intermediate search steps, Search4Pattern will 
compute at most β×(β-3) scores. In the worst case, Search4Pattern must run the iteration for κ=β, 
and will stop when κ>β at step 7. In this very last iteration, Ωκ can have only one element, which 
represents the situation when students learn the target composite concept directly from β basic con-
cepts. 
Hence, in the worst case, Search4Pattern computes at most (2β -1-1+ β×(β-3)+1) scores. The 
third row of Table 5 shows this quantity for different values of β. Note that the numbers are pessimis-
tic estimates of the number of times that Search4Pattern has to compute scores. For instance, when 
β is 4, Search4Pattern computes at most 11 scores rather 12 scores as discussed in Example 5. The 
difference between the actual times of computing the scores and their pessimistic estimates comes 
from two sources. First, we do not necessarily reach the case when κ>β for all different ways of learn-
ing the target composite concept. In addition, Ωκ must have fewer than β successors in Ωκ+1 when κ is 
between 3 and β-1. The fourth row of Table 5 shows a lower bound of the avoided computation in 
percentage. To obtain the percentage in each column, we subtract the quantity in the third row from 
the quantity in the second row, and divide the difference by the quantity in the second row. 
Model-Based Methods: ANNs and SVMs 
In addition to using the heuristic method and the search-based method, we build classifiers by employ-
ing the data about mutual information measures to train artificial neural networks (ANNs) (Bishop, 
1995) and support vector machines (SVMs) (Cortes & Vapnik, 1995) for better performance. We ex-
periment using two specific classes of ANNs: probabilistic neural networks (PNNs) (Wasserman, 
1993), which are a variant of radial basis networks, and feed forward back-propagation networks 
(BPNs) that are implemented in MATLAB (http://www.mathworks.com). Support vector machines 
are a relatively new tool that can be applied to the task of classifications, and we try the C-SVC SVMs 
that are implemented in the LIBSVM package (Chang & Lin, 2001). We can train ANNs and SVMs 
with training patterns that are associated with known class labels, and the trained ANNs and SVMs 
can be used to classify the classes of test patterns. 
We must determine what features the ANNs and SVMs will use to do classification. In addition 
to the estimated mutual information that we have to compute to apply Heuristic 1, we introduce more 
features that are computed from these original features. Based on the evidence that we gathered in ex-
Table 5. Percentage of avoided computation by using Search4Pattern grows with β 
number of basic concepts in τ (i.e., β) 3 4 5 6 7 8 9 10 
total number of candidate structures 4 14 51 202 876 4139 21146 115974 
an upper bound of checked structures 4 12 26 50 92 168 310 582 
a lower bound of saving in percentage 0.00 14.3 49.0 75.2 89.5 95.9 98.5 99.5 
periments (Liu, 2006b), we found that the classifiers performed relatively poorly when the estimated 
values of the largest mutual information and the second largest mutual information were close, so we 
chose to add the ratios between the estimated mutual information as features of the training instances. 
We divide each of the raw (estimated) mutual information by the largest mutual information to create 
new features. We also divided the largest mutual information by the second largest, and divided the 
largest mutual information by the average mutual information. 
Example 6. Table 6 shows a training instance for learning dABC by integrating dAB and cC, which is 
indicated by the class label AB_C. Let max denote the largest mutual information among the original 
features, runnerUp the second largest, and avg the average of all original features. We need to com-
pute the scores for four competing structures that are shown in Figures 1 and 3, and they are shown in 
the leftmost column of the table. A simple comparison and calculation show that max=0.17, run-
nerUp=0.08, and avg=0.0875 in this example. 
We also compute new features that are defined based on the original features. For instance, 
MI(dBC,cA;dABC)/max=0.04/0.17=0.23 and max/avg=0.17/0.0875=1.94. Among these new features, 
we observed in experiments that max/runnerUp is quite indicative of the danger that a wrong decision 
can be made. When this ratio is small, it is generally dangerous to apply Heuristic 1. In this particular 
case, the fact that max/runnerUp is 2.1 indicates that it is quite safe for us to choose AB_C as the way 
students that learn dABC. The chance of choosing a wrong solution by applying our heuristics in-
creased when this ratio fell below 1.2 in many of our pilot experiments. ■ 
We can compute the number of features for this procedure of preparing the training instances. 
When there are β basic concepts included in the composite concept, there will be )(βS)  original fea-
tures and 2)( +βS)  derived features. As we have shown in Table 3, the total number of features can 
grow explosively. Trying to examine the possibility and effects of reducing the computational load, we 
will reduce the number of features using the principle component analysis (PCA) (Jolliffe, 2002) in 
Effects of Methods. 
There are further details that we should provide about how we applied the ANNs and SVMs. For 
instance, we had to choose different parameters in applying both the ANNs and SVMs, and we scaled 
all feature values into the range [-1, 1] to improve the performance of the resulting ANNs and SVMs. 
These details are important but it is more appropriate to discuss them along with the experiments, so 
we defer such discussion until then. 
Table 6. A sample instance for training ANNs and SVMs 
class label: AB_C 
original features derived features 
MI(dAB,cC;dABC) 0.17 MI(dAB,cC;dABC)/max 1.00 max/runnerUP 2.12 
MI(dBC,cA;dABC) 0.04 MI(dBC,cA;dABC)/max 0.23 max/avg 1.94 
MI(dAC,cB;dABC) 0.06 MI(dAC,cB;dABC)/max 0.33   
MI(cA,cB,cC;dABC) 0.08 MI(cA,cB,cC;dABC)/max 0.47   
DESIGN OF THE EXPERIMENTS 
We explain the generation of student data, the major steps for an individual experiment, the evaluation 
of the classification results, and the major categories of experiments in four subsections. 
Generating Datasets 
Figure 6 shows the main flow of how we create the test records for the simulated students. The simula-
tor requires three different types of input. They include the skeleton of a Bayesian network that en-
codes the learning patterns of the simulated students, the Q-matrix that specifies the competence pat-
terns of the simulated students, and simulation parameters groupInfluence and fuzziness that control 
the degrees of uncertainty in the students’ item responses. The Bayesian networks can be provided by 
domain experts who have good reasons to employ the competing structures, and each of these struc-
tures represents a possible learning pattern of students. The Q-matrices are related to students’ compe-
tence and how the students apply their knowledge (Martin & VanLehn, 1995). Recall that we ex-
plained, in Generating Student Records, that the provided Q-matrix, groupInfluence, and fuzziness 
will influence the underlying joint distribution that we randomly create for the provided skeletal 
Bayesian network. We discussed a sample output in Example 2. Both the network structures and the 
simulated data will be used in further experiments. 
Although we have been using examples in which students need the knowledge about three basic 
concepts to learn the composite concept dABC, we will also present results of the experiments in 
which students need the knowledge about four basic concepts to learn the composite concept dABCD. 
We have shown the networks for cases for three basic concepts in Figures 1 and 3, and have applied 
their computational forms to refer to these network skeletons in The Target Question and Assump-
tions. With our non-overlapping assumption (stated in Definition 2), there can be only four ways to 
learn dABC: A_B_C, AB_C, AC_B, and BC_A.  
Figure 7 shows the networks for cases when four basic concepts are included in the target com-
posite concept, dABCD. In Figure 7(a), we do not show the nodes for the test items for readability. 
There would be 45 (=3×15) extra nodes otherwise. Note that, except for dABCD, the parent nodes of 
all nodes for composite concepts are nodes for basic concepts. This is not a necessary assumption, and 
the composite concepts that require the knowledge of three basic concepts can be learned by any con-
ceivable way. In drawing the networks shown in Figure 7(b), we only draw the node for dABCD and 
its parent nodes. All the other parts are exactly the same as their counterparts as already drawn in Fig-
ure 7(a). For instance, the parent concepts of dABD in the network that used the leftmost sub-network 
in the top row of Figure 7(b) are also cA, cB, and cD. For convenience, we refer to these skeletal net-
 item responses for
10000 simulated students
our simulator actual network
 structure
Q-matrix
simulator parameters:
groupInfluence and
fuzziness
 
Fig. 6. Creating records of item responses of simulated students 
works by their computational forms. Namely, from top to the bottom row and from left to right in both 
rows, we have A_B_CD, AB_C_D, ACD_B, ABD_C, and A_B_C_D in Figure 7(b). 
We will use 3baiscs and 4basics as defined below to specify the setups of the experiments. 
Based on the non-overlapping assumption and data shown in Table 3, 3baiscs includes all four dif-
ferent ways to learn dABC. There are 14 possible ways to learn dABCD, and we arbitrarily choose two 
cases that contain two parent concepts, two cases that contain three parent concepts, and one case that 
contains four parent concepts. 
Definition 4. When we try to learn the learning pattern for dABC, we provide A_B_C, AB_C, AC_B, 
and BC_A to the simulator, and call this set 3baiscs. 
Definition 5. When we try to learn the learning pattern for dABCD, we provide A_B_CD, AB_C_D, 
ACD_B, ABD_C, A_B_C_D to the simulator, and call this set 4basics. 
We employed the Q-matrices in Table 1 for the learning problems of dABC, when experimenting 
with 3baiscs. Table 7 shows a Q-matrix that we used in many of our experiments when we used 
4basics for the learning problems of dABCD. The contents of the Q-matrix in Table 7 are special in 
that we chose to set all the columns for the basic concepts and the target composite concepts to 1. This 
is equivalent to assuming the nature of the types of the students we recruit for a study of learning how 
they learn. If we are interested in learning how students learn dABCD, it should be reasonable to sup-
pose that we will recruit students who appear to be competent in all required basic concepts and the 
target composite concept. In addition to using this Q-matrix, we may also change the contents for dif-
ferent purposes in other experiments. For instance, in the experiments reported in Alternative Q-
Matrices, we set some numbers in the basic concepts and the target composite concept to 0. 
In the experimental evaluation, we set groupInfluence and fuzziness to different values in {0.05, 
0.10, 0.15, 0.20, 0.25, 0.30}. Hence, there can be 36 combinations of groupInfluence and fuzziness in 
our experiments. We did not try values larger than 0.3 because they were beyond the considerations 
normally discussed in the literature (e.g., VanLehn et al., 1998; Junker, 2006; Pardos et al., 2007). 
dABCD
dBD
dCD
group
dAB
dBC
cA cB cC cD
dAD dBCDdACDdABDdAC dABC
(a)
dABCD dCD
cDcA cB cC
dABCDdACD
cDcA cB cC
dABCDdAB
cDcA cB cC
dABCD
cDcA cB cC
dABCDdABD
cDcA cB cC
(b) 
Fig. 7. (a) One possible way to learn dABCD (b) Five other ways to learn dABCD 
Some researchers have reported observations of larger values for these parameters for special reasons; 
for instance, Beck and Sison (2004) observed a large value for the case of guess, and linked the obser-
vation with the speech recognition technology. 
For a network structure, a Q-matrix, and a particular combination of groupInfluence and fuzziness, 
we typically created test records for 10000 simulated students. A test record contains the correctness 
of a student’s item responses to all test items. Table 2 shows some sample test records. The setting for 
an experiment is constituted by a particular combination of groupInfluence and fuzziness, a structure 
for the Bayesian network that represents the candidate learning pattern, and a given Q-matrix. For con-
venience, we use the term a subset of an experiment to refer to a group of the settings in which we 
considered a specific combination of groupInfluence and fuzziness, the structures in 4basics (or 
3baiscs), and a given Q-matrix. In an experiment, we used many different subsets of experiments 
to compare the effects of the influential factors. 
Recall that we discussed the creation of the joint probability distribution for a Bayesian network 
with the help of random numbers in Generating Student Records. Hence, the generated Bayesian net-
works and the simulated test records varied with the seed for the random number generator. In order to 
obtain information about the average performance of our classifiers, we created 600 network instances 
for each setting in an experiment, and simulated 10000 students from each of these network instances. 
For convenience, we will refer to data that are created from a set of such 10000 simulated students as 
an instance. 
Example 7. An experiment for studying the learning patterns for dABCD may employ 1.08 billion 
simulated students, if we consider all 36 combinations of groupInfluence and fuzziness. In this case, 
each subset demands 30 million students. We obtained 30 million by multiplying three factors: five 
candidate networks in groupInfluence and fuzziness, 600 network instances per candidate network, and 
10000 students per network instance. The total of 1.08 billion is the result of multiplying 30 million by 
36. ■ 
Table 7. Competence patterns in a Q-matrix 
competence in (integrating) concepts SID cA cB cC cD dAB dAC dAD dBC dBD dCD dABC dABD dACD dBCD dABCD 
g1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 
g2 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 
g3 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 
g4 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 
g5 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 
g6 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 
g7 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 
g8 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 
g9 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 
g10 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 
g11 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 
g12 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 
g13 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 
g14 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 
g15 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 
g16 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 
Steps of the Experiments 
Due to the different nature of the heuristic methods, the search-based methods, and the machine learn-
ing-based methods, we evaluated the classifiers in slightly different ways. 
Figure 8 shows the main steps for evaluating the heuristic principle. We duplicate the shadowy 
part from Figure 6 to show how the simulator worked for our classifiers. When we worked on the 
learning problems of dABCD, )4(S
)
 is 14 as shown in Table 3. To guess the hidden structure of each of 
the 3000 (=5×600) network instances that we had generated for a subset of an experiment in which we 
considered the structures in 4basics, our classifier estimated the 14 mutual information measures 
based on the test records of 10000 simulated students, and guessed the hidden structure based on Heu-
ristic 1. We conducted the experiments for the learning problems of dABC analogously. 
Figure 9 shows that we evaluated Search4Pattern with almost the same method that we had 
used to evaluate Heuristic 1. The major difference was that we computed the scores for candidate 
structures hierarchically as explained in Search-Based Methods. Due to this hierarchical search 
procedure, we may save costs in computing scores for all the candidate solutions as analysed in 
Search-Based Methods. 
Figure 10 summarises the main steps that we took to apply ANNs and SVMs in our work. In a 
subset of an experiment that was designed for the learning problems of dABCD, we created 600 net-
work instances for each of the candidate networks shown in Figure 7(b). We split the network in-
test results
test data
100 instances for each of
the five candidate networks
training data
500 instances for each of
the five candidate networks
a trained ANN/SVM
guess the hidden structure
of the test instance
with the  trained ANN/SVM
train an ANN/SVM
(ANNs: try different types of ANNs;
SVMs: try different kernel functions;
try different combination of cost and gamma)
 
Fig. 10. Training and evaluating ANNs and SVMs 
estimate the MI measures for
14 possible hidden structures
predicted structure
record and evaluate
choose the structure that
has the largest estimated MI
simulator parameters:
groupInfluence and
fuzziness
Q-matrix
actual network
 structure
our simulator
 item responses for
10000 simulated students
 
Fig. 8. Flow for evaluating Heuristics 1 
Q-matrix
predicted structure
record and evaluateactual network
 structure
simulator parameters:
groupInfluence and
fuzziness
our simulator
 item responses for
10000 simulated students
choose the structure that
has the largest estimated MI  
Fig. 9. Flow for evaluating Search4Pattern 
sigmoid function: )tanh(),( jTiji xxxxK γ=  (6) 
In fact, although we had adopted some default settings in LIBSVM, we still had to search for the 
best parameters for SVMs at the time of training the SVMs. In particular, we ran experiments that 
used different values for the cost, C, which is the penalty parameter for misclassification, and γ, which 
appears in the kernel functions listed in Formulae (4), (5), and (6). Different combinations of C and γ 
led to different accuracy in guessing the hidden structures for the test data. We show a contour graph 
of the accuracy for a subset of the experiment in Figure 15(b), which we created for different combina-
tions of C and γ (when groupInfluence and fuzziness were 0.30 and 0.25, respectively). The numbers 
on the curves indicate the accuracies in percentage. In our experiments we tried combinations of C and 
γ from values in {0.1, 0.2, …, 1.9}, and used the best accuracy for the test data in these 361 (=19×19) 
cases when we prepared the charts in Figure 15(a). 
The charts shown in Figure 15(a) show the experimental results. The vertical axis, the horizontal 
axis, and the legend carry the same meanings as those for the charts in Figure 14. The titles of the 
charts indicate the types of SVMs that were used in the experiments. Similar to the charts in Figure 14, 
all three charts in Figure 15 show the general trend that the accuracy degraded with increasing 
groupInfluence and fuzziness. When these parameters were small, it was possible to achieve high accu-
racy. 
The effort in collecting information about the possible set of hidden structures and the Q-matrix 
proved rewarding again. Comparing the curves for the related experiments in Figures 13 and 15(a) 
shows us that significant improvements were achieved by using the SVMs. In the middle chart in Fig-
ure 15(a), the accuracy stays above 0.75 even when groupInfluence and fuzziness are 0.3. The heuris-
tics-based method achieved only 0.2 in accuracy under the same situation. The problem that occurred 
in the upper left corners of the charts in Figure 13 was also absent. The charts shown in Figure 15(a) 
indicate that using polynomial and radial basis kernels gave almost the same accuracy, and both per-
formed better than the sigmoid kernel. However, it took a longer time for us to train an SVM when we 
used the polynomial kernel. For instance, it took, respectively, 118 and 18 minutes to try 361 different 
combinations of C and γ when groupInfluence and fuzziness were both 0.3 in the left chart and in the 
middle chart in Figure 15(a). 
 (a) (b) 
Fig. 15. Using SVMs for prediction: (a) experimental results (b) search for the best parameters 
The best performing ANN and SVM models seemed to have achieved the same accuracy. Com-
paring the charts in Figures 14 and 15(a), we find that different ways of using the ANN and SVM 
techniques may offer different qualities in prediction. However, the middle charts in Figures 14 and 
15(a) suggest that the best-performing ANNs and SVMs offered almost the same performance. 
Table 8 shows the actual values of the data that we used to plot the middle chart in Figure 15(a) 
as well as their corresponding F measures (Witten & Frank, 2005). The precision rates and the recall 
rates were calculated for each of the five classes in 4basics first, and the F measure for each of 
these five classes was set to the average of the precision rate and the recall rate for that class. The re-
ported F measure in the table is the average of the F measures for the five classes. The observed accu-
racy and F measures were close, as we noted in Measurement of Quality. 
Depending on the values of groupInfluence and fuzziness, it took different lengths of time to run 
each of the experiments, even when we were using the same setting for SVMs. For instance, it took 
206 seconds to compare the effects of 361 combinations of C and γ when groupInfluence and fuzziness 
were both 0.05, and it took us 1083 seconds when groupInfluence and fuzziness were both 0.3. On av-
erage, we spent nearly 3 seconds for trying out the effects of a combination of C and γ when groupIn-
fluence and fuzziness were 0.3. (We measured the execution time on a Windows XP machine with 
LIBSVM 2.84 in C, Pentium IV 2.8G CPU, and 1.24G RAM.) 
Alternative Q-Matrices 
In this subsection, we investigate the effects of using different Q-matrices in the experiments. Since 
the experimental results discussed in Effects of Methods and Parameters suggested that using 
ANNs and SVMs could provide a similar performance, we used only the best performing SVMs, i.e., 
c-svm-rb in Figure 15(a) in this subsection. (We made this choice partially because LIBSVM is a 
freeware that we can run on many machines. In contrast, we have only one license for using MAT-
LAB.) We will change (1) the number of basic concepts that are included in the target composite con-
cepts in the first sub-subsection, (2) the way in which we set the values for other intermediate concepts 
in the second and (3) the competence patterns for the basic and the target composite concepts last. 
Effects of number of basic concepts 
We ran experiments with 3basics, the right Q-matrix in Table 1, and 36 combinations of groupIn-
fluence and fuzziness. Notice that we must use different Q-matrices for the structures in 3basics and 
4basics. Hence the differences in the accuracy of the resulting classification cannot be attributed 
exclusively to the number of basic concepts. 
Table 8. Accuracy versus F measures (shown in the form of accuracy/F)  
groupInfluence  0.05 0.10 0.15 0.20 0.25 0.30 
0.05 1.0000/1.0000 1.0000/1.0000 1.0000/1.0000 1.0000/1.0000 1.0000/1.0000 1.0000/1.0000
0.10 1.0000/1.0000 1.0000/1.0000 1.0000/1.0000 1.0000/1.0000 1.0000/1.0000 0.9960/0.9960
0.15 1.0000/1.0000 1.0000/1.0000 1.0000/1.0000 1.0000/1.0000 1.0000/1.0000 0.9900/0.9901
0.20 1.0000/1.0000 1.0000/1.0000 1.0000/1.0000 0.9980/0.9980 0.9940/0.9941 0.9600/0.9610
0.25 1.0000/1.0000 0.9960/0.9960 0.9980/0.9980 0.9840/0.9844 0.9480/0.9516 0.8860/0.8952
fuzziness 
0.30 0.9980/0.9980 0.9860/0.9865 0.9680/0.9697 0.9340/0.9374 0.8740/0.8854 0.7500/0.7719
Figure 16 shows the differences in the accuracy of classification when we reduced the number of 
basic concepts in the experiments. The chart titled 4basic is a duplicate of the c-svm-rb chart in Figure 
15(a), and the chart titled 3basic was produced from the new experiment. When groupInfluence and 
fuzziness are large, learning the way students learn dABC is easier than learning how they learn 
dABCD by a margin of nearly 10% in this pair of experiments. When groupInfluence and fuzziness are 
small, reducing the number of basic concepts did not yield obvious differences. 
All else being equal, a problem that considers three basic concepts is not as complex as one that 
considers four basic concepts in nature. Hence, what we have observed should not be surprising. How-
ever, experimental results are affected by many factors including those that we will discuss in the re-
mainder of this paper, so we cannot claim that problems that consider only three basic concepts must 
be easier than those that consider four basic concepts. 
Effects of competence patterns for the intermediate concepts 
We refer to the composite concepts that can serve as the parent concepts of the target composite con-
cept as the intermediate concepts. When we study the learning problems of dABCD, there can be 10 
intermediate concepts, including those composite concepts that involve two or three basic concepts, 
i.e., dAB, dAC, …, and dBCD in Table 7. 
Recall that, in the Q-matrix in Table 7, we assumed that all of the recruited students were compe-
tent in the basic concepts and were able to integrate the parent concepts of dABCD. Based on such a 
setting, the problem of changing the competence patterns for the intermediate concepts can be rede-
fined as one of choosing the number of student groups in the Q-matrix. Hence, we selected some stu-
dent groups that we used in Table 7 as the Q-matrices that we used in the new experiments. 
We conducted experiments in which the Q-matrices contained one, two, four, eight, and sixteen 
student groups, and we called the Q-matrices used in these experiments Q1, Q2, Q4, Q8, and Q16, re-
spectively. Hoping to do a more meaningful comparison between results of different experiments, we 
made Qi a subset of Qj when i<j. Namely, a student group must belong to Qj if that student group be-
longs to Qi, for any i<j. The first group, g1, was the obvious choice for Q1 because it represented the 
group of perfect students. For easier reference and comparison, Q16 is a complete duplicate of the Q-
matrix in Table 7. This was how we determined Q1 and Q16 in Table 9. 
10 20 30
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
fuzziness
a
cc
u
ra
cy
3basic
 
 
.05
.10
.15
.20
.25
.30
10 20 30
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
fuzziness
a
cc
u
ra
cy
4basic
 
 
.05
.10
.15
.20
.25
.30
 
Fig. 16. Using the Q-matrices contained in Table 7 and in the right part of Table 1 
We anticipated that student subgroups that had stronger contrasting competence patterns would 
help our classifiers make correct decisions, so we chose a student group that was most different from 
g1 to be included in Q2. We computed the distance between all pairs of student groups based on the 
Euclidean distance between the competence patterns of two student groups. Equation (7) shows the 
definition for the current experiment, where 15 is the number of different concepts in the Q-matrix in 
Table 7. Based on this notion of distance, g16 was chosen to be the second student group in Q2. 
∑=
=
−= 15
1
2
,, )(),(
k
k
kgkgji ji qqggdistance    (7) 
Fig. 17. Effects of changing the competence patterns for the intermediate concepts 
We then calculated the distances between all pairs of student groups except g1 and g16 that were in 
Q2. We put the pair, g7 and g9, that had the largest distance between them into Q4 and the pairs that had 
the second and the third largest distance into Q8. Table 9 shows the resulting Q1, Q2, Q4, Q8, and Q16. 
We used the structures in 4basics, the new Q-matrices, and 36 combinations of groupInfluence 
and fuzziness in the new experiments. The charts in Figure 17 depict the results of this sequence of 
experiments. From the left to the right, the results came from the experiments in which we used Q1, Q2, 
Q4, and Q8 to create the training and test data. The rightmost chart is a duplicate of the c-svm-rb chart 
in Figure 15(a). 
The results show some interesting trends. Although we used perfect students in Q1, it was very 
difficult to learn how students learn when we collected data exclusively from perfect students. This 
phenomenon became less surprising when we came to believe that it is hard to tell how students learn 
if they are competent in all relevant concepts. Hence, we consider this simulated result interesting be-
cause the simulated results taught us something that we had not expected. 
As we added more and more contrasting pairs of student groups into the Q-matrices, the average 
accuracy improved from the leftmost to the rightmost chart. The curves in the individual chart move 
upward gradually. In addition, we see that the curves for cases that used smaller groupInfluence and 
fuzziness do not necessarily fall below the curves for cases that used larger groupInfluence and fuzzi-
ness in an individual chart. This is particularly so in the charts on the left side of Figure 17. This ob-
servation shows that the intermediate patterns are as influential as groupInfluence and fuzziness on the 
experimental results. 
Effects of competence in the basic and the target composite concepts 
Table 9. Competence patterns in four sub-matrices of the Q-matrix in Table 7 
competence in (integrating) concepts  SID cA cB cC cD dAB dAC dAD dBC dBD dCD dABC dABD dACD dBCD dABCD 
Q1 g1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 
g1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Q2 g16 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 
g1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 
g7 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 
g9 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 
Q4 
g16 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 
g1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 
g3 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 
g4 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 
g5 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 
g7 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 
g9 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 
g13 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 
Q8 
g16 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 
Q16 … duplicate the contents in Table 7 
In conducting the experiments, we reused parts of the Q-matrix as listed in Table 7, and revised part of 
the Q-matrix to meet the purposes of the experiments. We changed the columns for the basic concepts 
when we explored the influence of the basic concepts, and we changed the column for the target com-
posite concept, i.e., dABCD, when we investigated the influence of competence in the target composite 
concept. Table 10 shows part of the Q-matrices that we used in the experiments for testing the influ-
ence of the basic and the target composite concepts. The contents of Qb are the same as those of the Q-
matrix listed in Table 7, except for the values in the columns for basic concepts in Table 10. We set 
the values for the basic concepts to all 16 possible combinations. Analogously, we copied most of the 
contents of Qt from the Q-matrix listed in Table 7, except for the dABCD column that is shown in the 
right part of Table 10. The values for the target composite concept were set to 0 and 1 arbitrarily. We 
employed, respectively, Qb and Qt to explore the influence of the competence in the basic and the tar-
get composite concepts. Again, we ran experiments with the structures in 4basics and 36 combina-
tions of groupInfluence and fuzziness. 
The basics and target charts in Figure 18, respectively, depict the experimental results that we ob-
tained when we used Qb and Qt in the experiments. Again, we duplicate the c-svm-rb chart from Fig-
ure 15(a) for comparison purpose. 
The results support our argument for the selection of the Q-matrix in Table 7, at the beginning of 
Design of the Experiments. The differences between the basics and the c-svm-rb charts suggest that 
it will not be very fruitful if we recruit students who are not competent in the basic concepts in order to 
study how they might learn the target composite concept. The differences between the target and the c-
svm-rb charts are not as salient as those between the basics and the c-svm-rb charts, but the trends still 
support that we should recruit students who appear to be competent in the target composite concept. 
Table 10. Competence patterns in the Q-matrices for testing the influence of 
the basic and the target composite concepts 
Qb Qt Qb Qt SID cA cB cC cD dABCD SID cA cB cC cD dABCD 
g1 1 1 1 1 1 g9 0 1 1 1 1 
g2 1 1 1 0 0 g10 0 1 1 0 0 
g3 1 1 0 1 1 g11 0 1 0 1 1 
g4 1 1 0 0 0 g12 0 1 0 0 0 
g5 1 0 1 1 1 g13 0 0 1 1 1 
g6 1 0 1 0 0 g14 0 0 1 0 0 
g7 1 0 0 1 1 g15 0 0 0 1 1 
g8 1 0 0 0 0 g16 0 0 0 0 0 
 
Fig. 18. Effects of changing the competence patterns for the basic and the 
target composite concepts 
sulting accuracies in experiments. It was interesting to observe that the SVMs did not necessarily out-
perform Search4Pattern when we had only imperfect information about Q-matrices, groupInflu-
ence, and fuzziness. Hence, it would definitely be rewarding to seek more exact information about 
these influential factors. 
Although we spent the greater part of our time in this present work in learning the learning pat-
terns for a composite concept that involves four basic concepts, due to computational costs also dis-
cussed in (DiBello et al., 1995, page 364), the proposed approach can be applied to learning the learn-
ing patterns of more complex composite concepts. What is required is that we should explore the prob-
lem space incrementally. Namely, building the structures for simpler composite concepts before trying 
to learn how students learn more complex composite concepts, where the “simple vs. complex” notion 
is based on the number of basic concepts included in the composite concepts. With appropriate basic 
building blocks (sometimes called “objects” in computer science), we will be able to build models for 
more complex composite concepts. 
The use of simulated students in the experiments can appear as a weakness in this study. Under 
no circumstances can simulated students replace real students for decisive answers. In practice, student 
modelling for CATs must choose some levels of abstraction for the students in the models, and this 
practical imperfectness also exists in systems that aim at mental simulation (Weng & Huang, 2006). 
Nevertheless, we have considered many important factors, including groupInfluence, fuzziness, com-
petence patterns in the Q-matrices, and imperfect guesses in the experiments. Hence, we hope that the 
scale of the experiments and the reported observations justify the plan of using the simulated results to 
identify important issues that we may encounter when we use data for real students in future studies. 
Obviously, we have not completed all paths of the exploration for this problem in this already 
lengthy paper. For instance, we mentioned that the search-based method and SVMs complemented 
each other in the more realistic experiments in More Realistic Evaluations. This observation sug-
gests that one may seek to combine the predictions made by these two methods to achieve better re-
sults, which is the so-called stacking method as used in the machine learning community (Witten & 
Frank, 2005). However, we would prefer to explore this opportunity with real students when possible. 
More on Related Work 
What we have discussed so far involves the issues of (1) the definition of “causal relationships,” (2) 
representing the causal relationships with Bayesian networks, and (3) learning the causal models for 
variables of interest from indirect evidences. Using the most intuitive interpretation of the word 
“causal,” we believe that being competent in a parent concept, say dAB, is a fundamental basis for a 
student to be able to learn a more complex concept, say dABC, under the normal conditions. Hence, 
we believe that the first issue is not a major concern in this paper. 
It cannot be denied that our work is related to the modelling of causal relationships among ran-
dom variables with the use of only indirect evidence. Inferring the causal relationships among vari-
ables of interest can have a wide range of applications. Hence, it should not be surprising that re-
searchers of many disciplines have studied this topic in the literature, e.g., (Rost & Langeheine, 1997; 
Glymour & Cooper, 1999; Chockler & Halpern, 2004; Halpern & Pearl, 2005). In fact, the learning of 
graphical structures to represent causal relationships among factors of interest is a common interest in 
science, and is not limited to the learning of Bayesian networks; for instance, Desjardins (2001) at-
tempts to learn causal structures of chemical reactions with unobservable variables. 
Bayesian networks themselves do not necessarily represent causal relationships (Pearl, 1988), but 
it is possible to represent causal relationships with Bayesian networks (Cooper, 1999; Glymour, 2003). 
Not all applications of Bayesian networks to student assessment aim at building causal models, and 
may choose whatever structures that will fulfil the needs of probabilistic reasoning (Millán & Pérez-
de-la-Cruz, 2002). For instance, when considering a capability that has multiple prerequisites, all the 
nodes that represent the prerequisites may be used as the parent nodes of the node that represents the 
integrated capability, very similar to the approach taken by people who use Concept Maps (Novak, 
1990). Some researchers also reverse the arc directions between nodes for the prerequisites and the 
integrated capability (Millán & Pérez-de-la-Cruz, 2002). Nevertheless, using the nodes that represent 
the prerequisites as the parent nodes is a more common and intuitive choice (Martin & VanLehn,1995; 
Millán & Pérez-de-la-Cruz 2002). 
Among the research works that adopt Bayesian networks for student modelling, the way we build 
Bayesian networks is related to Millán and Pérez-de-la-Cruz’s (2002) categorising nodes for represent-
ing subjects, topics, concepts, and questions. In their continuing work, Carmona et al. (2005) showed 
that adding appropriate links for encoding prerequisite relationships in Bayesian networks can improve 
the efficiency in adaptive student assessment. Yet another related work considering the prerequisite 
relationships in Bayesian networks is by Reye (2004), but the structures proposed by Reye are quite 
different from what we see in this paper and Millán’s models. 
Our work is also related to the research of multilevel models based on the Item Response Theory 
(IRT) (Fox, 2005). If we take the relationships between the test items and the basic concepts as the 
first-level IRT model, and the relationships between the basic concepts and the composite concepts as 
the higher levels, our models, e.g., the one shown in Figure 7(a), are related to multilevel IRT models. 
From this viewpoint, our work is an instance of studying how computers can help experts determine 
the structures of their multilevel IRT models. However, to make our models be more qualified as IRT 
models, we have to strengthen our models by adding more parameters to quantify the relationships 
between item responses and competence in concepts. 
Given that we chose to represent the prerequisite relationships with Bayesian networks, our prob-
lems become instances of learning the hidden structures among the related concepts (Heckerman, 1999; 
Neapolitan, 2004). Learning the structures directly from data is not an easy task, particularly when the 
values of many of the random variables are completely missing. The domain knowledge provided by 
domain experts is believed to help us learn models of higher qualities (AUAI, 2006). Although we 
cannot explore all the problem instances that one can imagine due to the number possible combina-
tions as discussed in Computational Complexity, we explored some interesting settings in the ex-
periments, and the results show the importance of the quality of source information. 
It is possible to learn the prerequisite relationships from some related work, e.g., theory about 
knowledge structure (Falmagne et al., 2003) and item-to-item knowledge structure (Desmarais et al., 
2006). Learning item-to-item knowledge structure requires certain special techniques. Figure 4(a) as 
discussed in Impacts of Latent Variables is an item-to-item structure that we learned with the PC 
algorithm in Hugin. Clearly there are places in the structure where we can improve, e.g., the directions 
of some arcs should be reversed, and interested readers can refer to (Desmarais et al., 2006). Certain 
recent research results, e.g., (Albert et al., 2007; Guzmán et al., 2007a) report the applications of hier-
archical structures are also related to our work. 
Concluding Remarks 
We have achieved a wide range of classification accuracies in our experiments, depending on the qual-
ity of our preparation of the training data and the students’ responses. Experimental results suggest 
that, when we can acquire sufficiently good advice on a problem, machine-learning techniques (both 
the best performing ANNs and SVMs) may help us identify the hidden learning processes nearly 90% 
of the time in favourable conditions. When we cannot acquire advises of higher quality, search-based 
methods, i.e., Search4Pattern, can become a good alternative. When we do not have adequate 
information about the students and when the relationship between students’ item responses and their 
competence levels are very uncertain, it becomes very difficult to infer how students learn based on 
their item response patterns.  
We have identified a method, that we discussed along with Formulae (8) and (9), to predict the in-
fluences of different Q-matrices. This analytical viewpoint helps us choose student subgroups that can 
help us achieve higher accuracies in learning student models. The selection of Q-matrices in experi-
ments is an important issue in realistic studies (DiBello et al., 1995, pp. 370–371). All else being equal, 
increasing the total_distance, which is defined in Formula (9), increased the chances of identifying the 
correct learning patterns.  
Although the use of simulators must result in some degree of distance or abstraction from the real 
situations and cannot mimic all the characteristics of real students perfectly, we believe that results 
observed in our simulation-based experiments have shed some light on the nature of this learning 
problem about learning. 
Do we really need to know and include the prerequisite relationship among concepts in student 
models? Mislevy and Gitomer (1996) state and we agree that “The nature and the grain-size of a stu-
dent model in an intelligent tutoring system ought therefore to be targeted to the instructional options 
available.” If we cannot take advantage of the detailed models, there is perhaps no incentive for en-
deavouring to find comprehensive models. Carmona et al. (2005) have shown that student models 
that consider prerequisite relationships make their adaptive student assessment more efficient. We 
also hope that more instructional options will become available with the advent of detailed student 
models, thereby forming a synergistic relation between the two.  
The work reported in this paper is related to cognitive diagnostic assessment for education. Cog-
nitively informed models have the potential to help computers assist human’s learning activities in a 
more effective and efficient way (Nichols et al., 1995; Conati, 2002; Alkhalifa, 2006; Leighton & 
Gierl, 2007). More specifically, in a recently edited book by Leighton and Gierl (2007), Huff and 
Goodman (2007) elaborate several issues that are related to employing cognitive diagnostic assess-
ment for providing instructionally relevant information that serves the needs for education in addition 
to scoring. Gierl et al. (2007) discuss four possible structures for describing the relationships between 
attributes in test development. We hope that the proposed methods and the experimental results pre-
sented here may contribute to the efforts in mapping the human learning process and cognitive diag-
nostic assessment. 
ACKNOWLEDGEMENTS 
We are grateful to several anonymous researchers for their comments on previous versions of this pa-
per (Liu, 2006a,b,c). The comments led us to conduct more experiments which helped us enrich the 
breadth and depth of this paper. Special thanks go to the anonymous reviewers of this journal for their 
directions to help this paper improve on the sections on interdisciplinary discussion. Miss Yu-Ting 
Wang participated in this project when we had just begun the exploration (Liu & Wang, 2006), and 
Miss Moira Breen helped the proofreading of the manuscript. Work reported in this paper was sup-
ported in part by the grants NSC-93-2213-E-004-004, NSC-94-2213-E-004-008 and NSC-95-2221-E-
004-013-MY2 from the National Science Council and in part by the project 96H061 in National 
Chengchi University under the ATU plan of the Ministry of Education of Taiwan. 
REFERENCES 
Albert, D., Hockemeyer, C., Mayer, B., & Steiner, C. M. (2007). Cognitive structural modelling of skills for 
technology-enhanced learning. Proceedings of the Seventh IEEE International Conference on Advanced 
Learning Technologies, 322–324.  
Alkhalifa, E. M. (Ed.). (2006). Cognitively Informed Systems: Utilizing Practical Approaches to Enrich Informa-
tion Presentation and Transfer. PA, USA: Idea Group.  
Almond, R. G, Mislevy, R. J., Williamson, D. M., & Yan, D. (2008). Bayesian Networks in Education Assess-
ment, a pre-conference training session presented in the 2008 Annual Meeting of the National Council 
Measurement in Education, http://www.ncme.org. 
AUAI. (2006). Association for Uncertainty in Artificial Intelligence: discussion about definition of causality and 
learning structural models between Joseph Mitola III, Joseph Y. Halpern, and Lotfi A. Zadeh, 14-26 July 
2006 on the mailing list of the AUAI, http://www.auai.org.  
Beck, J. E., & Sison, J. (2004). Using Knowledge Tracing to Measure Student Reading Proficiencies, Lecture 
Notes in Computer Science 3220: Proceedings of the Seventh International Conference on Intelligent Tu-
toring Systems, 624–634. 
Birenbaum, M., Kelly, A. E., Tatsuoka, K. K., & Gutvirtz, Y. (1994). Attribute-mastery patterns from rule space 
as the basis for student models in algebra. International Journal of Human-Computer Studies, 40(3), 497–
508.  
Bishop, C. M. (1995). Neural Networks for Pattern Recognition. Oxford, UK: Oxford University Press.  
Carmona, C., Millán, E., Pérez-de-la-Cruz, J. L., Trella, M., & Conejo, R. (2005). Introducing prerequisite rela-
tions in a multi-layered Bayesian student model. Proceedings of the Tenth International Conference on 
User Modeling, 347–356.  
Chang, C.-C., & Lin, C.-J. (2001). LIBSVM: A library for support vector machines, 
http://www.csie.ntu.edu.tw/~cjlin/libsvm/. (last visited on 20 February, 2008)  
Chang, K.-M., Beck, J., Mostow, J., & Corbett, A. (2006). A Bayes net toolkit for student modeling in intelligent 
tutoring systems. Proceedings of the Eighth International Conference on Intelligent Tutoring Systems, 104–
113.  
Chockler, H., & Halpern, J. Y. (2004). Responsibility and blame: A structural-model approach. Journal of Artifi-
cial Intelligence Research, 22, 95–115.  
Conati, C. (2002). Probabilistic assessment of user’s emotions in educational games. Applied Artificial Intelli-
gence, 16(7–8), 555–575.  
Conati, C., Gertner, A., & VanLehn, K. (2002). Using Bayesian networks to manage uncertainty in student mod-
eling. User Modeling and User-Adapted Interaction, 12(4), 371–417.  
Conejo, R., Guzmán, E., Millán, E., Trella, M., Pérez-de-la-Cruz, J. L., & Rios, A. (2004). SIETTE: A Web-
based tool for adaptive testing. International Journal of Artificial Intelligence in Education, 14, 29–61.  
Cooper, G. F. (1999). An overview of the representation and discovery of causal relationships using Bayesian 
networks. In C. Glymour & G. F. Cooper (Eds.), Computation, Causation, and Discovery (pp. 3–62). MA, 
USA: AAAI Press/The MIT Press.  
Cortes, C., & Vapnik, V. (1995). Support-vector network. Machine Learning, 20(3), 273–297.  
Cover, T. M., & Thomas, J. A. (2006). Elements of Information Theory (second ed.). NY, USA: John Wiley & 
Sons.  
Desjardins, B. (2001). Inference of causal structure using the unobservable. Journal of Experimental and Theo-
retical Artificial Intelligence, 13(3), 291–305.  
Desmarais, M. C., Meshkinfam, P., & Gagnon, M. (2006). Learned student models with item to item knowledge 
structures. User Modeling and User-Adapted Interaction, 16(5), 403–434.  
DiBello, L. V., Stout, W. F., & Roussos, L. A. (1995). Unified cognitive/psychometric diagnostic assessment 
likelihood-based classification techniques. In P. D. Nichols, S. F. Chipman, & R. L. Brennan (Eds.), Cogni-
tively Diagnostic Assessment (pp. 361–389). Hove, UK: Lawrence Erlbaum Associates.  
Duda, R. O., Hart, P. E., & Stork, D. G. (2001). Pattern Classification. NY, USA: John Wiley & Sons.  
Falmagne, J.-C., Doignon, J.-P., Cosyn, E., & Thiery, N. (2003). The assessment of knowledge in theory and in 
practice (Paper No. 26). Institute for Mathematical Behavioral Sciences, University of California, Irvine, 
California, USA.  
Fox, J.-P. (2005). Multilevel IRT using dichotomous and polytomous response data. British Journal of Mathe-
matical and Statistical Psychology, 58(1), 145–172.  
Gierl, M. J., Leighton, J. P., & Hunka, S. M. (2007). Using the attribute hierarchy method to make diagnostic 
inferences about examinees’ cognitive skills. In J. P. Leighton and M. J. Gierl (Eds.), Cognitive Diagnostic 
Assessment for Education (pp. 242–274). NY, USA: Cambridge University Press. 
Glymour, C. (2003). Learning, prediction and causal Bayes nets. Trends in Cognitive Sciences, 7(1), 43–46.  
Glymour, C., & Cooper, G. F. (Eds.). (1999). Computation, Causation, and Discovery. CA/MA, USA: AAAI 
Press/The MIT Press.  
Guzmán, E., Conejo, R., & Pérez-de-la-Cruz, J. L. (2007a). Adaptive testing for hierarchical student models. 
User Modeling and User-Adapted Interaction, 17(1), 119–157.  
Guzmán, E., Conejo, R., & Pérez-de-la-Cruz, J. L. (2007b). Improving student performance using self-
assessment tests. IEEE Intelligent Systems, 22(4), 46–54.  
Halpern, J. Y., & Pearl, J. (2005). Causes and explanations: A structural-model approach. Part I: Causes. British 
Journal for the Philosophy of Science, 56(4), 843–887.  
Heckerman, D. (1999). A tutorial on learning with Bayesian networks. In M. I. Jordan (Ed.), Learning in 
Graphical Models (pp. 301–354). MA, USA: The MIT Press.  
Huff, K., & Goodman D. P. (2007). The demand for cognitive diagnostic assessment. In J. P. Leighton and M. J. 
Gierl (Eds.), Cognitive Diagnostic Assessment for Education (pp. 19–60). NY, USA: Cambridge University 
Press. 
Jensen, F. V., & Nielsen, T. D. (2007). Bayesian Networks and Decision Graphs (second ed.). NY, USA: 
Springer-Verlag.  
Jolliffe, I. T. (2002). Principal Component Analysis. NY, USA: Springer-Verlag.  
Jordan, M. I. (Ed.). (1999). Learning in Graphical Models. MA, USA: The MIT Press.  
Junker, B. W. (2006). Using on-line tutoring records to predict end-of-year exam scores: Experience with the 
ASSISTments project and MCAS 8th grade mathematics. In Lissitz, R. W. (Ed.), Assessing and Modeling 
Cognitive Development in School: Intellectual Growth and Standard Setting. Maple Grove, MN: JAM 
Press. 
Knuth, D. E. (1973). The Art of Computer Programming: Fundamental Algorithms. MA, USA: Addison-Wesley. 
(p. 73)  
Leighton, J. P., & Gierl M. J. (2007). Cognitive Diagnostic Assessment for Education. NY, USA: Cambridge 
University Press. 
Liu, C.-L. (2005). Using mutual information for adaptive item comparison and student assessment. Journal of 
Educational Technology & Society, 8(4), 100–119.  
Liu, C.-L. (2006a). Learning how students learn with Bayes nets. Lecture Notes in Computer Science 4053: Pro-
ceedings of the Eighth International Conference on Intelligent Tutoring Systems, 772–774.  
Liu, C.-L. (2006b). Learning students’ learning patterns with neural computing. Proceedings of the 2006 IEEE 
International Conference on Systems, Man, and Cybernetics, 2434–2439.  
Liu, C.-L. (2006c). Learning students’ learning patterns with support vector machines. Lecture Notes in Com-
puter Science 4203: Proceedings of the Sixteenth International Symposium on Methodologies for Intelligent 
Systems, 601–611.  
Liu, C.-L. (2006d). Using Bayesian networks for student modeling. In E. M. Alkhalifa (Ed.), Cognitively In-
formed Systems: Utilizing Practical Approaches to Enrich Information Presentation and Transfer (pp. 
282–309). PA, USA: Idea Group.  
Liu, C.-L., & Wang, Y.-T. (2006). An experience in learning about learning composite concepts. Proceedings of 
the Sixth IEEE International Conference on Advanced Learning Technologies, 187–189.  
Martin, J., & VanLehn, K. (1995). Student assessment using Bayesian nets. International Journal of Human-
Computer Studies, 42(6), 575–591.  
Matsuda, N., Cohen, W. W., Sewall, J., Lacerda, G., & Koedinger, K. R. (2007). Predicting students’ perform-
ance with SimStudent learning cognitive skills from observation. Proceedings of the Thirteenth Interna-
tional Conference on Artificial Intelligence and Education, 467–476.  
Mayo, M., & Mitrovic, A. (2001). Optimising ITS behaviour with Bayesian networks and decision theory. Inter-
national Journal of Artificial Intelligence in Education, 12, 124–153.  
Millán, E., & Pérez-de-la-Cruz, J. L. (2002). A Bayesian diagnostic algorithm for student modeling and its 
evaluation. User Modeling and User-Adapted Interaction, 12(2-3), 281–330.  
Mislevy, R. J., Almond, R. G., Yan, D., & Steinberg, L. S. (1999). Bayes nets in educational assessment: Where 
do the numbers come from? Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelli-
gence, 437–446.  
Mislevy, R. J., & Gitomer, D. H. (1996). The role of probability-based inference in an intelligent tutoring system. 
User Modeling and User-Adapted Interaction, 5(4), 253–282.  
Naveh-Benjamin, M., Lin, Y.-G., & McKeachie, W. J. (1995). Inferring students’ cognitive structures and their 
development using the “fill-in-the-structure” (FITS) technique. In P. D. Nichols, S. F. Chip-man, & R. L. 
Brennan (Eds.), Cognitively Diagnostic Assessment (pp. 279–304). Hove, UK: Lawrence Erlbaum Associ-
ates.  
Neapolitan, R. E. (2004). Learning Bayesian Networks. NJ, USA: Prentice Hall.  
Nichols, P. D., Chipman, S. F., & Brennan, R. L. (Eds.). (1995). Cognitively Diagnostic Assessment. Hove, UK: 
Lawrence Erlbaum Associates.  
Novak, J. D. (1990). Concept maps and Vee diagrams: Two metacognitive tools to facilitate meaningful learning. 
Instructional Science, 19(1), 29–52.  
Pardos, Z., Feng, M.,  Heffernan, N. T., & Heffernan-Lindquist, C. (2007). Analyzing fine-grained skill models 
using Bayesian and mixed effect methods. Proceedings of the Thirteenth Conference on Artificial Intelli-
gence in Education. 
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. CA, USA: 
Morgan Kaufmann.  
Reye, J. (2004). Student modelling based on belief networks. International Journal of Artificial Intelligence in 
Education, 14, 63–96.  
Rost, J., & Langeheine, R. (Eds.). (1997). Applications of Latent Trait and Latent Class Models in the Social 
Sciences. Münster, Germany: Waxmann.  
Russell, S. J., & Norvig, P. (2002). Artificial Intelligence: A Modern Approach. NJ, USA: Prentice Hall.  
Shachter, R. D. (1988). Probabilistic inference and influence diagrams. Operation Research, 36(4), 589–604.  
Spirtes, P., Glymour, C., & Scheines, R. (2000). Causation, Prediction, and Search (second ed.). MA, USA: The 
MIT Press.  
Tatsuoka, K. K. (1983). Rule space: An approach for dealing with misconceptions based on item response theory. 
Journal of Educational Measurement, 20, 345–354.  
van der Linden, W. J., & Glas, C. A. (Eds.). (2000). Computerized Adaptive Testing: Theory and Practice. 
Dordrecht, The Netherlands: Kluwer.  
van der Linden, W. J., & Hambleton, R. K. (Eds.). (1997). Handbook of Modern Item Response Theory. NY, 
USA: Springer.  
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 93–96,
Columbus, Ohio, USA, June 2008. c©2008 Association for Computational Linguistics
Using Structural Information for Identifying Similar Chinese Characters 
Chao-Lin Liu Jen-Hsiang Lin 
Department of Computer Science, National Chengchi University, Taipei 11605, Taiwan 
{chaolin, g9429}@cs.nccu.edu.tw
Abstract
Chinese characters that are similar in their 
pronunciations or in their internal structures 
are useful for computer-assisted language 
learning and for psycholinguistic studies. Al-
though it is possible for us to employ image-
based methods to identify visually similar 
characters, the resulting computational costs 
can be very high. We propose methods for 
identifying visually similar Chinese characters 
by adopting and extending the basic concepts 
of a proven Chinese input method--Cangjie. 
We present the methods, illustrate how they 
work, and discuss their weakness in this paper. 
1 Introduction 
A Chinese sentence consists of a sequence of char-
acters that are not separated by spaces. The func-
tion of a Chinese character is not exactly the same 
as the function of an English word. Normally, two 
or more Chinese characters form a Chinese word to 
carry a meaning, although there are Chinese words 
that contain only one Chinese character. For in-
stance, a translation for “conference” is “ࣴ૸཮”
and a translation for “go” is “ѐ”. Here “ࣴ૸཮”
is a word formed by three characters, and “ѐ” is a 
word with only one character. 
Just like that there are English words that are 
spelled similarly, there are Chinese characters that 
are pronounced or written alike. For instance, in 
English, the sentence “John plays an important roll 
in this event.” contains an incorrect word. We 
should replace “roll” with “role”. In Chinese, the 
sentence “ϞϺ΢ϱךॺٰ၂൑ວ๼” contains an 
incorrect word. We should replace “၂൑” (a place 
for taking examinations) with “ѱ൑” (a market). 
These two words have the same pronunciation, 
shi(4) chang(3) †, and both represent locations. The 
sentence “࿶౛ाךᄬວ΋೽ीᆉᐒ” also con-
                                                          
† We use Arabic digits to denote the four tones in Mandarin. 
tains an error, and we need to replace “ᄬວ” with 
“ᖼວ”. “ᄬວ” is considered an incorrect word, 
but can be confused with “ᖼວ” because the first 
characters in these words look similar. 
Characters that are similar in their appear-
ances or in their pronunciations are useful for 
computer-assisted language learning (cf. Burstein 
& Leacock, 2005). When preparing test items for 
testing students’ knowledge about correct words in 
a computer-assisted environment, a teacher pro-
vides a sentence which contains the character that 
will be replaced by an incorrect character. The 
teacher needs to specify the answer character, and 
the software will provide two types of incorrect 
characters which the teachers will use as distracters 
in the test items. The first type includes characters 
that look similar to the answer character, and the 
second includes characters that have the same or 
similar pronunciations with the answer character. 
Similar characters are also useful for studies 
in Psycholinguistics. Yeh and Li (2002) studied 
how similar characters influenced the judgments 
made by skilled readers of Chinese. Taft, Zhu, and 
Peng (1999) investigated the effects of positions of 
radicals on subjects’ lexical decisions and naming 
responses. Computer programs that can automati-
cally provide similar characters are thus potentially 
helpful for designing related experiments. 
2 Identifying Similar Characters with In-
formation about the Internal Structures 
We present some similar Chinese characters in the 
first subsection, illustrate how we encode Chinese 
characters in the second subsection, elaborate how 
we improve the current encoding method to facili-
tate the identification of similar characters in the 
third subsection, and discuss the weakness of our 
current approach in the last subsection. 
2.1 Examples of Similar Chinese Characters 
We show three categories of confusing Chinese 
characters in Figures 1, 2, and 3. Groups of similar 
93
characters are separated by spaces in these figures. 
In Figure 1, characters in each group differ at the 
stroke level. Similar characters in every group in 
the first row in Figure 2 share a common part, but 
the shared part is not the radical of these characters. 
Similar characters in every group in the second 
row in Figure 2 share a common part, which is the 
radical of these characters. Similar characters in 
every group in Figure 2 have different pronuncia-
tions. We show six groups of homophones that 
also share a component in Figure 3. Characters that 
are similar in both pronunciations and internal 
structures are most confusing to new learners. 
It is not difficult to list all of those characters 
that have the same or similar pronunciations, e.g., 
“၂൑” and “ѱ൑”, if we have a machine readable 
lexicon that provides information about pronuncia-
tions of characters and when we ignore special pat-
terns for tone sandhi in Chinese (Chen, 2000).  
In contrast, it is relatively difficult to find 
characters that are written in similar ways, e.g., 
“ᄬ” with “ᖼ”, in an efficient way. It is intriguing 
to resort to image processing methods to find such 
structurally similar words, but the computational 
costs can be very high, considering that there can 
be tens of thousands of Chinese characters. There 
are more than 22000 different characters in large 
corpus of Chinese documents (Juang et al., 2005), 
so directly computing the similarity between im-
ages of these characters demands a lot of computa-
tion. There can be more than 4.9 billion 
combinations of character pairs. The Ministry of 
Education in Taiwan suggests that about 5000 
characters are needed for ordinary usage. In this 
case, there are about 25 million pairs. 
The quantity of combinations is just one of 
the bottlenecks. We may have to shift the positions 
of the characters “appropriately” to find the com-
mon part of a character pair. The appropriateness 
for shifting characters is not easy to define, making 
the image-based method less directly useful; for 
instance, the common part of the characters in the 
right group in the second row in Figure 3 appears 
in different places in the characters. 
Lexicographers employ radicals of Chinese 
characters to organize Chinese characters into sec-
tions in dictionaries. Hence, the information should 
be useful. The groups in the second row in Figure 
2 show some examples. The shared components in 
these groups are radicals of the characters, so we 
can find the characters of the same group in the 
same section in a Chinese dictionary. However, 
information about radicals as they are defined by 
the lexicographers is not sufficient. The groups of 
characters shown in the first row in Figure 2 have 
shared components. Nevertheless, the shared com-
ponents are not considered as radicals, so the char-
acters, e.g., “ᓍ”and “ࠂ”, are listed in different 
sections in the dictionary.   
2.2 Encoding the Chinese Characters 
The Cangjie‡ method is one of the most popular 
methods for people to enter Chinese into com-
puters. The designer of the Cangjie method, Mr. 
Bong-Foo Chu, selected a set of 24 basic elements 
in Chinese characters, and proposed a set of rules 
to decompose Chinese characters into elements 
that belong to this set of building blocks (Chu, 
2008). Hence, it is possible to define the similarity 
between two Chinese characters based on the simi-
larity between their Cangjie codes.  
Table 1, not counting the first row, has three 
                                                          
‡ http://en.wikipedia.org/wiki/Cangjie_method 
γβπυί!ԉԊԋ!ҖҗҘҙ
҆Й!Ϯц!ΓΕ!҂҃!ߍٙ!пс
Figure 1. Some similar Chinese characters 
ᓍࠂ!རྎ!ഉ७!ะ౜!ᇘස!བᑎ
ӈӉ!ࣧ੽ޒࣩ!Ӣ֚ѥ!໔໕ଢ໒
Figure 2. Some similar Chinese characters that have 
different pronunciations 
׎Ӊࠠ!ᒞᅿဍ!ᖼᄬར इ૶ी
༜༝঩!⅂೵৩ยࠂ
Figure 3. Homophones with a shared component
 Cangjie Codes  Cangjie Codes
γ Μ΋! β! β!
π ΋ύ΋! υ! ΋Μ!
Ϯ ЈԮԮ! ц!! ԮҖЈ!
҂ ΜЕ! ҃! ЕΜ!
ᓍ ΋΋΋Дߎ! ࠂ! ΋΋εν!
ะ ΋αДξξ! ౜! ΋βДξξ!
བ ЋΝαД! ᑎ! ԮΝαД!
ӈ ΋ψύψ! Ӊ! ΋Ѕύψ!
Ӣ Җε! ֚! ҖЕ!
໔ ВψВ! ໕! ВψД!
ᒞ α΋ԮΜβ! ᅿ! ԮЕԮΜβ!
ဍ ДԮΜβ! इ! ζОνξ!
ᖼ ДߎЅЅД! ᄬ! ЕЅЅД!
૶ Νανξ! ी! ΝαΜ!
༝ ҖαДߎ! ঩! αДξߎ!
⅂ Д΋ζ΋! ೵! Ν΋ζ΋!
৩ ԮΓ΋ζ΋! ย! ε΋ζ΋!
Table 1. Cangjie codes for some characters
94
sections, each showing the Cangjie codes for some 
characters in Figures 1, 2, and 3. Every Chinese 
character is decomposed into an ordered sequence 
of elements. (We will find that a subsequence of 
these elements comes from a major component of a 
character, shortly.) Evidently, computing the num-
ber of shared elements provides a viable way to 
determine “visually similar” characters for charac-
ters that appeared in Figure 2 and Figure 3. For 
instance, we can tell that “བ” and “ᑎ” are similar 
because their Cangjie codes share “ΝαД”, which 
in fact represent “ଯ”.
Unfortunately, the Cangjie codes do not ap-
pear to be as helpful for identifying the similarities 
between characters that differ subtly at the stroke 
level, e.g., “γβπυ” and other characters listed 
in Figure 1. There are special rules for decompos-
ing these relatively basic characters in the Cangjie 
method, and these special encodings make the re-
sulting codes less useful for our tasks. 
The Cangjie codes for characters that contain 
multiple components were intentionally simplified 
to allow users to input Chinese characters more 
efficiently. The longest Cangjie code for any Chi-
nese character contains no more than five elements. 
In the Cangjie codes for “⅂” and “৩”, we see “΋
ζ΋” for the component “ᦩ”, but this component 
is represented only by “΋΋” in the Cangjie codes 
for “ᓍ” and “ࠂ”. The simplification makes it 
relatively harder to identify visually similar charac-
ters by comparing the actual Cangjie codes.  
2.3 Engineering the Original Cangjie Codes 
Although useful for the sake of designing input 
method, the simplification of Cangjie codes causes 
difficulties when we use the codes to find similar 
characters. Hence, we choose to use the complete 
codes for the components in our database. For in-
stance, in our database, the codes for “ᦩ”, “⅂”,
“৩”, “ᓍ”, and “ࠂ” are, respectively, “΋ζζ΋”,
“Д΋ζζ΋”, “ԮΓ΋ζζ΋”, “΋ζζ΋΋Д
ξߎ”, and “΋ζζ΋εν”.
The knowledge about the graphical structures 
of the Chinese characters (cf. Juang et al., 2005; 
Lee, 2008) can be instrumental as well. Consider 
the examples in Figure 2. Some characters can be 
decomposed vertically; e.g., “ࣩ” can be split into 
two smaller components, i.e., “ύ” and “ҝ”. Some 
characters can be decomposed horizontally; e.g., 
“౜” is consisted of “Ц” and “ـ”. Some have 
enclosing components; e.g., “Γ” is enclosed in 
“᢭” in “ѥ”. Hence, we can consider the locations 
of the components as well as the number of shared 
components in determining the similarity between 
characters. 
Figure 4 illustrates possible layouts of the 
components in Chinese characters that were 
adopted by the Cangjie method (cf. Lee, 2008). A 
sample character is placed below each of these 
layouts. A box in a layout indicates a component in 
a character, and there can be at most three compo-
nents in a character.  We use digits to indicate the 
ordering the components. Notice that, in the sec-
ond row, there are two boxes in the second to the 
rightmost layout. A larger box contains a smaller 
one. There are three boxes in the rightmost layout, 
and two smaller boxes are inside the outer box. 
Due to space limits, we do not show “1” for this 
outer box. 
After recovering the simplified Cangjie code 
for a character, we can associate the character with 
a tag that indicates the overall layout of its compo-
nents, and separate the code sequence of the char-
acter according to the layout of its components. 
Hence, the information about a character includes 
the tag for its layout and between one to three se-
quences of code elements. Table 2 shows the anno-
܍ ॕ ᖴࡿ
։ හ ဘ ୯Ӣ
1 1 2 1 2 3
1
2 3 3
2
1
1
2
3
2
2
1
1
2
3
Figure 4. Arrangements of components in Chinese 
 Layout Part 1 Part 2 Part 3
܍ 1 ψψЋΓ! ! !
ॕ 2 εД! ψύ! !
ࡿ 3 В! νԮ! α!
ᖴ 4 Ν΋΋α! ԮᜤԮ! ЕЉ!
։ 5 νε! α! !
හ 6 Е! Е! Е!
ဘ 7 Ѕ! Е΋! Ћ!
Ӣ 8 Җ! ε! !
୯ 9 Җ! Љ! α΋!
ᓍ 2 ΋ζζ΋! ΋Дξߎ! !
৩ 2 ԮΓ! ΋ζζ΋! !
঩ 5 α! Дξߎ! !
༝ 9 Җ! α! Дξߎ
࣬ 2 Е! Дξ! !
གྷ 5 ЕДξ! Ј! !
ጃ 6 Ԯ! Е! Дξ!
Table 2. Annotated and expanded code
95
tated and expanded codes of the sample characters 
in Figure 4 and the codes for some characters that 
we will discuss. The layouts are numbered from 
left to right and from top to bottom in Figure 4. 
Elements that do not belong to the original Canjie 
codes of the characters are shown in smaller font.  
Recovering the elements that were dropped 
out by the Cangjie method and organizing the sub-
sequences of elements into parts facilitate the iden-
tification of similar characters. It is now easier to 
find that the character (ᓍ) that is represented by 
“΋ζζ΋” and “΋Дξߎ” looks similar to the 
character (৩) that is represented by “ԮΓ” and 
“΋ζζ΋” in our database than using their origi-
nal Cangjie codes in Table 1. Checking the codes 
for “঩” and “༝” in Table 1 and Table 2 will offer 
an additional support for our design decisions. 
In the worst case, we have to compare nine 
pairs of code sequences for two characters that 
both have three components. Since we do not sim-
plify codes for components and all components 
have no more than five elements, conducting the 
comparisons operations are simple. 
2.4 Drawbacks of Using the Cangjie Codes 
Using the Cangjie codes as the basis for comparing 
the similarity between characters introduces some 
potential problems.  
It appears that the Cangjie codes for some 
characters, particular those simple ones, were not 
assigned without ambiguous principles. Relying on 
Cangjie codes to compute the similarity between 
such characters can be difficult. For instance, “ϩ”
uses the fifth layout, but “ի” uses the first layout 
in Figure 4.!The first section in Table 1 shows the 
Cangjie codes for some character pairs that are dif-
ficult to compare.
Due to the design of the Cangjie codes, there 
can be at most one component at the left hand side 
and at most one component at the top in the layouts. 
The last three entries in Table 2 provide an exam-
ple for these constraints. As a standalone character, 
“࣬” uses the second layout. Like the standalone 
“࣬”, the “࣬” in “ጃ” was divided into two parts. 
However, in “གྷ”,  “࣬” is treated as an individual 
component because it is on top of “གྷ”. Similar 
problems may occur elsewhere, e.g., “හก” and 
“ৱӢ”. There are also some exceptional cases; e.g., 
“ࠔ” uses the sixth layout, but “ᗥ” uses the fifth 
layout. 
3 Concluding Remarks 
We adopt the Cangjie alphabet to encode Chinese 
characters, but choose not to simplify the code se-
quences, and annotate the characters with the lay-
out information of their components. The resulting 
method is not perfect, but allows us to find visually 
similar characters more efficient than employing 
the image-based methods.  
Trying to find conceptually similar but con-
textually inappropriate characters should be a natu-
ral step after being able to find characters that have 
similar pronunciations and that are visually similar. 
Acknowledgments 
Work reported in this paper was supported in part 
by the plan NSC-95-2221-E-004-013-MY2 from 
the National Science Council and in part by the 
plan ATU-NCCU-96H061 from the Ministry of 
Education of Taiwan. 
References  
Jill Burstein and Claudia Leacock. editors. 2005. Pro-
ceedings of the Second Workshop on Building Educa-
tional Applications Using NLP, ACL. 
Matthew Y. Chen. 2000. Tone Sandhi: Patterns across 
Chinese Dialects. (Cambridge. Studies in Linguistics 
92.) Cambridge: Cambridge University Press. 
Bong-Foo Chu. 2008. Handbook of the Fifth Generation 
of the Cangjie Input Method, web version, available 
at http://www.cbflabs.com/book/ocj5/ocj5/index.html. 
Last visited on 14 Mar. 2008. 
Hsiang Lee. 2008. Cangjie Input Methods in 30 Days,
http://input.foruto.com/cjdict/Search_1.php, Foruto 
Company, Hong Kong. Last visited on 14 Mar. 2008. 
Derming Juang, Jenq-Haur Wang, Chen-Yu Lai, Ching-
Chun Hsieh, Lee-Feng Chien, and Jan-Ming Ho. 
2005. Resolving the unencoded character problem for 
Chinese digital libraries. Proceedings of the Fifth 
ACM/IEEE Joint Conference on Digital Libraries,
311–319. 
Marcus Taft, Xiaoping Zhu, and Danling Peng. 1999. 
Positional specificity of radicals in Chinese character 
recognition, Journal of Memory and Language, 40,
498–519. 
Su-Ling Yeh and Jing-Ling Li. 2002. Role of structure 
and component in judgments of visual similarity of 
Chinese characters, Journal of Experimental Psy-
chology: Human Perception and Performance, 28(4), 
933–947. 
96
  
ᑑಖᇠᑇڗΔ೗ڕڶࠟଡאՂऱᑇڗല CSխ່՛ࡉ່Օऱ STCᑑಖڇ n[/STC/]ՂΔ௑
ڤ੡ n[/່՛-່Օ/]Ζ 
೗ڕݺଚ෼ڇ૞ᑑಖቹնรԫᐋऱᆏរ VPΔঞؘᏁലᆏរ VP ऱ՗ᆏរ VBD ࡉ
ADJPऱ VBD[3//]֗ ADJP[4-6//]խऱ STREEףԵ ESխΔڂڼ ESץܶԱ 3Ε4ࡉ 6Կ
ଡᑇڗΔࢬא VP[STREE//]խऱ STREEᑑಖ੡ 3-6Ζ൷ထᑑಖ STCΔലᆏរ VPऱ՗ᆏ
រ VBDࡉ ADJPऱ VBD[3/0/]֗ ADJP[4-6/4-6/]խऱ STCףԵ CSխΔڂ੡ 0լᄎ๯ף
Ե CSխΔڂڼ CS׽ڶ 4ࡉ 6ࠟଡᑇڗΔࢬא VP[3-6/STC/]խऱ STCᑑಖ੡ 4-6Ζ 
່৵Δᖞལଳ࣫ᖫऱ STREEᇿ STCຟբᆖᑑಖݙګΔڕቹնΔ׽ໍՀ ORDERᝫ
޲ᑑಖՂΖORDER๠෻ֱڤ։੡ࠟຝ։ΔรԫຝٝΚSTC੡ 0հכݬᆏរਊᅃطؐ۟
׳ऱႉݧᒳᇆΙรԲຝ։Κֺለ n ፖ STCॺ 0հכݬᆏរऱՕ՛Δࠀ൷ڇรԫຝٝऱ
ᒳᇆ৵Δط՛ࠩՕᤉᥛᑑಖᒳᇆΖࠏڕቹնૉ૞ᑑಖ JJ[4/6/ORDER]ࡉ PP[5-6/4/ORDER]
ऱ ORDERΔঞല JJ[4/STC/]խऱ STC=6ࡉ PP[5-6/STC/]խऱ STC=4ط՛ඈࠩՕΔࢬא
PP[5-6/4/ORDER]խऱ ORDERᑑಖ੡ 1ΔJJ[4/6/ ORDER] խऱ ORDERᑑಖ੡ 2Ζ 
ܓشՂ૪ऱֱऄ൓ࠩᒤࠏᖫΔڕቹԿΖڕऴ൷شᖞଡ؁՗ऱᒤࠏᖫࠩᇷற஄խ܂ჼ
༈Δലৰᣄჼ༈ࠩઌٵऱᒤࠏᖫΔڂ੡؁՗။९؁՗ऱ࿨ዌᄎ။ᓤᠧΔࢬאઌٵ࿨ዌऱ
؁՗ૹᓤנ෼ऱױ౨ৰ܅ΖڂڼΔݺଚലᒤࠏᖫऱࢬڶ՗ᖫ։ܑ࠷נࠐΔޢԫଡ՗ᖫࢬ
ץܶऱᒤ໮ऱຟਢ૎֮؁ऱ՗؁Δڇլٵऱ؁՗ᇙױ౨ᄎڶઌٵ࿨ዌऱ՗؁Δլ܀ױא
ᏺףֺኙࠩऱᖲ෷ΔՈ౨ᏺףᒤࠏᖫऱᑇၦΖ່৵ಖᙕڇᒤࠏᖫᇷற஄ऱփ୲Δ׽ڶᒤ
ࠏᖫࡉ ORDER೶ᑇΖSTREEࡉ STCլᏁಖᙕऱ଺ڂਢޢԫଡ؁՗ऱޢଡဲნຟڇլ
ٵऱۯᆜՂΔঞڇᇷற஄խլᏁ૞ಖᙕ STREEࡉ STCΖ 
ᒤࠏᖫऱ࿨ዌڶױ౨ઌٵΔۖဲݧլٵΖࠏڕ³NP(NP(NN fork))(PP(IN of)(NP(DT 
the)(NN road)))´Δխ֮៬᤟੡³ݣሁ´Δۖ³NP(NP(NN leader))(PP(IN of)(NP(DT a)(NN 
company)))´Δ խ֮៬᤟੡³ԫၴֆ׹ऱᏆᖄृ´Ζৰࣔ᧩৵ृխ૎֮شဲႉݧլٵΖຍ
ᇙݺଚආشڍᑇެΔലנ෼መઌٵᒤࠏᖫ࿨ዌऱޢጟဲݧ܂อૠΔڇᒤࠏᖫᇷற஄խಖ
ᙕנ෼່ڍڻဲݧऱ࿨ዌΖڕנ෼່ڍڻऱڻᑇઌٵΔঞאᙟᖲֱڤᙇᖗԫጟಖᙕڇᒤ
ࠏᖫᇷற஄խΖ່৵٦ലᒤࠏᖫᇷற஄խ޲ڶဲݧٌངऱᒤࠏᖫܔೈΔ׽অఎڶဲݧٌ
ངऱᒤࠏᖫΔױא྇֟ჼ༈ઌٵᒤࠏᖫऱழၴΖ 
3.3 ჼ༈ઌٵᒤࠏᖫ 
ᒤࠏᖫᇷற஄ᇙΔޢԫ࿝ᇷறຟץܶᒤࠏᖫࡉᒤࠏᖫऱ ORDERΔۖᒤࠏᖫ༉ਢشࠐᅝ
܂ᓳᖞဲݧऱ೶ەΖലᙁԵऱ૎֮؁Δ٣ຘመ StanfordLexParser-1.6[17]৬مଳ࣫ᖫΔ٦
ലଳ࣫ᖫխװൾᆺ՗ᆏរऱ࿨ዌΔࠩᒤࠏᖫᇷற஄װჼ༈ਢܡڶઌٵ࿨ዌऱᒤࠏᖫΔຍ
ᇙݺଚലࢬჼ༈ࠩઌٵऱᒤࠏᖫጠ੡֐಻՗ᖫΖڕቹքࢬقΔદۥဠᒵ௃ਢԫལ՗ᖫࠡ
࿨ዌ੡³(NP(NP(DT)(NNS))(PP(IN)(NP(CD)(NNS))))´Δֱݮ௃੡ᒤࠏᖫᇷற஄խࠡխԫ
ལᒤࠏᖫ࿨ዌ੡ ³(NP(NP[//2](DT[//1]) (NNS[//2])) (PP[//1](IN[//1]) (NP[//2](CD[//1]) 
(NNS[//2])))) Δ´ݺଚױא࿇෼ᒤࠏᖫװೈORDER৵ऱ࿨ዌΔᄎᇿ՗ᖫऱ࿨ዌݙ٤ઌٵΔ
ਚലڼᒤࠏᖫᎁࡳ੡֐಻՗ᖫΖ 
 
 
௅ᖕჼ༈ᒤࠏᖫዝጩऄऱੌ࿓ΔڕቹԮΖଈ٣ലࠐᄭ؁ऱଳ࣫ᖫףࠩ۱٨(queue)
ᇙΔൕ۱٨ᇙ૿࠷נԫལଳ࣫ᖫࠩᒤࠏᖫᇷற஄խΔჼ༈ਢܡڶઌٵ࿨ዌऱᒤࠏᖫΙڕ
੡ܡΔঞലڼལᖫऱՀԫᐋऱ՗ᖫףԵ۱٨ΔףԵ۱٨ऱႉݧ੡ؐ՗ᖫࠩ׳՗ᖫΙڕ੡
ਢΔঞലᇠᖫऱ ORDERᑑಖڇࠐᄭ؁ऱଳ࣫ᖫՂΔᤉᥛ࠷נ۱٨փऱଳ࣫ᖫΔऴࠩ۱
٨ᇙ޲ڶଳ࣫ᖫ੡ַΖࢬאࠐᄭ؁ऱଳ࣫ᖫਢطԫଡאՂऱ֐಻՗ᖫࢬิګΖ 
 
ᙁԵΚࠐᄭ؁ଳ࣫ᖫ S 
      ᒤࠏᖫᇷற஄ D={D1, D2, ..., Dm}ΔDi㺃D 
         Diץܶ Tiፖ Oi Δi੡ 1ࠩ m 
Tiਢร iལᒤࠏᖫΔOiਢ Tiࢬᑑಖऱဲݧ 
ၲࡨ  
      ๻۱٨ Qشࠐᚏژଳ࣫ᖫΔॣࡨ੡ NULL 
      SףԵ Q  
      ᅝ QЋNULL 
          ൕ Qխ popԫལᒤࠏᖫ 
          ڕ࣠ڇ Dխჼ༈ࠩઌٵऱᒤࠏᖫ Ti 
            ঞല Oiᑑಖڇ SՂ 
          ܡঞലՀԫᐋ՗ᖫףԵ Q 
࿨ޔ 
ᙁנΚᑑಖړ ORDERऱଳ࣫ᖫ 
ቹԮΕჼ༈ᒤࠏᖫዝጩऄ 
ቹք੡ଳ࣫ᖫჼ༈ᒤࠏᖫऱൣݮΖࠐᄭ؁Κ³The graph shows the heights of four girls´Δ
ଳ ࣫ ᖫ ੡ ³(S(NP(DT The)(NN graph))(VP(VBZ shows)(NP(NP(DT the)(NNS 
 
ቹքΕଳ࣫ᖫፖᒤࠏᖫऱኙᚨᣂএ 
 
 
heights))(PP(IN of)(NP(CD four)(NNS girls)))))´Ζຘመჼ༈ᒤࠏᖫዝጩऄބנ֐಻՗ᖫΔ
ଈ٣אᆏរ S੡ᖫ௅ऱଳ࣫ᖫࠩᇷற஄܂ჼ༈Δჼ༈ழլץܶᆺ՗ᆏរΔڼࠏ՗޲ჼ༈
ࠩ֐಻՗ᖫΔঞലᆏរ Sऱ՗ᖫ NPࡉ VPףԵ۱٨խΖ൷Հࠐലൕ۱٨խ࠷נऱ՗ᖫ
੡ NPΔࠩᒤࠏᖫᇷற஄ჼ༈֐಻՗ᖫΔ܀ᇷற஄խ޲ڶઌٵऱᒤࠏᖫΔڼழ NP ऱ՗
ᖫઃ੡ᆺ՗ᆏរΔࢬאࠀྤ՗ᖫڇףԵ۱٨խΖࠉᅃ٣ၞ٣נऱ଺ঞՀԫଡൕ۱٨࠷נ
ऱਢ S ऱ׳՗ᖫ VPΔڇᒤࠏᖫᇷற஄խᝫਢჼ༈լࠩΔڂڼ૞ല VP ऱ՗ᖫ VBZ ࡉ
NPףԵ۱٨խΔ܀ VBZ੡ᆺ՗ᆏរΔਚ׽ڶ NPףԵ۱٨խΖ൷Հࠐਢ՗ᖫ NPൕ۱
٨խ๯࠷נࠐΔ՗ᖫ NPڇᇷற஄խჼ༈ࠩઌٵऱᒤࠏᖫΔڕቹքऱᒤࠏᖫ༉ਢࢬჼ༈
ࠩऱ֐಻՗ᖫΔڂڼലᒤࠏᖫऱ ORDERᑑಖՂװΔᑑಖ৵ऱଳ࣫ᖫലڕቹԶࢬقΖڼ
ழ۱٨խբᆖ੡़Δჼ༈ᒤࠏᖫऱੌ࿓ࠩڼ੡ַΖ 
ᑑಖݙ ORDERհ৵Δല޲ڶᑑಖऱ՗ᖫ܂ଥ೪ΔՈ༉ਢലլش܂ဲݧٌངऱ՗ᖫ
ଥ೪່ࠩ՛ᐋᖫΖڕቹԶᆏរ Sऱ׳՗ᖫΕNP[2]ࡉ NP[1]ऱ՗ᖫઃլᏁ૞܂ဲݧٌངΔ
ڂڼଥ೪ऱ࿨࣠੡³(S(NP The graph)(VP(VBZ shows)(NP(NP[2] the heights)(PP[1](IN[2] 
of)(NP[1] four girls))))) ´Δڕቹ԰ࢬقΖ່৵ൕᐋᑇ່Օऱޢଡכݬᆏរၲࡨດᐋ࢓Ղ
ࠉᅃᚌ٣ᦞႉݧᓳᖞଳ࣫ᖫऱ࿨ዌΙᓳᖞ৵ऱ࿨࣠ലᄎᙁԵࠩ៬᤟ᑓิขس៬᤟Ζૉݺ
ଚऴ൷࠷ࠐᄭ؁ଳ࣫ᖫऱᆺ՗ᆏរ܂៬᤟Δലᄎګ੡໢ڗڤऱ៬᤟Δݺଚലྤऄኙဲิ
ࢨׂ፿܂៬᤟Ζ៬᤟ऱຝ։ᄎڇՀԫᆏᄎ܂ᇡาᎅࣔΖ 
ቹ԰ऱଳ࣫ᖫڶ؄ᐋΔଈ٣ലร؄ᐋऱכݬᆏរ³(IN[2] of)(NP[1] four girls)´Δࠉᅃ
ORDERऱႉݧᓳᖞ৵ऱႉݧ੡³(NP[1] four girls) (IN[2] of) Δ´൷ՀࠐรԿᐋऱכݬᆏរ
³(NP [2] the heights)(PP[1] (NP[1] four girls)(IN[2] of))´ٌང৵ऱႉݧ੡³(PP[1] (NP[1] 
four girls)(IN[2] of)) (NP [2] the heights)Δڼࠏ՗൷Հࠐဲݧ޲ڶ٦ᓳ೯ΔڕቹԼࢬقΙ
່৵ᙁԵ៬᤟ᑓิऱႉݧ੡³The graph´Ε³shows´Ε³four girls´Ε³of´Ε³the heights´Δط
ڼႉݧ։ܑ܂៬᤟๠෻Ζ 
ቹԶΕݙګ ORDERᑑಖ ቹ԰Εଳ࣫ᖫଥ೪৵ऱ࿨࣠ 
 
 
 
ቹԼΕᓳᖞဲݧ৵ऱ࿨࣠ 
3.4 ៬᤟๠෻ 
ᆖመՂԫᆏ๠෻່৵൓ࠩଥ೪ᖫΔଥ೪ᖫऱᆺ՗ᆏរױ౨੡૎֮໢ڗ(word)Εဲิ(term)Ζ
ဲิܛ੡ᑇଡ໢ڗ࿨ٽऱڗۭΔլԫࡳ੡ݙᖞऱ؁՗Δڕ³would be left on the floor´ࢨׂ
፿(phraseΔڕټဲׂ፿Ε೯ဲׂ፿Εݮ୲ဲׂ፿࿛) Δڕ³in order to´Ζڇ៬᤟๠෻Ղᄎ
ሖࠩ૎֮໢ڗࢨဲิΔڇ૎֮໢ڗऱຝ։Δऴ൷਷༈ڗࠢᚾ܂៬᤟Ιဲิऱຝ։ܓش๵
ঞဲࠢᚾऱׂ፿Δࡉဲิၞ۩ڗֺۭኙΔאބנฤٽऱׂ፿֗խ֮៬᤟ΖאՀ੡ڗࠢᚾ
֗๵ঞဲࠢᚾ։ႈᎅࣔΖ 
ڗࠢᚾΚڗࠢᚾຝ։ݺଚࠌش Concise Oxford English Dictionary[8](੍ׄ෼ז૎ዧᠨᇞဲ
ࠢΔگᙕ 39429 ଡဲნ)Δലছ๠෻መ৵ऱ૎֮໢ڗࢨׂ፿೚៬᤟ኙ࿛ڗჼ༈ऱ೯܂Δ
ބנࢬڶࡉᇠ૎֮໢ڗऱխ֮ဲิΔ܂੡៬᤟ऱଢᙇټ໢Ζڕྤऄڇڗࠢᚾխჼ༈ࠩኙ
ᚨऱխ֮៬᤟ΖڕࡩټࡉറڶټဲΔঞऴ൷ᙁנᇠ૎֮ڗΖ 
๵ঞဲࠢᚾΚ੡ൄشऱټဲׂ፿Ε೯ဲׂ፿Εݮ୲ဲׂ፿࿛ဲิΔא֗ᇢᠲ៬᤟՛ิࢬ
ެᤜհอԫ៬᤟ဲิאԳՠऱֱڤ৬مऱխ૎៬᤟ኙᅃᚾΔڕ in order to(੡Ա)Ζ 
     ։ګ໢ڗࡉဲิ៬᤟ਢڂ੡ૉڇ๵ঞဲࠢᚾֺኙլࠩΔঞش़ػࠐ೚ԫ౳ڗࡉڗ
հၴऱឰဲΔՈ༉᧢ګ໢ڗऱ៬᤟Δڂ੡ဲิለ౨ݙᖞ।෼נ೯܂ࢨඖ૪Ζڕ׽ش໢ڗ
܂៬᤟Δᄎທګ៬᤟ՂऱᙑᎄΖ൫ႊࣹრऱਢֺኙऱ؁ীૉڶઌۿ࿨ዌ܀լٵ९৫ऱڗ
ۭᑌڤΔঞ࠷९৫່९ऱ੡࿨࣠Ζڕԫ૎֮؁՗੡³Ξas shown in diagramΞ´Δٵழየ
ߩ๵ঞဲࠢᚾփऱ³as shown in diagram´ࡉ³in diagram´ׂ፿؁ীΔঞݺଚᄎᙇᖗ९৫ለ
९ऱ³as shown in diagram´ۖլਢᙇᖗ³in diagram´ףՂ³as show´܂੡ឰဲऱ࿨࣠Ζ 
    ڇ૎֮៬᤟ګխ֮ऱመ࿓խΔڶࠄ૎֮໢ڗլᏁ૞៬᤟ࢨਢྤრᆠऱൣݮΔࢬאݺ
ଚലຍࠄ໢ڗመៀլ៬᤟Δຍࠄ໢ڗጠ੡ stop wordΖࠏڕΚগဲ theऴ൷װೈΖտߓဲ
forΕtoΕof࿛Δૉছԫ໢ڗ੡ whatΕhowΕwhoΕwhenΕwhy࿛ጊംဲΔঞւאܔೈΔ
׼؆Δtoנ෼ڇ؁ଈऴ൷ܔೈΖܗ೯ဲ doΕdoes࿛ΔܒឰֱڤፖտߓဲઌٵΖ 
     
 
 
ڇ៬᤟መ࿓խᝫױ౨נ෼ဲი᧢֏(ڕ~ingΕ~ed࿛)ࡉဲࢤ᧢֏(ڕ೯ဲ breakΔࠡመ
װڤ੡ brokeΔ๯೯ڤ੡ brokenΔא֗ټဲ໢ᓤᑇীኪ)Ζဲი᧢֏ऱຝٝΔݺଚܓش
Porter[22]ዝጩऄᝫ଺ٺဲࢤ(ټဲΕ೯ဲΕݮ୲ဲΕ೫ဲ)Ιဲࢤ᧢֏ऱຝ։Δڶࠄਢլ
๵ঞऱ᧢֏Δለᣄشዝጩऄ๠෻ΖڂڼΔݺଚຘመMXPOST[14]ဲࢤᑑಖՠࠠല໢ڗף
ԵᑑಖΔ٦ܓشWordNet[23]ࠉᅃဲࢤ೚ڗࠢᚾჼ༈ބࠩ଺ࡨऱীኪΖ 
3.5 อૠڤᑓิᙇဲ 
ءߓอല૎֮ဲნܓشՂԫᆏտฯऱ៬᤟ֱڤΔ਷ᇬဲࠢބנࢬڶױ౨ᔞٽ૎֮ဲნऱ
៬᤟࿨࣠Δ٦ܓشอૠڤᑓิބנ່ڶױ౨ऱխ֮ဲნΔڼຝ։բᆖڶܨࣲࣔ࿛ᖂृൕ
ࠃຍԫႈઔߒՠ܂[3]ΖאՀ੡ݺଚଥޏ৵ऱᖲ෷ᑓীΖ 
argmax
ܥ1,݊
Pr൫ܥ1,݊หܧ1,݊൯ = argmax
ܥ1,݊
ෑሾPrሺܧ݅ȁܥ݅ሻPrሺܥ݅ȁܥ݅െ1ሻሿ
݊
݅=1  
(1) 
    ֆڤ(1)խࡳᆠ C ੡խ֮៬᤟ဲნΔE ੡૎֮ဲნΔ ܧ1,݊੡૎֮؁ڶ 1 ࠩ n ଡ૎֮
ဲნΔխ֮៬᤟ဲნՈᄎڶ 1ࠩ nଡΔܛ ܥ1,݊Ζൕֆڤխױ࿇෼խ֮ဲნ៬᤟ګ૎֮ဲ
ნऱᖲ෷Δጠ੡խ૎ဲნኙ٨ΔܛPr(ܧ݅|ܥ݅)Ιא֗ܓشছԫଡխ֮៬᤟ᙇဲऱ࿨࣠ ܥ݅െ1Δ
ބנؾছխ֮៬᤟ဲნ ܥ݅٥ٵנ෼ऱᖲ෷Δጠ੡ bi-gram ፿ߢᑓীΔܛPr(ܥ݅|ܥ݅െ1)Δല
ࠟृઌଊ࠷ૠጩ৵່Օऱᖲ෷ଖΔא२ۿPr(ܥ1,݊ |ܧ1,݊)ऱᖲ෷ଖΔ܂੡ࢬᙇᖗऱխ֮៬
᤟ဲნΖڇᙇဲऱመ࿓խΔPr(ܧ݅|ܥ݅)ፖPr(ܥ݅|ܥ݅െ1)ऱᖲ෷ଖઃڶױ౨੡ 0Δݺଚലଊ 0
ངګଊՂԫଡᄕ՛ᑇ(ݺଚቃ๻੡ 10-6) Δ੡Աᝩ܍ᖲ෷ଖ੡ 0ऱൣݮΔᄎᐙ᥼ᙇဲऱ࿨
࣠ΖאՀലಾኙխ૎ဲნኙ٨ࡉ bi-gramᑓীᇡาտฯΖ 
խ૎ဲნኙ٨Κലխ૎፿றᠨ፿፿றΔᆖመԳՠऱխ૎፿؁ኙ٨(sentence alignment)ݾ
๬Δ൷ထലխ֮፿றܓشխઔೃ CKIPឰဲߓอ[1]ףאឰဲΙ૎֮፿றঞਢᆖመՕ՛ᐊ
᠏ང֗ܓشڗࡉڗհၴ़ػឰဲΔ່৵ᙁԵ۟ GIZA++[16]֗ mkcls[15]࿛ՠࠠΔขسխ
૎ဲნኙ٨࿨࣠א֗խ૎ဲნኙᅃᖲ෷।Ζ 
bi-gram፿ߢᑓীΚലխ֮፿றอૠٺխ֮ဲნࡉՀԫଡխ֮ဲნנ෼ऱڻᑇΔૠጩࠡ
נ෼ᖲ෷Ζݺଚਢܓش SRI Speech Technology and Research Laboratory ࢬၲ࿇ऱ۞ྥ፿
ߢՠࠠ SRILM[18]ࠐ৬م bi-gram፿ߢᑓীΖ 
4. ߓอ៬᤟ய࣠ေ۷ 
ءᆏ׌૞տฯܓشءߓอ៬᤟ഏᎾᑇᖂፖઝᖂඒߛګ༉᝟Ⴈᓳ਷ 2003 ڣەᠲΔ១ጠ
TIMSS2003Δࠀലᇢᠲࠉᅃڣ᤿ܑࡉઝؾܑΔ։ֺܑለ៬᤟ऱ঴ᔆΖ່৵ലፖᒵՂ៬᤟
א֗ܨࣲࣔ࿛ᖂृઔ࿇ऱ៬᤟ߓอ܂ֺለΖေ۷ֱڤ੡ܓش BLEU ֗ NIST ਐᑑΖ 
4.1 ኔ᧭ࠐᄭ 
ݺଚ׌૞شࠐ៬᤟ऱࠐᄭ੡ TIMSS2003 ᇢᠲΔ೴։ᑇᖂ֗ઝᖂᣊܑΔࠀ׊א؄ڣ్֗
Զڣ్੡ەᇢኙွΔ٥ڶ؄ጟᇢᠲ։ܑ੡؄ڣ్ᑇᖂᏆ഑ 31 ᠲΙ؄ڣ్ઝᖂᏆ഑ 70
ᠲΙԶڣ్ᑇᖂᏆ഑ 41ᠲΙԶڣ్ઝᖂᏆ഑ 38ᠲΖࢬڶᇢᠲຟڶ૎֮଺֮ᇢᠲࡉஃՕ
ઝඒխ֨ࢬ៬᤟ऱխ֮ᇢᠲΖ 
 
 
    ࢬڶኔ᧭፿ற؁ኙᑇΕխ૎ဲნᑇΕխ૎᜔ဲნଡᑇ֗ؓ݁؁९Δઃڕ।ԫࢬقΖ
شࠐ৬مᒤࠏᖫऱࠐᄭڶඒߛຝࡡಜࡵᥞᗼ৬ᆜ፿֮ᖂ฾Ꮖ഑ഏխඒઝ஼ᇖךᇷறᠲ
஄[4] (אՀ១ጠഏխᇖךᇷறᠲ஄)֗ઝᖂԳᠧ፾Ζഏխᇖךᇷறᠲ஄אԳՠֱڤݙګխ
૎፿؁ኙ٨(sentence alignment)Δ٦ᆖመᒤࠏᖫऱᗴᙇ॰ាଖ੡ 0.6ऱൣउՀڶ 565؁Ζ 
    شࠐಝᒭᙇဲᖲ෷ᑓীऱࠐᄭڶ۞طழ໴խ૎ኙᅃᦰᄅፊ֗ઝᖂԳᠧ፾Ζ۞طழ໴
խ૎ኙᅃᦰᄅፊൕ 2005ڣ 2ִ 14ֲ۟ 2007ڣ 10ִ 31ֲΔۖ۞طழ໴խ૎ኙᅃᦰᄅ
ፊءߪ༉բᆖ܂ړխ૎፿؁ኙ٨ΖઝᖂԳᠧ፾ਢൕ 2002ڣ 3ִ໌עᇆ۟ 2006ڣ 12ִ
٥ 110 ᒧ੡፿றࠐᄭΖ 
।ԫΕኔ᧭፿றࠐᄭอૠ 
፿ற ፿ߢ ؁ኙᑇ ᢯ნᑇ ᜔ဲნଡᑇ(tokens) ؓ݁؁९ 
ഏխᇖךᇷறᠲ஄ խ֮ 2059؁ 2333 12460 6.1 
૎֮ 2887 13170 6.4 
ઝᖂԳ խ֮ 4247؁ 9279 70411 16.6 
૎֮ 10504 68434 16.1 
۞طழ໴խ૎ኙ 
ᅃᦰᄅፊ 
խ֮ 4248؁ 19188 145336 34.2 
૎֮ 25782 133123 31.3 
4.2 ኔ᧭๻ૠ 
ଈ٣Δല TIMSS2003 ᇢᠲം؁אຐᇆΕംᇆࢨ᧫ቮᇆ೚੡ឰ؁ऱ໢ۯΔޢଡᎈ࿠ᙇႈ
೚੡ឰ؁ऱ໢ۯΔૉԫሐᠲؾ੡ԫ؁ᇢᠲം؁֗؄ႈᎈ࿠ᙇႈࢬิګΔঞԫሐᠲؾױឰ
נն؁ΖᆖመԳՠឰ؁๠෻ TIMSS2003ᇢᠲΔ؄ڣ్ᑇᖂᏆ഑ڶ 165؁Ι؄ڣ్ઝᖂ
Ꮖ഑ڶ 262؁ΙԶڣ్ᑇᖂᏆ഑ڶ 439؁ΙԶڣ్ઝᖂᏆ഑ڶ 236؁Δࠀᖞ෻੡֮ڗᚾΖ
៬᤟ழխ֮ᇢᠲࢬሎشऱխ֮ឰဲ੡խઔೃ CKIPឰဲߓอ[1]Δ૎֮ᇢᠲࢬሎشऱଳ࣫
ᕴ੡ StanfordLexParser-1.6[17]Δ৬مᒤࠏᖫᇷற஄ࢬࠌشऱ፿ற੡ഏխᇖךᇷறᠲ஄Δ
ಝᒭᖲ෷ᑓীࢬࠌشऱ፿ற۞طழ໴խ૎ኙᅃᦰᄅፊףՂઝᖂԳᠧ፾Δࠡ խಝᒭ፿ߢᑓ
ী൓ࠩऱ bi-gram٥ڶ 134435ଡΙGIZA++ขسխ૎ဲნኙ٨࿨࣠ڶ 128551ิΖ 
।ԲΕTIMSSᇢᠲኔ᧭ิܑ। 
Զڣ్ 2003 M
ิ 
Զڣ్2003 Sิ ؄ڣ్ 2003 M
ิ 
؄ڣ్  2003 S
ิ 
Զڣ్ 2003 MS
ิ 
؄ڣ్ 2003 MS
ิ 
TIMSS2003ഏխ
ᑇᖂᏆ഑ᇢᠲ 
TIMSS2003ഏխ
ઝᖂᏆ഑ᇢᠲ 
TIMSS2003ഏ՛
ᑇᖂᏆ഑ᇢᠲ 
TIMSS2003ഏ՛
ઝᖂᏆ഑ᇢᠲ 
TIMSS2003ഏխ
ᑇᖂ֗ઝᖂᏆ഑
ᇢᠲ 
TIMSS200 ഏ՛
ᑇᖂ֗ઝᖂᏆ഑
ᇢᠲ 
    ݺଚေ۷ࢬࠌشऱՠࠠ੡ࠉᅃ BLEU ֗ NIST ᑑᄷऱ mteval-10Δࠀ׊ݺଚല೶ە
ऱխ֮ᑑᄷ៬᤟ࡉߓอ৬ᤜ៬᤟Δޢଡխ֮ڗᇿխ֮ڗհၴش़ػ܂։ሶΔૠጩנٺܑ
n-gram֗ีףٺଡ n-gramऱ BLEU֗ NISTଖΖ׌૞ေ۷ऱኙွڶ GoogleᒵՂ៬᤟Ε
Yahoo!ᒵՂ៬᤟Εܨࣲࣔᖂृऱߓอ(Lu)֗ءߓอյઌ೚ֺለΔࠀ׊ေ۷៬᤟ߓอڇլ
ٵڣ్ऱᇢᠲփ୲ՂΔ៬᤟঴ᔆਢܡᄎਊᅃ။܅ڣ్ࠡ៬᤟঴ᔆ။ړऱ᝟ႨΖڂڼΔݺ
ଚലኔ᧭ิܑ։੡Զڣ్ࡉ؄ڣ్ΙᑇᖂᏆ഑א M੡זᇆΙઝᖂᏆ഑א S੡זᇆΔᅝ
                                                 
ءᒧᓵ֮ TIMSSᇢᠲኔ᧭ิΔႛץܶ 2003ڣᇢᠲΔፖܨࣲࣔᖂृऱኔ᧭ิࠀլઌٵΖ 
ນ೜Ўਜᔠ઩س಍ 
ᙔৎؼ ᒘ௵๮ ҖٱЎ ቅࡿᡕ 
୯ҥࡹݯεᏢၗૻࣽᏢس 
{ g9542, g9523, g9627, chaolin }@cs.nccu.edu.tw 
 
ᄔा 
ӧນ೜Ўਜᔠ઩ሡ؃΢Ǵჹܭݤ۔ԶقǴ໪ाפډ
࣬՟ޑਢٯᇶշղ،ǹჹܭᏢಞ܈ࣴزݤᏢޑΓٰ
ᇥǴᙖҗᔠ઩εໆਢٯǴҔаϩ݋௖૸࣬ᜢ᝼ᚒǹ
Զ΋૓҇౲Ǵ߾ёᙖҗჴሞਢٯǴٰ֎ԏݤࡓ΢ޑ
୷ҁޕ᛽ǴҔаߥምԾي៾੻Ƕ 
ນ೜ਢҹᆶВॿቚǴట᎙᠐ֹ܌Ԗਢٯᡉฅό
৒ܰǴԜਔߡሡा΋঺ၨֹ๓ޑᔠ઩س಍ٰᇶշ٬
ҔޣǶךॺճҔ౜ԖԾฅᇟقೀ౛ᆶЎӷ௖୎฻מ
ೌǴ೛ी΋঺ϩᜪԄᔠ઩س಍Ǵ٩ᔠ઩చҹཛྷ൨࣬
ᜢਢٯǴ٠ஒ่݀ϩᜪᒡрǴБߡ٬ҔޣჹӚᜪձ
຾Չࢗ၌Ǵаය෧Ͽ٬Ҕޣ᎙᠐Ўҹ΢ޑॄᏼǴӕ
ਔᕇளၨֹ᏾ၗૻǶќ೛ीЎҹ኱૶ᆶຏှфૈǴ
ٮ٬ҔޣࡌҥঁΓϯၗ਑৤ǴߡܭВࡕᔠ઩܈ᙖҗ
Ԝၗૻঅ҅Ծ୏ϩᜪᐒڋǶ 
 
ᜢᗖຒǺ 
ݤᏢၗૻس಍ǵΓπඵችᆶݤࡓǵ໘ቫԄϩဂ 
1 ᆣፕ 
߈൳ԃႝတޑ೬ฯᡏמೌِೲԋߏǴᆛሞᆛၡޑଯ
ࡋදϷǴ೚ӭၗૻ೿࿶җኧՏϯǴளаِೲวթ໺
ኞǴЪ٬ளᔠ઩ᡂளΜϩߡճǴӧݤᏢ࣬ᜢၗૻ΢
ҭࢂӵԜǶך୯ޑљݤଣݤᏢၗ਑ᔠ઩س಍[2]Ǵ൩
ගٮΑύѧᆶӦБޑݤೕࢗ၌ǴаϷљݤှញکݤ
ଣޑղٯǴ໒ܫ๏ε౲٬ҔǶ 
٩љݤଣ಍ीೀ[3]ၗ਑ᡉҢǴѠ᡼ӧ 2007 ԃ
ӦБݤଣӉ٣ਢҹಖ่ҹኧၲ 41 ࿤ҹǴ҇٣ਢҹ
׳ӭၲ 268 ࿤ҹǶ؂ϺऊԖ΋࿤ҹޑਢҹ೏ಖ่Ǵ
аѠ᡼ऊ 2300 ࿤Γαٰ՗ीǴѳ֡ऊ؂ԃ؂ΎΓ
൩Ԗ΋Γ໪΢ݤ৥Ǵӵ݀ѐ௞҂ԋԃکԴΓ฻ၨค
ҍ࿾ૈΚޑΓǴКٯ߾׳ଯǶ೭ኬޑ௃׎٠όж߄
ޗ཮ᕉნޑൾӍǴԶࢂᒿ๱௲ػНྗޑගଯǴၗૻ
ڗள৒ܰǴΓॺჹܭݤݯޑཷۺВ੻ԋዕǴཇٰཇ
ӭΓᔉளճҔљݤٰߥምঁΓޑ៾੻Ƕ 
ฅԶ೭ኬ೷ԋΑݤ۔ᆶࡓৣ฻Γ঩Ǵπբໆ΢
ޑཱུεॄᏼǶԜѦǴջߡдॺዕ᠐Ӛ໨ݤࡓచЎǴ
ՠӧय़ჹჴሞਢҹਔǴϝԖӚᅿόӕޑ௃׎໪ाय़
ჹǴ໪ाୖԵၸѐޑղ،ٰ،ۓǶᗨฅҁ࿯΋໒ۈ
ගډኧՏϯё٬ၗૻᔠ઩ᡂள৒ܰǴՠय़ჹᔠ઩่
݀ϝх֖ޑεໆၗૻǴ߾ϝࢂ΋ҹ઻ਔޑ٣Ƕ 
୷ܭ΢ॊޑ౛җǴҁࣴزట೛ी΋঺ϩᜪԄᔠ
઩س಍Ǵڐշ٬Ҕޣӧڗளεໆ่݀ਔǴૈ࿶җس
಍Ծ୏ϩᜪᐒڋǴаϩᜪࣁрวᗺᔠ઩ນ೜ЎҹǴ
ჹܭၨό࣬ᜢޑϩᜪǴ࿶җϿኧ൳ጇޑᔠٰࢗၢၸ
ԜϩᜪǴૈ෧Ͽᔠ઩ޑਔ໔ᆶቚ຾ᔠ઩ਏ౗Ƕ 
ӧಃ 2࿯ύǴךॺஒϟಏ࣬ᜢࣴز᝼ᚒǹௗ๱
ӧಃ 3 ࿯ϟಏҁس಍ޑ೛ीаϷᏹբϟय़ǹӧಃ
4 ࿯ࢂҁس಍܌٬Ҕډޑ࣬ᜢמೌ೽ϩǹಃ 5 ࿯բ
΋٤߃؁ޑຑ՗ǹനࡕಃ 6࿯բ่ᇟǶ 
2 ࣬ᜢࣴز 
Hearst[14]ගډཛྷ൨ၗ਑ޑΓ׆ఈૈԖϟय़ૈᔅд
ॺע่݀բϩᜪǴ٬ၗ਑׳ԖཀကЪ׳Бߡځբᔠ
઩Ǵזೲၸᘠό಄ӝሡ؃ޑၗૻǶբޣ૸ፕΑϩဂ 
(clustering) ᆶࡌҥӚঁय़ӛᜪձ (faceted categori-
zation) ٿᅿБݤޑᓬલᗺǶϩဂݤᓬᗺࢂ᏾ঁၸำ
ёԾ୏ϯǴёפډ΋٤Ԗ፪ޑᜪձ੝ቻǹલᗺ߾ࢂ
ਥᏵ಍ी่݀܌ளрޑᜪձӜᆀࡐёૈόڀж߄
܄ǴࣗԿ೷ԋᜪձޑঐ໶Ǵ٬ள٬Ҕޣคݤ೸ၸᜪ
ձӜᆀٰڐշځᔠ઩ၗૻǶԶќ΋ᅿБݤ߾ࢂаΓ
πБԄЋ୏ࡌҥᜪձǴӆ٩Ᏽᜪձޑ੝ቻКჹཛྷ൨
่݀Ǵࢎᄬؼӳޑᜪձё٬ϩᜪၨమཱǴࢂ࣬ჹܭ
ϩဂݤၨӳޑ೽ϩǴฅԶ೭ኬёૈคݤх֖܌Ԗᜪ
ձǴځᜪձޑ،ۓΨѸ໪ࢂཛྷ൨ၗ਑ޑΓޕၰޑǴ
೭ኬჹځωԖཀကǶ 
Ֆ։ᇬ[10]ஒ໘ቫԄϩဂݤᔈҔӧ҇٣ຊղा
ԑϩဂ΢Ǵݤ۔ёаᙖҗϩဂࡕޑٰ่݀΋΋ᔠຎ
ဂ໣Ǵ྽ᔠຎֹဂ໣ύ൳ጇຊղाԑǴว౜ό಄ӝ
дޑሡ؃Ǵߡёа۹ౣԜဂ໣ޑځдຊղाԑǴᙖ
а෧Ͽݤ۔઻຤ӧ҇٣ຊղाԑ᎙᠐ਔ໔Ƕբޣճ
Ҕᆫӝݤ[13]ޑБԄٰϩဂǴόᘐӝٳ࣬՟ޑਢٯ
Կ΋ঁߐᘖॶǶ೭္ᙖҗဂኧ܈࣬՟ࡋߐᘖٰ೛ۓ
ӝٳಖЗచҹǴҗ٬ҔޣঁձᔠᡍǴჹܭคݤаᙁ
ൂඔॊ๏ϒᜪձۓကޑୢᚒǴࢂၨޔௗޑբݤǶ 
Schweighofer ฻Ꮲޣ[18]ගډஒݤࡓЎҹаӛ
ໆᆢࡋޑБԄ৖౜ࢂࡐӳޑբݤǴхࡴӧीᆉ࣬՟
ࡋǵϩᜪ܈ࢂϣ৒ޑඔॊ΢ǴΨࡰрൂપ٬Ҕ tf±idf 
(term frequency ± inverse document frequency) 
[15][16]ٰीᆉӛໆа߄ҢЎҹǴջ٬ճҔֹ๓ޑी
ᆉ࣬՟ࡋϦԄϝᡉόىǴբޣ٬Ҕݤࡓሦୱၨᙁൂ
ޑҁᡏࢎᄬ (ontology) ٰׯ຾೭ኬޑલѨǴќѦΨ
ଞჹ੝ۓޑݤࡓӜຒբуख़៾ख़ޑ୏բǶՠ೭٤ᗋ
ࢂ཮ӣᘜ΋ঁ୷ҁୢᚒǴ൩ࢂ໪ाԖ΋ঁϦߞΚޑ
БԄ೛ीҁᡏࢎᄬǴҔаࡌ࿼ຒڂ܈ࢂೕ߾Ƕ 
ᖴదၲ[12]ࣴز࣬՟ນ೜Ўਜޑᔠ઩ǴճҔຒ
ಔࣁ୷ᘵǴஒЎകᙯඤࣁӛໆǴа k ന߈ᎃۚݤ(k 
nearest neighbors methodsǴᙁᆀ kNN)ཷۺ೛ीϩᜪ
ᄽᆉݤǴբນ೜ЎਜޑϩᜪπբǶ 
 
ᎄΓᇬ[11]૸ፕຒ༼ٰྍᆶ៾ख़ჹύЎຊղਜ
ϩᜪԋਏቹៜǴКၨவ HowNet[5]ᘏڗрຒ༼ౢғ
ϐຒڂᆶTermSpotterᄽᆉݤ[12]ڗрນ೜Ўҹ੝ۓ
ຒ༼ᇶаΓπঅ҅Ǵ܌ࡌҥϐຒڂǴჹܭϩᜪਏ݀
ϐቹៜǶᢀჸਢٯޑ࣬՟ࡋϩթǴפډ፾྽ୖኧǴ
ගϲϩᜪਏ Ƕ݀ճҔ kNNբࣁس಍ϩᜪᐒڋϩ݋ϩ
ᜪਏ݀Ƕќ٩Ծ࣪ԄᏢಞݤᆒઓǴࡌҥ៾ख़ፓ᏾ᐒ
ڋǴϩ݋៾ख़ፓ᏾ჹϩᜪਏ݀ޑቹៜǶ 
3 س಍೛ी 
౜ԖޑљݤଣݤᏢၗ਑ᔠ઩س಍[2]ύޑຊղਜࢗ
၌ǴගٮΑ୷ҁޑᔠ઩ຊղਜфૈǴё೛ۓݤଣǵ
ຊղᜪձǵਢҗǵਔ໔Ϸᜢᗖӷ฻చҹǴڗள಄ӝ
ޑ่݀Ǵӧϟय़΢ᡉҢԿӭ 100฽ޑၗૻǶ೭ኬޑ
ᔠ઩ϟय़ёаڐշݤ۔کࡓৣǴࣗԿ΋૓҇౲ڗள
܌ሡޑၗૻǶฅԶ٩ྣᜢᗖӷཛྷ൨рٰޑ่݀Ǵࡐ
ёૈх֖εໆޑၗૻǴךॺϝा೸ၸ೴΋ᔠຎٰၸ
ᘠǴ܈ख़ཥཤଜᔠ઩చҹޑ೛ۓǶ೭ኬޑୢᚒࢂӧ
ᔠ઩εໆຊղਢҹਔ৒ܰวғޑ௃׎Ƕ 
ҁࣴزట೛ी΋঺ນ೜Ўਜᔠ઩س಍Ǵගٮό
ӕܭ΢ॊޑᔠ઩фૈǶჹܭᔠ઩่݀ԶقǴӵ݀܌
ᕇள಄ӝచҹޑ่݀ࡐӭǴΞό׆ఈ೛ۓ׳ፄᚇޑ
ᔠ઩చҹǴٗԜਔ໪ाׯ຾ޑߡࢂᔠ઩่݀ޑև౜
БԄǶऩૈ٬ளᔠ઩่݀٩ྣຊղਜϣ৒ޑόӕ຾
ՉϩᜪǴ߾ёڐշ٬Ҕޣӧᔠ઩่݀ਔǴ٩ྣᜪձ
բᔠ઩Ƕځύό಄ӝځሡ؃ޑᜪձǴёаӧၨอਔ
໔ϣ،ۓౣၸǴБߡځᔠ઩ޑਏ౗аϷಔᙃၗૻǶ
ჹܭᔠ઩చҹԶقǴу΢ᙁൂޑᔠ઩చҹǴڐշҁ
س಍٩ԜచҹբϩᜪǴࣁ٬Ҕޣ᏾౛่݀ǴёБߡ
ځ᎙᠐Ƕ 
3.1 ߻ೀ౛ 
ӧຊղਜޑٰྍ΢ǴךॺճҔљݤଣݤᏢၗ਑ᔠ઩
س಍ύޑຊղਜࢗ၌[2]фૈڗளӉ٣ਢҹޑຊղ
ਜǴ٬Ҕᆛၡݽ਩Ꮤ (crawler) ᘏڗຊղਜǴ٠ၸ
ᘠځύԖୢᚒޑЎҹǴӵ໶ዸ฻ୢᚒޑЎҹǶҞ߻
ҁࣴز܌٬Ҕຊղਜጄൎࢂவ҇୯ 88 ԃԿ҇୯ 95
ԃϐ໔ӚӦБݤଣǴຊղਢҗϐύឦܭᠷหǵཟღǵ
மหǵ៊ ނǵ໾ ǵ্৮ᓵک።റ೭ΎεᜪޑຊղਜǴ
а΋૓Ўӷᔞਢ਱ԄᓯӸǴҞ߻ᕴኧࣁ 9296 ጇǶ
೚ӭޑຊղਜύ٠ߚѝԖൂ΋ਢҗǴӧҁࣴزύǴ
ךॺஒຊղਜ΢ޑຊղਢҗឯՏຎࣁЬाਢҗǴΨ
٩Ԝ຾Չ಍ीǴӵ߄ 1܌ҢǶ 
ࣁΑှ،εໆЎҹޑࢗ၌ਏ౗Ǵ׬ڗΠٰޑຊ
ղਜ໪࿶ၸೀ౛Ǵࡌҥ઩Ї฻ၗૻǶεໆЎҹᔠ઩
ࣁќ΋ঁࣴز᝼ᚒǴҁࣴزύό௖૸Ԝ೽ϩǴԶࢂ
٬Ҕ౜Ԗޑπڀ LuceneǴඹຊղਜᇙբϸӛ઩Ї 
(inverted indexes) Ъගٮࢗ၌фૈǴаቚ຾ᔠ઩ਏ
౗ǶLuceneϣ೽ॄೢᇙբ઩Їϐ߻ᘐຒфૈޑำԄ
ћբ AnalyzerǴځύԖӭᅿόӕჴբБԄǴႽࢂ
Lucene ϣࡌޑ StandardAnalyzerǵCJKAnalyzer ک
WhitespaceAnalyzer ฻ǶځჹύЎӷޑᘐຒЍජѝ
ज़ܭ΋ঁӷ΋ঁӷϪപ܈ࢂٿٿԋ΋ຒѐϩപǴܭ
ࢂךॺᒧ᏷୯ϣࣴزදၹ௦ҔύࣴଣޑύЎᘐຒ
س಍[1]ǴႣӃ຾ՉᘐຒǴӆ΋΋ஒᘐຒࡕޑϣ৒Ҕ
ޜқ႖໒ǴനࡕӆճҔ WhitespaceAnalyzer ٩Ᏽޜ
қᘐຒٰࡌҥ઩ЇǶ 
ӧҁس಍ຊղਜޑϩᜪύǴ໪٬Ҕډᘐຒຒڂ
ٰڗрЎҹύຒ༼ǴҔа຾Չ࣬՟ࡋޑКჹǶऩ٬
Ҕ΋ঁࡐֹ᏾ޑ΋૓தҔຒڂǴܰ೷ԋӛໆᆢࡋၸ
ε (Ӣࣁຒ༼ޑಔӝࡐӭᅿ) Ǵ຾Զቹៜس಍ޑਏ
౗ǴЪёૈӢε೽ϩό࣬ᜢޑຒ༼Զቹៜ่݀Ƕჹ
ܭႽݤࡓ೭ኬ஑཰ޑሦୱԖ೚ӭ஑ԖӜຒǴԶԖ٤
ຒ༼߾ࢂڋԄ܈ಞᄍҔݤǴӵ݀ૈஒځࡌԋ஑཰ຒ
ڂǴߡૈε൯෧ե΋૓ຒڂ܌೷ԋӛໆᆢࡋၸεޑ
ୢᚒǶҁس಍٬Ҕಃ 2 ࿯ගډ TermSpotter ᄽᆉݤ
܌ᘏڗрޑຒ༼բࣁຒڂǴஒຊղਜᘐຒᙯඤࣁӛ
ໆǴٰ຾Չ٩࣬՟ࡋޑϩဂБݤǶ 
3.2 س಍фૈϷᏹբϟय़ 
კ 1ࣁ٬Ҕޣӧᏹբҁس಍຾ՉޑࢬำǶаΠϩձ
ᙁϟӚ໘ࢤϣ৒Ǵ၁ಒբݤךॺ཮ӧࡕय़բϟಏǶ 
 ΋໒ۈ٬Ҕޣёᒧ᏷ᒡΕటᔠ઩ޑຒ༼ᜢᗖ
ӷ܈ࢂ΋ࢤҍ࿾٣ჴഋॊǴаϷᒧ᏷่݀ޑ
և౜БԄǴҗόӕޑᔠ઩ำԄೀ౛Ƕ 
 ᔠ઩ำԄ٩ྣϩᜪኧҞǵӉࡋᒧ᏷аϷ٣ჴ
࣬ᜢำࡋࢗ၌฻όӕሡ؃చҹǴҗӚঁำԄ
வၗ਑৤ύᘏڗ࣬ᜢၗ਑຾ՉϩᜪǶ 
 ၗ਑৤ࣁΑගٮᔠ઩ำԄεໆၗ਑ǴନΑচ
ۈޑၗ਑ (ຊղਜ) ǴќႣӃࡌҥ઩Їکᙯ
ඤၗ਑Ǵගٮόӕᔠ઩ำԄ຾ՉזೲӸڗǶ 
 നࡕᒡрϩᜪගٮ٬ҔޣᒧڗǴᔠ઩ϣ৒Ƕ
ќѦךॺΨගٮ٬Ҕޣόӕᜪձύр౜ޑ࣬
ᜢຒ༼ၗૻǴ٬ځૈ׳זඓඝ࣬ᜢၗૻǶ 
 ӧ᎙᠐ЎҹਔǴёჹЎҹ຾Չ኱૶ǴӵǺᒧ
᏷Ԗᑫ፪ϐᜢᗖӷǴբࣁВࡕᔠ઩ਔ੝ձ኱
ҢрٰϐЎӷǴΨё຾Չ಍ी܈Ҕаׯ຾ϩ
ᜪ่݀ǶԶу΢ຏှޑфૈǴගٮ٬Ҕޣჹ
΋ࢤЎӷբຏှǴЪёԾՉۓကᜪձǴБߡ
Вࡕࢗ၌Ƕ 
ӧჴբБय़ךॺ௦Ҕ Javaᇟقٰ໒วǴЪ٬Ҕ
җ Java܌ࡌᄬϐπڀǴӵ Lucene[7]Ҕٰ຾ՉӄЎ
 
კ 1 س಍ᏹբࢬำ 
ᔠ઩
చҹ
ᔠ઩
ำԄ
ຊղ
ਜၗ
਑৤
ϩᜪ
ᔠ઩
่݀
኱૶
܈у
ຏှ
฻
߄ 1 ຊղਜӚձਢҗኧໆ಍ी 
ਢҗ ᠷห ཟღ மห ៊ނ ໾্ ৮ᓵ ።റ 
ᕴኧҞ 312 386 1267 2083 989 385 3874 
 
ᔠ઩ǹHSQLDB[6]ၗ਑৤ҔٰᓯӸ኱૶کຏှ฻
ၗૻǴߡܭس಍ϐ᏾ӝǶ 
კ 2ࣁҞ߻೛ीޑᏹբϟय़ǴЬाԖ AǵBک
CΟ೽ϩǶӧ AᒡΕᔠ઩ޑచҹǹཛྷ൨ϐࡕǴӧ B
ύᡉҢϩဂ่݀ک࣬ᜢၗૻٮ٬Ҕޣᔠ઩ǹC߾٩
ྣ BύޑᏹբᡉҢϩဂຊղਜӈ߄܈ຊղਜϣ৒Ƕ
ᛖܭጇ൯ज़ڋǴӧϐࡕ໻ߕ΢೽ϩᄒკǶ 
٩࣬՟ࡋ຾Չຊղਜϩဂᔠ઩ޑᏹբϟय़Ǵ
ӵკ 2 ޑ A ೽ϩࣁᔠ઩చҹޑ೛ۓǴх֖Αᔠ઩
ᜢᗖӷǵϩဂኧज़ڋǵϩဂ٩Ᏽ฻Ƕკ 2 ޑ B ೽
ϩ߾ࢂӧ೛ۓӳᔠ઩చҹǴࡪΠȨཛྷ൨ȩࡕޑϩဂ
่ Ǵ݀཮ᡉҢӚϩဂ܌х֖ຊղਜϐਢҗ಍ीޑ߻
ΟӜǶᗺᒧҺཀ΋ဂࡕǴᡉҢԜဂ໣ޑ܌Ԗຊղਜ
ӈ߄Ǵӵკ 2 ޑ C ೽ϩ܌ҢǶځύ؂΋ঁᒧ໨ග
ٮΑΟঁၗૻǺВයǵਢҗکЬЎǶҺᗺᒧ΋ጇࡕ
ߡ཮ᡉҢຊղਜϣ৒Ǵӕਔᏹբϟय़ϪඤډȨຊղ
ਜȩ኱ᠸǴӵკ 3܌ҢǴ΢य़཮ӈрԜຊղਜനࡕ
܌ຊۓϐӉݤݤచǶᗺᒧݤచࡕ཮Ծ୏ೱ่ډӄ୯
ݤೕၗ਑৤[4]ѐᔠ઩Ԝݤచϣ৒Ƕ 
߻ॊфૈࢂଞჹගٮၗૻၨϿЪၨόܴዴޑ
ᜢᗖӷǴ܌຾Չޑϩဂᔠ઩Ƕௗ๱ךॺाϟಏ࣬՟
ਢҹᔠ઩ф Ǵૈࢂёаӧᔠ઩చҹύᒡΕ΋ࢤҍ࿾
٣ჴǴ೸ၸஒځᙯඤԋӛໆٰीᆉᆶၗ਑৤ύޑຊ
ղਜ࣬՟ࡋǴᒡр࣬՟ޑຊղਜǴගٮ٬Ҕޣբຊ
ղޑୖԵ٩Ᏽ܈բ࣬ᜢਢҹޑࣴزǶᒡΕϟय़ӵკ
4܌ҢǴ྽ךॺӧკ 2ޑ A೽ϩύࡪΠȨᜢᗖӷȩ
ޑБ਱ǴёϪඤԿҍ࿾٣ჴޑᒡΕБ༧ǴࡪΠཛྷ൨
ϐࡕǴ཮ᡉҢ࣬՟ϩဂǶךॺගٮന࣬՟ޑٿঁਢ
җϩဂǴஒځ࣬՟ਢҹ٩࣬՟ࡋ௨ׇǴ٠ஒӚਢҗ
ϐਢҹϩࣁٿъǴᕴӅᒡрѤঁϩဂǴᏹբϟय़Ϸ
фૈӕϐ߻ޑϩဂᔠ઩Ƕ 
 
კ 2 ᏹբϟय़ 
 
კ 3 ղӉӈ߄ 
 
კ 4 ࣬՟ਢҹᔠ઩ 
A 
B C 
 
ௗΠٰϟಏໆӉᇶշᔠ઩ф ǴૈԜфૈёۯុ
ϐ߻ჹܭ҂ޕਢҹϩဂᔠ઩ࡕǴࣁΑڐշ٬Ҕޣᔠ
઩ຊղਜਔǴૈ ளޕၸѐ೭٤ਢҹ೏ղ،ޑӉᆦᇸ
ख़Ǵаόӕޑ୔໔ϩձӈрǴБߡϩ݋ᔠ઩Ƕךॺ
ճҔ҅ೕ߄Ңݤ (regular expression) வຊղਜЬ
Ў୔ࢤύᘏڗрӉᆦໆӉޑ೽ϩǴӵԖය২Ӊ (ୖ) 
ԃ (ມ) ДǶ࿶ၸ಍ीϷ௨ׇǴ٩ྣ୔໔ஒຊղਜ
ϩᜪᡉҢǶќѦךॺΨёаගٮݤ۔ǵࡓৣჹ੝ۓ
Ӊᆦޑ୔໔բᔠ઩ǴᇶշځԵቾਢҹղ،ᇸख़Ƕϟ
य़ӵკ 5܌ҢǴεठ΢ᆶ٩࣬՟ࡋ຾Չຊղਜϩဂ
ᔠ઩ϟय़࣬ӕǴس಍ஒᔠ઩่݀٩Ӊࡋٰ௨ׇǴკ
ύޑٯηஒ่݀ѳ֡ϩԋΎဂǴ҂ٰஒӆуΕკ׎
ϯޑϟय़ٰᡉҢࢌ΋ဂ࣬ᜢਢҹޑղӉϩթǶҞ߻
ѝԵቾԖය২ӉޑӉයǴ௨ׇࡕӆஒόӕޑ୔໔ঁ
ձϩԋ΋ဂǴ҂ٰёуΕځдໆӉຑ՗ᐒڋǴӵܥ
אǵᆦߎکܰܺമא฻Ƕ 
౜ӧς࿶ϟಏϩဂޑфૈϷϟय़ᏹբǴௗ๱ा
ϟಏךॺ܌ගٮޑځдၗૻǴ࣬ ᜢຒ༼ࢗ၌фૈǶ
ၗૻᔠ઩΢தҔډޑ୷ҁמ Ǵೌ൩ࢂճҔຒ༼ᆶຒ
༼ϐ໔ޑӅ౜ (collocation) Ǵёவ߻ࡕЎٰཛྷ൨р
ਔதӕਔр౜ޑӷՍǴԜӷՍࡐԖёૈࢂڀԖཀက
ޑǴж߄๱೭٤ຒ༼ёૈԖჴሞ΢ޑᜢೱ܄Ƕךॺ
టගٮ٬Ҕޣ೸ၸᒡΕຒ༼Ǵҁس಍פрதр౜ӧ
ځ߻ࡕޑຒǶჹܭ΋૓٬ҔޣٰᇥǴёаගٮᆶᜢ
ᗖӷਔதӅ౜ޑຒ༼ಔӝǴᗺᒧځύޑຒ༼Ǵᝩុ
ᔠ઩࣬ᜢຊղਜǶճҔԜБԄٰჹཛྷ൨่݀բϩᜪǴ
аຒᓎଯեբࣁ่݀௨ ǶׇԜфૈё٬΋૓҇౲܈
ᏢಞݤᏢޑ٬ҔޣΑှ࣬ᜢຒ༼ޑр౜௃׎ǴΨԖ
ᐒ཮ᙖԜפрࢌ٤ҍਢޑᜢೱ܄Ƕ 
ϟय़ӵკ 6܌ҢǴךॺӧࢗ၌ਔӧᜢᗖӷឯՏ
ᗖΕȨӼӄȩ೭ঁຒ༼Ǵཛྷ൨ϐࡕǴᗺᒧȨ࣬ᜢຒ
༼ȩޑ኱ᠸǴߡ཮р౜კύ E ೽ϩಃ΋ቫޑ࣬ᜢ
ຒ༼ӈ߄ǴᗺᒧȨ࠶ુȩຒ༼ߡ཮р౜ຊղਜӈ߄
ӵҁ࿯კ 2ޑ C೽ϩ׎ԄǴӕਔკ 6ύ F೽ϩΨ
཮р౜ᆶᔠ઩ຒ༼аϷ܌ᗺᒧޑ࣬ᜢຒ༼ࣣӅ౜
ޑຒ༼ӈ߄Ǵᗺᒧࡕӕኬ཮р౜࣬ᜢຊղਜӈ߄Ƕ
ќѦࣁᗉխಃ΋ቫ࣬ᜢຒ༼ӈ߄ၸε٬ளᡉҢೲ
ࡋ጗ᄌǴу΢კύ໔
ޑ௔ືٰϪඤ।य़Ǵ
؂।໻ᡉҢ 20໨Ǵ٩
ྣӅ౜ᓎ౗җεԿλ
௨ׇǶ 
٬Ҕޣӧ᎙᠐ຊ
ղਜਔǴךॺගٮЎ
ӷ኱૶ޑфૈǶӄౚ
ၗૻᆛ຾Ε Web 2.0
ޑਔжǴO'Reilly[17]
ගрᆶၸѐ Web 1.0
όӕޑځύ΋໨੝ቻ
ࢂவϩᜪᏢ  (tax-
onomy) ډ౜ӧޑε
౲ϩᜪݤ  (folkson-
omy) Ǵҗ٬Ҕޣٰ،
ۓᜪձǴᗨᇥӭኧኪ
Κ(tyranny of the ma-
jority)ࢂঁ໪ाుΕ
ࣴزޑ᝼ᚒǴՠ೭ኬޑϩᜪБԄ׳ຠ߈٬ҔޣǴӵ
ૈҔӧঁΓϯ΢ǴΨૈڐշ٬Ҕޣ׳ᆒዴِೲӦפ
ډ܌ሡޑၗૻǶӢԜךॺу΢ਢҹ኱૶ޑфૈǴڐ
շ٬Ҕޣჹຊղਜϣ৒բ኱૶ϷຏှǶ 
Ҟ߻ךॺޑ኱૶БԄϩࣁٿᅿǴ΋ᅿࢂᒧ᏷ࢌ
ࢤЎӷճҔ׳ׯङඳᚑՅբ኱૶Ǵу΢ཀـຏှǵ
೛ۓཀـޑᜪձǴ܈ࢂቚуཥޑᜪձǶԶЎӷङඳ
җᜪձٰ،ۓǴόӕᜪձёа೛ۓόӕᚑՅǶᏹբ
ϟय़ӵკ 7܌ҢǴӧᒧڗȨठғӒ্Ӽӄȩࡪѓᗖ
ᒧ᏷኱૶ϐࡕǴ཮ၢрЎӷ኱૶ޑຎืǴӧȨ኱૶
ЎӷȩឯՏёаႣំ኱૶ԋᜪձ 1ࡕޑᚑՅ೛ۓǴ
ќѦΨёаࡪΠȨཥቚȩǴཥቚ኱૶ᜪձаϷ೛ۓ
኱૶ޑᚑՅǴȨཀـȩឯՏ߾ёᒡΕाຏ૶ޑЎӷǶ
኱૶ࡕޑၗૻ཮ӸԿၗ਑৤ύǴаࡕ໒௴ԜЎҹ཮
ஒ೭٤೽ϩբ኱૶Ǵᗺᒧ኱૶೽ϩϐࡕ཮ӆख़ཥᡉ
Ңຏ૶ၗૻǶ 
კ 8ࣁќ΋ᅿ኱૶БԄǴख़ाຒ༼኱૶ࢂᒧ᏷
΋ࢤЎӷஒځ߻ඳ(ջЎӷ)ᚑՅբᡂ׳ǴВࡕӧ᎙
᠐ຊղਜਔǴҁس಍཮Ծ୏ஒ܌Ԗх֖ޑख़ाຒ༼
٩࣬ჹᔈޑᚑՅբ኱૶Ƕკ 7ਢҹύޑȨЈғࣙឨȩ
Ѥঁӷߡࢂკ 8኱૶ࡕޑ่݀Ƕ 
 
კ 5 ٩Ӊࡋϩဂ 
 
კ 6 ࣬ᜢຒ༼ӈ߄ 
 
კ 7 ኱૶ຏှЎӷ 
F 
E 
國際學術會議出席報告 
國立政治大學資訊科學系劉昭麟 
chaolin@nccu.edu.tw 
摘要 
劉昭麟（以下自稱為報告人）於二零零八年六月中赴美國俄亥俄州哥倫布市
(Columbus, Ohio, USA)，參與了計算語言學會(Association for Computational Linguis-
tics，簡稱 ACL)的年會，並且在會議中報告論文。這是這一次出席國際學術會議的報告。
本報告首先列出出席會議的時間、地點、所參與的會議的基本資料和相關網址；然後報
告參與會議所體驗的觀察和心得；最後提出簡短的結論。 
1 出訪地點、時間、參與會議 
1.1 基本資料 
出訪地點：美國俄亥俄州哥倫布市(Columbus, Ohio, USA) 
會議時間：二零零八年六月十五日至六月二十日 
參與會議：ACL 2008: The Forty Sixth Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies 
經費來源：國科會研究經費與政治大學資科系部份補助 
發表論文：Using Structural Information for Identifying Similar Chinese Characters （附
件五） 
相關網址： 
 ACL: http://www.aclweb.org 
 ACL 2008: http://www.ling.ohio-state.edu/acl08/   
1.2 參與過程 
ACL 的年會是歷史悠久的計算語言學學術會議，會議的時間從六月十五日到二十
日，其中十五日是主會議前的教學課程(tutorials)，十九日和二十日是主會議之後的工作
坊(workshops)。由於距離與時差的問題，報告人必須在台北時間十三日就從台北啟程，
於美國當地時間十五日參與了 Building Practical Spoken Dialog Systems 的教學課程，於
十六日報告論文，並且於十九日參加了 The Third Workshop on Innovative Use of NLP for 
Building Educational Applications，最後於美國當地時間二十日離開哥倫布市返國。 
參與本次會議的台灣學者明顯偏少，只有遇到前清華大學電機系的蘇克毅教授。我
們不能確定這一個低出席率是因為研究經費的限制或者是因為哥倫布市的交通明顯地
不是非常方便，須要在美國其他主要都市轉機過來。儘管這些可能的原因，本次會議仍
然有許多來自香港、新加坡等亞洲的學者。 
正想要的資訊，可能是比較務實的目標。Susan Dumais 介紹了許多往這一方向發展的相
關的軟體設計理念和實際系統。 
今年的 ACL 學術貢獻講(lifetime achievement award)頒給 University of Sheffield 的
Yorick Wilks。Wilks 的演講介紹了他在自然語言處理與人工智慧研究等多面的研究經
驗，常常也觸及更深層的科學研究理念，如果聽者本身沒有相當廣博的知識和很好的英
文聽力，這樣高階的演講可能是不容易立即吸收。附件三是 Wilks 的演講資料。 
關於報告者關於個別論文的聽講心得對於本報告的讀者或許沒有特別的吸引力，
ACL 所有的論文都公開在網路上面，請參閱附件四的議程，與網路上的電子版論文
(http://aclweb.org/anthology-new/)。其他例如六個教學課程和十個工作坊的資料，請分別
參考附件一和附件二的簡介。 
除了參與學術會議之外，由於出訪經費的拮据，因此報告人所暫住的旅店距離會議
的飯店有相當的距離，每次來回開會與住所之間，單程就須要步行大約二十幾分鐘，也
因此有許多天的機會來觀察哥倫布市的日常街景。此次由美國而起的世界金融海嘯對於
美國人確實有不小的影響，哥倫布市的大眾運輸系統的使用率看起來相當地高，上下班
時間有不少等車的民眾。這可能不是一般美國中小型城市所常見的景象。 
3 結論 
我國致力於推展學術研究國際化，近年以來資訊科學這一方面的國際學術研討會如
雨後春筍般的蓬勃發起，除了國際學術會的頂級會議之外，例如 AAAI、IJCAI、ACL、
ICML、UAI、ITS、AIED、COLING、ACM 各 SIG 的年會等等，我國參與其他的新興
的學術研討會的必要性似乎可以做一個整體性的規劃。新興的學術研討會雖然學術知名
度不高，但是常常是培養新領域的搖籃，學術價值不可謂不高；然而，如果長期投注在
這一類新領域的研討會的邊際效用則是可以檢討的。相對地，參與具有傳統聲譽的學術
研討會，則有立竿見影的觀摩效果，可以刺激參與者更加努力、以追求在這一類研討會
發表更好論文的機會。 
參考附件 
附件一：ACL 2008 教學課程簡介  
附件二：ACL 2008 工作坊簡介 
附件三：http://www.companions-project.org/downloads/Wilks_ACL08.pdf  
附件四：ACL 2008 論文議程 
附件五：報告人所發表之論文 
H
om
eȱ
ȱ
R
eg
is
tr
at
io
nȱ
ȱ
A
cc
om
m
od
at
io
nȱ
ȱ
Tr
av
el
ȱȱ
A
re
aȱ
In
fo
ȱȱ
Pr
es
en
ta
tio
nȱ
in
st
ru
ct
io
ns
ȱȱ
Po
st
er
ȱin
st
ru
ct
io
ns
ȱ
C
al
lȱf
or
ȱP
ap
er
sȱ
ȱ
Sc
he
du
le
ȱA
tȬA
Ȭ
G
la
nc
eȱ
ȱ
Fu
llȱ
Sc
he
du
le
ȱȱ
W
or
ks
ho
ps
ȱȱ
Tu
to
ri
al
sȱ
ȱ
St
ud
en
tȱR
es
ea
rc
hȱ
W
or
ks
ho
pȱ
ȱ
R
el
at
ed
ȱE
ve
nt
sȱ
ȱ
Fo
od
ȱȱ
R
ec
ep
tio
nȱ
ȱ
Ba
nq
ue
tȱȱ
St
ud
en
tȱL
un
ch
ȱȱ
A
C
L-
08
:H
LT
 T
ut
or
ia
ls
 
Tu
to
ria
l S
ch
ed
ul
e 
In
tr
od
uc
tio
n 
to
 C
om
pu
ta
tio
na
l A
dv
er
tis
in
g 
(E
vg
en
iy
 G
ab
ril
ov
ic
h,
 V
an
ja
 J
os
ifo
vs
ki
, a
nd
 B
o 
Pa
ng
) 
S
ho
rt 
ab
st
ra
ct
: 
W
eb
 a
dv
er
tis
in
g 
is
 th
e 
pr
im
ar
y 
dr
iv
in
g 
fo
rc
e 
be
hi
nd
 m
an
y 
W
eb
 a
ct
iv
iti
es
, i
nc
lu
di
ng
 
In
te
rn
et
 s
ea
rc
h 
as
 w
el
l a
s 
pu
bl
is
hi
ng
 o
f o
nl
in
e 
co
nt
en
t b
y 
th
ird
-p
ar
ty
 p
ro
vi
de
rs
. A
 
ne
w
 d
is
ci
pl
in
e 
- C
om
pu
ta
tio
na
l A
dv
er
tis
in
g 
- h
as
 re
ce
nt
ly
 e
m
er
ge
d,
 w
hi
ch
 s
tu
di
es
 
th
e 
pr
oc
es
s 
of
 a
dv
er
tis
in
g 
on
 th
e 
In
te
rn
et
 fr
om
 a
 v
ar
ie
ty
 o
f a
ng
le
s.
 A
 s
uc
ce
ss
fu
l 
ad
ve
rti
si
ng
 c
am
pa
ig
n 
sh
ou
ld
 b
e 
re
le
va
nt
 to
 th
e 
im
m
ed
ia
te
 u
se
r's
 in
fo
rm
at
io
n 
ne
ed
 
as
 w
el
l a
s 
m
or
e 
ge
ne
ra
lly
 to
 u
se
r's
 b
ac
kg
ro
un
d 
an
d 
pe
rs
on
al
iz
ed
 in
te
re
st
 p
ro
fil
e,
 
M
or
ni
ng
In
tro
du
ct
io
n 
to
 
C
om
pu
ta
tio
na
l
A
dv
er
tis
in
g
B
ui
ld
in
g 
P
ra
ct
ic
al
 
S
po
ke
n 
D
ia
lo
g 
S
ys
te
m
s
S
em
i-s
up
er
vi
se
d
Le
ar
ni
ng
 fo
r N
at
ur
al
 
La
ng
ua
ge
 P
ro
ce
ss
in
g
A
fte
rn
oo
n
A
dv
an
ce
d 
O
nl
in
e 
Le
ar
ni
ng
 fo
r N
at
ur
al
 
La
ng
ua
ge
 P
ro
ce
ss
in
g
S
pe
ec
h 
te
ch
no
lo
gy
 
fro
m
 re
se
ar
ch
 to
in
du
st
ry
In
te
ra
ct
iv
e 
V
is
ua
liz
at
io
n 
fo
r C
om
pu
ta
tio
na
l
Li
ng
ui
st
ic
s
9:
00
-1
0:
30
M
or
ni
ng
 tu
to
ria
l p
ar
t 1
10
:3
0-
11
:0
0
m
or
ni
ng
 b
re
ak
11
:0
0-
12
:3
0
M
or
ni
ng
 tu
to
ria
l p
ar
t 2
2:
00
-3
:3
0
A
fte
rn
oo
n 
tu
to
ria
l p
ar
t 1
3:
30
-4
A
fte
rn
oo
n 
br
ea
k
4:
00
-5
:3
0
A
fte
rn
oo
n 
tu
to
ria
l p
ar
t 2
ร
ʳ˄
ʳ଄
Δ
٥
ʳ˄
ˉʳ
଄
˔
˖˟
ˀ˃
ˋˍ
ʳ˛
˟˧
ʳˀʳ
˧̈
̇̂
̅˼˴
˿̆
˅˃
˃ˌ
˂˄
˂˅
ˈ
˻̇
̇̃
ˍ˂˂
̊
̊
̊
ˁ˿˼
́˺
ˁ̂
˻˼
̂ˀ
̆̇˴
̇˸
ˁ˸
˷̈
˂˴
˶˿
˃ˋ
˂˶
˹̇ˁ
˻̇
̀
˿
Sp
on
so
rs
ȱȱ
C
on
ta
ct
ȱȱ
H
os
te
d 
by
: 
Th
e  
O
hi
o 
S
ta
te
 
U
ni
ve
rs
ity
D
ep
ar
tm
en
t o
f 
Li
ng
ui
st
ic
s
Th
e 
O
hi
o 
S
ta
te
 
U
ni
ve
rs
ity
D
ep
ar
tm
en
t o
f 
C
om
pu
te
r S
ci
en
ce
 
an
d 
E
ng
in
ee
rin
g
be
 e
co
no
m
ic
al
ly
 w
or
th
w
hi
le
 to
 th
e 
ad
ve
rti
se
r a
nd
 th
e 
in
te
rm
ed
ia
rie
s 
(e
.g
., 
th
e 
se
ar
ch
 e
ng
in
e)
, a
s 
w
el
l a
s 
be
 a
es
th
et
ic
al
ly
 p
le
as
an
t a
nd
 n
ot
 d
et
rim
en
ta
l t
o 
us
er
 
ex
pe
rie
nc
e.
Th
e 
tu
to
ria
l d
oe
s 
no
t a
ss
um
e 
an
y 
pr
io
r k
no
w
le
dg
e 
of
 W
eb
 a
dv
er
tis
in
g,
 a
nd
 w
ill
 
be
gi
n 
w
ith
 a
 c
om
pr
eh
en
si
ve
 b
ac
kg
ro
un
d 
su
rv
ey
 o
f t
he
 to
pi
c.
 In
 th
is
 tu
to
ria
l, 
w
e 
fo
cu
s 
on
 o
ne
 im
po
rta
nt
 a
sp
ec
t o
f o
nl
in
e 
ad
ve
rti
si
ng
, n
am
el
y,
 c
on
te
xt
ua
l r
el
ev
an
ce
. 
It 
is
 e
ss
en
tia
l t
o 
em
ph
as
iz
e 
th
at
 in
 m
os
t c
as
es
 th
e 
co
nt
ex
t o
f u
se
r a
ct
io
ns
 is
 
de
fin
ed
 b
y 
a 
bo
dy
 o
f t
ex
t, 
he
nc
e 
th
e 
ad
 m
at
ch
in
g 
pr
ob
le
m
 le
nd
s 
its
el
f t
o 
m
an
y 
N
LP
 
m
et
ho
ds
. A
t f
irs
t a
pp
ro
xi
m
at
io
n,
 th
e 
pr
oc
es
s 
of
 o
bt
ai
ni
ng
 re
le
va
nt
 a
ds
 c
an
 b
e 
re
du
ce
d 
to
 c
on
ve
nt
io
na
l i
nf
or
m
at
io
n 
re
tri
ev
al
, w
he
re
 o
ne
 c
on
st
ru
ct
s 
a 
qu
er
y 
th
at
 
de
sc
rib
es
 th
e 
us
er
's
 c
on
te
xt
, a
nd
 th
en
 e
xe
cu
te
s 
th
is
 q
ue
ry
 a
ga
in
st
 a
 la
rg
e 
in
ve
rte
d 
in
de
x 
of
 a
ds
. W
e 
sh
ow
 h
ow
 to
 a
ug
m
en
t t
he
 s
ta
nd
ar
d 
in
fo
rm
at
io
n 
re
tri
ev
al
 
ap
pr
oa
ch
 u
si
ng
 q
ue
ry
 e
xp
an
si
on
 a
nd
 te
xt
 c
la
ss
ifi
ca
tio
n 
te
ch
ni
qu
es
. W
e 
de
m
on
st
ra
te
 h
ow
 to
 e
m
pl
oy
 a
 re
le
va
nc
e 
fe
ed
ba
ck
 a
ss
um
pt
io
n 
an
d 
us
e 
W
eb
 
se
ar
ch
 re
su
lts
 re
tri
ev
ed
 b
y 
th
e 
qu
er
y.
 T
hi
s 
st
ep
 a
llo
w
s 
on
e 
to
 u
se
 th
e 
W
eb
 a
s 
a 
re
po
si
to
ry
 o
f r
el
ev
an
t q
ue
ry
-s
pe
ci
fic
 k
no
w
le
dg
e.
 W
e 
al
so
 g
o 
be
yo
nd
 th
e 
co
nv
en
tio
na
l b
ag
 o
f w
or
ds
 in
de
xi
ng
, a
nd
 c
on
st
ru
ct
 a
dd
iti
on
al
 fe
at
ur
es
 u
si
ng
 a
 la
rg
e 
ex
te
rn
al
 ta
xo
no
m
y 
an
d 
a 
le
xi
co
n 
of
 n
am
ed
 e
nt
iti
es
 o
bt
ai
ne
d 
by
 a
na
ly
zi
ng
 th
e 
en
tir
e 
W
eb
 a
s 
a 
co
rp
us
. C
om
pu
ta
tio
na
l a
dv
er
tis
in
g 
po
se
s 
nu
m
er
ou
s 
ch
al
le
ng
es
 a
nd
 
op
en
 re
se
ar
ch
 p
ro
bl
em
s 
in
 te
xt
 s
um
m
ar
iz
at
io
n,
 n
at
ur
al
 la
ng
ua
ge
 g
en
er
at
io
n,
 
na
m
ed
 e
nt
ity
 e
xt
ra
ct
io
n,
 c
om
pu
te
r-
hu
m
an
 in
te
ra
ct
io
n,
 a
nd
 o
th
er
s.
 T
he
 la
st
 p
ar
t o
f 
th
e 
tu
to
ria
l w
ill
 b
e 
de
vo
te
d 
to
 re
ce
nt
 re
se
ar
ch
 re
su
lts
 a
s 
w
el
l a
s 
op
en
 p
ro
bl
em
s,
 
su
ch
 a
s 
au
to
m
at
ic
al
ly
 c
la
ss
ify
in
g 
ca
se
s 
w
he
n 
no
 a
ds
 s
ho
ul
d 
be
 s
ho
w
n,
 h
an
dl
in
g 
ge
og
ra
ph
ic
 n
am
es
, c
on
te
xt
 m
od
el
in
g 
fo
r v
er
tic
al
 p
or
ta
ls
, a
nd
 u
si
ng
 n
at
ur
al
 
la
ng
ua
ge
 g
en
er
at
io
n 
to
 a
ut
om
at
ic
al
ly
 c
re
at
e 
ad
ve
rti
si
ng
 c
am
pa
ig
ns
.  
Tu
to
ria
l o
ut
lin
e 
z
In
tro
du
ct
io
n
z
A
dv
er
tis
in
g 
on
 th
e 
W
eb
 
{
Th
e 
ev
ol
ut
io
n 
of
 W
eb
 a
dv
er
tis
in
g
{
A
dv
er
te
se
 (i
nt
ro
du
ct
io
n 
of
 te
rm
in
ol
og
y)
{
M
ai
n 
sc
en
ar
io
s 
of
 o
nl
in
e 
ad
ve
rti
si
ng
 
- S
po
ns
or
ed
 s
ea
rc
h
- C
on
te
nt
 m
at
ch
- E
xa
ct
 m
at
ch
 v
s.
 b
ro
ad
 m
at
ch
  
- T
he
 e
co
no
m
ic
s 
of
 W
eb
 a
dv
er
tis
in
g
z
M
ai
n 
te
ch
ni
ca
l c
ha
lle
ng
es
 fo
r N
LP
 a
nd
 IR
ร
ʳ˅
ʳ଄
Δ
٥
ʳ˄
ˉʳ
଄
˔
˖˟
ˀ˃
ˋˍ
ʳ˛
˟˧
ʳˀʳ
˧̈
̇̂
̅˼˴
˿̆
˅˃
˃ˌ
˂˄
˂˅
ˈ
˻̇
̇̃
ˍ˂˂
̊
̊
̊
ˁ˿˼
́˺
ˁ̂
˻˼
̂ˀ
̆̇˴
̇˸
ˁ˸
˷̈
˂˴
˶˿
˃ˋ
˂˶
˹̇ˁ
˻̇
̀
˿
z
B
ib
lio
gr
ap
hy
 s
ur
ve
y 
 
z
B
re
ak
  
z
IR
 m
od
el
in
g 
{
A
ds
 a
s 
in
fo
rm
at
io
n 
su
pp
ly
 a
nd
 re
du
ct
io
n 
to
 s
ea
rc
h
{
A
 u
ni
fie
d 
ap
pr
oa
ch
 to
 W
eb
 a
dv
er
tis
in
g
{
U
si
ng
 s
ea
rc
h 
re
su
lts
 a
s 
ex
te
rn
al
 k
no
w
le
dg
e 
 
{
Te
xt
 c
la
ss
ifi
ca
tio
n
{
N
am
ed
 e
nt
iti
es
z
Th
e 
re
se
ar
ch
 fr
on
tie
r 
{
Te
xt
 s
um
m
ar
iz
at
io
n 
/ j
us
t-i
n-
tim
e 
ad
ve
rti
si
ng
  
{
W
he
n 
no
t t
o 
ad
ve
rti
se
 / 
ad
 s
pa
m
{
Lo
ca
tio
n 
aw
ar
en
es
s 
/ g
eo
-ta
rg
et
in
g
{
C
on
te
xt
 m
od
el
in
g
{
N
at
ur
al
 la
ng
ua
ge
 g
en
er
at
io
n 
/ a
ut
om
at
ic
 a
d 
cr
ea
tio
n
z
D
is
cu
ss
io
n 
an
d 
qu
es
tio
ns
 fr
om
 th
e 
au
di
en
ce
  
Sh
or
t b
io
gr
ap
hi
ca
l d
es
cr
ip
tio
n 
of
 p
re
se
nt
er
(s
) 
E
vg
en
iy
 G
ab
ril
ov
ic
h 
ga
br
@
ya
ho
o -
in
c.
co
m
V
an
ja
 J
os
ifo
vs
ki
 v
an
ja
j@
ya
ho
o-
in
c.
co
m
B
o 
P
an
g 
bo
pa
ng
@
ya
ho
o-
in
c.
co
m
A
ffi
lia
tio
n:
 Y
ah
oo
! R
es
ea
rc
h,
 C
om
pu
ta
tio
na
l A
dv
er
tis
in
g 
an
d 
S
ea
rc
h 
Te
ch
no
lo
gy
 
G
ro
up
C
on
ta
ct
 in
fo
rm
at
io
n:
 
28
21
 M
is
si
on
 C
ol
le
ge
 B
lv
d,
 S
an
ta
 C
la
ra
, C
A
 9
50
54
 
Fa
x:
 4
08
-3
49
-2
27
0 
 
E
vg
en
iy
 G
ab
ril
ov
ic
h 
is
 a
 S
en
io
r R
es
ea
rc
h 
S
ci
en
tis
t a
t Y
ah
oo
! R
es
ea
rc
h.
 H
is
 
re
se
ar
ch
 in
te
re
st
s 
in
cl
ud
e 
in
fo
rm
at
io
n 
re
tri
ev
al
, m
ac
hi
ne
 le
ar
ni
ng
, a
nd
 
co
m
pu
ta
tio
na
l l
in
gu
is
tic
s.
 H
e 
se
rv
es
 o
n 
th
e 
pr
og
ra
m
 c
om
m
itt
ee
s 
of
 A
C
L-
08
:H
LT
, 
A
A
A
I '
08
, J
C
D
L 
'0
8,
 C
IK
M
 '0
8 
an
d 
W
W
W
 '0
8,
 a
nd
 in
 th
e 
pa
st
 h
e 
se
rv
ed
 o
n 
th
e 
pr
og
ra
m
 c
om
m
itt
ee
s 
of
 A
A
A
I, 
E
M
N
LP
-C
oN
LL
, C
O
LI
N
G
-A
C
L,
 s
er
ve
d 
as
 a
 m
en
to
r 
at
 S
IG
IR
 '0
7,
 a
s 
w
el
l a
s 
re
vi
ew
ed
 p
ap
er
s 
fo
r A
C
M
 T
O
IT
, I
P
&
M
, J
N
LE
, C
A
C
M
, 
A
A
A
I, 
A
A
M
A
S
, W
W
W
 a
nd
 C
IK
M
. E
vg
en
iy
 e
ar
ne
d 
hi
s 
M
S
c 
an
d 
P
hD
 d
eg
re
es
 in
 
C
om
pu
te
r S
ci
en
ce
 fr
om
 th
e 
Te
ch
ni
on
 - 
Is
ra
el
 In
st
itu
te
 o
f T
ec
hn
ol
og
y.
  
V
an
ja
 J
os
ifo
vs
ki
 is
 a
 P
rin
ci
pa
l R
es
ea
rc
h 
S
ci
en
tis
t a
t Y
ah
oo
! R
es
ea
rc
h,
 w
he
re
 h
e 
ร
ʳˆ
ʳ଄
Δ
٥
ʳ˄
ˉʳ
଄
˔
˖˟
ˀ˃
ˋˍ
ʳ˛
˟˧
ʳˀʳ
˧̈
̇̂
̅˼˴
˿̆
˅˃
˃ˌ
˂˄
˂˅
ˈ
˻̇
̇̃
ˍ˂˂
̊
̊
̊
ˁ˿˼
́˺
ˁ̂
˻˼
̂ˀ
̆̇˴
̇˸
ˁ˸
˷̈
˂˴
˶˿
˃ˋ
˂˶
˹̇ˁ
˻̇
̀
˿
w
or
ks
 o
n 
se
ar
ch
 a
nd
 a
dv
er
tis
em
en
t t
ec
hn
ol
og
ie
s 
fo
r t
he
 In
te
rn
et
. H
e 
is
 c
ur
re
nt
ly
 
ex
pl
or
in
g 
de
si
gn
s 
fo
r t
he
 n
ex
t g
en
er
at
io
n 
ad
 p
la
ce
m
en
t p
la
tfo
rm
s 
fo
r c
on
te
xt
ua
l 
an
d 
se
ar
ch
 a
dv
er
tis
in
g.
 P
re
vi
ou
sl
y,
 V
an
ja
 w
as
 a
 R
es
ea
rc
h 
S
ta
ff 
M
em
be
r a
t t
he
 
IB
M
 A
lm
ad
en
 R
es
ea
rc
h 
C
en
te
r w
or
ki
ng
 o
n 
se
ve
ra
l p
ro
je
ct
s 
in
 d
at
ab
as
e 
ru
nt
im
e 
an
d 
op
tim
iz
at
io
n,
 fe
de
ra
te
d 
da
ta
ba
se
s,
 a
nd
 e
nt
er
pr
is
e.
 H
e 
ea
rn
ed
 h
is
 M
S
c 
de
gr
ee
 
fro
m
 th
e 
U
ni
ve
rs
ity
 o
f F
lo
rid
a 
at
 G
ai
ne
sv
ill
e,
 a
nd
 h
is
 P
hD
 fr
om
 th
e 
Li
nk
op
in
g 
U
ni
ve
rs
ity
 in
 S
w
ed
en
. V
an
ja
 p
ub
lis
he
d 
ov
er
 th
irt
y 
pe
er
 re
vi
ew
ed
 p
ub
lic
at
io
ns
, 
au
th
or
ed
 a
ro
un
d 
20
 p
at
en
t a
pp
lic
at
io
ns
, a
nd
 w
as
 o
n 
th
e 
pr
og
ra
m
 c
om
m
itt
ee
s 
of
 
W
W
W
, S
IG
IR
, I
C
D
E
, V
LD
B
 a
nd
 o
th
er
 m
aj
or
 c
on
fe
re
nc
es
 in
 th
e 
da
ta
ba
se
, 
in
fo
rm
at
io
n 
re
tri
ev
al
, a
nd
 s
ea
rc
h 
ar
ea
s.
  
B
o 
P
an
g 
is
 a
 R
es
ea
rc
h 
S
ci
en
tis
t a
t Y
ah
oo
! R
es
ea
rc
h.
 H
er
 p
rim
ar
y 
re
se
ar
ch
 
in
te
re
st
s 
ar
e 
in
 n
at
ur
al
 la
ng
ua
ge
 p
ro
ce
ss
in
g,
 m
ac
hi
ne
 le
ar
ni
ng
, a
nd
 in
fo
rm
at
io
n 
re
tri
ev
al
. S
he
 o
bt
ai
ne
d 
he
r P
hD
 in
 C
om
pu
te
r S
ci
en
ce
 fr
om
 C
or
ne
ll 
U
ni
ve
rs
ity
, 
w
he
re
 s
he
 w
or
ke
d 
on
 a
ut
om
at
ic
 a
na
ly
si
s 
of
 s
en
tim
en
t i
n 
te
xt
 a
nd
 p
ar
ap
hr
as
e 
ex
tra
ct
io
n 
an
d 
ge
ne
ra
tio
n 
in
 th
e 
co
nt
ex
t o
f m
ac
hi
ne
 tr
an
sl
at
io
n.
 S
he
 h
as
 s
er
ve
d 
on
 
th
e 
pr
og
ra
m
 c
om
m
itt
ee
s 
of
 A
C
L,
 H
LT
-N
AA
C
L,
 E
M
N
LP
, a
nd
 A
A
A
I, 
an
d 
re
vi
ew
ed
 
fo
r j
ou
rn
al
s 
in
cl
ud
in
g 
A
C
M
 T
O
IS
, J
M
LR
, J
A
IR
, C
om
pu
te
r S
pe
ec
h 
an
d 
La
ng
ua
ge
, 
an
d 
C
om
pu
ta
tio
na
l L
in
gu
is
tic
s.
  
ba
ck
 to
 to
p
B
ui
ld
in
g 
Pr
ac
tic
al
 S
po
ke
n 
D
ia
lo
g 
Sy
st
em
s 
(A
nt
oi
ne
 R
au
x,
 B
ria
n 
La
ng
ne
r, 
M
ax
in
e 
Es
ke
na
zi
, A
la
n 
B
la
ck
) 
A
bs
tr
ac
t:
Th
is
 tu
to
ria
l w
ill
 g
iv
e 
a 
pr
ac
tic
al
 d
es
cr
ip
tio
n 
of
 th
e 
fre
e 
so
ftw
ar
e 
C
ar
ne
gi
e 
M
el
lo
n 
O
ly
m
pu
s 
2 
S
po
ke
n 
D
ia
lo
g 
A
rc
hi
te
ct
ur
e.
 B
ui
ld
in
g 
re
al
 w
or
ki
ng
 d
ia
lo
g 
sy
st
em
s 
th
at
 
ar
e 
ro
bu
st
 e
no
ug
h 
fo
r t
he
 g
en
er
al
 p
ub
lic
 to
 u
se
 is
 d
iff
ic
ul
t. 
M
os
t f
re
qu
en
tly
, t
he
 
fu
nc
tio
na
lit
y 
of
 th
e 
co
nv
er
sa
tio
ns
 is
 s
ev
er
el
y 
lim
ite
d 
- d
ow
n 
to
 s
im
pl
e 
qu
es
tio
n-
an
sw
er
 p
ai
rs
. W
hi
le
 o
ff-
th
e-
sh
el
f t
oo
lk
its
 h
el
p 
th
e 
de
ve
lo
pm
en
t o
f s
uc
h 
si
m
pl
e 
sy
st
em
s,
 th
ey
 d
o 
no
t s
up
po
rt 
m
or
e 
ad
va
nc
ed
, n
at
ur
al
 d
ia
lo
gs
 n
or
 d
o 
th
ey
 o
ffe
r t
he
 
tra
ns
pa
re
nc
y 
an
d 
fle
xi
bi
lit
y 
re
qu
ire
d 
by
 c
om
pu
ta
tio
na
l l
in
gu
is
tic
 re
se
ar
ch
er
s.
 
H
ow
ev
er
, O
ly
m
pu
s 
2 
of
fe
rs
 a
 c
om
pl
et
e 
di
al
og
 s
ys
te
m
 w
ith
 a
ut
om
at
ic
 s
pe
ec
h 
re
co
gn
iti
on
 (S
ph
in
x)
 a
nd
 s
yn
th
es
is
 (S
A
P
I, 
Fe
st
iv
al
) a
nd
 h
as
 b
ee
n 
us
ed
, a
lo
ng
 w
ith
 
ร
ʳˇ
ʳ଄
Δ
٥
ʳ˄
ˉʳ
଄
˔
˖˟
ˀ˃
ˋˍ
ʳ˛
˟˧
ʳˀʳ
˧̈
̇̂
̅˼˴
˿̆
˅˃
˃ˌ
˂˄
˂˅
ˈ
˻̇
̇̃
ˍ˂˂
̊
̊
̊
ˁ˿˼
́˺
ˁ̂
˻˼
̂ˀ
̆̇˴
̇˸
ˁ˸
˷̈
˂˴
˶˿
˃ˋ
˂˶
˹̇ˁ
˻̇
̀
˿
Jo
hn
 B
lit
ze
r 
33
30
 W
al
nu
t S
tre
et
 
P
hi
la
de
lp
hi
a,
 P
A
 1
91
04
 
em
ai
l:
bl
itz
er
@
ci
s.
up
en
n.
ed
u
Jo
hn
 B
lit
ze
r i
s 
cu
rr
en
tly
 a
 P
hD
 s
tu
de
nt
 in
 c
om
pu
te
r s
ci
en
ce
 u
nd
er
 F
er
na
nd
o 
P
er
ei
ra
 a
t t
he
 U
ni
ve
rs
ity
 o
f P
en
ns
yl
va
ni
a.
 B
eg
in
ni
ng
 in
 F
eb
ru
ar
y 
20
08
, h
e 
w
ill 
be
 a
 
vi
si
tin
g 
re
se
ar
ch
er
 a
t M
ic
ro
so
ft 
R
es
ea
rc
h 
La
bs
 A
si
a,
 a
nd
 in
 A
ug
us
t 2
00
8,
 h
e 
w
ill
 
st
ar
t a
 p
os
iti
on
 a
s 
a 
po
st
do
ct
or
al
 fe
llo
w
 u
nd
er
 D
an
 K
le
in
 a
t t
he
 U
ni
ve
rs
ity
 o
f 
C
al
ifo
rn
ia
, B
er
ke
le
y.
 J
oh
n'
s 
re
se
ar
ch
 a
re
a 
is
 m
ac
hi
ne
 le
ar
ni
ng
 fo
r n
at
ur
al
 la
ng
ua
ge
 
pr
oc
es
si
ng
, w
ith
 a
 p
rim
ar
y 
fo
cu
s 
on
 u
ns
up
er
vi
se
d 
di
m
en
si
on
al
ity
 re
du
ct
io
n 
of
 te
xt
. 
R
ec
en
tly
, h
e 
ha
s 
w
or
ke
d 
on
 e
m
pi
ric
al
 a
nd
 th
eo
re
tic
al
 a
na
ly
se
s 
of
 s
tru
ct
ur
al
 
le
ar
ni
ng
 fo
r s
em
i-s
up
er
vi
se
d 
do
m
ai
n 
ad
ap
ta
tio
n.
 H
e 
ha
s 
be
en
 a
 te
ac
hi
ng
 a
ss
is
ta
nt
 
fo
r c
ou
rs
es
 in
 c
og
ni
tiv
e 
sc
ie
nc
e 
an
d 
nu
m
er
ic
al
 li
ne
ar
 a
lg
eb
ra
 a
t t
he
 U
ni
ve
rs
ity
 o
f 
P
en
ns
yl
va
ni
a.
X
ia
oj
in
 Z
hu
 
U
ni
ve
rs
ity
 o
f W
is
co
ns
in
, M
ad
is
on
 
12
10
 W
es
t D
ay
to
n 
S
tre
et
 
M
ad
is
on
, W
I 5
37
06
-1
68
5 
em
ai
l:
je
rr
yz
hu
@
cs
.w
is
c.
ed
u
X
ia
oj
in
 Z
hu
 is
 a
n 
A
ss
is
ta
nt
 P
ro
fe
ss
or
 in
 C
om
pu
te
r S
ci
en
ce
s 
at
 U
ni
ve
rs
ity
 o
f 
W
is
co
ns
in
, M
ad
is
on
. H
is
 re
se
ar
ch
 in
te
re
st
s 
ar
e 
st
at
is
tic
al
 m
ac
hi
ne
 le
ar
ni
ng
 (i
n 
pa
rti
cu
la
r s
em
i-s
up
er
vi
se
d 
le
ar
ni
ng
), 
an
d 
its
 a
pp
lic
at
io
ns
 to
 n
at
ur
al
 la
ng
ua
ge
 
an
al
ys
is
. H
e 
re
ce
iv
ed
 a
 P
h.
D
. i
n 
La
ng
ua
ge
 T
ec
hn
ol
og
ie
s 
fro
m
 C
M
U
 in
 2
00
5,
 w
ith
 
th
es
is
 re
se
ar
ch
 o
n 
gr
ap
h-
ba
se
d 
se
m
i-s
up
er
vi
se
d 
le
ar
ni
ng
. H
is
 c
ur
re
nt
 re
se
ar
ch
 
pr
oj
ec
ts
 a
im
 a
t b
rid
gi
ng
 th
e 
di
ffe
re
nt
 a
pp
ro
ac
he
s 
in
 s
em
i-s
up
er
vi
se
d 
le
ar
ni
ng
, a
nd
 
m
ak
in
g 
th
em
 m
or
e 
ef
fe
ct
iv
e 
fo
r p
ra
ct
iti
on
er
s.
 H
e 
ha
s 
ta
ug
ht
 s
ev
er
al
 g
ra
du
at
e 
an
d 
un
de
rg
ra
du
at
e 
co
ur
se
s 
in
 A
I, 
m
ac
hi
ne
 le
ar
ni
ng
 a
nd
 N
LP
 a
t t
he
 U
ni
ve
rs
ity
 o
f 
W
is
co
ns
in
, M
ad
is
on
.
ba
ck
 to
 to
p
A
dv
an
ce
d 
O
nl
in
e 
Le
ar
ni
ng
 fo
r N
at
ur
al
 L
an
gu
ag
e 
Pr
oc
es
si
ng
 
(K
ob
y 
C
ra
m
m
er
) 
ร
ʳˌ
ʳ଄
Δ
٥
ʳ˄
ˉʳ
଄
˔
˖˟
ˀ˃
ˋˍ
ʳ˛
˟˧
ʳˀʳ
˧̈
̇̂
̅˼˴
˿̆
˅˃
˃ˌ
˂˄
˂˅
ˈ
˻̇
̇̃
ˍ˂˂
̊
̊
̊
ˁ˿˼
́˺
ˁ̂
˻˼
̂ˀ
̆̇˴
̇˸
ˁ˸
˷̈
˂˴
˶˿
˃ˋ
˂˶
˹̇ˁ
˻̇
̀
˿
M
os
t r
es
ea
rc
h 
in
 m
ac
hi
ne
 le
ar
ni
ng
 h
as
 b
ee
n 
fo
cu
se
d 
on
 b
in
ar
y 
cl
as
si
fic
at
io
n,
 in
 
w
hi
ch
 th
e 
le
ar
ne
d 
cl
as
si
fie
r o
ut
pu
ts
 o
ne
 o
f t
w
o 
po
ss
ib
le
 a
ns
w
er
s.
 Im
po
rta
nt
 
fu
nd
am
en
ta
l q
ue
st
io
ns
 c
an
 b
e 
an
al
yz
ed
 in
 te
rm
s 
of
 b
in
ar
y 
cl
as
si
fic
at
io
n,
bu
t r
ea
l-
w
or
ld
 n
at
ur
al
 la
ng
ua
ge
 p
ro
ce
ss
in
g 
pr
ob
le
m
s 
of
te
n 
in
vo
lv
e 
ric
he
r o
ut
pu
t s
pa
ce
s.
 In
 
th
is
 tu
to
ria
l, 
w
e 
w
ill
 fo
cu
s 
on
 c
la
ss
ifi
er
s 
w
ith
 a
 la
rg
e 
nu
m
be
r o
f p
os
si
bl
e 
ou
tp
ut
s 
w
ith
 in
te
re
st
in
g 
st
ru
ct
ur
e.
 N
ot
ab
le
 e
xa
m
pl
es
 in
cl
ud
e 
in
fo
rm
at
io
n 
re
tri
ev
al
, p
ar
t-o
f-
sp
ee
ch
 ta
gg
in
g,
 N
P
 c
hu
ck
in
g,
 p
ar
si
ng
, e
nt
ity
 e
xt
ra
ct
io
n,
 a
nd
 p
ho
ne
m
e 
re
co
gn
iti
on
.
O
ur
 a
lg
or
ith
m
ic
 fr
am
ew
or
k 
w
ill
 b
e 
th
at
 o
f o
nl
in
e 
le
ar
ni
ng
, f
or
 s
ev
er
al
 re
as
on
s.
 F
irs
t, 
on
lin
e 
al
go
rit
hm
s 
ar
e 
in
 g
en
er
al
 c
on
ce
pt
ua
lly
 s
im
pl
e 
an
d 
ea
sy
 to
 im
pl
em
en
t. 
In
 
pa
rti
cu
la
r, 
on
lin
e 
al
go
rit
hm
s 
pr
oc
es
s 
on
e 
ex
am
pl
e 
at
 a
 ti
m
e 
an
d 
th
us
 re
qu
ire
 li
ttl
e 
w
or
ki
ng
 m
em
or
y.
 S
ec
on
d,
 o
ur
 e
xa
m
pl
e 
ap
pl
ic
at
io
ns
 h
av
e 
al
l b
ee
n 
tre
at
ed
 
su
cc
es
sf
ul
ly
 u
si
ng
 o
nl
in
e 
al
go
rit
hm
s.
 T
hi
rd
, t
he
 a
na
ly
si
s 
of
 o
nl
in
e 
al
go
rit
hm
s 
us
es
 
si
m
pl
er
 m
at
he
m
at
ic
al
 to
ol
s 
th
an
 o
th
er
 ty
pe
s 
of
 a
lg
or
ith
m
s.
 F
ou
rth
, t
he
 o
nl
in
e 
le
ar
ni
ng
 fr
am
ew
or
k 
pr
ov
id
es
 a
 v
er
y 
ge
ne
ra
l s
et
tin
g 
w
hi
ch
 c
an
 b
e 
ap
pl
ie
d 
to
 a
 
br
oa
d 
se
tti
ng
 o
f p
ro
bl
em
s,
 w
he
re
 th
e 
on
ly
 m
ac
hi
ne
ry
 a
ss
um
ed
 is
 th
e 
ab
ili
ty
 to
 
pe
rfo
rm
 e
xa
ct
 in
fe
re
nc
e,
 w
hi
ch
 c
om
pu
te
s 
a 
m
ax
im
a 
ov
er
 s
om
e 
sc
or
e 
fu
nc
tio
n.
Th
e 
go
al
s 
of
 th
e 
tu
to
ria
l: 
1.
To
 p
ro
vi
de
 th
e 
au
di
en
ce
 s
ys
te
m
at
ic
 m
et
ho
ds
 to
 d
es
ig
n,
 a
na
ly
ze
 a
nd
 
im
pl
em
en
t e
ffi
ci
en
tly
 le
ar
ni
ng
 a
lg
or
ith
m
s 
fo
r t
he
ir 
sp
ec
ifi
c 
co
m
pl
ex
-o
ut
pu
t 
pr
ob
le
m
s:
 fr
om
 s
im
pl
e 
bi
na
ry
 c
la
ss
ifi
ca
tio
n 
th
ro
ug
h 
m
ul
ti-
 c
la
ss
 c
at
eg
or
iz
at
io
n 
to
 in
fo
rm
at
io
n 
ex
tra
ct
io
n,
 p
ar
si
ng
 a
nd
 s
pe
ec
h 
re
co
gn
iti
on
.
2.
To
 in
tro
du
ce
 n
ew
 o
nl
in
e 
al
go
rit
hm
s 
w
hi
ch
 p
ro
vi
de
 s
ta
te
-o
f-t
he
-a
rt 
pe
rfo
rm
an
ce
 in
 p
ra
ct
ic
e 
ba
ck
ed
 b
y 
in
te
re
st
in
g 
th
eo
re
tic
al
 g
ua
ra
nt
ee
s.
  
Th
eo
ry
 a
nd
 A
lg
or
ith
m
s 
z
Th
e 
on
lin
e 
le
ar
ni
ng
 p
ar
ad
ig
m
z
M
aj
or
 c
on
ce
pt
s 
: l
os
s 
fu
nc
tio
n,
 la
rg
e 
m
ar
gi
n
z
Th
e 
pe
rc
ep
tro
n 
al
go
rit
hm
 a
nd
 v
ar
ia
nt
s 
 
z
Th
e 
pa
ss
iv
e-
ag
gr
es
si
ve
 a
pp
ro
ac
h
z
Th
e 
ge
ne
ra
l-m
ar
gi
n 
ex
te
ns
io
n 
to
 th
e 
pa
ss
iv
e-
ag
gr
es
si
ve
 a
pp
ro
ac
h 
fo
r 
co
m
pl
ex
 p
ro
bl
em
s
Im
pl
em
en
ta
tio
n 
an
d 
Pr
ac
tic
e 
z
A
pp
lic
at
io
ns
  
ร
ʳ˄
˃ʳ
଄
Δ
٥
ʳ˄
ˉʳ
଄
˔
˖˟
ˀ˃
ˋˍ
ʳ˛
˟˧
ʳˀʳ
˧̈
̇̂
̅˼˴
˿̆
˅˃
˃ˌ
˂˄
˂˅
ˈ
˻̇
̇̃
ˍ˂˂
̊
̊
̊
ˁ˿˼
́˺
ˁ̂
˻˼
̂ˀ
̆̇˴
̇˸
ˁ˸
˷̈
˂˴
˶˿
˃ˋ
˂˶
˹̇ˁ
˻̇
̀
˿
z
M
ul
ti-
cl
as
s 
m
ul
ti-
la
be
l t
ex
t c
la
ss
ifi
ca
tio
n
z
S
pe
ec
h 
pr
oc
es
si
ng
z
In
fo
rm
at
io
n 
ex
tra
ct
io
n
z
P
ar
si
ng
  
z
A
lte
rn
at
iv
e 
al
go
rit
hm
s 
: L
og
is
tic
 re
gr
es
si
on
, C
R
Fs
z
Im
pr
ov
em
en
ts
 : 
To
p-
k 
in
fe
re
nc
e,
 A
ve
ra
gi
ng
z
C
on
cl
us
io
n 
an
 Q
ue
st
io
ns
 fr
om
 th
e 
au
di
en
ce
K
ob
y 
C
ra
m
m
er
 is
 a
 re
se
ar
ch
 a
ss
oc
ia
te
 a
t t
he
 U
ni
ve
rs
ity
 o
f P
en
ns
yl
va
ni
a 
(P
hD
 
H
eb
re
w
 U
ni
ve
rs
ity
). 
H
is
 re
se
ar
ch
 fo
cu
se
s 
on
 th
e 
de
si
gn
, a
na
ly
si
s 
an
d 
im
pl
em
en
ta
tio
n 
of
 m
ac
hi
ne
 le
ar
ni
ng
 a
lg
or
ith
m
s 
fo
r c
om
pl
ex
 p
re
di
ct
io
n 
pr
ob
le
m
s,
 
an
d 
ap
pl
yi
ng
 th
em
 fo
r v
ar
io
us
 n
at
ur
al
 la
ng
ua
ge
 p
ro
ce
ss
in
g 
ta
sk
s 
an
d 
ot
he
r 
st
ru
ct
ur
ed
 p
ro
bl
em
s.
ba
ck
 to
 to
p
In
te
ra
ct
iv
e 
Vi
su
al
iz
at
io
n 
fo
r C
om
pu
ta
tio
na
l L
in
gu
is
tic
s 
(C
hr
is
to
ph
er
 C
ol
lin
s,
 G
er
al
d 
Pe
nn
, a
nd
 S
he
el
ag
h 
C
ar
pe
nd
al
e)
 
In
te
ra
ct
iv
e 
in
fo
rm
at
io
n 
vi
su
al
iz
at
io
n 
is
 a
n 
em
er
gi
ng
 a
nd
 p
ow
er
fu
l r
es
ea
rc
h 
te
ch
ni
qu
e 
fo
r u
nd
er
st
an
di
ng
 m
od
el
s 
of
 la
ng
ua
ge
, a
nd
 th
ei
r a
bs
tra
ct
 
re
pr
es
en
ta
tio
ns
. M
uc
h 
of
 w
ha
t c
om
pu
ta
tio
na
l l
in
gu
is
ts
 fa
ll 
ba
ck
 u
po
n 
to
 im
pr
ov
e 
na
tu
ra
l l
an
gu
ag
e 
pr
oc
es
si
ng
 a
nd
 to
 m
od
el
 la
ng
ua
ge
 "u
nd
er
st
an
di
ng
" i
s 
st
ru
ct
ur
e 
th
at
 h
as
, a
t b
es
t, 
on
ly
 a
n 
in
di
re
ct
 a
tte
st
at
io
n 
in
 o
bs
er
va
bl
e 
da
ta
. A
n 
im
po
rta
nt
 p
ar
t 
of
 re
se
ar
ch
 p
ro
gr
es
s 
de
pe
nd
s 
on
 o
ur
 a
bi
lit
y 
to
 fu
lly
 in
ve
st
ig
at
e,
 e
xp
la
in
, a
nd
 
ex
pl
or
e 
th
es
e 
st
ru
ct
ur
es
, b
ot
h 
em
pi
ric
al
ly
 a
nd
 a
s 
ou
tc
om
es
 o
f g
ra
m
m
ar
 d
es
ig
n 
re
la
tiv
e 
to
 a
cc
ep
te
d 
lin
gu
is
tic
 th
eo
ry
. T
he
 s
he
er
 c
om
pl
ex
ity
 o
f t
he
se
 a
bs
tra
ct
 
st
ru
ct
ur
es
, a
nd
 th
e 
ob
se
rv
ab
le
 p
at
te
rn
s 
on
 w
hi
ch
 th
ey
 a
re
 b
as
ed
, h
ow
ev
er
, u
su
al
ly
 
lim
its
 th
ei
r a
cc
es
si
bi
lit
y,
 o
fte
n 
ev
en
 to
 th
e 
re
se
ar
ch
er
s 
cr
ea
tin
g 
or
 a
tte
m
pt
in
g 
to
 
le
ar
n 
th
em
.
To
 a
id
 in
 u
nd
er
st
an
di
ng
, v
is
ua
l '
ex
te
rn
al
iz
at
io
ns
' a
re
 u
se
d 
in
 C
L 
fo
r p
re
se
nt
at
io
n
an
d 
ex
pl
an
at
io
n 
- t
ra
di
tio
na
l s
ta
tis
tic
al
 g
ra
ph
s 
an
d 
cu
st
om
-d
es
ig
ne
d 
da
ta
 
ill
us
tra
tio
ns
 fi
ll 
th
e 
pa
ge
s 
of
 A
C
L 
pa
pe
rs
. S
uc
h 
vi
su
al
iz
at
io
ns
 d
o 
pr
ov
id
e 
in
si
gh
t i
nt
o 
th
e 
re
pr
es
en
ta
tio
ns
 a
nd
 a
lg
or
ith
m
s 
de
si
gn
ed
 b
y 
re
se
ar
ch
er
s,
 b
ut
 v
is
ua
liz
at
io
n 
ca
n 
al
so
 b
e 
us
ed
 a
s 
an
 a
id
 in
 th
e 
pr
oc
es
s 
of
 re
se
ar
ch
 it
se
lf.
 T
he
re
 a
re
 s
pe
ci
al
 
ร
ʳ˄
˄ʳ
଄
Δ
٥
ʳ˄
ˉʳ
଄
˔
˖˟
ˀ˃
ˋˍ
ʳ˛
˟˧
ʳˀʳ
˧̈
̇̂
̅˼˴
˿̆
˅˃
˃ˌ
˂˄
˂˅
ˈ
˻̇
̇̃
ˍ˂˂
̊
̊
̊
ˁ˿˼
́˺
ˁ̂
˻˼
̂ˀ
̆̇˴
̇˸
ˁ˸
˷̈
˂˴
˶˿
˃ˋ
˂˶
˹̇ˁ
˻̇
̀
˿
st
at
is
tic
al
 m
et
ho
ds
, f
al
lin
g 
un
de
r t
he
 ru
br
ic
 o
f "
ex
pl
or
at
or
y 
da
ta
 a
na
ly
si
s"
, a
nd
 
vi
su
al
iz
at
io
n 
te
ch
ni
qu
es
 ju
st
 fo
r t
hi
s 
pu
rp
os
e,
 in
 fa
ct
, b
ut
 th
es
e 
ar
e 
no
t w
id
el
y 
us
ed
 
or
 e
ve
n 
kn
ow
n 
in
 C
L.
 T
he
se
 n
ov
el
 d
at
a 
vi
su
al
iz
at
io
n 
te
ch
ni
qu
es
, w
hi
ch
 w
e 
ha
ve
 
us
ed
 s
uc
ce
ss
fu
lly
 in
 th
e 
C
L 
do
m
ai
n,
 o
ffe
r t
he
 p
ot
en
tia
l f
or
 c
re
at
in
g 
ne
w
 m
et
ho
ds
 
th
at
 re
ve
al
 s
tru
ct
ur
e 
an
d 
de
ta
il 
in
 d
at
a.
 In
st
ru
ct
ed
 b
y 
a 
te
am
 o
f c
om
pu
ta
tio
na
l 
lin
gu
is
ts
 a
nd
 in
fo
rm
at
io
n 
vi
su
al
iz
at
io
n 
re
se
ar
ch
er
s,
 th
is
 tu
to
ria
l w
ill
 b
rid
ge
 
co
m
pu
ta
tio
na
l l
in
gu
is
tic
 a
nd
 in
fo
rm
at
io
n 
vi
su
al
iz
at
io
n 
ex
pe
rti
se
, p
ro
vi
di
ng
 
at
te
nd
ee
s 
w
ith
 a
 b
as
is
 fr
om
 w
hi
ch
 th
ey
 c
an
 b
eg
in
 to
 a
cc
el
er
at
e 
th
ei
r o
w
n 
re
se
ar
ch
. 
Tu
to
ria
l O
bj
ec
tiv
es
: 
Th
is
 tu
to
ria
l w
ill
 e
qu
ip
 p
ar
tic
ip
an
ts
 w
ith
:
z
A
n 
un
de
rs
ta
nd
in
g 
of
 th
e 
im
po
rta
nc
e 
an
d 
ap
pl
ic
ab
ili
ty
 o
f i
nf
or
m
at
io
n 
vi
su
al
iz
at
io
n 
te
ch
ni
qu
es
 to
 c
om
pu
ta
tio
na
l l
in
gu
is
tic
s 
re
se
ar
ch
;
z
K
no
w
le
dg
e 
of
 th
e 
ba
si
c 
pr
in
ci
pl
es
 o
f i
nf
or
m
at
io
n 
vi
su
al
iz
at
io
n 
th
eo
ry
;
z
Th
e 
ab
ili
ty
 to
 id
en
tif
y 
ap
pr
op
ria
te
 v
is
ua
liz
at
io
n 
so
ftw
ar
e 
an
d 
te
ch
ni
qu
es
 th
at
 
ar
e 
av
ai
la
bl
e 
fo
r i
m
m
ed
ia
te
 u
se
 a
nd
 fo
r p
ro
to
ty
pi
ng
;  
z
A
 w
or
ki
ng
 k
no
w
le
dg
e 
of
 re
se
ar
ch
 to
 d
at
e 
in
 th
e 
ar
ea
 o
f l
in
gu
is
tic
 in
fo
rm
at
io
n 
vi
su
al
iz
at
io
n.
  
Tu
to
ria
l O
ut
lin
e:
 
1.
In
tro
du
ct
io
n 
 
2.
In
fo
rm
at
io
n 
V
is
ua
liz
at
io
n 
Th
eo
ry
 
{
R
ep
re
se
nt
at
io
na
l t
he
or
y,
 c
og
ni
tiv
e 
ps
yc
ho
lo
gy
, p
re
at
te
nt
iti
ve
 
pr
oc
es
si
ng
  
{
In
te
ra
ct
io
n 
&
 a
ni
m
at
io
n 
 
{
A
ss
es
si
ng
 a
nd
 v
al
id
at
in
g 
vi
su
al
iz
at
io
n 
i. 
E
va
lu
at
io
n 
ch
al
le
ng
es
ii.
 M
ea
su
rin
g 
in
si
gh
t  
iii
. M
et
ric
s 
fo
r e
va
lu
at
io
n 
 
iv
. H
eu
ris
tic
 a
pp
ro
ac
he
s 
to
 e
va
lu
at
io
n
3.
R
ev
ie
w
 o
f L
in
gu
is
tic
 V
is
ua
liz
at
io
n 
{
D
oc
um
en
t c
on
te
nt
 v
is
ua
liz
at
io
ns
  
{
Te
xt
 c
ol
le
ct
io
n 
an
al
ys
is
{
Li
te
ra
ry
 a
na
ly
si
s 
 
{
S
tre
am
in
g 
da
ta
 v
is
ua
liz
at
io
n
{
C
on
ve
rg
en
ce
 o
f l
an
gu
ag
e 
an
d 
ot
he
r d
at
a
ร
ʳ˄
˅ʳ
଄
Δ
٥
ʳ˄
ˉʳ
଄
˔
˖˟
ˀ˃
ˋˍ
ʳ˛
˟˧
ʳˀʳ
˧̈
̇̂
̅˼˴
˿̆
˅˃
˃ˌ
˂˄
˂˅
ˈ
˻̇
̇̃
ˍ˂˂
̊
̊
̊
ˁ˿˼
́˺
ˁ̂
˻˼
̂ˀ
̆̇˴
̇˸
ˁ˸
˷̈
˂˴
˶˿
˃ˋ
˂˶
˹̇ˁ
˻̇
̀
˿
附件二 
ACL 2008 工作坊簡介
附件三 
 Yorick Wilks 演講資料 
Wilks On Whose Shoulders?
I.1 ((*ANI 1)((SELF IN)(MOVE CAUSE))(*REAL 2))→(1(*JUDG) 2)
Or, in semi-English:
[animate-1 cause-to-move-in-self real-object-2]→[1 *judges 2]
I.2 (1 BE (GOOD KIND))↔((*ANI 2) WANT 1)
Or, again:
[1 is good]↔[animate-2 wants 1]
Figure 1
Inference rules in Preference Semantics.
out with the machines then available, but which I now choose to see as wanting to
do Information Extraction: though, of course, it was Naomi Sager who did IE ﬁrst on
medical texts at NYU (see Sager and Grishman 1975).
At Stanford as a post-doc, I was on the same corridor asWinograd, just arrived from
MIT; Schank, then starting to build his Conceptual Dependency empire; and Colby and
his large team building the PARRY dialogue system, which included Larry Tesler, later
the Apple software architect. Schank and I agreed on far more than we disagreed on and
saw that wewould be stronger together than separately, but neither of us wanted to give
up our notation: He realized, rightly, that there was more persuasive power in diagrams
than in talk of processes like “preference.” It was an extraordinary period, when AI and
NLP were probably closer than ever before or since: Around 1972 Colmerauer passed
though the Stanford AI Lab, describing Prolog for the ﬁrst time but, as you may or may
not remember, as a tool for machine translation! I spent my time there deﬁning and
expanding the coherence-based semantics underlying my thesis, calling it “Preference
Semantics” (PS), adding larger scale structures such as inference rules (see Figure 1)
and thesauri, and building it into the core of a small semantics-based English-to-French
machine translation system programmed in LISP. At one point the code of this MT
system ended up in the Boston Computer Museum, but I have no idea where it is now.
The principles behind PS were as follows:
 an emphasis on processes, not diagrams;
 the notion of afﬁnity and repulsion between sense representations
(cf. Waltz and Pollack’s WSD connectionism [1985]);
 seeking the “best ﬁt” interpretation—the one with most satisﬁed
preferences (normally of verbs, prepositions and adjectives);
 yielding the least informative/effort interpretation;
 using no explicit syntax, only segmentation and order of items;
 meaningfulness as being connected to a unique interpretation/sense
choice;
 meaning seen as represented in other words, since no other equivalent for
the notion works (e.g., objects or concepts);
 gists or templates of utterances as core underlying entities; and
 there is no correct interpretation or set of primitive concepts, only the best
available.
5
Computational Linguistics Volume 34, Number 4
One could put some of these, admittedly programmatic and imprecise, points as
follows:
 Semantics is not necessarily deep but also superﬁcial (see more recent
results on the interrelations between WSD, POS, and IE, e.g. Stevenson
and Wilks [2001]).
 Quantitative phenomena are unavoidable in language: John McCarthy
thought they had no place anywhere in AI, except perhaps in low-level
computer vision.
 Reference structures (like lexicons) are only temporary snapshots of a
language in a particular state (of expansion or contraction).
 What is important is to locate the update mechanism of language,
including crucially the creation of new word senses, which is not
Chomsky’s sense of the creativity of language.
Constructible Belief Systems
I returned to Europe in the mid 1970s, ﬁrst to the ISSCO institute in Lugano, where
Charniak was and Schank had just left, and then to Edinburgh as a visitor before taking
a job at Essex. I began a long period of interest in belief systems, in particular seeking
some representation of the beliefs of others, down to any required degree of nesting—
for example A’s belief about B’s belief about C—that could be constructed recursively
at need, rather than being set out in advance, as in the pioneering systems emerging
from the Toronto group under Ray Perrault (Allen and Perrault 1980). I began thinking
about this with Janusz Bien of the University of Warsaw, who had also published a
paper arguing that CL/NLP should consider “least effort” methods: in the sense that
the brain might well, due to evolution, be a lazy processor and seek methods for
understanding that minimized some value that could be identiﬁed with processing
effort. I had argued in PS for choosing shortest chains of inferences between templates,
and that the most connected/preferred template structure for a piece of text should be
the one found ﬁrst. I am not sure we ever proved any of this: It was just speculation,
as was the preference for the most semantically connected representation, and the
representation with the least information. All this is really only elementary information
theory: a random string of words contains the maximum information, but that is not
very helpful. Clearly, the preferred interpretation of “He was named after his father”
(i.e., named the same rather than later in time) is not the least informative, since the latter
contains no information at all—being necessarily true—so one would have to adapt
any such slogan to: “prefer the interpretation with the least information, unless it is
zero!”
The belief work, ﬁrst with Bien, later with Afzal Ballim (Wilks and Ballim 1987)
and John Barnden, has not been a successful paradigm in terms of take-up, in that
it has not got into the general discourse, even in the way that Fauconnier’s “Mental
Spaces” (Fauconnier 1985) has. That approach uses the same spatial metaphor, but for
strictly linguistic rather than belief and knowledge purposes. But I think the VIEWGEN
belief paradigm, as it became, had virtues, and I want to exploit this opportunity to
remind people of it. It was meant to capture the intuition that if we want, for language
6
Wilks On Whose Shoulders?
understanding purposes, to construct X’s beliefs about Y’s beliefs—what I called the
environment of Y-for-X—then:
1. It must be a construction that can be done in real time to any level of
nesting required, because we cannot imagine it pre-stored for all future
nestings, as Perrault el al. in effect assumed.
2. It must capture the intuition that much of our belief is accepted by default
from others: As VIEWGEN expresses it, I will accept as a belief what you
say, because I have normally no way of checking, or experimenting on, let
alone refuting, the things you tell me, e.g. that you had eggs for breakfast
yesterday. As someone in politics once put it, “There is no alternative.”
Unless, that is, what you say contradicts something I believe or can easily
prove from what I believe.
3. We must be able to maintain apparently contradictory beliefs, provided
they are held in separate spaces and will never meet as contradictions. I
can thus maintain within my-space-for-you beliefs of yours (according to
me) that I do not in fact hold.
In VIEWGEN, belief construction is done in terms of a “push down” metaphor: A
permeable “container” of your beliefs is pushed into a ”container” of my beliefs and
what percolates through the membrane, from me to you, will be believed and ascribed
to you, unless it is explicitly contradicted, namely, by some contrary belief I already
ascribe to you, and which, as it were, keeps mine from percolating through. The idea
is to construct the appropriate “inner belief space” at the relevant level of nesting, so
that inference can be done, and to derive consequences (within that constrained content
space) that also serve to model, in this case, you the belief holder in terms of goals
and desires, in addition to beliefs. This approach is quite different not only from the
Perrault/Toronto system of belief-relevant plans but also to AI theories that make use
of sets-of-support premises since this is about belief-inheritance-by-default. It is also
quite distinct from linguistic theories likeWilson and Sperber’s Relevance Theorywhich
take no account at all of belief as relative to individuals, but perform all operations
in some space that is the same for everyone, which is an essentially Chomskyan ideal
competence-style notion of belief that is not relative to individuals—which is of course
absurd.
Mark Lee and a number of my students have created implementations of this
approach and linked it to dialogue and other applications, but there has been no major
application showing its essential role in a functioning conversational theory where
complex belief states are created in real time. However, the ﬁeld is, I believe, now
moving in that direction (e.g., with POMDP theories [Williams and Young 2007]) since
the possibility of populating belief theories with a realistic base from text by means of
Information Extraction or Semantic Web parsing to RDF format is now real (a matter we
shall return to subsequently).
There were, for me at least, two connections between the VIEWGEN belief work
and Preference Semantics, in terms of meaning and its relation to processes. First,
there was the role of choice and alternatives, crucial to PS, in that an assigned mean-
ing interpretation for a text was no more than a choice of the best available among
alternatives, because preference implies choice, in a way that generative linguistics—
though not of course traditions like Halliday’s—always displayed alternatives but
considered choice between them a matter for mere performance. What was dispensable
7
Computational Linguistics Volume 34, Number 4
to generative linguistics was the heart of the matter, I argued, to NLP/CL. Secondly,
VIEWGEN suggested a view of meaning, consistent locally with PS, dependent on
which individuals or classes one chose to see in terms of each other—the key notion
here was seeing one thing as another and its consequences for meaning. So, if one chose
to identify (as being the same person under two names) Joe (and what one believed
about him) with Fred’s father (and what one knew about him), the hypothesis was that
a belief environment should be constructed for Joe-as-Fred’s-father by percolating one
set of beliefs into the other, just as was done by the basic algorithm for creating A’s-
beliefs-about-B’s-beliefs from the component beliefs of A and B. This process created
a hybrid entity, with intensional meaning captured by the set of propositions in that
inner environment of belief space, but which was now neither Joe nor Fred’s father but
rather the system’s point of view of their directional amalgamation: Joe-as-Fred’s-father
(which might contain different propositions from the result of Fred’s-father-as-Joe).
More natural, and fundable, scenarios were constructed for this technique in those
days, such as knowledge representations for Navy ships’ captains genuinely uncertain
as to whether ship-in-my-viewﬁnder-now was or was not to be identiﬁed with the
stored representation for enemy-ship-number-X. The important underlying notion was
one going back to Frege, and which ﬁrst had an outing in Winograd’s thesis (Winograd
1972), where he showed you could have representations for blocks that did not in fact
exist on the Blocks World table. A semantics must be able to represent things without
knowing whether they exist or not; that is a basic requirement.
Later, and working with John Barnden and Afzal Ballim, this same underly-
ing process of conﬂating two belief objects was extended to the representation of
“metaphorical objects,” which could be described, quite traditionally in the literature,
as A-viewed-as-B (e.g., an atom viewed as a billiard ball). The metaphorical object
atom-as-billiard-ball was again created by the same push-down or fusion of belief sets
as in the basic belief point-of-view procedure. All this may well have been fanciful,
and was never fully exploited in published work with programs, but it did have a
certain intellectual appeal in wanting to treat belief, points of view, metaphor and
identiﬁcation of intensional individuals—normally quite separate issues in semantics—
as being modellable by the same simple underlying process (see Ballim, Wilks, and
Barnden 1991). One novel element that did emerge from this analysis was that, in
the construction of these complex intensional identiﬁcations, such as between “today’s
Wimbledon winner” and “the top male tennis seed,” one could choose directions of
“viewing as” with the belief sets that led to objects which were neither the classic de re
nor de dicto outcomes: Those became just two among a range of choices, and the others
of course had no handy Latin names.
Adapting to the “Empirical Wave” in NLP
For me, as with many others, especially in Europe, the beginning of the empirical wave
in NLP was the work of Leech and his colleagues at Lancaster: CLAWS4 (a name which
hides a UK political joke), their part-of-speech tagger based on large-scale annotation of
corpora. Such tagging is now the standard ﬁrst stage of almost every NLP process and it
may be hard for some to realize the skepticsm its arrival provoked: ”What could anyone
want that for?” was a common reaction from those still preoccupied by computational
syntax or semantics. That system was sold to IBM, whose speech group, under Jelinek,
Mercer, and Brown, subsequently astonished the CL/NLP world with their statistical
machine translation system CANDIDE. I wrote critical papers about it at the time, not
totally unconnected to the fact that I was funded by DARPA on the PANGLOSS project
8
Wilks On Whose Shoulders?
the President is,” as if most of the world’s population did! PARRY was in fact a semi-
refutation of the claim that you need knowledge to understand and converse, because
it plainly knew nothing; what it had was primitive “intentionality,” in the sense that it
had things “it wanted to say.”
My own introduction to practical work on dialogue was when I was contacted in
the late 1990s by David Levy, who had written 40 books on chess and ran a company
that made chess machines. He already had a footnote in AI as the man who had bet
McCarthy, Michie, and other AI leaders that a chess machine would not beat him within
ten years, and he won the bet more than once. In the 1990s he conceived a desire to win
the Loebner Prize2 for the best dialogue program of the year, and came to us at Shefﬁeld
to fund a team to win it for him, which we did in 1997. I designed the system and drew
upon my memories of PARRY, along with obvious advances in the role of knowledge
bases and inference, and the importance of corpora and machine learning. For example,
we took the whole set of winning Loebner dialogues off the Web so as to learn the kinds
of things that the journalist-testers actually said to the trial systems to see if they were
really humans or machines.
Our system, called CONVERSE (see Levy et al. 1997), claimed to be Catherine, a
34-year old female British journalist living in New York, and it owed something to
PARRY, certainly in Catherine’s desire to tell people things. It was driven by frames
corresponding to each of about 80 topics that such a person might want to discuss;
death, God, clothes, make-up, sex, abortion, and so on. It was far too top–down and
unwilling to shift from topic to topic but it could seem quite smart on a good day, and
probably won because we had built in news from the night before the competition of
a meeting Bill Clinton had had that day at the White House with Ellen de Generes, a
lesbian actress. This gave a certain immediacy to the responses intended to sway the
judges, as in “Did you see that meeting Ellen had with Clinton last night?”
This was all great fun and gave me an interest in modeling dialogue that has
persisted for a decade and is now exercised through COMPANIONS (Wilks 2004), a
large EU 15-site four-year project that I run. COMPANIONS aims to change the way we
think about the relationships of people to computers and the Internet by developing a
virtual conversational “Companion.” This will be an agent or “presence” that stays with
the user for long periods of time, developing a relationship and “knowing” its owner’s
preferences and wishes. It will communicate with the user primarily by using and un-
derstanding speech, but also using other technologies such as touch screens and sensors.
Another general motivation for the project is the belief that the current Internet
cannot serve all social groups well, and it is one of our objectives to empower citizens
(including the non-technical, the disabled, and the elderly) with a new kind of interface
based on language technologies. The vision of the Senior Companion—currently our
main prototype—is that of an artiﬁcial agent that communicates with its user on a
long-term basis, adapting to their voice, needs, and interests: A companion that would
entertain, inform, and react to emergencies. It aims to provide access to information
and services as well as company for the elderly by chatting, remembering past con-
versations, and organizing (and making sense of) the owner’s photographic and image
memories. This Companion would assume a user with a low level of technical knowl-
edge, and who might have lost the ability to read or produce documents themselves
unaided, but who might need help dealing with letters, messages, bills, and getting in-
formation from the Internet. During its conversations with its user or owner, the system
2 See http://www.loebner.net/Prizef/loebner-prize.html.
13
Computational Linguistics Volume 34, Number 4
builds up a knowledge inventory of family relations, family events in photos, places
visited, and so on. This knowledge base is currently stored in RDF, the Semantic Web
format, which has two advantages: ﬁrst, a very simple inference scheme with which
to drive further conversational inferences, and second, the possibility, not yet fulﬁlled,
of accessing arbitrary amounts of world information from Wikipedia, already available
in RDF, which could not possibly have been pre-coded in the dialogue manager, nor
elicited in a conversation of reasonable length. So, if the user says a photo was taken in
Paris, the Companion should be able to ask a question about Paris without needing that
knowledge pre-coded, but only using rapidly accessed Wikipedia RDFs about Paris. An
ultimate aim of this aspect of the Senior Compantion is the provision of a life narrative,
an assisted autobiography for everyone, one that could be given to relatives later if the
owner chose to leave it to them. There is a lot of technical stuff in the Senior Companion:
script-like structures—called DAFs or Dialogue Action Forms—designed to capture the
course of dialogues on speciﬁc topics or individuals or images, and these DAFs we are
trying to learn from tiled corpora. The DAFs are pushed and popped on a single stack,
and that simple virtual machine is the Dialogue Manager, where DAFs being pushed,
popped, or reentered at a lower stack point are intended to capture the exits from, and
returns to, abandoned topics and the movement of conversational initiative between
the system and the user. We are halfway through the project and currently have two
prototype Companions: The other, based not at Shefﬁeld but at Tampere, is a Health
and Fitness Companion (HFC).3 It is more task-oriented than the Senior Companion
and aims to advise on exercise and diet. The HFC is on a mobile phone architecture as
well as a PC, andwemay seek to combine the two prototypes later. The central notion of
a Companion is that of the same “personality,” with its memory and voice being present
no matter what the platform. It is not a robot, and could be embodied later in something
like a chatty furry handbag, being held on a sofa and perhaps reminding you about the
previous episodes of your favorite TV program.
Finale
This article has had something of the form of a life story, and everyone wants to believe
their life is some kind of narrative rather than a random chase from funding agency to
funding agency, with occasional pauses to carry out a successful proposal. But let us
return to Newton for a moment in closing; for us in CL he is the great counter-example,
of why we do not do science or engineering in that classic solitary manner:
. . .where the statue stood
Of Newton, with his prism and silent face,
The marble index of a mind for ever
Voyaging through strange seas of Thought, alone.
— William Wordsworth (1770–1850)
The Prelude, book iii, line 61
The emphasis there for me is on alone, which is pretty much unthinkable in our research
world of teams and research groups. Our form of research is essentially corporate and
cooperative; we may not be sure whose shoulders we are standing on, but we know
whose hands we are holding. I have worked in such a way since my thirties and, at
3 An early demo of a Companion can be seen on YouTube at
http://www.youtube.com/watch?v=SqIP6sTt1Dw.
14
Wilks On Whose Shoulders?
Shefﬁeld, my work would not have been possible without a wide range of colleagues
and former students in the NLP group there over many years and including Louise
Guthrie, Rob Gaizauskas, Hamish Cunningham, Fabio Ciravegna, Mark Stevenson,
Mark Hepple, Kalina Bontcheva, Christopher Brewster, Nick Webb and many others. In
recent years, what one could call “DARPA culture”—of competitions and cooperation
subtlymixed—aswell as the great repositories of software and data like LDC and ELRA,
have gone a long way to mitigate the personal and group isolation in the ﬁeld.
But we do have to face the fact that, in many ways, we do not do classic science: We
have no Newtons and will never have any. That is not to deny that we need real ideas
and innovations, and now may be a time for fresh ones. We have stood on the shoulders
of Fred Jelinek, Ken Church, and others for nearly two decades now, and the strain is
beginning to tell as papers still strive to gain that extra 1% in their scores on some small
task. We know that some change is in the air and I have tried to hint in this article as
to some of the places where that might be, even if that will mean a partial return to
older, unfashionable, ideas; for there is nothing new under the sun. But locating them
and exploiting them will not be in my hands but in yours, readers of Computational
Linguistics!
Acknowledgments
First of course to all those who have worked
with me over many years and to whom I
owe so much, particularly in connection
with this award. Then to my current sponsor:
This work was funded by the Companions
project (www.companions-project.org)
sponsored by the European Commission as
part of the Information Society Technologies
(IST) programme under EC grant number
IST-FP6-034434.
References
Allen, James F. and C. Raymond Perrault.
1980. Analyzing intention in utterances.
Artiﬁcial Intelligence, 15:143–178.
Ballim, Afzal, Yorick Wilks, and John A.
Barnden. 1991. Belief ascription, metaphor,
and intensional identiﬁcation. Cognitive
Science, 15(1):133–171.
Berners-Lee, T., J. Hendler, and O. Lassila.Q1
2001. The semantic web. Scientiﬁc
American, 28–37.
Braithwaite, Richard Bevan. 1953.
Scientiﬁc Explanation. A Study of the
Function of Theory, Probability and Law in
Science. Cambridge University Press,
Cambridge, UK.
Carnap, Rudolf. 1937. The Logical Syntax of
Language. Kegan Paul, London.
Fauconnier, Gilles. 1985. Mental Spaces.
Cambridge University Press,
Cambridge, UK.
Iria, Jose´, Christopher Brewster, Fabio
Ciravegna, and Yorick Wilks. 2006. An
incremental tri-partite approach to
ontology learning. In Proceedings of the
Language Resources and Evaluation
Conference (LREC-06), 22–28 May.
Jones, Karen Spa¨rck. 1986. Synonymy and
semantic classiﬁcation. Edinburgh
University Press, Edinburgh, Scotland.
Jones, Karen Spa¨rck. 2003. Document Q2
retrieval: Shallow data, deep theories;
historical reﬂections, potential directions.
In Advances in Information Retrieval, Lecture
Notes in Computer Science. Springer,
Berlin/Heidelberg.
Lenat, Douglas B. 1995. CYC: A large-scale
investment in knowledge infrastructure.
Communications of the ACM, 38(11):33–38.
Levy, D., R. Catizone, B. Battacharia, Q3
A. Krotov, and Y. Wilks. 1997. Converse: A
conversational companion.
Masterman, Margaret. 1961. Semantic Q4
message detection for machine
translation, using an interlingua. In
Proceedings of the First International
Conference on Machine Translation of
Languages and Applied Language Analysis,
pages 438–475. HMSO.
Masterman, Margaret, Yorick Wilks,
Branimir Boguraev, Steven Bird, Don
Hindle, Martin Kay, David McDonald, and
Hans Uszkoreit. 2005. Language, Cohesion
and Form (Studies in Natural Language
Processing). Cambridge University Press,
New York.
McCarthy, J. and P. J. Hayes. 1969. Some
philosophical problems from the
standpoint of artiﬁcial intelligence. In
B. Meltzer and D. Michie, editors, Machine
Intelligence, volume 4. Edinburgh
University Press, Edinburgh,
pages 463–502.
15
Computational Linguistics Volume 34, Number 4
Nirenburg, Sergei and Yorick Wilks. 2000.
Machine translation. Advances in
Computers, 52:160–189.
Sager, Naomi and Ralph Grishman.
1975. The restriction language for
computer grammars of natural language.
Communications of the ACM,
18(7):390–400.
Schank, Roger C. 1975. Conceptual Information
Processing. Elsevier Science Inc., New York.
Stevenson, Mark and Yorick Wilks. 2001. The
interaction of knowledge sources in word
sense disambiguation. Computational
Linguistics, 27(3):321–349.
Waltz, David L. and Jordan B. Pollack.
1985. Massively parallel parsing: A
strongly interactive model of natural
language interpretation. Cognitive Science,
9(1):51–74.
Wilks, Y. 1975. Preference semantics.
In E. L. Keenan, editor, Formal Semantics of
Natural Language. Cambridge University
Press, Cambridge, pages 329–348.
Wilks, Yorick. 1971. Decidability and natural
language. Mind, 80:497–520.
Wilks, Yorick. 1972. Grammar, Meaning and
Machine Analysis of Language. Routledge
and Kegan Paul, London.
Wilks, Yorick. 1996. Statistical versus
knowledge-based machine translation.
IEEE Expert: Intelligent Systems and Their
Applications, 11(2):12–18.
Wilks, Yorick. 1997. Information extraction as Q5
a core language technology. In International
Summer School on Information Extraction: A
Multidisciplinary Approach to an Emerging
Information Technology, volume 1299 of
Lecture Notes In Computer Science,
pages 1–9.
Wilks, Yorick. 2004. Artiﬁcial companions. In
Machine Learning for Multimodal Interaction:
First International Workshop, pages 36–45.
Wilks, Yorick. 2008. The semantic web:
Apotheosis of annotation, but what are its
semantics? IEEE Intelligent Systems,
23(3):41–49.
Wilks, Yorick and Afzal Ballim. 1987. Q6
Multiple agents and the heuristic
ascription of belief. In Proceedings of the
International Joint Conference Artiﬁcial
Intelligence (IJCAI-87), pages 118–124.
Williams, Jason D. and Steve Young. 2007.
Partially observable Markov decision
processes for spoken dialog systems.
Computer Speech and Language,
21(2):393–422.
Winograd, Terry. 1972. Understanding Natural
Language. Academic Press, Orlando, FL.
Wittgenstein, Ludwig. 1973. Philosophical Q7
Investigations. Blackwell Publishers.
16
ˠ̂́˷˴̌ʿʳ˝̈́˸ʳ˄ˉʿʳ˅˃˃ˋʳ
ˌˍ˃˃Ωˌˍ˄˃ʳˢ̃˸́˼́˺ʳ˦˸̆̆˼̂́ʳʳ
ˌˍ˄˃Ω˄˃ˍ˄˃ʳ˜́̉˼̇˸˷ʳ˧˴˿˾ˍʳˠ˴̅˶ʳ˦̊˸̅̇̆ʿʳʵ˙˴˶˼˴˿ʳ˘̋̃̅˸̆̆˼̂́̆ʳ˼́ʳ˛̈̀˴́ˀ˛̈̀˴́ʳ
˴́˷ʳ˛̈̀˴́ˀˠ˴˶˻˼́˸ʳ˜́̇˸̅˴˶̇˼̂́̆ʵʳʳ
˄˃ˍ˄˃Ω˄˃ˍˇ˃ʳ˕̅˸˴˾ʳʳ
˦˸̆̆˼̂́ʳ˄˔ˍʳ˜́˹̂̅̀˴̇˼̂́ʳ˘̋̇̅˴˶̇˼̂́ʳ˄ʳ
ʳ
˄˃ˍˇ˃Ω˄˄ˍ˃ˈˍʳ˥˼˶˻̀˴́ʿʳ˔˿˸̋˴́˷˸̅ʳ˘ˁˎʳˣ˴̇̅˼˶˾ʳ˦˶˻̂́˸ʳˠ˼́˼́˺ʳ˪˼˾˼ʳ˥˸̆̂̈̅˶˸̆ʳ˹̂̅ʳˠ̈˿̇˼˿˼́˺̈˴˿ʳˡ˴̀˸˷ʳ˘́̇˼̇̌ʳ˥˸˶̂˺́˼̇˼̂́ʳʳ
˄˄ˍ˃ˈΩ˄˄ˍˆ˃ˍʳ˕˸̅˺̆̀˴ʿʳ˦˻˴́˸ˎʳ˗˸˾˴́˺ʳ˟˼́ˎʳ˥˴́˷̌ʳ˚̂˸˵˸˿ʳ˗˼̆̇̅˼˵̈̇˼̂́˴˿ʳ˜˷˸́̇˼˹˼˶˴̇˼̂́ʳ̂˹ʳˡ̂́ˀ˥˸˹˸̅˸́̇˼˴˿ʳˣ̅̂́̂̈́̆ʳʳ
˄˄ˍˆ˃Ω˄˄ˍˈˈˍʳˣ˴̆˶˴ʿʳˠ˴̅˼̈̆ˎʳ˕˸́˽˴̀˼́ʳ˩˴́ʳ˗̈̅̀˸ʳ˪˸˴˾˿̌ˀ˦̈̃˸̅̉˼̆˸˷ʳ˔˶̄̈˼̆˼̇˼̂́ʳ̂˹ʳˢ̃˸́ˀ˗̂̀˴˼́ʳ˖˿˴̆̆˸̆ʳ˴́˷ʳ˖˿˴̆̆ʳ
˔̇̇̅˼˵̈̇˸̆ʳ˹̅̂̀ʳ˪˸˵ʳ˗̂˶̈̀˸́̇̆ʳ˴́˷ʳˤ̈˸̅̌ʳ˟̂˺̆ʳʳ
˄˄ˍˈˈΩ˄˅ˍ˅˃ˍʳ˕˴́˾̂ʿʳˠ˼˶˻˸˿˸ˎʳˢ̅˸́ʳ˘̇̍˼̂́˼ʳ˧˻˸ʳ˧̅˴˷˸̂˹˹̆ʳ˕˸̇̊˸˸́ʳˢ̃˸́ʳ˴́˷ʳ˧̅˴˷˼̇˼̂́˴˿ʳ˥˸˿˴̇˼̂́ʳ˘̋̇̅˴˶̇˼̂́ʳʳ
˦˸̆̆˼̂́ʳ˄˕ˍʳ˟˴́˺̈˴˺˸ʳ˥˸̆̂̈̅˶˸̆ʳ˴́˷ʳ˘̉˴˿̈˴̇˼̂́ʳ
ʳ
˄˃ˍˇ˃Ω˄˄ˍ˃ˈˍʳˠ΀̅̂̉̆˾˯ʺ̌ʿʳ˝˼̅΀ʳˣ˗˧ʳ˅ˁ˃ʳ˥˸̄̈˼̅˸̀˸́̇̆ʳ̂́ʳ˴ʳˤ̈˸̅̌ʳ˟˴́˺̈˴˺˸ʳʳ
˄˄ˍ˃ˈΩ˄˄ˍˆ˃ˍʳˠ˼̌˴̂ʿʳˬ̈̆̈˾˸ˎʳ˥̈́˸ʳ˦̇̅˸ˎʳ˞˸́˽˼ʳ˦˴˺˴˸ˎʳ˧˴˾̈̌˴ʳˠ˴̇̆̈̍˴˾˼ˎʳ˝̈́ϗ˼˶˻˼ʳ˧̆̈˽˼˼ʳ˧˴̆˾ˀ̂̅˼˸́̇˸˷ʳ˘̉˴˿̈˴̇˼̂́ʳ̂˹ʳ
˦̌́̇˴˶̇˼˶ʳˣ˴̅̆˸̅̆ʳ˴́˷ʳ˧˻˸˼̅ʳ˥˸̃̅˸̆˸́̇˴̇˼̂́̆ʳʳ
˄˄ˍˆ˃Ω˄˄ˍˈˈˍʳ˖˻˴́ʿʳˬ˸˸ʳ˦˸́˺ˎʳ˛̊˸˸ʳ˧̂̈ʳˡ˺ʳˠ˔˫˦˜ˠˍʳ˔ʳˠ˴̋˼̀̈̀ʳ˦˼̀˼˿˴̅˼̇̌ʳˠ˸̇̅˼˶ʳ˹̂̅ʳˠ˴˶˻˼́˸ʳ˧̅˴́̆˿˴̇˼̂́ʳ˘̉˴˿̈˴̇˼̂́ʳ
˄˄ˍˈˈΩ˄˅ˍ˅˃ˍʳ˩̂̂̅˻˸˸̆ʿʳ˘˿˿˸́ʳˠˁʳ˖̂́̇̅˴˷˼˶̇˼̂́̆ʳ˴́˷ʳ˝̈̆̇˼˹˼˶˴̇˼̂́̆ˍʳ˘̋̇˸́̆˼̂́̆ʳ̇̂ʳ̇˻˸ʳ˧˸̋̇̈˴˿ʳ˘́̇˴˼˿̀˸́̇ʳ˧˴̆˾ʳʳ
˦˸̆̆˼̂́ʳ˄˖ˍʳˠ˴˶˻˼́˸ʳ˧̅˴́̆˿˴̇˼̂́ʳ˄ʳ
ʳ
˄˃ˍˇ˃Ω˄˄ˍ˃ˈˍʳ˖˻˸̅̅̌ʿʳ˖̂˿˼́ʳ˖̂˻˸̆˼̉˸ʳˣ˻̅˴̆˸ˀ˕˴̆˸˷ʳ˗˸˶̂˷˼́˺ʳ˹̂̅ʳ˦̇˴̇˼̆̇˼˶˴˿ʳˠ˴˶˻˼́˸ʳ˧̅˴́̆˿˴̇˼̂́ʳʳ
˄˄ˍ˃ˈΩ˄˄ˍˆ˃ˍʳ˗˸́˺ʿʳˬ̂́˺˺˴́˺ˎʳ˝˼˴ʳ˫̈ˎʳˬ̈̄˼́˺ʳ˚˴̂ʳˣ˻̅˴̆˸ʳ˧˴˵˿˸ʳ˧̅˴˼́˼́˺ʳ˹̂̅ʳˣ̅˸˶˼̆˼̂́ʳ˴́˷ʳ˥˸˶˴˿˿ˍʳ˪˻˴̇ʳˠ˴˾˸̆ʳ˴ʳ˚̂̂˷ʳ
ˣ˻̅˴̆˸ʳ˴́˷ʳ˴ʳ˚̂̂˷ʳˣ˻̅˴̆˸ʳˣ˴˼̅˒ʳʳ
˄˄ˍˆ˃Ω˄˄ˍˈˈˍʳ˭˻˴́˺ʿʳ˗̂́˺˷̂́˺ˎʳˠ̈ʳ˟˼ˎʳˡ˴́ʳ˗̈˴́ˎʳ˖˻˼ˀ˛̂ʳ˟˼ˎʳˠ˼́˺ʳ˭˻̂̈ʳˠ˸˴̆̈̅˸ʳ˪̂̅˷ʳ˚˸́˸̅˴̇˼̂́ʳ˹̂̅ʳ˘́˺˿˼̆˻ˀ˖˻˼́˸̆˸ʳ
˦ˠ˧ʳ˦̌̆̇˸̀̆ʳʳ
˄˄ˍˈˈΩ˄˅ˍ˅˃ˍʳ˭˻˴́˺ʿʳ˛˴̂ˎʳ˖˻̅˼̆ʳˤ̈˼̅˾ˎʳ˥̂˵˸̅̇ʳ˖ˁʳˠ̂̂̅˸ˎʳ˗˴́˼˸˿ʳ˚˼˿˷˸˴ʳ˕˴̌˸̆˼˴́ʳ˟˸˴̅́˼́˺ʳ̂˹ʳˡ̂́ˀ˖̂̀̃̂̆˼̇˼̂́˴˿ʳˣ˻̅˴̆˸̆ʳ
̊˼̇˻ʳ˦̌́˶˻̅̂́̂̈̆ʳˣ˴̅̆˼́˺ʳʳ
˦˸̆̆˼̂́ʳ˄˗ˍʳ˦̃˸˸˶˻ʳˣ̅̂˶˸̆̆˼́˺ʳ
ʳ
˄˃ˍˇ˃Ω˄˄ˍ˃ˈˍʳ˞˴̈˹̀˴́́ʿʳ˧̂˵˼˴̆ˎʳ˕˸˴̇ʳˣ˹˼̆̇˸̅ʳ˔̃̃˿̌˼́˺ʳ˴ʳ˚̅˴̀̀˴̅ˀ˕˴̆˸˷ʳ˟˴́˺̈˴˺˸ʳˠ̂˷˸˿ʳ̇̂ʳ˴ʳ˦˼̀̃˿˼˹˼˸˷ʳ˕̅̂˴˷˶˴̆̇ˀˡ˸̊̆ʳ
˧̅˴́̆˶̅˼̃̇˼̂́ʳ˧˴̆˾ʳʳ
˄˄ˍ˃ˈΩ˄˄ˍˆ˃ˍʳ˕˼̆˴́˼ʿʳˠ˴̋˼̀˼˿˼˴́ˎʳˣ˴̈˿ʳ˩̂̍˼˿˴ˎʳˢ˿˼̉˼˸̅ʳ˗˼̉˴̌ˎʳ˝˸˹˹ʳ˔˷˴̀̆ʳ˔̈̇̂̀˴̇˼˶ʳ˘˷˼̇˼́˺ʳ˼́ʳ˴ʳ˕˴˶˾ˀ˘́˷ʳ˦̃˸˸˶˻ˀ̇̂ˀ˧˸̋̇ʳ
˦̌̆̇˸̀ʳʳ
˄˄ˍˆ˃Ω˄˄ˍˈˈˍʳ˙˿˸˼̆˶˻̀˴́ʿʳˠ˼˶˻˴˸˿ˎʳ˗˸˵ʳ˥̂̌ʳ˚̅̂̈́˷˸˷ʳ˟˴́˺̈˴˺˸ʳˠ̂˷˸˿˼́˺ʳ˹̂̅ʳ˔̈̇̂̀˴̇˼˶ʳ˦̃˸˸˶˻ʳ˥˸˶̂˺́˼̇˼̂́ʳ̂˹ʳ˦̃̂̅̇̆ʳ
˩˼˷˸̂ʳʳ
˄˄ˍˈˈΩ˄˅ˍ˅˃ˍʳ˙˿˸˶˾ʿʳˠ˴̅˺˴̅˸̇ʳˠˁʳ˟˸̋˼˶˴˿˼̍˸˷ʳˣ˻̂́̂̇˴˶̇˼˶ʳ˪̂̅˷ʳ˦˸˺̀˸́̇˴̇˼̂́ʳʳ
˄˅ˍ˅˃Ω˅ˍ˃˃ʳ˟̈́˶˻ʳʳ
˦˸̆̆˼̂́ʳ˅˔ˍʳ˜́˹̂̅̀˴̇˼̂́ʳ˥˸̇̅˼˸̉˴˿ʳ˄ʳ
ʳ
˅ˍ˃˃Ω˅ˍ˅ˈˍʳ˙˴́˺ʿʳ˛̈˼ʳ˔ʳ˥˸ˀ˸̋˴̀˼́˴̇˼̂́ʳ̂˹ʳˤ̈˸̅̌ʳ˘̋̃˴́̆˼̂́ʳ˨̆˼́˺ʳ˟˸̋˼˶˴˿ʳ˥˸̆̂̈̅˶˸̆ʳʳ
รʳ˄ʳ଄Δ٥ʳˌʳ଄ˣ̅̂˶˸˸˷˼́˺̆
˅˃˃ˌ˂˄˂˅ˇ˻̇̇̃ˍ˂˂̊̊̊ˁ˿˼́˺ˁ̂˻˼̂ˀ̆̇˴̇˸ˁ˸˷̈˂˴˶˿˃ˋ˂̆˶˻˸˷̈˿˸ˁ˻̇̀˿
˅ˍ˅ˈΩ˅ˍˈ˃ˍʳ˖˴̂ʿʳ˚̈˼˻̂́˺ˎʳ˦̇˸̃˻˸́ʳ˥̂˵˸̅̇̆̂́ˎʳ˝˼˴́ˀˬ̈́ʳˡ˼˸ʳ˦˸˿˸˶̇˼́˺ʳˤ̈˸̅̌ʳ˧˸̅̀ʳ˔˿̇˸̅́˴̇˼̂́̆ʳ˹̂̅ʳ˪˸˵ʳ˦˸˴̅˶˻ʳ˵̌ʳ˘̋̃˿̂˼̇˼́˺ʳ
ˤ̈˸̅̌ʳ˖̂́̇˸̋̇̆ʳʳ
˅ˍˈ˃Ωˆˍ˄ˈˍʳ˗̈˴́ʿʳ˛̈˼̍˻̂́˺ˎʳˬ̈́˵̂ʳ˖˴̂ˎʳ˖˻˼́ˀˬ˸̊ʳ˟˼́ˎʳˬ̂́˺ʳˬ̈ʳ˦˸˴̅˶˻˼́˺ʳˤ̈˸̆̇˼̂́̆ʳ˵̌ʳ˜˷˸́̇˼˹̌˼́˺ʳˤ̈˸̆̇˼̂́ʳ˧̂̃˼˶ʳ˴́˷ʳ
ˤ̈˸̆̇˼̂́ʳ˙̂˶̈̆ʳʳ
˦˸̆̆˼̂́ʳ˅˕ˍʳ˟˴́˺̈˴˺˸ʳ˚˸́˸̅˴̇˼̂́ʳ
ʳ
˅ˍ˃˃Ω˅ˍ˅ˈˍʳˠ˴˼̅˸̆̆˸ʿʳ˙̅˴́ͺ̂˼̆ˎʳˠ˴̅˼˿̌́ʳ˪˴˿˾˸̅ʳ˧̅˴˼́˴˵˿˸ʳ˚˸́˸̅˴̇˼̂́ʳ̂˹ʳ˕˼˺ˀ˙˼̉˸ʳˣ˸̅̆̂́˴˿˼̇̌ʳ˦̇̌˿˸̆ʳ̇˻̅̂̈˺˻ʳ˗˴̇˴ˀ˗̅˼̉˸́ʳ
ˣ˴̅˴̀˸̇˸̅ʳ˘̆̇˼̀˴̇˼̂́ʳʳ
˅ˍ˅ˈΩ˅ˍˈ˃ˍʳ˟˸˸ʿʳ˝̂˻́ˎʳ˦̇˸̃˻˴́˼˸ʳ˦˸́˸˹˹ʳ˖̂̅̅˸˶̇˼́˺ʳˠ˼̆̈̆˸ʳ̂˹ʳ˩˸̅˵ʳ˙̂̅̀̆ʳʳ
˅ˍˈ˃Ωˆˍ˄ˈˍʳ˘̆̃˼́̂̆˴ʿʳ˗̂̀˼́˼˶ˎʳˠ˼˶˻˴˸˿ʳ˪˻˼̇˸ˎʳ˗˸́́˼̆ʳˠ˸˻˴̌ʳ˛̌̃˸̅̇˴˺˺˼́˺ˍʳ˦̈̃˸̅̇˴˺˺˼́˺ʳ˹̂̅ʳ˦̈̅˹˴˶˸ʳ˥˸˴˿˼̍˴̇˼̂́ʳ̊˼̇˻ʳ˖˖˚ʳ
˦˸̆̆˼̂́ʳ˅˖ˍʳˠ˴˶˻˼́˸ʳ˧̅˴́̆˿˴̇˼̂́ʳ˅ʳ
ʳ
˅ˍ˃˃Ω˅ˍ˅ˈˍʳˠ˼ʿʳ˛˴˼̇˴̂ˎʳ˟˼˴́˺ʳ˛̈˴́˺ˎʳˤ̈́ʳ˟˼̈ʳ˙̂̅˸̆̇ˀ˕˴̆˸˷ʳ˧̅˴́̆˿˴̇˼̂́ʳʳ
˅ˍ˅ˈΩ˅ˍˈ˃ˍʳ˕˿̈́̆̂̀ʿʳˣ˻˼˿ˎʳ˧̅˸̉̂̅ʳ˖̂˻́ˎʳˠ˼˿˸̆ʳˢ̆˵̂̅́˸ʳ˔ʳ˗˼̆˶̅˼̀˼́˴̇˼̉˸ʳ˟˴̇˸́̇ʳ˩˴̅˼˴˵˿˸ʳˠ̂˷˸˿ʳ˹̂̅ʳ˦̇˴̇˼̆̇˼˶˴˿ʳˠ˴˶˻˼́˸ʳ
˧̅˴́̆˿˴̇˼̂́ʳʳ
˅ˍˈ˃Ωˆˍ˄ˈˍʳ˭˻˴́˺ʿʳ˛˴̂ˎʳ˗˴́˼˸˿ʳ˚˼˿˷˸˴ʳ˘˹˹˼˶˼˸́̇ʳˠ̈˿̇˼ˀˣ˴̆̆ʳ˗˸˶̂˷˼́˺ʳ˹̂̅ʳ˦̌́˶˻̅̂́̂̈̆ʳ˖̂́̇˸̋̇ʳ˙̅˸˸ʳ˚̅˴̀̀˴̅̆ʳʳ
˦˸̆̆˼̂́ʳ˅˗ˍʳ˦˸̀˴́̇˼˶̆ʳ˄ʳ
ʳ
˅ˍ˃˃Ω˅ˍ˅ˈˍʳ˞̂˿˿˸̅ʿʳ˔˿˸̋˴́˷˸̅ˎʳˠ˼˶˻˴˸˿˴ʳ˥˸˺́˸̅˼ˎʳ˦̇˸˹˴́ʳ˧˻˴̇˸̅ʳ˥˸˺̈˿˴̅ʳ˧̅˸˸ʳ˚̅˴̀̀˴̅̆ʳ˴̆ʳ˴ʳ˙̂̅̀˴˿˼̆̀ʳ˹̂̅ʳ˦˶̂̃˸ʳ
˨́˷˸̅̆̃˸˶˼˹˼˶˴̇˼̂́ʳʳ
˅ˍ˅ˈΩ˅ˍˈ˃ˍʳ˗˴̉˼˷̂̉ʿʳ˗̀˼̇̅̌ˎʳ˔̅˼ʳ˥˴̃̃̂̃̂̅̇ʳ˖˿˴̆̆˼˹˼˶˴̇˼̂́ʳ̂˹ʳ˦˸̀˴́̇˼˶ʳ˥˸˿˴̇˼̂́̆˻˼̃̆ʳ˵˸̇̊˸˸́ʳˡ̂̀˼́˴˿̆ʳ˨̆˼́˺ʳˣ˴̇̇˸̅́ʳ
˖˿̈̆̇˸̅̆ʳʳ
˅ˍˈ˃Ωˆˍ˄ˈˍʳˠ˼̇˶˻˸˿˿ʿʳ˝˸˹˹ˎʳˠ˼̅˸˿˿˴ʳ˟˴̃˴̇˴ʳ˩˸˶̇̂̅ˀ˵˴̆˸˷ʳˠ̂˷˸˿̆ʳ̂˹ʳ˦˸̀˴́̇˼˶ʳ˖̂̀̃̂̆˼̇˼̂́ʳʳ
ˆˍ˄ˈΩˆˍˇˈʳ˕̅˸˴˾ʳʳ
˦˸̆̆˼̂́ʳˆ˔ˍʳ˜́˹̂̅̀˴̇˼̂́ʳ˘̋̇̅˴˶̇˼̂́ʳ˅ʳ
ʳ
ˆˍˇˈΩˇˍ˄˃ˍʳ˔̅́̂˿˷ʿʳ˔́˷̅˸̊ˎʳ˥˴̀˸̆˻ʳˡ˴˿˿˴̃˴̇˼ˎʳ˪˼˿˿˼˴̀ʳ˪ˁʳ˖̂˻˸́ʳ˘̋̃˿̂˼̇˼́˺ʳ˙˸˴̇̈̅˸ʳ˛˼˸̅˴̅˶˻̌ʳ˹̂̅ʳ˧̅˴́̆˹˸̅ʳ˟˸˴̅́˼́˺ʳ˼́ʳ
ˡ˴̀˸˷ʳ˘́̇˼̇̌ʳ˥˸˶̂˺́˼̇˼̂́ʳʳ
ˇˍ˄˃Ωˇˍˆˈˍʳ˝˼ʿʳ˛˸́˺ˎʳ˥˴˿̃˻ʳ˚̅˼̆˻̀˴́ʳ˥˸˹˼́˼́˺ʳ˘̉˸́̇ʳ˘̋̇̅˴˶̇˼̂́ʳ̇˻̅̂̈˺˻ʳ˖̅̂̆̆ˀ˗̂˶̈̀˸́̇ʳ˜́˹˸̅˸́˶˸ʳʳ
ˇˍˆˈΩˈˍ˃˃ˍʳ˕̅˴́˴̉˴́ʿʳ˦ˁ˥ˁ˞ˁˎʳ˛˴̅̅ʳ˖˻˸́ˎʳ˝˴˶̂˵ʳ˘˼̆˸́̆̇˸˼́ˎʳ˥˸˺˼́˴ʳ˕˴̅̍˼˿˴̌ʳ˟˸˴̅́˼́˺ʳ˗̂˶̈̀˸́̇ˀ˟˸̉˸˿ʳ˦˸̀˴́̇˼˶ʳˣ̅̂̃˸̅̇˼˸̆ʳ
˹̅̂̀ʳ˙̅˸˸ˀ˧˸̋̇ʳ˔́́̂̇˴̇˼̂́̆ʳʳ
ˈˍ˃˃Ωˈˍ˅ˈˍʳ˙˸́˺ʿʳˬ˴́̆̂́˺ˎʳˠ˼̅˸˿˿˴ʳ˟˴̃˴̇˴ʳ˔̈̇̂̀˴̇˼˶ʳ˜̀˴˺˸ʳ˔́́̂̇˴̇˼̂́ʳ˨̆˼́˺ʳ˔̈̋˼˿˼˴̅̌ʳ˧˸̋̇ʳ˜́˹̂̅̀˴̇˼̂́ʳʳ
˦˸̆̆˼̂́ʳˆ˕ˍʳ˦˸́̇˼̀˸́̇ʳ˔́˴˿̌̆˼̆ʳ
ʳ
ˆˍˇˈΩˇˍ˄˃ˍʳ˦̍˴̅̉˴̆ʿʳ˚̌Ή̅˺̌ʳ˛˸˷˺˸ʳ˖˿˴̆̆˼˹˼˶˴̇˼̂́ʳ˼́ʳ˕˼̂̀˸˷˼˶˴˿ʳ˧˸̋̇̆ʳ̊˼̇˻ʳ˴ʳ˪˸˴˾˿̌ʳ˦̈̃˸̅̉˼̆˸˷ʳ˦˸˿˸˶̇˼̂́ʳ̂˹ʳ˞˸̌̊̂̅˷̆ʳʳ
ˇˍ˄˃Ωˇˍˆˈˍʳ˔́˷̅˸˸̉̆˾˴˼˴ʿʳ˔˿˼́˴ˎʳ˦˴˵˼́˸ʳ˕˸̅˺˿˸̅ʳ˪˻˸́ʳ˦̃˸˶˼˴˿˼̆̇̆ʳ˴́˷ʳ˚˸́˸̅˴˿˼̆̇̆ʳ˪̂̅˾ʳ˧̂˺˸̇˻˸̅ˍʳˢ̉˸̅˶̂̀˼́˺ʳ˗̂̀˴˼́ʳ
˗˸̃˸́˷˸́˶˸ʳ˼́ʳ˦˸́̇˼̀˸́̇ʳ˧˴˺˺˼́˺ʳʳ
ˇˍˆˈΩˈˍ˃˃ˍʳˡ̂̀̂̇̂ʿʳ˧˴˷˴̆˻˼ʳ˔ʳ˚˸́˸̅˼˶ʳ˦˸́̇˸́˶˸ʳ˧̅˼̀̀˸̅ʳ̊˼̇˻ʳ˖˥˙̆ʳʳ
ˈˍ˃˃Ωˈˍ˅ˈˍʳ˧˼̇̂̉ʿʳ˜̉˴́ˎʳ˥̌˴́ʳˠ˶˗̂́˴˿˷ʳ˔ʳ˝̂˼́̇ʳˠ̂˷˸˿ʳ̂˹ʳ˧˸̋̇ʳ˴́˷ʳ˔̆̃˸˶̇ʳ˥˴̇˼́˺̆ʳ˹̂̅ʳ˦˸́̇˼̀˸́̇ʳ˦̈̀̀˴̅˼̍˴̇˼̂́ʳʳ
˦˸̆̆˼̂́ʳˆ˖ˍʳ˦̌́̇˴̋ʳʹʳˣ˴̅̆˼́˺ʳ˄ʳ
ʳ
ˆˍˇˈΩˇˍ˄˃ˍʳ˔˺˼̅̅˸ʿʳ˘́˸˾̂ˎʳ˧˼̀̂̇˻̌ʳ˕˴˿˷̊˼́ˎʳ˗˴̉˼˷ʳˠ˴̅̇˼́˸̍ʳ˜̀̃̅̂̉˼́˺ʳˣ˴̅̆˼́˺ʳ˴́˷ʳˣˣʳ˔̇̇˴˶˻̀˸́̇ʳˣ˸̅˹̂̅̀˴́˶˸ʳ̊˼̇˻ʳ˦˸́̆˸ʳ
˜́˹̂̅̀˴̇˼̂́ʳʳ
ˇˍ˄˃Ωˇˍˆˈˍʳ˛̂̌̇ʿʳ˙̅˸˷˸̅˼˶˾ˎʳ˝˴̆̂́ʳ˕˴˿˷̅˼˷˺˸ʳ˔ʳ˟̂˺˼˶˴˿ʳ˕˴̆˼̆ʳ˹̂̅ʳ̇˻˸ʳ˗ʳ˖̂̀˵˼́˴̇̂̅ʳ˴́˷ʳˡ̂̅̀˴˿ʳ˙̂̅̀ʳ˼́ʳ˖˖˚ʳʳ
ˇˍˆˈΩˈˍ˃˃ˍʳ˩˴˷˴̆ʿʳ˗˴̉˼˷ˎʳ˝˴̀˸̆ʳ˥ˁʳ˖̈̅̅˴́ʳˣ˴̅̆˼́˺ʳˡ̂̈́ʳˣ˻̅˴̆˸ʳ˦̇̅̈˶̇̈̅˸ʳ̊˼̇˻ʳ˖˖˚ʳʳ
รʳ˅ʳ଄Δ٥ʳˌʳ଄ˣ̅̂˶˸˸˷˼́˺̆
˅˃˃ˌ˂˄˂˅ˇ˻̇̇̃ˍ˂˂̊̊̊ˁ˿˼́˺ˁ̂˻˼̂ˀ̆̇˴̇˸ˁ˸˷̈˂˴˶˿˃ˋ˂̆˶˻˸˷̈˿˸ˁ˻̇̀˿
ˈˍ˃˃Ωˈˍ˅ˈˍʳ˩˼˶˾̅˸̌ʿʳ˗˴̉˼˷ˎʳ˗˴̃˻́˸ʳ˞̂˿˿˸̅ʳ˦˸́̇˸́˶˸ʳ˦˼̀̃˿˼˹˼˶˴̇˼̂́ʳ˹̂̅ʳ˦˸̀˴́̇˼˶ʳ˥̂˿˸ʳ˟˴˵˸˿˼́˺ʳʳ
ˆˍˇˈΩˈˍˈ˃ʳ˦˸̆̆˼̂́ʳˆ˗ˍʳ˦̇̈˷˸́̇ʳ˥˸̆˸˴̅˶˻ʳ˪̂̅˾̆˻̂̃ʳ
ʳ
ˆˍˇˈΩˇˍ˄˃ˍʳ˛˴˺˼̊˴̅˴ʿʳˠ˴̆˴̇̂ʳ˔ʳ˦̈̃˸̅̉˼̆˸˷ʳ˟˸˴̅́˼́˺ʳ˔̃̃̅̂˴˶˻ʳ̇̂ʳ˔̈̇̂̀˴̇˼˶ʳ˦̌́̂́̌̀ʳ˜˷˸́̇˼˹˼˶˴̇˼̂́ʳ˕˴̆˸˷ʳ̂́ʳ˗˼̆̇̅˼˵̈̇˼̂́˴˿ʳ
˙˸˴̇̈̅˸̆ʳʳ
ˇˍ˄˃Ωˇˍˆˈˍʳ˕˴́˼˾ʿʳ˘̉˴ʳ˔́ʳ˜́̇˸˺̅˴̇˸˷ʳ˔̅˶˻˼̇˸˶̇̈̅˸ʳ˹̂̅ʳ˚˸́˸̅˴̇˼́˺ʳˣ˴̅˸́̇˻˸̇˼˶˴˿ʳ˖̂́̆̇̅̈˶̇˼̂́̆ʳʳ
ˇˍˆˈΩˈˍ˃˃ˍʳ˘˼˷˸˿̀˴́ʿʳ˩˿˴˷˼̀˼̅ʳ˜́˹˸̅̅˼́˺ʳ˔˶̇˼̉˼̇̌ʳ˧˼̀˸ʳ˼́ʳˡ˸̊̆ʳ̇˻̅̂̈˺˻ʳ˘̉˸́̇ʳˠ̂˷˸˿˼́˺ʳʳ
ˈˍ˃˃Ωˈˍ˅ˈˍʳ˟˼˴̂ʿʳ˦˻˴̆˻˴ʳ˖̂̀˵˼́˼́˺ʳ˦̂̈̅˶˸ʳ˴́˷ʳ˧˴̅˺˸̇ʳ˟˴́˺̈˴˺˸ʳ˜́˹̂̅̀˴̇˼̂́ʳ˹̂̅ʳˡ˴̀˸ʳ˧˴˺˺˼́˺ʳ̂˹ʳˠ˴˶˻˼́˸ʳ˧̅˴́̆˿˴̇˼̂́ʳ
ˢ̈̇̃̈̇ʳʳ
ˈˍ˅ˈΩˈˍˈ˃ˍʳ˦̈́ʿʳ˦˻̈̄˼ˎʳˬ˼́ʳ˖˻˸́ˎʳ˝̈˹˸́˺ʳ˟˼ʳ˔ʳ˥˸ˀ˸̋˴̀˼́˴̇˼̂́ʳ̂́ʳ˙˸˴̇̈̅˸̆ʳ˼́ʳ˥˸˺̅˸̆̆˼̂́ʳ˕˴̆˸˷ʳ˔̃̃̅̂˴˶˻ʳ̇̂ʳ˔̈̇̂̀˴̇˼˶ʳˠ˧ʳ
˘̉˴˿̈˴̇˼̂́ʳʳ
ˣ̂̆̇˸̅ʳ˦˸̆̆˼̂́ʳ˦̇̈˷˸́̇ʳ˥˸̆˸˴̅˶˻ʳ˪̂̅˾̆˻̂̃ʳʻˉˍ˃˃ˀˋˍˆ˃ʼʳ
ʳ
˙̂̆̆˴̇˼ʿʳ˗˴̉˼˷˸ʳ˧˻˸ʳ˥̂˿˸ʳ̂˹ʳˣ̂̆˼̇˼̉˸ʳ˙˸˸˷˵˴˶˾ʳ˼́ʳ˜́̇˸˿˿˼˺˸́̇ʳ˧̈̇̂̅˼́˺ʳ˦̌̆̇˸̀̆ʳʳ
˛˸˼́̇̍ʿʳ˜˿˴́˴ʳ˔̅˴˵˼˶ʳ˟˴́˺̈˴˺˸ʳˠ̂˷˸˿˼́˺ʳ̊˼̇˻ʳ˙˼́˼̇˸ʳ˦̇˴̇˸ʳ˧̅˴́̆˷̈˶˸̅̆ʳʳ
˞˸̅̆˸̌ʿʳ˖̌́̇˻˼˴ʳ˜̀̃˴˶̇ʳ̂˹ʳ˜́˼̇˼˴̇˼̉˸ʳ̂́ʳ˖̂˿˿˴˵̂̅˴̇˼̉˸ʳˣ̅̂˵˿˸̀ʳ˦̂˿̉˼́˺ʳʳ
ˠ˶˜́́˸̆ʿʳ˕̅˼˷˺˸̇ʳ˔́ʳ˨́̆̈̃˸̅̉˼̆˸˷ʳ˩˸˶̇̂̅ʳ˔̃̃̅̂˴˶˻ʳ̇̂ʳ˕˼̂̀˸˷˼˶˴˿ʳ˧˸̅̀ʳ˗˼̆˴̀˵˼˺̈˴̇˼̂́ˍʳ˜́̇˸˺̅˴̇˼́˺ʳ˨ˠ˟˦ʳ˴́˷ʳˠ˸˷˿˼́˸ʳʳ
ˠ˸̆̆˼˴́̇ʿʳ˖ͼ˷̅˼˶ʳ˔ʳ˦̈˵˶˴̇˸˺̂̅˼̍˴̇˼̂́ʳ˔˶̄̈˼̆˼̇˼̂́ʳ˦̌̆̇˸̀ʳ˹̂̅ʳ˙̅˸́˶˻ʳ˩˸̅˵̆ʳʳ
˧̅́˾˴ʿʳ˞˸˼̇˻ʳ˔˷˴̃̇˼̉˸ʳ˟˴́˺̈˴˺˸ʳˠ̂˷˸˿˼́˺ʳ˹̂̅ʳ˪̂̅˷ʳˣ̅˸˷˼˶̇˼̂́ʳʳ
˭˻˴́˺ʿʳˬ˼̇˴̂ʳ˔ʳ˛˼˸̅˴̅˶˻˼˶˴˿ʳ˔̃̃̅̂˴˶˻ʳ̇̂ʳ˘́˶̂˷˼́˺ʳˠ˸˷˼˶˴˿ʳ˖̂́˶˸̃̇̆ʳ˹̂̅ʳ˖˿˼́˼˶˴˿ʳˡ̂̇˸̆ʳʳ
ˈˍ˅ˈΩˉˍ˃˃ʳ˕̅˸˴˾ʳʳ
ˣ̂̆̇˸̅ʳ˴́˷ʳ˗˸̀̂ʳ˦˸̆̆˼̂́ʳʻˉˍ˃˃ˀˋˍˆ˃ʼʳ
ʳ
˕˴̇˼̆̇˴ʿʳ˙˸̅́˴́˷̂ˎʳˡ̈́̂ʳˠ˴̀˸˷˸ˎʳ˜̆˴˵˸˿ʳ˧̅˴́˶̂̆̂ʳ˟˴́˺̈˴˺˸ʳ˗̌́˴̀˼˶̆ʳ˴́˷ʳ˖˴̃˼̇˴˿˼̍˴̇˼̂́ʳ̈̆˼́˺ʳˠ˴̋˼̀̈̀ʳ˘́̇̅̂̃̌ʳʳ
˕̂̆̇̂́ʿʳˠ˴̅˼̆˴ʳ˙˸̅̅˴̅˴ˎʳ˝̂˻́ʳ˧ˁʳ˛˴˿˸ˎʳ˥˸˼́˻̂˿˷ʳ˞˿˼˸˺˿ˎʳ˦˻̅˴̉˴́ʳ˩˴̆˼̆˻̇˻ʳ˦̈̅̃̅˼̆˼́˺ʳˣ˴̅̆˸̅ʳ˔˶̇˼̂́̆ʳ˴́˷ʳ˥˸˴˷˼́˺ʳ˗˼˹˹˼˶̈˿̇̌ʳʳ
˖˴̅˸́˼́˼ʿʳ˚˼̈̆˸̃̃˸ˎʳ˥˴̌̀̂́˷ʳ˧ˁʳˡ˺ˎʳ˫˼˴̂˷̂́˺ʳ˭˻̂̈ʳ˦̈̀̀˴̅˼̍˼́˺ʳ˘̀˴˼˿̆ʳ̊˼̇˻ʳ˖̂́̉˸̅̆˴̇˼̂́˴˿ʳ˖̂˻˸̆˼̂́ʳ˴́˷ʳ˦̈˵˽˸˶̇˼̉˼̇̌ʳʳ
˖˻˴˿˼ʿʳˬ˿˿˼˴̆ˎʳ˦˻˴˹˼̄ʳ˝̂̇̌ʳ˜̀̃̅̂̉˼́˺ʳ̇˻˸ʳˣ˸̅˹̂̅̀˴́˶˸ʳ̂˹ʳ̇˻˸ʳ˥˴́˷̂̀ʳ˪˴˿˾ʳˠ̂˷˸˿ʳ˹̂̅ʳ˔́̆̊˸̅˼́˺ʳ˖̂̀̃˿˸̋ʳˤ̈˸̆̇˼̂́̆ʳʳ
˖˻˸́ʿʳ˪˸˼ʳ˗˼̀˸́̆˼̂́̆ʳ̂˹ʳ˦̈˵˽˸˶̇˼̉˼̇̌ʳ˼́ʳˡ˴̇̈̅˴˿ʳ˟˴́˺̈˴˺˸ʳʳ
˖˻˼̇̇̈̅˼ʿʳ˥˴˻̈˿ˎʳ˝̂˻́ʳ˛˴́̆˸́ʳ˗˼˴˿˸˶̇ʳ˖˿˴̆̆˼˹˼˶˴̇˼̂́ʳ˹̂̅ʳˢ́˿˼́˸ʳˣ̂˷˶˴̆̇̆ʳ˙̈̆˼́˺ʳ˔˶̂̈̆̇˼˶ʳ˴́˷ʳ˟˴́˺̈˴˺˸ʳ˕˴̆˸˷ʳ˦̇̅̈˶̇̈̅˴˿ʳ˴́˷ʳ
˦˸̀˴́̇˼˶ʳ˜́˹̂̅̀˴̇˼̂́ʳʳ
˗˸ˡ˸̅̂ʿʳ˝̂˻́ˎʳ˗˴́ʳ˞˿˸˼́ʳ˧˻˸ʳ˖̂̀̃˿˸̋˼̇̌ʳ̂˹ʳˣ˻̅˴̆˸ʳ˔˿˼˺́̀˸́̇ʳˣ̅̂˵˿˸̀̆ʳʳ
˗˼˶˾˼́̆̂́ʿʳˠ˴̅˾̈̆ʳ˔˷ʳ˛̂˶ʳ˧̅˸˸˵˴́˾ʳ˦̇̅̈˶̇̈̅˸̆ʳʳ
˷˸ʳ˿˴ʳ˖˻˼˶˴ʿʳ˦˸˵˴̆̇˼˴́ˎʳ˙˴˼̆˴˿ʳ˔˻̀˴˷ˎʳ˝˴̀˸̆ʳ˛ˁʳˠ˴̅̇˼́ˎʳ˧˴̀˴̅˴ʳ˦̈̀́˸̅ʳ˘̋̇̅˴˶̇˼̉˸ʳ˦̈̀̀˴̅˼˸̆ʳ˹̂̅ʳ˘˷̈˶˴̇˼̂́˴˿ʳ˦˶˼˸́˶˸ʳ˖̂́̇˸́̇ʳ
ʳ
˗˿˼˺˴˶˻ʿʳ˗̀˼̇̅˼̌ˎʳˠ˴̅̇˻˴ʳˣ˴˿̀˸̅ʳˡ̂̉˸˿ʳ˦˸̀˴́̇˼˶ʳ˙˸˴̇̈̅˸̆ʳ˹̂̅ʳ˩˸̅˵ʳ˦˸́̆˸ʳ˗˼̆˴̀˵˼˺̈˴̇˼̂́ʳʳ
˗̅˸˷̍˸ʿʳˠ˴̅˾ˎʳ˝̂˸˿ʳ˪˴˿˿˸́˵˸̅˺ʳ˜˶˸˿˴́˷˼˶ʳ˗˴̇˴ʳ˗̅˼̉˸́ʳˣ˴̅̇ʳ̂˹ʳ˦̃˸˸˶˻ʳ˧˴˺˺˼́˺ʳʳ
˗̈˻ʿʳ˞˸̉˼́ˎʳ˞˴̇̅˼́ʳ˞˼̅˶˻˻̂˹˹ʳ˕˸̌̂́˷ʳ˟̂˺ˀ˟˼́˸˴̅ʳˠ̂˷˸˿̆ˍʳ˕̂̂̆̇˸˷ʳˠ˼́˼̀̈̀ʳ˘̅̅̂̅ʳ˥˴̇˸ʳ˧̅˴˼́˼́˺ʳ˹̂̅ʳˡˀ˵˸̆̇ʳ˥˸ˀ̅˴́˾˼́˺ʳʳ
˘˿̆́˸̅ʿʳˠ˼˶˻˴ˎʳ˘̈˺˸́˸ʳ˖˻˴̅́˼˴˾ʳ˖̂̅˸˹˸̅˸́˶˸ˀ˼́̆̃˼̅˸˷ʳ˖̂˻˸̅˸́˶˸ʳˠ̂˷˸˿˼́˺ʳʳ
˙˼́˾˸˿ʿʳ˝˸́́̌ʳ˥̂̆˸ˎʳ˖˻̅˼̆̇̂̃˻˸̅ʳ˗ˁʳˠ˴́́˼́˺ʳ˘́˹̂̅˶˼́˺ʳ˧̅˴́̆˼̇˼̉˼̇̌ʳ˼́ʳ˖̂̅˸˹˸̅˸́˶˸ʳ˥˸̆̂˿̈̇˼̂́ʳʳ
˚˸̂̅˺˼˿˴ʿʳ˞˴˿˿˼̅̅̂˼ˎʳˠ˴̅˼˴ʳ˪̂˿̇˸̅̆ˎʳ˝̂˻˴́́˴ʳˠ̂̂̅˸ʳ˦˼̀̈˿˴̇˼́˺ʳ̇˻˸ʳ˕˸˻˴̉˼̂̈̅ʳ̂˹ʳˢ˿˷˸̅ʳ̉˸̅̆̈̆ʳˬ̂̈́˺˸̅ʳ˨̆˸̅̆ʳ̊˻˸́ʳ˜́̇˸̅˴˶̇˼́˺ʳ
̊˼̇˻ʳ˦̃̂˾˸́ʳ˗˼˴˿̂˺̈˸ʳ˦̌̆̇˸̀̆ʳʳ
˚̂˿˷˵˸̅˺ʿʳˬ̂˴̉ˎʳ˥˸̈̇ʳ˧̆˴̅˹˴̇̌ʳ˔ʳ˦˼́˺˿˸ʳ˚˸́˸̅˴̇˼̉˸ʳˠ̂˷˸˿ʳ˹̂̅ʳ˝̂˼́̇ʳˠ̂̅̃˻̂˿̂˺˼˶˴˿ʳ˦˸˺̀˸́̇˴̇˼̂́ʳ˴́˷ʳ˦̌́̇˴˶̇˼˶ʳˣ˴̅̆˼́˺ʳʳ
˚̂˿˷̊˴̆̆˸̅ʿʳ˗˴́ˎʳ˗˴́ʳ˥̂̇˻ʳ˔˶̇˼̉˸ʳ˦˴̀̃˿˸ʳ˦˸˿˸˶̇˼̂́ʳ˹̂̅ʳˡ˴̀˸˷ʳ˘́̇˼̇̌ʳ˧̅˴́̆˿˼̇˸̅˴̇˼̂́ʳʳ
˚̂˿˷̊˴̇˸̅ʿʳ˦˻˴̅̂́ˎʳ˗˴́ʳ˝̈̅˴˹̆˾̌ˎʳ˖˻̅˼̆̇̂̃˻˸̅ʳ˗ˁʳˠ˴́́˼́˺ʳ˪˻˼˶˻ʳ˪̂̅˷̆ʳ˔̅˸ʳ˛˴̅˷ʳ̇̂ʳ˥˸˶̂˺́˼̍˸˒ʳˣ̅̂̆̂˷˼˶ʿʳ˟˸̋˼˶˴˿ʿʳ˴́˷ʳ
˗˼̆˹˿̈˸́˶̌ʳ˙˴˶̇̂̅̆ʳ̇˻˴̇ʳ˜́˶̅˸˴̆˸ʳ˔˦˥ʳ˘̅̅̂̅ʳ˥˴̇˸̆ʳʳ
˛˴˖̂˻˸́ˀ˞˸̅́˸̅ʿʳˬ˴˴˾̂̉ˎʳ˔̅˼˸˿ʳ˞˴̆̆ˎʳ˔̅˼˸˿ʳˣ˸̅˸̇̍ʳ˖̂̀˵˼́˸˷ʳˢ́˸ʳ˦˸́̆˸ʳ˗˼̆˴̀˵˼˺̈˴̇˼̂́ʳ̂˹ʳ˔˵˵̅˸̉˼˴̇˼̂́̆ʳʳ
˛˴˵˴̆˻ʿʳˡ˼̍˴̅ʳ˙̂̈̅ʳ˧˸˶˻́˼̄̈˸̆ʳ˹̂̅ʳˢ́˿˼́˸ʳ˛˴́˷˿˼́˺ʳ̂˹ʳˢ̈̇ˀ̂˹ˀ˩̂˶˴˵̈˿˴̅̌ʳ˪̂̅˷̆ʳ˼́ʳ˔̅˴˵˼˶ˀ˘́˺˿˼̆˻ʳ˦̇˴̇˼̆̇˼˶˴˿ʳˠ˴˶˻˼́˸ʳ
˧̅˴́̆˿˴̇˼̂́ʳʳ
˛˴˸̅̇˸˿ʿʳ˥̂˵˵˼˸ˎʳ˘̅˼˶ʳ˥˼́˺˺˸̅ˎʳ˞˸̉˼́ʳ˦˸̃̃˼ˎʳ˖˴̅̅̂˿˿ʳ˝˴̀˸̆ˎʳˠ˶˖˿˴́˴˻˴́ʳˣ˸̇˸̅ʳ˔̆̆˸̆̆˼́˺ʳ̇˻˸ʳ˖̂̆̇̆ʳ̂˹ʳ˦˴̀̃˿˼́˺ʳˠ˸̇˻̂˷̆ʳ˼́ʳ
˔˶̇˼̉˸ʳ˟˸˴̅́˼́˺ʳ˹̂̅ʳ˔́́̂̇˴̇˼̂́ʳʳ
˛˴̆˻˼̀̂̇̂ʿʳ˖˻˼˾˴̅˴ˎʳ˦˴˷˴̂ʳ˞̈̅̂˻˴̆˻˼ʳ˕˿̂˺ʳ˖˴̇˸˺̂̅˼̍˴̇˼̂́ʳ˘̋̃˿̂˼̇˼́˺ʳ˗̂̀˴˼́ʳ˗˼˶̇˼̂́˴̅̌ʳ˴́˷ʳ˗̌́˴̀˼˶˴˿˿̌ʳ˘̆̇˼̀˴̇˸˷ʳ˗̂̀˴˼́̆ʳ
̂˹ʳ˨́˾́̂̊́ʳ˪̂̅˷̆ʳʳ
˛˸́˷˸̅̆̂́ʿʳ˝˴̀˸̆ˎʳˢ˿˼̉˸̅ʳ˟˸̀̂́ʳˠ˼̋̇̈̅˸ʳˠ̂˷˸˿ʳˣˢˠ˗ˣ̆ʳ˹̂̅ʳ˘˹˹˼˶˼˸́̇ʳ˛˴́˷˿˼́˺ʳ̂˹ʳ˨́˶˸̅̇˴˼́̇̌ʳ˼́ʳ˗˼˴˿̂˺̈˸ʳˠ˴́˴˺˸̀˸́̇ʳʳ
˛˸̅̀˽˴˾̂˵ʿʳ˨˿˹ˎʳ˞˸̉˼́ʳ˞́˼˺˻̇ˎʳ˛˴˿ʳ˗˴̈̀ͼʳ˜˜˜ʳˡ˴̀˸ʳ˧̅˴́̆˿˴̇˼̂́ʳ˼́ʳ˦̇˴̇˼̆̇˼˶˴˿ʳˠ˴˶˻˼́˸ʳ˧̅˴́̆˿˴̇˼̂́ʳˀʳ˟˸˴̅́˼́˺ʳ˪˻˸́ʳ̇̂ʳ
รʳˆʳ଄Δ٥ʳˌʳ଄ˣ̅̂˶˸˸˷˼́˺̆
˅˃˃ˌ˂˄˂˅ˇ˻̇̇̃ˍ˂˂̊̊̊ˁ˿˼́˺ˁ̂˻˼̂ˀ̆̇˴̇˸ˁ˸˷̈˂˴˶˿˃ˋ˂̆˶˻˸˷̈˿˸ˁ˻̇̀˿
˧̅˴́̆˿˼̇˸̅˴̇˸ʳʳ
˛˼˿˷˸˵̅˴́˷ʿʳ˔˿̀̈̇ʳ˦˼˿˽˴ˎʳ˞˴̌ʳ˥̂̇̇̀˴́́ˎʳˠ̂˻˴̀˸˷ʳˡ̂˴̀˴́̌ˎʳˤ̈˼́ʳ˚˴̂ˎʳ˦˴́˽˼˾˴ʳ˛˸̊˴̉˼̇˻˴̅˴́˴ˎʳˡ˺̈̌˸́ʳ˕˴˶˻ˎʳ˦̇˸̃˻˴́ʳ
˩̂˺˸˿ʳ˥˸˶˸́̇ʳ˜̀̃̅̂̉˸̀˸́̇̆ʳ˼́ʳ̇˻˸ʳ˖ˠ˨ʳ˟˴̅˺˸ʳ˦˶˴˿˸ʳ˖˻˼́˸̆˸ˀ˘́˺˿˼̆˻ʳ˦ˠ˧ʳ˦̌̆̇˸̀ʳʳ
˝̂˻́̆̂́ʿʳˠ˴̅˾ʳ˨̆˼́˺ʳ˔˷˴̃̇̂̅ʳ˚̅˴̀̀˴̅̆ʳ̇̂ʳ˜˷˸́̇˼˹̌ʳ˦̌́˸̅˺˼˸̆ʳ˼́ʳ̇˻˸ʳ˨́̆̈̃˸̅̉˼̆˸˷ʳ˔˶̄̈˼̆˼̇˼̂́ʳ̂˹ʳ˟˼́˺̈˼̆̇˼˶ʳ˦̇̅̈˶̇̈̅˸ʳʳ
˞˴̅˴˾̂̆ʿʳ˗˴̀˼˴́̂̆ˎʳ˝˴̆̂́ʳ˘˼̆́˸̅ˎʳ˦˴́˽˸˸̉ʳ˞˻̈˷˴́̃̈̅ˎʳˠ˴̅˾̈̆ʳ˗̅˸̌˸̅ʳˠ˴˶˻˼́˸ʳ˧̅˴́̆˿˴̇˼̂́ʳ˦̌̆̇˸̀ʳ˖̂̀˵˼́˴̇˼̂́ʳ̈̆˼́˺ʳ˜˧˚ˀ
˵˴̆˸˷ʳ˔˿˼˺́̀˸́̇̆ʳʳ
˞˴̍˴̀˴ʿʳ˝̈́ϗ˼˶˻˼ˎʳ˞˸́̇˴̅̂ʳ˧̂̅˼̆˴̊˴ʳ˜́˷̈˶˼́˺ʳ˚˴̍˸̇̇˸˸̅̆ʳ˹̂̅ʳˡ˴̀˸˷ʳ˘́̇˼̇̌ʳ˥˸˶̂˺́˼̇˼̂́ʳ˵̌ʳ˟˴̅˺˸ˀ˦˶˴˿˸ʳ˖˿̈̆̇˸̅˼́˺ʳ̂˹ʳ
˗˸̃˸́˷˸́˶̌ʳ˥˸˿˴̇˼̂́̆ʳʳ
˞˸́́˸˷̌ʿʳ˔˿˼̆̇˴˼̅ˎʳ˦̇˴́ʳ˦̍̃˴˾̂̊˼˶̍ʳ˘̉˴˿̈˴̇˼́˺ʳ˥̂˺˸̇ϗ̆ʳ˧˻˸̆˴̈̅˼ʳʳ
˞̈˿˾˴̅́˼ʿʳ˔́˴˺˻˴ˎʳ˝˴̀˼˸ʳ˖˴˿˿˴́ʳ˗˼˶̇˼̂́˴̅̌ʳ˗˸˹˼́˼̇˼̂́̆ʳ˵˴̆˸˷ʳ˛̂̀̂˺̅˴̃˻ʳ˜˷˸́̇˼˹˼˶˴̇˼̂́ʳ̈̆˼́˺ʳ˴ʳ˚˸́˸̅˴̇˼̉˸ʳ˛˼˸̅˴̅˶˻˼˶˴˿ʳˠ̂˷˸˿ʳ
˟˼ʿʳ˪˸́˽˼˸ˎʳˣ˸́˺ʳ˭˻˴́˺ˎʳ˙̈̅̈ʳ˪˸˼ˎʳˬ̈˸̋˼˴́ʳ˛̂̈ˎʳˤ˼́ʳ˟̈ʳ˔ʳˡ̂̉˸˿ʳ˙˸˴̇̈̅˸ˀ˵˴̆˸˷ʳ˔̃̃̅̂˴˶˻ʳ̇̂ʳ˖˻˼́˸̆˸ʳ˘́̇˼̇̌ʳ˥˸˿˴̇˼̂́ʳ
˘̋̇̅˴˶̇˼̂́ʳʳ
˟˼ʿʳ˭˻˼˹˸˼ˎʳ˗˴̉˼˷ʳˬ˴̅̂̊̆˾̌ʳ˨́̆̈̃˸̅̉˼̆˸˷ʳ˧̅˴́̆˿˴̇˼̂́ʳ˜́˷̈˶̇˼̂́ʳ˹̂̅ʳ˖˻˼́˸̆˸ʳ˔˵˵̅˸̉˼˴̇˼̂́̆ʳ̈̆˼́˺ʳˠ̂́̂˿˼́˺̈˴˿ʳ˖̂̅̃̂̅˴ʳʳ
˟˼ʿʳ˝˼˴́˺̈̂ˎʳ˖˻̅˼̆ʳ˕̅˸̊ʳ˪˻˼˶˻ʳ˔̅˸ʳ̇˻˸ʳ˕˸̆̇ʳ˙˸˴̇̈̅˸̆ʳ˹̂̅ʳ˔̈̇̂̀˴̇˼˶ʳ˩˸̅˵ʳ˖˿˴̆̆˼˹˼˶˴̇˼̂́ʳʳ
˟˼̈ʿʳ˖˻˴̂ˀ˟˼́ˎʳ˝˸́ˀ˛̆˼˴́˺ʳ˟˼́ʳ˨̆˼́˺ʳ˦̇̅̈˶̇̈̅˴˿ʳ˜́˹̂̅̀˴̇˼̂́ʳ˹̂̅ʳ˜˷˸́̇˼˹̌˼́˺ʳ˦˼̀˼˿˴̅ʳ˖˻˼́˸̆˸ʳ˖˻˴̅˴˶̇˸̅̆ʳʳ
˟˼̈ʿʳˬ˴́˷̂́˺ˎʳ˘̈˺˸́˸ʳ˔˺˼˶˻̇˸˼́ʳˬ̂̈ϗ̉˸ʳ˚̂̇ʳ˔́̆̊˸̅̆ˍʳ˧̂̊˴̅˷̆ʳˣ˸̅̆̂́˴˿˼̍˸˷ʳˠ̂˷˸˿̆ʳ˹̂̅ʳˣ̅˸˷˼˶̇˼́˺ʳ˦̈˶˶˸̆̆ʳ˼́ʳ˖̂̀̀̈́˼̇̌ʳ
ˤ̈˸̆̇˼̂́ʳ˔́̆̊˸̅˼́˺ʳʳ
ˠ˶˖˿̂̆˾̌ʿʳ˗˴̉˼˷ˎʳ˘̈˺˸́˸ʳ˖˻˴̅́˼˴˾ʳ˦˸˿˹ˀ˧̅˴˼́˼́˺ʳ˹̂̅ʳ˕˼̂̀˸˷˼˶˴˿ʳˣ˴̅̆˼́˺ʳʳ
ˠ˼˿˿˸̅ʿʳ˧˼̀ˎʳ˪˼˿˿˼˴̀ʳ˦˶˻̈˿˸̅ʳ˔ʳ˨́˼˹˼˸˷ʳ˦̌́̇˴˶̇˼˶ʳˠ̂˷˸˿ʳ˹̂̅ʳˣ˴̅̆˼́˺ʳ˙˿̈˸́̇ʳ˴́˷ʳ˗˼̆˹˿̈˸́̇ʳ˦̃˸˸˶˻ʳʳ
ˠ̂˼˿˴́˸́ʿʳ˞˴̅̂ˎʳ˦̇˸̃˻˸́ʳˣ̈˿̀˴́ʳ˧˻˸ʳ˚̂̂˷ʿʳ̇˻˸ʳ˕˴˷ʿʳ˴́˷ʳ̇˻˸ʳ˨́˾́̂̊́ˍʳˠ̂̅̃˻̂̆̌˿˿˴˵˼˶ʳ˦˸́̇˼̀˸́̇ʳ˧˴˺˺˼́˺ʳ̂˹ʳ˨́̆˸˸́ʳ˪̂̅˷̆ʳ
ˠ̂̆˶˻˼̇̇˼ʿʳ˔˿˸̆̆˴́˷̅̂ˎʳ˦˼˿̉˼˴ʳˤ̈˴̅̇˸̅̂́˼ʳ˞˸̅́˸˿̆ʳ̂́ʳ˟˼́˺̈˼̆̇˼˶ʳ˦̇̅̈˶̇̈̅˸̆ʳ˹̂̅ʳ˔́̆̊˸̅ʳ˘̋̇̅˴˶̇˼̂́ʳʳ
ˠ̅̂̍˼́̆˾˼ʿʳ˝̂˴́́˴ˎʳ˘˷̊˴̅˷ʳ˪˻˼̇̇˴˾˸̅ˎʳ˦˴˷˴̂˾˼ʳ˙̈̅̈˼ʳ˖̂˿˿˸˶̇˼́˺ʳ˴ʳ˪˻̌ˀˤ̈˸̆̇˼̂́ʳ˖̂̅̃̈̆ʳ˹̂̅ʳ˗˸̉˸˿̂̃̀˸́̇ʳ˴́˷ʳ˘̉˴˿̈˴̇˼̂́ʳ̂˹ʳ
˴́ʳ˔̈̇̂̀˴̇˼˶ʳˤ˔ˀ˦̌̆̇˸̀ʳʳ
ˡ˴˾̂̉ʿʳˣ̅˸̆˿˴̉ˎʳˠ˴̅̇˼ʳ˔ˁʳ˛˸˴̅̆̇ʳ˦̂˿̉˼́˺ʳ˥˸˿˴̇˼̂́˴˿ʳ˦˼̀˼˿˴̅˼̇̌ʳˣ̅̂˵˿˸̀̆ʳ˨̆˼́˺ʳ̇˻˸ʳ˪˸˵ʳ˴̆ʳ˴ʳ˖̂̅̃̈̆ʳʳ
ˢ˿̆̆̂́ʿʳ˝ˁʳ˦˶̂̇̇ˎʳ˗̂̈˺˿˴̆ʳ˪ˁʳˢ˴̅˷ʳ˖̂̀˵˼́˼́˺ʳ˦̃˸˸˶˻ʳ˥˸̇̅˼˸̉˴˿ʳ˥˸̆̈˿̇̆ʳ̊˼̇˻ʳ˚˸́˸̅˴˿˼̍˸˷ʳ˔˷˷˼̇˼̉˸ʳˠ̂˷˸˿̆ʳʳ
ˣ˸́́ʿʳ˚˸̅˴˿˷ˎʳ˫˼˴̂˷˴́ʳ˭˻̈ʳ˔ʳ˖̅˼̇˼˶˴˿ʳ˥˸˴̆̆˸̆̆̀˸́̇ʳ̂˹ʳ˘̉˴˿̈˴̇˼̂́ʳ˕˴̆˸˿˼́˸̆ʳ˹̂̅ʳ˦̃˸˸˶˻ʳ˦̈̀̀˴̅˼̍˴̇˼̂́ʳʳ
ˣ̂˿˼˹̅̂́˼ʿʳ˝̂̆˸̃˻ˎʳˠ˴̅˼˿̌́ʳ˪˴˿˾˸̅ʳ˜́̇˸́̆˼̂́˴˿ʳ˦̈̀̀˴̅˼˸̆ʳ˴̆ʳ˖̂̂̃˸̅˴̇˼̉˸ʳ˥˸̆̃̂́̆˸̆ʳ˼́ʳ˗˼˴˿̂˺̈˸ˍʳ˔̈̇̂̀˴̇˼̂́ʳ˴́˷ʳ˘̉˴˿̈˴̇˼̂́ʳ
˥̂̇˻ʿʳ˥̌˴́ˎʳˢ̊˸́ʳ˥˴̀˵̂̊ˎʳˡ˼̍˴̅ʳ˛˴˵˴̆˻ˎʳˠ̂́˴ʳ˗˼˴˵ˎʳ˖̌́̇˻˼˴ʳ˥̈˷˼́ʳ˔̅˴˵˼˶ʳˠ̂̅̃˻̂˿̂˺˼˶˴˿ʳ˧˴˺˺˼́˺ʿʳ˗˼˴˶̅˼̇˼̍˴̇˼̂́ʿʳ˴́˷ʳ
˟˸̀̀˴̇˼̍˴̇˼̂́ʳ˨̆˼́˺ʳ˟˸̋˸̀˸ʳˠ̂˷˸˿̆ʳ˴́˷ʳ˙˸˴̇̈̅˸ʳ˥˴́˾˼́˺ʳʳ
˦˴˻˴ʿʳ˦̈˽˴́ʳ˞̈̀˴̅ˎʳˣ˴˵˼̇̅˴ʳˠ˼̇̅˴ˎʳ˦̈˷˸̆˻́˴ʳ˦˴̅˾˴̅ʳ˪̂̅˷ʳ˖˿̈̆̇˸̅˼́˺ʳ˴́˷ʳ˪̂̅˷ʳ˦˸˿˸˶̇˼̂́ʳ˕˴̆˸˷ʳ˙˸˴̇̈̅˸ʳ˥˸˷̈˶̇˼̂́ʳ˹̂̅ʳˠ˴̋˘́̇ʳ
˕˴̆˸˷ʳ˛˼́˷˼ʳˡ˘˥ʳʳ
˦˶˻̈˿̇˸ʳ˼̀ʳ˪˴˿˷˸ʿʳ˦˴˵˼́˸ˎʳ˖˻̅˼̆̇˼˴́ʳ˛̌˼́˺ˎʳ˖˻̅˼̆̇˼˴́ʳ˦˶˻˸˼˵˿˸ˎʳ˛˸˿̀̈̇ʳ˦˶˻̀˼˷ʳ˖̂̀˵˼́˼́˺ʳ˘ˠʳ˧̅˴˼́˼́˺ʳ˴́˷ʳ̇˻˸ʳˠ˗˟ʳ
ˣ̅˼́˶˼̃˿˸ʳ˹̂̅ʳ˴́ʳ˔̈̇̂̀˴̇˼˶ʳ˩˸̅˵ʳ˖˿˴̆̆˼˹˼˶˴̇˼̂́ʳ˜́˶̂̅̃̂̅˴̇˼́˺ʳ˦˸˿˸˶̇˼̂́˴˿ʳˣ̅˸˹˸̅˸́˶˸̆ʳʳ
˦̌˸˷ʿʳ˨̀˴̅ˎʳ˝˴̆̂́ʳ˪˼˿˿˼˴̀̆ʳ˨̆˼́˺ʳ˔̈̇̂̀˴̇˼˶˴˿˿̌ʳ˧̅˴́̆˶̅˼˵˸˷ʳ˗˼˴˿̂˺̆ʳ̇̂ʳ˟˸˴̅́ʳ˨̆˸̅ʳˠ̂˷˸˿̆ʳ˼́ʳ˴ʳ˦̃̂˾˸́ʳ˗˼˴˿̂˺ʳ˦̌̆̇˸̀ʳʳ
˧˴˿˵̂̇ʿʳ˗˴̉˼˷ˎʳ˧˻̂̅̆̇˸́ʳ˕̅˴́̇̆ʳ˥˴́˷̂̀˼̍˸˷ʳ˟˴́˺̈˴˺˸ʳˠ̂˷˸˿̆ʳ̉˼˴ʳˣ˸̅˹˸˶̇ʳ˛˴̆˻ʳ˙̈́˶̇˼̂́̆ʳʳ
˧̂̈̇˴́̂̉˴ʿʳ˞̅˼̆̇˼́˴ˎʳ˛˼̆˴̀˼ʳ˦̈̍̈˾˼ˎʳ˔˶˻˼̀ʳ˥̈̂̃̃ʳ˔̃̃˿̌˼́˺ʳˠ̂̅̃˻̂˿̂˺̌ʳ˚˸́˸̅˴̇˼̂́ʳˠ̂˷˸˿̆ʳ̇̂ʳˠ˴˶˻˼́˸ʳ˧̅˴́̆˿˴̇˼̂́ʳʳ
˧̆̈˶˻˼̌˴ʿʳˠ˴̆˴̇̂̆˻˼ˎʳ˦˻˼́̌˴ʳ˛˼˷˴ˎʳ˦˸˼˼˶˻˼ʳˡ˴˾˴˺˴̊˴ʳ˥̂˵̈̆̇ʳ˘̋̇̅˴˶̇˼̂́ʳ̂˹ʳˡ˴̀˸˷ʳ˘́̇˼̇̌ʳ˜́˶˿̈˷˼́˺ʳ˨́˹˴̀˼˿˼˴̅ʳ˪̂̅˷ʳʳ
˩˸˴˿˸ʿʳ˧̂́̌ˎʳˬ˴́˹˸́ʳ˛˴̂ˎʳ˚̈̂˹̈ʳ˟˼ʳˠ̈˿̇˼˿˼́˺̈˴˿ʳ˛˴̅̉˸̆̇˼́˺ʳ̂˹ʳ˖̅̂̆̆ˀ˖̈˿̇̈̅˴˿ʳ˦̇˸̅˸̂̇̌̃˸̆ʳʳ
˪˴́ʿʳ˦̇˸̃˻˸́ˎʳ˖˸˶˼˿˸ʳˣ˴̅˼̆ʳ˜́ˀ˕̅̂̊̆˸̅ʳ˦̈̀̀˴̅˼̆˴̇˼̂́ˍʳ˚˸́˸̅˴̇˼́˺ʳ˘˿˴˵̂̅˴̇˼̉˸ʳ˦̈̀̀˴̅˼˸̆ʳ˕˼˴̆˸˷ʳ˧̂̊˴̅˷̆ʳ̇˻˸ʳ˥˸˴˷˼́˺ʳ
˖̂́̇˸̋̇ʳʳ
˪˴́˺ʿʳˤ˼́ʳ˜̅˼̆ˎʳ˗˴˿˸ʳ˦˶˻̈̈̅̀˴́̆ˎʳ˗˸˾˴́˺ʳ˟˼́ʳ˦˸̀˼ˀ˦̈̃˸̅̉˼̆˸˷ʳ˖̂́̉˸̋ʳ˧̅˴˼́˼́˺ʳ˹̂̅ʳ˗˸̃˸́˷˸́˶̌ʳˣ˴̅̆˼́˺ʳʳ
˫˼˴ʿʳˬ̈́̄˼́˺ˎʳ˟˼́˿˼́ʳ˪˴́˺ˎʳ˞˴̀ˀ˙˴˼ʳ˪̂́˺ˎʳˠ˼́˺̋˼́˺ʳ˫̈ʳ˟̌̅˼˶ˀ˵˴̆˸˷ʳ˦̂́˺ʳ˦˸́̇˼̀˸́̇ʳ˖˿˴̆̆˼˹˼˶˴̇˼̂́ʳ̊˼̇˻ʳ˦˸́̇˼̀˸́̇ʳ˩˸˶̇̂̅ʳ
˦̃˴˶˸ʳˠ̂˷˸˿ʳʳ
ˬ˴̀˴́˺˼˿ʿʳ˘˿˼˹ˎʳ˥˴́˼ʳˡ˸˿˾˸́ʳˠ˼́˼́˺ʳ˪˼˾˼̃˸˷˼˴ʳ˥˸̉˼̆˼̂́ʳ˛˼̆̇̂̅˼˸̆ʳ˹̂̅ʳ˜̀̃̅̂̉˼́˺ʳ˦˸́̇˸́˶˸ʳ˖̂̀̃̅˸̆̆˼̂́ʳʳ
ˬ˴́˺ʿʳ˙˴́ˎʳ˝̈́ʳ˭˻˴̂ˎʳ˕̂ʳ˭̂̈ˎʳ˞˴́˺ʳ˟˼̈ˎʳ˙˸˼˹˴́ʳ˟˼̈ʳ˖˻˼́˸̆˸ˀ˘́˺˿˼̆˻ʳ˕˴˶˾̊˴̅˷ʳ˧̅˴́̆˿˼̇˸̅˴̇˼̂́ʳ˔̆̆˼̆̇˸˷ʳ̊˼̇˻ʳˠ˼́˼́˺ʳ
ˠ̂́̂˿˼́˺̈˴˿ʳ˪˸˵ʳˣ˴˺˸̆ʳʳ
ˬ̈̅˸̇ʿʳ˗˸́˼̍ʳ˦̀̂̂̇˻˼́˺ʳ˴ʳ˧˸̅˴ˀ̊̂̅˷ʳ˟˴́˺̈˴˺˸ʳˠ̂˷˸˿ʳʳ
˭˴̃˼̅˴˼́ʿʳ˕˸΄˴̇ˎʳ˘́˸˾̂ʳ˔˺˼̅̅˸ˎʳ˟˿̈΀̆ʳˠͳ̅̄̈˸̍ʳ˥̂˵̈̆̇́˸̆̆ʳ˴́˷ʳ˚˸́˸̅˴˿˼̍˴̇˼̂́ʳ̂˹ʳ˥̂˿˸ʳ˦˸̇̆ˍʳˣ̅̂̃˕˴́˾ʳ̉̆ˁʳ˩˸̅˵ˡ˸̇ʳʳ
˭˻˴́˺ʿʳˠ˼́ˎʳ˛̂́˺˹˸˼ʳ˝˼˴́˺ˎʳ˔˼̇˼ʳ˔̊ˎʳ˛˴˼̍˻̂̈ʳ˟˼ˎʳ˖˻˸̊ʳ˟˼̀ʳ˧˴́ˎʳ˦˻˸́˺ʳ˟˼ʳ˔ʳ˧̅˸˸ʳ˦˸̄̈˸́˶˸ʳ˔˿˼˺́̀˸́̇ˀ˵˴̆˸˷ʳ˧̅˸˸ˀ̇̂ˀ˧̅˸˸ʳ
˧̅˴́̆˿˴̇˼̂́ʳˠ̂˷˸˿ʳʳ
˗˸̀̂̆ʳʻˉˍ˃˃ˀˋˍˆ˃ʼʳ
ʳ
ˉˍ˃˃ˀˋˍˆ˃ˍʳ˪˼˿˿˼˴̀̆ʿʳ˝˴̆̂́ʳ˗˸̀̂́̆̇̅˴̇˼̂́ʳ̂˹ʳ˴ʳˣˢˠ˗ˣʳ˩̂˼˶˸ʳ˗˼˴˿˸̅ʳʳ
ˉˍ˃˃ˀˋˍˆ˃ˍʳ˦˼˷˷˻˴̅̇˻˴́ʿʳ˔˷̉˴˼̇˻ˎʳ˔́́ʳ˖̂̃˸̆̇˴˾˸ʳ˚˸́˸̅˴̇˼́˺ʳ˥˸̆˸˴̅˶˻ʳ˪˸˵̆˼̇˸̆ʳ˨̆˼́˺ʳ˦̈̀̀˴̅˼̆˴̇˼̂́ʳ˧˸˶˻́˼̄̈˸̆ʳʳ
ˉˍ˃˃ˀˋˍˆ˃ˍʳ˩˸̅̆˿˸̌ʿʳˬ˴́́˼˶˾ˎʳ˦˼̀̂́˸ʳˣ˴̂˿̂ʳˣ̂́̍˸̇̇̂ˎʳˠ˴̆̆˼̀̂ʳˣ̂˸̆˼̂ˎʳ˩˿˴˷˼̀˼̅ʳ˘˼˷˸˿̀˴́ˎʳ˔˿˴́ʳ˝˸̅́ˎʳ˝˴̆̂́ʳ˦̀˼̇˻ˎʳ˫˼˴̂˹˸́˺ʳ
ˬ˴́˺ˎʳ˔˿˸̆̆˴́˷̅̂ʳˠ̂̆˶˻˼̇̇˼ʳ˕˔˥˧ˍʳ˔ʳˠ̂˷̈˿˴̅ʳ˧̂̂˿˾˼̇ʳ˹̂̅ʳ˖̂̅˸˹˸̅˸́˶˸ʳ˥˸̆̂˿̈̇˼̂́ʳʳ
ˉˍ˃˃ˀˋˍˆ˃ˍʳˢϗ˗̂́́˸˿˿ʿʳˠ˼˶˾ʳ˗˸̀̂́̆̇̅˴̇˼̂́ʳ̂˹ʳ̇˻˸ʳ˨˔ˠʳ˖̂̅̃̈̆˧̂̂˿ʳ˹̂̅ʳ˧˸̋̇ʳ˴́˷ʳ˜̀˴˺˸ʳ˔́́̂̇˴̇˼̂́ʳʳ
ˉˍ˃˃ˀˋˍˆ˃ˍʳ˛̈˺˺˼́̆ˀ˗˴˼́˸̆ʿʳ˗˴̉˼˷ˎʳ˔˿˸̋˴́˷˸̅ʳ˜ˁʳ˥̈˷́˼˶˾̌ʳ˜́̇˸̅˴˶̇˼̉˸ʳ˔˦˥ʳ˘̅̅̂̅ʳ˖̂̅̅˸˶̇˼̂́ʳ˹̂̅ʳ˧̂̈˶˻̆˶̅˸˸́ʳ˗˸̉˼˶˸̆ʳʳ
ˉˍ˃˃ˀˋˍˆ˃ˍʳ˚˸̅̀˴́́ʿʳ˨˿̅˼˶˻ʳˬ˴̊˴̇ˍʳˬ˸̇ʳ˔́̂̇˻˸̅ʳ˪̂̅˷ʳ˔˿˼˺́̀˸́̇ʳ˧̂̂˿ʳʳ
ˉˍ˃˃ˀˋˍˆ˃ˍʳ˞˴́˺ʿʳˠ̂̂́̌̂̈́˺ˎʳ˦̂̈̅˼̆˻ʳ˖˻˴̈˷˻̈̅˼ˎʳˠ˴˻˸̆˻ʳ˝̂̆˻˼ˎʳ˖˴̅̂˿̌́ʳˣˁʳ˥̂̆ͼʳ˦˜˗˘ˍʳ˧˻˸ʳ˦̈̀̀˴̅˼̍˴̇˼̂́ʳ˜́̇˸˺̅˴̇˸˷ʳ
˗˸̉˸˿̂̃̀˸́̇ʳ˘́̉˼̅̂́̀˸́̇ʳʳ
ˉˍ˃˃ˀˋˍˆ˃ˍʳˬ˴̅̅˼́˺̇̂́ʿʳ˗˸˵̅˴ˎʳ˝̂˻́ʳ˚̅˴̌ˎʳ˖˻̅˼̆ʳˣ˸́́˼́˺̇̂́ˎʳ˛ˁʳ˧˼̀̂̇˻̌ʳ˕̈́́˸˿˿ˎʳ˔˿˿˸˺̅˴ʳ˖̂̅́˴˺˿˼˴ˎʳ˝˴̆̂́ʳ˟˼˿˿˸̌ˎʳ˞̌̂˾̂ʳ
รʳˇʳ଄Δ٥ʳˌʳ଄ˣ̅̂˶˸˸˷˼́˺̆
˅˃˃ˌ˂˄˂˅ˇ˻̇̇̃ˍ˂˂̊̊̊ˁ˿˼́˺ˁ̂˻˼̂ˀ̆̇˴̇˸ˁ˸˷̈˂˴˶˿˃ˋ˂̆˶˻˸˷̈˿˸ˁ˻̇̀˿
˅ˍˈˈΩˆˍ˅˃ˍʳ˪˸˸̅˾˴̀̃ʿʳ˪̂̈̇˸̅ˎʳˠ˴˴̅̇˸́ʳ˷˸ʳ˥˼˽˾˸ʳ˖̅˸˷˼˵˼˿˼̇̌ʳ˜̀̃̅̂̉˸̆ʳ˧̂̃˼˶˴˿ʳ˕˿̂˺ʳˣ̂̆̇ʳ˥˸̇̅˼˸̉˴˿ʳʳ
ˆˍ˅˃Ωˆˍˇˈˍʳ˖̆̂̀˴˼ʿʳ˔́˷̅˴̆ˎʳ˥˴˷˴ʳˠ˼˻˴˿˶˸˴ʳ˟˼́˺̈˼̆̇˼˶˴˿˿̌ʳˠ̂̇˼̉˴̇˸˷ʳ˙˸˴̇̈̅˸̆ʳ˹̂̅ʳ˘́˻˴́˶˸˷ʳ˕˴˶˾ˀ̂˹ˀ̇˻˸ˀ˕̂̂˾ʳ˜́˷˸̋˼́˺ʳʳ
ˆˍˇˈΩˇˍ˄˃ˍʳ˘˿̆˴̌˸˷ʿʳ˧˴̀˸̅ˎʳ˗̂̈˺˿˴̆ʳ˪ˁʳˢ˴̅˷ˎʳ˚˴˿˼˿˸̂ʳˡ˴̀˴̇˴ʳ˥˸̆̂˿̉˼́˺ʳˣ˸̅̆̂́˴˿ʳˡ˴̀˸̆ʳ˼́ʳ˘̀˴˼˿ʳ˨̆˼́˺ʳ˖̂́̇˸̋̇ʳ˘̋̃˴́̆˼̂́ʳ
˦˸̆̆˼̂́ʳˋ˕ˍʳ˦̌́̇˴̋ʳʹʳˣ˴̅̆˼́˺ʳˆʳ
ʳ
˅ˍˆ˃Ω˅ˍˈˈˍʳˡ˼̉̅˸ʿʳ˝̂˴˾˼̀ˎʳ˥̌˴́ʳˠ˶˗̂́˴˿˷ʳ˜́̇˸˺̅˴̇˼́˺ʳ˚̅˴̃˻ˀ˕˴̆˸˷ʳ˴́˷ʳ˧̅˴́̆˼̇˼̂́ˀ˕˴̆˸˷ʳ˗˸̃˸́˷˸́˶̌ʳˣ˴̅̆˸̅̆ʳʳ
˅ˍˈˈΩˆˍ˅˃ˍʳ˙˼́˾˸˿ʿʳ˝˸́́̌ʳ˥̂̆˸ˎʳ˔˿˸̋ʳ˞˿˸˸̀˴́ˎʳ˖˻̅˼̆̇̂̃˻˸̅ʳ˗ˁʳˠ˴́́˼́˺ʳ˘˹˹˼˶˼˸́̇ʿʳ˙˸˴̇̈̅˸ˀ˵˴̆˸˷ʿʳ˖̂́˷˼̇˼̂́˴˿ʳ˥˴́˷̂̀ʳ˙˼˸˿˷ʳ
ˣ˴̅̆˼́˺ʳʳ
ˆˍ˅˃Ωˆˍˇˈˍʳ˚Ά̀˸̍ˀ˥̂˷̅΀˺̈˸̍ʿʳ˖˴̅˿̂̆ˎʳ˝̂˻́ʳ˖˴̅̅̂˿˿ˎʳ˗˴̉˼˷ʳ˪˸˼̅ʳ˔ʳ˗˸˷̈˶̇˼̉˸ʳ˔̃̃̅̂˴˶˻ʳ̇̂ʳ˗˸̃˸́˷˸́˶̌ʳˣ˴̅̆˼́˺ʳʳ
ˆˍˇˈΩˇˍ˄˃ˍʳ˕˸́˷˸̅ʿʳ˘̀˼˿̌ʳˠˁʳ˘̉˴˿̈˴̇˼́˺ʳ˴ʳ˖̅̂̆̆˿˼́˺̈˼̆̇˼˶ʳ˚̅˴̀̀˴̅ʳ˥˸̆̂̈̅˶˸ˍʳ˔ʳ˖˴̆˸ʳ˦̇̈˷̌ʳ̂˹ʳ˪˴̀˵˴̌˴ʳʳ
˦˸̆̆˼̂́ʳˋ˖ˍʳˠ˴˶˻˼́˸ʳ˧̅˴́̆˿˴̇˼̂́ʳ˅ʳ
ʳ
˅ˍˆ˃Ω˅ˍˈˈˍʳ˚˴́˶˻˸̉ʿʳ˞̈̍̀˴́ˎʳ˝̂Ͷ̂ʳ˩ˁʳ˚̅˴ͺ˴ˎʳ˕˸́ʳ˧˴̆˾˴̅ʳ˕˸̇̇˸̅ʳ˔˿˼˺́̀˸́̇̆ʳːʳ˕˸̇̇˸̅ʳ˧̅˴́̆˿˴̇˼̂́̆˒ʳʳ
˅ˍˈˈΩˆˍ˅˃ˍʳ˟˼́ʿʳ˗˸˾˴́˺ˎʳ˦˻˴̂˽̈́ʳ˭˻˴̂ˎʳ˕˸́˽˴̀˼́ʳ˩˴́ʳ˗̈̅̀˸ˎʳˠ˴̅˼̈̆ʳˣ˴̆˶˴ʳˠ˼́˼́˺ʳˣ˴̅˸́̇˻˸̇˼˶˴˿ʳ˧̅˴́̆˿˴̇˼̂́̆ʳ˹̅̂̀ʳ̇˻˸ʳ˪˸˵ʳ
˵̌ʳ˪̂̅˷ʳ˔˿˼˺́̀˸́̇ʳʳ
ˆˍ˅˃Ωˆˍˇˈˍʳˠ˴̅̇̂́ʿʳˬ̈̉˴˿ˎʳˣ˻˼˿˼̃ʳ˥˸̆́˼˾ʳ˦̂˹̇ʳ˦̌́̇˴˶̇˼˶ʳ˖̂́̆̇̅˴˼́̇̆ʳ˹̂̅ʳ˛˼˸̅˴̅˶˻˼˶˴˿ʳˣ˻̅˴̆˸˷ˀ˕˴̆˸˷ʳ˧̅˴́̆˿˴̇˼̂́ʳʳ
ˆˍˇˈΩˇˍ˄˃ˍʳ˗̌˸̅ʿʳ˖˻̅˼̆̇̂̃˻˸̅ˎʳ˦̀˴̅˴́˷˴ʳˠ̈̅˸̆˴́ˎʳˣ˻˼˿˼̃ʳ˥˸̆́˼˾ʳ˚˸́˸̅˴˿˼̍˼́˺ʳ˪̂̅˷ʳ˟˴̇̇˼˶˸ʳ˧̅˴́̆˿˴̇˼̂́ʳʳ
˦˸̆̆˼̂́ʳˋ˗ˍʳ˦˸̀˴́̇˼˶̆ʳˇʳ
ʳ
˅ˍˆ˃Ω˅ˍˈˈˍʳ˭˻˴̂ʿʳ˦˻˼̄˼ˎʳ˖˻˸́˺ʳˡ˼̈ˎʳˠ˼́˺ʳ˭˻̂̈ˎʳ˧˼́˺ʳ˟˼̈ˎʳ˦˻˸́˺ʳ˟˼ʳ˖̂̀˵˼́˼́˺ʳˠ̈˿̇˼̃˿˸ʳ˥˸̆̂̈̅˶˸̆ʳ̇̂ʳ˜̀̃̅̂̉˸ʳ˦ˠ˧ˀ˵˴̆˸˷ʳ
ˣ˴̅˴̃˻̅˴̆˼́˺ʳˠ̂˷˸˿ʳʳ
˅ˍˈˈΩˆˍ˅˃ˍʳ˦̅˼˾̈̀˴̅ʿʳ˩˼̉˸˾ˎʳ˥̂˼ʳ˥˸˼˶˻˴̅̇ˎʳˠ˴̅˾ʳ˦˴̀̀̂́̆ˎʳ˔̅˼ʳ˥˴̃̃̂̃̂̅̇ˎʳ˗˴́ʳ˥̂̇˻ʳ˘̋̇̅˴˶̇˼̂́ʳ̂˹ʳ˘́̇˴˼˿˸˷ʳ˦˸̀˴́̇˼˶ʳ
˥˸˿˴̇˼̂́̆ʳ˧˻̅̂̈˺˻ʳ˦̌́̇˴̋ˀ˕˴̆˸˷ʳ˖̂̀̀˴ʳ˥˸̆̂˿̈̇˼̂́ʳʳ
ˆˍ˅˃Ωˆˍˇˈˍʳ˷˸ʳˠ˴̅́˸˹˹˸ʿʳˠ˴̅˼˸ˀ˖˴̇˻˸̅˼́˸ˎʳ˔́́˴ʳˡˁʳ˥˴˹˹˸̅̇̌ˎʳ˖˻̅˼̆̇̂̃˻˸̅ʳ˗ˁʳˠ˴́́˼́˺ʳ˙˼́˷˼́˺ʳ˖̂́̇̅˴˷˼˶̇˼̂́̆ʳ˼́ʳ˧˸̋̇ʳʳ
ˆˍˇˈΩˇˍ˄˃ˍʳ˞̂̍˴̅˸̉˴ʿʳ˭̂̅́˼̇̆˴ˎʳ˘˿˿˸́ʳ˥˼˿̂˹˹ˎʳ˘˷̈˴̅˷ʳ˛̂̉̌ʳ˦˸̀˴́̇˼˶ʳ˖˿˴̆̆ʳ˟˸˴̅́˼́˺ʳ˹̅̂̀ʳ̇˻˸ʳ˪˸˵ʳ̊˼̇˻ʳ˛̌̃̂́̌̀ʳˣ˴̇̇˸̅́ʳ
˟˼́˾˴˺˸ʳ˚̅˴̃˻̆ʳʳ
ˇˍˇ˃Ωˉˍ˄˃ʳ˟˼˹˸̇˼̀˸ʳ˔˶˻˼˸̉˸̀˸́̇ʳ˔̊˴̅˷ʳˣ̅˸̆˸́̇˴̇˼̂́ʳ˴́˷ʳ˖˿̂̆˼́˺ʳ˦˸̆̆˼̂́ʳʳ
รʳˌʳ଄Δ٥ʳˌʳ଄ˣ̅̂˶˸˸˷˼́˺̆
˅˃˃ˌ˂˄˂˅ˇ˻̇̇̃ˍ˂˂̊̊̊ˁ˿˼́˺ˁ̂˻˼̂ˀ̆̇˴̇˸ˁ˸˷̈˂˴˶˿˃ˋ˂̆˶˻˸˷̈˿˸ˁ˻̇̀˿
