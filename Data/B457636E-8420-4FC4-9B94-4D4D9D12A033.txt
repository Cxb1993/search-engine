 2 
  
目錄 
目錄 ........................................................................................................................................... 2 
1. Introduction ........................................................................................................................ 5 
2. Related work ...................................................................................................................... 6 
3. Automatic testing mechanism ............................................................................................ 7 
3.1. Unit testing .............................................................................................................. 7 
3.1.1. Algorithm for automatically generating test data ............................................. 8 
3.1.2. Algorithm for automatically generating test case ........................................... 10 
3.2. Coverage testing .................................................................................................... 13 
3.2.1. Coverage testing ............................................................................................. 13 
3.2.2. Algorithm for multi-round testing .................................................................. 14 
3.3. Parallel program performance measurement ......................................................... 16 
4. Automatic testing tool for embedded software ................................................................ 18 
3.4. ATEMES system module description ................................................................... 18 
4.1. System architecture layers ..................................................................................... 19 
4.2. Pre-processing testing module ............................................................................... 19 
4.3. Host-side testing module ....................................................................................... 21 
4.4. Target-side testing module .................................................................................... 23 
4.5. Post-processing testing module ............................................................................. 25 
4.6. Multi-round testing scenario .................................................................................. 26 
5. Experiment ....................................................................................................................... 27 
5.1. Coverage testing experiment ................................................................................. 27 
5.2. Unit testing experiment ......................................................................................... 28 
5.3. Multi-core  utilization  monitoring  experiment .................................................... 30 
5.4. Parallelism benchmark experiment ....................................................................... 31 
5.5. Usability assessment experiment ........................................................................... 32 
5.6. Discussion .............................................................................................................. 33 
5.7. Response time experiment for parallel program to handle different interrupt 
intervals with different core numbers ............................................................................... 34 
6. Conclusion ....................................................................................................................... 36 
References ............................................................................................................................... 37 
國科會補助專題研究計畫成果報告自評表 ........................................................................ 39 
國科會補助計畫衍生研發成果推廣資料表 ........................................................................ 41 
 
 
 4 
多核心嵌入式軟體之模型驅動整合開發環境-VMC－ 
子計畫七:多核心嵌入式軟體設計之測試支援系統(2/2) 
 
Abstract 
Software testing during the development process of embedded software is not only complex, but also the 
heart of quality control. Multi-core embedded software testing faces even more challenges. Major issues 
include: (1) how demanding efforts and repetitive tedious actions can be reduced; (2) how resource 
restraints of embedded system platform such as temporal and memory capacity can be tackled; (3) how 
embedded software parallelism degree can be controlled to empower multi-core CPU computing capacity; 
(4) how analysis is exercised to ensure sufficient coverage test of embedded software; (5) how to do data 
synchronization to address issues such as race conditions in the interrupt driven multi-core embedded 
system; (6) high level reliability testing to ensure customer satisfaction. To address these issues, this study 
develops an automatic testing environment for multi-core embedded software (ATEMES). Based on the 
automatic mechanism, the system can parse source code, instrument source code, generate testing programs 
for test case and test driver, support generating primitive, structure and object types of test input data, multi-
round cross-testing, and visualize testing results. To both reduce test engineer’s burden and enhance his 
efficiency when embedded software testing is in process, this system developed automatic testing functions 
including unit testing, coverage testing, multi-core performance monitoring. Moreover, ATEMES can 
perform automatic multi-round cross-testing benchmark testing on multi-core embedded platform for 
parallel programs adopting Intel TBB library to recommend optimized parallel parameters such as pipeline 
tokens. Using ATEMES on the ARM11 multi-core platform to conduct testing experiments, the results show 
that our constructed testing environment is effective, and can reduce burdens of test engineer, and can 
enhance efficiency of testing task. 
 
Keywords: Automatic testing Embedded software testing Coverage testing Unit testing Cross-testing 
Testing tool Test case generation Object testing Multi-core embedded software testing 
Parallelism degree testing TBB testing 
 6 
How to do data synchronization to address issues such as race conditions in the interrupt driven multi-core 
embedded system;  (6) High level reliability testing to ensure customer satisfaction. 
This study developed an automatic testing tool to support cross-testing to both reduce target program 
overhead from performing testing functionality on embedded platform and decrease efforts for test engineer.  
The ATEMES can not only automatically generate test data with primitive type, structure type, object type 
and array type but also generate CppUnit-based test case and test driver.  This addresses the first issue. 
Moreover, ATEMES can execute automatic multi-round performance testing over multi-core 
embedded software adopting Intel TBB library. The system can support locating recommended value for 
better parallel parameter token number, which not only allows embedded software parallelism but also 
facilitates computing capacity of multi-core CPU to operate more efficiently.  This feature addresses the 
issues of (2) and (3).   
 With the automatic multi-round mechanism, unit testing and coverage testing can be implemented to 
save test engineer from massive repetitive tasks. With the cross-testing technology between host-side 
(workstation) and target-side (embedded system), factors resulted from embedded system resource restraints 
can be reduced for testing.  With the cross-testing technology, test case, test driver and target program can 
be cross-compiled automatically and uploaded to target-side for automatic implementation.  Target-side test 
log data including runtime data, output result, and data of each core utilization during runtime from target-
side CPU can be passed to host-side for runtime analysis, results of which can also be visualized presented.  
This helps tackling the issues of (1) and (4). 
To address issue (5), we instrument proper lock mechanism, such as mutex of C++, to source code 
segment of target program pertinent to performance testing.  This way the share data can be shielded, 
and accurate testing results can be ensured.  As for multi-core system performance monitor, to analyze 
overhead of each CPU core in the embedded system, ARM Linux system call is also requisite to effectively 
monitor CPU core number allocated by the task.  Repetitive automatic multi-round testing, similarly, 
provides a vital scenario to analyze more precisely what performance the task is executing in embedded 
system, and to locate bottleneck to be tackled.  We measure the response time of target program(parallel 
program) with different interrupt intervals using different core processor numbers on the ARM11 multi-core 
platform.  The test data show that the response time generally decreases as number of cores increases.  This 
provides evidences that our constructed testing environment can not only reduce burdens of test engineer, 
but also enhance efficiency of multi-core embedded testing task.  This helps relieve both issues (3) and (5).  
Finally, a set of usability testing has been arranged to evaluate testing reliability for issue (6). 
 
 
2. Related work 
Delamaro et al. (2006) developed a coverage testing tool for mobile device software.  The tool, named 
JaBUTi/ME, mainly supports java source code and can solve restrictions of mobile device performance and 
storage. Testing conducted on mobile device is difficult since issues such as memory limitations, persistent 
storage, and network connection availability have to be taken into consideration.  
The tool not only can be implemented on emulators, but also can help testing on mobile device. With a 
desktop computer, test engineer can implement testing, instrument class, and generate test session on mobile 
devices. Communication between desktop computer and mobile device is exchanged through test server. 
Execution of test case can be monitored through the transfer of trace data when instrumented code is being 
executed on mobile device. This approach can reduce workloads from testing mobile device. Visualized 
interface of test result allows test engineer to be informed of what programs are to be executed. However, 
the tool does not support automatic testing. Failure of automatically generating test input data and test case 
results in more tasks from test engineer to edit test source code and manually generating test input data. 
 8 
 
For automatic testing, firstly, test input data is generated automatically from ATEMES. However, test 
result data is not included in test data since it cannot be generated automatically from ATEMES. Secondly, 
ATEMES generates programs of test case and test driver automatically. Finally, the system would perform 
multi-round testing. Exception or segment fault occurred from target program is recorded when testing 
is in process. Output is not checked by the system. 
 
For semi-automatic testing, firstly, test input data is either generated automatically from ATEMES or 
from manual input by test engineer. Secondly, test engineer has to input test result data. Thirdly, ATEMES 
generates programs of test case and test driver automatically. Finally, the system would perform multi-
round testing. Exception or segment fault occurred from target program is recorded when testing is in 
process. Likewise, output is checked according to test result data. 
 
 
3.1.1. Algorithm for automatically generating test data  
ATEMES can generate complex data type for automatic testing. Figure 1 displays algorithm for 
automatic generating test data. Following are the term descriptions pertinent to the algorithm.  The 
automatic generation of test data makes sure the data format is consistent with the testing requirements of 
ATEMES environments.  In addition, the generated test data allows input parameter of unit testing to 
support C\C++ complex data type.  This is a unique feature of the ATEMES, none seen in many existing test 
systems except for Putrycz’s system.  This is evidenced by Table 1 in the conclusion.  
 
Function-List: The term means saving all function/method information from target program, including 
name, parameter type, and return type. Function-List stands for data parsed by Code Analyzer 
module. 
Struct-List: The term means saving all structure type attributes from target program. 
Object-List: The term means saving all object type attributes from target program. 
Parameter-List: The term means saving all parameter types of function/method. 
TestInputDataGenerationMainFunction: This is the main function for generating test input data. It 
controls the flow for getting all parameter type and return type of function/method information 
from target program, and generate input data.  
TestInputDataGenerationGenerator: This function is mainly responsible for generating test data. 
 
Steps of the algorithm are described as follows: 
 
Step 1：TestInputDataGenerationMainFunction procedure gets method name, parameter type, and return 
type from Function-List. Parameter type and return type are then passed to TestInputDataGenerator 
procedure. Repeat previous actions till data of Function-List is empty. 
Step 2：TestInputDataGenerator creates corresponding test input data based on parameter type. 
Step 2-1：If the type is primitive, the system will create corresponding test data such as integer, float, 
and char. 
Step 2-2：If the type is array, the system will get array dimension and calculate the intended data volume, 
and creates corresponding test data. 
Step 2-3：If the type is structure type, the system will recognize all primitive type information from the 
structure recursively to create corresponding test data. 
Step 2-4：If the type is object type, the system will recognize all primitive type data from the object 
recursively to create corresponding test data. 
 10 
3.1.2. Algorithm for automatically generating test case 
After test data are created, all test cases of target function/method also have to be created.  By reading 
each test data from the file, ATEMES would generate test cases based on each test data type, and then 
would generate test drivers. When testing is in process, the generated test case would check if any exception 
or segment fault occurs from target program. The system then verifies executed result according to the 
expected result.  This feature is unique for ATEMES and not seen in other similar works as seen in Table 1 
of conclusion. 
Figure 2 displays algorithm for automatic generated test case. Following are the term descriptions 
pertinent to the algorithm. 
 
Function-List, Struct-List, Object-List and Parameter-List: Descriptions of these teams are the same 
with previous section. 
TestCaseGenerationFunction: This function is mainly responsible for generating test input data based on 
the parameter types of function/method. 
 
Steps of the algorithm are described as follows: 
 
Step 1：Get method name, parameter type, and return type from Function-List. 
Step 2：Generate function signature source code segment for test case. 
Step 3：Determine parameter types as primitive type, structure type, or object type. 
Step 4：Instrument a source code segment for variable declaration based on parameter type. 
Step 4-1：If parameter type is primitive, instrument source code segment to read a test input data which 
is assigned to the variable. 
Step 4-2：If parameter type is object or structure, get all primitive type attributes of object or structure, 
and instrument source code segment allowing all attributes to read test data. 
Step 4-3：If parameter type is array, calculate numbers of test input data need to be read and create 
source code segment which can read all test data using loop instruction. 
Step 5：Determine method return type. 
Step 5-1：If return types are primitive, structure, and object, instrument source code segment which can 
read all test data, call target function, and assertion function. Assertion function can be used to 
judge if method return value fulfills the expected value. 
Step 5-2：If return type is void, generate source code segment which can call target function. 
 
 
 
 
 
1. Algorithm : test case generation function 
2.  
3. Function-List:FL 
4. Object-List:OL 
5. Sturct-List: SL 
6. Struct-List:SL 
7. Parameter-List:PL 
8.  
9. Procedure TestCaseGenerationFunction() 
10. begin 
11.   read test input data file 
12.  
13.   while has test data set 
14.     get function name from FL 
 12 
81.          generate code for for-loop to get expected result from FL 
82.          generate code for function-call to get return value  
83.          generate code for CppUnit Assertion 
84.        end_if 
85.     Break; 
86.  
87. // Get return value for struct type 
88.      case struct_type :     
89.       if p = single element  
90.        then 
91.          generate code for struct variable declaration 
92.          get struct attribute set SAS from SL 
93.          generate code for getting expected result for SAS's element from FL 
94.          generate code for function-call to get return value  
95.          generate code for CppUnit Assertion  
96.        else // p= array type  
97.          generate code for struct array variable declaration 
98.          get struct array dimension from p 
99.          get struct attribute set SAS from SL 
100.         generate code for for-loop to get expected result for SAS's element from FL 
101.         generate code for function-call to get return value 
102.         generate code for CppUnit Assertion 
103.       end_if 
104.      break; 
105. 
106. // Get return value for object type 
107.     case object_type :     
108.      if p = single element  
109.       then 
110.         generate code for object variable declaration 
111.         get object attribute set OAS from OL 
112.         generate code for getting expected result for OAS's element from FL 
113.         generate code for function-call to get return value 
114.         generate code for CppUnit Assertion  
115.       else // p= array type  
116.         generate code for object array variable declaration 
117.         get object array dimension from p 
118.         get object attribute set OAS from OL 
119.         generate code for for-loop to get expected result for OAS's element from FL 
120.         generate code for function-call to get return value 
121.         generate code for CppUnit Assertion 
122.       end_if 
123.      break; 
124. 
125.     case void_type : 
126.      if p = single element 
127.        then generate code for function-call 
128.      break; 
129.     end_switch 
130.  end_while 
131. end 
 
Figure 2 Algorithm for automatically generating test case 
 
 
Following is an example for the actual executing flow from generating test data to performing testing. 
According to the target program, the system can generate testing program of test case and test driver using 
CppUnit library. 
 
Figure 3(a) shows the target function binsearch() with three parameter types, including structure array 
type, integer type, and integer type, respectively.  Return value type is integer. Figure 3(b) shows the 
generated test case program. The program can read test data corresponding to parameter types. Through 
 14 
POPM module of ATEMES simultaneously analyzes coverage testing log data (*.gcda and *.gcov files) and 
calculates line coverage and branch coverage.  
The increase of total target program coverage is done by summing up each round’s testing coverage 
result. Visualized testing coverage is presented dynamically on host-side during runtime. Example of 
calculation is shown in Figure 5.  Figure 6 illustrates the workflow of multi-round coverage testing. 
Algorithm of multi-round coverage testing is introduced as follows: 
 
Step 1： Read target program. 
Step 2： Parse target program and extract function information including parameter type. 
Step 3：Generate test data, test case, and test driver automatically.  
Step 4：Include an extra gcov parameter while cross-compiling target program and test driver into target 
execution files.  
Step 5：Upload executable target program, executable test case, and *.gcon generated by gcov to target-
side for execution. 
Step 6：When testing task is completed, test log generated by gcov is transferred back to host-side. 
Step 7：Analyze test log and deciding if the results fulfill coverage threshold value. 
Step 7-1：If the results fulfill coverage threshold value, the system stops testing and demonstrate 
visualized test log of each round to test engineer.  
Step 7-2： If the result does not reach coverage threshold value, the system generates test case 
automatically and go back to step 5 to continue next round of testing. The system simultaneously 
demonstrates visualized test log of each round to test engineer. 
 
3.2.2. Algorithm for multi-round testing  
Intel TBB pipeline allows user to decide the extent of parallelism.  Parameters such as pipeline token 
numbers and pipeline stage numbers can be modified as required. Experiences are essential in deciding the 
better pipeline token numbers and pipeline stage numbers.  Parallelism degree can be limited if the token 
numbers selected are too small.  On the contrary, resources can be consumed unnecessarily.  For example, 
more buffers may be needed if token numbers selected are too big.  Traditionally, numerous manual 
modifications are required from programmer before better pipeline token number parameter can be found.  
To address the issue, this study enhances performance testing by expanding functionalities of automatic 
multi-round testing.  To find better pipeline token number for target program, raw data returned from target-
side is calculated automatically and analyzed during runtime. 
Further more, multi-round testing functionality allows measuring of the response time of target 
program(parallel program) with different interrupt interval under embedded platform using different core 
processor numbers.   ATEMES is employed to automatically generate test driver and different test data, 
namely, core processor number, interrupt time interval, and interrupt times. With the multi-round testing 
functionality, the testing tool executes testing automatically and repetitively.   
 Following is the design for calculating multi-round coverage testing.  Figure 7 shows the 
algorithm for multi-round coverage testing.  The algorithm has 2 inputs, which are test logs generated by 
GCOV function, and total Coverage log saved from the summing records.  Flag set is used in the system to 
record the execution of each line/branch.  If the line/branch is executed, its record is set as 1.  If not, its record 
 16 
3.3. Parallel program performance measurement 
This section focuses on parallel program performance measurement approach supporting parallel 
program adopting Intel TBB library (TBB). This technology not only frees programmer from consuming 
extra energy to find parallel pipeline parameter, but also elevates parallel program performance in more 
precision. For pipeline stage numbers, due to task specificity, division of task unit and selection of different 
stage numbers still relies mainly on programmer’s expertise.  After selecting each set of stage numbers, 
automatic multi-round testing can analyze and find the better combination of token numbers and stage 
numbers. 
Firstly, test engineer must select target program containing TBB pipeline function. The system then 
automatically instruments testing code segment into target program to collect performance data, as shown in 
Figure 8. Within the testing code segment, timer provided by TBB library is for collecting pipeline program 
execution time and replacing the execution parameter ( ppline.run(int) ) of the original pipeline program. 
This allows testing code segment to read the automatically generated pipeline token numbers. Finally, 
target-host mechanism begins performing automatic multi-round testing. Distribution chart of execution 
time and utilization of each CPU core is presented on host-side when target program execution is completed. 
The system can analyze the collected testing data automatically. For pipeline parallel program to achieve 
better performance, test engineer is provided by the system with a recommended range of parallel token 
numbers. 
 
instrument
 
Figure 8 Example for instrumented code to TBB parallel program 
 
According to our experience, our analysis method concludes that grouping 5 token numbers as a unit is 
likely to get the best mean value. Figure 9 exemplifies the execution time of the measured token numbers. 
Mean value is calculated before a suggested range of token numbers is found. As shown in Figure 10, token 
numbers being measured are 1~10. Mean value of execution time for token number 1~5 is 17.6 millisecond, 
for 2~6 is 12.4 millisecond, for 3~7 is 11.8 millisecond, for 4~8 is 12.4 millisecond.  
 18 
4. Automatic testing tool for embedded software 
3.4. ATEMES system module description 
The Automatic Testing Environment for Multi-core Embedded Software (ATEMES) is composed of four 
parts: Pre-Processing Module (PRPM), Host-Side Auto-Testing Module (HSATM), Target-Side Auto-
Testing Module (TSATM), and Post-Processing Module (POPM). Cross-testing for embedded software 
can be realized with these 4 modules.  Testing functions supported by ATEMES system include 
coverage testing, unit testing, multi-core performance testing, and race condition testing. This paper focuses 
mainly on coverage testing, unit testing, and multi-core performance testing. 
 
Figure 12 illustrates system module.  
 
Figure 13 presents system architecture layers. Functions and implementation details of respective modules 
are detailed in the following sections. 
 
 20 
 
Figure 14 Generated test case framework example 
 
PRPM is responsible for receiving target program. Source code of target program is parsed and tokenized 
through Code Manager. CodeParser extracts program information such as function name, parameter, and 
keyword to UnitTestCaseGenerator for generating test case automatically. Further, CodeParser locates the 
source code segment to be instrumented for InstrumentCodeEditor to generate the intended 
instrumented code such as the collection of multi-core utilization code segment. Through a user interface, 
test engineer can also input test input data, test result data, and specially requested instrument code segment 
semi-automatically. Figure 15 shows the module structure. Figure 16 displays class diagram. 
 
 
 
Figure 15 PRPM module Figure 16  PRPM class diagram 
Figure 17 illustrates PRPM sequence diagram. Purpose and steps are detailed as follows: 
 
Scenario 1 
Purpose: generating test case automatically 
 
Step 1: Code Manager reads target program code. 
Step 2: CodeParser parses and extracts all function information from target program, including function 
name, parameter type, internal structure flow of program, and other relevant data.  
Step 3: UnitTestCaseGeneratror analyzes information extracted by step2. 
Step 4: UnitTestCaseGeneratror generates test data after completing analyzing target program. 
Step 5: UnitTestCaseGeneratror generates test case based on class method and output to files. 
 
 
 22 
 
 
Figure 18 HSATM module Figure 19 HSATM class diagram 
 
 
 
Figure 20 and Figure 21 illustrate HSATM sequence diagram. Purpose and steps are detailed as follows: 
 
 
Scenario 1： 
Purpose: generating test driver automatically 
 
Step 1: TestDriverGenerator reads test case information generated by PRPM. 
Step 2: TestDriverGenerator generates test driver based on the parsed test case information. 
Step 3: AutomaticTestHost reads instrumented source code generated by PRPM. 
Step 4: AutomaticTestHost compiles instrumented source code and test driver into executable files of target-
side. 
Step 5: AutomaticTestHost passes executable file to TSATM for performing testing. 
Step 6: HSATM receives test log when testing is completed. 
Step 7: Test log is passed to POPM.  
 
 24 
 
 
Figure 22 TSATM module Figure 23 TSATM class diagram 
 
Figure 24 illustrate TSATM sequence diagram. Purpose and steps are detailed as follows: 
 
Scenario 1： 
Purpose: automatic multi-round testing 
 
Step 1: TargetSideTestController receives related testing program (instrumented executable target 
program, test driver) from HSATM. 
Step 2: TargetSideTestController receives related testing data (test input data) from HSATM. 
Step 3: TargetSideTestController triggers test driver to perform testing. 
Step 4: TargetSideTestController triggers test driver to call instrumented executable code to perform 
testing, and repeat testing based on test case. 
Step 5: TargetSideTestController triggers Profile Manager to collect testing data. 
Step 6: Profiler continues collecting testing result. 
Step 7: Profiler writes the collected testing data and test result data into test log file. 
Step 8: TargetSideTestController passes test log to HSATM. 
Step 9: Repeating step4 to step8 until test engineer stops testing. 
 26 
 
Figure 28 illustrate POPM sequence diagram. Purpose and steps are detailed as follows: 
Scenario 1： 
Purpose: presenting graphic test result dynamically 
Step 1: TestLogParser reads test log from TSATM. 
Step 2: TestLogParser gets the catalogue of test result and writes the log into files based on different 
catalogues including testResultInfo, testLogInfo, and TestDriverLog. 
Step 3: PaintChart reads testResultInfo when testing is completed. 
Step 4: PaintChart presents different visualized testing report according to testing demands. 
 
 
 
 
 
Figure 28 Sequence diagram  for presenting 
graphic test result 
Figure 29  Sequence diagram for multi-round testing 
 
 
4.6. Multi-round testing scenario 
This section demonstrates multi-round automatic testing scenario which is illustrated in UML sequence 
diagram as shown in Figure 29. 
 The scenario includes a series of tasks: parsing source code, generating intended data such as test input 
data, instrument code segment, test case, and test driver, executing testing automatically, collecting and 
parsing test log file, and executing automatically the next round testing based on the parsed result till testing 
conditions are met. Actions are detailed as follows: 
 
Step 1: PRPM reads source code. 
Step 2: Basing on demand script file, PRPM parses the read source code, and proceeds getting program 
function name, parameter, program internal structure, and related data. 
Step 3: PRPM analyzes the extracted data gathered from step 2. Test case is either generated automatically 
after analysis, or new test case (test input data) is generated based on the test result provided by POPM 
during testing. 
 28 
However, coverage of some functions could hardly reach to 100%. For fractionCalculator(), line 
coverage and branch coverage in the 4
th
 round reached to 92% and 65%. Coverage of which could hardly 
increase beyond that. Possible interpretation could be that the algorithm adopted is random testing method. 
Higher coverage could be foreseen if other algorithms were adopted. 
 
 
 
Figure 30 Line and branch coverage testing result 
 
 
 
Table 1 Line coverage 
 Round 
Function 
1st 2nd 3rd 4th 5th 6th 7th 8th 9th 10th 20th 
seqsearch 92% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 
binsearch 79% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 
insertion_sort 91% 91% 91% 91% 100% 100% 100% 100% 100% 100% 100% 
merge 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 
merge_pass 92% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 
merge_sort 92% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 
quicksort 26% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 
heapsort 92% 92% 92% 92% 92% 92% 92% 92% 92% 100% 100% 
adjust 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 
rollDiceGame 72% 83% 83% 83% 83% 94% 94% 94% 100% 100% 100% 
getRollDiceSum 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 
fractionCalculator 55% 63% 80% 92% 92% 92% 92% 92% 92% 92% 92% 
Execute Time (ms) 958 907 939 889 980 891 994 926 1006 977 909 
 
 
 
Table 2 Branch coverage 
Round 
Function 
1st 2nd 3rd 4th 5th 6th 7th 8th 9th 10th 20th 
seqsearch 83% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 
binsearch 60% 90% 90% 90% 90% 90% 90% 90% 90% 90% 90% 
insertion_sort 90% 90% 90% 90% 100% 100% 100% 100% 100% 100% 100% 
merge 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 
merge_pass 88% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 
merge_sort 75% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 
quicksort 17% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 
heapsort 83% 83% 83% 83% 83% 83% 83% 83% 83% 100% 100% 
adjust 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 
rollDiceGame 64% 82% 82% 82% 82% 91% 91% 91% 100% 100% 100% 
getRollDiceSum 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 
fractionCalculator 29% 41% 53% 65% 65% 65% 65% 65% 65% 65% 65% 
Execute Time(ms) 958 907 939 889 980 891 994 926 1006 977 909 
 
5.2.Unit testing experiment 
For unit testing, ATEMES can support automatic and semi-automatic testing. This experiment adopted 
semi-automatic unit testing method. Six homework programs of data structures course were used as the 
target programs. After analyzing source code done by ATEMES testing environment, target functions were 
identified and located automatically, and the corresponding input dialog of test data was shown. When test 
engineers input test data and test result, the ATEMES can automatically generate test case programs and test 
 30 
 
 
 
5.3.Multi-core  utilization  monitoring  experiment 
This experiment adopted a multi-thread program with the multiplication of 2 dimensions array, which 
are 256x128 and 128x256. After analyzing source code done by ATEMES, target functions were identified 
and located automatically. The ATEMES can automatically instrument code to monitor target program 
execution time, conduct automatic cross-compiling, upload test package to ARM 11 MPCore platform, and 
perform testing. During program runtime on target-side, the utilization of each core can be monitored. Test 
result logs can be transferred back to host-side for simultaneous analysis during runtime. 
Experiment result  
The result of experiment is shown in Figure 32 and Table 4. Figure 32 (a)~(d) show performances at 
different timing during executing of 2 dimensions array multiplication. Performance of Core 1 stays stable 
all the time. Core 2 has lower performance. Table 4 illustrates statistics information. In terms of average 
CPU utilization, Core 3 and core 4 have higher performance (29% and 31%). Core 1 performs the second 
(18%), and core 2 has the lowest 18% performance. Core 3 and Core 4 have the maximum performance 
value. 
 
 
  
(a)start executing (b)during executing-1 
  
(c)during executing-2 (d) end executing 
Figure 32 Visualization of multi-core CPU 
utilization at different timing 
  
Table 4  Multi-core CPU utilization 
Core No. Average 
(%) 
Max(%
) 
Min(%
) 
core 1 25.05969 28 22 
core 2 18.97810 62 0 
core 3 29.93431 76 0 
core 4 31.05839 75 0 
 
 32 
5.5. Usability assessment experiment 
To test our system usability, in terms of efficiency and acceptability, we asked 30 graduate students 
from computer and information science department as subjects to engage user trial testing and fill in post-
testing questionnaire. 
 
Experiment arrangement: Subjects use ATEMES to perform a series of testing task on unit testing, coverage 
testing, multi-core utilization performance monitor, and TBB pipeline parallelism benchmark 
experiment. 
Target program: Subjects are provided with sample programs to be edited as wished. Sample programs 
include data structure, array multiplication, and image processing programs. 
Duration: Two hours 
 
Pearson's correlation coefficient two-tailed test is adopted in this research. Total questionnaire items 
are forty four, divided by functional and non-functional categories of 6 dimensions. Four functional 
dimensions, including unit test, coverage test, multi-core utilization monitor, and TBB pipeline parallelism 
degree, are used for testing operation precision of the system. Two non-functional dimensions are employed 
for checking user’s acceptability toward easy to use and reduce testing effort (time-saving for writing testing 
program). 
Questionnaire reliability adopts Cronbach’s alpha (α) to test scale’s internal consistency. Table 6 
indicates Cronbach’s alpha (α) of total item is 0.928, which is larger than 0.7. Reliability is thus credited. 
 
 
Table 6 Item reliability analysis 
 Mean Variance Cronbach's Alpha Std. Cronbach's Alpha Number of 
items 
Value 4.533 .012 .928 .927 44 
 
Reduce testing effort
Easy to use
Parallelism degree testing
Multi-core utilization monitor
Coverage Test
Unit  test
4.4
4.438
4.558
4.543
4.552
4.553
4.567
4.5 4.64.35 4.45 4.55  
Figure 34 Mean value of each dimension 
 
Figure 34 displays results of questionnaire analysis. Total mean is above 4.5(highest point is 5, and 
lowest point is 1). The result suggests that subjects are satisfied with the questionnaire items and functions 
supported by the system. 
For unit test dimension, results of questionnaire analysis confirm functions provided fulfill subjects’ 
demands. When unit test is in process, test data, test case, and test driver required by unit testing can be 
generated automatically. In addition, the system can help checking testing result for subjects so as to save 
time and enhance task efficiency. 
 34 
According to results from experiments 1 and 2, efforts demanded from test engineer can be greatly 
reduced by the functionalities of automatically generated test input data, test case programs and test driver 
programs. Likewise, mechanism supported by ATEMES also provided efficient solutions including multi-
round cross-testing between workstation and embedded system. 
According to results from experiments 3 and 4, ATEMES can facilitate test engineer with multiple tasks. 
Test engineer can remotely monitor target program performance on multi-core embedded platform. When 
executing multi-round embedded parallel program, the incorporation of different CPU core numbers with 
various token numbers can help obtaining better parallel parameter. The method not only can alleviate 
resource limitations but also can improve embedded program performance and parallelism. 
According to results from experiment 5, subjects approve ATEMES’s functionalities in saving testing 
time, increasing testing efficiency, and credit the system with high acceptability. 
 
5.7. Response time experiment for parallel program to handle different interrupt intervals with different 
core numbers 
The experiment aims at measuring the response time of target program(parallel program) with different 
interrupt interval under embedded platform using different core processor numbers.  Our tool is employed to 
automatically generate test driver and different test data, namely, core processor number, interrupt time 
interval, and interrupt times.  Automation in the context refers to tester must be capable of identifying the 
function name of interrupt handler under the testing tool. With the multi-round testing functionality, the 
testing tool executes testing automatically and repetitively. Further, the tool can also simulates embedded 
platform with different core processor numbers to handle different interrupt time interval. Basing on each 
time interval, the tool would generate 20 interrupt events sequentially for target program to measure the 
response time of each  interruption. 
Using TBB library, target program executes actions with TBB pipeline. Main program of target 
program would execute video processing based on video streaming data. The frame size of each 
streaming video is 512*384. Actions of image processing are divided into six steps: (1) Read video stream; 
(2) Reduce salt-and-pepper noise ; (3) Image opening (Erosion and Dilation); (4) Remove Noise and 
Rotation; (5) Edge detection（Laplacian）; and (6) Write video stream. Among which, step 2 through step 
6 are being under parallelized processing by TBB pipeline stage. Whenever interrupt event occurs, main 
program would call interrupt handler to execute image thinning. Figure 35 displays the pipeline design 
description of target program. Figure 36 shows the executing flow of tested program. 
Figure 37~Figure 40 present testing results generated automatically by the testing tool, core processor 
numbers are 1~4. As can be seen from the results, when target program uses embedded platform with 1 core 
processor, the system can handle the workload, if interrupt time interval is greater than or equal to 4 seconds. 
Similarly, when target program uses embedded platform with 2 core processors, the workload is also 
manageable, if interrupt time interval is greater than or equal to 2 seconds. When using embedded platform 
with 3 or 4 core processors, video processing execution is smooth and stays intact from interrupt. The 
existing test results only reflect how current target program responded to workload. Test results may be vary 
accordingly to different workloads posed on main program and interrupt handler. In general, repetitive 
testing is required before assessing outcome of what lower budget hardware should embedded software use 
to properly handle external interrupt workload. In other words, the result is obtained with the cost of 
massive time and efforts. Instead, our testing tool provides an easier option to detect under what embedded 
platform should embedded software operate to handle external interrupt workload.  
 36 
6. Conclusion 
This paper recognized the need for proper automatic testing tool and developed an automatic cross-
testing environment (ATEMES) to support multi-core embedded software testing.  
Table 1 is the functionality comparison chart between our system ATEMES and Erik Putrycz, Alexei 
Alexandrov, and William Thies.  The features of the various algorithms implemented by this test tool set are 
listed and compared with those of the relevant testing suites.  The significance of these algorithms over 
those of other systems is evident immediately.   
 
Table 1 functionality comparison chart between our system ATEMES and relevant techniques 
             Tools  
Features 
ATEMES Erik Putrycz Alexei 
Alexandrov 
William Thies 
Pure software solution Yes Yes Yes Yes 
Automatic multi 
round testing 
Yes none none none 
Automatic generate 
test driver 
Yes none none none 
Automatic generate 
test data/cases 
Yes Yes none none 
Supporting 
Complexity data 
structure 
Yes none none none 
Supporting parallel 
 /TBB performance 
testing 
Yes none Yes Yes 
Test result 
visualization 
Yes Yes Yes none 
Run time test result 
presentation 
Yes none none none 
Cross Platform 
Testing 
Yes Yes none none 
Supporting in Multi-
Core Platform 
Yes none Yes Yes 
Supporting interrupt 
performance test 
Yes none none none 
Supporting Coverage 
test 
Yes none none none 
 
The unique features of this technique not seen in other relevant techniques are highlighted in the table.   
The ATEMES can not only automatically generate test data with primitive type, structure type, object 
type and array type but also generate CppUnit-based test case and test driver.  The system can automatically 
initiate testing without having test engineer to edit any code, which can help reducing massive efforts 
demanded from test engineer.  Functionalities supported by ATEMES are multifold.  
 With the automatic multi-round mechanism, unit testing and coverage testing can be implemented to 
save test engineer from massive repetitive tasks. With the cross-testing technology between host-side 
(workstation) and target-side (embedded system), factors resulted from embedded system resource restraints 
can be reduced for testing. With the cross-testing technology, test case, test driver and target program can be 
cross-compiled automatically and uploaded to target-side for automatic implementation. Target-side test log 
data including runtime data, output result, and data of each core utilization during runtime from target-side 
CPU can be passed to host-side for runtime analysis, results of which can also be visualized presented.  
Moreover, ATEMES can execute automatic multi-round performance testing over multi-core 
embedded software adopting Intel TBB library. The system can support locating recommended value for 
better parallel parameter token number, which not only allows embedded software parallelism but also 
facilitates computing capacity of multi-core CPU to operate more efficiently.   
 38 
Hailpern, B., Santhanam, P., 2002. Software Debugging, Testing, and Verification. IBM Systems Journal 41, 
4-12. 
Hung, S.H.,  Huang, S.J.,  Tu, C.H., 2008. New Tracing and Performance Analysis Techniques for Embedded 
Applications, in: Shu-Jheng, H., Chia-Heng, T. (Eds.), pp. 143-152. 
JFreeChart. http://www.jfree.org/jfreechart/. 
Kaner, C.,  Falk, J.L., Nguyen, H.Q., 1999. Testing Computer Software, Second Edition (2nd ed.). John 
Wiley & Sons, Inc., New York, NY, USA. 
Ki, Y., Seo, J., Choi, B., La, K., 2008. Tool support for new test criteria on embedded systems: Justitia, 
Proceedings of the 2nd international conference on Ubiquitous information management and 
communication. ACM, Suwon, Korea.  
King, J. C., 1976. Symbolic execution and program testing. Commun. ACM 19, 385-394. 
Lyu, M.R., Horgan, J.R., London, S., 1994. A coverage analysis tool for the effectiveness of software testing. 
Reliability, IEEE Transactions on 43, 527-535. 
Michael, J.B., Bossuyt, B.J., Snyder, B.B., 2002. Metrics for measuring the effectiveness of software-testing 
tools, Software Reliability Engineering, 2002. ISSRE 2002. Proceedings. 13th International Symposium 
on, pp. 117-128. 
Myers, G.J., 2004. The Art of Software Testing, second ed. John Wiley & Sons Inc, New York. 
Pacheco, C., Lahiri, S.K., Ernst, M.D., Ball, T., 2007. Feedback-directed random test generation. In: 
Proceedings of the 29th International Conference on Software Engineering (ICSE 2007). IEEE Computer 
Society Press, Los Alamitos, CA, pp. 75–84. 
Sen, K., Marinov, D., Agha, G., 2005. CUTE: a concolic unit testing engine for C, Proceedings of the 10th 
European software engineering conference held jointly with 13th ACM SIGSOFT international 
symposium on Foundations of software engineering. ACM, Lisbon, Portugal. 
Tasse, G., 2002. The economic impacts of inadequate infrastructure for software testing. National Institute of 
Standards and Technology. 
TBB. Intel Threading Buildng Blocks. http:// http://threadingbuildingblocks.org/. 
Yin, Y., Liu, B., 2009. A Method of Test Case Automatic Generation for Embedded Software, Information 
Engineering and Computer Science, 2009. ICIECS 2009. International Conference on, pp. 1-5. 
 
 
 40 
Test result presentation Module 等。本研究的成果可以包裝成軟體開發
測試的工具，對於發展嵌入式軟體的產業而言，可提供一個 純軟體方案的
自動化測試環境，節省軟體測試的人力，進而加速軟體的開發。此外，未來
若再加強 測試的精準度的演算法或技術或是再搜集更多的測試案例後，則
其測試能力會更好。技術的特色有 
1. 支援嵌入式軟體自動測試的環境 
2. Host-side 及 Target-side 的 Cross 測試架構 
3. 提供自動/半自動的基本測試功能 
4. 系統可以解析原始程式碼，插入基於測試所需的的程式片段 
5. 自動產生測試案例和測試驅動程式，支援自動產生 primitive, 
structure and object types 測試輸入資料。 
6. 多核心嵌入軟體平行參數的測試，找出較佳的平行度參數 
7. 圖形化方式顯示測試結果 
8. 自動化多回合測試功能 
9. 單元測試 
10. 覆蓋測試 
11. 可以直接測試執行在多核心嵌入式平台的嵌入式軟體.(ARM11 MPCORE 
platform) 
 
 
 42 
（英文） 
Software testing during the development process of 
embedded software is not only complex, but also the 
heart of quality control. Multi-core embedded software 
testing faces even more challenges. Major issues 
include: (1) how demanding efforts and repetitive 
tedious actions can be reduced; (2) how resource 
restraints of embedded system platform such as temporal 
and memory capacity can be tackled; (3) how embedded 
software parallelism degree can be controlled to empower 
multi-core CPU computing capacity; (4) how analysis is 
exercised to ensure sufficient coverage test of embedded 
software; (5) how to do data synchronization to address 
issues such as race conditions in the interrupt driven 
multi-core embedded system; (6) high level reliability 
testing to ensure customer satisfaction. To address 
these issues, this study develops an automatic testing 
environment for multi-core embedded software (ATEMES). 
Based on the automatic mechanism, the system can parse 
source code, instrument source code, generate testing 
programs for test case and test driver, support 
generating primitive, structure and object types of test 
input data, multi-round cross-testing, and visualize 
testing results. To both reduce test engineer’s burden 
and enhance his efficiency when embedded software 
testing is in process, this system developed automatic 
testing functions including unit testing, coverage 
testing, multi-core performance monitoring. Moreover, 
ATEMES can perform automatic multi-round cross-testing 
benchmark testing on multi-core embedded platform for 
parallel programs adopting Intel TBB library to 
recommend optimized parallel parameters such as pipeline 
tokens. Using ATEMES on the ARM11 multi-core platform to 
conduct testing experiments, the results show that our 
constructed testing environment is effective, and can 
reduce burdens of test engineer, and can enhance 
efficiency of testing task. 
 
 1
 
出席國際學術會議心得報告 
計畫編號 NSC  NSC 99-2220-E-142 -001 - 
計畫名稱 多核心嵌入式軟體之模型驅動整合開發環境-VMC－子
計畫七:多核心嵌入式軟體設計之測試支援系統 
( 2 / 2 ) 
出國人員姓名服務機關
及職稱 
姓名：孔崇旭 
職稱：副教授 
服務機關：國立台中教育大學 
會議名稱 BAI2011 International Conference on Business and 
Information (BAI2011) 
會議時間地點  July 4-6, 2011 
The Landmark Bangkok Hotel, Bangkok, Thailand 
發表論文題目 TBB Performance Testing Tool for Multi-Core 
Embedded Software 
 
一、參加會議經過 
International Conference on Business and Information已辦多年了，主
要討論是以資訊管理、電子商務、資訊系統和技術、網頁技術和管理、研究方法、
技術創新應用、健康管理、經濟管理、及實務應用等相關議題，每一議題包含很
多的Track，從理論到實務應用等皆有相關的Track。本人也獲邀擔任其中一場的
section chair。 
 
論文發表的資訊 
發表的時間: Thursday, 16:50-18:00, July 6, 2011 
Section:  J5 
Section Chair: 本人獲邀擔任本section的chair 
Chorng-Shiuh Koong 
Department of Computer and Information Science  
National Taichung University of Education Taichung, 
Taiwan 
發表論文題目: TBB Performance Testing Tool for Multi-Core Embedded 
Software 
 
本研討會也舉辦了一場特別的workshop ”How to publish in top business 
journals”，說明如下: 
: Thu, 14 Apr 2011 04:17:28 -0700 (PDT) 
: "BAI2011 Organizing Committee" <BAI@atisr.org>   
: <csko@mail.ntcu.edu.tw> 
: BAI2011 Acceptance Notification and Invitation Letter (8409) 
  
  
Acceptance Notification and Invitation Letter  
for BAI 2011 International Conference on Business and Information  
at The Landmark Bangkok Hotel, Thailand, 04-06 July 2011  
http://bai-conference.org  
Chorng-Shiuh Koong  
National Taichung University  
csko@mail.ntcu.edu.tw  
Dear Chorng-Shiuh Koong,  
I am pleased to inform you that based upon the recommendations of two blind reviewers your paper has 
been accepted for presentation at  
the BAI2011, to be held in Bangkok, Thailand on 04-06 July 2011.  
Paper #: 8409  
Title: TBB Performance Testing Tool for Multi-Core Embedded Software  
Author(s): Chorng-Shiuh  Koong,Hung-Jui  Lai  
At this time, please make sure that you take care of the following details:  
1.Please upload your camera-ready final submission (in DOC or PDF Format) via the conference website 
before May 15, 2011.  
 Final manuscripts received after the deadline may not be included in the proceedings.  
 Detailed instructions can be found at http://bai-conference.org.  
2.The authors should register and pay the registration fee for the conference before 15 May 2011.  
 The related information about registration and conference fee could be available at online submission 
system http://182.50.142.182/BAI/.  
 At least one author must register and pay the registration fee by the deadline.  
3.The Landmark Bangkok Hotel provides the conference participants a block of rooms at reasonable rates. 
 Please use the reservation form available at the conference website to reserve your room.  
 Room confirmation is subject to hotel availability. Please reserve early.  
The details about the above can be found at http://bai-conference.org.  
Once again thanks for your interest in the conference.  
We look forward to your participation in this very important event for the business and information 
community.  
Yours sincerely,  
BAI2011 Organization Committee  
1 / 1: 4091 - csko@mail.ntcu.edu.tw - 6483.4MB (32.4%) - 09/24/2011 22:...
2011/9/24http://mail.ntcu.edu.tw/mail/openwebmail-read.pl?sessionid=csko*mail.ntcu.edu.tw-se...
Testing engineers also have to struggle with monitoring the target platform. These issues 
make embedded software testing a very complex and difficult task. Therefore, testing 
tasks often require additional assisting hardware devices during the testing process. 
However, most testing tools are only capable of automatically generating test case 
program framework. Test engineers are obliged to manually write testing program/code 
segment and input test input data under the generated program framework, or simply 
generating test case manually (Koushik, Darko, & Gul, 2005). Test case and test input 
data are generated from manual or automatic input. Automation as it is, approaches still 
need to be improved in generating test case (Michael, Bossuyt, & Snyder, 2002). 
As multi-core embedded platform is getting its increasing popularity in recent years, 
program parallelism becomes obligatory in order to optimize multi-core computing 
power. There are some popular options to engage program parallelism. Adopting the 
present parallel library is the most common approach. Programs can be made paralleled 
or multi-thread under high level APIs. Libraries of OpenMP("OpenMP,") or 
TBB("Intel® Threading Building Blocks (TBB),") are currently well-accepted. 
Programmer may also outline and write multi-thread programs as needed. However, 
sophisticated facets such as how many threads are required for implementation, how 
many token numbers need to be set, or how many stage numbers have to be divided to 
achieve a more competitive execution performance are some factors to be well 
considered when programs are being paralleled. Also, to achieve better execution 
performance, different parallel parameter combination should be adopted when the same 
parallel program is being implemented on different multi-core embedded platform. 
However, the keys to how parallel degree should the programs be to achieve better 
performance, and what parallel parameters are most needed must depend on massive 
experiences and repetitive testing. 
 
Parallel program combinations can be varied with different token numbers and stage 
numbers. Manually testing, collecting, and analyzing performance data from 
implementation alone is a massive task. Incorporating the programs into different 
embedded platforms is yet another laborious challenge. 
In terms of testing performance, how embedded software execute on the Target platform 
can hardly be monitored by test engineer, making it even difficult to improve testing 
performance during the testing process (Shih-Hao, 2008). 
Our previous research(Koong, et al., 2010), in the perspective of software solution, 
developed an automatic supporting tool for testing embedded software. This tool can 
automatically generate test cases and test drivers, and supports unit test and coverage test 
which are based on cross testing technology and multiple rounds mechanism. Test results 
can be visually presented to facilitate observation. 
This system is an expansion of our prior embedded software automatic testing tool. 
We have developed embedded software runtime performance monitor 
functionalities to engage automatic multi-round performance testing over embedded 
software incorporating Microsoft TBB library pipeline technology. The strength of our 
system lies in locating better token and stage numbers for the target program during 
pipeline parallelism as reference data for the ultimate embedded software 
parallelism. 
Following that TBB Pipeline automatic performance measurement approach will be 
presented. 
 
A. ATEMES System Discription  
The Automatic Testing Environment for Multi-core Embedded Software (ATEMES) is 
composed of four parts: Pre-Processing Module (PRPM), Host-Side Auto-Testing 
Module (HSATM), Target-Side Auto-Testing Module (TSATM), and Post-Processing 
Module (POPM). Tasks of automatic testing are completed by two modules, which are 
the HSATM module on the host-side, and the TSATM module on the target-side. Testing 
functions provided by ATEMES system include coverage testing, unit testing, 
performance testing and race condition testing. In previous research, our focus was on 
coverage testing and unit testing. System module is shown as Figure 1. System 
architecture layers are shown as Figure 3. Statistics of the testing result will be shown in 
Figure 2. Functions of respective modules will be detailed in the following sections: 
1
H o s t - S i d e
Test Case
Test Driver
Instrument Code
Test 
Driver 
Generator
 
i  
Test Driver  i
Ĥ̆̇-̆i˷˸ 
Ä̇̂̀ȧi˶
T˸̆̇˸̅ 
M̂˷̈˿˸
̇ i  
̇ ̇i
̇  
˿
H S A T M P O P M
Test Log
Analyzer
 
l
Test Result
Information
  l
I i
Test Result
Presentation
Module
  l
i
l
Test Log 
T S A T M
ProfilerilInstrument
ed Code
I  
Ta̅g˸̇-̆i˷˸ T˸̆̇ 
Ĉ́̇̅̂˿˿˸̅
̇ i  ̇ 
̇ ˿˿
Test Driver  iTest Case  Test Log 
Test Log
Testing Engineer
Source 
code
 
Test Case 
Generation
Module
   
i
l
Code
Instrumentation
Module
I i
l
Instrumented
Code
I
External 
Library
Code 
Analyzer
 
l
Test Case 
P R P M
T a r g e t - S i d e
 
 
Figure 1. System modules 
 
Unit test report Coverage report
 
 
Figure 2 Statistics chart for testing result 
 
 
. 
B. TBB Pipeline automatic performance measurement  
The focus of this section will be on performance measurement approach supporting TBB 
parallel program pipeline. TBB pipeline allows user to decide the extent of parallelism. 
Parameters such as pipeline token numbers and pipeline stage numbers can be modified 
as needed. Experiences are essential factor in deciding the right pipeline token numbers 
and pipeline stage numbers. Parallelism will be limited if the token numbers selected are 
too small. On the other hand, resources will be consumed unnecessarily, for example, 
more buffers may be needed, if token numbers selected are too big. Traditionally, 
numerous manual modification has to be done before programmer can find the right 
pipeline token parameter. To solve this problem, we have expanded functionalities of 
automatic multi-round testing by enhancing performance measurement. Data returned 
from target-side will be automatically calculated and analyzed in finding the right 
pipeline token numbers. This technology not only frees programmer from consuming 
extra energy to test parallel pipeline parameter, but also more precisely elevates parallel 
program performance. For pipeline stage numbers, due to task specificity, the division of 
task unit and selection of different stage numbers still have to rely on programmer’s 
expertise. After the selection of each set of stage numbers, right combination of token 
numbers can be searched under automatic multi-round testing of our system. 
 
Testing engineer will firstly have to select source code containing TBB pipeline function. 
Our system will automatically instrument testing code segment to collect performance 
data, as shown in Figure 4. Within the testing code segment, timer provided by TBB is 
used for collecting pipeline program execution time and replacing the execution 
parameter (ppline.run(int)) of the original program pipeline. This will allow testing code 
segment to read the automatically generated pipeline token number test case. Finally, 
Target-Host mechanism will implement automatic multi-round testing. Distribution chart 
of the execution time and the utilization of each CPU core will be presented on Host-Side 
when the program has completed execution. Our system will automatically analyze the 
collected testing data. We will also provide testing engineer with a suggested range of 
parallel threshold value for pipeline parallel program to achieve better performance. 
 
 
 
Figure 4 Instrument code to TBB parallel program 
 
Summing up from our analysis, we have found that token numbers being measured 
reflect on execution time is distributed as Figure 5. Therefore, we have grouped 5 token 
 
Figure 7 Algorithm for TBB pipeline performance analysis 
C. Multi-Round testing scenario for pipeline performance testing 
UML sequence diagram Figure 8 will be used in this section to illustrate system operation 
mechanism. Multi-round testing scenario is selected to describe how the system operates 
in order to suggest the pipeline token number for better executing performance. The 
scenario of multi-round automatic testing is parsing source code, generating test input 
data with token number, instrument code segment, automatically executing testing, 
collecting performance data and parsing test log file, and automatically executing the 
next round testing task until testing termination conditions are met. Actions are as follows: 
 
Step 1: PRPM read source code containing TBB Pipeline function  
Step 2: PRPM parse the source code which has been read and extracts program 
function name and parameter, internal structure of program, and other 
information.  
Step 3: PRPM analyzes the extracted information based on the parsed data by step 2 
for TBB pipeline function. Code segment containing collecting function of 
TBB pipeline performance and multi-round executing function are generated 
automatically after analysis. 
Step 4: PRPM Instrument code segment generated from step3 to collect executing 
time and to read the test case with token number data 
Step 5: PRPM automatic generate test case with token numbers  
Step 6: HSATM reads test case data generated from PRPM 
Step 7: HSATM reads the instrumented source code generated from PRPM.  
Step 8: HSATM cross-compiles the instrumented source code to target-side 
executable files and uploads to TSATM.  
Step 9: TSATM is triggered to execute automatic testing for each test case. It collects 
TBB pipeline performance measurement targeting different CPU core number; (3) 
Engaging TBB pipeline performance measurement targeting different stage numbers and 
token numbers, and finding recommended value of better token numbers. 
 
The testing tasks of ATEMES are fulfilled on the ARM11 multi-core platform. 
Automatic multi-round testing mechanism is incorporated to multi-core performance 
monitor testing experiment and TBB pipeline performance measurement testing 
experiment. Test experiment result is presented in visualized way.  
 
Software and hardware platforms for the experiments are detailed as follows: 
1. Host-side hardware platform: Intel® Core™ 2 Duo CPU P8400. 
2. Host-side OS platform: Linux UBUNTU 9.10. 
3. Target-side hardware platform: The platform baseboard for ARM 11 MPCore with 4 
ARM11MPCore CPUs.  
4. Target-side OS platform: Linux Kernel 2.6.24. 
 
D. Multi-core performance monitor 
The goal of this experiment is incorporating our developed tool to monitor how 
parallel program is being utilized in each CPU, and of the result can be runtime observed 
visually on host-side. 
 
Experiment description 
This experiment aims at executing multi-core performance monitor targeting 
parallel program model for matrix multiplication. Program parallelism is done by 
generating multiple threads. Array size is 256*128 及 128*256. Our system will engage 
automatic target program analysis and instrument segment code. This will help 
monitoring execution time, conducting automatic cross-compile, and uploading the 
testing execution file being cross-compiled to ARM 11MPCore for automatic testing. 
When the program is under execution, user can runtime observe utilization of each CPU 
core. Testing results will be returned to Host-Side for runtime analysis. 
 
Experiment Result 
CPU utilization of the program being executed on the target-side can be runtime 
presented on the host-side. Visualized presentation of different stages is shown as Figure 
9. Data of the CPU utilization of each core being executed is shown as Table 1. Figure 9 
shows that performance of core 1 has been steady while utilization of core 2 is 
comparatively lower. Table 1 indicates that mean CPU utilization of core 3 and core 4 are 
relatively higher than the other two cores. The third utilization is core1, and the last is 
core2. The result has clearly shown that utilization of core 3 and core 4 is the highest 
during program execution, and that there is still room to enhance CPU utilization. 
 
 
Experiment Result 
The result is shown in Figure 10. We can learn from the result that the more CPU core 
number there are, the more efficient it is. Take pipeline token number for instance, when 
token number is smaller then 5, it is less efficient. However, situation has greatly 
reversed when token number is bigger then 5. Therefore, we can conclude that the more 
pipeline token numbers are, the more efficiency it will be. As for TBB pipeline, when 
token numbers reach to a certain point, not only no obvious efficiency is raised but also 
system burden is increased. Following (1)~(4) is the execution illustration under different 
CPU core numbers and token numbers, and our suggested token number (Table 2). 
 
Figure 10 TBB pipeline execution 
performance measurement under 1~4  CPU core 
 
Table 2 Suggested token number under 1~4  
CPU core 
Number 
f core 
1 2 3 4 
Token No. 5~9 6~10 8~12 7~11
 
F. TBB pipeline performance monitor use different stage number with difference 
tokern number 
This experiment is centered on parallel program using TBB library pipeline 
technology. The goal is to find out recommended value of better pipeline token numbers 
by testing CPU utilization of each combination with different stage numbers and different 
token numbers. 
 
Experiment description 
This experiment has incorporated TBB library to conduct parallel image processing 
model program. Four CPU cores are used to execute the task. Different stages of image 
effect processing task is processed by TBB pipeline parallel technology. Image size is 
1280*1024. The experiment is mainly executing 12 kinds of image effect processing 
tasks. Different algorithms are adopted to achieve gradient, laplacian, Prewitt, smooth, 
median, erosion/dilation, thinning, rotation and threshold. Sequent execution conducted 
in pipeline is read image and output image, while gradient, laplacian, Prewitt, thinning, 
Table 4  Execution time for TBB pipeline 1~12 stage 
Token 
No. 
Stage No. 
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 25 30 35 36
1 9578 9016 9575 8947 8902 8867 9595 9123 8877 9603 9259 8914 8890 8914 8897 9023 8895 8940 9042 8945
2 9028 9037 8958 9054 9039 9190 9230 9084 9004 9022 9176 9037 9090 9114 9100 9076 9011 9035 9077 9145
3 8942 4633 3679 2777 2734 2797 2713 2766 2716 2776 2746 2812 2738 2768 3018 2744 2726 2703 2708 2810
4 8988 4751 3579 2826 2702 2721 2754 2710 2863 2700 2768 2722 2736 2751 2729 2706 2687 2689 2725 2693
5 9304 4613 3709 3055 2802 2723 2724 2731 2710 2704 2680 2779 2709 2763 3025 2702 2714 2670 2839 2685
6 9220 4797 3649 3759 3833 3435 3380 3674 3551 3369 3309 3509 3565 3576 3429 3486 3385 3414 3431 3562
7 9235 4693 3586 2778 2981 2861 2726 2751 2710 2803 2938 2744 2722 2732 2710 2754 2726 2755 2749 2742
8 9188 5042 3605 2834 2753 2760 2750 2732 2793 2753 2720 2719 2696 2749 2744 2757 2842 2729 2745 2732
9 9030 4848 3612 2708 2757 2848 2994 2739 2783 2693 2751 2784 2601 2755 2734 2715 2732 2707 2808 2750
10 8995 4579 3859 2823 2951 2740 2743 2758 2738 2701 2738 2701 2699 2784 2757 2842 2732 2904 3007 2772
11 8986 4800 3704 3035 2724 2773 2720 2747 2749 2718 2752 2731 2779 2771 2718 2866 2719 2756 2771 2711
12 8876 4889 3664 2917 2847 2773 2756 2763 2801 2718 2745 2753 2712 2769 2804 2717 2763 2712 2759 2722
Execute time unit: ms 
 
 
(a). Using 1 pipeline 
stage 
(b).Using 2 pipeline 
stage 
(c). Using 3 pipeline 
stage 
(d). Using 4 pipeline 
stage 
(e). Using 5 pipeline 
stage 
(f). Using 6 pipeline 
stage 
ACKNOWLEDGEMENT 
The authors would like to thank the National Science Council of the Republic of China, Taiwan, for 
financially supporting this research under grants NSC97-2218-E-142-001, NSC98-2220-E-142-001 and 
NSC99-2220-E-142-001 
 
REFERENCES 
Alexandrov, A., Armstrong, D., Rajic, H., Voss, M., & Hayes, D. (2010). High-level performance modeling of task-based 
algorithms. Paper presented at the Performance Analysis of Systems & Software (ISPASS), 2010 IEEE International 
Symposium on. 
Bart Broekman, E. N. (2002). Testing Embedded Software. London: Addisson-wesley. 
Brent Hailpern, P. S. (2002). Software Debugging, Testing, and Verification. IBM Systems Journal, 41(1), 4-12. 
GCOV. from http://gcc.gnu.org/ 
Intel® Threading Building Blocks (TBB). from http://www.threadingbuildingblocks.org/ 
Koong, C.-S., Lai, H.-J., Chang, C.-H., Chu, W. C., Hsueh, N.-L., Hsiung, P.-A., et al. (2010). Supporting Tool for Embedded 
Software Testing. Paper presented at the Quality Software (QSIC), 2010 10th International Conference on. 
Koushik, S., Darko, M., & Gul, A. (2005). CUTE: a concolic unit testing engine for C. Paper presented at the Proceedings of the 
10th European software engineering conference held jointly with 13th ACM SIGSOFT international symposium on 
Foundations of software engineering.  
Laakso, M.-J., Myller, N., & Korhonen, A. (2009). Comparing Learning Performance of Students Using Algorithm Visualizations 
Collaboratively on Different Engagement Levels. [Article]. Journal of Educational Technology & Society, 12(2), 267-282. 
Michael, J. B., Bossuyt, B. J., & Snyder, B. B. (2002). Metrics for measuring the effectiveness of software-testing tools. Paper 
presented at the Software Reliability Engineering, 2002. ISSRE 2002. Proceedings. 13th International Symposium on. 
Navarro, A., Asenjo, R., Tabik, S., & Cascaval, C. (2009). Analytical Modeling of Pipeline Parallelism. Paper presented at the 
Proceedings of the 2009 18th International Conference on Parallel Architectures and Compilation Techniques.  
OpenMP. from http://openmp.org/wp/ 
Putrycz, E. (2004). Using trace analysis for improving performance in COTS systems. Paper presented at the Proceedings of the 
2004 conference of the Centre for Advanced Studies on Collaborative research.  
Raman, E., Ottoni, G., Raman, A., Bridges, M. J., & August, D. I. (2008). Parallel-stage decoupled software pipelining. Paper 
presented at the Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization.  
Shih-Hao, H. (2008). New Tracing and Performance Analysis Techniques for Embedded Applications. 
Tassey, G. (2002). The economic impacts of inadequate infrastructure for software testing (Tech. Rep.): National Institute of 
Standards and Technology. 
Thies, W., Chandrasekhar, V., & Amarasinghe, S. (2007). A Practical Approach to Exploiting Coarse-Grained Pipeline 
Parallelism in C Programs. Paper presented at the Proceedings of the 40th Annual IEEE/ACM International Symposium 
on Microarchitecture.  
Wecker, C., Kohnle, C., & Fischer, F. (2007). Computer literacy and inquiry learning: when geeks learn less. [Article]. Journal of 
Computer Assisted Learning, 23(2), 133-144. 
Yongfeng, Y., & Bin, L. (2009). A Method of Test Case Automatic Generation for Embedded Software. Paper presented at the 2009 
International Conference on Information Engineering and Computer Science (ICIECS 2009). 
 
 
 
 
 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：孔崇旭 計畫編號：99-2220-E-142-001- 
計畫名稱：多多核心嵌入式軟體之模型驅動整合開發環境-VMC--子計畫七:多核心嵌入式軟體設計之測
試支援系統(2/2) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 1 1 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100 字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100 字為限） 
(已被 Journal of systems and software(sci) 接受 ) 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
本研究主要是以建構「支援多核心嵌入式軟體自動測試的工具」為主，設計及開發
Host-side 及 Target-side 的 Cross 測試架構及環境，並提供自動/半自動的基本測試功
能，本系統能提供基本的單元測試(Unit test)、Coverage 的測試、資料 race condition 
測試、平行度效能測試等，並以圖形化方式顯示測試結果，系統重要模組有 Test Case 
Generation Module, Code Instrumentation Module, Test Driver Generator Module, 
Host-Side/Target-Side Automatic Tester Module, Test Log Analyzer, Test result 
presentation Module 等。本研究的成果可以包裝成軟體開發測試的工具，對於發展嵌入
式軟體的產業而言，可提供一個 純軟體方案的自動化測試環境，節省軟體測試的人力，
進而加速軟體的開發。此外，未來若再加強 測試的精準度的演算法或技術或是再搜集更
多的測試案例後，則其測試能力會更好。技術的特色有 
1.支援嵌入式軟體自動測試的環境 
2.Host-side 及 Target-side 的 Cross 測試架構 
3.提供自動/半自動的基本測試功能 
4.系統可以解析原始程式碼，插入基於測試所需的的程式片段 
5.自動產生測試案例和測試驅動程式，支援自動產生 primitive, structure and object 
types 測試輸入資料。 
6.多核心嵌入軟體平行參數的測試，找出較佳的平行度參數 
