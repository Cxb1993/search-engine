二、緣由與目的 
目前在機器人研究的科技上，正朝向著智慧型機器人的方向發展。未來的智慧型機
器人除了將具有行走與視覺、聽覺、嗅覺、觸覺等多種感覺能力之外，還具有邏輯思維、
學習、判斷及決策的功能。它不但可根據人們的要求和環境的資訊做隨機的應變，還可
對環境有完全及選擇性的採取行動，並且靈活地處理問題，自主地完成所受託的任務，
而這項特性表現在人類和動物身體上，是不需要利用數學模型，直接就可由大腦下達命
令來控制肢體，以完成任務。但相反的對機器人來說，為了使機器人能夠適應複雜多變
的環境，進而完成各式各樣的任務，利用傳統控制方法的數學模型已越來越難以勝任。
為解決這問題，勢必得尋求新的突破，對機器人的控制提出更高、更新的智慧型控制方
法，其中又以路徑規劃之研究為一個最受關注的話題。 
一直以來，路徑規劃遇到的困難點在於環境資訊的多變性，特別是對於動態障礙物
是很難預測其移動方向，必須要智慧型機器人在未知的環境中行進時，根據感測器探測
到周圍有限範圍內的資訊；即使智慧型機器人在移動過程中對周遭環境進行判斷，也只
能獲取該瞬間之障礙物資訊，因此要在動態障礙物的環境下進行路徑規劃，無疑地具有
極高的困難度；至今仍不斷有世界各國專家學者對其進行了廣泛的研究討論，並提出了
許多不同的機器人路徑規劃方法[1][2]。 
本計劃主要將針對生物資訊處理系統中的人工免疫演算法則為基礎，加以設計並應
用於智慧型機器人之路徑規劃，同時也將和 Yao[3]提出之人工神經網路文獻、Tu[4]提出
之基因演算法文獻以及 Ishiguro[5]提出之人工免疫文獻作為本計劃研究比較的研究範
疇。經由所提出的強化學習之人工免疫演算法則，控制智慧型機器人於環境中，實現智
慧型機器人閃避障礙物的能力及完成較佳之路徑規劃目的。在計劃利用電腦模擬進行驗
證，其結果也證實了此方法的效果及其可行性。 
 
三、研究方法 
本計劃提出強化學習之人工免疫演算法則設計，將其可分為兩部分：其一為智慧型
機器人之感測器架構改良，其主要是重新規劃設計感測器偵測區域及方式，使智慧型機
器人能具有全方位感測與行進的能力；其二則是在人工免疫演算法則中加入扁平 S 函數
(squashing function)，使抗體濃度穩定不致於發散，並結合了期望方向[6]、Q 學習
(Q-learning)[7]及速度障礙(velocity obstacle)[8]之設計。進而強化對智慧型機器人之避障學
習與路徑規劃之能力，並將其設計應用於智慧型機器人上，其系統架構如圖 1 所示。 
 2
步驟 1：單一以智慧型機器人對目標物而言之趨近為原則，由(1)式規劃出一個目標區域
，如圖 3(a)所示。 goalS
goalgoalS θ=                                (1) 
步驟 2：為有效避開障礙物，在判斷單一個或多個相鄰的障礙物時，由(2)式規劃出 之
第一危險區域，如圖 3(b)所示；並同時考量到智慧型機器人的體積大小，故由(3)式規劃
出 之第二危險區域，其中 為偵測區域的規劃數量，如圖 3(c)所示。最後，由(4)式運
算構成 之智慧型機器人禁止行為區域，如圖 3(d)所示。 
1S
2S n
obstS
   { 8211 obstobstobstS }θθθ ∪∪∪= L                         (2) 
( ) ( )
( ) ( )⎪⎪⎩
⎪⎪⎨
⎧
⎥⎦
⎤⎢⎣
⎡ +=
⎥⎦
⎤⎢⎣
⎡ −=
=
n
r
n
r
S
SSL
SSR
πθθ
πθθ
2max,max
2min,min
11
11
2                     (3) 
21 SSSobst ∪=                                (4) 
 
       
     (a) 形成目標區域          (b) 形成第一危險區域        (c) 形成第二危險區域  goalS 1S 2S
       
(d) 形成禁止行為區域     (e) 形成行為決策區域obstS ∅=mdS    (f) 形成行為決策區域 ∅≠mdS  
圖 3 智慧型機器人期望方向之設計示意圖 
 4
其中，機器人選擇一個動作(action)於環境中，環境(environment)接受該動作後，其狀態
(state)開始發生變化，同時產生一個回饋值(reward)給予機器人，接著其根據回饋值和環
境中之狀態，再選擇下一個動作，此時其選擇的原則是受到回饋機率大小。 
另外，在速度障礙之設計主要目的是為克服較複雜的動態環境中，當智慧型機器人
遠區段感測區域有障礙物反應時，可透過速度障礙設計清楚判斷此障礙物是否會碰撞威
脅到智慧型機器人之可能性，以利智慧型機器人做出必要之行為反應。 
 
四、模擬結果 
模擬一：智慧型機器人處於靜態環境中 
模擬智慧型機器人需避開五個靜態障礙物趨近於目標物的模擬環境，如圖 5 (a)所示
為初始環境狀態模擬圖。在第 7 次學習時本研究所提出的強化學習之人工免疫網路已達
穩定狀態，因此以第 7 次學習為例進行比較分析，分別以如圖 5(b)~(e)所示之。在表 1 所
示為第 3、5、7、10 次的學習結果比較表，由表中分析得知由 Yao 所提出的研究方法在
此模擬多障礙物的環境中，學習速度已明顯變慢；在 CPU 時間上由 Tu 所提出的研究方
法則仍然有明顯CPU時間較高之情形；而運用人工免疫的研究方法則有不錯的避障結果。 
 
0 10 20 30 40 50 60 70 80 90 100 110 120
0
10
20
30
40
50
60
70
80
90
100
GoalStart
x coordinate
y 
co
or
di
na
te
 
(a) 初始環境狀態模擬圖 
 
(b) Yao 提出之研究方法                     (c) Tu 提出之研究方法 
 圖 5 模擬一：智慧型機器人模擬結果路徑圖 
0 10 20 30 40 50 60 70 80 90 100 110 120
0
10
20
30
40
50
60
70
80
90
100
x coordinate
y 
co
or
di
na
te
Goal
n = 7 d = 112.4558 t = 17.6 3
Start
59
0 10 20 30 40 50 60 70 80 90 100 110 120
0
10
20
30
40
50
60
70
80
90
100
x coordinate
y 
co
or
di
na
te
Goal
n = 7 d = 135 t = 10.9539
Start
6 
 6
 
 (b) Yao 提出之研究方法                     (c) Tu 提出之研究方法 
 
(d) Ishiguro 提出之研究方法                 (e) 本研究所提出之研究方法 
圖 6(續) 模擬二：智慧型機器人模擬結果路徑圖 
 
表 2  模擬二：智慧型機器人學習結果比較表 
移動式機器人學習次數 3 6 10 15
   Yao文獻之人工神經網路 碰撞 150 147 (10.61秒) 碰撞
   Tu文獻之基因演算法 335 234 140 (49.75秒) 156
   Ishiguro文獻之人工免疫網路 177 147 132 (5.30秒) 126
 強化學習之人工免疫網路 150 132 126(6.27秒) 126  
 
五、結論 
本計劃提出的強化學習之人工免疫演算法則應用於智慧型機器人上，其目的是設計
一適應環境多變化之行為融合方法，並應用於智慧型機器人之路徑規劃上，模擬驗證之
結果得知，證明了此一方法能有效地完成智慧型機器人較佳之路徑規劃。本計劃主要以
人工免疫為基礎加以改良其設計原理，結合期望方向、Q 學習及速度障礙之設計。在期
望方向之設計中，即用於環境有迫切之需要時，直接由期望方向決定智慧型機器人之行
為反應，並立即做出避障之動作；在 Q 學習設計中，利用人工免疫網路與 Q 學習法則結
合應用於智慧型機器人之路徑規劃，其 Q 學習具有對其行為反應做評價之能力，進而對
0 10 20 30 40 50 60 70 80 90 100 110 120
0
10
20
30
40
50
60
70
80
90
100
x coordinate
y 
co
or
di
na
te
Goal
n = 10 d = 126 t = 6.2715
Start
0 10 20 30 40 50 60 70 80 90 100 110 120
0
10
20
30
40
50
60
70
80
90
100
x coordinate
y 
co
or
di
na
te
Goal
n = 10 d = 132 t = 5.3015
Start
0 10 20 30 40 50 60 70 80 90 100 110 120
0
10
20
30
40
50
60
70
80
90
100
x coordinate
y 
co
or
di
na
te
Goal
n = 10 d = 140.1838 t = 49.7 1
Start
45
0 10 20 30 40 50 60 70 80 90 100 110 120
0
10
20
30
40
50
60
70
80
90
100
x coordinate
y 
co
or
di
na
te
Goal
n = 10 d = 147 t = 10.61 9
Start
1 5 
  
 8
