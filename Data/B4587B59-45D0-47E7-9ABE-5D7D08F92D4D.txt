2 
Increasing Class-Attribute to Improve Classification Performance for Small 
Data Sets 
1. Introduction 
A “small” data set is very much a relative and subjective concept that needs to be defined. 
Shawe-Taylor et al. [47] proposed a measure called Probably Approximately Correct (PAC) to 
determine the minimum sample size required for the necessary accuracy. Muto and Hamamoto [48] 
stated a rule of thumb for the size of sample data based on the ratio of the training sample size to the 
number of attributes. Many researchers, such as Harville [36], Laird and Ware [37], and Jennrich 
and Schluchter [38], proposed mixed linear models to analyze small data sets. Moreover, Schwarz 
[39] derived Schwarz Information Criterion (SIC) using a Bayesian perspective for model selection, 
where the Bayes solution is used to choose the model with the largest posterior probability of being 
correct. From the computational learning viewpoint, small sample sizes are very important in 
machine learning problems, because too few samples will contain incomplete information. With a 
classifier for instance, it is hard to make accurate forecasts because small data sets not only make 
the modeling procedure prone to overfitting, but also cause problems in predicting specific 
correlations between the inputs and outputs.  
Another approach, virtual sample generation, was proposed to enhance the classification 
performance for small data set analysis. The original idea of virtual data generation was proposed 
by Niyogi et al. [43], who used prior knowledge obtained from a given small training set to create 
virtual samples to improve the recognition performance. Li et al. [44] used the mega-trend diffusion 
(MTD) technique to solve the scheduling problem in early flexible manufacturing systems. Their 
research used virtual sample generation to increase the amount of training data to improve the 
classification accuracy of BPNN. They found that the classification accuracy could be increased 
from 69.3% to 78.23% when the data set was as small as five items.  
Kernel methods, such as a support vector machine, are other commonly used classifiers to deal 
with the small data set problem. The main purpose of kernel methods is extending data space into a 
higher dimensional feature space by using the chosen kernel. However, kernel methods are 
processed by operations over the kernel function (such as Gaussian and polynomial kernels) for the 
data, ignoring the structure of the input data and the dimensionality problem, and thus cannot 
always guarantee that the transformed space is useful for classification. Hence, the motivation of 
this study is to propose a new attribute transformation approach to extend a small data set into a 
purpose oriented and higher dimensional feature space to enhance the classification accuracy. 
Different from the virtual sample generation method, this report develops a process called 
dimension extension to generate new attributes into the data set. There are two main methods 
included in the attribute transformation approach: one is building up the class-possibility as new 
attributes; the other is constructing other new attributes (here named synthetic attributes) using the 
original ones which have high correlation. 
this study creates class-possibility attributes and constructs the synthetic attributes for small 
data sets. The fuzzy membership computational approach [44] is employed to compute the new 
class-possibility attribute values for each data in each class, and we construct the synthetic attributes 
4 
3. Methodology 
This section presents a detailed procedure to explain the proposed method. Figure 1 is the flow 
chart of the proposed method that starts from collecting the data, building MTD functions, and 
computing the overlap area of MTD function, and then moves on to class possibility attribute 
transformation, attribute construction, attribute merging and finally SVM model building.  
 
Figure 1 Method structure 
3.1 Building a mega-trend diffusion function for each class 
The mega-trend diffusion (MTD) function was proposed by Li et al. [44] to deal with the small 
data set problem for scheduling strategies in early flexible manufacturing systems (FMS), and it is a 
triangular fuzzy membership function. For dealing with the small data set problem, this report 
employs the MTD fuzzy membership function to substitue the statistic normal distribution to 
present the degree of probability of a sample belonging to a class. Figure 2 shows the concept of the 
fuzzy theorem applied to the MTD function. The triangle is the membership function, and the 
height of samples m, and n are the possibility values of the membership function, denoted as ( )M m  
and ( )M n .  
 
Fig. 2 MTD function  
 
3.2 Computing the overlap area of MTD function 
After building the MTD function for each class in every attribute, finding the overlap area of 
MTD functions is an important step for data information extension. Figure 3 shows an example of 
MTD function overlapping.  
6 
denoted as 1( )M x . The triangle with a dotted line represents the transformation function for class 2, 
denoted as 2 ( )M x . Thus, for the two-class one-attribute classification problem, the transformed data 
is 1 2( ) ( , ( ), ( ))x x M x M x  . 
 
Figure 4 The fuzzy based transformation function for a two-class problem 
3.4 Attribute construction 
3.4.1Compute the correlation matrix 
In statistical analysis, the correlation coefficient plays an important role in measuring the 
strength of the linear relationship between two variables. In the field of computation, the correlation 
coefficient is one of the most well-known criteria for measuring similarity between two random 
variables [6]. Piramuthu [33] showed that similarity-based construction features can improve the 
classification accuracy better than decision tree based ones. The Pearson correlation coefficient is 
defined as: 
cov( , )
( , )
var( ) var( )
i j
i j
i j
  y yy y
y y
 
3.4.2Attribute combination with highly correlated attributes 
After computing the correlation of each pair of attributes, this section will combine those with 
a high correlation value by using three constructive operators.  
Gomez and Morales [32] proposed seven constructive operators: A＋B, A*B, A B , B－A, 
A/B, B/A, and A2. This study only extracts three of these, A*B, A/B, and B/A, as the constructive 
operators. The rest operators are instead by PCA because the main purpose of PCA is to extract the 
features by maximum the variance of the linear combination for all attributes. Finally, PCA will be 
used to extract the features by using attributes A, B, A*B, A/B, and B/A.  
4 Experiments 
4.1. The results for the experiment with a single data set 
(a) The FMS data set 
Table 9 shows the average accuracy results with the proposed method, and the Gaussian and 
polynomial kernels for KPCA using SVM with two kernels. PCA is not used in the comparison 
because there are only three attributes in the FMS data set. Table 9 is summarized as follows: 
In the SVM with a polynomial kernel, the proposed method has statistically significant superiority 
to the KPCA with a polynomial kernel with a p-value of less than 0.05 when data size is 10, 20, 30, 
50, and 100.In the SVM with a polynomial kernel, when data sizes are 10, 20, 30, 50, and 100, the 
proposed method has statistically significant superiority to the KPCA with a Gaussian kernel and 
with a p-value less than 0.05. 
 
8 
a p-value less than 0.05. In the SVM with a Gaussian kernel, when the data size is 10, 20, and 30, 
the proposed method is statistically significantly superior to the other methods with a p-value less 
than 0.05. In the SVM with a Gaussian kernel, PCA and the proposed method are statistically 
significantly superior to KPCA with polynomial and Gaussian kernels with a p-value less than 0.05. 
 
5. Conclusions 
Learning from small data sets is fundamentally difficult, and many data preprocessing methods 
have been proposed to improve the analysis performance, such as the PCA and KPCA. While 
KPCA is a widely used non-linear feature extraction method, there are still weaknesses in using 
general purpose kernels to extend data into higher dimensional spaces, because one can not be sure 
that the transformed space is suitable for a specific research purpose, such as classification. Three 
data sets, the FMS, Pima Indians, and Wisconsin breast cancer sets, are examined in this study. 
When the data size is small, there is a high level of uncertainty, and the insufficient information 
often results in less robust analysis. This report thus proposed a data attribute transformation 
procedure to extend the data information of small data sets to improve classification performance. 
In the proposed method, we compute the classification-oriented membership degrees to 
construct new attributes, so-called class-possibility attributes, and develop an attribute construction 
procedure to construct new attributes, so-called synthetic attributes, to increase the amount of 
information for small data set analysis. The results show that this method has better classification 
performance than the PCA and KPCA methods when the data size is less than 30. These results 
suggest that increasing the amount of new purpose-related data information is an effective way to 
improve the analysis performance for small data sets.  
References 
1. Rokach L. Genetic algorithm-based feature set partitioning for classification problems, Pattern 
Recognition 2008; 41(5): 1676-1700. 
2. Thawonmas R, Abe S. A novel approach to feature selection based on analysis of class regions. 
IEEE Transaction on Systems, Man, and Cybernetics, Part B: Cybernetics 1997;29:196-207. 
3. Mitra, P., 2002, Unsupervised feature selection using feature similarity. IEEE transactions on 
pattern analysis and machine intelligence. 24(3), 301-312.  
4. Devijver, P. A., and Kittler, J., 1982, Pattern Recognition: A Statistical Approach. Engledwood 
Cliffs: Prentice Hall.  
5. Kudo, M., and Sklansky, J., 2000, Comparison of algorithms that selects features for pattern 
classifiers. Pattern Recognition, 33, 25-41. 
6. Pal, S. K., De, R. K. and Basak, J., 2000, Unsupervised feature evaluation: a neuro-fuzzy 
approach. IEEE Transaction on Neural Network, 11, 366-376.  
7.   Hong G, Asoke KN. Breast cancer diagnosis using genetic programming generated feature. 
Pattern Recognition 2006; 39:980-987. 
8.   Bach FR., Jordan MI. Kernel independent component analysis. Journal of Machine Learning 
Research 2003;3:1-48. 
9.  Rosipal R, Kramer N. Overview and recent advances in partial least squares. Lecture Notes in 
10 
samples. International Journal of Approximate Reasoning, 35, 137-161. 
28. Li, D. C and Liu, C. W., A neural network weight determination model designed uniquely for 
small data set learning. Expert Systems with Applications. 36, 9853-9858. 
29. Li, D. C., and Yeh, C. W., 2008, A non-parametric learning algorithm for small manufacturing 
data sets. Expert Systems with Applications, 34, 391-398. 
30. Niyoqi, P., Girosi, F., Tomaso, P., 1998. Incorporating prior information in machine learning by 
creating virtual examples. Proceeding of the IEEE, 275-298. 
31. Li, D. C., Wu, C. S., Tsai, T. I, and Lina, Y. S., 2007, Using mega-trend-diffusion and artifical 
samples in small data set learning for early flexible manufacturing system scheduling 
knowledge. Computers and Operations Research, 34, 966-982. 
32. Bazaraa, M. S., Jarvis, J. J., and Sherali, H. D., 1990, Linear Programming and Network Flows, 
2nd, New York, Wiley.  
33. Shawkat, A., Kate, A., Smith, M., 2006, A meta-learning approach to Automatic Kernel 
Selection for Support Vector Machines. Neurocomputing, 70, 173-186. 
34. Muto, Y. and Hamamoto, Y., Improvement of the Parzen classifier in small training sample size 
situations. Intelligent Data Analysis. 2001, 5(6), 477-490. 
35. Vapnik, V. The Nature of Statistical Learning Theory, Springer, 1995. 
36. Vapnik, V. Universal Learning Technology: Support Vector Machines. NEC Journal of advanced 
Technology, 2(2), 137-144. 
 
6. 計畫成果自評 
本報告建立了一個有別於虛擬樣本資料方法來針對小樣本分類問題以增加類別屬性來擴展資
料訊息以改善分類正確率.  
在學術成就上,這是一項全新的研究方向並獲得 IEEE TKDE 期刊的認同. 
在社會影響與應用價值上,小樣本研究除了可以為企業節省收集分析資料的時間與成本之外,
更能有效且快速的反應市場的快速變動.而能為企業帶來高度的競爭力與商業價值. 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：利德江 計畫編號：99-2221-E-006-095- 
計畫名稱：增加資料之類別屬性以改善小樣本資料分析效果 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 2 0 20%  
博士後研究員 1 0 40%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
