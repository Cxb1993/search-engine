 
 I
目 錄 
中英文摘要、關鍵詞 
1. 前言 ……………………………………………………………………………1 
2. 研究動機與目的 ………………………………………………………………1 
3. 文獻回顧 ………………………………………………………………………2 
4. 研究方法 ………………………………………………………………………3 
5. 結果與討論 ……………………………………………………………………7 
6. 參考文獻 ………………………………………………………………………15 
7. 與計畫相關之論文發表 ………………………………………………………16
 1
1. 前言 
行車安全一直是交通上重要的議題，近年來為因應駕駛人不當行為或突發狀況，主動式
安全系統不斷被提出，如防鎖死剎車系統(Antilock Brake System, ABS)、電子制動力分配
(Electronic Brake-Force Distribution, EBD)等。而在智慧型運輸系統的發展下，先進安全車輛的
概念也受到研究者的重視，且許多車輛製造商亦在從事車輛安全系統之研究開發，如瑞典
Volvo 公司發表的主動式安全科技碰撞警示暨主動完全煞車系統(Collision Warning with Full 
Auto Brake, CWFAB)、駕駛警示控制系統(Driver Alert Control, DAC)。由於駕駛者駕駛車輛時
很容易因為一時注意力不集中或接聽電話使用導航裝置等外務而未能專注於駕駛，使車輛偏
離行駛車道造成事故的發生。基於防範車輛偏移希望能夠提供一個車輛行駛偏移系統(Lane 
Departure Warning System, LDWS)提供車輛是否偏移的警示資訊，以避免駕駛一時的疏忽而造
成交通事故。 
 
2. 研究動機與目的 
在電腦視覺或是機器視覺的研究領域中，移動物體偵測與追蹤是非常重要的研究之一，
現今在這方面的研究已經廣泛的運用在許多層面，而在智慧型運輸系統實現上，目的在解決
行駛於道路時可能碰到的各種問題。本系統主要是針對車輛非預期偏移預警的研究，其中方
法先經數位攝影機擷取連續影像，透過左右標線的偵測找出車輛的移動方向，並對車輛的行
駛方向做出預測，作為車輛偏移警示的參考。近年來，由於汽車工業的迅速發展，以及對大
眾運輸工具需求與依賴不斷提高，伴隨而來的問題便是交通肇事頻率的升高及乘員傷亡的增
加。智慧型運輸與先進安全車輛的研究，近年來受到高度的重視及發展。根據美國國家功率
交通安全管理局(National Highway Traffic Safety Administration, NHTSA) 所編撰之交通安全
實錄所統計數據顯示，因駕駛人疏忽而導致車輛非預期偏離車道的意外事故，約佔所有肇事
原因的 41% 0。Mercedes-Benz 公司的研究調查中亦發現，在事故發生一秒前駕駛者能獲得有
效的警示，能避免 90%的碰撞意外 [2]。因此若能藉由車輛搭載之主動式安全系統，於駕駛
者在危險發生前，提前發出警訊給駕駛者，將可有效避免意外發生。為有效降低因駕駛不專
心所造成的事故，目前許多廠商在多款車輛中裝有車道偏移警示系統(Lane Departure Warning 
System, LDWS)，主要功能為車輛即將偏離車道時，能事先提醒駕駛者，使其能即時做出應變
措施，防止車輛偏移車道所可能產生的事故。2007 年 2 月 ISO 組織公布第一版 LDWS 標準
規範－ISO 17361 [3]，其中規範系統警示時車輛與車道線之相對位置－偏移率(Rate of 
Departure)試驗，以避免系統過晚或過早警示。2005 年美國聯邦汽車運輸安全管理局(Federal 
Motor Carrier Safety Administration, FMCSA) 亦對 LDWS 操作需求制訂參考標準 [4]，其中部
分內容參考 ISO 17361。 
LDWS 可依車道線感知器形式的不同加以分類，目前市面銷售之產品絕大部分屬於影像
偵測式，利用架式於車內的數位攝影機擷取影像，再以車載電腦根據分析與控制警示機制進
行圖形識別。因此影像道路邊笑偵測策略與影像處裡的演算法為此種系統精確可到與否的關
鍵。系統架構依據 ISO 17361 所建議之 LDWS 架構如圖(1) 所示。由車道位置偵測、車道偏
移警示與系統狀態監控和警示系統構成。車道位置偵測將持續不中斷地偵測車輛與車道線的
相對位置，並將偵測結果即時傳入車道偏移系統中進行辦別；車道偏移系統將依照事先定義
 3
之後檢視此區域，若灰階值大於最大值之上，假設其為道路標線，灰階值在最小值之下，假
設為物體。接著利用道路特徵將可能為道路的物體篩選出來。最後使用決策樹分析此物體是
否為道路標線。此系統具有高效率和低運算量之特色，不過交通量過大時效果較差。2001 年，
J. B. McDonald 提出利用霍氏轉換將道路視為一段一段線性的曲線 [13]，發展出新的道路偵
測方式，此方法最大的優點在於高效的雜訊處理與障礙物迴避能力。Y. He 於 2004 年提出將
影像作投影轉換後找出物件邊緣 [14]，並做邊緣統計，藉由比對已定義之車道標線模型和小
區塊的道路標線色彩樣本，達到道路標線的效果，進而增加其正確性。H. Cheng 在 2006 年提
出先依色彩分割找出可能為車道標線之物件，再針對色彩與車道標線相似之物件進行形狀與
大小判斷，並藉由輸入攝影機參數計算其預期移動位置來判斷是否為車道標線 [15]。 
 
4. 研究方法 
本計劃所建置之車道標線偵測系統流程圖如圖(2)所示。系統分為三部分： 
第一部分為影像處理，系統將會對輸入的影像進行色彩過濾、灰階化、二值化和雜訊過
濾等過程將背景消除。 
第二部分為車道標線識別，藉由邊緣偵測演算法與直線偵測演算法等演算法找出左右車
道標線。 
第三部分為車道偏移警示，透過先前車道標線識別流程計算出的車道標線，計算出車輛
是否偏移車道中線，以及是否需要警示使用者。 
 
圖(2) 系統流程圖 
4.1 色彩過濾 
HSV 色彩空間將 RGB 色彩轉換為更直觀的色彩表示方法，藉由觀察道路標線色相與飽
和度的分布，我們可找出一原則事先過濾可能為標線之顏色區域。 
 5
 
圖(4) 車道標線參數定義 
依據完成的參數的定義後，進行影像的標線識別，由於左車道標線的 lθ 介於0 ~ 90−o o間，
而右車道標線的 rθ 介於0 ~ 90o o之間，因此我們可以分別進行不同角度的霍氏轉換；為右車道
標線做霍氏轉換時其 rθ 的範圍是0 < <90rθo o，轉換式為： 
                       ( ) sin ( ) coso oX X Y Yρ θ θ= − × + − ×  (1) 
( , )o oX Y 為影像中坐標軸原點， ( , )X Y 為影像中各點像素。經由此轉換式(1)可得到參數空間座
標 ( , )r rρ θ 。並同樣為左車道標線針對 lθ 範圍在 90 0lθ− ° ≤ ≤ °區間做霍氏轉換。左右兩組θ分
別進行霍氏轉換後可找出多組可能的 ( , )r rρ θ 及 ( , )l lρ θ ，我們取出累加值最大的前五組為我們
所尋找的道路標線候選組別。由於車道標線通常為成對出現的，因此可比對 ( , )r rρ θ 及
( , )l lρ θ ，找出其中θ角度最為接近的兩組分別為左右子畫面中的車道標線。最後再將這兩組
( , )i iρ θ 重新對應回 xy 平面產生道路邊線。 
 
4.3 車道偏移警示系統 
依據所得到之 ( , )r rρ θ 和 ( , )l lρ θ ，繪出車道標線的表示圖，如圖(5)。藉由標示出的兩條車
道邊線圖，可判斷出車輛目前行駛位置與標線的相對狀況 [22]。圖(6)為車輛正常行駛於車道
 
 
W 
H 
 7
(a) (b) 
 
圖(7) 偏移車道偵測圖 (a) 向左偏移； (b) 向右偏移。 
5. 結果與討論  
5.1 計畫結果 
本計劃實驗作業環境的硬體設備包含數位攝影機及影像處理系統。將數位攝影機錄製之
真實道路影像擷取至影像處理系統中做處理分析；本計劃所執行之影像處理演算法，皆以 C
語言撰寫，配合 Microsoft Visual Studio.net 2010 使用 C#語言編寫，完成圖(8)車道偏移警示介
面。 
   
圖(8) 車道偏移警示介面 
經過影像的處理以及標線偵測，我們可得到標線偵測的結果，本節將展示連續影像之車
道識別研究結果。圖(9)展示日間、高速道路平坦且車流量較少時，車載攝影機擷取之影像經
系統處理後之成果，其中(a)為原始輸入影像；(b)為標線識別後影像；表(2)為車道標線識別
結果與其左右標線夾角和 sθ 。圖(10)展示日間、一般市區道路且車流量中等時，車載攝影機
擷取之影像經系統處理後之成果，其中(a)為原始輸入影像；(b)為標線識別後影像；表(3)
為車道標線識別結果與其左右標線夾角和 sθ 。圖(11)展示夜間、高速道路平坦且車流量較少
時，車載攝影機擷取之影像經系統處理後之成果，其中(a)為原始輸入影像；(b)為標線識別
 9
 
Frame 1 
  
Frame 2 
  
Frame 3 
  
Frame 4 
 
(a) 
 
(b) 
圖(10) 日間一般道路連續車道標線偵測結果 (a) 原始輸入影像 (b) 標線偵測結果
 11
 
Frame 1 
  
Frame 2 
  
Frame 3 
  
Frame 4 
 
(a) 
 
(b) 
 圖(12) 夜間一般道路連續車道標線偵測結果 (a) 原始輸入影像 (b) 標線偵測結果 
 13
將標線標示為黃色；發生向右偏移時系統將標線標示為紅色，藉由不同顏色之標線顯示來提
醒駕駛。圖(13) (a)為日間道路行駛時發生向左偏移時，系統警示結果。圖(13) (b)為日間道路
行駛時發生向右偏移時，系統警示結果。其標線偵測結果表列於表(6)。圖(14) (a)為夜間道路
行駛時發生向左偏移時，系統警示結果。圖(14) (b)為夜間道路行駛時發生向右偏移時，系統
警示結果。其標線偵測結果表列於表(7)。 
 
圖(13) 日間偏移警示結果圖 
表(6) 日間偏移標線偵測結果表 
 Left  Right  θs 
 ρ θ  ρ θ   
向左偏移 152 -60  171 34  -26 
向右偏移 214 -35  98 58  23 
 
 
圖(14) 夜間偏移警示結果圖 
表(7) 夜間偏移標線偵測結果表 
 Left  Right  θs 
 ρ θ  ρ θ   
向左偏移 152 -45  201 31  -14 
向右偏移 230 -38  132 49  11 
 
5.1.2 車道彎道警示系統模擬 
    使用虛線，線段長四公尺，間距六公尺，線寬十公分。每十公分等於 1 bit，因此四公尺
等於 40 bit = 5 byte，間隔 10 cm (1 bit)。格式如表(8)所示。 
 
(a)向左偏移 
 
(b)向右偏移 
 
(a)向左偏移 
 
(b)向右偏移 
 15
理的步驟：色彩過濾、對比調整、二值化、雜訊排除等，並使用 Sobel 邊緣偵測演算法搜尋
車道標線邊緣同時減少計算資料量，再經由霍氏轉換獲得最有可能之車道標線位置，將之標
示於顯示畫面中。最後利用左右標線之角度差值來判斷是否產生車道偏移，依照狀況決定是
否提出車道偏移警示以提醒駕駛。 
由實驗結果可知，在良好的環境下，例如，天氣晴朗、路面平坦、車流量少時，不論日
間或夜間本研究可得到不錯的結果。在夜間駕駛時即使環境光源不足時僅仰賴車輛大燈，亦
能可達到良好之偵測結果。藉由 HSV 遮罩過濾除不可能為標線之色彩，可大幅減少所需之運
算時間，提高車道標線識別效果。而本文攝影機架設位置僅要求位於車輛中心可拍攝地面車
道影像位置即可，不需另行輸入攝影機參數做攝影機校正是為本研究之特點。同時，本計劃
也設計一彎道偵測路線標示設計，可以成功偵測出行車路線前方彎道的角度。 
 
6. 參考文獻 
[1] 陳柏全、柯亮宇，「智慧安全車輛與車載無線通訊國際發展趨勢簡介」，車輛工業月刊，第 175 期，pp. 33-49，
Nov. 2008。 
[2] X. An, M. Wu, and H. He, "A novel approach to provide lane departure warning using only one forward-looking 
camera," CTS, pp. 356-362, 2006. 
[3] Technical Committee ISO/TC 204, ISO 17361: Intelligent transport systems - Lane departure warning systems – 
Performance requirements and test procedures, ISO copyright office, Switzerland, 2007. 
[4] A. Houser, J. Pierowicz, and D. Fuglewicz, "Concept of operations and voluntary operational requirements for 
lane departure warning systems (LDWS) on-board commercial motor vehicles," FMCSA-MCRR-05-005, 2008. 
[5] T.M. Jochem, D.A. Pomerleau, and C.E. Thorpe, "MANIAC: A next generation neurally based autonomous road 
follower," in Proc. Int. Conf. Intelligent Autonomous Systems, IAS-3, Feb 1993. 
[6] E. D. Dickmanns and B. D. Mysliwetz, "Recursive 3-d road and relative ego-state recognition," IEEE Trans. 
Pattern Analysis and Machine Intelligence, vol. 14, pp. 199-213, Feb. 1992. 
[7] Q.-T. Luong, J. Weber, D. Koller, and J. Malik, "An integrated stereo-based approach to automatic vehicle 
guidance," in Proc. Fifth ICCV, pp. 12-20, 1995. 
[8] D. Pomerleau, "RALPH: Rapidly adapting lateral position handler," in Proc. IEEE Intelligent Vehicles '95, pp. 
506-511, 1995. 
[9] D. Pomerlau and T. Jochem, "Rapidly adapting machine vision for automated vehicle steering," IEEE Expert, vol. 
11, no. 2, pp. 19-27, Apr 1996. 
[10] M. Bertozzi and A. Broggi, "GOLD: a parallel real-time stereo vision system for generic obstacle and lane 
detection," IEEE Trans. on Image Processing, vol. 7, no. 1, pp. 62-81, Jan. 1998. 
[11] K. Kluge and S. Lakshmanan, "A deformable-template approach to lane detection," in Proc. Intelligent Vehicles 
'95, pp. 54-59, 1995. 
[12] J.P. Gonzalez and U. Ozguner, "Lane detection using histogram-based segmentation and decision trees," in Proc. 
IEEE ITS, pp. 346-351, 2000. 
[13] J. B. McDonald, J. Franz, and R. Shorten, "Application of the Hough transform to lane detection in motorway 
driving scenarios," in Proc. Irish Signals and Systems Conference, pp. 340–345, July 2001. 
[14] Y. He, H. Wang, and B. Zhang, "Color-based road detection in urban traffic scenes," IEEE Trans. ITS, vol. 5, no. 4, 
pp. 309 – 318, 2004. 
Production of Facial Expressions Using Facial 
Feature Positioning and Deformation  
 
Jia-Shing Sheu1*     Ying-Ming Wu2    Yung-Chuan Chuang3        Ying-Tung Hsiao3          Ching-Guo Chen4 
1Department of Computer Science, National Taipei University of Education, Taipei, Taiwan 
2Department of Electrical Engineering, Tamkang University, New Taipei, Taiwan 
3Department of Digital Technology Design, National Taipei University of Education, Taipei, Taiwan 
4Department of Electrical Engineering, Tungnan University, New Taipei, Taiwan 
*E-mail: jiashing@tea.ntue.edu.tw 
 
 
Abstract-Based on image processing technique, this paper used 
face images of users as virtual images to automatically produce 
different facial expression images. The face images in neural 
state can be transformed into the facial expression images.  
First, for the face images in neutral state, skin color 
segmentation and morphological noise processing were 
conducted to separate the possible face zone from the 
background. In terms of feature points (FP) and facial 
animation parameters (FAP) defined in MPEG-4, feature shape 
control points were marked for features of the facial 
expressions: color information of brows, eyes, and mouth. The 
feature control points were marked on the images during 
production of the facial expressions. Delaunay Triangulation, 
image registration and image interpolation were performed to 
move the feature control points to change face images through 
interactive interface. The image texture was changed by 
adjusting positions and shapes of feature texture in order to 
automatically produce new facial expression images. 
Keywords: Facial Expression Production, Feature Extraction, 
Triangular Segmentation, Image Registration  
 
I. INTRODUCTION 
In recent years, games and animations often use dress and 
action sensor of performers for capture and making, and thus 
facial expressions of performers are rich and vivid. However, 
this technique is difficult to popularize. Generally, people 
often express their feelings through computer interface, and 
they are letters and symbols such as, the symbol “:)” or small 
graphic presentations “☺” performs a smiling face. Currently, 
no popular method has been used to construct virtual 
characters or use user images to produce different facial 
feelings. This study explored interaction of basic feelings 
(happiness, anger and grief) to produce different facial 
expressions with human images in neutral state, and thus the 
method that employs neutral expression images to produce 
facial expressions is proposed. The human face features that 
can be used to recognize facial expressions: brows, eyes and 
mouth are extracted and segmented. Distortion and 
deformation are conducted for textures in the control point 
area to produce different facial expressions.  
This paper used MPEG-4 control feature points to position 
features of the 2D face images. To achieve the goal, the skin 
color segmentation and noise filtering should be performed. 
Based on the studies by Phung, Bouzerdoum and Chai [3], 
Bayes classifier and histogram have the best skin color 
recognition rate among various classifiers; using the 
categorizer, CbCr has the best effect in the different color 
spaces before and after removal of image brightness. After 
skin color segmentation, with reference to FAP feature points 
given by MPEG-4, detection and extraction of image features 
can be conduced to position boundary points of brows, eyes 
and mouth as control points. New facial expressions can be 
produced by moving the control points. Image registration 
technique is used during image deformation. The facial 
expression features are detected by using feature extraction 
technique, and thus the steps in the studies by Zitová and 
Flusser [4] were omitted. The detected images were 
transformed into the reference images. A user can move the 
control points on the interactive interface, and transform the 
features of neutral expression images with texture into preset 
expression shapes to produce new facial expressions. 
The paper proceeds as follow; related works are given in 
Section II. In the Section III, the basic theory of experimental 
design is introduced. The experiments and results are 
presented in the Section IV. Section V gives a brief summary 
of the obtained results and the conclusion. 
   
II. BASIC THEORY OF EXPERIMENTAL DESIGN 
Based on [5], this paper used neutral expression images and 
automatic feature positioning method to extract the face 
features points. Further, users can select the feature points via 
interactive interface to change position of the feature points. 
Image processing transformed the target features to yield the 
facial expression effect.  The processing can be divided into 
four steps:  (1) skin color segmentation, (2) noise filtering, (3) 
feature positioning, (4) image deformation. 
1. Skin color segmentation 
Colorful and nonsolid colorful images are used. As 
compared to gray scale image, color image has color hints to 
enhance position of face features, and this can facilitate 
subsequent image processing. The result display is reflected 
more obviously [6][7][8][9]. To remove influence of 
background or other objects on the face position, skin color 
and non-skin color zones were separated for segmenting 
subsequent face zone.  This can reduce quantity of pixels and 
978-1-4673-0157-2/12/$31.00 ©2012 IEEE 1158
segmentation. In Delaunay Triangulation, for DT (P) set 
consisting of point set P, no point from P appears in the 
circumscribed circle of the each triangle. The Minimum 
Angle is maximized to prevent formation of a narrow and 
long triangle. As for the point set, Delaunay Triangulation 
will produce the convex hulls consisting of the set points at 
the boundary and form convex polygon by connecting the 
hulls, and the polygon is divided into 2n-2-k triangles. If the 
segmentation is conducted in limited region or concave 
polygon, the segmentation boundary is limited to form finite 
Delaunay Triangulation. 
2. Image registration: After the transformed region shape 
is segmented into several triangles through Delaunay 
Triangulation, the feature shape from the original neutral 
expression images and textures are transformed into the 
preset shape of facial expressions.  In this paper, the 
transformed features are used as reference images, and the 
original untransformed features are used as sensed images. 
The features of the facial expressions are detected through 
feature extraction technique, and thus additional feature 
detection comparison step in [4] is omitted. The 
transformation model prediction and image interpolation are 
directly conducted. 
The triangular segmentation may yield different results for 
feature images of two shapes, namely convex polygon and 
concave polygon, and thus, it is necessary to judge shape of 
the feature images. After the feature images are judged to 
belong to convex polygon or concave polygon, and the 
corresponding triangle blocks are cut through triangular 
segmentation, different image transformation models are used 
for calculation, as follows:  
1. Convex polygon: the piecewise linear transformation in 
regional images in literature [4] is used. Based on different 
transition places, texture in the region is extended along 
direction, and the end point prior to transformation is used to 
estimate the end point after transformation through image 
transformation model to achieve the shape transformation 
effect. 
2. Concave polygon: the piecewise linear transformation is 
extension of affine image application [6], and conducts affine 
mapping operation for triangle within the limited boundary of 
the concave polygon. Thus, texture in each triangle can be 
extended along the boundary. After operation, each block is 
merged and stitched to form original expected shape. The 
destructive clearance of images due to triangular 
segmentation of feature images and transformation of control 
points needs image interposition to fill the missed pixels. 
Bicubic interpolation in [4] is performed to prevent greater 
distortion. 
 
III. EXPERIMENTS AND RESULTS 
1. Image preprocessing –skin color segmentation and 
noise filtering  
The database provide by Matlab Georgia Tech Face 
Database [41] were used, and the fifteen different persons 
have 15 640 * 480 JPEG color images with different angles, 
lights, and facial expressions. Face images in neutral state 
were selected. 
After processing images, the skin color segmentation was 
conducted for images. Histogram analysis is conducted for 
different color spaces in tested images to obtain optimal skin 
color distribution space. The following Fig. 2 is the examples 
of tested images. 
 
Fig. 2. Tested images  
 
Based on the basic analysis, as shown the results of Fig. 3, 
it can be observed pixels in the pixel values of the RGB space 
have extensive distribution range, and too many color tones 
of different gray scales exist, and are suitable for skin color 
segmentation. Also, Fig. 4 shows YCbCr histogram, and it 
can be seen YCbCr pixels are intensively distributed in the 
Cb and Cr, and the total percentage of covered pixels is high 
to form low-contrast distribution. The search range is limited 
to the minimum range.  Fig. 5 shows HSV histogram. Due to 
impact of H component, the distribution of the gray values is 
extreme, and the search range will evidently cover the whole 
HSV space [0, 1], and unnecessary pixels may be selected 
easily.  
 
Fig. 3.  RGB Histogram of tested image  
 
Fig. 4. YCbCr Histogram of tested image  
 
1160
After the zones within the face were judged, the rules are 
constructed to filter the non-eye block. Next, the 2-D 
correlation coefficients are used to conduct pairwise 
comparison for each block and identify the similarity. The 
black zones which are eyes are judged and cannot be marked, 
as shown in Fig. 16. After that the brow range can be found 
through the eyes position, and the results are stored through 
binary images. The non-skin color region is used to form 
image map, and finally the image map is contrasted with the 
original images, and the position is marked, as shown in Fig. 
17. Finally, the mouth positioning uses color difference 
between the mouth and surrounding skin in YCbCr space, and 
the morphological operations will remove the excessive 
noise, as shown in Fig. 18. Rule analysis is conducted for the 
white connected sets of the binary image, and the zone can be 
judged through rules. Then, with reference to RGB space, the 
image in the zone is displayed and marked to form mouth 
zone, as shown in Fig. 19.   
 
Fig. 16. Selected mark of eyes (left)    
Fig. 17. Brow position mark  (right) 
 
Fig. 18. Binary image after noise processing (left) 
Fig. 19. Mouth mark  (right) 
 
The facial expression features detected in this experiment 
were positioned, and next the FAP controls point of each 
feature was marked. FAP control point is denoted by the 
green cross. To prevent incorrect FAP control point position 
due to binary image segmentation result, this study used lip 
line of the month in HSV space to facilitate positioning.  The 
findings are shown in Fig. 20. The obtained FAP control 
point is send to the facial expressions to produce interactive 
interface.   
 In facial expression experiment, the green cross point is all 
control points. The white point is the selected control point, 
and the cross center is used to select the control point 
position. The position where control points move would 
produce a yellow round point. Next, the control point is 
moved according to the sequence, as shown in Fig. 21.  
 
Fig. 20 Control point of comprehensive FAP mark (left) 
Fig. 21 Interactive interface of facial expressions (right) 
 
 The unchanged control point will be at the original 
position, and the whole image processing is shown in the Fig. 
22. Based on the feature control points, the corresponding 
transformed polygon was constructed. The results are shown 
in Fig. 23.   
 
Fig. 22. Result of interactive interface (left) 
Fig. 23. Result of facial expressions (right) 
 
IV. CONCLUSIONS 
This paper used face images in neutral state to conduct the 
experiment to produce facial expressions, and conducted skin 
color segmentations and noise filtering of images to separate 
the face zone. The facial expressions were automatically 
positioned, and the brows, eyes and mouth were found 
through the color information. The FAP feature points 
subjected to MPEG-4 are used as the control points of each 
feature in the experiment, and minimum feature information 
was used as basis of the different facial expressions. After 
completion of the feature control point positioning, users can 
freely move the feature control points via one interactive 
interface. For the difference between the front and back 
feature point position, the triangular segmentation was 
conducted to segment the images into different blocks. The 
feature image texture can produce different shapes through 
image interpolation and the correction operation to make 
facial expressions. 
1162
 Implementation of Lane Departure Warning System  
Based on Machine Vision 
Jia-Shing Sheu1,a , Hao Chu 1,b , and Chun-Chi Liu 1,c  
1 Department of Computer Science, National Taipei University of Education, Taipei, 106, Taiwan 
a jiashing@tea.ntue.edu.tw, b nimula@hotmail.com, c rich6790@yahoo.com.tw 
Keywords: Image processing, Lane departure warning system, Road detection, Hough transform. 
 
Abstract. Purpose of this paper raise a vehicle lane departure warning system based on the machine 
vision. It does not need to use the parameters of the camera which do the road marking recognition 
system installed in the interior of the camera by algorithms. Regarding the time line of the intelligent 
transport system, it is to solve the various problems possibly arising from driving on the road. This 
system is mainly for the warning of the unexpected departure of vehicle. The method is to use the 
digital camera to capture continuous images and identify the vehicle moving direction by the 
detection of the left and right markings, as well as forecast the driving direction of the vehicle for the 
reference of vehicle departure warning. In this paper, the used algorithms include brightness 
adjustment, binarization, dilation, erosion, and edge detection image processing techniques.  
Introduction 
Driving safety has been an important issue of transportation. In the development of the intelligent 
transport systems, the concept of advanced safety vehicle has attracted much attention of researchers. 
Drivers can easily depart the driving lane to cause the road accidents when they are unable to focus on 
driving due to absentmindedness or answering phone call. For the prevention of vehicle departure, it 
is expected to provide the information of vehicle departure from the lane to avoid the traffic accidents 
caused by negligence. In the research fields of computer vision or machine vision, the detection and 
tracking of moving object is one of the very important study topics. At present, results of related 
studies have been widely applied in many perspectives. Regarding the time line of the intelligent 
transport system, it is to solve the various problems possibly arising from driving on the road. This 
system is mainly for the warning of the unexpected departure of vehicle. The method is to use the 
digital camera to capture continuous images and identify the vehicle moving direction by the 
detection of the left and right markings, as well as forecast the driving direction of the vehicle for the 
reference of vehicle departure warning. In recent years,  due to the rapid development of the 
automobile industry and the ever increasing needs for and dependency on public transportation 
means, traffic accident frequency and passenger casualties increase accordingly. Studies on 
intelligent transport and advanced safety vehicle have been highly concerned and developed in recent 
years. 
According to the traffic safety statistical data compiled by the U. S. NHTSA (National Highway 
Traffic Safety Administration), accidents from unexpected departure from lane due to driver 
oversight account for approximately 41% of all accidents [1]. The research survey by the 
Mercedes-Benz also suggests that 90% collision accidents can be avoided if the driver can be 
effectively warned one second before the accident [2]. If the active safety system can be installed in 
the vehicle to warn the driver of coming danger, accidents can be effectively prevented. To 
effectively reduce accidents caused by driving oversight, many automakers have installed the LDWS 
(Lane Departure Warning System) in vehicles. The main function of the system is to warn and allow 
the driver to take responsive measures in advance when the vehicle is to depart the lane to prevent the 
possible accident of vehicle lane departure. In February 2007, ISO issued the first version of LDWS 
standards-ISO 17361 [3], which regulate the vehicle and the lane line relative location-rate of 
departure test in time of system warning to prevent too early or too late warning by the system. In 
 The binarization processing of the adjusted images is then conduct. Binarization is also known as the 
grayscale division (threshold). The grayscale of the image is divided into two grayscale values. A 
grayscale value is set. Pixels of grayscale greater than the set value are bright ones, and pixels of 
grayscale lower than the set value are dark ones. After image binarization, the size, shape and 
location of the object can be highlighted and the information amount can be reduced to speed up 
image processing. Dilation and erosion are two common morphological image processing [5] 
methods. The binarized image may have noise or graphic discontinuity caused by binarization. 
Erosion and  dilation  can eliminate noise and smooth images. In this paper, we select 8-adjacent point 
for computation. To effectively judge the existence of lane marking in the image, edge detection 
should be conducted before Hough transformation to highlight the object edges. Sobel filter is used 
for edge detection. Sobel filter can conduct the first derivative of the space to find out the edges of the 
images.  
Hough Transform 
The flowchart Hough transformation [6] was proposed in 1962 to find out possible lines in the image. 
With the common x y− plane, as an example, the linear equation by inclined interception of a point 
( , )x y  in x y−  plane is y ax b= + . An infinite number of lines pass through ( , )x y , and all the ,a b  values 
can satisfy the equation of y ax b= + . Hence, by rewriting the equation into b ax y= − +  and using the 
plane of ,a b  as the parameter space, it can produce a single line equation of the fixed coordinates of 
( , )x y . All the points in the plane of x y− , ( , )x y  can correspond to their counterparts in the parameter 
space. The main lines of the plane can have intersection points in the parameter space. Hence, this 
study uses the normal line representation of the straight line as in Eq. 2. 
cos sinx yρ θ θ= +  (2) 
where θ is the angle of the straight line normal and the x -axis, ρ  is the normal length. The 
computation is limited in limited range of angles using the characteristics of the triangular function to 
solve the problem of infinite slope, as shown in Fig. 3. Hough transformation maps each point ( , )x y  
to multiple parameter space coordinates ( , )i jρ θ , hence, the cumulative matrix value ( , )A i j  is used to 
record the occurrence of ( , )i jρ θ . It can be expected that the range of Hough transformation in the ρθ  
parameter space is: 90 90θ− ≤ ≤o o  and D Dρ− ≤ ≤ , D  is the maximum distance of the diagonal line of 
the image. It is located in coordinates of ( , )i j  with cumulative value array ( , )A i j  corresponding to the 
accompanying parameter space coordinates ( , )i jρ θ . Initially, these cumulative arrays are set as zero. 
Then, regarding all the points ( , )k kx y on the plane of x y− , if θ  equals to the segmentation value on 
the axis of θ , the equation cos sink kx yρ θ θ= +  can be used to work out the corresponding ρ . The 
produced ρ  will be rounded up to the tolerable value of the axis of ρ . The two points of the same 
straight line on the plane x y−  will converge in a single point in case of the two curves in the 
parameter space ( , )ρ θ , namely, the cumulative matrix value is 2. After the procedure of Hough 
transformation, the value of B in ( , )A i j represents that there are B  points on the straight line 
cos sini j jx yρ θ θ= +  on the plane of x y− . Before lane marking identification, this study segments the 
images. Since the camera is installed in the vehicle facing the road in a head-on position. The part of 
the road is captured in the lower part of the image. As the lane markings often appear in pairs and the 
driving is assumed in the center of the road, the two sub-images in the lower part of the image can 
capture the markings on the left and right. Hence, the Y-axis of the coordinate system is set in the 
center of the image, and X-axis at the bottom of the image to segment the image into two parts for 
Hough transformation of  0 90rθ≤ ≤o o  at the right part and Hough transformation of 90 0lθ− ≤ ≤o o  at the 
left part. The Hough transformation of the two sub-images can find out multiple groups of possible 
( , )r rρ θ  and ( , )l lρ θ . The top five groups of cumulative value are selected as the candidate groups of 
road markings. Since the lane markings are usually in pairs, this study compares ( , )r rρ θ  and ( , )l lρ θ  to 
find out the two groups of closest θ  angle as the lane markings of the left and right sub-images. 
  
 
 
 
 
 
 
Figure 7. Nighttime image original grayscale     Figure 8. Nighttime image road edge detection results  
Conclusions 
In this paper, the adequate daytime lighting conditions is enough, the system can successfully detect 
the road marking. But the results will be effect by the shelter environment, such as dirt, vehicle or 
marking aging unclear. Therefore, road marking detection will be impact. The follow-up improved 
unilateral marking detection and virtual marking assisted to improve the detection rate of complete 
offset warning. Limited ambient lighting in the night environment, may have headlamps lighting the 
detected road marking, however, will directly affect the marking to the vehicle headlight glare direct 
detection. Improved brightness adjustment and glare filter should be able to improve this part. 
Study found that in view of the above, sufficient light of situation, the system can detect road 
markings in the day and night. The future can make the system more complete could be expected. 
Improved marking by the identification of environmental masking, low light at night, brightness 
adjustment and to improve the system response speed.  
Acknowledgments:  
This research was supported by the National Science Council, R. O. C., under grant NSC 
100-2221-E-152-007. 
References 
[1] W. C. Chen, H. W. Ko, Introduction to the International Development Trends of Intelligent 
Safety Vehicle and In-vehicle Wireless Communications, Automobile Industry Monthly, (2008), 
pp. 33-49. 
[2] X. An, M. Wu, H. He, A novel approach to provide lane departure warning using only one 
forward-looking camera,  International Symposium on Collaborative Technologies and Systems 
(CTS’06), (2006), pp. 356-362. 
[3] Technical Committee ISO/TC 204, ISO 17361: Intelligent transport systems - Lane departure 
warning systems – Performance requirements and test procedures, ISO copyright office, 
Switzerland, (2007). 
[4] Houser, A., Pierowicz, J., Fuglewicz, D., Concept of operations and voluntary operational 
requirements for lane departure warning systems (LDWS) on-board commercial motor vehicles, 
Technical Report of Federal Motor Carrier Safety Administration, No. FMCSA-MCRR-05-005, 
(2005). 
[5] Rafael C. Gonzalez and Richard E. Woods, Digital Image Processing, (2008), pp.630-634. 
[6] Richard O. Duda and Peter E. Hart, Use of the Hough transform to detect lines and curves in 
pictures, Comm. ACM, vol. 15, (1972), pp.11-15. 
表 Y04 
Advanced power semiconductor technologies and their trends, Dr. Gourab Majumdar, Mitsubishi Electric Corporation, 
Power Device Works. 
 
議程主題包含： 
1. Power Electronics 
2. Electrical Machines and Drives 
3. Control Systems, Computational Intelligence and Applications 
4. Mechatronics and Robotics 
5. Power Systems, PHEV and Renewable Energy 
6. Sensors, Actuators, System Integration and Signal Processing 
7. Industrial Informatics and Factory Automation 
8. Inertial Systems for Positioning and Orientation 
9. Positioning Trends and New Technologies 
10. Novel Applications 
 
二、與會心得 
此次參與21th IEEE International Symposium on Industrial Electronics 2012會議，申請人除了參加
相關的學術研討會外，並聆聽了一場半天專題講授課程，成果相當豐碩，而申請人也聆聽多
場與申請人研究較接近的論文，並與作者討論，對未來研究的助益甚巨。申請人在5月29 日
16:00~17:30 發表了有關影像辨識與應用相關研究方法以及技術，發表方式為口頭發表(oral 
presentation)， 並與參加人員充分討論，回答聽眾相關問題，也獲得良好的回響。 
 
玆將出席本次會議心得分述如下： 
1. 會中與各國專家學者交換意見。其中與日本、美國與東南亞各國之教授交換部分研究心
得，並收獲良多。與會學者並有多人針對該國實際之影像技術使用在工業電子應用問題提
出討論，其研究之取材令我印象深刻。 
2. 瞭解IEEE ISIE會議組織運作情形，收獲良多。 
3. Signal與Image Processing 仍為現今的熱門話題，今年亦有多篇相關論文，此亦為目前各國
發展之趨勢，我國在此方面也有相當之成果，但其未來之趨勢為我們應努力之處。 
4. 由於地利之便，本次會議我國參與之人數在全部被接受的論文之中有相當高之比例，顯見
國科會推動國際化已漸有成果。 
 
三、考察參觀活動(無是項活動者省略) 
無 
 
 
四、建議 
無 
 
五、攜回資料名稱及內容 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/24
國科會補助計畫
計畫名稱: 結合影像辨識於車道偏移警示系統之建立
計畫主持人: 許佳興
計畫編號: 100-2221-E-152-007- 學門領域: 人工智慧
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
