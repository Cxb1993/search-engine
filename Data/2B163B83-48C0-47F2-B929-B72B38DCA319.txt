??????
霍夫曼演算法 (Huffman algorithm) 可以提供一個擁有最小多餘量的最佳前綴碼
(亦即霍夫曼碼)，是訊源編碼理論中一個廣為人知的事實。由 Shannon 的訊源編碼定理，
霍夫曼碼的多餘量介於 0 和 1 之間，在訊源的機率分佈為未知的前提下，這個上界與下
界是最好的。然而在許多大家感到有興趣的情況下，我們可以針對訊源的機率分佈進行
估測，通常最少可以對一部分訊源符號的機率估測到接近真確值。在可以獲得這一部分
訊源符號機率的情形下，激發了我們研究霍夫曼碼多餘量的興趣。在此計畫中，我們推
導出當訊源符號的最大機率已知的情況下，霍夫曼碼多餘量的最佳上界，同時我們也推
導出當任意一個訊源符號的機率已知的情況下，霍夫曼碼多餘量的最佳上界。
一個同樣重要的問題是延伸訊源的編碼問題。令 L_n 為在使用霍夫曼碼對 n 個訊源
符號為一組進行編碼時，每一訊源符號的字碼平均長度。在此計畫中，我們推導出在訊
源符號的最大機率或霍夫曼碼最小字碼長度已知的情況下，L_{n_1n_2}<L_{n_1}的充分
條件。
???：延伸訊源、霍夫曼碼、最小字碼長度、最小多餘量
??????
It is well known that the Huffman algorithm produces an optimal prefix code, the Huffman
code, in the sense that it minimizes the redundancy over all prefix codes for a given source.
Shannon's source coding theorem prescribes that the redundancy R of a Huffman code is
bounded by 0<= R<=1, and these bounds are tight when the source symbol probabilities are
unknown. In most situations of interest, however, the source symbol probabilities could be
estimated based on the source data collected, and some of the source symbol probabilities
could be estimated to be very close to their corresponding true probabilities. This motivates
our research on tighter bounds on the redundancy of Huffman codes when partial information
about the source symbol probabilities is available. In this research, we obtain tight upper
bounds on the redundancy of Huffman codes when the probability of the most likely source
symbol is available. We also obtain tight upper bounds on the redundancy of Huffman codes
when the probability of an arbitrary source symbol is available.
An equally important line of interest is the compression of extended sources. Let L_n be
the average codeword length per source symbol when encoding blocks of n source symbols
by using Huffman codes. In this research, we obtain sufficient conditions for
L_{n_1n_2}<L_{n_1} when the probability of the most likely source symbol and/or the
minimum codeword length of a Huffman code for the original source is known.
Keywords: Extended sources, Huffman codes, minimum codeword length, minimum
redundancy.
where fU(p1) is given by
fU(p1) =


2− p1 −H(p1), if 0.5 ≤ p1 < 1,
3− 5p1 −H(2p1), if 0.4505 ≤ p1 ≤ 0.5,
1 + 0.5(1− p1)−H(p1), if 1/3 ≤ p1 ≤ 0.4505,
3− 7.7548p1 −H(3p1), if 0.3138 ≤ p1 ≤ 1/3,
2− 1.25(1− p1)−H(p1), if 0.2 ≤ p1 ≤ 0.3138,
4− 18.6096p1 −H(5p1), if 0.1971 ≤ p1 ≤ 0.2,
2− 1.3219(1− p1)−H(p1), if 1/6 ≤ p1 ≤ 0.1971,
p1 + 0.086, if p1 ≤ 1/6,
(4)
in which
H(p) = −p log p− (1− p) log(1− p).
The upper bounds in (3) are tight for 1/6 ≤ p1 < 1 and were obtained by Gallager
[4] for 0.5 ≤ p1 < 1, by Johnson [5] for 0.4 ≤ p1 ≤ 0.5, by Capocelli, Giancarlo,
and Taneja [6] for 1/3 ≤ p1 ≤ 0.4, and by Manstetten [15] for 1/6 ≤ p1 ≤ 1/3.
In fact, Manstetten has provided tight upper bounds for 1/127 ≤ p1 ≤ 1/6 and his
approach could possibly be adopted to derive tight upper bounds for p1 < 1/127.
Other tight upper bounds have also been derived by Capocelli and De Santis [11] for
2/(2k+1) ≤ p1 < 1/(2
k−1), k ≥ 2. The upper bound for p1 < 1/6 in (3) was obtained
by Gallager [4] but is not the tightest possible.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Upper bound on the minimum redundancy
p1
f U
(p 1
)
Figure 1: Upper bounds in terms of the probability p1 of the most likely source symbol.
• Tight lower bound in terms of the probability q of the most likely source symbol (see
Figure 3): The tight lower bound in terms of the probability p1 of the most likely
source symbol was derived by Montgomery and Abrahams [7] as
r ≥ fL(p1), (8)
where fL(p1) is given by
fL(p1) = k − (1− p1) log(2
k − 1)−H(p1), (9)
in which k is determined by p1 according to
1− 1/ log
(
2k+1−1
2k−1
)
≤ p1 < 1− 1/ log
(
2k−1
2k−1−1
)
, k ≥ 2,
1− 1/ log 3 ≤ p1 < 1, k = 1. (10)
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Lower bound on the minimum redundancy
p1
f L(
p 1
)
Figure 3: Lower bounds in terms of the probability p1 of the most likely source symbol.
• Tight lower bound in terms of the probability q of an arbitrary source symbol(see
Figure 3): It turns out that the tight lower bound in terms of the probability q of an
arbitrary source symbol is also given by (8) and (10) with p1 replaced by q [17].
B. Bounds on the Redundancy of Extended Sources
The nth extension of the source X, denoted Xn, is characterized by the probability
distribution p(i1, i2, . . . , in) =
∏n
j=1 pij for all (i1, i2, . . . , in) ∈ X
n. For a given code C =
{C(i1, i2, . . . , in) : (i1, i2, . . . , in) ∈ X
n} forXn, the expected codeword length of the code C is
A. Bounds on the Redundancy of Unextended Sources
A1. Tight upper bound in terms of the probability of an arbitrary source symbol
Suppose we are given the probabilities q1, . . . , qn of any n source symbols, where n+1 ≤ N ,
namely, we don’t know the ranks of these n source symbols relative to the rest of the source
symbols in terms of the order of probability. Without loss of generality, we assume that
q1 ≥ q2 ≥ · · · ≥ qn and let qj = pij , j = 1, . . . , n.
In the following, we will upper bound the redundancy of a Huffman code by that of
a prefix code with codeword lengths of special form. With this in mind, let li1, . . . , lin be
positive integers such that
∑n
j=1 2
−lij < 1, and let
li =
⌈
− log
(
pi
1−
∑n
j=1 2
−lij
1− qn1
)⌉
, i 6= i1 . . . , in, (15)
where for convenience we have denoted qn1 =
∑n
j=1 qj . It is easy to check that the codeword
lengths l1, . . . , lN specified above satisfy the Kraft inequality, and hence there exists a prefix
code C with these codeword lengths. The redundancy of the code C is upper bounded by
r(C) =
n∑
j=1
qj(lij + log qj) +
∑
i6=i1...,in
pi(li + log pi)
<
n∑
j=1
qj(lij + log qj) +
∑
i6=i1...,in
pi
(
− log
(
pi
1−
∑n
j=1 2
−lij
1− qn1
)
+ 1 + log pi
)
=
n∑
j=1
qj log
qj
2−lij
+ (1− qn1 )
(
log
1− qn1
1−
∑n
j=1 2
−lij
+ 1
)
= 1− qn1 +D
(
(q1, . . . , qn, 1− q
n
1 )||
(
2−li1 , . . . , 2−lin , 1−
n∑
j=1
2−lij
))
, (16)
where D(·||·) is the relative entropy defined for any two probability distributions (p1, . . . , pN)
and (p′1, . . . , p
′
N) as [3]
D((p1, . . . , pN)||(p
′
1, . . . , p
′
N)) =
N∑
i=1
pi log
pi
p′i
. (17)
By the optimality of Huffman codes, the following theorem is obtained.
Theorem 1 Let X be a source with N source symbols and let q1, . . . , qn be the probabilities
of any n source symbols, where n + 1 ≤ N . Then
r ≤ gU(q1, . . . , qn, li1, . . . , lin), (18)
+ min
(l1,...,ln)∈A
D
(
(q1, . . . , qn, 1− q
n
1 )||
(
2−l1, . . . , 2−ln, 1−
n∑
j=1
2−lj
))
, (20)
in which
A =
{
(l1, . . . , ln) : 1 ≤ l1 ≤ · · · ≤ ln,
n∑
j=1
2−lj < 1
}
. (21)
Note that we have put the monotonicity restriction on the eligible codeword lengths required
by the optimality of an optimal prefix code.
When only the probability of an arbitrary source symbol is known, we are able to derive
the tight upper bound in the following theorem.
Theorem 3 Let q be the probability of any source symbol. Then
r ≤
{
2− q −H(q), if 0.5 ≤ q < 1,
1 + q −H(q), if q < 0.5.
(22)
Furthermore, the upper bound given by (22) is tight and the bound is achieved by a source
with probability distribution (q, 1− q − ǫ, ǫ), if 0.5 ≤ q < 1, or (1− q − ǫ, q, ǫ), if q < 0.5, as
ǫ tends to 0.
Proof. Due to (5), it suffices to prove the conjecture of Ye and Yeung that
max
q<p1<1−q
min{fU(p1), g1(p1, q), g2(p1, q)} ≤ 1 + q −H(q),
if 0.18 ≤ q < 0.5.
We discuss two cases separately. First consider the case that 1/3 ≤ q < 0.5. If q < p1 <
1− q, then p1 > q ≥ (1− q)/2 and 0 < (1− p1 − q)/(1− q) < 1/2, and it follows that
g2(p1, q) = 3− 2p1 − q −H(q)− (1− q)H
(
1− p1 − q
1− q
)
≤ 3− 2p1 − q −H(q)− (1− q)
2(1− p1 − q)
1− q
= 1 + q −H(q),
where we have use the fact that H(p) ≥ 2p, if 0 ≤ p ≤ 1/2. Therefore,
r ≤ max
q<p1<1−q
min{fU(p1), g1(p1, q), g2(p1, q)}
≤ max
q<p1<1−q
g2(p1, q)
≤ max
q<p1<1−q
1 + q −H(q)
= 1 + q −H(q).
Proof. Let Ci be an optimal prefix code for the extended source X
ni with expected
codeword length L(Ci) for i = 1, 2. In other words, Lni(X) = L(Ci)/ni for i = 1, 2. Let C
be the concatenation of k1 C1’s and k2 C2’s, then C is a prefix code for the extended source
Xk1n1+k2n2 with expected codeword length L(C) = k1L(C1) + k2L(C2) = k1n1Ln1(X) +
k2n2Ln2(X). As C is not necessarily an optimal prefix code for X
k1n1+k2n2, k2n2 ≥ 1, and
Ln2(X) < Ln1(X), it follows that
Lk1n1+k2n2(X)≤
L(C)
k1n1 + k2n2
=
k1n1Ln1(X) + k2n2Ln2(X)
k1n1 + k2n2
<
k1n1Ln1(X) + k2n2Ln1(X)
k1n1 + k2n2
= Ln1(X).
The theorem is proved.
It is well known that Lkn(X) ≤ Ln(X) for positive integers k and n. The following
corollary to Theorem 4 gives more details about the two possible cases Lkn(X) < Ln(X) and
Lkn(X) = Ln(X).
Corollary 5 Let k ≥ 1 and n ≥ 1.
(i) If Lkn(X) < Ln(X), then Lk′n(X) < Ln(X) for all k
′ ≥ k.
(ii) If Lkn(X) = Ln(X), then Lk′n(X) = Ln(X) for all 1 ≤ k
′ ≤ k.
Proof. (i) If Lkn(X) < Ln(X), then by taking n1 = n, n2 = kn, and k2 = 1 in (24), we
have L(k1+k)n(X) < Ln(X) for all k1 ≥ 0. In other words, we have Lk′n(X) < Ln(X) for all
k′ ≥ k.
(ii) If Lkn(X) = Ln(X), then we show that Lk′n(X) = Ln(X) for all 1 ≤ k
′ ≤ k by
contradiction. Suppose that Lk′n(X) < Ln(X) for some 1 ≤ k
′ ≤ k, then from part (i) we
have Lkn(X) < Ln(X), a contradiction.
We note that Corollary 5(ii) implies that if Lk(X) = L1(X), then Lk′(X) = L1(X) for
all 1 ≤ k′ ≤ k. In Theorem 3 of [8], a sufficient condition is given such that Lk(X) = L1(X).
Our result in Corollary 5(ii) shows that such a sufficient condition indeed guarantees that
Lk′(X) = L1(X) for all 1 ≤ k
′ ≤ k.
In the following, we recall a result in [8] relating the probability of the most likely source
symbol with the minimum codeword length of an optimal prefix code for X. Then we give
sufficient conditions for Lkn(X) < Ln(X) in terms of k, n, the probability of the most likely
source symbol p1, and/or the minimum codeword length of an optimal prefix code for the
extended source Xn.
Let C = Ck1 be the concatenation of k C1’s, then C is a prefix code for the extended
source Xkn, and the minimum codeword length ℓ1(C) of C is given by ℓ1(C) = kℓ1(C1). In
other words, if r = s = 0, then ℓ1(C) = k; on the other hand, if r 6= 0 or s 6= 0, then either
ℓ1(C) = k(rn+s) or ℓ1(C) = k(rn+s+1). As such, the prefix code C can not be an optimal
prefix code for Xkn as ℓ1(C) does not satisfy (26). It then follows from L(C) = kL(C1) and
C1 is an optimal prefix code for X
n that
Lkn(X) <
L(C)
kn
=
kL(C1)
kn
=
L(C1)
n
= Ln(X),
and the proof is completed.
We note that from k ≥ 2, we have
1
2k(rn+s+1)+1 − 1
<
2
2k(rn+s+1) + 1
<
1
2k(rn+s)+1 − 1
.
As such, the open intervals Is, s = 0, 1, . . . , n− 1, are disjoint, where
Is =
((
2
2k(rn+s+1) + 1
)1/kn
,
(
1
2k(rn+s)+1 − 1
)1/kn)
.
Furthermore, from
1
2r+1
<
(
2
2kn(r+1) + 1
)1/kn
<
(
1
2knr+1 − 1
)1/kn
≤
1
2r
,
we see that Is ⊆ (
1
2r+1
, 1
2r
) for all s = 0, 1, . . . , n− 1. If p1 ∈ Is, then p1 can not be a dyadic
probability, which is a necessary condition for Lkn(X) < Ln(X) to hold in general. (Note that
if p1 =
1
2r
for some r ≥ 1, then for a source X uniformly distributed over X = {1, 2, . . . , 2r},
we have Ln(X) = r for all n ≥ 1 and hence Lkn(X) < Ln(X) does not hold in this case).
Given that n ≥ 1 and pn1 is non-dyadic, in the following corollary we see that Theorem 7
can be used to obtain an integer k∗ ≥ 2 such that Lk′n(X) < Ln(X) for all k
′ ≥ k∗. As
such, the coding efficiency of encoding blocks of symbols of length kn is higher than that of
encoding blocks of symbols of length n for all k ≥ k∗.
Corollary 8 Suppose that n ≥ 1 and pn1 is non-dyadic so that there exist unique integers
r ≥ 0 and 0 ≤ s ≤ n− 1 such that
1
2rn+s+1
< pn1 <
1
2rn+s
. (27)
Then we have Lk′n(X) < Ln(X) for all k
′ ≥ k∗, where
k∗ = min
{
k ≥ 2 :
(
2
2k(rn+s+1) + 1
)1/kn
< p1 <
(
1
2k(rn+s)+1 − 1
)1/kn}
. (28)
Proof. Again, by Corollary 5(i), it suffices to show that Lkn(X) < Ln(X). From (30), we
have
pkn1 <
1
2k(rn+s)+1 − 1
,
and it follows from Lemma 6(i) that the minimum codeword length, say ℓ1, of an optimal
prefix code for the extended source Xkn must satisfy
ℓ1 ≥ k(rn+ s) + 1.
Let C = Ck1 be the concatenation of k C1’s, then C is a prefix code for the extended source
Xkn and the minimum codeword length ℓ1(C) of C is given by ℓ1(C) = kℓ1(C1) = k(rn+ s).
As such, C can not be an optimal prefix code for Xkn, and we then have
Lkn(X) <
L(C)
kn
=
kL(C1)
kn
= Ln(X).
The proof is completed.
We make the following observations:
(i) Note that as k ≥ 2 and r and s are not both equal to zero, a simple calculus shows
that
1
2k(rn+s+1)+1 − 1
<
1
(2rn+s+1 − 1)k
<
1
2k(rn+s)+1 − 1
.
It follows that the half-open intervals Js, s = 0, 1, . . . , n− 1, are disjoint, where
Js =
[(
1
2rn+s+1 − 1
)1/n
,
(
1
2k(rn+s)+1 − 1
)1/kn)
.
It is clear that each Js ⊆ (
1
2r+1
, 1
2r
) and hence can not contains a dyadic probability p1, which
is a necessary condition for Lkn(X) < Ln(X) to hold in general as mentioned before.
(ii) Furthermore, as it is easy to deduce from (30) that
2
2rn+s+2 + 1
< pn1 <
1
2rn+s − 1
,
it follows from Lemma 6 that the minimum codeword length of an optimal prefix code for
the extended source Xn must be equal to either rn+ s or rn+ s+ 1. As such, the range of
p1 in (30) is consistent with the minimum codeword length assumption that ℓ1(C1) = rn+s.
(iii) Finally, observe that
(
2
22(rn+s+1)+1
)1/2n
>
(
1
2rn+s+1−1
)1/n
and
(
2
2k(rn+s+1)+1
)1/kn
is
strictly decreasing in k and tends to
(
1
2rn+s+1
)1/n
as k approaches infinity, it follows that
References
[1] D. A. Huffman, “A method for the construction of minimum-redundancy codes,” Proc.
IRE , vol. 40, pp. 1098–1101, Sept. 1952.
[2] C. E. Shannon, “A mathematical theory of communication,” Bell Sys. Tech. J., vol. 27,
pt. I, pp. 379–423, July. 1948; pt. II, pp. 623–656, Oct. 1948.
[3] T. M. Cover and J. A. Thomas, Elements of Information Theory , New York, NY: John
Wiley & Sons, 1991.
[4] R. G. Gallager, “Variations on a theme by Huffman,” IEEE Trans. Inform. Theory ,
vol. 24, pp. 668–674, Nov. 1978.
[5] O. Johnsen, “On the redundancy of binary Huffman codes,” IEEE Trans. Inform. The-
ory , vol. 26, pp. 220–222, Mar. 1980.
[6] R. M. Capocelli, R. Giancarlo, and I. J. Taneja, “Bounds on the redundancy of Huffman
codes,” IEEE Trans. Inform. Theory , vol. 32, pp. 854–857, Nov. 1986.
[7] B. L. Montgomery and J. Abrahams, “On the redundancy of optimal binary prefix-
condition codes for finite and infinite sources,” IEEE Trans. Inform. Theory , vol. 33,
pp. 156–160, Jan. 1987.
[8] B. L. Montgomery and B. V. K. V. Kumar, “On the average codeword lenth of optimal
binary codes for extended sources,” IEEE Trans. Inform. Theory , vol. 33, pp. 293–296,
Mar. 1987.
[9] J. Golic´ and M. M. Obradovic´, “A lower bound on the redundancy of D-ary Huffman
codes,” IEEE Trans. Inform. Theory , vol. 33, pp. 910–911, Nov. 1987.
[10] J. Golic´ and M. M. Obradovic´, “Correction to “A lower bound on the redundancy of
D-ary Huffman codes”,” IEEE Trans. Inform. Theory , vol. 36, p. 443, Mar. 1990.
[11] R. M. Capocelli and A. De Santis, “Tight upper bounds on the redundancy of Huffman
codes,” IEEE Trans. Inform. Theory , vol. 35, pp. 1084–1091, Sept. 1989.
[12] R. M. Capocelli and A. De Santis, “A note on D-ary Huffman codes,” IEEE Trans.
Inform. Theory , vol. 37, pp. 174–179, Jan. 1991.
????????????(一)
一、會議名稱 : (中文) 2006 亞洲量子訊息科學會議
(英文) 2006 Asian Conference on Quantum Information Science (AQIS'06)
二、會議時間 : 09/01/2006–09/04/2006
三、會議地點 : 中國 北京市
四、發表論文題目 : Lower Bounds on the Average Base Length of Lossless Quantum Data
Compression
五、參加會議經過 :
AQIS 2006 是由 Chinese Academy of Science 主辦、Japan Science and Technology 協辦的
大型國際會議，今年與會人數將近兩百人。會議於 9月 1日至 9 月 4 日在中國北京
市的 BeiJing Friendship Hotel 舉行，總共發表 107 篇論文。我們的論文於 9 月 3 日以
海報展出的方式和各國學者分享研究成果。
六、與會心得 :
1. Keynote Speech:
(1) Quantum Communication in Standard Optical Fibers (Speaker: Nicolas Gisin)
Dr. Gisin 指出一些目前量子通訊在實作上所遇到的困境，並提出幾個可能的解
決方法，包括 quantum memory 和 quantum repeater 的建造，若此方面的相關研
究得以有重大成果，那麼量子通訊或許就不再受限於短距離的傳輸。
(2) Quantum Information Processing with Polar Molecules (Speaker: Michael P. Zoller)
Dr. Zoller 探討的有極性分子的物理特性，並試圖使用這些特性建造出量子記
憶體。然而目前尚在理論發展階段，實作方面尚未看到完整的成果。
2. 會議論文：
本次會議共錄取 107 篇論文，其中 55 篇是以口頭報告的方式進行，另外 52 篇是
用海報展出的方式讓學者們進行討論，主要包括實作方面和純理論方面的相關研
究成果。
以下是與我們的研究主題較為相關或我們較有興趣深入研究的論文主題：
????????????(二)
一、會議名稱 : (中文) 2006 IEEE 訊息理論研討會
(英文) 2006 IEEE Information Theory Workshop (ITW'06)
二、會議時間 : 10/22/2006 – 10/26/2006
三、會議地點 : 中國 四川省 成都市
四、發表論文題目 : New Upper and Lower Bounds on Exponentially Weighted Average
Length of Optimal Binary Prefix Codes
五、參加會議經過 :
IEEE 訊息理論研討會是訊息理論領域幾個著名的大型研討會之一，依慣例每年春
季及秋季共舉辦兩次，提供更多機會讓世界各地在訊息理論研究領域之學者專家交
換研究發展心得。本次會議於 10 月 22 日至 10 月 26 日在中國四川省成都市的加州
花園飯店舉行，總共發表 89 篇論文及 45 篇海報成果發表。我們於 10 月 22 日前往
成都市參加會議至 10 月 26 日會議結束。
六、與會心得 :
1. Keynote Speech:
(1) A Unified Approach to Multiterminal Source Coding (Speaker: Richard Blahut)
對於多端訊源編碼的問題，先前有一些學者從幾種不同觀點提供了不同的答
案。Dr. Blahut 宣稱他們的研究成果能夠將早期的結果統一，從某一個觀點分
析問題並提供答案。這對於多端訊源編碼理論，可說是一個重大的進展。
(2) Analog Iterative Decoders: Myth or Reality? (Speaker: Sergio Benedetto)
我們都知道，對於渦輪碼及低密度同位檢查碼，遞迴式解碼可以發揮這兩種
編碼的良好性能。但遞迴式解碼要以數位式晶片實作並應用在高速通訊系統
中，還是無法實現的。Dr. Benedetto 介紹了一些以類比電路實現遞迴式解碼器
的想法，並討論這些方法在實際應用上仍會遭遇的困難點。
(3) Doubly-Generalized Low-Density Parity-Check Codes (Speaker: Marc P.C. Fossorier)
Dr. Fossorier 提出雙重廣義低密度同位檢查碼的設計方式，以理論分析及模擬
結果來證實他們提供了有彈性的方法以建造性能良好的低密度同位檢查碼。
線性網路編碼可分為 linear multicast、linear broadcast、linear dispersion 及 generic
linear network code等四種。作者探討linear dispersion和generic linear network code
之間的一種 nontrivial 關係。
(7) Network Error Correction Coding in Packetized Networks :
在傳統的通訊系統中，我們使用錯誤更正碼來減低錯誤造成的損失。同樣地
我們也都知道通訊網路可能會有封包遺失或遭到惡意的竄改，或雜訊干擾造
成的錯誤等情況發生，所以作者在這篇論文中探討應用於網路編碼的網路錯
誤更正碼。

probabilities of any n source symbols are available, where
n + 1 ≤ N . Section IV concludes this paper.
II. New Upper Bounds on Lt(X) When the
Probabilities of Any n Source Symbols Are
Available
Suppose we are given the probabilities q1, . . . , qn of
any n source symbols, where n + 1 ≤ N , namely, we
don’t know the ranks of these n source symbols relative
to the rest of the source symbols in terms of the order of
probability. Without loss of generality, we assume that
q1 ≥ q2 ≥ · · · ≥ qn and let qj = pij , j = 1, . . . , n. In this
section, new upper bounds on Lt(X) will be obtained by
exploiting such information.
Suppose that Hα(X) is available, we now give an upper
bound on Lt(X) by constructing a preﬁx code with code-
word lengths of a special form as described below. With
this in mind, let li1 , . . . , lin be positive integers such that∑n
j=1 2
−lij < 1, and for all i = i1, . . . , in, let
li =
⌈
− log
(
pαi (1−
∑n
j=1 2
−lij )
2(1−α)Hα(X) −
∑n
j=1 q
α
j
)⌉
. (3)
It is easy to check that the codeword lengths l1, . . . , lN
given by (3) satisfy the Kraft inequality, and hence there
exists a preﬁx code C with these codeword lengths. The
exponentiated expected length of the code C is upper
bounded by
Lt(C) =
1
t
log
⎛
⎝ n∑
j=1
qj2
tlij +
∑
i=i1,...,in
pi2
tli
⎞
⎠
<
1
t
log
⎛
⎝ n∑
j=1
qj2
tlij
+2t
∑
i=i1,...,in
pi
(
pαi (1 −
∑n
j=1 2
−lij )
2(1−α)Hα(X) −
∑n
j=1 q
α
j
)−t⎞⎠
=
1
t
log
⎛
⎝ n∑
j=1
qj2
tlij
+2t
⎛
⎝2(1−α)Hα(X) − n∑
j=1
qαj
⎞
⎠
t+1⎛
⎝1− n∑
j=1
2−lij
⎞
⎠
−t
⎞
⎟⎠ ,(4)
where we have used the fact that 1− αt = α.
The next theorem follows immediately.
Theorem 1 Let Hα(X) be the Re´nyi entropy of X of
order α. Let q1, . . . , qn be the probabilities of any n source
symbols, where n + 1 ≤ N . Then
Lt(X) ≤ gt,u(Hα(X), q1, . . . , qn, li1 , . . . , lin), (5)
where li1 , . . . , lin are positive integers such that∑n
j=1 2
−lij < 1, and gt,u(Hα(X), q1, . . . , qn, li1 , . . . , lin)
is given by the RHS of (4).
Obviously the upper bound given by (5) depends on
the lengths li1 , . . . , lin , and by choosing these eligible
lengths wisely, good upper bounds on Lt(X) could be
obtained.
For example, let
lij =
⌈
(1 − α)Hα(X)− log(q
α
j )
⌉
, j = 1 . . . , n. (6)
It is easy to see that these lengths are eligible
lengths. We denote the upper bound such obtained as
gSt,u(Hα(X), q1, . . . , qn). In the following, we show that
gSt,u(Hα(X), q1, . . . , qn) < Hα(X) + 1.
For all i = i1, . . . , in, let
xi =
pαi 2
(1−α)Hα(X)∑
i=i1,...,in
pαi
⎛
⎝1− n∑
j=1
2−(1−α)Hα(X)−log(q
α
j )
⎞
⎠
−pαi . (7)
Note that for all 1 ≤ j ≤ n, we have
2−(1−α)Hα(X)−log(q
α
j ) ≤
qαj∑N
i=1 p
α
i
.
It follows that
1−
n∑
j=1
2−(1−α)Hα(X)−log(q
α
j ) ≥
∑
i=i1,...,in
pαi∑N
i=1 p
α
i
,
which leads to
xi ≥
(
pαi
∑N
i=1 p
α
i∑
i=i1,...,in
pαi
)(∑
i=i1,...,in
pαi∑N
i=1 p
α
i
)
− pαi = 0.
Therefore, we have
gSt,u(Hα(X), q1, . . . , qn)
=
1
t
log
⎛
⎝ n∑
j=1
qj2
t(1−α)Hα(X)−log(q
α
j )
+
∑
i=i1,...,in
pi
(
pαi + xi∑N
i=1 p
α
i
)−t
2t
⎞
⎠
≤
1
t
log
⎛
⎝ n∑
j=1
qj2
t(1−α)Hα(X)−log(q
α
j )
+
∑
i=i1,...,in
pi
(
pαi∑N
i=1 p
α
i
)−t
2t
⎞
⎠ ,
where the inequality follows since xi ≥ 0 for all i =
1-4244-0067-8/06/$20.00 ©2006 IEEE.       319
