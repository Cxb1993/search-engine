 lights, and weather change because a game often lasts for 
several hours. Therefore, the segmentation of playfields 
based on colors is rather challenging. In this work, 
Learning Vector Quantization (LVQ) is employed to 
classify three primary components of a frame in [U,V] 
color space. The detailed algorithm is explained as 
follows. 
2.1. LVQ Algorithm 
LVQ is based on a supervised learning principle, 
where a training set with known class labels is used to 
first organize a neural network, and then apply the 
network in a recall mode for classification of the feature 
patterns. It defines near-optimal decision borders between 
the classes, even in the sense of classical Bayesian 
decision theory [5]. 
In LVQ, training is performed in two successive 
steps: the hidden layer parameters are estimated using an 
unsupervised approach, and afterwards, the output layer 
weights are updated in a supervised manner using the 
hidden layer parameters evaluated in the previous step [6]. 
In our implementation, we manually collect grass, soil 
and other (neither grass nor soil) pixels from different 
fields as training data, where the input vectors are 
two-dimensional [U,V] values of pixels, this accounts for 
the greater sensitivity of the human visual system to 
changes in luminance than changes in chromatic 
components. The number of units in the hidden layer 
depends on the overlap among the population of [U,V] 
for grass, soil and other in different field structures and 
broadcasting devices, etc. According to our extensive 
inside test, 12 hidden units (2 for grass, 3 for soil and 7 
for other) can obtain the best performance (93.4%) in our 
work. Afterward, each input vector is connected to the 
neurons in the hidden layer and the winner is determined 
by using Eq. (1-2). 
Where 1S  is the number of neurons in hidden layer, and 
1n  represents a vector in 1 1S × dimension, which is 
obtained by calculating the distance (Euclidean distance) 
of the training vector p  to the weight vector   1iW  
of each neuron. Notation 1a  is a transfer function with 
the same dimension as 1n . The nearest neuron is 
declared as the winner, it will transfer the corresponding 
winner neuron to 1, and all the others are 0. The labeled 
result of Eq. (2) will propagate to linear layer. 
In the supervised part (liner layer) of the learning 
procedure, the weight vector 2W  of the output layer, 
which group the clusters found by the hidden layer into 
classes [8] by using Eq. (3). 
where 2a  is the expected class of the training vector. If 
the actual and training network outputs are coincidence, 
the winning neuron is reinforced (pull) toward the 
training vector; otherwise, the connected weight of the 
neuron is moved away (push) from the training vector. 
The update mechanism for these weights is described by 
the following expression. 
where ( )1i sW t  represents thi  neuron’s weighting at 
time st ,α is learning rate, α (0,1]∈  and a = 1± (pull 
or push). After the learning stage, the network 
implements the input-output mapping rule and can 
generalize it to input vectors not being part of the training 
set [6]. 
 
2.2. Filed Map 
After the training procedure, the LVQ neural network is 
able to classify pixels which are identified as soil, grass 
and other (player, uniform, caption and audience) classes. 
The next step is to transfer the pixels of a frame into 
these 3 primary color components. The set of the 
transferred pixels is called “field map” in this paper. The 
technique proposed in the paper has two advantages: 1) 
Each transferred pixel possesses class concept rather than 
low-level feature. 2) The “field map” preserves spatial 
feature to characterize the distribution of baseball field 
layout. 
 3. Template-based scene classification 
As mentioned in Section 1, keyframe approach is 
unable to achieve satisfactory classification accuracy for 
sport videos sine these videos often contain many fast 
changed shots caused by high camera motion, for 
examples, players running and a ball flying on the field. 
Whereas, prior multiple features extraction and a high 
computational complexity are required for hierarchical 
mechanism. As to develop a universal model approach 
that suits for various baseball videos is still a very 
challenging issue.  
In this paper, we propose a template-based learning 
algorithm for scene classification without shot detection 
or keyframe extraction in advance. In this work, we 
manually collect templates as ground truth for every 
scene type in various baseball videos initially. These 
selected templates of the same scene should be 
distinguishable enough in layout or color distribution 
from different baseball fields. In the training phase, the 
system will add the misclassified frames to the correct 
class and update scene data set. Then, the inside and 
outside tests are explored to verify the effectiveness of 
ground truth in testing phase. For a new and different 
baseball video, our template-based approach only needs 
to add some follow-up new distinct samples into database 
rather than to rebuild the whole scene model. However, it 
is observed that the increasing templates consideration 
can lead to degrade the retrieval effectiveness. Therefore, 
1
1
1
1
21
1
S
W p
W p
n
W p
 
−
 
 
−
 = −
 
 
−  
M
, 
 (1) 
1 1( )a compet n= , (2) 
2 2 1a W a= ,  (3) 
( ) ( ) ( )( )1 1 11 α+ = + −i s i s winner sW t W t a p W t ,  (4) 
 Fig.  3. The template-based classification system. 
4. Self Evaluation  
To demonstrate the performance of our proposed 
method, the Major League Baseball (MLB, USA), and 
Nippon Professional Baseball (NPB, Japan) including 
pitch, infield, outfield, close-up and audience scene types 
are conducted, and the frame size is 320x240. 
To test the performance of our proposed 
template-based scene classification, inside and outside 
tests have been performed. For inside test, we collect 
various training templates from one and half innings of 
MLB (Milwaukee Brewers vs. Colorado Rockies), and 
two and half innings for testing. Since our training 
templates possess representative of testing sequence 
completely, a satisfactory result is expected as shown in 
Table 1. It is worth to mention that there are only 9 
templates in close-up data set, 15 templates for pitch set, 
and 30-40 templates for infield and outfield, respectively. 
For outside test, The NPB testing sequences are 
conducted. Observation on the pitch and infield layouts 
of “Tokyo Dome” are quite different than that of “Yankee 
Stadium”, since the soil components around home base or 
first base bag in “Tokyo Dome” is lesser than that of 
ground truth. Then, using the template-based similarity 
measure will misclassify the pitch and infield of testing 
sequences into outfield as shown in Table 2. However, we 
are well aware that without sufficient template space, a 
poor result is inevitable. After appending 3-5 
representative misclassified frames of testing sequences 
into the corresponding scene data sets, a satisfactory 
result is obtained. 
The partial results of the project have been published 
in The third International Conference on Intelligent 
Information Hiding and Multimedia Signal Processing 
2007.11. The complete results have been accepted by 
Multimedia Tools and Applications and now published 
online. 
 
 
 
TABLE 1: Inside test for Mil. vs. Col. 
Major League Baseball Mil vs. Col, (inside test )  
Ground 
Test 
Pitch Infield 
Out 
field 
Close-
up 
Audience Precision Recall 
Pitch 36 0 0 0 0 100% 100% 
In-field 0 10 0 0 0 100% 100% 
Out-field 0 0 7 0 0 100% 100% 
Close-up 0 0 0 71 0 100% 100% 
Audience 0 0 0 0 5 100% 100% 
 
TABLE 2: Outside test for Tigers vs. Giants. 
Nippon Professional Baseball  Tigers vs. Giants, 
(outside test ) 
Ground 
Test 
Pitch Infield 
Out 
field 
Close-
up 
Audience Precision Recall 
Pitch 0 0 0 0 0 0% 0% 
In-field 0 0 0 0 0 0% 0% 
Out-field 76 6 7 0 0 7.87% 100% 
Close-up 0 0 0 37 0 100% 100% 
Audience 0 0 0 0 5 100% 100% 
Reference 
[1] [1] H. Lu and Y.P. Tan, “An unsupervised approach to 
dominant video scene clustering”, IEEE International 
Symposium on Circuits and Systems, 2003, pp. 680-683. 
[2] [2] D. Zhong and S.F. Chang, “Real-time view 
recognition and event detection for Sports video”, J. Vis. 
Commun. Image R., vol.15, 2004, pp.330-347. 
[3] S. C. Pei and F. Chen, “Semantic scenes detection and 
classification in sports videos”, The 16th IPPR 
Conference on Computer Vision, Graphics and Image 
Processing (CVGIP), 2003, pp. 210-217. 
[4] L. Wang, B. Zeng, S. Lin, G. Xu, and H.Y. Shum, 
“Automatic extraction of semantic colors in sports video”, 
IEEE International conf. on Acoustics, Speech, and 
Signal Processing, vol.3, May 2004, pp.617-620. 
[5] T. Kohonen, “The self-organization map”, 
Proceedings of the IEEE, vol. 78, no. 9, Sep. 1990, pp. 
1464-1480. 
[6]  I.  Giakoumis, N. Nikolaidis, and I. Pitas, “Digital 
Image processing techniques for the detection and 
removal of cracks in digitized paintings”, IEEE 
Transactions on Image Processing, vol. 15, no. 1, 2006, 
pp. 178 -188.
 
98年度專題研究計畫研究成果彙整表 
計畫主持人：郭忠民 計畫編號：98-2221-E-214-054- 
計畫名稱：廣播棒球視訊之事件偵測與內容摘要 (III) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 0 100% 
篇 
本計畫為分年審
核，但為連續執行
三年的計畫，因此
論文發表有前後
計畫都有關聯 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 2 3 100% 
本計畫為分年審
核，但為連續執行
三年的計畫，因此
論文發表有前後
計畫都有關聯 
研究報告/技術報告 0 0 100%  
研討會論文 4 0 100% 
篇 
本計畫為分年審
核，但為連續執行
三年的計畫，因此
論文發表有前後
計畫都有關聯 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 0 30%  
國外 
參與計畫人力 
（外國籍） 博士生 2 0 40% 
人次 
 
 
