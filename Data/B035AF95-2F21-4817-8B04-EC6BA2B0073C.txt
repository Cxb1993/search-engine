 2
  
 
1. Introduction 
Function approximation for a set of input-output pairs has numerous scientific and engineering 
applications such as signal processing, image restoration, pattern recognition, control system, system 
identification. The fuzzy logic is a universal function approximators [1][2], they can get good performance for 
nonlinear function, provided that sufficient rules in fuzzy logic or hidden neurons in neural network.  
But in real application systems, data domain often suffers from noise and/or outliers. When there is noise 
and/or outlier exist in sampling data, the fuzzy models and neural networks may try to fit those improper data 
and obtained systems may have the phenomenon of overfitting  [3][4]. It is indeed true that algorithms 
utilized in engineering and scientific applications need to be robust. 
Cerebellar Model Articulation Controller (CMAC) was first proposed by J.S. Albus in 1975[5][6]. 
Similar to other supervised neural networks, CMAC is able to approach nonlinear functions. However, due to 
its global nature of weight updating, the architecture of CMAC is quite different from other supervised neural 
networks that learn nonlinear functions slowly. CMAC neural networks are fast learning convergence, capable 
of mapping nonlinear functions quickly due to its local nature of weight updating. There were many studies 
using CMAC models to be employed in various applications. On improving architecture of conventional 
CMAC, Yeh and Lu proposed grey relational analysis to adaptively quantize the input states of CMAC during 
learning process, but had insignificant effects [7]. On improving the learning rate and accuracy, Su etc. [8] and 
Lu and Chang [9] made use of credit assignment to reform the learning strategy of CMAC.  
2. Data Fuzzy Clustering Using Distance Relation 
A disadvantage of some of popular fuzzy clustering approaches is that the number of clusters must be 
predetermined. Even if the number of clusters is given, the clustering results of these algorithms are 
influenced by the choice of initial cluster centers. Another problem occurs when some of the training data 
incur large errors due to the outliers in real application systems. When there are outliers in training data, the 
CMAC algorithm usually cannot come up with acceptable performance. Shieh etc. [4] proposed an 
unsupervised clustering algorithm by which the reasonable clusters are automatically generated based on the 
data distribution of input data set without the need of pre-specifying the number of resultant clusters. Instead 
of randomly choosing initial cluster centers, the centers of the resultant clusters generated by this algorithm 
are then used as initial cluster centers by the RFCM to search out good resultant cluster centers by mitigating 
the influence of noise and outlier data. 
Let vi and vj represent the centers of cluster i and j respectively, the distance relation between centers of 
two clusters is defined by the following equation, 
rij ,,,2,1,,,2,1,)
2
exp(
2
2
njni
vv ji KK ==−−= σ   (1) 
where ||vi -vj || represents the Euclidean distance between vi and vj , and σ  is the width of the Gaussian 
function. Initially, each data point is formed one cluster containing the data point itself. Clusters are then to be 
merged into larger clusters based on how separated among clusters, which, in other words, on how separated 
among cluster centers. 
The value of rij indicates the distance relation between two cluster centers. In other words, for a cluster 
with center vi, any other cluster with center vj that has high value of distance relation with center vi indicates 
these two clusters have similar features and should be combined together into a new larger cluster. The center 
'
iv  of the newly combined cluster is defined by Equation (2) 
,1,...,,2,1,
1
1' ≥==
∑
∑
=
= mnj
r
vr
v n
j
m
ij
n
j
j
m
ij
i     (2) 
where m is a factor that weighs the importance of the distance relation rij. By Equation (2), the 
referenced cluster center iv  is replaced by the weighted-average of those cluster centers with high distance 
relations to the referenced center, which means the new cluster center is located even closer to the real 
 4
actual output y, desired output ∧y , and error. The CMAC model defines a three-layer neural network of two 
mapping procedures and repeats the training process to achieve learning objective. Firstly, it needs to 
determine the domain of learning space, and then quantizes the learning space into many discrete states that 
are state variables of the CMAC model and are represented to a set S in Fig. 1. Each state variable is mapped 
from associative memory to corresponding real memory cells that store the information of state variables and 
are summed as actual output value. The error is defined by the difference between actual output and desired 
output value and is used to update the contents of associative memory cells by uniform distribution. The 
learning process is repeated until the error is below a predefined value or a predefined iterated number is 
satisfied. 
 
Fig.1: Basic structure of CMAC model 
 
Fig. 2 shows the structure of two dimensional CMAC. The state variables of S1 and S2 are quantized 
into twenty one units that are named as elements encoded number 0 to number 20. The width of each element 
is the resolution of quantization. Assume a block is composed of 5 elements. There are 21 elements that are 
divided into 5 blocks including 4 complete blocks {A, B, C, D} and 1 residue block {E} in the S1 axis. Then, 
the L1 layer is formed with blocks {A, B, C, D, E}. The L2 layer in S1 axis could be formed by shifting one 
element of layer L1 and by the way would reach 5 layers. The L3-L5 layer is formed by same as layers L2. 
The same art of composition is in the S2 axis. Two blocks in the same layers of S1 axis and S2 axis formed a 
hypercube. In CMAC, each hypercube is corresponded a memory cell in real memory.  
 
Fig. 2 A 2-D CMAC 
 
If there are Nh hyper cubes and each input state is made use of Ne hyper cubes, then the actual output for 
a quantized state of the traditional CMAC is distributively stored in locations of actual memory. The actual 
output for an input state is obtained by the sum of stored contents of hypercubes covering this state. That is, 
the actual output of a specific input state can be expressed as following equation (9).  
 
 
 , and as,j = 0 or 1. (9) 
 
 
Where the ys is the actual output of input state s. Tsa is an index vector for association memory selection 
∑
w
A
 
Memory cell 
Actual  
output y 
－
＋
Desired 
 output 
∧
y
Error 
 S 
s1 
s2 
 
sn 
State variables 
Associative 
memory 
1
2
,1 ,2 , ,
1
[ , , , ]
h
h
h
N
T
s s s s N s s j j
j
N
w
w
y a a a a w
w
=
⎡ ⎤⎢ ⎥⎢ ⎥= = =⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
∑a wL M
 6
Fig. 4(b) in which dotted point denote the resultant cluster centers. As can be seen here, the influence of data 
noise has been greatly minimized. Fig. 4(c) shows the result of proposed method. Fig. 4(d) shows the result 
obtained by traditional CMAC algorithm adopted to model the centers of RFCM. The finial RMSE value is 
2.0326e-004 of proposed method and 2.1241e-004 of traditional CMAC algorithm. The RMSE value shows 
that proposed method is better than traditional CMAC algorithm. 
 
(a) Sample data points   (b) RFCM result 
 
(c) Finial result of our methods  (d) Finial result of traditional CMAC 
                Fig. 4 Equation 
x
xy sin=  added noise N(0, 0.03) 
 
Reference 
[1] Wen Yu and Xiaoou Li, “Fuzzy Identification Using Fuzzy Neural Networks with Stable Learning 
Algorithms, ” IEEE Tran. On Fuzzy systems, Vol.12, No. 3, pp. 411~420, 2004. 
[2] L.-X. Wang, “Fuzzy systems are universal approximators,” Proc. IEEE International Conf. On Fuzzy 
Systems, San Diego, pp. 1163-1170,1992. 
[3] C.C. Chung, S.F Su, and C.C. Hsiao, “The Annealing Robust Backpropagation (ARBP) Learning 
Algorithm,” IEEE Trans. on Neural Networks, Vol. 11, No 5, pp.1067-1077, 2000. 
[4] Horng-Lin Shieh, Ying-Kuei Yang and Chien-Nan Lee, “A Robust Approach of Fuzzy Clustering on Data 
Sets with Noise and Outliers,”  Cybernetics and Systems: An International Journal, Vol.37, No.7, pp. 
755-778, 2006.  
[5] J. S. Albus, “A New Approach to Manipulator Control: TheCerebellar Model Articulation Controller 
(CMAC)”, ASME J.Dynamic Systems, Measurement, Control, pp.220-227, 1975. 
[6] J. S. Albus, “Data Storage in the Cerebellar Model Articulation Controller (CMAC)”, ASME J. Dynamic 
Systems, Measurement,Control, pp.228-233, 1975. 
[7] M. F. Yeh and H. C. Lu, “On-Line Adaptive Quantization Input Space in CMAC Neural Network”, IEEE 
International Conference on Systems, Man and Cybernetics, vol.4, 2002 
[8] S. F. Su, T. Tao, and T. H. Hung, “Credit Assigned CMAC and Its Application to Online Learning Robust 
Controllers”, IEEE Transaction on System, Man, and Cybernetics-Part B: Cybernetics, vol.33, no.2, 
pp.202-213, 2003 
[9] H. C. Lu and J. C. Chang, “Enhance the Performance of CMAC Neural Network via Fuzzy Theory and 
 8
 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
▓達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明： 
 
 
 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：▓已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 □無 
技轉：□已技轉 □洽談中 □無 
其他：（以 100 字為限） 
 
 
 
3.  
This report proposes a new framework for robust fuzzy Cerebellar Model 
Articulation Controller (CMAC) algorithm.in the training phase, the 
error between traditional CMAC output and desired output is equally 
distribution to associated memory. This policy leads a phenomenon of
learning interference to reduce accuracy. In this report, in order to 
overcome the problems of CMAC for function approximation of a nonlinear 
system, a novel robust fuzzy CMAC architecture is proposed to learn 
the nonlinear system＇s features of function approximation. 
 
附件二 
七月十一日下午七點搭乘中華航空到香港，下午搭乘中國航空到大陸青島，
行李放置完畢，住進會場，並辦理報到。該大會會場位於青島海爾洲際酒店的國
際技術交流會議中心，報到後領取大會相關論文之論文集及其它有關之資料。本
次大會的與會人士約有十餘國並七百多人，發表論文六百餘篇，規模相當龐大，
大會並邀請許多知名學者進行專題演講，互相交流所研究之內容與經驗。 
十二日前往聆聽參與Josef Kittler專題演講，講題是：「Multiple Kernel 
Learning and Feature Space Denoising」。並參與Intelligent Information 
Mining session的討論。下午為本人論文的發表，會中，多人提出問題並對本人
所做研究多加研討。 
    十三日一早參加Intelligent Techniques Applications in Engineering 
session的論文發表，並對論文一問之處提出詢問，下午聽取Fuzzy Systems 
session 的論文發表，並收集與本人研究相關之論文。 
十四日參加 Data Mining and Machine Learning 及 Neural Networks and 
Support Vector Machines I,II sessions 的論文發表並參加問題討論。 
十五參日觀了中國海洋大學，經由校方人員之介紹，讓我們明白到中國海洋
大學之歷史。中國海洋大學的前身是私立青島大學，始建於  1924  年。後經國立
青島大學、國立山東大學、山東大學、山東海洋學院、青島海洋大學等幾個時期
的變遷，於2002  年更名為中國海洋大學。 
與會心得 
本次參與國際研討會最大之收獲是聆聽參與了 Josef Kittler專題演講，講題是：
「Multiple Kernel Learning and Feature Space Denoising」。之演講，以及參與有
關「Intelligent Techniques Applications in Engineering session」之研究領域的報告
場次，此兩場演講有助於本人對未來研究之助益，本人發表之論文編號 4197，
無研發成果推廣資料 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
