 [22-23], and neural network, and the classifier 
obtained by Adaboost algorithm is proved to be the 
most efficient one among them [5]. Since the real-
time performance is the concern of our proposed 
approach, Adaboost approach is adopted in our work.  
In this paper, in order to recognize the gender 
of human subjects under uncontrolled environment, 
a novel mechanism is proposed. First, we calculate 
the RGB ratio of a racial skin to eliminate the noises 
from the face detector. The context-regions 
enhancement, which enhances some regions related 
in spatial domain, is developed to make the evidence 
of these regions stronger than the original ones. To 
deal with the problem of variant viewing angles of 
human subjects, a voting strategy, in which gender 
information is collected from the surrounding faces 
of the original one, is weighted by the novel 
confidence ratio.  
The remainder of this paper is organized as 
follows. In Section II, we describe the approaches of 
the face detection and skin-color-filter. Section III 
shows the proposed feature set and Section IV 
introduces the mechanism of gender recognition. We 
then detail our experimental results in Section V and 
present some concluding remarks in Section VI. 
2 FACE DETECTION AND FACE 
FILTERING 
 
2.1 Face Detection 
Since face-based gender recognition is our 
concern, it is necessary to detect the frontal or near 
frontal faces. Therefore, face detection is the first 
important step to be accomplished. To satisfy the 
real-time requirement, we use the Viola and Jones 
face detector, which can provide fast detections of 
face regions. The face detector is trained by using 
Adaboost algorithm. Haar-like features including 
edge features and center surround features are 
extracted, in which integral images are employed for 
efficient computing. In order to detect the region 
rotated 45 degrees, we add the Haar-like features 
that rotate 45 degrees. The details of the face 
detector can be found in [24]. 
2.2 Skin-Color Filter 
The face detector [24] can achieve success rate of 
80%-90%. However, their detection rate of false 
positives is in the 10%-20% range. Most false 
positives are detected as faces due to their patterns 
of Haar-like features are highly similar to the real 
faces. To distinguish between real faces and non-
faces, skin colors are important features for noise 
filtering. Considering the different skin colors of 
different races, one race from another should have 
distinct color characteristics. In this work, we focus 
on the race of Asians.  
We compute the RGB ratio  from face 
images as follows: 
)( arP
 )(1
0
P(r) ∑
=
=
K
a
arPK
μ ,                         (1) 
      
aaa
a
a bgr
rrP ++=)(
 ,                         (2) 
where the  ,  ,  are one pixel RGB values 
from face images. The mean 
ar ag ab
P(r)μ  of the RGB ratio 
is defined. K is the pixels from faces. The variance 
 from the faces RGB ratio can thus be 
computed by 
2
)(rPσ
∑
=
−=
K
a
arP rPK 0
2
P(r)
2
)( ])([
1 μσ .                     (3) 
According to the RGB color distribution, a face 
candidate is considered as a non-face if its color 
distribution was out of the range of 2σp(r). 
3 FEATURE EXTRACTION 
In this section, we shall present the hybrid feature 
set and the training algorithm used in the proposed 
system. In the hybrid feature set, simple but 
effective block-based color and edge features are 
computed. Furthermore, the efficient algorithm of 
Adaboost is employed for training purpose. 
 
3.1 A Hybrid Feature Set 
To show the hybrid feature set, the color feature and 
the edge feature are demonstrated in Figs.1(a)-(b), 
respectively. In the first step, we transform the RGB 
color images into gray-scale images and then divide 
into 8 × 8 blocks for each image. The quantized 
gray-scale in one block region , which is classed 
templates  is defined as follows:  
g
l
),/(INT ),(),( ρτω ll jiji =                          (4) 
                  , ;80 ,2 N∈<<∈ φφρ φ
where the  and  are the original gray-scale 
value and the quantized gray-scale one in the 
coordinate (i, j), respectively. 
l
),( jiτ l ),( jiω
To compute the representative color for a block, 
the resulting color of a block is defined by 
∑∑
= =
=
c
j
c
i
ji ppJ
1 1
),( ),()|( llll ω                   (5) 
 ,),...,,max( 21 lJJJGd =                    (6) 
where the value  is one template probability of 
the pixels in one block. The representative color  
lJ
dG
 detect the faces in the surrounding regions near the 
original face detected by the face detector, which is 
so called surrounding faces detection. To emphasize 
some important regions of a detected face, an 
approach so called context-regions enhancement is 
proposed. Furthermore, we evaluate the confidence 
of the gender of the face with local region enhanced 
and compute the linear combination of these faces 
weighted by the confidence value. The details of 
context-regions enhancement are illustrated in the 
following section.  
 
4.2 Context-Regions Enhancement 
In this section, we describe the method of context-
regions enhancement. With applying this method, 
we can first enhance some important face zones and 
also reduce some disturbance of the gender 
recognition.  
The symmetric structure in shape is important 
for recognizing genders. We observe that some 
effective features learned from Adaboost are from 
the symmetric regions. Insufficient information 
obtained from these regions would result in fewer 
evidence of their corresponding gender. Therefore, 
an approach named context-regions enhancement is 
proposed to overcome this problem. We enhance the 
symmetric regions in external face zones by 
,1)|( ≥=× kk IBIpϖ                      (9) 
where ϖ  is the threshold, B  is the maximum 
cumulative density of the contrast value, and  is a 
value of a context region. If I
kI
k satisfies Eq.(9), then 
Ik is replaced by 
,*II k =                              (10) 
where the value *I  is the enhancement parameter 
determined empirically. We can examine the 
context regions based on Eqs.(9)-(10) to verify if 
they possess coherent features. In contrast, if a 
value of a context regions satisfies 
  ,1)|( ≤=× kk IBIpϖ                     (11) 
then 
,Δ= II k                              (12) 
where the value ΔI  is the reduced disturbance 
parameter. 
 
4.3 Confidence Evaluation and Voting 
Strategy 
After detecting surrounding faces of the original one, 
we evaluate the confidence of the gender of the face 
with local region enhanced and compute the linear 
combination of these faces weighted by the 
confidence value. Obtaining from the Adaboost 
classifier, we compute two values that are the 
positive and negative values for the genders and then 
combine them together by measuring the distance 
between these two values. In this way, a normalized 
weighting for the confidence of the gender can be 
obtained. The distribution of our training dataset is 
shown in Fig. 2 and then a fitting line is 
approximated for the further weighting analysis.  
 
 
  Fig. 2. Distributions of the database generate from 
Adaboost algorithm. 
 
From Fig. 2, we can get a fitting line and the 
fitting line is also called the confidence line 
computed by 
,0=++ dbyax                         (13) 
where and c  are constant. After the fitting line 
is obtained by Eq.(13), the extreme values of the 
fitting line should be estimated for normalizing 
confidence values. The input samples closer to the 
extremities of the fitting line are projected onto the 
line, which are , and the mean of the 
positive samples with high confidence is computed. 
The mean is then considered as the maximum 
confidence for the positive samples. 
 ,ba
),( 00 yxPL
The extreme 
value of the fitting line is formulated as: 
∑
=
×+=
η
ηξ 1),( ,
1
L
ffemale
L
f
yx PB D              (14) 
where  is the female’s f
yxB ),( boundary position, ξ  is 
the adjustable error, η  is the numbers of female’s 
data and  is the female’s positions. When  is 
‘1’, it means that  belongs to the high 
confidence values.  
female
LP
fD
female
LP
In contrast, the negative samples further to the 
origin of the fitting line are projected onto the line 
and the mean of these points are computed and 
considered as the minimum value of negative 
samples . Confidence of the recognized gender 
defined by this approach can reveal its normalized 
value. For an unknown face, the sample is first 
projected onto the fitting line for measuring its 
m
yxB ),(
 6. CONCLUSION 
In this work, a system of real-time gender 
recognition for real life images has been proposed. 
The contribution of this work is four-fold. A skin-
color filter has been developed to filter out non-face 
noises. In order to make the system robust, a 
mechanism of decision making based on the 
combination of surrounding face detection, context-
regions enhancement and confidence-based 
weighting assignment has been designed. 
Experimental results obtained by using extensive 
dataset have shown that our system is effective and 
efficient in recognizing genders in uncontrolled real 
life images. 
REFERENCES 
[1] Y. WANG, H. AI, and B. WU, C. HUANG, “Real 
Time Facial Expression Recognition with Adaboost,” 
International Conference on Pattern Recognition, vol. 
3, pp. 926-929, Aug 2004. 
[2] Y. Andreu, R. A. Mollineda, and P. Garc´ıa-Sevilla, 
“Gender Recognition from a Partial View of the Face 
Using Local Feature Vectors,” Lecture Notes in 
Computer Science, vol. 5524, pp. 481-488, June 2009. 
[3] A.C. Gallagher, T. Chen, “Understanding Images of 
Groups of People,” IEEE Conference on Computer 
Vision and Pattern Recognition, pp. 256-263, 2009. 
[4] L. Cao, M. Dikmen, Y. Fu, and T. S. Huang, ”Gender 
Recognition from Body,” ACM international 
conference on Multimedia, session 2, pp. 725-729, 
2008.
[5] B.C. Shen, C.S. Chen, and H.H. Hsu, “Fast Gender 
Recognition by Using A shared-Integral-Image 
Approach,” IEEE Conference Proceeding, pp. 521-
524, April 2009.
[6] H. Lu and H. Lin, “Gender Recognition using 
Adaboosted Feature,” International Conference on 
Natural Computation, vol. 2, pp. 646-650, Aug 2007. 
[7] H. Lin, H. Lu ,and L. Zhang, “A New Automatic 
Recognition System of Gender, Age and Ethnicity,” 
The Sixth World Congress on Intelligent Control and 
Automation, vol. 2, pp. 9988-9991, 2006. 
[8] K. Balci and V. Atalay, “PCA for gender estimation: 
which eigenvectors contribute?,” International 
Conference on Pattern Recognition, vol. 3, pp. 363–
366,  2002. 
[9] V. Rodrigo, R. D. S. Javier, and C. Mauricio, 
“Gender Classification of Faces Using Adaboost*,” 
CIARP, vol. 4225, pp. 68-78, 2006. 
[10] Y. FANG and Z. WANG, “Improving LBP Features 
for Gender Classification,” International Conference 
on Wavelet Analysis and Pattern Recognition, vol.1, 
pp. 373-377, Aug 2008. 
[11] H. C. LIAN and B. L. LU, “Multi-View Gender 
Classification Using Multi-Resolution Local Binary 
Patterns and Support Vector Machines”, International 
Journal of Neural Systems, vol.17, pp.479-487, 2007. 
[12] B. Wu, H. Ai, and C. Huang, “LUT-Based Adaboost 
for Gender Classification,” Audio- and Video-Based 
Biometric Person Authentication, vol. 2688, pp. 104-
110, 2003. 
[13] B. Wu, H. Ai and C. Huang, ”Facial image retrieval 
based on demographic classification,” International 
Conference on Pattern Recognition, vol. 3, pp. 914-917, 
2004. 
[14] H. Lu, Yi. Huang, Y. Chen and D. Yang, “Automatic 
gender recognition based on pixel-pattern-based 
texture feature,” Journal of Real-Time Image 
Processing, vol. 3, pp. 109-116, March 2003.
[15] R. Nikolaus, “Learning the Parts of Objects using 
Non-negative Matrix Factorization,” Term Paper, Feb 
2007. 
[16] D. D. Lee and H. S. Seung, “Learning the parts of 
objects with nonnegative matrix factorization,” 
Nature, vol. 401, pp. 788–791, July 1999. 
[17] A. Lapedriza, D. Masip, and J. Vitria, “Are External 
Face Features Useful for Automatic Face 
Classification?,” Computer Vision and Pattern 
Recognition, pp. 151-158, June 2005. 
[18] A. Lapedriza, M.J. Marin-Jimenez, and J. Vitria, 
“Gender Recognition in Non Controlled 
Environments,” International Conference on Pattern 
Recognition, vol.3, pp. 834-837, 2006. 
[19] M. Mayo and E. Zhang, “Improving Face Gender 
Classification By Adding Deliberately Misaligned 
Faces To The Training Data,” Image and Vision 
Computing New Zealand, pp. 1-5, Nov 2008. 
[20] E. Osuna and R. Freund, “An Improved training 
algorithm for Support Vector Machine,” Proceedings 
of 1997 IEEE Workshop on Neural Networks for 
Signal Processing, pp. 276-285, 1997. 
[21] B. Moghaddam and M.H. Yang, “Gender 
Classification with Support Vector Machines,” IEEE 
Conference Proceeding, pp. 306-311, March 2000. 
[22] Y. Freund and R. E. Schapire, “Experiments with a 
New Boosting Algorithm,” Machine Learning: 
Proceedings of the Thirteenth International 
Conference, pp.148-156, 1996. 
[23] Robert E. Schapire and Yoram Singer, “Improved 
boosting algorithms using confidence-rated 
predictions,” Mach. Learn., vol. 37, no. 3, pp. 297–
336, Dec 1999. 
[24] P. Viola and M. Jones, “Rapid object detection using 
a boosted cascade of simple features,” IEEE 
Computer Society Conference on Computer Vision 
and Pattern Recognition, vol. 1, pp. 511–518, 2001. 
 
ACKNOWLEDGEMENT 
This work is supported by the National Science 
Council under Contract No. NSC98-2218-E-155-001. 
 
REAL-TIME GENDER RECOGNITION FOR UNCONTROLLED 
ENVIRONMENT OF REAL-LIFE IMAGES 
 
 
 
 
 
Keywords: gender recognition, uncontrolled environment, real-life images 
Abstract: Gender recognition is a challenging task in real life images and surveillance videos due to their relatively 
low-resolution, under uncontrolled environment and variant viewing angles of human subject. Therefore, in 
this paper, a system of real-time gender recognition for real life images is proposed. The contribution of this 
work is fourfold. A skin-color filter is first developed to filter out non-face noises. In order to make the 
system robust, a mechanism of decision making based on the combination of surrounding face detection, 
context-regions enhancement and confidence-based weighting assignment is designed. Experiment results 
obtained by using extensive dataset show that our system is effective and efficient in recognizing genders 
for uncontrolled environment of real life images. 
1 INTRODUCTION 
Gender recognition is a challenging task in real life 
images and surveillance videos due to their relatively 
low-resolution, under uncontrolled environment and 
variant viewing angles of human subject. To recognize 
the gender of a human subject, the selection of a set of 
effective features on an appropriate part(s) of human 
body is necessary. The face of a human subject contains 
some information and could be a useful clue for 
recognizing emotion and facial expressions by Wang 
et al. [1]. On the other hand, Andreu et al. [2] apply a 
partial view of face for gender recognition. In [2], they 
consider the eyes zone to recognize gender by using 
local feature vectors. In [3], Gallagher and Chen 
combine social context with appearance to recognize 
gender. In addition, the full body [4] of a human 
subject that provides the silhouette of a person was 
adopted for gender recognition. In the literature, 
some edge-based features were extracted from face 
zones, such as Haar-like features [5-6], Gabor 
wavelets [5-7], LBP [9-11], LUT [12-13] and 
quantized edge features [14]. For color-based 
features, the PCA [8-10] and NNM [15-16] of 
relatively higher computation complexity are well 
known methods for analyzing the facial 
characteristics. However, for real-time applications, 
a feature set that is of light computational cost is 
unavoidable.  
In the related works, most approaches focus on 
recognizing the gender of human subjects in the 
images obtained under well-controlled lighting 
condition and pure non-textured background. 
Besides, the face of human subjects captured is 
frontal and of high resolution. For the faces obtained 
from daily life images and surveillance videos, the 
resolution is relatively much lower than those from 
ID photos. Approaches of gender recognition proposed 
for ID photos could not work well for low-resolution 
ones. Under these circumstances, the face information 
could be insufficient. In order to tackle this problem, 
some researchers tried to extract features either from 
the internal or the external face zones, or both of 
them. Lapedriza et al. [17] recognized the gender by 
using the external face features. In [18], features are 
computed from the external and internal face zones. 
The internal features composed by eyes, nose and 
mouth and the external features located in head, ears 
and chin. The fragment-based face features are thus 
extracted. It has proved that the external face zone 
can provide rich information for gender recognition. 
Therefore, in our proposed approach, both the 
 ∑
=
−=
K
a
arP rPK 0
P(r)
2
)( ])([
1 μσ .                     (3) 
 
According to the RGB color distribution, a face 
candidate is considered as a non-face if its color 
distribution was out of the range of           . 
3 FEATURE EXTRACTION 
In this section, we shall present the hybrid 
feature set and the training algorithm used in the 
proposed system. In the hybrid feature set, simple 
but effective block-based color and edge features are 
computed. Furthermore, the efficient algorithm of 
Adaboost is employed for training purpose. 
 
3.1 A Hybrid Feature Set 
 
To show the hybrid feature set, the color feature 
and the edge feature are demonstrated in Fig.2 and 
Fig.3, respectively. In the first step, we transform the 
RGB color images into gray-scale images and then 
divide into 8 8 blocks for each image. The 
quantized gray-scale in one block region , which is 
classed templates  is defined as follows:  
×
g
l
 
),/(INT ),(),( ρτω ll jiji =                          (4) 
λφφρ φ ,...,1 , ;80 ,2 =∈<<∈ lN               
    
where the  and  are the original gray-scale 
value and the quantized gray-scale one in the 
coordinate (i, j), respectively. 
     l
),( jiτ l ),( jiω
To compute the representative color for a block, 
the resulting color of a block is defined by 
 
∑∑
= =
=
c
j
c
i
ji ppJ
1 1
),( ),()|( llll ω                   (5) 
 ,),...,,max( 21 lJJJGd =                    (6) 
 
where the value  is one template probability of 
the pixels in one block. The representative color  
is determined by choosing the color with maximum 
probability. 
lJ
dG
To compute the edge features, Canny edge 
detector is employed to conduct the edge computing 
in one block. The feature vectors E  are computed 
and considered as templates γ  by: 
 
 
                           (7) ),,...,,max( 21 γoooE =
                  χγ ,...,1=  
where the sum of edge vectors  in one block, 
which combines with 
γo
NN × mask is defined by: 
 
              (8) ,),(       
2/)1(
2/)1(
2/)1(
2/)1(
),( ∑ ∑−+
−−=
−−=
−−=
=
Ni
Nii
Njj
Njj
ji jio
γγ h
 
where γ is the type of template, and the binary 
function  is value ‘1’ if this pixel value is ),( jiγh
not zero. Otherwise, the pixel is set to ‘0’ if its value 
is zero.  
 
 
Fig.2. The color features process. 
 
 
Fig.3. The vector features process. 
 
3.2 Training Mechanism 
 
In the training mechanism, we collect 1948 face 
images to be the training dataset. To overcome the 
problem of the variant viewing angles of human 
subjects, we simulate the actual rotated faces by 
cutting a portion of the right or the left side face 
regions. Some examples of training dataset are 
shown in Fig. 4.  
 
 
Fig.4. Training data images. 
 
We then adopt the set of color and edge features, 
in which the total dimension is 128 and 64 for each 
feature in our experiment, for the Adaboost training 
algorithm. The algorithm of Adaboost is shown as 
follows. 
)2 rσ p(
 4.2 Context-Regions Enhancement 
 
In this section, we shall describe the method of 
context-regions enhancement. With applying this 
method, we can first enhance some important face 
zones and then reduce some disturbance of the 
gender recognition.  
The symmetric structure in shape is important 
for recognizing genders. We observe that some 
effective features learned from Adaboost are from 
the symmetric regions. Insufficient information 
obtained from these regions would result in fewer 
evidence of their corresponding gender. Therefore, 
an approach named context-regions enhancement is 
proposed to overcome this problem. We enhance the 
symmetric regions in external face zones by 
 
,1)|( ≥=× kk IBIpϖ                      (9) 
 
where ϖ  is the threshold, B  is the maximum 
cumulative density of the contrast value, and  is a 
value of a context region. If I
kI
k satisfies Eq.(9), then 
Ik is replaced by 
 
,*II k =                              (10) 
 
where the value *I  is the enhancement parameter 
determined empirically. We can examine the 
contexts regions based on Eqs.(9)-(10) to verify if 
they possess coherent features. In contrast, if a 
value of a context regions satisfies 
 
  ,1)|( ≤=× kk IBIpϖ                     (11) 
then 
,Δ= II k                              (12) 
 
where the value ΔI  is the reduced disturbance 
parameter. 
 
4.3 Confidence Evaluation and Voting 
Strategy 
 
After detecting surrounding faces of the 
original one, we evaluate the confidence of the 
gender of the face with local region enhanced and 
compute the linear combination of these faces 
weighted by the confidence value. Obtaining from 
the Adaboost classifier, we compute two values that 
are the positive and negative values for the genders 
and then combine them together by measuring the 
distance between these two values. In this way, a 
normalized weighting for the confidence of the 
gender can be obtained. The distribution of our 
training dataset is shown in Fig. 7 and then a fitting 
line is approximated for the further weighting 
analysis.  
From Fig. 7, we can get a fitting line and the 
fitting line is also called the confidence line 
computed by 
,0=++ dbyax                         (13) 
where and  are constant. After the fitting line 
is obtained by Eq.(13), the extreme values of the 
fitting line should be estimated for normalizing 
confidence values. The mean of the positive samples  
 ,ba c
 
  Fig. 7. Distributions of the database generate from 
Adaboost algorithm. 
 
with high confidence is computed. The mean is then 
considered as the maximum confidence for the 
positive samples. The extreme value of the fitting 
line is formulated as: 
 
∑
=
×+=
η
ηξ 1),( ,
1
L
ffemale
L
f
yx PB D              (14) 
 
where  is the female’s f
yxB ),( boundary position, ξ  is 
the adjustable error, η  is the numbers of female’s 
data and  is the female’s positions. When  is 
‘1’, it means that  belongs to the high 
confidence values.  
female
LP
fD
female
LP
In contrast, the negative samples further to the 
origin of the fitting line are projected onto the line 
and the mean of these points are computed and 
considered as the minimum confidence of male’s 
boundary position . Confidence of the recognized 
gender defined by this approach can reveal its 
normalized value. For an unknown face, the sample 
point 
m
yxB ),(
),( 00 yxPL  is first projected onto the fitting line 
for measuring its confidence. The projection 
 and confidence values )cos(|||| ),( θ×− m yxL BP LC  can 
be obtained by 
 Some examples of the result of gender 
recognition in real life images are illustrated in Fig. 
10. These images are of different lighting conditions, 
different size of human subjects, different groups of 
people, etc. It can be observed that most human 
subjects can be recognized by their gender 
successfully. A few human subjects are not detected 
because the face detector employed does focus on 
frontal or near frontal faces. However, in this paper, 
this face detector satisfied our requirement since 
recognizing genders in frontal or near frontal faces is 
our primary concern.  
 
6. CONCLUSION 
 
In this work, a system of real-time gender 
recognition for real life images has been proposed. 
The contribution of this work is four-fold. A skin-
color filter has been developed to filter out non-face 
noises. In order to make the system robust, a mechanism 
of decision making based on the combination of 
surrounding face detection, context-regions enhancement 
and confidence-based weighting assignment has been 
designed. Experiment results obtained by using extensive 
dataset have shown that our system is effective and 
efficient in recognizing genders in uncontrolled real life 
images. 
 
 
 
 
 
Fig.10. The results of real life image. 
REFERENCES 
[1] Y. WANG, H. AI, and B. WU, C. HUANG, “Real 
Time Facial Expression Recognition with Adaboost,” 
Proc. of International Conference on Pattern 
Recognition, vol. 3, pp. 926-929, Aug 2004. 
[2] Y. Andreu, R. A. Mollineda, and P. Garc´ıa-Sevilla, 
“Gender Recognition from a Partial View of the Face 
Using Local Feature Vectors,” Lecture Notes in 
Computer Science, vol. 5524, pp. 481-488, June 2009. 
[3] A.C. Gallagher, T. Chen, “Understanding Images of 
Groups of People,” Proc. of IEEE Conference on 
國科會補助計畫衍生研發成果推廣資料表
日期 2010年09月06日
國科會補助計畫
研發成果名稱
發明人
(創作人)
技術說明
技術移轉可行性及
預期效益
技術/產品應用範圍
產業別
計畫名稱:
計畫主持人:
計畫編號: 學門領域:
(中文)
(英文)
成果歸屬機構
(中文)
(英文)
植基於線上學習之智慧型即時人次計算系統
陳敦裕
98 -2218-E -155 -001 - 影像處理
即時人臉性別辨識系統
REAL-TIME GENDER RECOGNITION FOR UNCONTROLLED ENVIRONMENT OF REAL-
LIFE IMAGES
元智大學 陳敦裕,林冠儀
自動化性別辨識的功能在大眾場所例如街道、購物商場、或者地鐵等地方對於
許多應用是十分重要的議題。比如說特定性別之人群聚集在某些特定場所可能
表示某些事件或行為正要發生。另一方面，性別辨識在購物商場能夠提供相當
有價值的資訊，比如哪些商店或商品對於特定性別較具有吸引力等等可用資訊
，皆可利用自動化性別辨識系統達成這些目的。因此，我們研究並設計一創新
的智慧型性別辨識系統，此一系統考量系統效率之外，亦須考量硬體成本，因
此，在硬體方面，先採用單一攝影機之架構下，研發本系統。攝影機可架設地
點包含電子廣告看板、商店櫥窗以及商品週遭等地方，目的在於計算駐足觀看
廣告或商品以及逗留之人物性別。因此，首先研發臉部偵測技術，以判斷觀看
者是否專注在目標區域，此外，為了辨識逗留之人物或人群，我們也研發性別
辨識技術在人臉之外之人體部位，在此研究中，我們擷取有效的人體特徵，以
達成高辨識率。
Gender recognition is a challenging task in real life images and
surveillance videos due to their relatively low-resolution, under
uncontrolled environment and variant viewing angles of human subject.
Therefore, in this paper, a system of real-time gender recognition
for real life images is proposed. The contribution of this work is
fourfold. A skin-color filter is first developed to filter out non-
face noises. In order to make the system robust, a mechanism of
decision making based on the combination of surrounding face
detection, context-regions enhancement and confidence-based weighting
assignment is designed. Experimental results obtained by using
extensive dataset show that our system is effective and efficient in
廣告業；研究發展服務業
視訊傳播以及監控產業
可技術移轉予視訊監控廠商,並預期可帶來可觀之廣告效益
註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
