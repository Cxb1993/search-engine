2一、計畫中文摘要
隨著個人化數位相機與數位攝影機的普及，在旅遊過程中以數位資料留下旅遊經驗已經成為最普
遍的應用方式。因為有效地瀏覽與管理消費性多媒體內容的急迫需求，已經有許多研究專注於對消費
性照片與視訊資料進行自動分群或分類。許多研究已經利用顏色、紋理、拍攝時間等資訊來進行分類。
然而，在消費性多媒體內容中，“人”的出現往往扮演最重要的角色。我們認為在家庭聚會或旅遊過
程中拍下的多媒體內容中，依照家庭成員的出現將應該是此類資料最重要的分類依據。因此，在本計
畫中，我們將發展一個以人臉資訊為依據的自動分群系統。
消費性多媒體內容中在非控制環境下所拍攝的資料常有大幅度的光影、姿態、以及表情變化。這
些變化都將造成傳統人臉辨識或分類極大的挑戰。近年來，越來越多的研究在臉部資訊之外訴諸於
“脈絡資訊”的使用，希望能以此彌補只利用人臉資訊(如特徵臉 eigenface)的不足。基於這樣的概念，
我們計畫利用區域特徵點(local feature point)比對的資訊，開發出一個可用於描述人臉上重要部份的機
率模型，以完成在消費性多媒體資料上的人臉分群系統。
我們所提的人臉分群系統不僅使用到人臉資訊，如特徵臉，還利用到衣服、區域特徵點比對等脈
絡資訊(context information)。我們預計使用兩年的時間開發以下幾個技術：
(1) 萃取脈絡資訊，包括衣服及以 SIFT為基礎的區域特徵點比對
(2) 融合各項資訊以提昇人臉分群的效能
(3) 發展基於區域特徵點比對的視覺語言模型
(4) 自動偵測出人臉相對應的不規則衣服區塊
(5) 視訊資料中的人臉分群及相關應用
本計畫所提的技術將在不同類型的消費性多媒體內容中進行驗證，以符合使用者的需求。此外，
本計畫的研究成果將可實際應用於消費性電子產品或相關軟體系統中，對於產業推動與應用科技人才
的培育有直接的助益。
關鍵字：人臉分群、消費性多媒體內容、區域特徵點、脈絡資訊
二、計畫英文摘要
As the increasing popularity of personal digital camera and digital camcorder, recording everything
during trips in digital forms has been the most popular way to store travel experience. Due to the urgent
demands of efficient browsing and managing multimedia content, many studies have been conducted on
automatic clustering or classifying consumer photos or videos. Many projects focus on classifying
multimedia content on the basis of color, texture, and time information. However, we think that the
appearance of humans plays the most important role in the audiovisual content taken in family party or
tourism. The most important factor for classifying consumer multimedia content should be“human beings.”
Therefore, we will be devoted to develop an automatic content clustering based on human faces.
In consumer multimedia content, large variations in lighting, poses, and expression captured in
uncontrolled environments harm the application of conventional face recognition process. They pose
significant challenges in both face recognition and image classification. Recently, more and more studies
appeal to “context information”and overcome the shortage of the systems that solely utilize facial
information. Based on this idea, we plan to take advantage of local feature point and develop a probabilistic
model to describe the context of“important face landmarks.”
4系統不需辨識出“她/他是誰”，只需將“同樣是她/他出現的照片聚集在一起。”因此，在本計畫中，
我們將以人臉辨識(face recognition)來稱呼大家所熟知的辨識程序，以人臉標註(face annotation)或人
臉分群(face clustering)來稱呼本計畫所進行的主要內容。稱之為人臉分群的原因不言可喻，而稱之為
人臉標註的原因在於分群後，使用者最常使用到的功能是將同一群人臉標註上姓名，如同 Google
Picasa最近所推出的 Name tag功能一樣[1]。
此外，本計畫預計將影像上的人臉標註模組進一步擴展到視訊資料上。在視訊資料上進行人臉分
群有相當多的應用。例如，利用人臉分群結果找到主要人臉(major face)，並藉此在新聞節目中找到主
播[2]、在戲劇或電影中找到主要角色[3]等，以利於進一步的段落分析(story segmentation)、社群分析
(community analysis)、內容瞭解(content understanding)等。
四、研究目的
本計畫的研究目的在於處理家庭生活中拍攝的影像及視訊資料，自動進行人臉分群與整合性的瀏
覽、管理。本計畫預計以兩年時間分別對其進行研究。
2.1 特徵臉(eigenface)辨識
我們先根據使用者所選定的 training data建出由家庭成員的人臉所造出的 eigenspace。在進行 face
clustering 時，我們將偵測到的人臉區域投影到 eigenspace 中，藉此得到每個人臉區域在特徵空間
(eigenspace, 或稱 face space)中各個基底(basis，即 eigenface)的投影值，形成描述此人臉區域的特徵向
量(feature vector)。
在傳統的人臉辨識中，我們可以將欲測試人臉區域的特徵向量與 training data的 feature vector進
行比較，取其距離最近者即辨識為該人物。然而，在本計畫處理的問題中，測試人臉區域的特徵向量
僅能當作部份參考。此特徵向量往往因為側臉的關係而造成辨識時很大的干擾。
2.2以衣服資訊為基礎的重新分群 (re-clustering based on clothes information)
基於人臉區域的特徵向量進行分群後，在同一群中的人臉往往不是同一人。造成這麼大誤差的主
要原因仍是側臉或不同表情的關係。因此，在以往的研究中[4][5]，研究者尋求臉部之外的資訊來提
高人臉標註的準確率。
使用衣服資訊重新分群的基本假設是：同一人在短時間內穿的衣服是一樣的。因此，我們找出每
個人臉區域所對應的身體區域，萃取其衣服相關特徵，如顏色與紋理，並根據衣服特徵的比對來修正
上述分群的結果。
2.3以區域特徵點(local feature point)比對為基礎的重新分群法
雖然衣服資訊能補足側臉或表情所造成的干擾，但如何找到適當的衣服區域仍是很大的問題。此
外，在團體合影時，衣服區域常被被他人擋住也造成很大的干擾。
為了再提高準確性，我們引進了近年來在電腦視覺領域被廣泛使用的區域特徵點比對。在本計畫
中，我們預計使用 SIFT (Scale Invariant Feature Transform) [6]來描述區域特徵點，並藉此比對不同的
人臉。圖 2為人臉上 SIFT特徵點的範例，我們將使用 128維度的 SIFT特徵向量當作基礎，比對不同
影像中的對應點，並藉此找出那張臉跟那張臉是相似的。
我們利用 SIFT 對於位移、亮度、旋轉、縮放皆能有效克服的特性，希望能有效的增加準確性。
目前在生活照片的人臉分類相關研究當中，尚未有人引進這個做法。
6網絡(social network)，促進人際網絡分析(social network analysis)在多媒體內容的發展，如我們之前所
做的研究[12]。
五、研究方法
為了達到人臉分群，我們必須先做一些前處理。首先我們利用 Adaboost learning 的方法來偵測照
片中的人臉。然而，人臉偵測(face detection)的技術還無法達到百分之百的準確性，某些人臉可能會因
為「拍攝環境」或「人臉角度」等問題而無法被偵測出來。甚至會有一些錯誤的偵測結果，例如照片
中原本不是人臉的區塊，卻被誤判為人臉，我們將這些錯誤的偵測稱為雜訊(noise)。這些雜訊會影響
往後整個系統的結果，所以我們加入一個判斷來將這些雜訊消除，也就是利用眼睛偵測(eye detection)
來判斷被偵測出的物件是否為雜訊。這個方法的出發點是，我們認為一張真正的人臉肯定有眼睛存在，
而假如某個物件無法偵測到任何眼睛，則它就會被系統認為是雜訊。眼睛偵測的技術一樣是利用
Adaboost learning這個方法。
經過了上述的前處理，我們可以得到照片中的人臉，接下來，我們要取出每張人臉的特徵以進一
步進行比對及分析。日常生活照片是在光影變化與人臉姿勢及表情皆未受控制的條件下拍攝，也因此
對自動化分析造成極大的挑戰。為了克服這個問題，我們引進了近年來在電腦視覺領域被廣泛使用的
尺度不變特徵轉換(Scale Invariant Feature Transform)區域特徵，利用它對於縮放、旋轉、視角、亮度皆
能有效克服的特性，來增加系統的準確度。根據尺度不變特徵轉換，每張人臉都可以利用
Difference-of-Gaussian(DoG)偵測器取出特徵點。之後，每個特徵點皆依其周圍其它點的方向特性來描
述，每個特徵點最後都會被表示成一個 128維度的特徵向量。
我們利用尺度不變特徵轉換得到了每張人臉的特徵點，接著，我們要描述兩兩人臉之間特徵點的
比對狀況，且將比對的結果以機率的形式來呈現。其機率代表兩兩人臉的相似程度，這些相似程度最
後可用來進行人臉分群。我們所提出的這套做法跟傳統人臉辨認的方法是完全相異的思維，傳統人臉
辨認的方法通常是每張人臉都會得到一個固定的特徵向量，且根據所得到的特徵向量來跟其它人臉進
行比對。
當兩張人臉之間的特徵點經過比對後，我們會利用視覺語句(visual sentence)來呈現比對的結果。
視覺語句與呈現方式的介紹如下：視覺語句是由一些視覺單字(visual word)所構成，這些單字是經由我
們訓練後所得。我們由訓練資料(training data)當中收集了很多張人臉，每張人臉首先會被平均切成上中
下這三個區域，在意義上分別代表眼睛、鼻子、嘴巴這三個人臉上的區域。當人臉經過尺度不變特徵
轉換作用後，每個區域中會有若干個特徵點被偵測出來且被描述成特徵向量。接下來，我們會進行訓
練資料中兩兩人臉之間特徵點的比對，每張人臉中的三個區域都將會有若干個特徵點與另外一張人臉
上的特徵點比對成功。而每個區域中的所有特徵點將會被聚集成單一特徵點，此聚集後的特徵點即代
表該區域的特徵，我們將每個區域中聚集後的特徵點稱為「聚集特徵點」。接下來我們集合了訓練資料
中兩兩人臉所產生的所有聚集特徵點，並且利用 k-means 演算法將這些聚集特徵點做分群，分完群後
的每一個群心即代表一個視覺單字。因此，每一個視覺單字皆代表著某個人臉區域上的特徵，而所有
的視覺單字會被聚集且產成出一本視覺字典(visual vocabulary)。經由以上的過程，我們可以建立出一
本視覺字典，這本字典裡典藏著一些視覺單字，每個視覺單字皆表示人臉上每個區域的不同特徵。
有了這本視覺字典，根據以上做法，我們可以利用尺度不變特徵轉換對兩張人臉做比對，經由比
對後，每個區域同樣會產生聚集特徵點，這些聚集特徵點會被拿去跟視覺字典中的每個視覺單字做比
對，找出最接近的視覺單字並且將聚集特徵點量化成該視覺單字。因此，兩張人臉的比對結果將會產
生一個由三個視覺單字所組成的視覺語句，其概念如圖 3所示。
8更強健的特徵可以提昇整體效能。關於視覺語言模型，它本身存在著一些改善空間，也有相當多的研
究著墨於此，之後，我們的做法可以套用及比較這些進一步的研究成果。而跟我們的相關的研究中，
有些研究利用人物的衣服或其它脈絡資訊來(context information)來幫助判斷照片中出現的是否為同一
個人，但如何取出這些資訊也是目前大家積極研究的題目，當往後有更好的方法，亦可套用在我們這
個研究上。最後，對於我們的需求，如何選取一個合適的分群演算法也是一個值得研究的方向。
Face
detection
SIFT
matching
Images Face images
Visual sentence
transformation
Visual
sentence
VLM 1
VLM 2
Face
Eye detection
for noise
elimination
Face image
candidates
Prob. 1
Prob. 2
Clustering
圖 4、系統流程圖。
圖 5、部份實驗結果。
六、研究成果
此計劃相關論文發表如下：
 W.-T. Chu, C.-J. Li, and T.-C. Lin, "Travel Video Scene Detection by Search," accepted by
Pacific-Rim Symposium on Image and Video Technology, 2010.
 W.-T. Chu and W.-L. Liu, "Age Classification for Pose Variant and Occluded Faces," accepted by
ACM Multimedia Conference, 2010.
 W.-T. Chu, P.-C. Chuang, and J.-J. Yu, "Video Copy Detection Based on Bag of Trajectory and
Two-Level Approximate Sequence Matching ," Proceedings of the 23th Computer Vision, Graphics,
10
[9] Shi, J., and Malik, J. (2000) Normalized cuts and image segmentation. IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 22, no. 8, pp. 888-905.
[10] Gallagher, A.C., and Chen, T. (2007) Using group prior to identify people in consumer images.
Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition.
[11] Tao, J., and Tan, Y.-P. (2008) Face clustering in videos using constraint propagation. Proceedings of
IEEE Symposium of Circuits and Systems, pp. 3246-3249.
[12] Weng, C.-Y., Chu, W.-T., and Wu, J.-L. (2007) RoleNet: Treat a movie as a small society. Proceedings
of ACM SIGMM International Workshop on Multimedia Information Retrieval, pp. 51-60.
[13] Open Computer Vision Library, http://sourceforge.net/projects/opencvlibrary/
[14] Wilson, P.I., and Fernandez, J. (2006) Facial feature detection using Haar classifiers. Journal of
Computing Sciences in Colleges, 21(4), 127-133.
[15] Turk, M., and Pentland, A. (1991). Eigenfaces for recognition. Journal of Cognitive Neuroscience, 3(1),
71-86.
[16] Bicego, M., Lagorio, A., Grosso, E., and Tistarelli, M. (2006) On the use of SIFT features for face
authentication. Proceedings of Conference on Computer Vision and Pattern Recognition Workshop,
35-41.
[17] Pardeshi, S.A., and Talbar, S.N. (2008) Face recognition using local invariant features. Journal of
Computational Intelligence in Bioinformatics, 1(1), 73-81.
[18] Nahm, F., Perret, A., Amaral, D., and Albright, T. (1997) How do monkeys look at faces? Journal of
Cognitive Neuroscience, 9, 611–623.
[19] The Gallagher Collection Person Dataset, http://amp.ece.cmu.edu/people/andy/GallagherDataset.html
[20] Toews, M. and Arbel, T. (2009) Detection, localization, and sex classification of faces from arbitrary
viewpoints and under occlusion. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.
31, no. 9, 1567-1581.
PSIVT
#****
PSIVT 2010 Submission ****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE. P
#****
2
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
scenic spot, and search each scenic spot’s images from the
web by text query. Sequences of keyframes extracted from
videos and sequences of images retrieved from the web are
then matched to determine their correspondence. After
some post-processing, a shot is claimed to be in the scene of
“Eiffel tower,”for example, if its keyframes correspond to
images retrieved from the text query“Eiffel tower.”
Contributions of this work are summarized as follows:
We transform the idea of “annotation by search”[2]
into “video scene detection by search.”This method
explores cross-media correlation to facilitate media
management.
For approximately matching keyframe sequences and
image sequences, we introduce an algorithm that is
different from similar tasks proposed before. More
flexible and practical solutions can be obtained.
The remainder of this paper is organized as follows.
Section 2 gives an overview of the proposed system
framework. The details of developed components are
described in Section 3, including image search and the
algorithm for finding correspondence between media. We
provide evaluation results in Section 4, followed by the
concluding remarks in Section 5.
2. Overview of system framework
Assume that we have a video captured in journeys and the
text-based schedule corresponding to this journey. The idea
of video scene detection is to explore the correlation
between the video and the travel schedule, and then use the
scene boundaries defined in the schedule to determine scene
boundaries in the video. We transform this problem as a
sequence matching problem, with the processes described
as follows.
Figure 1 shows the proposed system framework. For the
video, we first detect video shots and extract appropriate
number of keyframes for each video shot by the global
k-means algorithm [3]. Feature points such as
scale-invariant feature transform (SIFT) [4] are extracted
from each keyframe, and then quantized into visual words
[5]. Statistics of visual words are collected to present each
keyframe. Finally, the video is transformed into a sequence
of keyframes, in the representation of visual word
histograms, with the temporal order same as visiting.
For the travel schedule, we first extract name entities of
visited scenic spots and then use them to retrieve related
images from image search engines, such as Yahoo!, Google,
and Flickr. Images related to each scenic spot are sorted in
the order of visiting, and are respectively transformed into a
sequence of visual word histograms, with the same
procedure as that for video keyframes.
With the processes described above, we are able to find
the correspondence between two modalities with the same
representation. Because not all scenic spots were captured
in videos and there are many noises in retrieved images, we
conduct approximate sequence matching between them.
With the discovered correspondence, keyframes that are
matched with images retrieved by the same keyword are
claimed to belong to the same video scene.
Video Shot change
detection
Keyframe
extraction
Travel
schedule
Name entity
extraction
Query images
by keyword
Approximate
sequence matching
Video scene
boundary
determination
Visual word
representation
Visual word
representation
Figure 1. The proposed system framework.
3. Video scene detection
3.1. Video preprocessing
We first find shot boundaries based on color histogram
difference between adjacent frames. Each video frame is
described by a 16-bin HSV normalized histogram, in which
8 bins are for hue, and 4 bins are for saturation and value,
respectively.
To efficiently represent each video shot, we adopt the
approach proposed in [6], which automatically determines
the most appropriate number of keyframes based on the
global k-means algorithm [3]. Global k-means is an
incremental deterministic clustering algorithm that
iteratively performs k-means clustering while increasing k
by one at each step. The clustering process proceeds until
clustering results converge. By this algorithm, we overcome
the initialization problem of conventional k-means
algorithm, and adaptively determine appropriate number of
clusters for each shot. Frames in a video shot are clustered
into groups, and the frame closest to the centroid of each
group is selected as a keyframe.
After extracting keyframes, we would like to filter out
keyframes with severe blurred effects, which may damage
the matching process later. Edge characteristics based on a
wavelet-based method [7] are used to detect occurrence and
extent of blur. In addition, illumination information is
examined to detect overexposure or underexposure
conditions. These processes not only reduce consumption
time of determining cross-media correlations, but also
eliminate influence of bad-quality images.
Due to uncontrolled environments in journeys, we have
to represent data by features that resist to significant visual
variations caused by bad photography skills and different
settings of various capture devices. In this work, we
characterize images by bag of visual words. We apply the
difference-of-Gaussian (DoG) detector to detect feature
points in keyframes and photos, and use the SIFT
PSIVT
#****
PSIVT 2010 Submission ****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE. P
#****
4
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
Note that the sequence may contain both negative and
positive real numbers.
Corresponding to the scene , we would like to find an
interval in , , such that
is the maximum-sum segment of
, i.e. is maximal in all cases in
. The values and respectively denotes the
lower and upper bounds for searching the maximum-sum
segment, and as a consequence they are used to constrain
that the maximum-sum segment corresponding to
should appear before that corresponding to if . To
this end, we set the search interval as:
and . (3)
The value is the number of visited scenic spots, i.e. the
number of groups of photos retrieved by keywords. Note
that the search intervals for successive scenic spots are
overlapped. Because travelers may not equally capture
content of the same length for different scenic spots, the
search interval for each scenic spot is designed to be three
times larger than the proportion it corresponds to.
The aforementioned problem can be viewed as a range
maximum-sum segment query (RMSQ) problem [8], which
is able to be solved by a linear time algorithm. In this work,
we apply the algorithm proposed by Chen and Chao [8] to
find correspondence between a subsequence in
and the photos retrieved by a keyword.
Note again that photos in are not temporally ordered.
Therefore, although the keyframes in are temporally
ordered, we cannot adopt the well-known LCS algorithm to
conduct approximate sequence matching. Moreover, the
LCS algorithm finds the global optimal matching between
two sequences. We cannot control the quality of retrieval,
however, and thus many irrelevant photos are in .
Strictly finding the global matching between retrieved
photos and keyframes is not reasonable, and the matching
result may be disturbed by noises.
3.4. Video scene boundary determination
After determining the correspondence, keyframes in the
selected maximum-sum segment are assigned a scene label
according to the corresponding photos. Because boundaries
of scenic spots have been defined in the travel schedule, we
can accordingly estimate scene boundaries in videos. For
example, if we find that the scenic spot corresponds to
some keyframes in the representation of visual word
histograms , these keyframes are then
assigned as in the th scenic spot.
Note that lengths of max-sum segments corresponding to
different scenic spots may be varied. Moreover, because the
search intervals for successive scenic spots are overlapped
(see Equation (3)), the max-sum segments corresponding to
different scenic spots may be overlapped. To handle this
problem, we especially examine max-sum segments for any
two successive scenic spots. Figure 2 illustrates three
possible cases.
Figure 2(a) shows the simplest case, in which two
max-sum segments for successive scenic spots are not
overlapped. Keyframes are assigned as in the
th scenic spot, and keyframes are assigned
as in the -th scenic spot. For those keyframes
in-between and , the first
keyframes are assigned as in the th scene, and the
remaining keyframes are assigned to the -th scene.
If two max-sum segments are overlapped as in Figure
2(b), the keyframes from to are assigned to the th
scene, where . In the case of Figure 2(c), the
keyframes from to are assigned to the th scene,
while to are assigned to the -th scene. In
the case of Figure 2(d), the keyframes from to are
assigned to the th scene, while to are assigned
to the -th scene.
keyframes
photos retrieved for
the ith scenic spot
photos retrieved for
the (i+1)th scenic spot
(a)
(b)
(c)
(d)
Figure 2. Illustrations of different situations in results of
finding max-sum segments.
Table 1. Information of the evaluation dataset.
# visited scenes length # keyframes
Video 1 6 12:58 227
Video 2 4 15:07 153
Video 3 5 08:29 98
Video 4 4 11:03 176
Video 5 3 16:29 136
Video 6 2 05:34 67
Video 7 6 15:18 227
PSIVT
#****
PSIVT 2010 Submission ****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE. P
#****
6
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
Table 2. Performance of video scene detection in terms of
purity.
V1 V2 V3 V4 V5 V6 V7 Avg.
Exp 1 0.66 0.52 0.91 0.48 0.80 0.59 0.62 0.654
Exp 2 0.77 0.50 0.68 0.61 1 0.59 0.41 0.651
Exp 3 0.21 0.62 0.78 0.78 0.49 0.80 0.45 0.59
Retrieval performance of search engines: Although it’s
hard to measure retrieval performance of different search
engines, accuracy of keyword-based image retrieval directly
affect the reliability of correlation determination. Relative
to Exp 1, we obtain better performance from Exp 2 in
Videos 1, 4, and 5, because more accurate photos can be
retrieved from Flickr (due to more accurate tags provided
by users). In these cases, we are able to discover more
accurate correlation between video keyframes and retrieved
photos. On the other hand, retrieval based on user’s tags is
not always good. For the visited scenic spots corresponding
to Videos 3 and 7, more related photos are retrieved from
Google and Yahoo! image search, and we can see their
superior performance.
Popularity of visited scenic spots: If the visited scenic
spots are popular, more related photos can be retrieved and
ranked first by image search engines. We cannot retrieve
enough related photos from Google and Yahoo! for Videos
1, 4, and 5. On the other hand, we can find a few photos that
are highly related to the visited scenic spots from Flickr.
User’s capturing habits: The naïve approach has the
worst performance because no visual correlation is
considered in this method. Actually, its performance
depends on user’s capturing habits. If the traveler equally
captures content in every scenic spot, the naïve approach
may achieve satisfactory performance, which states why we
obtain higher purity values in Videos 3, 4, and 6.
Overall, the Exp 1 provides the best performance, though
the difference between it and Exp 2 is very limited.
Although it may be expected that Flickr would provide
more accurate search results and therefore derive more
accurate correlation, the travel videos captured by amateur
photographers may not contain the most popular buildings
or landmarks that would be returned as the top results of
Flickr.
Name ambiguity would be another problem. The
retrieved results of “Arc of Triumph”and “Arch of
Triumph”may be different. These effects would be more
severe in specific scenic spots that have different nicknames,
or in some languages such as Chinese that may indicate the
same place by many different names.
5. Conclusion
We have presented a video scene detection method that
focuses on travel videos and specially considers
characteristics of information related to journeys. Instead of
simply analyzing visual content in videos, we discover
temporal and visual correlation between travel videos and
their corresponding travel schedules. We search photos
related to scenic spots from image search engines, by the
name entities of visited scenic spots extracted from the
text-based schedules. Correlation between video keyframes
and retrieved photos is then determined by the max-sum
segment algorithm. Because scene boundaries have been
clearly defined in travel schedules, scene boundaries in the
keyframe sequence can be determined by checking the
discovered cross-media correlation. The experimental
results verify the effectiveness of the proposed method. To
the best of our knowledge, this work would be one of the
first studies to exploit general-purpose image search
engines in segmenting user’s own videos.
6. Acknowledgement
This work was partially supported by the National
Science Council of the Republic of China under grants NSC
98-2221-E-194-056.
References
[1] W.-T. Chu, C.-C. Lin, and J.-Y. Yu. Using cross-media
correlation for scene detection in travel videos. In Proc. of
ACM International Conference on Image and Video
Retrieval, 2009.
[2] X.-J. Wang, L. Zhang, F. Jing, and W.-Y. Ma. Annosearch:
image auto-annotation by search. In Proc. of IEEE Computer
Society Conference on Computer Vision and Pattern
Recognition, vol. 2, pp. 1483-1490, 2006.
[3] A. Likas, N. Vlassis, and J.J. Verbeek. The global k-means
clustering algorithm. Pattern Recognition, vol. 36, pp.
451-461, 2003.
[4] D. Lowe. Distinctive image features from scale-invariant
keypoints. International Journal of Computer Vision, 60, 2,
pp. 91-110, 2004.
[5] J. Sivic and A. Zisserman. Efficient video search for objects
in videos. Proceedings of the IEEE, 96, 4, pp. 548-566, 2008.
[6] V.T. Chasanis, A.C. Likas, and N.P. Galatsanos. Scene
detection in videos using shot clustering and sequence
alignment. IEEE Transactions on Multimedia, vol. 11, no. 1,
pp. 89-100, 2009.
[7] H. Tong, M. Li, H.-J. Zhang, and C. Zhang. Blur detection for
digital images using wavelet transform. In Proc. of IEEE
International Conference on Multimedia & Expo, pp. 17-20,
2004.
[8] K.-Y. Chen and K.-M. Chao. On the range maximum-sum
segment query problem. Discrete Applied Mathematics, vol.
155, no. 16, pp. 2043-2052, 2007.
[9] A. Vinciarelli and S. Favre. Broadcast news story
segmentation using social network analysis and hidden
Markov models. In Proc. of ACM Multimedia, pp. 261-264,
2007.
2.1 Probabilistic Model of OCI
In the theory proposed in [6], a face can be described as an OCI
and a set of feature points . A feature point is described
by three parameters . Because the way we describe
feature points is more general than that in other applications, we
would use model parts and feature points interchangeably in this
paper. The value is a binary variable indicating whether the
feature point is present or absent. The value
denotes geometry structure of , including the xy-coordinate,
the orientation of main gradient, and the scale. The value
denotes feature appearance, which is described by a 128-
dimensional orientation histogram (i.e. SIFT descriptor [7]). An
OCI is denoted as . The value is a binary variable
representing presence or absence of the OCI . The value
has the parameters similar to .
The probabilistic relationship between an OCI and a set of
features can be described as
, (1)
where the second equality is derived from the assumption that
features are conditional independent given the OCI. The value
is the likelihood that represents the relationship between
individual features and the OCI. To efficiently compute the value
, we make simplification under the following three
assumptions.
 Assumption 1 - conditional independence between feature
appearance / occurrence and feature geometry given the OCI
: This assumption is made since we consider appearance and
occurrence of a feature provide no additional information
about feature geometry.
. (2)
 Assumption 2 - conditional independence of feature
appearance and the OCI given feature occurrence :
This assumption is made since we consider the OCI provides
no additional information with feature appearance when a
feature occurs. Therefore, we can express
(3)
 Assumption 3 - conditional independence of feature
occurrence and OCI geometry given OCI occurrence
: This assumption makes the previous expression as
(4)
Under these assumptions, the probability is separated
into three parts: The appearance likelihood describes
the probability density function of model part appearance with the
occurrence of a valid model part ( ). Generally, it can be
modeled as a multivariate Gaussian distribution model [6]. When
, it represents all invalid model parts appearance, and can be
modeled as a uniform distribution.
The occurrence probability describes the probability of
occurrence of a model part with the occurrence of the OCI. Four
different situations are considered, i.e. ,
, , and . We count the
number of these situations in training data, and denote them as
. According to these counts, the likelihood
ratio of true vs. false model parts occurrences can be expressed as
. (5)
We call this ratio distinctiveness of a model part, and it is an
important attribute in later processes since it measures reliability
of a model part [8].
The geometry likelihood expresses the relationship
between model part geometry and OCI occurrence and
geometry . The model part geometry can be transformed
into the OCI geometry by a transformation, denoted by
. When , can be modeled as a
multivariate Gaussian distribution. When , and
are undefined, and we treat as a
uniform distribution.
2.2 Model Learning
Figure 2 shows the flowchart of model learning. First, we
manually label the OCI for each training image, and thus
geometry information of each OCI can be calculated. We then
extract scale-invariant features in the presentation of .
With and , the transformation information is
estimated [6].
After feature extraction, we cluster extracted features based on
their appearance and geometry information. For each extracted
feature , we identify two sets and , which consist of
features that are similar to in terms of geometry and
appearance, respectively. The set is defined as
, (6)
where is a transformation converting to . The value
denotes the OCI predicted by , based on the
transformation derived from and . The function
calculates the distance between the predicted
OCI and the truth OCI. The value denotes a set of threshold
involving location, orientation, and scale. A simple example is
illustrated in Figure 3. If and
, then and are
geometrically similar, i.e. .
The set consists of features that have similar appearance to ,
and is defined as
, (7)
where is the Euclidean distance between and
. The threshold changes adaptively according to the ratio
of number of features in and :
. (8)
in the database. Therefore, for each class we randomly select ten
images for training, and test the remaining images. The
experiment results are obtained from 5-fold cross validation.
4.1 Age Classification for Non-occluded Faces
We compare our approach with the method proposed in [11],
which uses Active Appearance Model (AAM) to extract face
appearance features and then classify features based on a
Multilayer perception (MLP) neural network. Note that we
experiment the method in [11] based on the truth face regions
defined in the FG-NET dataset. Thus the following results for
AAM+MLP are actually over optimistic. Figure 5 shows the
overall F-measure of age classification for male and female
images, respectively. Note that the F-measure jointly considers
precision and recall rates and is defined as
. In the male case, the OCI
approach achieves better performances in four classes and nearly
the same performance as AAM+MLP in the class of 20~29 years
old. Overall, the OCI approach respectively achieves 0.26 and
0.20 F-measure for male and female images, which are superior to
0.20 and 0.19 F-measure achieved by AAM+MLP.
The performances in classes 20~29, 30~39, and 40~49 are
relatively lower. The reasons for this may be: 1) few number of
testing images, e.g. only nine images are tested in the class 40~49
in the female case; 2) images are misclassified into neighboring
classes. For example, images in 19 years old may be misclassified
into the class of 20~29. In this experiment, the FG-NET dataset
doesn’t have balance data in different age groups. Moreover, finer
age analysis, such as age estimation, is needed for future studies.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0-9 10-19 20-29 30-39 40-49
OCI
AAM+MLP
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0-9 10-19 20-29 30-39 40-49
OCI
AAM+MLP
F-measure F-measure
Male Female
Figure 5. F-measure of age classification in (a) male and (b)
female images.
4.2 Age Classification for Occluded Faces
We purposely add a black circle on the center of each test image
to evaluation whether the classification methods resist to
occlusion. Figure 6 shows the classification results with occluded
circles with different radii. In the male case, F-measure descends
when the radius of the occluded circle increases. Our OCI-based
approach achieves remarkably better performance than that in
AAM+MLP. In the female case, the F-measure doesn’t have the
clear trend as that in the male case. This may result from that the
number of images in the class 10~19 is significantly larger than
that in other classes, and bad classification performance in this
class dominates the overall statistics. Relative to males, OCI
features specific to females are less evident. This may be due to
females’makeup or highly changeable hair styles. Though the
performance variation in the female case is not inversely
proportional to radius of the occluded circle, our OCI-based
approach consistently obtains better performance than that in
AAM+MLP.
0
0.05
0.1
0.15
0.2
0.25
0.3
0 10 20 40 60 80
radius of the occluded cirle
OCI
AAM+MLP
0
0.05
0.1
0.15
0.2
0.25
0.3
0 10 20 40 60 80
radius of the occluded circle
OCI
AAM+MLP
Male FemaleF-measure F-measure
Figure 6. F-measure of age classification in occluded cases: (a)
male images; (b) female images.
5. CONCLUSION
We have presented face localization and age classification for
faces captured with varying illumination, pose, expression, scale,
and occlusion. Such challenging work is conducted on the basis
of OCI, which describes probabilistic relationship between local
feature points and the invariant vector. The features that are used
to estimate the OCI are demonstrated to be effective in age
classification as well. We show that some features are distinctive
in a specific age class, and the OCI-based approach is more
reliable to localize faces and classify pose variant and occluded
faces. In the future, finer analysis such as age estimation should
be studied. Moreover, age estimation/classification in difference
races could be studied more to facilitate more accurate results.
6. ACKNOWLEDGEMENT
This work was partially supported by the National Science
Council of the Republic of China under grants NSC 98-2221-E-
194-056.
7. REFERENCES
[1] Kwon, Y.H., and da Vitoria Lobo, N. 1999. Age classification from
facial images. Computer Vision and Image Understanding, vol. 74,
no. 1, pp. 1-21.
[2] Geng, X., Zhou Z.-H., and Smith-Miles, K. 2007. Automatic age
estimation based on facial aging patterns. IEEE Trans. on Pattern
Analysis and Machine Intelligence, vol. 29, no. 12, pp. 2234-2240.
[3] Fu, Y., and Huang, T.S. 2008. Human age estimation with
regression on discriminative aging manifold. IEEE Trans. on
Multimedia, vol. 10, no. 4, pp. 578-584.
[4] Ni, B., Song, Z., and Yan, S. 2009. Web image mining towards
universal age estimator. Proceedings of ACM Multimedia, pp. 85-
94.
[5] Ramanathan, N., Chellappa, R., and Biswas, S. 2009.
Computational methods for modeling facial aging: a survey. Journal
of Visual Languages and Computing, vol. 20, pp. 131-144.
[6] Toews, M., and Arbel, T. 2009. Detection, localization and sex
classification of faces from arbitrary viewpoints and under occlusion.
IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 31,
no. 9, pp. 1567-1581.
[7] Lowe, D.G. 2004. Distinctive image features from scale-invariant
keypoints. International Journal on Computer Vision, vol. 60, no. 2,
pp. 91-110.
[8] Dorko, G., and Schmid, C. 2003. Selection of scale-invariant parts
for object class recognition. Proceedings of International Conference
on Computer Vision, pp. 634-640.
[9] The color FERET database, http://face.nist.gov/colorferet/
[10] The FG-NET Aging Database, http://www.fgnet.rsunit.com/
[11] Wilhelm, T., Bohme, H.-J., and Gross, H.-M. 2005. Classification
of face images for gender, age, facial expression, and identity.
Proceedings of International Conference on Artificial Neural
Networks, LNCS 3696, vol. I, pp. 569-574.
may have significant appearance. Due to these characteristics, conventional
image/video analysis techniques cannot be directly applied to travel media.
People often take both digital cameras and digital camcorders in journeys. They
usually capture static objects such as landmark or human faces by cameras and
capture evolution of events such as performance on streets or human’s activities by
camcorders. Even with only one of these devices, digital cameras have been equipped
with video capturing functions, and on the other hand, digital camcorders have the
“photo mode”to facilitate taking high-resolution photos. Therefore, photos and
videos in the same journey often have similar content, and the correlation between
two modalities can be utilized to develop techniques especially for travel media.
In our previous work [1], we investigate content-based correlation between photos
and videos, and develop an effective scene detection module for travel videos. The
essential idea of this work is to solve a harder problem (video scene detection) by first
solving an easier problem (photo scene detection) accompanied with cross-media
correlation. In this paper, we try to further take advantage of cross-media correlation
to facilitate photo summarization and video summarization. We advocate that
summarizing a media can be assisted by other media’s characteristics and the
correlation between them.
Contributions of this paper are summarized as follows.
 We explore cross-media correlation based on features resisting to significant
visual variations and bad quality. Two-level cross media correlations are
investigated to facilitate the targeted tasks.
 We advocate that the correlated video segments influence selection of photos
in photo summaries, and in the opposite way, the correlated photos influence
selection of video segments in video summaries.
The rest of this paper is organized as follows. Section 2 gives literature survey.
Section 3 describes the main idea of this work and the components developed for
determining cross-media correlation. Photo summarization and video summarization
are addressed in Section 4. Section 5 gives experimental results, and Section 6
concludes this paper.
2 Related Works
We briefly review works on home video structuring and editing. Then, studies
especially about highlight generation and summarization are reviewed as well. Gatica-
Perez et al. [2] cluster video shots based on visual similarity, duration, and temporal
adjacency, and therefore find hierarchical structure of videos. On the basis of motion
information, Pan and Ngo [3] decompose videos into snippets, which are then used to
index home videos. For the purpose of automatic editing, temporal structure and
music information are extracted, and subsets of video shots are selected to generate
highlights [4] or MTV-style summaries [5]. Peng et al. [6] further take media
aesthetics and editing theory into account to perform home video skimming.
For summarizing videos, most studies exploit features such as motion and color
variations to estimate the importance of video segments. However, different from
 Keyframe Extraction
For the video, we first segment it into shots based on difference of HSV color
histograms in consecutive video frames. To efficiently represent each video shot, one
or more keyframes are extracted. We adopt the method proposed in [8], which
automatically determines the most appropriate number of keyframes based on an
unsupervised global k-means algorithm [9]. The global k-means algorithm is an
incremental deterministic clustering algorithm that iteratively performs k-means
clustering while increasing k by one at each step. The clustering process ends until the
clustering results converge.
 Keyframe Filtering
Video shots with blurred content often convey less information, and would largely
degrade the performance of correlation determination. To detect blurred keyframe, we
check edge information in different resolutions [10]. The video shots with blurred
keyframes are then put aside from the following processes.
Video shot filtering brings two advantages to the proposed work. First, fewer video
shots (keyframes) are needed to be examined in the matching process described later.
Moreover, this kind of filtering reduces the influence of blurred content, which may
cause false matching between keyframes and photos.
 Visual Word Representation
After the processes above, correlation between photos and videos is determined by
matching photos and keyframes. Image matching is an age-old problem, and is widely
conducted based on color and texture features. However, especially in travel media,
the same place may have significantly different appearance, which may be caused by
viewing angles, large camera motion, and overexposure/underexposure. In addition,
landmarks or buildings with apparent structure are often important clues for image
matching. Therefore, we need features that resist to luminance and viewpoint changes,
and are able to effectively represent local structure.
We extract SIFT (Scale-Invariant Feature Transform) features [11] from photos
and keyframes. The DoG (difference of Gaussian) detector is used to locate feature
points first, and then orientation information around each point is extracted to form
128-dimenional feature descriptors.
SIFT features from a set of training photos and keyframes are clustered by the k-
means algorithm. Feature points belong to the same cluster are claimed to belong to
the same visual word. Before matching photos with keyframes, SIFT features are first
extracted, and each feature point is quantized into one of visual words. The obtained
visual words in photos and keyframes are finally collected as visual word histograms.
Based on this representation, the problem of matching two image sequences has been
transformed into matching two sequences of visual word histograms. According to the
experiments in [1], we present photos and keyframes by 20-bin visual word
histograms.
Conceptually, each SIFT feature point represents texture information around a
small image patch. After clustering, a visual word presents a concept, which may
correspond to corner of building, tip of leaves, and so on. The visual word histogram
presents what concepts compose the image. To discover cross-media correlation, we
would like to find photos and keyframes that have similar concepts.
4 Photo Summarization and Video Summarization
On the basis of photo scenes and video scenes, each of which corresponds to a scenic
spot, we develop summarization modules that consider characteristics of the
correlated media. In a word, how video content evolves affects the selection of photos
in photo summary. On the other hand, how photo being taken affects the selection of
video segments in video summary. This idea is totally different from conventional
approaches, such as attention modeling in photo summarization and motion analysis
in video summarization.
4.1 Local Cross-Media Correlation
Matching based on visual word histogram and the LCS algorithm comes from two
factors: First, the matched photos and keyframes contain objects with similar concepts,
e.g., both images contain large portion of grass and tree, or both images contain
artificial objects. Second, the matched images were taken in the same temporal order,
i.e., a photo at the beginning of a journey unlikely matches with a keyframe at the end
of a journey.
Correlation determined by this process suffices for scene boundary detection.
However, to select important data as summaries, finer cross-media correlation is
needed to define importance value of each photo and keyframe. In this work, we call
the correlation described in Section 3 global cross-media correlation, which describes
matching in terms of visual concepts. In this section, we need further analyze local
cross-media correlation to find matching in terms of objects.
For the photos and keyframes in the same scene, we perform finer matching
between them by the SIFT matching algorithm [11]. Let denote a feature point in
the photo , we calculate the Euclidean distance between and each of the
feature points in the keyframe , and find the feature point that are nearest to
. That is,
. (3)
Similarly, we can find the second nearest feature point to . The feature point
is claimed to match with the point if
, (4)
where the threshold is set as 0.8 according to the suggestion in [11].
For the photo and the keyframe , we claim they contain the same object,
such as a building or a statue, if the number of matched feature points exceeds a
predefined threshold . This threshold can be adjusted dynamically according to the
requirements of users. The experiment section will show the influence of different
thresholds on summarization performance.
We have to emphasize that local cross-media correlation is determined based on
SIFT feature matching rather than visual word histograms. Visual word histograms
describe global distribution of concepts (visual words), while feature points describe
local characteristics that more appropriate whether two images have the same building
or other objects.
pick the most important photo in each scene to the summary. After the first round, we
sort photos according to their corresponding importance values in descending order,
and pick photos sequentially until the desired number is achieved.
According to the definitions above, only photos that are matched with keyframes
have importance values larger than zero. If all photos with importance values larger
than zero are picked but the desired number hasn’t achieved, we define the
importance value of a photo not picked yet by calculating the similarity between
and its temporally closest photo that has nonzero importance value, i.e.,
. (8)
We sort the remaining photos according to these alternative importance values in
descending order, and pick photos sequentially until the desired number is achieved.
4.3 Video Summarization
Similar to photo summarization, we advocate that photo taking characteristics in a
scene affect selection of important video segments in video summaries. Two factors
are also involved with video summary generation. The first factor is the same as that
in photo summarization, i.e., video shots whose content also appears in photos are
more important. Moreover, a video shot in which many keyframes match with photos
is relatively more important. Two factors defining importance values can be
mathematically expressed as follows.
 Factor 1:
The first importance value of a keyframe is defined as
, (9)
where M is the number of keyframes in this dataset. The value is calculated
as
where and are visual word histograms of keyframe and the photo
, respectively.
 Factor 2:
The second importance value of the keyframe is defined as
, (10)
where the value is the sum of visual word histogram similarities between
keyframes at the same shot as and their matched photos. That is,
. (11)
This expression means there are keyframes in the shot containing , and the
notation denotes the photo matched with the keyframe .
The second dataset has the worst performance, because photos and videos in this
dataset don’t have high content correlation as that in others, and the content in them is
involved with large amounts of natural scenes such that local cross-media correlation
based on SIFT matching cannot be effectively obtained. This result conforms that
cross-media correlation really impacts on the proposed photo and video
summarization methods.
We also conduct subjective evaluation by asking content owners to judge
summarization results. They give a score from five to one, in which a larger score
means higher satisfaction. Table 2 shows results of subjective evaluation. Overall,
both video and photo summarization achieves more than 3.7. The worse performance
on the second dataset also reflects in this table.
Table 1. Information of evaluation data.
Dataset # scenes length #kf #photos #kf in manual
sum.
#photos in
manual sum.
S1 6 12:57 227 101 98 48
S2 4 15:07 153 30 32 12
S3 5 8:29 98 44 71 11
S4 5 11:03 176 62 97 21
S5 3 16:29 136 50 103 15
S6 2 5:34 67 23 43 12
S7 6 15:18 227 113 112 32
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 6 7
5 feature points
10 feature points
20 feature points
50 feature points
100 feature points
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 6 7
Precision,recall
Precision,recall
(a) Video summarization (b) Photo summarization
Figure 3. Performance of (a) video summarization and (b) photo summarization.
Table 2. Subjective evaluation on summarization results.
S1 S2 S3 S4 S5 S6 S7 Overall
Video sum. 4 2 4 3 5 4 4 3.7
Photo sum. 5 2 4 3 4 5 4 3.8
6 Conclusion
Two novel ideas have been presented in this paper. Because photos and videos
captured in journeys often contain similar content, we can find correlation between
them based on an approximate sequence matching algorithm. After that, we first solve
an easier problem (photo scene detection), and then solve a harder problem (video
scene detection) by consulting with the correlation. To summarize photos and videos,
Video Copy Detection Based on Bag of Trajectory and Two-Level Approximate
Sequence Matching
Wei-Ta Chu
Department of Computer Science and
Information Engineering,
National Chung Cheng University
Chiayi, Taiwan
wtchu@cs.ccu.edu.tw
Po-Chi Chuang
Department of Computer Science and
Information Engineering,
National Chung Cheng University
Chiayi, Taiwan
b22585699@gmail.com
Jen-Yu Yu
Information and Communication
Research Labs,
Industrial Technology Research Institute
Hsinchu, Taiwan
KenvinYu@itri.org.tw
Abstract—We present a video copy detection system that
detects video copy segments based on the task settings and
dataset in TRECVID 2010. Contributions of this work are two-
fold. First, we extract feature-based trajectories from videos,
and then model trajectories by a bag of word model. This
representation effectively describes information of object
movement, and is robust to various visual transformations.
Second, to find locally optimal matching between the query
video and videos in database, we conduct approximate
sequence matching by first finding the longest common
subsequence, followed by localizing the max-sum segment. We
compare our method with a watershed-based approach and
demonstrate the effectiveness and robustness of the proposed
method.
Keywords-video copy detection; bag of trajectory; the
maximum-sum segment algorithm; approximate sequence
matching
I. INTRODUCTION
Tremendous amounts of videos have been created, edited,
and shared on the internet. Recently, the video sharing site
YouTube [1] has become the second largest search engine
due to its extremely easy video sharing and search
functionalities. Everyone can produce their own videos and
disseminate them quickly via such kind of Web 2.0 platform.
Although users can easily distribute and retrieve videos from
the web, convenience of video sharing deteriorates the
problem of copyright infringement and video counterfeits.
Therefore, videos of similar content flood on the web, and
we can easily obtain many copies with little variations,
which may be illegally edited by unknown users.
Recently, various video copy detection systems [2] have
been designed to detect videos with duplicate content. With
effective video copy detection, video copies can be clustered
together to facilitate efficient browsing. On the other hand,
detecting similar videos forms the foundation of video
retrieval systems. From the perspective of copyright
protection, finding video copies can be used to monitor
dissemination situations and examine whether a copyright
protected video is illegally edited or used. Because the
importance of video copy detection is well recognized
around the research community, a task force has been
established in TRECVID [3] since year 2008, in which a
common video database is built to prompt development of
various techniques.
Different video copies of the same content often have
variations caused by various transformations, such as change
of gamma, strong re-encoding, flip, and pattern insertion. To
robustly detect video copies with these transformations,
many works extract global descriptors like color histogram,
and local descriptors like SIFT features [4], to represent
video frames. Spatiotemporal relationships between video
frames are considered to find duplicate video segments [15].
Motivated by the work proposed in [5], we extract
motion trajectories from video shots and encode trajectories
by a bag of word model. After appropriate weightings on
different bag of words, video shots are transformed into
feature vectors, by which shot-based video matching is
conducted. Different from the dataset used in [5], we follow
the content-based copy detection task in TRECVID 2010, in
which a video query may be significantly shorter than the
targeted videos, and the video query may be concatenated
with irrelevant video segments of arbitrary lengths at the
beginning or at the end. Therefore, we introduce the
maximum-sum segment algorithm to find locally optimal
matching between video sequences.
Contributions of this paper are summarized as follows.
 We extend the idea of bag of word to model motion
trajectories in video shots. We improve the work
proposed in [5] by introducing faster feature
extraction/matching, and investigating the
effectiveness of the method in a more realistic
situation described in TRECVID 2010.
 We applying the maximum-sum algorithm to find
locally optimal matching between two video
sequences. This algorithm is originally designed to
solve constrained sequence matching problem in
bioinformatics. With this algorithm, we are able to
well handle the characteristics of TRECVID 2010
benchmark, and provide superior detection
performance in this challenging dataset.
The rest of this paper is organized as follows. Section II
provides a literature survey and a brief introduction of
content-based copy detection in TRECVID. Section III gives
Shot/Subshot
segmentation
Trajectory
extraction
Bag of trajectory
model Weightings
Orientation
histogram
Video
corpus
Query
video
Feature representation
Approximate sequence matching
Similarity
matrix
construction
Sequences of features
Approximate
sequence
matching
Results of video
copy detection
Figure 1. Flowchart of video copy detection.
IV. BOT REPRESENTATION
Based on examining color histogram difference between
consecutive frames, we first segment a video into video shots.
The HSV (hue, saturation, value) color space is used, and the
HSV color histogram for each frame consists of sixteen
dimensions, with eight dimensions for hue and four
dimensions for saturation and value, respectively.
For each video shot, we would like to further segment it
into smaller units, called subshot, so that object movement in
the same subshot is consistent. To adaptively determine
keyframes for each shot, we adopt the global k-means
algorithm [7] to cluster videos frames and find the most
appropriate number of clusters. The frame that is closest to
its clustering centroid is selected as the keyframe. The video
segment from a keyframe to its temporally adjacent
keyframe forms a subshot. This procedure is illustrated in
Figure 2.
We are able to apply the optical flow algorithm to
estimate object motion in each subshot. However, high
computation cost and tremendous amounts of videos make
this approach infeasible. We instead extract distinct feature
points at the start of each subshot, and then just track these
features to construct motion trajectories. Because the features
points are often located on the corners of objects, how they
move appropriately describe how objects move.
In this work, we extract the SURF [8] (Speech Up Robust
Features) feature points from keyframes, followed by feature
tracking with the KLT [9] (Kanade-Lucas-Tomasi) algorithm.
SURF features can be efficiently detected and are invariant
to scaling, rotation, and some degree of illumination changes
and viewpoint changes. Based on these features, time cost
for motion estimation is largely reduced.
For a subshot, a large number of trajectories with
different lengths (frame number) may be extracted. To
efficiently represent a trajectory, we collect statistics of
moving directions between two consecutive frames. Moving
direction is categorized into five classes and each of which is
denoted by a number from 0 to 4: moving toward up-right
(denoted by 1), moving toward up-left (denoted by 2),
moving toward left-bottom (denoted by 3), moving toward
right-bottom (denoted by 4), and no movement (denoted by
0). We calculate the probability of each moving direction and
form a 5-dimensional vector to describe a trajectory. For
example, if moving directions of a trajectory of four frames
are (4, 1, 2, 2), it is transformed as the vector (0:0.0, 1:0.25,
2:0.5, 3:0.0, 4:0.25), in which (m:n) indicates the probability
of moving toward direction m is n. With this representation,
trajectories of various lengths are described in the same way.
Motivated by the bag of word model that is originally
proposed in natural language processing, we try to view
trajectories as the basic elements to describe videos [5]. We
conceptually map a video into a document, and map
trajectories into visual words for constituting the document
[10]. Given the training corpus, we extract trajectories from
each subshot and transform them into 5-dim orientation
histograms. Feature vectors collected from the training
corpus are then clustered by the k-means algorithm. Feature
vectors that are grouped into the same cluster are claimed to
represent the same bag of trajectory (BoT) word. A BoT
word conceptually represents a set of trajectories that are
similar in moving evolution. A video shot that consists of
many trajectories, therefore, is transformed into a BoT word
histogram , in which denotes
the number of trajectories corresponding to the th BoT word
. The value is the number of different BoT words, i.e.
number of clusters.
Different BoT words have different influences on
describing documents. From the study of natural language
processing, we can measure the importance of a BoT word
by TF-IDF (term frequency–inverse document frequency):
, (1)
where denotes the number of BoT words (number of
trajectories) in the document (video) , denotes the
number of documents that contain , and denotes the
number of document in the training corpus. If occurs
frequently in the document but rarely occurs in other
documents, it’s a more important BoT word to describe the
document .
Shot 1 Shot 2 Shot 3 Shot 4
Subshot 1 Subshot 2 Subshot 3 Subshot 4 Subshot 5
keyframes
Feature
extraction
Trajectory
const.
Feature
extraction
Trajectory
const.
Feature
extraction
Trajectory
const.
…
…
BoT representation and weighting
Figure 2. Procedure of constructing BoT representation.
similarity value, where , , and
.
To find the segment corresponding to the video
copy segment, we first transform the sequence
into a real number sequence
as follows. The mean similarity of this
path is calculated:
. (4)
After mean removing, we obtain
. (5)
Note that the sequence may contain both negative and
positive real numbers.
We would like to find an interval in ,
, such that is the
maximum-sum segment of , i.e. is maximal in all
possible substrings in .
The aforementioned problem can be viewed as a range
maximum-sum segment query (RMSQ) problem [6], which
is able to be solved by a linear time algorithm. In this work,
we apply the algorithm proposed by Chen and Chao [6] to
find the segment in , which conceptually indicates the most
similar segment between the query video and the video in
database, along the current LCS (the current matching
situation).
With the processes above, we can find a maximum-sum
segment for each possible matching (each LCS). Assume
that the maximum-sum segments , , …,
are respectively found from the sequence
matchings backtracking from , , …,
. We determine the best local matching
between the query video and the video in database by finding
the maximum-sum segment that has the largest
average similarity :
, (6)
. (7)
With this decision, we finally find the optimal local
matching between the query video and a video in the
database. We respectively find best local matching between
the query video and all videos in database, and then rank the
retrieval results by average similarity values of
corresponding maximum-sum segments.
Figure 4 shows the overall scheme for video copy
detection. Given the query video that would consist of only a
segment of video copy, we first compare it with every video
in the database and respectively construct a similarity matrix.
Based on a similarity matrix, we find all possible matchings
between two videos, and then find the maximum-sum
segment in each matching. The best local matching
corresponds to the maximum-sum segment that has the
largest average similarity value. After obtaining the best
local matchings between the query video and all videos in
database, the retrieval results are ranked according to
average similarity values.
Query video
Video 1
Video 2
…
Video X
Similarity matrix
construction
Similarity matrix
construction
…
…
…
Similarity matrix
construction
Videos in DB
LCS & Max-
sum seg.
Seg.
evaluation
LCS & Max-
sum seg.
LCS & Max-
sum seg.
Seg.
evaluation
Seg.
evaluation
Ranking…
…
…
…
Ranked
results
Figure 4. Scheme for video copy detection.
Original Gamma change Pattern insertion
Picture in picture Cropping Shift
Figure 5. Samples of query videos with different
transformations.
VI. PERFORMANCE EVALUATION
To evaluate the proposed video copy detection method,
we generate query videos according to the transformations
defined in TRECVID 2010, including change of gamma,
insertions of pattern, picture in picture, cropping, and shift.
We randomly select nine videos from the TRECVID 2010
database, randomly select a segment from each video,
followed by applying a transformation on it. In this work,
Corel Digital Studio [11] is used to implement these
transformations. At the beginning and the end of the
transformed video segment, we concatenate it with irrelevant
video segments (not in the TRECVID dataset) of arbitrary
lengths. By respectively applying five transformations to
nine videos, we finally generate 45 query videos that have
partial video copies. Figure 5 gives examples of query videos
generated from the same video but with different
transformations.
There are totally 3173 videos in the database. Length of
each video ranges from 3.6 minutes to 4.1 minutes, and
totally more than 200 hours of videos are in the database.
A. Performance Variations with Different Numbers of BoT
Words
We evaluate the influence of the number of BoT words
on detection accuracy. Recall that a BoT word represents a
kind of trajectory. In this experiment, we respectively
evaluate detection performance based on feature vectors
derived from 50, 100, 150, 200, and 250 BoT words, with
the 45 query videos. Figure 6 shows the performance
variations. The vertical axis shows the percentage of queries
that successfully retrieve the correct video copy in the top k
representation effectively describes the information of object
movement. We compare videos based on this representation,
and transform video copy detection as an approximate
sequence matching problem. In addition to finding the
longest common subsequence between two sequences, we
further find the locally optimal matching by the maximum-
sum segment algorithm. Different types of queries are
evaluated in experiments, based on the TRECVID 2010
benchmark, and the experimental results demonstrate the
effectiveness and superiority of the proposed method.
In the future, queries with more visual transformations
will be studied, and audio information will be considered as
well to conduct multimodal video copy detection.
ACKNOWLEDGMENT
This work was partially supported by the National
Science Council of ROC under NSC 98-2221-E-194-056.
REFERENCES
[1] YouTube, http://www.youtube.com/
[2] J. Law-To, L. Chen, A. Joly, I. Laptev, O. Buisson, V. Gouet-Brunet,
N. Boujemaa, and F. Stentiford, “Video copy detection: a
comparative study,”Proceedings of ACM International Conference
on Image and Video Retrieval, pp. 371-378, 2007.
[3] TREC Video Retrieval Evaluation, http://trecvid.nist.gov/
[4] D. Lowe. Distinctive image features from scale-invariant keypoints.
International Journal of Computer Vision, 60, 2, pp. 91-110, 2004.
[5] X. Wu, Y. Zhang, Y. Wu, J. Guo, and J. Li,“Invariant visual patterns
for video copy detection,”Proceedings of International Conference
on Pattern Recognition, 2008.
[6] K.-Y. Chen and K.-M. Chao. On the range maximum-sum segment
query problem. Discrete Applied Mathematics, vol. 155, no. 16, pp.
2043-2052, 2007.
[7] A. Likas, N. Vlassis, and J.J. Verbeek, “The global k-means
clustering algorithm,”Pattern Recognition, vol. 36, pp. 451-461, 2003.
[8] H. Bay, A. Ess, T. Tuytelaars, L. Van Gool, “SURF: speeded up
robust features,”Computer Vision and Image Understanding, vol.
110, no. 3, pp. 346-359, 2008.
[9] J. Shi and C. Tomasi,“Good features to track,”Proceedings of IEEE
Conference on Computer Vision and Pattern Recongition, pp. 593-
600, 1994.
[10] J. Sivic and A. Zisserman, “Efficient video search for objects in
videos,”Proceedings of the IEEE, vol. 96, no. 4, pp. 548-566, 2008.
[11] Corel Corporation, http://www.corel.com
[12] J. Law-To, A. Joly, and N. Boujemaa. “Muscle-VCD-2007: a live
benchmark for video copy detection,” http://www-
rocq.inria.fr/imedia/civr-bench, 2007.
[13] C.-Y. Chiu, C.-S. Chen, and L.-F. Chien,“A framework for handling
spatiotemporal variations in video copy detection,” IEEE
Transactions on Circuits and Systems for Video Technology, vol. 18,
no. 3, pp. 412-417, 2008.
[14] M.-C. Yeh and K.-T. Cheng,“Video copy detection by fast sequence
matching,”Proceedings of ACM International Conference on Image
and Video Retrieval, 2009.
[15] M. Douze, H. Jegou, and C. Schmid, “An image-based approach to
video copy detection with spatio-temporal post-filtering,” IEEE
Transactions on Multimedia, vol. 12, no. 4, pp. 257-266, 2010.
學研究課題上的重要進展。此外，我們也可看到每位上台報告者嚴謹且準備完善的態度，這
在許多論文品質良萎不齊的研討會中是看不到的。
本人在 10月 19日晚間抵達北京，緊接著在 20日上午即發表兩篇短篇論文。此次會議的
會場座落於北京的精華地段─長安大街─上的北京飯店。該飯店不僅設備齊全，更已成為在
北京舉辦會議的象徵之一。10 月 20 日早上進行開幕式，隨即進行此次會議的第一個演講。
今年的演講者是微軟亞洲工程院院長張宏江博士，題目是 Multimedia Content Analysis and
Search: New perspectives and approaches。張博士是相關領域的先驅者，在以往多年的研究中
發表大量的論文，也帶動相關領域的蓬勃發展。張博士在此演講中整理過去在多媒體內容分
析上的成就，也提出幾個未來的研究方向，讓與會者受益良多。
在開幕演講之後，本人此次發表的兩篇短篇論文皆被安排於 20 日上午的 10:30 至 12:30
的議程中，題目分別為“Feature Classification for Representative Photo Selection”以及“Visual
Language Model for Face Clustering in Consumer Photos”。這是我第一次要在同一議程內講解
兩篇海報論文，雖然壓力較大但是也相當有趣。兩篇論文分別都有許多與會者有興趣，並且
與我有熱烈的討論。藉由這樣的過程，我們能增加本校以及本研究團隊的知名度，另一方面
也能在討論的過程中聽取他人的建議。20日晚間的歡迎茶會在北京電影學院舉行，它是亞洲
唯一的電影學院，培育出許多藝能界的明星。我們在歡迎會會場參觀了電影製作及道具展示，
算是為本次會議帶來一些不一樣的感覺。
10月 21日早上的議程帶來第二個高潮─Best paper session。四篇由會議技術委員挑選出
來的長篇論文依序發表，在評審後於當天的晚宴中公佈今年的最佳論文。今年的得獎論文是
來自微軟亞洲研究院的作品，Visual query suggestion。他們延伸目前在 Google、Yahoo及其他
著名的搜尋引擎中的 text query suggestion功能，進一步提供視覺化的搜尋輔助以達到更好的
搜尋結果。
10月 21日下午的另一個重頭戲是今年首次舉辦的Multimedia Grand Challenge。由幾家
大公司提出幾個在未來三到五年重要的多媒體產品與研究的挑戰，讓全世界的學者共同提出
解決方案並且在會議當場展示報告。此議程強調實用、互動、以及娛樂性，每篇論文限定三
分鐘發表，接著有兩分鐘進行答辯。評審一樣於當天的晚宴公布最佳的解決方案，並頒發由
這家公司贊助的獎金。
此次我們以“Automatic Summarization of Travel Photos Using Near-Duplication Detection
and Feature Filtering”為題，解決由 CeWe 這家在歐洲頗負盛名的公司所提的“The Next
Generation of Tangible Multimedia Products”挑戰。由於攝影裝置的普及使得數位照片數量快
速地增加。人們習慣利用數位相機記錄他們的日常活動或是旅遊經驗。因此，如何在數位生
活時代有效地管理及瀏覽數位相片變得相當重要。近年來，照片分享網站為人們最常使用的
管理平台，像是Wretch、Flickr和 Picasa。雖然這些網站提供許多功能來幫助使用者進行照片
管理及瀏覽，但龐大的照片數量使得使用者必須花費大量的時間及人力。這個挑戰徵求能自
動對大量個人生活照片進行摘要的技術。我們提出針對旅遊照片，利用近似偵測(near-duplicate)
系統架構來達成上述這個研究目標。
為了過濾特徵點匹配時所產生的錯誤結果，我們提出三個特徵點過濾的方法，包括機率
式潛在語意分析(probability latent semantic analysis)、單一特徵點和區域特徵點過濾。這三種
過濾方法將幫助我們過濾落於自然景觀的特徵點(即雜訊)。對於一組使用者在相同景點所拍
攝的旅遊照片，為了決定任兩張照片內容是否相似，我們建構出一個支持向量機(support vector
無研發成果推廣資料 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
在本計劃的相關研究中, 主持人所指導的林家弘同學在 2009 年獲中國電機工
程學會青年論文獎第二名 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100字為限） 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500字為限） 
1. 學術成就 
本計劃相關成果已整理成論文, 並且分別被2009及2010年的ACM Multimedia Conference
所接受.  
W.-T. Chu and W.-L. Liu, Age Classification for Pose Variant and Occluded Faces, 
ACM MM, 2010. 
W.-T. Chu, Y.-L. Lee, and J.-Y. Yu, Visual Language Model for Face Clustering in 
Consumer Photos, ACM MM, pp. 625-628, 2009. 
2. 技術創新 
2009年我們提出 visual language model 的概念描述人臉配對的狀況, 而非以傳統作法描
述人臉的全域特徵. 此創新方法提供一條人臉分群比對的道路, 並且在國際會議發表場
合中獲得好評.  
2010 年我們利用 OCI 模型來找出不同姿勢的人臉位置, 並利用推算出物件不動量的區域
特徵點來進行人臉年齡分類. 
3. 社會影響 
對人臉分類相關產業有正面幫助, 如動態廣告系統, 相片整理 
