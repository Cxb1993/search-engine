1 Introduction 
Internet use has grown exponentially in 
recent years. The development of information 
technology in the last few decades has led to 
increasing network services and user demands, 
especially for multimedia. However, 
restrictions of network bandwidth and storage 
space limit the size of files transmitted across 
the Internet. In summary, the file size must 
remain small in order to maintain a fast 
response after “click”, making image 
compression into a widely researched topic. 
Hence, data compression is an essential and 
significant process for those digital contents 
owing to the smaller data size and 
transmission. 
 In general, Image compression schemes 
can be classified as lossy or lossless. Lossy 
compression models lower redundancy and 
irrelevancy through a series of transform in 
frequency domain to reconstruct the image 
with the minimum of errors. Redundancy 
reduction eliminates duplication from images, 
and irrelevancy reduction removes visually 
insignificant signals. Lossless compression 
involves compressing an image such that it 
can be rebuilt to the original image 
identically. 
This project present two new codebook design 
algorithms for image compression called 
ET-LBG and LISA. ET-LBG adopts an 
Expanding-Tree technique and based on LBG. 
In the first stage, ET-LBG utilizes an arbitrary 
expanding tree scheme and an effective 
estimation function to calculate the degree of 
distortion of each leaf, and thus may 
dynamically determine the rough 
representative vectors that forms the initial 
codebook. In the final stage, a regular LBG is 
performed to train the initial codebook until a 
stopping criterion is met. The most popular 
VQ scheme is Self-Organizing Map (SOM), 
since it discovers better codebooks than other 
VQ approaches. However, SOM approaches 
have high computation cost, making them 
impractical despite obtaining high-quality 
compressed images. Therefore, this project 
propose a hybrid scheme involving LBG and 
an Improved SOM based on an Asymmetric 
hierarchical architecture called “LISA”. 
 
2 Related Works 
This section describes the merits and 
limitations of various related algorithms. 
2.1 LBG 
LBG, which was proposed by Linde, 
Buzo and Gray in 1980 [1], is the best-known 
VQ technique for image compression, and is 
conceptually similar to the K-means data 
clustering algorithm. LBG builds a codebook 
by continuously reassigning codewords until 
the average distortion is below a specified 
threshold (ε). The simple algorithm of LBG 
means that it can obtain the codebook 
promptly. LBG has two major steps, namely 
data grouping and updating centroid. In 
 2
Square Error (MSE) and Peak Signal-to-Noise 
Ratio (PSNR) are generally used to measure 
quality efficiently, and are represented by the 
following equations: 
             
     …(1) 
or 
                                …(2) 
           …  (3) 
The Bits per Pixel (BPP), refers to the 
number of bit representing each pixel. The 
BPP is defined as Eqn. (4) below, where 
Codewords denotes the codebook size, and 
BlockSize indicates the dimension size of a 
codeword. For instance, the BPPs of 16 
dimensional images consisting of 128 and 
1024 codewords are 0.4375 and 0.625, 
respectively. 
                       
     …(4)  
 
3. The Proposed Algorithms for Image 
Compression 
This section describes the proposed two 
algorithms named ET-LBG and LISA as 
follows. 
3.1. The Proposed ET-LBG Algorithm  
As discussed earlier, although LBG is 
fast, it generates unstable clustering results 
because it randomly selects the initial 
codebooks. Moreover, although SOM and 
HSOM yield excellent result, they are much 
slower than other approaches. Accordingly, 
this work develops an image compression 
algorithm to combine the benefits of different 
algorithms. 
 This section describes the principle of the 
proposed image compression algorithm, 
namely ET-LBG. First, the ET-LBG 
algorithm settles the initial codebook by 
“codewords tree expanding” where the 
codewords are dynamically produced by a 
modified LBG. “Leaf growth estimation” is 
then employed to determine which leaf is 
qualified and need to expand. Finally, 
“codebook convergence” is adopted to 
stabilize the codewords and upgrade the 
quality of each leaf using a regular LBG. The 
detailed implementation of ET-LBG has three 
stages, which are described as follows: 
(1)Codewords tree expanding: In this stage, 
LBG is applied with the codeword that has 
maximum distortion currently to output two 
new clusters as Fig. 1 illustrated. In Fig. 1(a), 
the initial and single codeword ‘A’ represents 
all vectors. In Fig. 1(b), as the previous 
codeword ‘A’ is with maximum distortion, it 
is thus split into two new codewords ‘A’ and 
‘B’. In Fig. 1(c), all existing codewords (e.g. 
the previous codewords ‘A’ and ‘B’) whose 
distortion is the biggest must be divided into 
 4
To verify the efficiency and quality of 
ET-LBG, a series of 30 independent runs were 
undertaken for five well-known and one 
private image, which are illustrated in Fig. 3. 
The algorithm was implemented in Java 
programming language using a personal 
computer with Intel Pentium4 3.2GHz CPU 
and 1GB RAM. Those images were tested at a 
size of 512x512 pixels and grayscale with 8 
bits per pixel (BPP), i.e. 256 levels, and the 
block size is 4x4, namely 16 dimensions per 
vector. Significantly, HSOM cannot process 
an image with a codebook size that is not a 
square number, so its results will be discarded 
in some tests, given as N/A (Not Available). 
The experimental results were listed as Fig. 4.  
 
Fig. 3. To verify the efficiency and quality of 
ET-LBG, a series of 30 independent runs were 
undertaken for five well-known and one 
private image (a) Lena (b) Airplane (c) 
Peppers (d) Boat (e) Sailboat (f) Ejung. 
 
Fig. 4. An instance format to compare PSNR (dB) and time-cost (Second) of reconstructed 
images for the proposed ET-LBG and other algorithms. 
 6
average distortion after completing each 
training process, and employs Eqn. (7) to 
derive the stopping threshold ε representing 
the variation of average distortion. The 
average distortion is determined by Eqn. (8), 
where x_num denotes the number of samples. 
Finally, the training is completed if ε is less 
than a pre-setting value that would be 
discussed in next section. This stopping 
mechanism is similar to that of the LBG 
algorithm, except that has a limit of 500 
epochs. 
)(
)1()(
tD
tDtD           (7) 
numx
xcentroidxd
tD Xx
_
))(,(
=)(
∑
∈ ,      (8)  
x_num: the number of samples            
The third major step solves the problem 
that the fixed neuronal number in each sub 
map may cause the algorithm to fall into a 
local optimum. A dynamic scheme that 
assigns neuronal number within the proportion 
of squared Euclidean distance of each group 
in second level, and with a limit of at least 1, 
is utilized. Eqn. (9) represents the squared 
Euclidean distance, where i denotes the vector 
number; j indicates the dimension number, 
and c is the centroid of the cluster. Eqn. (10) 
computes the number of neurons in each 
sub-group. 

 

n
i
k
j
jij cxd
1 1
2)(           (9) 
sizecodebook
d
d
numberneuron
i
i
i _×=_ ∑ (10) 
Experimental Setting 
The experiment comprising quality of 
compressed images and time cost of the 
presented LISA algorithm was demonstrated. 
The program of each algorithm will be 
conducted in a Java-based program and run on 
a desktop computer with 2GB RAM, an Intel 
T7300 2.0GHz CPU on Microsoft Windows 
XP professional operational system. Six gray 
images with image size of 512×512 were 
employed involving Lena, Airplane, Boat, 
Peppers, Ann and Sweets. Simulation results 
were calculated with the average of 30 rounds. 
For fair comparison, the parameters of HSOM 
approach will be set as in paper [4]. Moreover, 
the stopping threshold (ε) of the LBG was set 
to 0.0001, while the training epoch was set to 
200 for 1D SOM. For the proposed LISA, the 
number of clusters was set to 256 among 
every case, while the stopping thresholds (ε) 
in the first and second levels were set to 
0.00001 and 0.000001, respectively. The 
learning rate in all cases will be set to 1, and 
the numbers of neurons in the first level of 
codebooks with size 128, 256, 512 and 1024 
were set to 40, 60, 120 and 240 respectively.  
 8
 
 
Reference 
 
[1] Linde, Y., Buzo, A., Gray, R.M.: An 
algorithm for vector quantization design. 
IEEE Trans. Commun., vol. COM-28, 
pp.84-95 (1980) 
[2]Kohonen, T.: Self-organizing maps. 
Proceedings of the IEEE, vol. 78, no. 9, pp. 
1464-1480 (1995) 
[3] Madeiro, F., Vilar, R.M., Aguiar, B.G.: A 
Self-Organizing Algorithm for Image 
Compression. IEEE Neural Networks., 
Vol. 28, pp.146 -150 (1998) 
[4] Barbalho, M., Duarte, A., Neto, D., Costa, 
A.F., Netto, L.A.: Hierarchical SOM 
applied to image compression. 
Proceedings of International Joint 
Conference on Neural Networks, 
pp.442-447 (2001) 
 
計畫成果自評 
本研究計畫就研究內容而言，與原計畫可謂
相符，並已達成部分預期目標。本研究計劃
於原計畫書內提出以ET-LBG及LISA影像壓
縮之方法，此研究之成果已分別發表於： 
1. New Advance in Intelligent Decision 
Technology 專書， SCI 199, pp. 
31-40, 2009 及 
2. Lecture Notes in Lecture Notes in 
Computer Science, Vol. 5553, pp. 
476-485, 2009, 接受率約為 16％； 
3. 將 本 方 法 修 改 後 亦 分 別 提 出
LazySOM 與 ELSA 影像壓縮之方法，
此 研 究 之 成 果 已 分 別 發 表 於
Lecture Notes in Lecture Notes in 
Computer Science, Vol. 5414, pp. 
118-129, 2009, 接受率約為 16％及 
4. Lecture Notes in Lecture Notes in 
Artificial Intelligence, Vol. 5579, pp. 
624-633, 2009, 接受率約 30％。 
5. 已將此數方法整理擴充後投稿至
國際期刊審查中。 
6. 此外，與本方法相關之影像壓縮技
術，業已通過屏東科技大學之全額
經費補助獎勵，委請五洲專利事務
所向經濟部專利標準局申請中華
民國（四個）與美國發明專利（三
個）中。 
7. LISA 影像壓縮之技術榮獲本校屏
東科技大學全校教師研發競賽
「2009 年教師研發成果競賽」創新
服務類第一名。 
8. LISA 影像壓縮之技術獲選國科會
2009 台北國際發明展優良作品 
9. 參與南台灣產學合作盛事-技術轉
移成果發表會(工研院南分院主
辦) 
          
 
 10
98年度專題研究計畫研究成果彙整表 
計畫主持人：蔡正發 計畫編號：98-2221-E-020-032- 
計畫名稱：植基於智慧型自我組織特徵圖與擴展樹之新影像壓縮技術設計 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 4 4 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 4 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 4 4 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 3 3 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 4 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
