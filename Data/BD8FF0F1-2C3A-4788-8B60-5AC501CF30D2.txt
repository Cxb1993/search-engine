 transcoder???????????????????????????? DVC?????
??? low-complexity video encoder? low-complexity video decoder???????????
encoder????????????? robust media hash????????????????
??? (??????????)??????????? (quantization) ???? (entropy 
encoding) ???? decoder?Decoder????????????????????????
?? motion estimation????????? motion??????? DVC?????????
?? DVC??????????????????????????????? 
??????DVC?????????2006 IEEE International Conference on Image 
Processing (ICIP2006) [1] ?2006 IEEE International Workshop on Multimedia Signal Processing 
(MMSP2006) [2]???????????? 
 
[1] Li-Wei Kang and Chun-Shien Lu, “Wyner-Ziv video coding with coding mode-aided motion 
compensation,” in Proc. of 2006 IEEE Int. Conf. on Image Processing, Atlanta, GA, USA, Oct. 
2006, pp. 237-240. 
[2] Li-Wei Kang and Chun-Shien Lu, “Low-complexity Wyner-Ziv video coding based on robust 
media hashing,” in Proc. of 2006 IEEE Int. Workshop on Multimedia Signal Processing, 
Victoria, BC, Canada, Oct. 2006, pp. 267-272. 
 2
Wyner-Ziv bits from the encoder via a feedback channel 
optionally dependent on current decoding efficiency. 
More specifically, most existing DVC schemes [3]-[4], 
[7]-[18] modeled Wyner-Ziv video coding as a channel coding 
problem. The statistical dependence between two correlated 
sources W and Y is modeled as a virtual correlation channel 
analogous to binary symmetric channel (BSC) or additive white 
Gaussian noise (AWGN) channel. The side information Y is 
viewed as a noisy version of the source W. At the encoder, the 
compression of W can be achieved by transmitting only parity 
bits derived from error correcting codes (ECC) (e.g., turbo 
codes [7]-[10], [13]-[18]). Here, the parity bits form the 
Wyner-Ziv bits. The size of the transmitted Wyner-Ziv bits is 
usually smaller than that of the original source W. The decoder 
concatenates the received parity bits with the side information 
Y and performs error correction decoding to correct some 
“errors” in Y, i.e., the noisy version of the source W, for the 
reconstruction of W. The realization of such a channel coding 
approach for Wyner-Ziv video coding can be divided into two 
categories, i.e., pixel-domain Wyner-Ziv video coding and 
transform-domain Wyner-Ziv video coding. They are briefly 
described in the following two subsections. 
 
Key frames
K’
Decoded Wyner-Ziv 
frames
Wyner-Ziv 
frames
W
K Conventional 
Intra encoder
Conventional 
Intra decoder
Interframe 
Decoder
Intraframe 
Encoder
Distributed 
encoder
Decoded 
Key frames
Distributed 
decoder
Side information 
generator
Extra information 
generator
W’
Extra 
information
Wyner-Ziv 
bits
Key frame 
bits
Side 
information Y
Feedback 
channel
 
Fig. 1.  General framework of a Wyner-Ziv video codec. 
 
B. Pixel-domain Wyner-Ziv Video Coding 
Aaron, Setton, and Girod [7] proposed the first pixel-domain 
Wyner-Ziv video codec. At the encoder, an input video 
sequence is divided into key frames and Wyner-Ziv frames. 
Each key frame (K) is encoded using a conventional intraframe 
encoder (e.g., H.263 intraframe coding) while each Wyner-Ziv 
frame (W) is encoded using a Wyner-Ziv intraframe encoder to 
generate Wyner-Ziv bits. For each Wyner-Ziv frame, W, each 
pixel value is quantized using a uniform scalar quantizer with 
2M intervals to form the quantized symbol stream q. Then, q is 
fed into a turbo encoder to form the parity bits (Wyner-Ziv bits) 
stored in a buffer. The buffer transmits a subset of the parity 
bits to the decoder upon request. 
At the decoder, each key frame is decoded using a 
conventional intraframe decoder. For each Wyner-Ziv frame, 
the decoder generates the side information (Y) by interpolation 
or extrapolation of previously decoded key frames and, 
possibly, previously decoded Wyner-Ziv frames. To exploit the 
side information, the decoder assumes a statistical dependency 
model between W and Y. The turbo decoder combines the side 
information Y and the received parity bits to recover the symbol 
stream q’. If the decoder cannot reliably decode the original 
symbols, it requests additional parity bits from the encoder 
buffer through feedback until an acceptable probability of 
symbol error is reached. The decoder usually needs to request r 
? M bits to decode which of the 2M bins a pixel belongs to and, 
hence, compression is achieved. After decoding q’, the decoder 
reconstructs the Wyner-Ziv frame W’ as follows. If the side 
information Y is within the reconstructed bin, the reconstructed 
pixel will take a value very close to the side information. 
Otherwise, the function clips the reconstruction towards the 
boundary of the bin closest to Y. In addition, based on this video 
coding paradigm [7], several pixel-domain Wyner-Ziv video 
coding schemes were similarly proposed [8]-[10], [17]. 
C. Transform-domain Wyner-Ziv Video Coding 
Puri and Ramchandran [11]-[12] proposed the first 
transform-domain Wyner-Ziv video codec, called 
“Power-efficient, Robust, hIgh-compression, Syndrome-based 
Multimedia coding (PRISM).” In PRISM, a key frame is 
encoded and decoded using the H.263 intraframe codec. A 
Wyner-Ziv frame is transformed using block DCT followed by 
uniform scalar quantization. For each block, the 
lower-frequency coefficients are compressed using a syndrome 
encoder. The higher-frequency coefficients are conventionally 
entropy-encoded. The encoder also sends a cyclic redundancy 
check (CRC) of the quantized coefficients. The decoder 
performs motion compensation to generate side information. 
The syndrome decoder combines the side information and the 
syndrome bits to recover the symbol stream. Finally, the 
decoder can reconstruct the Wyner-Ziv frame based on the 
symbol stream and the side information, followed by inverse 
DCT. On the other hand, Aaron, Rane, Setton, and Girod [13] 
proposed a transform-domain Wyner-Ziv video codec, 
modified from their pixel-domain codec in [7]. Similarly, based 
on this video coding paradigm [13], several transform-domain 
Wyner-Ziv video coding schemes were also proposed [14]-[16], 
[18]. 
D. Overview of the Proposed Wyner-Ziv Video Coding 
Schemes 
In this paper, firstly, a DCT-based Wyner-Ziv video codec 
with coding mode-aided motion compensation at the decoder is 
proposed, denoted by “ProposedDVC1.” The major 
characteristics include: (a) for each block, a large amount of 
candidate blocks are evaluated based on some criteria derived 
from Reed-Solomon (RS) decoding and best neighborhood 
matching to find the best candidate block as the side 
information; (b) ECC decoding is applied to participate in 
generating side information; (c) no feedback channel is 
required. In most existing Wyner-Ziv video codecs, the 
decoder generates side information firstly without considering 
ECC decoding, and then concatenates the parity bits and the 
side information to perform ECC decoding. If the decoding 
result is unacceptable, the decoder will request more parity bits 
via the feedback channel. Hence, we observe that (a) the 
generated side information may be not the best one for ECC 
 4
mode. If T1 < di ≤ T2, the coding mode of bi is declared to be 
non-skip with RS coding mode. Otherwise, the coding mode of 
bi is declared to be non-skip without RS coding mode. Here, T1 
and T2 are two predefined positive thresholds and T1 < T2. The 
extra overhead in the encoder is a buffer with the size the same 
as that of an uncompressed frame. Here, the coding mode 
information for each Wyner-Ziv frame will be encoded using 
the run-length coding followed by the entropy coding to form 
the extra information bits. 
 
Key frame
Decoded 
Key frame
K’
Decoded Wyner-Ziv 
frame
Wyner-Ziv frame
Xr
K H.264 intra encoder H.264 intra decoder
IDCT
DCT
Interframe 
Decoder
Intraframe 
Encoder
Scalar 
quantization
RS 
encoding
Coding mode decision
Coding mode information
Decoder motion compensation:
(1) DCT symbols comparisons,
(2) RS decoding,
(3) Best neighborhood 
matching
Reconstruction
Side 
information
(Yr)
X’r
qr
q’r
Key frame bits
Wyner-Ziv bits
Wyner-Ziv 
bits
W
W’
Is current block 
with skip mode?
No
Yes
Run-length 
coding and 
entropy coding
Extra information 
bits
 
Fig. 2.  The proposed Wyner-Ziv video codec (ProposedDVC1). 
 
After performing block coding mode decision, each block in 
a Wyner-Ziv frame can now be encoded. For a block with skip 
mode, no data will be encoded. For a block with non-skip mode, 
similar to [13], a block DCT will be performed followed by a 
scalar quantization to obtain a symbol block containing N×N 
symbols. The four employed quantizers are shown in Fig. 3 
with N = 4. For example, if the quantizer shown in Fig. 3(a) is 
used, the DC value will be quantized to a symbol with at most 
64 levels (denoted by 6 bits). 
 
64 32 16 4
32 16 4 4
16 4 4 0
4 4 0 0
32 16 8 4
16 8 4 4
8 4 4 0
4 4 0 0
32 8 0 0
8 0 0 0
0 0 0 0
0 0 0 0
16 8 0 0
8 0 0 0
0 0 0 0
0 0 0 0  
(a)                       (b)                      (c)                      (d) 
Fig. 3.  Four scalar quantizers used in ProposedDVC1. 
 
For a block with non-skip with RS coding mode, the three 
most important symbols corresponding to the three lowest 
frequency DCT coefficients will be encoded directly. The 
remaining symbols will be encoded using (u, v) RS codes with z 
bits for each [21] to generate parity symbols, where v is the 
number of DCT subbands having non-zero quantization levels. 
Only RS parity symbols will be encoded. For example, if a 4×4 
DCT block is quantized using the quantizer shown in Fig. 3(a), 
the three most important symbols will be encoded directly with 
6 bits, 5 bits, and 5 bits, respectively. The remaining 10 
non-zero symbols denoted by 4 bits (z = 4) for each can be 
encoded using (14, 10) RS code to generate 4 parity symbols. 
That is, there are, in total, 4×4 = 16 parity bits required. In Fig. 
3(a), although some symbols with a quantization level of 4 can 
be denoted by 2 bits, in order to use a common RS code to 
encode all the remaining 10 non-zero symbols, each of them 
can be denoted by 4 bits by simply adding two zero bits. In this 
case, a block is totally encoded with 32 (= 6 + 5 + 5 + 16) bits. 
On the other hand, for a block with non-skip without RS coding 
mode, all the symbols will be encoded directly. Usually, this 
kind of block is rare. Finally, the resultant encoded symbols for 
all the blocks with non-skip mode in a Wyner-Ziv frame 
constitute the Wyner-Ziv bits. The key frame bits, the 
Wyner-Ziv bits, and the extra information bits will be 
transmitted to the decoder. 
C. Proposed Wyner-Ziv Video Decoder in ProposedDVC1 
At the decoder, each key frame will be decoded using the 
H.264/AVC intraframe decoder. For a Wyner-Ziv frame, the 
coding mode information will be decoded first, and then all the 
blocks with skip mode will be reconstructed by assigning the 
co-located blocks of the previous reconstructed frame. On the 
other hand, for each block with non-skip mode, the proposed 
coding mode-aided motion compensation scheme is employed 
to find the corresponding side information. First, the search 
windows in the previous reconstructed frames are formed so 
that each block in the search windows will be a candidate side 
information block. Then, similar to the encoder operations, the 
N×N DCT followed by the scalar quantization will be applied to 
each candidate side information block. In addition, the 
reconstructed 8-connected neighboring blocks for each 
candidate side information block will be also extracted. The 
decoding of blocks with non-skip mode can be divided into two 
cases discussed below. 
Case 1: For a block, bi, with non-skip with RS coding mode, 
each candidate block, ck, in the search window(s) will be 
evaluated. The best candidate block satisfying the following 
three criteria will be selected to be the side information for bi. 
(a) The difference between the three most important symbols 
of bi and those of ck should be minimized. Let (DCTbi)j and 
(DCTck)j, j = 1, 2, 3, be the representative values for the 
quantization levels which the three most important symbols 
belong to, respectively. The difference between the three most 
important symbols of bi and those of ck, DCT_Diffi(bi, ck), is 
defined as 
 
( ) ( ) ( )∑
=
−=
3
1
,_
j
jckjbikii DCTDCTcbDiffDCT .     (3) 
 
 (b) The number of incorrect RS-decoded symbols (denoted 
by NRSi(bi, ck)), returned from the RS-decoder based on the 
parity symbols of bi and the symbols (except the three most 
important symbols) of ck should be minimized. For a (u, v) RS 
code, the number of allowable maximum symbol errors is (u – v) 
/ 2. If the RS code cannot correct all the error symbols, i.e., the 
number of incorrect RS-decoded symbols exceeds the 
maximum allowed, NRSi(bi, ck) is set to (u – v) / 2 + 1. 
 (c) The difference between the 8-conncected neighboring 
blocks of bi and those of ck should be minimized. Let bij and ckj 
 6
 
yx
yx MVMVMSNR 22
max,
2
max,
2
10log10 ∆+∆
+= ,                   (9) 
 
where (MVmax, x, MVmax, y) is the maximum possible motion 
vector. 
 Here, for a video sequence, its true motion vectors are 
obtained by means of a block-based exhaustive search. On the 
other hand, the two kinds of estimated motion vectors are, 
respectively, obtained performing H.264/AVC full search at 
the encoder with quarter-pixel accuracy and the decoder motion 
estimation in ProposedDVC1. Due to the fact that motion 
estimation in ProposedDVC1 is with integer-pixel accuracy, 
for simplicity, the true motion vectors also use integer-pixel 
accuracy. For H.264/AVC interframe coding with quarter-pixel 
accuracy, only the motion vectors with integer-pixel accuracy 
are extracted to be the estimated motion vectors. Then, for a 
sequence, the MSNR values for all frames based on the two 
kinds of estimated motion vectors are calculated and averaged 
to obtain the MSNR value for the sequence. The MSNR results 
for the Carphone, Hall monitor, and Salesman sequences with 
different GOP sizes (GOPSize) are, respectively, shown in Figs. 
5-7. 
 
3.0
3.1
3.2
3.3
3.4
3.5
3.6
3.7
3.8
3.9
4.0
45 60 75 90 105 120 135 150 Bitrate (kbps)
MSNR (dB) H.264 Inter (GOP=2) ProposedDVC1 (GOP=2)
 
Fig. 5.  MSNR performance for the Carphone sequence with GOPSize = 2. 
 
3.2
3.3
3.4
3.5
3.6
3.7
45 60 75 90 105 120 135 Bitrate (kbps)
MSNR (dB) H.264 Inter (GOP=4) ProposedDVC1 (GOP=4)
 
Fig. 6.  MSNR performance for the Hall monitor sequence with GOPSize = 4. 
 
4.8
4.9
5.0
5.1
5.2
30 45 60 75 90 105 120 Bitrate (kbps)
MSNR (dB) H.264 Inter (GOP=8) ProposedDVC1 (GOP=8)
 
Fig. 7.  MSNR performance for the Salesman sequence with GOPSize = 8. 
 
It can be observed from Figs. 5-7 that the MSNR 
performance gaps between ProposedDVC1 and H.264/AVC 
interframe coding are from 0.1 to 0.4 dB. For fast-motion 
sequences (e.g., Carphone), higher bitrates can allow more 
blocks with non-skip mode to be used so as to produce larger 
MSNR value in ProposedDVC1. Therefore, the MSNR gap 
between H.264/AVC and ProposedDVC1 is smaller for higher 
bitrates. For slow/middle-motion sequences (e.g., Hall monitor 
and Salesman), most blocks are with zero or small motion 
vectors, and too many blocks with non-skip mode are 
meaningless. Hence, in cases with higher bitrates, more bits 
cannot be efficiently used for motion estimation, and MSNR 
gaps will be larger. On the other hand, for smaller GOP sizes 
(e.g., GOPSize = 2), the motion estimation for H.264/AVC 
interframe coding is less efficient than that with larger GOP 
sizes. Hence, the MSNR gaps can become smaller when the 
bitrate increases. On the contrary, for larger GOP sizes (e.g., 
GOPSize = 8), the motion estimation for H.264/AVC 
interframe coding is efficient. Hence, the MSNR gaps will 
become larger when the bitrate increases. Based on the analysis 
for the motion vector accuracy, the performance for 
ProposedDVC1 can be evaluated. The complete evaluation of 
rate-distortion (RD) performance is in Section IV. 
III. PROPOSED DISTRIBUTED VIDEO CODING BASED ON 
ROBUST MEDIA HASHING 
In this section, a new Wyner-Ziv video codec based on 
robust media hashing is described, denoted by 
“ProposedDVC2.” In ProposedDVC2, no motion-compensated 
interpolation/extrapolation is performed at both the encoder 
and the decoder, no ECC is applied, and no feedback channel is 
required. In particular, the key characteristic is that both the 
encoder and the decoder are with low-complexity. 
A. Problem Formulation 
In view of the fact that image hashing [23]-[24] is able to 
capture the essence of an image (or a video frame) while 
reducing storage requirement, ProposedDVC2 is presented by 
exploiting hash modification to achieve Wyner-Ziv frame 
recovery without needing motion estimation. This problem is 
associated with the proper selection of the length of an image 
(or frame) hash under the constraint of trade-off between visual 
quality and coding efficiency. The problem can be defined as 
follows. 
Problem 2 (Media hashing for Wyner-Ziv frame 
reconstruction): For a Wyner-Ziv frame, W, its most 
significant features, extracted by comparing the hash values of 
W and those of its reference frame I, should be properly 
selected such that 
 
PSNR(W, Ŵ) ≥ desired PSNR value, and              (10) 
PSNR(W, Ŵ) >> PSNR(I, Ŵ),                                (11) 
 
where PSNR denotes the peak signal to noise ratio (PSNR), and 
Ŵ is an estimate of W, obtained by modifying I using the 
significant features of W. 
 To solve this problem, our robust image hashing scheme, 
 8
At the encoder, for each frame Ii, the luminance component 
of the central nn 22 ×  square area within the frame is extracted 
as Fi. For example, for a QCIF frame Ii of size 176×144, the 
luminance component of the central 128×128 (n = 7) square 
area is extracted as Fi. The remaining area within Ii is denoted 
by Bi. In addition, the chrominance components of the whole 
frame Ii are included in Bi. Usually, Fi and Bi correspond to the 
foreground and the background of a frame, respectively, as 
shown in Fig. 9. 
For convenience, some notations are defined as follows. The 
wavelet image of Fi is denoted by Xi. The reconstructed Fi at the 
decoder is denoted by
iFˆ . The wavelet image of iFˆ  is denoted 
by Yi. The SDS of Fi is denoted by Si. The length of Si is 
denoted by Li. The reconstructed Bi at the decoder is denoted 
by
iBˆ . 
Each key frame Ii is encoded using the H.264/AVC 
intraframe encoder. An additional operation for a key frame is 
to extract the SDS Si from its foreground Fi of size nn 22 × . 
During the process of extracting Si, the DWT is applied to 
transform Fi to the wavelet image Xi. The SDS Si for Fi with 
length Li is extracted from Xi and stored in the encoder buffer 
for encoding the previous/next Wyner-Ziv frame. The outputs 
of the H.264/AVC intraframe encoder form the key frame bits. 
For each Wyner-Ziv frame Ii, its foreground Fi of size 
nn 22 ×  is extracted and wavelet transformed to be Xi. Then, the 
similarity between Fi and its foreground reference frame Ri is 
evaluated, where Ri can be available from the encoder buffer. 
Here, the reference frame Ri for Fi is determined as follows. (a) 
If the immediate previous frame (Ii-1) of Ii is a key frame, the 
reference frame for Fi is set to the foreground (Fi-1) of Ii-1. That 
is, Ri = Fi-1. (b) If the immediate next frame (Ii+1) of Ii is a key 
frame, the reference frame for Fi is set to the foreground (Fi+1) 
of Ii+1. That is, Ri = Fi+1. (c) If Ii is between two Wyner-Ziv 
frames, the reference frame for Fi is set to the frame simply 
interpolated (averaged) by the foregrounds of the two nearest 
key frames. 
 
Fi Bi
Ii
 
Fig. 9.  The decomposition of a QCIF video frame Ii into a foreground 
component (Fi) and a background component (Bi). 
 
 For example, consider a video sequence, I0, I1, I2, I3, I4, …, 
with GOPSize = 4, i.e., I0, I4, I8, … are key frames while the 
others are Wyner-Ziv frames. Based on the above definitions 
for reference frames, the reference frame R1 for F1 is F0, the 
reference frame R2 for F2 is the frame averaged by F0 and F4, 
the reference frame R3 for F3 is F4, and so on. The major 
principle is that the reference frame for a Wyner-Ziv frame is 
derived from neighboring key frames, instead of Wyner-Ziv 
frames. That is, key frames are always intra-encoded with 
higher quality and intra-decoded. They are more suitable to be 
reference frames for Wyner-Ziv frames. 
 In this study, the PSNR value, PSNR(Fi, Ri), is used to 
evaluate the similarity between Fi and Ri. If PSNR(Fi, Ri) < Ta, 
the SDS length Li of Fi is set to L1. If Ta ≤ PSNR(Fi, Ri) < Tb, Li 
is set to L2. If PSNR(Fi, Ri) ≥ Tb, Li is set to L3. The relationship 
among L1, L2, and L3, and the selection of Ta and Tb will be 
described later. Finally, the SDS Si for Fi with length Li is 
extracted from Xi (the wavelet image of Fi). 
 The remaining work for encoding Fi is to extract the most 
significant wavelet coefficients in Xi by comparing Si and SRi, 
which is available from the encoder buffer and extracted from 
the reference frame Ri. 
For Si, each signature symbol symi(p, c) (= +1, -1, +2, -2, or 0) 
will be compared with the corresponding symbol symRi(p, c) in 
SRi with the same position for the parent node. If symi(p, c) ≠ 
symRi(p, c), the corresponding parent-4 children pair of symi(p, 
c) in Si is determined to be significant. If symi(p, c) = symRi(p, c) 
≠ 0, then their corresponding maximum magnitude difference 
(Eq. (13)) will be compared. If the difference of their maximum 
magnitude differences is larger than a threshold Di, then the 
parent-4 children pair corresponding to symi(p, c) is determined 
to be significant; otherwise, it is insignificant. That is, we 
intend to efficiently extract the wavelet coefficients from the 
wavelet domain Xi of Fi, that are significantly different from the 
corresponding ones from XRi of Ri. For each significant 
parent-4 children pair, the position of the parent node and their 
corresponding five quantized wavelet coefficients form the 
Wyner-Ziv bits. Here, for a nn 22 × square area Fi, it takes 
log2( nn 22 × ) bits to denote a parent-node position. Similar to 
[25], a wavelet coefficient, w, is quantized as 
 
 5.0ˆ += sQww ,                                (15) 
 
where Qs denotes the quantization parameter for the wavelet 
scale s that w belongs in, and    denotes the floor operation. 
Then all the quantized significant wavelet coefficients are 
entropy encoded. Finally, both the key frame bits and the 
Wyner-Ziv bits will be transmitted to the decoder. An 
illustrated example for encoding with GOPSize = 4 in 
ProposedDVC2 is shown in Fig. 10. 
 Similar to the selection for Li, Di is selected as follows. If 
PSNR(Fi, Ri) < Ta, Di is set to D1. If Ta ≤ PSNR(Fi, Ri) < Tb, Di is 
set to D2. If PSNR(Fi, Ri) ≥ Tb, Di is set to D3. Obviously, the 
larger PSNR(Fi, Ri) is, the more similar Fi and Ri are. When Fi 
and Ri are similar, significant wavelet coefficients in Fi 
different from the corresponding ones in Ri that should be 
extracted are few, implying that smaller Li and larger Di should 
be used, and vice versa. Here, L1, L2, L3, D1, D2, and D3 can be 
adjusted to generate different amounts of the Wyner-Ziv bits 
 10
transforming iRˆ . Then, Yi is reconstructed by filling the 
decoded significant wavelet coefficients into the wavelet image 
YRi. Finally, Yi is directly used to reconstruct Fi by inverse 
wavelet transforming Yi to iFˆ . On the other hand, if Fi is 
intra-encoded at the encoder, it is decoded using the 
H.264/AVC intraframe decoder as iFˆ . For reconstruction of 
the background component, if Bi is skipped at the encoder, it is 
reconstructed by copying the corresponding regions in the 
immediate previous reconstructed frame as iBˆ . Otherwise, it is 
decoded using the H.264/AVC intraframe decoder as iBˆ . 
Finally iFˆ  and iBˆ  are combined to reconstruct Ii as Îi. 
5) Computational Complexity of ProposedDVC2: 
The computational complexity of ProposedDVC2 is 
dominated by those of the DWT, SDS extraction, and entropy 
encoding. The heaviest task in the SDS extraction is the sorting 
operation, which can be efficiently performed by using the 
quick sort algorithm. Without performing motion estimation, 
the computational complexity of the proposed encoder should 
be in the similar order of that of a conventional intraframe 
encoder, consisting of the DCT and entropy coding. On the 
other hand, the computational complexity of the proposed 
Wyner-Ziv video decoder is dominated by those of the inverse 
DWT and entropy decoding, which is in the order of a 
conventional intraframe decoder. Hence, unlike most existing 
Wyner-Ziv video codecs, ProposedDVC2 is with light encoder 
and light decoder. However, the decoder will induce some 
delay due to the fact that the reference frames for some 
Wyner-Ziv frames should be derived from the next key frame 
of the next GOP. For example, the second Wyner-Ziv frame in 
a GOP with GOPSize = 4 will have 2-frame delays. The third 
Wyner-Ziv frame in a GOP with GOPSize = 4 will have 
1-frame delay. The maximum possible decoding delay (DD) for 
GOPSize ≥ 3 is GOPSize – 2, i.e., 0 ≤ DD ≤ GOPSize – 2 for 
GOPSize ≥ 3. When 1 ≤ GOPSize ≤ 2, there is no decoding 
delay (DD = 0). 
IV. SIMULATION RESULTS 
A. Experimental Setting 
Several QCIF video sequences formatted with different GOP 
sizes (GOPSize = 2, 4, and 8), frame rate (10 frames per second 
(fps)), and encoded with several different bitrates were used to 
evaluate the two proposed Wyner-Ziv video codecs 
(ProposedDVC1 and ProposedDVC2). In ProposedDVC1, 4×4 
DCT (N = 4) was used. Here, the bitrates were adjusted by 
changing the quantization parameters (QPs) of the H.264/AVC 
encoder for key frames, changing the quantizers for Wyner-Ziv 
frames, and changing the number of blocks for each coding 
mode (i.e., changing T1 and T2). The number of reference 
frames for motion compensation at the decoder was set to 1. 
In ProposedDVC2, the central square area with size 
128×128 (n = 7) was extracted for each frame. The SDS for 
each 128×128 square area was extracted by setting the size of 
lowest frequency subband in the wavelet domain to 16×16. 
Here, the bitrates were adjusted by changing the QPs of the 
H.264/AVC encoder for key frames and the parameters, L1, L2, 
L3, D1, D2, D3, Qs, TF, and TG for Wyner-Ziv frames. A 
guideline for empirically adjusting L1, L2, L3, D1, D2, and D3 
under the constraints, L1 ≥ L2 ≥ L3 and D1 ≤ D2 ≤ D3, was found 
to be 4096 ≥ L1 ≥ L2 ≥ L3 ≥ 512 and 30 ≤ D1 ≤ D2 ≤ D3 ≤ 240. 
For example, if the QP of the H.264/AVC intraframe encoder is 
set to 33, then L1 = 2048, L2 = 1024, L3 = 1024, D1 = 20, D2 = 20, 
D3 = 25, Q0 = 20, Q1 = 20, Q2 = 30, Q3 = 30, TF = 29 dB, and TG 
= 28 dB can be set to yield the bitrate = 62.39 kbps and PSNR = 
34.88 dB for the Hall Monitor sequence. Here, all the 
parameters are adjusted in order to achieve the desired bitrates. 
One can adjust the QP of the H.264/AVC intraframe encoder to 
approximately achieve the desired bitrate, and then, adjust L1, 
L2, L3, D1, D2, D3, and Qs to accurately achieve the desired 
bitrate. The larger the QP of the H.264/AVC intraframe 
encoder is, the smaller the achieved bitrate is, and vice versa. 
The larger L1, L2, and L3 are, the higher the achieved bitrate is, 
and vice versa. The larger D1, D2, and D3 are, the smaller the 
achieved bitrate is, and vice versa. The larger Qs is, the smaller 
the achieved bitrate is, and vice versa. Given a fixed QP of the 
H.264/AVC intraframe encoder, gradually adjusting L1, L2, L3, 
D1, D2, D3, and Qs will gradually change the achieved RD 
performance, and then make the RD performance become 
saturation. On the other hand, TF and TG are usually set to be 
smaller to make intraframe refresh for a Wyner-Ziv frame 
occur infrequently. 
The H.264/AVC intraframe coding (GOPSize = 1) and 
H.264/AVC interframe coding [2] were employed for 
comparison with the two proposed codecs. Here, the setting for 
the H.264/AVC interframe coding is as follows: (a) the same 
GOP sizes (GOPSize = 2, 4, and 8) were employed; (b) each I 
frame was adjusted to the same as the corresponding key 
frames in the two proposed codecs; (c) the bitrates were 
adjusted by changing the QPs for each frame; (d) the number of 
the reference frames was set to 1; and (e) the RD optimization 
was off. The test video sequences were categorized as very 
slow-motion (Claire), slow/middle-motion (Hall monitor, 
Mother and daughter, and Salesman), and fast-motion 
(Carphone) sequences. 
B. RD Performance Comparison 
In this section, the RD performance comparison was 
conducted under (very) low bitrates for the Carphone, Claire, 
Hall monitor, Mother and daughter, and Salesman sequences 
under four different methods, i.e., ProposedDVC1, 
ProposedDVC2, H.264/AVC interframe coding, and 
H.264/AVC intraframe coding. The obtained results are shown 
in Figs. 12-19. Examples of the video frames for the Hall 
Monitor sequence with GOPSize = 4 decoded using the 
H.264/AVC interframe coding, ProposedDVC1, 
ProposedDVC2, and the H.264/AVC intraframe coding at 
similar bitrates are shown in Fig. 20 for visual quality 
inspection, where the initial frame number is zero. 
For the very slow-motion sequence (Claire), several 
 12
between ProposedDVC2 and the H.264/AVC interframe 
coding are from 3 to 5 dB. The results of ProposedDVC1 for 
GOPSize = 8 are comparable with the results shown in [15]. 
 
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
30 45 60 75 90 105 120 135 Bitrates (kbps)
PSNR (dB) H.264 Inter (GOP=2) ProposedDVC1 (GOP=2) ProposedDVC2 (GOP=2)
H.264 Inter (GOP=4) ProposedDVC1 (GOP=4) ProposedDVC2 (GOP=4)
H.264 Inter (GOP=8) ProposedDVC1 (GOP=8) ProposedDVC2 (GOP=8)
H.264 Intra
 
Fig. 14.  RD performance for the Hall monitor sequence. 
 
Based on the above observations for the Hall monitor 
sequence, when GOPSize = 2, for ProposedDVC1, good side 
information can be obtained and the background areas are 
almost still due to most motions occur in the foreground areas. 
For ProposedDVC2, only a few Wyner-Ziv bits are generated. 
Hence, the performance of the two proposed codecs is very 
close to that of H.264/AVC interframe coding. When GOPSize 
= 4, ProposedDVC1 can slightly outperform ProposedDVC2 
due to the decoder’s motion compensation in ProposedDVC1 is 
efficient for some foreground motions whereas ProposedDVC2 
should spend more bits for significant differences between 
successive frames due to foreground motions. When GOPSize 
= 8, ProposedDVC1 can significantly outperform 
ProposedDVC2 due to GOPSize = 8 is too large for 
ProposedDVC2 without performing motion estimation. In 
addition, it can be observed from Fig. 14 that ProposedDVC1 
with GOPSize = 4 consistently outperform that with GOPSize = 
2 and can outperform that with GOPSize = 8 when the bitrate 
approaches 120 kbps. Similarly, ProposedDVC2 with GOPSize 
= 4 consistently outperforms that with GOPSize = 2 and can 
outperform that with GOPSize = 8 when the bitrate approaches 
70 kbps. In summary, it is recommended, again, that the two 
proposed codecs with GOPSize = 4 are more suitable for the 
Hall monitor sequence. 
For the Salesman sequence (slow/middle-motion), several 
observations can be found from the obtained RD performance, 
as shown in Fig. 15. The simulation results with GOPSize = 2 
and GOPSize = 4 are similar to those of the Hall monitor 
sequence with GOPSize = 2 and GOPSize = 4, respectively. 
When GOPSize = 8 (also highlighted in Fig. 16), the PSNR 
performance gains of ProposedDVC1 are 0.5~1 dB higher than 
those of ProposedDVC2. The PSNR performance gains of 
ProposedDVC1 above those of the H.264/AVC intraframe 
coding are from 6 to 8 dB. The PSNR performance gains of 
ProposedDVC2 above those of the H.264/AVC intraframe 
coding are from 6 to 7 dB. The PSNR performance gaps 
between ProposedDVC1 and the H.264/AVC interframe 
coding are from 1 to 3 dB. The PSNR performance gaps 
between ProposedDVC2 and the H.264/AVC interframe 
coding are from 1 to 4 dB. The results of the two proposed 
codecs for GOPSize = 8 are comparable with the results shown 
in [15]. 
 
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
30 45 60 75 90 105 120 135 150Bitrates (kbps)
PSNR (dB) H.264 Inter (GOP=2) ProposedDVC1 (GOP=2) ProposedDVC2 (GOP=2)
H.264 Inter (GOP=4) ProposedDVC1 (GOP=4) ProposedDVC2 (GOP=4)
H.264 Inter (GOP=8) ProposedDVC1 (GOP=8) ProposedDVC2 (GOP=8)
H.264 Intra
 
Fig. 15.  RD performance for the Salesman sequence. 
 
 In Fig. 16, it can be observed that when GOPSize = 8, the 
performance gap between the two proposed codecs is smaller 
than that of the Hall monitor sequence. The reason is that some 
slight “occlusions” occur in the Hall monitor sequence. That is, 
an object appears in a frame, but does not exist in the previous 
frame. In this situation, ProposedDVC2 will spend more bits to 
denote the frame containing the appeared object. However, no 
such situations appear in the Salesman sequence. In addition, it 
can be observed from Fig. 15 that the two proposed codecs with 
GOPSize = 8 outperform those with GOPSize = 2 and GOPSize 
= 4. In summary, it is recommended again that the two 
proposed codecs with GOPSize = 8 are more suitable for the 
Salesman sequence. 
 
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
30 45 60 75 90 105 120 135 150Bitrate (kbps)
PSNR (dB) H.264 Inter (GOP = 8) ProposedDVC1 (GOP = 8)
ProposedDVC2 (GOP = 8) H.264 Intra
 
Fig. 16.  RD performance for the Salesman sequence with GOPSize = 8. 
 
 For the Mother and daughter sequence 
(slow/middle-motion), several observations can be found from 
the obtained RD performance, as shown in Fig. 17. The 
simulation results with GOPSize = 2 and GOPSize = 4 are 
similar to those of the Hall monitor sequence with GOPSize = 2 
and GOPSize = 4, respectively, except that the PSNR 
performance gains of the two proposed codecs above those of 
the H.264/AVC intraframe coding are slightly worse than those 
of the Hall monitor sequence. The reasons are that a few fast 
motions (e.g., the movement of the mother’s hand) appear 
occasionally and in fact, the performance gaps between the 
 14
estimation is much higher than those of the two proposed 
encoders, in order to make the comparisons as fair as possible, 
the same GOP size is used and the number of reference frames 
is fixed to 1 during RD performance comparisons. However, 
the computational complexity of the employed H.264/AVC 
interframe encoder is still much higher than those of the two 
proposed encoders. On the other hand, although the 
performance of ProposedDVC2 is usually (slightly) worse than 
that of ProposedDVC1, the computational complexity of the 
decoder in ProposedDVC2 is significantly lower than that in 
ProposedDVC1. 
 
V. CONCLUSIONS AND FUTURE WORKS 
In this paper, a Wyner-Ziv video codec with coding 
mode-aided motion compensation at the decoder 
(ProposedDVC1) and a Wyner-Ziv video codec based on the 
robust media hashing (ProposedDVC2) were proposed. 
ProposedDVC1 is with light encoder and heavy decoder, and 
has the following characteristics: (a) for each block, a large 
amount of candidate blocks are evaluated based on some 
criteria derived from the RS decoding and the best 
neighborhood matching to find the best candidate block as the 
side information; (b) ECC decoding is applied to participate in 
generating side information; (c) no feedback channel is 
required. ProposedDVC2 is with light encoder and light 
decoder, and has the following characteristics: (a) no 
motion-compensated interpolation/extrapolation is performed 
at both the encoder and the decoder; (b) no ECC is applied; (c) 
no feedback channel is required. The two proposed Wyner-Ziv 
video codecs have been shown to present significant gains over 
the H.264/AVC intraframe coding while having comparable 
encoding complexity. Unavoidably, there is still a performance 
gap from the H.264/AVC interframe coding due to the 
H.264/AVC interframe coding performs complex motion 
estimation at the encoder. 
ProposedDVC1 is suitable for a scenario where the decoder 
can support high computational capability. For example, in a 
video sensor network, there may be thousands of 
low-complexity encoder devices (video sensors) and only one 
or a few high-complexity decoder devices (decoding center). 
ProposedDVC2 is suitable for a scenario where both the 
encoder and decoder are with low-complexity restrictions. For 
example, a pair of wireless mobile camera phones can 
communicate with each other directly without intermediate 
transcoder support. For future research, the two proposed 
Wyner-Ziv video codecs will be extended to multiview 
distributed video coding scenarios [26]-[27], in which more 
accurate side information may be generated based on the 
information from multiple views. In the multiview DVC 
methods [26]-[27], the basic paradigm for single view DVC 
similar to [7], [13] is naturally extended to multiview DVC. 
Video frames from different views are encoded independently 
and decoded jointly. That is, inter-view communication is not 
allowed at the encoder. However, we observe that if few data 
exchanges (e.g., hash information exchange) among views can 
be allowed at the encoder, more inter-view redundancies can be 
removed. In addition, the two proposed codecs will be applied 
in practical distributed environments (e.g., video sensor 
network [28] or multihop wireless network [29]). On the other 
hand, the rate control, error resilience, and security issues 
deserve further studying. 
REFERENCES 
[1] T. Sikora, “Trends and perspectives in image and video coding,” 
Proceedings of the IEEE, vol. 93, no. 1, pp. 6-17, Jan. 2005. 
[2] T. Wiegand, G. J. Sullivan, G. Bjøntegaard, and A. Luthra, “Overview of 
the H.264/AVC video coding standard,” IEEE Trans. on Circuits and 
Systems for Video Technology, vol. 13, no. 7, pp. 560-576, July 2003. 
[3]  B. Girod, A. Aaron, S. Rane, and D. Rebollo-Monedero, “Distributed 
video coding,” Proceedings of the IEEE, vol. 93, no. 1, pp. 71-83, Jan. 
2005. 
[4] R. Puri, A. Majumdar, P. Ishwar, and K. Ramchandran, “Distributed 
video coding in wireless sensor networks,” IEEE Signal Processing 
Magazine, vol. 23, no. 4, pp. 94-106, July 2006. 
[5] A. Wyner and J. Ziv, “The rate-distortion function for source coding with 
side information at the decoder,” IEEE Trans. on Information Theory, vol. 
IT-22, no. 1, pp. 1-10, Jan. 1976. 
[6] D. Slepian and J. K. Wolf, “Noiseless coding of correlated information 
sources,” IEEE Trans. on Information Theory, vol. IT-19, no. 4, pp. 
471-480, July 1973. 
[7] A. Aaron, E. Setton, and B. Girod, “Towards practical Wyner-Ziv coding 
of video,” in Proc. of IEEE Int. Conf. on Image Processing, Barcelona, 
Spain, Sept. 2003, pp. 869-872. 
[8] A. B. B. Adikari, W. A. C. Fernando, H. K. Arachchi, and W. A. R. J. 
Weerakkody, “Sequential motion estimation using luminance and 
chrominance information for distributed video coding of Wyner-Ziv 
frames,” IEE Electronics Letters, vol. 42, no. 7, pp. 398-399, March 2006. 
[9] M. Tagliasacchi, A. Trapanese, S. Tubaro, J. Ascenso, C. Brites, and F. 
Pereira, “Exploiting spatial redundancy in pixel domain Wyner-Ziv video 
coding,” in Proc. of IEEE Int. Conf. on Image Processing, Atlanta, GA, 
USA, Oct. 2006, pp. 253-256. 
[10] C. Brites, J. Ascenso, and F. Pereira, “Feedback channel in pixel domain 
Wyner-Ziv video coding: myths and realities,” in Proc. of European 
Signal Processing Conference, Florence, Italy, Sept. 2006. 
[11] R. Puri and K. Ramchandran, “PRISM: a new robust video coding 
architecture based on distributed compression principles,” in Proc. of 
Allerton Conf. on Communication, Control and Computing, Allerton, 
USA, Oct. 2002. 
[12] R. Puri and K. Ramchandran, “PRISM: a “reversed” multimedia coding 
paradigm,” in Proc. of IEEE Int. Conf. on Image Processing, Barcelona, 
Spain, Sept. 2003, pp. 617-620. 
[13] A. Aaron, S. Rane, E. Setton, and B. Girod, “Transform-domain 
Wyner-Ziv codec for video,” in Proc. of SPIE Visual Communications 
and Image Processing, San Jose, CA, USA, Jan. 2004, pp. 520-528. 
[14] A. Aaron, S. Rane, and B. Girod, “Wyner-Ziv video coding with 
hash-based motion compensation at the receiver,” in Proc. of IEEE Int. 
Conf. on Image Processing, Singapore, Oct. 2004, pp. 3097-3100. 
[15] A. Aaron and B. Girod, “Wyner-Ziv video coding with low encoder 
complexity,” in Proc. of Picture Coding Symposium, San Francisco, CA, 
USA, Dec. 2004. 
[16] J. Ascenso, C. Brites, and F. Pereira, “Content adaptive Wyner-Ziv video 
coding driven by motion activity,” in Proc. of IEEE Int. Conf. on Image 
Processing, Atlanta, GA, USA, Oct. 2006, pp. 605-608. 
[17] Z. Li, L. Liu, and E. J. Delp, “Wyner-Ziv video coding with universal 
prediction,” IEEE Trans. on Circuits and Systems for Video Technology, 
vol. 16, no. 11, pp. 1430-1436, Nov. 2006. 
[18] X. Artigas and L. Torres, “Iterative generation of motion-compensated 
side information for distributed video coding,” in Proc. of IEEE Int. Conf. 
on Image Processing, Genova, Italy, Sept. 2005, pp. 833-836. 
[19] L. W. Kang and C. S. Lu, “Wyner-Ziv video coding with coding 
mode-aided motion compensation,” in Proc. of IEEE Int. Conf. on Image 
Processing, Atlanta, GA, USA, Oct. 2006, pp. 237-240. 
[20] L. W. Kang and C. S. Lu, “Low-complexity Wyner-Ziv video coding 
based on robust media hashing,” in Proc. of IEEE Int. Workshop on 
參加國際學術會議活動概況簡表 
ACTIVITIES WHILE ATTENDING INTERNATIONAL CONFERENCES 
姓   名 
Name 
呂  俊  賢 
服務單位及職稱 
Institute & Position 
中央研究院資訊所 副研究員 
中文：IEEE國際多媒體會議 
Chinese 會  議  名  稱 
Title of Meeting 
英文：IEEE Int. Conference on Multimedia and Expo (ICME)
English 
日   期 
Date of Conference 
July 2~June 5, 2007 地  點 
Location 
Beijing, PROC 
經  費  來  源 
Source of Travel Support g申請補助 Applying for Support（補助單位：NSC） 
□會方補助 Conference Unit Support □自籌 Self 
 
中文
Chinese
 名稱 
 Name 
英文
English
IEEE 
國際組 
織 
Inter-nation
al Organ- 
ization(s) 與我 
關係 
Relationship
□國家會員 National Member  □團體會員 Delegation  
g個人會員 Individual Member  □贊助會員 Honorary Member 
□觀察員 Observer □其它 Other 
g國際組織
主辦 
Sponsored by 
International 
Organization(s)
 
地主國承辦單位
Home 
Beking University 
性質 Nature
□政府機關 Government      學校 School 
□民間團體 Non-Government or Private Sector  □其他 Other
 
主 辦 單
位 
Organizer 
名稱 Name  
性質 
Nature 
 
 
□非國際組 
織主辦 
Sponsored by 
Non-International
Organization(s)
協 辦 單 位 
Assisting Group 
 
重要性 
Importance 
涉及我與會之 Affects Our 
□會籍 Membership   □名稱 Title   □權利 Voting Rights    □地位 Position 
請說明 Remarks：
 
世界各國 
國 
際 
會 
議 
資 
料 
b 
a 
c 
k 
g 
r 
o 
u 
n 
d 
 
o 
f 
 
c 
o 
n 
f 
e 
r 
e 
n 
c 
e 參加國家或地區  
Participating Countries 
 中共有無參加人員 
Participants from PRC g有 Yes    □無 No    □不確知 Not certain
 
其  他  說  明 
Other Remarks 
 
＊本表請附於會後報告前頁，篇幅如不敷使用，可另備 A4紙張橫式繕打。（Please 
attach this form to your report；use additional A4 blank pages if needed. ） 
填表人簽章 signature    呂俊賢               填表人電話 Tel：(02)27883799 ext. 1513 
                                         填表日期 Date：96/07/07 
 
JOINT MULTIMEDIA FINGERPRINTING AND ENCRYPTION:
SECURITY ISSUES AND SOME SOLUTIONS
Shih-Wei Sun1,2, Chun-Shien Lu1,∗, and Pao-Chi Chang2,3
1Institute of Information Science, Academia Sinica, Taipei, Taiwan 115, ROC
2Dept. Electrical Engineering, National Central Univ., Chung-Li, Taiwan 320, ROC
3Dept. Communication Engineering, National Central Univ., Chung-Li, Taiwan 320, ROC
ABSTRACT
In this paper, a new multimedia joint ﬁngerprinting and en-
cryption (JFE) scheme embedded into the advanced access
content system (AACS) is proposed. Like other security-
related systems, there exist some security threats to the pro-
posed framework. To cope with these difﬁculties, the con-
tributions of this paper include: (i) we apply multimedia
encryption at different points to resist some attacks points;
and (ii) we propose rewritable ﬁngerprint embedding (RFE)
to deal with some multi-point collusion attacks. Experimen-
tal results are provided to demonstrate the proposed AACS-
compatible JFE method.
1. INTRODUCTION
In multimedia security, encryption plays the ﬁrst line of de-
fense for secure multimedia transmission. However, it is
known that the decrypted data loses the imposed protection
capability and may be illegally distributed. In order to per-
sistently preserve the capability of multimedia protection,
ﬁngerprinting is further applied to give traitor tracing. In
this paper, we study the dual goals of multimedia content
access control and traitor tracing. The emerging joint mul-
timedia encryption and ﬁngerprinting technology, divided
into three categories [5], is brieﬂy described as follows.
a) Transmitter-Side Encryption and Fingerprint Embed-
ding: A multimedia plaintext is separately embedded with
a user’s ﬁngerprint and then encrypted with a global key to
form a multimedia ciphertext [3, 7]. This scenario incurs
some disadvantages: (1) Inefﬁcient bandwidth utilization:
since multimedia ﬁngerprint embedding is done at the trans-
mission side, repeat request of the same copy will waste
bandwidth; (2) Insecure encryption: since a single global
encryption key is used, if a malicious user eavesdrops other
user’s data, then the multimedia plaintext belonging to that
user can be obtained.
b) Transmitter-side Encryption and Receiver-side Fin-
gerprint Embedding: Fingerprint embedding at the receiver-
*Corresponding author: Dr. C. S. Lu (lcs@iis.sinica.edu.tw)
side was ﬁrst proposed in [6] for digital TV and then applied
to digital right management (DRM) of digital cinema [2].
At the transmitter side, only global key-based encryption
is necessary. This kind of design can save a lot of com-
putation time and bandwidth usage. In this scenario, the
receiver plays like a super node in a network, not merely
the user end. Thus, multimedia data can be sent to differ-
ent users via the receiver (super node) for multicasting. At
the receiver side, the received multimedia ciphertext could
be decrypted according to the global key. Meanwhile, the
user ﬁngerprint should be embedded into the multimedia
data to generate the ﬁngerprinted multimedia data for each
user. However, the total load of decryption and ﬁngerprint
embedding gathered at the receiver side (super node) will
increase computational complexity.
c) Joint Fingerprinting and Decryption: In order to re-
duce system complexity and achieve real-time requirement,
Kundur and Karthik [5] proposed a joint ﬁngerprinting and
decryption method. The idea is that the multimedia cipher-
text is partially decrypted such that the un-decrypted parts
imitate ﬁngerprinted multimedia. This kind of method is
conceptually promising, achieving multimedia partial de-
cryption and multimedia ﬁngerprint embedding at the same
time. However, the un-decrypted content must satisfy two
conﬂicting requirements. On the one hand, the un-decrypted
parts should not affect the whole transparency of ﬁnger-
printedmultimedia data. On the other hand, the un-decrypted
parts should preserve meaningful encryption, i.e., the en-
crypted parts can intrinsically hide their original content.
In this paper, we will present a new joint ﬁngerprinting
and encryption (JFE) scheme, which can be incorporated
with the advanced access content system (AACS). AACS
[1] is a leading technology proposed by many famous com-
panies. Basically, AACS contains four major parts: content
owner end, licensed replicator end, licensing entity for key
management, and licensed player at the user end. The con-
tributions of our AACS-compatible JFE method include: (i)
we discuss the security threats to the proposed method and
present solutions to cope with some of them; (ii) we ap-
ply multimedia encryption at different points to resist some
15311-4244-1017-7/07/$25.00 ©2007 IEEE ICME 2007
Host
Media
DCT
Y
Channel from Owner
to Replicator
Light-
weight
Enk
Inverse
to DCT
Fingerprint
Generation
RFE
iF
Y Light-
weight
Enk
Key Enk and
Forwarding
Replicator EndContent Owner End
X
Entropy
Coding
Enk
Entropy
Coding
Dek
r
K
Key Enk and
Forwarding
Key Management Center
RFE
Ko
F
oFingerprint
Generation
Yi
Network
A
B
C D
E
gK iK
iE(K  )gE(K  )
r
K
r
E(K  )
Key
Dek
Domain
Channel from
Replicator to Users
Light-
weight
Dek
Light-
weight
Dek
Fingerprint
extraction
Inverse
to DCT
Key Dek
User End
Sealed Set-Top Box
Network
F
H
G
)( ikE
)( gkE
'iY
ik gk
iY iX
'iF
Figure 1: AACS-compatible multimedia joint ﬁngerprinting and encryption method.
by
'by
1'bf
1'bf
1bf
1bf
bx
Figure 2: An example of rewritable ﬁngerprint embedding.
α′(+1,−1), α
′
(+1,+1), α
′
(−1,+1), and α′(−1,−1) at the replica-
tor end. In the following, we will describe how these scaling
factors can be deﬁned to satisfy robust ﬁngerprint extraction
in a non-blind watermarking scenario, which is considered
reasonable in multimedia ﬁngerprinting [7, 8].
Let us ﬁrst consider the case of α′(+1,−1), i.e., fb = +1
and f ′b = −1. At the replicator end, the user ﬁngerprint
embedding, similar to Eq. (1), is deﬁned as:
y′b = yb(1 + α
′
(+1,−1) · f ′b), (2)
where y′b is the b-th stego data corresponding to the b-th
cover data yb. By substituting fb = +1, f ′b = −1, and Eq.
(1) into Eq. (2), we have:
y′b = xb(1 + α− α′(+1,−1) − α · α′(+1,−1)). (3)
If f ′b = −1 is expected to be successfully extracted under
non-blind detection, then y ′b < xb is required to be achieved.
As a result, Eq. (3) can be rewritten as:
xb > xb(1 + α− α′(+1,−1) − α · α′(+1,−1)). (4)
We can further derive to obtain:
α′(+1,−1) >
α
(1 + α)
. (5)
The similar derivations can be derived for the remaining
three cases of scaling factors as:
α′(−1,+1) >
α
(1− α) for fb = −1, f
′
b = +1; (6)
α′(−1,−1) >
α
(α− 1) for fb = −1, f
′
b = −1; (7)
α′(+1,+1) >
−α
(1 + α)
for fb = +1, f ′b = +1. (8)
However, the prior knowledge of the ﬁngerprint state
change cannot be obtained neither at the content owner end
nor at the replicator end. On the contrary, a global parameter
of α′ should be determined and sent to the replicator end
for user ﬁngerprint embedding. According to Eqs.(5)∼(8),
the lower bound of the scaling factor of embedding at the
replicator end, α′, is deﬁned as:
α′ > max{α′(+1,−1), α′(−1,+1), α′(−1,−1), α′(+1,+1)}
= max{ α(1+α) , α(1−α) , α(α−1) , −α(1+α)}.
(9)
Since 0 < α′ < 1 and 0 < α < 1 hold, we can derive
α′ >
α
(1 − α) . (10)
4. EXPERIMENTAL RESULTS
Some common images of size 512×512were used for joint
ﬁngerprinting and encryption. AES was selected for per-
forming encryption with the encryption unit of 128 bits. In
order to select at least 128 signs of DCT coefﬁcients for
blockwise-encryption, an image was divided into blocks of
size 16 × 16. In the experiments, the 128 largest DCT AC
coefﬁcients in a 16×16 block were selected for encryption.
The size of ﬁngerprints embedded using ko and ku was 64
bits. There was 1 ﬁngerprint bit embedded in the (1, 2)-th
subband of a DCT block. In addition, α was set as 0.1 and
α′ was set as 0.12 to the satisfy the RFE requirements.
1533
VIDEO HALFTONING PRESERVING TEMPORAL CONSISTENCY
Chao-Yong Hsu,1,2 Chun-Shien Lu,2,∗ and Soo-Chang Pei1
1Graduate Institute of Communication Eng., National Taiwan University, Taipei, Taiwan, ROC
2Institute of Information Science, Academia Sinica, Taipei, Taiwan, ROC
ABSTRACT
Video halftoning is a key technology for use in the new dis-
play device, electronic paper (e-paper). This is still a rather
unexplored ſeld. The challenging issue of video halfton-
ing is the elimination of ƀicker ƀaw that will appear due to
error diffusion in the temporal domain. In this paper, we
propose a new video halftoning method, which is composed
of spatial error diffusion and inter-frame reference error
diffusion. In addition, since our method can efſciently re-
duce the ƀicker ƀaws, another advantage is that the halftone
video sequence can be efſciently compressed. When com-
pared with traditional 2D and 3D error diffusion techniques,
experimental results show that our method can signiſcantly
reduce the average ƀicker rates.
1. INTRODUCTION
Monitor industry is actively ſnding new display technol-
ogy to make the monitor lighter, thinner, and more portable.
The electronic paper (e-paper) is exactly the advanced and
emerging display technology. Compared with traditional
monitors, e-paper also consumes little power. Since a fa-
mous manufacturer, e-ink, already began to sell their prod-
ucts in 2005, it can be envisioned in the future that most
applications based on traditional paper and liquid crystal
display (LCD) will be replaced by e-paper. Since digital
halftone image is the only format that many electronic de-
vices can read, the development of digital halftone image/video
technique is the key to meet the requirement of e-paper. In
view of this, the goal of this paper is to develop a new digital
halftone video method for e-paper.
Digital halftoning [7, 10] refers to the physical process
of converting a continuous tone images to black and white
dots. Video halftoning is a technique that transfers a gen-
eral video sequence into a format that can be displayed on
devices with limited intensity resolutions and color palettes.
In the literature, only few video halftoning techniques were
discussed. A three-dimensional (3D) error diffusion algo-
rithm that is used to mitigate the “ƀicker” ƀaw was proposed
in [4]. Gotsman [3] applied an iterative image halftoning
*Corresponding Author: Dr. C. S. Lu (lcs@iis.sinica.edu.tw)
algorithm to deal with the problem when temporal ƀickers
exist between video frames. This algorithm achieves visual
results better than those obtained from classic halftoning al-
gorithms at the expense of increasing computational load.
In [1], Atkins et al. investigated the display of color image
sequences using a model-based approach for multilevel er-
ror diffusion. Their algorithm achieves an improvement in
image quality over that yielded by frame-independent quan-
tization when the frame rate is sufſciently high to support
temporal average by the human visual system. In [5], a
direct-binary-search algorithm was applied to 3-D error dif-
fusion. In [9], Sun presented a motion adaptive gain control-
based 3-D error diffusion method to enhance the temporal
consistency of the visual patterns by minimizing the ƀicker
ƀaws.
However, we ſnd that ƀicker ƀaws still are not efſ-
ciently reduced by means of the existing methods. In this
paper, we will focus on developing a new digital halfton-
ing video technique to solve the problem of ƀicker ƀaws
for use in e-paper. Furthermore, since our method can ef-
ſciently reduce the ƀicker ƀaws, another advantage is that
the halftone video sequence can be efſciently compressed.
Extensive experimental results show that our method can
signiſcantly reduce the ƀicker rates without raising other
side effects.
2. FLICKER FLAWS IN VIDEO HALFTONING
A general video halftoning method consists of spatial error
diffusion and temporal error diffusion, both of which cre-
ate the ƀicker phenomenon. Flicker means the change of
halftone values (either from black to while or from white
to black) that will be easily perceived by human eyes. For
temporal error diffusion, this procedure will cause the pixels
located at the same positions of neighboring video frames to
have different halftone values even the pixels have the same
or similar gray values. For spatial error diffusion, since the
probability that a pixel with gray level 128 halftoned to a
white or black halftone value is the same, the ƀicker ƀaw
will be generated when the pixel values are close to 128.
Fig. 1 shows the effect of ƀicker ƀaws. Speciſcally, the
while dots in Fig. 1(c) indicate the changes of halftone val-
19381-4244-1017-7/07/$25.00 ©2007 IEEE ICME 2007
(a) (b)
Fig. 3. Spot defect in a halftone video frame generated using
inter-frame reference error diffusion: (a) original frame; (b)
halftone frame with spot defect on the ſnger.
to be suppressed.
In order to completely release the quantization errors
generated during the halftoning process, they must be con-
trolled to not exceed the maximum value +1 or −1. If the
quantity of quantization error accumulated at a pixel ex-
ceeds 1, then its halftone value cannot be determined using
inter-frame reference. On the contrary, the halftone value
must be set to the value opposite to that decided using inter-
frame reference in order not to accumulate more quantiza-
tion errors. The cost is that the ƀicker ƀaw at single points
will occur. However, single ƀicker point will not lead to
apparent visual quality degradation.
In order to endow the proposed method with the ability
of suppressing error propagation, similar to video coding,
the video frames are divided into many group of pictures
(GOPs) for video halftoning. Each GOP consists of an I
frame and a number of P frames, where P frames refer to
I frame during the halftoning process. There may be two
ways to achieve the segmentation of GOPs. The ſrst one
is similar to video shot change detection in that an abrupt
change of video content will represent an I frame, i.e., a
start of a new GOP. This is done in the gray-scale or color
domain. The second one refers to the change of average
ƀicker rate to determine the existence of an I frame and is
adopted in this paper. In order to evaluate the performance
of ƀicker reduction, the average ƀicker rate (AFR) between
a pair of neighboring halftone video frames deſned as
AFR =
Y∑
y=1
X∑
x=1
|I(x, y, z)− I(x, y, z − 1)|)/(X ·Y ) (1)
is used as a metric for indicating the degree of ƀicker ƀaw.
In Eq. (1), X × Y denotes the size of a video frame and z
denotes frame number. If AFR (0 ≤ AFR ≤ 1) is larger
than a threshold, then the z frame is regarded as an I frame,
which is halftoned by means of 2D error diffusion. Since I
frame is used to refresh the halftoning process, spot defects
can be blocked at the expense of increasing ƀicker ƀaws
between I frame and its previous frame. As for P frames,
the algorithm described in Sec. 3.1 is used to achieve video
halftoning.
3.4. Efſcient Halftone Video Compression
Since ƀicker ƀaw can be efſciently reduced by our method,
the background during a series of video frames can be al-
most kept the same. As a result, this will lead to efſcient
compression of a halftone video sequence by saving a lot of
bit rates used for encoding the changeable background due
to the introduction of ƀicker ƀaws. In addition, it is worth
noting that since the GOP structure is adopted in our video
halftoning method, compression of halftone video is com-
patible to the current video coding standards.
4. EXPERIMENTAL RESULTS
In the experiments, a number of 24-bits color video sequences
were used. Among them, the results obtained from the “Vas-
sar” video [8], “Secretary” video, which was excerpted from
the movie “The devil wears the Prada,” and “Dolphin” video,
which was captured by a hand-held camera, are reported
here. These video sequences, respectively, contain small,
middle, and large motion contents. The frame size ranges
from 320× 240 to 436× 1024.
In order to evaluate the quality of halftone video by
means of temporal consistency, the change of ƀicker rate
(Eq. (1)) is a good indicator. Fig. 4∼Fig. 6 show the av-
erage ƀiker rate of every pair of neighboring halftone frames
for the “Vassar”, “Secretary”, and “Dolphin” video sequences,
respectively. In these ſgures, the 3D error diffusion method
[4], the 2D error diffusion method [2], and our method were
used for comparison. It can be observed from these results
that (i) 3D error diffusion outperforms 2D error diffusion
in video halftoning; (ii) our method consistently obtains
the lowest average ƀicker rates among the three methods.
We also ſnd that some AFRs of the three methods are very
close for those frames with large motions. In fact, they (the
peaks in the curve generated using our method) occur in
I frames. Furthermore, we ſnd that for videos with large
motions more I frame will be determined (e.g., many peaks
appear in Fig. 6).
5 10 15 20 25 30 35 40 45 50 55
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Frame No.
F
lic
ke
r 
ra
te
2D error diffusion
3D error diffusion
Our method
Fig. 4. Comparison of average ƀicker rates for the Vassar
video.
1940
