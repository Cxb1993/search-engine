routes by person following techniques for autonomous 
vehicle navigation； and 
3. use of two coaxially-aligned omni-directional 
cameras for indoor vehicle guidance. 
The above research results and those achieved in the 
passed three years together have been published as 9 
international journal papers, 4 international 
conference papers, and 6 domestic conference papers. 
Furthermore, three patent applications have been 
filed, with two granted, one being a Republic of 
China patent, and the other a U. S. A. patent. 
英文關鍵詞： autonomous vehicle, computer vision, sensing, 
calibration, localization, guidance, learning, omni-
directional camera 
 
 
目錄 
 
目錄.................................................................................................................................................. I 
中文摘要.........................................................................................................................................II 
英文摘要....................................................................................................................................... III 
一、前言......................................................................................................................................... 1 
二、研究目的................................................................................................................................. 1 
三、已論文發表及專利申請......................................................................................................... 2 
四、文獻探討................................................................................................................................. 5 
五、研究方法與成果..................................................................................................................... 6 
成果一：利用最新多部立體數位相機作環境影像之整合與展示.................................... 6 
成果二：利用跟人技術自動學習環境特徵與導航路線.................................................... 9 
成果三：用車上同軸雙環場攝影機組合作室內導航...................................................... 11 
六、結果與討論........................................................................................................................... 16 
七、成果自評............................................................................................................................... 16 
八、參考文獻............................................................................................................................... 17 
九、可供推廣之研發成果資料表............................................................................................... 18 
可推廣成果一：利用最新多部立體數位相機作環境影像之整合與展示...................... 18 
可推廣成果二：利用跟人技術自動學習環境特徵與導航路線...................................... 20 
可推廣成果三：用車上同軸雙環場攝影機組合作室內導航.......................................... 22 
十、附件 (IEEE 期刊論文四篇及已獲台灣及美國專利各一項之原文 PDF) 
 
 I
 III
 
英文摘要 
(English Abstract) 
 
Keywords: autonomous vehicle, computer vision, sensing, calibration, localization, guidance, 
learning, omni-directional camera 
 
The application domains of autonomous vehicles are widened day by day. More and more 
developments of new-type cameras and sensing devices have emerged, which can be used to 
improve the functions of autonomous vehicles. Therefore, the topics of autonomous vehicle 
guidance should be studied continuously using the new devices. This 3-year project, “a study on 
vision-based autonomous vehicle applications using new imaging, sensing, and calibration 
techniques,” has conducted analysis of the effects of various new-type cameras and sensors for 
environment sensing and modeling, and developed new intelligent computer vision functions, so 
as to enable the vehicle to understand the environment more precisely and to carry out 
autonomous vehicle guidance more effectively. 
All the 3-year goals of this project have been achieved successfully, and the following 
techniques have been yielded in this year: 
1. technique of using multiple new stereo digital cameras for sensing and integrated display 
of environment images; 
2. learning of environment features and guidance routes by person following techniques for 
autonomous vehicle navigation; and 
3. use of two coaxially-aligned omni-directional cameras for indoor vehicle guidance. 
The above research results and those achieved in the passed three years together have been 
published as 9 international journal papers, 4 international conference papers, and 6 domestic 
conference papers. Furthermore, three patent applications have been filed, with two granted, one 
being a Republic of China patent, and the other a U. S. A. patent. 
 
 
已論文發表及專利成果： 
在此計畫期間，本研究團隊致力於完成本計畫所預定之九項研究課題，並衍伸發表許
多相關論文。在此計畫的三年中(2009/08 至 2012/07)，本研究團隊共發表了 9 篇國際期刊
論文、4 篇國際會議論文，及 6 篇國內會議論文，並申請三個專利，其中兩個已分別獲得
中華民國及美國專利，詳列如下： 
一、國際期刊論文 
1. C. J. Wu and W. H. Tsai, “Unwarping of images taken by misaligned omni-cameras without 
camera calibration by curved quadrilateral morphing using quadratic pattern classifiers,” 
Optical Engineering, vol. 48, no. 8, pp. 087003-1~11, 2009 
2. C. J. Wu and W. H. Tsai, “A novel method for lateral vehicle localization by omni-cameras 
for car driving assistance,” Knowledge-Based and Intelligent Information and Engineering 
Systems (KES 2009) - Lecture Notes in Computer Science (LNCS), vol. 5712, J. D. 
Velázquez et al. (eds.), Springer, Berlin/Heidelberg, Germany, pp. 403-410, 2009. 
3. C. J. Wu and W. H. Tsai, “Adaptation of space-mapping methods for object location 
estimation to camera setup changes — a new study,” Knowledge-Based and Intelligent 
Information and Engineering Systems (KES 2009) - Lecture Notes in Computer Science 
(LNCS), vol. 5712, J. D. Velázquez et al. (eds.), Springer, Berlin/Heidelberg, Germany, pp. 
395-402, 2009. 
4. K. C. Chen and W. H. Tsai, “Guidance of indoor vision-based autonomous vehicles for 
security patrolling by a SIFT-based vehicle localization technique using along-path object 
information,” IEEE Transactions on Vehicular Technology, vol. 59, no. 7, Sept. 2010, pp. 
3261-3271, 2010. 
5. S. E. Shih and W. H. Tsai, “A new two-omni-camera system with a console table for 
versatile 3D vision applications and its automatic adaptation to imprecise camera setups,” 
Advances in Multimedia Modeling (MMM 2011) - Lecture Notes in Computer Science 
(LNCS), vol. 6523, K. T. Lee, W. H. Tsai, H. Y. M. Liao, T. Chen, J. W. Hsieh and T. T. 
Tseng. (eds.), Springer, Berlin/Heidelberg, Germany, pp. 193-205, 2011. 
6. Y. T. Kuo and W. H. Tsai, “A new 3D imaging system using a portable two-camera 
omni-imaging device for construction and browsing of human-reachable environments,” 
Advances in Computing - Lecture Notes in Computer Science (LNCS), vol. 6938, G. Bebis, et 
al. (eds.), Springer, Berlin/Heidelberg, Germany, pp. 484-495, 2011. 
7. P. H. Yuan, K. F. Yang and W. H. Tsai, “Real-time security monitoring around a video 
surveillance vehicle with a pair of two-camera omni-imaging devices,” IEEE Transactions 
on Vehicular Technology, vol. 60, no. 8, pp. 3603 – 3614, 2011. 
8. C. J. Wu and W. H. Tsai, “A space-mapping method for object location estimation adaptive 
to camera setup changes for vision-based automation applications,” IEEE Transactions on 
Circuits and Systems for Video Technology, vol. 22, no. 1, pp. 157-162, 2012. 
9. S. E. Shih and W. H. Tsai, “A two-omni-camera stereo vision system with an automatic 
adaptation capability to any system setup for 3d vision applications,” IEEE Transactions on 
Circuits and Systems for Video Technology, accepted and to appear, 2012. 
 2
四、專利發表 
1. K. F. Chien, W. H. Tsai, and C. J. Wu (2010). “Video surveillance system and video 
encoding method based on data hiding (基於資訊隱藏之視訊監控系統及其資訊隱藏之視
訊編碼方法),＂ Republic of China Patent, No. I330341, 2010/09/11. 
2. C. J. Wu, S. Y. Tsai and W. H. Tsai (2012). “Automatic ultrasonic and computer-vision 
navigation device and method using the same,＂ U. S. A. Patent, No. 8,116,928 B2, 
2012/02/14. 
3. C. J. Wu, S. Y. Tsai and W. H. Tsai (2012). “Automatic ultrasonic and computer-vision 
navigation device and method using the same (利用超音波與電腦視覺偵測之自動導航裝
置及其導航方法),＂ Republic of China Patent, 申請中. 
 
 4
 
研究方法與成果： 
一、成果一：利用最新多部立體數位相機作環境影像之整合與展示 
在本研究中，我們將兩部雙曲面反射式環場攝影機 (hyperboloidal catadioptric 
omni-directional camera, 以下簡稱環場攝影機)架設於自動車上，並藉由分析環境中護壁板
(mopboard)、大門及窗戶等特徵資訊，建構出完整的室內環境模型。在系統架構中，此二
環場攝影機為背對背架設，使其光軸位於同一直線上，並垂直架設於自動車上(如圖一)。在
建立室內空間之立體模型方面，我們首先將建立該室內環境地面之架構圖，並設計一套系
統性之方法，導領自動車循序漸進地對室內環境進行立體建構，並同時偵測環境中之大門
及窗戶等資訊，結合於室內環境模型中，最後並以電腦圖學方式呈現，讓使用者可自行切
換視角及視點，徹底了解此室內空間概況。 
更詳言之，本系統首先將引領自動車在一室內環境中，利用牆壁上之護壁板做為特徵
資訊，在此完全未知之室內空間繞行一周，並產生最初步之地面架構圖(floor-layout)。隨後，
我們將利用此地面架構圖做為基礎，引領自動車再次繞行一周，將環境中之大門、窗戶等
特徵資訊納入模型中，產生完整的室內空間模型。在此繞行過程中，令 N1, N2, …, Nm為自
動車自此繞行過程中駐足拍攝環境資訊的導航點(如圖二(a))，因為環場攝影機所擷取之環
境資訊為 360 度，我們必須研究一方法來決定所拍攝之環境資訊中，有哪些範圍是確實可
納入環境立體模型中，且有最準確之資訊。在本研究中，我們提出一最近距離法，保證讓
牆壁上之特徵點，皆能被最接近之導航點所拍攝之環場影像所建立，以得到最準確之資訊。
更詳言之，我們在每兩個鄰近之導航點間計算其中點 Mij (如圖二(a))，並將其投射於最接近
之牆壁上，得到投射點 Mij′ (如圖二(a))。在得到投射點 Mij′後，我們便可利用相鄰兩個投射
點，決定環場影像中被納入立體模型之範圍(如圖二(b))，同時並保證牆壁上之特徵點皆可
由最接近之導航點所建構。 
在建立環場影像與立體模型之間的關係後，我們便可分析室內空間中之大門及窗戶等
特徵點。由於此系統為上下環場攝影機組成，牆壁上之物體皆可能被切成上下兩分，並分
別成像於上方之環場影像及下方之環場影像。更甚者，此一物體也可能被切分由兩個不同
之導航點所建構。為了將此分別成像之物體結合回原本單一之物體，我們提出一演算法解
決此問題，並同時辨別其屬於大門或是窗戶。在此演算法中，我們首先將偵測左右相鄰之
影像，將位於同高度之物體視為同一物體(如圖三)，代表此物體是由不同之導航點所建構。
接著，我們判斷此物體之左右邊界是否為於上下環場影像之相同處，若是，則此二物體也
將被再次結合(如圖三)，代表此物體是由同一導航點之上下環場攝影機分別拍攝所得。最
後，我們判斷此物體若與地板接合，代表其為大門，若不是，則將其分類為窗戶(如圖三所
示)。 
在引領自動車繞行此空間一周，並經過以上所述之處理後，此室內空間之立體模型將
可被完整建構出來。我們利用電腦圖學技術，以 OpenGL 函市庫將此立體模型呈現於使用
者眼前(如圖四)。使用者可利用滑鼠任意改變視點及視角，並可對此室內環境有透徹之瞭解。 
 
 6
 
(a) 
 
(b) 
圖三、將被切分之物體結合回實際空間之物體。(a)物體被切分成像之示意圖。(b)最終偵
測到之物體。 
 
 
(a) 
 
(b) 
圖四、以電腦圖學技術呈現此室內空間之立體模型。(a)斜視俯瞰圖。(b)鳥瞰圖。 
 
 8
確保此路線會經過每一個學習到的監控物品所在地。而在其餘非監控物品所在地之處，我
們將利用直線套合(line fitting)技術，將兩個相鄰監控物品之間的距離修飾為直線，加速自
動車往後巡邏之效率。 
 
 
 
圖五、計步器校正示意圖。 
 
 
(a) 
 
(b) 
圖六、人物追蹤。(a)人物走到轉彎點。(b)人物轉彎後可能導致影像中失去人的蹤影。 
 
Start
End
 
(a) 
Start
E nd
 
(b) 
Start
E nd
 
(c) 
圖七、路線規劃的三種方式。(a)由起點走向終點，但因可能有障礙物而不可行。(b)每個學
習點都走過，但會造成效率不高。(c)本計畫中提出的方法可同時解決以上二問題。 
 
 10
在推得立體資訊的推導方式後，我們便可在自動車航行的圖中蒐集環境資訊，並分析
其立體資訊。在本研究中，我們利用牆壁上之踢腳板(mopboard)做為一導航依據，並提出
一演算法偵測環境中踢腳板之立體位置。在踢腳板的偵測中，我們首先要對環場影像做分
析，擷取其踢腳板部分之特徵點。更詳言之，我們在擷取環場影像後，首先對其做灰階轉
換，轉成 256 色的灰階圖，並用一事先訂一之門檻值對其做二值化處理。為了降低雜訊之
影響，我們先對此圖做腐蝕處理(erosion)後，並做膨脹處理(dilation)。接著，因為空間中任
一垂直於地面之直線，必定通過環場影像之影像中心，我們定義每一條由影像中心向邊緣
發散之射線為一掃描線。此每一條掃描線，可代表空間中某個方向之垂直線段 L。如圖九
所示，此垂直線段 L 上之踢腳板成像位置，將較接近於環場影像中心。因此，我們將由影
像中心沿著此掃描線像影像外圍掃描，若碰到連續 10 個以上的像素有值。則代表這些像素
為踢腳板所成像之位置，並將這些踢腳板之像素位置丟入集合 Mk中，存放所有的踢腳板像
素值。 
在得到踢腳板在環場影像中的位置後，我們進而將其分類，分析出自動車前方、左方
及右方之牆壁位置。首先，我們將自動車周遭分為上述之三個方向(如圖十(a))，並將所偵
測之踢腳板像素集合 Mk，根據其像素位置，分別丟入對應於三個方向的像素集合，得到前
方、左方及右方之踢腳板像素資訊。接著，我們利用此三面牆互相垂直之特性，套用最小
平方誤差法(least square error, LSE)，就可推得實際牆壁之位置及角度。在得到牆壁位置後，
我們即可藉此修正自動車行進方向(如圖十(b)(c))，帶領自動車正確地沿著牆壁前進。當自
動車向前行進至接近前方牆壁時，我們將帶領自動車轉彎，朝向未探索的區域前進。在此
步驟中，我們將其分為兩類。第一類為當自動車進入兩個牆壁之夾角處時，自動車應向沒
有牆壁的那方旋轉(如圖十(d))；第二類為當自動車駛離牆壁時，這時自動車應向有牆壁的
那方旋轉(如圖十(e))，這樣自動車才可繼續沿著牆壁行進。 
如上所述，自動車將延著牆壁在未知環境中前進，同時並紀錄周遭牆壁之位置資訊。
在自動車完成環境繞行後，我們便可依據所蒐集之牆壁資訊，建立此室內環境之地面架構
圖(floor-layout)。在建立地面架構圖的研究中，我們首先對每一面所偵測的牆壁套用最小平
方誤差法(LSE)，得到最可描述此牆壁之一條直線。接著，我們利用所觀察到之相鄰二牆壁
互相垂直的特性，在-10 度到+10 度之間，每隔 0.1 度都去計算最適合所有牆壁的角度，得
到最符合整個室內環境的全局最佳化結果。在實驗中，我們實際將自動車放入一室內空間
中(如圖十一(a))，依據上述之演算法，在環場影像中偵測環境的踢腳板位置(如圖十一(b))，
並分析自動車周遭之牆壁資訊(如圖十一(c)(d))。自動車在此未知空間中航行完畢後，就可
利用所擷取之牆壁位置資訊(如圖十一(e))，建立此環境之地面架構圖(如圖十一(f))。 
 
 12
 (d) 
 
(e) 
圖十、導引自動車自行探索未知環境之步驟。(a)將自動車周遭分未前方、左方即右方。(b)(c)
利用環境資訊自動修正自動車行進角度，(d)(e)導引自動車沿牆壁探索未知區域。 
 
 
(a) 
 
(b) 
 
(c) 
 
(d) 
 14
 
結果與討論： 
利用自動車做環境建構一直是自動車研究中努力的目標。如何讓自動車在未知環境中
正確航行，並利用環境資訊做即時校準並繪製環境地圖皆是自動車研究中努力克服之課
題。在本年度的研究成果中，我們提出一個課服以上問題的解決方案。我們利用牆壁上之
踢腳板做為特徵資訊，分析自動車周遭之牆壁位置，使其能在未知環境中航行，並提出諸
多方法，使其產生出準確的環境地面架構圖(floor-layout)。在此研究中，我們也解決了計步
器(odometer)在自動車定位中，因各種機械誤差及誤差傳遞所導致的自動車航行問題。在本
研究的第三項成果中，我們利用室內環境的牆壁資訊，分析牆壁的位置及角度資訊，藉此
即時修正自動車之行進方向，確保其行進方位之正確性。 
除了建構出平面之地面架構圖，本計畫更進一步地分析環境中之物體資訊，將具有高
度資訊的大門、窗戶等物體也納入考量，建立出室內之立體模型。在本計畫的研究中，我
們利用架設於自動車上之上視及下視的環場攝影機，對大範圍環境進行取景。藉著環場攝
影機廣大視角的特性，在導引自動車航行的同時，並偵測室內環境的大門及窗戶資訊，並
且為了讓所偵測之資訊最精準，提出一方法讓環境中的物體皆由最接近之環場影像所建
構。最後，並利用電腦圖學技術，將室內立體模型完整呈現給使用者看。 
另一方面，自動車之自動學習及自主航行也是自動車巡邏應用中兩個主要的研究課
題。在本年度的研究中，我們也開發了一套結合 PTZ 攝影機與自動車之巡邏系統，不僅可
自動跟人做路線學習，也可在學習路線的同時紀錄環境特徵，最後並規劃一有效率之巡邏
路徑，使自動車在環境內有效的巡邏每個監控物品。在本研究中，我們也在跟人課題及監
控物品特徵擷取上研究了許多方法，也利用了監控物品的位置及角度資訊，校正自動車的
位置，解決計步器(odometer)中諸多的機械誤差及誤差傳遞等課題。 
 
成果自評： 
成果自評說明如下。 
A. 技術方面 --- 本年度研究內容符合原計畫預定進程，細節如下： 
1. 在利用最新多部立體數位相機作環境影像之整合與展示的研究中，我們利用室內
環境中牆壁上的踢腳板做為導航依據，使用上視及下視雙環場攝影機擷取環境資
訊，將牆壁、大門及窗戶等資訊繪製於一立體室內模型中，並結合電腦圖學技術，
讓使用者可自由的切換視點及視角來了解該室內環境架構。 
2. 在利用跟人技術自動學習環境特徵與導航路線技術方面，我們提出跟人及丟失尋
回之技術，並在跟隨使用者的同時，自動產生巡邏路線並同時紀錄監控物品的特
徵資訊。另外，也提出了一套計算巡邏路徑，使自動車可有效地巡邏此室內空間。 
3. 在用車上同軸雙環場攝影機組合作室內導航方面，我們分析室內環境中牆壁上的
踢腳板，並以此計算牆壁上之特徵點立體位置。在導航途中，我們利用牆壁的角
度資訊修正自動車行進方向，並依照牆壁位置自動在未知環境中規劃自動車行進
路線，最後並提出一全局最佳化方法，產生精準的室內空間地板架構圖。 
B. 成果運用方面 --- 過去成果已充分發表論文及申請專利，細節如下： 
1. 三年共發表 9 篇國際期刊論文 (過去一年內共 2 篇，皆為 IEEE 期刊論文) 
2. 三年共發表 4 篇國際會議論文 
3. 三年共發表 6 篇國內會議論文 (過去一年內共 2 篇) 
 16
 可供推廣之研發成果資料表 
■可申請專利  ■可技術移轉                                      日期：101 年 6 月30日 
國科會補助計畫 
計畫名稱：新型攝影、感測與校正技術在電腦視覺自動車上之應用
計畫主持人：蔡文祥 
計畫編號：NSC 98-2221-E-009-116-MY3 
學門領域：工程處 
技術/創作名稱 
利用最新多部立體數位相機作環境影像之整合與展示 
Technique of using multiple new stereo digital cameras for sensing and 
integrated display of environment images 
發明人/創作人 尤柏智、蔡文祥 
中文： 
本技術利用一般的室內環境中所常見的牆壁上踢腳板之特徵點做
為導航依據，使用兩個分別為上視及下視之雙環場攝影機取像系統
來擷取該室內環境之大範圍環場資訊。此技術並可利用此取像系統
所擷取之資訊，將牆壁、大門及窗戶等物體資訊全部繪製於一個整
合後之立體室內模型中。在室內模型建構完成後，此技術並可結合
電腦圖學技術，利用開放原始碼之 OpenGL 函市庫繪製完整的室內
立體模型，讓使用者可自由的切換視點及視角來了解該室內環境架
構。 技術說明 英文： 
This technique uses the features on mopboards to navigate an 
autonomous vehicle within an indoor environment. It uses a top-view 
and down-view catadioptric omni-directional cameras to detect the 
environment features, and draw the walls, doors, and windows on a 3D 
indoor model. Furthermore, this technique combines the computer 
graphics technologies, and draw this integrated 3D indoor model by the 
use of the open source OpenGL Library, so that a user can freely 
change the viewpoint and viewing direction to thoroughly understand 
the 3D indoor environment. 
可利用之產業 
及 
可開發之產品 
安全監控產業 
房屋業者 
保全機器人 
附件二 
 18
可供推廣之研發成果資料表 
■可申請專利  ■可技術移轉                                      日期：101 年 6 月30日 
國科會補助計畫 
計畫名稱：新型攝影、感測與校正技術在電腦視覺自動車上之應用
計畫主持人：蔡文祥         
計畫編號：NSC 98-2221-E-009-116-MY3 
學門領域：工程處 
技術/創作名稱 
利用跟人技術自動學習環境特徵與導航路線 
Learning of environment features and guidance routes by person 
following techniques for autonomous vehicle navigation 
發明人/創作人 蔡雙圓、蔡文祥 
中文： 
此技術會自動分析使用者身上之特徵點，導引自動車自動跟隨使用
者移動。並且在跟人的過程中，此技術擁有即時跟人之能力，並在
使用者轉彎時導致丟失使用者影像時，有自動尋回之能力。並且，
在自動車跟隨使用者前進的同時，此技術也將自動讓自動車學習此
巡邏路線，並同時沿途紀錄環境中監控物品的特徵資訊。在路線學
習完畢後，此技術也包含了一套計算全局最佳巡邏路徑的技術，使
產生之巡邏路線不僅可監控所有的監控物件，也可使自動車有效地
巡邏此室內空間。 
技術說明 
英文： 
This technique analyze the features on the user’s body, and uses these 
features to command the autonomous vehicle to follow the user’s 
movement. This person following functionality has the ability to follow 
the user in real-time, and is capable to follow the user in the case that 
the user is lost in the captured images when turning left or right. In the 
same time of following the user, this vehicle system learns the patrol 
route, and records the features on the monitoring objects. After the 
learning procedure is conducted, a global optimization method will be 
conducted to compute the optimal patrol route. In this optimal patrol 
route, the autonomous vehicle will be able to monitor all the 
monitoring objects in this route, and maintain the simplicity and 
efficiency at the same time. 
可利用之產業 
及 
可開發之產品 
安全監控產業 
圖書館保全系統 
保全機器人 
附件二 
 20
 22
可供推廣之研發成果資料表 
■可申請專利  ■可技術移轉                                      日期：101 年 6 月30日 
國科會補助計畫 
計畫名稱：新型攝影、感測與校正技術在電腦視覺自動車上之應用
計畫主持人：蔡文祥         
計畫編號：NSC 98-2221-E-009-116-MY3 
學門領域：工程處 
技術/創作名稱 
用車上同軸雙環場攝影機組合作室內導航 
Use of two coaxially-aligned omni-directional cameras for indoor 
vehicle guidance 
發明人/創作人 尤柏智、蔡文祥 
中文： 
本技術分析室內環境中牆壁上踢腳板之特徵資訊，並利用自動車上
搭載之之同軸雙環場攝影機計算牆壁上之特徵點立體位置。在導航
途中，我們利用牆壁的角度資訊修正自動車行進方向，並依照牆壁
之位置資訊，自動在未知環境中規劃自動車行進路線，最後並提出
一全局最佳化方法，產生精準的室內空間地板架構圖。 
技術說明 
英文： 
In an indoor environment, the features on the mopboards of the walls 
are analyzed in this technique, and the 3D data of the features on the 
walls can be calculated by the proposed technique with the use of the 
two coaxial catadioptric omni-directional cameras which are mounted 
on the autonomous vehicle. During the navigation, the direction of the 
wall is used to correct the moving direction of the autonomous vehicle, 
and the location of the walls are used to generate the route of the 
autonomous vehicle within an unknown environment. After the 
navigation is finished, a global optimization method is conducted to 
generate an accurate floor layout of this indoor environment. 
可利用之產業 
及 
可開發之產品 
安全監控產業 
房屋業者 
保全機器人 
技術特點 
本技術利用牆壁上之踢腳板特徵，直接在環場影像中快速找到踢腳
板位置，並且進而得到自動車周圍牆壁之位置與方向。在得到牆壁
位置後，此技術提出一方法來導引自動車在完全未知的環境中航
行，並逐步將此室內環境之資訊建構出來。在自動車完成航行任務
後，此技術並可利用一全局最佳化演算法，精準地建構出此室內空
IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 59, NO. 7, SEPTEMBER 2010 3261
Vision-Based Autonomous Vehicle Guidance for
Indoor Security Patrolling by a SIFT-Based
Vehicle-Localization Technique
Kuan-Chieh Chen and Wen-Hsiang Tsai, Senior Member, IEEE
Abstract—A novel method for guidance of vision-based
autonomous vehicles for indoor security patrolling using scale-
invariant feature transformation (SIFT) and vehicle localization
techniques is proposed. Along-path objects to be monitored are
used as landmarks for vehicle localization. The localization work
is accomplished by three steps: SIFT-based object image feature
matching, 2-D affine transformation using the Hough transform,
and analytic 3-D space transformation. Object monitoring can be
simultaneously achieved during the vehicle-localization process,
and most planar-surfaced objects can be utilized in the process,
greatly enhancing the applicability of the proposed method. Vehi-
cle trajectory deviations from the planned path due to mechanic
error accumulation are also estimated by setting up a calibra-
tion line on the monitored object image and applying the 3-D
space transformation. Moreover, a path-correction technique is
proposed to conduct a path adjustment and guide the vehicle to
navigate to the next path node. Analysis of the accuracy of the
vehicle-localization and path-correction results is finally included.
The experimental results show that the proposed method, utilizing
only a single view of each object, can guide the vehicle to navigate
accurately and monitor objects successfully.
Index Terms—Autonomous vehicle, computer vision, guidance,
landmarks, planar-surfaced objects, scale-invariant feature trans-
formation (SIFT), security patrolling, vehicle localization, 3-D
space transformation.
I. INTRODUCTION
IN RECENT years, due to fast developments of computervision techniques, studies on vision-based autonomous ve-
hicle navigation have high prominence because of their great
potential in various applications [1]–[8]. Autonomous vehicles
are becoming increasingly capable of performing a great va-
riety of dangerous or dreary work to replace human beings
Manuscript received August 17, 2009; revised December 7, 2009 and
February 18, 2010; accepted April 27, 2010. Date of publication June 7, 2010;
date of current version September 17, 2010. This work was supported by the
Ministry of Economic Affairs under Project MOEA 98-EC-17-A-02-S2-0047
in the Technology Development Program for Academia. The review of this
paper was coordinated by Prof. S. Ci.
K.-C. Chen was with the Institute of Multimedia Engineering, College of
Computer Science, National Chiao Tung University, Hsinchu 30010, Taiwan.
He is now with the Research Center for Information Technology Innovation,
Academia Sinica, Taipei 115, Taiwan (e-mail: jackie.cs94g@nctu.edu.tw).
W.-H. Tsai is with the Department of Computer Science, College of Com-
puter Science, National Chiao Tung University, Hsinchu 30010, Taiwan, and
also with the Department of Information Communication, Asia University,
Taichung 41354, Taiwan (e-mail: whtsai@cis.nctu.edu.tw).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TVT.2010.2052079
in applications of interoffice document delivering, unmanned
transportation, house cleaning, security patrolling, etc. The goal
of this study is to develop an autonomous vehicle system
for indoor security patrolling applications. The major issue
in this study is to make the vehicle precisely navigate in its
patrolling path. This may be accomplished by accurate vehicle
localization in each navigation cycle.
Traditionally, an autonomous vehicle is equipped with an
odometer to measure the current location of the vehicle with
respect to a starting point. However, this scheme usually suffers
from incremental mechanic errors that are caused by the vehicle
wheel system. On the other hand, it is common to equip vehicles
with visual cameras that provide more flexible visual informa-
tion for navigation control. It is, thus, desired to develop an
automatic vision-based vehicle-localization technique to over-
come the mechanic error-accumulation problem. One way to
achieve this goal is to utilize the features of artificial landmarks
or natural scenes in the environment to locate the vehicle by fea-
ture matching. Chou and Tsai [9] proposed a method to utilize
house corners to estimate vehicle locations. Okuma et al. [10]
used a colored marker for camera pose estimation. Huang et al.
[11] used a colored rectangular signboard to obtain the relative
position of the vehicle by calculating the vanishing points in
the signboard image. Wu and Tsai [12] used a circular shape
that is attached on the ceiling for vehicle-location estimation
using an upward-looking omni-camera equipped on the vehicle.
Xu et al. [13] utilized parallel lines and corner points on ceilings
as features for vehicle localization.
Most of the aforementioned methods can only deal with land-
marks with specific shapes or in ideal backgrounds, resulting
in unreasonable restrictions on the environment in which the
vehicle can navigate. Another approach to vehicle localization
[14]–[17], [26] is to represent vehicle locations as a visual
path that consists of a set of reference node images and a
topological graph. Each node in the graph corresponds to a
vehicle location, and each between-node link represents a path
segment through which the vehicle can navigate. The vehicle-
localization process is performed by finding the reference node
image that is most similar to the current location view and
then accordingly estimating the relative vehicle position. By
repeating this process, the vehicle may be guided to follow
the visual path. However, such a continuous image-matching
process acquires the along-path environmental images very
frequently, even at nonnode spots, and are therefore very time
consuming, resulting in slow navigation speeds, in general.
0018-9545/$26.00 © 2010 IEEE
CHEN AND TSAI: VEHICLE GUIDANCE FOR INDOOR SECURITY PATROLLING BY SIFT-BASED VEHICLE-LOCALIZATION TECHNIQUE 3263
Fig. 2. Two methods for creating a calibration line. (a) Using a line stand
affixed with a line structure. (b) Manually drawing a virtual line in the image
(the green one on the top of the poster).
To set up the calibration line, it is noted that a manmade
object often has rectangular surface shapes, on which a real
line can be detected for this purpose. For an object that does not
have line features on its planar surface, for example, a granite-
surfaced wall, it is proposed in this paper to create for it a
virtual (physically nonexistent) calibration line by two ways:
1) putting right under the object a “line stand” affixed with a
line structure (e.g., a straight steel wire or a line on the surface
of the stand) and then automatically detecting the line in the
image or 2) drawing in the object image a line that is parallel
to the environment floor using any clue of the image features.
See Fig. 2 for an illustration.
With the allowance of a virtual line as an axis of the RCS,
the flexibility of choosing objects for use as vehicle-localization
landmarks is greatly raised. This is another advantage of the
proposed method.
C. Sketch of the Proposed Navigation Process
In the navigation stage, the vehicle moves from one node to
another according to the path that is learned in the learning
stage. Vehicle localization at a node along the path is imple-
mented to include three stages: SIFT-based feature matching,
2-D affine transformation, and analytic 3-D space transforma-
tion. The first stage is to match the SIFT feature vectors of two
monitored object images, i.e., one taken in the navigation stage
and the other taken in the learning stage. The second stage is
to find an affine transformation from the matching results using
the Hough transform [21]. In addition, the third stage is to find
the vehicle location based on a new technique of analytic 3-D
space transformation proposed in this paper. More details are
described in the following, where the two images mentioned
above taken in learning and navigation stages are denoted as IL
and IN, respectively.
Algorithm 1: autonomous vehicle navigation process.
Stage 1—SIFT-based feature matching for finding similar
feature point pairs.
1) Apply the SIFT to IN to obtain a set FN of SIFT feature
vectors, and retrieve the corresponding feature vector set
FL of IL, which was obtained in the learning stage.
2) Take every pair of similar feature vectors, i.e., one from
FL and the other from FN, to define a group of four
Fig. 3. Object images IL and IN taken in learning and navigation stages.
(a) Red calibration line lL detected or selected manually. (b) Cyan calibration
line lN (on the top of the poster) generated by applying the transform T .
parameters of an affine transformation from IL to IN,
where the feature vector similarity is as defined in [18].
Stage 2—2-D affine transformation for finding the relative
mapping between images taken in learning and navigation.
3) Put all found parameter groups into a Hough space.
4) Detect the peak in the space.
5) Find the affine transform T corresponding to the peak,
and take it to represent the best relative mapping from IL
to IN.
Stage 3—analytic 3-D space transformation for computing
the deviation of the vehicle from the planned trajectory and
conducting path correction.
6) Use T to transform the calibration line lL, which was
automatically detected or manually selected in IL in the
learning stage, into the image space of IN, resulting in a
new image line in IN, which is denoted as lN (see Fig. 3
for an example, and note that if the navigation incurs
no deviation from the planned path, then lL and lN will
perfectly match in position).
7) Unambiguously transform lL and lN into the RCS accord-
ing to an analytic 3-D transformation process (proposed
in this paper and described later in the next section) from
the image space to the RCS to obtain two sets SL and
SN of vehicle poses with respect to the RCS, i.e., one for
the learning stage and the other for the navigation stage,
respectively.
8) Use SL and SN to derive the translation Vt and the ori-
entation θt of the vehicle’s current location with respect
to its location planned in the learning stage, and call
the parameter set (Vt, θt) the deviation of the vehicle
trajectory.
9) Use the deviation (Vt, θt) to derive a sequence of vehicle
commands to guide the vehicle to the correct path learned
in advance, and continue the navigation work for the next
session.
D. Advantages of Using SIFT for Object Image Matching
The use of the SIFT as described above is advantageous in
several aspects. First, in traditional vehicle-localization tech-
niques, obvious features like lines and curves on landmarks are
detected. However, this limits the variety of objects for use as
CHEN AND TSAI: VEHICLE GUIDANCE FOR INDOOR SECURITY PATROLLING BY SIFT-BASED VEHICLE-LOCALIZATION TECHNIQUE 3265
we transform the RCS coordinates into the CCS coordinates
using four steps: 1) Translate the origin R0 of the RCS with
coordinates (Xc, Yc, Zc) to the origin of the CCS; 2) rotate the
X−Y plane about the Z-axis through the pan angle θ such that
the Y−Z plane is parallel to the V−W plane of the CCS (see
Fig. 4); 3) rotate the Y−Z plane about the X-axis through the
tilt angle ψ such that the X−Y plane is parallel to the U−V
plane; and 4) reverse the Z-axis such that the positive direction
of the Z-axis is identical to the negative direction of the W -axis.
As a result, the transformation of the RCS coordinates
(x, y, z) of a point in the 3-D real-world space into the CCS
coordinates (u, v, w) may be described by
(u, v, w, 1) = (x, y, z, 1) · Tr (1)
where
Tr =
⎡
⎢⎣
cos θ − sin θ cosψ − sin θ sinψ 0
sin θ cos θ cosψ cos θ sinψ 0
0 sinψ − cosψ 0
x0 y0 z0 1
⎤
⎥⎦ (2)
with
x0 = −Xc cos θ − Yc sin θ
y0 =(Xc sin θ − Yc cos θ) cosψ − Zc sinψ
z0 =(Xc sin θ − Yc cos θ) sinψ + Zc cosψ. (3)
Next, let P be any point on the X-axis (the calibration line
L) with RCS coordinates (x, 0, 0). Then, its CCS coordinates
(ux, vx, wx) can be derived, using the matrix Tr described
above, to be
(ux, vx, wx, 1) = (x, 0, 0, 1) · Tr
=(x cos θ + x0,−x sin θ cosψ + y0
− x sin θ sinψ + z0, 1). (4)
Therefore, we have
ux =x cos θ + x0
vx = − x sin θ cosψ + y0
wx = − x sin θ sinψ + z0. (5)
Also, let (up, vp) be the image coordinates of the projection
of P in the ICS. Then, according to the camera’s optical
geometry, we have the following:
ux =
wx · up
f
vx =
wx · vp
f
(6)
where f is the camera focus length, which is assumed to be
known. Substituting the values of ux, vx, and wx of the three
equations of (5) into the above two equations and eliminating
the variable x, we get the equation for the projection of the
calibration line l (the X-axis) in the image plane as follows:
up +
z0 cos θ + x0 sin θ sinψ
−y0 sin θ sinψ + z0 sin θ cosψvp
− f · (y0 cos θ + x0 sin θ cosψ)−y0 sin θ sinψ + z0 sin θ cosψ = 0. (7)
Furthermore, assume that the equation of the calibration line
(lN or lL mentioned previously) in the ICS is up + bvp + c =
0, which can be obtained from the image, i.e., b and c are
known. Then, by comparing the coefficients of this equation
with those of (7) and substituting the values of x0, y0, and
z0 previously described in (3) into the result, we obtain the
following equalities:
b =
Yc sinψ − Zc cos θ cosψ
−Zc sin θ (8)
c =
f · (−Yc cosψ − Zc cos θ sinψ)
−Zc sin θ . (9)
Since b, c, and Zc in the above equations are known values,
we can now derive the values of Yc and θ. This is done first by
eliminating Zc and Yc from the above equalities to get
tan θ =
f
fb cosψ + c sinψ
(10)
from which the value of θ can be obtained. Also, from (8), we
can get the value of Yc as
Yc =
Zc cos θ cosψ − bZc sin θ
sinψ
. (11)
By substituting the values of θ and Yc above as well as the
coordinates (u0, v0) of r0 (the projection of the origin R0 of
the RCS) into (7) with the values of x0, y0, and z0 described
by (3), we get, finally, after some equation simplifications, the
following for computing Xc:
Xc =
u0(Yc cos θ cosψ + Zc sinψ sin θ) − Ycu0 sin θ
u0 sin θ cosψ + v0 cos θ
. (12)
In summary, we have completed the derivations of (10)–(12)
for computing the parameters Xc, Yc, and θ of the camera pose
using the following known data:
1) the coefficients b and c of the equation u + bv + c = 0 of
the calibration line in the image;
2) the height Zc of the virtual ceiling above the camera (i.e.,
the height of the calibration line above the camera in the
real-world space);
3) the tilt angle ψ of the camera provided by the PTZ camera
system;
4) the image coordinates (u0, v0) of the origin R0 of the
RCS and the focal length f of the camera.
IV. PATH CORRECTION
We now describe the proposed path-correction process for
use in the navigation stage. With the camera pose described by
(Xc, Yc) and θ with respect to the RCS on the planar surface
CHEN AND TSAI: VEHICLE GUIDANCE FOR INDOOR SECURITY PATROLLING BY SIFT-BASED VEHICLE-LOCALIZATION TECHNIQUE 3267
Fig. 6. Top view of the relations among the RCS, the VCS, and the GCS, as
well as corresponding angles of the vehicle, where the learned vehicle location
is at position (XL, YL) in the RCS, the current vehicle location is at position
(XN, YN) in the RCS, and the location of the next node N ′ is at position
(X′′L , Y
′′
L ) in the GCS.
b) Move ahead for the following distance to reach N ′:
|Vt| =
√
(X ′′L −X ′N)2 + (Y ′′L − Y ′N)2.
c) Turn the angle θ′t = θ′′L − θt after arriving at N ′ to
resume the original vehicle moving direction at N ′.
V. OBJECT IMAGE MATCHING
In the navigation stage, the vehicle stops in front of each
object to be monitored by the use of the learned path data.
However, the stop position in front of an object may not be
accurate in every navigation session but is just close to the
one recorded in the learning stage. This results in a slight
change in the viewing angle of the object from the camera
or, equivalently, in the camera pose with respect to the object.
An image acquired of the same object will so be different
in translation, scaling, and orientation from the one taken in
the learning stage. Thus, a method with the ability to match
corresponding objects in images that are taken with different
camera poses is needed.
In the past, SIFT has been proven to be one of the robust
image-matching techniques that use local invariant feature de-
scriptors with respect to different geometrical changes [19].
To allow efficient matching between images, each image is
processed to extract feature points, each of which is then repre-
sented as a SIFT feature vector, as mentioned previously. Each
SIFT feature vector consists of local image measures that are
invariant to image translation, scaling, and rotation and partially
invariant to 3-D viewpoint changes. In this study, we utilize
such invariance properties of SIFT feature vectors to match
object images. Specifically, the SIFT process [18] includes
four major stages to generate a set of SIFT feature vectors:
1) selection of a set of feature points in a scale space of the
input image; 2) localization of them to determine their locations
(x, y) and scales s; 3) assignment of orientations θ to them; and
4) generation of a descriptor Φ for each of the feature points as
a vector of all the gradient-orientation histogram entries in a
region around the feature point.
By using the aforementioned SIFT process for our case here,
the object images IN and IL taken in the navigation and learning
stages are first transformed to obtain two sets of SIFT feature
vectors: one set FN from IN and the other FL from IL, as
mentioned previously. According to the matching algorithm
proposed by Lowe [18], [20], the best candidate match for
each feature vector fN in FN can be found by identifying its
nearest neighbor in FL. Each nearest neighbor is defined as the
feature vector with the closest Euclidean distance to the feature
vector fN. Then, an affine transform between the match pairs is
estimated. For this purpose, many well-known fitting methods,
such as the random sample consensus algorithm [22], can be
used. However, according to [18], a better performance can
be obtained using the Hough transform [21], which is adopted
in this paper to identify the best affine transform between the
match pairs.
In more detail, using the aforementioned notations, for
each feature vector denoted as fN(xN, yN, sN, θN,ΦN), a point
(x′N, y
′
N) in the neighborhood of (xN, yN) can be generated
using sN and θN to be
x′N =xN + k · sN · cos θN
y′N = yN + k · sN · sin θN (21)
where k is a constant value. In addition, the corresponding point
(x′L, y
′
L) in FL can be computed in the same manner. Then, an
affine transform defined in the following way can be used to
obtain mapping from the pair (xL, yL) and (x′L, y′L) in FL to the
pair (xN, yN) and (x′N, y′N) in FN in terms of a scaling factor
s, an orientation θ, and a translation (tx, ty) described in the
following:⎡
⎢⎣
xL −yL 1 0
yL xL 0 1
x′L −y′L 1 0
y′L x
′
L 0 1
⎤
⎥⎦×
⎡
⎢⎣
m
n
tx
ty
⎤
⎥⎦ =
⎡
⎢⎣
xN
yN
x′N
y′N
⎤
⎥⎦ (22)
where m = s · cos θ, and n = s · sin θ. Equation (22) can be
solved to obtain the parameters m, n, and (tx, ty). In addi-
tion, accordingly, the values s and θ can be computed by the
following:
θ = tan−1
( n
m
)
and s = m
cos θ
. (23)
The above process creates a set of pose parameters
(tx, ty, θ, s) for each match pair of the SIFT feature vectors.
Next, a Hough space is created to find the best affine transform
for the computed pose parameters (tx, ty, θ, s) of all the match
pairs. Each match pair will yield a vote in the Hough space,
CHEN AND TSAI: VEHICLE GUIDANCE FOR INDOOR SECURITY PATROLLING BY SIFT-BASED VEHICLE-LOCALIZATION TECHNIQUE 3269
TABLE II
RESULTS OF PATH-CORRECTION ACCURACY ANALYSIS
B. Path-Correction Accuracy
To show the effectiveness of the proposed path correction
process, we compared the corrected vehicle location yielded
by the process with the one learned in the learning stage at
each of the three selected path nodes N1 through N3. More
specifically, at each node, we put the vehicle at three differ-
ent testing locations described by Sn = (Xn, Yn, θn) in the
RCS. Next, we performed the vehicle-localization and path-
correction processes to guide the vehicle to move toward the
learned location Sr = (Xr, Yr, θr), yielding a corrected loca-
tion S ′n = (X ′n, Y ′n, θ′n). Then, we computed the path-correction
accuracy by finding the location errors (X ′err, Y ′err, θ′err), which
are the absolute differences between the parameters of S ′n and
Sr, respectively. Here, the values of the location parameters
in Sr, Sn, and S ′n were all measured manually. The results
are summarized in Table II, in which the testing locations
are denoted as P11 through P33. From the table, we can see
that the largest distance error ratio in the X- and Y -directions
occurred at P11, which is about 6%, and the largest orientation
error occurred at P21, which is about 2.4◦. The averages of
the distance error ratios in the X- and Y -directions as well
as that of the orientation error are 2.37%, 0.92%, and 0.8◦,
respectively. The standard deviations of these error parameters
are 1.74%, 0.68%, and 0.7◦, respectively. Again, these data
are all small in value. Accordingly, the maximum allowable
between-node vehicle traveling distance may be computed by
40 cm/ sin(0.8◦) = 28.6 m, under the condition that the largest
deviation from the trajectory is set to be the reasonable value of
40 cm in the corridor of our experimental environment, whose
width is 2.5 m. Even in the worst case with an orientation error
of 2.4◦, the distance is 40 cm/ sin(2.4◦) = 9.55 m, which is
Fig. 8. Experimental results of object monitoring and path correction at four
selected path nodes. (a) Labels of monitored objects. (b) Vehicle monitoring the
objects at the nodes. (c) Matching results and the calibration lines used for path
correction. (d) Images of the learned objects.
still larger than the average between-node distance of 7.5 m
in our experimental environment mentioned previously. As a
summary, as these experiment results illustrate, the proposed
method is feasible for guiding the vehicle to accurately navigate
to the next path node in a normal corridor.
C. Indoor Security-Patrolling Navigation
We have also conducted many experiments of complete
security patrolling navigations in our M-shaped corridor envi-
ronment, all according to the learned navigation path, as shown
in Fig. 7. Whenever the vehicle was guided to arrive at a
learned node in front of an object, object image matching was
performed to check the existence of the object. If successful,
the deviation of the trajectory of the vehicle was computed ac-
cording to the matching result, and the proposed path correction
was preformed to continue its navigation on the path; otherwise,
a warning message was issued. Some results, together with
the acquired monitored object images, are shown in Fig. 8,
where Fig. 8(b) shows a view of the vehicle arriving at a node
with a monitored object, Fig. 8(c) shows the matching result
and a computed calibration line in the image, and Fig. 8(d)
shows the object image taken in the learning stage. The average
navigation speed of the vehicle is 0.3 m/s or, equivalently,
1.08 km/h. The average navigation cycle, which includes the
five processes of image taking, object image matching, vehicle
localization, object monitoring, and path correction, is about
CHEN AND TSAI: VEHICLE GUIDANCE FOR INDOOR SECURITY PATROLLING BY SIFT-BASED VEHICLE-LOCALIZATION TECHNIQUE 3271
for vehicle-location estimation, resulting in the improvement of
flexibility in choosing landmarks. By adopting the SIFT and the
Hough transform, as long as a sufficient number of feature pairs
are available, the vehicle can precisely estimate its location with
respect to every monitored object. The experimental results and
the accuracy analysis of the yielded data revealed the feasi-
bility and practicality of the proposed system for smooth and
accurate navigation and object-security monitoring in indoor
environments.
Since the SIFT process in the proposed system can be
substituted separately, more accurate and efficient feature-
matching techniques, like some recently proposed SIFT ac-
celerations and feature-transformation techniques [23]–[25],
may be fitted into the proposed method to yield better results.
Other future works may be directed to extending the proposed
method to deal with more types of objects in more complicated
environments.
REFERENCES
[1] G. N. DeSouza and A. C. Kak, “Vision for mobile robot navigation: A
survey,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 24, no. 2, pp. 237–
267, Feb. 2002.
[2] G. J. Jang, S. H. Lee, and I. S. Kweon, “Color landmark based self-
localization for indoor mobile robots,” in Proc. IEEE Int. Conf. Robot.
Autom., Washington, DC, May 2002, vol. 1, pp. 1037–1042.
[3] S. Segvic and S. Ribaric, “Determining the absolute orientation in a
corridor using projective geometry and active vision,” IEEE Trans. Ind.
Electron., vol. 48, no. 3, pp. 696–710, Jun. 2001.
[4] C. H. Ku and W. H. Tsai, “Obstacle avoidance in person following for
vision-based autonomous land vehicle guidance using vehicle location
estimation and quadratic pattern classifier,” IEEE Trans. Ind. Electron.,
vol. 48, no. 1, pp. 205–215, Feb. 2001.
[5] N. X. Dao, B.-J. You, S.-R. Oh, and M. Hwangbo, “Visual self-
localization for indoor mobile robots using natural lines,” in Proc.
IEEE/RSJ Int. Conf. Intell. Robots Syst., Las Vegas, NV, Oct. 2003, vol. 2,
pp. 1252–1257.
[6] Z. Jia, A. Balasuriya, and S. Challa, “Recent developments in vision
based target tracking for autonomous vehicles navigation,” in Proc. IEEE
Conf. Intell. Transp. Syst., Toronto, ON, Canada, Sep. 2006, pp. 765–770.
[7] K. L. Chiang and W. H. Tsai, “Vision-based autonomous vehicle guid-
ance in indoor environments using odometer and house corner location
information,” in Proc. IEEE Int. Conf. Intell. Inf. Hiding Multimed. Signal
Process., Pasadena, CA, Dec. 2006, pp. 415–418.
[8] C. Micheloni, G. L. Foresti, C. Piciarelli, and L. Cinque, “An autonomous
vehicle for video surveillance of indoor environments,” IEEE Trans. Veh.
Technol., vol. 56, no. 2, pp. 487–498, Mar. 2007.
[9] H. L. Chou and W. H. Tsai, “A new approach to robot location by house
corners,” Pattern Recognit., vol. 19, no. 6, pp. 439–451, 1986.
[10] T. Okuma, K. Sakaue, H. Takemura, and N. Yokoya, “Real-time camera
parameter estimation from images for a mixed reality system,” in Proc.
15th Int. Conf. Pattern Recognit., Barcelona, Spain, Sep. 2000, vol. 4,
pp. 482–486.
[11] J. Huang, C. Zhao, Y. Ohtake, H. Li, and Q. Zhao, “Robot position
identification using specially designed landmarks,” in Proc. IEEE Conf.
Instrum. Meas. Technol., Sorrento, Italy, Apr. 2006, pp. 2091–2094.
[12] C. J. Wu and W. H. Tsai, “Location estimation for indoor autonomous
vehicle navigation by omni-directional vision using circular landmarks on
ceilings,” Robot. Auton. Syst., vol. 57, no. 5, pp. 546–555, May 2009.
[13] D. Xu, L. Han, M. Tan, and Y. F. Li, “Ceiling-based visual positioning
for an indoor mobile robot with monocular vision,” IEEE Trans. Ind.
Electron., vol. 56, no. 5, pp. 1617–1628, May 2009.
[14] Y. Matsumoto, K. Sakai, M. Inaba, and H. Inoue, “View-based approach
to robot navigation,” in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst.,
Takamatsu, Japan, Nov. 2000, vol. 3, pp. 1702–1708.
[15] G. Blanc, Y. Mezouar, and P. Martinet, “Indoor navigation of a wheeled
mobile robot along visual routes,” in Proc. IEEE Int. Conf. Robot. Autom.,
Barcelona, Spain, Apr. 2005, pp. 3365–3370.
[16] A. Remazeilles and F. Chaumette, “Image-based robot navigation from
an image memory,” Robot. Auton. Syst., vol. 55, no. 4, pp. 345–356,
Apr. 2007.
[17] F. Fraundorfer, C. Engels, and D. Nister, “Topological mapping, localiza-
tion and navigation using image collections,” in Proc. IEEE/RSJ Int. Conf.
Intell. Robots Syst., San Diego, CA, Oct. 2007, pp. 3872–3877.
[18] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
Int. J. Comput. Vis., vol. 60, no. 2, pp. 91–110, Nov. 2004.
[19] K. Mikolajczyk and C. Schmid, “A performance evaluation of local de-
scriptors,” in Proc. Int. Conf. Comput. Vis. Pattern Recog., Madison, WI,
Jun. 2003, vol. 2, pp. 257–263.
[20] D. G. Lowe, “Local feature view clustering for 3D object recognition,”
in Proc. IEEE Conf. Comput. Vis. Pattern Recog., Kauai, HI, Dec. 2001,
vol. 1, pp. 682–688.
[21] D. H. Ballard, “Generalizing the Hough transform to detect arbitrary
patterns,” Pattern Recognit., vol. 13, no. 2, pp. 111–122, 1981.
[22] M. A. Fischler and R. C. Bolles, “Random sample consensus: A para-
digm for model fitting with applications to image analysis and automated
cartography,” Commun. ACM, vol. 24, no. 6, pp. 381–395, Jun. 1981.
[23] Y. Ke and R. Sukthankar, “PCA-SIFT: A more distinctive representation
for local image descriptors,” in Proc. IEEE Conf. Comput. Vis. Pattern
Recog., Washington, DC, Jun. 2004, vol. 2, pp. 506–513.
[24] H. Lejsek, F. H. Asmundsson, B. T. Jonsson, and L. Amsaleg, “Scalability
of local image descriptors: A comparative study,” in Proc. 14th Annu.
ACM Int. Conf. Multimed., Santa Barbara, CA, Oct. 2006, pp. 589–598.
[25] J. M. Morel and G. Yu, “ASIFT: A new framework for fully affine invari-
ant image comparison,” SIAM J. Imag. Sci., vol. 2, no. 2, pp. 438–469,
Apr. 2009.
[26] S. Achar and C. V. Jawahar, “Adaptation and learning for image based
navigation,” in Proc. 6th Indian Conf. Comput. Vis. Graph. Image
Process., Bhubaneswar, India, Dec. 2008, pp. 103–110.
Kuan-Chieh Chen received the B.S. and M.S. de-
grees in computer science from National Chiao Tung
University, Hsinchu, Taiwan, in 2005 and 2008,
respectively.
He is currently a Research Assistant with the
Research Center for Information Technology
Innovation, Academia Sinica, Taipei, Taiwan. His
current research interests include computer vision,
robotics, pattern recognition, and image processing,
as well as all of their applications.
Wen-Hsiang Tsai (SM’91) received the B.S. degree
in electrical engineering from National Taiwan
University, Taipei, Taiwan, in 1973, the M.S. degree
in electrical engineering from Brown University,
Providence, RI, in 1977, and the Ph.D. degree in
electrical engineering from Purdue University, West
Lafayette, IN, in 1979.
Since 1979, he has been with National Chiao Tung
University (NCTU), Hsinchu, Taiwan, where he is
currently a Chair Professor of Computer Science. At
NCTU, he has served as the Head of the Department
of Computer Science, the Dean of General Affairs, the Dean of Academic
Affairs, and a Vice President. From 1999 to 2000, he was the Chair of
the Chinese Image Processing and Pattern Recognition Society of Taiwan
and, from 2004 to 2008, the Chair of the Computer Society of the IEEE
Taipei Section in Taiwan. From 2004 to 2007, he was the President of Asia
University, Taichung, Taiwan. He has been an Editor or the Editor-in-Chief of
several international journals, including Pattern Recognition, the International
Journal of Pattern Recognition and Artificial Intelligence, and the Journal of
Information Science and Engineering. He has been a consultant to many major
research organizations in Taiwan. He has published 141 journal papers and
223 conference papers. His current research interests include computer vision,
information security, video surveillance, and autonomous vehicle applications.
Dr. Tsai has received many awards, including the Annual Paper Award
from the Pattern Recognition Society of the USA; the Academic Award
of the Ministry of Education, Taiwan; the Outstanding Research Award of
the National Science Council, Taiwan; the ISI Citation Classic Award from
Thomson Scientific, and more than 40 other academic paper awards from
various academic societies. He is a Life Member of the Chinese Pattern
Recognition and Image Processing Society in Taiwan.
 
catadioptric cameras with their optical axes vertically aligned, 
and affix a pair of such omni-imaging devices to the roof of a 
vehicle to acquire the environment data. One device is affixed 
to the right-front corner of the vehicle’s roof and the other to 
the left-rear corner (see Fig. 1(a)). A new method, which 
combines skillfully the uses of such a pair of omni-imaging 
devices, a so-called pano-mapping technique [12], as well as 
the rotational invariance property of the omni-image to 
compute the 3D data of real-world points, is then proposed. The 
computation is based on table lookup and analytic formulas and 
so can be carried out in real-time to construct both top- and 
perspective-view images for instantaneous observation of the 
vehicle’s surrounding environment. Also, a vision-based 
scheme using a vertical-line property in the omni-image for 
detecting passing-by persons, computing their locations and 
heights, and displaying such feature data both from the top and 
the perspective views for convenient inspection is proposed as 
well, which are not found in previous works. A detailed 
comparison of the proposed method with related existing 
methods will be given later. 
In the remainder of this paper, the configuration of the 
proposed system, the design of the omni-camera system, and 
the proposed technique for computing 3D data are described in 
Section II. The proposed techniques for detection and 
monitoring of a passing-by person are described in Section III. 
And the schemes proposed for generating top- and 
perspective-view images are described in Section IV, followed 
by some experimental results and a comparison of the proposed 
method with related methods in Section V. Finally, conclusions 
and descriptions of the contributions made in this study are 
given in Section VI. 
 
 
(a) (b) 
Figure 1.  Video surveillance vehicle and network used in this study. (a) Vehicle 
with omni-cameras (circled in red). (b) Network connecting computers. 
II. SYSTEM CONFIGURATION AND DESIGN OF PROPOSED 
OMNI-IMAGING DEVICES 
A. System Configuration 
As illustrated in Fig. 1(b), the proposed system used in this 
study includes a video surveillance vehicle, a pair of 
two-camera omni-imaging devices CSA and CSB affixed on the 
vehicle’s roof, and two laptop computers COMA and COMB 
inside the vehicle. Each two-camera omni-imaging device is 
controlled by a laptop computer, and a local network was 
designed to connect the two computers. Specifically, COMA is 
used to display the top-view image of the surrounding area of 
the video surveillance vehicle, and COMB to display the 
perspective-view images of specified directions outside the 
surveillance vehicle. The system process is divided into two 
phases, the learning phase and the patrolling phase. In the 
former, the pano-mapping tables of the used omni-cameras are 
constructed in advance, and in the latter, the video surveillance 
vehicle is driven in the outdoor environment for real security 
monitoring applications. 
B. Camera Design Principle 
The structure of each catadioptric omni-camera with a 
reflective mirror of the hyperboloidal shape used in this study is 
illustrated in Fig. 2, where the world coordinate system (WCS) 
is specified by (X, Y, Z) with its origin Om located at the mirror 
base center which we assume to be coincident with one focal 
point of the hyperboloidal shape of the mirror. The shape of the 
mirror in the camera coordinate system (CCS), which is 
specified by (x, y, z) with its origin Oa located at the middle 
point between the point Om and the lens center of the projective 
camera, may be described [12] as 
2 2
2 2
2 2 1,
s z s x y
a b
 (1) − = − = +
where a and b are two parameters of the hyperboloidal shape. 
The parameter d, as shown in Fig. 2(b), is the distance between 
the camera lens center and the mirror base center, whose value 
can be obtained by a simple formula d = 2c with c2 = a2 + b2. Also, 
the axis of the camera is aligned with that of the mirror, and the 
lens center is fixed at the other focal point of the mirror shape. 
 
  
(a) (b) 
Figure 2.  Omni-camera structure. (a) Omni-camera geometry. (b) Geometry 
between hyperboloidal-shaped mirror and projective camera. 
By the shape geometry of a hyperboloid described by (1), the 
value α specifying the elevation angle of a real-world point P 
shown in Fig. 2(a) can be computed [10] by: 
2 2
2 2
( ) sin 2tan
( ) cos
b c bc
b c
+ −= −
βα β  (2) 
where the angle β as shown in Fig. 2(a) can be computed as: 
β = π /2 − θ (3) 
with  
θ = tan−1(r/2c) (4) 
where r is the distance of P to Om on the XY plane and equals the 
radius of the circular mirror base when α = 0. In Fig. 2(b), by the 
principle of similar triangles, we have 
w
d f
r S
=  (5) 
Computer A 
Cross-over 
cable 
Camera system B Video surveillance vehicle 
Computer B 
Camera system A 
 
 
to be located in entry Eij with coordinates (uij, vij) in Table 1, the 
azimuth-elevation angle pair of the corresponding real-world 
point P can be obtained to be (θi. αj). Note that this 
azimuth-elevation angle pair only says that P is located on a 
light ray R with the azimuth direction θi and the elevation angle 
αj; it does not specify the 3D position of P sufficiently ⎯ any 
real-world point on light ray R will appear identically to be the 
pixel p in the image with the same coordinates (uij, vij). Also 
note that the pano-mapping table involves no camera parameter 
and is invariant in nature with respective to the camera position 
(i.e., it is not changed when the camera is moved around). 
Back to the discussion about computing the 3D coordinates 
for a real-world point P in the WCS. Since two omni-cameras 
are used in each imaging device, after the two image pixels p1 
and p2 corresponding to real-world point P are identified in the 
two omni-images taken by the two cameras, two elevation 
angles α1 and α2 as shown in Fig. 4(a) can be obtained by the 
above table lookup process using the image coordinates (u1, v1) 
and (u2, v2) of p1 and p2, respectively. With the upper mirror 
base center being located at the WCS origin with world 
coordinates (0, 0, 0), the goal of 3D data computation now can 
be achieved by using α1 and α2 to compute the world 
coordinates (X, Y, Z) of P. For this, as shown in Fig. 4(b), by the 
law of sines in trigonometry, we have: 
2 1sin(90 ) sin( )
Pd e=+ − 2α α α
 (7) 
where dP is the distance between P and the base center of the 
upper mirror C1, and e is the disparity between the two cameras 
in the omni-imaging device, which is measured manually in 
advance. Eq. (7) may be reduced to get dP as: 
1 1 2
1
cos tan tanP
ed α α α= × − . (8)
 
And so the horizontal distance dw and the vertical distance Z of 
P as shown in Fig. 4(a) may be computed respectively to be: 
w 1
1 2
cos
tan tanP
d d α ;eα α= = −  
1
1
tansinP
eZ d
1 2tan tan
αα α α
×= = − .
 (9) 
Furthermore, according to Fig. 3, the azimuth angle θ in the 
figure can be described in terms of the pixel coordinates (u, v) 
as follows: 
2 2 2 2
u
u v u v+ +sin ; cos
vθ θ= =  (10) 
from which the value of θ in the ICS can be computed. 
Now, according to the characteristic that the axes of the 
cameras are aligned with the axis of the mirror, the rotational 
invariance property of the omni-camera says that the azimuth 
angle of point P in the WCS and the azimuth angle of the 
corresponding pixel p in the ICS are identical [12]. Denoting 
both of the angles by θ, we can compute the values of  X and Y 
in the WCS as: 
 
w
1 2
cos ;
tan tan
X d cose θθ α α= × = −
×  (11) 
 w
1 2
sinsin
tan tan
eY d θθ α α
×= × = − . (12) 
In summary, given a pair of matching image points 
corresponding to a real-world point P, we can compute the 
azimuth θ of P by Eqs. (10), and obtain also a pair of elevation 
angles α1 and α2 of P by pano-mapping table lookup. Then, the 
world coordinates (X, Y, Z) describing the unique 3D position 
of P can be computed by Eqs. (9), (11), and (12) analytically. 
III. AUTOMATIC DETECTION OF PASSING-BY PERSONS 
A. Detection of Moving Objects in Omni-images 
Before extracting moving objects around a surveillance 
vehicle, background images without objects are captured in 
advance in the learning phase with the vehicle in a static state, 
or, whenever necessary, in the patrolling phase with the vehicle 
also in a static state. An example is shown in Fig. 5. Then, 
foreground images possibly with objects are taken in real-time 
in the patrolling phase. Both background and foreground 
images are color ones, which are transformed into grayscale 
ones at the beginning of the object detection process. By 
subtracting the background image from the foreground one, a 
difference image is obtained. Because there usually exist noise 
pixels in the difference image, such as those caused by light 
variations, the difference image is thresholded next into a 
binary one to eliminate such noise pixels using the 
moment-preserving thresholding technique proposed by Tsai 
[13]. After these steps, we can obtain a bi-level image Ibl with 
detected moving objects all labeled as black pixels. An example 
of the result is shown in Fig. 6. 
 
  
(a) (b) 
Figure 5.  Background images taken by a two-camera imaging device. (a) 
Background taken by upper camera. (b) Background taken by lower camera. 
  
(a) (b) 
  
(c) (d) 
Figure 6.  Passing-by person detection. (a) Background image. (b) Foreground 
image. (c) Difference image obtained by subtracting (b) from (a) (d) Binary 
image obtained by bi-level thresholding. 
 
 
cos ; sin .u r v r
θ  = cos−1(X/dw) = sin−1(Y/dw). (15) 
On the other hand, because dw = dh×cotα where dh is the height 
of the upper camera’s mirror base, we can compute the 
elevation angle α of P by: 
α  = tan−1(dh/dw). (16) 
Furthermore, using the radial stretching function r = fs(α) 
mentioned previously, the radial distance r corresponding to α 
can be derived. Accordingly, by the rotational-invariance 
property of omni-imaging, the coordinates (u, v) of the image 
pixel p corresponding to P can be obtained from (15) as: 
θ θ= =
feasible to superimpose a real surveillance vehicle shape Srv on 
ch 
pi
 (17) 
As a result, a complete top-view image can be computed. A 
top-view image of a parking area so computed with the 
surveillance vehicle in the middle is shown in Fig. 10. Note that 
all the cars appearing in the figure are distorted, and this 
phenomenon comes from their violation of the assumption that 
the processed points are projections of real-world points on the 
ground, as mentioned previously. However, the result is still 
good for visual inspection of the vehicle’s surround. 
 
  
(a) (b) 
Figure 10.  An omni-image and its corresponding top-view images. (a) 
Omni-image. (b) Top-view image obtained from backward mapping. 
B. Merging of two top-view images into a single one 
By the way describe above, we can construct two top-view 
images It1 and It2 using the omni-images taken from the two 
upper cameras, like those shown in Figs. 11(a) and 11(b). It is 
assumed that the relative position of the two upper cameras on 
the vehicle roof are measured manually in advance and 
described by a pair of offsets (CW, CL) with CW and CL being 
the horizontal and vertical distances between the two imaging 
devices, respectively. It is desired to merge the two top-view 
images It1 and It2 into a single one, denoted as Iint, to get a wider 
FOV of the vehicle’s surround. To do this, we divide first the 
FOV covered by the desired top-view image Iint into two parts: 
one from the upper half of It1, and the other from the lower half 
of It2. All the pixels in the latter part then are shifted for the 
offset (CW, CL) before being merged into the former. An 
example of a top-view image so obtained is shown in Fig. 11(c). 
To obtain a better-looking top-view image, an elliptical shape is 
used further as a viewing window, and all the pixels outside the 
window are discarded, yielding an elliptical-shaped image. For 
Fig. 11(c), the result of this process is shown in Fig. 11(d). 
C. Superimposition of Surveillance Vehicle Shape on 
Top-view Image 
Because the imaging devices are affixed on the vehicle’s 
roof, the vehicle shape in the top-view images always appears 
at fixed locations in the taken omni-images. Therefore, it is 
the elliptical-shaped top-view image like that in Fig. 11(d) as 
the central landmark of the image, making observation of the 
vehicle’s position in the surround more convenient. For this, 
the following steps are conducted: (1) trace and segment out 
manually the real top-view shapes S1 and S2 of the surveillance 
vehicle in the two top-view images It1o and It2o, respectively in 
the learning phase; (2) superimpose S1 and S2 respectively on It1 
and It2 acquired in each patrolling phase to segment out by 
image matching, and mark in yellow the surveillance vehicle’s 
shape area A in It1 and It2 before they are merged into one, 
resulting in a figure like that shown in Fig. 12(a); (3) apply a 
texture synthesis scheme to fill ground texture into the marked 
surveillance vehicle shape area A; and (4) superimpose a 
pre-segmented real top-view vehicle shape Srv like that shown 
in Fig. 12(b) on the resulting image of the last step, yielding a 
better-looking output image like that shown in Fig. 12(c). Note 
that Srv is obtained in the learning phase by manual 
segmentation from a top-view image like that of Fig. 11(a). 
Step (3) above is carried out by the following way: for ea
xel pc in the surveillance vehicle shape A, find the pixel pn 
which is outside A and closest to pc, and use the color of pn to 
fill up pc. Also, the relations of pixels in A with those outside A 
are kept in a table for use in the later top-view image generation 
process to improve the top-view image display speed. 
 
  
(a) (b) 
  
(c) (d) 
Figur uction. (a) Right-front top-view im  (b) 
 
Figur posing real surveillance v top-view 
D. Generation of Perspective-view Images 
The construction of a perspective-view image IP from an 
e 11.  Top-v  image constriew age.
Left-rear top-view image. (c) Integrated top-view image (red dots: centers of 
original images). (d) Top-view image viewed in an elliptical shape. 
 
 
(a) (b
e 12.  Superim
 
  
) (c) 
ehicle shape onto 
image. (a) Top-view image with vehicle shape marked in yellow. (b) 
Pre-segmentd real shape of video surveillance vehicle. (c) Result of ground 
texture filling and superimposition of real surveillance vehicle shape. 
 
 
two 
variables θmouse an
 
(a) (b) 
increases gradually as the mouse moves from top to bottom in 
Fig. 15(a); and then modify Eq. (19) to be: 
α = tan−1(Hp/L) + αmouse. (21) 
The user interface becomes friendlier after adding the 
d αmouse, and a user of the surveillance vehicle 
can now choose any view direction conveniently by mouse 
clicking (or panel touching) to observe the corresponding 
perspective-view image of a scene of his/her interest. Two 
more experimental results of perspective-view images 
generated in this way are shown in Figs. 15(c) and 15(d). 
 
   
  
(c) (d) 
Figure 15.  Corresponding omni-image and perspective-view image.
perspective-view image. (b) Omni-image part (enclosed roughly by e ed 
triangle) from whi ) was generated. (c) and (d) Two o spective-view 
ISCUSSIONS 
A. Experimental Results of 3D Data Computation 
on of 
s of some 
re
ion pattern using the 
ta
Figure 16.  Omni-ima  with black and white grids and 
black cross shapes on a wall taken by a laid on the ground. 
 (a) A 
h r
 
 t
ch (a ther per
images generated from (b) by mouse clicking on (a). 
V. EXPERIMENTAL RESULTS AND D
At first, we conducted an experiment to test the precisi
the computed 3D data by computing the position
al-world points and compared them with those obtained 
manually. We laid an omni-imaging device on the ground and 
took images of a calibration pattern with alternating black and 
white grids and some black cross shapes as shown in Fig. 16. 
The width between every two consecutive grids on the pattern 
is 5cm, and the bigger width between every two crosses is 25 
cm. We picked out 15 pairs of corresponding pixels from these 
grids and shapes in the two taken images. An example is shown 
in Fig. 17. We then calculated, by Eqs. (9) in Section II.C, the 
horizontal distance dw and the height Z of the real-world point 
corresponding to each pair of corresponding image pixels. In 
addition, to increase the computation accuracy, we used four 
radial stretching functions instead of just one in the 
pano-mapping process, each function dealing with a quarter of 
the omni-image, as shown in Fig. 18. This measure is especially 
necessary when the assumption of radial symmetry of the 
mirror surface mentioned previously is not valid, which 
happens to be the case encountered in this study because of 
imperfect manufacturing of the mirrors. 
The results of calculations of the 3D data, dw and Z, of 15 
selected landmark points on the calibrat
ken images are shown in Tables 2 and 3, which resulted 
respectively from the use of the pano-mapping tables of Tables 
A and B depicted in Fig. 18. As can be seen, the differences 
between the measured data and the computed ones are all small, 
resulting in small RMSE values (each defined to be the square 
root of the average of all the difference values). All the error 
rates (each defined to be the ratio of the RMSE over the average 
measured data value) can be seen to be about 5%, which are 
good for practical applications. We also computed the values of 
the correlation coefficient between the difference values and 
the azimuth/elevation angles, respectively, to see the affections 
of such angles to the 3D computation results. The computed 
correlation coefficient values show that only the variations of 
the elevation angle have medium influences (with correlation 
coefficient values of −0.48 and −0.41) on the precisions of the 
computed location values (dw) of the real-world points. 
θ
 
ge of a calibration pattern
 an omni-camer
  
(a) (b) 
Figure 17.  Illustration of picked out pairs of corresponding pixels in two 
omni-images (mar  two d dots). ked by  re
 
Figure 18.  Four image regions corresponding to four radial stretching 
functions used in constructing pano-mapping tables.
Point No. o o
d 3D real
 
Table 2. Statistics of computed 3D data using pano-mapping table Table A. 
Azimuth  Elevation (1) Measured 3D real- (2) Compute
angle ( ) angle ( ) world point data (cm) 
-
world point data (cm) (1) − (2) (cm) 
(3) Differences     
 θ  α dw Z dw Z dw Z 
1 
2 
0.00  
0.00  
54.95 
52.96 
249 355 
249 330 
250.2 355.4 
251.4 331.4 
-
-2
0. 50.77 310.2 -5.20  
227.96 
203.45 
154.89 
Average
1.20  -0.40  
.40  -1.40  
3 00  249 305 255 -6.00  
4 0.00  48.35 249 280 260.6 290.7 -11.60  -10.70  
5 0.00  45.68 249 255 247.6 253.6 1.40  1.40  
6 0.00  42.73 249 230 257.7 237.2 -8.70  -7.20  
7 0.00  39.46 249 205 236.6 197 12.40  8.00  
8 0.00  37.35 249 190 227.7 175 21.30  15.00  
9 0.00  34.32 249 170 223 154.1 26.00  15.90  
10 0.00  31.07 249 150 231.8 140.5 17.20  9.50  
11 0.00  27.57 249 130 238 124.2 11.00  5.80  
12 10.69  54.45 319 219.5 300.2 8.46  18.80  
13 12.00  57.47 319 185.5 286.7 17.95  32.30  
14 15.85  64.10 319 156.1 323.3 -1.21  -4.30  
15 18.84  67.67 131.02 319 130.5 321.2 0.51  -2.20  
  230.42 258.4     
R  
Error rate =  A
ang
MSE       12.52 12.336 
 RMSE /
A
verage 
 
    5.43% 
-0.03 
4.77% 
0.17 zimuth les     Correlation 
coefficient
M
Elevation angle     -0.48 -0.18 
M
x 
 
 
ral differencing after 
tion-fixed virtual perspecti
ey paper) with t
s been 
designed properly for use on the top of a video surveillan
vehicle to m he two devices 
af
 third generation 
surveillance systems,” Pr 9, no. 10, pp. 1355–1539, Oct. 
2001. 
-352, Aug. 2004. 
t. 2006, pp. 293-308. 
of indoor vision-based 
71. 
 30-Aug. 1, 2003. 
ia, BC, Canada, 2008. 
-preserving thresholding: a new approach,” 
1, pp. 223-243. 
 and M. M. Trivedi, “Motion analysis for event detection and 
tracking with a mobile omni-directional camera,” Multimedia Systems 
Journal, vol. 10, no. 2, 2004, pp. 131-143. 
covers the entire vehicle surrounding. Liu, et al. [23] affixed six 
fisheye cameras around a car to generate a bird’s-eye-view 
image of the surround for driving assistance. No 3D data 
computation was considered, contrasting with our method 
which computes 3D feature point data. 
(e) About human detection ⎯ Liu et al. [24] detected human 
motion with an omni-camera on a mobile robot using 
ego-motion compensation and tempo
unwarping omni-images. The proposed method of this study 
instead detects passing-by persons directly from omni-images. 
Ng, et al. [25] used multiple omni-vision sensors to synthesize 
perspective views and track human activities using N-ocular 
stereo techniques without acquiring range information. In 
contrast, our method detects human beings in vehicle 
surroundings and computes their 3D features (location and 
height) for various applications. 
(f) About perspective-view image generation ⎯ Huang et al. 
[26] presented an in-car omni-imaging system which processes 
acquired videos to obtain direc ve 
views on the driver, passengers, and frontal scenes using pan, 
tilt, and zoom parameters, in contrast with our method which 
generates direction-changing perspective views on passing-by 
persons without using camera parameters. Ng, et al. [25] 
generated images of both a walking person’s view and an 
observer’s view using a range-space search technique, while 
our method uses the pano-mapping approach. Kawasaki et al. 
[27] obtained 3D information by spatio- temporal analysis of 
omni-images using calibrated camera parameters, contrasting 
again with the tabular pano-mapping technique used by our 
method, involving no camera parameter. 
(g) Comparisons about implemented functions ⎯ The result of 
a more detailed comparison of the previously-mentioned 
methods (except [17] which is a surv he 
proposed one in terms of a set of implemented functions is 
listed in Table 4, from which it can be seen that the proposed 
method has integrated more functions than the others. 
VI. CONCLUSIONS 
A pair of two-camera omni-imaging devices ha
ce 
[
onitor passing-by persons. T are 
fixed to the right-front and left-rear of the vehicle roof 
efficiently to facilitate generation of a top-view image which 
covers the vehicle surrounding area. A new 3D data 
computation technique based on the pano-mapping concept and 
the rotational invariance property of the omni-image has been 
proposed. Because of the use of table lookup and analytic 
computation formulas, the technique can be implemented to 
satisfy real-time applications. A passing-by person appearing 
around the surveillance vehicle can be detected automatically 
using an omni-image property about upright objects, with 
his/her location and body height computed by the proposed 3D 
data acquisition technique. A top-view image of the vehicle’s 
surrounding area with a real vehicle shape inserted properly in 
the middle is generated by registering two omni-images taken 
by two upper cameras. Perspective-view images covering the 
detected person or any interesting scene spot can also be 
generated in real-time automatically or by mouse clicking for 
convenient inspection. The system is useful for many security 
monitoring applications around the video surveillance vehicle. 
The experimental results show the feasibility and precision of 
the proposed method for practical applications. 
REFERENCES 
[1] C. Regazzoni, V. Ramesh and G. Foresti, “Special issue on video 
communications, processing, and understanding for
oc. of IEEE, vol. 8
[2] W. Hu, T. Tan, L. Wang and S. Maybank, “A survey on visual 
surveillance of object motion and behaviors,” IEEE Trans. on Systems, 
Man, and Cybernetics, Part C: Applications and Reviews, vol. 34, no. 3, 
pp. 334
[3] T. Gandhi and M. M. Trivedi, “Vehicle surround capture: survey of 
techniques and a novel omni-video-based approach for dynamic 
panoramic surround maps,” IEEE Trans. on Intelligent Transportation 
Systems, vol. 7, no. 3, Sep
[4] C. Micheloni, G. L. Foresti, C. Piciarelli and L. Cinque, “An autonomous 
vehicle for video surveillance of indoor environments,” IEEE Trans. on 
Vehicular Technology, vol. 56, no. 2, pp. 487-498, March 2007. 
[5] K. C. Chen and W. H. Tsai, "Guidance 
autonomous vehicles for security patrolling by a SIFT-based vehicle 
localization technique using along-path object information," IEEE Trans. 
on Vehicular Technology, vol. 59, no. 7, Sept. 2010, pp. 3261-32
[6] Y. Onoe, N. Yokoya, K Yamazawa, and H. Takemura, “Visual 
surveillance and monitoring system using an omnidirectional video 
camera,” Proc. of 1998 Int’l Conf. Pattern Recog., Vol. 1, pp.588-592, 
Brisbane, Australia, Aug. 16-20, 1998.  
[7] T. Mituyosi, Y. Yagi, and M. Yachida, “Real-time human feature 
acquisition and human tracking by omnidirectional image sensor,” Proc. 
of IEEE Int’l Conf. on Multisensor Fusion & Integration for Intelligent 
Systems, pp. 258-263, Tokyo, Japan, July
[8] S. Morita, K. Yamazawa, and N. Yokoya, “Networked video surveillance 
using multiple omnidirectional cameras,” Proc. of 2003 IEEE Int’l 
Symp.on Computational Intelligence in Robotics & Automation, vol. 3, pp. 
1245-1250, Kobe, Japan, July 16-20, 2003.  
[9] H. Koyasu, J. Miura and Y. Shirai, “Real-time omnidirectional stereo for 
obstacle detection and tracking in dynamic environments,” Proc. of 2001 
IEEE/RSJ Int’l Conf. on Intelligent Robots & Systems, vol. 1, pp. 31-36, 
Maui, Hawaii, USA, Oct. - Nov., 2001. 
[10] H. Ukida, N. Yamato, Y Tanimoto, T Sano, and H. Yamamoto, 
“Omni-directional 3D measurement by hyperbolic mirror cameras and 
pattern projection,” Proc. of 2008 IEEE Conf. on Instrumentation & 
Measurement Technology, pp. 365-370, Victor
11] J. I. Meguro, J, I, Takiguchi, and Y, A, T, Hashizume, “3D reconstruction 
using multi-baseline omni-directional motion stereo based on GPS/DR 
compound navigation system,” Int’l J. of Robotics Research, vol. 26, no. 6, 
pp.625-636, June 2007. 
[12] S. W. Jeng and W. H. Tsai, “Using pano-mapping tables for unwarping of 
omni-images into panoramic and perspective-view images,” J. of IET 
Image Proc., vol. 1, no. 2, pp. 149-155, June 2007. 
[13] W. H. Tsai, “Moment
Computer Vision, Graphics, & Image Proc., vol. 29, pp. 377-393, 1985. 
[14] R.C. Gonzalez, R.E. Woods, Digital Image Processing, Prentice-Hall: 
Englewood Cliffs, NJ, USA, 2002. 
[15] S. Baker and S. Nayar, “A theory of single-viewpoint catadioptric image 
formation,” Int’l J. of Computer Vision, vol. 35, no. 2, 1999, pp. 175-196. 
[16] C. Geyer and K. Daniilidis, “Catadioptric projective geometry,” Int’l J. of 
Computer Vision, vol. 45, no. 3, 200
[17] Z. Xiong, W. Chen and M. J. Zhang, “Catadioptric omni-directional 
stereo vision and its applications in moving objects detection,” in 
Computer Vision, Z. Xiong, ed., InTech: Rijeka, Croatia, Nov. 2008, pp. 
493-518. 
[18] L. C. Su, C. J. Luo and F. Zhu, “Obtaining obstacle information by an 
omnidirectional stereo vision system,” IEEE Int’l Conf. on Information 
Acquisition, Aug. 20-23, 2006, Weihai, Shandong, China, pp. 48-52. 
[19] T. Gandhi
Copyright (c) 2011 IEEE. Personal use is permitted. For any other purposes, Permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.
Copyright (c) 2011 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from the IEEE by sending an email to pubs-permissions@ieee.org. 
 
Abstract—A new space-mapping method for object location 
estimation, which is adaptive to camera setup changes, for use in 
various automation applications is proposed. The location of an 
object appearing in an image is estimated by mapping image 
coordinates of object points to corresponding real-world coordi-
nates using a mapping table, which is constructed in two stages, 
with the first stage for establishing a basic table using bilinear 
interpolation in the camera manufacturing environment and the 
second for adapting the table to changes of camera heights and 
orientations in the application field. Analytic equations for table 
adaptation are derived by skillful utilization of both image for-
mation and camera geometry properties. Good experimental 
results are shown to prove the feasibility of the proposed method. 
 
Index Terms—object location estimation, space mapping, table 
adaptation, camera setup changes. 
 
I. INTRODUCTION 
Video cameras are used in various automation applications, 
including automatic estimations of object locations in indoor 
environments using object images acquired by cameras affixed 
to walls or ceilings. A conventional solution to the object lo-
cation estimation problem is to conduct camera calibration to 
obtain a set of camera parameters, followed by using the pa-
rameters to compute the object location [1-6]. Most camera 
calibration methods use landmarks to compute camera para-
meters [4, 5], and the process is generally complicated. A 
flexible method based on this approach is proposed recently by 
Zhang [6], which only requires the camera to observe a planar 
pattern shown in a few different orientations. An alternative 
approach to object location estimation is to use a 
space-mapping table [7-10] which transforms the image space 
into the real-world space, thus avoiding the work of computing 
camera parameters. The table is constructed with the aid of a 
calibration pattern before the camera is deployed in an appli-
cation environment. 
 
†
 This work was supported financially by the Ministry of Economic Affairs 
under Project No. MOEA 97-EC-17-A-02-S1-032 in Technology Development 
Program for Academia. 
C. J. Wu, Department of Computer Science, National Chiao Tung University, 
Hsinchu 30010, Taiwan; e-mail: gis91813@cis.nctu.edu.tw; Tel: 
+886-3-5131545; fax: +886-3-572-1490. 
W. H. Tsai, Department of Computer Science, National Chiao Tung Uni-
versity, Hsinchu 30010, Taiwan; email: whtsai@asia.edu.tw; Tel: 
+886-3-5721489; fax: +886-3-572-1490; also with the Department of Infor-
mation Communication, Asia University, Taichung 41354, Taiwan. 
The space-mapping based approach to object location esti-
mation, however, is sensitive to camera setup changes. That is, 
after a space-mapping table is constructed for a specific camera 
setup according to a certain camera-environment configuration, 
the camera should be used in identical configurations thereafter; 
otherwise, the table will not work. This weakness causes in-
convenience in using the camera in various application fields. 
To solve such a camera-setup sensitive problem for the 
space-mapping approach, one way is to construct a new table 
for every new camera-environment configuration. But this is 
often difficult to carry out after the camera is delivered to a user 
who does not know the mapping table construction process 
or/and has no calibration pattern for use in the field. In this 
study, we investigate the possibility of automatically modify-
ing the original space-mapping table for use in the new envi-
ronment. Note that this problem of adapting the space-mapping 
table to new camera setups has not been studied so far. The 
camera is assumed to be general in type with a fixed focal 
length and affixed to a ceiling. In case sharpness of taken im-
ages is concerned, a wide-angle camera with a small hyperfocal 
length of just a few inches may be used, which always takes 
sharp scene images at distances beyond a half of the hyperfocal 
length. 
In the following, we first describe the idea and the detail of 
the proposed method in Sections II and III, respectively. Some 
experimental results are given in Section IV, followed by con-
clusions in Section V. 
II. IDEA OF PROPOSED METHOD 
The proposed method includes two stages, one conducted in 
an in-factory environment and the other in an in-field one. The 
details are described in the following algorithm. See Fig. 1 for 
an illustration.  
Algorithm 1. Object location estimation by space-mapping 
table construction and modification. 
Stage 1. Construction of a basic space-mapping table in the 
factory (see Fig. 1(a)). 
Step. 1 Affix the camera to the ceiling at a certain height H0 
with the optical axis of the camera pointing to the floor 
perpendicularly. 
Step. 2 Place a calibration pattern O on the ground right under 
the camera, take an image of it, extract relevant feature 
points from the image, and find the coordinates of them. 
Step. 3 Measure the real-world coordinates of those feature 
A Space-Mapping Method for Object Location Estimation 
Adaptive to Camera Setup Changes for Vision-based  
Automation Applications† 
Chih-Jen Wu and Wen-Hsiang Tsai
Copyright (c) 2011 IEEE. Personal use is permitted. For any other purposes, Permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.
 3
1p 2
p
4p
3p
1P 2P
4P3P
 
(a) 
 
(b) 
Fig. 3. Quadrilateral mapping. (a) Mapping of corresponding quadrilaterals in 
the image and the calibration pattern. (b) Location estimation of a space point 
by inverse bilinear interpolation. 
Now, assume that the camera is affixed to the ceiling diffe-
rently with a tilt angle of θ and a height of L with respect to 
floor F1, as shown in Fig. 4(b). Here, the location of object 
point P1 on F1 to be estimated is specified by the real-world 
coordinates (x1, y1) with respect to the downward projection 
point O of the camera’s lens center onto F1, where the x-axis is 
assumed to be coincident with the projection of the camera’s 
optical axis on F1. Let the coordinates of P1 in the acquired 
image be (u, v). Again the basic table is inapplicable here; the 
table lookup results, namely, the real-world coordinates (x0, y0), 
are actually those of a real-world point on a floor F0 at a dis-
tance H0 to the camera’s lens center, instead of being the de-
sired real-world coordinates (x1, y1) of P1 on F1. Again, table 
modification is necessary here, which is called camera orien-
tation adaptation in Step 6 of Algorithm 1. 
To correct the values (x0, y0) into (x1, y1), we rotate F1 
through an angle of 90o − θ with P1 as the rotation pivot point, 
such that the resulting plane F1' becomes perpendicular to the 
camera’s optical axis and the lateral view of the rotation result 
seen from the positive y-axis direction becomes the one shown 
in Fig. 5. The original floor F0 is also shown in the figure. 
Assume that the distance of P1 on F1' to the camera’s optical 
axis is x'. Then, according to the concept of side proportionality 
again, we have 
0 0
1
x H
x' H
= . (2) 
Also, by geometry and trigonometry we have 
sin
x'
M
θ = ; (3) 
0
sin
L
N H
θ = + ; (4) 
1
0
cos
x M
N H
θ −= + ; (5) 
1 0( )cos
H N H
M
θ − += . (6) 
From (4) and (5), we get N + H0 = L/sinθ = (x1 − M)/cosθ, or 
equivalently, 
(x1 − M)sinθ = Lcosθ.  (7) 
Also, from (2) and (3), we get x0/sinθ  = MH0/H1, or equiva-
lently, 
0
1
0
sinH
H
x
M θ= ; (8) 
From (4), (6), and (8), we get 
0
0 0sin sin cos
L x
M
H xθ θ θ= × − . (9) 
And from (7) and (9), we get one of two desired coordinates: 
x1 = 0 0
0 0
cos sin
sin cos
H x
L
H x
θ θ
θ θ
+× − . (10) 
 
(a) 
 
(b) 
Fig. 4. Illustration of camera setup changes. (a) Use of side propor-
tionality to compute coordinates of point P1 on a floor F1 with a ceiling 
height H1. (b) A camera with tilt angle θ. 
On the other hand, because the x-axis on F1 is assumed to be 
coincident with the projection of the camera’s optical axis on F1 
Copyright (c) 2011 IEEE. Personal use is permitted. For any other purposes, Permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.
 5
heights and orientations, which often occur in different appli-
cation environments. Experimental results show that the me-
thod yields results with error ratios smaller than 5% in most 
cases, meaning the practicality of the method for various ap-
plications. 
         
(a)                                    (b) 
  
(c)                                        (d) 
Fig. 6. Fish-eye camera and images used for experiments. (a) Camera setup. 
(b)-(d) Images taken respectively with the camera looking downward at heights 
200cm and 250cm, and tilted for 50o at height 200cm. 
ACKNOWLEDGEMENTS 
A preliminary version of this paper appeared in Know-
ledge-Based & Intelligent Information & Engineering Systems 
(Proc. KES 2009), LNCS, Vol. 5712, pp. 395-402, Sept. 2009. 
REFERENCES 
[1] Yang and W. H. Tsai, “Viewing corridors as right parallelepipeds for 
vision-based vehicle localization,” IEEE Trans. on Industrial Electronics, 
Vol. 46, No. 3, pp. 653-661, June 1999. 
[2] E. E. Hemayed, “A survey of camera self-calibration,” Proc. IEEE Conf. on 
Advanced Video & Signal Based Surveillance, pp. 351- 357, Miami, 
Florida, USA, July 21-22, 2003. 
[3] O. A. Aider, P. Hoppenot, and E. Colle, “A model-based method for indoor 
mobile robot localization using monocular vision and straight-line cor-
respondences,” Robotics & Autonomous Systems, Vol. 52, Issues 2-3, pp. 
229-246, Aug. 2005. 
[4] M. Betke and L. Gurvits, “Mobile robot localization using landmarks,” 
IEEE Trans. on Robotics & Automation, Vol. 13, No. 2, pp. 251-263, April 
1997. 
[5] H. L. Chou and W. H. Tsai, “A new approach to robot location by house 
corners,” Pattern Recognition, Vol. 19, No. 6, pp. 439-451, 1986. 
[6] Z. Zhang, "A flexible new technique for camera calibration", IEEE Trans. 
on Pattern Analysis & Machine Intelligence, Vol. 22, No.11, pp. 
1330-1334, 2000. 
[7] H. C. Chen and W. H. Tsai, “Optimal security patrolling by multiple 
vision-based autonomous vehicles with omni-monitoring from the ceil-
ing,” Proc. 2008 Int’l Computer Symp., Taipei, Taiwan, Nov. 13-15, 2008. 
[8] S. W. Jeng and W. H. Tsai, "Using pano-mapping tables to unwarping of 
omni-images into panoramic and perspective-view Images," IET Image 
Processing, Vol. 1, No. 2, pp. 149-155, June 2007. 
[9] Y. T. Wang and W. H. Tsai, “Indoor security patrolling with intruding 
person detection and following capabilities by vision-based autonomous 
vehicle navigation,” Proc. 2006 Int’l Computer Symposium (ICS 2006) - 
Workshop on Image Processing, Computer Graphics, & Multimedia 
Technologies, Taipei, Taiwan, Dec. 4-6, 2006.  
[10] T. Takeshita, T. Tomizawa and A. Ohya, “A house cleaning robot system – 
path indication and position estimation using ceiling camera,” Proc. Int’l 
Joint Conf. on SICE-ICASE, pp. 2653-2656, Busan, Korea, Oct. 18-21, 
2006. 
[11] S. Y. Park, S. C. Jung, Y. S. Song and H. J. Kim, “Mobile robot localization 
in indoor environment using scale-invariant visual landmarks,” Proc. 2008 
IAPR Workshop on Cognitive Information Processing, pp. 159-163, San-
torini, Greece, June 2008. 
TABLE 1 
ERROR RATIOS WITH CAMERA LOOKING DOWN AT DIFFERENT CEILING HEIGHT 200CM O, 225CM, AND 250CM. 
real x,y  (cm) real dis-tance (cm) 
200cm 225cm 250cm 
estimated (x, y) (cm) error ratios (type-1, type-2) estimated (x, y) (cm) error ratios (type-1, type-2) estimated (x, y) (cm) error ratios (type-1, type-2) 
(-7, -24) 25 (-8, -23) (0.7%, 0.4%) (-8, -24) (0.5%, 0.3%) (-9, -23) (0.9%, 0.7%) 
(-37, 36) 52 (-36, 36) (0.5%, 0.3%) (-37, 35) (0.5%, 0.3%) (-38, 37) (0.6%, 0.4%) 
(-20, 96) 98 (-20, 94) (0.9%, 0.6%) (-20, 95) (0.4%, 0.3%) (-21, 94) (0.8%, 0.7%) 
(-45, -107) 116 (-44, -106) (0.6%, 0.4%) (-46, -107) (0.4%, 0.3%) (-48, -109) (1.3%, 1.1%) 
(-111, -55) 124 (-112, -56) (0.6%, 0.4%) (-114, -56) (1.3%, 1.0%) (-117, -57) (2.3%, 2.0%) 
(-140, 62) 153 (-140, 60) (0.8%, 0.6%) (-143, 61) (1.3%, 1.0%) (-145, 62) (1.7%, 1.6%) 
(-229, 
-101) 
250 (-228, -104) (1.0%, 1.0%) (-233, -106) (2.0%, 2.0%) (-238, -110) (3.6%, 4.0%) 
(-253, 76) 264 (-257, 82) (2.2%, 2.3%) (-260, 82) (2.8%, 2.9%) (-264, 80) (3.2%, 3.7%) 
(-320, -15) 320 (-317, -15) (0.8%, 0.9%) (-331, -14) (2.9%, 3.4%) (-335, -14) (3.9%, 4.7%) 
average error ratio (type-1, type-2) (0.9%, 0.7%)  (1.4%, 1.3%)  (2.0%, 2.1%) 
TABLE 2 
ERROR RATIOS WITH CAMERA AT CEILING HEIGHT 200CM FOR DIFFERENT TILTED ANGLE 90O, 70O, AND 50O. 
real x,y  (cm) real dis-tance (cm) 
Titled for 90o Titled for 70o Titled for 50o 
estimated (x,y) (cm) error ratios (type-1, type-2) estimated (x,y) (cm) error ratios (type-1, type-2) estimated (x,y) (cm) error ratios (type-1, type-2) 
(-7, -24) 25 (-8, -23) (0.7%, 0.4%) (-7, -22) (1.0%, 0.6%) (-7, -22) (1.0%, 0.6%) 
(-37, 36) 52 (-36, 36) (0.5%, 0.3%) (-37, 52) (1.5%, 0.9%) (-38, 34) (1.1%, 0.7%) 
(-20, 96) 98 (-20, 94) (0.9%, 0.6%) (-22, 94) (1.3%, 0.9%) (-21, 90) (2.7%, 1.9%) 
(-45, -107) 116 (-44, -106) (0.6%, 0.4%) (-45, 103) (1.7%, 1.2%) (-42, -103) (2.2%, 1.6%) 
(-111, -55) 124 (-112, -56) (0.6%, 0.4%) (-115, -58) (2.1%, 1.6%) (-110, -60) (2.2%, 1.6%) 
(-140, 62) 153 (-140, 60) (0.8%, 0.6%) (-144, 66) (2.2%, 1.8%) (-141, 58) (1.6%, 1.3%) 
(-229, -101) 250 (-228, -104) (1.0%, 1.0%) (-224, -95) (2.4%, 2.4%) (-238, -110) (4.0%, 4.0%) 
(-253, 76) 264 (-257, 82) (2.2%, 2.3%) (-259, 82) (2.6%, 2.6%) (-271, 81) (4.5%, 4.6%) 
(-320, -15) 320 (-317, -15) (0.8%, 0.9%) (-330, -16) (2.7%, 3.1%) (-340, -26) (6.0%, 7.1%) 
average error ratio  (type-1, type-2) (0.9%, 0.7%)  (1.9%, 1.7%)  (2.8%, 2.6%) 
TABLE 3 
COMPARASION OF VEHICLE LOCALIZATION ACCURACY 
 Takeshita et al Park et al. Our Method 
Average distance error 4.6cm 26cm 5.8cm 
Approximate range of vehicle movement 2.1m 7m 6.4m 
Average error percentage 5.6% 3.7% 0.9% 
> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 
 
2
of false detections and requires no parameter tuning. Wu and 
Tsai [16] proposed a method to detect lines directly in an 
omni-image using a Hough transform process without 
unwarping the omni-image. Maybank et al. [17] proposed a 
method based on the Fisher-Rao metric to detect lines in 
paracatdioptric images, which has the advantage that it does not 
produce multiple detections of a single space line. Yamazawa 
et al. [18] proposed a method to reconstruct 3D line segments 
in images taken with three omni-cameras in known poses based 
on trinocular vision by the use of the Gaussian sphere and a 
cubic Hough space [19]. Li et al. [20] proposed a vanishing 
point detection method based on cascaded 1-D Hough 
transforms, which requires only a small amount of computation 
time without losing accuracy. 
In this study, we propose a new 3D vision system using two 
omni-cameras, which has a capability of automatic adaptation 
to any system setup for convenient in-field uses. Specifically, 
the proposed vision system, as shown in Fig. 1, consists of two 
omni-cameras facing the user’s activity area. Each camera is 
affixed firmly to the top of a rod, forming an omni-camera 
stand, with the camera’s optical axis adjusted to be horizontal 
(i.e., parallel to the ground). The cameras are allowed to be 
placed freely in the environment at any location in any 
orientation, resulting in an arbitrary system setup. Then, by the 
use of space line features in environments, the proposed vision 
system can adapt automatically to the arbitrarily-established 
system configuration by just asking the user to stand still for a 
little moment in the middle region of the activity area in front of 
the two cameras. After this adaptation operation, 3D data can 
be computed precisely as will be shown by experimental results 
in this paper. 
As an illustration of the proposed system, Fig. 1(c) shows the 
case of a user using a cot-covered fingertip as a 3D cursor point, 
which is useful for 3D space exploration in video games, 
virtual/augmented reality, 3D graphic designs, and so on. The 
fingertip is detected and marked as red in that figure, whose 3D 
location can be computed by triangulation. 
In contrast with a conventional vision system with two 
cameras whose configuration is fixed, the proposed system has 
several advantages. First, the system can be established freely, 
making it suitable for wide-ranging applications. This is a 
highly desired property especially for consumer electronics 
applications such as home entertainment or in-house 
surveillance, since the user can place the system components 
flexibly without the need to adjust the positions of the existing 
furniture in the application environment. Second, since the 
proposed vision system uses omni-cameras, the viewing angle 
of the system is very wide. This can be seen as an improvement 
over commercial products like Microsoft Kinect since the 
player can now move more freely at a close distance to the 
sensors. This advantage is very useful for people who only 
have small spaces for entertainments. Also, the two cameras in 
the proposed system are totally separated from each other at a 
larger distance, resulting in the additional merit of yielding 
better triangulation precision and 3D computation results due to 
the resulting longer baseline between the two cameras. 
  
X1
Y1
Z1 
X2
Y2 
Z2  
(a) 
  
(b) (c) 
Fig. 1. Configuration and an illustration of the usage of proposed system. (a)
An illustration. (b) Real system used in this study. (c) An omni-image of a user 
wearing a finger cot (marked as red). 
In the remainder of this paper, an overview of the proposed 
system is described in Section II, and the details of the 
proposed techniques for use in the system are presented in 
Sections III through VI. Experimental results are included in 
Section VII, followed by conclusions in Section VIII. 
II. OVERVIEW OF PROPOSED SYSTEM 
The use of the proposed system for 3D vision applications 
includes three stages: (1) in-factory calibration; (2) in-field 
system adaptation; and (3) 3D data computation. The goal of 
the first stage is to calibrate the camera parameters efficiently in 
the factory environment. For this, a technique using landmarks 
and certain conveniently-measurable system features is 
proposed. In the second stage, an in-field adaptation process is 
performed, which uses line features in environments to 
automatically compute the orientations of the cameras and the 
distance between them (i.e., the baseline of the system). In this 
stage, a user with a known height is asked to stand in the middle 
region in front of the two cameras to complete the adaptation. 
Subsequently, the 3D data of any feature point (like the finger 
tip shown in Fig. 1(c)) can be computed in the third stage. 
A sketch of the three operation stages of the proposed system 
is described in the following. To simplify the expressions, we 
will call the left and right cameras as Cameras 1 and 2, and their 
camera coordinate systems as CCSs 1 and 2, respectively. 
Algorithm 1. Sketch of the proposed system’s operation. 
Stage 1. Calibration of omni-cameras. 
 Step 1. Set up a landmark and select at least two feature 
points Pi on it, called landmark points. 
 Step 2. Perform the following steps to calibrate Camera 1. 
 2.1. Measure manually the radius of the mirror base of 
the camera as well as the distance between the 
camera and the mirror, as stated in Section VII-A. 
 2.2. Take an omni-image I1 of landmark points Pi with 
Camera 1 and extract the image coordinates of 
those pixels pi which correspond to Pi. 
 2.3. Detect the circular boundary of the mirror base in I1, 
> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 
 
4
This section is organized as follows. First, a quadratic 
formula describing the projection of a space line in an 
omni-image is derived in Section IV-A. Next, a refined Hough 
transform technique for detecting space lines is proposed in 
Section IV-B, which uses a novel adaptive thresholding scheme 
to produce robust detection results. Also, the projection of a 
vertical space line is derived and analyzed in Section IV-C. A 
peak cell extraction technique proposed for use in the refined 
Hough process is described at last in Section IV-D. 
A. Projection of a Space Line in an Omni-Image 
Given a space line L, we can construct a plane S which goes 
through L and the origin Om of a CCS as shown in Fig. 3. Let NS
= (l, m, n) denote the normal vector of S. Then, any point P = (X, 
Y, Z) on L satisfies the following plane equation: 
 NS·P = lX + mY + nZ = 0. (4) 
where “” denotes the inner-product operator. Combining (4) 
with (1) and (3), we get 
 lRcosT + mRsinT + nRtanD = 0, (5) 
where R = 2 2X Y . Dividing (5) by 2 2 2R l m n   leads to 
 
2 2 2 2 2 2 2 2 2
cos sin tan 0l m n
l m n l m n l m n
T T D 
     
 , 
which can be transformed into the following form 
 2 2cos 1 sin tan 0A A B BT T     D   
with the two parameters A and B defined as 

2 2 2
lA
l m n
    2 2 2
nB
l m n
     
Accordingly, the normal vector NS of plane S, originally being 
(l, m, n), can now be expressed alternatively as 
 2 2( ,  1 ,  )SN A A B   B  
It is assumed that m t 0 in (6) and (8) above. In case that m < 0, 
we may consider NS = (l, m, n) instead, which also 
represents the same space plane S. Also, it can be seen from (7) 
that, parameters A and B satisfy the constraint A2 + B2  1, 
implying that the Hough space is of a circle shape. 
 
 
Fig. 3. Illustration of a space line L projected on an omni-image as IL. 
The parameters A and B are used in the Hough transform to 
detect space lines in omni-images. These two parameters are 
skillfully defined in (7), leading to several advantages. First, 
removals of vertical space lines can be easily achieved by 
ignoring periphery regions as described later in Section IV-C. 
Next, since the possible values of A and B range from 1 to 1, 
the size of the Hough space is fixed within this range. This is a 
necessary property in order to use the Hough transform 
technique, and is an improvement on a previous work [16]. 
Also, the parameters A and B are used directly to describe the 
directional vector of the space line L as will be shown later in 
(14). Hence, one may divide the Hough space into more cells to 
yield a better precision. 
Combining (6) with (1) through (3), we can derive a conic
section equation to describe the projection of a space line L 
onto an omni-image as follows: 
 2 2, 1 2 3 4 5 6( , ) 0A BF u v C u C uv C v C u C v C        
where the coefficients C1 through C6 are: 
  22 21 7 1C A B C   ; 2 22 2 1C A A B   ; 
 ; ;  22 23 71C A C   B 7 f4 2C ABC 
 2 25 72 1C BC A B   f ;   2 26C B f 
 
2
7 2
1.
1
C HH
    
The quadratic formula (9) will be called the target equation in 
the Hough transform subsequently, since the goal of the 
detection process is to find curves described by it in an 
omni-image. 
B. Hough Space Generation with Adaptive Thresholding 
We define the Hough space to be two-dimensional with the 
parameters A and B described previously. Furthermore, we 
define the cell support for a cell at (A, B) in the Hough space as 
the set of those pixels which contribute to the accumulation of 
the value of that cell. Let L denote a space line described by the 
two parameters (A, B). Two properties of cell supports are 
desirable: (1) the pixels of the projection IL of L onto the 
omni-image are all included in the cell support for the cell (A, 
B); and (2) the pixels not on IL are not included in this cell 
support. Furthermore, it is desired that the shape of the cell 
support is of a certain fixed width and not too “thin,” so that 
(edge) pixels originally belonging to IL but with small detection 
errors can still contribute to the cell value. In short, a cell 
support is desired to be a space line projection with a certain 
width everywhere along the line, which is called an 
equal-width projection curve hereafter. In this section, we first 
show that commonly-used curve detection methods do not 
generate desired equal-width projection curves as cell supports 
as shown in Figs. 4(a) and 4(b), so we propose in this study an 
adaptive method to solve this problem to yield better results 
like the one shown in Fig. 4(c). 
 
> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 
 
6
Then, we extract peak cells by choosing the cells with large 
values in the filtered Hough space to yield a better detection 
result, as shown by Figs. 5(c) and 5(d). 
By the way, it is noted that when applying the filter to the 
Hough space, one of the side effects is the removal of the 
periphery region. This is a desired property mentioned in 
Section IV-C: the removal of the periphery region is equivalent 
to the removal of vertical space lines. Thus, expectedly we can 
get more horizontal lines as desired. To sum up, we have 
proposed a new method to detect horizontal space lines in 
omni-images, with several novel techniques also proposed in 
Sections IV-A through IV-D to improve the detection result. 
The proposed method for horizontal space line detection is 
summarized as an algorithm in the following. 
Algorithm 2.  Detection of horizontal space lines in the form of 
conic sections in an omni-image. 
Input: an omni-image I. 
Output: 2-tuple values (Ai, Bi) as defined in (7) which describe 
detected horizontal space lines in I. 
 Step 1. Extract the edge points in I by an edge detection 
algorithm [25]. 
 Step 2. Set up a 2D Hough space H with two parameters A 
and B, and set all the initial cell values to be zeros. 
 Step 3. For each detected edge point at coordinates (u, v) and 
each cell C with parameters (A, B), if (u, v, A, B) 
satisfies (10) in which the threshold value G is 
adaptively calculated by (11), then increment the 
value of C by one. 
 Step 4. Apply the filter described by (12) to Hough space H, 
choose those cells with maximum values, and take 
their corresponding parameters (Ai, Bi) as output. 
 
 
 
(a) 
 
(b) 
 
(c) 
 
(d) 
Fig. 5. Comparison of traditional peak cell extraction method and 
proposed one. (a) Hough space. (b) 50 detected space lines using 
traditional method. (c) Post-processed Hough space. (d) 50 detected space 
lines using proposed method. 
V. CALCULATION OF INCLUDED ANGLE I BETWEEN TWO 
CAMERAS’ OPTICAL AXES USING DETECTED LINES 
In the proposed vision system, the omni-cameras are 
mounted on two vertical stands with the optical axes being 
parallel to the floor plane as mentioned previously, but the 
cameras’ optical axes are allowed to be non-parallel, making an 
included angle I as depicted in Fig. 1(a). To accomplish the 3D 
data computation work under an arbitrary system setup, the 
included angle I must be calculated first. A method to calculate 
the angle I using a single manually chosen horizontal space 
line is proposed first in Section V-A. However, in order to 
conduct the adaptation process automatically, we have to 
calculate the angle I using multiple automatically extracted 
horizontal space lines. To achieve this, a novel method is 
proposed next in Section V-B, which utilizes all the detected 
space lines from the two omni-images taken with the cameras. 
The proposed method has several advantages. First, only the 
directional information of the space line, which is a robust 
feature against noise, is used. Next, no line correspondence 
between the two omni-images need be derived; that is, it is 
unnecessary to decide which line in the left omni-image 
corresponds to which one in the right omni-image. This makes 
the proposed method fast, reliable, and suitable for a 
wide-baseline stereo system like the one proposed in this study. 
Also, the proposed method makes use of a good property of the 
man-made environment — many line edges in such 
environments are parallel to one another, leading to an 
improvement on the robustness and correctness of the 
computation result. 
A. Calculating Angle I Using a Single Horizontal Space Line 
In this section, a method to calculate the angle I between the 
two cameras’ optical axes is proposed, using a single horizontal 
space line L in the environment. Let (A1, B1) be the parameters 
corresponding to line L in an omni-image taken with Camera 1, 
vL = (vx, vy, vz) be the directional vector of L in CCS 1, and S1 be 
the space plane going through line L and the origin of CCS 1. 
The normal vector of S1 can be derived, according to (8), to be 
 2 21 1 1 1 1( ,  1 ,  )n A A B B   .  
Since S1 goes through line L, we get to know that vL and n1 are 
perpendicular, resulting in the following equalityĻ 
 vL·n1 = 2 21 1 1 11x y zv A v A B v B 0     . (13) 
Furthermore, since L, being horizontal, is parallel to the 
XZ-plane as shown in Fig. 1(a), we get another constraint vy = 0. 
This constraint can be combined with (13) to get 
 vL = (vx, vy, vz) = (B1, 0, A1). (14) 
Next, by referring to Fig. 6(a), it can be seen that the angle I1 
between the X-axis of CCS 1 and space line L is 
 I1 = tan1(A1/B1).  
> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 
 
8
VI. PROPOSED TECHNIQUE FOR BASELINE DERIVATION AND 
ANALYTIC COMPUTATION OF 3D DATA 
The world coordinate system X-Y-Z is defined as depicted in 
Fig. 8. The X-axis goes through the two camera centers O1 and 
O2; the Y-axis is taken to be parallel to the Y-axes of both CCSs; 
the Z-axis is defined to be perpendicular to the XY-plane; and 
the origin is defined to be the origin O1 of CCS 1. It is noted 
here that, since the two omni-cameras are affixed firmly on the 
omni-camera stands and adjusted to be of an identical height as 
described in Section I, the axes X, Z, X1, Z1, X2, and Z2 are all on 
the same plane as illustrated in Fig. 8. 
Since the two omni-cameras are allowed to be placed 
arbitrarily at any location with any orientation, it is necessary to 
find the baseline D and the orientation angles E1 and E2 (as 
defined in Fig. 8) in advance to calculate the 3D data of space 
points. A novel method to calculate the orientation angles is 
proposed first in Section VI-A. After the orientations are 
derived, the 3D data can be determined up to a scale as 
discussed in Section VI-B. Then, a method using the known 
height of the user to determine the baseline D is proposed in 
Section VI-C. After the baseline D is derived, the absolute 3D 
data of space feature points can be derived by a similar method 
as proposed in Section VI-B. It is emphasized that all 
computations involved in these steps are done analytically, i.e., 
by the use of formulas without resorting to iterative algorithms.  
A. Finding Two Cameras’ Orientations 
Let the camera coordinates of CCS 1 be denoted as (X1, Y1, 
Z1), and those of CCS 2 as (X2, Y2, Z2), as shown in Fig. 8. As 
mentioned previously, the two CCSs X1-Y1-Z1 and X2-Y2-Z2 are 
allowed to be oriented arbitrarily (with Y1 and Y2 parallel to 
each other), and the only knowledge acquired by the proposed 
system is the angle I between the two optical axes Z1 and Z2, 
which is derived using the detected space lines, as described 
previously in Section V. 
To derive the angles E1 and E2, the user is asked to stand in 
the middle region in front of the two omni-cameras so that a 
feature point Puser on the user’s body may be utilized to draw a 
mid-perpendicular plane of the line segment O1O2 as shown in 
Fig. 8. Let (X1, Y1, Z1) be the coordinates of Puser in CCS 1, and 
(u1, v1) be the corresponding pixel’s image coordinates in the 
left omni-image. From (1) and (3), we have the equality: 
 > @ > @T T2 21 1 1 1 1 1 1 1cos sin tanX Y Z X Y T T D  ,  
where cosT1, sinT1, and tanD1 are computed from (u1, v1) 
according to (1) and (3). This equality shows that the 
directional vector between O1 and Puser is (cosT1, sinT1, tanD1) 
in CCS 1. An angle \1 is defined on the XZ-plane as illustrated 
in Fig. 8, which can be expressed as \1 = tan1(tanD1/cosT1). 
Similarly, the angle \2 defined on the XZ-plane can be derived 
to be tan1(tanD2/cosT2). Accordingly, we can derive E1 to be 
 2 1 1 21 1 2 2 2
\ \ I \ \ I
 
 
2 1
2
\ \ I 
Fig. 8. A top-view of the coordinate systems. The baseline D, orientation 
angles E1 and E2, and a point Puser on the user’s body are also drawn. 
and E2 is just E2 = E1  I. This completes the derivations of the 
orientation angles E1 and E2 of the two cameras. 
B. Calculating 3D Data of Space Feature Points 
Let P be a space feature point with coordinates (X, Y, Z) in 
CCS 1, and let the projection of P onto the omni-image taken 
by Camera 1 be the pixel p1 located at image coordinates (u1, v1). 
From (1) and (3) with R1 = 2 21 1X Y , we have 
 > @ > @T T1 1 1 1 1 1 1cos sin tanX Y Z R T T D  (16) 
where cosT1, sinT1, and tanD1 are computed from (u1, v1) by (1) 
and (3). Equation (16) describes a light ray L1 going through the 
origin O1 with directional vector d1ƍ = [cosT1 sinT1 tanD1]T in 
CCS 1. To transform the vector into the coordinate system 
X-Y-Z, we have to rotate d1ƍ along the Y-axis through the angle 
E1 as illustrated in Fig. 8. As a result, the transformed light ray 
L1 goes through (0, 0, 0) with its directional vector d1 being 
 d1 = 
1 1
1
1 1
cos 0 sin cos
0 1 0 sin
sin 0 cos tan
1
1
E E T
T
E E D
ª º ª º« » « »« » « »« » « »¬ ¼ ¬ ¼
. (17) 
Similarly, let the space feature point P be located at (Xƍ, Yƍ, Zƍ) 
in CCS 2 and its projection onto the omni-image taken by 
Camera 2 be the pixel p2 located at image coordinates (u2, v2). 
Then, similarly to the derivation of (16) we can obtain the 
following equation to describe L2 in CCS 2: 
 > @ > @T T2 2 2 2 2 2 2cos sin tanX Y Z R T T D , (18) 
where R2 = 22 22X Y . As illustrated in Fig. 8, we can 
transform the light ray L2 from CCS 2 to the coordinate system 
X-Y-Z by rotating the ray through the angle E2 and translating it 
by the vector [D 0 0]T. As a result, the transformed light ray L2 
goes through (D, 0, 0) with its directional vector d2 being  
 d2 = 
2 2
2
2 2
cos 0 sin cos
0 1 0 sin
sin 0 cos tan
2
2
E E T
T
E E D
ª º ª º« » « »« » « »« » « »¬ ¼ ¬ ¼
. (19) 
2
S SE \    § ·    ¨ ¸© ¹ ,  
We now have two light rays L1 and L2 both going through 
the space point P. If everything including the works of system 
setup, camera calibration, and feature detection is conducted 
> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 
 
10
VII-A. Then, we present several experimental results to show 
the feasibility, reliability, and accuracy of the proposed line 
detection method, the system setup adaptation method, and the 
3D computation process in Section VII-B through VII-D. 
A. Omni-Camera Calibration 
In the first step, the lens center and the focal length of the 
perspective camera should be calibrated. As illustrated in Figs. 
10(a) and 10(b), the mirror boundary, appearing as a circle in 
each captured omni-image, was extracted to robustly estimate 
the camera center and the focal length according to [23]. 
Specifically, we found a circle to fit the circular mirror 
boundary like that appearing in Fig. 10(b), and defined the 
camera center as the center of the fitting circle. Also, as shown 
in Fig. 10(a), we derived the camera’s focal length f, according 
to the properties of similar triangles and the rotational 
invariance of the omni-camera [27][28], as 
 rf M
R
  (25) 
where M is the distance from the lens center to the camera 
center Om, R is the radius of the mirror base in the real-world 
space, and r is the radius of the mirror base in the taken image. 
The measured values in our experiments are R = 4.0 cm, M = 
8.6 cm, and r = 243 pixels for both cameras, from which the 
focal lengths f were derived to be 522.45 according to (25). 
Next, we solve H from (1) to get 
 sec sec
tan tan
E DH E D
  .  
Combining the above equality with (1) and (2), we can get 
 
2 2
2 2 2 2
2 2 2 2
1 1f Z
u v X Y
f Z
u v X Y
H
   
 
 . (26) 
The above equation shows that, if we have a landmark point 
with known image coordinates (u, v) and known camera 
coordinates (X, Y, Z), then the eccentricity H can be calculated. 
Although the eccentricity H is theoretically a constant value, 
we found in this study that we can achieve better accuracy in 
3D data computation if a linear polynomial can be used to 
describe H. The reason is that such a polynomial can be used to 
cope with some types of errors, including the radial distortion 
of the perspective camera’s lens, the imprecise measurements 
coming from the calibration process, and the manufacturing 
imprecision of the hyperboloidal mirror shape. Accordingly, 
we propose the following first-order equation to describe the 
eccentricity H, which comes from a functional expansion of H 
with respect to the mirror’s radius r according to the rotational 
invariance property as used in several studies [27][28]: 
 H = g·r + h, (27) 
where g and h are two coefficients, and r is as defined in (2). 
 
(a) 
 
(b) 
Fig. 10. Illustration of omni-camera calibration. (a) Relationship between 
mirror and image plane. (b) An omni-image of a calibration board. 
 
In our experiments, a calibration board as shown in Fig. 10(b) 
was designed and put in front of the omni-camera. Each cross 
point Pi on the board was taken as a landmark point as stated in 
Algorithm 1, and used to calculate the eccentricity Hi by (26). 
After the values Hi corresponding to all the landmark points 
were derived according to (26), the coefficients g and h in (27) 
were computed finally using a Levenberg–Marquardt 
algorithm [29] to be 0.0022 and 1.9211, respectively. 
To demonstrate the effectiveness of the first-order 
approximation method, we conduct two experiments as follows. 
In these experiments, we measure the 3D data of the 60 
landmarks on a calibration board, and compute the 3D 
measurement errors. The average 3D measurement error is 
6.3% with a standard deviation of 1.4% when using a constant 
eccentricity, which is reduced to an average error of 1.9% with 
a standard deviation of 0.71% when using the first-order 
approximation. This shows the effectiveness of the first-order 
approximation method for computing the eccentricity H. It is 
noted here that the first-order coefficient g is supposed to be 
small since it should be a constant in theory. Otherwise, it 
means any of the three possibilities: (1) the measurements in 
the calibration are not accurate enough; (2) the lens of the 
perspective camera is heavily distorted; or (3) the mirror is not 
of a good hyperboloidal shape.  
B. Space Line Detection Ability 
In Sections IV-A, IV-D, and IV-B, three techniques of 
improvements on increasing the detection ability and reliability 
of the proposed Hough-based space line detection method have 
been proposed, which are called parameterization, peak cell 
extraction, and accumulation, respectively, henceforth. Some 
comparisons are provided here to show the effectiveness of the 
proposed improvement techniques. About parameterization, 
we compare the effect of our technique with that proposed in 
[16]. About peak cell extraction, we compare our technique 
using the proposed filter with a traditional method. And about 
accumulation, we compare the adaptive thresholding technique 
we propose with a traditional accumulation method [31][32]. 
Accordingly, four different space line detection experiments 
have been designed, which are listed in Table I. 
The input omni-image of the four experiments is shown in 
Fig. 11(a). In each experiment, at first we found the edges in the 
omni-image to get those shown in Fig. 11(b). We then applied 
the Hough-based space line detection method to find 50 space 
> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 
 
12
In Experiments 1 and 2, since the lines in the corridor and 
hall are relatively simple and obvious, the adaptation result is 
accurate with errors of about 2° as shown by the green and 
purple curves in Fig. 12(c). Also, since we use omni-cameras in 
these experiments, the lines can still be captured even when the 
two cameras were oriented with a large angle. Thus, the 
adaptation result remains accurate when the angle I is large. In 
Experiment 3, since the space lines in the room are more 
complicated, the adaptation becomes more difficult. However, 
since the omni-cameras can capture a large field of view of the 
environment, a plenty number of space lines can be captured. 
Therefore, the adaptation result is accurate as well, with errors 
of about 4° as shown by the red curve in Fig. 12(c). In contrast, 
the adaptation errors are about 10° when perspective cameras 
were used, as shown by the blue curve in Fig. 12(c), and they 
become unacceptable (larger than 20°) when the included angle 
I is large. These experimental results show the feasibility of the 
proposed adaptation methods, as well as the power of the 
omni-cameras in the automatic adaptation process. 
D. Adaptation and 3D Acquisition Ability 
A series of experiments are conducted to test the adaptation 
ability and the 3D acquisition precision in the room 
environment shown in Fig. 12(b). In each of the experiments, 
the two cameras were placed at a distance about 180cm to each 
other, and both were oriented randomly within the range of 
±40°. After the cameras were set up, two omni-images of the 
environment were captured as shown, for example, in Figs. 
13(a) and 13(d), respectively, and used to calculate the included 
angle I according to Step 5 of Algorithm 1. Next, a user was 
asked to stand in the middle region in front of the two cameras, 
as shown in Figs. 13(b) and 13(e), to calculate the orientation 
angles E1 and E2 and the baseline D according to Step 6 of 
Algorithm 1. After these adaptation tasks were done, a board 
with 60 landmarks was held by the user, as shown in Figs. 13(c) 
and 13(f), to test the precision of the resulting 3D computation.  
In these experiments, three different degrees of adaptation 
were implemented and the corresponding results compared: (1) 
no adaptation was conducted with the camera orientations and 
baseline set to be E1 = E2 = 0° and D = 180 cm (D is the 
ground-truth value); (2) the left omni-camera was set up to face 
forward with the values E1 = 0°, D = 180cm, and E2 adapted to 
be I; and (3) all the parameters E1, E2, and D were adapted 
according to the proposed method. Denoting (Xi, Yi, Zi) as the 
ground-truth location of a landmark point, and (Xiƍ, Yiƍ, Ziƍ) as 
the calculated location, we define the 3D error E of each 
landmark point as 
      2 2 2 2 2 2i i i i i i i i iE X X ' Y Y' Z Z ' X Y Z        . (28) 
The comparison results are shown in Fig. 14 in which the 
vertical axis specifies the average of the 3D errors, and the 
horizontal axis specifies the system orientation angle which is 
defined as the maximum of the two orientation angles E1 and E2. 
(a) 
 
(b) (c) 
(d) 
 
(e) (f) 
Fig. 13. Sample omni-images of an experiment. (a)(d) Taking a shot of the 
environment to calculate I. (b)(e) A user standing in the middle region in 
front of the cameras to calculate baseline D and orientation angles E1 and 
E2. (c)(f) A board held by the user to test the 3D computation precision. 
 
As can be seen from Figs. 14(a) and 14(b), when no 
parameter is adapted with the results shown by the blue curve, 
the 3D errors are seen to become larger as the orientation angle 
becomes larger, showing the necessity of an automatic system 
adaptation process. When only the orientation E2 of the right 
omni-camera is adapted with the result shown by the red curve, 
it is observed that the 3D errors are sometimes lower but vary 
largely. This results from the fact that the left omni-camera is 
assumed to face forward in this case. Thus, if the left 
omni-camera is actually placed to face forward in the 
experiment, the error measure is lowered; otherwise, the error 
is large as expected. Finally, when all the parameters E1, E2 and 
D are adapted with the results shown by the purple curve, the 
3D errors are lower than 8% even when the system orientation 
angle is large. This shows the feasibility, reliability, and 
validity of the proposed system adaptation method.  
It is noted that these 3D measurements are calculated under a 
certain unintended inaccurate system setup. For example, it is 
required that the two omni-camera stands be adjusted to be at 
an identical height, but there might still exist a small distance, 
say 1cm, between the heights of the two stands. Similarly, 
although the optical axes are assumed to be parallel to the 
XZ-plane, a small angle, say 1°, might be included between the 
optical axes and the XZ-plane. To see the effect of such 
unintended system setup inaccuracy, a plot of the average 3D 
errors resulting from a series of planned inaccurate setups is 
drawn in Fig. 14(c). As can be seen, at the reasonable setup 
errors of 1 cm in height and 2o in included angle, the average 
3D error is 2.805%, which is tolerable in real-time game 
playing according to our experimental experience. 
Using the proposed vision system, we have also created a 
game application in our experiments, which allows a user to 
play a 3D maze game, as illustrated by Fig. 15. The game is 
played mainly by the use of a finger with a yellow cot as a 
cursor, controlling the avatar going around and up and down in 
the maze to reach the destination. The 3D position of the 
simulated cursor is computed by analyzing the omni-image pair 
to detect the feature point of the finger cot and calculating its 
> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 
 
14
IEEE Trans. on Systems, Man, and CyberneticsPart A: Systems and 
Humans, vol. 37, no. 2, pp. 262272, March 2007. 
[2] B. J. Tippetts, D. J. Lee, J. K. Archibald, and K. D. Lillywhite, “Dense 
disparity real-time stereo vision algorithm for resource-limited systems,” 
IEEE Trans. on Circuits and Systems for Video Technology, vol. 21, no. 
10, pp. 15471555, Oct. 2011. 
[23] J. Fabrizio, J. P. Tarel, and R. Benosman, “Calibration of panoramic 
catadioptric sensors made easier,” Proc. IEEE Workshop on 
Omnidirectional Vision, pp. 4552, 2002. 
[24] R. I. Hartley and P. Sturm, “Triangulation,” Proc. ARPA Image 
Understanding Workshop, pp. 957966, 1994. 
[3] Y. Sun, X. Chen, M. Rosato, and L. Yin, “Tracking vertex flow and model 
adaptation for three-dimensional spatiotemporal face analysis,” IEEE
Trans.on Systems, Man and CuberneticsPart A: Systems and Humans, 
vol. 40, no. 3, pp. 461474, May 2010. 
[25] R. C. Gonzalez and R.E. Woods, Digital Image Processing, New Jersey: 
Prentice Hall, 2002. 
[26] B. Cyganek, J. P. Sievert, Introduction to 3D Computer Vision 
Techniques and Algorithms, Wiley, John & Sons, Incorporated, 2009. 
[4] K. Li, Q. Dai, W. Xu, J. Yang, and J. Jiang, “Three-dimensional motion 
esimtation via matrix completion,” IEEE Trans. on Systems, Man, and 
CyberneticsPart B: Cybernetics, vol. 42, no. 2, pp. 539551, April 2012. 
[27] D. Scaramuzza, “Omnidirectional vision: from calibration to robot motion 
estimation”, ETH Zurich, PhD Thesis no. 17635. Zurich, February 22, 
2008. 
[5] Wikipedia contributors, “Kinect,” Wikipedia, The Free Encyclopedia, 
http://en.wikipedia.org/wiki/Kinect (accessed Dec. 15, 2010). 
[6] S. Laakso and M. Laakso, “Design of a body-driven multiplayer game 
system,” Computers in Entertainment (CIE), vol. 4, no. 4, Article 4C, 
Oct.-Dec. 2006. 
[28] D. Scaramuzza, A. Martinelli, and R. Siegwart, “A flexible technique for 
accurate omnidirectional camera calibration and structure from motion,” 
Proc. IEEE Int’l Conf. on Computer Vision Systems, pp. 4552, Jan 2006. 
[29] K. Levenberg, “A method for the solution of certain non-linear problems 
in least squares,” Quarterly of Applied Mathematics, pp. 164168, 1944. 
[7] J. J. Magee, M. Betke, J. Gips, M. R. Scott, and B. N. Waber, “A 
humancomputer interface using symmetry between eyes to detect gaze 
direction,” IEEE Trans. on Systems, Man, and CyberneticsPart A: 
Systems and Humans, vol. 38, no. 6, pp. 12481261, Nov. 2008. 
[30] J. Illingworth and J. Kittler, “A survey of the Hough transform,” Comput. 
Vision, Graph. Image Processing, vol. 44, pp. 87–116, 1988. 
[8] X. Zabulis, T. Sarmis, D. Grammenos and A. A. Argyros, “A multicamera 
vision system supporting the development of wide-area exertainment 
applications,” IAPR Conf.on Machine Vision Applications (MVA 2009), 
Yokohama, Japan, May 20-22, 2009, pp. 269-272. 
[31] R. D. Duda and P. E. Hart, “Use of the Hough transform to detect lines 
and curves in pictures,” Communications of the ACM, vol. 15, pp. 1115, 
1972. 
[32] N. Bennett, R. Burridge, N. Saito, “A method to detect and characterize 
ellipses using the Hough transform,” IEEE Trans.on Pattern Analysis and 
Machine Intelligence, vol. 21 no.7, pp. 652–657, 1999. 
[9] J. Starck, A. Maki, S. Nobuhara, A. Hilton, and T. Matsuyama, “The 
multiple-camera 3-D production studio,” IEEE Trans. on Circuits and 
Systems for Video Technology, vol. 19, no. 6, June 2009. 
[10] S. Sefvic and S. Ribaric, “Determining the absolute orientation in a 
corridor using projective geometry and active vision,” IEEE Trans. on 
Industrial Electronics, vol. 48, no. 3, pp. 696710, June 2001. 
[33] S. E. Shih, and W. H. Tsai, “A new two-omni-camera system with a 
console table for versatile 3D vision applications and its automatic 
adaptation to imprecise camera setups,” Advances in Multimedia 
Modeling (MMM 2011) - Lecture Notes in Computer Science (LNCS), Vol. 
6523, K. T. Lee, W. H. Tsai, H. Y. M. Liao, T. Chen, J. W. Hsieh and T. T. 
Tseng. (eds.), Springer, Berlin/Heidelberg, Germany, pp. 193-205, 2011. 
[11] R. Carelli, R. Kelly, O. H. Nasisi, C. Soria, and V. Mut, “Control based on 
perspective lines of a non-holonomic mobile robot with 
camera-on-board,” Int’l Journal of Control, vol. 79, no. 4, pp. 362371, 
2006. 
 
Shen-En Shih received the B.S. degree in computer 
science from National Chiao Tung University, Taiwan, in 
2009, and is currently pursuing the Ph.D. degree at the 
College of Computer Science, National Chiao Tung 
University. He has been a research assistant at the 
Computer Vision Laboratory in the Department of 
Computer Science at National Chiao Tung University 
from August 2009. His current research interests include 
computer vision, image processing, human-machine 
interfacing, and stereo vision. 
[12] X. Ying and H. Zha, “Simultaneously calibrating catadioptric camera and 
detecting line features using Hough transform,” Proc. IEEE/RSJ Int’l 
Conf. on Intelligent Robots and Systems, pp. 412417, Aug. 2005. 
[13] X. Ying, “Catadioptric Camera Calibration Using Geometric Invariants,” 
Proc. IEEE Int’l Conf. on Computer Vision, vol. 2, pp. 13511358, Oct. 
2003. 
[14] F. Duan, F. Wu, M. Zhou, X. Deng, and Y. Tian, “Calibrating effective 
focal length for central catadioptric cameras using one space line,” 
Pattern Recognition Letters, vol. 33, pp. 646-653, 2012. 
 
[15] R. G. von Gioi, J. Jakubowicz, J.-M. Morel and G. Randall, “LSD: A fast 
line segment detector with a false detection control,” IEEE Trans. on 
Pattern Analysis and Machine Intelligence, vol. 32, no. 4, pp. 722732, 
April 2010. 
[16] C. J. Wu and W. H. Tsai, “An omni-vision based localization method for 
automatic helicopter landing assistance on standard helipads,” Proc. Int’l 
Conf. on Computer and Automation Engineering, Singapore, pp. 327332, 
2010. 
[17] S. J. Maybank, S. Ieng, and R. Benosman, “A Fisher-Rao metric for 
paracatadioptric images of lines,” Int’l Journal of Computer Vision, vol. 
99, no. 2, pp. 147165, 2012. 
[18] K. Yamazawa, Y. Yagi and M. Yachida, “3D line segment reconstruction 
by using hyperomni vision and omnidirectional Hough transforming,” 
Proc. Int’l Conf. on Pattern Recognition, vol. 3, IEEE Computer Society, 
Washington, DC, USA, pp.34873490, 2000. 
[19] S. T. Barnard, “Interpreting perspective images,” Artifical Intelligence, 
vol. 21, pp. 435462, 1983. 
[20] B. Li, K. Peng, X. Ying, and H. Zha, “Vanishing point detection using 
cascaded 1D Hough Transform from single images,” Pattern Recognition 
Letters, vol. 33, pp. 1-8, 2012. 
[21] C. Geyer and K. Danilidis, “Catadioptric projective geometry,” Int’l
Journal of Computer Vision, vol. 45, no. 3, pp. 223243, 2001. 
[22] H. Ukida, N. Yamato, Y. Tanimoto, T. Sano and H. Yamamoto, 
“Omni-directional 3D measurement by hyperbolic mirror cameras and 
pattern projection,” Proc. 2008 IEEE Conf. on Instrumentation and 
Measurement Technology, Victoria, BC, Canada, pp. 365370, May 
2008. 
Wen-Hsiang Tsai received the B.S. degree in EE from 
National Taiwan University, Taiwan, in 1973, the M.S. 
degree in EE from Brown University, USA in 1977, and 
the Ph.D. degree in EE from Purdue University, USA in 
1979. Since 1979, he has been with National Chiao Tung 
University (NCTU), Taiwan, where he is now a Chair 
Professor of Computer Science. At NCTU, he has served 
as the Head of the Dept. of Computer Science, the Dean of 
General Affairs, the Dean of Academic Affairs, and a Vice 
President. From 1999 to 2000, he was the Chair of the 
Chinese Image Processing and Pattern Recognition Society of Taiwan, and 
from 2004 to 2008, the Chair of the Computer Society of the IEEE Taipei 
Section in Taiwan. From 2004 to 2007, he was the President of Asia University, 
Taiwan. Dr. Tsai has been an Editor or the Editor-in-Chief of several 
international journals, including Pattern Recognition, the International Journal 
of Pattern Recognition and Artificial Intelligence, and the Journal of 
Information Science and Engineering. He has published 144 journal papers and 
227 conference papers received many awards, including the Annual Paper 
Award from the Pattern Recognition Society of the USA; the Academic Award 
of the Ministry of Education, Taiwan; the Outstanding Research Award of the 
National Science Council, Taiwan; the ISI Citation Classic Award from 
Thomson Scientific, and more than 40 other academic paper awards from 
various academic societies. His current research interests include computer 
vision, information security, video surveillance, and autonomous vehicle 
applications. He is a Life Member of the Chinese Pattern Recognition and 
Image Processing Society, Taiwan and a Senior Member of the IEEE.
一藏密頻率訊號係對應於一藏密視訊畫面；一秘密資訊抽取單元，係利用上述既定金鑰
解密上述每一藏密頻率訊號，抽取對應於每一藏密頻率訊號之一秘密資訊；以及一資訊
合併單元，將上述每一秘密資訊合併，產生一秘密資訊報告。
6.　如申請專利範圍第 1項之基於資訊隱藏之視訊監控系統，其中，上述視訊編碼單元更包
含將上述視訊畫面中之一移動物體資訊根據上述既定金鑰加密以不可視浮水印方式嵌入
上述視訊畫面中。
7.　如申請專利範圍第 6項之基於資訊隱藏之視訊監控系統，更包含有一移動物體偵測與分
類單元，連接於上述資訊嵌入單元，偵測並分類上述視訊畫面中的一移動物體，並將上
述移動物體資訊傳送至上述資訊嵌入單元。
8.　如申請專利範圍第 7項之基於資訊隱藏之視訊監控系統，上述移動物體偵測與分類單元
將上述移動物體資訊分類為非人物物體、人物以及沒有移動物體三種。
9.　如申請專利範圍第 7項之基於資訊隱藏之視訊監控系統，其中，上述視訊搜尋單元更包
含搜尋上述秘密資訊包含有上述移動物體資訊的視訊畫面。
10.   一種基於資訊隱藏之視訊監控系統，包含有：一記憶體單元；一視訊擷取單元，用於擷
取一視訊畫面；一移動物體偵測與分類單元，連接於上述視訊擷取單元，偵測並分類上
述視訊畫面中的移動物體，產生一移動物體資訊；以及一視訊編碼單元，讀取上述移動
物體資訊與上述視訊畫面，將上述移動物體資訊根據一既定金鑰(secret key)加密以不可
視浮水印方式嵌入於上述視訊畫面中，編碼成一藏密視訊(stego video)並儲存於上述記憶
體中。
11.   如申請專利範圍第 10項之基於資訊隱藏之視訊監控系統，其中，上述移動物體偵測與分
類單元將上述移動物體資訊分類為非人物物體、人物以及沒有移動物體三種。
12.   如申請專利範圍第 10項之基於資訊隱藏之視訊監控系統，其中，上述藏密視訊係以
MPEG4格式編碼。
13.   如申請專利範圍第 10項之基於資訊隱藏之視訊監控系統，其中，上述視訊編碼單元包
含：一色度抽樣單元，讀取上述視訊畫面，對上述視訊畫面做色度抽樣，輸出一色度抽
樣後的視訊畫面；一轉換單元，對上述色度抽樣後的視訊畫面做離散餘弦轉換，將上述
色度抽樣後的視訊畫面轉換到頻率域，得到一頻率訊號；一量化單元，將上述頻率訊號
作線性量化，得到一量化後的頻率訊號；一資訊嵌入單元，讀取移動物體資訊，將上述
移動物體資訊根據一既定金鑰(secret key)加密以不可視浮水印方式嵌入上述量化後的頻
率訊號中，得到一藏密頻率訊號；一編碼單元，將上述藏密頻率訊號以可變長度編碼方
式編碼成一藏密視訊(stego－video)。
14.   如申請專利範圍第 10項之基於資訊隱藏之視訊監控系統，更包含一視訊搜尋單元，搜尋
上述藏密視訊中藏有上述移動物體資訊係的視訊畫面。
15.   如申請專利範圍第 14項之基於資訊隱藏之視訊監控系統，上述視訊搜尋單元包含：一解
碼單元，將上述藏密視訊以上述可變長度方式解碼成數個藏密頻率訊號，其中，上述每
一藏密頻率訊號係對應於一藏密視訊畫面；一秘密資訊抽取單元，係利用上述既定金鑰
解密上述每一藏密頻率訊號，抽取對應於每一藏密頻率訊號之一秘密資訊，其中，上述
每一秘密資訊係以一字元字串表示；以及一資訊合併單元，將上述每一秘密資訊合併，
產生一秘密資訊報告，其中，上述秘密資訊報告係由上述字元字串合併之結果。
16.   一種資訊隱藏之視訊編碼方法，包含：接收一視訊畫面；將上述視訊畫面做色度抽樣；
將上述色度抽樣後的視訊畫面做離散餘弦轉換，得到一對應之頻率訊號；將上述頻率訊
號作量化處理；根據一既定金鑰加密以不可視浮水印方式將一秘密資訊嵌入上述量化後
的頻率訊號中；以及將上述已嵌入上述秘密資訊之頻率訊號以可變長度編碼方式編碼一
藏密視訊。
(2)
- 2364 -
(4)
- 2366 -
(6)
- 2368 -









98年度專題研究計畫研究成果彙整表 
計畫主持人：蔡文祥 計畫編號：98-2221-E-009-116-MY3 
計畫名稱：新型攝影、感測與校正技術在電腦視覺自動車上之應用 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際
已達成數)
本計畫實
際貢獻百
分比 
單位 
備註（質化說明：如
數 個 計 畫 共 同 成
果、成果列為該期
刊之封面故事 ...
等） 
期刊論文 0 0 100%  國內 論文著作 
研究報告/技術報告 6 6 100% 
篇 
1. M. F. Chen and W. 
H. 
Tsai, ＇＇Automatic 
learning and 
guidance for indoor 
autonomous vehicle 
navigation by 
ultrasonic signal 
analysis and fuzzy 
control 
techniques,＇＇ in 
Proceedings of 2009 
Workshop on Image 
Processing, 
Computer Graphics, 
and Multimedia 
Technologies, 
National Computer 
Symposium, pp. 
473-482, Taipei, 
Taiwan, Republic of 
China, 2009. 
2. B. J. You and W. H. 
omni-vision 
techniques using 
mobile devices,＇ in 
Proceedings of 2012 
Conference on 
Computer Vision, 
Graphics and Image 
Processing, Nantou, 
Taiwan, 2012. 
 
研討會論文 0 0 100%  
專書 1 1 100%  
3. C. J. Wu, S. Y. 
Tsai and W. H. Tsai 
(2012). ＇Automatic 
ultrasonic and 
computer-vision 
navigation device 
and method using the 
same (利用超音波與
電腦視覺偵測之自動
導航裝置及其導航方
法),＇ Republic of 
China Patent, 申請
中. 
申請中件數 1 1 100% 
1. K. F. Chien, W. H. 
Tsai, and C. J. Wu 
(2010). ＇ Video 
surveillance system 
and video encoding 
method based on data 
hiding (基於資訊隱
藏之視訊監控系統及
其資訊隱藏之視訊編
碼方法),＇ Republic 
of China Patent, No. 
I330341, 
2010/09/11. 
專利 
已獲得件數 0 0 100% 
件 
 
件數 0 0 100% 件  
技術移轉 
權利金 9 9 100% 千元  
碩士生 6 6 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
參與計畫人力 
（本國籍） 
專任助理 0 0 0% 
人次 
無 
期刊論文 0 0 100%  國外 論文著作 
研究報告/技術報告 4 4 100% 
篇 
皆為此三年期計畫的
Proceedings of 2nd 
International 
Conference on 
Computer and 
Automation 
Engineering (2nd 
ICCAE), pp. 327-332, 
vol. 3, Singapore, 
2010. 
4. P. H. Yuan, K. F. 
Yang and W. H. 
Tsai, ＇Security 
monitoring around a 
video surveillance 
car with a pair of 
two-camera 
omni-directional 
imaging devices,＇ 
in Proceedings of 
2010 Workshop on 
Image Processing, 
Computer Graphics, 
and Multimedia 
Technologies, 
International 
Computer Symposium, 
pp. 325-330, Tainan, 
Taiwan, 2010. 
 
研討會論文 0 0 100%  
專書 0 0 100% 章/本  
申請中件數 1 1 100% 
2. C. J. Wu, S. Y. 
Tsai and W. H. Tsai 
(2012). ＇Automatic 
ultrasonic and 
computer-vision 
navigation device 
and method using the 
same,＇ U. S. A. 
Patent, No. 
8,116,928 B2, 
2012/02/14. 
專利 
已獲得件數 0 0 100% 
件 
 
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  參與計畫人力 
（外國籍） 博士生 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100字為限） 
在此計畫期間，本研究團隊致力於完成本計畫所預定之九項研究課題，並衍伸發表許多相
關論文。在此計畫的三年中(2009/08 至 2012/07)，本研究團隊共發表了 9 篇國際期刊論文、
4篇國際會議論文，及 6篇國內會議論文。詳細發表情形請參閱結案報告書。 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500字為限） 
自動車的應用日廣，與自動車相關的攝影機與感測技術也日新月異，各種用於提升自動車
功能之相關技術值得持續研究。本計畫「新型攝影、感測與校正技術在電腦視學自動車上
之應用」以三年時間，分析各種攝影機與感測器組合對環境感知與模式化的效果，開發電
腦視覺之智慧性新功能，讓自動車能更精細地了解環境，做到最佳化的自動導航。 
本三年期計畫已完成所有預定之研究項目： 
(1)自動車之多元感測、攝影、校正與應用 
  第一年：整合超音波與模糊控制建構有環境學習能力之自動車作室內導覽 
  第二年：歪斜環場攝影機之校正、影像攤平與應用 
  第三年：利用最新多部立體數位相機作環境影像之整合與展示 
(2)自動車之自動定位、學習、導航與應用 
  第一年：以直線在環場影像下之幾何變形作自動車定位與導航 
  第二年：以環場攝影機利用屋角作室內導航 
  第三年：利用跟人技術自動學習環境特徵與導航路線 
(3)利用新型與多部攝影機作自動車新應用 
  第一年：整合天花板上之多環場攝影機輔助自動車在不規則室內作航行 
  第二年：用前視的雙反射鏡環場攝影機作戶外環境安全監控 
