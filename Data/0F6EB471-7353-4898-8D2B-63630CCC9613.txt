行政院國家科學委員會補助專題研究計畫  成果報告 
用於支援視障者之智慧型“讀文件＂資訊輔具 
Intelligent “Reading Documents” Information Aids for Visually Impaired  
計畫編號：NSC99－2221－E－133－002－ 
執行期限：99 年 8 月1 日至100 年7 月31 日 
主持人： 蔡俊明  臺北市立教育大學資訊科學系 
 
一、中文摘要 
此計畫提出兩個重要演算法，一是智慧型區域二值化演算法，另一是有效率和有效地
物件抽取演算法，此兩演算法，可以應用於文件的二值化和文件影像的分析，更進一步延
伸，搭配OCR、文字轉語音和發聲，可以用於輔助視障者“讀期刊文章＂。 
This project has proposed two important algorithms. One is an intelligent 
region-based thresholding for color document images. The other is an efficient and 
effective object extraction algorithm for color journal article. These two 
algorithms can be applied for binarization document images and document image 
analysis. For further extension, these two methods can combine OCR and 
text-to-speech to help the visually impaired to “reading journal article.＂ 
 
二、研究背景與目的 
從『臺灣數位有聲書推展學會』[1]得知：根據內政部的統計，台灣的視障人口從民國
87年的32000人，到民國97年的55000人，每年以約2000人逐年增加中。視障人口約佔台灣
總人口數的0.24%，較之歐美國家視障人口約佔總人口數的0.5%，顯然我國的數據是偏低
的。而在全數的視障人口中，18歲以下的青少年和兒童人數約有1800人，而45歲以上的中
高齡視障人口約有44000人(約佔總視障人口的八成)，這個數字告訴我們，造成視障的主要
原因並非是所謂的「先天盲」，其實有許許多多的視障人口是來自後天的疾病和因意外而
造成的，主要是因年紀漸長，視功能退化，而變成視障的也不在少數。以上這些視障朋友，
要讀文件，需要將文件轉為點字，但是，除了少數教科書有點字，其他大多數文件，是沒
有點字。另外，還有一群『弱視』朋友，他們的學習，需要帶放大鏡，或是將課本中的字
體放大，但是這樣，書本變的又大，若是小學生，書包變的超重，對這些弱視朋友而言，
真是『沈重的負擔』。 
雖然，這些「先天盲」和「後天盲」同胞的學習，大都利用點字，「先天盲」朋友觸
感比較好，從小學點字，學的很快，但是，「後天盲」的朋友，觸感就沒有那麼好，那要
怎樣幫助這些視障朋友來學習呢？加上，計畫申請人在下我，這幾年因常常看電腦，讀論
文，寫程式，視力越來越差，過不久，可能會步上「後天盲」的地步，所以，為了這些「先
天盲」、「後天盲」以及自己將來的學習方便，在此，提出計畫『用於支援視障者之智慧
型“讀文件＂資訊輔具』之『讀期刊論文(Reading journal papers)』，期望可以為這些視障
朋友，盡一份心力。 
 
三、研究方法與步驟 
Figure 1是所提出用於支援視障者之智慧型“讀文件＂資訊輔具』之『讀期刊論文
3.1. 1 Document border determination 
When printing an article, foreground objects such as texts and graphics are placed at the center of 
the document image. The layout of the article document is arranged by the background space surrounding 
the printed regions in the image. Thus, if the document border of the article is determined, the background 
distribution of the article document can be found. Then, the foreground and the background in the article 
document image can be separated by the background range. The method for document border 
determination is explained first. It is assumed that the color documents’ background distribution is 
unimodel Gaussian distribution or that the background distribution is gradually changing and no tilted. 
3.1.1.1 Scan line color differentiation 
There is a question of how to determine the foreground border in the color document image. An 
example of a color document whose layout includes Chinese text lines which are surrounded by 
background color is shown in Fig. 3(a). Here, the background color gradually changing from left to right. 
The foreground border (see Fig. 3(a)) of this image is defined as the top, the bottom, the left, and the right 
boundaries between the background space and the foreground texts. To obtain the left, the right, the top, 
and the bottom boundaries, a scan line color differentiation algorithm is proposed, which is based on 
these ideas: (1) the color difference between two successive scan lines in the background space is similar, 
and (2) the color difference between two successive scan lines, one being the background space and the 
 
(a) 
  
 (b) (c) 
Fig. 3. Schema of the foreground border determination: (a) foreground border definition, (b) the left and the right
boundaries detection, (c) the top and the bottom boundaries detection. 
Figure 4 shows the unsupervised learning method which is used to determine the average luminance 
differential threshold value. Herein, thirty color document images were used as the training set. From 
these training procedures, the histogram for all scan line differentiation is shown in Fig. 4(a). After 
applying the modified triangle method [2], the average luminance differential threshold value (TALD = 6) is 
shown in Fig. 4(b). 
   
 (a) (b) 
Fig. 4. The average luminance differential threshold is obtained from an unsupervised learning method: (a) 
histogram for scan line differentiation, (b) the average luminance differential threshold obtained by using a modified 
triangle method [2]. 
For most color document images, wide background boundaries are reserved at the border. In order to 
reduce the time complexity of the scanning process, a threshold value, TC, for the scan line counter C is 
used. This value is set as W * 0.05, where W is the width of the document image. Thus, the maximum 
number of the scanned line does not exceed TC scan lines.  
The algorithm to detect the right (Fig. 3(b)), the top, and the bottom boundaries (Fig. 3(c)) are 
similar to the algorithm used to detect the left boundary (as above). This border determination method has 
many contributions as follows: (1) use fewer parameters, (2) applies unsupervised learning approach to 
obtain the parameters, and (3) uses the scan line counter C to reduce the scanning time complexity. 
3.1.1.2 An example for document border determination 
In Fig 3(a), the width and the height are 964 and 130, respectively. The current average luminance, 
Vj, in the first (j = 0) vertical scan line is 124. The next average luminance, Vj+1, in the next vertical scan 
line is 121. The scan line differentiation, ΔV = |Vj - Vj+1| is 3. This value is smaller than the average 
luminance difference threshold, TALD = 6. Thus, the current average luminance, Vj, is updated by the next 
average luminance, Vj+1. The algorithm repeats Step 3 to Step 6 until the scan line differentiation is 
greater than the average luminance difference threshold. In the twentieth vertical scan lines, the current 
average luminance, Vj, is 121. The next average luminance, Vj+1, in the next vertical scan line is 109. The 
scan line differentiation, ΔV = |Vj - Vj+1| is 12. This value is greater than the average luminance difference 
kTTHR HighHigh ×+= σ      (3) 
Here, σ is the standard deviation as computed in the border pixels. k is a noise removal value, 
which is used to adjust the low bound and the high bound of the background distribution. If this 
noise removal value k is large, the range of the background distribution is large. This can 
remove more backgrounds and can reserve fewer foregrounds.  That is, the noise will be 
removed and the foreground objects will be broken. If the noise removal value k is small, the 
range of the background distribution is small, thus removing fewer backgrounds and reserving 
more foregrounds.  That is, the noise will not be removed and the foreground objects will be 
touched. Herein, k is set as 2.2 to properly remove backgrounds and reserve foregrounds. 
6. If the peak numbers are two, the peak with the maximum population is the background 
distribution. The low bound (THRLow) and the high bound (THRHigh) of the background 
distribution are determined by using Eq. (2) and Eq. (3), also. 
7. If the peak numbers are more than two, the algorithm of the gradually changing detection (see 
Subsection 3.2) is proposed to detect whether the background is gradually changing or not. 
8. If the background is not gradually changing, the peak with the maximum population is the 
background distribution. The low bound (THRLow) and the high bound (THRHigh) of the 
background distribution are determined by using Eq. (2) and Eq. (3), also. 
9. If the background is gradually changing, the following algorithm is applied. 
3.1.2.2. Gradually changing detection 
Many color documents have gradually changing background. For example, Fig. 3(a) is shown as a 
typical color document image with a gradually changing background. The characteristics of the gradually 
changing background are: (1) the hue of the background is from pure color to impure color; (2) the 
saturation of the color is from full saturation to not saturated, and (3) the intensity of the color is from 
dark gray level to bright gray level. From characteristic (3), the gradually changing background is 
represented by many close, large, and successive peaks. Thus, we use this characteristic to detect the 
gradually changing background. If background of the color document is detected as gradually changing, 
the low threshold value, TLow, is obtained from the lowest peak in the successive peaks, and the high 
threshold value, THigh, is obtained from the highest peak in the successive peaks. The modified triangle 
method [2] is used to compute these values. The low bound (THRLow) and the high bound (THRHigh) of the 
background distribution are determined by using Eq. (2) and Eq. (3), also. Because the variance in 
gradually changing background is very large, the noise removal value k is set as 0.2 to remove the noise.  
3.1.2.3 An example for background distribution determination 
Fig. 6. Example of journal paper, which is come from the journal in Expert Systems with Applications [3]. 
 
3.2 Document Image Analysis (DIA) 
The layout of the first page in every journal article has much information. For example, the layout in 
Fig. 6 includes journal title with volume number, publish year, and page range; journal title with publisher 
trademark, cover image and journal homepage; article title; author names; affiliations; article info; 
abstract; right statement; introduction; corresponding author; and doi regions. Each region has its 
representative meanings. To analyze and understand the layout in this kind document image, each region 
must be extracted firstly. 
3.2. 1 Layout Extraction 
The layout of a color journal article is composed of different types of foreground regions, such as 
horizontal lines, text lines, text blocks, highlighted regions, tables, and graphics. To segment these 
foreground regions, conventional methods, such as projection profile [4] and connected-component-based 
methods [5], can be applied. However, these methods are very time-consuming when used to segment a 
color journal article image with large foreground regions. Herein, the restricted background projection 
profile is proposed to extract the foreground regions from the color journal article images.  
3.2.1.1 Restricted background projection (RBP) 
Conventional projection (CP) methods, for example [4], scanned the binary image (black pixels in the 
foreground, white pixels in the background) in the horizontal and vertical directions alternately to obtain 
the projection profile. For horizontal scanning, every pixel in the scanning line is checked and the black 
pixels summed to obtain the vertical projection profile. The vertical disjoint blocks are extracted by 
cutting on the valleys in the vertical projection profile. Similarly, the horizontal disjoint blocks are 
extracted by cutting the horizontal projection profile. However, before applying CP methods, the input 
document images must be converted into binary document images. Moreover, such CP methods scan the 
binary pixels in the background and foreground regions at least twice. 
The object of foreground region extraction is to determine the foreground regions automatically in the 
color journal article images. These extracted foreground regions are usually displayed as a rectangle 
(bounding box) or polygon [6]. The simple method to extract the foreground region is to locate the 
left-top and the right-bottom coordinates of the foreground region. That is, the background pixels and the 
pixels in the boundary of the foreground regions need to be scanned, whereas the pixels in the foreground 
regions do not need to be scanned. The proposed RBP algorithm is based on this idea of using the concept 
of the CP method, but it only scans the gray level pixels in the background and in the foreground region 
boundary. This RBP algorithm includes horizontal and vertical restricted background projection methods. 
These two methods are described more detail as follows [7]:  
coordinates from the RBPP. The consecutive non-straight-scan lines from top straight-scan line to bottom 
straight-scan line are grouped as a strip. The straight-scan line before and after the consecutive 
non-straight-scan lines are denoted as the top and bottom coordinates, respectively. The minimum value in 
the LBPP is determined and denoted as the left coordinate. The maximum value in the RBPP is detected 
and denoted as the right coordinates.  
Figure 8 provides an example for horizontal restricted background projection (HRBP) and illustrates 
how to locate the top, bottom, left, and right coordinates of the extraction regions. The LBPP and RBPP 
are obtained by using the leftward and the rightward restricted background projection, as shown in Fig. 
8(a). The LBPP is drawn with red; the RBPP is drawn with green. The top, bottom, and left coordinates 
are obtained from the LBPP. The right coordinate is obtained from the RBPP. The coordinates of the 
second foreground region are shown in Fig. 8(a). After the HRBP, two foreground regions are extracted, 
which include one horizontal text line and one horizontal strip, as shown in Fig. 8(b). The horizontal text 
line includes journal title, volume, publish year, and page ranges. The horizontal strip includes one thin 
horizontal split line, publisher, journal title, journal homepage, and cover image. Thus, this strip need to 
be further processed. 
(a) 
(b) 
Fig. 9. Example for vertical restricted background projection (VRBP). (a) The top background projection profile
(TBPP) is drawn with blue. The bottom background projection profile (BBPP) is shown in purple. (b) The extracted 
foreground regions by using VRBP.  
horizontal split lines to divide this figure into six groups: (1) journal title with volume number, publish 
year, and page range and journal title with publisher trademark, journal cover image, and journal 
homepage; (2) article title, author names, and affiliations; (3) article info and abstract; (4) keywords, text 
of abstract, and right statement; (5) introduction; and (6) corresponding author and doi. The group 
extraction algorithm is described as follows: 
(1) The background range of the color journal article is determined. 
(2) The HRBP algorithm is firstly applied to extract the horizontal strips of color journal article. The 
extraction horizontal strips can be represented as 
( ) ,1, niiHS L=  (4) 
where n is the number of the extraction horizontal strips. HS(i) is ith horizontal strip which is 
represented by a rectangle.  
(3) The candidates of the division line are detected. The detection rule is that if the height of the 
horizontal strip is smaller than TH1 then this horizontal strip is the division line candidate. This 
detection rule can be represented as 
,1,).()()( 1 mjTHeightiHSifiHSjDLC H L=<=  (5) 
where m is the number of the division line candidate. DLC(j) is jth division line candidate which 
is represented by a rectangle. 
(4) The division lines are verified. The verify rule is that if the differential value (DV) between the 
left coordinate of the division line candidate and the left coordinate of the border is smaller than 
TLD then this division line candidate is the true division line. This verify rule can be represented 
as 
,1,)()( okTDVifjDLCkDL LD L=<=  (6) 
LeftborderLeftjDLCDV .).( −=  (7) 
where o is the number of the true division line. DL(k) is kth true division line which is 
represented by a rectangle. 
(5) The groups of the layout are extracted. One division line split two groups. That is, after applying 
above-mentioned steps, o+1 groups will be extracted. These groups can obtained as follows: 
( )
( )
( )
( )
( ) ( )
( ) ( ) ,..
..
).().(
).().(
).().(
).().(
and
TopiiDLBottomiHS
BottomiiDLTopiHS
if
RightiHSMaxRightiiG
BottomiHSMaxBottomiiG
LeftiHSMinLeftiiG
TopiHSMinTopiiG
⎩⎨
⎧
<
>
⎪⎪⎩
⎪⎪⎨
⎧
=
=
=
=
 (8) 
where G(ii) is iith group which is represented by a rectangle. 
Figure 10 presents an example result for HRBP of Fig. 6. After applying step (3) in the proposed 
Continue to apply step (4), the true division lines are verified and are shown in Fig. 12.  
 
Fig. 11. Division candidate lines are detected after applying step (3) in group extraction algorithm. 
However, different group has different meaning. Thus, each group needs further process. 
 
Fig. 13. Extraction results of group extraction algorithm for Fig. 6. 
Fig. 15. Final extraction results in the journal title group. 
group. This result has been shown in Fig. 9(b). Two foreground objects are extracted. One is 
compound object [6]. The other is graphic. The compound object includes one thin horizontal 
split line, publisher, journal title, and journal homepage. Thus, this strip need to be further 
processed. 
(2) Use LBPP to split the compound object in the result of previous step. This result is shown in Fig. 
14(a). Three objects are extracted: one horizontal split line and two compound objects.  
(3) To obtain the new TBPP in the second object in Fig. 14(a). This new TBPP and the original 
BBPP are used to split the second object in Fig. 14(a). This result is shown in Fig. 14(b).Two 
objects are extracted: the publisher’s trademark and journal title with source description. 
(4) Use the new TBPP and the original BBPP to split the second object in Fig. 14(b). This result is 
shown in Fig. 14(c). Two objects are extracted: source description and journal title. 
(5) Use BBPP to split the third object in Fig. 14(a). This result is shown in Fig. 14(d). Two objects 
are extracted: publisher’s name and journal homepage. 
(6) Merge publisher’s trademark and name. The final result for extracting objects in journal title 
group is shown in Fig. 15. 
3.2.1.4 Objects extraction in article title group 
The second group is the article title group. It includes three parts: article title, author names, 
and affiliations. When observation this group in Fig. 13, the font size among article title, author 
names, and affiliations are different. Thus, the height of the horizontal strips in Fig. 10 is used to 
merge article title, author names, and affiliations. The method of objects extraction in article title 
group is described as follows: 
(1) Extracting the horizontal strips (Fig. 10) in the article title group. 
(2) Merging article title, author names, and affiliations. The merge rule is that if the heights of two 
adjacent horizontal strips are similar, these two adjacent horizontal strips are merged. 
The final result for extracting objects in article title group is shown in Fig. 16. Article title, author 
names, and affiliations are merged and are displayed with bounding box, respectively. 
Fig. 18. Final extraction results in the keywords group. 
respectively. 
3.2.1.7 Objects extraction in introduction group 
The fifth group is the introduction group. It includes two columns: left and right column 
texts. When observation the horizontal strips in introduction group in Figs. 10 and 13, the sizes 
of the width are different. Thus, the width sizes of the horizontal strips are used to split 
introduction group into two columns: left and right text blocks [6]. The method of objects 
extraction in introduction group is described as follows: 
(1) Extracting the horizontal strips (Fig. 10) in the introduction group (Fig. 13). 
(2) Use the left coordinate in each horizontal strip to project to obtain the left coordinate projection 
profile (LCPP).  
(3) Use LCPP to extract objects in introduction group. This result is shown in Fig. 19. Two column 
text blocks are extracted and are displayed with bounding box, respectively. 
Fig. 19. Extraction results in the information group. 
  
Fig. 21. Final layout extraction results for Fig. 6. 
   
 (a) (b) (c) (d) 
   
 (e) (f) (g) (h) 
Fig. 22. Binarization of a color document image with highlighted regions: (a) original image, (b) the proposed 
method, (c) Otsu’s method, (d) Chou’s method, (e) region-based Niblack’s method, (f) Niblack’s method, (g) 
Sauvola’s method, and (h) Tseng’s method. 
automatically by determining the background range in the border and extracting the foreground region 
using background pixels and the boundary foreground pixels. 
4.2. Quality measurement  
Although high-speed image binarization processing is priority concern, the quality of the results must 
also be considered. Figure 22(a) displays an original color document image taken from a magazine that 
involves two highlighted regions. The main background color of this image is red, and the background 
colors of the two highlighted regions are black and yellow. When using the proposed method, as shown in 
Fig. 22(b), the characters located in both the non-highlighted and highlighted regions were effectively 
separated from the background. When using the Otsu’s global thresholding method, as shown in Fig. 
22(c), the characters located in the highlighted region with yellow background color were separated from 
the background and the characters located in the other regions were not. Figures 22(d) and 22(e) show the 
binarization results of Chou’s and Niblack’s region-based thresholding methods. These results are very 
similar to Otsu’s results and are still unsatisfactory. Figures 22(f) and 22(g) show the binarization result of 
Niblack’s and Sauvola’s local thresholding methods. It can be see that a lot of noise interference occurs in 
main background color is red, the second is yellow and the third is black. (2) When the color document is 
transformed into gray level image, the main background color lies in the middle gray level, the second 
background color is close to high gray level, and the third background color is at the lowest gray level. 
When applying the compared global, region-based, local, and hybrid thresholding methods, the resulting 
binarization is unsatisfactory, as it is seen that the compared methods thresholding the color document 
image with background color are prone to a high gray level. However, the compared methods cannot 
threshold the color document image with a background color close to a low gray level properly. Further, 
when the color document image has highlighted regions with different background colors, the compared 
method do not produce satisfactory binarization results.  
Figure 23(a) illustrates an original color document image taken from a magazine containing gradually 
changing background color. The saturation of the background color is changing from left to right. When 
using the proposed method, as shown in Fig. 23(b), the characters located in both the saturated and the 
unsaturated regions were effectively separated from the background. When using Otsu’s global 
thresholding method, shown in Fig. 23(c), the characters located in the saturated region were not 
separated from the background. Figures 23(d) and 23(e) show the binarization results of Chou’s and 
Niblack’s region-based thresholding methods, in which characters were segmented successfully from the 
background; however, significant noises interference still occurred in the area without characters. Figures 
23(f) and 23(g) display the binary document images obtained by applying Niblack’s and Sauvola’s local 
thresholding methods. The characters were successfully extracted from the background; however, noises 
interference still occurred in the area without characters. The binarization quality for Sauvola’s method 
was also acceptable; however, noises still occurred at the top and bottom borders. Figure 23(h) 
summarizes the binarization results of Tseng’s thresholding method. The characters located in the 
saturated region were successfully thresholded from the background; however, minimal noise noises 
interference still occurred in the area without characters. From above-mentioned comparison, the 
proposed method can produce the most satisfactory binarization results for a color document image with a 
gradually-changing background. 
binary document images obtained by applying Niblack’s and Sauvola’s local thresholding method. Under 
these circumstances, the foreground characters were successfully extracted from the background; however, 
significant noises interference still occurred in the area without characters. The binarization quality for 
Sauvola’s method was also acceptable; however, some minimal noise still resulted. Further, the 
foreground characters in the Sauvola’s result are hollow. Figure 24(h) summarizes the binarization results 
of Tseng’s hybrid thresholding method. The foreground characters were successfully thresholded from 
the background; however, the reversing action must be applied again to obtain a normal binarization 
result. From the above-mentioned comparison, the proposed method can produce satisfactory binarization 
result for a color document image with a dark background color and a bright foreground color. 
Figure 25(a) illustrates an original color document image taken from a name card containing 
inseparable foreground and background. The foreground characters in yellow, “ ”, are mixed 
with the background color. When this image is converted into a gray level image, both background color 
and foreground characters are prone to high gray level; they are inseparable in the resulting gray level 
image. When using the proposed method, shown in Fig. 25(b), the foreground characters were separated 
from the background. When using Otsu’s global thresholding method, shown in Fig. 25(c), the 
foreground characters were separated from the background; however, some foreground characters 
“ ” in the binarization result are lost. Figures 25(d) and 25(e) show the binarization results of 
Chou’s and Niblack’s region-based thresholding methods, in which most of foreground characters were 
segmented successfully from the background; however, some foreground characters are “broken” in 
Chou’s result and some are blurred in Niblack’s result. Figures 25(f) and 25(g) display the binary 
document images obtained by applying Niblack’s and Sauvola’s local thresholding method. The 
foreground characters were successfully extracted from the background; however, the characters in the 
Niblack’s and Sauvola’s results are hollow. Figure 25(h) summarizes the binarization results of Tseng’s 
hybrid thresholding method. The foreground characters were successfully thresholded from the 
background; however, some foreground characters are lost. From the above-mentioned comparison, the 
proposed method can produce the most satisfactory binarization result for a color document image where 
where X is the number of characters in the above-mentioned experimental document images and Z is the 
number of characters correctly recognized by MODI. Further, the OCR precision rate [11] is defined by 
YZ /     (10) 
where Y is the number of characters recognized by MODI. To determine the impact o the different 
binarization methods on recall and precision rates, all binarized results are fed into the MODI. The recall 
and the precision rates are shown in Tables 2 and 3, respectively. The boldface figures in the table 
indicate the best performances.  
According to Table 2, the MODI OCR achieves average recall rates of 94.65%, 64.61%, 80.55%, 
41.11%, 19.68%, 45.48%, and 75.08% when using the proposed, Otsu’s, Chou’s, region-based Niblack’s, 
Niblack’s, Sauvola’s, and Tseng’s methods, respectively. According to Table 3, the MODI OCR achieves 
average precision rates of 94.49%, 65.70%, 89.47%, 42.57%, 24.34%, 48.71%, and 68.07%, respectively. 
Figures 26, 27, 28, and 29 show the MODI OCR results for the binarization results in Figs. 22, 23, 24, 
and 25, respectively. 
Table 2 
Comparison of MODI OCR recall rate among the proposed, Otsu’s, Chou’s, region-based Niblack’s, Niblack’s, 
Sauvola’s, and Tseng’s methods. 
 Tested color document images 
Binarization method Fig. 22(a) Fig. 23(a) Fig. 24(a) Fig. 25(a) Average
Proposed 97.56 100 95.22 85.82 94.65 
Otsu’s 82.93 0 91.17 84.33 64.61 
Chou’s 54.88 100 88.23 79.10 80.55 
region-based Niblack’s 31.71 81.25 0.74 50.74 41.11 
Niblack’s 0 0 0.37 78.36 19.68 
Sauvola’s 2.44 100 0.37 79.10 45.48 
Tseng’s 82.93 93.75 40.07 83.58 75.08 
Table 3 
Comparison of MODI OCR precision rate among the proposed, Otsu’s, Chou’s, region-based Niblack’s, Niblack’s, 
Sauvola’s, and Tseng’s methods. 
 Tested color document images 
Binarization method Fig. 22(a) Fig. 23(a) Fig. 24(a) Fig. 25(a) Average
Proposed 97.56 100 95.22 85.19 94.49 
Otsu’s 83.95 0 93.23 85.61 65.70 
Chou’s 76.27 100 90.23 91.38 89.47 
region-based Niblack’s 76.47 33.33 1.36 59.13 42.57 
Niblack’s 0 0 5.26 92.11 24.34 
Sauvola’s 1.29 100 0.57 92.98 48.71 
Tseng’s 83.95 62.50 40.98 84.85 68.07 
gradually-changing background. However, these methods create unacceptable foreground noises in 
background areas, as demonstrated in Figs. 23(d) and 23(g). 
Fig. 28. The MODI OCR results for the binarization results in Fig. 24. 
(3) Sauvola’s method produces the highest OCR precision rate for color document image with 
inseparable foreground and backgrounds. However, its OCR recall rate is 79.10%, and visual quality 
is poor, as it produces hollow foreground characters, as shown in Fig. 25(g). 
5. Conclusion and future works 
First, this work presents a method for intelligent region thresholding using color document images 
Fig. 29. The MODI OCR results for the binarization results in Fig. 25. 
另外，還有1篇在準備中，將投稿於國際期刊。由上述發表成果，可以看出本次國科會計劃成
果豐碩，感謝國科會的支持，謝謝。 
 
 
References 
[1]  臺灣數位有聲書推展學會http://daisy.web66.com.tw/web/UPT?UPID=50645 . 
[2]  C.M. Tsai, H. J. Lee, Binarization of color document images via luminance and saturation color 
features, IEEE Transactions on Image Processing 11 (4) (2002) 434-451. 
[3]  Y.L. Chen, Z.W. Hong, C.H. Chuang, A knowledge-based system for extracting text-lines from 
mixed and overlapping text/graphics compound document images, Expert Systems with Applications 
39 (1) (2012) 494-507. 
[4]  Nagy G., Seth S., and Viswanathan M., A prototype document image analysis system for technical 
journals, IEEE Computer 25 (7) (1992) 10-22. 
[5]  Fletcher L. A., and Kasturi R., A robust algorithm for text string separation from mixed text/graphics 
images, IEEE Transactions on Pattern Analysis and Machine Intelligence 10 (1998) 910-918. 
[6]  C.M. Tsai and H.J. Lee, Efficiently extracting and classifying objects for analyzing color documents, 
Machine Vision and Applications 22 (1) (2011) 1-19. 
[7]  C.M. Tsai, Intelligent region-based thresholding for color document images with highlighted regions, 
Pattern Recognition, doi:10.1016/j.patcog.2011.09.024. 
[8]  Otsu N., “A thresholding selection method from gray-scale histogram,” IEEE Trans. Systems, Men, 
and Cybernetics, Vol. 9, pp. 62-66, 1979. 
[9]  Chou C. H., Lin W. H., and Chang F., “A binarization method with learning-built rules for document 
images produced by cameras,” Pattern Recognition, Vol. 43, No. 4, pp. 1518-1530, 2010. 
[10]  Niblack W., An Introduction to Digital Image Processing, Englewood Cliffs, N. J.: Prentice Hall, pp. 
115-116, 1986. 
[11]  Sauvola J. and Pietikainen M., “Adaptive Document image binarization,” Pattern Recognition, Vol. 
33, pp. 225-236, 2000. 
[12]  Tseng Y. H. and Lee H. J., “Document Image Binarization by Two-Stage Block Extraction and 
Background Intensity Determination,” Pattern Analysis and Applications, Vol. 11, pp. 33-44, 2008. 
 
 
 
 
出席國際學術會議心得報告 
                                                             
計畫編號 NSC 99-2221-E-133-002- 
計畫名稱 用於支援視障者之智慧型"讀文件"資訊輔具 
出國人員姓名 
服務機關及職稱 
蔡俊明 
臺北市立教育大學資訊科學學系 
副教授 
會議時間地點 
December 25 – 26, 2010 
Shanghai Olympic Hotel, Shanghai, China 
會議名稱 2010 International Conference on Engineering Education and Education Technology (EEET 2010) 
發表論文題目 Improving Students’ Reading Attitude via the Construction of Class Library with Book-Borrowing System 
 
三、參加會議經過 
2010/12/25下午於松山國際機場搭乘 1415長榮航空飛往上海虹橋機場，轉乘 10號
和 4號地鐵於傍晚抵達會議場所 Shanghai Olympic Hotel，註冊取得資料後，到下榻房間
休息。 
2010/12/26早上參加 Open Ceremony 並聆聽 Prof. Chin-Chen Chang的 Keynote 
Speech: Visual Cryptography。因為 EEET2010與 ICRCCS 2010 and ICCSP 2010一起合辦，
所以，利用時間聆聽 ICRCCS 2010 and ICCSP 2010的 Oral Session 1 and Oral Session 2。 
2010/12/26中午參加 Buffet Lunch。 
2010/12/26下午參加 EEET2010 Oral Session 3報告並發表論文。 
2010/12/26晚上參加Welcome Banquet。 
2010/12/27白天參加當地自由行，順道了解上海。晚上 1950搭乘長榮航空機離開上
海，回到松山國際機場。 
 
四、與會心得 
這次 EEET2010與 2010 International Conference on Research Challenges in Computer 
Science (ICRCCS 2010), 2010 The 2nd International Joint Conference on Photonics and 
Image in Agriculture Engineering (PIAGENG 2010), 2010 Second IITA International Joint 
Conference on Artificial Intelligence (IITA-JCAI 2010), 2010 International Conference on 
Circuit and Signal Processing (ICCSP 2010)一起合辦，所以，可以順道聆聽其他會議所發
表的文章，得到許多新的知識，也認識了不少人，讓我增廣見聞，感謝國科會提供經費，
讓我有機會參與國際學術研討的盛會。 
 
 
 
 
 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：蔡俊明 計畫編號：99-2221-E-133-002- 
計畫名稱：用於支援視障者之智慧型’讀文件’資訊輔具 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 3 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 4 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
