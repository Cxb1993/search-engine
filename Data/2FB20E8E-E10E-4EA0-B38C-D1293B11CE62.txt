污染，而導致微陣列資料產生了遺失值(Missing 
Values)，而遺失值的存在，會影響到資料分析的品
質。而有關遺失值的回復，最主要可以分成兩個步
驟，第一就是選擇前 k個和含有遺失值基因在表現
值上最相似的基因群出來，第二就是利用選出來基
因群的資訊，來回填遺失值。而到目前為止，在第
一個步驟下，大部份的方法，都是利用基因之間的
表現值，來選擇前 k個具有相似表現值的基因群，
而這些方法，往往都忽略了生物上的意義。所以在
本論文中除了考慮表現值的部份外，我們提出了一
方法 PGAKNN (Protein and Gene Annotation K 
Nearest Neighbors)，整合了表現值、基因和蛋白質
功能相似的特性，將之列入一起評估考量。 
 
圖1 PGAKNN方法流程圖 
在本研究方法中，一共可分為三個部份，第
一個為 Filtering phase，第二個為 Selection phase，
第三個為 Recovering phase。圖 1 是我們整個方法
的流程圖。在第一個階段，由於原始的資料中，已
經含有遺失值的存在，所以我們必須要先把含有遺
失值的基因刪除掉，接著再把沒有註解的基因和蛋
白質也刪除掉，以確保資料中的每一個基因或基因
所對應的蛋白質皆有被註解到，這樣子我們才能計
算出兩個基因或蛋白質的功能相似度。在第二個階
段下，我們會先去定遺失率的多少，來決定遺失值
的個數。而在 selection phase下會有三個步驟，而
此三個步驟是去計算含有遺失值基因和其他基因
之間在表現值、基因和蛋白質功能相似度上的比
較，逐步篩選。最後挑出最具代表性的前 k名基因
出來，當作回復遺失值的參考基因。最後第四個步
驟，利用這 k個基因的資訊，來回復遺失值。 
 
3.1.1距離量測計算（Distance Measurements） 
 
尤拉距離是以幾何空間的概念，將每一個特
徵(feature)看作一個維度，而每個項目(item)看作一
個點，然後去計算點與點在p維空間裡的距離而
得，距離愈大相似度愈小，反之距離愈小則相似度
愈大，兩個具有p個維度(dimension)的g1與g2,，,, 其
中g1=<e1,e2,…,ep>，g2=<f1,f2,…,fp>尤拉距離公式定
義如下： 
2
1
21 )(),( ∑
=
−=
p
k
jkik feggd
             (1) 
 
3.1.2 Gene Ontology 的基本架構 
 
基本上 GO[7][8]是由三個部份所組成
的:Molecular Function(MF)、Biological Process(BP)
和 Cellular Component(CC)。其中MF的用途為以分
子的角度，描述基因的產物的活動，例如催化
(catalytic activity)或轉錄調控(transcription regulator 
activity)等，BP是由一個或是多 MF來完成，有時
很難去分辨 MF 和 BP 的不同，不過 BP 所描述的
動作必定是包含一個以上的步驟，不像MF只描述
單一的生化活動，而 BP 不等於 pathway，它沒有
描述 pathway 裡複雜的關係，CC 的用途表示細胞
的某個部份或位置，描述基因在細胞的那個位置發
揮它的功能。這些分支都是由 GO term所構成，而
在 GO上每一個節點皆代表一 GO term，而這些 GO 
term 是有結構性的，在愈高層的 term 所代表的意
義愈廣泛，而愈低層 term 的意義愈狹隘，而生物
學家可以用利用GO所提供的功能來查詢不同階層
的 term。 
 
3.1.3 GO Term之權重值計算 
 
要計算在 GO 上每一個 term 的權重值的方
法，是採用在[9]中所提出在“is-a”的架構中，使用
Information content 方法來計算。使用 Information 
Content 的方法來計算權重值的時候，越常用來被
註解的 GO Term 其重要性和權重值愈低，而較不
常用來被註解的 GO Term 則其重要性和權重值愈
高。當一個 GO Term 被用來註解一個基因之後，
這個 GO Term的註解個數會加一，並且在這個 GO 
Term上層的所有父節點的GO Terms的頻率也會累
加一。得到各 GO Term 之註解個數之後，將註解
個數轉換成機率，而每一個 term 的機率值，我們
以 p(t)來表示。最後得到各 GO Term之機率 p(t)將
其代入公式(2)，經過轉換後即可到各 GO Term之
權重值。 
))(ln()( tptw −=          (2) 
 
3.1.4 基因語意相似度計算 (Semantic Similarity) 
 
在本論文中，我們假設每一個基因至少都會
有一個或一個以上的 term 來註解此基因，若要去
計算不同基因之間的兩個 term的在GO上的語意相
似度時，我們使用 information content 的方法去做
交叉比對。 
若要計算t1和t2在GO上的語意相似度，則分別
找出這兩個term的所有共同父節點(share parents)出
來，然後再對這所有共同父節點找出其權重值最大
者，以權重為最大的值，來當作t1和t2在GO上的語
意相似度。而其計算方式如公式(2)所示。 
)))(ln(max(),( tpttsim ji −=        (3) 
 2
之分群分析。本研究成果已被 IEEE Transactions on 
Fuzzy Systems 期刊所接受, 其為國際上關於模糊
理論及系統領域上最高水準之期刊之ㄧ，亦足見本
研究具有高度之學術價值。 
 
 4
3.2.1 Similarity-Based PCM ( SPCM ) 簡介 
 
SPCM是一個創新之模糊分群方法，可適用於
分析非空間相似度量測之資料。在 SPCM中，我們
設 計 了 新 穎 的 Mountain Function, Fuzzy 
Membership Update Equation, Center Update 
Equation, Mountain-Elimination Function 等, 使能
適用於分析非空間相似度量測之資料。因此，SPCM
之最大優點在於它可以自動產生分群結果而不需
要使用者輸入任何參數，達到自動化分群之目標。
同時，SPCM對於以相似度為基礎的分群分析，即
使在高雜訊的環境之下，仍具有高效率、高分群品
質與高度自動化，其分群品質可顯著超越既有之方
法。 
在執行 SPCM 之前，必須先根據給定的資料
計算出一個相似矩陣（Similarity Matrix）S，此矩
陣紀錄著各基因間的相似程度，值的範圍為[0, 1]。
此矩陣可以用任何方法得到，只要符合應用即可 
(例如： Euclidean distance, Pearson’s correlation 
coefficient 等)。根據此相似矩陣，SPCM即可以把
資料分群而不需要使用者輸入分群的群數。SPCM
演算法列於圖 3。下列將詳細介紹 SPCM的演算法。 
 
3.2.1.1 重新定義的Mountain Function in Mountain 
Method 
 
Possibilistic c-Means (PCM)最大的優勢就是
membership function及分群群數是相互獨立的，而
且分群結果不易受雜訊的干擾。然而 PCM 需要好
的初始值和好的量化估量在這個研究中，mountain 
function被重新定義為： 
0,))
)1(
),(1(exp()(
1
1 >−
−−= ∑
=
r
S
jiSxM
n
j
r
i ,(10) (11) 
其中，ski 是物件xk和物件xi之間的相似度，s代
表整個相似矩陣S的平均值。在統計上來說，絕大
多數位於同一群的物件之間的相似度會高於平均
值 s，除了雜訊之外。另外，power parameter r在此
函式上扮演很重要的角色，因為它跟正規劃後的數
值 1- s 有關係。因此，Correlation Comparison 
Algorithm CCA) [14]被拿來選擇較好的r，CCA可
以用來估算mountain function接近的密度形狀，在
執行過mountain function後，在mountain function上
擁有最大值的物件會被選為第c群初始的中心v
(
3.2.1.2 在 PCM 上重新定義的 Membership Update 
在 PCM上Membership Update Equation原本
是無
c，接
下來，執行 PCM可以得到每一個物件 xk的
membership degree uck。 
 
Equation 
 
法執行以相似度為基礎的資料，所以需要重新
定義。在此研究中，Membership Update Equation
被重新定義為： 
⎪⎪
⎪⎪
⎩
⎪⎪
⎪⎪
⎨
⎧ ⎛
⎟⎟⎠
⎞
⎜⎜⎝
⎛
≥
⎟⎟⎠
⎞
⎜⎜⎝ −
−
−
=
−
−
otherwise.             ,
2
, if      ,
2
1
1
1
1
2
1
2
m
c
ck
cck
m
c
ck
ck
sv
sv
sv
u
η
ηη  
                                    (11),  
degre
其中，uck是物件xk相對於第c群的membership 
e，svck代表物件xk和中心vc的相似度。m∈[1, ∞]
被稱為PCM的fuzzifier。ηc決定當membership degree
等於 0.5的相似度，被重新定義為： 
 
∑
∑
=
== N
k
m
ck
N
k
ck
m
ck
c
u
svu
K
1
1
2
η .                (12) 
在一般的PCM中，K通常被設為 1，對於SPCM
而言，membership degrees不用依靠不同群中的相同
資料。因此，在SPCM中，每一群都和其他分群是
彼此獨立的。圖 2是SPCM的membership degrees，
其中ηc = 0.5，m = 2，結果顯示在實際應用上是很
棒的。 
 
3.2.1.3 在 PCM 上重新定義的 Center Update 
大多數的fuzzy clustering方法只可以處理空間
性的資
圖 2. SPCM membership function，ηc = 0.5 相
對於不同數值的 fuzzifier parameter m. 
 
Equation 
 
料(例如：在Euclidean 空間)而不能處理非空
間性的資料 (例如：使用 Pearson’s correlation 
coefficient為相似度測量)。跟一般的PCM不同，我
們提出的SPCM可以處理非空間性的資料。首先，
針對每一群，藉由重新定義的mountain function找
到中心點vc，SPCM會重新計算相似向量svc = {svc1, 
svc2, …, svcn}去模擬re-centering的步驟，其中svck 代
表在第c群中，物件 xk 和虛擬中心 vc 間的相似
度。Similarity Vector Updating Equation定義如下： 
 
其中 X  和 Y各表示X和Y向量的平均值，SX和SY表
示其各自的標準差(standard deviation)。 
 接下來，將 correlation-base 的 PCC 轉成
distance-based 的相似度，我們使用下列的轉換公
式: 
PCC’(X, Y)=(1－∣PCC(X, Y) ∣)f      (17) 
公式中的參數 f 是用來調整高與低相似度間
的差異。Zhou et al. [19] 和 Zhu et al. [20]各自在論
文中建議 PCC轉換中最佳的 f值為 6，因此我們也
延用此項參數設定。 
在兩群比較類型部份，我們定義如下的公式
來計算兩基因g1和g2在兩群比較類型裡的相似度: 
定義 1: 假設在Group 1和Group 2兩端，基
因g1 和 g2 的表現值距離各自為d1 和 d2，則其相
似度定義如下: 
SimG(g1, g2) = (d1+d2) × cosθ,  
 6
，其中θ代表d1和d2中心點連線之線段與水平線夾角
值。 
例如，在圖 4中，令k=P，基因g1 和 g2表現
向 量 為 Vg1{C}=<
},{
1
1CTPe , > and 
V
}{
1
2Ce
g2
{C}=<
},{
2
1CTPe , >。d
}{
2
2Ce
1 和 d2中點連線x1，而X1
與水平虛線的夾角θ，可以利用三角函數tan的反函
數(tan-1)來計算。在這個例子裡，d1=d2=2 和θ等於
45 度角，我們可以求出SimG(g1, g2) = (2+2) × 
cos45= 22 。此外，在圖 4(b)中，由於d1=d2=2 且
θ=45度，所以可以得到相同的SimG(g1, g2)相似度
值。由於，我們希望找出在兩群間具有顯著表現值
差異的基因配對，因此，我們定義一個θ門檻值，
θmin，來控制基因配對的差異程度所形成的夾角必
須大於此門檻值。 
1
1
0
x
y
3
},{
1
1CTpe
}{
1
2Ce
}{
2
2Ce
},{
2
1CTpe
d1
d2
},{ 1CTpm
}{ 2Cm
h1
2
x1
1
1
0
x
y
3
},{
1
1CTpe
}{
1
2Ce
}{
2
2Ce
},{
2
1CTpe
d1
d2
},{ 1CTpm
}{ 2Cm
h1
2
x1
 
 
 
 
 
定義 2: 對於每ㄧ個基因對(gene pair) g1 和 
g2而言，令時間序列與兩群比較類型的整合相似度
為 TGmix(g1,g2)=(SimG(g1,g2), PCC’(g1,g2))。 
如圖 5，我們將每ㄧ對的基因相似度對應到 2
為空間上的ㄧ個點時。 下一步，我們使用密度基
礎叢集演算法，如 CAST[21]，來對所有的基因對
進行分群，其目的是要把 2維空間上，距離相近的
點分到同ㄧ群裡。由於我們在時間序列相似度部分
已經將 correlation-based 轉為 distance-based，因此
在 Y 軸上，數值愈小代表兩基因表現向量月相似;
而在兩群比較類型裡，SimG(g1,g2)代表著兩基因在
不同群間的差異與在同ㄧ群內表現值相近的程
度，其值越小表示顯著性越高。由此可知，在圖三
的 2維空間中，越接近原點(0,0)者，即是我們越關
注的基因對。 
1
2
PCC＇
1 2 3 40
Cluster 1
Cluster 2
Cluster 3
SimG  
 
 圖 5. TGmix 相似度範例 
 
3.4 效率評測 
 
3.4.1高品質資料前處理之評測 
 
在這裡我們使用真實的酵母菌微陣列資料來
測 試 我 們 的 方 法 ， 是 由 一 公 開 網 站
（http://sgdlite.princeton.edu/）所下載而來，而有關
這些酵母菌基因的註解資料是由 Gene Ontology
（http://www.geneontology.org/）的網站下載而來。
phosphate 是酵母菌在 phosphate accumulation 和
polyphosphate metabolic 的研究[22]，共有 6015基
因和 8個實驗樣本，在扣除資料中原來就含有遺失
值基因、GO上沒有被註解到的基因(BP:有 5241個
基因有被註解到)和基因所對應的蛋白質在 GO 上
沒有被註解到的(BP:4371 個蛋白質有被註解到)還
有 4371 基因，而註解率為 0.90%。表 1 為實驗資
料的摘要。 
表格 1 兩個酵母菌微陣列資料摘要 
Dataset phosphate 
Total Genes 6015 
After filtering 5785 
Filtering Non annotated genes 5241 
Filtering Non Protein annotate genes 5490 
Condition 8 
Annotate rate 90% 
 圖 4. 基因 g1 和 g2 間兩群比較類型之相似度範
例。 
在實驗效能評估上，我們採用正規化均方根
（Normalized Root Mean Square）的評估方式，來
決定準確性的高低，因為我們知道遺失值的設計，
是由人工的方式產生出來，而當 NRMSE逼近於 0
的時候，則預測出來的值愈準確，若 NRMSE愈大
則預測出來的值愈差。 
][var
])[( 2
answer
answerguess
yiance
yymean
NRMSE
−=        (18) 
yguess：表示預測值。yanswer：表示實際值。 
若遺失值的個數有 100個的話，那麼其預估值和正
確值也會各有 100個，而此評估方法則是去利用預
  
表格 2. 
Countries Data (CD) Dissimilarity Matrix 
Country C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12 
C1: Belgium 0.00 5.58 7.00 7.08 4.83 2.17 6.42 3.42 2.50 6.08 5.25 4.75 
C2: Brazil 5.58 0.00 6.50 7.00 5.08 5.75 5.00 5.50 4.92 6.67 6.83 3.00 
C3:China 7.00 6.50 0.00 3.83 8.17 6.67 5.58 6.42 6.25 4.25 4.50 6.08 
C4: Cuba 7.08 7.00 3.83 0.00 5.83 6.92 6.00 6.42 7.33 2.67 3.75 6.67 
C5: Egypt 4.83 5.08 8.17 5.83 0.00 4.92 4.67 5.00 4.50 6.00 5.75 5.00 
C6: France 2.17 5.75 6.67 6.92 4.92 0.00 6.42 3.92 2.25 6.17 5.42 5.58 
C7: India 6.42 5.00 5.58 6.00 4.67 6.42 0.00 6.17 6.33 6.17 6.08 4.83 
C8: Israel 3.42 5.50 6.42 6.42 5.00 3.92 6.17 0.00 2.75 6.92 5.83 6.17 
C9: USA 2.50 4.92 6.25 7.33 4.50 2.25 6.33 2.75 0.00 6.17 6.67 5.67 
C10: USSR 6.08 6.67 4.25 2.67 6.00 6.17 6.17 6.92 6.17 0.00 3.67 6.50 
C11: Yugoslavia 5.25 6.83 4.50 3.75 5.75 5.42 6.08 5.83 6.67 3.67 0.00 6.92 
C12: Zaire 4.75 3.00 6.08 6.67 5.00 5.58 4.83 6.17 5.67 6.50 6.92 0.00 
 
(a) 
圖 8. (a)TGmix正負調控相關基因對分布圖 
 
表格 3. 
Results for Countries Data (CD) 
Mountain Value (10-2) 
Country 
M1 M2 M3 M4 C1
France 4.876 0 0 0 0.9
USA 4.641 0 0 0 0.9
Belgium 4.323 0 0 0 0.8
Israel 1.408 0 0 0 0.6
Cuba 1.407 1.407 0 0 0.0
USSR 1.360 1.360 0 0 0.1
Yugoslavia 0.455 0.455 0 0 0.1
China 0.289 0.289 0 0 0.0
Zaire 0.715 0.715 0.715 0 0.2
Brazil 0.700 0.700 0.700 0 0.2
India 0.110 0.110 0.110 0.110 0.0
Egypt 0.208 0.208 0.208 0.208 0.3 
(b) 
圖 8. (b)圖 8(a)所對應之調控網路圖 
 
 
 
 Membership Belong to Cluster 
C2 C3 SPCM FRC (3) R-FRC (3) 
71 0.093 0.246 1 1 1 
25 0.065 0.310 1 1 1 
85 0.085 0.364 1 1 1 
40 0.093 0.204 1 1 1 
37 0.973 0.072 2 0 0 
23 0.938 0.099 2 0 0 
74 0.669 0.065 2 0 0 
76 0.617 0.142 2 0 0 
18 0.095 0.960 3 2 2 
21 0.065 0.931 3 2 2 
96 0.170 0.415 - 2 2 
50 0.199 0.382 - 2 3 8
 10
Groups in Data: An Introduction to Cluster 
Analysis. New York: Wiley, 1990. 
[26] R. N. Dave and S. Sen, “Robust fuzzy clustering 
of relational data,” IEEE Transactions on Fuzzy 
Systems, vol. 10, no. 6, pp. 713-727, 2002. 
[27]V. S. Tseng and C. P. Kao, “Efficiently Mining 
Gene Expression via A Parameter-less Clustering 
Method,” IEEE/ACM Transactions on 
Computational Biology and Bioinformatics, vol. 
2, no. 4, pp. 355-365, 2005.  
[28]Vincent S. Tseng and H. H. Yu, 
“Multi-Information based Gene Scoring with 
Applications on Analysis of Hepatocellular 
Carcinoma ,” to appear in LNCS Transactions on 
Computational Systems Biology. 
[29] Vincent S. Tseng and C. P. Kao, “A Novel 
Similarity-based Fuzzy Clustering Algorithm by 
Integrating PCM and Mountain Method,” 
accepted for publication in IEEE Transactions on 
Fuzzy Systems. 
[30] Vincent S. Tseng, Y. H. Chen, C. H. Chen, J. W. 
Shin, “Mining Fuzzy Association Patterns in 
Gene Expression Patterns,” in International 
Journal of Fuzzy Systems, vol. 8, no. 2, 2006. 
[31]Vincent S. Tseng, L. C. Chen and J. J. Liu, 
“Discovering Gene Relations by Mining Similar 
Subsequences from Time Series Microarray,” in 
Proc. Int’l Workshop on Science of Artificial, 
Taiwan, 2005 (Invited Paper). 
[32]Vincent S. Tseng, L. C. Chen and Y. D. Hsieh, 
“Discovering Gene Clusters via Integrated 
Analysis on Time-Series and Group-Comparative 
Microarray Datasets,” in Proc. 19th IEEE Int’l 
Conf. on Computer-based Medical Systems 
(CBMS’06), USA, 2006. 
[33] Vincent S. Tseng, Lien-Chin Chen, and Jian-Jie 
Liu, “Gene Relation Discovery by Mining 
Similar Subsequences in Time-Series Microarray 
Data”, in IEEE Symposium on Computational 
Intelligence in Bioinformatics and Computational 
Biology (CIBCB), USA, April, 2007. 
[34] Vincent S. Tseng and C. P. Kao, “Towards 
Parameter-less and Similarity-based Fuzzy 
Clustering based on PCM Method,” in Proc. 
IEEE International Conference on Systems, Man, 
and Cybernetics, Taiwan, Oct., 2006. 
 
 
報告內容應包括下列各項： 
一、參加會議經過 
 
 
二、與會心得 
 
 
三、考察參觀活動(無是項活動者省略) 
 
 
四、建議 
 
 
五、攜回資料名稱及內容 
 
 
六、其他 
 
 
 
 
 
 
 2
• Integration of genetics, environmental, imaging, and clinical data of patients for medical decision 
support and public health applications. 
• Activity monitoring and anomaly detection for 
o Bio-surveillance,  
o Health activity monitoring, and  
o Environmental monitoring (e.g. air/water pollution, deforestation, forest fire).  
• Systems biologic approach in integration, modeling, and synthesis of marco/micro level of biomedical 
data to advance public and human health. 
 
Workshops 
Workshop 1 - Data Mining for Business Applications (DMBA) 
Workshop 2 - Second Utility-Based Data Mining (SUBDM) 
Workshop 3 - Data Mining Standards, Services and Platforms (DM-SSP06) 
Workshop 4 - WEBKDD: Knowledge Discovery on the Web (WEBKDD) 
Workshop 5 - Link Analysis: Dynamics and Statics of Large Networks (LinkKDD) 
Workshop 6 - MDM/KDD2006: The Seventh International Workshop on Multimedia Data Mining (SIWMDM) 
Workshop 7 - 6TH Workshop on Data Mining in Bioinformatics (BIOKDD06) 
Workshop 8 - 4th Workshop on Temporal Data Mining (4WTDM) 
 
本人的論文是安排在 8/20 之 MDM/KDD2006: The Seventh International Workshop on Multimedia Data 
Mining (SIWMDM)之議程中發表，主題為” A Novel Video Annotation Method by Integrating Visual 
Features and Frequent Patterns”，獲得與會專家學者熱烈之迴響，也得到許多寶貴的建議，將可
使本研究做更深入之拓展。 
 
 經由參加此次的國際會議，由於有機會與國外知名學者接觸討論、並交換對未來研究之看
法，對個人在未來之研究有很大之助益，不近更增廣了個人的視野，對於研究之深度及廣度更
有許多進步。另外，也與國外知名學者建立交流管道，未來將有合作之空間。這是此次與會最
大的收穫。 
 
 攜回之資料有一、會議論文集，二、知識發掘與資料探勘相關國際會議及學會活動資訊，
將可供國內學者參考。 
 4
attempted in annotating a video but in vain since the 
utilized association rules may be too specialized and do 
not fit for a wide range of videos. That is to say, 
annotations by using only the specialized association rules 
may lead to high errors.  
In this paper, we propose a novel method for video 
annotation by integrating visual features and frequent 
patterns existing in the video. Basically, our proposed 
method consists of two main phases: 1) construction of 
three kinds of prediction models, namely association, 
sequential and statistical models from annotated videos, 
and 2) fusion of these models for annotating unknown 
videos automatically. The main advantages of the 
proposed method lie in that both of visual features and 
semantic patterns are considered simultaneously through 
fusion approach so as to enhance the accuracy of 
annotation. The experimental results show that our 
approach can effectively enhance the annotation accuracy 
in terms of precision and recall. 
 
 
Figure 1. An image versus a video sequence. 
 
The remaining of this paper is organized as follows. A 
review of previous work is briefly described in section 2. 
In section 3, we describe the details of our proposed 
method for annotating videos. Empirical evaluations of 
the proposed method are expressed in section 4. Finally, 
conclusions and future work are stated in section 5. 
 
2. Previous Work 
 
In general, traditional multimedia retrieval method has 
been popular among multiple types of multimedia data for 
a long time. The major purpose of multimedia data 
retrieval is to hunt the related multimedia data effectively 
and efficiently. Such multimedia retrieval methods as 
CBIR [4][6][8][10][11][14] tend to represent a 
key-frame/image by a set of visual features and provide 
similarity-based retrieval further. Unfortunately, it still 
cannot satisfy the requirements from users until now. 
Consequently, more and more researchers are devoted to 
the image classification/annotation to get the higher-level 
retrieval results nowadays. For image 
classification/annotation, existing well-known methods, 
such as decision tree, HMM, KNN, association mining, 
SVM and etc [12][13], perhaps have made good efforts 
yet. However, the annotation of a video is more difficult 
than one of an image since a video contains more kinds of 
scenes and the scene can be viewed as a story out of 
human sense. Hence, the solution to endow a video shot 
with some semantics is, actually, not easy to get unlike 
image classification/annotation.  
One solution to deal with such annotation problems is 
the computation of the probabilities between visual 
features and candidate keywords, namely 
feature/statistics-based method. The CRM (Continuous 
Relevance Model) method was developed by Lavrenko et 
al. [5][7] for annotating a video on the basis of statistics. 
They segment each sequential key-frame into several 
rectangle regions and then extracted the related visual 
features from these segmented regions. The annotation of 
each image is yielded soon after calculating the related 
probabilities with Gaussian Mixture Function.  By 
exploiting the temporal continuity of video sequences and 
assuming Markovian property between image frames, 
DBNs (Dynamic Bayesian Networks) proposed by Luo et 
al. [9] maps low-level features to high-level concepts. In 
addition to visual features, some researches combine 
speech, visual and textual features extracted from videos 
to facilitate the video annotation. On the basis of 
combining visual, textual and speech features, Adams et 
al. [1] proposed a method integrating Gaussian Mixtures 
Model and Hidden Markov Model to discover the 
concepts of videos. The motivation behind this-like 
method is that the probabilities of keywords frequently 
occurring in the same visual features will be increased. In 
fact, a sequence of images/key-frames consists of some 
relationships. These relationships cannot be found just by 
the comparisons between individual images since images 
with the same feature may be shared by different 
semantics along different videos.  
Another solution is to discover the associated concepts 
hidden in the sequential images, namely rule-based 
method. More consideration than association rules, 
[15][16][17] took account of temporal continuity and used 
event detection to index and explore sequential 
association rules in the sequential images. Based on these 
sequential association rules, the annotations can be 
generated accordingly. However, the effort of sequential 
association rules is limited in the range of video types. 
Once the rule set is too small, we may not get sufficient 
matching rules for accomplishing the video annotation.  
 
3. Proposed Method 
 
To overcome the problems pointed out in previous 
sections, our research goal is to take advantage of visual 
features and frequent patterns as a hybrid solution for the 
annotations of video shots. To reach this goal, as 
illustrated in Figure 2, the whole procedure contains the 
following two phases that involve two types of prediction 
models, called rule-based model including Modelseq and 
Modelasso, and statistics-based model including 
ModelCRM. 
I. Training phase: This phase concerns the construction of 
three kinds of prediction models for automatic 
annotations. The annotated videos are preprocessed and 
 6
is a set of all referred keywords {w1, w2, …., wm}. For 
example as shown in Figure 5, assume that the set of all 
candidate keywords used to annotate shots in the training 
phase are {k1, k2, k3, k4, k5, k6}. An annotated shot with 
the keywords {k1, k3, k4} are segmented into 3*2=6 
regions and the referred visual features are extracted from 
each region. Then [l1, 1]=1, [l1, 3]=1, [l1, 4]=1 and the 
others of this tuple are 0. That is to say, the relevant tuple 
of keyframe-matching matrix with respect to the idea of 
bit-map index is {1, 0, 1, 1, 0, 0}. At last, the matching 
matrix and the visual features associated with each region 
are stored as the prediction set in the database. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4. The concept of CRM. 
 
 
Figure 5. Example for building ModelCRM. 
 
3.2.2. Rule-Based Annotation Model: Modelasso and 
Modelseq. This type of models is based on the discovery 
of the relationships or co-occurrences among the shots of 
a scene. The property we want to obtain is such a 
relationship as “if a shot is annotated with ‘sky’ and 
‘water_body’, the next shot is very likely to contain 
keywords of ‘sky’, ‘water_body’ and ‘boat’“. Before 
constructing ModelCRM and Modelasso, we have to 
construct a scene-transaction table that can be used to 
discover the association rules. To construct the 
scene-transaction table, encoding every shot segmented 
from every scene is necessary. In this work, we adopt 
k-means method to cluster the shots by the visual features. 
Hence, the similar shots in a cluster are endowed with the 
same symbol. As shown in Figure 6, a scene can be 
considered as a transaction and a transaction contains 
several shots viewed as items. Accordingly, a 
scene-transaction table contains {A, B}, {A, A, D}, {C}. 
Since the unannotated shots within a scene are 
sequentially annotated by previous shots in our method, 
the rule has to be limited in one item in right-hand 
descendant. To suit our method, notice that the association 
rule whether with temporal continuity or not can be 
defined as following. 
Definition 1. Given an annotated scene that is composed 
of n encoded shots S={s1, s2, …., sn} and the nth shot is 
annotated with m keywords Wn={w1, w2, …., wm}. The 
form of an association rule occurring in S is: 
…. 
…. W1 W2 Wn 
R1 R2 R6 
J P(J) 
P(w|J) 
P(r|J) 
 (s1, s2, ….si ) → sj
where 1≦i≦n, 1≦j≦n and i≠j. 
 
 
Figure 6. The encoding process. 
 
Construction of Modelseq. To carry out the paradigm of 
sequential association mining in our method, Modelseq is 
constructed in two steps: 
I. Generation of sequential association rules: From the 
scene-transaction table, we use an association mining 
method similar to Apriori [2] to discover the frequent 
itemsets that sufficiently exceed the minimum support, 
as shown in Figure 7 and 8. These generated frequent 
itemsets can be viewed as association rules directly 
since temporal continuities are inherent in them. For 
example, the sequential association rule (A→B) is 
derived from frequent sequential itemset {A, B}. 
 
Input: A scene-transaction table D and minimum support 
minsup 
Output: Frequent itemsets F 
Algorithm: Gen_FreItems 
1. F1 = scan for each item and get the frequent 1-itemsets 
2. k = 2 
3. while Fk-1 ≠ φ do 
4.   for ∀ p, q ∈ Fk-1 do 
5.     if p1 = q1 & p2 = q2 & … & pk-1 = qk-1 then 
6.       r = concatenate { p1, p2, … pk-1, pk, qk} 
7.       Ck = r ∪ Ck 
8.     end if 
9.   end for 
10.  Fk = {X | sup(X, D) ≥ minsup, X ∈ Ck} 
11.  k = k + 1 
12. end while 
Figure 7. Algorithm for generating sequential association 
rules. 
 
II. Construction of rule-matching matrix: From the 
scene-transaction table, each generated frequent 
itemset is sought for its associated keywords and the 
 8
containing item A tend to contain item B. Without 
considering the sequence between shots, it is different 
from the construction of scene-transaction table of 
Modelasso. The major difference is that we have to prune 
the duplicate items in a tuple, as shown in Figure 12.  
 
 
Figure 12. An example of scene-transaction table of Modelasso. 
 
 
Figure 13. The frequent itemsets of Modelasso. 
 
 
Figure 14. The association rules of Modelasso. 
 
Table 2. The accumulated rule-matching matrix of Modelasso. 
 
∏ ∏
∈ ∈
=
w r
JPJPJrwP
ω γ
γω )|()|()|,(
N
N
N
N
JP
J
J ωω λλω )1()|( , −+=
∑
Τ∈
=
J
JrwPJPrwP )|,()(),(
)(/),()|( rPrwPrwP =
 
Continuously, the generation of the frequent itemsets and the 
rule-matching matrix in Modelasso are the same as those in 
Modelseq. For example, given the minimum support is 1/2 and 
the minimum confidence is 2/3, Figure 13 shows that the 
generated frequent itemsets and Figure 14 shows the generated 
association rules. Rule (A→B) appears in the 1st tuple and 2nd 
tuple as shown in Figure 12. Its relevant position sets are 
{(position3→position2), (position3→position6)} and 
{(position1→position2)}. In the 1st set, item B contains 
keywords {k3, k4} in position2 and keywords {k3, k6} in 
position6 of the 1st tuple in the scene-transaction table. In the 2nd 
set, item B contains {k5, k3} in position2 of the 2nd tuple in the 
scene-transaction table. Therefore, the frequency-vector of 
(A→B) is {0, 0, 3, 1, 1, 1, 0, 0}. As shown in table 2, the others 
can be deduced as the same as the example.  
 
3.3. Prediction Phase 
 
In this subsection, we describe how to make the 
annotation of an unknown video by employing ModelCRM, 
Modelseq and Modelasso in the predicting phase. As 
mentioned in the preceding subsection, three matching 
matrixes are derived from three models that can represent 
the relationships between key-frames and keywords and 
ones between rules and keywords. These derived matrixes 
provide a good support to annotate unknown shots. As 
depicted in Figure 15, the procedure starts with the input 
of a sequence of shots segmented from an unannotated 
video. For statistics-based model, the sequential shots are 
annotated one by one. For rule-based model, the 
sequential shots that have to be further formed a scene are 
annotated one by one, too. Indeed, each prediction model 
yields different predicting probability list and makes 
different effort for annotating shots. Since these models 
perhaps bring out wide variations, the generated 
probabilities have to be normalized while integrating 
three prediction models. 
 
Figure 15. The flow diagram of predicting phase. 
 
3.3.1. Prediction by ModelCRM. Since this model is 
constructed based on the CRM method [5][7], the 
prediction is done by making use of the following 
functions, 
 
(1) 
 
(2) 
 
(3) 
 
(4) 
 
 10
all kinds of video just by finite rules. Hence, the primary 
aim of this design is to avoid the problem of missing rules 
in rule-based models. In other words, by employing 
ModelCRM, we can annotate any shot with at least one 
keyword whether joining with the rule-based models or 
not. Anyway, the result of each prediction model is on the 
basis of each associated probability. For instance, given 
the cardinality of desired keywords that is 3, table 5 
shows the resulting example leaded from different 
combinations of three prediction models. Because of 
various concentrations, some resulting keywords are the 
same and some ones are different. It represents the critical 
issue that we want to show. 
 
4. Empirical Evaluation 
 
In this paper, three prediction models have been 
proposed for annotating videos. The experiments are 
presented in this section for comparison of three 
prediction models.  
 
4.1. Experimental Data 
 
The experimental data is based on the collection of 
TREC Video Retrieval Evaluation (TRECVID) sponsored 
by the National Institute of Standards and Technology 
(NIST). TRECVID is a well-known laboratory-style 
evaluation that attempts to model situations or significant 
component tasks in real life. Among the TREC videos, we 
chose four CNN and four ABC news videos to be our 
experimental data.  
 
Table 6. The characteristics of experimental data. 
Name Size Length Scene# Shot#
19980202_CNN.mpg 412MB 29:40 25 156
19980204_CNN.mpg 389MB 29:15 20 170
19980205_CNN.mpg 405MB 29:52 26 168
19980207_CNN.mpg 397MB 29:18 24 226
19980218_ABC.mpg 395MB 29:09 16 194
19980319_ABC.mpg 382MB 28:10 19 160
19980324_ABC.mpg 384MB 28:19 13 143
19980329_ABC.mpg 394MB 29:04 18 197
 
Table 7. The parameter settings for experiments. 
Parameter Default value 
N*M (size of segment) 3*2 
CL (number of clusters) 30 
Minsup (minimum support) 0.08 
Con (confidence) 0 
TOPK (number of resulting keywords) 5 
 
For keyword setting, we selected 133 basic 
vocabularies regulated by NIST to be our relevant 
keywords from four testing videos since the types of 
vocabularies totaling 1038 in TRECVID are too large. 
Besides, 83 keywords were, finally, preserved after 
filtering the keywords for event and speech type. For 
preprocess operation, shot detection was performed by 
VideoAnnEx Annotation Tool developed by IBM [3] and 
scene detection was accomplished manually. Additionally, 
the unnecessary advertisement scenes were also dropped. 
Table 6 shows the characteristics of experimental data. 
The parameter settings are shown in table 7. 
The evaluation is investigated in terms of two 
measures, namely precision and recall. Precision 
represents the ratio between the cardinality of correctly 
annotated keywords and the cardinality of resulting 
keywords in a shot. Recall explains anther ratio between 
the cardinality of correctly annotated keywords and the 
cardinality of relevant keywords in a shot.  
Precision = 
result
correct
 * 100% 
Recall = 
relevant
correct
 * 100% 
For example, assume that the relevant keyword set is 
{car, road, sky} and the resulting keyword set is {sea, car, 
person, road, outdoors}. Precision is 2 / 5 * 100% = 40% 
and recall is 2 / 3 * 100% = 67%. 
 
4.2. Experimental Results 
 
In our experiments, we adopted the 8-fold approach to 
carry out the evaluations. That is, seven videos took turns 
as a testing video and others were taken as training videos. 
The evaluations were conducted in two aspects: 1) 
comparisons of the three kinds of models namely 
ModelCRM, Modelseq and Modelasso; 2) comparisons 
between ModelCRM and different combinations of 
ModelCRM and rule-based models. The fusion models are 
defined as follows: 
 
 Fusion 1 = ModelCRM + Modelseq
Fusion 2 = ModelCRM + Modelasso
Fusion 3 = ModelCRM + Modelseq + Modelasso
 
 
In the following, we describe some representative 
experimental results on the CNN videos. Figure 17 shows 
that both of rule-based models, Modelseq and Modelasso, 
outperform ModelCRM in terms of the precision, and 
Modelasso outperforms Modelseq slightly better than in 
average. This indicates that the rule-based models can 
effectively capture the intra-relations or inter-relations 
among the shots as we expect. In contrast, annotations by 
using only visual features encounter higher difficulty in 
dealing with high variations of visual features in the 
videos. Figure 18 and Figure 19 show the precision and 
recall results for ModelCRM, Fusion 1, Fusion 2 and Fusion 
3, respectively. Some interesting observations were made 
here. First, the results show that any fusion model can 
 12
