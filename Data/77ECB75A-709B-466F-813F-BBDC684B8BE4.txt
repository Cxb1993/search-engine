By the way, the sequence can be automatically labeled with certain GO terms after it has been
annotated with a SWISS-PROT keyword. Some researches (Poulito et al., 2001; Xie et al.,
2002) attempted to find function relations between GO terms and genes from the scientific
literature. Perez et al. (2004) proposed a method to establish the mappings between terms
from the MEDLINE database of scientific literature and GO. Some researchers (Ray and
Craven, 2004; Verspoor et al., 2004) attempted to expand the GO terms by finding the related
terms, and these approaches slightly improved the recall or coverage rate.
In this paper, we introduce a supporting evidence in a literature while GO terms are
proposed. That will help the annotators speed up their work without reading full texts, and
provide them some GO candidate terms accompanying with the evidences. GeneRIF in the
LocusLink database (http://www.ncbi.nlm.nih.gov/LocusLink/) provides a simple mechanism
to allow scientists to add the functional annotation of genes. We treat a GeneRIF of a gene
product as a piece of supporting evidence, and try to find the suitable GO terms.
Intuitively, we can compute the similarity of GeneRIF and GO terms based on the number
of matching words. However, they are too short to reflect complete concepts even though
including GO definitions. Here, we introduce the information retrieval technology to deal
with this issue. The postulation is: if the document sets retrieved by a GeneRIF and a GO
concept are similar, they are considered as relevant to each other, and a link can be established
in between. For example, a literature with PMID 11798066 was referenced by a GeneRIF of
“Dag1”, and GO terms “protein binding”, “morphogenesis of an epithelial sheet”and
“dystroglycan complex”together. Meanwhile, “protein binding”comes from the GO
category“molecular function”,“morphogenesis of an epithelial sheet”belongs to the category
“biological process” and “dystroglycan complex” locates in the category “cellular
component”. It shows that the postulation, to relate GeneRIFs and GO terms with all GO
categories, may be reasonable.
The rest of this paper is organized as follows. In Section 2, we present the flow of our
annotating procedure. The basic idea and the experimental methods in this study are
introduced in Section 3. Section 4 shows the results and makes some discussions. Finally,
Section 5 concludes the remarks and suggests the direction of future research.
2 ANNOTATION FLOW
Generally, a gene name may have several aliases, and different functions may be discovered
in different literatures. For example, “Gli3”has aliases “Xt”, “Bph”,“Pdn” and“add”.
Meanwhile, there are nineteen functions discovered from several literatures (e.g., PMID
12435627, 12435361, 12435629, etc.) for the gene“Gli3”. Users interested in the functions
of a given gene can consult the existing resources such as GeneRIF in the LocusLink database,
or read those newly published literatures that are not yet referenced and annotated by the
database curators. Relevance detection will help database-curators or ontology-annotators
maintain the existing resources and keep them up to date. An annotation system may consist
of two major stages–say, (1) the extraction of molecular function of a gene from a literature,
and (2) the annotation of this function with a term in a controlled vocabulary (ontology). At
the first stage, we extract the evidence text from the literature that will support for the GO
annotation at the second stage. In this paper, MEDLINE abstracts, GeneRIF and GO are
served as experimental objects.
Figure 1 shows an example illustrating our idea. The left part is a MEDLINE abstract
with the function description highlighted. The middle part is the corresponding GeneRIF,
which is extracted from the partial sentence of the abstract. The right part lists the GO
annotations that are annotated by referencing the same abstract. The matching words
between MEDLINE and GeneRIF, or between GO and GeneRIF are in bold. The issues at
GeneRIFs. The manual checking procedure is to guarantee that the GeneRIFs and GO terms
not only come from the same documents, but also have meaningful relationships. Figure 2
shows an example of gene“Dock2”. The symbol * denotes the corresponding GO terms are
filtered out from the possible answer keys. In this way, total 550 pairs of GeneRIFs and GO
terms were obtained for 335 distinct genes, and used for testing.
3.2 Similarity Measure
A GeneRIF is often a sentence or a small passage that describes the function of a particular
gene product. A GO term is composed of a few words and often comes along with its
definition. In the following sections, “a GO term plus its definition”is denoted as “a GO
concept”. Intuitively, we can compute the similarity between a GeneRIF and all the GO
concepts by keyword matching to set up the linkage. However, the challenging issue is that
a GeneRIF and a GO concept are both too short to provide sufficient information, which
makes this approach less promising. In the example shown in Figure 1, only one common
stemmed word, i.e., ”development”, appears in both the GeneRIF and one GO concept. To
deal with this problem, we introduce the idea of relevance detection in TREC Novelty Track
(Harman, 2002). The novelty track aimed at detecting relevance and novelty from a set of
sentences, and that requires the computation of similarity between two sentences. Tagging
molecular functions with GO terms could be modeled as a relevance detection problem. An
information retrieval (IR) approach with reference corpus proposed by Chen, Tsai and Hsu
(2004) was adopted to resolve this problem in this study.
With this IR approach, the similarity or relevance between two sentences is obtained as
follows. Each sentence is treated as a query to an IR system, and the top 200 relevant
documents along with a weight for each document are retrieved from the reference corpus.
The sentence is then represented by a list of weighted documents. Finally, the Cosine
function is used to measure the similarity between the two sentences.
Fig. 2. GeneRIFs and GO terms extracted from gene“Dock2”(LocusID: 94176).
GeneRIF: 12871644 | DOCK2
regulates T cell responsiveness
through remodeling of actin
cytoskeleton via Rac activation.
GeneRIFs GO terms
GO: 0030675: Rac GTPase activator activity
GO: 0042110: T-cell activation
GO:0042098: T-cell proliferation
GO: 0046631: alpha-beta T-cell activation
GO: 0046633: alpha-beta T-cell proliferation
*GO: 0001768: establishment of T-cell polarity
*GO: 0001771: formation of immunological
synapse
*GO: 0001766: lipid raft polarization
GO: 0045060: negative thymic T-cell selection
GO: 0035022: positive regulation of Rac protein
signal transduction
GO: 0045059: positive thymic T-cell selection
GeneRIF: 11518968 |
haematopoietic
cell-specific CDM protein
family member is indispensable
for lymphocyte chemotaxis
PMID:
GO:0006935: chemotaxis
*GO:0007010: cytoskeleton organization and
biogenesis
PMID:
4 RESULTS AND DISCUSSIONS
GO terms have a hierarchical structure which is appropriate for a flexible annotation process
(Ashburner et al., 2000). Perez et al. (2004) proposed a metric to evaluate the annotation
performance under the GO hierarchy. They analyzed the distance in the GO hierarchy
between the GO terms predicted and those of the answer set. The matched distance is n if
the number of the shortest edges between the predicted term and the answer term is n. For
example, if the predicted term is “minus-end-directed microtubule motor activity”(which
depends on “microtubule motor activity”) and the correct term is “microfilament motor
activity”(which depends on“motor activity”), then there is a match with a distance of 3 in the
hierarchy. We adopted the same evaluation metric as Perez et al.’s (2004).
The number of predicted GO terms is also considered in this study. If we propose only
one GO term, the curators have only one candidate to choose. However, it is not always the
case. Figure 2 shows that GeneRIF (PMID: 12871644) for “Dock2”can be related to eight
non-filtered GO terms. Thus, the GeneRIF can obviously warrant the assignment of more
than one GO term. However, how many candidates we should propose is worthy of
investigation. At the primary experiment, we proposed the predicted GO terms from top 1 to
top 5. Figures 4 and 5 show the recall rates and the precision rates under different settings,
respectively. Different distances in the GO hierarchy between the predicted terms and the
answer terms are evaluated.
The experimental results show the recall rate is better than precision rate if at least two GO
terms are proposed. It is reasonable because there is an average of 1.41 GO terms for each
GeneRIF in our answer set. It is helpful for human curators because they can collect more
possible answers. These two figures demonstrate the same performance trend that
“distance”and “recall/precision”are positively correlated, and they reach the stable situation
at distance 12. The recall rates show Top5 > Top4 > Top3 > Top2 > Top1 while the precision
rates show Top1 > Top2 > Top3 > Top4 > Top5. That is rational because proposing more
Propose Top N
0
0.2
0.4
0.6
0.8
1
1.2
Distance
R
ec
al
l
Top1 0.082 0.133 0.196 0.253 0.289 0.316 0.36 0.382 0.402 0.42 0.442 0.46 0.469 0.469 0.469 0.471 0.471 0.471 0.471 0.471 0.471 1
Top2 0.122 0.204 0.305 0.358 0.404 0.451 0.496 0.518 0.538 0.56 0.585 0.602 0.613 0.615 0.616 0.618 0.618 0.618 0.618 0.618 0.618 1
Top3 0.151 0.235 0.345 0.407 0.465 0.516 0.564 0.589 0.615 0.642 0.671 0.689 0.702 0.704 0.704 0.705 0.705 0.705 0.705 0.705 0.705 1
Top4 0.175 0.262 0.378 0.438 0.5 0.549 0.598 0.631 0.651 0.678 0.705 0.722 0.729 0.731 0.731 0.733 0.733 0.733 0.733 0.733 0.733 1
Top5 0.198 0.284 0.405 0.473 0.529 0.584 0.635 0.682 0.709 0.735 0.756 0.775 0.782 0.784 0.784 0.785 0.785 0.785 0.785 0.785 0.785 1
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 100
Fig. 4. Recall rates in different numbers of predicted GO terms.
were 96-99% coverage and 65-80% reproducibility. Because the experiments were made on
different test sets and the evaluation criteria were different, it had not enough evidences to
conclude which methods performed better. Here, we made another experiment without
reference corpus for comparison, i.e., direct word matching. Considering the example
shown in Figure 1, only the word “development”among correct GO concepts was matched
under this experiment, and it was possible that words of GeneRIF matched with other
incorrect GO concepts. We define increase ratio as follows to measure the performance
difference between methods with/without reference corpus.
Increase ratio =
matchingexactwitheperformanc
matchingexactwitheperformanc-corpusreferencewitheperformanc
The results of the methods with/without reference corpus are shown in Figures 6 and 7,
which illustrated increase ratios of recall rates and precision rates, respectively. We
observed that the increase ratios of recall and precision rates are similar. As illustrated in
Figures 6 and 7, the increase ratios are between 73.02% and 81.08% at distance 0. It
indicates that reference corpus is quite useful, especially for perfect matches. After distance
8, the performances of these two approaches are nearly of no difference. It shows that the
information retrieval approach with reference corpus has better performance within fewer
distances. In the statistics of GO branching, there are maximum 522 branches and minimum
1 branch. The average is 4.17 with standard deviation 11.33. Furthermore, the most
frequent depth is 9. In such a case, GO curators will not want to examine the predicted
terms with large distances because long distances of the GO hierarchy means that the trace
scope to determine the correct GO terms by curators is also wide.
Reference Corpus vs Direct Word Matching
-0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Distance
R
ec
al
lI
nc
re
as
e
ra
ti
o
Top1 0.8 0.489796 0.35 0.230088 0.12766 0.0875 0.106145 0.05 0.03271 0.008734 0 0 0.003891 -0.00769
Top2 0.810811 0.365854 0.333333 0.23125 0.14433 0.087719 0.066406 0.036364 -0.00671 -0.01597 -0.00923 0.00303 0.008982 0.002967
Top3 0.804348 0.316327 0.266667 0.166667 0.098712 0.067669 0.054422 0.015674 -0.02029 -0.01944 -0.00539 0.002646 0.013123 0.005195
Top4 0.777778 0.297297 0.283951 0.169903 0.1 0.052265 0.012308 0.01462 -0.03504 -0.021 -0.01272 -0.00998 -0.01474 -0.01471
Top5 0.730159 0.248 0.225275 0.150442 0.062044 0.025559 0 0.03022 -0.00256 0.004975 -0.0024 0.004717 0 0
0 1 2 3 4 5 6 7 8 9 10 11 12 13
Fig. 6. Increase ratios of recall rates with/without reference corpus at different distances.
M.J., Michoud, K., O’Donovan, C., Phan, I., Pilbout, S. and Schneider, M. (2003) The
SWISS-PROT Protein Knowledgebase and its Supplement TrEMBL in 2003. Nucleic
Acids Research, 31, 365-370.
Camon, E., Magrane, M., Barrell, D., Binns, D., Fleischmann, W., Kersey, P., Mulder, N.,
Oinn, T., Maslen, J., Cox, A. and Apweiler, R. (2003) The Gene Ontology Annotation
(GOA) project: implementation of GO in SWISS-PROT, TrEMBL, and InterPro.
Genome Research, 13, 1-11.
Chen, H.H., Tsai, M.F. and Hsu, M.H. (2004) Identification of Relevant and Novel Sentences,
Proceedings of 26th European Conference on Information Retrieval, Lecture Notes in
Computer Science, 2997, Springer-Verlag, 85-98.
deBruin, B. and Martin, J. (2003) Finding Gene Function Using LitMiner. The Twelfth Text
Retrieval Conference: TREC 2003, Gaithersburg, MD. NIST.
Gene Ontology Consortium. http://www.geneontology.org/GO.consortiumlist.html
Harman, D. (2002) Overview of the TREC 2002 Novelty Track, Proc. TREC 2002.
Hersh, W. and Bhupatiraju, R.T. (2003) TREC Genomics Track Overview, Proc. TREC 2003.
Jelier, R., Schuemie, M., et al. (2003) Searching for GeneRIFs: Concept-based Query
Expansion and Bayes Classification. The Twelfth Text Retrieval Conference: TREC 2003,
Gaithersburg, MD. NIST.
Kayaap, M., Aronson, A., et al. (2003) Methods for Accurate Retrieval of MEDLINE
Citations in Functional Genomics. The Twelfth Text Retrieval Conference: TREC 2003,
Gaithersburg, MD. NIST.
Lee, C., Hou, W.J. and Chen, H.H. (2004) Support Vector Machine Approach to Extracting
Gene References into Function from Biological Documents, Proc. COLING 2004: Joint
Workshop on Natural Language Processing in Biomedicine and its Applications.
LocusLink database. http://www.ncbi.nlm.nih.gov/LocusLink/
Mitchell, J., Aronson, A., et al. (2003) Gene Indexing: Characterization and Analysis of
NLM’s GeneRIFs. Proceedings of the AMIA 2003 Annual Symposium, Washiongton, DC.
Hanley & Belfus, 460-464.
Perez, A.J., Perez-Iratxeta, C., Bork, P., Thode, G. and Andrade, M.A. (2004) Gene
Annotation from Scientific Literature Using Mappings between Keyword Systems.
Bioinformatics, 20(13), 2084-2091.
Pouliot, Y., Gao, J., Su, Q.J., Liu, G.G. and Ling, X.B. (2001) DIAN: a Novel Algorithm for
Genome Ontological Classification. Genome Research, 11 1766-1779.
Ray, S. and Craven, M. (2004) Learning Statistical Models for Annotating Proteins with
Function Information using Biomedical Text, BioCreative Workshop Handouts.
Robertson, S.E., Walker, S. and Beaulieu, M. (1998) Okapi at TREC-7: automatic ad hoc,
filtering, VLC and interactive, Proc. Seventh Text REtrieval Conference, 253-264.
TREC Genomics Track. http://medir.ohsu.edu/~genomics/
Verspoor, K., Cohn, J., Joslyn, C., Mniszewski, S., Rechtsteiner, A., Rocha, L.M., and Simas,
T. (2004) Protein Annotation as Term Categorization in the Gene Ontology, BioCreative
Workshop Handouts.
Xie, H., Wasserman, A., Levine, Z., Novik, A., Grebinskiy, V., Shoshan, A. and Mintz, L.
(2002) Large-scale Protein Annotation through Gene ontology. Genome Research, 12
785-794.
