行政院國家科學委員會專題研究計畫結案報告【第一年】 
聽障生語者特徵及臉部資訊 之電腦輔助發音學習與評量系統研製 
計畫編號:NSC96-2221-E-167-026-MY3  
執行期限:96年8月1日至97年7月31日 
主持人：賴雲龍  國立勤益科技大學  電子系 
計畫參與人員： 楊惠貞 1 林明輝 2 賴士偉 3 
1國立勤益科技大學 資訊管理系 
2國立勤益科技大學 電子系(所) 
3私立東海大學 資工系 
 
一、摘要 
 
聽障者喪失了大部分的聽力，特別是中度和
重度聽覺損傷對於聲音察覺能力是不足的。因為
無法聽到外界聲音，大都無法學習講話，長大成
人後，與人的溝通只能用手語，使得在社會上或
職場上遭遇極大的不便。一般，他們除了在聽障
學校接受一些聽覺訓練幫助增加聽力外，並沒有
類似的聽覺訓練聽覺環境，因此本研究主要研究
以嵌入式系統(Embedded System)建立一套輔助聽
障者的攜帶型聽覺訓練工具「中文訓練與學習系
統」。「中文訓練與學習系統」適用對象為啟聰小
學聽覺障礙兒童，本系統結合聽覺訓練系統和注
音拼音學習系統。以中文語音合成 (Speech 
Synthesis)技術為基礎，克服硬體資源之限制以實
現一個運算量低、記憶體需求量低、合成語音佳
且成本低的嵌入裝置之攜帶型聽覺訓練工具，讓
聽障者能夠隨時隨地進行中文聽覺訓練及注音拼
音學習。 
 
關鍵詞：攜帶式聽力訓練系統、嵌入式系統、 
語音合成、國語注音 
Abstract 
 
Hearing-impaired loss most of hearing ability, 
especially for the middle and heavy hearing 
impaired students who have not enough ability to 
detect the sound, therefore they can not speak. They 
use sign language to communicate with other people. 
This causes a great inconvenient in the social life 
and working environment. Generally speaking, 
hearing-impaired students only accept some 
language training classes in the school. They do not 
have enough learning tools to practice their 
pronunciation. Therefore, the main purpose of this 
study uses an embedded system to implement a 
portable pronunciation training system for 
hearing-impaired people to learn Mandarin phonetic. 
This system implemented on an embedded portable 
training system with low operation, low memory, 
better speech synthesis and low cost for 
hearing-impaired people to learn Mandarin phonetic 
without time and place limitation. 
 
Keywords: portable auditory training system; 
embedded system; speech synthesis;  
Mandarin phonetic 
 
二、簡介 
 
聽障及語言障礙者，因為無法聽到外界聲
音，所以大都無法學習講話，聽障是所有知覺殘
障項目中最遭的一種，視障者他們和人接觸最大
的不便是事物的阻礙，而聽障者和人接觸最大的
不便是與人的溝通(Darrow, 1989)。溝通是我們社
會和認知的基礎，沒了溝通就完全被社會隔離
了。因此在特殊教育的領域中，中重度聽覺障礙
者不論學習語言或者是發音都相當吃力，且要理
解箇中的含義要比一般人花費更多時間。 
聽覺障礙者平時最常使用的語言溝通法有三
種：手語法、口語法、綜合溝通法三種，其中手
語法是聽障者平常溝通時最愛用、不學而會的自
然母語。一般聽人與聽障者之間最大的障礙就是
溝通：聽障者聽不到聽人說的話及看不懂聽人寫
的中文；聽人卻不懂聽障者的手語及其所用的文
字結構。所以聽障者應積極學習第二語言”中文”。 
嵌入裝置之攜帶型聽覺訓練工具，包含中文
鍵盤輸入、LCD 顯示中文字以及耳機，讓聽障者
能夠隨時隨地同時進行中文聽覺訓練及注音拼音
學習。聽障者進行聽覺訓練不再受限於特定的環
境，不只隨時隨地可進行聽障者的第二語言”中
文”的聽覺訓練，且也同時進行中文的注音拼音學
習，達到絕佳的效果。 
 
三、中文語音之特性 
 
國 語 的 一 個 詞 素 通 常 就 是 一 個 音 節
(Syllable)，一般用一個國字來表示。國字分為聲
母(Initial)、韻母(Final)、聲調(Tone)三部分。聲母
於語音學上稱為子音(Consonant)，而韻母稱為母
音(Vowel)，皆稱為次音節(Sub-Syllable)。國語音
節具有聲調的特性，共有陰、陽、上、去四種聲
調和一個輕聲調，而陰、陽、上、去即為平常使
用的一聲、二聲、三聲、和四聲，其基頻軌跡（pitch 
contour）的特性如圖 1.[1]。 
77E58/E5XX 處理得到中文合成語音經過數位類
比轉換器經由耳機播放，讓聽障者同時進行中文
的聽覺訓練和注音拼音學習。 
 
(1)  WINeZ 77E58/E5XX 發展系統 
 
 WINeZ 77E58/E5XX 發 展 系 統 是 新 華
(Microtime)新一代的 Tubro-51 微電腦發展系統，
cost、performance、utilization 之平衡點是此系統
設計考量的重點。 
新一代 WINeZ 77E58/E5XX 提供以下主要功能： 
(1) 目前提供 Winbond&Dallas Tubro-51 Series 
之模擬 
(2) 模擬速度最高可達 40MHz 
(3) 工作電壓為 5V、3.3V 或 3V（外加） 
(4) 256K byte 程式及資料模擬記憶體提供 
(5) 多功能即時硬體斷點 
(6) 硬體即時追蹤篩選功能 
(7) 邏輯分析功能 
(8) 採用標準的 PC 並列傳輸介面(Printer Port 
Interface SPP、EPP Mode) 
WINeZ 77E58/E5XX 包含下列幾種硬體組件： 
(1)  WINeZ 77E58/E5XX 主機： 
包含一控制模版，及電源模版，所有的除錯
硬體資源，模擬電路及通信介面均在此模版
上完成。 
(2)  ICE Cable: 
ICE 模擬信號經此 Adapter 直接語帶測系統
上該 CPU 的 IC 座連接。模擬 CPU 置於 ICE 
Cable 之 CPU POD(W2K77E532 CPU POD)
上，做最真實模擬。 
(3)  Trace Probe 模組： 
此模組提供四個輸入端子，可做即時追蹤記
錄信號源及硬體斷點觸發條件設定之用。 
(4)  Remote Control 模組： 
此模組提供四個輸出入端子，可做多模擬系
統同步，事件觸發輸出等用途。 
WINeZ 77E58/E5XX 擁有可在 PC 上執行的
Windows 版 ICE 驅動軟體。 
 
 
圖 3. WINeZ 77E58/E5XX 的基本結構圖 
 
(2)  單晶片中文系統 
 
 單晶片中文系統說明： 
(1) 此中文系統採用 16X16，字形均存在 ROM
中。ROM 中儲存 BIG-5 碼的 A140-F9F8，共
13942 個中文字及特殊符號。每個字形佔 32 
bytes ， 共 用 記 憶 體 13942 X 
32=449144=6CEC0H，約佔 435K。故選用
512K 的 ROM 27C4001 來燒錄這些字形及
符號。 
(2) ROM 中的中文字均依序儲存，不因 BIG-5
碼的不連續而有間斷。每個中文字或符號佔
32 bytes。 
以繪圖型 LCD 顯示中文字的方法，將每個中文均
視為一個圖形，以每個點的亮或不亮來呈現中文
字形。 
 
(3)  數位類比轉換器 
 
使用 Analog devices 公司所生產的 AD5424
晶片[7]將數位訊號轉為類比訊號，並接上耳機插
座以耳機播放輸出。這顆 8bit DAC 具有 Update 
Rate 為 20.4MSPS、Setting Time 為 30ns 以及並列
傳輸等特性。 
 
 
圖 4. 中文系統元件配置圖 
 
六、次音節語音資料庫之建立 
 
 進行語音合成前，必須先建立次音節語音資
料庫以儲存次音節之語音訊號資料，以便即時處
理成合成單元以供合成階段使用。除了建立次音
節語音資料庫外，還必須提供次音節的基週標位
和切割點位置，才能依此資訊進行次音節的切割
和韻律調整成為合成單元，以供合成階段使用。
 
圖 10. 基週尺度函數 
 
圖 11. 各聲調之基頻軌跡 
 
(2)  調整音長 
 
 TD-PSOLA 演算法除了能夠調整音高外，皆
能調整語音長度。少數聲母因無變化轉折點，無
法切割出前部分之語音，故對其調整音長來作為
聲母合成單元。一般音節中聲母所佔的時間長度
大約為 45ms（如圖 12.），故在合成階段針對要合
成的少數聲母以 TD-PSOLA 演算法將其音長縮短
為大約 45ms。 
 
 
圖 12. “發＂（「ㄈㄚ」）音節之波形圖 
 
(3)  合成語音之輸出 
 
 處理完成之中文語音合成訊號，將由AD5424
晶片由數位訊號轉為類比訊號輸出，並透過耳機
播放。 
 
八、結論 
 
本研究目的再開發一個適合在嵌入式系統
上運作之中文輸入顯示與語音合成系統，以實現
一個運算量低、記憶體需求低、音質佳且可實現
低成本嵌入式裝置之攜帶型中文訓練與學習系
統。 
實驗結果顯示合成語音品質有很好的可辨
度，而自然度方面在可接受之範圍內，至於清晰
度也有不錯的結果，並證明以此小型語音資料庫
進行即時語音合成處理輸出是可行之方法。 
系統已完成雛型並交給台中啟聰學校小學生
實際測試，本系統可以讓聽障者了解正確的注音
拼音，在研究中對於注音拼音資料庫的建立，必
須找到許多相同年齡男女學生的語音資料建檔，
才可以發展客製化系統，另外在資料的建立過程
中，本系統實際應用於啟聰學校聽障學生的訓練
中，初步發現在中度聽障但佩帶電子耳輔助聽力
的受測學童初次使用本系統時，拼音測試在二十
次的發音中獲得正確的次數不到十次，且需要老
師從旁校正其發音，但經過多次訓練後其獲得的
辨識率則提昇至十次以上，經過二至三個月每星
期一次訓練並針對注音拼音作練習可以達成十五
次以上的正確率，國小一年級姓名，7 歲歐同學，
中度聽障程度之測試報告如圖 13.，顯示對於聽障
學童的注音發音有正面且加速學會正確發音之效
果。 
本評分系統可以讓聽障者了解自己發音的正
確率有多高，也讓老師了解聽障者的程度，比較
容易從旁協助他們校正發音。此系統讓聽障者可
以自己隨時練習，不需要總是靠老師或家人來教
導。這套系統用於啟聰學校聽障學童的注音教學
訓練，發揮了科技輔具對弱勢者的積極幫助外並
減輕老師在注音教學的負擔。，預期再經過訓練
後可提升聽障學童正確發音的能力並進一步能和
一般人一樣能發音正確的說話，與人溝通無礙。 
 
8
10
13
11
15
13
17 16
0
2
4
6
8
10
12
14
16
18
1 2 3 4 5 6 7 8
20
次
發
音
正
確
次
數
週次
 
圖 13. 國音”爸”字正確發音次數表 
 
 
表 1. 國語聲母表 
發音方式 唇 齒 齒齦 捲舌 齶 軟齶 
爆破音 ㄅ(b) 
ㄆ(p) 
 ㄉ(d) 
ㄊ(t) 
  ㄍ（g） 
鼻音 ㄇ(m)  ㄋ(n)   ㄎ(k) 
摩擦音 ㄈ(f) ㄙ(s)  ㄕ(sh) 
ㄖ(r) 
ㄒ(x) ㄏ(h) 
邊音   ㄌ(l)    
塞擦音  ㄗ(z) 
ㄘ(c) 
 ㄓ(zh) 
ㄔ(ch) 
ㄐ（j） 
ㄑ(q) 
 
 
表 2. 國語韻母表（不包含空韻母） 
發音方式 單元音韻母 複合元音韻母 鼻尾音韻母 
開口 ㄚ(a) ㄛ(o) 
ㄜ(e) ㄦ(er) 
ㄝ(e) 
ㄞ(ai) ㄟ(ei) 
ㄠ(ao) ㄡ(ou) 
ㄢ(an) ㄣ(en) 
ㄤ(ang) ㄥ(eng) 
齊齒 ㄧ(i) ㄧㄚ(ia) ㄧㄝ(ie) 
ㄧㄠ(iao) ㄧㄡ(iu) 
ㄧㄞ(iai) ㄧㄛ(io) 
ㄧㄢ(ian) ㄧㄣ(in) 
ㄧㄤ(iang) ㄧㄥ(ing) 
合口 ㄨ(u) ㄨㄚ(ua) ㄨㄛ(uo) 
ㄨㄞ(uai) ㄨㄟ(uei)
ㄨㄢ(uan) ㄩㄣ(un) 
ㄨ尢(ang) ㄨㄥ(ong) 
撮口 ㄩ(v) ㄩㄝ(ue) ㄩㄢ(uan) ㄩㄣ(un) 
ㄩㄥ(iong) 
 
 
圖 6. 有效語音段範圍 
 
圖 7. 「ㄇ」和「ㄐ」波形上的變化轉折點位置 
 
 
可供推廣之研發成果資料表 
▉ 可申請專利  □ 可技術移轉                                      日期：97 年 5 月 10 日 
國科會補助計畫 
計畫名稱：聽障生語者特徵及臉部資訊 之電腦輔助發音學
習與評量系統研製 
計畫主持人：  賴雲龍 
計畫編號：NSC96-2221-E-167-026-MY3          
學門領域：醫學工程 殘障輔具 
技術/創作名稱 聽障生注音學習系統 
發明人/創作人 賴雲龍 楊惠貞 林明輝 賴士偉 
技術說明 
中文： 
以嵌入式系統(Embedded System)建立一套輔助聽障者的攜帶型聽
覺訓練工具「中文訓練與學習系統」。適用對象為啟聰小學聽覺障礙兒
童，本系統結合聽覺訓練系統和注音拼音學習系統。以中文語音合成
(Speech Synthesis)技術為基礎，克服硬體資源之限制以實現一個運算量
低、記憶體需求量低、合成語音佳且成本低的嵌入裝置之攜帶型聽覺訓
練工具，讓聽障者能夠隨時隨地進行中文聽覺訓練及注音拼音學習。 
 
英文： 
This system implemented on an embedded portable training 
system with low operation, low memory, better speech synthesis and 
low cost for hearing-impaired people to learn Mandarin phonetic 
without time and place limitation. 
 
可利用之產業 
及 
可開發之產品 
聽障教育  幼兒教育 
技術特點 
利用語音合成技術減少語音資料庫容量 
單字之四聲發音為此系統之特點 
推廣及運用的價值 
可利用於聽障教育， 幼兒教育，及外國人士之中文發音訓練設備 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位研
發成果推廣單位（如技術移轉中心） 
附件二 
數、x0 與 x1 分別為兩個間距門檻值。這個方法同時
可以應用於灰階影像與彩色影像，應用在彩色影像的
時候，將 R、G、B 三個維度分別代入公式中演算即
可。 
傳統 Gamma 與自適性 Gamma 的比較，如圖 2。我
們可明顯發覺自適性 Gamma 可以同時提升暗部並降
低亮部，可以達到較佳的校正結果。 
 
 
(a) γ=0.6         (b) γ=0.6    (c) Active Gamma 
圖 2 傳統 Gamma 校正與自適性 Gamma 校正 
 
3.2 Color Space-色彩空間轉換 
膚色是人臉的一個重要特徵，雖然不同種族，不
同年齡，人的膚色有所不同，主要是由於亮度上的差
異造成的。去除亮度後，不同人的膚色分布有聚類性，
因此可以利用膚色聚類性來檢測人臉，為了減小由亮
度到成的誤差。有效地利用膚色特徵在色彩空間的聚
類性，必須合理的選擇一種顏色模型使得亮度信息造
成的影響最小。 
 
3.2.1 正規化 RGB 色彩模型 
由於彩色影像中每個像素的亮度是由其 RGB 三
個分量之和。因此，正規化 RGB 色彩模型能減少亮
度對人臉偵測的影響。正規化 RGB 色彩模型定義如
下[1]: 
 
⎪⎩
⎪⎨
⎧
++=
++=
++=
)/(
)/(
)/(
BGRBb
BGRGg
BGRRr
                    (3) 
其中 1=++ bgr 。 
 
3.2.2 YCbCr 色彩模型 
YCbCr 色彩空間將色彩分離為強度（Y）與彩度
（Cb、Cr），對於顏色具有合理性的解釋，且該色彩
空間為 MPEG 與 JPEG 等壓縮格式所採用，十分適合
數位影像處理所需。且根據研究 Chai et. al. 2000[3]，
各種人種的膚色在 YCbCr 空間上的分佈差異不大。正
規化 YCbCr 色彩模型定義如下[1]: 
⎪⎩
⎪⎨
⎧
−−=
+−−=
++=
B08131.0G41869.0R5.0C
B5.00G33126.0R16875.0C
B114.0G587.0R299.0Y
r
b    (4) 
 
3.3 人臉膚色切割 
正規化 RGB 色彩模型雖然能減小亮度的影響，
但由於同樣受亮度影響的色飽和度還沒分離出來，對
亮度變化仍比較敏感。因此，同時採用上述的色彩模
型作為皮膚顏色的分類標準。經由分析結果，分別得
到了在 RGB 色彩模型的分佈範圍[4]: 
 
⎩⎨
⎧
≤≤
≤≤
363.028.0
465.036.0
g
r
                       (5) 
 
在 YCbCr 色彩模型色彩模型的分佈範圍: 
 
⎪⎩
⎪⎨
⎧
≤≤
≤≤
≤≤
160C120
140C100
220Y20
r
b                         (6) 
 
對輸入影像經過轉換色彩空間後，結合式(5)式(6)兩
個判斷式，產生一個二值圖像。如圖 3所示。 
 
 
圖 3 膚色切割 
 
3.4 唇色切割 
將膚色切割之範圍，接著進行唇色切割運算，
採用 RGB 色彩模型作為嘴唇顏色的分類標準。如圖 4
所示。經由分析後得到嘴唇在 RGB 色彩模型的分佈
範圍: 
 
⎩⎨
⎧
2.0 <=)G  / R (
1.6 >=)G  / R (
                        (7) 
 
 
圖 4 唇色切割 
y2=a2'x=a21x1+a22x2+…+a2MxM, The 2nd PCA 
   ： ： ︰    ： ： ︰  
yn=aM'x=an1x1+an2x2+…+anMxM, The M PCA 
 
如圖(8)所示，ai'向量代表第 1主成分分析之主軸，
其 ai'向量中的係數為我們所要之特徵值。 
 
1st
2nd
 
圖(8) 主成分分析之主軸 
 
4.2 特徵值擷取-嘴唇變動率 
Step 1. 找出嘴唇的長度與寬度 
將嘴唇影像做邊緣化處理,計算出連接成份最大者為
嘴唇輪廓。如圖 9 所示。 
 
 
圖 9 嘴唇之長度與寬度 
 
Step 2. 計算嘴唇變動率 V 
1n1,2,...i , )/HW(-)/HW(V ii1i1ii −== ++   (12) 
where V is Mouth’s variable rate. 
W is Mouth's Width. 
H is Mouth's Height. 
 
 
 
5. Self-Organizing Map 
自組織映射網路的主要目標，就是以特徵映射的
方式，將任意維度的輸入向量，映射至一維或二維的
特徵映射圖上。 
基本原理可朔自大腦結構的特性，大腦中具有相似功
能的腦細胞聚集在一起，例如人類大腦中明顯的有區
分視覺、聽覺、味覺等區塊，也就是說腦神經細胞有
「物以類聚」的特性。 
自組織映射圖網路模仿這種特性，其輸出單元會
互相影響，當網路學習完畢後，其輸出單元相鄰近者
會具有相似功能，也就是具有相似的特徵值。 
 
5.1 網路拓撲座標 
 拓撲座標是指標定一輸出層單元在網路拓撲中
位置的座標。對於一個二維的型態排列的網路拓撲，
每一個輸出單元具有一個二維的拓撲座標。拓撲座標
語樣本空間座標必須釐清，樣本空間座標的維度是由
輸入層單元的數目決定，通常從數維到數十維都有可
能，是用來標示一訓練範例的輸入向量，或稱特徵向
量，在樣本空間中的位置，即訓練範例所映射的樣本
點之位置。如圖 10 所示，假設有 A 向量與 B 向量，
特徵值各為 N 個，則輸入 SOM 運算後，A 向量所映
射之座標為( 1 , 4 )，而 B 向量所映射之座標為( 3 , 1 )。 
 
…
N
Eigenvalue
Xt
Yt
Topology coordinate system 
…
A ( 1 , 4 )
B ( 3 , 1 )A B  
圖 10 網路拓撲座標 
 
5.2 鄰近區域 
鄰近區域是指網路拓撲中，以某一輸出單元為中
心的區域，稱此單元之鄰近區域。鄰近區域內的輸出
單元會互相影響。在網路學習的過程中鄰近區域會逐
漸縮小。 
Xt
Yt
n
n + 1
n + 2
 
圖 11 鄰近區域 
5.3 Algorithm-自組織應射圖網路演算 
Step 1. 計算訓練資料與各輸出單元的距離[6] 
 
2)]Cj(Xi)C(Xi[||)Cj(X)C(X|| −Σ=−    (13) 
 
其中 X(C)是第 C 個訓練資料的特徵向量， 
X(Cj)是第 j 個輸出單元映射之權重值。 
 
Step 2. 找出優勝單元 
在拓撲座標上，找出與第 C個訓練資料距離最短的輸
出單元。 
||)Cj(X)C(X||mint Winner Uni
j
−=      (14) 
 
Step 3. 調整輸入層與輸出層間的連結加權值 
 
jijij factor_R)WX(W ⋅−⋅+=Δ η         (15) 
其中 η是學習速率，R_factor 是第 j 輸出單元的鄰近
係數。 
 
6. 實驗結果 
十、 計畫成果自評 (請就研究內容與原計畫相符程度、達成預期目標情
況、研究成果之學術或應用價值、是否適合在學術期刊發表或申請專
利、主要發現或其他有關價值等，作一綜合評估 
 
   
人類語言溝通的過程中，講話者臉部表情及嘴唇形狀的變化，蘊藏著非常豐
富的語言資訊。聽障者與人溝通時，除了利用殘餘聽力外，唇語對聽障者而言更
是一項協助溝通的方式，因此聽障者學會唇語後，在與人溝通時，將會更容易了
解講話者的發言內容。聽障者藉由與人對談來學習唇語，不論時間地點都非常有
限，因此我們以電腦科技來辨識唇語正確與否，採用影像處理方法、物件導向語
言、類神經資料庫，製作一套唇語辨識系統，針對中文發音為辨識目標。利用自
組織映射圖網路(Self-Organizing Map)找出影像信號鄰近距離的相關位置完成唇
語辨識。系統已完成雛型並交給台中啟聰學校老師實際測試，本系統可以讓聽障
者了解注音發音時的唇形變化。讓聽障者了解自己發音時唇形變化的正確率有多
高，方便聽障者校正發音。 
 
 
參加之國際研討會有二場，第一場為 2009 年 7 月 19~24, San Diego, USA, 
HCI International 2009.2009 
 
1. Yun-Long Lay,  Hui- Jen Yang,  Chern-Sheng Lin, Computer-Assisted Lip 
Reading Recognition for Hearing Impaired, , HCI International 2009, July 19-24, 
2009, San Diego, LA, USA. 
 
2. Hui- Jen Yang,  Yun-Long Lay, An Empirical Study the Effect of Language 
Factors on Intention to Use Commercial Web Site, HCI International 2009, July 
19-24, 2009, San Diego, LA, USA. 
 
 
 
 
 
 
 
 
 
 
 
 
 
行政院國家科學委員會專題研究計畫成果報告【第三年】 
聽障生語者特徵及臉部資訊 之電腦輔助發音學習與評量系統研製 
計畫編號:NSC96-2221-E-167-026-MY3  
執行期限:98年8月1日至99年7月31日 
主持人：賴雲龍  國立勤益科技大學  電子系 
計畫參與人員： 楊惠貞 1 陳韋宇 2 嚴家佑 2 
1國立勤益科技大學 資訊管理系 
2國立勤益科技大學 電子系(所) 
 
 
摘要 
利用陰影疊紋量測嘴部輪廓並繪製立體圖像，以
非接觸方式量測嘴形輪廓。以數位相機拍攝疊紋影
像，經過影像處理後獲得類似等高線的紋路，計算後
填入相對應的高度數值，將各個等高線之間做平滑插
值後繪圖出立體的嘴巴圖形。方法簡易而且不需要昂
貴的量測設備即可得到精確的立體圖像。 
關鍵詞：陰影疊紋、非接觸量測、影像處理  
 
1. 前言  
聽覺機能障礙學生由於先天病毒感染、後天疾病
或外力傷害導致永久性的聽覺受損，輕度障礙者可使
用助聽器輔助，借助老師的教學指導，聽障者不致於
喪失語言溝通的能力，但在學習過程中若有輔助的教
學工具對聽障生將更有所幫助。聽障者在學習口語的
過程中，必須模仿學習老師的唇形及發音，並不斷地
修正，對話中會靠近發話者並會仔細觀察嘴唇的變
化，瞭解其語意；聽障者學習唇語後對溝通的能力提
升是有實質幫助的，本研究為量測發音時實際嘴形變
化，提供3D的圖像結果。 
人類在語言都是模仿學習聽、說、讀、寫來完
成，但對聽障生來說聽覺障礙導致語言的學習基礎受
到阻礙，無法回饋清晰的聲音修正自己的發音，造成
學習上的困難。對於聽障生的溝通學習方法有很多
種：手語教學、口手標音法、聽覺口語法、語調聽覺
法、口語教學法等等，通常透過口語教學法[9]學習
視覺讀唇、聽覺訓練、發話訓練，能瞭解別人所說的
話並發出正確的聲音給對方，其中在視覺讀唇學習注
意力要非常集中，利用殘餘聽力搭配上下文連貫的嘴
唇變化，發話者不需刻意放慢速度或誇大嘴型變化，
否則唇語資訊過度扭曲造成聽障生更難以理解，學生
上課時以口語交談，並加強注音符號發音練習得到正
確的口形發音，加強短句作文能力以得到正確的詞句
用法。 
在第二節將詳述陰影疊紋的量測及後續的影像處
理方法，第三節為實際的量測結果及可得到立體的圖
像，第四節為本篇論文結論。 
 
2. 非接觸式三維量測 
非接觸式的量測方式是利用雷射或光線干涉量測
物體，不會有接觸式探針磨損或接觸震動導致誤差問
題，非常適合量測軟性物質，且應用領域非常廣泛，
如工業模具設計加工、逆向工程、醫學工程、外科手
術模擬、動畫遊戲3D物件設計等等。以下為幾種常
見的非接觸立體輪廓量測方法[8]。 
 
雷射掃瞄： 
利用雷射探頭測距以點狀掃描整個物體表面深度變
化，再運用三角量測法計算出位置的偏移量，圖1為
雷射測距三角量測示意圖。雷射照射到物體表面時，
在物體上會成一個光點經過透鏡將光點距焦在偵測器
上，當物體深度有變化時，光點在偵測器上將會有橫
向的位移，可找出物體表面深度變化與檢測器位移量
的關係式如2.1式。之後可依自訂的解析度決定下個
偵測點位置，目前更有利用透鏡將圓形的雷射光源，
折射成線形雷射以增加掃描的面積。 
)sin('sin'
sin'
βαα
β
+−= dl
ldd             (2.1) 
 
α
ß
Detector
 
圖1.雷射測距示意圖 
 
光柵投影： 
利用投影設備，將亮暗分明的直線條紋投射在待測物
上，直線條紋因物體表面起伏造成彎曲變形，取像後
經由電腦量測出變形程度後運算物體表面的起伏，如
圖2(a)是一馬克杯底部投影光柵後直條紋彎曲現象，
圖2(b)是電話按鍵圓弧狀條紋，圖2(c)在彎曲線段的
端點連接起來，在垂直方向畫出可得兩直角三角形，
並計算出d長度即可計物體算表面高度。 
)
2
cos(tan θφ =     (2.2) 
φ :疊紋紋路與固定光柵之夾角 
o90
2
=+ θφ  
若取用不同間距之光柵，即 p ≠ 'p 則可得關係式： 
φ
θφ
sin
)sin(' −= pp     (2.3) 
疊紋斜率: 
'cos
sintan
pp
p
−= θ
θφ    (2.4) 
疊紋間距： 
θcos'2'
'
22 pppp
ppl −+=   (2.5) 
當θ =0， θcos =1，則
'
'
pp
ppl −= ，此時則表示兩
光柵片條紋不同間距平行疊合所產生的疊紋間距如圖
6，以上可以得知疊紋間距、斜率與光柵間距、角度
的關係式。 
 
 
圖6.兩組不同間距光柵平行疊合的疊紋 
 
3.2 陰影疊紋 
疊紋是把兩個光柵片疊何產生另一種紋路，陰影
疊紋則是只使用一片光柵當作參考光柵置於待測物
前，光線透射後在待測產生陰影，形成一組虛擬光
柵，我們稱之為陰影光柵，如圖6為陰影疊紋的量測
架構，此時所觀測到的干涉條紋是由光柵陰影與參考
光柵疊合而成，光柵的大小必須注意不能太細密產生
繞射效應，且能夠在物體表面產生清晰的直線條紋陰
影。 
圖7為陰影疊紋量測架構圖放大圖[4]，在觀測中
可同時看到兩組光柵AD及AE所疊合的疊紋，AD的
直條紋數量為m條，AE有n條，則 
1θ :光源入射角 
2θ :取像設備觀測角 
d :待測物體與參考光柵距離 
N :在AD範圍內的疊紋數量 
p :光柵的間距 
 
從2.6式中可以對每條疊紋計算出深度變化，深度間
距是一個定值，由此可得知每條疊紋為一等高線，在
此我們可以對每一圈疊紋標號以利計算。 
mpAD = , npBD =  
NppnmABADBD =−=−= )(  
)tan(tan 21 θθ += dBD  
21 tantan θθ +=
Npd    (2.6) 
m
oNp ×=  
 
 
圖7.陰影疊紋量測架構放大圖 
 
3.3取像後影像處理方法 
3.3.1去除參考光柵直條紋 
在疊紋影像中，若參考光柵間距較大，擷取疊紋
資訊後，參考光柵的條紋將非常明顯，這是不需要的
資訊，條紋對影像的影響是全面性的且在整個影像中
可以視為一種週期性的信號，必須使用頻率域的濾波
才能去除[2]。 
假設原始矩陣是f(x , y)，x與y為矩陣元素，傅立
葉轉換後輸出矩陣為F(u ,v)，假設索引M×N矩陣的x
索引範圍為0到M-1，y索引範圍0到N-1，以下為二維
的離散傅立葉轉換公式： 
其反轉換為 
 
∑∑−
=
−
=
⎥⎦
⎤⎢⎣
⎡ ⎟⎠
⎞⎜⎝
⎛ +−=
1
0
1
0
2exp),(),(
M
x
N
y N
yu
M
xuiyxfvuF π  (2.7) 
∑∑−
=
−
=
⎥⎦
⎤⎢⎣
⎡ ⎟⎠
⎞⎜⎝
⎛ +=
1
0
1
0
2exp),(1),(
M
x
N
y N
yu
M
xuivuF
MN
yxf π  (2.8) 
 
如圖8(a)為一條紋影像將其傅立葉轉換後如圖8(b)可
觀察出好幾個星狀，距離越近時代表直條紋越細密也
就是頻率越高，反之距離越遠代表低頻，直條紋間距
較大。使用陷波濾波後如圖8(c)，反轉後可發現大部
分直線條紋已經去除了如圖8(d)。 
 
(a)                  (b) 
     
(a)                 (b ) 
圖11.影像處理(a)去除光柵條紋與平均濾波(b)加強對
比 
 
對影像二值化後，不需要的部分須裁剪區域或手動去
除如圖12(a)，對疊紋紋路作標記，可以填入不同灰
階值做區別如圖12(b)。 
。 
     
(a)                           (b) 
圖12.去除不需要的部分及填入灰階值 
 
接下來可以利用高斯模糊[2]，將填入的灰階值
做平滑的漸層效果如圖13(a)，以圖13(a)的灰階值當
作三維圖形的z軸，x, y軸分別為圖像長寬大小，並將
最外圈數值線性位移至0 mm座標，即可完成如圖14
的立體圖形，圖15為三張嘴唇變化所量測的結果。 
 
 
圖13.高斯模糊的灰階圖 
 
 
圖14.完成三維繪圖 
 
 
(a1)                     (a2) 
 
(b1)                    (b2) 
圖15. 三張連續嘴唇的變化圖形(a1)到(a2)為擷取的影
像，(b1)到(b2)為量測的結果圖形 
 
4.結論 
究使用陰影疊紋量測嘴部輪廓，可自由地調整不
同光柵間距及光源和拍攝角度得到不同的解析度，越
細密的光柵解析度越高，但呈現在物體上的陰影條紋
會較不明顯，增加後續處理的困難度，拍攝時光場不
均勻，光圈及快門曝光量皆不同，影像無法以一定的
設定值做處理，有時需要部分手工調整及處理。在未
來的工作裡實驗投影疊紋的可行性，更希望能夠建立
更多不同發音時嘴部的動態，並研究如何將所量測的
資訊配合3D圖像設計軟體建立頭像模型。 
參考文獻 
[1] A. T. T. D. Tran, J. J. Lee, K. Zhang, Y.-H. Lo, 
Ultrafine Motion Detection of Micromechanical 
Structures Using Optical Moire Patter, IEEE 
Photonics Technology Letters, vol. 8, no. 8,1996. 
[2] Alasdair McAndrew, Introduction to Digital Image 
Processing with MATLAB, 2004, Thomdon 
Learning Company. 
[3] Andrew S. Glassner, Andrew Glassner's Notebook: 
Recreational Computer Graphics, IEEE Computer 
Graphics and Applications. Morgan 
Kaufmann,1999. 
[4] M. Batouche,A Knowledge Based System for 
diagnosing spinal deformations:Moire Pattern 
Analysis and Interpretation. CRIN/CNRS-INRIA 
Lorraine Campus scientifiquen BP 239, 54506 
Vandoeuvre,France,1992. 
可供推廣之研發成果資料表 
▉ 可申請專利  □ 可技術移轉                          日期：99 年 7 月 31 日 
國科會補助計畫 
計畫名稱：聽障生語者特徵及臉部資訊 之電腦輔助發音學
習與評量系統研製 
計畫主持人：  賴雲龍 
計畫編號：NSC96-2221-E-167-026-MY3          
學門領域：醫學工程 殘障輔具 
技術/創作名稱 臉部資訊與發音學習系統研製 
發明人/創作人 賴雲龍、楊惠貞、陳韋宇、嚴家佑 
技術說明 
中文： 
以陰影疊紋使用一片光柵當作參考光柵置於待測物前，光線透
射後在待測產生陰影，形成一組虛擬光柵，稱之為陰影光柵，陰影
疊紋的量測架構，所觀測到的干涉條紋是由光柵陰影與參考光柵疊
合而成，光柵的大小必須注意不能太細密產生繞射效應，且能夠在
物體表面產生清晰的直線條紋陰影。 
 
英文： 
Moiré grating is forming by two overlapped grating with low spatial frequency 
texture. Shadow Moiré is only using a single grating as the reference grating placed 
on the front of object. After the light projection on the reference grating through the 
test object which generates a distorted shadow called shadow grating. The shadow 
grating overlaps with the reference grating formed a shadow moiré.   
可利用之產業 
及 
可開發之產品 
聽障教育  幼兒教育 
技術特點 
究使用陰影疊紋量測嘴部輪廓，可自由地調整不同光柵間距及
光源和拍攝角度得到不同的解析度，越細密的光柵解析度越高，建
立不同發音時嘴部的動態，將所量測的資訊配合3D圖像設計軟體建
立頭像模型。 
推廣及運用的價值 可利用於聽障教育，幼兒教育，及外國人士之中文發音訓練設備，光學量測 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份
送 貴單位研發成果推廣單位（如技術移轉中心）。 
附件二 
UNIVERSITY 擔任訪問學者，所以去 San Diego 的國際會議是由 DC 往返，行程上比
由台灣往返輕鬆許多，會議結束後我順道拜訪 University of California at San Diego，及
California Institute of Technology 了解該校硬體建設。會議期間我儘量參與和我研究相
關的場次，我吸收了非常多的新知。當我報告時也有許多外國學者發問，他們對我們
的研究很感興趣，會後還有法國學者和我討論想更進一步了解論文細節，我感覺這是
一次很成功的學術交流。其中幾場 Keynote Speech 講述美軍經歷戰爭所造成的心理傷
害及其治療，內容相當珍貴與實用。另外虛擬實境方面論文也相當多，這些系統將對
未來的生活，無論是教育、娛樂、或是醫療帶來相當程度的影響。此次會議亞洲參與
人數最多的國家竟然是韓國，我們得惕勵自己不能輸給韓國人。 
 
 
 
論文發表現場 
 
 
 
 
 
 
 
 
 
The Implementation of Graphic Mode Phoneme Learning System for Hearing 
Impaired 
Yun-Long Lay 1  Justin S. Lay2  Hui-Jen Yang 3  
 1 Professor, Department of Electronic, National Chin-Yi University of Technology, Taiwan 
yllay@ncut.edu.tw  
2 Undergraduate, Department of Computer Science and Information Engineering, Tunghai 
Speech recognition initially should extract the speech features to establish the speech 
feature database for neural network training data (L. R. Rabiner and Juang B. H. 1993). A 
three- layer Back propagation neural network and Self-Organizing Maps (SOM) are applied 
in this system. BPN is used to recognize the speech and SOM is used to create the needle 
pointer graph. The speech features database is separately delivered to BPN and SOM for 
training. BPN gets the adjusted weighting values and SOM obtains the speech features. The 
trained weighting values are then delivered to BPN as a testing recognition reference. The 
two-dimensional topology is transferred into the needle pointer graph coordinates 
distribution graph. When the user operates the system, a microphone is used to input the 
speech signal and then the software extracts the speech features and sends them to BPN for 
recognition. The result is displayed in the needle pointer graph. The system framework is 
shown in Fig.1. 
 
Fig.1 Framework of the graphic speech learning system 
 
 
3. Feature extraction 
During the speech recognition process, speech recognition extraction is the first step 
to develop the speech feature database as a neural-network training database.  An 
efficient method to represent the appropriate feature parameters is necessary to process the 
speech data. The obtained feature parameters will indirectly affect the speech recognition 
rate, through the sampling process speech signals are extracted from soundcard inside the 
PC. The sampling rate of the system is 8 K/sec. The resolution is 8 bits, which means the 
speech signal waveform is 8000 dots per second. The extraction of speech features must 
go through some complicated processes. The sampling process initially goes through the 
starting-point and the ending-point to detect the start and end position of speech. The 
speech signal interval is every 15 ms to construct the sound-frame, following with the 
procedures including Endpoint detection, Segmentation, Pre-emphasize, Hamming 
window, Autocorrelation, LPC analysis and Cepstrum to get the speech feature 
coefficients (Pramod B., 1998 ). The size of speech data is very large and can not be stored 
as the reference sample for speech recognition. The speech features parameter is replaced 
by Cepstrum coefficients.  
       
3.1 Linear Prediction Code (LPC) analysis 
LPC analysis decreases the errors of actual and predicted speech signals. The method is 
to measure the pitch period and resonance frequency and gets the useful speech parameters 
quickly. 
 
The algorithm of SOM applies the Euclidean distance to calculate the output unit and 
network j  topology and distance from the center.  Eq.(3) jD  is to calculate the Euclidean 
distance of ( jX ， jY ) to C  on the coordinate diagram (Linske R. 1988). 
2 2( ) ( )j j x j yD X C Y C= − − −     (3) 
where ( jX ， jY ) are output j  topology coordinates and C is the center of the topology 
coordinates. 
 
The Mandarin phonetic signal distribution on the needle pointer graph is using the 
cluster feature of the Self-Organizing Map (SOM) neural network. SOM calculates the 
Euclidean distance of all of the Mandarin phonetic signals between each other and signals 
with similar characteristics move closer together. The similar Mandarin phonetic symbols 
cluster on the needle pointer graph. Each Mandarin phonetic symbol selects 20 features in 
the Self-Organizing Map and trains 1000 times. The result shows on the corresponding 
position on the needle pointer graph. In Eq. (3), jD  is to calculate the Euclidean distance 
of N  to M on the coordinate diagram.  
 
4.2 Training 
    When the network begins to learn, the first step must set the network parameters, the 
input vector X , the hidden number H , the output vectorY , the learning cycle, and the 
learning rateη . The Network randomly produces the weighting values including the input 
layer to the hidden layer ihWxh , the hidden layer to output layer hjWhy , the hidden layer’s bias 
hhθ and the output layer’s bias jyθ . 
4.3 Testing 
When BPN is testing, all Cepstrum coefficient data is delivered into the network and 
starts the iteration by the training data of hjWhy , hhθ , jyθ , and ihWxh . The neural network is 
based on the connected weight and bias to adjust the construction from testing data to get the 
target vector T . 
 
Eq. (4) is inserted into Eq. (5) to get the hidden layer vector H .  
h ih i h
i
net Wxh X hθ= −∑                (4) 
1( )
1 hh h net
H f net
e−
= = +                (5) 
Eq. (6) is inserted into Eq. (7) to get the input layer X  to target vector T  
k hk h k
h
net Wht H tθ= −∑               (6) 
1( )
1 kk k net
T f net
e−
= = +           (7) 
 
5. Needle Pointer Graph 
user’s mark of the testing pronunciation. When the pronunciation is correct, the two lines 
overlap. If the pronunciation is not correct, the two lines are separated. The distance between 
the two lines is the reference to adjust the pronunciation. The system calculates the corrected 
pronunciation and similarity of the test and then the score is displayed on the screen, shown 
in Fig. 4. 
 
Fig. 4  The implementation of the graphic-displaying speech learning system 
  
20 features from each phoneme were extracted for training in BPN. Through the 
adjustment of the experimental process, each phoneme trains 1000 times. After the adjusted 
weighting value, the recognition rate is appropriated. The recognition accuracy rates are 
shown in Table 1. The accuracy rate over 80% is good enough for hearing impaired to learn 
the phoneme pronunciation. All of the remaining phonemes under 80% recognition rates 
should not be used as the learning sets. Those low recognition rate phonemes are caused by 
the similarity in features which need more study to resolve.   
 
 
 
 
 
Table 1. Mandarin phonetic alphabet recognition rate 
Accuracy 
rate Chinese phonetic alphabet
100%~90% 
ㄇㄈㄑㄒㄔㄖㄙㄨㄚㄛㄝ
ㄞㄠㄢㄤㄥㄦ 
90%~80% ㄐㄧㄩㄣ 
80%~60% ㄋㄗㄘㄜㄟㄡ 
<60% ㄅㄆㄉㄊㄌㄍㄎㄏㄓㄕ 
 
計畫編號 NSC-96-2221-E-167-026-MY3 
計畫名稱 聽障生語者特徵及臉部資訊之電腦輔助發音學習與評量系統研製
出國人員姓名 
服務機關及職稱 
賴雲龍 
勤益科技大學 電子系 教授 
會議時間地點 July 14-18,2008 
Las Vegas, Nevada, USA. 
會議名稱 The 2008 International Conference on Embedded Systems and 
Applications (ESA’08) 
發表論文之題目 1. Yun-Long Lay, Minder Chen, Hui-Jen Yang, Portable Chinese 
Phonetic Learning System for Hearing-Impaired, The 2008 
International Conference on Embedded Systems and 
Applications (ESA’08), July 14-18,2008, Las Vegas, Nevada, 
USA. 
2. Yun-Long Lay, Minder Chen, Chern-Sheng Lin, Hui-Jen Yang, 
The Voice to Graphic Phoneme Learning System, The 2008 
International Conference on Frontiers in Education: Computer 
Science and Computer Engineering (FECS’08), July 14-18,2008, 
Las Vegas, Nevada, USA. 
3. Hui-Jen Yang, Minder Chen, Justin, S. Lay, Yun-Long Lay, The 
Development and Evaluation of Game Type Computer-Aided 
Phoneme Learning System for Hearing Impaired, The 2008 
International Conference on Frontiers in Education: Computer 
Science and Computer Engineering (FECS’08), July 14-18,2008, 
Las Vegas, Nevada, USA. 
4. Hui-Jen Yang, Yun-Long Lay, Chern-Sheng Lin, Minder Chen, 
Bing-Feng Li, The Development of Computer-Aided Magnifer 
System with Digital Zooming for Eye-Impaired, The 2008 World 
Congress in Computer Science, Computer Engineering and 
Applied Computing (WORLDCOMP’08- FECS’08), July 
14-18,2008, Las Vegas, Nevada, USA. 
 
 
 
 
 
 
 
 
 
論文發表現場 參與學者喜歡坐在最後面 
 
 
 
 
  Portable Chinese Phonetic Learning System for Hearing-Impaired 
 
Yun-Long Lay1  Minder Chen2  Chern-Sheng Lin3  Hui- Jen Yang 4 
1 Professor, Department of Electronic, National Chin-Yi University of Technology, Taiwan 
 2 Associate-Professor, Information Systems and Operations Management, School of 
Management, George Mason University, USA 
3Professor, Department of Automatic Control Engineering, Feng Chia University. 
4Professor, Department of Information Management, National Chin-Yi University of 
Technology, Taiwan 
 
Abstract- Generally speaking, hearing-impaired students only accept limited language 
training classes in the school. They do not have complementary learning tools to practice 
their pronunciation correctly. Therefore, the main purpose of this study uses an embedded 
system to implement a portable pronunciation learning tool for hearing-impaired people to 
learn Chinese phoneme. This system is implementing at low cost with an easy operation, a 
low memory and a better speech synthesis. Hearing-impaired people can use this tool to 
learn Chinese phonetic without time and space constraints. As experiment to test system has 
processed to verify the system performance. 
Key words: phonetic training; embedded portable system; speech synthesis; Chinese 
phoneme 
pitch mark, shown in Fig.1.  The pronunciation synthesis usually uses an autocorrelation 
function (ACF) to get the pitch contour [3,4]. After getting the pitch contour of 
pronunciation, the pitch mark position was then obtained. The equation of ACF is as follows.   
' 1
0
0
1( ) [ ( ) ( ) ] [ ( ) ( ) ], 0 1
N
n
m x n w n x n m w n m m M
N
φ −
=
= + + + +        ≤ ≤ −∑l l l   (1) 
Where N is the length of short-time distance and w(n) is the window to choose some 
short-time distance pronunciation signal x(n).  
 
Fig. 1 The pitch mark of vertical lines 
 
2.4 Pitch Synchronous Overlap-Add: PSOLA 
 In the proposed pronunciation system, phoneme synthesis is better to use the method of 
pitch synchronous overlap (PSOLA)[5]. There are three processes of PSOLA, which 
includes pitch-synchronous analysis, pitch-synchronous modifications, and 
pitch-synchronous overlap-add synthesis.  
(a)  Pitch-synchronous analysis 
Multiplying the original pronunciation signal with a pitch-synchronization Windows 
obtains a serial short-time signal: 
( ) ( )m m mx h t n x n= −   (2) 
where mt  is the location of pitch-synchronous mark. 
 
 (b) Pitch-synchronous modifications 
   Transforming the short-time signal serial ( )mx n into synthesis pronunciation pitch marks 
is qt%  and synchronization synthesis short-time signal serial is ( )qx n% . The transformation 
processes include three basic operations: changing the number of short-time signal, changing 
the time-delay of short-time signal, and changing the wave-form of each short-time signal. 
The operation of TS-PSOLA is easy and has a good synthesis performance. TD-PSOLA is a 
synthesis short-time signal and directly uses the correspondence analysis signal by shifting 
TD-PSOLA in the time axis   
mqq tt −=~δ ，  )~()()(~ qmmqmq ttnxnxnx −+=−= δ . (3) 
 The algorithm of TD-PSOLA is only for deleting or repeating some short-time signal 
based on time-changed and pitch-changed to adjust the time delaying for shot-time signal. 
 
 (c) Pitch-synchronous overlap-add synthesis 
   The equation of simple overlap-add procedure is 
 
Fig. 4 The Non-changing Turning-Points of “ㄅ” and “ㄈ” Waves 
 
Table 1 The Initials of Three Types 
type Initial phonemes 
1 ” ㄜ ” 
type 
ㄇ ㄉ ㄊ ㄋ ㄌ ㄍ ㄎ ㄏ
2.” ㄧ ” 
type 
ㄐ ㄑ ㄒ      
3 ” ㄖ ” 
type 
ㄓ ㄔ ㄕ ㄖ ㄗ ㄘ ㄙ  
 
3.2 The tone conversion 
     The experiment’s results found that the tone conversion is good for processing the 
vowels before synthesis. The Pitch Scaling Function, which normalized the voice signals 
between 0 and 1, is a good way to convert different tones. A four-order function is good 
enough to simulate these tone functions with different coefficients as shown in equation 5. 
 
( , ) 1Synthesized i th pitch period ratio tone x i th pitch period of tone −   = × −       
(5) 
where i is the index of the current process pitch, tone is for 4 different tones, 
and x=i / the total number of pitch marks. 
The light tone in Chinese has no specific pitch contour. The characteristic of the light 
tone is pronounced shortly. Hence, the initial and vowel need be adjusted to a  
suitable length. The pitch contour map of the four tones is shown in Fig 5.  
 
Fig.5 The Pitch Contour of each Tone 
 
出席國際學術會議心得報告 (第二年) 
 
計畫編號 NSC-96-2221-E-167-026-MY3 
計畫名稱 聽障生語者特徵及臉部資訊之電腦輔助發音學習與評量系統
研製 
 
出國人員姓名 
服務機關及職稱 
賴雲龍 
勤益科技大學電子系 教授 
會議時間地點 2009 年 9 月 19-24 日 
San Diego, CA, USA. 
會議名稱 13th International Conference, HCI International 2009, San 
Diego, CA, USA, July 19-24 
發表論文之題目  
1. Yun-Long Lay, Hui- Jen Yang, Chern-Sheng Lin, 
Computer-Assisted Lip Reading Recognition for Hearing 
Impaired, HCI International 2009, July 19-24, 2009, San 
Diego, CA, USA. 
2. Hui- Jen Yang, Yun-Long Lay, An Empirical Study the 
Effects of Language Factors on Web Site Use Intention, HCI 
International 2009, July 19-24, 2009, San Diego, CA, USA. 
 
 
一、參加會議經過 
     此次會議由 Springer-Verlag Berlin Heidelberg 所主辦，由 11 個研究主題組成會
議，研究領域包含:Ergonomics and Health Aspects of Work with Computers， Human 
Interface and the Management of Information, Human-Computer Interaction， Engineering 
Psychology and Cognitive Ergonomics， Universal Access in Human-Computer Interaction， 
Virtual and Mixed Reality， Internationalization， Design and Global Development， Online 
Communities and Social Computing， Augmented Cognition， Digital Human Modeling， 
Human Centered Design 會議分五天舉行，內容相當豐富。  San Diego 距 Los angles 兩
小時車程，因此租車往返最經濟方便，利用 GPS 導航，在美國開車輕鬆沒有壓力，會
後我們順道拜訪南加大，參觀該校之各項建設。 
 
二、 與會心得 
Computer-Assisted Lip Reading Recognition for Hearing Impaired 
Yun-Long Lay1   Hui- Jen Yang2   Chern-Sheng Lin3   
1 Department of Electronic Engineering, National Chin-Yi University of Technology  
2Professor, Department of Information Management, National Chin-Yi University of Technology 
1 -2No. 35, Lane 215, Sec. 1, Chung-San Rd., Taiping City. Taichung Hsien, Taiwan, 411 
3Department of Automatic Control Engineering, Feng Chia University 
3 No.100, Wenhwa Rd, Seatwen, Taichung, Taiwan, 40724 
 
Abstract. Within the communication process of human beings, the speaker’s facial expression and 
lip-shape movement contains extremely rich language information. The hearing impaired, aside from using 
residual listening to communicate with other people, can also use lip reading as a communication tool. As the 
hearing impaired learn the lip reading using a computer-assisted lip reading system, they can freely learn lip 
reading without the constraints of time, place or situation. Therefore, we propose a computer-assisted lip 
reading system (CALRS) for phonetic pronunciation recognition of the correct lip-shape with an image 
processing method, object-oriented language and neuro-network. This system can accurately compare the 
lip-image of Mandarin phonetic pronunciation using Self-Organizing Map Neuro-Network (SOMNN) and 
extension theory to help hearing impaired correct their pronunciation. 
Keywords: hearing impaired, Self-Organizing Map Neuro-Network, extension, lip reading reorganization 
1. Introduction 
    In research, the McGurk effect displays that the combination of sound and image would affect people’s 
cognition of correct word utterance (Green & Gerdeman, 1995). For instance, the Mandarin pronunciation is 
“ba”(巴) and the corresponding image is “ga”(嘎), then people will perceive the combination of sound and 
image as “ta”(他). Obviously, sound and image can simultaneously affect people’s cognition of language. 
Moreover, many pronunciations have their unique sound and image characteristics. For instance, the Mandarin 
pronunciations of “ba” (爸) and “da” (搭) are easy to distinguish by lip image, but is actually not easy to 
distinguish by the sound. On the contrary, the pronunciations of “ba”(爸) and “bai” (掰) are easily 
distinguished by the sound, but not easily distinguished by the lip image. Thus, in the lip reading teaching 
system, the pronunciation and lip image may complement one another (Black, & Taylor, 1997).  
Although the lip-reading recognition is not absolutely accurate and reliable, normal-hearing people and 
hearing impaired people actually have to use it every day. If hearing impaired people want to enhance their 
communication with other people aside from wearing hearing aid instrument, lip reading recognition is one 
essential learning method. According to a study of the University of Manchester (Neil, 2003), hearing impaired 
people can normally use residual hearing to recognize 21% of spoken words. If the hearing impaired use 
hearing aid instruments or lip recognition to properly support their communication, the recognition rate of these 
words would be 65%. Moreover, if hearing impaired people use hearing aids and lip reading simultaneously, 
the recognition rate of these words would reach 90%. Obviously, lip reading recognition is important for 
hearing impaired people. 
In order for hearing impaired people to conveniently understand pronunciation, the purpose of this 
research is to propose a computer-assisted lip reading system for lip reading recognition to help 
hearing-impaired people quickly understand pronunciation by lip reading. A dynamic lip reading image of a 
phoneme was grabbed, then through neuro-network and extension techniques each phoneme in the database can 
quickly be searched for. After the pronunciation of the hearing impaired, the system will quickly pick up the 
change of lip-shape by computer to help hearing impaired people modify the lip-shape while pronouncing a 
Mandarin phoneme. A Self-Organizing Map Neuro-Network (SOMNN) technique is used to classify and find 
related position of the image signal. This system also combines extension method to complete the lip 
recognition by the computer system for higher recognition rate. The hearing impaired can self-evaluate their 
pronunciation and lip reading accurately to enhance their language learning and communication capability.    
 
2. System Structure and Research Method 
procedure includes pre-processing, the module of extension self-organizing neuro-network recognition and 
experiment results.  
   
3.1 Pre-processing 
After the face image is extracted from the color CCD, shown in Fig.2, the most difficult step is how to 
separate the lip-shape image from the complex background image. During the separation process, the lips are 
red and their color is possibly affected by the environment noise which would cause the lips’ color change and 
be inconsistent. Therefore, we rely on the main color R(red) of the lips pattern to process (assume R: 120; G 
(green) <60 or G>120; B (blue) <40 or B>120) and make the original red color more obvious, and the green 
and the blue colors clear (Wang & Lau, 2004). Thus, the separation of lip-shape image will not affect its 
accuracy because of these external factors. The quality of whole recognition system performance will be 
increased.  From chromatics theory, color is composed of RGB (three original colors). So, if we want to 
constitute red, its R-value is obviously higher than its B and G values.  
  Taking the red area to get the image, the lip-shape image uses template matching to search for the ROI 
(region of interest). Fig. 3a is the graph of the ROI treating process (Kou, Wang, Chen, & Ye, 2001)). Thus, 
the separated lip-shape image’s corresponds to the original image coordinate position to process the 
transformation to grey-scale. After the transformation, the grey-scale image undergoes a shrinking process, 
shown in Fig. 3b. The previous obtained lip-shape image would become the lip recognition process’s input 
data for Self-Organizing Map Neuro-Network. 
 
3.2 The Module of Extension Self-Organizing Map Neuro-Network Recognition 
After the pre-process transformation of all phonemes lip-reading image, the obtained input lip-shape 
image would be sent into the module of extension-SOMNN to process.  In this module, the front end uses 
SOMNN to process the characteristic extraction of lip reading image data. The back end uses the extension 
technique to deal with the distinction between the phoneme database and the inputted lip reading image. The 
detailed algorithm is as follows:  
Step 1: setup the network parameters 
Step 2: read weight value matrix W. This W is the weight value after training all phonetic notation images. It 
sets also up the topology coordinate of the output layer.   
       XNode [j] [k] = j 
YNode [j] [k] = k                                        (14) 
Step 3: input lip reading image of all phonemes as input vector X. 
Step 4: find the superior output processing unit Node [j*][k*] 
       2[ ][ ] ( [ ] [ ][ ][ ])
i
net j k X i W i j k= −∑                       (15) 
     Where W [i] [j] [k] represents as the linking weight value of ith input unit, the network topology 
coordinate kth and the net value’s smallest hidden layer process unit Node [j*][k*]. That is:  
[ *][ *] min [ ][ ]
j k
Node j k net j k=                         (16)    
Step 5: calculate output vector Y in the output layer and access the corresponding index value of topology.  
       [ ][ ][ ][ ]
* *1, ;
0,
Y j k j j k k
Y j k other
⎧ = = =⎪⎨ =⎪⎩
                          (17) 
After the front-end process, the index value of the continuing lip-shape image can be obtained. Relying on 
these values and carrying them into the back-end Extension method processes, the distinction among the 
phoneme database to map the corresponding phonetic pronunciation. The recognition process is as follows: 
Step 1: set up the element module of each phonetic pronunciation type. 
( )
1 1
2 2
,,
,
, ,
... ...
,
ii
i
i i i ij
n in
c vPN
c v
R PN C V
c v
⎡ ⎤⎢ ⎥⎢ ⎥= = ⎢ ⎥⎢ ⎥⎣ ⎦
, i = 1, 2, 3….10                 (18)   
recognition. Fig. 4 displays the dynamic lip-reading images of phoneme “ㄈ” (“fo”) grabbed by CCD. After the 
extraction, the lip reading image would be in order from the pre-processing method to carry on the deduction 
sampling to the 22 x 19 pixel size.   
   After the completion of all lip-reading images in pre-process, SOMNN was applied to process the 
characteristic extraction of the lip reading image. Thus, each lip reading would have ten characteristic values of 
lip-shape images. For instance, Fig. 4 is the topological distribution graph with the lip reading image of 
phoneme “ㄈ” after SOMNN training.  
The trained characteristic values were used to set up the lip-shape image database. The lip reading 
characteristic values of each phonetic pronunciation through the matter-element model of extension shown as 
Rij (classic domain and node domain). In the lip-reading matter-element model, it uses Mandarin phoneme 
notation as the “Name” of the matter-element, the number of dynamic lip-images extracted as the 
“Characteristics” of the matter-element and through the training of SOMNN, get the index coordinate values as 
the “Value” of the matter-element. The setting of measured value range adopts the index coordinate 
value 10%± .  For example, the value of the 1st set of R01 is 5 calculated by SOMNN. After the modification 
by the measured value range, the value is changed to 4.9~5.1. The measured value point is decided by the index 
coordinate value in the most number of each phoneme within the 100 group-training database.  
As for testing the 100-group lip reading images, Table 2a has shown the lip reading image testing data 
after SOMNN calculation. Each phoneme has ten sets of data, and each piece of data is the topological 
distribution of the testing phoneme images, which is the input data of extension. The most highly correlated 
data has the maximum value and is highlighted with a shaded background.  Finally, the total recognition rate 
of the system is 91.6%. 
5. Conclusion 
   This research provides a lip-reading recognition system with the combination of SOMNN and extension 
theory. The main advantage of this research is that the non-supervising learning of SOMNN can automatically 
classify lip-reading image data into different characteristic value without increasing the hidden layers during 
the operation to get the convergent effect between the input signal and weighting values. Extension is based on 
the foundation of matter-element theory to recognize the fuzzy area for appropriate distinguishing and 
operating. It does not invoke complicated mathematics, so it will save some time during operating time. Based 
on the characteristic information, the relationships of 10 phonemes for Mandarin phoneme lip-shape images 
and individual phoneme lip-shape images were found by extension methodology. The total recognition rate is 
91.6% in this research as shown in Table 1. 
    From the experiment results, the performance of the recognition system is only good enough for 10 
phonemes, not for all 37 Mandarin phonemes. If the recognition process can be supplemented with sound 
recognition and high-speed photography instruments, all 37 phonemes can possibly have good recognition rates 
for advanced study.    
 
Acknowledgement 
 Authors thank the National Science Council support this research. The research number is 
NSC93-2213-E-167-010 and NSC95-2416-H-167-013.  
 
References 
1. Black, A. & Taylor, P., Festival Speech Synthesis System: System documentation (1.1.1), Human 
Communication Research Center Technical Report HCRC/TR-83, Edinburgh, (1997) 
2. Green, K.P. & Gerdeman, A., Cross-Modal Discrepancies in Coarticulation and the Integration of Speech 
Information: the McGurk Effect with Mismatched Vowels. Journal of Experiment Psychology: Human 
Perception and Performance, 21(6), pp. 1409-1426, (1995) 
3. Hatzilygeroudis, Ioannis & Prentzas, Jim, Integrating (Rules, Neural Networks) and Cases for Knowledge 
Representation and Reasoning in Expert Systems, Expert Systems with Application, 27(1), July, pp. 63-75, 
(2004)   
4. Ju, Yijing, Yu, Yongquan, Ju, Guangming, & Cai, Wen , Extension Set and Restricting Qualifications of 
Matter-elements’ Extension, IEEE CNF, 1, pp.395-398, (2005) 
5. Kohonen, T., The Self-Organizing Map, Proc. IEEE, 78 (9), pp. 1464-1480, (1990)  
                      
Fig. 3a The graph of the ROI lip-shape extraction 
                    Fig. 3b Lip-shape image after the shrinking process 
 
 
Fig. 4 The lip-reading dynamic images of phoneme “ㄈ” after the reduction sampling 
 
Table Legends 
     Table 1 The recognition results of lip reading images with 10-phoneme notation 
Phoneme recognition Total recognition rate 
Phoneme ㄆ ㄈ ㄍ ㄐ ㄓ 
91.6% 
Recognition 
rate 98% 98% 99% 70% 81%
Phoneme ㄚ ㄟ ㄦ ㄧ ㄨ 
Recognition 
rate 97% 96% 98% 97% 82%
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
研討會報到現場 
 
此次會議發表的論文已所錄至 Engineering Village (EI)索引，文章內容如下所述。 
 
 
3D Object Measurement by Shadow Moiré 
aYun-Long Lay,  bHui-Jen Yang, cChern-Sheng Lin, dWei-Yu Chen 
aProfessor, Dept. of Electronic Engineering, National Chin-Yi University of Technology, Taichung, Taiwan 
bProfessor, Dept. of Information Management, National Chin-Yi University of Technology, Taichung, Taiwan 
cProfessor, Institute of Automatic Control Engineering, Feng Chia University, Taichung, Taiwan           
Graduate Student, Dept. of Electronic Engineering, National Chin-Yi University of Technology, Taichung, Taiwan 
Keywords:  Shadow Moiré, optical measurement system, mouth shape measurement, image processing. 
Abstract: In order to get the 3 dimensional data of the speaking lip, this research applied a shadow Moiré method to 
build an optical measurement system without touching people’s mouth to measure the mouth-shaped 
contour for creating a 3D lip language tutor. A digital camera was used to capture the Moiré images through 
an image processing to get the texture of each contour line. After calculated the texture of each contour line, 
the correspondence values are then filled. Each correspondence value will make smooth contour 
interpolation and then three-dimensional image of mouth was plotted. This method for 3D measurement is 
simple and does not need an expensive measurement device to get precisely image information. 
 p: pitch of grating. 
θ1: incidence of light. 
θ2:Observation angle of image. 
 
 
21 tantan θθ +=
Npd  
 
(3.1)
m
oNp ×=   
 d: Distance of a testing object and reference grating. 
 
 
Figure 2: Enlargement of Shadow Moiré measurement framework.[4] 
From the equation 3.1, each moiré fringe can be calculated.  It can be seen that each Moiré is a contour, in which 
we can label each moiré fringe in order to facilitate the depth calculation of the measurement surface. 
3.2 The image processing methods 
3.2.1 Remove the reference grating stripes 
In the Moiré image, if the reference grating pitch is too large then the reference grating stripes will be very obvious, 
which is not the required information. The impact of stripes on the images are comprehensive and can be seen as a 
periodical signal in the whole image. It must use a low pass filter to remove [6]. 
Suppose the original matrix is f (x , y), x and y are matrix elements. Through Fourier transformed, the output matrix 
is F(u ,v). Suppose index the matrix is M×N, the x-index ranges are from 0 to M-1 and the y-index ranges are from 0 to 
N-1. Equations 3.2 and 3.3 are the two-dimensional discrete Fourier transformation formula: 
∑∑−
=
−
=
⎥⎦
⎤⎢⎣
⎡ ⎟⎠
⎞⎜⎝
⎛ +−=
1
0
1
0
2exp),(),(
M
x
N
y N
yu
M
xuiyxfvuF π   (3.2)
the reversed transformation is 
∑∑−
=
−
=
⎥⎦
⎤⎢⎣
⎡ ⎟⎠
⎞⎜⎝
⎛ +=
1
0
1
0
2exp),(1),(
M
x
N
y N
yu
M
xuivuF
MN
yxf π  
 
 
(3.3)
Figure 3 (a) is a stripe of its Fourier transformed images shown in Figure 3 (b), which could be observed out of 
several star shapes. The more close of the distance represents more thin and dense of straight line, that is the frequency 
is more higher. Reversely, the far of the distance represents the frequency is low and the pitch of straight stripes is larger. 
After using the notch filter is shown in Figure 3 (c). After inversion, the straight-line stripes has been removed, shown 
in Figure 3 (d). 
 
(a)                                 (b) 
 
(a)                      (b)                      (c) 
Figure 5: Remove the unwanted unneeded parts and fill in the gray-scale value. 
 
Figure 6: The completion of three-dimensional graphics 
 
 
(a1) (a2) 
 
(b1) (b2) 
Figure 7: (a1)(b1) are original photos (a2)(b2) are three-dimensional measurement results. 
5  CONCLUSION 
This research applies shadow Moiré to measure the shape of the mouth. It can freely adjust the different grating 
pitch, light sources and different image grabbing angle to get different resolutions. The more the density of the grating is, 
the higher the resolution is. However, when the shadow stripe on the object is not obvious, it will increase the difficulty 
to process. If the light is not uniform while projecting, aperture and shutter exposure are not all the same. The images 
can’t be processed by a fixed step of the camera operation. Sometimes it requires manual adjustment and handling. In 
the future, in order to increase its feasibility of projection Moiré in the experiment, it can also build more voice samples 
of the dynamic mouth images. The further study also needs to use the measured information matching with the 3D 
graphic design software to create head models. 
REFERENCES 
[1]. Lin, B.G., Lee, L., ,1995. The teaching and performance of intonation auditory method for hearing impaired, The 
journal Speaking and Listening, No.11,p43-56 (In Chinese). 
[2]. Fu, G. B., Precise Electro-Optic Technology, 1997, Taipei, Kau-Li Book Co. (In Chinese). 
[3]. Glassner, A., 1999. Glassner's Notebook: Recreational Computer Graphics, IEEE Computer Graphics and 
Applications. , Morgan Kaufmann. 
[4]. Tran, A.T., Lee, J.J., Zhang, K., Lo, Y.H., 1996. “Ultrafine Motion Detection of Micromechanical Structures Using 
Optical Moiré Patter”, IEEE Photonics Technology Letters, vol. 8, no. 8,pp.1058-1060. 
[5]. Batouche, M., 1992.  “A Knowledge Based System for diagnosing spinal deformations: Moiré Pattern Analysis 
and Interpretation.” CRIN/CNRS-INRIA Lorraine Campus scientifiquen Vandoeuvre, pp. 591-594,France. 
[6]. McAndrew, A., 2004. Introduction to Digital Image Processing with MATLAB, Thomdon Learning Company. 
[7]. Lin, C.S., 2001.  Digital Signal-Image and Voice Processing, Taipei, Chan-Hwa Technology Book Co. (In 
Chinese). 
無研發成果推廣資料 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
