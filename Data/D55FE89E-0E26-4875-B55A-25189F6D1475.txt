heel of one foot strikes the ground to the time at which the same 
one contacts the ground again. 
We apply the novel approach in [5] to estimate the gait period. 
It utilizes the periodicity of swing distances to estimate the gait 
period. The swing distance sw of the silhouette is obtained as 
( ) ( )
( )
2 ,
255
ct
t
cb
r
l
y yy x
y y x x
I x y
x xsw
−−
= == −∑ ∑ ×        (1) 
Where (xc, yc) is the centroid of the binary silhouette, xl  and xr 
denote the horizontal positions of the left-most and the right-most 
boundary pixels of the silhouette respectively, yb and yt denote the 
vertical positions of the bottom-most and the top-most boundary 
pixels of the silhouette respectively, and I(x, y) represents the pixel 
intensity at (x, y). 
The periodicity of gait is implied by variation of swing distance. 
The frame with the local maximum sw value is the frame in 
double-support stance, and the frame with local minimum sw value 
is the frame in legs-together stance. The differences of frame 
number between every two adjacent odd maximum frames are 
calculated and sorted The peaks t+ and valleys t− of the periodical 
curve f(t) are separated and these vertices t+∪t− serve as the 
marginal frames of each sub-sequence to determine τ. Furthermore, 
the gait period of current subject can be estimated as follows: 
3 14
 4
2  
C
CpiC i
p −== ×∑ p                            (2) 
Let pi be the ith difference in the sorted list, and C be the 
number of differences in the sorted list. The experimental results 
for measuring the periodicity of binary gate sequence are shown in 
Fig. 2. Finally, binary image sequence is separated to a number of 
sub-cycles according to those vertices in turn. 
 
Fig. 2. The Swing distance corresponds to frame numbers. 
 
（二）、步伐特徵萃取 
To get the higher recognition rate, we must find the characteristics 
that can uniquely stand for walking style of each person. Using the 
motion vectors as the dynamic features is not effective when the 
walking velocity of training video sequence is different from the 
test video sequence. It is necessary to utilize the static features 
which are not related to walking velocity. For the static part, using 
the Fourier descriptors can represent the contour of static region, 
and reduce amount of the static characteristics. Then a recognition 
mode decision described in section 4.3 is used to choose the better 
feature for gait recognition. 
 
1) Dynamic feature extraction 
 
We evaluate the histogram distribution of the optical flow 
vector field, which are treated as the dynamic feature vector. We 
partition the magnitude distribution and the direction distribution 
of the motion vectors into 10 and 13 intervals respectively. Then 
we combine the magnitude and direction histogram to 2-D 
histogram. Supposing there are N gait cycles in the sequence, after 
evaluating the overall N 2-D combinational histograms, we utilize 
the Bhattacharyya distance to measure the similarity of two 
discrete probability distributions. For two discrete probability 
distributions p(x) and q(x), over the same domain X, the 
Bhattacharyya distance dbhatt(p,q) is defined as 
( ) ( ) ( ),bhatt
x X
p q p x q xd
∈
= ∑                  (3) 
Let MVC(i) denote the 2-D combinational histogram belonging to 
the ith gait period interval. The 2-D combinational histograms, 
MVC(i), i=1,…N whose dbhatt(p, q) is lower than the threshold are 
truncated as ( ),  1,... where1= < <t tCMV i i N N N . Then we average 
the spare 2-D combinational histograms 
( )
= 
N
i 1
i
   =  C
MV
hist MV
N
′
′∑                       (4) 
Finally, we transform average 2-D combinational histogram to 
column vector of size 130×1. The column vector expresses the 
dynamic features in the video sequence.  
 
2) Static feature extraction 
 
The static part of the human body represents the common 
regions in each frame within one half cycles. Assuming S(x, y) 
represents the common region in each frame of one half cycles, 
where non-moving pixels are highlighted in binary image S. For 
silhouette sequence I, S can be obtained utilizing intersecting 
operation of I: 
( ) (
1
,     , ,
t
S x y I x y t )τ
=
= ∩                    (5) 
where I(x,y,t) is the intensity of the pixel (x,y) in tth frame, and τ is 
the time duration of the motion sequence.  
Fourier descriptors (FDs) are often used for the shape 
description of the foreground object. Their main advantage is 
invariant to translation, rotation and scaling of the object. Let the 
complex array p0, p1,…, pN-1 represents the boundary belonging to 
the region of static part whose shape needs to be described. The kth 
Fourier transform coefficient is calculated as 
21
0
    
ikn
N
N
n
nk pz e
π−
=
−= ∑    (6) 
 zk describes the frequency contents of the shape. Lower frequency 
components depict the overall shape, whereas higher frequency 
components describe details of the shape. The FDs are 
2 1 , 0,, ,    k k k Nc z z+ = 3−= …             (7) 
 ( ) ( )( ) 
min  
1  
,
 
min
,
,  
,
 t ik k
i t
kt i
N N
t i
i
ti
N N
D C C C C
c c=−= ∑=  . (14) 
Finally, Ct is assigned belong to the kth subject if and only if 
, i=1,…n. ( ) ( )    , min ,  t ik
i
t iD C C D C Ci=
 
Fig. 4. The flowchart of the recognition mode decision. 
 
Moreover, the effective feature is affected by the walking 
velocity of subject. If the walking cycle period of the testing 
sequence is similar to the training sequence, the dynamic feature is 
effective; otherwise, the static feature is more suitable. The 
recognition mode decision is based on two judgments. First, we 
calculate the difference of period between the testing video 
sequence and the highest rank of human ID from the database. If 
the difference is lower than threshold TP, we will recognize human 
ID by dynamic ranking. Second, if the difference of the periods is 
higher than threshold TP, we utilize average silhouette distance TD 
to determine what ranking is better. The flowchart of the 
recognition mode decision is described in Fig. 4. 
 
四、實驗結果與討論   
The CASIA Gait database was used to evaluate the 
effectiveness of our proposed system. The database consists of 124 
subjects captured from 11 different views simultaneously and has 
10 walking sequences for each individual consisting of six normal 
walking sequences (Set A), two carrying-bag sequences (Set B) 
and two wearing-coat sequences (Set C). In our experiments, we 
use the first three sequences of each individual in Set A as the 
training set (Set A1) and the rest two sequences in Set A (Set A2), 
Set B and Set C as the test set. Here, we only select three walking 
paths, 72° (Path 1), 90° (Path 2), and 108° (Path 3). 
Once the walking path is identified, we conduct experiments of 
testing 124 human subjects from Set A2, Set B, and Set C in the 
three different walking paths. The average human ID recognition 
rates are shown in Table 1. In the 2rd experiment, we use only the 
video sequences in 90° view as our training sequence to recognize 
people captured at three different view angles. Table 2 
demonstrates the comparison result of our system with other 
methods where Sup is the method with supervised feature selection 
[7], and CAS is the method using direct GEI (Gait Energy Image) 
shape match [8]. The experimental results show that our proposed 
method is not affected easily by the different test paths using only 
the training datasets in path 2. We find that the extracted dynamic 
features have less influence on carrying a handbag testing 
sequence than the other two proposed methods. Table 3 compares 
our proposed method with four existing methods using 90° view 
for both training and testing datasets. 
 
In this paper, we use a novel hybrid system which combines the 
dynamic feature extracted by optical flow estimation and the static 
feature formed from the Fourier descriptors to recognize the 
human walking path and human identification. The proposed 
method is not restricted to the same walking path for the training 
and testing video sequence. The experimental results show the 
effectiveness of our system. 
 
五 參考文獻 
[1]  N. V. Boulgouris, K. N. Plataniotis, and D. Hatzinakos. “Gait 
analysis and recognition using angular transforms”, Electrical 
and Computer Engineering, 2004. Canadian Conference. 
[2] N. V. Boulgouris and Z. X. Chi, “Gait recognition using 
random transform and linear discriminant analysis”, IEEE 
TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, 
NO. 3, MARCH 2007. 
[3]  J. Han and B. Bhanu, “Individual recognition using gait energy 
image”, IEEE TRANSACTIONS ON PATTERN 
ANALYSIS AND MACHINE INTELLIGENCE, VOL. 28, 
NO. 2, FEBRUARY 2006. 
[4]  J. Liu and N. Zheng, “Gait history image: a novel temporal 
template for gait recognition”, ICME 2007. 
[5]  Q. Ma, S. Wang, D. Nie, and J. Qiu, “Recognizing humans 
based on gait moment image”, Eighth ACIS International 
Conference on Software Engineering, Artificial Intelligence, 
Networking, and Parallel/Distributed Computing 2007 IEEE. 
[6]  A. Bissacco, P. Saisan and S. Soatto, “ Gait Recognition using 
Dynamic Affine Invariants”, Proc. of the MTNS, 2004. 
[7]  K. Bashir, T. Xiang, S. Gong, and Q. Mary, “Feature selection 
on gait energy image for human identification”, IEEE ICASSP 
2008. 
[8]  S. Yu, D. Tan, and T. Tan, “A Framework for Evaluating the 
Effect of View Angle, Clothing and Carrying Condition on 
Gait Recognition.” 18th International Conference on Pattern 
Recognition, pp. 441-444, 2006. 
六圖表 
Table 1. The experimental results for average Human ID 
recognition in multiple paths. 
Correctly recognized sequence # / Total test sequence #        
Dynamic Feature Only Hybrid features 
Test 
Sequence Rank1 Rank3 Rank1 Rank3 
Set A2 97.45% (725/744) 
99.33% 
(739/744) 
97.98% 
(729/744) 
99.73% 
(742/744) 
Set B 86.02% (320/372) 
95.70% 
(356/372) 
86.02% 
(320/372) 
95.70% 
(356/372) 
Set C 83.06% (309/372) 
92.20% 
(343/372) 
85.22% 
(317/372) 
93.82% 
(349/372) 
Average 
Rate 
90.99% 
(1354/1488) 
96.64% 
(1438/1488) 
91.80% 
(1366/1488) 
97.24% 
(1447/1488) 
