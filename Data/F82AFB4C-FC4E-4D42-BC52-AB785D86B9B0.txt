2 
relative scale of the patches. Obdržálek and Matas [7] developed the MSER detector 
to identify salient parts and exploited the idea that local distortion can be 
approximated by an affine transformation. 
To construct an object recognition model based on discriminative parts, there are 
two kinds of methods. The first kind is that the discriminative parts can be selected 
from a set of object parts in advance and a learning method is applied to those 
selected. For example, Lazebnik et al. [8] grouped several salient parts found by 
scale-invariant detector into a discriminative semi-local part and adopted a maximum 
entropy framework to construct an object recognition model. Dorkó and Schmid [9] 
used likelihood ratio and mutual information to rank salient parts’ distinctiveness and 
then the Support Vector Machine (SVM) and Gaussian Mixture Model (GMM) were 
applied. Liebelt et al. [10] extracted local parts from synthetic 3D objects. Local parts 
which may have more discriminative ability were selected by weighting their 
stabilities under pose and background variations. Then a visual codebook was 
constructed by using the K-Means clustering algorithm. The second kind of method is 
to integrate the selection of discriminative parts with the model construction instead 
of separating the two processes. Amores et al. [11] coded appearance and spatial 
relationships using correlogram and the AdaBoost algorithm was applied to construct 
a discriminative model. Opelt et al. [12] used image segmentation techniques and 
interest point detectors to derive the salient parts of an object. For an object category, 
e.g., bicycles, the AdaBoost algorithm was used to derive a detector of the category. 
Schneiderman and Kanade [13] retrieved local part representations from image 
wavelet coefficients and trained part classifiers using Adaboost with 
Confidence-Weighted Predictions.  
 
3. The Proposed Method 
The proposed method consists of three phases. First, we use some sliding 
windows that are different in scale to retrieve a number of local parts from each model 
object. For an object, each sliding window slides over it once. The object part covered 
by the sliding window is retrieved. In addition, the sliding window slides in an 
overlapping manner. That is, the sliding window moves a fixed number of pixels for a 
slide such that the next retrieved part partially overlaps the current retrieved part. By 
extracting local parts of different scales, our method does find potential discriminative 
parts. For each local part retrieved, we extract four types of descriptors from it to form 
a feature vector. Next, we call the prototype construction procedure, which is revised 
from the Crisp Construction Process (CCP) algorithm [14], to construct the prototype 
feature vectors (prototypes for short) of the model objects by using the feature vectors 
obtained in the first phase. The Crisp Construction Process is a supervised clustering 
4 
Table 1. The prototype construction procedure. 
1 For each model object M, compute the average of all feature vectors of M and 
use it as an initial M-prototype. 
2 For each feature vector v, find its nearest prototype r. If F(v) is equal to O(r), v 
is marked, if not, v is unmarked. If all the feature vectors are marked, go to step 
5.   
3 For each model object M, if any unmarked feature vector exists, we generate a 
new M-prototype whose feature values are the same as those of these unmarked 
features’ medoid. 
4 For the model object M in which the new M-prototype is added in step 3, apply 
the K-Means clustering algorithm to adjust the prototypes by using the current 
M-prototypes as seeds. Go to step 2. 
5 Remove the overfitting prototypes that contain few feature vectors and are close 
to other prototypes.  
 
In the phase of object recognition, we also use sliding windows to retrieve a 
number of local parts for a test object. For a feature vector representing a local part of 
the test object, we find its nearest prototype from the set of prototypes constructed in 
the prototype construction phase by calculating the Euclidean distance. Since the 
nearest prototype denotes a discriminative part of a certain model object, it denotes 
that the local part of the test object is matched by the discriminative part of the model 
object. Therefore, each feature vector of the test object is matched by a discriminative 
part of a model object. It is possible that not all local parts of the test object are 
matched by the discriminative parts of the same model object. Thus, a similarity 
measure is required to measure the similarity between the test object and each model 
object. The similarity measure Sim between a test object T and a model object M is 
defined as, 
TM
TCMC
AreaArea
AreaArea
Sim


 ,                                              (2) 
where AreaM is the area of M, AreaT is the area of T, AreaTC is the total area of T’s 
local parts which are matched by the discriminative parts of M, and AreaMC is the total 
area of M’s discriminative parts which match the local parts of T. The area of an 
object or a local part is the number of pixels contained in the object or the local part in 
the image. After the similarity between a test object and each model object is 
calculated, the test object is assigned to the model object with the highest similarity. 
We adopt the C4.5 decision tree algorithm to accelerate the phase of object 
6 
“Junk” sets. Because there are overlaps in these sets, the union of these sets is 
constructed to be the test data for the query building. To compute recognition 
accuracy, we manually marked the instances of the model objects in each test images 
by using minimum bounding rectangles. 
The recognition rates are listed in Table 2 and Table 3 lists the average 
recognition time in seconds for recognizing a test image. The number of prototypes 
generated by the proposed method is 7,743. The vocabulary size is 20,000 for the 
AKM method and 15,000 for the CLR method. The number of clusters at each level is 
10 for the vocabulary tree. Although the proposed method is not the fastest method, it 
tops the other methods in terms of recognition rate.  
 
Table 2. The recognition rates for       Table 3. The recognition time for 
the Oxford buildings dataset.           the Oxford buildings dataset. 
 
 
 
 
 
 
 
 
 
The various conditions including illumination and viewpoint changes, occlusion, 
cluttered background, scaling, and distortion, are well handled by the proposed 
method in the experiments. 
 
4.2 Experiments on the ETH-80 dataset 
 The ETH-80 dataset contains eight object categories, each of which contains ten 
objects. Each object presents 41 instances from viewpoints spaced equally over the 
upper hemisphere (at distances of 22.5-26°). Objects in some categories such as apple 
and tomato present very small within-category variations.  
The experiments are composed of three experimental setups. The first setup takes 
an instance (number 000-000) of each object as training data; the second uses three 
instances (number 000-000, 068-090, and 068-270) of each object as training data; the 
third uses five instances (number 000-000, 068-090, 068-270, 090-000, and 090-180) 
of each object as training data. For each experimental setup, the remaining viewpoint 
instances of each object serve as test data. A test object is said to be correctly 
recognized if it is matched by a correct model object. The recognition rate is defined 
Method Recognition rate 
DP without tree 83.02% 
DP with tree 86.32% 
AKM [23] 54.99% 
LAFs [8] 63.18%, 
CLR [24] 71.12% 
Vocabulary tree [13] 51.44% 
 
Method Time (sec) 
DP without tree 15.64 
DP with tree 3.71 
AKM 1.19 
LAFs 1.98 
CLR 5.11 
Vocabulary tree 0.93 
 
8 
be learned in the real world is infinite; therefore, a faster learning process will always 
be preferred. 
 
Reference 
[1] D.G. Lowe, Distinctive image features from scale-invariant keypoints, 
International Journal of Computer Vision, 60 (2) (2004) 91-110. 
[2] S. Savarese and Li Fei-Fei, 3D generic object categorization, localization and 
pose estimation, In Proc. International Conference on Computer Vision, 2007. 
[3] A. Berg, T. Berg, and J. Malik, Shape matching and object recognition using low 
distortion correspondence, In Proc. International Conference on Computer 
Vision and Pattern Recognition, 2005, pp. 26-33. 
[4] A. Leibe, A. Leonardis, and B. Schiele, Combined object categorization and 
segmentation with an implicit shape model, In Proc. European Conference on 
Computer Vision, 2004, pp. 17-32. 
[5] M. Weber, M. Welling, and P. Perona, Unsupervised learning of models for 
recognition, In Proc. European Conference on Computer Vision, 2000, pp. 
18-32. 
[6] R. Fergus, P. Perona, and A. Zisserman, Object class recognition by 
unsupervised scale-invariant learning, In Proc. International Conference on 
Computer Vision and Pattern Recognition, 2003, pp. 264-271. 
[7] S. Obdržálek and J. Matas, Object recognition using local affine frames on 
maximally stable extremal regions, In J. Ponce, M. Hebert, C. Schmid, and A. 
Zisserman, editors, Toward Category-Level Object Recognition, Lecture Notes 
in Computer Science, 4522 (2006) 83-104. 
[8] S. Lazebnik, C. Schmid, and J. Ponce. A discriminative framework for texture 
and object recognition using local image features. In J. Ponce, M. Hebert, C. 
Schmid, and A. Zisserman, editors, Toward Category-Level Object Recognition, 
(2006) 423–442. 
[9] G.Y. Dorkó and C. Schmid, Selection of scale-invariant parts for object class 
recognition, In Proc. International Conference on Computer Vision, 2003, pp. 
634-640. 
[10] J. Liebelt, C. Schmid, and K. Schertler, Viewpoint-Independent Object Class 
Detection using 3D Feature Maps, In Proc. International Conference on 
Computer Vision and Pattern Recognition, 2008. 
[11] J. Amores, N. Sebe, and P. Radeva, Fast Spatial Pattern Discovery Integrating 
Boosting with Constellations of Contextual Descriptors, In Proc. International 
Conference on Computer Vision and Pattern Recognition, vol.2, 2005, 
pp.769-774. 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/27
國科會補助計畫
計畫名稱: 應用物體具鑑別力部份進行物體辨識
計畫主持人: 劉英和
計畫編號: 99-2218-E-259-001- 學門領域: 圖形辨識
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
