NSC Report for Feature Extraction of Botnet
Generated Spam on Cloud-based Mail Services
I. INTRODUCTION
The huge number of spam mails have been a serious
problem to both ISPs and normal users. A very common
email abuse is sending unsolicited bulk email messages, also
known as junk mail, email spam or simply spam, in a large
scale. Spam mails not only consume bandwidth but also waste
the storage space of mail servers. Most spam messages are
distributed through a botnet, a huge group of hosts under the
control of a botmaster. This strategy has two major advantages.
First, the huge number of bots can efficiently distribute spam,
and the number of spam messages distributed by any single bot
can be reduced to be stealthy. Second, it is difficult to name a
list of spamming bots as the bots can be changed dynamically.
Keeping an up-to-date blacklist becomes unrealistic and prone
to errors. The motivation of sending spam mails mainly comes
from making profits or spreading malware to infect more hosts
[1], [2]. Spammers can even collect private information such as
personal account or credit card number by social-engineering
techniques.
To evade spam detection, spammers tend to generate suf-
ficiently polymorphic spam content to confuse the filtering
system based on static filtering rules with the features such as
keywords. Modern spam distribution uses templates to gener-
ate various forms of spam mails while keeping its readability
and semantics [3], [4]. A template is a pre-developed layout,
which contains invariant parts, namely anchors, and variable
parts, namely macros. Spamming hosts can generate a large
number of instances from a template by customising a word
bank or random strings to fill in the macros.
A countermeasure to defeat template-based systems is to
infer their templates from numerous spam samples and match
newly incoming mails against the templates [5], [6]. In brief,
the inference process can recover the inferred templates by
representing them in regular expressions. However, spammers
can complicate the inference process by constantly changing
the spam templates, such as swapping the sequence of URLs
or paragraphs. Since the inference process looks for com-
mon anchors from the spam samples to derive the templates
represented in regular expressions, a slight revision of the
spam templates will make regular expressions adapted to the
revision. The regular expressions may be too general, or many
regular expressions are required to represent the variations
of the templates. In the former, false positives are likely to
happen. In the latter, more memory space is required to store
the increasing number of regular expressions.
The inference process also takes non-trivial efforts to set
up an isolated environment for running the bot instances to
prevents them from attacking or delivering spam messages to
the other hosts in the Internet [7]. The establishment of such
an environment needs great care, so that the establishment
will not be easily discovered by the bot instances, or they will
refuse to behave as usual (i.e., send spam). Another problem
is that the huge number of regular expressions will seriously
slow down the scanning of spam filtering.
Moreover, as cloud-based mail services become popular,
the users of the cloud-based services do not have to keep the
filtering signatures and perform spam filtering locally on their
own hosts. Providing anti-spam in the cloud-based services
has the following two advantages. (1) The analysis of mail
samples can be centralized rather than distributed among users.
A cloud mail server can collect samples across countries and
areas, using the samples to infer up-to-date templates. (2) The
users of the mail service do not have to keep and update their
own signatures. Examples of commercial solutions include as
Nopam of Green-Computing [8] and Symantec.cloud [9].
This work is motivated to construct a scalable system for
analysing spam mails. To better adapt to possible mutations of
templates and reduce the time complexity, we use the concept
of similarity in web documents to analyse the clustering of
spam mails [10], [11]. A slight change of the template formats
can be still recognized because the templates are similar to
known ones. The web documentation similarity strategies are
good at handling a huge number of spam samples in a scalable
manner (to be discussed later).
We observe that over 80% of spam samples contain HTML
parts. This observation motivates us to measure the similarity
between the HTML tags of spam samples. This analysis
brings a great advantage that random strings or word bank
will not affect the similarity from the perspective of HTML
tags. Finding similar or duplicated mails for identifying spam
campaigns has been studied in [12], [13]. The analysis of tags
helps us to identify the clusters of spam mails based on the
structures of spam content, rather than the spam content itself.
Focusing on the structures can make it difficult for spammers
to evade by mutating the spam content or slightly changing
the structures, and it is more efficient to represent and match
the structures than the mail content.
In this work, we first collect a number of spam samples,
and extract tag content from the samples . We then use the
Charikar’s simhash algorithm to generate the fingerprints from
the samples [14]. The observations show that spam mails
of similar structures tend to have similar tag content, i.e., a
sequence of HTML tags extracted from the mails. We also
the shingling method is not adopted in this work because we
cannot generate a centroid fingerprint from the fingerprints of
minhash.
Moreover, we can also consider HTML features to find
similar web pages rather than merely rely on syntactic
content. Examples of detecting similar documents based
on the HTML features are [22], [23]. They extracted the
URLs as a feature to detect near-duplicate documents and
spam mails. Their detection schema treat message body as byte
stream which represents the message body by a feature vector
based on the statistic of the value of byte stream. Another work
of Tanguy et al., demonstrated their work that track web spam
with HTML style similarities [24]. They analyse a document
in multiple view points, such as: tags, words, full text, and
alpha. These features being hashed into fingerprints using the
technique of locality sensitive hash, LSH is a hashing function
maps similar items closer in the vector space [25].
C. Clustering Method
Clustering is essential to group a large scale of samples into
clusters of similar ones, and can save the space needed to store
the characteristics of each individual sample. In this work, we
assign a spam mail into a cluster according to its tag content
to avoid the complexity of pair-wise comparisons between the
samples in the whole dataset of spam mails.
In this work, the clustering method is based on the leader-
follower algorithm [21]. This method initially supposes a point
in the vector space to be a cluster. As a new sample comes to
be tested for similarity with one of the clusters, it is added to
a cluster if it is similar to that cluster. If none of the clusters
are found in range of similarity threshold, the new sample
will become a new cluster. We enhance this method into a
two-phase clustering. First, we look for only those clusters of
spam messages which have similar tag counts. Second, these
clusters will be merged based on their similarity and extracted
features to reduce the number of clusters. The details will be
discussed in Chapter 3.
III. METHODOLOGY
We propose a multi-level clustering method to handle a
large number of spam mails while keeping the accuracy of
clustering result. Each level accounts for different missions. In
the first level, we extract the HTML tag content and obtain a
rough clustering result based on the hashing result of the tag
content. In the second level, we merge similar clusters into
one to ensure the accuracy of clustering according to their
hashing similarity. The last level is responsible for validation
and analysis of the spam clusters. We will discuss the potential
evasion methods in the end of this chapter.
A. Observation
The idea of multi-level clustering comes from four main
observations from real spam samples and the analysis of them.
The observations of spam samples inspire us to take the HTML
tag content as the key feature in the clustering because the
spam samples have the same HTML tag content even though
Fig. 1. Spam case 1.
they are different in the content. As a result, we choose HTML
tag content as a basis of the clustering method. Another benefit
is that we can ignore randomized strings in the spam mails.
1) Case Studies: As discussed in Chapter 2, spammers can
evade spam filtering by changing their instances. Figure 1
demonstrates an example of two spam mails that are different
in their sender, subject, embedded URL, and mail content.
Spammer also add randomized strings in the subject and mail
content to confuse the detection system. Although both spam
mails share little resemblance, they have the same HTML
tag content, i.e., the same layout or structure. Ignoring the
random strings and changeable phrases is the main advantage
of evaluating spam mails from the view point of HTML tag
content. We found many such cases in the collected spam
mails. This is a proof that spammers create spam instances
from the common templates and replace the macros with the
terms from a dictionary. Since there are few common terms
(i.e., the anchors) between the two spam mails, the regular
expressions to represent the anchor parts may be too general.
Any mails having the anchor terms is potentially be matched
as spam mails.
The spam mail in Figure 2 is another case of similar mails
with different HTML tag content. The difference of their tag
counts is 10, and the layout is also different. We apply simhash
to measure their similarity, which is 1.0 for their whole mail
body and 0.996 for their tag files, meaning they are very
similar in terms of simhash. This result shows that simhash
can handle the slight change of spam tag content.
2) Data set analysis: We collected about 36,480 of spam
samples from a high school campus during two months. This
data set contains 30,455 spam samples that can be correctly
decoded and contains an HTML sub-part. The HTML tag
content can reveal the structure of mail content. We evaluate
the sub-parts of the spam samples in the data set, over 82%
mails of the data set contains HTML tag. In other words,
most spammers use the HTML language to create spam. This
observation encourages us to analyse the HTML feature in
the spam mails. In Figure 3, we further analyse the sub-part
information, TXT represents plain text sub-part, and ELSE
Algorithm 1 Tag retrieval process
Require: Mail sample Set D;
m kth decoded mail sample
Gi The group of mail that tag count is i
Ci,j jth cluster of group i
TCk Tag content of mail k
TCi,j Tag content of cluster j of group i
TNi The number of cluster of group i
sim(i, j)Hash Similarity between mi and mj
t Similarity Threshold
Ensure: Rough clustering result
1: for each mk ∈ D do
2: Initialize each TNi to 0
3: if Gi =∅ then
4: TNi++
5: Gi ← Ci,1 ∪ {mk}
6: else if ∃ j, s.t. TCk == TCi,j then
7: Gi ← Ci,j ∪ {mk}
8: else
9: TNi++
10: Gi ← Ci,TNi ∪ {mk}
11: end if
12: end for
A cluster is denoted as Ci,j , which means the cluster j in
the group tag count i. A group is composed of mail clusters
having the same number of HTML tags. Each cluster is filled
with mails that have exactly the same HTML tag content. If a
tag content is not matched or the group is empty, we will add
a new cluster in that group. The sequence number of a cluster
is determined by the sequence of a cluster being created.
2) Cluster Merge: The ultimate goal of this level is to
merge similar tag content obtained in the first level into clus-
ters. Using this method allows us to handle the kind of spam
mails that the tag structure has been modified. The similarity
between two points is derived from hamming distance, which
represents how many corresponding bit positions are different
in two equal length bit vectors, denoted as DVi,Vj . If the length
of given bit vector is Len, then the similarity is defined as
Sim(Vi, Vj) = 1−Dbit
Len
. In other words, the similarity implies
how many bits should be changed to equate two bit vectors.
In the beginning, each tag content is hashing into a bit
vector. An example of cluster is demonstrated in Figure 4, in
which each cluster contains at least one vector and a centroid
Vc. For each tag content, we try to find similar tag content in
the nearby tag group. If the similarity exceeds the similarity
threshold T , they are assigned to a new cluster, and the value
of centroid is the mean of them. If a vector Vk exceeds the
threshold with a vector in a cluster, it can be added into the
cluster unless Sim(Vc, Vk) > T . It ensures that each pair of
vectors in the same cluster are similar. The definition is the
similarity between any give pair less than 2T .
Definition 3.1: ∀ Vj ∈ Ci ⇒ D(Vj , Vc) ≤ k
Fig. 4. An example of clustering.
Theorem 3.1: D(Vnew, Vc) ≤ k ⇒ D(Vnew, Vj) ≤ 2k ,
where Vj ∈ Ci
Lemma 3.2: D(Vj , Vc) = k1 , k1 ≤ k
D(Vnew, Vc) = k2 , k2 ≤ k
D(Vnew, Vj) ≤ k1 + k2
Proof: Suppose D(Vnew, Vj) = k3 , k3 ≥ k1 + k2
because D(Vnew, Vc) = k2 ≤ k
therefore we can change k2 bits in Vk s.t. V ′k = Vc
therefore D(V ′new, Vj) = k3− k2 ≥ k1, which conflict with
D(Vj , Vc) = k1
therefore we have D(Vnew, Vj) ≤ k1 + k2
because D(Vj , Vc) = k and D(Vnew, Vc) = k
therefore D(Vnew, Vj) ≤ 2k
3) Cluster Validation: The final level of our algorithm is
validation. We analyse the clusters and discuss what factors
effect the result. One of the factor is how many bits are used
in the hashing function. The length of bit vector significantly
affects the result of clustering. For small tag count, a short
length is required to avoid unused bits. On the other hand,
a large tag count needs more bits to avoid collisions that
two samples are hashed into similar bit vectors due to limited
hashing bit length. To optimize the clustering result, the length
of hashing varies with tag count of mail samples.
In our observation, a sample with many tags tends to be
a spam or generated from template. For a normal user, a
mail generated from template is not always a spam mail. We
validate our clustering result with mail collected from normal
users to figure out what differences between spam mails and
normal mails in the structural level.
C. Detection Algorithm
In this section, we discuss about how to deploy our system
into real-time application. After we build up the mail database,
we can easily check whether a mail has been collected in our
database or not. We decode a mail and retrieve tag content, and
then try to find whether this mail matches a cluster fingerprint
or not. The match process works like we did in the previous
clustering stage. Each mail is indexed to a tag content group
based on tag count. If this mail matches a tag content, we
can say that this mail belongs to a cluster to which this tag
content belongs. The process eliminates the storage space and
the time of matching fingerprint.
TABLE V
THE DETECTION RATE BASED ON SIMHASH.
Hash bit length D1 D2 Known Spam
128 49.73% 21.60% 100%
96 50.2% 23.39% 100%
64 54.82% 28.29% 100%
32 57.18% 38.75% 100%
TABLE VI
A CASE OF FINGERPRINT DOMINATED BY REPEATED TAGS.
Tag content Fingerprint
<html><body><br><br><br><br>
<br><br><br><br><br><br><br><br>
11011010
<html><head><a><font><br><br><br> 11011010
taken into consideration, that is, the threshold of Jaccard sim-
ilarity. The Jaccard similarity threshold make up the defect of
simhash, while it also raises false negatives in the known spam
instances. The reason is in qualification of two fingerprints. We
measure the average Jaccard similarity between all members
in the cluster. In the end, the average of Jaccard similarity
might be affected by other tag content, and it leads to false
negatives. On the other hand, it is undeniable that the Jaccard
similarity enhances the detection rate of our experiment. It
also decreases the possibility that any mail is mismatched due
to the defect of simhash similarity.
D. Case Studies
We observed the false alarms raised from normal mails.
Each normal mail is compared with the spam mails in the
cluster, and we categorized them based on our subjective
judgment whether they are similar in the tag content. We
conclude three reasons about why normal mails are considered
to be similar to spam samples.
First, some mails are automatically generated from pro-
grams such as announcement documentation or response mes-
sages from websites. These mails tend to use many tags to
create a pre-developed layout just like template. Second, a
normal mail that contains tag such as URLs and newline
tag looks similar to spam mails. Many spam mails contain
only advertisement URLs and some strings. In our work, we
ignored the body context which leads to this false alarm. The
TABLE VII
JACCARD SIMILARITY THRESHOLD = 0.5.
Hash bit length D1 D2 Known Spam
128 17.11% 6.01% 99.35%
96 17.98% 6.24% 99.03%
64 18.95% 6.90% 98.38%
32 19.42% 12.03% 100%
TABLE VIII
JACCARD SIMILARITY THRESHOLD = 0.6.
Hash bit length D1 D2 Known Spam
128 9.29% 3.12% 99.35%
96 9.33% 3.34% 99.03%
64 10.28% 3.34% 98.38%
32 10.51% 7.13% 98.06%
TABLE IX
JACCARD SIMILARITY THRESHOLD = 0.7.
Hash bit length D1 D2 Known Spam
128 3.96% 1.11% 96.44%
96 3.89% 0.89% 94.50%
64 4.64% 0.89% 95.79%
32 4.67% 2.00% 95.15%
TABLE X
JACCARD SIMILARITY THRESHOLD = 0.8.
Hash bit length D1 D2 Known Spam
128 1.23% 0.22% 93.85%
96 1.20% 0.22% 83.82%
64 1.72% 0.22% 94.50%
32 1.65% 0.45% 83.17%
above samples are considered to be similar to spam samples
since they do have similar patterns with spam mails, such as
in table XI.
The last sample is considered to be different from the spam
messages in the assigned cluster. The reason that they exceed
the threshold is due to the nature of simhash that the fingerprint
are extremely dominated by some specific tags. Moreover, the
tag content of this kind of samples is also similar to the spam
clusters that exceed that Jaccard threshold, and thus raised this
false alarm.
V. CONCLUSION
There are still few issues in the real-time deployment. The
first is the volume of spam database. Our method significantly
relies on the variety of spam database. We can fast determine
whether a mail has captured or not while keeping the size
fingerprint in a reasonable range with a diverse spam data
base. A cloud-based mail system is suitable for us since it
can collect a large volume of spam. Moreover, we can collect
spam samples from honeypot or botnet to expand the scale of
our database.
It is impossible for us to keep all fingerprints forever, since
spammers usually update their evasion strategies. To define
a reasonable expiration mechanism, we should keep track of
how long a spam campaign can survive. If no mail is matched
against a given cluster for a given time, we can remove it from
our database. The situation can be explained as that perhaps
spammers have changed the spam template or stop sending
mails based on a template any more. Even if a cluster has
been removed from our database, we can rapidly update our
database while receiving it. In conclusion, the expiration timer
can be the average spam survival duration.
Improving the detection rate without raising false negatives
is the most important in this work. This work still lacks a
decisive factor to distinguish a mail is spam or not. We hope
TABLE XI
A FALSE-POSITIVE CASE.
Matched Spam <br><br><br><br><br><a><a>
Normal mail <a><a><br><br><br><br><br>
 1 
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                    日期：2012年 10月 28日 
                                 
一、參加會議經過 
這個會議一共有 3天的時間(實際議程為 2天半)，我在這次會議中全程參與。這個
會議同時有 6個平行的議程，所以我盡可能挑與資訊安全相關的議程參加(包括我們
發表論文的議程)。其中也有遇到同樣來自台灣之其他大學的教授，針對目前正在研
究與本計畫相關的議題，也做了一些想法上的交換。 
二、與會心得 
計畫編號 NSC 100-2221-E-194-029- 
計畫名稱 雲端郵件服務上 Botnet垃圾郵件之特徵擷取 
出國人員
姓名 
林柏青 
服務機構
及職稱 
中正大學資訊工程系 
會議時間 
2012年 2月 19日
至 
2012年 2月 22日 
會議地點 
韓國江原道 
會議名稱 
(中文)國際先進通訊技術研討會 
(英文)International Conference on Advanced Communications 
Technology (ICACT 2012) 
發表論文
題目 
(中文)藉由自動推論敏感的應用層欄位來進行封包匿名化 
(英文)Towards Packet Anonymization by Automatically Inferring 
Sensitive Application Fields 
附件四 
主旨: [ICACT2012] Notification of Acceptance
從: "Secretariat of ICACT2012" <secretariat@icact.org>
日期: 2011/11/22 下午 10:06
到: "Prof. Po-Ching Lin" <pclin@cs.ccu.edu.tw>
Dear Prof. Po-Ching Lin,
Congratulations on your acceptance!!!
It is great pleasure for us to inform you that your paper has been officially accepted by
ICACT2012 committee.
1st Author name is Prof. Po-Ching Lin from National Chung Cheng University.
Paper Title is Towards Packet Anonymization by Automati …...
Paper number is 20120262.
Email Address  is pclin@cs.ccu.edu.tw.
 
We kindly recommend you to visit the Paper Author's page in ICACT2012 Website for further
processes, Step 5. Payment Registration and Step 6.Camera Ready Manuscript
Submission.
Please keep in mind the due date of the payment registration is until Dec. 20, 2011.
We are looking forward to seeing you at ICACT2012 in Pheonix park, Pyeongchang, Korea(south).
 
Sincerely yours,
 
Dr. Dae-Young Kim
Chair, ICACT2012 Technical Program Committee
 
Dr. Thomas Byeong-Nam Yoon
Chair, ICACT2012 Organization & Operation Committee
If you don't want this type of information or e-mail, please click the [unsubscription] GIRI, 1713
Bundang-Obelisk bldg., 216 Seohyunno (Seohyun-dong), Bundang-gu, Sungnam-city,
Kyunggi-do, 463-820, Republic of Korea Email:secretariat@icact.org
[ICACT2012] Notification of Acceptance  
1／1 2012/10/29 下午 02:47
classification method that help to identify potentially sensitive
application fields.
Contributions: To the best of our knowledge, there is no
prior work regarding automatically inferring sensitive infor-
mation in the application messages for packet anonymization.
The contributions of this work are as follows:
1) We present a method to automatically identify the fields
in the application messages if there is not a proper
protocol parser.
2) We present the measures and the classification method
to identify potentially sensitive information in the appli-
cation messages.
3) We evaluate the accuracy of the methods in this work
with real packet traces, and demonstrate the possibility
of inferring sensitivity of application fields.
The rest of this work is organized as follows. Section 2
reviews related work and discusses the limitations. Section 3
presents the methods to automatically infer application fields
and judge potentially sensitive application fields. Section 4
studies the accuracy of the algorithms with several experi-
ments, and investigates the cases of false negatives and false
positives. Section 5 concludes this work and points out the
future work.
II. RELATED WORK
In this section, we review the open resources of packet
traces, and the anonymation tools and techniques. We will
discuss their limitations to justify the motivation of this work.
Finally, we will introduce the related work on inferring fields
of application protocols, which is closely related to this work.
A. Sharing the network traces
Despite the benefit of sharing packet traces, open re-
sources available on the Internet are quite limited due
to privacy concerns. OpenPacket.org (www.openpacket.org),
Packetlife.net (packetlife.net), and Wireshark SampleCaptures
(wiki.wireshark.org/SampleCaptures) are examples of the re-
sources. The traces on these sites are uploaded from active
participants and usually left unanoymized because the partici-
pants can submit only those packet traces they allow. Although
the traces are frequently updated, the volumes of individual
traces are too small for intensive network analysis and testing
network security equipment.
The Web site CAIDA (www.caida.org) provides a large
volume of packet traces from an OC192 Internet backbone.
The IP addresses in the packets are anonymized by the
CryptoPAn prefix-preserving algorithm [5], and the payloads
are completely removed. However, removing the payloads
will significantly reduce the value of the traces for network
security analysis, which usually inspects the payloads. A
recent work called NCTU BetaSite [9] records network traffic
from voluntary students in a university campus. The BetaSite
supports anonymizing sensitive information by specifying the
application fields and patterns, but the specification is still a
manual process.
B. Anonymization tools and techniques
Due to the limitations of open resources, it is common that
network researchers collect packet traces by themselves, say
from the network of their affiliations. Several anonymization
techniques and tools have been developed for the network
administrator to hide sensitive information in the packet traces.
They usually support three strategies: (1) directly dropping the
payloads, (2) specifying the protocol fields to be anonymized,
and (3) specifying the patterns that may be sensitive in-
formation. According to the preceding discussions, neither
strategies can effectively anonymize sensitive information in
packet traces.
Anonymizing packet traces inevitably distorts the original
semantics of the application messages, thereby degrading the
utility of the packet traces for analysis. Preserving the utility
after packet anonymization is therefore an important issue.
For example, a technique called prefix-preserving can preserve
the relationship between the anonymized IP addresses in
the same subnet [4]. That is, if two original IP addresses
share a k-bit prefix (i.e., belonging to the same subnet), their
anonymized addresses will still share a k-bit prefix — just
the prefix is different. The preservation strategy can promote
the utility of flow analysis. The work in [8] compares the
utility of anonymized packet traces by several tools to intrusion
detection systems (IDSs). The work in [10] particularly studies
the utility of anonymized HTTP fields to IDSs. The studies are
complementary to this work, which focuses on improving the
ability to precisely anonymize sensitive information.
C. Inferring the fields of application protocols
Inferring the fields of application protocols has been studied
in the past. The main idea, derived from multiple sequence
alignment (MSA) (en.wikipedia.org/wiki/Multiple sequence
alignment) in bioinformatics, is aligning similar application
messages and identifying the constant and variant components
in them. One of the MSA approaches is called progressive
alignment construction, which uses the Needleman-Wunsch
algorithm [11] to perform an alignment on two closest se-
quences derived from similarity comparisons on a global set
of sequences.
Roleplayer [12] modifies the Needleman-Wunsch algorithm
to improve its performance by adjusting the weight in aligning
two application messages to find the most appropriate align-
ment. Discoverer [13] improves the alignment method by first
breaking up a message into a sequence of tokens, and then
inferring the fields by comparing the counterparts in another
message. The alignment is based on the tokens rather than raw
byte sequences. The work in [14] tracks the program binary
implementing the protocol to further improve the precision
of protocol reverse engineering, but tracking all the binaries
for the protocols involved in large packet traces will be
extremely complicated, if feasible. This work is inspired from
Discoverer for clustering and merging application messages.
Rather than rely on the precision of tokenization and semantic
inferences, this work simplifies the process by clustering based
on similarity between constant parts (usually keywords) in the
– After initial clustering
∗ C1={S1,S2} with MCS = <Cookie:>
∗ C2={S3,S4} with MCS = <GET /20, HTTP/1.1>
∗ C3={S5,S6} with MCS = <GET /2., .1/build/,
HTTP/1.1>
∗ C4={S7,S8,S9}, with MCS = <GET /album, .php
HTTP/1.1>
– After merging
∗ new cluster C′1 from C1
∗ new cluster C′2 from C2, C3, and C4
Fig. 2. Result of initial clustering and merging.
• Merging: The initial clustering may generate small clus-
ters. In other words, similar application messages may be
scattered over multiple clusters. In the preceding example,
S3 · · ·S9 are all HTTP GET requests, and should be in
the same cluster. Merging can mitigate the problem.
Let mj be the string concatenated from the MCS in Cj .
The similarity between mi and mj is defined by
Similarity(mi,mj) =
2 ∗ |LCS(mi,mj)|
|mi|+ |mj | , (1)
where LCS(mi,mj) is the longest common substring
of mi and mj . Due to low similarity, this step does not
merge C1 and C2, but C2, C3 and C4 will be eventually
merged into a cluster because the MCSs between them
are similar. Figure 2 presents the merging result.
2) Alignment: The alignment algorithm is inspired by the
work in [16]. For a cluster Cj , we first arbitrarily choose an
application message in it, and move a sliding window starting
from the prefix of the message. The sliding window is a
substring to be tested with the other messages in the same
cluster, and its initial length is 3 characters.
Let wj be the substring in the sliding window. If wj is also
a substring of all the other messages in the same cluster, the
sliding window will be expanded one more character to find
a longer common substring. The sliding window will keep
expanded as such, until the substring in it is no longer a
common substring in the cluster. The common substring found
is therefore the longest in the iteration. The sliding window is
then shrunk to the original length of 3 characters, and shifted
to the next character, starting a new iteration to find the next
longest common substring. The common substrings found in
the iterations are constant parts of the messages (e.g., GET and
HTTP/1.1 in C ′2). The remaining substrings are the variant
parts, and are likely to be application fields.
We are aware of the work named Protocol Informatics
(PI) alignment in [15], which leverages the multiple sequence
alignment (MSA) algorithm in bioinformatics to find specific
fields and constant parts in the protocol messages. We do use
the PI alignment algorithm because the similarity calculation
in the first stage of PI is time-consuming.
C. Computing the measures of sensitivity
Since the degree of sensitivity of an application field is
usually subject, some objective measures of sensitivity are re-
quired to infer how sensitive a field may be. In this subsection,
we will introduce the measures that help to identify sensitive
information in the application messages. We first present how
the source IP addresses and application fields are extracted for
sensitivity analysis, and then explain the rationale behind the
measures to infer sensitivity.
1) Extracting source IP addresses and application fields:
Let f be an application field either derived by the protocol
parser or inferred by the procedure introduced in Section III-B.
Suppose the field f appears n times in the packet trace, and
the value of each instance is f1, f2,. . . , fn. We associate
each instance with its source IP address, s1,s2,. . . ,sn. The
association can be a hint of whether the value of a field
is common for multiple sources or not. We represent the
association with the instance fi as the pair (si, fi).
The pairs may be redundant if a source host sends an
application message for multiple times. For example, the value
in the User-agent field of the HTTP requests (denoting
the browser program) from the same user is very likely to
be identical. We do not want the redundancy affects the
sensitivity analysis, so if two pairs (si, fi) and (sj , fj) are
the same, only one pair is considered. We finally have the set
{(s1, f1), (s2, f2), ..., (sm, fm)} after the redundancy elimina-
tion, where m ≤ n. The set of distinct field values in the
instances is denoted as {v1, v2, . . . , vk}, where k ≤ m.
2) Inferring sensitivity of application fields: The analysis
uses entropy, diversity, and one-to-one mapping with the
source IP address as the measures to infer the sensitivity of
application fields. The implications of the measures to the
sensitivity are discussed below.
• Entropy: The entropy of a field f is defined as
−Σpi log pi/ log k, where pi=|vi| /m and |vi| denotes
the number of occurrences in which the value of f is
vi. Entropy implies the degree of unpredictability. If the
entropy is close to zero, it means a field’s value is nearly
predictable, and one can easily guess the value. It is
therefore ineffective to anonymize a predictable field;
otherwise, if a field’s value is unpredictable, leaving
them unanonymized means revealing the information that
is supposed to be unknown, and the analysis therefore
suggests to anonymize such a field.
• Diversity: We define the diversity of a field as k/m, where
k, as defined above, is the number of different values that
may appear in a field. If the possible values are few, it
will be easy to make a guess from the few values, and it
is ineffective to anonymize such a field.
• One-to-one mapping: Consider the mapping from the
source IP addresses to a field’s values in the above
association. If the mapping is one-to-one, only one source
IP address ever sends a certain field’s value, e.g., a
user’s account, implying that the field value is likely
to be personal information. Otherwise, multiple source
IP addresses can generate some value. In other words,
the field’s value is shared among multiple hosts and
known by several users. The measure computes the rate
of field values mapped from only one source IP address.
TABLE III
THE ACCURACY OF CLUSTERING WITH DIFFERENT THRESHOLDS.
similarity threshold precision recall F-measure
0.6 0.611 0.879 0.714
0.7 0.678 0.866 0.760
0.8 0.731 0.831 0.778
0.9 0.741 0.814 0.776
2) Discussions: Table III presents the accuracy of cluster-
ing. The threshold of 0.8 has better performance than the
others. Increasing the threshold will result in many small
clusters, whereas decreasing the threshold will increase the
chances of unnecessary merges, and can significantly influence
the precision value. The cluster sizes in the clustering can vary
wildly, ranging from only two messages to more than 100 in
large clusters. If the cluster size is too small after merging,
we prefer to anonymize the entire messages directly in the
clusters because they provide few hints for locating the fields
with the alignment; otherwise, we proceed to locate the fields.
The F-measure is limited by two factors. First, some
substrings in the application messages are rather short, and
also easily appear in the other messages, making the MCS
and merging computations sometimes inaccurate. Second, the
sorting prior to the clustering is based on the heuristic that the
messages of similar semantics will be ranked together after the
sorting, but the heuristic may be inaccurate if the variant parts
are in the first few bytes of the messages. For example, in
the following four messages, the first, the third, and the fourth
messages are of similar semantics, but they are separated after
the sorting because the variant parts (i.e., the domain names),
rather than the command, are in the first few bytes in the
messages.
221 2.0.0 cannoli.messagescreen.com closing connection
221 2.0.0 closing connection 14si3295638icg.127
221 2.0.0 fbmail003.snc1.facebook.com closing connection
221 2.0.0 fmgw.net.mexline.com closing connection
The solution is searching for similar messages throughout
the entire set of messages during the clustering, but the
clustering will become inefficient in the search process, and it
is a trade-off between efficiency and accuracy.
V. CONCLUSION
A network trace may include various protocols. These
protocols have different values and rich semantics result that
packet anonymization is a highly manual and time-consumed
work today. Our purpose is to mitigate this problem. We have
presented the design and effective of our automatic anonymiza-
tion and this work have not seen in the past. Our method is
divided to three main steps. In first step, we have developed
a clustering method for messages that parser cannot parse.
In second step, that we have developed a heuristic alignment
called MCS. In third step, we conduct three attributes to
describe a protocol field. The key of automatic inference is that
finding the variant part in messages with alignment and using
the attributes to know whether a field should be anonymized
or not. For checking the anonymized fields is really sensitive,
we manually examining values of each protocol field. We use
precision/recall to evaluate the effect of the anonymization that
achieves the best recall up to 96%. In the future, we plan to
look for more measures to further improve the accuracy of the
inferences.
REFERENCES
[1] J. Sommers and P. Barfold, “Self-configuring network traffic generation,”
In Proceedings of the 4th ACM SIGCOMM conference on Internet
measurement (IMC), Oct. 2004.
[2] Know Your Enemy: Passive Fingerprinting, Honeypot Project, Mar. 2002.
[3] R. Pang, M. Allman, V. Paxson and J. Lee, “The Devil and Packet Trace
Anonymization,” Computer Communication Review, 36(1), pp. 29-38,
Jan. 2006.
[4] R. Ramaswamy and T. Wolf, “High-speed Prefix-preserving IP Address
Anonymization for Passive Measurement Systems,” IEEE/ACM Trans.
Networking, 15(1), pp. 26-39, Feb. 2007.
[5] J. Xu, J. Fan, M. H. Ammar and S. B. Moon, “Prefix-preserving IP
Address Anonymization: Measurement-based Security Evaluation and a
New Cryptography-based Scheme,” In Proceedings of Intl. Conf. Network
Protocols (ICNP), Nov. 2002.
[6] Tcpanon, available at http://www.ing.unibs.it/ntw/tools/tcpanon.
[7] R. Pang and V. Paxson, “A High-level Programming Environment for
Packet Trace Anonymization and Transformation,” In Proceedings of
ACM SIGCOMM, Aug. 2003.
[8] D. Koukis, S. Antonato, D. Antoniades, E. P. Markatos and P. Trim-
intzios, “A Generic Anonymization Framework for Network Traffic,” In
Proceedings of IEEE Intl. Conf. on Communications (ICC), Jun. 2006.
[9] Y. D. Lin, I. W. Chen, P. C. Lin, C. S. Chen and C. H. Hsu, “On Campus
Beta Site: Architecture Designs, Operational Experience, and Top Product
Defects,” IEEE Communications Magazine, Vol. 48, Issue 12, Dec. 2010.
[10] V. E. Seeberg and S. Petrovic, “A New Classification Scheme for
Anonymization for Real Data Used in IDS Benchmarking,” In Proceed-
ings of the Intl. Conf. Availability, Reliability and Security (ARES), Apr.
2007.
[11] S. B. Needleman and C. D. Wunsch, “A General Method Applicable to
the Search for Similarities in the Amino Acid Sequence of Two Proteins,”
Journal of Molecular Biology, 48(3):443–453, 1970.
[12] W. Cui, V. Paxson, N. C. Weaver and R. H. Katz, “Protocol-Independent
Adaptive Replay of Application Dialog,” In Proc. of the 13th Annual
Network and Distributed System Security Symposium (NDSS), Feb 2006.
[13] W. Cui, J. Kannan and H. J. Wang, “Discoverer: Automatic Protocol
Reverse Engineering from Network Traces,” In Proc. USENIX Security
Symp., Aug 2007.
[14] J. Caballero, H. Yin, Z. Liang and D. Song, “Polyglot: Automatic
Extraction of Protocol Message Format using Dynamic Binary Analysis,”
In ACM CCS, Oct.-Nov. 2007.
[15] M. A. Beddoe, “Network Protocol Analysis Using Bioinformatics Al-
gorithms,” http://www.baselinearserach.net/PI.
[16] A. Pitsillidis, K. Levchenko, C. Kreibich, C. Kanich, G. Voelker, V.
Paxson, N. Weaver and S. Savage, “Botnet Judo: Fighting Spam with
Itself.” Proc. of the 17th Annual Network and Distributed System Security
Symposium (NDSS), Feb. 2010.
[17] P. Li, L. Lu, D. Gao and M. Reiter, “On challenges in evaluating malware
clustering,” In Proceedings of the International Symposium on Recent
Advances in Intrusion Detection (RAID), Sept. 2010.
100年度專題研究計畫研究成果彙整表 
計畫主持人：林柏青 計畫編號：100-2221-E-194-029- 
計畫名稱：雲端郵件服務上 Botnet 垃圾郵件之特徵擷取 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 1 0 0% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 3 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 1 0% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
