 ii
Contents 
 
ABSTRACT.................................................................................................................. I 
CONTENTS.................................................................................................................II 
LIST OF FIGURES .................................................................................................. VI 
LIST OF TABLES..................................................................................................... XI 
CHAPTER 1  INTRODUCTION..............................................................................1 
1.1  INTRODUCTION....................................................................................................1 
1.2  LITERATURE REVIEW...........................................................................................3 
1.3  MOTIVATION AND OBJECTIVE ..............................................................................9 
1.4  CONTRIBUTIONS OF THE REPORT.......................................................................15 
1.5  ORGANIZATION OF THE REPORT.........................................................................16 
CHAPTER 2  SYSTEM STRUCTURE AND CONTROL ARCHITECTURE..17 
2.1  INTRODUCTION..................................................................................................17 
2.2  SYSTEM DESCRIPTION OF THE TOUR-GUIDE ROBOT ..........................................21 
2.2.1  Description of the Laser Scanning Module Subsystem.............................23 
2.2.2  Communication Protocol and Interfacing Setup of the Laser Scanner ....24 
2.2.3  Description of the ultrasonic Subsystem...................................................27 
2.2.4  Voltage Indicator Circuitry.......................................................................30 
2.3  BASIC STRUCTURE OF THE FOUR-WHEELED OMNIDIRECTIONAL MOBILE 
PLATFORM.................................................................................................................32 
2.3.1  DC Servomotor Drive ...............................................................................34 
2.3.2  Odometer Development ............................................................................35 
2.3.3  DAC Card .................................................................................................38 
 iv
4.5.2  Regulation.................................................................................................98 
4.5.3  Straight-Line Trajectory Tracking ..........................................................100 
4.5.4  Circular Trajectory Tracking..................................................................102 
4.5.5  Elliptic Trajectory Tracking....................................................................104 
4.5.6  Cardioids trajectory Tracking.................................................................105 
4.5.7  Experimental Results and Discussion.....................................................107 
4.6  CONCLUDING REMARKS..................................................................................112 
CHAPTER 5  ZIGBEE LOCALIZATION SUBSYSTEM................................. 113 
5.1  INTRODUCTION................................................................................................113 
5.2  MESH NETWORKS ...........................................................................................115 
5.3  LOCALIZATION ALGORITHM ............................................................................117 
5.3.1  Global Localization Method Using RSSI................................................ 117 
5.3.2  Calibration of the ZigBee System ........................................................... 118 
5.3.3  Robot Position Estimation Using Least Square Method ........................ 119 
5.3.4  Robot Orientation Estimation.................................................................121 
5.3.5  Real-Time ZigBee Global Pose Initialization Algorithm........................122 
5.3.6  Experiment Results of Global Pose Initialization...................................123 
5.4  SIGNAL FUSION ...............................................................................................126 
CHAPTER 6  AUTONOMOUS NAVIGATION .................................................129 
6.1  INTRODUCTION................................................................................................129 
6.2  DYNAMIC PATH PLANNING ..............................................................................130 
6.3  OBSTACLE AVOIDANCE BEHAVIOR USING LASER SCANNER AND SONAR.........133 
6.4  HYBRID NAVIGATION DESIGN..........................................................................137 
6.5  AUTONOMOUS ROBOT CONTROL VIA PETRI-NET ............................................142 
6.6  EXPERIMENTAL RESULTS AND DISCUSSION .....................................................144 
 vi
List of Figures 
 
Figure 1.1. Photograph of the enon................................................................................4 
Figure 1.2. Photograph of mobile robot named SeQ-1..................................................6 
Figure 1.3. Three entertainment robots developed by IPA. ...........................................6 
Figure 1.4. RoboX robot. ...............................................................................................7 
Figure 1.5. Jinny robot. ..................................................................................................7 
Figure 1.6. Aichi the reception-tour guide robots called Wakamaru .............................8 
Figure 1.7. NSTM robots. ..............................................................................................8 
Figure 1.8. Photograph of tour-guide robot named UPITOR. .......................................9 
Figure 1.9. The general control structure of robots. ....................................................11 
Figure 2.1. Tour-Guide Robot Generation 1. ...............................................................19 
Figure 2.2. Tour-Guide Robot Generation 2. ...............................................................19 
Figure 2.3. Improved Tour-Guide Robot Generation 2................................................20 
Figure 2.4. Tour-Guide Robot Generation 3 ................................................................20 
Figure 2.5. Block diagram of the overall system structure. .........................................21 
Figure 2.6. The real physical structure of the tour-guide robot. ..................................22 
Figure 2.7. Picture of the laser scanner. .......................................................................23 
Figure 2.8 Measurement methods of LMS. .................................................................24 
Figure 2.9 Direction of transmission for LMS 291-S05. .............................................24 
Figure 2.10. Flow chart of operating principle of the LMS 291-S05. .........................27 
Figure 2.11. SRF05 Ultrasonic Ranger modules .........................................................28 
Figure 2.12. The connection methods of SRF05 .........................................................28 
Figure 2.13. SRF05 timing diagrams...........................................................................29 
Figure 2.14. Nios development board, Stratix II edition. ............................................29 
Figure 2.15. Diagram of the Nios development board.................................................30 
 viii
Figure 3.9. Relationship between local coordinate and global coordinate ..................60 
Figure 3.10. The moving structure of wheels 1, 2, 3 and 4. ........................................61 
Figure 3.11. Experiment trajectories of the proposed kinematic controller for 
achieving point-to-point stabilization. .........................................................................71 
Figure 3.12. (a) Experiment straight-line trajectory tracking start point. (b~e)The 
situation for the mobile robot moving toward the straight line. (f) The overall tracking 
result.............................................................................................................................73 
Figure 3.13. The time historical pictures of tracking the straight line trajectory.........74 
Figure.3.14. (a) The mobile robot in the initial position. (b~i) Experimental result of 
the circular trajectory tracking. (j) The mobile robot in the end position....................77 
Figure 3.15. The time historical pictures of the circular trajectory tracking. ..............78 
Figure.3.16. (a) The mobile robot in the initial position. (b~i) Experimental result of 
the random trajectory tracking. (h) The mobile robot in the end position. ..................81 
Figure 3.17 The time historical pictures of the random trajectory tracking. ...............81 
Figure 4.1. Block diagram of the motion control system. ...........................................84 
Figure 4.2. Geometry of the omni-directional robot with simplified top-view. ..........85 
Figure 4.3. Experimental omnidirectional four-wheeled mobile platform: (a) 
bottom-view; (b) side-view. .........................................................................................91 
Figure 4.4. Simulink model of the adaptive dynamic motion controller: (a) main block 
diagram; (b) subsystem block diagram. .......................................................................97 
Figure 4.5. (a) Point to point stabilization result. (b) Time history of the vehicle 
orientation. .................................................................................................................100 
Figure 4.6. (a) Straight-line trajectory tracking result.  (b) Line trajectory tracking 
errors of the platform. ................................................................................................101 
Figure 4.7. (a) Simulation result of the circular trajectory tracking. (b) The errors of 
the circular trajectory tracking. ..................................................................................103 
 x
Figure 6.8. Block diagram of the tour-guide robot hybrid navigation.......................138 
Figure 6.9. The method of two behaviors selection...................................................140 
Figure 6.10. Petri-net model of executing autonomous navigation. ..........................141 
Figure 6.11. Hybrid navigation experiment result. (a) The mobile robot stop at the 
initial point (b) The mobile robot executed the autonomous navigation behavior (c) 
The mobile robot moved to the destination and played sound thank for use. ...........146 
Figure 7.1. Flow chart of the operation scenario with human-robot interactions......148 
Figure 7.2. Photograph of the facial expression system. ...........................................150 
Figure 7.3. Four kinds of facial expressions. (a) Wake-up state. (b) Start-to-move state. 
(c) Explanation state. (d) Sleep state..........................................................................151 
Figure 7.4. Control circuitry of arms swing angle and LED bar................................152 
Figure 7.5. Swing behavior of the dual arms. (a)(b) Swing of the dual arms during 
navigation. (c) Two arms at the stop condition..........................................................153 
Figure 7.6. Experimental pictures of the tour-guided robot operation scenario. .......156 
 
  
 
1
Chapter 1  Introduction 
 
1.1  Introduction 
Recently, much research has paid to considerable interests in robots which can 
help people in their daily life. Robots are growing in complexity and their use in 
industry is becoming popular. Industrial robots can be manufactured in a wide range 
of sizes and so can handle more tasks requiring heavy lifting than human does. They 
are also useful in dangerous environments for humans to work in, for example, bomb 
disposal, work in space or underwater, in mining, and for the cleaning of toxic waste. 
On the other hand, there are more and more non-industry applications [1, 2] in public 
spaces or in the living houses; they may be entertainment robots, home-care robots, 
nursing robots, recreational robot, hobby robots, medical robots, security robots, 
special-purposed service robots and etc. Nowadays, several mobile robots used for the 
museum have been successfully installed to provide with the designed missions. Two 
of the missions of the tour-guide robot are to guide visitors to see exhibitions around 
the museums, and to provide the knowledge or the information of exhibitions or 
answer the question to people. With the help of tour guide robots, museums or 
exhibition rooms could reduce the labor power resources and attract the visitor, 
thereby improving their education purposes. 
  The research on tour guide robots in America, Japan, Germany and Korea has 
already been significantly growing and fast flourishing in order to provide them with 
some timely innovative functions, features and appearances. From all the worldwide 
tour guide robots, there are two critical research issues: how to safely navigation in 
dynamic and crowded public environments and how to design effective and 
interesting human-robot interactions between the visitor and the robot. In order to 
achieve reliable and safe navigation in large museum environments, a complete 
  
 
3
the visitors are able to remotely steer the tour guide robot to visit museum exhibitions 
and obtain desired vivid images at the very visiting moment.  
To date, tour guide robots, one important branch of intelligent service robots, 
have become ever prevalent at various museums or other important commercial 
public places. At World Expo 2005 Aichi the reception-tour guide robots, called 
Actroid [5] and Wakamaru [6], successfully demonstrated the significance and 
application of tour guide robots using four languages to welcome visitors and help 
them without human intervention. In addition to constructing system configuration of 
a tour guide robot, the report will go further to study its two crucial problems: safe 
navigation in dynamic and crowded public environments, and effective and 
interesting human-robot interactions between the visitor and the robot, in order to 
attract attention of visitors and help them enjoy learning and education during 
visiting. 
  
1.2  Literature Review 
Up to now, some of famous tour-guide robots have been proposed by several 
researchers and companies. In the following, we will review some international and 
national tour guide robots.  
  Two famous companies, Fujitsu Frontech and Fujitsu Laboratories, began limited 
sales of their new service robot, named enon [7] , on a limited basis in Japan from 
September 13, 2005. Figure 1.1 shows the photograph of the robot enon. As a fully 
developed practical-use service robot, enon posses features enhancements such as 
lighter weight, smaller size, and more safety features than the prototype that Fujitsu 
developed last year. The robot, enon, is an advanced service robot capable of 
accomplishing multiple tasks such as providing guidance, escorting people,  
  
 
5
 2. Transport of objects 
enon can carry parcels in an internal storage compartment using its torso and 
deliver them to a designated location. Through network interconnection, users can call 
for enon to come from a remote location and have goods delivered to a specified 
designation.  
3. Security patrolling 
enon is capable of regularly patrolling facilities following a pre-set route, and by 
using a network has the ability to transmit images of stipulated locations to a remote 
surveillance station. enon can also respond flexibly or to users' spontaneous requests 
through a network, such as directing enon to view specific sites. 
  As depicted in Figure 1.2, SeQ-1 [4] is a tour guide and security robot 
concurrently developed by Industrial Technology Research Institute and Shin Kong 
Security Company. The robot’s name means the security number one. SeQ-1 eyes 
are equipped with bright LEDs; when running into a dark region the robot will open 
LED lights automatically. SeQ-1 equipped with the 360 degree vision. When 
accidents occur in the building, SeQ-1 would zoom and transmit the screen signal to 
the control staffs. About the autonomous system, SeQ-1 can build the environment 
map, plan the dynamic path in the space and automatic avoid the obstacles.  SeQ-1 
also can send vivid images to the main controller and send out a warning signal. 
SeQ-1 at night carries out patrolling task and at daytime can provide tour guide or 
reception for the customer and the visitor. 
  
 
7
      
Figure 1.4. RoboX robot. 
 
    
Figure 1.5. Jinny robot. 
  
 
9
 
 
Figure 1.8. Photograph of tour-guide robot named UPITOR. 
 
1.3  Motivation and Objective 
    Although several tour-guide mobile robots have been developed in the research 
organizations or companies, most of the mobile robots have not yet operated in the 
human society. It is a trend to the future that the use of a great quantity of intelligent 
mobile robots helps people work in their daily life. This kind of robot is usually 
equipped with multiple sensors like laser scanner, ultrasonic sensors and etc. In our 
laboratory, the design and implementation of the tour-guide robot has been reported in 
[8], and autonomous navigation and interactive operation of the tour-guide robot has 
been developed in [9]. However, the robots in [8] and [10] suffered from 
nonhononomic mobility and maneuver so that they were not operated easily and 
  
 
11
omnidirectional mobile platform incorporating with the dynamic effect and 
uncertainties. 
    Although the adaptive dynamic motion controller can be realized by a 
small-scale personal computer, this kind of controller is too bulky, expansive and of 
high power consumption. To circumvent this shortcoming, the SoPC technology will 
be employed to implement such a controller in real time so as to meet industrial and 
commercial requirements. 
 
 
 
Figure 1.9. The general control structure of robots. 
 
Generally speaking, the general control structure of the tour-guide robots, which have 
been regarded as one of autonomous wheeled mobile robots, includes four main 
modules: sensing and perception, localization and mapping, cognition and planning, 
and motion control, as proposed in [11] in some detail. Figure 1.9 depicts the general 
control structure. Although the four main modules may be included in any kind of 
tour-guide robot, this report is devoted to investigating some important issues for 
localization and motion control. 
  
 
13
Based on the results from the experiments Pinedo-Frausto et al. [14] have performed, 
and from other aspects they have analyzed, they can situate ZigBee as a protocol well 
suited for applications in classes 3 to 5 according to the usage classes defined by ISA. 
But it would not be adequate for emergency applications or for closed loop control 
applications.  Sottile et al. [15] proposed a complete system for nodes localization in 
a Wireless Sensor Network (WSN) based on the ZigBee standard. The system 
includes a real-time location engine, which adopts a Received Signal Strength 
Indicator (RSSI)-based localization algorithm, and three tools, namely an 
Environment Description Tool (EDT), a Channel Modeling Tool (CMT) and a 
Network Planning Tool (NPT), which enable efficient deployment and accurate 
operation. 
Omnidirectional mobile robots can achieve 3 DOF motion on a two-dimensional 
flat terrain due to the unique feature that such robots can move in an arbitrary 
direction without changing the direction of the wheels. To date, a variety of 
omnidirectional mobile robots have been proposed by several researchers; some 
popular examples among them are universal wheels [16-17], ball wheels [18] and 
off-centered wheels [19] The omnidirectional mobile robots using omnidirectional 
wheels usually have 3 or 4 wheels. The three-wheeled omnidirectional mobile robots 
can be designed by driving 3 independent actuators [20-21], but they may have 
stability problem due to the triangular contact area with the ground, especially when 
traveling on a ramp with the high center of gravity owing to the payload they carry. It 
is desirable, therefore, that four-wheeled vehicles be used when stability is of great 
concern [22]. However, independent drive of four wheels creates one extra DOF.  
Four-wheeled omnidirectional mobile robots have been investigated as well. Muir and 
Neuman [22] constructed a four-wheeled omnidirectional robot with Mecanum 
wheels, called Uranus, but they did not propose its dynamic controller. Byun et al. 
  
 
15
FPGA-based designs [34-36]. With the advantages of SoPC, these kind of embedded 
system designs [37] are becoming more popular not only in robotics studies but also 
in many disciplines. 
 
1.4  Contributions of the Report 
    This report aims to develop methodologies and techniques for control and 
navigation of the four-wheeled motion base control, and tour-guide robot executing 
tasks in an indoor environment. The methods of how to construct the omnidirectional 
platform with a shock resistant system and how to communicate with the motor driver 
are also presented in the report. Therefore, a great effort has been paid on improving 
the functions of the omnidirectional robot, and making it more useful in the museum. 
The main contributions of the robot are fourfold. 
1. A overall system structure and control architecture of the tour-guide robot are 
described in detail. In particularly, an FPGA-based platform controller is 
developed and implemented to replace an expansive IPC-based motion controller.. 
2. The kinematical and dynamic models for the four-wheeled omnidirectional base 
are derived. With the model, a nonlinear kinematics control law and an adaptive 
dynamic control law are presented to achieve point-to-point stabilization and 
trajectory tracking.  
3. The proposed point-to-point stabilization law and the proposed obstacle avoidance 
behavior are combined together to accomplish autonomous navigation.        
4. The human-robot interaction functions are constructed with a user friendly 
interface. 
5. The ZigBee localization module is applied to the tour-guide robot instead of the 
previous RFID localization module, and a complete localization module is 
proposed by fusing the ZigBee location module and a dead-reckoning unit. 
  
 
17
Chapter 2  System Structure and Control 
Architecture 
 
2.1  Introduction 
  This chapter is aimed to describe the system structure and control architecture of an 
omnidirectional mobile tour-guide robot. The beginning of this chapter, mentions the 
histories of the tour-guide mobile robot developed in our laboratory, AECL. Figures 
2.1, 2.2, 2.3 and 2.4 show its evolution from the first-generation to the 
third-generation. The first-generation of the omnidirectional mobile robot in AECL 
lab was established in [8], in which the physical differential mobile platform was 
equipped with a differential-driving mechanism and three sensors: laser scanner, 
ultrasonic arrays, and infrared ray sensors. Several experiments were conducted to 
show the feasibility and efficacy of the proposed methods and techniques for the 
first-generation tour-guide robot. To solve the problem of the mobile robot 
localization and human robot interaction, Wang [9] considered methodologies and 
techniques for navigation and human-robot interaction of a tour-guide robot with an 
improved system configuration which was regarded as the second generation of the 
tour-guide robot. In the second-generation mobile robot, a least-squares algorithm was 
adopted to solve the RFID localization problem. Thanks to the robot contest held by 
PMC (Precision Machinery Research & Development Centre) on 27 October 2007, 
we improved the tour-guide robot in second generation about several subsystems, but 
this competition robot put emphasis on the stability issue of the communication 
system, the appearance design, and the entertainment performance including voice, 
light and facial expression. Some operation scenario with human-robot interactions 
was presented to show the applicability of the companion robot.  
  
 
19
  
(a)                        (b) 
Figure 2.1. Tour-Guide Robot Generation 1. 
  
(a)                         (b) 
Figure 2.2. Tour-Guide Robot Generation 2. 
  
 
21
 
 
Figure 2.5. Block diagram of the overall system structure. 
 
 
2.2  System Description of the Tour-guide Robot 
The physical configuration of the tour-guide robot is shown in Figure 2.4. Such a 
robot is equipped with an embedded personal computer, a RFID localization system, a 
laser scanner, a MAX II edition CPLD development board, a Stratix II edition Nios 
development board, a human robot interaction system, a ZigBee localization module, 
a SoPC-based adaptive motion controller incorporating with an embedded processor 
and an omnidirectional mobile platform. The radius of the mobile robot is 25 cm and 
  
 
23
 
 
Figure 2.7. Picture of the laser scanner. 
 
2.2.1  Description of the Laser Scanning Module Subsystem  
This subsection introduces the laser measurement module. The LMS 291-S05 
manufactured by SICK Electro-Optics is used to acquire the surrounding environment 
information. Figure 2.7 shows the SICK laser scanner. This laser device can be 
programmed to scan for a 100º or 180º fields of view with the resolution of 0.25º, 
0.50º and 1º. The laser scanner LMS 291 has a maximum scanning range of 80m, and 
the output data can be programmed to output at 9.6, 19.2, 38.4, or 500 kbits per 
second.  
This laser system is based on the time-of- flight (TOF) measurement principle, as 
shown in Figure 2.8.  A single pulses laser beam is sent out and reflected by an 
object surface within the scanning range of the scanner. The laser pulses sweep a 
radial range in front of the scanner platform via an integrated rotation mirror. The time 
between emanation and reception of the laser pulse is used to compute the distance 
between arbitrary object and the laser scanner. Figure 2.9 displays the 
two-dimensional measurement field and detection range. The measurement data is 
available in real-time for further evaluation via a serial interface (RS-232 or RS-422). 
  
 
25
drive. According to the LMS 291-S05 telegram structure, the user can design some 
communicated language with the LMS. The Borland C++ Builder programming 
language is used to write the codes for the com port communication with the support 
of communication VCL. 
Next, the RS-232 communication code and the laser system basic configuration 
will be mentioned. Tables 2.1 and 2.2 show the request send data between LMS and 
PC.  
Table 2.1. Commands and responses for initializing the LMS. 
Status 
Telegram code in hex 
(PC → LMS) 
Reply telegram in hex  
(PC → LMS) 
Password for initialization mode 
02 00 0A 00 20 00 53 
49 43 4B 5F 4C 4D 
53 BE C5 
06 02 80 03 00 A0 00 10 16 
0A 
baud rate   
9600 
02 00 02 00 20 42 52 
08 
19200 
02 00 02 00 20 41 51 
08 
38400 
02 00 02 00 20 40 50 
08 
LMS 
baud rate 
setting 
500K 
02 00 02 00 20 48 58 
08 
 06 02 80 03 00 A0 00 10 16 
0A 
Angular 
range 
Angular 
resolution 
  
1 o 
02 00 05 00 3B 64 00 
64 00 1D 0F 
06 02 81 07 00 BB 01 64 00 
64 00 10 4A 3F 
0.5 o 
02 00 05 00 3B 64 00 
32 00 B1 59 
06 02 81 07 00 BB 01 64 00 
32 00 10 12 92 
0 100D D∼  
0.25 o 
02 00 05 00 3B 64 00 
19 00 E7 72 
06 02 81 07 00 BB 01 64 00 
19 00 10 BE C4 
1 o 
02 00 05 00 3B B4 00 
64 00 97 49 
06 02 81 07 00 BB 01 B4 00 
64 00 10 5E B2 
LMS 
resolutio
n modes 
0 180D D∼  
0.5 o 
02 00 05 00 3B B4 00 
32 00 3B 1F 
06 02 81 07 00 BB 01 B4 00 
32 00 10 06 1F 
  
 
27
Start Prepare data structure for LMS data capture
Start continuous LMS data output
Set specific command to set measurement mode
Stop continuous LMS data output
Does the header be 
found?
Yes
No
Timer (every 0.5 second)
Store the LMS data structure into buffer
Evaluate the number of data bytes
End
Does the End header be 
found?
Detect the header from the incoming data 
stream
  
Figure 2.10. Flow chart of operating principle of the LMS 291-S05. 
 
2.2.3  Description of the ultrasonic Subsystem 
In the navigation system, the obstacle detection is a key issue. Although the 
mobile robot has the laser scanner to detect the front space, the ultrasonic ranger 
modules are used to detect the obstacles and environment data around 360 degrees. 
The ultrasonic ranger module is shown in Figure 2.11. Figure 2.12 shows the 
connection methods for the ultrasonic ranger module SRF05. The reasons why we use  
  
 
29
 
Figure 2.13. SRF05 timing diagrams 
 
Figure 2.14. Nios development board, Stratix II edition. 
 
The timing diagrams of the SRF05 are shown in Figure 2.12.  To start the ranging, 
one needs to supply a short 10uS pulse to the trigger input. The SRF05 will send out 
an 8-cycle burst of ultrasound at 40kHz and raise its echo line high. It then listens for 
an echo, and as soon as it detects one it lowers the echo line again. The echo line is 
therefore a pulse whose width is proportional to the distance to the object. By timing 
the pulse it is possible to calculate the range in inches/centimeters or anything else. If 
nothing is detected then the SRF05 will lower its echo line anyway after about 30mS. 
As shown in Figure 2.14, the Nios development board, Stratix II Edition, 
provides a hardware platform in developing any embedded system based on Altera 
Stratix II EP2S60 device. Figure 2.15 displays all the important components on the 
Nios development board.  
  
 
31
 
(a) 
 
                    (b) 
Figure 2.16. (a) Photograph of physical battery indicator. (b)Voltage detection circuit. 
The main component of the voltage detection circuit is LM 3914 IC. The 
LM3914 is a chip made specifically for driving LEDs based on the present voltage of 
the batteries, capable of displaying the energy information in a bar-graph mode or a 
dot mode. The voltage settings for the LM3914 IC has two extreme values: the upper 
bound of the voltage is set as 26V DC from two serial batteries and the lower bound is 
  
 
33
 
Figure 2.17. Bottom view of the four-wheeled omnidirectional motion base  
 
 
Figure 2.18. The drive circuits of the DC brushless motors.  
  
 
35
  
              (a)AXHD-50K                   (b)AXHD450K-10 
Figure 2.20. Pictures of the DC brushless servomotor and its control kit. 
  
Figure 2.21. Signal connection and speed control characteristics of the servomotor. 
 
2.3.2  Odometer Development 
   The position of the motors can be determined by photo encoders mounted on 
motors. FPGA-based development board MAX II is used to construct such an 
odometer. When the motor outputs 300 pulses means the wheel turns one revolution. 
Since the diameter of the omnidirectional wheel is 10.16 cm, the encoder will output a 
pulse if the wheel moves 0.106395 cm. Such an odometer made by the counter board 
can be used to obtain the posture data, velocity and distance of mobile vehicle, and  
  
 
37  
  
 
39
 
Figure 2.25. Photograph of the built D/A card. 
 
The D/A board offers four channels with 8-bit analog output. The performance of this 
board is acceptable because the output is very stable without any noise. The output 
range can be limited within 0~1.5 VDC voltage output, and supply 4~20mA current 
from dual operational amplifiers. The innovative design improves several drawbacks 
of the conventional D/A boards. In our board, we will use the specific range of 
0~1.5V because the robot in the crowd environment cannot move too fast. Figure.2.25 
shows the photograph of the D/A card, and Figure 2.26 presents the circuit inside the 
chip (DAC0800). Figure 2.27 depicts the typical application connection to the other 
components while Figure 2.28 shows the all the wires connect by the PCB developed 
program (Protel).  
  
 
41
 
Figure 2.28. The wire connection schematics of DAC card.   
 
2.4  Description of the Suspension Structure for the Mobile Platform   
  An omni-directional moving platform with four wheels is developed and its 
controller is designed for tracking desired trajectories. The four wheels of the 
  
 
43
  
(a)                     (b) 
 
(c) 
 
(d) 
  
 
45
 
(k) 
 
(l) 
Figure 2.29. Pictures of the all mobile platform components. 
 
2.5  Remote Control  
As shown in Figure 2.30, the ASUS WL-330g wireless access point and 
WL-167g USB2.0 wireless LAN adapter are used for remote control in HRI system. 
The compact WL-330g is not only a wireless access point (AP) but also a wireless  
  
 
47
 
Figure 2.31. Photograph of the control interface on the wireless control computer. 
 
in Figure 2.31, terminal central computer uses ASUS wireless device to send out 
control commands to the mobile robot via TCP/IP network. These data packet are then 
transmitted to the navigation PC which is responsible for controlling the brushless 
servomotors. The Borland C++ Builder programming language is used to write the 
codes for the wireless communication with the support of communication VCL.  
 
2.6  Concluding Remarks   
  This chapter has described the overall system structure and control architecture of 
the tour-guide robot. The method of how to construct mobile robot motion base, 
including the shock resistance mechanism, DAC circuit card, FPGA MAXII odometer 
and transmission interface to the DC motor driver. In this chapter, we also introduced 
the key components of the navigation system. The sensors comprise one laser 
scanning sensor, one ultrasonic ranging module for accomplishing autonomous 
  
 
49
Chapter 3  Kinematic Control of the Four-Wheeled 
Omnidirectional Mobile Platform  
3.1  Introduction 
Recently, omnidirectional mobile robots have attracted much attention in the 
robotics and control systems societies. Such robots have been extensively used for the 
well-known world cup soccer robots competition in the RoboCup association, in order 
to have agile movements. Moreover, this type of robot has also been shown very 
suitable for material handling in manufacturing factories. Unlike conventional 
two-wheeled or four-wheeled (car-like) mobile robots, the omnidirectional mobile 
mechanism has the superior agile capability to move towards any position and to 
simultaneously attain any desired orientation. In addition to the annual RoboCup 
competition, this kind of maneuvering capability is also particularly useful in 
designing several service robots of interest, such as home-care robots, medical robots, 
nursing-care robots, mobile manipulators and etc.  
Omnidirectional mobile robots have been investigated by several researchers. 
Muir and Neuman [22] constructed a four-wheeled omnidirectional robot with 
Mecanum wheels, called Uranus, and solved its kinematic equations of motion to 
calculate the actuated inverse and sensed forward solutions; furthermore, they applied 
the kinematic model to dead reckoning, wheel slip detection and feedback control 
algorithm design. Béktourné and Campion [41] established kinematic models of a 
class of omnidirectional mobile robots equipped with centered orientable wheels. 
Watanabe [42] described several control approaches, such as resolved acceleration 
control method, PID method, fuzzy model method, and stochastic fuzzy servo method, 
for a holonomic and omnidirectional mobile robot with four lateral orthogonal-wheel 
assemblies. Yi and Kim [43] introduced two different kinematic approaches for 
  
 
51
degrees from one to another. Such controllers are adequate while the robot navigates 
over its working environment at slow speeds. The chapter is written in two principal 
contributions; one is that the continuous position and orientation of the robot over 
short traveling distances can be directly obtained from the proposed dead-reckoning 
approach, and the other is that the proposed controllers are proven globally 
asymptotically stable via the Lyapunov stability theory. Moreover, the proposed 
kinematics controllers along with the dead-reckoning method are verified by means of 
an experimental omnidirectional mobile robot.  
The rest of this chapter is organized as follows. Section 3.2 introduces the 
kinematic model of the omnidirectional mobile robot, and proposes the 
dead-reckoning approach to continuously keeping trace of the robot’s poses. Section 
3.3 elucidates how to design kinematics-based nonlinear controllers via feedback 
linearization to achieve point stabilization and trajectory tracking. Section 3.4 presents 
experiment results that are used for illustration of the effectiveness of the proposed 
control methods. Section 3.5 concludes the chapter. 
 
3.2  Kinematic Model and Dead-Reckoning 
This section is devoted to briefly introducing the kinematic model of the 
omnidirectional mobile robot, and developing the dead-reckoning method for the 
robot based on the kinematic model. When the robot navigates along its planned path 
at slow speeds (less than 100 cm/sec.), the kinematic model is valid and particular 
useful for finding the present pose of the robot with respect to a reference frame, or 
synthesizing regulators and controllers to steer the robot to achieve its scheduled 
missions. Furthermore, the accuracy of the proposed dead-reckoning method is 
investigated as well. 
 
  
 
53
δ
θ
1T
2T
3T
4T
Xω
Yω
mx
my
 
Figure 3.2. Structure and geometry of the omnidirectional driving configuration. 
 
The four wheels omnidirectional mobile base and the velocity of each wheel are 
shown in Figure 3.3. The motor modules including omnidirectional wheels are 
organized at 90 degrees from one to another. In this section, the local coordinate of 
the omnidirectional mobile robot are defined by mx and my , δ  in the Figure 3.2 is 
45 degrees; 1v  is the velocity of  wheel one, 2v  is the velocity of wheel two, 3v  
is the velocity of wheel three, and 4v  is the velocity of wheel four.   
  
 
55
iv
L
.
iθ
 
Figure 3.4. Diagram of the velocity display.  
 
When mobile robot moves at a velocity mv  and an angular velocity φ   in one 
assigned direction in the global coordinate, mv  can be taken apart in the local 
coordinate as a projection in mx  and my  . Thus, mv  in mx  and my  is shown as 
mx  and my . Next, consider the kinematic model of wheels 1, 2, 3 and 4. Figure 3.5 
shows the moving diagram of the first wheel. Let the velocity 1v  of the first wheel 
approach the velocity mv  and move to the assigned direction. The first wheel will 
provide 1v  which equals the sum of mx , my  on the local coordinate and angular 
velocity is 
.
Lφ . Therefore, 1v  can be rewritten as follows: 
1 sin( ) cos( )m mv x y Lδ δ φ= − + +                  (3.2) 
  
 
57
my
mv
mx
.
mx
.
my
δ
2v
  
Figure 3.6. Moving diagram of the second wheel.  
 
Figure 3.7 shows the third wheel moving diagram. Let the velocity 3v  of third wheel 
approaches the velocity mv  and move to the assigned direction. The wheel three will 
provide 3v  which equals to the sum of mx , 
.
my  on the local coordinate and the 
angular velocity 
.
Lφ . Therefore, 3v  can be rewritten as follows: 
. . .
3 sin( ) cos( )m mv x y Lδ δ φ= − +                  (3.4) 
 
  
 
59
my
mv
mx
mx
i
my
i
δ
4v
 
Figure 3.8. Moving diagram of the fourth wheel.  
According to the inverse kinematic of Jacobin matrix, one rewrites 1 2 3 4, , ,v v v v  in a 
matrix form. 
1 1
2 2
3 3
4 4
( ) ( )
( )
( ) ( )
( ) ( ( )) ( )
( ) ( )
( )
( ) ( )
t r t
x t
t r t
t P t y t
t r t
t
t r t
υ ω
υ ωυ φυ ω φυ ω
⎡ ⎤ ⎡ ⎤ ⎡ ⎤⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥= = = ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎣ ⎦⎣ ⎦ ⎣ ⎦


            (3.6) 
where  
s i n ( ) c o s ( )
c o s ( ) s i n ( )
( ( ) )
s i n ( ) c o s ( )
c o s ( ) s i n ( )
L
L
P t
L
L
δ δ
δ δφ δ δ
δ δ
−⎡ ⎤⎢ ⎥− −⎢ ⎥= ⎢ ⎥−⎢ ⎥⎣ ⎦
               (3.7) 
Notice that although the matrix ( ( ))P tφ  is singular for anyθ , but its left inverse 
matrix can be found, i.e., # 3( ( )) ( ( ))P t P t Iφ φ = , and expressed by  
  
 
61
mx
my
δ
1v
φ
φ
wx
wy
mx
my
δ
φ
φ
wx
wy
2v  
(a)                                (b) 
3v
mx
my
δ
φ
φ
wx
wy
 
mx
my
δ
φ
φ
wx
wy
4v
 
(c)                                 (d) 
Figure 3.10. The moving structure of wheels 1, 2, 3 and 4.  
 
1 1
2 2
3 3
4 4
( ) ( )
( )
( ) ( )
( ) ( ( )) ( )
( ) ( )
( )
( ) ( )
t r t
x t
t r t
t P t y t
t r t
t
t r t
υ ω
υ ωυ φυ ω φυ ω
⎡ ⎤ ⎡ ⎤ ⎡ ⎤⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥= = = ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎣ ⎦⎣ ⎦ ⎣ ⎦


          (3.9) 
where  
s i n ( ) c o s ( )
c o s ( ) s i n ( )
( ( ) )
s i n ( ) c o s ( )
c o s ( ) s i n ( )
L
L
P t
L
L
δ θ δ θ
δ θ δ θφ δ θ δ θ
δ θ δ θ
− + +⎡ ⎤⎢ ⎥− + − +⎢ ⎥= ⎢ ⎥+ − +⎢ ⎥+ +⎣ ⎦
       (3.10) 
  
 
63
sin( ) cos cos cos sin
cos( ) cos cos sin sin
a b a b a b
a b a b a b
+ = +
+ = −                          (3.16) 
 
3.2.2  Dead-Reckoning   
 The purpose of the dead-reckoning of the omnidirectional mobile robot is, given 
a correct initial pose, to continuously keep trace of its correct poses with respect to the 
world frame. Dead-reckoning is the real-time calculation of the robot’s position from 
wheel encoder measurements.  The dead-reckoning problem of the robot can be 
easily solved by using the numerical approach, the velocity information from the 
encoders mounted on the driving wheels and the forward kinematical model described 
by     
#
( )
( ) ( ( )) ( )
( )
w
w
x t
y t P t t
t
φ υ
θ
⎡ ⎤⎢ ⎥ =⎢ ⎥⎢ ⎥⎣ ⎦


                   (3.17) 
To find the continuous poses of the robot from Equation (3.17), many existing 
numerical approaches, such as the Euler’s formula, the Runge-Kutta method and so 
on, can be employed according to the required numerical accuracy and the step size.  
One of the simplest dead-reckoning methods is based on the second-order 
Runge-Kutta formula which approximates the pose differentiation of the robot by the 
following equation. 
       
# #
( ) ( 1)
( ( 1)) ( 1) ( ( ) ( )( ) ( 1)
2
( ) ( 1)
w w
w w
x k x k
P k k P k ky k y k T
k k
θ υ θ υ
θ θ
−⎡ ⎤ ⎡ ⎤ − − +⎢ ⎥ ⎢ ⎥= − +⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥−⎣ ⎦ ⎣ ⎦
           (3.18) 
Where T is the sampling period and sufficiently small; ( )x k , ( )y k  and  ( )kθ  denote 
respectively the values of ( )x t , ( )y t  and ( )tθ  at the k-th sampling instant. Notice 
that (3.18) gives the recursive formula to approximately obtain the robot’s position 
and orientation at the next sampling instant. Worthy of mention is that the accuracy of 
  
 
65
that is,   
( ) ( )
( ) ( )
( ) ( )
e d
e d
e d
x t x t x
y t y t y
t tθ θ θ
⎡ ⎤ ⎡ ⎤ ⎡ ⎤⎢ ⎥ ⎢ ⎥ ⎢ ⎥= −⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣ ⎦ ⎣ ⎦
                               (3.19) 
Which gives 
1
2#
3
4
( )
( ) ( )
( )
( ) ( ) ( ( ))
( )
( ) ( )
( )
e
e
e
r t
x t x t
r t
y t y t P t
r t
t t
r t
ω
ωθ ωθ θ ω
⎡ ⎤⎡ ⎤ ⎡ ⎤ ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥= =⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣ ⎦ ⎣ ⎦
 
 
 
                         (3.20) 
To asymptotically stabilize the system, we propose the nonlinear stabilization 
law where the matrices PK and IK  are symmetric and positive definite. 
1 0
2
0
3
4 0
( ) ( )
( )
( ) 1 ( ( )) ( ) ( ) , 0 ,   0
( )
( )
( )( )
t
e
e
t T T
p e I e P P I I
te
e
t x d
x t
t
P t K y t K y d K K K K
t r
t
dt
ω τ τ
ω θ τ τω θ θ τ τω
⎛ ⎞⎡ ⎤⎡ ⎤ ⎜ ⎟⎢ ⎥⎡ ⎤⎢ ⎥ ⎜ ⎟⎢ ⎥⎢ ⎥⎢ ⎥ = − − = > = >⎜ ⎟⎢ ⎥⎢ ⎥⎢ ⎥ ⎜ ⎟⎢ ⎥⎢ ⎥⎢ ⎥ ⎣ ⎦⎜ ⎟⎢ ⎥⎜ ⎟⎣ ⎦ ⎣ ⎦⎝ ⎠
∫
∫
∫
 (3.21)  
Taking (3.21) into (3.20), the dynamics of the closed-loop error system becomes 
0 0
#
0 0
0 0
( ) ( )
( ) ( ) ( )
( ) ( ( )) ( ( )) ( ) ( ) ( ) ( )
( ) ( ) ( )
( ) ( )
t t
e e
e e e
t t
e p e I e p e I e
t te e e
e e
x d x d
x t x t x t
y t P t P t K y t K y d K y t K y d
t t t
d d
τ τ τ τ
θ θ τ τ τ τ
θ θ θθ τ τ θ τ τ
⎛ ⎞⎡ ⎤ ⎡⎜ ⎟⎢ ⎥ ⎢⎡ ⎤ ⎡ ⎤ ⎡ ⎤⎜ ⎟⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥= − − = − −⎜ ⎟⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎜ ⎟⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣ ⎦ ⎣ ⎦⎜ ⎟⎢ ⎥⎜ ⎟⎣ ⎦ ⎣⎝ ⎠
∫ ∫
∫ ∫
∫ ∫



⎤⎥⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥⎦
     (3.22) 
For the asymptotical stability of the closed-loop error system, a radially 
unbounded Lyapunov function candidate is chosen as follows: 
[ ]
0
1 0 0 0 0
0
( )
( )
1 1( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )
2 2
( )
( )
t
e
e
t t t t
e e e e e e e I e
te
e
x d
x t
V t x t y t t y t x d y d d K y d
t
d
τ τ
θ τ τ τ τ θ τ τ τ τ
θ θ τ τ
⎡ ⎤⎢ ⎥⎡ ⎤ ⎢ ⎥⎢ ⎥ ⎡ ⎤= + ⎢ ⎥⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎢ ⎥⎢ ⎥⎣ ⎦ ⎢ ⎥⎣ ⎦
∫
∫ ∫ ∫ ∫
∫
      
(3.23) 
Taking the time derivative of 1( )V t  along the trajectory of (3.27) obtains 
  
 
67
Theorem 3.1: For the robot’s kinematic model (3.17) and the nonlinear kinematic 
control (3.21), the robot can be steered to reach any destination pose in the sense of 
globally asymptotical stability, i.e., ( ) dx t x→ , ( ) dy t y→  and ( ) dtθ θ→  as t →∞ . 
Moreover, if the speed limitations of the motors are considered, then the practical 
control law (3.25) is used so that the origin in the closed-loop error system is globally 
asymptotically stable. 
Remark 3.1: If the two gain matrices PK and IK  are diagonal, i.e., 
1 2 3{ , , }P P P PK diag K K K=  and 1 2 3{ , , }I I I IK diag K K K= , then the closed-loop error 
system (3.29) is decoupled into four independent standard second-order equations. By 
matching to the desired second-order characteristic equations, the diagonal entries can 
be easily obtained from 2Pi i iK ς ω= , 2Ii iK ω= , i=1,2,3, where iς  and iω  
represent respectively the desired damping ratio and natural frequency.  
 
Remark 3.2:  If the matrix IK  in the nonlinear kinematic control (3.21) is set to 
zero and the Lyapunov function [ ][ ]2 1 ( ) ( ) ( ) ( ) ( ) ( )2
T
e e e e e eV x t y t t x t y t tθ θ=  is 
selected, then the robot can be steered to reach any destination pose in the sense of 
globally exponential stability. 
 
3.3.2  Trajectory Tracking  
This subsection considers the trajectory tracking problem. Unlike all 
nonholonomic conventional mobile robots, the trajectories of the omnidirectional 
mobile robots can not be generated using their kinematic models, i.e., any smooth and 
differentiable trajectories for the omnidirectional robots can be arbitrarily planned. 
Given the smooth and differentiable trajectory [ ] 1( ) ( ) ( ) Td d dx t y t t Cθ ∈ , we define the 
following tracking error vector 
  
 
69
smooth and differentiable  trajectory [ ]( ) ( ) ( ) Td d dx t y t tθ and the trajectory tracking 
control (3.28), the robot can be steered to exactly follow the trajectory in the sense of 
globally asymptotical stability, i.e., i.e., ( ) ( )dx t x t→ , ( ) ( )dy t y t→  and ( ) ( )dt tθ θ→  as 
t →∞ . 
 
Remark 3.3:  If the two gain matrices PK and IK  in the control law (3.28) are 
diagonal, i.e., 1 2 3{ , , }P P P PK diag K K K=  and 1 2 3{ , , }I I I IK diag K K K= , then their 
entries, PiK  and IiK , i=1,2,3, can be designed via the same way of Remark 3. 1.  
 
Remark 3.4: If the matrix IK  in the nonlinear kinematic control (3.28) is set to zero 
and the Lyapunov function [ ][ ]2 1 ( ) ( ) ( ) ( ) ( ) ( )2
T
e e e e e eV x t y t t x t y t tθ θ=  is chosen, then 
the robot can be steered to exactly follow any smooth and differentiable trajectory in 
the sense of globally exponential stability. 
  Before closing this section, it is interesting to point out that the previous point 
stabilization and trajectory tracking control problems can be simultaneously achieved 
by the control law (3.28). The control law (3.28) becomes a point stabilization one if 
the desired pose [ ]( ) ( ) ( ) Td d dx t y t tθ can be either the time-dependent trajectory or the 
fixed destination posture.  
 
3.4  Experimental Results and Discussion  
    The aims of these experiments are to examine the effectiveness and performance 
of the proposed control methods. The experiments contain point-to-point stabilization, 
circular trajectory tracking and random trajectory tracking. The main idea of these 
trajectory tracking methods is to place an imagine goal in front of the mobile robot, 
when robot approached the imagine goal, the imagine goal would change along the 
  
 
71
 
Figure 3.11. Experiment trajectories of the proposed kinematic controller for 
achieving point-to-point stabilization. 
 
Table 3.1 depicts the mean error and the standard deviation from the point-to-point 
stabilization experimental result. The sampling period is every 0.5 second. The total 
number of the standard deviation samples is 180. 
 
Table 3.1. Mean error and standard deviation of the point-to-point stabilization.  
mean error standard deviation 
9.248 cm 4.624cm 
 
3.4.2  Line Path Experiment   
  The second simulation was used to study the tracking performance of kinematic 
controllers. The initial position and desired trajectory are respectively given by 
Unit:cm cm 
cm 
X coordinate 
Y coordinate
  
 
73
    
(b)                           (c) 
   
(d)                          (e) 
 
(f) 
Figure 3.12. (a) Experiment straight-line trajectory tracking start point. (b~e)The 
situation for the mobile robot moving toward the straight line. (f) The overall tracking 
result. 
Unit:cm 
X coordinate 
Y coordinate
cm 
cm 
  
 
75
the kinematic-based trajectory tracking controller was shown useful in achieving the 
circular tracking. Figure 3.15 shows the experimental pictures of the circular trajectory 
tracking. Table 3.3 depicts the mean error and the standard deviation from the circle 
trajectory tracking experimental result. The sampling period is every 0.5 second. The 
total number of the standard deviation samples is 160. 
 
Table 3.3. Mean error and standard deviation of the circle trajectory tracking 
experiment. 
mean error standard deviation 
13.248 cm 7.624cm 
 
 
 
(a) 
Unit:cm cm 
X coordinate 
Y coordinate
cm 
  
 
77
    
(h)                            (i) 
 
(j) 
Figure.3.14. (a) The mobile robot in the initial position. (b~i) Experimental result of 
the circular trajectory tracking. (j) The mobile robot in the end position. 
cm 
cm 
Unit:cm 
X coordinate 
Y coordinate 
  
 
79
steering the omnidirectional mobile robot to exactly track the random trajectory. 
Figure 3.17 depicts the experimental pictures of the random trajectory tracking. Table 
3.4 depicts the mean error and the standard deviation from the random trajectory 
tracking experimental result. The sampling period is every 0.5 second. The total 
number of the standard deviation samples is 108. 
 
Table 3.4. Mean error and standard deviation of the random trajectory tracking 
experiment. 
mean error standard deviation 
15.248 cm 8.086cm 
 
 
(a) 
Unit:cm cm 
cm 
X coordinate 
Y coordinate
  
 
81
 
(h) 
Figure.3.16. (a) The mobile robot in the initial position. (b~i) Experimental result of 
the random trajectory tracking. (h) The mobile robot in the end position. 
   
(b)                     (b)                   (c) 
   
(d)                    (e)                   (f) 
   
(g)                    (h)                   (i) 
Figure 3.17 The time historical pictures of the random trajectory tracking. 
Unit:cm cm 
cm 
X coordinate 
Y coordinate 
  
 
83
Chapter 4  Adaptive Dynamic Motion Control 
 
4.1  Introduction and Control Architecture 
This chapter develops an adaptive dynamic motion controller for position control 
and trajectory tracking of the omnidirectional mobile robot equipped with four 
independent omnidirectional wheels equally spaced at 90 degrees from one to another. 
Such a controller is synthesized by backstepping and will be proven globally 
asymptotically stable via the Lyapunov stability theory. Figure 4.1 shows the block 
diagram of the motion control system where the adaptive backstepping controller is 
used to achieve various motions and the estimator is employed to on-line estimate the 
required one controller parameter whose is unknown but constant. Finally, the 
feasibility and efficacy of the proposed control method are exemplified by conducting 
several simulations results on steering the mobile robot.  
The rest of the chapter is organized as follows. Section 4.2 describes the dynamic 
model in the world frame. In Section 4.3 the dynamic motion controller design is 
presented. The adaptive dynamic motion controller is designed in Section 4.4 for the 
mobile robot with an unknown parameter k. Section 4.5 presents some simulations 
and discussion. Finally, the conclusions of the chapter are stated in Section 4.6. 
 
  
 
85
δ θ
1T
2T
3T
4T
Xω
Yω
mx
my
 
Figure 4.2. Geometry of the omni-directional robot with simplified top-view. 
 
By assumed the mobile robot is moving on a flat field with no slip, the initial 
world coordinate frame X Yω ω  be fixed; and the moving coordinate frame m mX Y  is 
attached to the mass center of the mobile robot. Where the distance from the centre of 
mass to each wheel centre is L , δ is a fixed but adjustable angle and θ  is the 
orientation of the mobile robot with respect to world frame Xω . One defines two 
vectors with respect to world frame such that; 
[ ] ,w w Tw S x y θ=                 (4.1) 
and  
x y G
TwF F F M= ⎡ ⎤⎣ ⎦                            (4.2) 
where GM  is the torque applied to the robot; and wm R  is the transform matrix between 
the world frame and moving frame shown as follows;  
cos sin 0
sin cos 0
0 0 1
w
m R
θ θ
θ θ
−⎡ ⎤⎢ ⎥= ⎢ ⎥⎢ ⎥⎣ ⎦
                       (4.3) 
Next, defines two vectors with respect to the moving frame such that 
[ ]m m Tm s x y θ=                       (4.4) 
and  
x y G
Tm f f f M= ⎡ ⎤⎣ ⎦                        (4.5) 
  
 
87
and 
1 1
2 2
3
4
1 1( sin cos ) ( sin cos )
1 1( cos sin ) ( cos sin
1 (sin cos )
1 (cos sin )
m m m m
m m m m
m m
m m
x y L x y L
r r
x y L x y L
r r
x y L
r
x y L
r
ω δ δ θ ω δ δ θ
ω δ δ θ ω δ δ
ω δ δ θ
ω δ δ θ
⎧ ⎫= − ⋅ + ⋅ + = − ⋅ + ⋅ +⎪ ⎪⎪ ⎪⎪ ⎪= − ⋅ − ⋅ + = − ⋅ − ⋅ +⎪ ⎪⇒⎨ ⎬⎪ ⎪= ⋅ − ⋅ +⎪ ⎪⎪ ⎪⎪ ⎪= ⋅ + ⋅ +⎩ ⎭
    
    
 
 
3
4
)
1 (sin cos )
1 (cos sin )
m m
m m
x y L
r
x y L
r
θ
ω δ δ θ
ω δ δ θ
⎧ ⎫⎪ ⎪⎪ ⎪⎪ ⎪⎪ ⎪⎨ ⎬⎪ ⎪= ⋅ − ⋅ +⎪ ⎪⎪ ⎪⎪ ⎪= ⋅ + ⋅ +⎩ ⎭

  
  
 
 (4.12) 
The substitution of (4.12) into (4.11) yields 
1 1 1
1 1
2 2
1 ( sin cos ) ( sin cos )
( sin cos ) ( sin cos )
m m m m
m m m m
I cT ku f x y L x y L
r r r
Iku f cx y L x y L
r r r r
ω
ω
δ δ θ δ δ θ
δ δ θ δ δ θ
⎛ ⎞= − − − ⋅ + ⋅ + − − ⋅ + ⋅ +⎜ ⎟⎝ ⎠
= − − − ⋅ + ⋅ + − − ⋅ + ⋅ +
    
    
 
     (4.13) 
2 2 2
2 2
2 2
1 ( cos sin ) ( cos sin )
( cos sin ) ( cos sin )
m m m m
m m m m
I cT ku f x y L x y L
r r r
Iku f cx y L x y L
r r r r
ω
ω
δ δ θ δ δ θ
δ δ θ δ δ θ
⎛ ⎞= − − − ⋅ − ⋅ + − − ⋅ − ⋅ +⎜ ⎟⎝ ⎠
= − − − ⋅ − ⋅ + − − ⋅ − ⋅ +
    
    
   (4.14) 
3 3 3
3 3
2 2
1 (sin cos ) (sin cos )
(sin cos ) (sin cos )
m m m m
m m m m
I cT ku f x y L x y L
r r r
ku f I cx y L x y L
r r r r
ω
ω
δ δ θ δ δ θ
δ δ θ δ δ θ
⎛ ⎞= − − ⋅ − ⋅ + − ⋅ − ⋅ +⎜ ⎟⎝ ⎠
= − − ⋅ − ⋅ + − ⋅ − ⋅ +
    
    
      (4.15) 
4 4 4
4 4
2 2
1 (cos sin ) (cos sin )
(cos sin ) (cos sin )
m m m m
m m m m
I cT ku f x y L x y L
r r r
Iku f cx y L x y L
r r r r
ω
ω
δ δ θ δ δ θ
δ δ θ δ δ θ
⎛ ⎞= − − ⋅ + ⋅ + − ⋅ + ⋅ +⎜ ⎟⎝ ⎠
= − − ⋅ + ⋅ + − ⋅ + ⋅ +
    
    
     (4.16) 
Substituting (4.13 -16) into (4.7) and (4.8) gives 
( )
( )
( )
1
2
3
4
1 2 3 4
1 2 3 4
sin cos sin cos
cos sin cos sin
or
sin cos sin cos
cos sin cos sin
m m
m m
m m
m m
m
T
x y
T
M y x
T
L L L L
T
M x y T T T T
M y x T T T T
I
θ δ δ δ δ
θ δ δ δ δ
θ
θ δ δ δ δ
θ δ δ δ δ
θ
⎡ ⎤⎡ ⎤− − −⎡ ⎤ ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥+ = − −⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎣ ⎦⎣ ⎦ ⎣ ⎦
− = − ⋅ − ⋅ + ⋅ + ⋅
+ = ⋅ − ⋅ − ⋅ + ⋅
=
 
 

 
 

1 2 3 4( )L T T T T+ + +
         (4.17) 
  
 
89
{ } { } { }{ }m mP s N s R U K F+ = +                          (4.22) 
where      
2
2
2
2
3 3
2
0 0
2
0 0
4
0 0 m
X
I
M
r
I
P M
r
I
I L
r
ω
ω
ω
⎡ ⎤+⎢ ⎥⎢ ⎥⎢ ⎥= +⎢ ⎥⎢ ⎥⎢ ⎥+⎢ ⎥⎣ ⎦
        (4.23) 
{ }
2
2
2
2
3 1
2
2
4
m m
m
m m
c x M y
r
cN s M x y
r
c L
r
θ
θ
θ
×
⎡ ⎤−⎢ ⎥⎢ ⎥⎢ ⎥= +⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
 
  

                  (4.24) 
3 4
sin cos sin cos
cos sin cos sin
X
kR
r
L L L L
δ δ δ δ
δ δ δ δ
− −⎡ ⎤⎢ ⎥= − −⎢ ⎥⎢ ⎥⎣ ⎦
               (4.25) 
3 4
sin cos sin cos
1 cos sin cos sinK
r
L L L L
δ δ δ δ
δ δ δ δ
×
− −⎡ ⎤⎢ ⎥= − −⎢ ⎥⎢ ⎥− − − −⎣ ⎦
                 (4.26) 
Next, we desire to express (4.22) with respect to the world frame. With (4.6), it 
follows that    
w w m w m
m mS R s R s= +                           (4.27) 
which gives  
( )m w T wms R S=   and ( ) ( ( ) )m w T w w w T wm m ms R S R R S= −            (4.28) 
Substituting (4.28) into (4.22) yields  
{ } { } { } { }( ) ( ( ) )w T w w w T w wm m mP R S R R S N S R U K F− + = +        (4.29) 
where  { } { } ( )m w T wmw m s R SN S N s ==   . Since the mass matrix P is always invertible, 
(4.27) becomes 
  
 
91
 
(a) 
 
(b) 
Figure 4.3. Experimental omnidirectional four-wheeled mobile platform: (a) 
bottom-view; (b) side-view. 
 
 
4.3  Dynamic Motion Controller Design 
This section aims to design a dynamic control law for both regulation 
(stabilization) and trajectory tracking of the four-wheeled omnidirectional robot with 
the dynamic model (4.32) such that the omnidirectional mobile robot exactly follows 
the desired differentiable trajectory described by [ ]( ) ( ) ( ) ( ) Tr r r rY t x t y t tθ= .  The design 
  
 
93
choosing the radial, unbounded and quadratic Lyapunov function 2 2
1 1
2 2
T T
e P eV KY Y η η= + . 
Then the time derivative of the Lyapunov equation 2V becomes 
2
2 3 2 3
2( )T Te P e
T T T T T T
e P e P e P e e P e
V Y K Y
Y K Y K K K Y K KY Y Y
η η
η η η η η η
= +
= − − − − −=
  
            (4.41) 
which indicates that 2V  is negative definite because the two matrices 3PK and K are 
symmetric, positive definite and even diagonal, and 2V  is a Lyapunov function for 
the proposed controller (4.38).  Hence the two vectors eY  and η  approach zero as 
time goes to infinity, that is, 1 rY Y→  and 2 rY Y→   as t →∞ . This result indicates that 
the proposed dynamic control law (4.38) is capable of steering the robot with the 
dynamic model (4.32) to follow any destination pose or any differentiable and 
time-varying trajectory. Hence, this main result is summarized as follows.  
Theorem 4.1 For the robot’s dynamic model (4.32) with the desired differentiable 
trajectory [ ]( ) ( ) ( ) Tr r r rY x t y t tθ= and the dynamic control law (4.38), the platform 
can be steered to reach any destination pose or follow any differentiable and 
time-varying trajectory in the sense of globally asymptotical stability, i.e., 1 rY Y→  
and 2 rY Y→   as t →∞ . 
 
 
4.4  Adaptive Dynamic Motion Controller Design 
This section is devoted to developing an adaptive dynamic motion controller for 
the mobile robot with an unknown parameter k. In doing so, it is necessary to define 
r
k
θ =  and θˆ  as an estimate of r
k
θ = . With the symbol, one obtains    
  
 
95
2 21 1 1 , 0
2 2 2
T T
e p eV Y K Y η η θ γγ= + + >
 .                (4.47) 
With (4.36) and (4.45) ,the time derivative of the Lyapunov equation V becomes 
( )
( ) ( ) ( )
( )
2
2 2
3
1 ˆ
1 ˆ
ˆ
T T
e p e
T T
e p p e p e p r
T T T
e p e p r
V Y K Y
Y K K Y K Y K K N Y
Y K Y K K K N Y
η η θ θγ
θη η η θ θθ γ
θ θη η η η θθ γ
= + + −
⎧ ⎫⎡ ⎤= − + − + + − − + −⎨ ⎬⎣ ⎦⎩ ⎭
⎛ ⎞−⎡ ⎤ ⎜ ⎟= − − + + − − +⎣ ⎦ ⎜ ⎟⎝ ⎠
  
 
 
 
or 
( )
3
ˆT p rT T
e p e
K K N Y
V Y K Y K
η η θη η θ θ γ
⎧ ⎫⎡ ⎤+ − −⎪ ⎪⎣ ⎦= − − + −⎨ ⎬⎪ ⎪⎩ ⎭
 
          (4.48) 
If the parameter adjustment rule for θˆ  is chosen as 
( )ˆ T p rK K N Yη ηθ
γ θ
⎡ ⎤+ − −⎣ ⎦=

 
or 
 
( )
( )
ˆ
T
p r
T
p r
K K N Y
K K N Y where
η ηθ γ θ
γγ η η γ θ
⎧ ⎫⎡ ⎤+ − −⎪ ⎪⎣ ⎦= ⎨ ⎬⎪ ⎪⎩ ⎭
⎡ ⎤′ ′= + − − =⎣ ⎦


            (4.49) 
Then 3 0T Te p eV Y K Y Kη η= − − ≤ . Barbalat’s lemma implies that 0eY → , 0η → as 
t →∞ . 
 
Theorem 4.2 For the dynamic model (4.32) with the desired differentiable trajectory  
and the adaptive dynamic control law (4.43) with the parameter updating rule (4.49), 
the mobile platform can be steered to reach any destination pose or follow any 
differentiable and time-varying trajectory in the sense of globally asymptotical 
stability, i.e., 1 rY Y→  and 2 rY Y→   as t →∞ . 
  
 
97
(a) 
 
3
fnn3
2
fnn2
1
fnn1
fnn3
S-Function2
fnn2
S-Function1
fnn1
S-Function
11
pfnn3
10
pfnn2
9
pfnn1
8
ptheta
7
theta
6
pdtheta
5
dtheta
4
pdy
3
dy
2
pdx
1
dx
 
(b) 
Figure 4.4. Simulink model of the adaptive dynamic motion controller: (a) main block 
diagram; (b) subsystem block diagram. 
 
 
 
 
 
  
 
99
the origin to the goal poses, and Figure 4.5(b) shows the heading behavior of the 
proposed stabilization law for the robot moving towards the desired orientation 
2
π . 
The results in Figure 4.5(a) indicated that the trajectories have almost minimum 
distances, namely that the robot moved along with straight lines towards the goal 
poses. More importantly, the mobile robot moved toward any direction and 
simultaneously attained the desired orientation, showing the unique property of the 
omnidirectional wheeled mobile robot. Through simulation results, the mobile robot 
with the proposed stabilization method has been shown capable of reaching the 
desired postures with satisfactory performance. 
-10 -8 -6 -4 -2 0 2 4 6 8 10
-10
-8
-6
-4
-2
0
2
4
6
8
10
x(m)
y(
m
)
dynamic controller point to point stabilization and learn k
 
(a) 
  
 
101
0 10 20 30 40 50 60 70 80 90 100
0
10
20
30
40
50
60
70
80
90
100
x(m)
y(
m
)
line trajectory tracking and learn k
 
 
actual
desired
 
(a) 
0 1 2 3 4 5 6 7 8 9 10
-0.8
-0.7
-0.6
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
time(sec)
E
rro
rs
(x
e,
ye
:c
m
 th
et
ae
:ra
d)
line trajectory tracking errors
 
 
thetae
xe
ye
 
(b) 
Figure 4.6. (a) Straight-line trajectory tracking result.  (b) Line trajectory tracking 
errors of the platform. 
  
 
103
-4 -3 -2 -1 0 1 2 3 4
-4
-3
-2
-1
0
1
2
3
4
x(m)
y(
m
)
kinematic controller circular trajectory tracking and learn k
 
 
actual
desired
 
(a) 
0 2 4 6 8 10 12 14 16 18 20
-3
-2.5
-2
-1.5
-1
-0.5
0
0.5
time(sec)
E
rro
rs
(x
e,
ye
:c
m
 th
et
ae
:ra
d)
kinematic controller errors with circular tracking
 
 
thetae
xe
ye
 
(b) 
Figure 4.7. (a) Simulation result of the circular trajectory tracking. (b) The errors of 
the circular trajectory tracking. 
  
 
105
(a) 
0 5 10 15 20 25
-2
-1.5
-1
-0.5
0
0.5
time(sec)
E
rro
rs
(x
e,
ye
:c
m
 th
et
ae
:ra
d)
elliptic trajectory tracking error
 
 
thetae
xe
ye
(b) 
Figure 4.8. (a) Result of the elliptic trajectory tracking. (b) Errors response of the 
elliptic trajectory tracking. 
  
4.5.6  Cardioids trajectory Tracking  
The last simulation was conducted to investigate the performance of the proposed 
dynamic control law (4.38).to steer the mobile robot following a cardioids trajectory 
described by 
( ) ( )[(1+cos(t))*cos(t)]*2 [(1+cos(t))*sin(t)]*2 /2T Td d dX Y θ π=  
The simulation assumed that the robot started with o o o(X , Y , θ ) (0 ,  0 ,  0 )T cm cm rad= . The 
parameters in the cardioids trajectory tracking experiment were taken as 
follows: 0 (rad/sec)oω = , 2(rad/sec)rω = , x0=4,y0=-0.09 , 0doY = (cm) and 
  
 
107
0 10 20 30 40 50 60 70 80
-4
-3.5
-3
-2.5
-2
-1.5
-1
-0.5
0
0.5
time(sec)
E
rro
rs
(x
e,
ye
:m
 th
et
ae
:ra
d)
Dynamic controller errors for simulation with cardioid
 
 
thetae
xe
ye
(b) 
Figure 4.9. (a) Result of the cardioids trajectory tracking. (b) Errors response of the 
cardioids trajectory tracking. 
 
4.5.7  Experimental Results and Discussion 
The following two experiments are conducted to examine the effectiveness and 
performance of the proposed dynamic control method on the constructed 
four-wheeled omnidirectional robot. Both experiments adopted the dead-reckoning 
method mentioned in Section 2.4.2, and used the small-scale PC as the main motion 
controller such that the control codes can be easily implemented by C language. First, 
the line trajectory tracking experiment is   used to study the tracking performance of 
the dynamic controller. The initial position and desired trajectory were respectively 
were respectively set by [ ] [ ]0 0 0 250cm 100cm / 2radsT Tx y θ π=  , and  
[ ] co s s in ,  2 0  cm / sec , 0 .
4 4 4
T
T
r r rx y V t V t V t
π π πθ ⎡ ⎤= ⋅ ⋅ = ≥⎢ ⎥⎣ ⎦
 
  
 
109
   
(b)                           (c) 
   
(d)                          (e) 
 
 
(f) 
Figure 4.10. (a) Experiment straight-line trajectory tracking start point. (b~e)The 
trajectories of the mobile robot moving toward the straight line. (f) The overall 
tracking result. 
Unit:cm 
Unit:cm 
X coordinate 
Y coordinate 
  
 
111
the mean error and the standard deviation from the random trajectory tracking 
experimental result. The sampling period is every 10 milliseconds. The total number 
of the samples is 5400. 
 
Table 4.3. Mean error and standard deviation of the random trajectory tracking 
experiment. 
mean error standard deviation 
9.248 cm 5.086cm 
 
 
 
Figure 4.12. Experimental result of the random destination tracking. 
Unit:cm 
cm 
Unit:cm 
cm 
cm 
X coordinate 
Y coordinate 
  
 
113
Chapter 5  ZigBee Localization Subsystem 
 
5.1  Introduction 
ZigBee is a wireless technology developed as an open global standard to address 
the unique needs of low-cost, low-power, wireless sensor networks. The standard has 
full advantage of the IEEE 802.15.4 physical radio specification and operates in 
unlicensed bands worldwide at the following frequencies: 2.400–2.484 GHz, 902-928 
MHz and 868.0–868.6 MHz. The 802.15.4 specification was developed at the Institute 
of Electrical and Electronics Engineers (IEEE). The specification is a packet-based 
radio protocol that meets the needs of low-cost, battery-operated devices. The 
protocol allows devices to intercommunicate and be powered by batteries that last 
years instead of hours.  
The ZigBee protocol was engineered by the ZigBee Alliance, a non-profit 
consortium of leading semiconductor manufacturers, technology providers, OEMs 
and end-users worldwide. The protocol was designed to provide OEMs and 
integrators with an easy-to-use wireless data solution characterized by low-power 
consumption, support for multiple network structures and secure connections. ZigBee 
enables broad-based deployment of wireless networks with low-cost, low-power 
solutions. It provides the ability to run for years on inexpensive batteries for a host of 
monitoring applications: Lighting controls, AMR (Automatic Meter Reading), smoke 
and CO detectors, wireless telemetry, HVAC control, heating control, home security, 
Environmental controls, drapery and shade controls, etc. 
The ZigBee protocol was designed to carry data through the hostile RF 
environments that routinely exist in commercial and industrial applications. There are 
some reasons why we change RFID to ZigBee, 
  
 
115
Table 5.1 shows the comparison of various devices. The ZigBee specification 
provides a security toolbox approach to ensuring reliable and secure networks. Access 
control lists, packet freshness timers and 128-bit encryption based on the NIST 
Certified Advanced Encryption Standard (AES) help protect transmitted data. 
 
5.2  Mesh Networks 
A key component of the ZigBee protocol is the ability to support mesh networks. 
In a mesh network, nodes are interconnected with other nodes so that at least two 
pathways connect each node. Connections between nodes are dynamically updated 
and optimized in difficult conditions. In some cases, a partial mesh network is 
established with some of the nodes only connected to one other node.  
Mesh networks are decentralized in nature; each node is self-routing and able to 
connect to other nodes as needed. The characteristics of mesh topology and ad-hoc 
routing provide greater stability in changing conditions or failure at single nodes.  
In our experimental setup, the ZigBee readers are used to receive the RSSI 
(Received Signal Strength Indication) of the tag on the robot. Then a calibration 
method is used to convert the RSSI data into the corresponding distances for the 
active RFID system. These distance data are then employed to compute the position 
and orientation approach for global localization of the robot by the least-squares 
method. Figure 5.1 displays the physical picture of the ZigBee readers and the tags. 
Figure 5.2 introduces application diagram of the ZigBee operation system. Table 5.2 
illustrates the specifications of the ZigBee module. 
  
 
117
Table 5.2. Specifications of the ZigBee module. 
 Parameter Master Reader Tag 
Frequency 2.4GHz 2.4GHz 2.4GHz 
Max Rate 250Kbps 250Kbps 250Kbps 
Sensitivity -94dBm -94dBm -94dBm 
Trans Range 350m LOS 350m LOS 100m LOS 
Channel 16(5MHz) 16(5MHz) 16(5MHz) 
Launch 
Power 
-15 ~ +10dBm -15 ~ +10dBm -25 ~ +0dBm 
Data 
Encryption 
128-bit AES 128-bit AES 128-bit AES 
Radio 
frequency 
Antenna Internal External Internal 
TX <100mA <100mA <37mA 
RX <43mA <43mA <43mA 
Power 
consumption 
Sleep 40uA 40uA 40uA 
Size(mm) 125*90*25 62*54*28 68*40*17 
Operating 
Temp 
-20 ~ +70 CD  -20 ~ +70 CD  -20 ~ +70 CD  
Physical 
parameter 
Humidity 10% ~ 90% 10% ~ 90% 10% ~ 90% 
 
5.3  Localization Algorithm 
5.3.1  Global Localization Method Using RSSI  
The proposed global localization method is based on the RSSI measurements 
between the tags and the reader, and the information of the given position of these 
active tags. It is known that the RSSI measurements are corrupted by several factors, 
  
 
119
where 10 p=log K   and  q = n . Given m pairs of the RSSI and distance 
measurement, ( RSSIi , id ), i=1, 2, …, m, one obtains the following matrix equation  
   
10 1 10 1
10 2 10 2
10 10
1   log log RSSI
1   log log RSSIp
       AX B       
q              
1   log log RSSIm m
d
d
d
−⎡ ⎤ ⎡ ⎤⎢ ⎥ ⎢ ⎥− ⎡ ⎤⎢ ⎥ ⎢ ⎥= ⇒ =⎢ ⎥⎢ ⎥ ⎢ ⎥⎣ ⎦⎢ ⎥ ⎢ ⎥−⎣ ⎦ ⎣ ⎦
# #   (5.2) 
where  
10 1 10 1
10 2 10 2
10 10
1   log log RSSI
1   log log RSSI
A= ,
              
1   log log RSSIm m
d
d
B
d
−⎡ ⎤ ⎡ ⎤⎢ ⎥ ⎢ ⎥−⎢ ⎥ ⎢ ⎥=⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥−⎣ ⎦ ⎣ ⎦
# #  
The parameters p and q are solved via the least-square method  
 
T -1 T pX=(A A) A B
q
⎡ ⎤= ⎢ ⎥⎣ ⎦                            (5.3) 
which implies that p K=10 ,  =qn .  
The calibration procedure to find the two parameters K and n will be repeated for 
each tag until all the tags are calibrated. 
 
5.3.3  Robot Position Estimation Using Least Square Method 
This subsection aims to use the calibration models to obtain the distances 
between all the installed readers and the tag, in order to achieve global localization of 
the tour-guide robot. The basic idea to do so is to put m  readers at known positions 
respectively given by 1 1 1 1T ( , , )
Tx y z= , 2 2 2 2T ( , , ) ,       ,T ( , , )T Tm m m mx y z x y z= =" , to 
measure the RSSI data from the tag, and to convert the RSSI data into the distances. 
Figure 5.3 shows the physical configuration of the proposed ZigBee localization 
system where ( , , , )Tx y z θ  represents the posture of robot.  
  
 
121
where 
2 2 2 2 2 2 2 2
2 1 1 2 1 2 1 21 2 1 2 1 2
2 2 2 2 2 2 2 2
1 1 1 1 1 1 1
2 21 1 1 1
- +( ) ( ) ( )2( )   2( )   2( )
A= 2( )   2( )   2( )  ,  B= - +( ) ( ) ( )
2( )   2( )   2( ) - +
i i i i i i i
m m m m
d d x x y y z zx x y y z z
x x y y z z d d x x y y z z
x x y y z z d d
− + − + −− − −⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥− − − − + − + −⎢ ⎥⎢ ⎥⎢ ⎥− − −⎣ ⎦
##
# #
2 2 2 2 2 2
1 1 1( ) ( ) ( )m m mx x y y z z
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥− + − + −⎢ ⎥⎣ ⎦
  
The least square method is again used to solve the matrix equation. 
T -1 T X=(A A) A B                                 (5.5) 
which gives the robot position ( , , )Tx y z .  
 
 
5.3.4  Robot Orientation Estimation 
    This subsection presents an orientation estimation method to find the initial robot 
heading with respect to the world frame. Since the robot position ( , , )Tx y z can be 
calculated via (5.5), the robot heading would be found by letting the robot to move in 
a straight line with the original heading direction, where the straight line is given as 
follows: 
     y = X +ci im                             (5.6) 
In (5.6) the parameter m  is slope of the straight line and c is the constant offset. 
Assuming two more robot positions to be obtained from (5.5), one re-arranges (5.6) in 
a matrix-vector form as   
1 1 1 1
2 2 2 2
1  X 1  X
1  X 1  Xc c
    CY=D,  Y= ,  C= ,  D= 
      
1  X 1  Xn n n n
y y
y y
m m
y y
⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎡ ⎤ ⎡ ⎤⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥= ⇒⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣ ⎦⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣ ⎦ ⎣ ⎦ ⎣ ⎦
# # # #         (5.7) 
Again, the least-square method is adopted to solve the matrix equation (5.7). 
T -1 Tc =Y (C C) C D
m
⎡ ⎤ =⎢ ⎥⎣ ⎦                            (5.8) 
From (5.8) determines the slop m  where =tan2   m θ which indicates =Atan( )mθ . In 
  
 
123
where 1 0 0ˆˆ ˆ = cosx x v θ− , 1 0 0ˆˆ ˆ = siny y v θ−  and v  represents the linear speed of the robot. 
Note that the angle range of two argument arc tangent function, tan 2A  is 
between π−  and π . 
 
5.3.6  Experiment Results of Global Pose Initialization 
The first experiment was performed to investigate the accuracy of the proposed 
method for ZigBee static pose estimate of the tour-guide robot with one reader on the 
head and the four tags on the ceiling. The four readers were installed at the positions 
1 1 1( , , ) (47,544,300),x y z = 2 2 2( , , ) (400,680,299),x y z = 3 3 3( , , ) (473,313, 299)x y z =  
(unit: cm) with respect to the world frame. The true position of the robot in both x and 
y coordinate frame was given by (17.5cm, 240cm) and the true heading 
angle   θ was 90D . Before experimentation, all the four readers were calibrated using 
equation (5.1). Figure 5.4 shows the four calibration curves which covert the RSSI 
values into their corresponding distances. Afterwards, the real-time pose initialization 
algorithm was applied to calculate the pose estimates of the robot 
(0cm, 226.85cm,74.36 ).o  In Figure 5.5, the circle represents the true position, and the 
cross represents the least-squares estimate. We observe that the proposed ZigBee 
global localization method is proven capable of having the position error of less than 
30 cm and the heading error of less than 20 .ο  These experimental results indicate that 
the proposed pose initialization method can be effectively used to find the correct 
initial pose of the robot with respect to the world frame. 
 
  
 
125
1 2 3 4 5 6 7 8 9 10
190
195
200
205
210
215
220
Distance (meters)
A
ve
ra
ge
 R
S
S
I
 
 
True state
Formula solving
 
Figure 5.4. Calibration curves relating the RSSI values to their corresponding 
distances. (a)Reader 1.(b) Reader 2.(c) Reader 3. 
0 50 100 150 200 250 300 350 400 450 500
200
250
300
350
400
450
500
550
600
650
700
X coorinate frame (unit: cm)
Y
 c
oo
rin
at
e 
fra
m
e 
(u
ni
t: 
cm
)
 
 
True position
Estimate position
Reader position
  
 
127
The Kalman filter is a set of mathematical equation that provides an efficient 
computational (recursive) means to estimate the state of the process of interest in a 
way to minimize the mean of the square error. The Kalman filter has been shown very 
powerful in several aspects, supporting smoothing, estimation and prediction of past, 
present, and even future states. In the following, the discrete-time Kalman filter for 
signal fusion is introduced.  
Consider the signal fusion system with the following linear discrete-time state 
space model and measurement equation: 
# #( ( 1)) ( 1) ( ( )) ( )( 1) ( ) ( )
2
P k k P k kX k AX k T W kθ υ θ υ− − ++ = + +           (5.11) 
                ( ) ( ) ( )Z k CX k V k= +                            (5.12) 
where k  denotes the discrete-time index, )(kX  is an 1×n  state vector, )(kZ  is an 
1×m  measurement vector, )(kW  and )(kV  two zero-mean mutually independent, 
white Gaussian noise processes with covariance matrices Q  and R , respectively. n  
the state dimension and m  be the measurement dimension. Moreover,  
1 0 0
0 1 0
0 0 1
A
⎡ ⎤⎢ ⎥= ⎢ ⎥⎢ ⎥⎣ ⎦
, 
1 0 0
0 1 0
0 0 1
C
⎡ ⎤⎢ ⎥= ⎢ ⎥⎢ ⎥⎣ ⎦
 
The Kalman filter provides a recursive state estimate )|(ˆ kkX  at time k  
through a prediction estimate )|1(ˆ kkX +  given all information up to time 1+k  and 
a new measurement ( 1)Z k + , ( )1K k + is a Kalman filter gain and ( 1)r k +  is the 
innovation given by 
ˆ( 1) ( 1) ( 1| )r k Z k CX k k+ = + − +                   (5.13) 
( ) l ( ) ( ) 11 1 / ( 1 / )T TK k P k k C CP K k C R −+ = + + +           (5.14) 
The Kalman filtering algorithm is described by the following equations. 
(i) One-step-ahead Prediction: 
  
 
129
Chapter 6  Autonomous Navigation 
 
6.1  Introduction 
  This chapter aims to make a further step toward into safe navigation, one of 
the major tasks of the tour-guide robot. Tour-guide robots are expected to work in 
museums, commercial show rooms and public institutions. There are many existing 
objects, such as glass windows of exhibitions, exhibition display platform, 
information boards and black decoration material in the showroom. These objects 
may result in possible dangerous accidents while the robot moving around them. 
Therefore, the way of how to acquire environment information and detect obstacles 
surrounding the robot is a very important task for such kind robot. In order to prevent 
navigation based on erroneous sensor data or single sensor from breakdown, 
multi-sensors would be mounted on the mobile robot for guaranteeing that the people 
will not be hurt nor objects be damaged. Finally, the most important goal is to 
navigate the tour-guide robot in dynamic, clustered and crowded condition. 
By fusing the meaningful data of laser scanner, ultrasonic, odometer and wireless 
network, the vehicle is capable to achieve autonomous navigation or 
semi-autonomous navigation. When tour-guide robots move, the dead-reckoning 
methods based on wheel encoders have been used to keep track of the current position 
and orientation of the mobile robot. As the operator in the control room desires to 
monitor the mobile robot or command the robot, the wireless network would be used 
to change the trajectory immediately.  
The rest of this chapter is organized as follows. Section 6.2 introduces the 
dynamic programming for global path planning. Section 6.3 introduces the obstacle 
avoidance behavior using laser scanner and sonar. Section 6.4 briefly describes the 
hybrid navigation design. Section 6.5 describes experimental results and discussion. 
  
 
131
planning.  
 There are three types of maps utilized in mobile robot. Figure 6.1 shows a metric 
-topological map. The metric-topological map not only contains only information 
about the connectivity of places in the environment, but also the distances between the 
nodes. Our map representation adopts the metric-topological map due to its simplicity 
and   distance information. 
 Consider the problem of planning optimal paths for a mobile robot motion in a 
known environment. We use a dynamic programming approach to solve the optimal 
path problem. Dynamic programming approach utilized herein does not suffer the 
difficulties associated with the virtual local minima caused by the artificial potential 
field approach. The globally optimal path is guaranteed to be found via dynamic 
programming. The dynamic programming method is to begin from the terminal point, 
and to trace back the start point step by step. When this reverse searching method 
reached the starting point, all nodes have been already noted down its best next node 
to the destination and the searching algorithm is terminated. Therefore, we just only 
advance towards the next node that is noted down step by step from the terminal node, 
we can find an optimal route.  
 Our optimal path planning algorithm is based on the shortest distance criterion, 
define the following cost function.  
   
( )
( ) min { ( , ) ( )}
u v
MinDistance v w v u MinDistance uδ∈= +            (6.1) 
where MinDistance is the shortest distance of the node up till now; u  is the previous  
node and v  is the current node; ( )uδ  is the set of nodes in close proximity to node 
v ; ( , )w v u  is the distance from node v  to u . 
The procedure of path planning is described as following several steps: 
Step 1: Initialize and load the map, then transform real environment map into a metric 
  
 
133
 (a) Initialization and map loading.  
 
 
(c) Delete of impossible routes. 
 
(b) Selection of the starting and ending 
nodes. 
 (d) Finding the final optimal path.
Figure 6.3. Simulation results of each procedure for the proposed method 
6.3  Obstacle Avoidance Behavior Using Laser Scanner and Sonar 
  In this subsection, obstacles in the working environment of the robot are detected by a laser 
scanner and twelve ultrasonic rangers equipped on the robot. By fusing these sensing data with 
control algorithm, the robot could move around the obstacles without collision. Mobile robots 
navigation has been investigated by several researchers. Several methods like fuzzy algorithm, 
neural network algorithm, genetic algorithm et al. This obstacle avoidance behavior adopted the 
idea in [39]; the author presented an autonomous terrain navigation system for a mobile robot. He 
employs a two dimensional laser range finder (LRF) for terrain mapping. A so-called “traversability 
field histogram” (TFH) method is proposed to guide the robot. We change the (TFH) method to the 
traversability distance histogram (TDH). The main idea behind this method is to find the candidate 
hill and calculate the cost along each hill in TDH that was transfer form ranging finders data, and 
  
 
135
Measure the best 
candidate direction 
Filter 
(delete the noise)
Drive four wheels to the command 
direction and rotate to the right poseure
Terrain traversability 
analysis algorithm
Rough data
Measure sensors
Create two 
dimensional matrix
RS-232
 
Figure 6.4. The overall flow chart of the obstacle avoidance method. 
ih
[1] 0°[181] 180°
[91] 90°
robot
 
 Figure 6.5. The traversability map transformed from terrain map. 
 
  
 
137
 
The parameters of the mobile robot rotation vectors are shown as below: 
1
1
( ) cos( )
( ) sin( )
( )
h
h
w i h
x k vl k
y k vl k
k k kφ
= ×
= ×
= =



                           (6.2) 
where  k is the parameter designed to measure the velocity of the motor , vl  and hk  are obtained 
from the navigation algorithm. Once the information of x , y  and φ  is found by the navigation 
algorithm, the real voltage control commands, 1 2 3 4, , ,v v v v , for the four motor drive  are obtained 
from the inverse kinematical matrix in Chapter 3. Figures 6.7(a) and (b) depict the pictures of the 
environment detection and the terrain traversability distance analysis histogram. The program 
chooses the middle of the region which is the best direction to the way to go. The used variables of 
the traversability distance histogram are explained as below.  
: the th traversable distant data 
: the average of traversable distant data
[ , ] :  traversable sector
: velocity of robot
: the direction of moving
: the direction of goal from robot head
i
avg
R L
i i i
h
t
w
h i
h
v k k
vl
k
k
k
=
: angle velocity of robot
_ : nearly sector index
_ : widely sector index
_ : higher sector index
near id
wide id
high id
            (6.3) 
In the proposed navigation, a sector with the highest distances in Figure 6.7 is adopted to guarantee 
the mobile robot to move to the deepest region.  
 
6.4  Hybrid Navigation Design  
    For a tour-guide robot navigating over its crowded and dynamic working environment, it is 
necessary for the robot to acquire its localization with respect to a world frame and where can move 
  
 
139
 
  When the robot works at the autonomous navigation function, the scanning data are the main 
information used for behavior determination of hybrid navigation. If the distance between the 
destination and the current position is bigger than the environment data toward to the goal, it means 
there are some obstacles in the middle of the moving line. In this condition the parameters are given 
by goal environmentd d< , goald is the distance from the location to the goal in the global coordinate 
and environmentd  is the sensing information from laser scanning and ultrasonic data. Once this 
situation occurs, the autonomous navigation system would change its operation mode form the 
point-point trajectory tracking behavior to the obstacles-avoidance behavior. Otherwise the behavior 
will remain in the point-to-point trajectory tracking mode. The basic concept behind this idea is to 
choose the suitable linear velocity v (divided into x
i
, y
i
) and the angle velocity φi  for the 
tour-guide robot that it can complete the mission without crash. Figure 6.9 illustrates the basic 
concept for switching behavior between two behaviors. Finally, if the operator desires to monitor 
and control the robot, the remote control behavior via wireless network will be activated 
immediately, thereby steering the robot without autonomous navigation.         
  
 
141
0t
8t
7t
9t 12
t
11t
13t
6t
1t
2t
10t
4t
3t
5t
 
Figure 6.10. Petri-net model of executing autonomous navigation.  
 
Table 6.1. Description of places and transitions in Figure 6.10. 
Place Description 
P0 Standby. 
P1 Executing the autonomous navigation behavior.  
P2 State: the original via-point has not been changed. 
P3 State: normal localization. 
P4 State: abnormal localization. 
P5 Fault: the robot fails to find out its position. 
P6 Finishing the behavior of autonomous navigation.  
P7 State: the original via-point has been changed. 
P8 
Error: reach the temporary via-point and stop moving until the original 
via-point is restored. 
P9 
Fault: reaching the original via-point is failed since it is occupied by 
an obstacle. 
P10 Error: the path to the via-point does not exist. 
P11 Fault: the path to the via-point does not exist. 
P12 
Error: The autonomous navigation behavior can not extract the desired 
via-point from the set of via-points selected by dynamic programming 
  
 
143
whether if the next via-point is occupied by visitors, observing the planned path blocked by the 
visitors, and so on. Hence, the implementation of the autonomous navigation must incorporate all 
the core techniques of navigation, including self-localizer, local path planner, goal-seeking and 
obstacle- avoidance. Therefore, the Petri-net method is used to design the logic sequence 
constructing from the present states and event-driven conditions of the tour-guide robot in 
autonomous navigation.  
The Petri net was introduced at the Ph.D thesis of Carl Adam Petri in 1962. A Petri net also 
named place/transition net is the representation of discrete distributed system. A Petri net consists of 
place nodes, transition nodes, and directed arcs connecting places with transitions. Place may 
contain any number of tokens. A distribution of tokens over the places is called a marking. 
Transitions act on input tokens by a process of firing. When a transition fires, it consumes the 
tokens from its input places, performs some processing task, and places a specified number of 
tokens into each of its output places.  
As the concerned events increase, the robot control becomes more complex and the if-then 
logics are difficult to manage these situations. Therefore, the Petri net based configuration is 
developed such that the issues of the task decomposition, error/fault detection and recovery can be 
solved easily. Here, the autonomous navigation behavior must consider the events from the 
localizer, the path planner, and behaviors. Figure 6.10 and Table 6.1 represent the Petri net model of 
the autonomous navigation behavior. These tokens explicitly monitor the state of the behavior, the 
localizer, and the local path planner. If any event is transmitted from one of these components, the 
distribution of tokens is changed. The Petri-net based autonomous navigation behavior has 
following advantages: (i) the autonomous navigation configuration is event-driven and the states of 
the tour-guide robot can be divided into the possible discrete states thus could easy to check the 
states of the robot. (ii) this Petri-net model via tokens transition clearly indicate the state transition 
with respect to each event firing. 
 
  
 
145
 
(b) 
 
(c) 
obstacle
obstacle
obstacle
obstacle
obstacle
obstacle
obstacle
obstacle
obstacle
obstacle
obstacle
obstacle
obstacle
obstacle
  
 
147
Chapter 7  Human-Robot Interaction and Tour-guide Mode 
Execution 
 
7.1  Introduction 
Human-robot interaction (HRI) aims to construct funny and interesting interactions between 
the visitor and the robot. The basic goal of HRI is to develop a principle and algorithm to provide 
more natural and effective communication and interactions between the visitor and the robot. 
Traditional service robots are used for industrial proposes or repeating actions, thus require few 
human-robot interactions. Some illustrative robots have specially been designed to have human 
symbiotic interactions; for example, they are AIBO, Kismet, and Actroid robot. To date, more 
researchers have been increasingly interested in aspects of human-robot interaction involving 
speech, gesture movement, facial expressions, emotions and etc.   
    Figure 7.1 shows an interactive operation scenario for human-robot interactions of our 
tour-guide robot. The main functions of the interactive operation scenario include four parts: 
teleoperation, welcome/reception mode, facial expressions and dual arms swing, static information 
explore mode and dynamic tour guide mode. 
The rest of the chapter is organized as follows. In Section 7.2, the welcome/reception mode is 
briefly discussed. Section 7.3 illustrates 5-DOF facial expressions designed This section also state 
the 2-DOF arms swing control system with swing control of the upper arms swing and elbow-joint 
angle control of the lower arms. Section 7.4 presents static information explore mode. Section 7.5 
describes the designed procedure of the tour guide mode. Section 7.6 describes the experimental 
result. In Section 7.7 concludes this chapter. 
  
 
149
7.3.1  Facial Expressions  
As mentioned before, the facial expression module is a 5-DOF mechanism device adopted to 
construct several different facial expressions. The main function of the facial expression system 
module is to make funny and interesting human-robot interaction interface. Several amazing facial 
expression are designed to meet the requirements of the operation scenario, and increase 
interactions with the robot and the visitors. The robot then outputs the stored speech for the visitors 
to explain contents and materials of exhibitions, via the built-in mini-microphone in the facial 
expression system. 
     
7.3.1.1  Design and Experimentation  
    In this subsection, four facial expressions, such as wake-up, start-to-move, explanation and 
sleep, are designed for the corresponding tour-guide mode. Such facial expressions are 
accomplished by controlling the five RC servomotors to rotate toward their desired angles, which 
can be heuristically synthesized by the built-in functions in the facial expression subsystem. Figure 
7.2 shows the picture of the facial expression system. These four states of facial expression are 
elucidated as follows. While starting the tour- guide mode, the system will first enable the wake-up 
state shown in Figure 7.3(a), and then wait for visitors to choose which kind of the tour guide mode. 
Once the visitor has finished selecting the desire goal to tour guide via the touch screen, the facial 
expression module will display the start-to-move state shown in Figure 7.3(b); this state means that 
the tour guide robot is ready to move, and this module first looks around its surrounding and then 
say, for example, “The target is Lab 601. Let’s go!”. When the robot arrives at the target position, 
the explanation state in Figure 7.3(c) shows the nod and opens the mouth to explain the contents of 
exhibitions. Finally, while ending the tour, the robot disables the subsystem, namely that the sleep 
state shown in Figure 7.3(d) is enabled.   
  
 
151
    
    
(c) Explanation state. 
    
    
(d) Sleep state. 
Figure 7.3. Four kinds of facial expressions. (a) Wake-up state. (b) Start-to-move state. (c) 
Explanation state. (d) Sleep state. 
 
7.3.2  Dual-Arm Swing  
    Each arm in the dual arms is designed as a 2-DOF structure with its upper arms driven by one 
24VDC motor, and its lower arms driven by two DC motors. Unlike mechanical manipulators 
accurately catching desired objects, the swing-controlled  
  
 
153
   
               (a)                 (b)               (c) 
Figure 7.5. Swing behavior of the dual arms. (a)(b) Swing of the dual arms during navigation. (c) 
Two arms at the stop condition. 
 
7.3.2.1  Experiment Results of Dual-Arm Swing  
In this subsection, the dual-arm swing control is designed to increase the motion effect of the 
tour-guide robot while moving. When robot navigates in its working environment, the dual arms 
swings forth and back continuously and each elbow keeps the fixed angle, as shown in Figure 7.5(a) 
- (b). Once the tour-guide robot has reached the goal, the robot will stop and, meanwhile, then upper 
arms will stop swing where the elbow is relaxed downward the stop position, as shown in Figure 
7.5(c).  
 
7.4  Exploration Mode  
    The exploration mode provides static explanation of exhibition information in the museum via 
our pre-designed database. Sometimes visitors just want to know the information of the particular 
exhibitions without tour-guide operation. In this situation, the tour-guide robot functions like 
several kinds of existing information stations installed inside museums. Thus the exploration mode 
is necessary for providing the visitor exhibition information service without human intervention, 
thus saving much manpower.  
7.5  Tour-Guide Mode  
  
 
155
(a)                                (b) 
  
(b)                               (c) 
  
(d)                              (e) 
  
(f)                              (g) 
  
  
 
157
Chapter 8  Conclusions 
 
This report has presented techniques to establish a tour-guide robot with a four-wheeled 
omnidirectional mobile platform equipped with a laser scanner, ultrasonic ranging finder, 5-DOF 
facial expressions, and swing dual arms, ZigBee module and touching screen. The main hardware 
techniques developed in this report include the construction of four-wheeled motion base and the 
FPGA-based drive board because of reduction of the physical volume and the power consumption 
on the DC - DC conversion. For autonomous navigation, motion control, remote control, and 
obstacle avoidance have been developed. To achieve desired tasks execution, the human-robot 
interaction with an interactive operation scenario has been designed for effective use of the 
tour-guide robot. The 5-DOFs facial expression module and the 2-DOFs dual-arm module have 
been utilized to enhance interesting interactions with visitors. A ZigBee localization module and a 
SoPC-based adaptive motion controller incorporating with an embedded processor have been 
constructed for robot’s localization. Simulations and experimental results are used for illustration of 
the effectiveness and applicability of the proposed methods. The main results of the report are 
summarized as below.  
First, in order to enhance the flexibility of the mobile base, the four-wheeled omnidirectional 
mobile platform with its drive and motion controller has been successfully constructed.   
  Second, a nonlinear unified kinematical control method is presented for point stabilization and 
trajectory tracking of an omnidirectional wheeled mobile robot with four independent driving 
omnidirectional wheels equally spaced at 90 degrees from one place to another. After the 
kinematical model of the robot is established, the dead-reckoning method using the velocity 
information from the encoders directly mounted on the driving motors is developed to find the 
continuous position and orientation of the robot over short traveling distances. A unified nonlinear 
kinematical PI control law is proposed to achieve point stabilization and trajectory tracking.  
Several experimental results are conducted to show the efficacy and usefulness of the proposed 
  
 
159
 
 
 
  
 
161
[11] R. Siegwart and I. R. Nourbakhsh , Introduction to autonomous mobile robots, A Bradford 
Book, The MIT Press, Cambridge, Massachusetts, London, England, 2004. 
[12] F. G. Pin and S. M. Killough, “A new family of omni-directional and homonymic wheeled 
platforms for mobile robots,” IEEE Transactions on Robotics and Automation, vol.10, 
pp.480-489, August 1994. 
[13] http://www.digi.com/technology/rf-articles/wireless-zigbee.jsp. 
[14] Pinedo-Frausto, E.D. and Garcia-Macias, J.A. ,” An experimental analysis of ZigBee 
networks”, Local Computer Networks, 2008. LCN 2008. 33rd IEEE Conference, pp. 
723–729, Oct 2008. 
[15] F. Sottile and R. Giannantonio and M.A. Spirito and F.L. Bellifemine,” Design, deployment 
and performance of a complete real-time ZigBee localization system,” Wireless Days, 2008. 
WD '08. 1st IFIP, pp. 1-5,Nov. 2008. 
[16] J. F. Blumrich, Omnidirectional vehicle, United States Patent 3,789,947, 1974. 
[17] B. E. Ilou, Wheels for a course stable self-propelling vehicle movable in any desired direction 
on the ground or some other base, United States Patent 3,876,255, 1975. 
[18] M. West, H. Asada, “Design of ball wheel mechanisms for omnidirectional vehicles with full 
mobility and invariant kinematics,” Journal of Mechanical Design, pp. 119-161, 1997. 
[19] M. Wada, S. Mory “Holonomic and omnidirectional vehicle with conventional tires,” 
Proceedings of 1996 IEEE International conference on Robotics and Automation, pp. 
3671-3676, 1996. 
[20] Carlisle, B, “An omnidirectional mobile robot,” Development in Robotics, Kempston, 
pp.79-87, 1983. 
[21] F. G. Pin, S. M. Killough, “A new family of omnidirectional and holonomic wheeled 
platforms for mobile robot,” IEEE Transactions on Robotics and Automation, Vol. 15, No. 6, 
pp. 978-989, 1999. 
  
 
163
[31] S. S. Solano, A. J. Cabrera, I. Baturone, F. J. Moreno-Velo and M. Brox, “FPGA 
implementation of embedded fuzzy controllers for robotic applications,” IEEE Transactions 
on Industrial Electronics, vol.54, no.4, pp.1937-1945, August 2007. 
[32] Y. F. Chan, M. Moallem and W. Wang, “Design and implementation of modular FPGA-based 
PID controllers,” IEEE Transactions on Industrial Electronics, vol.54, no.4, pp.1898-1906, 
August 2007. 
[33] S. H. Han, M. H. Lee and R. R. Mohler, “Real-time implementation of a robust adaptive 
controller for a robotic manipulator based on digital signal processors,” IEEE Transactions on 
Systems, Man, and Cybernetics-Part A: System and Humans, vol.29, no.2, pp.194-204, March 
1999. 
[34] D. Zhang and H. Li, “A stochastic-based FPGA controller for an induction motor drive with 
integrated neural network algorithms,” IEEE Transactions on Industrial Electronics, vol.55, 
no.2, pp.551-561, February 2008. 
[35] C. F. Juang and C. H. Hsu, “Temperature control by chip-implemented adaptive recurrent 
fuzzy controller designed by evolutionary algorithm,” IEEE Transactions on Circuits and 
Systems-I: Regular Paper, vol.52, no.11, pp.2376-2384, November 2005. 
[36] C. F. Juang and J. S. Chen, “Water bath temperature control by a recurrent fuzzy controller 
and its FPGA implementation,” IEEE Transactions on Industrial Electronics, vol.53, no.3, 
pp.941-949, June 2006. 
[37] H. C. Huang and C. C. Tsai, “FPGA implementation of an embedded robust adaptive 
controller for autonomous omnidirectional mobile platform,” IEEE Transaction on Industrial 
Electronics, vol. 56, no. 5, pp. 1604-1616 ,May 2009. 
[38] Y. J. Feng , Motion Control, navigation and mission execution of a tour-guided robot with 
four-wheeled omnidirectional platform, M.S. Thesis, Department of Electrical Engineering, 
National Chung-Hsing University, Taichung, Taiwan, July 2008. 
  
 
165
proceeding of the 8th international conference on Automation Technology (Automation 2005), 
Taichung, Taiwan, pp.727-732, May 2005. 
  
 
167
之研發。所研發的系統軟硬體建置，系統整合與相關實務技術，不但有助於現今導覽機器人
系統的實際研製技術，同時可推廣應用於其他之服務機械人研發。 
 
 
 2
量測、遙測、網路感測系統、光電量測、非破壞檢測、線性系統、強健控制、最佳控制、適應
控制、非線性控制、觀測和追蹤、智慧型控制、學習控制、模糊控制、類神經網路控制、可變
結構、分散式控制、多變數控制、穩定度、離散事件動態系統、模式預測控制、與線性系統、
非完整控制系統等控制理論和其計算、實驗、應用等相關課題，至濾波理論、估計、追蹤、故
障偵測與隔離、模式辨識與驗證、等隨機技術領域研究，以至於如混合控制、飛行機器人，飛
行控制、動力系統、生物系統控制、製造系統、程序控制、彈性機械臂與結構控制，嵌入式系
統，DSP 應用等多方面的實務技術。在諸多的主題裡，筆者僅選擇相關有興趣機器人，感測與
轉換，非線性系統、預估控制、適應控制以及控制應用等相關的論文聆聽，參與相關的研討。
短暫的交流，對目前研究動態的掌握，未來研究方向的釐定，甚有助益。 雖是 筆者參與 SICE2008 的論文共兩篇，第一篇名為「Dynamic Modeling and Sliding-Mode Control 
of a Ball Robot with Inverse Mouse-Ball Drive」，隸屬於 8 月 22 日（星期一）上午的『Intelligent 
methods for Autonomous Mobile Robots』Session，由博士後研究員廖慶文與本人共同發表，其內
容為探討在同時驅動情況下，球型機器人的雙驅動數學模型，以及提出以階層滑動模式控制器，
使該球型機器人可自我平衡，路徑追蹤與點對點追蹤。第二篇名為「Global Posture Estimation of 
a Tour-guide Robot using RFID and Laser Scanning Measurements」，隸屬於 8 月 22 日同一時段內
（『Intelligent methods for Autonomous Mobile Robots』Session），由本人發表，其內容為一提
出以 RFID 的靜態初值估測方法，再結合 Laser 掃描資料，達成有效率的全域定位技術，並進
行該技術之實作技術說明與實驗。 
    主辦單位為加強學者專家、與會者間的交流，特別在新宿區Washington 飯店會客廳安排與
會者共三百餘名，在輕鬆情況下共進宴會晚餐，互增友誼，眾人互飲言歡暢談，預約一年後福
岡見。主辦單位在全部議程結束後，有閉幕式以及歡送宴會，以感謝所有與會者的蒞臨。 
 
（四）與會心得 
儀表、量測，自動控制科技、資訊技術之科技是具傳統性，又能不斷地隨時代的需求推陳
出新的重要技術。它對人類的影響無遠弗界。在此次的研討會中，相當多的研究主題是儀表、
量測，自動控制科技、資訊技術的各種應用，可預測的是未來朝往簽入式系統，生醫控制系統
與訊號處理、交換系統之控制設計與設計、非線性系統控制與估測、認知與通訊、機電整合、
機器人相關技術、工業系統，交通系統與車輛、生物與生態系統、社會系統的控制研究課題者
將逐漸增多。 
    本次SICE2008 年會暨儀表、控制與資訊科技國際研討會，相當成功。會場寬敞，各樣議
程順暢，。本次大會的三場大會演講與六場邀請演講，節有獨到的技術發展，亦有相當有趣的
技術，相當值得聆聽。130個技術會議(Technical Session)不只有嶄新的理論，亦有來自許多著
名產業廠商與業界的實際應用範例。展示場有許多來自日本廠商的展示攤位，亦有來自研究單
位的研究成果，但僅有少數的書商展示攤位。 
與SICE2008年會暨儀表、控制與資訊科技國際研討會比較，目前國內所辦的自動控制的場
次較少，規模亦小，但仍逐年漸長，目前已設立學生論文獎與實作競賽，也有安排書商與廠商
的展示場，技術方面的研討已相當興盛，博碩士學生參與研討會也相當熱烈。但國內業界參予
研討會的人數不多，若能邀請對控制應用實務的業界研究員，講述其控制設計實務經驗，以增
加與會專家學者的另類觀點。 
兩年後，該SICE2010將於台灣圓山大飯店舉辦。本人將擔任該次(SICE Annual Conference 
2010)的國際會議共同議程主席，期望為此一台灣與東北亞國家，在儀表、量測，自動控制科技、
資訊技術、各領域應用實務之的研究學者，能提供更好文化與科技之交流場所與節目。 
 
（五）攜回資料 
參加此次研討會帶回 Reprints of the SICE2008 年會暨儀表、控制與資訊科技國際研討會 
  
 
Paper presentation in the session room of SICE Annual Conference 2008 
 
 
 4
 2
統，Human-centered systems, 醫學機電，Swarm 智慧，Granular 計算，複雜分析與認知心理，
醫學資訊學，機器人系統，合作系統，無線網路系統，離散事件動態系統、模式預測控制控制
理論和其計算、實驗、應用等相關課題，至智慧濾波理論、模式辨識與驗證、混合控制、生物
系統控制、製造系統、嵌入式系統，DSP 應用等多方面的實務技術。在諸多的主題裡，筆者僅
選擇相關有興趣的仿生技術，影像處理，感測網路，機器人，學習控制，智慧控制等相關的論
文聆聽，參與相關的研討。雖是短暫的交流，對目前研究動態的掌握，未來研究方向的釐定，
甚有助益。 
    筆者參與 SMC 的論文共兩篇，第一篇名為「Adaptive Nonlinear Control Using RBFNN for an 
Electric Unicycle」，隸屬於 10 月 14 日（星期二）上午的『Intelligent control 』Session，由我本
人與林水春共同發表，其內容為探討電動獨輪車的數學模型，以及提出以適應非線性控制法則，
使該獨輪車，可進行平衡前進。 
    主辦單位為加強學者專家、與會者間的交流，特別在研討會會場內安排與會者共四五百餘
名，在輕鬆情況下共進宴會晚餐，互增友誼，眾人互飲言歡暢談，預約一年後美國德州San Antonio
見。主辦單位在全部議程結束後，有閉幕式以及歡送宴會，以感謝所有與會者的蒞臨。 
 
（四）與會心得 
系統人與仿生技術之科技是一不斷地隨時代的需求推陳出新，符合人性需求的重要技術。
它對人類的影響無遠弗界。在此次的研討會中，相當多的研究主題是系統人與仿生技術的各種
理論，技術與應用，可預測的是未來朝往學習與控制，智慧機器人，認知與通訊，各種仿生科
學理論等研究課題者將逐漸增多。 
    本次SMC2008 國際研討會，相當成功。會場寬敞，各樣議程順暢。本次大會的三場大會
演講，皆有獨到的技術發展，亦有相當有趣的技術，相當值得聆聽。100個技術會議(Technical 
Session)不只有嶄新的理論，亦有來自許多著名產業廠商與業界的實際應用範例。展示場有許
多來自日本廠商的展示攤位，亦有來自研究單位的研究成果，亦有書商展示攤位。 
與SMC2008年會國際研討會比較，目前國內所辦的系統技術的場次較少，規模亦小，但仍
逐年漸長，也有安排書商與廠商的展示場，技術方面的研討已相當興盛，博碩士學生參與研討
會也相當熱烈。但國內業界參予研討會的人數不多，若能邀請對控制應用實務的業界研究員，
講述其控制設計實務經驗，以增加與會專家學者的另類觀點。 
一年後，該SMC2009將於美國德州San Antonio 舉辦。本人將擔任該次國際會議獎勵委員會
共同主席，期望為此研討會盡力，使該研討會更成功。。 
 
（五）攜回資料 
參加此次研討會帶回 e-proceeding of the SMC2008 國際研討會，內容包括所有的會議論文。 
 
（六）Some pictures in the IEEE SMC 2008 . 
 
