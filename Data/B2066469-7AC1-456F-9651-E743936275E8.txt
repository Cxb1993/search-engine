results show the proposed approach has great promise 
in unsupervised classification. 
英文關鍵詞： Fisher＇s linear discriminate analysis (FLDA), 
Gaussian pyramid, Iterative Fisher＇s linear 
discriminate analysis (IFLDA), Pixel purity index 
(PPI). Support vector machine (SVM). 
 
前言 
高頻譜影像在遙測領域中已經成為一種先進的技術，由於在台灣缺乏潛在的應用，目前仍然是屬於初
期發展的階段。由於高頻譜影像遙測技術已漸趨成熟，高頻譜影像實際應用上包括廣泛的軍事和民間
應用。其中特別感興趣的是在資料中搜索屬於小而不易發現的目標。一般而言低出現機率的目標物。
這類的目標常出現在農業和生態學方面。然而在非監督式的分類方法中，有兩項重要的議題，第一是
在毫無背景知識的資料中如何產生訓練樣本。第二是如何尋找適當的分類器將資料做有效的分類。為
解決第一項議題，我們運用了純度像素索引(pixel purity index)簡稱PPI做為擷取訓練樣本的主要演
算法來擷取一組適合的訓練樣本。再者，我們使用支援向量器(SVM)以及費雪線性辨別分析Fisher’s 
linear discriminate analysis (FLDA)來解決第二項議題，實驗結果顯示我們所提出的方法結合 
(FLDA)將資料在非監督式的過程中可獲得較好的分類。 
關鍵詞：高頻譜影像，純度像素索引，費雪線性辨別分析，支援向量器，監督式的分類 
 
ABSTRACT 
Two major challenging issues arise in unsupervised classification. One is how to generate desired 
knowledge directly from the data in an unsupervised manner. The other is how to find an appropriate 
follow-up classifier to use the obtained unsupervised knowledge to perform supervised classification.This 
report presents a new approach to unsupervised classification for multispectral imagery. To address the first 
issue the pixel purity index (PPI) which is commonly used in hyperspeftral imaging for endmember extraction 
is used to find a good set of initial training samples without prior knowledge. It first uses a Gaussian pyramid 
multi-resolution technique to reduce image size from which the pixel purity index (PPI) is implemented to 
find regions of interest (ROIs) with PPI counts greater than zero. To address the second issue, these PPI-found 
samples are further used as support vectors for a support vector machine (SVM) to classify data. The resulting 
SVM-classified data samples are further processed by a new designed iterative Fisher’s linear discriminate 
analysis (IFLDA) which implements FLDA in an iterative manner to refine classification results. The 
experimental results show the proposed approach has great promise in unsupervised classification. 
 
Keywords: Fisher’s linear discriminate analysis (FLDA), Gaussian pyramid, Iterative Fisher’s linear 
discriminate analysis (IFLDA), Pixel purity index (PPI). Support vector machine (SVM). 
 
be used for endmember extraction in hyperespectral images turns out to be an advantage for PPI to extract 
training samples instead of endmembers from multispectral images. It is our brief that PPI may be one of very 
few endmember extraction algorithms used in hyperspectral imaging that is actually applicable to 
multispectral imagery.  
This report takes advantage of PPI described above to derive an interesting endmember extraction-based 
unsupervised classification technique for multispectral imagery. It makes use of PPI to find samples of 
interest to be considered as seed training samples for the data. These PPI-found samples are then used as 
support vectors to train a kernel-based SVM [13] to produce a set of initial training samples where the number 
of classes, which can be determined by the full band dimensionality of multispectral imagery. Since PPI uses 
a random generator to produce skewers on which data samples are orthogonally projected, the PPI-found 
samples are not reproducible. In order to resolve this inconsistency issue, an iterative Fisher’s linear 
discriminant analysis (IFLDA) is further developed to implement FLDA [14] iteratively to mitigate the 
instability caused by the use of skewers and the sensitivity of using PPI-found samples as support vectors by 
SVM. These resulting IFLDA-classified sample vectors are then used as a final set of training samples which 
are in fact obtained by a series of processes by first finding seed samples via PPI, then by using SVM to 
generate initial training samples which are further refined and corrected by IFLDA to produce a final desired 
set of training samples. So, an algorithm implementing PPI coupled with SVM in conjunction with IFLDA to 
find unsupervised training samples is called PPI-SVM-IFLDA algorithm. Finally, these 
PPI-SVM-IFLDA-produced samples are then used as training samples for a follow-up classifier to perform 
supervised classification on the original data samples in which case this supervised classifier is once again 
implemented by IFLDA with only difference in that this IFLDA is now performed on the entire data space 
compared to the IFLDA used in the PPI-SVM-IFLDA which only performs the classification on 
SVM-generated training samples, not original data samples.  
On many occasions the image data size is too large for PPI to process due to its high computational 
complexity. To alleviate this problem, a Gaussian pyramid image processing [15] is proposed to reduce the 
original image size to help users to select appropriate regions of interest (ROIs). This method not only can 
save processing time, but also can effectively pinpoint ROIs. Such benefits will be demonstrated by 
experiments. 
It should be noted that PPI cannot work alone by itself. It must be coupled with SVM and implemented in 
 Due to convexity, all the sample vectors inside the triangle in Fig. 1 have their PPI counts = 0 which 
implies that they are linear mixtures of the three endmembers located at the vertices of the triangle indicated 
by dashed lines. It should be noted that end points of a skewer are those that yield either the maximal or 
minimal value projected by all the sample vectors. Also, a projection can be positive or negative depending 
upon whether the projection occurs in the same or opposite direction of a skewer. 
However, in order for the PPI to be effective, a large number of skewers, K are generally required so that 
these skewers can cover as many random directions as possible. For example, in Fig. 1 the shade sample 
vector labeled by x outside the triangle has the same NPPI = 1 as e1 and e2 do, it is obviously not an 
endmember. The occurrence of such an incident is the result of insufficient numbers of skewers used for the 
PPI. Unfortunately, by far there is no guideline suggested for users to determine how many skewers should be 
used for the PPI. Empirically, it generally requires hundreds if not thousands of skewers for the PPI to 
perform well in hyperspectral data exploitation. 
 Now, we briefly describe how to implement PPI algorithmically. First of all, randomly generate a set of 
K unit vectors called “skewers”, { }Kkk 1=skewer  where K is a pre-assumed sufficiently large positive integer. For 
each skewerk, all the data sample vectors are projected onto skewerk to find sample vectors whose projections 
occur at its extreme positions and form an extrema set for this particular skewerk, denoted by ( )kextremaS skewer . 
Despite the fact that a different skewerk may generate a different extrema set ( )kextremaS skewer , it is very likely 
that some sample vectors may appear in more than one extrema set. Define an indicator function of a set S, 
( )rSI  by 
( )



∉
∈
=
S
S
SI r
r
r
 if ;0
 if ;1
 
, ∑= k S kextremaIN )()( )(PPI rr skewer                                                   (1) 
where )(rPPIN  is defined as the PPI count of sample vector r. Finally, let t be a threshold value set for the 
PPI count to determine if a data sample vector is an endmember. Extract all sample vectors with tN ≥)(PPI r  as 
endmembers.  
Since the details of the specific steps to implement ENVI’s PPI are not available in the literature, a 
MATLAB version of the PPI described below is only based on the limited published results and our own 
interpretation. Nevertheless, both our algorithm and the PPI with ENVI 3.6 produce the same results. 
MATLAB PPI Algorithm 
1. Initialization:  
It should be noted that (2) is modified from (2) by replacing decision 0 with decision 1 when 
0)( <+== bgy i
T
ii rwr  so that two decisions in (2) can be combined to one equation (1). 
By virtue of (2)-(3), the linear separability problem for the SVM is to find an optimal weight vector w* that 
minimizes  
( ) ( ) 2||||2/12/1( wwww) ==Φ T                                                               (4) 
subject to constraints specified by (4). 
The above SVM is developed to separate two classes which are linearly separable. However, in many 
applications, such desired situation may not occur. In other words, some data sample vectors fall in the region 
within the distance less than ρ from the hyperplance or even on the wrong side of the hyperplane. These data 
sample vectors can be considered to be either bad or confusing data sample vectors and they cannot be 
linearly separated. In this case, the SVM developed for linear separable problems must be re-derived to take 
care of such confusing data sample vectors. In doing so, a new set of positive parameters, denoted by { }nii 1=ξ  
and referred to as slack variables must be introduced to measure the deviation of a data sample vector from 
the ideal condition of linear separability, in which case 0<iξ .If 10 ≤≤ iξ , the i
th data sample vector xi falls 
in within the region with distance less than margin of separation but on correct side of the decision surface 
specified by the hyperpalne. On the other hand, if 1>iξ , the i
th data sample vector xi falls on the wrong side 
of its decision surface. In light of the mathematical interpretation, these issues can be addressed by the 
following inequalities 
( ) iiTi bd ξ−≥+ 1rw  for  ni ≤≤1                                                            (5) 
0≥iξ  for ni ≤≤1 .                                                                     (6) 
By incorporating (5)-(6) in (4) the object function Φ(w) can be modified as 
( ) ∑ =+=Φ
n
i i
T C
1
2/1( ξwww)  with 0>C .                                                     (7) 
By virtue of (5)-(7) a linear non-separable problem solved by the SVM can be described as follows. 
Given a set of training pool, { }niii d 1),( =r , find an optimal weight vector w
SVM and bias bSVM such that they 
satisfy the constraints specified by (5)-(6) and such that the pair of (wSVM,bSVM) and { }nii 1=ξ  minimizes (7). Or 
an alternative form can be obtained as follows. 
Given a set of training pool, { }niii d 1),( =x , find a set of Lagrange multiplier vector ( )
T
nααα ,,, 21 =α  that 
maximizes Q(α) given by 
( )∑ ∑∑ = == −=
n
i
n
j j
T
ijij
n
i i
ddQ i1 11 2/1)( rrα ααα                                                    (8) 
subject to the following constraints 
∑== s
n
i
s
i
s
ii d1
SVMSVM rw α                                                                   (16) 
where ns is the number of support vectors. 
The key issue to solve the kernel-based SVM is to determine the kernel used in (16). Interestingly, 
according to Mercer’s theorem, if a kernel-based inner product K(x,y) is continuous and symmetric in a closed 
interval ba ≤≤ yx, , K(x,y) can be expanded in a series  
)
1
()(),( ∑∞== j jjjK yxyx φφλ                                                                (17) 
where { }jλ  are eigenvlaues and { }jφ  are their associated eigenfunctions. 
There are three commonly used kernels of K(r,ri) defined in (17). 
1. Polynomial learning machine, ( )piT rr+1 . 
2. Radial basis function neural network, 





−− 22 ||||2
1
exp irrσ
. 
3. Two-layer perceptron, ( )iT rr10tanh ββ + . 
 
4. ITERATIVE FLDA (IFLDA) 
The idea of designing an iterative FLDA is derived to resolve two issues arising in training samples 
described in Section III. As noted earlier, the FLDA requires a large number of training samples. This need 
can be accommodated by taking classification results from running KSVM on a small number of training 
samples. However, since the training samples used by the KSVM are assumed to be worst-case samples, the 
KSVM-produced classification results may not represent desired data statistics for FLDA. The IFLDA allows 
the FLDA implements the FLDA iteratively in the sense that the classification results produced by the FLDA 
in previous stage are used as training samples for next stage FLDA implementation. The iterative process will 
be terminated when the classification results produced by FLDA in two consecutive runs remain unchanged. 
Additionally, on some cases, due to random selected set of training samples the final classification may not be 
reproducible and the results are not consistent. Another advantage of using the IFLDA is to mitigate this 
dilemma. The details of implementing IFLDA step-by-step are described in the following with a flow chart 
depicted in Fig. 3. 
 
IFLDA 
1.Initial condition:  
independent component analysis (ICA) by making the sample zero to remove the 1st order of statistics as well 
as making the data variance of each band image unity to remove the 2nd order of statistics. After the data is 
sphered, the PPI is then applied to find all the samples with their PPI counts, NPPI(r) greater than zero, denoted 
by PPIir , i.e., 0)(
PPI
PPI >iN r . The set of { }PPIir  is used as initial training samples for an SVM to produce a 
final set of training samples, denoted by { }trainingir  for a follow-up supervised classifier, Fisher’s linear 
discriminant analysis (FLDA) [4] which will be implemented iteratively. 
 
Unsupervised PPI-IFLDA Classification Algorithm 
1. Initial condition: Set the number for classes to the number of spectral bands. 
2. Sphere the data. 
3. Operate PPI on the sphered data to find { }PPIir . 
4. Implement an SVM using { }PPIir  as training samples to produce a final set of training samples 
{ }trainingir . 
5. Apply FLDA using { }trainingir  as training samples to perform supervised classification. 
6. Repeat step 5 until two consecutive runs of FLDA produce the same classification results. 
 
It should be noted that the procedure of running steps 5 and 6 is called Iterative FLDA (IFLDA). Fig. 3 
depicts a flow chart of the Unsupervised PPI-IFLDA Classification Algorithm. 
 
 
 
Figure 3. Flow chart of Unsupervised PPI-IFLDA Classification Algorithm 
Generate Training Samples by PPI 
 
Classfication 
 
Support Vector Machine 
 
Data Sphering 
 
Select ROI 
 
Fisher Linear Discriminant Analysis 
 
iteratively 
 
Figure 4.  A SPOT image of a mountain area 
 
Fig. 4(c) shows the bright samples extracted by the PPI with their PPI counts greater than zero. These 
PPI-extracted samples are then used as training samples for an RBF kernel-based SVM(σ=0.5, C=1) to refine 
training samples into 4 classes shown in Fig. 4(d) where the number of classes, 4 was determined by the 
number of bands which is 4. The samples classified by the SVM in Fig. 4(d) were assumed to be training 
samples for each of 4 classes for a follow-up supervised FLDA. Fig. 5 shows the classification results of the 
IFLDA with 5 iteration results produced by the FLDA. 
 
    
1st iteration of FLDA 
    
2nd iteration of FLDA 
    
3rd iteration of FLDA 
    
4th iteration of FLDA 
                 
bare land      vegetation     low density grass    shadow 
5th iteration and final classification results 
Figure 5. Classification by IFLDA using PPI 
 
Since no prior knowledge was used, the Google Earth was used to verify the results where the 4 classes were 
identified as bare land, vegetation, low density grass and shadow. Interestingly, according to the ground truth 
        
(b) ROI of 64x64 pixels      (c) PPI-extracted samples 
    
(d) training samples of 4 classes produced by RBF kernel-based SVM(σ=0.5, C=1) 
Figure 7.  A SPOT image of Mill Creek Park 
 
    
1st iteration of FLDA 
    
2nd iteration of FLDA 
    
3rd iteration of FLDA 
    
4th iteration of FLDA 
    
5th iteration of FLDA 
                        
                Buildings        vegetation         roads             lakes 
final classification results 
Fig. 8 Classification by IFLDA 
 
Supervised method 
In supervised method, we select 9 pixels in each class as training samples according to Google Earth. Fig. 
10 shows the location of training samples. Fig. 9 is the flowchart of supervised method.  
     
1st iteration of FLDA 
    
2nd iteration of FLDA 
    
3rd iteration of FLDA 
    
4th iteration of FLDA 
    
5th iteration of FLDA 
    
buildings  getation   roads     lake 
final classification results 
Figure 11. Classification by IFLDA using supervised method 
 
Since no prior knowledge was used, the Google Earth was used to verify the results where the 4 classes were 
identified as buildings, vegetation, roads and lake. Fig. 12 is the ground truth obtained from Google Earth. 
 
 
Fig. 12 The ground truth of Mill Creek Park 
Geoscience and Remote Sensing Letters, vol. 7, no. 2, pp. 324-328, April 2010. 
[8] M.E. Winter, “N-finder: an algorithm for fast autonomous spectral endmember determination in 
hyperspectral data,” Image Spectrometry V, Proc. SPIE 3753, pp. 266-277, 1999. 
[9] W. Xiong; C.-I Chang; C.-C. Wu, K. Kalpakis, K. and H. M. Chen, “Fast algorithms to implement 
N-FINDR for hyperspectral endmember extraction,” IEEE Journal of Selected Topics in Applied Earth 
Observations and Remote Sensing, vol. 4, no 3, pp. 545 – 564, 2011. 
[10] J.M.P. Nascimento and J.M. Bioucas-Dias, “Vertex component analysis: a fast algorithm to unmix 
hyperspectral data,” IEEE Trans. Geoscience and Remote Sensing, vol. 43, no 4, pp. 898-910, April 2005 
[11] C.-I Chang, C. Wu, W. Liu and Y.C. Ouyang, “A growing method for simplex-based endmember 
extraction algorithms,” IEEE Trans. on Geoscience and Remote Sensing, vol. 44, no. 10, pp. 2804-2819, 
October 2006. 
[12] R.A. Schowengerdt, Remote Sensing: Models and Methods for Image Processing, 2nd ed. New York: 
Academic, 1997. 
[13] S. Haykin, Neural Networks: A Comprehensive Foundation, 2nd ed., Prentice-Hall, 1999, Chapter 6. 
[14] Duda, R.O. and P.E. Hart, Pattern Classification and Scene Analysis, John Wiley & Sons, 1973. 
[15] P. Burt and T. Adelson, "The Laplacian pyramid as a compact image code", IEEE Trans. 
Communications, vol. COM-9, no. 4, pp. 532–540, 1983. 
[16] V.N. Vapnik, Statistical Learning Theory, John Wiley & Sons, 1998. 
[17] H. Ren, Q. Du, J. Wang, C.-I Chang and J. Jensen, “Automatic target recognition hyperspectral imagery 
using high order statistics,” IEEE Trans. on Aerospace and Electronic Systems, , vol. 42, no. 4, pp. 
1372-1385, Oct. 2006. 
[18] A. Hyvarinen, J. Karhunen and E. Oja, Independent Component Analysis, John Wiley & Sons, 2001. 
[19] C.-H. Li, H.-H. Ho, Y.-L. Liu, C.-T. Lin, B.-C. Kuo and J.-S. Tau, “An automatic method for selecting 
the parameter of the normalized kernel function to support vector machines,” J. of Information Science 
and Engineering, vol. 28, pp. 1-15, 2012. 
[20] C.-I Chang and Q. Du, "Estimation of number of spectrally distinct signal sources in hyperspectral 
imagery," IEEE Trans. on Geoscience and Remote Sensing, vol. 42, no. 3, pp. 608-619, March 2008 
 
 
出國報告(出國類別：國際會議)  
 
 
 
參加2012年IEEE地理遙測國際研討會 
 
 
 
 
 
 
 
 
 
 
 
服務機關：中興大學電機系 
姓名職稱：歐陽彥杰教授 
派赴國家：德國 
出國期間：民國101 年 7 月 22 日至7 月29 日 
報告日期：民國101 年 8 月 1 日
 2 
一、目的 
本次出國主要目的是參加一年一度由國際電機電子工程師學會 (IEEE)主辦的
IEEE地理遙測會議(IEEE Geoscience and Remote Sensing Society and the IGARSS 
2012) 並在會議中張貼發表論文，該會議每年均舉行一次，本次會議適逢32週
年。國內學者參與此研討會大約十多人，IEEE IGARSS 2012將聚集來自全球2千
位在遙測領域的世界級科學工程師及教育界人士討論新技術及新知識，該會議
已舉辦31屆為IEEE遙測領域最重要的年度盛會。國內學者參與此研討會大約十多
人，會議涵蓋五整天且天天會議行程滿檔，每篇張貼論文發表均須張貼兩天，大
會同時安排每篇張貼論文需要對論文議程主席做20分鐘的口頭報告，並需要從
17:20-19:00站在自己張貼論文前發表，大會同時準備豐富點心以吸引人潮因此會
場更是人山人海，每一個張貼論文前討論聲不斷並期待未來有進一步合作之機
會。 
 
二、 參加會議經過 
 7 月22 日晚上入關，7 月23 日早上8:30點到達會場報到，大會地點位於慕尼黑
國際會展中心(International Congress Center Munich, ICM) ， 慕尼黑為一個著名的
觀光景點，也是德國南部巴伐利亞省的首都，工商業相當發達，夏季氣候宜人
遊人如幟，清風拂來令人心曠神怡真是一個適合居住的好地方。雖然如此但我
們仍然未忘記此行最重要的目的就是學術交流。九點參加大會opening Session,
本次會議共收到3370篇的論文摘要總數其中大約有1495篇的論文被接受在會議
中發表，出席人數比去年大幅增加。其中論文口頭發表(Oral)有288個場次的1440
篇論文，海報(Poster) 展示總數146個共1752篇論文。論文主題包括：remote sensing 
of land, oceans, atmosphere and cryosphere, electromagnetic modeling, advanced image 
processing, design of sensors and missions as well as specialized applications, education 
and policy等。為開幕式由Dr. Alberto Moreira and Dr. Yves-Louis Desnos - General 
Co-Chairs發表 Welcome from the Bavarian Government接下來由IGARSS President, 
Jón Atli Benediktsson, 發表 Welcome from IEEE GRS Society EEE President-Elect  
2012, Dr. Peter W. Staecker發表Welcome from IEEE並由Dr. Werner  Wiesbeck,頒發
2012 IEEE Fellows 
2012 IEEE W.R.G. Baker Paper Award 2012 IEEE GRSS Education Award 
2012 IEEE GRSS Distinguished Achievement Award等獎項接下來就是Plenary 
Session10:50 Remote Sensing for a Dynamic Earth Johann-Dietrich Wörner, Chairman of 
the Executive Board of the German Aerospace Center (DLR) 11:20 The European Earth 
Observation Program Volker Liebig, Director of Earth Observation Programmes, European 
Space Agency (ESA) 11:50 Recent Progress and Future Opportunities in Earth 
Observations Ghassem Asrar, Director of the World Climate Research Programme 
(WCRP) 
。下午開始為各技術場次(Technical Session)，內容豐富充實。13:30-15:00參加Session 
 4 
聽取簡報了解其研究領域與成果。 
2. Space Applications Mission Directorate, JAXA.  
聽取簡報了解其研究領域與成果，並探討合作可行性。 
3. The CEOS Visualization Environment (COVE) 
聽取簡報了解其研究領域與成果。 
   
Fig. 5 大會指示版                      Fig. 6 大會展示廳 
   
Fig. 7 本人及論文Poster展示           Fig. 8  論文Poster展示 
   
Fig. 9 論文Poster展示及討論        Fig. 10 論文Poster展示及討論 
 
四、與會心得及建議 
此次參加在德國Munich所舉辦的IEEE Geoscience and Remote Sensing Society and 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/11/02
國科會補助計畫
計畫名稱: 非監督式遙測影像分類技術研發
計畫主持人: 歐陽彥杰
計畫編號: 100-2221-E-005-084- 學門領域: 影像處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
參加 IGARSS2012,Munich, Germany, 研討會並發表論文，相關研究發表於 IEEE 
JSTAR 期刊。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
