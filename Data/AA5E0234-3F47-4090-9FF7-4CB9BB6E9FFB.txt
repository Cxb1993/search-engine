中文摘要 
本計畫主要是在網路分類目錄下對於影像內容搜尋系統之研究。首先對影像偵測物件所在的區
域，再擷取影像特徵，其可避免背景雜訊的干擾，在物件偵測上使用兩種方法：分別為 AdaBoost 偵
測器和 Edge + PCA 演算法。AdaBoost 偵測器是特定物件的偵測演算法，可以從大量簡單特徵為主
的弱分類器裡，經過 AdaBoost 演算法，合併有效的弱分類器形成一個快速並精確的強分類器。
Edge + PCA 演算法是先對影像做邊緣偵測，再利用 Principle Component Analysis (PCA) 找出影像中
邊緣密集的區域。本論文更進一步對系統回傳的影像利用 SIFT 特徵比對進行重新排序，排序後的
影像是顏色與紋理都和查詢影像最相似的結果。實驗將對兩種物件偵測方式所建立的系統與重新排
序的結果之效能進行比較，並實作手機上的搜尋介面，達成無所不在的行動搜尋。 
英文摘要 
Recently, both online auction and camera-phones become popular. Mobile visual search combines these 
two factors to provide users just-in-time information anytime and anywhere they see a cool stuff. This 
project studies object detection for categoried web directories in the development of a mobile visual search 
system. The database images are processed by object detection to find the object region, in which the color 
features can be extracted without the influence of noises from the background. We propose two approaches 
for object detection. The first approach, called AdaBoost detector, is a machine learning algorithm that can 
be trained to detect objects of a specific class. Several weak classifiers are combined into a fast and accurate 
strong classifier via AdaBoost algorithm. The second approach, called "Edge + PCA (Principle Component 
Analysis)", first detects the edges of an image and then fits an ellipse region where most edges appear by 
PCA algorithm. In order to search database images similar to the query image in both color and texture, we 
use SIFT (Scale-invariant Feature Transform) to re-rank the returned images. The performance of image 
search systems based on AdaBoost detector and Edge + PCA algorithm are evaluated respectively. A 
prototype image search system is realized in a camera-phone as the first step to accomplish ubiquitous 
mobile search. 
報告內容 
本計畫研究在網路分類目錄對影像內容做物件偵測與概念學習來改進影像搜尋。原本預計為兩
年期計畫，分別執行物件偵測與概念學習兩種方式，由於計畫僅通過一年，故僅實現利用物件偵測
來改進對於網路分類目錄的搜尋。計畫中使用的資料庫是 Yahoo 拍賣下的分類目錄，我們對於選定
的兩種類別：衣服和帽子，分別利用 AdaBoost 訓練出物件偵測器。AdaBoost 是文獻上已知對於人
臉偵測快速又有效率的方式，但是對於拍賣網頁的圖片而言，物件有多種拍攝的角度以及各種型態
及紋理，故偵測率僅能達到七成左右，無法偵測到物件的圖片，我們利用 PCA 演算法計算出圖片中
物件可能所在的區域。本計畫分別產出兩篇會議論文 [1][2]，一篇碩士論文 [3]。相關研究內容請參
考附錄論文。 
參考文獻 
[1] 周怡君, 梁尹蓁, 劉震昌, “應用於拍賣網頁分類目錄下的影像搜尋系統”, TANET, Taiwan, 2008. 
[2] 許耀仁, 梁尹蓁, 劉震昌, “基於物件偵測技術的行動視覺搜尋”, TANET, Taiwan, 2009. 
[3] 許耀仁, 應用物件偵測於拍賣網頁分類目錄下的影像搜尋系統, 碩士論文, 暨南大學資工系, 2009. 
基於物件偵測技術的行動視覺搜尋 
 
許耀仁    梁尹蓁    劉震昌 
 國立暨南國際大學資訊工程系 
Taiwan, R.O.C. 
{s96321517, s94321908, jcliu} @ncnu.edu.tw 
 
 
摘要 
近年來，網路拍賣的盛行及行動裝置的普及化，利用行動
視覺搜尋技術，讓使用者可以更方便、即時可獲得所想要
購買的物品之相關資訊。本論文主要是在行動視覺搜尋技
術上對於影像內容搜尋系統之研究。首先對影像偵測物件
所在的區域，再擷取影像特徵，其可避免背景雜訊的干
擾，在物件偵測上使用兩種方法：分別為 AdaBoost 偵測
器和 Edge + PCA 演算法。AdaBoost 偵測器是特定物件
的偵測演算法，可以從大量簡單特徵為主的弱分類器裡，
經過 AdaBoost 演算法，合併有效的弱分類器形成一個快
速並精確的強分類器。Edge + PCA 演算法是先對影像做
邊緣偵測，再利用 Principle Component Analysis (PCA) 
找出影像中邊緣密集的區域。本論文更進一步對系統回傳
的影像利用 SIFT 特徵比對進行重新排序，排序後的影像
是顏色與紋理都和查詢影像最相似的結果。實驗將對兩種
物件偵測方式所建立的系統與重新排序的結果之效能進行
比較，並實作手機上的搜尋介面，達成無所不在的行動搜
尋。 
關鍵字: 行動視覺搜尋, AdaBoost 偵測器, Edge + PCA 演算法 
1. 前言 
目前全世界擁有手機的人已高達數十億之多，進而讓
網際網路搜尋引擎市場的廠商都開發符合行動裝置搜尋的
版 本 ， 例 如  Google (www.google.com/xhtml) 、 Yahoo 
(mobile.yahoo.com)等，使得使用者隨時可以利用手持裝置
上網搜尋。另一方面，手持式行動裝置的通訊方式已經不
限於 GSM 通訊，可以包含通訊頻寬更高的  3G/3.5G 
(2~14Mbps)、Wi-Fi (54Mbps) 等，例如，2008 年 12 月在
台灣上市的 iPhone 3G 便支援 UMTS, HSDPA, GSM, Wi-Fi, 
EDGE, GPS, 和 Bluetooth 2.0 [1]。一般搜尋系統都需藉由
使用者鍵入所需查詢的文字資料；根據論文 [2] 的研究，
行動裝置上輸入的困難是影響行動搜尋普及的重要因素。
所以使用行動裝置所提供的內建攝影機作為搜尋的輸入便
成為另一個重要的選項。例如，在戶外看到廣告板上自己
喜歡的物品想進而知道它的價錢和款式時，一般是利用文
字輸入物品關鍵字搜尋，這樣的方法較為不方便且費時；
所以，我們將手機上的攝影裝置及搜尋技術結合，將物品
的照片上傳至以影像為基礎的搜尋系統，可即時在手機上
瀏覽相關的商品資訊，以提升使用者的便利性。上述結合
網路資源、行動裝置和影像處理技術三者的相關研究統稱
為行動視覺搜尋 (Mobile Visual Search) [3]，如圖 1 所
示。 
 
圖 1. 行動視覺搜尋概念示意圖 
在影像搜尋系統方面的研究目前多半是以文字當作搜
尋的依據，例如：雅虎奇摩、Google 等各大搜尋網站。只
要是網頁上影像的註解、周遭的文字或者是標題包含使用
者輸入的文字，就回傳這些影像給使用者。以文字為基礎
的影像搜尋的優點是速度快，缺點是每個人的主觀意識會
影響對同一張影像的文字註解，所以回傳結果往往不如預
期。以影像內容為基礎 (Content-Based Image Retrieval，
CBIR) 的搜尋系統可以解決以文字搜尋影像的缺點，尤其
是對一張很難用文字形容的影像，以影像內容比以文字描
述做為搜尋的依據更適合。但是以影像內容為基礎的搜尋
系統的瓶頸在於影像的低階特徵與使用者之間存在著語意
鴻溝 (semantic gap)，例如：一張影像包含藍天和沙灘，使
用者認為這是海灘的場景，系統則是根據顏色分佈認為只
要存在藍色和白色的分布為主的影像便為使用者期望的影
像。再加上面對網路上龐大的影像數量，導致搜尋速度緩
慢。因此，本論文是發展應用在拍賣網站的分類目錄下，
針對單一類型的物件進行以影像內涵為基礎的行動影像搜
尋，在特定類別下搜尋不但可以利用特定類別物件的偵測
技術化解語意鴻溝，也可以提升搜尋速度。 
拍賣網站的使用者通常希望根據影像中的物件特徵來
做搜尋，因此，本論文使用物件偵測先找出影像中物件所
在的區域，再擷取影像特徵作搜尋比對，可避免背景雜訊
的干擾。在物件偵測上使用兩種方法：分別為 AdaBoost 
偵測器和 Edge + PCA 演算法 [13]。AdaBoost 偵測器是特
定影像物件的偵測演算法，可以從大量簡單特徵為主的弱
分類器裡，經過 AdaBoost 演算法，合併有效的弱分類器
形成一個快速並精確的強分類器。Edge + PCA 演算法是先
對影像做邊緣偵測，再利用 Principle Component Analysis 
(PCA) 找出影像中邊緣密集的區域，並認為是物件存在的
為了評估本系統的效能，本論文另外實做一個系統
(EPCA-CBIR)，兩系統之間的差異只在於物件偵測的方式
不同，圖 4 為 EPCA-CBIR 的物件偵測方式，因為是直接
對原始影像偵測，所以，擷取出的物件影像還是殘留一些
背景。 
 
圖 4. (a) EPCA-CBIR 物件偵測流程； 
(b) 實際影像偵測流程 
 
3.2. 影像資料庫的建立 
利用兩種物件偵測系統的影像資料庫建立流程如下： 
Step 1：抓取影像及網頁 
抓取拍賣網站分類目錄下的所有網頁和影像並將
資訊儲存至資料庫。 
Step 2：物件偵測 
對資料庫所有的影像做物件偵測。 
Step 3：取出影像特徵 
對每一張物件影像取出影像特徵，最後存回資料
庫。 
以下會對於物件偵測方式與特徵擷取方式做詳細描
述。 
4. 物件偵測方法 
4.1. AdaBoost 偵測器 
AdaBoost 物件偵測器是利用  AdaBoost 演算法和
cascade 結構訓練而成的，訓練流程如圖 5。我們先收集訓
練樣本，分為正樣本和負樣本。接著為了能快速的計算矩
形特徵值，將每一張訓練樣本計算為積分影像。另外根據
所使用的矩形特徵類型，在特定大小的視窗內產生大量的
矩形特徵，並將每一矩形特徵訓練為弱分類器，使其有分
類能力，之後利用 AdaBoost 演算法，從大量的弱分類器
中挑選少數且關鍵的弱分類器組成強分類器，最後再將多
個強分類器組成 cascade 結構，也就是 AdaBoost 偵測器。 
正樣本 負樣本
訓練樣本
矩形特徵
AdaBoost演算法
產生大量的矩形特徵
AdaBoost偵測器
積分影像
快速計算矩形特徵值
1C 2C C
Object
image
Sub-
window n
T
Non-object image
Cascade
T T T
F F F
挑選出有效的矩形特徵
 
圖 5. AdaBoost 訓練流程 
 
偵測器內有 nCC ~1 為 n 個強分類器，每個強分類器
內有數量不等的矩形特徵。偵測一張影像時，會利用不
同大小的偵測視窗 (detection window) 在影像中移動，每
移動一個位置，就把視窗內的影像傳送給 AdaBoost 偵測
器，若通過所有強分類器判斷，偵測器則輸出此偵測視
窗為物件影像；若被任何一個強分類器判斷為非物件，
則立即被捨棄。其實一張影像裡大部份的偵測視窗內都
是非物件，所以 AdaBoost 偵測器就利用 cascade 結構前
面的少數強分類器將大量且非物件的偵測視窗捨棄，達
到即時偵測的效果。 
4.2. Edge + PCA 演算法 
Edge + PCA 演算法的偵測流程如圖 6 所示： 
Step 1：利用 Sobel 濾波器偵測影像的邊緣 [14]。 
Step 2：將邊緣座標當作資料分佈的範圍。 
Step 3：利用 PCA 演算法找出資料密集的區域。 
 
 
圖 6. Edge + PCA 演算法偵測流程 
 
5. 影像特徵擷取與相似度排序 
本論文對物件影像所擷取的顏色特徵計算流程如下： 
Step 1：轉換顏色空間 
  將影像轉換為 HSV 顏色空間 [15]。 
用這些數值畫出一條曲線。再從這條曲線中，挑選出最
佳參數的數值，能讓偵測器在高偵測率與低誤判率兩個
條件同時成立下進行偵測。本論文使用的參數為
min_neighbor，此參數的原理是偵測視窗在影像中移動
時，其實會對同一個物件抓取出不同大小和位置的偵測
視窗，此參數就是計算影像中裡，每個物件周圍有多少
個偵測視窗被偵測器判斷為物件。可知此參數設定越
高，偵測率和誤判數目也會越高；設定越低，偵測率和
誤判數目也會越低。 
圖 9 為帽子偵測器的 ROC 曲線，最高的偵測率在誤
判數目為 46 個下為 71%，我們設定偵測器的錯誤率 (誤
判數目/物件總數) 不超過 15%，所以 min_neighbor 設定
為 1。 
 
圖 9. 帽子偵測器的 ROC 曲線 
 
圖 10 為 T-shirt 偵測器的 ROC 曲線，最高的偵測率在
誤判數目為 83 個下為 77%，在相同的錯誤率標準下
min_neighbor 設定為 2。 
 
圖 10. T-shirt 偵測器的 ROC 曲線 
 
圖 11 為人臉偵測器的 ROC 曲線，最高的偵測率在誤
判數目為 361 個下為 88%，min_neighbor 設定為 3。 
ROC 曲線
False detections
D
et
ec
tio
n 
ra
te
 
圖 11. 人臉偵測器的 ROC 曲線 
6.2. 評估 CBIR 系統之效能 
評估的方式是在 09 年 7 月 27 日，從 Google 圖片搜尋
中，輸入兩個關鍵字，“baseball hat”和“T-shirt”，在回傳
影像中先刪除非物件的影像，然後抓取前 50 張物件影
像，從中隨機挑選出 5 張當作兩物件的查詢影像給使用
者，如圖 12(a)為 5 張帽子的查詢影像，圖 12(b)為 5 張 T-
shirt 的查詢影像。然後請 10 位使用者使用兩種查詢方式
評估三個系統對每一張查詢影像的 Top 10 precision，如公
式(2)，最後計算平均的準確率。查詢的第一種方式為使用
者自行對查詢影像框選出感興趣的區域，第二種方式為系
統自動物件偵測。 
 rPrecision
n
                                 (2) 
n 為回傳影像的張數，r 為回傳且正確的影像 
(a)
(b)  
圖 12. (a) 5 張帽子的查詢影像； (b) 5 張 T-shirt 的查詢影像 
 
表 3 為 10 位使用者使用兩種查詢方式評估三個系統對
全部帽子和 T-shirt 查詢影像平均的 Top 10 precision。 
表 3. 帽子和 T-shirt 平均的 Top 10 precision 
分類目錄 帽子 T-shirt 
查詢方式 手動 自動 手動 自動 
EPCA-CBIR 34% 28% 27% 29% 
AdaBoost-CBIR 36% 35% 32% 40% 
重新排序 42% 47% 44% 44% 
 
在實驗的過程中，因為不同的使用者對同一張查詢影
像感興趣的區域不同，所以間接影響到回傳的結果，從實
驗結果來看，系統自動物件偵測方式的準確率比使用者自
行框選較好。重新排序的結果比兩個系統較好，證明重新
