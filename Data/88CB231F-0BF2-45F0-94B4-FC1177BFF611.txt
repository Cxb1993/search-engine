search facilities that include keywords, sound, 
examples, shape, color, texture, spatial structure and 
motion. Traditionally, texture features such as 
filenames, captions, and keywords have been used to 
annotate and retrieve images. As they are applied to a 
large database, the use of keywords becomes not only 
cumbersome but also inadequate to represent the image 
content. Many content-based image retrieval systems 
have been proposed in the literature [1-3]. To access 
images based on their content, low-level features such 
as colors [3], textures [4], and shapes of object [5] are 
widely used as indexing features for image retrieval. 
Although content-based image retrieval is extremely 
desirable in many applications, it must overcome 
several challenges including image segmentation, 
extracting feature from the images that capture the 
perceptual and semantic meanings, and matching the 
images in a database with a query image based on the 
extracted features. Due to these difficulties, an 
automatic image content-based retrieval method on the 
basis of low-level features can neither achieve very 
good results, nor will it replace the traditional 
text-based retrievals in the near future. 
Humans tend to use high-level concepts in 
everyday life. However, what existing computer vision 
techniques can automatically extract from image are 
mostly low-level features. Object segmentation and 
recognition is the primary step of computer vision for 
applying to image retrieval of higher-level image 
analysis [8-10]. Current technique in image retrieval 
fall into broad categories: view-based and model-based 
[6]. View-based techniques are based on recognition of 
similar visual attributes such as color and texture, 
model-based retrieval image based on shape, while 
requires segmentation of image into objects. The 
segmented objects are described by means of a 2D 
model that outlines the interrelations between primitive 
geometric image features such as line, vertices and 
ellipses. Extracted image features are used to define 
the model properties and construct a model of a 
manmade object of interest [7]. Computer perceptual 
of manmade objects in outdoor scenes is a challenging 
task due to the presence of both manmade and natural 
objects.  
Manmade objects in the contour are generally rigid, 
therefore these models can adequately define their 
shape descriptions. However, the model-based 
approach is limited by the lack of 3D or 2D 
representation of natural objects. They require a priori 
knowledge about the shape of objects (object models), 
which is used to predict image features or matching to 
features in the image or in a transformed feature space. 
Natural object, such as tree and other flower, river, 
rock and clouds, with manmade objects and are 
unspecified; and their appearance is unpredictable. 
Very few objects have compact shape descriptions, and 
it is difficult to establish complete boundaries between 
the objects of interest and background objects. The 
complexity of this task depends on many factors, such 
as the number of objects in the model database, and the 
amount of a priori information about the scene. Iqbal 
and Aggarwal proposed an interesting way to extract 
the image structure from large manmade objects for 
image retrieval by perceptual grouping [11]. 
Broadly speaking, the shape recognition problem 
can be approached with three main frameworks [14]: 
the statistical approach, the syntactic approach, and the 
hybrid of the two methods. The statistical approach 
uses global shape features, such as moments [15], 
autoregressive coefficients [16], and Fourier 
descriptors [17], to recognize shapes. It is not difficult 
to compute global features, but the local structures of a 
shape cannot be described in detail. The syntactic 
approach uses local structural features like arcs and 
segments as primitives, and then the shape is 
recognized by a parsed grammar. However, the 
syntactic approach is sensitive to noise so that it can 
not effectively solve the problem. The hybrid approach 
is the method combining the statistical (or 
decision-theoretic) and syntactic (or structural) 
approaches to shape recognition [18]. The approach is 
somewhat unable for the non-consistent results of 
shape segmentation.  
In the past, polygonal approximation has been a 
very popular shape representation technique. It can not 
only satisfactorily represent a shape, but also 
significantly reduce the amount of processing data for 
further applications. Hence, through polygonal 
approximation, many shape recognition (matching) 
methods [18-20] have been proposed. However, some 
conventional methods are somewhat sensitive to 
non-consistent results of polygonal approximation. For 
example, the method [18] using attributed string 
matching cannot accuracy define the edit distance (cost) 
for insertion and deletion operations. Other methods, 
such as the shape matching using polygonal 
approximation and dynamic alignment [20] and 
recognition scheme based on relative distance of 
feature points (polygonal vertices) from the centroid, 
may have the same segmentation problem, which will 
potentially reduce the recognition rate or increase the 
processing time for better segmentation results.  
Edge features are recognized as an important 
aspect of human visual perception, are commonly used 
in shape analysis. In [11], the system analyzes each 
image to extract features that are strong evidence of the 
M  
),(),(),( ,333,223,113 mjRiRmiRmjRiRmiRmjRiRmiR
QQPIIPYYP −=−=−= +++ ααα  
and ,21,αα and 3α are the weighting factors of color 
and set to be 0.4, 0.3, 0.3, respectively in this study. 
,, IY  and Q  denote the mean color of whole regions 
in an image. The major advantages of the above 
definitions are: 
(1) The dominant color is easily extracted using the 
mean color of a region to minus the mean color of 
whole regions in an image. For example, if the 
color in an image tends to be ‘white’ but if there is 
a ‘black’ color of a region in this image, then the 
‘black’ color is dominant. This approach is the 
same as the sense of human vision to capture an 
image. 
(2) The number of primitives for any region in an 
image may be different based on its neighboring 
number of regions. The more number of neighbors 
for a region, the more primitive value of color in 
this region due to it being an important region 
when an image is interpreted.  
The research for the importance of primitives in an 
image belongs to Gaussian distribution domain [26]. 
Every region is given a weighting to reveal the 
dominance of a region using the primitive value of 
color, which is called weighting of dominant region 
and defined as follows: 
            vS
iRVP
i eRW
−−= 1           (2) 
where sv denotes the covariance of primitive value of 
color in an image, and iRVP is a  primitive value of 
region Ri from (1), which is defined by 
                              
.
331
∑
+≤≤
=
mk
K
iRi
R PVP        (3) 
The RWi indicates the weighting of dominant region Ri, 
which is obtained using the primitive difference from 
the region Ri and its neighboring regions. If the value 
of RW is larger, then the region in an image is more 
dominant. An image is segmented into n’s 
non-overlapping regions, that is niRi ,...,1, = . The 
arrangement of RWs in ascending order 
( nRWIRWIRWI ≥≥≥ L21 ), associates the same 
ordering to the region Ris  
          nRRRR ≥≥≥≥ L321         (4) 
where R1 is called the most dominant region and Rn is 
the least dominant region. A ratio is decided to reserve 
how much of the regions can capture the semantics of 
an image and the remainder will be ignored. The 
radio ρ is defined by 
            ∑
≤≤
∑
≤≤=
nj
jRW
pi
iRW
1
1ρ               (5) 
where n denotes the number of regions and p (p<n) 
denotes preceding sequences in eq. (4).  
 
4. Shape Descriptions 
 
Segmentation of an image to obtain different 
regions, followed by the analysis of their structural 
information to recognize the desired features, is a 
top-down approach. Automatic segmentation and 
recognition of objects via object models is a difficult 
task, and the quality of segmentation directly affects 
the performance of an image retrieval system. Many 
researches focus on the moment-based operators have 
been successfully developed for image edge detection, 
such as edge location computing [21], Pei and Cheng 
[22] applied the BQMP (binary 
quaternion-moment-preserving) thresholding technique 
to detect color edges with the precision to subpixel 
accuracy. A survey on moment-preserving thresholding 
algorithm has been developed by Tasi [24]. The 
moment-preserving principle has also applied to detect 
the edge of images in this report.   
 
4.1. Proposed of edge detection 
A subpixel edge-detection modeling using the 
moment-preserving principle has been proposed in 
[23,24]. In this model, the image will be partitioned 
into non-overlapping 4 × 4 blocks, and apply the 
moment-preserving to find the solutions of the 
parameters h1, h2, l, and θ  of the edge [25], where h1 
and h2 are two representative colors of the block and 
the values of l and θ  are used to locate the edge at 
the block. In the uniform block, the mean color of the 
block to represent the block, and quantize the 
representative colors of each block by c-means 
clustering method and replace each pixel value with its 
corresponding quantized color. Let N denote the 
number of region found so far and N is zero initially. 
Assign the label of each pixel of Q to be zero, where q 
denotes the detected region. Scan the pixel of Q in 
top-to-down and left-to-right fashion and for each 
non-edge pixel p with coordinates (x,y), do the 
following steps: 
(1) If the label of p is zero, assign the label of p to be 
the value of N and increase the value of N by 1. 
(2) If the label of p is non-zero, assign the labels of the 
pixels located at (x+1,y), (x,y+1), and (x+1,y+1) to be 
the label of p if their pixel values are same as that of p. 
neighboring points among which there is one edge 
point at least. The boundary chain-code is the direction 
description of the current point to the next one. The 
number 0 to 7 is used for direction description in 
8-direction chain-code as shown in Fig. 2.  
 
Fig.2.  3×3 grid and 8-direction codes 
An object edge in an image can be described with a 
start point and sequence direction codes as above. 
There are some deficiencies of the chain-code method. 
First, selecting the proper start point on the chain-code 
is commonly a complicate process. With different start 
point, the chain-code varies dramatically. Moreover, 
these varying are neither regulation nor correction. 
Another problem is that the chain-code method cannot 
overcome to judge the same of object rotating and 
scaling of image.  
Based on the chain-code method, the consecutive 
primitive difference is proposed to describe the shape 
of object. The concept of consecutive primitive edge 
difference (CPD) is as follows: 
CPD (Hi+1~Hi)=|Hi+1-Hi|              (6) 
where Hi and Hi+1 denote the consecutive primitive 
edges respectively. The detected CPD is mapping to 7 
types of visual pattern. This assumes that the possible 
CPD in a 3×3 block are limited to multiple of 450, it is 
quantized to be the nearest multiple of 450.  
   Some types of CPD in the sense of human vision is 
same. The equation (1) is reformulated as follows: 
               
.
4|||,|8
4|||,|
)~(
11
11
1 ⎭⎬
⎫
⎩⎨
⎧
>−−−
<−−=
++
+++
iiii
iiii
ii HHHH
HHHH
HHCPD  (7) 
Shape representation [28] can be categorized into 
either boundary-based or region-based. Well known 
method include Fourier descriptors and moment 
invariants, other methods include wavelet descriptors 
and histogram of edge directions. The histogram of 
virtual pattern is used to describe the shape 
representation in this report. In practical, the CPD 
method cannot efficient and correcting to described the 
shape of object in equation (2). Some of the histograms 
of virtual pattern are same using the CPD method 
because it is discrete. To solve this problem, the 
equation (2) is modified as follows: 
)()()~( 1212 ++++ −∪−= iiiiii HHCPDHHCPDHHCPD  (8) 
where notation ∪ denotes a union operation. Instead 
of representing edges in any possible combination, the 
detected TCPD is mapping to 56 types of virtual 
pattern. Note that the relationships among 
virtual-pattern organization, some virtual-pattern 
structures are same from the sense of human vision. 
Thus, the 56 types of virtual pattern are further to 
merge into 20 types of virtual-pattern, the remainder 
can be mapped into the 20 types of virtual-pattern 
through it is rotation or scaling. The 20 types of 
virtual-pattern are organized as shown in Fig.3. 
The main reason for mapping two consecutive 
primitive edges to a virtual-pattern is to encode an 
image with visual-patterns block by block moved alone 
the edge pixels. Hence, two given image block moved 
alone the edge pixel can be encoded and index to 
indicate which virtual-pattern is mapped.  
 
Fig. 3.  Possible 20 types of visual-patterns in two 
consecutive CPD(TCPD). 
 
5.2. Extraction of TCPD from the objects of image for 
retrieval 
At the lowest level of computer vision, potentially 
useful image events such as edges and lines segments 
can be extracted from an image without any 
knowledge of the image content. For unconstrained 
environments, where the viewing angle and depth are 
not known, the bottom-up approach of hierarchically 
grouping meaningful edge segments into higher-level 
structure appears to be promising for the extraction of 
semantic information. As mentioned above, the 
meaningful edge segments can be categorized into four 
classes as Fig. 1. After the edge detection using the 
proposed edge detection in section 4, the primitive 
edge will be connected and found which belong to. 
And the next, the possible directions of an edge in a 4×
4 block are limited to multiples of 450, or equivalent, i×
450, i=1,…,7. If the actual direction of an edge is not a 
object is usually grouped from multiple regions which 
shapes are connected (containing coterminal structure 
or coterminous edge) each other. A semantic object is 
defined using a group of multiple regions or a single 
region dependent on whether the regions are connected 
each other in this report. The approach finds a serial of 
TCPD from a semantic object as following algorithm. 
To obtain the same of TCPD for the any structure 
in which is including one or more enclosed regions, In 
the PTCPOM algorithm is considered by the following 
decision rule: (1) the starting edge and ending edge are 
overlapped in the first scanning region; (2) the 
scanning sequence uses the clockwise in any enclosed 
region; (3) the cotermination edge is only scanned one 
time. The next we need to decide the starting edge in 
the first scanning region if Step (5) of the above 
algorithm is executed. The starting point is found by 
following Algorithm and an example is shown as Fig. 
8 to illustrate the process of PSPER. 
Algorithm 3. Proposed a searching algorithm to find 
the starting point of enclosed region (PSPER). 
Input: An enclosed region. 
Output: A starting point. 
Method: 
1. Determinate the long axis of the region. 
2. Compute the center of long axis. 
3. Compare the distance between the primitive edge of 
region to long axis, assume the distances are 
represented as A={a1, a2, a3,…, an} and B={b1, b2, 
b3,…, bn}respectively. 
4. Find the max(|ai-bi|), i=1,2,…n. 
5. If the (max(|ai-bi|)) is not single, the max(|ai-bi|) is 
determined according to the far distance of ai and 
the center of long axis. 
6. If(ai>bi) ai is starting point. 
7. else bi is starting point. 
 
Fig. 5. An example is given to illustrate the extraction 
of starting point. 
 
5.3 Similarity measurement of shape representation 
Consider a database DB consisting of a large 
number of images, each of them is represented as a 
high-dimensional feature vector 
},...,1,20,...,1;,...,1},,,{{ mkjnipfF kji ==== φ
, where fi , jφ and pk are the ith edge line, the 
corresponding of possible 20 types of virtual-patterns, 
and the corresponding of number of TCPD, 
respectively. The total number of TCPD in an image 
can be computed as 
       
20,...,1,
1 1
=∑
=
∑
=
= jn
i
m
k
jkPjχ        (10) 
Let }20,...,1},,{{ == iiciX χ and 
}20,...,1},,{{ == iieiY χ  be feature vectors in two 
images, and the ci and ei are the corresponding of 
TCPD number. Then the distance between X and Y is 
computed as 
       
∑
=
−−= 20
1
||1),{
i
ieiciYXD η       (11) 
The role of each TCPD in the any image is different 
from the sense of human vision. Such as the square 
shape, the perpendicular angle is more significant then 
the straight line. If the perpendicular angle is lost 
anything, it can not form the square shape. On the 
opposite view, increase or decrease the straight lines, it 
may be still maintained the square shape. In general, 
the less of TCPD number is more important then the 
many of TCPD number in an image from the sense of 
human vision. Thus, the parameter of η  in eq. (10) 
can be defined as 
∑
=
+
+
=
20
1
|11|
|11|
j jejc
ieic
iη
           (11) 
where the value ci, cj, ei, and ej is assumed as non-zero, 
otherwise the corresponding value of 
jc
1 or 
je
1 is 
assigned as 0. 
 
6. Experimental results 
   In order to evaluate the proposed approach, a series 
of experiments were conducted on an Intel 
PENTIUM-VI 2.5GHz PC and a database of 10456 
natural images was built for the proposed of 
performance evaluation. Each image is first tailored 
into 256 × 256 for testing the proposed retrieval 
approach. The performance of the proposed image 
retrieval method is evaluated in terms of execution 
speed and retrieval accuracy.  
  The retrieval techniques based on large manmade objects 
using perceptual grouping proposed by Iqbal and Aggarwal 
recognition in cluttered range image using 
recognition strategist, Comput. Vision, Graphics, 
Image Processing: Image Understand, 58(1), 33-48, 
1993. 
8. Y. Rui, T. S. Huang, and S.-F. Chang, Image 
retrieval: current techniques, Promising directions, 
and open Issues, J. Vis. Commun. Image 
Representation, 10, 39-62, 1999. 
9. M. W. Smeulders, S. Santini, A. Gupta, and R. Jain, 
Content-based image retrieval at the end of early 
years, IEEE Trans. Pattern Anal. Mach. Intell., 22, 
1340-1380, 2000. 
10. J. Z. Wand, J. Li, and G. Widerhold, SIMPlIcity: 
semantic-sensitive integrated matching for picture 
libraries, IEEE Trans. Pattern Anal. Mach. Intell., 
23(9), 947-963, 2001. 
11. Q. Iqbal, J. K. Aggarwal, Retrieval by 
classification of images containing large manmade 
objects using perceptual grouping, Pattern 
Recognition, 35, 1463-1479, 2002. 
12. Q. Iqbal and J. K. Aggarwal, Retrieval by 
classification of images containing large manmade 
objects using perceptual grouping, Pattern 
Recognition, 35, 1463-1479, 2002. 
13. H. Q. Lu, and J. K. Aggarwal, Applying perceptual 
organization to the detection of man-made objects in 
non-urban scenes, Pattern Recognition, 25(8), 
835-853, 1992. 
14. J. Mantas, methodologies in pattern recognition 
and image analysis—a brief survey, Pattern 
Recognition, 20, 1-6, 1987. 
15. R. J. Prokop and A. P. Reeves, A survey of 
moment-based techniques for unoccluded object 
representation and recognition, CVGIP: Graphical 
Models Image Process, 54, 438-460, 1992. 
16. S. R. Dubois and F. H. Glanz, An autoregressive 
model approach to two-dimensional shape 
classification, IEEE Trans. Pattern Anal. Mach. 
Intell., 8(1), 55-66, 1986. 
17. E. Persoon and K. S. Fu, shape discrimination 
using Fourier descriptors, IEEE Trans. Syst. Man 
Cyber., 7, 170-179, 1977. 
18.  W. H. Tsai and S. S. Yu, Attributed string 
matching with merging for shape recognition, IEEE 
Trans. Pattern Anal. Mach. Intell., 7(4), 453-462, 
1985.  
19. C. C. Lu and J. G. Dunhan, Shape matching using 
polygon approximation and dynamic alignment, 
Pattern Recognition Lett. 14, 945-949, 1993. 
20. C. C. Chang, S. M. Hwang and D. J. Buehrer, A 
shape recognition scheme based on relevance 
distances of feature points from the centroid, Pattern 
recognition, 24, 1053-1063, 1991. 
21. J. Tabatabai, O. R. Mitchell, Edge location to 
subpixel values in digital imagery, IEEE Trans. 
Pattern Anal. Mach. Intell., 6, 188-201, 1984. 
22. W. H. Tsai, Moment-preserving thresholding: a 
new approach, Computer Vision, Graphics, and 
Image Processing, 29, 377-393, 1984. 
23. S. C. Cheng, Content-based image retrieval using 
moment-preserving edge detection, Image and 
Vision Computing, 809-826, 2003. 
24. S. C. Cheng and T. L. Wu, Content-Based image 
retrieval using moment-preserving edge detection, 
Proc. International Computer Symposium, Hualien, 
Taiwan, Dec. 18-21, 2002. 
25. H. Samet, Hierarchical representations of 
collections of small rectangles, ACM Computer 
Surveys, 20(4), 271-309, 1988. 
26. H. Freeman, On the Encoding of Arbitrary 
Geometric Configurations, IRE Transaction on 
Electronic Computers, EC-10, 260-268, June 1961. 
27.  J. X. Yuan and C. Y. Suan, An Optimal Algorithm 
for Detecting Straight Line in Chain Codes, Pattern 
Recognition, 3, 692-695, 1992.   
28. X. Wang, and K. Xie, A Novel Direction Chain 
Code-Based Image Retrieval, Proc. 4th International 
Conf. on Computer and Information Technology, pp. 
190-193, Sept. 2004.  
29. T. H. Comen, C. E. Leiserson, R. L. Rivest and C. 
Stein, Introduction to Algorithms, 2nd Ed. The MIT 
Press 2001. 
 
 
 


