 2
in the domain [ )5.0 ,0.0 . 
Because the VPRS model has no 
formal historical background of having 
empirical evidence to support any 
particular method of β -reduct selection 
(Beynon, 2002), VPRS-related research 
studies do not focus in detail on the 
choice of the precision parameter (β ) 
value. Ziarko (1993) proposed the β  
value to be specified by the decision 
maker. Beynon (2000) proposed two 
methods of selecting a β -reduct 
without such a known β  value. Beynon 
(2001) proposed the allowable β  value 
range to be an interval, where the quality 
of classification may be known prior to 
determining the β  value range.  
The extended VPRS was 
introduced by Katzberg, et al. (1996), 
which allowed asymmetric bounds l  
and u  to be used. The VPRS models 
the restriction 5.0<l  and lu −= 1  
must hold. Beynon (2002) introduced 
the ),( ul -quality graph, which 
elucidates the associated level of quality 
of classification, based on the selected l  
and u values. The results in this paper, 
within a criteria for the effective choice 
of l  and u  values is still required. 
However, without a β  value to control 
the choice of β -reducts, this may lead 
to the full set of β -reducts becoming 
too large, such that an addition to a 
search scope is needed to find a suitable 
β -reduct. 
In this study we propose a method 
to determine the precision parameter 
value based on the least upper bound of 
data misclassification error. In addition 
we will use a simple example to explain 
our proposed method and a medical 
examination example to demonstrate the 
method. 
 
Variable Precision Rough Sets Model 
The variable precision rough sets 
(VPRS) model is an extension of the 
original rough set model (Ziarko, 2001), 
which was proposed to analyze and 
identify data patterns that represent 
statistical trends rather than being 
functional.  
VPRS deals with a partial 
classification by introducing a precision 
parameter β . The β  value represents a 
bound on the conditional probability of a 
proportion of objects in a condition class 
that are classified to the same decision 
class. Ziarko (1993) defined the β  
value as a classification error and the 
range in the domain [ )5.0 ,0.0 . However, 
An, et al. (1996) and Beynon (2001) 
considered β  to denote the proportion 
of correct classifications, in which case 
the appropriate range is ( ]0.1 ,5.0 . In this 
study we use the Ziarko notion. The 
procedure of the VPRS model has five 
steps and they are as follows: 
Step 1: Choosing the precision 
parameter (β ) value. 
Step 2: Find the full set of β -reduct.  
Step 3: Elimination of duplicate objects. 
Step 4: Elimination of superfluous 
values of attributes. 
Step 5: Rules extraction. 
VPRS operates on what may be 
described as a knowledge representation 
system or information system. An 
information system (S) consisting of 
four parts is shown as: 
     S = (U, A, V, f),  
where U is a non-empty set of objects; 
        A is the collection of objects; we 
have DCA U= and φ=DC I , 
where C is a non-empty set of 
condition attributes, and D is a 
non-empty set of decision 
attributes; 
      V is the union of attribute 
domains, i.e., a
Aa
VV
∈
= U , where 
aV  is a finite attribute domain 
and the elements of aV are called 
values of attribute a; 
 4
attributes, which leads to the same 
partition of the data as the whole set of 
attributes A. To do this, we have to 
construct the discernibility 
function )(Af . This is a Boolean 
function constructed in the following 
way: to each attribute from the set of 
attributes, which discern two elementary 
sets, (e.g., },,,{ 4321 aaaa ), we assign a 
Boolean variable ‘a’, and the resulting 
Boolean function attains the form 
( 4321 aaaa +++ ), or it can be 
presented as ( 4321 aaaa ∨∨∨ ). If the 
set of attributes is empty, then we assign 
to it the Boolean constant 1 (Walczak, et 
al. 1999). 
 
Rules Extraction 
Procedures for generating decision 
rules from an information system has 
two main steps as follows: 
   Step 1: Selection of the best minimal 
set of attributes (i.e. β -reduct 
selection). 
Step 2: Simplification of the information 
system can be achieved by dropping 
certain values of attributes that are 
unnecessary for the information 
system. 
          Ziarko (1993) indicated that every 
minimal set of attributes may be 
perceived as an alternative group of 
attributes, which could be used instead 
of all the available attributes in the 
decision making based on cases. The 
main difficulty is how to select an 
optimal β -reduct. Two approaches can 
be used in this case. In the first one, the 
β -reduct with the minimal number of 
attributes is selected. In the second 
approach, the β -reduct that has the 
least number of combinations of values 
of its attributes is selected.  
 
三、結果與討論 
   When performing a VPRS analysis, 
how the β -reducts are selected is a key 
point of the process. The precision 
parameter value can control the choice 
of β -reducts. Previous related research 
studies lacked an effective method to 
determine a precision parameter value. 
Ziarko (1993) defined the measure of 
the relative degree of misclassification 
of the set X with respect to Y 
as:
⎪⎩
⎪⎨
⎧
=
>−=
0)(    if                             0
0)(    if          
)(
)(1
) ,(
Xcard
Xcard
Xcard
YXcard
YXc
I
       
where card denotes set cardinality.  
Let X and Y be non-empty subsets 
of U. We say that X is included in Y, if 
for every element that belongs to X, 
then that also belongs to Y. The measure 
of relative misclassification can define 
the inclusion relationship between X and 
Y without explicitly using a general 
quantifier: 
                     
.0),( =⇔⊇ YXcXY  
According to the specified majority 
requirement, the admissible β  must be 
within the range .5.00 <β≤  Based on 
this assumption the majority inclusion 
relation is defined as: 
                      
In this study we propose an 
effective method to find the β -reducts, 
which involves two steps: 
Step 1: Find the candidates of 
β -reducts using precision parameter 
(β ). 
 In this study we propose an 
effective method to determine the β  
value in the VPRS model, which is 
based on the least upper bound 
),( DCξ of the data set, where C is the 
condition attributes set, D is the 
decision attributes set, and 
},...,,{ 21 nEEEC =∗ is the equivalence 
classes. The following equation is used 
for calculating the least upper bound 
of data set. 
 6
by multi-dimensional information about 
the current health status of patients, 
which makes it difficult to diagnose 
other diseases based on such a large 
amount of information. Until now, the 
relationship between the medical 
examination data and liver malfunction 
symptom is still ambiguous. 
In this case 168 instances are 
collected. These instances are separated 
into a training set that includes 101 
instances (54 instances that are normal; 
47 instances of liver malfunctions) and a 
test set that includes 67 instances (35 
instances that are normal; 32 instances 
of liver malfunctions). Labeling “liver 
malfunction patients” is based on the 
medical history of the patients as judged 
by medical doctors. 
Using the proposed approach 
Since VPRS needs the data in a 
categorical form, the continuous 
attributes must be discretized before the 
VPRS analysis is performed. In this case 
the items of the medical examination 
standard (MES) are utilized to discretize 
the continuous attributes. The results are 
listed in Table 4.1. From Table 4.1, we 
know that each condition attribute is 
classified into two or three ranges.  
Table 4.1  Condition attributes ranges for MES discretization 
Examination Items Range ‘1’ Range ‘2’ Range ‘3’ 
Age (a1) 23~34 35~55 56~63 
Neutrophil (a2) 0.0~36.9 37.0~75.0 75.1~ 
Lymphovyte (a3) 0.0~19.9 20.0~55.0 55.1~ 
Moncyte (a4) 0.0~2.4 2.5~10.0 10.1~ 
Basophil (a5) — 0.0~2.0 2.1~ 
GLUAC (a6) 0~69 70~110 111~ 
ALK-P (a7) 0~59 60~205 206~ 
GOT (a8) 0~7 8~35 36~ 
GPT (a9) — 0~35 36~ γ -GT (a10) — 0~45 46~ 
D-Bil. (a11) 0 0.1~0.5 0.6~ 
T-Protein (a12) 0.0~6.2 6.3~8.5 8.6~ 
TG (a13) 0~59 60~105 106~ 
BUN (a14) 0~7 8~25 26~ 
Uric Acid (a15) 0.0~2.4 2.5~8.0 8.1~ 
In this information system the 
objects have been classified into one of 
two categories, 0 (normal) and 1 
(malfunction). The condition classes of 
objects can distinguish 40 groups, and 
the precision parameter value is equal to 
6
1 . Following the method of analysis 
given previously, four subsets and a 
β -reducts can be obtained. The subsets 
are: 13109872 ,,,,, aaaaaa }, 
{ 13109873 ,,,,, aaaaaa }, 
{ 151310987 ,,,,, aaaaaa }, and 
{ 131210987 ,,,,, aaaaaa }; the β -reduct is 
}.,,,,{ 1310987 aaaaa  The final version of 
the information system in the subspace 
{ 1310987 ,,,, aaaaa } is shown in Table 
4.2. 
 
 
 
 
 
 
 
 
 8
Notes: ( / ) indicates (number of correct instances/number of total instances). 
 
Using the neural network approach 
In this section the Professional II 
Plus software package (Neural Ware, 
Inc., 1992) is used to perform the 
computation in order to obtain the 
structure with a maximum classification 
rate. After trial and error, we choose 
0.25 and 0.30 as the learning rates in the 
hidden layer and the output layer, 
respectively. The momentum is set at 
0.95, and the number of iterations is set 
at 20000. Structure 15-10-1 is the 
optimal structure through the trained 
back-propagation neural network. 
After the features selection, eight 
features are deleted and seven features 
are retained. They are Lymphocyte, 
Monocyte, GOT, GPT, γ -GT, D-Bil, 
and TG. These seven features are used to 
retrain a new network. Structure 7-5-1 is 
chosen for further analysis. 
After pruning the unnecessary 
connections from network 7-5-1, only 
three attributes, GOT, GPT, and γ -GT 
could affect the result. The simplified 
7-5-1 structure is used to extract the 
rules, and the results are listed in Table 
4.5.  
Table 4.5  Results of rule extraction (neural networks) 
Accuracy (%) Rules Training Set Test Set 
1. If GOT≤ 35,GPT≤ 35 and γ -GT≤ 45, then one is normal. 88.14% (52/57) 90% (36/40) 
2. If 36≤GOP, then one has a malfunction. 100% (12/12) 100% (9/9) 
3. If 36≤GPT, then one has a malfunction. 97.56% (40/41) 100% (23/23) 
4. If 46≤ γ -GT, then one has a malfunction. 100% (6/6) 100% (11/11) 
Notes: ( / ) indicates (number of correct instances/number of total instances). 
Despite its diverse applications in 
many domains, the VPRS model lacks a 
feasible method to determine a precision 
parameter value to control the choice of 
β -reducts. The primary motivation of 
this study is to develop a method to 
select a precision parameter to control 
the choice of β -reducts.  
The proposed method is based on 
the least upper bound of the data 
misclassification error. A simple 
example has been provided to illustrate 
the effectiveness and speed in obtaining 
a suitable β -reduct with our proposed 
method. Furthermore, a medical 
examination case has also been 
analyzed.  
In this study we propose an 
effective method to find the β -reducts. 
The notion of the approach being 
effective is a subjective opinion. First, 
we calculate a precision parameter value 
to find the subsets of an information 
system that is based on the least upper 
bound of the data misclassification error. 
Next, we measure the quality of 
classification and remove redundant 
attributes from each subset. Two 
numerical examples have been 
conducted to demonstrate the feasibility 
of the proposed method.  
Future work should consider if the 
information system has missing data and 
should develop a continuous attributes’ 
dicretization method without the domain 
knowledge of experts. 
 
五、參考文獻 
1. An, A., Shan, N., Chan, C., Cercone, 
N. and Ziarko, W., 1996, 
“Discovering Rules for Water 
Demand Prediction: An Enhanced 
Rough-set Approach,” Engineering 
Applications in Artificial Intelligence, 
Vol. 9, No.6, pp. 645-653. 
2. Beynon, M., 2000, “An Investigating 
of β -Reduct Selection within the 
Variable Precision Rough Sets 
Model,” The Second International 
Conference on Rough Sets and 
