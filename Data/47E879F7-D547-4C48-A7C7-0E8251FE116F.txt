  1
摘 要 
決策樹及其相關應用在這幾年陸續被提出來，決策樹應用在辨識的問題上也
非常廣泛，但目前決策樹有幾個潛在的難題，第一是目前所提的決策樹都是固定分
支，也就是存在每個節點的資料都被分成固定的類別數，但是並不是每個節點都適
合分類成相同的類別數，第二個難題是決策樹的節點通常代表著一個群聚的中心
點，然而利用此一群聚的中心點當作決策樹分類輸入資料的參考點不是很適當，常
常造成分類的錯誤，第三個難題是決策樹在建立完成後，如何利用另外的新訓練資
料進行動態的調整，使決策樹可以適應新的訓練資料，提高辨識率。針對以上說明，
本計畫中擬提出五大部分，第一部份是擬提出一個利用模糊遺傳演算法所建立的模
糊可變分支決策樹，模糊遺傳演算法可以依據每一個節點所包含的資料分布 ，要
求降低決策樹的分類錯誤與節省分類時間前提下，自動找出最適合的分支數，所以
在模糊可變分支決策樹中，每個節點的分支數是依照整體決策樹的辨識率與辨識時
間而決定，如此使得模糊可變分支決策樹達到最佳化，第二部分是本計劃擬提出一
個遺傳演算法自動找出每個節點所需要參考點的個數，並且決定參考點的位置，每
個節點的參考點將取代傳統以群聚中心為參考點的決策樹設計，這對於提升決策樹
的辨識效率有所幫助，第三部份本計劃擬提出一個在有限空間與分類錯誤率條件下
之模糊可變分支決策樹最佳化設計，第四部份著重在決策樹的自動調整與適應，本
計劃擬提出一個節點中參考點的自動調整方法，這個參考點自動調整方法的效率必
須快速，以符合實際使用者的需求，另外也提出一個調整分支與合併的方法，這個
方法可以讓決策樹在接受不同的新資料時，可以做動態的調整，使決策樹可以保持
較高的辨識效率。本計劃最後一部份是將所提出的決策樹與其他已發表的決策樹作
詳實的實驗比較，預計將使用多種不同的資料庫，這對於決策樹效率的了解可以有
很大的貢獻。 
 
關鍵詞：決策樹, 遺傳演算法, 分類參考點。 
 
 
 
 
Abstract 
The decision trees and their variants recently have been proposed. However, many 
problems are existed in the traditional decision trees. First, all trees are fixed M-ary 
tree-structured trees, such that the training samples in each node must be artificially divided 
into a fixed number of branches. In some case, the samples contained in the node are not 
proper to be divided into a fixed number of branches. Next, the center of samples contained 
in the node is usually used to classify the input vector in the decision tree. However, the 
performance of decision tree is reduced while the center contained in the node is used as the 
classifier in the decision tree. The third problem is that it is hard to adjust the decision tree 
when the new training samples are given to the decision tree. In this study, four design 
  3
策樹分類時間(computing time)，自動找出分支節點適合的分支樹，也就是說，
例如，可調適性化模糊可變分支決策樹的成長方法(Growing method)是依據分
類錯誤率與決策樹分類時間的比值而定。 
(二) 本計畫提出一個以遺傳演算法為基礎的群聚分類參考點搜尋方法
(classification points search method)搜尋決策樹節點適合的分類參考點，
未來這些分類點將取代傳統以群聚中心當分類點的方式，這些分類參考點將取
代傳統以群聚中心當分類點，這對於決策樹的辨識率有顯著的提升。 
(三) 本計畫擬提出一個在有限空間與分類錯誤率條件下的模糊可變分支決策
樹 (Fuzzy Variable-Branches Decision Tree with Storage and 
Classification Error Rate Constraints)建立的方法，模糊可變分支決策樹
會依照使用者事先設定的可用空間與分類錯誤率的門檻值，進行設計模糊可變
分支決策樹，在已發表的學術文章中上缺乏決策樹建立時同時滿足分類錯誤率
與空間限制的門檻值，所以本計畫擬提出的在有限空間與分類錯誤率條件下的
模糊可變分支決策樹依照空間與辨識錯誤率的比值達到最佳化的設計。 
(四) 本計畫擬提出一個決策樹動態適應調整法(Decision Tree Adaptive 
Adjustment Method, CTAAM)，這種決策樹動態適應調整法將使得決策樹隨著
接收不同的訓練資料進行可適應系的動態調整，目前已發表的文章中對於決策
樹的動態調整如[18]-[19]，但這些調整只是針對群聚的大小跟距離進行分支
與合併的調整，但卻缺乏探討對決策樹調整後辨識率的提升問題，本計畫著重
依據群聚的特性進行調整決策樹，決策樹動態適應調整法主要分為兩部分的調
整，第一是分類參考點動態調整方法，第二是決策樹分支個數動態調整，藉著
計算兄弟節點間分類點的關係性，進而對節點的分類參考點進行調整，並且增
加新的分類點或刪除不需要的分類點，所以在決策樹中每個節點所包含的分類
點個數及位置會隨著決策樹的測試資料進行修正。 
 
文獻探討 
決策樹已經普遍應用在辨識與分類等應用上([1]-[11])，決策樹大致可以分兩
種，第一種決策樹稱為單一特徵比對決策樹(single-variable decision tree)，
是指每一個結點包含有一個判斷值，輸入樣本依照在決策數中特徵比對的順序進行
分類，也就是輸入樣本每次只在每個節點比對一個特徵，這類的方法如[12-15]，
第二種決策樹稱為多特徵比對決策樹，多特徵比對決策樹的每個節點包還有一個多
維的比對向量，當輸入向量在多特徵比對決策樹進行分類時，輸入向量會與節點中
的比對向量進行比對，這類多特徵比對決策樹近年來已有發表如[11]，這兩種決策
樹近年來已備廣泛應用，通常當多維度樣本資料中的特徵彼此間的關係度很高時，
例如將決策樹應用在語音與影像辨識時，由於語音與影像的資料特徵彼此關係度相
當高，多特徵比對決策樹表現會比單一特徵比對決策樹為優[11]，所以本計畫擬發
展多特徵比對決策樹為目標。 
  5
代表當輸入樣本分類到節點 it 時所需比較的次數，另外， )( itR 代表決策樹的節點 it
的分類錯誤率(classification error rate)，則 
2
))(),((
))())((()( ∑
×∈
−=
ii OCkYkX
iii MkYkXutR  (2) 
其中， ))(( kXui 代表訓練樣本 X(k)在節點 it 的模糊歸屬度，Y(k)代表 X(k)所屬已
知的類別， iM 代表節點 it 所表示的類別， iM 定義如下： 
∑
∑
∈
∈=
i
i
tkYkX
i
tkYkX
i
i kXu
kYkXu
M
))(),((
))(),((
))((
)())((
 (3) 
當 )( itR =0 時表示節點 it 所包含的資料皆屬同一類別(pure class)，當 )( itR 值越
大，代表著節點 it 包含有不同類別的資料，這些節點很可能會在決策樹成長過程當
中進一步的被分支，最後，假設 R(T)代表決策樹 T的分類錯誤率，則 
∑
∈
=
Ht
ii
i
tRtPTR )()()(  (4) 
本計畫所擬提出的模糊可變分支決策樹，最重要的是決策樹成長方法
(Growing Method)，本計畫擬提出的成長方法是一個同時考慮分類錯誤率與分類計
算時間的最佳化成長方式，假設節點 it 在決策樹 T中使用模糊遺傳演算法進行分成
z個分支 ziii ttt ,2,1, ,...,, 而產生一個新決策樹T ′，則 
∑
≠∈
+=
itj
Hj
ii tVtPjVjPTV )()()()()( , (5) 
∑
≠∈
+=
itj
Hj
ii tRtPjRjPTR )()()()()( , (6) 
∑ ∑
≠∈ =
+=′
itj
Hj
z
k
kiki tVtPjVjPTV
1
,, )()()()()(  (7) 
∑ ∑
≠∈ =
+=′
itj
Hj
z
k
kiki tRtPjRjPTR
1
,, )()()()()( . (8) 
依照圖三，定義該節點 it 的λ值，如下： 
)()(])()([
)()(-)()(
)(-)(
)(-)(
1
,,
1
,,
ii
z
k
kiki
z
k
kikiii
tVtPtVtP
tRtPtRtP
TVTV
TRTR
V
R
λ
−
=′
′=Δ
Δ=
∑
∑
=
=  (9) 
在本計畫所設計的成長方法中，選擇有最大λ值的節點優先進行分支，因為λ值越
大的節點所代表的意義是當對此節點進行分支時，決策樹在相同的計算時間下，分
類錯誤率可以達到最低，所以對此節點進行分支將使得模糊可變分支決策樹達到趨
  7
j
j ChX ∈)( ，若 )(rX i 滿足  
)()(min)()(
)( 
kXhXrXhX ij
CkX
ij
i
i
−=−
∈
, ji ≠ . (11) 
則稱 )(rX i 為群聚 Ci的一個相鄰點，我們可以依照每個群聚間分別去找出每個群聚
的相鄰點，這些相鄰點在群聚分類點搜尋方法的第二階段將會被確定哪些相鄰點可
以進一步被視為群聚的分類參考點，所以在第一階段結束後，假設節點 t有 z個分
支，可以得到群聚 Ci 的相鄰點集合稱為φ(Ci)， zi ≤≤1 。 
第二階段︰ 
在群聚分類點搜尋方法的第二階段，本計畫擬提出一個分類參考點遺傳演算
法用來確定群聚中的分類參考點，假設群聚 Ci 的相鄰點集合稱為φ(Ci)，則在遺傳
演算法中的位元字串設計長度為包含在φ(Ci)的相鄰點數目 ni，則位元字串 R 如下
所示：  
b1 b2 …………… … bni 
其中，若 bi=1，表示第 i個相鄰點被視為分類參考點，若 bi=0，則否，若位元字
串 R 有 k 個位元為 1，則表示該位元字串會產生 k 個分類參考點，所以群聚 Ci 應
該要有幾個分類參考點完全由遺傳演算法的適應度函數決定，遺傳演算法的適應度
函數定義如下： 
若 )(kX i 表示群聚 Ci 的第 k個訓練樣本資料，則下式必須滿足  
)()(minmin)(
)(
   
1
qXkXPkX ji
CqX
ji
zji
i
j
j
−<−
∈≠≤≤
j
i
ji
zj
PkX −≤
≠≤≤
)(min
   
1
 (12) 
令 Gδ  為滿足第(5)式的訓練樣本數目，則若 Gδ 等於群聚 Ci 所包含的樣本數目，
則表示位元字串所表示的分類點 Pi 可以取代群聚 Ci.的中心點，作為分類輸入測試
樣本的參考點，另外，我們希望分類點在群聚中越靠近其他以外的群聚越好，如此
可以降低測試樣本若在群聚間時決策樹分類錯誤的情況，所以，令
)}(ˆ),...,2(ˆ),1(ˆ{)( i
iii
i fXXXC =φ  表示群聚 Ci 中靠近其他群聚的訓練樣本資料集
合，則定義 GΩ 表示分類參考點靠近其他群聚的測量值，如下： 
i
f
k
i
i
G f
kXP
i∑
=
−
=Ω 1
)(ˆ
 (13) 
當 GΩ 越小表示分類參考點越靠近其他的群聚，所以遺傳演算法的適應度函數值可
以歸類為如下： 
⎪⎪⎩
⎪⎪⎨
⎧
=Ω+
<
=
iG
G
iG
i
G
m δif
m δif
mGFitness
     11
       
)(
δ
, (14) 
其中，θ 為一個常數且 im 表示群聚 Ci 所含的訓練樣本資料量，當某的位元
  9
考量 )(TW 跟節點的比對時間，所謂節點的比對時間 )(TV 定義如下︰ 
∑
∈
=
Ht
ii
i
tLtPTV )()()(  (16) 
其中 ti 表示模糊可變分支決策樹的葉節點，P( it )表示樣本資料落在節點 ti 的機
率，L( it )表示輸入樣本資料到達葉節點 ti 所需要的比對節點次數。 
在消除階段時，本計畫會計算每個內節點的δ 值，δ 值定義如下︰ 
)()(
1
)()(
)()(
TTVTVTV
TWTW
V
W
it −′=−′
′−=Δ
Δ=δ  (17) 
其中，T ′表示 T在消除節點 ti 後的模糊可變分支決策樹，在消除階段中，我們選
擇擁有最大δ 值的節點進行消除，如此，可以得到一個最佳化的模糊可變分支決策
樹。 
特別注意的地方是在消除階段中我們只選擇內節點進行消除，對於葉節點不
在消除的選擇範圍內，因為每個葉節點代表一種類別的輸出，我們不可以隨便消除
這些葉節點，依照消除階段對內節點的消除方式，直到模糊可變分支決策樹的節點
(空間)需求達到使用者設定的門檻為止。 
 
結果與討論 
本年度的實驗採用 speech 和 texture 資料庫，speech 資料庫包含有 600 個
母音，其中 300 個母音為訓練樣本，另外 300 個母音為測試樣本，每一個母音以
11025Hz 錄製成 16 位元的音節，每個母音被切成 5ms 為一個音框，每個音框以複
利葉轉換成一個包含有 32 維度的向量，在 texture 資料庫中，如下圖一所示，
texture 資料庫包含有 30 個紋理影像，每個紋理影像被切割成兩半，每一半影像
分別當成訓練影像與測試影像，每張紋理影像必須切割成 4*4 像素的區塊，每個區
塊透過 wavelet 轉換成一個 16 維度的向量，由圖二可知，本計畫所提出的 FVBDT
與 CFDT 做比較，我們可以發現 FVBDT 的辨識率比傳統的 CFDT 為高。 
 
 
  11
(b) 本年度計劃最重要的貢獻事提出分類參考點概念跟作法，節點中的分類參
考點將取代傳統以群聚中心當分類點的方式，分類參考點對於決策樹辨識
效率提升有很大的幫助，事實上，群聚中心並不適合當作決策樹節點的分
類比對，所以本計劃也提出一個群聚分類參考點搜尋方法，這個方法可以
用在目前已發表的決策樹上，可用的貢獻很大。 
(c) 本年度研究的貢獻是提出一個有限空間與辨識錯誤率條件下的模糊可變
分支決策樹設計方法，設計方法首先提出以決策樹的節點個數與節點比對
次數的比值來決定哪一個內節點要被刪除，並且對內節點刪除時對決策樹
的辨識錯誤率並不會提高，這種設計方法值得傳統決策樹用來降低儲存空
間的一個最佳化方法。 
參 考 文 獻 
[1] J. R. Quinlann, “Induction of decision tree,” Mach. Learn. 1, pp. 81-106, 1986. 
[2]  L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, Vlassification and 
Regression Trees. Belmont, CA:Wadsworth, 1984. 
[3]  A. Dobra and M. Schlosser, “Non-linear decision trees-NDT,” in Proc. 13th Int. 
Conf. Machine Learning (ICML’96), Bari, Italy, Jul. 3-6, 1996. 
[4]  S. K. Murthy, S. Kasif, and S. Salzberg, “A system for induction of oblique 
decision trees,” J. Artificial Intelligence Res., vol. 2, pp. 1-32, 1994. 
[5]  W. Pedrycz and Z. A. Sosnowski, “Designing decision trees with the use of 
fuzzy granulation,” IEEE Trans. Syst., Man, Cybern. A, Syst., Humans, vol. 30, 
no. 2, pp. 151-159, 2000. 
[6]  R. Weber, “Fuzzy ID3; A class of methods for automatic knowledge 
acquisition,” in Proc. 2nd Int. Conf. Fuzzy Logic Neural Networks, pp. 265-268, 
1992. 
[7]  S. B. Gelfand, C. S. Ravishankar, and E. J. Delp, “An iterative growing and 
pruning algorithm for classification tree design,” IEEE Trans. Pattern Analysis 
and Machine Intell., vol. 13, no. 2, pp. 163-174, 1991. 
[8]  O. T. Yildiz and E. Alpaydin, “Omnivariate decision trees,” IEEE Trans. Neural 
Network, vol. 12, no. 6, pp. 1539-1546, 2001. 
[9]  H. Zhao, and S. Ram, “Constrained cascade generalization of decision trees,” 
IEEE Trans. Knowledgement and data Engineering, vol. 16, no. 6, pp. 727-739, 
2004. 
[10]  M. M. Gonzalo, and S. Alberto, “Using all data to generate decision tree 
ensembles,” IEEE Trans. Systems, Man, Cybernetics, C, Applications and 
Reviews, vol. 34, no. 4, pp. 393-397, 2004. 
[11]  P. Witold, and A. S. Zenon, “C-fuzzy decision trees,” IEEE Trans. Systems, Man, 
Cybernetics, C, Applications and Reviews, vol. 35, no. 4, pp. 498-511, 2005. 
[12]  X. Wang, B. Chen, G. Qian, and F. Ye, “ On the optimization of fuzzy decision 
