                                           2
行政院國家科學委員會專題研究計畫 
第二年期末報告 
 
總計畫: 以生理訊號為基礎之人機介面設計與應用 
—人類操控機器的新模式 
 
子計畫二﹕人類情緒與生理訊號監控系統研發及其於健康
狀態偵測與維護之應用(2/2) 
 
(Subproject II) A Study of Emotional and physiological Signal Monitoring 
System for Intelligent Health Care Application (II) 
 
計畫編號 : NSC–95–2221–E–009–212 
 
執行期限：95/08/01–96/07/31 
 
主持人：張志永    交通大學電機與控制工程學系教授 
 
 
一、 中文摘要 
本計畫研發一個以影像為基礎的人類動
作、表情情緒與生理訊號監控系統，並使用機器
學習技術，由所監控影像之動作、表情與生理訊
號共同建構一個健康狀態辨認知識系統，以驗證
其於健康狀態自動偵測與健康維護應用之可行
性。上年度已研發藉由彩色 CCD 攝影機拍攝家
裏人的運動情況，並能自動分析他的動作；亦即
本報告提出一個能夠自動監控並且辨識人類動
作的初步研究。為提高監控系統的之穩定性與可
靠性，增強危害安全行為偵測系統的全天候適應
性，我們本年年度研發提出一個在前、後景色彩
深淺差別小時，仍然可以自動監控、追蹤辨識人
類動作的系統。在一般前、後景色彩深淺差別大
時，可以簡單的使用亮度的資訊將前後景分離，
但當前後景亮度接近時，例如; 當辨識的目標穿
著和背景相似的衣服時，若只使用灰階影像並無
法將完整的前景資訊分離，因此我們使用 HSV
色彩空間加入像素點色彩成分的考慮建立背景
模型，達到前、後景的分離，且能對陰影的問題
加以消除改進。但是使用 HSV 色彩空間必須先
解決色調一些不穩定的問題，所以我們在色調不
穩定的區域加以限制，以增加抽取前景影像的準
確性。  
將抽取的影像以二值化，再將經過特徵空間
以及標準空間轉換，投影至標準空間。經由樣板
比對的方法將三張影像合為一個姿態變化序
列，此影像序列乃從動作視訊 5:1 減低抽樣獲
得。接著，利用模糊法則的推論方法，將這組時
序姿態序列分類為某一個動作類別。跟單用亮度
成分的方法比較，實驗證明，HSV 色彩空間不但
在前景影像抽取有明顯的改進,而且在人體動作
辨識結果也有顯著的改進；也相當地提高了監控
系統的之穩定性與可靠性。 
 
關鍵詞：前後景分離、HSV色彩空間、動作
辨識、健康監控系統 
 
                                                                                      4 
 
                                        
each position. If the pixel’s absolute value of the 
subtraction operation is over the maximum 
interframe difference, the pixel is a foreground one. 
W4 only consider the luminance change to subtract 
the background. Under the W4 framework, we 
cannot detect a foreground pixel correctly when it is 
similar in color to the background pixel. To make 
fully use of the spectrum of a pixel, it is imperative 
to do the segmentation in the color domain. To this 
end, foreground subject extraction is done in the 
HSV color space. We can have both the luminance 
information and the chromatic information in the 
background subtraction task. Furthermore, the 
moving cast shadows mostly exhibit a challenge for 
accurate foreground subject detection. A lot of 
attempts have been developed to tackle the shadow 
suppression [6]–[8] encountered in background 
subtraction. Horprasert et al. [6] and Cucchiara et al. 
[7] utilized the rationale that shadows have similar 
chromaticity, but lower brightness than the 
background model. Under the proposed framework 
in the HSV color space, we can effectively identify 
the shadow existent in our detected foreground 
subject.  
Our action recognition system can be 
separated into three components. The first 
component is foreground subject extraction. The 
second component is transformation of image data 
in a space smaller and easier for posture recognition. 
The third component is posture classification of an 
image frame and activity recognition using posture 
sequences. 
 
2.  The HSV Color Space 
The HSV (hue, saturation and value) color 
space corresponds closely to the human perception 
of color. Conceptually, the HSV color space is a 
cone. Viewed from the circular side of the cone, the 
hues are represented by the angle of each color in 
the cone relative to the 0o line, which is 
traditionally assigned to be red. The saturation is 
represented as the distance from the center of the 
circle. Highly saturation color is on the outer edge 
of the cone, whereas gray tones (which have no 
saturation) are at the very center. The brightness is 
determined by the colors vertical position in the 
cone. At the point end of the cone, there is no 
brightness, so all colors are blacks. At the fat end of 
the cone are the brightness colors. 
The hue parameter is the value which 
represents color information without brightness. 
Therefore, the hue is not affected by change of the 
illumination brightness and direction. Although hue 
is the most useful attribute, there are three problems 
in using hue attribute for color segmentation: (1) 
hue is meaningless when the intensity value is very 
low; (2) hue is unstable when the saturation is very 
low; and (3) saturation is meaningless when the 
intensity value is very low [8]. Accordingly, Ohba 
et al. [9] used three criteria (intensity value, 
saturation, and hue) to obtain the hue value reliably. 
 
z Intensity Threshold Value: 
If 
tV V< , then 0H = , where V , tV , and H are 
an intensity value, the intensity threshold value, and 
a hue value, respectively. If measured color is not 
bright enough, the color is discarded. Then, the hue 
value is set to a predetermined value, i.e., 0. 
z Saturation Threshold Value: 
If 
tS S< , then 0H = , where S , tS , and H are 
an saturation value, the saturation threshold value, 
and a hue value, respectively. Using this equation, 
measured color close to gray is discarded in the 
image. 
z Hue Threshold Value: 
If 
tH P< Δ  or 2 tH Pπ− < Δ , then 0H = . The 
                                                                                      6 
 
                                        
    0,           if  ( , ) ( , ) ( , )  
                 or ( , ) ( , ) ( , )( , )
255,          otherwise                                         
V V V
i V
V V V
i V
I x y m x y k d x y
I x y n x y k d x yB x y
⎧ <⎪ <⎪= ⎨⎪⎪⎩
       (1) 
 
where ( ),  ViI x y  is the intensity of a pixel which is 
located at ( )yx, , ( )yxB  ,  is the gray level of a pixel 
in a binary image, and 
Vk  is a threshold, 
determined by light sufficiency of the scene. The 
value of 
Vk  is normally set to 1.3 for normal light 
condition, and 
Vk  will be reduced for in-sufficient 
light condition and increased otherwise. 
B.  Shadow Suppression 
The pixels of the moving cast shadows are 
easily detected as the foreground pixel in normal 
condition. Because the shadow pixels and the object 
pixels share two important visual features: motion 
model and detectability. For this reason, the moving 
shadows cause object merging and object shape 
distortion. Horprasert et al. [4] and Cucchiara et al. 
[5] utilize the rationale that shadows have similar 
chromaticity, but lower brightness than the 
background model. Hence, we can detect the 
shadow from foreground subject in the HSV color 
space. 
We define a shadow mask S  for each ( , )x y  
point as follows:   
shadow,           if     ( , ) ( , ) 0
                       and  ( , ) ( , ) (x,y) 
( , )                        and  ( , ) ( , ) (x,y)                         
   
  object,    
V V
i
H H H
i H
S S S
i S
I x y n x y
I x y m x y k d
S x y I x y m x y k d
− <
− <
= − <
        otherwise                                                                      
⎧⎪⎪⎪⎨⎪⎪⎪⎩
(2) 
where ( , )HiI x y , ( , )SiI x y , and ( ),  ViI x y  are 
respectively the HSV channel of a pixel located at 
( )yx, . Values Sk  and Hk  are selected threshold 
values used to measure the similarities of the hue 
and saturation between the background image and 
the current observed image. We can utilize the 
shadow mask ( , )S x y  to change the shadow pixels 
into background in ( , )B x y . 
C. Object Segmentation 
According to binary image B, we extract the 
region of foreground object to minimize the image 
size. Foreground region extraction can be 
accomplished by simply introducing a threshold on 
the histograms in X and Y direction. We can use 
these boundary coordinates as the corner of a 
rectangle to extract foreground region sB . 
D. Color Compensation 
Some colors such as yellow, pink, and light 
blue have similar luminance value. If we only use 
the luminance component to do background 
subtraction, we cannot detect foreground pixel 
correctly when its luminance is similar to that of a 
background pixel. In order to improve detectability, 
background subtraction is computed by taking into 
account not only a point’s luminance, but also its 
chromaticity. We want to use the chromaticity to 
enhance the accuracy of the foreground object. 
Based on the amount of the chromaticity change, 
we reanalyze its background in sB  to be changed 
to a foreground of object, by 
255,          if  ( , ) ( , ) (x,y)  
               or ( , ) ( , ) (x,y) ( , )
0,          otherwise                                        
S S S
i S
H H H
i H
f
I x y m x y k d
I x y m x y k dB x y
⎧ − >⎪⎪ − >= ⎨⎪⎪⎩
   (3) 
where ( , )HiI x y and ( , )SiI x y are respectively the hue 
and saturation components of a pixel at ( )yx, , Sk  
and Hk  are selected threshold values. fB  is the 
final foreground object after the refined step of Eq. 
(3). 
 
4.  Fuzzy Rule Construction and Classi- 
fication 
A.  Transformation of Image Data 
In video and image processing, the dimensions 
of image data are often extremely large. Because 
                                                                                      8 
 
                                        
sequence constitutes an input-output pair to be 
learned in the fuzzy rule base. In this setting, the 
generated rules are a series of associations of the 
form 
“IF antecedent conditions hold, THEN 
consequent conditions hold.” 
For example, a posture sequence, its first 
image belonging to P18, second image belonging to 
P20, third image belonging to P21, and belonging 
action WRL is given by 
 ( ) ( )LRWPPPDIII  ; , , ; , , 2120181321 =             (7) 
Hence this posture sequence supports the rule 
of 
“IF the activity’s I1 is P18 AND its I2 is P20 
AND its I3 is P21, THEN the activity is WLR.” 
 
E.  Classification Algorithm 
Firstly, a frame is fetched with a fix interval. 
The frame is transformed to canonical space and 
corresponding membership function is calculated. 
The membership functions of three images is 
gathered as a group to select the most similar rule in 
the fuzzy rule base built in training phase. The 
unknown image sequence is classified to the action 
recorded in the most similar rule [13]. 
 
5.  Experimental Results  
 
In our experiment, we tested our system on 
videos taken by digital camera. We took the video 
in our laboratory at the 5th Engineering Building in 
NCTU campus. The camera has a frame rate of 
thirty frames per second and image resolution is   
pixels. The background is not complex and we 
equipped a table in the scene. The light source is 
fluorescent lamps and is stable. 
We test the foreground detection capability in 
two cases depending on the color of clothing worn 
by action subjects. That the action subject wore the 
clothing with color different from that of 
background is the first case. And the second case is 
that the action subject wore the clothing with color 
similar to that of background. When the color of 
clothing and background are similar in the second 
case, a moving object, such as human body, may 
not be segmented easily from image frame. We 
compare the result in these two cases and the color 
compensation in our proposed system demonstrates 
eminent improvement in the segmentation quality. 
 
A. Background Model Construction 
We built the background model in the HSV 
color space. On observing the H, S, and V 
variations of pixels during the first 300 frames of 
the background video, we have found that V 
component is the most stable of the background 
model. H and S components are less stable than V. 
Thus, we have to use three criteria ( , ,t t tV S H ) to 
obtain the hue value reliably in building the 
background model. In our experiment, we set three 
criteria of Sec. 2 by  
50,  50, and 25t t tV S H= = =               (8) 
to make hue value more reliable. Fig. 1(a) shows 
the background image in the H color component. 
Fig. 1(b) is the corresponding H component image 
after adopting the above criteria to redefine H 
values; we can find that the hue values in the 
background image have become much more stable. 
 
B. The Experiment of Foreground Subject 
Extraction 
We use the shadow mask in Eq. (2), with Vk = 
1.3, Hk = 2, Sk = 2, and Vk = 1.3 in Eq. (1), to 
classify the pixels whether it is a shadow point or 
not. Fig. 4 shows the process result in shadow 
detection. Fig. 2(a) is input image. Fig. 2(b) is the 
                                                                                      10 
 
                                        
References 
 
[1] J. Yamato, J. Ohya, K. Ishii, “Recognizing Human 
Action in Time- Sequential Images using Hidden 
Markov Model,” In Proc. IEEE CVPR, pp. 379 385, 
1992. 
[2] F. Bobick and J. W. Davis, “The recognition of 
human movement using temporal templates,” IEEE 
Trans. Pattern Anal. Machine Intell., vol. 23, no. 3, 
2001. 
[3] S. Park and J. K. Aggarwal, “Segmentation and 
Tracking of Interacting Human Body Parts under 
Occlusion and Shadowing,” in Proc. of the 
Workshop on Motion and Video Computing, pp.105 
111, 2002. 
[4] S. Jabri, Z. Duric, H. Wechsler, and A. Rosenfeld, 
“Detection and Location of People in Video Images 
Using Adaptive Fusion of Color and Edge 
Information,” in Proc. Int. Conf. Pattern 
Recognition, pp. 627 630, 2000. 
[5] I. Haritaoglu, D. Harwood, and L. S. Davis, “W4: 
Real-Time Surveillance of People and Their 
Activities,” IEEE Trans. on Pattern Analysis and 
Machine Intelligence, vol. 22, no. 8, pp. 809 830, 
2000. 
[6] T. Horprasert, D. Harwood, and L.S. Davis, “A 
Statistical Approach for Real-Time Robust 
Background Subtraction and Shadow Detection,” in 
Proc. IEEE ICCV, 1999.  
[7] R. Cucchiara, C. Grana, M. Piccardi, and A. Prati, 
“Improving Shadow Suppression in Moving Object 
Detection with HSV Color Information,” in Proc. 
IEEE Intelligent transportation System Conference, 
pp. 334 339, 2001.  
[8] B. Chen and Y. Lei, “Indoor and Outdoor People 
detection and Shadow Suppression by Exploiting 
HSV Color Information,” in Proc. Fourth 
International Conference on Computer and 
Information Technology, pp. 137 142, 2004.  
[9] K. Ohba, Y. Sato, and K. Ikeuchi, 
“Appearance-based visual learning and object 
recognition with illumination invariance,” Machine 
Vision and Applications, Vol. 12, No. 4, pp. 189 196, 
2000. 
[10] P. S. Huang, C. J. Harris, and M. S. Nixon, 
“Canonical space representation for recognizing 
humans by gait or face,” in Proc. IEEE Southwest 
Symp. Image Anal. Interpretation, pp. 180–185, 
1998. 
[11] L. Li, W. Huang, I. Y. H. Gu, and Q. Tian, 
“Statistical Modeling of Complex Backgrounds for 
Foreground Object Detection,” IEEE Trans. Image 
Process, vol.13, no.11, pp.1459 1472, 2004. 
[12] L. X. Wang and J. M. Mendel, “Generating fuzzy 
rules by learning from examples,” IEEE Trans. Syst., 
Man Cybern, vol. 22, no. 6, pp. 1414–1427, 1992. 
[13] J. Y. Chang, C. T. Leu, and C. W. Cho, “Fuzzy rule 
inference based human activity recognition for 
health care applications,” in Proc. 2006 CACS Conf., 
2006. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
                                                                                            1 
 
國立交通大學補助教師 
 
出席國際學術會議報告 
 
 
 
 
研究計畫所屬類別：自然■工程生物人文科教 
 
報告人服務機構單位： 國立交通大學電機與控制工程學系 
 
報告人姓名： 張志永教授 
 
會議期間：  2007 年 6 月 13 日至 6 月 15 日 
 
會議地點： 土耳其 伊斯坦堡 
 
會議名稱： IEEE 2007 年智慧型車輛國際會議 (IEEE Intelligent 
Vehicles Symposium, IEEE IV2007) 
 
發表論文：“利用影像為基礎之前看交通影像場景分析 ＂張志永、卓
建文合作 
 
“Vision-Based Forward-Looking Traffic Scene Analysis 
Scheme” by J.Y. Chang and C.W. Cho 
 
 
電話：（公）(03) 5712121 (ext) 54336 
（宅）(03) 5722472 
 
 
報告日期： 中華民國九十六年七月二日 
                                                                                            3 
Japan; Gerd Wanielik, Germany,  
講題： Panel Discussion on State of Intelligent Vehicle Research/Deployment 
Around the World；  
 
教導演講於 6 月 12 日舉行，包括：  
(1) Mr. Richard Bishop, Italy.  
講題： World Tour: Intelligent Vehicle Product and Research Trends； 
(2) , Dr. Dale Sogge,  Sensata Technologies, United States 
講題：Vision Processing - Optics to Output； 
(3) O. Altintas and W. Chen, Workshop Chairs, 
講題：Workshop on Vehicle-to-Vehicle Communications (V2VCOM) 
等共 4 場次，每場次三小時，相當廣泛豐富。 
與會期間，本人除參與研究領域與我相近論文發表場次外，每天均參加大會
演講，以借鏡大師風範，吸收研發最新成果與方向。我的論文，“利用影像為基
礎之前看交通影像場景分析＂我及學生卓建文合作 (“A New Scene Analysis 
Vision-Based Forward-Looking Traffic Scene Analysis Scheme” by J.Y. Chang and 
C.W. Cho) 發表場次，於 6 月 13 日早上車輛環境感知場次張貼發表， 論文重點
如下：我們先利用模糊適應共振理論 (Fuzzy Automatic Resonance Theory) 對影像
之像素亮度，像素 Hue 值，像素垂直位置，做模糊適應共振分類，然後利用各類
的平均值以及變異數來建構對應類別之模糊集歸屬函數，依據這些歸屬函數，我
們可以利用監督式學習法來產生場景分析模糊若則法則，最後再利用模糊最大-
最小推論法來分辨影像中的天然景物達到影像場景分析。接著我們更加上前看交
通影像一些基本事實，如路在天空下方，車輛須行駛於路上，再輔以影像邊緣偵
測技術，可將車前影像場景分割(析)成道路、天空、樹、車輛與其他等區塊。我
們測試了 20 張一般的道路影像以評估我們的系統的效能，我們達到 86.6%正確
率。發表演講期間發問熱烈，計有德國、美國、大陸等學者、公司研發人員詢問
討論廣泛，氣氛極佳。6 月 15 日早上，本人代表本系宋開泰教授張貼發表其論文。 
