二、研究目的與方法
本研究計畫以小腦模型[1]-[2]與灰關
聯分析[3]為理論核心，並以兩種一維適應
成長式小腦模型[4]-[5]為基礎，提出二維架
構之理論延伸；進而以二維適應成長式小
腦模型為核心，建立高維小腦模型之網路
架構；最後，分別研究與實現二維或高維
的適應成長式小腦模型在函數近似和路徑
規劃等方面之理論應用。
(一)傳統一維小腦模型
假設小腦模型所欲近似之目標函數為
f(x)且已知的訓練範例有 p組，即(xi, ti)，其
中 ti = f(xi)，i = 1, 2, …, p，且每個狀態均使
用 Ne個記憶單元；若假設初始之學習空間
僅被等距量化成 m (m << p)個狀態，亦即數
個不同之訓練範例可能對應到同一個狀
態，則一維小腦模型所使用之記憶單元個
數為 n = m + Ne1。若輸入 xi屬於第 k個
狀態 sk時，即 xi sk，則按第(1)式便可求
得 xi所對應之實際輸出值 yi，即
ksi
yy  ,
xisk。
wcTss kky  
n
j jjs wc k1 , , (1)
在第(1)式中， ]...,,,[ 21 n
T wwww 表示記
憶單元所儲存的值， ...,,,[ 2,1, kkk ss
T
s ccc
],nskc 代表一組記憶單元指標並表示狀態 sk
應對應到哪些記憶單元。至於學習規則乃
是利用實際輸出值 yi與目標輸出值 di的誤
差來更新記憶單元內容值，即
).( )1()1()(   tiis
e
tt yd
N k
cww

(2)
其中表學習速率。若每一狀態均對應一目
標輸出值時，則按第(3)式，便可求得記憶
單元儲存值 w之“理論”解 w*，此理論解即
為記憶單元之初始值。
yCw ˆ , (3)
其中 Tnyyy ]ˆ...,,ˆ,ˆ[ˆ 21y ， 1)(  TT CCCC
為記憶單元指標矩陣 C之 pseudo-inverse。
(二)二維適應成長式小腦模型
1. 網路架構及其運算
本論題以文獻[6]所提之 MS_CMAC
網路架構為基礎，此網路架構為一種二位
元之樹狀結構，每一節點(node)均由一個一
維小腦模型來實現；圖 1 為一個二層樹狀
結構之MS_CMAC示意圖，此網路架構可
實現一個二維小腦模型。本論題所提出之
二維適應成長式小腦模型(2D AG-CMAC)
的網路架構如圖 2 所示，此架構類似於圖
1，然而，為降低一維小腦模型之使用個
數，Root 層之一維小腦模型擬以一個具線
性內插功能的節點(I)來取代，而 Leaf層之
各節點(N)則是以 1D AG-CMAC來實現；
因每一個小腦模型的學習過程均是獨立而
互不相干的，故可形成每一個小腦模型的
量化結果不相同，如圖 3所示。
若以 p = ),( 21 xx  表示二維小腦模型之
任一輸入，M 表示所使用之一維小腦模型
個數，則 2D AG-CMAC之運算過程如下：
圖 1. 二維MS_CMAC之網路架構
圖 2. 二維適應成長式小腦模型之
網路架構















 




.if
,if,
,if,
)(
)()( ,22,,
,221,2,
1,2,2
1,22
1,
1,2,2
2,2
1,221,
2
i
NNi
i
j
i
jjii
j
i
j
i
j
jii
j
i
j
i
j
i
i
i
isis
xxy
xxxy
xx
xx
y
xx
xx
xxy
xC
(9)
其中Ns(i)表第 i個小腦模型所使用之狀態個
數，而 jiy , 表狀態 si,j所對應之實際輸出，。
採用線性內差來計算輸出值之主要目的在
於降低小腦模型因量化所產生的誤差。
步驟 4：評估狀態成長停止條件。
可依學習誤差或狀態個數來做為評估
停止條件的依據。若滿足停止條件，則中
斷狀態成長過程；反之，則跳至步驟 5。
步驟 5：進行灰關聯分析。
對某一 1D AG-CMAC而言，其每一狀
態學習結果的優劣，可由以下灰關聯分析
來判斷。
1)建立參考與比較序列。
參考序列： )
~
...,,
~
,
~
(
2,2,1,
)(
pNiii
r
i dddv
比較序列： ...,),(),(( 2,21,2
)( xCxC ii
c
i v
)).(
2,2 pNi
xC
其中
,),(
)(
1
)(
1~
,1,1
,2,1,, 


ii UxiUxi
i xxfUN
d
UN
d


而 N(Ui)表示落在區域 Ui = [gi1, gi]內 x1,
的個數。以此平均方式所求得之 ,
~
id 值可當
作 )( ,2 xCi 所對應之目標值。
2)計算灰關聯係數。
max
maxmin
,2, ))(,
~
(

 


 xCdr ii , (10)
其中 )(
~
,2,  xCd ii  ， ]1,0( 表示分辨
係數，   maxmax ，   minmin x 。
3)計算灰關聯度。
),(:))(,
~
(
)(
1
),( ,,2,
,
)()(
,,2
ji
Vx
ii
ji
c
i
r
ij sgxCdrVN
g
ji
 

vv
(11)
其中 N(Vi,j)表示落在區域 Vi,j = [qi,j1, qi,j]內
x2,的個數。
步驟 6：狀態成長。
若 g(si,j)，表示一預設之狀態分割
門檻值，則表示狀態 si,j之學習誤差較大；
為增加 AG-CMAC之學習精確度，可在狀
態 si,j 所對應之區間[qi,j1, qi,j]內新增一節
點，將此狀態由一分割為二，亦即狀態個
數會成長 1 個。若有數個狀態同時滿足上
述條件，則這些狀態需同時重新分割。
因輸入空間重新分割後，需重新計算
虛擬訓練範例，故需返回步驟 1，進行下一
階段之狀態成長流程。
3. 小腦模型成長過程
以圖 3 為例，上述狀態成長過程主要
在於針對 x2 軸進行量化，因每一個 1D
AG-CMAC 之狀態成長過程均是獨立的，
故會造成不同的量化結果；而小腦模型成
長過程則是針對 x1軸進行量化，如此便可
產生二維輸入空間適應性量化的結果。小
腦模型成長過程的步驟如下所述：
步驟 1：計算網路的輸出。
利用第(6)式針對每一個輸入 p, =
(x1,, x2,)計算其對應之輸出 y,。
步驟 2：進行灰關聯分析。
1)建立參考與比較矩陣。
參考矩陣：D = [d,]，Np1Np2矩陣
比較矩陣：Y = [y,]，Np1Np2矩陣
2)計算灰關聯係數。
max
maxmin
,, ),(ˆ 


 
ydr , (12)
其中 ,,,  yd  ，   maxmaxmax  ，

 minminmin  ，而表示分辨係數。
3)計算灰關聯度。
,),(ˆ
)(
1
)(ˆ
,
,,


kRk
k ydrRN
Rg


p
(13)
其中 )( kRN 表示落在區域 kR 內輸入的個
數，而 iR ],[],[ 22,11,1 ULii xxxx  ，i = 1, 2, …, 
M + 1， Lxx 10,1  ， UM xx 11,1  。
步驟 3：評估小腦模型成長停止條件。
依學習誤差或小腦模型個數來做為評
圖 8. 範例一各階段之輸出
圖 9. 範例二之學習結果
經九次學習循環之後，適應成長的結
果為：12個小腦模型、狀態總數為 76個，
量化結果如圖 7所示。由圖 8可知，因為
目標函數 f為一對稱函數，經由學習之後，
可發現所提之狀態與小腦模型的成長過程
亦可將輸入空間做對稱式的量化；而且當
斜率越大時，所分割的狀態數或小腦模型
的個數亦越多，亦即所提方法可達到適應
性量化小腦模型輸入空間之目的。
【範例二】2維函數
f(x1, x2) =
2.5exp(2x10.5x2)·sinc(x1)·sinc(x2),
(x1, x2)[0.0, 2.0][0.0, 2.0]
初始條件：兩個 1D AG-CMAC，且每一個
1D AG-CMAC之初始狀態個數為 2。
圖 9 為範例二之學習結果，經七次學
習循環之後，適應成長的結果為：8個小腦
模型、狀態總數為 61個。當 x1 > 1時，目
標函數的值幾乎等於 1，類似一個非常平坦
的平面，故此區域經由適應成長之後，僅
需使用三個 1D AG-CMAC，但其狀態個數
依然維持 2個（狀態個數沒成長）。當 x1 <
1時，目標函數變化情形較大，故該區域需
使用到五個 1D AG-CMAC，且狀態個數從
8到 13個不等。整體而言，也可以達到適
應性量化小腦模型輸入空間之目的。
(二)高維適應成長式小腦模型
【範例三】3維函數
f(x1, x2, x3) = sin(x1)·sin(2x2)·sin(3x3),
(x1, x2, x3)[0, 1][0, 1][0, 1]
初始條件：每一個 2D AG-CMAC均包含兩
個 1D AG-CMAC，且每一個 1D AG-CMAC
之初始狀態個數為 2。
圖 10為範例三之網路拓樸架構，其需
使用兩個 2D AG-CMAC。圖 11為其學習
結果與每一個 2D AG-CMAC 的輸入空間
適應性量化的結果，相關數據列於表 1 之
中。經學習之後，整個網路需使用 15個 1D
AG-CMAC，而狀態總數為 340，學習誤差
為 0.0046。因無法直接顯示三維之目標函
數，故在圖 11當中，僅表示 f(0.5, x2, x3)與
f(x1, x2, 0.5)這兩個特定函數之目標值與對
應之學習結果。
表 1. 範例三相關數據
小腦模型
個數
狀態個數 學習誤差
MSE
CMAC 1 7 152 0.0069
CMAC 2 8 188 0.0046
整體網路 15 340 0.0046
2x1x 3x
y
12y
圖 10. 範例三之網路拓樸架構
(二)達成預期目標情況
本研究計畫除提出二維和高維適應成
長式小腦模型之理論延伸外，亦研究其在
函數近似之理論應用，這些項目均按照預
期目標達成。
(三)研究成果之學術或應用價值、是否適合
在學術期刊發表或申請專利
1. 提出適應成長式小腦模型的理論延伸，
在學術上可拓展小腦模型的研究領域。
2. 提出新的小腦模型控制系統的設計概
念，可拓展智慧型控制的研究領域與小
腦模型控制系統的設計方式。
3. 本計畫至目前已衍生出的 3 篇學術論文
[12]-[14]，其中 1篇已出版[12]，1篇已
接受[13]，另外一篇[14]則是正在審查
中，其他相關論文持續研究中。
(四)主要發現或其他相關價值
可訓練參與計畫之人員研究小腦模型
理論和其他相關控制理論，並可奠定將來
深入研究之基礎。
參考文獻
[1] J.S. Albus, “A new approach to 
manipulator control: the cerebellar
model articulation controler (CMAC),” 
Journal of Dynamic Systems,
Measurement, and Control, Trans. of
ASME, vol. 97, no. 3, pp. 220-227, 1975.
[2] J.S. Albus, “Data storage in the 
cerebellar model articulation controller
(CMAC),” Journal of Dynamic Systems,
Measurement, and Control, Trans. of
ASME, vol. 97, no. 3, pp. 228-233, 1975.
[3] J.L. Deng, “Introduction to grey system 
theory,” The Journal of Grey System, vol.
1, pp.1-24, 1989.
[4] M.F. Yeh and K.C. Chang, “Adaptive 
Growing Quantization for 1D CMAC
Network,” in Proc. of the Sixth 
International Symposium on Neural
Networks (ISNN 2009), Wuhan, China,
Lecture Notes in Computer Science
(LNCS), vol. 5551, pp. 118-127, 2009.
[5] M.F. Yeh and J.H. Tsai, “Grey adaptive 
quantization approach for 1D CMAC
network,” Journal of Grey System, vol.
12, no. 2, pp. 69-76, 2009.
[6] J. C. Jan and S. L. Hung, “High-order
MS_CMAC neural network,” IEEE
Trans. Neural Netw., vol. 12, no. 3, pp.
598-603, 2001.
[7] 鄧聚龍, 灰色系統理論與應用, 高立圖
書有限公司, 台北, 2000.
[8] 翁慶昌、陳家叢、賴仁宏, 灰色系統基
本方法及其應用, 高立圖書有限公司,
台北, 2001.
[9] J.L. Deng, “Introduction to grey system 
theory,” The Journal of Grey System, vol.
1, pp.1-24, 1989.
[10] H. M. Lee, C. M. Chen and Y. F. Lu, “A 
self-organizing HCMAC neural-network
classifier,” IEEE Trans. Neural Netw.,
vol. 14, no. 1, pp. 15-27, 2003
[11] C. M. Chen, C. M. Hong and Y. F. Lu,
“A pruning structure of self-organizing
HCMAC neural network classifier,” Proc.
2004 Int. Joint Conf. Neural Netw.,
Budapest, Hungary, vol. 2, p. 861-866,
2004.
[12] M. F. Yeh, “Two-dimensional adaptive
growing CMAC network,”Lecture Notes
in Computer Science (LNCS), vol. 6063,
pp. 262-271, 2010.
[13] M. F. Yeh and M. S. Leu, “Grey 
adaptive growing CMAC network,” 
Applied Soft Computing, revised.
[14] M. F. Yeh and M. S. Leu, “Hierarchical
adaptive growing CMAC network,” 
submitted to Neurocomputing.
無衍生研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
