 2
行政院國家科學委員會專題研究計畫成果報告 
 
發展一個以 PSO 為基礎之星狀座標軸視覺化資料分群方法  
 
計畫編號：95-2221-E-155-004- 
執行期限：95 年 8 月 1 日至 96 年 7 月 31 日 
 
主持人：蔡介元 元智大學工業工程與管理學系 
計畫參與人員：邱創政、陳致融、蕭清泉、林昭妏    
 
一、中文摘要 
對於資料分群分析(Data cluster analysis)而言，使用者總是關切所得到的分群結果是否能可靠地反映
真實的群聚現象，而「群集個數」與「初始群心位置」是分割式分群演算法的兩個會影響分群結果品質
之重要參數。在本研究中，我們提出一個以分群為導向之星狀座標的資料視覺方法(COSCT)，足以幫助
使用者以視覺方式決定這兩個重要參數。為了提升 COSCT 方法顯示資料群聚之能力，我們加入「衡量
屬性權重」與「調整座標軸角度」兩個程序。我們以資料彼此之間的相異度為基礎，建構一相異度之高
斯混合模型(Gaussian mixture model)，進而從此模型推導出可衡量屬性權重之指標。此外，我們有系統地
分析座標軸之間的夾角角度與群聚顯示品質兩者之間的關係，並建構成一個座標軸角度最佳化之問題。
進一步地運用粒子群最佳化方法(Particle swarm optimization)求解出最佳的座標軸角度。如此一來，透過
COSCT 方法，原本在多維度空間的資料點皆被投影到此分群為導向之星狀座標軸二維平面上，使用者便
可輕易地從中觀察出資料的群聚現象。 
 
關鍵詞：群集分析，資料視覺，星狀座標軸，高斯混合模型，粒子群最佳化 
 
 
Abstract 
When conducting a clustering process, users are always concerned whether the clustering result is reliable 
enough to reflect the actual clustering phenomenon. In partitional clustering algorithms, the number of clusters 
and initial cluster centers are two critical parameters that influence the reliability of clustering results highly. In 
this research, we propose a Clustering-Oriented Star Coordinate Translation (COSCT) method to help users 
determining the two parameters confidently. Through the COSCT method, all objects from a multi-dimensional 
space are adaptively translated to a 2D star-coordinate plane, so that the clustering parameterization can be 
easily conducted by observing the clustering phenomenon in the plane. To enhance the cluster-displaying quality 
in the star-coordinate plane, two new procedures are developed in the COSCT method. In the first procedure, the 
unit lengths of all coordinate axes are assigned based on their corresponding feature weights. A novel weight 
measure that evaluates the importance of each feature is developed using the Gaussian mixture model of data 
dissimilarity. In the second procedure, the rotation angles of all coordinate axes are adaptively arranged by the 
particle swarm optimization algorithm. The effectiveness of the proposed COSCT method is demonstrated using 
a set of experiments. 
 
Keywords: Clustering, Data visualization, Star coordinate, Gaussian mixture model, Expectation maximization, 
Particle swarm optimization.  
 
 
二、緣由與目的 
 
Clustering is a process of grouping objects into clusters so that objects in a cluster are similar to each other 
and are different from objects in different clusters [1]. Partitional clustering algorithms, such as K-means [2], 
Fuzzy C-means [3], and Possibilistic C-Means [4], are used most frequently in practice since they are not only 
implemented easily but also executed efficiently. However, users still raise their question whether the clustering 
results generated by these algorithms are reliable enough to reflect actual clustering phenomenon or not [5].  
When using partitional clustering algorithms, users first ask themselves how many clusters are proper to 
 4
variance associated with m1G , and 
m
2µ  and mvar2  are the mean and variance associated with m2G . The 
mixture probability density function of mndiss , termed as )(
m
ndissp , is expressed as:  
 
),|(),|()( 222111
mmm
n
mmmm
n
mm
n vardisspvardisspdissp µαµα ×+×=  (1) 
 
where m1α  and m2α  are the occurrence proportions of m1G and m2G in this mixture, and 
01 ≥mα ; 02 ≥mα ; 121 =+ mm αα . Given the N dissimilarities in terms of fm, the logarithm transform for the 
likelihood function of } ,,,,,{ 212111
mmmmmm varvarµµαα≡Φ  in this mixture is shown as Equation (2):  
 
( )∑ ×+×=
=
N
n
mmm
n
mmmm
n
m vardisspvardisspL
1
222111 ),|(),|(log)(log µαµαΦ  (2) 
 
We apply the expectation maximization (EM) algorithm [10] to infer Φ so that logL(Φ) can be maximized. 
For each dissimilarity mndiss , let 
m
np 1  and 
m
np 2  be the likelihood originated from the two Gaussian 
distributions m1G and 
m
2G  where 121 =+ mnmn pp . Therefore, Equation (2) can be further transformed as Equation 
(3):  
 
( ) ( )[ ]∑ +++=
=
N
n
mmmm
n
m
n
mmmm
n
m
n vardissppvardissppL
1
22221111 log),|(loglog),|(log)(log αµαµΦ     (3) 
 
To infer the optimal Φ, the expectation (E) step and maximization (M) step are alternately performed in the EM 
algorithm. Let ),(),(),(),({)( 2121 ttttt
mmmm µµαα=Φ  )}(),( 21 tvartvar mm  be the estimate of Φ at the iteration t. At 
the beginning, i.e. t=0, Φ(0) is generated randomly. At the iteration t, the E step computes the expectations of 
likelihood )(1 tp
m
n  and )(1 tp
m
n , n=1,2, …,N, by including the current estimate Φ(t) into Equation (3), which are 
shown as Equation (4) and Equation (5) respectively.  
 ( ) ( )
( ) ( )[ ] Nntvartdisstvart
tvartdisstvarttp
a
m
a
m
a
m
n
m
a
m
a
mmm
n
mm
m
n ,,1for     
)()]([exp)(1)(
)()]([exp)(1)(
)( 2
1
1111
1 L=∑ −−××
−−××=
=
µα
µα  (4) 
Nntptp mn
m
n ,,1for     )(1)( 12 L=−=  (5) 
 
Then, the M step infers the estimate Φ(t+1) which will be used in the next iteration (t+1) by including )(1 tp
m
n  
and )(1 tp
m
n  at the current iteration t into Equation (3). The equations used to calculate the six parameters in 
Φ(t+1) are shown as Equation (6), Equation (7), and Equation (8) respectively. The E and M steps are 
alternately repeated until |logL(Φ(t+1))–logL(Φ(t))| ≤ ε where ε is the user-specified stop criteria. When the EM 
algorithm has been stopped at the iteration t, the estimate Φ(t) can serve as the optimal Φ.  
 
⎪⎩
⎪⎨
⎧
∑⎟⎠
⎞⎜⎝
⎛∑ ×=+
∑⎟⎠
⎞⎜⎝
⎛∑ ×=+
==
==
N
n
m
n
N
n
m
n
m
n
m
N
n
m
n
N
n
m
n
m
n
m
tpdisstpt
tpdisstpt
1
2
1
22
1
1
1
11
)()()1(
)()()1(
µ
µ
 (6) 
( )
( )⎪⎩
⎪⎨
⎧
∑⎟⎠
⎞⎜⎝
⎛∑ +−×=+
∑⎟⎠
⎞⎜⎝
⎛∑ +−×=+
==
==
N
n
m
n
N
n
mm
n
m
n
m
N
n
m
n
N
n
mm
n
m
n
m
tptdisstptvar
tptdisstptvar
1
2
1
222
1
1
1
111
)()1()()1(
)()1()()1(
µ
µ
 (7) 
⎪⎩
⎪⎨
⎧
+−=+
∑×=+
=
)1(1)1(
)(1)1(
12
1
11
tt
tp
N
t
mm
N
n
m
n
m
αα
α  (8) 
 
Each of the N dissimilarities between two objects in fm can be decomposed as two types of components. One 
is the “intra-cluster” dissimilarity with a relative small value, which means the two objects could be located in 
the same cluster. Another is the “inter-cluster” dissimilarity with a relative large value, which means the two 
objects should be located in different clusters. Further, the mean of the N “intra-cluster” dissimilarities in fm, 
termed as m1µ , and the mean of the N “inter-cluster” dissimilarities in fm, termed as m2µ , are both known from 
the optimal Φ after performing the EM algorithm. According to the objective of clustering, when m1µ  is smaller 
and m2µ  is larger simultaneously, the importance of fm to clustering quality should be higher. Therefore, the 
proposed weight measure used to evaluate the importance of a feature fm to clustering is defined as Equation (9). 
In addition, the pseudo-code of the feature weighting procedure is shown as Fig. 2. Through this procedure, the 
 6
 
                   (a) right angle             (b) acute angle                 (c) obtuse angle  
Fig. 3.  When 0 < ρ(fm,fn) ≤ 1, let Ang(θm,θn) be acute to enhance the cluster-displaying quality. 
 
 
(a) right angle          (b) acute angle              (c) obtuse angle  
Fig. 4.  When -1 ≤ ρ(fm,fn) < 0, let Ang(θm,θn) be obtuse to enhance the cluster-displaying quality.   
 
(a) right angle             (b) acute angle                 (c) obtuse angle  
Fig. 5.  When ρ(fm,fn) = 0, the cluster-displaying quality is the same whether Ang(θm,θn) become acute or obtuse.   
According to the above analysis, the adjustment for the rotation angles of the two coordinate axes SCm and SCn 
for optimizing the cluster-displaying quality of the visualizing result can be modeled as an optimization 
problem:  
 
( )[ ] 2 ),(cos),(   nmnm AngMinimize θθρ −ff  (11) 
 
subject to 0o ≤ θm, θn < 360o. Based on similar concept, we extend the problem as an optimization problem in 
which the rotation angles of all M coordinate axes in SC are taken in account simultaneously. In addition, the M 
corresponding feature weights are also considered. That is, the coordinate arrangement of all coordinate axes for 
optimizing the cluster-displaying quality of the visualizing result can be modeled as:  
 
( )[ ]∑ ∑ −××−
= +=
1
1 1
2 ),(cos),( )()(    
M
m
M
mn
nmnmnm AngwfwfMinimize θθρ ffff  (12) 
 
subject to 0o ≤ θm < 360o for m=1,2,…,M. The minimization problem in Equation (12) can be considered as a 
combination of M(M-1)/2 problems of Equation (11) since all pairs of coordinate axes are considered 
synthetically. The coordinate arrangement procedure aims at find out the optimal angle set θ={θ1,…,θm,…,θM} 
which minimizes Equation (12).  
 
3.2.2  Using the PSO algorithm for optimizing the coordinate arrangement  
 
In the coordinate arrangement procedure, we adopt the particle swarm optimization (PSO) algorithm [9] to 
find the optimal angle set θ={θ1,…,θm,…,θM} for Equation (12). In the PSO algorithm, all particles in a swarm 
are evolved by cooperation and competition among themselves through generations. Let a swarm 
G={pr|r=1,…,R} consist of R particles. Each particle pr=(θr1,…,θrm,…,θrM) represents a feasible solution for θ 
where θrm is the rotation angle of the mth coordinate axis found in the rth particle. At the beginning, the initial 
value of each rotation angle θrm in each particle pr is randomly generated such that 0o ≤ θrm < 360o for 
m=1,2,…,M. In addition, the current flying velocity of pr is represented as vr=(vr1,…,vrm,…,vrM) in which the 
initial value of each velocity element vrm is randomly generated such that -10o ≤ vrm ≤ 10o for m=1,2,…,M. 
Because Equation (12) is a minimization problem, the fitness of each particle pr, termed as fitness(pr), is defined 
as:  
 ( )
( )[ ]∑ ∑ −××=
=
−
= +=
1
1 1
2 
1
),(cos),( )()(1                   
),,,,( )( 
M
m
M
mn
rnrmnmnm
rMrmrr
Angwfwf
fitnessfitness
θθρ
θθθ
ffff
p LL
 (13) 
 8
point position of a object xi in the star-coordinate plane, <X(xi),Y(xi)>, is defined as Equation (18) and Equation 
(19):  
 
IixfwX
M
m
mimmi ,...,1for     )cos()()(
1
=∑ ××=
=
θfx  (18) 
IixfwY
M
m
mimmi ,...,1for     )sin()()(
1
=∑ ××=
=
θfx  (19) 
 
where fw(fm) is the weight of the mth feature fm in F, and θm is the rotation angle of the mth coordinate axis SCr 
associated with fm. Fig. 7 shows an example of translating an object from a 5-dimensional space as a point in the 
star-coordinate plane. Through the COSCT method, all I objects are translated as points. It makes users observe 
clearly whether any clustering phenomenon of these points occurs in the star-coordinate or not. Therefore, users 
should be confident of their determined number of clusters and initial cluster centers.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 7.  Translating an object from a 5-dimensional space as a point in the star-coordinate plane. 
 
 
四、研究成果  
 
4.1   Determination of the number of clusters  
 
In this section, the Breast cancer, Iris, and Wine datasets derived from UCI machine learning repository [11] 
are used to demonstrate whether the number of clusters and initial cluster centers parameterized through the 
proposed COSCT method are reliable or not. The properties of the three datasets are listed as Table 1. In 
addition, the Principle Component Analysis (PCA) [12] and Multi-dimensional Scaling (MDS) [13], two 
common data dimension reduction techniques, serve as the comparison methods with the COSCT method. With 
PCA, the first two components are extracted from all objects and displayed as a 2D plane. Similarly, only two 
dimensions are retrieved through MDS to form a 2D plane. Note that the number of particles R is set as 100 and 
the maximum number of iterations Niter is set as 200 in the COSCT method for all experiments.  
Table 1.  The properties of the three multi-dimensional datasets derived from [11].  
dataset number of objects 
number of 
features 
number of 
classes 
Breast 
cancer 683 9 2 
Iris 150 4 3 
Wine 178 13 3 
 
Fig. 8 shows the cluster-displaying results of all objects using the three methods for the Breast cancer 
dataset. In each of the three results, obviously, a small and dense cluster can be easily observed while a large 
cluster grouped from other distributed points. No matter which methods are used, the two clusters can be clearly 
identified. It indeed conforms to the actual number of classes. Accordingly, the cluster-displaying results of all 
objects using the three methods for the Iris dataset are shown in Fig. 9. With PCA and MDS, the number of 
clusters identified by observing Fig. 9(a) and Fig. 9(b) equals to two. Although the suggestion is reasonable, no 
additional information can reveal that the larger cluster could be further partitioned into two smaller clusters. On 
the contrary, the result generated by the COSTC method reveals the number of clusters could be set as 2 or 3, 
depending on whether the larger cluster is partitioned into two smaller clusters or not. In this case, therefore, the 
COSCT method can provide more information for determining the number of clusters, which should cause more 
reliable clustering results. Finally, the cluster-displaying results of all objects using the three methods for the 
Wine dataset are shown in Fig. 10. Among the three results, the three clusters can be recognized most clearly 
through the COSCT method, shown in Fig. 10(c), since the discrimination among clusters are the most obvious. 
 10
 
4.3   Performance of the PSO algorithm for the coordinate arrangement  
 
The PSO algorithm is used to find the optimal rotation angle for each coordinate axis for uplifting the 
cluster-displaying quality of the star coordinate plane. To evaluate the efficiency of PSO, the genetic algorithm 
(GA) [14] is implemented as comparison. In GA, each chromosome, similar to a particle in PSO, in a population 
represents a feasible solution for θ using the real-valued encoding expression. Therefore, the fitness of a 
chromosome is also defined using Equation (13). Besides, the reproduction probability of a chromosome 
depends on its fitness, and the crossover operation is executed based on a single-point approach. The optimal 
fitness found by PSO and GA for the three datasets are illustrated as Fig. 12. Obviously, the efficiency of PSO is 
superior to the one of GA. As the number of coordinate axes is increasing, in addition, the iteration makes PSO 
start to converge its found optimal solution is delaying. For example, the optimal solution starts converging at 
about the 40th iteration in the Iris dataset. Relatively, the optimal solution in the Breast cancer dataset starts 
converging at about the 160th iteration.  
 
Fig. 12.  Computational efficiency of the PSO and GA algorithms for the three datasets.  
 
 
五、結論 
 
We present a Clustering-Oriented Star Coordinate Transaction (COSCT) method to enable users to be 
confident of determining the number of clusters and initial cluster centers for partitional clustering algorithms. 
The parameters determined using the COSCT method is more reliable than the ones decided through the 
trial-and-error validation process. Furthermore, no partitional clustering algorithm is employed in such a 
parameterization process, so that it can be considered as a common preprocess suitable for all partitional 
clustering algorithms. Through our demonstrations, the effectiveness of the COSCT method is also superior to 
the Principle Component Analysis (PCA) and Multi-dimensional Scaling (MDS) methods.  
The reliability of knowledge discovery can be affected by data oriented, knowledge oriented, and algorithm 
oriented factors [15]. By observing the cluster-displaying result in the star-coordinate plane, two data oriented 
factors for clustering reliability, including the number of clusters and initial cluster centers, is reliably conducted 
in this research. In the future, we will study how to retrieve more useful information from the cluster-displaying 
result in the star-coordinate plane for evaluating objectively whether a partitional clustering algorithm is 
appropriate or not. It will be useful for users to manage the algorithm oriented factors for achieving reliable 
clustering results.  
 
 
六、參考文獻 
 
1. Han, J., Kamber, M.: Data Mining: Concepts and Techniques. Morgan Kaufmann, San Francisco (2001)  
2. MacQueen, J.: Some Methods for Classification and Analysis of Multivariate Observations. In: the 5th Berkeley 
Symposium (1967) 281–297  
3. Bezdek, J.C.: Pattern Recognition with Fuzzy Objective Function Algorithms. Plenum Press, New York (1981)  
4. Krishnapuram, R., Keller, J.M.: A Possibilistic Approach to Clustering. IEEE Transactions on Fuzzy Systems 1 (1993) 
98–110  
5. Feng, Y., Wu, Z.: Enhancing Reliability Throughout Knowledge Discovery Process. In: the 6th IEEE International 
Conference on Data Mining Workshops (2006) 754–758  
6. Pal, N.R., Bezdek, J.C.: On Cluster Validity for Fuzzy C-Means Model. IEEE Transactions on Fuzzy Systems 3 (1995) 
370–379  
7. Jain, A.K., Dubes, R.C.: Algorithms for Clustering Data. Prentice-Hall, Englewood Cliffs, New Jersey (1988)  
8. Kandogan, E.: Visualizing Multi-dimensional Clusters, Trends, and Outliers Using Star Coordinates. In: the 7th ACM 
SIGKDD International Conference on Knowledge Discovery and Data Mining (2001) 107–116  
9. Kennedy, J., Eberhart, R.C.: Particle Swarm Optimization. In: the 1995 IEEE International Conference on Neural 
Networks (1995) 1942–1948  
10. Dempster, A.P., Laird, N.M., Rubin, D.B.: Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal 
of the Royal Statistical Society Series B 39 (1977) 1–38  
11. Newman, D.J., Hettich, S., Blake, C.L., Merz, C.J.: UCI Repository of Machine Learning Databases. 
