successive scans to incrementally build a complete 3D 
color model. The result showed that the proposed 
framework can obtain an accurate model with both 
color and 3D information. 
英文關鍵詞： 3D modeling, corridor, registration, laser range 
finder, omnidirectional camera, data fussion 
 
II. RELATED WORK 
In the previous section, the three crucial problems of 
this paper were already listed. To solve the first problem, 
an omnidirectional camera calibration must be done. In 
the second issue, data fusion of LRF and omnidirectional 
camera requires an extrinsic calibration process to embed 
the range information in omnidirectional images. The 
problem of 3D reconstruction from successive scans 
could be done by using matching and registration method. 
In this section, previous work which relates to these three 
problems is presented. 
A. Omnidirectional camera calibration 
The work on omnidirectional camera calibration may 
be categorized into two main groups. The first one 
includes the method from self-calibration processes 
which are performed from point correspondences and 
epipolar constraint through minimizing an objective 
function. The second one covers techniques that exploit 
the presence of calibration pattern. The methods 
described in [7, 11] are classified as the first group. The 
calibration process is done by taking advantage of 
epipolar geometry without using calibration pattern. 
However, these self-calibration techniques just focus on 
particular sensor types, such as parabolic and hyperbolic 
mirror. In contrast with approaches mentioned so far, the 
authors in [10, 13] present methods to calibrate 
omnidirectional camera by using checkerboard-like 
calibration pattern. In addition, these techniques may be 
applied for different types of omnidirectional camera. D. 
Scaramuzza in [18] also developed a toolbox for 
calibrating omnidirectional camera. This toolbox is 
flexible and easy to use since it just asks users to capture 
calibration pattern images at few positions and 
orientations. 
B. Laser-Camera Calibration 
While most previous works on laser and camera 
extrinsic calibration are deal with perspective camera 
and 2D laser range finder (see [15]), little research has 
been published on using omnidirectional camera. 
Moreover, some methods uses visible laser beams such 
as in [9]. Conversely, the authors [12] propose a method 
for fast extrinsic calibration between camera and 3D 
laser scanner in case of invisible laser. A Matlab toobox 
is also provided to implement this method. The toolbox 
requires user collect laser and image data of calibration 
pattern at several poses. After that, the region of points in 
image, which is on the pattern, is selected by user. The 
calibration is finally done by minimizing the difference 
in orientation and distances of the pattern planes 
observed by two sensors. Bacca in [1] adapt the method 
propose by Zhang and Pless [15] to perform an extrinsic 
calibration between a 2D laser range finder and 
omnidrectional camera. The proposed method then is 
evaluated at pixel error by using ground truth data from 
the calibration patterns projected on image. 
C. 3D model reconstruction   
To reconstruct a 3D environment with color infor-
mation, P. Biber in [2] used a sensor system consists of 
an omnidirectional camera and a 2D laser range finder. 
From the laser scan, an accurate 2D map is built by scan 
matching based on normal distribution transform. Im-
ages are captured from panoramic camera used to gen-
erate texture later. The input images are warped onto the 
planes assigned to walls and ﬂoor according to the laser 
scan data. As a result, the 3D environment consists of 
textured wall and floor planes. Later, P. Biber also ex-
tended this work by using graph cut algorithm [3]. His 
new model is formed not only by textured walls and 
floors but also 3D point clouds, thus structures like 
windows and stair are visualized as well. 
In [17, 5] and [14], the authors have used a CCD 
camera mounted on the top of a 2D laser range finder. 
The key ideal here is to combine the strong visual cor-
respondences with the depth accuracy obtained from the 
laser scanner. In [17, 14], image correspondences were 
extracted by using Scale Invariant Feature Transform 
(SIFT) [8] algorithm. However, the authors in [17] 
treated these SIFT features as a corresponding set for a 
method called trimmed Generalized Total Least Squares 
Iterative closest point (GTLS-ICP) to infer a relative 
motion. Therefore, their ICP algorithm did not require 
establishing the correspondences. In contrast, the author 
in [14] used directly these found features to calculate a 
rough relative transformation. After that, this computed 
relative transformation is used as an initial guest for a 
modified color ICP algorithm. Y. Bo in [5] proposes 
another method to search for correspondences. The laser 
data is projected to the current image to find the corre-
sponding points in the next image by using image 
matching method called Kanade-Lucas-Tomasi (KLT) 
tracker. Motion between frames is inferred from these 
correspondences. All these above methods result in a 
high precision color 3D model. 
Johnson and Kang in [7] discuss a methodology of  
integrating multiple textured 3D data sets. This method 
requires the 3D data sets be firstly recovered from om-
nidirectional multi-baseline stereo. The two consecutive 
data sets are registered by using an ICP variant which 
they called as color ICP. This ICP version takes into 
account not only geometric information but also the 
 
Fig. 2.   An Overview of proposed framework 
 
camera and the calibration plane. This constraint can be 
described by the following equation. 
 
                .cos.
2
CCCCC NPNPN          (3) 
Substitute PL for PC by using (1), it leads to the fol-
lowing  non-linear cost function where R and t are the 
parameter need to be estimated, and PL is a laser point. 
 
          .).(),(
2
CLC NtRPNtRf        (4)  
        
To achieve the parameters R and t, data must be 
captured at several positions; thus (4) becomes: 
    .))((),(
2
1 1

 

K
i
M
j
ciLij
ci
ci NtRP
N
N
tRf    (5)   
      
Where K is the total number of calibration poses, M  is 
the number of laser point in the i-th target pattern and NCi 
is the vector defined from (2) for the i-th calibration 
plane. The solution to the above non-linear cost function 
may be obtained by using Levernberg-Marquardt (LM) 
optimization algorithm which is offered from Matlab. 
The result of calibration process is shown in Fig.5. 
Laser points are projected on image as the green dots to 
validate the result. The laser points on the checkerboard 
are better fit to the pattern’s edge after calibration. 
V. DATA FUSION 
A laser range finder provides a set of distances to 
objects of surrounding environment. The relative posi-
tion (from bird eye view) of these objects to scan pose 
could be easily inferred from these distances. Since, 
these points lie on a plane that is parallel to ground plane, 
thus their third coordinate that describe the height could 
be assigned to be equal to the difference between laser 
pose and the ground. Moreover, our third assumption 
states that all wall surfaces are supposed to be flat; thus 
the 3D coordinates of the other points on vertical surface 
are easily obtained from laser scanner results.  
On the other hand, our goal is to build a 3D color 
model. Hence, the color information, which corresponds 
to each specific 3D point, could be extracted by using the 
mapping function from calibration process. As a result, a 
set of 3D color points is obtained by information from 
both of laser range finder and camera model.  
This process could be described as follow. First, the 
laser scan points are transformed into Cartesian system. 
Second, the 3D point set is generated by assigning the 
third z-coordinate. The last step is that mapping all the 
3D points into image plane to retrieve their correspond-
ing color values.(see the Fig. 6) 
VI. MODEL GENERATION 
This section describes the process of generating a 
complete 3D color model from pairs of successive scans. 
J. Computing 2D relative motion 
This part deals with issue:  how to compute the 2D 
relative motion between these two scan poses. The 
corresponding features are firstly obtained by using SIFT 
algorithm. The depth of matched features are estimated 
from laser scanner. The relative transformation between 
two poses is computed by using RANSAC-based method. 
Lastly, the calculated result is refined with ICP. 
1) Identifying image correspondences and estimating 
depth value of matched features 
The corresponding features are obtained through 
SIFT. Their depth values are estimated from the range 
measurement as the following example (see Fig. 7). As 
the result of calibration, laser points could be reprojected 
on omnidirectional image. Let P’ is one of the feature lie 
on the region of laser range finder described as 
arch-shaped. According to the assumption: the wall is 
flat, the depth of P’ is the same as the point Pi (one of the 
reprojected laser points) which is a point on the extended 
ray OP’. Therefore, the depth value of all the matched 
SIFT features can be obtained through this ideal. 
2) Relative motion estimation 
Our goal is to obtain the displacement between two 
successive scans: (Δx, Δy, Δθ) which are the change in 
two axes (x, y) and heading orientation. It is achieved by 
using a RANSAC-based method proposed at [4]. The 
input of this process is the matched features from above 
step. 
3) Iterative Closest Point  
The estimation of relative motion from the previous 
process is used as an initial guess for ICP (Iterative 
Closest Point) algorithm.  
In Fig. 8, the matching process is performed on two 
consecutive laser scans by using RANSAC-based tech-
   
(a)                                         (b) 
Fig. 5.  Calibration result (a) Before Calibration (b) After Calibration 
 
 
Fig. 6.   Generation 3D color point at one pose. 
 
dinate to incrementally build the complete model. 
VII. RESULTS 
The experiment was performed with the corridor 
environment of 60m x 100m using the proposed 
framework. The result of reconstruction is shown in Fig. 
9(a). The red dots represent the positions where scans 
were captured. Another view of this reconstructed model 
is visualized in Fig. 10. 
To evaluate the performance of the proposed 
framework, the experiment is conducted under the closed 
loop environments. Robot was set to move in loop path, 
and the change in distance between first and the last po-
sition could be considered as the error.  The result is 
shown in Fig. 11 and table I with different sizes. 
VIII. CONCLUSION 
In this paper, we have proposed the scheme for 
building a 3D model of corridor environment. By em-
bedding the range measurement of laser range finder into 
wide-view omnidirectional images, a large scale struc-
tured environment could be reconstructed with not just 
3D geometric information but also color features. It is 
well known that ICP requires a good initial guess to ob-
tain the good matching result. We proposed a method to 
obtain these initial parameters by combining visual cor-
respondences and the range measurement from laser 
scanner. The computed 2D relative transformations be-
tween each two scan poses are converted into 3D coor-
dinate and again, are put as initial guest for pair-wise 
registration color constraint-based method to incremen-
tally reconstruct a complete 3D model. The performance 
of proposed framework is examined under the loop 
closure case. The results show that the reconstructed 
model is good for visualization goal.  
References 
[1] E. B. Bacca, E. Mouaddib and X. Cufi, “Embedding range 
information in omnidirectional images through laser range 
finder,” IEEE/RSJ International Conference on Intelligent Robot 
and system, pp.2053-2058, 2010. 
[2] P. Biber, H. Andreasson, T. Duckett and A. Schilling, “3D 
Modeling of Indoor Environments by a Mobile Robot with a 
Laser Scanner and Panoramic Camera,” Proc. of IEEE/RSJ 
International Conference on Intelligent Robot and system,  vol. 4, 
pp. 3430, 2004. 
[3] P. Biber, S. Fleck and T. Duckett, ”3D Modeling of Indoor 
Environments for a Robotic security guard,” Proc. of the  IEEE 
Computer Society Conference on Computer Vision and Pattern 
Recognition, pp. 124, 2005. 
[4] J. L. Blanco, J. Gonzalez and J. A. F. Madrigal, ”A new method 
for robust and efficient occupancy grid-map matching,” In Proc of 
the 3rd Iberian conference on Pattern Recognition and Image 
Analysis, Part II, Springer-Verlag Berlin, Heidelberg © 2007.  
[5] Y. Bo, Y. B. Hwang, and I. S. Kweon, “Accurate Motion 
Estiamation and High-precision 3D reconstruction by Sensor 
fusion,” IEEE International Conference on Robotics and 
Automation, pp. 4721 – 4726, 2007. 
[6] A. Johnson and S. B. Kang, “Registration and integration of 
textured 3-D data,” In Proc. Int. Conf. On Recent Advances in 3-D 
Digital Imaging and Modeling, pp 234-241, May 1997. 
[7] S. B. Kang, “Catadioptric self-calibration,” IEEE Conference on 
Computer Vision and Pattern Recogniton, pp. 201-207, 2000. 
[8] D. G. Lowe, "Distinctive Image Features from Scale-Invariant 
Keypoints," International Journal of Computer Vision, vol. 60, 
pp. 91-110, 2004. 
[9] C. Mei and P. Rives, ”Calibration between a Central Catadioptric 
Camera and a Laser Range Finder for robotic applications,” In 
Prof. of IEEE International Conference on Robotics and 
Automation, pp. 532-537, 2006. 
[10] C. Mei and P. Rives, ”Single View Point Omnidirectional Camera 
Calibration from Planar Grids,” IEEE International Conference 
on Robotics and Automation, pp. 3945 - 3950, 2007. 
[11] B. Micusik and T. Pajdla, “Para-catadioptric Camera 
Autocalibration from Epipolar Geometry,” Asian Conference on 
Computer Vision, pp. 748-753, January 2004. 
[12] R. Unnikrishnan and M. Herbert, “Fast Extrinsic Calibration of a 
laser range finder to a Camera,” Tech. Report CMU-RI-TR-05-09, 
Robotics Institute, Carnegie Mellon University, July 2005. 
[13] D. Scaramuzza, A. Martinelli and R. Siegwart, “A Toolbox for 
Easily Calibrating Omnidirectional Cameras,” IEEE/RSJ 
International Conference on Intelligent Robot and system, pp. 
5695 – 5701, Beijing China, October 7-15, 2006. 
[14] J. H. Young, K. H. An, J. W. Kang, M. J. Chung and W. Yu, “3D 
Environment Reconstruction using modified color ICP algorithm 
by Fusion of a camera and a 3D Laser Range Finder,” IEEE/RSJ 
International Conference on Intelligent Robot and system, pp. 
3082 – 3088, 2009. 
[15] Q. Zhang and R. Pless, “Extrinsic Calibration of a camera and 
Laser Range Finder,” In Proc. of IEEE/RSJ International 
Conference on Intelligent Robots and System, vol. 3, pp. 
2301-2306, 2004. 
[16] Z. Zhang, “Iterative Point Matching for Registration of Free-Form 
Curves and Surfaces,”  International Journal of Computer Vision, 
vol. 13:2, pp. 119-152, 1994. 
[17] H. Andreasson and A. J. Lilienthal, “6D Scan Registration using 
Depth-Interpolated Local Image Features,” Robotics and 
Autonomous Systems, vol. 58, no. 2, 2010. 
[18] An omnidirectional Camera toolbox’s tutorial, D. Scaramuzza 
http://robotics.ethz.ch/~scaramuzza/Davide_Scaramuzza_files/R
esearch/OcamCalib_Tutorial.htm#omnimodelpartial. 
 
 
Fig. 11.  The reconstructed model under loop closure  
TABLE I 
LOOP CLOSURE IN DIFFERENT ENVIRONMENT SIZES 
Size(m2) X(m) Y(m) Z(m) Number of 
Scans 
8.6 x 6.5 0.0045 0.0067 -0.0009 39 
11.3 x 6.3 -0.049 0.0064 -0.0029 52 
17.5 x 7.2 -0.918 0.57 -0.002 67 
 
眾並詢問我研究中當作人出勤依據的熱點的判定與後序進入與離席的偵測。利用影像處理的技術
支援控制的系統設計在這個 session 中似乎很引人注意。 
 
二、與會心得 
本次大會由大連理工主辦。大部份的時間，議程是以三到四組同時進行。大部份的作者都是中國人，
一般英文報告水準欠佳。會場中，有聽到與國防、產業相關的控制系統報告。大陸控制學者之多以
及研究題目的廣度讓我印象深刻。大會每天晚上都有晚宴，國內外的與會學者，齊聚一堂，交流學
術思想，討論學術問題。此次參加會議過程中，個人覺得最大的收獲是與國內外學者進行研究想法
上的交流。報告現場聽眾的提問，可以讓我知道研究同樣學者關心的技術瓶頸，而他們的建議也擴
展我後續的研究目標。透過與會人士報告自己最新的研究成果，並積極參與、交換研究心得，是我
覺得最大的收獲。 
 
 
 
2
4
During the past decades, intelligent building has been 
developed. By using the cameras, the researchers extracted the 
information from the environment into a map. In [4], the map is 
labeled already. In [7], [8], [9], [10], and [11], the map is 
learned automatically to indicate interesting image regions and 
the way objects move between these places. In the case of the 
camera that was used, a static camera is used by the system as 
in [3], [4], [9], [10]. In [11], they applied omnidirectional 
camera to their system. The others, [2], [7], [8], used stereo 
camera to reduce the effect of lighting intensity and occlusion. 
By integrating the existing open source library with 
additional algorithms, we have designed an intelligent 
attendance logging system which works in two phases. 
Corresponding to the objectives, there are three contributions in 
this paper: 
1) Learning mechanism that locates seats in an unknown 
environment. 
2) Monitoring mechanism that detects entering and leaving 
events of occupants. 
3) Integrating system with real-time performance up to 16 
fps, ready for context-aware applications.  
This paper is organized with the following sections. Section 
II describes the problem definition and preliminary. Section III 
explains about the proposed approach. Section IV discusses 
about the experiment results. Finally, the concluding remark 
and future work are given in Section V. 
II. PROBLEM DEFINITION AND PRELIMINARY 
We made an observation on the sample image taken from 
the camera in the working environment. The working 
environment that we used in this paper is our laboratory. There 
are some desks in a line. To have a better view and considering 
the limitation of our camera’s view, we set the camera 
perpendicular to the desks. We recorded some image sequences 
as a video clips. We would like to know what we can learn 
from the one sample image taken by a camera in the real 
condition. Fig. 2 shows the snapshot of the image samples. The 
observations on these samples are listed as the following: 
1) The path of the walking occupants in the scene is 
horizontal.  
2) A person may walk behind the sitting person. 
3) When a person sits, he/she is occluded by the chair. 
4) The part of the person has similar color with the 
surrounding furniture. 
5) An occlusion of two people happened sometimes. 
6) There are seats in the both edge. This could be a 
problem if the occupant suddenly moves outside of the 
scene. 
The system will be designed by considering the result from 
the observation. For instance, the horizontal path of the 
walking occupants can make the grouping algorithm of broken 
blobs easier. Also, we used four attribute features for tracking 
which are centroid, size, ground position, and color. We 
defined centroid as a center of the occupant’s bounding box. 
Size feature is pixel density of one detected occupant. Ground 
position is the position of the occupant’s foot which is 
calculated at the bottom-centered of the occupant’s bounding 
box. Color is the RGB histogram from the occupant. 
 
 
A. Tracking states 
During the appearance of the object in the scene, we would 
like to keep tracking the object which is an occupant. Since the 
system can track a person, we just follow the person using the 
four attribute features. The system has four tracking states that 
have responsibility to track each person. The tracking states are 
entering state (ES), sitting state (SS), standing-up state (US), 
and leaving state (LS). The tracking states will be explained in 
section 3. Fig. 3 shows the diagram of the system. A person 
enters to the working room. The person sits into his/her seat. 
He/her stands up from the seat. Finally, the person leaves the 
working room. There will be two phases in this system, 
learning phase and monitoring phase. Both phases are triggered 
by sitting event and standing-up event. During the learning 
phase, sitting event is used to locate the seating area. In the 
monitoring phase, sitting event is used to give a time stamp of 
the sitting time and standing-up event is used to give time 
stamp of the leaving time. 
B. Approaches and Performance Criteria 
This intelligent attendance logging system is designed 
based on two assumptions. The first assumption is the 
environment is unknown, in that, the number of seats and the 
locations of these seats are not known before the system 
monitors. The second assumption is each occupant has his/her 
own seat, as such, detecting the presence/absence of a 
particular seat amounts to answering the presence/absence of 
that corresponding occupant. In order to achieve the research 
 
Figure 3.The diagram of the system. 
  
 
  
Figure 2.  Snapshot of some image samples. 
6
three cases in which there is no movement from the 
occupant for a defined time. First, the x-axis 
displacement is zero more than 20 frames. Second, the 
optical flow result is zero more than 100 frames. Third, 
the track appears in the scene more than 100 frames. As 
a note, the camera’s image is chosen to be 340x240 
pixels. Using this image size, the experiment shows that 
the occupant’s movements such as breathing or typing 
on the keyboard do not change the x-axis of the 
bounding box. 
(3) Standing-up state (US), detects when the sitting 
occupant starts to move to leave his/her desk. In the 
experiments, a standing up occupant is detected when 
the sitting occupant produces movements, the height 
increases above 75%, and the size changes to 80%-
140% compared to the size of the current bounding box.  
(4) Leaving state (LS), deletes the occupant from the list. A 
leaving occupant is detected when occupant moves to 
the edge of the scene and occupant’s track loses its blob 
for 5 frames. 
Among these four states, the tracking features are used in 
different way, meaning that not all of the features are used for 
each state. There are two groups of states that used the tracking 
features in the same way to match the track from frame to 
frame. The first group is the sitting state (SS) and standing-up 
state (US). They use all of the tracking features. The track’s 
position will be updated with the blob’s position in the next 
frame when meets these requirements: the distance of their 
centroids is below the threshold TI, their ground positions are 
lying in the range of the predefined ellipse boundary, their 
color matching score is above 0.8, and their size ration is in the 
range of  ± 20%. The second group is entering state (ES) and 
leaving state (LS). They only use centroid feature with the 
same handling as the first group. 
B. Learning phase 
It is easier when we had the location of the occupants’ 
sitting areas previously. This system intends to build the map 
which consists of a set of occupant’s sitting areas automatically. 
It is called a learning phase which has responsibility to locate 
the seats. The sitting state (SS) is used to build the sitting area. 
Once the sitting event is triggered, the system starts to count. If 
the occupant stays more than 200 frames then the current 
location of the occupant’s bounding box is declared as a sitting 
area. The system will remember that sitting area and put it into 
the map as shown in Fig. 1. 
The learning phase is running continuously for predefined 
duration such that the map is expected to be finished 
completely. After the learning phase is terminated, the system 
switches to the monitoring phase.   
C. Monitoring phase 
The monitoring phase takes place after the learning phase is 
finished to construct the map. This phase has responsibility to 
monitor the occupant’s presence. The monitoring phase utilizes 
the sitting state (SS) and the standing-up state (US). Sitting 
state (SS) sends an event to tell that the occupant is detected as 
sitting. After sitting state is triggered and if rule (2) is satisfied 
then the system generates a sitting time report. 
 
 
    {
                            
                                         
                               
 
When standing up state (US) is triggered to inform that the 
occupant is leaving his/her seat, the system generates a leaving 
time report. Fig. 6 shows an example of generated report. The 
monitoring phase runs forever or until the user terminates the 
program to report the occupant’s attendance. Next time when 
the user restarts the program, the system tries to find the map. 
If the current map exists then the system runs the monitoring 
phase directly. On the other hand, if the current map does not 
exist then the system runs the learning phase. It is also 
allowable for the user to restart the system in the learning phase, 
for instance, when the scene is changed. 
D. Differentiation of occluded individuals 
A challenging situation may happen when two occupants or 
more are in the scene. Their objects can make collision each 
other. We called this situation as merge and split. Merge and 
split can be detected by using proximity matrix as in [22]. In 
the merge condition, only centroid feature is used to match the 
track position to the next possible position since the other three 
features are not useful when the objects merge. After a group of 
occupants split, the color feature will be used to match who is 
who. This can be done since during tracking, the previous color 
information of the occupants is saved continuously. For this 
purpose, during the objects merge, their color information is 
not saved to maintain the color information before they merge.  
In the experiments, when more than two occupants merge 
after that they split, sometimes an occupant remains occluded. 
Later, the occluded occupant splits. Fig. 7 shows the occluded 
problem. In the left image, three occupants merged then split 
become two, the remaining occupant is still occluded. The 
system detects this event then marks the splited object. The 
system keeps watching on them until one of the marked objects 
splits, shown in the right image. When the hiding occupant 
splits, the system will re-identify each occupant and correct 
their previous ID number just before they have merged. 
  
Figure 7. Occluded occupant problem. 
 
Sitting area number Event Time stamp 
1 Sitting 09:02:09 Wed 2 June 2010 
2 Sitting 09:07:54 Wed 2 June 2010 
3 Sitting 09:12:16 Wed 2 June 2010 
2 Leaving 10:46:38 Wed 2 June 2010 
2 Sitting 10:49:54 Wed 2 June 2010 
3 Leaving 12:46:38 Wed 2 June 2010 
Figure 6. A report example. 
8
color of the background image. This caused the occupant 
produced small blob. The system cannot track the occupant 
because his/her blob’ size becomes too small. 
Table 2 shows the test result of scene type 2. The system 
monitored the occupants based on the map that has been found. 
The experiments were done 10 times without occlusion. There 
are some errors that the system failed to recognize the sitting 
occupant. The system failed to detect the occupant because of 
the same problems in the previous discussion; the system lost 
to track the occupant because the occupant has the similar color 
to the background image so that the occupant suddenly has 
small blob. The system also failed to recognize the leaving 
event from desk number 1. The system detects a leaving 
occupant when the occupant split with his/her seat. Since the 
desk number 1 is near the edge, the system can not detect the 
standing-up event. The system still detected that the desk 
number 1 is always occupied even the corresponding occupant 
has left that location.  
Table 3 shows the test result of scene type 2. The 
experiments were done 10 times with occlusion. The system 
should be able to keep tracking the occupants. To test the 
system, three occupants enter to the scene to make the scenario 
as shown in Table 3. Some occupants walk through behind the 
sitting occupant or the occupants just walk and cross each other. 
Most of the cases, the system can detect which is which after 
they split. The error happened because of the occupant’s color 
and the sitting occupant. If the occupants have a similar color 
then the system may get confuse to differentiate them. Another 
time, when the sitting occupant makes a movement, it creates a 
blob. However, the system still does not have enough evidence 
to determine that this blob will change the status of sitting 
occupant become standing-up occupant. Another occupant 
walked closer and merged with this blob. After they split, the 
system confused since the blob had no previous information. 
As the result, the system missed count the previous track being 
merged. The ID number of occupant is restored incorrectly 
V. CONCLUSION 
We have already designed an intelligent attendance logging 
system by integrating the open source with additional algorithm. 
The system works in two phases; learning phase and 
monitoring phase. The system can achieve real-time 
performance up to 16 fps. We also demonstrate that the system 
can handle the occlusion up to three occupants considering that 
the scene seems become too crowded for more than three 
occupants. While the regular time recorder only reports the 
time stamp of the beginning and the ending of the occupant’s 
working hour, this system provides more detail about the 
timing information. Some unexpected behavior may cause an 
error. For instance, the occupant has the color similar to the 
background, the desk position, or the occupant moves while 
sitting.  
In the future, the events generated by this system can be 
used to deliver a message to another system. It is possible to 
control the environment automatically such as adjust the 
lighting, playing a relaxation music, setting the air conditioner 
when an occupant enters or leaves the room. The summary 
report of the occupant’s attendance also can be used for activity 
analysis. The current system does not include the recognition 
capability since it only detect whether the working desk is 
occupied or not. However, if occupant recognition is needed 
then there are two ways. After the map of sitting areas are 
found, user may label each sitting area manually or a 
recognition system can be added. Having a larger view or 
changing the camera orientation also become our consideration 
for the future. Fisheye camera or omnidirectional camera are 
able to capture wider angle in a small room like in the office or 
laboratory. 
VI. ACKNOWLEDGEMENT 
This  research was supported in part by NSC 99-2221-E-
011 -134 and NSC 100-2221-E-011 -051. 
REFERENCES 
[1] Wikipedia, “Time clock,” http://en.wikipedia.org/ wiki/ Time_clock 
(June 24, 2010). 
[2] B. Brumitt, B. Meyers, J. Krumm, A. Kern, and S. Shafers, “EasyLiving 
technologies for intelligent environments,” Lecture Notes in Computer 
Science, Volume 1927/2000, pp. 97-119, 2000. 
[3] S. -L. Chung and W. –Y. Chen, “MyHome: A residential server for 
smart homes,” Lecture Notes in Computer Science, Volume 4693/2010, 
pp. 664-670, 2007. 
[4] Z. Zhou, X. Chen, Y. –C. Chung, Z. He, T. X. Han, and J. M. Keller, 
“Activity analysis, summarization, and visualization for indoor human 
activity monitoring,” IEEE Transactions on Circuits and Systems for 
Video Technology, Volume 18, Issue 11, pp. 1489-1498, 2008. 
[5] OpenCV. Available: http://sourceforge.net/projects/ opencvlibrary/  
[6] cvBlob. Available : http://code.google.com/p/cvblob/  
[7] D. Demirdjian, K. Tollmar, K. Koile, N. Checka, and T. Darrell, 
“Activity maps for location-aware computing,” Proceedings of the Sixth 
IEEE Workshop on Applications of Computer Vision, pp. 70-75, 2002. 
[8] K. Koile, K. Tollmar, D. Demirdjian, H. Shrobe, and T. Darrell, 
“Activity zones for context-aware computing,” Lecture Notes in 
Computer Science, Volume 2864/2003, pp. 90-106, 2003. 
[9] N. Brandie, D. Bauer, and S. Seer, “Track-based finding of stopping 
pedestrians - A practical approach for analyzing a public infrastructure,” 
Proceeding of IEEE Conference on Intelligent Transportation Systems, 
pp. 115-120, 2006. 
[10] A. Girgensohn, F. Shipman, and L. Wilcox, “Determining activity 
patterns in retail spaces through video analysis,” Proceeding of the 16th 
ACM International Conference on Multimedia, pp. 889-892, 2008. 
[11] B. Morris and M. Trivedi, “An adaptive scene description for activity 
analysis in surveillance video,” Proceeding of IEEE International 
Conference on Pattern Recognition, 2008. 
 
TABLE III. TEST RESULTS OF SCENE TYPE 2. THE NUMBER OF OCCUPANT 
MISTAKENLY ASSIGNED IN MERGE-SPLIT CASE FOR 10 TIMES MERGED. 
Total 
occupant 
Sitting 
occupant 
Walking 
occupant 
Merge 
Split 
Succeeded Failed 
2 0 2 10 9 1 
2 1 1 10 8 2 
3 0 3 10 9 1 
3 1 2 10 9 1 
3 2 1 10 9 1 
 
10
100年度專題研究計畫研究成果彙整表 
計畫主持人：鍾聖倫 計畫編號：100-2221-E-011-051- 
計畫名稱：利用距離估測以及紋理地圖改良以場景地標影像為依據的自走式機器人之視覺導航技術 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 2 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100% 期刊論文正在整理中，將投稿。 
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 ■洽談中 □無 
其他：（以 100字為限） 
本研究成果精簡內容已發表於： 
Hung-Duy Nguyen, Sheng-Luen Chung, ＇3D Model Reconstruction of Corridor 
Environment by Data fusion of Laser range finder and Omnidirectional Camera,＇ in 
Proceedings of the 43rd Intl. Symposium on Robotics (ISR2012), Taipei, Taiwan, Aug. 
9-31, 2012。 
目前另外著手較完整期刊論文的準備。 
 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500字為限） 
精密的環境三維地圖模型在機器人同步建圖與定位的問題中非常重要：自主式機器人有了
精確的地圖才能掌握精確的定位，以達到自主行走的精度。傳統行走式機器人單獨使用電
射掃描儀 (laser range finder) 的缺點是在像走道 (corridor) 的場景中，牆面直線的
距離資訊，容易造成定位的誤差，據此，本研究方法是在架設電射掃描儀的機器人載具上，
另外使用全向攝影機 (omnidirectional camera) 擷取場景的影像資訊以建置較精確的立
體紋理地圖。本計劃的技術創新利用：(1) 經由校正程序取得所採用兩個感應器座標間的
相對關係，以及 (2) 將全向鏡的球面影像矯正成平面影像的關係。如此，機器人在不定
間隔的停駐點所收集到其與垂直牆面的距離，以及由全向鏡矯正後平面的影像，可建構出
周遭走道環境中局部的立體紋理塑模。再利用 RANSAC (random sample consensus) 以及
ICP (Iterative Closest Point) 演算法，我們可由相臨兩停駐點所得到的影像特徵點比
對，推估出兩停駐點間較精確的相對位置與面向，作為由局部的三維塑模拼貼出較全面性
區域的模型的依據。利用室內牆面垂直的特性，以及整合不同感測器在資訊匯整時所需考
慮的校正技術，本計劃採用較低成本的兩種感測器，可建置出只用其中一種感測器所無法
