可供推廣之研發成果資料表 
□ 可申請專利  □ 可技術移轉                                      日期：95 年 10 月 31 日 
國科會補助計畫 
計畫名稱：一個使用延續同倫法來進行新的調控費雪區別分析及其
在偵測有失真特徵上的應用 
計畫主持人：張欽圳         
計畫編號：NSC 94-2213-E-019-011    學門領域：資訊 II 
技術/創作名稱  
發明人/創作人 張欽圳 
技術說明 
中文：線性特徵抽取通常是有效率的分類器的第一步驟。本報告主
要研究如何強健地產生有效率的線性特徵抽取器。我們發現離異類
別(outlier class)與過度符合問題(over fitting problem)為造
成一般演算法產生沒效率的線性特徵抽取器的主要因素。一個特徵
抽取演算法若對離異類別不夠強健，則會過度強調或完全忽略離異
類別，因此可能產生無效率的線性特徵抽取器。在這份報告裡，我
們提出了一個結合配對與總體區分分析來強健地產生有效率的線
性特徵抽取器的新方法。過度符合問題通常發生於少樣本的應用。
為了避免過度符合問題，現行方式皆會偏好數值比較大的特徵成
分，因此會造成真正有用的特徵成分遭到某種程度的忽視。在這份
報告裡我們提出一個對特徵成分正規化的方式，藉由這種方式有效
的特徵成分無論其數值大小皆可被充分利用 
附件二 
應用於線性特徵抽取之強健式區分分析 
張欽圳 
摘要 
線性特徵抽取通常是有效率的分類器的第一步驟。本報告主要研究如何強健地產生有效率
的線性特徵抽取器。我們發現離異類別(outlier class)與過度符合問題(over fitting problem)為造成
一般演算法產生沒效率的線性特徵抽取器的主要因素。一個特徵抽取演算法若對離異類別不夠強
健，則會過度強調或完全忽略離異類別，因此可能產生無效率的線性特徵抽取器。在這份報告裡，
我們提出了一個結合配對與總體區分分析來強健地產生有效率的線性特徵抽取器的新方法。過度
符合問題通常發生於少樣本的應用。為了避免過度符合問題，現行方式皆會偏好數值比較大的特
徵成分，因此會造成真正有用的特徵成分遭到某種程度的忽視。在這份報告裡我們提出一個對特
徵成分正規化的方式，藉由這種方式有效的特徵成分無論其數值大小皆可被充分利用。 
關鍵字：區分分析，線性特徵抽取器，費雪線性區分 
2I. INTRODUCTION
Extracting features effective in discrimination is an important step of a classifier. The extracted features have a more economic
size than the original one and may also simplify the complexity of the classifier [1]. The linear discriminant analysis (LDA)
is one of the approaches to extracting effective features for classification. The LDA aims at finding a linear transformation L
with linearly independent columns, which reduces the dimension of a feature vector x from n to a lower dimension l by the
transformation y = LTx, and meanwhile makes the transformed vector y preserve most of the discriminant information of
x. Fisher’s linear discriminant [2], [3] is the most popular approach; however, it has some shortcomings as follows. First, the
number of extracted features is bounded above by the number of classes minus one. Second, as illustrated in Fukunaga [1],
the Fisher criterion cannot be a good measure of the class separability when the class means are closed. Third, since Fisher’s
linear discriminant is based on mixed class information, the outlier class may be overemphasized or neglected. Besides, for
the undersampled problem in which the number of training samples is less than the dimension of the measurement vector, the
Fisher criterion is ill-posed. Hence, some techniques, such as the regulatization, should be adopted for LDA to cope with the
undersampled problem. In the following, some extensions of Fisher’s linear discriminant and the approaches of LDA for the
undersampled problem are surveyed.
A. Related researches about extension of Fisher’s linear discriminant
Researches about LDA without the mentioned drawbacks of Fisher’s linear discriminant have come out. Some of them are
as follows. Here, we focus on the approach based on the scatter matrices without complicated nonlinear optimization for the
sake of facilitating mathematical handling of the problem. The first kind of approaches, which is a global approach, is to
extend the Fisher criterion or develop a new criterion to cope with the drawback of Fisher’s linear discriminant. Loog et al.
[4] generalized Fisher’s linear discriminant by introducing a weighting function on the squared distance between the means of
every pair of classes. They indicated that this generalization of the Fisher criterion can suppress the influence of the outlier
class. To deal with heteroscedastic data, Loog and Duin [5] introduced a heteroscedastic extension of the Fisher criterion based
on the Chernoff distance. This extension of the Fisher criterion can capture the discriminatory information in the covariance
matrices. In addition, Qin et al. [6] introduced a weighting function to generalized Loog and Duins’ approach. Li et al. [7]
proposed a new criterion known as the maximum margin criterion. This criterion aims at maximizing the difference between the
between-class scatter and the within-class scatter in the reduced space. The main characteristic of the first kind of approaches
is that an analytic formula for the transformation matrix based on mixed class information is often available.
The second kind of approaches has two main steps in general. The first step derives discriminatory information by global
or pairwise discriminant analysis. The second step integrates the discriminatory information to form the transformation matrix.
Brunzell and Eriksson [8] proposed a separation measure proportional to the geometrical mean of the Mahalanobis distances
between all classes. This separation measure is preserved under the transformation matrix whose range covers the range of
the matrix U defined as U = [S¯−112 u12 S¯
−1
13 u13...S¯
−1
ij uij ...S¯
−1
c−1,cuc−1,c], where S¯ij = Si + Sj and uij = ui − uj with c
the number of classes, and ui and Si, respectively, the mean vector and covariance matrix of Class i. Let U = QSVT be
a singular value decomposition (SVD) of U where S is a diagonal matrix consisting of the singular values of U in non-
increasing order. The transformation matrix is then formed by the first l columns of Q. Tubbs et al. [9] applied the SVD
on the matrix T = QSVT where T is defined as T =[(u2 − u1) (u3 − u1)...(uc − u1) (S2 − S1) ...(Sc − S1)], and then
formed the transformation matrix by the first l columns of Q. Hsieh et al. [10] proposed a two-stage approach to construct
the transformation matrix. This two-stage approach is a cascade of the discriminant analysis feature extraction (DAFE), which
is Loog’s approach [4], and common-mean feature extraction (CMFE) [11]. First, the DAFE provides at most c − 1 vectors
for the c-class problem. Next, the CMFE finds the other vectors effective for discrimination from the subspace orthogonal to
the vectors provided by the DAFE. The vectors generated by the DAFE and the CMFE are, respectively, ordered according to
their discriminatory capabilities. Finally, by using the backward binary search to merge the vectors provided by the DAFE and
CMFE and their proposed method to estimate the classification error of the merged result, the desired transformation matrix
is the one with the minimum classification error.
The main advantages of the first kind of approaches is that the transformation matrix is found in one step and overall
effective to all classes. However, this kind of approaches may be less robust to outlier classes because it deals with mixed
class information. On the other hand, the influence of outlier classes would become less significant if the transformation matrix
is based on the integrated result of pairwise discriminant analysis. However, the pairwise discriminant analysis may produce
result not overall effective. Therefore, integrating the results of these two kinds of approaches seems a promising approach to
robustly producing an effective transformation matrix.
Brunzell and Eriksson [8], and Tubbs et al. [9] used the SVD, an unsupervised method, to integrate discriminatory information;
however, the SVD may produce ineffective results because the SVD is based on the linear dependence of the discriminatory
information in the measurement space without utilizing much class information, such as a priori probabilities of the classes. In
the literature, the techniques for the feature subset selection problem [12], [1], [13], [14], [15] can be applied to select a subset
of the discriminatory information for such an integration. The filter approach, that selects features by ranking them, and the
wrapper approach, which accesses subsets of features according to their effectiveness to a specified classifier, are two main types
4associated with the generalized eigenvalues of one, the largest generalized eigenvalue of the matrix pencil λ(Sb,St) [1], [23].
Hence, the columns of the desired linear transformation can be obtained by solving the generalized eigenvalue problem of the
matrix pencil λ(Sb,St). The QZ decomposition [22] and the homotopy continuation method [27], which are called the direct
methods in this study, can be applied to directly solve the generalized eigenvalue problem of the matrix pencil λ(Sb,St) even
though St is singular. However, the feature vector may contain noise components, which may come from the original source
of data or numerical errors of computations and should be excluded from the extracted features. As will be shown latter, for
the undersampled problem, the linear transformation derived by the direct method may overfit the training samples so that the
noise component may dominate the derived linear transformation.
On the other hand, some robust methods, which can both deal with the singularities of the scatter matrices and provide stable
results, have come out. In general, these robust methods have two main kinds of approaches. The first kind of approaches,
such as the PCA+LDA [26], the LDA/QR [28], the GA-Fisher [29], the pseudo LDA [30], [31], the LDA/GSVD [32], and the
Robust algorithm [33], first explicitly or implicitly finds an appropriate intermediate subspace and then applies the LDA in this
subspace. The LDA/QR finds the desired linear transformation in ran(Sb). The PCA+LDA finds the optimal transformation
in an intermediate subspace formed by the eigenvectors of Sw associated with the N − c largest eigenvalues. The GA-Fisher
performs the LDA in an intermediate subspace formed by an subset of the eigenvectors of Sw selected by a genetic algorithm.
The pseudo LDA forms the desired linear transformation by the eigenvectors of S+t Sb associated with the l largest eigenvalues,
where S+t denotes the pseudo inverse of St. As shown in [31] and [28], the LDA/GSVD and the LDA/QR are special cases of
the pseudo LDA. However, the LDA/GSVD uses the generalized singular value decomposition (SVD) to accomplish this task
in one stage. Once the threshold value for the singular value is chosen, the range of S+t is determined and the pseudo LDA
actually finds the columns of the desired transformation in ran(S+t ). Martı´nez and Zhu [33] proposed the Robust algorithm
which first applies a “normalized” PCA and then the LDA. Furthermore, they also proposed a measure for the stableness of
the derived linear transformation in the Bayesian sense.
Since the LDA on the undersampled problem through maximizing the Fisher criterion is ill-posed, the second kind of
approaches, such as the regularized LDA [34], [35], finds the desired linear transformation by maximizing a regularized Fisher
criterion. The regularized LDA adds regularization terms to St or Sw to make the scatter matrix nonsingular and is capable
of obtaining a stable result. The technique of cross validation is often used to find appropriate regularization parameters.
To improve the generalization error of the pseudo LDA, Skurichina and Duin [30] regularized the pseudo LDA by adding
redundant noise components to the feature vector.
C. Overview of the proposed approaches
1) A novel approach to robustly producing effective linear feature extractor: In this study, to robustly construct an effective
linear feature extractor, the proposed approach has two main steps. The first step produces discriminatory information by
the approach of Qin et al. [6], a global approach, and pairwise discriminant analysis based on the Bhattacharyya distance
[1], [3]. The second step integrates these discriminatory information to form the transformation matrix. In order to facilitate
the integration of the discriminatory information, a kernel function which measures linear dependence of the discriminatory
information in the measurement space and the consistency in the effectiveness of the discriminatory information to every pair
of classes is defined. Then, the QR factorization with column pivoting [22] is adopted to rank these discriminatory information
according to their importance for forming an independent subset of the discriminatory information in the kernel feature space
associated with the proposed kernel function. Finally, an efficient method based on the sequential forward selection (SFS) [12],
called the windowed SFS, is proposed to include the discriminatory information according to the ranked order to form the
transformation matrix.
2) Robust LDA on the undersampled problem: The mentioned robust methods except the regularizd LDA find the desired
transformation in an intermediate subspace obtained by some dimensionality reduction algorithm, such as the PCA, which
takes no discriminant information into consideration. Hence, some discriminative information may be abandoned but some
information useless for discrimination may be retained in the subspace [29]. As will be shown in this study, the retained
information useless for discrimination may have a significant negative influence on the linear transformation obtained through
maximizing the Fisher criterion. On the other hand, the regularized LDA prefers the transformation having a particular form
with respect to the regularization term. The proposed approach is similar to the regularized LDA in this aspect. In this study,
instead of maximizing the Fisher criterion, the desired linear transformation is found by following the proposed principle,
which states that if the values of the Fisher criterion with respect to two linear transformations are about the same, the one
with the smaller rate of projection onto null(Sb) should be preferred. The rationaliy of the proposed principle can be explained
by the following derived properties of the LDA through maximizing the Fisher criterion.
• First, a qualified extracted feature should have a large value of the Fisher criterion. However, for the undersampled problem,
this property is only a necessary but not sufficient property for a qualified extracted feature. This fact will be illustrated
by starting from a monotone property of the Fisher criterion with respect to the dimension of the feature vector. This
monotone property causes that the LDA through maximizing the Fisher criterion not to be resistant to noise components
when the undersampled problem occurs.
6where
pii =
p(ωi)
p(ωi) + p(ωj)
pij =
p(ωj)
p(ωi) + p(ωj)
Sw =
c∑
i=1
p(ωi)Si
S˜ij = S
− 12
w SijS
− 12
w
S˜i = S
− 12
w SiS
− 12
w
S˜j = S
− 12
w SjS
− 12
w
dij =
piipij
2
(ui − uj)TS−1ij (ui − uj) +
1
2
(log |Sij | − pii log |Si| − pij log |Sj |)
w(d) =
1
2d2
erf
(
d
2
√
2
)
with Sij = piiSi + pijSj , and lnH for an n× n positive definite matrix H is defined as lnH = Φdiag(lnλ1, ..., lnλn)ΦT
with H = Φdiag(λ1, ..., λn)ΦT an eigen-decomposition of H and diag(λ1, ..., λn) the diagonal matrix with the diagonal
entries λ1, ..., λn. The columns of L∗C can be the eigenvectors associated with the fC , which is a pre-specified number, largest
eigenvalues of the following n× n matrix:
c−1∑
i=1
c∑
j=i+1
p(ωi)p(ωj)w(dij)S−1w S
1
2
w(S˜
− 12
ij S
− 12
w (ui − uj)
× (ui − uj)T S−
1
2
w S˜
− 12
ij +
1
piipij
(
ln S˜ij − pii ln S˜i − pij ln S˜j
)
)S
1
2
w.
2) The definition of the β-component: The β-component of d =(α,β) is a c(c−1)2 -dimensional vector β = [β12 · · · βij
· · · βc−1,c]T , where βij is defined as
βij = p(ωi)p(ωj)(1− εi,j(α)) (2)
with p(ωi) and p(ωj), respectively, the a priori probabilities of Classes ωi and ωj , and εi,j(α) the Bayes error of the two-class
classification problem for Classes ωi and ωj with respect to the class distributions projected onto the vector α. βij is the rate
to correctly classify the patterns of Classes ωi and ωj . In addition,
∑c−1
i=1
∑c
j=i+1 βij is the average classification rate for all
pairs of classes. Representing the discriminatory capability in this manner is advantageous to find the correlation between the
discriminatory capabilities of two vectors.
Provided that the distributions of the c classes are multivariate normal distributions N(ui,Si), i = 1, ..., c, εi,j(α) can be
obtained by
εi,j(α) = E
(
p(ωi)
p(ωi)+p(ωj)
, N(αTui,αTSiα);
p(ωj)
p(ωi)+p(ωj)
, N(αTuj ,αTSjα)
)
,
where E (p1, N(u1, σ21);p2, N(u2, σ22)) denotes the Bayes error of two univariate normal densities N(u1, σ21) and N(u2, σ22)
with a priori probabilities p1 and p2. E
(
p1, N(u1, σ21);p2, N(u2, σ
2
2)
)
can be obtained by
E (p1, N(u1, σ21);p2, N(u2, σ22)) =
p2√
2piσ2
∫
g(x)≥0
exp
(−(x− u2)2
2σ22
)
dx+
p1√
2piσ1
∫
g(x)<0
exp
(−(x− u1)2
2σ21
)
dx
where
g(x) = 2 ln
p1
p2
+ ln
σ2
σ1
+
(x− u2)2
σ22
− (x− u1)
2
σ21
.
81) The definition of the kernel function: To facilitate the integration of the elements in D, a kernel function k for measuring
the correlation of the elements in D is defined as:
k(d,d
′
) =
(
αTα
′)× ∑
i=1...c−1,
j=i+1,...,c
min
{
βij , β
′
ij
}
,
where d and d′ denote the elements in D. The validity of the proposed kernel function can be verified by the fact that the
minimum of two numbers is a kernel for numbers in the range [0, 1] [36] and the fact that the product of kernels and the sum
of kernels are valid kernels [37], [36].
Let φ be the nonlinear mapping associated with the kernel function k. Hence, < φ(d), φ(d) >= k(d,d) =
∑c−1
i=1
∑c
j=i+1 βij
is the average classification rate with respect to the α-component of d. In fact, the first term of the kernel function k measures
the linear dependence of the α-components of d and d′ in the measurement space, and the second term can be regarded
as the intersection of the discriminatory capabilities of the α-components of d and d′ with respect to all pairs of classes.
Hence, if the α-components of d and d′ are linearly independent in the measurement space, or have low consistency in the
discriminatory capabilities with respect to all pairs of classes, φ(d) and φ(d′) are almost linearly independent in the kernel
feature space, and vice versa.
2) The kernel QR factorization with column pivoting for ranking discriminatory information: Define Matrix X as X =
[φ(d1)...φ(dt)]. The QR factorization with column pivoting ranks the columns of X according to their importance for forming
an independent subset of the columns of X. Specifically, X can be expressed by
QR = XE,
where Q is a matrix with orthonormal columns, E is a permutation matrix, and R is an upper triangular matrix with the
magnitudes of the diagonal entries in a non-increasing order, |R1,1| ≥ ... ≥ |Rt,t|, and moreover |Ri,i| ≥ ‖Ri:j,j‖2 , j =
i+1, ..., t withRi:j,j denoting the ith to the jth entries of the jth column ofR. Hence, the columns ofXE are ranked according to
their importance for forming an independent subset, and the ranked version of A, A′ , can be obtained by A′= [α
′
1...α
′
t] = AE.
As described in the following, the permutation matrix E can be obtained by applying the QR factorization with column pivoting
on the Cholesky decomposition of the Gram matrix without using φ to explicitly map the elements in D to the kernel feature
space.
a) The permutation matrix E for ranking discriminatory information: Define the Gram matrix G as G = XTX with
k(di,dj) the entry at the ith row and jth column. Without lost of generality, we may assume that X has full column rank
and thus have that the Gram matrix G is positive definite. As will be illustrated in the following lemma, the permutation
matrix E is also the permutation matrix of the QR factorization with column pivoting of Matrix C which is the Cholesky
decomposition [22], [36] of the Gram matrix G.
Lemma 1: The QR factorization with column pivoting of C shares the same permutation matrix with the QR factorization
with column pivoting of X where G = CTC is the Cholesky decomposition of G.
Proof: Since X has full column rank, the thin QR factorization of X can be uniquely expressed as X = Q′R′ , where
Q
′ is a matrix with orthonormal columns, and R′ is a t× t upper triangular matrix with positive diagonal entries [22]. Since
G = XTX = R
′TQTQR
′
= R
′TR
′
, and G can be uniquely factorized as G = CTC by the Cholesky decomposition [22]
where C is a t × t upper triangular matrix with positive diagonal entries, we have R′ = C. Let Q˜R˜ = CE be the QR
factorization with column pivoting of C. Hence, we have R′ = C = Q˜R˜E
T
. It should be noticed that R˜ has the property
of the upper triangular matrix of the QR factorization with column pivoting. Hence, we have X = Q′Q˜R˜ET by substituting
Q˜R˜E
T
for R′ , and obtain that Q′Q˜R˜ = XE is the QR factorization with column pivoting of X because Q′Q˜ is a matrix
with orthonormal columns, R˜ satisfies the property of the upper triangular matrix of the QR factorization with column pivoting,
and E is a permutation matrix.
3) The windowed SFS for forming the transformation matrix: After obtaining the permutation matrix E, the ranked version
of A, A′ , can be obtained by A′= [α
′
1...α
′
t] = AE. Here, the order of the columns of A
′ defines precedence of the columns
for forming the transformation matrix. That is, the column of A′ at the front should be preferred over than the column at
the rear. However, this rule is not appropriate to neighboring columns of A′ because the differences in the discriminatory
capabilities of neighboring columns may be insignificant. In this study, the windowed SFS, which is a modification of the
SFS by introducing a searching window to the SFS, is proposed to utilize the precedence of the columns of A′ to form the
transformation matrix.
The SFS starts with an initial empty subset of feature components, and enlarges the subset by including the component
among all unselected ones which maximizes a criterion function along with the selected components. In order to utilize the
precedence of the columns of A′ , a searching window [1, s] is introduced to the SFS such that for every round of searches,
only the columns of A′ with indices within the range [1, s] and linearly independent to the selected columns are evaluated. In
addition, the searching window is small initially and enlarged gradually to cover more unselected columns of A′ after every
round of searches. Under this mechanism, only a few columns of A′ in the front would be evaluated and selected in the
10
shown not to be noise resistant from a monotone property of the Fisher criterion. Third, the direct method is shown to be
inappropriate to solve the LDA when the feature vector contains lots of noise components and the sample size is small. Besides,
an unqualified extracted feature is shown possible to have a large value of the Fisher critierion. In other words, having a large
value of the Fisher criterion may not be a sufficient condition of a qualified extracted feature. This result indicates that the
LDA through maximizing the Fisher criterion is not always suitable for the undersampled problem. Finally, if the feature vector
contains an adequate number of effective features, the vector in ran(Sb) is shown to be unlikely to cover none of the effective
features. This result motivates the proposed approach described in the next section.
1) Having a large value of the Fisher criterion: a necessary property of a qualified extracted feature: Since the Fisher
criterion gauges the discrimination capability of the extracted feature, how large value of the criterion a qualified extracted
feature should have must be known. This discussion starts from the value of the Fisher criterion with respect to a Gaussian
random noise.
Let l be a non-zero vector and only cover the noise components. Suppose that the feature extracted by l is a Gaussian random
noise. The following proposition indicates that if the number of classes or samples is not too small, the feature extracted by l
is unlikely to have a large value of the Fisher criterion.
Proposition 1: If there are c classes, every class has K samples, and the features of all cK samples extracted by vector
l are independent random noises, xij , i = 1, ..., c, j = 1, ...,K, which have an identical normal distribution, N(µ, σ2), then
Pr((lTSwl)
−1 (lTSbl)≥ρ), the probability of the value of the Fisher criterion with respect to l not smaller than ρ, is
Pr((lTSwl)
−1 (lTSbl)≥ρ) =
1
Γ( c−12 )Γ(
cK−c
2 )2
cK−1
2
∫∞
0
∫ w
ρ
0 w
(c−1)/2−1yc(K−1)/2−1e−(y+w)/2dydw ρ>0
1 ρ = 0
, (3)
where Γ(t) is the gamma function defined by Γ (t) = ∫∞
0
e−xxt−1dx with t > 0.
Proof: See Appendix A.
From a series representation for the incomplete gamma function, Pr((lTSwl)−1
(
lTSbl
)≥ρ) with ρ>0 has a series formula
as follows:
Pr((lTSwl)
−1 (lTSbl)≥ρ) = f(c,K, ρ)
1
Γ( c−12 )2
cK−1
2
∞∑
n=0
Γ((cK − 1)/2 + n)
Γ(c(K − 1)/2 + 1 + n)2nρc(K−1)/2+n
(
2ρ
ρ+ 1
)(cK−1)/2+n
, ρ>0
which is more suitable for implementation. Roughly speaking, Pr((lTSwl)−1
(
lTSbl
)≥ρ) is nonincreasing with respect to
c, K, and ρ. Table I, which shows Pr((lTSwl)−1
(
lTSbl
)≥ρ) with respect to some values of c, K, and ρ, indicates that
samples drawn from a Gaussion random noise are unlikely to have a large value of the Fisher criterion. Hence, a necessary
property of the desired linear transformation could be designed as that the features of the samples extracted by the desired
linear transformation should have a large value of the Fisher criterion. To ensure that the linear transformation derived through
maximizing the Fisher criterion are always qualified, this property should also be a sufficient property for a qualified linear
transformation. Unfortunely, this property may not be sufficient especially when the sample size is small and the feature vector
contains a lot of noise components. An illustration of this phenomenon starts from a monotone property of the Fisher criterion
in the following section.
2) A monotone property of the Fisher criterion: Define J∗F as J∗F=maxL tr((LTSFwL)−1(LTSFb L)) with L subject to
det(LTSFb L) 6= 0 where SFw and SFb , repectively, represent the intra-class and inter-class scatter matrices with respect to the
feature vector composed of all of the feature components in the set F . Proposition 2 shows that the LDA through maximizing
the Fisher criterion tends to include more features.
Proposition 2: J∗F satisfies monotonicity, which is defined by J∗F1≤J∗F2 if F1 ⊆ F2.
Proof: See Appendix B.
This monotone property of the Fisher criterion gives rise to that the columns of the derived linear transformation tend to
cover more components including the noise components. Hence, if the feature vector contains a lot of noise components, the
possibility of the extracted feature being dominated by the noise component should be discussed. A similar argument like
Proposition 2 can be applied to show that the maximum of the Fisher criterion does not increase if the columns of the desired
linear transformation is subject to be in some subspace.
3) Having a large value of the Fisher criterion: not always a sufficient property for a qualified extracted feature for
undersampled problems: Suppose that the feature vector contains d effective components and n−d noise components. Without
loss of generality, we assume that the last n−dcomponents of a feature vector are the noise components. Let [dT pT ]T denote
a column of the desired linear transformation where d and p, respectively, are d and (n− d)-dimensional vectors. Hence, the
contribution of d may be meaningful but that of p is completely useless for discrimination.
12
• First, the magnitude of a feature component can be rescaled inverse proportion to the probability of the feature component
conveying no discriminatory information.
• Second, if the values of the Fisher criterion with respect to two linear transformations are about the same, the one having
the smaller rate of projection onto null(Sb) should be preferred.
By following the principle, the proposed approach first finds an n× r linear transformation T, where r is the rank of Sb,
and then selects the d most effective vectors in ran(T) to form the desired linear transformation L.
In the following, the first is a normalization process which changes the preference of the algorithms of linear algebra
such the SVD algorthm from the feature component of large magnitude to the feature component of a large probability for
conveying discriminatory information. The second is a derivation of an alternative form of the Fisher criterion. The second is
the presentation of the mathematical formulation of the proposed LDA which is based on the alternative form of the Fisher
criterion. The next is a numerically stable algorithm for the proposed LDA and an analysis of the complexity of the proposed
algorithm. The last is a direct method for solving the Fisher criterion. This direct method will be used to show a linear
transformation with a maximum value of the Fisher cirterion not always qualified. In order not to explicitly form the scatter
matrices, which may be huge matrices, matrices Hb and Hw defined as
Hb =
[ √
N1
N (m1 −m) ...
√
Nc
N (mc −m)
]
,
Hw =
1√
N
[
x11 −m1 · · · x1N1 −m1 · · · xc1 −mc · · · xcNc −mc
]
where xij is the jth sample of the ith class such that Sb = HbHTb and Sw = HwHTw are used.
1) A normalization process: Before performing the proposed LDA, the feature components of a measurement vector x are
normalized by the following formula
x
′
= Fx
where F is a diagonal matrix with the ith diagonal entry fi defined as
fi =
1√
f(c,K, ρi)hi
,
with ρi the value of the Fisher criterion associated with the ith feature component and hi = 1N
∑c
u=1
∑Nu
v=1(xuvi −mi)2.
After such a normalization process, the magnitude of a feature component can is inverse proportion to the probability of the
feature component conveying no discriminatory information.
2) An alternative form of the Fisher criterion: From tr((LTSbL)−1LTStL) = tr((LTSbL)−1LTSwL)+l and ∂tr((L
TSbL)
−1LTStL)
∂L =
0, the eigenvectors associated with the l smallest generalized eigenvalues of the matrix pencil λ(St,Sb) mimimizes the problem
argmin
L
Ja(L) = tr((L
TSbL)
−1LTSwL), subject to det(LTSbL) 6= 0. (6)
Since the eigenvectors of the matrix pencil λ(Sb,St) associated with the l largest generalized eigenvalues are the eigenvectors
of the matrix pencil λ(St,Sb) associated with the l smallest generalized eigenvalues, the linear transformation optimizing
the Fisher criterion optimizes Ja(L) and vice vesa. It can be checked that Ja(L) is invariant to any l × l nonsingular linear
transformations. This alternative form of the Fisher criterion is also called the inverse Fisher criterion [38].
3) The mathematical formulation of the proposed approach: By expressing T as T = BΣb +BcΣcb, Ja(T) becomes
Ja(BΣb +BcΣcb) = tr((Σ
T
b B
TSbBΣb)
−1((BΣb +BcΣcb)
TSw(BΣb +BcΣcb)), (7)
where Σb and Σcb are, respectively, r×r and (n−r)×r matrices, and Σb is nonsingular because TTSbT should be nonsingular.
Let αi and αci , i = 1, ..., r, be the columns of BΣb and BcΣcb, respectively. Thus, the ith column of T is equal to αi+αci
where αi and αci can be determined separately as follows.
Since Ja(T) is invariant to any r × r nonsingular linear transformations, αi, i = 1, ..., r, can be chosen in such a way that
ΣTb B
TSbBΣb= Ir×r and thus Eq. (7) becomes
Ja(BΣb +BcΣcb) = tr((BΣb +B
cΣcb)
TSw(BΣb +BcΣcb))
=
r∑
i=1
(αi +αci )
TSw(αi +αci ). (8)
Note that (αi +αci )TSw(αi +αci ) is equal to 1J(αi+αci ) because (αi +α
c
i )
TSb(αi +αci ) = 1.
14
TABLE I
A LIST OF Pr((lTSwl)−1
 
lTSbl
≥ ρ) WITH RESPECT TO SOME VALUES OF c, K , AND ρ.
c 2 3
K 2 3 4 5 2 3 4 5
1 2.93E-1 1.16E-1 4.98E-2 2.22E-2 3.54E-1 1.25E-1 4.42E-2 1.56E-2
2 1.84E-1 4.74E-2 1.34E-2 3.95E-3 1.92E-1 3.70E-2 7.13E-3 1.37E-3
3 1.34E-1 2.57E-2 5.42E-3 1.20E-3 1.25E-1 1.56E-2 1.95E-3 2.44E-4
4 1.06E-1 1.61E-2 2.71E-3 4.78E-4 8.94E-2 8.00E-3 7.16E-4 6.40E-5
ρ 5 8.71E-2 1.11E-2 1.55E-3 2.27E-4 6.80E-2 4.63E-3 3.15E-4 2.14E-5
6 7.42E-2 8.05E-3 9.65E-4 1.21E-4 5.40E-2 2.92E-3 1.57E-4 8.50E-6
7 6.46E-2 6.12E-3 6.41E-4 7.04E-5 4.42E-2 1.95E-3 8.63E-5 3.81E-6
8 5.72E-2 4.81E-3 4.48E-4 4.37E-5 3.70E-2 1.37E-3 5.08E-5 1.88E-6
9 5.13E-2 3.88E-3 3.25E-4 2.85E-5 3.16E-2 1.00E-3 3.16E-5 1.00E-6
10 4.65E-2 3.20E-3 2.43E-4 1.94E-5 2.74E-2 7.51E-4 2.06E-5 5.64E-7
TABLE II
A COMPARISON OF THE COMPLEXITY OF PROPOSED ALGORITHM WITH THOSE OF VARIOUS ROBUST METHODS [28], WHERE N , n, c DENOTE THE
NUMBER OF TRAINING SAMPLES, THE DIMENSION OF A FEATURE VECTOR, AND THE NUMBER OF CLASSES, RESPECTIVELY.
algorithm time space
LDA/IDDC O(nN2) O(nN)
PCA O(nN2) O(nN)
PCA+LDA O(nN2) O(nN)
LDA/QR O(cnN) O(cn)
LDA/GSVD O(n(N + c)2)) O(nN)
regularized LDA O(nN2) O(nN)
c) Main step 3: After determining T, if l is equal to r, the desired linear transformation L is T; otherwise, L can
be determined by TΩ where the columns of Ω are formed by the eigenvectors of TTSwT associated with the l smallest
eigenvalues.
Finally, the whole algorithm for solving the proposed LDA is summarized as follows.
• Step 1. Determine BΣb = [α1...αr] and B by U[:,1:r]Λ−1[1:r,1:r] and U[:,1:r], respectively, where UΛV
T is a thin SVD
of Hb.
• Step 2. Let T = BΣb +BcΣcb where BcΣcb = [αc1...αcr] is determined by Main step 2.
• Step 3. If l is equal to r, the desired linear transformation L is T; otherwise, L = TΩ where the columns of Ω are
formed by the eigenvectors of TTHwHTwT associated with the l smallest eigenvalues.
d) The complexity of the proposed algorithm: From r < c n, c ≤ N, and N  n and the algorithm described in [22],
the time complexity of the second main step of the proposed algorithm is O(n(c+N)2)+O((N+c)2c+c3)+O(N3)+O(ckN) =
O(n(N + c)2), a sum of the time complexities of the four steps of the second main step, where k is the maximum number of
iterations for root finding. The time complexity of the proposed algorithm is O(n(N + c)2), a sum of the time complexities
of the three main steps, namely, O(c2n+ c3), O(n(N + c)2), and O(cnN + c3). The space complexity is O(nN) because the
second main step needs to store Hw in the memory. As shown in Table II, the time and space complexities achieved by the
proposed algorithm are competitive with those of the other robust methods [28].
5) A direct method for solving the alternative form of the Fisher criterion: In this study, a direct method for the LDA is
also developed through optimizing the alternative form of the Fisher criterion. From Eqs. (8) and (17), we have that Ja(T)
can be mimimized by β
′
i, i = 1, ..., r, determined by
β
′
ij =

0 if Θjj = 0, j = 1...min{row(Θ), col(Θ)}
− α
′
ij
Θjj
if Θjj 6= 0, j = 1...min{row(Θ), col(Θ)}
0 j = min{row(Θ), col(Θ)}+ 1, ..., col(Θ)
(12)
with α1, ..., and αr obtained at Step 1 of Algorithm 1. Hence, the steps of the direct method are identical to those of the
algorithm for the proposed LDA except Step 4 of the main step two where the direct method determines β
′
i by Eq. (12).
IV. EXPERIMENTAL RESULTS
A. Experimental results of the proposed approach robust to outlier classes
The proposed approach was tested against synthesized data and real data, and compared to fourteen related approaches,
namely, Full, PCA [3], Fisher [2], [3], Loog [4], MMC [7], Chernoff [5], WChernoff [6], Mahalanobis [8], Tubbs [9], Hsieh [10],
16
SFSJ , SFSJ˜ , SFFSJ , and SFFSJ˜ where Full do not reduce the dimension of the sample vector, and SFSJ , SFSJ˜ , SFFSJ ,
and SFFSJ˜ produce the transformation matrix by integrating the α-components of the elements in D via the SFS and SFFS
algorithms with the criterion functions J(L) and J˜(L), respectively. The linear classifier and the quadratic classifier [3] were
adopted for evaluating these approaches. The covariance matrix for the linear classifier was estimated by the total scatter matrix.
In order to prevent the covariance matrices for the two classifiers from singular, the covariance matrices were regularized by
adding small positive numbers to the main diagonals of the matrices. PCA retained at least c− 1 largest principal components
such that the retained components preserve at least 95% of the squared norm of the total scatter matrix. For each method, the
reduced dimension is ranged from one to the theoretical limit of the method but no more than the number of retained principal
components. The numbers fB and fC for forming L∗ij and L∗C were empirically determined by two and the dimension of
the measurement vector, respectively. For every dataset, the following steps were repeated 100 times to estimate the average
classification errors for these methods.
1) For the experiment of the synthesized data, generate the dataset according to the specification shown in Table V.
2) Partition the samples of the data set into two sets, namely, the training set and the test set, randomly. The training set
contains about ninety percent of the samples and the test set holds the rest.
3) For each of the methods to be compared, apply it on the training set to derive the transformation matrix, perform the
linear classifier and the quadratic classifier to classify the samples of the test set after dimensionality reduction, and
compute the classification error.
1) Experimental results of the synthesized data: Table V shows the parameters for generating five sets of synthesized data
where every dataset has six classes and every sample vector has twelve feature components. The jth component of a sample
vector of the ith class is drawn from the normal distribution N(uij , σ2ij) independently. Datasets (b) to (e) were designed to
show the weak points of the global and pairwise approaches of LDA. Table shows the experimental results where the least
classification errors and the associated reduced dimensions of each method with respect to the five datasets are shown. The
experimental results are discussed as follows.
The first six feature components of Dataset (a) have larger ratios of the interclass distance to the intraclass distance than the
other six. The performances of the twelve method except PCA are satisfied because PCA discards some components important
to discrimination.
Dataset (b) contains an abnormal class, Class one, with samples of magnitude incompatible to those of the other classes.
Because Dataset (b) are heteroscedastic, the experimental results with respect to the linear classifier are all poor. For the case
of the quadratic classifier, five methods, namely, Fisher, Loog, MMC, Chernoff, and WChernoff are sensitive to Class one and
have results less effective.
The first class of Dataset (c) is an abnormal class. The intraclass distances of the first six feature components of Class one
are much larger than those of the last six, which is contrary to the relation of the twelve feature components of the other five
classes. The experimental results with respect to the linear classifier are all poor because Dataset (c) are heteroscedastic. For
the quadratic classifier, five methods, namely, Fisher, Loog, MMC, Chernoff, WChernoff, and Hsieh are less effective. Hence,
the criterion functions of these five methods are significantly influenced by the intraclass scatter matrix of Class one.
Classes 3 to 6 of Dataset (d) have a priori probabilities much less than those of Classes 1 and 2. In addition, the last six
feature components of Classes 3 to 6 have large interclass distances. PCA, Fisher, MMC, Tubb, Chernoff, Mahalanobis, SFSJ ,
and SFFSJ˜ are ineffective in this case because the interclass distances of Classes 3 to 6 dominate the criterion functions of
the seven methods but the populations of Classes 3 to 6 are only one-third of the entire population.
PCA, Fisher, Loog, MMC, Tubb, Chernoff, SFSJ , SFSJ˜ , SFFSJ , and SFFSJ˜ are ineffective in Dataset (e). In fact, these
approaches fail to discriminate between Classes one and two because the criterion functions of them are diminated by Classes
1, and 3 to 6.
From the definitions of the approaches in comparison, some of them are inherently sensitive to some configurations of
the classes to be discriminated. PCA and MMC prefer the feature component of large magnitude. Tubb and Mahalanobis
regard that all classes have equal a priori probabilities. Fisher assumes that all classes have equal intraclass scatter matrices. In
addition, the class having a mean with magnitude incompatible to the other classes may be overemphasized or neglected. Loog,
Chernoff, and WChernoff are more robust than Fisher; however, their criterion functions may be dominated by the intraclass
scatter matrix of an outlier class with magnitude much larger than the others’. Hsieh needs the result of Loog; therefore, Hsieh
may be influenced by the outlier class having an intraclass scatter matrix with magnitude much larger than the others’. On
the other hand, the proposed approach succeed in these five datasets. This result shows that integrating global and pairwise
discriminatory information is a promising approach to robustly producing a linear feature extractor. Besides, since SFSJ and
SFFSJ are ineffective in Datasets (d) and (e), and SFSJ˜ and SFFSJ˜ are ineffective in Dataset (e), we can also know that the
ranking the discriminatory information by the QR decomposition with column pivoting helps to robustly produce an effective
linear feature extractor.
2) Experimental results of the real data: Twelve sets of real data from the UCI databases [39] were adopted to evaluate the
proposed approach. The twelve datasets are described in Table VI. Tables VII and VIII show the experimental results of the
fourteen methods where the least classification errors and the associated reduced dimensions of each method with respect to
18
TABLE V
EXPERIMENTAL RESULTS OF THE SYNTHESIZED DATA WITH RESPECT TO THE QUADRATIC CLASSIFIER.
Dataset
(a) (b) (c) (d) (e)
Full 0(12) 0(12) 0(12) 0(12) 0(12)
PCA 0.014(9) 0(6) 0(5) 0.137(5) 0.01(5)
Fisher 0(5) 0.467(5) 0.52(5) 0.007(5) 0.25(5)
Loog 0(4) 0.446(5) 0.56(5) 0(2) 0.192(5)
MMC 0(9) 0.453(5) 0.58(6) 0.019(5) 0.189(5)
Tubbs 0.072(9) 0(5) 0(5) 0.081(5) 0.01(5)
Chernoff 0(5) 0.133(6) 0.11(6) 0.061(5) 0.077(5)
WChernoff 0(5) 0.117(6) 0.057(6) 0(3) 0(4)
Hsieh 0(4) 0.006(6) 0.001(6) 0(2) 0(5)
Mahalanobis 0(5) 0(4) 0(4) 0.013(5) 0(4)
Proposed 0(5) 0(5) 0(4) 0(3) 0(4)
SFSJ 0(5) 0(4) 0(3) 0.031(5) 0.032(5)
SFSJ˜ 0(5) 0(4) 0(3) 0(3) 0.011(5)
SFFSJ 0(5) 0(4) 0(3) 0.031(5) 0.032(5)
SFFSJ˜ 0(5) 0(4) 0(3) 0(3) 0.011(5)
TABLE VI
DESCRIPTIONS OF THE REAL DATASETS.
no. Dataset n c PC # of samples
a Wisconsin breast cancer 9 2 7 682
b BUPA liver disorders 6 2 3 345
c Pima Indians diabetes 8 2 3 768
d Wisconsin diagnostic breast cancer 30 2 1 569
e Cleveland heart-disease 13 2 3 297
f SPECTF heart 44 2 24 349
g Iris plants 4 3 2 150
h Thyroid gland 5 3 4 215
i Vowel Recognition 10 11 10 900
j Stalog (Landsat satellite) 10 5 6 5473
k Multifeature digit (Zernike moments) 13 3 13 178
l Glass identification 18 4 5 846
TABLE VII
EXPERIMENTAL RESULTS OF THE REAL DATA WITH RESPECT TO THE LINEAR CLASSIFIER.
Dataset
(a) (b) (c) (d) (e) (f)
Full 0.042(9) 0.321(6) 0.221(8) 0.044(30) 0.413(13) 0.231(44)
PCA 0.041(4) 0.41(2) 0.251(1) 0.129(1) 0.461(2) 0.215(24)
Fisher 0.04(1) 0.456(1) 0.246(1) 0.256(1) 0.378(1) 0.298(1)
Loog 0.042(1) 0.321(1) 0.221(1) 0.044(1) 0.384(1) 0.222(1)
MMC 0.042(2) 0.34(3) 0.225(3) 0.129(1) 0.424(1) 0.221(24)
Tubbs 0.041(6) 0.416(3) 0.247(2) 0.128(1) 0.449(3) 0.209(24)
Chernoff 0.042(1) 0.322(3) 0.22(1) 0.058(1) 0.399(2) 0.206(10)
WChernoff 0.042(1) 0.322(3) 0.22(1) 0.058(1) 0.379(1) 0.206(10)
Hsieh 0.042(1) 0.321(1) 0.221(1) 0.044(1) 0.384(1) 0.23(24)
Mahalanobis 0.041(1) 0.322(1) 0.23(1) 0.04(1) 0.39(2) 0.221(1)
Proposed 0.042(1) 0.319(3) 0.22(1) 0.04(1) 0.379(1) 0.22(24)
SFSJ 0.041(1) 0.319(3) 0.222(2) 0.04(1) 0.417(3) 0.206(10)
SFSJ˜ 0.041(1) 0.319(3) 0.222(2) 0.04(1) 0.376(1) 0.206(10)
SFFSJ 0.041(1) 0.319(3) 0.222(2) 0.04(1) 0.417(3) 0.206(10)
SFFSJ˜ 0.041(1) 0.319(3) 0.222(2) 0.04(1) 0.376(1) 0.206(10)
20
TABLE IX
ERROR RATES OF THE PROPOSED ROBUST LDA VERSUS VARIOUS APPROACHES.
Noise level of noise
components Proposed PCA+LDA GSVD QR Robust Regularized LDA
0 0.22 0.33 0.25 0.66 0.32 0.37
1 0.22 0.39 0.26 0.66 0.39 0.33
2 0.22 0.66 0.63 0.65 0.66 0.64
3 0.22 0.79 0.76 0.78 0.79 0.77
4 0.22 0.80 0.78 0.8 0.8 0.79
extractors have been proposed. To be robust to the outlier class, we have proposed a novel approach to integrating the results
of pairwise and global approaches of discriminant analysis to produce an effective feature extractor. Such an integration is based
on the idea that the pairwise approach of discriminant analysis is more robust to the outlier class and the global approach
of discrimant analysis can produce overall effective linear feature extractor. To overcome the overfitting problem, we have
realized that the method of LDA on the undersampled problem should have some preferred properties for the desired linear
transformation. Most existing approaches of LDA for the undersampled problem implicitly prefer the feature components of
large magnitude. However, this preference is unjustified. Here, a normalization procedure has been proposed to rescale the
magnitude of the feature component according to the propability of the feature component having discriminatory information.
Based on the normalized measurement vector, the preference of the approaches of LDA on the undersampled problem is changed
from the feature component of large magnitude to the feature component possibly having more discriminatory information.
The experimental results have proved the feasibility of the proposed approaches.
APPENDIX
A. Proof of Proposition 1
Let mi and m denote the mean vector of the ith class and the mixture mean of xij , i = 1, ..., c, j = 1, ...,K, respectively.
We can obtain
Pr((lTSwl)
−1 (lTSbl)≥ ρ) = Pr(1
c
c∑
i=1
(mi −m)2≥ ρ
cK
c∑
i=1
K∑
j=1
(xij −mi)2). (13)
Dividing both sides of the inequality of the right-handed side of Eq. (13) by σ2cK , we obtain
Pr((lTSwl)
−1 (lTSbl)≥ ρ) = Pr(∑ci=1(mi −m)2
σ2/K
≥ρ
∑c
i=1
∑K
j=1(xij −mi)2
σ2
). (14)
Since xij ,i = 1, ..., c, j = 1, ..,K, are independent random variables with the normal distribution, N(µ, σ2), mi and
PK
j=1(xij−mi)2
σ2 , i =
1, ..., c, are independent and have distributions N(µ, σ2/K) and the chi-square distribution with K − 1 degrees of freedom,
denoted by χ2(K − 1), respectively [40]. Furthermore,
Pc
i=1(mi−m)2
σ2/K and
Pc
i=1
PK
j=1(xij−mi)2
σ2 are independent and have
distributions χ2(c− 1) and χ2(c(K − 1)), respectively. Accordingly, from Eq. (14), we can obtain
Pr((lTSwl)
−1 (lTSbl)≥ ρ)
=
∫ ∞
0
Pr(
∑c
i=1(mi −m)2
σ2/K
= w) Pr(
∑c
i=1
∑K
j=1(xij −mi)2
σ2
≤ w
ρ
)dw
=
1
Γ( c−12 )Γ(
cK−c
2 )2
cK−1
2
∫ ∞
0
∫ w
ρ
0
w(c−1)/2−1yc(K−1)/2−1e−(y+w)/2dydw.
B. Proof of Proposition 2
Let L1 denote the optimal linear transformation with respect to the scatter matrices SF1b and SF1w . Hence, J∗F1 = tr((L
T
1 S
F1
w L1)
−1(LT1 S
F1
b L1))
and det(LT1 SF1b L1) 6= 0. Without lost of generality, the feature vector composed of the features in F2 can be formed by
extending the feature vector with respect to F1 in such a way that SF2b and SF2w have the following forms:
SF2b =
[
SF2b11 S
F2
b12
SF2b21 S
F2
b22
]
, and SF2w =
[
SF2w11 S
F2
w12
SF2w21 S
F2
w22
]
,
where SF2b11 = S
F1
b and SF2w11 = S
F1
w . Define L
′
1 = [ L
T
1 0|F2−F1|×l ]
T . Since det(L′T1 S
F2
b L
′
1) = det(L
T
1 S
F1
b L1) 6= 0, L
′
1
is a feasible solution with respect to the scatter matrices SF2b and SF2w . Let L2 denote the optimal linear transformation with
22
Hence, if h(0) ≤ 0, β′i, i = 1, ..., r, determined by Eq. (11) with µ = 0 are feasible and optimal. Otherwise, the optimal
solution should be on the boundary of the feasible region; that is, µ should be a root of h(µ) = 0. The root of h(µ) = 0 can
be found by some root finding algorithms described later. Finally, after determining β
′
i, i = 1, ..., r, α
c
i can be obtained by
αci = ΦΠβ
′
i, i = 1, ..., r.
For simplicity, the root of h(µ) = 0 is found by a bracket method in case h(0) > 0. Specifically, let [µb, µa] denote a range
where a positive root of h(µ) = 0 is enclosed. From the mean value theorem, a root of h(µ) = 0 should be in [µb, µa] provided
h(µb) ≥ 0 and h(µa) ≤ 0. Define two functions ha(µ) and hb(µ) as
hb(µ) =
(
1− µmaxj{Θ
2
jj}
1 + µmaxj{Θ2jj}
)2 r∑
i=1
∑
j∈{k|Θkk 6=0,k=1,...,s}
α
′2
ij − ρ,
ha(µ) =
(
1− µminj{Θ
2
jj}
1 + µminj{Θ2jj}
)2 r∑
i=1
∑
j∈{k|Θkk 6=0,k=1,...,s}
α
′2
ij − ρ,
which satisfy hb(µ) ≤ h(µ) ≤ ha(µ). Hence, a positive root of h(µ) = 0 should be in [ γmaxj{Θ2jj} ,
γ
minj{Θ2jj} ] where
γ =
√∑r
i=1
∑
j∈{k|Θkk 6=0,k=1,...,s} α
′2
ij
ρ
− 1
because h(µ) with h(0) > 0 is monotone non-increasing for µ ≥ 0, hb( γmaxj{Θ2jj} ) = 0, and ha(
γ
minj{Θ2jj} ) = 0. With
[µb, µa] = [
γ
maxj{Θ2jj} ,
γ
minj{Θ2jj} ] as an initial range for enclosing the positive root of h(µ) = 0, the positive root can be found
by the following steps.
• Step 1. [µb, µa]←− [ γmaxj{Θ2jj} ,
γ
minj{Θ2jj} ].
• Step 2. µ←− √µb × µa.
• Step 3. Repeat the following steps until |h(µ)| ≤  or the number of iterations exceeds a pre-defined threshold.
– Step 3.1. If h(µ) > 0, µb ←− µ; otherwise, µa ←− µ.
– Step 3.2. µ←− √µb × µa.
• Step 4. Output µ as a root of h(µ) = 0.
