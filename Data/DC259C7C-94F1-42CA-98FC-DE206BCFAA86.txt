中文摘要：
本計畫開發一套以顯微鏡為載體的條紋投影輪廓儀，並且利用相位光罩(phase mask)、
及影像處理技術，以延長顯微鏡的焦深(depth of focus)及景深(depth of field)，建構一套高放
大倍率、高深度量測範圍、全域、非掃瞄式的形貌檢測系統。相位光罩是以 liquid-crystal
spatial light modulator (LC-SLM) 搭配一個 polarizer 和一個 analyzer 而成。目前已知的技術，
雖然可將光學系統的景深延長十倍，然而影像解晰度不僅變差，perspective distortion 的情
況也十分明顯。相對而言，本計畫所提出的量測技術，即使延長了二十倍景深，條紋投影
輪廓儀的精密度，也能維持在一個 micron 之內，perspective distortion 也能透過校正系統彌
補。不須作縱向掃瞄或深度變焦，即可進行量測，對動態物件的即時形狀檢測有莫大助益。
開發本計畫的優勢在於：
1. 相較於現有的形貌檢測系統，本計畫的縱深量測範圍延長了二十倍，因此不須作縱
向掃瞄或深度變焦，即可進行量測。
2. 本計畫提出的量測方式為 one-shot measurement，特別適用於動態物體的 3D 量測(例
如微機電動態元件)。
3. 此裝置為全域式的光學系統，不須額外提供掃瞄機組，故結構簡單輕便，對於可攜
式的形貌量測系統，提供極佳的便利性。
4. 搭配本計畫提出的 transmittance-encoded fringes 原理，可進行表面深度有大落差的
形貌量測(surfaces with large depth discontinuities)。
5. 本計畫提出延長系統景深及焦深的技術，不僅可擴展條紋投影輪廓儀的適用彈性，
對所有以高倍率光學系統(例如顯微鏡)為載體的量測工具，都有莫大幫助。
關鍵詞：三維形貌量測，光學量測，條紋投影技術， phase mask，phase-shifting技術，Fourier
transform method，「相位－縱深」關係式，三維影像處理。
英文摘要：
An approach using a liquid-crystal spatial light modulator (LC-SLM) to enlarge the depth
measuring range of the projected fringe profilometry is presented. This approach is especially
applicable to detect dynamic objects with micro-scale sizes. Compared with a typical 2D image
system, the LC-SLM provides a better performance for a 3D shape sensing system. The main
advantages include (1) a much higher allowance to increase in the depth measuring range, (2)
easiness to compensate perspective distortion and geometric distortion, (3) very high accuracy (in
the micron-range) and (4) only one phase measurement needed for operation.
Keywords：3D shape sensing, optical inspection, phase mask fringe projection, phase-to-depth
relation, 3D image processing
extend the depth measuring range without seriously reducing the optical power and image
resolution.
The DOF is increased as the strength of the phase mask is increased. Unfortunately, increasing
the mask strength reduces the point-spread function at high frequencies, resulting in a lower
spatial resolution on the retrieved image. There is a trade-off between the extended DOF and the
image resolution. The task to optimize the extended DOF by choosing a proper strength of the
phase mask is somewhat time-consuming. Recently, Hong et al. [7] presented a flexible DOF
system using a liquid-crystal spatial light modulator (LC-SLM). With the proper choice of the
angle of the polarizer and analyzer in the front and the back of the LC-SLM, it could serve as a
cubic phase mask. The strength of the phase mask was tunable by the external video signal. Thus,
it provided a convenient way to identify the associated mask strength. However, accuracy was
limited by the pixel resolution of the LC-SLM. Additional modulation errors also occurred due to
the nonlinear signal response and the inconstant intensity output.
The above techniques were developed to cope with 2-D image problems. For a 3D shape
sensing system, it is still necessary to extend the DOF. For example, projected fringe
profilometry [8] is a powerful tool to describe the 3D shape of the inspected object. Its depth
measuring range is confined by the depth of field of the image acquisition system and the depth
of focus of the fringe projection system. To inspect micro-scale objects, microscopes are usually
adopted both into the image acquisition system and the fringe projection system [9, 10]. The
depth measuring range is therefore limited by the DOF of the microscopes. Our previous work
has shown that supercontinuum illumination can be employed to extend the DOF of the fringe
projection system [11]. However, this approach can only be applied to objects with large-scale
sizes. When the microscope is employed for detection, it is still required to develop an approach
to enlarge the DOF of the image acquisition system.
In this paper, we investigate an approach based on the LC-SLM to enlarge the depth
measuring range for the projected fringe profilometry. A microscope combined with a wide-angle
eyepiece lens is employed to project a fringe pattern onto the inspected surface. At a different
viewpoint, a CCD camera observes the projected fringes through another microscope and a cubic
phase mask. Phase of the fringes is extracted by the Fourier transform method [8]. With the
presented calibration schemes [12, 13], depth information can be determined from the phase of
the fringes. The cubic phase mask enlarges the depth of field of the image acquisition system,
while the wide-angle eyepiece lens increases the depth of focus of the fringe projection system. It
is found that the depth measuring range could be extended up to 1600m, even though the DOF
of the microscope was only 80m.
Compared with 2D image problems, the depth measuring range has been superiorly enlarged.
In addition, it is found that the limitations of LC-SLM (i.e., low pixel resolution, nonlinear signal
response, and inconstant intensity output) are not significant in 3D shape measurements. For
example, the LC-SLM extended the DOF of the microscope from 80m to 600m, but the details
transform of the point spread function, as given by




 














yx
ff
k
j
yyxx
ff
k
j
yyxx
yx
dd
effP
effP
ff
yyxx
yyxx






])2/()2/[(
2*
])2/()2/[(
2
22
22
)2/,2/(
)2/,2/(
),,(H
,
(4)
where “*”is the complex conjugate operation, and fx and fy are the spatial frequencies. The
intensity of the image can therefore be carried out by the convolution theorem,
)},({),,()},,({ oooyxiii yxIffyxI   H , (5)
where {} denotes the Fourier transform operation. Our purpose is to design a specific pupil
function on the square aperture so that H is not sensitive to defocus, i.e.,
)0,,(),,( yxyx ffff HH  . (6)
In such a case, the object function Io(xo, yo) can be retrieved by de-convoluting the defocused
image Ii(x,), as mathematical expressed as















 
)0,,(
)},,({
),,(
)},,({
),( 11
yx
iii
yx
iii
ooo ff
yxI
ff
yxI
yxI
HH


 , (7)
where {}1 denotes the inverse Fourier transform operation. It has been shown that H can be
insensitive to defocus if the pupil function is given by [1]


 
otherwise,0
2/,2/,)](exp[
),(
33
2
1 LyLxyxj
yxP

,
(8)
whereis a constant value, and L is the width of the square aperture. The OTF becomes







 ,
0if,1
0if)],(
3
exp[)](
4
exp[
1
12),,(
2
33
yx
yxyxyx
yxyx
ff
ffffjffj
ffff 



H (9)
For a large vale of , both fx and fy are much smaller than . Eq. (9) can then be
represented as
,
0if,1
0if)],(
4
exp[
1
12)0,,(),,(
33








yx
yxyx
yxyxyx
ff
ffffj
ffffff



 HH (10)
Consequently, a large value ofminimizes the sensitivity of defocus. An example of the pupil
function is depicted as Fig. 2, in which theis 40.
experiments, phase modulation could be approximately linear if the input gray level was ranging
from 50 to 190. Intensity variation could be less than 2.5% on the same input range.
(a) (b)
Fig. 5. Response of the LC-SLM when p = 170° and a = -150: (a) Phase modulation, and (b) intensity
variation.
The LC-SLM modulated the phase linearly between 0 and 1.6. Thus, phase of the pupil
function should be wrapped with the 1.6modulo operation. The residue of Fig. 3 after the
modulo operation is shown in Fig. 6. This pattern is then used to be the input video signal.
Fig. 6. Distribution of the input signals ranging from level 50 to 190: the brighter one represents the stronger
input signal.
The LC-SLM was combined with a microscope to form an extended DOF system, as shown
in Fig. 7(a). A square aperture with size of 15mm15mm was located on the LC-SLM, so that Eq.
(8) was applicable to this system. A simple way to identify a suitable value for this system is
illustrated as Fig. 7(b). A halogen lamp was selected as the incoherent light source. Lightwaves
were launched into a pin hole to generate a point source. For a suitable value of , the OTF
obtained by the CCD camera should be similar to those shown on Fig. 2, and should be not
sensitive to defocus. Once thevalue was identified, this extended DOF system could be used as
the image acquisition system of the projected fringe profilometry.
3.2 Calibration
Our previous work [12] has shown that correspondence between the unwrapped phaseand the
depth position z can be determined by
,)(
0



N
n
n
ncz  (11)
where cn are coefficients of the polynomial. To identify these coefficients, a flat plate was
selected as the inspected sample. As shown in Fig. 9(a), this plate was mounted on a translation
stage and could be moved along the z-axis. The CCD camera recorded the projected fringes on
the flat surface. Phase measurements were repeated as the plate was successively translated to
different depth position (at planes z = z1, z2,…, zi). Hence, a series of unwrapped phases and the
associated depths were then obtained at each pixel location. With a curve fitting algorithm,
coefficients in Eq. (11) could be determined.
Note that the LC-SLM increased the DOF of the image acquisition system. Thus, the
calibrated depth range (i.e., distance between z1 and zi) was enlarged.
The transverse positions x and y can also be expressed as functions of z, as given by





o
o
bzby
azax
1
1
, (12)
where a1, ao, b1, and bo are undetermined coefficients. To identify these coefficients, two 1D
sinusoidal patterns with their fringes parallel to the x and the y directions were sequentially
placed at planes z = z1, z2,…, zi. The setup is depicted in Fig. 9(b), but only a sinusoidal pattern
with their fringes parallel to the x direction is shown. Again, phase measurements were performed
at various depth positions. Since the period of the fringes was known, absolute phases could be
converted to transverse positions. With a subsequent curve fitting process, coefficients at each
pixel could be determined.
For an extended DOF system, lateral distortion due to the perspective projection of the
imaging geometry might appear on the retrieved image. With the presented calibration scheme,
such kind of perspective distortions could be compensated.
Once the coefficients in Eq. (11) and Eq. (12) were identified, the depth position of an
inspected object could be described from the unwrapped phase. Then the corresponding
transverse position could be carried out by the depth value.
(a) (b) (c)
Fig. 11. Appearance of the recorded images at (a) z = -750m, (b) z = 30m, and (c) z = 700m.
(a) (b) (c)
Fig. 12. Retrieved images at (a) z = -750m, (b) z = 30m, and (c) z = 700m.
The images shown as Fig. 11 were processed with Eq. (7) to retrieve the un-blurred data sets.
Shown in Fig. 12 is the evaluated result. Figure 13 shows the wrapped phases extracted by the
Fourier transform method. Phase fluctuated on the image-focused area. The reason came from
that the inspected surface was not diffusive enough. The observed image on the focused area was
not uniformly shiny. As a result, the signal-to-noise ratio on the image-focused area was weak.
Errors therefore occurred on that area.
(a) (b) (c)
Fig. 13. Phase distributions at (a) z = -750m, (b) z = 30m, and (c) z = 700m.
(a) (b)
Fig. 15. (a) Appearance of the projected fringes on the tilted object. (b) Retrieved image.
Fig. 16. Retrieved 3D profile of the tilted object.
4.2 Some comments on this technique
Compared with 2D image problems, the depth measuring range has been superiorly enlarged. An
example is shown as Fig. 17. The microscope used in our 3D profile measurements was
employed to observe a 2D pattern. Its DOF was extended with the LC-SLM. The coefficientof
the phase mask was the same as that used in our 3D profile measurements. Appearance of the
in-focused image is shown as Fig. 17(a). When this pattern was shifted with a displacement of
300m behind the object plane, the detail of the retrieved image was damaged, as shown in Fig.
17(b). The pattern’s fine details were mainly damaged due to the low pixel resolution of the
LC-SLM and low signal-to-noise ratio of the point-spread function at high frequencies. Nonlinear
signal response and inconstant intensity output of the LC-SLM
(a) (b)
measuring range is realized.
9. Reference
1. E. R. Dowski, Jr. and W. T. Cathey, “Extended depth of field through wave-front coding,” 
Appl. Opt. 34, 1859-1866 (1995).
2. S. Bradburn, E. R. Dowski, Jr. and W. T. Cathey, “Realizations of focus invariance in
optical-digital systems with wave-front coding,”Appl. Opt. 36, 9157-9166 (1997).
3. S. C. Tucker, W. T. Cathey, and E. R. Dowski, Jr.“Extended depth of feld and aberation 
control for inexpensive digital microscope systems,” Opt. Express 4, 467-474 (1999).
4. M. Mino, and Y. Okano,“Improvement in the OTF of a defocused optical system through
the use of shaded apertures,”Appl. Opt. 10, 2219-2225 (1971).
5. G. Ha¨usler,“A method to increase the depth of focus by two step image processing,”Opt.
Commun. 6, 38-42 (1972).
6. J. Ojeda-Castaneda and L. R. Berriel-Valdos,“Zone plate for arbitrarily high focal depth,”
Appl. Opt. 29, 994-997 (1990).
7. D. Hong, K. Park, H. Cho, and M. Kim, “Flexible depth-of-field imaging system using a
spatial light modulator,”Appl. Opt. 46, 8591-8599 (2007).
8. M. Takeda, and K. Mutoh, “Fourier transform profilometry for the automatic measurement 
of 3-D object shaped,” Appl. Opt. 22, 3977-3982 (1983).
9. K. Körner, R. Windecker, M. Fleischer, and H. J. Tiziani,“One-grating projection for
absolute three-dimensional profiling,” Opt. Eng. 40, 1653-1660 (2001).
10. R. Windecker, M. Fleischer, K. Körner, and H. J. Tiziani,“Testing micro devices with
fringe projection and white-light interfeometry,”Opt. Lasers in Eng. 36, 141-154 (2001).
11. W. H. Su, Z. Liu, K. Shi, B. Wang, K. Reichard, and S. Yin,“ A large depth of field 
projected fringe profilometry using a supercontinuum light illumination,”Opt. Express 13,
1025-1032 (2005).
12. H. Liu, W. H. Su, K. Reichard, and S. Yin, “Calibration-based phase-shifting projected
fringe profilometry for accurate absolute 3D surface profile measurement,” Optics 
Communications 216, 65-80 (2003).
13. W. H. Su, C. Y. Kuo, C. C. Wang, and C. F. Tu, “Projected fringe profilometry with
multiple measurements to form an entire shape,”Opt. Express 16, 4069-4077 (2008).
14. J. W. Goodman, Introduction to Fourier Optics, (Roberts & Company, Englewood,
Colorado, USA, 2005), Chap. 5.
15.K. Lu and B. E. A. Saleh, “Theory and design of the liquid crystal TV as an optical spatial
light modulator,” Opt.  Eng. 29,240–245 (1990).
16. E. Zappa, and G. Busca, “Comparison of eight unwrapping algorithms applied to
Fourier-transform profilometry,”Opt. Lasers Eng. 46, 106-116 (2008).
32. 蘇威宏，蕭智鴻，「條紋投影輪廓儀」，中華民國專利，申請案號：097146126 (2009
年)。
33. 蘇威宏，許俊翔，「使用相位光罩之顯微量測系統及方法」，中華民國專利，申請案
號：097146126 (2010 年)。
計畫成果自評
本計劃研究內容與原計畫十分相符，並且於期限內達成預期目標。本計畫執行成效豐碩，
主要有：
1. 四篇國際期刊(SCI)的發表[18-21]；
2. 參與國際研討會的發表，計 6 篇[22-27]；
3. 參與國內研討會的發表，計 3 篇[28-30]；
4. 一位碩士班研究生依此計劃於 2010 年六月順利畢業，碩士班畢業論文計 1 篇[31]；
5. 申請中之中華民國專利二篇[32, 33]。
技術特點
1. 相較於現有的形貌檢測系統，本計畫的縱深量測範圍延長了二
十倍，因此不須作縱向掃瞄或深度變焦，即可進行量測。
2. 本計畫提出的量測方式為 one-shot measurement，特別適用於動
態物體的 3D 量測(例如微機電動態元件)。
3. 此裝置為全域式的光學系統，不須額外提供掃瞄機組，故結構
簡單輕便，對於可攜式的形貌量測系統，提供極佳的便利性。
4. 搭配本計畫提出的 transmittance-encoded fringes 原理，可進行
表面深度有大落差的形貌量測 (surfaces with large depth
discontinuities)。
5. 由 於 影 像 擷 取 系 統 只 需 要 擷 取 一 次 資 料 (one-shot
measurement)，因此可進行動態物體的三維形貌量測。
推廣及運用的價值
形貌檢測 (Profile Inspection) 在工業界的應用日趨廣泛。實際的例
子，在國防工業如飛彈、魚雷、機翼與螺旋槳等形貌之鑑定，醫學
工程如生物細胞，電子產業如 PC 板面，機械製造如齒輪形狀探測
等，皆需要對待測物之三維形貌作精密的檢定。故發展精確的形貌
量測技術已成為產業界的重大課題之一。
※ 本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
Profile measurements for images blurred by motion
Wei-Hung Su
Department of Material Science and Optoelectronic Engineering,
National Sun Yat-Sen University
Kaohsiung 804, Taiwan
ABSTRACT
An algorithm using fringe projection to retrieve the 3D shape from images blurred by motion is
described. Theoretical analysis shows that objects moving within one period of the projected fringe can
be directly described by the projected fringe profilometry. Thus, the cost of the detection system is
effectively reduced.
Keywords: fringe projection, projected fringe profilometry, sinusoidal grating
I. INTRODUCTION
3D profile measurements play important roles inreversed engineering, machine vision, automatic
manufacturing, and other industrial applications. The use of a full-field technique, such as stereovision
[1, 2], fringe projection [3,4] and structured light illumination [5, 6], has been recognized as a
promising method for the measurement of a surface profile.
One of the major studies for 3D sensing is the measurement of dynamic objects. For objects with fast
moving speeds, 3D shape measurements always require the observed images not blurred by motion. A
high-speed camera or stroboscopic illuminations are commonly used to obtain the un-blurred images.
Unfortunately, when speed of the objects still exceeds the temporal resolution of the sensor, the image
is blurred. Of course, an ultra short laser pulse can still be used to freeze the motion on the image.
However, the illumination intensity might not be sufficient to perform large-scale measurements, and
the cost of such kind of light sources is generally high.
In this paper, we show that the projected fringe profilometry [3,4] needs not to avoid the blurred
images. As a typical setup, we use a fringe pattern to illuminate the dynamic object and utilize a CCD
camera to record the fringe distribution. Fringes on the obtained image are deformed by the topography
of the object, and also, blurred by motion. Theoretical analysis shows that objects moving within one
period of the projected fringe can be directly described by the projected fringe profilometry. Thus, the
cost of the detection system is effectively reduced.
The image sensor array obtains a blurred image within the exposure time t. The intensity of the
blurred image can be expressed as








 

2
2
),,(
22
cos),(),(
),(cos),(),(),(
ttt
ttt zx
yxyx
blurredblurred
o
o
dttyxZ
TT
x
tytxbRtytxaR
yxyxByxAMyMxI


(3)
where blurred is the phase of the blurred fringes, A(x, y) is the measured background intensity or dc
level, B(x, y) is the measured modulation amplitude, R(x, y) is the reflectivity of the measured object,
and M, the magnitude of the imaging lens.
For objects in which R(x, y) varies slowly with x and y, Eq. (3) can be expressed as








 
2
2
blurred ),,(
22
cos),(),(),(
ttt
ttt zx
o
o
dttyxZ
TT
x
yxbRyxaRMyMxI
 (4)
Substituting Eq. (2) into Eq. (4), intensity of the blurred image is then simplified as





  ),(22cos)(sinc),(),(),(blurred yxZTT
x
ttyxbRtyxaRMyMxI o
zx
 (5)
where
zzyxo TyxZ /])(),([  
 , and sinc(x) = sin(x)/(x).
Compared Eq. (3) with Eq. (5), Zo(x, y) can be fully identified from the blurred fringes, as given by
x
T
T
yx
T
yxZ
x
zz  ),(
2
),( blurredo 
(6)
Thus, the 3D shape of the dynamic object at t = to can be directly retrieved from the blurred fringe.
III.experiments
A ball with moving speedx =0.62 mm/sec,y = 0.62 mm/sec andz = 0.13 mm/sec was chosen as the
dynamic sample. Its diameter was approximately 40mm. A sinusoidal fringe pattern was illuminated by
a halogen lamp and then projected onto this dynamic sample. A CCD camera with 10241024 pixels at
12-bit pixel resolution was used to record the fringe distribution. Figure 2(a) shows the fringe
distribution, in which the exposure time was 4.0 second. Fringes were blurred by linear motion.
Figure 3. (a) Retrieved profile Zo(x, y) for the inspected object when the object was dynamic. A color bar
is used to address the depth values. (b) One-dimensional surface plot of (a).
To evaluate the measurement accuracy, a 3D profile measurement was performed the when the sample
was static. Appearance of the projected fringes on the inspected object is shown as Fig. 4(a). The
retrieved 3D profile and one-dimensional profile is shown in Fig. 4(b) and (c) respectively. To address
the measured errors, difference between two profiles is depicted as Fig. 5, in which the shifting
displacement has been compensated. Errors occurred at the edge of the object. The reason is that light
intensity varied fast on the boundary of the object.
In out setup, systematic accuracy for a static object was approximately 150m. The errors were mainly
from the spatially sampling density of the CCD camera and phase-extraction. The sampling resolution
was approximately 100m, which was determined by the field of view and the pixel numbers of the
CCD camera.
Fig. 5 Difference between the profiles in Fig. 3(b) and 4(c). Shifting displacement between the two
profiles has been compensated.
IV. conclusion
We have presented a discussion on how to retrieve the 3D shape from an image blurred by motion.
With the fringe projection method, objects moving within one period of the projected fringe can be
fully described. Thus, it is not necessary to avoid the blurred image. This makes it desirable to reduce
the cost of the detection system.
REFERENCES
[1] Thacker, N. A. and Mayhew, J. E. W.,“Optimal combination of stereo camera calibration
from arbitrary stereo images,”Image & Vision Comput., 9, 27-32 (1990).
[2] Lane, R. A., Thacker, N. A. and Seed, N L.,“Stretch-correlation as a real-time alternative to
feature-based stereo matching algorithms,”Image & Vision Comput., 12, 203-212 (1994).
[3] Takeda, M. and Mutoh, K., “Fourier transform profilometry for the automatic measurement of
3-D object shaped,” Appl. Opt. 22, 3977-3982 (1983).
[4] Srinivasan, V., Liu, H. C. and Halioua, M., “Automated phase-measuring profilometry of 3-D
difuse objects,” Appl. Opt. 23, 3105-3108 (1984).
[5] Su, W. H., “Color-encoded fringe projection for 3D shape measurements,”Opt. Express 15,
13167-13181 (2007).
[6] Su, W. H., “Projected fringe profilometry using the area-encoded algorithm for spatially
isolated and dynamic objects,”Opt. Express 16, 2590-2596 (2008).
[7] Zappa, E. and Busca, G.,“Comparison of eight unwrapping algorithms applied to
Fourier-transform profilometry,”Opt. Lasers Eng. 46, 106-116 (2008).
modes to low frequency modes. Speckles are then reduced by removing some IMFs which contain
higher frequency bands. Without using any predetermined filter wavelet function, it decomposes the
signal empirically bythe local characteristic scale of the data. The image’s detail structure is therefore 
not damaged by any artificial filter function. Recently, Bernini et al. [9, 10] employ the1-D EMD and
2-D EMD to reduce speckles in computer-simulated fringes. It has been shown that speckles on the
simulated fringes can be removed extensively by eliminating some IMFs. However, our simulation
showed that the number to removed IMFs is changed when the fringe period or the signal-to-noise ratio
varies. Efforts are still required to determine which IMFs that needs to be removed to eliminate
speckles without affecting the fringe structure.
To efficiently and robotically perform speckle-reduction by the EMD, we present an improvement
based on Bernini’s method. The improvement includes designing a computer generated signal to
precisely represent the interference pattern, and creating a database system from the simulation to
identify the required number of removed IMFs. Only 1-D EMD is required for processing. This makes
the computation algorithm simpler and saves enormous processing time.
II. Theory
The EMD decomposes a one-dimensional signal into a finite number of intrinsic mode functions
(IMFs), which is expressed as
(a) (b)
of the speckle image is shown in Fig. 1(a). Speckle noises are harmful to uniformity of the illumination.
Its distribution in frequency domain is shown in Fig. 1(b), in which the spectral density distributed over
the frequency domain. Figure 1(c) shows all the IMF components and the residue. Their corresponding
distributions in frequency domain are shown as Fig. 1(d). The EMD can therefore be treated as a type
of decomposition whose subbands are created empirically to separate the different components of the
signal
Fig. 2 The sifting process.
The IMFs are built up by means of the sifting process. Figure 2 depicts the flow diagram of the
sifting process. According to Huang et al. [8], an IMF must be satisfied with the following two
conditions: First, the decomposition assumes that the signal has at least two extrema –one maximum
and one minimum. The number of local extrema and the number of zero crossings must either equal or
differ at most by one. Thus the local maxima of the data set are always positive and the local minima
are negative, respectively. Next, the mean value of the envelope defined by the local maxima and the
Fig. 3. Probability density function. <I> = 1.
Our experiments have shown that most kinds of measured histograms can be approximately described
by Eq. (4), with a suitable parameter m. Consequently, finding the measured histogram of speckles
becomes a problem of searching for the parameter m. Once the parameter m is identified, N(x) can be
created according to Eq. (3).
Figure 4 illustrates an example to identify the parameter m. Appearance of a diffusive plate illuminated
by a coherent light source is shown as Fig. 4(a). A CCD camera with 10241360 pixels at 12-bit pixel
resolution was used to obtain this image. Histogram of this image is shown as Fig. 4(b). The mean of
the speckle images was <N> = 1439. In our case, the shape of histogram was fitted by Eq. (4) with
m = 6. Thus, the computer generated signal I(x) can be carried out by Eq. (2) and Eq. (3), with m =
6.
(a) (b)
Fig. 4. (a) Appearance of a diffusive flat plate illuminated with a coherent light source. (b) Histogram of
the speckle pattern.
If the illuminating power is spatially modulated with a sinusoidal function S(x), Eq. (2) then
becomes








N
xN
x
T
BA
N
xN
xSxI
O
)(
)
2
cos(
)(
)(
 , (5)
where A is the background intensity or dc level, B is the modulation amplitude, and TO is the period of
the fringes.
Fig. 6. Signal decomposed by the EMD.
(a)
S(0)(x) S(1)(x) S(2)(x) S(3)(x) S(4)(x) S(5)(x) S(6)(x)
Standard
deviation
SD(l)
589.8962 402.8619 322.3405 253.2666 206.4122 518.1671 696.8195
(b)
Fig. 7. Noise removal by the EMD: (a) various refined data S(l), and (b) statistically computed deviations.
the histogram of the speckles from the diffusive plate can be used to create the histogram function P(N).
Note that the histogram of speckles varied when the lens system or laser source is changed. In our case,
the gamma function with m = 3 could fairly describe the histogram of speckles P(N). Once the
parameter m was carried out, the database shown in Fig. 8 can be employed to identify the number of
removed IMFs.
The database shown in Fig. 8 suggests that l = 3 for To < 27, l = 4 for 27 < To < 40, and l = 5 for 40 <
To, corresponding to m = 3. Thus, with reference to the database, parts of the data points in S(3), S(4),
and S(5) were picked out to reassemble a noiseless data set. As shown in Fig. 10(a), data points enclosed
with dotted lines were used to form a noiseless signal. Figure 10(b) illustrates the reassembled
distribution. The reassembled signal fairly consists with the intensity distribution shown in Fig. 9(d).
Figure 11 illustrated the reassembled 2D image.
Fig. 9. Appearance of projected fringes on a ball when using (a) coherent illumination, and (b)
incoherent illumination. Intensity distribution for one image row corresponding to (a) and (b) is
shown as (c) and (d), respectively
Fig. 12. Phase extraction using the 5-step phase-shifting algorithm for (a) the reassembled image, and (b)
the image using coherent illumination. Phase distribution for one image row corresponding to (a)
and (b) is shown as (c) and (d), respectively.
Phases were evaluated using the five-step phase-shifting technique. The retrieved phase from the
reassembled data is shown as Fig. 12(a). Figure 12(b) shows the phase distribution extracted from Fig.
9(d). The phase-shifting technique involves the arctangent operation. Thus, the extracted phases have
principle values ranging from–and, and have discontinuities with 2phase jumps. Figure 12(c) and
12(d) show the phase distributions for one line of the images. It is found that phases can be extracted
with high accuracy once noise-reduction is performed by the proposed method.
VI. conclusion
A method based on the 1-D EMD to eliminate speckles on fringe patterns has been proposed. This
method employed a computer generated signal to describe the speckle-introduced fringes. The
histogram of the simulated signal was calibrated with that of the measured speckles. Thus, the
computer generated signal performed very well to address the characteristics of speckles. The computer
generated signal was decomposed into a number of IMFs. Speckle-reduction was then performed by
eliminating some IMFs. The simulation indicated that the number of removed IMFs was highly related
to the fringe period and the signal-to-noise ratio, or say, the parameter m. Thus, we created a database
from statistic simulation to identify the number of removed IMFs. With this database, speckles on the
fringe pattern can be efficiently and robotically reduced by the EMD.
REFERENCES
[1] Brooks R. E. and Heflinger, L. O.,“Moiré gauging using optical interference paterns,” Appl. 
Opt. 8, 935-939 (1969).
Speed sensing using projected fringe profilometry
Wei-Hung Su
Department of Material Science and Optoelectronic Engineering,
National Sun Yat-Sen University
Kaohsiung 804, Taiwan
ABSTRACT
An algorithm using fringe projection to perform the speed measurement is described. A fringe pattern
is illuminated onto the dynamic object, and a CCD camera is used to record the fringe distribution.
Fringes on the obtained image are deformed by the topography of the object, and also, blurred by
motion. Thus, the blurred fringes supply additional information to describe the speed of motion. Only
one shot measurement is required for data processing. This makes it possible to perform the speed
measurements with low environmental vulnerability.
Keywords: fringe projection, projected fringe profilometry, sinusoidal grating, speed sensing
I. INTRODUCTION
In visual systems using image processing, an expansion of the field of vision for dynamic objects is
greatly desired. Therefore, speed sensing for the moving objects has come to be an interesting topic in
computer vision. However, most algorithms [1-4] using a sequence of images or to analyze the speed
of motion can only cope with 2D problems. It is a challenge to identify the moving vector in a depth
axis.
A common resolution to inspect the speed of motion is performing the 3D shape measurements at a
sequence of time. Speed of motion is then addressed by the change in shapes or positions from the
sequent measurements. To obtain a sequence of images which are not blurred by motion, a high-speed
camera or stroboscopic illuminations are generally used. 3D shape sensing is then performed by
stereovision [5, 6], fringe projection [7, 8] or structured light illumination [9, 10]. These techniques
have been recognized as promising methods for non-contact and full-field measurements. However, the
computation cost for a sequence of measurements is relatively high, and the image registration [11] for
each pixel to identify the change in positions is a complicated task.
In this paper, an algorithm using fringe projection to perform the speed measurement is described. A
fringe pattern is illuminated onto the dynamic object, and a CCD camera is used to record the fringe
distribution. Fringes on the obtained image are deformed by the topography of the object, and also,
blurred by motion. Thus, the blurred fringes supply additional information to describe the speed of
Figure 1. Schematic setup of projected fringe profilometry.
The image sensor array obtains a blurred image within the exposure time t. Let the phase of the
blurred fringes be blurred. Then the intensity of the blurred image is an integration of exposure time,
and can be expressed as








 

2
2
),,(
22
cos),(),(
),(cos),(),(),(
ttt
ttt zx
yxyx
blurredblurred
o
o
dttyxZ
TT
x
tytxbRtytxaR
yxyxByxAMyMxI


,
(3)
where A(x, y) is the measured background intensity or dc level, B(x, y) is the measured modulation
amplitude, R(x, y) is the reflectivity of the measured object, and M is the magnitude of the imaging
lens.
For objects in which R(x, y) varies slowly with x and y, Eq. (3) can be expressed as








 
2
2
blurred ),,(
22
cos),(),(),(
ttt
ttt zx
o
o
dttyxZ
TT
x
yxbRyxaRMyMxI

.
(4)
Substituting Eq. (2) into Eq. (4), intensity of the blurred image is then simplified as





  ),(22cos)(sinc),(),(),(blurred yxZTT
x
ttyxbRtyxaRMyMxI o
zx

,
(5)
where
zzyxo TyxZ /])(),([  
 , and sinc(x) = sin(x)/(x).
Compared Eq. (3) with Eq. (5), Zo(x, y) can be fully identified from the blurred fringes, as given by
x
T
T
yx
T
yxZ
x
zz  ),(
2
),( blurredo 
. (6)
Thus, the 3D shape of the dynamic object at t = to can be directly retrieved from the blurred fringe.
III.speed detection for a dynamic object
As depicted in Eq. (5), the modulation amplitude is expressed in terms of speed. This makes it
desirable to obtain the moving velocity from the fringe contrast. The quality of fringe contrast can be
described as visibility, which is given by [12]
minmax
minmax),(
II
II
yxv

 , (7)
where Imax and Imin are the intensity corresponding to the maximum and adjacent minimum, and (x, y) is
a data point located within that specific fringe.
For objects in which the reflectivity varies slowly with x and y, Eq. (7) can further be simplified as
Figure 2(a) shows the fringe distribution, in which the exposure time was 4.0 second. Fringes were
blurred by linear motion.
Figure 2. (a) Fringes on the inspected object observed by a CCD camera when the object was shifting
along a specific direction. (b) Phase distribution of the dynamic object. A gray-level bar is used to
address the phase values.
Phase-extraction was performed with the Fourier transform method [7]. Figure 2(b) shows the
computed phases blurred, which were within the interval between–and. Unwrapping was an
inevitable procedure to eliminate the discontinuities. In our experiment, we use Goldstein’s algorithm
[13] to restore the absolute phases.
With Eq. (6), the profile Zo(x, y) was determined. Figure 3(a) shows the retrieved profile. Its
one-dimensional profile is shown in Fig. 3(b). Errors occured at the edge of the object. The reason is
that light intensity varied fast on the boundary of the object.
Once Zo(x, y) was retrieved, Zo(x, y) could be carried out. In our system, the discrete functions
),( )()(o ii yxZx
 and ),( )()(o ii yxZy
 were represented as ),1(),( )()(o)()(o iiii yxZyxZ 
and )1,(),( )()(o)()(o  iiii yxZyxZ , respectively. There were 24 data points chosen to identify the
velocities.
With Eq. (12), the computed velocities werex =0.62 mm/sec,y = 0.63 mm/sec andz = 0.12 mm/sec.
[4] Jin, T. S., Park, J. W. and Lee, J. M.,“Trajectory generation for capturing a moving object in
predictable environments,”JSME International J. A 47(2), 722-730 (2004).
[5] Thacker, N. A. and Mayhew J. E. W.,“Optimal combination of stereo camera calibration from
arbitrary stereo images,”Image & Vision Comput., 9, 27-32 (1990).
[6] Lane, R. A., Thacker, N. A., and Seed, N. L.,“Stretch-correlation as a real-time alternative
to feature-based stereo matching algorithms,”Image & Vision Comput., 12, 203-212 (1994).
[7] Takeda, M. and Mutoh, K.,“Fourier transform profilometry for the automatic measurement of
3-D object shaped,” Appl. Opt. 22, 3977-3982 (1983).
[8] Srinivasan, V., Liu, H. C. and Halioua, M.,“Automated phase-measuring profilometry of 3-D
difuse objects,” Appl. Opt. 23, 3105-3108 (1984).
[9] Su, W. H., “Color-encoded fringe projection for 3D shape measurements,”Opt. Express 15,
13167-13181 (2007).
[10] Su, W. H., “Projected fringe profilometry using the area-encoded algorithm for spatially
isolated and dynamic objects,”Opt. Express 16, 2590-2596 (2008).
[11] Gonzalez, R. C. and Woods, R. E., “Digital Image Processing,”Addison-Wesley Inc., New York, 302
(1993).
[12] Kreysig, E. “Advanced Engineering Mathematics,”John Wiley & Sons Inc., New York, Chap. 6, (1999).
[13] Zappa, E. and Busca, G., “Comparison of eight unwrapping algorithms applied to 
Fourier-transform profilometry,” Opt. Lasers Eng. 46, 106-116 (2008).
produced due to the low pixel resolution of the LC-SLM, the nonlinearity of the signal response, and
the variation of the intensity when the phase modulation was performed.
The above techniques were originally developed to cope with 2-D image problems. For a 3D shape
sensing system, it is still necessary to extend the DOF. For example, a 3D shape sensing system that
uses microscopes to describe the profile of a dynamic MEMs device requires a large depth measuring
range. Fourier transform profilometry [8] is a desirable tool to perform this kind of measurements.
Unfortunately, there is no well-developed scheme to extend its depth measurement range. Our previous
work has shown that supercontinuum illumination can be employed to extend the DOF of the fringe
projection system [9]. However, this approach can only be applied to objects with large-scale size.
When the microscope is employed for detection, it is still required to develop an approach to enlarge
the DOF for the image acquisition system.
In this paper, we investigate an approach based on the LC-SLM to enlarge the depth measuring range
for Fourier transform profilometry. A microscope combined with a wide-angle eyepiece lens is
employed to project a fringe pattern onto the inspected surface. At a different viewpoint, a CCD
camera observes the projected fringes through another microscope and a phase mask. Phases of the
fringes are extracted by the Fourier transform method [8]. With proper calibration approaches [10, 11],
depth information can be identified from the phases of the fringes. The phase mask enlarges the depth
of field of the image acquisition system, while the wide-angle eyepiece lens increases the depth of
focus of the fringe projection system. It is found that the depth measuring range could be extended up
to 1600m, even though the depth of field of the image acquisition system is only 80m.
Compared with 2D image problems, the depth measuring range has been extensively enlarged. The
reason is that the mask strength reduces the point-spread function at high frequencies, but not very
much at the low frequencies. Fortunately, the frequency of the projected fringes is generally not very
high. Phase of the projected fringes can therefore be extracted accurately even though the strength of
the phase mask is increased. Thus, a non-scanning, full-field 3D shape sensing system with a very large
depth measuring range is realized.
II. Theory
2.1 Extending the Depth of Field using the cubic phase mask
An incoherent optical system is depicted as Fig. 1. A planar object with intensity Io(xo, yo) is placed a
distance do in front of the projection lens. At a distance di behind the projection lens there appears a
defocused image Ii(xi, yi). A pupil function P(x, y) is placed on x-y plane.
where {} denotes the Fourier transform operation. Our purpose is to design a specific pupil function
so that H is not sensitive to misfocus, i.e.,
)0,,(),,( yxyx ffff HH  . (6)
In such a case, the object function Io(xo, yo) can be retrieved by de-convoluting the misfocused image
Ii(x,), as mathematical expressed as















 
)0,,(
)},,({
),,(
)},,({
),( 11
yx
iii
yx
iii
ooo ff
yxI
ff
yxI
yxI
HH


 , (7)
where {}1 denotes the inverse Fourier transform operation. It has been shown that H can be
insensitive to misfocus if the pupil function is given by [1]


 
otherwise,0
2,)](exp[),(
33
2
1 Lxyxj
yxP

,
(8)
whereis a constant value. The OTF becomes





















0if,1
0if)],(
3
exp[)](
4
exp[
1
12
]2)2/()2/(exp[
]2)2/()2/(exp[
),,(
2
33
33
33
yx
yxyxyx
yx
yx
yyyyyy
xxxxxx
yx
ff
ffffjffj
ff
dd
ffjfj
ffjfj
ff







H
.
(9)
For a large vale of, bothfx andfy are much smaller than. Eq. (9) can then be represented as








0if,1
0if)],(
4
exp[
1
12)0,,(),,(
33
yx
yxyx
yxyxyx
ff
ffffj
ffffff



 HH
.
(10)
Consequently, large values ofminimize the sensitivity of misfocus.
An example of the pupil function is depicted as Fig. 2, in which theis 40.
Fig.4. A schematic diagram for phase modulation.
The LC-SLM modulates the phase between 0 and 2at 532nm [14]. Thus, phase of the pupil function
should be wrapped with the 2modulo operation. The residue of Fig.3 after the 2modulo is shown in
Fig. 5. This pattern is then used to be the input video signal. Unfortunately, phase modulation is not
linearly responded by the input signal. Light intensity is varied by the input signal, too [13]. Errors are
therefore produced while using the pattern shown in Fig.5 to generate a cubic phase mask.
Fig. 5. Distribution of the input video signal. signals were mapped to the corresponding colors: the
brighter one represented the stronger signal.
A simple way to produce a cubic phase mask with a suitable value is illustrated as Fig. 6. A white
LED was selected as the incoherent light source. Lightwaves were launched into a pin hole or an
optical fiber to generate a point source. Its OTF on the image sensor array should be similar to those
shown on Fig. 2. For a suitable value of, the shape of impulse response was not sensitive to misfocus.
However, even though the DOF has been extended, the low pixel resolution of the LC-SLM always
damages the object’s fine details. A detail analysis is described in Section 4.2.
In our system, phases of the projected fringes were evaluated by Fourier transform method [8].
Unwrapping was then performed to reconstitute the continuous phase distribution. We usedGoldstein’s
algorithm [17] to restore the absolute phases.
Correspondences between the world coordinates and the unwrapped phases are expressed as











01
01
0
bzby
azax
cz n
N
n
n , (11)
where ai, bi, and ci are undetermined parameters. Our previous works have described the way to
identify these parameters [10, 11]. The depth position z can be found out from the unwrapped phase.
Then the corresponding transverse position can be carried out by the depth value.
IV. Experiments
An object with size of 2000m2000m was selected as an inspected sample. On the in-focus area, the
field of view of the image acquisition system was approximately 400m400m. The magnitude of the
microscopes was approximately 10. Appearance of the fringes projected onto the tilted sample with its
focus on the central region is illustrated as Fig. 8(a). The fringe contrast decreases quickly with the
defocus introduced by the tilt. The restored image using the phase mask is shown as Fig. 8(b).
(a) (b)
Fig. 8 (a) Appearance of the projected fringes on the tilted object. (b) Retrieved image.
[6] Ojeda-Castaneda, J. and Berriel-Valdos, L. R.,“Zone plate for arbitrarily high focal depth,” 
Appl. Opt. 29, 994-997 (1990).
[7] Hong, D., Park, K., Cho, H. and Kim M, “Flexible depth-of-field imaging system using a
spatial light modulator,” Appl. Opt. 46, 8591-8599 (2007).
[8] Takeda, M. and Mutoh, K.,“Fourier transform profilometry for the automatic measurement of
3-D object shaped,” Appl. Opt. 22, 3977-3982 (1983).
[9] Körner, K., Windecker, R., Fleischer, M. and Tiziani, H. J., “One-grating projection for
absolute three-dimensional profiling,” Opt. Eng. 40, 1653-1660 (2001).
[10] Windecker, R. Fleischer, M. Körner, K. and Tiziani, H. J.,“Testing micro devices with fringe 
projection and white-light interfeometry,”Opt. Lasers in Eng. 36, 141-154 (2001).
[11] Su, W. H., Liu, Z., Shi, K., Wang, B., Reichard, K. and Yin, S. “ A large depth of field
projected fringe profilometry using a supercontinuum light ilumination,” Opt. Express 13, 
1025-1032 (2005).
[12] Liu, H., Su, W. H., Reichard, K. and Yin, S. “Calibration-based phase-shifting projected
fringe profilometry for accurate absolute 3D surface profile measurement,” Optics
Communications 216, 65-80 (2003).
[13] Su, W. H., Kuo, C. Y., Wang, C. C. and Tu, C. F., “Projected fringe profilometry with 
multiple measurements to form an entire shape,” Opt. Express 16, 4069-4077 (2008).
[14] Jackson, L. B.,“Digital Filters and Signal Processing,”Toppan, (1996).
[15] Goodman, J. W.“Introduction to Fourier Optics,”Roberts & Company, Colorado (2005).
[16] Lu, K. and Saleh, B. E. A., “Theory and design of the liquid crystal TV as an optical spatial 
light modulator,” Opt.  Eng. 29,240–245 (1990).
[17] Zappa, E. and Busca, G., “Comparison of eight unwrapping algorithms applied to 
Fourier-transform profilometry,” Opt. Lasers Eng. 46, 106-116 (2008).
98年度專題研究計畫研究成果彙整表 
計畫主持人：蘇威宏 計畫編號：98-2221-E-110-018- 
計畫名稱：以顯微鏡為載體之非掃瞄式形貌量測技術 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 1 100% 
篇 
個人很榮幸擔任
2010 光 電 年 會
OPT4, Oral 
Session 4 之會議
主持人 
論文著作 
專書 0 0 100%   
申請中件數 2 1 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 4 2 100%  
研究報告/技術報告 0 0 100%  
研討會論文 6 2 100% 
篇 
個人很榮幸擔任
2010 SPIE 
Conference 7781, 
Room: Conv. Ctr. 
19 Session2 之會
議主持人 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
