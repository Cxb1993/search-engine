 I
摘要 
有別於過去的分散式計算、網格計算和平行計算，雲端計算似乎可說是 IT 技術不斷發展下的產物，
它是種共享的網路訊息交換服務的模式，使用者看到的只有服務本身而不用考慮關於此服務更底層的
一些基礎設施。過去有不少文獻探討關於虛擬機器與網格計算，而隨著科技的進步，如何針對雲端計
算之資源有效利用與虛擬機器之負載平衡，發展一套合適的解決方案將是相當重要的議題。 
本計畫發展一適應性動態資源配置調整以及負載平衡機制，使虛擬主機中的負載平衡伺服器，能
根據可用資源分布、預測器之準確率以及待處理之任務之工作量，來動態調整進行工作轉移，並配置
合適資源之虛擬主機予該工作，從而改善以往使用靜態負載平衡工作分配不均之問題；接著，預測虛
擬主機環境能有效控制系統狀態，進而減輕其負擔及節省整體計算之頻寬；最後，預測訊息傳輸之行
為，以減少環境整體記憶體之浪費，增加支援雲端計算技術之可擴性，並針對使用者端提出一低耗能
之解決方案。 
效能評估方面將探討資源配置調整之準確性以及程式執行之效能，並於計畫中實現環境的可靠性
及方便使用性。我們期望能藉此提升電腦資源的利用，在支援雲端計算之虛擬主機環境中，針對工作
轉移及資源配置議題有更佳的解決方案。 
 
關鍵詞：虛擬機器、平行計算、雲端計算環境、工作轉移、負載平衡 
 1
一、前言 
在高速網路、分散式系統及多核心技術的蓬勃發展下，網際網路的資源分享可透過雲端計算與虛
擬機器監視器(VMM; Virtual Machine Monitor)結合的方式來實現。雲端計算開發者及工程師可以致力於
下列四個項目：(1)大規模的組織化之資源分享；(2)安全且可靠的存取；(3)有效使用資源的負載平衡機
制；(4)提高各種虛擬主機與虛擬主機之間及虛擬主機對客戶端之間應用程式的相容性。過去有不少文
獻探討關於虛擬機器與網格計算方面的議題，但隨著科技的進步雲端計算之資源有效利用的發展與虛
擬機器負載平衡方面，如何有一套合適的解決方案亦是可加以探討的主題。 
因此本計畫針對目前虛擬化技術在動態運算資源配置方面機制的研究及實作，修正過去技術的缺
點，並考量能源消耗的議題，建立一套具有適應性資源使用預測及負載平衡機制之虛擬主機環境以支
援雲端計算。由於多核心技術及高速網路技術的發展，如何充分發揮多核心處理器的效能，在雲端計
算環境中採用動態調整資源配置的架構來改善整體效能似乎是種可行的解決方案，但實際要建置在多
個虛擬主機上，卻又存在著溝通上必須面臨的挑戰。 
在提高雲端計算環境資源使用度之後，雖可以平衡 CPU 的負載工作與記算環境資源使用率，但能
源消耗亦是另一可提高更多可用資源的方案，實務上，如何設計一個低成本、低耗能及高可靠度之雲
端計算環境架構便成了未來研究方向的重點。由於具備多核心處理器的電腦日漸普及，設計上將不再
只侷限於過去單核心處理器的方式與記憶體架構，若能以過去網格計算及平行計算的機制為基礎加以
整合，以達到整體效能提升的目標。 
二、研究目的 
本計畫發展一適應性動態資源配置調整以及負載平衡機制，使虛擬主機中的負載平衡伺服器，能
根據可用資源分布、預測器之準確率以及待處理之任務，動態調整工作排程演算法，分配適當工作量
予各硬體資源，以大幅改善以往使用靜態負載平衡，而造成各處理器工作分配不均之問題；其次，預
測虛擬主機環境能有效控制系統狀態，進而減輕其負擔及節省整體計算之頻寬；第三，預測訊息傳輸
之行為，以減少環境整體資源之浪費，增加雲端計算技術之可擴性；最後在計畫中的效能評估部分，
將探討資源配置調整之準確性以及程式執行之效能。 
我們所規劃建立的支援雲端計算之虛擬主機環境，具備動態平衡負載機制，而為了能順利讓工作
轉移，系統將能為此加入相對應的機制，以確保工作狀態能完整轉移，並且導入回復機制以確保工作
順利完成。此外也將針對節能解決方案、資源的使用效率以及多個核心之間的溝通進行最佳化。有效
地利用處理器運算能力，不造成各種軟硬體資源浪費，一直是分散式處理的重要議題，依據預測器提
供之硬體資源使用率適當分配至各個合適的虛擬主機上來進行處理(可能包含多台電腦主機的硬體資
源)，達到高效能的計算效果；此外，針對雲端計算資源使用預測機制，更能減少頻寬、記憶體使用之
浪費，以減輕各個計算環境硬體資源之負擔，並增加雲端計算技術之可擴性；最後，研究各個與雲端
運算有關之參數，對虛擬主機環境運行效能之影響，作為評估本計畫環境之可行性與必要性的重要指
標。本計畫藉此提升電腦資源的利用，在支援雲端計算之虛擬主機環境中，針對工作轉移及資源配置
議題有更佳的解決方案。 
三、文獻探討 
多核心處理器的電腦近年來亦日漸普及，而原來在單一核心處理器環境下執行的大部分程式卻無
法在多核心處理器下發揮應有的執行效率。平行計算及多核心處理器計算環境之程式語言發展，以提
升平行計算及多核心處理器計之可程式度，乃成為國內外一重要之研究課題。在本計畫中，我們將發
 3
3.3 虛擬機器遷移 
虛擬機器的遷移主要的概念是把實體機器上的 VM 移轉到另一台實體機器上（如圖 1），並且能正
常運作，通常是為了解決故障管理和負載平衡等問題。遷移分成兩種方式：離線（offline）遷移和動態
（live）遷移。執行離線遷移時，如果 Guest OS 還在運行中，必須將 VM 中斷，系統狀態會被暫停(pause),
才進行遷移，在執行遷移完成前 VM 上的 Guest OS 是無法提供任何服務，直到轉移到另外一台實體主
機上。而動態遷移程序上比離線遷移較複雜些，首先複製本地端實體主機與目的端實體主機兩台記憶
體不同的地方，此時 VM 還是可以在本地端上運作，直到兩台的主機相差的部份極小，是被可容許的
範圍，在此時才會將 VM 關機並轉移到目的端主機上，而在最後轉移所暫停的時間極短不會讓使用者
感覺到。以上可知離線遷移的好處是遷移速度較快，但是 Guset OS 必須暫停，而動態遷移由於可以在
Guest OS 運行的狀態下遷移，但是由於在運行中的 Guest OS 記憶體內的資料會改變，所以在整體遷移
速度上會比離線遷移還要慢。 
 
圖 1. 虛擬機器遷移 
3.4 CPU 熱抽換 
現行的作業系統啟動時就已經把 CPU 與記憶體的資訊載入作業系統中，因此作業系統是無法讀取
到在開機後才變動的 CPU 與記憶體。在 Linux 2.6 的版本以後支援 CPU 在啟動後的作業系統中增減，
即 CPU 熱抽換（hot-plug），熱抽換可分為邏輯上的熱抽換與物理上的熱抽換。物理上的熱抽換是指直
接對硬體在進行增加/移除設備。這邊使用的是邏輯上的熱抽換，原理為更變作業系統內部的設備資
訊，作業系統會依照更變後的設備資訊作設備上的變更。 
目前 CPU 的熱抽換技術可以實現熱添加（hot-add）與熱刪除（hot-remove）。其原理是先給予作業
系統足夠的 CPU 數量，作業系統啟動後會讀取到 CPU 資訊的系統文件，而 CPU 在 Linux 的狀態分為
online 與 offline 兩種，作業系統的 CPU 排程會根據此狀態將工作排給 CPU。在移除/新增 CPU 方面，
不是直接拔出會插入 CPU 於硬體，而使用邏輯上的增減 CPU。方法是修改先前所提的系統文件，以調
整 CPU 的 offline/online 狀態，最後作業系統會立即根據跟改後的系統文件作變動，因此會將系統內的
CPU 作關閉或是開啟的動作。 
 
 5
區域網路將資訊傳給此模組的主機端，主機端則接收客戶端所傳的資訊，並將資訊交由資料分析模組
進行後續判斷。 
4.2 本計畫之動態調整與判斷流程 
資料分析模組會收集監視模組收集的 vCPU 使用資訊，並且依據判斷規則決定 VM 資源得調整動
作。判斷規則如下： 
1. 監視每個 VM 的 UvCPU 以及 M 值(多核心的平行化程度)。 
2. 如果 UvCPU大於 90%並 M 值等於 1 時，而且持續次數為 5 次，跳到步驟 4，若持續次數小於 5 時
跳至步驟 1；若未達 UvCPU大於 90%或 M 值等於 1 則到步驟 8。 
3. 若 vCPU 已開啟數達到了上限則到步驟 1，反之則到步驟 4。 
4. 如果主機上可對應 vCPU 的 CPU 數量小於 1，跳至步驟 5；若數量大於等於 1，則到步驟 7。 
5. 檢查主機是否已有 VM 正在作動態遷移，若正在進行動態遷移，返回步驟 1。若無進行動態遷移，
則到步驟 6。 
6. VM 會遷移到 CPU 數大於此 VM 的實體機器，CPU 使用率越低者則為遷移目標，並且執行動態遷
移，當遷移完成，則進行步驟 7。 
7. VM 增加一個 vCPU，並且到步驟 1。 
8. 如果 UvCPU小於 50%並 M 值等於 0 並且 vCPU 數大於 1 個，持續次數為 5 次，跳到步驟 9，若持
續次數未達 5 次，跳至步驟 1。若 UvCPU無小於 50%或 M 值非等於 0 或是 vCPU 數為 1 個，則到
步驟 1。 
9. VM 減少一顆 vCPU，返回步驟 1。 
以上的判斷規則中的 90%與 50%以及計數器中的持續 5 次為實驗推導；當監視模組蒐集的數據經
過資料分析判斷後，若有需要調整的 VM，則會交由調整模組進行動態調整。調整模組所採用的動態
調整方法是熱抽換與虛擬機器動態遷移技術，在遷移方面不須暫停 VM，因此不會影響運行狀況。當
VM 的 vCPU 數需要增加/減少的情況時，調整模組會採取熱抽換方式調整 vCPU，方法是由系統透過
區域網路對 VM 下命令，修改 VM 中的系統文件，開啟/關閉 vCPU，Guest OS 的排程器會根據此文件
作調整，決定工作的排程。由於方法是修改完文件後即可讓 Guest OS 存取新的資訊，動態調整 vCPU
的動作很快就能完成。若遇到 VM 需要增加 vCPU 數，但是主機上的 vCPU 已經被其他 VM 的 vCPU
給對應完了的情況，則會將此 VM 動態遷移到另一台主機並增加 vCPU。動態遷移後的 VM 與先前的
VM 資源配置是無法更動的，因此 vCPU 部分的配置還是使用熱抽換調整 vCPU 數，在進行動態遷移
的 VM，可以不必關機並且繼續執行它本身的工作，直到本地端的記憶體頁面資訊完全移到目的端的
主機後才會開啟在目的端 VM 並暫停本地端的 VM。 
五、結果與討論 
5.1 實驗結果與分析 
本計畫實驗環境使用 4 核心 Intel(R) Core 2 Quad Q6600 2 台，每個核心時脈為 2.40 GHz，記憶體
4 Gbyte，實體主機總共有 2 台(PM1 與 PM2)。作業系統使用 Linux Ubuntu 10.10；VM 採用 KVM 全虛
擬化的環境，初始為 1 個 vCPU，最多可開啟 4 顆 VCPU，作業系統使用 CentOS 5.4。實驗使用 NPB 3.3
版本，其中因運作方式分為 MPI、OpenMP 和 Serial 版，我們採用的是 OpenMP。OpenMP 為使用共享
式記憶體的平行程式，用於 C、C++和 Fortran 程式語言。NPB 程式中我們採用其中的平行程式 EP
（Embarrassing Parallel benchmark），EP 是作數學函數的浮點運算，特性是 CPU 之間的訊息傳輸很少，
偏重在 CPU 計算上。 
EP benchmark 為 CPU bound 的平行運算程式，測試當 VM 需求更多 CPU 運算時，在各種不同狀
 7
 
5.2 結論 
傳統的虛擬環境下，並不能在虛擬機器運作時變動系統資源配置。但是虛擬機器若需要更多資源
來執行程式或是有許多資源處於閒置狀態，若要調整 VM 資源配置就必須關閉 VM 才能。本計畫以現
有的 KVM 虛擬化環境下，提出自動化動態虛擬 CPU 資源管理機制，管理多個實體主機上面正在運行
的虛擬機器。在管理系統啟動後，會開始監視已開啟 VM 的 vCPU 的使用率資訊，如果系統發現 VM
的 vCPU 資源需求更多時，系統會動態增加 VM 的 vCPU 數以提升 VM 效能。若偵測到 vCPU 使用率
很低時，動態減少 vCPU 以便其他的 VM 能使用，可以提升硬體資源使用率。經由實驗證明，使用本
計畫提出的機制，在執行 CPU 高密集運算程式時，CPU 需求量很大，因此透過系統動態增加虛擬機器
的 vCPU，可以有效地提升整體雲端計算環境之效能。 
參考文獻 
[1] J. Che, Q. He, Q. Gao and D. Huang, "Performance Measuring and Comparing of Virtual Machine 
Monitors," International Conference on Embedded and Ubiquitous Computing, 2008 (EUC '08. 
IEEE/IFIP), pp. 381-386, 2008. 
[2] D.W. Cheung, S.D. Lee and Y. Xiao, "Effect of Data Skewness and Workload Balance in Parallel Data 
Mining," IEEE Trans. Knownl. Data Eng., Vol.14, No.3, pp. 498-514, Dec. 2002. 
[3] K. Czajkowski, I. Foster and C. Kesselman, , "Co-allocation Services for Computational Grids," in Proc. 
8th IEEE Symposium on High Performance Distributed Computing, 1999, IEEE Press. 
[4] K. Czajkowski, I. Foster, N. Karonis, C. Kesselman, S. Martin, W. Smith, S. Tuecke, "A Resource 
Management Architecture for Metacomputing Systems," Proc. IPPS/SPDP'98 Workshop on Job 
Scheduling Strategies for Parallel Processing, pp. 62-82, 1998. 
[5] K. Czajkowski, S. Fitzgerald, I. Foster, C. Kesselman, "Grid Information Services for Distributed 
Resource Sharing," Proceedings of the Tenth IEEE International Symposium on High-Performance 
Distributed Computing (HPDC-10), IEEE Press, August 2001. 
[6] E. Elmroth and L. Larsson, "Interfaces for Placement, Migration, and Monitoring of Virtual Machines in 
Federated Clouds," International Conference on Grid and Cooperative Computing, 2009 (GCC '09), pp. 
253 – 260, 2009. 
[7] I. Foster and N. T. Karonis, "A Grid-Enabled MPI: Message Passing in Heterogeneous 
DistributedComputing Systems," SC'01, IEEE, 2001. 
[8] I. Foster, "The Grid: A New Infrastructure for 21st Century Science.", Physics Today, 55(2):42-47, 2002. 
[9] F. Freitag, J. Corbalan and J. Labarta. "A dynamic periodicity detector: Application to Speedup 
Computation," in Proceedings of International Parallel and Distributed Processing Symposium (IPDPS 
2001), 2001. 
[10] O. H. Ibarra and C. E. Kim, "Heuristic Algorithms for Scheduling Independent Tasks on Nonidentical 
Processors," Journal of the ACM, 24(2):280-289, 1977. 
[11] S. Jenks and J.L. Gaudiot, "A Multithreaded Runtime System With Thread Migration for Distributed 
Memory Parallel Computing," in Proceedings of High Performance Computing Symposium, 2003 
Advanced Simulation Technologies Conference, Orlando, FL, 2003. 
[12] S. Kamitsuji and R. Shibata, "Effectiveness of Stochastic Neural Network for Prediction of Fall or Rise 
of TOPIX", Research Paper, Keio University, 2004. 
 9
[29] B. Zhang et al., “Resource management policy for cloud testbed of China railway,” International 
Conference on Computer Application and System Modeling (ICCASM), pp. v4-375 - v4-379, 2010. 
[30] C. Clark et al., “Live Migration of Virtual Machines,” NSDI'05 Proceedings of the 2nd conference on 
Symposium on Networked Systems Design & Implementation, 2:273-286, 2005. 
[31] J. Brandt et al., “Combining Virtualization, Resource Characterization, and Resource Management to 
Enable Efficient High Performance Compute Platforms Through Intelligent Dynamic Resource 
Allocation, ” 2010 IEEE International Symposium on Parallel & Distributed Processing, Workshops and 
Phd Forum (IPDPSW), pp. 1-8, 2010. 
[32] J. Che et al., "Performance Measuring and Comparing of Virtual Machine Monitors," IEEE/IFIP 
International Conference on Embedded and Ubiquitous Computin( EUC '08), pp. 381-386, 2008. 
[33] Q. Li et al., "Adaptive Management of Virtualized Resources in Cloud Computing Using Feedback 
Control," International Conference on Information Science and Engineering (ICISE), pp. 99-102, 2009. 
[34] Y. Li et al., "A Survey of Virtual Machine System: Current Technology and Future Trends," 2010 Third 
International Symposium on Electronic Commerce and Security, pp. 332-336, 2010. 
[35] Z. Shao et al., "PMonitor: A Lightweight Performance Monitor for Virtual Machines," First International 
Workshop on  Education Technology and Computer Science, ( ETCS '09), pp. 689-693, 2009. 
[36] Kernel documentaion /cpu-hotplug, http://www.mjmwired.net/kernel/Documentation/cpu-hotplug.txt. 
Ubiquity on Learning Objects”、以及由澳洲 University of Tasmania 之 Byeongho 
Kang 教 授 主 講 “Push in Web Information Management: Limitations and 
Possibilities”。筆者於 4 月 13 日從台北飛韓國首爾再轉赴大田韓南大學 (Hannam 
University)參加本次研討會，並發表論文  “An Adaptive Embedded Multi-core 
Real-time System Scheduling”。 
 
二、 與會心得   
    此次研討會由 570 篇投稿論文中選出 86 篇(接受率約 15%)，計有八個領域分
成十二個組別討論及論文發表。由於研討會幾乎涵蓋普及計算與多媒體應用方面
各研究領域，筆者主要從事平行與分散式計算、雲端計算以及嵌入式系統方面之
研究，除參與研究相關場次之研討外，亦參與其他領域之研討，在會場與多位學
者認識，相互切磋獲益良多，咸認雲端計算為普及計算重要之一環，實為資訊科
技之下一次大變革。此外，大會議程主席韓南大學 Tai-hoon Kim 教授亦親自引導
介紹該校，韓南大學成立於 1956 年，為一基督教私立大學，現有兩個校園在大
田市並有 7 個研究所，10 個學院，十分重視國際交流。 
 
三、攜回資料名稱及內容 
    參加本次會議一共攜回下列資料： 
1. The 2nd International Conference on Ubiquitous Computing and Multimedia 
Applications (UCMA 2011) Conference Program 一冊。 
2. The 2nd International Conference on Ubiquitous Computing and Multimedia 
Applications (UCMA 2011) Pre-Proceeding一冊 (Conference Proceeding 收
錄在CCIS (Communications in Computer and Information Science)內將另於
會後郵寄。 
T.-h. Kim et al. (Eds.): UCMA 2011, Part I, CCIS 150, pp. 263–272, 2011. 
© Springer-Verlag Berlin Heidelberg 2011 
An Adaptive Embedded Multi-core Real-Time System 
Scheduling 
Liang-Teh Lee1, Hung-Yuan Chang1,2, and Wai-Min Luk1 
1
 Dept. of Computer Science and Engineering, Tatung University, Taiwan 
2
 Dept. of Electronic Engineering,  
Technology and Science Institute of Northern Taiwan, Taiwan 
ltlee@ttu.edu.tw, hychang@tsint.edu.tw, charis.mimi@gmail.com 
Abstract. Due to the popularity of multi-core systems in recent years, it is 
important to design a practical multi-core scheduling for multi-core system, to 
improve the performance of the embedded multi-core real-time system. The 
majority of well-known real-time scheduling algorithm is applying priority-
driven, to ensure hard real-time tasks to be completed before their deadlines, 
and to service soft real-time tasks using remaining time. The proposed adaptive 
scheduler, an adaptive scheduling algorithm for embedded multi-core real-time 
systems, can dynamically assign task and cross over global and local scheduler 
well, and take into account both of soft and hard real-time tasks. Moreover, by 
established a special non-preemptive region, the hard-heavy tasks can execute 
independently, so as to decrease the context switch overhead and improve the 
system performance. The experiment results show that, compared with other 
multi-core schedulers, the proposed method can get better results.  
Keywords: embedded, multi-core, real-time system, scheduling. 
1   Introduction 
In recent years, multi-core system has become more and more important, because in 
comparison with single-core system, with the same performance requirements, multi-
core systems have the advantages of low power consumption, low-cost properties and 
allowing instruction-level parallel processing. In environment of embedded systems, 
now also face the need to apply to a wide range of multimedia software integration. 
Even if most of single-core embedded systems often offer some special IP, or speed 
up by using DSP, the single-core system performance still remains a major bottleneck 
that can not be able to cope with more complex applications.  
The embedded systems will progress toward multi-core, multi-threading trends. 
With the gradual popularization of embedded multi-core systems, it can not achieve a 
satisfactory performance if only using single-core scheduling rules. It is important to 
design a practical multi-core scheduling method for multi-core system, to improve the 
performance of multi-core system effectively. 
Real-time scheduling algorithms generally can be classified into non-priority-
driven algorithm and priority-driven algorithm. A general real-time scheduling 
algorithm strategy already have not been unable to satisfy the situation that an hybrid 
 An Adaptive Embedded Multi-core Real-Time System Scheduling 265 
2.2   Traditional Schedule Algorithm 
Scheduler manages the execution order of tasks in the system. Considering a real-time 
system, assume that there are p cores, and a periodic task set N = {n1, n2, …, nm}. A 
task ni has its period Ti, computation time Ci and deadline Di, will be placed in a ready 
queue until the scheduler allocates it to the core for execution. The well-known 
scheduling algorithms for real-time systems are EDF (Earliest Deadline First 
Scheduling) and H-EDF (Hierarchical-EDF) algorithm, respectively. We will also 
discuss the advantages and shortcomings of these scheduling algorithms and provide 
some guidelines to construct the proposed scheduling algorithm. 
EDF scheduling algorithm is the most famous and efficient in real-time operating 
systems. In the single-core system, EDF is an optimal scheduling algorithm [4]. It is a 
dynamic priority scheduling algorithm; the task priorities are not fixed but can be 
changed depending on the closeness of their absolute deadlines. In such a framework, 
we assume that there is a set of m periodic tasks as mentioned previously, the relative 
deadline of each task is equal to its period. If the total utilization of the task set is no 
greater than 1, the task set can be feasible scheduled on a single-core system by the 
EDF algorithm, as shown in the equation (1): 
1≤∑
T
C
=U
m
1=i i
i
 (1)
Where task ni is with computation time Ci and the period Ti. The above equation is 
also implying that maximum utilization of the system is 1. In other words, if EDF 
cannot schedule a task set on a single-core system, there are no other scheduling 
algorithms can [1][7]. However, the algorithm with the best performance in the 
single-core platform, cannot assure to play a good role in the multi-core platforms. It 
is also well known that optimal scheduling for multiprocessor or multi-core system is 
an NP-Hard problem [2]. 
AS (Adaptive Scheduling) algorithm belongs to one of H-EDF algorithms that a 
simple hierarchical scheduling architecture similar to the EDF algorithm. For the H-
EDF scheduling, in the first layer, tasks are classified according to task’s category. In 
the second layer, tasks are scheduled according to their priorities, to ensure hard real-
time tasks are not affected by soft real-time tasks to prevent from missing their 
deadlines. But this kind of algorithm is often ignore the requirements of the soft real 
time tasks, thus degrades the system performance.  
The proposed AFS method can be viewed as an improved version of the H-EDF 
algorithm. It is not only to keep the advantage of H-EDF but also to improve the 
schedulability of soft real-time tasks. Full migration can achieve better performance 
[5]. The proposed algorithm is also an improvement of the restricted migration 
scheduling policy, which separates the migration and no migration tasks to reduce 
the regular overhead of migration caused by full migration. 
3   Multi-core Scheduling Scheme 
In this section, the proposed AFS scheduling algorithm will be introduced. We 
propose a mechanism for multi-core real-time scheduling algorithm. The multi-core 
 An Adaptive Embedded Multi-core Real-Time System Scheduling 267 
priority of the task will be decided by its weight and classification. As a result, tasks 
will be classified into four types: 
1. THH: Hard-heavy task 
2. THL: Hard-light task 
3. TSH: Soft-heavy task 
4. TSL: Soft-light task 
According to the task type, a task will be assigned to a core and enter the field of local 
scheduling which is further divided into two categories: preemptive region and non-
preemptive region. Preemptive region follows the general single-core scheduling 
rules, such as RM, EDF or LLF, and so on. For the non-preemptive region, when a 
task enters this zone, it will be executed according to the first come first serve policy 
and will not be preempted by other tasks. Thus, tasks in this region can be guaranteed 
that it will be executed and complete quickly, Hard-heavy tasks can be executed 
independently to reduce the context switching overhead. In addition, to ensure the 
process in the non-preemptive region, once the task assigned to this category of 
scheduler, it will not be re-scheduled by outside global scheduler until the task is 
terminated [7][9]. 
For the strategy of distributing tasks to the local scheduler, Hard-heavy task will 
always be assigned to the non-preemptive region without conditions. The remainder 
tasks will be assigned to the preemptive region according to their weights, and then 
assigned to the core with the lightest loading. After entering the local scheduler, a low 
priority task may miss its deadline caused by waiting for execution. In local 
scheduler, though the deadline is approaching, the priority of the task can not be 
dynamically changed between cores.  
In order to improve this kind of situation, an adaptive feedback mechanism is 
adopted in the proposed scheme. Once there is a task terminated in its period, global 
scheduler will start re-scheduling to re-calculate weights of all tasks in the system and 
reallocate tasks between cores to improve the system schedulability. By selecting the 
appropriate computation of feedback mechanism intervals, will avoid shortcomings of 
over-frequent context switch which produced in the LLF algorithm [6]. When any 
task is terminated in its period, the system will automatically start the adaptive 
feedback mechanism as illustrated by straight arrow lines in the Fig. 1. 
Fig. 1 presents the AFS algorithm of the entire process. At first, global scheduler 
computes weight and classifies the type of each task, and then tasks are assigned to 
the corresponding local scheduler. THH will be decided whether there is enough space 
to enter the non-preemptive region. If there is no more space for scheduling the THH, 
the task will be assigned to other preemptive region with the higher priority. Adaptive 
feedback mechanism will be performed when any task is terminated in its period, the 
global scheduler will be activated again, and reallocate tasks between cores. However, 
tasks in non-preemptive region will not be rescheduled to guarantee accuracy of the 
region and decrease the context switch overhead. 
The AFS algorithm is described as follows: 
1. First of all, calculating the weight of each task in the global scheduler 
according to the equations described in subsection 3.2. 
 An Adaptive Embedded Multi-core Real-Time System Scheduling 269 
with other tasks with lower priorities. By selecting the appropriate number of 
preemptive regions and non-preemptive regions for the scheduler, system will not 
only maintain the accuracy of hard real-time tasks and reduce the context switch 
overhead, but also improve the schedulability of the soft real-time tasks. 
4   Experimental Results 
In this section, we will compare the proposed AFS algorithm with the H-EDF 
algorithm. In the experiment, simulations act on the premise that all hard real-time 
tasks can be scheduled to be compared with the miss rate of soft real-time tasks. 
Through simulations, the effectiveness of the proposed scheduling mechanism will be 
demonstrated. 
4.1   Experimental Environment 
We establish the experiment environment is as follows. The multi-core number in 
hardware environment consists of 4 and 8 cores. The ratio of the number of non-
preemptive regions and preemptive regions is 1:3 and miss rate is measured in each 
type with different loadings. Here assume that all tasks are periodic and independent, 
and their periods are equal to their deadlines. In addition, the context switch overhead 
of the two algorithms in this simulation is not practical to be considered. The system 
loading is also classified into two types: heavy loading and over heavy loading.  
In the simulation, light loading task set is not considered because the total loading 
of the light loading task set in the system will less then 100% and all real-time tasks 
will meet their deadline, the simulation results will be no significant difference. The 
heavy loading task set is more modest degree of system loading, the total loading is 
about 105%, while the over heavy loading task set is higher degree of system loading, 
the total loading is about 110%. Every single task’s loading does not exceed 60%. 
With heavy and over heavy system loading testing, more soft real-time tasks will miss 
their deadlines, thus, the performance difference between two algorithms can be 
presented clearly. 
4.2   Simulation 
We will compare the proposed AFS algorithm with the H-EDF algorithm. In the 
simulation, simulations act on the premise that all hard real-time tasks can be 
scheduled to compared with the miss rate of soft real-time tasks. In the first 
simulation, light loading task set is not considered because the total loading of the 
light loading task set in the system will not exceed 100%, the simulation results will 
be no significant difference. With heavy and over heavy loading testing, more soft 
real-time tasks will miss their deadlines, thus, the performance difference between 
two algorithms can be presented clearly. 
Fig. 2 shows the results of the comparison of two different algorithms under the 4 
cores environment. The proposed method gets lower miss rate than H-EDF, the 
difference becomes clearer especially under the over heavy loading testing. It means 
that the more the loading grows, the better effect will be illustrated by applying the 
proposed AFS algorithm. AFS algorithm can dynamically re-schedule the task to 
 An Adaptive Embedded Multi-core Real-Time System Scheduling 271 
 
Fig. 4. Miss Rate in Different Loadings under 4 Cores 
 
Fig. 5. Miss Rate in Different Loadings under 8 Cores 
obviously be greater. Miss rate occurs rarely when the loading does not exceed 80%, 
and the results can not be distinguished clearly. As the loading gradually increases, 
the difference of miss rate between two algorithms became apparent, especially when 
the loading reaches 110%. The miss rate increases abruptly in both algorithms when 
the loading becomes larger than 110%. Because in the heavy loading environment, it 
has seriously exceeded the loading that system can support. But comparing with the 
H-EDF algorithm, the miss rate of AFS algorithm is relatively low. 
5   Conclusion 
In the multi-core real-time platform, we propose a modified scheduling algorithm 
with adaptive scheduling and the feedback mechanism which can take care of both 
hard real-time and soft real-time tasks. It is a multi-level dynamic priority-driven 
algorithm that is based on the weight-driven to assign the task to the appropriate local 
scheduler, and by using feedback mechanism to monitor the system for reducing the 
miss rate of the soft real-time tasks. This scheduling algorithm can be easily adapted 
and changed to fit a variety of embedded environments. The simulation results show 
 出席國際學術會議心得報告 
 
                                                             
計畫編號 NSC 99-2221-E-036-018 
計畫名稱 支援雲端計算虛擬主機動態資源配置與節能方案之研究 
出國人員姓名 
服務機關及職
稱 
李良德 
大同大學資訊工程學系教授 
會議時間地點 100 年 7 月 3 日 至 100 年 7 月 4 日  巴西聖保羅 
會議名稱 第四屆國際普及媒體計算研討會 
發表論文題目 An Extenics-based Dynamic Resource Adjustment for the Virtual Machine in Cloud Computing Environment 
三、 參加會議經過 
第四屆國際普及媒體計算研討會(Ubi-media 2011)，從 7 月 3 日至 7 月 4 日
計 2 天在巴西聖保羅 Mackenzie 大學舉行。此次大會同時也包含二個
Workshop ： 2011 International Workshop on Advanced Distance Education
Technologies (ADET 2011)以及 2011 International Workshop on Mobile Systems, 
E-Commerce, and Agent Technology (MSEAT 2011)。研討會分成 17 個場次進行
研討，論文內容主要為 Ubi-media 四大相關領域：Ubi-media Infrastructure、
Ubi-media Middleware、 Ubi-media Human-Computer Interaction 、 Ubi-media 
Applications，以及 ADET、MSEAT 兩 workshop 相關論文。此外尚有三場專題
演講，由我國中央大學施國琛教授主講 “Video Forgery”、由巴西聖保羅大學 Jo 
Ueyama 教授主講 “Wireless Sensor Networks” 以及由 IBM Research-Brazil 之
Dr. Claudio Pinhanez 主講 “Ubiquitous Services”。 筆者於 6 月 29 日從台北經香
港、杜拜，於 6 月 30 日下午抵達巴西聖保羅。7 月 1 日即由大會安排參觀聖保
羅市區以及聖保羅大學與 Mackenzie 大學。聖保羅大學是巴西最大的公立大學
 三、攜回資料名稱及內容 
    參加本次會議一共攜回下列資料： 
1. The 4th International Conference on Ubi-media Computing (U-Media 2011) 
Conference Program 一冊。 
2. The 4th International Conference on Ubi-media Computing (U-Media 2011) 
Conference proceeding 光碟一片。 
 
An Extenics-based Dynamic Resource Adjustment for the Virtual Machine in 
Cloud Computing Environment 
Liang-Teh Lee, Shin-Tsung Lee, and Jui-Chih Chang 
Department of Computer Science and Engineering, Tatung University, Taiwan 
ltlee@ttu.edu.tw 
Abstract 
In order to integrate the system resource, utilize the 
resources flexibly, and meet the demand of various data 
management and applications in the cloud computing 
environment, one of the positive solutions is to apply the 
virtualization technology. Various applications performed 
on a virtual platform, the required resources for 
individual application should be adjusted flexibly and 
effectively. An extenics-based dynamic resource 
adjustment mechanism for supporting cloud computing is 
proposed in this paper. Based on a KVM (Kernel-based 
Virtual Machine) technology, an associative function has 
been established by applying the extenics theory to 
determine the degree of CPU requirement for each 
application. According to the degree of CPU requirement, 
the proposed resource adjustment mechanism can 
provide a proper number of CPUs for each virtual 
machine dynamically. The experimental results show that 
the proposed mechanism can improve the performance of 
the system and distribute the source information 
effectively in real time. 
1. Introduction 
Cloud computing is becoming an important 
opportunities for industry to provide a high degree of 
scalability and serviceability computing resources. In 
addition to meeting the requirements of high availability 
and flexibility of shared resources, it requires a dynamic 
resource adjustment mechanism to improve the resource 
utilization. For serving a large and various requirements, 
a more flexible resource provisioning scheme is required 
in cloud computing environment via virtualization 
techniques. In the virtualization environment, the system 
resource management is an important issue, including the 
provisioning and monitoring of system resources. From 
monitoring the utilization of the resource and considering 
the predefined condition for resource adjustment, the 
loading of the system can be effectively balanced and the 
system performance can be further improved. In this 
paper, an extenics-based dynamic resource adjustment 
mechanism for supporting cloud computing has been 
proposed to support the resource provisioning. 
2. Related work 
2.1 Cloud computing and computing models 
   Applying the virtualization and integrating information 
technologies, cloud computing environment provides the 
services, such as computation, storage, and bandwidth of 
the network to the user through the internet. Under the 
cloud computing environment, users can just consider the 
service as a black box to provide the desired operation for 
obtaining the result [1-3]. According to the type of 
services, cloud computing can be classified into three 
models: 
   (1) Software as a Service (SaaS) is to provide users 
with software applications. Through the software on 
demand, the software applications such as email, word 
processing in office, data analysis, and business 
management, etc., are provided. (2) Platform as a Service 
(PaaS) provides the cloud platform for users to develop 
and execute the services of their applications. (3) 
Infrastructure as a Service (IaaS) provides users with 
facilities of infrastructure of the computer system. 
Usually the provided platform is virtualized, virtual 
resources include servers, CPU, memory, network 
equipments, and storage devices. Resources of the system 
should be adjusted dynamically to meet the users’ 
requirements and improving the system performance. 
2.2 Virtualization technologies 
Virtualization is a technology to provide users or 
application programs an easy way to access the desired 
computing resources that are not confined by the initial 
installation. In other words, virtualization offers 
computing power, storage capacity, and other resources to 
a logical category, rather than a limitation of the physical 
machine. It allows computing resources not to be limited 
by the physical entity, up to a lot of virtual resources, just 
like the single entity host to monitor and manage multiple 
virtual resources, as well as the entire infrastructure, 
dynamically changing and adjusting these virtual 
resources [4]. There are several virtual machine platforms 
available, such as Xen [5] and KVM [6]. 
Fourth International Conference on Ubi-Media Computing
978-0-7695-4493-9/11 $26.00 © 2011 IEEE
DOI 10.1109/U-MEDIA.2011.33
65
3. Proposed Dynamic Resource Adjustment 
Mechanism
3.1 System overview
The proposed system is established by using KVM 
(Kernel based Virtual Machine) fully virtualized 
architecture. By applying VMM (Virtual Machine 
Manager) tools [8] to design an extenics-based dynamic 
resource adjustment mechanism for the virtual machine in 
the cloud computing environment. The proposed system 
consists of two control threads, resource adjustment and 
monitor. Figure 4 illustrates the flowchart of the monitor 
thread, including the extenics mechanism, to monitor the 
utilization of the vCPU(virtual CPU) and record the 
number of vCPUs used in the virtual machine.  
Figure 4. Flowchart of the monitor thread 
The utilization of vCPU can be measured for 
determining if the current utilization is in the extenics 
region and it is in the high utilization or low utilization 
area. Once the utilization is in the extenics region, the 
adjustment mechanism can be applied. 
Figure 5 shows the flowchart of the resource 
adjustment thread. Flags “Add_Flag” and “Del_Flag” are 
global variables for determining the increment or 
decrement of vCPU in a virtual machine.  Of course, if 
the vCPU used is 1, it cannot be further decreased. 
Similarly, if the vCPU used is the maximum value 
predefined, it cannot be further increased. 
ʳ
Figure 5. Flowchart of the resource adjustment thread 
3.2 Resource adjustment by applying extenics 
theory
      The utilization of vCPU can be measured for 
determining if the current utilization is in the extenics 
region and it is in the high utilization or low utilization 
area. Once the utilization is in the extenics region, the 
adjustment mechanism can be applied. Two examples are 
described as follows. 
    Example 1: Let two utilization regions be [50%, 90%] 
= X0 for high utilization, and [30%, 50%] = X0 for low 
utilization. Assume that the current utilization obtained is 
80%, the value of the associative function for the current 
utilization with two regions can be calculated. The value 
of the associative function, KH(x), for the high utilization 
region is calculated as below: 
   
  105090
2
1
2
905080
2
1
20
......
abbaxX,x






  10 X,X,xD
     100
0 .
X,X,xD
X,x
xK H 

Similarly, the value of the associative function, KL(x), for 
the low utilization region can be calculated as: 
67
ab –n 24 –c 4  http://VMIP/index.htm 
Where n is the number of connections, c is the number of 
concurrent connections allowed, and the last portion is the 
web site to be connected.  
Table 1 hardware and software for the experiment 
CPU AMD Athlon II X4 640 2.8G  
RAM 4GB
Hardwar
e
Network interface 1Giga/bit 
Operation system Ubuntu 10.10 
Virtualization 
software KVM84 
Virtual machine 
management 
Red Hat Virtual 
Machine 
Manager 0.8.2 
Virtualization library LinVirt 0.7.1 
software 
SSH for Python Paramiko 2.0 
In the experiments, each connection is to ask for 
finding prime numbers from 1 to 700,000. During 
experiments, average number of adjustments and the 
average execution time are measured under 1, 2, 4, and 8- 
VM environments.  The experimental environment is 
shown in Figure 7. 
Figures 8 and 9 illustrate the comparisons of average 
number of adjustments and average execution time, 
respectively, from the experimental results. The 
Comparison of the number of adjustments from running 
on 1 VM with the continuation variables 1-sec and 5-sec, 
the situation of 5-sec can reduce the number of 
adjustments significantly, and average decrement is 61%. 
By applying the proposed extenics-based mechanism, the 
experimental results show that the decrement of the 
number of adjustments are 68% and 15% for comparing 
with the situations of continuation variables 1-sec and 5-
sec, respectively. When running on 1 VM, 4 vCPUs are 
corresponding to 4 real CPUs of the host machine which 
can support enough CPU resource, thus, there is no 
significant difference in execution time for different 
situations in the experiment.  When running on 4 VMs, 
for continuation variable is 1-sec, the number of 
adjustments increased significantly, since the allocated 
real CPUs decreased for each VM, results in resource 
competition among VMs. For continuation variable is 5-
sec, the number of adjustments is decreased about 25% 
by applying the proposed mechanism. When running on 8 
VMs, since significant competition of the CPU resource 
occurs, the number of adjustments will affect the 
execution time significantly. For continuation variable is 
5-sec, comparing to the proposed mechanism, the number 
of adjustments is decreased about 48% and execution 
time is 19% decreased. Under 1-sec continuation variable 
situation, the number of adjustments and execution time 
are decreased 87% and 43% respectively.  
From experimental results, we can find that the 
system by applying the proposed extenics-based 
mechanism, under 1, 2, and 4-VM situations, the number 
of adjustments and execution times do not change 
significantly, since 4 real CPUs can provide resource for  
each VM fairly. However, in 8-VM situation, increasing 
the competition of the CPU resource results in the 
significant difference performance presented in different 
situations. For considering the fixed continuation time to 
adjust resource will decrease the system performance, 
while adjusting the resource allocation only in the 
extenics region will improve the system performance 
significantly. 
Figure 7. Experimental environment 
ʳ
Figure 8.  Average number of adjustments comparison 
69
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/27
國科會補助計畫
計畫名稱: 支援雲端計算虛擬主機動態資源配置與節能方案之研究
計畫主持人: 李良德
計畫編號: 99-2221-E-036-018- 學門領域: 計算機結構與計算機系統
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
