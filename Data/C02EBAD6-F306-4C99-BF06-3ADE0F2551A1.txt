ii 
中文摘要 
隨著網際網路新服務的引入與新多媒體服務應用，使用者對於網路的需求已
不是下層傳輸網路可以滿足，使用者希望能更快速得到所需要的傳輸資源。然
而，要支援這類次世代的服務，網路服務提供者，除了提供要有完善 QoS 支援
的傳輸網路，也要有 Computation 及 Storage 能力支援。(如 Video Cache, Data 
Cache, Video Proxy/Agents 等)，而網路本身也要能對抗敵意的入侵與內部的故
障。因此本群體計畫目標能在於設計規劃一個能滿足次世代服務完善的網路傳輸
環境與控管架構。本群體計畫除了以 MPLS 為基礎的 Transport Network，更引進
Content Aware Classification、Intrusion Defense 與網路運算資源的偵測等先進功
能。 
此群體計畫共規劃一個總計畫及四個子計畫。在總計畫中我們建置一個以網
路處理器為基礎，且有品質控制與管理能力的多重標籤網路，做為各子計畫整合
及系統開發的基礎。子計畫一針對服務品質的控制，MPLS 網路 Traffic 
Engineering、Fast Reroute 及 VPN 的實作及相關研究作一探討；子計畫二針對
MPLS 網路提出內容感知(Content-Aware)及安全考量的解決方案，並在實驗平台
上做分析與研究；子計畫三分析研究以 MPLS 網路為主架構之基礎網路及考量
Content Aware 與 Security 需求的監測管制策略以及網路傳輸與運算資源分配管
理。子計畫四在支援 Content-Aware 與 QoS 之 Secured Diffserv/MPLS 網路
上，透過查詢子計畫三所管控的網路資源，利用子計畫一、二所提供具服務品質
保證能力的網路，研究互動式多媒體存取。我們也利用子計畫四來展示前三項子
計畫的成效。此一期末報告描述我們計畫執行的成果。 
 
關鍵詞:多重標籤交換協定,服務品質保證,網際網路,網路安全,網路監控,多媒體存
取 
iv 
目錄 
中文摘要……………………………………………………………………………ii 
Abstract………………………………………………………………………………iii 
第 1 章 緣由與目的............................................................................................1 
第 2 章 <子計畫一> 具服務品質保證及提供虛擬私有網路與行動服務之
多重協定標籤交換路由器的設計與實作....................................................................3 
2.1 支援 Fast Reroute、QoS Control 之 MPLS 路由器 ..................................4 
2.1.1 完整系統架構概觀..........................................................................4 
2.1.2 控制平面（Control Plane）概述 ...................................................5 
2.1.2.1 對 Quagga 軟體的擴充 ........................................................5 
2.1.2.2 Quagga 擴充軟體的實驗環境與功能驗證 ...........................6 
2.1.3 資料平面（Data Plane） ..............................................................13 
2.1.3.1 系統架構與處理流程..........................................................13 
2.1.3.2 資料平面功能驗證與效能測試..........................................19 
2.2 虛擬私有區域網路服務控制平面模組整合與其橋接器的實現............29 
2.2.1 系統架構........................................................................................29 
2.2.2 VPLS Service 內部網路連線建立流程........................................30 
2.2.3 VPLS 服務功能驗證....................................................................31 
2.3 成果自評....................................................................................................38 
2.4 參考文獻....................................................................................................39 
第 3 章 <子計畫二> 內容感知與高安全 MPLS 網路之設計和實驗研究...41 
3.1 子計畫二之研究背景................................................................................42 
3.1.1 跳板式 (Stepping stones) 的攻擊................................................42 
3.1.2 即時通訊網路應用之盛行............................................................43 
3.2 國內外有關本計畫之研究情況................................................................43 
3.2.1 跳板式 (Stepping stones) 的鑑識................................................44 
3.2.2 網路應用層鑑識之實作................................................................45 
3.3 結果與討論................................................................................................47 
3.3.1 Locality Steering Traceback Algorithm .........................................47 
3.3.2 實驗結果與評估............................................................................50 
3.3.3 MSN-Shot 之實作 ........................................................................52 
3.3.4 MSN-shot 功能比較 ......................................................................54 
3.4 成果自評....................................................................................................56 
3.5 參考文獻....................................................................................................58 
第 4 章 <子計畫三> 內容感知高安全性多重協定標籤交換網路上之運算
與傳輸資源監控..........................................................................................................60 
vi 
圖片目錄 
 
圖 2.1.1-1 以網路處理器為基礎的路由器系統軟體架構 .......................4 
圖 2.1.2-1 Quagga 初始架構.........................................................................6 
圖 2.1.2-2 Quagga 擴充後之架構.................................................................6 
圖 2.1.2-3 Traffic engineering 實驗環境 ......................................................7 
圖 2.1.2-4 LSP 建立說明 ..............................................................................7 
圖 2.1.2-5 EXPLICIT ROUTE Object ..........................................................8 
圖 2.1.2-6 R1-R2 Link 的頻寬變化情況 ......................................................9 
圖 2.1.2-7 R2-R4 Link 的頻寬變化情況 ......................................................9 
圖 2.1.2-8 LSP 建立示意圖 ........................................................................10 
圖 2.1.2-9 從 R1 抓取的 PATH message 內容 ..........................................11 
圖 2.1.2-10 R1 到 R3 的 Outgoing link 頻寬變動情況..............................11 
圖 2.1.2-11 R3 到 R2 的 Outgoing link 頻寬變動情況..............................12 
圖 2.1.3-1 Data Plane Fast Path 細部架構 ...............................................13 
圖 2.1.3-2 DiffServ / MPLS data flow......................................................15 
圖 2.1.3-3 Packet header cache allocation ................................................16 
圖 2.1.3-4 Byte-alignment for packet header caching ..............................16 
圖 2.1.3-5 Slow-path 結構與 Exception Packet 處理流程 ......................18 
圖 2.1.3-6 Classifier 與 MPLS FTN 功能驗證實驗環境 ........................19 
圖 2.1.3-7 Classifier 功能驗證所必須下達的指令 .................................19 
圖 2.1.3-8 被貼上 label 53 的 SIP 封包...................................................21 
圖 2.1.3-9 被標為 AF23 的封包代表著被計量成紅色 ..........................22 
圖 2.1.3-10 Traffic 稍為緩和封包從 AF23 變成 AF22 成為黃色封包 .22 
圖 2.1.3-11 MPLS FRR 因應 Link Failure 之實驗環境 .........................23 
圖 2.1.3-12 尚未發生 Link Failure 時帶有 Label 40 的封包 .................23 
圖 2.1.3-13 發生 Link Failure 後實施 FRR Swap-push 的封包 .............24 
圖 2.1.3-14 One-to-one transmission throughput vs packet size ..............25 
圖 2.1.3-15 DRR experiment 1 之實驗環境 ............................................26 
圖 2.1.3-16 DRR Scheduling result #1 .....................................................26 
圖 2.1.3-17 DRR experiment 2 之實驗環境 ............................................27 
圖 2.1.3-18 等量頻寬的流量結果 ...........................................................27 
圖 2.1.3-19 DRR Scheduling result #2 .....................................................27 
圖 2.2.1-1 VPLS 系統架構 ........................................................................29 
圖 2.2.2-1 核心網路建立連線流程 ...........................................................30 
圖 2.2.3-1 Simple VPLS Service Model......................................................31 
viii 
圖 4.2.4-7：LSP Level 管理表格 ..............................................................78 
圖 4.2.4-8  Link(5.3->5.2)斷線後的網路拓樸 ........................................79 
圖 4.2.4-9 Link failure handle for class 1 ...................................................80 
圖 4.2.4-10 Link failure handle for class 2 .................................................81 
圖 4.2.4-11 Link failure situation for class 3 ..............................................81 
圖 4.2.4-12 Link failure handle for class 3 .................................................81 
圖 4.2.5-1：核心路由器監控管理系統與 NRB 的互動 ..........................82 
圖 4.2.5-2：LSP 錯誤型態 ........................................................................83 
圖 4.2.5-3：IETF-MPLS PING..................................................................84 
圖 4.2.5-4：ITU-T -Connection Verification .............................................85 
圖 4.2.5-5：Intel IXA4.1 軟體架構圖.......................................................86 
圖 4.2.5-6：CV 的運作流程......................................................................87 
圖 4.2.5-7：PM 封包格式 .........................................................................88 
圖 4.2.5-8：PM 的運作流程 .....................................................................88 
圖 4.2.5-9：規劃的架構 ............................................................................90 
圖 4.2.5-10：Ingress 端的流程..................................................................90 
圖 4.2.5-11：Egress 端的流程...................................................................91 
圖 5.1.1-1：本技術欲提供服務之示意圖 ................................................97 
圖 5.1.2-1：系統網路環境架構 ................................................................97 
圖 5.1.4-1SIP Session Mobility 的訊息流程圖 .......................................101 
圖 5.1.4-2 傳統 Cross device 流程圖.......................................................101 
圖 5.1.4-3：提出之 session mobility 及 multicast 流程圖 .....................102 
圖 5.1.4-4 P2P Streaming 範例之訊息溝通示意圖.................................105 
圖 5.1.5-1 系統網路環境架構 .................................................................106 
圖 5.1.5-2 本系統開發平台之硬體架構圖 .............................................107 
圖 5.1.5-3：使用軟體實現 L3 Switch Router 之架構圖........................108 
圖 5.1.5-4：Home Gateway 中訊息流程的邏輯示意圖 .......................108 
圖 5.1.5-5：軟體發展系統架構圖 ..........................................................109 
圖 5.2.1-1 SIP-based VoIP over MPLS Architecture ................................112 
圖 5.2.1-2 Signaling Procedure.................................................................116 
圖 5.2.2-1 Partysip Functions ...................................................................117 
圖 5.2.2-2 oSIP related SIP Parser ............................................................118 
圖 5.2.2-3 Partysip Threads 與 SIP Transactions .....................................119 
圖 5.2.2-4 Threads Process Received Message Flowchart .......................120 
圖 5.2.2-5 Threads Transmit SIP Message Flowchart ..............................121 
圖 5.2.3-1 Example with no CAC Policy1 ...............................................123 
圖 5.2.3-2 Example with CAC Policy1 ....................................................123 
圖 5.2.3-3 Session 的建立與終結的訊息流程圖 ....................................124 
1 
第1章 緣由與目的 
網際網路在過去十餘年的蓬勃發展，已廣為大眾所接受與使用，目前已成為
大部份人生活與工作上不可缺乏的”必需品”。除了 WWW 已成為網路最重要的
應用，各類不論是否與 WWW 相結合的新型網路應用，例如：語音電話 (如: 
VoIP)、影像電話、視訊會議、遠距教學、隨選視訊 (Video on Demand, VoD) 等，
也隨之廣為開發及使用。這使得網路早先以 "能夠通訊" 為主的設計目標，已拓
展至對 "通訊品質" 的要求，亦即網路對於服務品質 (Quality of Service, QoS) 的
支援，這也已成為新一代寬頻網路基礎建設與營運管理時所必備的需求與能力。
然而欲提供完善的 QoS 支援，其牽扯層面甚廣，從使用者應用程式與作業平台、
網路傳輸媒介與協定、路由器計算能力、到相當複雜的網路互連，缺一不可。而
各個層面中所需技術的研究與開發，皆極具挑戰性。而隨著近年來產業界與學術
界的熱烈參與討論研究及規格制訂，新一代網路需求與藍圖已逐漸成型，主要方
向是以增強傳輸通道與網路互連頻寬容量、提供訊務控制以及 QoS 能力、並配
合信令協定以提供使用者端 QoS 參數傳輸能力、支援群播 (Multicast) 能力以節
約並有效利用網路資源與頻寬、提供 Ubiquitous 行動通訊所需之 Mobile IP 功
能，以達成支援高要求且複雜的多樣化網路應用等。然而由於各個規格多為獨立
制訂，其所需技術與整合時所將帶來的問題皆是關鍵所在。如何掌握這些新一代
網路核心技術，取得先機，已是提升國家網路產業能力所勢在必行的方向。 
近年來，多重協定標籤交換(Multi Protocol Label Switching)的技術已趨於成
熟，並逐漸為業者接受，陸續佈建中。究其原因，MPLS 可以(1)有效整合 IP 與
底層協定（如 ATM, SONET/SDH, DWDM），提供高速且有效率的資料傳輸；(2) 
MPLS 可與 Diffserv 結合，提供 Service class 的區分；(3) MPLS 可以容易達成
Traffic Engineering，使整體網路頻寬的使用達到最佳效益；(4) MPLS 支援 Virtual 
Private Network (VPN)，可以在公共網路上建造出一個虛擬企業網路。因此 MPLS
可以說是未來最被看好的網路技術，它同時針對目前大家最關心的頻寬使用，服
務品質保証，服務等級區隔等問題提出一解決方案。 
隨著多年來中正大學電機所通訊與網路組於寬頻 ATM 光纖網路實驗平台的
相繼建立和擴充，配合教育部科技顧問室 "高速網路及其應用實驗平台" 計畫，
及電信國家型計畫 "以多重協定標記交換技術為基礎的服務品質導向網路研究
與實驗" 等計畫的執行，以及各種相關的理論研究與實作實驗，我們已經累積了
相當的網路技術與互連的經驗與成果。在此一群體計畫，我們延伸過去三年以網
路處理器為平台所研發 MPLS Router 的成果，繼續在 Traffic Engineering、QoS、
VPN 等 MPLS 的功能及應用作研究與實驗，同時引進 Content Aware 
Classification、Intrusion Defense 與網路運算資源的偵測等先進功能，並以此平
台，研究、設計、實驗次世代服務(如互動式多媒體通訊)。 
此群體計畫共含四個子計畫。: 
1. 具服務品質保證及提供虛擬私有網路與行動服務之多重協定標籤交換
3 
第2章 <子計畫一> 
具服務品質保證及提供虛擬私有網路與行動服
務之多重協定標籤交換路由器的設計與實作 
本子計畫的實作分別於資料平面(Data Plane)以及控制平面(Control Plane)兩
方面進行，並利用 CP-PDK(Contrl Plane Product Development Kit)的技術將資料平
面與控制平面的訊息加以同步並整合。在資料平面方面，以 Intel IXP（Internet 
eXchange Processor）2400 高階網路處理器為平台，以 Intel IXA（Internet eXchange 
Architecture）的軟體開發框架為基礎，實作完整的支援 DiffServ 和 TE-FRR 的
MPLS 邊界/核心路由器之資料平面；在控制平面方面，除了完成 TE 的路由協定
OSPF-TE 和 BGP 之外，為了實現 DiffServ，LDP 必須對 PHB / FEC signaling 予
以擴充。而 RSVP-TE 延伸自 RSVP（Resource Reservation Protocol），支援 Explicit 
routes 與資源保留的功能，是作為建立具 QoS 保證與支援 TE 之 MPLS LSP 的重
要 signaling protocol。；CP-PDK 提供了控制平面經由資料平面傳送和接收 control 
packets 的 virtual interfaces；一組定義完善的 APIs 用以對資料平面進行控制與操
作；用於控制平面與資料平面之間通信（Messaging）的協定標準 ForCES；以及
提供平面之間相互 interfacing 的抽象層（Forwarding Plug-in and Backend APIs）。
在 MPLS VPN（VPLS）的部分，核心網路扮演一個 bridge 的角色，將同一 VPN 
群組的 PE (Provider Edge)路由器全部橋接起來，提供相同 VPN 成員處於一個模
擬的 LAN 環境之下。VPLS 技術主要區分為控制平面(Control Plane)和資料平面
(Data Plane)兩大部分，主要設計並實作基於 Quagga 的控制平面架構，並且於
PC 環境下實現 VPLS 資料平面的功能。 
各層級的實作分別透過不同的實驗來驗證系統的功能，測試與評估系統的效
能。 
5 
2.1.2 控制平面（Control Plane）概述 
 Control Plane 負責 Routing 和 Signaling 等功能。圖 2.1.1-1 中 OSPF-TE[20] 
daemon 是具備 Traffic Engineering 能力的改良版路由協定，延伸自 Open Shortest 
Path First；當中採用了 CSPF（Constraint-based Shortest Path First）[2]路由演算法
來計算符合 TE 的路由。除了 OSPF-TE，完整的路由系統還包括了 RIP（Routing 
Information Protocol）與 BGP（Border Gateway Protocol）等協定[17]。 
 MPLS 是建立在 IP 網路的虛擬線路交換技術，在使用 MPLS 進行傳輸之前，
必先透過 signaling protocol 來建立 LSP；LDP（Label Distribution Protocol）就是
專為 MPLS 而設計的最基本的 LSP signaling protocol，其原理就是 LSRs 之間會
將 LSP 使用到的 incoming label / outgoing label 與 FEC 之對應關係往上游或下游
逐級散播。為了實現 DiffServ，LDP 必須對 PHB / FEC signaling 予以擴充。
RSVP-TE 延伸自 RSVP（Resource Reservation Protocol），支援 Explicit routes 與
資源保留的功能，是作為建立具 QoS 保證與支援 TE 之 MPLS LSP 的重要
signaling protocol。 
 Zebra與MPLSd的目的，是為了在PC based的環境提供 control plane software
抽象和一致的底層介面，來對應到 IP與MPLS的實體資料平面。整個 control plane 
protocol software 建立於 Quagga 架構，使用到統一的系統 APIs；當中 VTYSH 是
內部 daemons 之間以及與 CLI 之間的標準通訊介面。更多的資訊可參考[21]。 
2.1.2.1 對 Quagga 軟體的擴充 
Quagga 是一套擁有相當完整路由協定的軟體，但是它對於 traffic engineering
功能的擴充並不完備，如圖 2.1.2-1 所示，在 Quagga 之中包含一個鏈結狀態資料
庫（Link State Database）、路由計算（Routing calculator）、最短路徑優先路由表
（SPF routing table）和不完整的 TE Database，假如我們要在 Quagga 實現 CSPF，
我們必須擴充 Quagga。我們增加了兩個新功能分別是 local interface address 和
remote interface address [20]的設定，使得我們可以藉由 vty command 在 OSPF-TE 
node之下手動輸入 local interface address 和 remote interface address 的資訊以更新
TE Database 內容，並且新增兩個頻寬管理的功能 reduce unreserved bandwidth 和
increase unreserved bandwidth，讓我們可以手動增加或減少 unreserved bandwidth
的值。 
 
7 
 
圖 2.1.2-3 Traffic engineering 實驗環境 
在一開始我們比較在 NHP-CSPF 演算法尚未加入之前，LSP 的建立情況，
我們於 R2 使用 Ethereal 封包檢視軟體抓取封包，證實 LSP 的建立確實根據最短
路徑優先（SPF）演算法，選擇一條最短的路徑，如圖 2.1.2-4 為 LSP 建立說明。 
 
192.168.4.254
192.168.4.254
192.168.5.254
192.168.2.76
192.168.3.49
192.168.2.95
192.168.4.76 192.168.3.76
192.168.2.76
192.168.3.49
192.168.5.49
192.168.2.95
R1
R2
R3
R4
 
圖 2.1.2-4 LSP 建立說明 
在啟用 NHP-CSPF 演算法和 OSPF-TE 的功能之後（TE-Class[i] UB 皆為
1.25M bytes），我們驗證 NHP-CSPF 是否有選擇較佳頻寬路徑的能力，為達到此
目的，撰寫了一支測試程式，檔名為 QoS_LSP，它的目的是建立 R1ÆR2ÆR4
的明確路徑並指定要求頻寬為 700k bytes，class type 為 0。 
9 
 
圖 2.1.2-6 R1-R2 Link 的頻寬變化情況 
 
圖 2.1.2-7 R2-R4 Link 的頻寬變化情況 
執行 QoS_LSP 的另一個主要目的是為了將 UB（Unreserved Bandwidth）變
小，接下來我們將會加入 NHP-CSPF module，並且再一次要求 700K bytes 的頻
寬，class type 為 0。我們啟動 CSPF module 和 SLA 程式，在 SLA 程式執行之後
我們輸入目的端 R2 的位址以及要求 class type 為 0，頻寬為 700K bytes 的資訊，
CSPF module 隨即算出一條明確路徑，並傳回給 SLA，SLA 將明確路徑和
11 
 
圖 2.1.2-9 從 R1 抓取的 PATH message 內容 
 而R1ÆR3（圖2.1.2-10）和R3ÆR2（圖2.1.2-11）的Outgoing link的TE-Class[0]
和 TE-Class[1]的 UB 各別都從原本的 1.25M bytes 被扣除了 700K bytes，也都剩
下了 550K bytes。 
 
圖 2.1.2-10 R1 到 R3 的 Outgoing link 頻寬變動情況 
 
13 
2.1.3 資料平面（Data Plane） 
2.1.3.1 系統架構與處理流程 
ME3
ME2 ME6
ME0
ME4
ME7ME3
ME4
ME0
DiffServ Egress Pipeline (ME1, ME5, ME7)
DiffServ Ingress Pipeline (ME1, ME2, ME5, ME6)
Classifier
for IPv4
MPLS
FTN
IPv4 
Fwder
MeterAF classes
BE
MPLS
ILM
Ethernet
Rx
L2
 C
la
ss
ify
dl
_s
ou
rc
e
dl
_q
m
_s
in
k
QM
Scheduler
CSIX 
TX
CSIX 
RX
WRED
dl
_s
ou
rc
eEthernet
ARPQM
Scheduler
dl
_q
m
_s
in
kEthernet 
TX
EF class
Swap, Swap-Push, Pop-Forward
Pop: IPv4
Meter EF class
AF, BE classes
Ingress IXP2400
Egress IXP2400
M
SF
(local loopback)
 
圖 2.1.3-1 Data Plane Fast Path 細部架構 
圖 2.1.3-1 所示為詳細的資料平面快速封包處理路徑的系統架構，由於所使
用之平台 IXDP 2410 是典型的 Ingress/Egress 雙 NPU（Network Processor Unit）
架構，所以系統也被劃分成 Ingress 與 Egress 兩個部分。Ingress 主要負責接收封
包，根據封包標頭進行處理與轉送；Egress 主要負責毋須參考封包內容的工作：
Buffer Management、排程、Layer 2 封裝，與封包傳送等任務。兩者之間透過 MSF 
local loopback 連接，之間以 CSIX frame 作為封包移送過程的單位[22]。 
這個架構結合了 Hyper Task Chaining（HTC）[22]和 Pool of Threads（POT）
[22]兩種 programming models。從整體的觀點來看，若把 processing stage（即
DiffServ Ingress / Egress pipeline）視為一個功能執行於一個 ME，則架構中每個
ME 就只負責一個獨立的功能，這就是 HTC 模型中的 Context Pipelining[22]做
法。再進一步看 processing stage 事實上是以 POT 為基礎來建立的：在這裏每一
個 ME 的每一個 thread 都執行同樣的程式，並且負起對被服務的封包完成所有處
理的責任。例如 DiffServ Ingress Pipeline 中，一個空閒的 thread 會從 RX ring 接
收一個封包，然後對這個封包依序完成 Classification、Metering、MPLS FTN 直
到 Enqueue。為了提升封包處理的效能，Ingress / Egress Pipeline 採用多個 MEs
實現平行的封包處理，這些 MEs 則被分配自兩個 ME Clusters 以提高平行運算時
存取 I/O 之效能。 
每一個 Stage 又稱為 dispatch loop[9]，dispatch loop 將一個或多個 MB(s)結合
起來並於一個 ME 上運作，針對 application 的需求實作出 MBs 之間特定的 data 
15 
6-tuple 
classifier
Output:
FEC
flowID
ClassID
ColorID
NextBlock
AF classes?
(NextBlock 
== Meter)
TRTCM(flowID, colorID)Yes
FTN_Marker(fec, 
flowID, colorID)
Output:
flowID (DSCP), ColorID
IP Forwarder
EF class?
(NextBlock 
== Marker)
classID < 8 ?
(EF Class)SRTCM(0x1, 0x0) Yes
WRED(out_port, 
classID, ColorID)If dropNo
Scheduler()
ILM()
Drop the packet
Output:
flowID
ClassID
ColorID
NextBlock
flowID = 0
For FRR, uses PortState 
and PHBMask to select an 
NHLFE entry from a set
Incoming PHB 
determination
For L-LSP, uses flowID to 
select an NHLFE entry 
from a set
No
No
Yes
Cache header 
in localmem
with byte 
aligned
L2 
classify
Incoming IP packet
Incoming labeled packet
Packet in
Packet out
QM.Enqueue(portID, 
classID)
QM.dequeue(portID, 
classID)
Queues
QM feedback
IPv4 
exposed? Yes
No
No
flowID = 7
Yes
flowID = 0
flowID = 1 ~ 4
 
圖 2.1.3-2 DiffServ / MPLS data flow 
 圖 2.1.3-2 所示為 Data Plane Fast Path 的封包處理流程圖，為了方便討論，
圖中並沒有明確劃分 Ingress 與 Egress，RX、TX、Ethernet ARP 等細節部分亦沒
有明確顯示於圖上。以下以一個封包在系統內的生命週期依序說明： 
¾ Cache header with byte-alignment：為了避免重複地對外部記憶體存取封
包標頭，在一開始會把 incoming packet 的標頭 cache 到 ME 內高速的儲
存媒體，如 Transfer Register 或 Local memory。本計畫採用 Local memory
作為 cache 媒體，原因有二：1) Transfer registers 數量非常有限；2) Local 
memory 可讀可寫，非常適合用於 read-modify-read-modify 的封包處理。
至於 cache 的大小視 application 的需求而定，本計畫設定了 header cache
大小為 12 LWs（longwords），如圖 2.1.3-3 所示，首 4 個 LWs 是預留作
MPLS Label Push 時用的 headroom；接著的 5 個 LWs 儲存著完整的 IPv4
標頭；由於 6-tuple classifier 需要存取到部分的 L4 標頭，為此還需要 2
個 LWs，最後並預先保留一個 LW 作延伸之用。圖 2.1.3-4 是 byte 
alignment 示意圖，由於 header cache 是以 1 longword（4 bytes）為單位，
而 Ethernet frame header 長 14 bytes，導致封裝在內的各層標頭無法以
LW boundary 對齊。為了更有效率地存取標頭欄位，當進行 header 
caching 時應該同時進行 byte alignment。 
17 
並從 result rule 取出 FEC、flowID、classID、colorID，以及 next block
等資訊，最後將之儲存 packet metadata當中。其中 flowID 對於 AF classes
是指向一個 meter instance 的索引，對於 EF 與 BE class 則代表著
Incoming PHB 的 DSCP；classID 代表了封包的等級，事實上會在 Egress
端被用來當作 queue id；colorID 代表了這類封包的 drop level，
classification 結果中的 colorID 是 administrative color。colorID 會被 Meter
所引用和修改，而 Egress WRED 則依照 colorID 決定封包被丟棄的機
率。由於本計畫支援 EF、AF、BE 三個 PHB groups，因此 dl_next_block
會被對應到 Marker、Meter、IP forwarder。詳細說明請見 5.4 節。 
¾ TRTCM：[23-25]是支援雙速三顏色的計量器，利用由 classifier 所得的
flowID 來選擇一個 meter instance。每個 instance 包含有 administrative 
traffic profile。在 meter 對封包進行計量時，traffic profile 會作為 dual 
token buckets 演算法的重要參數。計量的結果，會以三種顏色來標記；
輸出結果包括儲存在 flowID 中代表著 outgoing PHB 的 DSCP。最後，
next block 總是被設定成 marker（MPLS FTN）。詳細說明請見 5.5 節。 
¾ MPLS FTN：負責以來自於 classifier 或 IP forwarder 所得結果之 FEC，
對接收到封包進行 NHLFE 查找，若 LSP 受到備援路徑保護，ILM 會先
參考 TE-FRR 資料結構決定是否切換到 backup tunnel；然後根據 NHLFE
所記錄的 LSP type 與 Tunnel model，並引用 flowID 與 colorID 對封包進
行對應的 DSCP Marking 及 label stack 操作；最後將封包往 Egress 傳遞。
詳細說明請見 5.1 節。 
¾ 到這裏，事實上 Ingress 部分的流程已經告一段落；從圖 2.1.3-2 下方封
包類別檢查開始，是為 Egress 的部分。這個檢查的目的是為了把 EF class
從其他 classes 區分開來；在 EF class 所使用的佇列擁有絕對的優先處
理順序，因此利用了 meter 對 EF class traffic 進行計量，當 EF 超量時就
予以直接丟棄，來避免 EF class 壟斷系統所有資源。 
¾ WRED：為了確保服務品質，提升網路傳輸效能，首要就是設備能有效
做到主動的 congestion avoidance。透過 WRED 實施 buffer management
是簡單和有效的 congestion avoidance 方法。在本計畫架構中 WRED 被
實施於 AFx 的 output queues；而 BE queues 則以 Single instance WRED
（RED）來管理。傳入參數中，封包的 output_port 和 classID 用以取得
封包的真正 output queue；colorID 則用來選擇這個 output queue 的一個
RED instance。 
¾ 當封包通過了面臨丟棄的考驗後，代表了它已準備好被傳送出去。
Egress pipeline 會透過 Scratch ring 向 QM 請求對這個封包作 Enqueue 的
動作。之後 Enqueue 的動作就會交由 QM 以及位於 SRAM 的 Q-Array
硬體來完成。到了這一步，對於這個封包的 pipeline 事實上已經結束。
對於 QM 的更進一步介紹見[9]。 
19 
2.1.3.2 資料平面功能驗證與效能測試 
<功能驗證> 
 驗證 6-tuple Exact Match Classifier 與 MPLS FTN：圖 2.1.3-6 所示，
我們把一台安裝有 Ethereal 的個人電腦連接到 IXP 的 1 號 Gigabit port
作為擷取分析封包的 destination host，IP address 為 11.0.0.83。而另一台
安裝有 SjPhone 的個人電腦則連接上 IXP 的 port 0 作為 source host，IP 
address 為 10.0.0.2。Source host 利用 SjPhone 打出 SIP Call Setup traffic，
同時用 Ping 及其他的方式產生非 SIP 的 traffic。IXP 以 6-tuple Exact 
Match Classifier 根據針對 SIP traffic 所制訂的原則對封包進行分類，將
符合分類規則的封包 tunneled 到屬於 Best-Effort 的 MPLS LSP；而不符
合者則以一般的 IP 傳送。這個規則是封包必須符合 Source IP：10.0.0.2，
Destination IP：11.0.0.83，TOS：0，Protocol Type：UDP，Source Port：
5060，Destination Port：5060 等六個欄位。圖 2.1.3-7 所示為所須下達
的指令。圖 2.1.3-8 顯示了 SIP 封包確實被貼上了數值為 53 的 label。 
 
圖 2.1.3-6 Classifier 與 MPLS FTN 功能驗證實驗環境 
 
圖 2.1.3-7 Classifier 功能驗證所必須下達的指令 
 
 首先是建立將會被使用到的 NHLFE，第一個參數指定了封包在傳
送出去前所需要經過之 Egress IXP 所屬的 blade，接著必須指定
21 
 
圖 2.1.3-8 被貼上 label 53 的 SIP 封包 
 
 Metering and Marking：同樣以圖 2.1.3-6 的實驗環境，對第一個實驗
予以擴充，把 SIP 封包分類成 AF2 等級，並經由 Meter 對其予以計量，
計量的結果會以 E-LSP，Pipe Tunnel 的方式進行 Marking。 
 
 在實驗一的基礎上，以 Add6TRule 新增 qos rule 方式設定對應到
fwd 規則所需要的 flow_id、class_id、以及 color_id。flow id、class 
id、color id事實上代表了完整的PHB；而 output id則是要讓 fast path
判斷封包在 classifier 之後是要通往 Meter（1）還是 Marker（2）。 
 最後使用 qosconfig 的 AddTcm 來建立一個關聯到 flow id 的 meter 
instance。參數 Meter No.並不代表 meter instance，而是用來指定
meter 引擎，本計畫架構中只有一個 meter 引擎。<COLOR>_DSCP
與<COLOR>_OUT 代表著計量結果三種顏色所對應的 outgoing 
DSCP 以及 fast path 對封包前途（繼續處理 1 / 直接丟棄 2）的決
定。Srtcm 代表使用 Single Rate meter；接著輸入 srtcm 所使用的參
數。在這裏，由於 SJphone 進行 call setup 時，SIP 封包是以小型的
爆衝形式進入到 IXP，於是我們粗糙地將 CIR 設定為 100，總 bucket 
size 為 600，希望藉此計量到封包的不同顏色（黃色與紅色），並將
結果印記到 MPLS Exp 上去以作簡單的驗證。 
圖 2.1.3-9 所示為封包被標成 AF23 的情況，由於 source 在送出的 SIP 
Invite 時造成了短暫且很大的爆衝，在 token 空乏情況下 Meter 計量出
紅色；由於 destination 不會作出回應，於是 Invite timeout，SIP Cancel
被傳送，在 EBS 尚有 token 的情況下，封包被標成 AF22，如圖 2.1.3-10
所示。 
23 
 MPLS TE-FRR：圖 2.1.3-11 所示為驗證 MPLS TE-FRR 之實驗環
境：利用 SmartBits 以 line rate 產生 500K 個攜帶 Label 30（0x1e）
的 MPLS 封包，並從 port 0 進入 IXP；經連接 port 1 的 working link
送往 next hop；而 port 2 所連接的是 backup LSP 所使用的 outgoing 
link。接著將兩條 output links 接到一個 Gigabit hub，另掛載一台
PC 到此 hub 上來擷取封包。接著以本計畫所延伸的 NhlfeSetCreate
工具來建立一個包含上述兩組 NHLFEs 的 NHLFE Set；最後設定
一個 ILM 來對應到此 NHLFE Set，為此 flags 的值必須滿足（flags 
& 0x40）＝TRUE 的條件。在封包的傳輸過程中，我們手動移除
working link；觀察結果，發現封包確實經由 backup link 繼續傳送，
如圖 2.1.3-12，圖 2.1.3-13 所示。 
 
圖 2.1.3-11 MPLS FRR 因應 Link Failure 之實驗環境 
 
 
圖 2.1.3-12 尚未發生 Link Failure 時帶有 Label 40 的封包 
25 
<效能測試> 
 Transmission throughput vs packet size：本計畫利用 SmarBits 的
Smart Test 在 one-to-one transmission 的基礎上，以 line rate 作為輸
入來源，分別以四組測試針對不同的封包大小來評估系統的傳輸效
能：1) SmartBits 自我檢測以求取得 SmartBits 的 output throughput；
2) 測試最快速的核心路由器處理路徑效能，即針對 EF class traffic
的 MPLS label forwarding；3) MPLS 核心交換針對 AF class 並採納
WRED 機制的封包轉送效能測試；4) DiffServ MPLS 邊界路由器最
長處理路徑效能測試。圖 2.1.3-14 所示為四組測試各自針對不同封
包大小的傳輸效能；不難發現第 2 組 ~ 第 4 組的測試結果都非常
接近 SmartBits 的 output throughput；只有在小封包的情況下，存在
著些微差異；其中以 MPLS Edge 最長處理路徑的結果表現較差，
這是因為過程中 Classifier、Meter、MPLS FTN w/Marking，以及
WRED 都被執行。然而，這個差異大約維持在 3%左右；因此，傳
輸效能可謂理想。 
72.00%
77.00%
82.00%
87.00%
92.00%
97.00%
64 128 256 384 512 640 768 896 1024 1152 128 1408
SmarBits self test MPLS ILM with EF class
MPLS ILM with AF class MPLS Edge with AF class
 
圖 2.1.3-14 One-to-one transmission throughput vs packet size 
 DRR Scheduling 與頻寬保證：圖 2.1.3-15 所示為 DRR 實驗一的環
境，利用 SmartBits 產生兩條各 1 Gbps 的 MPLS E-LSP traffics 分別
以 Exp = 1 與 Exp = 5 表示，從 port 1 和 port 2 進入 IXP，並競爭
port 0 送往同一個 next hop；我們將負責擷取封包與分析流量的 PC
連接到 IXP port 0。 
27 
 
圖 2.1.3-17 DRR experiment 2 之實驗環境 
 
圖 2.1.3-18 等量頻寬的流量結果 
 
 
圖 2.1.3-19 DRR Scheduling result #2 
 Fast Re-Route 效能：回顧圖 2.1.3-13，在發生 link failure 時，最後
一個經由 original working LSP 到達負責封包擷取的 PC 之時間為
2.063013 秒；接著在 2.437457 秒開始接收到經由 backup LSP 而來
29 
2.2 虛擬私有區域網路服務控制平面模組整合與其橋接器的
實現 
2.2.1 系統架構 
我們設計的 VPLS 系統架構，如圖 2.2.1-1 所示。 
我們所採用的 LDP 協定的工作流程如下，首先透過 zebra 獲得整個網路拓樸
的資訊，再以 signaling 方式將標籤值分散至各個 MPLS 路由器，一旦標籤值確
定之後便需要更新 MPLS 的 NHLFE or ILM table。一般 Linux 的網路核心並沒有
支援 MPLS 協定，所以我們必須將 MPLS 協定加入 Linux 網路核心，而 LDP 模
組將 signaling 結束後將所獲得的資訊再透過 ioctl 方式更新 MPLS 的
NHLFE/ILM table。 
在 VPLS 服務中 RSVP-TE 負責建立外層 Tunnel LSP，而 RSVP-TE 亦是使用
ioctl 方式去更新 NHLFE/ILM table 的資訊，但 RSVP-TE 是採用 mplsadm library
去對底層作更新的動作。 
除了加入 VPLS 所需的 Signaling Protocol 之外，仍須加入 VPLS Manager 
Modules 作為整體的運作以及協調的行為。當然必須在 Linux 網路核心部分必須
加入 bridge 的功能，才能夠使得 Core Network 模擬成 bridge 的裝置。 
 
圖 2.2.1-1 VPLS 系統架構 
31 
2.2.3 VPLS 服務功能驗證 
環境介紹 
我們利用三台 PC 來建置 Service Provider Core Network，並假設我們需要提
供 VPLS 的服務，在核心網路中建置外層的 Tunnel 以及對應的內層
Pseudo-Wires，最終目的讓核心網路模擬成為一個 bridge 的裝置，進而建置出
VPLS 技術。 
我們透過以下幾種方式來呈現其成果。 
 透過 VPN Manager 建構出 VPLS Service 
 透過 ethereal 軟體擷取封包資訊 
 藉由各個常駐程式來顯示 VPLS instance 的相關資訊 
 在相同的 VPN 群組的成員，藉由 ARP table 資訊來察看是否可獲得不同
地域的成員資訊 
在相同的 VPN 群組的成員，藉由 sshd 的程式連線到彼此。 
Network Topology  
 
圖 2.2.3-1 Simple VPLS Service Model 
如圖 2.2.3-1 所示，目前提供一個 VPN 群組，其 VPN id 為 100。其中 PE1、
PE2 和 P 路由器所運行的 Routing Protocol 為 OSPF 協定，另外仍必須運作
RSVP-TE 協定用來建立外層的 MPLS Tunnel，針對內層的 Pseudo-Wires 的建立
則只需在 Edge Router 上運行 LDP 協定。此外 VPN Manager 以及 bridge 裝置的
功能也需要運行在 Edger Router 上。 
PE1 送往 PE2 的資料訊框會貼上外層 600 標籤以及內層 200 標籤的 MPLS 
shim header，當 P 路由器收到標籤為 600 的資料訊框時，則把標籤 600 切換成
800 的標籤送往 PE2。反之當 PE2 送往 PE1 的資料訊框會貼上外層 601 標籤以及
內層 100 標籤的 MPLS shim header，當 P 路由器收到標籤為 601 的資料訊框時，
則把標籤 601 切換成 700 的標籤後再傳送至 PE1。 
以下針對 VPLS 服務的成果展現，其網路拓樸如圖 2.2.3-2。我們提供一個
VPLS instance 其 VPN ID 為 100 的群組。A 電腦連接於 PE1 的 customer-facing 
33 
 
圖 2.2.3-5 PE1 的 VPLS bridge 資訊 
 
圖 2.2.3-6 PE2 的 VPLS bridge 資訊 
mplsd 顯示 mpls router 底層的 NHLFE/ILM table 資訊 
我們透過 mplsd 常駐程式來顯示底層 NHLFE/ILM table 資訊，首先說明 PE1
的底層資訊，如圖 2.2.3-7 所示。其中 NHLFE table 資訊，0x02 的 nhlfe_key 之值
代表送往 P 路由器會貼上 700 的標籤，0x04 的 nhlfe_key 之值表示將內層 200 的
標籤堆疊於外層700的標籤。另外指明送往nexthop為mpls100的封包其nhlfe_key
的值為 0x03。對於 ILM table 資訊:將 700 的標籤移除後，如果內層標籤為 100
時則將封包送達 mpls100 的 virtual tunnel interface。Cross-Connect table (XC table)
表示收到 incoming label 為 100 的 mpls 封包經由 nhlfe_key 0x03 查找進而得知如
何處理它，在這裡我們將封包導至 mpls100 的 virtual tunnel interface。 
 
圖 2.2.3-7 PE1 底層的 NHLFE/ILM table 資訊 
如下圖 2.2.3-8 所示，我們針對 XC table 作說明，收到 incoming label 為 600
的封包透過 nhlfe_key 0x02 得知將 600 標籤 swap 成 800 送往 PE2，另外一筆則
記錄將 incoming label 601 swap 成 700 並且經由 eth1 網路卡介面送往 PE1。 
圖 2.2.3-9 為 PE2 的底層 NHLFE/ILM table 資訊。PE2 與 PE1 底層資訊大同
小異故不再描述。 
35 
payload，其目的端的 MAC 位址填入廣播位址。下圖 2.2.3-11 則為由核心網路收
到的 ARP reply 封包，其外層 700 的標籤值及內層 100 的標籤值，且來源端的
MAC 位址已填入 B 主機 00:14:2a:22:c4:da 的 MAC 位址。 
圖 2.2.3-12則為 PE2收到由核心網路送來的ARP request的封包而圖 2.2.3-13
則為 PE2 送至核心網路的 ARP reply 的封包，並且貼上對應的內外層 MPLS shim 
header。 
 
圖 2.2.3-11 PE1 收到的 ARP reply 封包內容 
 
圖 2.2.3-12 PE2 收到的 ARP request 封包內容 
37 
圖 2.2.3-16 為 PE1 的 MAC Learning Table，其中 port 1 為 mpls100 介面，將
遠端 PE2 customer-facing port 所連接的 B 電腦的 MAC address 記錄在表格中。圖
2.2.3-17 則為 PE2 的 MAC Learning Table，其中 port 1 為 mpls100 介面亦有記錄
到遠端 PE1 customer-facing port 所連接的 A 電腦的 MAC address。 
 
圖 2.2.3-16 PE1 的 MAC Learnig Table 
 
圖 2.2.3-17 PE2 的 MAC Learnig Table 
39 
2.4 參考文獻 
[1] R. Braden, D. Clark, S. Shenker, Intergrated Services in the Internet 
Architecture: an Overview, RFC1633 
[2] H-J. Chao, X. Guo, Quality of Service Control in High-Speed Networks, 
John Wiley & Sons, 2002. 
[3] S. Blake, D. Black, M. Carlson, E. Davies, Z. Wang, W. Weiss, An 
Architecure for Differentiated Services, RFC2475, 1998. 
[4] E. Rosen, A. Viswanathan, R. Callon, Multiprotocol Label Switching 
Architecture, RFC3031, 2001. 
[5] F-L. Faucheur, W. Lai, Requirement for Support of Differentiated 
Services-aware MPLS Traffic Engineering, RFC3564, 2003 
[6] D. E. Comer, Network Systems Design using Network Processors, Prentice 
Hall, 2006. 
[7] B. Carlson, Intel Internet Exchange Architecture and Applications, Intell 
Press, 2003. 
[8] Intel IXA 4.1 Documentation: IXP2400 Hardware Manual. 
[9] Intel IXA 4.1 Documentation: Framework Developer Manual. 
[10] D. Grossman, New Terminology and Clarifications for DiffServ, RFC3260, 
2002. 
[11] A. Charny, J.C.R. Bennett, K. Benson, J.Y. Le Boudec, A. Chiu, W. 
Courtney, S. Davari, V. Firouiu, C. Kalmanek, K-K. Ramakrishnan, Supplemental 
Information for the New Definition of the EF PHB, RFC3247. 
[12] J. Heinanen, F. Baker, W. Weiss, J. Wroclawski, Assured Forwarding PHB 
Group, RFC2597. 
[13] S. Floyd, V. Jacobson, Random Early Detection Gateways for Congestion 
Avoidance, IEEE/ACM Transactions on Networking, 1993. 
[14] M. Shreedhar, G. Varghese, Efficient fair queueing using deficit round-robin, 
IEEE/ACM Transactions on Networking, 1996. 
[15] L. Anderson, P. Doolan, F. Feldman, A. Fredette, B Thomas, LDP（Label 
Distribution Protocol）Specification, RFC3036, 2001. 
[16] R. Braden, L. Zhang, S. Berson, S. Herzog, S. Jamin, Resource ReSerVation 
Protocol （RSVP）, RFC2205, 1997. 
[17] D-E. Comer, Internetworking with TCP/IP - Volume 1. Fourth Edtion, 
Prentice Hall, 2004. 
[18] F. Le Faucheur, L. Wu, B. Davie, S. Davari, P. Vannanen, R. Krishnan, P. 
Cheval, J. Heinance, MPLS support of Differentiated Services, RFC3270, 2002. 
41 
第3章 <子計畫二> 
內容感知與高安全 MPLS 網路之設計和實驗研
究 
本子計畫延續去年成果:以「網路處理器」加「內容檢索引擎」設計而成的
Content Aware Processor (CAP)，能使「多重協定標籤交換技術」— MPLS 網路
具有內容感知(content aware)與高速網路入侵偵測功能之後，本年度重心著重在
數位鑑識領域。百分百網路安全是無法達到的，我們需要加入數位鑑識的機制來
大量降低風險。數位鑑識是用來重建被攻擊時的現場，藉由分析得到的資訊將其
入侵者的身份、入侵的方法和入侵的途徑等證據能保存下來，用以在法律程序
時，提供具有法律效力的證據。 
此子計畫在本年度分別在理論與實作方面，均有研發成果。在理論方面，我
们根據 Random Moonwalk Trace-Back Algorithm (RMT)，提出了一套改進方案
Locality Steering Trace-Back Algorithm (LST)，以攻擊與感染的行為當作 traceback
引導依據，而不是以任意隨機為導向。經過實驗証明 LST 以突出的高效率與高
準確率優於 RMT，來辨認攻擊者的來源、攻擊途徑與攻擊影響範圍。在實作方
面，以「網路處理器」實作一套 MSN 網路鑑識系統，直接針對 MSN 的資訊稽
核，將網路上的行為進行嚴密行為紀錄，以便當資訊安全事件發生時，可以提出
有效的證據並且還原事件現場。 
 
關鍵詞：網路安全、數位鑑識、網路處理器、嵌入式系統、阻斷服務攻擊 
 
43 
些間接接觸的主機群作影響範圍的分析，大大減低還原(recovery)措施的成本。
因此我們提出了 Locality Steering Trace-Back Algorithm (LST)來幫助網路管理者
在發生類似 DDoS 或 Worm Infection 的攻擊之後，施行正確的鑑識動作，期望此
套系統能補強安全管理上的不足。 
 
3.1.2 即時通訊網路應用之盛行 
目前的網路安全設備(如防火牆與入侵偵測系統)，無法在資訊安全事件發生
之後呈現整個事件的來龍去脈，更無法提出有效證據、還原事件真相與重建犯罪
現場。雖然監視網路活動的工具非常多，但大部分的網路監視工具無法提供完整
的安全資訊，往往只能做到封包紀錄與監測並產生大量的原始封包內容紀錄，無
法直接呈現使用者行為的全貌。例如，入侵偵測系統只能提供資料收集與分析，
但不能達到資料的鑑識之功效[16]。然而，大部分的網路鑑識系統最高只能做到
網路第四層或第三層，而對即時通訊軟體之鑑識工作必須要在 OSI 第七層進行
解析與統合，才能還原與重建整個事件現場。只擁有第三層與第四層的傳統網路
鑑識系統與一般的 Sniffer 軟體是無法還原當時所發生的情況。例如：當時當事
者利用即時通訊軟體說了哪些話、傳送哪些檔案…等等，這些情境都是傳統的網
路鑑識系統無法做到的。 
另一方面，即時通訊是目前最熱門的網路應用之一，同時也日趨重要，根據
全球 IT 產業分析的領導者 Gartner Group 表示[17]：即時通訊將會變成無線電子
商務、即時共同研究合作、虛擬博奕與其他網路應用的核心。而即時通訊相關的
應用在 2006 年將達到 42 億美金的市場規模，使用人數也突破上億關卡，根據
Symantec 的預測：預計 2006 年底，即時通訊訊息的通訊量將超越電子郵件。目
前熱門的即時通訊軟體有 AOL Instant Messenger、MSN Messenger 與 Yahoo 
Messenger，其中以 MSN 的使用者最多(亞太地區)。MSN 是微軟(Microsoft)開發
的即時通訊軟體。微軟的 MSN 歷史里程碑顯示[18]，MSN 用戶每天在網際網路
上傳送超過二十五億封訊息。 
MSN 帶來無限方便的溝通模式與工作效率的提昇，但同時也隱含資訊安全
問題，因此，我們想要以嵌入式系統的架構，設計與實作一套 MSN 的網路鑑識
系統，降低 MSN 帶來的威脅，替 MSN 的應用增加一層保障。 
 
 
3.2 國內外有關本計畫之研究情況 
因為本計畫分為兩部分，此章節也就跳板式(Stepping stones)的鑑識與網路應
45 
3.2.2 網路應用層鑑識之實作 
分析收集到的證據，以期增加對證據的瞭解程度並決定證據隱含的意義與目
的，這對於事件的釐清有相當大的幫助。基本分析程序可分列如下： 
z Basic process (基本的封包處理): 將網路封包進行網路協定解碼與內容取
出。利用封包標頭解析封包內容(包含的資料鏈結層、網路層、傳輸層與
應用層等資料)，提供最原始的封包資訊與全部的內容(圖 3.2.2-1-a)。另
外，這類工具，在防火牆偵錯時特別有用，檢查是否有防火牆是否有漏洞，
並在網路 Trouble shooting 方面有獨到之處。此類工具最具代表性的就是
網路監聽軟體，如：TCPDump、Ethereal 等工具。此類工具擁有能看到封
包所有內容，並紀錄所有網路封包。但封包紀錄龐大且毫無系統性，以人
工的方式很難從封包的內容，將一個個獨立的封包重新建立關係，而且工
具本身缺乏資料分析能力。以網路鑑識的角度來看，固然擁有完整的證據
很重要，但由於資料雜亂無章，使得證據的可用性卻很低。 
z L3/L4 forensic tool：大部分網路應用使用 TCP/IP 來傳送資料，因此利用
TCP 的機制重建現場。結合 Basic process 封包處理能力與封包第三、四層
資料分析，將收集到的網路封包進行分類，以 Session 的方式呈現結果。
這類工具能將封包 Session 化(圖 3.2.2-1-b)，而經過分析之後的結果是有
系統的 Session 資訊，而不是獨立的網路封包。有了 Sesssion 的處理之後，
對於網路活動與證據的呈現，增加不少可讀性與可利用性，此類工具如：
Argus、Ibmonitor 等工具。然而，這類工具擺脫不易判讀的網路封包，以
Session 的角度檢視收集到的證據，雖然封包經過“Sessionalize”可以增加其
可讀性，但是“Sessionalize”後的資訊卻不足以表示隱藏在封包內的資料，
無法從 Session 提供的資訊發現應用層的資料。 
 
Packet Packet 
Packet 
Packet
Packet 
Packet Packet 
Packet 
Session 2 
Session 4 
Session 5 
Session 3 
Session 1
User 
User 5 
User 
Session B2            4 
Session A2          4 
Session C 4                    2 
Session E2               4 
Session F 4         2 
  
(a) Basic process (b) L3/L4 forensic tool (c) Application forensic tool 
47 
 
表格 1 資料重建技術比較 
 Basic process L3/L4 forensic tool Application forensic tool 
分析速度 快 中 慢 
分析複雜度 低 中 高 
證據可讀性 劣 中 佳 
特性 資料詳細完整 掌握資料流向 分析結果易於人類閱讀 
MSN 是目前即時通訊軟體中的當紅炸子雞，但目前 MSN 應用服務上缺乏網
路鑑識機制，因此希望能以網路處理器實作網路鑑識系統，為以 MSN 為基礎的
即時通訊工具加上更完善的安全機制，減低安全風險與威脅。我們提出了
「MSN-shot」作為 MSN 應用之網路鑑識系統(MSN Network Forensic System)的
解決方案，並實作於 Intel IXP425 之上。誠如其名，「MSN-shot」會把 MSN 之各
種活動一一拍照下來，以作為往後的數位證據。 
3.3 結果與討論 
此章節也分別說明計畫兩大方向之進展。 
3.3.1 Locality Steering Traceback Algorithm 
在長時間觀察蠕蟲的感染行為後[9][10]，我們發現到蠕蟲的感染行為其實擁
有許多共同的特徵，我們將這些共同的特徵稱作 locality。相同種類的蠕蟲在擴
散時，感染節點的行為之特徵都大致相同。因此當我們在追蹤蠕蟲擴散的路徑
時，我們可以依據該種類蠕蟲的擴散特徵來進行 traceback。如此可以使系統在追
蹤蠕蟲的攻擊來源時較有規則可循，不僅僅可以縮短系統運作時間，在準確度上
更是大幅度的提升，再者其 false positive rate 及 false negative rate 也可以降到最
低。根據蠕蟲擴散行為特徵而我們定義出的四種 locality： 
z Strategy Locality:在此部份，我們訂出網路上各節點的安全層級 (security 
level) －Si，i 表示在網路中的各節點。當節點的安全層級愈高，也就代表
此節點受到 IDS、firewall、防毒軟體或是一些安全設備的保護愈多，相對
地受到蠕蟲感染的機會也就越低。當系統在追蹤時，我們便可以依據此安
全層級 Si 作為追蹤的依據；當網路上某節點的安全層級 Si 越低，在追蹤
時我們就會給予較高的機率對其追蹤。安全層級 Si 也可將其表示為此次蠕
蟲擴散所會影響的作業系統，當網路上一節點為此次蠕蟲爆發擴散所會波
及該作業系統時，我們也會給予一較高的機率對其追蹤。例如 Ramen、
49 
對 Nimda 這隻蠕蟲來說，其對於以 Win32 為作業系統的節點時，就要給予
較高的機率對其追蹤，並且在節點受感染的 10 分鐘內 (bomb period) 就會產生
超過 3000 條 fan-out flow。而 Code Red 的特徵是在 10 分鐘內會產生許多目的端
port 為 80 的連線，對設有 IIS 的節點進行弱點攻擊。而當一節點受到 Code Red 感
染時其 fan-out 的數量減去 fan-in 的數量大約為 100 以上 (fin ＜ fout，fout － fin 
≧ 100)。我們可以以此特徵將 Code Red 感染行為的描述參數當作 traceback 時
的選擇依據。同樣的 MsBlast 擁有與 Code Red 類似的行為模式，其鎖定的 port
為 139，並且蠕蟲設計有對不同網段以不同機率掃描的作法，也就是 Spatial 
Locality 的特徵。 
我們提出了 Locality Steering Trace-Back Algorithm (LST)，根據 Worm 攻擊
與感染行為中所觀察到的蠕蟲四種 locality 做為 traceback 之依據。利用 LST 不
僅可以在網路爆發大規模攻擊時，可以迅速找出最原始的感染來源，並確實可做
到 Attacker Identification 及 Attack Reconstruction。LST 的 pseudo code 如圖 
3.3.1-1 所示，其中參數說明於表格 3。 
 
圖 3.3.1-1 LST pseudo code 
表格 3 LST 之參數說明 
 說明 初值 
W 演算法施行一次時所必須取樣的次數。 輸入 
d 當每次取樣時所能處理的 edge 最大值。 輸入 
△t 
此參數控制著一時間間隔，並以此時間間隔內是否有
edge 存在來判斷是否繼續此次的取樣或是跳到下次
取樣(the sampling window size)。 
輸入 
begin 
while( W_walk != W) do 
choose an edge e1(u1, v2) according to L (u1: Source, u2: Destination); 
the count of e1 + 1; 
d_walk++; 
while( d_walk != d && ∃(e) in the △t) do 
choose an edge e2(u2, v2) according to L (v2 = u1); 
the count of e2 + 1; 
d_walk++; 
end while 
W_walk++; 
end while 
return the n edges with the highest counts 
end 
51 
 
圖 3.3.2-2 False positive rate vs. walks 
 
下圖（圖 3.3.2-3）Ｘ軸代表回傳多少個 edges 的數目，Ｙ軸代表 False Negative 
Rate，其結果為取樣 300 次之後運算而得。由圖 3.3.2-3 可以發現三件事情： 
z 由整體觀察而知，LST 之 False Negative Rate 比 RMT 來得較小。 
z 當回傳計數值為前 6 大之 edges 時(n=6)，LST 及 RMT 之 False Negative Rate
都會下降到一定程度並漸趨穩定。 
z 當回傳計數值為前 16 大的 edges 時(n=16)，LST 之 False Negative Rate 會降
到一低點，且比所有 RMT 之結果還要低。 
 
圖 3.3.2-3 False negative rate vs. returned edges 
下圖（圖 3.3.2-4）中Ｘ軸表示每次運行所 walk 的次數，Ｙ軸表示為運行一
次演算法所需花費的時間，而其結果是由回傳計數值為前 20 大之 edges(n=20)所
得來的。由圖 3.3.2-4 我們則可以發現兩件事情： 
z LST 在執行時所花費的時間小於 RMT 所花費的時間。 
z LST 在取樣次數少以及取樣次數多時所花費的時間相差不大，而 RMT 會在
取樣次數愈多時花費愈多時間，互相比較起來 LST 較 RMT 來得穩定許多。 
 
53 
而在 PCI 上使用了一般 Fast Ethernet 的網路卡，用來連接 MSN-shot EAS 
Server。MSN-shot sensor 程式核心 (MSN-shot core) 主要是以 Intel 所提供的
IXP400 Software Architecture 為藍圖，其所在位置與 Intel 的 Codelet library
是一樣的。MSN-shot core 是以 Kernel module 型式執行，它可以透過 Access 
layer 中的 ixEthAcc API (Intel 所提供的基本函式庫)，依照不同的需求，撰寫
封包處理過程。換言之，MSN-shot core 是作業系統與硬體底層之間溝通的
驅動程式。為了要能夠即時收集證據，MSN-shot 採取在線模式(In-line mode)
來運作： NPE 截取封包後，轉交由 Access layer 中的 ixEthAcc API 處理，
之後再交由 MSN-shot core 作主要網路行為鑑識。由資料收集引擎，將封包
複製給資料分析引擎盡情處理，同時直接將封包下傳到 Access Layer，使用
ixEthAcc API 將封包由 NPE 送出。以下是軟體各部分的功能性介紹： 
z 收集引擎(Acquisition engine)：在收集引擎中包含 Filter、Collector、Session 
management 與 Classifier 四個部份。 
[1] Filter: 在這麼多的封包流量下，系統如果要有效率的擷取鑑識所要的封
包，就必須要有準確的網路封包過濾系統，Filter 是 MSN-shot 架構最前端的
資料處理器，此 Filter 可以準確判斷出該封包是否屬於 MSN 通訊協定或我
們想要監視的網路封包，系統即時呼叫下一個單元 Collector 去複製封包 
[2] Collector: 在經過 Filter 過濾封包後，該單位就會將封包複製到後端，
這也是整個系統可以保持鑑識資料完整性。 
[3] Session management: MSN-shot 被設計用來同時對很多正在進行通訊的
資料進行鑑識，以即時通訊的應用來說，會有一對多、多對一與多對多這些
情形發生，這時必須有一個 Session 的管理者整理些正在進行通話的
Session，讓後端的分析引擎可以依據 Session management 提供的資料，完成
資料重建。 
[4] Classifier: 對網路封包內容進行分類，分成三類：即時通訊訊息、VoIP
封包檔案傳送，依據分類結果將封包送到後面所屬的 Parser。 
z 分析引擎(Analysis engine)：分析引擎利用 Parser 與 Reconstructor 負責資
料分析與重建。 
[1] Parser: 經過 Collector 將封包完整的複製完後，系統將呼叫 Parser 完成
封包內容解析，在 Parser 的部分，我們細分三個子系統，分別為：MSN 
Parser、Message Parser、Voice Parser、File Parser。分門別類將屬於 MSN、
文字、語音跟檔案分別擷取出來，讓之後的系統可以更有效率的管理資料，
達到便利且有效率的網路鑑識系統。 
[2] Reconstructor: 當前面的 Parser 分門別類的將資料型態整理出來後，即
將交給 Reconstructor 作資料狀態與個別使用者的重組動作，系統利用資料
55 
式或雙位元組(Double bytes)文字(如：中文字或是 Unicode)對內切割過的封包內
容進行搜尋，TCPDump 與 Argus 皆無此功能，有支援此功能的有 MSN-shot、
NetDetector 與 NetIntercept。Read-only collection 能使系統在收集證據的時候工作
在唯讀模式，系統不會對 ARP(Address Resolution Protocol) request 回應，並且使
用離線式的 DNS，而 TCPDump 與 Argus 都必須視作業系統而定，工具本身無法
決定與控制，然而 MSN-shot、NetDetector 與 NetIntercept 都有支援此功能，
MSN-shot 之網路鑑識介面其工作模式是橋接器(Bridging)模式，網路介面本身沒
有 IP 位址，以唯讀的工作模式進行網路鑑識。Integrity checking mechanism 能夠
保證收集的證據與目前顯示給使用者的資料是完全符合的，在這項功能比較中，
只有 MSN-shot 能夠做到，MSN-shot 在收集到證據的第一時間，立即把證據利用
MD5 演算法產生雜湊碼，然後放置安全的地方，當使用者調閱相關證據的時候，
對取出的證據再做一次 MD5，並且把結果與證據收到時產生的 MD5 雜湊碼比
對，以保證目前看到的據與最初的證據是完全相同的！TCPDump、Argus、
NetDetector 與 NetIntercept 無支援此機制。我們可以從 MSN support 了解工具有
無支援 MSN 協定，在 MSN support 中只有 MSN-shot 與 NetDetector 能解析 MSN
封包內的訊息，而 TCPDump、Argus 與 NetIntercept 無法解析其內容。最後的功
能比較是管理介面，方便人性化的管理介面能讓管理者與使用者更容易掌控系統
運作與訊息，TCPDump 與 Argus 是文字模式的介面，使用者在文字指令模式下，
執行並且根據使用經驗下達參數，對證據搜尋與分析都必須配合其他文字處理工
具以人工的方式進行，NetIntercept 本身就是一個應用層式，使用者利用操作視
窗系統的習慣可以用滑鼠在應用程式裡進行證據查看，但使用者必須在主機端或
是利用 SSH 的方式進入操作，MSN-shot 與 NetDetector 都是使用簡捷與人性化
的網頁管理介面，在 MSN-shot 中，使用者可以利用瀏覽器，通過首頁認證之後
開始使用網路鑑識系統，MSN-shot 網路鑑識系統所有鑑識資訊的取得盡在彈指
之間，透過滑鼠移動、選擇與點閱，輕輕鬆鬆操作 MSN-shot。 
57 
1. 鄭伯炤、廖威捷、蔡子浩, "LST: An Efficient Traceback Algorithm Driven 
by Worm Infection Locality", accepted by 『2006 全國電信研討會』 
2. B.C. Cheng and Huan Chen, “Context-Aware Gateway for Ubiquitous 
SIP-Based Services in Smart Homes”, accepted by the First International 
Workshop on Smart Home (IWSH 20006) 
3. Huan Chen and B.C. Cheng, “Smart Home Sensor Networks Pose 
Goal-Driven Solutions to Wireless Vacuum Systems”, accepted by the First 
International Workshop on Smart Home (IWSH 20006)   
4. B.C. Cheng and Huan Chen, “Evidence Collection with Quality of 
Assurance (QoA) for Network Forensics,” The 7th International Workshop 
on Information Security Applications (WISA’06), 2006, pp.265-276 [EI] 
5. Hui-Kai Su, Huan Chen, Bo-Chao Cheng, and Cheng-Shong Wu, 
"Performance Analysis of Bandwidth Provisioning for AF+ VoIP Service 
Models over DiffServ-based MPLS Networks", International Symposium 
on Performance Evaluation of Computer and Telecommunication Systems 
(SPECTS'06)  
 
 業界合作成就: 藉由本計畫成果，與業界合作與互動如下 
 工研院合作開發 network forensic 技術研究，提供產界相關技術以分享
本計畫成果。 
 本實驗室因本計劃之執行而成為 Intel 所支持實驗室，獲得 Intel 之建教
合作案，並維運管理 IXP425 developer forum (ixp.comm.ccu.edu.tw)。在
今年 5 月也主辦一場 IXP425 研討會。 
 
 社會責任: 藉由本計劃執行也負起社會教育之責任，因此在國內 RUNPC 雜
誌撰寫兩篇 network forensic 相關文章。 
1. 鄭伯炤、張維騏, “網路鑑識概觀(上) ⎯ 網路安全事件管理”, RunPC, 
08/2006, p. 103-107 
2. 鄭伯炤、張維騏, “網路鑑識概觀(下) ⎯ 網路鑑識工具簡介”, RunPC, 
09/2006, p. 122-128 
 
 人員成就:藉由此計畫中不斷的實驗及討論，使得計畫成員不斷的精進自身
的知識及經驗，並不斷複製所得到的成果，分享於實驗室其他成員，進而使
本實驗室在網路處理器及資訊安全兩個領域均有長足的進步。 
 
59 
[17]. Gartner, Inc. Gartner’s Instant Message Survey Show America Online Leading 
Microsoft. Press Release May 1, 200. 
[18]. MSN Historical Timeline, 
http://www.microsoft.com/presspass/press/2002/Nov02/11-08MSN8GlobalTime
Line.mspx. 
61 
4.1 子計畫三之研究背景 
本子計畫為進行網路傳輸與運算資源監控與管理，我們預計設計 NRB 
(Network Resource Broker) 與 CCA (Call Control Agent)。NRB 是網路傳輸資源
管理者，負責整體網路傳輸與運算資源管理，進行網路資源分配管理與網路資源
使用最佳化, 及網路監督與決策者，負責整體網路傳輸資源監控、監督管理與事
件決策處理；CCA 是網路傳輸與運算資源與內容傳輸協調者，負責與 CDP 
(Content Delivery Processor) 及使用者進行資源協調，當使用者對網路有特殊需
求時，CCA 會判斷如何滿足使用者的請求，並將請求需求轉成網路服務品質參
數向 NRB 借調網路資源，如果資源借調無誤，NRB 再以 On-line 方式向網路
傳輸與運算設定資源給該用戶使用。透過這三個元件，我們可以建構一完整網路
資源監控與管理系統。 
 
 
圖 4.1-1： 子計畫關係圖 
 
透過管理系統的設計，本子計畫在子計畫一 Diffserv/MPLS 網路外圍，建
構一 Content Aware 整合網路，使用者不需要瞭解下層是透過何種網路傳輸，使
用者只需要與 Content Aware Network 協調與並放心地將資料交給 Content 
Aware Network 傳輸即可。由整體網路來看，使用者有兩個介面與 Content Aware 
Network 溝通。第一種方式，使用者必須知道 CCA 的存在，當有特殊傳輸資源
需求時，必須由使用者直接向 CCA 提出 Content Aware Request。第二種方式，
使用者不需要知道 CCA 的存在，使用者只需要像使用一般網際網路服務一樣，
63 
 
圖 4.1-2：子計畫間訊息交換關係圖 
 
本計畫主要研究範圍包括：Transport Network (MPLS) QoS Management 
Architecture 的研究、Content Aware 資源監控與管理、Security Event 處理與 
Surveillance、實驗平台建置。依序描述如下： 
4.1.1 Transport Network (MPLS) QoS Management Architecture  
Transport Network (MPLS) QoS Management Architecture 內容可分為 
Transport Network QoS Management 與 Transport Network OAM (Operation and 
Management)。Transport Network QoS Management 提供網路 QoS 管理與資源分
配系統，以達到網路資源使用最高效益；Transport Network OAM 提供一完善網
路操作與管理環境，給予更佳網路服務品質。 
4.1.2 Transport Network QoS Management 
MPLS Network QoS Management 在於提供網路 QoS 管理機制，並提高網
路資源使用之最高效益。『網路 QoS 保證』與提高『網路效益』是一兩難問題，
如何滿足使用者對網路服務品質的要求，又要能提高網路效益，這做網路資源管
理一直以來所必面臨的一大挑戰。線路交換網路 (Circuit Switching Network) 以
服務品質為導向，因此使用者在建立通道後，可以得到一穩定且足夠之資源，但
是以網路提供者來說，網路效益不高；然而，分封交換網路 (Packet Switching 
Network) 以網路效益為導向，提供 Best-Effort Service，但是當網路壅塞時，大
65 
並實作於 IXP Diffser/MPLS Router。與其他子計畫部分，將研究如何進行網路資
源協調，該使用何種通訊協定進行協調，如：CCA 與 CDP (Content Delivery 
Processor) (子計畫二)、CCA 與 Multimedia Proxy (子計畫四) 間之互動。 
Link
Control
MPLS
Diffserv
NRB
Diffserv ER
Diffserv ER
Diffserv ER
CR
CRER
ER
CDP
CDP
 
圖 4.1.2-1：NRB 與 LSRs 互動圖 
4.1.3 Transport Network OAM 
由於傳統 IP 網路對 OAM 功能支援的不足，然而每當 IP 網路線路出現問
題時，往往造成使用者許多困擾，因此在 MPLS 技術成熟的同時，我們必須在 
MPLS 上設計 OAM 功能，提供一完善網路操作與管理環境，提供更佳網路服
務品質。目前並未有任何標準規範，建議 MPLS OAM 如何實作，而且 MPLS 環
境與傳統電信網路也有所差異，因此在 MPLS OAM 的研究與設計上，尚有許
多可以努力的空間。 
本子計畫在 MPLS OAM 部分預計實現如下功能：Inter-AS 以及 Intra-AS 
MPLS OAM 管理、LSP QoS 狀態偵測機制、網路錯誤通報機制。在 MPLS OAM 
管理上，我們將分為兩層次，Intra-AS 與 Inter-AS，如下圖 4.1.3-1 所示。Intra-AS 
MPLS OAM 負責 AS 內部網路操作與管理，當內部網路發生問題時，內部可以
自動協調找出問題點，並將事件回報給 NMSM；Inter-AS MPLS OAM 負責 AS 
與 AS 間網路操作與管理，以使用者角度來看，Inter-AS MPLS OAM 才能達到 
End-to-End QoS Monitor。另一方面，Intra-AS and Inter-AS MPLS OAM 管理與
LSP QoS 狀態偵測機制必須仰賴偵測封包，如 IP 網路之 ping 封包，用來偵測
網路狀態，統計網路品質數據。在 MPLS 網路使用偵測封包，目前有兩種作法，
但皆未標準化，一種方式是擴充 IP 網路之 ICMP format，使之支援與攜帶 
MPLS OAM information；另一種方式是利用 MPLS Shim Header 中預留的欄
位，來攜帶 MPLS OAM information。無論哪種作法，各有其優劣，因此在本子
計畫中，將評估其優缺點，並設計一合適之偵測機制。此外，我們將設計一網路
錯誤通報機制，當網路或 LSP 出現問題時，將錯誤事件回報給 NMSM，給 
67 
4.2 成果與討論 
4.2.1 Content Aware Network (CAN) overview 
Content Aware Network 是一種網路型態，這種網路可以有智慧的了解使用
者請求(content request)、如何找到使用者以及如何用有效率的方式傳送使用
者需要的資料，傳統的 routing protocol(OSPF、RIP…)基本上是以 IP address
為 routing 的 index，而 Content Aware Network 在 routing 除了看 IP address
之外，還會看更上層 header 的內容，Content Aware Network 可以較有智慧的
方式使用網路資源，因此提供比傳統的網路更具智慧與更有效率的服務，並且透
過本計畫與子計畫二的合作，可提供網路安全以及 QOS 的保障，因此使用者可放
心的使用我們所提供的網路，可以沒有網路安全的顧慮。 
在本計畫中主要的元件包括 Call Control Agent (CCA)、Network Resource 
Broker (NRB)以及 IXP2400 MPLS edge router (gateway)，做 Content Aware
資源管控 CAA 扮演中央控管(centralized)的角色，主要管理 SIP 應用的服務，
所有需要用到我們所提供的服務的使用者都把 request 傳送到 CCA，CAA 會根據
與使用者所簽訂的 service level agreement (SLA)分配網路資源(LSP)給該使
用者，假使 CAA 所掌控的網路資源(LSP)不夠用，就會請 NRB 再借調新的網路資
源(LSP)，而分配網路資源(LSP)的動作是透過向 IXP2400 MPLS edge router 下
達設定 classification rule，CAA 再針對 request 裡的內容把 request 傳送到
目的地。在此 IXP2400 MPLS edge router 做為 SIP service 的 gateway，CAA
為 gateway controller 的角色，而 NRB 為 resource manager 的角色。 
 
4.2.2 Call Control Agent (CCA) 
在本計畫中，我們所選擇 SIP 為 signaling protocol，接下來就開始介紹
SIP 在本計畫中與各個元件互動的情況。 
69 
 
圖 4.2.2-2：Signaling flow chart 
圖 4.2.2-1 與圖 4.2.2-2 建立通話的流程最主要不同在於接收到 Invite 的
200 OK message 以及在接收到 BYE 請求時。當收到 Invite 的 200 OK message
時，必須為該通話配置網路資源，根據當初客戶與我們簽訂的 SLA 合約來配置是
當的網路資源給該使用者，也就是將該通通話導到適當的 LSP 裡，我們必須透過
NRB 來配置 LSP 的資源，LSP 的 class 分為 EF、AF1、AF2、AF3 以及 BE。CCA 傳
max_resv_bandwidth、min_resv_bandwidth、LSP class、gateways、、、等參
數給 NRB，讓 NRB 根據 CCA 的需求配置 LSP 網路資源，並回覆 LSP ID 給 CCA，此
時，CCA 就可以下達建立 classification rule(configure rule)到 gateway，
因此通話將會被導至適當的 LSP 裡。當通話要結束時，之前所配置的 LSP 資源必
須被釋放，並且更新LSP的使用狀態，CCA下達刪除classification rule(delete 
rule)到 gateway。 
如圖 4.2.2-3，在網路裡會有已經建立好了 LSPs，CCA 有能力去分配 LSP 的
使用，換句話說，CCA 要根據使用者當初與我們簽約的 SLA 分配 call 到 LSPs 裡，
而這些 LSPs 的建立交給 NRB 去處理，CCA 的工作只是決定如何使用 LSPs。 
71 
4.2.4 Network Resource Broker 
網路拓樸,如圖 4.2.4-1 所示，NRB 在網路環境獨立出來，不在 Routing Domain
裡，因此不會和網路環境中的路由器交換路由資訊，但 NRB 又必需要獲取網路環
境的資訊，才能針對提供網路資源的控管，所以額外寫程式去同步 Router (Intel 
IXP 2400 網路處理器)間的網路資訊，並建立一個  Resource Database 後，然
後事先規畫網路的資源，即網路頻寬，如圖 4.2.4-2 所示，把一條單向的 Link
分為六個服務等級，服務等級高低先後為 EF、AF1、AF2、AF3、AF4、BE，每個
服務等級裡給固定的頻寬配額，希望藉由這種作法能夠以事先配額的分式來增加
或調整網路的效能，並對某些客戶提供更高級的服務品質。在算路方面可分為事
先計算最佳路徑(Off Line)和即時去計算最佳路徑(On Line)，Off line 的方法
則是事先根據和 VoIP Service Provider 簽定的 Service Level Agreement 
(SLA)，事先為使用者建立符合 SLA 所要求的服務等級的路徑，像要多少頻寬、
那種服務等級、多少的Delay限制、小於多少的Loss…的QoS路徑，這種Off-line
建立路徑的好處，可以減少建立 LSP 所花費的時間和減少 NRB 的負載，主要是應
用於大頻寬的 Traffic，像網路影像的傳輸。而 On Line 而言，則是針對即時性
的語音或影像，而 NRB 可以臨時根據目前網路狀況動態建立符合 QoS 要求的路
徑，以達到網路最高的效率及使用率，而均勻地分配資源，不至發生資源浪費或
擁塞的情況。在算出一條最佳路徑之後，NRB 需要對 Ingress Router 進行組態，
進而把參數傳給 RSVP-TE Daemon，然後建立一條 LSP。 
在這個網路架構中有個重要的網管協定，Simple Network Management 
Protocol (SNMP)方面,先簡單介紹其運作，在 SNMP 裡，NRB 扮演一個 Network 
Management Station (NMS)的角色，而圖 4.2.4-1 裡所有 Routers 為 SNMP Agents, 
NMS 負責收集網路的情況，透過收集相關的 SNMP MIBs 得知，然後 NMS 透過網管
軟體向所有的 SNMP Agents 裡 SNMP Daemon，收集相關 MPLS MIBs，方便去做統
計和得知網路的健康及使用狀況，而當網路有發生突發情況時，如：網路斷線、
機器重開機…等情況，SNMP Agent 可以主動發送 Trap 的訊息去通知 NRB，讓網
路管理者得知目前網路發生什麼狀況，以方便進行處理。而 SNMP 在子計畫中扮
演的角色，主要是當建立一條最佳化的路徑後，需要對此提供網路即時監控和量
測，才能夠保證網路服務品質，然後如何去得網路品質，則是透過 SNMP ，當然
網路量測要從底層 Data Plane 去負責量測的工作，而這是子計畫三裡的另一部
份，是要實作 MPLS OAM 並要量測網路中的一條 LSP 的 Jitter、Loss、Delay…
等 MPLS 量測值，而這些量測出來的值，必需要透過一個 Process，稱為 MPLS OAM 
Process 定期去更新和管理這些變數，然後此 Process 透過 XAgent Protocol 
(SNMP Agent Extensibility,參考 RFC 2257)和 SNMP Daemon 做溝通及連繫，當
有網管軟體 Query 這些 MPLS MIBs 值時，SNMP Daemon 會透過 XAgent Protocol
向 MPLS OAM Process 索取相關的值，然後 SNMP Daemon 再把此值回應給網管軟
體，然後網管軟體在進行統計的工作。  
 
73 
要是記載一些目前網路的可用的資源、網路使用狀況、
Metric、像網路目前剩餘頻寬…等 TE 資訊，用來計算一條最
佳路徑。 
b. 網路拓樸收集（Topology Collector）：主要是透過 OSPF 提供
API 來收集網路上之拓樸和資源的資訊並寫入 Resource 
Database 來供具有服務品質之路由（QoS Routing）運算路徑。 
c. 具有服務品質之路由（QoS Routing）：主要是要對網路的使用
率做完善的規劃，除了根據使用者的需求來算出一條符合使
用者需求的路徑之外，必須要提升網路的使用率及平均每一
條連線的負載；為了避免流量集中於固定的路徑上，造成網
路擁塞發生，減少網路故障的因素。具有服務品質之路由根
據其功能劃分為： 
 Off-Line：這只會在開始網路服務時，根據 SLA DB 
（Service Level Argument Database）內的參數建立符合的
路徑。此算路機制為了提供較高的網路使用率，所以在對
於建立備援路徑（Baclup path）上引入頻寬分享
（Share-Bandwidth）的概念以減少頻寬的浪費。 
 On-Line：當開始提供網路傳輸服務之後，無論是網路管
理者透過 Web 介面來驅動建路或是使用者透過 customer
介面來要求建立路徑及增加頻寬等要求，都會透過它來找
到符合需求的路徑。 
Fault and Performance Management 方面 
  監控網路中 LSP 的使用情況要跟本計畫中的 MPLS OAM 搭配實
作，由 MPLS OAM 量測出來的值，把它放進 MPLS MIBs，而 NRB 在透
過網管軟體，去讀取並統計 MPLS MIBs, 而此方面提供的功能如下: 
a. 實作 SNMP Agent:主要為提供有 SNMP 的 Router ,所以要在
Router (以 Intel IXP 2400 平台)上 Porting SNMP Daemon,
使其 Router 成為 SNMP Agent.。 
b. 擴展 SNMP MIBs:擴展 SNMP 的功能，擴充 SNMP Management 
Information Base(MIB)s，使 MIBs 支援 MPLS,。 
c. 建立新的 SNMP MPLS QoS 模組:新增一個 MPLS Process,而這
個 Daemon 的功能是把 IXP 2400 裡的 Data Plane 統計有關
MPLS OAM 量測的值, 透過 XAgent 和 SNMP Daemon 溝通，此種
作法的好處，可以不用修改 SNMP Daemon，而以類似外掛的方
式去掛載 MPLS MIB Module，而且可以很有彈性擴充所要加的
MPLS MIBs。 
d. SNMP Trap 訊息:當 SNMP Agent 發生一些例外情況時，例如斷
線或是重開機時，SNMP Agent 可以傳送 Trap 訊息，通知 NRB，
75 
Intra D
om
ain 
Interface
 
圖 4.2.4-3：NRB 的元件圖 
其它部份,NRB 和本計畫中的 Call Control Agent (CCA)之間存在溝通介面，
例如:當 CCA 遇到 VoIP 使用者需要即時性的影音時，而 CCA 目前沒有一條 LSP
可以使用，CCA 則要向 NRB 動態要求一條 LSP，而 NRB 則會是計算一條最佳路徑，
並通知網路中的 Router 建立 LSP，而把此條 LSP 提供給 CCA。而 NRB 和子計畫中
的 CCA 溝通方式目前的規畫是透過 Unix Socket 然後自訂 Protocol 進行溝通。 
 關於 QoS Routing 方面，一方面是分為兩個過程，第一個過程在最佳化每條
單向的 Link 中的 Metric 和頻寬，至於要如何最佳化，目前尚化有結論，因此圖 
4.2.4-4 中每條單向的 Link 的 Metric 和頻寬都是手動設定的，為一個參考值，而
每條單向 Link 的(X,Y)中的 X表示 Metric、Y 表示頻寬。 
而第二個過程，是根據每條 Link 中的(X,Y)，而如何求出一條最佳路徑，之
前有提過。而下以圖 4.2.4-4 則為測試環境，針對目前而言，NRB 則是在遠端，
不在 Routing Domain 裡，因此必需要透過額外的機制，才能從 Router 進行同步
的動作，A、B、C、D四台 Routers 則是執行 Routing Protocol，在此選用免費
軟體 Quagga Routing Suite，而四台 Router 的代稱均以 Router ID，而圖 4.2.4-4
的 IP 省略了 192.168，以後兩碼代替，而紅色的 IP 位址是 Router ID,而 Router 
ID選定原則為Router中所有參與Routing Domain的網路介面中的IP位址最大。 
77 
 
圖 4.2.4-6：Class Level 
其中，核心網路為支援 Diffserv 和 MPLS-TE 的網路，所以提供一個可靠
且具有服務保證的網路。我們除了將 Traffic 分為 DiffServ 的 Forwarding  Class
之外，我們還分三種不同的存活能力來提供更多的服務選擇，如以下所示： 
9 Class 1 （Local Protection and Path Protection）：這是保護等級
最高的。當實體層發生故障時，MPLS FRR（Fast Reroute）
會馬上切換到備援路徑，讓使用者感覺不到故障發生；當是
LSP 發生故障時，透過 MPLS OAM 偵測發生故障的路徑並通
知 NRB，NRB 會根據制定的決策來做處理。除了故障偵測和
處理之外，我們亦對這個等級的路徑做效能監控（Performance 
Monitor），包括：delay、jitter、loss、、、等等，針對網路環
境會影響多媒體傳輸的因素監控，來證明所提供的網路服務
是否符合使用者的需要。 
9 Class 2 （Path Protection）：這個保護等級為中等的。只針對
LSP 發生故障作偵測和處理，但是不包括實體層發生故障的
情況。 
9 Class 3 （IP-based Recovery）：不做任何保護的機制，靠網路
協定（如：OSPF、IGP）來驅動重新算路，是屬於拓樸驅動
（Topology-driven）。 
接著，從網頁介面，如圖 4.2.4-7，下達參數建立一條 Class2 LSP，網頁會
在 NRB 背景裡呼叫算路徑的程式，接著算出一條最佳化的路徑，然後更新網頁，
呈現路徑，這時我們可以發現這條路徑，滿足每條單向 Link 128 Kbps 的頻寬限
制，然後這條路徑的每條 Link 的 Metric 的總和是最小，經過 192.168.2.0、
192.168.5.0 兩條 Link。 
79 
當網路發生故障時，對於不同存活能力的 LSP 的影響都不一樣；在此會說明
故障的發生對於各個不同的存活等級有何影響，以及不同的存活等級對於故障的
處理。 
當網路發生路徑斷線時，如圖 4.2.4-8 所示，路由器 B 和 D 的路徑斷線，
我們看 Class 1 的會如何處理這個問題。 
 
圖 4.2.4-8  Link(5.3->5.2)斷線後的網路拓樸 
 
如下圖 4.2.4-9，當路由器偵測到 Link 故障時，發現有一個 Class 1 的 LSP
－LSP ID 27，經過此 Link。因為 Class 1 的 LSP 建立時，會和 backup path 做
binding 動作，所以當發現 Link 斷掉，會馬上以 Label stacking 的動作把封包
導入備援路徑，讓使用者感覺不到網路出問題。 
 
81 
 
圖 4.2.4-10 Link failure handle for class 2 
 
關於 Class 3 的故障處理的例子，我們以 Link(192.168.5.3 -> 192.168.4.3)
斷掉為例子，如圖 4.2.4-11 所示。 
 
圖 4.2.4-11 Link failure situation for class 3 
因為 Class 3 是 Topology-driven 的方式，所以當 Link(5.3 -> 4.3)斷掉
時，NRB 並不會馬上重新算一條路徑，而且會等 OSPF 收斂完，更新完 Resource DB
後，才會觸發 NRB 去重新一條路，如下圖 4.2.4-12 所示。 
 
圖 4.2.4-12 Link failure handle for class 3 
 
83 
 MPLS MIBs 
 削減假警報、偵測 DoS 攻擊。 
 
 解決底層 LSP 斷線問題 
a. LSP 斷線問題  
 
使用 IP PING 能到達的 LSRs，並不代表此設備上的 LSPs 就會正常運作。雖
然 RSVP-TE 與 MIBs log 的紀錄，也可以作為偵測斷線的功能，考慮特別的應用
如 L3 VPN，這些都不是最有效益的考量，另外 RSVP-TE 也無法偵測因為路由訊
息錯誤所產生的斷線問題！ 
想要偵測 LSP 斷線訊息，最主要的目的就是希望在用戶抱怨前提早偵測並解
決，除了被動等待硬體層次發現問題外，我們也可以透過不斷地從 ILSR 到 ELSR
發 probe封包主動偵測特定重要的LSPs,這也是LSP PING的主要精神。圖 4.2.5-2
提出 LSP 發生問題的各種型態：<1>封包遺失，<2>錯誤繞送，<3>swap 錯誤，<4> 
Mismerging， <5> loop，錯誤複製封包。LSP PING 定期自 A 發送至 A’可以偵
測<1>~<4>的狀況，加上欲偵測 CRs 的封包，便可以偵測<5>的狀況。 
A A’
B
B
A’
A A’
A
A A’
B’
B
A’A
copy
loop
<1>
<2>
<3>
<4>
<5>  
圖 4.2.5-2：LSP 錯誤型態 
   
b. LSP PING 能達到的 MPLS OAM 大部分的需求 
 
 MPLS 基於多重協定的因素本來在管理上就不是單一機制就能實現全部，考
慮在 LSRs 上提供 MPLS OAM 的需求，我們想要選擇一個直覺有有效益首先能解決
LSP 錯誤的機制－LSP PING.此機制在 IETF, ITU-T 都有討論，主要的目的就是
解決底層 LSP 的錯誤，此封包與所欲偵測的 LSP 走相同路徑，比信令協定偵測到
的錯誤範圍還大；如果在 probe 封包後加上時間欄位以及在機制上加入計數封包
85 
 
圖 4.2.5-4：ITU-T -Connection Verification 
 
LSA Measurement  
a. Delay & jitter 
RFC2679[27]提及：在使用 GPS 或 NMP 的同步前提下再計算一對
LSRs 的時間誤差，可以估計單向延遲。LSP PING 的時候曾提及可以利
用 probe 封包夾帶時間欄位，藉此可以估算單向延遲，進一步再計算
jitter。 
 
b. Throughput, Packet loss 
若是定時發送 probe 封包，在底層針對特定 LSP 計數兩兩 probe
收到的封包在夾帶於 probe 封包後頭，在接收端 ELSR 可以計數與之比
對，進一步算出 Throughput 與 Packet loss rate. 
 
實作考量 
在現有的硬體平台－Intel IXDP2400，與軟體架構－Intel IXA4.1 環境下
設計管理系統（圖 4.2.5-5）。Microcode 控制 microengines 實際處理封包的程序 
(fast path)，能接受 control plane 的對傳送封包所依據的資料庫做設定；在
Xscale kernel space 上的 core component (CC)專門處理某一 microblock 無法
分析的封包(slow path)；Xscale user space 上可執行搭配 MPLS 的協定包括
RSVP-TE, OSPF, SNMP…等。監控管理系統在此架構下，在 microengines 主要負
責解析 probe 封包與配合上層對 LSPs 發送 probe 封包；Xscale kernel space
87 
驗證方法如圖 4.2.5-6 所示。其中 CV 封包的格式與 ITU-T CV 的格式相同。 
 
圖 4.2.5-6：CV 的運作流程 
首先說明連線驗證（CV 封包）的運作流程。如圖 4.2.5-6 所示。假設 MPLS
網路已經有多條 LSP 存在。此時，NRB 想要監控某條 LSP。則其流程如下： 
 
1. NRB 發送要求監控 LSP 的命令（TCP session）到此 LSP 的 Egress 端，
並告知其待機的時間（等待時間由 NRB 依網路拓蹼決定）。 
2. NRB 發送要求監控 LSP 的命令（TCP session）到此 LSP 的 Ingress 端。 
3. Ingress 開始發送 CV 封包，當網路的狀態正常時，CV 封包會沿著 LSP
到達目的地。 
4. 假設在一段時間之後，網路發生斷線。 
5. Egress 發現 LSP 的狀態改變，會立即通知 NRB（TCP session）。 
 
上述的操作主要的目的在於當網路發生問題時，NRB（網路資源管理中心）
可以即時得知受到影響的 LSP 有哪些。NRB 可以進一步修正或調度現有資源以保
障網路的正常運作。 
 
接著說明效能量測（PM 封包）的運作流程。我們所設計出的封包格式如圖 
4.2.5-7 所示，而 PM 封包主要的目的為量測網路的效能，因此我們設計出了
TimeStamp、PacketSend、PacketRecv…等相關欄位來量測 Delay、Jitter、Loss
參數。 
 
0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 
Label （top label same as user packets） Exp S TTL 
89 
1. NRB 發送要求監控 LSP 效能的命令（TCP session）到此 LSP 的 Egress
端，並告知其待機的時間（等待時間由 NRB 依網路拓蹼決定）。 
2. NRB 發送要求監控 LSP 效能的命令（TCP session）到此 LSP 的 Ingress
端。 
3. Ingress 開始持續地發送 PM 封包，並填入值到 TimeStampSend、
TotalPacketSend…等欄位，且在第一個 PM 封包上將 Status 狀態設為
1。將此狀態設為 1 的理由為當第一個 PM 封包到達 Egress 時，Egress
若檢查到此封包的 Status 為 1，就會將自己的所記錄的 Counter
（TotalPacketRecv、TotalSizeRecv）重設（清除為零），如此可以避
免封包 loss 計算的結果不正常。當網路的狀態正常時，PM 封包會沿著
LSP 到達目的地。 
4. Egress 收到的時候，會填入 TimeStampRecv、PacketRecv…等參數，並
使用這些參數計算以下效能參數：Delay、Loss、Jitter。 
5. 在待機時間結束後，NRB 可以開始定期地對 Egress 端要求目前的效能
參數。 
 
經由以上的操作，我們可以讓 NRB 依據需求來取得 LSP 上的效能，進一步的
話，可以檢查出是否有符合當初所訂定的 SLA，如此即可達到 SLA measurement
的效果。 
 
 
MPLS OAM 機制的實現 
由於 MPLS 網路特性的關係，在實作的規劃上，我們將發送 OAM 封包與接收
分成兩個獨立的部分（Ingress、Egress）來處理。而藉由第一年所得到的經驗，
為了因應 IXP2400 的封包處理流程。所有 OAM 封包的處理流程除了由上層的應用
程式控管外，均會涉及到 Intel 的 IXA 可移植性架構。亦即不論是發送或是接收
OAM 封包，均會包含三個部分： 
 
 上層應用程式：負責對 Core Component 下命令，或是接收 Core 
Component 送上來的 OAM 封包，並與 NRB 溝通。 
 Core Component：負責發送與接收 OAM 封包，以及控管與 Micro Block
分享的資源。 
 Micro Block ：處理一般資料的轉送，以及在 OAM 封包貼上
TotalPacketSend、TotalPacketRecv…等參數。 
 
91 
Application
Core Component
Micro Block
NRB
IXP2400機架
封包 封包 封包
應用程式層
系統核心層
Micro Engine層
12
3
4
5
6 7
 
圖 4.2.5-11：Egress 端的流程 
 
 
1. NRB 會依據需求發送命令（建立監控、或是解除監控）。 
2. 應用程式解讀命令，並記錄所要監控的 LSP 相關資訊。並等待下層所傳
送上來的 OAM 封包。 
3. Micro Block 收到封包，會依不同的情況處理： 
 MPLS 資料封包、CV 封包：更新封包數量計數器的值。 
 PM 封包：填入必要資訊到欄位中。 
4. MPLS Core Component 在收到 OAM 封包後，會再轉送到應用程式。 
5. 應用程式收到封包之後，再依情況來更新狀態或是效能參數。 
6. 當發生狀況時，應用程式會負責通知 NRB。 
7. NRB 可發送要求來取得效能參數。 
93 
[3] Thomas D. Nadeau et al, Multiprotocol Label Switching (MPLS) Forwarding 
Equivalence Class To Next Hop Label Forwarding Entry (FEC-To-NHLFE) 
Management Information Base, IETF Draft, October 2003. 
[4] T. Nadeau et al, Definitions of Textual Conventions for Multiprotocol Label 
Switching (MPLS) Management, IETF  Draft, October 2003. 
[5] Thomas D. Nadeau et al, Multiprotocol Label Switching (MPLS) Management 
Overview, September 2003. 
[6] Martin Dubuc et al, Traffic Engineering Link Management Information Base, 
September 2003. 
[7] Athena Vakali and George, Content Delivery Networks: Status and Trends, Dec 
2003 
 
[8] Intel® Internet Exchange Architecture Software Building Blocks Applications, 
design guide, November 2003 
 
[9] Intel® Internet Exchange Architecture Software Building Blocks, Developer’s 
manual, March 2004 
[10] Intel® Internet Exchange Architecture Software Building Blocks, Reference 
manual, March 2004 
[11] Intel® Internet Exchange Architecture Portability Framework, Developer’s 
manual, November 2003 
[12] Intel® Internet Exchange Architecture Portability Framework, Reference 
manual, November 2003 
[13] R. Coltun,The OSPF Opaque LSA Option,IETF RFC,July 1998 
[14] K. Kompella et al,Traffic Engineering (TE) Extensions to OSPF Version 2,IETF 
RFC,September 2003 
[15] F. Le Faucheur et al,Multi-Protocol Label Switching (MPLS) Support of 
Differentiated Services,IETF RFC,May 2002 
[16] M. Daniele et al,Agent Extensibility (AgentX) Protocol Version 1 ,IETF 
RFC,January 2000 
[17] Thomas D.Nadeau,MPLS Network Management 
[18] Eric Osborne et al,Traffic Engineering with MPLS,July 2002 
[19] Thomas D. Nadeau et al, OAM Requirements for MPLS Networks, IETF Draft, 
Sep. 2004. 
[20] K. Kompella et al, Detecting MPLS Data Plane Failures, IETF Draft, October 
2003. 
[21] RFC3429, OAM Alert Label, H. Ohta et al.,November 2002 
95 
第5章 <子計畫四>互動式多媒體服務於具內容
感知安全性之寬頻網際網路 
本子計畫設定之目標在研究多媒體存取與網路資源管理間之各議題並實現
於DiffServ/MPLS 網路上進行包括聲音、影像、圖文等之互動式多媒體存取服務
技術。本文就本子計畫執行完成之四項成果進行報告: (1) 建置多媒體播放系統 
(2) 發展多媒體服務之session移動性技術 (3) 開發以SIP 為基礎之多媒體服務
session 管理與允入控制策略與介面管理機制 (4) 開發適於DiffServ/MPLS 核心
網路之多媒體服務模型，並進行效能評估。本子計畫之執行對於理論研究,系統
實現與人員訓練上都有成果。其中共完成之直接與間接之研究成果
(2004.9-2006.10) 包括2篇SCI, 11篇國內外會議論文, 9篇碩士論文，其中session 
移動性技術更得到2006教育部通訊競賽之研究所組佳作。 
 
關鍵詞: MoMPLS，多媒體伺服器，多媒體傳輸，資源使用管理，服務品質保證 
策略，具內容感知安全性網路。 
97 
器，將命名為 CMH Gateway (中正大學多媒體家庭閘道器，CCU Multimedia Home 
Gateway)。 
 
圖 5.1.1-1：本技術欲提供服務之示意圖 
5.1.2 技術研究目的 
現階段有許多針對可攜式串流服務的討論和研究，但對於小型企業網路或家
庭網路，卻未看見一有效且實際的解決方案被提出，而本技術即是研究在這樣的
網路環境下如何有效的提供可攜式的串流服務，並加以實現。此外，技術中還加
入了分享的概念，提出 Multicast + Peer-to-Peer 的機制，讓使用者人數不在受限
制，本技術系統架構中的智慧型閘道器，將是一嵌入式系統，再配合其他裝置的
開發，來完成整個技術。 
 
圖 5.1.2-1：系統網路環境架構 
99 
幾項資訊是必備的，如什麼人移動到哪裡用什麼裝置在用什麼樣的服務，我們只
針對這些必要的資訊做討論，而感測技術將不在本技術作深入的探討。 
另外一個在利用 Session Mobility 功能時，還有一個很重要的問題需要被討
論，這就是交換了裝置之後，每個裝置的網路狀況和運算能力都不同，所以就有
了適應性的問題產生，在多媒體服務中資料量佔最大的莫過於視訊資料，我們可
以利用現行的網路和感測技術來知道每個裝置的網路狀況和運算能力(顯示螢幕
的大小)，然而要利用閘道器來幫忙做轉碼的動作似乎是不可行的，因為轉碼需
要的運算量很大，如果要用其他機器幫忙做又會造成時間上的延遲，這樣看來轉
碼似乎是不可行的，所以在技術中我們假設視訊的資料是採用了 Scalable Video 
Coding(SVC)的編碼技術，這樣閘道器當知道每個裝置的能力之後，只需從多媒
體串流中選擇其所需要的部分即可。 
目前本技術著重於 Home Gateway 的開發，利用 Intel IXP425 Network 
Processor 為核心的平台來實現，搭配 Montavista Embedded Linux 和 Intel IXP400 
Software Codelet，來完成整個技術最核心的部份。 
 
5.1.3 技術重要貢獻 
本技術的系統環境，採用嵌入式系統為 Intel IXP425 Network Processor 為核
心的平台，利用 ADI Engineering 公司所生產 Coyote Gateway Reference Design 
Based on the Intel IXP425 Network Processor，此平台提供兩個 SLIC/CODEC 電
路，給家用電話；四路 10/100 Mbps Ethernet port 給家中網路設備使用；一路 ADSL 
PHY 提供對外 ADSL 服務和連接 PSTN 網路，介面為 RJ-11；亦有一路 Ethernet 
PHY 提供對外的網路服務，介面為 RJ-45。另外，此硬體提供兩個 Mini PCI 插
槽，可用來作為無線網路基地台使用，這些硬體所支援的功能，都非常適合本技
術的利用與開發。 
先前本實驗室的研究團隊已在 Intel IXP425 的發展平台上，設計與實作一家
庭整合型語音閘道器 (Home Gateway)，提供外部電話網路 (Internet、PSTN) 和
家庭內部電話 (POTS Phone、SIP Phone、H.323 Phone) 之間轉接通話服務，並
提出 SIP-Based 內部共同信令的概念，降低電話信令整合的複雜度，以更簡單
更有效益的方式解決信令互連相容性問題；整合 ENUM 服務，希望能在不改變
使用者習慣下，以電話號碼撥打方式打電話，並透過 ENUM 服務功能，提供使
用者往後維護個人電話號碼轉換資訊等功能，此研究技術名為『Integration of 
ENUM-Based PSTN/VOIP Phone Gateway Embedded in ADI Coyote』。此研究成果
已在去年六月發表於教育部九十三學年度大專校院通訊科技技術製作競賽，並獲
入圍肯定。其中此屆比賽本技術的一名參賽的隊員-邱志偉 同學，也參與了上次
的比賽，相信透過他的經驗也將大大的幫助了本技術的完成。 
101 
電腦與他人進行通話，當他想離開位置且持續通話時，他可以轉移此session到他
手邊的手機上。 
 
 
圖 5.1.4-1SIP Session Mobility 的訊息流程圖 
接下來則是討論，在本技術中的 Session Mobility 與上述的 Session Mobility
的做法有何不同，並搭配 Multicast 功能所帶來的好處與影響，在這先簡單的說
明，本技術的 Session Mobility 是結合了 Mobile IP 的好處，就是由 Home Gateway
幫忙轉送封包，而不需重建 session 而造成不必要的延遲。 
 
圖 5.1.4-2 傳統 Cross device 流程圖 
首先，讓我們來想像一個一般家中的情況，客廳有台能提供 IPTV 服務的電
視(dev1)，並且有台有網路功能的 PDA(dev2)，一開始有人從客廳的電視觀賞體
育節目，但後來想移動到家中的另一個位置，但移動之後的地點沒有任何可接收
IPTV 服務的設備，但卻想繼續收看體育節目，因為想將畫面轉移至另一攜帶式
103 
Server的 IP及其提供服務的 port; Src IP:poer則是 device的 IP及其所使用的 port；
Service 則是用來辨認不同的服務；Multicast 欄位個式與 Src IP:port 相同，用來
提供 multicast 的服務。 
當使用者想從原本的客廳移動到另一處沒有可提供 IPTV 服務的裝置的位置
時，可將原本傳送給 dev1 的 streaming data 轉至 dev2 (PDA)，以繼續提供使用者
服務。當使用者攜帶 dev2 時，dev2 上的感應裝置將偵測到使用者身上的 sensor， 
並且從使用者上的 sensor 得知該使用者之前已要求一個尚未結束的 IPTV 服務，
再將此訊息通知 HG，告訴 HG 建立該 session 的使用者已經移動至 dev2 所服務
的範圍，之後 HG 會修改之前所紀錄的資訊，如表 3。dev2 不需再對外部的 server
重新建立 session，減少重建 session 造成的 delay，之後 HG 只要收到 session1 的
streaming data 都會轉送至 dev2，達到 cross device 的服務。 
此外，若使用者想在 dev2 上收看原本的節目但卻不想將原本 dev1 上的節目
中斷，也就是說 dev1 繼續收看原本的節目而讓 dev2 也能收看同節目或者說使用
者想將 dev2 上的節目分享給其它 device(dev3)，此時，透過我們所提供的 multicast
即可達到。首先，dev2 會通知 HG 要求 multicast 給 dev3 的服務，HG 收到此通
知之後會紀錄此 multicast 的訊息，如表 4。之後所有要送給 dev2 的 streaming data
也會轉送至 dev3，讓 dev3 可以享受與 dev2 相同的服務但又不用重建 session 也
不會增加對外的頻寬造成多餘的負擔。 
由於我們所提供的 cross device 及 multicast 可以省去重建 session 的 delay 但
是相對地，由於我們必須在 HG 檢查封包至應用層才能知道那些封包是屬於
RTSP 或 SIP 的封包而需要轉送，這些察看封包至應用層的動作會耗費大量的時
間而造成 performance 的降低，因為我們在一開始收到 device 要求 streaming 的服
務時，會啟動一個學習 IP:port 的功能，根據 HG 所紀錄的 session maintain table
中的 dest ip:port 以及 src ip:port 可知封包是否為需要轉送的封包，因此當下層收
到封包並在檢查後確定符合轉送的條件時，立即轉送封包而無需再將封包往上層
送，此學習 ip:port 的機制可以加速轉送而提高 performance。 
 
表 1. device IP and capacity 
 
表 2. HG session maintain table 
 
105 
個 Device 透過 HG 而成功取得一個新的媒體之後，它將有機會成
為 Seed。在任何 Device 成為 Seed 之前，必須向 Tracker 告知此媒
體 P2P 機制的啟動。當一個 Stream 的所有需求端可能因為處理效
能不足以成為 Seed 時，Tracker 可以成為此 Stream 的 Seed。 
Peer 
當 Tracker 向所有裝置廣播一個媒體已經啟動 P2P 機制的訊息之
後，所有對此媒體有需求的裝置都稱為 Peer。而 Peer 不能再向 HG
提出這一媒體的請求，必須向 Tracker 申請該媒體的 Seed 位址。然
後轉向 Seed 申請該媒體。 
B. 運作流程 
圖 5.1.4-4 表示了在一般情況下，此 3 種元件的作用流程。(1)Dev 1
向 HG 要求建立一個新的 Session。(2)HG 同時將此媒體轉送給要求端
與 Tracker。在此時 HG 可決定是否將此 Stream 的 P2P 機制啟動，或可
由 Dev 1 提出申請，當 P2P 啟動之後，Dev 1 即成為此媒體的 Seed。
(3)Tracker 將廣播 P2P 機制啟動的訊息給所有 Device。(4)當 Dev 3 需要
此媒體時，我們稱它為 Peer，且根據 Tracker 的廣播內容得知該媒體已
經啟動 P2P 機制，必須直接向 Tracker 索取 Seed 的位址。(5)Tracker 估
計 Dev 1 可以繼續承擔 Seed 的腳色，將 Dev 1 的位址告知 Dev 3。(6)Dev 
3 根據位址向 Dev 1 請求該媒體。(7)Dev 1 提供該媒體給 Dev3。則 Dev 
3 對此媒體的傳輸上，皆不會造成 HG 的額外負擔。 
 
圖 5.1.4-4 P2P Streaming 範例之訊息溝通示意圖 
5.1.5 軟硬體系統 
(1) 系統環境與硬體架構 
107 
 
圖 5.1.5-2 本系統開發平台之硬體架構圖 
(2) Layer 3 Switch Router 的開發 
Intel IXP425 開發板上提供的 Ethernet Interface 中，其中有四個 LAN Port 與
一個 WAN Port。四個 LAN Port 共用同一張 Ethernet 網路卡，而 WAN Port 則是
自己獨立一張網路卡。本技術利用 Intel 提供的 IXP400 Software Codelet 實作出
利用軟體支援的 Layer 3 Switch Router。而我們主要利用 IXP400 Software Codelet
中的 Ixp400EthAccCodelet、Ixp400EthDBCodelet 與 Ixp400EthMiiCodelet 三支 API
來進行開發。 
109 
收並送至上層的 3P2S Agent 做服務確認與紀錄，一但確定之後，則告訴
Mobility/Multicast Dispatcher 有哪些封包是需要處理的，自此之後所有要處理的
封包都將在 Layer 3 Switch Router 功能方塊中被處理裡完畢。另外，多媒體串流
的資料都會在 Layer 3 Switch Router 被處理，而不會送到上層的 3P2S Agent。 
(4)軟體發展系統架構 
在我們的技術中，Layer 3 Switch Router 我們利用 Intel IXP400 Software 
Library 所提供的 Codelets 的 API 來做修改，自行開發我們要的功能。在軟體上
我們套用了 MontaVista 3.10 與 Intel 公司所 Release 出的 IXP400 Software 
Version1.3 軟體。而 3P2S Agent 也分別利用 oSIP Libaray 與 oRTSP 來進行相關
功能的程式開發。 
 
圖 5.1.5-5：軟體發展系統架構圖 
圖 5.1.5-5 為我們 Home Gateway 軟體發展系統架構圖。有關上下層的部分，
我們皆採用現行 Open Source 之程式，其中包含：oSIP Library、oRTSP Library、
Intel Ixp400 Software Library。下面將述敘各功能方塊和這些軟體的搭配： 
1. Layer 3 Switch Router。Intel IXP400 Software 提供的 Codelets Library，
我 們 利 用 其 中 的 Ixp400EthAccCodelet 、 Ixp400EthDBCodelet 與
Ixp400EthMiiCodelet 三支 API，來實現 Software 的 Layer 3 Switch 
Router，這都將在 Montavista Linux 中的 Kernel Mode 來完成。 
2. 3P2S Agent。其中包含 SIP Session Mobility、RTSP Session Mobility 與
Multicast/P2P Streaming 三個功能方塊。由於我們要進行 SIP 和 RTSP 的
111 
小事而打斷他們的使用，所以就有了可攜式串流服務的提出，並配合網路多媒體
分享的概念，也因此有了本技術 Session Mobility + Multicast + P2P Streaming 服
務的提出。 
在技術創新部分，我們所提供的服務整合到 IXP425 的平台上，利用此平台
在多媒體串流的 Session Mobility 服務加入了 Mobile IP 的優點，使得當啟動
Session Mobility 服務時，不必重建 Session，造成過多的延遲，另外，因應小型
企業網路或家庭網路這樣的私有網路環境，我們提出了 Multicast 配合 P2P 
Streaming 的機制，既可排除使用人數的限制，又可不必擔心這樣的 P2P 機制造
成網路的癱瘓，而這些都是我們技術的創新。 
在本技術裡我們最大的貢獻在於為像家庭或小型企業的網路環境，提供有效的可
攜式串流服務，並配合 Multicast 與 P2P Streaming 的機制，整合於 Embedded 
System 的閘道器上，相信在不久未來，「Portable/Peer-to-Peer Streaming Service」
能夠提供人們生活更多的便利和樂趣，也提供未來多媒體串流服務一個很好的例
子。 
 
 
5.2 開發以 SIP 為基礎之多媒體服務 session 管理與允入控
制策略與介面管理機制 
5.2.1 以 SIP 導向之 VoIP 於 MPLS 架構設計 
(1) 架構設計與系統元件 
圖 5.2.1-1所示的架構是以MPLS Domain為Data Plane做資料快速傳送的平
台，以 SIP 做為 VoIP 的 signal 協定，可讓使用者與機器之間做更多 Session 的服
務。此架構可被分成三個部份 : 邊元件 (Edge Devices)、存取網域 (Access 
Network)、MPLS 核心網域(MPLS Core Network)。 
 
A. 邊元件(Edge Devices)  
邊元件(Edge Devices)被定義在終端機發送 Request 訊息或存取多媒體服務
的機器。在架構圖中 Caller 是扮演 UAC (User Agent Client)的角色，負責初始化
Session，Callee 則是扮演 UAS (User Agent Server)，負責回應 Caller 並建立通道。
兩者之間的關係是UAC開始發送建立Session的訊息給UAS，而存取網域(Access 
Network)提供控制與 Signaling 的功能，以期可以成功地建立通道，做一進步的
113 
II. Security 模組: 
CAP (Content Awareness Processor): CAP 主要還是偵測攻擊並防禦，假設有
偵測到可疑的封包，CAP 將會捨棄非法封包，並回報訊息。CAP 可與 NRB
溝通並即時分配頻寬，圖 5.2.1-1 中兩台 Intel IXP 425 是執行 CAP 的工作平
台。 
III. SIP Signaling 模組:  
SIP Proxies: SIP Proxy 是傳送與接收 Requests 給 UAS 和 Response 給 UAC 的
元件。Proxy Servers 根據儲存 Session State 資料可以分辨 Call 的差異性，而
SIP server 的 Proxy State 是設定 Stateful。 
SIP Server: 本計劃的 SIP Server 包含 Registrar 和 Proxy 兩個功能，採用的
SIP Proxy 軟體是 partysip 與 oSIP Library，並且將 CCA 的功能加在 partysip
中，而 Open Source OSIP 提供 SIP 相關的資料庫。 
 
C. MPLS 核心網域(MPLS Core Network) 
核心網域主要是分成 Ingress LSR、Core LSR、Egress LSR，以 Diffserv/MPLS
為網路環境，工作平台分別是 IXP2400、IXP1200*2、IXP2400。簡單地說，其負
責的工作是快速地將資料轉送到下一個目的地。兩台 IXP1200 是當 Edge LSR，
一個當 Ingress LSR;一個當 Egress LSR，另外兩台 IXP2400 當 Core LSR。Ingress 
LSR 會將送進 MPLS Domain 的資料貼上標籤(push labels)，依照 DSCP ( DiffServ 
codepoint )分配的等級給定不定的標籤，除了可以保證頻寬和快速轉送到下一個
目的地，最重要的是遇到斷線時啟動的快速修復機制(fast recovery)與快速繞路機
制(fast reroute)。Core LSR 最重要的工作就是快速將資料轉送到下一個目的地，
並且支援斷線時，可以當備援的 LSR，與 NRB 溝通並計算網路資源的使用狀況，
適時做出決策。而 Egress LSR 的工作是將標籤移除(pop labels)，因為 packets 要
出 MPLS Domain，所以除了 pop labels 之外，還要將資源釋出，以供其他使用者
的使用。以上所提為 MPLS 核心網域的基本工作。 
115 
鎖，不讓此帳號進行任何的 VoIP 服務，做到簡單的 CAC 工作。最後是由
SIP Server 傳給 Callee 端的 Proxy Server，再傳到 Callee 端的使用者機器。 
II. 200 OK和ACK訊息流: INVITE訊息流到達Callee端之後，Callee端的 Proxy 
Server 就會回覆 200 OK 的訊息去通知 Caller 端，同意這通 Call 的建立。因
為Proxy State是設定Stateful，所以可由Record-Route與Via等欄位將200 OK
訊息流經由 INVITE 訊息流所走的路徑反向傳回給 Caller 端的使用者，告知
可以開始進行 VoIP 的語音通話了，200 OK 訊息流再經過 SIP Server 時，SDP 
Parser 會將雙方的同意的 Media Codec、型態、頻寬取出來，並告知 Caller
端進行 VoIP 服務時，要用 Media Codec、型態等。而 ACK 訊息流並沒有夾
帶任何 SDP 的資訊，它的目的是防止通話逾時(Timeout)，所以收到 ACK 之
後，如果沒有收到 CANCEL 或 BYE 等訊息，是不會結束通話的。 
117 
5.2.2 SIP Proxy Server 之軟體系統架構 
(1) Partysip 和 oSIP Library 
本計劃元件中的 SIP Proxy、與 SIP Server 是採用 Open Source Software 的
partysip 架設而成的，並且以 oSIP 為 partysip 提供 Library，OS 為 RedHat 
Linux9.0。底下分別為 partysip 與 oSIP 做個簡介。 
A. The partysip SIP proxy server 
Partysip 是一套架設 SIP Proxy Server 的軟體，也是一套模組化的應用程式，
可以隨時增加外或移除外掛程式。此套軟體更包含了多種 GPL 外掛程式，基於
不同的目的，可將 partysip 與外掛架設成 SIP Registrar、SIP Redirect Server、SIP 
Statefull Proxy Server。 
Partysip 完全是由 C 語法寫成，比較底層的 SIP 運作流程完全都得依靠 oSIP
來實現。關於 partysip 有很多的功能並且現在的板本 partysip-2.2.2 已經都可以正
常運行，下圖 5.2.2-1 所示列出它的基本 Functions[13]。 
 
圖 5.2.2-1 Partysip Functions 
119 
(2) Partysip 軟體架構 
A. Partysip Threads 與 SIP Transactions 
 對於架設 SIP proxy server 的軟體 partysip，本節會做一個詳細的說明。一開
始要了解的是控制整個 partysip 的作業流程是由 4 個 threads 組成，分別是 sfp 
(State Full Processing)、core、tlp (Transport Layer Processing)、resolv (Address 
Resolution)。sfp 是指 proxy 設定成 stateful 的一切行為。在 SIP Transactions 就是
TU 的角色，根據不同的請求(request)，送進適當的 Transaction List 做處置。core
負責的是 SIP Transactions 的相關工作，tlp 是開一個 socket 讓處理完的封包可以
傳送出去或接收進來做下一步的處理，resolv 的工作是去詢問 DNS Server，解析
目的地 SIP URI 的實際位址。下圖 5.2.2-3 是 partysip threads 與 SIP Transactions
的關係圖。以 Proxy 的角度來看，SIP transactions 可以同時並存的有 ICT、IST
或 NICT、NIST。 
 
圖 5.2.2-3 Partysip Threads 與 SIP Transactions 
 
Partysip 會利用 oSIP Library 已經定義好的 SIP Transactions 去處理 sfp thread
送來的 SIP messages，一個 SIP message 可視為一個 Event，將收到的 Event 存到
適合的 SIP transaction 的 Link-list，當成一個 Event list 處理。假如是收到 INVITE，
Event list 就是 INVITE、Ringing、200 OK…，然後送給 ICT 做處理。
121 
C. Threads 傳送 SIP Message 的流程 
 如下圖 5.2.2-5 threads 傳送 SIP message 的流程圖所示，接收 SIP message 的
thread 是 tlp，而傳送 SIP message 的 thread 是 sfp。一開始收到 SIP message，此
message 可能是由 caller 發出，不過現在是以 proxy 的角色去看，此 message 是由
IST或NIST送來的。Proxy接收到SIP message (INVITE or non-INVITE)，sfp thread
會配置系統資源給需要處理此message的 ICT或NICT，並將此message視為 event
新增加到所屬 client transaction的 event list，然後交給 core thread處理 ICT或NICT
的 state machine 流程。等待 state machine 的相關動作做完之後，就通知 tlp thread
將 message 傳出去。此時，如果需要知道 UAS 的 Domain name 就要先去詢問 DNS 
server，利用 resolv thread 去詢問 DNS server 下一級 UAS 的 SIP URI 實際位址，
這樣才能將 message 成功地傳到下一級 UAS。 
 
 
圖 5.2.2-5 Threads Transmit SIP Message Flowchart 
123 
相同的，以保證此通call的通話服務品質。 
 我們的服務策略主要設計方針可以區分為三大項。第一，以服務付費為導向
保證發話者與受話者的通話服務品質。第二，當EF-LSP網路資源充足時，應該
採用的是將具有AF+服務使用者升級的策略，完善的使用網路資源。第三，當
EF-LSP網路資源不足時，應該採用的是將具有AF+的使用者導到所屬的LSP，或
是將之前升級到EF-LSP的call再導向AF-LSP的降級策略，防止call blocking的發
生。以上三項設計方針都是以提高網路資源的使用率與減少call blocking的發生
機率為目標。 
 
 
圖 5.2.3-1 Example with no CAC Policy1 
 
 
圖 5.2.3-2 Example with CAC Policy1 
125 
而花比較少錢卻能只享用次品質AF等級的服務，這是很合理的。站在使用者的
角度來看，大部份的人都希望用最好的品質，但卻不想花太多的錢;若花錢買到
最好的品質，又不能常常使用，反而也造成浪費，所以衡量的結果，在
DiffServ/MPLS網路上，最多使用者的簽定是AF等級的，所以有可能造成AF-LSP
的不夠用或EF-LSP使用的效率不彰，更或者造成AF的使用者的call blocking發生
的機率提高，都是因為DiffServ網路硬性分級的缺點。 
為了解決上述的缺點，提出一個AF+ Service Model。為了區別RFC訂定
DiffServ網路的分級規則，而又不違反DiffServ的運作流程，將設定一個新的級
別，稱AF+，並可稱此服務模型為AF+ Service Model。凡是有簽訂為AF+的使用
者，有可能享用跟EF同等級品質的服務。當資源充足時，即EF-LSP有多餘的頻
寬時，就可將AF+的使用者升級到EF，讓此通話的品質有跟EF使用者一樣的品
質，並將此升級過的通話記錄下來。當資源不足時，即EF-LSP沒有足夠的頻寬
來保留給真正簽訂EF的使用者時，便要將之前有升級過的通話降到AF等級，並
導到AF-LSP，這是為了確保簽訂EF使用者的權利，不過這機率會發生在當先建
立的EF-LSP不足時，並且跟NRB要求新的EF-SLP，被拒絕時，才會發生的。 
圖 5.2.3-4為服務策略的設計流程圖。開始的時候，SIP message已經進行到
了接收200 OK，解析caller與callee端的codec，並檢查帳號是屬於SLA簽訂的何種
等級。第一步的策略是將caller端到callee端與callee端回到caller端的LSP路徑都設
定為同等級的，並以caller端與SLA簽訂的等級為基準。在做CAC policy之前，先
把此通call是何等級記錄下來，目的是為了知道做完CAC policy之後的等級做比
較，便會得知此通call是升級或降級亦或不變。 
開始執行CAC policy的函數時，一開始要先判斷此通call是否屬於EF，假如
是的話，再判斷EF-LSP是否有足夠的頻寬可使用，如果有的話，再將此通call導
向EF-LSP;如果沒有的話，便要搜尋有無AF+使用者升級到EF的call，再將其導回
AF-LSP，這樣真正的EF使用者才可以使用EF等級的服務，不過這發生的原因是
EF-LSP資源不足時。因為大部份的使用者以AF+或AF等級的居多。現在假設有
EF-LSP資源足夠時，而發話者亦是屬於AF+的等級，結果只會將此通call導向
EF-LSP，直到EF-LSP的頻寬不足時，如果AF+的使用者還要進行VoIP語音服務
時，便會將它導向AF-LSP。至於其他不是EF也不是AF+等級的使用者，只能使
用所屬的LSP資源。如果每一條的LSP的頻寬都不足時，便會開始blocking call，
這表示有很多通call都同時在進行VoIP服務，而且每一條LSP都有充份的利用，
這時如果還有人想要進行語音服務時，下下之策，才會開始blocking call。 
127 
5.2.4 SIP Proxy Server 之功能整合與功能驗證 
本章節說明 Proxy Server 在既有的功能下，整合 Database 與規劃它的應用。 
(1)測試環境(Environment) 
首先，在測試環境(Environment)方面，需要兩台主機加上一台 SIP Proxy 
Server。兩台主機一台是 Caller 的角色，負責發送請求(requests);一台是 Callee 的
角色，專門接收 request 並回覆 response。而處理有關 SIP URI 註冊、發送 request
或 response 到 Callee 或 Caller 等，則都是 SIP Proxy Server 的工作。 
 
(2) SIP Proxy Server 整合 MySQL Database C API 
本計劃中的 SIP Proxy Server 欲整合 MySQL Database，以儲存正在通話狀態
(Call State)和通聯記錄，記錄使用者的通話次數、通話時間來計算通話費。採用
架設 Database 的軟體是 MySQL Server，加上 PHP Server 提供一個網頁介面可以
方便控管 Database 的資料存取。表 7 是 Database MySQL 中所有的 table 與 table
負責記錄的內容。 
 
Table 內容 
Rcv_msg 記錄收到的 SIP msg 
Snd_msg 記錄送出去的 SIP msg 
SDP_parser 記錄由 200 OK msg parser 出來的 SDP 欄位的資訊 
Call_counter 記錄每一通 call 的通話時間、與通話費用 
表 7 MySQL 中的表格內容 
 
通話者(Caller)與受話者(Callee)的角色是由 SJ Labs 提出的 SJphone 來擔任，
SJphone 是一套支援 SIP 與 H.323 的 Software Phone，但是此一軟體沒有釋出原
始碼，其功能有點對點(Point-to-point)直接輸入 IP，而不需要做 Signaling 的動作，
就可以通話，還有顯示未接來電、建立電話簿等功能。 
由 SIP signaling 流程圖來說明封包的傳送與將欲從 SIP server/CCA 的資料轉
存到 Database 的流程。 一開始啟動 SJphone 向 Partysip Server 註冊，Caller 端要
與 Calle 端通話發出 INVITE 訊息，經過 Partysip Server，將訊息截取下來做解析，
再填上下一個目的地，並將從各欄位解析出來的資訊存到 MySQL Database，雖
然 Proxy State 為 Statefull，但 Partysip Server 只是將它暫時存到 RAM，通話一結
束就不會有任何資訊可查。在實作上，我們先將 Log 檔從 RAM 轉儲至 MySQL 
129 
 
 
 
圖 5.2.4-2 Media Type 與 SDP codec 
 
 
圖 5.2.4-3 通話時間計算與通話費 
 
 
131 
Edge Router 做到封包分類，但也具有接收 classified rules 才能讓 packet 通過的功
能，如下圖 5.2.4-4 所示。 
 
圖 5.2.4-4 測試環境 
假設 caller id 為 111 有簽訂 AF+服務的 AF 等級; callee id 為 555 是簽訂 EF
等級。為了測試方便，將在 CCA 簽定的 LSP 的最大與最小保留頻寬定成只能同
時容納三通 call。因為一通 call 的 codec(GSM/8000)所佔的頻寬為 3.57kbps，所
以就設定 EF-LSP 的保留頻寬為 max_resv_bandwidth = min_resv_bandwidth = 
11.0kbps，AF-LSP 的最大與最小保留頻寬分別為 max_resv_bandwidth = 20.0kbps, 
min_resv_banad = 11.0kbps。那為什麼要這麼設定呢?主要是因為 EF 等級有保證
最大與最小的保留頻寬，而 AF 等級只有保證最小的保留頻寬，將 AF-LSP 的
max_resv_bandwidth = 20.0kbps 還有一個目的，就是讓 EF-LSP 降級回來的 call
有足夠的頻寬可以降回來，不然就會造成線路忙碌中的現象，並發 486 Busy Here
的 error message 給 caller 端，如圖 5.2.4-5。 
 
圖 5.2.4-5 AF-LSP 無足夠頻寬可供降級的決策 
 
 一開始先由 111 發 INVITE 給 555，因為是 111 是有簽訂 AF+，這裡以 AF1
中的一個 user id 來表示 AF+等級，意即只要是 AF 系列中有加簽 AF+的，都有
可能升級到 EF，得到更好的通話品質，如圖 5.2.4-6 所示，可以在 AF 系列(AF1、
AF2、AF3、AF4)中的 user id 加簽成 AF+。在經過了 CAC policy 之後，便將此
通 call 導到 EF-LSP，如圖 5.2.4-7、圖 5.2.4-8 所示。有加簽 AF+等級的使用者，
是可以想像成 AF 系統有加值服務的觀念，舉個例子來說，在 AF1 中的使用者有
11、12、13、111 總共 4 個使用者，但是這 4 個使用者只有 111 有加簽 AF+的服
務，所以他可能可以享用跟 EF 同等級的品質，只要 AF 系的使用者有意願加錢，
133 
 
圖 5.2.4-7 Call State Information1 
 
圖 5.2.4-8 LSP Information1 
 
當 SIP call generator 持續發 INVITE 時，call 建到第三通，發現 EF-LSP 已經
不夠用時，再由 111 發一 INVITE 時，就會將此 call 導到 AF-LSP，如圖 5.2.4-9、
圖 5.2.4-10 所示。 
135 
假設此時由另一台機器是簽訂 EF 的使用者 314 也發 INVITE 給 callee id 是
555，那會發生什麼事?由 CAC policy 的機制來看，此通 call 的建立會讓第一通
由 AF 升級到 EF 的 call 再降回 AF，重新將 call 導至 AF-LSP。如圖 5.2.4-11、
圖 5.2.4-12 所示。 
 
圖 5.2.4-11 Call State Information3 
 
圖 5.2.4-12 LSP Information3 
 在降級這個部份有一點需要注意的是，重新導到 AF-LSP，需要將 IXP 2400 
MPLS Edge Router 上的 classified rules 移除掉，再重新下 classified rules 到 IXP 
2400 MPLS Edge Router 上，這樣才能算是真正的將 LSP 重導。如圖 5.2.4-13、
137 
 
圖 5.2.4-15 下達移除並新增 classified rules 的指令 
 
當 LSP 的頻寬不足跟 NRB 要求配置新的 LSP 又被拒絕時，所做的決策就是
限制此 call 的語音服務，亦即是服務策略的設計流程圖中的 blocking the call。假
設一條 EF-LSP 不夠頻寬可以容納新的 call 時，又被要求 NRB 配置新的 LSP 又
被拒絕，要發送新的 INVITE 要建立 call 時，會先去檢查 所屬的 LSP 是否真的
不夠頻寬，假如是真的，CCA 便會告知 SIP Server, 將此通 call blocked，並發一
個錯誤訊息 486 Busy Here 通知 users 線路忙線中，如圖 5.2.4-16 所示。 
 
圖 5.2.4-16 486 Busy Here error message for blocking call 
139 
在於 RAAF + 模型可以做到將 AF+的使用者升級到 EF-LSP 中，還可從 EF-LSP 中降
級到原來所屬 AF 等級的 LSP 中，而不會影響到 EF 使用者通話的權利。圖 
5.2.4-18 說明的是一剛開進來的都是 EF 的使用者，將 EF-LSP 的頻寬用到上限，
所以簽訂 AF+的使用者也無法升級，而另一個例子如圖 5.2.4-19 說明的是一開
剛進行語音服務的是 AF+的使用者，最後 EF 的使用者才進行語音服務，迫使
AF+的使用者做降級的動作。以上兩個例子充份顯現 RAAF + 模型可以克服 non RAAF + −
模型的缺失，所以此模型可以適用在多種不同等級使用者組合的網路環境中。假
設這兩種模型都應用在多 AF 使用者;少 EF 使用者的環境下， RAAF + 模型的效能還
是比 non RAAF + − 模型高些，而這原因是顯而易見的，因為 RAAF + 模型可以處理 AF+在
EF-LSP 中滿載的情形，而 non RAAF + − 模型卻不行。 
 本計劃在這兩種模型下，設置了一個 Threshold_bandwidth，VoIP server 
provider 的管理者可以自行規劃此一臨界頻寬，做為要升級多少通 calls 的指標。
因為 Max_resv_bw 與 Min_resv_bw 都是已經簽定在 SLA 合約中，如果當成指標，
便顯得太沒有彈性，所以才會有一個想法來加設一個變數做臨界頻寬。此臨界值
範圍在 0 與 Max_resv_bw 之間，至於要設定多少端視網路環境與使用者簽訂 SLA
的數量與 VoIP service provider 跟 network provider 簽訂 SLA 中 LSP 的頻寬來決
定。我們的做法是直接採用 Min_resv_bw 來當一臨界值。 
141 
5.3 成果自評 
本計畫之目的，即在支援 Content-Aware Secured 與 QoS 之 Diffserv/MPLS 網
路上，研究互動式多媒體存取與網路資源管理之議題。工作重點與完成之進度及
其他成果分述如下: 
 
1.『服務等級規範介紹與語音影像壓縮技術研究』 
2. 建置多媒體播放系統 
3. 發展多媒體服務之 session 移動性技術 
4. 開發以 SIP 為基礎之多媒體服務 session 管理與允入控制策略與介面管理    
機制 
5. 開發適於 DiffServ/MPLS 核心網路之多媒體服務模型，並進行效能評估。 
6. 人員訓練 
7. 國際會議投稿 
8. 碩士論文與科技報告 
1. 『服務等級規範介紹與語音影像壓縮技術研究』: 
其研究的資料都已整理至計畫書中。 
 
2.  建置多媒體播放系統 
 我們以 DirectShow 實作一個媒體播放器與架設 Real-Time Media Server 來測
試其他各子計畫的多媒體存取服務技術，為了配合計畫的用途，其中包含 H.264
與 MPEG4 等影像壓縮技術。所以播放器可以讓使用者輸入自己所需要的 codec，
為了配合計畫的用途，所以在播放器上也會設計讓使用者可以自由選擇播放的
bit rate…等選項，可供日後研究用途，而不是只有單一不能修改的功能。 
 
3. 發展多媒體服務之 session 移動性技術 
本技術將針對公司內部網路或家庭網路，開發對外部網路連接的智慧型閘道
器(Gateway, 選用 Intel IXP425 Network Processor 為核心的平台來實現，並命名
為 CMH Gateway ,中正大學多媒體家庭閘道器，CCU Multimedia Home 
Gateway)，對 VoIP 和 IPTV 等多媒體服務提供 Session Mobility 的機制，讓使用
者可以在任何地方，使用任何裝置不中斷地享受這些服務；另外加上 Multicast
並配合 P2P 的機制，減輕閘道器的負擔，並讓使用者人數可以不受限制，來實
現可攜式且強調分享的未來多媒體串流服務。其主要貢獻如下： 
 (1) 在小型區域網路和家庭網路中，利用閘道器來提供使用者可以享受
多媒體串流具有 Session Mobility 的服務，且避免了了 Mobile IP 的三角遶徑
的缺點，來提升整體的效率。 
143 
不彰，更或者造成AF的使用者的call blocking發生的機率提高，都是因為DiffServ
網路硬性分級的缺點。因此我們提出一個 AF+ Service Model，在不違反 DiffServ
的運作流程下，設定一個新的級別稱為 AF+，並可稱此服務模型為 AF+ Service 
Model。實驗結果顯示此 Service Policy 目的就是在提高頻寬的使用效率並減小
Call Blocking 的發生，讓 VoIP Service Provider 可以提高整體利益。 
 
6. 人員訓練 
藉由本計畫的研究及討論，使計畫中參與成員可以不斷地增加相關理論知識
及實作經驗，並藉由實驗得到的成果，分享於總計畫其他成員，進而使本實驗室
在以下領域皆有更長足的進步: 
(1) Microsoft Directshow 元件與設計、視訊編碼。 
(2) Network processor 平台之開發與設計。 
(3) SIP component 規劃設置與應用。 
 
7. 國際期刊與會議論文投稿 (2004.9-2006.10) 
 
其中共完成之直接與間接之研究成果(2004.9-2006.10) 包括2篇SCI, 11篇國內外
會議論文, 9 篇碩士論文。尚有數篇期刊及會議論文在審查中。 
Journal Papers: (ordered by year) 
[1]. B.C. Cheng and Huan Chen, “Evidence Collection with Quality of Assurance 
(QoA) for Network Forensics,” Lecture Notes in Computer Science (LNCS), To 
appear [SCI, EI]  
[2]. Huan Chen, S. Kumar and C.-C. Jay Kuo, QoS-Aware Radio Resource 
Management Scheme for CDMA Cellular Networks based on Dynamic 
Interference Guard Margin (IGM), Computer Networks, Vol. 46, pp. 867-879, 
2004. [SCI, EI] 
Conferences: (ordered by year) 
[3]. Huan Chen and B.C. Cheng, “Smart Home Sensor Networks Pose Goal-Driven 
Solutions to Wireless Vacuum Systems”, the First International Workshop on 
Smart Home (IWSH 20006) To appear [EI]  
[4]. B.C. Cheng and Huan Chen, “Context-Aware Gateway for Ubiquitous SIPBased 
Services in Smart Homes”, the First International Workshop on Smart Home 
(IWSH 20006) To appear [EI] 
145 
3. (95 年度) 碩士論文:蔡立光, "Energy Residue Aware（ERA）Clustering 
Algorithm for Wireless Sensor Network" 
4. (95 年度) 碩士論文:陳楙仕, "On optimal Adaptive Contention Window 
Control in Wireless LAN" 
5. (96 年度) 碩士論文: 鄭志川, "A Sensitivity Analysis Approach to the 
Optimal Policy Range of the Resource Access Control in Wireless Systems", 
(EE Master, NCCU, Adisor: Huan Chen)  
6. (96 年度) 碩士論文: 張聖翊, "A PWCE based Fast Rerouting Approach in 
MPLS Traffic Engineering for Link and Node Restoration" (EE Master, 
NCCU, Adisor: Huan Chen)  
7. (96 年度) 碩士論文: 沈紋興, "SIP based Multimedia Service Network 
Platform - Planning and Implementation", (EE Master, NCCU, Adisor: Huan 
Chen)  
8. (96 年度 ) 碩士論文 : 林家彬 , "The Signaling Exchange with Power 
Constrant in Cluster-based Wireless Sensor Networks", (EE Master, NCCU, 
Adisor: Huan Chen)  
9. (96 年度) 碩士論文: 黃政偉, "On Optimal Power Management Policy for 
IEEE 802.11 WLAN Systems" (EE Master, NCCU, Adisor: Huan Chen)  
