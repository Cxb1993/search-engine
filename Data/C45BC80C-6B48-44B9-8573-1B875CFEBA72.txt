 2
recognition (species decision) as described in detail in 
the following. 
 
3.1 Preprocessing 
 Preprocessing filters the signal and properly segments 
the syllables for feature extraction. The procedure for 
preprocessing in this study is shown in Fig. 3.2 
containing four steps: syllable endpoint detection, 
normalization, pre-emphasis and segmentation. 
 
3.1.1 Syllable endpoint detection 
 The applied syllable endpoint detection method is 
described in the following. 
1. Compute the short time Fourier transform of x(t) with 
frame size N = 512, and form the spectrogram of the 
signal. The Hamming window for short time analysis 
has the form of 
⎪⎩
⎪⎨
⎧ −≤≤⎟⎠
⎞⎜⎝
⎛
−−=
otherwise ,                                   0
10 ,
1
2cos46.054.0
][
Nn
N
n
nw
π
 (3-1) 
2. For each frame m, find the frequency Bin mbin  with 
the greatest magnitude. 
3. Initialize the syllable index j, 1=j . 
4. Compute the frame t at which the maximum 
magnitude occurs 
)][(maxarg
1 mMm
binXt
≤≤
= , (3-2) 
and set the amplitude of syllable j as 
 (dB)][log20 10 tj binXA ⋅= , (3-3) 
in which M is the number of frames of x(t), and X[⋅] 
denotes the spectrum of x(t). 
5. Start from frame t and move backward and forward 
up to frames jh  and jt  such that both 
][log20 10 jhbinX⋅  and ][log20 10 jtbinX⋅  are 
smaller than )20( −jA (dB). 
6. Start from frames jh  and jt , find frames α−jh  
and β+jt  (α, β > 0) such that both 
][log20 110 −−⋅ αjhbinX  and ][log20 110 ++⋅ βjtbinX  
are greater than )20( −jA . Then α−jh  and 
β+jt  are called the head frame and tail frame of 
syllable j. 
7. Set ,,h,hm,]bin[X jjm L1 0 +−−== αα  
  ββ +−+ jj t,t 1 . (3-4) 
8. Let 1+= jj . 
9. Repeat Step 4 to Step 8 until 201−< AA j . 
3.1.2 Normalization and pre-emphasis 
 The normalization process regulates the discrepancy 
of the voice amplitudes caused by the diversity of 
recording environments. In this study, the amplitudes 
were linearly normalized to the region of [-1, 1]. Besides, 
since the amplitude of the high frequency is usually 
much smaller than that of the low frequency, pre-
emphasis was applied to intensify the signal of the high 
frequencies. The intensification was accomplished by a 
finite impulse response (FIR) filter with the following 
form 
11)( −⋅−= zazH , (3-5) 
so that a signal x[n] after the filtering process has the 
following property 
],1[][][ −−= naxnxnx  (3-6) 
in which α is a scalar usually between 0.9 and 1, which 
was set as 0.95 in this study. 
 
3.1.3 Segmentation 
  Focusing on the recognition of a section of birdsong, 
the segmentation process in this system aimed at 
segmenting a period of syllables rather than an individual 
syllable similar to many other researches [2]-[4], [7]-[9]. 
Extracting the feature vector of a period of syllables is 
practical for birdsong recognition because the syllables 
of a birdsong are usually repetitive. After endpoint 
detection, normalization and pre-emphasis, the 
segmentation process was done by detecting the 
repetition of syllables as described in the following 
1. Set 1=i  the index of the first syllable of the 
segmentation. 
2. Find the nearest syllable j such that the similarity 
between syllables i and j, ijsim , is smaller than α, j 
is the last syllable of the segmentation. 
3. Set the segmentation length jl = . 
4. Set 1+= jk , the checking index. 
5. Set 1=i , jl = . 
6. Compute the similarity, kisim , between syllable k 
and syllable i. 
7. If α>kisim  (the same type), then 
If jkl −=  then Stop. The segmentation is from 
syllable 1 to syllable l. 
If ji = , then 1+= jj  go to Step 5. 
set 1+= ii  and 1+= kk , and go to Step 6. 
8. If ji = , then 1+= jj  go to Step 5. 
9. Set 1+= kk , 1+= jj , 1+= ll  and go to Step 6. 
 The similarity between two syllables was determined 
by computing the difference between the amplitudes of 
corresponding frequency Bins. A normalization process 
was applied for the syllable lengths before the 
computation. Since the syllable types of a birdsong are 
usually within 6, in our experiment, α was set as 
82 ≤≤ l . In fact, syllables of the same type in a section 
of a birdsong usually have small variances in amplitude 
and length. After segmentation, the segmented syllables 
were aligned for feature extraction. 
 
3.2 Feature extraction – the WMFCCs 
After segmenting a period of syllables, the aligned 
syllables were used to compute the feature vector of the 
birdsong. The process for obtaining the feature vector 
WMFCCs is shown in Fig. 3.3 and is described below. 
 
3.2.1 Computing the MFCCs of each frame 
The steps for computing the MFCCs of each frame 
are as follows: 
1. Compute the fast Fourier transform (FFT) of each 
 4
learning 
 ),( gl
g
l
g
l ggg
η wxww φ∇+= , (3-16a) 
 else the anti-reinforcement learning 
 ),( gl
g
l
g
l ggg
η wxww φ∇−= , (3-16b) 
 where gl gw  denotes the weight vector of 
g
l g
φ  and 
η is the learning rate. 
 In this study, the function ijφ  was defined as a 
radius basis function as shown in (3-15), so the gradient 
operation result in (3-16) was 
 gl
g
l gg
wxwx −=∇ ),(φ . (3-17) 
 In the recognition process, the feature vector of a test 
birdsong was obtained by the same process as the 
training part. After inputting the feature vector to the 
DBNN, the global winner of the network (i.e. the 
network output) indicated the species class the test 
birdsong belonged to. 
 
4. Experimental results 
 The bird species vocalization database used in this 
study was obtained from a commercial CD [24] 
containing both birdcall and birdsong files of 420 bird 
species recorded in the field in Japan. Each file contained 
vocalizations of the same bird species. The database of 
420 bird species made it much larger than any other used 
in previous studies. Meanwhile, recordings in the field 
were usually in a noisy environment, incomplete and 
interrupted. Because of this, sometimes, only some 
syllables of a birdsong, rather than a complete birdsong, 
can be used in the recognition process. So, two 
recognition problems were applied in the experiments, 
one was the recognition of a set of arbitrary syllables of a 
bird species and the other was the recognition of a 
section of a birdsong. The sampling rate of these 
vocalization signals was 44.1 kHz with 16-bit resolution 
and a monotone type PCM format. In the experiment, the 
frame size was set as 512 samples with three-fourths 
frame overlapping. 
 
4.1 Recognition of a set of arbitrary syllables from the 
song of a bird species 
The purpose of the first experiment was to examine 
the efficiency of the DBNN. After segmentation, half the 
syllables of each birdsong file were randomly selected 
for training and the remaining for testing. The MFCCs of 
each frame of a training syllable were obtained by using 
the steps in sec. 3.2.1. After computing the first 15 order 
MFCCs of each frame, the coefficients of the same order 
of all frames were averaged and then used to form a 15-
dimensional syllable feature vector. Such a feature vector 
had been successfully used in many studies, so it was 
used for checking. In the NN training process, the feature 
vector of the training syllable was used as the input and 
the corresponding bird species as the desired output. 
Because back-propagation NN (BPNN) was widely 
applied in many studies, both BPNN and DBNN were 
applied in this experiment for comparison. 
For recognition, the same feature extraction process 
was applied to the test syllables. The extracted feature 
vector of a test syllable was used as the input of the NN 
whose output indicated the bird species of the test 
syllable. By comparing the network output and the actual 
species for each test syllable, the recognition rates (RRs) 
of all test syllables were obtained. Table 4.1 shows the 
RRs of both types of NN. It shows that the DBNN 
improved the RR from about 5% on the BPNN. 
 
4.2 Recognition of a section of a birdsong 
 The second experiment was the recognition of a 
section of a birdsong containing several consecutive 
syllables. In this case, both types of feature vectors 
included the MFCCs of a single syllable (FV1) and the 
WMFCCs of a section of birdsong (FV2) were applied 
for comparison. Meanwhile, both types of NN were used. 
In the experiment, half of each birdsong file was used for 
training and the remaining for testing. By using the 
segmentation process, both training and testing data sets 
contained several birdsong sections of each bird species. 
When FV1 was applied, all the syllables in a birdsong 
section (training or testing) were treated individually. In 
the recognition of a birdsong section, the species was 
determined by finding the species class with the largest 
number of NN output. For FV2, the feature vector of a 
period of syllables in the birdsong section (training or 
testing) was used as the input of NN. In the recognition 
process, the NN output indicated the species class of the 
test birdsong section. In this experiment, the recognition 
rate RR was defined as 
%
(%)RR
100
sections birdsong test ofnumber 
correctly recognized sections birdsong test ofnumber ⋅
=
  (4-1) 
 The RRs of all four structures are shown in Table 4.2. 
It was found that when the segmentation method and the 
feature vector were fixed, the DBNN improved the RRs 
of 10.68%, 7.35%, 4.48% and 4.72% on BPNN. When 
the segmentation method and the NN were fixed, the 
proposed WMFCCs improved the RRs of 18.72%, 
17.38%, 12.34% and 14.75% on the MFCCs. The above 
comparisons exhibited the efficiency of the proposed 
methods, especially the feature extraction. When the 
proposed two methods FV2 and DBNN were combined, 
the system achieved an RR of 77.89%. 
 
4.3 Recognition with threatened birdsongs taken into 
account 
 The territorial instinct is innate in some bird species 
so that they will make threatening noises when there is 
an intruder. The threatening voice is used to warn other 
birds in the same group and frighten the invaders away. 
Such vocalization is usually distinct from the regular 
birdsong requiring additional categorization. That is, two 
types of birdsongs are considered if the bird species also 
makes threatening noises. In this case, the feature 
extraction process was applied for both regular birdsongs 
and threatening noises so that some bird species were 
represented by two types of feature vectors. In the 
recognition process, the feature vector of the test 
birdsong section was matched by the NN to the feature 
vectors of all species to determine the most likely species. 
 The RRs of all four structures used to recognize 
 6
 
Preprocessing 
Feature 
Extraction 
Birdsong 
signal 
Recognition
(species decision)
Bird species
database
 
Figure 3.1 Block diagram of the proposed system 
 
Improved syllable 
endpoint detection Normalization 
Birdsong 
signal Pre-emphasis Segmentation
 
Fig. 3.2 Block diagram of the preprocessing 
 
 
 
 
 
Figure 3.3 Flow chart of computing the feature vector 
WMFCCs 
 
 
 
 
 
 
 
 
 
 
 
Figure 3.4 Applied 3-level wavelet transformation. 
 
 
 
 
 
 
 
 
 
 
(a) First-order MFCCs of the birdsong segment. 
 
 
 
 
 
 
 
 
 
 
(b) Six WMFCC sequences obtained from (a) 
Figure 3.5 WMFCC sequences obtained from a birdsong 
segment of the Accipiter nisus. 
 
 
 
 
 
 
 
 
 
 
Figure 3.6 Structure of the applied DBNN. 
 
 
 
Table 4.1 RRs of using different types of NN 
MFCC+BP MFCC+DBNN 
59.76% 64.93% 
 
Table 4.2 RRs of using various structures 
Structure RR 
FV1+BPNN 54.69% 
FV1+DBNN 65.37% 
FV2+BPNN 73.41% 
FV2+DBNN 77.89% 
 
Table 4.3 RRs of using various structures 
Structure A B
RS+FV1+BPNN 54.69% 59.16%
RS+FV1+DBNN 65.37% 69.24%
RS+FV2+BPNN 73.41% 76.97%
RS+FV2+DBNN 77.89% 83.11%
 A: Recognition without the distinction of threatened voices. 
 B: Recognition with the distinction of threatened voices. 
First order MFCCs 
 Compute the 
MFCCs of 
each frame 
Align the 
MFCCs of 
all frames 
Aligned 
birdsong 
signal 
Compute the
WMFCCs 
Compute the
mean of the 
WMFCCs
 
H 
L 
HH
HL 
LH 
HHH 
HHL 
LLH 
LL LLL 
Sm 
 
max max max
max 
… … 
… … 
Class network 1 Class network i Class network C
Local winner 1 Local winner i Local winner C
Global winner 
input 
Actual classThe same 
Reinforcement learning
Anti-reinforcement
learning
yes 
no
1
1 φ 12φ 13φ i1 φ i2φ  i3φ  C1 φ C2φ C3φ
表 Y04 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                           98 年 9 月 30 日 
報告人姓名  周智勳 
 
服務機構
及職稱 
 
中華大學資訊工程系 
 
     時間 
會議 
     地點 
2009/9/12~2009/9/14 
日本-京都 
本會核定
補助文號
計劃編號： 
NSC 97-2221-E-216-014 
會議 
名稱 
(英文) The Fifth International Conference on Intelligent Information Hiding and 
Multimedia Signal Processing (IIHMSP 2009) 
發表 
論文 
題目 
(英文) 1. A New Feature Integration Approach and its Application to 3D Model Retrieval 
2. Modulation Spectral Analysis of Static and Transitional Information of Cepstral and Spectral 
Features for Music Genre Classification 
報告內容應包括下列各項： 
一、參加會議經過 
這次會議的地點在京都車站前的一棟會議廳，交通相當方便。我和同事在會議
前一天晚上抵達，就住在車站附近。 
 
報告時間被排在 session 議程第一天的早上，會議流程同一時間只有三、四個
sessions 同時進行，因此每個 session 聆聽的人數也比較多。會議室場地還不錯，
然而會議室外的空間較小，不是很方便做其他的交流，也沒看到什麼參展的廠商，
也許是因為這樣，所以沒有 poster 型式，交流機會也較少。 
 
午餐發的餐卷可以直接拿去附近的餐廳用餐，主要是蕎麵類的定食。第一天晚
上的迎賓宴採半雞尾酒式，大家站著聊天，台灣來的學者相當多，大陸的也不少，
會場到處充斥著中文。 
 
第二天晚上正式的晚宴也是自助式，菜色不錯，涵蓋了不少日本食物的特色。
晚宴中也頒發了一些獎狀及謝函，並穿插了茶藝的表演，只是表演擺的位置不是很
好，很多人看不到。 
 
 
二、與會心得 
1. 會場的佈置似乎不像國內辦研討會的熱鬧，國內辦研討會，會花不少心思在會
場佈置上，感覺比較有氣氛。 
2. 國內研討會都會提供甜點，國外好像大都不來這一套，較為陽春? 
3. 此次研討會沒有參展的攤位，較特殊。 
4. 會議路徑指示不甚明確，需加強。 
5. 此次之與會人員，相當專業，參與態度非常積極，值得效法。 
附件三
 
