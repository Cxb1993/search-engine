will extend the interface of VMC parallel program and support more 
architecture and applications, such as multi-core library like 
OpenMP and Intel Threading Building Block(TBB), and strengthen 
the interface of other sub-projects. At the same time, we will build a 
set of multi-core embedded systems which support parallel program 
optimization model for OpenMP and Intel Threading Building 
Block API. 
 
 中、英文摘要及關鍵詞(keywords)。 
 
本子計畫主要設計與實作多核心嵌入式軟體之平行程式優化支援。今日，處理器研發
業者已不再依賴提高時脈速度方法來增進效能，因絕對的效能、增加的耗電量、以及攀高
的成本使邊際效益呈現逐漸遞減的趨勢，目前，多數產業都已體認多重核心(Multi-core)
是未來的發展方向，其主要設計挑戰已逐漸被成功克服，實際的建置行動更已經展開。多
處理技術存在著三大挑戰，程式分割、程式平行和最佳化。目前最需要的是一個編譯模型
，使其得以開發平行應用程式，將這些應用程式映射到平行硬體並進行最佳化，以及彙集
資料以作出最佳化的決策。有效的平行迴圈切割與排程可以顯著地減少程式在多核心處理
器系統環境中的整體回應時間，特別是針對具有許多迴圈的應用程式與針對新興的多核心
嵌入式系統環境，其優化處理也更為困難。本子計畫第一年(2009/8~2010/7)，將使用編譯
器技術如資料相依性分析與軟體管線化技術，完成指令階層平行化與平行迴圈切割與迴圈
排程優化支援。本子計畫第二年(2010/8~2011/7)，將擴充 VMC 平行程式優化介面支援至
更多的架構平台與應用，如支援多核心程式庫例如OpenMP及 Intel Threading Building Block 
(TBB)及強化與其他子計畫的介面。同時將建置一套在多核心嵌入式系統上平行程式優化
支援模式適用於 OpenMP及 Intel Threading Building Block應用程式介面。 
 
關鍵詞：平行程式、優化支援、資料相依性分析、指令階層平行化、平行迴圈 
 
The main goal of this sub-project is to design and implement the parallel program optimization 
supporting for VERTAF/Multi-Core (VMC) embedded software. Today, the processor 
researcher is not dependent on the method of enhanced clock speed to improve performance. 
Because of absolute performance, the increase of power consumption, as well as the rising cost 
of marginal benefit to make a gradually descending trend. At present, most industries have been 
considered multi-core is the future of development. The main design challenge has succeeded 
gradually overcomes, the actual operations have already started to build. The multiprocessing 
and multi-core technology has three major challenges, program partition, and program 
parallelization and program optimization. The most important need is a compiler model, which 
enables to develop parallel applications. Using these applications mapped to the parallel 
hardware and run the action of optimization, as well as to collect information in order to make 
the best decision. Effective parallel loop splitting and scheduling can obviously reduce the 
program response time in the multiple processor environment, especially that optimize process 
is also more difficult when aims at a many circle application programs and emerging multi-core 
embedded system environment. First year, we will use compiler technology, such as the data 
dependency analysis and software pipelining that implement parallel instruction, parallel loop 
partitioning, and loop scheduling and optimization. Second year, we will extend the interface of 
VMC parallel program and support more architecture and applications, such as multi-core 
library like OpenMP and Intel Threading Building Block(TBB), and strengthen the interface of 
other sub-projects. At the same time, we will build a set of multi-core embedded systems which 
support parallel program optimization model for OpenMP and Intel Threading Building Block 
API. 
報告內容：請包括前言、研究目的、文獻探討、研究方法、結果與討論（含結論與
建議）…等。若該計畫已有論文發表者，可以 A4 紙影印，作為成果報告內容或附
錄，並請註明發表刊物名稱、卷期及出版日期。若有與執行本計畫相關之著作、專
利、技術報告、或學生畢業論文等，請在參考文獻內註明之，俾可供進一步查考。 
 
1. 前言 
    由於半導體運作溫度和功耗的問題限制了單核心微處理器效能增長。這個原因導致許
多微處理器供應商轉向多核心晶片。當今多核心處理器已經是硬體市場的主流，不僅處理
器走向隨之而來的多核心(Multicore)處理器趨勢。同時，並行處理不僅只是一個機會，也是
一個挑戰。程式設計師或編譯器將軟體平行化是在多核心晶片上提高效能的關鍵，而且編
譯人員也必須面對多核心處理器所帶來的利益與衝擊。平行程式化(Parallel Programming)
技術存在著三大挑戰，程式分割、程式平行和最佳化。目前最需要的是一個平行編譯模型，
使其得以開發平行應用程式，將這些應用程式映射到平行硬體並進行最佳化，以及彙集資
料以作出最佳化的決策。有效的平行迴圈切割與排程可以顯著地減少程式在多核心處理器
系統環境中的整體回應時間，特別是針對具有許多迴圈的應用程式與針對新興的多核心嵌
入式系統環境，其優化處理也更為重要。本子計畫的研究重點主要是設計與實作多核心嵌
入式軟體之平行程式優化支援。在本計畫中，我們實作一套基於 OpenMP 的自動平行化工
具，以產生能運行在多核心系統上之平行程式碼，其中我們使用 ROSE 開放原始碼編譯器
為核心，針對 ROSE 深入探討並實作一個使用者介面以簡化使用之複雜度。本計畫完成一
套自動程式碼平行化整合系統，系統包含如何去判斷該程式是否適合平行化及所需要的背
景知識以及技巧等內容。並介紹如何讓使用者透過我們提供的 GUI圖形使用者介面或是使
用 Eclipse外掛(Plug-In)，透過圖形操作方式，來使程式碼自動平行使得自動平行更加的容
易。在多核心嵌入式系統上如果只運行普通的程式將無法達到應有的效能，所以透過適當
的平行化轉換，會節省許多的時間。本計畫實驗有兩個部分，首先，我們證明了這些自動
平行化的工具可行性與正確性。接著，分別於一般的處理器和嵌入式系統上運行並討論其
效能比較。最後，我們將展示如何應用此工具來使得程式在多核心系統上有更佳的效能。 
 
關鍵詞：嵌入式軟體 (Embedded Software)、多核心 (Multicore)、平行程式化 (Parallel 
Programming)、優化處理(Optimization)、OpenMP 
 
2. 研究目的 
有效的平行迴圈切割與排程可以顯著地減少程式在多核心處理器系統環境中的整體回
應時間，特別是針對具有許多迴圈的應用程式與針對新興的多核心嵌入式系統環境，其優
化處理也更為重要。本子計畫的研究重點主要是設計與實作多核心嵌入式軟體之平行程式
優化支援。在本計畫中，我們實作一套基於 OpenMP 的自動平行化工具，以產生能運行在
多核心系統上之平行程式碼，其中我們使用 ROSE 開放原始碼編譯器為核心，針對 ROSE
深入探討並實作一個使用者介面以簡化使用之複雜度。本計畫完成一套自動程式碼平行化
整合系統，系統包含如何去判斷該程式是否適合平行化及所需要的背景知識以及技巧等內
容。並介紹如何讓使用者透過我們提供的 GUI 圖形使用者介面或是使用 Eclipse 外掛
(Plug-In)，透過圖形操作方式，來使程式碼自動平行使得自動平行更加的容易。在多核心嵌
入式系統上如果只運行普通的程式將無法達到應有的效能，所以透過適當的平行化轉換，
會節省許多的時間。本計畫實驗有兩個部分，首先，我們證明了這些自動平行化的工具可
行性與正確性。接著，分別於一般的處理器和嵌入式系統上運行並討論其效能比較。最後，
 
3.3 OpenMP 
OpenMP（Open Multi-Processing）是由OpenMP Architecture Review Board牽頭提出的，
並已被廣泛接受的，用於共享內存並行系統的多執行緒程序設計的一套指導性注釋
（Compiler Directive）。OpenMP支持的程式語言包括C語言、C++和Fortran；而支持
OpenMP的編譯器包括Sun Studio和Intel Compiler，以及開放源碼的GCC和Open64編譯
器。OpenMP提供了對並行演算法的高層的抽象描述，程式設計師通過在原始碼中加入
專用的pragma來指明自己的意圖，由此編譯器可以自動將程序進行並行化，並在必要
之處加入同步互斥以及通信。當選擇忽略這些pragma，或者編譯器不支持OpenMP時，
程序又可退化為通常的程序（一般為串列），代碼仍然可以正常運作，只是不能利用
多執行緒來加速程序執行。 
OpenMP提供的這種對於並行描述的高層抽象降低了並行編程的難度和複雜度，這樣程
式設計師可以把更多的精力投入到並行演算法本身，而非其具體實現細節。對基於數
據分集的多執行緒程序設計，OpenMP是一個很好的選擇。同時，使用OpenMP也提供
了更強的靈活性，可以較容易的適應不同的並行系統配置。執行緒粒度和負載平衡等
是傳統多執行緒程序設計中的難題，但在OpenMP中，OpenMP庫從程式設計師手中接
管了部分這兩方面的工作。 
 
3.4 Intel Threading Building Block 
Intel® Threading Building Blocks (TBB) 是一個協助將演算法平行化處理的函式庫。利
用多核心可以同時處理多執行緒的優點，TBB自動將資料拆成數塊，分別由各個執行
緒執行後，再將結果彙整。以往要自己手動處理多個執行緒的管理以及同步等問題，
TBB都已經自動做好了。 
1. TBB 大量縮減程式碼大小 
2. TBB 利用抽象化，隱藏了大量 Multi-threading 的程式複雜度 
3. TBB's task manager 自動分析系統與軟體所執行的環境，自動選擇最佳的 thread數
目，會完成 load balance 在所有處理器核心上，達到系統最佳化。 
4. 所以, TBB threaded applications 會自動有效的放大，適應將來更多核心的硬體環 
境。 
 
3.5 Python 
是一種物件導向、直譯式電腦程式語言，也是一種功能強大而完善的通用型語言，已
經具有十多年的發展歷史，成熟且穩定。這種語言具有非常簡捷而清晰的語法特點，
適合完成各種高層任務，幾乎可以在所有的作業系統中執行。目前，基於這種語言的
相關技術正在飛速的發展，使用者數量急劇擴大，相關的資源非常多。 
Python可能被粗略地分類為「指令碼語言」（script language），但實際上一些大規模
軟體開發計劃例如Zope、Mnet及BitTorrent，Google也廣泛地使用它。Python的支持者
較喜歡稱它為一種高階動態編程語言，原因是「指令碼語言」泛指僅作簡單編程任務
的語言，如shell script、JavaScript等只能處理簡單任務的編程語言，並不能與Python相
提並論。 
此外，由於Python對於C和其他語言的良好支援，很多人還把Python作為一種「膠水語
言」（glue language）使用。使用Python將其他語言編寫的程式進行整合和封裝。在Google
內部的很多項目使用C++編寫性能要求極高的部分，然後用Python呼叫相應的模組。 
 
 
4. 研究方法 
本子計畫範圍包含建置下列主系統與各項子系統，主系統為： 
 平行程式優化支援實作-VMC_PPO(Implementation of Parallel Program Optimization 
Supporting for VMC – VMC_PPO) 
各子系統分別為 
 Source Program Parsing Subsystem 
 Parallelism Subsystem 
 Optimization Subsystem  
 Logging Subsystem 
隨著處理器從單核心進展至雙核心、四核心，甚至部分應用於伺服器的八核心之計算平
臺，如何有效利用多核心的特性，並使應用程式於多執行緒的架構下加速運行，就成了
開發人員極需研究的課題。關於嵌入式計算領域，也從單核心成長至多核心，為使原本
效能備受限制之嵌入式裝置，透過多核心同步運行之機制，盡可能提高其運算效能，本
計畫將建置一高效能平行編譯器，載入既有之程式碼，透過逐行逐段分析，輔以高效能
平行演算法，基於 OpenMP 及 Intel Threading Building Block多核心函式庫，產出平行
程式碼，使其達到多核心同步執行，加速其運算速度，並達到最大之產出，其概觀如圖 
1高效能平行編譯器概念圖所示。 
 
 
 
圖 1高效能平行編譯器概念圖 
 
本子計畫所開發，結合了平行技術與程式碼優化演算法，提供程式碼優化的基本功能
和學習模式。本計畫建置一多核心嵌入式軟體之高效能平行編譯器 (PPO)，載入既有
之程式碼，透過逐行逐段分析，輔以高效能平行演算法，基於 OpenMP 及 Intel Threading 
Building Block多核心函式庫，產出平行程式碼，使其達到多核心同步執行，加速其運
算速度，並達到最大之產出。有鑒於以往程式優化系統的全面性，我們採取了區塊優
化，也就是經過學習模式就每個迴圏區塊做一次優化並記錄下來，以迴圏執行效率做
為比較，去判定運算迴圏是否需要程式優化，因此本系統利用 Intel TBB 與 OpenMP
以及 ROSE來達到程式碼平行與優化的效果。 
Code 
Source 
Code Optimization Code Parallelism Revised 
Code Output 
Input 
directory
Control flow analysis
Data flow analysis
More control flow 
analysis
Static-single assignment
Global optimizations
Array dependence 
analysis
Loop transformations
Self-optimizing 
programs
basic blocks & loops
lattice theory, iterative frameworks, 
reaching definitions, liveness
dominators, postdominators, control 
dependence
static-single assignment, constant 
propagation
loop invariant code motion, common 
subexpression elimination, strength 
reduction
integer linear programming, dependence 
abstractions
linear loop transformations, loop fusion/
fission, enhancing parallelism and locality
-I    input directory
-o    outptut directory
-omp    auto add openMP
-e    auto execute
-c    auto compile
-l    learning mode
-t    auto tuning(need -p)
-p   performance log
Output 
directory
essential non-essential
Auto add OpenMP directives depends on 
pre-analysis Performance records
 
圖 3 系統流程示意圖 
圖 3 系統流程示意圖為系統執行流程，當程式碼由子計畫四接收後，中間共會經過八
道處理程序，以下一一列舉說明： 
1. Control flow analysis 
 分析程式中的內部區塊為階層式與計算迴圏影響值。 
2. Data flow analysis 
分析程式中變數的生命周期。 
3. More Control flow analysis 
 分析程式中控制相依性。 
4. Static-single assignment 
 常數影響係數、靜態單一賦值。 
5. Global Optimizations 
迴圏不變碼的移動、常見子表示消除與簡化。 
6. Array dependence analysis 
 整數線性程式化、相依性抽象化。 
7. Loop transformation 
平台的環境。 
 
2. 平行子系統(Parallelism Subsystem, PS) 
當接受前端程式剖析子系統之結果，針對剖析子系統所判斷出各個可平行化之標記
部份，予以新增符合 TBB 或 OpenMP 之函式庫導引器，使其達到平行優化之目
標。圖 6 PS子系統架構圖為 PS子系統架構圖。 
 本子系統接受的輸入為剖析子系統之剖析紀錄檔，針對使用者指定之平行函式目
標(TBB 或 OpenMP)，於各個可平行化之部份，增加函式導引器，最後產生平行程
式碼。 
 
圖 2 PS子系統架構圖 
 
3. 優化子系統(Optimization Subsystem, OS) 
將上一個子系統所產生出來的程式碼，更進一步去做最佳化。就是在除了平行化的
程式碼外，再去尋找可以使之增進執行效率的程式碼區塊，加以最佳化並產出最後
的完成碼。圖 7 OS子系統架構圖為 OS子系統架構圖，本子系統之組成如下： 
Scanning：提供程式碼掃視  
Analysis：程式碼優化區塊分析  
Modify：處理欲優化程式區塊行為 
 
圖 3 OS子系統架構圖 
4. 紀錄子系統(Logging Subsystem, LS) 
LS  功能為紀錄程式編譯優化過程中，所標記之程式碼行號以及所加入之程式導引
器，並將之輸出以做檢驗或查詢用途。圖 8  LS子系統架構圖為 LS子系統架構圖，
本子系統之組成如下： 
A. Markup code log：紀錄程式碼被剖析時的所有 SPPS 行為 
B. Parallel code log：紀錄程式碼在平行化時所有 PS 行為 
C. Optimize code log：紀錄程式碼優化其間 OS 的所有行為 
 
圖 5 Auto Parallel執行畫面 
Loop Optimizer在迴圏優化中主要使用三種不同的演算法： 
A. Loop fusion 
 將有相依性的 Loops做結合 
B. Loop splitting 
 將 Loop分裂成其他 loops 
C. Loop unrolling 
 直接將 Loop展開 
圖 10  Loop Optimizer執行畫面為執行 Loop Optimizer的畫面，呼叫的參數
定義為： 
  -i   指定為優化/平行原始碼的資料夾位置 
  -o  指定優化/平行過後原始碼輸出的資料夾位置 
  -optimize 是否執行優化功能 
 
接下來我們將我們的函式庫做成 Eclipse的 plugin，整個流程如圖 12 
 
圖 12 判斷流程 
 
圖 13  Eclipse使用者圖形介面 
接下來開啟所想要平行化的 C/C++的程式碼，並按下圖 13中被紅色圈所框起來的按鈕，或
是使用上方工具列的[VMC-PPO-Parallelization] -> [Auto Parallelization]，接下來我們就會跳
出一個視窗如圖 14。 
 
 
VMC-PPO使用手冊 
一、事前準備 
1. 建議作業系統：Fedora 12。 
2. Linux 環境變數：$VMC_PPO_PATH(VMC-PPO套件路徑)。 
 3. 必要軟體套件：JRE(執行 Eclipse所需), Eclipse 3.5(VMC-PPO所用之整合開發環境), 
unzip(解壓縮軟體), Eclipse CDT(開發 C/C++所需之 Eclipse Plugin)。 
 
二、安裝步驟及使用方式 
ROSE 
Environment: Ubuntu 9.10 
Before install ROSE, following as command 
I. $ sudo aptget install libboost1.4-* 
II. $ sudo aptget install sun-jdk-6-sun 
 
The steps of installation as following: 
I. Download package from https://outreach.scidac.gov/projects/rose/ 
II. Untar package 
$ tar –zxfv rose-0.9.5a-without-EDG-15163.tar.gz  
III. Run the configure script 
$ ./configure –with-java={path of JAVA} 
IV. Run make 
$ make 
V. To install ROSE, type make install 
$ make install 
VI. Set PATH 
 
Par4All 
The steps of installation as following: 
I. Download package from http://www.par4all.org/download/ 
II. Untar package 
$ tar –zxfv <package>.tar.gz  
    
 
 
 3. 安裝 Eclipse Plugin 
    $ su  (取得 root最高權限) 
    # cd /usr/lib/eclipse (進入 eclipse主目錄) 
    # unzip {plugin壓縮檔的路徑/vmcppo.zip} 
 
 4. 使用方式 
  * 啟動 Eclipse，便會看到已安裝之 VMC-PPO Plugin 
   
 
    * 以及可設定 perspective 
     (請設定 perspective為 VMC-SP6-PPO 以利平行化工具使用) 
計畫成果自評部份，請就研究內容與原計畫相符程度、達成預期目標情況、研究成
果之學術或應用價值、是否適合在學術期刊發表或申請專利、主要發現或其他有關
價值等，作一綜合評估。 
 
Publication list 
Journal papers 
[1] Chao-Tung Yang*, Chih-Lin Huang and Cheng-Fang Lin, "Hybrid CUDA, OpenMP, 
and MPI Parallel Programming on Multicore GPU Clusters", Computer Physics 
Communications, Volume 182, Issue 1, January 2011, Pages 266-269. (ISSN: 0010-4655, 
SCI JCR=2.300, 6/54, EI) 
[2] Chao-Tung Yang*, Chao-Chin Wu, and Jen-Hsiang Chang, "Performance-based Parallel 
Loop Self-Scheduling Using Hybrid OpenMP and MPI Programming on Multicore SMP 
Clusters", Concurrency and Computation: Practice and Experience, Volume 23, Issue 
8, pages 721–744, 10 June 2011. (ISSN: 1532-0626, SCI JCR IF=0.907, EI)  
[3] Chao-Tung Yang*, Wen-Chung Shih, and Lung-Hsing Cheng, "A Performance-based 
Dynamic Loop Scheduling on Heterogeneous Clusters", Journal of Supercomputing, 
Springer Netherlands, Article in Press, April 2010. (ISSN: 1573-0484, SCI JCR 
IF=0.545, EI) 
[4] Chao-Chin Wu*, Chao-Tung Yang, Kuan-Chou Li, and Po-Hsun Chiu, "Designing 
parallel loop self-scheduling schemes using the hybrid MPI and OpenMP programming 
model for multi-core grid systems," Journal of Supercomputing, Springer Netherlands, 
Article in Press 2011. (ISSN: 1573-0484, SCI JCR IF=0.545, EI) 
[5] Chao-Chin Wu*, Lien-Fu Lai, Chao-Tung Yang, and Po-Hsun Chiu, “Using Hybrid 
MPI and OpenMP Programming to Optimize Communications in Parallel Loop 
Self-Scheduling Schemes for Multicore PC Clusters”, Journal of Supercomputing, 
Springer Netherlands, Article in Press 2011. (ISSN: 1573-0484, SCI JCR IF=0.545, EI) 
[6] Chao-Tung Yang* and Kuan-Chou Lai, "A Directive-based MPI Code Generator on 
Linux PC Clusters," Journal of Supercomputing, Springer Netherlands, Volume 50, 
Number 2, pp. 177-207, Nov., 2009. (ISSN: 1573-0484, SCI JCR IF=0.615, EI) 
Conference papers 
[1] Hsu, Ching-Hsien; Chen, Shih-Chang; Yang, Chao-Tung; Dynamic Voltage 
Adjustment for Energy Efficient Scheduling on Multi-core Systems, 2011 Ninth IEEE 
International Symposium on Parallel and Distributed Processing with Applications 
Workshops (ISPAW), Publication Year: 2011, Page(s): 197 – 202. 
[2] Yang, Chao-Tung; Chang, Tzu-Chieh; Wang, Hsien-Yi; Chu, William C.C.; Chang, 
Chih-Hung; Performance Comparison with OpenMP Parallelization for Multi-core 
Systems, Parallel and Distributed Processing with Applications (ISPA), 2011 IEEE 9th 
International Symposium on, Digital Object Identifier: 10.1109/ISPA.2011.60, 
Publication Year: 2011 , Page(s): 232 – 237 
[3] Chao-Tung Yang; Lung-Teng Chen; Wei-Li Chou; Kuan-Chieh Wang; Implementation 
of a Medical Image File Accessing System on Cloud Computing, Computational Science 
and Engineering (CSE), 2010 IEEE 13th International Conference on, Digital Object 
Identifier: 10.1109/CSE.2010.48, Publication Year: 2010 , Page(s): 321 - 326  
[4] Chao-Tung Yang*, Chih-Lin Huang, Cheng-Fang Lin, and Tzu-Chieh Chang, “Hybrid 
Parallel Programming on GPU Clusters”, Parallel and Distributed Processing with 
Applications (ISPA), 2010 International Symposium on, pp. 142-147, Taipei, Taiwan, 
Sept. 6-9, 2010. 
可供推廣之研發成果資料表 
□ 可申請專利  ■ 可技術移轉                                     日期：100年10月31日 
國科會補助計畫 
計畫名稱：多核心嵌入式軟體之模型驅動整合開發環境-VMC--子計
畫六:多核心嵌入式軟體設計工具系統之平行程式優化支援實作
-VMC_PPO(2/2) 
計畫主持人：楊朝棟 
計畫編號：NSC 99-2220-E-029-004  學門領域：資訊 
技術/創作名稱 多核心嵌入式軟體設計工具系統之平行程式優化支援實作 
發明人/創作人 楊朝棟 
技術說明 
中文： 
本子計畫主要設計與實作多核心嵌入式軟體之平行程式優化支
援。今日，處理器研發業者已不再依賴提高時脈速度方法來增進效
能，因絕對的效能、增加的耗電量、以及攀高的成本使邊際效益呈
現逐漸遞減的趨勢，目前，多數產業都已體認多重核心(Multi-core)
是未來的發展方向，其主要設計挑戰已逐漸被成功克服，實際的建
置行動更已經展開。多處理技術存在著三大挑戰，程式分割、程式
平行和最佳化。目前最需要的是一個編譯模型，使其得以開發平行
應用程式，將這些應用程式映射到平行硬體並進行最佳化，以及彙
集資料以作出最佳化的決策。有效的平行迴圈切割與排程可以顯著
地減少程式在多核心處理器系統環境中的整體回應時間，特別是針
對具有許多迴圈的應用程式與針對新興的多核心嵌入式系統環
境，其優化處理也更為困難。本子計畫第一年(2009/8~2010/7)，將
使用編譯器技術如資料相依性分析與軟體管線化技術，完成指令階
層平行化與平行迴圈切割與迴圈排程優化支援。本子計畫第二年
(2010/8~2011/7)，將擴充 VMC 平行程式優化介面支援至更多的架
構平台與應用，如支援多核心程式庫例如OpenMP及 Intel Threading 
Building Block (TBB)及強化與其他子計畫的介面。同時將建置一套在
多核心嵌入式系統上平行程式優化支援模式適用於 OpenMP 及
Intel Threading Building Block應用程式介面。 
附件二 
技術特點 
本系統的主要功能可區分為四個部份，分別敘述如下： 
 程式剖析子系統(Source Program Parsing Subsystem, 
SPPS) 
SPPS接受使用者輸入之原始程式碼，逐行進行剖析，主要
鎖定多數可平行運行之迴圈，以平行程式撰寫時常用的關
鍵字與平行化之架構為基礎，針對輸入程式之上下文，以
及各種區塊符號，判斷出各個程式區塊，最後交付後端之
平行優化子系統。 
 平行子系統(Parallelism Subsystem, PS) 
PS 接受前端程式剖析子系統之結果，針對剖析子系統所
判斷出各個可平行化之標記部份，予以新增符合 TBB 或
OpenMP之函式庫導引器，使其達到平行優化之目標。 
 優化子系統(Optimization Subsystem, OS) 
OS 提供將平行子系統產生之程式碼進行最佳化，並將之
輸出。 
 紀錄子系統(Logging Subsystem, LS) 
LS紀錄程式編譯優化過程中，所標記之程式碼行號以及所
加入之程式導引器，並將之輸出以做檢驗或查詢用途。 
推廣及運用的價值 
本計畫針對 PPO 擬定了各種標準，以因應軟體開發上可能遭遇到
的限制，各標準條例如下： 
 使用性：所選軟體是否易於操作開發 
 可攜性：各種作業系統之移植可攜 
 擴充性：系統後續擴充是否容易 
 支援性：對於周邊設備是否具備足夠支援度 
 維護性：系統是否易於維護與測試 
隨著處理器從單核心進展至雙核心、四核心，甚至部分應用於伺服
器的八核心之計算平臺，如何有效利用多核心的特性，並使應用程
式於多執行緒的架構下加速運行，就成了開發人員極需研究的課
題。關於嵌入式計算領域，也從單核心成長至多核心，為使原本效
能備受限制之嵌入式裝置，透過多核心同步運行之機制，盡可能提
高其運算效能，本計畫將建置一高效能平行編譯器，載入既有之程
式碼，透過逐行逐段分析，輔以高效能平行演算法，基於 OpenMP 
及 Intel Threading Building Block多核心函式庫，產出平行程式碼，
使其達到多核心同步執行，加速其運算速度，並達到最大之產出。 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位研
發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
 II 
Table of Contents 
Table of Contents .............................................................................................................. II 
版次變更記錄(Change History) ................................................................................... VII 
1. 物件以及接受準則（Objectives and Acceptance Criteria） .................................. 1 
1.1 系統說明 (System Introduction) ............................................................. 1 
1.2 PPO系統(PPO 1.0.0) ................................................................................ 1 
1.2.1 測試說明(Scope of Testing) ............................................................. 2 
1.2.2 接受準則(Acceptance Criteria) ........................................................ 2 
1.3 SPPS子系統(SPPS.1.0) ............................................................................ 2 
1.3.1 測試說明(Scope of Testing) ............................................................. 3 
1.3.2 接受準則(Acceptance Criteria) ........................................................ 3 
1.4 PS子系統(PS 1.2.0) .................................................................................. 3 
1.4.1 測試說明(Scope of Testing) ............................................................. 3 
1.4.2 接受準則(Acceptance Criteria) ........................................................ 3 
1.5 OS子系統(OS 1.3.0) ................................................................................ 4 
1.5.1 測試說明(Scope of Testing) ............................................................. 4 
1.5.2 接受準則(Acceptance Criteria) ........................................................ 4 
1.6 LS子系統(LS 1.4.0) ................................................................................. 4 
1.6.1 測試說明(Scope of Testing) ............................................................. 5 
1.6.2 接受準則(Acceptance Criteria) ........................................................ 5 
2. 環境測試(Testing Environment) .............................................................................. 5 
2.1 PPO系統(1.0.0) ........................................................................................ 5 
2.1.1 操作環境(Operational Environment) ............................................... 5 
2.1.2 硬體規格(Hardware Specification) .................................................. 6 
2.1.3 軟體規格(Software Specification) ................................................... 6 
2.2 SPPS子系統(1.2.0) ................................................................................... 6 
 IV 
3.2.2 測試工作分配(Personnel and Responsibility) ............................... 18 
3.3 PS子系統(1.2.0) ..................................................................................... 18 
3.3.1 測試排程(Testing Schedule) .......................................................... 18 
3.3.2 測試程序(Testing Procedure) ......................................................... 19 
3.3.3 測試工作分配(Personnel and Responsibility) ............................... 20 
3.4 OS子系統(1.3.0) .................................................................................... 20 
3.4.1 測試排程(Testing Schedule) .......................................................... 20 
3.4.2 測試程序(Testing Procedure) ......................................................... 21 
3.4.3 測試工作分配(Personnel and Responsibility) ............................... 22 
3.5 LS系統(1.4.0) ......................................................................................... 23 
3.5.1 測試排程(Testing Schedule) .......................................................... 23 
3.5.2 測試程序(Testing Procedure) ......................................................... 23 
3.5.3 測試工作分配(Personnel and Responsibility) ............................... 24 
4. 測試案例 ................................................................................................................ 25 
4.1 PPO系統(1.0.0) ...................................................................................... 25 
4.1.1 整合測試案例(Integration Testing Cases) ..................................... 25 
4.1.2 接受度測試案例(Acceptance Testing Cases) ................................ 26 
4.2 SPPS系統(1.1.0) ..................................................................................... 29 
4.2.1 整合測試案例(Integration Testing Cases) ..................................... 29 
4.2.2 接受度測試案例(Acceptance Testing Cases) ................................ 29 
4.3 PS子系統(1.2.0) ..................................................................................... 30 
4.3.1 整合測試案例(Integration Testing Cases) ..................................... 30 
4.3.2 接受度測試案例(Acceptance Testing Cases) ................................ 31 
4.4 OS子系統(1.3.0) .................................................................................... 31 
4.4.1 整合測試案例(Integration Testing Cases) ..................................... 31 
4.4.2 接受度測試案例(Acceptance Testing Cases) ................................ 32 
 VI 
6.2 子系統與測試案例(Subsystem and Test Cases).................................... 47 
References ...................................................................................................................... 49 
 
 
 1 
1. 物件以及接受準則（Objectives and Acceptance 
Criteria） 
1.1 系統說明 (System Introduction) 
  本子計畫所開發，結合了平行技術與程式碼優化演算法，提供程式碼優化的 
基本功能和學習模式。本計畫建置一多核心嵌入式軟體之高效能平行編譯器  
(PPO)，載入既有之程式碼，透過逐行逐段分析，輔以高效能平行演算法，基於 
OpenMP  及 Intel  Threading  Building  Block 多核心函式庫，產出平行程式
碼，使其達到多核心同步執行，加速其運算速度，並達到最大之產出。有鑒於以
往程式優化系統的全面性，我們採取了區塊優化，也就是經過學習模式就每個迴
圏區塊做一次優化並記錄下來，以迴圏執行效率做為比較，去判定運算迴圏是否
需要程式優化，因此本系統利用 Intel TBB 與 OpenMP 以及 ROSE 來達到程
式碼平行與優化的效果。 
1.2 PPO系統(PPO 1.0.0) 
  PPO 依其功能性可分為數個子系統，本文件將針對 PPO 各子系統進行個別
測試以及整合測試，各子系統之主要功能如下： 
 程式剖析子系統(Source Program Parsing Subsystem, SPPS)  
SPPS 接受使用者輸入之原始程式碼，逐行進行剖析，主要鎖定多數可平行
運行之迴圈，以平行程式撰寫時常用的關鍵字與平行化之架構為基礎，針對
輸入程式之上下文，以及各種區塊符號，判斷出各個程式區塊，最後交付後
端之平行優化子系統。  
 平行子系統(Parallelism Subsystem, PS)  
PS  接受前端程式剖析子系統之結果，針對剖析子系統所判斷出各個可平
 3 
1.3.1 測試說明(Scope of Testing) 
  本文件將針對 SPPS進行其子系統之整合測試，並且對整合系統進行接受度
測試。SPPS 之測試計畫、規範以及內容，主要參考系統需求規格書與系統設計
報告書，並且分析 SPPS測試，以供未來改進之參考。 
1.3.2 接受準則(Acceptance Criteria) 
 對所有列為急迫(critical)、重要(important)以及必須(desirable)之需求做
完整之測試。 
 依照本文件所定之程序進行測試，測試結果必須符合預期。 
 以案例為單位進行測試，若測試未通過，則必須針對該單元進行測試，
接受準則同上。 
 
1.4 PS子系統(PS 1.2.0) 
  當接受前端程式剖析子系統之結果，針對剖析子系統所判斷出各個可平行化
之標記部份，予以新增符合 TBB 或 OpenMP 之函式庫導引器，使其達到平行
優化之目標。 
1.4.1 測試說明(Scope of Testing) 
  PS主要著重於針對使用者指定之平行函式目標(TBB 或 OpenMP)，於各個
可平行化之部份，增加函式導引器，測試方式為採用不同的平行方式，並且以平
行後的程式碼為其輸出結果，本文件將針對 PS進行其子系統之整合測試，並且
對整合系統進行接受度測試。PS 之測試計畫、規範以及內容，主要參考系統需
求規格書與系統設計報告書，並且分析 PS測試，以供未來改進之參考。 
1.4.2 接受準則(Acceptance Criteria) 
 對所有列為急迫(critical)、重要(important)以及必須(desirable)之需求做
完整之測試。 
 5 
1.6.1 測試說明(Scope of Testing) 
  此一子系統主要著重於記錄部分，因此我們將針對其紀錄檔作測試，主要測
試方式為檢查其紀錄的準確性，並將比對的結果檔為其輸出結果。本文件將針對
LS進行其子系統之整合測試，並且對整合系統進行接受度測試。LS之測試計畫、
規範以及內容，主要參考系統需求規格書與系統設計報告書，並且分析 LS測試，
以供未來改進之參考。 
1.6.2 接受準則(Acceptance Criteria) 
 對所有列為急迫(critical)、重要(important)以及必須(desirable)之需求做
完整之測試。 
 依照本文件所定之程序進行測試，測試結果必須符合預期。 
 以案例為單位進行測試，若測試未通過，則必須針對該單元進行測試，
接受準則同上。 
2. 環境測試(Testing Environment) 
2.1 PPO系統(1.0.0) 
  在本年度的計畫中，我們將著重於系統整合以及建構發展平台，並以程式碼
優化之實作來驗證系統運行之可行性與效能評估。PPO系統可讓使用者，載入既
有之程式碼，透過逐行逐段分析，輔以高效能平行演算法，基於OpenMP  及 Intel  
Threading  Building  Block 多核心函式庫，產出平行程式碼，使其達到多核心
同步執行，加速其運算速度，並達到最大之產出。 
2.1.1 操作環境(Operational Environment) 
  本系統將以一般主機，裝有 Linux作業系統之環境，針對程式碼進行測試。 
 7 
判斷符號與關鍵字之前後配對架構，濾出關鍵符號及框架，最後交付給後端之平
行子系統。 
2.2.2 硬體規格(Hardware Specification) 
剖析優化系統 
 處理器：Intel(R) Pentium(R) 4 CPU 3.20GHz 
 記憶體：2 GB以上 
 儲存空間：250 GB以上 
 網路：有線：10/100 Mbps 乙太網卡 
2.2.3 軟體規格(Software Specification) 
 作業系統： 
- GNU/Linux Ubutun: kernel 2.6.26-2-686 
 程式開發： 
- GNU C compiler: gcc 4.3 
- GNU C++ compiler: g++ 4.3 
- Optimaization Tools : ROSE 
- Python 3.1 
 函示庫： 
- GNU C Library: libc6 2.7-12 
- GNU C++ Library: libstdc++ 4.3.1-2 
- OpenMP 
- Intel TBB 3.0 
2.2.4 測試資料來源(Test Data Source) 
  本文件針對不同程式碼狀況與要平行的要求去測試 SPPS，並將 SPPS 所完
成的結果，提供給各子系統使用；總計擷取資料為 50筆。 
 
 9 
- Intel TBB 3.0 
2.3.4 測試資料來源(Test Data Source) 
  為了測試經由 PS所產生出的平行程式碼，本實驗樣除應有之各種常見的程
式範例外，還包括有各種程式錯誤的測試，也就是將程式碼的除錯功能也一併進
行測試，並將 PS所產生的平行程式碼，提供給各子系統使用；總計擷取資料為
50筆。 
2.4 OS子系統(1.4.0) 
2.4.1 操作環境(Operational Environment) 
  OS 子系統之測試，主要用 PS 子系統所產生出來的程式碼，更進一步去做
最佳化。就是在除了平行化的程式碼外，再去尋找可以使之增進執行效率的程式
碼區塊，加以最佳化並產出最後的完成碼。 
2.4.2 硬體規格(Hardware Specification) 
剖析優化系統 
 處理器：Intel(R) Pentium(R) 4 CPU 3.20GHz 
 記憶體：2 GB以上 
 儲存空間：250 GB以上 
 網路：有線：10/100 Mbps 乙太網卡 
2.4.3 軟體規格(Software Specification) 
 作業系統： 
- GNU/Linux Ubutun: kernel 2.6.26-2-686 
 程式開發： 
- GNU C compiler: gcc 4.3 
- GNU C++ compiler: g++ 4.3 
- Optimaization Tools : ROSE 
 11 
- GNU C compiler: gcc 4.3 
- GNU C++ compiler: g++ 4.3 
- Optimaization Tools : ROSE 
- Python 3.1 
 函示庫： 
- GNU C Library: libc6 2.7-12 
- GNU C++ Library: libstdc++ 4.3.1-2 
- OpenMP 
 Intel TBB 3.0 
2.5.4 測試資料來源(Test Data Source) 
  由於各種利用不同程式碼並無一固定之樣式，且不同程式對於撰寫方式會有
不同認知，所以此次設計之測試樣本以大部分人們所既定認為之撰寫方式做為一
測試樣本而其他具有相同結果但不同撰寫方式則不列入本實驗之採樣範圍內，我
們將利用對 PPO測試所取得之各子系統資料，對 LS子系統進行紀錄測試，以驗
證與分析 LS子系統效率，對於 PPO之採樣樣本。 
 
 13 
IT1
SPPS PS OS
 
 
IT2
PS OS
LS
SPPS
 
 
IT3
PS OS LSSPPS
 
圖表 3-1: PPO各子系統整合架構圖 
 
接受度測試(Acceptance Testing) 
PPO經由適當修改後，必須達到需求規格報告書所列之功能，如下表所示： 
 
需求編號 優先順序 需求描述 
PPO-F-001 1 必須能夠讓使用者輸入程式碼 
 15 
User
PPO-AT1
PPO-AT2
PPO-AT3
PPO-AT4
Inp
ut
Output
Code Source
Parallelism
Optimization
PPO
PPO-AT5
 
圖表 3-2: PPO使用案例圖 
 
3.1.3 測試工作分配(Personnel and Responsibility) 
測試 負責人員 
PPO-IT1 YWS 
PPO-IT2 YWS 
PPO-IT3 YWS 
PPO-IT4 YWS 
PPO-AT1 YWS 
PPO-AT2 YWS 
PPO-AT3 YWS 
PPO-AT4 YWS 
 
 17 
IT1
Parsing Analyzing
 
 
圖表 3-3: SPPS整合測試架構圖 
 
接受度測試(Acceptance Testing) 
SPPS經由適當修改後，必須達到需求規格報告書所列之功能，如下表所示： 
 
需求編號 優先順序 需求描述 
SPPS-F-001 1 Parsing必須能夠正確的將程式解譯 
SPPS-F-002 1 Analyzing必須能夠分析 loop的狀況 
SPPS-F-003 1 SPPS必須能夠將資料傳給 PS 
SPPS使用案例如圖 3.4所示，而其接受度測試包含下列部份： 
 SPPS-AT1：使用者輸入原始程式碼 
 SPPS-AT2：程式碼解譯 
 SPPS-AT3：程式碼分析 
 
 19 
 PS系統接受度測試：100/5/29-100/6/1 
查核點(Checkpoint) 
 各子系統驗證：100/5/17 
 PS系統整合測試：100/5/23 
 PS系統接受度測試：100/6/1 
3.3.2 測試程序(Testing Procedure) 
各子系統驗證(Subsystem Validation) 
  PS之子系統驗證，主要為測試各子系統間之整合，而各子系統內部之整合，
則經由開發負責人員進行測試。 
整合測試(Integration Testing) 
PS各子系統整合架構如圖 3.5所示，主要包含下列部分： 
 PS-IT1：結合 Intel TBB與 OpenMP，測試使用不同平行函式庫功能之
整合 
IT1
Intel Tbb OpenMP
 
圖表 3-5: PS整合測試架構圖 
接受度測試(Acceptance Testing) 
PS經由適當修改後，必須達到需求規格報告書所列之功能，如下表所示： 
 
需求編號 優先順序 需求描述 
PS-F-001 1 必須能將程式碼加上 OpenMP標記 
PS-F-002 1 必須能判斷以哪種方式為平行化 
 21 
 OS系統整合驗證：100/5/23-100/5/25 
 OS系統接受度測試：100/5/29-100/6/1 
查核點(Checkpoint) 
 各子系統驗證：100/5/18 
 OS系統整合測試：100/5/25 
 OS系統接受度測試：100/6/1 
3.4.2 測試程序(Testing Procedure) 
各子系統驗證(Subsystem Validation) 
  OS之子系統驗證，主要為測試各子系統間之整合，而各子系統內部之整合，
則經由開發負責人員進行測試。 
整合測試(Integration Testing) 
OS各子系統整合架構如圖 3.7所示，主要包含下列部分： 
 OS-IT1：目的為掃描、分析與修正，測試程式碼內部可最佳化區塊功能 
 
IT1
Scanning ModifyAnalysis
 
圖表 3-7: OS整合測試架構圖 
接受度測試(Acceptance Testing) 
OS經由適當修改後，必須達到需求規格報告書所列之功能，如下表所示： 
需求編號 優先順序 需求描述 
OS-F-001 1 Scanning必須提供掃描結果給 Analysis 
OS-F-002 1 Analysis必須提供分析結果給Modify 
 23 
3.5 LS系統(1.4.0) 
3.5.1 測試排程(Testing Schedule) 
時程(schedule) 
 各子系統驗證：100/5/18-100/5/19 
 LS系統整合驗證：100/5/25-100/5/27 
 LS系統接受度測試：100/5/29-100/6/1 
查核點(Checkpoint) 
 各子系統驗證：100/5/19 
 LS系統整合測試：100/5/27 
 LS系統接受度測試：100/6/1 
3.5.2 測試程序(Testing Procedure) 
各子系統驗證(Subsystem Validation) 
  LS之子系統驗證，主要為測試各子系統間之整合，而各子系統內部之整合，
則經由開發負責人員進行測試。 
整合測試(Integration Testing) 
LS各子系統整合架構如圖 3.9所示，主要包含下列部分： 
 LS-IT1：整合 SPPS、PS與 OS，測試紀錄正確性與記錄的檔案格式 
IT1
SPPS OSPS
 
圖表 3-9: LS整合測試架構圖 
 
 25 
LS-IT1 YWS , HYW 
LS-AT1 YWS , HYW 
LS-AT2 YWS , HYW 
 
4. 測試案例 
4.1 PPO系統(1.0.0) 
4.1.1 整合測試案例(Integration Testing Cases) 
ID PPO-IT1 
目的 整合 SPPS、PS 與 OS，測試程式平行化與優化功能 
測試目標 SPPS、PS與 OS 
參考 PPO-F-001與 PPO-F-005 
重要性 1(critical) 
操作 1.SPPS、 PS與 OS初始化 
2.使用者輸入原始程式碼 
3.使用者觸發程式平行優化 
預期結果 伺服端確認取得優化原始程式碼 
清除 關閉檔案傳輸 
 
ID PPO-IT2 
目的 整合 LS、SPPS、PS 與 OS，測試記錄各子系統行為功
能 
測試目標 SPPS、PS、OS與 LS 
參考 PPO-F-002 、 PPO-F-003 、 PPO-F-004 、 PPO-F-005 、
 27 
重要性 1(critical) 
操作 1.SPPS初始化 
2.SPPS輸入出系統 
預期結果 正確取得原始程式碼資料夾路徑，產生相對應之輸出路徑 
清除 刪除錯誤產生後輸出資料夾 
 
ID PPO-AT2 
目的 程式碼平行 
測試目標 SPPS與 PS 
參考 PPO-F-005、PPO-F-008 
重要性 1(critical) 
操作 1.傳輸剖析的原始程式碼資訊 
2.取得平行程式碼 
預期結果 剖析正確，取得正確平行程式結果 
清除 關閉連線 
 
ID PPO-AT3 
目的 程式碼優化 
測試目標 PS與 OS 
參考 PPO-F-006與 PPO-F-009 
重要性 1(critical) 
操作 1.接收平行後程式碼資料 
2.優化程式碼 
3.產生最佳後的程式碼結果 
預期結果 正確取得平行程式碼資料，產生正確優化結果 
 29 
4.2 SPPS系統(1.1.0) 
4.2.1 整合測試案例(Integration Testing Cases) 
ID SPPS-IT1 
目的 測試程式碼解譯分析功能 
測試目標 Parsing與 Analyzing 
參考 SPPS-F-001、SPPS-F-002與 SPPS-F-003 
重要性 1(critical) 
操作 1.程式語法解譯分析 
3.產生剖析後的資訊 
預期結果 當剖析完成時，即產生正確之剖析資訊 
清除 關閉連線 
 
4.2.2 接受度測試案例(Acceptance Testing Cases) 
ID SPPS-AT1 
目的 使用者輸入原始程式碼 
測試目標 SPPS 
參考 無 
重要性 1(critical) 
操作 1.SPPS初始化 
2.輸入原始程式碼路徑 
預期結果 SPPS可正確讀入原始程式碼 
清除 關閉連線 
 
ID SPPS-AT2 
 31 
參考 PS-F-001到 PS-F-003 
重要性 1(critical) 
操作 1. 輸入不同要求的程式碼 
2. 取得平行與否後的結果 
預期結果 可正確判斷程式中是否還有需要平行的區塊 
清除 關閉連線 
 
 
4.3.2 接受度測試案例(Acceptance Testing Cases) 
ID PS-AT1 
目的 程式碼平行透過 OpenMP 或著 Intel TBB 
測試目標 無 
參考 PS-F-001、PS-F-002與 PS-F-003 
重要性 1(critical) 
操作 1.PS初始化 
2.根據要求對程式碼做平行化 
預期結果 正確取得平行化後的程式碼 
清除 關閉連線 
 
4.4 OS子系統(1.3.0) 
4.4.1 整合測試案例(Integration Testing Cases) 
ID OS-IT1 
目的 目的為掃描、分析與修正，測試程式碼內部可最佳化區塊
功能 
 33 
參考 OS-F-003 
重要性 1(critical) 
操作 1.根據剖析的資料做回圈的優化修正  
預期結果 程式碼優化正確 
清除 關閉連線 
 
 
 
4.5 LS子系統(1.4.0) 
4.5.1 整合測試案例(Integration Testing Cases) 
ID LS-IT1 
目的 整合 SPPS、PS 與 OS，測試紀錄正確性與記錄的檔案格
式 
測試目標 SPPS、PS與 OS 
參考 LS-F-001、LS-F-002、LS-F-003與 LS-F-004 
重要性 1(critical) 
操作 1. 接收 SPPS、PS與 OS每個行為的紀錄 
2. 將記錄轉成檔案 
預期結果 取得行為歷史檔 
清除 清除過期歷史檔 
 
4.5.2 接受度測試案例(Acceptance Testing Cases) 
ID LS-AT1 
目的 LS 可記錄各子系統行為資料 
 35 
Code2-true_dep_2.c 
 
Code-pointer.c 
 
Code-output_dep.c 
 
Code-matrixmultiply.c 
 
#include <stdio.h> 
void foo() 
{ 
  int i,x; 
 
  for (i=0;i<100;i++) 
    x=i; 
  printf("x=%d",x); 
} 
void ser(int *a, int *b, int *c) 
{ 
  int i; 
  for (i=0; i<9900; i++) 
    a[i] = a[i] + b[i] * c[i]; 
} 
void foo() 
{ 
  int n=100, m=100; 
  double a[n][m]; 
  int i,j; 
  for (i=1;i<n;i++) 
    for (j=1;j<m;j++) 
      a[i][j]=a[i][j-1]+a[i-1][j]; 
} 
 37 
 
Code-LoopOptimization.C 
int g; 
void foo() 
{ 
  int i,x; 
  int a[100]; 
  int b[100]; 
// x should be recognized as a private variable during parallelization 
// yet it introduces a set of dependencies which can be eliminated 
  for (i=0;i<100;i++) 
  { 
   int y=i+1; 
//   g = y; 
   x= a[i]+g; 
    //b[i]=x+1+y; 
  } 
} 
 39 
 
Code-LoopOptimization_fusion.C 
 
Code-LoopOptimization_mm.C 
main() { 
 
 int x[30], i; 
 
  for (i = 1; i <= 10; i += 1) { 
    x[2 * i] = x[2 * i + 1] + 2; 
  } 
  for (i = 1; i <= 10; i += 1) { 
    x[2 * i + 3] = x[2 * i] + i; 
  } 
 
} 
// Function prototype 
void dgemm(double *a, double *b, double *c, int n); 
 
// Function definition 
void dgemm(double *a, double *b, double *c, int n) 
   { 
     int i, j, k; 
 
  // int n; 
 
     for(k=0;k<n;k+=1){ 
       for(j=0;j<n;j+=1){ 
         for(i=0;i<n;i+=1){ 
           c[j*n+i]=c[j*n+i]+a[k*n+i]*b[j*n+k]; 
         } 
       } 
     } 
   } 
 41 
 
#define n 100 
 
  double a[n], b[n], c[n], d[n], e[n]; 
  double tot[n][ n]; 
  double dux[n][n][n], duy[n][n][n], duz[n][n][n]; 
 
main() 
{ 
  int i,j,k; 
      for ( j=0; j <=n-1; j+=1) 
      for ( i=0; i<=n-1; i+=1) 
         duz[i][j][0] = duz[i][j][0]*b[0]; 
 
      for (k=1; k<=n-2; k+=1) 
      for (j = 0; j <= n-1; j+=1) 
      for (i=0; i<=n-1; i+=1) 
         duz[i][j][k]=(duz[i][j][k]-a[k]*duz[i][j][k-1])*b[k]; 
 
      for ( j=0; j <=n-1; j+=1) 
      for ( i=0; i<=n-1; i+=1) 
         tot[i][j] = 0; 
 
      for ( k=0; k<=n-2; k+=1) 
      for ( j=0; j <=n-1; j+=1) 
      for ( i=0; i<=n-1; i+=1) 
            tot[i][j] = tot[i][j] + d[k]*duz[i][j][k]; 
 
      for ( j=0; j <=n-1; j+=1) 
      for ( i=0; i<=n-1; i+=1) 
         duz[i][j][n-1] = (duz[i][j][n-1] - tot[i][j])*b[n-1]; 
 
      for ( j=0; j <=n-1; j+=1) 
      for ( i=0; i<=n-1; i+=1) 
         duz[i][j][n-2]=duz[i][j][n-2] - e[n-2]*duz[i][j][n-1]; 
 
      for (k=n-3; k>=0; k+=-1) 
      for (j = 0; j <= n-1; j+=1) 
      for (i=0; i<=n-1; i+=1) 
         duz[i][j][k] = duz[i][j][k] - c[k]*duz[i][j][k+1] - e[k]*duz[i][j][n-1]; 
 
} 
 43 
5.2.2 接受度測試案例(Acceptance Testing Cases) 
編號 測試結果(PASS/FAIL) 備註 
SPPS-AT1 PASS  
SPPS-AT2 PASS  
SPPS-AT3 PASS  
 
5.3 PS系統(1.2.0) 
5.3.1 整合測試案例(Integration Testing Cases) 
編號 測試結果(PASS/FAIL) 備註 
PS-IT1 PASS  
 
5.3.2 接受度測試案例(Acceptance Testing Cases) 
編號 測試結果(PASS/FAIL) 備註 
PS-AT1 PASS  
 
5.4 OS系統(1.3.0) 
5.4.1 整合測試案例(Integration Testing Cases) 
編號 測試結果(PASS/FAIL) 備註 
OS-IT1 PASS  
 
5.4.2 接受度測試案例(Acceptance Testing Cases) 
編號 測試結果(PASS/FAIL) 備註 
OS-AT1 PASS  
 45 
PPO -F-006  V  V 
PPO -F-007  V  V 
PPO -F-008  V   
PPO -F-009     
PPO-F-010     
PPO-F-011     
PPO-F-012  V   
 
 PPO-AT1 PPO-AT2 PPO-AT3 PPO-AT4 
PPO-F-001 V    
PPO -F-002  V V  
PPO -F-003  V V  
PPO -F-004  V  V 
PPO -F-005     
PPO -F-006     
PPO -F-007    V 
PPO -F-008     
PPO -F-009     
PPO-F-010     
PPO-F-011     
PPO-F-012     
 
6.1.2 SPPS 
 SPPS-IT1 SPPS-IT2 SPPS-AT1 SPPS-AT2 SPPS-AT3 
SPPS-F-001 V V   V 
 47 
6.1.5 LS 
 LS-IT1 LS-AT1 LS-AT2 
LS-F-001 V   
LS-F-002    
LS-F-003 V  V 
LS-F-004 V   
LS-F-005 V   
LS-F-006    
 
6.2 子系統與測試案例(Subsystem and Test Cases) 
 SPPS PS OS LS SOS 
PPO-IT1 V V V   
PPO-IT2   V  V 
PPO-IT3   V V  
PPO-IT4 V V V V V 
PPO-AT1 V V    
PPO-AT2 V V V V V 
PPO-AT3   V V  
PPO-AT4   V  V 
      
SPPS-IT1 V     
SPPS-AT1 V     
SPPS-AT2 V     
SPPS-AT3 V     
      
 49 
References 
[1] Alessandro Tognetti, Nicola Carbonaro, Giuseppe Zupone and Danilo  
DeRossi, “Characterization of a Novel Data Glove Based on Textile  
Integrated Sensors”, Engineering in Medicine and Biology Society, 2006 
[2] Chunhua  Liao,  Daniel  J.  Quinlan  ,  Thomas  Panas  and  Bronis  
de  Supinski,  A  ROSE-based  
[3] OpenMP 3.0 Research Compiler Supporting Multiple Runtime Libraries, 
international Workshop on OpenMP (IWOMP) 2010, accepted in March. 2010 
LLNL-CONF-422873 
[4] Chunhua  Liao,  Daniel  J.  Quinlan,  Jeremiah  J.  Willcock  and  
Thomas  Panas,  Semantic-Aware Automatic  Parallelization  of  Modern  
Applications  Using  High-Level  Abstractions,  Journal  of Parallel 
Programming, Accepted in Jan. 2010 
[5] Robison, A.   Voss, M. Kukanov, A. Intel Corp., Santa Clara, CA, Optimization 
via Reflection on Work Stealing in TBB, Parallel and Distributed Processing, 
2008, pages: 1 – 8 
[6] Jianfeng Yang Hong Zheng Yinbo Xie Jolly Wang Bao, N. , Adding TBB 
Contents to the Multi-Core Related Curriculums, Intelligent Networks and 
Intelligent Systems, 2008. ICINIS '08, pages: 685 - 688 
[7] Quinn  Michael  J, Parallel  Programming in C with  MPI  and  OpenMP 
McGraw-Hill  Inc. 2004. ISBN 0-07-058201-7 
[8] R. Eigenmann (Editor), M. Voss (Editor), OpenMP Shared Memory Parallel   
Programming: International   Workshop   on   OpenMP   Applications   
and   Tools,   WOMPAT   2001,   West Lafayette, IN, USA, July 30-31, 
2001. (Lecture Notes in Computer Science). Springer 2001 
[9] ROSE http://www.rosecompiler.org/ROSE_HTML_Reference/index.html  
[10] Intel TBB http://www.threadingbuildingblocks.org/  
[11] OpenMP http://openmp.org/wp/  
[12] Python http://www.python.org/ 
Author's personal copy
Computer Physics Communications 182 (2011) 266–269
Contents lists available at ScienceDirect
Computer Physics Communications
www.elsevier.com/locate/cpc
Hybrid CUDA, OpenMP, and MPI parallel programming on multicore GPU
clusters✩
Chao-Tung Yang ∗, Chih-Lin Huang, Cheng-Fang Lin
Department of Computer Science, Tunghai University, Taichung City, 40704, Taiwan
a r t i c l e i n f o a b s t r a c t
Article history:
Received 1 March 2010
Received in revised form 18 June 2010
Accepted 25 June 2010
Available online 16 July 2010
Keywords:
CUDA
GPU
MPI
OpenMP
Hybrid
Parallel programming
Nowadays, NVIDIA’s CUDA is a general purpose scalable parallel programming model for writing highly
parallel applications. It provides several key abstractions – a hierarchy of thread blocks, shared memory,
and barrier synchronization. This model has proven quite successful at programming multithreaded many
core GPUs and scales transparently to hundreds of cores: scientists throughout industry and academia are
already using CUDA to achieve dramatic speedups on production and research codes. In this paper, we
propose a parallel programming approach using hybrid CUDA OpenMP, and MPI programming, which
partition loop iterations according to the number of C1060 GPU nodes in a GPU cluster which consists
of one C1060 and one S1070. Loop iterations assigned to one MPI process are processed in parallel by
CUDA run by the processor cores in the same computational node.
© 2010 Elsevier B.V. All rights reserved.
1. Introduction
Nowadays, NVIDIA’s CUDA [1] is a general purpose scalable
parallel programming model for writing highly parallel applica-
tions. It provides several key abstractions – a hierarchy of thread
blocks, shared memory, and barrier synchronization. This model
has proven quite successful at programming multithreaded many
core GPUs and scales transparently to hundreds of cores: scientists
throughout industry and academia are already using CUDA [1] to
achieve dramatic speedups on production and research codes.
This paper proposes a solution to not only simplify the use
of hardware acceleration in conventional general purpose applica-
tions, but also to keep the application code portable. In this paper,
we propose a parallel programming approach using hybrid CUDA,
OpenMP and MPI [3] programming, which partition loop iterations
according to the performance weighting of multicore [4] nodes in
a cluster. Because iterations assigned to one MPI process are pro-
cessed in parallel by OpenMP threads run by the processor cores in
the same computational node, the number of loop iterations allo-
cated to one computational node at each scheduling step depends
on the number of processor cores in that node.
In this paper, we propose a general approach that uses perfor-
mance functions to estimate performance weights for each node.
To verify the proposed approach, a cluster with hybrid CUDA was
✩ This work is supported in part by the National Science Council, Taiwan, under
grants Nos. NSC 98-2220-E-029-004- and NSC 99-2220-E-029-004-.
* Corresponding author. Tel.: +886 4 23590415; fax: +886 4 23591567.
E-mail address: ctyang@thu.edu.tw (C.-T. Yang).
built in our implementation. Empirical results show that in the hy-
brid CUDA clusters environments, the proposed approach improved
performance over all previous schemes.
The rest of this paper is organized as follows. In Section 2,
we introduce several typical and well-known parallel programming
schemes. In Section 3, we deﬁne our model and describe our ap-
proach. Our system conﬁguration is then speciﬁed in Section 4,
and experimental results for three types of application program
are presented. Concluding remarks and future work are given in
Section 5.
2. Background review
2.1. CUDA programming
CUDA (an acronym for Compute Uniﬁed Device Architecture) is
a parallel computing [2] architecture developed by NVIDIA. CUDA
is the computing engine in NVIDIA graphics processing units or
GPUs that is accessible to software developers through industry
standard programming languages. CUDA architecture supports a
range of computational interfaces including OpenGL [9] and Direct
Compute. CUDA’s parallel programming model is designed to over-
come this challenge while maintaining a low learning curve for
programmers familiar with standard programming languages such
as C. At its core are three key abstractions – a hierarchy of thread
groups, shared memories, and barrier synchronization – that are
simply exposed to the programmer as a minimal set of language
extensions.
0010-4655/$ – see front matter © 2010 Elsevier B.V. All rights reserved.
doi:10.1016/j.cpc.2010.06.035
Author's personal copy
268 C.-T. Yang et al. / Computer Physics Communications 182 (2011) 266–269
Fig. 2. Matrix multiplication with problem sizes from 256 to 2048. (For interpretation of the colors in this ﬁgure, the reader is referred to the web version of this article.)
Fig. 3. Md5 hashing on 10 to 2,098,651 words. (For interpretation of the colors in this ﬁgure, the reader is referred to the web version of this article.)
Fig. 4. Sorting numbers 640 times from 65,536 to 16,777,216 ﬂoating point numbers. (For interpretation of the colors in this ﬁgure, the reader is referred to the web version
of this article.)
CONCURRENCY AND COMPUTATION: PRACTICE AND EXPERIENCE
Concurrency Computat.: Pract. Exper. (2010)
Published online in Wiley Online Library (wileyonlinelibrary.com). DOI: 10.1002/cpe.1627
Performance-based parallel loop self-scheduling using hybrid
OpenMP and MPI programming on multicore SMP clusters
Chao-Tung Yang1,∗,†, Chao-Chin Wu2 and Jen-Hsiang Chang1,3
1Department of Computer Science, Tunghai University, Taichung City 40704, Taiwan
2Department of Computer Science and Information Engineering, National Changhua University of Education,
Changhua City 500, Taiwan
3Tungs’ Taichung MeteroHarbor Hospital, Taichung County 435, Taiwan
SUMMARY
Parallel loop self-scheduling on parallel and distributed systems has been a critical problem and it is
becoming more difficult to deal with in the emerging heterogeneous cluster computing environments.
In the past, some self-scheduling schemes have been proposed as applicable to heterogeneous cluster
computing environments. In recent years, multicore computers have been widely included in cluster
systems. However, previous researches into parallel loop self-scheduling did not consider certain aspects
of multicore computers; for example, it is more appropriate for shared-memory multiprocessors to adopt
Open Multi-Processing (OpenMP) for parallel programming. In this paper, we propose a performance-
based approach using hybrid OpenMP and MPI parallel programming, which partition loop iterations
according to the performance weighting of multicore nodes in a cluster. Because iterations assigned to
one MPI process are processed in parallel by OpenMP threads run by the processor cores in the same
computational node, the number of loop iterations allocated to one computational node at each scheduling
step depends on the number of processor cores in that node. Experimental results show that the proposed
approach performs better than previous schemes. Copyright  2010 John Wiley & Sons, Ltd.
Received 6 August 2009; Revised 25 May 2010; Accepted 25 May 2010
KEY WORDS: parallel loop; self-scheduling; multicore; SMP cluster; hybrid; OpenMP; MPI
1. INTRODUCTION
Extraordinary technological improvements over the past 10 years in areas, such as microprocessors,
memory, buses, networks, and software, have made it possible to assemble groups of inexpensive
personal computers and workstations into cost-effective systems that function in concert and possess
∗Correspondence to: Chao-Tung Yang, Department of Computer Science, Tunghai University, Taichung City 40704,
Taiwan.
†E-mail: ctyang@thu.edu.tw
Contract/grant sponsor: National Science Council, Taiwan; contract/grant numbers: NSC 98-2220-E-029-004,
NSC 99-2220-E-029-004, NSC98-2221-E-018-008-MY2
Copyright  2010 John Wiley & Sons, Ltd.
PERFORMANCE-BASED PARALLEL LOOP SELF-SCHEDULING
2. BACKGROUND REVIEW
In this section, we will review several important backgrounds. Section 2.1 introduces multicore and
cluster systems. Section 2.2 describes MPI and Open Multi-Processing (OpenMP) programming
models. Section 2.3 presents several conventional loop self-scheduling schemes. Section 2.4 reviews
several new loop self-scheduling approaches for cluster or grid systems.
2.1. Multicore and cluster systems
In a multicore processor, two or more independent cores are combined in a single integrated circuit.
The primary benefit from multicore architectures is that multiple processor cores on the same chip
can communicate with one another by directly accessing data in shared memory. By contrast,
computers in distributed systems each have individual memory systems and thus must rely on
message-passing mechanisms to communicate with one another [22, 23].
Increasing numbers of cluster systems now include multicore computers because almost all
commodity personal computers now feature multicore architectures. A cluster system, consisting of
multiple computers, can execute a large-scale program in parallel-in order to reduce its processing
time. The computers in a cluster communicate mainly through local area networks (LANs). Clus-
ters can be broadly divided into homogeneous and heterogeneous systems. The most important
difference between them is the homogeneity within each slave environment of available resources
such as computing power, memory, etc. as in [24], vs the opposite situation in heterogeneous
environments, where heterogeneity among slaves with differing capacities may result in some
computers finishing before others due to problems with synchronization or message exchanges,
and then being forced to wait for the other computers, which brings up the question of load
balancing [25, 26].
2.2. MPI and OpenMP
MPI offers portability, standardization, performance, and functionality, and includes point-to-
point message-passing and collective (global) operations, all scoped to user-specified groups of
processes [27]. MPI provides a substantial set of libraries for writing, debugging, and performance-
testing distributed programs. Our system currently uses LAM/MPI, a portable implementation of
the MPI standard.
The advantage for the user is that MPI is standardized on many levels. For example, since
the syntax is standardized, you can be sure that your MPI code will execute under any MPI
implementation running on your architecture. Since the functional behavior of MPI calls is also
standardized, your MPI calls should behave the same regardless of the implementation, thus
guaranteeing the portability of your parallel programs. Performance, however, may vary from
implementation to implementation.
The MPI library is often used for parallel programming in cluster systems because it is a message-
passing programming language. However, MPI is not the most appropriate programming language
for multicore computers because even when there are still many tasks assigned to overloaded slave
processors remaining in shared memory, other slave MPI processors on the same computing node
cannot access the tasks. Instead, all slave processors must communicate directly with the master
MPI processor to obtain new tasks. In large cluster systems, the master processor may become a
bottleneck on system performance because of excessive amounts of communication.
Copyright  2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
PERFORMANCE-BASED PARALLEL LOOP SELF-SCHEDULING
Table I. Partition sizes.
Scheme Partition size
PSS 1,1,1,1,1,1,1 . . .
CSS (125) 125,125,125,125,125,125,125,125
FSS 125,125,125,125,63,63,63,63,31, . . .
GSS 250,188,141,106,79,59,45,33,25, . . .
TSS 125,117,109,101,93,85,77,69,61, . . .
required to execute k loop iterations. Conversely, a small chunk-size is likely to result in too much
runtime overhead. When k is equal to 1, CSS deteriorates to PSS. Thus, choosing the proper
chunk-size is important and can be difficult.
GSS dispatches decreasing numbers of iterations [8]. More specifically, the next chunk-
size is calculated by dividing the number of iterations remaining by the number of available
processors. The aim is to reduce the dispatch frequency to minimize the scheduling overhead
and reduce the number of iterations assigned to the last few processors to achieve better load
balancing.
Factoring self-scheduling (FSS) assigns loop iterations to processors in phases [32]. During
each phase, half the remaining loop iterations are equally divided among available processors. FSS
prevents too much workload being assigned to the first few processors. As a result, it balances
workloads better than GSS when loop iteration computation times vary substantially.
Trapezoid self-scheduling (TSS) reduces the scheduling frequency while providing reason-
able load balancing [17]. Two parameters must be specified either by the programmer or the
compiler: the number of initial iterations assigned to the processor starting the loop, Ns , and the
number of final iterations assigned to the processor performing the last fetch, N f . Depending
on the values of Ns and N f , the number of iterations assigned in each step is decreased in
a constant ratio. Table I shows chunk-sizes for a problem with N =1000 iterations and p=4
processors.
2.4. Related work
Chronopoulos et al. proposed a distributed self-scheduling algorithm that takes CPU computation
powers as weights that scale the size of iterations each computer is assigned to compute [33, 34].
Some terms used in distributed self-scheduling algorithm are defined as follows:
• The number of nodes in the system is P .
• The virtual power of node i is Vi , where 1≤ i ≤ P , and Vk =1 represents the slowest node.
• The number of processes in the run queue of node i is Qi , which represents the workload of
node i .
• The available computing power (ACP) of node i , Ai , is Vi/Qi .
• The total ACP of the cluster, A, is the sum of all ACPs of the cluster.
They suggested that the ACPs can be used as weights to adjust the size of iterations assigned to
each computer. Their method can be used to improve the performance of various self-scheduling
algorithms. Take GSS as an example. Assume that Rk is the number of unassigned iterations, and
P is the number of nodes in the cluster. Instead of taking Rk/P iterations for execution, distributed
GSS adjusts the chunk-size of iterations to Rk ×(Ai/A).
Copyright  2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
PERFORMANCE-BASED PARALLEL LOOP SELF-SCHEDULING
cost. As some node completes its work, the second part (1−%) of work is assigned by traditional
self-scheduling algorithms. In this way, the workload can be well balanced.
In this subsection, we demonstrate partitioning % of a workload using performance weighting
as determined by a combination of CPU clock speed and HPCC [31] measurement of all nodes,
with the remaining workload dispatched using some well-known self-scheduling schemes such
as GSS, FSS, or TSS [8]. To use this approach, we need to know the real computer perfor-
mance according to the HPCC benchmark. We can then distribute appropriate workloads to each
node and achieve load balancing. The more accurate the estimation, the better the load balance
will be.
The HPCC is a useful computer benchmarking tool [31]. Its first goal is to examine the perfor-
mance of HPC architectures using kernels with more challenging memory access patterns than
High Performance Linpack (HPL). It also provides benchmarks that bound the performance of
many real applications as a function of memory access characteristics—e.g. spatial and temporal
locality—and augments the TOP500 list. Unlike conventional benchmarks, the HPCC benchmark
consists of seven basic tests: HPL, DGEMM, STREAM, PTRANS, Random Access, FFT, and
Communication bandwidth and latency. In our work, we use the HPL measurement as the perfor-
mance value, and include it in our algorithm.
To estimate node performance, we define node j ’s performance weighting (PW ) as
PW j (V −1,V2, . . . ,VM ), (2)
where Vi , 1<i<M , is a performance weighting variable. Our PW for node j is defined as
PW j = CS j∑
∀nodei∈S C Si
+(1−) HPL j∑
∀nodei∈S HPLi
, 0≤≤1, (3)
where nodei represents Node i , S is the set of all cluster nodes, CSi is the CPU clock speed of
nodei , HPLi is the HPCC HPL measurement, and  is a value between zero and one. The PW
consists of two parts: the factor of CPU clock speed and the factor of the HPL measurement. The
weight of each factor in the PW is determined by the parameter .
Assume 24 loop iterations are to be scheduled, the value of parameter  is 50%, and the PW
values of three nodes are 3, 2, and 1. Thus, the scheduler will assign six iterations to the first node,
four iterations to the second, and two to the third.
3.2. Algorithm
We propose an algorithm for performance-based loop scheduling on heterogeneous cluster environ-
ments based on the information about workload distribution and node performance. The algorithm
comprises two modules: a master module and a slave module, and employs message-passing.
The master module makes scheduling decisions and dispatches workloads to slaves, which then
process the assigned work. The algorithm presented is just a skeleton, a detailed implementation
including data preparation, parameter passing, etc. might differ depending on specific application
requirements.
The algorithm proceeds in several steps: relevant information gathering, performance weight
calculation, static scheduling of % of the total workload according to performance weighting
Copyright  2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
PERFORMANCE-BASED PARALLEL LOOP SELF-SCHEDULING
Figure 2. Local scheduler algorithm with OpenMP.
Information collection 
(CPU clock speed and 
HPL measurement)
Start End
MPI/OpenMP
Loop scheudling
PWj analyze
Nodes in a 
heterogeneous 
cluster
Set alpha/beta 
values
Figure 3. Self-scheduling flowchart.
4. EXPERIMENTAL ENVIRONMENT AND RESULTS
To verify our approach, illustrate our cluster environment, and describe the terminology for our
application, we implemented programs with MPI/OpenMP for execution on our testbed. We
then compared the performance of our scheme with those of other static and dynamic schemes
using heterogeneous and homogeneous clusters to solve problems in Matrix Multiplication, Sparse
Copyright  2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
PERFORMANCE-BASED PARALLEL LOOP SELF-SCHEDULING
Table III. Our heterogeneous cluster system configuration.
Host
name Processor model
Number
of CPUs
Number
of cores Memory NIC OS version
LAM/MPI
Version
HPCC
(K =1000)
quad1 IntelTM CoreTM2
Quad CPU Q6600 @
2.40 GHz
1 4 2 GB 1 G 2.6.23.1-
42.fc8
lam-7.1.2-
10.fc7
24.61 Gflops
(N =15K )
quad2
quad3
quad4
oct1 Intel(R) Xeon(R) CPU
E5410 @ 2.33 GHz
2 8 8 G 1 G 2.6.21-
xen_Xen
lam-7.1.2 34.45 Gflops
(N =15K )
oct2
oct3
oct4
t1 Intel(R) Xeon(R) CPU
E5310 @ 1.60 GHz
2 8 8 GB 1G 2.6.25.4-
10.fc8
lam-7.1.2-
10.fc7
25.12 Gflops
(N =15K )
t2
t3 Intel(R) Xeon(R) CPU
E5410 @ 2.33 GHz
2 8 8 GB 1G 34.45 Gflops
(N =15K )
t4
eta9 Intel(R) Xeon(R) CPU
E5420 @ 2.50 GHz
2 8 4G 1G 2.6.25-
14.fc9
lam-7.1.2-
11.fc9
39.15 Gflops
(N =15K )
s1 Intel(R) Xeon(R) CPU
E5310 @ 1.60 GHz
2 8 4 GB 1G 2.6.18-
128.1.6.el5
lam-7.1.2-
10.fc7
24.39 Gflops
(N =15K )
Table IV. Bandwidth (Mbps) between host and host in our heterogeneous cluster system.
quad1
576.3 quad2
571.0 576.0 quad3
571.7 572.4 571.7 quad4
72.0 73.6 72.7 73.6 oct1
73.4 76.5 73.6 67.1 828.4 oct2
77.1 76.5 74.2 76.1 824.0 829.6 oct3
71.6 75.6 66.5 74.7 844.9 833.1 844.0 oct4
82.7 80.7 82.3 81.8 822.2 823.0 824.1 782.5 eta9
72.0 79.9 72.5 79.1 64.4 57.8 62.4 54.4 66.7 s1
3.3 3.3 3.2 3.3 3.3 3.3 3.2 3.3 3.4 3.4 t1
3.5 3.4 3.6 3.5 3.5 3.6 3.4 3.5 3.6 3.5 875.3 t2
3.2 3.3 3.2 3.2 3.2 3.4 3.2 3.4 3.3 3.2 857.3 846.7 t3
3.1 3.3 3.2 3.3 3.1 3.4 3.2 3.4 3.5 3.3 865.2 865.7 871.1 t4
4.3. Experimental results
In our experiments, we collected HPL measurements and CPU speeds for all nodes, and investigated
the impact of parameters , , on performance. Parameters  and  are set by the programmer
and choosing appropriate values adaptable to dynamic environments is difficult. The partitioning
principle was based on PW ratios. If parameter  is too large, the dominant computer will not
finish its work in time; if it is too small, the dynamic scheduling overhead will become significant.
Copyright  2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
PERFORMANCE-BASED PARALLEL LOOP SELF-SCHEDULING
Figure 5. Algorithm for Matrix Multiplication.
by n columns. The algorithm, shown in Figure 5, uses MPI with OpenMP to compute the final
solution.
Figure 6 shows how parameters influenced the performance of our heterogeneous cluster. In the
experiment, with heterogeneous clusters we found that the proposed schemes performed better
when  was 50 and  had various optimum values based on scheduling. Figure 7 shows how
parameters influenced the performance of our homogeneous cluster. In the experiment, we found
that the proposed schemes performed better when  was 90 and  was about 0.9. After selecting 
and  values we carried out a set of experiments with them, and compared the results with those
for previous scheduling algorithms. Each experiment was run 10 times and the average fetched in
order to achieve better accuracy.
We investigated execution times on our heterogeneous and homogeneous clusters for the GSS,
FSS, and TSS groups. We found that the execution times for static partitioning (matstat) were orders
Copyright  2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
PERFORMANCE-BASED PARALLEL LOOP SELF-SCHEDULING
Table VI. Proposed scheme Matrix Multiplication execution times on our heterogeneous
cluster compared with the previous schemes.
Size
Approach 1024×1024 2048×2048 4096×4096
matgss 7.06 25.99 130.77
matFgss 3.26 20.90 109.66
matAgss 2.63 21.27 103.78
matPgss 3.91 20.02 113.62
matPgss_omp 3.76 16.82 106.48
matfss 3.78 25.25 108.62
matFfss 3.95 22.65 100.67
matAfss 3.12 17.75 100.37
matPfss 3.99 19.55 99.85
matPfss_omp 4.77 16.79 92.60
mattss 3.26 26.06 128.27
matFtss 3.76 25.28 108.50
matAtss 3.04 24.48 104.12
matPtss 5.37 19.22 119.42
matPtss_omp 4.60 17.60 104.99
Table VII. Proposed scheme Matrix Multiplication execution times on our homogeneous
cluster compared with the previous schemes.
Size
Approach 1024×1024 2048×2048 4096×4096
matstat 10.81 44.39 170.58
matgss 0.29 16.74 141.73
matFgss 0.50 16.38 143.80
matAgss 0.44 16.33 143.24
matPgss 0.44 16.46 142.68
matPgss_omp 0.20 15.62 137.50
matfss 0.38 16.41 144.31
matFfss 0.45 15.67 143.53
matAfss 0.35 16.84 143.76
matPfss 0.44 16.63 143.42
matPfss_omp 0.24 15.71 137.68
mattss 0.31 19.78 145.64
matFtss 0.46 16.97 147.70
matAtss 0.46 17.70 157.91
matPtss 0.54 16.71 143.84
matPtss_omp 0.20 15.59 135.85
The algorithm, shown in Figure 8, uses MPI with OpenMP to compute its final solution.
Figure 9 shows how parameters influenced the performance of our heterogeneous cluster. In the
experiment, we found that the proposed schemes performed better when  was 0 and  had various
optimum values based on scheduling. Figure 10 shows how parameters influenced the performance
of our homogeneous cluster. In the experiment, we found that the proposed schemes performed
better when  was 100 and  was about 0.1. After selecting  and  values, we carried out a set of
Copyright  2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
PERFORMANCE-BASED PARALLEL LOOP SELF-SCHEDULING
Figure 9. 4096×4096 Sparse Matrix Multiplication execution times on our heterogeneous
cluster, performance influenced by parameters  and .
Figure 10. 4096×4096 Sparse Matrix Multiplication execution times on our homogeneous
cluster, performance influenced by parameters  and .
Tables VIII and IX show execution times for the conventional (mat*ss), dynamic hybrid
(matF*ss,matA*ss), and the proposed (matP*ss) and OpenMP (matP*ss_omp) schemes on the
FSS, GSS, and TSS group approaches with 1024×1024, 2048×2048, and 4096×4096 matrices
input to heterogeneous and homogeneous clusters. The results show that the proposed scheduling
scheme performed better than the static and previous schemes.
4.3.3. Application 3: Mandelbrot Set computation. A Mandelbrot Set problem involves performing
the same computation on different data points with different convergence rates. Named after
Benoit Mandelbrot, Mandelbrot Sets are fractals, belonging to a class of objects that display self-
similarity at various scales. Magnifying a fractal reveals small-scale details similar to its large-scale
characteristics. Although the Mandelbrot Set is self-similar at magnified scales, its small-scale
details are not identical to the whole. In fact, the Mandelbrot Set is infinitely complex. Yet the
process of generating it is based on an extremely simple equation involving complex numbers.
This operation derives a resultant image by processing an input matrix, A, where A is an image
of m pixels by n pixels. The resultant image is one of m pixels by n pixels.
Our proposed scheme was implemented for Mandelbrot Set Computation. The master module
is responsible for workload distribution. When a slave node falls idle, the master node sends two
Copyright  2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
PERFORMANCE-BASED PARALLEL LOOP SELF-SCHEDULING
Figure 11. Algorithm for Mandelbrot Set Computation.
Therefore, the workload distribution performance depends on the degree of variation between
iterations. The algorithm, shown in Figure 11, uses MPI with OpenMP to compute its final solution.
Figure 12 shows how parameters influenced the performance of our heterogeneous cluster. In the
experiment, we found that the proposed schemes performed better when  was 30 and  had various
optimum values based on scheduling. Figure 13 shows how parameters influenced the performance
of our homogeneous cluster. In the experiment, we found that the proposed schemes performed
better when  was 50 and  was about 0.1. After selecting  and  values we carried out a set of
experiments with them, and compared the results with those for previous scheduling algorithms.
Each experiment was run 10 times and the average fetched in order to achieve better accuracy.
We investigated GSS, FSS, and TSS group execution times on our heterogeneous and
homogeneous clusters. In this experiment, static partitioning (manstat) execution times for the
heterogeneous cluster were: image size 512×512, 487.05 s, image size 1024×1024, 2001.85 s,
and image size 2048×2048, 4326.4 s, and for the homogeneous cluster: image size 512×512,
117.77 s, image size 1024×1024, 470.18 s, and image size 2048×2048, 1880.30 s.
Copyright  2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
PERFORMANCE-BASED PARALLEL LOOP SELF-SCHEDULING
Table X. Proposed scheme Mandelbrot Set execution times on our heterogeneous
cluster compared with the previous schemes.
Size
Approach 512×512 1024×1024 2048×2048
manstat 487.05 2001.85 4326.4
mangss 5.28 18.40 71.30
manFgss 5.28 18.43 72.24
manAgss 4.72 19.58 77.23
manPgss 5.30 17.94 71.39
manPgss_omp 4.55 15.32 55.80
manfss 3.94 15.81 62.93
manFfss 3.24 12.66 49.72
manAfss 4.53 18.66 71.19
manPfss 3.12 12.67 46.66
manPfss_omp 3.05 8.72 23.23
mantss 3.32 14.46 59.02
manFtss 3.15 11.00 46.26
manAtss 4.67 17.30 72.54
manPtss 2.64 11.39 47.16
manPtss_omp 2.65 8.49 24.22
Table XI. Proposed scheme Mandelbrot Set execution times on our homogeneous
cluster compared with the previous schemes.
Size
Approach 512×512 1024×1024 2048×2048
manstat 117.77 470.18 1880.30
mangss 2.55 9.57 37.34
manFgss 2.36 9.08 35.63
manAgss 4.26 16.17 62.95
manPgss 2.37 9.09 35.64
manPgss_omp 2.07 7.11 26.80
manfss 2.33 8.79 35.06
manFfss 2.36 8.61 34.74
manAfss 4.26 16.17 62.95
manPfss 2.34 8.62 34.73
manPfss_omp 1.71 5.50 18.72
mantss 2.47 10.36 41.45
manFtss 2.18 8.69 37.10
manAtss 4.26 16.17 62.95
manPtss 2.22 8.68 37.07
manPtss_omp 1.67 5.50 19.47
is NP-complete, and no known algorithms can solve it in polynomial time. In our experiment,
we found the solutions via an exhaustive search. Our operation got the number of Boolean variables
in the expression as an input, then exhaustively computed all combinations of these values. The
Circuit Satisfiability Problem is implemented in a similar way. The master module is responsible
for workload distribution, and when a slave node falls idle, sends two integers to it representing
Copyright  2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
PERFORMANCE-BASED PARALLEL LOOP SELF-SCHEDULING
0
40
80
120
160
200
0 10 20 30 40 50 60 70 80 90 100
α Value
Ex
ec
ut
io
n 
Ti
m
e 
(se
c)
satFgss satFfss satFtss
0
20
40
60
80
100
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
β Value
Ex
ec
ut
io
n 
Ti
m
e 
(S
ec
)
satPgss_omp satPfss_omp satPtss_omp
Figure 15. Parameter  and  influence on Circuit Satisfiability execution times when using
20 variable numbers with our heterogeneous cluster.
0
50
100
150
200
0 10 20 30 40 50 60 70 80 90 100
α Value
Ex
ec
ut
io
n 
Ti
m
e 
(se
c)
satFgss satFfss satFtss
0
40
80
120
160
200
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
β Value
Ex
ec
ut
io
n 
Ti
m
e 
(S
ec
)
satPgss_omp satPfss_omp satPtss_omp
Figure 16. Parameter  and  influence on Circuit Satisfiability execution times when using
20 variable numbers with our homogeneous cluster.
and for the homogeneous cluster: 18 variable numbers, 2.83 s, 19 variable numbers, 11.48 s, and
20 variable numbers, 159.83 s.
Tables XII and XIII show heterogeneous and homogeneous cluster execution times for the
static (satstat), conventional (sat*ss), and dynamic hybrid (satF*ss, satA*ss) schemes, and for
the proposed (satP*ss) and OpenMP (satP*ss_omp) schemes on the FSS, GSS, and TSS group
approaches with 18, 19, and 20 variable numbers. Our heterogeneous cluster experimental results
show that the proposed scheduling scheme performed better than the static and previous ones.
4.4. Discussion
To understand how scheduling overhead affects the performance, we use the Mandelbrot Set appli-
cation to evaluate the scheduling overheads incurred by OpenMP and MPI, as shown in Figure 17.
Two different multicore computers are used for evaluation. Quad1 is a four-core computer, whereas
eta9 is an eight-core computer. In addition, we use the pure OpenMP model and the pure MPI
model to implement two parallel versions of the application. The scheduling overhead is derived
Copyright  2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
PERFORMANCE-BASED PARALLEL LOOP SELF-SCHEDULING
Figure 17. Scheduling overheads for OpenMP and MPI on computational nodes eta9 and Quad1.
where Tp represents the parallel execution time, Ts represents the sequential execution time, and
Nc represents the total number of processor cores. Furthermore, Tp is obtained by executing the
Mandelbrot Set application with either Nc MPI processes or Nc OpenMP threads. Tc is obtained by
executing the application without MPI or OpenMP codes. In the legend of Figure 17, OpenMP(8)
and MPI(8) represent that Nc is equal to 8 and the parallel version is implemented by OpenMP
threads and MPI processes, respectively. In other words, the data for OpenMP(8) and MPI(8) are
from the computational node eta9. Similarly, the data for OpenMP(4) and MPI(4) are from the
computational node Quad1. The scheduling overhead in the pure MPI model is higher than that
in the pure OpenMP model. Moreover, the scheduling overhead becomes larger when the image
size is increased. Note that the data of OpenMP(4) and OpenMP(4) are obtained from different
computational nodes. It is inappropriate to compare the data.
For an application needing to send a large amount of data at each scheduling step, such as Matrix
Multiplication and Sparse Matrix Multiplication, the long data communication time will diminish
the factor of performance improvement because our proposed algorithm can reduce computation
time rather than communication time. On the other hand, although the Circuit Satisfiability requires
little communications at each scheduling step, the performance improvement is not so much because
its workload is distributed regularly. For an application with regular workload distribution, creating
an MPI process for each processor core on a computational node only imposes more overheads
on loop scheduling because the workload distribution is regular and the multiple cores in a node
is homogeneous. Therefore, to reduce the total scheduling overhead, our proposed approach uses
one MPI process for each multicore node to communicate with the master process and adopts the
OpenMP scheduler as the local scheduler. Since the Mandelbrot Set Computation has irregular
workload distribution and requires little data communications at each scheduling step, our approach
can provide the best performance improvement for the application.
5. CONCLUSIONS AND FUTURE WORK
In this paper, we report using the hybrid programming model MPI with OpenMP to design parallel
loop self-scheduling schemes for heterogeneous and homogeneous cluster systems that run on
multicore computers.
Copyright  2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
PERFORMANCE-BASED PARALLEL LOOP SELF-SCHEDULING
12. Shih W-C, Yang C-T, Tseng S-S. A performance-based parallel loop scheduling on grid environments. The Journal
of Supercomputing 2007; 41(3):247–267.
13. Beaumont O, Casanova H, Legrand A, Robert Y, Yang Y. Scheduling divisible loads on star and tree networks:
Results and open problems. IEEE Transactions on Parallel and Distributed Systems 2005; 16:207–218.
14. Bohn CA, Lamont GB. Load balancing for heterogeneous clusters of PCs. Future Generation Computer Systems
2002; 18:389–400.
15. Tang P, Yew PC. Processor self-scheduling for multiple-nested parallel loops. Proceedings of the 1986 International
Conference on Parallel Processing, Pennsylvania State University, University Park, PA, U.S.A., 1986; 528–535,
16. Post E, Goosen HA. Evaluation the parallel performance of a heterogeneous system. Proceedings of Fifth
International Conference and Exhibition on High-performance Computing in the Asia–Pacific Region (HPC Asia),
Gold Coast, Queensland, Australia, 2001.
17. Tzen TH, Ni LM. Trapezoid self-scheduling: A practical scheduling scheme for parallel compilers. IEEE
Transactions on Parallel and Distributed Systems 1993; 4:87–98.
18. Yang C-T, Shun-Chyi Chang. A parallel loop self-scheduling on extremely heterogeneous PC clusters. Journal
of Information Science and Engineering 2004; 20(2):263–273.
19. Yang C-T, Cheng K-W, Li K-C. An enhanced parallel loop self-scheduling scheme for cluster environments. The
Journal of Supercomputing 2005; 34(3):315–335.
20. Yang C-T, Shih W-C, Tseng S-S. Dynamic partitioning of loop iterations on heterogeneous PC clusters. The Journal
of Supercomputing 2007; 44(1):1–23.
21. Mandelbrot BB. Fractals and Chaos: The Mandelbrot Set and Beyond. Springer: Berlin, 2004.
22. Magoules F, Pan J, Tan K-A, Kumar A. Introduction to Grid Computing. CRC Press: Boca Raton, 2009.
23. Gropp W, Lusk E, Sterling T. Beowulf Cluster Computing with Linux (2nd edn). MIT Press: Cambridge, MA,
2003.
24. Wilkinson B, Allen M. Parallel Programming: Techniques and Applications Using Networked Workstations and
Parallel Computers (2nd edn). Prentice-Hall: Englewood Cliffs, NJ, 2005.
25. Bohn CA, Lamont GB. Load balancing for heterogeneous clusters of PCs. Future Generation Computer Systems
2002; 18:389–400.
26. Yagoubi B, Slimani Y. Load balancing strategy in grid environment. Journal of Information Technology and
Applications 2007; 1(4):285–296.
27. Karniadakis GE, Kirby RM II. Parallel Scientific Computing in C ++ and MPI: A Seamless Approach to Parallel
Algorithms and their Implementation. Cambridge University Press: Cambridge, 2003.
28. Chapman B, Jost G, van der Pas R. Using OpenMP: Portable Shared Memory Parallel Programming. MIT Press:
Cambridge, MA, 2007.
29. Yang C-T, Cheng K-W, Shih W-C. On development of an efficient parallel loop self-scheduling for grid computing
environments. Parallel Computing 2007; 33(7–8):467–487.
30. Bennett BH, Davis E, Kunau T, Wren W. Beowulf parallel processing for dynamic load-balancing. Proceedings
on IEEE Aerospace Conference, Big Sky, Montana, U.S.A., vol. 4. 2000; 389–395.
31. Aggarwal V, Sabharwal Y, Garg R, Heidelberger P. HPCC random access benchmark for next generation
supercomputers. IEEE International Symposium on Parallel and Distributed Processing, Rome, Italy, 2009;
389–400.
32. Hummel SF, Schonberg E, Flynn LE. Factoring: A method scheme for scheduling parallel loops. Communications
of the ACM 1992; 35:90–101.
33. Chronopoulos AT, Penmatsa S, Yu N. Scalable loop self-scheduling schemes for heterogeneous clusters.
Proceedings of the 2002 IEEE International Conference on Cluster Computing, Chicago, IL, U.S.A., 2002;
353–359.
34. Chronopoulos AT, Penmatsa S, Xu J, Ali S. Distributed loop-scheduling scheme for heterogeneous computer
systems. Concurrency and Computation: Practice and Experience 2006; 18:771–785.
35. Wu C-C, Lai L-F, Yang C-T, Chiu P-H. Using hybrid MPI and OpenMP programming to optimize communications
in parallel loop self-scheduling schemes for multicore PC clusters. Journal of Supercomputing 2009; DOI:
10.1007/s11227-009-0271-z.
36. Yang C-T, Shih W-C, Cheng L-H. A performance-based dynamic loop scheduling on heterogeneous clusters.
Journal of Supercomputing 2010. ISSN: 1573-0484.
Copyright  2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
C.-C. Wu et al.
1 Introduction
As computers become more and more inexpensive and powerful, computational grids
which consist of various computational and storage resources have become promising
alternatives to traditional multiprocessors and computing clusters [6, 7]. Basically,
grids are distributed systems which share resources through the Internet. Users can
access more computing resources through grid technologies. However, bad manage-
ment of grid environments might result in using grid resources in an inefficient way.
Moreover, the heterogeneity and dynamic changing of the grid environment makes it
different from conventional parallel and distributed computing systems, such as mul-
tiprocessors and computing clusters. Therefore, it becomes more difficult to utilize
the grid efficiently.
Loop scheduling on parallel and distributed systems is an important problem,
and has been thoroughly investigated on traditional parallel computers in the past
[11, 12, 17, 22]. Traditional loop scheduling approaches include static scheduling and
dynamic scheduling. The former is not suitable in dynamic environments. The latter,
especially self-scheduling, has to be adapted to be applied to heterogeneous plat-
forms. Therefore, it is difficult to schedule parallel loops on heterogeneous and dy-
namic grid environments. In recent years, several pieces of work have been devoted to
parallel loop scheduling for cluster computing environments [1, 3, 4, 23–26, 28, 29],
addressing the heterogeneity of computing power.
To adapt to grid systems, we have an approach to enhance some well-known loop
self-scheduling schemes [27]. The HINT Performance Analyzer [10] is used to deter-
mine whether target systems are relatively homogeneous or relatively heterogeneous.
We then partition loop iterations into four classes to achieve good performance in any
given computing environment. Finally, a heuristic approach based upon α-based self-
scheduling scheme is used to solve parallel regular loop scheduling problem on an
extremely heterogeneous Grid computing environment. Furthermore, we have pro-
posed another improved loop self-scheduling approach called PLS (Performance-
based Loop Scheduling) for grid systems [18]. In this work, dynamic information
acquired from a monitoring tool is utilized to adapt to the dynamic environment. Fur-
thermore, a sampling method is proposed to estimate the proportion of the workload
to be assigned statically.
Although our previous approaches improved system performance, they did not in-
vestigate multi-core architecture [18, 27] or they did not consider the feature of grid
systems [23, 24]. Recently, more grid systems are including multi-core computers
because nearly all commodity personal computers have the multi-core architecture.
The primary feature of multi-core architecture is multiple processors on the same
chip that communicate with each other by directly accessing the data in shared mem-
ory. Unlike multi-core computers, each computer in a distributed system has its own
memory system and thus relies on a message-passing mechanism to communicate
with other computers. The MPI library is usually used for parallel programming in
the grid system because it is a message-passing programming model [14]. However,
MPI is not the best programming model for multi-core computers. OpenMP is suit-
able for multi-core computers because it is a shared-memory programming model.
Therefore, we propose using hybrid MPI and OpenMP programming mode to design
C.-C. Wu et al.
Table 2 Partition sizes
Scheme Partition size
PSS 1,1,1,1,1,1,1, . . .
CSS(125) 125,125,125,125,125,125,125,125
FSS 125,125,125,125,63,63,63,63,31, . . .
GSS 250,188,141,106,79,59,45,33,25, . . .
TSS(125, 1, 8) 125,117,109,101,93,85,77,69,61, . . .
2.2 Grid computing and programming models
Grid computing [5, 6, 8] can be thought of as distributed and large-scale cluster com-
puting and as a form of networked parallel processing. It can be confined to the net-
work of computer workstations within a corporation or it can be a public collabora-
tion. In this paper, Ganglia is utilized to acquire dynamic system information, such
as CPU loading of available nodes.
MPI is a message-passing library standard that was published in May 1994.
MPICH-G2 [15] is a grid-enabled implementation of the MPI v1.1 standard. In
contrast, Open Multi-Processing (OpenMP), a kind of shared-memory architecture
API [16], provides a multi-threaded capacity. A loop can be parallelized easily by
invoking subroutine calls from OpenMP thread libraries and inserting the OpenMP
compiler directives. In this way, the threads can obtain new tasks, the un-processed
loop iterations, directly from local shared memory.
2.3 Related work
The EasyGrid middleware is a hierarchically distributed Application Management
System (AMS) which is embedded automatically into a user’s parallel MPI applica-
tion without modifications to the original code by a scheduling portal [2]. Each Easy-
Grid AMS is a three-level hierarchical management system for application-specific
self-scheduling and distinct scheduling policies can be used at each level, even within
the same level. A low intrusion implementation of a hybrid scheduling strategy has
been proposed to cope with the dynamic behavior of grid environments.
TITAN is a multi-tiered scheduling architecture for grid workload management,
which employs a performance prediction system (PACE) and task distribution bro-
kers to meet user-defined deadlines and improve resource usage efficiency [19]. The
PACE system is developed to facilitate model generation for applications. The sched-
uler uses the evaluation engine to identify expected execution run-time from the ap-
plication models and from the resource models that represent the cluster or multi-
processor system of homogeneous processing nodes.
Herrera et al. proposed a new loop distribution scheme to overcome the follow-
ing three limitations when the MPI programming model is adopted in computational
Grids [9]. (1) All required resources must be simultaneously allocated to begin execu-
tion of the application. (2) The whole application must be restarted when a resource
fails. (3) Newly added resources cannot be allocated to a currently running appli-
cation. Their approach is implemented using the Distributed Resource Management
Application API (DRMAA) standard and the GridWay meta-scheduling framework.
C.-C. Wu et al.
Each computational node runs an MPI process regardless how many processor
cores it has. However, OpenMP is used for intra-node communications. Each MPI
process will fork OpenMP threads depending on the number of processor cores in
its underlying computational node. Every processor core runs one OpenMP thread.
OpenMP is a shared-memory multi-threaded programming model, which matches
the multi-core computational node feature.
The scheduling scheme we proposed consists of one global scheduler and multi-
ple local schedulers. Each worker computational node has one local scheduler. One
processor core from each worker computational node is responsible for local sched-
uler execution. The processor core running the local scheduler is called the master
core and the others are called worker cores. The global scheduler and the local sched-
ulers are all MPI processes.
In the first-level scheduling, the global scheduler is responsible for deciding how
many iterations will be assigned whenever a local scheduler issues a request. The
number of processor cores in the computational node, from which the request comes,
should be taken into consideration when the decision is made. In the second-level
scheduling, because all of the processor cores are homogeneous, the local scheduler
dispatches the iterations assigned by the global scheduler to all processor cores pri-
marily based on whether the iteration workload is regular or not. The iteration work-
load of a loop is regular if the difference between the execution times for any two of
the iterations is very small. Otherwise, the iteration workload is irregular. Basically,
static scheduling is preferred in the second level. However, dynamic scheduling is
adopted if the iteration workload distribution is irregular. Nevertheless, the default
OpenMP built-in self-scheduling scheme, i.e., static scheduling, is adopted for the
second level in this work.
3.2 Proposed approach
In the first-level scheduling, we propose a two-phase scheduling approach as fol-
lows. In the first phase, the SWR (Static-Workload Ratio) and Performance Ratio are
calculated and then SWR percentage of the total workload is statically scheduled ac-
cording to the performance ratio among all worker nodes [18]. In the second phase,
the remainder of the workload is dynamically scheduled using any well-known self-
scheduling scheme such as CSS, GSS, FSS or TSS. When an MPI process requests
new iterations at each scheduling step, we must take the number of processor cores
into consideration when the master core determines the number of iterations to be
allocated for the worker core because the assigned iterations will be processed by
parallel OpenMP threads. If there are pi processor cores in the computational node i,
the master will use the applied self-scheduling scheme, such as GSS, to calculate the
total number of iterations by adding up the next pi allocations. For instance, if there
are 4 processor cores in a computational node and the CSS scheme with a chunk size
of 128 iterations adopted, the master will assign 512 iterations whenever the MPI
process running on the computational node asks for new iterations.
In the second-level scheduling, the local scheduler dispatches the iterations as-
signed by the global scheduler to the parallel OpenMP threads by invoking the
OpenMP built-in scheduling routine. The scheduling scheme can be any one of
C.-C. Wu et al.
We also propose to use a parameter, SWR, to alleviate the effect of irregular work-
load. In order to take advantage of static scheduling, SWR percentage of the total
workload is dispatched according to Performance Ratio. We propose to randomly
take five sampling iterations, and compute their execution time. Then, the SWR of
the target application i is determined by the following formula.
SWR = mini
MAXi
(4)
where mini is the minimum execution time of all sampled iterations for application i;
MAXi is the maximum execution time of all sampled iterations for application i. If
the workload of the target application is regular, SWR can be set to be 100. However,
if the application has an irregular workload, it is efficient to reserve some amount of
workload for load balancing.
For example, for a regular application with uniform workload distribution, the
five sampled iterations are the same. Therefore, the SWR is 100%, and the whole
workload can be dispatched according to Performance Ratio, with good load balance.
However, for another application, the five sampling execution times might be 7, 7.5,
8, 8.5 and 10 seconds, respectively. Then the SWR is 7/10, i.e. a percentage of 70.
Therefore, 70 percentages of the iterations would be scheduled statically according
to PR, while 30 percentages of the iterations would be scheduled dynamically by any
one of the well-known self-scheduling scheme such as GSS.
4 Performance evaluations
To verify our approach, we constructed a Grid system consisting of four sites, 19
computational nodes and 49 processor cores. The configurations of the constituent
computational nodes are shown in Tables 3 and 4 in Appendix. The four sites are
built at four educational institutions, including National Changhua University of Ed-
ucation (NCUE), Tatung University (TTU), National Taichung University (NTCU)
and A-Lien Elementary School (ALES). The locations, machine types, node counts
and core counts per node of the four Grid sites are shown in Table 3 in Appendix.
These four Grid sites are distributed over Taiwan as shown in Fig. 8 in Appendix.
Three types of application programs are implemented to verify our approach in
this testbed: Matrix Multiplication, sparse matrix multiplication and Mandelbrot set
computation. The Performance-based Loop Scheduling (PLS) proposed by Yang
et al. [18] is compared with our approach, Hierarchical Loop Scheduling (HLS).
The PLS approach adopts the single-level, two-phase scheduling method. The pure
MPI programming paradigm is used for parallel programming in PLS. The two-phase
scheduling adopted by PLS is similar to that in HLS except the following two designs.
(1) Each processor core, instead of a computational node, has its own performance
ratio. Therefore, the performance functions adopted by PLS and HLS are different.
(2) GSS is the only scheme adopted in the second phase. In the following subsections,
the experimental results from the three applications are presented.
C.-C. Wu et al.
Fig. 1 The local scheduler algorithm of Matrix Multiplication
every processor core, requests tasks from the global scheduler. After receiving tasks
form the global scheduler, the local scheduler assigns the tasks to all processor cores
in the same computational node. Therefore, no matter the global scheduler adopts
C.-C. Wu et al.
Fig. 4 Execution time comparisons for Sparse Matrix Multiplication. The chunk size of CSS is 64
4.3 Application 3: Mandelbrot set computation
This operation derives a resultant image by processing an input matrix, A, where A
is an image of m pixels by n pixels [13]. The resultant image is one of m pixels by n
pixels.
The proposed scheme was implemented for Mandelbrot set computation. The
global scheduler is responsible for the workload distribution. When a local sched-
uler becomes idle, the global scheduler sends two integers to the local scheduler. The
two numbers represent the beginning index and the size of the assigned chunk, re-
spectively. The tasks assigned to the local scheduler are then dispatched to OpenMP
threads based on a specified self-scheduling scheme. Unlike Matrix Multiplication,
the communication cost between the global scheduler and local scheduler is low,
and the dominant cost is the Mandelbrot set computation. The C/MPI+OpenMP code
fragment for the local scheduler for Mandelbrot set computation is listed as shown
in Fig. 5. In this application, the workload for each outer loop iteration is irregular
because the number of executions for convergence is not a fixed number. Moreover, it
requires no data communication at each scheduling step. Therefore, the performance
for workload distribution depends on the degree of variation for each iteration.
We compare Yang’s proposed PLS approach and our proposed HLS approach us-
ing execution time, as shown in Fig. 6. If the same self-scheduling scheme is adopted
by the global scheduler, HLS will outperform PLS. Surprisingly, CSS has the best
performance for PLS. The reason is described as follows. The long communication
latency in Grid increases the scheduling overhead. When the execution time for run-
ning one chunk of iterations is too short, the scheduling overhead for such a small
chunk has a significantly negative effect on the overall performance. Unlike GSS,
FSS and TSS, the chunk size for CSS is fixed to 64 iterations at each scheduling step,
avoiding assigning too small chunks in the end of scheduling.
On the other hand, TSS rather than CSS provides the best performance for HLS.
It is because HLS uses another way to avoid assigning too small chunks to one com-
putational node in the end of scheduling. In HLS, all the processor cores in the same
computational node rely on the local scheduler to issue requests for them. Conse-
C.-C. Wu et al.
Fig. 6 Execution time comparisons for Mandelbrot set computation. The chunk size of CSS is 64
effect, HLS uses a single message to assign a larger task to all processor cores in the
same node. Consequently, HLS can reduce the overhead of each scheduling step by
increasing the chunk size near the end of scheduling.
4.4 Summary
According to the results shown in the above three subsection, HLS outperforms PLS
for three different kinds of applications no matter which self-scheduling scheme is
adopted. Furthermore, TSS is the best choice for the global scheduler in HLS because
it requires the shortest execution time for any one of the three applications.
We summarize the performance improvements obtained by our proposed HLS for
three applications in Fig. 7. The speedup is derived from dividing the execution time
for PLS by the execution time for HLS, where the same self-scheduling scheme is
adopted. In the figure, MM represents Matrix Multiplication of size 4096 × 4096,
SMM represents Sparse Matrix Multiplication of size 4096 × 4096, and MS repre-
sents Mandelbrot set computation of size 7000 × 7000.
For the Matrix Multiplication, it has regular workload distribution and requires
data communications at each scheduling step. The speedups range from 1.13 to 1.30.
Because FSS is the best choice for PLS, the speedup of HLS over PLS is the smallest
if FSS is adopted. On the other hand, if CSS is adopted, HLS has the best speedup
with a factor of 1.30. For the Sparse Matrix Multiplication, it has irregular workload
distribution and requires data communications at each scheduling step. Compared
with Matrix Multiplication, HLS can provide better speedups for Sparse Matrix Mul-
tiplication. In other words, HLS is more effective than PLS for irregularly workload
distributed applications requiring data communication at each scheduling step. The
speedups range from 1.22 to 1.63. HLS can provide the best speedup when TSS is
adopted. For the Mandelbrot set computation, it has irregular workload distribution
and requires little data communications at each scheduling step. The speedups range
from 1.36 to 1.75. The best speedup is provided when FSS is adopted. It is because
FSS is not suitable for this kind of applications when PLS is applied because too much
C.-C. Wu et al.
is the best choice depends on the characteristics of the respective application. The
speedups range from 1.13 to 1.75.
In our future work, we will implement more application program types to verify
our approach. Moreover, although the experiments are carried out on a Taiwan-wide
Grid system, the average parallelism per node is only 2.6, i.e., 49/19. We will study
the performance impact when the computational nodes have more cores. According
to our experiences, allocating too much workload at a time may degrade the per-
formance for GSS. Therefore, it is possible that we have to design a new approach
when the computational nodes have more cores. Furthermore, we hope to find better
ways of modeling performance weighting using factors, such as amount of memory
available, memory access costs, network information, and CPU loading. We will also
address theoretical analysis of the proposed method.
Acknowledgements The authors would like to thank the National Science Council, Taiwan, for finan-
cially supporting this research under Contract No. NSC98-2221-E-018-008-MY2 and NSC98-2220-E-
029-004.
Appendix
Table 3 The configuration of our Grid system—part 1
C.-C. Wu et al.
Fig. 8 The distribution of the
four Grid sites over Taiwan
References
1. Banicescu I, Carino RL, Pabico JP, Balasubramaniam M (2005) Overhead analysis of a dynamic load
balancing library for cluster computing In: Proceedings of the 19th IEEE international parallel and
distributed processing symposium, pp 122.2
2. Boeres C, Nascimento AP, Rebello VEF Sena AC (2005) Efficient hierarchical self-scheduling for
MPI applications executing in computational Grids. In: Proceedings of the 3rd international workshop
on middleware for grid computing, pp 1–6
3. Chronopoulos AT, Penmatsa S, Yu N (2002) Scalable loop self-scheduling schemes for heterogeneous
clusters. In: Proceedings of the 2002 IEEE international conference on cluster computing, pp 353–
359
4. Chronopoulos AT, Penmatsa S, Xu J, Ali S (2006) Distributed loop-self-scheduling schemes for het-
erogeneous computer systems. Concurr Comput Pract Experience 18(7):771–785
5. Foster I (2002) The Grid: a new infrastructure for 21st century science. Phys Today 55(2):42–47
6. Foster I, Kesselman C (1997) Globus: a metacomputing infrastructure toolkit. Int J Supercomput Appl
High Perform Comput 11(2):115–128
7. Foster I, Kesselman C (2003) The Grid 2: blueprint for a new computing infrastructure. Morgan
Kaufmann, San Mateo
J Supercomput
DOI 10.1007/s11227-009-0271-z
Using hybrid MPI and OpenMP programming
to optimize communications in parallel loop
self-scheduling schemes for multicore PC clusters
Chao-Chin Wu · Lien-Fu Lai · Chao-Tung Yang ·
Po-Hsun Chiu
© Springer Science+Business Media, LLC 2009
Abstract Recently, a series of parallel loop self-scheduling schemes have been pro-
posed, especially for heterogeneous cluster systems. However, they employed the
MPI programming model to construct the applications without considering whether
the computing node is multicore architecture or not. As a result, every processor core
has to communicate directly with the master node for requesting new tasks no mat-
ter the fact that the processor cores on the same node can communicate with each
other through the underlying shared memory. To address the problem of higher com-
munication overhead, in this paper we propose to adopt hybrid MPI and OpenMP
programming model to design two-level parallel loop self-scheduling schemes. In
the first level, each computing node runs an MPI process for inter-node communica-
tions. In the second level, each processor core runs an OpenMP thread to execute the
iterations assigned for its resident node. Experimental results show that our method
outperforms the previous works.
Keywords Parallel loop scheduling · Cluster computing · Multicore architecture ·
MPI programming · OpenMP programming · Hybrid programming
1 Introduction
A cluster system is composed of loosely coupled computers that work together to
solve a problem in parallel by dividing a job into several smaller jobs [1, 15]. As
C.-C. Wu () · L.-F. Lai · P.-H. Chiu
Department of Computer Science and Information Engineering, National Changhua University
of Education, Changhua City, 500, Taiwan
e-mail: ccwu@cc.ncue.edu.tw
C.-T. Yang
High-Performance Computing Laboratory, Department of Computer Science and Information
Engineering, Tunghai University, Taichung, 40704, Taiwan
Optimizing self-scheduling schemes for multicore PC clusters
node to communicate with each other directly through the underlying shared mem-
ory. OpenMP is employed for this purpose because it is a shared-memory multi-
threaded programming language. Therefore, in this paper we propose a two-level self-
scheduling scheme based on the hybrid MPI and OpenMP programming mode for the
heterogeneous cluster system with multicore computers. In the first level, the system
is comprised of one global scheduler and multiple local schedulers. Each computing
node has exactly one local scheduler that will request tasks from the global scheduler
whenever it becomes idle. Every scheduler is an MPI process no matter if it is global
or local. In the second level, every local scheduler will create one OpenMP thread for
each processor core on its resident computing node. The tasks assigned to the local
scheduler will be dispatched to the parallel OpenMP threads. In our approach, only
the local scheduler will issue requests to the global scheduler, resulting in reduced
amount of inter-node communication.
To verify the proposed approach, a heterogeneous cluster is built, and three types
of application programs, matrix multiplication, sparse matrix multiplication and
Mandelbrot set computation, are implemented to be executed in this testbed. Em-
pirical results show that the proposed approach can obtain performance improvement
on previous schemes, especially for the applications that have irregular workload dis-
tribution and require a large amount of data communication at each scheduling step.
The rest of this paper is organized as follows. In Sect. 2, we introduce several
typical and well-known self-scheduling schemes. In Sect. 3, we describe how to em-
ploy the hybrid MPI and OpenMP programming model to develop the two-level self-
scheduling schemes. Next, our system configuration is specified and experimental
results on two types of application programs are also presented in Sect. 4. Finally, the
conclusion remarks and future work are given in the last section.
2 Related work
A parallel loop is a loop having no cross-iteration data dependences. If a parallel loop
has N iterations, it can be executed at most by N processors in parallel without any
interaction among processors. However, because the number of available processors
in a system is always much smaller than N , each processor has to execute more than
one loop iteration. Static scheduling schemes decide how many loop iterations are
assigned for each processor at compile time. The advantage of this kind of schedul-
ing schemes is no scheduling overhead at runtime. In addition, it is very applicable
to the homogeneous computing system when each loop iteration takes roughly the
same amount of time. However, it is hard to estimate the computation power of every
processor in the heterogeneous computing system and to predict the amount of time
each iteration takes for irregular programs, resulting in load imbalance usually.
Dynamic scheduling is more suitable for load balance because it makes scheduling
decisions at runtime. No estimations and predictions are required. Self-scheduling is
a large class of adaptive and dynamic centralized loop scheduling schemes. Initially,
a portion of the loop iterations is scheduled to all processors. As soon as a slave
processor becomes idle after it has finished the assigned workload, it requests the
scheduler of unscheduled iterations. The total number of iterations that a processor
Optimizing self-scheduling schemes for multicore PC clusters
loop iterations in two phases. In the first phase, α% of workload is partitioned ac-
cording to the performance weighted by CPU clock speed. In the second phase, the
rest (100 − α)% of workload is distributed according to a traditional self-scheduling
scheme. The first phase adopts static scheduling to reduce the scheduling overhead. In
contrast, the second phase uses dynamic scheduling to achieve good load balancing.
The success of the proposed heuristic heavily depends on the appropriate selection of
the α value that is specified either by programmer or by the compiler.
Unlike their first work using CPU clock speed for performance estimation in the
first phase, they evaluated computer performance by using HINT Performance Ana-
lyzer Tool in their second work [20] because many attributes influence system per-
formance, including CPU clock speed, available memory, communication cost, and
so forth [12]. HINT (Hierarchical INTegration) is a computer benchmarking tool de-
veloped at the Ames Laboratory Scalable Computing Laboratory (SCL). Unlike con-
ventional benchmarks, HINT neither fixes the problem size nor the calculation time.
Instead, it measures “QUIPS” (QUality Improvement Per Second) as a function of
time. HINT is used to determine whether target systems are relatively homogeneous
or relatively heterogeneous. They also adaptively adjust the α value according to the
heterogeneity of the cluster [6]. If the target system is relatively heterogeneous, the
α value is set to be their defined Heterogeneous Ratio (HR); otherwise, the α value
is set to be 100. As a consequence, the computing power can be more accurately
estimated and the α value is determined automatically.
In the latest scheme [22], they proposed a performance-based approach, which
partitions loop iterations according to the performance ratio of cluster nodes. They
defined a performance function for each computing node. However, no performance
functions are explicitly defined in the paper. Instead, application execution time is
used to estimate the values of performance function for all nodes. The reciprocal
of the execution time of the target program on each computing node is recorded
to form the performance function value. The ratios of these reciprocals are defined
as performance ratio of cluster nodes. According to the performance ratio among
all slave nodes, firstly α percent of the total workload is statically scheduled. Next,
the remainder of the workload is dynamically scheduled by a known self-scheduling
scheme.
Although previous works have studied the hybrid MPI and OpenMP program-
ming model, they used the self-scheduling scheme only in the OpenMP paradigm
[7, 13]. In this paper, we apply the self-scheduling scheme to both the MPI processes
and the OpenMP threads. To improve the performance for the proposed two-level
self-scheduling scheme, we investigate how to redesign the allocation functions for
various well-known self-scheduling schemes.
3 Two-level loop self-scheduling
The feature of the heterogeneous cluster system with multicore computers is that
it combines the main concepts of distributed-memory and shared-memory parallel
machines in a single system. Accordingly, we propose a two-level self-scheduling
scheme in this section, and then present how to apply our method to the four well-
known self-scheduling schemes by amending their allocation functions.
Optimizing self-scheduling schemes for multicore PC clusters
Fig. 2 Communications with
MPI processes and OpenMP
threads on the heterogeneous
cluster system with multicore
computing nodes
Moreover, the master process may become a bottleneck of the system performance
because there are so many and frequent requests from slave processes at the runtime.
To exploit the feature of the hierarchical structure of the emerging cluster sys-
tem, we propose the following two-level scheduling scheme. The hybrid MPI and
OpenMP programming model is adopted to develop parallel applications. Like most
cluster systems, the MPI de facto standard, a message-passing programming lan-
guage, is employed to design parallel programs for inter-node communications. Each
computing node runs an MPI process no matter how many processor cores it has.
However, OpenMP is used for intra-node communications. Each MPI process will
fork OpenMP threads depending on the number of processor cores in its underly-
ing computing node. Every processor core runs one OpenMP thread. OpenMP is a
shared-memory multithreaded programming language, which well matches with the
feature of the multicore computing node. Unlike MPI processes having completely
separate program contexts with their own variables and memory allocations, OpenMP
threads share the same memory space and global variables between routines. Conse-
quently, OpenMP threads require less memory space than MPI processes. This prop-
erty is very beneficial for applications requiring more memory space than the physical
memory space. Frequent memory swapping caused by the virtual memory technique
will degrade the system performance significantly. Therefore, we can combine both
the advantages of message-passing programming and shared-memory programming
by the hybrid MPI and OpenMP programming model for the multicore PC cluster.
The scheduling scheme consists of one global scheduler and multiple local sched-
ulers. Each slave computing mode has one local scheduler. One processor core of
each slave computing node is responsible for the execution of the local scheduler.
The processor core running the local scheduler is called the master core and the oth-
ers are called slave cores. The global scheduler and the local schedulers are all MPI
processes. We give an example, as shown in Fig. 2, to explain our approach. Initially,
there are three MPI processes created, one for the global scheduler and two for the
local schedulers. In addition, all the slave cores on these two slave computing nodes
run nothing. All the loop iterations are kept in the global scheduler. No slave cores
are allowed to request iterations directly from the global scheduler. Instead, they have
to request from the local scheduler in the same computing node. To utilize the feature
of shared-memory architecture, every MPI process of the local scheduler will create
Optimizing self-scheduling schemes for multicore PC clusters
global scheduler employs the performance-based scheduling approach. We describe
the performance function used in this paper as follows.
Let M denote the number of computing nodes, P denote the total number of
processor cores. Computing node i is represented by mi , and the total number of
processor cores in computing node mi is represented by pi , where 1 ≤ i ≤ M . In
consequence, P = ∑Mi=1 pi . The j th processor core in computing node i is repre-
sented by cij , where 1 ≤ i ≤ M and 1 ≤ j ≤ pi . N denotes the total number of
iterations in some application program and f () is an allocation function to produce
the chunk-size at each step. The output of f is the chunk-size for the next iteration.
At the sth scheduling step, the global scheduler computes the chunk-size Cs for the
computing node i and the remaining number of tasks Rs ,
R0 = N,Cs = f (s, i), Rs = Rs−1 − Cs, (1)
where f () possibly has more parameters than just s and i, such as Ri−1. To estimate
the performance of each computing node, we define a performance function (PF) for
a computing node i as
PFi (V1,V2, . . . , VX), (2)
where Vr,1 ≤ r ≤ X, is a variable of the performance function. In this paper, our PF
for a computing node i is defined as
PFi =
∑pi
k=1
1
tik
∑M
q=1
∑pi
k=1
1
tqk
, (3)
where tij is the execution time (seconds) of processor core j on computing node i
for some application program, such as matrix multiplication.
Our proposed two-level scheduling approach does not necessarily outperform
the single-level scheduling approach proposed by Yang et al. [22] if the allocation
functions for self-scheduling schemes are not carefully designed. In the single-level
scheduling approach, each MPI process requests iterations from the scheduler only
for himself. By contrast, in the two-level scheduling approach, the iterations assigned
to the local scheduler will be processed in parallel by OpenMP threads. The number
of iterations allocated at each scheduling step in the two-level scheduling approach
might become much larger than that in the single-level scheduling approach. Dur-
ing the global scheduler transmits the required information and data to some local
scheduler, no more requests from other local schedulers can be processed, resulting
in longer waiting time for other local scheduler. If the amount of communication
for one scheduling step is too large, the system performance will be degraded. For
this reason, this paper also focuses on the design of allocation functions for various
self-scheduling schemes.
According to the performance function, we propose four scheduling schemes in
the following by modifying the allocation functions of four well-known scheduling
schemes, where ρ denotes an application-dependent constant.
Layered Chunk Self-Scheduling (LCSS) is similar to CSS except that the allo-
cation function f () is amended as follows: Cs = f (s, i,P ) = PFi × k, where the
constant k denotes the fixed chunk size.
Optimizing self-scheduling schemes for multicore PC clusters
Fig. 3 The algorithm of the global scheduler
each scheduling step. Sparse matrix multiplication has irregular workload distribution
and requires data communication at each scheduling step. Mandelbrot set computa-
tion has irregular workload distribution and requires no data communication at each
scheduling step. The heuristic self-scheduling (HSS) proposed by Yang et al. [22] is
compared with our approach, layered self-scheduling (LSS).
4.1 Application 1: Matrix Multiplication
Matrix Multiplication is a fundamental operation in many numerical linear algebraic
applications. Its efficient implementation on parallel computers is an issue of prime
importance when providing such systems with scientific software libraries. Conse-
quently, considerable effort has been devoted in the past to the development of effi-
cient parallel matrix multiplication algorithms, and this will remain a task in the fu-
Optimizing self-scheduling schemes for multicore PC clusters
ture as well. Many parallel algorithms have been designed, implemented, and tested
on different parallel computers or cluster of workstations for matrix multiplication.
We have implemented the proposed scheme for Matrix Multiplication. The in-
put matrix A will be partitioned into a set of rows and kept in the global sched-
uler. At runtime, after the global scheduler decides which rows will be assigned to at
each scheduling step, the corresponding row data will be sent to the requesting slave
process. On the other hand, every local scheduler has a copy of the input matrix B
because it is needed to calculate every row of the matrix C. The global scheduler,
viz. the Master module, is responsible for the distribution of workloads to the lo-
cal schedulers, viz. slave nodes. When a local scheduler becomes idle, the global
scheduler sends the local scheduler one integer indicating how many rows will be
assigned. Next, the global scheduler sends the corresponding data to the local sched-
uler. Finally, the OpenMP threads will follow the specified scheduling scheme such
as guided self-scheduling to calculate the assigned rows. The C/MPI+OpenMP code
fragment of the Slave module for Matrix Multiplication is listed as shown in Fig. 5.
As the source code shows, a row is the atomic unit of allocation.
We evaluate the performances of four kinds of scheduling schemes in the follow-
ing. First, we compare chuck self-scheduling based schemes as shown in Fig. 6. The
label HCSS(α = x) in the legend denotes the heuristic CSS proposed by Yang et al.
[22] and the value of α is equal to x. The label LCSS_openmp(T ) in the legend repre-
sents our proposed layered CSS and the self-scheduling scheme adopted by the local
scheduler is T . The speedup is obtained by dividing the execution time of the HCSS
with α equal to 50 by the execution time of some scheme with the same matrix size. In
this experiment, a larger α value leads to a poorer performance in the HCSS approach.
The performance difference between different α values is enlarged when the matrix
size becomes larger. On the other hand, our scheme outperforms the HCSS no matter
which kind of self-scheduling scheme the local scheduler adopts. The speedups of
our scheme are almost all larger than 2. Because the workload distribution is regular,
if the local scheduler adopts the static scheme, we can get the best performance. In
this case, our scheme for input size 4096 × 4096 provides 49.2%, 40.2%, or 40.0%
performance improvement over the HCSS if static scheme, CSS, or GSS is adopted
by the local scheduler, respectively.
Second, we compare guided self-scheduling based schemes as shown in Fig. 7.
The label HGSS(α = x) in the legend denotes the heuristic GSS proposed by Yang
et al. [22] and the value of α is equal to x. The label LGSS_openmp(T ) in the leg-
end represents our proposed layered GSS and the self-scheduling scheme adopted
by the local scheduler is T . In this experiment, a larger α value leads to a better
performance in the HGSS approach. Our scheme outperforms the HGSS no matter
which kind of self-scheduling scheme the local scheduler adopts. The speedups of
our schemes range from 1.4 to 1.58. Because the workload distribution is regular,
if the static scheme is adopted in the second-level scheduling, we can get the best
performance improvement. The performance differences between static and dynamic
schemes become larger when the matrix size becomes larger. In this case, our scheme
for input size 4096 × 4096 provides 56.2%, 47.3% or 47.1% performance improve-
ment over the HGSS if static scheme, CSS, or GSS is adopted by the local scheduler,
respectively.
Optimizing self-scheduling schemes for multicore PC clusters
Fig. 6 Comparison of CSS based scheme for matrix multiplication. The chunk size is 128
Fig. 7 Comparison of GSS based scheme for matrix multiplication
represents our proposed layered GFSS and the self-scheduling scheme adopted by the
local scheduler is T . In this experiment, no better α value can guarantee better per-
formances for different matrix sizes in the LFSS approach. Our scheme outperforms
the HFSS only when the matrix size is equal to 4096 × 4096. The best speedup that
our schemes can provide is 1.13 when the local scheduler employs static schedul-
ing and the matrix size is equal to 4096 × 4096. In this case, our scheme for input
size 4096 × 4096 provides 12.9%, 5.9%, or 6.8% performance improvement over the
HFSS if static scheme, CSS, or GSS is adopted by the local scheduler, respectively.
Fourth, we compare trapezoid self-scheduling based schemes as shown in Fig. 9.
The label HTSS(α = x) in the legend denotes the heuristic TSS proposed by Yang et
al. and the value of α is equal to x. The label LTSS_openmp(T ) in the legend repre-
Optimizing self-scheduling schemes for multicore PC clusters
Fig. 10 The algorithm of sparse
matrix multiplication
Fig. 11 Comparison of CSS based scheme for sparse matrix multiplication. The chunk size is 128
4.2 Application 2: Sparse Matrix Multiplication
Sparse Matrix Multiplication is the same as Matrix Multiplication, as described in
Sect. 4.1, except that the input A is a sparse matrix. Assume that 50% of elements in
matrix A are zero and all the zeros are in the lower rectangular. If an element in matrix
A is zero, the corresponding calculation is omitted as shown in Fig. 10. Therefore,
the workload distribution of iterations in Sparse Matrix Multiplication is irregular.
We evaluate the performances of four kinds of scheduling schemes in the follow-
ing. First, we compare chuck self-scheduling based schemes as shown in Fig. 11. In
this experiment, a larger α value does not necessarily lead to a better performance in
the HCSS approach. On the other hand, our scheme outperforms the HCSS no matter
which kind of self-scheduling scheme the local scheduler adopts. The speedups are
all larger than 1.7. However, the improvement decreases when the matrix size is en-
larged. For the input size 4096 × 4096, our scheme provides 84.2%, 69.6% or 69.3%
performance improvement over the HCSS if static scheme, CSS, or GSS is adopted
by the local scheduler, respectively.
Though the workload distribution is irregular, static scheme adopted in the second
level provides better performance than dynamic scheme. To explain the reason, we
chose twenty chunks evenly from 2048 chunks and depicted their execution times
in Fig. 12. The workload is decreased constantly and the execution time difference
between any two consecutive sampled chunks is very small. Beware that there are 105
Optimizing self-scheduling schemes for multicore PC clusters
Fig. 14 Comparison of FSS based scheme for sparse matrix multiplication
Our scheme outperforms the HGSS no matter which kind of self-scheduling scheme
is adopted in the second-level scheduling. The speedups are all larger than 1.6 and
the static scheme always provides the best performance when the matrix size is the
same. In the second level scheduling, these two kinds of dynamic schemes provide
similar performance improvements. In this case, our scheme for input size 4096 ×
4096 provides 80.3%, 68.5% or 67.3% performance improvement over the HGSS if
static scheme, CSS, or GSS is adopted by the local scheduler, respectively.
Third, we compare factoring self-scheduling based schemes as shown in Fig. 14.
In this experiment, a larger α value may lead to a poorer performance in the HFSS ap-
proach. Our scheme outperforms the HFSS no matter which kind of self-scheduling
scheme is adopted in the second level scheduling. Our schemes can provide more
performance improvements when the matrix size becomes larger. Static scheduling
adopted in the second level still provides the best speedup. In this case, our scheme
for input size 4096 × 4096 provides 48.5%, 37.3% or 37.8% performance improve-
ment over the HFSS if static scheme, CSS, or GSS is adopted by the local scheduler,
respectively.
Fourth, we compare trapezoid self-scheduling based schemes as shown in Fig. 15.
In this experiment, a larger α value will degrade performance in the HTSS approach.
On the other hand, our scheme outperforms the HTSS no matter which kind of
scheduling schemes is employed in the OpenMP parallel section. The largest matrix
size can provide the best performance improvement. In the second-level scheduling,
static scheme provides the best performance improvement. Guided and chunk self-
scheduling schemes adopted by the local scheduler provide almost the same speedup.
In this case, our scheme for input size 4096 × 4096 provides 52.1%, 37.3% or 37.8%
performance improvement over the HTSS if static scheme, CSS, or GSS is adopted
by the local scheduler, respectively.
4.3 Application 3: Mandelbrot set computation
The Mandelbrot set is a problem involving the same computation on different data
points which have different convergence rates [9]. The Mandelbrot set, named after
Optimizing self-scheduling schemes for multicore PC clusters
Fig. 16 The local scheduler algorithm of Mandelbrot set computation
ment over the HCSS if static scheme, CSS, or GSS is adopted by the local scheduler,
respectively.
Second, we compare guided self-scheduling based schemes as shown in Fig. 18.
In this experiment, increasing the α value may degrade performance in the HGSS ap-
Optimizing self-scheduling schemes for multicore PC clusters
Fig. 19 Comparison of FSS based scheme for Mandelbrot set computation
Fig. 20 Comparison of TSS based scheme for Mandelbrot set computation
provides 14.3%, 24.6% or 17.0% performance improvement over the HFSS if static
scheme, CSS, or GSS is adopted by the local scheduler, respectively.
Fourth, we compare trapezoid self-scheduling based schemes as shown in Fig. 20.
In this experiment, a larger α value does not necessarily improve performance in the
HTSS approach. Our scheme outperforms the HTSS no matter which kind of self-
scheduling schemes the local scheduler adopts. If the second level scheduling adopts
the chunk self-scheduling, we can get the best performance. Dynamical scheduling
is more suitable for Mandelbrot set computation because it has irregular workload
distribution and requires little amount of communication at every scheduling step.
However, if guided self-scheduling is adopted by the local scheduler, the performance
improvement is similar to that if static self-scheduling is adopted because of the ex-
cessive scheduling overhead. In this case, our scheme for input size 1500×1500 pro-
Optimizing self-scheduling schemes for multicore PC clusters
ter which kind of scheme is adopted in the second level scheduling, our scheme can
improve the performance over 20%. If any one of LCSS, LFSS, or LTSS is adopted,
dynamic scheduling is preferred in the second level scheduling, especially the guided
self-scheduling.
For the sparse matrix multiplication, it has irregular workload distribution and
requires data communications at each scheduling step. Increasing the α value does
not necessarily improve the performance for different schemes in the heuristic self-
scheduling. For our scheme, the second level scheduling prefers the static scheme no
matter which scheme is adopted in the first level scheduling. The reason may be that
the chunk size dispatched to the local scheduler at each scheduling step is not large
enough. Consequently, it is hard to achieve load balancing by dynamic scheduling
and the scheduling overhead will degrade the performance. On the other hand, com-
pared with the results of Mandelbrot set computation, our approach can improve the
performance in a higher degree for sparse matrix multiplication, especially if LCSS
is adopted. Therefore, the amount of data communications at each scheduling step
will affect the performance improvement provided by our approach significantly.
For the matrix multiplication, it has regular workload distribution and requires
data communications at each scheduling step. The results are similar to that of sparse
matrix multiplication. However, its improvements are less than that of sparse matrix
multiplication. Therefore, our approach is more beneficial to the applications with
irregular workload distribution if the amount of data communication required is fixed.
According to the execution time, LGSS with the static local scheduling is the best
choice regardless of program characteristics. That is, combining the GSS dynamic
scheduling in the first level and the static scheduling in the second level can provide
more stable performance improvements. The first level scheduling aims at balancing
the workload assignment dynamically among heterogeneous computational nodes by
the GSS scheme, while the local scheduling emphasizes the reduction of the local
scheduling overhead since the amount of workload per scheduling step is not very
large.
In the following, the Mandelbrot set computation implemented by GSS based
schemes will be further analyzed to explain the source of performance improvement
and degradation for different approaches. The HGSS scheme has much longer aver-
age idle time than the LGSS scheme, as shown in Fig. 22, where average idle time
denotes how long a process has to wait for the completion of the program. Further-
more, the maximum idle time in the HGSS schemes is much longer than that in the
LGSS scheme, as shown in Fig. 23, where the maximum idle time is the longest idle
time among all processes. According to the results shown in Fig. 22 and Fig. 23, the
LGSS scheme has the better load balancing than the HGSS scheme.
Next, we investigate how many messages have to transmit from the global sched-
uler to the local schedulers for various GSS based schemes. For the HGSS scheme,
because the α value specifies how much the workload will be scheduled statically,
the larger the α value is, the less the communication is required, as shown in Fig. 24.
Regardless the α value, our proposed LGSS requires a much smaller amount of ex-
pensive inter-node communication.
Finally, we compare the scheduling overheads for MPI processes and OpenMP
threads as follows. We constructed a cluster system consisting of one master node
Optimizing self-scheduling schemes for multicore PC clusters
Fig. 24 The number of inter-node messages required for dispatching tasks from the global scheduler to
the local schedulers
OpenMP model, the scheduling overhead is calculated as follows:
ETw − ET1
w
, (9)
where ETw represents the execution time when either w slave MPI processes or w
parallel OpenMP threads are created, and ET1 represents the execution time when
there is only one slave MPI process or one OpenMP thread. The scheduling overhead
in the pure MPI model is much higher than that in the pure OpenMP model, as shown
in Fig. 25. Moreover, the scheduling overhead becomes larger when the image size
is increased because the GSS scheme requires more steps to finish the task assign-
ment. The lower overhead for local scheduling explains why our proposed method
outperforms the HSS approach.
5 Conclusions and future work
Previous research did not consider the feature of multicore computers when they pro-
posed enhanced self-scheduling methods. In addition, they developed applications
based on MPI message-passing programming model. In this paper, a cluster system
with multicore computing nodes is regarded as a two-level hierarchical structure. The
first level consists of computing nodes and the second level is comprised of proces-
sor cores. Accordingly, we proposed a two-level loop self-scheduling scheme based
on the hybrid MPI and OpenMP programming model. MPI processes are used for
inter-node communications and OpenMP threads are employed for intra-node com-
munications. MPI processes communicate with each other by transmitting messages
Optimizing self-scheduling schemes for multicore PC clusters
References
1. Baker M, Buyya R (1999) Cluster computing: the commodity supercomputer. Int J Softw Pract Exp
29(6):551–575
2. Beaumont O, Casanova H, Legrand A, Robert Y, Yang Y (2005) Scheduling divisible loads on star
and tree networks: results and open problems. IEEE Trans Parallel Distrib Syst 16:207–218
3. Bennett BH, Davis E, Kunau T, Wren W (2000) Beowulf parallel processing for dynamic loadbalanc-
ing. In: Proceedings of IEEE aerospace conference, 2000, vol 4, pp 389–395
4. Bohn CA, Lamont GB (2002) Load balancing for heterogeneous clusters of PCs. Future Gener Com-
put Syst 18:389–400
5. Cheng K-W, Yang C-T, Lai C-L, Chang S-C (2004) A parallel loop self-scheduling on grid computing
environments. In: Proceedings of the 2004 IEEE international symposium on parallel architectures,
algorithms and networks, KH, China, May 2004, pp 409–414
6. Chronopoulos AT, Andonie R, Benche M, Grosu D (2001) A class of loop self-scheduling for hetero-
geneous clusters. In: Proceedings of the 2001 IEEE international conference on cluster computing,
2001, pp 282–291
7. He Y, Ding HQ (2002) MPI and OpenMP paradigms on cluster of SMP architectures: the vacancy
tracking algorithm for multi-dimensional array transposition. In: Proceedings of the 2002 ACM/IEEE
conference on supercomputing, 2002, pp 1–14
8. Hummel SF, Schonberg E, Flynn LE (1992) Factoring: a method scheme for scheduling parallel loops.
Commun ACM 35:90–101
9. Introduction to the Mandelbrot set (2008) http://www.ddewey.net/mandelbrot/
10. Li H, Tandri S, Stumm M, Sevcik KC (1993) Locality and loop scheduling on NUMA multiproces-
sors. In: Proceedings of the 1993 international conference on parallel processing, vol II, 1993, pp
140–147
11. Polychronopoulos CD, Kuck D (1987) Guided self-scheduling: a practical scheduling scheme for
parallel supercomputers. IEEE Trans Comput 36(12):1425–1439
12. Post E, Goosen HA (2001) Evaluation the parallel performance of a heterogeneous system. In: Pro-
ceedings of 5th international conference and exhibition on high-performance computing in the Asia-
Pacific region (HPC Asia 2001)
13. Rosenberg R, Norton G, Novarini JC, Anderson W, Lanzagorta M (2006) Modeling pulse propagation
and scattering in a dispersive medium: performance of MPI/OpenMP hybrid code. In: Proceedings of
the ACM/IEEE conference on supercomputing, 2006, pp 47–47
14. Shih W-C, Yang C-T, Tseng S-S (2007) A performance-based parallel loop scheduling on grid envi-
ronments. J Supercomput 41(3):247–267
15. Sterling T, Bell G, Kowalik JS (2002) Beowulf cluster computing with Linux. MIT Press, Cambridge
16. Tang P, Yew PC (1986) Processor self-scheduling for multiple-nested parallel loops. In: Proceedings
of the 1986 international conference on parallel processing, 1986, pp 528–535
17. The Scalable Computing Laboratory (SCL) (2008) http://www.scl.ameslab.gov/
18. Tzen TH, Ni LM (1993) Trapezoid self-scheduling: a practical scheduling scheme for parallel com-
pilers. IEEE Trans Parallel Distrib Syst 4:87–98
19. Yang C-T, Chang S-C (2004) A parallel loop self-scheduling on extremely heterogeneous PC clusters.
J Inf Sci Eng 20(2):263–273
20. Yang C-T, Cheng K-W, Li K-C (2005) An enhanced parallel loop self-scheduling scheme for cluster
environments. J Supercomput 34(3):315–335
21. Yang C-T, Cheng K-W, Shih W-C (2007) On development of an efficient parallel loop self-scheduling
for grid computing environments. Parallel Comput 33(7–8):467–487
22. Yang C-T, Shih W-C, Tseng S-S (2008) Dynamic partitioning of loop iterations on heterogeneous PC
clusters. J Supercomput 44(1):1–23
Optimizing self-scheduling schemes for multicore PC clusters
Po-Hsun Chiu is an undergraduate in the Department of Computer Sci-
ence and Information Engineering at National Changhua University of
Education in Taiwan. After receiving the B.Sc. degree in June 2009, he
will become a graduate student in the Department of Computer Science
and Information Engineering at National Taiwan University in Taiwan.
His research interests include parallel computing and multicore embed-
ded systems.
C.-T. Yang et al.
As more and more inexpensive personal computers (PC) become available, clusters of
PCs are becoming alternatives to the supercomputers many research projects cannot
afford. As computer architectures become more and more diverse and heterogenic,
and computer expiration rates are higher than before, we can put old and unused
computers to efficient use in our research. Therefore, it is natural because of the
fast development of information technology that clusters consist of computers with
various processors, memories and hard disk drives. However, it is difficult to deal
with such heterogeneity in a cluster [2, 7, 13, 14, 16, 19, 22–24].
Loop scheduling and load balancing on parallel and distributed systems are crit-
ical problems that are difficult to cope with, especially on the emerging PC-based
clusters [3, 4]. In this aspect, an important issue is how to assign tasks to nodes
so that the nodes’ loads are well balanced. Conventional self-scheduling loop ap-
proaches [11] include static scheduling and dynamic scheduling. However, the for-
mer considers computing nodes as homogeneous resources, and thus not suitable for
heterogeneous environments, and the latter, especially self-scheduling, can still be
improved [6, 22, 24–26].
Previous researchers proposed some useful self-scheduling schemes applicable
to PC-based clusters [6, 25, 26] and grid computing environments [22, 24]. These
schemes consist of two phases. In the first phase, system configuration information
is collected, and some portion of the workload is distributed among slave nodes ac-
cording to their CPU clock speeds [22] or HINT measurements [6, 24, 25]. After
that, the remaining workload is scheduled using some well-known self-scheduling
scheme, such as GSS [11]. The performance of this approach depends on an appro-
priate choice of scheduling parameters since it estimates node performance using
only CPU speed or HINT benchmark, which are factors affecting node performance.
In [6], an enhanced scheme, which dynamically adjusts scheduling parameters ac-
cording to system heterogeneity, is proposed.
Intuitively, we may want to partition loop iterations according to CPU clock speed.
However, the CPU clock is not the only factor which affects the node performance.
Many other factors also have dramatic influences, such as the amount of memory
available, the cost of memory accesses, and the communication medium between
nodes. Using the intuitive approach will result in degraded performance if the perfor-
mance estimation is not accurate.
In this paper, we propose a general approach that utilizes performance functions to
estimate performance weights for each node. To verify the proposed approach, a het-
erogeneous cluster was built, and three types of application program, matrix multipli-
cation, circuit satisfiability, and Mandelbrot set computation [9], were implemented
for execution on this testbed. Empirical results show that for heterogeneous cluster
environments, the proposed approach obtained improved performance compared to
the previous schemes.
Previous work in [6, 24–26] and this paper were all inspired by [22], the α self-
scheduling scheme. However, this work has a different perspective and a unique con-
tribution. First, while [6, 24] partition α% of workloads according to CPU clock
speed performance weighting in phase one, the proposed scheme partitions accord-
ing to a general performance function (PF). In this paper, we do not define an explicit
performance function. Instead, CPU clock and HPC Challenge (HPCC) [5] perfor-
mance measurement are used to estimate the value of performance weighting (PW)
C.-T. Yang et al.
Table 1 Partition size examples
Scheme Partition size
PSS 1, 1, 1, 1, 1, 1, 1, 1, 1, . . . , 1
CSS(128) 128, 128, 128, 128, 128, 128, 128, 128
GSS [7] 256, 192, 144, 108, 81, 61, 46, 34, 26, . . .
FSS [10] 128, 128, 128, 128, 64, 64, 64, 64, 32, . . .
TSS [17] 128, 120, 112, 104, 96, 88, 80, 72, 64, . . .
divided by the number of available processors. At the ith scheduling step, the master
computes the chunk-size Ci and the remaining number of tasks Ri :
RO = N, Ci = f (i,p), Ri = Ri−1 − Ci (1)
where f () may have more parameters than just i and p, such as Ri−1. The master
assigns Ci tasks to an idle slave and the load-imbalance will depend on the execution
time gap between the nodes [7, 10]. Different ways of computing Ci have given rise
to various scheduling schemes. The most notable examples are Pure Self-Scheduling
(PSS), Chunk Self-Scheduling (CSS), Factoring Self-Scheduling (FSS), Guided Self-
Scheduling (GSS), and Trapezoid Self-Scheduling (TSS) [8, 11, 21]. Table 1 shows
the various chunk sizes for a problem with iterations numbering N = 1024 and
processors numbering p = 4.
Pure Self-Scheduling (PSS) was the first straightforward dynamic loop scheduling
algorithm. In this paper, a processor is said to be idle if it has not been assigned a
chunk of workload or it has finished its assigned workload. Whenever a processor
falls idle, iterations are assigned to it. This algorithm achieves good load balancing
but induces excessive overhead [10].
Chunk Self-Scheduling (CSS) assigns k iterations each time, where k, the chunk-
size, is fixed and must be specified by either the programmer or the compiler.
When k is 1, the scheme is purely self-scheduling, as discussed above. Large chunk
sizes cause load-imbalance, while small chunk sizes are likely to produce excessive
scheduling overhead [10].
Guided Self-Scheduling (GSS) can dynamically change the number of iterations
assigned to idle processors [11]. More specifically, the next chunk-size is determined
by dividing the number of iterations remaining in a parallel loop by the number of
available processors. The property of decreasing chunk-size implies that an effort
is made to achieve load balancing and to reduce scheduling overhead. By assigning
large chunks at the beginning of a parallel loop, one can reduce the frequency of
communication between master and slaves. The small chunks at the end of a loop
partition serve to balance the workload across all working processors.
Factoring Self-Scheduling (FSS) assigns loop iterations to working processors in
phases [8]. During each phase, only a subset of remaining loop iterations (usually
half) is divided equally among available processors. Because FSS assigns a subset of
the remaining iterations in each phase, it balances workloads better than GSS when
loop iteration computation times vary substantially. The synchronization overhead of
FSS is not significantly greater than that of GSS.
C.-T. Yang et al.
2.2.2 MPICH-G2
MPI is a message-passing library standard that was published in May 1994. The
“standard” of MPI is based on the consensus of the participants in the MPI Fo-
rums [2], organized by over 40 organizations. Participants include vendors, re-
searchers, academics, software library developers and users. MPI offers portability,
standardization, performance and functionality [2].
MPICH-G2 [1, 2] is a Grid-enabled implementation of the MPI v1.1 standard.
That is, using services from the Globus Toolkit (e.g., job startup, security), MPICH-
G2 allows you to couple multiple machines, potentially of different architectures, to
run MPI applications. MPICH-G2 automatically converts data in messages sent be-
tween machines of different architectures and supports multi-protocol communica-
tion by automatically selecting TCP for inter-machine messaging and (where avail-
able) vendor-supplied MPI for intra-machine messaging. Existing parallel programs
written for MPI can be executed over the Globus infrastructure just after recompila-
tion [1].
2.3 HPC challenge benchmark
The HPC Challenge (HPCC) is a useful computer benchmarking tool [5]. It first ex-
amines the performance of HPC architectures using kernels with more challenging
memory access patterns than High Performance Linpack (HPL). It also augments the
TOP500 list. It provides benchmarks that bound the performance of many real ap-
plications as a function of memory access characteristics—e.g. spatial and temporal
locality. Unlike conventional benchmarks, the HPCC benchmark consists of 7 basic
tests, consisting of HPL, DGEMM, STREAM, PTRANS, Random Access, FFT, and
Communication bandwidth and latency. In our work, we use the HPL measurement
as a performance value and include it in our scheduling algorithm.
3 Proposed approach
Cluster computers have different performance scales in heterogeneous environments.
In such situations, additional slave computers may not perform well because known
self-scheduling schemes partition workloads according to formulas rather than com-
puter performance. The aim of this work is to promote the performance of loop
scheduling for the emerging heterogeneous computing environments. By considering
a new parameter, β (proportion of HPCC benchmark results in performance estima-
tion), performance of loop scheduling can benefit from more balanced load distrib-
ution. In FSS, for example, every slave gets a workload of size N/2P , where N is
the total workload and P is the number of processors. If the performance difference
between the fastest and the slowest computer is larger than N/2P , a load-imbalance
occurs. In this section, we first introduce the system model, then describe the per-
formance weighting parameters and static–workload ratio, and finally, present the
skeleton algorithm for performance-based loop scheduling.
C.-T. Yang et al.
Algorithms MASTER and SLAVE in pseudo code:
Module MASTER /* scheduler */
/* perform task scheduling, load balancing and some computation */
[Initialization]
/* Stage 1: Gathering the information. */
Collect CPU clock speed and HPL measurements
/* Stage 2: Calculate the performance weighted */
Calculate PWj by formula (3)
/* Stage 3: Static Scheduling */
r=0;
for (i= 1; i< member_of_nodes; i+
{
partition α% of loop iterations according to the performance
weighted;
send data to all nodes;
r+
}
Master does its own computation work
/* Stage 4: Dynamic Scheduling */
Partition (100−α) of loop iterations into the task queue using
some well-known self-scheduling scheme
/* Stage 5: Probe for returned results */
Do{
Distinguish source and receive returned data
If the task queue is not empty then
Send another data to the idle slave
r−
else
send TAG= to the idle slave
}while (r >
[Finalization]
END MASTER
Module SLAVE /* worker */
[Initialization]
Probe if some data in
While (TAG>
{
Receive initial solution and size of subtask work and
compute fine solution
Send the result to the master
Probe if some data in
}
[Finalization]
END SLAVE
3.3 System model
The HPCC benchmark assisted us in comparatively analyzing all cluster nodes. We
must have a proper response and an appropriate self-scheduling scheme for change-
able system architectures and loop styles.
C.-T. Yang et al.
Table 2 Our implementation of
all matrix multiplication
programs
Master Name Description Reference #
compute
Y matstat Static scheduling [10]
N matgss GSS [11]
N matngss0 Fixed α scheduling + GSS [22]
Y matngss1 Fixed α scheduling + GSS [26]
N matngss2 Adaptive α scheduling + GSS [24, 25]
N matngss3 Proposed scheduling + GSS
Y matngss4 Proposed scheduling + GSS
N matfss FSS [8]
N matnfss0 Fixed α scheduling + FSS [22]
Y matnfss1 Fixed α scheduling + FSS [26]
N matnfss2 Adaptive α scheduling + FSS [24, 25]
N matnfss3 Proposed scheduling + FSS
Y matnfss4 Proposed scheduling + FSS
N mattss TSS [21]
N matntss0 Fixed α scheduling + TSS [22]
Y matntss1 Fixed α scheduling + TSS [26]
N matntss2 Adaptive α scheduling + TSS [24, 25]
N matntss3 Proposed scheduling + TSS
Y matntss4 Proposed scheduling + TSS
Conventional static scheduling schemes distribute the total workload equally to
all workers at compiling time. However, such schemes are obviously not suitable for
dynamic and heterogeneous environments. Therefore, a weighted static scheduling
scheme is adopted in this experiment. The partitioning principle follows PWs ratios.
Faster nodes get proportionally more workload than the slower ones.
4.1 Experiments on three applications
We implemented three classes of application in C language, with message-passing
interface (MPI) directives to parallelize code segments for execution on our testbed:
Matrix Multiplication, Mandelbrot Set Computation, and Circuit Satisfiability. The
first has regular workloads, the last irregular workloads. To enhance the readability
of experimental results, brief descriptions of all implemented programs are given in
Tables 2– 4.
4.2 Hardware configuration and terminology
We built a heterogeneous cluster consisting of eleven nodes. The hardware and soft-
ware configurations are specified in Tables 5 and 6, respectively. Figures 2 and 3
show, respectively, the network route state and topology.
C.-T. Yang et al.
Ta
bl
e
5
H
ar
dw
ar
e
co
n
fig
ur
at
io
n
H
os
tn
am
e
Pr
oc
es
so
rm
o
de
l
N
u
m
be
ro
f
M
em
o
ry
N
IC
O
S
v
er
sio
n
pr
oc
es
so
rs
siz
e
am
d6
4-
du
al
21
D
u
al
-C
or
e
A
M
D
O
pt
er
on
(tm
)P
ro
ce
ss
o
r
22
12
4
2
G
B
1G
2.
6.
21
-1
.3
19
4.
fc
7
am
d6
4-
du
al
11
A
M
D
O
pt
er
on
(tm
)P
ro
ce
ss
o
r
24
6
2
2
G
B
1G
2.
6.
21
-1
.3
19
4.
fc
7
am
d6
4-
du
al
31
A
M
D
O
pt
er
on
(tm
)P
ro
ce
ss
o
r
24
6
2
2
G
B
1G
2.
6.
18
-1
.2
79
8.
fc
6
x
eo
n
2
In
te
l(R
)X
eo
n(T
M
)
2
1.
5
G
B
1G
2.
6.
15
-1
.2
05
4_
FC
5
qu
ad
1
In
te
l(R
)C
or
e(T
M
)2
Qu
ad
Q6
60
0
4
2
G
B
1G
2.
6.
23
.1
-4
2.
fc
8
o
m
eg
a
In
te
l(R
)X
eo
n
(T
M
)
2
51
2
M
B
1G
2.
6.
15
-1
.2
05
4_
FC
5
ci
rc
a
A
M
D
A
th
lo
n(t
m)
M
P
20
00
+
2
1
G
B
10
0M
2.
6.
21
-1
.3
19
4.
fc
7
am
d-
m
pd
ua
l1
A
M
D
A
th
lo
n(t
m)
M
P
20
00
+
2
2
G
B
10
0M
2.
6.
18
-1
.2
79
8.
fc
6
co
n
do
r1
In
te
l(R
)P
en
tiu
m
(R
)4
2
51
2
M
B
10
0M
2.
6.
18
-1
.2
79
8.
fc
6
co
n
do
r2
In
te
l(R
)P
en
tiu
m
(R
)4
2
51
2
M
B
10
0M
2.
6.
18
-1
.2
79
8.
fc
6
s1
In
te
l(R
)X
eo
n(R
)C
PU
E5
31
0
8
4
G
B
1G
2.
6.
24
.4
-6
4.
fc
8
C.-T. Yang et al.
Fig. 3 Network topology
set by the programmer and choosing appropriate values adaptable to dynamic en-
vironments is difficult. In this work, the master node also participated in computa-
tion.
4.3.1 Application 1: matrix multiplication
Matrix multiplication is a fundamental operation in many numerical linear alge-
bra applications. Its efficient implementation on parallel computers is an issue of
prime importance when providing such systems with scientific software libraries.
Consequently, considerable effort has been devoted in the past to developing effi-
cient parallel matrix multiplication algorithms, and this will remain a task in the
future as well. Many parallel algorithms for matrix multiplication have been de-
signed, implemented, and tested on various parallel computers and clusters of work-
stations.
We implemented the proposed scheme for matrix multiplication. The master mod-
ule is responsible for distributing workloads. When a slave node becomes idle, the
master node sends two integers to it representing the beginning and end pointers to
an assigned chunk. In other words, every node has a local copy of the input matrices,
so data communication is not significant in this kind of implementation. This means
the communication cost between the master and the slave is low, and the dominant
cost is the matrix multiplication computation. The slave module C/MPI code frag-
ment for matrix multiplication is listed below. As the source code shows, the column
is the atomic unit of allocation.
C.-T. Yang et al.
Fig. 5 Execution times for
parameter β influence on matrix
multiplication performance with
matrix size 4096 × 4096
Fig. 6 Execution times for the proposed matrix multiplication scheduling compared with the previous
GSS group schemes
execution times for static partitioning (matstat) were orders of magnitude worse than
any of the dynamic approaches and made the results of the dynamic systems very
difficult to distinguish visually from one another, so we state them as follows: matrix
size 1024×1024 cost 57.83 seconds, matrix size 2048×2048 cost 251.7 seconds, and
matrix size 4096×4096 cost 1853.27 seconds. Figures 6–8 show execution times for
the conventional scheme (mat*ss), dynamic hybrid (matn*ss0-2), and the proposed
scheme (matn*ss3-4), on the FSS, GSS, and TSS group approaches with input matrix
sizes of 1024 × 1024, 2048 × 2048 and 4096 × 4096. Experimental results show that
the proposed scheduling scheme got better performance than the static and previous
schemes. Note that on the 4096 × 4096 matrix, our approach achieved speedups of
1.17, 1.27 and 1.07 over GSS, FSS and TSS in non-master participation, and speedups
of 1.17, 1.22 and 1.05 in master participation.
C.-T. Yang et al.
scales. Magnifying a fractal reveals small-scale details similar to its large-scale char-
acteristics. Although the Mandelbrot set is self-similar at magnified scales, its small-
scale details are not identical to the whole. In fact, the Mandelbrot set is infinitely
complex. Yet the process of generating it is based on an extremely simple equation
involving complex numbers. This operation derives a resultant image by processing
an input matrix, A, where A is an image of m by n pixels. The resultant image is one
of m by n pixels.
The proposed scheme was implemented for Mandelbrot set computation. The mas-
ter module is responsible for workload distribution. When a slave node becomes idle,
the master node sends two integers to it representing the beginning and end point-
ers to an assigned chunk. As in the matrix multiplication implementation, this keeps
communication costs between the master and the slave low, and the dominant cost is
the Mandelbrot set computation. The slave module C/MPI code fragment for Man-
delbrot set computation is listed below. In this application, the workloads for outer
loop iterations are irregular because the number of executions required for conver-
gence is not fixed. Therefore, the workload distribution performance depends on the
degree of variation between iterations.
MPI_Probe(0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);
source= status.MPI_SOURCE;
tag= status.MPI_TAG;
MPI_Get_count(&status, MPI_INT, &count);
MPI_Recv(&b[0], count, MPI_INT, source, tag, MPI_COMM_WORLD,
&status);
while (status.MPI_TAG> 0) {
/* Compute pixels in parallel */
for (i= 0; i< Nx × NY; i+ +)pix_tmp[i]= 0.0;
for (y= b[0]; y< b[1]; y+ +){
for (x= 0; x< Nx; x+ +){
c.real= Rx_min+ ((double)x× (Rx_max− Rx_min)/(double)(Nx− 1));
c.imag= Ry_min+ ((double)y× (Ry_max− Ry_min)/(double)(Ny− 1));
pix_tmp[y × Nx+ x]= cal_pixel(c);
}//for x
}//for y
/* sent result */
MPI_Send(&b[0], count, MPI_INT, 0, tag, MPI_COMM_WORLD);
/* get another size */
MPI_Probe(0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);
source= status.MPI_SOURCE;
tag= status.MPI_TAG;
MPI_Get_count(&status, MPI_INT, &count);
MPI_Recv(&b[0], count, MPI_INT, source, tag, MPI_COMM_WORLD,
&status);
}
Figures 9 and 10 show how parameters influence the performance. In this experi-
ment, we found that the proposed schemes got better performance when α was 40 and
β was about 0.7. After selecting α and β values, we carried out a set of experiments
C.-T. Yang et al.
Fig. 11 Execution times for the proposed Mandelbrot set scheduling and the previous GSS group schemes
Fig. 12 Execution times for the proposed Mandelbrot set scheduling and the previous FSS group schemes
C.-T. Yang et al.
MPI_Probe(0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);
source= status.MPI_SOURCE;
tag= status.MPI_TAG;
MPI_Get_count(&status, MPI_INT, &count);
MPI_Recv(&b[0], count, MPI_INT, source, tag, MPI_COMM_WORLD,
&status);
while (status.MPI_TAG> 0) {
/* Compute pixels in parallel */
Count_true− 0;
for (y= b[0]; y< b[1]; y+ +){
count_true+ = check_circuit (rank_no, y);
}//for y
/* sent result */
MPI_Send(&b[0], count, MPI_INT, 0, tag, MPI_COMM_WORLD);
/* get another size */
MPI_Probe(0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);
source= status.MPI_SOURCE;
tag= status.MPI_TAG;
MPI_Get_count(&status, MPI_INT, &count);
MPI_Recv(&b[0], count, MPI_INT, source, tag, MPI_COMM_WORLD,
&status);
}
Figures 14 and 15 show how parameters influence the performance. In this experi-
ment, we found that the proposed schemes got better performance when α was 40 and
β was about 0.6. After selecting α and β values, we carried out a set of experiments
with them, and compared the results with the previous scheduling algorithms. Each
experiment was averaged over ten time runs in order to achieve a better accuracy.
We first investigated GSS group execution times on the heterogeneous cluster,
then FSS group execution times, and finally TSS group execution times. Static parti-
tioning execution times (satstat) for this experiment were: 18 variable numbers cost
22.63 seconds, 19 variable numbers cost 96.12 seconds, and 20 variable numbers cost
412.45 seconds. Figures 16–18 show execution times for the static (satstat), conven-
tional (sat*ss), and dynamic hybrid (satn*ss0-2) schemes, and the proposed scheme
(satn*ss3-4), on the FSS, GSS, and TSS group approaches with 18, 19, and 20 vari-
able numbers. Experimental results show the proposed scheduling scheme got better
performance than the static and the previous ones. Master node participation in com-
putation did not raise the performance in our experiments. In this case, our scheme
achieved speedups of 1.12, 1.16, and 1.10 over GSS, FSS, and TSS for input size 20
in non-master participation and of 1.15, 1.20 and 1.10 in master participation.
We have built a Grid testbed consisting of eleven nodes. The hardware and soft-
ware configurations are specified in Tables 3 and 4, respectively. Figures 1 and 2 show
the network route state and topology.
4.4 Experimental result
In our experiments, first, the HPL measurements and CPU speed of all nodes were
collected. Next, the impact of the parameters α,β , on performance was investigated.
C.-T. Yang et al.
Fig. 16 Execution times for the proposed circuit satisfiability scheduling and the previous GSS group
schemes
Fig. 17 Execution times for the proposed circuit satisfiability scheduling and the previous FSS group
schemes
2738 seconds is for size 4096 × 4096. Figures 23–25 illustrate execution time of
traditional scheme (mat*ss), dynamic hybrid (matn*ss0-2) and the proposed scheme
(matn*ss3-4), with input matrix of size 1024 × 1024, 2048 × 2048 and 4096 × 4096,
respectively. Experimental results show that the proposed scheduling scheme got bet-
ter performance than the static and previous ones.
C.-T. Yang et al.
Fig. 20 Network topology
Fig. 21 Execution time of
parameters α influence on
matrix multiplication
performance with matrix size
4096 × 4096
5 Conclusion
In this paper, we proposed a heuristic scheme that combines the advantages of static
and dynamic loop scheduling schemes, and compared it with the previous algorithms
C.-T. Yang et al.
Fig. 24 Execution times for the proposed matrix multiplication scheduling compared with the previous
FSS group schemes
x
Fig. 25 Execution times for the proposed matrix multiplication scheduling compared with the previous
TSS group schemes
References
1. Baker M, Buyya R (2002) Cluster computing: the commodity supercomputer. Softw Pract Exp
29(6):551–575, 1999
2. Beaumont O, Casanova H, Legrand A, Robert Y, Yang Y (2005) Scheduling divisible loads on star
and tree networks: results and open problems. IEEE Trans Parall Distrib Syst 16:207–218
J Supercomput (2009) 50: 177–207
DOI 10.1007/s11227-008-0258-1
A directive-based MPI code generator for Linux PC
clusters
Chao-Tung Yang · Kuan-Chou Lai
Published online: 16 December 2008
© Springer Science+Business Media, LLC 2008
Abstract Computation requirements in scientific fields are getting heavier and heav-
ier. The advent of clustering systems provides an affordable alternative to expensive
conventional supercomputers. However, parallel programming is not easy for non-
computer scientists to do. We developed the Directive-Based MPI Code Generator
(DMCG) that transforms C program codes from sequential form to parallel message-
passing form. We also introduce a loop scheduling method for load balancing that
depends on a message-passing analyzer, and is easy and straightforward to use. This
approach provides a completely different view of loop parallelism from that in the
literature, which relies on dependence abstractions. Experimental results show our
approach can achieve efficient outcomes, and DMCG could be a general-purpose tool
to help parallel programming beginners construct programs quickly and port existing
sequential programs to PC Clusters.
Keywords Linux PC cluster · Message passing programming · Parallel loops ·
Speedup · Cluster computing
1 Introduction
As computation requirements in the science field become more demanding, the role
of parallel architectures in helping to satisfy those requirements becomes increas-
C.-T. Yang ()
High-Performance Computing Laboratory, Department of Computer Science, Tunghai University,
Taichung 40704, Taiwan
e-mail: ctyang@thu.edu.tw
K.-C. Lai
Department of Computer and Information Science, National Taichung University, Taichung 40306,
Taiwan
e-mail: kclai@ntcu.edu.tw
A directive-based MPI code generator for Linux PC clusters 179
Table 1 Parallelizing compilers for distributed-memory systems
Compiler Company or Laboratory Availability/Cost
Paraguin University of North Carolina at Wilmington Under development; prototype available
PARADIGM University of Illinois Not available
VAST-HPF Crescent Bay Software $2,395, 8-node license
Bert 77 HPC Design $2,449, 8-node academic
single-user license
PGI PGHPF Portland Group Compiler Technology team $2,958, 10 processors node-locked
(part of CDK) at STMicroelectronics
Users may adopt the Distributed-Shared Memory (DSM) library to create convenient
abstractions that allow parallel codes written for shared-memory systems to be run by
distributed systems. However, distributed shared memories are still sources of ineffi-
ciency. Even though a few companies and laboratories have developed parallelizing
compilers for distributed-memory systems, none has a low cost/performance ratio, as
shown in Table 1. The lack of such parallelizing compilers for distributed-memory
systems prompted us to develop one.
In this study, we report on developing an MPI code generator called the Directive-
Based MPI Code Generator (DMCG), and an assisted-learning tool for MPI pro-
gramming beginners. Our approach was based on analyzing communication models
for distributed-memory systems. It provided a completely different view of loop par-
allelism from those that rely on dependence abstractions. Experimental results show
our approach outperforms hand-revised codes, and that DMCG could be a general-
purpose tool for creating parallel programming quickly and porting existing sequen-
tial programs to PC Clusters.
The rest of this paper is organized as follows. Section 2 introduces the concepts
of data dependencies, loop partitioning, message-passing interfacing and communi-
cation models. Section 3 presents a performance evaluation of collective communi-
cation. Section 4 describes our system and kernel technologies. In Sect. 5, we present
experimental results, and comparisons with hand-revised codes. Finally, concluding
remarks and future directions are given in Sect. 6.
2 Background
2.1 Data dependence
There are two types of dependence: control and data [10–12]. If, in a control flow
graph, there is a path from statement S1 to statement S2, i.e., S1 determines whether
S2 can be executed, we then say that a control dependence exists between S1 and S2.
For example:
S1 if (a = 5){
S2 b = 2
S3 }.
A directive-based MPI code generator for Linux PC clusters 181
Table 2 Margin formulas and examples of dynamic loop scheduling
Scheme Formulas N = 1000, P = 4
SS Ki = 1 1 1 1 1 1 1 1 1 1 1 1 1 1. . .
CSS(k) Ki = k 125 125 125 125 125 125 125 125
CSS/λ Ki = N/λ 250 250 250 250
GSS Ki = Ri/P ,R0 = N,Ri+1 = Ri − Ki 250 188 141 106 79 59 45 33 25 19 14 11 8 6 4 3 3
2 1 1 1 1
FSS Ki = (1/2)i/P  × N/P 125 125 125 125 62 62 62 62 32 32 32 32 16 16 16
16 8 8 8 8 4 4 4 4 2 2 2 2 1 1 1 1
chunk of workload or it has finished the assigned workload. That is, an idle node does
not have a chunk of workload to execute. Whenever a processor gets idle, iterations
are assigned to it. This algorithm achieves good load balancing, but induces excessive
overhead [10].
Chunk Self-Scheduling (CSS) assigns k iterations each time, where k, the chunk
size, is fixed and must be specified by either the programmer or by the compiler.
When k is 1, the scheme is purely self-scheduling, as discussed above. Large chunk
sizes cause load imbalances, while small chunk sizes are likely to produce excessive
scheduling overhead [10].
Guided Self-Scheduling (GSS) can dynamically change the numbers of iterations
assigned to idle processors [11]. More specifically, the next chunk size is determined
by dividing the number of remaining iterations of a parallel loop by the number of
available processors. The property of decreasing chunk size implies that an effort is
made to achieve load balancing and to reduce the scheduling overhead. By assigning
large chunks at the beginning of a parallel loop, one can reduce the frequency of
communication between master and slaves. The small chunks at the end of a loop
partition serve to balance the workload across all working processors.
Factoring Self-Scheduling (FSS) assigns loop iterations to working processors in
phases [9]. During each phase, only a subset of remaining loops iterations (usually
half) is equally divided among available processors. Because FSS assigns a subset of
the remaining iterations in each phase, it balances workloads better than GSS when
loop iteration computation times vary substantially. The synchronization overhead of
FSS is not significantly greater than that of GSS.
Assume P available processors, N iterations in the DOALL loop, and a Ki -size ith
partition. Several algorithms’ formulae for calculating Ki are listed in Table 2, where
the CSS/k algorithm distributes the DOALL loop in k equal-sized chunks. Table 2
also gives sample sizes for SS, CSS(125), CSS/4, GSS, FSS, and when N = 1000
and P = 4.
The approach above is dynamic loop scheduling in which each loop partition must
be mapped to a processor. An alternative approach is static scheduling. There are two
static loop scheduling methods: block and cyclic [19]. In static scheduling, the num-
ber of chunks equals the number of processors, i.e., a scheduler is not needed when
each partition is assigned to one processor. Adopting block or cyclic scheduling in-
volves a trade-off between locality and workload distribution since block scheduling
A directive-based MPI code generator for Linux PC clusters 183
MPI was established for writing message-passing programs, and its main advan-
tages are portability and ease-of-use. The benefits of standardization are especially
apparent in distributed-memory communication environments in which higher-level
routines and/or abstractions are built upon lower-level message passing routines. Fur-
thermore, defining a message-passing standard provides vendors with well-defined
routines they can implement efficiently, or in some cases provide hardware support
for enhancing scalability. Before MPI, many message-passing libraries were offered
by various parallel computing system vendors; however, their portability was a big
problem.
There are various implementations of MPI, for example, LAM/MPI [20], now
supported and maintained by the University of Notre Dame, MPICH, implemented
by the Argonne National Lab/Mississippi State University [21], MPI/Pro, a commer-
cial implementation for clusters, Windows NT, and multicomputers provided by MPI
Software Technology, Inc. [22]. We use the LAM/MPI implementation in this study
because it is implemented according to the MPI standard and creates no MPI pro-
gramming portability problems in our system.
2.5 Communication model
Most loop parallelization technologies depend on abstract dependence analyses.
Communication model analysis plays an important role in message-passing paral-
lel programming; given the importance of translating sequential codes into parallel
ones and the fact that various models have different send/receive patterns.
In [23], McGarvey et al., classified four categories of point update methodol-
ogy: Independent, Nearest Neighbor, Quasi-Global, and Global. Since we care most
about communication behavior among processors, we simplify classification into
three categories: Independent, Semi-Global (merging Neighbor and Quasi-Global),
and Global, as shown in Fig. 1.
Each node (processor) executes update algorithms and depends on data from pre-
vious steps. If an update algorithm requires only data from previous steps, it is Inde-
pendent and needs no communication with other processors. It is commonly referred
to as “embarrassingly parallel”, such as calculating the value of PI, Mandelbrot set,
matrix manipulation, etc.
If an update algorithm requires some outside data, it is Semi-Global. This com-
plicates the mapping from sequential to parallel because semantics must be parsed
precisely to determine which nodes need which data. For this problem, we introduce
a new concept, “data distance” directives annotated by users to tell the compiler how
far away the data is from it. Note that the data distance here resembles the distance
vector in data dependence, but they are not the same. Our system does not parallelize
program blocks subject to any form of data dependence. For example, Jacobi itera-
tion, which updates each item with values from its neighbors, is in this category. For
this communication model, users must indicate data distance from the compiler: (−1,
−1), (−1, 1), (1, −1), and (1, 1). We leave parallelizing loops in this communication
model for future work.
The last communication model is Global in which the update algorithm requires
data from all others. All-pairs shortest-path solved by Floyd’s algorithm [24], which
A directive-based MPI code generator for Linux PC clusters 185
Fig. 2 Collective
communication functions
MPI collective communication functions can be divided into three categories:
synchronization, data movement, and global computation. Here, we discuss only
data movement functions, as shown in Fig. 2. Some say collective communication
functions are more efficient than point-to-point communication functions, but that
is not so clear-cut because collective communication implementations are some-
times related to synchronizations, some of which are redundant. Some suggest that
send/receive in parallel programs should be “considered harmful” and avoided in
favor of collective communication functions as far as possible. The benefits over
send/receive include: simplicity, programmability, performance, expressiveness, and
predictability [25].
In this section, we report on experiments conducted to evaluate the performance
of MPI collective communication data movement functions; the results were taken
into consideration in our parallelizing system.
A directive-based MPI code generator for Linux PC clusters 187
Fig. 3 Timing methodologies
and MPI_Scatter behave similarly except in data movement direction, we may use
MPI_Scatter to evaluate data movement behavior. Assume the root processor owns
p messages m0, m1, . . . ,mp−1, each of size M bytes, and each is sent to Processors
1,2, . . . , p − 1. First, Processor 0 sends mp/2, . . . ,mp−1 to Processor p/2. Next,
Processor 0 sends mp/4, . . . ,mp/2−1 to Processor p/4, and concurrently, Processor
p/2 sends messages m3p/4, . . . ,mp−1 to 3p/4. The scatter is completed by repeat-
ing log(p) steps. The total number of pieces sent is (p − 1), so the execution time is
α log(p) + (p − 1)Mβ .
MPI_Allgather can be seen as MPI_Gather where all processes receive the re-
sult along with the root. It is usually implemented to cyclically shift messages
A directive-based MPI code generator for Linux PC clusters 189
Table 5 Network transformation times
Parameters Execution time (s)
Intra-node Inter-node Ratio
-np 2 Average n0-1 Average
Data size: 10 MB; Transfer times: 1 0.24 0.24 1.05 1.05 4.3750
Data size: 10 MB; Transfer times: 2 0.43 0.22 2.02 1.01 4.5909
Data size: 10 MB; Transfer times: 4 0.81 0.20 3.97 0.99 4.9500
Data size: 10 MB; Transfer times: 8 1.56 0.20 7.84 0.98 4.9000
Data size: 100 MB; Transfer times: 1 2.38 2.38 10.39 10.39 4.3655
Data size: 100 MB; Transfer times: 2 4.18 2.09 20.16 10.08 4.8230
Data size: 100 MB; Transfer times: 4 7.98 2.00 39.69 9.92 4.9600
Data size: 100 MB; Transfer times: 8 15.52 1.94 78.73 9.84 5.0722
Table 6 Send/receive to broadcast ratios
Parameter Execution time (s)
Point-to-point Broadcast Ratio
Data size: 10 MB; processors: 2 0.21 0.21 1.0000
Data size: 10 MB; processors: 4 2.11 2.04 1.0343
Data size: 10 MB; processors: 8 2.12 2.10 1.0095
Data size: 10 MB; processors: 16 5.92 4.62 1.2814
Data size: 100 MB; processors: 2 13.53 8.36 1.6184
Data size: 100 MB; processors: 4 21.33 20.38 1.0466
Data size: 100 MB; processors: 8 59.54 47.00 1.2668
Data size: 100 MB; processors: 16 135.98 89.55 1.5185
3.4.3 Ratios of various communication functions
Here, we report on experiments in two categories: performance ratios for point-to-
point to broadcast, and to scatter. As described above, allgather can be viewed as
“broadcast after gather”, and its performance depends on the combination of these
two functions.
Table 6 shows execution times and point-to-point to collective communication
ratios for MPI_Bcast. Note that the larger the message and the more processors used,
the more obvious the effect of collective communication becomes. The experimental
results suggest that broadcasting is especially effective for large groups of processors
and large amounts of message transmissions.
Table 7 shows point-to-point to collective communication ratios for MPI_Scatter.
According to the analysis above, it has few advantages over point-to-point behavior,
and Table 8 shows scatter offers no advantage over point-to-point behavior. Perhaps
this is because LAM was implemented with synchronization in some way. Our ex-
periments show scatter provides no advantage over point-to-point collective commu-
nication.
A directive-based MPI code generator for Linux PC clusters 191
Ta
bl
e
8
M
at
rix
m
u
lti
pl
ic
at
io
n
ex
ec
u
tio
n
tim
es
Pr
ob
le
m
siz
e
Ex
ec
u
tio
n
tim
e
1
2
4
8
16
Se
nd
/R
ec
v
Co
lle
ct
iv
e
Se
n
d/
Re
cv
Co
lle
ct
iv
e
Se
n
d/
Re
cv
Co
lle
ct
iv
e
Se
n
d/
Re
cv
Co
lle
ct
iv
e
Se
n
d/
Re
cv
Co
lle
ct
iv
e
12
8
Co
m
pu
ta
tio
n
0.
07
0.
07
0.
03
0.
03
0.
02
0.
02
0.
01
0.
01
0.
01
0.
01
Co
m
m
un
ic
at
io
n
0.
00
0.
00
0.
00
0.
00
0.
02
0.
02
0.
04
0.
04
0.
09
0.
06
25
6
Co
m
pu
ta
tio
n
1.
46
1.
72
0.
79
1.
08
0.
53
0.
52
0.
27
0.
31
0.
13
0.
13
Co
m
m
un
ic
at
io
n
0.
00
0.
00
0.
01
0.
01
0.
08
0.
08
0.
42
0.
16
0.
60
0.
25
51
2
Co
m
pu
ta
tio
n
21
.4
8
22
.2
0
12
.7
9
13
.2
2
6.
40
6.
57
3.
21
3.
30
1.
61
1.
68
Co
m
m
un
ic
at
io
n
0.
00
0.
01
0.
05
0.
05
0.
31
0.
32
0.
73
0.
63
1.
65
1.
03
10
24
Co
m
pu
ta
tio
n
17
6.
80
18
2.
95
10
4.
72
10
7.
58
52
.3
5
53
.7
8
26
.2
2
26
.9
4
13
.1
2
13
.4
8
Co
m
m
un
ic
at
io
n
0.
00
0.
04
0.
20
0.
21
1.
29
1.
27
2.
99
2.
51
6.
14
4.
32
A directive-based MPI code generator for Linux PC clusters 193
Fig. 6 A pseudo source code
Table 9 Directives for parallelism
Name: / ∗ DOALL_BEGINP = XXX ∗ /
Usage: Tell the system to parallelize the following loop block. Character P stands for loop partitioning
option. The system now supports only static partitioning: BLK for block, CYC for cyclic. CSS,
GSS, FSS, TSS are reserved for future versions.
Name: /* DOALL_END */
Usage: Enclose the block parallelized with respect to DOALL_BEGIN.
Name: /* INIT_BEGIN */
Usage: Tell the system this block will be initialized for all nodes.
Name: /* INIT_END */
Usage: Enclose the block initialized with respect to INIT_BEGIN.
Name: /* SYN */
Usage: Tell the system to synchronize. Synchronization is done automatically done for parallelizable
loops, so this directive works only when a loop is not parallelized, but a user wants synchro-
nization for safety.
Name: /* DD par(X,X . . .) */
Usage: Tell the system the data distance vector for array variable par. This directive is used when the
communication model is semi-global.
4.2 System model
Our system is a directive-based MPI code generator that translates sequential source
code into parallel coding; Fig. 5 shows a skeleton diagram of our system. The sample
source program shown in Fig. 6 was fed into our system. It is a sequential C program
with the comment format directives listed in Table 9 as examples of source code. The
implementation is based on these assumptions:
• The loop is expressed by a for statement: There is much syntax for loop expres-
sion. Some technologies format while loops as for loops [28], but for simplicity
parallelizable loops should be expressed as for statements.
A directive-based MPI code generator for Linux PC clusters 195
Fig. 8 For loop preprocessing
4.3.2 Loop preparation
The preprocessor substitutes constants and formats loop iterations, especially for
statement incremental expressions. Figure 8a shows incremental expression format-
ting, and Fig. 8b formatting for incremental expressions and constant substitution.
Loop optimization encompasses many techniques, e.g., redundancy elimination,
normalization, reordering, fusion, etc. A detailed list of loop transformations is given
in [8]. We implemented loop normalization for easy and convenient subscript analy-
sis. Other transformations could be added to the loop preparation phase for further
optimization.
Loop normalization [31] converts all loops such that the induction variable is ini-
tially 0 (1 in Fortran) and is incremented by 1 during each iteration. This ensures that
the loop iteration space is regular. The transformation (algorithm shown in Fig. 9) is
very simple. Only two actions in iteration space are necessary: shifting the induction
variable to 0 and scaling the distance of each item to 1. After the boundaries have
been set, the original index is recomputed to its original value from the new index
during each iteration. The normalization produces two assignment statements on the
initial loop index. The first, at the beginning of the loop body, assigns its new index
value function to it, and the second, at the end of the loop, assigns its final value to it.
The loop body remains unchanged.
An iteration table is built during this phase. With loop normalization, loop control
variables are inserted into the table. Each item consists of a variable name, upper
boundary, and a unique loop identification number.
4.3.3 Def-use symbol table
Our system is block-oriented. Source programs are separated into blocks according
to DOALL loops bracketed by DOALL_BEGIN and DOALL_END directives. Thus,
DOALL loops are break points. Each block is indexed by a global counter that starts
at 1 and increases by 1 during each iteration. A def-use symbol table based on this is
established to analyze message-passing behavior. Each item in the table consists of
three fields: name tuple, def-chain tuple, and use-chain tuple. Table 10 gives defini-
tions for each of these.
A directive-based MPI code generator for Linux PC clusters 197
Fig. 10 Matrix multiplication code segment
Table 12 Def-use symbol table
for the example code Def-use field Name field
(c, 1) (a, 1) (b, 1)
Def-chain (1, 0, 1, 1)
(2, 1, 2, 2)
Use-chain (2, 1, 2) (2, 1, 1) (2, 1, 1)
(3, 0, 3)
• if η is new to the table, create a new item in the table, the tuple field name is (N,
A);
• if η is defined (write to η), add (B, D, S, OP) to the def-chain field (here, OP
indicates whether variable η is self-referenced);
• if η is used (read from η), add (B, D, S) to the use-chain field.
For example, the matrix multiplication code segment shown in Fig. 10 has three
blocks. When S1 is parsed, Array C is new and defined, so a new item is created with
the name field (c, 1). Because S1 is for variable initialization, the def-chain of (c, 1)
is (1, 0, 1, 1), meaning that Array C is in block 1, which is not a DOALL loop, its
sequence order is 1, and the variable is initialized for all nodes. Next comes a DOALL
loop. Iteration variables are first parsed and the information recorded in the iteration
table early in the loop preparation phase. In S6, the block index is 2 and the sequence
order is 2. Array c has self-reference on add operation and so (2, 1, 2, 2) is linked
to the (c, 1) def-chain. After parsing this code segment from S1 to S6, the def-chain
symbol table shown in Table 12 is built.
4.4 Pass two
4.4.1 Message-passing analyzer
The analyzer checks the def-use symbol table for message-passing behavior and fur-
ther communication models, performing two analyses: read-write relation analysis
and data space analysis. The read-write relation affects the demand and direction of
data movement from the master to slaves (message in: into the loop) and from slaves
A directive-based MPI code generator for Linux PC clusters 199
Fig. 11 Pattern of code
generation
Fig. 12 Static partitioning
4.4.2 Code generation
Code generation is also block-oriented, non-DOALL and noninitialization blocks,
excluding variable declaration, belong only to the master. Other parts of source pro-
grams belong to both masters and slaves. Parts of the master will be enclosed by if
(adppg_rank == 0) control flows in cooperation with an error-handling mechanism
that ensures all processes exit at the same time when one or more errors occur.
After message-passing analysis, code generation focuses on DOALL loops. The
code generated for DOALL loops is shown in Fig. 11. Control flows are not changed,
but loop partitioning and data movement do occur. A loop partitioning function that
partitions the iteration space for each node comes first. We use block and cyclic static
partitioning, described in Sect. 2. Figure 12 shows schematically how the loop parti-
tioning function partitions the iteration space. Before and after DOALL loops, there
is communication for data movement from the master to slaves and from slaves to the
master.
Communication is also necessary during each iteration if the communication
model inside a parallelizable loop is Global. After a self-referenced variable with
a slaves-to-master data movement direction has been sent, the operation stored in the
def-chain for this variable is performed in order to “collect” all data from slaves.
Table 13 shows the MPI functions used in our system. Only six primitives are
required to make an MPI program. According to the evaluation in Sect. 3, using
broadcasting is worthwhile, so we include MPI_Bcast. Collective communication
allows Global communication mode programs to exchange data for all processes.
Taking the matrix multiplication in Fig. 10 as an example, S1 is inside an ini-
tialization block, thus, it belongs to all nodes, and remains unchanged; for all nodes
each variable inside an initialization block holds its value. Next is a loop-partitioning
function and in this case the option is block. According to the message-passing an-
A directive-based MPI code generator for Linux PC clusters 201
Table 14 Matrix multiplication
execution times Processors Problem size
256 × 256 512 × 512 1024 × 1024
Sequential 1 1.494 20.819 170.530
Our system 2 1.456 13.088 102.410
4 1.382 7.616 55.337
8 1.166 4.989 31.490
16 1.355 6.223 25.161
Hand-revised 2 1.515 12.136 101.545
4 1.326 7.265 56.811
8 1.136 4.893 31.352
16 1.195 5.596 22.891
Table 15 Prime number
detection execution times Processors Problem size
1,000,000 10,000,000 100,000,000
Sequential 1 1.178 29.566 780.085
Our system 2 1.128 15.552 393.220
4 0.845 8.258 202.569
8 0.719 4.407 101.564
16 0.646 2.508 51.096
Hand-revised 2 1.128 15.619 393.854
4 0.837 8.273 202.532
8 0.724 4.405 101.579
16 0.645 2.508 51.086
5.2 Applications
Four study cases were assessed for correctness and performance. The first three were:
matrix multiplication, prime number detection, and Mandelbrot set. While they are
all “independent” communication models, they behave differently from one another.
Since they are independent, processors do not have to communicate with one an-
other while computing. The last study case is all-pairs shortest path. It is a “Global”
communication model. Each processor must access data from all other processors to
update its own data. For each case, we used three program sequencing, versions gen-
erated by our system and hand-revised. Experiments were conducted with various
numbers of iteration using various numbers of processors. A comparison between
using our system and hand-revision is also given.
Execution times for the four applications: matrix multiplication, prime number
detection, Mandelbrot set, and all-pairs shortest path, are shown in Tables 14, 15, 16,
and 17, respectively. Speedups are shown in Figs. 14 and 15.
A directive-based MPI code generator for Linux PC clusters 203
Fig. 14 Matrix multiplication and prime number detection speedups
6 Conclusions and future work
We developed the Directive-based MPI Code Generator (DMCG) for translating se-
quential C source code into parallel code using C language with MPI. We provided
A directive-based MPI code generator for Linux PC clusters 205
Table 18 Comparison of our system and hand-revised approach
Approach Time/Effort Performance Applicability
Our system Annotation required:
parallelism directives and
scheduling parameter
methods for performance
Depends completely on
program communication
model; it is excellent if
model is independent and
user tunes the code well
Cannot handle structure,
pointer, indirect array ref-
erence, and loop-carried
dependence
Hand-revised Requires extensive code
modification; time-
consuming and error-prone
Excellent when
implementation is adjusted
to problems and optimized
to parallel environments
Applicable to any code
to implement loop scheduling. Since the generated parallel codes perform nearly as
well as optimized codes, it is a good tool for speeding up solution steps and porting
current applications to parallel architectures with MPI implementation.
In the near future, we will reimplement our system, using the Stanford University
Intermediate Format (SUIF) [29] as a platform, to make it more flexible and suitable
for collaborative development. Of course, since it is a learning tool for beginners to
parallel programming, the graphical interface will be clarified and made friendlier. It
will show the relationship between sequential code and parallel code, as well as loop
transformation processing.
Other future projects relate to performance. One is introducing dynamic schedul-
ing into our system to give users more tuning options in adjusting generated codes
to their environments (homogeneous and heterogeneous). It will be a long road to
realizing this. The other concerns loop and communication optimization. There are
many approaches to loop optimization, e.g., redundancy elimination, loop reordering,
loop fusion, etc. These approaches can be added to the loop preparation phase, as de-
scribed in the design approach. For communication optimization, we will improve
send/receive behaviors for various communication models and use the technology
described in [30] to reconstruct communication behavior.
References
1. Sterling TL, Salmon J, Becker DJ, Savarese DF (1999) How to build a Beowulf: a guide to the imple-
mentation and application of PC clusters, 2nd edn. MIT Press, Cambridge
2. Wilkinson B, Allen M (1999) Parallel programming: techniques and applications using networked
workstations and parallel computers. Prentice Hall, New York
3. Buyya R (1999) High performance cluster computing: architectures and systems, vol. 1. Prentice Hall,
New York
4. Message passing interface forum. http://www.mpi-forum.org/
5. PVM—parallel virtual machine. http://www.epm.ornl.gov/pvm/
6. TOP500 supercomputer sites. http://www.top500.org
7. Wolfe M (1996) Parallelizing compilers. ACM Comput Surv 28(1):261–262
8. Wolfe M (1996) High performance compilers for parallel computing. Addison-Wesley, Reading
9. Boulet P, Darte A, Silber G-A, Vivien F (1998) Loop parallelization algorithms: from parallelism
extraction to code generation. Parallel Comput 24:421–444
10. Banerjee U (1988) An introduction to a formal theory of dependence analysis. J Supercomput
2(2):133–149
A directive-based MPI code generator for Linux PC clusters 207
Computing (NCHC), and the Ministry of Education. His present research interests are in grid and clus-
ter computing, parallel and high-performance computing, and internet-based applications. He is both a
member of the IEEE Computer Society and ACM.
Kuan-Chou Lai received his M.S. degree in computer science and in-
formation engineering from the National Cheng Kung University in
1991, and the Ph.D. degree in computer science and information en-
gineering from the National Chiao Tung University in 1996. Currently,
he is an associate professor in the Department of Computer and Infor-
mation Science and the director of the Computer and Network Center
at the National Taichung University. His research interests include par-
allel processing, heterogeneous computing, system architecture, P2P,
grid computing, and multimedia systems. He is a member of the IEEE
and the IEEE Computer Society.
same study, they also presented a HPACS system but lacked 
of management interface. 
 
B. Hadoop and HDFS 
Hadoop is one of the most salient pieces of the data 
mining renaissance which offers the ability to tackle large 
data sets in ways that weren’t previously possible due to 
time and cost constraints. It is a part of the apache software 
foundation and its being built by the community of 
contributor in all over the world. The Hadoop project 
promotes the development of open source software and 
supplies a framework for the development of highly scalable 
distributed computing applications [23]. 
Hadoop is the top-leveled project in Apache Software 
Foundation and it supports the development of open source 
software [1]. Hadoop provides a framework for developing 
highly scalable distributed applications. The developer just 
focuses on applying logic instead of processing detail of 
data sets. The HDFS (Hadoop Distributed File System) file 
system stores large files across multiple machines. It 
achieves reliability by replicating the data across multiple 
hosts, and hence does not require RAID storage on hosts. 
The HDFS file system is built from a cluster of data nodes, 
each of which serves up blocks of data over the network 
using a block protocol. They also serve the data over HTTP, 
allowing access to all content from a web browser or other 
client. Data nodes can connect to each other to rebalance 
data, to move copies around, and to keep the replication of 
data high. A file system requires one unique server, the 
name node. This is a single point of failure for an HDFS 
installation. If the name node goes down, the file system 
will be off-lined. When it comes back up, the name node 
must replay all outstanding operations. This replay process 
can take over half an hour for a big cluster [26]. 
C. Co-allocation Mechanism 
Co-allocation architecture enables parallel downloading 
from data node. It can also speed up downloads and 
overcome network faults. The architecture proposed [14] 
consists of three main components: an information service, a 
broker/co-allocator, and local storage systems. Co-allocation 
of data transfers is an extension of the basic template for 
resource management [8]. Applications specify the 
characteristics of desired data and pass attribute descriptions 
to a broker. The broker searches for available resources, and 
gets replica locations from the Information Service [7] and 
Replica Management Service [11]; then, obtains the lists of 
physical file locations. We have implemented the following 
eight co-allocation schemes: Brute-Force (Brute), History-
based (History), Conservative Load Balancing 
(Conservative), Aggressive Load Balancing (Aggressive), 
Dynamic Co-allocation with Duplicate Assignments 
(DCDA), Recursively-Adjusting Mechanism (RAM), 
Dynamic Adjustment Strategy (DAS), and Anticipative 
Recursively-Adjusting Mechanism (ARAM)[16,17,18]. 
III. SYSTEM DESIGN AND IMPLEMENTATION 
In Figure 1, we described the current overview of 
Distribution File System. The MIFAS has three HDFS 
groups. The first group, THU1 and the second group, THU2 
are both in Tunghai University. And the third group is 
CSMU in Chung Shan Medical University Hospital. All of 
the groups are under 100Mbps network bandwidth in 
TANET (Taiwan Academic Network) network 
environment. The HDFS group number can be very flexible. 
The minimum is one but the maximum can be many. The 
more HDFS group we have the more duplication source we 
get. It means that the PCAS images source is from the 
HDFS. Thus, if we increase the source number (build more 
HDFS group), the effects will definitely be different based 
on source numbers. 
 
 
Figure 1.  Overview of Distribution File System. 
A. System Architecture 
In Figure 2, MIFAS was developed on cloud 
environment. The distribution file system was built on HDFS 
of Hadoop environment (Section 2.2). This Hadoop platform 
could be described as PaaS (Platform as a Service). We 
extended a SaaS (Software as Service) based on PaaS. As the 
shown illustration, the top level of MIFAS was web-based 
interface. MIFAS provided a nice and friendly interface that 
users could easily queries the Medical Images.  
 
Figure 2.  System architecture of MIFAS 
322
TABLE II.  EXPERIMENT MEDICAL IMAGES 
Image Type 
Medical Image Attribute 
Pixel QTY. Total Size Testing Times 
CR Chest 2804*2931 1 7.1MB 500 times 
A series of 
CT 512*512 389 65MB 500 times 
 
 
Figure 5.  System workflow of MIFAS 
We tested 500 times for each site, and get the average 
time result in Figure 6.  Results in proximal site can be 
discovered to be more time-consuming in Figure 7. The 
result of Figure 6 and Figure 7 means the retrieve efficiency 
of PACS is better than MIFAS. The main reason of the result 
is that PACS was built on a High-Performance Server and is 
also a high cost Medical Image System. MIFAS is not easy 
to cross this threshold. But in experiment 2, the advantages 
of MIFAS can be displayed in the following experiment. 
 
 
Figure 6.  Image Retrieval Times Result 
 
Figure 7.  Image Retrieval Times Result in Proximal 
C. Experiment 2: Compare PACS and MIFAS with 
Proximal Failure Problem 
In this experiment, we want to compare PACS and 
MIFAS with proximal failure problem. As we addressed on 
MIFAS, the MIFAS provides a Co-allocation strategy, so 
basically the Single Site Failure issue will not affect the 
MIFAS operation. On the other hand, if the PACS system 
occurs to the same problem, the only one solution is to use 
another PACS site. In the experiment 2, we assume that 
PACS in CSMU occurs to system failure, and the Medical 
Staff must retrieval the Medical Images to another CSMU 
branch hospital. If the same situation, a HDFS group failure 
problem, occurs in MIFAS. Even the MIFAS has only two 
left HDFS groups, users still can retrieval the Medical 
Images from the others HDFS group. In this experiment, we 
assume both failures are in proximal site. This experiment 
result can be described in Figure 8, PACS in CSMU is not 
available, but the MIFAS can still work under a good 
situation. 
 
 
Figure 8.  System configuration for Experiment 2 
324
(WoGTA’05), pp. 73-80, December 2005. 
[13] C.T. Yang, C.H. Chen, K.C. Li, and C.H. Hsu, “Performance 
Analysis of Applying Replica Selection Technology for Data Grid 
Environments,” PaCT 2005, Lecture Notes in Computer Science, 
vol. 3603, pp. 278-287, Springer-Verlag, September 2005. 
[14] C.T. Yang, S.Y. Wang, and C.P. Fu, “A Dynamic Adjustment 
Mechanism for Data Transfer in Data Grids,” Network and Parallel 
Computing: IFIP International Conference, NPC 2007, Lecture 
Notes in Computer Science, vol. 4672, pp. 61-70, Springer, ISSN 
1611-3349, September 17-20, 2007. 
[15] C.T. Yang, Y.C. Chi, T.F. Han and C.H. Hsu, “Redundant Parallel 
File Transfer with Anticipative Recursively-Adjusting Scheme in 
Data Grids”, ICA3PP 2007, pp. 242–253, 2007.. 
[16] C.T. Yang, M.F. Yang, Wen-Chung Chiang “Enhancement of 
anticipative recursively adjusting mechanism for redundant parallel 
file transfer in data grids,” J Network Comput Appl (2009), 
doi:10.1016/j.jnca.2009.02.002 
[17] C.T. Yang, I.H. Yang, K.C. Li, and C.H. Hsu “A Recursively-
Adjusting Co-Allocation Scheme in Data Grid Environments,” 
ICA3PP 2005 Algorithm and Architecture for Parallel Processing, 
Lecture Notes in Computer Science, vol. 3719, pp. 40-49, Springer-
Verlag, October 2005. 
[18] C.T. Yang, I.H. Yang, K.C. Li, and S.Y. Wang, “Improvements on 
Dynamic Adjustment Mechanism in Co-Allocation Data Grid 
Environments,” The Journal of Supercomputing, Springer, vol. 40, 
no. 3, pp. 269-280, 2007. 
[19] Z. Zhou, S. S. Chao, J. Lee, B. Liu, J. Documet, H.K Huang “A 
Data Grid for Imaging-based Clinical Trials” Medical Imaging 
2007: PACS and Imaging Informatics, edited by Steven C. Horii, 
Katherine P. Andriole, Proc. Of SPIE Vol. 6516, pp. 65160U, 2007. 
[20] Chao-Tung Yang, Shih-Yu Wang, and William C. Chu, “A 
Dynamic Adjustment Strategy for Parallel File Transfer in Co-
Allocation Data Grids,” Journal of Supercomputing, Springer 
Netherlands, Article in Press, June 2009. (DOI 10.1007/s11227-
009-0307-4) 
[21] Chao-Tung Yang, Yao-Chun Chi, Ming-Feng Yang and Ching-
Hsien Hsu, “An Anticipative Recursively-Adjusting Mechanism for 
Redundant Parallel File Transfer in Data Grids,” Proceedings of the 
Thirteenth IEEE Asia-Pacific Computer Systems Architecture 
Conference (ACSAC 2008), Aug. 4-6, 2008 in Hsinchu, Taiwan. 
[22] Gopinath Ganapathy and S. Sagayaraj, "Circumventing Picture 
Archiving and Communication Systems Server with Hadoop 
Framework in Health Care Services," Journal of Social Sciences, 
vol. 6, pp. 310-314, 2010. 
http://www.scipub.org/fulltext/jss/jss63310-314.pdf  
[23] Venner, J., 2009. Pro Hadoop. 1st Edn., Apress, ISBN:13: 
9781430219439, pp: 440. 
[24] L. Faggioni, et al., "The future of PACS in healthcare enterprises," 
European Journal of Radiology, 2010. 
[25] E. Bellon, et al., "Trends in PACS architecture," European Journal 
of Radiology, 2010. 
[26] R. Grossman, et al., "Compute and storage clouds using wide area 
high performance networks," Future Generation Computer Systems, 
vol. 25, pp. 179-183, 2009. 
[27] Sutton LN. PACS and diagnostic imaging service delivery—A UK 
perspective. Eur J Radiol (2010), doi:10.1016/j.ejrad.2010.05.012. 
[28] Ganglia Monitoring System, http://ganglia.sourceforge.net/ 
 
326
A. CTM 
CTM (Close To Metal) [16] gave developers direct 
access to the native instruction set and memory of the 
massively parallel computational elements in modern AMD 
video cards. CTM was support by AMD, but AMD recently 
switched from CTM to OpenCL. 
B. OpenCL 
OpenCL (Open Computing Language) [17] is a 
framework for writing programs that execute across 
heterogeneous platforms consisting of CPUs, GPUs, and 
other processors. OpenCL provides parallel computing using 
task-based and data-based parallelism. Its architecture shares 
a range of computational interfaces with two competitors, 
NVidia's Compute Unified Device Architecture and 
Microsoft's Direct Compute, while it is adopted by 
AMD/ATI for its GPGPU technology Stream SDK. 
C. CUDA 
CUDA [10, 11, 13] (an acronym for Compute Unified 
Device Architecture) is a parallel computing architecture 
developed by NVIDIA; CUDA is the computing engine in 
NVIDIA graphics processing units or GPUs that is 
accessible to software developers through industry standard 
programming language. 
D. MPI 
Message Passing Interface (MPI) [20] is a specification 
for message passing operations. It defines each worker as a 
process. MPI is currently the de-facto standard for 
developing HPC applications on distributed memory 
architecture. It provides language bindings for C, C++, and 
FORTRAN. 
E. OpenMP 
OpenMP [1] supports multi-platform shared memory 
multiprocessing programming in C, C++ and FORTRAN on 
much architecture which including UNIX and Microsoft 
Windows platforms. It consists of a set of compiler 
directives, library routines, and environment variables that 
influence run-time behavior.  
F. Pthread 
POSIX Threads (Pthread) [18] is a POSIX standard for 
threads. The standard, POSIX.1c, Threads extensions (IEEE 
Std 1003.1c-1995), defines an API for creating and 
manipulating threads. Implementations of the API are 
available on many Unix-like POSIX systems such as 
FreeBSD, NetBSD, GNU/Linux, Mac OS X and Solaris, but 
Microsoft Windows implementations also exist. 
G. TBB 
Intel® Threading Building Blocks (Intel TBB) [19] 
offers a rich and complete approach to expressing parallelism 
in a C++ program. It is a library for writing software 
programs that to help us take advantage of multi-core 
processor performance without having to be a threading 
expert. 
III. AUTO-PARALLELISM 
When we want a code to be parallel, we must sure that 
the variables are independent each other in the loop. So auto-
parallelizing compilers try to analyze codes and 
automatically generate parallel codes. Building an auto-
parallelizing compiler has proven to be a very difficult task 
when producing efficient code in all cases for a wide variety 
of applications. Thus, auto-parallelization remains a hot 
research topic. There are some auto-parallel compilers we 
found. 
A. ROSE 
ROSE [2] is an open source compiler, and ROSE builds 
source-to-source program transformation and analysis tools 
for large-scale Fortran 77/95/2003, C, C++, OpenMP, and 
UPC applications. 
B. Cetus 
Cetus [4] is a source-to-source C compiler written in Java 
and maintained at Purdue University. Cetus provides 
automatic parallelization, and many other applications have 
emerged. Current Cetus 1.2.1 release, it provides OpenMP-
to-CUDA package.  
C. PAR4ALL 
PAR4ALL [3] is an open-source environment to do 
source-to-source transformations on C and Fortran programs 
for parallelizing, optimizing, instrumenting, reverse-
engineering, etc. on various targets, from multicore system-
on-chip with some accelerators up to high performance 
computers and GPU. 
D. Open64 
Open64 [5] derives from SGI’s MIPSPro compiler and 
from work done by Intel Corp, in conjunction with the 
Chinese Academy of Sciences, also licensed under the GNU 
Public License (GPL v2). The Open64 suite currently 
includes compilers for C, C++, and Fortran90/95 compilers 
for the IA-64 Linux ABI and API standards. 
E. Intel 
Intel® Composer XE 2011 [6] is an optimized compiler 
combined with high-performance libraries, advanced 
vectorization, support for Intel® Parallel Building Blocks 
and OpenMP*, speeding and simplifying improved multicore 
performance on multiple operating systems with the same 
code base. 
F. PGI 
PGI [14] Unified Binary™ technology simplifies cross-
platform support by combining into a single executable file, 
code sequences optimized for multi-core x64 processor 
families from Intel and AMD and GPU accelerators from 
NVIDIA. 
233
 
Figure 3.  Matrix Multiplication with size from 256 to 2048 
Figure 3 shows that the performance on processing the 
massively parallel execution as the application of Matrix 
Multiplication from size 256 to 2048. 
Figure 4 shows that using N loops to compute the answer 
of pi. But the program which is transformed by PAR4ALL is 
unstable. When N= 100000000 and 1000000000, it got the 
wrong answer. And the compilers of ROSE and Cetus also 
cannot transform the source code to the code with OpenMP 
directives successfully. 
Figure 5 and Figure 6 show that the performance 
measurements of nbody and fasta with a particular value N. 
When we compile the codes which are transformed by 
ROSE and Cetus, we got error. Although we could only 
transform successfully by using PAR4ALL, from the 
performance result in Figure 5, PAR4ALL cannot give us a 
better performance than other compilers, even giving us a 
lower performance than the program with original code. 
 
Figure 4.  pi with N from 100000000 to 10000000000 
 
Figure 5.  nbody with N from 500000 to 50000000 
 
 
Figure 6.  fasta with N from 250000 to 25000000 
When we transformed the code of fasta by those 
compilers, it failed at PAR4ALL, Cetus, and Open64. In 
Figure 6, the performance is similar. The Figure 7 shows the 
performance measurements of jacobi. It’s fatser when we use 
Intel compiler and then open64. 
 
Figure 7.  jacobi 
B. Desktop Computer (CUDA version) 
• CPU: Intel(R) Xeon(R) CPU E5410 @ 2.33GHz( 8 
cores) [15] 
• RAM: 4GB 
• GPU: Tesla C1060 
The compilers mentioned earlier, not everyone is support 
automatically translate to CUDA. PAR4ALL and PGI are 
tools that we used in the current environment, and using 
these tools to generate CUDA application runs on GPU and 
comparing with C application runs on CPU. The benchmarks 
of this part are the same with the benchmarks of part A, but 
the method of parallelize is different from part A. 
 
Figure 8.  Matrix Multiplication with size from 256 to 2048 
235
 
Figure 15.  Jacobi 
In Figure 13 to 15, the performance results are similar as 
the performance results in Part A. Finally, we make a list of 
comparisons in Figure 16. 
VII. CONCLUSIONS 
As we know, the methods of OpenMP and CUDA are 
famous in parallel programming. The performance could get 
better after the serial codes translated to the codes which 
using these two methods. But not any code is suitable to 
parallelize. 
Parallel programming is not easy to programmers. So 
these compilers which support auto-parallel can help us 
easily transform our non-parallel codes to parallel codes. 
In general computer, the compilers of Intel and PGI have 
the best performance in the version of OpenMP; and in the 
version of CUDA, the compiler of PGI has the best 
performance.  
As for embedded systems, ROSE, PAR4ALL, and Cetus 
have similar performance. From the experimental results in 
this paper, we found that only PAR4ALL could transform 
the code of nbody successfully. But it gave us a terrible 
result; the performance is much lower than original. So, the 
perfect auto-parallelizing compiler is yet to be produced. 
However, there are some cases where auto-parallelization is 
perfectly suited. 
 
Figure 16.  list of comparisons 
ACKNOWLEDGMENT 
This work is supported in part by the National Science 
Council, Taiwan R.O.C., under grants no. NSC 99-2220-E-
029-004. 
REFERENCES 
[1] Open MP Specification, http://openmp.org/wp/about-openmp/ 
[2] ROSE, http://www.rosecompiler.org/ 
[3] PAR4ALL, http://www.par4all.org/ 
[4] Cetus, http://cetus.ecn.purdue.edu/ 
[5] Open64, http://www.open64.net/ 
[6] Intel, http://software.intel.com/en-us/articles/intel-parallel-studio-xe/ 
[7] Benchmarcks, 
http://shootout.alioth.debian.org/u32q/benchmark.php?test=nbody&la
ng=gcc 
[8] Arm11MP Core 
http://www.arm.com/products/processors/classic/arm11/arm11-
mpcore.php 
[9] NVIDIA Tesla C1060 Computing Processor, 
http://www.nvidia.com/object/product_tesla_c1060_us.html 
[10] CUDA, http://en.wikipedia.org/wiki/CUDA 
[11] Download cuda, http://developer.nvidia.com/object/cuda.htm 
[12] Top 500 Super Computer Sites, What is Gflop/s, 
http://www.top500.org/faq/what_gflop_s 
[13] NVIDIA CUDA Programming Guide, 
http://developer.download.nvidia.com/compute/cuda/2_3/toolkit/docs
/NVIDIA_CUDA_Programming_Guide_2.3.pdf 
[14] The Potland Group, http://www.pgroup.com/index.htm 
[15] Quad-Core Intel® Xeon® Processor 5400 Series, 
http://www.intel.com/Assets/en_US/PDF/datasheet/318589.pdf 
[16] Close To Metal wiki, http://en.wikipedia.org/wiki/Close_to_Metal 
[17] OpenCL, http://www.khronos.org/opencl/ 
[18] POSIX Threads Programming, 
https://computing.llnl.gov/tutorials/pthreads/ 
[19] Intel® Threading Building Blocks, 
http://www.threadingbuildingblocks.org/ 
[20] MPI, http://www.mcs.anl.gov/research/projects/mpi/ 
237
504 C.-T. Yang et al. 
by this system, causes the global warming. Therefore, the virtual cluster computing 
system is a trend to save energy and protect environment in the future development. 
This paper introduces how to implement and integrate virtualization with cluster 
computing system on Xen and how it works, and, also explains and contrasts the 
differences between non-virtualization and virtualization [1-7]. The virtual cluster 
computing system is more efficient in operations and economic in power than the 
traditional cluster computing system; the virtualized cluster computing system is a 
trend in the cluster system. This virtual cluster computing system is used to deal with 
large-scale problems instead of using the transitional cluster computing system. 
The rest of this paper is stated as follows. In section 2, we will discuss the virtual 
cluster computing system in details. In section 3, we will mention our system archi-
tecture, web interface, resource broker, and experiments by using efficient virtualiza-
tion. Finally, conclusions and the future work are stated in section 4. 
2   Background Review 
A virtual Cluster Computing System is a system consisting of cluster computers con-
nected and virtualization of computing. 
2.1  Cluster Systems 
A cluster system is a group of linked computers, working together so closely to com-
plete jobs that in many respects they form a single computer. The components of a 
cluster are commonly, but not always, connected to each other through a fast LAN, 
and each of them is called a “node”. A cluster skeleton is divided into homogeneous 
and heterogeneous types. Frame clusters, based on function, can be categorized as 
three types: High Availability Clusters, Load Balancing Clusters and High Perform-
ance Computing Clusters. 
2.2   Virtualization 
Virtualization is simply the logical separation of the request for some service from 
the physical resources that actually provide that service. There are five or more vir-
tualization technologies, but we just discuss two of them, which are known by most 
people. 
In order to evaluate the viability of the difference between virtualization and non-
virtualization, the virtualization software we used in this paper is XEN. XEN is cho-
sen to be our system’s virtual machine monitor because it provides better efficiency, 
supports different operating systems simultaneously, and gives each operating system 
an independent system environment. 
3   System Implementation 
In our entire framework, a cluster computing system with XEN is a major architecture 
to achieve the economization of power.  
506 C.-T. Yang et al. 
3. Request management and booting resources 
4. Start computing 
5. Get results 
6. Use UI to download results 
 
Fig. 2. Flow of resource broker 
We define the following equation for the resource broker.  
• NodeTotalvnum: Total available virtual machines. 
• NodeRealvnum: Real virtual machines. 
• Cn: Total available virtual machines per physic node. 
• Memp: Physic memory on one physic node. 
• Memv: Virtual memory on virtual node. 
• L: License counts. 
   
Using this equation, we can  
1. Count virtual machine’s numbers. 
2. Avoid using physics swap. Performance is achieved because total virtual ma-
chine’s memory can be no larger than physics machine’s memory. 
3. Control license counts. 
3.3   Experimental Results 
We focus on economization of power and compare efficiency between a traditional 
cluster and a virtual cluster. Therefore, by using matrix multiplication, LINPACK and 
LU test sets are used to verify that a virtual cluster will economize more power and 
operate more efficiently than a traditional cluster. 
508 C.-T. Yang et al. 
 
Fig. 3. Thermal power matrix multiplication with watt hour 
References 
1. Dong, Y., Li, S., Mallick, A., Nakajima, J., Tian, K., Xu, X., Yang, F., Yu, W.: Extending 
Xen with Intel Virtualization Technology. Intel® Technology Journal 10(03), 1–14 (2006) 
2. Sharifi, M., Hassani, M., Mousavi, S.L.M., Mirtaheri, S.L.: VCE: A New Personated Vir-
tual Cluster Engine for Cluster Computing. In: 3rd International Conference on Information 
and Communication Technologies: From Theory to Applications, 2008. ICTTA 2008, April 
7-11, pp. 1–6 (2008) 
3. Liu, J., Huang, W., Abali, B., Panda, D.K.: High Performance VMM-Bypass I/O in Virtual 
Machines. In: Proceedings of USENIX 2006 (2006) 
4. Huang, W., Liu, J., Abali, B., Panda, D.K.: A Case for High Performance Computing with 
Virtual Machines. In: ICS 2006: Proceedings of the 20th annual international conference on 
Supercomputing, pp. 125–134. ACM Press, New York (2006) 
5. Menon, A., Santos, J.R., Turner, Y., Janakiraman, G., Zwaenepoel, W.: Diagnosing Per-
formance Overheads in the Xen Virtual Machine Environment. In: VEE 2005: Proceedings 
of the 1st ACM/USENIX international conference on Virtual execution environments,  
pp. 13–23. ACM Press, New York (2005) 
6. Cherkasova, L., Gardner, R.: Measuring CPU Overhead for I/O Processing in the Xen Vir-
tual Machine Monitor. In: USENIX 2005 Annual Technical Conference, pp. 387–390 
(2005), http://www.usenix.org/events/usenix05/tech/general/ 
cherkasova.html 
7. Smith, J.E., Nair, R.: The Architecture of Virtual Machines. Computer 38(5), 32–38 (2005) 
510 C.-T. Yang, J.-H. Chang, and C.-C. Wu 
shared-memory programming language. Therefore, in this paper we propose to use 
hybrid MPI and OpenMP programming mode to design the loop self-scheduling 
scheme for the cluster system with multicore computers. 
2   Background Review 
2.1   Multicore and Cluster Systems 
In a multicore processor, two or more independent cores are combined in a single 
integrated circuit. A system with n cores is most effective when presented with n or 
more concurrent threads. The degree of performance gain, resulting from use of a 
multicore processor, depends on the problem being solved and the algorithms used, as 
well as on their implementation in software. The main features allow dividing large-
scale programs into several smaller programs for parallel execution by more than one 
computer in order to reduce processing times, and include modern cluster systems that 
communicate mainly through Local Area Networks (LANs), and can be broadly di-
vided into homogeneous and heterogeneous systems. 
2.2   Loop Self-scheduling Schemes  
Self-scheduling schemes are mainly used to deal with load balancing [8] and they can 
be divided into two types: static and dynamic [6, 7]. Static scheduling schemes decide 
how many loop iterations are assigned for each processor at compile time. The  
number of processors available for distribution and the calculated dynamics of each 
machine are taken into consideration when implementing programs with static sched-
uling. The advantage of static scheduling schemes is no scheduling overhead at run-
time. 
In contrast, dynamic scheduling is more suitable for load balancing because it 
makes scheduling decisions at runtime. No estimations and predictions are required. 
Self-scheduling is a large class of adaptive and dynamic centralized loop scheduling 
schemes. Initially, a portion of the loop iterations is scheduled to all processors. As 
soon as a slave processor becomes idle after it has finished the assigned workload, it 
requests the scheduling of unscheduled iterations. 
3   Proposed Approach 
Fig.1 explains our approach. All the loop iterations are kept in the global scheduler. 
No slave cores are allowed to request iterations directly from the global scheduler. 
Instead, they have to request from the local scheduler in the same computing node. To 
utilize the feature of shared-memory architectures, every MPI process of the local 
scheduler will create OpenMP threads for each processor core on its resident comput-
ing node. The messages between the global scheduler and the local scheduler are 
inter-node communications. They are MPI messages. In contrast, the messages be-
tween the local scheduler and the processor core are intra-node communications. 
 
 
512 C.-T. Yang, J.-H. Chang, and C.-C. Wu 
 
 
Fig. 2. Network route state 
Table 1. Our cluster system configuration 
Host Processor Model #o f 
CPU 
# of 
Core 
RAM NIC OS version 
quad1 Intel™ Core™2 Quad CPU Q6600 @ 
2.40GHz 
1 4 2GB 1G 2.6.23.1-42.fc8 
quad2 Intel™ Core™2 Quad CPU Q6600 @ 
2.40GHz 
1 4 2GB 1G 2.6.23.1-42.fc8 
quad3 Intel™ Core™2 Quad CPU Q6600 @ 
2.40GHz 
1 4 2GB 1G 2.6.23.1-42.fc8 
quad4 Intel™ Core™2 Quad CPU Q6600 @ 
2.40GHz 
1 4 2GB 1G 2.6.23.1-42.fc8 
oct1 Intel(R) Xeon(R) CPU E5410 @ 2.33GHz 2 8 8G 1G 2.6.21-xen_Xen 
oct2 Intel(R) Xeon(R) CPU E5410 @ 2.33GHz 2 8 8G 1G 2.6.21-xen_Xen 
oct3 Intel(R) Xeon(R) CPU E5410 @ 2.33GHz 2 8 8G 1G 2.6.21-xen_Xen 
oct4 Intel(R) Xeon(R) CPU E5410 @ 2.33GHz 2 8 8G 1G 2.6.21-xen_Xen 
t1 Intel(R) Xeon(R) CPU E5310 @ 1.60GHz 2 8 8GB 1G 2.6.25.4-10.fc8 
t2 Intel(R) Xeon(R) CPU E5310 @ 1.60GHz 2 8 8GB 1G 2.6.25.4-10.fc8 
t3 Intel(R) Xeon(R) CPU E5410 @ 2.33GHz 2 8 8GB 1G 2.6.25.4-10.fc8 
t4 Intel(R) Xeon(R) CPU E5410 @ 2.33GHz 2 8 8GB 1G 2.6.25.4-10.fc8 
eta9 Intel(R) Xeon(R) CPU E5420 @ 2.50GHz 2 8 4G 1G 2.6.25-14.fc9 
s1 Intel(R) Xeon(R) CPU E5310 @ 1.60GHz 2 8 4GB 1G 2.6.18-128.1.6.el5 
 
4.2   Experimental Results  
In our experiments, first, the HPL measurements and CPU speed of all nodes  
were collected. Next, the impact of the parameters α, β, on performance was inves-
tigated. With this approach, a faster node will get more workloads than a slower one 
proportionally.  
In the matrix multiplicatoin experiment, we find that the proposed schemes get 
better performance when α is 40 and β is 0.4. Fig. 3 illustrates execution time of 
traditional scheme, dynamic hybrid (matn*ss3) and the proposed scheme 
(matn*ss3_omp). 
514 C.-T. Yang, J.-H. Chang, and C.-C. Wu 
In Circuit Satisfiability example, we found that the proposed schemes got better 
performance when α was 30 and β was about 0.5. Fig. 5 shows execution times  
for the proposed scheme (satn*ss3) and the proposed scheme of OpenMP 
(satn*ss3_omp). 
5   Conclusion and Future Work  
In this paper, we use the hybrid programming model MPI with OpenMP to design 
parallel loop self-scheduling schemes for heterogeneous cluster systems with multi-
core computers. We propose a heuristic scheme, which combines the advantages of 
static and dynamic loop scheduling schemes, and compare it with previous algorithms 
by experiments in this environment. In each case, our approach can obtain perform-
ance improvement on previous schemes. Furthermore, we hope to find better ways to 
model the performance functions, such as incorporating amount of memory available, 
memory access costs, network information, CPU loading, etc. Also, a theoretical 
analysis of the proposed method will be addressed. 
References 
1. HPC Challenge Benchmark, http://icl.cs.utk.edu/hpcc/  
2. Bennett, B.H., Davis, E., Kunau, T., Wren, W.: Beowulf Parallel Processing for Dynamic 
Load-balancing. In: Proceedings on IEEE Aerospace Conference, vol. 4, pp. 389–395 
(2000) 
3. Chronopoulos, A.T., Andonie, R., Benche, M., Grosu, D.: A Class of Loop Self-Scheduling 
for Heterogeneous Clusters. In: Proceedings of the 2001 IEEE International Conference on 
Cluster Computing, pp. 282–291 (2001) 
4. Hummel, S.F., Schonberg, E., Flynn, L.E.: Factoring: a method scheme for scheduling par-
allel loops. Communications of the ACM 35, 90–101 (1992) 
5. Polychronopoulos, C.D., Kuck, D.: Guided Self-Scheduling: a Practical Scheduling Scheme 
for Parallel Supercomputers. IEEE Trans. on Computers 36(12), 1425–1439 (1987) 
6. Yang, C.-T., Cheng, K.-W., Shih, W.-C.: On Development of an Efficient Parallel Loop 
Self-Scheduling for Grid Computing Environments. Parallel Computing 33(7-8), 467–487 
(2007) 
7. Yang, C.-T., Cheng, K.-W., Li, K.-C.: An Enhanced Parallel Loop Self-Scheduling Scheme 
for Cluster Environments. The Journal of Supercomputing 34(3), 315–335 (2005) 
8. Yagoubi, B., Slimani, Y.: Load Balancing Strategy in Grid Environment. Journal of Infor-
mation Technology and Applications 1(4), 285–296 (2007) 
 A Xen-Based Paravirtualization System 127 
corresponding policies.  With the improved technique of a virtual machine (VM) and 
development of hardware, it is the best solution to save energy and the consequences 
by using a VM to construct the virtual cluster computing system   
In this paper, implementing and integrating a virtualization [1-7] system with a 
cluster computing system on Xen [8-11] is well introduced.  The categories of Xen 
are listed and the reasons of choosing different types of Xen are given to judge which 
one is better for research.  The details of user interfaces, resource brokers and system 
architectures are given to illustrate each part of the work flow. 
In Section 2, we will discuss the virtual cluster computing system in details.  In 
Section 3, we will mention our system architecture, web interfaces, resource brokers, 
and experiments by using efficient virtualization.  Finally, conclusions and the future 
work are stated in Section 4.  
2   Preliminary 
In order to evaluate the viability of the difference between virtualization and non-
virtualization, the virtualization software we used in this paper is Xen.  Xen is a 
virtual machine monitor (hypervisor) that allows you to use one physical computer to 
run many virtual computers.  Xen has a combination of features that make it uniquely 
well suited for many important applications.  Xen is chosen to be our system’s virtual 
machine monitor because it provides better efficiency, supports different operating 
systems simultaneously, and gives each operating system an independent system 
environment. 
This free software is mainly divided into two kinds of simulate types, 
paravirtualization and Full virtualization.  Paravirtualization implements virtualization 
technology, mostly via the modified kernel of Linux.  In general, most virtualization 
strategies are classified into two major categories: 
z Full virtualization (also called native virtualization) is similar to emulation. As 
in emulation, unmodified operating systems and applications run inside a virtual 
machine. Full virtualization differs from emulation in that operating systems 
and applications are designed to run on the same architecture as the underlying 
physical machine. This allows a full virtualization system to run many 
instructions directly on the raw hardware.  The hypervisor in this case monitors 
access to the underlying hardware, and gives each guest operating system the 
illusion of having its own copy. It must no longer use software to simulate a 
different basic architecture. 
z Paravirtualization: In some instances, this technique is also referred to as 
enlightenment. In paravirtualization, the hypervisor exports a modified version 
of the underlying physical hardware. The exported virtual machine is of the 
same architecture, which is not necessarily the case in emulation. Instead, 
targeted modifications are introduced to make it simpler and faster to support 
multiple guest operating systems. 
The characteristic of paravirtualization is as follows:  
z A virtual machine quite like a real machine on operating efficacy 
z Supporting more than 32 cores of computer structures at most 
 A Xen-Based Paravirtualization System 129 
 
Fig. 1. Software stack without Xen 
In this paper, the cluster system is established by using four homogeneous 
computers, all with Intel Xeon E5410 CPU, 8 GB physical memory, 500 GB HD and 
the fedora 8 OS installed. A gigabit switch is used to construct the network 
environment in the cluster system. 
While the architecture of compute node in Fig. 1 represents the non-virtualization 
system, the architecture with Xen in Fig. 2 represents the virtualization system.  In the 
experiment section, virtualization system and non-virtualization system are compared 
to show the effect on computing efficiency and economizing on power. 
 
 
Fig. 2. Software stack of master node with Xen 
3.2   User Interface and Resource Broker 
3.2.1   End User’s Web Page  
We design a useful web interface for end users to submit their jobs, and for 
administrators to manage their VM templates and host machines.  Easy to submit and 
easy to get result are very important for end users.  We design a single page for all of 
the steps when they want to submit their jobs. In a single page, they can choose VM 
types, number of VM processors, the size of memory, and how many virtual machines 
they want to use. At the same page, they can also upload their job files (include 
Makefile, source code and etc.). After they upload their job files, this interface will 
compile their job files automatically. More details for the end users’ web pages are 
shown on Fig. 3, 4, 5, 6, and 7. 
Linux OS (Fedora)
NFS NIS MPI
Compute Node
System Service
Linux OS (Fedora)
XEN
NFS NIS MPI
Compute Node
System Service
 A Xen-Based Paravirtualization System 131 
 
Fig. 6. Virtual machine lists 
 
Fig. 7. File upload and result 
3.2.2    Resource Broker  
A resource broker is the most important part of the whole system because it manages 
resources, jobs and licenses.  To combine the resource broker with a web portal, it lets 
end users easily use the system to complete their jobs.  The work flows among end 
users, user interfaces and resource brokers are given in Fig. 8 and listed as follows: 
• Step 1. End users use UI to upload source code. 
• Step 2. The system call GetAvailableResource() to gather available resources for 
end users. 
• Step 3. The resource broker assigns and utilizes resources for end users. 
• Step 4. Start the computation progress of jobs for end users. 
• Step 5. The computational system returns results. 
• Step 6. End users receive results through UI. 
In order to count the number of virtual machines, it avoids physical swap and control 
licenses. Variables are defined in the following for the equation (1) of total available 
virtual machines: 
• NodeTotalvnum: Total available virtual machines. 
• Cn: Total available virtual machines per physic node. 
 A Xen-Based Paravirtualization System 133 
computes the QR and singular value decompositions of rectangular matrices, 
and applies them to least-squares problems. LINPACK uses column-oriented 
algorithms to increase efficiency by preserving locality of reference.  
z LU (Lower-Upper Triangular), demands that the number of CPU must be 
power of 2, and majors in calculating for degree of particle of non-linear 
communication.  
z Primes is a test program give the parameter that an integer any number than 
itself will find out how many primes below this number.  
 
Since an Intel Xeon E5410 cpu needs 80 watts to work, it requires 160 watts to work 
for each machine.  Therefore it requires 640 watts for the traditional cluster with four 
machines, but only 160 watts for the virtualized system with only one machine.  Table 
1 provides the information of thermal power of cluster on power consumption by 
performing matrix multiplication.  The detail information shows that the virtualization 
system has great improvement compared with physical machines.  
Table 1. Thermal energy of physical and virtualization (unit: watt hour) 
Array Size Physical Virtualization 
32 0.000475 0.000092828 
64 0.00091 0.000184 
128 0.001878 0.000633 
256 0.006573 0.00185 
512 0.038035 0.012013 
1024 0.477288 0.353829 
2048 32.99902 7.764905 
4096 252.4356 58.53319 
About efficiency, LU, LINPACK and Primes test sets are used to prove our 
presumption that virtualization is better than traditional cluster. The result of 
LINPACK sets in Fig. 9 has shown that while array size is large than 645, the 
efficiency of physical cluster is worse than Virtualizations. 
 
Fig. 9. LINPACK sets 
 A Xen-Based Paravirtualization System 135 
submit their jobs on the system, and is established with a resource broker.  The 
experiments show that the virtualization system consumes less power and provides 
comparable or better computing efficiency than a non-virtualization system.  The 
future work of this system is to improve the resource broker and build more functions 
for end users. 
Acknowledgement 
The work of this paper was supported by National Science Council of Taiwan under 
Grant number NSC-98-2220-E-029-004. 
References 
1. Cherkasova, L., Gardner, R.: Measuring CPU Overhead for I/O Processing in the Xen 
Virtual Machine Monitor. In: Proceedings of the annual conference on USENIX Annual 
Technical Conference, pp. 387–390. USENIX Press, California (2005) 
2. Dong, Y., Li, S., Mallick, A., Nakajima, J., Tian, K., Xu, X., Yang, F., Yu, W.: Extending 
Xen with Intel Virtualization Technology. Intel Technology Journal 10(3), 1–14 (2006) 
3. Huang, W., Liu, J., Abali, B., Panda, D.K.: A Case for High Performance Computing with 
Virtual Machines. In: Proceedings of the 20th annual international conference on 
Supercomputing, pp. 125–134. ACM Press, New York (2006) 
4. Liu, J., Huang, W., Abali, B., Panda, D.K.: High Performance VMM-Bypass I/O in Virtual 
Machines. In: Proceedings of the annual conference on USENIX ’06 Annual Technical 
Conference, p. 3. USENIX Press, California (2006) 
5. Menon, A., Santos, J.R., Turner, Y., Janakiraman, G., Zwaenepoel, W.: Diagnosing 
Performance Overheads in the Xen Virtual Machine Environment. In: VEE ’05: 
Proceedings of the 1st ACM/USENIX international conference on Virtual execution 
environments, pp. 13–23. ACM Press, New York (2005) 
6. Sharifi, M., Hassani, M., Mousavi, S.L.M., Mirtaheri, S.L.: VCE: A New Personated 
Virtual Cluster Engine for Cluster Computing. In: 3rd IEEE International Conference on 
Information and Communication Technologies: from Theory to Applications, pp. 7–11 
(2008) 
7. Smith, J.E., Nair, R.: The Architecture of Virtual Machines. Computer 38(5), 32–38 
(2005) 
8. Barham, P., Dragovic, B., Fraser, K., Hand, S., Harris, T., Ho, A., Neugebauer, R., Pratt, 
I., Warfield, A.: Xen and the Art of Virtualization. In: Proceedings of the nineteenth ACM 
symposium on Operating systems principles, pp. 164–177. ACM Press, New York (2003) 
9. Emeneker, W., Jackson, D., Butikofer, J., Stanzione, D.: Dynamic Virtual Clustering with 
Xen and Moab. In: Min, G., Di Martino, B., Yang, L.T., Guo, M., Rünger, G. (eds.) ISPA 
Workshops 2006. LNCS, vol. 4331, pp. 440–451. Springer, Heidelberg (2006) 
10. Emeneker, W., Stanzione, D.: HPC Cluster Readiness of Xen and User Mode Linux.  
In: 2006 IEEE International Conference on Cluster Computing, pp. 1–8. IEEE Press,  
New York (2006) 
11. Vallee, G., Scott, S.L.: OSCAR Testing with Xen. In: 20th International Symposium on 
High-Performance Computing in an Advanced Collaborative Environment, p. 43. IEEE 
Press, New York (2006) 
Tyng-Yeu Liang) 
Implementation of a Medical Image File Accessing System on Cloud 
Computing 於 12月 12日上午之 Session E14中報告 
 
CSE 會議規模中等，第一天早上報到之後，每天上下午時段各
有 5個 Session在進行加上四場 Keynote約有百多人與會。這顯示出
了這個會議的重要性以及其在國際會議中的指標性地位，並且吸引
大量學者專家競相在此會議投稿發表論文。由今年論文的投稿總數
以及發表數量可見，目前世界各地學者在 Computational Science and 
Engineering, Parallel和Distributed Computing和Networking相關研究
的相關研究仍然持續地在創新以及進展中。 
 
    參與過程中其中有不少學者專家來至台灣各個學府，看的出台
灣學術在這一些領域有所專長與發展，也有許多中國及香港當地的
學者與會，雖然大家來自世界各地，但透過英文，大家在資訊界上
的經驗還是可以有所交流。參加這次會議，提供了自己一個擴展眼
界與學習的機會。 
考察參
觀活動
(無是
項活動
者省
略) 
無 
建議 聽了許多國外及國內學者於會中所發表之研究成果，我發現其
實台灣在硬體及嵌入式系統這一領域之成果是相當傑出的，若我們
能就語言暨表達能力再做訓練提升，將能在國際上提高能見度，並
使我們的研究成果能影響與幫助其他同領域的學者。 
 
參與國際會議可以讓國內的教師或研究人員了解目前國際上相
關研究的發展現況與未來趨勢，也可以拓展其國際視野，對於國內
的教師或研究人員幫助相當大，因此國內教師或相關人員可以多多
參與國際會議，了解國際上相關產業的走向。若是國內學術單位有
爭取到在國內舉辦大型國際會議的機會，不僅能促進國際學術交流
與合作，提昇國家國際能見度。 
攜回資
料名稱
及內容 
1. CSE 2010 conference program 一本 
2. CSE 2010 13th IEEE International Conference on Computational 
Science and Engineering CD一片 
 
  
 
 
Program at a Glance Page 1-3 
Keynote Speakers  Page 4-7 
Sessions and Papers in CSE-10 Page 8-9 
Sessions and Papers in EUC-10 Page 10-11 
Sessions and Papers in TrustCom-10 Page 12-14 
Sessions and Papers in EUC-10 WORKSHOPS Page 15-16 
  Sessions and Papers in SEC-10 and WMSC-10   Page 17-18 
Organizing and Program Committees Page 19-36 
 
 
 
 
 
 
 
 
 
TABLE OF CONTENTS 
 2
December 12 (Sunday) 
08:15-09:05 Keynote 3: Finding the Real Source of Internet Crimes by Wanlei Zhou (Chiang Chen Studio Theatre) 
09:05-10:00 Keynote 4: Trends in Post-Petascale Computing by Mitshuhisa Sato (Chiang Chen Studio Theatre) 
10:00-10:20 Coffee/Tea Break  
10:20-12:15 
CSE-10 
Session A14 
(Chiang Chen Studio Theatre) 
 
EUC-10  
Session B14 
(Room M104) 
TrustCom-10 
Session C14 
(Room M108) 
EUC10-Workshop  
Session D14 
(Room M111) 
SEC-10  
Session E14 
(Room M110) 
12:15-13:45 Lunch at Color Crystal Restaurant 
13:45-15:30 
CSE-10   
Session A15 
(Lecture Theatre N002)   
 
EUC-10  
Session B15 
(Room M104) 
TrustCom-10 
Session C15 
(Room M108) 
EUC10-Workshop  
Session D15 
(Room M111) 
SEC-10  
Session E15 
(Room M110) 
15:30-15:50 Coffee/Tea Break  
15:50-17:20 
CSE-10   
Session A16 
(Lecture Theatre N002)  
 
EUC-10  
Session B16 
(Room M104) 
 
TrustCom-10 
Session C16 
(Room M108) 
EUC10-Workshop  
Session D16 
(Room M111) 
SEC-10  
Session E16 
(Room M110) 
18:00-22:30 Conference Banquet, 6 pm - 10:30 pm, at Hung Kee Restaurant 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 4
 
The 8th IEEE/IFIP International Conference on Embedded and Ubiquitous 
Computing (EUC-10) 
Keynote: Context Sensing for Ubiquitous Computing 
Prof. Hans Gellersen, Lancaster University, UK 
  
About the keynote speaker 
 
Hans Gellersen is a Professor of Interactive Systems in the School of 
Computing and Communications at Lancaster University, UK. He has a 
long-standing track record in ubiquitous computing where his work is focused 
on context sensing, location systems and user interface technologies. Hans 
has led European research collaborations on topics such as smart objects 
and relative positioning and is widely cited for work on augmentation of 
everyday objects, multi-modal sensing of context, and novel devices for 
interaction. His recent work includes research on eye movement as a context 
for ubiquitous computing, ad hoc location systems for use in emergency 
response, and sensor-based device authentication. Hans is closely involved 
with the International Conference on Ubiquitous Computing, which he founded in 1999, and the 
International Conference on Pervasive Computing, for which he served as Chair of the Steering 
Committee from 2007-2009. He is also an Editorial Board Member of IEEE Pervasive Computing 
Magazine, and an Editor of the Journal on Personal and Ubiquitous Computing. Hans holds a 
PhD in Computer Science from the University of Karlsruhe, Germany. 
 
Summary:  
Whereas digital worlds were once seen as something largely separate from the everyday physical 
world, they are now increasingly interwoven. Context sensing has become a central feature of 
mobile and ubiquitous computing systems, employing sensors to observe real world activity, and 
data analysis to extract contextual information. This talk briefly reflects on how context sensing 
has evolved in ubiquitous computing. It then discusses research projects at Lancaster that, in very 
different ways, employ embedded sensing and analysis of movement: of devices, for the 
purposes of secure pairing; of users, for tracking in unknown environments; and of people's eyes, 
for activity inference. 
 
 
 
 
 
 
 
 
 
 
 
 6
The 2010 International Symposium on Trusted Computing and 
Communications (TrustCom-10) 
Keynote: Finding the Real Source of Internet Crimes 
Prof. Wanlei Zhou, Deakin University, Australia 
  
About the keynote speaker 
 
Wanlei Zhou received the B. Eng (Computer Science and Engineering) and 
M. Eng (Computer Science and Engineering) degrees from Harbin Institute 
of Technology, Harbin, China, in 1982 and 1984, respectively, and the Ph.D. 
degree from the Australian National University, Canberra, Australia, in 
1991. He also received a DS.c. degree (a higher Doctorate degree) from 
Deakin University in 2002 for his substantial contribution to knowledge and 
authoritative standing in the field of distributed computing. He is currently 
the Chair Professor in Information Technology and Head of School, School 
of Information Technology, Deakin University. Before joining Deakin 
University, he has been a system programmer in HP at Massachusetts, 
USA; a lecturer in Monash University, Melbourne, Australia; and a lecturer in National University 
of Singapore, Singapore. His research interests include theory and practical issues of building 
distributed systems, security and reliability of computer networks, bioinformatics, and 
e-learning.Professor Zhou has published more than 200 papers in refereed international journals 
and refereed international conferences proceedings. He has also edited 5 books and authored 1 
book. He has also chaired a number of international conferences. He is also a Senior Member of 
the IEEE. 
 
Summary:  
Internet crimes can result in serious consequences such as disrupting critical infrastructure; 
causing significant financial losses; and threatening public life. Although a number of 
countermeasures and legislations against Internet crimes are developed, the crimes are still on 
the rise. One critical reason is that researchers and law enforcement agencies still can not answer 
a simple question easily: who and where is the real source of Internet crimes? With the support of 
a number of Australian Research Council grants, my research group has developed effective 
ways to discover the real source of Internet crimes. In this talk, I will try to explain how we 
achieveed this research goal through traceback schemes that we have developed. This talk will 
be based on the following two papers: 
1. Shui Yu, Wanlei Zhou, Robin Doss, and Weijia Jia, "Traceback of DDoS Attacks using 
Entropy Variations", Accepted by IEEE Transactions on Parallel and Distributed Systems, 
accepted 09/2009. Published online 30 Apr. 2010,  
http://doi.ieeecomputersociety.org/10.1109/TPDS.2010.97. 
2. Yang Xiang, Wanlei Zhou, and Minyi Guo, "Flexible Deterministic Packet Marking: An IP 
Traceback System to Find the Real Source of Attacks", IEEE Transactions on Parallel and 
Distributed Systems, vol. 20, no. 4, pp. 567-580, April 2009. 
 
 
 
 
 8
A. SESSIONS AND PAPERS IN CSE-10 
The 2010 IEEE International Conference on Computational Science and Engineering 
(CSE-10)  
Keynote: Trends in Post-petascale computing (Chair: Hai Jiang) 
  Prof. Mitsuhisa Sato, University of Tsukuba and RIKEN AICS, Japan    
Session A11: Mobile Computing and Wireless Communications I (Chair: Xingang Liu) 
  Adaptive Data Compression in Wireless Body Sensor Networks 
   Kun Hua, Honggang Wang, Wei Wang, and Shaoen Wu 
  Modeling and Prolonging Techniques on Operational Lifetime of Wireless Sensor Networks 
   Fuu-Cheng Jiang, Chao-Tung Yang, Shih-Meng Teng, Hsiang-Wei Wu, and Yi-Ju Chiang 
  Performance Evaluation of an Adaptive Congestion Avoidance Algorithm for IEEE 802.15.4 
   Ki-Chul Noh, Seung-Yeon Lee, Youn-Soon Shin, Kang-Woo Lee, Jong-Suk Ahn 
  Augmented Reality System Design and Scenario Study for Location-Based Adaptive Mobile Learning 
      William Chang and Qing Tan 
Session A12: Cluster, Grid and Cloud Computing (Chair: Kuan-Chou Lai) 
  ColorCom2: A Transparent Co-located Virtual Machine Communication Mechanism 
   Liang Zhang, Yuebin Bai, Ming Liu, and Hanwen Xu 
  QAFT: A QoS-Aware Fault-Tolerant Scheduling Algorithm for Real-Time Tasks in Heterogeneous Systems 
   Xiaomin Zhu, Jianghan Zhu, Manhao Ma, and Dishan Qiu 
  A Semi-structured Overlay for Multi-attribute Range Queries in Cloud Computing 
   You-Fu Yu and Kuan-Chou Lai 
 
Session A13: CSE Applications I (Chair: Joseph Tan) 
  Revealing Feasibility of FMM on ASIC: Efficient Implementation of N-Body Problem on FPGA 
   Zhe Zheng, Yongxin Zhu, Xu Wang, Zhiqiang Que, Tian Huang, Xiaojing Yin, Hui Wang, Guoguang Rong, and   
      Meikang Qiu 
  A Hybrid Harmony Search Method Based on OBL 
   X. Z. Gao, X. Wang, and S. J. Ovaska 
  Gateway to Quality Living for the Elderly: Charting an Innovative Approach to Evidence-Based E-Health    
     Technologies for Serving the Chronically Ill 
   Joseph Tan, Patrick C. K. Hung, Michael Dohan, Thomas Trojer, Matthias Farwick, and Jayshiro Toshiro 
 
Session A14: Distributed and Parallel Computing II (Chair: Yuanquan Zhang) 
  Implementing Parallel LU Factorization with Pipelining on a MultiCore Using OpenMP 
   Panagiotis D. Michailidis and Konstantinos G. Margaritis 
  A Locality-Aware Publish/Subscribe Scheme for High Level Architecture on Structured Peer-to-Peer Networks 
   Wei-Chao Chang, Shih-Hsiang Lo, Kuan-Chou Lai, Kuan-Ching Li, and Yeh-Ching Chung 
  LogGPH: A Parallel Computational Model with Hierarchical Communication Awareness 
   Liang Yuan, Yunquan Zhang, Yuxin Tang, Li Rao, and Xiangzheng Sun 
    DCMTs: Supporting Dynamically Created Migratory Threads 
      Yueting Zhu, Wu Zhang, Hai Jiang, Yu Lei, and Junjie Peng 
 
Session A15: CSE Applications II (Chair: Xingang Liu) 
  Minimizing Thermal Disparities during Placement in 3D ICs 
   Prasun Ghosal, Hafizur Rahaman, and Parthasarathi Dasgupta 
  GPU-RMAP: Accelerating Short-Read Mapping on Graphics Processors 
   Ashwin M. Aji, Liqing Zhang, and Wu-chun Feng 
  Medical Image Retrieval with Query-Dependent Feature Fusion Based on One-Class SVM 
   Yonggang Huang, Jun Zhang, Yongwang Zhao, and Dianfu Ma 
 Intelligent Mode Decision Procedure for MVC Inter-view Frame 
   Xingang Liu, Kwanghoon Sohn, Laurence T. Yang, and Wei Zhu 
Session A16: Database and Data Mining (Chair: Jinjun Chen) 
  Time-Series Classification Based on Individualised Error Prediction 
   Krisztian Buza, Alexandros Nanopoulos, and Lars Schmidt-Thieme 
  Research on Stage Classification of Flight Parameter Based on PTSVM 
   Hui Lu and Kefei Mao 
  CORER: A New Rule Generator Classifier 
 10
B. SESSIONS AND PAPERS IN EUC-10 
The 8th IEEE/IFIP International Conference on Embedded and Ubiquitous Computing 
(EUC-10)  
Keynote: Context Sensing for Ubiquitous Computing (Chair: TBA) 
  Prof. Hans Gellersen, Lancaster University, UK    
Keynote: Multi-touch HCI: A hardware and software co-evolution (Chair: TBA)   
Prof. Chia Shen, Harvard University, USA 
Session B11: Embedded Systems Software and Optimization (Chair: TBA) 
 User-Level Network Protocol Stacks for Automotive Infotainment Systems 
   Mu-Youl Lee, Hyun-Wook Jin 
 Replay Debugging for Multi-threaded Embedded Software 
   Yann-Hang Lee, Young Wn Song, Rohit Girme, Sagar Zaveri, Yan Chen 
 Real-time Enhancement for Xen Hypervisor 
   Peijie Yu, Mingyuan Xia, Qian Lin, Min Zhu, Shang Gao, Zhengwei Qi, Kai Chen, Haibing Guan 
 Schedule Swapping: A Technique for Temperature Management of Distributed Embedded Systems   
   Farzad Samie Ghahfarokhi, Alireza Ejlali 
 
Session B12: Embedded Systems and Hardware/Software Co-Design I (Chair: TBA) 
 Optimized Schedule Synthesis under Real-Time Constraints for the Dynamic Segment of FlexRay   
   Reinhard Schneider, Unmesh Bordoloi, Dip Goswami, Samarjit Chakraborty 
 Optimizing Runtime Reconfiguration Decisions 
   Thilo Pionteck, Steffen Sammann, Carsten Albrecht 
 Architectural Support for Reducing Parallel Processing Overhead in an Embedded Multiprocessor 
   Jian Wang, Joar Sohl, Dake Liu 
 Trading Conditional Execution for More Registers on ARM Processors 
  Huang-Jia Cheng, Yuan-Shin Hwang, Rong-Guey Chang, Cheng-Wei Chen 
 
Session B13: Embedded Systems and Hardware/Software Co-Design II (Chair: TBA) 
 An Environment for Design Software and Hardware Aspects of Clock Synchronization and Communication in DRTES 
   Brahim Hamid, Adel Ziani 
 Implementation of a Floating Point Adder and Subtracter in NoGAP, a Comparative Case Study 
   Per Karlström, Wenbia Zhou, Dake Liu 
 Co-Simulation of Self-Adaptive Automotive Embedded Systems 
   Marc Zeller, Gereon Weiss, Dirk Eilers, Rudi Knorr 
 
Session B14: Mobile and Context-aware Computing (Chair: TBA) 
GEDS: GPU Execution of Continuous Queries on Spatio-Temporal Data Streams 
  Jonathan Cazalas, Ratan Guha 
Rule-based Approach for Context Inconsistency Management in Ubiquitous Computing 
  Yong-jae Lee, Jaehyoung Lim, Soon J. Hyun, Dongman Lee 
Transferring Ontologies between Mobile Devices and Knowledge-based Systems 
  Xiang Su, Jukka Riekki 
Middleware Support for Context-awareness in Asynchronous Pervasive Computing Environments   
  Jianping Yu, Yu Huang, Jiannong Cao, Xianping Tao 
 
Session B15: Middleware for Ubiquitous and Autonomous Computing (Chair: TBA) 
Empirical Evaluation of Content-based Pub/Sub Systems over Cloud Infrastructure 
  Biao Zhang, Beihong Jin, Haibiao Chen, Ziyuan Qin 
A Reflective Service Gateway for Integrating Evolvable Sensor-Actuator Networks with Pervasive Infrastructure 
  Seong Hoon Kim, Daeyoung Kim, Jeong Seok Kang, Hong Seong Park 
Handling Mobility on a QoS-Aware Service-based Framework for Mobile Systems 
  Joel Gonçalves, Luis Lino Ferreira, Luis Miguel Pinho, Guilherme Silva 
Trust Measurement Methods in Organic Computing Systems by Direct Observation 
  Rolf Kiefhaber, Benjamin Satzger, Julia Schmitt, Michael Roth, Theo Ungerer 
 
 
Session B16: Cyber-Physical Systems and the Internet of Things (Chair: TBA) 
An Application Framework for Loosely Coupled Networked Cyber-Physical Systems 
  Minyoung Kim, Mark-Oliver Stehr, Jinwoo Kim, Soonhoi Ha 
 12
C. SESSIONS AND PAPERS IN TRUSTCOM-10 
The 2010 International Symposium on Trusted Computing and Communications 
(TrustCom-10)  
 
  Keynote: Finding the Real Source of Internet Crimes 
Prof. Wanlei Zhou, Deakin University, Australia 
  
Session C11: Trust Management and Evaluation 
M-Trust: A Trust Management Scheme for Mobile P2P Networks 
Basit Qureshi, Geyong Min, Demetres Kouvatsos 
A Dynamic Trust Establishment and Management Framework for Wireless Sensor Networks 
Junqi Zhang, Rajan Shankaran, Mehmet A. Orgun, Vijay Varadharajan, Abdul Sattar 
Trusted Risk Evaluation and Attribute Analysis in Ad-Hoc Networks Security 
Mechanism Based on Projection Pursuit Principal Component Analysis 
Jihang Ye, Mengyao Liu, Cai Fu 
Trust Management in Wireless Mobile Networks with Cooperative Communications 
Reyhaneh Changiz, Hassan Halabian, F. Richard Yu, Ioannis Lambadaris, Helen Tang 
CredibilityRank: A Framework for Design and Evaluation of Rank-based Credibility Models for Web Applications 
Sara Guimarães, Arlei Silva, Wagner Meira Jr, Adriano Pereira. 
Node Trust Assessment in Mobile Ad Hoc Networks Based on Multi-dimensional Fuzzy Decision Making 
Zhang Feng, Jia Zhiping, Li Xin, Xia Hui 
The Deficit and Dynamics of Trust 
Abhaya Nayak 
 
Session C12: Attack Detection 
On the Security of Non-Linear HB (NLHB) Protocol against Passive Attack 
Mohammad Reza Sohizadeh Abyaneh 
Detecting Security Attacks in Trusted Virtual Domains 
Udaya Kiran Tupakula, Vijay Varadharajan 
PDVDS: A Pattern-Driven Software Vulnerability Detection System 
Shaoyin Cheng, Jinding Wang, Jiajie Wang, Jun Yang, Fan Jiang 
Detection and Prevention of Routing Intrusions on Mobile Ad Hoc Networks 
Pradeep Moradiya, Srinivas Sampalli 
Monitoring Library Function-based Intrusion Prevention System with Continuing Execution Mechanism 
Yudai Kato, Yuji Makimoto, Hironori Shirai, Hiromi Shimizu, Yusuke Furuya, Shoichi Saito,Hiroshi Matsuo 
On the Sender Cover Traffic Countermeasure against an Improved Statistical Disclosure Attack 
Rajiv Bagai, Huabo Lu, Bin Tang 
Passive Worm and Malware Detection in Peer-to-Peer Networks 
Sahar Fahimian, Amirvala Movahed, Mehdi Kharrazi 
 
Session C13: Authentication and Digital Signature 
Combined Authentication and Quality of Service in Cooperative Communication Networks 
Ramya Ramamoorthy, F. Richard Yu, Helen Tang, Peter Mason 
Hierarchical Certificateless Signatures 
Lei Zhang, Qianhong Wu, Josep Domingo-Ferrer, Bo Qin 
A Novel Secure Bilinear Pairing Based Remote User Authentication Scheme with Smart Card 
Majid Bayat, Mohammad Sabzinejad, Amirvala Movahed 
Efficient RFID Authentication Scheme for Supply Chain Applications 
Fei Bi, Yi Mu 
Trust Based Authentication for Secure Communication in Cognitive Radio Networks 
Sazia Parvin, Song Han, Biming Tian, Farookh Kadeer Hussain 
 
Session C14: Cryptography and Security Protocols 
A Scalable Encryption Scheme for Multi-Privileged Group Communications 
Qiushuang Du, Guojun Wang, Qin Liu 
Side-Channel Resistance Evaluation of a Neural Network Based Lightweight Cryptography Scheme 
Marc Stoettinger, Sorin A. Huss, Sascha Muehlbach, Andreas Koch 
Security of the TCG Privacy-CA Solution 
Liqun Chen, Bogdan Warinschi 
 14
 
Session C19: Trust Model, Platform and Framework 
A Cloud Architecture of Virtual Trusted Platform Modules 
Dongxi Liu, Jack Lee, Julian Jang, Surya Nepal, John Zic 
Trusted Computing Platform in Your Pocket 
Surya Nepal, John Zic, Dongxi Liu, Julian Jang 
A Distributed Trust Model for Securing Mobile Ad Hoc Networks 
Pushpita Chatterjee, I. Sengupta, S. K. Ghosh 
OST: A Transaction Based Online Social Trust Model for Social Network and File Sharing Security 
Ming Li, Bonti Alessio, Wanlei Zhou 
Analysis of Property Based Attestation in Trusted platforms 
Aarthi Nagarajan, Vijay Varadharajan, Michael Hitchens 
PBTrust: A Priority-Based Trust Model for Service Selection in General Service-Oriented Environments 
Xing Su, Minjie Zhang, Yi Mu, Kwang Mong Sim 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 16
Claudio Venezia and Marco Marengo 
When Ambient Intelligence Meets TCP/IP Protocol Stack: User Layer Design  
Yu Lu, Mehul Motani, and Wong Wai-Choong 
 
Session D17: SeNAmI – 3 Cluster and Ambient Intelligence (Chair: TBA) 
Repair Policies of Coverage Holes Based Dynamic Node Activation in Wireless Sensor Networks  
Xiao-heng Deng, Chu-gui Xu, Fu-yao Zhao, and Yi Liu 
An Efficient Clustering Algorithm using Evolutionary HMM in Wireless Sensor Networks  
    Rouhollah Goudarzi, Behrouz Jedari, and Masud Sabaei 
Two-Tier Cluster Based Routing Protocol for Wireless Sensor Networks  
Asif U. Khattak, Ghalib A. Shah, and M. Ahsan 
 
Third International Workshop on Wireless Network Algorithm and Theory (WiNA 2010) 
Session D18: WiNA – 1 Secured and Efficient Sensor Networks (Chair: TBA) 
A Deterministic Key Management Scheme for Securing Cluster-Based Sensors Networks  
Mandicou BA, Ibrahima Niang, Bamba Gueye, and Thomas Noel  
 A fast heuristic for solving the D1EC coloring problem  
Fabio Campoccia and Vincenzo Mancuso 
 Energy Balanced Routing Strategy in Wireless Sensor Networks  
Xiaoguang Zhang and Zhang Da Wu 
 
Session D19: WiNA – 2 Interference, Coverage and Power Control (Chair: TBA) 
Interference Minimization in Wireless Networks  
Hakob Aslanyan and Jose Rolim 
 Avoidance of Blind Period for Node Coverage Grouping on Wireless Sensor Networks  
Chow-Sing Lin and Chih-Chung Chen  
 Predictive Power Control for Mobile Wireless Networks with Time-Varying Delay  
Cunwu Han, Dehui Sun, Zhijun Li, and Mingyue Zhao 
 
2010 International workshop on Challenged Network Architecture (ICNA-10) 
Session D20: ICNA10-1 Mobility and Routing (Chair: TBA) 
RWPAD: A Mobility Model for DTN Routing Performance Evaluation  
Yong Wang, Wei Peng, Xilong Mao, and Zhenghu Gong 
Uncertainty Reasoning on Fuzziness and Randomness in Challenged Networks  
Yang Li, Jing Wang, Yaowen Yuan, Xiumei Fan, and Qian He 
 Spray and Routing for Message Delivery in Challenged Networks  
Wanrong Yu, Chunqing Wu, and Xiaofeng Hu 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 18
Session E18: WMSC – 2 Quality of Services for Workflow Execution (Chair: Jianxun Liu) 
 A DMC Based Dynamically Self-Configuration Service Process Engine 
   Xuezhi Chai and Jian Cao 
 Research on the Mechanism of Tourism Information Change Management Based on ECA Rules 
   Tao Hu and BaoHong Li 
 An Algorithm on the Mining of Batch Processing Process 
   Yiping Wen, Jianxun Liu, and Zhigang Chen 
 
Session E19: WMSC – 3 Scientific Computing in the Cloud (Chair: Jian Cao) 
 A Taxonomy for the Analysis of Scientific Workflow Faults 
   Marco Lackovic, Domenico Talia, Rafael Tolosana-Calasanz, José A. Bañares, and Omer F. Rana 
 Persistent Locality Management of Scientific Application Workflows 
   Lamine Aouad, Tahar Kechadi, and Serge Petiton 
 Discovering Batch Processing Area from Workflow Logs 
   Yiping Wen, Jianxun Liu, and Zhigang Chen 
 A Dynamic Self-Adaptive Trust Model for P2P E-Commerce System 
   Jie Wang, Miaomiao Li, Yang Yu, and Zhenguang Huang 
 
Session E20: WMSC – 4 Cloud Workflow Applications (Chair: Wanchun Dou) 
 A Framework: Workflow-Based Social Network Discovery and Analysis 
   Jihye Song, Minjoon Kim, Haksung Kim, and Kwanghoon Kim 
 Cost-Aware Virtual USB Drive: Providing Cost-Effective Block I/O Management Commercial Cloud Storage for  
     Mobile Devices 
   Young Jin Nam, Young Kyun Park, Jong Tae Lee, and Fredrick Ishengoma 
 Bridging to the Cloud: Solution Design Trends Helping "Legacy" Systems Leverage Cloud Computing 
   Brian D. Goodman and Ran SH. Zhou 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 20
Steering Committee 
Laurence T. Yang (Chair), St. Francis Xavier University, Canada 
Hai-xiang Lin, Delft University of Technology, Netherlands 
Kuan-Ching Li, Providence University, Taiwan 
Rodrigo F. de Mello, University of São Paulo, Brazil 
 
Workshop Chairs  
Ligang He, University of Warwick, UK 
Kuan-Chou Lai, National Taichung University, Taiwan 
 
Panel Chair 
Yeh-Ching Chung, National Tsing Hua University, Taiwan 
 
Local Chair 
Alvin Chan, Hong Kong Polytechnic University, China 
 
Web and Submission Chair 
Andy Yongwen Pan, St. Francis Xavier University, Canada 
 
Program Committee 
 
Intelligent and Bio-inspired Computing 
Soon Chung, Wright State University, USA  
Dakopoulos Dimitrios, Wright State University, USA 
Praveen Kakumanu, Meridian Group, Inc., USA 
Raghudeep Kannavara, Intel Corporation, USA 
Alexandros Karargyris, National Institutes of Health (NIH), USA 
Alexandros Pantelopoulos, Wright State University, USA 
Athanasios Tsitsoulis, Wright State University, USA 
 Xiaojie Wang, Beijing University of Posts and Telecommunications, China 
Ming Yang, Marquette University, USA 
 
Distributed and Parallel Computing 
Erik Elmroth, Umea University, Sweden 
Jing He, Georgia State University, USA 
Michael Hobbs, Deakin University, Australia 
Thilo Kielmann, Vrije University Amsterdam, Netherlands 
Kuan-Chou Lai, National Taichung University, Taiwan 
Mitsuhisa Sato, University of Tsukuba, Japan 
Wen-Chung Shih, Asia University, Taiwan 
Lei Shu, Osaka University, Japan 
Lizhe Wang, Indiana University, USA 
Chao-Chin Wu, National Changhua University of Education, Taiwan 
Feng Xia, Dalian University of Technology, China 
Mehmet Yildiz, IBM, Australia  
 
Database and Data Mining 
Alberto Abello, Polytechnical University of Catalunya, Spain 
Ajith Abraham, Norwegian University of Science and Technology, Norway 
Luca Becchetti, University La Sapienza of Rome, Italy 
Ladjel Bellatreche, LISI-ENSA and University of Poitiers, France 
Bettina Berendt, Catholic University of Leuven, Belgium 
Christos Bouras, University of Patras, Greece 
Elena Camossi, Joint Research Centre ISPRA, Italy 
Barbara Catania, University of Genova, Italy 
Jerome Darmont, University of Lyon 2, France 
Antonios Deligiannakis, Technical University of Crete, Greece 
Curtis Dyreson, Utah State University, USA 
Eric Gregoire, CRIL-CNRS and University of Artois, France 
 22
CSE Applications 
David E. Bernholdt, Oak Ridge National Lab, USA 
Jee-Gong Chang, National Center for HPC, Taiwan 
Wenguang Chen, Tsinghua University, China 
Kuo Ning Chiang, National Center for HPC, Taiwan 
Jung Hong Chuang, National Chiao Tung University, Taiwan 
Jose Alfredo F. Costa, Universidade Federal do Rio Grande do Norte, Brazil 
Georgios Goumas, National Technical University of Athens, Greece 
Kuo-Chan Huang, National Taichung University, Taiwan 
Hai Jin, Huazhong University of Science & Technology, China 
Boo Cheong Khoo, National University of Singapore, Singapore 
Ming-Hsien Lee, Tamkang University, Taiwan 
Xingang Liu, Yonsei University, Korea 
Kengo Nakajima, University of Tokyo, Japan 
Thomas Rauber, University Bayreuth, Germany 
Sheng-De Wang, National Taiwan University, Taiwan 
I-Chen Wu, National Chiao-Tung University, Taiwan 
Shi-Jim Yen, National Dong Hwa University, Taiwan 
 
Scientific and Engineering Computing 
Martin Buecker, RWTH Aachen University, Germany 
Zizhong Chen, Colorado School of Mines, USA 
Robert van Engelen, Florida State University, USA 
Jose-Ramon Herrero, Universitat Politecnica de Catalunya (UPC), Spain 
Kornilios Kourtis, National Technical University of Athens, Greece 
Heshan Lin, Virginia Polytechnic Institute and State University, USA 
Dimitrios Nikolopoulos, FORTH-ICS & University of Crete, Greece 
Thomas Rauber, University of Bayreuth, Germany 
Gudula Ruenger, Chemnitz University of Technology, Germany 
Chau-Wen Tseng, University of Maryland at College Park, USA 
Xuemin Tu, University of California at Berkeley, USA 
  
Embedded and Ubiquitous Computing 
Joseph Arul, Fu Jen Catholic University, Taiwan 
Sung Woo Chung, Korea University, Korea 
Jiman Hong, Soongsil University, Korea 
Wei-Lien Hsu, AMD Inc., USA 
Chia-Tien Dan Lo, Southern Polytechnic State University, USA 
Thomas Nolte, Malardalen University, Sweden 
Chien-Min Ou, Chin Yun University, Taiwan 
Hiren D. Patel, University of Waterloo, Canada 
Shanq-Jang Ruan, National Taiwan University of Science and Technology, Taiwan 
Feilong Tang, Shanghai Jiao Tong University, China 
Russell Tessier, University of Massachusetts at Amherst, USA 
Hiroyuki Tomiyama, Ritsumeikan University, Japan 
Qixin Wang, Hong Kong Polytechnic University, Hong Kong 
Yi-Chong Zeng, Academia Sinica, Taiwan 
 Wei Zhang, Nanyang Technological University, Singapore 
 
Advanced Networking and Applications 
Jimin Chen, Zhejiang University, China 
Hua Fang, University of Massachusetts Medical School, USA 
Wen-Chen Hu, University of North Dakota, USA 
Kun Hua, Lawrence Technological University, USA 
Gang Kou, University of Electronic Science and Technology of China, China 
Haiyan Luo, University of Nebraska-Lincoln, USA 
Lei Shu, Osaka University, Japan 
Wei Wang, South Dakota State University, USA 
Dalei Wu, University of Nebraska-Lincoln, USA 
 24
Ke Wang, Jilin University, China 
Xiaokun Zhang, Athabasca University, Canada 
  
 
Additional Reviewers    
 
 
Heithem Abbes   
David Allenotor 
Yanmeng Ba 
Anupam Bhattacharjee 
Toon Calders 
Che-Wei Chang 
Dongsheng Che 
Jianwen Chen 
Rick Chen 
Rui Chen 
Jaeyoung Choi 
Thilan Costa 
Jörg Dümmler 
Robert Dew 
Rubing Duan 
Nandan Garg 
Jiaqi Ge 
Guan Haibing 
Ralf Hoffmann 
Yueh-Min Huang 
Kitae Jeong 
Girish Jha 
Samia Kouki 
Kevin Kreiser 
Raphael Kunis 
Mohsen Laabidi 
Fenglien Lee 
Mun-Kyu Lee 
Rong-Jhang Liao 
Yu-Chia Lin 
Luis David Lopez 
Hao Luan 
Glenn Luecke 
Richard Mcclatchey 
Jose Maria Monteiro 
Chandima Nadungodage 
Mariusz Pajer 
Heng Qi 
Bahaa Saify 
Chris Sung 
Andy Tyrrell 
Bo Wang 
Tongyuan Wang 
Wenhua Wang 
Hiroshi Watanabe 
Eriko Werbet 
Jufeng Xu 
Bo Yang 
Chuan-Yue Yang 
Yanli Yu 
Sharrukh Zaman 
Lianyu Zhao 
Zhipeng Zhao 
Cheng Zhu 
Zongliang Zhuansu 
 
 
 
 
 26
 
Steering Chairs 
Minyi Guo, Shanghai Jiaotong University, China 
Laurence T. Yang, St Francis Xavier University, Canada 
 
Publicity Chairs 
Habib Ammari, Hofstra University, USA  
Julien Bourgeois, University of Franche-Comte, France  
Weigang Wu, Sun Yat-Sen University, China 
 
Local Organizing Chair 
Alvin Chan, Hong Kong Polytechnic University, China 
 
Program Committee 
 
Embedded Systems Software and Optimization 
Xiaoying Bai, Tsinghua University, China 
Francois Bodin, IRISA, France 
Peng-Sheng Chen, National Chung Cheng University, Taiwan 
Stephen A. Edwards, Columbia University, USA 
Chin-Yu Huang, National Tsing-Hua University, Taiwan 
Tohru Ishihara, Kyushu University, Japan 
Roy D.C. Ju, AMD, USA 
Jaejin Lee, Seoul National University, Korea 
Shih-Wei Liao, Google Inc., USA 
Haibo (Jason) Lin, IBM China Research Laboratory, China 
YungChia Lin, MediaTek, Taiwan 
Ruey-Liang Ma, ITRI ICL, Taiwan 
Lawrence Rauchwerger, Texas A&M University, USA 
Jean Shann, National Chiao Tung University, Taiwan 
Jingling Xue, University of New South Wales, Australia 
 
Embedded Systems and Hardware/Software Co-Design 
Marco Domenico Santambrogio, Politecnico di Milano, Italy 
Rabi N. Mahapatra, Texas A&M University, USA 
Maziar Goudarzi, Sharif University of Technology, Iran 
Sheng-De Wang, National Taiwan University, Taiwan 
Alex Doboli State, University of New York at Stony Brook, USA 
Gul N. Khan, Ryerson University, Canada 
Trong-Yen Lee, National Taipei University of Technology, Taiwan 
Samarjit Chakraborty, Technical University of Munich, Germany 
Shuvra S. Bhattacharyya, University of Maryland, USA 
Stefan Kowalewski, RWTH Aachen University, Germany 
Neeraj Suri, Technische Universitat Darmstadt, Germany 
Masahiro Fujita, University of Tokyo, Japan 
Peter Balog, University of Applied Sciences Technikum Wien, Austria 
Bernhard Rinner, Technical University of Graz, Austria 
Ana Sokolova, University of Salzburg, Austria 
Anshul Kumar, Indian Institute of Technology at Delhi, India 
Paul Pettersson, Mälardalen University University, Sweden 
 
Cyber-Physical Systems and the Internet of Things 
Zhu Han, University of Houston, USA 
Tie Liu, Texas A&M University, USA 
Yu Wang, University of North Carolina at Charlotte, USA 
Lei Ying, Iowa State University, USA 
Shuguang Cui, Texas A&M University, USA 
Yu Cheng, Illinois Institute of Technology, USA 
Linda Xie, University of North Carolina at Charlotte, USA 
Yanchao Zhang, New Jersey Institute of Technology, USA 
 28
Justin Lipman, Intel Shanghai, China 
Stefan Ruehrup, OFFIS, Oldenburg, Germany 
Min-Te Sun, National Central University, Taiwan 
Natalija Vlajic, York University, Canada 
Qin Xin, Norway, Simula School of Research and Innovation, Norway 
Chih-Wei Yi, National Chiao-Tung University, Taiwan 
Jianhui Zhang, Hangzhou DianZi University, China 
Yanmin Zhu, Shanghai Jiaotong University, China 
 
Sensor Networks 
Qing Cao (Charles), University of Tennessee, USA 
Christian Decker, University of Karlsruhe, Germany 
Murat Demirbas, SUNY Buffalo, USA 
Lin Gu, Hong University of Science and Technology, China 
Xiaoxia Huang, Shenzhen Institute of Advance Technology, China 
Polly Huang, National Taiwan University, China 
Sudha Krishnamurthy, United Technologies Research Center (UTRC), USA 
Xenofon D. Koutsoukos, Vanderbilt University, USA 
Leo Selavo, University of Latvia, Latvia 
Radu Stoleru, Texas A&M University, USA 
Yoshito Tobe, Tokyo Denki University, Japan 
Guoliang Xing, Michigan State University, USA 
Ai Chen, Shenzhen Institute of Advance Technology, China 
Yu Gu, Singapore University of Technology and Design, Singapore 
Lin Shan, Temple University, USA 
Chi Yin Chow, City University of Hong Kong, China 
 
Mobile and Context-aware Computing 
Jalal Al-Muhtadi, King Saud University, Saudi Arabia 
Markus Endler, Pontificia Universaidade Catolica do Rio de Janeiro, Brazil 
Jorg Hahner, Leibniz Universitat Hannover, Germany 
Marcus Handte, University of Duisburg-Essen and Fraunhofer IAIS, Germany 
Achilles Kameas, University of Patras, Greece 
Brent Lagesse, Oak Ridge National Laboratory, USA 
David Levine, University of Texas at Arlington, USA 
Daniela Nicklas, University of Oldenburg, Germany 
Jorg Ott, Helsinki University of Technology, Finland 
Justin Mazzola Paluska, MIT, USA 
Hubert Pham, MIT, USA 
Daniele Riboni, University of Milano, Italy 
Massimo Valla, Telecom Italia, Italy 
 
Agent and Distributed Computing 
Hongen Lu, La Trobe University, Australia 
Omer Rana, University of Cardiff, UK 
Arkady Zaslavsky, Lulea University, Sweden 
Stefan Poslad, University of London, UK 
Bao Vo, Swinburne University, Australia 
Xiaolong Jin, University of Bradford, UK 
Ichiro Satoh, National Institute of Informatics, Japan 
Rajiv Ranjan, University of New South Wales, Australia 
Evaggelia Pitoura, University of Ioannina, Greece 
 
Middleware for Embedded and Ubiquitous Computing 
Kwan-Wu Chin, University of Wollongong, Australia 
Mario Di Fransesco, The University of Texas at Arlington, USA 
Tao Gu, University of Southern Denmark, Denmark 
Bin Guo, Institut Telecom SudParis, France 
Mi Jeom Kim, Korea Teelecom, South Korea 
Brent Lagesse, Oak Ridge National Labs, USA 
 30
TrustCom-10 Organizing and Program Committees 
 
 
TrustCom-10 Steering Chairs 
Guojun Wang, Central South University, China 
Laurence T. Yang, St. Francis Xavier University, Canada 
 
TrustCom-10 General Chairs 
Jie Wu, Temple University, USA 
Srinivas Sampalli, Dalhousie University, Canada 
 
TrustCom-10 Program Chairs 
Helen Y. Tang, Defence Research & Development Canada–Ottawa, Canada  
Xinwen Fu, University of Massachusetts Lowell, USA 
 
Program Committee  
Adel Cherif, Qatar University, Qatar 
Akashi Aggarwal, University of Windsor, Canada 
Avinash Srinivasan, Bloomsburg University, USA 
Benjamin Fabian, Humboldt-University of Berlin, Germany 
Bin Lu, West Chester University of Pennsylvania, USA 
Bin Tang, Wichita State University, USA 
Bin Xiao, Hong Kong Polytechnic University, Hong Kong 
Chin-Tser Huang, University of South Carolina, USA 
Christian Callegari, University of Pisa, Italy 
Cliff Zou, University of Central Florida, USA 
Constantinos Lambrinoudakis, University of Piraeus, Greece 
David Chadwick, University of Kent, UK 
Dengguo Feng, Institute of Software, Chinese Academy of Sciences, China 
Dong Seong Kim, Duke University, USA 
Dong Xiang, Tsinghua University, China 
El-Sayed El-Alfy, King Fahd University of Petroleum and Minerals, Saudi Arabia 
Emmanuelle Anceaume, IRISA, France 
Fang Qi, Central South University, China 
Farid Nait-Abdesselam, University of Lille, France 
Gang Li, Institute of Computing Technology, Chinese Academy of Sciences, China 
Gaocai Wang, Guangxi University, China 
Gianluigi Me, University of Rome "Tor Vergata", Italy 
Gregorio Martinez, University of Murcia (UMU), Spain 
Hai Jiang, Arkansas State University, USA 
Ioannis Broustis, Alcatel-Lucent, USA 
Jean-Marc Seigneur, University of Geneva, Switzerland 
Jemal Abawajy, Deakin University, Australia 
Jian Ren, Michigan State University East Lansing, USA 
Jian Yu, University of Adelaide, Australia 
Jianxun Liu, Hunan University of Science and Technology, China 
Jin-Hee Cho, U.S. Army Research Laboratory, USA 
Jinhua Xiong, Institute of Computing Technology, Chinese Academy of Sciences, China 
Jinzhao Wu, Beijing Jiaotong University, China 
Jun Shen, University of Wollongong, Australia 
Justin Zhan, Carnegie Mellon CyLab, Japan 
Kaiqi Xiong, North Carolina State University, USA 
Kenji Saito, Keio University, Japan 
Khaled Salah, King Fahd University of Petroleum & Minerals, Saudi Arabia 
Maoyu Wang, Communication Research Center Canada, Canada 
Marco Aiello, University of Groningen, the Netherlands 
Marten J. Van Sinderen, Universty of Twente, the Netherlands 
Meng Yu, Western Illinois University, USA 
Mohamed Hamdi, University of Carthage, Tunisia 
 32
UUWSN 2010 Organizing and Program Committees 
 
Steering Chairs 
Laurence T. Yang, St Francis Xavier University, Canada 
Changhwa Kim, Gangnung-Wonju National University, Korea 
 
General Chair 
Soo-Hyun Park, Kookmin University, Korea 
 
Program Chair 
Young-Sik Jeong, Wonkwang University, Korea 
Shengli Zhou, University of Connecticut, US 
 
Publication Chairs 
Sangkyung Kim, Gangnung-Wonju National University, Korea  
Hideki Tode, Osaka Prefecture University, Japan 
 
Web Chair 
Sung-joon Park, Gangnung-Wonju National University, Korea  
 
Program Committee 
Incheon Paik,  University of Aizu, Japan 
Toshiaki Miyazaki, University of Aizu, Japan 
Arno Puder,  San Francisco State University, US 
Cheng-Zhong Xu,  Wayne State University, US 
Zhou, Xiaobo,  University of Colorado at Colorado Springs, US 
Petr Hnetynka Charles,  University in Prague, Czech Republic 
Kiman Kim,  Korea Maritime University, Korea 
Seong-Dong Kim,  KETI, Korea 
Jason Choong,  Daintree Ltd., US 
Kiseon Kim, GIST, Korea 
Ho-Shin Cho, Kyungpook National University, Korea 
Wenyi Zhang, University of Southern California, US 
Michael B. Porter,  Heat, Light, and Sound Research, Inc. US 
Gaotao Shi, Tianjin University , China 
Yantai Shu, Tianjin University, China 
Xiaomei Xu, Xiamen University, China 
Zhongwen Guo, Ocean University of China, China 
Dale Green, Teledyne Benthos, US 
Milica Stojanovic,  Northeastern University, US 
Sunshin An,  Korea University, Korea 
Urbashi Mitra, University of Southern California, US 
Hoonki Kim,  Dongyang Technical College, Korea 
Jun-Hong Cui,  University of Connecticut, US 
Humut Hlavacs,  University of Vienna, Austria 
M.Rasit Eskicioglu,  University of Manitoba, Canada 
Lorenzo Rossi,  Istituto Italiano di Tecnologia, Italy 
Shibata Naoki,  Shiga University, Japan 
Jianhua Ma,  Hosei University, Japan 
Weidong Yi,  Graduate University of Chinese Academy of Science, China 
Zaihan Jiang,  Naval Research Laboratory, USA 
C.Srimathi,  VIT University, India 
 
 
 
 
 
 
 
 34
WiNA 2010 Organizing and Program Committees 
 
 
Program Chair 
Chang Wu Yu (James), Chung Hua University, Taiwan 
 
Program Committee  
Yusun Chang, Georgia Institute of Technology, USA 
Xiuzhen (Susan) Cheng, George Washington University, USA 
Kuang-Hui Chi, National Yunlin University, Taiwan 
Han-Chieh Chao, National Ilan University, Taiwan 
Naveen Chilamkurti, La Trobe University, Australia 
Shengming Jiang, South China University of Technology, China 
GuoJun Dai, HanZhou DianZi University, China 
Song Guo, University of Aizu, Japan 
Shiow-Fen Hwang, Feng Chia University, Taiwan 
Ivan Stojmenovic, University of Ottawa, Canada 
A.P Sathish Kumar, PSG Institute of Advanced Studies, India 
Fan Li, Beijing Institute of Tech., China 
Fei Li, George Mason University, USA 
MinMing Li, HongKong City University, Hong Kong 
Xu Li, INRIA, France 
Benyuan Liu, Umass Lowell, USA 
Chuan-Ming Liu, National Taipei University of Technology, Taiwan 
Jia Liu, Ohio State University, USA  
Kami Makki, Lamar University, USA 
Manki Min, South Dakota State University, USA 
Yong Qi, Xi’An JiaoTong University, China 
Kui Ren, Illinois Institute of Technology, USA 
Lei SHU, Osaka University, Japan 
Min Song, Old Dominion University, USA 
Jian Tang, Montana State University, USA 
Binod Vaidya, University of Beira Interior, Portugal 
Jie Wang, Univ. of Massechusetts Lowell, USA 
Weichao Wang, Univ. of North Carolina at Charlotte, USA 
Qin Xin, Simula, Norway 
Xinqing Yan, North China University of Water Resource and Electric Power, China  
Boting Yang, University of Regina, Canada 
Li-Hsing Yen, National University of Kaohsiung, Taiwan  
Alex Zelikovsky, Georgia State University, USA 
 
 
 
 
 
 
 
 
 
 
 
 36
WMSC-10 Organizing and Program Committees 
 
General Chair 
Rajkumar Buyya, University of Melbourne, Australia 
 
Program Chairs 
Jianxun Liu, Hunan University of Science and Technology, China 
Jinjun Chen, Swinburne University of Technology, Australia 
 
Program Committee 
Gagan Agrawal, Ohio State University, USA 
Danilo Ardagna, Politecnico di Milano, Italy 
Atta Badii, University of Reading, UK 
Shawn Bowers, University of California at Davis, USA 
Christoph Bussler, Merced Systems, Inc., USA 
Jian Cao, Shanghai Jiaotong University, China 
Massimo Cafaro, University of Salento, Italy 
Peter Dadam, University Ulm, Germany 
Wanchun Dou, Nanjing University, China 
Schahram Dustdar, Vienna University of Technology, Austria 
Yushun Fan, Tsinghua University, China 
Geoffrey Fox, Indiana University, USA 
Joerg Haehner, University of Hannover, Germany 
Ken Hawick, Massey University, New Zealand 
Robert C. H. Hsu, Chung Hua University, Taiwan 
Michel Hurfin, INRIA, France 
Marta Indulska, University of Queensland, Australia 
Qing Li, City University of Hong Kong, China 
Shiyong Lu, Wayne State University, USA 
Xiangfeng Luo, Shanghai University, China 
Zongwei Luo, University of Hong Kong, China 
Dan C. Marinescu, University of Central Florida, USA 
Jose A. Montenegro, Universidad de Malaga, Spain 
Ethan L. Miller, University of California, USA 
Helen Paik, University of New South Wales, Australia 
Cesare Pautasso, University of Lugano, Switzerland 
Sabri Pllana, University of Vienna, Austria 
Radu Prodan, University of Innsbruck, Austria 
David De Roure, University of Southampton, UK 
Michael Sheng, University of Adelaide, Australia 
Wei Tan, University of Chicago, USA 
Paul de Vrieze, Bournemouth University, UK 
Jiacun Wang, Monmouth University, USA 
Jianwu Wang, San Diego Supercomputer Center, USA 
Martijn Warnier, Delft University of Technology, Netherlands 
Maggie Minhong Wang, Hong Kong University, China 
Lai Xu, Bournemouth University, UK 
Yang Yu, Sun Yat-sen University, China 
 
Implementation of a Medical Image File Accessing System on Cloud 
Computing 
 
Chao-Tung Yang       Lung-Teng Chen      Wei-Li Chou       Kuan-Chieh Wang 
 
High-Performance Computing Laboratory 
Department of Computer Science, Tunghai University 
Taichung, 40704, Taiwan R.O.C 
ctyang@thu.edu.tw, g97357002@thu.edu.tw, g98357002@thu.edu.tw, g98357006@thu.edu.tw 
 
 
Abstract—Large scale cluster based on cloud technologies has 
been widely used in many areas, including the data center and 
cloud computing environment. The purpose of presenting the 
research paper in this field was to solve the challenge in 
Medical Image exchanging, storing and sharing issues of EMR 
(Electronic Medical Record). In recent years, many countries 
invested significant resources on the projects of EMR topics. 
The benefit of the EMR included: Patient-centered Care, 
Collaborative Teams, Evidence-based Care, Redesigned 
Business Processes, Relevant Data Capture and Analysis and 
Timely Feedback and Education.  For instance, the ARRA-
HIT project in Untied States (2011 - 2015), Health Infoway 
project in Canada (2001 - 2015) and NHIP project in Taiwan, 
etc. Aim to the topic of EMR, we presented a system called 
MIFAS (Medical Image File Accessing System) to solve the 
exchanging, storing and sharing on Medical Images of crossing 
the different hospitals issues. Through this system we can 
enhance efficiency of sharing information between patients and 
their caregivers. Furthermore, the system can make the best-
possible patient-care decisions.  
Keywords- EMR, PACS, Hadoop, HDFS, Co-allocation, 
Cloud Computing 
I.  INTRODUCTION 
EMR is an acronym from Electronic Medical Records. 
This refers to a paperless, digital and computerized system of 
maintaining patient data. It is designed to increase the 
efficiency and reduce documentation errors by streamlining 
the process. Implementing EMR is a complex, expensive 
investment that has created a demand for Healthcare IT 
professionals and accounts for a growing segment of the 
healthcare workforce [6]. 
An electronic medical record (EMR) is a computerized 
medical record created in an organization that delivers care, 
such as a hospital and doctors’ surgery. Electronic medical 
records tend to be a part of a local stand-alone health 
information system that allows storage, retrieval and 
modification of records. According to the Medical Records 
Institute, the Electronic Medical Record can be described as 
five stages which are Automated Medical Record, 
Computerized Medical Record Provider-based, Electronic 
Medical Record, Electronic Patient Record and Electronic 
Health Record [6]. This paper tries to solve the issues in 
Electronic Medical Records which are exchanging, storing 
and sharing in Medical Images. 
Based on the “Medical Images Exchange” issue of EMR, 
we presented a system called MIFAS (Medical Image File 
Accessing System) in this paper; it was built on the Hadoop 
[1] platform to solve the exchanging, storing, sharing issues 
in Medical Images. In this paper, we also presented a new 
strategy for processing medical image inspecting, which was 
co-allocation mechanism for cloud environment. We utilized 
the Hadoop platform and Co-allocation mechanism to 
establish the Cloud environment for MIFAS. MIFAS could 
easily help users to retrieve, share and store Medical Images 
between different hospitals. The remainder of this paper was 
organized as following. Background review and studies were 
presented in Section 2. The System Architecture was 
introduced in Section 3. Experimental results were presented 
in Section 4. Finally, Section 6 concluded this article. 
II. BACKGROUND 
A. Challenge in Medical Image Exchanging 
For over a decade, the majority of all hospital and private 
radiology practices have transformed from film-based image 
management systems to a fully digital (filmless and 
paperless) environment but subtly dissimilar (in concept 
only) to convert from a paper medical chart to an HER. Film 
and film libraries have given ways to modern picture 
archiving and communication systems (PACS). And they 
offer highly redundant archives that tightly integrate with 
historical patient metadata derived from the radiology 
information system. These systems may be not only more 
efficient than film and paper but also more secure as they 
incorporate with safeguards to limit access and sophisticate 
auditing systems to track the scanned data. However, 
although radiologists are in favor of efficient access to the 
comprehensive imaging records of our patients within our 
facilities, we ostensibly have no reliable methods to discover 
or obtain access to similar records which might be stored 
elsewhere [24, 25, 27]. 
According to our research, there were few Medical Image 
implementations on cloud environment. However, a familiar 
research presented the benefits of Medical Images on cloud 
were: Scalability, Cost effective and Replication [22]. In the 
2010 13th IEEE International Conference on Computational Science and Engineering
978-0-7695-4323-9/10 $26.00 © 2010 IEEE
DOI 10.1109/CSE.2010.48
321
Middleware: There was a mechanism to handle the 
transmission issue in MIFAS. In this research, we called the 
mechanism as MIFAS Middleware. The Middleware’s 
purpose is to assign/acquire the best transmission path to 
distribution file system. This Middleware also collected 
necessary information such as bandwidth between server and 
server, the server utilization rate, and network efficiency. 
The information provided entirety MIFAS Co-allocation 
Distribution Files System to determine the best solution of 
downloading allocation jobs (Figure 3). 
Information Service: To obtained analysis in the status 
of host. The Middleware of MIFAS had a mechanism to 
fetch the information of hosts called Information Service. In 
this research, we installed the Ganglia [28] in each member 
of Hadoop node to get the real-time state from all 
members.  Therefore, we could get the best strategy of 
transmission data from Information Service which is one of 
the components of MIFAS Middleware. 
Co-allocation: As our researched into section 2.3, Co-
allocation mechanism could conquest the parallel 
downloading from data nodes.  Besides, it also sped up 
downloading and solved network faults problems. 
Replication Location Service: In this research, we built 
three groups of HDFS in different locations, and each HDFS 
owned an amount of data nodes. The Replication Location 
Service means that the Service would automatically make 
duplication from private cloud to one another when medical 
images uploaded to MIFAS. 
B. System Workflow 
In Figure 3, it shows our efforts on MIFAS. In this 
research, we also made a real system to achieve our thesis. 
The system’s workflow shows in the shown illustration. 
Firstly, users input username and password to authenticate. 
Secondly, users could input search condition to query 
patients’ information. Thirdly, users could also view 
patients’ Medical Images. Fourthly, users can configure in 
MIFAS. Fifthly, if users can present MIFAS downing 
mechanism, it means the Middleware is workable in MIFAS.  
 
Figure 3.  System workflow of MIFAS 
IV. EXPERIMENTAL ENVIRONMENTS AND RESULTS 
A. Experimental Environments 
In this section, we compared MIFAS with PACS. The 
PACS system in Chung Shan Medical University Hospital is 
shown in Figure 4. It is a production PACS in CSMU and 
there are total three PACS systems in CSMU, CSUM Chung 
Kang Branch Hospital and CSUM Tai-Yuan Branch 
Hospital. Each PACS system has a synchronization 
mechanism. The network bandwidth between each PACS is 
under 100Mbps. 
 
 
Figure 4.  System workflow of MIFAS 
TABLE I.  BANDWIDTH OF PACS IN CSMU 
End-to-End Transmission Rates (Mbps) of PACS in CSMU 
Node From Node To Bandwidth 
CSMU Chung Kang Branch 100 Mbps 
CSMU Tai-Yuan Branch 100 Mbps 
Chung Kang Branch Tai-Yuan Branch 100 Mbps 
 
B. Experiment 1:  Compare Images Retrieval Times from 
PCSA and MIFAS 
In this experiment, we gave same Medical Images as 
Table 1 in PCAS and MIFAS. In a general situation we can 
describe the PACS vs. MIFAS in Figure 5. The illustration 
shows the experiment 1 environment. All the nodes are 
under a good situation, then we download the same files 
from each PACS and MIFAS. The purpose of this 
experiment is to compare the images Retrieval Times from 
PCAS and MIFAS. 
 
 
 
323
D. Hardware Broken in Both Environments 
In Figure 9, we describe a hardware broken state in both 
systems. Obviously, only one PACS is left in CSUM Tai-
Yuan branch but a strong contrast could be seen in MIFAS. 
And MIFAS still supports the medical images to transfer to 
users under a Co-allocation mechanism. In the other word, 
the benefit of MIFAS is that it conquests the network / 
hardware faults.  
PACS has its own synchronization mechanism. 
However, if there is a hardware or network broken in PACS, 
the only way is to use the survival site and try to reduce the 
resume time. The PACS system also has its limitation on 
concurrent user numbers. Compare to the MIFAS, it can 
distribute the workload from users accessing under the Co-
allocation mechanism. This section shows MIFAS can 
effectively reduce the single site fault, problems of broken 
network and hardware.  
 
Figure 9.  Hardware broken in both environments 
V. CONCLUSIONS 
MIFAS is a flexible, stable and reliable system and 
proves Medical Images sharing, storing and exchanging 
issues is available under our thesis. Therefore, Medical 
Images can easily share cross different hospitals. MIFAS 
offers advantages as below:  
Scalability: Always extend the nodes as per requirement. 
Adding a node to the network is as simple as hooking a 
Linux box to the network and copying few configuration 
files. Also, Hadoop provides details about the available space 
in the Cluster. Therefore, according to this report, one can 
decide on adding a node or not.  
Cost effective: Since the Linux nodes are always cheap; 
there is no need to invest much on the hardware as well as 
OS.  
Best Strategy: Besides the Hadoop platform could offer 
us a distributed file systems, we also use the Co-allocation 
mechanism to get the best strategy to retrieve Medical 
Images.  
Replication: Because the Replication Location Service 
in MIFAS Middleware data can be saved completed, the data 
can be easily shared through different private cloud.  
Easy Management: We provide friendly management 
interface. And through the interface we can easy setting and 
management of the private cloud environment in MIFAS.  
Furthermore, the MIFAS is not only a good practice of 
implementation medical image file accessing on cloud but 
also the goal of Medical Image Exchanging to enhance 
patients and their caregivers share information can be fully 
achieved. Through the MIFAS system with a lot less 
expense, redundancy in medical resources is believed to be 
acceptable. 
 
ACKNOWLEDGMENT 
This work is supported in part by the National Science 
Council, Taiwan R.O.C., under grants no. NSC 99-2220-E-
029-004 and NSC 99-2622-E-029-001-CC2. 
 
REFERENCES 
[1] Apache Hadoop Project, http://hadoop.apache.org/hdfs/ 
[2] Canada Health Infoway, http://www.infoway-inforoute.ca 
[3] American Recovery And Reinvestment 
Act(ARRA),http://www.ama-assn.org 
[4] Department of Health, Executive Yuan, R.O.C. (TAIWAN) 
“Electronic medical record project”, http://emr.doh.gov.tw 
[5] Mental Research Institute, http://www.mri.org 
[6] Wiki-Electronic medical record, 
http://en.wikipedia.org/wiki/Electronic_medical_record 
[7] Chervenak, E. Deelman, I. Foster, L. Guy, W. Hoschek, A. 
Iamnitchi, C. Kesselman, P. Kunszt, and M. Ripeanu, B. Schwarz, 
H. Stockinger, K. Stockinger, and B. Tierney. “Giggle: A 
Framework for Constructing Scalable Replica Location Services,” 
in Proc. SC, pp. 1-17, 2002. 
[8] Chervenak, I. Foster, C. Kesselman, C. Salisbury, and S. Tuecke, 
“The data grid: Towards an architecture for the distributed 
management and analysis of large scientific datasets,” Journal of 
Network and Computer Applications, 23(3), pp. 187-200, 2001. 
[9] G. V. Koutelakis, D. K. Lymperopoulos, Member, IEEE, “A Grid 
PACS Architecture: Providing Data-centric Applications through a 
Grid Infrastructure”, Proceedings of the 29th Annual International 
Conference of the IEEE EMBS Cite International, Lyon, France, 
ISBN: 978-1-4244-0787-3, August 23-26, 2007. 
[10] N. E. King , B. Liu , Z. Zhou , J. Documet  , H.K. Huang “The Data 
Storage Grid: The Next Generation of Fault-Tolerant Storage for 
Backup and Disaster Recovery of Clinical Images” Medical 
Imaging 2005: PACS and Imaging Informatics, edited by Osman 
M. Ratib, Steven C. Horii, Proceedings of SPIE Vol. 5748, pp. 208-
217, 2005. 
[11] S. Vazhkudai, “Enabling the Co-Allocation of Grid Data 
Transfers,” Proceedings of Fourth International Workshop on Grid 
Computing, pp. 44-51, 17 November 2003. 
[12] C.T. Yang, S.Y. Wang, C.H. Lin, M.H Lee, and T.Y. Wu, “Cyber 
Transformer: A Toolkit for Files Transfer with Replica 
Management in Data Grid Environments,” Proceedings of the 
Second Workshop on Grid Technologies and Applications 
325
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
100年 5月 30日 
報告人
姓名 張子杰 
服務機構 
及職稱 
東海大學 
資訊工程學系 
會議時
間 
會議地
點 
2011/05/26~28 
Busan lotte hotel, Korea 
本會核定 
補助文號 
 
會議 
名稱 
(英文)：The 9th IEEE International Symposium on Parallel and 
Distributed Processing with Applications (ISPA 2011) 
發表 
論文 
題目 
(英文)：Performance Comparison with OpenMP Parallelization for 
Multicore Systems 
 報告內容應包括下列各項： 
參加會
議經過 
 
The 9th IEEE International Symposium on Parallel and Distributed 
Processing with Applications (ISPA 2011) 所舉辦的會議主要是一
平行與分散式計算研究領域的會議，讓學術界或工業界的工程師
交換並且討論他們彼此擅長的領域與經驗，並分享關於網路方面
的新想法、研究結果和應用的環境。ISPA 2011於 2011年 05月 26
日至 28日在韓國 Busan Lotte Hotel 所舉行 
 
ISPA 此次共有 4場 Keynotes  
 
Keynote 1. : Software Engineering Approaches to the Challenges in 
Technology Education and System Development in the Software 
Ecosystem Environment 
Keynote 2. : Research Frontiers in The Clouds, Many-Core and 
Internet of Things 
Keynote 3. : International Collaboration for Privacy and Information 
Security 
Keynote 4. : Cyber-Individual: Visions and Challenges 
 
Session 1-A: ISPA 2011 (Reliability, Fault Tolerance, and Security) 
Session 1-B: ISPA 2011 (Middleware) 
Session 1-C: ISPA 2011 (Network and Pervasive Computing) 
Session 1-D: SGH 2011 
Session 1-E: UMES 2011 
Session 2- A: ISPA 2011 (Middleware & Network and Pervasive 
 
與會心
得 
ISPA 2011的重點是Parallel和Distributed Computing和Networking
相關研究，包括可靠性，容錯性和安全性問題以及算法及在平行
和分散式計算環境的效能評價和測量。 
 
本人發表的 Paper為： 
1. Performance Comparison with OpenMP Parallelization for 
Multicore Systems 於 5月 26日下午之 Session 2-B中報告 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
100年 5月 30日 
報告人
姓名 王顯義 
服務機構 
及職稱 
東海大學 
資訊工程學系 
會議時
間 
會議地
點 
2011/05/26~28 
Busan lotte hotel, Korea 
本會核定 
補助文號 
 
會議 
名稱 
(英文)：The 9th IEEE International Symposium on Parallel and 
Distributed Processing with Applications (ISPA 2011) 
發表 
論文 
題目 
(英文)：Performance Comparison with OpenMP Parallelization for 
Multicore Systems 
 報告內容應包括下列各項： 
參加會
議經過 
 
The 9th IEEE International Symposium on Parallel and Distributed 
Processing with Applications (ISPA 2011) 所舉辦的會議主要是一
平行與分散式計算研究領域的會議，讓學術界或工業界的工程師
交換並且討論他們彼此擅長的領域與經驗，並分享關於網路方面
的新想法、研究結果和應用的環境。ISPA 2011於 2011年 05月 26
日至 28日在韓國 Busan Lotte Hotel所舉行 
 
ISPA 此次共有 4場 Keynotes  
 
Keynote 1. : Software Engineering Approaches to the Challenges in 
Technology Education and System Development in the Software 
Ecosystem Environment 
Keynote 2. : Research Frontiers in The Clouds, Many-Core and 
Internet of Things 
Keynote 3. : International Collaboration for Privacy and Information 
Security 
Keynote 4. : Cyber-Individual: Visions and Challenges 
 
Session 1-A: ISPA 2011 (Reliability, Fault Tolerance, and Security) 
Session 1-B: ISPA 2011 (Middleware) 
Session 1-C: ISPA 2011 (Network and Pervasive Computing) 
Session 1-D: SGH 2011 
Session 1-E: UMES 2011 
Session 2- A: ISPA 2011 (Middleware & Network and Pervasive 
 與會心
得 
ISPA 2011的重點是Parallel和Distributed Computing和Networking
相關研究，包括可靠性，容錯性和安全性問題以及算法及在平行
和分散式計算環境的效能評價和測量。 
 
本人參與的 Paper為： 
1. Performance Comparison with OpenMP Parallelization for 
Multicore Systems 於 5月 26日下午之 Session 2-B中報告 
 
The 9th IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA 2011) 
1 
          
PROGRAM SCHEDULE 
 
 
May 26, 2011 (Thursday) 
09:00
-
09:30 
Registration 
09:40
-
12:00 
Session 1-A 
(ISPA-11) 
(Chair: SangOh 
Park) 
 
(Room: Sapphire) 
Session 1-B 
(ISPA-11) 
(Chair: Menghui 
Yang) 
 
(Room: Astor) 
Session 1-C 
(ISPA-11) 
(Chair: Jongsung Kim) 
 
 
(Room: Charlotte) 
Session 1-D 
(SGH-11) 
(Chair: Sanghyun 
Seo) 
 
(Room: Belle-Vue) 
Session 1-E 
(UMES-11) 
(Chair: Fan Jing) 
 
 
(Room: Berkeley) 
12:00
-
13:30 
Lunch (Room: Emerald – LL fl.) 
13:30
-
14:50 
Opening Remark by Prof. Young-Sik Jeong 
 (ISPA 2011, ICASE 2011, GSDP 2011) 
 
Keynote Speech 1, (Chair: Young-Sik Jeong) 
Title: Software Engineering Approaches to the Challenges in Technology Education and System 
Development in the Software Ecosystem Environment 
by Dr. C.V. Ramamoorthy, Professor, University of California at Berkeley, USA 
(Room: Sapphire – LL fl.) 
14:50
-
15:10 
Coffee Break 
15:10
-
16:20 
Keynote Speech 2, (Chair: Cho-Li Wang) 
 
Title: Research Frontiers in The Clouds, Many-Core and Internet of Things 
by Dr. Kai Hwang, Professor, University of Southern California, USA  
(Room: Sapphire – LL fl.) 
16:20
-
16:40 
Break 
16:40
-
18:00 
Session 2-A 
(ISPA-11) 
(Chair: Sejun Song) 
 
 
(Room: Sapphire) 
Session 2-B 
(ISPA-11) 
(Chair: Carla 
Osthoff) 
 
(Room: Astor) 
Session 2-C 
(ISPA-11) 
(Chair: Yen-Chun  
Lin) 
 
(Room: Charlotte) 
Session 2-D 
(ICASE-11) 
(Chair: Amy Poh  
Ai Ling) 
 
(Room: Belle-Vue) 
Session 2-E 
(PETSE & GSDP-11) 
(Chair: LI Ye-hua) 
 
 
(Room: Berkeley) 
18:30
-
20:00 
Conference Reception (Room: Emerald – LL fl.) 
The 9th IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA 2011) 
3 
          
 
 
 
May 28, 2011 (Saturday) 
09:00
-
09:30 
Registration 
09:40
-
12:00 
Session 5-A 
(ISPA-11) 
(Chair: Jongsung 
Kim) 
 
(Room: Astor) 
Session 5-B 
(UMES & Cloud-11) 
(Chair: Rod Fatoohi) 
 
 
(Room: Charlotte) 
Session 5-C 
(SGSC & ICASE-11) 
(Chair: Jungsik Cho) 
 
 
(Room: Belle-Vue) 
Session 5-D 
(DATICS & SGSC-11) 
(Chair: Sang Oh Park) 
 
 
(Room: Berkeley) 
Session 5-E 
(ISPA & CIA-11) 
(Chair: Yangsun 
Lee) 
 
(Room: Sapphire) 
  
Note 
1. A paper presentation should be made by one of authors of the paper, during a 20 minute time slot  
(15 minutes for the presentation itself and 5 minutes for Q/A). 
2. All speakers of each session should meet the session chair at its room 10 minutes before the session. 
3. We will prepare Windows 7 laptops running the Adobe Reader and Microsoft Office 2007 for paper 
presentations. Please prepare your presentation files for being read by those applications. 
4. If you have any query, please feel free to contact via (busancon@ftrai.org). 
 
The 9th IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA 2011) 
5 
          
- Hybrid OpenCL: Enhancing OpenCL for Distributed Processing 
Ryo Aoki, Shuichi Oikawa, Takashi Nakamura, and Satoshi Miki 
 
 
09:40 - 12:00   Session 1-C: ISPA 2011 (Network and Pervasive Computing) 
(Room: Charlotte, 42nd floor) 
(Chair: Jongsung Kim) 
 
- Spatial Ordering Derivation for One-dimensional Wireless Sensor Networks  
Xiaojun Zhu and Guihai Chen 
- A Novel Architecture and Routing Algorithm for Dynamic Reconfigurable Network-
On-Chip 
Li-Wei Wu, Wei-Xiang Tang, and Yarsun Hsu 
- Optimizing CDN Infrastructure for Live Streaming with Constrained Server Chaining 
Zhenyun Zhuang and Chun Guo 
- An Efficient VoD Scheme Combining Fast Broadcasting with Patching 
Sung Soo Moon, Kyung Tae Kim, Seongwoo Lee, Hee Yong Youn, and Ohyoung Song 
- A Two-Level Caching Protocol for Hierarchical Peer-to-Peer File Sharing Systems 
Qiying Wei, Tingting Qin, and Satoshi Fujita 
- Searching in Internet of Things: Vision and Challenges 
Daqiang Zhang, Laurence T. Yang, and Hongyu Huang 
 
 
09:40 - 12:00   Session 1-D: SGH 2011 
(Room: Belle-Vue, 42nd floor) 
(Chair: Sanghyun Seo) 
 
- Parallel Extended Basic Operations for Conic Curves Cryptography over Ring Zn 
Yongnan Li, Limin Xiao, Siming Chen, Hongyun Tian, Li Ruan, and Binbin Yu 
- Secure Communication Using Reed-Muller Codes and Partially Balanced Design In 
Wireless Sensor Network 
Pinaki Sarkar and Amrita Saha 
- Evaluating the Memory System Performance of Software-Initiated Inter-Core LLC 
Prepushing 
Min Cai and Zhimin Gu 
- Yet Another Certificateless three-party authenticated key agreement protocol 
Jianbin Hu, Hu Xiong, Zhi Guan, Cong Tang, Yonggang Wang, Wei Xin, and Zhong Chen 
- Balanced Chain-Based Routing Protocol(BCBRP) for Energy Efficient Wireless Sensor 
Networks 
Kyu Sung Ahn, Dae Gun Kim, Back Sun Sim, Hee Yong Youn, and Ohyoung Song 
- Simulation and Evaluation Snoopy cache coherence protocols with update strategy in 
shared memory multiprocessor systems 
Bahman Hashemi 
The 9th IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA 2011) 
7 
          
14:50 - 15:10   Coffee Break 
 
 
15:10 - 16:20   Keynote Speech 2 : ISPA 2011 
(Room: Sapphire, LL floor) 
(Chair: Cho-Li Wang) 
 
Research Frontiers in The Clouds, Many-Core and Internet of Things 
Dr. Kai Hwang, Professor, University of Southern California, USA 
 
 
16:20 - 16:40   Break 
 
 
16:40 - 18:00   Session 2- A: ISPA 2011 (Middleware & Network and Pervasive Computing) 
(Room: Sapphire, LL floor) 
(Chair: Sejun Song) 
 
- A Hybrid Shared-nothing/Shared-data Storage Scheme for Large-scale Data 
Processing 
Huaiming Song, Xian-He Sun, and Yong Chen 
- Tool Chain Support with Dynamic Profiling for RISP 
Chao Wang, Huizhen Zhang, Xuehai Zhou, Jinsong Ji, and Aili Wang  
- Splitting TCP for MPI applications executed on grids 
Olivier Glück and Jean-Christophe Mignot 
- Energy-Efficient DRX Scheduling for QoS Traffic in LTE Networks 
Kuo-Chang Ting, Hwang-Cheng Wang, Chih-Cheng Tseng, and Fang-Chang Kuo 
 
 
16:40 - 18:00   Session 2-B: ISPA 2011 (Performance Simulations and Evaluations) 
(Room: Astor, 42nd floor) 
(Chair: Carla Osthoff) 
 
- End-to-End Schedulability Analysis for Bi-directional Real-Time Multistage Pipeline 
Execution 
Shenglin Gui and Lei Luo 
- Schedulability Analysis for Distributed Systems Using Network of Action Automata 
and Environment Automata 
Shenglin Gui, Lei Luo, Miao Yu, Jianhua Xu, and Yun Li 
- Performance Comparison with OpenMP Parallelization for Multi-core Systems 
Chao-Tung Yang, Tzu-Chieh Chang, Hsien-Yi Wang, William C.C. Chu, and Chih-Hung 
Chang  
The 9th IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA 2011) 
9 
          
- Application of an ERM Framework and Its Real-Time System for Disaster Risk 
Management 
Taeho Park, Beom-Cheol Park, Dhong Ha Lee, Jae-Bong Kim 
- Pangandaran Village Resiliency Level due to Earthquake and Tsunami Risk 
Mizan Bustanul Fuady Bisri 
 
 
18:30 - 20:00   Conference Reception (Room: Emerald – LL floor) 
The 9th IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA 2011) 
11 
          
 
 
10:00 - 12:00   Session 3-C: ICASE 2011 
(Room: Charlotte, 42nd floor) 
(Chair: Sanghyun Seo) 
 
- Distribution-Transparency in Runtime Verification 
Somayeh Malakuti, Mehmet Aksit, and Christoph Bockisch 
- Process & Evidence enable to automate ALM (Application Lifecycle Management) 
Jeong Ah Kim, Seung Young Choi, and Sun Myung Hwang 
- A Lightweight, Component-based Approach to Engineering Reconfigurable Embedded 
Real-Time Control Software 
Jagun Kwon and Stephen Hailes 
- A Visualization Technique for the Passage Rates of Unit Testing and Static Checking 
with Caller-Callee Relationships 
Yuko Muto, Kozo Okano, and Shinji Kusumoto 
- Exploiting Module Locality to Improve Software Fault Prediction 
Cheng-Zen Yang, Ing-Xiang Chen, and Chin-Sung Fan-Chiang 
 
 
10:00 - 12:00   Session 3-D: SGH 2011 
(Room: Belle-Vue, 42nd floor) 
(Chair: Xiaojun Zhu) 
 
- Interaction-Aware Dynamic Power Optimization Scheme for Wireless Networks 
Interface Cards 
Wang Jing, Guan Xuetao, and Cheng Xu 
- Traceable Anonymous Authentication Scheme for Vehicular Ad-hoc Networks 
Zeen Kim, Junhyun Yim, Jangseong Kim, Kwangjo Kim, and Taeshik Sohn 
- A Configurable Approach to Tolerate Soft Errors via Partial Software Protection 
Lei Xiong and Qingping Tan 
- Untraceable and Serverless RFID Authentication and Search Protocols 
Zeen Kim, Jangseong Kim, Kwangjo Kim, Imsung Choi, and Taeshik Shon 
- Using folksonomies for Building User Preference List 
Harshit Kumar, Pil Seong Park, and Hong Gee Kim 
 
 
10:00 - 12:00   Session 3-E: CIA 2011 & NSTC 2011 
(Room: Berkeley, 42nd floor) 
(Chair: Changhoon Lee) 
 
- Adaptive Motion Vector Smoothing for Improving Side Information in Distributed 
Video coding 
Ok-Hue Cho, Yong-Chul Kwon, Won-Hyung Lee 
The 9th IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA 2011) 
13 
          
17:00 - 18:00   Session 4-A: ISPA 2011 (Architectures and Virtualization) 
(Room: Sapphire, LL floor) 
(Chair: Min Choi) 
 
- A Novel Predictive and Self Adaptive Dynamic Thread Pool Management 
Kang-Lyul Lee, Hong Nhat Pham, Hee-seong Kim, Hee Yong Youn, and Ohyoung Song 
- Vega LingCloud: A Resource Single Leasing Point System to Support Heterogeneous 
Application Modes on Shared Infrastructure 
Xiaoyi Lu, Jian Lin, Li Zha, and Zhiwei Xu 
- A Flexible High Speed Star Network Based on Peer to Peer Links on FPGA 
Chao Wang, Junneng Zhang, Xuehai Zhou, Xiaojing Feng, and Aili Wang 
 
 
17:00 - 18:00   Session 4-B: ISPA 2011 (Performance Simulations and Evaluations) 
(Room: Astor, 42nd floor) 
(Chair: Jan-Jan Wu) 
 
- Robustness Analysis of Artificial Neural Networks and Support Vector Machine in 
Making Prediction 
Saiful Anwar and Rifki Ismal 
- Preventing Denial of Service Attacks in Government E-Services Using a New Efficient 
Packet Filtering Technique  
Mohammed Alhabeeb, Phu Dung Le, and Bala Srinivasan 
 
 
17:00 - 18:00   Session 4-C: ICASE 2011 
(Room: Charlotte, 42nd floor) 
(Chair: Seung Ho Lim) 
 
- Development of the RESTful JPIC SDK for the application using public information 
WoongChul Choi and JungHun Kim 
- A Practical Web Service Composition 
HwaYoung Jeong and BongHwa Hong 
- Fair Scheduling Strategy for Delay-Sensitive Handover of IEEE 802.16 Real-Time 
Communications 
DongHo Kim, SoonSeok Kim, and YongHee Lee 
 
 
17:00 - 18:00   Session 4-D: SGH 2011 
(Room: Belle-Vue, 42nd floor) 
(Chair: Pinaki Sarkar) 
 
- The Effect of Correlated Failure on the Reliability of HPC Systems 
Thanadech Thanakornworakij, Raja Nassar, Chokchai Box Leangsuksun, and Mihaela Paun 
The 9th IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA 2011) 
15 
          
May 28, 2011 (Saturday) 
 
 
09:00 - 09:30   Registration 
 
 
09:40 - 12:00   Session 5-A: ISPA 2011 (Algorithms and Applications) 
(Room: Astor, 42nd floor) 
(Chair: Jongsung Kim) 
 
- GPGPU Acceleration Algorithm for Medical Image Reconstruction 
Virasin Archirapatkave, Hendra Sumilo, Simon Chong Wee See, and Tiranee Achalakul  
- An Equilibrium-based Approach for Determining Winners in Combinatorial Auctions 
ChenKun Tsung, HannJang Ho, and SingLing Lee 
- An Impulse Noise Removal Algorithm by Considering Region Property for Color 
Image 
Yi-Ta Wu, Yih-Tyng Wu, Shau-Yin Tseng, and Chao-Yi Cho 
- Efficient Resource Selection Algorithm for Enterprise Grid Systems 
Wan Nor Shuhadah Wan Nik, Bing Bing Zhou, Albert Y. Zomaya, and Jemal H. Abawajy 
- Performance Evaluation of Wireless Sensor Networks for Mobile Sensor Nodes 
Considering Goodput and Depletion Metrics 
Leonard Barolli, Tao Yang, Gjergji Mino, Arjan Durresi, Fatos Xhafa, and Makoto 
Takizawa 
- Improving Performance on Atmospheric Models through a Hybrid OpenMP/MPI 
Implementation 
Carla Osthoff, Pablo Grunmann, Francieli Boito, Rodrigo Kassick, Laercio Pilla, Philippe 
Navaux, Claudio Schepke, Jairo Panetta, Nicolas Maillard, Pedro L. Silva Dias, and Robert 
Walko 
- On Using Pattern Matching Algorithms in MapReduce Applications 
Nikzad Babaii Rizvandi, Javid Taheri, and Albert Y. Zomaya 
 
 
 
09:40 - 12:00   Session 5-B: UMES 2011 & CloudGrid 2011 
(Room: Charlotte, 42nd floor) 
(Chair: Rod Fatoohi) 
 
- Design and Implementation of a Self-Configuring Instrument Control System 
Iztok Marjanovic and Rod Fatoohi  
- Utterance Classification for Combination of Multiple Simple Dialog Systems 
Seong-Jun Hahm, Akinori Ito, Kentaro Awano, Masashi Ito, and Shozo Makino 
- A System Proposal for Multimodal Retrieval of Multimedia Documents 
Mohamed Kharrat, Anis Jedidi, and Faiez Gargouri 
The 9th IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA 2011) 
17 
          
- Concurrent Online Test Architecture for Multiple Controller Blocks with Minimum 
Fault Latency 
Philemon Daniel and Rajeevan Chandel 
- Modeling and Analysis of Radiation Therapy System with Respiratory Compensation 
using Uppaal 
Ka Lok Man, Tomas Krilavicius, Kaiyu Wan, Danny Hughes, and Kevin Lee 
- Scheduling of Energy Storage Systems with Geographically Distributed Renewables 
Alberto J. Lamadrid, Tim D. Mount, and Robert J. Thomas 
 
 
09:40 - 12:00   Session 5-E: ISPA 2011 
(Room: Sapphire, LL floor) 
(Chair: Yangsun Lee) 
 
- A Hierarchical Middleware Architecture for Efficient Home Control through an IPv6-
based Global Network 
Sang Oh Park, Tae Hoon Do, Sung Jo Kim 
- Hash-based RFID tag Mutual Authentication Scheme with Retrieval Efficiency 
Jung-Sik Cho, Soo-Cheol Kim, Sung Kwon Kim 
- RFID System Security Analysis, Response Strategies and Research Directions 
Jung-Sik Cho, Soo-Cheol Kim 
- GPU Computing for Longwave Radiation Physics: A RRTM_LW scheme case study 
Fengshun Lu, Xiaoqun Cao, Junqiang Song, Xiaoqian Zhu 
- Tomogarphical Medical Image Reconstruction Using Kalman Filter Technique 
Shayan Goliei, Seyed Ghorshi 
 
 
 
The 9th IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA 2011) 
19 
          
MEMO 
A. CTM 
CTM (Close To Metal) [16] gave developers direct 
access to the native instruction set and memory of the 
massively parallel computational elements in modern AMD 
video cards. CTM was support by AMD, but AMD recently 
switched from CTM to OpenCL. 
B. OpenCL 
OpenCL (Open Computing Language) [17] is a 
framework for writing programs that execute across 
heterogeneous platforms consisting of CPUs, GPUs, and 
other processors. OpenCL provides parallel computing using 
task-based and data-based parallelism. Its architecture shares 
a range of computational interfaces with two competitors, 
NVidia's Compute Unified Device Architecture and 
Microsoft's Direct Compute, while it is adopted by 
AMD/ATI for its GPGPU technology Stream SDK. 
C. CUDA 
CUDA [10, 11, 13] (an acronym for Compute Unified 
Device Architecture) is a parallel computing architecture 
developed by NVIDIA; CUDA is the computing engine in 
NVIDIA graphics processing units or GPUs that is 
accessible to software developers through industry standard 
programming language. 
D. MPI 
Message Passing Interface (MPI) [20] is a specification 
for message passing operations. It defines each worker as a 
process. MPI is currently the de-facto standard for 
developing HPC applications on distributed memory 
architecture. It provides language bindings for C, C++, and 
FORTRAN. 
E. OpenMP 
OpenMP [1] supports multi-platform shared memory 
multiprocessing programming in C, C++ and FORTRAN on 
much architecture which including UNIX and Microsoft 
Windows platforms. It consists of a set of compiler 
directives, library routines, and environment variables that 
influence run-time behavior.  
F. Pthread 
POSIX Threads (Pthread) [18] is a POSIX standard for 
threads. The standard, POSIX.1c, Threads extensions (IEEE 
Std 1003.1c-1995), defines an API for creating and 
manipulating threads. Implementations of the API are 
available on many Unix-like POSIX systems such as 
FreeBSD, NetBSD, GNU/Linux, Mac OS X and Solaris, but 
Microsoft Windows implementations also exist. 
G. TBB 
Intel® Threading Building Blocks (Intel TBB) [19] 
offers a rich and complete approach to expressing parallelism 
in a C++ program. It is a library for writing software 
programs that to help us take advantage of multi-core 
processor performance without having to be a threading 
expert. 
III. AUTO-PARALLELISM 
When we want a code to be parallel, we must sure that 
the variables are independent each other in the loop. So auto-
parallelizing compilers try to analyze codes and 
automatically generate parallel codes. Building an auto-
parallelizing compiler has proven to be a very difficult task 
when producing efficient code in all cases for a wide variety 
of applications. Thus, auto-parallelization remains a hot 
research topic. There are some auto-parallel compilers we 
found. 
A. ROSE 
ROSE [2] is an open source compiler, and ROSE builds 
source-to-source program transformation and analysis tools 
for large-scale Fortran 77/95/2003, C, C++, OpenMP, and 
UPC applications. 
B. Cetus 
Cetus [4] is a source-to-source C compiler written in Java 
and maintained at Purdue University. Cetus provides 
automatic parallelization, and many other applications have 
emerged. Current Cetus 1.2.1 release, it provides OpenMP-
to-CUDA package.  
C. PAR4ALL 
PAR4ALL [3] is an open-source environment to do 
source-to-source transformations on C and Fortran programs 
for parallelizing, optimizing, instrumenting, reverse-
engineering, etc. on various targets, from multicore system-
on-chip with some accelerators up to high performance 
computers and GPU. 
D. Open64 
Open64 [5] derives from SGI’s MIPSPro compiler and 
from work done by Intel Corp, in conjunction with the 
Chinese Academy of Sciences, also licensed under the GNU 
Public License (GPL v2). The Open64 suite currently 
includes compilers for C, C++, and Fortran90/95 compilers 
for the IA-64 Linux ABI and API standards. 
E. Intel 
Intel® Composer XE 2011 [6] is an optimized compiler 
combined with high-performance libraries, advanced 
vectorization, support for Intel® Parallel Building Blocks 
and OpenMP*, speeding and simplifying improved multicore 
performance on multiple operating systems with the same 
code base. 
F. PGI 
PGI [14] Unified Binary™ technology simplifies cross-
platform support by combining into a single executable file, 
code sequences optimized for multi-core x64 processor 
families from Intel and AMD and GPU accelerators from 
NVIDIA. 
233
 
Figure 3.  Matrix Multiplication with size from 256 to 2048 
Figure 3 shows that the performance on processing the 
massively parallel execution as the application of Matrix 
Multiplication from size 256 to 2048. 
Figure 4 shows that using N loops to compute the answer 
of pi. But the program which is transformed by PAR4ALL is 
unstable. When N= 100000000 and 1000000000, it got the 
wrong answer. And the compilers of ROSE and Cetus also 
cannot transform the source code to the code with OpenMP 
directives successfully. 
Figure 5 and Figure 6 show that the performance 
measurements of nbody and fasta with a particular value N. 
When we compile the codes which are transformed by 
ROSE and Cetus, we got error. Although we could only 
transform successfully by using PAR4ALL, from the 
performance result in Figure 5, PAR4ALL cannot give us a 
better performance than other compilers, even giving us a 
lower performance than the program with original code. 
 
Figure 4.  pi with N from 100000000 to 10000000000 
 
Figure 5.  nbody with N from 500000 to 50000000 
 
 
Figure 6.  fasta with N from 250000 to 25000000 
When we transformed the code of fasta by those 
compilers, it failed at PAR4ALL, Cetus, and Open64. In 
Figure 6, the performance is similar. The Figure 7 shows the 
performance measurements of jacobi. It’s fatser when we use 
Intel compiler and then open64. 
 
Figure 7.  jacobi 
B. Desktop Computer (CUDA version) 
• CPU: Intel(R) Xeon(R) CPU E5410 @ 2.33GHz( 8 
cores) [15] 
• RAM: 4GB 
• GPU: Tesla C1060 
The compilers mentioned earlier, not everyone is support 
automatically translate to CUDA. PAR4ALL and PGI are 
tools that we used in the current environment, and using 
these tools to generate CUDA application runs on GPU and 
comparing with C application runs on CPU. The benchmarks 
of this part are the same with the benchmarks of part A, but 
the method of parallelize is different from part A. 
 
Figure 8.  Matrix Multiplication with size from 256 to 2048 
235
 
Figure 15.  Jacobi 
In Figure 13 to 15, the performance results are similar as 
the performance results in Part A. Finally, we make a list of 
comparisons in Figure 16. 
VII. CONCLUSIONS 
As we know, the methods of OpenMP and CUDA are 
famous in parallel programming. The performance could get 
better after the serial codes translated to the codes which 
using these two methods. But not any code is suitable to 
parallelize. 
Parallel programming is not easy to programmers. So 
these compilers which support auto-parallel can help us 
easily transform our non-parallel codes to parallel codes. 
In general computer, the compilers of Intel and PGI have 
the best performance in the version of OpenMP; and in the 
version of CUDA, the compiler of PGI has the best 
performance.  
As for embedded systems, ROSE, PAR4ALL, and Cetus 
have similar performance. From the experimental results in 
this paper, we found that only PAR4ALL could transform 
the code of nbody successfully. But it gave us a terrible 
result; the performance is much lower than original. So, the 
perfect auto-parallelizing compiler is yet to be produced. 
However, there are some cases where auto-parallelization is 
perfectly suited. 
 
Figure 16.  list of comparisons 
ACKNOWLEDGMENT 
This work is supported in part by the National Science 
Council, Taiwan R.O.C., under grants no. NSC 99-2220-E-
029-004. 
REFERENCES 
[1] Open MP Specification, http://openmp.org/wp/about-openmp/ 
[2] ROSE, http://www.rosecompiler.org/ 
[3] PAR4ALL, http://www.par4all.org/ 
[4] Cetus, http://cetus.ecn.purdue.edu/ 
[5] Open64, http://www.open64.net/ 
[6] Intel, http://software.intel.com/en-us/articles/intel-parallel-studio-xe/ 
[7] Benchmarcks, 
http://shootout.alioth.debian.org/u32q/benchmark.php?test=nbody&la
ng=gcc 
[8] Arm11MP Core 
http://www.arm.com/products/processors/classic/arm11/arm11-
mpcore.php 
[9] NVIDIA Tesla C1060 Computing Processor, 
http://www.nvidia.com/object/product_tesla_c1060_us.html 
[10] CUDA, http://en.wikipedia.org/wiki/CUDA 
[11] Download cuda, http://developer.nvidia.com/object/cuda.htm 
[12] Top 500 Super Computer Sites, What is Gflop/s, 
http://www.top500.org/faq/what_gflop_s 
[13] NVIDIA CUDA Programming Guide, 
http://developer.download.nvidia.com/compute/cuda/2_3/toolkit/docs
/NVIDIA_CUDA_Programming_Guide_2.3.pdf 
[14] The Potland Group, http://www.pgroup.com/index.htm 
[15] Quad-Core Intel® Xeon® Processor 5400 Series, 
http://www.intel.com/Assets/en_US/PDF/datasheet/318589.pdf 
[16] Close To Metal wiki, http://en.wikipedia.org/wiki/Close_to_Metal 
[17] OpenCL, http://www.khronos.org/opencl/ 
[18] POSIX Threads Programming, 
https://computing.llnl.gov/tutorials/pthreads/ 
[19] Intel® Threading Building Blocks, 
http://www.threadingbuildingblocks.org/ 
[20] MPI, http://www.mcs.anl.gov/research/projects/mpi/ 
237
99 年度專題研究計畫研究成果彙整表 
計畫主持人：楊朝棟 計畫編號：99-2220-E-029-004- 
計畫名稱：多核心嵌入式軟體之模型驅動整合開發環境-VMC--子計畫六:多核心嵌入式軟體設計工具系
統之平行程式優化支援實作-VMC_PPO (I)(2/2) 
量化 
成果項目 
實際已達
成數（被
接受或已
發表）
預期總達
成數(含實
際已達成
數) 
本計畫
實際貢
獻百分
比 
單位
備註（質化說明：如數個計畫
共同成果、成果列為該期刊
之封面故事...等） 
期刊論文 0 0 100%  
研究報告/技術報
告 2 2 100%  
研討會論文 0 0 100% 
篇
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 5 5 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國
內 
參與計畫人
力 
（本國籍） 
專任助理 0 0 100% 
人次
 
國
外 論文著作 期刊論文 6 3 200% 篇 [1] Chao-Tung Yang*, Chih-Lin 
Huang and Cheng-Fang 
Lin, ’’Hybrid CUDA, OpenMP, 
and MPI Parallel Programming 
on Multicore GPU 
Clusters’’, Computer 
Physics Communications, 
Volume 182, Issue 1, January 
2011, Pages 266-269. (ISSN: 
0010-4655, SCI JCR=2.300, 
6/54, EI) 
[2] Chao-Tung Yang*, 
Chao-Chin Wu, and Jen-Hsiang 
Chang, ’’Performance-based 
Parallel Loop Self-Scheduling 
Chao-Tung； Dynamic Voltage 
Adjustment for Energy 
Efficient Scheduling on 
Multi-core Systems, 2011 
Ninth IEEE International 
Symposium on Parallel and 
Distributed Processing with 
Applications Workshops 
(ISPAW), Publication Year: 
2011, Page(s): 197 – 202. 
[2] Yang, Chao-Tung； Chang, 
Tzu-Chieh； Wang, Hsien-Yi；
Chu, William C.C.； Chang, 
Chih-Hung ；  Performance 
Comparison with OpenMP 
Parallelization for 
Multi-core Systems, Parallel 
and Distributed Processing 
with Applications (ISPA), 
2011 IEEE 9th International 
Symposium on, Digital Object 
Identifier: 
10.1109/ISPA.2011.60, 
Publication Year: 2011 , 
Page(s): 232 – 237 
[3] Chao-Tung Yang； Lung-Teng 
Chen ；  Wei-Li Chou ；
Kuan-Chieh Wang ；
Implementation of a Medical 
Image File Accessing System on 
Cloud Computing, 
Computational Science and 
Engineering (CSE), 2010 IEEE 
13th International Conference 
on, Digital Object 
Identifier: 
10.1109/CSE.2010.48, 
Publication Year: 2010 , 
Page(s): 321 - 326  
[4] Chao-Tung Yang*, Chih-Lin 
Huang, Cheng-Fang Lin, and 
Tzu-Chieh Chang, ’Hybrid 
Parallel Programming on GPU 
Clusters’, Parallel and 
Distributed Processing with 
Applications (ISPA), 2010 
International Symposium on, 
pp. 142-147, Taipei, Taiwan, 
Sept. 6-9, 2010. 
其他成果 
(無法以量化表達
之成果如辦理學術
活動、獲得獎項、
重要國際合作、研
究成果國際影響力
及其他協助產業技
術發展之具體效益
事項等，請以文字
敘述填列。) 
教育部SOC人才培育先導型計畫東海資工系楊朝棟教授獲95至 99年優良教材特優
獎 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
