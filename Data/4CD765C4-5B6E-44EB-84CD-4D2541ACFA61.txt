I 
目錄與圖目錄 
目錄與圖目錄 ................................................................................................................................... I 
一、 摘要 .................................................................................................................................. II 
二、 前言與研究目的 .............................................................................................................. 1 
三、 研究方法 .......................................................................................................................... 2 
複音音高及倍頻軌跡追蹤 ...................................................................................................... 2 
樂譜對齊與合成演算法 .......................................................................................................... 3 
四、 研究結果與現況 .............................................................................................................. 5 
五、 結果與討論 ...................................................................................................................... 7 
六、 參考文獻 .......................................................................................................................... 8 
七、 計劃成果自評 .................................................................................................................. 9 
八、 附錄一、 .......................................................................................................................... 9 
九、 附錄二、 ........................................................................................................................ 18 
 
 
圖 一、轉換合成系統架構 ............................................................................................................ 2 
圖 二、隱藏式馬可夫模型。(a) 起音機率 λ，延音機率 ψ。(b)狀態轉換權重陣列。 .......... 3 
圖 三、二階隱藏式馬可夫模型之格狀結構。 ............................................................................ 3 
圖 四、樂譜對齊流程。 ................................................................................................................ 4 
圖 五、chromagram 表示圖。(a)BWV1001 現場演奏音訊。(b)BWV1001 MIDI 檔 ............... 6 
圖 六、第一首測驗音訊。(a)頻譜。(b)倍頻軌跡追蹤結果。 ................................................... 6 
圖 七、第二首測驗音訊。(a)頻譜。(b)無樂譜對齊之倍頻軌跡追蹤結果。(c)有樂譜對齊之
倍頻軌跡追蹤結果。 ........................................................................................................ 6 
圖 八、巴哈 BWV1001 片段。(a)原音頻譜。(b)無樂譜對齊之倍頻軌跡追蹤結果。(c)有樂
譜對齊之倍頻軌跡追蹤結果。(d)合成二胡音色的頻譜。(e) 合成小號音色的頻譜。
 ............................................................................................................................................ 7 
 
 
  
1 
二、 前言與研究目的 
在過去，有研究成果顯示出單音二胡樂曲的轉換合成已可實現[1]。本系統試著針對多
音二胡樂曲進行分析，並再合成成其他如小提琴、小號、或雙簧管等樂器。為了達成此一
目標，我們使用 WGCDV (weighted greatest common divisor and vote)演算法[2]來擷取出多音
基音，並根據這些結果進行簡單的倍頻軌跡追蹤[1]。然而，於研究成果[1]與[2]中提到的方
法，並無法直接應用在多音音樂唱片中，有些樂器的演奏中即使只有單把樂器依舊經常出
現多音樂句，如小提琴在同時拉兩條絃時同時也會影響其他絃的顫動，此時訊號的表現上
可能同時便會有四個音源出現。 
在多音音樂唱片的分析中，多音源的基音偵測是必要的。Yeh[3]提出一個評分公式，
用來評斷所有可能基音排列組合的機率，這些音高的音源被當成是一種類諧波的、穩定的、
而且非殘響的訊號，這個訊號基於三個物理原則：諧波結構、頻譜平滑化及同一音源上振
幅的同步演化。在論文[4]中，Wen 研發出一個基於動態修定一階頻率預估的倍頻搜尋演算
法。然而，該演算法必須有個前提假設，就是要先知道音源的個數，而通常很難精確的推
出正確個數。在論文[5]中，張維城博士以論文[3]的結果為基礎，發展出高階隱藏式馬可夫
模型(high-order Hidden Markov Model)，在他們的論文中，該評分系統不但包含了共用倍頻
的特殊處理演算法，並且結合了多音樂曲的樹狀搜尋法，其優異的實驗結果於 2008 年
MIREX 的比賽中拔得頭籌[6]。 
在本計畫中，我們修改論文[5]的高階隱藏式馬可夫模型，將其應用在不同基音的倍頻
追蹤上，讓我們在連續的音框中獲得每個基音較精確的倍頻資訊。此外，我們注意到精確
的起頭音位置是十分重要的，論文[7]提出以音符為基礎的樂譜資訊比對演算法，論文[8]則
將 chroma 表示法應用在多音音樂唱片以音符標記的樂譜資訊來校正，我們採用其方法來取
得起頭音資訊，此一音符校正演算法可以修正前面多音基音預估的錯誤，同時也參考倍頻
追蹤的結果來調整起頭音的時間點。本計畫所提出的系統雖然是以擦絃樂器為主進行分析，
它亦可應用在其他類型的樂器分析上。最後，我們使用 additive 合成法[9]來合成其他樂器
的聲音，同時，此系統可讓使用者自己手動調整每個倍頻的參數，也可以自行加入一些演
奏上常用的特效，如顫音。 
整個轉換合成系統的架構圖如圖一所示。一開始我們先用多音音高偵測演算法來求出
基音候選人，再利用這些結果來進行倍頻軌跡偵測；隨後，樂譜對齊及起頭音偵測即應用
於基音與倍頻軌跡上；最後，我們即可計算出每個音源的音高及能量變化，並透過 additive
合成法進一步將原本的樂曲合成成另一把樂器，或加入我們想要的音效。 
本結案報告一共分為以下章節來介紹。第二章我們將會介紹倍頻軌跡追蹤演算法；第三章
將介紹複音樂譜對齊演算法；第四章將介紹如何計算各個音源的能量及音高；第五章為實
驗結果；最後一章為結論。 
3 
另一方面，這個格狀結構下的最佳路徑不只一條，也就是說倍頻的軌跡不只一條，因此傳
統的 Viterbi 演算法並不適用於我們的系統。因此，我們採用相連權重的前向遺傳預值方式，
每一條軌跡的機率將與相關路徑中點與點之間的遺傳權重有關聯，而由延續狀態所產生的
觀察機率被定義成一個高斯分佈模型，其式子如下： 
 
2 2
2 2
1 2 1 2
1
, exp
2 2 2
f m
f m
   
      
         
     
                      (1) 
其中， f 與 m 為兩兩倍頻候選人中的頻率差與振幅差，其頻率的單位是十二平均律，振
幅的單位是 dB。在本系統中， 1 被設為 0.25，代表四分之一個全音； 2 被設為 1.0，代表
1dB。 
 
圖 二、隱藏式馬可夫模型。(a) 起音機率 λ，延音機率 ψ。(b)狀態轉換權重陣列。 
 
圖 三、二階隱藏式馬可夫模型之格狀結構。 
樂譜對齊與合成演算法 
經過前一章的處理，仍有些許的誤差如五度音或八度音的音高預估遺漏會發生，如果
我們已有樂譜資訊的話，便可用其來修正這些誤差。現今坊間多數的古典或流行音樂，都
有留下紙本樂譜與唱片，還有一些由手動或自動樂譜轉換所產生的 MIDI 檔案。在本章我
們採用現有的 MIDI 檔案為輸入，透過音樂訊號檢索的技術來實作，舉例來說，如一些實
作多音錄音的排序然後透過音符來實現樂譜配對的演算法[7,12]。有個稱為 chromagram 的
5 
當動態時間規正調整樂譜後，跟樂譜無關的倍頻軌跡將被去除。然而，對譜過後的起
頭音時間點不一定非常精確。在演奏中，每個音符的起頭音時間點準確度是非常重要的，
一般而言，演奏時的起頭音時間與樂譜上所標記的節拍會差少許個音框。因此，我們必須
校正上一章倍頻軌跡追蹤後的結果。這裡我們先定義第 i 個音框的能量為： 
 
/ 2 1
2
0
( )
N
i i
j
E A j


                                 (5) 
然後我們用下個式子來決定第 i 個音框是否為起頭音音框： 
11       ,if / and  
( )
0       ,
i i iE E E T
Onset i
otherwise
  
 

                       (6) 
在我們的實驗中，α 被設為 2。另一點必須要注意的是，當樂譜資訊(MIDI 檔案)跟音
樂唱片訊號有一定程度上的差異時，樂譜對齊演算法將會失敗，此時便需要靠手動來修正
錯誤。 
經過前面的流程後，我們可獲得基音、倍頻軌跡與起頭音時間，最後便是要測量每個
音源的能量。本系統採取的計算方法，是將每個音框中隸屬於同一組倍頻群組的頻域能量
加總起來。在現實生活中，人們對於倍頻之間的能量差不敏感[13]，如果兩個音符的音高與
音量很接近，他們的倍頻通常也很接近。在論文[1]中，他建立了一個中國古典樂器－二胡
的基音與倍頻能量參數庫，紀錄了幾組音高所對應的二十個倍頻能量，以供合成用。如果
要合成的音高是參數庫沒有的話，就透過參數庫中臨近兩音高的參數來做線性內插來求出
近似值。有了能量資訊後，我們便可以透過 additive 合成法很容易的合成出我們想要的各
種音色或表情。論文[1]中有提供相關的工具，其展示影片在[14]中。 
四、 研究結果與現況 
本系統的實驗輸入採用兩個由 MIDI 合成器所產生的測試訊號，以及一首巴哈的
BWV1001 小提琴無伴奏片段，為了能更清析的展現結果，頻譜的頻率範圍只展現在 2KHz
以下。第一個實驗訊號一開始同時出現 C4, E4, G4 三個音，之後接 D4, F4, A4 三個音，圖
六(a)與(b)分別秀出該訊號的頻譜及倍頻軌跡追蹤的結果。第二個實驗訊號一開始同時出現
C3 及 C4 兩個八度音，之後接 F3 及 F4 兩個八度音。顯然的，倍頻軌跡追蹤的演算法無法
精確算出兩個八度音的基音及倍頻軌跡，圖七(b)展現出樂譜對齊前的結果，圖七(c)則展現
出樂譜對齊後的結果，圓圈所連成的軌跡是基音軌跡，實線則為倍頻軌跡。對照圖七(a)可
以清楚看出樂譜對齊前八度音只找到較低的音，經過樂譜對齊後即可找出兩個音。 
最後，圖八展現出巴哈 BWV1001 的實驗結果，圖八(b)是倍頻軌跡追蹤的結果，圖八
(c)則是樂譜對齊後的結果。與前兩個實驗不一樣的地方在於，輸入的音樂訊號與樂譜資訊
有一部份有差異，圖五(b)便是這個音訊的MIDI樂譜資訊；前兩個實的輸入則是直接由MIDI
樂譜所合成，所以起頭音時間會完全一樣。圖八(d)與(e)則是用 additive 合成法去合成為二
胡與小號的音色的頻譜圖，其合成的結果聽起來演奏手法及表情跟原訊號十分相似。然後，
有一部份的合成樂曲有不連續的情況發生，這是因為倍頻軌跡追蹤後所統計的倍頻能量過
低所致。較好的解決方法是使用效果較佳的多音基音偵測演算法，如論文[5]所述。另外，
原現場演奏的音訊，其顫音手法比合成的結果更明顯，這是因為在倍頻軌跡追蹤的演算法
中，我們為了去除一些雜訊有實作頻譜的平滑化，因此導致此一副作用的產生。 
7 
(a) 
    
(b)                                         (c) 
    
(d)                                         (e) 
圖 八、巴哈 BWV1001 片段。(a)原音頻譜。(b)無樂譜對齊之倍頻軌跡追蹤結果。(c)有樂
譜對齊之倍頻軌跡追蹤結果。(d)合成二胡音色的頻譜。(e) 合成小號音色的頻譜。 
五、 結果與討論 
本研究成功的呈現一個適用於擦絃樂器複音音樂訊號的轉換合成系統。在分析的部份，
我們採用以音框為基礎的複音基音預估演算法，先估算出基音候選人後，再將結果輸入至
高階隱藏式馬可夫模型以追蹤倍頻軌跡。為了修正倍頻軌跡的精確度，我們利用 chroma 向
量與動態時間規正演算法來實作樂譜對齊處理，它可以依據輸入的 MIDI 檔案同時校正起
頭音的時間點與基音或倍頻的頻率位置。最後，我們可以計算出每個獨立音源在每個音框
中的能量大小，如此一來，我們便可很輕易的透過 additive 合成演算法將某一首樂曲合成
成另一把樂器，也可以合成出各種不同的演奏手法，如顫音或滑音。 
如要讓本研究的結果更好，有幾個方向可以改進。首先，在分析階段，現今的複音音
高偵測演算法皆還無法做到百分之百準確，錯誤的音高預估將會影響倍頻軌跡追蹤的準確
度，即使輸入訊號是的小提琴獨奏仍容易出錯，更不用說提琴四重奏這類複雜的音樂訊號。
另一方面，有些能量較弱的倍頻軌跡容易在分析時被忽略，如將高階隱藏式馬可夫模型狀
態間的轉換機率修改成動態改變而非固定值，將有助於解決問題。此外，如果 MIDI 樂譜
與原音樂訊號的時間如果相差太多，樂譜對齊處理將容易失敗，因此，發展出片段樂譜對
齊法將更適用於本研究上。最後合成的部份，儘管 additive 合成法在樂器的延續音合成上
已臻完美，但合成樂器的起頭摩擦音其真實度較差，這是可以加強改進的地方。 
9 
七、 計劃成果自評 
本計劃執行迄今，相關成果豐碩：計有將出版之國際期刊論文一篇及會議論文一篇；同
時，更將系統操作過程教學上傳至網路以與同好分享。茲將以上成果簡述如下： 
1. 本研究於 2009 年 7 月在 JISE(Journal of Information Science and Engineering)國際期刊第
25 卷中出版一篇期刊論文”Pitch detection/tracking strategy for musical recordings of solo 
bowed-string and wind instruments”。 
2. 本研究於 2009 年 9 月 3 日在 DAFx09(12th International Conference on Digital Audio 
Effects)國際研討會上發表會議論文”Trans-Synthesis System for Polyphonic Musical 
Recordings of Bowed-String Instruments”。 
3. 整理二胡之音高、音量及定音高雜音參數的萃取，並建立二胡及小號音色庫，以提供合
成所需資訊，並將分析與合成兩部份整合成一系統。 
4. 提供樂譜自動修正功能，節省大部份手動修正參數的時間。 
 
本計劃經自我綜合評估之後，達到比預期還要好的成果，主要歸功於有系統的整體的規
劃及確實實施。研究成果之學術能力不但受到國際會議及國際期刊的肯定，同時，相關的
學術應用價值及推廣效能亦在學生的碩士畢業論文及教學影片等項目上得到實質的回饋功
效。 
 
八、 附錄一、 
 
Pitch Detection/Tracking Strategy for Musical Recording of Solo 
Bowed-String and Wind Instruments 
 
YI-SONG SIAO, WEI-CHEN CHANG AND ALVIN W. Y. SU 
SCREAM Lab., Department of Computer Science and Information Engineering 
National Cheng-Kung University 
Tainan, 701 Taiwan 
 
A robust pitch detection/tracking strategy for solo bowed-string and wind 
musical instrumental recordings is presented. To avoid the missing fundamental 
problem, we adopted the greatest common divisor method and modified it with a 
weighted-and-voting technique that can reveal more information of strong partials 
in the target signal. Moreover, a frame-based correction method with 
consideration of the performing aspects of the instruments is also proposed to 
emendate possible misjudgments in the transition from one note to the next note. 
11 
toward building a synthesis database. Instead, a detailed spectra analysis may obtain both pitch and timbre 
parameters, especially considering specific instrumental characteristics [14]. Building a practical music synthesis 
database, however, lies outside the scope of this paper. So, we focus on extracting a set of useful pitch information. 
The basic procedure is illustrated in Fig. 1. The audio samples are first divided into analysis frames. Then, short-term 
Fourier transform (STFT) is adopted to convert the data into frequency domain. Based on the harmonic assumption 
of tones of target instruments, a method called weighted greatest common divisor and vote (WGCDV) is employed 
to find the possible pitch for each frame. By exploring the relationship among neighboring audio frames according 
to the knowledge of the instruments and performing aspects, a post-processing called frame based correction (FBC) 
is designed to correct the possible errors produced from the previous step. The simulation results showed that the 
proposed approach is more suitable for analyzing solo musical recordings of the target instruments than all 
previously given tools [11]-[13]. 
The rest of the paper is organized as follows. In Section 2, the concept of WGCDV method is introduced and its 
detailed steps are given. FBC is presented in Section 3. Computer simulation and case studies are given in Section 4. 
The performances of different methods are also compared. Conclusions and future works are suggested in Section 
5. 
2. WGCDV PITCH DETECTION METHOD 
Generally speaking, tones of most of sustaining-driven musical instruments, such as violin and trumpet, can 
have a longer lasting pitch than those of plucked or struck string instruments, such as guitar and piano. In this point 
of view, it seems an easier task to extract the pitch information from such specific musical instruments than general 
cases. However, some performing techniques, especially for sustaining-driven musical instruments, will introduce 
lots of obstructions to confuse most pitch detection strategies. For example, there is no fret made for violin and 
Erhu. Thus, players can play fast trill, vibrato, portamento by taping or sliding the fingers on finger board and strings 
or applying the larger bowing pressure. All these are common in bowed-string instrument playing. The pitch 
variation may be over an octave for Erhu playing sometimes. There are other factors that reduce the accuracy of 
some F0 estimation algorithms. For example, the energy levels of the first two or three partials of Erhu are usually 
much weaker than higher partials. Based on our observation, such effects greatly bias the estimate. In our 
experience with different algorithms, if the pitch of a tone is misidentified, it is usually one octave higher or lower 
than the actual pitch. In fewer cases, it is 7 semitones higher than the actual pitch (1.5 times the actual 
fundamental frequency). If it falls into 1/2 semitone range, it is usually very close to the actual pitch identified by an 
invited Erhu player in advance. 
As shown in Fig. 1, WGCDV estimates F0 in three steps: (a) locates the peaks of the transformed magnitude 
information; (b) finds a candidate GCD value for each partial pair using a look-up-table method; (c) weights the 
candidate GCD values according to the spectral energy and determine the final GCD by voting. In the following 
sub-sections, we will discuss each step in more details. 
 
Fig. 2. Location of a peak A in a smoothed spectrum  
where B and C are the left and right valley points. 
 
2.1 Locate possible partial positions 
 
Since our goal is to extract the pitch information from strong harmonic musical signals, we first need to locate 
those large peaks as possible partial positions. After a frame of audio data was transformed into frequency domain, 
we calculate the protrude values of peaks from a smoothed spectrum. In the smoothed spectrum, there are three 
kinds of points, peak point, valley point, and slope point, with respect to local maximum, local minimum, and others. 
Taking Fig. 2 as an example, the protrude value P  of peak point A  then can be defined by 
 
13 
is close to 0.4, its harmonic relation will be (2, 5) and the candidate GCD of this pair will be 5j . 
 
 
2.3 Energy weighted and voting 
 
After we calculate candidate GCDs from all partial pairs in Section 2.2, one needs to choose from them to 
determine F0. Since the critical partials are always of higher energy than most other frequency components, we 
design a weight factor corresponding to each partial pair according to their magnitudes. The advantage is to further 
reduce the effects of inharmonicity and noise. Let i  be the corresponding magnitude of i  and a weight factor 
ijw  for ij  is defined by 
 
 jiijw  ,min .  (2) 
 
To start a voting procedure, all candidate GCDs are roughly assigned to several musical note partitions 
determined by a quantization factor Q , 
 








 5.0
Q
floorc
ij
ij

. (3) 
 
Moreover, an indicator function can be defined by 
 
 


 

otherwise
kcorkcif
k
ijij
ij
0
11
 . (4) 
 
Next, the weighted sum of each partition is evaluated by 
 
    
ji
ijij kwkS
,
 . (5) 
 
The most probable pitch position will fall into the partition of the greatest weighted sum. The centroid method 
[16] is then used to calculate a more accurate pitch position r  by involving all the candidate GCDs in that partition, 
i.e., 
 
  
 kS
kw
r
ji
ijijij 

,

. (6) 
 
With the window size W  and sampling frequency Fs , the estimated fundamental frequency pf  is obtained 
by 
 
Fs
W
r
f p  . (7) 
 
3. FRAME BASED CORRECTION METHOD 
In some occasions, very weak and unstable tones are produced because of light and uneven bowing or blowing 
pressure. In such cases, fundamentals may disappear or the tones are too weak for many pitch detection algorithms 
including the proposed WGCDV method. No matter how accurate a F0 estimation method based on a single audio 
frame is, its accuracy can be improved if the context information from consecutive frames is involved. The basic idea 
of the pitch correction procedure is that the pitch from a note of any musical performance cannot change abruptly. 
Thus, the first step is to segment the source into different note regions. In general, the spectrum has a large change 
in both timbre and energy in the transition region between two notes. A measure is defined in Eq (8) to determine 
the degree of change for two successive frames. 
 
15 
2048 with 50% overlap between two adjacent frames and the STFT window type is Hamming. 
Table 2. Estimation errors with different programs. (2.973% margin) 
 
 
An estimation error rate is designed to evaluate their performances and is able to calculated by 
 
%100
total
error
F
F
e , (9) 
 
where totalF  is the total number of non-silence audio frames and errorF  is the number of frames where 
wrong estimates occur. The actual pitch of each frame is identified manually by a musician who is an Erhu player. 
When the estimated pitch falls within a half of a semitone distance from the actual pitch (about 2.973% margin), it 
is denoted as a correct estimate. Table 2 shows the performances of the methods. While most methods are quite 
good for signals that are easy to analyze, such like the synthetic sound, there are some occasions that can make the 
detectors confused. To illustrate the reliability of these methods in more details, we discuss three special cases as 
follows. 
The first case is the missing fundamental problem. The second tone shown in Fig.5 is a typical missing 
fundamental sound in which the fundamental’s energy is far below the other partials’. Spectrogram clearly shows 
that the energy of the fundamental component (~300Hz) stays below the noise floor. The actual (solid line) and 
estimated pitch contours are indicated in bottom subplot.  
Most detectors mentioned in this paper work well except some reasonable errors due to the strong energies of 
the second and fourth partials. After FBC is applied, most errors can be corrected. It is noted that Praat failed the 
test in the later half of the tone. In addition, perceptual based detectors should be good performers in this regard, 
too [18]. 
The second case is the under-estimated case. The top subplot in Fig. 6 shows its spectrogram in which we can 
observe that there is strong energy appearing in the regions around 05.0 F  and 05.1 F  as well. This often 
appears when Erhu is played with low bowing speed and small bowing pressure. For most frequency-domain 
methods, detection errors easily occur because of its seemingly harmonic structure. Compared to HPS, the 
proposed WGCDV method prevents some misjudgments due to the weighted and voting strategy. 
The third case is the reverberation case. An oboe song synthesized with wavetable synthesis method is used 
for example. Small degree of artificial reverberation created more realism as well as introduced ambiguous 
situations when we tried to detect the fundamental frequency. By observing the spectrogram shown in Fig. 7, we 
can see that a clear harmonic structure remained due to the lingering sound of the preceding tone. It confuses all 
pitch detectors and delays the correct estimation of the new pitch. The WGCDV method is again benefit from the 
weighting and voting strategy and shows better result. 
 
17 
 
Similar experiments and analysis are performed over various bowed-string and wind musical instruments. In 
our experiments, bowed-string instruments are more difficult than wind instruments. The reasons why these 
methods produced unsatisfactory results are quite similar. That is, the testing samples are extracted from 
commercially available compact disks, and they usually contain certain degree of reverberation. The proposed 
WGCDV+FBC method performs well in the provided samples. However, all methods performed poorly if the signals 
are overly reverberant. One overly reverberant example can be heard from [17]. More investigation is required in 
this aspect. 
5. CONCLUSION 
A pitch detection method called weighted greatest common divisor and vote (WGCDV) for recordings of solo 
bowed-string and wind instruments is presented. The proposed method was tested over a wide range of audio 
recordings extracted from commercially available compact disks. The idea of GCD look-up table method makes the 
GCD approach detour its mathematical restriction and provide a more robust estimate than the traditional way. 
Based on the performing aspects of the target instruments, a frame-based correction (FBC) method is also proposed 
to track the pitch contour and improve the existing methods. The proposed strategy is also compared favorably to 
several pitch tools and achieves a better performance in most test recordings. As mentioned in [14], tracking the 
rapid pitch variation more accurately may be more important than finding the exact hertz of a tone. Most listeners 
do not feel the pitch problem with the re-synthesis results if there is no large pitch tracking error. This re-synthesis 
software is also available at [17] for reference. The lightweight computation makes the proposed strategy a practical 
approach to design a real-time analysis and synthesis application for solo bowed-string and wind instruments. 
REFERENCES 
1. Benjamin Kedem, “Spectral analysis and discrimination by zero-crossing,” Proceedings of the IEEE, 
74(11):1477-1493, Nov. 1986. 
2. Rabiner, L.R. and Schafer, R.W., Digital Processing of Speech Signals, Prentice-Hall, 1978, pp. 116-130. 
3. Roads, C. “Autocorrelation Pitch Detection.” Computer Music Tutorial, MIT Press, 1996, 509-511. 
4. Om Deshmukh, Carol Y. Espy-Wilson, Ariel Salomon, and Jawahar Singh, “Use of Temporal Information: 
Detection of Periodicity, Aperiodicity, and Pitch in Speech,” IEEE Transactions on Speech and Audio Processing, 
vol. 13, no. 5, pp. 776-786, September, 2005. 
5. A. M. Noll, “Pitch determination of human speech by the harmonic product spectrum, the harmonic sum 
spectrum, and maximum likelihood estimate,” Proceedings of the Symposium on Computer Processing in 
Communications, April, 1969. 
6. Quast, H., Schreiner, O., Schroeder, M. R. “Robust Pitch Tracking in the Car Environment,” Proceedings of the 
International Conference on Acoustics, Speech, and Signal Processing - ICASSP 2002 Orlando (IEEE 2002). 
7. W. J. Hess, Pitch Determination of Speech Signals, New York: Springer-Verlag, 1983. 
8. W. J. Hess, „„Pitch and voicing determination,‟‟ in Advances in Speech Signal Processing, edited by S. Furui and 
M. M. Sohndi, Marcel Dekker, pp. 3–48., New York, 1992. 
9. D. J. Hermes, „„Pitch analysis,‟‟ in Visual Representations of Speech Signals, edited by M. Cooke, S. Beet, and 
M. Crawford, Wiley, pp. 3–25., New York, 1993. 
10. B. C. J. Moore, An Introduction to the Psychology of Hearing, 4
th
 ed. New York: Academic, 1997. 
11. Boersma, Paul & Weenink, David (2007). Praat: doing phonetics by computer (Version 4.5.13) [Computer 
program]. Retrieved February 3, 2007, from http://www.praat.org/ 
12. Hideki Kawahara, Ikuyo Masuda-Katsuse and Alain de Cheveigné, “Restructuring speech representations using a 
pitch-adaptive time-frequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a 
repetitive structure in sounds,” Speech Communication, 27, 3-4, pp.187-207, 1999. 
13. Alain de Cheveigné and Hideki Kawahara, “YIN, a fundamental frequency estimator for speech and music,” J. 
Acoust. Soc. Amer., vol. 111, pp. 1917–1929, 2002. 
14. Yi-Song Siao, Wei-Lun Chang, and Alvin Su, “Analysis and Transsynthesis of Solo Erhu Recordings Using 
Adaptive Additive/Subtractive Synthesis,” 120th Convention of Audio Engineering Society, Paris, France, May 
20-23, 2006. 
15. H. Järveläinen, V. Välimäki, and M. Karjalainen, “Audibility of the timbral effects of inharmonicity in stringed 
instrument tones,” Acoust. Res. Lett. Online, vol. 2, no. 3, pp. 79–84, 2001. 
16. Honsberger, R., Episodes in Nineteenth and Twentieth Century Euclidean Geometry, Washington, DC: Math. 
Assoc. Amer., 1995. 
17. ERHU ANALYSIS / SYNTHESIS TOOL: http://scream.csie.ncku.edu.tw/~al/ErhuSynth WWW/ 
18. Alain de Cheveigné, “Pitch perception models,” in Pitch, C. Plack and A. Oxenham, Eds. New York: Springer, 
2005. 
 
  
19 
the pitch candidate trajectories. In [5], Chang et. al. use high order HMM based on the results 
produced by the method reported in [3]. In their paper, the scoring machine which involves 
special processing for close or shared partials, is coupled with a tree searching method for 
polyphonic transcription task. Excellent results have been reported in [6]. 
In this paper, a high-order HMM for partial tracking of different F0s modified from the method 
in [5] is proposed. Hence, one can obtain more information about the partials of individual 
source in consecutive frames. Though reasonable results can be achieved, errors do occur. 
Moreover, accurate onset is important musically. The symbolic score-matching algorithm 
proposed in [7] is employed and the alignment approach [8] of polyphonic musical recording to 
symbolic score information by using chroma representation is applied to give onset information. 
The alignment results are also used to correct possible estimation errors produced in the 
previous stage. Onset timing for each source is further calibrated based on the partial tracking 
results, too. The proposed analysis system is now tested on polyphonic musical recordings of 
bowed-string instruments though it is possible to apply it for recordings using other kinds of 
musical instruments. 
Finally, additive synthesis [9] employed in [1] is again used in this paper with acceptable 
synthesis sound quality. In the system, manual control of each partial is equipped. Other 
features include changing the timbres and extra sound effects such as vibrato. 
The overall trans-synthesis system flow diagram is shown in Figure. 1. In the preprocessing stage, 
a simple multiple pitch detection is used. Then, the proposed partial tracking method starts by 
using the previously detected F0s. Score alignment and onset detection are performed with 
respect to the pitch and partial information. Finally, evaluation of energy of each individual 
source for each audio frame is done and additive synthesis is applied to re-synthesize the same 
piece of music using the desired timbre and effects. 
 
Figure 1: the overall trans-synthesis system 
The rest of this paper is organized as follows. The proposed partial tracking is presented in 
section 2. Polyphonic score alignment is described in section 3. The technique to evaluate 
energy of each individual source is presented in section 4. Experimental results are shown in 
section 5. Conclusions are given in section 6. 
MULTIPLE PITCHES AND PARTIALS TRACKING 
FFT (Fast Fourier Transform) is applied to the observed signal to obtain the instantaneous 
spectrum. Then, a simple frame-based multiple-F0 estimation algorithm modified from [2] is 
21 
weights propagated from node to node within the related path. The observation probability 
emitted by the sustain state is defined as a Gaussian distribution and calculated as 
 
 
2 2
2 2
1 2 1 2
1
, exp
2 2 2
f m
f m
   
      
            
     
,                                   (1) 
 
where f and m are the frequency difference and the magnitude difference between two 
concerned partial candidates, respectively. It is noted that the frequency of a possible partial is 
first converted to tone scale and its magnitude is converted to dB scale. In this paper, 1  is set 
to 0.25, which corresponds to one quarter tone and 2  is set to 1.0, which corresponds to 1dB. 
 
Figure 2: HMM note model with the attack probability λ and the sustain probability ψ: (a) graphical representation; (b) 
state transition weight matrix. 
 
Figure 3: trellis structure of the 2nd order HMM model. 
SCORE ALIGNMENT AND RE-SYNTHESIS 
After extracting the trajectories of F0s and partials, errors such as detection miss of octave 
and/or quint pitches may still occur. Therefore, these errors can be corrected if score 
information is available. For most pieces of classical or popular music, there are printed scores, 
audio recordings, and often MIDI files created by manual transcription or score conversion. In 
this paper, we use MIDI files though other forms are always possible. Techniques used in music 
23 
( , 1)
( , ) min ( 1, 1) ( , )
( 1, )
D i j
D i j D i j dist i j
D i j
 
 
    
  
,                          (3) 
where  
 
 
12
2
1
( , ) ik ik
k
dist i j s t

  .                                (4) 
 
The initial condition is set as: ( ,1) , 2D i i   and (1, )D j   ( , )dist i j , for all i. In (4), iks  is the  k-th 
element of the i-th input chroma vector, jkt  is the k-th element of the j-th reference chroma 
vector, and D(i, j) is the overall DTW distance. 
 
After the DTW score alignment, the partial trajectories that do not coincide with the score are 
eliminated. Moreover, the onset timing obtained from the score alignment may not be very 
accurate. It is known that onset timing is very critical for music performance. Usually, the onset 
timing found in the score alignment part deviates from the correct onset by several audio 
frames. Therefore, it has to be calibrated by using the results obtained in the partial tracking 
part. Define the energy of the i-th frame as 
 
 
/ 2 1
2
0
( )
N
i i
j
E A j


                                    (5) 
 
The following equation is used to decide whether the i-th frame is an onset frame for an 
individual source. 
 
11       ,if / and  
( )
0       ,
i i iE E E T
Onset i
otherwise
  
 

,                          (6) 
 
where α is set as 2 in our experiments and energy has to be greater than a certain threshold. 
It is noted that the score alignment may fail when the score (the MIDI file) has certain degree 
of difference from the audio recording. In such cases, manual correction of the score file will be 
necessary. 
After the information about F0, partial tracking and onset is obtained, the energy for each 
individual source has to be measured. We sum up the spectral energy of the corresponding 
partial group for each audio frame. Accurate partial tracking produce good results when 
calculating the individual source energy. In real-world case, human beings are not sensitive to 
loudness differences among partials [13]. If the energies and frequencies of two notes are 
sufficiently close for a specific instrument, their corresponding partials are also close to each 
other. In [1], a parametric table is built for a Chinese traditional bowed-string instrument, Erhu, 
for some F0 frequencies and energies of 20 partials of each F0. The partial energies of 
25 
 
(a) 
    
(b)                                (c) 
Figure 7: The second test signal: (a) spectrogram; (b) partial tracking 
without score alignment; (c) partial tracking with score alignment. 
 
 
(a) 
    
(b)                                  (c) 
 
