I專題研究計畫名稱:
以統計式中對中機器翻譯模式為基礎之中文繁簡對應詞彙採擷模型
A Statistical Chinese-to-Chinese Machine Translation Model for Mining Aligned
Simplified-Traditional Chinese Terms
摘要:
區域性的用語差異, 可能造成不同地區民眾溝通的重大障礙, 即便使用相同的語言核心,
也是一樣. 這種情形在中港台三地極為明顯. 雖然這些地區都使用中文為核心, 但在使
用的詞彙上, 卻有相當顯著的差異. 比方說, 台灣以 “雷射” 來代表 “laser”, 而同
樣的詞彙, 在中國大陸地區則是翻譯為 “激光”; 兩者不但使用了不同的字元, 在意
義表達方面, 也用了相當不一樣的方式. 同樣的區域性差異, 也發生在使用漢字於不同
語言的狀況. 最明顯的例子, 就是中文跟日文.
為了去除這樣的語言障礙, 方便異地資訊的處理與應用, 為兩地編撰一部大型的翻譯辭
典可能是個必要的措施. 本研究將這樣的區域性用語差異, 視為一種特殊的語言翻譯問
題. 為此, 我們提出了一個 “中對中統計式機器翻譯模式” (C2C SMT model), 以便將
大陸地區 (簡體中文, SC) 的特殊用語, 連結到台灣地區 (繁體/正體中文, TC) 的同義特
用詞彙. 利用一個以 EM Algorithm 為基礎的非教導式機器學習模式, 我們預期可以由
大量的繁簡非平行語料, 自動或半自動地建構一部大型的 “繁簡用語翻譯辭典”, 以
去除區域性差異所造成的語言障礙.
關鍵詞:
繁簡對應詞彙採擷, 詞彙抽取, 中對中機器翻譯模式, 統計式機器翻譯系統
11 Introduction & Motivation
1.1 Problems with Variations between Simplified and Traditional Chinese
Regional variation in language usage is an important barrier for communication between
people of different regions, even though they may share a common core of one language. This
situation had been observed across the mainland China, Hong Kong and Taiwan regions,
where the Chinese language is used as the same core but with different variations in
vocabularies. For instance, while “雷射”is used in Taiwan for “laser”, the same term is
expressed as “激光”in the mainland China, which not only uses different characters but also
represents the same meaning in a very different way. The variation had also been observed
even across regions that use Chinese characters in different languages. This is exactly the
situation between the Chinese and Japanese languages.
In language processing applications, such regional variation will result in difficulties that
are frequently observed in cross-language applications. For instance, to acquire information
from a search engine, one may have to provide the two forms of “laser”as shown above in
order not to miss any interesting information for things like “laser printer”. Therefore,
documents with such variant terms should be categorized to the same class.
On the other hand, such variant terms can be regarded, in some sense, as synonyms.
Therefore, word sense disambiguation tasks using mixed corpora with regional variations as
the training materials may have to normalize the corpora to the same canonical form before
being used for training. In summary, many information-processing applications will suffer
from such regional variation if correct translation between the variant terms cannot be well
resolved.
In particular, the current research is interested in the variation of term usage between the
Taiwan region, which uses traditional Chinese (TC) characters in documents, and the
mainland China region, which uses simplified Chinese (SC) characters in documents. The
reason is clear due to the booming economical and political interaction across the two regions.
1.2 Statistical Machine Translation Model for SC-TC Term Alignment
To remove the above-mentioned language barrier in information processing applications,
a large-scale translation lexicon for two different regions might be required. In the current
research, the variation is modeled as a special language translation problem. In particular, a
Chinese-to-Chinese Statistical Machine Translation (C2C SMT) model is proposed to“align”
terms that are specific to the mainland China region (where simplified Chinese (“SC”)
characters are used) and those specific to the Taiwan region (where traditional Chinese (“TC”)
characters are adopted.). With an unsupervised learning process, based on the EM algorithm,
and large non-parallel SC/TC text corpora or webpages, it is expected that a large“translation
lexicon”between the simplified Chinese terms and traditional Chinese terms can be
automatically or semi-automatically compiled to remove the regional language barrier
between the two regions.
By modeling the problem as a C2C machine translation problem, one might quickly
jump to the though that the state-of-the-art SMT models [Brown 90, 93] and tools, such as
GIZA++ [Och 00a, 00b, 04], could be used if parallel TC-SC corpora are available.
3To find the best traditional Chinese term, ‘t’, corresponding to a simplified Chinese term
‘s’in the simplified Chinese lexicon, DS, is equivalent to finding the one with the highest a
posterior probability,  |P t s , among those terms in the traditional-specific dictionary DT.
That is,
 
 
* arg max |
arg max ,
: TC-specific term
:SC-specific term
: TC-specific dictionary
:SC-specific dictionary
T
T
S
t D
t D
T
S
t P t s s D
P t s
t
s
D
D


 

(1)
Since there are so many terms that are specific to the traditional Chinese texts and
simplified Chinese texts respectively, it is not easy to identify the correct correspondence
without consulting the context of the simplified terms and the traditional terms. Intuitively, if
a sub-window , ,s sl s r is“similar”to the triple , ,t tl t r , where the former is formed by the
left context, sl , of a simplified Chinese term (s), the simplified Chinese term, s, itself and its
right context sr , and the latter is a sub-window of the context for a traditional Chinese term, t,
where ,t tl r are its left/right neighbors, then s and t are likely to be translation equivalent of
each other. In other words, Equation (1) can be modeled as
 
 
, : , ,
, : , ,
*
, : , ,
, : , ,
( , ) , , , , ,
arg max , , , , ,
: traditional Chinese text corpus
: simplified Chinese text corpus
t t t t T
s s s s S
T t t t t T
s s s s S
t t s s
l r l t r T
l r l s r T
t t s s
t D l r l t r T
l r l s r T
T
S
P t s P l t r l s r
t P l t r l s r
T
T


 




 (2)
Note that, the degree of “similarity”between , ,s sl s r and , ,t tl t r might be easier to
estimate than the degree of “equivalence”between s and t, which can not be judged directly
without any contextual information. For example, if s tl l then the two triples will be
similar to some extent; if, in addition, s tr r , then the“similarity”will be further enhanced.
The above formulation therefore provides a feasible way to divide the complicated
context-free s-to-t translation problem into a large number of context-dependent translation
sub-problems, each of which is easier to resolve. Also, the summation operation, over all
contextual windows, suggests that the probability  ,P t s is contributed by each , ,s sl s r
and , ,t tl t r pair; those pairs with higher similarity will contribute more than those that are
unrelated. With such formulation, one may hopefully estimate  ,P t s easier with more
information.
5     
1 0 1 1 0 1
1 0 1 1 0 1
, , , ,
ˆ , , , , , , ,
ja j
t t t s s s A j
c t s P A t t t s s s t t s s 
 
     (5)
where  1 if event is true
0 if otherwise
e
e 

.
The expected counts, in turn, can be used to estimate  ,P t s as
    
,
ˆ ,ˆ ,
ˆ ,
t s
c t s
P t s
c t s
 , (6)
and  1 0 1 1 0 1, , , , , ,P A t t t s s s  can be re-estimated according to Equation (4). The training
process then repeat itself until it converges (or a maximum number of training iteration is
reached.) The term alignment process, based on the EM training process, can be summarized
as follows.
EM Algorithm for Term Alignment:
Step1 (Identify TC-specific terms from SC-specific terms): Acquire a large TC-specific dictionary and
a SC-specific dictionary. Divide the terms in the two dictionaries into three categories, namely
TC specific-terms (DT), SC-specific terms (DS) and common vocabularies (DTS). The terms in DT
and DS will then be the target for term alignment.
Step2 (Generate contextual windows): Apply a word segmentation algorithm, such as [Chiang 92, Lin
93], to the SC documents and TC documents, using the respective SC dictionary and TC
dictionary, respectively. Then extract a sub-window of word tokens around SC-specific terms in
the SC corpus, and do the same for TC-specific terms with the TC corpus. On the other hand, if
the web corpus is to be used as the training corpus, the SC-terms (or TC-terms) can be
submitted to a search engine, and the search results (or snippets) can then be used to
generate the context windows for matching. Without loss of generality, the algorithm will
assume that each sub-window consists of a word n-gram with the same number of word tokens to
the left/right of a TC-specific or SC-specific term.
Step3 (Initialize alignment probability): For each pair of TC-specific context window and SC-specific
context window, assign an initial probability to  1 0 1 1 0 1, , , , , ,P A t t t s s s  for all possible
alignments in A.
Step4 (E-Step: Compute expected counts of s-t term pairs): Estimate the expected counts  ˆ ,c t s for
each TC-specific term t, and SC-specific term s, using Equation (5).
Step5 (M-Step: Re-estimate the model parameters and alignment probabilities): Once the expected
counts are available, the estimated joint probability  ˆ ,P t s for each term pairs can be estimated
according to Equation (6). The alignment probability can then be re-estimated using Equation (4).
Step6 (Repeat the EM Steps4 ~5): The training process can be repeated until converge or until a
maximum number of iterations is reached.
Step7 (Extract highly probable translation pairs): Extract those <s, t> pairs with high joint
probabilities as the translation lexicon.
7each other at the respective iteration. For instance, ‘激光’and ‘雷射’was initially not
recognized as an aligned term pair, but they begin to be correctly paired from iteration-3.
As expected, the translation probabilities of correct alignment pairs get higher and
higher scores with the help of the EM training process. This shows that the proposed EM
algorithm did reduce the estimation error of the SMT model, and thus improve the
accuracy of S-T term alignment, iteratively.
S-T Term Pairs iteration0 iteration1 iteration2 iteration3 iteration4 iteration5
互聯網:網際網路 -5.80539 -3.92528 -2.76368 -1.88505 -1.22829 -0.75455
打印機:印表機 -4.72155 -3.01915 -2.10074 -1.39606 -0.85826 -0.46924
激光:雷射 -4.45118 -3.01697 -2.41179 -1.9353 -1.53044 -1.16807
Table 3. Improved estimation with EM for correct term pairs.
3.2.2 Comparison with Previous Works
To know the improvement in S-T term alignment accuracy, the same contextual
windows data set used in [Chang 07] is also used with the EM algorithm in order to
compare the EM-based results with this previous work. [Chang 07] used 4 different
criteria in measuring the similarity between two contextual windows. Such similarity
scores are, in fact, kinds of expected counts in proportional to the number of matched
contextual words or characters within words; such highly simplified expected counts are
used in turn to estimate the joint probabilities of S-T pairs. The 4 criteria (or sub-models)
are listed as follows. Throughout the comparisons, we will run the Lw-Rw sub-model for
the EM algorithm, and compare the results with those of the previous work. In particular,
the results of the 4-th iteration (iteration4) will be used for comparison since it almost
converges there.
Sub-Model Description
Lc-Rc Ignore center terms. Estimate similarity based on characters in left &
right contexts.
Lw-Rw Ignore center terms. Estimate similarity based on words in left & right
contexts.
Lw-Cc-Rw Estimate similarity based on characters in center terms and words in left
& right contexts.
Lc-Cc-Rc Estimate similarity based on characters in center terms, left & right
contexts.
Table 4. Criteria for evaluating similarity between two contextual windows.
In comparison with the Lw-Rw model of the previous work, it is clear that the EM
model outperforms significantly in all the three different domains. In particular, the
improvement in the transliteration domain (D2) is surprisingly impressive. The results
are shown in Table 5.
9In the previous work, both Lw-Cc-Rw and Lc-Cc-Rc models are better than the
Lw-Rw model, probably due to the additional match against the center term and more
partial matches at the character level. To our surprise, without consulting the characters
in the center term of the contextual windows, the EM/Lw-Rw model remains better
than the Lw-Cc-Rw model of the previous work in the technical and transliteration
domains. The results are shown in Table 7.
Previous Work EM(Lw-Rw)
domain Direction # of Correct Acc # of Correct Acc △%
S2T 96/136 70.59% 96/136 70.59% +0.00%
T2S 87/136 63.97% 99/136 72.79% +8.82%D1
S2T+T2S 101/136 74.26% 102/136 75.00% +0.74%
S2T 47/82 57.32% 63/82 76.83% +19.51%
T2S 52/82 63.41% 66/82 80.49% +17.07%D2
S2T+T2S 47/82 57.32% 70/82 85.37% +28.05%
S2T 84/128 57.32% 60/128 46.88% -10.44%
T2S 75/128 63.41% 61/128 47.66% -15.76%D3
S2T+T2S 77/128 57.32% 64/128 50.00% -7.32%
Table 7. Comparison with previous work using Lw-Cc-Rw model.
(EM: Lw-Rw model, iteration4)
Finally, in comparison with the best model of the previous work, the EM estimation
with its Lw-Rw model is still comparable in performance. The improved estimation
offsets the worse model, and gains a comparable performance in the end. The results
are shown in Table 8.
Previous Work EM(Lw-Rw)
domain Direction # of Correct Acc # of Correct Acc △%
S2T 101/136 74.26% 96/136 70.59% -3.68%
T2S 97/136 71.32% 99/136 72.79% +1.47%D1
S2T+T2S 95/136 69.85% 102/136 75.00% +5.15%
S2T 52/82 63.41% 63/82 76.83% +13.41%
T2S 50/82 60.98% 66/82 80.49% +19.51%D2
S2T+T2S 53/82 64.63% 70/82 85.37% +20.73%
S2T 91/128 63.41% 60/128 46.88% -16.54%
T2S 69/128 60.98% 61/128 47.66% -13.32%D3
S2T+T2S 80/128 64.63% 64/128 50.00% -14.63%
Table 8. Comparison with previous work using Lc-Cc-Rc model.
(EM: Lw-Rw model, iteration4)
These comparisons clearly indicate that the EM re-estimation algorithm greatly
boost the S-T term alignment performance.
11
Bibliography (參考文獻)
[Brown 90] Brown, Peter F., J. Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra,
Frederick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. “A 
statisticalapproach to machine translation.”Computational Linguistics, 16(2):79–85.
[Brown 93] Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and R. L.
Mercer. 1993. “The mathematics of statistical machine translation: Parameter estimation.” 
Computational Linguistics, 19(2):263–311.
[Chang 97a] Chang, J.-S, Automatic Lexicon Acquisition and Precision-Recall Maximization
for Untagged Text Corpora, Ph.D. dissertation, Department of Electrical Engineering,
National Tsing-Hua University, Hsinchu, Taiwan, R.O.C., July, 1997.
[Chang 97b] Jing-Shin Chang and Keh-Yih Su, "An Unsupervised Iterative Method for
Chinese New Lexicon Extraction", International Journal of Computational Linguistics and
Chinese Language Processing (CLCLP), vol. 2, no. 2, pp. 97-148, August, 1997.
[Chang 07] Jing-Shin Chang and Chun-Kai Kung, "A Chinese-to-Chinese Statistical Machine
Translation Model for Mining Synonymous Simplified-Traditional Chinese Terms,"
Proceedings of Machine Translation Summit XI, pages 81-88, Copenhagen, Denmark,
10-14, September, 2007.
[Chiang 92] Tung-Hui Chiang, Jing-Shin Chang, Ming-Yu Lin and Keh-Yih Su, "Statistical
Models for Word Segmentation and Unknown Word Resolution," Proceedings of
ROCLING-V, pp. 123-146, Taipei, Taiwan, R.O.C., 1992.
[Dempster 77] Dempster, A. P., N. M. Laird, and D. B. Rubin, “Maximum Likelihood from 
Incomplete Data via the EM Algorithm”, Journal of the Royal Statistical Society, 39 (b), pp.
1-38, 1977.
[Emerson 05] Emerson, Thomas. (2005). "The Second International Chinese Word
Segmentation Bakeoff", In Proceedings of the Fourth SIGHAN Workshop on Chinese
Language Processing, Oct. 14-15, 2005, Jeju Island, Korea.
[Lin 93] Lin, Ming-Yu, Tung-Hui Chiang and Keh-Yih Su, "A Preliminary Study on
Unknown Word Problem in Chinese Word Segmentation," Proceedings of ROCLING VI,
pp. 119-142, 1993.
[Och 00a] Och, Franz J. and Hermann Ney. 2000. “Acomparison of alignment models for
statistical machine translation.” In COLING’00: The 18th International Conference on
Computational Linguistics, pages 1086–1090, Saarbr ¨ ucken, Germany, August.
[Och 00b] Och, Franz Josef and Hermann Ney. 2000. “Improved statisticalalignment
models.” In Proceedings of the 38thAnnual Meeting of the ACL, pages 440–447.
[Och 04] Och, Franz Josef and Hermann Ney. 2004. “The alignmenttemplate approach to
statistical machine translation.”Computational Linguistics, 30:417–449.
[Sproat 03] Sproat, Richard and Thomas Emerson. “The First International Chinese Word 
Segmentation Bakeof”, 2003. (http://www.sighan.org/bakeoff2003/bakeoff_instr.html)
