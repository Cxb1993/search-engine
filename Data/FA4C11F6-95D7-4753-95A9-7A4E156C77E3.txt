行政院國家科學委員會補助專題研究計畫 ■ 成 果 報 告   
□期中進度報告 
 
（部份可觀的馬可夫鏈之風險敏感控制） 
 
計畫類別：■ 個別型計畫  □ 整合型計畫 
計畫編號： NSC  99-2221-E-005-068- 
執行期間： 99 年 8 月 1 日至 100 年 7 月 31 日 
 
計畫主持人： 許舜斌 
共同主持人： 無 
計畫參與人員： 吳鎮亞，秦輝烟 (研究生) 
 
 
成果報告類型(依經費核定清單規定繳交)：■ 精簡報告  □完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
■出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
          
執行單位：中興大學 
 
中   華   民   國  100   年   10  月   31   日 
2Trans. Kernel
P (·|Xk−1, Ak−1)
Controller
U : X → A
Evaluation
Criteria
- -
ﬀ ﬀ
- ﬀ
A
Xk
Fig. 1. Control of Markov chain with perfect observation
The standard approach to dealing with the model with partial observation is to extend the system space
from X to Ψ = P(X), the space of probability distributions on X or called the probability simplex
whose element is the probability distribution [ψ(1), ψ(2), · · · , ψ(NX)]. Using Bayesian estimation we can
estimate the system state x via the observation y and obtain a probability distribution called information
state that satisfies
ψt+1 = T (ψt, yt+1, at) :=
∑
y∈Y
ψt ·Q(yt+1, at)
ψt ·Q(yt+1, at) · 1c · 1{yt+1=y} ,
where 1c is the column vector of 1’s and 1{E} is indicator function whose value is 1 as E holds and is 0
otherwise.
For the new generalized stochastic system whose system state, called information state, is the probability
distribution among the original finite system states, its evolution is governed by the transition law:
K(B|ψ, a) := Ppiψ0{ψt+1 ∈ B|ψt = ψ,At = a} , (1)
where t is a natural number and B ∈ B(Ψ ), the sigma algebra induced by Ψ . Given ψt, if decision at is
chosen, yt+1 occurs with probability
V (ψt, yt+1, at) := ψt ·Q(yt+1, at) · 1c .
We can rewrite (1) as
K(B|ψt, at) =
∑
yt+1∈Y
V (ψt, yt+1, ut) · 1{T (ψt,yt+1,at)∈B} .
The incurred cost now becomes
c˜(ψ, a) =
∑
x∈X
c(x, a)ψ(x) ∀ψ ∈ Ψ , a ∈ A .
The flow chart for the operation of Markov chain with imperfection observation is in Figure 2.
The objective function we consider for the risk-sensitive model is
Jγ(ψ0, pi) = lim sup
T→∞
1
γ
1
T
logEpiψ0
(
eγ
∑T−1
t=0 c(xt,at)
)
(2)
where γ > 0 and Epiψ0 is the expectation operator induced by the initial information ψ0 and the policy pi
under which the controller assigns the map U according to the histories of system states and actions. A
policy is called optimal if it minimizes this objective function.
4To make ψ˜t a probability distribution, we define its normalized version ψˆt = ψ˜t/ψ˜t1c, which satisfies
ψˆ0 = ψ0 ,
ψˆt+1 =
ψˆtQˆ(yt+1, at)
ψˆtQˆ(yt+1, at)1c
:= Tˆ (ψˆt, yt+1, at) ,
(11)
and rewrite (10) as
Jγ(ψ0, pi) = lim sup
T→∞
1
T
1
γ
logE+ψ0
[
T−1∏
t=0
(
ψˆt · Qˆ(yt+1, at) · 1c
)]
= lim sup
T→∞
1
γ
1
T
logE+ψ0
(
e
∑T−1
t=0 cˆ(ψˆt,yt+1,at)
) (12)
where
cˆ(ψˆ, y, a) := log(ψˆ · Qˆ(y, u) · 1c) . (13)
Suppose µ1 and µ2 are two discrete probability distribution vectors. Their Kullback–Leibler distance is
defined as
R(µ1‖µ2) :=

I∑
i=1
µ1(i) log
(
µ1(i)
µ2(i)
)
if µ1 ≪ µ2 ,
∞ otherwise ,
(14)
where µ1 ≪ µ2 means µ1 is absolutely continuous with respect to µ2. That is, µ2(E) = 0 implies
µ1(E) = 0 for any event E. By Jensen’s inequality it can be proved that
logEµ1eh(I) ≥
I∑
i=1
µ2(i) log
(
eh(i)
µ1(i)
µ2(i)
)
= Eµ2h(I)−R(µ2‖µ1) ,
(15)
where the equality in (15) holds as
µ2(i) =
µ1(i)e
h(i)∑I
i=1 µ1(i)e
h(i)
. (16)
Applying (15) to (10), we obtain a two-controller, or two-player stochastic game
γJγ(ψ0, pi, λ) = lim sup
T→∞
1
T
Epi,λψ0
[
T−1∑
t=0
(
cˆ(ψˆt, yt+1, at)−R(λt+1‖UY )
)]
, (17)
where UY is uniformly distributed. The stochastic game evolves as follows: at time t, the first player takes
action at based on state ψˆt, the second player takes action λt based on ψˆt and at. Player 1 pays
cˆ(ψˆt, yt+1, at)−R(λt+1‖UY ) , (18)
to player 2 and the time moves on. The normalized information state ψˆt moves to ψˆt+1 according to (11).
6Furthermore, since h¯β(ψ) := hβ(ψ) − hβ(ψ∗) is convex, if for β ∈ (0, 1), h¯β(ψ) is uniformly bounded,
namely h¯β(ψ) is bounded for ψ ∈ Ψ , then h¯β(ψ) can be shown to be equi-Lipschitzian and thus equi-
continuous. By Arzela-Ascoli theory there exists a subsequence {βn′a}∞a=1 and a continuous and convave
function h(ψ) such that
h¯βn′a
(ψ) −→ h(ψ) as βn′a → 1 . (29)
We thus have the long-run average version of optimality equation
eΓ+h(ψ) = inf
a∈A
∫
ecˆ(ψ,y,a)+h(Tˆ (ψˆ,y,a))UY (dy) , (30)
or
eΓ+h(ψ) = inf
a∈A
{
E+ψe
cˆ(ψ,y,a)+h(Tˆ (ψˆ,y,a))
}
. (31)
To see the optimality, note that
E+ψe
cˆ(ψ,y,a)+h(Tˆ (ψˆ,y,a))−Γ−h(ψ) ≥ 1 . (32)
As a result,
E+ψe
∑T−1
t=0 cˆ(ψt,yt+1,at)+h(ψ0)−TΓ−h(ψT ) ≥ 1 , (33)
lim
T→∞
1
T
logE+ψe
∑T−1
t=0 cˆ(ψt,yt+1,at) ≥ Γ , (34)
lim
T→∞
1
T
logEpiψ0e
∑T−1
t=0 c(xt,at) ≥ Γ . (35)
The above derivation justifies the optimality of the policy satisfying (30), in the sense that it achieves
the minimum long-run average cost for the risk-sensitive performance index. The remaining section is
dedicated to the sufficient conditions for the uniform boundedness of h¯β(·), β ∈ (0, 1).
II. MAIN RESULTS
Consider the assumption:
Assumption 2.1: There exist constants ε > 0, N0 ∈ N and β0 < 1 such that ∀β ∈ [β0, 1) we have
max
1≤k≤N0
Ppiβψ∗{T (ψ∗, Y k, Uk−1) ≥ ε · T (ψ∗, Y k, Uk−1) ,
V (ψ∗, Y k, Uk−1) ≥ ε · V (ψ∗, Y k, Uk−1)} ≥ ε .
(36)
where ψ∗ = arg suphβ(ψ).
This assumption has been used in the risk-neutral case in [18] and is satisfied by a wide class of
practical problems. We will see that this assumption is also the sufficient condition in the risk-sensitive
case, by looking at the following result:
Lemma 2.1: Suppose for k = 1, 2, · · · , nonnegative matrices A(k) and B(k) have the same size and
A¯ :=
∏
k A
(k), B¯ :=
∏
k B
(k). Let a(k)i,j , b
(k)
i,j , a¯i,j , b¯i,j be the components of A
(k), B(k), A¯ and B¯ respectively.
If for each i, j, k, a(k)i,j and b
(k)
i,j are both zeros or both non-zeros, then so are a¯i,j and b¯i,j .
Theorem 2.2: Under Assumption 2.1, let {βn}∞n=1 ⊂ [β0, 1) be a sequence and βn → 1, then the
sequence of differential discounted value functions {hβn(·)}∞n=1 is uniformly bounded on Ψ .
Proof: By Lemma 2.1, the definition in (9) and (11), Assumption 2.1 is equivalent to that there exist
constants ε1 > 0, N0 ∈ N and β0 < 1 such that ∀β ∈ [β0, 1) we have
max
1≤k≤N0
Ppiβψ∗{Tˆ (ψ∗, Y k, Uk−1) ≥ ε1 · Tˆ (ψ∗, Y k, Uk−1) ,
V (ψ∗, Y k, Uk−1) ≥ ε1 · V (ψ∗, Y k, Uk−1)} ≥ ε1 .
8= N0‖c‖∞ + βk
∑
yk∈Ykc
(V (ψ∗, yk, uk−1∗ )− ε · V (ψ∗, yk, uk−1∗ ))
× {hβ(Tˆ (ψ∗, yk, uk−1∗ ))− hβ(ψ∗)}
− βk
∑
yk∈Ykc
V (ψ∗, yk, uk−1∗ )(1− ε){hβ( ˆ˜ψ(yk))− hβ(ψ∗)}
+ βk
∑
yk∈Yk∗\Ykc
V (ψ∗, yk, uk−1∗ ){hβ(Tˆ (ψ∗, yk, uk−1∗ ))− hβ(ψ∗)}
− βk
∑
yk∈Yk∗\Ykc
V (ψ∗, yk, uk−1∗ ){hβ(Tˆ (ψ∗, yk, uk−1∗ ))− hβ(ψ∗)} .
Let β=1, drop the minus terms and replace the appropriate terms in {·} with ∆h, then we have
∆h ≤ N0‖c‖∞ +
∑
yk∈Ykc
(V (ψ∗, yk, uk−1∗ )− ε · V (ψ∗, yk, uk−1∗ ))∆h
+
∑
yk∈Yk∗\Ykc
V (ψ∗, yk, uk−1∗ )∆h
≤ N0‖c‖∞ + (1− ε2)∆h .
Thus,
hβ(ψ) ≤ ∆h ≤ N0‖c‖∞
ε2
,
and the proof is completed.
Remark 2.1: Currently the representative sufficient conditions include those by Di Masi and Stettner
[22] and Fleming and Herna´ndez-Herna´ndez [12]. Their ideas are to apply the general ergocity condition
for general state Markov processes to this particular case with finite state case but with imperfect
observation. The condition requires the positivity or primitivity of the transition matrix Q(y, u). Namely,
for each y and u, Q(y, u) should be positive (component-wisely) or there exists a finite number T such
that the matrix
∏T
i=1Q(yi, ui) is positive. However, these conditions are not satisfied by some practical
problems. For example, suppose the system state space, observation space and action space all have two
elements, and the Markov transition matrix is
P (0) =
[
1− η η
0 1
]
, P (1) =
[
1 0
1 0
]
. (37)
Also, the observation and the reality follow the probability relation
Prob(y|x) :=
{
q if x = y
1− q otherwise . (38)
That is,
Q(y, a) = P (a)O(y) , a, y ∈ {0, 1} , (39)
where
O(0) :=
[
q 0
0 1− q
]
, O(1) :=
[
1− q 0
0 q
]
.
The cost c(1, 1) = c(2, 1) > c(x, 0) > 0 for x ∈ {0, 1}. Clearly the positivity or primitivity conditions
are not satisfied due to the appearance of zeros in P (0) and P (1). Chuang [9] gave an elegant sufficient
condition that asks the information state, starting from any vertex of the probability simplex Ψ and under
the optimal policy, to enter the interior of the Ψ in finite time steps with positive probability. For this
example, he showed that under the optimal policy, the corresponding actions for information states [1, 0]
10
[22] G. B. Di Masi and L. Stettner, ”Risk-sensitive control of discrete-time Markov processes with infinite horizon, SIAM J. Control and
Optimization, 38 (1999), 61–78.
[23] G. B. Di Masi and L. Stettner, ”Infinite horizon risk sensitive control of discrete time Markov processes with small risk, Systems and
Control Letters, 40 (2000), 15–20.
[24] A. Jas´kiewicz, ”Average optimality for risk-sensitive control with general state space, Annals of Applied Probability, 17 (2007), 654–675.
[25] L. Stettner, ”Risk sensitive portfolio optimization, Mathematical Methods of Operations Research, 50 (1999), 463–474.
[26] W. J. Runggaldier and L. Stettner, Approximations of discrete time partially observed control problems, Applied Mathematics
Monographs 6, Giardini Editori E StampatoriIn Pisa, Italy, 1994.
[27] P. Whittle, Risk-sensitive Optimal Control, John Wiley & Sons, New York 1990.
Control of Continuous-Time Markov Chains With Safety Upper Bounds
Shun-Pin Hsu
Abstract— This work introduces the notion of safety for the
controlled Markov chains in the continuous-time horizon. The
concept is a non-trivial extension of safety control for stochastic
systems modeled as discrete-time Markov decision processes,
where the safety means that the probability distributions of
the system states will not visit the given forbidden set at any
time. In this paper an unit-interval valued vector that serves
as an upper bound on the state probability distribution vector
characterizes the forbidden set. A probability distribution is
then called safe if it does not exceed the upper bound. Under
mild conditions the author derives two results: 1) the necessary
and sufficient conditions that guarantee the all-time safety of
the probability distributions if the starting distribution is safe,
and 2) the characterization of the supreme set of safe initial
probability vectors that remain safe as time passes. In particular
the paper identifies an upper bound on time and shows that if
a distribution is always safe before that time, the distribution is
safe at all times. Numerical examples are provided to illustrate
the two results.
I. INTRODUCTION
The Markov chain is one of the most common model for
the dynamic system that evolves stochastically with time.
The popularity comes largely from its conceptual simplicity
and theoretic elegance. If the evolution rule of the system
modeled by the Markov chain can be changed by manipulat-
ing the parameters of the chain, the dynamics of the system
is said to follow a controlled Markov chain, which is also
known as the Markov decision process. The major purpose
of modeling the system by a controlled Markov chain is to
find out the best way to operate the system. The solution
of the model is called the optimal policy in the sense that
implementing the policy leads to the minimum cost or the
maximum reward. Conventionally, the cost or reward is an
accumulated amount over the time horizon and reflects the
long-run or statistically average performance of the policy
(see, for example [1], [5]–[7]). The determination of the
optimal policy according to such cost or reward is meaningful
only if the short-term poor performance of the policy can be
tolerated. In case the stochastic system is highly sensitive
to some singular situation and should probabilistically avoid
visiting some states at any moment, implementation of the
optimal policy constructed by the traditional approach seems
inappropriate.
In this paper we introduce the safety concept for the
continuous-time model of controlled Markov chains. The
result is a non-trivial extension of the discrete-time case
This work was supported by National Science Council of Taiwan
under grant 97-2221-E-005-062.
Shun-Pin Hsu is with the Department of Electrical Engi-
neering, National Chung-Hsing University, Taiwan (email:
shsu@nchu.edu.tw).
proposed previously (see [2]–[4]). We first define the set
of safety specification, which is characterized by an unit-
interval valued vector as the upper bound. A state probability
vector is called safe if it is in the set, or equivalently if each
element of the probability vector does not exceed the speci-
fied upper bound. Our major task is then to ensure the safety
of the probability vectors at any moment. Since the evolution
of the probability distributions of system states depends on
the initial condition and the governing policy, we might
choose an appropriate starting distribution corresponding to
the given policy to complete the task. Suppose the collection
of such appropriate starting distributions is called the set
of safe initial distributions. Apparently this set is a subset
of the set of safety specification. Our first result provides
the necessary and sufficient condition for the set of safety
specification itself to be the set of safe initial distributions.
When the condition is satisfied, the controller is called safety
enforcing since under this policy any starting distribution that
is safe remains safe as time passes. As the condition fails, the
set of safe initial distributions becomes the strict subset of the
set of safety specification. Our second result then presents
the formulation to characterize this subset. For each of the
two cases, we present numerical examples to illustrate the
results.
II. PRELIMINARIES
Consider the stochastic system with state space S. Assume
S is finite with size n, namely, S = {1, 2, · · · , n}. Suppose
P (t) is its transition matrix whose (i, j)th entry, written as
pij(t), is the probability of transition from state i to state j
after any fixed time interval t. That is
pij(t) = prob(x(t+ s) = j|x(s) = i) (1)
for any time s. Here we assume that pij(t) is a continuous
function of t for all i, j in S. Clearly P (t) satisfies the
Chapman-Kolmogorov equation P (s + t) = P (s)P (t) for
each s, t ≥ 0. If we define
Q := lim
t↓0
P (t)− In
t
(2)
where In is the identity matrix of dimension n, the prop-
erty of P (t) allows one to derive the Kolmogorov forward
equation
dP (t)
dt
= P (t)Q , t ≥ 0 (3)
and the Kolmogorov backward equation
dP (t)
dt
= QP (t) , t ≥ 0 . (4)
6th annual IEEE Conference on Automation Science and
Engineering
Marriott Eaton Centre Hotel
Toronto, Ontario, Canada, August 21-24, 2010
TuB4.1
978-1-4244-5448-8/10/$26.00 ©2010 IEEE 990
Proof: To show that the control U is safety enforcing,
or equivalently, Πb is invariant under U , we need to show
that for each j in S and t ≥ 0 satisfying pi(t)j = b¯j , it holds
that
dpi
(t)
j
dt
∣∣∣pi(t)j =b¯j ≤ 0 .
That is, pij = b¯j implies
(piQU )j ≤ 0 , (14)
or, ∑
i6=j
piiq
(j)
i ≤ −b¯jq(j)j , (15)
for each j in S. Note that∑
i 6=j
piiq
(j)
i =
n−1∑
i=1
piσj(i)q
(j)
σj(i)
=
n−1∑
i=1,i6=lj
piσj(i)q
(j)
σj(i)
+
1− b¯j − n−1∑
i=1,i6=lj
piσj(i)
 q(j)σj(lj)
=
n−1∑
i=1,i6=lj
piσj(i)
(
q
(j)
σj(i)
− q(j)σj(lj)
)
+ (1− b¯j)q(j)σj(lj)
≤
lj−1∑
i=1
piσj(i)
(
q
(j)
σj(i)
− q(j)σj(lj)
)
+ (1− b¯j)q(j)σj(lj)
≤
lj−1∑
i=1
b¯i
(
q
(j)
σj(i)
− q(j)σj(lj)
)
+ (1− b¯j)q(j)σj(lj) .
Therefore (14) or (15) holds if and only if (13) holds and
the proof is completed.
Remark 3.2: If we define the following:
pi∗ji :=

b¯i if i ∈ ∆j ,
1− b¯j −
∑σj(lj−1)
k=1 b¯k if i = lj ,
0 otherwise ,
(16)
where ∆j := {σj(1), σj(2), · · · , σj(lj − 1), j}, then clearly
for each j in S, pi∗j· := [pi
∗
j1 pi
∗
j2 · · · pi∗jn] in Π and
pi∗j· = arg max
pi∈Π ,pij=b¯j
piq(j) . (17)
Example 3.1: Consider the controlled Markov chain with
the induced generator QU where
QU =
 −2 1 14 −7 3
2 3 −5
 . (18)
We can thus write according to the definition of q(i)σi(j) in (12)
that
QU =

q
(1)
σ1(3)
q
(2)
σ2(2)
q
(3)
σ3(2)
q
(1)
σ1(1)
q
(2)
σ2(3)
q
(3)
σ3(1)
q
(1)
σ1(2)
q
(2)
σ2(1)
q
(3)
σ3(3)
 .
If the set of safety specification pib is given by
Πb := {pi ∈ Π|pi ≤ b¯ = [.6 .2 .3]} , (19)
then we have from (16) pi∗1·pi∗2·
pi∗3·
 =
 .6 .2 .2.5 .2 .3
.5 .2 .3
 .
Since
3∑
i=1
pi∗1iq
(1)
i = 0 ,
3∑
i=1
pi∗2iq
(2)
i = 0 ,
3∑
i=1
pi∗3iq
(3)
i = −.4 ,
we conclude that controller U is safety enforcing according
to Theorem 3.1, meaning that for each t ≥ 0,
Πbe
QU t ⊆ Πb . (20)
Example 3.2: Following Example 3.1, if the set of safety
specification Πb is given instead by
Πb := {pi ∈ Π|pi ≤ b¯ = [.6 .2 .4]} , (21)
then  pi∗1·pi∗2·
pi∗3·
 =
 .6 .2 .2.4 .2 .4
.4 .2 .4

and
3∑
i=1
pi∗1iq
(1)
i = 0 ,
3∑
i=1
pi∗2iq
(2)
i = 0.2 , (22)
3∑
i=1
pi∗3iq
(3)
i = −1 .
As a result, the condition in Theorem 3.1 fails due to (22)
and controller U is not safety enforcing. Namely, there exists
t > 0 such that
Πbe
QU t * Πb . (23)
B. General Supreme Invariant Safety Set
We study in this section how to characterize the set of
safe initial probability distributions Πs in (11) when it is a
strict subset of the set of safety specification Πb. Let Ai· be
the ith row of matrix A and ‖ · ‖1 be the L1 norm. That is,
‖v‖1 =
∑n
i=1 |vi| if v = [v1 v2 · · · vn]. Define τ(A) to be
the ergodic coefficient of matrix A. Namely,
τ(A) :=
1
2
max
i,j
‖Ai· −Aj·‖1 . (24)
It is well-known that for a stochastic matrix P it holds that
( [8])
‖(pi1 − pi2)P‖1 ≤ τ(P )‖pi1 − pi2‖1 (25)
for any pi1, pi2 in Π . If the induced generator matrix QU by
the policy U does not satisfy the condition in Theorem 3.1,
978-1-4244-5448-8/10/$26.00 ©2010 IEEE 992
表 Y04 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                             99年 9 月 24 日 
報告人姓名 許舜斌 
 
服務機構
及職稱 
國立中興大學電機系助理教授 
 
 
時間 
地點 
8/21-8/24, 2010 
Toronto, Ontario, Canada 
本會核定
補助文號
99-2221-E-005-068 
會議 
名稱 
 (中文) 2010電機電子工程師學會自動化科學與工程會議 
 (英文) 2010 IEEE Conference on Automation Science and Engineering 
發表 
論文 
題目 
 (中文) 滿足上界之連續時態馬可夫鏈控制 
 (英文) Control of Continuous-Time Markov Chains With Safety Upper Bounds 
報告內容 
經過: 
     本人被安排於 8/24/2010 下午 1:30-1:50發表論文，題目是“Control of Continuous-Time 
Markov Chains With Safety Upper Bounds” (滿足上界之連續時態馬可夫鏈控制)，此主題為馬
可夫決策過程研究之新方向。我們導出了以連續時態的馬可夫決策過程 (continuous-time 
Markov decision process)為模型的隨機控制系統，其狀態機率總是不超過給訂上界的條
件。由於此理論結果深具應用價值，引起會議現場學者的興趣，包含來自印度，中國，
和 加拿大的學者都提出了一些問題，本人則依序予以答覆，並有良好互動。會議結束
後有機會繼續和數位學者交談，並討論學術圈相關領域的研究概況，特別是國際頂尖學
術機構目前的基礎研究與應用方向，感覺受益良多。 
 
心得: 
本會議為電機電子工程學會機器人與自動化分會(IEEE Automation Science and 
Engineering Society)所主辦之旗艦會議 (flagship conference)，屬國際控制與系統工程界
重要會議之一，每年吸引數百名各國學術界與工程實務人士參與，發表論文之質與量均
具一定水準，可惜開辦至今僅第六屆，知名度未開，國內參與人數有限，甚為可惜。最
近在政府鼓勵之下，國內舉辦大型國際會議的風氣逐漸形成，就規模而言，多數在國內
舉辦之會議約與此次會議相當，平心而論，就硬體設備而言，國內舉辦之國際會議已不
輸給在先進國家舉辦之會議，然而在參加者方面仍無法吸引較多元國家與地區的專家學
者，這部分仍有待國內控制與自動化領域的學者，就國際化與提升能見度方面來一起來
努力。 
 
參訪   
   多倫多市區，多倫多大學 
攜回 
   2010 CASE 大會手冊與會議光碟。 
 
 
 
附件三
 
Both equations lead to the expression
P (t) = eQt , t ≥ 0 , (5)
which meets the initial condition that P (0) is an identity
matrix. The matrix Q is called the generator. As a result,
we have
pi(t) := pi(0)eQt , t ≥ 0 (6)
where pi(t) := [pi(t)1 pi
(t)
2 · · · pi(t)n ] and pi(t)i is the probability
that the system is at state i at time t. The triple (S,Q, pi0)
constructs a continuous-time Markov chain. We define
Π := P(S) (7)
to be the set of probability distributions on the state space S,
and call a matrix ‘stochastic matrix’ if it is a non-negative
matrix with row sum equal to 1 (clearly pi(t) ∈ Π and P (t) is
the stochastic matrix, for each t ≥ 0 ). The Markov chain is
called irreducible if for any pair i, j in S, pij(t) > 0 for some
t. Actually for any pair i, j in S, the continuous-time Markov
chain has the property that either pij(t) > 0 for all t > 0 or
pij(t) = 0 for all t > 0. A vector pi is called the stationary
distribution of the Markov chain with transition matrix P (t)
if pi ∈ Π and piP (t) = pi for all t ≥ 0. The necessary and
sufficient condition for the probability distribution pi to be
stationary is piQ = 0. For an irreducible Markov chain, if pi
is the stationary distribution then it is unique and satisfies
lim
t→∞pi
(0)eQt = pi , ∀pi(0) ∈ Π . (8)
Suppose qij is the (i, j)th element of Q. The definition in
(2) implies that qij ≥ 0 for j 6= i and
∑
j 6=i qij = −qii ≤ 0.
Actually, −qii is the parameter of the exponential distribution
that characterizes the sojourn time after the system reaches
state i. We can classify the state i according to the value of
qii as follows:
state i is
 instantaneous if qii = −∞ ,stable if qii ∈ (−∞, 0) ,absorbing if qii = 0 . (9)
To simplify the analysis we assume throughout the paper
that each state i of the chain is stable and thus the stationary
distribution is unique. As the sojourn comes to an end the
system jumps to other state j with probability −qij/qii. If the
information regarding the status of system at any time is fully
available and without incertitude, then the Markov chain is
called completely observable. The mechanism to control the
Markov chain is explained as follows. Let U be a finite set of
controls and U : S → U be a mapping from the set of system
states to the set of controls. Given the state information
i in S, the controller U assigns the control input u in U
and incurs the associated generator Q(u) that governs the
dynamics of the system. Under the assumption of complete
observation, as the system is in state i, only the ith row of
Q(u) carries the parameters for the system evolution at that
instance. Collecting these rows (i = 1, 2, · · ·n) yields the
generator QU , whose ith row depends on the control input
u through U(i) = u. We thus call QU the generator induced
by the controller U . As a result, given the controller U , the
evolution of the controlled Markov chain is determined with
the state probability distribution pi(t) = pi(0)eQU t for any
t ≥ 0.
III. MAIN RESULTS
Given two vectors
v1 = [v11 v12 · · · v1n] , v2 = [v21 v22 · · · v2n] ,
we say v1 ≤ v2 if v1i ≤ v2i for i = 1, 2, · · · , n. For some
matrix M and two collections V1, V2 of row vectors, we
use V1M ⊆ V2 to express that v1 ∈ V1 implies v1M ∈ V2.
Consider the row vector b¯ = [b¯1 b¯2 · · · b¯n] and the incurred
Πb where
Πb := {pi ∈ Π|pi ≤ b¯} . (10)
Here Πb is called the set of safety specification. A probability
distribution pi is safe if pi ∈ Πb. The major objective of safety
control is to identify the subset Πs of Πb where
Πs := {pi ∈ Πb|pieQU t ∈ Πb,∀ t ≥ 0} . (11)
We call Πs the set of safe initial probability distributions.
Apparently, for Πs to be non-empty, the unique stationary
distribution induced by the generator QU should not fall
outside the set of safety specification Πb. Note that the
maximum candidate of Πs is Πb itself. As Πs = Πb, the
controller U is called safety enforcing, since it guarantees
all the probability distributions that are initially safe remain
safe as times evolves. In Subsection III-A the necessary and
sufficient condition for the controller to be safety enforcing
is presented. If the condition is not satisfied, the characteri-
zation of Πs is derived in Section III-B.
A. Safety Enforcing Controller
For each j in S, let q(j) = [q(j)1 q
(j)
2 , · · · q(j)n ]T be the jth
column of QU , and σj(·) be a mapping of i ∈ S such that
q
(j)
σj(1)
≥ q(j)σj(2) ≥ · · · ≥ q
(j)
σj(n)
. (12)
Also, lj is the smallest integer such that
lj∑
i=1
b¯σj(i) ≥ 1− b¯j .
Namely,
lj∑
i=1
b¯σj(i) ≥ 1− b¯j ≥
lj−1∑
i=1
b¯σj(i) .
Then we have the following result:
Theorem 3.1: The necessary and sufficient condition for
Πbe
Qt ⊆ Πb , ∀ t ≥ 0
is
lj−1∑
i=1
b¯σj(i)q
(j)
σj(i)
+
1− b¯j − lj−1∑
i=1
b¯σj(i)
 q(j)σj(lj) ≤ −q(j)j b¯j .
(13)
for each j in S.
978-1-4244-5448-8/10/$26.00 ©2010 IEEE 991
the following result suggests a formulation to characterize
Πs in (11).
Theorem 3.3: Suppose the stationary distribution pi∗ :=
[pi∗1 pi
∗
2 · · · pi∗n ] is in the interior of the set of safety
specification Πb, meaning that ε := mini(b¯i − pi∗i ) > 0.
Then there exists a t′ with
t′ ≤ 1 + log ε
log τ(eQU )
(26)
such that Πs in (11) can be expressed as
Πs := {pi ∈ Πb|pieQU t ∈ Πb,∀ t ∈ [0, t′]} . (27)
Proof: For each pi in Π we have
‖pieQU t − pi∗‖1 = ‖(pi − pi∗)eQU t‖1
= ‖(pi − pi∗)eQU (t−btc+btc)‖1
= ‖(pi′ − pi∗)eQUbtc‖1
≤ τ(eQU )btc‖pi′ − pi∗‖1 .
where btc is the greatest integer not exceeding t. Since
pi(t) = pieQU t = [pi(t)1 pi
(t)
2 · · · pi(t)n ] , (28)
we have for each i in S
|pi(t)i − pi∗i | ≤
1
2
τ(eQU )btc‖pi′ − pi∗‖1
≤ τ(eQU )btc .
Therefore,
btc > log ε
log τ(eQU )
(29)
implies
pieQU t ∈ Πb , ∀pi ∈ Π . (30)
As a result,
Πˆs :=
{
pi ∈ Πb | pieQU t ∈ Πb , ∀ btc ∈
(
0,
log ε
log τ(eQU )
)}
guarantees
Πˆse
QU t ⊆ Πb , ∀ t ≥ 0 (31)
and the proof is completed.
Corollary 3.4: Suppose t′′ is any number greater than t′
in Theorem 3.3 and
Π ′s := {pi ∈ Πb|pieQU t ∈ Πb,∀ t ∈ [0, t′′]} ,
then Πs = Π ′s.
Example 3.3: In Example 3.2, the set of safety specifica-
tion Πb makes the controller U not safety enforcing. Given
QU in (18) we have the stationary distribution
pi∗ = [0.5909 0.1818 0.2273] (32)
and
eQU =
 0.5943 0.1805 0.22520.5881 0.18295 0.22895
0.5843 0.1843 0.2314
 :=
 R1R2
R3
 .
By (25) we have
τ(eQU ) =
1
2
‖R1 −R3‖1 = 0.01
and
ε = min{b¯i − pi∗i } = .0091 ,
therefore the set of safe initial probability distributions Πs
can be characterized by
Πs = {pi ∈ Πb|pieQU t ∈ Πb,∀ t ∈ (0, 2.0205]} . (33)
IV. CONCLUSIONS
We introduce in this work the concept of safety for the
continuous-time controlled Markov chains. The results are
analogous to their counterpart in the discrete-time case.
Suppose a unit-interval valued vector that serves as an
upper bound is given, all the probability distribution vectors
not exceeding the upper bound construct the set of safety
specification. We manage to identify the supreme subset
of the set of safety specification such that any starting
distribution from the subset has all its future distributions
in the set of safety specification. Our first result presents
the necessary and sufficient condition for the subset to be
the set of safety specification itself. If the condition is not
satisfied, we propose to characterize the strict subset of the
safety specification by giving a finite time stamp. The subset
is then the collection of all the starting distributions that are
in and remain in the set of safety specification by the time
stamp.
REFERENCES
[1] E. Altman, Constrained Markov Decision Processes, Chapman and
Hall/CRC, 1999.
[2] A. Arapostathis, R. Kumar, and S.-P Hsu, “State-feedback control
of Markov chains with safety bounds”, Proc. IEEE Conference on
Decision and Control, Maui, HI, pp. 6283–6288, 2003.
[3] A. Arapostathis, R. Kumar, and S.-P Hsu, “Control of Markov chains
with safety bounds”, IEEE Transactions on Automation Science and
Engineering, Vol 2, pp. 333–343, 2005.
[4] A. Arapostathis, R. Kumar, and S. Tangirala, “Controlled Markov
chains with safety upper bounds”, IEEE Transactions on Automatic
Control, Vol 48, No. 7, pp. 1230–1234, 2003.
[5] E. A. Feinberg, “Constrained finite continuous-time Markov decision
processes with average rewards”, Proc. IEEE Conference on Decisions
and Control, pp. 3805–3810, 2002.
[6] X. Guo and O. Hernandez-Lerma,“Continuous-time controlled Markov
chains”, The Annals of Applied Probability, Vol. 13, No. 1, pp. 363-
388, 2003.
[7] M. L. Puterman, Markov Decision Process: discrete stochastic dy-
namic programming, John Wiley & Sons, 1994.
[8] E. Seneta, Non-negative Matrices and Markov Chains, Springer-
Verlag, New York, NY, 2 edition, 1981.
978-1-4244-5448-8/10/$26.00 ©2010 IEEE 993
99 年度專題研究計畫研究成果彙整表 
計畫主持人：許舜斌 計畫編號：99-2221-E-005-068- 
計畫名稱：部份可觀的馬可夫鏈之風險敏感控制 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 2 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 2 50% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
