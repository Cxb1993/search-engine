 2
控制系統，使用單鍵開關輸入來取代以往複
雜的使用者介面，進而將使用者推廣至具有
運動神經疾病、多重硬化症的患者、老年人
以及因意外而無法自行動手進食的傷患。目
前，「Handy 1」的上半部為五個自由度的機械
手臂及一個餐盤，而下半部則是具有五個輪
子的可移動式平台，此移動平台是採用人力
來移動的方式，照顧者在用餐時間，將 Handy 
1 移動至行動不便的使用者旁邊，接著再把菜
餚一樣一樣放在不同的欄位上，開啟電源，
使用者在想吃的食物的 LED 燈亮起時，按下
按鈕，機械手臂便會將那欄的食物舀起，移
動至定位點，使用者即可將食物吃下，完成
餵食的動作。 
而日本的餵食用機器人「My Spoon」的
研發，則肇始於 1991 年。SECOM 公司於 1995
年，所開發出的「My Spoon」[3]是一台五軸
的機械手臂，其製造理念是來自 SCARA 型的
機械手臂，將原本末端為線性移動的部份改
成旋轉，以便於給使用者進食，其操作介面
為雷射與雷射感應器，將雷射發送端設置在
使用者頭上，介面板上設置多個雷射感應
器，每個雷射感應器都有其對應使機械手臂
動作的命令，機械手臂所有的動作皆依靠使
用者的命令來進行。目前，市面上的「My 
Spoon」的使用者介面已改為操縱桿，且照顧
者可將操作模式分為：手動、半自動、自動
三種模式，其中食物盤為方形且分成四格，
無論是以哪一種模式進行餵食，抓取食物
後，機械手臂便會將食物送至固定位置，當
夾爪抓取完食物要送到使用者口中時，上夾
爪便會往回縮，只剩下夾爪盛著食物，上夾
爪的設計為叉子狀，下夾爪的設計為湯匙狀。 
綜觀以上兩種餵食用機器人，雖然
「Handy 1」後來將餐盤設計為可替換成其他
盤面，以達多功能用途，但就餵食用途而言，
只用單支湯匙能舀起的東西有限，且舀起的
成功率不高，當食物舀起後，只能移動至固
定的位置，無法依據使用者嘴巴的位置來調
整，容易造成使用者吃東西的不便。而「My 
Spoon」尚未克服的是在自動模式下，機械手
臂有可能會往格子中無食物的地方夾取，且
機械手臂將食物送到的位置是固定的，這個
問題與 Handy 1 一樣，容易造成使用者吃東西
的不便;除此之外，使用者在手動的模式下吃
東西時，需要不停地用下巴來操控操縱桿，
時間一長，使用者易感到疲累，甚至心情煩
躁，因而影響食慾。 
計畫目的 
為了提昇餵食用機器人的功能，本計畫
的目的在於研發能支援無法獨立進食的人們
之智慧型餵食用人形機器人。為了使所研發
的餵食用機器人，能讓手部不便的長者或殘
障者，安心且便利地進食，並使他們覺得機
器人是他們的看護，願意與其親近，則必須
考慮到機器人之自主性、機動性、互動性與
安全性。本計畫雖僅針對智慧型餵食用人形
機器人進行研發，但其為追求以上的四個特
質，所發展出之多項技術將可被移植或應用
到其他「與人共存型服務用機器人」上。  
三、研究方法 
3.1 餵食用人形機器人之設計 
為了使餵食用機器人能融入人類生活環
境中，機器人若能以擬人形活動於人類生活
環境，將有助於增加機器人對人類的親和
力。然而，完全仿似人類外形的機器人，在
製作上存在著一定程度的複雜性，因此適當
簡化人形機器人外形，不但在製作上容易完
成，且可使機器人保有足夠的親和力。本計
畫已完成一個七自由度的左機械手臂與一個
二自由度左機械手之建構，並將其安裝於前
二年所完成的機器人身上，完整的餵食用人
形機器人，如圖一所示，主要是由全方向輪
式移動平台、架設於平台上之身軀、兩個各
七自由度的機械手臂、兩個各二自由度的機
械手與機械頭所組成。左、右機械手主要是
仿人類手部的外形，但將四指合併為一體，
僅設計姆指具有兩個自由度，用以使左、右
機械手可握持餐盤；由於右手除了與左手協
力端餐盤外，尚須抓取食物，因此，另將餐
叉設計成可隱藏於姆指背面，而餐匙設計成
可隱藏於手掌背面，於右手抓取食物時，可
透過致動器的驅動伸出餐叉與餐匙，並藉由
出姆指的二個關節來夾取食物，當夾爪夾取
完食物要送到待餵食者口中時，餐叉便會縮
回，僅剩下餐匙盛著食物。  
 4
v
v
v
y
y
y
     


 

dy
dy
dy
e
e D
K
v
r
ˆ ( )f x
1 2
TJF 
( )Kin 
J
q
q
y
y
sgn( )sgn( )y
extFeˆxtF
F
F
F
y
y
y
     


d
d
d
F
F
F
y
y
y
     


qq PP
C
cpF
7 F
F CJ J
1
7
F F
CA A
  
7
FJ
1
7
F F
CA A
  
17 F
F CJ J
  
 
圖三 右手臂側邊智慧型阻抗控制架構 
3.3 雙手臂之智慧型協調阻抗控制 
機器人利用其雙手臂/手協力端餐盤的
過程中，機械手與餐盤要時時刻刻保持接
觸，使餐盤與機械手間不會有滑動甚至於脫
落的情形發生，因此機械手臂必須於機械手
處提供一足夠的內力。本計畫採用以內力為
基礎之阻抗控制法則，進行雙機械手臂/手之
協調控制，如圖四所示，主要是由左機械手
臂之智慧型控制與右機械手臂之智慧型阻抗
控制所組成。 
 
圖四 雙手臂智慧型協調阻抗控制示意圖 
由於機器人除了在右手臂側邊外罩內裝
有力量感測器外，僅在右手臂之手腕上安裝
一力量感測器，再應用右手臂側邊智慧型阻
抗控制法則進行控制，僅須將原先位置和(實
際作用力)之間的動態關係，改成位置和(實
際作用力與所須內力間之差)之間的動態關
係即可因此，圖五所描述的即為右機械手臂
之智慧型阻抗控制。 
v
v
v
y
y
y
     


 

dy
dy
dy
e
e D
K
v
r
ˆ( )f x
1 2
F 
( )Kin 
J
q
q
y
y
sgn( )sgn( )y
extFeˆxtF
dq
TJ
 
圖五 右機械手臂之智慧型阻抗控制架構 
左機械手臂之智慧型控制，因屬於位置
控制，其控制架構(圖六)與右手臂之控制架構
類似，但移除了力量感測器之訊號輸入，此
架構仍具有一倒傳遞神經網路，用以學習左
手臂之動力學模型，透過此控制架構可使左
手臂依期望之運動軌跡進行移動。 
I  I 
I

dy
dy
dy
e
e
+
Robust
Term
DK
v
r
+
+
+
-
ˆ ( )f x
1 2
F Plant
( )Kin 
J
q
q
y
y
sgn( )sgn( )y
-
-
+
+
+ ++ +
Inverse 
Kinematic
dq
TJ
 
圖六 左機械手臂之智慧型控制架構 
在機器人以其雙手臂/手執行端餐盤的
任務中，首先，透過類神經映射求得所需之
餐盤位置，再依其座標轉換之相對關係求得
所需之雙手掌抓取位置，並由餐盤在工作空
間之位置或軌跡，可依據雙機械手臂/手與餐
盤的運動關係，求解得各機械手臂/手之規劃
位置或軌跡，最後，透過左、右手臂各自之
智慧型控制架構來完成雙手臂協調移動餐
盤。 
3.4 機器人基於行為模式之導航及人物跟隨 
本計畫結合機器人擁有的雷射測距儀
(Laser Range Finder; LRF)及全景式攝影機所
提供之訊息，提出一個可以使機器人在散佈
 6
3.4.2 人物跟隨 
人物跟隨對自主式機器人來說，是重要的
功能之一，其中最容易遇到兩個問題是:避障
及目標資訊獲得。在未知的環境中達成跟隨
任務是一項挑戰，因為在跟隨的途中必定存
在固定或移動的障礙物，如何在途中閃過障
礙物並且能跟隨目標，行為的融合策略就十
分重要，本計畫利用機器人具備之 LRF 及全
景式攝影機器，以得到目標的距離及方向。 
全景式攝影機的特點是可以獲得 360 度
的環境影像，因此可以擁有很好的視野，但
是為了快速得到環場的資訊，必定要放棄解
析度來達成，此外全景式攝影機的影像透過
壓克力外殼，再經一個凸面鏡反射得到，因
此利用得到的影像進行目標物辨識，顏色特
徵的擷取是較容易且強健的方法。因為機器
人只裝配一台攝影機，無法獲得機器人與目
標準確的距離，因此，與目標的距離資訊，
則利用 LRF 所取得的腿部資訊來計算求得。 
 雷射測距儀獲得的距離資訊除了可用以
執行避障外，還可用來獲得腿部的資訊。首
先分類成最有可能腿部及背景的距離資訊，
其分類的依據為：原始資料(Raw Data)兩兩之
間距離差是否超過 20cm，若超過則稱為起始
斷差點，再由起始斷差點開始偵測終止斷差
點，若兩個斷差點在機器人座標上的距離小
於 25cm 及大於 14cm 則記錄為腿部候選(Leg 
Candidate)，也就是記錄其起始及終止斷差
點。獲得腿部中心之後，接著，將候選腿部
位置兩兩互相比較距離是否為一般人雙腿的
距離，設定為 30 至 50cm，若在此範圍內則認
定為一個人的雙腿，並求得該人物中心的座
標，以便計算出人物相對於機器人的距離及
角度。通常人物是動態並在環境中存在著其
他可能使機器人誤判的因素，因此為了使人
物的跟隨更為強健，本計畫結合了 LRF 及全
景式攝影機兩種感測器，彼此互相確認所獲
得的訊息，使誤判的機率降低。 
全景式攝影機可以得知目標的方向，而
LRF 可以得知目標的距離及方向，所以如果
影像上得到兩個目標物，這個時候就要依靠
LRF 得到的目標角度和影像得到的角度差在
30°以內則判定為跟隨目標。因此當影像上及
LRF 皆存在目標且其角度相差在 30°以內，機
器人才會開始做跟隨的動作，反之則停留在
原地，其整合流程如圖八所示，如此一來機
器人就不會因為影像之雜訊，或是在環境中
存在著類似人的腿部大小的物體，而使機器
人誤判。 
 
圖八 感測器資訊整合之流程 
3. 5 智慧型餵食用人形機器人之性能測試 
3.5.1 機器人端餐盤之性能測試 
在機器人以雙手臂/手執行端餐盤並移動
之性能測試中，規劃之餐盤移動軌跡如圖九
所示。首先，操作者將餐盤任意擺放機器人
前方餐桌上，機器人再利用其頭之二維旋轉
變焦式攝影機，透過類神經映射求得餐盤之
位置，並依贅餘自由度之運動學逆解，求得
平滑之軸解來移動雙手去抓取餐盤。一但抓
取後，雙手將餐盤端往上方，此時，於左手
臂上進行智慧型位置控制，而於右手臂上進
行智慧型阻抗控制，依據預先定義的物理阻
抗模型並透過阻抗控制的方法，可以控制右
手掌的位置且同時使其與左手掌之間保持適
當的內力，在內力的設定上，由於手掌之握
力為 5N，在Y 方向為左右手掌之拉扯方向，
為了防止此方向產生滑動的情況，故需保持
5N 之內力；另在 Z 方向，為了防止餐盤造成
水平之轉動，故需保持 0N 之力，此兩力亦即
為所設定之目標力量；而 X 方向為重力場之
 8
 
圖十二 機器人之人物追蹤軌跡 
四、結論 
延續先前的計畫，本計畫已如計畫書所規
劃，除了已完成機器人左手臂與左手之建
構，以及將其安裝於前計畫所完成的機器人
身上，完成完整的餵食用人形機器人之建
構，並提出雙機械手臂智慧型協調阻抗控制
之方法，使得機器人能兩手協同端餐盤，另
提出機器人導航及跟隨人物的策略，使機器
人在散佈著障礙物的環境中，得以自行往目
標前進，或亦步亦趨地跟隨著人前進；所以，
在實際應用上，使用者或其家人可能要求機
器人自行或跟隨著他，以雙手端著裝有食物
之餐盤至家中之某個位置，再將餐盤放至餐
桌上。經過多組實驗測試，顯示所提出的機
器人導航、人物跟隨與雙手臂協調阻抗控制
策略，能令機器人擁有預期之性能。 
五、參考文獻 
[1] R. A. Brooks, "A Robust Layered Control 
System for a Mobile Robot," IEEE Journal 
of Robotics and Automaton, Vol. RA-2, No. 
1, pp. 14-23, 1986. 
[2] N. Hogan, “Impedance Control: An 
approach to Manipulator, Parts-I-III,” 
ASME Journal of Dynamic Systems, 
Measurement, and Control, Vol. 107, No. 1, 
pp. 1-24, 1985. 
[3] S. Ishii, S. Tanaka and F. Hiramatsu, 
“Meal Assistance Robot for Severely 
Handicapped People,＂  Proceedings of 
IEEE International Conference on 
Robotics and Automation, Vol. 2, pp. 
1308-1313, May 1995. 
[4] H. Kazerooni, T. B. Sheridan, and P. K. 
Houpt, “ Robust compliant motion for 
manipulators, Part I-II,” IEEE Journal of 
Robotics and Automation, Vol. RA-2, No. 
2, pp. 83-105, 1986. 
[5] W.-S. Lu and Q.-H. Meng, “Impedance 
Control with Adaptation for robotic 
manipulators,” IEEE Transactions on 
Robotics and Automation, Vol. 7, No. 3, pp. 
408-415, 1991. 
[6] M. H. Raibert and J. J. Craig, “Hybrid 
Position/Force Control of Manipulators,” 
ASME Journal of Dynamic Systems, 
Measurement and Control, June 1981. 
[7] A. Saffiotti, “Fuzzy Logic in Autonomous 
Robotics: Behavior Coordination,” 
Proceedings of the IEEE International 
Conference on Fuzzy Systems, Vol. 1, pp. 
573-578, July 1997. 
[8] M. Topping, “An Overview of the 
Development of Handy 1, a Rehabilitation 
Robot to Assist the Severely Disabled,” 
Journal of Intelligent and Robotic Systems, 
Vol. 34, No. 3, pp. 253-263, July 2002. 
[9] C. Ye and D. Wang, “A Novel Behavior 
Fusion Method for the Navigation of 
Mobile Robots,” Proceedings of IEEE 
International Conference on Systems, Man 
and Cybernetics, Vol. 5, pp. 3526-3531, 
2000. 
[10] N. H. C. Yung and C. Ye, “Avoidance of 
Moving Obstacles Through Behavior 
Fusion and Motion Prediction,” 
Proceedings of IEEE International 
Conference on Systems, Man and 
Cybernetics, Vol. 4, pp. 3424-3429, 1998. 
 
表 Y04 2
個人亦收獲不少。而筆者的論文“Skill Learning for a Humanoid Robotic Ping-Pong 
Player＂則被安排於第三天下午的『Humanoid and Mobile Robots』場次中發表；當
天晚上，大會還準備有一簡單的頒獎典禮與歡送酒會，很慶幸地筆者的論文入圍最
佳論文獎，除接受大會主席頒發獎狀，增加我國機器人研究在國際上的能見度外，
亦加多了和與會者交流之機會，並接受大會邀請將論文轉投電腦科學雜誌。 
二、與會心得 
國際機器人學與智慧型感測器研討會的目標，在於針對機器人學與感測器發展
出新的技術。此一研討會不但著重智慧型感測器的發展，而且希望這些感測器能應
用於各類型的機器人，尤其是能將智慧型感測器與機器人學充分結合在一起。此會
議(IRIS)為第一個以結合機器人學與感測器為主題的國際會議，本次大會聚集了各
種感測器及機器人應用專家於一堂，除了能宣傳發表之論文外，更能同時吸收最新
近的研究知識與研究趨勢，此行最大的收穫莫過於得到一些前瞻性的研究趨勢。 
三、考察參觀活動 
主辦單位於本次研討會第四天，精心策劃了一個參觀行程，包括﹕TOYOTA 汽
車博物館、RIKEN-TRI 與人互動式機器人共同研究中心(RTC)及名古屋城。早上九
點鐘，搭乘大會安排的遊覽車，首先前往 TOYOTA 汽車博物館，此博物館於 1989
年豐田創立 50 週年時落成，收藏記錄著世界汽車工業與豐田公司一路走來的歷史
進程；博物館共分為本館與新館，前者陳列有 120 輛古董車，展示著從 19 世紀末
到 20 世紀的汽車歷史，新館則展出日本的汽車文化的變遷，陳列最早的消防車、
賽車等 40 輛車，每輛車都令人大開眼界；值得一提的是，本館內所有的古董車並
非只是單純的展示品，每輛車都保持著可以行使的最佳狀態，每年並有二次開放時
間供遊客選擇搭乘。接著，中午用完午餐後，再前往 RTC 參觀，該中心先安排人
員為我們做簡報，然後再現場示範機器人之操作；該中心的目的是，利用機器人技
術來對付老年社會中生活上會遭遇的問題；RIKEN 于 2006 年 3 月，完成了 RI-MAN
機器人，但因其安全性、負載、結構與精度不足，僅能以雙手抱起 18.5 公斤的假人；
該中心成立後，針對 RI-MAN 的缺失，提昇其結構強度、精度與安全性，而于 2009
年 8 月，完成了 RIBA 機器人，RIBA 已能成功地以雙手將人抱起，並將其移往目
的地；現場示範部分，RIBA 機器人成功地將躺在床上的人抱起，並將其抱至輪椅
上，可以減輕照護人員的負擔。最後，再轉往名古屋城參觀，名古屋城是西元 1612
年德川家康所建造的尾張德川家城堡，裝飾著金色獸頭瓦的天守閣內部，有當時的
藩主、城下的生活及文化等介紹，是名古屋最眾所周知的歷史遺產。 
四、攜回資料名稱及內容 
研討會論文摘要一本與論文集光碟一片。 
五、誌謝 
非常感謝國科會補助筆者參加此一國際研討會，使筆者能夠參與盛會，受益良
多。 
D.O.F. hands are designed to imitate the visual and 
ambidextrous behavior of humans, respectively. 
The hardware architecture of the entire stand-alone real-
time control system for the humanoid robot consists of the 
following: images acquired from the four color cameras are 
processed using four image-processing subsystems; 40 
servomotors are controlled using eight motor boards; and the 
control strategies, e.g., calculating robot dynamics, are 
handled using four processor boards. Figure 2 shows the 
proposed configuration for the control system of the 
humanoid robot. In the servo control loop, all robot actuators 
are handled using motor boards and driver boards. The fuzzy 
slide mode controllers are distributed in multiple processor 
boards to achieve real-time control. Video signals from the 
four cameras are processed by four image-processing 
subsystems to obtain visual information for locating the 
ping-pong ball. The sampling time for the entire servo 
controller is 0.005 sec, while the sampling rate for image 
processing is 60Hz for each video stream. 
 
Figure 1. Overview of the developed humanoid robot 
 
 
 
 
 
 
 
 
Figure 2. Architecture of control system 
III. STRIKE BEHAVIOR OF HUMANOID ROBOTIC PING-PONG 
PLAYER 
Humans must determine the strike trajectory and 
forehand/backhand strike according to past experience before 
the ball enters their court, and decide upon the desired strike 
pose within a short time. The ball speed is generally 4~5m/s, 
and up to 15-20m/s for a smash for human ping-pong players. 
Despite its anthropomorphic nature, the developed humanoid 
robot is not as dexterous or fast as a human and cannot 
handle smashes. Therefore, in this work, the ball speed is 
limited to 4-5m/s, and the robot generates a paddle speed of 
0.5m/s to return the ball, as analyzed using motion capture 
system, which is described in the next section. Given that the 
robot cannot quickly switch between the forehand and 
backhand strikes, only the backhand strike motion and 
teaching are considered. 
 
Figure 3. Lagrangian network for inverse kinematic solution 
 
 
EP
WP
PP
SP
UR
FR
PR
1V
2V
3V
4V
5V
6V
,U iD
,F iD
,P iD
 
Figure 4. Shortest distance between robotic arm links and the boundary of 
a robotic body 
The developed robotic arm has a 7-DOF redundant 
structure. Among the various methods adopted to solve the 
inverse kinematics problem of a redundant arm include 
inverse transformation, screw algebra, and geometric 
approaches [7]. Here, a Lagrangian network [13] is applied 
to obtain the inverse kinematics solution by assigning the 
desired velocity vector from the known endpoint of the robot 
arm to the desired target position in the task space. Figure 3 
shows the neural network, whose structure includes the 
weighting matrix represented as follows. 

1
2
0 0
0
0
0 0 n
w
w
W
w
       

 
  

 
where 
Proceedings of the 2010 International Symposium on Robotics and Intelligent Sensors (IRIS2010) 
Nagoya University, Nagoya, Japan, March 8-11, 2010. ISBN: 987-4-9905048-0-9, ISSN: 1884-1023
66
the expressions of ,U iH , ,F iH , and ,P iH  are given by (8)-(10). 
IV. STRIKE MOTION CAPTURE OF HUMAN AND ROBOT 
BEHAVIOR TEACHING 
Motion capture system 
This work also constructs an optical/inertial motion-
capture system to acquire the central position/orientation of 
the paddle during play in order to acquire the strike motion 
trajectory of the paddle and orient the robot on how to 
perform the strike motion in real-time. An inertial 
measurement device provides three groups of sensor data: 3-
axis linear accelerations, 3-axis angular velocities, and 3-axis 
magnetic field distributions. The optical tracking reduces the 
influence of surrounding environment light by applying 
infrared illuminators as active markers. Figure 5 shows the 
system architecture and coordinate relationships, in which 
the relationship between the robot and the motion capture 
system is elucidated based on the calibration pattern. In our 
application, the flight trajectory condition of the ball must be 
known, as obtained from the stereo vision of the robot, 
subsequently allowing us to align the trajectory data of the 
ball with respect to the base coordinate of the motion capture 
system through the calibration pattern. 
The ability to directly install the infrared illuminators in 
the center of the paddle surface influences the collision 
between the ball and the paddle surface. Therefore, in this 
work, three infrared illuminators are installed to form a 
circumference arrangement based on the center of the paddle 
surface, subsequently allowing us to obtain the actual motion 
trajectory of the center of the paddle surface without 
influencing play. According to Fig. 5, coordinates 1 2 3, ,o o o  
are the installed positions of the active illuminated markers. 
The inertial measurement device is then installed in the back 
of the paddle surface. Therefore, the correct 
position/orientation of the paddle center is obtained using a 
fusion filter [10]. The proposed architecture consists of the 
inertial measurement device with a high sampling rate and 
optical tracking with a low sampling rate, as shown in Fig. 6. 
For inertial measurements, the desired position ,P I P  and 
rotation matrix ,P IS
Θ  are estimated according to the inertial 
navigation equation. For optical tracking, the features 
( [ , ];    1, ,6j ju v j   ) of the three active illuminated markers 
in the image plane of the two infrared cameras are then 
located and the 3D position ( 1 2 3, ,
O O Op p p ) of three active 
illuminated markers is reconstructed using stereo 
triangulation. Next, the position/orientation ( , ,,P O P O P Θ ) of 
the paddle center is calculated using geometric relationships. 
Finally, the measurement information is calibrated using the 
Kalman filter [15] to obtain an accurate position/orientation 
( , P Θ ) of the paddle center. Otherwise, the calibrated 
linear/angular velocity ( , V Θ ) of the paddle center is also 
applied in the stopping detector. 
Teaching the humanoid robot ping-pong  
Given the difficulty in describing the desired strike 
motion trajectory of the robot in space with mathematic 
equations, the strike motion trajectory must be obtained, 
which is generated by humans as a reference for robot 
motion using the motion capture system. The total strike 
motion trajectory at the strike point cannot be obtained 
because strike points are not identical in actual strike motion 
of ping-pong. Therefore, this work proposes a strike pattern 
method to orient the robot on the strike motion.  
First, a ball served by a pitching machine is struck 
continuously with a backhand strike. The strike point is 
selected to be a region that the robot can strike. Given that all 
continuous strike trajectories are recorded on the computer 
during play, the data must be segmented and classified into 
groups. Obtaining the breaking points in continuous data is 
extremely difficult owing to the influence of the small 
continuous velocity. Therefore, a stopping detector is 
implemented as a simple adaptive two-class classifier in 
order to segment the motion data into piece records. For the 
two-class classifier, a three dimensional feature vector is 
used as components of the magnitude at time t , the 
magnitude at time 1t  , and the magnitude difference 
between the velocity at time t  and 1t  .  
 2 2 2x y zmagnitude V V V    
where xV , yV , and zV  denote velocity values recorded 
on the X-, Y- , and Z-Axis. In this application, two Gaussian 
density functions 1 2{ , }     are used to model the static 
  Θ
V
1, ,6j  
P
Θ
ZP
ZΘ,P OΘ
,P OP
1 2 3, ,
O O Op p p[ , ]j ju v
,P I P
,P I
S
Θ
,P IV
,P IΘ
, ,P I P Ia g
Sω
S Sa g
, ,
0
0
0
P I P I S
S
z y
S
z x
y x

 
  
 
   
       
Θ Θ
, , ( )P I P I P S SS  a g Θ a g 1S
1
S
1
S
x
 
Figure 6. Architecture of fusion filter for motion capture system 
Proceedings of the 2010 International Symposium on Robotics and Intelligent Sensors (IRIS2010) 
Nagoya University, Nagoya, Japan, March 8-11, 2010. ISBN: 987-4-9905048-0-9, ISSN: 1884-1023
68
0 50 100 150 200 250 300
-50
0
50
100
150
200
time step (10ms)
Variation on x-dir from Motion Capture
(m
m
)
 
 
Optical
INS
INS-Optical
0 50 100 150 200 250 300
-50
0
50
100
150
time step (10ms)
Variation on y-dir from Motion Capture
(m
m
)
 
 
Optical
INS
INS-Optical
0 50 100 150 200 250 300
-40
-20
0
20
40
time step (10ms)
Variation on z-dir from Motion Capture
(m
m
)
 
 
Optical
INS
INS-Optical
0 50 100 150 200 250 300
-6
-4
-2
0
2
4
6
time step (10ms)
Variation on rot
x
 from Motion Capture
(d
eg
)
 
 
Optical
INS
INS-Optical
0 50 100 150 200 250 300
-4
-3
-2
-1
0
1
time step (10ms)
Variation on rot
y
 from Motion Capture
(d
eg
)
 
 
Optical
INS
INS-Optical
0 50 100 150 200 250 300
-15
-10
-5
0
5
time step (10ms)
Variation on rot
z
 from Motion Capture
(d
eg
)
 
 
Optical
INS
INS-Optical
 
-50
0
50
100
150
200
-20 -10
0 10
20 30
40
-20
0
20
40
60
80
100
120
 
Z (mm)
Variation on the 3D Trajectory of the paddle from motion capture
X (mm)
 
Y
 (
m
m
)
start point
end point
 
Figure 10. Measurement results of the optical/inertial tracking motion 
capture 
 
Figure 11. Snapshots from a strike motion video 
Finally, the right arm of the developed humanoid robot 
was instructed to perform the actual ping-pong strike using 
the generated trajectory. Figure 11 shows snapshots from a 
strike motion video when using the guiding method. 
VI. CONCLUSION AND FUTURE WORK 
This work presented a motion capture system capable of 
orienting a humanoid robotic ping-pong player on ping-pong 
skills. Owing to its anthropomorphic nature, the humanoid 
robot can be oriented directly using human skills. Finally, a 
simplified ping-pong strike is made through a guiding 
method. As for future research, efforts are underway to 
develop an appropriate intelligent learning strategy to 
increase robotic skills in order to improve playing skilling by 
self learning. As is anticipated, the robot can rally smoothly 
with human players during a ping-pong session. 
Equation Section (Next) 
ACKNOWLEDGMENT 
The authors would like to thank the National Science 
Council of the Republic of China, for financially supporting 
this research under Contract No. NSC 94-2212-E-006-022. 
REFERENCES 
[1] L. Acosta, J. J. Rodrigo, J. A. Mendez, G. N. Marichal and M. Sigut, 
“Ping-pong Player Prototype,” IEEE Robot. Autom. Mag., vol. 10, 
issue 4, pp. 44-52, 2003. 
[2] R. L. Andersson, A Robot Ping-Pong Player: Experiment in Real-
Time Intelligent Control,  MIT Press, Cambridge, 1988. 
[3] J. K. Aggarwal and Q. Cai, “Human Motion Analysis: A Review,” 
Comput. Vis. Image Und., vol. 73, no. 3, pp. 428-440, 1999. 
[4] D. C. Bentivegna and C. G. Atkeson, “A Framework for Learning 
from Observation Using Primitives,” Proc. RoboCup Int. Symp., 
Japan, 2002. 
[5] J. Bortz, “A New Mathematical Formulation for Strapdown Inertial 
Navigation,” IEEE Trans. Aerosp. Electron. Syst., AES-7, pp. 61–66, 
1971. 
[6] F. T. Cheng, Y. D. Lu, and Y. Y. Sun, “Window-Shaped Obstacle 
Avoidance for a Redundant Manipulator,” IEEE Trans. Syst. Man 
Cybern., part B, vol. 28, issue 6, pp. 806-815, 1998. 
[7] R. C. Gonzalez, K.S. Fu and C.S.G. Lee, Robotics: Control, Sensing, 
Vision, and Intelligence, McGraw-Hill: New York, 1987. 
[8] Y. B. Kim, S. H. Han, S. J. Kim, E. J. Kim and C. G. Song, “Multi-
Player Virtual Ping-Pong Game,” Intl. Conf. Artificial Reality and 
Telexistence, pp. 269-273, 2007. 
[9] M. Matsushima, T. Hashimoto, M. Takeuchi and F. Miyazaki, “A 
Learning Approach to Robotic Table Tennis,” IEEE Trans. Robotics, 
vol. 21, issue 4, pp. 767-771, 2005. 
[10] D. Roetenberg, P. J. Slycke and P. H. Veltink, “Ambulatory Position 
and Orientation Tracking Fusing Magnetic and Inertial Sensing,” 
IEEE Trans. Biomed. Eng., vol. 54, issue 5, pp. 883-890, 2007. 
[11] S. Rusdorf, G. Brunnett, M. Lorenz, and T. Winkler, “Real-Time 
Interaction with a Humanoid Avatar in an Immersive Table Tennis 
Simulation,” IEEE Trans. Vis. Comput. Gr., vol. 13, no. 1, pp. 15-25, 
2007. 
[12] T. I. James Tsay and C. H. Lai, “Design and Hand-Eye Coordination 
of a Humanoid Robot,” Proceedings of the 39th Intl. Symposium on 
Robotics, pp. 124-129, 2008. 
[13] J. Wang, Q. Hu and D. Jiang, “A Lagrangian Network for Kinematic 
Control of Redundant Robot Manipulators,” IEEE Trans. on Neural 
Networ., vol. 10, issue 5, pp. 1123-1132, 1999. 
[14] L. Wang, W. Hu and T. Tan, “Recent Developments in Human 
Motion Analysis,” Pattern Recogn., vol. 36, no. 3, pp. 585-601, 2003. 
[15] G. Welch and G. Bishop, “An Introduction to the Kalman Filter,” 
Technical Report 95-041, University of North Carolina at Chapel Hill, 
2001. 
Proceedings of the 2010 International Symposium on Robotics and Intelligent Sensors (IRIS2010) 
Nagoya University, Nagoya, Japan, March 8-11, 2010. ISBN: 987-4-9905048-0-9, ISSN: 1884-1023
70
98年度專題研究計畫研究成果彙整表 
計畫主持人：蔡清元 計畫編號：98-2221-E-006-163- 
計畫名稱：智慧型餵食用人形機器人之發展(II) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 2 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 2 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 2 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
