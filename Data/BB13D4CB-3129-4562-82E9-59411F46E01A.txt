 II
目    錄 
中文摘要.................................................................................................................. III 
Abstract ....................................................................................................................IV 
I. 前言 .......................................................................................................................1 
II. 相關背景與文獻回顧..........................................................................................3 
2.1 國語發音方式 ...............................................................................................3 
2.2 構音與音韻異常 ...........................................................................................3 
2.3 虛擬合成臉部動畫 .......................................................................................5 
2.4 舌位分析相關研究 .......................................................................................7 
2.5 文獻回顧 .......................................................................................................7 
III. 研究架構與方法 ..............................................................................................12 
3.1 構音異常類型判斷 .....................................................................................12 
3.2 構音教學活動之回饋 .................................................................................18 
3.3 語者調適 .....................................................................................................20 
3.4 構音訓練回饋 .............................................................................................21 
3.4.1 整合長度模型之兩階段語音切割 ..................................................22 
3.4.2 3D 動畫合成建立..............................................................................27 
IV. 實驗結果與討論...............................................................................................32 
4.1 構音異常類型判斷 .....................................................................................32 
4.2 人工標記之構音異常類型判斷 .................................................................33 
4.3 語音自動化構音異常類型判斷 .................................................................43 
4.4 人工標記之自動化構音異常類型判斷 .....................................................48 
4.5 構音教學活動之回饋介面 .........................................................................50 
4.6 語者調適的效能 .........................................................................................52 
4.7 華語語音訊號切割實驗 .............................................................................54 
4.7.1 IF 邊界切割實驗 ...............................................................................54 
4.7.2 語音段落邊界切割實驗 ..................................................................54 
4.7.3 決策樹於語音長度模組實驗 ..........................................................56 
4.7.4 動畫回饋介面與效果評估 ..............................................................58 
V 結論.....................................................................................................................61 
VI 參考文獻 ...........................................................................................................63 
計畫成果自評 .........................................................................................................67 
 IV
ABSTRACT 
Articulation errors will seriously reduce speech intelligibility and the ease of spoken 
communication. Typically, a speech-language pathologist uses his or her clinical experience to 
identify articulation error patterns, a time-consuming and expensive process. In this study, an 
articulation diagnostic and teaching activities system is proposed to assist language therapists 
and articulation disorders. The articulation error patterns in phonetic can be identified and 
considered to generate pronunciation confusion network. Using speech recognition technique, the 
pronunciation errors can be automatically identified and labeled. The articulation error pattern 
can be identified by modified dependency network. Articulation teaching activities for each type 
of articulation errors is feed back to users. A 3D virtual facial animation including lip and tongue 
information is proposed to provide multi-model feedback such as speech signal, lip motion, and 
tongue motion. Experimental results reveal the practicability of proposed method and system. 
 
Keywords : Articulation Disorder, Articulation Diagnostic, Speech Recognition, 3D Virtual 
Facial Animation 
 
 2
以慢慢達到一個可接受的程度；因此辨識技術可以廣泛應用，舉凡病歷輸入、醫藥咨詢、
衛生教育、語音控制、電子商務、自動總機、安全控管、教學輔助、視聽娛樂、以及各類
查詢交易系統等等，真可說是不勝枚舉。另一方面，由於近年來電腦科技的發達，3D 虛擬
人臉動畫(3D Facial Animation)之應用日漸廣泛，諸如娛樂、網路資訊與教育學習之電腦多
媒體介面，而利用電腦多媒體技術之輔助介面已經成為現今提升教育學習品質與學習效率
之有利工具。在溝通學習輔具方面，結合 3D 虛擬臉部動畫之輔助學習系統已成會現階段
資訊教育之新思潮，利用動畫合成之技術，解決了以往在電腦輔助語言學習系統利用預製
的影片或動畫進行發聲器官運動之回饋播放的動作【9,10,11,12】之缺點，給予使用者更充
足的回饋資訊，且對於教材制訂變化彈性相當大，可達到大量語言上與知識上之學習輔助。 
除此之外，由於外語學習的風氣極為盛行，不只是要求聽讀寫方面的能力，更希望能
有足夠的能力用外語溝通，故目前相當多研究透過語音辨識技術的電腦輔助語言學習系
統，使用者不僅可以在一個無壓力的環境下學習，更可以針對個別的發音問題，給予適當
的糾正與回饋機制，讓使用者針對個人的錯誤反覆的練習【13,14,15,16】，這不僅節省了人
力、時間，同時也可達到較高的學習效果。 
然而以醫療方面而言，近幾年來語音辨識技術對於語障者復健上的應用已具備極大潛
力，但卻甚少被技術開發者所認識到。雖然國內已有若干研究提到輔助語言障礙者方面的
技術開發。但大多研究也都只朝向音控、聽寫、助講器…等方面著手【17,18】，而關於說
話訓練部分，也就是語音辨識系統用於輔助語障者臨床上練習構音正確性的研究則是幾乎
沒有。就上述因素而論，為了提昇醫療效能，必要自行開發一套針對華語構音與音韻異常
的系統，以協助臨床語言治療師診斷和幫助使用者矯正治療。 
語言障礙兒童因其智能、閱讀能力、分析聽覺順序、面對社會等能力不及一般兒童，
容易造成心理上極大的挫折感，而進一步的嚴重影響到人格發展與人緣關係【1】。但就目
前台灣的語言治療師人數看來，其專業人才極度缺乏且不敷使用，故應妥善利用其他工具，
來克服並協助語言治療師，以提升語言治療品質。 
本研究針對構音障礙病患，嘗試結合語音辨識技術與互動式人機介面，建立構音練習
輔助系統，並應用於輔助臨床語音治療師診斷，藉由本研究之研究可達成： 
一. 發展本土化構音輔助教學系統； 
二. 利用語音辨識協助構音異常之偵測； 
三. 以統計模型作構音異常類型診斷； 
四. 增加教材隨使用者需求隨時進行調整的彈性； 
五. 透過構音模仿輔助說話發音品質之提升； 
六. 利用回饋構音教學動畫與活動以達到居家練習。 
 4
 
表 2.1 國語聲母發音的方法及發音位置 
塞音(Stops) 塞擦音(Affricate) 
清音(Voiceless) 清音(Voiceless) 
發音 
方法 
發音 
位置 
不送氣
(Unaspirated) 
送氣
(Aspirated)
不送氣
(Unaspirated)
送氣
(Aspirated)
擦音
(Fricative)
鼻音
(Nasal) 
邊音
(Lateral) 
顫音
(Trill)
雙唇音
(Bilabial) 
b(ㄅ) P(ㄆ)    m(ㄇ)   
唇齒音
(Labio-dental) 
    f(ㄈ)    
舌尖前音
(Dento-alveolar) 
  zi(ㄗ) ci(ㄘ) si(ㄙ)    
舌尖中音
(Alveolar) 
d(ㄉ) T(ㄊ)    n(ㄋ) l(ㄌ)  
舌尖後音
(Retroflex) 
  zhi(ㄓ) chi(ㄔ) shi(ㄕ)   ri(ㄖ)
舌面音
(Alveo-palatal) 
  j(ㄐ) q(ㄑ) x(ㄒ)    
舌根音 
(Velar) 
g(ㄍ) K(ㄎ)   h(ㄏ)    
 
表 2.1 國語韻母發音的方法及發音位置 
 
開口呼 
(kai kou hu) 
齊齒呼 
(qi chi hu) 
合口呼 
(he kou hu) 
撮口呼 
(cuo kou hu) 
 yi (ㄧ) wu (ㄨ) yu (ㄩ) 
a (ㄚ) ya (ㄧㄚ) wa (ㄨㄚ)  
o (ㄛ) yo (ㄧㄛ) wo (ㄨㄛ)  
e (ㄜ)    
e (ㄝ) ye (ㄧㄝ)  yue (ㄩㄝ) 
單韻母(Mono-Vowel) 
er (ㄦ)    
ai (ㄞ) yai (ㄧㄞ) wai (ㄨㄞ)  
ei (ㄟ)  wei (ㄨㄟ)  
ao (ㄠ) yao (ㄧㄠ)   
複韻母
(Composed-Vowel) 
ou (ㄡ) you (ㄧㄡ)   
an (ㄢ) yan (ㄧㄢ) wan (ㄨㄢ) yuan (ㄩㄢ) 
en (ㄣ) yin (ㄧㄣ) wen (ㄨㄣ) yun (ㄩㄣ) 
ang (ㄤ) yang (ㄧㄤ) wang (ㄨㄤ)  
鼻韻母(Nasal-Vowel) 
eng (ㄥ) ying (ㄧㄥ) weng (ㄨㄥ) yung (ㄩㄥ)
 6
 
圖 2.1 使用雷射掃瞄器所產生的三維人頭模型 
(B) 電腦圖學技術為基礎 
電腦圖學為基礎表現立體物件的方式主要是利用幾何曲線和幾何圖形表現出人臉表面
區度，而這些曲線仍利用數學多項是逼近而得，再藉由經驗法則以人工方式進行圖形形狀
之調整以符合其需求【24】。一般常用的表面描述技巧包含：絕對表面(Implicit Surfaces)、
參數表面(Parametric Surfaces)、多邊形表面(Polygonal Surfaces)。下 2.2 與 2.3 分別為利用曲
線所建立與多邊型所建立之人臉模型範例。 
 
圖 2.2 曲線建立之人臉模型 
 
圖 2.3 多邊形組成的人臉 
當臉部 3D 模型建立完成後，接下來步驟便是模擬其動作位移，而在臉部動作模擬方
 8
 
南台科技大學建置一套可攜式的語音辨識訓練器為要改善語障者的發音【18】，此訓練
器架構圖如圖 2.7 所示；主要是在語言治療師幫患者做構音矯治時，能錄製到語障者自己
最佳的發音，讓語障者隨時都能依據自己最佳的語音做反覆的練習，但是著重於硬體設計，
且只能給於固定回饋訊號與發音訊號的評分，亦無法提供相關構音錯誤特性。 
在國外方面，Glassman 應用語音特徵參數 (頻率特性)，針對 TBI、CHI 及中風患者，
進行 14 組輔音的兩兩混淆音之間的辨別【32】；而 Bunnell 則在在 2000 年時，運用 DHMMs
的語音辨識模型，針對 4-6 歲的兒童作輔音兩兩之間的音節辨識，判斷是否有替代的構音
問題【33】，而以上兩篇都無法進行詞彙測試。 
 
圖 2.4 構音與音韻異常語料庫網站系統架構圖 
 
語音訊號
收集 前置處理 模糊化
分 類
(調變加權值)
類神經網路
主頻率 (principal frequency)
諧頻 (harmonic frequency)
快速傅利葉法
模仿、取代、扭曲及省略
17個待測音（聲母）
 
圖 2.6 類神經網路在兒童構音異常診斷上之流程設計方塊圖 
 10
限於針對使用者發音錯誤類型以詞彙方式給予回饋，且對於華語適用度並不高。圖 2.8 為
此系統之發音動畫回饋介面。 
 
圖 2.8 ARTUR 發音動畫回饋介面 
Ting 的研究中，提出了一套針對馬來西亞語系之構音錯誤辨識與發聲器官視覺聽覺回
饋訓練【12】，設計針對單一音素、音節與詞彙進行發聲器官資訊的回饋，如圖 2.9(a)(b)(c)
所示，但是，回饋之教材只針對爆破音(Plosive)，變化彈性不大，且對於華語適用度較低。 
  
(a) (b) (c) 
圖 2.9 Malay Articulation Training 介面 
成功大學，張志民建置一套應用於語障者之發音訓練器【34】，其主要架構圖如圖 2.10
所示，利用頻譜分析的技術來幫助語言治療師語藉由頻譜圖回饋分析障者構音部位的問
題，而圖形化之頻譜圖回饋可以讓語障者明白自己的發音是否和之前訓練時比較正確的音
相似，但是回饋頻譜圖並無法直接的讓使用者得知正確的發音方式。 
成功大學，鄭智仁研究中建立了中文轉譯手語與手語影像生成之手語影像輔助教學系
統【35】，可直接將中文語句轉譯成手語序列影像進行回饋，並可進一部整合成手語真人電
子書籍 CAI 軟體，並應用於手語教育訓練。利用系統合成之真人影像的自然比劃動作，讓
聾人可以最親切自然的模式理解訊息，最終達成聾人與聽人間溝通之輔助目標。中文轉譯
手語影像生成系統如下圖 2.11 所示。 
 12
III. 研究架構與方法 
本研究輔助系統之完整架構圖，如下圖 3.1 所示。首先語言治療師會選擇圖卡，使用
者則根據圖卡將語音訊號利用麥克風輸入系統，而語音再經參數擷取的動作，將語音轉成
語音特徵向量，整合臨床語言治療之構音錯誤規則，依據提示圖卡內容建立構音障礙偵測
發音混淆網路，並應用於導因語音特徵向量與系統模組比對之搜尋空間，加快搜尋速度並
提高正確性，偵測的結果在依據統計模型做構音異常類型的判斷，而此判斷將診斷出使用
者是屬於臨床上的哪一種構音障礙類型，此外，系統會依據構音異常類型的判斷結果，與
臨床上各類型構音矯治活動作分析，提供適當的矯治練習活動給語言治療師及使用者參
考，並給予構音訓練之動畫回饋，以達到復健治療的效果。在構音回饋互動介面中，針對
構音器官之特性合成輸出之動畫，包含正確的舌位、唇形之視覺回饋與發音之聽覺回饋，
使用者得到動畫回饋資訊的輔助能更加容易理解教材內容，且得到回饋之訊後再進行模仿
發音，達到使用者發音品質的提升。由於目前研究所使用的聲學模型為成人語料所訓練得
到，所以希望利用語者調適的技術將模型做調整，而模型調適的精神在於，將原有的聲學
模型，運用少量調適語料，經由參數的調適，來產生一組新的聲學模型，使它更具有強健
性，讓語音辨識系統的準確率提升，達到更好的構音偵測效果。 
構音動畫回饋
語言治療師
語言學專家 構音訓練活動設計
構音訓練活動
使用者
構音異常之
聲學特性偵測
構音異常之
聲學特性分析
構音異常之聲學特性
構音評估圖卡
語言學之構音特性
臨床構音異常類型
顯示圖卡
語音
回饋適合使用者之構音訓練活動
構音訓練活動
挑選
回饋構音之訓練動畫
 
圖 3.1  構音輔助系統架構圖 
3.1 構音異常類型判斷 
臨床構音異常的鑑定方式最常使用的為測試詞彙檢查，語言治療師為了判別、分析、
整理構音障礙的類型，利用預先準備圖卡，令兒童說出圖卡上所畫物品的名稱，內容應為
日常生活熟悉的物品，並且包括所有韻母與聲母的語句。再以上述取得的語言樣本，加以
分析錯誤音的種類、類型、一致性等【1】。將所有測試詞彙集合 (W ) 定義為下： 
 14
( ) ( ) 11 ˆgh h g g g hL E P E S S O HG η η⎛ ⎞∝ >⎜ ⎟⎝ ⎠∑  (3.4) 
其中 ˆgS 為 gS 之系統/人工判斷標記結果，η為常數係數，當η為 1 時，則是求平均，當η為
∞則是求最大值， hH 為第 h 種構音錯誤類型之臨界值。 
g
hE
gS
gO
ˆ
gA ˆ gD
ˆ
gS
 
圖 3.4 構音異常類型判斷之 Modified Dependency Networks 
G
hE
GS
GO
ˆ
GA ˆGD
ˆ
GS
hE
1
hE
1S
1O
1Aˆ 1Dˆ
1ˆS
2
hE
2S
2O
2Aˆ 2Dˆ
2Sˆ
 
圖 3.5 構音異常類型判斷之整體架構圖 
 16
( ) ( )( ) ( )( )
ˆ ˆ
ˆ |
g g g g
g g
g g
P S S C S S
P D S
P S C S
= =  (3.9) 
其中 ( )gC S 為 gS 在訓練資料中出現次數。然後根據機率，統計出常犯的發音錯誤類型，在
辨識時考慮此詞句所有可能的發音錯誤，建立其對應的發音混淆網路  (Pronunciation 
Confusion Network)，圖 3.6 為發音混淆辨識網路示意圖，其中
nnk
S 為第 n個目標音的第 nk 個
混淆音。 
 
圖 3.6 發音混淆辨識網路示意圖 
再利用維特比 (Viterbi) 演算法之搜尋空間，進行動態樣型比對，且透過發音混淆辨識
網路可降低搜尋空間，取得比對路徑之最大事後機率，透過簡單的回溯方式取得最佳比對
路徑及對應聲學單位斷點，如此便能偵測出發音錯誤的類型。而維特比演算法包含四個步
驟，分別是初始、遞迴、結束、迴朔路徑，介紹如下【36】： 
首先定義維特比演算法中的四個變數： 
(1) ( )gt iδ  計算出累加狀態序列的最佳路徑的機率值； 
(2) ( )gt iψ  紀錄目前的最佳狀態是由那個狀態轉移而來的； 
(3) *gP 為 Viterbi algorithm 最後的機率結果； 
(4) * * * * *1 2 3, , , ,g g g g gTS S S S S= " 為最佳狀態序列。 
維特比演算法的四個步驟如下： 
Step1. 初始 (Initialization) 
( ) ( )
( )
1 1
1
,1
0
g gi gi g
g
i b o
i N
i
δ π
ψ
⎧ =⎪ ≤ ≤⎨ =⎪⎩
 (3.10) 
Step2. 遞迴 (Recursion) 
( )11 1ˆ |P S S
( )1 1*ˆ|P O S
11ˆk
S
12Sˆ
11Sˆ 21Sˆ
22Sˆ
1
ˆ
nS
2
ˆ
nS
22
ˆ
kS ˆ nnkS
( )12 1ˆ |P S S
( )11 1ˆ |kP S S
( )2 2*ˆ|P O S ( )*ˆ|n nP O S
( )21 2ˆ |P S S
( )22 2ˆ |P S S
( )22 2ˆ |kP S S
( )1ˆ |n nP S S
( )2ˆ |n nP S S
( )1ˆ |nk nP S S
 18
g
hE
gS
ˆ
gS
 
圖 3.7 人工標記的構音異常類型判斷之 Dependency Networks 
3.2 構音教學活動之回饋 
語言治療師余玻莉提出臨床上構音異常治療步驟【37】如下： 
(1) 增加自問題意識：讓孩子知道自己是哪些構音不清楚，能夠進而自提醒。 
(2) 分析異常音及正確音：讓小孩能分辨那個音是正確的發音，那個是錯誤的，提高
自便別的能力。 
(3) 建立正確之發音 A.發音刺激、B.移音矯正、C.分析語音、D.詞句練習、E.交談式練
習。 
(4) 語音語言遊戲：所有的語言遊戲都是要激發孩子說更多的語音及更多的話出來。 
且語言治療師吳咸蘭亦編寫構音教學活動彙編【38】，整合上述臨床治療步驟和教學活動，
回饋矯治活動建議合計歸納整理出六種的構音錯誤類型所需達成的活動目標，再針對不同
活動與錯誤類型，設計出十二種的矯治活動建議，如下表 3.1，而詳細的活動內容可參考表
3.2，讓受測者能在平時以其他活動方式達到更有效的矯治效果。 
根據語言治療師的臨床經驗，功能性構音障礙具有相當高的治療率，使用者必須透過
不斷構音練習，經由日積月累的訓練過程來培養正確構音習慣，讓其獲得正常的構音行為
及語音清晰度，完成構音矯治療程。因此本系統會根據使用者的構音異常類型，透過構音
訓練活動作分析進行構音教學活動挑選，讓使用者能以多元化的構音訓練方式，讓使用者
在語言遊戲進行中，能習慣化及自動化使用用校正後的構音行為，而提高構音矯治效率。 
 20
3.3 語者調適 
將結合最大事後機率調適法 (Maximum a posteriori，簡稱 MAP) 【39】以及最大相似
度線性迴歸調適法 (Maximum-Likelihood Linear Regression，簡稱 MLLR) 【40】，稱之為結
合最大相似度線性迴歸與最大事後機率調適法 (Joint MLLR and MAP adaptation) 【41】，來
做語者調適，其示意圖如下： 
 
圖 3.8 結合最大相似度線性迴歸與最大事後機率調適法之示意圖 
首先，初始模型參數會先經由最大相似度線性迴歸法來作調適，因為所有的模型參數
都會被調適到，所以稱之為全域調適 (Global Adaptation)；其演算法的主要觀念，為假設調
適後語音模型的高斯平均參數和初始模型為一線性回歸函數的關係，所以此演算的目標就
是，對一群集 s ，計算一轉換矩陣 Ws，使得群集內所有調適資料的相似度最大，最大相似
度線性迴歸調適演算法的好處在於，調適語料不需要完全涵蓋所有模型，即使沒有調適資
料的模型，也可以經由相同類別的轉換矩陣進行調適。其最大相似度線性迴歸演算法示意
圖如下圖 3.9，而最大相似度線性迴歸演算法如下所示： 
ˆ s s sWμ ξ=  (3.15) 
其中 ˆsμ 為調適後的 Gaussion mean， sW 代表狀態 s 所屬的轉換矩陣， sξ 可以看成為初始模
型的 Gaussion mean，稱為 extended mean，vector [ ]1 2ˆ 1   nsξ μ μ μ= " 。 
然後，經由最大相似度線性迴歸法調適後之模型參數，再使用最大事後機率調適法作
進一部之調適動作，因為只有部分模型之參數有被調適到，所以稱之為區域調適 (Local 
Adaptation)。其基本精神在於結合了事前機率以及調適語料，來估測出新的模型參數。 MAPΦ
表示模型參數根據最大事後機率估測值，利用貝氏定理將事後機率拆解成事前機率與相似
度的結合， ( )|p xΦ 為 ( )p Φ 是觀測值 x 的機率， ( )p Φ 表示參數事前機率， ( )|p x Φ 為觀
 22
資料庫中得到文字資訊，經由文字轉語音系統(Text-to-Speech, TTS)進行語音合成並取得語
音對應音節邊界、時間週期長度及前後文相關之語言學資訊，依據音節邊界與時間週期，
使用兩階段語音切割演算法對於語音訊號進行音素語音區段切割，藉由語音切割技術，將
能給予各音素在不同的音節組合下的週期長度資訊，輔助提升人工標記之效率，並進而有
效降低 3D 人頭的嘴唇與舌位之合成單元數量。 
在嘴唇與人頭的模型，運用臉部表情動作與嘴唇的控制點進行整個人頭的動作合成，
藉由控制點的技巧，可有效降低合成單元的資料量。而在舌頭模型中，舌頭最為難量測，
因此透過語言學的知識與 2D 的舌頭位置，將其對應至 3D 舌位控制點。在合成方面，依據
語音的長度對於 3D 虛擬人頭的參數進行內插與 B-spline 平滑化，如此控制點參數在畫面間
的變化平順，且經由各控制點的座標與相鄰網點的距離計算出權重值，再利用控制點在各
時間之移動量乘上鄰近網點之權重計算出各網點在各時間之移動量，最後合成出 3D 虛擬
合成人頭動畫。經由構音回饋互動介面所合成輸出之動畫，可給予包含正確的舌位、唇形
之視覺回饋與發音之聽覺回饋，使用者得到動畫回饋資訊的輔助能更加容易理解教材內
容，且得到回饋之訊後再進行模仿發音，達到使用者發音品質的提升。 
 
圖 3.11 構音回饋輔助訓練系統架構圖 
3.4.1 整合長度模型之兩階段語音切割 
流程圖如圖 3.12 所示，首先應用小量已標記之語音資料，依據語音長度經由決策樹
(Decision Tree, DT)分析後，建立具上下文相關之語音長度模組，並整合於兩階段語音切割
中；在兩階段語音切割方法中，首先取得語音訊號之語音參數，依據上下文關係，並利用
本研究可得知斷點個數之特性，依序透過逐次前饋式搜尋法(Sequential Forward Selection, 
SFS)【42】挑選適當斷點，針對 IF 斷點(子音接母音或母音接子音)與 FF 斷點(母音接母音)
進行語音切割，而其切割之判斷準則分別為 Hotelling’s T2 Test 與 BIC。 
 24
式所描述的三角形濾波器，然後累積起來，求出對數值，得到： 
⎪⎭
⎪⎬
⎫
⎪⎩
⎪⎨
⎧= ∑+
−=
1
1
)()(log)( 2
m
m
f
fk
m kBkXmY  (3.18) 
最後對全部 M 個濾波器輸出的能量再經餘弦轉換(Cosine Transform)求得梅爾倒頻譜係數。
表示如下： 
∑
= ⎟⎟
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜⎜
⎜
⎝
⎛ ⎟⎠
⎞⎜⎝
⎛ −
=′′
M
m
x M
mn
mY
M
nc
1
2
1
cos)(1)(
π
 (3.19) 
)(ncx′′ 就是訊號 )(nx 的梅爾倒頻譜係數。 
 
圖 3.6 梅爾倒頻譜參數擷取流程 
(B) 逐次前饋式搜尋法 
逐次前饋式搜尋法(Sequential Forward Selection, SFS)是一種 Bottom Up 的特徵變數選
取方法，主要利用一維的變數組合開始開始逐一測試，依據特徵變數的分類效果一次增加
一個維度的特徵變數，直到達到所有要求的特徵變數數目為止。假設欲從 N 個特徵變數中
找出 M 個特徵參數，則選取步驟如下： 
1. 首先定義系統中的 Criterion Function(CF)，接著由{ }Nxxxx ...........,, 321 之特徵變數組合
中找出單一維度 CF 為最佳之特徵變數，假設為{ }1x 。 
2. 以上一個步驟選取到的變數為基礎，增加一個維度，並計算所有可能的二維組合之
CF 值，即 ),( 21 xxC 、 ),( 31 xxC 、….. ),( 1 nxxC ，並選出最佳的 CF 值組合。 
3. 同第二步驟再增加一個維度，找出最佳 CF 值的三維特徵變數組合，直到挑選出的維
度數目符合需求 M 為止。 
以此觀點應用到斷點選擇的問題上，即假設欲從 T 個樣本點中找出 M 個來當作斷點，
其示意圖如下圖 3.14。在語音樣本序列中假設欲選取出 M=2 之斷點個數，首先可由語音區
段中選取單一個 CF 為最佳值之樣本點為斷點 t1，接下來考慮以 t1 為邊界之 1Λ 與 1Λ 區段各
 26
(D) 賀德臨 T2 檢定 
賀德臨 T2 檢定係由 Hotelling【44】所提出之檢定方法，假設觀察樣本序列 X={x1x2…xN}，
其中 xi為第 i 個獨立之 M 維常態特徵向量，假設第 b 與 b+1 個觀察樣本間有斷點，則假設
此兩群組具備最大差異性，其假設檢定方式如下： 
),(~,...,2,1:0 ΣμNNxxxH  (3.24) 
與 
),2(~,...,2,1      
),1(~,...,2,1:1
Σ++
Σ
μ
μ
NNxbxbx
NbxxxH  (3.25) 
透過 Likelihood-ratio 進行估算此假設檢定，可得 Hotelling’s T2 Test 如下： 
∑−′= 12 b bbb yyT  (3.26) 
其中 
( ) ( )21 μμ −−= N
bNbyb  (3.27) 
Σ代表觀測樣本特徵向量之共變異矩陣， 1μ 與 2μ 分別為斷點之前後語音區段之平均值。當
b 點之檢定值為最大時(拒絕虛無假設 H0)，此時該 b 點為聲學轉換點b′。 
2maxarg b
b
Tb =′  (3.28) 
(E) 長度模型 
在語音長度模組建構方面，應用變異係數(Coefficient of Variance)進行評估語音長度模
組之適合度，其定義如下： 
100×= d
n
d
n
n x
sV  (3.29) 
其中 dnx 與 dns 分別為語音長度之平均值與標準差，在挑選不適當模組之門檻值為 
cvcv wσμτ +=  (3.30) 
其中 
∑=
n
n
cv V
N
1μ  (3.31) 
與 
( )∑ −=
n
cv
n
cv V
N
21 μσ  (3.32) 
 28
使的語言豐富而多變化。在聲母方面其發音位置與發音方法主要依口腔隊氣流阻塞的情況
來區分【19,48】，由表 2.1 所示，可得知國語聲母之發音位置與發音方法，並且可針對各發
音位置與發音方法得到其發音描述，如下表 3.3 與表 3.4 所示；韻母其分類方式如表 2.2 所
示，依據發音時口腔不受阻塞，且發音模式主要依據唇形與舌面的位置來區分【19,48】，
如下表 3.5 所示。 
表 3.3 國語聲母發音位置之描述 
發音位置 描述 類別 
雙唇音 雙唇是活動部分，上下唇接觸發出的聲音。 ㄅㄆㄇ 
唇齒音 下唇是活動部分，下唇和上齒接觸發出的聲音。 ㄈ 
舌尖音 舌尖是活動部分，讓舌尖接觸上齒齦發出的聲音。 ㄉㄊㄋㄌ 
舌尖前音 舌尖是活動部分，讓舌尖接觸上門齒齒背所發出的聲音。 ㄗㄘㄙ 
舌尖後音 
舌尖是活動部分，讓舌尖翹起接觸
硬顎前部所發出的聲音。舌尖後音
也叫「翹舌音」。 
ㄓㄔㄕㄖ 
舌面音 舌面是活動部分，讓舌面前部接觸硬顎前部發出的聲音。 ㄐㄑㄒ 
舌根音 舌面後面是活動部分，讓舌面後部接觸軟顎前部所發出的聲音。 ㄍㄎㄏ 
表 3.4 國語聲母發音方法之描述 
發音方法 描述 類別 
塞音 發音時氣流通路完全阻塞，然後突然打開，讓氣流爆發出來。 ㄅㄆㄉㄊㄍㄎ 
擦音 
發音時氣流通路縮小，並不完全阻塞，
讓氣流從這個狹小的通路中擠出去，發
出摩擦的聲音。 
ㄈㄙㄕㄖㄒㄏ 
塞擦音 
發音時氣流通路完全阻塞，然後慢慢打
開，留出一個狹小的通路，讓氣流擠出
來。發這種音是先阻塞後摩擦。 
ㄗㄘㄓㄔㄐ 
鼻音 
發音時口腔的通路完全塞，軟顎下降，
打開鼻腔的通路，讓氣流完全從鼻腔出
去。 
ㄇㄋ 
邊音 發音時舌頭中間的通路阻塞，氣流從舌頭的兩邊出來。 ㄌ 
 30
 
圖 3.10 唇部特徵點位移對應到 3D 模型之示意圖【16,45,46】 
(C) 動畫合成 
在舌位合成部份，由兩階段切音得到音素長度資訊，利用此週期長度估算各音素之動
畫所需的 Frame 數，並利用所得資訊依序挑選並串接適當動畫基本單元，接下來使用
B-spline 曲線進行平滑化處理，其方式為假設 )(tP 代表 B-spline 曲線上任一點的位置向量，
對於一個 p 次(Degree)的 B-spline 曲線定義如下： 
∑
=
≤≤=
n
t
pit tNBtP
0
, 1t0               )()(  (3.34) 
其中向量 iB 為 B-spline 曲線之控制點，其個數總共有(n+1)個。而 piN , 為 t 方向、次數為 p的
多項式，又稱為 B-spline 曲線的基礎函數(Basis Function)，可用片段 Cox-deBoor 遞迴公式
計算，其定義如下： 
⎩⎨
⎧ <≤≤= ++
otherwise
ttandtttif
tN iiiii      0
                    1
)( 110,  (3.35) 
)()()( 1,1
11
1
1,, tNtt
tt
tN
tt
tt
tN pi
ipi
pi
pi
ipi
i
pi −+
+++
++
−
+ −
−+−
−=  (3.36) 
在 3.20 式中， it 為 t 方向節點向量(Knot Vector)中的節點(Knot)。 
B-spline 曲線具有以下特性： 
1. 控制點數目不受曲線次數限制。 
2. 具有凸殼特性(Convex Hull Property)， p次 B-spline 曲線上的所有點 )(tP ，都會落在由
B-spline 控制點所構成的最小凸多邊形內部。 
3. 具有局部性(Local)，即改變或重複一控制點僅會影響此控制點鄰近曲線的形狀，如圖
 32
IV. 實驗結果與討論 
4.1 構音異常類型判斷 
臨床病例資料 
統計資料為成大醫院耳鼻喉科語言治療室所收集的臨床病例 453 例，包括男生 312 人，
女生 141 人，平均年紀約 6 歲，其年齡分佈長條圖如下圖 4.1，病歷資料年份為表 4.1。在
病例中含有其他生理疾病或語言問題之人數統計如下表 4.2；在病例中大多為複合式構音異
常之類型，共有前置化病徵有 45 例、後置化病徵有 179 例、不送氣化病徵有 88 例、塞音
化病徵有 297 例、塞擦音化病徵有 106 例、省略音化病徵有 42 例。 
0
20
40
60
80
100
120
140
160
2歲 4歲 6歲 8歲 10歲 12歲 14歲 17歲 19歲 21歲 25歲 31歲
年齡
人
數
 
圖 4.1 臨床病例年齡之長條圖 
表 4.1 病歷資料年份 
年份 人數 
1998 1 
1999 4 
2000 19 
2001 97 
2002 134 
2003 102 
2004 96 
 34
( )
( )
( )
( )
( )
( )
( )
( )      
TP FN TP FP
C
TP TN FP FN TP TN FP FN
FN TN FP TN
TP TN FP FN TP TN FP FN
⎧ ⎫ ⎧ ⎫+ +⎪ ⎪ ⎪ ⎪= × ×⎨ ⎬ ⎨ ⎬+ + + + + +⎪ ⎪ ⎪ ⎪⎩ ⎭ ⎩ ⎭
⎧ ⎫ ⎧ ⎫+ +⎪ ⎪ ⎪ ⎪×⎨ ⎬ ⎨ ⎬+ + + + + +⎪ ⎪ ⎪ ⎪⎩ ⎭ ⎩ ⎭
；
 (21) 
Kappa︰ ( )( )1
O C
C
−
−  (22)  
將系統以上述 ( )ˆ|gh g gP E S S 統計所得的結果再計算，分別依據前置化、後置化、不送氣
化、塞音化、塞擦音化、省略化以曲線分佈，並且畫出每個構音異常類型的 ROC 曲線所得
如下列各圖 4.2，根據 ROC 曲線圖，將判斷各構音異常類型的臨界值定義為相等錯誤率 
(Equal Error Rate)，可得臨界值如下表 4.5，每個錯誤類型的相等錯誤率如下表 4.6。由於
ROC 曲線是描繪系統真陽性率與偽陽性率的線性圖，因此選擇越接近左上角落的點作為臨
界點，可在敏感度及特異度取得平衡點。從實驗結果表 4.6 中可觀察出後置化的判斷效率
有比其他構音障礙類型判斷效果略低現象，但在各個構音異常類型的判斷上都有九成以上
的辨識率。 
 36
(b) 後置化 
 錯誤音 
 ㄅ ㄆ ㄇ ㄈ ㄉ ㄊ ㄋ ㄌ ㄍ ㄎ ㄏ ㄐ ㄑ ㄒ ㄓ ㄔ ㄕ ㄖ ㄗ ㄘ ㄙ Null
ㄅ                       
ㄆ                       
ㄇ                       
ㄈ                       
ㄉ                       
ㄊ                       
ㄋ                       
ㄌ                       
ㄍ                       
ㄎ                       
ㄏ                       
ㄐ                       
ㄑ                       
ㄒ                       
ㄓ                       
ㄔ                       
ㄕ                       
ㄖ                       
ㄗ                       
ㄘ                       
目標音
 
ㄙ                       
 
 38
(d) 塞音化 
 錯誤音 
 ㄅ ㄆ ㄇ ㄈ ㄉ ㄊ ㄋ ㄌ ㄍ ㄎ ㄏ ㄐ ㄑ ㄒ ㄓ ㄔ ㄕ ㄖ ㄗ ㄘ ㄙ Null
ㄅ                       
ㄆ                       
ㄇ                       
ㄈ                       
ㄉ                       
ㄊ                       
ㄋ                       
ㄌ                       
ㄍ                       
ㄎ                       
ㄏ                       
ㄐ                       
ㄑ                       
ㄒ                       
ㄓ                       
ㄔ                       
ㄕ                       
ㄖ                       
ㄗ                       
ㄘ                       
目標音
 
ㄙ                       
 
 40
(f) 省略音化 
 錯誤音 
 ㄅ ㄆ ㄇ ㄈ ㄉ ㄊ ㄋ ㄌ ㄍ ㄎ ㄏ ㄐ ㄑ ㄒ ㄓ ㄔ ㄕ ㄖ ㄗ ㄘ ㄙ Null
ㄅ                       
ㄆ                       
ㄇ                       
ㄈ                       
ㄉ                       
ㄊ                       
ㄋ                       
ㄌ                       
ㄍ                       
ㄎ                       
ㄏ                       
ㄐ                       
ㄑ                       
ㄒ                       
ㄓ                       
ㄔ                       
ㄕ                       
ㄖ                       
ㄗ                       
ㄘ                       
目標音
 
ㄙ                       
 
表 4.4 系統判別公式示意圖 
實際 
結果 
True False 
Positive 
True Positive  
(TP) 
False Positive 
(FP) 
Negative 
False Negative 
(FN) 
True Negative 
(TN) 
 
 
 
 42
 
(d) 
 
 
 
(e) 
 
 
(f) 
 
圖 4.2 各構音異常類型判斷之分佈圖及 ROC 曲線 
(a) 前置化 (b) 後置化 (c) 不送氣 (d) 塞音化 (e) 塞擦音化 (f) 省略化 
 44
實驗結果 
在語音辨識系統中由於當比對的字彙量大時，混淆字組便增多而影響辨認率，且當字
彙量多時則儲存之記憶體及辨認時比對之次數便顯著增加，而使辨認速度變慢；故系統針
對構音異常的特性建立發音混亂辨識網路，首先統計臨床病例的目標音發音成異常音機率
之分佈 ( )ˆ |g gP S S ，可得到下表 4.7。並統計出所有機率值對應累積次數的直方圖，為下圖
4.3 所示。並將 60%之過低的異常音機率音剔除，產生構音異常中替代音及省略音之常發生
發音錯誤型態，以達到將辨識範圍縮小，增加辨識速度及提高辨識率。 
在使用構音異常之發音混淆網路的語音辨識器，對構音正常語料作辨識率實驗可得到
表 4.8，在表中編號 1-5 為男生語料，編號 6 為女生語料，實驗結果平均辨識率有 87.72 %；
而構音異常語料的辨識結果，得到下表 4.9；而表 4.9 中發音正確的情況下，辨識率依舊有
八成以上，而在發音錯誤的辨識率裡，辨識率就下降至七成，而在發音錯誤且辨識錯誤中，
又可分成兩種情況，斜線上方為受測者實際發音錯誤而辨識器辨識成目標音的機率，而斜
線下方為實際發音錯誤而沒有被辨識器準確的辨識出實際發音，斜線上方的結果會影響構
音異常類型判斷，將原本會有的構音異常特徵被系統誤認為構音正常特徵，導致實際是構
音異常卻判斷為構音正常；而斜線下方的結果雖然沒如同斜線上會把異常特徵誤認為正常
特徵，但會使的在構音異常類型判斷時，將原本應具有的構音異常類型特徵判斷成其他構
音異常種類的特徵；因此為了提升構音偵測的辦識率，在將實驗發音錯誤部分詳細觀察，
並將製成表 4.10，此發音錯誤中包含了 408 音 1136 個次音節與非 408 音 201 個次音節，從
表 4.10 可觀察到在雖然發音錯誤但屬於 408 音裡辨識率還是為持有八成，而在非 408 音上
辨識率只剩下四成，而雖然系統有此問題產生，但由於這錯誤的特徵分佈分散，且非 408
音的比例不高，所以在做後判斷時，會因為異常特徵不明顯而被忽略，為了解決實驗所發
現的問題，未來可大量錄製兒童異常構音語料，將構音異常中的非 408 音的部分製作成聲
學模型，以提高異常構音辨識率。 
 46
表 4.8 在正常人的構音辨識率 (%) 
編號 辨識率 
1 93.85 
2 92.98 
3 91.23 
4 88.60 
5 86.84 
6 72.80 
平均辨識率 87.72 
 
表 4.9 在構音異常下的辨識結果 (%) 
 發音正確 發音錯誤 
正確 87.70 76.00 
錯誤 12.30 4.93 
19.07
 
表 4.10 針對發音錯誤作 408 音分析 (%) 
發音錯誤 
 408 音 非 408 音 
辨識正確 81.60 44.28 
目標音 3.26 14.43 
其他錯誤 15.14 41.29 
 
再將此構音異常語料的偵測標記結果經由構音異常類型判斷之統計模型，和表八系統
臨界值作決策得到結果如下表 4.11。雖然部分的判斷效果並不相當理想，但根據表 4.12 則
可以看出系統整體上在判斷構音異常類型效能還是有 75% 以上的表現，而部分效果有明顯
的較低，其主要應為自動化構音偵測標記誤差所造成的影響。 
 48
表 4.12 語音自動化之系統整體判斷效能 
Sensitivity Specificity Accuracy Kappa 
75.13 % 87.83 % 87.5 % 86.45 % 
4.4 人工標記之自動化構音異常類型判斷 
為了驗證語音自動化構音異常類型判斷的效果明顯下降，其主要是因自動化構音偵測
的標記誤差影響所造成，故在此實驗將系統改用人工標記方式作為輸入，再經由
( )ˆ|gh g gP E S S 之統計模型做構音異常類型判斷，觀察判斷結果的效果。 
首先將實驗 4-1-2 所錄製的異常構音語料，採用人工標記方式讓系統自動化判斷構音異
常類型，可得到結果表 4.13，而整體判斷效能為表 4.14。在這實驗結果中，可以觀察到使
用人工標記之自動化構音異常類型判斷效果會比使用語音自動化構音異常類型判斷好，因
此可以證明再用語音自動化標記上，構音異常的偵測會有相當明顯的影響。 
將實驗 4-1-1、實驗 4-1-2 與實驗 4-1-3 之間效能比較，可整理成下圖 4.4，從圖中可以
看出在人工標記裡自動化與非自動化的構音異常判斷有些許差異，由於差異極小，表示人
工化標記自動化判斷系統已經相當接近人工標記且人工判斷的部分，而這差異應是實驗案
例本身就已是系統的判斷錯誤率裡，或者是臨床的主觀判斷與系統判斷上的差異所造成，
且非自動化構音異常判斷實驗中，判斷的方式並非使用到所有詞彙的次音節；除此之外，
圖中的自動化構音異常判斷裡又可觀察到人工標記比語音自動化標記的判斷效果好，這驗
證了構音偵測辨識率的高低會嚴重影響系統對構音異常的判斷。 
 50
表 4.14 人工標記之系統判斷整體效能 
Sensitivity Specificity Accuracy Kappa 
83.33 % 98.61 % 92.5 % 92.20 % 
0
10
20
30
40
50
60
70
80
90
100
Sensitivity Specificity Accuracy Kappa
人工標記 人工標記之自動化 語音自動化
 
圖 4.4 不同標記方法的效果比較 
4.5 構音教學活動之回饋介面 
系統主要操作介面如下圖 4.5 所示，使用成大醫院國語構音測量表中的平衡詞彙，首
先依據使用者所選擇的測試詞彙呈現對應圖片，將系統/人工標記結果顯示在所選則測試詞
彙的右邊，而系統判斷構音異常類型結果如下圖 4.6 所示，並將各種構音異常類型以適當
的活動教學作為回饋，且可以使用教學影片方式播放活動，其介面如下圖 4.7 所示；在系
統的主要操作介面中也有構音障礙的介紹，說明的介面如圖 4.8 所示。 
 52
 
圖 4.7 構音活動影片教學 
 
圖 4.8 系統說明構音障礙介面 
4.6 語者調適的效能 
實驗語料 
此語料為成大醫院耳鼻喉科語言治療室所錄製，主要為其他研究所用，使用的詞彙為
33 個單音，所有單音為下表 4.15 所示，由 39 名平均 5 歲的兒童錄製，每名兒童都錄製 2
 54
從表 4.16 中可以觀察出，聲學模型調適前後辨識率有相當大的程度提升，而 MAP 的
效果都比 MLLR 的效果佳，其原因是單音屬性類別太少，且本實驗為 Inside test，所以才會
有此現象，最後結合 MLLR 與 MAP 的調適方法則將辨識率再稍微提升，因此藉由實驗結
果可證明雖然系統都以成人為實驗對象，且系統之聲學模型也使用成人的聲學模型，但利
用調適的方法也可讓系統適用於兒童。 
4.7 華語語音訊號切割實驗 
在華語語音訊號切割之實驗中，將驗證本系統對於音節之音素切割效能，並評估兩階
段語音切割技術之效果。在音素基本單元定義部分，本研究定義 31 類基本單元，如下表
4.17 所示： 
表 4.17 基本音素之定義 
b p m f d t n l g k h Initial： 
j q x zh ch sh r z c s  
Final： a o eh e i u n ng yu er  
語料方面，使用成功大學多媒體人機通訊實驗室所建立之 TTS 文字轉譯語音系統所轉
譯之 408 音節單音，每個單音包含 4 聲調，合計 1248 個單音，其中每個單音包含 1~4 個音
素不等，分別挑選 1047 個音素區段給予訓練使用，3647 個音素區段給予測試使用。語音
訊號規格之取樣頻率為 16kHz，解析度為 16bits。語音參數合計 36 維度，其中包含 12 階
MFCC、12 階△MFCC、12 階△△MFCC。 
4.7.1 IF 邊界切割實驗 
首先先針對 IF 邊界適用度決定，分別單獨使用 BIC、Hotelling’s T2 Test 與隱藏式馬可
夫模型為基礎之 Viterbi 搜尋演算法進行第一個斷點偵測，由下圖 4.9 得知 Hotelling’s T2 Test
對於 IF 邊界較為敏感，可得到較佳之切割效果；所以將 Hotelling’s T2 Test 定義為第一階段
IF 邊界切割使用。 
4.7.2 語音段落邊界切割實驗 
由於本研究所使用之 Viterbi 演算法其模型為以次音節進行訓練，因此 Viterbi 演算法在
此階段實驗並不適用，在此實驗中使用 SFS 演算法於 Hotelling’s T2 Test、BIC 與兩階段語
音切割技術進行比較，實驗結果如圖 4.10 所示。在誤差小於 30ms 情況中兩階段語音切割
技術效果較佳，但誤差時間超過 30ms 以後，其效果比 BIC 的正確率較差，主要原因乃是
 56
 
圖 4.11 理想狀況之切音結果(ㄒㄧㄠ) 
Manual 
Segmentation
Two-stages 
Segmentation
 
圖 4.12 IF 斷點錯切為 FF 斷點情況(ㄌㄧㄤˊ) 
 
4.7.3 決策樹於語音長度模組實驗 
本研究建立了以決策樹為基礎之長度模型，並整合於兩階段語音切割，減少 IF 錯切為
FF 斷點之發生而導致 30ms 後之效果不理想情況。 
(A) 決策樹之分類規則 
在語音切割的基本單元中，上下文無關的音素合計 30 個，依據每一個基本單元的變異
係數分佈，取得語音長度模組較差的基本單元，進行決策樹分群，而 Question Set 包含了基
本單元的語言學特性及上下文相關特性之 Decision Rule，部分 Decision Rule 列於下表 4.18，
分類終止之門檻值權重設定為 W=1，經由變異係數分析與決策樹的分群後，最終的語音長
度模組的分類得到 36 群組，如下表 4.19 所示。圖 4.13 為分群前與分群後的變異係數分佈
圖，由此可知，經由決策樹的分析後，可以各群組織變異度，取得較佳的語音長度模組。 
(B) 決策樹分類之長度模型整合於兩階段語音切割效果 
整合此分群後的語音長度模組於兩階段語音切割技術中，當權重值 Wd 設定為 0.7 時，
可得最佳之語音切割效果，實驗結果如下圖 4.14 所示，減少了第一階段中取得 VV 語音段
落的錯誤後，可以明顯提升語音切割的效果。 
 58
 
圖 4.14 決策樹分類之週期長度模型整合兩階段語音切割之效果比較 
4.7.4 動畫回饋介面與效果評估 
本研究參考語言學知識與 2D 舌位動畫圖建立 31 舌位模型基本單元，在唇形部份使用
成功大學多媒體人機通訊實驗室所建立之虛擬人臉動畫系統，其利用省略與歸納法將 408
音之唇形簡化為 107 類【40】。利用 B-spline 曲線演算法之運算，達到動畫播放之平滑效果。
圖 4.15(a)為聲母ㄖ發音時之舌位動畫序列範例，(b)為韻母ㄚ之範例。 
本系統主要操作介面如下圖 4.16 所示。首先系統介面左半部將提供使用者中文形式之
教才內容，其教材內容可事先視需求進行變更或編輯，其文字訊息範例如圖 4.17 所示，而
系統將依照文字訊息之內容即時合成 3D 動畫並給予播放回饋，並可自行選擇欲播放之文
字區段，讓使用者可針對某些較難以理解之發音反覆練習，提升成效。在介面呈現的部份，
使用者在閱讀文字訊息時可同時由虛擬人臉正面與側面觀察發音時之舌位與唇形，並與中
文所轉譯出之語音同步播放(圖 4.18)，達到輔助使用者更有效率之學習目的。 
 
 60
 
圖 4.18 動畫與語音回饋範例(風) 
本實驗擬評估系統動畫回饋的整體自然度與效能。經由 7 個使用者之個案實測，系統
介面以文字介面配合中文轉譯語音系統及中文轉譯為包含舌位與嘴形之合成動畫回饋，並
對於其理解力、自然度以滿意度分為五等級進行效果評估，其中五分為最佳，一分為最差。
測試之語料類型包含隨機挑選之單一字彙、2~4 個字彙所組成之詞彙與 5~10 個字彙所組成
之短句各 10 組，進行播放回饋。其所得結果如下表 4.20 所示。 
 
表 4.20 平均意見分數 
使用者編號與給分 回饋類型 一 二 三 四 五 六 七 平均分數(MOS) 
字 3 4 4 3 4 3 2 3.2 
詞彙 4 4 4 4 3 4 3 3.7 
短句 3 2 3 2 3 2 3 2.5 
 
由上實驗得知，單一字彙與詞彙得到較高之給分，而短句類型所得之分數最低。討論
其原因在於，在正常情況下每個字彙基本音素之週期長度相當短，導致合成之舌位動畫變
化相當快速，對一些使用者而言，因短句之資料量較長，使得動畫在連續播放時，無法完
全跟上播放速度，導致給分較為偏低，但平均得分還是維持在 2.5 分以上。 
 62
同播放速度之語料，並進行分析，讓使用者可依據自己的學習特性，設定回饋播放速度，
以達到最佳之學習效果。為讓本系統之構音回饋動畫達到更加的精準與自然，可對於模型
之參數再進行調整，並且可改善動畫合成之平滑程度，未來也可進一步考慮音素間 Context 
dependent 之情況，達到更佳的動畫輸出效果。 
 64
【15】徐偉棠，以語音辨識技術輔助英文母音發音之偵錯，碩士論文，國立清華大學資訊
工程學系，民 93 年。 
【16】湯士民，應用錯誤型態分析於英語發音輔助學習，碩士論文，國立成功大學學資訊
工程學系，民 93 年。 
【17】陳雅菁，類神經網路在兒童構音異常診斷上之應用，碩士論文，國立成功大學工程
科學研究所，民 84 年。 
【18】陳信全，可攜式語障者發音訓練器之研製，碩士論文，南台科技大學電機工程系，
民 92 年。 
【19】王理嘉、林燾，語言學教程，五南圖書出版社，民 84 年。 
【20】賴湘君，構音異常的診斷及矯治，語言治療教育專題研討專輯，台北市政府教育局，
pp.123-133，民 79 年。 
【21】J. Ostermann , A. Weissenfeld, “Talking Faces - Technologies and Applications, ” IEEE 
Trans. Pattern Recognition, Vol. 3, pp. 826-833, 2004. 
【22】Y. C. Lee, D. Terzopoulos, and K. Waters, “Realistic Modeling for Facial Animation, ” In 
Computer Graphics, Annual Conference Series, pp. 55-62, 1995. 
【23】Y. C. Lee, D. Terzopoulos, and K. Waters, “Constructing Physics-based Facial Models of 
Individuals, ” In Proceeding of Graphics Interface, pp. 1-8, 1993. 
【24】J. Bloomenthal, “Interactive Techniques for Implicit Modeling, ” Symposium on 
Interactive 3D Graphics, pp.109-116, 1990. 
【25】N. Magnenat-Thalmann, H. Minh, M. deAngelis, and D. Thalmann, “Design, 
Transformation and Animation of Human Face, ” The Visual Computer, Vol. 5, pp. 32-39, 
1988. 
【26】W. T. Reeves, “Simple and Complex Facial Animation: Case Studies. In State of the Art in 
Facial Animation, ” SIGGRAPH ’90 Course Note ACM, pp.88-106, 1990. 
【27】O. Balter, O. Engwall, A.M. Oster, and H. Kjellstrom, “Wizard-of-Oz Test of ARTUR – a 
Computer-Based Speech Training Sysytem with Articulation Correction, ” ACM 
SIGACCESS Conference on Assistive Technologies, pp. 36-43, 2005. 
【28】Y.S. Akgul, C. Kambhamettu and M. Stone, “Automatic Motion Analysis of the Tongue 
 66
【41】O. Siohan, C. Chesta, and C. H. Lee, "Joint Maximum a Posteriori Adaptation of 
Transformation and HMM Parameters," IEEE Trans. Speech and Audio Processing, vol. 9, 
no. 4, 2001. 
【42】A. Whitney, “A Direct Method of Nonparametric Measurement Selection, ” IEEE Trans. 
on Computers, Vol. 20, pp. 1100-1103, 1971. 
【43】G. Schwarz, “Estimating The Dimension of A Model, ” Ann. Math Statist, pp.461-464, 
1978. 
【44】H. Hotelling, “The Selection of Variates for Use in Prediction, with Some Comments on 
the General Problem of Nuisance Parameters, ” Annals of Mathematical Statistics, Vol. 11, 
pp. 271-283, 1940. 
【45】Z. J. Chuang and C. H. Wu, “Text-to-Visual Speech Synthesus for General Objects Using 
Parameter-Based Lip Models, ” Lecture Notes in Computer Science Series, Vol. 2532, pp. 
589-597, 2002. 
【46】莊則敬，針對聽語障人士之語音及手與處理技術之研究，博士論文，國立成功大學
資訊工程學系，民 95 年。 
【47】Phonetics: The Sounds of American English, http://www.uiowa.edu/~acadtech 
/phonetics/english/frameset.html 
【48】王小川，語音訊號處理，全華科技，民 94 年。 
【49】O. Engwall, O. Balter, “Pronunciation Feedback from Real and Virtual Language 
Teachers, ” Computer Assisted Language Learning, Vol. 20, Issue 3, pp. 235-262, 2007. 
 68
 
可供推廣之研發成果資料表 
■ 可申請專利  □ 可技術移轉                                      日期：97 年 8 月 15 日 
國科會補助計畫 
計畫名稱：多感覺回饋於構音障礙評估與輔助訓練之研究 
計畫主持人：陳有圳 
計畫編號：NSC 95-2221-E-218-002-MY2 
學門領域：身心障礙輔助科技 
技術/創作名稱 一種基於語音識別之構音障礙特性偵測系統 
發明人/創作人 陳有圳 
中文：語言障礙會嚴重影響兒童的溝通能力、學習成效、人格發展、社
會適應力和人際關係發展等，是兒童發展障礙中常見問題。然國內專業
語言治療師極度缺乏，急需應用輔助系統提升語言治療與復健效率；國
外發展之語言輔助系統，因語言特性差異極大無法導入為國人所用。本
研究應用構音障礙評估與矯治訓練輔助平台協助語言治療師進行矯治訓
練與學童居家學習；整合語言治療師之臨床經驗，針對構音錯誤類型設
計多模構音矯治訓練活動與構音錯誤之語音特性擷取；應用語音識別技
術於語言障礙患者之構音障礙之聲學特性擷取；應用多模之視聽覺回饋
資訊於構音矯治訓練。初步的實驗結果顯現，針對構音障礙學童應用多
模視聽覺回饋資訊進行互動式構音矯治與訓練，能有效協助語言治療師
提升語言治療效果，並讓學童進行居家學習。 
技術說明 英文：Articulation problems seriously reduce speech intelligent and 
speech communication and affect person’s interpersonal 
communication, personality, social adaptive capacity, and learning 
ability. In this paper, an articulation assessment and training system is 
proposed to assist language therapists and articulation disorders. The 
articulation errors in phonetic are analyzed and modeled by clinical 
linguist. Using clinical experience of language therapists, articulation 
training strategies for each type of articulation errors are designed. The 
articulation characteristics of user can be effectively detected. 
Speechreading information is responded to improve the performance of 
training program. The articulation training strategy is automatically 
selected to suggest articulation disorder in language training activities.
可利用之產業 
及 
可開發之產品 
數位語言學習、構音障礙特性偵測系統、構音訓練系統 
技術特點 
1. 整合臨床語言治療師經驗設計構音測試詞彙 
2. 應用語音識別技術於構音特性偵測 
3. 結合臨床語言治療之構音類型於自動化障礙識別 
 70
可供推廣之研發成果資料表 
■ 可申請專利  □ 可技術移轉                                      日期：97 年 8 月 15 日 
國科會補助計畫 
計畫名稱：多感覺回饋於構音障礙評估與輔助訓練之研究 
計畫主持人：陳有圳 
計畫編號：NSC 95-2221-E-218-002-MY2 
學門領域：身心障礙輔助科技 
技術/創作名稱 一種具舌位與唇形資訊之構音合成系統 
發明人/創作人 陳有圳 
中文：語言障礙會嚴重影響兒童的溝通能力、學習成效、人格發展、社
會適應力和人際關係發展等，是兒童發展障礙中常見問題。然國內專業
語言治療師極度缺乏，急需應用輔助系統提升語言治療與復健效率；國
外發展之語言輔助系統，因語言特性差異極大無法導入為國人所用。本
研究應用 3D 虛擬臉部動畫之回饋介面，給予聽障兒在語言與學習上之輔
助；首先利用語音長度模組於兩階段(賀德臨 t 與貝氏訊息準則)語音切割
技術以協助 3D 合成動畫各基本單元之語音長度資料標記；應用逐次前饋
式搜尋法演算法於語音斷點切割；在 3D 動畫之建構與合成方面，整合語
言學知識與 2D 發音口腔動畫分別對於舌位與唇形模型之建置與針對華
語發音動畫之虛擬合成輸出；利用 B-spline 曲線進行動畫之合成單元的
串接平滑化處理。初步結果顯示本系統針對構音障礙學童應用多模視聽
覺回饋資訊進行互動式構音矯治與訓練，能有效協助語言治療師提升語
言治療效果，並讓學童進行居家學習。 技術說明 
英文：Articulation problems seriously reduce speech intelligent and 
speech communication and affect person’s interpersonal 
communication, personality, social adaptive capacity, and learning 
ability. In this paper, an articulation assessment and training system is 
proposed to assist language therapists and articulation disorders. The 
articulation errors in phonetic are analyzed and modeled by clinical 
linguist. Using clinical experience of language therapists, articulation 
training strategies for each type of articulation errors are designed. The 
articulation characteristics of user can be effectively detected. 
Speechreading information is responded to improve the performance of 
training program. The articulation training strategy is automatically 
selected to suggest articulation disorder in language training activities.
可利用之產業 
及 
可開發之產品 
數位語言學習、構音障礙特性偵測系統、構音訓練系統 
技術特點 
1. 整合臨床語言治療師經驗設計構音合成基本單元 
2. 應用 3D 合成技術合成具舌位與唇型資訊之 3D 虛擬臉部動畫 
3. 結合臨床語言治療之構音類型於居家構音訓練 
 1
國家科學委員會補助出席國際學術會議報告 
97年6月9日 
計畫編號 NSC 95-2221-E-218-002-MY2 
計畫名稱 多感覺回饋於構音障礙評估與輔助訓練之研究 
出國人員姓名 
服務機關及職稱 
報告人: 陳有圳 
南台科技大學 電機工程系 
會議時間地點 97年6月1-6日中國 香港 
會議名稱 
二OO八年IEEE國際計算機智慧國際會議 
2008 IEEE World Congress on Computational Intelligence (WCCI 2008) 
發表論文題目 Automatic Speech Recognition and Dependency Network to Identification of Articulation Error Patterns 
 
 
會議摘要 
 
本人依計畫參加於香港舉行之2008 IEEE國際計算機智慧國際會議(2008 
IEEE World Congress on Computational Intelligence 簡稱 WCCI 2008)出國其目的
如下：  
     1) 發表一篇會議論文； 
     2) 蒐集國際語音相關應用技術發展趨勢。 
 
一、會議大要 
「二OO八年IEEE國際計算機智慧國際會議」係國際電子電機工程師學會
計算機智慧學術會(IEEE Computational Intelligence Society)主辦之全球年會，此
一會議為計算機智慧方面最大且最被肯定會議之一，其中包含聲學語音及信號
處理等相關應用。本年在香港的香港會議展覽中心舉行，參與人數超過2000人
以上。本會議分三個領域主題：IJCNN 2008, FUZZ-IEEE 2008, CEC 2008，其中
的次主題包含： 
IJCNN 2008: 
•  Computational neuroscience  
•  Connectionist theory and cognitive science  
•  Mathematical modeling of neural systems  
•  Neurodynamic analysis  
•  Associative memories  
•  Neurodynamic optimization and adaptive dynamic programming  
•  Embedded neural systems  
•  Probabilistic and information-theoretic methods  
•  Principal and independent component analysis  
•  Hybrid intelligent systems  
•  Supervised, unsupervised and reinforcement learning  
•  Brain Imaging and neural information processing  
•  Brain computer interface  
 3
•  Differential evolution  
•  Estimation of Distribution Algorithms  
•  Evolutionary computation for bioinformatics  
•  Evolutionary computation in dynamic and uncertain environments  
•  Evolutionary computer vision  
•  Evolutionary data mining  
•  Evolutionary design  
•  Evolutionary games  
•  Evolutionary intelligent agents  
•  Evolutionary learning systems  
•  Evolutionary robotics  
•  Evolutionary techniques in economics, finance and marketing  
•  Evolvable hardware and software  
•  Evolved art and music  
•  Evolving neural networks and fuzzy systems  
•  Hybrid optimisation algorithms  
•  Memetic and hybrid algorithms  
•  Multi-objective optimization  
•  Molecular and quantum computing  
•  Real-world applications  
•  Representation and operators  
•  Self-adaptation in evolutionary algorithms  
•  Swarm Intelligence (e.g., ant colony, particle swarm, differential 
evolution, cultural algorithms)  
•  Theory of evolutionary computation  
•  Emerging areas 
並以19個會場同時進行，共228個口頭報告場次，每篇論文發表20分鐘。共有
1622篇論文發表。 
本次大會主席由香港中文大學的Jun Wang教授擔任，議程主席由美國
University of Illinois-Chicago的Derong Liu教授、香港City University of Hong 
Kong的Gary G. Feng及澳洲University of Adelaide and SolveIT Software的
Zbigniew Michalewicz教授共同擔任。大會於6月2-6日每天早上一場主題演講
(Plenary Session)，分別由Jim Bezdek博士主講「Visual Cluster Validity」、David 
B. Fogel博士主講「The Burden of Proof – Part II」、Teuvo Kohonen教授主講
「Data Management by Self-Organizing Maps」、Takeshi Yamakawa 教授主講
「Bio-Inspired Self-Organizing Relationship Network as Knowledge Acquisition 
Tool and Fuzzy Inference Engine」及Christopher M. Bishop教授主講「A New 
Framework for Machine Learning」. 
 
二、參與會議 
本人6月1日下午出國搭機飛直達香港，並於6月1日抵達香港之旅館，並熟
悉香港及大會場地之狀況，準備研討會之論文發表工作。大會於6月1日舉行
Tutorial課程，內容相當充實。本人此次參加國際電子電機工程師學會計算機智
慧學術會(IEEE Computational Intelligence Society)學會相關會議及論文發表，分
述如下： 
 
 
 
Automatic Speech Recognition and Dependency Network to 
Identification of Articulation Error Patterns 
Yeou-Jiunn Chen, Jiunn-Liang Wu, and Hui-Mei Yang 
t  
Abstract—Articulation errors will seriously reduce speech 
intelligibility and the ease of spoken communication. Typically, 
a language therapist uses his or her clinical experience to 
identify articulation error patterns, a time-consuming and 
expensive process.  This paper presents a novel automatic 
approach to identifying articulation error patterns and 
providing error information of pronunciation to assist the 
linguistic therapist. A photo naming task is used to capture 
examples of an individual's articulation patterns. The collected 
speech is automatically segmented and labeled by a speech 
recognizer. The recognizer's pronunciation confusion network 
is adapted to improve the accuracy of the speech recognizer. 
The modified dependency network and a multiattribute decision 
model are applied to identify articulation error patterns. 
Experimental results reveal the usefulness of the proposed 
method and system. 
I. INTRODUCTION 
RTICULATION errors, which generate different 
degrees of abnormality in articulation, seriously reduce 
speech intelligibility and the ease of spoken 
communication. Typically, a speech-language pathologist 
uses his or her clinical experience to identify articulation 
error patterns, a time-consuming and expensive process. 
Therefore, an automatic process for identification of 
articulation error patterns is very helpful to assist 
speech-language pathologist in clinical speech evaluation. 
Most articulation errors fall into those three categories: 
omissions, substitutions, or distortions. For speech-language 
pathologist, the articulation errors are examined in terms of 
the place and manner of articulation and can be classified into 
six articulation error patterns: fronting, backing, 
de-aspiration, stopping, affrication, and omission [1]. In a 
typical fronting error, for example, a child may say /t/ instead 
of /k/ in the Chinese word /kan4/ so it would be heard as 
/tan4/. For backing error, the /q/ will be pronounced as the /k/ 
and /qi4/ would be heard as /ki4/. 
Recently, researches usually aimed at computer-assisted 
treatment by using visual feedback [2, 3]. Visual feedback 
was very useful to improve articulation ability. However, 
those approaches focused on visual feedback based responses 
in training and treating process. Those researches could not 
propose an automatic process for identification of articulation 
error patterns. Moreover, error information of pronunciation 
could not be provided to assist speech-language pathologist. 
 
Manuscript received December 31, 2007. This work was supported in part 
by the National Science Council, Republic of China under Grant 
NSC95-2221-E-218-002-MY2. 
Yeou-Jiunn Chen is with the Department of Electrical Engineering, 
Southern Taiwan University, Tainan County, Taiwan, R.O.C. (phone: 
886-6-2533131 ext. 3325, fax: 886-6-3010073, e-mail: 
chenyj@mail.stut.edu.tw). 
Jiunn-Liang Wu and Hui-Mei Yang are with Department of 
Otolaryngology, National Cheng Kung University Hospital, Tainan, Taiwan, 
R.O.C. (e-mail: jiunn@mail.ncku.edu.tw, tannas00@yahoo.com.tw). 
Other researchers proposed various approaches to identify 
articulation errors using statistical models [4] or tongue 
detection models [5-7]. For statistical models, Georgoulas et 
al. applied support vector machine to classify only three 
consonant phonemes. Only those phonemes were insufficient 
to identify the articulation error patterns and the error 
information of pronunciation was insufficient. For tongue 
detection models, using ultrasound to examine speech 
production was gaining popularity because of its portability 
and noninvasiveness. The place of tongue could be detected 
from the ultrasound image. However, the resolution of 
detected results was insufficient to distinguish the five 
articulation error patterns. Moreover, the manner of 
articulation was also cannot be detected by this approaches. 
Automatic speech recognition had been applied to many 
applications [8, 9] and the articulation attributes can be 
effectively estimated by speech technology [10, 11]. 
Therefore, it would be very useful to identify the error 
information of pronunciation. Besides, dependency network 
(DN) technique was also applied to collective classification 
and suitable to knowledge discovery [12, 13]. From the 
clinical practice and experience, the information for 
identification of articulation error patterns is dependency and 
dependency network technique is appropriate to identify 
articulation error patterns. 
In this paper, a novel automatic approach integrating 
automatic speech recognition and dependency network is 
proposed to identify articulation error patterns. A photo 
naming task (PNT) is used to capture examples of an 
individual’s articulation patterns. Then, the collected speech 
signals are automatically segmented and labeled by automatic 
speech recognition. Besides, the recognizer’s pronunciation 
confusion network (PCN) is adapted to improve the accuracy 
of the speech recognizer. DN is applied to estimate the 
likelihood of articulation error pattern by integrating the 
information of testing phoneme, labeled results, and speech. 
Finally, multiattribute decision model is used to decide the 
results of articulation error patterns. 
II. METHODS 
As shown in Fig. 1, the articulation disorder is actuated to 
articulate the names in photo naming task and the 
corresponding speech signal is captured. Automatic speech 
recognition is applied to segment and label user’s speech 
signal. Then, the labeling result and corresponding likelihood 
A 
 
 
 
It is clearly that the probability ( )ˆlm mP s s  and ( )ˆam mP o s  can 
be computed in the process of segmentation and labeling by 
automatic speech recognition. ( )ˆim m mP E s s  is estimated by 
MLE as 
( ) ( )( )ˆ ˆmi m m m m
C E
P E s s
C s s
= . (6) 
III. RESULTS AND DISCUSSION 
Samples were collected from 553 children (346 males and 
207 females) with multiple articulation error patterns. 421 
and 132 samples were used for training and testing, 
respectively. The articulation error patterns of those samples 
were manually labeled by speech-language pathologists. In 
the training database, there are 45, 179, 88, 297, 106, and 42 
samples for fronting, backing, de-aspiration, stopping, 
affrication, and omission, respectively. Moreover, in the 
testing database, there were 15, 57, 28, 95, 33, and 13 samples 
for fronting, backing, de-aspiration, stopping, affrication, and 
omission, respectively. 
In Eq. (6), the priori probability of each pronunciation 
error is different to determine the articulation error patterns 
and should be estimated in the training database. The 
probability of distributions of pronunciation errors for 
articulation error patterns is shown in Table I. It is clear that 
the correlation between pronunciation error and articulation 
error pattern is different. Some pronunciation errors can give 
confident to identify a articulation error pattern. However, for 
clinical practice, to identify a articulation error pattern should 
be verify by different phoneme’s pronunciation characteristic. 
Thus, the multiattribute decision model is proper to this 
mechanism. 
To decide the identification results, a threshold of 
articulation error pattern should be determined. The receiver 
operating characteristic (ROC) curves for each identification 
results of articulation error patterns in training database were 
shown in Fig. 7. The equal error rates of Fronting, Backing, 
De-aspiration, Stopping, Affrication, and Omission were 
7.32%, 11.78%, 9.87%, 8.76%, 7.07%, and 4.89%. Moreover, 
the thresholds with equal error rate were 0.16, 0.086, 0.092, 
0.2, 0.2, and 0.1, respectively. 
In order to evaluate the performance, the accuracy, 
specificity, sensitivity, and Kappa is used in this paper. The 
accuracy is number and proportion of all the observations in 
the table which have been classified correctly as 
( )iL E
1O
1ˆ
aS
1ˆ
lS 1S
1
iE
mO
ˆ a
mS
ˆ l
mS mS
i
mE
MO
ˆ a
MS
ˆ l
MS MS
i
ME
 
Fig. 3  The multiattribute decision model for identifying articulation error 
patterns. 
 
i
mE
mS
ˆ
mSmO  
Fig. 4  DN for identification of articulation error pattern with articulation 
disorder. 
 
i
mE
mS
ˆ
mSmO  
Fig. 5  DN for identification of articulation error pattern 
 
i
mE
mS
ˆa
mS ˆ lmSmO
ˆ
mS
 
Fig. 6  DN for identification of articulation error pattern according to 
acoustic and language information 
( )
( )
TP TN
Accuracy
TP TN FP FN
+= + + + . (7) 
The specificity of a test can be described as the proportion of 
true negatives it detects of all the negatives as 
( )
TNSpecificity
TN FP
= + . (8) 
The sensitivity of a test can be described as the proportion of 
true positives it detects of all the positives as 
( )
TPSensitivity
TP FN
= + . (9) 
The Kappa is a measure of agreement between predicted and 
observed as 
( ) ( )/ 1o e eKappa P P P= − −  (10) 
where 
Po = TP+TN (11) 
and 
Pe = (TP+FN)(TP+FP) + (FN+TN)(FP+TN). (12) 
It takes on the value zero if there is no more agreement 
between test and outcome then can be expected on the basis 
of chance. Kappa takes on the value 1 if there is perfect 
agreement; i.e. the test always correctly predicts the outcome. 
For the testing database, the accuracy, specificity, 
 
 
 
[7] F. Robineau, F. Boy, J.P. Orliaguet, J. Demongeot, and Y. Payan, 
“Guiding the Surgical Gesture Using an Electro-Tactile Stimulus Array 
on the Tongue: A Feasibility Study,” IEEE Trans. Biomedical 
Engineering, vol. 54, issue 4, pp. 711-717, 2007. 
[8] T. Orzechowski, A. Izworski, R. Tadeusiewicz, K. Chmurzynska, P. 
Radkowski, and I. Gatkowska, “Processing of pathological changes in 
speech caused by dysarthria,” in Proc. of 2005 International 
Symposium on Intelligent Signal Processing and Communication 
Systems, pp. 49-52, Dec. 2005. 
[9] Gustavo P., Joao F. M., Jose H. S., and Alexandre L. M. L., “Voice 
Command Recognition with Dynamic Time Warping (DTW) using 
Graphics Processing Units (GPU) with Compute Unified Device 
Architecture (CUDA),” 19th International Symposium on Computer 
Architecture and High Performance Computing, pp. 19-25, Oct. 2007. 
[10] S.M. Siniscalchi, P. Schwarz, and C.H. Lee, “High-Accuracy Phone 
Recognition By Combining High-Performance Lattice Generation and 
Knowledge Based Rescoring,” in Proc. of IEEE International 
Conference on Acoustics, Speech and Signal Processing, vol. 4, pp. 
869-872, April 2007. 
[11] M. Rajamanohar  and E. Fosler-Lussier, “An evaluation of hierarchical 
articulatory feature detectors,” in Proc. of 2005 IEEE Workshop on 
Automatic Speech Recognition and Understanding, pp. 59-64, Nov. 
2005. 
[12] Y. Tian, Q. Yang, T. Huang, C.X. Ling, and W. Gao, “Learning 
Contextual Dependency Network Models for Link-Based 
Classification,” IEEE Trans. Knowledge and Data Engineering, vol. 18, 
issue 11, pp. 1482-1496, Nov. 2006. 
[13] C. Preisach and L. Schmidt-Thieme, “Relational Ensemble 
Classification,” in Proc. Sixth International Conference on Data 
Mining, pp. 499-509, Dec. 2006. 
[14] X. Huang, A. Acero, and H. Wuen, Spoken Language Processing: A 
Guide to Theory, Algorithm and System Development. Prentice-Hall, 
2001. 
 
 2
•  Neuroinformatics and bioinformatics  
•  Support vector machines and kernel methods  
•  Autonomous mental development  
•  Data mining  
•  Pattern recognition  
•  Time series analysis  
•  Image and signal processing  
•  Robotic and control applications  
•  Telecommunications  
•  Transportation systems  
•  Intrusion detection and fault diagnosis  
•  Hardware implementation  
•  Real-world applications  
•  Emerging areas 
FUZZ-IEEE 2008 
•  Fuzzy logic and fuzzy set theory  
•  Fuzzy-neuro-evolutionary -rough hybrids  
•  Fuzzy and rough data analysis  
•  Fuzzy optimization and design  
•  Fuzzy decision making  
•  Fuzzy systems modeling and identification  
•  Fuzzy mathematics  
•  Fuzzy systems architectures and hardware  
•  Fuzzy pattern recognition  
•  Fuzzy image processing  
•  Fuzzy control and systems  
•  Fuzzy data mining and forecasting  
•  Fuzzy information processing  
•  Fuzzy human interface  
•  Fuzzy internet and multimedia  
•  Fuzzy computing with words  
•  Granular computing  
•  Real-world applications  
•  Emerging areas 
CEC 2008 
•  Ant colony optimization  
•  Artificial immune systems  
•  Artificial ecology  
•  Artificial life  
•  Autonomous mental and behaviour  development  
•  Bioinformatics and bioengineering  
•  Classifier systems  
•  Coevolution and collective behaviour  
•  Cognitive systems and applications  
•  Combinatorial and numerical optimization  
•  Constraint and uncertainty handling  
•  Cultural algorithms  
