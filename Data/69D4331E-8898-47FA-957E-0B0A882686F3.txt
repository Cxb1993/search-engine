Physically-based Analysis and Rendering of Bidirectional Texture Functions 
Data 
 
 
Sin-Jhen Chiu, Ying-Chieh Chen, Hsiang-Ting Chen, Chun-Fa Chang 
National Tsing Hua University 
{nobirdm, louis, timchen, chang}@ibr.cs.nthu.edu.tw 
 
ABSTRACT 
In order to draw a photorealistic surface, 
Bidirectional Texture Function (BTF), a 6D texture 
function which extends Bidirectional Reflectance 
Distribution Function (BRDF) to include the 
self-shadowing, self-occlusion and inter-reflection 
effects, has been used frequently in recent years. Its 
main drawback is its massive data size. To solve this, 
the Spatial Bidirectional Reflectance Function (SBRDF) 
techniques compress BTFs into reflectance model 
parameters. However, SBRDF cannot produce the 
self-shadowing and self-occlusion effects in real-world 
surface geometry. This work is aimed to this drawback. 
We investigate how self-shadowing and self-occlusion 
affect the surface appearance by additional 
physically-based analysis and rely on two physical 
phenomena to separate self-shadowing and 
self-occlusion into two independent effects. First, 
self-shadowing is view independent. Second, 
self-occlusion is independent of lighting direction 
changes. After these analyses, we add self-shadowing 
and self-occlusion to SBRDF to achieve rendering 
quality that is much closer to the original uncompressed 
BTF data. 
 
1: INTRODUCTIONS 
 
In recent years, we have seen continuing 
improvement of visual realism in computer graphics 
based feature movies and video games. As the visual 
realism in computer graphics based feature movies and 
video games keeps improving, the importance of 
advanced surface materials and appearance models also 
increases. Another driving force of advanced surface and 
appearance models is the increasing power of modern 
graphics hardware which supports programmable 
shading stages. Simply scanning the shape of a 3D object 
and then applying uniform colors, simple Phong shading 
model, or static textures to the surfaces no longer 
suffices. 
In addition, good surface appearance models can 
even hide the deficiency in the geometric shape, 
especially if advanced texturing techniques such as bump 
mapping [13], displacement mapping, or view-dependent 
texture mapping [5] is used. Imagine that a sphere model 
with a golf ball texture on it can be easily recognized as a 
golf ball even though we do not explicitly model the 
geometric shape of each dimple. In fact, view-dependent 
texturing effects can replace fine geometry should be no 
surprise if we treat those textures as surface light fields 
[14] [9][3] or lumigraph[2][6]. From a different point of 
view, we could also consider advanced surface 
appearance models such as the surface light fields and 
bidirectional texture functions as a way to avoid the 
daunting task of modeling the geometric shapes of 
micro-level or meso-level structures of complex 
materials. 
There are many surface models that are extended 
from the well-known Bidirectional Reflectance 
Distribution Function (BRDF. If the incident light 
position is different from its extent position, then we 
have the Bidirectional Surface Scattering Distribution 
Function (BSSRDF). This extension allows BSSRDF to 
model sub-surface scattering effects of translucent 
materials, such as human skin or a lighting candle[7]. 
Furthermore, if different surface points contain different 
BRDFs, then the Spatial Bidirectional Reflectance 
Distribution Function (SBRDF) [11] and the 
Bidirectional Texture Function (BTF) [4] may be 
applied. The Bidirectional Texture Function (BTF) is a 
6D function which describes the spatially-variant 
reflectance and the mesostructure effects, such as 
self-shadowing, self-occlusion, and self-reflectance. The 
6D function contains two parameters for surface 
position, two for viewing direction and the other two for 
illumination direction. Therefore, during rendering, we 
can determine the color from BTF by the given surface 
position, light direction and viewing direction. Because 
BTF captures the self-shadowing, self-occlusion, and 
self-reflectance effects that are exhibited by the 
mesostructures of complex materials, it can reproduce 
the surface appearance of real-world objects with 
extremely high fidelity. A drawback of BTF, however, 
is its requirement of huge storage space since it is a 6D 
function. Many works have been focusing on the 
compression of the BTF database. A good survey can be 
found in [12]. Most of those works are based on the 
fitting of analytical BRDF models [11][10] or the matrix 
factorization methods. 
In this paper, we demonstrate a physically-based 
analysis of BTF model and its rendering in real-time. In 
the first part of our work, we present a method to detect 
the data points under self-shadowing by treating them as 
outliers during the SBRDF [11] fitting process. And we 
later store those outliers as a 6D shadow function. 
   
   
(a)           (b)             (c) 
Figure 2 The original pixel-wise BRDF vs. SBRDF 
reconstructed BRDF at two different pixel locations. (a) 
is the original, (b) is the SBRDF, (c) is the error 
between original and SBRDF where darker levels 
represent large errors
 
   
   
 
Figure 3 Original pixel-wise BRDF vs. refined SBRDF 
reconstructed BRDF. (a) is original, (b) is refined 
SBRDF, and (c) is the error between them where darker 
levels represent large errors
 
   
 
   
(a)              (b)             (c) 
Figure 4 Original BTF image vs. SBRDF reconstructed 
surface. (a) is original [Bonn], (b) is SBRDF, (c) is the 
result of refined SBRDF multiplied by shadow map, 
 
topV
ettV arg
Figure 5 Shadow regions are view independent, 
meaning they do not vary with the viewing position 
when the lighting is fixed. 
 
Shifted Hidden Surface  
 
topV topV  
ettV arg  
ettV arg
 
Figure 6 Occlusion results in error of applying top view 
shadow to other views. (a) is P occluded by Q, and (b) is 
Q is a invisible point at top view 
(a) (b) 
 
2.2: SHADOW ANALYSIS 
 
In the section 2.1, we present the method to generate the 
approximate shadow map, but there is a thorny problem 
that is those shadow maps are too irregular to find an 
effective representation. The reason why shadow map is 
irregular is that surface geometry effects such as 
self-shadowing and self-occlusion affect it complexly, 
so the whole shadow maps can only be represented as a 
6D function. In our observation there is a lot of 
redundant data in the 6D representation, and it is 
possible to reduce the 6D function down to 4D if there 
is no occlusion effect. Take Figure 5 for an example. 
When the lighting direction is fixed, P is always in the 
shadow side and Q is always in the bright side no matter 
how the viewing direction could be changed if there is 
no self-occlusion. In other words, without self-occlusion, 
the shadow regions will not change with the viewing 
direction. So it only needs to save the surface shadow 
regions once for each lighting direction regardless of the 
viewing direction. The representative view we choose is 
the Top View because it is the symmetrical center of all 
sampled viewing directions, and the Top View Shadow 
(TVS) is called the shadow regions. After this step, the 
dimensionality of shadow function is reduced from 6D 
function ShadowMap(x,L,V) to 4D function 4D 
TopViewShadowMap(x,L) and BTF reconstruction 
function will be modified to 
 
BTF(x,L,V)≒STVS(x,L,V) 
        ＝SBTDF(x,L,V) × TopviewShadowMap(x,L), 
    STVS is the acronym of SBRDF multiplied by TVS. 
 
(a) (b) (c) 
 
Figure 10 Illustration of searching the most similar 
reconstructed point (green points) to replace hidden 
surface point (red point), orange window is the 
searching region. 
 
 
According to this assumption, all the images with 
the same view direction can be considered as a group 
and replace the two images in original optical flow 
algorithm with the corresponding two image sets. Then 
the new result of the two image sets is several optical 
flow results. These several optical flow results can vote 
a best occlusion shift map from all occlusion shift maps 
and determine whether the pixels are hidden surface 
points or not by all hidden surface maps. The condition 
to determine that a pixel x is a hidden surface point is 
that HiddenRatio(x) >Thresholdhidden. HiddenRatio(x) is 
defined as follow. 
 
HiddenRatio(x)＝avg(HiddenMap(x)) 
 
If x is not a hidden surface point, then the motion vector 
of x should be calculated. The method of how to vote its 
motion vector is as follow. VotedOcclusionShift(x) is a 
function to query the motion vector of point x. It is 
decided by majority from motion vectors of all sample 
lighting pairs between two views. And this is why we 
call it vote-based optical flow. 
Figure 9 shows vote-based optical flow results. The 
hidden surface map is detected at the fillisters of the 
brick as Figure 9(d) as we expect. But the detected 
result as Figure 9(e) is not well enough. There are some 
parts like the four upstairs squares on the surface in 
Figure 9(a)(b). They should not be the hidden surface 
but we consider them as hidden one after the threshold 
determined. So we want to correct those parts by a 
refinement process which will be introduced in the next 
subsection. 
 
2.3.2 Refinement of Hidden Surface Map. The idea of 
the refinement process is very simple. The method is to 
minimize the color different between the reconstructed 
point color and the hidden point color. The 
reconstructed color is generated by the function STVS 
which is defined in section 2.2. The distance function is 
as follow: 
 
directions   lighting  all    
,),,(x)   ( ),,(  
),(tan
∈
+−=∑
i
ii
L
VLormotionVectxSTVSVLxBTF
Vxcedis
 
 
If this error is below the threshold, then this hidden 
surface point can be elevated to a non-hidden one and 
its motion vector will direct to the reconstructed surface. 
  
       (a)             (b)             (c) 
  
       (d)            (e) 
Figure 11 After hidden surface map refinement. And 
hidden surface maps at different depressive angle. (a) is 
before refinement process, (b) is after refinement 
process (c) is 30°, (d) is 45°,(e) is 60°,and horizontal 
angle is 0° 
 
Before Refinement After Refinement 
 Non-hidden Surface 
Points 
Hidden 
Surface 
Points 
Non-hidden 
Surface 
Points 
Hidden 
Surface 
Points 
IM
PA
L
L
A
 
209542 122234 274269 57507
Table 1 The number of hidden surface points and 
non-hidden surface points before and after the hidden 
surface map refinement. Test cases’ resolutions are 64 x 
64 and number of simple view directions is 81, so total 
surface points are 64 x 64 x 81= 331776 
 
 
The reconstructed point will be searched in a 
window relative to the hidden surface point as shown in 
Figure 10. The hidden surface point will be replaced by 
the green point which has the minimum distance, 
distancemin. In our experiment, the window size is 256 x 
256.  
As of the threshold for elevating a hidden surface 
point to non-hidden one, instead of determining by user 
himself, we present a reasonable and automatic 
determining system. As the result of vote-based optical 
flow, we distinguish hidden surface points and 
non-hidden ones roughly. We assume the non-hidden 
parts that the vote-based optical flow found are correct. 
So, we can find the distances of all non-hidden surface 
points at target view by the distance function which is 
defined in subsection 2.3.2, and the threshold will be the 
average distance of them as follow: 
 
threshold = avg(distance(x, Vtarget)), 
          x is all non-hidden surface points 
 
If a hidden surface point can match some reconstructed 
surface point where the distancemin is lower than the 
threshold, then it means that this hidden surface point 
can match a surface point which has more similar 
behaviors than non-hidden surface points. 
increasing depressive angle like Figure 12(a). For 
shadow maps is used one 8-bit channel to store it.  
Second, for each sample view direction, there is a 
corresponding hidden surface map and an occlusion 
shifted map. Hidden surface map is a Boolean texture 
which represents whether the surface point at this view 
direction is hidden or not. It can be store in one 8-bit 
channel. As for occlusion shifted map, It is stored in two 
8-bit channels. If the surface point is non-hidden, one of 
these two channels stores the x shift and the other stores 
y shift. If the surface point is hidden, we use these two 
channels to encode the cluster center index in our 
implementation. So in our experiment, the maximum 
number of cluster center index is 256x256 = 65536. 
Fortunately, the number of sample lighting direction and 
sample viewing direction is just the same, so those maps  
can be put into texture just like top view shadow map as 
shown in Figure 12(b). 
Because the texture size which GPU supports is 2 to 
the power, there may have some unused texture spaces.  
Figure 12(b) show the unused area as black. In order to 
achieve maximum utility of this texture, we use these 
free spaces to store the cluster centers. In our 
experiment, the resolution of the sample surface is 64 x 
64 and number of sampling directions is 81. The texture 
which is used to store these data is 1024 x 512. So 80 
maps take up 1024 x 320 texture spaces. The remainder 
texture spaces are 1024 x 192. The cluster center is a 1 x 
81 vector, so the number of cluster centers which can be 
filled in is (1024 x floor(192/81)) = 2048. But there is a 
reminder one map, so the maximum number of center 
which can be filled in is 2048-64 = 1984. 
 
4: RESULTS AND COMPARISON 
 
Our experiments are performed on a PC containing a 
2.8GHz Pentium4 CPU and an nVidia GeForce 6600 
graphics card. The output image size is 300 x 300. We 
achieve real time rendering of the BTF data at above 24 
frames per second (FPS) and the compression ratios are 
shown in Table 2. 
Figure 13 shows the rendering results. Compare 
with the original BTF (Figure 13(a)), our rendering 
results (Figure 13(d)) present more surface detail than 
both SBRDF and STVS. Those textures can also be 
applied to any 3D model which only has rough 
geometry to represent more complex mesostructure, 
relighting, and render them in real-time. 
 
 
 BTF SBRDF STVS 
Our 
method
Storage Size 
(MB) 
80.621 0.596 1.008 2.58 
Compression 
Ratio 
0% 99.26% 
98.75
% 
96.80%
Table 2 Compression ratio of all methods in this paper. 
 
Figure 13 Rendering results and comparisons. (a) is 
rendered by original BTF. (b) is by SBRDF. (c) is STVS, 
(d) is my final method. 
(a) 
(b) 
(c) 
(d) 
 
 
5: CONCLUSION AND FUTURE WORK 
 
In this paper we represent a physically-based analysis 
of BTF data. This analysis is based on traditional SBRDF 
method first represented by McAllister [11]. we find that 
traditional SBRDF cannot represent the self-shadowing 
and self-occlusion effects very well because they assume 
each sample points on the surface has individual BRDF 
and fit them into a reflectance model. As we know that all 
reflectance models are smooth curve functions. Both 
self-shadowing and self- occlusion will make the shape 
of BRDF sharply, so they cannot be reconstructed very 
well by SBRDF. So we analysis when they are occurred 
by some phenomena.. 
As for further work, first is to find the outliers more 
correct. Currently, we only use the color different to find 
them. In the future, they may be decided by some 
computer vision methods. And the occlusion effects can 
be detected by using another method to find a more 
efficient and reliable shifted maps and hidden surface 
maps to make the rendering result more correct. 
 
REFERENCES 
 
[1] BTF Database of University of Bonn.  
http://btf.cs.uni-bonn.de 
[2] Buehler C., Bosse M., McMillan L., Gortler S, and 
Cohen M.  Unstructured Lumigraph Rendering.  In 
Proceedings of ACM SIGGRAPH 2001. pages 425-432 
[3] Chen W. C., Bouguet J. Y., Chu M. H., and 
Grzeszczuk R.  Light Field Mapping: Efficient 
Representation and Hardware Rendering of Surface 
Light Fields. In Proceedings of ACM SIGGRAPH 2002, 
pages 447–456. 
