2polynomial can be computed as (3); α is a primitive element
in GF (2m) and can be determined in advance. b is an arbitrary
integer, and gi is in GF (2m). It follows from (2) and (3) that
αb, αb+1, . . ., αb+2t−1 are the roots of g(x) and c(x).
u(x) = u0 + u1x+ · · ·+ uk−1xk−1 (1)
c(x) = u(x)g(x) (2)
g(x) = (x− αb)(x− αb+1) · · · (x− αb+2t−1)
= g0 + g1x+ g2x
2 + · · ·+ g2tx2t (3)
An alternative encoding mechanism for RS codes is the
matrix-vector product shown in (4) [9]. G in (5) is the Van-
dermonde matrix, and any k columns are linearly independent.
The constructions in (2) and (4) result in the same Reed-
Solomon code, when b equals 1.
c = uG (4)
G =

1 1 1 · · · 1
α α2 α3 · · · αn
α2 (α2)2 (α3)2 · · · (αn)2
...
αk−1 (α2)k−1 (α3)k−1 · · · (αn)k−1
 (5)
C. Decoding
Denote the received polynomial as r(x); if there are nei-
ther erasures nor errors in the received codeword, then r(x)
matches the original codeword polynomial, c(x). When errors
and erasures exist in r(x), the decoding is a multi-step
procedure: Auxiliary polynomials, also known as syndromes,
are first computed from the original codeword c(x). These
syndromes are then used to compute an error-locator polyno-
mial that is used to identify erroneous symbols in the received
codeword. After all errors have been identified and corrected
in r(x), any k symbols are selected and the original set of
information symbols u(x) can be derived by inverting the k
columns of the Vandermonde matrix that correspond to the k
selected symbols.
We discuss each step in detail, beginning with decoding
when there are only Byzantine nodes. Later, we generalize
to codewords having erasures. The received polynomial can
be written as a sum of the original codeword and the errors
introduced, as shown in (6). e(x) is the polynomial indicating
the locations and values of each error in r(x). Syndrome Si is
given by the expression in (7), for each i in {b, b+1, . . . , b+
2t − 1}. Since g(x) and c(x) have αb, αb+1, . . . , αb+2t−1 as
roots, then if the i-th symbol in the received codeword r(x)
is not erroneous, Si evaluates to zero.
r(x) = c(x) + e(x) where e(x) =
n−1∑
j=0
ejx
j (6)
Si = r(α
i) = e(αi) =
n−1∑
j=0
ejα
ij (7)
Fig. 1. Reed-Solomon decoding for an arbitrary codeword
Assume that v ≤ t errors occur in unknown locations
j1, j2, . . . , jv of the received polynomial. Then e(x) in (6) can
be expressed as (8), and ejℓ is the value of the ℓ-th error, for
each ℓ in {1, 2, . . . , v}. A requirement for successful decoding
is to determine jℓ and ejℓ for all ℓ. Instead of solving the
2t syndrome equations in (7), an error-locator polynomial
is introduced in (9). For all ℓ in {1, 2, . . . , v}, Λℓ are the
coefficient of Λ(x), when the product is simplified, where
Λ0 is unit. By Newton’s identity, we then have (10), where
i ∈ {b+ v, b+ v + 1, . . . , b+ 2t− 1} [10].
e(x) =
v∑
ℓ=1
ejℓx
jℓ (8)
Λ(x) =
v∏
ℓ=1
(1− xαjℓ) =
v∑
ℓ=0
Λℓx
ℓ (9)
Si = −
v∑
j=1
ΛjSi−j (10)
The coefficients of the error-locator polynomial can be
determined by applying the classical Berlekamp-Massey algo-
rithm, Welch-Berlekamp or Euclid’s algorithm to solve (10).
Once all coefficients of the error-locator polynomial are found,
the values αjℓ can be determined by successive substitution
through a procedure known as the Chien search. We then solve
for each ejℓ via the Forney formula [10]. Figure 1 summarizes
the RS decoding process.
When both errors and erasures exist, an error-erasure de-
coding algorithm must be implemented in order to decode
the received vector efficiently [10]–[12]. Let r(x) be the
received polynomial and r(x) = c(x) + e(x) + γ(x) =
c(x) + λ(x), where e(x) =
∑n−1
j=0 ejx
j is the error poly-
nomial, γ(x) =
∑n−1
j=0 γjx
j the erasure polynomial, and
λ(x) =
∑n−1
j=0 λjx
j = e(x)+ γ(x) the errata polynomial. For
error-erasure decoding, two extra decoding steps are needed
compared with the error-only decoding: calculation of erasure-
locator polynomial and computation of the modified syn-
dromes. The modified syndromes are then used to replace the
normal syndromes in the decoding procedure. If the location
of the erasures are given, the erasure-locator polynomial can
be calculated directly as follows. Let i1, i2, . . . , is be the s
4where
Gˆ =

1 1 · · · 1
αj0 αj1 · · · αjkˆ−1
(αj0)2 (αj1)2 · · · (αjkˆ−1)2
...
(αj0)kˆ−1 (αj1)kˆ−1 · · · (αjkˆ−1)kˆ−1
 .
Gˆ can be constructed by the primitive element and the index
associated with rijℓ , 0 ≤ ℓ < kˆ.
From Section II, we know that RS codes can recover from
any v errors, so long as v ≤ ⌊n−kˆ−s2 ⌋. Therefore, if the
number of compromised nodes is small, more erasures can be
tolerated and less nodes can be accessed by assuming these
nodes are unavailable.
Our proposed data reconstruction algorithm, PDR, is illus-
trated in Figure 2. PDR proceeds in stages, where l errors are
assumed at stage l and each stage corresponds to an attempt to
reconstruct the original codeword (Line 12-14). At the core of
PDR is an efficient RS decoding algorithm, IRD, illustrated
in Figure 3. IRD allows for data reconstruction even when
some nodes are Byzantine or crash-stop, and it can utilize
intermediate computation results from previous stages and
decode incrementally to avoid unnecessary computations.
If a call to IRD does not successfully decode on Line 15,
or the decoded codeword fails the CRC check on Line 7, then
there must exist more Byzantine nodes than IRD can handle
at this stage. In order to correct one more error, PDR retrieves
two additional symbols given that the number of erasures
allowed is reduced by two. Therefore, the total number of
symbols retrieved at stage l is kˆ+2l. Using a (1023, 401)-MDS
code and a 1% error Byzantine node rate, 409.2 storage nodes
are accessed on average (c.f.: Section VII). Hence, PDR makes⌈
409.2−401
2
⌉
= 5 decoding requests to IRD. Consider a naive
scheme that retrieves coded symbols from all reachable storage
nodes and decodes only once. The naive scheme may incur less
total computation than PDR, but has a high communication
cost, since coded symbols must be retrieved from every node
that is not crash-stop. PDR handles such trade-offs between
computation and communication more gracefully. For any
coding group, if all symbols have been retrieved and PDR still
cannot successfully decode, then Line 17 signals a decoding
failure.
PDR achieves minimum communication cost as additional
symbols are retrieved only when necessary. Moreover, we may
substitute in the state of the art Berlekamp-Massey algorithm
(BMA) or either of the Welch-Berlekamp (WB) or Euclidean
algorithm in for IRD, on Line 15 to reconstruct the original
codeword [9], [23], [24]. Our implementation results, however,
show that IRD is up to 35 times faster than the BMA and we
provide the reasons why in Section V.
V. IRD
Compared to classical RS decoding, IRD utilizes interme-
diate computation results and decodes incrementally as more
symbols become available. In this section, we characterize
the operations in Figure 3 and highlight the incremental
computation process of IRD.
Require: (n, kˆ)-MDS encoding of original data, file
Ensure: Reconstructed data, file′, equals file or 0
1: create an empty data-file, file′
2: for each encoded group [r0, r1, . . . , rn−1] of file do
3: i← kˆ
4: ri ← retrieve [rj0 , rj1 , . . . , rjkˆ−1 ] from any kˆ nodes
5: repeat
6: u← riGˆ−1
7: if CRC(u) is correct then
8: remove CRC header from u to obtain u0
9: append contents of u0 to file′
10: end if
11: repeat
12: i← i+ 2
13: retrieve ri1 , ri2 from any two remaining nodes
14: update ri with ri1 and ri2
15: until IRD(kˆ, n, i−kˆ2 , ri) ̸= 0 or i ≥ n− 1
16: until i ≥ n− 1
17: file′ ← 0
18: exit
19: end for
Fig. 2. PDR
A. The basic algorithm
Given the received coded symbols [r0, r1, . . . , rn−1] with
erasures set to be zero, the generalized syndrome polynomial
S(x) can be calculated as follows:
n−1∑
j=0
rjα
jbQ(x)−Q(αj)
x− αj =
n−1∑
j=0
λjα
jbQ(x)−Q(αj)
x− αj (16)
where Q(x) is an arbitrary polynomial with degree n− kˆ [25].
Assume that v errors occur in unknown locations j1, j2, . . . , jv
and s erasures in known locations m1,m2, . . . ,ms of the
received polynomial. Then
e(x) =
v∑
ℓ=1
ejℓx
jℓ and γ(x) =
s∑
ℓ=1
γmℓx
mℓ .
ejℓ is the value of the ℓ-th error, ℓ = 1, · · · , v, and γmℓ is
the value of the ℓ-th erasure, ℓ = 1, · · · , s. Since the received
values in the erased positions are zero, γmℓ = −cmℓ for ℓ =
1, · · · , s. The decoding problem is to determine all jℓ, ejℓ ,
and γmℓ . Let E = {j1, · · · , jv}, M = {m1, · · · ,ms}, and
D = E∪M. Clearly, E∩M = ∅. A key equation for decoding
RS codes is
Λ(x)S(x) = Ψ(x)Q(x) + Ω(x) , (17)
where
Λ(x) =
∏
j∈D
(
x− αj) = ∏
j∈E
(
x− αj) ∏
j∈M
(
x− αj)
= ΛE(x)ΛM(x) (18)
Ψ(x) =
∑
j∈D
λjα
jb
∏
i∈D,i ̸=j
(
x− αi) (19)
Ω(x) = −
∑
j∈D
λjα
jbQ(αj)
∏
i∈D,i̸=j
(
x− αi) . (20)
6exist via (22). By the definition of the generalized syndrome
polynomial in (16), for i ∈ U0\Uℓ, we have
S(ℓ)(αi) =
n−1∑
j=0
rjα
jQ(α
i)−Q(αj)
αi − αj
=
n−1∑
j=0,j /∈U0
rjα
j Q(α
j)
αj − αi + riα
iQ′(αi)
=
n−1∑
j=0,j /∈U0
Fj
αj − αi + riα
iQ′(αi) , (27)
where Q′(x) is the derivative of Q(x) and Fj = rjαjQ(αj).
Note that (28) follows, and S(ℓ)(αi) is not related to any rj ,
where j ∈ U0 and j ̸= i.
Q′(αi) =
∏
j∈U0,mj ̸=i
αi − αmj (28)
Hence, S(ℓ−1)(αi) = S(ℓ)(αi) for all i ∈ U0\Uℓ−1. This fact
implies that all sampled values in previous iterations can be
directly used in current iteration of the WB algorithm.
For any two polynomials, N(x) and W (x), we define the
rank[N(x),W (x)] as max[2 · deg(W (x)), 1+ 2 · deg(N(x))].
IRD returns kˆ error-free symbols whenever a codeword can
be successfully decoded and 0 otherwise. Specifically, if the
degree of the error-locator polynomial does not equal to the
number of roots determined by the Chien in Line 33, then the
codeword was not successfully decoded.
VI. PERFORMANCE ANALYSIS
In this section, we quantitatively analyze PDR and compare
it to Decentralized Erasure Codes (DEC) and Decentralized
Fountain Codes (DFC) [7], [8]. Our network consists of k data
nodes and n storage nodes, and each data node requires robust
distributed storage of its file of size T0. We proceed first by
briefly describing each coding scheme. Then we characterize
each one over the following: the total storage and its associated
overhead, communication and computation corresponding to
data dissemination and data retrieval when nodes fail, and the
security-strength.
DEC: Each storage node selects random and independent
coefficients in a finite field Fq , and stores a linear combination
of all the received data from the k data nodes. Randomized
linear codes are used, where each data node routes its packet to
at least nk log k storage nodes. A data collector queries at least
k storage nodes for data retrieval. In the absence of Byzantine
storage nodes, data retrieval is successful with high likelihood
if the finite field size is large.
DFC: Each storage node si, chooses a degree di, defined as
the number of data symbols required to form a linear combina-
tion. Node si then XORs di data symbols, from di arbitrarily
chosen data nodes, where each data symbol is selected from
Fq . DFC trades off communication for computation in that
decoding requires more than k storage nodes to be contacted,
though both encoding and decoding computations are linear
in the number of original symbols. We will show that the
probability of successful decoding is given by a parameter, δ.
PDR: The k data nodes collectively generate kT0 bits, T0 bits
per data node. Since each PDR data node adds r CRC bits to
its data, the file size at each data node is T = T0 + r. In our
analysis, the product of an m1 and m2-bit symbol requires
m1m2 XORs, and an addition requires only m1 XORs, where
the field of operation for the symbols is Fm1 and m1 ≥ m2.
DEC and DFC both have one data symbol per data node
for each coding group, while PDR encodes k data symbols
per data node for each group. Consequently, each data node
in DEC/DFC and PDR has ⌈ T0log q ⌉ and d groups per data
node, respectively, where d is given in (13). We show later in
this section that, although fountain codes have low encoding
and decoding computational complexities, PDR offers a much
lower communication cost for data retrieval.
A. Storage
The storage complexity for DEC is ⌈ T0log q ⌉ · log q ≈ T0 bits
for each storage node, since each storage symbol contains log q
bits, the field of operation is Fq , and there are ⌈ T0log q ⌉ groups.
Since each storage node stores linear combination coefficients,
where log k data nodes map to one storage node, the overhead
to store the coefficients, per storage node is
log k ·
(
log q + ⌈log k⌉+
⌈
log
⌈
T0
log q
⌉⌉)
≈ log k log q bits .
The outer log k term corresponds to the set of data nodes
mapped to a storage node, while the inner one corresponds to
the bits required to identify any mapped data node. The last
term identifies the coding group.
Similar to DEC, the storage complexity for DFC per storage
node is also T0 bits. The overhead complexity per storage node
is given by
log
k
δ
·
(
⌈log k⌉+
⌈
log
⌈
T0
log q
⌉⌉)
bits.
The derivation is similar to that of DEC except that the average
degree of a storage symbol is log kδ and there are no linear
combination coefficients, since every linear combination is
simply an XOR of a set of data symbols.
Each data node in PDR encodes its own data with kˆ symbols
per group to all storage nodes. Given there are d groups and
the application of the CRC, the per-node storage complexity
is T bits. Because PDR utilizes a code structure known to
all storage nodes, its overhead complexity per storage node is
⌈log d⌉, the bits required to index each group.
B. Data Dissemination
The DEC dissemination cost is
k · n
k
log k ·
⌈
T0
log q
⌉
· log q ≈ nT0 log k bits,
given that there are k data nodes, and each one sends its
data nk log k times, for all
⌈
T0
log q
⌉
groups. Similarly, the
dissemination cost for DFC is
n · log k
δ
·
⌈
T0
log q
⌉
· log q ≈ nT0 log k
δ
bits,
8N¯(n, kˆ) =
n−kˆ∑
v=0
(n
v
)
pv(1− p)n−v
min(v,⌊n−kˆ
2
⌋,n−v−kˆ)∑
i=0
(kˆ + 2i)
( n−v
i+kˆ−1
)(v
i
)( n
2i+kˆ−1
) · kˆ
i+ kˆ
· n− v − (i+ kˆ − 1)
n− (2i+ kˆ − 1)
+
n−kˆ∑
v=0
n
(n
v
)
pv(1− p)n−v
1− min(v,⌊
n−kˆ
2
⌋,n−v−kˆ)∑
i=0
( n−v
i+kˆ−1
)(v
i
)( n
2i+kˆ−1
) · kˆ
i+ kˆ
· n− v − (i+ kˆ − 1)
n− (2i+ kˆ − 1)

+
n∑
v=n−kˆ+1
n
(n
v
)
pv(1− p)n−v (29)
Prsuc(n, kˆ) =
n−kˆ∑
v=0
(n
v
)
pv(1− p)n−v
min(v,⌊n−kˆ
2
⌋,n−v−kˆ)∑
i=0
( n−v
i+kˆ−1
)(e
i
)( n
2i+kˆ−1
) · kˆ
i+ kˆ
· n− v − (i+ kˆ − 1)
n− (2i+ kˆ − 1) (30)
values computed. In the next iteration, two more symbols
are added to (16). Hence, the updated syndrome values can
be obtained by an extra O(kˆ) + O(n − kˆ) computations. To
determine the error-locator polynomial, the loop on (Line 8)
of IRD is executed twice, with complexity O(ℓ). Since we
consider a software implementation, the Chien search can
be implemented substituting powers of α into the error-
locator polynomial. The Chien search tests at most kˆ + ℓ
positions to locate kˆ error-free positions such that it takes
O((kˆ+ ℓ) · ℓ) computations. To invert Gˆ, we use an algorithm
closely related to the Lagrangian interpolation formula with
complexity, O(kˆ2) [26].
In summary, the computation in the ℓ-th iteration, for any
ℓ > 1, is given by (31). Summing over all v iterations and
accounting for the cost of determining the Fj values on Line 2
of IRD, we have (32).
Lv(ℓ) = O(kˆ
2) +O(n− kˆ) +O(kˆℓ+ ℓ2) (31)
Lv = O(vkˆ
2) +O(kˆ(n− kˆ)) +O(v2kˆ)
+O(v(n− kˆ)) +O(v3) (32)
Since v is bounded above by the error-correction capac-
ity (n − kˆ)/2, the decoding complexity is no greater than
O(kˆ(n − kˆ)2). When v is small, the term corresponding to
computing syndromes, O(kˆ(n− kˆ)) dominates. PDR requires
kd iterations to decode T . Hence for the entire data, the decod-
ing complexity is Tmk2 XORs in the absence of Byzantine
storage nodes. When v nodes are Byzantine, the complexity
is given by
Tm(vk2 + k(n− k) + v2k + v(n− k) + v3) XORs . (33)
G. Security
For generalized content distribution, decentralized erasure
and fountain codes achieve sub-quadratic computational de-
coding complexity, even in the midst of Byzantine nodes.
Indeed, fountain codes can support incremental decoding [27].
For distributed storage systems, however, none of these codes
support data reconstruction in polynomial time when a subset
of the networked storage nodes are Byzantine. In this section,
we address the security-strength of our proposed algorithm,
PDR when there are Byzantine nodes.
Without loss of generality, assume that the Byzantine nodes
are j0, j1, . . . , jv−1. Furthermore, these nodes collude to forge
data that can pass a CRC test. Suppose f i is the portion of
the forged vector of information symbols in the i-th group
that can pass the CRC test. Let fˆ i = f i+ui, where ui is the
original vector of information symbols in the i-th group after
successful decoding. Then (34) holds, since PDR leverages
Reed-Solomon codes. ci is the original codeword and v is the
compromised data due to the Byzantine nodes.
f iG = (fˆ i + ui)G = fˆ iG+ uiG = v + ci (34)
Let the number of nonzero symbols in v be h. It is clear
that h ≥ n− kˆ+1, where n− kˆ+1 is the minimum Hamming
distance of RS code, since v is a codeword. In the worst case,
h matches n− kˆ+1. An attacker must compromise a subset of
storage nodes, such that they store the corresponding encoded
symbols in f iG, the codeword corresponding to the forged
vector of information symbols.
If the attacker compromises kˆ storage nodes, the attacker
can forge the data successfully, when the data collector ac-
cesses these compromised storage nodes using PDR. Suppose
the attacker compromises b < kˆ storage nodes. Using PDR,
when h− b is no more than ⌊n−kˆ2 ⌋, PDR can still decode the
received vector to f iG. The minimum possible value of b is
⌈n−kˆ+22 ⌉; hence, the security-strength, the maximum number
of compromised nodes that cannot forge the information data
even when they collude, for IRD is min{kˆ, ⌈n−kˆ+22 ⌉} − 1.
Moreover, any other cryptographic hash function to increase
the security-strength against forging can be utilized. In par-
ticular, the 32-bit CRC code can be replaced with a 128-bit
MD5 code.
H. Summary
Table I summarizes the analysis in this section. The bits
required to identify a coding group is negligible in the over-
head of all three schemes. Clearly, PDR has the minimum
dissemination cost. The encoding and decoding of DFC can
be made highly efficient, but only at the expense of a high
communication cost, as governed by δ. Since the CRC over-
head of PDR is negligible (c.f.: Section IV), PDR achieves the
minimum cost for data dissemination and retrieval.
VII. IMPLEMENTATION AND EVALUATION
In Section VII-A and Section VII-B, we corroborate our
findings from Theorem 1 with network simulations. Then
10
0 20 40 60 800
0.5
1
1.5
2
2.5
3 x 10
14
Number of information symbols
D
is
se
m
in
at
io
n 
co
st
 (b
yte
s)
 
 
k=100
k=200
k=300
Fig. 6. The impact of the number of information symbols, kˆ, and data nodes,
k, on the communication cost of dispersing storage data.
doubled.
knm
⌈⌈T/m⌉
kˆ
⌉
(35)
C. Implementation setup
We have implemented the proposed and baseline algorithms,
where each data node’s information is a memory buffer in
a single machine having 2.66 GHz Intel Xeon CPU, 4096
KB cache and 2 GB RAM. A randomly generated message
is first partitioned into either 101 or 401 information symbols
and then encoded into n = 1023 coded symbols of length
10230 bits, and the finite field size is 1024. A stored symbol
is independently corrupted with error probability p.
Comparing our error-erasure code to either decentralized
or fountain erasure codes for error correction performance
is pointless, since these codes cannot feasibly guarantee data
availability in the presence of erroneous symbols. Instead, we
provide a comparison to the state of the art error-erasure de-
coding algorithm, BMA. Here, we progressively retrieves data
from each storage node and performs decoding until either the
decoded symbols passes the CRC test or cannot be decoded.
The work of Goodson et al. employs this algorithm [2]. In
addition, we consider a genie form of BMA, BMA-genie.
This algorithm assumes full knowledge of the number of
symbols required for successful decoding. It decodes only
once after retrieving the sufficient number of symbols. BMA-
genie cannot be implemented in practice and is included for
comparison only.
D. Total computation time
Figure 7(a) and 7(b) illustrates the computation time, in
log scale, spent in decoding when k = 101 and k = 401,
respectively; in these simulations, kˆ matches k. The storage
overhead n/k is 10.13 and 2.55 with the maximum number
of errors correctable being 461 and 311. From Figure 7, we
observe that the BMA and IRD computation time increases
as p increases, but the rate of growth of IRD is much slower.
In Figure 7(a), where the code-redundancy is high, IRD is
faster than the genie-aided BMA. This is because the cost,
O((n−k)2), of computing erasure polynomials dominates the
decoding time when p is small. Unlike BMA-genie, IRD does
not compute erasure polynomials. In our implementation, the
Byzantine node rate is 0.2, and there are 200 data nodes. We
employ a (1023, kˆ)−MDS code, where kˆ ∈ {50, 100, ..., 550}.
The computation cost for encoding is independent of kˆ, and
negligible compared to the computation cost for decoding cost.
The nonlinear increase in the decoding complexity supports
our theoretical derivations in (32).
E. Decoding Breakdown
We break down the decoding computation time to un-
derstand the dominant operations as the error probability
increases. The breakdown includes the times to find the error-
locator polynomial, find the error locations, and solve for
the information polynomial. We represent these three times
as elp-time, chien-time and inv-mat-time, respectively. This
breakdown is also illustrated in Figure 1, where the first and
second blocks correspond to the elp-time, the third block to
the chien-time, and the fourth block to the inv-mat-time.
In Figure 8(a), when the error probability is low, the
computation of error-locator polynomials dominates for small
kˆ, while the matrix inversion time becomes significant when kˆ
is large. In our implementation, the cost of a matrix inversion
is quadratic in the number of symbols decoded. Although the
Chien search has the highest asymptotic complexity, it has
the shortest running time. When the error probability is high,
Figure 8(b) and (c) show that the computation of error-locator
polynomials dominates except in IRD.
The computation time in matrix inversion is negligible and
on the order of tens of milliseconds in BMA and IRD; it is
comparable to that of the BMA-genie, which assumes knowl-
edge of the number of errors in advance , and thus performs
a matrix inversion only once. The negligible running time
in matrix inversion occurs because the decoding algorithm is
likely to fail during or prior to the Chien search on Line 33
of IRD; this holds even if the Byzantine node rate is high. In
most cases, BMA and IRD perform a matrix-inversion only
once.
VIII. CONCLUSIONS
We have designed a highly computation-efficient and
communication-optimal algorithm, PDR, for distributed stor-
age systems, where storage nodes can be either Byzantine or
crash-stop. The communication and computation costs for data
retrieval are minimized by utilizing intermediate computation
results and retrieving only the minimum data required for
successful data reconstruction, respectively.
Our in-depth analysis shows that decentralized fountain
codes and PDR are most suitable to networks without Byzan-
tine storage nodes because of the associated minimal computa-
tion cost for fountain codes, and the minimal data retrieval cost
for PDR. However, when Byzantine nodes exist, only PDR is
suitable since it allows for error detection and correction.
REFERENCES
[1] C. Karlof and D. Wagner, “Secure routing in wireless sensor networks:
Attacks and countermeasures,” Ad hoc networks, vol. 1, no. 2-3, pp.
293–315, 2003.
12
Symposium on Multiple-Valued Logic, Sendai, Japan, May 1992, pp.
138–145.
[26] H. William, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery,
Numerical Recipes in C: The art of scientific computing, Cambridge
university press New York, NY, USA, 1988.
[27] M.N. Krohn, M.J. Freedman, and D. Mazieres, “On-the-fly verification
of rateless erasure codes for efficient content distribution,” in Security
and Privacy, 2004. Proceedings. 2004 IEEE Symposium on, may 2004,
pp. 226 – 240.
表 Y04 
行政院國家科學委員會補助國內專家學者國外短期研究報告 
                                                             100 年 8 月 30 日 
報告人姓名  
韓永祥 
 
服務機構 
及職稱 
國立台灣科技大學 
教授 
 
     時間 
研究 
     地點 
100 年 6 月 27 日至 100 年 7
月 31 日 
美國休斯頓大學 
本會核定 
補助文號 
NSC 99-2221-E-011-157 
研究期間與休斯頓大學計算機科學學系教授 Rong Zheng 合作。我們合作的研究課題是
如何改進分散式儲存的問題。我們提出一個具備錯誤更正能力的再生碼(regenerating 
codes)的一個全新機制。此機制將一個再生碼的資料分散至一群儲存節點並可從崩潰或
拜占庭錯誤中回復資料。在崩潰或拜占庭錯誤的威脅下，我們提出的機制是一個漸進
式資料擷取機制。每次資料擷取時，此機制僅需從健康的節點擷取剛好足夠的資料以
恢復原始資料。此結果並且已投稿至 2012 IEEE INFOCOM。 
 
 
 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：韓永祥 計畫編號：99-2221-E-011-157- 
計畫名稱：基於錯誤控制碼的有效分散式網路儲存的研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 6 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
