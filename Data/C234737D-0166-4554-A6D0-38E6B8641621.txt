行政院國家科學委員會專題研究計畫成果報告 
具隱私權保護之視訊監控與影像搜尋 
Privacy-Preserving Video Surveillance and Image Retrieval 
計畫編號：NSC － 99 － 2221 － E － 468 － 023 
執行期限：99 年 8 月 1 日至 100 年 9 月 30 日 
主持人：林智揚   
執行機構及單位名稱: 亞洲大學資訊工程學系 
 
[註]:本計畫內容已國際期刊接受: Chin-Chen Chang, Chih-Yang Lin, and Yi-Pei Hsieh, “Reversible Steganographic 
Method for VQ images Using Multiple Bases and Declustering,” Information Sciences, accepted. 
另一個研究成果亦發表在 IIH-MSP 2011 國際會議上: Chih-Yang Lin,Chao-Chin Chang,Yi-Hui Chen,and Panyaporn 
Prangjarote, "Multimedia Privacy Protection System for Mobil Environments," Proceedings of The 7th International 
Conference on Intelligent Information Hiding and Multimedia Signal Processing(IIH-MSP 2011), Dalian,China. 
 
 
 Abstract 
Abstract—In recent years, mobile phones have become very 
popular due to technology advances. However, unprotected 
personal photos and videos can easily be leaked when the 
phone falls into the hands of a malicious user. In this paper, we 
propose a multimedia privacy-preserving system for mobile 
phones. The system can protect information of the faces on the 
images or encrypt the whole image. Since most of the mobile 
phones are equipped with low computational power, low 
complexity and high accuracy are the two chief considerations 
for our system. Experimental results show the feasibility of the 
proposed method. 
Keywords-component; mobile phones; privacy-preserving; face 
recognition 
 
 中文摘要 
最近幾年來隨著科技越來越進步，手機已經大眾化，手機拍
照功能也越來越普遍，然而因儲存方便，造成個人照片和影
片外流的可能性大幅提高。最常發生的情況就是手機遺失或
手機維修時，維修人員可能任意觀看手機內容，並加以竊
取，照成個人隱私資料外流。因此本系統以隱私權保護為目
標，不讓方便造成隨便，卻又能將個人照片留存而不被他人
拿來傷害自己。由於手機的普遍性，本系統將實現於 Windows 
Mobile 手機上，以人臉偵測和辨識方法發展出一套自動化隱
私保護系統。在本論文中，使用膚色判斷配合連通圖和人臉
五官特徵來搜尋人臉位置，並透過自動膚色切割方法將人臉
和背景切開，接著從影像中切出的人臉以 LBP(Local Binary 
Patterns)方法加以計算辨識，保護加解密方面則採用
Lookup-Table-Based 對照方法來完成加解密步驟。我們的計
畫目標為：確保使用者隱私獲得保護、確保使用者不遺失資
訊、滿足人臉偵測和人臉特徵擷取在手機上的基本運行、能
得到惡意使用者第一手相關線索。以上方法使用不僅簡單，
運算快速亦能達到隱私權保護之效果。 
關鍵詞：隱私權保護，多媒體系統，手機照相裝置、數位監
控 
 
計畫緣由及目的 
With the advent of digital technology, multimedia data 
are increasingly distributed over the Internet. Although 
digital data are beneficial for transmission, they can be easily 
distributed by malicious attackers. This is particularly event 
when using cell phones as an example. According to 
statistical data, the present population of Taiwan stands at 
about 22,700,000 people, and nearly 18,700,000 of them are 
used to using cell phones. That is to say that nearly 82% of 
the people in Taiwan use cell phones for communications 
and other applications. Nowadays, almost all the cell phones 
are equipped with cameras, which can shoot pictures or 
video anywhere. Since the images and video in mobile 
phones always raise privacy issues, how to effectively 
protect privacy when the mobile phone is lost, repaired, or 
out of the owner’s hands has become an important issue. 
In the traditional way, the mobile phone usually protects 
its content with a password. However, using a password is 
not convenient for users, so people usually choose a 
memorable or catchy numerical sequence, such as that from 
an ID number, birth date, or even a simple consecutive 
sequence. This increases the probability that a sequence will 
be cracked. Therefore, some cryptographic methods, such as 
DES or RSA [7] have been proposed to avoid password 
memorization. These methods, however, also encounter 
challenges: (1) protecting the secret key is required in these 
   (a)                             (b) 
Fig. 4. Skin color distribution in HSV and YCrCb spaces [3].
 
The linear transformation from RGB to YCrCb is shown in 
Eq. (1). 
  
B
G
R
214.18786.93000.112
000.112203.74797.37
966.24553.128481.65
128
128
16



































Cr
Cb
Y  
(1) 
 
From the previous research result [3], the skin color 
range based on YCrCb is 90~200 for Y, 134~165 for Cr, and 
80~128 for Cb. Based on these ranges, the result of face 
detection can be transformed into a binary image B1. 
However, this result may contain many breaks, mainly due to 
features like the eyes, brows, and mouth, so some 
morphlogical operations, such as dilation and erosion, should 
be applied to generate B2. The connected component method, 
in turn, is used to filter out small noise. Then, use B1 XOR B2 
to obtain B3, which only contains the eyes, brows, and mouth. 
Finally, partition B3 into two parts, an upper and lower part, 
and use horizontal and vertical projections to identify the 
locations of the eyes, brows, and mouth. According to these 
locations, the face region can be exactly identified. 
C. Face Recognition 
After the face region is determined, the corresponding 
color face image is transformed into a grey-level image and 
renormalized to the size 126147 [1]. The new normalized 
image is then partitioned into non-overlapping 33, 55, and 
77 blocks, as shown in Fig. 5. In this paper, a 77 
segmentation approach is applied. 
 
 
(a)                       (b)                       (c) 
Fig. 5. Face segmentation [1]. 
 
For each non-overlapping block, Ahonen et al. [1] 
proposed local binary patterns (LBP) code, which comes 
from the convolution of the block by using a 33 mask to 
generate its sub-histogram, denoted as follows: 
                 (2) 
In Eq(2), gc represents the grey value of the center pixel (xc, 
yc), whereas gp represents grey values of P neighborhood 
pixels of the center pixel. The function s(x) is denoted as 
follows: 
                                               (3) 
An example of an LBP value for a pixel is illustrated in Fig. 
6. Assume that the pixel value is 6 and its four surrounding 
pixels are 5, 9, 3, and 1 in the counterclockwise order. If the 
central pixel is greater than its neighbor, a bit 1 can be 
generated; otherwise, bit 0 is produced. Fig. 6(b) shows the 
result of the binary pattern, which indicates that the value of 
LBP is 2. 
 
(a) (b) 
Fig. 6. An example of generating LBP. 
 
In this paper, eight neighbors as shown in Fig. 7 are used 
for the LBP in the order of g4, g6, g7, g8, g5, g3, g2, and g1. In 
addition, Ahonen et al. only keep the LBP code with uniform 
patterns. An LBP code is called uniform if the binary pattern 
regarded as a circular pattern has at most two bitwise 
transitions from 0 to 1 or vice versa. For example, 01100000 
(a uniform pattern) has two transitions, and 01010011 (a 
non-uniform pattern) has six transitions. Only the uniform 
patterns contribute to the corresponding sub-histogram. 
Since the face image has been divided into 49 blocks, there 
will be 49 sub-histograms generated for the face description. 
Different sub-histograms have different weights as shown in 
Fig. 8, where black squares indicate weight 0.0, dark gray 
1.0, light gray 2.0 and white 4.0. Based on the 49 sub-
histograms and their corresponding weights, the similarity of 
two face images can be obtained by Eq. (4). The larger the χ2 
value, the more likely the two face images are related. 
 
g1 g2 g3
g4 gc g5
g6 g7 g8
 
Fig. 7. The LBP 33 mask. 
 
 
(a)
 
(b) 
Fig. 8. The weight distribution for LBP sub-histograms [1].
 
   
jiji
jiji
ij
j x
x
x
,,
2
,,
,
2
, 
 
.
 (4) 
D. Image Encryption and Decryption 
Since mobile phones are usually equipped with a low-
power CPU, the encryption and decryption processes should 
be performed in an uncomplicated manner. In this paper, we 
adopt Celik et al.’s method [2] based on a lookup table. This 
method not only encrypts the face image, but also embeds 
the fingerprint of the owner in the image. Therefore, the 
owner can claim the copyright of the image even when the 
image is camouflaged [4, 5]. The detail of Celik et al.’s 
method is described as follows. Assume that the number of 
the coefficients to be encrypted is t, i.e., c0, c1, …, ct1, and 
the size of the encryption table is L, where E[0], E[1], …, 
E[L1] denote the entries of the table. The size of the 
decryption table for user u is also L, whose entries are 
denoted as Du[0], Du[1], …, Du[L1], where Du[ℓ] =  
 
Fig. 14. The results of face recognition. 
 
 
Fig. 15. The result of mage encryption and decryption. 
 
 
4. 討論 
In this paper, we propose an effective method to protect 
privacy for mobile phones. All the operations in our system 
require quite considerably few computations and can be 
efficiently applied to real cell phone systems. In sum, our 
system can achieve four main functions: (1) protecting 
privacy in the mobile phone; (2) identifying the cell phone 
holder on the fly; (3) protecting copyrights of images; (4) 
getting firsthand clues on malicious users. 
ACKNOWLEDGMENT 
This work was supported by National Science Council, 
Taiwan, under Grant NSC 99-2221-E-468-023. 
 
 5. 參考資料 
 
[1] T. Ahonen, A. Hadid, and M. Pietikäinen, “Face Description with 
Local Binary Patterns: Application to Face Recognition,” IEEE 
Transactions on Pattern Analysis and Machine Intelligence, vol. 28, 
no. 12, pp. 2037-2041, 2006. 
[2]  M. U. Celik, A. N. Lemma, S. Katzenbeisser, and M. van der Veen, 
“Lookup-Table-Based Secure Client-Side Embedding for Spread-
Spectrum Watermarks,” IEEE Transactions on Information 
Forensics and Security, vol. 3, no. 3, pp. 475-487, 2008. 
[3] C. Garcia and G.Tziritas, “Face Detection Using Quantized Skin 
Color Regions Merging and Wavelet Packet Analysis,” IEEE 
Transactions on Multimedia, vol. 1, no. 3, pp. 264-227, 1999. 
[4] S. Lian, Multimedia Content Encryption: Techniques and 
Applications: Auerbach Publications, 2008. 
[5]  C. Y. Lin and C. S. Lu, “A New Joint Fingerprinting and Decryption 
Scheme Resistant to Content Leakage and Tampering Attack,” 
Proceedings of the 22th Computer Vision, Graphics and Image 
Processing Conference, pp. 10-18, 2009. 
[6]  K. H. Seo, W. Kim, C. Oh, and J.-J. Lee, “Face Detection and Facial 
Feature Extraction Using Color Snake,” Proceedings of the 2002 
IEEE International Symposium on Industrial Electronics, pp. 457-
462, 2002. 
[7]  W. Stallings, Cryptography and Network Security (4th Edition): 
Prentice Hall, 2005. 
 
 
 
 
 
 
 
 
 
3. Conclusions 
Many thanks for NSC’s supports that I can attend this conference to exchange 
some research experiences and I listened to a valuable keynote speech which is 
related to my research topic. Additionally, I am behalf of a session chair at 
Sessions B02. All the details are listed below. 
 
(1) Keynote speech 
Keynote Speech I- Professor Wen-Lian Hsu 
Room 104 (in International Conference Hall of HITSGS) 
 
Keynote Speech II- Professor Ajith Abraham 
Room 104 (in International Conference Hall of HITSGS) 
 
Keynote Speech III - Professor Jianrong Tan 
Room 104 (in International Conference Hall of HITSGS)  
 
Keynote Speech Ⅳ- Professor David Power 
Room 104 (in International Conference Hall of HITSGS) 
 
   (2) The conference program is listed below. 
 
December 13, 2010 
Registration and Welcome Reception --------------------------------- 08:00-09:00 
Opening ------------------------------------------------------------------- 09:00-09:15 
Keynote Speech (I) ------------------------------------------------------ 09:15-10:15 
Coffee Break -------------------------------------------------------------- 10:15-10:30 
Session A01-A03, P1 ---------------------------------------------------- 10:30-12:30 
Lunch ---------------------------------------------------------------------- 12:30-14:00 
Session A04-A06, P2 ---------------------------------------------------- 14:00-15:40 
Coffee Break -------------------------------------------------------------- 15:40-16:00 
Session A07-A09, P3 ---------------------------------------------------- 16:00-17:40 
Dinner --------------------------------------------------------------------- 18:30-20:30 
December 14, 2010 
Keynote Speech (II) ----------------------------------------------------- 08:00-09:00 
Keynote Speech (III) ---------------------------------------------------- 09:00-10:00 
Coffee Break -------------------------------------------------------------- 10:00-10:20 
Session B01-B03, P4 ---------------------------------------------------- 10:20-12:20 
Lunch ---------------------------------------------------------------------- 12:20-14:00 
Real-Time Robust Background Modeling Based on Joint Color and Texture 
Descriptions
Chih-Yang Lin 
Dept. of Computer Science & 
Information Engineering 
Asia University 
Taichung, Taiwan 
andrewlin@asia.edu.tw 
Chao-Chin Chang, Wei-Wen Chang, Meng-
Hui Chen 
Dept. of Computer Science & Information 
Engineering 
Nanhua University, 
Chiayi, Taiwan 
Li-Wei Kang 
Institute of Information Science 
Academia Sinica 
Taipei, Taiwan 
lwkang@iis.sinica.edu.tw 
Abstract—Background construction is the base of object 
detection and tracking for the machine vision system. 
Traditional background modeling methods often require 
complicated computations and are sensitive to illumination 
changes and shadow interference. In this paper, we propose a 
block-based background modeling method, which fully utilizes 
the color and texture characteristics of each incoming frame. 
The proposed method is quite efficient and is capable of 
resisting illumination changes and shadow disturbance. 
Experimental results show that our method is suitable for real-
word scenes and real-time applications. 
Keywords-component; Background modeling; motion 
detection; Gaussian mixture modeling 
I. INTRODUCTION
Object detection is a very important task in video 
surveillance. In general, background modeling is utilized for 
distinguishing between foreground and background. With a 
robust background model, the objects can then be 
successfully extracted from the background. 
In literature, a number of methods for detecting moving 
objects have been proposed, where many different features 
are employed for background modeling. The most 
frequently used features are based on color information. For 
example, a color statistical approach [13] accomplished 
background subtraction without being affected by shadow; 
furthermore, the algorithm is also implemented by a DM270 
iMX and DSP subsystem [9] for DV applications. In 
addition to the running statistics (e.g. average) of 
neighboring frames, a one-Gaussian adaptive modeling 
method is also a popular approach that can be found in [14].  
However, one-Gaussian modeling cannot cope with 
dynamic background changes. Therefore, the Gaussian 
mixture modeling (GMM) approach [11-12, 15] was 
developed by means of using more than one Gaussian model 
for each pixel. Pixel values that do not fit the model would 
be recognized as foreground areas. One of the examples 
using GMM was developed (three Gaussian) for traffic 
monitoring [4]. Other discussions on implementation using 
GMM can be found in [2] and [10], which provide details of 
Stauffer and Grimson’s algorithm [12]. 
Motion-based and edge-based methods are other 
approaches for background modeling. The motion-based 
method [13] utilizes optical flow to detect salient motion 
over frames. This approach usually suffers from 
complicated computations. The edge-based method [8] 
considers only edge information in frames and constructs 
edge histograms as a feature description for background 
modeling. The histogram-matching process determines the 
performance of this method. 
Recently, Heikkilä and Pietikäinen [5-6] proposed a 
texture-based background construction method using local 
binary patterns (LBPs). LBPs have the property of tolerance 
for illumination changes. However, LBPs are not robust as 
shown when the central pixel value used in LBP is affected 
by noise or swaying trees, the corresponding LBP histogram 
would not be stable. This would increase the possibilities of 
false positive and false negative cases, respectively. Besides, 
the overlapping block strategy and histogram-matching 
process proposed in [5-6] make their method inefficient.  
In this paper, we propose a hybrid method for background 
modeling based on the texture and color information from 
each frame. Instead of using LBPs, we propose a new 
texture descriptor, the idea of which is inspired by BTC 
(block truncation coding), [3] to enhance the tolerance of 
illumination changes and shadow interference. Furthermore, 
the background model integrated color and texture can 
effectively resist noise disturbance. Due to the 
computational simplicity of the proposed method, our 
background modeling has high efficiency and is suitable for 
real-time applications. 
The remainder of this paper is organized as follows. First, 
we briefly review Heikkilä and Pietikäinen’s method in 
Section 2. Then, in Section 3, we present our new 
background-modeling scheme based on texture and color 
descriptions. Empirical results and discussions are given in 
Section 4, and the conclusions are presented in Section 5. 
II. HEIKKILÄ AND PIETIKÄINEN’S METHOD
The texture-based method proposed by Heikkilä and 
Pietikäinen [5-6] first partitions each image frame into 
overlapping blocks so that the extracted shape of the moving 
object can be more accurately described. Then, the pixels in 
each block produce a histogram according to their LBP 
values. An example of an LBP value for a pixel is illustrated 
in Fig. 1. Assume that the pixel value is 6 and its surrounding 
2010 Fourth International Conference on Genetic and Evolutionary Computing
978-0-7695-4281-2/10 $26.00 © 2010 IEEE
DOI 10.1109/ICGEC.2010.159
622
the background model are renormalized in order to have a 
sum of one. 
In the above description, the incoming block may 
match the bitmap with a low weight and is regarded as a 
background block. However, the low weight means that the 
corresponding bitmap has a low probability of being a 
background block. To solve this problem, the weights of the 
background model are sorted in decreasing order, and only 
the first B bitmaps are selected as the background model, 
such that: 
1
B
i
i
w TH

 B , where THB is a predefined threshold. (8) 
C. Motion Detection Based on Joint Color and Texture 
Background Models 
The proposed method is to detect moving objects in the 
captured frames based on the constructed background 
models. In Section 3.2, we have proposed a texture-based 
background modeling method for moving object detection. 
However, a false negative may occur when the texture 
description of the moving object is similar to that of the 
background. Such a case can be greatly alleviated with the 
help of a color background model. The construction of a 
color background model for each block is done by using the 
Stauffer and Grimson’s algorithm [12], where the input for a 
block is the mean vector containing R, G, and B channel 
mean values calculated by Eq. (1), respectively. 
After the color background model is constructed, the moving 
objects can also be detected. The final result of motion 
detection is the intersection of CM and TM, where CM and 
TM represent the detection results using the color and texture 
models, respectively. 
IV. EXPERIMENTAL RESULTS
The performance of the proposed method is compared 
with two state-of-the-art approaches [5, 12] using several 
video sequences. The video sequences were acquired from 
real indoor and outdoor environments. The simulated 
environment for the experiments is equipped with a 1.8 GHz 
Core 2 Intel processor and 2 GB of memory. The image 
resolution was set to 320240 pixels. All algorithms were 
implemented in C++. 
For the sake of labeling and segmenting the foreground 
pixels, the connected components algorithm [1, 7] is applied 
to each background modeling method. The parameters used 
in the experiments are listed as in Table 1, where  is the 
learning rate, THB is used in Eq. (10), K denotes the number 
of Gaussians, X represents the fact that the parameter is not 
required in this method, BS means the block size, and 
LBPP,R is only used in Heikkilä and Pietikäinen’s method 
[5-6] and represents using radius R to find P neighbors such 
as the example shown in Fig. 1(a).  
The performance comparisons of these three methods 
are presented in Table 2, where the last row denotes the 
connected components labeling (CCL) method that is also 
involved in the background construction. From this table we 
can observe that the proposed method is much faster than 
other methods because the proposed block-based approach 
requires only bit operations. Heikkilä and Pietikäinen’s 
method is slowest since they divide each frame into 
overlapping blocks, and the size of the LBP histogram 
significantly affects the performance. 
Figs. 3 and 4 show the indoor scenes with some 
illumination changes, where people were walking towards 
the camera. The detection results can be observed that 
Stauffer and Grimson’s method [12] is very sensitive to 
illumination changes, and Heikkilä and Pietikäinen’s method 
[5-6] also suffers from noise interference. On the contrary, 
the proposed method has a better ability to resist illumination 
changes because we simultaneously consider the color and 
texture information and gather more information from an 
area instead of a single pixel. Since the non-overlapping 
block approach is adopted, the contour of the proposed 
method is coarser than that of the compared methods. 
More detection results of the outdoor scenes compared 
with Stauffer and Grimson’s method are shown in Figs. 5 
and 6. Although the proposed method may result in more 
holes on the inner areas of the moving object, such an effect 
can be eliminated by the morphology operations, as shown in 
Fig. 6.  
V. CONCLUSIONS
In this paper, we proposed a joint color- and texture-
based method for background modeling. The proposed 
method has the following advantages: (1) tolerance for 
illumination changes, (2) low computations, and (3) easy 
implementation. The experimental results show that our 
method is quite efficient (a frame rate of 62.5 can be 
achieved). Since the proposed method possesses a very high 
frame rate, it is quite suitable for real-time applications or 
low-power computation systems such as cell phones and 
personal digital assistants (PDAs). 
ACKNOWLEDGMENT
This work was supported by National Science Council, 
Taiwan, under Grants NSC 99-2218-E-468-005 and NSC 99-
2815-C-343-017-E. 
REFERENCES
[1] F. Chang, C. J. Chen, and C. J. Lu, “A Linear-Time Component-
Labeling Algorithm Using Contour Tracing Technique,” Computer
Vision and Image. Understanding, vol. 93, no. 2, pp. 206-220, 
2004. 
[2] Y. Dedeoglu, B. U. Töreyin, U. Güdükbay, and A. E. Çetin, 
“Silhouette-Based Method for Object Classification and Human 
Action Recognition in Video,” Proceedings of  ECCV Workshop 
on Computer Vision in Human-Computer Interaction, Graz, 
Austria, pp. 64-77, 2006. 
[3] E. J. Delp and O. R. Mitchell, “Image Compression Using Block 
Truncation Coding,” IEEE Transactions on Communications, vol. 
27, no. 9, pp. 1335-1341, 1979. 
[4] N. Friedman and S. Russell, “Image Segmentation in Video 
Sequences: A Probabilistic Approach,” Proceedings of the 13th 
624
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/12
國科會補助計畫
計畫名稱: 具隱私權保護之視訊監控與影像搜尋
計畫主持人: 林智揚
計畫編號: 99-2221-E-468-023- 學門領域: 影像處理
無研發成果推廣資料
報告 
研討會論文 4 0 100% 
1. Chih-Yang Lin,Wei-Wen 
Chang,and Yi-Hui 
Chen, ’’’’Intelligent 
Projector System base on Computer 
Vision,’’’’ Proceedings of 
Fifth International Conference 
on Genetic and Evolutionary 
Computing(ICGEC-2011) 
Kimen,Taiwan 
2. Chih-Yang Lin,Chao-Chin 
Chang,Yi-Hui Chen,and Panyaporn 
Prangjarote, ’’’’Multimedia 
Privacy Protection System for 
Mobil Environments,’’’’ 
Proceedings of The 7th 
International Conference on 
Intelligent Information Hiding 
and Multimedia Signal 
Processing(IIH-MSP 2011), 
Dalian,China 
3. Kahlil Muchtar,Chih-Yang 
Lin,Li-Wei Kang and Chia-Hung 
Yeh,’’’’Rubust Background 
Modeling Based on Multiscale 
Color Description,’’’’ 
Proceedings of Asia-Pacific 
Signal and Information 
Processing Association Annual 
Summit and Conference(APSIPASC 
2011) Xi'an,China 
4. Li-Wei KangChih-Yang 
Lin,Hung-Wei Chen,Chia-Mu Yu, 
Chun-Shien Lu, Chao-Yung Hsu, and 
Soo-Chang Pei, ’’’’Secure 
Transcoding for Compressive 
Multimedia Sensing,’’’’ 
Proceedings of IEEE 
International Conference on 
Image Processing(ICIP-2011) 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  
專利 
已獲得件數 1 0 100% 
件
Jau-Hong Kao, Chih-Yang Lin, 
Wen-How Wang, Yi-Ta 
Wu, ’’’’Methods and systems 
for creating a hierarchical 
appearance model,’’’’ 
