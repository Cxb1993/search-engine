mechanism of memetic algorithms. In the second year, 
the degree distribution adopted in the LT code (Luby 
transform code) was employed as a target problem. By 
empirically conducting the optimization of LT code 
degree distribution, we focused on observing and 
understanding the behavior of the optimization 
framework as well as obtaining some high-quality LT 
code degree distributions. Finally, with the support 
of this grant, we took a step further and improved 
the coding mechanism of LT code. A new fountain code 
which can be easily customized for various 
application scenarios of nature was developed. 
英文關鍵詞： Optimization technique, Evolutionary computation, 
Wireless network, Transmitting technique, Fountain 
code, LT code. 
 
 2
化技術。其中一種是使用經驗法則 (heuristic)，在可行解範圍逐步尋求最佳化。此
類演算法包括基因演算法 (Genetic algorithm) [1, 2]、模擬退火演算法 (Simulated 
annealing) [3, 4]、螞蟻族群演算法 (Ant colony algorithm) [5]、粒子群最佳化 
(Particle Swarm Optimization) [6, 7]… 等等。這類方法藉由模擬自然界的運作來達
到最佳化目的。這類方法不再受限於目標函數的數學特性，可以應用於非線性、不
可微分、或是不連續函數。無法用數學函數描述的問題，都可以設計模型，根據模
擬得到的回饋進行最佳化演算。只要兩組解的優勝劣敗能夠被某種方式比較，甚至
連不存在目標函數的問題也能適用，例如：個人化之樂音片段產生 [8]。此類演算
法的可行性與實用性非常高，具有一定的求解能力，在有限時間內通常可以獲得在
品質方面可被接受解，因此漸漸地被廣泛應用於現實世界問題。 
然而，針對演化計算中之基因演算法而言，在過去的文獻中，相關學者已曾指
出，基因演算法在解編碼不適當 (亦即有相依關係的變數未能被安排在一起) 的情
形下，最佳化效能將極其低落 [9]。因此，基因演算法之主要改進方向之一，就在
於鏈結學習 (Linkage learning)，亦即偵測變數間之相依性，並利用該資訊輔以各種
方式，包括動態調整解編碼、設計特殊演算子… 等，來增進基因演算法之最佳化
效能 [10]。此外，自從 No-free-lunch 定理 [11] 提出後，「泛用型」最佳化演算
法的存在即在理論層面遭到質疑；同時，現今對於最佳化演算方法的核心機制在理
論層面的認識不足，亦是目前演化計算領域無法長足發展的主因之一。是故，在本
計畫的第一年度中，吾人對於這幾個問題進行深入的探討與研究。 
在應用層面，本計畫於第二年度內針對 LT 編碼中採用的編碼密度機率模型
進行最佳化研究，並且也提出了 LT 編碼方法的改進方式，創造出能夠針對不同
應用情境客製化的可能性。LT 編碼目前已被以基本元件的形式採用於許多重要的 
rateless 編碼框架中，因此，增進 LT 編碼本身的效能是一件極為重要的事。為了 
LT 編碼的效能，許多研究提出了 LT 編碼部分元件的改進方式。[12, 13] 針對 LT 
編碼中的解碼演算法加以改進，使用不同的機制以還原來源資料。[14] 則將亂數
產生器置換為混沌 (chaos) 數列產生演算法，以提供 LT 編碼做為亂數使用。 
除了這些研究以外，更多的相關研究者則聚焦在設計編碼密度機率模型上，以
期得到比已被證明在來源資料符號數趨向無限大時非常接近最佳解的 Robust 
soliton distribution 能提供 LT 編碼更佳效能的編碼密度機率模型。於是，這類研
究 [15, 16] 專注於處理來源資料符號數較少的情境上，即便這些符號數小於 30 
的情境事實上目前可以使用高斯消除法來處理 [17, 18]。為了要能對需使用較多、
但距離無限大很遠的 LT 編碼情境最佳化，[19] 首先提出了使用經驗法則來進行
編碼密度機率模型的最佳化動作，並測試了符號數為 100 的情況。然而，就實務
層面及需求 (例如，即時多媒體資料傳輸、音訊與視訊串流等) 來看，符號數約在
數面到數萬之間，才是亟需研究的區段。此乃由於這種數量級的符號數仍然距離無
限大很遠，Robust soliton distribution 無法幫得上忙，但卻又多到非常難以尋得可
提供較佳 LT 編碼效能的編碼密度機率模型。包含本實驗室及合作研究者在內，
過去已有數項以演化計算方法來對編碼密度機率模型最佳化的研究 [20-22]，故在
本計畫中，我們在第二年度中更進一步地將前一年度內所得之成果，應用於此重要
的最佳化的問題上，並且亦試圖改進 LT 編碼原設計機制。 
 4
3. 建立彌因演算法核心運作機制之數學模型 
彌因演算法 (Memetic algorithm) 與基因演算法的不同之處，在於彌
因演算法強調以「後天學習所獲得之技能」的觀點，來思考區域搜尋 (Local 
search) 運算子在最佳化過程中所扮演的角色。而最佳化方法要達到優良
效能，其必要條件即為所使用之廣域搜尋 (Global search) 運算子與區域
搜尋運算子能合作與平衡。然而，長期以來縱有一些零星的理論探討研
究，大多流於範圍狹窄或模型不實際。因此上述概念的本身雖被大部分相
關學者所認同，但一直停留在觀念階段，無法落實為可進行運算的數學標
的。本實驗室針對此點，建立起具體卻又不失一般性之彌因演算法數學模
型，用以探討廣域搜尋機制與區域搜尋機制的相對性質與平衡之取得，並
得以之做為設計出效能更佳之演算法的指導原則。 
而由我們的研究結果可知，如下圖所示，廣域搜尋機制與區域搜尋機
制的平衡乃奠基於搜尋資源是否平均分配於這兩種搜尋機制。這裡所謂的
「平均分配」並非是以主動且直接的方式，讓廣域搜尋和區域搜尋各佔一
半的搜尋資源，而是要藉由調整其採用的最佳化演算法本身所具有的參數
來達成。若該演算化在某組參數設定下，能恰好在廣域搜尋行為和區域搜
尋行為展現時使用各半的搜尋資源，則應是其最有效率的情境。 
  
 6
5. 尋找對 LT 編碼最佳化時所需的替代評估函數 
改進 LT 編碼最直覺的想法就是降低其所需之 overhead，因此評估
函數通常是去計算 LT 編碼搭配某編碼密度機率模型的 overhead。遺憾
的是目前沒有一個 closed form 可以去計算平均所需的 overhead，只能依
靠大量模擬資料。若我們從另一角度來觀察 LT 編解碼行為，當收集到
的 output symbols 越來越多時，LT 編碼有越高的機率可以完全解開 
input symbols。這表示我們可以固定 LT 編碼接收到的 overhead，求出並
最小化 LT 編碼失敗機率。在本計畫中，我們參考了兩種見於文獻的評
估計算方式 [28, 29]，並使用於編碼密度機率模型的最佳化過程，進而獲
得如下表所條列之高效能 LT 編碼編碼密度機率模型。 
 
6. 研究稀疏機率分佈的選擇 
在最佳化架構中為了減輕搜尋負擔，我們採用稀疏機率分佈來取代完
整機率分佈。此方法可以有效地降低搜尋空間的大小，但同時也限制了找
到全域最佳解的可能。在先前的研究中，我們依靠實驗經驗，手動決定適
合的 degree 來組成稀疏機率分佈中的非零項，雖然最佳化後的結果確實
優於 Robust soliton distribution，但我們不曉得這些分佈是否已經逼近全
域最佳解，抑或有其他的 degree 組合能夠找出更佳的稀疏機率分佈。根
據文獻指出，不同 degree 的 output symbols 各自有其解碼作用。為了釐
清不同 degree 對於 LT 解碼率的影響，我們考慮量化各個 degree 上的
機率跟 LT 解碼率間的關係。分析這些數據有助於我們找到最適當的
degree 集合，使其所對應到的子搜尋空間能儘量逼近全域最佳解位置。 
 
 
 8
五、 結果與討論 
本計畫於兩年期間，進行偵測決策變數間關係技術之深入研究、探討演化計算
方法之理論根基，並且針對 LT 編碼問題進行研究，包括編碼密度機率模型的最
佳化以及改進 LT 編碼機制等。在這些主題中，已完成之具體工作項目如下： 
 參與人員獲得以下之訓練： 
 培養研究生分工合作之能力； 
 訓練參與人員研究、統合與論文寫作能力； 
 統整研究成果並發表學術論文； 
 強化參與人員之資料分析、演化計算、機械學習、數值分析與最佳化技術
等相關技能。 
 深入探討歸納式鏈結學習法之性質: 包括其對於含有不同大小與型別的建構
基石之問題的表現，以及建立數學模型以顯示並預測歸納式鏈結學習法所需之
人口數目相對於問題大小的成長關係。 
 奠定泛用型方法存在之可能性及其範圍: 提出數學框架以理論觀點配合計算
實務以合理地解釋 No-free-lunch 定理雖然在其定義的範圍內為真，但並不會
對於發展實務計算各項方法造成任何影響。 
 建立演化計算方法之數學模型: 提出數學模型並用以探討最佳化計算法方中
所含有之廣域搜尋機制與區域搜尋機制的相對性質，以及其應如何調配方能取
得平衡，以利設計出效能更佳之演算法。 
 分析粒子群演算法的收歛時間: 奠基於本實驗室之前所提出之粒子群演算法
收歛的數學模型，進行收歛時間的推導，並將實際之粒子群演算法運用於處理
具體的數學函數以獲得可驗證所推導之收歛時間的數值結果。 
 將研究成應用於 LT 編碼中之編碼密度機率模型最佳化: 我們首先尋找對 
LT 編碼最佳化時所需的替代評估函數，再配合研究稀疏機率分佈的選擇以降
低最佳化演算法的負擔，從而成功對編碼密度機率模型進行最佳化。 
 改進 LT 編碼機制使其得以客製化: 將演化計算領域經常使用的 tournament 
selection 概念，應用於改進 LT 編碼機制中，致使 LT 編碼採用者有機會能
針對其不同的應用情境與狀況，將 LT 編碼客製化與最佳化。 
 撰寫報告並投稿相關期刊與重要會議論文以公開發表本計畫各項研究成果。本
實驗室目前基於國科會之研究計畫補助，投稿與發表了以下的學術論文： 
 期刊論文： 
 Chen, C.-M., Chen, Y.-p., Shen, T.-C., & Zao, J. K. A Practical 
Optimization Framework for the Degree Distribution in LT Codes. IET 
Communications. (Submitted) 
 10
參考文獻 
[1] D. E. Goldberg, Genetic algorithms in search, optimization, and machine 
learning. Reading, Mass.: Addison-Wesley Pub. Co., 1989. 
[2] D. E. Goldberg, The design of innovation : lessons from and for competent 
genetic algorithms. Boston: Kluwer Academic Publishers, 2002. 
[3] S. Kirkpatrick, et al., "Optimization by Simulated Annealing," Science, vol. 220, 
pp. 671-680, 1983. 
[4] V. Černý, "Thermodynamical approach to the traveling salesman problem: An 
efficient simulation algorithm," Journal of Optimization Theory and Applications, 
vol. 45, pp. 41-51, 1985. 
[5] M. Dorigo, et al., "Ant algorithms for discrete optimization," Artificial Life, vol. 
5, pp. 137-172, 1999. 
[6] R. C. Eberhart and J. Kennedy, "A new optimizer using particle swarm theory," 
Proceedings of the Sixth International Symposium on Micromachine and Human 
Science, pp. 39-43, 1995. 
[7] J. Kennedy and R. C. Eberhart, "Particle swarm optimization," Proceedings of 
IEEE International Conference on Neural Networks, pp. 1942-1948, 1995. 
[8] T. Y. Fu, et al., "Evolutionary interactive music composition," Proceedings of 
ACM SIGEVO Genetic and Evolutionary Computation Conference 2006 
(GECCO-2006), pp. 1863-1864, 2006. 
[9] D. E. Goldberg, et al., "Messy genetic algorithms: Motivation, analysis, and first 
results," Complex Systems, vol. 3, pp. 493-530, 1989. 
[10] Y.-p. Chen, Extending the scalability of linkage learning genetic algorithms: 
Theory and practice, 2005. 
[11] D. H. Wolpert and W. G. Macready, "No free lunch theorems for optimization," 
IEEE Transactions on Evolutionary Computation, vol. 1, pp. 67-82, 1997. 
[12] H. Tarus, et al., "Exploiting Redundancies to Improve Performance of LT 
Decoding," Proceedings of the 6th Annual Conference on Communication 
Networks and Services Research (CNSR 2008), pp. 198-202, 2008. 
[13] F. Lu, et al., "LT codes decoding: Design and analysis," Proceedings of the IEEE 
International Symposium on Information Theory (ISIT 2009), pp. 2492-2496, 
2009. 
[14] Q. Zhou, et al., "Encoding and Decoding of LT Codes Based on Chaos," 
Proceedings of the 3rd International Conference on Innovative Computing 
Information and Control (ICICIC ’08), pp. 451-451, 2008. 
[15] E. Hyyti¨a, et al., "Optimal Degree Distribution for LT Codes with Small 
Message Length," Proceedings of the 26th IEEE International Conference on 
Computer Communications (INFOCOM 2007), pp. 2576-2580, 2007. 
[16] E. A. Bodine and M. K. Cheng, "Characterization of Luby Transform Codes with 
Small Message Size for Low-Latency Decoding," Proceedings of the IEEE 
International Conference on Communications, pp. 1195-1199, 2008. 
[17] V. Bioglio, et al., "On the fly Gaussian elimination for LT codes," IEEE 
Communications Letters, pp. 953-955, 2009. 
[18] M. Rossi, et al., "SYNAPSE++: Code Dissemination in Wireless Sensor 
Networks Using Fountain Codes," IEEE Transactions on Mobile Computing, vol. 
 12
附錄: 已發表之論文全文 
期刊論文： 
1. Chen, Y.-p., Chuang, C.-Y., & Huang, Y.-W. (2012). Inductive linkage 
identification on building blocks of different sizes and types. International Journal 
of Systems Science, 43(12), 2202–2213. doi: 10.1080/00207721.2011.566639. (SCI, 
EI). 
2. Lee, M.-C., Leu, F.-Y., & Chen, Y.-p. (2012). PFRF: An adaptive data replication 
algorithm based on star-topology data grids. Future Generation Computer Systems, 
28(7), 1045–1057. doi: 10.1016/j.future.2011.08.015. (SCI, EI). 
3. Chen, C.-H., & Chen, Y.-p. (2011). Convergence time analysis of particle swarm 
optimization based on particle interaction. Advances in Artificial Intelligence, 
2011(204750), 1–7. doi: 10.1155/2011/204750. 
4. Lin, J.-Y., & Chen, Y.-p. (2011). Analysis on the collaboration between global 
search and local search in memetic computation. IEEE Transactions on Evolutionary 
Computation, 15(5), 608–623. doi: 10.1109/TEVC.2011.2150754. (SCI, EI). 
5. Jiang, P., & Chen, Y.-p. (2011). Free lunches on the discrete Lipschitz class. 
Theoretical Computer Science, 412(17), 1614–1628. doi: 10.1016/j.tcs.2010.12.028. 
(SCI, EI). 
會議論文： 
1. Tsai, P.-C., Chen, C.-M., & Chen, Y.-p. (2012). Sparse degrees analysis for LT 
codes optimization. In Proceedings of 2012 IEEE Congress on Evolutionary 
Computation (CEC 2012) (pp. 2463–2468). doi: 10.1109/CEC.2012.6252861. (EI). 
2. Lin, J.-Y., & Chen, Y.-p. (2012). When and what kind of memetic algorithms 
perform well. In Proceedings of 2012 IEEE Congress on Evolutionary Computation 
(CEC 2012) (pp. 2716–2723). doi: 10.1109/CEC.2012.6252894. (EI). 
 
International Journal of Systems Science
Vol. 43, No. 12, December 2012, 2202–2213
Inductive linkage identification on building blocks of different sizes and types
Ying-ping Chen*, Chung-Yao Chuang and Yuan-Wei Huang
Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan
(Received 24 March 2010; final version received 17 February 2011)
The goal of linkage identification is to obtain the dependencies among decision variables. Such information or
knowledge can be applied to design crossover operators and/or the encoding schemes in genetic and evolutionary
methods. Thus, promising sub-solutions to the problem will be disrupted less likely, and successful convergence
may be achieved more likely. To obtain linkage information, a linkage identification technique, called Inductive
Linkage Identification (ILI), was proposed recently. ILI was established upon the mechanism of perturbation and
the idea of decision tree learning. By constructing a decision tree according to decision variables and fitness
difference values, the interdependent variables will be determined by the adopted decision tree learning
algorithm. In this article, we aim to acquire a better understanding on the characteristics of ILI, especially its
behaviour under problems composed of different-sized and different-type building blocks (BBs) which are not
overlapped. Experiments showed that ILI can efficiently handle BBs of different sizes and is insensitive to BB
types. Our experimental observations indicate the flexibility and the applicability of ILI on various elementary
BB types that are commonly adopted in related experiments.
Keywords: inductive linkage identification; ILI; linkage learning; BBs; genetic algorithms; evolutionary
computation
1. Introduction
Previous studies (Goldberg, Korb, and Deb 1989;
Harik 1997) on genetic algorithms (GAs), which are
widely utilised to handle control and engineering
problems (Wang 2009; Li and Li 2010; Gladwin,
Stewart, and Stewart 2011), have shown that the
encoding scheme of solutions is one of the key factors
to the success of GAs by demonstrating that simple
GAs fail to handle problems of which the solutions are
represented with loose encodings while genetic algo-
rithms capable of learning linkage succeed. If strongly
related variables, which are usually referred to as
building blocks (BBs), are arranged loosely with the
adopted representation, they are likely to be disrupted
by crossover operations. Such a condition contributes
to the divergence of population, instead of the
convergence towards optimal solutions. Although
encoding strongly related variables tightly or making
crossover operators aware of such relationships could
mitigate the problem and improve the GA perfor-
mance (Stonedahl, Rand, and Wilensky 2008), both
measures require the foreknowledge of the target
problem, which is often not the case in which evolu-
tionary algorithms are adopted.
In order to overcome the BB disruption problem,
a variety of techniques have been proposed and
developed in the past two decades and can be roughly
classified into three categories (Munetomo and
Goldberg 1998; Chen, Yu, Sastry, and Goldberg 2007):
(1) Evolving representations or operators;
(2) Probabilistic modelling for promising
solutions;
(3) Perturbation methods.
The objective of the techniques in the first class is to
make individual promising sub-solutions separated
and less likely to be disrupted by crossover via
manipulating the representation of solutions during
optimisation. Various reordering and mapping opera-
tors have been proposed in the literature, such as self-
crossover (Pal, Nandi, and Kundu 1998), which is
proven able to generate any arbitrary permutation of
the symbols, the messy GA (mGA) (Goldberg et al.
1989), and the fast mGA (fmGA) (Kargupta 1995),
which is the more efficient descendant of mGA. The
difficulty faced by these methods is that the reordering
operator usually reacts too slow and loses the race
against selection. Therefore, premature convergence at
local optima occurs. Another technique, the linkage
learning GA (LLGA) proposed by Harik (1997), uses
circular structures as the representation with two-point
crossover such that the tight linkage might be more
likely preserved. LLGA works well while the shares of
BBs are exponentially apportioned in the total fitness,
*Corresponding author. Email: ypchen@cs.nctu.edu.tw
ISSN 0020–7721 print/ISSN 1464–5319 online
 2012 Taylor & Francis
http://dx.doi.org/10.1080/00207721.2011.566639
http://www.tandfonline.com
D
ow
nl
oa
de
d 
by
 [N
ati
on
al 
Ch
iao
 T
un
g U
niv
ers
ity
] a
t 2
3:0
9 2
4 O
cto
be
r 2
01
2 
Its scalability and efficiency against the increasing
problem sizes have been demonstrated (Chuang and
Chen 2007, 2008; Huang and Chen 2009, 2010).
Compared to the conventional perturbation methods,
such as LINC and LIMD, ILI utilises a data mining
technique to analyse objective functions. Compared to
D5, which uses clustering, and the method proposed by
Ting et al. (2010), which uses Apriori algorithm, ILI
adopts the ID3 algorithm and behaves quite differ-
ently. In this article, we aim to address more detailed
characteristics of ILI in order to gain deeper insights
and better understandings of linkage learning. In
particular, problems constructed by non-overlapped
BBs of different sizes and sub-functions are studied
and experimented upon. Our experimental results
indicate that ILI holds the properties of robustness
and efficiency when facing various configurations
of BBs.
The remainder of this article is organised as
follows. In Section 2, the background of linkage
leaning in GA and decomposability of problems is
briefly introduced. Section 3 gives an introduction to
ILI, including a review of the ID3 decision tree
learning algorithm, an example illustrating the pro-
posed approach, and an algorithmic description of ILI.
Section 4 presents the experiments conducted in this
study and the results revealing the behaviour of ILI.
Finally, Section 5 summaries and concludes this article.
2. Linkage and BBs
In this section, we briefly review the definitions and
terminologies which will be used through out this
article. As stated by de Jong, Watson, and Thierens
(2005), ‘two variables in a problem are interdependent
if the fitness contribution or optimal setting for one
variable depends on the setting of the other variable’,
and such relationship between variables is often
referred to as linkage in the GA literature. In order
to obtain the full linkage information of a pair of
variables, the fitness contribution or optimal setting of
these two variables will be examined on all possible
settings of the other variables.
Although obtaining the full linkage information is
computationally expensive, linkage should be esti-
mated using a reasonable amount of efforts if the
target problem is decomposable. According to the
Schema theorem (Holland 1992), short, low-order and
highly fit substrings increase their share to be com-
bined. Also stated in the BB hypothesis, GAs implicitly
decompose a problem into sub-problems by processing
BBs. It is considered that combining small parts is
important for GAs and is consistent with human
innovation (Goldberg 2002). These lead to a problem
model called the additively decomposable function
(ADF), which can be written as a sum of low-order
sub-functions.
Let a string s of length ‘ be described as a series of
variables, s¼ s1s2    s‘. We assume that s¼ s1s2    s‘ is
a permutation of the decision variables x¼ x1x2    x‘
to represent the encoding scheme adopted by GA
users. The fitness of string s is then defined as
f ðsÞ ¼
Xm
i¼1
fiðsviÞ, ð1Þ
where m is the number of sub-functions, fi is the i-th
sub-function and svi is the substring to fi. Each vi is a
vector specifying the substring svi . For example, if
vi¼ (1, 2, 4, 8), svi ¼ s1s2s4s8. If fi is also a sum of other
sub-functions, it can be replaced by those sub-
functions. Thus, each fi can be considered as a
nonlinear function.
By eliminating the ordering property of vi, we can
obtain a set Vi containing the elements of vi. The
variables belonging to the same set of Vi is regarded as
interdependent because fi is nonlinear. Thus, we refer
to the set Vi as a linkage set. A related term, BBs,
is referred to as the candidate solutions to sub-function
fi. In this article, only a subclass of the ADFs is
considered. We concentrate on non-overlapping sub-
functions. That is, Vi\Vj¼; if i 6¼ j. In addition, the
strings are assumed to be composed of binary variables.
3. Inductive linkage learning
In this section, the ideas behind ILI will be presented.
Then, the ID3 algorithm, which is proposed and widely
utilised in the field of machine learning, will be briefly
introduced. An example is given to illustratively
explain the mechanism of ILI, followed by the
pseudo code.
In ILI, linkage learning is regarded as the issue of
decision tree learning. As an illustration, the fitness
difference can be derived in the following equation
within the ADF model:
f ðs1s2    s8Þ ¼ f1ðs1s2s3s4s5Þ þ f2ðs6s7s8Þ
df1ðsÞ ¼ f ðs1s2    s8Þ  f ðs1s2    s8Þ
¼ ð f1ðs1s2s3s4s5Þ þ f2ðs6s7s8ÞÞ
 ð f1ðs1s2s3s4s5Þ þ f2ðs6s7s8ÞÞ
¼ f1ðs1s2s3s4s5Þ  f1ðs1s2s3s4s5Þ: ð2Þ
Equation (2) indicates that the fitness difference df1
should be affected only by the bits belonging to the
same sub-functions as the perturbed bits s1, which
are s1s2    s5. Since certain fitness difference values are
respectively caused by particular bits arranged in some
2204 Y.-p. Chen et al.
D
ow
nl
oa
de
d 
by
 [N
ati
on
al 
Ch
iao
 T
un
g U
niv
ers
ity
] a
t 2
3:0
9 2
4 O
cto
be
r 2
01
2 
identify the two linkage sets V1¼ {1, 2, 3, 4, 5} and
V2¼ {6, 7, 8}, which correspond to the problem struc-
tural decomposition.
In the beginning, a population of strings is
randomly generated as listed in Table 1. The first
column lists the solution strings, and the second
column lists the fitness values of the corresponding
strings. After initializing the population, we perturb
the first variable s1 (0! 1 or 1! 0) for all strings in
the population in order to detect the variables
interdependent on s1. Note that the choice of first
operating on s1 in this example is not mandatory. Any
un-grouped decision variable in the encoding may be
chosen as the root node. The third column of Table 1
records the fitness differences, df1, caused by pertur-
bations at variable s1.
Then, we construct an ID3 decision tree by using
the perturbed population of strings as the training
instances and the perturbed variable s1 as the tree root.
Variables in s1s2    s8 are regarded as attributes of the
instances, and the fitness differences df1 are the target
values/class labels. Corresponding to Table 1, an ID3
decision tree shown in Figure 1 is constructed. By
gathering all the decision variables on the non-leaf
nodes, we can identify a group of s1, s2, s3, s4 and s5. As
a consequence, linkage set V1 is correctly identified.
For the remainder of this example, since s1, s2, s3, s4
and s5 are already identified as linkage set V1, we
proceed at s6. The fitness differences after perturbing
variable s6 are shown in Table 2. Conducting the same
procedure, an ID3 decision tree presented in Figure 2 is
obtained. By gathering all the decision variables used
in the decision tree, we obtain variables s6, s7 and s8,
which form linkage set V2. Because all the decision
variables are classified into their respective linkage sets,
the linkage detecting task is accomplished. ILI finally
reports two linkage sets, V1¼ {s1, s2, s3, s4, s5} and
V2¼ {s6, s7, s8}.
As illustrated in the example, the mechanism of ILI
can detect size-varied BBs without assumptions. Such
an ability implies that ILI should be capable of finding
all relations among these variables as long as the
Table 2. Population perturbed at s6.
s1s2    s8 f df6 s1s2    s8 f df6
11100 000 1 0 10101 100 1 0
10011 000 1 0 01101 100 1 0
11011 001 0 0 00100 100 3 0
01111 001 0 0 10010 101 2 0
00100 001 3 0 10110 101 1 0
11111 010 5 0 11110 101 0 0
10101 010 1 0 01101 101 1 0
11100 010 1 0 01110 110 1 0
10001 010 2 0 01111 110 0 0
11011 010 0 0 01110 110 1 0
10000 010 3 0 10101 110 1 0
01101 010 1 0 01111 110 0 0
00001 011 3 3 10010 110 2 0
00001 011 3 3 00011 111 5 3
11010 011 1 3 00011 111 5 3
11001 011 1 3 01000 111 6 3
11111 011 5 3 00101 111 5 3
11100 011 1 3 11001 111 4 3
01010 011 2 3 00110 111 5 3
10111 100 0 0 01111 111 3 3
Figure 1. ID3 decision tree constructed according to Table 1.
Table 1. Population perturbed at s1.
s1s2    s8 f df1 s1s2    s8 f df1
00001 011 3 1 10010 110 2 1
00011 111 5 1 10011 000 1 1
00100 001 3 1 10101 010 1 1
00100 100 3 1 10101 100 1 1
00101 111 5 1 10101 110 1 1
00110 111 5 1 10110 101 1 1
01000 111 6 1 10111 100 0 1
01010 011 2 1 11001 011 1 1
01101 010 1 1 11001 111 4 1
01101 100 1 1 11010 011 1 1
01101 101 1 1 11011 001 0 1
01110 110 1 1 11011 010 0 1
01111 001 0 5 11100 000 1 1
01111 110 0 5 11100 010 1 1
01111 111 3 5 11100 011 1 1
10000 010 3 1 11110 101 0 1
10001 010 2 1 11111 010 5 5
10010 101 2 1 11111 011 5 5
2206 Y.-p. Chen et al.
D
ow
nl
oa
de
d 
by
 [N
ati
on
al 
Ch
iao
 T
un
g U
niv
ers
ity
] a
t 2
3:0
9 2
4 O
cto
be
r 2
01
2 
obtain the minimal population sizes required for
different problem configurations. For a given problem,
first a population size assuring successful trials of
linkage identification, which means correctly identify-
ing all the BBs within the problem for 30 consecutive
and independent runs, is obtained by doubling the
population size from 2500 until the first successful trial
is archived. Once the upper bound of population sizes
PU is found, the required population size is determined
in a bisection manner: the population size P¼
(PLþPU)/2 will be configured for ILI, where PL¼ 1
for the first iteration. If ILI can succeed with this
population size P, then P will be regarded sufficiently
large for the problem. The next iteration will perform
on the range [PL,P]. Otherwise, the range [P,PU] will
be used. This procedure repeats until the range is
smaller than a predefined distance, which is 2 in this
study, and the last tested population size is considered
as the minimal requirement for the current problem.
4.1. Different BB sizes
This section describes the experiment on problems of
identical overall sizes but with different-sized sub-
functions. From our experimental results with different
configurations of the BB size k and the number of BBs
m, we group those results with the overall problem
sizes and arrange them with the BB size k. Thus, the
results of the same problem size with different k can be
examined.
Figure 3(a) and (b) shows the experimental results
where the overall problem sizes are 60 bits, 240 bits,
420 bits and 600 bits with a log-scaled y-axis. The
straight lines indicate that for identical overall problem
sizes, the requirements of both the population size and
the function evaluation grow exponentially.
With the exponential regression of the experimental
results, an estimation of y¼C 2ak can be obtained,
where a is a constant around 0.8 and C varies with
different problem sizes. Earlier studies by Munetomo
and Goldberg (1998) and Heckendorn and Wright
(2004), respectively, suggested an empirical and a
theoretical upper bounds of function evaluations,
which are both in the form of 2k‘ j log() for problems
of ‘ bits, composed of order-k BBs and each BB
sharing j bits with others. Reviewing our empirical
results with the upper bounds, ILI shows the same
computational complexity of the exponential growth
with k when overall sizes remain constant, such an
observation is consistent with the upper bounds
reported in the literature. However, the regression
gives 0.8 as the base of exponent and thus indicates a
practically better efficiency compared to the suggested
upper bound when the complexity of sub-problem
increases.
4.2. Mixed BB sizes
One of the key features of ILI is unsupervised. In this
section, we inspect this feature by conducting experi-
ments on the problems consisting of non-overlapping
BBs of order-k1 and order-k2 trap functions as
trapk1þk2 ðÞ ¼
Xm
i¼1
trapk1ðÞ þ trapk2 ðÞ
 
, ð7Þ
where m is the number of trapk1 and trapk2 . By
designing the experiments in this way, the empirical
results can be easily compared with those from
problems consisting of identical sub-problem complex-
ities in the following manner: for each problem size
obtained from the experiment of trapk1þk2ðÞ, two
3 4 5 6
100
250
500
1000
2000
BB size
Po
pu
la
tio
n 
siz
e
Overall problem size = 60 bits
Overall problem size = 240 bits
Overall problem size = 420 bits
Overall problem size = 600 bits
3 4 5 6
2000
5000
10,000
25,000
50,000
100,000
250,000
500,000
BB size
Fu
nc
tio
n 
ev
al
ua
tio
n
Overall problem size = 60 bits
Overall problem size = 240 bits
Overall problem size = 420 bits
Overall problem size = 600 bits
(a) (b)
Figure 3. Requirements on different BB sizes; (a) Population size and (b) Function evaluation.
2208 Y.-p. Chen et al.
D
ow
nl
oa
de
d 
by
 [N
ati
on
al 
Ch
iao
 T
un
g U
niv
ers
ity
] a
t 2
3:0
9 2
4 O
cto
be
r 2
01
2 
evolutionary algorithm (SAEA) (Sastry, Goldberg, and
Pelikan 2001; Jin 2003; Lim, Jin, Ong, and Sendhoff
2010a; Lim, Ong, Setiawan, and Idris 2010b) may be
adopted and utilised.
Another observation is that when ILI performs on
problems composed of mixed-sized BBs, the computa-
tional complexity of ILI is still in the same order.
This phenomenon indicates that detecting these more
complicated problem structures poses no particular
difficulty for ILI. Finally, the experimental results
obtained by using four different elementary functions
to construct BBs are quite similar. Thus, this series of
experiments evidentially proves that ILI behaves sim-
ilarly when handling sub-problem of different types.
0 1 2 3 4
0
1
2
3
4
Unitation
Fu
nc
tio
n 
va
lu
e
trap4
(a)
0 1 2 3 4
0
1
2
3
4
Unitation
Fu
nc
tio
n 
va
lu
e
nith4
(b)
0 1 2 3 4
0
1
2
3
4
Unitation
Fu
nc
tio
n 
va
lu
e
tmmp4
(c)
0 1 2 3 4
0
1
2
3
4
Unitation
Fu
nc
tio
n 
va
lu
e
valley4
(d)
Figure 5. Elementary functions adopted in the series of experiments in Section 4.3; (a) trap4, (b) nith4, (c) tmmp4 and (d) valley4.
0 20 40 60 80 100 120 140 160
250
300
350
400
Number of BB
(overall problem size = BB * 4)
Po
pu
la
tio
n 
siz
e
trap4
nith4
tmmp4
valley4
(a)
0 20 40 60 80 100 120 140 160
0
1
2
3
4
5
6
µ104
Number of BB
(overall problem size = BB * 4)
Fu
nc
tio
n 
ev
al
ua
tio
n
trap4
nith4
tmmp4
valley4
(b)
Figure 6. Experimental results on different 4-bits BB types: (a) required population sizes and (b) required function evaluations.
2210 Y.-p. Chen et al.
D
ow
nl
oa
de
d 
by
 [N
ati
on
al 
Ch
iao
 T
un
g U
niv
ers
ity
] a
t 2
3:0
9 2
4 O
cto
be
r 2
01
2 
Gladwin, D., Stewart, P., and Stewart, J. (2011), ‘Internal
Combustion Engine Control for Series Hybrid Electric
Vehicles by Parallel and Distributed Genetic
Programming/Multiobjective Genetic Algorithms’,
International Journal of Systems Science, 42, 249–261.
Goldberg, D.E. (2002), The Design of Innovation: Lessons
from and for Competent Genetic Algorithms, Norwell, MA,
USA, Kluwer Academic Publishers.
Goldberg, D.E., Korb, B., and Deb, K. (1989), ‘Messy
Genetic Algorithms: Motivation, Analysis, and First
Results’, Complex Systems, 3, 493–530.
Harik, G.R. (1997), ‘Learning Gene Linkage to Efficiently
Solve Problems of Bounded Difficulty Using Genetic
Algorithms’, Ph.D. Dissertation, University of Michigan,
Ann Arbor, MI, USA.
Harik, G. (1999), ‘Linkage Learning via Probabilistic
Modeling in the ECGA, IlliGAL Report No. 99010,
Illinois Genetic Algorithms Laboratory, University of
Illinois at Urbana-Champaign.
Harik, G.R., Lobo, F.G., and Goldberg, D.E. (1999), ‘The
Compact Genetic Algorithm’, IEEE Transactions on
Evolutionary Computation, 3, 287–297.
Heckendorn, R.B., and Wright, A.H. (2004), ‘Efficient
Linkage Discovery by Limited Probing’, Evolutionary
Computation, 12, 517–545.
Holland, J.H. (1992), Adaptation in Natural and Artificial
Systems, Cambridge, MA, USA: MIT Press.
Huang, Y.W., and Chen, Y.-p. (2009), ‘On the Detection of
General Problem Structures by Using Inductive Linkage
Identification’, in Proceedings of ACM SIGEVO Genetic
and Evolutionary Computation Conference 2009 (GECCO,
2009), pp. 1853–1854.
Huang, Y.W., and Chen, Y.-p. (2010), ‘Detecting General
Problem Structures with Inductive Linkage Identification’,
in Proceedings of the 2010 Conference on Technologies and
Applications of Artificial Intelligence, TAAI, pp. 508–515.
Jin, Y. (2003), ‘A Comprehensive Survey of Fitness
Approximation in Evolutionary Computation’, Soft
Computing, 9, 3–12.
Kargupta, H. (1995), SEARCH, ‘Polynomial Complexity,
and the Fast Messy Genetic Algorithm’, Technical Report,
University of Illinois.
Kargupta, H. (1996), ‘The Gene Expression Messy Genetic
Algorithm’, in Proceedings of IEEE International
Conference on Evolutionary Computation, pp. 814–819.
Larran˜aga, P., and Lozano, J.A. (2001), Estimation of
Distribution Algorithms: A New Tool for Evolutionary
Computation, Boston, MA: Kluwer Academic Publishers.
Li, S., and Li, Z.Z. (2010), ‘Spare Parts Allocation by
Improved Genetic Algorithm and Monte Carlo
Simulation’, International Journal of Systems Science,
First published on: 01 March 2010 (iFirst). DOI:
10.1080/00207720802556252.
Lim, D., Jin, Y., Ong, Y.S.O., and Sendhoff, B. (2010a),
‘Generalizing Surrogate-assisted Evolutionary
Computation’, IEEE Transactions on Evolutionary
Computation, 14, 329–355.
Lim, D., Ong, Y.S., Setiawan, R., and Idris, M. (2010b),
‘Classifier-assisted Constrained Evolutionary Optimization
for Automated Geometry Selection of Orthodontic
Retraction Spring’, in Proceedings of 2010 IEEE
Congress on Evolutionary Computation (CEC, 2010),
pp. 1449–1456.
Mu¨hlenbein, H., and Ho¨ns, R. (2005), ‘The Estimation of
Distributions and the Minimum Relative Entropy
Principle’, Evolutionary Computation, 13, 1–27.
Mu¨hlenbein, H., and Mahnig, T. (1999), ‘FDA – A Scalable
Evolutionary Algorithm for the Optimization of Additively
Decomposed Functions’, Evolutionary Computation, 7,
353–376.
Mu¨hlenbein, H., and Paaß, G. (1996), ‘From Recombination
of Genes to the Estimation of Distributions I. Binary
Parameters’, in Proceedings of the 4th International
Conference on Parallel Problem Solving from Nature
(PPSN IV), pp. 178–187.
Munetomo, M., and Goldberg, D.E. (1998), ‘Identifying
Linkage by Nonlinearity Check’, IlliGAL Report No.
98012, Illinois Genetic Algorithms Laboratory, University
of Illinois at Urbana-Champaign.
Munetomo, M., and Goldberg, D.E. (1999), ‘Identifying
Linkage Groups by Nonlinearity/Non-monotonicity
Detection’, in Proceedings of Genetic and Evolutionary
Computation Conference 1999 (GECCO-99), pp. 433–440.
Pal, N.R., Nandi, S., and Kundu, M.K. (1998), ‘Self-
crossover – A New Genetic Operator and Its Application
to Feature Selection’, International Journal of Systems
Science, 29, 207–212.
Pelikan, M., Goldberg, D.E., and Cantu´-Paz, E. (1999),
‘BOA: The Bayesian Optimization Algorithm’,
in Proceedings of Genetic and Evolutionary Computation
Conference 1999 (GECCO-99), pp. 525–532.
Pelikan, M., Goldberg, D.E., and Lobo, F.G. (2002),
‘A Survey of Optimization by Building and Using
Probabilistic Models’, Computational Optimization and
Applications, 21, 5–20.
Pelikan, M., and Mu¨hlenbein, H. (1999), ‘The Bivariate
Marginal Distribution Algorithm’, Advances in Soft
Computing – Engineering Design and Manufacturing,
521–535.
Quinlan, J.R. (1986), ‘Induction of Decision Trees’, Machine
Learning, 1, 81–106.
Saridakis, K., and Dentsoras, A. (2009), ‘Integration of
Genetic Optimisation and Neuro-fuzzy Approximation in
Parametric Engineering Design’, International Journal of
Systems Science, 40, 131–145.
Sastry, K., Goldberg, D.E., and Pelikan, M. (2001), ‘Don’t
Evaluate, Inherit’, in Proceedings of Genetic and
Evolutionary Computation Conference 2001 (GECCO-
2001), pp. 551–558.
Stonedahl, F., Rand, W., and Wilensky, U. (2008),
‘CrossNet: A Framework for Crossover with Network-
based Chromosomal Representations’, in Proceedings of
ACM SIGEVO Genetic and Evolutionary Computation
Conference 2008 (GECCO, 2008), pp. 1057–1064.
2212 Y.-p. Chen et al.
D
ow
nl
oa
de
d 
by
 [N
ati
on
al 
Ch
iao
 T
un
g U
niv
ers
ity
] a
t 2
3:0
9 2
4 O
cto
be
r 2
01
2 
This article appeared in a journal published by Elsevier. The attached
copy is furnished to the author for internal non-commercial research
and education use, including for instruction at the authors institution
and sharing with colleagues.
Other uses, including reproduction and distribution, or selling or
licensing copies, or posting to personal, institutional or third party
websites are prohibited.
In most cases authors are permitted to post their version of the
article (e.g. in Word or Tex form) to their personal website or
institutional repository. Authors requiring further information
regarding Elsevier’s archiving and manuscript policies are
encouraged to visit:
http://www.elsevier.com/copyright
Author's personal copy
1046 M.-C. Lee et al. / Future Generation Computer Systems 28 (2012) 1045–1057
not consider the data access pattern. Hence, it might lead to
inefficient data access as the users’ access pattern changes; the
strategy proposed in [24] only replicates the file most frequently
accessed in the last time period, consequently resulting in long file
transmission delays for those files with similar but low weights.
In this study, we propose an adaptive data replication
algorithm, called the Popular F ile Replicate F irst algorithm (PFRF
for short), which is developed on a star-topology data grid with
limited storage space. A star-topology data grid is a simplified tree-
topology data grid with a central cluster that connects all other
clusters. A link l between two arbitrary clusters will go through the
central cluster, and lmight comprise several routers, and physical
links. Directly evaluating the components of l is difficult since too
many analytical items might be involved. Hence, this study treats
l as a logical link to simplify the original topology as a whole [33,
34]. The simplification process will be proposed. To adapt to the
changes of users’ interests in files, the PFRF aggregates file access
information and replicates popular files to suitable clusters/sites.
We simulate several cases in which file popularity follows a Zipf-
like distribution, geometric distribution, and uniform distribution
under the assumption that user behaviors vary with the changes
of users’ interests. The simulation results show that PFRF provides
users with a system that has higher data availabilities, lower data
transmission delays, and less bandwidth consumption for data
access.
The rest of the paper is organized as follows. Section 2
introduces background and related work of this study. Section 3
describes the architecture of a star-topology data grid and the
details of the PFRF. Simulation results are presented and discussed
in Section 4. Section 5 concludes this article and addresses our
future research.
2. Background and related work
In this section, we describe the architectures of data grids and
several existing replication strategies and algorithms.
2.1. Data grid architecture
Data grids can be classified into multi-tier data grids, first
proposed by the MONARC project [35], and cluster data grids,
initially introduced by Chang et al. [32]. The multi-tier data
grid architecture in which a leaf node represents a user or a
computational node, and internal nodes are resource sites keeping
sharable files. In this architecture, a file held by a site will also
be held by all its ancestor sites. Therefore, the root site holds all
files stored in the data grid. When an end user requires a file F
which does not exist in his/her site, the user requests F from its
immediate ancestor. If the ancestor does not have the file, it in turn
requests F from its immediate ancestor. The process repeats until
the user obtains the file from a node which holds the file. After
that, the file will be replicated to all the nodes on this requesting
path following the reverse direction of the requests. It is clear that
file access latency can be reduced in a multi-tier data grid, but it
leads to higher storage cost since files will be redundantly stored
in multiple locations.
A cluster data grid consists of n clusters connected by the
Internet [24]. Files are stored in these clusters. Each cluster has
a header node (a header for short) responsible for managing site
information and exchanging file access information with other
cluster headers. A header periodically determineswhich file should
be replicated by computing file weights. After that, the file with
the highest weight will be replicated to clusters that need the file.
Sites in these clusters can then locally and quickly retrieve the file.
Compared with a multi-tier data grid, a cluster data grid consumes
less storage to hold files.
2.2. Existing data replication algorithms/strategies
Least Frequently Used (LFU) [36] and Most Frequently Used
(MFU) [36] are two simple dynamic replication strategies widely
used in many areas, such as disk and cache memory duplication. If
a storage device has insufficient space to hold a new file, LFU (MFU)
will be invoked to choose the files that have been the least (most)
frequently used as the victims tomake room for the newone. In the
experiments of this study, MFU and LFU are both involved, called
the MFU/LFU strategy (M/LFU for short) in which MFU is used to
choose themost frequently used files and LFU is employed to select
victims once the destination cluster has insufficient storage space
to save the replicated files.
Ranganathan and Foster [22] presented six replication/caching
strategies for a multi-tier data grid: No Replication or Caching,
Best Client, Cascading Replication, Plain Caching, Caching plus
Cascading Replication, and Fast Spread, and three types of
localities: temporal locality, geographical locality, and spatial
locality. The experimental results showed that the Fast Spread and
Cascading Replication outperform the other four strategies and
their file access latencies are shorter than those of the other four
strategies. They also found that Fast Spread (Cascading) is better
when the data access pattern is random (geographical locality).
However, the six strategies cannot avoid the disadvantages of a
multi-tier data grid, i.e., a file may be redundantly stored in a
multi-tier. In fact, the storage space utilization and access latency
are a trade-off [32]. Ranganathan and Foster [31] also proposed
a suite of job scheduling and data replication algorithms for a
multi-tier data grid and evaluated the performance of different
combinations of the replication and scheduling strategies. One
of the data replication algorithms, called DataRandom (DR for
short), replicates a file when the corresponding access frequency
exceeds a pre-defined threshold. Although DR is designed for an
unlimited storage environment, it can also be run on a limited
storage environment. DR is therefore involved in the experiments
of this study.
Tang et al. [23] introduced Simple Bottom-Up (SBU) and
Aggregate Bottom-Up (ABU) algorithms to reduce the average data
access response time for amulti-tier data grid. The basic idea of the
two algorithms is to replicate a file to sites close to its requesting
clients when the file’s access rate is higher than a pre-defined
threshold. SBU considers the file access history for individual site,
but ABU aggregates the file access history for a system. With ABU,
a node sends aggregated historical access records to its upper tiers,
and the upper tiers do the same until these records reach the root.
Due to the aggregation capability, ABU has a shorter job response
time and less bandwidth consumption than those of SBU.
Khanli et al. [37] proposed an algorithm called Predictive
Hierarchical Fast Spread (PHFS), which is an extended version of
fast spread [22], in a multi-tier data grid. PHFS utilizes spatial
locality [22,38] to predict data files required in the future,
and pre-replicates these files to suitable sites to improve the
performance of file accesses. Kunszt et al. [39] presented a
replicamanagement gridmiddleware to reduce file access/transfer
time. Their experimental results showed that this middleware
significantly reduces wide area transfer times. However, this
model was developed for multi-tier data grids with unlimited
storage space.
Chang et al. [24,32] presented two dynamic replication strate-
gies, Latest Access Largest Weight (LALW) [24] and Hierarchical
Replication Strategy (HRS) [32], on cluster-based data grids. LALW
utilizes the half-life concept to evaluate file weights. A file with a
higher access frequency has a larger weight. Their experimental
results show that LALW outperforms LFU and no-replication data
replication strategies [22] in network utilization and efficiency.
However, LALW only replicates the most popular file in each time
Author's personal copy
1048 M.-C. Lee et al. / Future Generation Computer Systems 28 (2012) 1045–1057
Fig. 3. PFRF data replication algorithm.
file will be accessed more frequently than unpopular ones [23].
Breslau et al. [28] showed that webpage requests follow a Zipf-like
distribution [29,41] derived from Zipf’s law [42]. In the Zipf-like
distribution, the access probability of the i-th most popular file,
denoted by P(fi), is
P(fi) = 1/iα (1)
where i = 1, 2, . . . , n and α is a factor determining the file access
distribution, 0 ≤ α < 1.
Ranganathan and Foster [30,31] adopted the geometric distri-
bution to simulate file popularity in which the access probability
of the i-th most popular file, denoted by P(i), is
P(i) = (1− p)i−1 · p (2)
where i = 1, 2, . . . , n and 0 < p < 1. A larger p represents that
a smaller portion of files has been frequently accessed. As stated
above, we assume that our users’ access behaviors follow either
Zipf-like or geometric distributions with different parameters.
3.4. Popular file replicate first (PFRF) algorithm
The PFRF algorithm, as illustrated in Fig. 3, is performed by the
GRC at the end of a round, where a round is a fixed time period Td
in which y jobs, y ≥ 0, are submitted by users from each cluster.
A job might require several files as its input data. The algorithm
comprises four phases: file access aggregate phase, file popularity
calculation phase, file selection phase, and file replication phase.
1. File access aggregate phase: Between lines 2 and 5 of the
algorithm, PFRF aggregates the access count for each file fi
stored in cluster c at round r , denoted by Arc(fi), sorts all the
files onArc(fi)s in a descending order, and stores the sorted result
into a set S. After that, PFRF calculates the total number of files
having been accessed by all sites in cluster c at round r , denoted
by TNF rc , based on the information stored in LRT C . Note that
1 ≤ i ≤ Nk, and 1 ≤ c ≤ Nc where Nk is the number of files
in cluster c in round r , and Nc is the number of clusters that the
data grid has, and r = 1, 2, 3, . . ..
2. File popularity calculation phase: In line 6, PFRF calculates a
popularity weight for file fi, denoted by PW rc(fi),
PW rc(fi) =

PW r−1c (fi)+ Arc(fi) · a, if Arc(fi) > 0
PW r−1c (fi)− b, otherwise ,
r ≥ 1, c ≥ 1, i ≥ 1. (3)
where a and b are constants and a < b. The reason why a < b
is described later. If Arc(fi) > 0, i.e., fi has been accessed by users
in round r , PFRF increases PW r−1c (fi) by Arc(fi) · a. Otherwise,
it decreases PW r−1c (fi) by b. Basically, a higher PW
r
c(fi) implies
that fi ismore popular.We assume that in round 0 all files follow
the binomial distribution, i.e., PW 0c (fi) =0.5, which means that
the initial access probability of fi is 0.5. Note that the minimum
value of each PW r−1c (fi) is 0. From previous access records of fi,
PFRF derives the variation of the popularity of fi and predicts
the popularity of fi for the next round, where 1 ≤ i ≤ Nk. For
instance, if f3 has been accessed 5 times by cluster 2 in round
1, PW 12(f3) = 0.5 + 5 · a. After the derivation and prediction,
PFRF calculates the average popularity of the files in all clusters,
denoted by PW ravg(fi),
PW ravg(fi) =
Nq
k=1
PW rc(fi)
Nq
(4)
where Nq is the total number of clusters holding fi in the data
grid.
3. File selection phase: Between lines 7 and 10, PFRF sorts the set S
on the average popularweights in a decreasing order, calculates
Nf which is the number of files that might be replicated, and
Author's personal copy
1050 M.-C. Lee et al. / Future Generation Computer Systems 28 (2012) 1045–1057
Table 1
Resources specifications of the following experiments.
Resources Value
Total number of clusters 8
Total number of processors in a cluster 24
Single processor rate (MIPS) 1600
Total processor rate of a cluster 38,400
(=1600× 24)
Storage available in a cluster 75 GB
Inter-router bandwidth 10 Gb/s
Router-to-site bandwidth 2.5 Gb/s
User-to-router bandwidth 100 Mb/s
GRC-to-router bandwidth 2.5 Gb/s
LRC-to-router bandwidth 1 Gb/s
Table 2
Job parameters of the following experiments.
Job parameters Value
Total number of master files 200
Size of a master file 1 GB
Average number of jobs submitted by each cluster in a
round
10
Average number of jobs submitted by all clusters in each
round
80
(8× 10 jobs)
Number of files accessed by a job 5–10
The duration of a round (Td) 1600 s
higher access count than mk in round r, F will be replicated to c
at the end of r, 1 ≤ c ≤ 8. The other is DR-Global, in which the
replication threshold is set to the average access counts of all files
in all clusters, i.e., if the 200 files have been accessed h times in
round r , the replication threshold will be h200 for all clusters. When
F has been accessed at least h200 times by c in round r , it will be
replicated to c. If c has insufficient space to hold the file, DR-Local
and DR-Global will delete the files that have been least frequently
accessed from c to make room for F .
Two types of NR were also implemented, denoted by NR-GRC
and NR-LRC. In the NR-GRC, the 200 master files are all stored in
the GRC. When a job requires a file, it remotely accesses the file
from the GRC without storing the file locally. In the NR-LRC, the
200master files are randomly distributed to the eight clusters. The
GRC only maintains the GRT. A job locally accesses a file if the file
is locally available. When it requires a remote file, it has to consult
the GRC for the file location, and then remotely access the file. Due
to duplicating no files, files are only stored in fixed sites and fixed
clusters. Table 3 summarizes the master file settings for the tested
algorithms.
4.2. Access patterns
Five access patterns listed in Table 4were employed to simulate
user access behaviors. File popularities follow Zipf-like (ZipfL
for short), geometric (Geo for short), and uniform distributions
(Uniform for short) where a uniform distribution represents that
the probability of accessing a file by each user is the same. JRR,
standing for job repeating rate, of round r is the probability of re-
accessing those files that have been accessed in round r − 1, 0 ≤
Table 4
Different data access patterns employed.
No. Data access pattern α/p JRR (%) p(fi)/p(i)
1 ZipfL-0.8 0.8 25 1/i0.8
2 ZipfL-0.6 0.6 25 1/i0.6
3 Geo-0.2 0.2 25 (1−0.2)i−1 ·0.2
4 Geo-0.5 0.5 25 (1−0.5)i−1 ·0.5
5 Uniform None 25 None
JRR ≤ 1, and parameters α and p are respectively used when ZipfL
and Geo are employed.
To effectively analyze the algorithms, we evenly partitioned
the popularities of the 200 master files into 10 levels and divided
twenty consecutive rounds into three phases. As listed in Table 5,
the first, the second, and the third phase respectively contain
rounds 1–7, 8–14, and 15–20. In the first phase, we assume
that File0–File19 are the most popular files, i.e., belonging to
the first popularity level. File20–File39 are the second popular
files, thus belonging to the second popularity level, and so on. In
the second phase, we swap the files of the first two popularity
levels, i.e., File20–File39 become the most popular, File0–File19
become the second, to simulate the change of file popularities,
and other files’ popularities remain unchanged. In the third phase,
File40–File59 are the most popular files, File20–File39 the second,
and File0–File19 the third. Other levels’ file popularities remain
unchanged.
To better understand the behaviors of data access patterns,
i.e., file popularities, over the three phases, we first conduct
the following experiments: 2000 jobs, instead of 80 jobs, were
submitted for file accesses in each phase. The experimental
results of ZipfL-0.8, ZipfL-0.6, Geo-0.2, Geo-0.5, and Uniform
are illustrated in Figs. 5–9, respectively. Fig. 5(a) shows users’
file access behaviors in the first phase; Fig. 5(b) and Fig. 5(c)
respectively plot those in the second and the third phases. The
access count (AC) of each most popular file in all the three
phases/figures is about 215, and unpopular files, i.e., File60 to
File199, are accessed less. In the case of ZipfL-0.6 (see Fig. 6), the
AC of each most popular file in all the three phases is about 170.
However, the ACs of the other popularity levels, i.e., between levels
4 and 10, are not evidently different, like a uniform distribution,
in all three phases. When Geo-0.2 is invoked (see Fig. 7), the AC of
eachmost popular file is about 175.When file IDs increase, the ACs
decline more sharply than those in ZipfL-0.6 and ZipfL-0.8. In the
Geo-0.5 case (see Fig. 8), the difference between/among the most
popular files’ ACs and those of the second and the third popular
files in the three phases is significant. Generally, the ACs of the first
three popularity levels on all access patterns are clearly different
from those of the other popularity levels, implying that File0 to
File59 are frequently accessed, while accesses of File140 to File199
are rare. Thedifference among theACs of different popularity levels
on the Uniform as shown in Fig. 9 is insignificant.
4.3. Simulation results
The tested algorithms were run on the same experimental
environment so their performance can be fairly compared. Several
Table 3
Master files settings for PFRF, M/LFU, DR-Local, DR-Global, NR-GRC, and NR-LRC algorithms.
No. Data replication algorithm Setting
1 PFRF 200 master files are randomly distributed to the eight clusters
2 M/LFU ’’
3 DR-Local ’’
4 DR-Global ’’
5 NR-GRC 200 master files are all stored in the GRC
6 NR-LRC 200 master files are randomly distributed to the eight clusters
Author's personal copy
1052 M.-C. Lee et al. / Future Generation Computer Systems 28 (2012) 1045–1057
0
0
20
40
60
Ac
ce
ss
 C
ou
nt
s
80
100
120
20 40 60 80 100
File ID
120 140 160 180 199 0
0
20
40
60
Ac
ce
ss
 C
ou
nt
s
80
100
120
20 40 60 80 100
File ID
120 140 160 180 199 0
0
20
40
60
Ac
ce
ss
 C
ou
nt
s
80
100
120
20 40 60 80 100
File ID
120 140 160 180 199
(a) Phase 1. (b) Phase 2. (c) Phase 3.
Fig. 9. Distributions of file requests against the 200 master files in the simulation process on Uniform.
data availability (ADA). Data availability, which was proposed by
GridSim [43] for a job, e.g., jobx, in cluster c to access a file fi of size
dxi , denoted by Availx,c , is defined as
Availx,c =
k
i=1
txi,c
k
i=1
dxi
(6)
where k is the number of files accessed by jobx running on cluster
c in round r , and txi,c is the time that jobx consumes to acquire fi,
locally or remotely. Let avgAvailc be the average data availability of
cluster c which is defined as
avgAvailc =

x∈jobsc
Availx,c
|jobsc |
(7)
where jobsc is the set of jobs submitted to cluster c by users to
access files. The ADA of all clusters is defined as
ADA =
Nc
m=1
avgAvailm
Nc
(8)
where Nc is the number of clusters in the data grid. The last is
average bandwidth cost ratio (ABCR). Bandwidth cost ratio of cluster
c in a round, denoted by BCRc , is defined as
BCRc = LFAc · LC c + RFAc · RC cAFAc · Cbaseline
= LFAc · LC c + RFAc · RC c
(LFAc + RFAc) · Cbaseline (9)
where LFAc(RFAc) is the number of files that users in cluster c can
locally (should remotely) access,AFAc = LFAc+RFAc, LC c is the cost
for a user in cluster c to locally access a file. RC c is the cost for the
user to access a file from a remote cluster or the GRC, and Cbaseline is
the average cost for a user to access a file from a remote cluster to
local cluster c. If LFAc is larger than RFAc , that implies the particular
data replication algorithm can more accurately predict user access
behaviors. Otherwise, the algorithm due to inaccurate prediction
would consume a lot of network resources to remotely access files.
Eq. (9) only involves the number of files and neglects file sizes
since in the simulation all files are of the same size, i.e., 1 GB. The
ABCRused to determinewhether a data replication algorithmcould
accurately predict popular files or not is defined as
ABCR =
Nc
m=1
BCRm
Nc
(10)
where Nc is the total number of clusters in the data grid. A data
replication algorithm with a smaller ABCR value will lead to better
grid performance since most data can be locally retrieved. In the
following, each simulation was performed ten times to obtain the
values of the three performance metrics.
Fig. 10. Average job turnaround times for PFRF, M/LFU, DR-Local, DR-Global, and
NR-LRC on ZipfL-0.8 with JRR= 25%.
Fig. 11. Average job turnaround times for PFRF, M/LFU, DR-Local, DR-Global, and
NR-LRC on ZipfL-0.6 with JRR= 25%.
4.3.1. Average job turnaround time (ATT) and average data availabil-
ity (ADA)
Fig. 10 show the experimental results of ATTs for PFRF, M/LFU,
DR-Local, DR-Global, and NR-LRC on access pattern ZipfL-0.8 with
JRR= 25%. The results of ZipfL-0.6, Geo-0.2, Geo-0.5, and Uniform
are illustrated in Figs. 11–14, respectively. When ZipfL-0.8 with
JRR= 25% is employed, as shown in Fig. 10, PFRF’s ATTs are shorter
than those ofM/LFU, DR-Local, andDR-Global after the sixth round.
This is also true on ZipfL-0.6 and Geo-0.2 (see Figs. 11 and 12,
respectively). Fig. 13 plots the experimental results of Geo-0.5. On
Uniform with JRR= 25% as shown in Fig. 14, ATTs of PFRF, M/LFU,
DR-Local, and DR-Global are longer than those shown in Figs. 10–
13 since the tested algorithms cannot effectively discriminate file
popularity. It is clear that PFRF has shorter ATTs than those of
M/LFU, DR-Local, and DR-Global on all of ZipfL-0.8, ZipfL-0.6, and
Geo-0.2.
Author's personal copy
1054 M.-C. Lee et al. / Future Generation Computer Systems 28 (2012) 1045–1057
Fig. 16. Average data availabilities for PFRF, M/LFU, DR-Local, DR-Global, and NR-
LRC on ZipfL-0.8 with JRR= 25%.
Fig. 16 illustrates the ADAs for PFRF, M/LFU, DR-Local, DR-
Global, and NR-LRC on ZipfL-0.8 with JRR= 25%. According to the
definition of Availx,c presented in Eq. (6), the numerator txi,c is the
turnaround time of jobx in cluster c , and hence the ADAs of all the
tested algorithms have similar trends to that of their ATTs on all
data access patterns. For example, all curves in Fig. 10 are similar
to those in Fig. 16 on ZipfL-0.8 with JRR= 25%. Note that the ADAs
of NR-GRC in each round are also omitted from Fig. 16 because they
are the worst (about 0.637 which is far above the top scale, 0.23, of
Fig. 16). The cause of these high ADAs was mentioned above.
4.3.2. Average bandwidth cost ratio (ABCR)
Fig. 17 illustrates the ABCRs of PFRF, M/LFU, DR-Local, DR-
Global, NR-GRC, and NR-LRC on ZipfL-0.8 with JRR = 25%. The
bandwidths of the inter-router link, router-to-site link, user-
to-router link, and GRC-to-router link as listed in Table 1 are
respectively 10, 2.5, 0.1, and 2.5 Gb/s, and the corresponding
unit costs are respectively 110 ,
1
2.5 ,
1
0.1 , and
1
2.5 . With the tested
algorithms other than NR-GRC, files required by a job may be
stored in remote clusters or a local cluster. To fairly compare all
these algorithms, the router-to-site link andGRC-to-router link are
given the same bandwidth, i.e., 2.5 Gb/s.
With NR-GRC, LC c and LFAc in Eq. (9) are zero since all master
files are located at the GRC, i.e., AFAc = RFAc, RC c = 12.5 + 110 +
1
0.1 = 10.5, and Cbaseline = 12.5+ 110+ 110+ 10.1 = 10.6. Thus, BCRc =
RCc
Cbaseline
= 0.99, ABCR = BCRc , and ABCR = 0.99 in all rounds on all
access patterns. For PFRF,M/LFU, DR-Local, DR-Global, andNR-LRC,
RC c = Cbaseline = 10.6, and LC c = 12.5 + 10.1 = 10.4.
Comparing the plots shown in Figs. 17–21, ABCRs of NR-LRC
in the five figures are all the worst, between 0.997 and 0.998.
The reason is that NR-LRC does not replicate files among clusters;
hence, each cluster has to access required files from remote
clusters, even though it has frequently accessed these files. PFRF
can effectively adjust file weights and lead to the best ABCRs on
ZipfL-0.8, ZipfL-0.6, and Geo-0.2 after the sixth round, as compared
with all the other algorithms. However,ABCRs of NR-GRC are better
than those of other algorithms on ZipfL-0.6 (see Fig. 18) since all
unpopular files have similar access count (see Fig. 6), like those
of a uniform distribution. As shown in Fig. 20, PFRF, M/LFU, and
DR-Global on Geo-0.5 have similar ABCRs, which are better than
those of DR-Local, NR-GRC, and NR-LRC since the former three
algorithms can effectively identify popular files and replicate them
to the clusters requiring these files.
As shown in Fig. 21, NR-GRC on Uniform with JRR = 25%
outperforms all the other algorithms since all required files can
be accessed from the GRC rather than from remote clusters. On
the other hand, ABCRs of PFRF, M/LFU, DR-Local, and DR-Global
Fig. 17. Average bandwidth cost ratios for PFRF, M/LFU, DR-Local, DR-Global, NR-
GRC, and NR-LRC on ZipfL-0.8 with JRR= 25%.
Fig. 18. Average bandwidth cost ratios for PFRF, M/LFU, DR-Local, DR-Global, NR-
GRC, and NR-LRC on ZipfL-0.6 with JRR= 25%.
Fig. 19. Average bandwidth cost ratios for PFRF, M/LFU, DR-Local, DR-Global, NR-
GRC, and NR-LRC on Geo-0.2 with JRR= 25%.
are similar because popularities of all files as shown in Fig. 9 are
similar and remain unchanged over time. According to Eqs. (9)
and (10), the increase of RFAcs will result in higher BCRcs and
ABCRs, indicating that the bandwidths consumed by all algorithms
except NR-GRC on Uniform, of which ABCR is about 0.993 after
the sixth (see Fig. 21), are higher than those on the other access
patterns. Please compare theABCR value 0.993with those shown in
Figs. 17–20, all between 0.983 and 0.992.
If we change the bandwidth of the user-to-router links shown
Fig. 4 from 100 Mb/s to 1 Gb/s, when NR-GRC is employed, RC c =
1.5, Cbaseline = 1.6, and BCRc = ABCR = 0.937 which is lower
Author's personal copy
1056 M.-C. Lee et al. / Future Generation Computer Systems 28 (2012) 1045–1057
access behaviors over time. Therefore, PFRF is designed to improve
these weaknesses. It can effectively adapt to the changes of users’
interests by dynamically adjusting file weights and replicating
these files to appropriate clusters to improve performance of the
whole system. We also analyze the average job turnaround time,
average data availability, and average bandwidth cost ratio as the
performance metrics of PFRF and compare them with those of five
existing algorithms on five data access patterns. The simulation
results show that PFRF outperforms all the tested algorithmswhen
file popularity changes with time.
In the future, we plan to validate our simulation results on real
data grids so that the proposed scheme can be evaluated on a
real testbed. We would also like to enhance the reliability of the
architecture by providing a hot-standby GRC like that presented
in [49] to take over as the GRCwhen theGRC cannotwork properly.
Wewill also try to replicate popular files to users’ local sites, rather
than to users’ local clusters. This can further reduce intra-cluster
bandwidth consumption and unnecessary data transmission time.
Finally, we plan to develop a reliability model to evaluate how
many replicas are required for a file such that the file can stand
against site failures. Those constitute our future studies.
Acknowledgments
Theworkwas supported in part by the National Science Council
of Taiwan under Grants NSC 99-2221-E-009-123-MY2 and NSC
100-2221-E-029-018.
The authors would like to thank the anonymous referees for
their helpful comments that improved the quality of this paper.
References
[1] A. Folling, C. Grimme, J. Lepping, A. Papaspyrou, Robust load delegation
in service grid environments, IEEE Transactions on Parallel and Distributed
Systems 21 (9) (2010) 1304–1316.
[2] O. Sonmez, H. Mohamed, D. Epema, On the benefit of processor coallocation
in multicluster grid systems, IEEE Transactions on Parallel and Distributed
Systems 21 (6) (2010) 778–789.
[3] H. Li, Realistic workload modeling and its performance impacts in large-scale
escience grids, IEEE Transactions on Parallel and Distributed Systems 21 (4)
(2010) 480–493.
[4] H.Mohamed, D. Epema, Koala: a co-allocating grid scheduler, Concurrency and
Computation: Practice and Experience 20 (16) (2008) 1851–1876.
[5] B. Tierney, W. Johnston, J. Lee, M. Thompson, A data intensive distributed
computing architecture for grid applications, Future Generation Computer
Systems 16 (5) (2000) 473–481.
[6] BIRN. http://www.nbrin.net/.
[7] LHC accelerator project. http://www-td.fnal.gov/LHC/USLHC.html.
[8] European DataGrid Project (EDG). http://www.eu-egee.org.
[9] GriPhyN: The Grid physics network project, 12 July 2010.
http://www.griphyn.org.
[10] PPDG. http://www.ppdg.net.
[11] R.S. Chang, M.S. Hu, A resource discovery tree using bitmap for grids, Future
Generation Computer Systems 26 (1) (2010) 29–37.
[12] J. Wu, X. Xu, P. Zhang, C. Liu, A novel multi-agent reinforcement learning
approach for job scheduling in grid computing, Future Generation Computer
Systems 27 (5) (2011) 430–439.
[13] S. Ebadi, L.M. Khanli, A newdistributed and hierarchicalmechanism for service
discovery in a grid environment, Future Generation Computer Systems 27 (6)
(2011) 836–842.
[14] M.E.J. Newman, Power laws, Pareto distributions and Zipf’s law, Contemporary
Physics 46 (2005) 323–351.
[15] K. Sashi, A.S. Thanamani, Dynamic replication in a data grid using a modified
BHR region based algorithm, Future Generation Computer Systems 27 (2)
(2011) 202–210.
[16] M. Lei, S.V. Vrbsky, X. Hong, An on-line replication strategy to increase
availability in data grids, Future Generation Computer Systems 24 (2) (2008)
85–98.
[17] O. Wolfson, S. Jajodia, Y. Huang, An adaptive data replication algorithm, ACM
Transactions on Database Systems 22 (2) (1997) 255–314.
[18] M. Rabinovich, I. Rabinovich, R. Rajaraman, Dynamic replication on the
internet, Technical Report, HA6177000–980305-01-TM, AT&T Labs, March
1998.
[19] J.M. Perez, F. Garcia-Carballeira, J. Carretero, A. Calderon, J. Fernandez, Branch
replication scheme: a newmodel for data replication in large scale data grids,
Future Generation Computer Systems 26 (1) (2010) 12–20.
[20] M. Vrable, S. Savage, G.M. Voelker, Cumulus: filesystem backup to the cloud,
ACM Transactions on Storage 5 (4) (2009).
[21] H. Shen, An efficient and adaptive decentralized file replication algorithm in
P2P file sharing systems, IEEE Transactions on Parallel andDistributed Systems
21 (6) (2010) 827–840.
[22] K. Ranganathan, I. Foster, Identifying dynamic replication strategies for a high
performance data grid, in: Proceedings of the Second International Workshop
on Grid Computing, Denver, CO, November 2001, pp. 75–86.
[23] M. Tang, B.S. Lee, C.K. Yeo, X. Tang, Dynamic replication algorithms for
the multi-tier data grid, Future Generation Computer Systems 21 (2005)
775–790.
[24] R.S. Chang, H.P. Chang, A dynamic data replication strategy using access-
weights in data grids, Journal of Supercomputing 45 (3) (2008) 277–295.
[25] S.Y. Ko, R. Morales, I. Gupta, New worker-centric scheduling strategies for
data-intensive grid applications, in: Proc. ACM/IFIP/USENIX Int’l Conference
on Middleware, 2007, pp. 121–142.
[26] L. Meyer, J. Annis, M. Wilde, M. Mattoso, I. Foster, Planning spatial workflows
to optimize grid performance, in: Proc. ACM Symp. Applied Computing, 2006,
pp. 786–790.
[27] S.J. Pan, Q. Yang, A survey on transfer learning, IEEE Transactions onKnowledge
and Data Engineering 22 (10) (2010).
[28] L. Breslau, P. Cao, L. Fan, G. Phillips, S. Shenker, Web caching and Zipf-like
distributions: evidence and implications, in: Proceedings of IEEE INFOCOM’99,
no.1, New York, USA, ppp. 126–134, 21–25 March 1999.
[29] D.G. Cameron, R. Carvajal-Schiaffino, A. PaulMillar, C. Nicholson, K. Stockinger,
F. Zini, Evaluating scheduling and replica optimisation strategies in optorsim,
in: The International Workshop on Grid Computing, Phoenix, Arizona,
November 17, IEEE Computer Society Press, 2003.
[30] K. Ranganathan, I. Foster, Decoupling computation and data scheduling in
distributed data intensive applications, in: International Symposium for High
Performance Distributed Computing, HPDC-11, Edinburgh, 2002.
[31] K. Ranganathan, I. Foster, Simulation studies of computation and data
scheduling algorithms for data grids, Journal of Grid Computing 1 (2003)
53–62.
[32] R.S. Chang, J.S. Chang, S.Y. Lin, Job scheduling anddata replication ondata grids,
Future Generation Computer Systems 23 (7) (2007) 846–860.
[33] M. Coates, A. Hero, R. Nowak, B. Yu, Internet tomography, IEEE Signal
Processing Magazine 19 (3) (2002).
[34] G. Levitin, Y.S. Dai, B.H. Hanoch, Reliability and performance of star
topology grid service with precedence constraints on subtask execution, IEEE
Transactions on Reliability 55 (3) (2006) 507–515.
[35] The MONARC project. http://monarc.web.cern.ch/MONARC/.
[36] A. Silberschatz, P.B. Galvin, G. Gagne, Operating System Concepts, 7th ed.,
Wiley, 2004.
[37] L.M. Khanli, A. Isazadeh, T.N. Shishavan, PHFS: a dynamic replication method,
to decrease access latency in the multi-tier data grid, Future Generation
Computer Systems 27 (3) (2011) 233–244.
[38] M.L. Yiu, H. Lu, N. Mamoulis, M. Vaitis, Ranking spatial data by quality
preferences, IEEE Transactions on Knowledge and Data Engineering 23 (3)
(2011).
[39] P. Kunszt, E. Laure, H. Stockinger, K. Stockinger, File-based replica manage-
ment, Future Generation Computer Systems 21 (2005) 115–123.
[40] S.M. Park, J.H. Kim, Y.B. Ko, W.S. Yoon, Dynamic Data Grid Replication Strategy
Based on Internet Hierarchy, in: Lecture Notes in Computer Science, vol. 3033,
2004, pp. 838–846.
[41] D.G. Cameron, R.C. Schiaffino, J. Ferguson, P. Millar, C. Nicholson, K.
Stockinger, F. Zini, OptorSim v2.0 installation and user guide, November 2004.
http://edgwp2.web.cern.ch/edg-wp2/optimization/optorsim.html.
[42] G. Kingsley Zipf, Relative frequency as a determinant of phonetic change,
Reprinted from the Harvard Studies in Classical Philiology, Volume XL, 1929.
[43] A. Sulistio, U. Cibej, S. Venugopal, B. Robic, R. Buyya, A toolkit for modelling
and simulating data grids: an extension to gridsim, in: Concurrency &
Computation: Practice and Experience, Wiley Press, New York, USA, 2008.
[44] H.J. Song, X. Liu, D. Jakobsen, R. Bhagwan, X. Zhang, K. Taura, A. Chien, The
microgrid: a scientific tool for modeling computational grids, in: Proc. of IEEE
Supercomputing Conference, Dallas, USA, November 4–10 2000.
[45] W. Bell, D. Cameron, L. Capozza, P. Millar, K. Stockinger, F. Zini, Simulation
of dynamic grid replication strategies in optorsim, in: Proc. of the 3rd
International Workshop on Grid Computing, GRID, Baltimore, USA, 18
November 2002.
Hindawi Publishing Corporation
Advances in Artificial Intelligence
Volume 2011, Article ID 204750, 7 pages
doi:10.1155/2011/204750
Research Article
Convergence Time Analysis of Particle Swarm Optimization
Based on Particle Interaction
Chao-Hong Chen and Ying-ping Chen
Department of Computer Science, National Chiao Tung University, HsinChu 300, Taiwan
Correspondence should be addressed to Ying-ping Chen, ypchen@cs.nctu.edu.tw
Received 8 August 2011; Revised 7 December 2011; Accepted 21 December 2011
Academic Editor: Peter Tino
Copyright © 2011 C.-H. Chen and Y.-p. Chen. This is an open access article distributed under the Creative Commons Attribution
License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly
cited.
We analyze the convergence time of particle swarm optimization (PSO) on the facet of particle interaction. We firstly introduce
a statistical interpretation of social-only PSO in order to capture the essence of particle interaction, which is one of the key
mechanisms of PSO. We then use the statistical model to obtain theoretical results on the convergence time. Since the theoretical
analysis is conducted on the social-only model of PSO, instead of on common models in practice, to verify the validity of our
results, numerical experiments are executed on benchmark functions with a regular PSO program.
1. Introduction
Particle swarm optimizer (PSO), introduced by [1, 2], is a
stochastic population-based algorithm for solving continu-
ous optimization problems. As shown by [3] and by lots
of real-world applications, PSO is an eﬃcient and eﬀective
optimization framework. Although PSO has been widely
applied in many fields [4–7], understanding of PSO from
the theoretical point of view is still quite limited. Most of
previous theoretical results [8–18] are derived under the sys-
tem that assumes a fixed attractor or a swarm consisting of a
single particle.
Due to the lack of theoretical analysis on PSO particle
interaction, in this paper, we will make an attempt to analyze
the convergence time for PSO on the facet of particle
interaction. In particular, we will firstly introduce a statistical
interpretation of PSO, proposed by [19], to capture the
essence of particle interaction. We will then analyze the
convergence time based on the statistical model. Finally,
numerical experiments will be conducted to confirm the
validity of our theoretical results obtained on simplified PSO,
the social-only model, in a normal PSO configuration.
In the next section, we will briefly introduce the algo-
rithm of PSO and the statistical interpretation of social-
only PSO. In Section 3, we will analyze the convergence time
of PSO based on the statistical model. The experimental
results are presented in Section 4, followed by Section 5
which concludes this work.
2. Particle Swarm Optimization and the
Statistical Interpretation
The social-only model of PSO can be described as pseu-
docode shown in Algorithm 1. In this paper, we will use bold-
face for vectors, for example, Xi, Vi. Without loss of gener-
ality, we assume that the goal is to minimize the objective
function.
According to Algorithm 1, in the beginning, m particles
are initialized, where m is the swarm size, an algorithmic
parameter of PSO. Each particle contains three types of
information: its location (Xi), velocity (Vi), and personal
best position (Pbi). At each generation, each particle updates
its personal best position (Pbi) and neighborhood best
position (Nb) according to its objective value. After updating
the personal and neighborhood best positions, each particle
updates the velocity according to Pbi and Nb. In the velocity
update formula, w is the weight of inertia which is usually
a constant. Cp and Cn are random values sampled from
uniform distributions U(0, cp) and U(0, cn), where cp and
cn are called acceleration coeﬃcients. Finally, each particle
updates its position according to the velocity and then goes
to next generation.
Advances in Artificial Intelligence 3
procedure Statistical interpretation of social-only PSO (Objective
Function F : Rn → R)
Initialize: σ ← σ0, μ← μ0
while the stopping criterion is not satisfied do
for i = 1, 2, . . . ,m do
for j = 1, 2, . . . ,n do
Pi j ∼ N(μj , σ2j )
end for
end for
Pa = minPi{F (Pi)}
for i = 1, 2, . . . ,m do
for j = 1, 2, . . . ,m do
P′i j ← Pi j + C(Pa j − Pi j)
end for
end for
μ← (∑mi=1P′i)/m
σ2 ←MLE(P′1,P′2, . . . ,P′m)
end while
end procedure
Algorithm 2: Statistical model of social-only PSO. Distribution θ is represented by μ = (μ1,μ2, . . . ,μn) and σ = (σ1, σ2, . . . , σn). Acceleration
coeﬃcient C ∼ U(0, c).
To find σt+12i that maximizes L(σt
2
i ), we diﬀerentiate L(σt
2
i )
with respect to σt+12i :
L′
(
σt
2
i
) = −
(
m
2
)(
1√
2π
)m
σt+1
−m−2
i
× exp
⎛
⎜⎝
−∑mj=1
(
y′j − y
)2
2σt+12i
⎞
⎟⎠
+
(
1√
2π
)m ∑m
j=1
(
y′j − y
)2
2
σt+1
−m−4
i
× exp
⎛
⎜⎝
−∑mj=1
(
y′j − y
)2
2σt+12i
⎞
⎟⎠,
(4)
the value of σt+12i that maximizes L(σt
2
i ) is
∑m
j=1(y
′
j − y)2/m.
As a result, in our model of PSO, the results of MLE is σt+12i =∑m
j=1(y
′
j − y)2/m for i = 1, 2, . . . ,n.
3. Convergence Time Analysis
In this section, we will analyze the PSO convergence time
based on the aforementioned statistical interpretation of the
social-only model. As the first step, we must define the state
of convergence. Since, in this work, we regard the entire
swarm as a distribution, the state of convergence is then
referred to as the variance of the distribution. We define the
state of convergence as the variance for every dimension is
less than a given value  > 0. By using this definition, we can
now start our analysis of PSO convergence time. To estimate
the variance after distribution update, we need the following
lemma from [21].
Lemma 1. Let X1,X2, . . . ,Xm ∼ N(μ, σ2). Define S =∑m
i=1(Xi − X)2/(m − 1), where X =
∑m
i=1Xi/m. One has
(m − 1)S ∼ σ2χ2m−1, where χ2m−1 is the chi-square distribution
with m− 1 degrees of freedom.
With this lemma, we can obtain the following.
Lemma 2. Given the swarm size m, acceleration coeﬃcient c,
and variance of the ith dimension at the tth generation σt2i , one
has E[σt+12i ] = [(1/3)c2 − c + 1][(m− 1)/m]σt2i .
Proof. We know σt+12i =
∑m
j=1(y
′
j−y)2/m. The expected value
is
E
⎡
⎣ 1
m
m∑
j=1
(
y′j −
∑m
k=1y
′
k
m
)2⎤
⎦
= 1
m
E
⎡
⎣
m∑
j=1
(
yj + C
(
ya − yj
)
−
∑m
k=1yk + C
(
ya − yk
)
m
)2⎤
⎦
= 1
m
E
⎡
⎣
m∑
j=1
(
m(1− C)yj − (1− C)
∑m
k=1yk
m
)2⎤
⎦
= 1
m3
E
⎡
⎢⎣(1− C)2
m∑
j=1
⎛
⎝myj −
m∑
k=1
yk
⎞
⎠
2
⎤
⎥⎦
= 1
m
E
[
(1− C)2
]
E
⎡
⎢⎣
m∑
j=1
⎛
⎝yj − 1
m
m∑
k=1
yk
⎞
⎠
2
⎤
⎥⎦.
(5)
Let S = ∑mj=1(yj − (1/m)
∑m
k=1yk)
2. Since yj ∼ N(μt+1i,
σt+1
2
i ) for j = 1, 2, . . . ,m and y1, y2, . . . , ym are i.i.d.,
Advances in Artificial Intelligence 5
10−1010−910−810−710−610−510−410−310−210−1
40
60
80
100
120
140
160
Sphere function
−4.6 log() + 43
Figure 1: Comparison of experimental results and theoretical re-
sults from Corollary 5 of f1(x). The x-axis represents the value of
, and y-axis represents the mean number of generation. The ex-
perimental results are very close to O(− log ).
10−1010−910−810−710−610−510−410−310−210−1
40
60
80
100
120
140
160
Schwefels´ problem 1.2
−4.7 log() + 43.5
Figure 2: Comparison of experimental results and theoretical
results from Corollary 5 of f2(x). The x-axis represents the value
of , and y-axis represents the mean number of generation. The
experimental results are very close to O(− log ).
from initialization to the state in which variances for all
dimensions are smaller than , and we calculate the mean
number of generations for the 100 runs.
The comparison of these experimental results and our
theoretical results is shown in Figures 1 and 2. From Figure 1,
we can see that the experimental results of f1(x) are very
close to −4.6 log  + 43 = O(− log ), and from Figure 2, the
experimental results of f2(x) are very close to −4.7 log  +
43.5 = O(− log ). The experimental results agree with our
estimation in Corollary 5, in which the value of − ln  and
the PSO convergence time are linearly related.
Sphere function
−64.9/ log(0.555(1− 1/m))
50 100 150 200
100
105
110
115
120
Figure 3: Comparison of experimental results and theoretical
results from Corollary 6 of f1(x). x-axis represents the swarm
size ranging from 50 to 200, and y-axis represents the mean
number of generation. The experimental results are very close to
O(−1/ log c′(1− 1/m)) with c′ < 1.
Sphere function
−64.9/ log(0.555(1− 1/m))
90
100
110
120
130
50100 200 300 400 500 600 700 800 900 1000
Figure 4: Comparison of experimental results and theoretical
results from Corollary 6 of f1(x). x-axis represents the swarm
size ranging from 50 to 1000, and y-axis represents the mean
number of generation. The experimental results are very close to
O(−1/ log c′(1− 1/m)) with c′ < 1.
After Corollary 5 is empirically verified with the standard
PSO, we now examine Corollary 6. The parameters we used
in PSO are given as cp = 1, cn = 1, w = 1/(2 ln 2), and  =
10−6. The swarm size ranges from 50 to 1000 with step 5.
For each swarm size, we perform 100 independent runs and
record the mean as we did in last experiment.
The comparison of experimental and theoretical results is
shown in Figures 3, 4, 5, and 6. From Figures 3 and 4, we can
see that the convergence time is close to −64.9/ log 0.555(1−
1/m) = O(−1/ log c′(1 − 1/m)), where c′ = 0.555,
and in Figures 5 and 6, the convergence time is close to
Advances in Artificial Intelligence 7
[5] P. Yan and L. Tang, “PSO algorithm for a scheduling parallel
unit batch process with batching,” in In Proceedings of the 1st
ACM/SIGEVO Summit on Genetic and Evolutionary Computa-
tion (GEC ’09), pp. 703–708, Shanghai, China, June 2009.
[6] T. M. Alkhamis and M. A. Ahmed, “Simulation-based opti-
mization for repairable systems using particle swarm algo-
rithm,” in In Proceedings of the 37th Conference on Winter
Simulation, pp. 857–861, Orlando, Fla, USA, December 2005.
[7] R. Wrobel and P. H. Mellor, “Particle swarm optimisation for
the design of brushless permanent magnet machines,” in In
Proceedings of the IEEE Industry Applications Conference: 41st
IAS Annual Meeting, vol. 4, pp. 1891–1897, Tampa, Fla, USA,
October 2006.
[8] J. Kennedy, “The behavior of particles,” in Proceedings of the
7th International Conference on Evolutionary Programming, pp.
581–589, 1998.
[9] E. Ozcan and C. K. Mohan, “Analysis of a simple particle
swarm optimization system,” Intelligent Engineering Systems
Through Artificial Neural Networks, vol. 8, pp. 253–258, 1998.
[10] E. Ozcan and C.K. Mohan, “Particle swarm optimization:
surfing the waves,” in Proceedings of the IEEE Congress on
Evolutionary Computation (CEC ’99), pp. 1939–1944, 1999.
[11] M. Clerc and J. Kennedy, “The particle swarm-explosion, sta-
bility, and convergence in a multidimensional complex space,”
IEEE Transactions on Evolutionary Computation, vol. 6, no. 1,
pp. 58–73, 2002.
[12] F. van den Bergh, An analysis of particle swarm optimizers,
Ph.D. thesis, University of Pretoria, 2002.
[13] K. Yasuda, A. Ide, and N. Iwasaki, “Adaptive particle swarm
optimization,” in In Proceedings of the IEEE International Con-
ference on Systems, Man and Cybernetics, pp. 1554–1559, Octo-
ber 2003.
[14] Y. L. Zheng, L. H. Ma, L. Y. Zhang, and J. X. Qian, “On the
convergence analysis and parameter selection in particle
swarm optimization,” in In Proceedings of the 2nd International
Conference on Machine Learning and Cybernetics, pp. 1802–
1807, November 2003.
[15] I. C. Trelea, “The particle swarm optimization algorithm: con-
vergence analysis and parameter selection,” Information Pro-
cessing Letters, vol. 85, no. 6, pp. 317–325, 2003.
[16] F. van den Bergh and A. P. Engelbrecht, “A study of particle
swarm optimization particle trajectories,” Information Sci-
ences, vol. 176, no. 8, pp. 937–971, 2006.
[17] V. Kadirkamanathan, K. Selvarajah, and P. J. Fleming, “Stabil-
ity analysis of the particle dynamics in particle swarm opti-
mizer,” IEEE Transactions on Evolutionary Computation, vol.
10, no. 3, pp. 245–255, 2006.
[18] M. Jiang, Y. P. Luo, and S. Y. Yang, “Stochastic convergence
analysis and parameter selection of the standard particle
swarm optimization algorithm,” Information Processing Let-
ters, vol. 102, no. 1, pp. 8–16, 2007.
[19] Y. P. Chen and P. Jiang, “Analysis of particle interaction in
particle swarm optimization,” Theoretical Computer Science,
vol. 411, no. 21, pp. 2101–2115, 2010.
[20] J. Kennedy, “Particle swarm: social adaptation of knowledge,”
in Proceedings of the IEEE International Conference on Evolu-
tionary Computation (ICEC ’97), pp. 303–308, April 1997.
[21] M.R. Spiegel, Schaum’s Outline of Theory and Problems of Prob-
ability and Statistics, Mcgraw-Hill, 1975.
[22] X. Yao, Y. Liu, and G. Lin, “Evolutionary programming made
faster,” IEEE Transactions on Evolutionary Computation, vol. 3,
no. 2, pp. 82–102, 1999.
LIN AND CHEN: ANALYSIS ON THE COLLABORATION BETWEEN GLOBAL SEARCH AND LOCAL SEARCH IN MEMETIC COMPUTATION 609
starts and long running time of local search. They also
proposed several renowned strategies for selecting solution
candidates on which the local search operator is applied: the
fitness based selection and the diversity based selection. How-
ever, with the aids of these guidelines, designing a memetic
algorithm for a specific problem still requires considerable
time as the optimal design is not only algorithm specific but
also problem dependent. To cope with this issue, the concept
of systematically adjusting the parameters of local search is
proposed [11]. Although this technique is robust, it does not
guarantee the best performance. Another line of research is
regarding the concept of memes [12]–[14]. In these studies,
the local search algorithms, encoded as memes, can adapt to
the underlying problem and thus improve the efficiency as
the memetic algorithm progresses. This framework is robust
as well as efficient with the expense of the learning cost of
memes.
In spite of the light shed on the design issue of memetic
algorithms by the aforementioned studies, the question of how
one can achieve the optimal design of memetic algorithms on
a specific problem remains. The key to achieve this ultimate
goal apparently include a full awareness of the physics behind
the algorithm and the problem. As theoretical studies can help
to understand the internal mechanism of algorithms, they can
provide important insights to the design issue. Compared to the
progress of theoretical studies on evolutionary computation,
which is still in its infancy [15]–[23], theoretical studies of
memetic algorithms are even scarce. Recent studies [24], [25]
investigated the behavior of simple memetic algorithms on sev-
eral classes of functions. The proposed theoretical models on
the demonstrative classes of functions reaffirmed that param-
eterizing memetic evolutionary algorithms can be extremely
difficult. As these theoretical models are developed according
to different classes of functions, they are capable of depicting
the algorithmic behavior from their respective perspectives on
the adopted classes of functions instead of providing a unified
principle for the design of memetic algorithms.
The concept of basins of attraction [26] provides another
perspective and gives an opportunity to conduct general anal-
ysis on memetic algorithms. In [27] and [28], the search space
is viewed as a union of basins of attraction, and the optimal
allowable local search length of simple memetic algorithms
is theoretically estimated. A similar concept, quasi-basins
defined by the subthreshold seeker, was adopted to prove the
searchability of general functions [29] and to investigate the
subthreshold seeking behavior [30].
In this paper, we aim to establish a theoretical model that
can depict the collaboration between global search and local
search in memetic computation on a wide range of problems.
To achieve this, we propose the concept of local search zones
which are the regions that local search exploits. In this per-
spective, these local search zones are defined by the landscape
of the problem as well as the collaboration between global
search and local search. As local search zones are generally
not easy to assess, we adopt quasi-basins to estimate local
search zones and define the quasi-basin class (QBC) which
categorizes problems by their quasi-basin distributions as the
basis on which memetic algorithms are investigated. Then, we
analyze the performance of the subthreshold seeker, which is
regarded as a representative archetype of memetic algorithms,
to develop a theoretical model for the global-local search
collaboration in memetic computation. The derived theoretical
model can describe how the distribution of local search zones
and the efficiency of the global search algorithm and the local
search algorithm are related to the expected time for a memetic
algorithm to find the optimal solution. Because this model,
empirically verified, is consistent with the observations made
in many previous studies in the literature, it may be considered
valid for representing various memetic algorithms on a wide
range of problems and may give important insights to the
future design of advanced memetic algorithms.
The rest of this paper is arranged in the following manner.
Section II gives a survey on the current progress of analysis on
memetic algorithms and elaborates the need of a general the-
oretical model which can describe the collaboration between
global search and local search in memetic computation on a
broad range of problems. Section III expounds the fundamental
concepts on the analysis of memetic algorithms and provides
the definitions of our framework to form the basis for further
derivation. As a memetic algorithm comprises global search
and local search, we first analyze the global search component
of the subthreshold seeker and discuss how this analysis is
related to the behavior of common global search algorithms
in Section IV. Based on the analysis of global search and
the concept of QBC, we derive and empirically verify the
formula that describes the behavior of the subthreshold seeker
working with local search operators of different efficiency on
various QBCs in Section V. After the empirically verifying
the proposed model, we expound how our model can repre-
sent the general behavior of memetic algorithms and discuss
possible extensions and future work of the proposed model in
Section VI. Finally, we recap the significance of our model
and conclude this paper in Section VII.
II. Background
Designing a memetic algorithm requires not only selecting
a global search mechanism as well as a local search operators
but also establishing a subtle coordination to exhibit the van-
tage of both ends. Hart [9] in his seminal study for designing
efficient memetic algorithms investigated the following four
questions on continuous optimization problems.
1) How often should local search be applied?
2) On which solutions should local search be used?
3) How long should local search be run?
4) How efficient does local search need to be?
In his framework, he noted that the memetic algorithms that
employ elitism will be most efficient with large population
sizes and infrequent local search. He also proposed two
strategies, fitness based selection and diversity based selection,
for selecting solution candidates to apply local search. He
concluded that these two strategies help much. Land [10]
extended Hart’s study to combinatorial domains. In his study,
he adopted steady state genetic algorithms as global search
and proposed a local search potential based strategy in se-
lecting local search candidates. The local search potential
LIN AND CHEN: ANALYSIS ON THE COLLABORATION BETWEEN GLOBAL SEARCH AND LOCAL SEARCH IN MEMETIC COMPUTATION 611
III. Quasi-Basin Classes and Subthreshold Seeker
In this section, we introduce the concept of local search
zones and give definitions to the fundamental terminologies
of our framework. The concept of the local search zones is
described based on the formal definitions of the search process
of an algorithm on a problem and the search space viewed by
a search process. Then, based on the concept of local search
zones, we introduce the QBC and the generalized subthreshold
seeker on which the theoretical analysis is based.
A. Local Search Zones
The task to handle an optimization problem is to optimize
a given objective function f : X → Y . For convenience, we
specify our optimization goal as to find a point x∗ ∈ X with
the minimum value y∗ ∈ Y . We assume that both X and Y are
finite sets. Such an assumption makes a practical sense because
optimization problems are generally numerically solved on
digital computers. In this paper, for simplifying the derivation,
we also assume that every function maps different x ∈ X
to different y ∈ Y . In order to formally describe a search
process of an algorithm on a function, we adopt part of the
terminologies defined in [47] as the following definitions.
Definition 1 (Search Process): Given two finite sets X and
Y:
1) A trace of length m is a sequence Tm := ((xi, yi))m1 =
((x1, y1), (x2, y2), . . . , (xm, ym)) ∈ (X × Y)m with dis-
tinct xi. “x ∈ Tm” denotes that x = xi for some
i ∈ {1, 2, . . . , m}. Let T0 be the empty sequence and T 
be the set containing all the traces of a length smaller
than or equal to .
2) Let AT , where T ∈ T |X |−1, be a random variable over
X satisfying that Prob{AT = x} = 0 for all x ∈ T . An
algorithm A is a collection of such random variables,
i.e., A = {AT | T ∈ T |X |−1}.
3) The search process of A on f , S(A, f ), is a stochastic
process (Xi, Yi := f (Xi)) over X × Y defined by X1 ∼
AT0 and Xk+1 ∼ A(Xi,Yi)k1 .
For generality, we interpret the search space viewed by a
memetic algorithm as a graph. Since a local search algorithm
usually starts from a candidate solution and iteratively moves
to a neighbor solution, the local search algorithm defines the
neighborhood of a candidate solution in the search space
viewed by the memetic algorithm which utilizes it. Thus,
we define the search space viewed by a memetic algorithm
as a graph of which the vertices are the set of points of
X and the edges are the set of pairs of points connected
by the local search algorithm of the memetic algorithm as
follows.
Definition 2 (Search Space): Given a memetic algorithm
MA, a function f , and LS, the local search algorithm adopted
by MA. Let NLS(v) denote the neighborhood of a vertex v
defined by LS. The search space viewed by MA on f can
be represented by a graph G = (V,E), where V (G) := X and
E(G) := {〈vi, vj〉 | vj ∈ NLS(vi),∀i, j}.
In the rest of this paper, the terms X and V (G) are used
exchangeably. Now, with all these fundamental terminologies,
we can formally define the local search zone as follows.
Definition 3 (Local Search Zone): Given a search process
of a memetic algorithm MA on function f , S(MA, f ), and
LS, the local search algorithm adopted by MA:
1) The local search points of a search space G viewed
by S(MA, f ) are defined as the set of points SLSZ =
{v | E[Pr(Xk+1 = u, u ∈ NLS(v)|v ∈ Tk, u /∈ Tk)] >
0.5,∀v ∈ V (G)}.1
2) A local search zone LSZ is defined as a maximal subset
in SLSZ such that there exists a path2 between all the
pairs of vertices.
3) The size of local search zones is denoted by |SLSZ|.
By this definition, a local search point is a vertex which if is
visited by MA, MA would tend to visit one of its unvisited
adjacent vertices in the future, and the local search zones are
where the local search points reside. In other words, the local
search zones are where local search prefers when a memetic
algorithm is applied. In our perspective, the distribution of the
local search zones has a great influence on the performance
of a memetic algorithm. As the local search of practical
memetic algorithms favors the points that have high potential
to lead to the optimal point, fitness-relevant and diversity-
relevant criteria are adopted. These criteria are somehow
dynamic, complicated, and difficult to analyze. Abstracting
the exploration behavior of global search and the exploitation
behavior of local search, we consider fitness as the prime
index of the potential to find the optimal point regardless
of the diversity-relevant metrics which are often auxiliary for
diversity maintenance. Thus, the local search zones can be
estimated by zones consisting of qualified high-fitness points.
Based on this way of thinking, we define the QBC to represent
different problem classes which possess different local search
zone distribution. Then, we take the subthreshold seeker as a
representative archetype of memetic algorithms and analyze
its behavior on various QBCs to develop a general theoretical
model for the core mechanism of memetic algorithms.
B. Quasi-Basin Classes
The QBC conceptually defines problem classes according
to the number of local search zones and the size of local
search zones. To define QBC, we first define the quasi-basin
(QB) as follows.
Definition 4 (QB):
1) For any function f , function value βm(f ), defined as
βm(f ) := min
{
argy {| {x ∈ X | f (x) ≤ y} | = m}
}
, deno-
tes a threshold, and there are m − 1 points with an
objective value less than βm(f ).
2) For any function f , the set that contains all the points
with an objective value less than βm(f ) is defined as
Sm(f ) := {x ∈ X |f (x) ≤ βm(f )}.
3) Given a graph G, for any function f : V (G) → Y , a
quasi-basin QB is defined as a maximal subset in Sm(f )
such that there is a path between all the pairs of vertices.
1The E[] notation indicates the expected value of Pr(Xk+1 = u, u ∈
NLS (v)|v ∈ Tk, u /∈ Tk) over all k ∈ |X | and all possible Tk which containing
v and not containing u.
2A path in a graph is a sequence of vertices such that from each vertex
there exists an edge to the next vertex in the sequence except for the last one.
LIN AND CHEN: ANALYSIS ON THE COLLABORATION BETWEEN GLOBAL SEARCH AND LOCAL SEARCH IN MEMETIC COMPUTATION 613
Fig. 2. Expected Tθ with respect to m when N = 100. Exp represents the
actual average Tθ over 1000 independent simulation runs. Theo1 represents
the theoretical expected Tθ of non-repeated uniform random sampling, and
Theo2 represents the theoretical expected Tθ of uniform random sampling
that allows us to sample visited points.
The probability to hit a subthreshold point at the qth visited
points is therefore
1
q
P(X = 1; N,m, q) .
Let E(Tθ) be the expected first global search time. We have
E(Tθ) =
N−m+1∑
i=1
i
1
i
P(X = 1; N,m, i)
=
N−m+1∑
i=1
(
m
1
)(
N−m
i−1
)
(
N
i
)
= m
N−m+1∑
i=1
i
N
i−2∏
j=0
N − m − j
N − 1 − j . (1)
Fig. 2 illustrates the expected value of Tθ with respect to
m when N = 100. In this figure, we compare (1) (the solid
line, Theo1) with N/m (the crosses, Theo2) and the average
first global search time in 1000 independent simulation runs
(the circles, Exp). The N/m is the expected Tθ for allowing
sampling visited points which is obviously an upper bound of
(1). In the figure, we can find that (1) consists of the empirical
result perfectly, while the trend of N/m gradually converges
toward the other two. For non-repeated random sampling, half
of points in the search space are expected to be visited before
finding the minimum point. As m increases, indicating that
Sm(f ) contains more points, time to meet a point in Sm(f )
decreases rapidly regardless of whether or not sampling visited
points is allowed. It indicates that although finding several
specific points in a search space via random search takes a con-
siderable amount of time, finding a point in a small but large
enough set can be attained within a relatively shorter time.
Fig. 3 illustrates the differences among the actual Tθ av-
eraged over 1000 runs, and the two theoretical expected Tθ
in ratio. The circle represents the difference between the
empirical result and N/m in ratio with respect to the empirical
result, and the cross represents that between (1) and N/m.
From this figure we can find that (1) can be approximated
by N/m as it only deviates significantly from (1) when m is
Fig. 3. Difference ratio of the expected Tθ with respect to m when N = 100.
The Exp diff represents the difference ratio between the actual average Tθ
and N/m. The Org diff represents the difference ratio between the theoretical
expected Tθ and N/m.
rather small. As (1) is a complicated formula and difficult to
analyze, we approximate the expected Tθ with N/m.
B. kth Global Search Time
In this section, we further measure the expected time for
global search to find a subthreshold point after k − 1 runs
of local search have been executed. In other words, we
estimate the time for the kth global search. As the local
search frequency and the global search frequency are related
to the landscape of the problem, for simplicity, we derive
the model on the uniform quasi-basin class Qu(G,Y,m, b).
All the problems in this class have their quasi-basin sizes
fixed to m/b or m/b. Because each local search run in
a quasi-basin will eventually visit about 1/s of the points
in the quasi-basin, each local search run will visit m/bs
or m/bs points. For convenience of derivation, we adopt
m/bs instead of m/bs or m/bs for the number of points
visited by a local search run. For non-revisit search, since the
first global search time is approximated as N/m, when in the
second global search run, there will be N − N/m − m/bs
unvisited points and m − m/bs unvisited subthreshold points,
the time required for the second global search run is
N − N
m
− m
bs
m − m
bs
.
The ith global search time is denoted as Fi, where i is
referred to as the number of global search runs. We have
F1 =
N
m
F2 =
N − F1 − m
bs
m − m
bs
F3 =
N − (F1 + F2) − 2m
bs
m − 2m
bs
LIN AND CHEN: ANALYSIS ON THE COLLABORATION BETWEEN GLOBAL SEARCH AND LOCAL SEARCH IN MEMETIC COMPUTATION 615
Fig. 5. Last global search time divided by the first global search time, Tθ ,
with respect to m when the exhaustive local search is applied.
search time would be greater than half of the amount of the
first global search. Fig. 5 illustrates the last global search time,
Ff , divided by the first global search time, Tθ , with respect to
m when the exhaustive local search is applied. The last global
search times of b = 10 and b = 100, initially increase as m
increases and reach a peak, followed by gradually degradation.
The smaller b is, the smaller m the peak appears at with
a greater peak value. Generally, when the number of quasi-
basins is considerably large, the smaller the last global search
time is. Overall, for N = 1000 the last global search time
is within 0.6 to 1.6 times of Tθ . As indicated in Fig. 4, the
variation of the global search time with respect to the number
of global search runs is approximately linear. Thus, we can
approximate the average global search time as the average of
the first global search time and the last global search time. The
resultant upper bound and lower bound of the approximated
average global search time are then 0.75 and 1.5 times of Tθ .
C. Discussion
Overall, in this section, we can see that the expected global
search time to hit a subthreshold point in local search zones is
inversely proportional to the size of local search zones in the
search space. Because the uniform random search is employed
as the global search component, such results illustrate a
baseline behavior of global search in common definitions. It
can be observed that when the size ratio between local search
zones and the whole search space is very small, the expected
global search time will be immensely long because finding
a local search zone is very difficult. If the ratio is not very
small and permits an acceptable probability to be hit by global
search, the expected global search time will drop dramatically.
In this case, since the size of local search zones is still small,
the local search operator requires a relatively short time to find
the optimum solution.
V. Subthreshold Seeker on QBC
In this section, we formulate the expected evaluation time
for a subthreshold seeker on a Q(G,Y,m, b) as the sum
of expected total global search time and the expected total
local search time. The expected total global search time is the
product of the expected time for global search to enter a local
search zone and the expected number of global search runs.
The expected total local search time is merely the expected
time for local search to find the global optimal points among
the local search zones which is proportional to the size of
the local search zones in the search space. In this manner,
the derived formula can depict how the collaboration between
global search and local search influences the performance of
memetic algorithms. Then, we propose a sampling test scheme
to empirically verify the behavior of the subthreshold seeker
on various QBCs. Finally, the empirical results are illustrated
to validate the proposed theoretical model.
A. Evaluation Time of Subthreshold Seeker
With the global search time ready, we can now estimate the
time to find the minimum point, i.e., the evaluation time T of
subthreshold seeker, with the equation
T =
cN
m
⌈
bs
2
⌉
+
m + 1
2
. (7)
The expected total time over a QBC is considered as the sum
of the expected total global search time, the first term, and the
expected total local search time, the second term. As discussed
in the previous section, it is expected to apply bs/2 local
search runs in order to find the global optima, and thus, bs/2
global search runs.
cN
m
represents the average global time
with c varies between 0.75 and 1.5, and m + 1
2
corresponds
to the expected time for the local search to find the minimum
among subthreshold points. To derive the m that achieves the
minimum evaluation time, we solve the following equation
with the first derivative of (7)4 to be zero:
T ′ = −bscN
2m2
+
1
2
= 0 . (8)
The solution of this equation is m =
√
bscN. Setting m to
about
√
bscN in (7), the subthreshold seeker can achieve the
minimum evaluation time T around
√
bscN.5 Note that the
total global search time and the total local search time are
near identical when the overall evaluation time is minimum.
The following sections verify (7) with the results obtained by
our experiments.
B. Sampling Test Scheme
For empirical convenience, we implement the simplest case
of QBC, pathwise quasi-basin class (PQBC). PQBC is the
class of functions with a simple path spatial structure and a
distinct integer value in Y = {1, 2, · · · , n}, where n = |X |, on
each vertex. PQBC is formally defined as
Definition 7 (PQBC): Given a finite set Y =
{1, 2, · · · , n} ⊂ N and a simple path G = v1v2 . . . vn,
the pathwise QBC with b distinct quasi-basins and m
subthreshold vertices is defined as Q+(G,Y,m, b).
To investigate the expected subthreshold seeker behavior
over a specific PQBC, we sample functions from a specific
4For convenience, we omit the ceiling.
5The actual value is
√
bscN+0.5, we omit 0.5 as it is a rather small quantity.
LIN AND CHEN: ANALYSIS ON THE COLLABORATION BETWEEN GLOBAL SEARCH AND LOCAL SEARCH IN MEMETIC COMPUTATION 617
Fig. 8. Time for a subthreshold seeker to find the minimum with respect to
m when (a) n = 1000 and b = 1, (b) n = 1000 and b = 10, and (c) n = 1000
and b = 250. The lines, Ttheo, Ttheo1, and Ttheo2, represent the theoretical
values derived from (7), and the dot, Texp, represents the average time for a
subthreshold seeker to find the minimum on different PQBCs. The average
total sampling counts used by local search and global search are also recorded
as ls and gs, respectively.
agreement, and therefore, (7) is dimensionally validated for
different factors.
Fig. 12 illustrates the evaluation time with respect to m
when non-exhaustive local search components, i.e., s > 1,
are used. In both cases, the solid lines represent the theo-
retical evaluation time predicted by (7) with c = 1.5. These
Fig. 9. n = 1000, b = 10, non-uniform quasi-basin.
Fig. 10. Optimal evaluation time versus b when n = 100. The solid line
indicates the theoretical value predicted by (7) with c = 1, and the dots
indicate the empirical results.
empirical results also well match the proposed theoretical
model (7).
VI. Discussion
In this section, we first explain how the subthreshold seeker
can be regarded as a representative archetype of MAs and how
the theoretical model can depict the general behavior of MAs.
Then, we connect the proposed model to previous related
studies in the literature. Finally, we discuss the extensions and
future work of the proposed model.
A. Subthreshold Seeker as a Representative Archetype of MA
Since the proposed model of the subthreshold seeker on
different QBCs has been validated by the empirical results in
the previous section, in this section, we revisit our framework
and discuss how our theoretical model is representative of
MAs on a broad range of problems. The origin of our
framework is the concept of local search zones. Based on
this concept, the search space viewed by a search process
can be partitioned into local search zones, which are areas
preferred by exploitation, and parts of no interests. Global
LIN AND CHEN: ANALYSIS ON THE COLLABORATION BETWEEN GLOBAL SEARCH AND LOCAL SEARCH IN MEMETIC COMPUTATION 619
We can assume that the average global search time to enter
a local search zone is inversely proportional to the size ratio
between local search zones and the search space, while the
average total local search time is proportional to the size
of the local search zones. Putting these two terms together,
we can obtain the “V-shaped” curve which resembles those
derived from (7). This V-shaped curve implies that a good
collaboration between global search and local search should
guarantee a short average global search time to hit local zones
and sufficiently small sizes of local zones for the local search
to exploit. Regarding the influence of the size of local search
zones on the average global search time to find local search
zones and the average time for local search to find the optimal
point, memetic algorithms which have small sized local search
zones will perform better. As mentioned in Section III-A, local
search zones are generally zones consisting of qualified high-
fitness points. A small size of local search zones implies that
local search only be applied to few qualified individuals. This
observation is consistent with the use of elitism in local search
candidate selection and the infrequent local search principle in
quite a number of research works [9], [10], [31]–[33]. It is also
notable that several studies adopt a local search/global search
ratio which is consistent with our theoretical model [36].
In our model, the local search component adopted by the
subthreshold seeker exploits a quasi-basin via visiting the
neighbors of current search point. The local search parameter
of the employed local search operator is connected to how
well a quasi-basin is exploited. Recall that when s = 1, the
exhaustive local search will eventually visit all the points in a
quasi-basin. In this case, the local minimum of a quasi-basin,
which may be the global minimum, will be visited, and thus,
only one local search run for each basin is required. For other
local search parameter greater than one, there are chances for
one local search run to miss the global minimum in a quasi-
basin, and thus, more local search runs on this quasi-basin and
more global search runs to hit this quasi-basin are required.
The cost will be the extra global search time to enter the
quasi-basin again when the algorithm guarantees non-repeated
sampling. Fig. 12 and the factor s in (7) demonstrate the effect
of the degree of exploitation of a basin. Such an effect implies
that a good local search operator ought to fully exploit the
given quasi-basin, at least the local minimum resides in the
quasi-basin should be found, to guarantee a good local search
and global search coordination. This inference is consistent
with the empirical results of those studies [10], [39], [40]
concluding that longer but not excessive local search lengths
are favored in memetic algorithms.
Another notable factor is the number of local search zones
in the search space. Our model illustrates that the number of
global search runs is proportional to this factor. Recall that
we represent the search space viewed by a search process as a
graph composed of the neighborhood defined by the employed
local search algorithm. The size of local search zones is
determined by the local search criterion and the connectivity
formed by the local search operator. Given the same local
search criterion, the local search operator which forms fewer
local search zones will perform better. This suggests that a
good local search operator should be able to find local search
points regardless of the physical landscape of a problem. This
is somehow difficult for naive greedy local search algorithms
to achieve and may require landscape knowledge given by the
user or learned from the search process. However, operators
with this kind of ability to cross the physical landscape of a
problem somehow deviate from the traditional definition of
local searchers. Thus, for typical local search, the number
of local search zones are primarily defined by the physical
landscape of a problem and the local search criterion. The
physical landscape of a problem is usually connected to the
number of niches of a problem. Our model also takes into
account this crucial factor and delineates the relationship
between this factor and the evaluation time. The proposed
theoretical model indicates that for a fair memetic algorithm,
the expected evaluation time should scale at most as the square
root of the number of niches of a problem.
Despite the aforementioned consistency between the pro-
posed model and the elitism based strategy in local search
candidate selection, infrequent local search, long local search
length, and local search/global search ratio, some previous
studies also show a strong connection to our model. In an
investigation on the balance between genetic search and local
search in memetic algorithms for multiobjective permutation
flowshop scheduling [39], the authors examined 132 com-
binations of 11 values of k, which is the maximum num-
ber of examined neighbors of the current solution, and 12
values of pLS , which is the local search probability applied
to the tournament selected individuals. The former factor k
connects to the degree of how well a feasible sub-region
can be exploited, and the second factor pLS connects to the
threshold that triggers local search. The authors found that
the combination of the maximum k value and the minimum
nonzero pLS value achieved the best performance, the lowest
cost of flowshop scheduling, in their experiments. The V-
shaped curve of cost along the axis of the maximum k with
respect to pLS in their Fig. 13 resembles our V-shaped curve
of the evaluation time in Fig. 8. Because the stop criterion of
their experiments is the evaluation of a fixed number of points,
the factor combinations that require less evaluation time to
find the global minimum will have better solution quality, i.e.,
lower cost. This agreement implies that the proposed model
may be adopted to give a theoretical explanation to the internal
working of their multiobjective memetic algorithms.
Another set of intriguing empirical results is presented in
the study of parameterizing local search [11]. In that study,
the authors applied a hybrid approach to the memory cost
minimization problem with various local search parameter
settings. The local search parameter refers to the intensity
of the local search method, a tractable algorithm called code
size dynamic programming post optimization, applied to every
individual in the population. The authors depicted in Fig. 13
in their paper that when a fixed runtime is used, the number of
generations completed decreases rapidly as the local parameter
increases. That the global search time is proportional to the
number of generations implies the curve, indicating the global
search time, resembles the expected Tθ in our Fig. 2. As the
expected global search time is illustrated and the expected
local search time will be proportional to the intensity of local
LIN AND CHEN: ANALYSIS ON THE COLLABORATION BETWEEN GLOBAL SEARCH AND LOCAL SEARCH IN MEMETIC COMPUTATION 621
Fig. 13. (μ+λ)-memetic algorithm.
Fig. 14(b), although the evaluation time could not be properly
approximated by (7), the V-shape remains. The triangles, lscnt,
in these two figures represent the number of local search runs.
From these figures, we can find that the applied greedy local
search is much less efficient than the exhaustive search with
its limited total local search time denoted by the circles, ls.
The higher lscnt than the total local search time in the case
of MA with greedy local search suggests revisiting of local
searched points. Note also that in both figures, the memetic
algorithm resembles the global search behavior of random
search with an offset and the greedy local search resembles
the exhaustive search with a degraded gradient. In these two
memetic algorithms, our theoretical model is still capable
of capturing the essence of the collaboration between global
search and local search.
To accurately estimate the evaluation time of an instance
of MA search process, one needs to take into account the
influence of the population size and the exploration ability
limited by its greediness to estimate its expected global
search time and assess the efficiency of local search which
corresponds to the parameter s. Another notable characteristic
is that both memetic algorithms in the two cases perform
worse than the subthreshold seeker on QBCs. This is consistent
with our earlier statement that the random sampling is a
better global explorer than any greedy algorithms on QBCs.
This may seem contrary to the practical memetic algorithms.
However, as the QBC categorizes arbitrary problems according
to the quasi-basin distribution, it does not guarantee that all the
problems within a QBC exhibit a regularity which a greedy
algorithm can take advantages of. To manifest the optimization
characteristics of greedy algorithms, fast convergence versus
degrading diversity, the QBC framework must be extended
to define classes of continuous-like discrete problems. In
our opinion, adopting the concept of discrete Lipschitz class
(DLC) [47] may be a good choice. In [47], the Lipschitz func-
tions, functions with bounded slope, are transferred to DLC to
describe continuous problems in discrete domains. Combining
Fig. 14. Evaluation time for (20+20) memetic algorithms. (a) n = 1024,
b = 10, MA with exhaustive local search. (b) n = 1024, b = 10, MA with
greedy local search.
the DLC and QBC may provide a desired model in discrete
domains that exhibits the characteristics of optimization in
continuous domains.
For continuous problems, further efforts are required to
extend all the analysis from discrete problems to continuous
problems. In continuous domains, both X and Y are infinite
sets. The first question may be how to extend the search space
represented by a graph to fit the continuous scenario. If we can
define the local search zones in a similar manner in continuous
domains, we may start to investigate the behavior of memetic
algorithms based on the modified framework. It may be much
harder to estimate in continuous domains the expected global
search time to enter local search zones, the expected total local
search time to find the optimal solution, and the efficiency
of local search of a given memetic algorithm. Once all these
issues are resolved, an instance of algorithm-problem complex
in continuous domains can be successfully delineated as well
as the performance of a global search algorithm and a local
search algorithm can be measured and compared in continuous
domains. Memetic algorithm designers can thus select their
global search algorithms and local search algorithms and
design the local search criterion by following the guideline
provided by the modified framework. As practical problems
LIN AND CHEN: ANALYSIS ON THE COLLABORATION BETWEEN GLOBAL SEARCH AND LOCAL SEARCH IN MEMETIC COMPUTATION 623
[23] T. Jansen and I. Wegener, “The analysis of evolutionary algorithms: A
proof that crossover really can help,” Algorithmica, vol. 34, no. 1, pp.
47–66, 2008.
[24] D. Sudholt, “On the analysis of the (1 + 1) memetic algorithm,” in Proc.
ACM SIGEVO GECCO, 2006, pp. 493–500.
[25] D. Sudholt, “The impact of parametrization in memetic evolutionary
algorithms,” Theor. Comput. Sci., vol. 410, no. 26, pp. 2511–2528, 2009.
[26] L. Kallel, B. Naudts, and C. R. Reeves, “Properties of fitness functions
and search landscapes,” in Theoretical Aspects of Evolutionary Comput-
ing. Berlin, Germany: Springer-Verlag, 2001, pp. 175–206.
[27] A. Sinha, Y.-P. Chen, and D. E. Goldberg, “Designing efficient genetic
and evolutionary algorithm hybrids,” in Recent Advances in Memetic
Algorithms (Studies in Fuzziness and Soft Computing, vol. 166). Hei-
delberg, Germany: Physica-Verlag, 2004, pp. 259–288.
[28] Q. H. Nguyen, Y.-S. Ong, and M. H. Lim, “A probabilistic memetic
framework,” IEEE Trans. Evol. Comput., vol. 13, no. 3, pp. 604–623,
Jun. 2009.
[29] S. Christensen and F. Oppacher, “What can we learn from no free lunch?
A first attempt to characterize the concept of a searchable function,” in
Proc. GECCO, 2001, pp. 1219–1226.
[30] D. Whitley and J. Rowe, “Subthreshold-seeking local search,” Theor.
Comput. Sci., vol. 361, no. 1, pp. 2–17, 2006.
[31] K.-H. Liang, X. Yao, and C. Newton, “Evolutionary search of approxi-
mated n-dimensional landscapes,” Int. J. Knowl. Based Intell. Eng. Syst.,
vol. 4, no. 3, pp. 172–183, 2000.
[32] M. Tang and X. Yao, “A memetic algorithm for VLSI floorplanning,”
IEEE Trans. Syst. Man Cybern. B Cybern., vol. 37, no. 1, pp. 62–69,
Feb. 2007.
[33] Y.-H. Liu, “A hybrid scatter search for the probabilistic traveling
salesman problem,” Comput. Oper. Res., vol. 34, no. 10, pp. 2949–2963,
2007.
[34] M. Lozano, F. Herrera, N. Krasnogor, and D. Molina, “Real-coded
memetic algorithms with crossover hill-climbing,” Evol. Comput.,
vol. 12, no. 3, pp. 273–302, 2004.
[35] S. Garcia, J. R. Cano, and F. Herrera, “A memetic algorithm for evolu-
tionary prototype selection: A scaling up approach,” Pattern Recognit.,
vol. 41, no. 8, pp. 2693–2709, 2008.
[36] D. Molina, M. Lozano, C. Garcia-Martinez, and F. Herrera, “Memetic
algorithms for continuous optimization based on local search chains,”
Evol. Comput., vol. 18, no. 1, pp. 27–63, 2010.
[37] F. Neri, J. Toivanen, G. L. Cascella, and Y.-S. Ong, “An adaptive mul-
timeme algorithm for designing HIV multidrug therapies,” IEEE/ACM
Trans. Comput. Biol. Bioinform., vol. 4, no. 2, pp. 264–278, Apr.–Jun.
2007.
[38] J. Tang, M. H. Lim, and Y. S. Ong, “Diversity-adaptive parallel memetic
algorithm for solving large scale combinatorial optimization problems,”
Soft Comput., vol. 11, no. 9, pp. 873–888, 2007.
[39] H. Ishibuchi, T. Yoshida, and T. Murata, “Balance between genetic
search and local search in memetic algorithms for multiobjective permu-
tation flowshop scheduling,” IEEE Trans. Evol. Comput., vol. 7, no. 2,
pp. 204–223, Apr. 2003.
[40] Z. Zhu, Y.-S. Ong, and M. Dash, “Wrapper-filter feature selection
algorithm using a memetic framework,” IEEE Trans. Syst. Man Cybern.
B Cybern., vol. 37, no. 1, pp. 70–76, Feb. 2007.
[41] P. Merz and B. Freisleben, “Fitness landscapes and memetic algorithm
design,” in New Ideas in Optimization. New York: McGraw-Hill, 1999,
pp. 245–260.
[42] I. Paenke, T. J. Kawecki, and B. Sendhoff, “The influence of learning
on evolution: A mathematical framework,” Artif. Life, vol. 15, no. 2, pp.
227–245, 2009.
[43] M. Le, Y.-S. Ong, Y. Jin, and B. Sendhoff, “Lamarckian memetic
algorithms: Local optimum and connectivity structure analysis,” Memetic
Comput., vol. 1, no. 3, pp. 175–190, 2009.
[44] C. Witt, “Runtime analysis of the (μ + 1) EA on simple pseudo-Boolean
functions,” Evol. Comput., vol. 14, no. 1, pp. 65–86, 2006.
[45] B. Doerr, N. Hebbinghaus, and F. Neumann, “Speeding up evolutionary
algorithms through asymmetric mutation operators,” Evol. Comput.,
vol. 15, no. 4, pp. 401–410, 2007.
[46] A. H. Wright and J. N. Richter, “Strong recombination, weak selection,
and mutation,” in Proc. ACM SIGEVO GECCO, 2006, pp. 1369–1376.
[47] P. Jiang and Y.-P. Chen, “Free lunches on the discrete Lipschitz class,”
Theor. Comput. Sci., vol. 412, no. 17, pp. 1614–1628, 2011.
[48] C.-L. Chang and Y.-D. Lyuu, “Optimal bounds on finding fixed points
of contraction mappings,” Theor. Comput. Sci., vol. 411, nos. 16–18, pp.
1742–1749, 2010.
Jih-Yiing Lin received the B.S. and M.S. degrees
in electronics engineering from National Chiao Tung
University, Hsinchu, Taiwan, in 2000 and 2002, re-
spectively. She is currently working toward the Ph.D.
degree in computer science from the Department of
Computer Science, National Chiao Tung University.
She was with Sunplus Technology, Hsinchu, from
2002 to 2007. Her current research interests in
the field of genetic and evolutionary computation
include theories, working principles, particle swarm
optimization, and linkage learning techniques.
Ying-Ping Chen (M’04) received the B.S. and M.S.
degrees in computer science and information en-
gineering from National Taiwan University, Taipei,
Taiwan, in 1995 and 1997, respectively, and the
Ph.D. degree from the Department of Computer
Science, University of Illinois, Urbana, in 2004.
He was an Assistant Professor from 2004 to
2009 and has been an Associate Professor since
2009 with the Department of Computer Science,
National Chiao Tung University, Hsinchu, Taiwan.
His current research interests in the field of genetic
and evolutionary computation include theories, working principles, particle
swarm optimization, estimation of distribution algorithms, linkage learning
techniques, and dimensional/facet-wise models.
Author's personal copy
Theoretical Computer Science 412 (2011) 1614–1628
Contents lists available at ScienceDirect
Theoretical Computer Science
journal homepage: www.elsevier.com/locate/tcs
Free lunches on the discrete Lipschitz class
Pei Jiang, Ying-ping Chen ∗
Department of Computer Science, National Chiao Tung University, 1001 Ta Hsueh Road, HsinChu City 300, Taiwan
a r t i c l e i n f o
Article history:
Received 14 January 2009
Received in revised form 30 November
2010
Accepted 8 December 2010
Communicated by G. Rozenberg
Keywords:
No-Free-Lunch theorem
Lipschitz continuity
Discrete Lipschitz class
Subthreshold-seeker
Sampling-test scheme
a b s t r a c t
The No-Free-Lunch theorem states that there does not exist a genuine general-purpose
optimizer because all algorithms have the identical performance on average over all
functions. However, such a result does not imply that search heuristics or optimization
algorithms are futile if we are more cautious with the applicability of these methods and
the search space. In this paper, within the No-Free-Lunch framework, we firstly introduce
the discrete Lipschitz class by transferring the Lipschitz functions, i.e., functions with
bounded slope, as a measure to fulfill the notion of continuity in discrete functions. We
then investigate the properties of the discrete Lipschitz class, generalize an algorithmcalled
subthreshold-seeker for optimization, and show that the generalized subthreshold-seeker
outperforms random search on this class. Finally, we propose a tractable sampling-test
scheme to empirically demonstrate the superiority of the generalized subthreshold-
seeker under practical configurations. This study concludes that there exist algorithms
outperforming random search on the discrete Lipschitz class in both theoretical and
practical aspects and indicates that the effectiveness of search heuristics may not be
universal but still general in some broad sense.
© 2010 Elsevier B.V. All rights reserved.
1. Introduction
In the 1980s there was a belief in the field of evolutionary computation that evolutionary algorithms are more widely
applicable and have superior overall performance while they may not perform as well as the specialized algorithm for a
specific optimization problem. However, in 1995, Wolpert and Macready proposed the No-Free-Lunch (NFL) theorem [1,2]
which formally states that every algorithm performs equally well on average over all functions. A direct implication of NFL
is that, given a performance measure, the better performance of an algorithm on some problems is balanced by the worse
performance on others. In other words, there is no such thing as robustness under the NFL framework, or all algorithms are
considered robust. Therefore, it is no surprise that the proposition of the NFL theorem causes a great deal of controversy in
the optimization and heuristic search community [3], as the NFL theorem sets a limitation on the pursuit of general-purpose
optimizers.
Indeed, the implications of the NFL theorem seem to disagree with empirical observations of the effectiveness
of optimization algorithms and search heuristics, since general-purpose optimizers such as gradient-based methods,
simulated-annealing, and biologically inspired algorithms do have their share of significance in real-world applications. On
the other hand, the NFL theorem is amathematical theorem,whichmeans that it is absolutely truewhen all the assumptions
are given. As a consequence, previous studies intending to address the incoherence between theoretical results and empirical
observations are mostly aiming at the assumptions of the NFL theorem, especially the notion of ‘‘all functions’’. Droste
et al. [4,5] systematically described a few scenarios of functions and claimed that the scope of the NFL theorem is too
enormous to be realistic. Streeter [6] proved that theNFL theoremdoes not hold over the problemswith sufficiently bounded
∗ Corresponding author. Tel.: +886 3 5712121; fax: +886 3 6126520.
E-mail addresses: pjiang@nclab.tw (P. Jiang), ypchen@nclab.tw, ypchen@cs.nctu.edu.tw (Y.-p. Chen).
0304-3975/$ – see front matter© 2010 Elsevier B.V. All rights reserved.
doi:10.1016/j.tcs.2010.12.028
Author's personal copy
1616 P. Jiang, Y.-p. Chen / Theoretical Computer Science 412 (2011) 1614–1628
(4) The search process of A on f , S(A, f ), is the stochastic process (Xi, Yi := f (Xi)) over X × Y defined by X1 ∼ AT0 and
Xk+1 ∼ A((Xi,Yi))k1 . Let S(A, f , k) := ((Xi, Yi))
k
1, and Sy(A, f , k) := (Yi)k1 is called the performance vector.
(5) LetV :=|X|i=1 Yi be the set containing all possible performance vectors. A performancemeasure is any functionmapping
V to R.
The terminology in Definition 1mostly follows those adopted in [2] and [19] with a few slightmodifications applied to avoid
the situation that an algorithm is undefinable on a complete trace and to make search processes able to be expressed in a
naturally stochastic way.
Example 2 (Random Search in NFL). Let RTm be a random variable that Prob{RTm = x} = 1/(|X| − m) for all x /∈ Tm. In the
NFL framework, random search can be accordingly defined as RS :=|X|−1m=0 {RTm | Tm ∈ T |X|−1}.
Now, the NFL theorem can be given as Theorem 3. The complete proof can be found in the original NFL papers [1,2].
Theorem 3 (NFL Theorem). If v ∈ V is a performance vector with length ℓ,∑f Prob{Sy(A, f , ℓ) = v} = c, where c is a constant
independent of A.
3. Discrete Lipschitz class
3.1. Definition of the discrete Lipschitz class
In real analysis, Lipschitz functions refer to the functions with bounded slope. Given a set C ⊆ R, f : C → R is a Lipschitz
function if there exists a constant K > 0 such that |f (a) − f (b)| ≤ K |a − b| for all a, b ∈ C. The Lipschitz condition is a
stronger condition than normal continuity, because any Lipschitz function is uniformly continuous. On the other hand, the
functions that are not everywhere differentiable may still be Lipschitz, e.g., f (x) = |x|. On a closed interval, the Lipschitz
class lies between continuous functions and the functions having continuous derivatives [20].
For the discrete space, there is no such thing as continuity. However, if there is some sort of distance defined in some
discrete space, the Lipschitz condition can still be applied, and therefore a natural way to simulate continuity in the discrete
space can be obtained. In combinatorics, the spatial structures are typically formed via graph theory. If we view the vertex
set as the search space and the edge set as the specification of the geometry, the Lipschitz condition can be transferred here
by restricting the difference of objective values between any two adjacent vertexes. Themerit of such definition is thatwe do
not put any constraints on the global structure directly such as to demand the functions to be polynomial or the description
length to be bounded. Instead, we only expect some similarities of the objective values within a neighborhood in the search
space.
Before commencing to detail our work, we explain the notations adopted in this paper for clarity. Given a graph G,
the vertex set and the edge set of G are denoted as V (G) and E(G). Also, deg(v) and N(v) indicate the degree and the
neighborhood of a vertex v ∈ V (G), respectively. Since we will focus on the discrete Lipschitz class in the remainder of
this paper, the domainX is always the vertex set V (G), representing the spatial structure. Hence, the two notationsX and
V (G) are used exchangeably. Furthermore, the property of Y of interest is the ordering, so without loss of generality, Y is
assumed to be a subset of N of the form {0, 1, . . . ,m} unless specified otherwise. The discrete Lipschitz class (DLC) can now
be introduced.
Definition 4 (Discrete Lipschitz Class, DLC). Given a connected graph G and a finite set Y ⊂ R, the corresponding discrete
Lipschitz class with Lipschitz constant K is defined as
L(G,Y, K) := {f : V (G)→ Y | ∀v1v2 ∈ E(G), |f (v1)− f (v2)| ≤ K} .
Such a definition provides a means to represent the intrinsically real-parameter optimization problems through
discretization (for practical computing devices). For instance, if a cube C ⊂ Rn is discretized uniformly into a set of grid
points, V (G) = {x1, x2, . . . , xM}n ⊂ Rn with xi+1 − xi = u > 0, we can let E(G) = {vivj | vi, vj ∈ V (G) and ‖vi − vj‖1 = u}.
L(G,Y, K) then forms a class containing all functions, defined on C differentiable with the absolute values of partial
derivatives upper-bounded by K/u, discretized over V (G). Furthermore, sinceY is bounded, this class contains all functions
mapping V (G) to Y with sufficient large K (e.g., K = maxY −minY).
The simplest case of DLC is the class of functions defined on R, in which the graph representing the spatial structure is a
simple path. Fig. 1 gives an illustrative example of such functions.
Definition 5 (Pathwise Discrete Lipschitz Class, PDLC). Given a finite set Y ⊂ R and a simple path G = v1v2 . . . vn, the
pathwise discrete Lipschitz class with Lipschitz constant K is defined asL(G,Y, K).
3.2. Combinatorial optimization problems and DLC
Because DLC is discrete by definition, if a combinatorial problem can be represented in the form of its solutions, we can
then consider it an instance in DLC by specifying the neighborhood structure and the Lipschitz constant. For example, the
search space of the Traveling Salesman Problem (TSP, see, e.g., [21]) consists of all possible tours, i.e., Hamiltonian cycles.
For each tour, there is a corresponding edge set containing all the edges through which the tour passes. Therefore, we can
Author's personal copy
1618 P. Jiang, Y.-p. Chen / Theoretical Computer Science 412 (2011) 1614–1628
Proof. By Lemma 10, it is sufficient to show thatL(G,Y, K) is closed under permutation if and only if G is complete.
• If G is complete, for every f ∈ L(G,Y, K), we have |f (vi)− f (vj)| ≤ K for all vi and vj ∈ V (G). For any permutation π on
X and for all vi and vj ∈ V (G), |fπ (vi)− fπ (vj)| = |f (π(vi))− f (π(vj))| ≤ K . Therefore, fπ ∈ L(G,Y, K).• If L(G,Y, K) is closed under permutation, suppose for contradiction that G is not complete. The incompleteness and
connectivity of G imply that there exist vi and vj ∈ V (G) with vivj /∈ E(G). Select vk ∈ N(vi), where N(vi) is the
neighborhood of vi. Obviously, vk ≠ vj. Consider the function f ∈ L(G,Y, K):
f (v) =

0 if v = vi;
K + 1 if v = vj;
K otherwise.
and the permutation π :
π(v) =

vk if v = vj;
tvj if v = vk;
v otherwise.
|fπ (vk)− fπ (vi)| = |f (π(vk))− f (π(vi))| = |f (vj)− f (vi)| = K + 1, so fπ /∈ L(G,Y, K), a contradiction. 
The condition of completeness implies that the NFL theorem holds over DLC only in the extreme case that the entire search
space is in the same neighborhood. While such a situation is theoretically possible, yet somewhat trivial. Taking PDLC as an
example, whenm > K , the NFL theorem sustains over a PDLC only if there are merely two vertexes in the search space.
4. DLC and subthreshold-seeker
The subthreshold-seeker (STS), introduced by Whitley and Rowe [9] and proved to outperform random search on
uniformly sampled polynomials of one variable, is a metaheuristic that employs the threshold as a switch of local search.
In essence, it is a selective local search method as it conducts local search if a given condition is satisfied. In this section,
a generalization of subthreshold-seeker is firstly presented, and we will demonstrate that the generalized subthreshold-
seeker can outperform random search on DLC.
4.1. Generalized subthreshold-seeker
In Whitley and Rowe’s work, the subthreshold-seeker is an optimization algorithm aiming at functions with a one-
dimensional domain, i.e., functions defined on a subsetC ⊆ R. The subthreshold-seekerwill successively select a point from
the search space uniformly at random (u.a.r.) until a subthreshold point is encountered. Once encountering a subthreshold
point, the subthreshold-seeker will search through the quasi-basin where that subthreshold point resides. In Whitley and
Rowe’s definition, a quasi-basin is a set of contiguous points with objective values below the threshold. In other words, the
threshold is used to determine whether the subthreshold-seeker enters the local search phase, and the subthreshold-seeker
can be viewed as an optimizer with an exhaustively local search operator.
According to this point of view, we generalize the subthreshold-seeker to the extent that it is applicable to any function
of which the domain possesses a neighborhood structure as in Algorithm 1.
Algorithm 1 (Generalized Subthreshold-seeker).
procedure Subthreshold-seeker(X, Y, N : X→ 2X, f : X→ Y)
while the stopping criterion is not satisfied do
if Queue is not empty then
x ← Queue.pop();
else
Select x fromX u.a.r.
end if
if f (x) ≤ θ then
Queue.push(N(x))
end if
end while
end procedure
Following the NFL framework, the parts of selecting and pushing are both restricted to unvisited points. Such a task can
be achieved by a bookkeeping manner. Since the performance of an algorithm is judged by the performance vector, all
overheads other than function evaluations will not count under the NFL framework.
The only control parameter of the subthreshold-seeker is the threshold. The elegance of the subthreshold-seeker is that it
comprises the two fundamental operations of search heuristics, local search and global restart, and yet still stays in a simple
form.
Author's personal copy
1620 P. Jiang, Y.-p. Chen / Theoretical Computer Science 412 (2011) 1614–1628
=
−
(xi)Li=1∈CL
1 · Prob{(Xsi)Li=1 = (xi)Li=1}
+
−
(xi)Li=1 /∈CL
U − {xi ∈ (xi)Li=1 | f (xi) ≤ βα(f )}
|X| − L Prob{(Xsi)
L
i=1 = (xi)Li=1}
≥
−
(xi)Li=1∈XL
U − {xi ∈ (xi)Li=1 | f (xi) ≤ βα(f )}
|X| − L Prob{(Xsi)
L
i=1 = (xi)Li=1}
=
L−
k=0
U − k
|X| − LProb

L−
i=1
Isi = k

. (2)
Substituting into (1),
E

L+1−
i=1
Isi

≥
L−
k=0
kProb

L−
i=1
Isi = k

+
L−
k=0
U − k
|X| − LProb

L−
i=1
Isi = k

= U|X| − L +
|X| − L− 1
|X| − L
L−
k=0
kProb

L−
i=1
Isi = k

= U|X| − L +
|X| − L− 1
|X| − L E

L−
i=1
Isi

≥ U|X| − L +
|X| − L− 1
|X| − L E

L−
i=1
Iri

=
L−
k=0
kProb

L−
i=1
Iri = k

+
L−
k=0
U − k
|X| − LProb

L−
i=1
Iri = k

= E

L−
i=1
Iri

+
L−
k=0
E

IrL+1
 L−
i=1
Iri = k

Prob

L−
i=1
Iri = k

= E

L+1−
i=1
Iri

. (3)
Inequality (3) follows from the induction hypothesis. 
Furthermore, next theorem guarantees that for any f ∈ L(G,Y, K), if there exists a point above performance threshold and
the subthreshold-seeker ever enters the local search phase, the subthreshold-seeker will outperform random search strictly
in expectation according to the performance measure Ψα,f .
Theorem 13 (Strictly Better Performance of STS on DLC). Let L(G,Y = {0, 1, . . . ,m}, K) with m > K be a DLC. For all
f ∈ L(G,Y, K) and for every subthreshold-seeker STS with θ ≤ βα(f )− K satisfy:
(1) ∃v ∈ V (G) with f (v) > βα(f ), and
(2) ∃v ∈ V (G) with f (v) ≤ θ ,
E[Ψα,f (Sy(STS, f , L))] > E[Ψα,f (Sy(RS, f , L))] for all L ∈ [2, |X| − 1].
Proof. If there are no such functions inL(G,Y, K), the theorem holds vacuously. Otherwise, let f be any function satisfying
the two conditions and define ((Xsi, Ysi))Li=1, ((Xri, Yri))
L
i=1, Isi, Iri, U , and CL in the same way as in Theorem 12. We prove
by induction on L. When L = 2, since in the second step, the queue is nonempty if and only if f (Xsi) ≤ θ , C1 = {v ∈
V (G) | f (v) ≤ θ} ≠ ∅ by Condition (2). Therefore,
E[Is1 + Is2] = E[Is1] +
−
x∈X
E[Is2 | Xs1 = x]Prob{Xs1 = x}
= E[Is1] +
−
x:f (x)≤θ
1 · Prob{Xs1 = x} +
−
x:θ<f (x)≤βα(f )
U − 1
|X| − 1Prob{Xs1 = x}
+
−
x:f (x)>βα(f )
U
|X| − 1Prob{Xs1 = x}
Author's personal copy
1622 P. Jiang, Y.-p. Chen / Theoretical Computer Science 412 (2011) 1614–1628
For PDLC, since the maximum degree is upper-bounded by 2, the sufficient condition can be reduced to a simpler form.
Corollary 16. LetL(G,Y = {0, 1, . . . ,m}, K) be a PDLC. Given α ∈ (0, 1/2] and an integer C > 1 with CK + 1 ≤ m, if
|V (G)| > (1+ 2C)α−1,
then there exists a function f ∈ L(G,Y, K) such that E[Ψα,f (Sy(STS, f , L))] > E[Ψα,f (Sy(RS, f , L))] for all L with 2 ≤ L ≤
|X| − 1, where STS is a subthreshold-seeker with θ ∈ βα(f )− [K , CK ].
Proof. For any v0 ∈ V (G), |vo| + |{v ∈ V (G) | 1 ≤ dis(v, v0) ≤ C}| ≤ 1+ 2C . 
Combining Theorems 12 and 13, if we manage to set θ ≤ βα(f ) − K , the subthreshold-seeker will perform at least as
good as random search on a DLC. If the subthreshold-seeker has a chance to conduct local search, it will strictly outperform
random search. Corollaries 15 and 16 show that if d and C remain unchanged, we can obtain a DLC satisfying the conditions
by increasing |V (G)|, for α as a predefined performance threshold. In other words, with the same neighborhood structure,
if the capability of a subthreshold-seeker to sample a decent threshold is unaffected by the increasing domain size, which is
generally true and will be discussed later, then the conditions in Corollaries 15 and 16 hold with a sufficiently large domain.
Estimating θ within some range should be more practical than gauging a specific value such as βα(f ). In the next section,
we will explore this possibility and empirically confirm the theoretical results obtained in this section by proposing and
adopting a sampling-test scheme.
5. Sampling-test scheme
Conventionally, the effectiveness of an optimizer is examined via experiments on a suite of test functions that serves
as a benchmark. These test functions are selected according to some prior knowledge of the importance thereof. Here we
propose and adopt a different approach in order to confirm the theoretical results obtained in the previous section from
an empirical aspect. We draw a sample of functions randomly from PDLC in a manner similar to select respondents in a
campaign survey and conduct experiments on these sampled functions. There is no bias in favor of which functions should
be selected. We expect the arbitrariness delivers information about the composition of the problem class.
A uniform sampler for PDLC is firstly given in Section 5.1. Experiments are then presented to summarize this section and
demonstrate how the Lipschitz condition facilitates the search process in a practical standpoint.
5.1. A uniform sampler for PDLC
In order to conduct the sampling test, we need a uniform sampler in the first place. The following algorithm generates
problem instances of PDLC with Lipschitz constant K u.a.r.
Algorithm 2 (Uniform PDLC Sampler).
procedure Uniform PDLC Sampler(v1v2 . . . vn, Y = {0, 1, . . . ,m}, K )
f (v1)← Uniform([0,m])
i ← 2
while i ≤ n do
f (vi)← f (vi−1)+ Uniform([−K , K ])
if f (vi) > m or f (vi) < 0 then
f (v1)← Uniform([0,m])
i ← 1;
end if
i ← i+ 1
end while
return f
end procedure
HereUniform([a, b]) denotes the function that selects an integer u.a.r. from the closed interval [a, b]. Such a sampler belongs
to the category of accept–reject algorithms [25]. It generates a problem instance with bounded difference between any two
successive vertexes u.a.r., and if the instance at hand exceeds the range of the codomain, the sampler rejects the instance.
The accept–reject mechanism guarantees the uniformity. Once the sampler halts, the output is always an instance of the
PDLC.
Since this sampler is a Las Vegas algorithm, in which the answer is guaranteed to be correct but the resources used are
not bounded [26], we need to address its time complexity for the practicality. For each candidate instance, the sampler will
go through at most |X| steps to assign all the vertex objective values, so it remains to show howmany candidate instances
it takes to generate a legit instance successfully. The accept–reject process is geometrically distributed, and therefore the
expected number of instances consumed is the inverse of the acceptance probability. The following theorem provides an
upper bound for the rejection probability.
Author's personal copy
1624 P. Jiang, Y.-p. Chen / Theoretical Computer Science 412 (2011) 1614–1628
According to the assumption that Var[Sn−1]/(m+ 1)2 < 1,
Prob{rejection} < 4
√
Var[Sn−1] − 2Var[Sn−1]m + 5
2m+ 1
= 4

(n− 1)(K 2 + K)√
3(2m+ 1) −
2(n− 1)(K 2 + K)
3m(2m+ 1) +
5
2m+ 1
<
4

(n− 1)(K 2 + K)√
3|Y| −
4(n− 1)(K 2 + K)
3|Y|2 +
5
|Y| . 
Theorem 18 (Upper Bound for the Rejection Probability). Define m := ⌊(|Y| − 1)/2⌋. If m > (n− 1)(K 2 + K)/3 ≥ 2 , then
the rejection probability is less than
4

(n− 1)(K 2 + K)√
3|Y| −
4(n− 1)(K 2 + K)
3|Y|2 + O
|Y|−1 .
Proof. If |Y| = 2m + 1, then we are done by the previous lemma. Otherwise, if |Y| = 2m + 2, without loss of generality,
suppose that Y = {−m,−m+ 1, . . . ,m+ 1} and let Y′ = {−m,−m+ 1, . . . ,m}. Therefore,
Prob{rejection} = Prob{f (v1) ∈ Y′}Prob{rejection | f (v1) ∈ Y′}
+ Prob{f (v1) /∈ Y′}Prob{rejection | f (v1) /∈ Y′}
=

2m+ 1
2m+ 2

Prob{rejection | f (v1) ∈ Y′}
+

1
2m+ 2

Prob{rejection | f (v1) = m+ 1}.
When f (v1) ∈ Y′, if f exceeds the range of Y, then f also exceeds the range of Y′, so from the previous lemma we have
Prob{rejection | f (v1) ∈ Y′} < 4

(n− 1)(K 2 + K)√
3(2m+ 1) −
4(n− 1)(K 2 + K)
3(2m+ 1)2 +
5
2m+ 1 .
As a result,
Prob{rejection} ≤

2m+ 1
2m+ 2

Prob{rejection | f (v1) ∈ Y′} +

1
2m+ 2

<
4

(n− 1)(K 2 + K)√
3(2m+ 2) −
4(n− 1)(K 2 + K)
3(2m+ 1)(2m+ 2) +
6
2m+ 2
<
4

(n− 1)(K 2 + K)√
3|Y| −
4(n− 1)(K 2 + K)
3|Y|2 + O
|Y|−1 . 
Corollary 19. If |Y| = C(n− 1)(K 2 + K) > C · 2√3 for some constant C ≥ √3, then the rejection probability is less than
4
√
3C − 4
3C2
+ O |Y|−1 .
Proof. If C ≥ √3,
m =
 |Y| − 1
2

≥ |Y|
2
− 1
≥

3(n− 1)(K 2 + K)
2
− 1
=

(n− 1)(K 2 + K)
3
+

(n− 1)(K 2 + K)
2
√
3
− 1
>

(n− 1)(K 2 + K)
3
.
Substituting

(n− 1)(K 2 + K)/|Y| by 1/C and applying Theorem 18, the corollary is proved. 
Author's personal copy
1626 P. Jiang, Y.-p. Chen / Theoretical Computer Science 412 (2011) 1614–1628
Table 1
Successful rate.
θ Category (|X|, |Y|)
(104, 104) (105, 105) (106, 106)
γ > 0.9995 (2,049) 0.9985 (2,047) 0.9976 (2,045)
> .2 0.9951 (2,040) 0.9620 (1,972) 0.9624 (1,973)
γˆ > 0.9995 (2,049) 0.9990 (2,048) 0.9985 (2,047)
> .2 0.9937 (2,037) 0.9620 (1,972) 0.9732 (1,995)
µˆ− σˆ > 1.0000 (2,050) 1.0000 (2,050) 1.0000 (2,050)
> .2 1.0000 (2,050) 1.0000 (2,050) 1.0000 (2,050)
γ :median. γˆ : estimatedmedian. µˆ: estimatedmean. σˆ : estimated standard deviation.
‘‘>": proportion of instances where the subthreshold-seeker outperforms random
search. ‘‘> .2": proportion of instances where the subthreshold-seeker outperforms
random search by a 20% margin.
Table 2
Mean time steps to locate the minimum.
Algorithm (|X|, |Y|)
(104, 104) (105, 105) (106, 106)
STS, θ = γ 2037.58 22913.23 229232.26
STS, θ = γˆ 2221.44 23170.58 229532.04
STS, θ = µˆ− σˆ 918.29 8095.78 80322.92
random search 4972.50 49724.74 496912.49
γ : median. γˆ : estimatedmedian. µˆ: estimatedmean. σˆ : estimated
standard deviation.
The sampler generates 2,050 instances of PDLCwith (|X|, |Y|) = (104, 104), (105, 105), and (106, 106), respectively. The
Lipschitz constant K is set to 100 for the concern of execution time, as previously discussed. For each problem instance, we
test each algorithm for 50 independent runs. If the average time of a subthreshold-seeker to find the optimum is less than
that of randomsearch, the instance is counted as a success.We also count the number of instances that a subthreshold-seeker
outperforms random search by a 20%margin, i.e., the instancewhere the average optimization time of a subthreshold-seeker
is less than 80% of that of random search. Table 1 displays the empirical results.
All three subthreshold-seekers outperform random search in most of the sampled problem instances. Furthermore, the
subthreshold-seeker with θ = µˆ− σˆ outperforms random search in all 2,050 instances sampled, evenwith the requirement
of a 20% margin. The statistical significance of such results is obvious to see: suppose the population proportion that
the subthreshold-seeker with θ = µˆ − σˆ outperforms random search is q. To obtain the result that random search is
outperformed in all instances, the probability is q2050. Even if q is as high as 0.995, the above probability is just 0.000034. To
more formally rephrase, if the null hypothesis is ‘‘q ≤ 0.995’’, the p-value is merely 0.000034.
Table 2 displays the averaged optimization time over the 2,050 sampled problem instances. The subthreshold-seeker
with θ = µˆ − σˆ outperforms others by a significant margin. Random search averages approximately |X|/2 to find the
minimum, which is expected. The subthreshold-seeker using the actual median and the one using the sample median both
take about half time steps of that needed by random search to optimize the function.
The subthreshold-seekerswith θ = µˆ−σˆ and θ = γˆ are indeed black-box algorithms, for there is no exterior knowledge
exerted and the only information they can use are function evaluations, but they outperform random search by a remarkable
difference.
The performance difference between θ = γˆ and θ = γ is insignificant. Such a result suggests that in this case, an
estimation of median may be adequate. Suppose that P with |P| = N is a subset of real numbers, and for all i ∈ P , R(i) is
defined to be the rank (i.e., ordering) of i in P . For instance, R(min P) = 1 and R(max P) = N . For simplicity, we assume
that N is odd and hence the median of P is the element i with R(i) = ⌈N/2⌉. Now we want to estimate the median of
P . If a point sample S of size n, where n is assumed odd, is drawn by successively selecting an element u.a.r. from P with
replacement, the estimated median, γ , is presumed to be the sampled median, and we want the error is bounded by ϵ > 0,
i.e., |R(γ )− ⌈N/2⌉ | ≤ ϵN .
If R(γ ) < ⌈N/2⌉ − ϵN , there are at least ⌈n/2⌉ selections with ranks less than ⌈N/2⌉ − ϵN . Let Xi be the indicator
variable that indicates if the ith selection is less than ⌈N/2⌉ − ϵN , Xi = 1 with probability p := (⌈N/2⌉ − ⌊ϵN⌋ − 1)/N .
R(γ ) < ⌈N/2⌉ − ϵN if and only if ∑ni=1 Xi ≥ ⌈n/2⌉. Since E[∑ni=1 Xi] = np, applying another form of Hoeffding’s
inequality [28], we have
Prob

R(γ ) <

N
2

− ϵN

= Prob

n−
i=1
Xi ≥
n
2

Author's personal copy
1628 P. Jiang, Y.-p. Chen / Theoretical Computer Science 412 (2011) 1614–1628
[7] C. Igel, M. Toussaint, A no-free-lunch theorem for non-uniform distributions of target functions, Journal of Mathematical Modelling and Algorithms 3
(4) (2004) 313–322.
[8] S. Christensen, F. Oppacher,What canwe learn from no free lunch? a first attempt to characterize the concept of a searchable function, in: Proceedings
of the Genetic and Evolutionary Computation Conference 2001, 2001, pp. 1219–1226.
[9] D. Whitley, J. Rowe, Subthreshold-seeking local search, Theoretical Computer Science 361 (2006) 2–17.
[10] H. Mülenbein, How genetic algorithms really work i. mutation and hillclimbing, in: Proceedings of the Parallel Problem Solving from Nature, 2, 1992,
pp. 15–26.
[11] S. Droste, T. Jansen, I. Wegener, On the analysis of the (1+ 1) evolutionary algorithm, Theoretical Computer Science 276 (2002) 51–81.
[12] J. He, X. Yao, Towards an analytic framework for analysing the computation time of evolutionary algorithms, Artificial Intelligence 145 (1–2) (2003)
59–97.
[13] P.S. Oliveto, C. Witt, Simplified drift analysis for proving lower bounds in evolutionary computation, in: Proceedings of Parallel Problem Solving from
Nature, 2008, pp. 82–91.
[14] O. Giel, I. Wegener, Evolutionary algorithms and the maximum matching problem, in: Proceedings of the 20th Annual Symposium on Theoretical
Aspects of Computer Science, 2003, pp. 415–426.
[15] F. Neumann, I. Wegener, Randomized local search, evolutionary algorithms, and the minimum spanning tree problem, Theoretical Computer Science
378 (1) (2007) 32–40.
[16] T. Jansen, I.Wegener, A comparison of simulated annealingwith a simple evolutionary algorithmonpseudo-boolean functions of unitation, Theoretical
Computer Science 386 (1–2) (2007) 73–93.
[17] A. Auger, O. Teytaud, Continuous lunches are free!, in: Proceedings of the Genetic and Evolutionary Computation Conference 2007, 2007, pp. 916–922.
[18] J. Rowe, M. Vose, A. Wright, Reinterpreting no free lunch, Evolutionary Computation 17 (1) (2009) 117–129.
[19] C. Schumacher, M.D. Vose, L.D. Whitley, The no free lunch and problem description length, in: Proceedings of the Genetic and Evolutionary
Computation Conference 2001, 2001, pp. 565–570.
[20] R. Courant, F. John, Introduction to Calculus and Analysis, Vol. 1, Springer-Verlag, 1989.
[21] T.H. Cormen, C.E. Leiserson, R.L. Rivest, C. Stein, Introduction to Algorithms, 2nd edition, The MIT Press, 2001.
[22] R. Motwani, P. Raghavan, Randomized Algorithms, Cambridge University Press, 1995.
[23] I. Rechenberg, Evolutionsstrategie ’94, Frommann Holzboog, 1994.
[24] J. Jägersküpper, Algorithmic analysis of a basic evolutionary algorithm for continuous optimization, Theoretical Computer Science 379 (3) (2007)
329–347.
[25] C. Robert, G. Casella, Monte Carlo Statistical Methods, Springer-Verlag, 1999.
[26] Mikhail J. Atallah (Ed.), Algorithms and Theory of Computation Handbook, CRC Press LLC, 1999, ISBN-10: 0849326494, ISBN-13: 978-0849326493.
[27] Y.S. Chow, H. Teicher, Probability Theory: Independence, Interchangeability, Martingales, 3rd edition, Springer, 1997.
[28] W. Hoeffding, Probability inequalities for sums of bounded random variables, Journal of the American Statistical Association 58 (1963) 13–30.
tags on decoding error probability of LT codes. Based on the
observations, a sparse degree selection algorithm is proposed
to define appropriate tags, and the optimal distribution formed
by the selected tags may get close to the global optimal.
The remainder of the paper is organized as follows. Sec-
tion II gives the details of coding mechanism of LT codes and
introduces two evaluation approaches for degree distributions.
Section III investigates the probability reallocation of degree
distributions and presents the influences of different tags. In
section IV, our selection algorithm is proposed and described
in detail. Several optimization results with different parameter
settings are presented in section V to examine the selection
algorithm. Finally, the conclusion and our contribution are
given in section VI.
II. LT CODES
Before describing main work of the paper, the operation of
LT codes is introduced in this section as a background. The
encoding and decoding procedures are given in section II-A.
Source data in general are divided into k fragments with an
identical length. These fragments are bit nodes or called input
symbols if the length of each fragment is only a bit. Similarly
encoding symbols denote the codewords generated by the
encoding procedure in LT codes. For a clear presentation, the
terms, input symbols, and encoding symbols, are consistently
used in this paper. Section II-B introduces soliton degree
distributions, which were proposed according to a theoretical
analysis. Furthermore, because evaluating the quality of degree
distributions is necessary in optimization, section II-C intro-
duces two different approaches for evaluating the decoding
error rate of LT codes for a given degree distribution.
A. Encoding and Decoding
Given the source data, we suppose that the source data
are cut into k input symbols. Before an encoding symbol is
generated, a degree d is chosen at random according to the
adopted degree distribution Ω(d), where 1 ≤ d ≤ k and∑k
d=1 Ω(d) = 1. The degree d decides how many distinct
input symbols are involved to compose an encoding symbol.
Then d input symbols, also named neighbors, are chosen
uniformly at random and accumulated by XOR. In the design
of LT codes, random number plays an essential role in the
encoding process. The approach employed by LT codes for a
sender to inform receivers of all information is to synchronize
a random number generator with a specified seed.
At the receiver side, when n, which is usually slightly larger
than k, encoding symbols arrive, belief propagation is used to
reconstruct the source data step by step. All encoding symbols
are initially covered in the beginning. In the first step, all
encoding symbols with only one neighbor can be directly
released to recover their unique neighbor. For an input symbol
that has been recovered but not processed yet, it is called a
ripple and will be pushed into a queue. At each subsequent
step, ripples are popped from the queue as a processing target
one by one. A ripple is removed from all encoding symbols
that have it as a neighbor. If an encoding symbol has only
one remaining neighbor after removing, the releasing action
repeats and may produce new ripples to maintain a stable
size of the queue. Maintaining the size of the ripple queue is
important because the decoding process fails when the ripple
queue becomes empty with uncovered input symbols. In other
words, more encoding symbols are required to continue the
decoding process. The decoding process succeeds if all input
symbols are recovered at the end.
B. Soliton distribution
The coding behavior of LT codes is determined by the
degree distribution, Ω(d), and the number of received encoding
symbols, n. Reception overhead, ε = n/k, denotes the delivery
efficiency of LT codes, and ε depends on the given degree
distribution. Based on the theoretical analysis, Luby proposed
the ideal soliton distribution which can achieve the best
performance, ε = 1, in the ideal case.
Ideal soliton distribution ρ(d):
ρ(d) =
{ 1
k
for d = 1
1
d(d−1) for d = 2, 3, . . . , k
. (1)
Ideal soliton distribution guarantees that all the release proba-
bilities are identical to 1/k at each decoding step. Hence, there
is exactly one expected ripple generated at each step when the
encoding symbol size is k. After k processing step, ideally,
the source data can be fully recovered.
However, ideal soliton distribution works poorly in practice.
Belief propagation may be suspended by a small variance
of the stochastic encoding/decoding situation in which no
ripple exists because the expected ripple size is only one at
any moment. According to the theory of random walk, the
probability that a random walk of length k deviates from its
mean by more than ln(k/δ)
√
k is at most δ. It is a baseline of
ripple size that must be maintained to complete the decoding
process. Therefore, in the same paper by Luby, a modified
version called robust soliton distribution was also proposed.
Robust soliton distribution µ(d):
S = c ln(k/δ)
√
k ,
τ(d) =


S/dk for d = 1, . . . , k/S − 1
S ln(S/δ)/k for d = k/S
0 for d = k/S + 1, . . . , k
, (2)
β =
k∑
d=1
(ρ(d) + τ(d)) , (3)
µ(d) =
ρ(d) + τ(d)
β
for d = 1, . . . , k , (4)
where c and δ are two parameters for controlling the character
of a robust soliton distribution. c controls the average degree
of the distribution, and δ estimates that there are ln(k/δ)
√
k
expected ripples as aforementioned. Robust soliton distribution
can ensure that only n = k + O(ln2(k/δ)√k) encoding
symbols are required to recover the source data successfully
with a probability at least 1-δ.
2464
Algorithm 1 Tags Selection Function
Input: The source symbol size k, the density parameter d;
Output: The set of sparse tags;
1: procedure TSF(k, d)
2: D ← Ideal soliton distribution for size k;
3: S ← 0, E ← 1, Tags← [];
4: while i < k do
5: S ← S + E ×D(i)
6: if S > (1/d) then
7: Tags← [Tags, i];
8: S ← 0;
9: E ← 1;
10: else
11: E ← E + 1;
12: end if
13: end while
14: return vector [1, Tags, k];
15: end procedure
observed by comparing the selections of (a, b) = (a1, 4 + i)
to (a, b) = (a1, 4 − i), e.g. comparisons between (1, 2) and
(1, 6). The distance influence of such two pairs are the same
but the pair (1, 6) with complementary property observably
has a smaller difference.
Given the above considerations, it turns out that the reallo-
cation of the probability to degrees 3 and 5, the nearest two
degrees to 4, would be the most close to the optimum. For
a better view of showing how close the adjusted distributions
could approach to the optimal one, we illustrate the differences
of error probabilities in logarithm scale in Figure 1(b). It
also gives the proof that the error probability of the original
distribution could be well approximated by the reallocated one.
These observations inspire us regarding how to choose the tags
for a sparse degree distribution.
IV. SELECTION FUNCTION FOR SPARSE TAGS
In addition to the factors observed from experiments, we
also take some intuitive properties into account. For example,
the higher probability to be reallocated would result in higher
difference of the error probability to the optimal one. Summing
up all of above, the main considerations of our degree selection
strategy for sparse tags would be as follows:
1) The number and value of probabilities around each
degree.
2) The distance between the probability reallocated degrees
and the removed one.
The first criterion comes from the positive correlation of the
reallocated probability and the error probability. The second
one accounts for the results of the experiments that replacing
the tag to be removed with two adjacent tags would have the
best approximation for error probability. According to these
criterion, we propose the sparse tags selection function in
Algorithm 1.
We consider the ideal soliton distribution since it would be
the optimal degree distribution in the ideal case. The density
5 10 15 20 25 300
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Degrees
Pr
ob
ab
ili
ty
 
 
Ideal soliton distribution (k=30)
TSF(30,10)
Fig. 2. Illustration of the work done by TSF(k, d).
parameter d acts as the bound that base on the first main con-
sideration, degree i would not be removed if its probability was
larger than 1/d. On the other hand, we group the degrees with
probabilities below 1/d and concentrate those probabilities to
a nearby degree. The second main consideration would be
applied to the selection of the representative degree of each
group. We accumulate the probabilities multiplied the distance
factors and take the degree while the sum exceeds the bound
1/d. In addition, considering the complementary property of
the selected degrees to distribute the probability, we reserve the
degrees 1 and k to ensure there always exists degrees satisfied
this property to be chosen. Figure 2 illustrates the work done
by Tags Selection Function (TSF) and shows the tags selected
for (k, d) = (30, 10). Tags 1 and 30 were selected to meet
the complementary property. The tags 2, 3 were selected since
the probabilities of these tags in ideal soliton distribution
were above the density criteria. The remaining tags were the
representations of the grouped tags of which probabilities were
below the density bound.
Following we provide some examples of sparse tags selected
by Algorithm 1 for k = 100:
1) TSF(100, 1) = [1, 4, 23, 100].
2) TSF(100, 3) = [1, 2, 5, 12, 30, 76, 100].
3) TSF(100, 5) = [1, 2, 4, 8, 16, 32, 64, 100].
4) TSF(100, 10) = [1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 100].
5) TSF(100, 20) = [1, 2, 3, 4, 6, 8, 11, 15, 21, 29, 40, 56,
78, 100].
We can see that the sparse tags selected in TSF(100, 5) were
close to the series of power of 2 and those selected in
TSF(100, 10) were close to Fibonacci series. Such a result
gives an explanation for the good performance of choosing
these series, power of 2 and the Fibonacci series, as approxi-
mation to the full tags in a certain extent.
2466
100 150 200 250 300
10−7
10−6
10−5
10−4
10−3
k sizes
Er
ro
r p
ro
ba
bi
lit
y 
di
ffe
re
nc
e
 
 
d = 3
d = 5
d = 10
d = 20
Full tags
Fig. 4. Differences between the error probabilities and the minimal one.
considered to have global minimal error probability because it
is the universal set of all distributions and forms the complete
search space. However, the optimization result of full degree
distributions gets worse and worse as input symbol size k
increases. For the same optimization algorithm and evaluation
function are implemented for each degree set, the worse results
of full degree distributions could be explained by that the
number of decision variables is too many for CMA-ES to
handled in limited function evaluations.
VI. CONCLUSION
Using evolutionary algorithms to optimize the degree distri-
bution for LT codes is a promising research topic. Sparse de-
gree distributions are frequently used to replace full degrees for
reducing the search space. How good the performance could
be achieved by a sparse degree distribution depends on the set
of its non-zero entries, i.e., tags. However, no investigation
has been done regarding how to decide appropriate tags to
construct sparse degree distributions with good performance.
In this paper, the authors analyzed the influence of different
degrees on decoding rate and proposed a tag selection algo-
rithm to choose tags for LT codes optimization. Finally, the
presented experimental results were evidentially illustrate the
practicality of the proposed tag selection algorithm.
In previous studies, researchers manually chose tags for
sparse degree distributions according to their own experimental
experience. Even though the chosen subset of degrees worked
well, the detailed mechanism was still unknown. This work
made an effort to find out guidelines for choosing appropriate
tags. The proposed selection algorithm can be applied for any
input size and control the level of sparseness conveniently
by adjusting the density parameter. This solution can help
researchers to put more attention in the optimization algorithm
rather than the individual encoding. The paper presented the
qualitative analysis of probability reallocation in a distribution.
The variances of error probability were compared for changing
the reallocated degree and then quantitative analysis is needed
in advance. If the quantity of variance can be measured
precisely, developing a local search based on the measure
approach to enhance certain optimization framework for LT
codes will be possible. Research of this line is definitely worth
pursuing, and the authors are currently taking the challenge.
ACKNOWLEDGMENTS
The work was supported in part by the National Science
Council of Taiwan under Grant NSC 99-2221-E-009-123-
MY2.
REFERENCES
[1] J. W. Byers, M. Luby, M. Mitzenmacher, and A. Rege, “A digital
fountain approach to reliable distribution of bulk data,” in Proceedings
of the ACM SIGCOMM ’98 conference on Applications, technologies,
architectures, and protocols for computer communication. ACM, 1998,
pp. 56–67.
[2] J. W. Byers, M. Luby, and M. Mitzenmacher, “A digital fountain
approach to asynchronous reliable multicast,” IEEE Journal on Selected
Areas in Communications, vol. 20, no. 8, pp. 1528–1540, 2002.
[3] M. Luby, “LT codes,” in Proceedings of the 43rd Symposium on
Foundations of Computer Science. IEEE Computer Society, 2002, pp.
271–282.
[4] E. A. Bodine and M. K. Cheng, “Characterization of Luby Transform
codes with small message size for low-latency decoding,” in Proceedings
of the IEEE International Conference on Communications, 2008, pp.
1195–1199.
[5] E. Hyytia¨, T. Tirronen, and J. Virtamo, “Optimal degree distribution for
LT codes with small message length,” in Proceedings of the 26th IEEE
International Conference on Computer Communications (INFOCOM
2007), 2007, pp. 2576–2580.
[6] ——, “Optimizing the degree distribution of LT codes with an impor-
tance sampling approach,” in Proceedings of the 6th InternationalWork-
shop on Rare Event Simulation (RESIM 2006), 2006, pp. 64–73.
[7] C.-M. Chen, Y.-p. Chen, T.-C. Shen, and J. K. Zao, “On the optimization
of degree distributions in LT code with covariance matrix adaptation evo-
lution strategy,” in Proceedings of the IEEE Congress on Evolutionary
Computation, 2010, pp. 3531–3538.
[8] ——, “Optimizing degree distributions in LT codes by using the multiob-
jective evolutionary algorithm based on decomposition,” in Proceedings
of the IEEE Congress on Evolutionary Computation, 2010, pp. 3635–
3642.
[9] A. Talari and N. Rahnavard, “Rateless codes with optimum interme-
diate performance,” in Proceedings of the Global Telecommunications
Conference (GLOBECOM 2009), 2009, pp. 1–6.
[10] R. Karp, M. Luby, and A. Shokrollahi, “Finite length analysis of
LT codes,” in Proceedings of the IEEE International Symposium on
Information Theory 2004 (ISIT 2004), 2004, p. 39.
[11] E. Maneva and A. Shokrollahi, “New model for rigorous analysis of
LT-codes,” in Proceedings of the IEEE International Symposium on
Information Theory (ISIT 2006), 2006, pp. 2677–2679.
[12] N. Hansen and A. Ostermeier, “Adapting arbitrary normal mutation
distributions in evolution strategies: the covariance matrix adaptation,”
in Proceedings of the IEEE International Conference on Evolutionary
Computation, 1996, pp. 312–317.
[13] A. Auger and N. Hansen, “Performance evaluation of an advanced
local search evolutionary algorithm,” in Proceedings of the 2005 IEEE
Congress on Evolutionary Computation (CEC 2005), 2005, pp. 1777–
1784.
[14] ——, “A restart CMA evolution strategy with increasing population
size,” in Proceedings of the 2005 IEEE Congress on Evolutionary
Computation (CEC 2005), 2005, pp. 1769–1776.
2468
archetype of memetic algorithms but also consists with the
empirical results in quite a few studies in the literature, further
investigation on the effect of problem landscapes and how the
explorability of global search and the exploitability of local
search affect the optimization performance is possible.
In this work, as practical problems generally exhibit some
degree of continuity rather than distributions of different sets
of stochastic points described in QBCs, we further extend the
QBC to Discrete Lipschitz Quasi-Basin Class(DLQBC) which
categorizes Lipschitz continuous problems according to the
number of basins and the roughness of landscape. Then we
apply several representative archetypes of memetic algorithms
on different DLQBCs to investigate the collaborative behavior
of memetic algorithms exhibiting different explorability and
exploitability on different problem landscapes. As the em-
pirical results of this framework not only consist with the
theoretical model proposed in our previous work but also well
delineate how different types of memetic algorithms behave on
different problems, this work may shed light into the design
of memetic algorithms.
The rest of this paper is arranged in the following manner.
We firstly introduce the framework of subthreshold seeker on
QBCs as the basis of this study in Section II, then provide the
definition of Discrete Lipschitz Quasi-Basin Class (DLQBC)
and its sampling test scheme in Section III, and propose
several representative archetypes of memetic algorithms for
investigation in Section IV. The empirical results of the
proposed representative archetypes memetic algorithms on
different DLQBCs are presented and discussed in Section V.
Finally, we conclude this paper in Section VI.
II. SUBTHRESHOLD SEEKER ON QUASI-BASIN CLASSES
In this section, the concept of local search zones and the
theoretical model proposed in our previous work [9] are briefly
reviewed to form the basis of this study.
A. Quasi-Basin Classes
In [9], an optimization problem is to optimize a given
objective function f : X → Y , and the optimization goal
is to find x∗ ∈ X with the minimum value y∗ ∈ Y . The X is
assumed a finite set as optimization problems are generally
numerically solved on digital computers. To simplify the
derivation, every function maps different x ∈ X to different
y ∈ Y is also assumed. For generality, the search space is
interpreted as a graph viewed by an optimization algorithm.
In this interpretation, the vertices are the set of points of X
and the edges are the set of pairs of points which are neighbors
viewed by an optimization algorithm. The terms X and V (G)
are used exchangeably in the following text.
Based on the fundamental definition of the search space, the
local search zones are defined as regions where local search
prefers and are virtually determined by the adopted local
search criterion and the landscape of the search space. Thus,
frameworks based on the concept of local search zones are
especially suitable for investigating the collaboration between
global search and local search. As local search zones are gen-
erally hard to measure, quasi-basins are used to approximate
local search zones. Given a threshold βm(f), which forms a
set Sm(f) consisting m vertices with their objective values
smaller than or equal to the threshold, a quasi-basin (QB) is
defined as a maximal connected subset in Sm(f). Accordingly,
the Quasi-Basin Class(QBC) is proposed to define a class of
problems with m subthreshold points, points with objective
values that are smaller than or equal to βm(f), distributed
among b distinct quasi-basins. As the vertices residing in
quasi-basins are better than the other vertices in the search
space and are favored by fitness-relevant local search criteria,
QBCs categorize the problems according to their distribution
of quasi-basins which is conceptually mapping to the distri-
bution of local search zones.
B. Subthreshold Seeker
To illustrate how the distribution of local search zones
affects the performance of a memetic algorithm, [9] adopts
Subthreshold Seeker (SS) as a representative archetype of
memetic algorithms. SS is a simplistic, minimal optimization
algorithm that coordinates random global search and exhaus-
tive local search according to a threshold. It explores the search
space by uniformly randomly sampling the search space. When
a subthreshold point is encountered by global search, SS
exploits the quasi-basin defined by the local search threshold
where the encountered subthreshold point resides in. In other
words, the exhaustive local search will eventually visit all the
points in the encountered quasi-basin. After local search in the
quasi-basin is done, SS continues to global search until another
subthreshold point is encountered. This switching between
global search and local search proceeds until the stopping
criterion is satisfied.
C. Theoretical Model of SS on QBCs
Now we are ready for the theoretical model of SS on
QBCs. Given a QBC Q(G,Y,m, b), the theoretical expected
samplings, T , for SS, with its local search threshold set to
βm(f), to find the minimum of a function in the QBC is
derived in [9] as
T =
cN
m
⌈
b
2
⌉
+
m+ 1
2
, (1)
where N denotes |G|, the size of G, and c denotes a parameter
between 0.75 and 1.5. The first term and the second term of
this formula are corresponding to the global search time and
the local search time of SS respectively. The global search
time of SS depends on the expected time for the random
search to find one of the m subthreshold points in N points.
It is inversely proportional to m and dramatically drops as
m increases. On the other hand, the local search time of
SS linearly scales with m as SS employs exhaustive local
search. Summing up the global search time and the local
search time, a v-shape of the expected evaluation time along
the m-axis with its optimal setting of m around
√
bcN can
be obtained. This theoretical model suggests that applying
2717
0 200 400 600 800 1000
0
200
400
600
800
1000
x
y
 
 
(a) b5-easy
0 200 400 600 800 1000
0
100
200
300
400
500
600
x
y
 
 
(b) b5-hard
Fig. 2. Instances of PDLQBCs
and hard denote that mo of the PDLQBC is set to 1 and 0.5
respectively. Fig. 2 illustrates two instances of PDLQBCs.
IV. REPRESENTATIVE ARCHETYPES OF MEMETIC
ALGORITHMS
In this framework, we investigate how the explorability and
exploitability of a memetic algorithm affect its performance.
In our perspective, according to the degree of explorability
and exploitability, basic optimization algorithms are mainly of
three types: random search, heuristic search, and exhaustive
search. Here we refer to pure random sampling as random
search, refer to sampling according to some rules as heuristic
search, and refer to sampling all the vertices as exhaustive
search. Accordingly, we investigate representative archetypes
of memetic algorithms consisting of different combinations of
random search, heuristic search, and exhaustive search.
A. Nelder-Mead Method
As heuristic search algorithms are various in forms, we
adopt the Nelder-Mead method (NM) as a representative
heuristic search algorithm. The Nelder-Mead method utilizes
the concept of a simplex, which is a special polytope of
N + 1 vertices in N dimensions, and features its capability
of approximating a local optimum of a problem with N
variables when the objective function varies smoothly and is
unimodal. As our sampling test scheme samples instances of
PDLQBC which are fundamentally one dimension problems,
we adopt the 1-D Nelder Mead Algorithm shown in Fig. 3.
The Uniform(X , 2) indicates a uniformly random sampling
of two vertices from the search space X . To avoid stagnation
during a global search, when x1 and x2 appear to be the same
vertex, we re-sample x1 and x2. Note that when this Nelder-
Mead method is utilized as a local search algorithm in the
following text, the stop criterion is set to the convergence of
the simplex. In other words, it will stop when x1 and x2 appear
to be the same vertex.
B. Memetic Algorithm Types
Fig. 4 illustrates the pseudo code of a generalized memetic
algorithm. The S is the resultant set of vertices of the current
global search and the reference for the next global search. If
one of the vertex si in S satisfies the local search criterion,
local search will be performed on the point. This process
1: procedure 1-D NELDER MEAD ALGORITHM(X , Y , f :
X → Y)
2: {x1, x2} ← Uniform(X , 2)
3: while the stopping criterion is not satisfied do
4: {x1, x2} ← Sort({x1, x2})
5: xr ← x1 + (x1 − x2)
6: if f(xr) < f(x1) then
7: xe ← x1 + 2(x1 − x2)
8: if f(xe) < f(x1) then
9: x2 ← xe
10: else
11: x2 ← xr
12: end if
13: else
14: xc ← x2 + (x1 − x2)/2
15: x2 ← xc
16: end if
17: if x1 = x2 then
18: {x1, x2} ← Uniform(X , 2)
19: end if
20: end while
21: end procedure
Fig. 3. 1-D Nelder-Mead Algorithm
1: procedure GENERALIZED MEMETIC ALGORITHM(X , Y ,
f : X → Y)
2: S ← Initialization()
3: while the stopping criterion is not satisfied do
4: S ← GlobalSearch(S,X ,Y, f : X → Y)
5: for si ∈ S do
6: if local search criterion is satisfied then
7: si ← LocalSearch(si,X ,Y, f : X → Y)
8: end if
9: end for
10: end while
11: end procedure
Fig. 4. A generalized memetic algorithm.
will continue until the stopping criterion is satisfied. In this
framework, we set the stopping criterion to the sampling of
the global optima, i.e., the minimum. To illustrate how local
search criteria affect the collaboration between global search
and local search based on the concept of local search zones, we
set the local search criterion as the subthreshold seeker does.
Thus, local search will be performed on any vertex si in S with
an objective value f(si) less than or equal to βm(f), the local
search threshold, and we can then investigate the behavior of
the subject memetic algorithms based on the model in [9].
To identify representative archetypes of memetic algorithms
for investigation, we firstly categorize all optimization algo-
rithms into three types: random search, heuristic search, and
exhaustive search. Among these three types, heuristic search
exhibits both fair explorability and exploitability while random
search and exhaustive search deliver full explorability and full
2719
0 200 400 600 800 1000
0
10
20
30
40
50
60
70
m
Ev
al
ua
tio
n 
Ti
m
e
 
 
MA1GS b1−easy
MA1GS b1−hard
NMGS b1−easy
NMGS b1−hard
theoGS
(a)
0 200 400 600 800 1000
0
50
100
150
200
m
Ev
al
ua
tio
n 
Ti
m
e
 
 
MA2LS b1−easy
MA2LS b1−hard
NMLS b1−easy
NMLS b1−hard
theoLS
(b)
Fig. 5. The global search behavior and local search behavior of NM on
DLQBCs. Fig. 5(a) illustrates the average evaluation time for the Nelder-
Mead method to find one subthreshold point among N points when there are
m subthreshold points. Fig. 5(b) illustrates the average evaluation time for
the Nelder-Mead method to find the minimum point among m points.
Note that in Fig. 6 and 7, the evaluation times of all the
algorithms exhibit either v-shapes or L-shapes along the m
axis. As the evaluation time is the summation of global search
time and local search time, and global search time generally
dramatically decreases as m increases while local search time
basically increases as m increases, the global search time and
the local search time dominate the evaluation time when m
is rather small and when m is sufficiently large, respectively.
Thus, when m is rather small, MA1, which employs NM as
its global search, has a much smaller evaluation time than
SS and MA2, which employ random global search. When
m is large, the employed local search algorithm dictates
the evaluation time of a memetic algorithm: SS and MA1,
employing exhaustive local search, have their evaluation times
linearly scale with respect to m as the exhaustive local search
does on all the four types of problems; MA2, employing
0 200 400 600 800 10000
100
200
300
400
500
m
Ev
al
ua
tio
n 
Ti
m
e
 
 
SS
NM
MA1
MA2−1
MA2−10
theo
(a) b1-easy
0 200 400 600 800 1000
0
100
200
300
400
500
m
Ev
al
ua
tio
n 
Ti
m
e
 
 
SS
NM
MA1
MA2−1
MA2−10
theo
(b) b1-hard
Fig. 6. The behavior of the proposed representative archetypes for memetic
algorithms on unimodal PDLQBCs.
NM as local search, has its evaluation time logarithmically
increases with respect to m on easy problems and linearly
increases with respect to m on hard problems as NM local
search does. The shapes and collaborative behavior of all
the subject algorithms basically consist with the modeling
approach in [9]. This implies that the proposed local search
zone concept could be adopted to model the collaborative
behavior of more realistic algorithm-problem complexes than
the one proposed in [9].
C. Memetic Algorithms on Unimodal Problems
In Fig. 6(a), NM captures the landscape well and achieves
an ideal evaluation time about the order of the logarithm
of the size of the search space. On these smooth unimodal
problems, both MA1, employing NM as global search, and
MA2, employing NM as local search, have their best per-
formance at some values of m better than NM; and as both
the global search algorithm and the local search algorithm
2721
rough landscapes, better initial local points do not guarantee
closer distances to the local minimum and do not help much
to reduce the local time.
D. Memetic Algorithms on Multi-modal Problems
In Fig. 7(a), NM performs worse than on b1-hard problems.
The reason for this performance degradation may be twofold.
Firstly, the multi-modal landscapes may induce deceptive
unimodal information and thus interfere the fast convergence
of NM. Secondly, as explorability is not a main concern of
NM, it may repeatedly visiting the same quasi-basin leaving
other quasi-basins, where the minimum may reside in, unex-
plored. On these multi-modal problems, MA2 exhibits the best
performance as it employs random global search, capable of
effectively exploring the basins, and NM local search, capable
of efficiently exploiting an encountered quasi-basin. Though
m has smaller impact on MA2, a feature of NM local search,
when the m is properly set to some values that guarantee
both small global search time and local search time, MA2 still
could further achieve its best performance as indicated in this
figure. SS also has its best performance outperforming NM on
these multi-modal problems as a result of good collaboration
between the random global search and exhaustive local search.
Note that MA1 does not perform well as its collaboration
of NM global search and exhaustive local search may not
suit this type of problems well. All of these observations
on b10-easy problems suggest that on smooth multi-modal
problems, coordinating random global search with an efficient
heuristic algorithm may provide a good optimization solution
which is in some degree robust to the local search criterion.
Furthermore, these observations also imply that when an
efficient problem-specific local search is at hand for multi-
modal problems, coordinating it with explorative global search
algorithm may provide salient performance than other kinds of
hybrids. In recent studies, the IPOP-CMA-ES, which benefits
from the efficiency of CMA-ES and the explorative restart and
increase population size mechanism, may be a good example
of MA2-type algorithms. It performs well on the Weierstrass
function of BBOB 2010 [10].
In Fig. 7(b), NM has its worst performance among the
four types of problems due to the roughness and the multi-
modality of the landscapes explained in b1-hard and b10-easy
problems. On these problems, all the MAs (considering MA2-
1 and MA2-10 as one) with a not-too-bad coordination of
global search and local search achieve better performance than
NM does. This implies that when the problem is extremely
hard, rough and multi-modal, any MAs with proper design
could outperform a mediocre heuristic algorithm. Note that
SS achieved the best performance among MAs. This suggests
that memetic algorithms employing an explorative global
search algorithm like random search and an exploitive local
search algorithm like exhaustive search, with an elaborated
coordination, have a chance to achieve better performance than
other combinations of memetic algorithms on hard problems.
Another interesting observation on the algorithmic perfor-
mance is that SS and MA1 perform better on b10-hard than
on b10-easy. This may be due to the larger deviations among
depths of the basins induced by the landscape roughness as
illustrated in Fig. 2(b). Within some value of m, the number of
quasi-basin viewed by the SS and MA1 on b10-hard problems
is less than that on b10-easy problems and thus requires less
evaluation time. Note that though with this virtual reduction
on the number of quasi-basins, the landscape roughness still
make the b10-hard problems harder than b10-easy for NM
and MA2 which lack of the exploitability of exhaustive local
search as depicted in Fig. 7(b).
VI. CONCLUSIONS
In this work, we made an attempt to investigate when
and what kind of memetic algorithms perform well. We
extended our previous framework to illustrate the collabora-
tive behavior of three representative archetypes of memetic
algorithms and compare them with a representative heuristic
algorithm, Nelder-Mead method, on four representative types
of problems. On each type of problems, the success and
failure of each algorithm are analyzed and discussed. This
study may provide a reasonable and systematic explanation to
why some memetic algorithms on certain problems outperform
heuristic algorithms and attain salient performance, and we, as
researchers on memetic algorithms, thus could gain insights
into the design principals of memetic algorithms.
ACKNOWLEDGMENTS
The work was supported in part by the National Science
Council of Taiwan under Grant NSC 99-2221-E-009-123-
MY2.
REFERENCES
[1] Y.-S. Ong, M. H. Lim, and X. Chen, “Memetic computation – past,
present & future,” IEEE Computational Intelligence Magazine, vol. 5,
no. 2, pp. 24–31, 2010.
[2] W. E. Hart, “Adaptive global optimization with local search,” Ph.D.
dissertation, University of California, 1994.
[3] M. W. S. Land, “Evolutionary algorithms with local search for combina-
torial optimization,” Ph.D. dissertation, University of California, 1998.
[4] D. Sudholt, “On the analysis of the (1+1) memetic algorithm,” in
Proceedings of ACM SIGEVO Genetic and Evolutionary Computation
Conference 2006 (GECCO-2006), 2006, pp. 493–500.
[5] ——, “The impact of parametrization in memetic evolutionary algo-
rithms,” Theoretical Computer Science, vol. 410, no. 26, pp. 2511–2528,
2009.
[6] N. Krasnogor, “Studies on the theory and design space of memetic
algorithms,” Ph.D. dissertation, University of the West of England, 2002.
[7] Y. S. Ong and A. J. Keane, “Meta-Lamarckian learning in memetic
algorithms,” IEEE Transactions on Evolutionary Computation, vol. 8,
no. 2, pp. 99–110, 2004.
[8] J. Smith, “Coevolving memetic algorithms: A review and progress
report,” IEEE Transactions on Systems, Man, and Cybernetics, Part B:
Cybernetics, vol. 37, no. 1, pp. 6–17, 2007.
[9] J.-Y. Lin and Y.-P. Chen, “Analysis on the collaboration between global
search and local search in memetic computation,” IEEE Transactions on
Evolutionary Computation, vol. 15, no. 5, pp. 608–623, 2011.
[10] A. Auger, S. Finck, N. Hansen, and R. Ros, “BBOB 2010: Comparison
tables of all algorithms on all noiseless functions,” Inria, Tech. Rep.
RT-388, 2010. [Online]. Available: http://hal.inria.fr/inria-00516689/en/
2723
 2
二、與會心得 
今年本實驗室參與 IEEE WCCI 2012 進行兩篇論文:「When and what kind of memetic 
algorithms perform well」與「Sparse degrees analysis for LT codes optimization」之全文口頭
報告，同時分別聽取各相關研究論文之口頭報告暨海報展示，增進與交流演化計算各領域
知識，獲益良多。由於此會議之水準甚高，因此，一如預期地，由大會所選出的論文品質
都相當優秀，口頭報告者亦或海報展示者皆盡力展示各自成果，與會人員皆盡心盡力，做
出最好的表現。 
由於本實驗室的研究主題，在演化計算領域中，屬於較為理論層面的部分，因此，國
內相關學者從事類似研究者相當稀少。故在此會議中，能有與國際學者交流的機會實屬難
得，並更於交流過程中，獲得研究上豐碩之經驗傳授與討論。在與國際級之相關學者切磋
最先進的研究議題的同時，本實驗室的研究主題亦吸引許多研究同儕之目光，收穫豐盛。 
三、發表論文全文或摘要 
如前項所述，本實驗室與會口頭報告兩篇論文全文，可詳見於本報告附件。 
 When and what kind of memetic algorithms perform well 
 Sparse degrees analysis for LT codes optimization 
四、建議 
能夠參與此一等級之國際大型且質優的研討會，確實令人對學術研究感到前途光明。
整個演化計算領域的研究主題不斷地前進、研究內容不斷地提升，而台灣學者被制度面壓
迫至專心投稿於發表進度慢如牛車的學術期刊，甚少有機會直接和國際級的頂尖學者面對
面談論研究議題，實為目前台灣學術發展亟待關切之重要問題。因此，若研究之評鑑制度
能放棄唯期刊論文獨尊之思維，並配合更優渥之國際學術會議出席補助制度，方能迅速提
升台灣學術界在全世界的能見度、從而全面性地增進台灣的研究水準與學術國力。 
五、攜回資料名稱及內容 
攜回資料有研討會論文集之光碟一片。 
六、其他 
無。 
to eight (8) pages in length (barring extra page charges), and must follow
the format instructions provided at:
 
    http://www.ieee-wcci2012.org
 
2. In order for your paper to be published in the conference
proceedings, a signed IEEE Copyright Form must be submitted for each
paper.  CEC 2012 has registered to use the IEEE Electronic Copyright
Form (eCF) service. The confirmation page shown after submitting your
final paper contains a button linking directly to a secure IEEE eCF site
which allows electronic completion of the copyright assignment process.
In case it fails, please have the completed IEEE Copyright Form, found
at http://www.ieee.org/web/publications/rights/copyrightmain.html,
emailed it to the Publication Chair, Dr. Daryl Essam
(d.essam@adfa.edu.au).
 
IMPORTANT: No paper can be published in the proceedings without being
accompanied by a Completed IEEE Copyright Transfer Form.  You must
complete and submit this form to have your paper included in the
conference proceedings.
 
3. Register for the conference at
    http://www.ieee-wcci2012.org
by clicking on the conference registration link on the left-hand side of the main 
page.
 
IMPORTANT: Each paper must have a corresponding registered author to be
included in the proceedings.  Papers that do not have an associated
registered author will not be included in the proceedings.  The deadline
for author registration is April 2, 2012 so be sure to register by that
time to ensure that your paper is included in the proceedings.
Registering late may mean that your paper may not appear in the
proceedings.  Please ensure that you complete your registration early.
 
4. Make your hotel reservation for the CEC 2012 with information
obtained on the hotel reservation link "Accommodation" of the main
CEC 2012 page at http://www.ieee-wcci2012.org.
 
Thank you for participating in what promises to be an excellent meeting.
 
Sincerely,
Hussein Abbass <hussein.abbass@gmail.com>
CEC 2012 Paper #525 Presentation Form Information
2／2
archetype of memetic algorithms but also consists with the
empirical results in quite a few studies in the literature, further
investigation on the effect of problem landscapes and how the
explorability of global search and the exploitability of local
search affect the optimization performance is possible.
In this work, as practical problems generally exhibit some
degree of continuity rather than distributions of different sets
of stochastic points described in QBCs, we further extend the
QBC to Discrete Lipschitz Quasi-Basin Class(DLQBC) which
categorizes Lipschitz continuous problems according to the
number of basins and the roughness of landscape. Then we
apply several representative archetypes of memetic algorithms
on different DLQBCs to investigate the collaborative behavior
of memetic algorithms exhibiting different explorability and
exploitability on different problem landscapes. As the em-
pirical results of this framework not only consist with the
theoretical model proposed in our previous work but also well
delineate how different types of memetic algorithms behave on
different problems, this work may shed light into the design
of memetic algorithms.
The rest of this paper is arranged in the following manner.
We firstly introduce the framework of subthreshold seeker on
QBCs as the basis of this study in Section II, then provide the
definition of Discrete Lipschitz Quasi-Basin Class (DLQBC)
and its sampling test scheme in Section III, and propose
several representative archetypes of memetic algorithms for
investigation in Section IV. The empirical results of the
proposed representative archetypes memetic algorithms on
different DLQBCs are presented and discussed in Section V.
Finally, we conclude this paper in Section VI.
II. SUBTHRESHOLD SEEKER ON QUASI-BASIN CLASSES
In this section, the concept of local search zones and the
theoretical model proposed in our previous work [9] are briefly
reviewed to form the basis of this study.
A. Quasi-Basin Classes
In [9], an optimization problem is to optimize a given
objective function f : X → Y , and the optimization goal
is to find x∗ ∈ X with the minimum value y∗ ∈ Y . The X is
assumed a finite set as optimization problems are generally
numerically solved on digital computers. To simplify the
derivation, every function maps different x ∈ X to different
y ∈ Y is also assumed. For generality, the search space is
interpreted as a graph viewed by an optimization algorithm.
In this interpretation, the vertices are the set of points of X
and the edges are the set of pairs of points which are neighbors
viewed by an optimization algorithm. The terms X and V (G)
are used exchangeably in the following text.
Based on the fundamental definition of the search space, the
local search zones are defined as regions where local search
prefers and are virtually determined by the adopted local
search criterion and the landscape of the search space. Thus,
frameworks based on the concept of local search zones are
especially suitable for investigating the collaboration between
global search and local search. As local search zones are gen-
erally hard to measure, quasi-basins are used to approximate
local search zones. Given a threshold βm(f), which forms a
set Sm(f) consisting m vertices with their objective values
smaller than or equal to the threshold, a quasi-basin (QB) is
defined as a maximal connected subset in Sm(f). Accordingly,
the Quasi-Basin Class(QBC) is proposed to define a class of
problems with m subthreshold points, points with objective
values that are smaller than or equal to βm(f), distributed
among b distinct quasi-basins. As the vertices residing in
quasi-basins are better than the other vertices in the search
space and are favored by fitness-relevant local search criteria,
QBCs categorize the problems according to their distribution
of quasi-basins which is conceptually mapping to the distri-
bution of local search zones.
B. Subthreshold Seeker
To illustrate how the distribution of local search zones
affects the performance of a memetic algorithm, [9] adopts
Subthreshold Seeker (SS) as a representative archetype of
memetic algorithms. SS is a simplistic, minimal optimization
algorithm that coordinates random global search and exhaus-
tive local search according to a threshold. It explores the search
space by uniformly randomly sampling the search space. When
a subthreshold point is encountered by global search, SS
exploits the quasi-basin defined by the local search threshold
where the encountered subthreshold point resides in. In other
words, the exhaustive local search will eventually visit all the
points in the encountered quasi-basin. After local search in the
quasi-basin is done, SS continues to global search until another
subthreshold point is encountered. This switching between
global search and local search proceeds until the stopping
criterion is satisfied.
C. Theoretical Model of SS on QBCs
Now we are ready for the theoretical model of SS on
QBCs. Given a QBC Q(G,Y,m, b), the theoretical expected
samplings, T , for SS, with its local search threshold set to
βm(f), to find the minimum of a function in the QBC is
derived in [9] as
T =
cN
m
⌈
b
2
⌉
+
m+ 1
2
, (1)
where N denotes |G|, the size of G, and c denotes a parameter
between 0.75 and 1.5. The first term and the second term of
this formula are corresponding to the global search time and
the local search time of SS respectively. The global search
time of SS depends on the expected time for the random
search to find one of the m subthreshold points in N points.
It is inversely proportional to m and dramatically drops as
m increases. On the other hand, the local search time of
SS linearly scales with m as SS employs exhaustive local
search. Summing up the global search time and the local
search time, a v-shape of the expected evaluation time along
the m-axis with its optimal setting of m around
√
bcN can
be obtained. This theoretical model suggests that applying
2717
0 200 400 600 800 1000
0
200
400
600
800
1000
x
y
 
 
(a) b5-easy
0 200 400 600 800 1000
0
100
200
300
400
500
600
x
y
 
 
(b) b5-hard
Fig. 2. Instances of PDLQBCs
and hard denote that mo of the PDLQBC is set to 1 and 0.5
respectively. Fig. 2 illustrates two instances of PDLQBCs.
IV. REPRESENTATIVE ARCHETYPES OF MEMETIC
ALGORITHMS
In this framework, we investigate how the explorability and
exploitability of a memetic algorithm affect its performance.
In our perspective, according to the degree of explorability
and exploitability, basic optimization algorithms are mainly of
three types: random search, heuristic search, and exhaustive
search. Here we refer to pure random sampling as random
search, refer to sampling according to some rules as heuristic
search, and refer to sampling all the vertices as exhaustive
search. Accordingly, we investigate representative archetypes
of memetic algorithms consisting of different combinations of
random search, heuristic search, and exhaustive search.
A. Nelder-Mead Method
As heuristic search algorithms are various in forms, we
adopt the Nelder-Mead method (NM) as a representative
heuristic search algorithm. The Nelder-Mead method utilizes
the concept of a simplex, which is a special polytope of
N + 1 vertices in N dimensions, and features its capability
of approximating a local optimum of a problem with N
variables when the objective function varies smoothly and is
unimodal. As our sampling test scheme samples instances of
PDLQBC which are fundamentally one dimension problems,
we adopt the 1-D Nelder Mead Algorithm shown in Fig. 3.
The Uniform(X , 2) indicates a uniformly random sampling
of two vertices from the search space X . To avoid stagnation
during a global search, when x1 and x2 appear to be the same
vertex, we re-sample x1 and x2. Note that when this Nelder-
Mead method is utilized as a local search algorithm in the
following text, the stop criterion is set to the convergence of
the simplex. In other words, it will stop when x1 and x2 appear
to be the same vertex.
B. Memetic Algorithm Types
Fig. 4 illustrates the pseudo code of a generalized memetic
algorithm. The S is the resultant set of vertices of the current
global search and the reference for the next global search. If
one of the vertex si in S satisfies the local search criterion,
local search will be performed on the point. This process
1: procedure 1-D NELDER MEAD ALGORITHM(X , Y , f :
X → Y)
2: {x1, x2} ← Uniform(X , 2)
3: while the stopping criterion is not satisfied do
4: {x1, x2} ← Sort({x1, x2})
5: xr ← x1 + (x1 − x2)
6: if f(xr) < f(x1) then
7: xe ← x1 + 2(x1 − x2)
8: if f(xe) < f(x1) then
9: x2 ← xe
10: else
11: x2 ← xr
12: end if
13: else
14: xc ← x2 + (x1 − x2)/2
15: x2 ← xc
16: end if
17: if x1 = x2 then
18: {x1, x2} ← Uniform(X , 2)
19: end if
20: end while
21: end procedure
Fig. 3. 1-D Nelder-Mead Algorithm
1: procedure GENERALIZED MEMETIC ALGORITHM(X , Y ,
f : X → Y)
2: S ← Initialization()
3: while the stopping criterion is not satisfied do
4: S ← GlobalSearch(S,X ,Y, f : X → Y)
5: for si ∈ S do
6: if local search criterion is satisfied then
7: si ← LocalSearch(si,X ,Y, f : X → Y)
8: end if
9: end for
10: end while
11: end procedure
Fig. 4. A generalized memetic algorithm.
will continue until the stopping criterion is satisfied. In this
framework, we set the stopping criterion to the sampling of
the global optima, i.e., the minimum. To illustrate how local
search criteria affect the collaboration between global search
and local search based on the concept of local search zones, we
set the local search criterion as the subthreshold seeker does.
Thus, local search will be performed on any vertex si in S with
an objective value f(si) less than or equal to βm(f), the local
search threshold, and we can then investigate the behavior of
the subject memetic algorithms based on the model in [9].
To identify representative archetypes of memetic algorithms
for investigation, we firstly categorize all optimization algo-
rithms into three types: random search, heuristic search, and
exhaustive search. Among these three types, heuristic search
exhibits both fair explorability and exploitability while random
search and exhaustive search deliver full explorability and full
2719
0 200 400 600 800 1000
0
10
20
30
40
50
60
70
m
Ev
al
ua
tio
n 
Ti
m
e
 
 
MA1GS b1−easy
MA1GS b1−hard
NMGS b1−easy
NMGS b1−hard
theoGS
(a)
0 200 400 600 800 1000
0
50
100
150
200
m
Ev
al
ua
tio
n 
Ti
m
e
 
 
MA2LS b1−easy
MA2LS b1−hard
NMLS b1−easy
NMLS b1−hard
theoLS
(b)
Fig. 5. The global search behavior and local search behavior of NM on
DLQBCs. Fig. 5(a) illustrates the average evaluation time for the Nelder-
Mead method to find one subthreshold point among N points when there are
m subthreshold points. Fig. 5(b) illustrates the average evaluation time for
the Nelder-Mead method to find the minimum point among m points.
Note that in Fig. 6 and 7, the evaluation times of all the
algorithms exhibit either v-shapes or L-shapes along the m
axis. As the evaluation time is the summation of global search
time and local search time, and global search time generally
dramatically decreases as m increases while local search time
basically increases as m increases, the global search time and
the local search time dominate the evaluation time when m
is rather small and when m is sufficiently large, respectively.
Thus, when m is rather small, MA1, which employs NM as
its global search, has a much smaller evaluation time than
SS and MA2, which employ random global search. When
m is large, the employed local search algorithm dictates
the evaluation time of a memetic algorithm: SS and MA1,
employing exhaustive local search, have their evaluation times
linearly scale with respect to m as the exhaustive local search
does on all the four types of problems; MA2, employing
0 200 400 600 800 10000
100
200
300
400
500
m
Ev
al
ua
tio
n 
Ti
m
e
 
 
SS
NM
MA1
MA2−1
MA2−10
theo
(a) b1-easy
0 200 400 600 800 1000
0
100
200
300
400
500
m
Ev
al
ua
tio
n 
Ti
m
e
 
 
SS
NM
MA1
MA2−1
MA2−10
theo
(b) b1-hard
Fig. 6. The behavior of the proposed representative archetypes for memetic
algorithms on unimodal PDLQBCs.
NM as local search, has its evaluation time logarithmically
increases with respect to m on easy problems and linearly
increases with respect to m on hard problems as NM local
search does. The shapes and collaborative behavior of all
the subject algorithms basically consist with the modeling
approach in [9]. This implies that the proposed local search
zone concept could be adopted to model the collaborative
behavior of more realistic algorithm-problem complexes than
the one proposed in [9].
C. Memetic Algorithms on Unimodal Problems
In Fig. 6(a), NM captures the landscape well and achieves
an ideal evaluation time about the order of the logarithm
of the size of the search space. On these smooth unimodal
problems, both MA1, employing NM as global search, and
MA2, employing NM as local search, have their best per-
formance at some values of m better than NM; and as both
the global search algorithm and the local search algorithm
2721
rough landscapes, better initial local points do not guarantee
closer distances to the local minimum and do not help much
to reduce the local time.
D. Memetic Algorithms on Multi-modal Problems
In Fig. 7(a), NM performs worse than on b1-hard problems.
The reason for this performance degradation may be twofold.
Firstly, the multi-modal landscapes may induce deceptive
unimodal information and thus interfere the fast convergence
of NM. Secondly, as explorability is not a main concern of
NM, it may repeatedly visiting the same quasi-basin leaving
other quasi-basins, where the minimum may reside in, unex-
plored. On these multi-modal problems, MA2 exhibits the best
performance as it employs random global search, capable of
effectively exploring the basins, and NM local search, capable
of efficiently exploiting an encountered quasi-basin. Though
m has smaller impact on MA2, a feature of NM local search,
when the m is properly set to some values that guarantee
both small global search time and local search time, MA2 still
could further achieve its best performance as indicated in this
figure. SS also has its best performance outperforming NM on
these multi-modal problems as a result of good collaboration
between the random global search and exhaustive local search.
Note that MA1 does not perform well as its collaboration
of NM global search and exhaustive local search may not
suit this type of problems well. All of these observations
on b10-easy problems suggest that on smooth multi-modal
problems, coordinating random global search with an efficient
heuristic algorithm may provide a good optimization solution
which is in some degree robust to the local search criterion.
Furthermore, these observations also imply that when an
efficient problem-specific local search is at hand for multi-
modal problems, coordinating it with explorative global search
algorithm may provide salient performance than other kinds of
hybrids. In recent studies, the IPOP-CMA-ES, which benefits
from the efficiency of CMA-ES and the explorative restart and
increase population size mechanism, may be a good example
of MA2-type algorithms. It performs well on the Weierstrass
function of BBOB 2010 [10].
In Fig. 7(b), NM has its worst performance among the
four types of problems due to the roughness and the multi-
modality of the landscapes explained in b1-hard and b10-easy
problems. On these problems, all the MAs (considering MA2-
1 and MA2-10 as one) with a not-too-bad coordination of
global search and local search achieve better performance than
NM does. This implies that when the problem is extremely
hard, rough and multi-modal, any MAs with proper design
could outperform a mediocre heuristic algorithm. Note that
SS achieved the best performance among MAs. This suggests
that memetic algorithms employing an explorative global
search algorithm like random search and an exploitive local
search algorithm like exhaustive search, with an elaborated
coordination, have a chance to achieve better performance than
other combinations of memetic algorithms on hard problems.
Another interesting observation on the algorithmic perfor-
mance is that SS and MA1 perform better on b10-hard than
on b10-easy. This may be due to the larger deviations among
depths of the basins induced by the landscape roughness as
illustrated in Fig. 2(b). Within some value of m, the number of
quasi-basin viewed by the SS and MA1 on b10-hard problems
is less than that on b10-easy problems and thus requires less
evaluation time. Note that though with this virtual reduction
on the number of quasi-basins, the landscape roughness still
make the b10-hard problems harder than b10-easy for NM
and MA2 which lack of the exploitability of exhaustive local
search as depicted in Fig. 7(b).
VI. CONCLUSIONS
In this work, we made an attempt to investigate when
and what kind of memetic algorithms perform well. We
extended our previous framework to illustrate the collabora-
tive behavior of three representative archetypes of memetic
algorithms and compare them with a representative heuristic
algorithm, Nelder-Mead method, on four representative types
of problems. On each type of problems, the success and
failure of each algorithm are analyzed and discussed. This
study may provide a reasonable and systematic explanation to
why some memetic algorithms on certain problems outperform
heuristic algorithms and attain salient performance, and we, as
researchers on memetic algorithms, thus could gain insights
into the design principals of memetic algorithms.
ACKNOWLEDGMENTS
The work was supported in part by the National Science
Council of Taiwan under Grant NSC 99-2221-E-009-123-
MY2.
REFERENCES
[1] Y.-S. Ong, M. H. Lim, and X. Chen, “Memetic computation – past,
present & future,” IEEE Computational Intelligence Magazine, vol. 5,
no. 2, pp. 24–31, 2010.
[2] W. E. Hart, “Adaptive global optimization with local search,” Ph.D.
dissertation, University of California, 1994.
[3] M. W. S. Land, “Evolutionary algorithms with local search for combina-
torial optimization,” Ph.D. dissertation, University of California, 1998.
[4] D. Sudholt, “On the analysis of the (1+1) memetic algorithm,” in
Proceedings of ACM SIGEVO Genetic and Evolutionary Computation
Conference 2006 (GECCO-2006), 2006, pp. 493–500.
[5] ——, “The impact of parametrization in memetic evolutionary algo-
rithms,” Theoretical Computer Science, vol. 410, no. 26, pp. 2511–2528,
2009.
[6] N. Krasnogor, “Studies on the theory and design space of memetic
algorithms,” Ph.D. dissertation, University of the West of England, 2002.
[7] Y. S. Ong and A. J. Keane, “Meta-Lamarckian learning in memetic
algorithms,” IEEE Transactions on Evolutionary Computation, vol. 8,
no. 2, pp. 99–110, 2004.
[8] J. Smith, “Coevolving memetic algorithms: A review and progress
report,” IEEE Transactions on Systems, Man, and Cybernetics, Part B:
Cybernetics, vol. 37, no. 1, pp. 6–17, 2007.
[9] J.-Y. Lin and Y.-P. Chen, “Analysis on the collaboration between global
search and local search in memetic computation,” IEEE Transactions on
Evolutionary Computation, vol. 15, no. 5, pp. 608–623, 2011.
[10] A. Auger, S. Finck, N. Hansen, and R. Ros, “BBOB 2010: Comparison
tables of all algorithms on all noiseless functions,” Inria, Tech. Rep.
RT-388, 2010. [Online]. Available: http://hal.inria.fr/inria-00516689/en/
2723
to eight (8) pages in length (barring extra page charges), and must follow
the format instructions provided at:
 
    http://www.ieee-wcci2012.org
 
2. In order for your paper to be published in the conference
proceedings, a signed IEEE Copyright Form must be submitted for each
paper.  CEC 2012 has registered to use the IEEE Electronic Copyright
Form (eCF) service. The confirmation page shown after submitting your
final paper contains a button linking directly to a secure IEEE eCF site
which allows electronic completion of the copyright assignment process.
In case it fails, please have the completed IEEE Copyright Form, found
at http://www.ieee.org/web/publications/rights/copyrightmain.html,
emailed it to the Publication Chair, Dr. Daryl Essam
(d.essam@adfa.edu.au).
 
IMPORTANT: No paper can be published in the proceedings without being
accompanied by a Completed IEEE Copyright Transfer Form.  You must
complete and submit this form to have your paper included in the
conference proceedings.
 
3. Register for the conference at
    http://www.ieee-wcci2012.org
by clicking on the conference registration link on the left-hand side of the main 
page.
 
IMPORTANT: Each paper must have a corresponding registered author to be
included in the proceedings.  Papers that do not have an associated
registered author will not be included in the proceedings.  The deadline
for author registration is April 2, 2012 so be sure to register by that
time to ensure that your paper is included in the proceedings.
Registering late may mean that your paper may not appear in the
proceedings.  Please ensure that you complete your registration early.
 
4. Make your hotel reservation for the CEC 2012 with information
obtained on the hotel reservation link "Accommodation" of the main
CEC 2012 page at http://www.ieee-wcci2012.org.
 
Thank you for participating in what promises to be an excellent meeting.
 
Sincerely,
Hussein Abbass <hussein.abbass@gmail.com>
CEC 2012 Paper #522 Presentation Form Information
2／2
tags on decoding error probability of LT codes. Based on the
observations, a sparse degree selection algorithm is proposed
to define appropriate tags, and the optimal distribution formed
by the selected tags may get close to the global optimal.
The remainder of the paper is organized as follows. Sec-
tion II gives the details of coding mechanism of LT codes and
introduces two evaluation approaches for degree distributions.
Section III investigates the probability reallocation of degree
distributions and presents the influences of different tags. In
section IV, our selection algorithm is proposed and described
in detail. Several optimization results with different parameter
settings are presented in section V to examine the selection
algorithm. Finally, the conclusion and our contribution are
given in section VI.
II. LT CODES
Before describing main work of the paper, the operation of
LT codes is introduced in this section as a background. The
encoding and decoding procedures are given in section II-A.
Source data in general are divided into k fragments with an
identical length. These fragments are bit nodes or called input
symbols if the length of each fragment is only a bit. Similarly
encoding symbols denote the codewords generated by the
encoding procedure in LT codes. For a clear presentation, the
terms, input symbols, and encoding symbols, are consistently
used in this paper. Section II-B introduces soliton degree
distributions, which were proposed according to a theoretical
analysis. Furthermore, because evaluating the quality of degree
distributions is necessary in optimization, section II-C intro-
duces two different approaches for evaluating the decoding
error rate of LT codes for a given degree distribution.
A. Encoding and Decoding
Given the source data, we suppose that the source data
are cut into k input symbols. Before an encoding symbol is
generated, a degree d is chosen at random according to the
adopted degree distribution Ω(d), where 1 ≤ d ≤ k and∑k
d=1 Ω(d) = 1. The degree d decides how many distinct
input symbols are involved to compose an encoding symbol.
Then d input symbols, also named neighbors, are chosen
uniformly at random and accumulated by XOR. In the design
of LT codes, random number plays an essential role in the
encoding process. The approach employed by LT codes for a
sender to inform receivers of all information is to synchronize
a random number generator with a specified seed.
At the receiver side, when n, which is usually slightly larger
than k, encoding symbols arrive, belief propagation is used to
reconstruct the source data step by step. All encoding symbols
are initially covered in the beginning. In the first step, all
encoding symbols with only one neighbor can be directly
released to recover their unique neighbor. For an input symbol
that has been recovered but not processed yet, it is called a
ripple and will be pushed into a queue. At each subsequent
step, ripples are popped from the queue as a processing target
one by one. A ripple is removed from all encoding symbols
that have it as a neighbor. If an encoding symbol has only
one remaining neighbor after removing, the releasing action
repeats and may produce new ripples to maintain a stable
size of the queue. Maintaining the size of the ripple queue is
important because the decoding process fails when the ripple
queue becomes empty with uncovered input symbols. In other
words, more encoding symbols are required to continue the
decoding process. The decoding process succeeds if all input
symbols are recovered at the end.
B. Soliton distribution
The coding behavior of LT codes is determined by the
degree distribution, Ω(d), and the number of received encoding
symbols, n. Reception overhead, ε = n/k, denotes the delivery
efficiency of LT codes, and ε depends on the given degree
distribution. Based on the theoretical analysis, Luby proposed
the ideal soliton distribution which can achieve the best
performance, ε = 1, in the ideal case.
Ideal soliton distribution ρ(d):
ρ(d) =
{ 1
k
for d = 1
1
d(d−1) for d = 2, 3, . . . , k
. (1)
Ideal soliton distribution guarantees that all the release proba-
bilities are identical to 1/k at each decoding step. Hence, there
is exactly one expected ripple generated at each step when the
encoding symbol size is k. After k processing step, ideally,
the source data can be fully recovered.
However, ideal soliton distribution works poorly in practice.
Belief propagation may be suspended by a small variance
of the stochastic encoding/decoding situation in which no
ripple exists because the expected ripple size is only one at
any moment. According to the theory of random walk, the
probability that a random walk of length k deviates from its
mean by more than ln(k/δ)
√
k is at most δ. It is a baseline of
ripple size that must be maintained to complete the decoding
process. Therefore, in the same paper by Luby, a modified
version called robust soliton distribution was also proposed.
Robust soliton distribution µ(d):
S = c ln(k/δ)
√
k ,
τ(d) =


S/dk for d = 1, . . . , k/S − 1
S ln(S/δ)/k for d = k/S
0 for d = k/S + 1, . . . , k
, (2)
β =
k∑
d=1
(ρ(d) + τ(d)) , (3)
µ(d) =
ρ(d) + τ(d)
β
for d = 1, . . . , k , (4)
where c and δ are two parameters for controlling the character
of a robust soliton distribution. c controls the average degree
of the distribution, and δ estimates that there are ln(k/δ)
√
k
expected ripples as aforementioned. Robust soliton distribution
can ensure that only n = k + O(ln2(k/δ)√k) encoding
symbols are required to recover the source data successfully
with a probability at least 1-δ.
2464
Algorithm 1 Tags Selection Function
Input: The source symbol size k, the density parameter d;
Output: The set of sparse tags;
1: procedure TSF(k, d)
2: D ← Ideal soliton distribution for size k;
3: S ← 0, E ← 1, Tags← [];
4: while i < k do
5: S ← S + E ×D(i)
6: if S > (1/d) then
7: Tags← [Tags, i];
8: S ← 0;
9: E ← 1;
10: else
11: E ← E + 1;
12: end if
13: end while
14: return vector [1, Tags, k];
15: end procedure
observed by comparing the selections of (a, b) = (a1, 4 + i)
to (a, b) = (a1, 4 − i), e.g. comparisons between (1, 2) and
(1, 6). The distance influence of such two pairs are the same
but the pair (1, 6) with complementary property observably
has a smaller difference.
Given the above considerations, it turns out that the reallo-
cation of the probability to degrees 3 and 5, the nearest two
degrees to 4, would be the most close to the optimum. For
a better view of showing how close the adjusted distributions
could approach to the optimal one, we illustrate the differences
of error probabilities in logarithm scale in Figure 1(b). It
also gives the proof that the error probability of the original
distribution could be well approximated by the reallocated one.
These observations inspire us regarding how to choose the tags
for a sparse degree distribution.
IV. SELECTION FUNCTION FOR SPARSE TAGS
In addition to the factors observed from experiments, we
also take some intuitive properties into account. For example,
the higher probability to be reallocated would result in higher
difference of the error probability to the optimal one. Summing
up all of above, the main considerations of our degree selection
strategy for sparse tags would be as follows:
1) The number and value of probabilities around each
degree.
2) The distance between the probability reallocated degrees
and the removed one.
The first criterion comes from the positive correlation of the
reallocated probability and the error probability. The second
one accounts for the results of the experiments that replacing
the tag to be removed with two adjacent tags would have the
best approximation for error probability. According to these
criterion, we propose the sparse tags selection function in
Algorithm 1.
We consider the ideal soliton distribution since it would be
the optimal degree distribution in the ideal case. The density
5 10 15 20 25 300
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Degrees
Pr
ob
ab
ili
ty
 
 
Ideal soliton distribution (k=30)
TSF(30,10)
Fig. 2. Illustration of the work done by TSF(k, d).
parameter d acts as the bound that base on the first main con-
sideration, degree i would not be removed if its probability was
larger than 1/d. On the other hand, we group the degrees with
probabilities below 1/d and concentrate those probabilities to
a nearby degree. The second main consideration would be
applied to the selection of the representative degree of each
group. We accumulate the probabilities multiplied the distance
factors and take the degree while the sum exceeds the bound
1/d. In addition, considering the complementary property of
the selected degrees to distribute the probability, we reserve the
degrees 1 and k to ensure there always exists degrees satisfied
this property to be chosen. Figure 2 illustrates the work done
by Tags Selection Function (TSF) and shows the tags selected
for (k, d) = (30, 10). Tags 1 and 30 were selected to meet
the complementary property. The tags 2, 3 were selected since
the probabilities of these tags in ideal soliton distribution
were above the density criteria. The remaining tags were the
representations of the grouped tags of which probabilities were
below the density bound.
Following we provide some examples of sparse tags selected
by Algorithm 1 for k = 100:
1) TSF(100, 1) = [1, 4, 23, 100].
2) TSF(100, 3) = [1, 2, 5, 12, 30, 76, 100].
3) TSF(100, 5) = [1, 2, 4, 8, 16, 32, 64, 100].
4) TSF(100, 10) = [1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 100].
5) TSF(100, 20) = [1, 2, 3, 4, 6, 8, 11, 15, 21, 29, 40, 56,
78, 100].
We can see that the sparse tags selected in TSF(100, 5) were
close to the series of power of 2 and those selected in
TSF(100, 10) were close to Fibonacci series. Such a result
gives an explanation for the good performance of choosing
these series, power of 2 and the Fibonacci series, as approxi-
mation to the full tags in a certain extent.
2466
100 150 200 250 300
10−7
10−6
10−5
10−4
10−3
k sizes
Er
ro
r p
ro
ba
bi
lit
y 
di
ffe
re
nc
e
 
 
d = 3
d = 5
d = 10
d = 20
Full tags
Fig. 4. Differences between the error probabilities and the minimal one.
considered to have global minimal error probability because it
is the universal set of all distributions and forms the complete
search space. However, the optimization result of full degree
distributions gets worse and worse as input symbol size k
increases. For the same optimization algorithm and evaluation
function are implemented for each degree set, the worse results
of full degree distributions could be explained by that the
number of decision variables is too many for CMA-ES to
handled in limited function evaluations.
VI. CONCLUSION
Using evolutionary algorithms to optimize the degree distri-
bution for LT codes is a promising research topic. Sparse de-
gree distributions are frequently used to replace full degrees for
reducing the search space. How good the performance could
be achieved by a sparse degree distribution depends on the set
of its non-zero entries, i.e., tags. However, no investigation
has been done regarding how to decide appropriate tags to
construct sparse degree distributions with good performance.
In this paper, the authors analyzed the influence of different
degrees on decoding rate and proposed a tag selection algo-
rithm to choose tags for LT codes optimization. Finally, the
presented experimental results were evidentially illustrate the
practicality of the proposed tag selection algorithm.
In previous studies, researchers manually chose tags for
sparse degree distributions according to their own experimental
experience. Even though the chosen subset of degrees worked
well, the detailed mechanism was still unknown. This work
made an effort to find out guidelines for choosing appropriate
tags. The proposed selection algorithm can be applied for any
input size and control the level of sparseness conveniently
by adjusting the density parameter. This solution can help
researchers to put more attention in the optimization algorithm
rather than the individual encoding. The paper presented the
qualitative analysis of probability reallocation in a distribution.
The variances of error probability were compared for changing
the reallocated degree and then quantitative analysis is needed
in advance. If the quantity of variance can be measured
precisely, developing a local search based on the measure
approach to enhance certain optimization framework for LT
codes will be possible. Research of this line is definitely worth
pursuing, and the authors are currently taking the challenge.
ACKNOWLEDGMENTS
The work was supported in part by the National Science
Council of Taiwan under Grant NSC 99-2221-E-009-123-
MY2.
REFERENCES
[1] J. W. Byers, M. Luby, M. Mitzenmacher, and A. Rege, “A digital
fountain approach to reliable distribution of bulk data,” in Proceedings
of the ACM SIGCOMM ’98 conference on Applications, technologies,
architectures, and protocols for computer communication. ACM, 1998,
pp. 56–67.
[2] J. W. Byers, M. Luby, and M. Mitzenmacher, “A digital fountain
approach to asynchronous reliable multicast,” IEEE Journal on Selected
Areas in Communications, vol. 20, no. 8, pp. 1528–1540, 2002.
[3] M. Luby, “LT codes,” in Proceedings of the 43rd Symposium on
Foundations of Computer Science. IEEE Computer Society, 2002, pp.
271–282.
[4] E. A. Bodine and M. K. Cheng, “Characterization of Luby Transform
codes with small message size for low-latency decoding,” in Proceedings
of the IEEE International Conference on Communications, 2008, pp.
1195–1199.
[5] E. Hyytia¨, T. Tirronen, and J. Virtamo, “Optimal degree distribution for
LT codes with small message length,” in Proceedings of the 26th IEEE
International Conference on Computer Communications (INFOCOM
2007), 2007, pp. 2576–2580.
[6] ——, “Optimizing the degree distribution of LT codes with an impor-
tance sampling approach,” in Proceedings of the 6th InternationalWork-
shop on Rare Event Simulation (RESIM 2006), 2006, pp. 64–73.
[7] C.-M. Chen, Y.-p. Chen, T.-C. Shen, and J. K. Zao, “On the optimization
of degree distributions in LT code with covariance matrix adaptation evo-
lution strategy,” in Proceedings of the IEEE Congress on Evolutionary
Computation, 2010, pp. 3531–3538.
[8] ——, “Optimizing degree distributions in LT codes by using the multiob-
jective evolutionary algorithm based on decomposition,” in Proceedings
of the IEEE Congress on Evolutionary Computation, 2010, pp. 3635–
3642.
[9] A. Talari and N. Rahnavard, “Rateless codes with optimum interme-
diate performance,” in Proceedings of the Global Telecommunications
Conference (GLOBECOM 2009), 2009, pp. 1–6.
[10] R. Karp, M. Luby, and A. Shokrollahi, “Finite length analysis of
LT codes,” in Proceedings of the IEEE International Symposium on
Information Theory 2004 (ISIT 2004), 2004, p. 39.
[11] E. Maneva and A. Shokrollahi, “New model for rigorous analysis of
LT-codes,” in Proceedings of the IEEE International Symposium on
Information Theory (ISIT 2006), 2006, pp. 2677–2679.
[12] N. Hansen and A. Ostermeier, “Adapting arbitrary normal mutation
distributions in evolution strategies: the covariance matrix adaptation,”
in Proceedings of the IEEE International Conference on Evolutionary
Computation, 1996, pp. 312–317.
[13] A. Auger and N. Hansen, “Performance evaluation of an advanced
local search evolutionary algorithm,” in Proceedings of the 2005 IEEE
Congress on Evolutionary Computation (CEC 2005), 2005, pp. 1777–
1784.
[14] ——, “A restart CMA evolution strategy with increasing population
size,” in Proceedings of the 2005 IEEE Congress on Evolutionary
Computation (CEC 2005), 2005, pp. 1769–1776.
2468
99年度專題研究計畫研究成果彙整表 
計畫主持人：陳穎平 計畫編號：99-2221-E-009-123-MY2 
計畫名稱：應用泛用型最佳化演算架構於無線網路傳輸技術最佳化問題之研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 6 6 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 6 6 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
