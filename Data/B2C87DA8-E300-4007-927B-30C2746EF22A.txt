 2
2.2 機器人之自我定位  
機器人的預設操作環境為辦公室、病房等已知 
 
圖1為機器人實際完成之外觀圖 
 
空間，並在特定位置上做指定的動作，如抓取物
件、放下物件、開門、關門等，要達成上述這些
動作的前提是機器人必須準確的移動到特定位置
上，因此精確的定位顯得相當重要。機器人在平
常移動時，所在位置的精確度較無在做指定動作
時重要，可容忍的誤差較大，所以在平常行走時，
機器人的定位利用馬達上的編碼器計算出機器人
速度後，在將其積分計算出機器人位置。當機器
人進入做指定動作之特定位置的延伸範圍時，則
改採用雷射來定位，如此在平常行走時可以節省
估算的時間，要做指定動作時也可以擁有精確的
定位。圖2為雷射landmark之實際外觀圖。在雷射
定位技術面，子計畫一將定位問題可視為非線性
規劃問題，即是求解(x,y)，使得式(1)中的J最小
化，其限制為 1 1,x x y y≥ ≤ ，計畫一利用牛頓法
(Newton’s Method)來計算此方程式的解。 
2
2 2 2
1 1
1
( ) ( ) ( 4.5)i
i
J x x y y d
=
= − + − − +∑         
                  (1) 
 
圖2 雷射landmark實際外觀圖 
 
2.3機器人之新型路徑規劃技術  
  在子計畫一中，機器人的路徑規劃是採用場效
函數法(Potential Field Methods)，此法主要是將機
器人視為空間中的一個點，並將此空間視為一虛
擬的能場(artificial potential field)，而其典型的場效
函數定義為一個將機器人拉向目標的吸力場與一
個將機器人推離障礙物的斥力場之和，機器人即
向著能場的梯度方向移動，如此反覆動作使機器
人可到達目標。利用場效函數法做路徑規劃，在
即時避障方面相當迅速、簡易，並且不需要預先
了解障礙物的位置、數量。但此法會有局部最小
值(local minimum)問題，研究解決這個問題的學
者，一般分為兩個方向:(1)重新定義場效函數，使
虛擬的能場不存在或減少局部最小值點(2)利用一
些額外的技巧，使機器人能脫離局部最小值點。
當機器人行進路線上有凹形障礙物或由一些非凹
形障礙物組合而成的凹形環境時，機器人容易陷
入局部最小值、振盪、無窮迴圈等情況，這些情
況會使機器人無法到達目標，故上述情況皆稱為
機器人的困境情況(trap-state)。傳統的場效法常利
用吸、斥合力向量)來判斷是否陷入以上所述的情
況，但通常無法順利解決問題。子計畫一所提的
新型場效函數法是利用吸、斥力向量之夾角來切
換傳統場效法與沿牆法(OBFM, Obstacle Boundary 
Following Method)而完成的。  
 
2.4 實驗結果與討論  
   子計畫一在第三年裡總共進行三種實驗。實驗
一是有關比較梯形積分法與辛普森法估測等兩方
位推算器之編碼器定位精準度；由數據得知辛普
森法較梯形法估測的數值較為接近實際結果。 
實驗二是有關雷射定位精準度實驗，在實驗二
中，將雷射測距儀的量測範圍設定為0°~180°，距
離解析度設定為公厘(mm)，角度解析度設定為
0.5°。知機器人當進入目標點以半徑一公尺內，即
由編碼器估測定位切換成雷射定位，所以機器人
的landmark放置至少得大於實驗的量測距離有120 
cm、160 cm、200 cm。其60 cm以上，但在實驗中
有兩個landmarks，在此選擇目標與之間距離來實
驗。、由其實驗結果可以看出當與目標距離120 cm
時較160 cm、200 cm來得精準，故landmark採用120 
cm的放置。 
   實驗三是雷射定位後執行指定動作實驗。實驗
三分為定位抓取物件與定位放下物件，雷射測距
儀的量測範皆設定為0°~180°，距離解析度設定為
公厘(mm)，角度解析度設定為0.5°，landmark採用
80 cm的放置。圖3為實際定位抓取物件結果。 
 
 4
LWIP來完成，設計上可減少許多開發的時間。另
外，當機臂手臂每一軸的轉動角度決定後，接下
來便是如何平順的使機械手臂平順的移動到目的
地，整個軌跡規劃 (Trajectory Planning)的法則運
算，皆在 Nios CPU 中完成。硬體部份，機械手臂
每一軸的馬達皆有光解碼器 (Photo Encoder)，來
得知馬達的轉動位置，此部份的電路由 VHDL 硬
體描述語言來完成。而馬達的位與速度控制皆由
Nios CPU來控制，在位置控制我們採用比例-積分
(PI)控制電路來完成，而在速度控制，也是採用 PI
控制器來完成。機械手臂的控制採用雙 PI控制系
統來達到最佳的效能。最後的控制命令輸出由脈
波調變電路(Pulse Width Modulation,PWM)輸出至
馬達驅動器，PWM電路以VHDL撰寫，並在 FPGA
晶片上合成。整體機械手臂馬達控制電路的硬體
及軟體皆整合在 Altera Stratix Ⅱ的發展板上，大大
的減少了電路的體積，並且得到絕佳的設計靈活
性和優越的性能表現。 
 
3.4嵌入式全方位移動平台控制 
當全方位移動式平台的Stratix Edition 發展板
接收到由準系統發送全方位平台所需行走之 X、Y
座標後，和機械手臂的控制一樣，全方位移動平
台的點對點(Point to Point)運動由 Stratix Edition發
展板的 Nios CPU負責，再由 Altera Stratix Edition 
PIO 埠輸出數位控制訊號至數位-類比轉換器(D/A 
Converter)，轉換成類比信號後，最後由 D/A轉換
器輸出類比信號驅動全方位平台的馬達，由 Nios 
CPU 扮演全方位移動平台的控制器，並配合整合
在 FPGA晶片內以 VHDL做成的電路，達到全方
位移動平台的控制。 
 
3.5 體體系統功能 
  居家照護機器人之各部系統實現，可分成下述
七大功能： 
1. 影像處理： 
利用雙CCD Camera架構，擷取目標物影像，
即時傳回準系統，利用C++ Builder撰寫程式進
行影像分析、處理，讀取目標物3-D位置。 
2. 避障控制： 
利用雷射掃描器URG-04LX進行靜態障礙物
的判斷及迴避，並進行固定目標物及已知環境
之路徑規劃，能依命令自走至目標物所在點。 
3. 全方位影像定位： 
利用全向式攝影機擷取環境影像(其環境有我
們已擺設的標物)，即時傳回準系統，利用C++ 
Builder撰寫程式進行影像分析、處理，來得知
機器人位置。 
4. 全方位移動平台控制： 
利用Stratix Edition發展板自準系統接收目標
位置命令後，透過NiosⅡ整合發展環境(IDE)
撰寫點對點軌跡追蹤法則，計算出全方位移動
車體之各馬達所需轉動速度值並輸出12位元
數位訊號控制車體馬達運轉。 
5. 機械手臂控制： 
利用Altera StratixⅡ發展板接收自準系統所發
送之機械手臂每一軸轉動角度命令時，我們透
過NIOSⅡ整合發展環境(IDE)撰寫機械手臂各
關節移動量之運算法則，依據每一軸之最大加
速度，最大速度做點對點運動(Point-to-Point)
軌跡規劃。 
6. 滅火功能設計： 
利用MaxⅡ發展板接收自準系統所發送之移
動平台與機械手臂的任務完成訊號後，控制噴
嘴與電磁閥開關，來完成滅火功能。 
7. 抓取器之製作： 
由於原本機器手臂的抓取器是無法拿取形狀
比較特殊的物品，所以我們自行製作一組人形
抓取器，以方便抓取任何物品。 
 
3.6 實驗 
    為證實所研製的照護功能，子計畫二進行以
下的實驗:導航，全方位影像定位，物件抓取， 
滅火功能，利用雙CCD camera架構完成物品辨
識、特定物品特徵之判別等。圖5為導航避障實驗
情形。圖6為物件抓取實驗情形。圖7為滅火實驗
情形。 
 
圖5為導航避障實驗情形。 
 
圖6為物件抓取實驗情形。 
 
 6
1 0 0
0 1 0 0
0 0 1
0 0 0 1
T
A
T
⎡ ⎤⎢ ⎥⎢ ⎥= ⎢ ⎥⎢ ⎥⎣ ⎦
. 1 0 0 0
0 0 1 0
H ⎡ ⎤= ⎢ ⎥⎣ ⎦
 
4.5 即時臉部追蹤實驗 
    為證實所研製的即時臉部追蹤功能，子計畫
三進行許多不同情境的實驗。圖10為第一種實驗
情境的即時臉部追蹤實驗情形。圖11為第二種實
驗情境的即時臉部追蹤實驗情形。圖12為第三種
實驗情境的即時臉部追蹤實驗情形。三種情境下
的實驗成效都足以說明本方法可得到相當好的即
時臉部追蹤效果. 
圖10為第一種實驗情境的即時臉部追蹤實驗情形 
圖11為第二種實驗情境的即時臉部追蹤實驗情
形。 
 
 
圖12為第三種實驗情境的即時臉部追蹤實驗情
形。 
 
五、以家庭閘道器為基礎之群組機器人學   
    習與合作策略 
一般機器人的動作設計，是依據特定的目的，由
設計者站在機器人的角度上，觀測感測訊號，來
撰寫控制的法則，這樣需要縝密的考量以及豐富
的機器人學知識，並且在做最佳化的時候，得以
嘗試錯誤的方式，來做動作的調整與修正。
Reinforcement Learning-based Decision Tree 
Algorithm 的方法則是從機器人的角度來看設計
者，藉由觀察設計者的操控，使用決策樹來歸納
並分割動作，把設計者的動作模仿起來。既降低
了設計動作的困難度，在最佳化方面，也可以讓
機器人自主地學習。子計劃四所提出的群組機器
人學習與合作策略，仍然保持著設計動作的簡單
性，同時能夠考量多個行為的動作，並且針對整
體的行為做最佳化。設計者只需針對多個單一行
為各自做操控示範，分別讓機器人學著模仿，然
後直觀地設計整體行為的回饋值，利用融合行為
學習演算法(FBQL)系統來將多個單一行為的動作
以權重值W值來做融合，並由學習法則來不斷地
調整權重值W值，找出各行為間的最恰當比重。   
5.1融合行為學習演算法(FBQL)架構 
     融合行為學習演算法 (FBQL)藉由從環境
(Environment)所觀察到的各種感測訊號 (sensory 
input)，以供學習系統來決策應該如何做出融合行
為(Fused Behavior)。而環境會根據這個融合行為所
造成影響的好壞，回傳一個回饋值(reward)給學習
系統，讓學習系統可以針對融合行為來做修正。
如此反覆地決策→學習→修正、再決策→再學習
→再修正……循環越多次，越能夠做出好的決
策。參考圖 13，是由多個單一行為(Behavior)所組
合而成的，每個單一行為都有一套輸出動作。這
裡要注意的是，這些輸出動作的控制標的都要相
同，才能夠做融合。首先要用第三章的 RL-based 
Decision Tree Algorithm來替每個單一行為做適當
地分類，以此減低學習系統所要學習的狀態量，
同時又要保有每個單一行為各自的功能。每個單
一行為所要接受的輸入訊號不盡相同，這裡稱感
測訊號(sensory input)為所有行為所需要輸入訊號
的集合。感測訊號會激發每個單一行為，各自產
生輸出動作。融合行為((Fused Behavior)便是把每
個輸出動作以不同的權重值W值來融合，因此權
重值W值便是這個系統所要學習的主要目標，會
直接影響到整個融合行為的好壞。 
 
5.2. 單一行為的模仿 
    單一行為經過了 RL-based Decision Tree 
Algorithm的分割之後，會被分類成幾區極具代表
性的輸出動作，可以利用邊界範圍來表示同一種
類的輸出情況。因此，當要模仿此單一行為時，
只需根據這兩個變數的值去查表，看是被歸類到
 8
次在做決策的時候，能夠有更可信賴的參考經
驗。以融合行為學習演算法(FBQL)來做實驗，總
失敗次數只有 4 次，最後累積到 50 次的成功結
果，總成功率為 92.59%。圖 16是以移動平均法來
計算成功率，移動期數為 17 期，計算每 17 次裡
面成功的機率值。成功率最低在 82.35%，最高可
到達 100%，可見經過融合學習演算法(FBQL)的學
習之後，成功率有顯著的提升，而且逐漸趨於穩
定。 
 
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45
實驗次數
成
功
率
 
圖 16以融合行為學習演算法(FBQL)融合行為的
成功率 
5.5. 實驗結果討論 
    將兩個實驗結果做比較。首先以成功比率來
做比較，根據表 1 與 2 的資料，可以明確的看出
使用融合行為學習演算法(FBQL)，從單筆資料的
角度來看，可以有較好的總成功率；以多筆資料
平均的角度來看，無論是成功率的最小值、最大
值、還是平均值也都是 FBQL 的結果比較突出。
由以上的實驗結果可以知道，用融合行為學習演
算法(FBQL)可以提高系統的成功率，並且經過一
次學習之後，可以越來越迅速地達成目標。雖然
在撞牆率的這個標的上，沒有學習這麼快，但是
從趨勢上可以觀察出效果已經好到跟 Subsumption 
Architecture差不多了。 
 
表 1Subsumption Architecture與 FBQL成功率比較 
 Subsumption  FBQL
Success 50 times 50 times 
Fails 12 times 4 times
Single 
Data 
Success 
Rate 80.65 % 
92.59 
% 
Min Rate 64.71 % 82.35 % 
Max Rate 94.12 % 100 %
Averag
e Data 
Ave Rate 83.38 % 90.94 % 
 
表 2 Subsumption Architecture與 FBQL總步數比較 
 Subsumption FBQL 
Single Max 7255 steps 5411 steps 
Min 595 steps 246 steps Data 
Ave 1907.9 steps 1122.5 steps
Max 3690.6 steps 2302.6 steps
Min 1225.8 steps 305 steps 
Averag
e Data 
Ave 1908.75 steps 1100 steps 
  
表 3 Subsumption Architecture與 FBQL撞牆率比較 
 Subsumption  FBQL 
Max 6.21% 21.03% 
Min 0% 0% 
Single 
Data 
Ave 2.06% 7.04% 
Max 3.08% 14.54% 
Min 1.01% 2.75% 
Averag
e Data 
Ave 2.01% 7.17% 
   
 
  
六、結論  
本整合計畫報告已精簡敘述如何完成協調統
合各子計畫完成三年預定工作項目，並協助各子
計畫完成必要的單項技術設計與實現。目前本整
合型計畫已進行多次小型成果交流、分享與交換
相關技術，解決共有問題，並協助各子計畫積極
進行預定的工作項目與實驗,且獲得上述研究成
果。 
 
七、參考文獻  
7.1 子計畫1 
[1] J. Aranda, A. Grau, and J. Climent, “Control 
architecture for a three-wheeled roller robot,” IEEE 
International Workshop on Advanced Motion 
Control, pp. 518-523, 1998. 
[2] F. G. Pin, and S. M. Killough, “A new family of 
omnidirectional and holonomic wheel platforms for 
mobile robots,”IEEE Transactions on Robotics and 
Automation, vol. 10, pp. 480-489, 1994.  
[3] K. S. Byun, S. J. Kim, and J. B. Song, “Design of 
a four-wheeled omnidirectional mobile robot with 
variable wheel arrangement mechanism,” 
Proceedings of the IEEE International Conference on 
Robotics and Automation, vol. 1, pp. 720-725, 2002.  
[4] H. Asama, M. Sato, L. Bogoni, H. Kaetsu,A. 
Mitsumoto, and I. Endo, “Development of an 
omni-directional mobile robot with 3 DOF 
decoupling drive mechanism,” proceedings of the 
IEEE International Conference on Robotics and 
Automation, vol. 2, pp. 1925-1930, 1995.  
[5] D. Feng, M. B. Friedman,B. H. Krogh, “The 
servo-control system for an omnidirectional mobile 
robot,” Proceedings of the IEEE International 
Conference on Robotics and Automation, vol. 3, pp. 
 10
SICE 2002. Proceedings of the 41st SICE Annual 
Conference, vol. 2, pp 719-724, Aug 5-7. 2002. 
[14] P. Khosla and R. Volpe. “Superquadric Artificial 
Potentials for Obstacle Avoidance and 
Approach,” in Proc. of IEEE Intl. Conf. on 
Robotics and Automation, Vol. 3, Philadelphia, 
PA, USA, pp.1778-1784, April 1988.  
[15] J. H. Chuang and N. Ahuja, “An Analytically 
Tractable Potential Field Model of Free Space 
and Its Application in Obstacle Avoidance,” 
IEEE Transactions on systems, Man, and 
Cybernetics-Part B : Cybernetics, vol. 28, no. 5, 
pp.729-736, October 1998. 
[16] W. Tang, L. Lam, and J. Wang, “Kinematic 
Control and Obstacle Avoidance for Redundant 
Manipulators using a Recurrent Neural 
Network,” in Proc. Int. Conf. Artificial Neural 
Networks, pp. 922-929, 2001. 
[17] S. X. Yang and M. Meng, “An Efficient Neural 
Network Approach to Dynamic Robot Motion 
Planning,” Nerual Nerworks, vol. 13, pp. 
143-148, 2000. 
[18] K. Sugihara and J. Smith, “Genetic Algorithms 
for Adaptive Motion Planning of an 
Autonomous Mobile Robot,” in Proceedings of 
IEEE Intl. Symposium on Computational 
Intelligence in Robotics and Automation, pp. 
138-143, 1997.. 
[19] C. L. Lin, Optimal Path Planning for Dynamic 
Platforms, M.S. Thesis, Department of 
Automatic Control Engineering, Feng Chia 
University, Taichung, Taiwan, June 2004. 
[20] J. G. Kang and J. M. Lee, “A Study on Optimal 
Configuration for the Mobile Manipulator 
Considering the Minimal Movement,” 
Proceedings of IEEE International Symposium 
on Industrial Electronics, vol.2, pp.546-551, 
Dec. 2000. 
[21] L. B. Jiang, Point-to-Point Optimal 
Configuration Planning and Control of an 
Omnidirectional Mobile Manipulator, M.S. 
Thesis, Department of Electrical Engineering, 
National Chung-Hsing University, Taichung, 
Taiwan, July 2005. 
[22] D. S. Wang, System Design, Trajectory planning 
and Control of an Omni-directional Mobile 
Robot, M.S. Thesis, Department of Electrical 
Engineering, National Chung-Hsing University, 
Taichung, Taiwan, July 2006. 
[23] D. S. Wang, System Design, Trajectory planning 
and Control of an Omni-directional Mobile 
Robot, M.S. Thesis, Department of Electrical 
Engineering, National Chung-Hsing University, 
Taichung, Taiwan, July 2006.  
[24] K. Sugihara and J. Smith, “Genetic Algorithms 
for Adaptive Motion Planning of an 
Autonomous Mobile Robot,” in Proceedings of 
IEEE Intl. Symposium on Computational 
Intelligence in Robotics and Automation, pp. 
138-143, 1997. 
[25] L. B. Jiang, Point-to-Point Optimal 
Configuration Planning and Control of an 
Omnidirectional Mobile Manipulator, M.S. 
Thesis, Department of Electrical Engineering, 
National Chung-Hsing University, Taichung, 
Taiwan, July 2005. 
[26] Y. Matsumoto, K. Ikeda, M. Inaba, and H. Inoue, 
“Visual Navigation using Omni-directional View 
Sequence,” International Conference on 
Intelligent Robots and Systems, vol.1, 
pp.317-322, 1999. 
 
7.3 子計畫3 
[1] E. Hjelmas and B.K. Low, “Face Detection: A 
Survey,” Computer Vision and Images 
Understanding, vol. 83, pp. 236-274, 2001. 
[2] M.Yang, D. Kriegman, and N. Ahuja, “Detecting 
Faces in Images: A Survey,” Proc. IEEE Trans. 
Pattern Analysis and Machine Intelligence, vol. 
24, no. 1, pp. 34-58, Jan. 2002. 
[3] C. Kotropoulos and I. Pitas, “Rule-based face 
detection in frontal views,” Proc.IEEE Int’l Conf. 
Acoustics, Speech and Signal Processing, vol. 4, 
pp. 2537-2540, 1997. 
[4] Y. B. Sun and J. T. Kim, and W. H. lee, 
“Extraction of face objects using skin color 
information,” In Proc. IEEE Int. Conf. 
Communications, Circuits and Systems and West 
Sino Expositions, vol. 2, pp. 1136-1140, 2002. 
[5] K.C. Yow and R. Cipolla, “Feature-Based Human 
Face Detection,” Proc. IEEE Cnf. Image and 
Vision Computing, vol. 15, no. 9, pp. 713-735, 
1997. 
[6] R. L. Hsu, M. A. Mottaleb, and A. K. Jain, “Face 
detection in color images,” IEEE Trans. IEEE 
Trans. Pattern Analysis and Machine 
Intelligence, vol. 24, no. 5, pp. 696-706, May 
2002. 
[7] J. Miao, B. Yin, K. Wang, L. Shen, and X. Chen, 
“A hierarchical multiscale and multiangle 
system for human face detection in a complex 
background using gravity-center template,” 
Pattern Recognition, vol. 32, no. 7, pp. 
1237-1248, 1999. 
[8] K.-K. Sung and T. Poggio, “Example-based 
learning for view-based human face detection,” 
IEEE Trans. Pattern Analysis and Machine 
Intelligence, vol. 20, no. 1, pp. 39-51, Jan. 1998. 
[9] C. Garica and M. Delakis, “Convolutional face 
finder: a neural architecture of fast and robust 
face detection,” IEEE Trans. Pattern Analysis 
and Machine Intelligence, pp. 1408-1423, Nov. 
2004. 
[10] H. Rowley, S. Baluja, and T. Kanade, “Neural 
network-based face detection,” IEEE Trans. 
Pattern Analysis and Machine intelligence, vol. 
20, no. 1, pp. 23-38, Jan. 1998. 
