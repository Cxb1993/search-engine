 I
????  
???????????????????????????????????????
????????????????????????????????????????
????????????????????????????????????????
????????????????????????????????????????
????????????????????????????????????????
????????? 14 ??????????????????????????????
????????????????????????????????????????
????????????????????????????????????????
????????????????????????????????????????
???????????????????????? 
 
???????????????????????????????? 
 
Abstract 
This project primarily focuses on the design and implementation of the multi-sensors and 
multi-functional autonomous control for smart cars. The full autonomous smart car integrates 
omni-direction image and ultrasonic sensors to obtain the relative information between the car and 
the surrounding. Fuzzy logic controllers are proposed to determine what behavior the smart car 
should perform. By omni-direction image processing, we present a transformation method to 
transform the posture of the smart car in omni-direction image to the posture in the Cartesian 
coordinate fixed on the parking lot. By parking strategy, we present a smooth path planned by two 
arcs and design a fuzzy logic controller for the smart car to track the path. We also arrange 14 
ultrasonic sensors to estimate the environment, and use the binaural and multi-channel methods to 
recognize obstacles and wall. Furthermore, we develop the intelligent parking control and obstacle 
avoidance methods, which are based on the fuzzy logic control. After sensor fusion, the smart car 
can execute wall following, parallel parking, garage parking, and obstacle avoidance. Moreover it 
can detect the obstacles on the forward path or the parking lot, and deal with the contingency. 
Finally, it is perceived that our intelligent multi-sensors and multi-functional intelligent autonomous 
parking controller is feasible and effective from the practical experiments. 
 
Keywords:, Omni-direction vision system, ultrasonic sensor, binaural method, multi-channel, 
autonomous parking, fuzzy logic control 
 III
3.5 A Fuzzy Path Tracking Controller …………….………….……………………...…..24 
3.5.1 Fuzzification ………………….…………….………….………….…………...…..24 
3.5.2 Inference……………….………….………….………….………….……...…..25 
3.5.3 Composition………………….………….………….………....…………...…..26 
3.5.4 Defuzzification………….…….……………………………………….…...…..26 
Section 4 Ultrasonic Sensing System…………………………………………………………...…..26 
        4.1 The Arrangement of the Ultrasonic Sensors Array……….……………………...…..26 
        4.2 Firing Interval of the Ultrasonic Sensors Array……….…...………………………...27 
        4.3 Determination of the Type and Position of reflector by Binaural Method.……..…..28 
           4.3.1 Position Calculation of a Reflector by Binaural Method…...………………....29 
           4.3.2 Determination of Different Type of Reflectors by Binaural Method...........…..30 
Section 5 Ultrasonic Sensors-Based Wall Following and Parking Control…………………………33 
5.1 Fuzzy Wall-Following Controller Design (FWFC)………………………………..…33 
5.1.1 Design Right-Side FWFC………………………………………...…………….34 
5.1.2 Design Left-Side FWFC………………………………………………………..35 
5.2 Fuzzy Parallel-Parking Mode…………………………………………………….…36 
5.3 Fuzzy Garage-Parking Mode…………………………………………………..…….38 
Section 6 Collision Avoidance System…………………………………………………………….41 
Section 7 Fusion of Omni-Directional Vision and Ultrasonic Sensors………………….…………43 
Section 8 Behavior Fusion Design………………………………………………………………….44 
Section 9 Experimental Results…………………………………………………………………….47 
        9.1 Omni-Directional Vision-Based Parking Control………………………………..….47 
        9.2 Ultrasonic Sensors Array-Based Parking Control and Collision Avoidance……..….47 
        9.3 Fusion of Omni-Directional Vision and Ultrasonic Sensors……………..………….48 
Section 10 Conclusions………………………………………………………………….………….51 
References…………………………………………………………………………………..………52 
??????………………………………………………………………………………………55 
????????????………………………………………………………………………56 
 
 
 
 
 2
to explore the information of the parking lot and path profile. Fuzzy set theory is arisen from the 
desire of linguistic description for complex system and it can be utilized to formulate and translate 
the human experience. This kind of human intelligence is easily represented by the fuzzy logic 
control structure. Most advanced control algorithms in autonomous mobile robots can benefit from 
fuzzy logic control. In this project, the CLMR is equipped with one omni-direction vision system 
and 14 ultrasonic sensors. We use the measurements to obtain the information in an unknown and 
dynamic environment, and utilize this information to determine the velocity and the steering angle 
of the car by the proposed FLC. 
 
Approach and steps of the research for the project 
The project is arranged with ten sections that would be interpreted as follows. 
 
Section 1. Hardware Architecture of Car-Like Mobile Robot 
1.1 The Car Body  
The car body is a four-wheel-electric-mobility vehicle manufactured by PIHSIANG MACHINERY 
MFG. CO., LTD. [28]. The brushless DC motor used for controlling the speed is also manipulated 
by [28]. In order to manipulate the steering angle of the CLMR, we reconstruct the steering wheel 
by setting up a DC servo motor together with the driver. The omni-directional vision system is 
constructed in the center point of the CLMR and these ultrasonic sensors are equipped around the 
CLMR. The PC is exploited to process the omni-directional images and sensors data, calculate the 
control scheme, and send the control command to the motors. Figure 1.1 shows the actual architecture 
of the CLMR. 
 
Figure 1.1  Actual appearance of CLMR 
 4
command from PC generates PWM (Pulse Width Modulation) signal and an optocoupler are 
utilized to deliver commands to the motor driver circuit. The method of using the optocoupler to 
divide the signal from motors can avoid physical damage on electric components. The power of the 
DC servo motor is 48W, which is same for DC motor. 
 
1.3.2 Signal Processing Circuit 
We can divide the signal processing circuit into three parts: PC commands, the driver for DC 
motor and the encoder circuit for DC servo motor. 
(1) PC Commands 
To consider that the driver system is suitable for use in all windows operating system, we use the 
parallel port to transform data from PC to driver. Since the parallel port can not push too much load, 
we design another decode circuit to expanse I/O ports. There are four 8255 interface chips and two 
8254 interface chips in the driver system. When PC delivers commands through the parallel port, 
two 8254 chips would generate PWM signal with corresponding commands. In order to prevent 
interference from other electronic devices, the frequency of PWM signals is 10KHz. 
 
(2) The Driver for DC Motor 
The power of the developed CLMR is 300W. To achieve the motor speed control, it must design 
another driver circuit for motors but this has a lot of difficulty on it. Hence, a better way to 
implement motor speed control is based on the original motor controller to add the additional relay 
circuit shown in Figure 1.4. The motor speed control is practicable by way of experiments. 
 
Figure 1.3 The motor driver circuit 
 
 6
 
Figure 1.5 Actual appearance of the quadrate decoder 
 
 
Figure 1.6 An overview of the omni-directional vision system 
 
1.4 Omni-Directional Vision System 
The omni-directional vision system has begun the most popular one because it has the advantages 
of having large fields of view by using an omni-directional camera with a horizontal field of view 
of 360 degree. Landmarks around the omni-directional vision system can be taken in and tracked in 
its broad horizontal field. Recently, various omni-directional image sensors have been proposed. In 
this project, the only sensor used for the CLMR is an omni-directional vision system, which is 
composed of a Video Grabber S-960 and a CCD camera upward looking at a catadioptric mirror 
depicted in Figure 1.6. All of the components in the omni-directional system will be introduced 
individually and in details in the following sections. 
 
1.4.1 The Catadioptric Mirror 
The catadioptric mirror in Figure 1.6 is designed by the Micro-Star-Int’l Co., Ltd. [29]. In general, 
3D lines in the environment are projected as curves in panoramic images. Hence, there exist some 
distortions in the panoramic image. Omni-directional range is calculated by translating the 
catadioptric mirror through a known distance. Estimation of range is based on the fact that the 
panoramic image is range-dependent, irrespective of the nature of the deformation. Using the range 
data, the estimated distance from the center of the CLMR to the object in environment can be 
 8
Table 1.2  Specification of the CCD sensor 
 
 
 
Figure 1.9 The image grabbing box. 
1.4.3 Image Grabbing Box 
The image grabbing box is USB 2.0 Hi-Speed Video Grabber S-960[17] produced by AME 
Optimedia Technology grabs the image real-time in 2-D image processing and then transmits the 
data to the external interface. Figure 1.9 shows the image grabbing box. 
 
1.5 Ultrasonic range sensor 
The measurement of the unknown environment is very important for the CLRM, such as the 
relationship between the CLMR and the obstacle or target. The Parallax PING))) ultrasonic range 
finder provides precise and non-contact distance measurements ranging from 3 cm to 3 meters. The 
sensor works by transmitting an ultrasonic (well above human hearing range) burst and providing 
an output pulse that corresponds to the time required for the burst echo to return to the sensor. By 
measuring the echo pulse width the distance to target can easily be calculated. While the echo 
arrives at time oT  after the emission, ECHO exhibits a transition at measured TOF (time-of-flight) 
 10
 
Table 1.3 Specifications of the Parallax PING))) ultrasonic distance sensor 
Items Specifications 
Measure Distances 30 cm to 3 meter 
Supply Voltage 5 V  
Supply current 30~35 mA 
Delay before next measurement 200µs 
Size 22 mm H × 46 mm W ×16 mm D 
 
 
Figure 2.1 Flow chart of the image processing 
 
Section 2. The Omni-Directional Vision System 
2.1 Image Processing Method 
The first step of the omni-directional vision system is to get the image information and 
preprocess it. An analog video stream produced by an NTSC (National Television System 
Committee) camera mounted on the CLMR is the only one input in the overall system. In the 
experiment, grayscale images and binary images are used to reduce the complex computer 
calculations and increase the image sampling rate. 
The overall flow chart of the image processing is shown in Figure 2.1. The corresponding image 
processes are addressed explicitly as follows. 
 
 12
1w 2w 3w
4w 5w 6w
7w 8w 9w
1z 2z 3z
4z 5z 6z
7z 8z 9z
 
Figure 2.3 An illustration of 3 3×  averaging filter mask 
 
2.1.2 Edge Detection 
Generally, an edge is a set of connected pixels that lie on the boundary between two regions. 
Since we are dealing with local computations, the method of determining a significant value is to 
use a threshold. Thus, we define a point in an image as an edge point if its two-dimensional 
first-order derivative is greater than a pre-specified threshold. A set of such points that are 
connected according to a predefined criterion of connectedness is by definition an edge. It is 
important to note that these definitions do not guarantee success in finding edges in an image. We 
simply use the gradient operator to look for edges. 
First-order derivatives in image processing are implemented using the magnitude of the gradient. 
The gradient of an image ( ),f x y  at location ( ),x y  is defined as the vector 
x
y
f
G xf fG
y
∂    ∂∇ = =    ∂    ∂  . 
(2.2)
An important quantity in edge detection is the magnitude of this vector, denoted by f∇ , where 
( )
1
2 221
2 2 2
x y
f ff mag f G G
x y
  ∂ ∂  ∇ = ∇ = + = +      ∂ ∂      . 
(2.3)
The computation burden of implemented equation (3.2) over an entire image is not trivial, and it is 
common practice to approximate the magnitude of the gradient by using absolute values instead of 
squares and square roots: 
x yf G G∇ ≈ + . (2.4)
Equation (3.3) is simpler to compute and it still preserves relative changes in gray levels. In order to 
simplify the discussion, Figure 2.4 is to denote image points in a 3 3×  region. For example, the 
center point, 5z , denotes ( ),f x y , 1z , denotes ( )1, 1f x y− − , and so on.  
 14
 
Figure 2.6 Edge detection using Sobel operator 
2.1.3 The Skeletonizing Algorithm 
An important approach to representing the structural shape of a plane region is to reduce it to a 
graph. This reduction may be accomplished by obtaining the skeleton of the region via thinning 
(also called skeletonizing [30]) algorithm. The skeletonizing algorithm plays a central role in image 
our image processing because it can effectively improve computational efficiency in a panoramic 
image and avoid wasting time on recording the unnecessary labeling pixels before next image 
processing step. As [30] stated, the skeletonizing algorithm can iteratively delete edge points of a 
region subject to the constraints that deletion of these points (1) does not remove end points, (2) 
does not break connectivity, and (3) does not cause excessive erosion of the region. In other respect, 
the use of skeletonizing algorithm can reduce the redundant unnecessary labels in panoramic 
images and decrease the load of the imaging system. Figure 2.7 is accomplished by utilizing the 
skeletonizing algorithm. 
 
 
Figure 2.7 The image after skeletonizing algorithm 
 
 16
    
Figure 2.9 A labeled result 
 
 
Figure 2.10 Remove un necessary blobs in Figure 3.10 
 
Step 2: 
Group all the associated blobs of the CLMR together into one blob and define the range of the 
CLMR in panoramic images. 
 
Step 3: 
Remove all the blobs, if the numbers of pixels in them are larger than a pre-specified upper 
threshold or smaller than a pre-assigned lower threshold. And the remainder of the blobs will be 
assigned a new blob label value, sees Figure 2.10. 
 
2.2.2 Get the Coordinates of Parking Lot 
Calculate the coordinates of the top-left, top-right, bottom-left and bottom-right points of the 
remaining labeled objects in the image frame. Applying the labeling method to Figure 3.7, we can 
extract the parking lot as in Figure 2.11 
 18
Y
X
L
maxφ
maxφ
minR
 
Figure 3.2 Curvature of the CLMR 
 
3.2 Parallel-Parking Trajectory Design 
Traditionally, a path planner who takes into account of the environmental model as well as the 
vehicle’s dynamics and constraints determines a reference trajectory. The difficulty is in finding a 
good trajectory that is able to simulate satisfactorily a human driver’s behavior, and meanwhile to 
meet the requirement of the steering angle limit of the CLMR. It is important to provide a 
reasonable reference trajectory such that the CLMR can successfully accomplish the parking 
mission. If the reference trajectory is far from a feasible one because of the un-modeled dynamics 
or modeling inaccuracy, the vehicle is unable to follow the trajectory accurately. To avoid the 
abnormality, we must adopt a precise reference trajectory to track. In this section, design of a 
reference trajectory for parallel parking is provided. 
 
3.2.1 Constraints of a Reference Trajectory for Parallel Parking 
In order to generate the appropriate path for parallel parking, we consider the vehicle kinematic 
constraint. The curvature radius of vehicle minR  is shown in Figure 3.2 and can be obtained by the 
following equation, 
min
maxtan
LR φ=  (3.1)
where L is the distance from the front wheel to the rear wheel and φ  is the steering angle. 
 
In the developed CLMR illustrated in section 1, L is 0.85m and maxφ  is about 40° . From 
equation (3.1), the curvature radius of vehicle minR  is 1.01m. In the experiment, the curvature 
radius of the designed reference trajectory is about 76 pixels in the image plane. From the above 
mentioned in section 1, the estimated distance from the center of the CLMR to the object in 
environment can be calculated by fitting polynomial equation (1.1). Substituting p=76 in above 
equation, one can obtain d=1.21m. Thus, the curvature radius of the designed reference trajectory is 
 20
2 2( ) ,  r x x r rx C A R y y H= − − − ≤  (3.2)
2 2(2 ) ( ) ,  < 2r r x x rx R H y C D W H y H= − − − − + ≤  (3.3)
where ( ) ( ) ( ) ( )2,  2,  2, 2.x x x y y y y y x xD A B D A B H B A W B A= + = + = − = −  
The starting point of the parallel parking is constrained by 2W with (WC+WP)/2<W and 2H with 
( )y yR A H− < , where WC is the width of the CLMR and WP is the width of the parking lot. 
The primary work to find the relationship between rx  and ry  is the acquirement of the center 
of the virtual circle at first. If (Ax, Ay) and (Bx, By) are known, the point (Cx, Cy) can be found out as 
shown in Figure 3.4 and the position of the point (Cx, Cy) can be obtained by the following 
equations: 
 
( ) 2x x xE A D= +  (3.4)
( ) 2y y yE A D= +  (3.5)
( ) ( )( )1tan y y x xD A D Aα −= − −  (3.6)
( ) ( )22x x y yAE A E A E= − + −  (3.7)
cosR AE α=  (3.8)
y yC A=  (3.9)
cos(180 2 )x xC D R α= + ° − . (3.10)
 
 22
 
(a) 
 
(b) 
Figure 3.5 (a) Object in the image plane (b) The corresponding location in the practical situation  
 
The change of the orientation angle of parking lot in omni-directional image plane is equal to that 
of the CLMR. Thus, the corresponding coordinate of the CLMR in the image plane and practical 
situation can be described as 
cosM x PCx B r θ= −  (3.13)
sinM y PCy B r θ= −  (3.14)
C Pθ θ=  (3.15)
where ( ),M Mx y  is the position of the CLMR in the transformed coordinate, r is the distance from 
the parking lot to the CLMR, θP is the orientation angle of the parking lot in the image plane and 
θPC is defined in Figure 3.5. θr is the including angle between vertical line and the line through the 
center of the CLMR and the center of the parking lot. The orientation angle θP is estimated by using 
the top-left point and top-right point on the parking lot. According to that, 90PC P rθ θ θ= ° − −  can 
be obtained from the known variables Pθ  and rθ . After the coordinate transformation, a new 
coordinate of relative positions between the CLMR and the parking lot is known.  
 
3.4 Path Error Definition 
After coordinate transformation, the position and the pose of the CLMR are known. The errors 
between the CLMR and the reference trajectory must be defined first and a fuzzy controller is 
applied to let the CLMR to follow the designed reference trajectory via the defined errors. Finally, 
the CLMR completed the parallel-parking mission. 
 24
φ,d θ∆ ∆
( ),M Mx y
( ),T Tx y
 
Figure 3.7 Omni-directional Vision-Based Fuzzy Parallel-Parking Control System 
 
3.5 A Fuzzy Path Tracking Controller 
The main strategy of the vision-based parking control is to make the CLMR follow the desired 
reference trajectory from the starting point to the ending point. The input variables which applied to 
construct the fuzzy path controller are path errors, the position error d∆  and orientation error θ∆ . 
The output of the fuzzy logic controller is the steering angle φ  of the vehicle. The overall control 
block diagram is shown in Figure 3.7. 
Our goal is to implement a path-tracking controller which emulates the human driver behavior by 
using the fuzzy control scheme. The objective is to design a control law so that the position error 
d∆  and orientation error θ∆  converge to 0. The designing process is addressed in the following 
sequence: (1) fuzzification, (2) inference, (3) composition and (4) defuzzification. 
 
3.5.1 Fuzzification 
Fuzzification represented the definition of fuzzy sets, and determination of the degree of 
membership of crisp input variables in appropriate fuzzy sets. The input variables of the proposed 
fuzzy controller are the position error d∆  and orientation error θ∆  which are fuzzified in 
triangular membership functions to simply the computations. The output variable of the fuzzy 
controller is the steering angle φ  of the vehicle. The membership functions of d∆ , θ∆  and φ  
are indicated in Figure 4.13 to 4.15, respectively. 
dµ∆
 
Fig. 4.13 Membership function of d∆  
 26
3.5.3 Composition 
Composition is the aggregation or combination of the outputs of all rules. The composition 
mechanism uses minimum-maximum operator. 
3.5.4 Defuzzification 
Defuzzification is the computation of crisp output. In order to determine the desired steering angle 
φ  from above fuzzy sets, a defuzzification process is required. The weighted average method used 
for defuzzification strategy can be described as  
( )
( )
1
1
n
i i
i
crisp n
i
i
µ φ φ
µ φ
φ
⋅∑
=
∑
=
=  (3.19)
where iφ  is the mean value of each fuzzy set i  and n  is the number of fuzzy rules, and ( )iµ φ  
is the membership function value of control input. 
 
Section 4. Ultrasonic Sensing System for the CLMR 
4.1 The Arrangement of the Ultrasonic Sensors Array 
Different from the previous related work, our robot is not a circular shape. So we can not 
arrange the ultrasonic sensors as a ring. Considering the shape and size of the acrylic fiber platform 
and the ultrasonic sensors, we arrange the 14 ultrasonic sensors as shown in Figure 4.1. The front 
and the rear are placed with two ultrasonic sensors, the right and the left sides are placed with three 
individually. The front and rear ultrasonic sensors mainly are used to detect obstacles, while the 
right and left ones are used to do wall following. Each four oblique ultrasonic sensor which used in 
special cases placed between front and rear two ultrasonic sensors.  
 28
ultrasonic sensors fire at the same time. Though this step may cause the crosstalk noise, we have 
known each distance between the reflector and ultrasonic sensor after the prior steps. So we can see 
the crosstalk noise as information to calculate the position of the reflector. In the next section, we 
will describe the binaural technique by using the crosstalk noise.  
4.3 Determination of the Type and Position of reflector by Binaural Method 
The ultrasonic sensor which we selected has an elliptical detection cone about 40°. If a single 
sensor is used for detection, only the relative distance between the sensor and the reflector can be 
determined, hence, a reflector could be located anywhere on a circular arc within the detection cone 
of the sensor. If two ultrasonic sensors with overlapping detection cones are adopted and a reflector 
is located within this overlapping region, both these two sensors can also receive the sonar waves 
emitted from the neighboring one, and thus the location of the reflector is determined. Another case 
is the reflector is located inside of one of the ultrasonic sensor’s detection range, and the exact 
location of the object can be calculated by binaural method.  
 
Figure 4.3 The firing order of the ultrasonic sensors array 
 
 30
almost circle arcs, and the position of the reflector is decided by these two circular arcs. The other 
case is that the reflector is in the detectable region of Sensor1only as shown in Figure 4.5 (b) that 
Sensor 2 can not detect the reflector by itself but can receive the echo transmitting from the 
transmitter of Sensor1 via the receiver of Sensor 2, so we can use this crosstalk to sketch an elliptic 
curve to determine the position by this elliptic curve and the circular arc from Sensor1. 
 
4.3.2 Determination of Different Type of Reflectors by Binaural Method 
The binaural approach enables us easily to determine the position of an ideal point-type reflector, 
but it is ambiguous to recognize the intersection point of the equidistant curves corresponds to a 
virtual non-existing target when detecting a planar wall as shown in Figure 4.6. As the behaviors of 
the CLMR are closely related to the types of reflectors, if we can not judge correctly whether the 
reflector is a planar wall or a point obstacle, the CLMR will run in the incorrect mode disastrously. 
In order to achieve the wall following mode correctly, we must exactly examine the reflector at the 
side whether it is a planar wall or not. There are two cooperative methods to ensure that the 
reflector is a wall: (1) the multi-channel method, and (2) the displacing position method. 
( , )x y
( , )r rx y( , )tr trx y( , )t tx y
 
Figure  4.4.  The diagram of the ellipse and parameters 
 
Figure 4.5.  Position calculation of a reflector by binaural method 
 32
identify the type of the reflector, we need additional method for justification. 
 
 
Figure 4.8  Detection judgment with additional displacing method 
 
(2) The displacing position method. 
Only with the multi-channel method, we still cannot judge whether the reflector is a wall or not. 
At this situation, we let the CLMR move ahead a little more and sense the reflector again. After the 
CLMR has changed its position, if the measurement data is similar to the previous one, then, we can 
conclude that the reflector is a planar wall due to the minor change of the echo signal as shown in 
Figure 4.8 (a). The other case is shown in Figure 4.8 (b), where the reflectors are two obstacles. 
After the CLMR moves forward a small distance, it results that the Sensor1 cannot detect the 
obstacle any more and only the intersection is result of emissions from Sensor 2 and Sensor 3. By 
this result, we can make sure that the reflectors are not just a planar wall. So we can efficiently 
judge the type of reflectors by the side-mounted sensors on the CLMR. 
 
 34
5.1.1 Design Right-Side FWFC 
The steering angle control is the kernel of the FWFC where the two inputs are the deviation of the 
CLMR from the safety distance dX  and the difference of distance to the wall sensed by the RF 
sensor and RB sensor eX , and φ  is the corresponding output as follows: 
1
2
      for forward driving
    for backward drivingd
d Dist
X
d Dist
−=  −
 
1 2eX d d= −  
IF dX  is a positive, it means that the wheel of the CLMR is located outside of the safety 
distance, i.e. the CLMR is moving away from the wall; else if dX  is negative, it indicates that the 
wheel of the CLMR is located inside of the desired path, i.e. the CLMR is near to the wall. IF eX  
is a positive number, it shows that the CLMR is outgoing away from the wall, else if eX  is a 
negative number, it presents that the CLMR is approaching to the wall.  
All two input values dX  and eX , and output value φ  of the FWFC are decomposed into five 
fuzzy partitions, denoted by negative big (NB), negative small (NS), Zero (ZE), positive small (PS), 
and positive big  (PB).The partitions and the shapes of the membership function are illustration in 
Figure 5.2.  
There are 25 rules for the right-side forward FWFC which can be expressed in the following 
linguistic forms  
R1?if dX is Positive Big and eX  is Positive Big, then φ  is Zero. 
R2?if dX  is Positive Big and eX  is Positive Small, then φ  is Negative Small. 
#  
R25?if dX  is Negative Big and eX  is Negative Big, then φ  is Zero. 
The right-side forward and backward FWFC rule table is summarized in Table 5.1. 
 36
 
Figure 5.3 Flow chart of wall following. 
We also choose the same membership functions and partitions of dX , eX , and φ  used at 
right-side FWFC. The physical meaning of the dX  is the same as that of the right-side FWFC, but 
the eX  is different from previous one. IF eX  is a positive, it shows that the CLMR is 
approaching to the wall, else if eX  is a negative, it presents that the CLMR is leaving away the wall. 
The flow chart of wall-following mode is depicted in Figure 5.3. 
 
5.2 Fuzzy Parallel-Parking Mode 
For the parallel-parking mode, there are some basic constraints must be set first to ensure the 
parking-lot has enough space for the CLMR parking: 
(1.2W< RFd <1.5W) and (1.35L< FRONTd  <1.75L) and (1.6L< OFRd  <2.2L) 
where W is the width of the CLMR, L is the length of the CLMR, RFd  is the distance detected by 
right front ultrasonic sensor, FRONTd  is the distance detected by front ultrasonic sensors, and OFRd  
is the distance detected by oblique front right ultrasonic sensor. 
The parallel-parking mode is shown in Figure 5.4. This mode has the ability not only to park the 
car correctly but also to avoid any accidental collision with the obstacle on the parking path and 
with the wall of the parking lot. We decompose the process of parking into four steps as shown in 
Figure 5.5. 
 38
Step 1) Check if there is any obstacle blocking on the parking path. As the RF and OFR have 
both conformed to the parallel-parking mode, but, suddenly, the front ultrasonic 
sensors detect an obstacle is intruding on the desired parking path or is trying to 
occupy the parallel-parking lot. The CLMR would halt for a while and deal with this 
unexpected situation, which results into two cases.  
Case 1) The obstacle moves away from the parking path, the CLMR could keep on the 
parallel-parking mode. Proceed to Step 2). 
Case 2) The obstacle blocks the path or occupies the parking lot, then; the CLMR would 
move forward to find the next potential parking lot. 
If there is no obstacle intrusion observed, proceed to Step 2). 
Step 2) The CLMR moves forward ahead of the parking lot a little and is ready for parking , 
once the sensors have detected the parallel-parking lot thoroughly and then the CLMR 
would turn its wheels left and moves forward slightly until oblique back right (OBR) 
ultrasonic sensor can detect the wall completely. 
Step 3) The CLMR turns its wheel to right and starts to move backwardly until that the OBR 
sensor can sense the suitable distance between the CLMR and the parking lot side, 
then, the CLMR would turn the steering wheel to the left end and drive the car 
backwardly until the rear ultrasonic sensors can detect that the CLMR is close enough 
to the wall. 
Step 4) Examine the position of the CLMR to see whether it is properly positioned in the 
parking lot via sensors. If the position of the CLMR does not meet the required 
condition, it uses the right-side FWFC to find the correct parking position. 
 
5.3 Fuzzy Garage-Parking Mode 
Similar to the parallel-parking mode, the basic constraint for garage-parking condition is: 
(1.4 LRd  < RFd <1.8 LRd ) and (1.8W < FRONTd <2.4W ), then (1.4 LRd < RMd <1.8 L ). 
where RMd  is the distance detected by the right middle ultrasonic sensor. The parking process of 
the procedure fuzzy garage-parking mode behavior is shown in Figure 5.6. We decompose the 
process into the five steps as shown in Figure 5.7. 
 40
 
Figure 5.7. Five steps for garage-parking mode: (a). step 1), (b). case1), (c). case 2), (d). case 1) of 
step 2), (e). case 2) of step 2), (f). step3), (g). step 4). (h). step 5) 
 
Step 1) Check if there is any obstacle blocked the parking path. As the RF has conformed to 
the garage-parking mode, but, suddenly, the front ultrasonic sensors detect an obstacle 
is intruding in the desired parking path or is trying to occupy the garage-parking lot. 
The CLMR would halt for a while and deal with this unexpected situation which could 
be divided into two cases.  
Case 1) The obstacle is moving away from the parking path, the CLMR could continue on 
the garage-parking mode.  
Case 2) The obstacle is trying to occupy the parking lot; the CLMR would move forward 
to find the next potential parking lot. 
 42
 
 
Figure 6.8.  Flow chart of the obstacle avoidance procedure. 
R              F
 
 (a) (b) (c) 
 
R                  F
R              F R                  F
R              F
 
 (d) (e) 
Figure 6.9.Three steps for obstacle avoidance mode: (a) step 1), (b). case 1, (c). case 2, (d). step 2), 
(e) step 3)  
 44
 
Figure 8.1 The behavior fusion mechanism 
Section 8. Behavior Fusion Design  
How to decide next movement via the sensing information is a crucial challenge while the CLMR 
is moving in the unknown and changing environments. As to accommodate to complete ground 
tests, we propose different fuzzy behavior modes to accomplish the associated goals. When the 
suitable behavior mode for the field is selected by the mode switching mechanism, we do not hope 
any other non-suitable behavior mode interferes with the correct one. Each mode is an independent 
one, so we adopt the modes switching mechanism to switch the system to the best suitable mode 
which is based on the sensor information and control commands. The Figure 8.1 shows the behavior 
fusion mechanism.  
The control commands comprise four kinds: 
1) Stop: Command the CLMR to stop immediately which is the highest priority command. 
2) Wall-following: Command the CLMR to follow a planar wall. 
3) Parallel-parking: Command the CLMR to seek any available parallel parking lot and 
accomplish the associated parking work. 
4) Garage-parking: Command the CLMR to seek any available garage parking lot and 
accomplish the associated parking work. 
5) Obstacle avoidance: Command the CLMR to sense any potential obstacle, and then find a 
safe path to bypass it. 
The entire behavior modes are fused together as autonomous driving mode to command the 
CLMR to act by itself autonomously at the nearby environment. 
Here are the situations as the CLMR will act autonomously: When the CLMR drives in 
parallel-parking mode or garage-parking mode, the safety distance of the right-side FWFC would be 
smaller and close slightly to the wall, and then the CLMR will stop for a while when the front 
ultrasonic sensors detect the obstacle until the obstacle is clear; In obstacle avoidance mode, the 
CLMR will keep wider safety distance of the FWFC and detour round the obstacle safely; If the 
 46
garage-parking lot, the right front ultrasonic sensor measures the length and front ultrasonic sensors 
measure the width of the parking lot. In this step, we judge whether the space is large enough for 
parking. At last we check if there is any object in the garage-parking lot by the right front and right 
middle ultrasonic sensors. If the field is recognized as a garage-parking field, we change the mode 
to the garage-parking mode. The last case is as shown in Figure 8.5. If the front ultrasonic sensors 
detect of an obstacle on the forward path by binaural method, we change the mode to the obstacle 
avoidance mode. According to the measured the right wall distance and the left wall distance to the 
CLMR, we find the suitable path to detour around the obstacle without any collision. 
 
Figure 8.3 The parallel-parking lot scanning 
 
 
Figure 8.4 The garage-parking lot scanning 
 
 
Figure 8.5 The obstacle scanning by the front end sensors 
 48
contingency.  
Figure 9.6 demonstrate the CLMR can select the suitable parking-lot and parking. In Figure 9.7, it 
is shown the forward obstacle avoidance mode. Figure 9.8 illustrate the CLMR will pass through 
the parking-lot in obstacle avoidance mode. Figure 9.9 demonstrate the CLMR will detour round 
the obstacle and parking in the parallel parking-lot.  It demonstrates the sequential images captured 
by a hand-held CCD camera. From the experimental results, we illustrate that the ultrasonic sensing 
system and autonomous parking and obstacle avoidance control is practicable. The experimental 
results demonstrate the practicability and effectiveness of the proposed autonomous parking and 
obstacle avoidance control. We like to emphasize that once the CLMR is powered on, it ahs the 
capability to autonomously maneuver in the test field.   
 
 
Figure 9.3 Experimental results of the parallel-parking mode 
 
 
Figure 9.4 Experimental results of the garage-parking mode 
 
 50
 
Figure 9.7 Experimental results of the obstacle avoidance mode 
 
 
Figure 9.8 Experimental results of the obstacle avoidance mode with parking-lot 
 
 
Figure 9.9 Experimental results of the autonomous mode with parking-lot 
 52
ultrasonic sensors array. The developed scheme has the potential applied to a real car if it equips 
with omni-directional vision system and these ultrasonic sensors array. 
 
Reference 
[1] M. Sugeno and K. Murakami, “An experimental study on fuzzy parking control using a 
model car,” in Industrial Applications of Fuzzy Control, M. Sugeno, Ed. North-Holland, The 
Netherlands, pp. 105–124, 1985. 
[2] M. Sugeno, T. Murofushi, T. Mori, T. Tatematsu, and J. Tanaka, “Fuzzy algorithmic control 
of a model car by oral instructions,” Fuzzy Sets Syst., vol. 32, pp. 207–219, 1989. 
[3] A. Ohata and M. Mio, “Parking control based on nonlinear trajectory control for low speed 
vehicles,” in Proc. IEEE Int. Conf. Industrial Electronics, pp. 107–112, 1991. 
[4] S. Yasunobu and Y. Murai, “Parking control based on predictive fuzzy control,” in Proc. 
IEEE Int. Conf. Fuzzy Systems, vol. 2, pp. 1338–1341, 1994. 
[5] W. A. Daxwanger and G. K. Schmidt, “Skill-based visual parking control using neural and 
fuzzy networks,” in Proc. IEEE Int. Conf. System, Man, Cybernetics, vol. 2, pp. 1659–1664, 
1995. 
[6] A. Tayebi and A. Rachid, “A time-varying-based robust control for the parking problem of a 
wheeled mobile robot,” in Proc. IEEE Int. Conf. Robotics and Automation, pp. 3099–3104, 
1996. 
[7] H. An, T. Yoshino, D. Kashimoto, M. Okubo, Y. Sakai, and T. Hamamoto, “Improvement of 
convergence to goal for wheeled mobile robot using parking motion,” in Proc. IEEE Int. 
Conf. Intelligent Robots Systems, pp. 1693–1698, 1999. 
[8] M. Ohkita, H. Mitita, M. Miura, and H. Kuono, “Traveling experiment of an autonomous 
mobile robot for a flush parking,” in Proc. 2nd IEEE Conf. Fuzzy System, vol. 2, Francisco, 
CA, pp. 327–332, 1993. 
[9] D. Lyon, “Parallel parking with curvature and nonholonomic constraints,” in Proc. Symp. 
Intelligent Vehicles, Detroit, MI, pp. 341–346, 1992. 
[10] I. E. Paromtchik and C. Laugire, “Motion generation and control for parking an autonomous 
vehicle,” in Proc. IEEE Conf. Robotics Automation, vol. 4, Minneapolis, MN, pp. 3117–3122, 
1996. 
[11] K.Y. Lian, C. S. Chin, and T. S. Chiang, “Parallel parking a car-like robot using fuzzy gain 
scheduling,” in Proc. 1999 IEEE Int. Conf. Control Applications, vol. 2, pp. 1686–1691, 
1999. 
[12] K. Jiang, “A sensor guided parallel parking system for nonholonomic vehicles,” in Proc. 
IEEE Conf. Intelligent Transportation Systems, pp. 270–275, 2000. 
[13] J. Xiu, G. Chen, and M. Xie, “Vision-guided automatic parking for smart car,” in Proc. IEEE 
Intelligent Vehicles Symp., pp. 725–730, 2000. 
[14] D. Gorinevsky, A. Kapitanovsky, and A. Goldenberg, “Neural network architecture for 
trajectory generation and control of automated car parking,” IEEE Trans. Contr. Syst. 
 54
[30] Rafael C. Gonzales, and Richard E. Woods, Digital Image Processing, Second Edition, 2002. 
[31] W. L. Xu and S. K. Tso, “Sensor-based fuzzy reactive navigation of a mobile robot through 
local target switching,” IEEE Trans. Syst., Man, Cybern. C, vol.29, pp. 451-459, Aug. 1999 
[32] D. Bank, “A novel ultrasonic sensing system for autonomous mobile system,” IEEE Sensors 
journal. vol. 2, pp. 597-605, Dec. 2002  
[33] R. Kazys and L. Mazeika, ”Determination of spatial position of multiple targets by ultrasonic 
binaural method,” Ultrasonics Volume: 40, Issue: 1-8, May, 2002, pp. 397-402 
[34] R. Kuc, Biomimetic sonar recognizes objects using binaural information, J. Acoust. Soc. 
Amer. 102 (2, Pt.1) (1997) 689 
[35] L. A. Zadeh, “Fussy Sets,” Informat. Contr., Vol. 8, pp. 338-353, 1965. 
[36] L. A. Zadeh, “Fuzzy Algorithms,” Inform. Control, Vol. 12, pp. 94-102, 1968. 
[37] D. Gorinevsky, A. Kapitanovsky, and A. Goldenberg, “Neural network architecture for 
trajectory generation and control of automated car parking,” IEEE Trans. Contr. Syst. 
Technol., vol. 4, pp. 50–56, Jan. 1996. 
[38] S. Lee, M. Kim, Y. Youm, and W. Chung, “Control of a car-like mobile robot for parking 
problem,” in Proc. IEEE Int. Conf. Robotics Automation, 1999, pp. 1–6. 
[39] T.-H. S. Li and S.-J. Chang, “Autonomous Fuzzy Parking Control of a Car-Like Mobile 
Robot,” IEEE Transactions on Systems, Man, and Cybernetics, vol. 33, pp. 451–465, 2003. 
[40] T.-H. S. Li, S.-J. Chang, and Y.X. Chen, “Implementation of human-like driving skills by 
autonomous fuzzy behavior control on an FPGA-based car-like mobile robot”, IEEE Trans 
on Industrial Electronics, vol. 50, NO.5, pp. 867-880, 2003. 
[41] C. S. Ting, T. H. S. Li, and F. C. Kung, “An approach to systematic design of the fuzzy 
control system,” Fuzzy Sets Syst., vol. 77, pp. 151–166, 1996. 
[42] T. H. S. Li and M. Y. Shieh, “Switching-type fuzzy sliding mode control of a cart-pole 
system,” Mechatronics, vol. 10, pp. 91–109, 2000. 
[43] O. Kaynak, K. Erbatur, and M. Ertugrul, “The fusion of computationally intelligent 
methodologies and sliding-mode control—a survey,” IEEE Trans. Ind. Electron., vol. 48, pp. 
4–17, Feb. 2001. 
[44] S. Hutchinson, G. D. Hager, and P. I. Corke, “A tutorial on visual servo control,” IEEE Trans. 
Robot. Automat., vol. 12, pp. 651–669, Oct. 1996. 
[45] J.-S. Cho, H. W. Kim, and I. S. Kweon, “Image-based visual servoing using position and 
angle of image features,” Electron. Lett., vol. 37, pp. 208–214, 2001. 
[46] http://www.altera.com 
[47] User’s Manual, L293D Quadruple Half-H Derivers, June, 2002. 
[48] I.-S. Chen, “The design and development of the car-like mobile robot intelligent parking 
functions,” M.S. project, Dept. Elect. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan, 2002. 
[49] C.-J. Chang, “The implementation of the dual-sensor autonomous car-like mobile robot with 
NIOS embedded system,” M.S. project, Dept. Elect. Eng., Nat. Cheng Kung Univ., Tainan, 
Taiwan, 2005.  
 
