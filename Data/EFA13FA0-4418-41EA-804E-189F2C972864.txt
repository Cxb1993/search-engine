switching no longer satisfy the diversity of the 
networking applications. To adapt to the fast change 
in network, the next generation packet switches not 
only need high speed switching ability, but also to 
be more intelligent in resolving and processing 
multitudinous Internet traffic flows aggregated by 
numerous packets, especially P2P application traffic. 
More specifically speaking, based the theories of 
classic packet inspection, classification and 
identification, it can adopt strategies for packet 
flows according to their behaviors through layer 2 to 
layer 7. Therefore, this sub-project arms to design 
and develop the related core technologies of 
the ’High-Speed Application Service Switch’ 
(HiSASS) based on the architecture of the main 
project ’The Research of High Speed Packet Switch’. 
Our goal is to construct the new generation of 
application service switch system with Packet in 
Strategy capability in all aspect of views: Packet in 
Silicon, Packet in Software, and Packet in System. In 
other words, this sub-project is mainly aimed at 
providing the High Speed Packet Switches with an 
Intelligent Strategy-based Application Service Switch 
Sub-system, which can efficiently enhance the 
capabilities of the High Speed Packet Switch system. 
To extend the research achievement in Program for 
Promoting Academic Excellence of Universities（Phase 
II）, we not only resolve and classify the packet 
flows, but also do the optimal operations such as 
forwarding, sinking, modifying, caching and proxy 
according the policies and strategies. Followings are 
the core technologies of the HiSASS to be developed: 
(1) Application Service Inspection Engine  
(2) Intelligent Strategy-based Engine  
(3) Flow-based Switching Engine  
(4) High-Availability Architecture  
(5) Traffic Emulator and Testbed  
 
This project is a three-year project. For the first 
year, we’ll design the architecture of the HiSASS, 
studying and estimating the suitable kernel platform 
and algorithms. In the 2nd year, we’ll implement the 
I 
 
行政院國家科學委員會補助專題研究計畫 
■ 成 果 報 告   
□期中進度報告 
 
高速封包交換機之研究--子計畫六:高速應用服務交換機之研製 
Research and implementation for high-speed application service switches 
 
計畫類別：□ 個別型計畫  ■整合型計畫 
計畫編號：NSC 97-2221-E-007-108-MY3 
執行期間： 97年 8月 1日至 100年 7月 31日 
 
計畫主持人：黃能富 教授   國立清華大學資訊工程學系 
共同主持人： 
計畫參與人員：許智堯、陳嘉祥、游鈞為、林聖雄、吳正鈺、馬志舜 
 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  ■完整報告 
 
本成果報告包括以下應繳交之附件： 
■赴國外出差或研習心得報告三份 
□赴大陸地區出差或研習心得報告一份 
□出席國際學術會議心得報告及發表之論文各二份 
□國際合作研究計畫國外研究報告書一份 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
          
執行單位：國立清華大學資訊工程學系 
 
 
中   華   民   國   100   年   9   月   31   日 
III 
 
Abstract 
With the popularity of network and prevailing of P2P applications, the conventional arts for 
routing and switching no longer satisfy the diversity of the networking applications. To adapt to the 
fast change in network, the next generation packet switches not only need high speed switching 
ability, but also to be more intelligent in resolving and processing multitudinous Internet traffic flows 
aggregated by numerous packets, especially P2P application traffic. More specifically speaking, 
based the theories of classic packet inspection, classification and identification, it can adopt strategies 
for packet flows according to their behaviors through layer 2 to layer 7. Therefore, this sub-project 
arms to design and develop the related core technologies of the “High-Speed Application Service 
Switch” (HiSASS) based on the architecture of the main project “The Research of High Speed Packet 
Switch”.  
Our goal is to construct the new generation of application service switch system with Packet in 
Strategy capability in all aspect of views: Packet in Silicon, Packet in Software, and Packet in System. 
In other words, this sub-project is mainly aimed at providing the High Speed Packet Switches with an 
Intelligent Strategy-based Application Service Switch Sub-system, which can efficiently enhance the 
capabilities of the High Speed Packet Switch system. To extend the research achievement in Program 
for Promoting Academic Excellence of Universities（Phase II）, we not only resolve and classify the 
packet flows, but also do the optimal operations such as forwarding, sinking, modifying, caching and 
proxy according the policies and strategies. Followings are the core technologies of the HiSASS to be 
developed:  
(1) Application Service Inspection Engine  
(2) Intelligent Strategy-based Engine  
(3) Flow-based Switching Engine  
(4) High-Availability Architecture  
(5) Traffic Emulator and Testbed  
 
This project is a three-year project. For the first year, we’ll design the architecture of the 
HiSASS, studying and estimating the suitable kernel platform and algorithms. In the 2nd year, we’ll 
implement the HiSASS components with related technologies. Finally, in the last year, we will 
integrate each component and complete the HiSASS system. The Traffic Emulator and Testbed will 
be employed for performance analysis and evaluation.  
 
Keywords: Application switch, Service Switch, Packet Inspection, Flow-based Switching  
 
 
 
 
 
 
2 
 
 
二、 文獻探討 
在網路安全檢測的技術之中，字串比對(string matching)是最為重要的技術。一項統計指
出，在著名的 IDS(intrusion detection system) Snort 系統運作時， string matching 佔了 80%
以上的 CPU 計算時間。因此，一個字串比對演算法的優劣，將大幅影響一個網路安全檢測
系統的穩定性及準確性。近年來各種字串比對的演算法及各種輔助的機制相繼被提出，而
其中一種基於狀態機(automata-based)的字串比對法被廣泛地運用，有大量的研究都根基於
此，以下將逐項介紹近年來相關之研究及文獻。  
因為”應用服務檢測引擎之研究與開發”為本年度研究的基礎，而 pattern matching 演
算法為網路應用程式檢測的關鍵技術之一，故在本此期中精簡報告中，我們先針對 pattern 
matching 演算法進行探討。  
AC 演算法[9]的想法是來自於有限狀態自動機。 AC 的優點在於用硬體實做容易，在
最壞的測試情況底下，其效能與一般情況的效能是一致的。不過它的問題在於有限狀態自
動機的實做需要大量的記憶體，對於像 ClamAV 這樣巨大病毒特徵資料庫，是無法在一般
的網路設備上實現。雖然壓縮有限狀態自動機可以解決記憶體不足的問題[10]，但副作用是
犧牲比對性能。  
WM 演算法[11]最早被使用於 UNIX 的 agrep 與 glimpse 工具程式[12] [13]。 WM 的
構想是來自於 BM 演算法[14] 的 Bad-Character matching 的改良， BM 一次只能比對單
一的特徵碼，而 WM 一次可以比對多個特徵碼。由於 WM 是使用 hash 的方法來進行字
串比對的加速，所以它的缺點也是在這上面，當特徵碼變多， hash collision 的次數增加， 
hash 無法有效過濾的時候，WM 的效能就會變不好。  
Clam AV 依照病毒特徵碼的種類的不同，使用不同的比對技術  ClamAV-AC 與 
ClamAV-BM，它們分別修改自 AC 與 BM，其中 ClamAV-BM 比較接近 WM。ClamAV-AC 
所做的改良為把傳統用來做有限狀態自動機的 array 改為 linked list，這樣可以節省記憶
體。  
Hash-AV[15]使用 Bloom Filter[16]，以改善 ClamAV 的比對性能，但它依然有 hash 的
問題，當前端的 Bloom Filter 無法精確過濾的時候，效能就會大打折扣。 
 
Aho-Corasick字串比對演算法 
這個演算法在 1975由 A. V. Aho and M. J. Corasick提出，主要是根據攻擊的特徵組 
(signatures)，建立出一有限狀態機，再對任一字串輸入進行比對之運算。所謂攻擊特徵組，是
由各種已知的攻擊中，找出其特殊且可辨認之樣式 (pattern)，蒐集而成一資料庫，再對於任何
的可疑輸入進行檢查，若有發現相同的樣式，即可依所存有的資訊做遭受何種攻擊之判斷。為
了有快速地進行字串比對的工作，AC演算法依據所蒐集之特徵組建立一有限狀態機，並保有一
當前狀態(初始化為零)，對於任一個輸入的字元，將依當前狀態決定下一狀態的值，而所有的
狀態轉換規則都存在此有限狀態機內。此演算法最大的優點是字串比對的時間只跟該字串的長
度有關，而與特徵組的多寡無關。 
以下舉例說明 AC演算法之運作過程。現假設有一包含 he, her, him, his四種樣式之特徵
組，且假設輸入之字串為 simhehihersr。首先建立一個只有一初始狀態為 0之有限狀態機。再
4 
 
AC-Bitmap  
由表一可知，大部分的儲存內容皆相同 (為零)。因此，若採用 NFA表示法，即在每個狀
態中只紀錄有路徑往下走的輸入值之下一狀態，將可大幅地節省空間。但如何判定是否有路徑
繼續往下及如何得知下一狀態? N. Tuck, T. Sherwood, B. Calder, G. Varghes [2] 在2004
年的 Infocom 中提出了一個解決此問題的演算法，即用一bitmap來紀錄某一狀態中輸入為某
一字元時是否有路徑，如表二所示，為對應到圖1之有限狀態機的bitmap。 
表二、圖1之有限狀態機的bitmap 
 
當查詢 bitmap時，若結果為 0，則表示當前狀態對目前的輸入字元沒有路徑。反之，若
結果 1，可知有路徑往下一狀態。 AC-bitmap中，會另外儲存一個下一狀態列，紀錄著所有當
前狀態有路徑往下走的下一狀態，即對每個 bitmap中為 1的位置，都要有一對應的下一狀態，
如表三所示。 
表三、圖1之有限狀態機的下一狀態列 
 
在執行狀態轉換的過程中，可藉由表三算出所對應的 bit之前有幾個 bit為 1(popcount運
算)，再查詢表三，即可得知下一狀態。例如，假設當前狀態為 1而輸入為 i時，由表二可知，
在 i之前有二個 bit為 1，因此下一狀態為表三中當前狀態為 1之下一狀態列 1 2 4中的 
4。此外，對於沒有路徑往下的情形，也頇額外儲存一失敗路徑表(failure path)，但由於失敗路
徑表只頇紀錄在每個狀態沒有路徑時應轉換到的下一狀態，所以對記憶體的消耗量相對 DFA
而言小的多。不過對一個輸入字元，可能有多次的 failure path轉換，最後才找到有路徑可走的
下一次狀態，因此 NFA的比對速度一般來說較 DFA為慢，而 AC-Bitmap又因為需要大量的 
popcount運算，因此只適合在硬體上實現，才能有比較好的效果。 
6 
 
 
圖 2、Experiment environment 
 P2P快取流量本地化演算法研製 
為了減少 BT 所產生壓倒性的的 ISP 之間的流量，我們提出一個基於快取(cache)的代
理伺服器，稱之為 B-代理伺服器(B-Proxy)。B-代理伺服器被設計成離線模式，因此即使錯
誤產生也不會對網路上原本的運作產生影響。 
在一個大型的自治系統(Autonomous System)中通常會有數百到數千個 torrent 需要被本
地化(Localize)，然而相關的資源包含儲存空間以及一台伺服器的網路功能將會成為瓶頸。
快取裝置所能提供的流量將會是 ISP 之間所能節省的最大流量。 
在 B-伺服器中有兩個主要的部分： BitTorrent 分析器(BitTorrent Analyzer, BA)以及 
BitTorrent 超級 Peer(BitTorrent Super Peer, BSP)。BitTorrent 分析器接收來自邊界路由器的
鏡像網路流量並且決定哪些 torrent 需要被本地化。在選定目標 torrent 之後，BitTorrent 伺
服器會立即將從將鏡像網路流量取得相關的資訊(包含 Tracker 名單以及內部 Peer 名單)傳
給 BitTorrent 超級 Peer 做為流量本地化之用。整個架構如下圖 2 所示： 
 
圖 3、代理伺服器的配置 
 機器學習即時分類演算法研製 
機器學習演算法的方法中，大部分的統計屬性都用於粹取整個 flow 的特徵，也因此這
些方式只適用於離線分類或線上流量分析。另一方面，就即時分類的方法而言，目前所提
出的方法多只針對 TCP 連線進行辨識， UDP 流量辨識仍屬未廣泛觸及的課題。也因此本
方向研究仍有許多待探討的課題。我們的出發點在於：若需要反映原來網路各點在應用層
的實際互動，並達到支援線上即時分類的目標，則應針對應用軟體初始協商的特徵，以應
8 
 
 
圖 5、特徵碼的分串方式 
 
圖 6、nextPat 的長度分佈 
我們從圖 5 整個 ClamAV 的特徵碼分析可以知道，每一個 pattern A 都可以找到另外一
個 pattern B，使得 A 的尾與 B 的頭至少有一個 byte 重複，而大部分的 patterns 的長度
座落在 100-199 這個區間(因為沒有 nextPat 落在 0-99 的區間)。 
為了縮減 nextPat 指標的記憶體使用量，我們把 nextPat 用 Bit Map 對應來改良，如
圖 6 所示。 
10 
 
圖 8、我們所研發的字串比對演算法效能分析 
 智慧型交換引擎之開發 
傳統的 Layer 2 switch，只要能夠識別 MAC address 就可以進行交換，後來因為網路
應用逐漸增多，必頇要能夠依照 IP address 與 TCP/UDP 的 service port number 進行封包
的排程與交換，故此 Layer 4 switch 應運而生。然而，應用程式的開發者並不想要被這樣
的網路設備所侷限，所以有些會把自己的通訊內容包裝在既有的通訊協定中(EX：HTTP)，
最簡單的例子為，非 HTTP 的通訊，卻使用 HTTP 的 service port 80，這樣可以欺瞞過比
較低階的 Layer 4 switch。而比較複雜的欺瞞方式可以這樣包裝 packet  
Packet=HTTP(Private(data)) 
這樣 packet 的外面看起來就真的像一個 HTTP，但是它裡面其實是別的應用，例如： 
P2P。 
針對只使用類似的 service port 的 application，我們比較好處理，只要看它 packet 的 
payload 是否直接符合 service port 的通訊協定，即可鑑別。對於這個部分，我們開發了一
個 HTTP packet checker 來驗證我們的想法。 
對於會真的把自己包在別的通訊協定中的應用通訊，只能一層層地經過  pattern 
matching，把它封裝的通訊協定解開，才會知道裡面包什麼，萬一它的 private(data) 是經
過加密的，在不知道 key 的情況下，要能解開的機會就不大。 
當我們精確地知道某個 flow 是屬於那個 application 的時候，才能給予適當 switching 
priority，也才能正確地實作出 smart switch。 
 以封包流交換技術開發封包處理流程架構 
封包流程處理在 Flow Router 中的概念如圖 8 所示。基本上，黃色的部分是希望被限
制的 application traffic (EX:P2P)，經過 Flow Engine 之後，黃色的部分可以精確地被控制，
而其它的 application 可以得到應得的 bandwidth。 
 
圖 9、Flow Router 中的 Flow Manager 機制 (Source: 
http://spectrum.ieee.org/computing/networks/a-radical-new-router/0) 
圖 8 是我們設計的 Flow Engine 的 packet processing flow chart。首先必頇針對進來的 
packets 進行完整性檢查，因為網路上有時會有為了攻擊的目的而產生的不完整封包。檢查
 
12 
 
 
圖 10、HAC 架構 
 
一般的研究使用了兩種截然不同的 HA scheme。例如在圖 1，在一般的操作中使用了
active /backup(AB) scheme 昨為傳導所有連結直接流量的方法。如果 SPE 的主要連結服務發
生故障，系統會將連線轉移至 backup 連線。而在另一種 active/active(AA)的方法中，在傳
輸單的交換機會透過 AA 連線方法平衡利用直接連接來做平衡傳輸流量。在這兩種傳輸方
法中，SPE 可以獲得可靠的服務，在任何失敗發生後可以透過直接連線進行備份及遷移。
在 AB 的方法中，兩個 SPE 的將扮演相同的角色提供相同的功能，以達備份的主要功能。
這兩種 HA scheme，在 SPE 的使用方面，都讓 SPE 扮演相同的角色，只是在 AA 的方法中，
將流量分為兩份，以達 Load-balance 的效果。無論是哪種方法，這樣的 HA scheme 都是以
簡單設及實線作為主要考量，但在現有環境中已不是十分適用。 
引此，這篇論文提出兩個新的資料結構，稱為 Flow Digest (FD) 和 Multi- level Counting 
Bloom Filter (MLCBF)，作為低資源消耗的狀態複製解決方案。根據之前的文獻探討與分
析，這是第一次有研究將雜湊函式引入了高可用度叢集的狀態複製機制之中。本論文利用
數學分析和大量測試 (包括利用實際網路流量測試來做模擬，以及測試平台上的測試)，來
評估所提出方法的效能和各種得失。 
14 
 
0
2
4
6
8
10
12
14
16
10 20 30 40 50 60 70 80 90
Exp. MLCBF(4,8) Ana. MLCBF(4,8)
Exp. MLCBF(8,4) Ana. MLCBF(8,4)
Exp. DLCBF(4,8) Exp. DLCBF(8,4)
Load (%)
A
vg
. s
e
ar
ch
 c
o
st
s 
(p
ro
b
in
g 
ce
lls
)
 
圖 13、MLCBF 搜尋成功機率比較 
 
以現今的網路需求中，能夠監測應用程式層封包的偵測引擎已經越來越重要，而在這
些內容監測的技術中最重要的是需要一種高效能的多重匹配演算法，而這種演算法應當可
以精確地用於數據封包間的字串匹配。我們提出了一個新型的增強型分層多重匹配算法
（EHMA）用於封包內容檢查。在一個普通的發生頻率模型基礎上，我們利用 EHMA 的方
法進行測試，發現 EHMA 採用了 two-tier 和 cluster-wise 結合的演算法，這顯著降低了外部
記憶體的使用以及記憶體的使用容量。使用 skippable 的掃描策略，加快了 EHMA 掃描的
速度。此外，平行和特殊功能的獨立，EHMA 僅需要簡單的軟體及硬體架構即可實現。而
在模擬的結果顯示使用 EHMA 明顯的提高了匹配的性能及效率，EHMA 匹配的速度約是目
前其他演算法的 0.89-1.161 倍如下圖 14，即使是在現實環境中，EHMA 依然展現出他良好
的 Performance。 
 
16 
 
提出將 P2P 流量快取本地化的方法。  
提出機器學習即時分類演算法。  
於重要國際會議 IEEE GLOBECOM 2008 發表一篇論文。  
於重要國際會議 IEEE ICC 2009 發表二篇論文 。 
於重要國際會議 IEEE ICC 2010 發表一篇論文。 
於重要國際會議 IEEE ICC 2011 發表兩篇論文。 
於國際重要期刊 IEEE Transactions on Dependable and Secure Computing  發表一篇論文 
於國際重要期刊 IEEE Transactions on Parallel and Distributed Systems 發表一篇論文 
 
五、 計畫自評 
本計劃於執行的三年期間提出了一個創新且有效的基於 P2P 應用程式的天性所開發
出來的機制，該機制用以辨認一個主機是否正在使用 P2P 應用程式。而重要的是，該機制
是非常節運算成本的，因為其只用到簡單的運算，而沒有做字串的比對，也沒有複雜的操
作。因此，我們將可以預期很容易的使用該機制開發出高速的 P2P 辨識引擎。除此之外，
我們提出對於 P2P 流量進行暫存(cache)的方法。暫存的觀念來至於 web 應用中的相同觀
念： 80%的使用者要求取用 20%的資料。所以當我們透過 HiSASS 將流經之 P2P 流量，
經過「資源重置」於暫存器 (cache)中，供其他 P2P 使用者在後續存取相同資料時，得以
透過「資源重置」將原本要消耗骨幹頻寬的資料，重新由暫存器中取得，減少骨幹頻寬的
浪費。另一方面我們所提出之機器學習處理流程。透過了過往的封包收集歷史資料，加以
分類、取樣後，透過了訓練學習的方式，讓機器得出屬於某種應用之行為特性：例如總共
傳遞之封包量、平均的封包長度、總資料量、封包標頭的總長度、建立連線的數目，封包
平均到達時間等等數據加以彙整。並透過高速的封包分析之技術，使得可以在眾多的封包
流之中快速地分析出封包流所吻合之應用服務特徵規格，並將這些資訊傳送至智慧型策略
引擎之中。本計劃所研發的成果已達到預期的規劃，本研究部分成果發表也在國際重要會
議發表，可見此研究的方向，受到學術界的肯定與認同。 
 
第一年計畫預計完成項目 自評 
 收集與觀察應用服務封包流所呈現之流量特性，尤其是以 Bittorrent 為代表
性的 P2P 應用。 
進度合乎預期  
分析與研究各種應用服務封包流之數學模型，尤其是以 P2P 為基礎之應用服
務。並於其中尋找出加速處理與流量最佳化之可能性。 
進度合乎預期  
研究與設計應用服務檢測引擎所需使用之相關演算法與技術。 進度合乎預期  
字串比對演算法之研究與設計。 進度合乎預期 
應用服務行為分析技術。 進度合乎預期 
研究與設計智慧型策略引擎所需之相關。 進度合乎預期  
 研究與設計封包流交換引擎之演算法與技術。 進度合乎預期 
研究與設計網路流量模擬器及測詴平台之架構設計。 進度合乎預期  
 
第二年計畫預計完成項目 自評 
18 
 
[5] Nen-Fu Huang, Gin-Yuan Jai, and Han-Chieh Chao, “A High Accurate Machine-Learning 
Algorithm for Identifying Application Traffic in Early Stage,” IEEE ICC2008, Beijing, China, 
May 2008.  
[6] [Online]. Available: http://www.defcon.org  
[7] [Online]. Available: http://www.packetfactory.net/projects/ISIC/  
[8] T Kojm, ClamAV, http://www.clamav.net  
[9] A. V. Aho and M. J. Corasick, “Efficient string matching: An aid to bibliographic search.” 
Communications of the ACM, 18(6):333–340, 1975.  
[10] Tuck, N., Sherwood, T, Calder, B and Varghese, G., “Deterministic Memory-Efficient String 
Matching Algorithms for Intrusion Detection.” IEEE INFOCOM2004, March 2004.  
[11] Sun Wu and Udi Manber, “A fast algorithm for multi-pattern searching.” Tech. Rep. TR94-17, 
Department of Computer Science, University of Arizona, May 1994.  
[12] Sun Wu and Udi Manber, “AGREP - A Fast Approximate Pattern-matching Tool.” Proceedings 
of the Winter 1992 USENIX Conference, Jan. 1992.  
[13] U. Manber and S. Wu, “GLIMPSE: A tool to search through entire file systems.” Winter 1994 
USENIX Conference, 1994.  
[14] R. S. Boyer and J. S. Moore, “A fast string searching algorithm.” Communications of the ACM, 
20(10): 762-772 , 1977.  
[15] Erdogan, O.and Pei Cao, “Hash-AV: fast virus signature scanning by cache-resident filters.” 
IEEE GLOBECOM 2005, 2005.  
[16] Burton H. Bloom, “Space time tradeoffs in hash coding with allowable errors.” 
Communications of the ACM, 13(7): 422-426, 1970.  
[17] Y. Miretskiy, A. Das, C. P. Wright, and E. Zadok, "AVFS: An on-access anti-virus file 
system." Proceedings of the 13th USENIX Security Symposium, Aug 2004.  
[18] Xin Zhou, Bo Xi, Yaxuan Qi and Jun Li, “MRSI: A Fast Pattern Matching Algorithm for 
Anti-virus Applications.” Seventh International Conference on Networking, 2008 (ICN 2008), 
Apr. 2008.  
[19] M. Roesch, “Snort - Lightweight Intrusion Detection for Networks.” Proceedings of the 
USENIX LISA '99 Conference, November 1999.  
 
 active/active (AA) mode for load balancing. A cluster in AB 
mode is composed of one primary (firewall) node (PN) and at 
least one idle backup (firewall) node (BN). Through the 
dedicated link (the replication link) between the cluster nodes, 
the replicated states are used to reflect the active flows on the 
pass-through link. 
A firewall cluster in AA mode is composed of two or more 
active nodes (ANs) sharing the pass-through loads, both 
playing as the message sender and receiver in replication 
management. The configuration in AA mode removes a 
possible throughput bottleneck and is superior in the areas of 
load balancing capability, better resource utilization, and a 
lower throughput penalty in the case of failure. In general, an 
AN/PN can be backed up by a number of cluster nodes. 
However, in this paper, we consider only the single backup 
configuration which is often used in practice. 
In Fig. 1, on the boundary of clustering pair, there are two 
load-balancing switches (LBSes) which distribute traffic loads 
evenly across two pass-through strings as well as ensure that a 
connection passes through the same string in both bi-direction 
while in AA mode. On the other hand, in AB mode, LBSes 
direct the traffic to a dedicated string. If a string failure is 
detected, LBSes redirect the traffic to the other string. 
III. STATE REPLICATION METHODS 
A.  Eager and selective replication 
In a firewall device, a state entry is used to store the 
information derived by TCP stateful tracking from the 
bi-directional traffic of a TCP flow. As a flow is initialized and 
terminated, the corresponding entry is inserted to and removed 
from the flow table. Each entry contains two types of flow 
sub-states: immutable and mutable. An immutable sub-state 
flowID, i.e., four-tuple <DstIP, SrcIP, DstPort, SrcPort>, 
remains constant and is used to identify a connection. Mutable 
sub-states may be changed very frequently, such as the latest 
packet arrival time, sequence and acknowledgement numbers, 
window advertisement and total flow bytes. 
Two replication methods are first considered: eager 
replication and selective replication. Besides the immutable 
information, eager replication synchronizes every change on 
the mutable information of a flow from SYN to its completion. 
For example, many advanced firewalls keep track of the 
sequence and acknowledgement numbers and TCP flags 
continuously to ensure the active flows are compliant with the 
TCP specification in all aspects. To meet these criteria after a 
failover, eager replication must be adopted by a stateful firewall 
cluster for synchronizing the mutable information. 
Though the message sizes of the pfsync and ct_sync 
both exceed 100 bytes, an explicit 32-byte-long representation 
is used to update the following data for investigating the costs 
of eager replication. 
y flowID 
y Sequence and acknowledgement numbers 
y Segment size, window scale, and TCP flag 
y Timestamp 
y Operation and direction flag 
Another method, selective replication, synchronizes three 
state changes (i.e., SYN_SENT, ESTABLISHED, and flow 
completion) only. Actually, both the pfsync and ct_sync 
use a strategy similar to selective replication to optimize state 
copying operations. In [6], a selective mechanism is used to 
save the processing time of the backup server. Furthermore, 
note that in our evaluation, a 16-byte-long message (only 
flowID and operation flags) is used by selective replication to 
evaluate the overheads. 
B. Flow Digest 
The scheme Flow Digest (FD) [16] improves the procedures 
of state replication through two factors: 1) an architectural 
improvement prevents the flow table from the access by 
replication traffic before a failover, and 2) a compact data 
structure employing randomization (i.e., Bloom filters) is 
designed to reflect the active flows. 
For PN and AN, all established flows are collected into a 
terse set representation and synchronized to the backup node by 
sending a Bloom filter or incremental messages. The scheme 
shifts to the recovery phase after a failover and the flow table is 
reconstructed in a packet-driven fashion by querying the 
backup Bloom filter. 
The memory requirement of FD can be kept small, while 
still achieving high accuracy. For example, for supporting 
1,000,000 connections in maximum, using 10,000,000 
elements, four bits per element, and four hash functions yields 
a false positive rate of 1.2% and requires a 7,500-KB memory 
space; not a concern in today’s equipments. Though false 
positives are possible, FD never rejects active flows after a 
failover under accurate state replication. 
We use the incremental updates to evaluate the overheads of 
the FD scheme. By large-size Bloom filters, FD can be viewed 
as a 2-state replication method to synchronize established 
flows and their terminations, where the message size is 32-bit. 
Table I. Traces used in experiments 
 
 
 
 
 
Fig. 1. Two types of dual-string, stateful HA clusters: (a) active/backup
mode and (b) active/active mode 
 t es of dual-string, stateful HA clusters for firewalls: (a)
active/backup mod  nd (b) active/active mode 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE "GLOBECOM" 2008 proceedings.
978-1-4244-2324-8/08/$25.00 © 2008 IEEE. 2
 files and then sent to the kernel space sequentially as the input 
packets. At the end of reading trace data, all active flows are 
forced to complete and then passed to flow analysis. 
To enable a fair comparison, we ignore purposely the 
replication for the flows whose SYN packets were not captured 
in the trace files, though this leads to an underestimation of the 
pass-through traffic (especially long flows) and replication 
costs. Furthermore, due to the fact that routes may be 
asymmetric at the backbone, there is a minor tuning in stateful 
tracking. 
For setting FD parameters, the maximum number of the 
allocated state entries in three traces is 159,394. Therefore, we 
set the maximum concurrent connections supported by a cluster 
node as 200K and set the Bloom filter size as 2,000K elements. 
The MD- is used as the hashing functions and the hash number 
is 4. 
B. Trace-based evaluation results 
To understand the effects of the imprecise replication, the 
reductions of the protected pass-through and replication traffic 
are studied by tuning the parameter tthreshold from 0 to 20,000 ms. 
Note that the active flows whose states are already replicated 
are referred to as the protected flows. On the other hand, the 
overhead of a replication method is measured by its bandwidth 
costs and the protected traffic bytes (TCP payloads). Let 
Nreplication and Nprotection be the total bytes of transmitted 
replication messages and pass-through packets whose states 
already have been replicated, respectively. Then, the overhead 
is defined as (Nreplication)/(Nprotection). Figs. 3–5 illustrate the 
evaluation results in AB mode. 
First, it is observed that eager and selective methods are 
vulnerable to one-way flows and malicious SYN packets. For 
example, in IPLS-1 and IPLS-3, 9.9% and 39.2% of the 
recycled state entries get stuck in SYN_SENT state. In the case 
of IPLS-3, the flow analysis shows that 87.1% of the recycled 
SYN_SENT entries are allocated by one-way flows (almost 
sending only 1 to 3 packets), and the remaining 11% are the 
two-packet flows (SYN and RST). In a cluster using 
eager/selective replication, a short one-way flow allocates two 
state entries (in PN/AN and its backup node), which are 
recycled and deleted immediately after an inactivity timeout 
(20 sec in our simulations). Thus, these one-way flows 
significantly aggravate the contention on the system resources 
of two cluster nodes and bandwidth consumptions on the 
replication link. By contrast, because FD and host-level 
aggregation only replicate the established flows (namely, from 
ESTABLISHED state), the number of the deletion events 
activated by the SYN_SENT timeouts are much less than those 
of eager and selective methods. The measurement of one-way 
and two-way flows has been the subject of research in [25]. 
In Figs. 3(a)–5(a), when tthreshold =50 ms, due to the savings of 
replicating one-way flows and malicious SYN packets, the 
reduction ratios of selective replication on the bandwidth costs 
are as high as 24.8%, 27.8%, and 16.7% in IPLS-1, IPLS-3, and 
AUCK-4. By contrast, the cost reduction ratios of FD and 
host-level aggregation are only 0.05%–6.3% by the same 
tthreshold. On the other hand, because most TCP flows of the 
Internet traffic are short-lasting, they dominate the state 
replication costs. Except for eager replication, a very clear rise 
on the replication traffic reductions is observed when tthreshold < 
1 sec in three packet traces. 
About the reductions on the protected pass-through bytes, 
Figs. 3(a)–5(a) show that only 0.09–1.6% of total pass-through 
bytes are not protected by tthreshold =50 ms, but up to 27.8% of 
the replication traffic are saved for selective replication. For FD 
in IPLS-3, reducing 50% (tthreshold=320 ms), 74.4% 
(tthreshold=500 ms), and 88.9% (tthreshold=2,000 ms) of the 
replication traffic excludes only 1.9%, 3.4%, and 11.8% of the 
pass-through bytes. The host-level aggregation has a very 
 
 
Fig. 4. (a) The traffic reduction and (b) the replication overheads (with the
IPLS-3 trace) 
 
 
 
Fig. 3. (a) The traffic reduction and (b) the replication overheads (with the
IPLS-1 trace) 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE "GLOBECOM" 2008 proceedings.
978-1-4244-2324-8/08/$25.00 © 2008 IEEE. 4
 replicate a TCP flow from its ESTABLISHED state. This 
strategy avoids the high costs, such as a high entry-recycling 
rate and unnecessary bandwidth consumptions, from very short 
one-way flows and malicious SYN packets. 
REFERENCES 
[1] R. Hinden, “Virtual Router Redundancy Protocol (VRRP),” RFC 3768, 
April 2004. 
[2] Ryan McBride, “Firewall Failover with pfsync and CARP,” available at: 
http://www.countersiege.com/doc/pfsync-carp/. 
[3] H. Welte, “ct_sync: state replication of ip_conntrack,” Linux Symposium, 
2004. 
[4] L. Alvisi, T. C. Bressoud, A. El-Khashab, K. Marzullo, and D. 
Zagorodnov, “Wrapping Server-Side TCP to Mask Connection Failures,” 
Proc. IEEE INFOCOM, pp. 329-337, 2001. 
[5] N. Aghdaie and Y. Tamir, “Client-transparent fault-tolerant web service,” 
20th IEEE International Performance, Computing, and Communications 
Conference, pp. 209–216, 2001. 
[6] R. Zhang, T. F. Abdelzaher, and J. A. Stankovic, “Efficient TCP 
Connection Failover in Web Server Clusters,” Proc. IEEE INFOCOM, 
2004. 
[7] A. C. Snoeren, D. G. Andersen, and H. Balakrishnan, “Fine-grained 
failover using connection migration,” Proc. 3rd USENIX Symposium on 
Internet Technologies and Systems, March 2001. 
[8] R. R. Koch, S. Hortikar, L. E. Moser, and P. M. Melliar-Smith, 
“Transparent TCP connection failover,” Proc. the IEEE Int. Conf. on 
Dependable Systems and Networks (DSN’03), June 2003. 
[9] M. Marwah, S. Mishra, and C. Fetzer, “TCP server fault tolerance using 
connection migration to a backup server,” Proc. IEEE Int. Conf. on 
Dependable Systems and Networks (DSN’03), June 2003. 
[10] F. Sultan, K. Srinivasan, D. Iyer, and L. Iftode, “Migratory TCP: Highly 
available Internet services using connection migration,” Proc. the 
International Conference on Distributed Computing Systems 
(ICDCS’02), pp. 469-470, 2002. 
[11] F. Sultan, A. Bohra, and L. Iftode, “Service Continuations: An Operating 
System Mechanism for Dynamic Migration of Internet Service Sessions,” 
Proc. the Symposium on Reliable Distributed Systems, Oct. 2003. 
[12] A. Shieh, A. Myers, and E. G. Sirer, “Trickles: A Stateless Network Stack 
for Improved Scalability, Resilience and Flexibility,” Proc. the 2nd 
USENIX Symposium on Networked Systems Design and Implementation 
(NSDI’05), pp. 175–188, May 2005. 
[13] R. Stewart and C. Mets, “SCTP: New transport protocol for TCP/IP,” 
IEEE Internet Comput., 2001. 
[14] M. Dahlin, B. Chandra, L. Gao, and A. Nayate, “End-to-end WAN 
Service Availability,” IEEE/ACM Transactions on Networking, 2003. 
[15] C. Boutremans, G. Iannaccone, and C. Diot, “Impact of link failures on 
VoIP performance,” in Proc. NOSSDAV, pp. 63–71, 2002. 
[16] Yi-Hsuan Feng, Nen-Fu Huang, Rong-Tie Liu, and Meng-Huan Wu, 
“Flow Digest: A State Replication Scheme for Stateful High Availability 
Cluster,” IEEE ICC2007, June 2007. 
[17] M. Allman, “A Web Server's View of the Transport Layer,” ACM 
Computer Communication Review, October 2000. 
[18] F. D. Smith, F. H. Campos, K. Jeffay and D. Ott, “What TCP/IP Protocol 
Header Can Tell Us About the Web,” Proc. ACM SIGMETRICS, June 
2001. 
[19] F. Hernandez-Campos, K. Jeffay, and F. Donelson-Smith, ‘‘Tracking the 
Evolution of Web Traffic: 1995-2003,’’ Proceedings of the 11th 
IEEE/ACM MASCOTS Conference, pp. 16-25, October 2003. 
[20] N. Brownlee and K.C. Claffy, “Understanding Internet Traffic Stream: 
Dragonflies and Tortoises,” IEEE Communications, Vol. 40, No. 10, pp. 
110-117, 2002. 
[21] A. Shaikh, J. Rexford, and K.G. Shin, “Load-Sensitive Routing of 
Long-Lived IP Flows,” Proc. ACM SIGCOMM, September 1999. 
[22] P. Bhoj, S. Ramanathan, and S. Singhal, “Web2K: Bringing QoS to Web 
Servers,” HPLabs Report No. HPL-2000-61, May 2000. 
[23] D. Katz and D. Ward, “Bidirectional forwarding detection,” Internet 
Draft – RFC, March 2005. 
[24] NLANR PMA Trace, available: http://pma.nlanr.net/. 
[25] DongJin Lee and Nevil Brownlee, “Passive Measurement of One-way 
and Two-way Flow Lifetimes,” Proc. ACM SIGCOMM, 2007. 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE "GLOBECOM" 2008 proceedings.
978-1-4244-2324-8/08/$25.00 © 2008 IEEE. 6
protocol analyzer uses a set of detection heuristics for the data it 
receives and plays as a dissector to extract the information 
when necessary. For example, the HTTP analyzer helps to 
ensure the traffic through port 80 adhere to the HTTP 
specification for detecting the tunneling of instant messaging 
and P2P software. 
By using the statistical analysis of traffic, the previous works 
[9, 10, 11] gave a different point of view on the traffic 
classification. These techniques use different parameters 
including the packet size distribution and the interactive 
relationship to classify the traffic into board categories. 
For identifying the application behaviors, the existing 
methods have the some limitations. First, each application has 
two phases that need to be tracked: the connections associated 
with the application and the packets with important information. 
However, previous works focused on the detection 
performance (evaluated by the false positives and false 
negatives) of connections and did not pay attentions on 
behavior detection. For instance, a network administrator might 
constitute a policy to block the file exchange by IM software for 
security and bandwidth management reason but allow the 
message exchange among the company's branches to save 
phone call cost. Therefore, to apply the different enforcement 
policies to different behaviors, a deeper visibility into the 
specific application is necessary. Second, the application 
detection by port-based, signature-based, or statistical-based 
approach only provides a coarse-grained information of 
whether a given protocol is in use or not. Nevertheless, this 
black-or-white result is not enough. It is essential for the 
behavior detection to have the capability of analyzing every 
application instance continuously and analyzing the parent 
connections to identify their children connections.  
III. METHODOLOGY 
From the observation of some famous P2P traffic 
(BT/eDonkey/Gnutella, etc), it is interesting to see that they 
have multiple long-term connections with different IP 
addresses simultaneously. This feature is quite different from 
normal applications which usually have few concurrent 
connections or many but short-term connections with the same 
host. Another more key observation is that normal applications 
are usually based on client-server architecture and the servers 
have domain names, but most of peers in P2P architecture do 
not have domain name. That is, the normal applications (clients) 
will first send DNS query and get DNS reply before the 
following communications while P2P applications send no or 
fewer DNS queries. Based on these two essential characteristics, 
criteria are developed to distinguish the P2P applications from 
general applications. 
To observe the mentioned behaviors, a tool is developed in 
Linux to record and parse the network traffic as well as 
behavior. In normal applications, a host can get the IP address 
of a domain name only when the DNS server returns a DNS 
reply. Here we define three metrics to measure: 
DC(DNS Count): Number of DNS replies received by a host. 
PC(Peer Count): Number of distinct remote connected hosts. 
CC(Connection Count): Number of active TCP connections. 
 
 Figure 1 demonstrates the experiment environment where a 
device with the developed tool is installed between a host and 
the Internet. All the traffic between the host to observe and the 
Internet are recorded and analyzed. Note that based on the 
defined metrics, a server in traditional client/server architecture 
will be treated as a peer. Since TCP is the major protocol used 
by most network applications, we first observe TCP behavior 
only.  
  
Figure 1: Experiment environment. 
Figures 2 to 4 illustrate the observed three metrics for famous 
eMule (P2P), BT (P2P), and HTTP (normal application), 
respectively. We can see that the eMule provides high PC and 
CC values with very low DC value. Nevertheless, the HTTP 
offers high DC value with lower PC and CC values. They can 
be obviously distinguished. But this is not clear for the BT, the 
introduced DC, PC and CC values are close. This is due to the 
BT peer initially communicates with tracker by HTTP and 
therefore its beginning behavior is just HTTP rather than P2P. It 
will perform P2P behavior later when the IP addresses of the 
peers are obtained from the tracker.  
 
Figure 2: Observed behavior of eMule. 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2009 proceedings
 
Figure 6: Observedβ value (UDP traffic) of voice-based and 
video-based P2P applications. 
 
IV. SYSTEM IMPLEMENTATION AND EXPERIMENTAL 
RESULTS 
A. Implementation 
We have implemented the proposed mechanism on the Linux 
with kernel 2.6.25.8 as a transparent device named P2P 
Detector. Figure 7 illustrates how the developed transparent 
device is working with gateway to identify the P2P applications. 
From the above observations we define a threshold of β to 
distinguish P2P applications. Packets are forwarded or blocked 
by ip_queue module in Linux. When a host triggers the 
threshold, it is identified as using the P2P applications, and 
therefore its packets are then blocked for evaluation purpose. 
  
 
 Figure 7: The experimental environment. 
 
B. Experimental Results 
To gather statistics, we first run 10 trials for each application 
and each trial lasts for 5 minutes. The average β of each trial is 
then calculated and listed in Table I. The average β can be 
referred as an index to distinguish the P2P applications later. 
We then verify our proposed mechanism by running P2P 
and HTTP applications with the P2P Detector. Two hosts with 
Windows XP sp2 behind the P2P Detector are used to run the 
applications. Apparently  other normal applications except 
HTTP will never be detected as P2P applications. It is expected 
that the HTTP applications should pass the P2P Detector and all 
the P2P applications should be correctly identified and blocked. 
It is easy to check if a P2P application is blocked by the client 
side as all its traffic will be dropped. 
Table 1. Average β of each application. 
Protocol
Trial 
BT eMule Gnutella Skype PPStream PPLive HTTP
TCP UDP TCP UDP TCP UDP UDP UDP UDP UDP 
1 79.98 80.33 681.23 502.40 241.50 1255.33 56.05 210.51 405.45 9.23 
2 54.58 84.59 612.60 449.03 195.06 427.61 67.32 476.78 433.21 11.30 
3 34.70 72.14 562.67 465.60 177.29 271.23 37.90 345.19 244.71 2.10 
4 61.49 52.78 547.83 458.20 194.32 1396.81 36.77 407.04 293.29 22.04 
5 59.20 63.86 594.17 474.77 182.51 296.29 37.42 348.14 911.81 4.81 
6 56.12 63.47 557.43 457.77 199.19 1364.94 37.38 307.24 599.75 3.19 
7 82.88 127.22 639.33 425.80 189.64 225.46 37.58 454.14 947.37 0.75 
8 21.00 74.79 490.07 442.83 190.23 1279.75 38.03 399.59 421.29 0.50 
9 31.29 193.78 495.73 449.46 193.68 83.78 35.85 435.62 637.24 9.73 
10 91.50 94.59 330.47 348.90 199.43 877.29 26.68 373.75 380.54 1.62 
Average 57.27 90.76 551.15 447.48 196.29 747.85 41.10 375.80 527.47 6.53 
 
We have 10 trials for each application and again, each trial 
lasts for 5 minutes. If a P2P application can’t be detected within 
5 minutes, it means that we fail at this trial and then have a false 
negative. The threshold of β is set as 80. If the β of a host is 
larger than 80, it is treated as using P2P. An HTTP application 
triggers the threshold within 5 minutes means a false positive as 
HTTP should not trigger the threshold. There are many 
programs developed for the same protocol, e.g. BT protocol has 
BitTorrent, BitComet, Azures, µTorrent, etc. We use programs 
from official website, i.e. BitTorrent for BT protocol and eMule 
for eMule protocol.  Each program uses default settings for test.  
The decision of threshold of β  is a tradeoff. A higher 
threshold produces lower false positive rate and longer 
detection time. On the contrary, a lower threshold obtains a 
shorter identification time but higher false positive rate. 
Apparently P2P applications with peers of public IP 
addresses perform better than that of private IP addresses 
(behind NAT). Thus is because initially the peers behind NAT 
are unable to communicate directly and need to be bridged via a 
super-node with public IP address. We evaluate the 
performance for both environments. Table2 demonstrates the 
time experienced to identify the use of P2P applications when 
hosts are with public IP addresses. BT and eMule can be 
detected by UDP because they implement DHT by UDP. 
Apparently, eMule soon can be detected while BT needs near 3 
minutes to be detected. The HTTP application didn’t trigger the 
threshold in each trial as we expected during experiment and 
then we don’t list in tables. Table 3 lists the same result as 
Table2 except that hosts are with private IP (behind NAT). It 
shows that the proposed mechanism still works well even with 
private IP. 
Table 4 lists the hit rate, false positive rate and false negative 
rate of each application when hosts are with public IP. The hit 
rates for all P2P applications are 100%. Table 5 is the same as 
Table 4 except that hosts are with private IP (behind NAT). BT 
has a false negative when threshold is 80. The hit rates for all 
other P2P applications are 100%, this indicates the proposed 
mechanism can accurately identify the P2P applications. Again, 
as we expected, the HTTP has no false positive under this 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2009 proceedings
A Resource-Efficient Traffic Localization Scheme for 
Multiple BitTorrents 
Nen-Fu Huang 
Department of Computer Science and Inst. 
of Communication Engineering 
National Tsing-Hua University 
Hsin-Chu, Taiwan 
Yen-Ming Chu 
Institute of Communications Engineering 
National Tsing-Hua University 
Hsin-Chu, Taiwan 
Chi-Hung Tsai, Wei-Zen Huang and 
Wei-Jin Tzeng  
Dep. of Computer Science 
National Tsing-Hua University 
Hsin-Chu, Taiwan 
 
Abstract — The emergence of peer-to-peer (P2P) applications has 
posed a threat to the operating cost of Internet Service Providers 
(ISPs) due to the large amount of inter-ISP traffic generated. The 
problem stems from the mismatch between the P2P overlay 
network formed randomly and the underlying physical network. 
Recently, BitTorrent  has attracted enormous users by its 
convenience of large-scale content distribution and has also 
become a major challenge for ISPs. Therefore, in this paper we 
proposed an effective B-Proxy scheme to evaluate through 
realistic simulation on PlanetLab, where hundreds of BitTorrent 
clients were executed during the experiment. Simulation results 
show that more than thirty percent of inter-ISP traffic could be 
saved in a torrent with a relatively small cache size consumed 
which is only eighth times that of the original file. 
Index Terms — P2P Acceleration, P2P Localization, Bit-Torrent 
Protocol Analysis. 
I.  INTRODUCTION  
With the increasing computation power of personal 
computers, peer-to-peer (P2P) technology becomes a 
promising approach for various applications, such as live 
video streaming, file sharing and voice delivery. Unlike 
traditional client-server model or content distribution network 
(e.g. Akamai [1]), P2P is self scalable with relatively small 
investments in infrastructure deployment. Peers connect to 
some active peers to join the overlay network formed by the 
logical links between them. P2P network is more resilient to 
“flash crowd” of new peers but also increases the traffic 
generated from peers at the edge of physical network. 
Many solutions have been proposed to address the location-
unaware problem in BitTorrent [2]. Traffic shaping devices 
are commonly deployed by ISPs to limit the bandwidth 
consumed by BitTorrent [3],[4]. However, this may 
substantially increase the download time and degrade the 
quality of experience on end-users. Another approach is to 
modify the mechanism of BitTorrent, either through the 
modification of BitTorrent tracker or through the extension of 
locality-aware protocols [5],[6],[7]. Nevertheless, no tracker 
and almost no BitTorrent clients [8] have integrated such a 
feature after several years from the proposition of these 
solutions. Cache is also a common solution adopted by ISPs. It 
will reduce the data transmitted to external peers by caching 
the previous requested content. BitTorrent and Cachelogic [9] 
introduced Cache Discovery Protocol (CDP) to allow ISPs 
detect the most popular files shared through BitTorrent. 
However, the characteristic of P2P traffic differs from that of 
web traffic and the direct mapping may be inadequate. More 
importantly, the caches size used would be very large when 
multiple files need to be cached concurrently.  
II. RELATED WORK 
The location-unaware problem in BitTorrent has been 
widely addressed in many studies [5],[10]. The main reason of 
the problem is that the neighbors of a peer are consisted of 
randomly selected peers without considering any locality 
information. Suppose there are N peers downloading the some 
content via BitTorrent and C peers of them are within a certain 
Autonomous system (AS). For a peer in the AS, the probability 
of choosing a peer in the same AS is C/N. In most cases, the 
probability is very small since C is generally far less than N. 
This may cause large unnecessary inter-ISP traffic and waste a 
lot of network resources. Many solutions have been proposed 
to reduce unnecessary inter-ISP traffic of P2P applications. 
We further summarize them as follows: 
A.  Improving Peer Selection 
To increase the connectivity between peers within the same 
ISP, the modification of BitTorrent tracker to consider the 
locality when return peer list to new participants is proposed. 
Simple IP prefix rules, domain names, BGP tables can be used 
for the tracker to distinguish whether two peers are in the near 
location [11]. Since the communication between tracker and 
peer is over HTTP, the ISP’s HTTP proxy can also append a 
location tag at the header. Therefore, tracker can easily know 
that peers with the same location tag are from the same ISP 
[6]. However, the modification would increase the workload 
of the tracker and the requirement of the cooperation between 
ISPs and tracker provider may be difficult.  
B. Cache 
The most natural way to reduce inter-ISP traffic may be 
deploying a cache device to lessen the bandwidth consumed 
by P2P applications [9]. Whether P2P traffic is appropriate to 
be cached is discussed in [12],[13] while some cache 
algorithms for P2P traffic are proposed in [14],[15]. Many 
cache replacement methods used for HTTP traffic are also 
This work was supported by National Science Council, Taiwan, under the
grant number NSC-95-2221-E-007-054-MY3 and Chung-Hwa 
Telecommunication Research Lab. Taiwan, under the grant number
TLR977046. 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2009 proceedings
978-1-4244-3435-0/09/$25.00 ©2009 IEEE
For the internal peers, BSP does not build incentives 
mechanism [18]. Instead, BSP tries to upload pieces to internal 
peers as soon as possible to avoid them get pieces from 
external neighbors (for a peer, the majority of pieces are 
usually obtained externally). For external peers, BSP restricts 
the upload/download ratio to be less or equal than two that 
obtains the balance between getting pieces from them quickly 
and generating too much inter-ISP traffic. 
1) Partial Cache Strategy 
To increase the storage utilization, BSP only caches a 
portion of pieces for each single torrent. Therefore, critical 
pieces are prioritized to be put into the disk or memory of BSP. 
In addition, since BSP would actively initiate connections to 
internal peers, it could obtain some less vital pieces from these 
neighbors internally. Unlike traditional cache-based inline 
mode methods which usually passively wait for pieces to be 
transmitted, BSP could decides which pieces to obtain by 
sending the request to some peers. The main concept of this 
strategy is to use the disk (memory) of BSP itself as primary 
cache and that of all internal peers as secondary cache. Figure 
2.  illustrates this idea. Therefore, two important problems 
need to be concerned: which pieces should be firstly put into 
the primary cache and how to quickly obtain pieces from the 
secondary cache which consists of several internal peers. 
 
Fig. 2  Partial Cache Strategy. 
2) Rarest Pieces First 
Let piece_weighti be the number of internal peers which has 
piecei, then we have piecei is more rare than piecej if 
piece_wieghti is less than piece_weightj. Note that 
piece_wieghti would change over time since more peers would 
have piecei or some peers may depart or join the system. 
Suppose that the allowed cache size for a certain torrent is 
one-fourth times the total file size of the torrent, BSP would 
start to request one-fourth pieces of the torrent with the 
smallest piece_weight. Once obtaining a piece, BSP would 
broadcast the HAVE message to all neighbors and starts to 
upload the piece to internal peers if they send request. BSP 
should not deliver pieces to a peer who does not send piece 
requests even if it lacks these pieces. Otherwise, the peer who 
obtains un-requested pieces would increase the error count of 
the communication to BSP and may drop the connection later.  
After the average upload rate to an internal peer is below 
some threshold (ex: 100 kbps), BSP would recalculate 
piece_weight and decide which pieces should be cached in the 
next round. For those previously cached pieces which are not 
chosen to be cached next, BSP would delete them because of 
the restriction of the cache size for one torrent. Before starting 
the next service round, BSP would close all the connections 
and reconnect to all neighbors to send the BitField message 
and HAVE message later since some pieces which BSP 
previously claims it has may be deleted now. The rarest first 
cache algorithm is shown below. 
 
1. Procedure Rarest_piece_first 
2. Input: The selected torrent for localization: torrent
The allowed cache size in pieces: cache_size
The minimum average upload rate to internal peers: min_upload 
3. Rarest_first_caching (torrent, cache_size, min_upload) 
4. begin 
5.   piece_weight [] Å NIL; 
6.   have_piece [] Å NIL; 
7.   foreach internal peer p do 
8.      if p has piecei then  
9.         piece_weight[ i ] Å piece_weight[ i ]+1; 
10.      endif 
11.   sorted_piece[ ] Å sort pieces by piece_weight[ ] in descending order; 
12.   for j Å cache_size to torrent.total_pieces – 1 do 
13.     begin 
14.       delete the sorted_piece[ j ]-th piece; 
15.       have_piece[ sorted_piece[ j ] ] Å 0; 
16.     end 
17.   Close all the connection and reconnects to all the neighbors. 
18.   for i Å 0 to cache_size – 1 do 
19.     begin 
20.       if have_piece[ sorted_piece[ i ] ] = 0 and sorted_piece[ i ] >= 1 
then 
21.         request the sorted_piece[ i ]-th piece; 
22.       endif 
23.       if receives sorted_piece[ i ]-th piece completely then 
24.         send HAVE sorted_piece[ i ] to each neighbor; 
25.         have_piece[ sorted_piece[ i ] ] Å 1; 
26.       endif; 
27.     end 
28.   while download_rate > min_upload do 
29.        service internal peers; 
30. end 
3) Hungry Peer First 
The number of concurrent TCP connections allowed to 
serve a particular torrent should be restricted since there may 
be thousands of peers in the target AS. Therefore, which peers 
are prioritized to be chosen in a torrent is necessary to be 
considered. Suppose that the number of pieces could be 
cached at the same time for a certain torrent is s. For a peer in 
the torrent, let b be the number of pieces which the peer 
already has and B is the number of all pieces of the file. The 
probability that BSP could provide at least n pieces to the peer 
is:  
⎪⎪
⎪
⎩
⎪⎪
⎪
⎨
⎧
≥−≤
≥−≤<−⎟⎟⎠
⎞
⎜⎜⎝
⎛
⎟⎟⎠
⎞
⎜⎜⎝
⎛ −
−>
<
=
nsnsb
nsnBbns
b
B
b
nB
nBb
ns
sbBnP
,,1
,/
,0
,0
),,|(
 
In addition, since the BSP’s contribution to a peer having 
more pieces would decrease gradually under partial cache 
strategy, the probability of BSP to be unchoked by such a peer 
also diminishes (the contribution is recalculated every 10 
seconds). This may substantially increase the access latency to 
secondary cache formed by internal peers. Therefore, hungry 
peer first also has the advantage of obtaining pieces from 
peers quickly because BSP’s contribution to them is generally 
considerable. 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2009 proceedings
To precisely observe the localization process, the 
corresponding ratios is categorized in Figure 5, respectively. 
Initially, most peers are hungry and the difference of BSP’s 
contributions between two piece selection algorithms is not 
significant since most pieces are rare internally. As peers 
acquire more pieces, the contribution of the BSP using the 
rarest piece first policy gradually surpasses that of the other 
BSP because random piece selection may cache several 
popular pieces and reduce its upload traffic. However, as most 
pieces are not rare and have been diversely distributed to 
internal peers, the upload rate of both BSPs decreases and 
finally makes both BSPs stop to operate. Therefore, the 
difference of the two algorithms is apparent only when the 
variance of the number of each piece copies is large enough. 
 
417.43
333.17288.56
330.61 339.82
314.45
0
100
200
300
400
500
1/8 1/1 1/3
Cache size (#concurrent cached pieces / #total pieces)A
ve
ra
ge
 d
ow
nl
oa
d 
tim
e 
(m
in
ut
es
)
Random piece selection Rares piece first
 
Fig. 4 Comparison of average download time inAS1(random) and 
AS2(rarest). 
 
R
at
io
 o
f c
um
ul
at
iv
e 
do
w
nl
oa
de
d 
tra
ff
ic
0
0.2
0.4
0.6
0.8
1
Timeline
USA inter-AS USA intra-AS USA BSP
EU inter-AS EU intra-AS EU BSP
 
Fig. 5 Different traffic in AS1(random) and AS2(rarest) with 1/16 cache size. 
V. CONCLUSION 
This paper explored a practical cache-based approach which 
is suitable for ISPs to localize inter-AS traffic generated by 
BitTorrent. The operation of the proposed B-Proxy which is 
designed to work in offline mode which is relatively safe 
compared with other inline mode solutions. Moreover, by 
actively connecting to all internal peers, the distribution of all 
internal pieces could be precisely probed by B-Proxy for 
further actions, such as obtain critical pieces by sending the 
request to internal peers. Unlike traditional cache methods 
which not only cache the whole pieces of the file but also 
evaluate merely for single torrent, some resource-efficient 
policies are also adopted in B-Proxy when localizing multiple 
torrents in real situations. One is rarest piece first that is likely 
to raise the contribution of B-Proxy and increase the piece 
diversity among internal peers for facilitating the interaction 
among them. Another is partial cache strategy which could 
localize more torrents with higher storage utilization. The 
other is the policy of hungry peer first that delivers pieces to 
peers having more missing pieces first under the limited 
number of concurrent TCP connections allowed for a single 
torrent. 
REFERENCES 
[1] Akamai. http://www.akamai.com  
[2] BitTorrent. Free, open source file-sharing application effective for 
distributing very large software and media 
files.http://www.bittorrent.com 
[3] P-Cube. http://www.p-cube.com 
[4] Sandvine. http://www.sandvine.com/ 
[5] Ruchir Bindal, Pei Cao, William Chan, Jan Medved, George Suwala, 
Tony Bates, and Amy Zhang, “Improving Traffic Locality in BitTorrent 
via Biased Neighbor Selection,” in Proceedings of IEEE ICDCS’06, 
Lisboa, Portugal, July 2006. 
[6] Yunhao Liu, Xiaomei Liu, Li Xiao, Lionel M. Ni, and Xiaodong Zhang, 
“Location-aware topology matching in P2P systems,” in Proceedings of 
IEEE INFOCOM’04, Hong Kong, March 2004, vol. 4. pp.2220-2230  
[7] Yunhao Liu, Li Xiao, Xiaomei Liu, Lionel M. Ni, and Xiaodong Zhang, 
“Location awareness in unstructured peer-to-peer systems,” IEEE 
Transactions on Parallel and Distributed Systems, Vol. 16, Issue 2, 2005, 
pp. 163-174. 
[8] BitTorrent clients. http://en.wikipedia.org/wiki/BitTorrent_client 
[9] CacheLogic networks.http://www.velocix.com/ 
[10] Lei, Zhang, “Study of the location awareness in bitTorrent-like 
networks,” in the 7th International Conference on Computer-Aided 
Industrial Design and Conceptual Design (CAIDCD '06), HangZhou, 
CHINA, November 2006, pp.1-7. 
[11] Thomas Karagiannis, Pablo Rodriguez, and Konstantina Papagiannaki, 
“Should internet service providers fear peer-assisted content 
distribution?” in Proceedings of the Internet Measurement Conference 
(IMC’05), Berkeley, CA, USA, October 2005, pp. 63-76. 
[12] Nathaniel Leibowitz, Aviv Bergman, Roy Ben-Shaul, and Aviv Shavit, 
“Are file swapping networks cacheable? Characterizing P2P traffic,” in 
Proceedings of the 7th International Workshop on Web Content Caching 
and Distribution(WCW’02), Boulder, Colorado, USA, 2002. 
[13] Mauro Andreolini, Riccardo Lancellotti, and Philip S. Yu, “Analysis of 
peer-to-peer systems: workload characterization and effects on traffic 
cacheability,” in Proceedings of the IEEE Computer Society's 12th 
Annual International Symposium on Modeling, Analysis, and 
Simulation of Computer and Telecommunications Systems 
(MASCOTS’04), Volendam, Netherlands, October 2004, pp.95-104. 
[14] Adam Wierzbicki, Nathaniel Leibowitz Matei Ripeanu, and Rafal 
Wozniak, “Cache replacement policies revisited: the case of P2P 
traffic,” in Proceedings of the 4th IEEE/ACM International Symposium 
on Cluster Computing and the Grid(CCGrid’04), Chicago, Illinois, USA, 
April 2004, pp. 182-189. 
[15] Osama Saleh and Mohamed Hefeeda, “Modeling and Caching of Peer-
to-Peer Traffic,” in Proceedings of the 14th IEEE International 
Conference on Network Protocols (ICNP '06), Santa Barbara, California, 
USA, November, 2006, pp. 249-258. 
[16] CDP. http://en.wikipedia.org/wiki/Cache_Discovery_Protocol  
[17] Ipoque. http://www.ipoque.com/ 
[18] Thanunchai Threepak, “Bittorrent Cache Using Virtual Tracker,” in 
Proceedings of the IEEE International Symposium on Communications 
and Information Technologies (ISCIT '06), Bangkok, Thailand, October 
2006, pp. 162-165. 
[19] Enhanced Ctorrent. http://www.rahul.net/dholmes/ctorrent/ 
[20] Libpcap. http://www.tcpdump.org/ 
[21] Bram Cohen, “Incentives Build Robustness in BitTorrent,” in the 1st 
Workshop on Economics of Peer-to-Peer Systems, Berkeley, CA, USA, 
June 2003. 
[22] The Pirate Bay, the world’s largest BitTorrent tracker. 
http://thepiratebay.org/  
[23] Ashwin R. Bharambe, Cormac Herley, and Venkata N. Padmanabhan, 
“Analyzing and Improving a BitTorrent Networks Performance 
Mechanisms,” in Proceedings of IEEE INFOCOM’06, Barcelona, 
Catalunya, Spain, April, 2006, pp. 1-12
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2009 proceedings
  
II. RELATED WORK 
The AC (Aho-Corasick) algorithm [2] was invented from 
the idea of finite automata. AC is well-known for its equivalent 
average and worst case matching performance; however, it 
exhibits the problem of consuming too much memory for a 
large database like that of ClamAV. Although the memory 
deficiency can be solved by compressing the constructed 
automaton [3], the side-effect is sacrificing the matching 
performance.  
Wu and Manber designed the WM algorithm [4] to use in the 
UNIX agrep and glimpse tools [5], [6]. WM extends the 
bad-character heuristic of the BM (Boyer-Moore) algorithm [7] 
to multi-pattern matching by building the shift table by a block 
of bytes rather than a single one. Hash technique for both 
pattern suffix and prefix is employed to reduce unnecessary 
exact comparisons when the shift value is zero. However, the 
WM algorithm doesn’t perform well than the naïve sequential 
matching approach in its worst case.  
ClamAV separates the virus patterns for multi-pattern 
matching into basic and several other types to match them 
using either AC-based or BM-based algorithm respectively. 
ClamAV modifies the original AC algorithm to convert the 
automaton into a trie structure to avoid excessive memory 
usage due to state expansion. In the matching phase, once the 
trie is traversed to a leaf node, a sequential matching is 
performed on each pattern of the linked list. Like WM, 
ClamAV extends the single-pattern BM algorithm by building 
the shift table with a block of three bytes. However, the prefix 
hash technique is not used for further filtering when the shift 
value is zero.  
Hash-AV [8] uses Bloom filter [9] to improve the ClamAV 
matching performance by weeding out clean data to avoid the 
cost of multi-pattern matching. However, when the front-end 
Bloom filter fails, it still relies on the ClamAV scanner to 
perform the exact matching. In [10], the ClamAV AC data 
structure is redesigned to support a trie with more tiers of states 
expanded while keeping memory consumption tolerable. This 
optimized AC structure is implemented in Linux for file system 
on–access virus scanning. The MRSI algorithm proposed in 
[11] performs virus scanning by shifting and matching text 
bytes using the BLT and PMT tables as a two-phase scan 
approach. It outperforms the ClamAV BM-based algorithm 
with 8 to 16Mbps for the real system file-scanning 
performance.  
III. THE ALGORITHM 
The proposed SHOCK (Shift/Hash with Overlap Check) 
algorithm consists of an offline preprocessing phase and an 
online pattern matching phase.  
A. The Off-line Preprocessing Phase  
Virus patterns in ClamAV are classified into four categories:  
y MD5 Patterns: MD5 checksums for executable virus files 
which may be encrypted or compressed to evade the 
anti-virus scanner.  
y Basic Patterns: All hexadecimal ASCII strings without 
regular expressions.  
y Regular Expression Patterns: Patterns include regular 
expression operators like *,?, and {min,max}.  
y Other Patterns: Except the above, there are archive metadata, 
whitelist and phishing patterns.  
Considering only the basic patterns, ClamAV further classifies 
them into nine types and Table II shows some important 
characteristics of them for the latest ClamAV (Jun. 2009) virus 
database. We take advantage of the long minimum (>= 10 
bytes) and average pattern length (>= 25 bytes) to construct the 
shift table because most of the shift values are nonzero and 
large by which we can fast shift unmatched text bytes to 
achieve sub-linear matching complexity. It is beneficial to 
separately match the 43 patterns of type 1(PE) and type 
3(HTML) with length shorter than 10 to avoid sacrificing the 
average performance.   
The shift table is constructed using the same approach as in 
the WM algorithm with block size two and we calculate the 
hash value of the 2-byte prefix of each pattern. To reduce 
computation complexity, the hash calculation is accomplished 
by shifting and accumulating the numeric value of the pattern 
prefix. Since size of the alphabet space of one single byte is 
256, there are at most 64K values (i.e. hash table size) by the 
simple hash calculation, h = (a << 8) + b, for each 2-byte prefix, 
“ab”.  On the other hand, when a matched pattern is found, 
there may be another consecutive pattern in the text with prefix 
overlapping suffix of the currently matched one. Therefore we 
can analyze the overlap relations between pairs of patterns in 
advance to facilitate later matching process. Let P = {P1, P2, P3 
…} be the set of patterns to match. Given two patterns, Pi = 
pi1pi2pi3…pim and Pj = pj1pj2pj3…pjn, Pi is said to be strictly 
suffix-prefix overlap Pj iff pik…pim = pj1…pjm-k+1 for m ≤ n and 1 
< k ≤ m; or n < m and m–n+1 < k ≤ m, where i and j are not 
necessarily different. If this condition holds for n < m and k = 
m-n+1, Pj must be a substring of Pi, and we merge Pj to the 
match list of Pi during the preprocessing phase. For example, 
given three patterns, “root”, “totoro” and “rose”, we note that 
the suffix of “totoro” overlaps the prefix of “rose” with the 
substring “ro”, and therefore we store the substring “se” and 
the pattern ID of “rose” in the nextPat list of “totoro” as shown 
in Fig.1. For those patterns with suffixes overlapping no 
prefixes of others, their nextPat lists are set to NULL like that 
of “rose” in Fig. 1. 
Table II.  Characteristics of ClamAV virus patterns 
Type Total 
Num 
Min  
Len 
Max  
Len 
Avg.  
Len 
Len < 10 
Num 
b (Basic)  28,819 10 210 66.7 0 
0 (Any) 1,098 11 337 94 0 
1 (PE) 50,239 4 392 122.2 42 
2 (OLE2) 185 23 176 104.7 0 
3 (HTML) 1,919 5 382 114.9 1 
4 (Mail) 306 12 172 55.2 0 
5 (Graphics)  1 25 25 25 0 
6 (ELF) 16 17 198 105.6 0 
7 (ASCII) 436 12 335 73.9 0 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2010 proceedings
  
Algorithm I. Pseudo-code of the SHOCK algorithm 
Method: shockMatchSH (x, PM) 
Input: Current matching position x and the set of matched patterns, PM 
Output: SH_MATCH or OC_MATCH  
1: if shift[x+m-1, x+m] = 0 and prefix_hash(x, x+1) fails then 
2:  Hi Å {pi}, for prefix_hash(pi, pi+1) = prefix_hash(x, x+1);  
3:  for each pij with length lj in Hi do 
4:   if x…x+ lj -1 = pij then  
5:    p Å pij; PM Å PM ∪ p; 
6:    x Å x + lj;  
7:     return OC_MATCH;  
8: x Å x + shift[x+m-1, x+m]; 
9: return SH_MATCH; 
Method: shockMatchOC (x, PM) 
Input: Current matching position x and the set of matched patterns, PM 
Output: SH_MATCH or OC_MATCH  
1:  p Å current matched pattern;  
2: if p.nextPat is a bitmap-offset-indexing structure then     
3:  if bit corresponding to x is set in the bitmap then 
4:   offsetx   Å offset of x in the bitmap-offset-indexing table;   
5:   offsetx’ Å offset of the first table entry after offsetx with substring  
      not starting with x;  
6:   for each substring sj with length lj in entries [offsetx, offsetx) do 
7:    if x…x+ lj -1 = sj then  
8:     p Å pattern with PID = sj.PID; 
9:     PM Å PM U p; x Å x + lj; 
10:     return OC_MATCH; 
11: elif any substring s with length l in p.nextPat is matched then  
12:   p Å pattern with PID = s.PID; 
13:   PM Å PM U p; x Å x + l; 
14:   return OC_MATCH; 
15:  x Å x – back_sh; 
16:    return SH_MATCH; 
 
If a pattern with length k is matched in the shockMatchSH 
method, its nextPat list or bitmap-offset-indexing structure is 
examined by the shockMatchOC method to find a consecutive 
overlapped pattern. In case this overlap check fails, the next 
matching iteration can start from text position behind one byte 
of the currently matched pattern, i.e. tk+1, since the failure of 
OC guarantees no pattern can exist within t1…tk. However, if 
we apply the PSP_TH parameter, then when OC fails, we must 
shift the matching position backward by back_sh bytes to 
ensure the correctness of the algorithm.  
C. Worst-Case Analysis  
We analyze the SHOCK algorithm to understand its behavior 
in the worst-case scenario. Let the number of times the scanner 
stays in the shockMatchSH and shockMatchOC methods be Ν
SH  and ΝOC. The overall matching complexity, ω, can be 
expressed as:   
 
ω = ΝSH * ωSH  + ΝOC * ωOC 
, where ωSH and ωOC  represent the cost of the shockMatchSH 
and shockMatchOC method respectively. We anatomize ωSH 
and ωOC in Table III by listing the exact number of memory 
accesses for each operation that constitutes the two methods. 
Here only the cost of accessing the bitmap-offset-indexing 
structure is shown because in worst case we use this structure 
to replace the original nextPat list.  
IV. TRACE-BASED EXPERIMENTAL RESULTS 
In this section, we compare memory consumption and 
matching performance of the ClamAV-AC, ClamAV-BM, and 
SHOCK algorithms. The experiment environment is a desktop 
PC equipped with a 2GHz Athlon 3600+ processor and 2GB 
RAM. The various algorithm parameters are configured as 
follows. We expand only the first three-tier states of the 
ClamAV-AC trie and use block size three to build the 
ClamAV-BM shift table as the same as the original ClamAV 
implementation. The two parameters of the SHOCK algorithm 
are configured as PSP_TH = 8 and PBMAP _TH = 50 for optimal 
performance.  
Virus patterns are taken from the latest ClamAV database 
and classified into nine groups. Because ClamAV matches 
patterns of type b, 0 and 1 using ClamAV-AC and others using 
ClamAV-BM, we further classify the pattern groups as one of 
AC-only(type b, 0, and 1) or BM (type 2 to 7). Accordingly, 
each pattern group has a corresponding ClamAV and SHOCK 
algorithm instance in memory. 
To measure the matching performance, we captured packets 
from several academic networks and treated them as the 
real-world traffic. After removing non-TCP packets, these 
real-world trace files are counted up to about 10GB with 
average packet size being 1500-byte. Each trace contains only 
a small number of common viruses like SQL Slammer, 
CodeRed, and some variants. We injected the ClamAV 
patterns to these traces to generate packet traces with light to 
heavy virus loads for each of the nine types. Table IV depicts 
the packet traces used in our experiments. The infection ratio is 
calculated by dividing the number of infected packets by the 
number of total packets in the given trace. We purposely use 
the synthetic trace containing 85% of inflected packets to 
simulate the extreme worst case.  
Table III. Worst-case complexity in terms of memory accesses 
Operation # memory accesses
ωSH 
Shift table lookup 1  
Prefix-hash table lookup 1 
Exact comparisons  due to prefix- 
hash collision 
Depends on pattern type  
(max < 100) 
ωOC 
Bitmap lookup & table offset 
retrieval 
1 
Exact comparisons for substrings 
in the bitmap-offset-indexing 
table  
5 for PSP_TH = 8  
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2010 proceedings
  
  
Abstract —Connection tracking by manipulating session tables 
is essential for stateful inspection capable applications such as 
stateful firewalls, network-based intrusion prevention systems 
(NIPS), traffic accounting and monitoring to process packets 
according to session state information. With the prevalence of 
multi-core computing, it is crucial to optimize the existing 
connection tracking structures and algorithms to fully utilize the 
underlying parallelism. In this paper, we propose a 
lock-controlled session table partitioning scheme accompanied 
with a dynamic resource balancing algorithm for session-aware 
multi-core networking systems. Experimental results show that 
the proposed scheme reduces the number of lock contentions to a 
maximum of 100 times less and, in turn, boosts the performance to 
3.5 Gbps higher than the baseline. 100% resource utilization is 
also achieved by overcoming the constraint of fixed-sized 
partitioning.  
 
Keywords— Connection tracking; Stateful inspection; Multi-core 
Architecture 
I. INTRODUCTION 
onnection tracking is a fundamental part of any   
session-aware networking applications. By associating 
each packet to its corresponding connection, more intelligence 
can be put for packet inspection to even accelerate the 
processing speed based on connection-level information. Each 
time when a packet passing through, the connection tracker 
retrieves a n-tuple from values of source IP, destination IP and, 
for L4 protocol like TCP or UDP, the source port and 
destination port to calculate a digested value used to lookup up 
the session table. If no corresponding entry in the table is found, 
after some necessary integrity checks, the tracker allocates a 
new entry to insert it into the session table. Otherwise, the 
associated entry is consulted and updated. Hash table and 
search trie are most frequently used structures to implement 
session table for fast lookup. Moreover, appropriate pruning of 
stale or embryonic sessions and releasing their table entries is 
important to avoid the explosion of session table which severely 
degrades connection tracking efficiency and even opens a 
security hole for attacker 
 
This work was supported by the NSC projects under the grant numbers NSC 
97-2221-E-007-108-MY3 and NSC 99-2219-E-007-007. 
 With the popularity of the ever-evolving modern multi-core 
platforms, networking systems should be designed in mind to 
achieve maximum aggregative performance by paralleling 
packet processing tasks. However, to avoid buggy race 
conditions and ensure correctness for applications running on 
the multi-core platform, synchronization mechanisms must be 
enforced. Locking and atomic operations are two frequently 
used techniques to achieve mutual exclusion while at the same 
time prevent the high-cost of context switches compared to 
sleeping semaphores. Unfortunately, synchronization alone is 
not the silver bullet and comes at a price. First, excessive 
synchronization operations lower the overall system 
performance due to their large overheads. Second, according to 
Amdahl’s law, multi-core system performance is maximized by 
minimizing serial code paths [1]. However, too coarse-grained 
locking and code path that is serial in intrinsic prevent 
parallelism. Third, even in the case that the application code can 
be parallelized in a much higher degree by fine-grained locks, 
program complexity grows exponentially and it introduces a 
great risk of system deadlock. Notwithstanding the lock order is 
guaranteed to be correct to avoid deadlock, overheads caused 
by lock contentions and cache line bouncing among CPU cores 
can greatly degrade the system performance.  
 In this paper, we propose a lock-controlled session table 
partitioning scheme which “localizes” connection tracking 
structures and their corresponding locks to each group of CPU 
cores in order to reduce the number of lock contentions and 
cache line bouncing. Accompanied with the dynamic resource 
balancing algorithm, resources are balanced according to their 
utilization rates in each group to achieve the same connection 
tracking capacity without session table partitioning. The entire 
connection tracking system is implemented in a Linux kernel 
module and its effectiveness is testified by the professional Ixia 
testing equipment. 
The rest of this paper is organized as follows. In Section II, 
we briefly describe some related works of session table 
manipulation and multi-core packet processing. Section III 
elaborates the proposed partitioning scheme and resource 
balancing algorithm. Section IV illustrates the experimental 
results. Finally we conclude this paper in Section V.  
A Lock-Controlled Session Table Partitioning 
Scheme with Dynamic Resource Balancing for 
Multi-Core Architecture 
Wen-Yen Tsai*, Nen-Fu Huang*,+, Senior Member, IEEE, and Hsien-Wei Hung* 
Institute of Communications Engineering*, Department of Computer Science+ 
National Tsing Hua University, Taiwan 
C
978-1-61284-231-8/11/$26.00 ©2011 IEEE
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2011 proceedings
  
more CPU cores must be used as the dispatcher and thus reduce 
the number of usable CPU cores. Problems of unbalanced 
dispatches and traffic burstness also degrade the performance. 
We propose to select Ν race_core according to the port 
configuration of the real application. Take IPS and VirusWall 
for example, most of these devices in the market are designed to 
operate in pairs of port, or segments. One port of each segment 
is connected to LAN and the other to WAN. In this 
configuration, packets of the same connection should appear in 
one of the two ports alternatively. Therefore, we can pin ports 
of one segment to two adjacent CPU cores in a physical CPU 
package for them to share the same connection tracking 
structures, i.e., Νrace_core = 2, as illustrated in Fig. 1(b). This 
approach achieves maximum performance since on most 
architectures CPU cores in the same physical package share the 
last level cache (LLC) and incurs no cross-CPU data access 
penalties. Similarly, for devices that support link aggregation, 
packets belong to a connection will appear among the ports that 
form a trunk. For example, if a trunk is composed of four ports, 
it is appropriate to set Νrace_core = 4 and pin the ports and cores 
accordingly.  
B. Dynamic Resource Balancing  
 To overcome the problem of unbalanced resource utilization 
due to fixed sized partitioning, we propose an algorithm to 
dynamically migrate connection tracking resources.  
Resource Migration Operations and Policy 
 Free connection entries can be linked in a list and migrated 
by first determining an appropriate number of entries to move 
and then splicing a sub-list from the lightly-loaded group to that 
of the heavily-loaded one as illustrated in Fig. 2. Resource 
balancing between two groups can thus be accomplished by a 
single list splice operation, or if there are other kinds of 
resources to balance, they can be easily moved in a similar 
manner.  We don’t consider balancing hash table entries, 
because other connection tracking implementations use 
structures like search tries to perform connection lookup. In 
these cases, only connection entries need to be balanced.  
 Concerning resource balancing policy, several factors must 
be considered. First, appropriate timing to perform resource 
balancing should be determined. If the time interval is chosen to 
be too short, large overheads may be introduced due to 
excessive resource migration operations. On the opposite, an 
overlong time interval may cause connection tracking to be 
inefficient or even fail due to unbalanced resource distribution. 
We choose to perform resource balancing at the time points 
when connection aging-outs are performed. For it introduces 
better timeliness than waiting a number of connections to 
terminate until, for example, TCP FIN or RST packets are 
received. Moreover, previous researches [4] proposed criteria 
of selecting age-out time intervals. Second, resource utilization 
is quantified by defining a utilization rate for each resource. A 
threshold Τ corresponding to each rate is also defined to 
suppress unnecessary resource balances. We use a simple 
formula to calculate the utilization rate as the ratio of the 
number of used resources to the total ones. Last, we have to 
choose an appropriate number of resources to migrate. To 
transfer resources from group j to i, let the utilization rate of j 
and i be ρj andρi, and Νj  be the number of free resources in j. 
Naturallyρi > Τ and ρi >ρj and therefore the number of 
resources to migrate is calculated as: Νj * (ρi –ρj). In the 
extreme case ofρi –ρj = 1, all free resources are migrated.  
Algorithm I summarizes the resource balancing algorithm.   
Resource Balancing Overhead 
 Let Νfree_connj be the size of free connection entries of group j, 
the  cost of finding the sub-list head to splice is O(Νfree_connj), 
and then it takes O(1) to splice the list starting from it. Other 
operations such as terminating the free connection entry list at 
the position of the sub-list head can be accomplished in O(1) 
time. Therefore, given Νcore_group  groups of CPU cores in the 
system, the total complexity of migrating free connection 
entries is as follows.  
 
( ) ( )jiconnfreeiΝ
i
Ν
ijj
j
connfree
groupcore groupcore
ρρρ −Δ∗Τ−Δ∗ΝΟ −
=
−
≠=
_
1
0
1
;0
_
_ _
)(  
, where Δ(a) = 0, for all a ≦ 0, and 1 for all a > 0. 
  
            
(a)                                                (b)                                                (c)                                                (d)            
Fig.1. (a) 1 to 1 port/core binding withΝrace_core = 4; (b) 1 to 1 port/core binding withΝrace_core = 2;  
(c) IRQ balancing withΝrace_core = 4; (d) Packet dispatcher withΝrace_core = 1 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2011 proceedings
  
second which is the same time the numbers of concurrent 
connections feeding to segment 1 and 4 exceed 55,000. Free 
connection entries of groups corresponding to segment 2 and 3 
are migrated to the group dedicated to segment 1, but then, 
resources of all the other three groups are migrated to the group 
dedicated to segment 4 in the time interval from 151 to 166 
second. Experimental results of one single heavily-loaded 
group of CPU cores consuming almost all the resources are 
shown in Fig. 6. To summarize, the resource balancing 
algorithm is effective for balancing resources under biased 
traffic profiles to achieve the same resource capacity as using 
global un-partitioned connection tracking structures.  
V. CONCLUSIONS 
In this paper, we propose a session table partitioning scheme 
to divide the global connection tracking structures into 
per-CPU localized ones to mitigate the lock contention and 
cache line bouncing problems. Combined with the resource 
balancing algorithm, the constraint of fixed-sized partitioning is 
removed. Experimental results highlight the effectiveness of 
our method in reducing a maximum of 100 times of lock 
contentions and, in turn, boosting performance to be 3.5Gbps 
higher. Moreover, resources are fully utilized under biased 
traffic profiles to achieve the same capability without session 
table partitioning.  
REFERENCES 
[1] Hill, M.D. and Marty, M.R., “Amdahl's Law in the Multicore Era”, 
IEEE Computer 41(7): 33-38, Jul. 2008. 
[2] Xin Li, Zheng-Zhou Ji and Ming-Zeng Hu, ” Stateful Inspection 
firewall session table processing”, in International Conference on 
Information Technology: Coding and Computing, 2005. 
[3] Ke Zhang, Juan Wang and Dasen Ren, “A matching algorithm of 
Netfilter connection tracking based on IP flow”, in International 
Conference on Anti-counterfeiting, Security and Identification, 
2008. 
[4] Hyogon Kim, Jin-Ho Kim, Inhye Kang and Saewoong Bahk, 
“Preventing session table explosion in packet inspection 
computers”, IEEE Transactions on Computers, 54(2): 238 – 240, 
Feb. 2005. 
[5] Fulp, E.W. and Farley, R.J, “A Function-Parallel Architecture for 
High-Speed Firewalls”, in IEEE International Conference on 
Communications, 2006. 
[6] [online] PF: The OpenBSD Packet Filter, 
http://www.openbsd.org/faq/pf 
[7] [online] netfilter.org, http://www.netfilter.org 
[8] [online] FreeBSD SMPng Project, http://www.freebsd.org/smp/ 
[9] [online] Windows Scalable Networking Initiative, 
http://www.microsoft.com/whdc/device/network/scale.mspx 
 
        Fig.3. Comparisons of number of lock contentions for 256, 512, and 1518-byte frames in combination of 1,000, 5,000, 10,000 and 65,536 concurrent 
connections.  
 
 
        Fig.4. Comparisons of throughput for 256, 512, and 1518-byte frames in combination of 1,000, 5,000, 10,000 and 65,536 concurrent connections. 
                 
Fig.5. Free connection entry distribution of Test I                                        Fig. 6. Free connection entry distribution of Test II 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2011 proceedings
servers, while accelerate the processing time in the gateways 
of NUF. The first idea is the local caching of URL analysis 
results in the gateways. The second idea is to use a hashing 
structure as the data representation of local caching. 
In this paper, to minimize the resource requirements of 
NUF, a novel hashing data structure, called Multi-Level 
Counting Bloom Filter (MLCBF) is introduced to address this 
issue, and specifically we show that how to integrate MLCBF 
into the gateway of NUF as local caching. Based on the idea 
of using Counting Bloom filter (CBF) [1],[2] to store state 
machine [3], MLCBF is used to cache the URL classification 
results to minimize the memory requirements. Analysis and 
experiments are employed to explore the properties of the 
proposed structure and evaluate efficiency by several metrics. 
The results show that our method significantly reduces the 
memory requirements of a gateway of NUF as compared to a 
normal hashing table, and provides with small and constant 
latency time for the operations on MLCBF. 
The remainder of this paper is organized as follows. We 
go over the related works in Sec. 2. In Sec. 3, we describe the 
architectural model of NUF of this work. In Sec. 4, we 
introduce the algorithm of MLCBF and its properties in 
details. Furthermore, we show that how to use MLCBF in 
NUF for local caching. The proposed method is evaluated by 
real URL access logs in Sec. 5. Finally, conclusions and 
future works are presented in Sec. 6. 
2. Related Works 
A wide range of techniques have been proposed for 
enhancing web applications, like web access security, URL 
forwarding and lookup engine [4], and web proxy caching [1]. 
Web content filtering is one of popular approaches to provide 
web access security. The key function of this method is the 
classification on web pages. In [5], it provides a hierarchical 
structure for classifying a large collection of web content. In 
the works of [6],[7],[8], different machine-learning-based 
methods are used to perform web content filtering. Although 
those methods provide accurate filtering results, it seems to 
take too much time to process each web page by multiple 
intelligent techniques. In contrast, NUF and GUF are more 
appropriate for ISP, enterprise, and SOHO networks. 
URL blacklist is another common method to implement 
web filtering engine. Allowing HTTP access or not depends 
on comparing the URL of an HTTP request to the URLs in 
the blacklist. In [9], URL filtering is performed based on 
caching mechanism. In [10], a Wu-Manber-like matching 
algorithm with a support of CRC32 is used in a URL filtering 
system. In [11], two functions are proposed for hashing the 
signatures of URLs which can get efficient URL lookup 
performance. In sum, similar to GUF, the above works store 
blacklist in the local filtering engines, and they therefore have 
to update the databases periodically. 
To the best of our knowledge, our work is the first 
literature to improve NUF by using hashing data structures as 
local caching. In this paper, three hashing structures are 
evaluated, including MLCBF, d-left CBF (DLCBF) [3],[12], 
and CBF. DLCBF is a simple and practical alternative to CBF 
[13]. Compared to CBF, DLCBF saves a factor of two at least 
on memory for the same false positive rate. Notably, MLCBF 
can be viewed as a modification of DLCBF. Motivated by 
multi-level hash table [14],[15],[16] as an improvement of 
d-left hashing table [17], in this work, we introduce skewness 
to the basic construction of DLCBF to improve the run-time 
false positive rate and storage utilization, and to retain its 
benefits of simple construction, and small filter size as 
compared to CBF. 
3. Design of Network-based URL Filtering 
In this paper, NUF uses precise message as communication 
protocol between a gateway and network server. Restated, the 
format of an analysis request from a gateway to the network 
server is <URL>, while an analysis result is <URL, integer>. 
The integer indicates the category of the corresponding URL. 
According to the trace-based evaluation presented in Sec. 5, 
the average size of URLs is about 55 to 59 bytes. 
To extract URLs from HTTP requests (or other 
applications, e.g., FTP, mail, and P2P), a lightweight agent 
runs on gateways that identifies URLs and sends them to the 
network server for analysis. 
3.1 Local Caching in NUF Gateway 
Once a classification result has been generated for a URL, it 
can be stored in a local cache on the gateway. This indicates 
that once a URL has been analyzed, subsequent accesses to 
that URL can be determined locally. This strategy can 
accelerate web traffic, alleviate the load of network servers, 
and reduce bandwidth costs of the entire NUF service. 
Moreover, once a single gateway has accessed an 
unclassified URL and sent it to the network server for 
analysis, any subsequent access of the same URL by other 
gateways can leverage the existing analysis result by sharing 
the result amongst the network servers. Cached analysis result 
stored in the network servers may also periodically be pushed 
to the gateways to update URL classification and be 
invalidated if necessary. 
4. Multi-level Counting Bloom Filter (MLCBF) 
The strategy of using hashing structure as data representation 
is widely used in various network applications to minimize 
resource requirements, like web caching proxy [1], P2P 
resource routing [18], distributed metadata management [19], 
and other applications [2]. In this section, a novel hashing 
structure, called as MLCBF, is introduced to be integrated 
into the gateway of NUF service. 
Fig. 1. The general overview of network-based URL filtering. 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2011 proceedings
as the ratio between the number of total inserted items before 
an overflow and the total cell number. 
A. Maximum Achievable Loads of MLCBF 
The ܮ݋ܽ݀௠௔௫  of MFFs are investigated in simulation by a 
variety of choices (2  ൑  D  ൑  8, 2  ൑  H  ൑ 8). Fig. 3 
summarizes those results (the simulation setup is described in 
Sec. 5). The next subsection discusses the expected ܮ݋ܽ݀௠௔௫  
of MLCBF. 
First, except for D=2, MLCBF has higher ܮ݋ܽ݀௠௔௫  than 
those of DLCBF by the same (D,H). Second, Fig. 3 indicates 
an MFF needs ݊/ܮ݋ܽ݀௠௔௫ cells at least to store n items. For 
example, to support 105 items, MLCBF and DLCBF by (4,4) 
need at least 115,207 and 117,994 cells, respectively. For 
MLCBF, (4,8) is almost as space-efficient 
(ܮ݋ܽ݀௠௔௫ =93.65%) as (8,4) (ܮ݋ܽ݀௠௔௫ =97.26%) but with 
less hash functions. This suggests a smaller latency in a 
platform without hash acceleration hardware. 
B. Storage Utilization and Load Distribution 
The proposed MLCBF scheme is analyzed next. The 
properties of DLCBF are investigated based on the 
experimental simulations. For simplicity, assume that the 
probability of fingerprint collisions is ignored; in addition, the 
analysis does not consider item deletion. The first task 
involves computing the expected storage utilization, ܮܦ௜ , and 
the load factor ߙ௜ of the level ܮ ௜ܸ  of MLCBF.  
If n items are inserted into a hash table with separate 
chaining by a uniform hash function, the fraction with load k 
is 1 ݇!⁄ ሺ݁ି௨ߤ௞ሻas n goes to infinity and the average load is 
µ. Most of the analysis on normal hashing is based on the 
above Poisson distribution.  
In MLCBF, given the number of items n to be inserted 
into a level with m bucket elements (BEs), the corresponding 
expected number of items lying in all BEs which have exactly 
the load of k mapped into them is then 
݇݉ ቀ݊݇ቁ
ሺ݉ െ 1ሻ௡ି௞
݉௡  
To calculate the load distribution of MLCBF, denote 
݊௅௏೔
௢௩௘௥௙௟௢௪as the expected number of items left from the level 
ܮ ௜ܸ to be inserted to ܮ ௜ܸାଵ, ݊௅௏೔௦௨௖௖௘௦௦ as the expected number 
of items inserted to ܮ ௜ܸ  successfully. Then,  
݊௅௏೔
௢௩௘௥௙௟௢௪ ൌ ݊௅௏೔శభ௜௡௦௘௥௧ ൌ ෍ ሺ݆ െ ܪሻ݉ ൬
݊
݆ ൰
ሺ݉ െ 1ሻ௡ି௝
݉௡
௡
௝ୀுାଵ
 ሺ1ሻ 
Applying Eq. (1) recursively, starting from ܮ ଵܸ  with 
݊௅௏భ௜௡௦௘௥௧  = n and ݊௅௏೔௦௨௖௖௘௦௦ ൌ ݊௅௏೔௜௡௦௘௥௧ െ ݊௅௏೔
௢௩௘௥௙௟௢௪, ݅ ൌ 1,2, … , ܦ, 
allow us to estimate ߙ௜ as ݊௅௏೔௦௨௖௖௘௦௦/ܤ ௜ܰ , storage utilization 
of ܮ ௜ܸ as ݊௅௏೔௦௨௖௖௘௦௦/ሺܤ ௜ܰ · ܪሻ, and ܮܦ௜  as ݊௅௏೔௦௨௖௖௘௦௦/݊. For 
instance, according to Eq. (1), to insert 15k items into an 
MLCBF(4,8) containing 20k cells, ݊௅௏೔௦௨௖௖௘௦௦ of ܮ ଵܸ  to ܮ ஽ܸ  
are 10,336, 4,237, 427, and 0, which are very close to the 
10k-trial simulation result: 10,328, 4,237, 432, and 0 on 
average. Finally, by a given total cell number, ܮ݋ܽ݀௠௔௫  of 
MLCBF(D,H) can be estimated by increasing the load till 
݊௅௏ವ௢௩௘௥௙௟௢௪  > 0. 
C. False Positive Rates 
For an MFF, a false positive occurs if and only if for a query 
of y ב ܵ , ݔ א ܵ  exists with ݄௙ሺݔሻ ൌ ݄௙ሺݕሻ  in any 
associated bucket element. Restated, the fraction that this 
event occurs, called as false positive rate (denoted as PFP), is 
calculated from the likelihood that one of all possible cells 
produces the same fingerprint for y ב ܵ . Thus, despite 
increasing ܮ݋ܽ݀௠௔௫ , a higher D or H increases the 
probability of hash collisions and the resulting PFP.  
The rate PFP of an MFF(D,H) can be upper bounded by 
D · H · 2-F. Moreover, PFP of an MFF can be expressed as 
       ௉ܲி ൌ ∑ ߙ௜ · 2ିி஽௜ୀଵ                                ሺ2ሻ 
Due to the skewness and insertion strategy of MLCBF, ∑ ߙ௜ 
of MLCBF at a given load and F-bit is smaller than that of 
DLCBF; even their load factors α are identical. Thus, by Eq. 
(2) and simulation, Fig. 4 reveals that MLCBF has a lower 
PFP than DLCBF does. Next, MFFs use less memory than 
CBF does; normally saving a factor of two, at least for the 
same PFP. Finally, if not specified explicitly, our analysis and 
experiments set ratio R as 0.5, fingerprint size F=20-bit, and 
(D,H) as (4,8). Using (4,8) and F=20-bit yield a PFP  upper 
bounded by 3.051 · 10-5 and maximum achievable load 
ܮ݋ܽ݀௠௔௫  of MFF(4,8) all exceed 90%. 
4.3 Using MLCBF as Local Cache of NUF Gateway 
Based on the idea of using CBF to store state machine [3], 
MLCBF is used to cache the URL classification results. 
Restated, the cell counter CC of MLCBF is used to store the 
classified integer of ݄௙ሺܷܴܮሻ directly, not used in the way 
of its original design; as a counter of a specific ݄௙ሺ݇݁ݕሻ. 
Fig. 5 illustrates the proposed model of using MLCBFs in 
a client engine of NUF. Initially, when a URL is extracted by 
the client engine from HTTP request, it is searched in the 
local MLCBF by ݄௙ሺܷܴܮሻ. The lookup complexity is O(1) 
as described in Sec. 4.1. If the URL is not found, the client 
engine sends the URL as an analysis request to the network 
server for classification. The analysis response is <URL, 
integer>. After the client engine receives the result, it inserts 
<݄௙ሺܷܴܮሻ,integer> into the local MLCBF. Next time when 
the same URL gets into the engine, the classification result 
 
Fig. 4. Measured and expected false positive rates of MLCBF and 
DLCBF with (4,8) under different F bits and filter loads. Each
measured rate is the average of 20-run experiment results. Each run
contains 107 false-positive tests. Total cell number is 10k. 
1E-05
0.0001
0.001
0.01
0.1
10 20 30 40 50 60 70 80 90
upper bound (16-bit) upper bound (20-bit) upper bound (24-bit)
MLCBF(16-bit) MLCBF(20-bit) MLCBF(24-bit)
DLCBF (16-bit) DLCBF (20-bit) DLCBF (24-bit)
MLCBF analysis(16-bit) MLCBF analysis(20-bit) MLCBF analysis(24-bit)
Load (%)
Fa
ls
e 
po
si
ti
ve
 r
at
e 
(%
)
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2011 proceedings
lookups. For instance, MLCBF yields an improvement of 2 to 
7 times for CBF on lookups. For all methods, their cycle 
times of modify() and delete() are similar to those of lookup(). 
Notably, the average times of the insertions and lookups on 
our implementation of hash table take 31,566 and 1,759 
cycles at 80% load. By contrast, they are all smaller than 
2,500 and 1,125 cycles for MLCBF, DLCBF, and CBF. 
Although heavily dependent on implementation, the above 
measurements on CPU provide a practical look on feasibility 
of using MLCBF as a local cache. 
6. Conclusions and Future Works 
The focus of this work is to reduce the resource requirements 
of network-based URL filtering (NUF) by using the hashing 
data representation for caching of URL analysis results. This 
work presents a new compact data structure, called as 
Multi-Level Counting Bloom Filter (MLCBF), to use the 
effect of skewness and insertion distribution over the levels of 
MLCBF for storing a large number of URLs. Specifically, we 
present that how MLCBF can be integrated into a NUF 
gateway. 
The proposed methods have been implemented by Linux as 
a real platform. By real URL access logs, trace-based 
simulation reveals that MLCBF reduces memory 
requirements of a local cache in NUF gateway typically by 
90.9%, as well as provides low operation latency. 
As a future work, we are studying how to utilize web 
caching proxy squid [22] and SquidGuard [23] to implement 
the network servers of our real testbed. Next, as a URL 
lookup method, the method of using MLCBF as URL cache 
should be compared to other methods like hash chain [4] and 
ufdbGuard [24]. Notably, in [4], it reports the memory 
requirement of hash chain is larger than CBF. Thus, we 
believe that MLCBF outperforms hash chain at least in the 
area of memory costs. 
References 
[1] L. Fan, P. Cao, J. Almeida, and A. Z. Broder, “Summary Cache: A 
Scalable Wide-area Web Cache Sharing Protocol,” IEEE/ACM 
Transactions on Networking, 2000. 
[2] A. Broder and M. Mitzenmacher, “Network Applications of Bloom 
Filter: A Survey,” Allerton, 2002. 
[3] F. Bonomi, M. Mitzenmacher, R. Panigraphy, S. Singh, and G. 
Varghese, “Beyond Bloom Filters: From Approximate Membership 
Checks to Approximate State Machines,” Proc. ACM SIGCOMM, 
2006. 
[4] B. Michel, K. Nikoloudakis, P. Reiher, and L. Zhang, “URL 
Forwarding and Compression in Adaptive Web Caching,” in IEEE 
INFOCOM, pp.670-678, March 2000. 
[5] Susan Dumais and Hao Chen, “Hierarchical Classification of Web 
Content,” ACM SIGIR conference, July 2000 
[6] P. Y. Lee, S. C. Hui, and A. C. M. Fong, IEEE “An Intelligent 
Categorization Engine for Bilingual Web Content Filtering,” IEEE 
Transactions on Multimedia, vol. 7, no. 6, 2005. 
[7] P. Y. Lee, S. C. Hui and A. C. M. Fong, ”Neural Networks for Web 
Content Filtering,” IEEE Intelligent Systems, 2002. 
[8] Mohamed Hammami, Youssef Chahir, and Liming Chen, 
“WebGuard: A Web Filtering Engine Combining Textual, Structural, 
and Visual Content-Based Analysis,” IEEE Transactions on 
Knowledge and Data Engineering, Vol. 18, No. 2, 2006. 
[9] Wang Hui-chang , Ruan Shu-hua and Tang Qi-jie, “The 
Implementation of a Web Crawler URL Filter Algorithm Based on 
Caching,” International Workshop on Computer Science and 
Engineering, 2009. 
[10] Zhou, Z., Song, T. and Jia, Y.,”A High-Performance URL Lookup 
Engine for URL Filtering Systems,” Proc. IEEE ICC, 2010. 
[11] Xiaoming Li and Wangsen Feng, “Two Effective Functions on 
Hashing URL,” Journal of Software, vol.14, pp. 177-192, 2004. 
[12] F. Bonomi, M. Mitzenmacher, R. Panigrahy, S. Singh, and G. 
Varghese, “An Improved Construction for Counting Bloom Filters,” 
LNCS 4168, 14th Annual European Symposium on Algorithms, pp. 
684–695, 2006. 
[13] D. Ficara, S. Giordano, G. Procissi, and F. Vitucci, “Multilayer 
Compressed Counting Bloom Filters,” Proc. IEEE INFOCOM, 2008. 
[14] Z. Broder and A. R. Karlin, “Multilevel adaptive hashing,” 
ACM-SIAM SODA, 1990. 
[15] Kirsch and M. Mitzenmacher, “Simple Summaries for Hashing with 
Choices,” IEEE/ACM Trans. Networking, 2008. 
[16] Yossi Kanizo, David Hay, and Isaac Keslassy, “Optimal Fast Hashing,” 
Proc. IEEE INFOCOM, 2009. 
[17] A. Broder and M. Mitzenmacher, ”Using Multiple Hash Functions to 
Improve IP Lookups,” Proc. IEEE INFOCOM, 2001. 
[18] A. Kumar, J. Xu and E. Zegura, “Efficient and Scalable Query 
Routing for Unstructured Peer-to-Peer Networks,” Proc. IEEE 
INFOCOM, 2005. 
[19] Y. Zhu, H. Jiang, J. Wang and F. Xian, “HBA: Distributed Metadata 
Management for Large Cluster-Based Storage Systems,” IEEE 
Transactions on Parallel and Distributed Systems, 2008. 
[20] NLANR PMA Trace, available: http://pma.nlanr.net/. 
[21] Intel, “Using the RDTSC Instruction for Performance Monitoring,” 
Technical report, 1997. 
[22] Squid [Online], available: http://www.squid-cache.org/. 
[23] SquidGuard [Online], available: http://www.squidguard.org/. 
[24] ufdbGuard [Online], available: http://www.urlfilterdb.com/. 
TABLE I: SIMULATION RESULTS OF NETWORK-BASED URL FILTERING BY REAL URL COLLECTIONS FROM NLANR [20]. 
Access list name 
and time 
Total access 
logs 
Total unique 
URLs 
Mean 
URL size 
(bytes) 
URL string 
size (kBs) 
MLCBF(4,8), 
F=20-bit 
DLCBF(4,8), 
F=20-bit 
CBF 
Memory 
(kBs) 
Memory 
(kBs) 
Memory 
(kBs) Reduction 
Memory 
(kBs) Reduction 
bo2(2007/01/09) 241,173 144,852 57 13,534 1,271 90.6% 1,375 89.8% 2,355 82.6% 
bo2(2007/01/10) 207,704 133,420 56 11,384 1,095 90.4% 1,172 89.7% 2,028 82.2% 
rtp(2007/01/09) 3,176,785 1,653,579 59 184,066 16,752 90.9% 17,933 90.2% 31,023 83.2% 
rtp(2007/01/10) 2,986,122 1,501,494 58 169,990 15,747 90.7% 17,201 89.8% 29,161 82.9% 
sd(2007/01/09) 1,426,885 879,114 55 77,341 7,534 90.3% 7,790 89.9% 13,934 82.0% 
sd(2007/01/10) 1,497,891 933,756 55 81,420 7,899 90.3% 8,320 89.7% 14,627 82.0% 
 
Fig. 6. Average insertion and lookup times (in CPU cycles) of CBF, 
DLCBF, and MLCBF at different loads. 
0
500
1000
1500
2000
2500
3000
0 20 40 60 80
CBF DLCBF(4,8)
DLCBF(8,4) MLCBF(4,8)
MLCBF(8,4)
Re
sp
on
se
 t
im
e 
of
 in
se
rt
io
ns
 (
cy
cl
es
)
Load (%)
0
200
400
600
800
1000
1200
0 20 40 60 80
CBF DLCBF(4,8)
DLCBF(8,4) MLCBF(4,8)
MLCBF(8,4)
R
es
po
ns
e 
tim
e 
of
 lo
ok
up
s 
(c
yc
le
s)
Load (%)
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2011 proceedings
This study proposes a novel Enhanced Hierarchical Multi-
pattern Matching Algorithm (EHMA) for fast packet inspec-
tion, which simultaneously searches the packet payload for a
set of patterns. This study contributes modifications to the
hierarchical matching algorithm (HMA) [9] and introduces
the idea of a sampling window and a Safety Shift Strategy in
addition. EHMA is a two-tier and cluster-wise matching
algorithm and can perform fast skippable payload scan.
Based on the occurrence frequency of grams, this study
discovers a small set of signatures from the patterns
themselves to narrow the searching domain. A Min-Max
strategy is used in the EHMA. The hit rate of the first-tier
table in the EHMA isminimized,while the spread of patterns
in the second-tier table is maximized. Accordingly, EHMA
significantly reduces the number of memory accesses and
pattern comparisons. EHMA can skip unnecessary payload
scans by applying the proposed Safety Shift Strategy, which is
based on a frequency-based bad gram heuristic. The frequency-
based bad gram heuristic is a modification of the bad grouped
character heuristic of Wu-Manber (WM) algorithm [10].
Therefore, EHMAhas the advantages of bothHMAandWM.
The memory space and the number of external memory
accesses required by the proposed EHMA are much smaller
than those required by state-of-the-art multipattern match-
ing algorithms. EHMA needs less than 40-Kbyte memory
space to construct required tables for the Snort patterns and,
therefore, enables small-scale and cost-effective hardware
implementations. Using only 768-byte on-chip memory,
EHMA reduces the average number of external memory
accesses to 0.06-0.19 and, thus, significantly improves the
matching time of the detection engine. Simulation results
reveal that EHMA outperforms the state-of-the-art algo-
rithms. Even under real-life intense attack, EHMA still
outperforms others. Because it employs only basic instruc-
tions and two small index tables, EHMA is very simple for
hardware and software implementations. Consequently,
the proposed EHMA is a very cost-effective and efficient
mechanism for real-life network detection systems.
The rest of this paper is organized as follows: Section 2
presents previously proposed pattern matching algorithms
and the fundamental definitions. Section 3 then describes
the proposed EHMA in detail. Next, Section 4 presents the
performance and memory requirements of EHMA. Conclu-
sions are finally drawn in Section 5.
2 RELATED WORK
This section discusses the main concepts and the limitations
of the state-of-the-art exact string matching algorithms that
have been used or modified for packet inspection. Some
fundamental definitions and notations used in this study
are presented.
2.1 Notations
An array is used to represent a string of characters from an
alphabet set . Namely, an element representing string T at
the position i is given by T ½i, where T ½i 2 . The absolute
value of an object means the size of the object. For instance,
jT j denotes the length of the string T , and jj is the number
of elements in the set . A function subðT; i; BÞ is defined as
the substring of T from T ½i to T ½iþB 1. A string can also
be denoted as a set of B-grams, where a gram is defined as a
group of characters, and B is the number of characters in a
gram. For instance, the string “green” can be converted into
a set of 2-grams {“gr”, “re”, “ee”, “en”} when B ¼ 2. The
ith B-gram of a string T is represented as TB½i.
Let P ¼ fpig be a set of distinct patterns, where pi
denotes a pattern with an identification number (ID) i. The
payload of an input packet T and the pattern pi 2 P are
both strings drawn over  with finite length jT j and jpij,
respectively. The notation e:f denotes the value of the field
(or offset) f at the entry (or address) e. If e is a table, then e:f
means all fields named f of the table e.
A single-pattern matching algorithm is used to search a
string (or text) T for the first occurrence or all occurrences of
one given pattern. A multipattern matching algorithm is
applied to search the inputT for all occurrences of anypattern
pi 2 P , or to corroborate that nopattern ofP is inT , where the
number of patterns is from hundreds to thousands. In other
words, the algorithm aims to find all the matched patterns in
T , say PM  P such that PM ¼ fpi j 8pi  T and pi 2 P g.
PM can be applied to any high-level detecting rule, such as
the high-priority-win, first-matched-win, or other state-
concerned rules.
2.2 Previous Work
Single-patternmatchingalgorithmswereoriginallyproposed
to perform text searching problem in computer systems. In
single-pattern matching, Boyer-Moore (BM)-based algo-
rithms provide the best average-case performance in terms
of computation complexity, which is sublinear to the input
string [3], [13]. The BM algorithm uses the bad character and
good suffix heuristics to build a skip table and a shift table,
respectively [13]. The Boyer-Moore-Horspool (BMH) algo-
rithm, which is a variant of BM, slightly modifies the bad
characterheuristic to construct a single skip table [3].The tables
of BM and BMH are precomputed and used to determine the
number of safety shifts of each character for the searching
process. Some characters of T can thus be skipped in the
matching process on specific conditions. Several approaches
apply the BM-based single-pattern matching algorithms
iteratively to solve the multipattern matching problem.
However, network equipment usually has a large pattern
database. Iteratively performing the single-pattern matching
for multipattern matching in the packet inspection engine is
inefficient. Markatos et al.’s approach promotes Snort by
usingabitmapfilterbeforeBMHbutstill searches foronlyone
pattern in each iteration [11].
Several modifications to BM-based algorithms have
been proposed for the multipattern matching. Risk and
Varghese’s (RV) approach groups all patterns to precalculate
the number of safety shifts of each character [5]. The
WM approach, which assumes that all patterns are larger
than M characters, groups B-grams of the M-character
prefixes of all patterns to build a shift table [10]. The WM’s
shift table contains the valid shifts of eachB-gram. Liu et al.’s
algorithm [a variant of the WM algorithm using a grouped
prefix hash (WM-PH)] groups the B-character prefixes of all
patterns to build a large hash table, in which each entry
contains valid shifts of the corresponding B-character prefix
[12]. However, themaximumshift value of RV andWMmust
be not larger than the minimum pattern length in P , in order
176 IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING, VOL. 7, NO. 2, APRIL-JUNE 2010
following sections describe the GFGS, CBS, and the Safety
Shift Strategy in detail. The hierarchical online matching
using these two index tables, namely Tier-1 Matching and
Tier-2 Matching, is then shown.
3.1 The GFGS Algorithm
In the high-layer intrusion detection, patterns may appear
anywhere in the packet payload, making the attacking
packets difficult to recognize. GFGS assumes that a small
set of signatures can be found from the patterns themselves,
then the suspicious substrings of T may be easier to
distinguish from the innocent parts, and the pattern
matching is therefore faster. A set of significant grams is
defined as representatives of a pattern set P , given by
=  B1 , where the size of a gram is B1 characters. The
set = is much smaller than B1 . Only when at least a
significant gram occurs in the payload, a pattern may exist.
That is, when at least one B1-gram of pi belonging to =
occurs in the payload T , the pattern pi 2 P may be found in
T . Many innocent B1-grams of T , which do not belong to =,
can be filtered in the Tier-1 Matching when scanning the
packet payload. Obviously, smaller = leads to fewer pattern
comparisons and, thus, faster pattern matching. The GFGS
is proposed to find the smallest = from P .
Define P g as a subset of P , with P g ¼ fpi j pi has the
gram g; 8 pi 2 P g, where g is called the common gram of
those patterns in the set P g. Notably, if a common gram
appears in the distinct patterns more frequently than other
grams and it is selected as one of the significant grams,
then a smaller = is found. Based on this inference, the
GFGS algorithm is designed to find the frequent-common
gram set F , such that F is the minimum set of significant
grams to represent a pattern set P . In the GFGS, the
common grams are searched only from the sampling window,
which is defined as the last W characters of the first
m characters of a pattern. The range of m is M  m  jpij,
where M denotes the minimum pattern length of all
patterns, and jpij is the current pattern length. Fig. 1
illustrates the sampling window, where B1 is the size of a
frequent-common gram, B1 W , and B2 is the size of the
second pivot in the H2 table, which is explained later.
The GFGS algorithm is presented in Fig. 2. A bitmap
vector V ¼ ðviÞ and a matrix R ¼ ðrijÞ are temporary
memories, where 0  i, j < jjB1 . Vector V records the
occurrence of each B1-gram in a pattern; R is used for
recording frequency, where rij, i 6¼ j, indicates the number
of concurrent occurrences of two B1-grams gi and gj in P ;
and rii records the frequency of the B1-gram gi occurring in
distinct patterns. For instance, rij ¼ 2 means there are
two patterns, each containing both gi and gj. In the
GFGS algorithm, each pattern is first transferred into a set
of B1-grams, and the occurrence of each B1-gram is
recorded in the bitmap V , where B1 is predefined and
depends on the available on-chip memory space. Matrix R
is then derived from V (as shown in line 4 of Fig. 2). Second,
the largest occurrence frequency rff is found, and its
corresponding gram gf is selected as one of F . The elements
of R relating to gf are subtracted accordingly to renew R.
GFGS is repeated until all elements on the diagonal of R
become zero. GFGS uses only a matrix and a vector to
discover F from P .
Fig. 3 plots the pattern spectrum of the Snort patterns
with different gram sizes. The pattern spectrum indicates
the occurrence frequency of grams of patterns. Fig. 3a
shows the distribution of 2-grams of patterns, and Fig. 3b is
the distribution of 1-gram of patterns. As shown in the
figures, they are not normally distributed and have several
peaks, which mean that some grams obviously occur more
178 IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING, VOL. 7, NO. 2, APRIL-JUNE 2010
Fig. 1. The sampling window.
Fig. 2. The GFGS algorithm.
Fig. 3. The pattern spectrum when jP j ¼ 1; 200. (a) Spectrum of 2-grams. (b) Spectrum of 1-gram.
Notably, the maximum shift of EHMA is m while
W ¼ B. The frequent-common grams and the sampling
window are introduced in the proposed frequency-based
bad gram heuristic to improve the flexibility and the
efficiency. Additionally, comparing EHMA with WM,
the maximum safety shift is raised from mBþ 1 to m.
The shift value of the proposed EHMA is similar to but
larger than the shift value of WM, when the given
parameters are m ¼M and W ¼ B.
3.4 Table Construction
The result of GFGS,F , is used to construct the small tableH1,
which is stored in the on-chip memory. A direct index table
of jjB1 entries is used for H1 to achieve fast lookup. B1 is
usually very small (B1 ¼ 1 or 2) and is predefined according
to the available size of on-chip memory. An entry of H1 is
denoted as H1ðaÞ, where a is a B1-gram, and each entry has
three fields: the frequent-common gram ID, H1ðaÞ:fid; the
pattern IDwhen a itself is a pattern,H1ðaÞ:pid, and the safety
shift number in the Tier-1 Matching, H1ðaÞ:shift. Namely,
H1ðaÞ:fid¼fi j a¼fi 2 F g, a n d H1ðaÞ:pid ¼ fi j jpij ¼
jfij ¼ B1; pi ¼ ‘a’ and pi 2 P g. The unused fields of H1 are
set toNULL. SinceH1 is a small table (for instance, 256 entries
in the case of 1-byte coding and B1 ¼ 1), it can be stored in
the on-chip cache. Later, H1 acts as a filter in the online
matching to quickly discover whether the packet contains a
pattern. Namely, EHMA employs H1 to quickly scan and
jump over the innocent substrings of the input packets and to
narrow the searching field to the most likely clusters.
The H2 table is built based on the cluster assignments.
H2 contains the pattern contents and formatted information
of patterns for fast online matching. Let H2ða; bÞ denote an
entry of H2, indicating the head pattern of the cluster P a;b,
and defined as
H2ða; bÞ ¼ H1ðaÞ:fid jjB2 þ b;
where B2 is the length of the second pivot b and is
predefined according to the available size of the external
memory. Each entry H2ða; bÞ consists of six fields: the
safety shift number in the Tier-2 Matching H2ða; bÞ:shift,
the position of the frequent-common gram in the
pattern H2ða; bÞ:offset, the pattern size H2ða; bÞ:size, the
pattern content H2ða; bÞ:data, the pattern ID H2ða; bÞ:pid,
and a pointer H2ða; bÞ:next to the entry of the next pattern
in the same cluster P a;b or the fragmented content of the
current pattern. Transferring the information of patterns
into a predefined format can accelerate the matching
procedure. The patterns in the same cluster Pa;b point to
the same head entry H2ða; bÞ and are linked by the linked
list structure to optimize the memory usage. The required
memory size of H2 is jF j  jjB2 entries plus the shared
memory pool.
For example, if pi is clustered to Pa;b by CBS and H
2ða; bÞ
is empty, then the information of pattern pi is saved into
H2ða; bÞ, where H2ða; bÞ:size ¼ jpij, H2ða; bÞ:data ¼ pi, and
H2ða; bÞ:offset ¼ k i f the kth B1- gram of pi i s a,
H2ða; bÞ:pid ¼ i, and H2ða; bÞ:next is NULL. If another pj is
also clustered to P a;b, then a free entry is also assigned to pj
and linked with the previous pattern pi. Similarly, if the
pattern size of pi is larger than the width of data field, then
pi is fragmented, and the remaining part is saved in a free
entry of the shared memory pool, and the address is saved
in H2ða; bÞ:next.
Fig. 4 shows an example of EHMA, which has five
patterns: “actress,” “teacher,” “firefighter,” “farmer,” and
“architect,” where the alphabet set comprises the 26 English
letters. The parameters for EHMA are assumed B1 ¼ 1,
B2 ¼ 1, m ¼ 6, and W ¼ 3. Fig. 4a demonstrates the GFGS.
According to the GFGS (lines 2-4 of Fig. 2), after scanning
the first W B2 characters of the sampling window of
every pattern (the underlined characters of the patterns in
Fig. 4a), the matrix R is obtained and shown in the figure. In
the first run, the maximum value on the diagonal of R is
three, and thus the corresponding gram “e” is added into F .
After refreshing the elements on the diagonal of R (lines 8
and 9 of Fig. 2), GFGS finds that the maximum value on the
diagonal of R is two in the second run, and the correspond-
ing gram is “h.” GFGS stops while all elements on the
diagonal of R are zero, and gets F ¼ f‘e’; ‘h’g. Fig. 4b
displays the logical architecture of the two-tier tables of
EHMA. BecauseB1 ¼ 1, and theH1 table has only 26 entries,
180 IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING, VOL. 7, NO. 2, APRIL-JUNE 2010
Fig. 4. An example of EHMA, whereB1 ¼ 1, B2 ¼ 1,m ¼M ¼ 6,W ¼ 3,
and F ¼ fe; hg. (a) An example of GFGS. (b) The architecture of the
hierarchical hash tables.
The online matching procedure of EHMA is described in
Fig. 5, including Tier-1 Matching and Tier-2 Matching. Since
EHMA introducesH1 andH2 as filters, andCBS is employed,
only a few suspected patterns are loaded from external
memory and comparedwith T . Because generallymost of the
packets are innocent over the network, and the frequent-
common grams ðF Þ narrow the searching field, EHMA
performs a fast scan over the packets. The returned resultPM
includes all matched patterns for a given T and is applied to
make the final decision and analyze the impending attacks.
The final decision depends on decision-making rules.
An example is provided to demonstrate the online
matching of EHMA. Assume that theH1 andH2 tables have
been built as Fig. 4,whereW ¼ 3 andM ¼ 6. Assume that the
input T is “kangaroo” as given in Fig. 6. The scan runs from
left to right. The scan starts at “g” (ðM W þ 1Þth gram),
obtainingH1ð‘g’Þ:shift ¼ 4. Therefore, Tier-1Matching shifts
four characters. Because the pointer goes beyond jT j B1
after the shift, EHMA completes scanning the input T . This
example only requires one on-cache table lookup and no
external memory access. By only checking T with the
embedded table H1, EHMA can know that T contains no
pattern.
Considering another example where T ¼ ‘iamanactress’
as shown in Fig. 7, the first scanned B1-gram is “a,” yielding
H1ð‘a’Þ:shift ¼ 1. Thus, the matching process stays in the
Tier-1 Matching, and the next B1-gram “n” is read after
shifting one character, yielding H1ð‘n’Þ:shift ¼ 4. Similarly,
staying in the Tier-1 Matching, and the next B1-gram “n” is
read after shifting one character, yielding H1ð‘n’Þ:shift ¼ 4.
Similarly, staying in the Tier-1 Matching, the matching
process obtains H1ð‘r’Þ:shift ¼ 1 and H1ð‘e’Þ:shift ¼ 0
in order after shifting. While H1ð‘e’Þ:shift ¼ 0, the
Tier-2 Matching is activated. After checking the field
H2ð‘e’; ‘s’Þ:pid and finding that it is not NULL, EHMA
knows a suspected pattern may exist. The Tier-2 Matching
then compares input T with the pattern in the cluster P e;s,
where H2ð‘e’; ‘s’Þ:data ¼ ‘actress’, and gets a match. Because
this cluster contains no other patterns, the matching process
returns to Tier-1 Matching with H2ð‘e’; ‘s’Þ:shift ¼ 2. Since
the pointer goes beyond jT j B1 after shifting two
characters, the matching process for the input T is finished.
In this case, H1 is checked four times, and H2 is fetched
only once for the string T of 12 characters. EHMA thus
significantly reduces the latency caused by memory
accesses.
3.6 Incremental Update
EHMA can achieve incremental update by adding a count
field in the H2, which records the current size of every
cluster. The count field has the same function as the
matrix N of CBS. When a pattern p is added into P , after
checking the count fields of the possible entries according
to the pivot pairs of p, the smallest cluster, say P x;y, can
be found. Then, p is added into the cluster P x;y by
following the steps of the table construction mentioned
previously. If no B1-gram of p belongs to F and p finds
no existing entry in the H2, then a random B1-gram of p,
say g, is chosen and added into F (H1ðgÞ is modified
accordingly), and a memory space is allocated for cluster
set P g in H
2. A random pivot pair of p, say ðg; hÞ, is
chosen and then p is added into the cluster P g;h. The shift
fields of H1 and H2 may be modified because of the
added p. Since the safety shift strategy scans the patterns
one by one to calculate the shift values, no modification to
the safety shift strategy is required for pattern addition.
The added p can be recognized as the last scanned pattern
of the safety shift strategy. At most jpj B1 þ 1 fields of
H1 and jpj B2 þ 1 fields of H2 are modified for a pattern
addition.
To delete a pattern p from P , the first step is to find the
pattern. When p is found, just link p’s previous entry to p’s
next entry bymodifying its next field inH2 and delete p from
182 IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING, VOL. 7, NO. 2, APRIL-JUNE 2010
Fig. 5. The online matching procedure, including Tier-1 Matching and
Tier-2 Matching.
Fig. 6. An example of matching process with input “kangaroo.”
Fig. 7. An example of matching process with input “iamanactress.”
4.2 Traffic Models
The simulations used two free and real pattern sets, R1 and
R2, from Snort in August 2004 and May 2008, respectively
[1], although the pattern set can be self-defined or any
commercial pattern set. The number of distinct patterns is
about 1,250 in the R1, where the average length of a pattern
is about 11.2 bytes (the statistics of the pattern set listed in
Table 2); while the number of distinct patterns becomes up
to about 5,000 in the R2. Since Snort patterns are written in
mixed plain text and hex formatted bytecodes, the alphabet
size ðjjÞwas set to 256 in the simulations. In the simulation
traffic models, Models I and II use R1, and Model III uses R2
as the matching pattern sets.
Table 3 shows the relationships between the number of
patterns jP j and the number of frequent-common grams jF j
of the EHMA, where the lengths of patterns are in the range
from 1 to 122, m ¼ jpij, and the patterns are randomly
selected from R1. The results in Table 3 reveal that the
growth rate of jF j is much slower than that of jP j.
4.2.1 Model I
In Model I, the synthetic malicious packets are generated by
randomly choosing patterns from the pattern set P and
spreading over the packet payloads. The attack load  is
defined to represent the expected number of malicious
patterns existing in one packet. For instance, if  ¼ 2, then
each packet contains two harmful patterns on average.
Except for the injected patterns parameterized by , the
background characters of a packet were randomly drawn
from  to imitate the normal packet content. Hence, the
random background may unconsciously contain patterns.
4.2.2 Model II
To evaluate the performance of algorithms in a real intense
attack, a trace from the Capture-the-Flag contest held at
Defcon9 was adopted as the input traffic in Model II. The
Defcon Capture-the-Flag contest is the largest security
hacking game, in which competitors try to break into the
servers of others while protecting their own servers, each
hiding several security holes [26].
4.2.3 Model III
Model III uses a real 2-hour trace as the input traffic, and
the more recent Snort rules R2 as the pattern set jP j. This
real trace recorded all IP packets in a laboratory of
Providence University for 2 hours. The laboratory has an
FTP server, a Web server, and three PCs running several
network application clients.
Table 5 lists the statistics of the traffic traces used in
Model II and Model III, where the values are measured by
traffic analysis tools: tcpstat and tcptrace.
4.3 Memory Requirements
For fast lookup and matching, the lookup information and
patterns are usually saved in the memory using a tabular
structure. Therefore, the memory requirements are esti-
mated according to the number of entries. Since all
algorithms need to keep the pattern content in the (external)
memory, this section only discusses the extra memory
requirement for the tables of each algorithm. In the
simulations, the numbers of characters in the clustering
pivots (B1 and B2) were both assumed to be 1. Because the
H1 of EHMA is a direct index table, the cache memory
space ðMIÞ of EHMA comprises jj entries. Based on GFGS
and CBS, the number of entries in H2 is the total number of
possible clusters (plus a small memory pool). Since the
domain of possible pivot pairs is F  , the external
memory space for H2 ðMEÞ of EHMA is OðjF j  jjÞ.
HMA has the same memory requirement as EHMA. The
shift table of WM is also a direct hash table. The gram size of
WM (block size B) was 3 in the simulations, so the shift table
of WM had jj3 entries. The grouped skip table of WM-PH
used in the simulations was a direct prefix hash table with a
prefix length of three characters. Therefore, the skip table of
WM-PH comprises jj3 entries. Every pattern in the BMH
has its own skip table of jj entries, so that the table of BMH
has jP j  jj entries. Because each skip table of BMH (for
one pattern) is small enough to be loaded into the local
memory, for fairness, a cache memory space was allocated
to lower the number of external memory accesses. The
BMH-O is the original BMH with no local cache and
assesses the latency penalty. Notably, WM-PH, AC-C, and
BMH-O also require cache memory to store the skip value
or one state during the matching process. Table 4 lists the
memory requirements of EHMA, HMA, WM, WM-PH,
BMH, and AC-C. The scale relation of the parameters is
jF j < jj  jP j < S  jj3.
184 IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING, VOL. 7, NO. 2, APRIL-JUNE 2010
TABLE 2
The Pattern Size Distribution of Snort Rule Set R1
TABLE 3
The Number of Frequent-Common Grams
versus the Pattern Set Size
TABLE 4
The Memory Requirements
TABLE 5
The Statistics of the Traffic Traces
much smaller table size. WM-PH gains performance by
having a large direct index table. Notably, the matching time
of the original AC using basic structure is independent from
jP j and . The curves of AC-C increase with jP j and  owing
to the popsum used in the AC-C algorithm. The increasing
jP j makes the matching time of BMH (BMH-O) rise steeply,
because the BMH is originally a single-pattern matching
algorithm that simply executes iteratively for multipattern
matching.
The case  ¼ 0 means that the traffic has no malicious
packets. In this case, the proposed EHMA needs only 9.5-
19.9 cycles per character on average, which is about 0.9,
3.3-5.3, 16.3-26.8, 40-117, and 408-1,161 times less than the
matching time of HMA, WM-PH, AC-C, BMH, and BMH-O,
respectively, under various pattern set sizes. We can say
that EHMA is very appropriate for network equipment,
because generally most packets are innocent ð  0Þ. The
time available for the detection engine to process the
malicious packets rises as the innocent packets are
processed more quickly.
When  ¼ 4, then the systems are under heavy attack,
and the traffic contains many monitored patterns. In this
situation, the matching time of EHMA is about 0.89-0.94,
3.1-4.5, 14.1-24.9, 33.2-96.4, and 335-957 times less than that
of HMA, WM-PH, AC-C, BMH, and BMH-O, respectively.
Additionally, the performance of EHMA is quite stable,
since  rises only slightly as  or jP j rises.
The processing time of the pattern matching includes the
time necessary for instructions ð IÞ and the time for
memory accesses ð MÞ. To investigate their impacts on the
algorithms, these two measurements are separated from
overall matching costs since different systems introduce
different implementation overheads. Fig. 9 displays the
proportion of  I to  and  M to , respectively, for all
approaches using Model I with jP j ¼ 1; 200, where Fig. 9a
shows the results under  ¼ 0, and Fig. 9b shows the results
under  ¼ 4. In Fig. 9, the upper part of the bar is  I and the
lower part of the bar is  M . The results show that the  I of
EHMA is close to that of HMA and WM-PH, but  M of
EHMA is much less than others. The proportion of  M to 
of BMH seems smaller than others, because the whole skip
table of a pattern is idealistically assumed to be loaded
within one external memory access and kept in the cache
during the matching process for each pattern. Because
AC-C compresses the data structure of the state machine, it
requires more time to derive the next state pointer.
Therefore, AC-C does not have the smallest  I . Simulation
results show that the  I does not significantly rise with  in
any of the experiments, because each algorithm has already
tried to reduce the computation load ð IÞ. However,  M
dominates the overall matching cost. This reveals that the
number of external memory accesses is the bottleneck of
almost all algorithms. This result also reflects our opinion
mentioned previously that the essential issue in designing a
high-speed detection engine is to reduce the number of
required external memory accesses.
Fig. 10 compares the average number of external
memory accesses per character ðEÞ of the state-of-the-art
pattern matching algorithms. The figure shows that the E of
EHMA is only 0.06-0.19, which is much smaller than others.
In other words, EHMA can successfully filter out about
94 percent payloads when jP j ¼ 200 and 81 percent when
jP j ¼ 1; 200, requiring no external memory accesses and
string comparisons. The E of EHMA rises only slightly with
rising . The increasing rate of E is slightly higher in EHMA
than in WM-PH when jP j rises, because EMHA has much
smaller table size than WM-PH. Since BMH is based on the
single-pattern matching algorithm, its E is proportional to
jP j. Consequently, the hierarchical matching along with the
safety shift strategy is highly effective in reducing the
memory latency.
Figs. 11 and 12 adopted Model II as a real-life network
environment under intense attack to evaluate the perfor-
mance of the state-of-the-art algorithms. Since different
implementation systems may have different external
memory costs ðwEÞ, Fig. 11 illustrates two results with
wE ¼ 100 and wE ¼ 10, respectively. To lower the impact
of wE on an algorithm, a very small value of wE is adopted
in Fig. 11b. The results in Fig. 11 indicate that EHMA
significantly outperforms others in both cases of small and
large pattern set sizes even in the intense attack. EHMA
still performs better than others even when the penalty on
the external memory access ðwEÞ is reduced (as shown in
Fig. 11b). Comparing EHMA with HMA in Figs. 8, 9, 10,
and 11 reveals that the proposed safety shift strategy
significantly reduces the number of external memory
accesses and thus improves the matching performance.
The minimum length of Snort patterns is one character.
However, some detection systems, such as virus detection
systems, have larger minimum pattern lengths. The perfor-
mance of matching algorithms with long minimum pattern
186 IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING, VOL. 7, NO. 2, APRIL-JUNE 2010
Fig. 12. The costs versus the number of patterns ðjP jÞ using Model II,
wE ¼ 100 and M ¼ 10. (a) Average matching time. (b) Extra memory
requirement.
REFERENCES
[1] Snort, http://www.snort.org, 2008.
[2] S. Antonatos, K.G. Anagnostakis, and E.P. Markatos, “Generating
Realistic Workloads for Network Intrusion Detection Systems,”
Proc. Fourth Int’l ACM Workshop Software and Performance (WOSP),
2004.
[3] R.N. Horspool, “Practical Fast Searching in Strings,” Software
Practice and Experience, vol. 10, no. 6, pp. 501-506, 1980.
[4] A.V. Aho and M.J. Corasick, “Efficient String Matching: An Aid to
Bibliographic Search,” Comm. ACM, vol. 18, no. 6, pp. 330-340,
June 1975.
[5] M. Fisk and G. Varghese, “Fast Content-Based Packet Handling
for Intrusion Detection,” UCSD Technical Report CS2001-0670,
May 2001.
[6] O. Erdogan and P. Cao, “Hash-AV: Fast Virus Signature Scanning
by Cache-Resident Filters,” Proc. IEEE Global Telecomm. Conf.
(GLOBECOM ’05), Nov. 2005.
[7] S. Lakshmanamurthy, K.-Y. Liu, Y. Pun, L. Huston, and U. Naik,
“Network Processor Performance Analysis Methodology,” Intel
Technology J., vol. 6, Aug. 2002.
[8] N. Tuck, T. Sherwood, B. Calder, and G. Varghese, “Deterministic
Memory-Efficient String Matching Algorithms for Intrusion
Detection,” Proc. IEEE INFOCOM ’04, Mar. 2004.
[9] T.-F. Sheu, N.-F. Huang, and H.-P. Lee, “A Novel Hierarchical
Matching Algorithm for Intrusion Detection Systems,”
Proc. IEEE Global Telecomm. Conf. (GLOBECOM ’05), Nov. 2005.
[10] S. Wu and U. Manber, “A Fast Algorithm for Multi-Pattern
Searching,” Technical Report TR94-17, Dept. Computer Science,
Univ. of Arizona, May 1994.
[11] E. Markatos, S. Antonatos, M. Polychronakis, and
K. Anagnostakis, “Exclusion-Based Signature Matching for Intru-
sion Detection,” Proc. IASTED Int’l Conf. Comm. and Computer
Networks (CCN ’02), Oct. 2002.
[12] R.-T. Liu, N.-F. Huang, C.-H. Chen, and C.-N. Kao, “A Fast String
Matching Algorithm for Network Processor-Based Intrusion
Detection System,” ACM Trans. Embedded Computing Systems,
vol. 3, no. 3, Aug. 2004.
[13] R.S. Boyer and J.S. Moor, “A Fast String Searching Algorithm,”
Comm. ACM, vol. 20, no. 10, pp. 762-772, Oct. 1977.
[14] T.-F. Sheu, N.-F. Huang, and H.-P. Lee, “A Time- and Memory-
Efficient String Matching Algorithm for Intrusion Detection
Systems,” Proc. IEEE Global Telecomm. Conf. (GLOBECOM ’06),
Nov. 2006.
[15] C.J. Coit, S. Staniford, and J. McAlerney, “Towards Faster String
Matching for Intrusion Detection or Exceeding the Speed of
Snort,” Proc. Second DARPA Information Survivability Conf. and
Exposition (DISCEX), 2001.
[16] S. Antonatos, M. Polychronakis, P. Akritidis, K.G. Anagnostakis,
and E.P. Markatos, “Piranha: Fast and Memory-Efficient
Pattern Matching for Intrusion Detection,” Proc. 20th IFIP Int’l
Information Security Conf. (SEC ’05), May 2005.
[17] S. Li, J. Torresen, and O. Soraasen, “Exploiting Reconfigur-
able Hardware for Network Security,” Proc. 11th Ann. IEEE
Symp. Field-Programmable Custom Computing Machines (FCCM),
2003.
[18] S. Kim and Y. Kim, “A Fast Multiple String-Pattern Matching
Algorithm,” Proc. 17th AoM/IAoM Int’l Conf. Computer Science,
Aug. 1999.
[19] S. Dharmapurikar, P. Krishnamurthy, T. Sproull, and J.
Lockwood, “Deep Packet Inspection Using Parallel Bloom
Filters,” Proc. 11th Symp. High Performance Interconnects, Aug.
2003.
[20] H. Lu, K. Zheng, B. Liu, X. Zhang, and Y. Liu, “A Memory-
Efficient Parallel String Matching Architecture for High-Speed
Intrusion Detection,” IEEE J. Selected Area in Comm., vol. 24, no. 10,
Oct. 2006.
[21] S. Dharmapurikar and J. Lockwood, “Fast and Scalable Pattern
Matching for Network Intrusion Detection Systems,” IEEE J.
Selected Area in Comm., vol. 24, no. 10, Oct. 2006.
[22] Vitesse Network Processors, http://www.vitesse.com, 2008.
[23] Intel Network Processors, http://www.intel.com/design/network/
products/npfamily/index.htm, 2008.
[24] C. Kruegel, F. Valeur, G. Vigna, and R. Kemmerer, “Stateful
Intrusion Detection for High-Speed Networks,” Proc. IEEE Symp.
Security and Privacy (SP ’02), May 2002.
[25] M. Handley, V. Paxson, and C. Kreibich, “Network Intrusion
Detection: Evasion, Traffic Normalization, and End-to-End
Protocol Semantics,” Proc. Ninth USENIX Security Symp., 2000.
[26] C. Cowan, “Defcon Capture the Flag: Defending Vulnerable Code
from Intense Attack,” Proc. DARPA Information Survivability Conf.
and Exposition (DISCEX III ’03), Apr. 2003.
Tzu-Fang Sheu received the PhD degree in
communication engineeering from National
Tsing Hua University, Taiwan, in 2009, and the
BE and ME degrees in electrical engineering
from Tamkang University, Taiwan, in 1998 and
2000. Since 2009, she is an assistant professor
of the Department of Computer Science and
Communication Engineering at Providence Uni-
versity, Taiwan. Her current research interests
include network security, pattern matching, and
telecommunication networks. She has been a member of the IEEE
since 2000.
Nen-Fu Huang received the BSEE degree
from the National Cheng Kung University,
Tainan, Taiwan, in 1981 and the MS and PhD
degrees in computer science from the National
Tsing Hua University, Hsinchu, Taiwan, in 1983
and 1986, respectively. Since 2008, he has
been a distinguished professor in the Depart-
ment of Computer Science, National Tsing Hua
University, where he was an associate profes-
sor from 1986 to 1994, the chairman from 1997
to 2000, and a professor from 1994 to 2008. His current research
interests include network security, high-speed switch/router, mobile
networks, IPv6, and P2P-based video streaming technology. He is a
member of the IEEE.
Hsiao-Ping Lee received the BE degree in
electrical engineering from National Cheng Kung
University, Taiwan, in 1992, the ME degree in
information engineering and computer science
from Feng Chia University, Taiwan, in 2001, and
the PhD degree in computer science from
National Tsing Hua University, Taiwan, in
2010. Since 2005, he’s taught in the Department
of Applied Information Sciences at Chung Shan
Medical University, Taiwan, R.O.C. He received
the award of the 44th Ten Outstanding Young People of Taiwan in 2006.
His current research interests include the network security, pattern
matching, Bioinformatics, and assistive technology.
. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.
188 IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING, VOL. 7, NO. 2, APRIL-JUNE 2010
accuracy, resource consumption, and operational latency.
Experimental results indicate that the proposed method
significantly reduces network and memory costs of replica-
tion, as well as provides replication with a small and
constant latency time. Hereinafter, the stateful replication
using precise key and state value is referred to as precise
replication. Additionally, the replication by hashing repre-
sentation is called as imprecise or approximate replication.
Next, an adaptive method is developed to prevent
system overloading by the replication of TCP flows, i.e.,
most Internet traffic. The proposed method prioritizes the
pass-through processing over replication at system over-
load to maintain optimal throughput dynamically. Testbed
evaluation demonstrates its feasibility and effectiveness in
an overloaded HAC.
The rest of this paper is organized as follows: Section 2
describes the model of stateful replication and HAC,
motivations, and design goals. Next, Section 3 introduces
MLCBF and its properties as well as explains its use for
stateful replication. Additionally, Section 4 describes an
adaptive mechanism to dynamically control TCP replica-
tion. Section 5 evaluates the feasibility of the proposed
methods based on trace-based and testbed-based experi-
ments. Following a discussion of related works in Section 6,
conclusions are finally drawn in Section 7.
2 PROPOSED MODEL, MOTIVATION, AND DESIGN
GOALS
This work considers a generic HAC, where two sequences
of SPEs are connected by two pass-through links. The SPEs
process pass-through traffic and replicate flow states
simultaneously to their backups through replication links.
Two distinct HA schemes are generally available. In
Fig. 1, active/backup (AB) scheme directs all traffic to the
primary pass-through link during normal operation. If an
SPE in primary link is out of service, a failover occurs
and the traffic is then directed to the backup link.
In active/active (AA) scheme, edge switches attempts to
balance traffic on pass-through links. In both HA schemes,
SPEs rely on replication for reliable service in face of failure
and flow migration due to load balancing [9]. In the AB
scheme, two SPEs of the same functionality function in
primary and backup roles, respectively. In AA scheme, an
SPE plays two roles at the same time. This work focuses
mainly on the AB scheme for simplicity. However, the
proposed methods can be applied equally to an HAC using
AA scheme like testbed tests in Section 5.5.
Fig. 1 shows schematically the SPE architecture of
existing precise replication solutions like OpenBSD pfsync
and Linux ct_sync. Our preliminary tests analyze replica-
tion bottlenecks by using TCP state replication (a modified
version of Linux ct_sync), with six states for each flow, as
described in Section 5.3.2. This work is motivated largely by
our observations.
First, the long-lasting flows replicated from another SPE
may occupy considerable table entries, which are only of use
when necessary. Second, existing precise replication incurs
considerable costs into SPEs and replication linksunder high-
rate traffic. Assume that steady TCP flow rate is 20k
connections per sec (cps), while a replication message
contains<four-tuple; state>whose size is 100 bits andupdate
interval is 30 seconds. An update introduces 20kðcpsÞ 
6ðstatesÞ  30ðsecÞ ¼ 3;600 k messages and 360 Mb of mem-
ory and network costs for replication.
Third, when an attempt to use CBF in stateful replication
as data representation, the bandwidth cost is even higher
than that of precise replication for certain applications.
Finally, CPU load is dominated by the number of incoming
pass-through packets and replication tasks. For an over-
loaded system, replication should be deprioritized for
optimal pass-through throughput.
In sum, this work focuses on the following design goals:
1) an architectural separation of pass-through and replica-
tion processing; 2) design of a hashing structure for stateful
replication at very low runtime costs; and 3) development
of a dynamic scheme to prioritize pass-through tasks over
replication ones for optimal pass-through throughput at
system overload.
3 MULTILEVEL COUNTING BLOOM FILTER
3.1 Filter Structure and Insertion Algorithm
Suppose that we have a set S ¼ fx1; x2; . . . ; xng (i.e., S
containls n items or keys over universe U) that is
changing by item insertion and deletion over time. With
the same functionalities of CBF, MLCBF represents S by
FENG ET AL.: EFFICIENT AND ADAPTIVE STATEFUL REPLICATION FOR STREAM PROCESSING ENGINES IN HIGH-AVAILABILITY CLUSTER 1789
Fig. 1. Practical example of SPEs employing replication in an HAC of
active/backup scheme. Primary and backup SPEs of TCP tracking and
URL categorization are located on two pass-through links. The numbers
in cycles and squares represent the steps of pass-through and
replication processing inside an SPE. Notice the pass-through through-
put of HAC is limited to the minimum throughput of the SPEs on a pass-
through link.
as multilevel fingerprint-based filters (MFFs) to highlight how
their construction concept differs from that of Bloom filter-
based filters like CBF.
Storage utilization or load of an MFF is defined as the ratio
between the number of inserted items and total cell number.
Load distribution of the level LVi (called as LDi) denotes the
ratio between the inserted item number in LVi and total
inserted item number. Load factor  of an MFF measures the
expected item number per bucket, while i denotes the load
factor of LVi. Finally, given D, H, and a total cell number,
consider new items are continuously inserted to an MFF
from scratch until an overflow. Maximum achievable load,
denoted as Loadmax, of an MFF is defined as the ratio
between the number of total inserted items before an
overflow and the total cell number.
3.2.1 Maximum Achievable Loads of MLCBF
The Loadsmax of MFFs are investigated in simulation by a
variety of choices ð2  D  8; 2  H  8Þ. Fig. 3 sum-
marizes those results (the simulation setup is described in
Section 5.1). The next section discusses the expected
Loadmax of MLCBF.
First, except for D ¼ 2, MLCBF has higher Loadsmax than
those of DLCBF by the same ðD;HÞ. Second, Fig. 3 indicates
an MFF needs n=Loadmax cells at least to store n items. For
example, to support 105 items, MLCBF and DLCBF by ð4; 4Þ
need at least 115,207 and 117,994 cells, respectively. For
MLCBF, ð4; 8Þ is almost as space-efficient ðLoadmax ¼
93:65%Þ as ð8; 4Þ ðLoadmax ¼ 97:26%Þ but with less hash
functions. This suggests a smaller latency in a platform
without hash acceleration hardware.
3.2.2 Storage Utilization and Load Distribution
The proposed MLCBF scheme is analyzed next. The
properties of DLCBF are investigated based on the experi-
mental simulations. For simplicity, assume that the prob-
ability of fingerprint collisions is ignored; in addition, the
analysis does not consider item deletion. The first task
involves computing the expected storage utilization, LDi,
and the load factor i of the level LVi of MLCBF.
If n items are inserted into a hash table with separate
chaining by a uniform hash function, the fraction with load
k is 1k! ðeukÞ as n goes to infinity and the average load is .
Most of the analysis on normal hashing is based on the
above Poisson distribution.
In MLCBF, given the number of items n to be inserted
into a level with m bucket elements ðBEsÞ, the
corresponding expected number of items lying in all
BEs which have exactly the load of k mapped into them
is then
km
n
k
  ðm 1Þnk
mn
:
To calculate the load distribution of MLCBF, denote noverflowLVi
as the expected number of items left from the level LVi to be
inserted to LViþ1;nsuccessLVi as the expected number of items
inserted to LVi successfully. Then,
noverflowLVi ¼ ninsertLViþ1 ¼
Xn
j¼Hþ1
ðjHÞ m  n
j
  ðm 1Þnj
mn
: ð1Þ
Applying (1) recursively, starting from LV1 with n
insert
LV1
¼ n
and nsuccessLVi ¼ ninsertLVi  n
overflow
LVi
; i ¼ 1; 2; . . . ; D, allow us to
estimate i as n
success
LVi
=BNi, storage utilization of LVi as
nsuccessLVi =ðBNi HÞ, and LDi as nsuccessLVi =n. For instance,
according to (1), to insert 15k items into an MLCBF(4, 8)
containing 20 k cells, nsuccessLVi of LV1 to LVD are 10,336, 4,237,
427, and 0, which are very close to the 10k-trial simulation
result: 10,328, 4,237, 432, and 0 on average. Finally, by a
given total cell number, Loadmax of MLCBF(D, H) can be
estimated by increasing the load till noverflowLVD > 0.
3.2.3 False Positive Rates
For an MFF, a false positive occurs if and only if for a query
of y 62 S; x 2 S exists with hfðxÞ ¼ hfðyÞ in any associated
bucket element. Restated, the fraction that this event occurs,
called as false positive rate (denoted as PFP ), is calculated
from the likelihood that one of all possible cells produces
the same fingerprint for y 62 S. Thus, despite increasing
Loadmax, a higher D or H increases the probability of hash
collisions and the resulting PFP .
The rate PFP of an MFF(D, H) can be upper bounded by
D H  2F , and it can be also expressed as
PPF ¼
XD
i¼1
i  2F : ð2Þ
Due to the skewness and insertion strategy of MLCBF,
P
i
of MLCBF at a given load and F -bit is smaller than that of
DLCBF; even their load factors  are identical. Thus, by (2)
and simulation, Fig. 4 reveals that MLCBF has a lower PFP
than DLCBF does. Next, MFFs use less memory than CBF
does; normally saving a factor of two, at least for the same
PFP . Finally, if not specified explicitly, our analysis and
experiments set ratio R as 0.5, fingerprint size F ¼ 20-bit,
and (D, H) as (4, 8). Using (4, 8) and F ¼ 20-bit yield a PFP
upper bounded by 3:051  105 and maximum achievable
load Loadmax of MFF ð4; 8Þ all exceed 90 percent.
3.3 Stateful Replication by MLCBF
In Fig. 2, replication is improved through an architectural
separation that prevents the state table from the access by
replication traffic. Consequently, resource competition on
the state table is alleviated and table entries occupied by the
other SPE are avoided before deemed necessary.
For key-and-state access, MLCBF is used to support
insertðkey; stateÞ;modifyðkey; stateÞ; lookupðkeyÞ, and delete
ðkeyÞ operations. Notably, 2c  1 is equal to the highest state
number. The notion simply involves storing the fingerprint
FENG ET AL.: EFFICIENT AND ADAPTIVE STATEFUL REPLICATION FOR STREAM PROCESSING ENGINES IN HIGH-AVAILABILITY CLUSTER 1791
Fig. 3. Average maximum achievable loads of 10k-trail simulation with
different ðD; HÞ settings. Total cell number is 10k.
range ½0; M and reevaluated on T1; T2; . . . ; Ti; . . . . When
Umeasured > Uthreshold (i.e., overloading), by tuning p; t
threshold
is supposed to reduce p=M replication operations and then
improve system performance.
Denote tthresholdi as the minimum lifetime to be replicated
in the interval between Ti and Tiþ1; tmeasuredi;j as the threshold
of jth degradation level estimated from the lifetime
distribution between Ti1; and Ti and t
past
j as the threshold
of jth level estimated from the lifetime distribution between
T0 and Ti1; for j ¼ 0; 1; . . . ;M. To balance stability and
responsiveness, the cumulative lifetime distribution ob-
served between Ti1; and Ti and the distribution observed
between T0 and Ti1; are both used to estimate tthreshold for
the next Tinterval. A function FpðiÞ defining tthresholdi at Ti is
considered as follows:
Fpð1Þ ¼ tmeasured1;0
FpðiÞ ¼ ð1RDLIÞ  Fpði 1Þ þRDLI  tmeasuredi;p :
(
ð4Þ
In (4), Fpði 1Þ ¼ tpastj . The factor RDLI is in the range ½0; 1
and set as 0.7 for better responsiveness.
Algorithm 2 describes our self-tuning algorithm. Mea-
suring CPU utilization Umeasured and adjusting p allow us to
dynamically control replication costs under various traffic
mixes. Finally, postponed state changes are still replicated
in-order to keep the design simple.
Algorithm 2. Pseudo-code of DLI
Function DLI
1: wait for Tinterval
2: if Umeasured > Uthreshold then
3: p pþ 1
4: else p p 1
5: endif
6: calculate the next lazy threshold by FpðiÞ, update the
latest statistics
7: to the past history array, and reset the array of the latest
interval
5 EVALUATIONS
5.1 Implementation and Testbed Setup
Experiments were performed on a testbed consisting of two
identical 3-port machines (Intel Pentium-4 2.0 GHz and
1,024 MBs RAM) as SPEs in our HAC. Two SPEs are
connected with 100 Mbps LAN (i.e., replication link) and
the external and internal ports are connected with Gigabit
Ethernet networks (i.e., pass-through link). By the design of
Fig. 2, replication methods and DLI are implemented in
Linux 2.4.20 kernel and replication is propagated by reliable
UDP. Next, a state table is implemented to store precise
data and verify false rates in tests by using a normal hash
table. The CPU cycles of the operations on each structure
are assessed by rdtsc. Finally, CPU utilization is evaluated
by executing dstatwith 1-sec bin.
For CBF, the number of hash functions is 4, and load
factor of CBF (i.e., m=n, the ratio of the number of items in
maximum to filter slots) is 10. Theoretically, FP rate (PFP ) is
about 1.2 percent. An SHA-1 implementation with mod-
ification is used for D hash functions, fingerprint, and
signatures of CBF. Table 3 lists the parameters. Notice that
the simulation used to elucidate the filter properties is also
performed by our prototype platform.
5.2 Real Traces of URL Requests and IP Packets
The replication methods are validated by the simulation
based on IP packet traces and URL access logs. After
processing all logs, FP rates are verified by a round of 106
random keys as URLs or TCP four tuples that do not exist
in the state table. FN and IS rates are verified by
comparing precise key and its state in the state table with
the data in filters.
For URL applications, six one-day collections of HTTP
requests from NLANR [13] are used during the analysis.
URL string size is the sum of all distinct URL string lengths.
For SPEs on TCP flows, all replication schemes are applied
to three bi-directional packet traces from NLANR (denoted
as IPLS-1, IPLS-3, and AUCK-4). Tables 1 and 2 list the
detailed information of real traces.
5.3 Replication for State-Machine Tracking
5.3.1 URL Categorization
In URL categorization, numerous servers collect, and
classify URLs through web content classification. According
to Fig. 1, the categorization outcome allows SPEs in
gateways to classify pass-through HTTP traffic by URLs.
An operator can thus establish management policy by
useful categories, such as malicious threats. Forty to ninety
categories are normally represented by integers.
A URL request received by an SPE is normally sent to one
of master servers for classification. A service provider
reported receiving over 100 million requests for categoriza-
tion daily. Thus, the caching of categorization results in the
SPEs can accelerate pass-through web traffic, alleviate
FENG ET AL.: EFFICIENT AND ADAPTIVE STATEFUL REPLICATION FOR STREAM PROCESSING ENGINES IN HIGH-AVAILABILITY CLUSTER 1793
TABLE 1
Simulation Results of URL Categorization by Real URL Collections from NLANR [13]
Memory and network bandwidth requirements of imprecise replication methods.
throughput. Two sets of short flows (27 and 10k cps) are then
inserted to the SPE.With the 1st set, although the throughput
degrades, CPU does not surpass Uthreshold. Following inser-
tion of 2nd set, system exhibits saturation. DLI quickly brings
CPU to oscillate aroundUthreshold by adjusting t
threshold around
48 to 68 ms.
In Fig. 6a, DLI controls replication effectively to
alleviate CPU load, thus enhancing the throughput of
SPE from 77 to 224 Mbps. In contrast, CPU without DLI
exhibits saturation by two short-flow sets. Without
replication, the maximum throughput under two sets of
short flows is 279 Mbps on average.
Next, the end-to-end throughput of our HAC in the AA
scheme consisting of two SPEs is determined using TCP
state replication. During testing, two pass-through links of
HAC are stressed by the same rates of short flows at first
and the aggregated throughput (i.e., pass-through through-
put of HAC) of HTTP traffic on two links are measured
under the high-rate flows. In Fig. 6b, DLI improve the pass-
through throughput of HAC using precise replication from
158 to 490 Mbps at 36k cps. SPEs using imprecise replication
outperform SPEs using precise replication. At 27k cps and
without DLI, MLCBF improves the aggregated throughput
from 716 to 850 Mbps. Finally, at 42k cps, two SPEs are
almost saturated by incoming pass-through packets; even
without any replication.
6 RELATED WORKS
For HACs, we focus on the performance of passive
replication [15] which is a popular technique to support
reliable service. Many solutions on fault-tolerant transport
protocols have been proposed (e.g., [16]). Our work
complements these studies by focusing on unified solutions
for stateful replication in an HAC.
DLCBF [6], [7] is a simple and practical alternative to
CBF. Compared to CBF, DLCBF saves a factor of two at least
on memory for the same PFP . Like the multilevel hash table
[17], [18], [19], [20] as an improvement on multiple-choice
hash table, we introduce skewness to DLCBF as well as a
different insertion strategy to lower runtime FP rate (PFP ),
and increase storage utilization, and retain its benefits of
simple construction, compact size, and, most importantly,
single incremental message per update. To our knowledge,
this work attempts for the first time to minimize the
resource requirements of stateful replication by using MFFs.
Other examples of using randomization in replication are
distributed metadata management [21] and resource rout-
ing on P2P networks [22], [3].
7 CONCLUSIONS
This work improves replication performance and in-
creases pass-through throughput of HACs by hashing
replication representation and an adaptive scheme to
prioritize pass-through processing over replication during
system overloading.
For efficiently key-and-state replication, this work pre-
sents a new compact data representation, called Multilevel
Counting Bloom Filter (MLCBF), to use the effect of
skewness and insertion distribution over MLCBF levels
for stateful replication of a large number of active flows.
The proposed methods have been implemented by
Linux as a real platform. Trace-based simulation reveals
that MLCBF reduces network and memory requirements
of replication typically by 94.7 and 90.9 percent, respec-
tively, for URL categorization, as well as provides low
operation latency.
Furthermore, this work presents a self-tuning scheme,
called as Dynamic Lazy Insertion, for TCP flows to control
replication costs of an overloaded system. Testbed and
trace-based results indicate that adaptation by flow lifetime
and CPU utilization alleviates the load from short flows,
protects a majority of the Internet traffic, and offers optimal
throughput for an HAC. The proposed mechanism typically
increases the pass-through throughput of an overloaded
HAC from 158 to 490 Mbps.
ACKNOWLEDGMENTS
This work was supported by National Science Council
(NSC) of Taiwan under the grant numbers NSC-97-2221-E-
007-108-MY3, NSC-98-2221-E-007-060-MY3, and NSC-99-
2219-E-007-007. The authors greatly appreciate the real-
world network traces provided by NLANR and construc-
tive comments from anonymous reviewers.
REFERENCES
[1] M. Balazinska, H. Balakrishnan, S.R. Madden, andM. Stonebraker,
“Fault-Tolerance in the Borealis Distributed Stream Processing
System,” ACM Trans. Database Systems, vol. 33, no. 1, pp. 1-44,
2008.
[2] L. Fan, P. Cao, J. Almeida, and A.Z. Broder, “Summary Cache: A
Scalable Wide-Area Web Cache Sharing Protocol,” IEEE/ACM
Trans. Networking, vol. 8, no. 3, pp. 281-293, June 2000.
[3] A. Broder and M. Mitzenmacher, “Network Applications of
Bloom Filter: A Survey,” Allerton, vol. 1, no. 4, pp. 485-509, 2002.
FENG ET AL.: EFFICIENT AND ADAPTIVE STATEFUL REPLICATION FOR STREAM PROCESSING ENGINES IN HIGH-AVAILABILITY CLUSTER 1795
Fig. 6. a) Behavior of primary SPE with precise TCP state replication
and DLI in the AB scheme, and b) the aggregated throughput of two
SPEs of HAC in the AA scheme under high-rate short flows.
 1
出席
 IEEE ICC2011 國際通訊會議 
出國報告
 
清華大學資訊工程系
 
黃能富教授
 
 
(一)參加會議目的 
 
 網路通訊(Communication networks) 技術及應用是現今與未來相當重要的關
鍵領育, 其中無線通訊網路(wireless communications)、通訊理論(communication 
theory)、光通訊網路(optical networks)、網路安全(network security)、網路應用
(Internet applications)、雲端計算 (Cloud computing)、甚至物聯網 (Internet of 
Things) 的興起更突顯網路通訊之重要性以及市場性。為了能瞭解並掌握網路通
訊相關技術及未來研發方向重點，本人參加於日本京都舉辦之 2011 國際通訊會
議 (IEEE 2011 International Conference on Communications, ICC2011), 並於會議
中發表兩篇論文: (1) “An Efficient Caching Mechanism for Network-based URL 
Filtering by Multi-level Counting Bloom Filters”, (2) “'A Lock-Controlled Session 
Table Partitioning Scheme with Dynamic Resource Balancing for Multi-Core 
Architecture”。 並就通訊網路技術與雲端計算應用之未來發展與專家學者廣泛交
換意見。 
 
（二）參加會議經過 
「IEEE ICC2011 國際通訊會議」於 2011/6/5-2011/6/9 在日本京都舉行。本人於 
2011/6/5 由台北搭乘長榮航空班機到日本京都參加會議。會議期間除了出席會議
與專家學者交換意見之外, 也於 6/7 晚上出席大會之接待晚宴 。此會議邀請三
位專題演講:  
(1) 由來自日本  DOCOMO 的資深執行副總裁  Ryuji Yamada 主講的
 3
 
Professor Maurizio Decina專題演講 
 
Professor Maurizio Decina專題演講(續) 
 
□ 赴國外出差或研習 
□ 赴大陸地區出差或研習 
□ 出席國際學術會議 
□ 國際合作研究計畫出國 
心得報告 
計 畫 名 稱  計 畫 編 號  
報 告 人 
姓 名 
馮乙軒 
服 務 機 構 
及 職 稱 
國立清華大學資訊工程研究
所博士班 博士候選人 
會議/訪問時間 
 地點 
Globecom 2008, 11/30 – 12.04, 2008. New Orleans, LA, US 
會 議 名 稱 IEEE Globecom 2008 
發表論文題目 （檢附論文檔案） 
 
一、主要任務摘要（五十字以內） 
 
於會議上報告被接受之論文, 並且參與其他論文報告和keynote speaking 
 
二、對計畫之效益（一百字以內） 
 
參與會議的目標主要在於: 瞭解其他相同領域之研究學者人員的方向, 在會議上針對
相關之問題作討論, 尋找新的解決方法, 以及開發其他領域之研究題目.  
以上之目標, 對於本計畫之實施與進步, 包含著相當的貢獻度. 
 
三、經過 
11/29 23:30: 抵達美國紐奧良阿姆斯壯機場 
12/1: IEEE Globecom 會議第一天： 
第一天的 Keynote Speaker’s presentation 是 Mr. Kaoru Yano (Representative Director of 
NEC). 算是 GC 2008 的開場, 相當的盛大. 另外, 於本日去參加下面的論文的報告: 
NG10PM1-2: Real-time P2P Traffic Identification  
NG10PM1-3: A Memory-optimized Bloom Filter using An Additional Hashing Function 
NG10PM1-4: H-SIP: Hybrid SIP Network 
NG10PM1-5: Improving Bit Torrent Traffic Performance by Exploiting Geographic Locality  
NG12PM1-3: An Ethernet Access Architecture for Highly Available IPTV 
CS02M3-2: Evaluation of a Rule-Based Approach for Context-Aware Services  
CS02M3-3: Design and Implementation of Multi-Platform Infrastructure of Extensible Network 
Functions 
NG03M3-3: A Framework for Network State Management in the Next-Generation Internet 
Architecture 
NS11PM3-3: Classification of Network Traffic via Packet-Level Hidden Markov Models 
NS11PM3-4: Inferring Speech Activity from Encrypted Skype Traffic 
NS11PM3-5: CRESTBOT: A New Family of Resilient Botnets 
NS12PM3-1: Secure Context Switch for Private Computing on Public Platforms 
Globecom算是相當盛大的會議. 參與的研究人員, 教授, 學生非常的多. 會議中討論
的氣氛也非常良好. 個人認為這次的會議雖然遠在 New Orleans, 但收穫也算不小. 
可惜很多的 poster session 的論文都沒有來與會, 算是美中不足的地方. 
 
五、建議與結語 
 
感謝這次的補助讓學生可以成行參加會議並且收穫良多. 
 
 
97 年度專題研究計畫研究成果彙整表 
計畫主持人：黃能富 計畫編號：97-2221-E-007-108-MY3 
計畫名稱：高速封包交換機之研究--子計畫六:高速應用服務交換機之研製 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 4 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 2 2 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 3 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
