  2
A. Color model 
This project uses the YUV color model that defines a color 
space in terms of a luminance (Y) and two chrominance (U,V) 
components. The RGB color space can be converted into YUV 
color space using the following equation, 
 
0.299   0.587   0.114
-0147  -0.289   0.436
0.615    -0.515  -0.100
Y R
U G
V B
⎡ ⎤ ⎡ ⎤ ⎡ ⎤⎢ ⎥ ⎢ ⎥ ⎢ ⎥
=⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣ ⎦ ⎣ ⎦
              (1)                                                
    In our system, we use the Y value to form a gray scale image. 
Every image is composed of 320*240 pixels, where the gray 
value of each pixel ranges from 0 to 255. The gray value 0 
presents black and 255 presents white. 
B. Moving object segmentation algorithm 
The algorithm consists of two major parts. First, we 
construct a reliable background image using frame differences 
and registering the stationary pixels. When a pixel is classified 
as stationary in several frames, we store it in background buffer 
until the background scene is complete. Then, the moving object 
region is separated from the background region using 
background difference, i.e., the difference between current 
image and the registered background.  
C. Frame Difference 
The difference between current frame and previous frame, 
which is stored in Frame Buffer, is calculated and segmented. 
Frame difference is defined as follows: 
1( , ) ( , ) ( , )
1       if  
( , )
0       if  
n n n
FD
n
FD
FD x y f x y f x y
FD Th
FDM x y
FD Th
−
= −
≥⎧
= ⎨
<⎩                (2) 
 
where ( , )nf x y  represents the Y value of a pixel with position 
( , )x y  in the n th frame. To decide whether a pixel belongs to 
a moving object or the background scene for two consecutive 
frames, we need a threshold value for distinction. The threshold 
value is used to find the difference between two consecutive 
input frames based on the concept of change detection. The 
method to decide the optimal FDTh  is shown in Section II-H. In 
frame difference between two consecutive frames, we focus on 
stationary background whose characteristic is well known and 
more reliable. The threshold value can be used to distinguish 
stationary from non-stationary pixels and mask the stationary 
ones. This information is sent to the pixels background 
registration step where the reliable background is constructed 
from the difference masks in several frames. 
D. Background Registration 
The goal of background registration step is to construct 
reliable background information from video sequences. 
According to FDM, pixels not moving for a long time are 
considered as reliable background pixels. The initial 
background registration can be shown as 
( , ) 1,     if  0
( , )
0,                      if  1
( , ),         if  ( , )
( , )
,     if  ( , )
n
n
n n th
n
n th
SI x y FDM
SI x y
FDM
f x y SI x y F
x y
undefined SI x y F
µ
+ =⎧
= ⎨
=⎩
≥⎧
= ⎨
<⎩
  (3) 
where  is stationary index, and  is the background information.SI µ
The initial values of SI andμare all set to 0. If a pixel is masked 
as stationary in frame difference, the corresponding value in 
registration stationary index is increased by one. On the contrary, 
if the pixel is classified as moving, the corresponding value in 
registration stationary index is cleared by zero. If a pixel is 
masked as stationary and for successive thF  frames, i.e., the 
accumulated value in registration stationary index exceeds thF , 
then it is classified as background region. We perform the above 
steps until the background scene is complete, i.e. all pixels in the 
images are classified as background region. 
One important factor on segmentation performance is the 
illumination effect that may cause segmentation errors for 
gray-level images. In our system, we use the background update 
method to reduce this effect. The background updating method 
is shown in the following:  
1
2 2 2
1
  
f     ( , ) ( , ) 2 ( , )
( , ) ( , ) ( , )
( , ) ( , ) ( , ) ( , )
(1 )
(1 )( )
n n n
n nn
n n nn
i f x y x y x y
x y x y x y
x y x y x y x y
f
f
µ σ
µ αµ α
σ ασ α µ
−
−
− <
⎧⎨⎩
= + −
= + − −
  (4) 
where ( , )n x yσ s’ are standard deviation values and α  
( 0 1α< < ) is a predefined constant. We use α = 0.7 in this 
project. 
E. Background Difference 
After we get a reliable background scene from the 
background registration, we use background subtraction to 
distinguish the moving object from background scene. This step 
generates a background difference mask by thresholding the 
difference between the current frame and the background scene. 
The operations of background difference can be shown as  
( , ) ( , ) ( , )
1,    if  
( , )
0,    if  
n n n
BD
n
BD
BD x y f x y x y
BD Th
BDM x y
BD Th
µ= −
≥⎧
= ⎨
<⎩
           (5) 
where BD is background difference and BDM is background 
difference mask. The threshold value BDTh  is also 
automatically determined by the Euler method in Section II-H. 
 
F. Object Detection 
    In this process, we can determine which areas are moving and 
which area are stationary in current frame. The object detection 
algorithm use both FDM and BDM to produce an initial object 
mask (IOM). The procedure of object detection can be presented 
as follows 
( , ),     if  ( , )
( , )
( , ),     else
n n th
n
n
BDM x y SI x y F
IOM x y
FDM x y
≥⎧
= ⎨⎩
 (6) 
There are four cases in segmenting the moving object, details of 
which are discussed as follows. In cases 1 and 2, the background 
information is not yet reliable, the separation uses only the 
frame difference information. So the segmented region of 
moving object in these cases is not complete. In the following, 
|BD| is the absolute value of difference between the current 
frame and background buffer, |FD| is the absolute value of frame 
difference, and BDTh and FDTh  are the threshold values for 
generating the background difference and frame difference, 
respectively.  
  4
0 50 100 150 200 250 300 350
0
2
4
6
8
10
12
14
16
18
20
 
0 50 100 150 200 250
0
5
10
15
20
25
30
 
(b) 
0 50 100 150 200 250 300 350
0
2
4
6
8
10
12
14
16
18
20
0 50 100 150 200 250
0
5
10
15
20
25
 
(c) 
0 50 100 150 200 250 300 350
0
2
4
6
8
10
12
14
16
18
20
0 50 100 150 200 250
0
5
10
15
20
25
30
 
(d) 
0 50 100 150 200 250 300 350
0
5
10
15
20
25
0 50 100 150 200 250
0
5
10
15
20
25
30
 
Fig. 2 DFT of histograms of the vertical (left column) and horizontal (right 
column) projection for postures of (a) standing, (b) bending, (c) sitting, and (d) 
lying. 
 
1
0
1
0
[ ] ( ) exp[ 2 / ]
[ ] ( ) exp[ 2 / ]
, 0,1,2,..., 1.
i N
i
i
i N
i
i
F u projection x j ui N
F v projection y j vi N
u v N
π
π
= −
=
= −
=
= −
= −
= −
∑
∑         (10) 
 
Then we can compress the data size by getting the first 20 
data and normalize to [1]F . The normalized DFT ( [ ])nF u  
can be defined as follows: 
[ ][ ]      0,1, 2,...., 1
[1]
F unF u u N
F
= = −      (11) 
 
IV. LOCAL RULE FEEDBACK NEURAL 
 FUZZY SYSTEM (LRFNFS) 
The postures in a sequence of images are context dependent. 
Foe example, a lying posture can not change directly to a 
standing posture. Thus a recurrent neural fuzzy system should 
be more suitable than a feedforward neural fuzzy system for this 
posture recognition problem. The proposed Local Rule 
Feedback Neural Fuzzy System (LRFNFS) is a recurrent neural 
fuzzy system. In LRFNFS, the past firing strength of each rule 
feeds back the rule itself. Thus, the firing strength of each rule 
depends not only on the current inputs but also past inputs. The 
consequent of each rule is a fuzzy-singleton. Let 
1( ),..., ( )nx k x k  be the input variables and 1( )y k , …, 
( )my k  be the LRFNFS output. Suppose the LRFNFS contains 
r  rules. The LRFNFS functions are introduced as follows.  
˙Fuzzification: The antecedent parts in LRFNFS uses 
Gaussian fuzzy sets. The number of fuzzy sets in each input 
variables is equal to the number of rules. The membership 
function of the i th fuzzy set in input variable jx  is  
2( ) exp{ ( ) }j ijij j
ij
x m
M x
σ
−
= −
              (12) 
where ijm  and ijσ  represent the center and width of the fuzzy 
set, respectively.  
˙Inference engine: The fuzzy AND operation is implemented by 
The algebraic product in fuzzy theory implements the fuzzy 
AND operation. Thus, given an input data set 1( , , )nx x x=
K … , 
the feedforward firing strength ( )i xφ K  of rule i  is calculated by 
2
11
( ) ( ) expn n j iji ij j jj
ij
x m
x M xφ
σ==
⎧ ⎫⎛ ⎞
−⎪ ⎪
= = − ⎜ ⎟⎨ ⎬⎜ ⎟⎪ ⎪⎝ ⎠⎩ ⎭
∑∏K
   (13) 
Due to the feedback of the previous firing strength, the actual 
recurrent firing strength for output jy is calculated  
( ( )) ( ( )) (1 ) ( ( 1))j ji i i i ix k x k x kλ φ λΦ = + − Φ −K K K (14) 
where 0 1jiλ≤ ≤  decides the weighting between the current 
feedforward firing strength ( ( ))i x kφ K  and recurrent firing 
strength ( ( 1))i x kΦ −
K
.  
˙Defuzzification: Let jia  denote the fuzzy singleton of the i the 
rule for output jy . The output ( ( ))jy x k
K
 of the system is 
calculated by the weighted average defuzzification method 
1
1
( ( ))
( ( ))
( ( ))
r j
i ii
j r
ii
x k a
y x k
x k
=
=
Φ
=
Φ
∑
∑
KK
K                  (15) 
             
Structure learning of LRFNFS uses the clustering algorithm in 
[18]. The gradient descent algorithm is used for tuning all of the 
free parameters in LRFNFS.  
We use the first 20 normalized coefficients of DFT of 
histograms of horizontal and vertical projections, respectively, 
together with the length-width ratio (LWR) values of silhouette 
of human body. These 41 feature values are sent into the 
LRFNFS system. LRFNFS is trained to recognize four postures 
of human body, and they postures of standing, bending, sitting, 
and lying. If training input data are postures of standing, 
bending, sitting, and lying, then the desired output are (1, 0, 0, 0), 
(0, 1, 0, 0), (0, 0, 1, 0) and (0, 0, 0, 1). To capture the temporal 
relationship between postures, training postures are collected 
from a sequence of images. When feature of an unknown 
posture is fed to LRFNFS, we have the LRFNFS output 
1 2 3 4( , , , )y y y y . If iy  is the largest one, then the unknown 
posture is classified as the i th posture. For example, if 3y  is 
the largest output, then the unknown posture is classified as 
posture of sitting. 
  6
Table  1. The recognition rate of the test results with three kinds of feature 
extraction methods by LRFNFS 
Features  DFT+L-W Moment Moment+θ
Standing 96.46% 41.33% 96.8% 
Bending 84.98% 4.4% 56.78% 
Sitting 93.47% 96.92% 90.95% 
Lying 99.8% 90.69% 94.94% 
Total 93.13% 57.66% 84.65% 
 
 
Fig. 3. The comparison of the segmentation results. the first column: original 
images; the second column: the proposed automatic thresholding with Euler 
number results; the third column: the zero-mean Gaussian distribution method 
[6]; the fourth column: 
4W  in [12]. 
 
0   100 200 300 400 500 600 700 800 900 1000
0
Lying   1
Sitting   2
Bending  3
Standing  4
5
Frames
Desired output
LRFNFS
SONFIN
 
Fig. 4 The desired output and the recognition results of LRFNS and SONFIN for 
test data (901 frames). 
 
i  
 
 
Fig. 5. Original and segmented images of falling-downing conditions. 
 
s added to the moment feature vector as another feature 
extraction method, and is denoted as “Moment+θ ”. Table 1 
shows that the recognition rate with DFT+L-W method is 
extremely superior to the other two feature extraction methods. 
Figure 4 shows the desired output and the LRFNFS and 
SONFIN outputs of one test data subset. The outputs of the 
LRFNNS are smoother and closer to the desired outputs than the 
SONFIN. 
 
C. Falling-down detection 
The experiments demonstrate an emergency when a person 
falls and remains in a lying posture. The original image and 
segmented silhouettes of two falling-down conditions are 
shown in Fig. 5. Results show that 1I = , and the system 
sends an “emergency” signal as this indicates a person has 
fallen and remains in the lying posture. That is, the system can 
detect an emergency successfully. 
 
VI. CONCLUSIONS 
This project has met the set goal. For human body 
extraction, the proposed moving object segmentation algorithm 
automatically determines thresholds and eases the threshold 
design task. Performance of the proposed algorithm is shown to 
be superior to the zero-mean Gaussian and 4W  methods. The 
comparison table indicates that the used DFT+L-W feature 
extraction method shows better performance than the moment 
method and moment+ θ  method. A new recurrent fuzzy 
network  has been proposed. The recognition of human postures 
is based on the LRFNFS which shows high accuracy for this 
temporal posture recognition problem. 
 
REFERENCES 
[1] S. McKena, S. Jabri, Z. Duric, A. Rosenfeld, and H. Wechsler, “Tracking 
group of people,” Computer Vison and Image Understanding, vol. 80, pp. 
42-56, 2000. 
[2]A. Lipton, H. Fujiyoshi, and R. Patil, “Moving target classification and 
tracking from real-time video,” Proceedings of IEEE Workshop Applications 
of computer Vision, pp. 8-14, Oct. 1998. 
[3] D. Meyer, J. Denzler, and H. Niemann, “Model based extraction of 
articulated objects in image sequences for gait analysis,” Proceedings of 
IEEE International Conference on Image, pp. 78-81, 1988. 
[4] J. L. Barron, D. J. Fleet, and S. Beauchemin, “Performance of optical flow 
Techniques,” Proceedings of IEEE International Conference of Computer 
Vision, vol. 12, no. 1, pp. 42-77, Feb. 1994. 
