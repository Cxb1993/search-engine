1. INTRODUCTION  
Construction was conceived as an experience-based discipline (Ardery, 1991); knowledge 
acquired from previous works plays a key role for successful performance of new projects. Not 
only the construction know-how’s of contractors, but also the design capabilities of design firms 
and the management skills of CM consultants rely heavily on such knowledge. This has made 
construction an ideal industry for knowledge-based economy. In the past decade, researchers 
from construction engineering and management area have engaged in developing methods and 
applications of knowledge discovery in databases (KDD) and data mining (DM). Soibelman and 
Kim (2002) applied KDD process to identify the cause(s) of construction activity delays from the 
data of RMS (Resident Management System) provided by the U. S. Corps of Engineers. Caldas, 
and Soibelman (2003) developed a construction information classification system (CICS) based 
on automatic hierarchical classification of construction project documents according to project 
components. In their work, pattern classification algorithms are used to create the classification 
models including naïve Bayes, k-nearest neighbors, Rocchio, and support vector machines 
(SVM). Such system can be applied to classify large project document databases. Another work 
by Soibelman et al (2003) applied similar concept to acquire corporate lesson-learned (CLL) 
from experienced personnel. They developed a Design Review Checking System (DrChecks) to 
provide a framework for a standardized review process. 
In spite of some research works conducted by academic institutions, modern KDD or DM 
technologies were not yet widely applied by the industry in practical problems to acquire valuable 
knowledge from historic databases, which results in leaking of knowledge from construction 
firms. Previous works identified two causes for the lack of industrial applications: (1) the 
construction industry is not familiar with KDD and DM technologies (Yu and Skibniewski, 1999); 
(2) the existing KDD and DM technologies do not fit the special characteristics of complex 
construction databases.  
 
2. RESEARCH OBJECTIVE 
As a result, this research is intended for two objectives: (1) development of DM algorithms 
for mining of construction databases with complex characteristics such as data scarcity, data 
 Data Mining (DM) 
DM is the most critical step in the KDD process. Fayyad et al. (1996) considered DM as an 
interdisciplinary field with a general goal of predicting outcomes and uncovering relationships in 
data. Mitra et al. (2002) defined DM as a process using automated tools, which employs 
sophisticated algorithms, to discover hidden patterns, associations, anomalies and/or structure 
from large amounts of data stored in data warehouses or other information repositories. 
Furnkranz et al. (1997) addressed that DM tasks can be descriptive (i.e., discovering interesting 
patterns describing the data) and predictive (i.e., predicting the behavior of the model based on 
available data). DM involves fitting models to or determining patterns from observed data. The 
fitted models are viewed as inferred knowledge. Mitra et al. (2002) concluded that, a typical DM 
algorithm constitutes some combination of the following components:  
z Model—the function of the model (e.g., prediction, association) and its form (e.g., linear 
discriminates, ANN). A model contains parameters that are to be determined from the data;  
z Preference criterion—a basis for judgment of better model, depending on the given data;  
z Search algorithm—that is a specific algorithm for finding particular models and parameters, 
which significantly influences the mining results. 
 
SOFT COMPUTING FOR MINING CONSTRUCTION DATABASES 
Characteristics of Construction Databases 
Construction industry differentiates itself from other industries in the way of production, the 
environment of workspace, the format of products, and the constitution of organization. Such 
characteristics have contributed to unique problems confronting mining of construction data. The 
following point out three types of complexity existing in construction databases. 
Data Scarcity 
Yu and Liu (2006) defined two types of data scarcity are existing in construction databases: (1) 
scarcity in data volume—due to unique nature and huge scales of construction projects, it is very 
Soft Computing Techniques 
In traditional hard computing, achieving precision, certainty, and rigor in calculation are the 
primary goals. On the contrast, soft computing paradigm perceives that precision and certainty 
are costly so that computational schemes should exploit (wherever possible) the tolerance for 
imprecision, uncertainty, and approximate reasoning for obtaining low-cost solutions (Mitra and 
Hayashi, 2000). The above perspective is especially true for construction industry, where 
historical data are usually uncertain, incomplete, and scarce in their nature. Therefore, the flexible 
information processing capability of soft computing provides promising solution for DM in 
construction industry. That is, devise the DM algorithms that lead to an acceptable solution at low 
cost by seeking for an “approximate”, instead of “accurate” or “exact”, solution to an 
unstructured or ill-defined problem (Zadeh, 1965). 
Due to the unique characteristics of construction databases, the model and associated search 
algorithms adopted should be specialized to tackle the abovementioned problems. Among the 
many existing data mining algorithms, soft computing techniques (involving fuzzy sets, neural 
networks, genetic algorithms, and rough sets) are most widely applied to a KDD process (Mitra et 
al., 2002). The fuzzy sets (FSs) provide a natural framework for the process to deal with 
uncertainty (Pedrycz, 1998). Artificial Neural networks (ANNs) (Tickle et al., 1998) and rough 
sets (RSs) (Hirano, et al., 2002) are adopted for classification and rule generation. Genetic 
algorithms (GAs) are involved in various optimization and search processes, like query 
optimization and template selection. Other approaches like case-based reasoning (CBR) (Yang 
and Yau, 2000) and decision trees (Furnkranz et al., 1997) are also widely employed to solve data 
mining problems.  
 
4. METHODOLOGY—Proposed Hybrid Soft Computing System (HSCS) 
 
Framework 
Basic Network 
In this section, the knowledge representation model and the computational algorithms of the 
input attribute b and c, where the attribute a (with missing attribute value) and its associated 
fuzzy term nodes are deleted from the network. The signal propagation process of the degraded 
network follows the rules of original FALCON. The modified network is named 
Variable-attribute Fuzzy Adaptive Control Network (VaFALCON) (Yu and Lin, 2006). The 
proposed HSCS adopts VaFALCON and is able to process the incomplete data with any 
combination of missing attributes. 
 
Algorithms 
The computational algorithms of the proposed HSCS consist of three categories: (1) 
Self-organization; (2) Supervised learning; (3) Global search. Following sub-sections describe 
these steps. 
 
Self-organization 
The self-organized learning process is employed to determine the primitive fuzzy partitions, the 
membership functions of the input and output fuzzy terms, and the primary structure of the rule 
base for the HSCS. There are two learning stages in the self-organization: (1) determining the 
centers and spreads of the membership functions associated with the input and output nodes by 
Kohonen learning rule (Kohonen, 1988); (2) determining fuzzy logic rules by reinforcement 
competitive learning (Yu and Skibniewski, 1999).  
The Kohonen learning rule consists of two stages: 
Similarity matching stage:   { }x w Min x wik j n jk− = −≤ ≤$ $1        (1) 
In this stage, the most similar cluster for input data x is found to be the ith cluster by 
minimizing the difference between x and the center of the cluster ( $wik ), where superscript k 
represents the kth iteration and $w  means a normalized value of the cluster center (w). 
Updating stage:  ( )$ $ $w w x wik ik k ik+ = + −1 η , 
$ $w wjk jk= , for j = 1, 2,..., n   j ≠ i.       (2) 
The backward error propagation begins with calculation of the difference between the actual 
output of the forward pass and the desired output provided by the training example. The error 
function E and error signal δ5 at Layer 5 are defined as follows: 
E = 
1
2
(d(t) - o(t))2,              (6) 
δ5 = calculated output - desired output = d(t) - o(t),        (7) 
where t indicates the time sequence of the learning iteration. Since only a single output is 
considered in the proposed methodology, there is no summation in Equation (6). In each layer, 
the error signal is calculated and used for back propagation. That is, the error signal of Layer 4 
(δ4) is a function of δ5, and so forth. The parameter adjustment principle is based on the 
steepest gradient descent method, which can be described as follows: 
Δw E
w
= η ∂∂ , w t w t w( ) ( )+ = +1 Δ ,          (8) 
where w can be any parameter to be adapted, η is a proper constant. 
 
Global search 
Previous research has found that traditional FACLON may encounter severe local minimum 
problems while dealing with complex construction data (Yu and Skibniewski, 1999). In this 
report, the messy genetic algorithm (mGA) is adopted to variably construct the fuzzy rule base in 
the NFS in light of global search. The traditional simple genetic algorithm (sGA) is a 
non-deterministic search algorithm based on the ideas of genetics. While applying to NFSs, the 
problem arises with the encoding of the NFS parameters. In sGA, a coded chromosome is in fixed 
length that highly fitted allele (feature value of gene) combinations are formed to obtain a 
convergence towards global optima. Unfortunately the required linkage format (or the structure 
of the NFS to be coded) is not exactly known and the chance of obtaining such a linkage in a 
random generation of coded string is poor. Although inversion and reordering methods can be 
used to adaptively search tight gene ordering, these are too slow to be considered useful. Some 
researchers had turned to mGAs for constructing NFSs (Chowdhury and Li, 1997). The primary 
NFS with roughly determined centers and spreads of the fuzzy membership functions in both 
input and output layers of the VaFALCON. In the mean while, the fuzzy IF-THEN rules are 
also roughly linked between Layer 3 and Layer 4 in the VaFALCON. After Phase I, a 
preliminary fuzzy IF-THEN rule-based system is constructed and the knowledge mined from 
construction databases is stored in terms of fuzzy IF-THEN rules. 
(2) Phase II—Parameters and Structure Optimization Phase that optimizes NFS structure with a 
messy GA (mGA) and fine-tunes the membership functions of the NFS with error 
back-propagation method. In Phase II, the parameters of the membership function in the 
output and input terms and the consequent connections of fuzzy IF-THEN rules are 
“globally” searched by a messy-genetic algorithm (mGA) and then “locally” fine-tuned by the 
error back-propagation (BP). Learning process in Phase II is repeated until the error is below 
the error goal or a pre-determined number of learning iterations has been performed. Should 
there exist a local minimum so that the network error is too high to be accepted, the mGA 
adaptation is performed again to escape from the local minimum. The learning process is 
repeated until the error goal is satisfied. 
 
Integrated into Knowledge Discovery Process 
This section describes the complete process of knowledge discovery process for the 
proposed hybrid soft computing approach. 
 
Data preprocessing 
The first step in knowledge discovery process is the pre-treatment of data, since without 
quality data there cannot be quality mining results. The data stored in construction databases are 
usually dirty, which means incomplete, noisy, and inconsistent. There are several major tasks in 
data preprocessing including (Han and Kamber, 2000): (1) Data cleaning—smoothing noisy data, 
identifying or removing outliers, and resolving inconsistencies; (2) Data integration—integrating 
multiple databases, data cubes, or files; (3) Data transformation—performing normalization and 
aggregation of data; (4) Data reduction—obtaining reduced representation in volume but 
In this section, system validation methodology for the proposed HSCS is described. Three 
case-studies are conducted by applying the proposed HSCS. The objective of system validation is 
to validate the capability of HSCS in mining of scarce, incomplete, and uncertain construction 
databases. Therefore, measures of the three types of complexity are established: (1) scarcity ratio 
(Rs); (2) percentage of overall incompleteness (POI); and (3) degree of uncertainty (p%). The 
validation is based on system performance with respect to various degrees of complexity 
measures. Three construction data repositories were collected from published literature, including: 
(1) Building Construction Cost Estimation (Yu, 2001); (2) Estimation of Curtain wall 
Construction Duration (Yang, 1997); and (3) Retaining Wall Selection (Yau and Yang, 1998). It 
was found that the running-time of HSCS is case-sensitive. For a generic application such as the 
demonstrated examples, it takes about 30 min. to 2 hr. to converge. 
 
Case Background 
(1) Case I—Building Construction Cost Estimation 
Yu (2001) applied CBR technique to the conceptual cost estimation of building construction 
projects. In his research, nearly 30 parameters are originally collected for knowledge 
representation of CBR. After reviewing with experienced engineers, four most important 
parameters were identified: (1) type of earth retaining method; (2) No. of floors above ground; (3) 
No. of floors under ground; and (4) total floor area. One single output, construction cost 
estimation, is provided by the CBR system. Totally 25 data are collected from historical building 
construction project through surveying the final project reports provided by public owners. 22 
data sets are used for learning and the rest 3 data are used for testing. 
(2) Estimation of Curtain wall Construction Duration 
Yang (1997) developed a CBR system for duration estimation of curtain wall construction in his 
Ph.D. research. Totally 27 historical datasets were collected from major consultant firms of 
Taiwan. Among which, 24 are used for training and 3 are used for testing. The input attributes 
identified by Yang are: (1) excavation depth; (2) quantity of walls; (3) construction method; and 
(4) soil type.  
available training sets; and NVmodel is the number of variables in the HSCS model to be 
determined by training examples. The NVmodel can be roughly estimated by summing up all 
attributes in the network including the centers and spreads of the membership functions in 
input layer, the consequence connections of rule nodes, and the centers and spreads of the 
membership functions in output layer. 
out
in
fp
n
x
n
x
in
fpel nxVxVNV
inin
2)()(2
11
mod +Π+×= ==∑ ,         (11) 
where Vfpin is the vector of fuzzy partitions in the input layer; nin is the number of input 
attributes; the mathematic symbol Π represents the product calculation of elements in the 
vector; and nout is the number of output attributes. It is noted that the first and third terms in the 
right hand side are multiplied by a coefficient, 2, which counts for the number of the 
undetermined variables, centers and spreads of the membership functions. 
The higher the value of Rs means the severer the data scarcity problem. The experiment is then 
designed for testing the data of the three cases with various degrees of Rs to see its impact on the 
system accuracy defined in Equation (9). 
 
Measure of Data Incompleteness 
Yu and Lin (2006) defined a measure of data incompleteness as “percentage of overall 
incompleteness (POI)”, which measures the ratio of the total number of incomplete attributes 
over the number of all attribute values of all datasets. The POI is defined in the following 
equation. 
tsin
N
x
i
ma
Nn
N
POI
ts
×=
∑
=1 ,              (12) 
where Nmai is the number of missing attributes in the ith training set; nin and nout are as defined 
previously. 
Similar to problem of data scarcity, the higher value of POI means the severer data 
incompleteness problem. The experiment is then designed for testing the data of the three cases 
with various degrees of POI to view its impact on the system accuracy. 
and 20%. The testing results on incompleteness are shown in Table 3, Figure 5, and Figure 6. The 
information recovery ratio (IRR) defined by Yu and Lin (2006) measures the ratio of the accuracy 
with incomplete data over the accuracy with complete data. It is found from the testing results 
that both accuracy and IRR are relatively high (> 83%) when the POI is less than 20%. It is 
validated that HSCS can mine incomplete data as data incompleteness is not severe (e.g., POI < 
20%). 
 
Uncertainty 
The uncertainty of data is modeled by Equation (13), where the original data were disturbed 
with random number of various uncertainty ranges, p. In the experiment, the p was controlled at 
5%, 10%, 15%, and 20% for training sets. The testing sets were not disturbed. The testing results 
on uncertainty are shown in Table 4 Figure 7. The testing results show that the proposed HSCS is 
more sensitive to uncertainty than the other two attributes (scarcity and incompleteness). 
However, the capability of HSCS in mining of uncertain data is validated as long as the 
disturbance of data is not very severe (e.g., p% < 15%). 
 
Knowledge Presentation 
The knowledge mined by the proposed HSCS is stored in the fuzzy rule base that contains a set 
of fuzzy IF-THEN rules. Each fuzzy IF-THEN rule consists of a set of fuzzy linguistic terms for 
expressing the values of attributes in the precondition part; it also contains a set of fuzzy 
linguistic terms for the single output in the consequence part. Each fuzzy linguistic term is 
associated with a fuzzy membership function. The fuzzy partitions for the input parameters are 
subjectively determined by the decision maker. As a result, there are totally )(
1
xV infp
n
x
in
=Π  fuzzy 
IF-THEN rules for each case; where Vfpin is the vector of fuzzy partitions in the input layer as 
defined in Equation (11). Therefore, there are [3×2×2×3]=36 rules for Case I, [2×2×2×3]=24 
rules for Case II, and [2×2×2×3]=24 rules for Case III.  
The fuzzy membership functions of the linguistic terms for the input/output attributes of the 
three cases are shown in Figure 8 to 10, respectively. Each fuzzy decision rule is defined by 
classification. Clustering and outliers analysis is not provided in the current version. However, 
with proper design of pre-processing and post-processing functions, the abovementioned DM 
functionalities can also be implemented.  
 
Validation Methodology  
The validation methodology adopted in this research is based a simple index, accuracy, of 
classification function of the proposed HSCS. It should be noted that classification serves the 
fundamental functions for all other DM functionalities. Should the function of classification be 
validated, further extension of functionalities can be developed. Validation of classification 
accuracy also validates other functionalities such as association, concept description, etc.  
 
6. CONCLUSION AND RECOMMENDATION 
This report presents the research results of a hybrid soft computing approach, namely HSCS, 
which integrates FLDS, ANN, and mGA to form a new and powerful scheme for mining of 
construction data. The proposed approach combines several merits of soft computing techniques, 
such as human understandable fuzzy IF-THEN rules, learning ability of ANN, and global 
searching of mGA. Such hybridization offers desirable features for problems confronted in 
mining of complex construction data such as scarcity, incompleteness, and uncertainty. Three 
cases of real world construction data repositories were tested with HSCS for validation of its 
capability in discovering knowledge from scarce, incomplete, and uncertain databases. The 
testing results show that the proposed HSCS provides promising solution for data mining in 
construction. 
Three major contributions are concluded for the research including: (1) proposing a new 
data mining system that can tackle complex characteristics of construction databases such as data 
scarcity, data incompleteness, and uncertainty; (2) establishing measures for softness of DM 
method such as scarcity ratio (Rs), percentage of overall incompleteness (POI), and degree of 
uncertainty (p%); (3) the proposed HSCS does not only provide prediction function of traditional 
linear and nonlinear mapping schemes (such as linear regression model or ANN), but also 
Han, J. and Kamber, K. (2000). Data Mining: Concepts and Techniques, Morgan Kaufmann 
Publishers, San Diego, U.S.A. 
Hendrickson,C., Project Management for Construction--Fundamental Concepts for Owners, 
Engineers, Architects and Builders, Prentice Hall, New York, USA, 1998. 
Hirano, S., Tsumoto, S., Okuzaki, T., Hata, Y., and Tsumoto, K. (2002). “Analysis of Biochemical 
Data Aided by a Rough Sets-Based Clustering Technique.” International Journal of Fuzzy 
Systems, Vol. 4, No. 3, pp. 759-765. 
Jang, J. S. (1993). “ANFIS: Adaptive-network-based fuzzy inference system.” IEEE Trans. on 
Systems, Man, and Cybernetics, Vol. 23, No. 3, pp. 665-685. 
Kohonen, T. (1988). Self-organization and Associative Memory, Springer-Verlag, Berlin, German. 
Lin, C. T. and Lee, C. S. G. (1991). “Neural-network-based fuzzy logic control and decision 
system.” IEEE Transactions on Computers, Vol. 40, No. 12, pp. 1320-1336. 
Leu, S.-S.; Chen, C.-N.; and Chang, S.-L., “Data mining for tunnel support stability: neural 
network approach,” Automation in Construction, Vol. 10, No. 4, pp. 429-441, 2001. 
Mitra, S. and Hayashi, Y. (2000). “Neuro-fuzzy rule generation: Survey in soft computing 
framework.” IEEE Trans. Neural Networks, Vol. 11, pp. 748-768. 
Mitra, S., Pal, S. K., and Mitra, P. (2002). “Data mining in soft computing framework: A survey”, 
IEEE Transactions on Neural Networks, Vol. 13, No. 1, pp 3-14. 
Pedrycz, W. (1998). “Fuzzy set technology in knowledge discovery.” Fuzzy Sets and Systems, Vol. 
98, pp. 279–290. 
Soibelman, S. and Kim, H. (2002) “Data Preparation Process for Construction Knowledge 
Generation through Knowledge Discovery in Databases,” Journal of Computing in Civil 
Engineering, Vol. 16, No. 1, pp. 39-48. 
Soibelman, L., Liu, L. Y., Kirby, J. G., East, E. W., Caldas, C. H., and Lin, K. Y. (2003) “Design 
Review Checking System with Corporate Lessons Learned,” Journal of Construction 
Engineering and Management, Vol. 129, No. 5, pp. 474-484. 
Table 1 Scarcity ratio of the three cases 
Input Output Case 
No. of 
inputs 
Fuzzy 
partition 
Fuzzy 
partition 
No. of 
training sets 
No. of 
testing sets 
NVmodel Rs 
I 4 [3×2×2×3] [3] 22 3 62 2.818
II 4 [2×2×2×3] [3] 24 3 48 2.000
III 4 [2×2×2×3] [3] 21 6 48 2.286
 
Table 2 Testing results on scarcity  
Accuracy % Case 
CBR ANN (BP) ANFIS Hybrid CBR HSCS
I 85% 86.63% 67% 93.50% 90.97%
II 82.6% 81.11% 79.30% 95.37% 94.74%
III 68% 66.70% 66.70% 100% 100% 
 
Table 3 Testing results on incompleteness 
POI Case  
0% 5% 10% 15% 20% 
Acc.* 92.6% 90.7% 89.6% 86.5% 83.4%I 
IRR** 100% 98% 97% 93% 90% 
Acc. 95.9% 89.5% 85.1% 86.2% 83.0%II 
IRR 100% 93% 89% 88% 86% 
Acc. 100% 100% 100% 100% 83.3%III 
IRR 100% 100% 100% 100% 83.3%
*Acc.—Accuracy defined in Equation (9) 
** IRR—Information recovery ratio 
 
 Figure 2 Connections of FALCON for incomplete attribute values (Modified from Lin and Lee, 
1991) 
 
Start
Find prelim inary fuzzy 
membership func tions
by Kohonen learning rule
Find prelim inary fuzzy 
IF-THEN Rules
by competitive learning
Optim ize neuro-fuzzy
system struc ture
by messy GA
Fine-tune fuzzy 
membership func tions
by error BP
Error goal
satisfied?
End
Yes
No
Phase I
Pre lim ina ry
Struc turing
Phase II
Pa ra m eters
& Struc ture
Op tim iza tion
 
Figure 3 Integrated learning process of the proposed HSCS 
  
0.00%
20.00%
40.00%
60.00%
80.00%
100.00%
0% 5% 10% 15% 20%
p
Ac
cu
rac
y Case I
Case II
Case III
 
Figure 7 Testing results on accuracy vs. uncertainty ratio p 
 
 
Figure 8 Memberships functions of Input/Output attributes of Case I  
 
 
Figure 9 Memberships functions of Input/Output attributes of Case II  
 
可供推廣之研發成果資料表 
□ 可申請專利  ■ 可技術移轉                                      日期：95 年 10 月 19 日 
國科會補助計畫 
計畫名稱：混合式柔性計算系統於營建知識發掘之研究—第三年 
計畫主持人：余文德         
計畫編號：NSC 94-2211-E-216 -024  學門領域：土木(營建) 
技術/創作名稱 混合式柔性計算系統 
發明人/創作人 余文德 
中文： 
本研究發展出一套混合式柔性運算系統(HSCS)作為複雜型營建資
料庫知識探勘之用。所提出之 HSCS 混合了模糊邏輯、類神經網路、
混原基因演算法等柔性運算技術，構成了一個探勘能力極強之工
具。在三項柔性運算測試(資料不足、資料缺漏及不確定性)後發
現，HSCS 能夠有效地發掘複雜資料之隱含知識。本 HSCS 對於發展
營建企業智能等相關應用，應具有潛在之價值。 
技術說明 
英文： 
The research develops a hybrid soft computing system for mining of 
complex construction databases. The proposed approach hybridizes 
soft computing techniques, such as fuzzy logic, artificial neural 
network (ANN), and messy genetic algorithms (mGA), to form a novel 
computational method for mining of human understandable knowledge 
from historical databases. The hybridization combines merits of 
explicit knowledge representation of fuzzy logic decision-making 
system (FLDS), learning abilities of ANN, and global search of mGA. 
A Hybrid Soft Computing System (HSCS) is developed for mining 
complex databases in construction with three characteristics: scarcity, 
incompleteness, and uncertainty. Real world construction data 
repositories are selected to test the capabilities of the proposed HSCS 
for data-mining under the abovementioned complex conditions. The 
testing results show promising potential of the proposed HSCS for 
mining of complex databases in construction. 
可利用之產業 
及 
可開發之產品 
營建及其他複雜資料庫之探勘工具 
技術特點 
對於資料不足、資料缺漏及不確定性等三種特性交互影響之資料庫
具有強大之資料探勘能力。 
