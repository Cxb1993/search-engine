strategy, an enhanced caching architecture with a dirty cache tree design is proposed to manage and search 
data efficiently, where unmodified and modified data are cached in DRAM and NVRAM, respectively. A 
block-based management scheme with a two-phase selection policy is presented to manage the NVRAM 
space and to allocate free NVRAM space efficiently. The proposed strategy is evaluated by a series of 
experiments based on the traces generated by the well-known benchmark "Iometer" [] over a 64GB SSD 
simulated by a modified DiskSim simulator [,]. It is shown that the proposed strategy could significantly 
improve the performance of file systems over SSDs. For example, when the NVRAM size is 64MB, the 
performance improvement is up to 28% in most cases.  
 
研究目的： 
 
Figure 1. System Architecture. 
 
SSDs are usually considered as the secondary storage devices and accessed by the host through a standard 
I/O interface such as Serial Advanced Technology Attachment (SATA), as shown in Figure 1(a). It usually 
adopts the hardware multi-channel architecture to access multiple flash chips simultaneously so that its 
read and write performance could be significantly improved. In the host side, the file systems usually 
implement a cache system that are usually implemented in DRAM so as to enhance the read and write 
performance to the secondary storage. In a file system, it usually consists of metadata and userdata. The 
metadata are to maintain the general information of the file system such as system information, journaling 
information, and attributes of files/directories, while the userdata are user data, i.e., the content of files and 
directories. Due to the considerations of space allocation and caching performance, a file system usually 
partitions the logical block address (LBA) space into consecutive blocks, each of which is of the same size 
(e.g., 4096B) and is composed of the same number of consecutive LBAs. Thus, the cache system could 
cache both of the metadata and userdata in the unit of one block.  
The project is motivated by the needs in the performance and reliability enhancement for the file systems 
with the caching support when the underlying secondary storage devices are SSDs. Our goal is to equip 
both DRAM and NVRAM in the cache system with the cost considerations. The technical question is how 
to efficiently identify and manage dirty data of file systems with the considerations of the characteristics 
of SSDs and the limited size of NVRAM. Such observations motivate this research to propose an efficient 
caching strategy to improve the reliability of file systems and the read/write performance of the 
SSD-based storage systems when both DRAM and NVRAM are adopted in the cache system. 
 
 2
data. Each slot in the root node and internal nodes points to a node, and each slot in the leaf nodes points 
to a page, where a page is used to store the data of one block of file systems. Note that dirty(/clean) pages 
are the pages that cache dirty(/clean) data. Suppose that each node contains 2n slots, where n is 6 in Figure 
2. If the largest key value of the cached data is less than 2n, the entire tree can be represented with a single 
node. (Note that a tree with a single node is a tree of height 0.) When the largest key value of the cached 
data becomes between 2n and 22n−1, a new root node is created and the original root node becomes an 
internal node pointed by the first slot of the new root node. Therefore, if the largest key value of the 
cached data is between 2dn and 2(d+1)n−1, the height of the needed radix tree is d. In a radix tree of height d, 
the least significant (d+1)n bits of the key value are used in the lookups. Then each level of the tree can be 
used to index n bits of the key from the most significant bits of the (d+1)n bits, and the least significant n 
bits indicate the slot that points to the page storing the corresponding block.  
  
Figure 2: The (Special) Radix Tree Used in the Cache System of Linux. 
 
In the cache system of modern operating systems such as Linux, each block device or device partition 
usually has its own device cache tree to maintain the cached metadata of the file system on the device. 
Each accessed file usually has its own file cache tree (instead of using one file cache tree for all of the 
accessed files) to maintain the cached content/userdata of the file so as to enhance the lookup performance 
for the cached content/userdata of each file. As shown in Figure 3, the device cache tree usually uses the 
block number (in the device) as the key to manage the cached metadata, where the block number is the 
sequence number of a block in the device. Each file cache tree uses the block index in the corresponding 
file as the key to manage the cached userdata or content of the corresponding file, where the block index is 
the sequence number of a block within the file. Note that each block within a file is physically stored in a 
block of the device. In practice, the cache trees and cached data are usually stored in volatile DRAM, so 
that file systems could lose their sanity and data consistency. That is because the cached dirty data would 
be lost when system crashes occur.  
 4
consecutive NVRAM blocks, each of which is used to store one dirty page or v (=int(sb/sn)) nodes of the 
dirty cache tree, where sb is the NVRAM block size and sn is the node size of the dirty cache tree. Note that 
the first NVRAM block is reserved to maintain pointers that point to the root nodes of trees and the heads 
and tails of lists in NVRAM. For example, Figure 4 shows that the first pointer points to the first free 
NVRAM block, and each free NVRAM block points its next free NVRAM block to form the free 
NVRAM block list. The second pointer points to the root node of the dirty cache tree, and the root node 
points other nodes of the tree.  
  
Figure 4: The Layout of NVRAM. 
When a new dirty page is created, the first free NVRAM block in the free NVRAM block list is allocated 
to contain this dirty page. If data of a dirty page is flushed back to the storage device, this dirty page is 
removed from all of the cache trees that point to it, and its corresponding NVRAM block is put back to the 
free NVRAM block list. On the other hand, the block-based management scheme also maintains a free 
tree node list. When a new tree node is created and there is no free tree node in the free tree node list, a 
free NVRAM block is allocated. Then this free NVRAM block is partitioned into v free tree nodes that are 
put to the free tree node list, and the first free tree node in the free tree node list is allocated to store this 
new tree node. If a tree node is removed from the dirty cache tree due to the merging of two tree nodes or 
the deletion of the last page pointed by a leaf node, the removed tree node is put back to the free tree node 
list for the future (free) tree node allocations. If the number of free tree nodes in the free tree node list is 
larger than a predetermined threshold, the NVRAM blocks with some free tree nodes or the largest 
number of free tree nodes are reclaimed by moving the used tree nodes in this NVRAM block to other free 
tree nodes so as to reduce the number of free tree nodes and improve the NVRAM space utilization. This 
space reclaiming process is repeated until the number of free tree nodes is smaller than the predetermined 
threshold. Although this process might decrease the caching performance, we can reduce the frequency of 
this process with a proper threshold selection. Of course, this is a trade-off between the performance and 
space utilization.  
In practice, operating systems usually flush all of the dirty data back to the storage device during their 
shutdown, and this operation empties the dirty pages and the dirty cache tree in NVRAM. If the system 
crash or power loss occurs, the system would find that the dirty cache tree is not empty when the system is 
booted. Thus, all of the dirty pages cached in NVRAM are flushed back to the storage device (i.e., SSD) in 
the order of block number before any data of the file system are cached into the cache system, so that all 
of the data of the file system are preserved and synchronized. Then the system could boot up as if it was 
properly shut down last time. This might increase the time on booting the system, but it could minimize 
the modifications to the cache system and reduce the management complexity on the cache system. Note 
that the procedure to flush the dirty pages from NVRAM to SSDs is very efficient because SSDs have 
good performance on larger sequential writes and the dirty cache tree is very efficient in looking up dirty 
pages in the order of block number. In other words, with the dirty cache tree, large sequential writes are 
 6
In this experiment, the probability-based policy of the proposed strategy gave sequential data of no more 
than 4KB and no less than 256KB in the victim pool 5% and 100% to be selected, respectively. For other 
sizes of sequential data between 4KB and 256KB, their probabilities to be selected was derived by 
interpolating the probabilities of 5% and 100% linearly according to the length of the sequential data. In 
addition, each NVRAM block could store 10 tree nodes, and the maximal number of free tree nodes in 
NVRAM was set as 20. As shown in Table 1, a 64GB SSD was simulated by a modified version of the 
DiskSim simulator executed on the Linux operating system, where the modified DiskSim simulator could 
support the simulation for SSDs with hardware multichannel and multiple chips. The size of the adopted 
NVRAM was 64MB. The trace was collected through the benchmark "Iometer" over the ext2 file system 
(where each block of ext2 was 4KB), and the total amount of written data was 1GB, and 50% of the 
collected access traces was random requests. The Iometer is an I/O subsystem measurement and 
characterization tool for single and clustered systems, and is a well-known benchmark to evaluate the 
performance of storage systems on Linux. 
 
Table 1. Characteristics of the Simulated SSD. 
 
Figure 6(a) shows the performance of the proposed strategy under different average request sizes, where 
the x-axis denotes the average request size of the collected access pattern, and y-axis denotes the 
completion time. The proposed strategy had much better performance than the cache system without 
NVRAM as the average request size is smaller. That is because the proposed strategy tries to flush more 
sequential data when the NVRAM cache is full. For example, when the average request size is 16KB, the 
proposed strategy outperformed the cached system without NVRAM for more than 38.2%.  
 
Figure 6: Different Access Patterns. 
A random request means that a request doesn't access the blocks that are next to the blocks accessed by the 
previous request. As shown in Figure 6(b), the proposed strategy outperformed the cache system without 
the NVRAM support for no less than 15.2%. The completion time of the proposed strategy is increased 
significantly as the percentage of random requests increases, because it is harder to find longer sequential 
data in the victim pool when the percentage of random requests increases. 
 
 8
 10
[6]. A. O. Company. Yet Another Flash Filing S 
[7]. T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms, Second Edition. The MIT Press, 
2001. 
[8]. I. H. Doh, J. Choi, D. Lee, and S. H. Noh. Exploiting Non-Volatile RAM to Enhance Flash File System Performance. In 
The International Conference on Embedded Software (EMSOFT), September 2007. 
[9]. Eklektix Inc. Trees I: Radix Trees, http://lwn.net/Articles/175432/, March 2006. 
[10]. A. Gupta, Y. Kim, and B. Urgaonkar. DFTL: a Flash Translation Layer Employing Demand-Based Selective Caching of 
Page-Level Address Mappings. In the 14th International Conference on Architectural Support for Programming 
Languages and Operating Systems (ASPLOS), March 2009. 
[11]. Iometer. Iometer User's Guide, http://iometer.cvs.sourceforge.net/*checkout* /iometer/iometer/Docs/Iometer.pdf, 12 2003. 
[12]. S. Kang, S. Park, H. Jung, H. Shim, and J. Cha. Performance trade-offs in using nvram write buffer for flash 
memory-based storage devices. IEEE Transactions on Computers, 58(6):744-758, 2009. 
[13]. K. Kim and G.-H. Koh. Future memory Technology Including Emerging New Memories. In the International Conference 
Microelectronics, May 2004. 
[14]. S.-W. Lee, W.-K. Choi, and D.-J. Park. FAST: An Efficient Flash Translation Layer for Flash Memory. Lecture Notes in 
Computer Science (LNCS), 4096:879-887, 2006. 
[15]. J.-H. Lin, Y.-H. Chang, J.-W. Hsieh, T.-W. Kuo, and C.-C. Yang. A NOR Emulation Strategy over NAND Flash Memory. 
In the 13th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA), 
2007. 
[16]. C.-H. Wu and T.-W. Kuo. An Adaptive Two-level Management for the Flash Translation Layer in Embedded Systems. In 
the IEEE/ACM International Conference on Computer-Aided Design (ICCAD), pages 601-606, 2006. 
 
四、 計畫成果自評 
In this project, we focus on the hybrid storage systems to enhance the reliability and performance of the 
flash-based storage devices and file systems (including the caching systems) in embedded systems. We 
adopt the new non-volatile RAM (NVRAM, e.g., PRAM) as the media for the cache system with other 
storage media as a hybrid storage system to improve the reliability and performance of existing file 
systems. We propose and design an enhanced caching architecture to maintain the clean data in DRAM 
and the dirty data in NVRAM so as to maintain the sanity of file systems with limited amount of NVRAM. 
In other words, clean pages are allocated in DRAM and dirty pages are allocated in NVRAM. In order to 
manage the dirty pages efficiently, we design a dirty cache tree to maintain the dirty pages, and the dirty 
pages with the dirty cache tree are stored in NVRAM. In this way, the file system could access all of the 
cached data through the device cache tree or file cache trees. Meanwhile, the cache system could easily 
find the victim dirty pages to replace through the dirty cache tree when there is not enough NVRAM space 
to cache new dirty pages. Therefore, the cache system only needs to reclaim clean pages when there is not 
enough DRAM space, since all of the dirty pages are cached in NVRAM. We conduct a series of 
experiments based on the traces generated by representative benchmarks. We show that the proposed 
approach could significantly improve the performance of file systems over SSDs. For example, in most 
cases, when the NVRAM size is 64MB, the proposed strategy could reduce the completion time up to 
28.34%, compared to the cache system without the NVRAM support. This project is a premier research on 
the hybrid storage system design.  
In this research, the developed migration-based caching architecture provides an important feeling on how 
to utilize the feature of PCM. The research result achieves the major goal of this project, but the studies on 
other NVRAMs are still under investigations, especially the detailed impacts to the existing file systems 
and storage systems due to the special behaviors and characteristics of other NVRAMS are still 
unrevealed. We still need some efforts to figure out the details.  
 
 The International Conference on Embedded Software (EMSOFT) brought together 
researchers and developers from academia, industry, and government to advance the 
science, engineering, and technology in embedded software development. EMSOFT 
is one of the three flagship conferences of Embedded Systems Week, where 
Embedded Systems Week is an exciting event which brings together conferences, 
tutorials, and workshops centered on various aspects of embedded systems research 
and development. The combined program of the three conferences (CASES, 
CODES+ISSS, and EMSOFT ) offered three plenary keynotes and over 100 technical 
paper presentations. An industrial panel concluded the conference. ESWEEK 2010 
also offered a number of half-day tutorials that survey hot topics of general interest to 
the embedded systems community. Additionally, the event hosted eight workshops on 
a wide range of embedded systems topics that allow researchers and practitioners to 
share and discuss. 
 
This year EMSOFT was held in Scottsdale, which is situated in the greater Phoenix 
area which provides a perfect gateway to the natural beauty of the Grand Canyon state, 
Arizona. In reply to took United Airline to Los Angeles, and transferred to Phoenix. It 
took only 20 minutes from Phoenix to Scottsdale by driving a car. The venue was the 
Westin - Kierland Resort & Spa hotel, where all additional events were taken place.  
 
一、參加會議經過 
In this conference, I attended three keynote speeches. The first one was “Embedded 
Market: Challenges and Opportunities,” and the speaker was Vida Ilderem, Vice 
President, Intel Labs, Director, Integrated Platform Research Lab. Vida Ilderem is 
vice president of Intel Labs and director of the Integrated Platform Research Lab for 
Intel Corporation.  This speech focused on the deep integration of highly integrated 
platform-on-chip architectures as well as digital, analog and physical design factors. 
There is a convergence trend in the computing, communication and consumer markets 
and with a forecast of an additional 1 billion connected computing users by 2015, it is 
of high value to provide a common experience between the devices. Intel’s vision of 
Compute Continuum enables the users to realize the potential of a seamless 
cross-device experience with more consistency and accessibility to their information. 
The convergence trend and the Compute Continuum make System-on-Chip (SoC) a 
 2
Programs in Automotive Embedded Systems.” In this work, it focused on the 
advanced features of next generation vehicles, the real-time programs in automotive 
embedded systems. For such large volume program codes, this paper proposes a novel 
framework to use high-density and low-cost nonvolatile memory, i.e., NAND flash 
memory, as a low-cost mean of storing and executing hard real-time programs. 
Regarding this, one challenge is that NAND flash memory allows only 2KB 
page-based read operations not per-byte random access, which requires RAM as 
working storage for code executions. In order to minimize the expensive RAM 
requirements, the proposed framework optimally partitions the RAM for multiple 
hard real-time tasks and optimally determines the pinning/LRU combination for each 
RAM partition such that all task deadlines are deterministically guaranteed. The 
proposed framework was verified with the actual real-time programs for unmanned 
autonomous driving. The other work presented in this session was “Janus-FTL: 
Finding Optimal Point on the Spectrum between Page and Block Mapping Schemes”. 
In this work, it focused on NAND flash memory based storages such as SSDs, 
because they are gaining popularity in commodity computer systems. Some low-end 
SSDs use the block mapping FTL (Flash Translation Layer) that is good for sequential 
write patterns but poor for random ones. On the other hand, high-end SSDs tend to 
use the page mapping FTL that is effective for random write patterns, but whose 
performance degrades after successive random writes. Designing an FTL that adapts 
to various workload patterns and provides long-term stable performance is a 
challenging issue. To resolve this issue, they proposed a new FTL called Janus-FTL, 
which provides a spectrum between the block and page mapping schemes. By 
adapting along the spectrum, Janus-FTL could provide long-term superior write 
performance for various workload patterns. We also present a cost model of 
Janus-FTL that shows the existence of the optimal point on the spectrum for a given 
workload.  
 
二、與會心得 
I met a lot of excellent researchers from Korea. They also have done many excellent 
works in embedded software including the researches on flash memory. This is 
because they are highly supported by Samsung, the biggest flash memory 
manufacturer in the world. In this conference, I met a lot of researchers who are doing 
excellent works on embedded systems and embedded software from all over the world, 
 4
 
Yuan-Hao (Johnson) Chang  
寄件者: <luca@cs.columbia.edu>
收件者: <d93944006@csie.ntu.edu.tw>
副本: <luca@cs.columbia.edu>; <stavros.tripakis@gmail.com>
傳送日期: 2010年6月29日 上午 08:27
主旨: Your EMSOFT 2010 Submission (Number 24)
b頁 1 - 9(B)
2011/10/13
Dear Prof. Yuan-Hao Chang: 
 
On behalf of the EMSOFT 2010 Program Committee, we are delighted  
to inform you that the following submission has been accepted  
to appear at the conference: 
 
     A Reliable MTD Design for MLC Flash-Memory Storage 
           Systems 
 
Your paper, up to 10 pages long, will be published in the conference proceedings. Your paper 
will be assigned a 30 minutes time slot, 25  
minutes for presentation and 5 minutes for questions. 
 
This year we received 89 submissions, of which 29 have been accepted. 
 
The Program Committee worked very hard to thoroughly review 
all the submitted papers.  Please repay their efforts, by  
following their suggestions when you revise your paper. 
The reviews and comments are attached below. 
 
You will soon be contacted by ACM/Sheridan Press with the details of preparing and uploading 
the camera-ready version of your paper.  
The tentative deadline for submitting the camera-ready version is  
Sunday, August 10, 2010. 
 
Congratulations on your fine work.  If you have any additional  
questions, please feel free to get in touch. 
 
 
Best Regards, 
Luca Carloni and Stavros Tripakis  
EMSOFT 2010  
 
======================================================================
 
EMSOFT 2010 Reviews for Submission #24 
======================================================================
 
 
Title: A Reliable MTD Design for MLC Flash-Memory Storage Systems 
 
Authors: Yuan-Hao Chang and Tei-Wei Kuo 
======================================================================
                            REVIEWER #1 
======================================================================
 
 
 
--------------------------------------------------------------------------- 
Reviewer's Scores 
--------------------------------------------------------------------------- 
 
introduced because updates of a data page and the corresponding ECC page are 
only loosely coupled. Moreover, ECC caching for performance enhancements 
introduces more possibilities for inconsistencies between data and the 
corresponding ECC. There should be some mention about the techniques to address 
such inconsistencies. 
 
3.          Reliability analysis 
-         Analysis formula: The formulas used for reliability analysis are based 
on RAID5/RAID6, whereas the proposed mirroring scheme is much like to RAID1. 
These similarities need to be mentioned in the paper. 
 
4.          Performance evaluation 
-         Experimental setup: Simulation environments (in particular fault 
modeling) need to be specified.. 
 
-         Performance: Figure 12 shows that the response times of block-level 
FTLs are much worse than those of page-level FTLs. Although performance 
comparison between page-level FTL and block-level FTL is not the main point of 
the experiments, the authors need to explain why there is such a large 
performance gap. 
 
5.          Typo 
-         Page 5, right column, line 11: head_brt --> head_bbs 
-         Page 5, right column, line 27: head_bbs --> head_brt 
-         Page 6, in Figure 9: ptr_emb --> ptr_emt 
-         Page 6, line 9 in algorithm1: block --> block_rb 
-         Page 7, left column, line 3 from the end: stipes --> stripes 
-         Page 8, right column, line 3: 0.1 --> 0.01 
 
============================================================================
                            REVIEWER #2 
============================================================================ 
 
 
--------------------------------------------------------------------------- 
Reviewer's Scores 
--------------------------------------------------------------------------- 
 
                     Relevance to EMSOFT: 3 
                Presentation/Readability: 2 
                             Originality: 3 
                     Technical Soundness: 2 
                  Overall Recommendation: 3 
                   Reviewer's Confidence: 3 
 
 
--------------------------------------------------------------------------- 
Comments 
--------------------------------------------------------------------------- 
 
Dear authors,  
 
The proposed paper introduces a flash-memory storage system at the device 
layer, addressing the issue of data redundancy and enhanced error correction, 
in order to guarantee a reliable operation during flash reads and writes. The 
proposed approach is interesting, as long as reliability is an emerging topic 
and can be addressed at any layer of design abstraction and considering either 
software, architectural or circuit-level techniques, so that the system's
b頁 3 - 9(B)
2011/10/13
which includes an initial overview and continues with the mirroring approach, 
which is the "key" point of the system's reliable operation (data redundancy 
through mirrored areas).  
 
6) The large number of references, which cover not only the state-of-the-art in 
the field of flash memory design, from the aspect of the flash memory cell, the 
package and the system, including the address translation layer (Flash 
translation layer, FTL). Also, several relevant references addressing technical 
papers of specific flash memory designs are included. 
 
Weak points of the paper: 
 
1)The main drawback of the proposed paper is that the authos attempt to present 
a flash memory management system supposed to guarantee a reliable system's 
operation, but without presenting how a system including a flash could take 
advantage of the proposed techniqes applied (mirroring, enhanced ECC) to boost 
its lifetime. 
The only example the authors give is at the description of the bad block 
manager's structure, regarding the block remapping tables for a specific bad 
block. In case of a semi-bad block, the only solution given in order to 
accelerate the system's performance while booting (or hibernating) is the 
storage of the good block to the main memory. However, the authors do not 
further elaborate on such special cases. Moreover, they do not demonstrate any 
results regarding system's performance metrics (e.g. number of cycles), 
considering valid and not valid data while executing programs from the flash 
(which is a typical case when embedded systems with one or more processors are 
in the boot phase).  
 
2) Another critical issue that bowls under the overall presentation's quality 
deals with a systematic design flow that could illustrate the proposed flash 
system's functionality. In perspective, the system is composed by the Operation 
Manager, which is assisted by the Bad Block and ECC managers. However, the 
various functions that each of these managers encapsulate are presented in a 
way separately the one from each other in the manuscript. Hence, in a real 
application, where the system boots and needs to operate with the flash (and 
with the main memory also), there is no detailed information about the series 
of operations that the Operation manager, the ECC and the Bad Block manager 
could make, to guarantee the reliable system's operation. Possibly, a flow 
chart could be the solution, considering either the boot or the hibernate phase 
of the system.  
 
3) Regarding the results, the authors consider only their own flash memory 
system, which takes as input the read/write operation and the translation of 
the logical to physical memory address from the flash translation layer. 
However, there is no comparison with other architectural approaches working at 
the same layer to the one of the proposed system. Hence, the authors do not 
include any comparison with relevant works in the literature, although some of 
them could be relevant in a way (like reference [13] in the manuscript).  
 
4) Regarding the experiments of Access Performance (Section 5.2.2) and 
especially the bar charts of Figure 12, the Average Response Time results are 
rather contradicting. In specific, regarding the Average Response Time vs the 
number of chips in the flash and considering ASA as the access policy, the 
average response time is larger in the case of having two chips, comparing to 
the cases of four, six and eight. However, the authors do not explain whether 
the increased parallelism reduces the access time, either with or without the 
use of the proposed flash system, RTMD.  
Moreover, the results of the same bar chart show that the Average Response Time 
b頁 5 - 9(B)
2011/10/13
(BL, NTFL and FTL) do not exploit the parallelism, but this clue does not 
comprise a clear answer of excluding them from the average response time 
experiments.   
 
10) In the experiments again, the authors claim at Sections 4.1 and 4.2 that 
the 
probability metrics are derived considering a RAID5/RAID6 flash system. 
However, in the beggining, it is noted in the manuscript that the proposed 
mirror-based approach performs better than the RAID5/RAID6 in 
reads-before-writes and in general could degrade the performance of flash 
memories. However, in Sections 4.1 and 4.2, it seems that the probabilities 
presented are based on such a RAID5/RAID6 system. Also, the authors do not 
address whether such a system implements data mirroring also. Therefore, if it 
does not, then there is a contradiction of Section 4 with what the authors 
propose as a flash system.  
 
In general, based on the aforementioned remarks, the proposed work could be 
characterized as of an adequate quality, but with several points that should be 
revised and rewritten in a different way and more analytically, so that the 
total contribution could be over the borderline. Hence, based on these 
comments, the proposed recommendation for the work's overall quality is 
borderline. It is also noted that several issues regarding the experiments 
should be resolved, as it was mentioned above, while the applications based on 
which the proposed redundancy-based approach is evaluated are rather poor and 
they are also not defined in the experimental setup's part. 
 
Kind Regards 
 
============================================================================
                            REVIEWER #3 
============================================================================ 
 
 
--------------------------------------------------------------------------- 
Reviewer's Scores 
--------------------------------------------------------------------------- 
 
                     Relevance to EMSOFT: 3 
                Presentation/Readability: 4 
                             Originality: 3 
                     Technical Soundness: 3 
                  Overall Recommendation: 4 
                   Reviewer's Confidence: 1 
 
 
--------------------------------------------------------------------------- 
Comments 
--------------------------------------------------------------------------- 
 
Reasonable paper with some interesting ideas on improving multi-chip flash 
memories. 
Experiments show improvements in the metrics of interest from teh proposed 
architecture. 
 
============================================================================
                            REVIEWER #4 
============================================================================ 
 
b頁 7 - 9(B)
2011/10/13
in the real world. 
 
Doing OS work with pen and paper only is insufficient, because many unexpected side effects can surface 
during practical experiments. 
b頁 9 - 9(B)
2011/10/13
As ﬂash memory gains its market share in various storage-
system applications, cost and capacity have become major
driving forces in the development of the ﬂash memory tech-
nology. Because of that, serious challenges are faced for re-
liability considerations, especially when the number of bits
in a ﬂash-memory cell and the number of chips in a stor-
age system increase. In that direction, researchers exploited
methodologies to distribute erases evenly over blocks so as
to improve the endurance of ﬂash blocks [5, 10, 7]. However,
little work is done in the considerations of the support of er-
ror correction for low-cost ﬂash memory. In particular, some
researchers have proposed methodologies on how to add the
parity check or additional error correction codes to improve
the reliability of MLC-based ﬂash-memory storage systems,
e.g., [11]. However, most research work in this area either
proposed a new FTL design or tried to revise existing FTL
designs, and little work considered burst errors, page/block
read-write failures, and chip failures.
This work is motivated by the needs of reliability enhance-
ment for multi-chipped ﬂash-memory storage systems so as
to extend their endurance. We are interested in low-cost
ﬂash-memory devices that have limited hardware resources
(such as RAM and computing power) and new write con-
straints imposed by MLC ﬂash memory (see Section 2). In
particular, a reliable MTD design is proposed to resolve the
reliability problems of many future MLC-based ﬂash mem-
ory products without any (or with very limited) modiﬁca-
tions to existing FTL designs. We propose a reliability-
enhancing system architecture to support correcting both
random errors and burst errors. A log-based write strategy
with a hash-based caching policy is presented for enhanced
ECC redundancy and performance improvement. The pro-
posed reliable MTD design was evaluated by a series of ex-
periments based on realistic traces. It was shown that the
proposed approach could signiﬁcantly improve the reliability
of ﬂash memory with very limited system overheads.
The rest of this paper is organized as follows: Section 2
presents the system architecture and the motivation of this
work. Section 3 proposes a reliable MTD design. Section
4 summarizes the experiment results on the reliability en-
hancement. Section 5 is the conclusion.
2. SYSTEM ARCHITECTURE AND
RESEARCH MOTIVATION
2.1 System Architecture
In SLC ﬂash memory, each block might consist of 64 pages,
and each page can store 2KB data and 64B spare informa-
tion [32]. In comparison, each block of MLC×2 ﬂash memory
might have 128 pages, each of which can store 2KB(/4KB)
data and 64B(/128B) spare information [34, 36]. The unit
price of MLC×2 ﬂash memory is less than one-third of that
of SLC ﬂash memory, so that MLC ﬂash memory is widely
adopted in ﬂash-based storage devices, especially low-cost
ones [16]. However, MLC ﬂash memory has lower perfor-
mance, lower endurance, and higher error rate than SLC
ﬂash memory. In addition, there are two additional new
write constraints in MLC ﬂash memory. That is, pages of
MLC ﬂash memory can only be written sequentially in a
block, and the partial programming/write in a page is now
prohibited.
Figure 1 shows a typical system architecture of ﬂash-memory
Multi-Chipped Memory Technology Device Layer
File Systems (e.g., FAT, NTFS, EXT3)
Flash Translation Layer
Garbage Collection
Wear Leveling
Address Translation
(Chip / Block Assignment)
(LBA, # of LBAs)
Host
Device
(e.g., SSD)
(chip, block, page)
or
(chip, subchip, 
block, page)
(cmd, addr)
NAND
Flash
Memory
MTD
driver
FTL
driver
Chip
die
plane
plane
plane
plane
die
Chip
die
plane
plane
plane
plane
die
Chip
die
plane
plane
plane
plane
die
Chip
die
plane
plane
plane
plane
die
Interface (e.g., SATA or USB)
...
Figure 1: System architecture.
storage systems: A ﬂash storage device is usually composed
of multiple chips and connected to a host system through a
standard interface. A Memory Technology Device (MTD)
driver (that controls ﬂash chips) provides primitive functions
(e.g., read, write, and erase) over ﬂash memory. A multi-
chipped MTD driver does not block and wait for the com-
pletion of any read, write, or erase operation because multi-
ple chips could be accessed in parallel. Meanwhile, a Flash
Translation Layer (FTL) driver is used to provide address
translation, garbage collection, and wear leveling for ﬂash-
memory management. The address translation is to trans-
late any given logical block addresses (LBAs) to their corre-
sponding physical block addresses (PBAs) on ﬂash memory
because the overwriting of data on a page must be done to
some free page, and the original page becomes invalid. Note
that an LBA is the address of a storage unit, e.g., a sector
with a size equal to 512B. In a multi-chipped ﬂash-memory
storage system, each PBA has four parts (chip, subchip,
block, page), where chip and subchip denote its chip number
and subchip number, respectively. The garbage collection is
to reclaim space that stores invalid data whenever there are
not enough free pages. The garbage collection should be
handled carefully because erase operations are slow, and the
overhead on live-page copyings is relatively high, where a
live-page copying is to copy valid data of live pages in a
to-be-erased block to other free pages. The wear leveling
is an optional component that distributes block erases as
evenly as possible over ﬂash memory so as to prevent some
blocks from being worn out excessively, since a worn-out
block (also referred to as a bad block) could suﬀer form fre-
quent read/write errors.
2.2 Research Motivation
As the density and capacity of ﬂash memory chips keep in-
creasing, the reliability of ﬂash memory becomes even worse.
The bit error rate of MLC×2 ﬂash memory is around 1000
times of that of SLC ﬂash memory. For the upcoming MLC×3
ﬂash memory, i.e., triple-level-cell (TLC) ﬂash memory, the
reliability problem could be further exaggerated, even though
their cost is much lower than the cost of MLC×2 and SLC
ﬂash memory [41]. This reliability problem is usually re-
solved by the adopting of ECCs to correct error bits, even
though the time on error correction usually grows rapidly
with the number of error bits and/or redundant bits of an
encoding unit [29]. A page is considered as an error page or
180
to another page of the same plane without any data trans-
mission between the ﬂash chip and the main-memory buﬀer
so as to enhance the performance on (live) page copyings.
7672
0
2
4
...
Spare-block Area
(260 Spare Blocks 
Per Plane)
1 Page  = 2KB
1 Block = 128 Pages
1 Plane =  4096 Blocks = 1GBSystem Block
6
7670
7672
7674
...
7676
1
3
5
...
7
9
7671
7673
7675
...
8
Subchip 0 (Die 0)
0
2
4
...
6
7670
7672
7674
...
7676
1
3
5
...
7
9
7671
7673
7675
...
7677
8
Subchip 1 (Die 1)
Chip 0
0
2
4
...
8
7670
7674
...
7677
1
3
5
...
7
9
7673
7675
...
7677
6
Subchip 0 (Die 0)
0
54
...
6
7670
7672
...
7676
1
32
...
7
9
7671
7673
...
7677
8
Subchip 1 (Die 1)
Chip 1
Data-block Area
(3584 Data Blocks
Per Plane)
Direction of spare block
allocation for bad blocks
Direction of spare block
allocation for block
remapping tables
7677
7674 7675
1 Sub-chip = 2 Planes
1 Chip = 2 Sub-chips (Dies) Data Block /
Spare Block Bad Block
Block Set 2
Block Set 4
Block Set 3836
Block Set 3838
7671
Block Set 3837
Mirror
8186 8187 8186 8187 8186 8187 8186 8187
8188 8189 8188 8189 8188 8189 8188 8189
8190 8191 8190 8191 8190 8191 8190 8191
Plane 0 Plane 1 Plane 0 Plane 1 Plane 0 Plane 1 Plane 0 Plane 1
headbrt
headbbs
... ... ... ... ... ... ... ...
7168 7169 7168 7169 7168 7168 71697169
7166 7167 7166 7167 7166 7166 71677167
ECC-block Area
(252 ECC Blocks
Per Plane)
Figure 3: Flash layout.
If a data block or an ECC block is bad, this block and
its corresponding block of the same block set (referred to as
a bad block set) are replaced with the a spare block set in
the same plane set with the consideration of the two-plane
operation. Here the two-plane operation is usually supported
by ﬂash-memory chips to access blocks of the same block set
simultaneously so as to enhance the degree of parallelism
[28, 34]. The spare block sets to replace bad block sets
are allocated according to their increasing physical order on
chips, and only the spare block sets that don’t have any bad
block can be allocated to replace bad block sets. As shown
in Figure 3, in Subchip 1 of Chip 1, the (spare) block sets
3836 and 3838 are used to replace the (bad) block sets 2
and 4, respectively. Note that the maximal number of spare
block sets that should be reserved to replace bad block sets
can be predicted because most of the ﬂash devices guarantee
the minimal number of good (or available) blocks during the
endurance life of the devices [27, 34].
In a multi-chipped ﬂash-memory storage system, some
blocks of the data-block areas could be conﬁgured as mir-
rored areas to store critical data, so that critical data could
be mirrored and recovered from burst errors1. In this work,
we consider the segment-based mirroring, as shown in Figure
4. The segment-based mirroring partitions ﬂash chips into
equal-sized segments, and uses a segment of a chip to backup
a segment that is with the same oﬀset to another chip so as
to enhance the reliability of some ﬂash area but without sac-
riﬁcing too much parallelism level. However, segment-based
mirroring can not recover every lost data due to chip failures,
even though other burst errors can still be corrected. This
problem could be resolved by conﬁguring the whole data-
block area as mirrored and mirroring areas, but the capacity
of the storage system could be signiﬁcantly reduced. This is
a tradeoﬀ between reliability and capacity.
1The mirroring mechanism is similar to the Redundant Ar-
ray of Independent Disk 1 (RAID 1). Although RAID 5 (or
6) could reduce space overheads, the write-once property of
ﬂash memory and the read-before-write overheads (to up-
date parity checks) of most RAID types might seriously in-
crease the management complexity and degrade the write
performance of ﬂash-memory storage systems, especially for
the MLC-based ones [17].
ECC-block
Area
Data-block
Area
Chip 0 Chip 1 Chip 2 Chip 3
Mirrored area Mirrorring area
Non-mirrored area
Mirrored
MTD View
(Real Flash Chips)
Chip 0 Chip 1 Chip 2
FTL View
(Virtual Flash Chips)
bd blocks
(bd - br) blocks
Mirrored area for critical data
Non-mirrored area for
non-critical data
Chip 3
Mirrored
MirroredMirrored
br blocks
Spare-block
Area
bs blocks
be blocks
Figure 4: Segment-based mirroring.
3.3 A Log-Based Write Strategy with ECC En-
hancement
Data-block
Area
Plane 0 Plane 1
ECC-block
Area
Plane 0 Plane 1
bd block sets
(nr regions) ... ...
... ...
b'd block sets
be block sets
(nr regions)
b'e block sets
...
E.g., bd = 3584 ( block sets 0-3583), be = 252 (block sets 3584-3835)
b'd = 128, b'e = 9, nr = 28
Data
region 0 ECC
region 0
Data
region nr-1
Ecc
region nr-1
Figure 5: Mappings between data block sets and
ECC block sets.
In order to reduce the management overheads of the en-
hanced ECC redundancy, a log-based write strategy is pro-
posed for the enhanced ECC manager. As shown in Figure
5, the strategy partitions the data block sets and ECC block
sets of a plane set into the same number of data regions and
ECC regions, respectively, where a region is composed of
consecutive block sets. There is a one-to-one mapping re-
lationship between data regions and ECC regions, and the
enhanced ECC redundancy of the data stored in a data re-
gion is saved and partitioned into a ﬁxed number of ECC
groups in its corresponding ECC region (when the adopted
ECC and its capability are determined). The enhanced ECC
redundancy of a data block is partitioned into a ﬁxed num-
ber (i.e., g) of ECC groups. Each ECC group contains the
enhanced ECC redundancy of consecutive p′ (= p
g
) pages
of the data block, and each ECC group should be able to
ﬁt into an ECC page, where p is the number of pages of a
block and g is the number of ECC groups of a data block.
As shown in Figure 6, pages of an ECC block, except the
last page, are used to store ECC groups (referred to as ECC
pages), and the last page of an ECC block is used to store the
ECC mapping table and its corresponding counters (referred
to as an ECC mapping page). Here the ECC mapping table
stores the mapping information from ECC groups to their
corresponding ECC pages, and the counters are to record
182
Spare Block
Bad Block
Plane 0 Plane 1
0 ...1 127
Page
Block
0 ...1 127
0 ...1 127
0 ...1 127
0 ...1 127
0 ...1 127
0 ...1 127 0 ...1 127
seg 0 seg 1 ... seg (m-1)
segsysnum
Data area Spare Area
seg m seg (m+1) ... seg (2m-1) ...
... ...
ri 0 ri 1 ... ri (r-1) ...
......
Data area Spare Area
ri r ri (r+1) ... ri (2r-1) ...
targetf1f0
...
Block Set
4095
4094
4093
4092
rootbrt
The block set pointed by segsys stores system parameters,
e.g., headbrt and headbbs
0 ...1 127 0 ...1 127 4091
id
      seg 0 : ri 4
11
seg 0 : ri 3
-100
      seg 0 : ri 2
0 (=3836)01 ...2 (=3838)
  seg 0 : ri 1
-100
Example:
id
duplicated
      seg 0 : ri 0
-100
Root segment
System segment
Remapping segment 0
Remapping segment 1
Bad block set
The number of 
remapping segments
Figure 7: A block remapping table.
set is then loaded to the cache. If the cache is full, the space
for the least-recently-used remapping information is simply
replaced. Note that the good block in a semi-bad block set
could be used to store the (system) data in the main memory
so as to enhance the system performance, such as that for
hibernation or booting.
If a bad block in the data-block/ECC-block area occurs,
the headbbs moves forward to ﬁnd a good spare block set to
replace the block set of the bad block, and the remapping
information of the bad block set is updated accordingly. In
order to prevent the wearing-out of the block set of the root
segment excessively, all of the remapping segments adopt
an in-place-update strategy whenever a new bad block is
developed at run time. Here an in-place-update strategy is
to erase the block and then rewrite data back to the original
block for each data update. Note that the probability in
developing new bad blocks for such an approach at run time
is comparatively low [27]. In order to improve the reliability
of the remapping information, a replaced bad block can be
ﬁlled up with pointers, all of which point to the spare block
set that replaces the bad block. As a result, if any remapping
segment crashes, then the lost remapping information can be
eﬃciently reconstructed by scanning the block sets that lose
their remapping information.
Whenever a spare block that stores a segment of the block
remapping table becomes bad, headbrt moves backward to
ﬁnd a good spare block set to replace the residing block set of
this newly developed bad block. If the to-be-replaced block
set is used to store a system segment or a remapping seg-
ment, the corresponding segment pointer in the root segment
should be updated and point to the newly allocated good
spare block set as well. In a multi-chipped ﬂash-memory
storage system, the root segment size of each plane set is
usually smaller than the size of one page2. In such a case,
the segment pointers of a root segment are initially stored in
the ﬁrst page set of the block set of the root segment. Any
modiﬁcation to a segment pointer of the root segment only
incurs a write operation to the next free page of the block
set, and block erases only occur whenever the block set is
full. As a result, the number of erases to the block set of
2Each segment pointer in the root segment and each remap-
ping information item in the remapping segment are usually
of at most four bytes, so that each page of the block of the
root segment maintain the remapping information of at least
225 block sets (=16TB) in a plane set.
the root segment could be signiﬁcantly reduced. The erase
operation should be resilient to accidental power failures or
system crashes. We propose to erase any block that stores
the block remapping table with a two-step operation. That
is, we should erase and update data to one block (for infor-
mation updating) and then to the other block of the same
block set (for information duplication). The root segment
could be found during the system initialization by scanning
ﬂash memory from the last block set in a backward fashion.
It could also be pointed by rootbrt (Please refer to Figure
7), and rootbrt can be stored in the system block (i.e., the
physical block 0 of each ﬂash-memory chip) and loaded into
the main memory during the system start-up. Note that a
system block is usually guaranteed being valid for 1000 erase
cycles and always used to store system data.
4. PERFORMANCE EVALUATION
4.1 Performance Metrics and Experimental
Setup
The purpose of this section is to evaluate the capabil-
ity of the proposed reliable MTD design (referred to as
RMTD) with existing popular FTL designs, in terms of en-
durance and access performance. The endurance evaluation
was based on the lifetime, where the lifetime is the ﬁrst time
that an error can not be corrected. The access performance
of the strategy was evaluated based on the average response
time per read/write request.
The proposed reliable MTD design was evaluated when
it worked with NFTL, FTL, and ASA management strate-
gies. NFTL adopts a block-level mechanism, where each
LBA is divided into a virtual block and a block oﬀset (i.e.,
page number). The residing virtual block of the newly writ-
ten data is mapped to a physical block (referred to as the
primary block). However, any updating to existing data is
sequentially written to another block (referred to as the re-
placement block). When the replacement block is full, the
replacement block and its corresponding primary block are
merged into a new primary block. FTL adopts a page-level
address translation mechanism to map any given LBA to its
corresponding ﬂash page. ASA adopts a page-level transla-
tion mechanism with an adaptive striping architecture. It
scatters write requests over chips simultaneously, and writes
hot data to blocks that are diﬀerent from blocks speciﬁcally
for cold data (i.e., hot-cold separation). In this experiment,
NFTL, FTL, and ASA adopted the same greedy policy for
garbage collector: That is, the erasing of a block with each
invalid(/valid) page resulted in one unit of recycling bene-
ﬁt(/cost), i.e., 1(/-1). Block candidates for recycling were
chosen by a cyclic scanning process over ﬂash memory if
their weighted sum of cost and beneﬁt was above zero. The
garbage collector was triggered when one of the chips had
only one free block left. Note that NFTL, FTL, and ASA
were slightly modiﬁed to cope with the write constraints of
MLC ﬂash memory.
As shown in Table 2, a 64GB four-chipped MLC ﬂash-
memory storage system was under investigation, and 80% of
the storage system was initially stored with data. The capa-
bility of the management strategies was evaluated through
a realistic trace. The trace was collected over a desktop PC
with a 250GB hard disk (by NTFS) for a month. The work-
load was mainly on daily activities, such as document edit-
ing, email accessing, ﬁle uploading and downloading, and
184
25
5
1
0.1
0.01
0.001
10-4
10-5
10-6
10-7
 4  6  8  10  12
L
if
et
im
e 
(Y
ea
rs
)
ECC Capability
NFTL
NFTL+RMTD(No Mir)
NFTL+RMTD(S=512)
NFTL+RMTD(S=1024)
(a) NFTL
25
5
1
0.1
0.01
0.001
10-4
10-5
10-6
10-7
 4  6  8  10  12
L
if
et
im
e 
(Y
ea
rs
)
ECC Capability
FTL
FTL+RMTD(No Mir)
FTL+RMTD(S=512)
FTL+RMTD(S=1024)
(b) FTL
25
5
1
0.1
0.01
0.001
10-4
10-5
10-6
10-7
 4  6  8  10  12
L
if
et
im
e 
(Y
ea
rs
)
ECC Capability
ASA
ASA+RMTD(No Mir)
ASA+RMTD(S=512)
ASA+RMTD(S=1024)
(c) ASA
Figure 9: Lifetime (bit error rate = 0.0001).
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
8642A
v
e
r
a
g
e
 
R
e
s
p
o
n
s
e
 
T
im
e 
(μ
s
)
Number of Chips
Without RMTD
With RMTD
(a) Number of chips (ASA)
 0
 1000
 2000
 3000
 4000
 5000
 6000
20161284A
v
e
r
a
g
e
 
R
e
s
p
o
n
s
e
 
T
im
e 
(μ
s
)
Ecc Capability
Without RMTD
With RMTD
(b) ECC capability (ASA)
 0
 2000
 4000
 6000
 8000
 10000
 12000
 14000
 16000
 18000
128GB64GB32GB16GBA
v
e
r
a
g
e
 
R
e
s
p
o
n
s
e
 
T
im
e 
(μ
s
)
Capacity
Without RMTD
With RMTD
(c) Capacity (ASA)
Figure 11: Average response time of requests (ASA).
when the number of chips increased. That was because ASA
accessed more diﬀerent data regions at the same time when
the number of chips increased. This problem was not seri-
ous in most low-cost ﬂash-memory product designs such as
Universal Serial Bus (USB) ﬂash drives because they usu-
ally adopted no more than four ﬂash chips in each device.
In addition, it could be simply resolved by increasing the
cache size to support up to dozens of chips that were ac-
cessed in parallel. However, this was a trade-oﬀ between the
performance and the cost (for RAM space). Note that FTL,
and NFTL did not access more data regions when the num-
ber of chips increased, since they did not scatter writes over
chips in parallel. Figure 11(b) shows that the performance
overhead introduced by RMTD was slightly increased when
the supported enhanced ECC capability was enhanced. The
reason was that the enhanced ECC redundancy of a data re-
gion increased when the ECC capability enhanced, and the
overheads on the reclaiming of an ECC block increased. In
practice, this issue could be resolved by reserving more ECC
blocks in each ECC region. It was a trade-oﬀ between the
performance and the capacity. When the capacity (or scal-
ability) issue was considered, ASA had poor performance
when the capacity was no more than 32GB, as shown in
Figure 11(c). It was because the collected trace ﬁlled up
most of the space in the ﬁrst 32GB. Moreover, RMTD did
not result in noticeable performance overheads when the ca-
pacity of a storage system increased. That was because the
capacity increasing did not result in the increasing of the
cache miss ratio or the overheads on the management of the
enhanced ECC redundancy.
5. CONCLUSION
This work is motivated by a strong demand to resolve the
endurance/reliability degradation problem of ﬂash-memory
chips. In this paper, a reliable MTD design is proposed to
improve the endurance/reliability of ﬂash-memory products
with very limited or no modiﬁcation to existing FTL designs.
In particular, a log-based write strategy is presented to pro-
vide extra ECC redundancy, where strategies for bad block
management and mirroring-based ﬂash layout are proposed.
A series of experiments was conducted based on realistic
workloads. We show that the proposed reliable MTD design
could signiﬁcantly improve the reliability of ﬂash memory
with very limited system overheads.
For the future research, we shall further explore the trade-
oﬀ between the space utilization and the reliability enhance-
ment. We shall also explore the access behaviors of existing
popular FTL and ﬁle-system designs for better designs of
ﬂash-memory device architectures.
6. ACKNOWLEDGMENTS
This work was supported in part by the NSC under grant
Nos. 99-2218-E-027-005 and 98-2219-E-002-021, by the Ex-
cellent Research Projects of National Taiwan University un-
der grant No. 99R80304, and by the Ministry of Economic
Aﬀairs under grant No. 98-EC-17-A-01-S1-034 in Taiwan.
7. REFERENCES
[1] Flash File System. US Patent 540,448. In Intel
Corporation.
[2] FTL Logger Exchanging Data with FTL Systems.
Technical report, Intel Corporation.
186
Embedded Computing Systems and Applications
(RTCSA), 2003.
[39] C.-H. Wu and T.-W. Kuo. An Adaptive Two-Level
Management for the Flash Translation Layer in
Embedded Systems. In the IEEE/ACM International
Conference on Computer-Aided Design (ICCAD),
November 2006.
[40] P.-L. Wu, Y.-H. Chang, and T.-W. Kuo. A
File-System-Aware FTL Design for Flash-Memory
Storage Systems. In the ACM/IEEE Design,
Automation and Test in Europe (DATE), 2009.
[41] S. Zernovizky and R. Singer. msystemsx4 Technology:
4-Bit/Cell NAND Usage Impossible? M-Systems,
2006.
188
99 年度專題研究計畫研究成果彙整表 
計畫主持人：張原豪 計畫編號：99-2218-E-027-005- 
計畫名稱：嵌入式系統之混合式儲存系統設計: 可靠性與效能提昇 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 3 0 50%  
研究報告/技術報告 0 0 100%  
研討會論文 3 1 50% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 3 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
