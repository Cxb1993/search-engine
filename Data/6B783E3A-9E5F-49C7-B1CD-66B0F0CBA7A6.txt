 1
行政院國家科學委員會補助專題研究計畫成果報告 
在語音增強系統中壓抑音樂型殘留雜訊之研究 
A study on musical residual noise suppression for a speech 
enhancement system 
 
計畫編號： NSC 96 – E – 2221 – 468 - 016 
執行期限：96年 08 月 01 日至 97年 07 月 31 日 
主持人：陸清達   (亞洲大學資訊傳播學系 副教授) 
 
壹、 摘要  
一、 中文摘要 
本研究計畫的目標是要在語音增強處
理系統中，壓抑音樂型殘留雜訊。首先，我
們在小波領域中，使用同質音框合併法則來
求取信號能量的估計值，並將此估計值代入
最小均方誤差估測器，使得音樂型殘留雜訊
的效應可以降低。由於在受干擾的環境中，
語音信號變異數的估計值對於雜訊壓抑具
有很關鍵的角色，因此我們透過局部視窗調
適為基礎之最大相似度的方式來估計語音
信號的變異數值，並將該變異數估計值代入
最小均方誤差估測器中，處理受干擾的語音
信號；這個方法使得語音增強器的增益因子
能在同質性的音框中，變化趨緩，達到壓抑
音樂型殘留雜訊效應的目的。 
 
關鍵詞：語音增強，音樂型殘留雜訊，雜訊
消除，彩色雜訊干擾，小波分解，雜訊遮蔽
域值。 
 
二、 英文摘要 
This project attempts to suppress the effect 
of musical residual noise for a speech 
enhancement system. Firstly, we utilize the 
minimum-mean-square-error (MMSE) esti- 
mator with homogeneous-frame-merged 
approach to reduce the effect of musical 
residual noise. Since the estimation of signal 
variance in a noisy environment is a critical 
issue in noise reduction, we modify the 
adaptive window-based maximum likelihood 
estimator to be applied in speech enhancement. 
This method enables the gain factor to vary 
smoothly over homogeneous frames. 
Accordingly, the effect of musical residual 
noise is reduced.  
Keywords: speech enhancement, musical 
residual noise, denoising, colored noise 
corruption, wavelet decomposition, noise 
masking threshold. 
 
貳、報告內容 
一、 前言 
Thresholding-based algorithms have been 
extensively applied in wavelet-based speech 
enhancement [1-15]. Some of them were 
developed to remove white noise [2, 3, 6, 7, 
16]. They subtract a threshold from the 
wavelet coefficients (WCs) of noisy speech, 
according to noise level [1, 2]. The enhanced 
speech signals are then obtained by inverse 
wavelet packet transform. However, the 
thresholding-based algorithms suffer from an 
annoying musical residual noise which is 
caused by the random spectral peaks that 
come and go in successive frames, and occur 
at random frequencies. Since musical residual 
noise is very annoying to the human ears, it is 
necessary to reduce the musical effect of 
residual noise. 
 
二、研究目的 
 This project aims to propose a speech 
enhancement algorithm which can reduce the 
effect of musical residual noise. 
 
三、文獻探討 
 In order to decrease the effect of musical 
residual noise, noise masking properties were 
incorporated to adapt an algorithm to enhance 
speech for colored/correlated noise 
corruptions [1, 4, 9, 10, 17-19].  
Ghanbari and Karami-Mollaei [13] 
proposed to use an adaptive threshold on a 
modified hard thresholding function. In order 
 3
the wavelet domain; 
)()()(  , , , kWkSkX jijiji +=  (1)
where )( , kX ji , )( , kS ji , and )( , kW ji  are the 
WCs of noisy speech, clean speech, and noise 
at subband i on a j−2  scale, respectively. k is 
the translation index. 
The WC of enhanced speech is obtained by 
multiplying the WC of noisy speech with a 
gain factor )( kG ji,  for each WC: 
)()()(ˆ    kXkGkS ji,ji,ji, ⋅=  (2)
where )(ˆ  , kS ji  denotes the WC of speech 
estimate. 
Assuming the WC of speech signal )( , kS ji  
is uncorrelated with that of noise )( , kW ji , and 
given the power of speech WCs )( , kP jiS . The 
gain factor )( kG ji,  can be obtained by the 
MMSE estimator, given as  
)()(
)(
)(
 
 
 kPkP
kP
kG
ji,Wi,jS
ji,S
ji, +=
 (3)
where )( , kP jiW  denotes the power of noise 
WCs. 
The proposed method is not to use a gain 
value for each frame, but to update the gain 
factor for each WC of a subband. Therefore, a 
burst change signal can be responded by the 
gain factor given in Eq. (3). 
 
 
2. Homogeneous- frame-merged approach 
 
A. Merging homogeneous frames 
Many speech enhancement algorithms 
suffer from an annoying musical residual 
noise. This residual noise is caused by 
randomly spaced spectral peaks that come and 
go in successive frames, and occurred at 
random frequencies. The randomly spaced 
spectral peaks are due to the inaccurate and 
large-variance estimates of the spectra of the 
noise and noisy signals [11]. Therefore, 
adequately controlling the variation of gain 
factor in a segment would improve the 
seriously change of spectrum in a segment. 
The effect of musical residual noise is 
consequently reduced. 
This research project aims at determining a 
proper merged frame with arbitrary shape to 
estimate the signal power for each WC. The 
proposed approach is not to use a 
variable-length frame, but to merge 
homogeneous frames to estimate the power of 
speech WCs which is utilized to calculate the 
gain factor in Eq. (3) for a WC. Starting from 
a given fixed-length frame in which the signal 
power should be estimated, hence, this frame 
is expanded in a segment until the 
homogeneity of the power is achieved. An 
arbitrarily-shaped frame for estimating the 
power of speech WCs is accordingly obtained. 
A fixed-length frame which contains only 
three WCs is employed to calculate the power 
of noisy WCs. The central frame of a segment 
is defined as a reference frame which is 
labeled as W0 in Fig. 2. In order to reduce the 
variation of gain factor over homogeneous 
frames, we merge homogeneous frames to 
estimate the power of noisy WCs. These 
merged frames are then exploited to estimate 
the power of speech WCs.  
In the case of a vowel signal, the long-term 
pitch information is important to speech 
quality. The corresponding number of 
homogeneous frames increases. Therefore, the 
gain factor in Eq. (3) is decided by the merged 
frames with a larger size, which fact enables 
the gain factors to vary smoothly over the 
homogeneous frames. Accordingly, the effect 
of musical residual noise is reduced.  
In the case of a consonant signal, this signal 
can be regarded as a broad-band signal 
without periodic property. The spectral peaks 
abruptly change over consecutive frames. A 
fixed-length frame which consists of only 
three WCs is employed to track this consonant 
signal. The gain factor in Eq. (3) is decided by 
the merged frame with a small size. The value 
of a gain factor would not be affected by a 
frame without the same property as the 
reference frame, i.e., the gain factor of a WC 
with high magnitude is not affected by the 
frame with low magnitude. Therefore, the 
estimated power of speech signal by the HFM 
approach can well respond to that of real 
signal. 
 
B. Criterion for merging frames 
Given a wavelet coefficient with translation 
k, it is set to the central point of reference 
frame for estimating the power of noisy WCs. 
The corresponding power in WC is denoted as 
)0 ,(
 
kP
ji,X
, where the notation “0” represents 
no offsetting from the reference frame. A 
frame with similar power to )0 ,(
 
kP
ji,X
 in a 
 5
exceeds 0.9 for 5 dB. Therefore, the merging 
threshold should increase more in the cases of 
low SNR. On the contrary, increasing the 
merging threshold would deteriorate the 
speech quality for the cases of high SNR.  
In order to render the merging threshold 
inversely varying with input SNR of a 
corrupted speech signal, the sigmoid function 
is employed to mapping the merging 
thresholds from low to high SNRs. The 
proposed merging threshold is given as 
))(ˆexp(1
1-1)(
, k
kt
prio
ji
i, j ξα ⋅−+=
 (7)
where α is empirically chosen as 0.1. The 
SNR of a WC )(ˆ , k
prio
iξ  is calculated by 
)
)(ˆ
)(ˆ
(log10)(ˆ 10, lP
kP
k
i, j
i, j
W
Sprio
ji ⋅=ξ  (8)
where )(ˆ kP
i, jS
 and )(ˆ lP
i, jW
 denote the power 
estimates of the speech WCs and of the noise 
WCs in segment l which will be introduced in 
subsection III. D, respectively. 
The power estimate of the speech signal can 
be roughly estimated by power spectral 
subtraction method, given as 
)(ˆ-)()(ˆ lPkPkP
i, ji, ji, j WXS
=  (9)
A homogeneous-frame-merged (HFM) 
estimator is proposed to estimate the power of 
speech WCs. It is expressed by 
⎟⎟
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜⎜
⎜
⎝
⎛ ⋅
=
∑
∑
−=
−= 0 ,ˆ-
) ,(
) ,() ,(
)(ˆ
i, j
i, j
i, j WQ
Qq
i, j
Q
Qq
i, jX
S P
qkF
qkFqkP
maxkP
 
(10)
Substitute the power estimate of speech 
WCs given in Eq. (10) into Eq. (3), yielding 
the gain factor.  In turn, substituting this gain 
factor into Eq. (2) would result in the WCs of 
enhanced speech. Finally, the enhanced 
speech can be achieved by inverse 
wavelet-packet transforming the enhanced 
version of WCs given in Eq. (2). 
 
C. Various shapes of merged frame 
Figure 2 demonstrates some examples of 
merged frames. In our experiments, a segment 
contains 21 fixed-length frames, so there are 
221-1=220 types of merged frames. The 
fundamental element of a merged frame is the 
reference frame denoted as W0, so the number 
of merged frame is at least one, which is 
shown in Fig. 2(a).  
 
 
Fig. 2 Examples of various shapes of merged 
frames with segment size 21. 
 
The merged frames can be constituted by 
some consecutive fixed-length frames to form 
a traditional frame which is shown in Fig. 2(b). 
Therefore, the traditional frame-based method 
is one case of the proposed method. In the 
case of a vowel signal, pitch information 
should be considered. The power of noisy 
speech signal is calculated by the frames with 
the same property (homogeneity) as the 
reference frame, the shape of homogeneous 
frames may be one of the cases shown in Figs. 
2(c), 2(d), and 2(e). It can be found that the 
frames to be merged are periodic. So the 
proposed method considers pitch information 
in merging frames for each subband. 
 A consonant signal can be regarded as a 
broad-band signal without periodic property. 
The corresponding merged frame can be either 
the case of Fig. 2(f) or Fig. 2(g), i.e., the shape 
of the merged frame is arbitrary without 
specific regular. In the case of a stationary 
signal, the homogeneities in successive frames 
are the same. The corresponding merged 
frame is shown in Fig. 2(h).  
Observing Figs. 2(a), 2(b), and 2(h), the 
proposed (HFM) approach can be regarded as 
a variable-frame-length estimator for each 
wavelet subband. Therefore, the variable- 
frame-length and the traditional frame-based 
algorithms are two special cases of the 
proposed approach. In Figs. 2(a)-2(h), the 
proposed method can be regarded as an 
arbitrarily-shaped frame estimator for speech 
enhancement. The shape is decided by the 
homogeneous properties of a signal. 
 
四、結果與討論 
1.實驗結果 
In the experiments, speech signals are 
Mandarin Chinese and spoken by five female 
and five male speakers. Noisy speech signals 
are obtained by corrupting clean speech with 
 7
The MBSD measure incorporated the concept 
of a noise masking threshold into the BSD 
measure, where any distortion below the 
noise masking threshold was not included in 
the BSD measure. It is due the fact to that the 
noise spectral components below the noise 
masking threshold are considered inaudible, 
the components are excluded in the 
calculation of the MBSD which is given as 
[26] 
∑ ∑
∈
−
=
−⋅=
}{
1
0
ˆ ),(),()(
1
Im
K
SS mLmLM
MBSD
ξ
ξξξη  (11)
where M and K represent the numbers of 
speech-activity frames and of critical bands.  
)(ξη  denotes the indicator of distortion at the 
critical band ξ . ) ,( ξmLS  and ) ,(ˆ ξmLS  
represent the Bark spectra of original speech 
and of enhanced speech at the critical band ξ  
of the frame m. 
 
Table 2 Comparisons of modified Bark 
specctral distortion for the enhanced speech in 
various noise corruptions. 
 SNR Modified BSD 
noise type (dB) MMSE adaptive proposed
 0 9.92 3.64 1.18 
white 5 4.56 1.61 0.61 
 10 1.86 0.64 0.24 
 0 6.75 4.26 1.12 
F16 5 2.92 1.92 0.31 
 10 1.07 0.76 0.09 
 0 6.70 5.27 2.99 
factory 5 2.86 2.38 1.26 
 10 1.08 0.94 0.54 
 0 3.03 1.53 1.26 
helicopter 5 1.20 0.69 0.49 
 10 0.36 0.28 0.24 
 0 8.54 8.94 4.84 
babble 5 3.66 3.92 1.95 
 10 1.41 1.57 0.75 
 
The minimum modified Bark spectral 
distortion (MBSD) corresponds to the best 
speech quality. Table 2 presents the 
performance comparisons of the MBSD for 
three speech enhancement methods. Without 
merging frames to estimate the power of 
speech WCs, the MMSE estimator and the 
adaptive thresholding method can well track 
the variation of speech signals. On the other 
hand, the proposed method decides the power 
estimate of speech WCs by averaging the 
powers of homogeneous frames in a segment. 
The gain factors are also averaged for 
homogeneous frames, enabling the gain value 
of the proposed method to vary slower than 
that of the MMSE estimator. It is the major 
reason why the proposed method is better 
able to reduce the effect of musical residual 
noise.  
In all cases of noise corruptions, the 
proposed approach significantly outperforms 
the MMSE estimator and the adaptive 
thresholding method in terms of the MBSD. It 
is attributed to that the larger the size of the 
merged frame is, the smaller the value of the 
gain factor is. So the residual noise is reduced 
in noise-dominated and speech-pause regions. 
 
C. Waveforms  
Figure 3 demonstrate an example of 
waveform plots. A Speech signals is corrupted 
by F16-cockpit noise with Avg_SegSNR = 5 
dB. The waveform plots of enhanced signals 
reveal that all methods can preserve the 
speech signals in speech-dominated regions. 
So the speech quality can be maintained by all 
methods. It is noted that the proposed method 
is better able to remove the background noise, 
especially in speech-pause and noise- 
dominated regions. The MMSE estimator and 
the adaptive thresholding method retain more 
residual noise containment than the proposed 
method in speech-pause regions.  
 
D. Subjective Listening Test   
A subjective measure reflects the way the 
signal is perceived by listeners. Such a 
measure expresses how pleasant the signal 
sounds. Herein, the mean opinion score 
(MOS) is used to evaluate the global 
appreciation of the residual noise 
contamination and the speech distortion. The 
subjective tests are conducted with seven 
listeners. Each listener gives to each test 
signal a score between one and five. Table 3 
presents the results of performance 
comparison. Initially, the clean and the noisy 
speech signals which have been corrupted by 
F16-cockpit noise are played back for the 
listening test. Then, the utterances of the 
enhanced speech produced by the EMSR, the 
 9
signal is spoken by a female speaker (Fig. 
4(a)), and is corrupted by F16-cockpit noise 
with Avg_SegSNR = 5 dB. (Fig. 4(b)). 
Comparing Figs. 4(c), 4(d), and 4(d), the 
proposed method outperforms the MMSE 
estimator and the adaptive thresholding 
method in removing the residual noise for 
speech-pause regions. In addition, all methods 
can preserve a broad-band signal, such as a 
consonant. 
 
2.討論  
Tables 1 and 2 reveal that the performances 
of the three approaches are comparable when 
the input Avg_SegSNRs are high 
(Avg_SegSNRs = 10dB). Thus, all methods 
perform comparable in removing corrupting 
noise in speech-activity regions. If the 
Avg_SegSNR inputs are less than 5dB, the 
proposed method outperforms the MMSE 
estimator and the adaptive thresholding 
method. It is attributed to the fact that the 
proposed approach is better able to remove 
corrupting noise. 
  As the number of merged frames increases, 
the effect of musical residual noise is reduced 
more. It is attributed to the fact that the gain 
factors vary smoothly among homogeneous 
frames, enabling the spectral peaks of 
enhanced speech using the proposed approach 
to vary slower than that without using 
merging frames to estimate the power of 
speech WCs. In the case of low-speech 
energy, the smoother the variation of the gain 
factor leads to the further reduction of the 
effect of musical residual noise. Therefore, 
the MMSE estimator incorporated with 
merged frames (the HFM approach) to decide 
the value of gain factor performs better than 
that without using merging frame in 
perception. 
  In Tables 1 and 2, the MMSE estimator and 
the adaptive thresholding method are better 
able to respond to the sudden variation of 
speech signal. However, they lack for 
considering the variation of WCs in adjacent 
translations. Therefore, these two methods 
perform well in objective measures, such as 
Avg_SegSNR improvement. But these two 
algorithms can not improve the perception of 
enhanced speech, yielding smaller values of 
the MBSD and of the MOS. 
  Residual noise is apparently perceived by 
human ears in low-energy and speech-pause 
regions. It is due to this residual noise can not 
be masked by a low-energy speech signal, so 
the effect of musical residual noise is 
apparent. Especially, the serious variation of 
spectral peaks causes the musical effect. The 
proposed method estimates the power 
estimate of speech WCs by merged frames, 
enabling the comparable amplitudes of gain 
factor over the homogeneous frames. These 
gain values vary slowly, so that the effect of 
musical residual noise is reduced. 
The proposed method merges only a little 
number of frames to obtain the power 
estimate of speech WCs in the transient 
between low and high energy regions. 
Accordingly, the corresponding gain factor 
varies rapidly to track a sudden change signal, 
such as a consonant signal. Comparing Figs. 
4(c), 4(d), and 4(e), the proposed method 
offers better reduction of residual noise than 
the MMSE estimator and the adaptive 
thresholding method in low-energy and 
speech-pause regions. Therefore, the 
proposed approach offers better performance 
in terms of reducing residual noise.  
 
3.結論 
An arbitrarily-shaped frame for 
estimating the power of speech WCs was 
proposed. Owing to the very short length of a 
fixed frame (a frame with 3 WCs), a sudden 
change of speech signal, such as a consonant, 
can be responded. Additionally, the gain 
factor is updated for each translation, i.e., a 
WC corresponds to a gain factor. So this gain 
factor can well track speech signals. 
Experimental results show that the MMSE 
estimator can be improved by merging 
homogeneous fixed-length frames to decide 
the gain factor for each subband. The effect of 
musical residual noise can be efficiently 
mitigated by enabling the gain factor to vary 
smoothly over homogeneous frames. 
Accordingly, the quality of the enhanced 
speech is improved. 
 
五、附錄 
1.本計畫之論文發表 
[1] Ching-Ta Lu, “Speech enhancement using 
MMSE estimator with homogeneous- 
frame-merged approach in wavelet 
domain,” International Journal of 
 11
2.本計畫發表論文[1] 
Ching-Ta Lu, “Speech enhancement using MMSE estimator with homogeneous-frame- 
merged approach in wavelet domain,” International Journal of Electrical Engineering, vol. 15, 
no. 4, Aug. 2008. (EI) (NSC 96-2221-E-468-016) 
 13
 15
 17
 19
 21
 
 23
 25
 27
4.本計畫發表論文[3] 
Ching-Ta Lu and Kun-Fu Tseng, “Noise reduction using simultaneous masking property and 
SNR variation for various noise corruptions,” IEEE International Conference on Acoustics, 
Speech, and Signal Processing (ICASSP), pp. 1601-1604, 2008. (NSC 
96-2221-E-243-001ÆNSC 96-2221-E-468- 016) 
 29
 31
叁、 計畫成果自評 
一、研究內容與原計畫相符程度 
    原計畫的內容提案是要在語音增強處
理系統中，壓抑音樂型殘留雜訊，而本研究
內容與成果即是發展語音增強處理的演算
法，特別著重於降低音樂型殘留雜訊效應，
因此研究內容與原計畫內容相符。 
 
二、達成預期目標情形 
    原計畫的目標是要在語音增強處理系
統中，壓抑音樂型殘留雜訊。並且實現一個
新型的語音增強處理系統，該系統可以同時
使用在一個音框內的人耳聽覺遮蔽效應及
考慮相鄰音框之訊雜比變化量為調適機
制，作為改善受到彩色雜訊干擾的語音信
號。並且嘗試在小波領域中，使用同質音框
合併法則來求取信號能量的估計值，並將此
估計值代入最小均方誤差估測器，使得音樂
型殘留雜訊的效應可以降低。而本計畫的研
究成果已經完成上列兩項的研究主題，並且
將 成 果 發 表 在 International Journal of 
Electrical Engineering (EI) 期刊中，與兩項
會議論文中(2007 年全國電信研討會、IEEE 
International Conference on Acoustics, 
Speech, and Signal Processing, ICASSP 
2008)，因此研究內容與原計畫內容相符。 
 
三、研究成果之學術或應用價值 
    我們提出來的兩種語音增強方法，可以
達到語音增強的效果，在增強後的語音中，
確實可以有效的降低音樂型殘留雜訊的效
應，因此極具學術價值。 
 
四、是否適合在學術期刊發表 
    本專題計畫的研究成果可以發表在學
術期刊中，目前本研究計畫的研究成果已經
被 International Journal of Electrical 
Engineering(EI)期刊接受，並即將於 2008 年
8 月刊登。 
 
五、主要發現或其他有關價值 
    在一個語音處理系統中估計信號的變
異數時，局部視窗的大小也是一個很重要的
因子，使用同質性音框合併的方法從事語音
增強的工作，可以讓增益音子在同質性音框
中變化趨緩，使得同質性音框間的頻譜變化
趨緩，可以有效的降低音樂型殘留雜訊的效
應，這個合併同質性音框的觀念可以適用在
很多種語音增強系統中，做為壓抑音樂型殘
留雜訊的相關策略。 
   此外，在這個研究計畫中，我們也提出
同時使用在一個音框內的人耳聽覺遮蔽效
應及考慮相鄰音框之訊雜比變化量為調適
機制，作為改善受到彩色雜訊干擾的語音信
號。在一個語音增強處理系統中，藉由透過
精確的估算一個音框的訊雜比的值來調適
一個語音增強系統，可以降低降低部分音樂
型殘留雜訊的影響，所以我們利用一個均化
因子來改善訊雜比的估計值，再將該訊雜比
值代入以人耳聽覺特性調適的語音增強處
理系統中，提高音樂型殘留雜訊的壓抑效
果。 
 
肆、出席國際學術會議心得報告 
計畫編號 NSC 96-2221-E-468-016 
計畫名稱 在語音增強系統中壓抑音
樂型殘留雜訊之研究 
出國人員姓名 
服務機關及職稱
1. 陸清達—亞洲大學資
訊傳播學系—副教授
2. 曾昆福—親民技術學
院電子工程系—助理
教授 
會議時間地點 
時間:2008 年 3 月 30 日~
2008 年 4 月 4 日 
地點:美國拉斯維加斯 
會議名稱 
IEEE International 
Conference on Acoustics, 
Speech, and Signal 
Processing (ICASSP), 
2008 
發表論文題目 
Noise reduction using 
simultaneous masking 
property and SNR
variation for various noise 
corruptions 
 
一、 參加會議經過 
    本次會議為 ICASSP第 33 年的年度會
議，會議的地點是在美國內華達州(Neveda)
的拉斯維加斯(Lasvegas)舉行，會議場所選
在當地非常大的凱薩皇宮大飯店(Casars 
 33
質的意見，經過適度修改與補做實驗之
後，論文品質一定可以有效提升，作為
將來投稿到期刊論文的準備。 
 
參考文獻 
[1] B. Carnero and A. Drygajlo, “Perceptual 
speech coding and enhancement using 
frame-synchronized fast wavelet packet 
transform algorithms,” IEEE Trans. Signal 
Processing, vol. 47, no. 6, pp. 1622-1635, 
June 1999.  
[2] D. L. Donoho, “De-noising by 
soft-thresholding,” IEEE Trans. Inform. 
Theory, vol.  41, no. 3, pp. 613-627, May 
1995. 
[3] M. Jansen and A. Bultheel, ”Asymptotic 
behavior of the minimum mean squared error 
threshold for noisy wavelet coefficients of 
piecewise smooth signals,” IEEE Trans. 
Signal Processing, vol. 49, no. 6, pp. 
1113-1118, 2001. 
[4] C.–T. Lu and H.–C. Wang, “Enhancement of 
single channel speech based on masking 
property and wavelet transform,” Speech 
Commun., vol. 41 (2-3), pp. 409-427, October 
2003. 
[5] C.–T. Lu and H.–C. Wang, “Speech 
enhancement using robust weighting factors 
for critical-band-wavelet-packet transform,” in 
Proc. Int. Conf. Acoust., Speech, Signal 
Processing, vol. 1, Quebec, Canada, May 
2004, pp. 721-724. 
[6] S. Mallat, A wavelet tour of signal processing. 
2nd ed., San Diego: Academic Press, 1999. 
[7] L. Singh and S. Sridharan, “Speech 
enhancement for forensic application using 
dynamic time warping and wavelet packet 
analysis,” in Proc. IEEE TENCON-SITCT, 
Brisbane, Australia, December 1997, pp. 
475-478. 
[8] H. Sheikhzadeh and H.R. Abutalebi, “An 
improved wavelet-based speech enhancement 
system,” in Proc. European Conf. Speech 
Commun. Technology, Rhodes, Greece, 
September 2001, pp. 1855-1858. 
[9] C.–T. Lu and H.–C. Wang, “Speech 
enhancement using hybrid gain factor in 
critical- band-wavelet packet transform,” 
Digital Signal Processing, vol. 17, no. 1, 2007, 
pp. 171-188, January 2007. 
[10] C.–T. Lu, “Speech Enhancement using 
homogeneous-frame-merged approach in 
wavelet domain,” in Proc. National 
Symposium on Telecommun., pp. 102-106, 
2007.  
[11] Y. Hu and P.C. Loizou, “Speech enhancement 
based on wavelet thresholding the multitaper 
spectrum,” IEEE Trans. Speech Audio 
Processing, vol. 12, no. 1, pp.59-67, January 
2004. 
[12] S. Chang, Y. Kwon, S.I. Yang, and I.J. Kim, 
“Speech enhancement for non-stationary noise 
environment by adaptive wavelet packet,” in 
Proc. Int. Conf. Acoust., Speech, Signal 
Processing, vol. 1, Orlando, Florida, May 
2002, pp. 561-564. 
[13] Y. Ghanbari and M.R. Karami-Mollaei, “A 
new approach for speech enhancement based 
on the adaptive thresholding of the wavelet 
packets,” Speech Commun., vol. 48, pp. 
927-940, 2006. 
[14] M.T. Johnson, X. Yuan, and Y. Ren, “Speech 
signal enhancement through adaptive wavelet 
thresholding,” Speech Commun., vol. 49, pp. 
123-133, 2007. 
[15] M. Bahoura and J. Rouat, “Wavelet speech 
enhancement based on time-scale adaptation,” 
Speech Commun., vol. 48, pp. 1620-1637, 
2006. 
[16] D.L. Donoho and I.M. Johnston, “Ideal spatial 
adaptation by wavelet shrinkage,” Biometrika, 
vol. 81, no. 3, pp. 425-455, September 1994. 
[17] Y. Hu and P.C. Loizou, “A perceptually 
motivated approach for speech enhancement,” 
IEEE Trans. Speech Audio Processing, vol. 11, 
no. 5, pp. 457-465, September 2003. 
[18] N. Virag, “Single channel speech enhancement 
based on masking properties of the human 
auditory system,” IEEE Trans. Speech Audio 
Processing, vol. 7, no. 2, pp. 126-137, March 
1999. 
[19] C.–T. Lu, “Reduction of musical residual noise 
for speech enhancement using masking 
properties and optimal smoothing,” Pattern 
Recognition Letters, vol. 28, no. 11, pp. 
1300-1306, August 2007. 
[20] B. Chen and P.C. Loizou, “A Laplacian- 
basedMMSE estimator for speech 
enhancement,” Speech Commun., vol. 49, pp. 
134-143, 2007. 
[21] Y. Ephraim and D. Malah, “Speech 
enhancement using a minimum mean-square 
error short-time spectral amplitude estimator,” 
IEEE Trans. Acoust., Speech, Signal Process., 
vol. 32, pp. 1109-1121, 1984. 
[22] I.K. Eom and Y.S. Kim, “Wavelet-based 
denoising with nearly arbitrarily shaped 
windows,” IEEE Signal Processing Lett., vol. 
11, no. 12, pp. 937-940, December 2004. 
[23] L. Sendur and I.W. Selesnick, “Bivariate 
shrinkage with local variance estimation,” 
IEEE Signal Processing Lett., vol. 9, no. 12, 
pp. 438-441, December 2002. 
[24] S.G. Chang, B. Yu, and M. Vetteri, “Spatially 
adaptive wavelet thresholding with context 
modeling for image denoising,” IEEE Trans. 
出席國際學術會議心得報告 
                                                             
計畫編號 NSC 96-2221-E-468-016 
計畫名稱 在語音增強系統中壓抑音樂型殘留雜訊之研究 
出國人員姓名 
服務機關及職稱 
1. 陸清達—亞洲大學資訊傳播學系—副教授 
2. 曾昆福—親民技術學院電子工程系—助理教授 
會議時間地點 
時間:2008 年 3 月 30 日~ 2008 年 4 月 4 日 
地點:美國拉斯維加斯 
會議名稱 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2008 
發表論文題目 NOISE REDUCTION USING SIMULTANEOUS MASKING PROPERTY AND SNR VARIATION FOR VARIOUS NOISE CORRUPTIONS 
 
一、參加會議經過 
    本次會議為 ICASSP 第 33 年的年度會議，會議的地點是在美國內華達州(Neveda)的拉斯
維加斯(Lasvegas)舉行，會議場所選在當地非常大的凱薩皇宮大飯店(Casars Palace) 舉辦。本
次會議中，總共投稿論文篇數為 2729 篇，接受發表與刊登篇數為 1352 篇，論文接受率為 48%，
除了發表論文的作者之外，也有許多業界人士前往參加會議討論最新之研究成果，會議內容
將上述論文分成 12 類，各篇論文則分到各個適當的 session，分別以 Oral 或 Poster 的方式
做報告，論文分類與其所包含的 sessions 如附錄所示。 
    由附錄的論文發表場次資訊可以看出，信號處理的應用領域擴增速度非常驚人，也可以
看出從事信號處理的研究團隊與人數越來越多；本次大會將每天的個會議分成三個時段，每
個時段有 5 到 6 個 oral presentation sessions；與 7 到 8 個 poster presentation sessions 同時舉行，
每個 oral presentation 的時段為 90 分鐘，所以每篇論文有 15 分鐘的報告時間，每一段 poster 
presentation 的時間為 120 分鐘，與會人士可以根據自己的研究興趣前往各個論文報告會場聽
取作者作論文報告，並與作者做充分的討論與意見交流。 
    為了讓與會學者有機會了解更多的信號處理研究領域，大會也安排了幾個前瞻性的研究
主題做專題演講，包括: 
1.演講主題: “A Review of Radio Frequency Identification Systems – Present Status, Design 
Challenges and Future Outlook”，主講人:Raj Mittra, Pennsylvania State University 
2. 演講主題: “From Mind to Matter: Using Brain Waves to link Thoughts and Action”，主講人: 
Nitish Thakor, Johns Hopkins School of Medicine 
3. 演講主題: “Unlocking Knowledge, Opening Minds – The MIT OpenCourseWare 
Experience”，主講人: Dick K.P. Yue, Massachusetts Institute of Technology 
4. “Media Signal Processing in Cell Phones – An Amazing Story”，主講人: Peter Kroon, 
Infineon Technologies 
    在這次參加 ICASSP08 國際會議中，由本計畫主持人陸清達及親民技術學院電子工程系
曾昆福老師共同發表一篇論文(如圖一所示)，論文標題為” Noise Reduction Using 
 
圖二 
 
    在會議當中，諸多學者與從事音訊處理的廠商，對於我們使用人耳聽覺遮蔽效應從事語
音增強的想法，得到不錯的語音品質，除了給予肯定之外，也認為相當可行；而且我們在推
導增益因子時，使用限制相鄰音框的訊雜比值的變化的機制來調適增益因子，使得增強後的
語音信號中，即時存在殘留雜訊頻譜，由於殘留雜訊譜變化已經被限制住，使得殘留雜訊頻
譜聽起來很平穩，所以不會很吵雜，達到壓抑音樂型殘留雜訊效應的目的，這種想法也是本
論文最有價值的地方。 
 
 
二、與會心得 
    參加完本次會議之後，我們有下面的心得與建議： 
1. 我們可以發現，語音增強在這項國際會議中，有 10 sessions 在討論語音增強的相關主題，
如下列的 sessions:  
AE-P5: Microphone Arrays 
AE-P6: Acoustics, Active Noise Control, and Hearing Aids 
ITT-P2: Applied Speech Processing 
MLSP-P4: Applications in Speech, Audio, and Biomedical Signal Processing 
SAM-P3: Sensor Array Processing 
SPTM-L1: Wavelets Analysis 
SPTM-P10: Adaptive Filtering Algorithms 
SPE-L5: Speech Enhancement I 
SPE-P10: Speech Enhancement II 
SPE-P17: Noise Reduction for Speech Enhancement 
IMDSP-L4: Feature Extraction and Analysis 
IMDSP-L5: Restoration and Enhancement I 
IMDSP-L6: Optical Flow and Motion Estimation 
IMDSP-L7: Image/Video Storage and Retrieval 
IMDSP-L8: Interpolation and Super-resolution II 
IMDSP-P1: Modeling and Quality Assessment 
IMDSP-P2: Image Filtering and Restoration 
IMDSP-P3: Image/Video Segmentation and Tracking 
IMDSP-P4: Biometrics / Feature Extraction and Analysis 
IMDSP-P5: Video Coding II 
IMDSP-P6: Stereoscopic and 3-D Processing 
IMDSP-P7: Image Formation, Scanning, Display, Printing 
IMDSP-P8: Video Coding, Distributed Coding, Transmission 
IMDSP-P9: Image Coding 
IMDSP-P10: Image/video Storage and Retrieval 
IMDSP-P11: Restoration and Enhancement II 
IMDSP-P12: Interpolation and Super-resolution I 
IMDSP-P13: Feature Extraction and Analysis 
IMDSP-P14: Image and Video Coding 
4. Implementation of Signal Processing Systems 
DISPS-L1: Implementation of Signal Processing Systems I 
DISPS-P1: Implementation of Signal Processing Systems II 
5. Industry Technology Track 
ITT-L1: Applied Multidimensional Signal Processing 
ITT-L2: Defense & Security Applications 
ITT-L3: Industrial DSP Applications I 
ITT-P1: Applied Speech Recognition 
ITT-P2: Applied Speech Processing 
ITT-P3: Industrial DSP Applications II 
Information Forensics and Security 
IFS-L1: Forensics 
IFS-L2: Watermarking 
IFS-L3: Biometrics 
IFS-P1: Watermarking and Steganography 
IFS-P2: Security, Cryptography and Privacy 
6. Machine Learning for Signal Processing 
MLSP-L1: Independent Component Analysis 
MLSP-L2: Blind Source Separation and Nonnegative Signal Decomposition 
MLSP-L3: Pattern Recognition and Classification I 
MLSP-L4: Graphical and Kernel Methods 
MLSP-P1: ICA and Signal Decomposition 
SPTM-L1: Wavelets Analysis 
SPTM-L2: Distributed Adaptive Processing 
SPTM-L3: Computational Linear Algebra 
SPTM-L4: Classification and Pattern Recognition 
SPTM-L5: Compressed Sensing II 
SPTM-L6: Robust Methods 
SPTM-L7: Particle Filtering 
SPTM-P1: Bayesian Methods and Bootstrap 
SPTM-P2: Communications Applications and Source Separation 
SPTM-P3: Filter Banks and Fourier Methods 
SPTM-P4: Adaptive Tracking and Channel Estimation 
SPTM-P5: Nonlinear Techniques and Applications 
SPTM-P6: Applications of Signal Processing Theory & Methods 
SPTM-P7: Estimation 
SPTM-P8: Sampling and Reconstruction 
SPTM-P9: Spectral Analysis and Time-Frequency Representation 
SPTM-P10: Adaptive Filtering Algorithms 
SPTM-P11: Compressed Sensing III 
SPTM-P12: Detection and Performance Bounds 
12. Speech Processing 
SPE-L1: Speech Analysis I 
SPE-L2: Speech Synthesis I 
SPE-L3: Speech Analysis II 
SPE-L4: Speech Coding I 
SPE-L5: Speech Enhancement I 
SPE-L6: Discriminative Training for ASR 
SPE-L7: Robust Speech Recognition II 
SPE-L8: Alternative Acoustic Models for ASR 
SPE-L9: Speaker Recognition I 
SPE-L10: Language and Dialect ID 
SPE-L11: Acoustic Feature Extraction and Modeling II 
SPE-P1: Speech Production and Perception 
SPE-P2: Multilingual Recognition and Identification 
SPE-P3: Speech Adaptation and Normalization 
SPE-P4: Speech Recognition: Miscellaneous Topics 
SPE-P5: Speaker Localization/Diarization/Clustering 
SPE-P6: Robust Speech Recognition I 
SPE-P7: ASR using Multiple Information Sources and Fast Decoding 
SPE-P8: Speech Analysis III 
SPE-P9: Acoustic Modeling for ASR 
SPE-P10: Speech Enhancement II 
