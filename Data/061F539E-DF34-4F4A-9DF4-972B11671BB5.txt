 2 
一、前言與研究目的 
由於攝影機的普及，影片的取得相當容易，而卡通是一般大眾非常容易接受的視覺形式，
因此影片卡通化技術的發展，是一個十分有趣也充滿無限的商機的研究課題。因為卡通動
畫在創意與想像力上遠遠地超過真人的實境拍攝，但是製作卡通的過程有一定的專業難
度，因為卡通動畫的製作需要一幀一幀的去繪製，中間也受到很多因素的限制，所以一般
人想要製做出個人卡通動畫並不容易。 
本計畫擬發展一個轉換系統，將卡通化與影片做結合，讓使用者能夠輕鬆地將所錄製的
影片轉換成卡通，此外，卡通有別於真實影像的一個因素是卡通擁有較多的趣味性，因此
本計畫擬在卡通化的同時，加上創意的特效呈現。本計畫將探討影片卡通化及如何與特效
作結合的技術，以增加所轉換的結果之創意及趣味性。 
二、文獻探討 
 近年來在影像處理中，Non-photorealistic rendering (NPR)獲得很多研究者的重視，NPR
在單張影像的應用有：(1)Painting Style，目的在於將影像原有的畫風轉換成另一種畫風 ，
如：Hertzmann[ 1]、Wang[ 2]、Chang[ 3]；(2)Image abstraction，將原圖的細節除去，予以
抽象化，如：Decarlo [ 4]、Christoudias [ 5]、Wen [ 6]；(3)Texture Synthesis，也就是模擬蠟
筆、毛筆之類的研究(Rudolf [ 7])等的議題。 
然而從原先的單張影像進展到現今的連續影像，製作帶有藝術風格的視頻影片是一個更
加充滿趣味和挑戰性的研究議題。大體而言，影片卡通化會有兩大問題，第一，影像必須
保持 Spatio-Temporally coherence，避免畫面產生令人不悅的跳動。第二，影片的內容必須
讓電腦知道畫面中哪些部分是屬於 High-Level regions。在 1997 年，Litwinowicz[ 8] 提出了
一種全自動的方法，將視頻影片轉換為帶有動畫風格的視頻影片，作者提出的方法是利用
Optical Flow 將筆觸沿著其像素的運動方向由一幀變化到另一幀，在 2000 年 Herzmann[ 9]
則是運用變形的方法對連續的幀進行修改，其演算法為將前一幀變形用來補償 Optical Flow
的變化，然後比較幀在時間上的差異性，其內容與前一幀內容差別比較大的部分再將之重
新繪製，其餘則保持與上一幀一樣，用此來解決時間一致性的問題。在 2001，Herzmann[ 10]
在特徵的選取加上了像素顏色的考量，其方法被改進為運用視頻畫面中像素的色彩差異性
以及 Optical Flow 的變化方向來引導繪製帶有藝術家風格的筆觸方向之變化。 
在 2002 年 Agarwala[ 11]利用 Active Contour Model 對影像中的前景邊緣作收斂，然後透
過收斂的結果對前景作卡通化，不過缺點是必須投入大量的人力來對影像作切割校正，而
且作者的背景極為單純，在前景的運動方面，畫面中的物體也是作輕微的運動而已。在 2004
年 Wang[ 12]根據第一張畫面上的顏色分佈利用 mean shift segmentation（Comanniciu [ 13]）
將之分成許多顏色區塊，接著利用追蹤這些顏色區塊的運動達到確定物體的移動軌跡。但
是畫面中擁有雜訊是不可避免的，而且畫面中也時常會出現不同物體但是其顏色極為相
近，這時候 Mean Shift Segmentation 就無法正確的做出切割。因此作者透過手工對前景邊
界標出一條圍繞的輪廓線，透過輪廓線上的關鍵點，使用者可以在 key frame 上做校正，此
時再利用線性內差對其餘 frame 的切割結果做自動校正，確保每張畫面切割的結果都儘量
正確，如果切割結果還是不理想，則人工就再次介入加上新的輪廓線對切割結果做更進一
步的校正，作者的 fps 為 25，如果畫面中動作比較劇烈，甚至每 3 張 frame 就需要人工作
校正，因此作者也是花費極大的人力在做校正的工作。 
在 2005 年 Collomosse [ 14]將系統分成三個部分：Front End、IR 和 Back End，Front End
 4 
 
Figure 1. The curve of SkinLabels variation. 
Table 1. Result of Hierarchical Video Segmentation. 
Shot Type Period type Start frame End frame 
Entry shot - 1 119 
 
Action shot 
Gimmickry 
period 
119 325 
Incubate period 325 428 
Shooting period 428，554 669 
Exit shot - 669 864 
Player from video 
Entry shot 
    
1               40               80              119 
Action shot 
 6 
率結果為 98.3。但目前我們所拍攝的影片，人在畫面中佔有較大的比例，若人所佔的比例
偏低，辨識率可能會下降。 
 
4.3 Video Tooning with Sepcial Effects  
我們以兩個影片作為此節的實驗影片。在影片一中，由於人物有兩次攻擊的動作，所以
特效會出現兩次，又因為我們第一次出手速度比較慢，而第二次出手速度比第一次快許多，
所以在Figure 3中，第一次的特效一次跳 5個 frames顯示，而第二次的特效一次跳 2個 frames
顯示，此外從圖中可以發現，背景有一棵數，而且我們的特效也會和環境有互動，因此當
特效撞擊到樹的時候，特效會反彈，由於我們的特效飛行角度與出手角度一樣，所以第一
次的出手其特效不會撞到數，但是第二次的特效就會撞到樹，最後一點，我們特效撞到障
礙物的設定是會加速反彈，所以從圖中可以發現，反彈之後的特效很快就消失在我們的圖
中，因為其飛行速度又更快速了。 
 
Video Tooning with Special Effects 
    
400              420               440              455 
    
460              465              470              475 
    
480              485              490              520 
    
550              579              580              582 
 8 
    
720              723              725              728 
    
731              734              737              740 
    
750              760              790              820 
Figure 4. Results of Video Tooning with Special Effects. 
由於我們的影片是擁有雙人互動的情形，因此在決鬥的途中，雙方會比較特效的控制力量
F，F 越大會使得特效越往對方那邊靠去，因此雙方的特效會暫時僵持不下，但是由於控制
F 的參數中有 3 項因數，G、M 和 r，而畫面中左邊的人除了 M 比較大之外，G 的權重也比
較大，所以可以從圖中發現，雙方的特效雖然互有輸贏，發生僵持，但是還是慢慢往右半
邊靠過去，此外從圖中還可以發現在比試的過程中，特效會以極緩慢的速度漸漸往上升高，
這也是我們的內部設定，因為我們覺得在比試過程中本來就會讓原本的特效路徑稍微做了
變化，此外我們系統的內部參數當中，在 shooting period 裡有一個參數叫 Lifetime of special 
effects，當表演者的手放下來沒有攻擊動作或是在決鬥中發生潰敗時，特效的生命都會隨之
結束，因此在 frame 為 723 的地方，右半邊的人舉貣了他的雙手，表示他在決鬥中輸了，
因此他的特效也會直接結束，此外我們為了增加可看性，我們還會加入爆炸的場面，也就
是整個畫面都是紅色的但是隱約可以看到背景，這也是與 alpha blending 結合的效果，透過
爆炸場面增加我們的視覺效果。 
 
五、計畫成果自評 
 本計畫目的在建立一個視訊卡通特效化的系統，本系統分為兩大部分，分別為
hierarchical video segmentation 與 video tooning with special effects。在系統的第一部分中，
首先利用人體動作定義一個 video structure，包含 Entry shot, Action shot, and Exit shot 
respectively，其中 Action shot 為影片的核心，因此會再被分為 Gimmickry period, Incubate 
period, and Shooting period。透過 video structure 幫助系統有效率的找出與特效互動的參數，
我們稱之為特效參數。在系統的第二部分，我們利用 mean shift segmentation 對前景做卡通
 10 
[ 9 ]  Hertzmann, A. ; Perlin, K. , “ Painterly Rendering for Video and Interaction ” , In 
Proceedings of NPAR , 2000 , pp.7-12 . 
[ 10 ]  Hertzmann, A.  , “ Paint by relaxation ” , In Proceedings of Computer Graphics, 2001 , 
pp.47-54 . 
[ 11 ]  Agarwala, A. , “SnakeToonz: a semi-automatic approach to creating cel animation from 
video ” , In Proceedings of ACM SIGGRAPH , 2002 , pp.139-ff . 
[ 12 ]  Wang, J. ; Xu, Y. ; Shum, H-Y. ; Cohen, M. , “ Video Tooning ” , In Proceedings of 
ACM SIGGRAPH , 2004 , pp.574-583 . 
[ 13 ]  Comanniciu, D. ; Meer, P. , “ Mean Shift : A Robust Approach Toward Feature Space 
Analysis ” , IEEE Transactions on Pattern Analysis and Machine Intelligence , Volume 24 , Issue 
5 , May 2002 , pp.603-619 . 
[ 14 ]  Collomosse, J.P. ; Rowntree, D. ; Hall, P.M. , “ Stroke Surfaces: Temporally Coherent 
Artistic Animations from Video ” , IEEE Transactions on Visualization and Computer Graphics , 
Volume 11 , Issue 5 , November 2005 , pp.540-549 . 
 
2?????,??????????? Sapporo convention center(???)?APSIPA????
?????????????????? 2009/10/04 ~ 2009/10/07?????
(a) Sapporo Convention Center
44. Circuits and Systems/VLSI
4.1 Biomedical circuits and systems
4.2 Nanoelectronics and gigascale systems
4.3 Intelligent systems and applications
4.4 VLSI systems and applications
4.5 Embedded systems
??????????KeyNote Speeches????????????
I. "Structural Subband Decomposition: A New Concept in Digital Signal Processing"
Professor Sanjit K. Mitra, Ph.D.
Department of Electrical and Computer Engineering
University of California, Santa Barbara, USA
Research Interests:
Digital signal, image and video processing, data compression, image and video enhancement, image
analysis, mixed analog-digital signal processing
Biographical Information:
Ph.D. in Electrical Engineering, 1962, University of California, Berkeley, CA.
Dr.  Mitra  received  the  B.Sc.  (Hons.)  degree  in  Physics  from  the  Utkal  University,  India  in  1953,
M.Sc. (Tech.) degree in Radio Physics & Electronics from the University of Calcutta, India in 1958,
and his M.S. and Ph.D. degrees in Electrical Engineering from the University of California,
Berkeley, California in 1960 and 1962, respectively. He joined the Department of Electrical
Engineering-Systems as the Stephen and Etta Varra Professor in July 2006. He has published over
640 papers in the areas of analog and digital signal processing, and image processing. He has also
authored and co-authored twelve books, and holds five patents.
Dr.  Mitra  has  served  IEEE  in  various  capacities  including  service  as  the  President  of  the  IEEE
Circuits & Systems Society in 1986, and has held visiting appointments in Australia, Austria,
Finland, Germany, India, Japan, Norway, Singapore and the United Kingdom. Dr. Mitra is the
recipient of the 1973 F.E. Terman Award and the 1985 AT&T Foundation Award of the American
Society of Engineering Education, the 1989 Education Award, the 2000 Mac Van Valkenburg
Society  Award  and  the  Golden  Jubilee  Medal  of  the  IEEE  Circuits  &  Systems  Society,  the
Distinguished Senior U.S. Scientist Award from the Alexander von Humboldt Foundation of
Germany in 1989, the 1996 Technical Achievement Award, the 2001 Society Award and the 2006
Education Award of the IEEE Signal Processing Society, the IEEE Millennium Medal in 2000, the
McGraw-Hill/Jacob Millman Award of the IEEE Education Society in 2001, the 2002 Technical
Achievement Award of the European Association for Signal Processing (EURASIP), the 2005 SPIE
6Professional qualifications:
Fellow of IEEE
Professor Emeritus at the University of Tokyo
?? ????
????????? 10? 5????MP-L1-4 0174?????????
Monday, 05 October 2009 16:00-18:00 Room E
MP-L1 3D Computer Graphics and Applications (oral session)
Chair: Kuo-Liang Chung (National Taiwan University of Science and Technology, Taiwan)
MP-L1-1 0115 VERTEX SHUFFLING : A NOVEL INFORMATION HIDING ON 3D
MODEL
Cong-Kai Lin National Dong Hwa University Taiwan
Charlie Irawan Tan National Dong Hwa University Taiwan
Wen-Kai Tai National Dong Hwa University Taiwan
MP-L1-2 0168 HIERARCHICAL EVOLUTION FOR ANIMATING 3D VIRTUAL
CREATURES
Tailin Chen National Chiayi University Taiwan
Daosheng Mu National Chiayi University Taiwan
Tainchi Lu National Chiayi University Taiwan
MP-L1-3 0138 A 3D VIRTUAL SHOW ROOM FOR ONLINE APPAREL RETAIL SHOP
Ching-I Cheng National Chung Cheng University Taiwan
Damon Shing-Min Liu National Chung Cheng University Taiwan
Chun-Hung Tsai National Chung Cheng University Taiwan
Li-Ting Chen National Chung Cheng University Taiwan
MP-L1-4 0174 VIDEO INPAINTING BASED ON MULTI-LAYER APPROACH
I-Cheng Chang National Dong Hwa University Taiwan
Chia-We Hsu National Dong Hwa University Taiwan
MP-L1-5 0038 POWER ESTIMATION FOR INTERACTIVE 3D GAME USING AN
EFFICIENT HIERARCHICAL-BASED FRAME WORKLOAD
PREDICTION
Da-Jing Zhang-Jian National Sun Yat-sen University Taiwan
Chung-Nan Lee National Sun Yat-sen University Taiwan
Ing-Jer Huang National Sun Yat-sen University Taiwan
Shiann-Rong Kuang National Sun Yat-sen University Taiwan
MP-L1-6 0084 AUTOMATIC GENERATION OF CHARACTER ANIMATIONS
EXPRESSING MUSIC FEATURES
Ka-Chon Loi National Chengchi University Taiwan
Tsai-Yen Li National Chengchi University Taiwan
8?? ????
?? ?????????
? APSIPA ASC Proceeding CD ??
Video Inpainting Based on Multi-Layer Approach
I-Cheng Chang* and Chia-We Hsu
Department of Computer Science and Information Engineering
 National Dong Hwa University, Hualien, Taiwan
Email: *icchang@mail.ndhu.edu.tw Tel: 886-3-8634022
Abstract—The objective of video inpainting is to fill the
removed region originally occupied by the selected object.
Because the selected object may occlude or be occluded by other
objects in the video, it is necessary to determine the sequence of
these objects to fill the removed region. In the paper, we present
a new framework for video inpainting which can repair the
selected region overlapped by multiple objects. Experimental
results demonstrate that we can arbitrarily remove any object in
a video and repair the removed area correctly.
I. INTRODUCTION
The topics of image inpainting and video inpainting attract
researchers’ attention since digital videos become more
popular. The major difference between these two techniques
is  that  a  single  frame lacks  of  the  temporal  information.  If  a
video contains the moving regions to be filled, it is not
possible to repair these regions by using image inpainting
technique. In the previous works, most of video inpainting
algorithms focus on repairing the removed region of a single
moving  object  in  a  video.  In  this  paper,  we  present  a  video
inpainting approach to fill the target video with region
occluded by multiple objects.
A. Related Work
Bertalmio [3] is the first pioneer to video inpainting. The
author repaired a video using image inpainting frame by
frame. Raphae1[4] used the proposed image inpainting
algorithm to repair a damaged frame, and then exploited it to
repair other damaged frames through frame alignment. Both
of them used PDE to find the structure information. For
repairing large miss area of a video, Wexler [5] defined an
optimal function to search the best matching patch in a
foreground and use the found patch to repair the moving
foreground. This algorithm filled the video frame patch by
patch. Jiaya [6] combined the inpainting algorithm ([1]) and
novel sample to repair the background and foreground
respectively. Snakjeev [7] adopted the image inpainting
algorithm of [2] to repair a video. Because of the color of a
video is easily interfered with the lighting, therefore, the
author used Fast Fourier Transform (FFT) to reduce the noise
of a video. In order to reduce the computation, Patwardhan
[8]-[11] established the foreground mosaic for repairing the
moving foreground. In addition, the works also adopted the
algorithm [2] to repair the background. Different from above
video inpainting system, the system [14] used the probability
model to repair the moving object.
B. System Framework
In the paper, we propose a novel video inpainting system.
Our system adopts the temporal information and uses
database of moving objects to repair removed region.
Especially, if there are multiple objects overlaid with the same
region, we are able to determine the present order of these
objects and rebuild the videos.
The framework of multi-layer video inpainting and
composition system is shown in Fig.1. This system consists of
two major parts: generation of depth map sequences and
multi-layer video inpainting. The first part aims at the object
segmentation and depth map computation. We use the
enhanced snake model to find the boundary of ROI, and then
the result of ROI is used to compute the depth map. Most of
the video inpainting algorithms cannot repair the removed
regions of multiple objects. Because it is difficult to
differentiate which object should be filled firstly when several
objects locate at the same region. Getting the depth
information of the corresponding objects is one way to solve
the problem. In the second part, we construct a database of
moving object, and then combined the depth information to
repair the removed region which is occluded by object. The
depth information can be used in video inpainting.
Fig. 1 System Framework of multi-layer video inpainting and composition.
                      (e)                                                (f)
Fig. 2 Results of foreground mask. (a) Background image. (b)The frame
contained foreground. (c)The result of background subtraction is derived
from  using  RGB  color  model.  (d)  The  result  of  noise  removal  (c).  (e)  The
extracted foreground by proposed method in YCbCr color model. (f) The
result of noise removal of (e).
We incorporate the foreground mask into the external force,
and the external force is defined as:.
2(| ( , ) | ( , ) ( , ))ext I fE G I x y EM x y EM x y?? ? ? ? ? ?             (8)
where the ( , )fEM x y  is the edge map of foreground mask.
The boundary found by enhanced Snake Model is shown in
Fig. 3(d). The foreground mask of the external force can
effectively reduce the influence of background intensity on
active contour.
(a)                                               (b)
(c)                                                (d)
Fig. 3 Find the boundary of ROI by using enhancement of Snake Model. (a)
Original image. (b) The boundary of ROI is found by only adding the Canny
edge to external force. (c) The foreground mask. (d) The boundary is found
by using enhanced Snake model.
B. Depth Map Computation
Two methods are commonly used to get depth information.
One approach is to use a hardware system to find the distance
between the camera and the objects by calculating the time of
reflected when the light is projected onto objects. The other
method used the stereo system to capture the images from the
target different views, and then to find the displacement of the
object. But the displacement would not be so accurate,
especially if the distribution of the vectors of the color is
smooth. Anantrasirichai [12] established a system with three
cameras to find the relation among objects. Lawrence Zitnick
[13] used dynamic block to increase the accuracy. The block
size is depended on the result of color segmentation during
the processing.
In order to effectively get the depth information, we apply
the displacement vector field with the result of object
segmentation. Two cameras, primary and secondary cameras,
form a stereo system. The disparity between stereo images is
computed by using block matching. The image captured by
the primary camera is divided into N N? blocks, and then we
search the correspondence block in the image captured by
secondary camera. And, the sum absolute difference (SAD) is
used to measure the similarity of these blocks. The disparity
( , )Dis x y of a block ( , )P x y is defined as follows:
,
1 1
0 0
ˆ( , ) | ( , ) ( , ) |
ˆˆ ( , ) arg min ( ( , ), ( , ))
ˆ( , ) ( , ) ( , )
i h j v
N N
k l
Disp x y P x y P x i y j
P x i y j P x y S x i y j
S x i y j P x k y l S x k i y l j
? ?
? ?
? ?
? ? ? ?
? ? ? ? ?
? ? ? ? ? ? ? ? ? ???
,      (9)
where ( , )Disp x y  is  the  disparity  of  a  block  and  (x,y)  is  the
position of the block center. ˆ( , )S x i y j? ? is the difference
from block ( , )P x y and block ( , )S x y in the corresponding
frame. The coordinate of most similar block is record in
ˆ ( , )P x y . The result of block matching is shown in Fig. 4(d).
If the color of a pixel is closer to red and the value of disparity
of a pixel is bigger, then the pixel is closer to camera. Fig. 4
(d) presents the rough depth information of Fig. 4(a). We use
the result of object segmentation to find the ROI displacement,
and count the displacement of the ROI to find the maximum
value  as  the  depth  of  ROI.  Fig.  4  (e)  shows  the  final  result.
The method can get the depth map sequence of a video and
also reduce the noise influence on the disparity maps.
(a)                                             (b)
(c)                                             (d)
(a)
(b)
(c)
Fig. 8. Remove the unwanted moving object and filling the dynamic miss
area of damaged frame. (a)The original sequences frame # 10, #20 and #30.
(b) The object is removed and masked with white. (c)The images are shown
the results of the inpainting dynamic miss area of damaged frames.
(b) Spatial approach
If the static object or damaged area occurs at the same
position during the whole video, there is no temporal
information for reference. For instance, all frames have a
pillar in the same location in a video (Fig. 9), where the
background information is unavailable. In the situation, the
temporal information is not enough to repair the damaged
frame. Therefore, the image inpainting algorithm is used to
reconstruct the missing area. And other damaged frames are
filled by copied the corresponding area from repaired
background, so as to maintain consistent background
throughout whole video.
(a)                            (b)                           (c)
Fig. 9 The result of using inpainting algorithm to reconstruct stationary
background. (a)Original image. (b)The unwanted area is masked with white.
(c)The result of reconstructed background.
B. Appropriate Template Selection from Moving Object
Database
The background information is generally not appropriate to
repair the region occupied by moving object. We use
undamaged frames to establish the moving object database,
where the damaged moving object can be repaired rapidly and
accurately. Fig. 10(a) shows the trajectory of a running kid
and we remove the pillar from background as Fig. 10 (b).
Because the kid is occluded by the pillar in some frames, we
search the kids in the other frames and use these images to
repair  the  lost  area.  In  order  to  build  up  the  database  of  all
moving object in the video, we record the size of minimum
boundary rectangle (MBR) of all candidates, and then use
their MBR to save all candidates as Fig. 10 (c).
(a)                                               (b)
(c)
Fig. 10. Use the max size of MBR to save all candidates in database. (a)The
trajectory of moving object. (b) The pillar is removed and we need to repair
the moving object in this area.(c) All candidates are saved with their MBR.
In Fig. 10, it is obvious that the MBR size of ROI is different.
Because the trajectory of person is not parallel with image
plane, all candidates are normalized by using Eq.(10) before
finding the best match template.
_ ' _  ,  _ ' _
_ _
  ,
_ _
x y
j j
x y
i i
j j j jL MBR L MBR W MBR W MBR
L MBR W MBR
L MBR W MBR
? ?
? ?
? ? ? ?
? ?
(10)
where, the x?  and y?  are the scale operators for the length and
width of image. _ iL MBR  and _ jL MBR are the length of
MBR related to object i and object j, respectively. _ iW MBR
and _ jW MBR are the width of MBR of object i and object j.
The sizes of all candidates were multiplied by x?  and y? .
Additionally, in order to calculate the objects shift
information, we also record the coordinates of all candidates
in the original video.
Because human walking is periodic motion, the position at
one point in some walking cycle can be found in another
cycle walking cycle. However the motion in the two half
periods will not be the same since the movement of left and
right legs is slightly different. For example, Fig. 11(a) and (b)
show the human walking for different leading legs, where the
body  center  in  the  MBR  is  moving  forth  and  back.  If  the
quality q is defined as the distance between the body center
and the left boundary of MBR, the variation of q during a
walking period can be represented as Fig. 11(c).
(a)
(b)
(c)
Fig. 14  Repair the moving foreground and static background. (a)The first
row is original frames of a video.(b) The removed pillar is marked with white
in middle row.(c) The result frames.
Experiment 2
We also remove the pillar of the background and repair the
occluded kid. But, the trajectory is not parallel to image plane.
Because the region of the object and the shift information
changes as kid more and more closes to camera. The
candidates need normalization before the procedures of
finding matching point. And the displacement also needs
adjustment in filing the miss trajectory too. In Fig. 15(b), the
kid is running rapidly and it will increase the difficult of
object segmentation. But the object is still segmented well by
enhancement Snake model. And the repaired moving object is
consistent in the sequences in Fig. 15 (c).
(a)
(b)
(c)
Fig. 15  Remove the static object and repaired occlusion. In addition, the
trajectory of moving is not parallel with the image. (a) The removed object is
mark with white .The first row is shown original frames. (b) The first row is
shown original frames. (c)And the repaired frames are shown in last.
Experiment 3
Our system can not only remove the static object in a video,
but also remove the moving objects of multiple layers. The
moving object on the first layer is removed and the occluded
object is repaired. Moreover, we use the object segmentation
to find the ground line of this object and remove its shadow.
The results of repaired frames are show in Fig.16.
(a)
(b)
(c)
Fig. 16  Remove the first layer moving and repair the occluded moving
object.(a) The original frames .(b) The removed object is depicted as white
area. (c) The result repaired frames.
V. CONCLUSIONS
In the paper, we present a new system framework for video
inpainting. Different from the previous works, our system can
repair the damaged video with more than two moving objects.
Instead of repairing the object patch by patch, we reconstruct
the occluded foreground using the object database. The
approach can complete a damaged video effectively. In
experimental results, we demonstrate that the objects can be
arbitrary removed in a video and the system can repair the
removed area well.
In the system, the method of finding besting point only
depends on the shape of the candidates. Although we have
constraints on selection of the target template; however, if the
result of the object segmentation is not accuracy enough, it
will increase the error rate of finding best matching point. In
the future, we want to add other features in finding the leading
leg of walking people to reduce the error. Furthermore, we
無衍生研發成果推廣資料
研討會論文 1 0 100% 
’Multi-Layer based Video 
Inpainting of Occluded 
Objects’, Asia-Pacific Signal 
and Information Processing 
Association Annual Summit and 
Conference (APSIPA ASC 2009), 
Japan, Oct 4-7, 2009. 
This work was supported by the 
Ministry of Economic Affairs 
under   Grant   No. 
97-EC-17-A-02-S1-032,   and 
National Science  Council 
under  Grant  No. 
NSC-98-2221-E-259-026,Taiwan, 
ROC. 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
參與計畫人
力 
（外國籍） 
專任助理 0 0 100% 
人次
 
其他成果 
(無法以量化表達
之成果如辦理學
術活動、獲得獎
項、重要國際合
作、研究成果國際
影響力及其他協
助產業技術發展
之具體效益事項
等，請以文字敘述
填列。) 
無 
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
