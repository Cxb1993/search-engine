 ii
出的設計。在第一種解碼法中，被產生的隨機樣本代表了一隨機錯誤指標向量集
合，其中每一個向量都指出了接收字碼中 n-k 個應該被擦拭的位置。我們將接
收字元中被指定的相對位置擦拭後即可利用只具擦拭（Erasures-Only）解碼器
還原成候選碼。在第二種方法中，n維的實數隨機向量被產生並代表著接收字碼
的可靠度向量，而其中 n-k 個最不可靠的座標會假設為應該要被擦拭。針對每個
隨機樣本，我們將其 k 個最可靠的座標做硬式決策（hard-decision）並利用只
具擦拭解碼器將其還原為合法碼。第三種演算法利用一連續位元翻轉演算法來將
隨機樣本向量轉換為合法碼。值得一提的是前兩種演算法只對可最大距離分離
（Maximum-Distance Separable）碼有用而第三種演算法則沒有這個限制。這些
演算法相對於信度傳遞演算法與部分現有的代數演算法提供了性能與複雜度上
的改善，尤其是針對具有高碼率、高密度位元檢測矩陣的方塊碼。 
 
關鍵字：低密度檢測矩陣碼、自由能、退火式信度傳遞演算法、高密度檢測矩陣、
交錯熵、庫柏克萊不勒距離、里得所羅門碼、隨機解碼法。 
the median vector will move to or the underlying distribution will eventually degenerate
to a singularity at the transmitted codeword, the parameters of the updated distribution
should be such that the new distribution is closest to the optimal distribution in the
sense of the Kullback-Leibler distance (i.e CE).
We propose three classes of stochastic decoding algorithms. The ﬁrst two are specif-
ically designed for decoding (n, k) Reed-Solomon (RS) codes. For the ﬁrst decoder, the
random samples represent a set of random error locator vectors, each indicates n − k
possible erasure positions within the received word. We associate each error locator
vector with a candidate codeword by erasures-only (EO) decoding the received word,
assuming that erasure locations are those indicated by the error locator vector. The
n-dimensional real random vectors in the second algorithm represent reliability vectors
whose least reliable n − k coordinates are assumed to be erasures. For each sample,
we make component-wise hard-decisions on the most reliable k coordinates and EO-
decoding the resulting binary vector. The third algorithm uses a sequential bit ﬂipping
algorithm to convert each random sample into a legitimate codewords. The ﬁrst two
algorithms are valid for MDS codes only while the third algorithm can be used for de-
coding any linear block code. Our algorithms oﬀer both complexity and performance
advantage over BP and some existing algebraic decoding algorithms, especially for high
rate linear block codes with HDPC matrices and short or medium lengths.
Index Terms-Low-density parity-check codes, free energy, annealing belief-propagation
algorithm, high-density parity-check matrix, cross-entropy, Kullback-Leibler distance,
Reed-Solomon codes, stochastic decoding algorithm.
iv
2.3 Annealing Belief Propagation Algorithm . . . . . . . . . . . . . . . . . . 19
2.3.1 Local Function Annealing . . . . . . . . . . . . . . . . . . . . . . 21
2.3.2 Factor Function Annealing . . . . . . . . . . . . . . . . . . . . . . 22
2.3.3 Joint Annealing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.4 Experimental Results and Discussions . . . . . . . . . . . . . . . . . . . . 24
3 The Cross-Entropy Method 27
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
3.2 The CE Method for Rare-Event Simulation . . . . . . . . . . . . . . . . . 28
3.3 The CE-Method for Optimization Problem . . . . . . . . . . . . . . . . . 33
3.4 Updating Rules of Some Useful Densities . . . . . . . . . . . . . . . . . . 35
4 Stochastic Erasure-Only List Decoding of RS codes 37
4.1 Preliminary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4.2 Stochastic List Decoding Algorithm . . . . . . . . . . . . . . . . . . . . . 39
4.2.1 Algebraic Erasures-Only (EO) Decoding . . . . . . . . . . . . . . 39
4.2.2 A Stochastic List Decoding Idea . . . . . . . . . . . . . . . . . . . 41
4.2.3 Convergence and Complexity . . . . . . . . . . . . . . . . . . . . 42
4.3 List Decoding via Erasure Location Estimation . . . . . . . . . . . . . . 44
4.3.1 Importance Density and Sample Format . . . . . . . . . . . . . . 44
4.3.2 Update Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . 45
vi
List of Figures
1.1 A correctly decoding example of a bounded distance decoder. . . . . . . . 3
1.2 An example of erroneous decoding for a bounded distance decoder. . . . 4
1.3 Decoding failure by a bounded distance decoder. . . . . . . . . . . . . . . 4
1.4 Decoding beyond FEC bound by enlarging the decoding sphere. . . . . . 5
1.5 Belief propagation - successful decoding. . . . . . . . . . . . . . . . . . . 6
1.6 Belief propagation - trapped in a pseudo codeword. . . . . . . . . . . . . 7
1.7 A set of random samples are generated and the random samples in the
small dash circle are better directions we want. . . . . . . . . . . . . . . . 8
1.8 After updating the parameter of the random mechanism, the new set of
generated random samples points the correct way more often. . . . . . . 8
2.1 Golay Code N=23 K=12, Code Rate: 0.522, Max Iteration Number: 200,
Frame Error Count: 200. . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.2 Mackay 816.33.164, Code Rate: 0.5, Max Iteration Number: 200, Frame
Error Count: 200. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
4.1 Idea of the algebraic erasures-only decoding. . . . . . . . . . . . . . . . . 40
4.2 Flow chart of a stochastic decoder for RS codes. . . . . . . . . . . . . . . 43
4.3 Virtual received words are generated around the received LLR vector Γ¯ by
hard-limiting the sample vectors generated by an importance probability
density whose parameter values evolved according to the CE principle. . 47
viii
List of Tables
4.1 A Stochastic List Decoding Algorithm. . . . . . . . . . . . . . . . . . . . 43
x
inference results at high temperature may be poor due to the unfaithful modelling of
actual free energy. On the other hand, there may be more than one local minimum at
low temperature. When searching on the energy cost surface at low temperature, we
can easily get stuck in one of these local extreme points. However, at low temperature,
inference results related with the global minimal free energy turn out to be much more
precise than that at the high temperature. Hence, the idea is to start the inference job at
a high temperature and smoothly decrease the temperature using some cooling strategies
to track the minimum point. Finally, when the temperature approaches to zero, stop
the algorithm and output the estimation results. Such a cooling strategy prevents BP
algorithms from sticking in some local minimum quickly, and helps the algorithms to
converge to the global minimum value with larger probability.
Linear block codes are popular forward error-correcting (FEC) codes due to their
simple structures and satisfactory FEC performance. For instance, Reed-Solomon (RS)
codes [5] are used in a wide variety of commercial applications, most prominently in CDs,
DVDs and Blue-ray discs, in data transmission technologies such as DSL andWiMAX, in
broadcast systems such as DVB and ATSC, and in computer applications such as RAID
6 systems. Low density parity-check (LDPC) codes [6] form another class of linear block
codes which oﬀer FEC capability close to the theoretical maximum–the Shannon limit
[7]. In recent years, LDPC codes have been adopted by several digital broadcast and
communication standards such as the DVB-S2 [8], the IEEE 802.3an (10GBASE-T) [9],
the IEEE 802.16e (WiMAX) [10], and the IEEE 802.11n (WiFi) [11]. Although many
decoding algorithms for block codes are available, more eﬃcient decoding algorithms
which can provide performance enhancement and complexity reduction are still of high
demand.
Most hard-decision decoding algorithms are bounded-distance decoders (BDD). They
select the codeword c, if exits, whose Hamming distance (HD) to the hard-limiting
received word z, say d(z, c), is less than or equal to (dmin− 1)/2 = tmin, where dmin is
2
cT c1
c2
d m in
z
Figure 1.2: An example of erroneous decoding for a bounded distance decoder.
cT c1
c2
d m in
z
Figure 1.3: Decoding failure by a bounded distance decoder.
4
cT c1
c2
dmin
z
z
z
z
Figure 1.5: Belief propagation - successful decoding.
cT ; see Fig. 1.5. Unfortunately, the BP process may be trapped in some local minimum
and the modiﬁed received vector coincides with a pseudo codeword cp as is shown in
Fig. 1.6. This phenomenon can be prevented by using some modiﬁcation of the BP
algorithms such as the annealed BP algorithm [25]. Another possible solution combines
the BP algorithm with the BDD such as the algorithms proposed in [12] and [26]. If
the pseudo codeword cp belongs to the decoding sphere of cT , successful decoding is
achieved although the BP algorithm makes z coincides with cp.
We investigate a novel idea of iterative decoding which is a randomized sphere de-
coding with moving center. If statistical information about possible locations of the
transmitted codeword cT around the received word z is given, the order of search should
follow the most possible direction. However, we don’t have such information usually
and hence we search follow a probability distribution which is learned by random sam-
pling. Each sample is transformed into a valid codeword and we choose samples whose
corresponding code words having smaller Euclidean distance (ED) to z to modify the
6
cT c1
c2
d m in
z
z
z
z
z z
Figure 1.7: A set of random samples are generated and the random samples in the small
dash circle are better directions we want.
cT c1
c2
d m in
z
z
z
z
z
z
Figure 1.8: After updating the parameter of the random mechanism, the new set of
generated random samples points the correct way more often.
8
Chapter 2
Constraint Relaxation and Annealed
Belief Propagation for Binary
Networks
In section I, we ﬁrst formulate the free energy of a belief network by taking into consid-
eration the temperature eﬀect. Next, we conduct the BP algorithms using the param-
eterized free energy in section II. Section III develops several cooling strategies when
dealing with binary belief networks, which are commonly used when decoding error
correcting codes especially for turbo-like codes. Section IV discusses several cooling
schedules and their performances. In section V, we gives the simulation results of the
proposed annealing algorithms applied to error correcting codes. Finally, we conclude
the works.
2.1 Minimum Free Energy under Bethe Approxima-
tion
It is well known that the conventional SPA can be derived from the minimization of Bethe
free energy when the temperature equals to one [1]. When developing the annealing
type BP algorithms, we begin with the Bethe free energy formulation, which retains the
temperature parameter. Considering a factor graph illustrated in Fig. 1, the Bethe free
10
where λμk, γμ, and γk are the Lagrange multipliers associated with constraints (2.6),(2.7),
and (2.8) respectively. N (k) denotes the set of neighbors of xk.
To ﬁnd the minimum value of LBethe, we ﬁrst take derivatives with respect to beliefs
b(xμ) and b(xk), and set the results to zero. We have
∂LBethe
∂bμ(xμ)
= − ln fμ(xμ) + T [1 + ln bμ(xμ)]− γμ −
∑
k∈N (μ)
λμk(xk) = 0. (2.10)
∂LBethe
∂bk(xk)
= − ln gk(xk) + Tck [1 + ln bk(xk)]− γk +
∑
μ∈N (k)
λμk(xk) = 0 (2.11)
Solving (2.10) and (2.11), we have the optimum values of beliefs
b(∗)μ (xμ) = f
β
μ (xμ) exp
⎧⎨
⎩β
⎡
⎣γμ + ∑
k∈N (μ)
λμk(xk)
⎤
⎦− 1
⎫⎬
⎭ (2.12)
b
(∗)
k (xk) = g
β
ck
k (xk) exp
⎧⎨
⎩ βck
⎡
⎣γk − ∑
μ∈N (k)
λμk(xk)
⎤
⎦− 1
⎫⎬
⎭ , (2.13)
where β
def
= 1/T is a parameter used to model the temperature eﬀect.
To conduct the BP algorithms, we have to derive the Lagrange duality function of
the Bethe free energy (2.9). The dual function GBethe is deﬁned as
GBethe
def
= inf
bμ(xμ),bk(xk)
LBethe (2.14)
Substituting (2.12) and (2.13) back to LBethe, we have
GBethe = LBethe(b
(∗)
μ (xμ), b
(∗)
k (xk), λμk(xk), γμ, γk)
= −
∑
μ
∑
xμ
b(∗)μ (xμ) ln fμ(xμ)−
∑
k
∑
xk
b
(∗)
k (xk) ln gk(xk)
+T
∑
μ
∑
xμ
b(∗)μ (xμ) ln b
(∗)
μ (xμ) + T
∑
k
ck
∑
xk
b
(∗)
k (xk) ln b
(∗)
k (xk)
+
∑
k
∑
μ∈N (k)
∑
xk
λμk(xk)
⎡
⎣b(∗)k (xk)− ∑
xμ\xk
b(∗)μ (xμ)
⎤
⎦
+
∑
μ
γμ
⎡
⎣1−∑
xμ
b(∗)μ (xμ)
⎤
⎦+∑
k
γk
[
1−
∑
xk
b
(∗)
k (xk)
]
(2.15)
12
k, and nkμ as message passing from variable node k to factor node μ,
mμk(xk)
def
= g
β
ck
k (xk) exp
⎡
⎣−βλμk(xk)− β
ck
∑
ξ∈N (k)
λξk(xk)
⎤
⎦ (2.23)
(2.18)
=
∑
xμ\xk
⎧⎨
⎩fβμ (xμ) exp
⎡
⎣β
⎛
⎝γμ + ∑
j∈N (μ)\k
λμj(xj)
⎞
⎠− β
ck
γk
⎤
⎦
⎫⎬
⎭ (2.24)
nkμ(xk)
def
= exp [βλμk(xk)] (2.25)
Combining (2.24) and (2.25), we have
mμk(xk) ∝
∑
xμ\xk
⎧⎨
⎩fβμ (xμ)
∏
j∈N (μ)\k
nμj(xj)
⎫⎬
⎭ . (2.26)
Also, from (2.23) and (2.25), message nkμ can be updated by
gβk (xk) ·
∏
ξ =μ,ξ∈N (k)
mξk(xk) = exp (βλμk(xk)) (2.27)
= nkμ(xk). (2.28)
Equations (2.26) and (2.28) recover the original BP algorithm when β equals to 1.
According to (2.13) and (2.23), belief of the variable node can be estimated as
b
(∗)
k (xk) ∝ mμk(xk) · exp(βλμk(xk)) (2.29)
= gβk (xk) ·mμk ·
∏
ξ =μ,ξ∈N (k)
mξk(xk) (2.30)
= gβk (xk)
∏
μ∈N (k)
mμk (2.31)
From (2.26) and (2.28), we conclude that the BP algorithm can be generalized by
incorporating a temperature parameter T , and replace the factor function fμ(xμ) by
fβμ (xμ), and local function gk(xk) by g
β
k (xk).
2.2 Annealing Belief Propagation for Binary Net-
work
From the discussions in section II, we know that the ﬁrst step to implement the annealing
type BP algorithm is to parameterize the original local and factor functions with an
14
Equation (2.36) generalizes the statistical modelling of variable function. Obviously,
the conventional likelihood ratio lk is only a special case when T → 1. As T → 0, which
means the code node is in a low temperature state. The generalized likelihood ratio
lβk approaches an impulse function and gives a deterministic decision of yk. Otherwise,
when T → +∞, the likelihood ratio tends to be a constant. That means every code
node is equally probable when the artiﬁcial temperature T approaches to inﬁnity. In
other words, by changing the temperature parameter T , we actually model the local
likelihood ratio lβk in diﬀerent quantization precisions.
2.2.2 Factor Function Modelling
When decoding an error correcting code, factor function fμ(xμ) can be described by an
indicator function of parity check equation associated with vector xμ, i.e.,
fμ(xμ) = δ(
{⊕j∈N (μ)cj}), where cj =
{
0 , if xj = 1
1 , if xj = −1 . (2.38)
⊕ is the exclusive-or operation, and δ(·) is the delta function
δ(y) =
{
1 , if y = 0
0 , otherwise
. (2.39)
Since a delta function will not changed by any power other than zero, i.e., δ(·) = δβ(·)
for β > 0, it is meaningless to replace the conventional factor function fμ(xμ) by f
β
μ (xμ).
An alternative way to parameterize the factor function is to approximate a delta
function by the limit of a Gaussian density of a real variable. Alternatively, the Dirac
delta function can be rewritten as
δ(y) = lim
β→∞
√
β
2π
exp
(
−β
2
y2
)
. (2.40)
In (2.40), since the delta function is just a limit case when β → ∞, we can generalize it
by deﬁning a new function δβ(·) as
δβ(y)
def
=
√
β
2π
exp
(
−β
2
y2
)
. (2.41)
16
Example
Consider a factor node fβμ (xμ) associated with a parity check equation
c1 ⊕ c2 ⊕ c3 = 0. (2.47)
According to (2.42), we have
mμ1,β(x1 = 1) ∝ n2μ(x2 = 1)n3μ(x3 = 1)e−β2 ·0 + n2μ(x2 = −1)n3μ(x3 = 1)e−β2 ·1
+ n2μ(x2 = 1)n3μ(x3 = −1)e−
β
2
·1 + n2μ(x2 = −1)n3μ(x3 = −1)e−
β
2
·0(2.48)
mμ1,β(x1 = −1) ∝ n2μ(x2 = 1)n3μ(x3 = 1)e−
β
2
·1 + n2μ(x2 = −1)n3μ(x3 = 1)e−
β
2
·0
+ n2μ(x2 = 1)n3μ(x3 = −1)e−
β
2
·0 + n2μ(x2 = −1)n3μ(x3 = −1)e−
β
2
·1(2.49)
In (2.48) and (2.49), we ﬁnd that when β 
= +∞, i.e., T 
= 0+, there is a non-
zero probability e−
β
2 accompanied with the terms that disappear in the calculation of
mμ1 in the deterministic case. Such a probability explains the idea of a “stochastic”
parity check, since we allow the existence of non-zero parity check value with a speciﬁed
probability.
Interestingly, when we decrease the temperature T from +∞ to 0+, stochastic parity
check will reach two limit states. First, when T = +∞ (i.e., β = 0+), both e−β2 ·0 and
e−
β
2
·1 equal to 1 such that mμ1(xk = 1) = mμ1(xk = −1). Hence, the message provided
by check node μ at T = +∞ is equally probable. That means we cannot gain any
information about code node x1 from f
β
μ .
On the other hand, as T = 0+ (i.e., β = +∞), e−β2 ·0 = 1 and e−β2 ·1 = 0. Message
mμ1,β(xk) will become
mμ1,β=0(x1 = 1) ∝ n2μ(x2 = 1)n3μ(x3 = 1) + n2μ(x2 = −1)n3μ(x3 = −1)(2.50)
mμ1,β=0(x1 = −1) ∝ n2μ(x2 = −1)n3μ(x3 = 1) + n2μ(x2 = 1)n3μ(x3 = −1),(2.51)
which gives exactly the deterministic parity check function. Hence, as T → 0, the
stochastic parity check will approach the deterministic case. This can also be explained
as the natural result of (2.40).
18
constraints on the decoding delay in order to maintain the total system latency. Second,
DA combined with the proposed annealing belief propagation (ABP) algorithms needs
only minor modiﬁcation of the conventional BP. In fact, ABP generalizes BP while
keeping most of the implementations details, including schedules of message passing,
the way that message exchanges, or even total iteration numbers. We describe below
the basic ABP framework.
1. Initialize :
Set initial temperature Tinit, minimum temperature Tmin.
Set maximum iteration number imax, iteration index i = 1, temperature index
Ti = Tinit, and βi =
1
Ti
.
2. Update :
Calculate the local function gβik (xk) of variable node xk for all k.
Calculate the factor function fβiμ (xμ) of the μ-th factor node for all xμ.
Calculate messages mμk,βi(xk) and nkμ,βi(xk) for all μ, k.
3. Check Temperature : If Ti ≤ Tmin, set Ti = 0.
4. Cooling Step : Calculate Ti+1 and βi+1 by decreasing Ti according to the speciﬁc
annealing schedule.
5. Check Status : Set i = i+1. Stop the algorithm when i > imax or some convergent
check is passed.
6. Go to 2).
Following the basic framework of ABP, we propose three types of ABP algorithms:
local function annealing (LFA), factor function annealing (FFA), and joint annealing
(JA). For diﬀerent types of ABP algorithms, the function nodes are generalized according
to diﬀerent models described in section III.
20
limit, in few iterations.
2.3.2 Factor Function Annealing
Similarly, factor function annealing is also called check node annealing when applying
ABP to decode error correcting codes. The reason for the name “factor function an-
nealing” is that we only perform annealing on the factor nodes. For LDPCC, we replace
the deterministic parity check function by the stochastic one described in Section III-B.
Local function gk(xk) is kept constant during the iteration, i.e.,
gk(xk) = δ
(
p(xk|yk) =
√
1
2π
exp
(−(yk − xk)2
2σ2
))
. (2.54)
According to (2.45), the generalized parity check function reaches two limits when
T = +∞, and T = 0. Hence, we suggest the following geometric schedule for FFA
Tfactor,(j+1)η = α
j+1 · Tfactor,init = α · Tfactor,jη, (2.55)
and Tfactor,i = Tfactor,jη for jη ≤ i < (j + 1)η. (2.56)
Similar to LFA, j is the update index and η is the corresponding update period of
the annealing process. i denotes the iteration index of ABP algorithms and the initial
condition is set as T0
def
= Tfactor,init. 0 < α ≤ 1 is a scaling constant that controls the
annealing rate. As α = 1, temperature Tfactor,i equals to Tfactor,init for all iteration. On
the other way, Tfactor,i approaches 0 in few iterations when α is small. Empirically, the
geometric scaling factor α is set between 0.8 and 0.99 [29].
A stopping criterion of the LFA and FFA can be determined either when the maxi-
mum iteration limit is reached or all the deterministic parity check functions are satisﬁed.
2.3.3 Joint Annealing
Combining LFA and FFA, joint annealing anneals both the local functions and the factor
functions. Temperature Ti is decreased according to (2.52) and (2.55) for local functions
and factor functions respectively. Intuitively, there are several ways to combine the LFA
and FFA. We suggest the following two approaches.
22
(3) Set Tfactor,i = Tfactor,jη for iteration i = jηfactor. If Tfactor,i < Tfactor,min, set
Tfactor,i = 0.
(4) Run FFA on factor nodes and LFA on variable nodes.
(5) Set i ← i+ 1.
(6) If i 
= (j + 1)ηfactor and i 
= (k + 1)ηlocal, go to step (4). If i = (j + 1)ηfactor and
i 
= (k + 1)ηlocal, set j ← j + 1, lower temperature of factor functions according to
(2.55). and goto step 3; else if i = (k + 1)ηlocal, goto step 7.
(7) Set k ← k + 1, and lower temperature of local functions according to (2.52).
(8) If i < imax, goto step 2; else, stop and output the results.
DLJA is consisted of two main loops. Inner loop, step 3 to step 6, performs FFA
and outer loop, i.e., step 2 to step 8, performs LFA. In step 2, the beginning of outer
loop, we can also re-heating Tfactor,i to further escape the local minimum and possibly
ﬁnd better solutions. In DLJA algorithm, we can also run LFA as inner loop and FFA
as outer loop. In fact, there are lots of conﬁgurations of joint annealing methods, which
need to be investigate further.
From the simulation results in the latter section, we observe that performance of FFA,
LFA and JA is mainly aﬀected by the settings of initial temperature and annealing rate.
That means we can get improvement of the conventional BP even by using the simplest
LFA scheme.
2.4 Experimental Results and Discussions
In this section, we report the results of the proposed annealing algorithms on binary
networks. We perform ABP to decode LDPCC of diﬀerent rates and code lengths. Some
test codes are chosen for their inherent short cycles, while others are chosen arbitrarily.
24
1.8
2.0
2.2
2.4
1E
-5
1E
-4
1E
-3
Bit Error Rate (BER)
S
N
R
 (dB
)
 B
P
 LFA
, γ=1.1, T
init =2
 F
F
A
, α=0.94, T
init =1
F
igu
re
2.2:
M
ackay
816.33.164,
C
o
d
e
R
ate:
0.5,
M
ax
Iteration
N
u
m
b
er:
200,
F
ram
e
E
rror
C
ou
n
t:
200.
26
can be readily applied by ﬁrst translating the underlying optimization problem into
an associated estimation problem, named associated stochastic problem (ASP), which
typically involves rare-event estimation. Estimating the rare-event probability and the
associated optimal reference parameter for the ASP via the CE method translates eﬀec-
tively back into solving the original optimization problem.
In general, the CE algorithm is an iterative procedure that consists of the following
two phases in each iteration.
• Generate samples from the speciﬁed importance density given by the parameters
from the previous iteration.
• Update the parameters for next iteration according to the order of the score values
associated with the drawn samples and the minimizing CE criterion.
The signiﬁcance of the CE concept is that it deﬁnes a precise mathematical framework
for deriving fast and good updating/learning rules.
3.2 The CE Method for Rare-Event Simulation
In this section, the basic idea behind the CE algorithm for rare event simulation is
illustrated. Let x be a random vector taking values in some space X . Let {f(·;v)} be a
family of probability density functions (pdfs) on X , with respect to some base measure
μ where v is a real-valued parameter (vector). Therefore,
E[H(x)] =
∫
X
H(x)f(x;v)μ(dx), (3.1)
for any function H . For simplicity, for the rest of this section we take μ(dx) = dx
because of μ is either a continuous measure or the Lebesgue measure in most cases.
Let S be some real function on X . Suppose we are interested in the probability that
S(x) is greater than or equal to some real number γ under f(x;u). This probability can
be expressed as
 = Pu(S(x) ≥ γ) = Eu[I{S(x)≥γ}]. (3.2)
28
which is also termed the cross-entropy (CE) between g and h.
Minimizing the Kullback-Leibler distance between g∗ in (3.5) and f(x;v) is equiva-
lent to solve the maximization problem
max
v
∫
g∗(x) ln f(x;v)dx (3.8)
Substituting g∗ from (3.5) into (3.8) we obtain the maximization program
max
v
∫
I{S(x)≥γ}f(x;u)

ln f(x;v)dx (3.9)
which is equivalent to the program
max
v
D(v) = max
v
Eu
[
I{S(x)≥γ} ln f(x;v)
]
(3.10)
where D is implicitly deﬁned above. Again using importance sampling, with a change
of measure f(x;w) we can rewrite (3.10) as
max
v
D(v) = max
v
Ew
[
I{S(x)≥γ}W (x;u,w) ln f(x;v)
]
, (3.11)
for any reference parameter w, where
W (x;u,w) =
f(x;u)
f(x;w)
(3.12)
is the likelihood ratio between f(x;u) and f(x;w). The optimal solution of (3.11) can
be written as
v∗ = argmax
v
Ew
[
I{S(x)≥γ}W (x;u,w) ln f(x;v)
]
. (3.13)
We may estimate v∗ by solving the following stochastic program
max
v
Dˆ(v) = max
v
1
N
N∑
i=1
[
I{S(xi)≥γ}W (xi;u,w) ln f(xi;v)
]
, (3.14)
where x1, · · · ,xN is a random sample from f(x;w). In typical applications the function
Dˆ in (3.14) is convex and diﬀerentiable with respect to v, in which case the solution of
(3.14) may be readily obtained by solving the following system of equations:
1
N
N∑
i=1
[
I{S(xi)≥γ}W (xi;u,w)∇ ln f(xi;v)
]
= 0. (3.15)
30
quantile as
γˆt = S((1−)N	) (3.18)
Note that S(j) is called the j-th order-statistic of the sequence S(x1), · · · , S(xN).
Note also that γˆt is chosen such that the event {S(x) ≥ γˆt} is not too rare (it has
a probability of around ), and therefore updating the reference parameter via a
procedure such as (3.18) is not void of meaning.
2. Adaptive updating of vt For ﬁxed γt and vt−1, derive vt from the solution of
the following CE program
maxvD(v) = Evt−1
[
I{S(x)≥γt}W (x;u,vt−1) ln f(x;v)
]
. (3.19)
The stochastic counterpart of the above equation is as follows: for ﬁxed γˆt and
vˆt−1, derive vˆ from the solution of following program
max
v
Dˆ(v) = max
v
1
N
N∑
i=1
[
I{S(xi)≥γˆt}W (xi;u, vˆt−1) ln f(xi;v)
]
. (3.20)
Thus, at the ﬁrst iteration, starting with vˆ0 = u, to get a good estimate for vˆ1,
the target event is artiﬁcially made less rare by (temporarily) using a level γˆ1 which is
chosen smaller than γ. The value of vˆ1 obtained in this way will (hopefully) make the
event {s(x) ≥ γ} less rare in the next iteration, so in the next iteration a value γˆ2 can
be used which is closer to γ itself. The algorithm terminates when at some iteration t a
level is reached which is at least γ and thus the original value of γ can be used without
getting too few samples.
The above rationale results in the following algorithm:
1. Deﬁne vˆ0 = u. Set t = 1.
2. Generate a sample z1, · · · ,xN from the density f(x;vt−1) and compute the sample
(1−)-quantile γˆt of the performances according to (3.18), provided γˆt is less than
γ. Otherwise set γˆt = γ.
32
and that f(x;u) is the uniform density on X . Note that, typically, (γ∗) = f(x∗;u) =
1/|X | where |X | denotes the number of elements in X is a very small number. Thus,
for γ = γ∗ a natural way to estimate (γ) would be to use the LR estimator (3.21) with
reference parameter v∗ given by
v∗ = argmax
v
Eu
[
I{S(x)≥γ} ln f(x;v)
]
. (3.24)
This parameter could be estimated by
vˆ∗ = argmax
v
1
N
[
I{S(xi)≥γ} ln f(xi;v)
]
(3.25)
where the xi are generated from pdf f(x;u). It is plausible that, if γ is close to γ
∗,
that f(x;v∗) assigns most of its probability mass close to x∗, and thus can be used to
generate an approximate solution to (3.22). However, it is important to note that the
estimator (3.25) is only of practical use when I{S(x)≥γ} = 1 for enough samples. This
means for example that when γ is close to γ∗, u needs to be such that Pu(S(x) ≥ γ)
is not too small. Thus, the choice of u and γ in (3.22) are closely related. On the one
hand we would like to choose γ as close as possible to γ∗, and ﬁnd (an estimate of) v∗
via the procedure above, which assigns almost all mass to state(s) close to the optimal
state. On the other hand, we would like to keep γ relative large in order to obtain an
accurate estimator for v∗.
The situation is very similar to the rare-event simulation case. The idea is to adopt
a two-phase multilevel approach in which we simultaneously construct a sequence of
levels γˆ1, γˆ2, · · · , γˆT and parameter (vectors) vˆ0, vˆ1, · · · , vˆT such that γˆT is close to the
optimal γ∗ and vˆT is such that the corresponding density assigns high probability mass
to the collection of states that give a high performance.
This strategy is embodied in the following procedure:
1. Deﬁne vˆ0 = u. Set t = 1.
2. Generate a sample x1, · · · ,xN from the density f(x;vt−1) and compute the sample
(1− )-quantile γˆt of the performance according to (3.18).
34
Suppose the random vector xi = (xi1, · · · , xin) ∼ Ber(p) where Ber(p) is Bernoulli
distribution with parameter p = (p1, · · · , pn). Consequently, the pdf is
f(xi;p) =
n∏
j=1
p
xij
i (1− pi)1−xij , (3.27)
and since each xij can only be 0 or 1,
∂
∂pj
ln f(xi;p) =
xij
pj
− 1− xij
1− pj
=
1
(1− pj)pj (xij − pj). (3.28)
Now we can ﬁnd the maximum in (3.20) (with W = 1) by setting the ﬁrst derivatives
with respect to pj equal to zero, for j = 1, · · · , n:
∂
∂pj
N∑
i=1
I{S(xi)≥γ} ln f(xi;p) =
1
(1− pj)pj
N∑
i=1
I{S(xi)≥γ}(xij − pj) = 0. (3.29)
Thus, we get the updating rule
pj =
∑N
i=1 I{S(xi)≥γ}xij∑N
i=1 I{S(xi)≥γ}
. (3.30)
Next, consider the Gaussian density
f(x;μ, σ2) =
1√
2πσ2
e−
1
2
(x−μ)2
σ2 , x ∈ R. (3.31)
The optimal solution of (3.20) (with W = 1) follows from minimization of
1
σ2
N∑
i=1
Ii(xi − μ)2 + ln(σ2)
N∑
i=1
Ii, (3.32)
where Ii = I{S(xi)≥γ}. It is easily seen that this minimum is obtained at (μˆ, σˆ
2) given by
μˆ =
∑N
i=1 Iixi∑N
i=1 Ii
(3.33)
and
σˆ2 =
∑N
i=1 Ii(xi − μˆ)2∑N
i=1 Ii
(3.34)
36
codeword c¯ into the bipolar vector
Ψ(c¯) = x¯ = (x¯0, · · · , x¯nm−1), x¯j = Ψ(c¯j) = (−1)c¯j (4.1)
and sends it over an additive white Gaussian noise (AWGN) channel with zero mean
and power spectral density N0/2. The received sequence at the output of the matched
ﬁlter is y¯ = (y¯0, · · · , y¯nm−1) where y¯j = x¯j + w¯j and w¯j’s are statistically independent
Gaussian random variables with zero mean and variance N0/2.
Let z¯ = (z¯0, · · · , z¯nm−1) be the hard decision binary vector of the received bit sequence
y¯, i.e.,
z¯j =
{
0, y¯j > 0
1, otherwise
(4.2)
and z = (z0, · · · , zn−1) be the corresponding symbol vector. Denoted by Γ¯ = (γ¯1, · · · , γ¯nm−1)
the reliability vector of y¯ in which γ¯j is the magnitude of the log-likelihood ratio (LLR)
associated with the corresponding hard-limited bit z¯j
L (c¯j) = log
P ( c¯j = 0| y¯)
P ( c¯j = 1| y¯) , (4.3)
and deﬁne the symbol reliability vector Γ = (γ0, · · · , γn−1) of z by
γi = min
j
γ¯j, j ∈ {im, · · · , (i+ 1)m− 1} (4.4)
Assume that the ith symbol ci of c is uniformly distributed over GF(2
m) and the n
received symbols are independent and uniformly drawn from GF(2m). Then P (ci = β|y¯),
the probability that ci = β was transmitted given the observation y¯ can be easily
evaluated [21]:
P (ci = β|y¯) = P (ci = β|y¯i)
=
P (y¯i|ci = β)P (ci = β)∑
ω∈GF(2m) P (y¯i|ci = ω)P (ci = ω)
=
P (y¯i|ci = β)∑
ω∈GF(2m) P (y¯i|ci = ω)
(4.5)
38
n-kz
: vectors  with Hamming dis tance n-k  away from z
: codewords belong to Cz
: transmitted codeword
: re-encode process
Figure 4.1: Idea of the algebraic erasures-only decoding.
40
and compute an initial estimate ηˆ for η. Ideally, we can use those drawn samples which
satisfy d(Ψ(c¯s), y¯) ≤ ηˆ to obtain new parameter value vˆ′ such that f(s; vˆ′) is closest to
f(s;v∗) in the Kullback-Leibler (KL) sense, i.e., the CE between f(s; vˆ′) and f(s;v∗)
is minimized. Since v∗ is unknown, we choose vˆ′ such that f(s; vˆ′) is closest to the
empirical distribution of s in those samples that are generated by f(s; vˆ) and satisfy
d(Ψ(c¯s), y¯) ≤ ηˆ for this empirical distribution is likely to be a good approximation of
f(s;v∗). New samples of s are then produced by the updated importance density f(s; vˆ′)
to compute new estimate ηˆ′. This iterative procedure continues until |ηˆ− ηˆ′| is less than
a predetermined threshold.
The above method is known as the CE method [33] which is an iterative procedure
that consists of the following two phases in each iteration.
• Generate samples from the speciﬁed importance density given by the parameters
from the previous iteration.
• Update the parameters for next iteration according to the order of the score values
associated with the drawn samples and the minimizing CE criterion.
Based on the above discussion, we propose a generic Monte Carlo based SDD algo-
rithm as shown in Fig. 4.2 and in Table 4.1 with some detailed description given in
Section 4.3 and 4.4.
4.2.3 Convergence and Complexity
Diﬀerent convergence conditions and results have been discussed for the deterministic CE
method and its extensions in [35] where it is also proved that convergence in distribution
or η can be guaranteed but needs a proper tuning of the parameters of the algorithm such
as the number of samples N , number of elites E, and smoothing factor ρ. Convergence to
the global minimum is ensured only if a large sample size N is used. On the other hand,
the computing complexity is related to N and is given by O(N(n − k)2). It is obvious
42
that the decoding performance can be improved by using a larger N . As we retain the
best sample at the end of each iteration, the decoding performance is also improved
by increasing the iteration number T . As an early-stopping at any iteration produces a
decoded codeword, we say the algorithm converges if the sequence of decoded codewords
converges. With a modest N , we found that the decoded codewords converge to the same
codeword within 10 iterations in most cases. Our algorithm yields good performance
although uniform convergence in distribution or η within a limited iterations is not
guaranteed.
4.3 List Decoding via Erasure Location Estimation
In this section, we propose an novel algorithm to solve the discrete optimization problem
described in (4.7) by utilizing the stochastic list decoding idea. This algorithm is named
as the ﬁrst kind stochastic erasure-only list decoding (SEOLD-I) algorithm which is used
to eﬃciently estimate the most possible dmin−1 locations of erasures about the received
word z.
4.3.1 Importance Density and Sample Format
Suppose the reliability matrix R is known at the receiver. Deﬁne the distrust function,
fd : GF(2
m) → (0,∞), of the ith coordinate zi of z as
fd(zi) =
∑
β Rβ,i
Rzi,i
, β ∈ GF(2m) \ {zi} (4.8)
The larger the value of fd(zi) is, the higher the probability that zi should be erased at
the decoder.
Let s = (s0, · · · , sn−1) be a random vector where s0, · · · , sn−1 are independent
Bernoulli random variables with success probabilities p0, · · · , pn−1, i.e., P (si = 1) =
1− P (si = 0) = pi. We write s ∼ Ber(p), where p = (p0, · · · , pn−1). In our case, si = 1
represents the ith symbol zi of z should be erased. On the other hand, zi will be reserved
because of higher reliability when si = 0.
44
4.4 List Decoding via Virtual Received Words
In Step 2 of Table 4.1 we try to ﬁnd the most likely message/error coordinates such
that the associated EO-decoded codeword is closest to the received vector. Note that
the random samples are used to determine the erasure locations only, and the searching
sphere of the algebraic list decoding described in Section 4.2.1 is always centered at the
hard-limited received word z with radius equals to n−k. To increase our search range and
improve decoding performance, we include some extra codewords which lie statistically
in a small neighborhood of the received word in our expanded search, such that some
of them may in fact be closer in ED to the true transmitted codeword c; see Fig. 4.3.
More speciﬁcally, the expansion is accomplished by eliminating the requirement that the
search be centered at z. Instead, we randomized the search center by EO-decoding the
hard-decision versions of the drawn sample vectors which we refer to as virtual received
words. If the importance density does converge to the desired density δ(s− s∗), such an
expanded search will eventually contract and converge to the true transmitted codeword.
4.4.1 Importance Density and Sample Format
Let s¯ = (s¯0, · · · , s¯nm−1) be a random vector where s¯0, · · · , s¯nm−1 are independent Gaus-
sian random variables with means μ0, · · · , μnm−1 and variances σ20, · · · , σ2nm−1. We write
s¯ ∼ N (μ, σ), where μ = (μ0, · · · , μnm−1) and σ = (σ0, · · · , σnm−1) are initialized by
μ
(0)
j = γ¯j (4.12)
σj
(0) =
√
|γ¯j| (4.13)
At the tth iteration, N random samples s¯
(t)
1 , s¯
(t)
2 , · · · , s¯(t)N are drawn from N
(
μ(t), σ(t)
)
to form the sample set S¯(t). Each sample vector represents the bit reliabilities of an
associated virtual received word. By using (4.4) to convert the bit reliabilities into
symbol reliability, the n−k coordinates with smallest symbol reliabilities are erased; the
46
and
σ
(t+1)
j = (1− η)σ(t)j + η ·
∑
s¯
(t)
 ∈S¯
(t)
E
(
s¯
(t)
	,j − μ(t+1)j
)2
E
(4.15)
where λ and η are real values between (0, 1) used to smooth the variation of these
parameters. The algorithm described in this section is called the second kind stochastic
erasures-only listing decoding (SEOLD-II) algorithm.
4.5 Experimental Results and Discussions
In this section, some simulated performance of two SEOLD algorithms (SEOLD-I and
SEOLD-II) are presented and compared with that of other well known RS decoding
algorithms. A standard binary input AWGN channel is assumed over which the BPSK
modulated codewords are transmitted. We model the receive matched ﬁlter output as the
sum of a ±1−valued sequence and Gaussian sequence with zero-mean i.i.d. components.
The average performance bounds on the ML error probability of RS codes over an AWGN
channel developed in [36] are used as the performance lower limits.
Due to the complexity and the decoding delay considerations, the SEOLD algorithms
will not terminate until convergence is assured. Instead, we limit our decoding procedure
to T iterations in all simulations.
Fig. 4.4 shows the codeword error rate (CER) performance of the (15,11) RS code
over an AWGN channel. HDD-BM refers to the performance of a hard decision bounded
minimum distance decoder such as the BM algorithm. GMD and KV refer to the per-
formance of the GMD algorithm proposed by Forney and the algebraic soft decision
decoding algorithm proposed by Koetter and Vardy, respectively. Note that the KV
algorithm concerned here is inﬁnite interpolation costs, i.e., the complexity is also inﬁ-
nite. For both SEOLD-I and SEOLD-II, the size of the sample set N and the size of the
elite set E at every iteration are set to be 20 and 6, respectively. After 10 iterations,
SEOLD-I has about 0.5 dB and 0.3 dB coding gain over GMD and KV at a CER of 10−5,
48
1 2 3 4 5 6 7 8
10-6
1x10 -5
1x10 -4
10-3
10-2
10-1
100
C
od
ew
or
d 
E
rr
or
 R
at
e
E
b
/N
0
 (dB)
 HDD-BM
 GMD
 KV
 SEOLD-I ( N =100)
 SEOLD-II ( N =100)
 SEOLD-II ( N =500)
 ML
Figure 4.5: Codeword error probability performance of the (31,25) Reed-Solomon code;
10 iterations.
50
transpose of H and 0 is a 1×M zero vector. For each row hm of H, m ∈ J , let
Cm = {c ∈ {0, 1}N : chTm = 0 mod 2}, (5.1)
then
C =
M⋂
m=1
Cm. (5.2)
Using the binary phase-shift-keying (BPSK) signal, the transmitter maps a codeword
c into the bipolar vector
Ψ(c) = x = (x0, · · · , xN−1), xn = Ψ(cn) = (−1)cn (5.3)
and sends it over an additive white Gaussian noise (AWGN) channel with zero mean and
power spectral density N0/2 W/Hz. The received sequence at the output of the matched
ﬁlter is given by y = (y0, · · · , yN−1), where yn = xn + wn and wn’s are statistically
independent Gaussian random variables with zero mean and variance N0/2.
Let z = (z0, · · · , zN−1) be the hard decision version of the received sequence y, i.e.,
zn =
{
0, yn > 0
1, otherwise
(5.4)
For m ∈ J , we deﬁne σm as the result of check sum-m based on the hard-decision vector
z:
σm =
⎡
⎣ ∑
n∈N (m)
znHmn
⎤
⎦ (mod 2) (5.5)
and deﬁne Σ = (σ1, · · · , σM) as the syndrome vector.
Denoted by Γ = (γn, · · · , γN−1) the reliability vector of y in which γn is the magnitude
of the log-likelihood ratio (LLR) associated with the corresponding hard-limited bit zn
Ln = log
P (cn = 0|y)
P (cn = 1|y) . (5.6)
We also denote L = (L1, · · · , LN) as the LLR vector of the received word.
52
Theorem 5.1 Given the received word y and the syndrome set Σn ≡ {σm : m ∈ M(n)},
the logarithm of the bit correctness probability ratio for bit n, say ξn, is
ξn = log
[
1− q˜n
q˜n
]
= log
[
P (zn = cn|y,Σn)
P (zn 
= cn|y,Σn)
]
∼= γn +
∑
m∈M(n)
[
(1− 2σm)
(
min
n′∈N (m)\n
γn′
)]
(5.10)
Proof :
See Appendix B.
5.2 Sequential Bit-Flipping Algorithm
In this section, we introduce a single-run sequential bit-ﬂipping (SBF) algorithm for
transforming z into a valid codeword. This procedure has a special constraint about
the parity-check matrix H that H has to be a systematic form. First of all, consider
the rows of the parity-check matrix H are linearly independent, i.e., M = N − K.
Using appropriate row operations, H can be transformed into a systematic form, say
H˜ = [IMP], where IM is an M ×M identity matrix and P is an M × (N −M) binary
matrix. Note that both H and H˜ are the null space of C, hence we can decode the
received word by using H˜ instead of H.
However, it is impossible to have this transformation when the rows of H are linearly
dependent, i.e., M > N −K. Fortunately, we can remove M −N +K rows of H which
can be represented by the linear combination of the remain rows to get a (N −K)×N
sub-matrix H′ where H′ has its systematic form H˜′.
Example 5.1 Consider the following parity check matrix:
H =
⎡
⎢⎢⎢⎢⎣
1 1 1 1 0 0 0 0 0 0
1 0 0 0 1 1 1 0 0 0
0 1 0 0 1 0 0 1 1 0
0 0 1 0 0 1 0 1 0 1
0 0 0 1 0 0 1 0 1 1
⎤
⎥⎥⎥⎥⎦ =
⎡
⎢⎣
h1
...
h5
⎤
⎥⎦ (5.11)
54
2. Let It = It−1∪N (o(t)). If dt−1 ∈ Co(t), let dt = dt−1. Otherwise, ﬁnd the solution,
say n∗, of
arg min
n∈{It\It−1}
ξn (5.15)
where ξn is evaluated by (5.10). Let
dt ← dt−1 + en∗ (mod 2), (5.16)
Lˆn∗ ← −Lˆn∗ , (5.17)
σm ← σm + 1 (mod 2) ∀m ∈ M(n∗), (5.18)
t ← t+ 1.
3. If t = M , stop the procedure and output both d = dM and Lˆ. Otherwise, go to
Step 2.
We denote the relationship between the inputs and outputs of the above procedure as
(d, Lˆ) = Ω(o,L) for simplicity.
Remark 5.1 We have to mention that once the number of error bits in {It\It−1} is
greater than or equal to 2, the output codeword dM won’t be the transmitted codeword.
Example 5.2 Consider a (8,4) linear block code with parity check matrix:
H =
⎡
⎢⎢⎣
1 0 0 0 1 1 1 0
0 0 1 0 1 0 1 1
0 1 0 0 1 1 0 1
0 0 0 1 0 1 1 1
⎤
⎥⎥⎦ (5.19)
Suppose all zero codeword is transmitted and the received word y is given by
y = (1.83, 2.07, 2.36,−0.21, 1.05, 1.91,−0.09, 1.63).
Then the hard limiting vector z is
z = (0, 0, 0, 1, 0, 0, 1, 0),
and an example of the SRSBFP is shown in Fig. 5.1 where the order of check sums are
3 → 2 → 1 → 4.
56
5.3 Predicament of Decoding via SBF algorithm
We have introduced a sequential bit ﬂipping procedure, say SRF algorithm, in previous
section. It is a simple uniﬁed framework for transforming the hard limiting vector z
into a codeword in C. Note that diﬀerent order may induce diﬀerent codeword to be
produced. For instance, the output codewords in Example 5.2 and 5.3 are diﬀerent
because of diﬀerent orders although they face the same received word y.
Example 5.3 Consider the same case described in Example 5.2. If we change the or-
der from 3 → 2 → 1 → 4 to 4 → 3 → 2 → 1, the output codeword d will become
(1, 0, 1, 1, 0, 0, 1, 0).
We observe that output codeword d in Example 5.2 is equal to the transmitted
codeword but in Example 5.3 is not. It is because the order used in Example 5.3 meets
the situation described in Remark 5.1. Obviously, it is a big problem if we want to
decode by SBF algorithm. Therefore, we try to solve this problem by the following two
ideas:
1. Find appropriate order to avoid the situation described in Remark 5.1.
2. Correct some error bits in advance such that the number of orders which can
decode the correct codeword increases.
Note that the ﬁrst idea is impractical because the complexity of ﬁnding appropriate
order grows quickly as M increases. Besides, the hardware implementation is ineﬃcient
if the order changes frequently. Consequently, we propose two modiﬁed methods for
decoding based on the SBF algorithm with a ﬁxed order. The ﬁrst one is designed
for cyclic codes that we apply the SBF algorithm to transform all of the cyclic shifted
received word into valid codewords. Note that cyclic shifting the received word is similar
to decode in diﬀerent order even though we don’t change the order actually. The another
method is to implement the second idea based on the concept of the randomized sphere
decoding with moving center which can correct errors iteratively.
58
2. Update the parameters of the random mechanism based on Es better candidates
in order to generate better virtual LLR vectors in next iteration.
Note that this is the basic idea of our randomized sphere decoding with moving center.
Next, we will illustrate the random mechanism further in next two subsections.
5.5.1 Importance Density and Sample Format
Let s = (s1, · · · , sN) be a random vector where s1, · · · , sN are independent Gaussian ran-
dom variables with means μ1, · · · , μN and variances ρ21, · · · , ρ2N . We write s ∼ N (μ, ρ),
where μ = (μ1, · · · , μN) and ρ = (ρ1, · · · , ρN) are initialized by
μ(0)n = Ln (5.21)
ρ(0)n =
4
N0
(5.22)
At the tth iteration, Ns random samples s
(t)
1 , s
(t)
2 , · · · , s(t)Ns are drawn from N
(
μ(t), ρ(t)
)
to form the sample set S(t). Each sample vector represents the LLRs of an associated
virtual received word. We decode them by the SBF algorithm based on an pre-designed
order and obtain sets of candidates d
(t)
	 and associated LLR vectors sˆ
(t)
	 = (sˆ	,1, · · · , sˆ	,N)
for 1 ≤  ≤ Ns.
5.5.2 Update Parameters
Let d
(t)
1 , · · · ,d(t)Ns be the output codewords of the SBF algorithm. We compute the ED
between each candidate codeword and the received word y and sort the corresponding
random vectors according to the descending order of their associated EDs. Deﬁne an
elite set E(t) which includes Es vectors with the smallest EDs to y, i.e., the corresponding
codewords are more likely to have been transmitted. We always store the best one in
E(t) up to the current iteration for the ﬁnal decision when the maximum number of
iteration is reached.
60
5.6 Experimental Results and Discussions
In the ﬁrst part of this section, some simulated performance of the proposed algorithm,
say SSBFA, are presented and compared with that of traditional bounded-distance de-
coding (BDD) and the sum-product algorithm (SPA). A standard binary input AWGN
channel is assumed over which the BPSK modulated codewords are transmitted. We
model the receive matched ﬁlter output as the sum of a ±1−valued sequence and Gaus-
sian sequence with zero-mean i.i.d. components.
The maximum iteration number of both SPA and SSBFA are set to be 50. But we
will stop the proposed algorithm earlier if the best of the output candidates are the
same for consecutive 5 iterations. In our simulations, SSBFA terminates quickly and
the average iteration number of is slightly more than ﬁve. Besides, the sample size Ns is
set to be 10 and the elite size Es is 1. Therefore, the computational complexity of both
algorithms in our simulation is approximately at the same level.
Fig. 5.2 - 5.6 are simulation results of ﬁve high rate block codes with HDPC matrix
which is in order (15,11) Hamming code, (7,5) RS code, (22,16) single error correction
(SEC) code, (39,32) SEC code, and (72,64) SEC code. The SSBFA has about 0.5 dB
- 0.8 dB coding gain over SPA at a bit error rate (BER) of 10−4 under approximately
same complexity.
Next, we consider two examples of cyclic codes, (31,26) BCH codes and (15,11) RS
codes. For the (31,26) BCH code, we compare our SBF algorithm, SBF with cyclic
shifts (CSSBF) and SSBFA with BDD and SPA. As shown in Fig. 5.7, the performance
of the SBF algorithm with a ﬁxed order is worse than BDD and SPA because of the
phenomenon described in Remark 5.1 may happen frequently. However, two kinds of
modiﬁed algorithms, SSBFA and CSSBF, have almost the same improved decoding
performance and outperform the other decoding methods. In other words, these modiﬁed
algorithms can greatly reduce the phenomenon described in Remark 5.1. For the (15,11)
RS code, similar decoding performance can be observed in Fig. 5.8. Our proposed
62
0 1 2 3 4 5 6
10-4
10-3
10-2
10-1
100
RS (7,5)
E
rr
or
 R
at
e
SNR (dB)
BER
 Uncoded
 SPA
 SSBFA
CER
 BDD
 SPA
 SSBFA
Figure 5.3: Error rate performance of the (7,5) RS Code; Ns = 10, Es = 1
0 1 2 3 4 5 6
10-5
10-4
10-3
10-2
10-1
100
 SEC (22,16)
E
rr
or
 R
at
e
SNR (dB)
BER
 Uncoded
 SPA
 SSBFA
CER
 BDD
 SPA
 SSBFA
Figure 5.4: Error rate performance of the (22,16) single error correction Code; Ns = 10,
Es = 1
64
0 1 2 3 4 5 6
1E-4
1E-3
0.01
0.1
1
 BCH (31,26)
E
rr
or
 R
at
e
SNR (dB)
Frame Error Rate
 BDD
 SBF
 SPA
 CSSBF
 SSBFA
Bit Error Rate
 Uncoded
 SBF
 SPA
 CSSBF
 SSBFA
Figure 5.7: Error rate performance of the (31,26) BCH Code.
0 1 2 3 4 5 6
10-3
10-2
10-1
100
 RS (15,11)
C
od
ew
or
d 
E
rr
or
 R
at
e
SNR( dB)
 BDD
 SBF
 SPA
 Chase-II
 CSSBF
 SSBFA
 KV
Figure 5.8: Error rate performance of the (15,11) RS Code.
66
fore, SEOLD-II outperforms SEOLD-I at the cost of increased complexity. Simulation
results verify that the performance of both algorithms is better than that of the GMD
algorithm and the KV algorithm.
In Chapter 5, we try to extend the concept of randomized sphere decoding to decode
general linear block codes which are not MDS codes. We ﬁrst investigate a novel sequen-
tial bit-ﬂipping (SBF) algorithm which can transform the hard-limited reliability vector
into a valid codeword with low complexity. When cyclic codes are in consideration, we
can cyclic shift the reliability vector and decode by SBF algorithm to form a set of can-
didates where the one with smallest ED to the received word is chosen as the decoder
output. On the other hand, we induce the concept of the randomized sphere decoding
with moving center when the codes are not cyclic. A set of random virtual reliability
vectors are generated and then decode by SBF algorithm. Again, the elite set of can-
didates is used to modify the random mechanism such that the center of the associated
sphere will gradually move more and more close to the transmitted codeword iteratively.
The proposed stochastic sequential bit ﬂipping algorithm (SSBFA) outperforms the SPA
for linear block codes with HDPC matrix. Although the proposed algorithms give sat-
isfactory performance, especially for high rate linear block codes with HDPC matrix.
There are several issues that require more research eﬀorts. We mention just three to
conclude this dissertation.
1. For long and/or low rate codes, the required complexity is still too high. Reducing
the sampling dimension without compromising performance is a very challenging
problem.
2. A stochastic decoding algorithm for decoding low density parity-check (LDPC)
codes with complexity much lower than BP-based algorithms is most welcome.
3. Establish a ﬁrm theoretical foundation of the randomized sphere decoder and apply
the same concept to solve other problems, e.g., MIMO detection.
68
Appendix B
The Proof of Theorem 5.1
From Bayes rule:
P (zn = cn|y,Σn)
P (zn 
= cn|y,Σn) =
P (zn = cn|y)
P (zn 
= cn|y)
P (Σn|zn = cn,y)
P (Σn|zn 
= cn,y) . (B.1)
From Lemma 5.1, the ﬁrst part of the right hand side of (B.1) is given by
P (zn 
= cn|y) = qn = 1− P (zn = cn|y). (B.2)
Assume the check sums in Σn are statistically independent. The probability P (Σn|zn =
cn,y) is given by
P (Σn|zn = cn,y) =
∏
m∈M(n)
P (σm|zn = cn,y), (B.3)
where
P (σm|zn = cn,y) =
{
1− rmn if σm = 0
rmn if σm = 1
. (B.4)
Similarly,
P (Σn|zn 
= cn,y) =
∏
m∈M(n)
P (σm|zn 
= cn,y), (B.5)
where
P (σm|zn 
= cn,y) =
{
rmn if σm = 0
1− rmn if σm = 1 . (B.6)
70
Bibliography
[1] J. S. Yedidia, “Constructing Free-Energy Approximations and Generalized Belief
Propagation Algorithms,” IEEE Trans. Inf. Theory, vol. 51, no. 7, pp.2282-2312,
July 2005.
[2] M. Welling and Y. W. Teh, “Belief optimization for binary networks: A stable
alternative to belief propagation,” Proc. Conf. Uncertainty in Artiﬁcial Intelligence,
Seattle, WA, Aug. 2001, pp. 554-561.
[3] A. Yuille, “CCCP algorithms to minimize the Bethe and Kikuchi free energies:
Convergent alternatives to belief propagtion,” Neural Computation, vol. 13, pp.
1691-1722.
[4] K. Rose, “Deterministic Annealing for Clustering, Compression, Classiﬁcation, Re-
gression, and Related Optimization Problems,” Proceedings, IEEE, vol. 86, no. 11,
pp. 2210-2239, November 1998.
[5] I. S. Reed and G. Solomon, “Polynomial codes over certain ﬁnite ﬁelds,” J. Soc.
Ind. Appl. Math, vol. 8, pp. 300-304, R.
[6] G. Gallager, Low Density Parity Check Codes, Monograph, M.I.T. Press, 1963.
[7] C. E. Shannon, “Communication in the presence of noise”, Proc. Institute of Radio
Engineers, vol. 37 (1): 10V21, Jan. 1949.
[8] The Digital Video Broadcasting Standard [Online]. Available: www.dvb.org
72
[21] R. Ko¨etter and A. Vardy, “Algebraic soft-decision decoding of Reed-Solomon
codes,” IEEE Trans. Inform. Theory, vol. 49, no. 11, pp. 2809-2825, Nov. 2003.
[22] D. Chase, “A class of algorithms for decoding block codes with channel measurement
information,” IEEE Trans. Inform. Theory, vol. IT-18, pp. 170-182, Jan. 1972.
[23] H. Tang, Y. Liu, M. Fosorier, and S. Lin, “On combining chase-2 and GMD decoding
algorithms for nonbinary block codes,” IEEE Commun. Lett., vol. 5, no. 5, pp. 209-
211, May 2001.
[24] N. Wiberg, “Codes and Decoding on General Graphs”, Ph. D. dissertation, Elec-
trical Engineering Dept., Linkopin Univ., Linkoping, Sweden, 1996.
[25] Y. C. Chen and Y. T. Su, “Constraint relaxation and annealed belief propagation
for binary networks”, ISIT 2007, pp. 24-29, June, 2007.
[26] M. E. Khamy and R. J. McEliece, “Iterative algebraic soft decision list decoding of
Reed-Solomon Codes”, IEEE J. Sel. Areas in Commun., vol. 24, pp. 481-490, Mar.,
2006.
[27] P. H. Tan and L. K. Rasmussen, “The Serial and Parallel Belief Propagation Algo-
rithms,”
[28] S. Lin and D. J. Costello, “Error Control Coding : Fundamentals and Applica-
tioins,” Prentice Hall, 2004.
[29] E. H. L. Aarts and J. Korst, Simulated Annealing and Boltzmann Machines. New
York: Wiley, 1989.
[30] Mackay’s Encyclopedia of Sparse Graph Codes,
http://www.inference.phy.cam.ac.uk/mackay/codes/data.html.
[31] R. Y. Rubinstein, “Optimization of computer simulation models with rare events”,
European of Operational Research,99:89-112, 1997.
74
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/26
國科會補助計畫
計畫名稱: 先進錯誤控制技術及其應用之研究
計畫主持人: 蘇育德
計畫編號: 97-2221-E-009-082-MY3 學門領域: 通訊
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
