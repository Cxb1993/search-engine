方法應用在改善吵雜環境(如客廳、行
進間小客車)中的語音分離與辨識問
題上。 
 目前大量應用在解決腦波訊號
[17-20] 及 胎 兒 心 電 圖 訊 號 分 離
[21-22]問題的線性獨立成份分析
[7-12](independent component 
analysis, ICA) 方 法 ， 例 如
JadeICA[13-14]、FastICA[15-16]及
筆者所提的PottsICA[21-22]，都是假
設陣列訊號是獨立源訊號的線性混
合，且相關的混合結構不具時間迴旋
特性。在這個假設下，線性ICA方法通
常針對特定目標函數的優化來估計混
合矩陣，例如JadeICA使用Kurtsis，
而 PottsICA 使 用
Kullback-Leibler(KL) divergence來
量化不同成份間的統計獨立性。所使
用的目標函數是否能有效量化不同成
份間的統計獨立性及相關優化方法是
否精確及具備高信賴度，是決定線性
ICA效能的兩大關鍵。 
在迴旋混合結構的假設下[3]，隨
時 間 t 改 變 的 麥 克 風 陣 列 訊 號
Ttxtxt )](..., ),([)( N1=x 可以表示如下， 
∑−
=
−=
1
0
)(A)(
L
l
l ltt sx     (1) 
其中 Tttst )](s..., ),([)( N1=s 代表N個未
知獨立訊號源， ll}{A 代表迴旋結構，L
代表迴旋結構的長度。 
 透過簡單的實驗就可以證明線性
ICA方法無法直接應用在陣列麥克風
語音分離問題上。圖一所示為從美國
沙克研究院網站[7]所下載的雙麥克
風陣列所錄下的陣列訊號，這兩組訊
號是兩個獨立聲源的真實混合，一個
獨立源來自搖滾樂播放，另一個獨立
源是人聲唸英文數字一到十。分別應
用上述三種線性ICA方法分析這兩組
訊號，都可以得到一個反混合矩陣，
將反混合矩陣乘以圖一的陣列訊號，
可以得到雙獨立源的估計訊號，從訊
號波形或實際播放都可以很清楚證實
上述線性ICA無法成功地將兩個獨立
聲源分離開來，確定上述線性ICA無法
解決麥克風陣列獨立源訊號分離問
題。 
 當混合結構具時間迴旋特性，也
就是L >> 1時，推導具體目標函數來
量化獨立成份間的統計相依性在實際
的研究上發生困難，這使得推廣上述
線性ICA方法解迴旋混合結構的獨立
成份分析問題的研究難有進展。 
 另一個想法是將陣列訊號以FFT
方法轉換成頻率域訊號，對式(1)的等
號兩邊進行FFT轉換，可以得到下述的
線性混合模型 
),(S)(A),X( tt ωωω =      (2) 
其中ω代表頻率。理論上，給定特定
頻率的陣列訊號{ ),X( tω }t，理應可以
應用線性 ICA 方法精確地估計單一混
合矩陣 )(A ω ，進而求得獨立訊號源
{ ),(S tω } t 的估計訊號{ ),(Y tω } t，接
著再結合所有頻率的頻譜域獨立成份
合併轉換為時間域訊號，最後得到獨
立源訊號{ )(ts }t 的估計訊號。但這樣
的做法在實際操作上遇到幾個問題
[3]：第一、線性 ICA 得到的獨立成份
需要有效地校正排列順序，才有可能
對不同頻率的獨立成份進行合併；第
二、不同頻率的獨立成份間的強度無
法相匹配；第三、樣本數太少使得 FFT
的頻普分析不精確，合併後無法得到
獨立成份；第四、無法保證反混合矩
認知機的加權輸出所形成的高維
函數，同時具備可調變的線性投影
向量及後非線轉換，所執行的後非
線 性 投 影 (post-nonlinear 
projection)不限於S型非線性轉
換。 
3. 結合Leave-one-out(loo)近似法
與MLPotts網路的LM學習法解迴旋
混合結構的獨立成份分析問題：考
慮兩個獨立源、兩個麥克風的訊號
源分離問題。如圖一所示，麥克風
陣列錄製的混合音輸入由MLPotts
網路所建置的解迴旋混合架構，就
可以得到圖二所示的獨立音源。因
為圖一的混合音是由美國沙克研
究院所公布，網站上並沒有公布該
組訊號的獨立音源。所以圖二的獨
立音源確實是本計劃成果所估計
的獨立音源。 
三、數值模擬 
 我們以論文[23-24]的 MLPotts網
路學習法則，解監督式多重迴旋結構
估計問題，可以進一步降低胎兒心電
圖檢測時使用的訊號頻道數。圖三是
我們發展的胎兒心電圖訊號分離軟
體，圖四所示為多頻道孕婦心電圖，
圖五所示為以PottsICA方法所得到的
獨立成份，其中第五組訊號為胎兒心
電圖。使用方法 2，只須要圖三的
channel 1 和 8 的混合訊號，就可以
得到胎兒心電圖的估計訊號，圖六是
MLPotts 網路的線性與非線性結構，圖
七的綠色訊號是圖三的 channel 1 訊
號，藍色訊號，是以 channel 8 透過
MLPotts學習法所得到channel 1近似
訊號，圖七的紅色訊號是綠色訊號減
掉藍色訊號的結果，代表由雙頻道孕
婦心電圖所擷取的胎兒心電圖。 
 圖一的反迴旋混合架構的模擬如
下: 
1. 將一組混合訊號視為目標訊號，
將另一組混合訊號視為參考訊
號，以參考訊號透過二 2 的方法
求目標訊號的近似訊號，回饋誤
差訊號，取代原代表目標訊號的
混合訊號。 
2. 交換兩組混合訊號的角色，同樣
以二 2 的方法求目標訊號的近似
訊號，回饋誤差訊號，取代原代
表目標訊號的混合訊號。 
3. 假如預先設定的停止條件成立，
程序停止，否則跳到步驟 1。 
 應用上述方法處理圖一的混合訊
號可以得到圖二的訊號源。 
四、總結 
 本計畫提出以MLPotts學習為基
礎的反迴旋混合架構，可以有效的分
離真實麥克風陣列所錄製的訊號。也
可以有效降低胎兒心電圖分離所使用
的測量頻道數。 
 (五)參考文獻 
[1] http://www.idiap.ch/~mccowan/
arrays/arrays.html 
[2] http://www.lems.brown.edu/arra
y/papers/ 
[3] Michael Syskind Pedersen1, Jan 
Larsen2, Ulrik Kjems1, and 
Lucas C. Parra3, A SURVEY 
OF CONVOLUTIVE BLIND 
SOURCE SEPARATION 
METHODS, Springer 
Handbook on Speech Processing 
and Speech Communication 
[4] Abdeldjalil Aïssa-El-Bey, 
Karim Abed-Meraim, Senior 
Member, IEEE, and Yves 
analysis,'' IEEE Transactions on 
Neural Networks 10(3):626-634, 
1999. 
[17] Makeig S., Jung T.P., Bell AJ, et 
al.,''Blind separation of auditory 
event-related brain responses 
into independent components,'' 
P NATL ACAD SCI USA 94: 
(20) 10979-10984 SEP 30 
(1997) 
[18] Makeig S, Westerfield M, 
Townsend J, et al., 
“Functionally independent 
components of early 
event-related potentials in a 
visual spatial attention task,” 
PHILOS T ROY SOC B 354: 
(1387) 1135-1144 JUL 29 1999  
[19] McKeown M.J., Tzyy-Ping Jung, 
Scott Makeig, Greg Brown, 
Sandra S. Kindermann, Te-Won 
Lee and Terrence J. Sejnowski, 
"Spatially independent activity 
patterns in functional magnetic 
resonance imaging data during 
the stroop color-naming task," 
Proc. Natl. Acad. Sci. USA 
95:803-810 1998.  
[20]McKeown MJ ,”Detection of 
consistently task-related 
activations in fMRI data with 
hybrid independent component 
analysis,”NEUROIMAGE 11: 
(1) 24-35 JAN 2000 
[21]Wu, J.M., S. J. Chiu, 
Independent component 
analysis using Potts models, 
IEEE Trans. On Neural 
Networks, Vol. 12, No. 2, 
March, 2001. 
[22]Jiann-Ming Wu, Fetal 
electrocardiogram 
extraction by annealed 
expectation maximization, 
accepted by Neurocomputing, 
Nov. 2006 
[23]Jiann-Ming Wu, Multilayer 
Potts perceptrons with 
Levenberg-Marquardt 
learning, revised for IEEE 
Trans. on Neural Networks, 
Oct. 2006. 
[24]Jiann-Ming Wu, Z.H. Lin and 
Pei-Hsun Hsu, Function 
approximation using 
generalized adalines, IEEE 
Trans. On Neural Networks 
VOL. 17, NO. 3, MAY 2006 
[25]Rumelhart DE, Learning 
Representations by 
back-propagation errors, 
Nature 323 : 533 1986  
[26]Rosenblatt, F., Principles 
of Neurodynamics: 
Perceptrons and the Theory 
of Brain Mechanisms, 
Washington, DC: Spartan 
Books, 1962. 
[27]Widrow, B., Generalization 
and Information Storage in 
Networks of Adaline Neuron 
in Self-Organizing Systems, 
1962, M. Yovitz, G. Jacobi, 
and G. Goldstein, Eds. 
Washington, DC: Spartan 
Books, 1962, pp.435-461. 
 
圖一、以 MLPotts 網路學習為基礎的
反迴旋混合架構 
圖二、處理圖一由真實麥克風陣列錄
製的混合訊號所得到的獨立音源 
圖三、PottsICA fetal ECG extractor 
圖四、Eight-channel maternal ECG 
圖五、Extracted fetal ECG 
圖 六 、 Linear and nonlinear 
structure of an MLPotts network for 
fetal ECG extraction from 
two-channel maternal ECG 
圖七、 Extracted fetal ECG from 
two-channel maternal ECG 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
圖二
圖四
圖六
國立東華大學 
專任教師出席國際學術會議報告 
 
報告人姓名 
 
吳建銘 
 
服務機構
及職稱 
 
應用數學系 
副教授 
會議時間 MAY 27–28, 2009 會議地點 Winnipeg, Manitoba, Canada 
會議 
名稱 
 (中文) 2009 年 FU-FEST 國際學術研討會 
 (英文) FU-FEST 2009: A Symposium in Honor of James C. Fu 
發表 
論文 
題目 
 (中文) 學習多層波茲認知機網路解函數近似問題 
 (英文) Learning multilayer Potts perceptrons for function approximation 
在程式展示過程中，我用筆記型電腦展示我所發展的多層波茲認知機網路學習軟。我的 MLPotts
學習軟體非常容易使用，使用者只要在函數欄位中輸入函數式，並在樣本個數欄位中輸入樣本
數，按下 sampling 就可以產生成對的樣本點(第二張圖)，給定波茲認知機個數(M)及神經元狀態
數(K)，按下 MLPotts 就可以產生近似函數(第三張圖)的線性及非線性結構，及完整的近似函數(第
四張圖)。由於函數近似問題就是統計的非線性迴歸問題。Statistics & Probability Letters 雜誌的
Funding Editor，R.A. Johnson 看完展示內容，馬上問我有關 noise 的問題，另外還有學者提出神
經元狀態個數的設定問題，由於時間的關係我回答完幾個問題後就結束我的演講。我的演講結
束後，來自英國的 J. Aston 和我討論許多有關時間序列預測及 fMRI 影像處理問題，我也和來
自法國的 G. Nuel 討論如何進一步發展新的計算工具解 run & pattern waiting time 的機率問
題。 
 
二、與會心得 
這次參加 Fu Fest 2009，更深刻體認到傅權教授的 FMCI 研究已吸引許多年輕機率學者參與，
大家跟隨傅權教授的研究腳步，深受啟發。我覺得 FMCI 的理論與方法的遞迴架構可以進一步
探討，回國後已有博士生表示有興趣研究，這是此行的重要收獲，此一方向的研究或許可以進
一步提升 waiting time 機率計算速度，強化 FMCI 的理論與方法的應用基礎。 
0940222 修訂/表 RD104 
Speakers:
John Aston, University of Warwick, UK & Academia Sinica, Taiwan
Zhidong Bai, National University of Singapore, Singapore
Narayanaswamy Balakrishnan, McMaster University, Canada
Yung-Ming (Eddie) Chang, National Taitung University, Taiwan
Leo Wang-Kit Cheung, Loyola University, Chicago, USA
Lirong Cui, Beijing Institute of Technology
Brad C. Johnson, University of Manitoba, Canada
Richard A. Johnson, University of Wisconsin, USA
Markos V. Koutras, University of Piraeus, Greece
Gang Li, Johnson & Johnson Pharmaceutical R&D
Lung-An Li, Academia Sinica, Taiwan
Wendy Lou, University of Toronto, Canada
Gre´gory Nuel, Universite´ Paris Descartes, France
Jiann-Ming Wu, National Dong Hwa University, Taiwan
Tung-Lung Wu, University of Manitoba, Canada
Organizing Committee:
Brad C. Johnson, Chair
Xikui Wang
John Brewster
Alexandre Leblanc
Katherine Davies
Sponsors:
Fields Institute
Department of Statistics, University of Manitoba
Faculty of Science, University of Manitoba
Office of the Vice President (Research), University of Manitoba
4 PROGRAM FU-FEST 2009
PROGRAM
Wednesday May 27, 2009
8:00 – 8:30 am Registration
8:30 – 9:00 am Opening Remarks
9:00 – 9:40 am Markos V. Koutras & Fotios S. Milienos, University of Piraeus, Greece; Anant
P. Godbole, East Tennessee State University, USA
Consecutive Covering Arrays
9:40 – 10:20 am Wendy Lou, University of Toronto, Canada
FMCI and Some Selected Biomedical Applications
10:20 – 10:40 am Coffee Break
10:40 – 11:20 am Zhidong Bai, National University of Singapore, Singapore
Corrections to LRT on Large Dimensional Covariance Matrix by RMT
11:20 – 12:00 am John Aston, University of Warwick, UK & Academia Sinica, Taiwan
Using FMCI within Statistical Analysis
12:00 – 1:30 pm Lunch (University Club, Pembina Hall)
1:30 – 2:10 pm Gre´gory Nuel, Universite´ Paris Descartes, France
Computing the k First Moments of a Pattern Count in a Random Se-
quence Generated by a Markov Source and Application to Near Gaussian
Approximations
2:10 – 2:50 pm Leo Wang-Kit Cheung, Loyola University, Chicago
Kernel-Imbedded Gaussian Processes for Microarray Gene Expression
Data Analysis
2:50 – 3:10 pm Coffee Break
3:10 – 3:50 pm Yung-Ming (Eddie) Chang, National Taitung University, Taiwan
Average Run Lengths of Control Charts for Autocorrelated Processes
3:50 – 4:30 pm Lirong Cui, Beijing Institute of Technology
Developments and Applications of Finite Markov Chain Imbedding Ap-
proach in Reliability
7:00 pm Dinner — Terrace Fifty-Five Restaurant, 55 Pavilion Crescent, Winnipeg.
Levenberg–Marquardt Learning of Multilayer Potts Perceptrons 
for Function Approximation and Time Series Prediction 
Jiann-MingWu 
Department of Applied Mathematics, National Dong Hwa University, Taiwan 
Abstract: 
 In this talk, I will present learning multilayer Potts perceptrons (MLPotts) for data 
driven function approximation and time series prediction. A Potts perceptron is composed of 
a receptive field and a state transfer function that is generalized from sigmoid-like transfer 
functions of traditional perceptrons. An MLPotts network is organized to perform translation 
from a high-dimensional input to the sum of multiple postnonlinear projections, each with its 
own postnonlinearity realized by a weighted state transfer function. MLPotts networks span a 
function space that theoretically covers network functions of multilayer perceptrons. 
Compared with traditional perceptrons, weighted Potts perceptrons realize more flexible 
postnonlinear functions for nonlinear mappings. Numerical simulations show MLPotts 
learning by the LevenbergMarquardt (LM) method significantly improves traditional 
supervised learning of multilayer perceptrons for data driven function approximation and time 
series prediction. 
2Network Function
}{}{}{
)tanh();( 0
1
mmm
M
m
m
T
mm
rb
rbrf
∪∪=
++=∑
=
a
xax
θ
θ
Multiple post-tanh projections
M=2,d=2
f(x1,x2)= (2*x1+3*x2+1)
+(2*x1-3*x2+2)
f(x1,x2)=tanh(2*x1+3*x2+1)
+tanh(2*x1-3*x2+2)           
Paired data for function approximation
z Target function 
z Paired Data
z Predictors
z Targets
RRd →:F
Nttyt ,...,1  ]),[],[( =x
T
d txtxt ])[],...,[(][ 1=x
noisetty += ])[(F][ x
Paired data
)cos()(F
2
2121 xxxx
d
+=+
=
Target function Paired data
Function approximation
z Approximating function : 
z Multilayer perceptrons
z Equivalently, multiple post-tanh projections
z Subject to given paired data, optimize 
network parameters, θ={a,b,r}, to 
reconstruct the unknown target function.
Approximating error
∑
=
−=
N
i
tfty
N 1
2))];[(][(1)E( θxθ
4Why tanh?
z Spin random variable s∈{+1,-1}
z s depends on h
z Conditional probability of s to h is defined 
by
z Then
)tanh(]|[
)exp()|Pr(
hhsEv
hshs
==
∝
Potts random variable
z Potts random variables δ
z δ ∈ {e1,e2,…,eK}
z K states
z ek : a unitary vector with the kth bit one 
and others zeros
Potts perceptrons
z Conditional probability of δ to h is defined by
2
k
2
  varianceand cmean with 
pdf normal  thedenotes 
),;()|Pr(
σ
σ
q
chqh kk ∝= eδ
∑
=
=≡=
∝=
K
j
j
k
kk
kk
chq
chqqh
chqh
1
2
2
2
),;(
),;()|Pr(
),;()|Pr(
σ
σ
σ
eδ
eδ
Multi-state transfer function
∑
=
=≡
==
K
k
kk
kk
qhh
qh
1
2 |),;(
)|Pr(
eδcg
eδ
σ
∑
=
=≡= K
j
j
k
kk
chq
chqqh
1
2
2
),;(
),;()|Pr(
σ
σeδ
[ ]TK
K
k
kk
qq
qhh
,,
|),;(
1
1
2
L=
=≡ ∑
=
eδcg σ
6Target function: y6
MLPotts(M=3,K=31)
5
3215
]2,2[
)cos()tanh()sin()(
ππ−∈
++=
x
xaxaxax TTTy
Target function: y8
MLPotts(M=3,K=31)
Figure 7
8
3218
],[
)cos()tanh()sin()(
ππ−∈
++=
x
xaxaxax TTTy
Numerical Simulations
(Wu 2008)
Numerical simulations
Ratio of parameters for post-nonlinear
z d: data dimension
z Ratio of parameters utilized for post-nonlinear 
representations
z MLP
z MLPotts
d+1
1
dK
K
++
+
21
12
Ex. d=8, K=31
RatioMLP :     12.50 %
RatioMLPotts :  80%
Conclusions
z Advantages of MLPotts
z Increasing the state number of Potts perceptrons is 
shown able to boost utilization of parameters for 
post-nonlinear representations
z Perceptrons are special cases of Potts perceptrons
z MLP networks are special cases of MLPotts
networks
z LM method is effective for MLPotts learning
z MLPotts learning outperforms MLP learning for 
function approximation as well as time series 
prediction (2008 Wu)

