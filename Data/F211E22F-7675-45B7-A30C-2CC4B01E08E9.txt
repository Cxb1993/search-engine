中文摘要 
 
影像註解(image annotation)為執行影像檢索(image retrieval)的關鍵要素之一。透過影像註
解，無須利用尚不精確的影像內容理解技術，便可有效依語意進行檢索影像。然而透過影
像註解進行影像檢索根基於精確的賦予影像文字型態之註解。對於少量之影像文件而言，
尚可利用人工乃至於半自動化的方式為影像加上註解。然而此類方式既費人工又缺乏一致
性，因此無法應用於大型資料庫上。為解決這個困難，自動影像註解(automatic image 
annotation)技術之研究便應運而生。本計畫將研發一以類神經網路為基礎之自動影像註解方
法。當以一組已註解之影像作訓練時，此方法將運用自我組織圖(self-organizing map)模式對
訓練影像切割後之區塊進行分群，並同時對其註解進行分群。隨後將對該分群之結果進行
探勘，以發掘影像區塊與文字註解間之關聯，此關聯將可運用於新影像之註解之上。在本
計畫之第一年中，我們使用基礎之影像特徵表達及區域切割方式進行研究，目的在於建立
主要研究架構及方法。在本計畫之第二年中，我們進一步採取更準確之特徵表達與影像切
割方式，並改善影像與註解之比對方法，使本計畫之成果更臻完善。本計畫所使用之技術，
將基礎於主持人之前所發展之多語言文本探勘(multilingual text mining)及影像語意發掘
(image semantics discovery)技術，並開發以影像內容為基礎之新的影像語意註解方法。本計
畫所發展之技術優勢在於三點：其一為該技術為全自動化方法，無需任何人工介入；其二
為使用類神經網路方法，其延伸性及容錯能力均佳；其三為影像語意發掘技術已有所本，
可行性高。本計畫成果將可提供使用者簡易且精確之影像檢索模式，並為數位典藏及數位
圖書館之相關應用提供可行之解決方案。 
 
1. 前言 
 
語意影像檢索為一十分困難的問題。主要的困難在於如何淬取出影像中所包含之語意
（或意涵）。目前的作法多將影像中的視覺特徵與人工定義之語意相連結以定義影像之語
意。隨後再以結合視覺特徵比對及語意相關度比對之方式進行檢索。這些方法最主要的問
題在於語意之定義大多須以人工方式事先定義或以關聯回饋(relevance feedback)方式加入
影像之描述中，造成其應用範圍受限或須花費過多之時間與成本。 
影像檢索之目標，如同其他資訊檢索工作之目標一般，無非是要滿足使用者之需求。當
使用者表達其意圖時，影像檢索系統必須判斷影像之語意並與之比對，再依其相似程度呈
現於使用者。因此事實上，上述的三類影像檢索方法廣義上皆是以語意來進行檢索，只是
語意之表達與淬取方式不同。因此我們可以將關鍵字與視覺特徵視為使用者詢問之方式，
影像檢索系統再藉由分析這些詢問以獲取語意，俾以進行語意比對並據以檢索。若以此觀
點視之，我們可以將影像檢索分為兩個步驟，即語意淬取(semantics extraction)與語意比對
(semantics matching)。在語意淬取部份，基於關鍵字之影像檢索技術藉由分析存在於影像所
附屬之註解(annotation)、影像標題(caption)或影像週圍之文字(surrounding text)以獲取語
意。基於內容之影像檢索技術則直接根據影像之視覺特徵，如色彩、形狀、紋理等以獲取
語意。然而這類內容特徵之分析直至目前尚未能有效的提供一解決方案來獲取語意。反之，
自文字中獲取語意之方法在計算語言學、資訊檢索等領域已有多年的研究，並獲得良好的
結果。此外，以文字來表達語意對於語意之相似度比對較為容易，這方面已有深入的研究
[1]。相關的研究並已成功應用於數位圖書館、搜尋引擎等案例上。事實上，目前之影像搜
尋引擎大多也是使用文字作為比對之依據。總體而言，使用文字來表達影像之語意應是較
為便利且準確的。使用文字來表達使用者意圖或影像語意時通常都是以關鍵字為基礎。使
用者以若干關鍵字來描述其所欲影像之意涵，影像資料庫中亦以關鍵字來表達所儲存影像
之意涵。 
綜合以上所言，以關鍵字為基礎進行影像檢索是於目前為較可行之方案。然而此類方法
之成功取決於一關鍵要素，即影像語意之正確描述。目前影像語意大都以影像註解之方式
來表達，即在影像附加一段文字或若干關鍵字以描述影像意涵。影像註解之附加傳統上以
人力為之，即由專家審視影像後，再將其意涵以文字描述後附加於影像文件之上。人工方
式雖可獲得較佳的語意，但費時費力，僅可應用於小型影像資料庫上，對於大型資料庫而
言實用性不高。自動影像註解(automatic image annotation)則可有效的解決人工註解的困
難。自動影像註解乃是藉由分析影像之意涵，淬取其語意，轉化為註解型式附加於影像。
其優點為無需或僅需少量人力介入，可迅速處理大量影像，並可即時處理新進影像。 
自動影像註解技術雖有諸端優點，但其難度頗高。主要的難處在於影像語意之淬取為一
高階認知過程(high-level cognitive process)，一般人雖可輕易的藉由觀視影像而得知其意
涵，但此一過程要以自動化之過程完成之卻頗為困難。一張影像所包含之資訊眾多，以傳
統的影像分析觀點視之，約可分為三種主要特徵，即色彩、形狀與紋理。然而雖然我們可
以有效的自影像中淬取出上述的特徵，但仍需克服兩個問題。其一為此特徵是否可以有效
的表達影像之全部語意。其二為如何將此特徵對應至以文字形式所描述之影像語意。第一
個問題其實與第二個問題相關，因為不論所採用之特徵為何，其後之對應方式若能完善，
均可獲致較佳的效果。但如前所言，此對應方式為一高階認知過程，頗難克服，也成為自
動影像註解中最關鍵的步驟。 
將影像之內容對應至符合其語意之文字註解的過程，在此稱之為影像語意發掘(image 
2. 文獻探討 
 
有關於自動影像註解最早的研究報告大概出現在 Mori 等人的報告[2]。他們將影像切割
成方形區塊並使用字詞與低階特徵之共現模型(co-occurrence model)來產生註解。他們計算
字詞與影像間所共同出現之次數以預測影像之註解字詞。在其後這方面的研究便分成兩個
方向：第一個方向使用影像切割演算法將影像切割為不規則之區域，稱之為 blob，並以此
為基礎進行註解。這種方式已被許多研究者所採用。Duygulu 等人[3]為影像中之 blob 群集
建立了一個離散式的字彙集(vocabulary)，並以機器翻譯(machine translation)之模型來將這些
blob 翻譯成註解。他們將影像註解過程視為一將“視覺語言＂翻譯至文字之過程。這個模
型會預估翻譯之機率以收集共現資訊。Jeon 等人[4]將影像註解問題視為跨語言資訊檢索
(cross-lingual information retrieval)問題。他們使用一種跨媒體關連模型(cross-media relevance 
model)來執行影像註解與檢索，並在檢索效能上得到比翻譯模型[3]更好的結果。Lavrenko 
等人[5]則採用[4]之模型，並使用連續機率密度函數來描述產生 blob 的過程，希望可以藉
此避免因量化(quantization)程序所造成的損失。在同樣的資料集上，他們得到顯著的改善效
果。Metzler 與 Manmatha[6]也同樣的將影像先切割成 blob 後，再利用推論網路(inference 
network)來連結影像與註解。新進影像之註解方法為以其影像區域啟動網路，並於網路中傳
遞信任度(believe)以至於代表該區域之字詞之節點。Feng 等人[7]則將 blob 替換為方形區
塊並使用多重伯努利分布(multiple Bernoulli distribution)來建立關鍵字模型，而得到比[5]與
[6]更好的結果。另外還有一相關的研究，Blei 與 Jordan[8]將 Latent Dirichlet Allocation 模
型[9]擴充，該模型將一組 latent factors 混合產生字詞與 blob 之特徵。他們則利用該模式以
指定字詞給 blob。第二個方向則是較簡單的場景(scene)導向方式。這類的方法首先由 Oliva 
與 Torralba 提出[10][11]，他們的研究顯示影像可以經由一些相關的低階全域特徵以簡單的
場景標籤來描述，如街道、建築物、公路等。他們進一步展示如何使用簡單的影像統計資
訊來推論出場景內之物件之存在與否[12]。Yavlinsky 等人[13]遵循上述之場景導向方式，
探討全域特徵在自動影像註解上之可行性。他們的方法基本於 nonparametric density 
estimation 之 kernel smoothing 技巧，他們的研究顯示在此架構下即使簡單的全域特徵，如
全域色彩統計等，亦能夠有效的進行自動影像註解之工作。 
 
已為關鍵字形式則僅需進行字根還原程序即可。若註解為中文字時字根還原程序可
省略。當所有訓練影像之註解皆已處理完畢後，再依各影像之註解所包含之關鍵字
進行編碼為二元向量 (binary vector)，以獲得每一影像之註解向量 (annotation 
vector)wi，其中 1 ≤ i ≤ M，M 為訓練影像數。 
(2) 註解向量自我組織圖訓練：上述之註解向量 wi，將作為訓練自我組織圖[14]之輸入
向量並進行訓練。自我組織圖之訓練結果可由一組神經元鍵結強度向量 xj 來表達，
其中 1 ≤ j ≤ J 為神經元索引值，J 為神經元總數。根據自我組織圖的特性，訓練完
成後，這些註解向量將被適當的分群。採用自我組織圖的原因是因其優越之分群效
果已在無數的應用中實證。另一方面，其結果也可進一步探勘以獲取更高階的知識
[15][16]並應用於影像語意發掘上。 
(3) 關鍵字標記(keyword labeling)：經自我組織圖訓練後，將進行關鍵字標記過程，將
關鍵字標記字神經元上，而獲得關鍵字分群圖(keyword cluster map, KCM)。標記過
程為檢查 xj之成份值，並將關鍵字標記於擁有最大成份值之神經元上。在此分群圖
中，被標記在同一神經元之關鍵字將被視為相關的，同時被標記在鄰近神經元上的
關鍵字也將被視為相關的。此分群圖可以表達關鍵字間之關聯，對往後之影像語意
發掘助益頗大。 
(4) 影像前置處理：一般在進行自動影像註解時，並不會使用完整影像直接處理，而是
先將影像進行切割(segmentation)後再以所得之區塊(block)或區域(region)作為基本
單位。本研究將分別採取這兩種切割方式，將影像切割成固定大小之區塊，以及運
用目前已被普遍使用的方法，如 normalized cuts[17]或 blobworld[18]等方法將影像
切割成適當的語意區域後再行淬取特徵。選用上述方法的原因是因其已被普遍使
用，較無爭議性，同時相關資源容易取得，可節省系統開發時間。 
(5) 影像區域特徵淬取：切割後之影像區域需進一步淬取其特徵。本研究初步將以色彩
為基礎嘗試各種不同的特徵，如色彩直方圖(color histogram)及其變形。選用色彩的
原因是自然影像中形狀及紋理變化較大，較難精確的淬取。相對的，色彩資訊在自
然物體中較為穩定，且其淬取過程也較為容易。經特徵淬取步驟後，我們可將一影
像區域轉換成一影像區域特徵向量。在轉換時，我們會先將彩色資訊轉換為其所對
應之灰階值以利處理。隨後我們會進行量化將灰階值量化為 256 灰階。據此得到
直方圖後，再進行正規化(normalization)程序，如 histogram equalization 等，將直方
圖正規化至同一動態範圍。如此可以得到一長度為 256 之向量。 
(6) 影像區域特徵向量自我組織圖訓練：和註解向量相同，我們將以自我組織圖模式對
影像區域特徵向量進行訓練。根據自我組織圖的特性，訓練完成後，這些影像區域
特徵向量將被適當的分群。在進行分群時，將依實驗結果調整訓練參數，如網路大
小，學習時間，學習速率，鄰近範圍大小等。 
(7) 影像區域標記：影像區域特徵向量經自我組織圖訓練後，將進行一影像區域標記過
程，將影像區域標記於自我組織圖之神經元上，以對影像區域進行分群，並獲得影
像區域分群圖。在此圖中可以看出影像區域間之關聯，若兩影像區域屬於同一群組
或相鄰之群組，則我們會認為它們是相關的。此外，影像區域間之相似度我們亦可
用群組之空間距離量度之。 
(8) 影像區域／關鍵字對應：針對上述之影像區域分群圖與關鍵字分群圖進行探勘，以
發掘影像與關鍵字間之關聯。然而這種關聯並不容易發掘出來。由於我們可以將分
群後之影像區域視為一符號字元，或稱之為視覺字詞(vizterm)，此關聯之發掘事實
上便是一多語言文本探勘問題[19]。本計畫將採取下列方法進行影像區域與關鍵字
本計畫第二年之目標為改善影像特徵描述、改善影像與關鍵字對應方法、進行系統評
估。本年度計畫中提出了新的影像特徵描述方法與新的影像與關鍵字對應方法。本計畫共
開發兩種新的影像與關鍵字關聯發掘法，分別是混合式特徵關聯探勘(hybrid feature 
relationship mining)法與階層式關聯探勘(hierarchical relationship mining)法。以上所提之新方
法將於下文中詳述之。 
本計畫第二年之研究基本架構如圖 2 所示。以下將以圖 2 為基礎詳細的說明各步驟所
採用之方法。 
 
圖 2 系統架構圖 
z 訓練階段： 
在訓練階段我們分別處理影像及其註解，如圖 2 之左半部所示。首先描述註解之處理
流程： 
(1) 註解前置處理：訓練影像之註解先被分離以作處理。若註解為文本(text)形式，必
須先進行切字(word segmentation)，常用字去除(stopword elimination)，字根還原
(stemming)，關鍵字選擇(keyword selection)等程序以獲得一組關鍵字集合。若註
解已為關鍵字形式則僅需進行字根還原程序即可。若註解為中文字時字根還原程
序可省略。當所有訓練影像之註解皆已處理完畢後，再依各影像之註解所包含之
關鍵字進行編碼為二元向量(binary vector)，以獲得每一影像之註解向量(annotation 
vector)wi，其中 1 ≤ i ≤ M，M 為訓練影像數。 
影像特徵向量 i 連結為一單一向量 h，h = w ~ i，其中~為連結運算子(concatenation 
operator)。在連結時要注意需賦予此二向量不同的權重值，因其長度並不相同。舉
例而言，若關鍵字向量長度為 100，但影像特徵向量長度為 500，則在往後進行自
我組織圖訓練時可能會發生偏重影像向量之情形。因此藉由適當之權重設計，我們
可以平衡此二向量之貢獻度。以上例而言，我們可以進行簡單的權重調整如下： 
iw
w
i
h ~normal =        (3) 
此外，亦可依所需來調整關鍵字與影像特徵之重要性。 
(7) 影像區域特徵向量自我組織圖訓練：和註解向量相同，我們將以自我組織圖模式對
影像區域特徵向量進行訓練。根據自我組織圖的特性，訓練完成後，這些影像區域
特徵向量將被適當的分群。在進行分群時，將依實驗結果調整訓練參數，如網路大
小，學習時間，學習速率，鄰近範圍大小等。 
(8) 影像區域標記： 
z 個別特徵向量：和上一年度相同。 
z 混合特徵向量：將訓練向量中之關鍵字部份與影像特徵部份分別進行標記。亦
即，當標記關鍵字時，我們只使用混合特徵向量 h 之關鍵字部份，即 w，來
標記關鍵字。同樣的，當標記影像時，我們也只使用 h 中之 i 部份來進行標
記。如此我們可以只藉由一自我組織圖便分別得到兩張特徵圖。 
(9) 影像區域／關鍵字對應：我們針對不同的特徵向量表達方式，發展新的影像與關鍵
字對應方法。分別是針對個別特徵向量之階層式關聯發掘與針對混合特徵向量之關
聯發掘。以下分別細述之。 
z 個別特徵向量：在此模式下，註解關鍵字與影像分別具有獨立之特徵向量並分
別進行自我組織圖訓練，因此將會產生兩張特徵圖。在上一年度中，我們已建
立了一種對應方式。本年度將採取另一種方式，亦即先進一步發掘關鍵字間之
階層式關聯與影像區域間之階層式關聯，再發掘此二階層間之關聯。使用階層
式關聯發掘的目的，是為了找出關鍵字群集與影像群集間更深入之語意關係，
並期望能夠達成更佳的註解結果。其方法概念如下圖所示： 
 
圖 3 階層式關聯發掘 
首先，我們必須建立關鍵字階層與影像區域階層。建立階層的方法，我們將採
用主持人之前所發展之自動化階層產生方法[16]來自動產生關鍵字與影像階
層。在此階層中之每一節點皆為一神經元，因此皆可賦予一鍵結向量。因此我
們可以藉由計算此二階層節點間之相似度，發掘其間之關聯。令 x1,j 為關鍵字
階層中第 j 個節點 node1,j 之鍵結向量，x2,k 為影像區域階層中第 k 個節點
node2,k 之鍵結向量。則此二節點之相似度可由其歐基里德距離求得： 
4. 結果與討論 
 
本計畫實驗所使用之影像來源為華盛頓大學之 groundtruth 影像註解資料庫。該資料庫
包含了 21 個類別之註解後之影像。我們從其中隨機蒐集了一百張影像，並將每一影像切割
為九張子影像，共獲得了 900 張子影像。將此 900 張子影像利用前述方法轉換為影像向量
與關鍵字向量，透過自我組織圖分群並標記後產生出來的影像區域分群圖和關鍵字分群圖
如圖 4 與圖 5 所示。隨後再利用此兩圖之關聯產生影像/關鍵字對應表，進而產生註解。圖
6 顯示兩張屬於分群 0 之影像區域所產生的註解。 
 
圖 4 影像區域分群圖 
 
圖 5 關鍵字分群圖 
驗中，我們利用全部影像的註解關鍵字共 118 個來針對自動產生之影像註解進行查
詢。所有關鍵字查詢之 recall 與 precision 平均後之結果分別為 0.54(第一年為 0.345641)
及 0.65(第一年為 0.3659)。平均 F 量度之值為 0.57(第一年為 0.335723)。可看出使用
第二年方法效果明顯提升。 
 
on Pattern Analysis and Machine Intelligence, 22(8), pp. 888-905. 
[18] Carson, C., Belongie, S., Greenspan, H., and Malik, J. (2002) “Blobworld: Image 
Segmentation Using Expectation-Maximization and Its Application to Image Querying.” 
IEEE Transactions on Pattern Analysis and Machine Intelligence. 24(8), pp. 1026-1038. 
 
可供推廣之研發成果資料表 
□ 可申請專利  □ 可技術移轉                                      日期： 年 月 日 
國科會補助計畫 
計畫名稱： 
計畫主持人：         
計畫編號：             學門領域： 
技術/創作名稱  
發明人/創作人  
技術說明 
中文： 
 
 
（100~500 字） 
英文： 
可利用之產業 
及 
可開發之產品 
 
技術特點 
 
推廣及運用的價值 
 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位
研發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
