The research project had been investigated thoroughly by the PI and his students. With the 
kind support from NSC, we investigated advanced research issues in social media search and 
mining by discovering the implicit links between the images and videos in large-scale social 
media (i.e., Flickr, YouTube, etc.). Extending from the original proposal, we also discovered  
the challenges and opportunities for indexing large-scale social media and seek the solution 
through crowd computing (i.e., Hadoop, see [2]). We also pointed some important directions 
in this research paradigm for the research community [3][4]. We also devised fruitful and 
significant results in terms of publications [1][2][3], research community services 
[4][5][6][7][8][9], and joining international technical committee. We append the accepted 
(also submitted) papers in this report. 
 
Publications: 
[1] “Semantic Query Expansion for Content-based Image Retrieval,” Wei-Shing Liao, Liang-Chi 
Hsieh, and Winston H. Hsu, IEEE Transaction on Multimedia (submitted), 2009. 
Abstract: Content-based image retrieval (CBIR) is one of the essential techniques for 
managing exponentially growing photos and the enabling technology for many applications 
such as annotation by search, computational photography, photo-based question and 
answering, etc. Though through decades of research, current solutions are limited due to the 
“semantic gap.” In this work, we argue to improve CBIR by automatically exploiting the 
auxiliary knowledge (i.e., tags, photos, blogs, metadata, etc.) from the booming 
media-sharing services (e.g., Flickr) and search engines (e.g., Google); i.e., finding more 
semantic-related images to enhance CBIR results. The framework discovers the semantic 
similarity between images by constructing a combinational image graph model and utilizing 
the noisy textual (i.e., tags) and visual information (i.e., visual words). We also consider the 
efficiency issues as deploying in the large-scale media-sharing sites. Meanwhile, the 
framework is generic and can be extended for keyword-based image retrieval and also image 
annotation. Experimenting over large-scale photo benchmarks, the proposed semantic 
expansion framework outperforms traditional CBIR systems over 3 times, on the average. 
 
[2] “Canonical Image Selection And Efficient Image Graph Construction For Large-Scale Flickr 
Photos,” Liang-Chi Hsieh, Kuan-Ting Chen, Chien-Hsing Chiang, Yi-Hsuan Yang, 
Guan-Long Wu, Chun-Sung Ferng, Hsiu-Wen Hsueh, Angela Charng-Rurng Tsai, Winston H. 
Hsu, ACM Multimedia 2009. 
Abstract: Efficient image search clustering is prominent for image search engines for 
exponentially growing photo collections. In this work, we propose an image search clustering 
approach which selects multiple canonical images from image search results and constructs 
image clusters in real time on an image sub-graph for the search results. The efficiency is 
achieved with the help of offline-computed image context graphs by distributed computing 
methods. Extending our prior works, we demonstrate the results of the proposed canonical 
image selection and preliminary outcomes of large-scale image graph construction in this 
proposal. We experiment in Flickr550 dataset, containing 540,321 Flickr photos. 
 
Travel Support: 
We also thank the travel support for this project, which has help the PI attend the important 
international conference and fulfilled some research service in the international community in 
terms of: 
• Serving in the ICME 2009 organizing committee as the publicity chair 
• Hosting and presenting in the proposed special session, “Knowledge Discovery Over 
Community-Sharing Media: From Signal To Intelligence” 
• Presenting paper “Foreground Segmentation For Static Video Via Multi-Core and 
Multi-Modal Graph Cut” 
• Invited as one of the panelists for the panel “What will multimedia be like on a hand-held 
device?” 
• Serving as the Technical Committee member for the premier (top) conference in 
multimedia community, ACM Multimedia 2009.  
 
2Fig. 2. Flow diagram of Semantic Query Expansion. Using the auxiliary knowledge, such as Flickr and Google to build a combinational image graph, to
refine initial image list generated by any of CBIR methods.
of media-sharing services (e.g., Flickr) [10]. As in 2, collecting
images from Flickr, we construct combinational image graph
models utilizing textual (e.g., tags) and visual (e.g., visual
words) information by capturing the feature co-occurrence
information [11]. Since tags are manually entered, they can
be subjective, wrong and ambiguous [12]. To compensate, we
exploit the knowledge from Google to discover the semantic
similarity between tags [13]. Another image graph is then built
and combined with the other two image graphs.
Fig. 1(f) shows the top 5 images retrieved by semantic
expansion on the initial results of the visual word modality.
The images retrieved are not only correct, but also exhibit
diversity. More formally, by experimenting over consumer
photo dataset, we show that semantic expansion is capable
of improving search performance by more than 200%.
Besides the exponential growth of images on the Web, the
prevalence of personal digital cameras has also given rise to
the number of images seen in personal storage. Means to
perform image retrieval in local storage become necessary.
Due to the lack of textual information such as tags for local
images, we have designed an extension to semantic expansion
framework to overcome the challenge. For ease of illustration,
we define two terms: social media resource and non-social
media resource. Social media resource refers to media resource
associated with rich textual information such as tags provided
by users. Non-social media resource refers to media resource
without any textual information such as a personal collection
of photos.
With the lack of textual information, non-social media
resource can only be searched using visual features. By
incorporating the combinational image graph into the search
process, rich textual information from social media resource
is utilized in improving search results on local resource. This
reduces the chance of search results being heavily biased by
the query image. Our experiments (cf. Sec. VI) show that
search results for non-social media are improved by 64%.
Through the flexibility of the proposed framework, keyword-
based image retrieval and image auto-annotation (cf. Sec.
V) are easily enhanced. For keyword-based image retrieval,
the retrieved results are nearly all correct. The image auto-
annotation results also outperform the prior work with less
constraint on the input (e.g., AnnoSearch [5]). In short,
semantic expansion is a novel approach, which leverages
information from multiple modalities to discover the semantic
similarity between images. It is also a flexible framework that
can be easily extended to keyword-based image retrieval and
image auto-annotation. Experimenting over consumer photo
benchmarks, we demonstrate the effectiveness of semantic
expansion for enhancing these systems.
II. RELATED WORKS
Traditional CBIR systems depends on color, texture, and
shape features extracted from data images. In representative
systems such as QBIC [14], Photobook [15], and NeTra [16],
these features are often combined to achieve better perfor-
mance. However, semantic gap problem still exists. [17], [18],
[19] try to discover the semantic meaning in visual features
by constructing aspect models. However, the semantic level
of visual features is much lower than that of textual words.
Hence, these approaches to extract semantic meaning from
visual features have not obtained the same success as applying
on words.
To bridge the semantic gap, multimedia analysts and on-
tology specialists propose concept ontology such as Large-
Scale Concept Ontology for Multimedia (LSCOM), composed
of a large set (∼1000) of high-level semantic concepts [6].
All these concepts are related to categories like events, ob-
jects, scene, people, etc., which can be found in most visual
databases such as broadcast news videos and photos [9].
However, the concepts are still limited to specific domains,
suffer from poor detection quality, and require extra time for
learning and training data acquisition.
Researchers have also utilized multimodal image retrieval
in attempt to close the semantic gap. Multimodal image
retrieval leverages simultaneously several data types to im-
prove retrieval performance [20], [21], [22]. [22] learns the
semantic similarity of web images by exploiting the mutual
reinforcement between images and textual information in sur-
rounding text. [21] uses image graph built by blocks extracted
from vision-based page segmentation, link information and
surrounding text to cluster and search WWW images. Both
[21] and [22] assume that accurate links between web sites
exist and the textual information is sufficient and correct.
[20] constraints the latent space by focusing on text first and
then learns the visual variations conditioned on the space
learned from text. However, in practical use, [20], [21], [22]
4B. Image Feature Extraction
Annotated image can be thought as the documents com-
bining two complementary modalities: textual and visual
modalities. Each modality is represented in a discrete vector-
space form, are called bag-of-words and bag-of-visual words,
respectively. Bag-of-feature stores the occurrence of features
in the image. We call the collected images Ias bag-of-images,
where I = {I1, I2, ..., Im}.
Textual:The set of captions of image is collected from the
title, description and tags. The textual modality of image is
represented as a vector T, where T = {t1, t2, ..., tn} stores
the occurrence of tags associated with the given image. T is
called bag-of-words. We can create a image-to-textual matrix
where rows represent image index, columns represent bag-of-
words index.
Visual: We also extract visual feature from images. Among
all methods, we choose visual words [8] which performs best
among all others in our experiment as in Fig. 1. Images are
scanned offline to detect salient regions (DoG), which are
described by local feature descriptors (SIFT) [26]. Theses
descriptors are quantized by k-means clustering to generate
a visual vocabulary as bag-of-visual-words, and each salient
regions is mapped to the visual word closest to it under this
clustering. The visual modality of an image is represented as
a vector V, where records the visual words histogram. We
create an image-to-visual matrix where rows represent image
index, columns represent bag-of-visual-words index. In our
experiment, we set the size of bag-of-visual words as 7000.
C. Feature Co-occurrence Capturing
With the observation that two documents sharing the same
words are usually with high similarity, some generative models
[11], [25] proposed models to capture the co-occurrence infor-
mation between documents and words to make comparisons
of similarity between documents. Among the previous works,
pLSA is an aspect model that probabilistically links the ele-
ments in the vector space through latent aspect variable one of
the models, which is also effective in solving some well-known
problems when facing discrete data: different elements from
the vector space can represent the same concept (synonymy)
and one element might have different meanings (polysemy).
Applying here, images can be thought as documents, textual
and visual information can be thought as the words in docu-
ments. With the image-to-textual and image-to-visual matrices
obtained in Sec. III-B, we apply pLSA to capture the co-
occurrence information on each matrix.
1) Aspect Model: With the assumption that features in
image are generated independently, aspect model assumes the
existence of a latent variable z (aspect) in the generative
process of each element wj in a particular document Ii .
Given this unobserved variable z which represents latent topic,
each occurrence is independent from the document it was
generated, which corresponds to the following joint proba-
bility: p(Ii, wj , zk) = p(zk)p(Ii|zk)p(wj |zk). Summing over
all the possible choices of z, we could get the co-occurrence
probability of Ii and wj as follows:
p(Ii, wj) =
∑
k
p(wj |zk)p(zk|Ii), (1)
The parameters are solved by Expectation Maximization
(EM) algorithm [11]. P (z|I), P (w|z) and P (z) are three
probability term learnt from the model which will be used in
the following sections. z denotes the latent topic vector, where
P (z) represents the probability of each latent topic occurs in
the dataset. P (z|Ii) d denotes the distribution probability of
image Ii on latent topics. P (wj |z) presents the probability of
word wj occurs in latent topics.
2) Constructing Image Graph based on Textual and Vi-
sual Co-occurrence Observation (G(1) and (G(2)): Given an
image-to-feature matrix as an input to an aspect model, we get
P (z|Ii) denoting the distribution of image Ii on all possible
choices of latent topic z. Intuitively, if two images has similar
distribution on z, they should contain the similar concept and
belong to the similar topic. Hence, to measure the similarity
of two image Ii and Ij , we calculate their distance in aspect
space measured with cosine distance between P (z|Ii) and
P (z|Ij). We can then build an image graph G, where nodes
are images, link between them are the transition probability
with G = [Gij ] = cos(P (z|Ii), P (z|Ij)).
Given two matrices: image-to-textual and image-to-visual
obtained in Sec. III-B, {P1(z), P1(z|I), P1(T|z)} and
P2(z), P2(z|I), P2(V|z) are then generated through Sec.
III-C, respectively. We build first image graph, G(1), where
G(1) = [G
(1)
ij ] = cos(P1(z|Ii), P1(z|Ij)). Same process
in second image graph, G(2), where G(2) = [G(2)ij ] =
cos(P2(z|Ii), P2(z|Ij)). To let the summation of all proba-
bilities induced from any state be one, we normalize each row
as sums to 1, which is more reasonable in our scenario.
D. Bag-of-words Semantic Similarity Discovering
When tagging, there is no vocabulary to follow with. Word
variations then occur frequently, and Long Tail proves that in
. For example, Eiffel, Eiffeltower, Eiffiel, Eiffle and Eiffeil
are associated with some photos in the Flickr. We believe
this five words point to the same meaning Eiffel. However,
aspect model as in Sec. III-C will treat them as individual
words. Such mistyping or subjective words occur frequently
and easily ruin the learning model. To solve this problem,
we crawls the knowledge from Google which is shown as
a good resource in measuring word similarity effectively
[23] to discover the semantic similarity between bag-of-words
conducted by web-based kernel function [13].
1) Semantic Meaning of Bag-of-words: To discover the
semantic meaning of bag-of-words to solve mistyping or
subjective words problem, we expand each word from bag-
of-words to a fixed-size vector used to describe topic or
concept of that given word. We take an example in Fig.
4, tour, paris, building, theatre, are the terms in expanded
vector used to describe the concept of given words: Eiffel,
Eiffeltower, Trocadero and Starbucks. We can find that Eiffel
and Eiffeltower are most similar among them, and Trocadero
which is a hotel located at France is less similar. Starbucks
6resource can also be used as the dataset to be retrieved.
Considering other needs, for example, user may want to
retrieve local images at the user end or any other social
media resources without tags associated with. To differentiate
different usages, the resources are named as In-Social Media
resource and Out-Social Media resource.
In-Social Media resource: With sufficient textual and visual
information. For example, Flickr. Images in this resource can
be also used for image graph construction, which are called
bag-of-images. Example-based image retrieval on this resource
will be described in Sec. IV-A.
Out-Social Media resource: Without textual information,
the only cue can be used is visual information. This kind
of resources are more common and even more practical,
but more challenging, though. For example, local images
at user end, or any social media resource without textual
information. Example-based image retrieval on this resource
will be described in Sec. IV-B.
A. Semantic Image Retrieval on Bag-of-images
Among all the approaches in example-based image re-
trieval , visual words performs the best among all of state-
of-the-art as in our experiment. Hence, we calculate the
distance between bag-of-images I against with a query image
q by L1 distance on visual words, denotes as L1(I, q) =
{L1(I1, q),L1(I2, q), ...,L1(Im, q)}. To let the distance be
more smoothing, it is normalized with sigmoid function. We
also do a revision to let the order be opposite to L1(I, q)
and let the value be between 0 and 1. The formulation is as
follows:
P˜ (Ii, q) = 2(1 − sigmoid(L1(Ii, q))), (4)
where sigmoid(x) = 1/(1 + e− xσ ) and σ is used as a
smoothing variant, which is usually set as standard deviation
of L1(I, q). The initial image list is then ordered as their value
in P˜ .
We define π(0) as an initial state vector denotes the
probability of each state random walk process starting with.
We choose top K images in the initial image ranking list,
normalize their scores in the list to be the value in π(0). In
the experiment, we set K as 50. Let us consider a random surf
on the image graph. Random surfer starts with initial states
π(0) and jump to different states iteratively. The probability
of jump from Ii to Ij is Gij which is also called transition
probability. When converges, the stationary states are set to be
the final image list ordered as the probability random surfer
stays. Since we filter most of the images in the dataset, only
50 images are set as the initial states, random walk could help
to find the representative images depends on this preference.
1) Random Walk: Besides G, usually, there is also a possi-
bility that the surfer does not follow the probability transition
matrix induced from the image graph G but jumps to an image
picked uniformly from the collection which is used to make
graph connected, and hence the stationary states of random
walk always exists [18]. Thus, after the refinement, the new
probability transition matrix P is denoted as follows:
P = ǫG + (1 − ǫ)U (5)
U is a uniform transition matrix with the same size of G.
Starting with the initial states with their probability in π(0),
the surfer follows the transition probability in P during each
iteration. At iteration t, the surfer stops at every images with
probability in π(t), where π(t) = π(t−1). When converges, we
set the final state vector as π˜, where π˜ = π˜ · P . In empirical
study, ǫ is set to be 0.1, which meets the anticipation as [21]
mentioned.
2) Random Walk with Restarts: There is a different ap-
proach called random walk with restarts (RWR) which is
based on the sample principle, however, during each iteration,
the random surfer would restart from the initial states with
probability 1 − ǫ , and jump between states following the
transition probability of G with probability ǫ . The equation
is as follows:
xt = ǫx(t−1)G + (1 − ǫ)v (6)
where v is the initial state vector π(0), x(0) is set as a uniform
vector. Different from the Eqn. 5, random surfer considers the
weight on initial states during each iteration. At iteration t, x(t)
is the states vector with the value as the probability random
surfer stays. In our experiment, the best setting of ǫ in RWR
is set as 0.9 which is also the best value reported in [28].
The experimental comparison between Eqn. 5 and 6 will be
discussed in Sec. VI-B1). We denote the final states vector
as with the value as the probability random surfer stays
representing how likely each image is similar to query image.
The final image ranking list is then generated, ordered as the
probability in .
B. Semantic Image Retrieval on New Resource
In the previous section, we introduced how does semantic
expansion work for example-based image retrieval on bag-
of-images. In some needs, user may want to retrieve images
inside any new resource without tags associated with which is
defined as Out-Social Media resource, such as local images.
Fig. 5 illustrates the process of our approach and the difference
to the conventional approach. In traditional way, the only cue
to use is through visual features. Within our framework, we
can use auxiliary knowledge learned from the In-Social Media
resource to do image query expansion and then search images
in Out-Social Media resource.
We denote images in new resource as J . Starting as the
same process in the retrieval on bag-of-images as in Sec. IV-A,
given an image Iq , we obtain an final image ranking list and
probabilities for stationary states, π˜. We collect top K images
from ranking list, denoted as I′ which is a subset of bag-of-
images, along with their value in π˜ as π˜′ . In the experiment,
we set K as 50.
Since images in I are the most representative images to the
query image q, we see I as the image query expansion to search
in the new resource. Since the only cue is visual feature, we
use in Eqn. 4 as the method to find the most similar images
in the new resource. Consider the probability each expanded
8of avoiding being biased to the image query. Besides, aspect
model can bridge bag-of-images to bag-of-words with latent
meaning.
VI. EXPERIMENTS AND APPLICATION RESULTS
In this section, we evaluate the performance of three appli-
cations: example-based image retrieval (Sec. VI-B), keyword-
based image retrieval (Sec. VI-C) and image auto-annotation
(Sec. VI-D). As we have mentioned, two different resources:
In-Social Media resource and Out-Social Media resource will
be evaluated separately in Sec. VI-B1) and Sec. VI-B2)
respectively.
The performance of each query is evaluated in terms of
average precision (AP), which approximates the area under
a non-interpolated recall/precision curve. We also use mean
average precision (MAP) to measure all the queries.
A. Data set
Flickr5k dataset: This is the subset of the Flickr550 dataset
[29] downloaded from Flickr. We manually labeled 2825 im-
ages as our ground truth across ten categories including Tower
Bridge (178), Triomphe (122), pisa tower (139), starbucks
(62), pyramid (198), eiffel tower (544), golden gate bridge
(190), colosseum (169), church (863) and beer (360). We also
randomly selected images among 550k images, all with tags,
to be used as noisy images. We fuse those ground truth images
and noisy images as our experiment dataset with size of 5k.
B. Example-based Image Retrieval
Given an unknown image without any textual information
associated with, we extract its visual features as Sec. III-B
mentioned. With the help of human-labeled ground truth,
we evaluate the retrieved images on average precision (AP).
We compare our approach against with CBIR state-of-the-
art methods (e.g., Grid Color Moment, Gabor Texture, visual
words, high-level semantic concepts) which are set as our
baseline. For detection of semantic concepts, we utilize the
SVM-based models donated by Columbia University to detect
the concept scores for the LSCOM lexicon [6], a set of 374
visual concepts annotated over an 80-hour subset of TRECVID
data. As the different needs mentioned in Sec. IV, we evaluate
two different resources separately: In-Social Media resource
and Out-Social Media resource.
1) Experiment on In-Social Media Resource: In In-Social
Media resource, we have images with tags associated with.
This resource is also the one where we built image graph
from. In our experiment as in Fig. 6, semantic expansion gets
0.542 in MAP which greatly refines the result of visual words
with 0.147 in MAP, which is the best one among state-of-the-
art. MAP of Grid Color Moment, Gabor Texture, and even
high-level semantic concepts are between 0.9 to 0.12. G(1),
G(2), and G(3) are the three image graphs introduced in Sec.
III. We evaluated them alone, which are all worse than se-
mantic expansion. Hence, improvement from the combination
of three images is evidenced in the experiment. To evaluate
different measurements for image retrieval in terms of visual
words, we also conduct an experiment using aspect model
to calculate image distance in aspect space. Comparing with
three experiments conducted in terms of visual words, G(2)
performs better than the distance calculated in aspect space
(0.21). As expected, L1 distance performs the worst (0.15).
To see each category performance, we show them in Fig.
7. Concept-related categories (e.g., church and beer) and less
importance objects within images (e.g., Starbucks) are very
difficult to be retrieved through visual features only. Concept-
related category is a well-known problem in CBIR since no
regular visual features exist in all images in the category.
Starbucks logo is usually less importance than people or
other objects within image. In other words, a large part of
visual points outside Starbucks logo become the noise features
which could lead to an unsatisfying result. Even so, we still
improve more than 200% in these three categories.
The above examples are some of the limitations in adopting
visual features. Nevertheless, other results are really encourag-
ing. For example, landmark-related categories (e.g., Triomphe,
Pisa, Eiffel tower, Golden Gate Bridge, and Colosseum) all get
higher than 0.6 in AP. In general, through semantic expansion,
we could improve more than 100% for each category and
200% in terms of MAP. Although, semantic expansion could
be sensitive to the specialty of visual features, it is still a
promising approach.
We also compare different random walk types as mentioned
in Sec. IV-A. In our experiment, MAP in RWR is far behind
that in simple random walk. We think the reason is that,
in RWR model, the current states would go back to initial
states with probabilities during each iteration; however, the
performance of initial states is unsatisfying. If we set ǫ in
Eqn. 6 as a small number, then random surfer would never
know the states as user anticipated. As ǫ is increasing, random
surfer finds out what uses interests are gradually, but also
gets affected by the performance of initial states which is
only 0.147 in MAP as in Fig. 6. Hence, simple random walk
performs better than RWR does.
2) Experiment on Out-Social Media Resource: In Out-
Social Media resource, images can be collected from local
images or from other resources without tags associated with.
In experiment, we randomly select 10% of images from In-
Social Media resource, and remove tags to create Out-Social
Media resource in each round. We use 10-fold cross validation
to measure the performance of our framework on Out-Social
Media resource. Since no tags associated, the targeting images
can only be retrieved through visual features in the conven-
tional approach. Hence, we use visual words measured by L1
distance as our baseline.
From Fig. 8, we can find that the MAP of baseline is
0.079 which is worse than that of in In-Social Media resource
(0.147). As only a few candidate images could be retrieved, the
varies on visual lead the difficulty when retrieving. Comparing
with Fig. 7 and Fig. 8, performance of categories in Out-
Social Media resource all drop due to the same reason as
baseline. Even so, our method still outperforms baseline in a
64% relative improvement.
There are two stages when retrieving on out-social media
resource. The first stage is doing semantic expansion. The
10
might be decreasing since some un-related words might be
also referred. The best of our approaches is 0.963 in MAP
which is nearly all correct.
D. Image Auto-annotation
In Sec. V-B, we have explained how image auto-annotation
works with our framework. Since our method can be classified
as search-based image annotation (SBIA), we compare our
method with AnnoSearch [5]. AnnoSearch proposed by Wang
et al. is a promising SBIA system leveraging the surrounding
text of web images. AnnoSearch assumes an initial keyword
associated with the input image before automatic image an-
notation process starts. However, such keyword is not always
available in practical applications, and in our experiment, the
image query has no keyword associated with. For the fairness
of comparison, we remove this assumption in AnnoSearch.
We measure the performance of auto-annotation with three
metrics: mean reciprocal rank (MRR), success at rank 1 (s@1)
and precision at rank 5 (P@5).
MRR: Evaluate how good an algorithm is by measuring
the rank of the first relevant annotation. S@1: Measure the
probability of the first annotation is a relevant annotation.
P@5: Measure the precision in Top 5 recommend annotations.
The performance comparison between AnnoSearch and our
approach is listed in Table III. In all three metrics, the
annotations returned by semantic expansion outperform that by
AnnoSearch. It is proved that semantic expansion could reduce
the possibility of being biased to image query. Since there
is no initial keyword given as an extra information for auto-
annotation process, AnnoSearch becomes unattractive. With
the observation that MRR and S@1 are both 0.82 in semantic
expansion, the first recommended annotation is certainly a
relevant one as long as some recommended annotations are
relevant. In other words, top 1 can decide whether the case is
success or not.
VII. CONCLUSIONS
In this paper we propose a novel method, semantic expan-
sion, which is proved to refine the image list generated by any
of CBIR methods. The proposed method combines two modal-
ities in consumer photo: textual and visual, which utilizes the
textual/visual feature co-occurrence information. Moreover, to
solve the noisy data, we find the semantic similarity between
tags by exploiting the knowledge in Google. In our experiment,
we evaluate our method on different resources (tags available
or not) separately and compare with CBIR methods. Our
method get 3 times improvement in average on both resources,
which is an encouraging result. With semantic expansion
framework, we can also do keyword-based image retrieval and
image auto-annotation with no more effort. Hence, semantic
expansion is a novel idea, shown as a promising method, which
improves the current works on example-based image retrieval,
keyword-based image retrieval and image auto-annotation.
REFERENCES
[1] N. Snavely, S. M. Seitz, and R. Szeliski, “Photo tourism: exploring photo
collections in 3d,” in SIGGRAPH ’06: ACM SIGGRAPH 2006 Papers.
New York, NY, USA: ACM, 2006, pp. 835–846.
Fig. 3. Tag frequency distribution on our Flickr5k dataset. Word variations
frequently occurs and long tail is observed. We take five tags as examples
which are all eiffel-related terms. They are labeled with name and frequency:
eiffel with 569 times, eiffeltower with 22 times, eiffiel with 8 times, eiffle
with 3 times and eiffeil with 1 time.
Fig. 4. An example of word expansion for bag-of-words – tour, paris,
building, theatre, – as expanded terms to describe the concept of given
words: Eiffel, Eiffeltower, trocadero, Starbucks, etc. The value represents the
frequency each expanded term occurs in snippets. We can find that (A) and
(B) are similar since their expanded vectors are similar. Compared with (B),
(C) is less similar to (A). (D) is totally different with (A)-(C) and has zero
value on these four expanded terms.
TABLE I
THE MOST SIMILAR FIFTEEN TAGS FOR THE TAG “EIFFEL” AND THE
(SEMANTIC) SIMILARITY SCORES BY GOOGLE SNIPPETS (CF. SECTION
??), WHICH EFFECTIVELY FINDS MANY RELATED TAGS CHALLENGING FOR
DICTIONARY (OR ONTOLOGY) BASED METHODS.
# tag (similarity) # tag (similarity) # tag (similarity)
1 Tour-eiffel (0.67) 6 Tour-de-eiffel (0.58) 11 Eiffellight (0.45)
2 Torre-eiffel (0.66) 7 Visittoureiffel (0.50) 12 Eiffle (0.41)
3 Toureiffel (0.65) 8 Visteiffel (0.50) 13 Effiel (0.41)
4 Eiffeltower (0.63) 9 Sparklyeiffel (0.50) 14 Eiffeil (0.40)
5 Eiffel-tower (0.62) 10 Eiffiel (0.46) 15 Effel (0.40)
TABLE II
MAP OF KEYWORD-BASED IMAGE RETRIEVAL. COMPARING WITH
BASELINE (KEYWORD SEARCH) SEPARATED BY TWO OPERATIONS: AND
(0.758) AND OR (0.599), OUR EXPANSION (I.E., RERANKING [31] HERE)
METHOD ACHIEVES SIGNIFICANTLY BASED ON TOP K (I.E., 1, 3, AND 5)
EXPANDED TAGS (CF. SECTION ???).
Fusion Method Top 1 Top 5 Top 10
AVG 0.858 0.963 0.962
MAX 0.860 0.962 0.961
MIN 0.768 0.768 0.767
TABLE III
MAP OF IMAGE ANNOTATION. OUR APPROACH OUTPERFORMS
ANNOSEARCH [5].
MRR S@1 P@5
AnnoSearch [5] 0.187 0.080 0.088
our method 0.820 0.820 0.404
12
!"#!$$
!"#%$$
!"&!$$
!"&%$$
!"%!$$
!"%%$$
!"'!$$
($ !")$ !"*$ !"+$ !"'$ !"%$ !"&$ !"#$ !",$ !"($ !$
!!-$$!
./0!
Fig. 11. Evaluation for the image graph combination with variant γ in Eqn.
3, and α = β = (1− γ)/2. The best of γ when experimenting on example-
based image retrieval is 0.8.
!"#!$$
!"#%$$
!"&!$$
!"&%$$
!"'!$$
!"'%$$
!"(!$$
!"(%$$
!"%!$$
#!$ &!$ '!$ (!$ %!$ )!$ *!$ +!$ ,!$ #!!$
-.$!"#$
-.$!"&$
-.$!"'$
-.$!"($
-.$!"%$
-.$!")$
-.$!"*$
/01$234567$829:;<9$9457!
=>?!
Fig. 12. As the percentage of untagged images is increasing, the importance
of visual-based approach is increasing.
auto-annotation by search,” in CVPR ’06: Proceedings of the 2006
IEEE Computer Society Conference on Computer Vision and Pattern
Recognition. Washington, DC, USA: IEEE Computer Society, 2006,
pp. 1483–1490.
[6] A. Yanagawa, S.-F. Chang, L. Kennedy, and W. Hsu, “Columbia
university’s baseline detectors for 374 lscom semantic visual concepts,”
Columbia University, Tech. Rep., March 2007.
[7] B. S. Manjunath and W. Y. Ma, “Texture features for browsing and
retrieval of image data,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 18, no. 8, pp. 837–842, 1996.
[8] J. Sivic and A. Zisserman, “Video google: A text retrieval approach
to object matching in videos,” Computer Vision, IEEE International
Conference on, vol. 2, p. 1470, 2003.
[9] A. P. Natsev, A. Haubold, J. Tesˇic´, L. Xie, and R. Yan, “Semantic
concept-based query expansion and re-ranking for multimedia retrieval,”
in MULTIMEDIA ’07: Proceedings of the 15th international conference
on Multimedia. New York, NY, USA: ACM, 2007, pp. 991–1000.
[10] M. Ames and M. Naaman, “Why we tag: motivations for annotation
in mobile and online media,” in CHI ’07: Proceedings of the SIGCHI
conference on Human factors in computing systems. New York, NY,
USA: ACM, 2007, pp. 971–980.
[11] T. Hofmann, “Probabilistic latent semantic indexing,” in SIGIR ’99:
Proceedings of the 22nd annual international ACM SIGIR conference
on Research and development in information retrieval. New York, NY,
USA: ACM, 1999, pp. 50–57.
[12] L. Kennedy, M. Naaman, S. Ahern, R. Nair, and T. Rattenbury,
“How flickr helps us make sense of the world: context and content
in community-contributed media collections,” in MULTIMEDIA ’07:
Proceedings of the 15th international conference on Multimedia. New
York, NY, USA: ACM, 2007, pp. 631–640.
[13] M. Sahami and T. D. Heilman, “A web-based kernel function for mea-
suring the similarity of short text snippets,” in WWW ’06: Proceedings
of the 15th international conference on World Wide Web. New York,
NY, USA: ACM, 2006, pp. 377–386.
[14] M. Flickner, H. Sawhney, W. Niblack, J. Ashley, Q. Huang, B. Dom,
M. Gorkani, J. Hafner, D. Lee, D. Petkovic, D. Steele, and P. Yanker,
“Query by image and video content: The qbic system,” Computer,
vol. 28, no. 9, pp. 23–32, 1995.
[15] A. Pentland, R. Picard, and S. Sclaroff, “Photobook: Content-based
manipulation of image databases,” International Journal of Computer
Vision, vol. 18, no. 1, 1996.
[16] W. ying Ma and B. Manjunath, “Netra: A toolbox for navigating large
image databases,” Multimedia Systems, pp. 568–571, 1999.
[17] R. Lienhart and M. Slaney, “Plsa on large scale image databases,” in
Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE
International Conference on, vol. 4, April 2007, pp. IV–1217–IV–1220.
[18] E. Ho¨rster, R. Lienhart, and M. Slaney, “Image retrieval on large-
scale image databases,” in CIVR ’07: Proceedings of the 6th ACM
international conference on Image and video retrieval. New York,
NY, USA: ACM, 2007, pp. 17–24.
[19] K. Barnard and N. V. Shirahatti, “A method for comparing content based
image retrieval methods,” in Internet Imaging IX, Electronic Imaging,
2003.
[20] F. Monay and D. Gatica-Perez, “Plsa-based image auto-annotation: Con-
straining the latent space,” in In Proc. ACM Int. Conf. on Multimedia,
2004, pp. 348–351.
[21] X. He, D. Cai, J. rong Wen, W. ying Ma, and H. jiang Zhang,
“Imageseer: Clustering and searching www images using link and page
layout analysis,” Microsoft, Tech. Rep., 2004.
[22] X.-J. Wang, W.-Y. Ma, G.-R. Xue, and X. Li, “Multi-model similarity
propagation and its application for web image retrieval,” in MULTIME-
DIA ’04: Proceedings of the 12th annual ACM international conference
on Multimedia. New York, NY, USA: ACM, 2004, pp. 944–951.
[23] R. L. Cilibrasi and P. M. B. Vitanyi, “The google similarity distance,”
IEEE Trans. on Knowl. and Data Eng., vol. 19, no. 3, pp. 370–383,
2007.
[24] L. Wu, X.-S. Hua, N. Yu, W.-Y. Ma, and S. Li, “Flickr distance,”
in MM ’08: Proceeding of the 16th ACM international conference on
Multimedia. New York, NY, USA: ACM, 2008, pp. 31–40.
[25] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,” J.
Mach. Learn. Res., vol. 3, pp. 993–1022, 2003.
[26] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
International Journal of Computer Vision, vol. 60, pp. 91–110, 2004.
[27] R. J. Bayardo, Y. Ma, and R. Srikant, “Scaling up all pairs similarity
search,” in WWW ’07: Proceedings of the 16th international conference
on World Wide Web. New York, NY, USA: ACM, 2007, pp. 131–140.
[28] A. N. Langville and C. D. Meyer, “A survey of eigenvector methods
for web information retrieval,” SIAM Rev., vol. 47, no. 1, pp. 135–161,
2005.
[29] Y. H. Yang, P. T. Wu, C. W. Lee, K. H. Lin, W. H. Hsu, and H. H.
Chen, “Contextseer: context search and recommendation at query time
for shared consumer photos,” in MM ’08: Proceeding of the 16th ACM
international conference on Multimedia. New York, NY, USA: ACM,
2008, pp. 199–208.
[30] http://www.semanticmetadata.net/2008/07/03/less-than-20-of-flickr-
images-tagged//.
[31] W. H. Hsu, L. S. Kennedy, and S.-F. Chang, “Video search reranking
through random walk over document-level context graph,” in ACM
Multimedia, Augsburg, Germany, 2007.
images are useful since they can be chosen as the cluster
centers to guarantee the diversity of generated image clus-
ters. Recently, we propose ContextSeer [6] which recom-
mends canonical images by leveraging rich context cues of
images and considers both coverage and diversification. The
method considers image representativeness, coverage and di-
versification in the collection. Because of the promising per-
formance of ContextSeer, we will apply this approach to
extract canonical images from image search results in order
to generate image clusters later.
In order to demonstrate the performance of cannoG [6],
the canonical image selection algorithm presented in Con-
textSeer, we show top 2 canonical image results for each of
two queries in Figure 2 that are generated by three algo-
rithms: Affinity Propagation [2], ImageRank and cannoG.
2.2 Large-Scale Image Graph Construction
We constructed image context graphs for large (0.5 mil-
lion) data collections using MapReduce model. The images
are now represented by 10K-dim. visual words and 50K-dim.
expanded tags by Google snippets. MapReduce is a scalable
model that simplifies large-scale computations. Considering
the scale of our data, we take the advantage of its sparse-
ness and use cosine measure in our work. Our algorithm
extends the method proposed in [1]. The algorithm uses a
two-phase MapReduce model: indexing phase and calcula-
tion phase, to calculate pairwise document similarity. At
indexing phase, each input image feature vector is mapped
to a list of feature-image-value tuples, and then all the tuples
are organized into inverted lists according to their feature.
At calculation phase, we use the inverted lists to compute
a similarity value for each pair of images in each inverted
lists, then the values for each pair of images are aggregated
into a single value, which is the pairwise similarity value
of corresponding pair of images. To reduce the calculation,
the authors in [1] implemented a df-cut. However, after try-
ing several cutting methods, we found that tf-idf is a better
choice for Flick photos.
Currently, we have experimented to compute pairwise im-
age similarity using different cutting thresholds of a 11K
image dataset to investigate the amount of time and ac-
curacies. The best accuracy (in MAP) occurs at the 0.07
tf-idf cut and the timing is 2.5 times faster than the base-
line. And at the 0.11 tf-idf cut, the MAP is almost as good
as the baseline but it only takes 7% of the baseline original
time. We have found that the tf-idf value distribution of our
550K image dataset is quite close to this dataset. We have
successfully finished two image context graphs (tag and vi-
sual words) over the 550K Flickr photos. For image graph
by visual words, the execution time is about 42 minutes on
17-node hadoop platform.
2.3 Graph-based Image Grouping
Once we have canonical images extracted from image search
results, next we group remaining images to these canoni-
cal images. Leveraging the large-scale image context graph
built using MapReduce, we propose to group the images on a
sub-graph sampled from these large-scale graphs (e.g., visual
words, tag). The sub-graph is formed by the remaining im-
ages as vertices and similarities of images as edges. Although
we can construct the sub-graph in real time when searchers
send queries, the time required to build the sub-graph grows
drastically with the size of image search results. It is not ap-
             (a) Affinity Propagation            (b) ImageRank                       (c) cannoG   
Figure 2: The canonical images selected by three
different algorithms for two queries: Golden Gate
Bridge (1st row) and Starbucks (2nd row). The pro-
posed cannoG selects more representative images for
Golden Gate Bridge than other two algorithms. For
Starbucks , cannoG are apparently better than Im-
ageRank and competes with Affinity Propagation.
plicable for a general image search engine. Instead, we only
sample images in image search results from the already built
large-scale graph. When searchers send queries, we sample
corresponding vertices and edges from the large-scale graph
with respect to images in retured search results. These sam-
pled vertices and edges are then constructed as wanted sub-
graph. Since we propose to construct the large-scale image
graph oﬄine with distributed MapReduce, this approach is
promising in either efficiency and effectiveness.
3. EXPERIMENTS AND EVALUATIONS
Dataset: As the dataset for image search clustering with
enough diversity, we take the Flickr550 dataset [6] as our
dataset. The dataset includes 540,321 images crawled from
Flickr. It is a large-scale image collection with high visual
and semantic diversity. We define 21 semantic queries cov-
ering several types of information needs and manually anno-
tate them. The queries can be classified to four categories
such as “Lamarks,”“Objects,”“Scenes,” and “Activity.”
Evaluation metrics: Considering the evaluation crite-
ria for the challenge of image search clustering, we are eval-
uating our proposed system by (1) subjective test for the
representativeness of canonical images with regard to user
query, (2) precision for the image clusters, (3) performance
efficiency of the underlying algorithm: the time from users
send query to generate final image clusters, and (4) the time
users spend to find an image satisfying user intent.
In order to better evaluate our proposed approach and
system, we build an online baseline for image search result
clustering that leverages text description of images as the
approach proposed by Jing et al. in [4].
4. REFERENCES
[1] T. Elsayed and et al. Pairwise document similarity in large
collections with mapreduce. In ACL, 2008.
[2] B. Frey and D. Dueck. Clustering by passing messages between
data points. Science, 315:972–976, 2007.
[3] W. Hsu and et al. Video search reranking through random walk
over document-level context graph. In ACM Multimedia 2007,
September.
[4] F. Jing and et al. Igroup: web image search results clustering. In
ACM Multimedia 2006.
[5] R. Leuken and et al. Visual diversification of image search
results. In WWW 2009.
[6] Y. Yang and et al. Contextseer: Context search and
recommendation at query time for shared consumer photos. In
ACM Multimedia 2008.
1122
3. MULTIMEDIA SEARCH
Current social media search approaches are mostly restricted to text-
based solutions which process keyword queries against the tags or
descriptions that are provided by users via some lightweight anno-
tation tools. The associated tags may contain abundant information,
yet their qualities are not uniformly guaranteed. Tags are therefore
often inaccurate, wrong or ambiguous [10]. In particular, due to the
complex motivations behind tag usage [4], tags do not necessarily
describe the content of the image [8].
To remedy the limitation in (noisy or missing) tags, the authors
propose ContextSeer, which formulates the keyword search as a
ranking problem and fuses rich context cues (e.g., time, geo-tags,
user-contributed tags, visual features, etc.) of shared consumer pho-
tos to improve the search result and even to recommend relevant
tags and canonical images [3]. In [16], the authors exploit visual
annotations (i.e., “notes” in Flickr) to enhance keyword-based photo
search. The notes generally highlight a certain region (of interest) in
the photo and associate a tag with the region—providing consistent
visual and textual coherence.
A promising aspect for user-contributed photos and videos is
their small-world phenomenon [5]. They are shown contextually
correlated and can be embedded in a graph weighted by visual and
context cues automatically. Such (context) graphs have shown ef-
fective for keyword-based photo [17] or video [18] [19] search by
efficient random-walk-like methods.
4. MULTIMEDIA ADVERTISING
While research on advertising has been predominantly studied in the
text domain since the end of 1990s, there has been an emerging trend
arising from multimedia advertising. This trend particularity evolves
with the unprecedented online delivery of social media, as well as the
fast and growing online advertising market in recent years. In gen-
eral, there are three key problems in an advertising system: (1) rele-
vance—which ads are to be selected from an ad database according
to a given image or video, (2) position—where the selected ads are
to be embedded, and (3) displaying—how the selected ads are to be
displayed at the detected positions. An effective advertising system
aims at maximizing the relevance between the media and ads while
minimizing the ad intrusiveness by taking the above three problems
in to account.
Conventional advertising treats image and video advertising as
general text advertising by displaying textually relevant ads based
on the contents of Web page. Compared with text, social media
like images and videos have unique advantages which consequently
make them become more effective information carriers for advertis-
ing [20]: they are more attractive and salient than plain text, thus
they can grab users’ attention instantly; they carry more information
that can be comprehended intuitively.
Recently, researchers have invented intelligent context-aware
multimedia advertising technologies that can take advantages of
the visual form of information representation. This new generation
of multimedia advertising selects the ads contextually relevant to
the media contents rather than the general Web pages, as well as
seamlessly embeds the ads within rather than around the media.
For example, the ads in [21] [22] are matched against the media
according to the multimodal relevance which consists of textual and
conceptual relevance, as well as visual similarity, while the ads in
[23] are selected by searching the exact local patches. By leveraging
vision techniques, a set of appropriate ad insertion positions can
be detected within video streams (i.e., spatio-temporal positions on
the frames) [21] [24] or images (i.e., non-salient areas) [22]. For
example, VideoSense detects ad positions on the timeline based on
the different combinations of content dissimilarity and attractiveness
[21], while the virtual content insertion system finds the low atten-
tive regions in the high attentive video shots as ad insertion positions
by visual saliency analysis [23]. While fully automatic detection
of non-salient areas within images for advertising is very challeng-
ing, ImageSense finds the image corners which are with the lowest
saliency for ad embedding [22]. The next generation of multimedia
advertising is envisioned to be game-like and more impressive [20].
5. SOCIAL MEDIA VISUALIZATION
The large amount of social media contents available on the Internet
is typically unstructured. An effective visualization of such large-
scale media collections can allow efficient indexing, browsing, and
even effective world exploration and social interaction. It has proved
effective to list tags of interest for a given location and visualize
representative media associated to these tags.
TheWorld Explorer system analyzes the tags associated with six
million geo-referenced Flickr photos, and exposes the “representa-
tive tags” for each map region and zoom level by a multi-level text
clustering process [25]. When a user points the mouse over a tag,
photos associated with that tag and from that area are visualized.
Kennedy et al. further studied how to select canonical views from
these photos to represent a landmark [26]. It suggests that visual and
geographic features can to be used to learn the visual models for se-
lecting representative photos, on the basis of the photos shared by
many individuals.
Rather than visualize social media by selecting representative
tags and views, the efforts from vision community have predomi-
nantly focused on enabling users to navigate in a virtual tour which
is created from a large photo collection. The Photo Tourism presents
a 3D interface for interactively browsing and exploring large col-
lections of unstructured photos of a scene [27]. It first estimates
the pose of camera (location, orientation, and field-of-view) based
on keypoint detection and matching, registers the photos in a global
geometry coordinate, and then provides smooth transitions between
photos by morphing techniques. As a result, it is able to present sev-
eral modes for navigating through a scene: (1) free-flight navigation,
(2) moving between related views, (3) object-based navigation, and
so on. Another work in [28] organizes the photos in themes and con-
structs a virtual 3D space for photo navigation based on the visual
similarities between the photos. The users are free to move from one
image to the next using intuitive 3D controls. In response to user
controls, the system retrieves the most similar images from several
million images, displays them in correct geometric and photometric
alignment with respect to the current photo.
For user-contributed videos, Kennedy proposed a system for
synchronizing and organizing YouTube videos for live music events
[29]. The work aggregates videos of the same venue and improves
the representation of the event content, including identifying key
moments of interest and descriptive text for important segments of
the show.
6. TRAINING DATA CROWDSOURCING
The sheer volume of these resources poses not only opportunities,
e.g., a simple non-parametric recognition approach supported by the
80 million tiny images collected from the Internet [30], but also chal-
lenges for the existing learning algorithms [31], most of which can-
not scale well to this volume straightforwardly, e.g., Support Vector
1449
Authorized licensed use limited to: National Taiwan University. Downloaded on December 8, 2009 at 10:32 from IEEE Xplore.  Restrictions apply. 
[6] L. Ahn and L. Dabbish, “Labeling images with a computer
game,” in Proceeding of SIGCHI Conference on Human Fac-
tors in Computing Systems, 2004, pp. 319–326.
[7] J. Li and J. Z. Wang, “Real-time computerized annotation of
pictures,” in Proceedings of ACM Multimedia, 2006, pp. 911–
920.
[8] L. S. Kennedy, S.-F. Chang, and I. V. Kozintsev, “To search
or to label? predicting the performance of search-based auto-
matic image classifiers,” in Proceedings of ACM International
Workshop on Multimedia Information Retrieval, 2006.
[9] J. Kustanowitz and B. Shneiderman, “Motivating annotation
for personal digital photo libraries: Lowering barriers while
raising incentives,” Technical report, Univ. of Maryland, 2004.
[10] L. Kennedy, M. Naaman, S. Ahern, R. Nair, and T. Rattenbury,
“How Flickr helps us make sense of the world: Context and
content in community-contributed media collections.,” in Pro-
ceedings of ACM Multimedia, 2007.
[11] Y.-R. Lin, H. Sundaram, M. D. Choudhury, and A. Kelliher,
“Temporal Patterns in Social Media Streams: Theme Discov-
ery and Evolution Using Joint Analysis of Content and Con-
text,” in Proceedings of IEEE International Conference on
Multimedia & Expo, Cancun, Mexico, June 2009.
[12] A. G. Hauptmann, W. H. Lin, R. Yan, J. Yang, and M. Y. Chen,
“Extreme video retrieval: Joint maximization of human and
computer performance,” in Proceedings of the ACM Interna-
tional Conference on Multimedia, 2006.
[13] K. Yang, M. Wang, and H.-J. Zhang, “Active tagging for image
indexing,” in Proceedings of International Workshop on Inter-
net Multimedia Search and Mining, in conjunction with ICME,
2009.
[14] D. Liu, X.-S. Hua, L. Yang, M. Wang, and H.-J. Zhang, “Tag
ranking,” in Proceedings of International World Wide Web
Conference, 2009.
[15] E. Moxley, J. Kleban, J. Xu, and B. S. Manjunath, “Not all
tags are created equal: Learning Flickr tag semantics for global
annotation,” in Proceedings of IEEE International Conference
on Multimedia & Expo, Cancun, Mexico, June 2009.
[16] X. Olivares, M. Ciaramita, and R. Zwol, “Boosting image re-
trieval through aggregating search results based on visual an-
notations,” in Proceedings of ACMMultimedia, 2008, pp. 189–
198.
[17] Y. Jing and S. Baluja, “PageRank for product image search,”
in Proceedings of International World Wide Web Conference,
Beijing, China, 2008.
[18] S. Baluja, R. Seth, D. Sivakumar, and et al., “Video suggestion
and discovery for youtube, taking random walks through the
view graph,” in Proceedings of International World Wide Web
Conference, 2008.
[19] W. Hsu and S.-F. Chang, “Video search reranking through ran-
dom walk over document-level context graph,” in Proceedings
of ACM Multimedia, Augsburg, Germany, 2007, pp. 971–980.
[20] X.-S. Hua, T. Mei, and S. Li, “When multimedia advertising
meets the new internet era,” in Proceedings of IEEE Interna-
tional Workshop on Multimedia Signal Processing, 2008, pp.
1–5.
[21] T. Mei, X.-S. Hua, L. Yang, and S. Li, “VideoSense: Towards
effective online video advertising,” in Proceedings of ACM
Multimedia, 2007, pp. 1075–1084.
[22] T. Mei, X.-S. Hua, and S. Li, “Contextual in-image advertis-
ing,” in Proceedings of ACM Multimedia, 2008, pp. 439–448.
[23] W.-S. Liao, K.-T. Chen, and W. H. Hsu, “AdImage: video ad-
vertising by image matching and ad scheduling optimization,”
in Proceedings of ACM SIGIR conference on Research and De-
velopment in Information Retrieval, 2008, pp. 767–768.
[24] H. Liu, S. Jiang, Q. Huang, and C. Xu, “A generic virtual con-
tent insertion system based on visual attention analysis,” in
Proceeding of the ACM International Conference on Multime-
dia, 2008, pp. 379–388.
[25] S. Ahern, M. Naaman, R. Nair, and J. Yang, “World ex-
plorer: Visualizing aggregate data from. unstructured text in
geo-referenced collections,” in Proceedings of the Seventh
ACM/IEEE-CS Joint Conference on Digital Libraries, 2007.
[26] L. Kennedy and M. Naaman, “Generating diverse and repre-
sentative image search results for landmarks,” in Proceedings
of International World Wide Web Conference, Beijing, China,
2008, pp. 297–306.
[27] N. Snavely, S. M. Seitz, and R. Szeliski, “Photo tourism: Ex-
ploring photo collection in 3d,” ACM Transactions on Graph-
ics, vol. 25, no. 3, pp. 835–846, 2006.
[28] J. Sivic, B. Kaneva, A. Torralba, S. Avidan, and W. Freeman,
“Creating and exploring a large photorealistic virtual space,”
in Proceedings of IEEE Conference on Computer Vision and
Pattern Recognition, Anchorage, AK, June 2008.
[29] L. Kennedy and M. Naaman, “Less talk, more rock: Auto-
mated organization of community-contributed collections of
concert videos,” in Proceedings of International World Wide
Web Conference, 2009.
[30] A. Torralba, “80 million tiny images: A large data set for non-
parametric object and scene recognition,” IEEE Trans. on Pat-
tern Analysis and Machine Intelligence, vol. 30, no. 11, pp.
1958–1970, 2008.
[31] L. Xie and R. Yan, Extracting Semantics fromMultimedia Con-
tent: Challenges and Solutions, Book Chapter of Multimedia
Content Analysis: Theory and Applications, 2008.
[32] A. Ulges, C. Schulze, D. Keysers, and T. Breuel, “Identifying
relevant frames in weakly labeled videos for training concept
detectors,” in Proceedings of ACM International Conference
on Image and Video Retrieval, 2008, pp. 9–16.
[33] R. Fergus, L. Fei-Fei, P. Perona, and A. Zisserman, “Learning
object categories from google’s image search,” in Proceed-
ings of International Conference on Computer Vision, 2005,
pp. 1816–1823.
[34] J. Yang, R. Yan, and A. G. Hauptmann, “Cross-domain video
concept detection using adaptive SVMs,” in Proceedings of
ACM Multimedia, 2007, pp. 188–197.
[35] W. Jiang, E. Zavesky, S.-F. Chang, and A. Loui, “Cross-
domain learning methods for high-level visual concept clas-
sification,” in Proceedings of IEEE International Conference
on Image Processing, 2008.
[36] A. Setz and C. G. M. Snoek, “Can social tagged images aid
concept-based video search?,” in Proceedings of IEEE Inter-
national Conference on Multimedia & Expo, Cancun, Mexico,
June 2009.
[37] J. Yu, D. Joshi and J. Luo, “Connecting People in Photo-
Sharing Sites by Photo Content and User Annotations,” in
Proceedings of IEEE International Conference on Multimedia
& Expo, Cancun, Mexico, June 2009.
1451
Authorized licensed use limited to: National Taiwan University. Downloaded on December 8, 2009 at 10:32 from IEEE Xplore.  Restrictions apply. 
 2 
Travel Report for ICME 2009 
New York, USA, June 2009 
 
網媒所 資工系  徐宏民  助理教授 
winston@csie.ntu.edu.tw 
 
The purposes for the conference visit (June 27 ~ July 4, 2009) in New York have been in 
several folds: (1) serving in the ICME 2009 organizing committee as the publicity chair; 
(2) hosting and presenting in the proposed special session, “Knowledge Discovery Over 
Community-Sharing Media: From Signal To Intelligence”; (3) presenting paper 
“Foreground Segmentation For Static Video Via Multi-Core and Multi-Modal Graph 
Cut”; (4) invited as one of the panelists for the panel “What will multimedia be like on a 
hand-held device?”; (5) serving as the Technical Committee member for the prestigious 
(top) conference in multimedia community, ACM Multimedia 2009.  
 
2009 IEEE International Conference on Multimedia & Expo (ICME 2009) was originally 
scheduled to be held in Cancun Mexico. It was relocated to New York City due to the 
outbreak of H1N1 in Mexico. I was invited as the publicity chair in the organizing 
committee. The relocation does incur extra efforts for organizing the conference (i.e., 
changing web site information, print-out designs). However, all the committee members 
and the research community do pitch in their supports for the welcomed conference. At 
end, the conference goes quite well in New York City and the attendees (560+) are more 
than those registered in Cancun. The budget for the conference, through fined by the 
Cancun conference hotel, did go in a healthy way with certain surplus. Meanwhile, due to 
the welcome of ICME, the conference will continue to be held in other venues.  
 
Our special session, “Knowledge Discovery Over Community-Sharing Media: From 
Signal To Intelligence,” in the conference is quite welcome and later accepted as a 
special issue, “Knowledge Discovery Over Community-Contributed Multimedia Data: 
Opportunities and Challenges." in IEEE Multimedia Magazine. The explosive growth of 
digital photos and videos; the prevalence of capture devices; and the advent of media-
sharing services, such as Flickr and YouTube, have drastically increased the volume of 
community-contributed multimedia resources. Billions of photos, videos, and music 
shared on Web sites profoundly impact human society and pose a new challenge for 
designing efficient indexing, search, mining, and visualization methods for manipulating 
such largescale media. Besides plain visual or audio signals, social media are augmented 
with rich context—such as user-provided tags, comments, geolocations, time, and device 
metadata—benefiting a wide variety of potential applications such as annotation, search, 
recommendation, advertising, and visualization. 
 
The goal of this special issue is to present a concise reference of state-ofthe-art efforts in 
knowledge discovery over large-scale social media, and in particular the entailed 
opportunities and challenges given the nascent status of this arena. Specifically, the 
special issue is intended to present both survey and original research articles (in a tutorial 
manner readable by nonspecialists) on emerging theoretical and practical deployments as 
well as illustrative applications for annotation, indexing and search, mining, 
 4 
Foreground detection is essential for semantic understanding and discovery for 
surveillance videos but still suffers from inefficiency and poor shape or silhouette 
detection. We argue to leverage multiple modalities (e.g., color appearance, 
foreground likelihood, spatial continuity, etc.) for foreground detection and propose a 
rigorous fusion method by graph cut. We further devise three strategies (e.g., dividing 
the graph cut problem into several subtasks, exploiting multi-core platform, etc.) to 
speed up the detection. Experimenting in open benchmarks, the proposed method 
outperforms other rival approaches in terms of detection accuracy and frame rate. 
 
2 
ICME 2009 – Winston Hsu 
In handheld devices, multimedia (software or content)  
might be … 
3 
ICME 2009 – Winston Hsu 
user needs (long tail) 
More personal, diverse, and to be served by a large 
number of providers/publishers 
  Handheld devices to be used in different cultures, countries, ages, 
genders, occasions, etc. 
–  The multimedia needs are very diverse (as the long tail?) 
–  Could not be fulfilled by few vendors  
–  To be served by a large number of providers (publishers) 
  A few phenomena seem to show the trend 
–  Purchasing songs through iTunes 
–  Purchasing numerous handheld applications  
online  
•  One billion downloads within 9 month for iPhone 
•  Google Android Market as an alternative 
4 
1 
