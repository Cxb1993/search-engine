行政院國家科學委員會專題研究計畫成果報告 
動態計算能量平衡 – 複合式平行計算方法與計算流體力學之應
用 
Dynamical Computing Power Balancing –The Application of 
Embedded Parallelism on CFD 
計畫編號：NSC 94－2212－E－492－002 
執行期限：94 年 08 月 01 日至 95 年 07 月 31 日 
主持人：黃維誠    財團法人國家實驗研究院國家高速網路與計算中心 
 
一、中文摘要 
 
本計畫最終目標是將一具創新性，名為
“動態計算能量平衡 dynamical computing 
power balancing”的獨特平行計算方法，將
其應用於大尺度平行式計算，包含流體力
學等，之數值模擬上。此一方法自提出與
實作研發至今已於解析非線性動態系統數
值方法的應用上達到一定的效果與成效。
其成果也陸續開始發表於相關期刊與國際
研討會中[1,2,5]。該類應用結合了非結構
性網格、網格細分法及 cell-to-cell mapping
等方法，並運用“動態計算能量平衡”之平
行計算模式，將計算資源與瞬息多變之計
算需求分佈相結合，以有效運用計算資
源，進而提供了有效而經濟的求解方法。
在“動態計算能量平衡 dynamical computing 
power balancing” 架構下，每一平行計算
程序 (process) 所操控的計算能量 (thread) 
將隨著計算負荷的變動而相互支援，藉以
確保計算效能的穩定。在此計畫中將以此
一方法於求解非線性動態系統應用之成功
經驗為根本，進一步嘗試將其應用於大尺
度流體力學之平行計算上。流體力學之計
算上，流場之狀態常無法預先得知，為求
流場解析之精確度，於物理現象豐富之處
須投入較多的計算格點數。若將整體流場
全面細分將導致計算資源極大的浪費。 
 
因此，於計算過程中視需求動態地將網格
細分的做法實為必要之手段。然而，在平
行計算環境下，即便計算負荷之初始平衡
可以達成，但隨著網格動態地細分，此一
平衡將無法維持。為因應此一計算負荷變
化的問題，通常會採用網格重分配
(mesh-repartitioning) 或 資 料 遷 徙
(data-migration)的方法。然而，因為以上
兩種方法各自的缺點，例如，如何擇定邊
界處須移動之格點以利計算負荷之平衡、
平行區域間資料傳輸形式之複雜化等。加
上系統動態的變化，使二者均須付出極大
的額外代價，故非完全的對策。因此，本
計畫乃探討將動態計算能量平衡法於平行
計算流體力學之應用，以有效運用有限之
計算資源從事大尺度之計算模擬。 
 
在將動態計算能量平衡法於平行計算流體
力學的要求下，平行計算程式必須具備分
散式與共享式兩種平行計算方式。進而可
以採用複合式平行計算架構，才能應用
“動態計算能量平衡”以有效應用計算資
源。本年度計畫之重點就是執行計算流體
力學程式之複合平行化。 
 
 
關鍵詞：大尺度數值模擬，負荷平衡，複
合式平行處理，網格細分，計算流體力學 
 
Abstract 
 
The final goal of  innovative parallel 
computing concept “dynamical computing 
power balancing” is applied to the research 
of large-scale CFD applications.  Since its 
dawn day, the “dynamical computing power 
 1
型大尺度科學計算之應用實力作為研究發
展之對象。 
平行計算硬體平台自 60及 70年代中期
問世以來，即有各種不同設計。不同之處
包括記憶體定址、內部網路、指令控制方
式…等。其中最常見的就是分散式記憶體
架構與共享式記憶體架構。曾幾何時，兩
者 間 的 區 隔 已 因 “Distributed Shared 
Memory (DSM)”架構之崛起而日漸模糊。 
另一方面，在平行程式撰寫方面也有
兩種方式；1) 共享式記憶體平行程式撰
寫方式；2)分散式記憶體平行程式撰寫方
式。兩者各有優缺點。分散式記憶體平行
程式撰寫方式採用標準的 PVM(Parallel 
Virtual Machine)及 MPI (Message Passing 
Interface) 等 “ 訊息傳輸功能庫 (message 
passing libraries)”來負責資料的傳輸。此
種作法較不易撰寫程式，但有較佳的跨平
台性與“資料當地化 (data locality)”的特
性。然而，使用者必須提供明確的計算負
荷分配以充分利用平行計算之功能。因
此，在使用上作法較繁瑣，也須對程式要
有較深入之了解與掌握。尤其是針對須有
高度同步需求的計算與較多訊息交換的應
用，諸如計算流體力學等。相反的，共享
式記憶體平行程式撰寫方式通常透過程式
指令或程式編譯器特殊選項的方式，由程
式編譯器代為完成計算量的均分。使用
上，相較於前者容易許多。使用者亦較易
入門，程式編輯時間與更動均少很多。計
算時計算負荷之分配，亦於計算當下由系
統代為決定。使用者不必勞心。程式的擴
充性與效能端視底層硬體與系統架構而
定。由於用戶無法掌握資料的分布，“資
料當地化(data locality)”常無法控制。因
此，可能導致高網路流量而使平行與計算
效能不彰。以往高速計算領域較不常採用
此種方式，原因在於各家廠商有自家的程
式指令介面。然而，自從業界標準
OpenMP 面市以來，此一問題即刻獲得解
決。使用者持續增加中。 
“ 複 合 式 平 行 計 算 (Embedded 
Parallelism)”共用兩者的優點，毫不偏廢，
並能與最新的硬體架構搭配。使用上，以
“區間分割法(domain decomposition)” 將整
體 計 算 空 間 區 隔 為 數 個 次 空 間
(sub-domains)，每一次空間分配與一計算
程序。而每個程序下則以 OpenMP 針對計
算迴圈及可同時演算部份，進行數個計算
器間的計算負荷的協調分配。Embedded 
parallelism 整合了 distributed 及 shared 
memory parallelism 兩者的優點，提供了另
一更具彈性的計算方式。例如，有時肇因
於物理問題、計算空間、計算法等因素，
計算負荷並無法根據計算區間的大小與快
取記憶體之效能來調配計算分配。“聯合
式平行計算(Embedded Parallelism)”即提供
了另一選擇。 
“Adaptive method”策略上是希望根據
數值解的“錯誤評估(error estimation)”，對
網格加以最佳化以求計算結果的改善。
“Adaptive method”的方式有數種之多。大
體上，依其針對的物件可分為兩種；第一
種是著重於數值計算方法本身，針對不同
的計算區域擁有不同的系統特性的情況，
提供不同的計算方法。例如，在流場中靠
近物體附近流場與遠離物體區域之流場特
性就大不相同，雖然有時可以用相同的方
法計算，但有時並不可能。既使是在可以
使用相同作法的情況下，採用不同 solver
的方式仍可以節省計算資源，並更有效的
算出主要的流場機制。此即是將計算區域 
“Adapted”到不同的計算方法，以不同的計
算方法提供不同的計算精確度的方式。另
一大類是以計算網格的調配來增加計算精
確度。此類為本計畫採用的方式。基本做
法就是將網格的分佈方式與計算結果相結
合 。 有 可 概 分 為 “re-meshing” 、
“re-distribution” 及 “refinement”三種。第
一種“re-meshing”的方式是重新產生整個
網格，其網格生成時之控制參數需根據計
算結果調整，以使重新產生的網格能符合
重新計算之所需。這種作法需將計算停
止，並將整個過程重新再來一次。在平行
計算的架構下，各區域計算量不易估計，
並無法從網格格點數判知，對平行計算的
效 率 達 成 是 一 大 考 驗 。 第 二 種 “re- 
distribution”的方法則是在維持現有的格點
 3
停止，且須以人工方式將計算範圍重新劃
分。就算是部份“資料重分配(repartition)”
方式可以自動化，但其重分配後之結果往
往使分配過程充滿困境。“資料遷移(data 
migration)”方式乍看之下似乎相當有吸引
力。但是，如何決定那一部分網格需被送
往那一個計算單位就必須花費相當大的心
力。且此一方法的結果並不可預期。到目
前為止，尚有眾多議題需要解決。本計畫
採取了與以上兩種方法截然不同的方法。
與其移動資料，吾人建議於 DSM 的架構
下，移動 CPU。將計算器於計算程序 
(processes)間依實際需求移動。如此一
來，就不須付出“資料重分配(repartition)”
與 “資料遷移(data migration)”兩種方式所
不需付出的代價。如此，必能大大增加硬
體架構之使用率與彈性。 
在邁往上述目標的過程中，如何將該
方法應用於計算流體力學的平行計算上的
第一步驟是將所謂的 “複合式平行計算法 
embedded parallelism” 導入到平行計算流
體力學程式中。再現有計算流體力學的平
行化過程中，礙於一般計算硬體設施架
構，一般提供如 MPI 等的 “分散式平行
計算法 distributed memory parallelism”作
法。提供  “共享式平行計算法  shared 
memory parallelism”平行計算的案例較
少。而提供“複合式平行計算法 embedded 
parallelism”本身就具備研發價值。可作為
達到最終目標的中間產物，本身即可提供
單獨的運算服務。 
 
三、文獻探討 
   
自始以來，“adaptive mesh”一直就是
數值分析重要的標準程序。其不但能善用
資源，同時能提供模擬對象之系統特性。
自平行計算平台日益普及以來，計算負荷
之平均分擔已成為在分散式平行架構下達
成高計算效能的第一要務。正如前述，當
面對因“adaptive mesh”而導致的計算負荷
不 平 衡 時 ， 不 論 是 以 “ 資 料 重 分 配
(repartition)”或“資料遷移(data migration)”
兩種方式均以額外的工作與時間及更複雜
的計算方式來換取計算資源的善用。但兩
者也都帶來副作用，如上述。雖則可寄望
以額外的心力與時間達成效果，姑且不論
成效如何，但卻可能使平行計算的美意盡
失[10]。各種不預見的問題，諸如，網格
邊界的不預期的增加、網格中間出現“小
島”、…等情況均可能發生。這些情形將
使計算量更形增加，訊息的溝通量亦不斷
增加，者些都是平行計算的主要欲避免的
重點[15]。本計畫採用另一種思考模式，
與其面對資料重新分配的諸多問題，不如
嘗試以計算能量的角度切入。植基於
“embedded parallelism”的“動態計算能量平
衡 dynamical computing power balancing”可
以大量減輕平行計算使用者所耗費的心力
與時間，而不需額望消耗太多的心力即可
享用平行計算所帶來的好處。 
 
四、研究方法 
本 研 究 提 議 以 平 行 計 算 及 網 格
adaptation方法在使用於非線性動態系統之
求解上的成果延伸到計算流體力學的求解
上。並使用“動態計算能量平衡 dynamical 
computing power balancing”的觀念與做
法，求解此類系統以增進平行效能。最
後，將此一做法延伸到平行計算流體力學
的使用上。在過程中，今年以 “複合式平
行計算 (embedded/hybrid parallelism)於流
體力學之研究為主軸。 
 
動態計算能量平衡(Dynamical Computing 
Power Balancing) 
“Embedded Parallelism”已成功地使用
於非線性動態系統之數值求解上，初步證
明此方法之可行性。由於非線性動態系統
其 transient 的特性，各個格點隨時會在各
個區域間流動，因而，每個區域的計算量
亦隨時改變。因此，除了 embedded 
parallelism 之外，必須採用能平衡此一特
性的方法。“資料重分配(repartition)”或“資
料遷移(data migration)”兩種方式所面臨的
缺點已如前述。尤其當數以百萬計的格點
用於計算時，此兩種方法並不實際。有鑑
於 此 ， 乃 提 出 “ 動 態 計 算 能 量 平 衡
dynamical computing power balancing”的觀
 5
Embedded Parallelism 之 功 能 。 並 以 
“Twin-well Duffing type Oscillator”之求解
為例，展示其功能與可行性。除了完成
MPI 與 OpenMP 版 本 外 ， 並 加 入
“Embedded Parallelism”的部分。其成果透
過 Benchmarking 的測試，展示如下圖所
示，求解之方程式直書如下： 
 
 
經由轉換過程可得以下方程組 
 
本系統主要現象豐富，包含一對
“limited cycles” 分別環繞 (1,0) 及 (0,1) 兩
點、一個“saddle point”位於(0,0)。另有 3
個 “basins”；分別以紅、藍、綠色表示。
紅色範圍代表發散掉的區域，藍色範圍代
表最後收斂於(1,0)位置之區域，綠色範圍
則代表終究收斂於(0,1)位置之區域。(註：
圖中之Y代表了 ) 
 
 
上圖顯示於計算範例中，四個平行區
域間因Adaptive Mesh Refinement的作用，
使運算負荷隨計算之進行而改變。尤其是
computation stage介於 10~20及 40~52間，
各 domain 間的負差異較明顯。”Dynamical 
Computing Power Balancing”即是希望針對
此一分布作出回應，依據各區域間計算負
荷之比例，動態調整計算能量分配，以達
到 work load 與 computing thread 的比例同
步變化的，進而提升平行計算效能的目
的。 
下二圖即為經動態調整計算能量分配
後的效能提昇狀況， 
 
 
 
 
在計算流體力學統之數值模擬程式方面: 
累積數年的經驗提供了數個計算流體
力學研究程式，應用對象包括可壓縮流與
不可壓縮流。程式平行化版本(MPI 版本)
以大體撰寫完畢。如下圖所示，程式計算
簡單的 NACA0012 transonic flow 的計算，
以確定計算無誤，由於非本年度重點，因
此，不再多述。該程式進一步轉移其使用
之計算平台到 pc cluster 上進行進一步測
試。 
 7
OneraM6 wing 的 Navier-Stokes Equations 
solution 展示於下二圖中，包含的 wing 的 
C-O mesh，與入流 0.85 Mach 情況下 shock 
wave 生成與流線之呈現。 
 
 
 
除了流體力學的成果外，原本的 
ASPCG computing kernel 也 用 在 
meta-computing 繼算環境的  performance 
benchmarking。此測試首先在相隔超過
100 公里的兩座主機上進行測試，所得結
果顯示於下表，並可歸結其效能與網路狀
況有密切的關聯。之後，在針對異質平台
上使用 MPICH-G2 進行測試，測試的環
境架構如下，其結果顯示於下圖中。 
P_N P_A MFLOPS Comment 
1  0.36322E+03  
2  0.71425E+03  
1 1 0.13361E+03  
1 1 0.40084E+03 1Gbps WAN
4  0.13616E+04  
2 2 0.86429E+02 a bad day 
2 2 0.69143E+03  
8  0.24259E+04  
4 4 0.33129E+03 4 interfaces 
4 4 0.45503E+03 2 interfaces 
4 4 0.10557E+04 1Gbps WAN
4 4 0.22946E+04 1 interface 
 
 
 
 
 
 
 
六、成果自評 
今年主要完成的工作項目可大略方為
以下四項： 
 
平行處理與 Grid Computing 的結合 
本計畫的平行程式撰寫方式包含了
MPI 與 OpenMP。有數個問題需要解決。
首先，計算方法中資料相互依存的狀況必
須改變。例如，在 serial code 中，“data 
lumping”階段，最新的結果必須被使用以
節省 mapping 的時間。但在平行計算的架
 9
總體而言，計畫目標順利達成，期間有些
許的微調，但仍符合原先設定的目標。 
 
七、參考文獻 
 
1. W. Huang, Eric Huang, Gary Wu, “The 
Implementation of a Workflow-based 
Computational Grid Environment,” 
CCGrid06 : 6th IEEE International 
Symposium on Cluster Computing and 
the Grid, Singapore, 2006. 
2. W. Huang, Eric Huang, Gary Wu, 
“Application of Grid Computing to the 
Parallel CFD Applications,” The 13th 
National Computational Fluid Dynamics 
Conference, Taipei, 2006. 
3. W. Huang, Serena Pan, “Performance 
Evaluation of Meta-Computing under 
Grid Computing Environment,” The 13th 
National Computational Fluid Dynamics 
Conference, Taipei, 2006. 
4. Serena Pan, W. Huang, “The 
Performance Test of MPICH-G2 on 
NCHC Grid Testbed,” PRAGMA 10, 
March, 2006. 
5. C. L. Huang, Gary Wu, W. Huang, 
“Computing Grid Portal,” PRAGMA 10, 
March, 2006. 
6. W. Huang, Eric Huang, Gary Wu, “The 
Grid Computing Environment in NCHC,” 
12th Workshop on compiler techniques 
for high performance computing, Tainan, 
Taiwan, 2006. 
7. Eric Huang, Gary Wu, W. Huang, 
“Computational Grid Portal of NCHC,” 
SC2005, Seattle, 2005. 
8. W. Huang, and D. K. Tafti, “A Parallel 
Computing Framework of Dynamic 
Power Balancing for Adaptive Mesh 
Refinement,” International Journal of 
High Performance Computing, Vol. 18, 
No. 2, May, 2004. 
9. W. Huang, “Dynamical Computing 
Power Balancing for Adaptive Mesh 
Refinement Applications,” Proceedings 
of International Parallel Computational 
Fluid Dynamics, Nara, Japan, 2002. 
10. W. Huang, and Y.G. Lai, “A Parallel 
Implementation of a Multi-Block 
Three-Dimensional Incompressible Flow 
Solver on a DSM Machine,” Fourth 
International Conference on 
Hydroinfomatics, Iowa City, Iowa, July 
23-27, 2000. 
11. D. K. Tafti, X. Zhang, W. Huang, G. 
Wang, “Large-Eddy Simulations of Flow 
and Heat Transfer in Complex 
Three-Dimensional Multi-louvered Fins,” 
Proceedings of FEDSM2000, ASME 
Fluid Engineering Division Summer 
Meeting, June, 2000. 
12. W. Huang, D. Tafti, “A Parallel 
Computing Framework of Dynamic 
Power Balancing for Adaptive Mesh 
Refinement Applications,” Proceedings 
of the Parallel CFD Conference’99, pp. 
249-256, 1999. 
13. D. K. Tafti, and G. Wang, "Application 
of Embedded Parallelism to Large Scale 
Computations of Complex Industrial 
Flows," Proceedings of the ASME Fluids 
Engineering Division, FED Vol. 247, pp. 
123 - 130, 1998 ASME-IMECE, 
Anaheim, CA., Nov. 1998. 
14. C.S. Hsu, Cell-to-Cell Mapping: A 
Method of Global Analysis for Nonlinear 
System, Springer-Verlag, 1987. 
15. M. H. Chou, “A Modified Cell-to-Cell 
Mapping Method for Nonlinear 
Systems,” Computers Math. Application, 
25(8),47-57, 1993. 
16. R. D. Williams, "Performance of 
dynamic load balancing algorithms for 
unstructured mesh calculations," 
Concurrency Practice and Experience, 
3(5), 457-481, 1991. 
17. C. Walshaw, and M. Berzins, "Dynamic 
load balancing for PDE solves and 
adaptive unstructured meshes," 
University of Leeds, School of Computer 
Studies, Report 92-32, 1992. 
18. M. C. Rivara, "Algorithms for Refining 
Triangular Grids Suitable for Adaptive 
and Multigrid Techniques," Int. J. for 
Numerical Methods in Engineering, 20, 
745-756, 1984. 
19. M. C. Rivara, "Mesh Refinement 
Processes Based on the Generalized 
Bisection of Simplices," SIAM J. 
 11
 
 
附件： papers published 
> Paper ID: 223 < 
 
2
 
Fig. 1. Integrated Grid Architecture 
 
As application scientists and engineers focus on their 
research activities, asking extra attention to the already 
complicated computing environment is highly unlikely to 
happen.  Therefore, strategically, a help in enabling the 
deployment of grid computing to the scientific researches and 
engineering activities is highly demanded.  Grid portals are 
there to fill up this gap between the grid computing technology 
and the existing application problem solving environment.  The 
Grid Computing Portal aims to promote the development and 
advancement of technologies that provide seamless and 
scalable access to wide-area distributed resources.  It has been 
recognized as an excellent channel for grid users to access the 
specialized information they need. [2] 
Portals are there to lower the barrier for scientists and engineers 
to access the distributed grid computing environment through 
the conventional web browser and thus the grid computing 
system can be accessed from anywhere as easily as internet 
sites nowadays.  The thought behind a portal is to build easy 
and secure grid interfaces for non-experts to use the grid 
computing. 
The courses of construction of grid portals can be 
distinguished into the following two strategic plans; the 
disciplinary specific portal and the generic computational 
portal.  There are various portals constructed for various grid 
projects or specific application program.  On the other hand, 
there are also several projects focused on constructing a generic 
portal environment based on which portals for specific 
applications can be customized from.  Different from the 
application specific portal which is designed to provide specific 
functionalities for particular application, a “generic” type of 
portal needs to support a wide range of grid computing 
behavior.  Readers are referred to the “Open Grid Portals” 
project as the starting point for further detailed research for 
various portal projects.[3] 
Among all the portal development activity, NCHC follows 
the step of OGCE from the beginning of its development and 
later on switch to LifeRay for the portal container.  Therefore, it 
will not be complete to discuss the activity of NCHC 
computational portal without reviewing the effort of OGCE.  
The “Open Grid Computing Environments” (OGCE)[4, 5] is 
supported by NMI (NSF Middleware Initiative)[6] of the US to 
support the standardization of grid computing middleware.  It 
was set out to develop standard compliant portlet components 
that can be reused various containers, given the portlet 
specification standard is followed, and to develop and integrate 
grid portal tools and services to support scientific gateways. [7]  
The JSR 168 Portlet Specification defines interoperability 
standards for portal containers.  It is understood by the 
community that a JSR 168 compliant portlet should be able to 
be deployed in multiple vendor containers.  The OGCE 
software distribution is currently in its second generation 
named as OGCE2. 
OGCE 2 Release Features a set of core grid porlets, grid 
programming interfaces, velocity portlets, and tools for 
building portal based on JSR168 compliant portal containers.  
The “Core Grid Portlets” provides a set of Grid portlets from 
proxy management, remote command execution, remote file 
management, to GPIR based information services.  The “Grid 
Programming Interfaces” is a high level grid programming 
interfaces using the Java CoG kit that bridges the developed 
portal and the underline grid toolkits, thus to save effort of 
compatibility issue between portal and grid toolkit.  OGCE uses 
velocity portlet tools to port Jetspeed-based portlets into 
JSR168 compliant portal.  It also integrates build and deploy 
tools compatible with open source containers such as uPortal 
and GridSphere, thus the choice of container can be more 
flexible.  Started from the OGCE, the NCHC portal has 
implemented the features of a generic Computational portal 
will need.  Gradually, the portal architecture has migrated to 
Liferay[8] for reasons documented in [9] over the year. 
In next section, the grid portal architecture along with the 
portal container Liferay will be described, followed by the 
basic requirements to be considered as well as the features in 
our Computational portal for compute intensive applications 
which is presented in section 3.  In section 4, an actual example 
of computing job submitted via Computational portal is given 
while the blueprint of future directions of the portal is drawn in 
the last section, the section 5.  
 
II. PORTAL ARCHITECTURE AND CONTAINER  
As illustrated in Fig. 2 and 3, the system wise architecture of 
NCHC portal is presented.  The portal architecture can be 
divided into several layers.  The basic computing facilities 
build the fabric layer of the portal based on which the Globus 
linked with Cog/Java API and other information services to 
form the middleware layer.  All the jobs submitted to the 
backend resources will be handled by Globus, with current 
version used as 3.2.  On top of the middleware layer, there 
comes the business layer.  This layer is composed of JBoss as 
the J2EE server to support the webserver Tomcat.  After 
receiving the requests from Tomcat Servler Container, JBoss 
Application Server start to process business logic based on the 
information provided by users. The JBoss will interact with the 
> Paper ID: 223 < 
 
4
 
Fig. 4. User-configurable Window in a Portal Pane 
 
III. FEATURES OF NCHC COMPUTATIONAL PORTAL  
 
The goals of our implementation of computing intensive 
portal is first aiming at providing HPC community with a 
grid-enabled portal which can satisfy their working scenario of 
their HPC applications.  The portal should provide its users 
with capability of program compilation, job submission, 
resource discovery, workflow construction and execution, job 
monitoring, data management including file transfer, data 
archiving, … etc.  Over all, the portal is to provide users with 
functions that one can run applications easily over grid 
computing environment.  In order to achieve this goal, some 
basic requirements for the portal are list in the following: 
 
A. User management/authentication 
The portal is to be accessed by using a regular web browser.  
The user logs into the portal by providing a user-id and 
password.  While better technologies exist for authentication, 
this one, best practice HTTPS security, is what users want and 
trust. Once the user is authenticated to the portal server, it is the 
job of the portal server to act as the user’s proxy in most grid 
interactions.[10]   
The user account, at current stage, can be obtained by 
requesting from the portal administrator with proper credential.  
The password of the user-id is, however, not changeable by the 
user currently.  
 
B. Accessibility improvement, the MyProxy portlet 
Once the user log in to the portal, the very next step should 
be provide grid computing credential.  This is the second level 
of security which involves providing the portal server access to 
remote Grid services.  This can be done by uploading the 
credential or, as provided by the system, requesting the 
credential from a credential repository currently hosted by the 
portal.  Due to the fact that most the major computing resources 
under the Grid Computing Environment (GCE) are running 
queuing system, therefore, there is no guarantee that the job 
submitted via portal will be completed by the time the user 
signs off the portal.  Hence, it is important that the user’s job 
can continue after he/she signs off the system and when user is 
back, he/she can access the job executed without difficulty.   
Currently, the portal adopt the MyProxy Credential 
Management Service provided by NCSA.[11]  The Proxy 
Manager Portlet allows users to obtain a proxy certificate that it 
can be considered as the user himself or herself. The standard 
approach is to have the user submit a proxy to a “MyProxy” 
server. The Proxy Manager Portlet then retrieves the proxy 
from the MyProxy server and holds it for the duration of the 
user’s session.  The lifetime of the proxy can be determined by 
the users.  The longer the proxy the more convenient the portal 
environment can be.  However, this will increase the risk of 
been intercepted.  It is always the wisdom of the users to 
determine the lifetime of the proxy.   This MyProxy server 
avoid the demand of uploading the keys/credential everytime a 
Globus command is issued. 
 
 
Fig. 5. Illustration of Gridftp portlet. 
 
C. Data transfer, Gridftp portlet 
 Tools to access to file metadata directories and remote file 
archives are central requirement for the Grid Computing portal. 
> Paper ID: 223 < 
 
6
Usually, this action will be taken if the user somehow 
determine that the workflow has stop somewhere for certain 
reason and he or she is not willing to wait on the project to 
complete anymore.  Then, the user can select to discharge the 
workflow to save the resources used. 
 
 
Fig. 7.  Screenshot of Job Status Report Portlet. 
 
IV. APPLICATION EXAMPLE 
As first officially exhibited in SC05 held in Seattle, the 
NCHC portal demoed itself by using the workflow to submit a 
typical CFD job.  The workflow process, as illustrated in Fig. 7, 
shows the scenario of the computation.  For simplicity, the 
executable program was pre-compiled and resided on a typical 
computing cluster call ASE.  Actually, if necessary, the portal 
can also be used to submit compile job via make file.  The input 
file of the computation resided on another server denoted as 
“data node” in the Fig. 8.  The workflow started with requesting 
a gridftp job sending the input file from the data node to the 
computing cluster, dispatch a parallel job on the cluster.  The 
cluster queued the job and executed the computation.  The 
computed result was then sent to another server, denoted as 
“local PC” with post-processing capability.  A commercial 
visualization program with its license server located elsewhere 
was then activated to display the computing result on a tiled 
display wall composed of LCD monitors.  The post processing 
server and display were actually sitting at the SC booth of 
NCHC out in Seattle, while other facilities involved were 
located back in NCHC facility in Taiwan. 
 
Fig. 8. Working Scenario of CFD Demonstration of 
Workflow.  
 
Other than this typical CFD application, the portal is also 
used to handle the complicated workflow for the computation 
of flooding forecasting models designed for the flood 
mitigation grid activities in Taiwan.  At this point, we do not 
have the luxury to address the activities yet. 
V. CONCLUSION AND FUTURE WORKS 
The key to the Grid Computing portals is being able to hide 
Grid details behind useful application interfaces. The user 
needs to be able to launch, configure and control remote 
applications in the same way he or she used to do on 
supercomputers.  In this paper, we have briefly introduced the 
grid computing effort in Taiwan focusing on how to lower the 
barrier of the deployment of grid computing environment.  A 
Computational portal project was launched one and half year 
ago with the goal to provide a generic Computational portal 
which can be later customized for specific applications.  The 
portal takes the advantage of Liferay as portal container and 
implemented several JST168 compliant portlets, including 
MDS Search Property Portlet, Proxy Manager Portlet, GridFTP 
Portlet, Workflow Portlet, Job Status Portlet, LDAP Browser 
Portlet, and Job Execution portlet. 
The development of this Computational portal is far from 
completed.  Issues like how to further enhance its capability, 
how to improve the user interface are constantly under 
discussion.  For the further development, the portal will migrate 
all available portlets from both Gridsphere and OGCE2 to 
enhance its functionality and yet without “reinventing the 
wheel”.  Also, the integration of SRB will be taken to 
strengthen the data management ability of the portal.  The 
workflow scripts will be stored into MySQL database for 
further improvement regarding user access management.  A 
graphical workflow interface is currently undertaken to 
improve the usability and friendliness of the workflow portlet.  
The “Parametric Study” portlet will be completed in the near 
future to deal with parametric sweeping type of high 
throughput computing. The mechanism to notify users of the 
status of their job will also be investigated.  A more timely 
第十三屆全國計算流體力學學術研討會                 台北縣，中華民國九十五年八月 
The 13th National Computational Fluid Dynamics Conference         Taipei county, August,2006 
                                                                               CFD13-0401 
網格計算於平行計算流體力學上之應用 
Application of Grid Computing to the Parallel CFD Applications 
黃維誠1 黃建霖1 吳建衡1 
 
1國家高速網路與計算中心 
 
Abstract 
Grid Computing has been a major adventure of the development of High Performance Computing (HPC) 
over the past few years. The target of such an adventure is to provide the IT environment for applications to 
draw computing power from a virtual resource pool to achieve high performance of supercomputing. At its 
dawning era, one of its main driven forces is without doubt the demand from computing intensive 
applications. Therefore, in this paper, the authors would like to present the development of a Computational 
Grid (CG) Environment and its application to the computing intensive application such parallel CFD. In 
order to utilize such a computing environment, the workflow which includes initial data movement, job 
submission and execution, post processing and visualization, is adopted to carry out the computing process. 
With the help from the Grid Computing technologies, it is possible for the CFD users to avoid the 
complexity of computing systems, especially when different computing resources are included. These 
features of the Computational Grid, along with the implementation are presented via the example of parallel 
CFD applications. 
 
Keyword : Grid Computing, Parallel Computational Fluid Dynamics, Workflow 
 
I. Introduction 
Over the years, numerical simulation has become 
one of the major methods for scientific exploration. 
As a result, the demand on the computing power as 
well as other resources increases dramatically. To 
meet such demands, supercomputing centers around 
the world can either further expand its capacity as 
currently underway, or think about how to utilize 
existing resources more effectively. To be 
innovative, researchers can take one step further, to 
coordinate geographically distributed computing 
resources in a collaborative manner thus to jointly 
advance the scientific exploration. The Grid 
Computing answered such a demand by providing 
collaborative concept such as Virtual Organization 
(VO) [1] supported by various middleware and 
sample applications to help realizing such 
collaborations. 
 
During the evolvement of the Grid Computing [1, 2, 
3, 4, 5, 6, 7], the HPC (High Performance 
Computing) doubtless played an important role as 
one of the major driven forces. Especially, at the 
dawning era of the Grid Computing, computing 
intensive applications were thirsty for the 
computing power which was beyond the capacity of 
any existing supercomputing centers. Besides, 
many of such heroic computations ask for only one 
第十三屆全國計算流體力學學術研討會                 台北縣，中華民國九十五年八月 
The 13th National Computational Fluid Dynamics Conference         Taipei county, August,2006 
                                                                               CFD13-0401 
be grid-enabled. The easiest way to be grid-enabled 
is to maintain the application the way it is but to 
reduce the dependency of the interaction between 
human and machine. However, with GCE in place, 
not only the traditional ways of research can be 
maintained, but also the innovative methods of 
researches become possible. 
 
The GCE uses the de facto middleware, the Globus, 
to provide the core Grid Computing services. The 
Globus is an open source package [1, 9] that can be 
used to build grid systems and applications. It 
provides the basic service for applications to access 
the underneath Grid infrastructure. One of the 
intentions of the Globus system is to integrate 
vertically the application, middleware and network. 
It is a low level toolkit that provides basic 
mechanisms such as communication, authentication, 
network information and data access. These 
mechanisms are then used to build various higher 
level services. [5] 
In terms of functionality, the features implemented 
in the GCE mentioned in this paper can be 
categorized into the following categories: 
1. Security : The security mainly involves the 
authorization and authentication of the user 
access and the user credential management 
which makes use of the public key 
cryptography (asymmetric cryptography) as the 
basis for its functionality. 
2. Data Management : The data management 
deals with the distributed data location and data 
transfer issues including data movement and 
data replication. The GridFTP[10]and Reliable 
File Transfer (RFT) service[11] are provided for 
data movement under such an environment. 
3. Job Management : The major features of the 
job management are the Grid Resource 
Allocation and Management (GRAM) which 
provides the basic mechanism for job initiation, 
monitoring, management, scheduling and 
coordinating of remote computations. When a 
GRAM job is submitted, the job will be 
submitted to the local queuing system of the 
machine to execute the computation. Thus the 
local policy will be honored even under the 
Grid Computing environment. 
4. Information Services : The resource monitoring 
and discovery mechanism are provided via the 
Monitoring and Discovery System (MDS). The 
information of grid resources, such as 
properties of machines (including number of 
processors, CPU load, network interfaces, file 
system, memory, storage devices, … etc.) and 
their status, is collected and published in a 
single location, so it is used for suggesting 
resource candidates for the computation. 
 
The Grid Computing Environment offers 
computational scientists with the access to a variety 
of distributed resources (compute, storage, etc.). As 
a consequence, it creates a much more complicated 
computational environment. Even the grid 
computing environment of different sites might be 
different. The situation further increases the 
complexity to the already complicated environment 
for the users. In order to allow the application 
scientists and engineers to focus on their research 
activities, the standardization of the fundamental 
middleware is necessary. For the Computational 
Grid (CG) environment developed, the Globus 
along with other middleware, such as MPICH-G2, 
MyProxy, …etc., are used to construct the 
foundation of Computational Grid environment.  
第十三屆全國計算流體力學學術研討會                 台北縣，中華民國九十五年八月 
The 13th National Computational Fluid Dynamics Conference         Taipei county, August,2006 
                                                                               CFD13-0401 
Job Status Report 
Portlet 
Job status monitoring, 
report and workflow  
deletion, … etc. 
NWS Network performance and status check 
Table 1, Major portlets of Computational Grid Portal 
 
The services provided by these portlets are 
categorized in the following 
1. Information Service, MDS and LDAP Portlet 
Every resource added into the Grid system is 
required to have an entry in the appointed LDAP 
server. The users are eligible to browse machines 
summaries, system status and the Grid capabilities. 
Such information is also used to filter out the 
appropriate resources for users to select from to 
execute their applications. 
2. User Management, Proxy Manager Portlet 
Since the portal is accessed by using a regular web 
browser, the user login with a valid password is 
required. Once the user is authenticated to the portal 
server, it is the job of the portal server to act as the 
user’s proxy in most grid interactions.[28] We adopt 
the MyProxy [29] from NCSA to handle the user 
credential. A proxy with limited life span will be 
created on behalf of the user. The portal, then, will 
create a session for the user to maintain the state of 
the user until either the user logs off or the specified 
life span of the proxy expires. However, a 
credential will remain valid after the sign-off of the 
user to ensure the submitted jobs can be executed 
successfully. During the job execution, every 
request from the user’s job will be carried out when 
an appropriate proxy is attached and thus no further 
login/password will be needed again. 
3. File Movement, GridFTP Portlet 
File Movement allows users to execute three main 
tasks: upload a file from a local system to any 
remote system in the Grid environment, download a 
file from a remote system to a user’s local system, 
and transfer files between remote hosts without files 
being pass through the portal server. The GridFTP 
is the key to these features and has to be available 
on all the systems involved with the file movement. 
4. Job Execution, Interactive Job and Workflow 
Portlet 
The Job Management in the Computational Grid is 
based on the GRAM service of Globus. Basically, 
there are two forms of job execution: interactive job 
and batch job execution. The execution of 
interactive jobs is relatively easy and 
straightforward. After user submits a job via the 
Interactive Job Portlet , the user will be able to view 
the output information and results immediately after 
completion of the job execution. The most 
ambitious Grid applications are those whose 
executions are defined by complex workflows. At 
this moment, the Workflow/Sequencer Portlet, 
shown in Figure 3, supports the execution of a 
series of jobs. The workflow consists of various file 
transfers and job submissions. Once a workflow is 
defined, it is fed into a flow control server which is 
a script server. The script server will then 
orchestrate the job executions step by step. 
 
Figure 3, Snapshot of Workflow Portlet 
第十三屆全國計算流體力學學術研討會                 台北縣，中華民國九十五年八月 
The 13th National Computational Fluid Dynamics Conference         Taipei county, August,2006 
                                                                               CFD13-0401 
 
Figure 4, C-O Mesh of Onera-M6 Wing. 
 
A computing workflow is constructed for the 
simulation. The workflow, which consists of seven 
pre-defined sequential steps, is submitted to the 
Computational Grid via the CG portal. The flow 
chart of the computation submitted to the 
Computational Grid is illustrated in Figure 5. The 
Computation starts with an input file for mesh 
generation. The mesh input file is sent to the mesh 
generation server which generates the mesh data. 
Then, the mesh is sent to the flow solver which 
resides on another cluster. In the meantime, the 
computing input files are transferred to the cluster 
as well. The flow simulation is then performed on 
the cluster with those inputs to obtain the flow 
solution. The mesh and flow data are sent to the 
post-processing host which runs the well-know 
Tecplot. The picture is rendered in this 
post-processing unit. Finally, the rendered image is 
displayed on the local display. The reason to use 
Tecplot as the solution rendering package is to show 
that the applications used in the GCE can be 
open-source software, self-developed program or 
commercial packages which provide no source code. 
Thus, we have a base to state that the Grid 
Computing is independent of the applications run 
on it. The Figure 6 shows the computed result with 
an iso-surface of Mach number equals to 1 and a set 
of streamline over the wing tip. Since the focus of 
this paper is the Computational Grid, the detailed 
flow analysis is not presented here for simplicity. 
 
Figure 5, Flow Chart of the Simulation 
 
 
Figure 6, Computed Result of OneraM6 Wing. 
(Iso-Surface Mach # = 1, Streamline at Wing Tip) 
 
The working scenario is illustrated in Figure 7. First, 
the user logs on to the portal after which the user 
credential and proxy are retrieved by step2 and 
step3. The user then retrieves the pre-defined 
workflow (step4). The script server which is also 
known as flow control server executed the 
workflow in the following sequence. First, the 
workflow sends a request to move the input file and 
the executable from the “data node” to the 
computing cluster (denoted as step5). Upon 
receiving this request, the data node sends the 
demanded files to the computing cluster directly 
(step6) without going through the portal server. 
After the completion of the file movement, the 
script server will dispatch a request (step7) to the 
flow solver 
mesh mesh 
post-processing display 
solution 
input 
input  
end 
Mesh  
generator 
第十三屆全國計算流體力學學術研討會                 台北縣，中華民國九十五年八月 
The 13th National Computational Fluid Dynamics Conference         Taipei county, August,2006 
                                                                               CFD13-0401 
Computational Grid Environements,” Parallel 
Computing, 2002, vol. 28, no. 5, pp. 749 – 771. 
[11]. B. Allcock, J. Bester, J. Bresnahan, L. Liming, S. 
Meder, and S. Tuecke, “GridFTP Protocol 
Specification,” GGF GridFTP Working Group 
Document, 2002. 
[12]. NPACI HotPage Grid Computing Portal, https:// 
hotpage.npaci.edu/. 
[13]. M. Thomas, S. Mock, J. Boisseau, M. Dahan, K. 
Mueller, D. Sutton, “The GridPort Toolkit 
Architecture for Building Grid Portals”, 
Proceedings of the 10th IEEE International 
Symposium on High Performance Distributed 
Computing, Aug 2001. 
[14]. A. Natrajan, A. Nguyen-Tuong, M. Humphrey, M. 
Herrick, B. Clarke, and A. Grimshaw,“The Legion 
Grid Portal”, Journal of Concurrency and 
Computation: Practice and Experience (CCPE), 
Volume 14, Issue 13-15, Wiley Press, Nov.-Dec., 
2002. 
[15]. Sun, “Sun Grid Engine Portal”, 
http://www.sun.com 
/solutions/hpc/pdfs/TCP-final.pdf 
[16]. uPortal of JA-SIG, http://www.uportal.org. 
[17]. IBM WebSphere software, http://www-306.ibm. 
com/software/websphere/. 
[18]. GridSphere portal framework, http://www. 
gridsphere.org/gridsphere/gridsphere. 
[19]. Open Grid Computing Environment, http://www. 
ogce.org / index.php, accessed 2005. 
[20]. Open Grid Computing Environment Collaboratory, 
“http://www.collab-ogce.org/nmi/index.jsp”, 
accessed 2005. 
[21]. Open Grid Portals project, http://www. 
opengridportals.org, 2005. 
[22]. Liferay Portal, http://www.liferay.com/. 
[23]. NSF Middleware Initiative, “http://www.nfs- 
middleware.org/default.aspx”, accessed 2005. 
[24]. NSF Middleware Initiative, “http://www.nfs- 
middleware.org/Lists/NMIR8/ogce.aspx, accessed 
2005. 
[25]. Liferay portal enterprise, http://www.liferay.com, 
accessed 2005. 
[26]. W. Huang, J.L. Huang, J.H. Wu, “Portal Servers 
Comparison and Review”, KING annual report 
2002, NCHC, Taiwan, 2002. 
[27]. Liferay Features, 
http://www.liferay.com/web/guest/ 
products/features. 
[28]. D. Gannon, G. Fox, M. and Co-Authors “Grid 
Portals: A Scientist’s Access Point for Grid 
Services (DRAFT 
1)”,http://forge.gridforum.org/projects/ggf- 
editor/document/GCE-Portal-working-draft/en/1/G
CE-Portal-working-draft.pdf. 
[29]. MyProxy Credential Management Service, http:// 
grid.ncsa.uiuc.edu/myproxy, 2005. 
[30]. Storage Resource Broker, http://www.sdsc.edu/srb.  
 
第十三屆全國計算流體力學學術研討會                 台北縣，中華民國九十五年八月 
The 13th National Computational Fluid Dynamics Conference         Taipei county, August,2006 
                                                                               CFD13-0403 
level of Gigabit level for most inter-continental 
connections, not to mention the up-rising of the 
light-path. The advance in network has largely 
reduced the difficulty imposed by the quality of the 
WAN and makes heterogeneous distributed 
computing platforms for advanced scientific 
application possible. On the other hand, the Grid 
Computing has grown mature than ever. The Grid 
computing technology enables the collaborative 
usage of computational resources across different 
organizations. Since the idea of the Grid Computing 
inherited from that of the distributed computing / 
meta-computing, the conventional High 
Performance Computing (HPC) is undoubtedly one 
of the key driven forces of the Grid technologies [1, 
2, 3, 4, 5, 6, 7]. Various software/middleware have 
addressed the issue of how to coordinate 
geographically distributed computational resources 
to provide excessively large computing power 
based on demands to individual scientists and 
large-scale supercomputing collaborations. The 
progresses in the Grid Computing have again 
brought alive the idea of using the meta-computing 
for solving a large single parallel job. An important 
part of the Grid research effort has actually been 
focused on this part of technology. The middleware 
have targeted several issues like heterogeneity, 
network performance, security, resource 
availability, … etc. and have gained certain degree 
of success. Thus, the meta-computing has become 
an increasingly popular and useful method of 
obtaining resources to solve large computational 
problems. 
 
The outline of this paper is as follows. Section2 
reviews the meta-computing methods along with 
the setup of our experiments in Section 3. Section 4 
describes the benchmark code used for this study. 
The test and performance results are detailed in 
section 5, followed by the conclusion in section 6. 
 
II. Meta-computing Environment 
A wider defined meta-computing system can 
consist of several Massively Parallel Processors 
(MPP) servers, advanced, networked instruments, 
database servers and gigabit-level networks to 
support high performance application. It first began 
in the 90’s to tackle large scientific problems, 
several top research centers conducted a 
cooperative use of geographically distributed 
computing resources which are conceived virtually 
as a single computer. In 1992, the term 
“meta-computing” was defined by Larry Smarr and 
Charles Catlett to describe this kind of innovative 
computations[8]. The “meta-computing” uses a few 
stable high performance computers with a secured 
environment through dedicated or non-dedicated 
networks. This definition is more specific compared 
to the current definition of the Grid Computing and 
have made the meta-computing as an important 
portion of the Grid Computing. The focus of this 
work is limited to the computational job spawning 
across different platforms to perform a single 
parallel computation. 
There are several so-called meta-computing 
projects (e.g., Legion [9], Condor [10], Dome [11]) 
tried to create infrastructures to support 
meta-computing. These projects investigate how to 
solve the problems that result from integrating 
distributed resources, such as heterogeneity, 
fault-tolerance, security, accounting, and load 
sharing. Recent years, the raise of Globus[12] 
middleware which initially targeted to build 
meta-computer has boost the meta-computing 
第十三屆全國計算流體力學學術研討會                 台北縣，中華民國九十五年八月 
The 13th National Computational Fluid Dynamics Conference         Taipei county, August,2006 
                                                                               CFD13-0403 
be integrated. It is an implementation of the MPI 
standard but specially designed for clustered 
distributed systems. There are three major concepts 
that build the PACX-MPI:  
(1). For a cluster, the library has to deal with two 
different levels of communication. Therefore, 
PACX-MPI uses two completely independent 
layers to handle communication operations of 
the two sub-systems. 
(2). For communication on the same machine, the 
library makes use of the locally tuned MPI thus 
to take full advantage of the underlying 
communication subsystem.  
(3). For communication between different machines, 
PACX-MPI introduces two communication 
daemons. These daemons allow buffering and 
packing of multiple wide area communications. 
Other than the characteristics stated above, there is 
another major difference between PACX-MPI and 
the MPI_Connect. Unlike the MPI_Connect that 
each machine has its own MPI_COMM_WORLD, 
the same one is used across all the machines 
involved in the MPI job handled by the PACX-MPI. 
This imposes a stronger centralized control over the 
distributed parallel job. 
 
4. MPICH-G2 
MPICH-G2[21] is a grid-enabled implementation 
based on the MPICH which is most widely used 
open source implementation of the MPI standard. It 
is capable of converting data between different 
architectures and supports multi-protocol 
communication. Joining with the Globus, 
MPICH-G2 possesses couple major Globus 
components that are related to the meta-computing, 
such as the Nexus communication library which is a 
multi-method communication toolkit and the 
Meta-computing Directory Service (MDS) that 
provides uniform access to up-to-date information 
about Globus resource structure and state. 
 
Unlike those of MPI_Connect and PACX-MPI, the 
communication of MPICH-G2 between compute 
nodes from different machines is passed directly 
from the source node to the destination, instead of 
going through a centralized daemon on each of the 
machines involved in the computation. For 
MPI_Connect and PACX-MPI, each machine uses 
the head-node to handle the messages in and out of 
the machine. This model further imposes another 
bottleneck on the already shaky communication 
quality. The MPICH-G2 takes another approach by 
sending and receiving messages directly from and 
to the compute node on each machine. This reduces 
the risk of running into communication bottleneck 
with the price of assigning public IPs to every 
compute nodes involved. 
 
III. Experiment Setup 
Two sets of meta-computing environment were set 
up for the performance benchmarking. The initial 
attempt was made on the IBM-SP2 computers that 
resided on different computing sites; the NCHC and 
the ASCC. The two sites are more than 100 
kilometers away geographically. Each of the two 
sites provided 2 nodes of IBM-SP2 with total 16 
processors. The computations were managed by 
Loadleveler which treated two IBM-SP2s as one 
single machine. The reason to choose IBM-SP2 was 
simply because at that time these two machines 
were de-commissioned and were available freely 
for the experiment. The purpose of tests performed 
on this environment was mainly to demonstrate the 
visibility of meta-computing. The performance 
第十三屆全國計算流體力學學術研討會                 台北縣，中華民國九十五年八月 
The 13th National Computational Fluid Dynamics Conference         Taipei county, August,2006 
                                                                               CFD13-0403 
Conjugated gradient (CG) methods are used widely 
for solving large sparse linear systems Au = f. 
Usually, an equivalent pre-conditional system M-1 
Au = M-1 f which exhibits better spectral properties 
is solved. M is the pre-ditioning matrix or the 
pre-conditioner, such as a polynomial 
pre-conditioner [16] or incomplete LU factorization 
[17]. In recent years, domain decomposition type 
pre-conditioners [18] have received much attention. 
The original domain or system is portioned into 
smaller sub-domains which are solved 
independently in an iterative manner. To enhance 
the effectiveness of the pre-conditioner for a large 
number of sub-domains, multilevel domain 
decomposition is used. It has been shown that the 
multilevel Additive Schwarz pre-conditioner not 
only enhances convergence but also enhances 
performance on cache based microprocessor 
architectures [19]. 
 
V. Performance Results and Discussion 
In the first experiment, the problem matrix size as 
1024 X 1024, running on IBM-SP2, with the 
bandwidth of WAN as 300Mbps. The benchmarking 
results of the first experiment are listed in the Table 
1, in which P_N stands for the number of 
processors provided by the NCHC machine and the 
P_A stands for that number provided by the ASCC 
machine. The third column is the performance 
results of the computation. 
P_N P_A MFLOPS Comment 
1  0.36322E+03  
2  0.71425E+03  
1 1 0.13361E+03  
1 1 0.40084E+03 1Gbps WAN 
4  0.13616E+04  
2 2 0.86429E+02 a bad day 
2 2 0.69143E+03  
8  0.24259E+04  
4 4 0.33129E+03 4 interfaces 
4 4 0.45503E+03 2 interfaces 
4 4 0.10557E+04 1Gbps WAN 
4 4 0.22946E+04 1 interface 
Table 1, Performance comparison on IBM-SP2 
 
Experiences learned from the benchmark results are 
recorded as follows,  
(1). The performance of meta-computing suffered a 
lot due to the latency and bandwidth limitation 
imposed by the WAN. The performance was 
hurt badly, almost 6 times slower than the 
computations carried out within a single 
machine. 
(2). A boost on the bandwidth of WAN can improve 
the performance proportionally. For example, 
when the bandwidth of WAN reaches 1Gbps 
which is more than 3 times faster than the 
standard setup of 300Mbps, the performance 
increased more than 3 times as well. 
(3). An unexpected hit on performance is possible 
simply because of heavy loading on the WAN, 
the “a bad day” case. 
(4). With improved domain decomposition layout, 
the performance can be improved dramatically. 
In the case of “2 MPI domain interfaces 
between the two sites”, the performance was 
almost 1.5 times better than that of “4 MPI 
domain interfaces case” between the two sites. 
(5). With some luck on the WAN and proper 
arrange of MPI domain layout, the performance 
of meta-computing can actually approaching 
that of a parallel job within one machine. 
Generally speaking, meta-computing over WAN 
cost more than conventional MPI jobs within a host. 
第十三屆全國計算流體力學學術研討會                 台北縣，中華民國九十五年八月 
The 13th National Computational Fluid Dynamics Conference         Taipei county, August,2006 
                                                                               CFD13-0403 
 
Figure 2, Performance comparison of MPICH-G2 
 
VI. Conclusion 
In this work, the meta-computing has been 
benchmarked with limited resources available. Both 
experiments have their unique target to tackle. The 
experience of the influence on the WAN and Grid 
Computing Environment are shared in this paper. 
As expected, the performance of meta-computing 
suffers due to the available bandwidth of WAN. In 
addition, with proper arrangement of the 
decomposed MPI domains, the performance lost 
can be largely reduced. In some cases, the 
meta-computing can actually gain some 
performance due to the limitation on the hardware 
of a single computer. The results have demonstrate 
the possibility of meta-computing for solving large 
scale simulation, especially when there is no single 
resource can handle the problem size under tackle. 
 
The factor of WAN will eventually be solved owing 
to the great advance in the technology of the 
internet. However, it is doubtless that to further 
realize the meta-computing, grid-aware/meta-aware 
numerical algorithms have to be developed, such as 
the latency tolerance numerical scheme and the 
dynamic load balancing under the Grid 
environment, etc. However, they are not concerned 
in this paper at this moment, but are important 
topics for further investigation. 
 
VII. References 
[1]. I. Foster, C. Kesselman, & S. Tuecke, “The 
Anatomy of the Grid: Enabling Scalable 
Virtual Organizations”, Lecture Notes in 
Computer Science 2150, 2001 
[2]. J. Novotny, “The Grid Portal Development 
Kit”, Journal of Concurrency and 
Computation: Practice and Experience 
(CCPE), Volume 14, Issue 13-15, Wiley Press, 
Nov-Dec 2002. 
[3]. I. Foster and C. Kesselman, “The Grid : 
Blueprint for a New Computing 
Infrastructure,” Morgan Kaufmann, 1st edition, 
1999. 
[4]. I. Foster, “The Grid, A New Infrastructure for 
21st Century Science,” Physics Today, vol. 55, 
no. 2, pp. 42-47, 2002. 
[5]. I. Foster, C. Kesselman, “Globus: A 
Metacomputing Infrastructure Toolkit,” The 
International Journal of Supercomputer 
Applications and High Performance 
Computing, vol. 11, no. 2, pp. 115-128, 1997. 
[6]. D. A. Reed, “Grids, the TeraGrid, and 
Beyond,” IEEE Computer, vol. 36, no. 1, 
pp.62-68, Jan. 2003. 
[7]. IBM Grid Computing, 
http://www-1.ibm.com/grid/ 
[8]. Smarr, L., Catlett, C.: Metacomputing, 
Communications of the ACM 35(6) (1992) 
45-52 
[9]. A.S. Grimshaw and Wm. A.Wulf. The Legion 
Vision of aWorldwide Virtual Computer. 
Comm. ACM, 40(1):39–45, January 1997. 
[10]. D.H.J. Epema, M. Livny, R. van Dantzig, X. 
Evers, and J. Pruyne. A Worldwide Flock of 
Condors: Load Sharing among Workstation 
Clusters. Future Generation Computer 
Systems, 12(1):53–66, May 1996. 
[11]. J. Arabe, A. Beguelin, B. Lowekamp, E. 
Seligman, M. Starkey, and P. Stephan. Dome: 
Parallel programming in a heterogeneous 
multi-user environment. In 10th International 
Parallel Processing Symposium, pages 
218–224, April 1996. 
[12]. Globus Toolkit, http://www.globus.org. 
 
 
附件： CCGrid2006 出國報告 
CCGrid 2006 Report  
報告人：黃維誠 
國科會計畫編號 : NSC 94-2212-E-492-002 
 
2006/05/26
INTRODUCTION ..............................................................................................2 
NCHC INTRODUCTION................................................................................................ 2 
CCGRID2006 INTRODUCTION .................................................................................... 2 
WHY WE ATTEND THIS EVENT ..................................................................................... 3 
ACHIEVEMENTS……. .......................................................................................3 
SCIENTIFIC PAPER PRESENTATIONS.............................................................................. 3 
TECHNICAL TRENDS AND DISCOVERY..........................................................4 
PEER-TO-PEER VS. GRID COMPUTING ............................................................................ 4 
CONCLUDING REMARKS ................................................................................5
• Resource Management and Scheduling 
• Computational Data and Information Grid 
• Architectures and Systems 
• Grid Economies and Service Architectures 
Why We Attend This Event 
CCGrid2006 focuses on the combined areas of cluster and grid computing 
which share many related technical issues, and are both areas of intense 
interest and rapid growth. Grid technology is also a core development of any 
high-performance computing software platform. Over the year, NCHC has 
devoted itself to development and promotion of both cluster and grid 
computing. Attending CCGrid will not only share our knowledge with 
international community in both cluster and grid computing arena, but also 
update ourselves with the heartbeats of both the international community of 
the cluster and the Grid. 
Achievements  
Two papers were presented in the CCGrid06. 
1. “A Semi-Empirical Model for Maximal LINPACK Performance 
Predictions” – Chau-Yi Chou, His-Ya Chang, Shuen-Tai Wang, Chang-Hsing Wu 
In general, the maximal LINPACK performance of a large cluster depends on 
the number of processors, the total memory capacities, the problem size, the 
block size, the middle-ware of message passing, and the BLAS (Basic Linear 
Algebra Subprograms) library. One must handle these multi-variables factors 
to predict the performance score.  
In this paper, a semi-empirical weighting function is proposed to improve the 
performance prediction model for High performance Linpack (HPL) for large 
clusters. In order to better predict the maximal LINPACK performance, we 
first divide the performance model into two parts: computational power, and 
message passing overhead. In the latter part, we adopt Xu and Hwang’s 
broadcast model and introduce a weighting function w to account for the 
other effects. The difference between scores based on our semi-empirical 
model and the measured scores are less than 5%. The clusters used in the 
study include Myrinet-based, Quadrics, Gigabits Ethernet, IA64 or IA32 
architectures. 
2. “The Development of a Computational Grid Portal” – Weicheng Huang, 
Chien-Lin Huang, Chien-Heng Wu 
It is the goal of this paper to present the motivation for the development and 
conference, and to highlight some new and emerging areas of interest to the 
community.  
CCGrid06 focused on the combined area of cluster and grid computing which 
share many related technical issues, and are both areas of intense interest 
and rapid growth. The five keynotes covered the Biomedical Informatics 
Research Network, grid computing and Web Services issues. Dr. Manuel 
Peitsch (Novartis) spoke on "Grid Computing in Drug Discovery". Dr. Satoshi 
Sekiguchi (Director of Grid Technology Research Center, AIST, Japan) spoke 
on "AIST Service Oriented Architecture". Mr. Nicholas Evered (Oarcle) spoke 
on "Embracing Enterprise Grid Computing". Dr. Jysoo Lee (Director of 
Supercomputing Center, KISTI, South Korea) spoke on “Cyber-infrastructure 
in Korea.” Finally, Dr. Phil Papadopoulos (Program Director of Grid & Cluster 
Computing, SDSC, USA) spoke on “Building Cyber-infrastructure for the 
Biomedical Informatics Research Network.” 
Other than these key notes, it is an obvious observation that the Grid 
Computing has become an important issue for all participants. It is no longer a 
IT related adventure, but also involves various science domains. The focuses, 
however, are very diverse, from cyber-infrastructure, middleware, grid 
problem solving environment, to applications, nearly cover all scientific 
domains. In the meantime, different from Taiwan, all the countries are not 
devoting even more resources trying to make the Grid Computing work, after 
its initiative several years ago. Over the years, through various national grid 
projects, all the nations involved in the Grid Computing came to learn a lot 
about the nature of the Grid, what it takes to make Grid works. Now, to 
continue the adventure, almost all the countries are supporting the second 
phase of their Grid Computing projects. From this aspect, what is happening 
in Taiwan in the Grid Computing probably requires a more venturous vision. 
> Paper ID: 223 < 
 
2
 
Fig. 1. Integrated Grid Architecture 
 
As application scientists and engineers focus on their 
research activities, asking extra attention to the already 
complicated computing environment is highly unlikely to 
happen.  Therefore, strategically, a help in enabling the 
deployment of grid computing to the scientific researches and 
engineering activities is highly demanded.  Grid portals are 
there to fill up this gap between the grid computing technology 
and the existing application problem solving environment.  The 
Grid Computing Portal aims to promote the development and 
advancement of technologies that provide seamless and 
scalable access to wide-area distributed resources.  It has been 
recognized as an excellent channel for grid users to access the 
specialized information they need. [2] 
Portals are there to lower the barrier for scientists and engineers 
to access the distributed grid computing environment through 
the conventional web browser and thus the grid computing 
system can be accessed from anywhere as easily as internet 
sites nowadays.  The thought behind a portal is to build easy 
and secure grid interfaces for non-experts to use the grid 
computing. 
The courses of construction of grid portals can be 
distinguished into the following two strategic plans; the 
disciplinary specific portal and the generic computational 
portal.  There are various portals constructed for various grid 
projects or specific application program.  On the other hand, 
there are also several projects focused on constructing a generic 
portal environment based on which portals for specific 
applications can be customized from.  Different from the 
application specific portal which is designed to provide specific 
functionalities for particular application, a “generic” type of 
portal needs to support a wide range of grid computing 
behavior.  Readers are referred to the “Open Grid Portals” 
project as the starting point for further detailed research for 
various portal projects.[3] 
Among all the portal development activity, NCHC follows 
the step of OGCE from the beginning of its development and 
later on switch to LifeRay for the portal container.  Therefore, it 
will not be complete to discuss the activity of NCHC 
computational portal without reviewing the effort of OGCE.  
The “Open Grid Computing Environments” (OGCE)[4, 5] is 
supported by NMI (NSF Middleware Initiative)[6] of the US to 
support the standardization of grid computing middleware.  It 
was set out to develop standard compliant portlet components 
that can be reused various containers, given the portlet 
specification standard is followed, and to develop and integrate 
grid portal tools and services to support scientific gateways. [7]  
The JSR 168 Portlet Specification defines interoperability 
standards for portal containers.  It is understood by the 
community that a JSR 168 compliant portlet should be able to 
be deployed in multiple vendor containers.  The OGCE 
software distribution is currently in its second generation 
named as OGCE2. 
OGCE 2 Release Features a set of core grid porlets, grid 
programming interfaces, velocity portlets, and tools for 
building portal based on JSR168 compliant portal containers.  
The “Core Grid Portlets” provides a set of Grid portlets from 
proxy management, remote command execution, remote file 
management, to GPIR based information services.  The “Grid 
Programming Interfaces” is a high level grid programming 
interfaces using the Java CoG kit that bridges the developed 
portal and the underline grid toolkits, thus to save effort of 
compatibility issue between portal and grid toolkit.  OGCE uses 
velocity portlet tools to port Jetspeed-based portlets into 
JSR168 compliant portal.  It also integrates build and deploy 
tools compatible with open source containers such as uPortal 
and GridSphere, thus the choice of container can be more 
flexible.  Started from the OGCE, the NCHC portal has 
implemented the features of a generic Computational portal 
will need.  Gradually, the portal architecture has migrated to 
Liferay[8] for reasons documented in [9] over the year. 
In next section, the grid portal architecture along with the 
portal container Liferay will be described, followed by the 
basic requirements to be considered as well as the features in 
our Computational portal for compute intensive applications 
which is presented in section 3.  In section 4, an actual example 
of computing job submitted via Computational portal is given 
while the blueprint of future directions of the portal is drawn in 
the last section, the section 5.  
 
II. PORTAL ARCHITECTURE AND CONTAINER  
As illustrated in Fig. 2 and 3, the system wise architecture of 
NCHC portal is presented.  The portal architecture can be 
divided into several layers.  The basic computing facilities 
build the fabric layer of the portal based on which the Globus 
linked with Cog/Java API and other information services to 
form the middleware layer.  All the jobs submitted to the 
backend resources will be handled by Globus, with current 
version used as 3.2.  On top of the middleware layer, there 
comes the business layer.  This layer is composed of JBoss as 
the J2EE server to support the webserver Tomcat.  After 
receiving the requests from Tomcat Servler Container, JBoss 
Application Server start to process business logic based on the 
information provided by users. The JBoss will interact with the 
> Paper ID: 223 < 
 
4
 
Fig. 4. User-configurable Window in a Portal Pane 
 
III. FEATURES OF NCHC COMPUTATIONAL PORTAL  
 
The goals of our implementation of computing intensive 
portal is first aiming at providing HPC community with a 
grid-enabled portal which can satisfy their working scenario of 
their HPC applications.  The portal should provide its users 
with capability of program compilation, job submission, 
resource discovery, workflow construction and execution, job 
monitoring, data management including file transfer, data 
archiving, … etc.  Over all, the portal is to provide users with 
functions that one can run applications easily over grid 
computing environment.  In order to achieve this goal, some 
basic requirements for the portal are list in the following: 
 
A. User management/authentication 
The portal is to be accessed by using a regular web browser.  
The user logs into the portal by providing a user-id and 
password.  While better technologies exist for authentication, 
this one, best practice HTTPS security, is what users want and 
trust. Once the user is authenticated to the portal server, it is the 
job of the portal server to act as the user’s proxy in most grid 
interactions.[10]   
The user account, at current stage, can be obtained by 
requesting from the portal administrator with proper credential.  
The password of the user-id is, however, not changeable by the 
user currently.  
 
B. Accessibility improvement, the MyProxy portlet 
Once the user log in to the portal, the very next step should 
be provide grid computing credential.  This is the second level 
of security which involves providing the portal server access to 
remote Grid services.  This can be done by uploading the 
credential or, as provided by the system, requesting the 
credential from a credential repository currently hosted by the 
portal.  Due to the fact that most the major computing resources 
under the Grid Computing Environment (GCE) are running 
queuing system, therefore, there is no guarantee that the job 
submitted via portal will be completed by the time the user 
signs off the portal.  Hence, it is important that the user’s job 
can continue after he/she signs off the system and when user is 
back, he/she can access the job executed without difficulty.   
Currently, the portal adopt the MyProxy Credential 
Management Service provided by NCSA.[11]  The Proxy 
Manager Portlet allows users to obtain a proxy certificate that it 
can be considered as the user himself or herself. The standard 
approach is to have the user submit a proxy to a “MyProxy” 
server. The Proxy Manager Portlet then retrieves the proxy 
from the MyProxy server and holds it for the duration of the 
user’s session.  The lifetime of the proxy can be determined by 
the users.  The longer the proxy the more convenient the portal 
environment can be.  However, this will increase the risk of 
been intercepted.  It is always the wisdom of the users to 
determine the lifetime of the proxy.   This MyProxy server 
avoid the demand of uploading the keys/credential everytime a 
Globus command is issued. 
 
 
Fig. 5. Illustration of Gridftp portlet. 
 
C. Data transfer, Gridftp portlet 
 Tools to access to file metadata directories and remote file 
archives are central requirement for the Grid Computing portal. 
> Paper ID: 223 < 
 
6
Usually, this action will be taken if the user somehow 
determine that the workflow has stop somewhere for certain 
reason and he or she is not willing to wait on the project to 
complete anymore.  Then, the user can select to discharge the 
workflow to save the resources used. 
 
 
Fig. 7.  Screenshot of Job Status Report Portlet. 
 
IV. APPLICATION EXAMPLE 
As first officially exhibited in SC05 held in Seattle, the 
NCHC portal demoed itself by using the workflow to submit a 
typical CFD job.  The workflow process, as illustrated in Fig. 7, 
shows the scenario of the computation.  For simplicity, the 
executable program was pre-compiled and resided on a typical 
computing cluster call ASE.  Actually, if necessary, the portal 
can also be used to submit compile job via make file.  The input 
file of the computation resided on another server denoted as 
“data node” in the Fig. 8.  The workflow started with requesting 
a gridftp job sending the input file from the data node to the 
computing cluster, dispatch a parallel job on the cluster.  The 
cluster queued the job and executed the computation.  The 
computed result was then sent to another server, denoted as 
“local PC” with post-processing capability.  A commercial 
visualization program with its license server located elsewhere 
was then activated to display the computing result on a tiled 
display wall composed of LCD monitors.  The post processing 
server and display were actually sitting at the SC booth of 
NCHC out in Seattle, while other facilities involved were 
located back in NCHC facility in Taiwan. 
 
Fig. 8. Working Scenario of CFD Demonstration of 
Workflow.  
 
Other than this typical CFD application, the portal is also 
used to handle the complicated workflow for the computation 
of flooding forecasting models designed for the flood 
mitigation grid activities in Taiwan.  At this point, we do not 
have the luxury to address the activities yet. 
V. CONCLUSION AND FUTURE WORKS 
The key to the Grid Computing portals is being able to hide 
Grid details behind useful application interfaces. The user 
needs to be able to launch, configure and control remote 
applications in the same way he or she used to do on 
supercomputers.  In this paper, we have briefly introduced the 
grid computing effort in Taiwan focusing on how to lower the 
barrier of the deployment of grid computing environment.  A 
Computational portal project was launched one and half year 
ago with the goal to provide a generic Computational portal 
which can be later customized for specific applications.  The 
portal takes the advantage of Liferay as portal container and 
implemented several JST168 compliant portlets, including 
MDS Search Property Portlet, Proxy Manager Portlet, GridFTP 
Portlet, Workflow Portlet, Job Status Portlet, LDAP Browser 
Portlet, and Job Execution portlet. 
The development of this Computational portal is far from 
completed.  Issues like how to further enhance its capability, 
how to improve the user interface are constantly under 
discussion.  For the further development, the portal will migrate 
all available portlets from both Gridsphere and OGCE2 to 
enhance its functionality and yet without “reinventing the 
wheel”.  Also, the integration of SRB will be taken to 
strengthen the data management ability of the portal.  The 
workflow scripts will be stored into MySQL database for 
further improvement regarding user access management.  A 
graphical workflow interface is currently undertaken to 
improve the usability and friendliness of the workflow portlet.  
The “Parametric Study” portlet will be completed in the near 
future to deal with parametric sweeping type of high 
throughput computing. The mechanism to notify users of the 
status of their job will also be investigated.  A more timely 
