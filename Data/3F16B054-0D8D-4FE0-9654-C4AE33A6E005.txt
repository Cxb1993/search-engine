Abstract
A new method is devised for linear and nonlin-
ear independent component analysis. Unlike tra-
ditional statistics oriented ICA algorithms, which
have been developed based on minimization of the
Kullback-Leibler(KL) divergence between retrieved
components, this method uses the recurrent optimal
post-nonlinear kernel method to realize blind separa-
tion of linear or nonlinear mixtures of independent
sources. The nonlinear mixing structure of indepen-
dent sources is realized by multiple generalized ada-
lines(gadalines). Following the leave-one-out learn-
ing strategy, hyper-parameters of each gadaline as
well as intermediate independent components are op-
timized under a mean-field-annealing process to sat-
isfy constraints given by multi-channel observations.
The new method is shown more feasible for nonlinear
independent component analysis. It indeed involves
with no computations of optimizing statistics, such
as the KL divergence or high order moments, which
have been known diﬃcult to be resolved for nonlinear
independent component analysis.
1 Introduction
Independent component analysis (ICA) [2][3][4][5]
has been developed over ten years for application to
blind source separation (BSS) [6] of real world signals,
including ECG, EEG, ERP and MRI signals and so
on. An ICA algorithm is aimed to recover statisti-
cally independent sources s, which form observations
through the following linear mixtures,
x = As (1)
where s = (s1[t], s2[t], ..., sd[t])0 is a vector with d
components and A is a d × d mixing matrix, both
of which are actually unknow to an ICA algorithm.
Given observation data for independent component
analysis are considered as a sample from x. An eﬀec-
tive demixing matrix P is expected identical to the
inverse of matrix A. Its product to both side of equa-
tion (1) could retrieve unknown independent sources
s,
y =Px = PAs = s
Blind separation of linear mixtures of independent
sources has been successfully resolved by many dif-
ferent ICA algorithms [2][3][4][5]. However, it is still
unknown how to extend these traditional ICA al-
gorithms for blind separation of post-nonlinear mix-
tures of independent sources,
x = F (As) (2)
where F = {f1, f2, ..., fd} and each fi denotes an ar-
bitrary post-nonlinear function [9][10][11][12]. Figure
1 shows the post-nonlinear mixture model character-
ized by equation (2). It is known complicate and diﬃ-
cult to express the joint pdf or marginal pdf of inde-
pendent sources and extend Kullback-Leibler based
ICA algorithm [9][10][11][12] due to unknown post-
nonlinearity for post-nonlinear independent compo-
nent analysis.
Post-nonlinear functions have been employed in the
field of neural networks for organization of some kinds
of nonlinear structures. This work is aimed to solve
the post-nonlinear ICA problem. The main challenge
is to estimate the inverse, H = {h1, h2, ..., hd}, of the
unknown post-nonlinear function F, where hi = f
−1
i .
Once obtained, H could be applied to both side of
equation (2), to translate the post-nonlinear ICA
problem back to linear independent component anal-
ysis,
x0 = H(x) = H(F (As)) = As (3)
where x0 denotes the original linear mixtures of s. By
equation (3), the post-nonlinear ICA problem reduces
to the original linear ICA problem, which could be
easily solved by many traditional ICA algorithms.
Here we employ gadaline networks to emulate
post-nonlinear mixture models and develope learn-
ing methods to estimate network parameters for blind
separation of post-nonlinear mixtures of independent
sources. The mean-field annealing process is also
introduced to compensate for uncertainty of esti-
mated independent sources. By numerical simula-
tion, we show eﬀectiveness of the proposed learning
method for post-nonlinear ICA problem. The pro-
posed method operates following the leave-one-out
stategy. Each time one of independent sources is se-
lected and set oﬀ to contribute the network output, it
1
3.2 Optimal gadaline
Optimizing a gadaline subject to constraints of given
paired data, {(s[t], y[t])}t, is reviewed in this subsec-
tion. The response of a weighted gadaline to an input
h[t] = w0s[t] is modeled by the following conditional
pdf,
q(y|h = h[t]) = f(y; rk, σ2y) if h[t] ∈ Ik (5)
Combining equation (4) and equation (5), we have
the following expression,
q(y|h = h[t]) =
KX
k=1
θK(w
0x[t]; c)0eKk f(y; rk, σ
2
y) (6)
Using the average log likelihood to measure the fit-
ness of q(y, h) to all (h[t], y[t]), leads to the following
criterion,
l =
1
N
log
N
Π
t
q(y[t]|h = h[t]) (7)
=
1
N
NX
t
log
KX
k
δ[t]0eKk f(y; rk, σ2y)
=
1
N
NX
t
log
KX
k
δk[t]f(y; rk, σ2y)
=
1
N
NX
t
KX
k
δk[t] log f(y; rk, σ2y)
where δk[t] is the kth component of δ[t] and
f(y; rk, σ2y) =
1√
2πσy
exp(− (y−rk)
2
2σ2y
) is the Gaussian
pdf .
On the other hand, consider the collection
Hk = {h[t]|δ[t] = eKk },
, which consists of all possible h[t] with δ[t] = eKk for
some k. Assume external fields inHk have a Gaussian
distribution centered at ck with common variance σ2h.
The average log likelihood that measures the fitness
of the correspondent pdf toHk is expressed as follows,
l0k =
1
Nk
log Π
t:δ [t]=eKk
f(h[t]; ck, σ
2
h)
=
1
Nk
X
t:δ [t]=eKk
log f(h[t]; ck, σ
2
h).
The weight sum of all l0k is as following
l0 =
KX
k
Nk
N
l0k (8)
=
1
N
KX
k
X
t:δ [t]=eKk
log f(h[t]; ck, σ
2
h)
=
1
N
KX
k
X
t
δ[t]0eKk log f(h[t]; ck, σ2h)
=
1
N
KX
k
X
t
δk[t] log f(h[t]; ck, σ
2
h).
Following equation (7) and equation (8), we ob-
tain the following mathematical framework for opti-
mization of a gadaline subject to given paired data
{(x[t], y[t])}t,
E({δ[t]},w, c, r,σ) = −1
2
(l + l0) (9)
=
1
N
NX
t
KX
k
δk[t] log f(y; rk, σ
2
y)
+
1
N
KX
k
X
t
δ[t] log f(h[t]; ck, σ
2
y)
where X
k
δk[t] = 1,∀t
δk[t] ∈ {0, 1},∀t, k.
Searching for optimal parameters w, c, r and σ is
translated to minimizing the mathematical frame-
work.
Since equation (9) contains both continuous and
discrete variables, we apply a hybrid of the mean-
field annealing (MFA) and gradient decent methods
to resolve the mathematical framework, and get an
free energy function,
ψ(u,v) = E(v,w, c, r,σ) +
X
t
X
k
vk[t]uk[t]
− 1
β
X
t
log
"X
k
exp(βuk[t])
#
3
3.4 A recurrent learning process for
post-nonlinear independent com-
ponent analysis
Let x[t] and s[n, t] denote the observa-
tions and unknown sources respectively,
where x[t] = (x1[t], x2[t], ..., xd[t])0 and
s[n, t] = (s1[n, t], s2[n, t], ..., sd[n, t])
0, and n de-
notes the iteration number. For each n leave-one-out
approximation approaches xi[t] by the output of
a weighted gadaline in responding {sj [n, t]}j 6=i.
Following the equation 9, it needs to minimize
E({δ[t]},w, c, r,σ) (18)
=
1
N
NX
t
KX
k
δk[t] log f(xi[t]; rk, σ2y) (19)
+
1
N
KX
k
NX
t
δk[t] log f(
X
j 6=i
wijsj [n, t]; ck, σ
2
h)
for optimization of w, c, r and σ, by which the post-
nonlinear function gi is defined as follows,
gi(h; ci, ri) =
X
k
rik exp(−β kh− cikk2)P
l
exp(−β kh− cilk2)
With the post-nonlinear function gi, the approxi-
mating error ei[n, t] can be expressed as follows
ei[n, t] = xi[t]− gi(
X
j 6=i
wijsj [n, t]; ci, ri),
which is then substituted to a transfer function for
updating current si[n, t].
Independent sources retrieved by the process of
leave-one-out approximation could be further em-
ployed to tune the post-nonlinear mixture model so
as to improve accuracy in estimation of the post-
nonlinearity G. As shown in equation (3), the inverse
function H of G could be employed to translate the
post-nonlinear ICA problem to the linear ICA prob-
lem.
The learning process for blind separation of PNL
mixtures of independent sources is summarized as fol-
lows
1. n = 0, si[n, t] = xi[t],∀i, t, and set β to a suﬃ-
ciently low value.
2. For all i
(a) Find wi, ci, ri by using equation (??)∼(17)
to minimize equation (18),
(b) ei[n, t] = xi[t]− gi(
P
j 6=i
wijsj [n, t]; ci, ri),∀t.
(c) si[n, t] = T (βei[n, t]),∀t.
3. If a halting condition holds, halt, otherwise n =
n+ 1, β = β0.995 and goto step 2.
Independent sources estimated by the above pro-
cedure are led to the following procedure
1. n = 0, si[n, t] = xi[t],∀i, t, and set β to a suﬃ-
ciently low value.
2. For all i, find wi, ci, ri by using equation
(??)∼(17) to minimize equation (18)
3. If a halting condition holds, halt, get inverse
function hi of each gi and return x0 = H(x),
otherwise set β = β0.995 and goto step 2.
4 Numerical simulations
Figure 7 shows independent sources, sampled from


sin( 2πt800)
sin(2πt300 + 6 cos(
2πt
60 ))
sin( 2πt90 )

 , for t = 1, ..., 800.
Figure 8 shows their PNLmixtures through randomly
created matrix


1.2701 −0.0084105 −0.026119
−0.16132 1.2348 −0.2889
0.064106 0.15726 1.1928


and post-byperbolic-tangent functions. The esti-
mated gi functions are shown in figure 9 and their
inverses are show in figure 10. Through the inverse
functions, the original observations are translated to
signals shown in figure 11, which are treated as lin-
ear mixtures of independent sources and processed
by Jade ICA [13][14].
5
Figure 1: Post-nonlinear mixture model
Figure 2: A gadaline network for the post-nonlinear
mixture model
Figure 3: Gadaline
Figure 4: Leave-one-out process
7
