(Self-Organizing Map) 的演算法  (稱為
Q-Kohon) 代替，當外部環境狀態輸入時，
可激發鄰近的區域來求得近似的狀態-行
動值[5]，並獲得良好的效果。 
彈性尺寸可適拓撲神經網路結構 
(FAST) [6] [7] 是一種非監督式學習的網
路模式，它是結合 ART [8] 可變警戒值 
(Variable Vigilance) 的優點和 GAR (Grow 
and Present) 模型  [9] 中的類別刪除 
(Pruning) 機制這兩個優點所衍生出的架
構，具有計算量不大的優點，其演算法可
處理動態分類  (Dynamic Categorization) 
或即時分堆 (Online Clustering) 的問題。
因此 Perez [7] 將 FAST 架構取代 AHC 中
的 Box [10]，並在控制效能上獲得很大的
改進。 
 本專題研究計劃的目的主要是研究一
改良的 FAST 架構，並延伸此一架構將其
與 Q 學習演算法結合。改良後的 FAST 架
構可以產生穩定且精確的分類結果，提供
Q 學習系統更適當的分類結果，使整個系
統具有更佳的學習效果。 
 
三、 適應共振理論(ART)與相關方法 
3.1 ART 
ART 是由 Grossberg 在 1976 年所提出
的。他將靜態的競爭式學習神經網路改變
成為具動態架構的神經網路，又引進了回
授機構（feedback mechanism），用以產生
由上而下的期望（top-down expectation），
因此神經元在贏得競爭之後還必須符合這
個期望，才有資格進行學習；此時，順向
路徑與回授路徑所產生的交互作用，會使
得神經元的輸出圖樣重複地出現，此即共
振狀態（resonant state），這個理論因而被
稱為適應共振理論。 
 
3.2 彈性可適尺寸結構(FAST) 
Pruning
<
<
Distance 
calculation
W T
Learning
Comparator
Comparator
Random Number 
Generator
Weight Vector Threshold
Cluster 
On/OffInput
圖 1、FAST 架構圖 
 
FAST 是由 Perez & Sanchez 在 1996 年 
所提出。它是一種非監督式學習類神經網
路，由 ART 衍生而來，可用來解決動態分
類 (Dynamic Categorization) 或稱即時分
群 (Online Clustering)的功能，且併入了
Alpaydin 所提出的動態警戒值 (Dynamic 
Vigilance Parameters) 觀念及即時刪除機
制(Online Pruning Mechanism) 的功能來實
現類神經網路分類。在 FAST 的類神經網
路中，每個神經元裡有一個 W (權重值) 與
外面鍵結、T (臨界值) 決定神經元感知域 
(Sensitivity Region) 大小。當有輸入樣本進
入時，會將每一筆的輸入樣本進行分類，
計算是進入那一個神經元的感知域；若此
輸入樣本不在感知域的範圍內，則新增一
個神經元產生新類別儲存此輸入樣本。樣
本進入類別時，動作中的神經元會依每次
學習後動態更新 W 及 T 值、並且若輸入樣
本一次進入多個類別時，會適當的對多個
有重覆感知域的神經元進行刪除的動作，
FAST 網路架構圖如圖 1 所示。 
由圖 1 中可得知當一筆輸入樣 P進入
運算時，會與來自Learning部份的神經元中
之方塊W，也就是權重值 (Weight Vector) 
在Distance中算出曼哈頓距離 ( )jWPD ,  (例
如平面上兩點的座標分別為 p1 在 (x1, y1) 
及 p2 在 (x2, y2), 則它們的曼哈頓距離為  
 2
四、可適接收域法和 Q 學習 
我們發現BOX配合Q學習應用於倒單
擺時，其中有許多 BOX 都未被使用到，而
造成系統資源上的浪費，所以我們試著利
用 FAST 的理論結合 Q 學習。並進一步嘗
試修改 FAST 與 Q 學習的機制，使系統能
更快更容易的學習。 
 
4.1 可適接收域法 
我們依下列三個方向來分別討論與修
正，並將其命名為可適性接收域法，簡稱
ARM (Adaptive Resonance Theory)，。 
a. 正規化 
b. 權重值調整 
c. 刪除機制 
 
a 正規化(Normalization) 
 當 FAST 在計算距離 D 時，是直接以
輸入樣本與神經元的中心點來做計算；但
由於每個維度每次的改變量並不一致，使
得某些參數稍微改變就超過臨界值 T，造
成神經元數的增加；或是某些參數已經大
量改變卻沒辦法超過臨界值 T，而無法產
生新的神經元。故我們加入了正規化的機
制，使各個參數在做變化的同時能夠被觀
察出來，而能夠適當的被歸類或是產生新
的神經元。 
 正規化公式如下： 
 
i
i
i PatternInputMax
PatternInputp
__
_=      (6) 
i 為輸入空間的維度。 
  
b 權重值調整 
 當一神經元被重覆激發時，它的中心
點會往新輸入樣本的方向移動；當激發次
數越多，該神經元的中心點會大幅偏移，
而造成狀態上的誤判，使Q學習無法參考到
正確的參數值。如圖 4 所示，(a)表示輸入
樣本P1，使得系統產生中心點為W0之神經
元；(b)表示輸入樣本P2落在W0神經元的接
受域內，但調整後使得神經元的中心點移
到W0'；(c)表示輸入樣本P3，也落在W0'神
經元接受域內，但使得中心點更為偏移。
(d)表示當輸入樣本P4 (實際上其值與P1相
同)輸入時，因中心點偏移太遠，已落在W0
神經元的接受域之外。致使系統無法參考
到之前的學習經驗，降低系統的控制效
能。故神經元的學習速率必須降低，即式
(2)中的α值不可太大，使ARM能夠明確的
將相似的類別做歸類，提供適當的歸類結
果給Q學習作為控制的依據。 
 
c 刪除機制 
 FAST 的刪除機制是由被激發的次數
來決定被刪除的機率：當激發次數越多，
被刪除的機率也就越高。但從原理得知，
越是經常被激發的神經元，最有可能被刪
除。導致系統一方面在刪除常用的神經元
時，很可能又會新增同樣的神經元，因而
造成系統的不穩定；另一方面其他不常用
的神經元卻不會被刪除，造成系統的資源
浪費。故我們重新擬定刪除的機制，將常
被激發的神經元保留住，將其他不常用的
神經元做刪除的動作。 
我們將刪除機制更改如下：當神經元
被激發時，則將此神經元的 Pr 值設為 1，
而未被激發的神經元的 Pr 值依(4)式更
新。我們設定神經元個數的上限為 K，當
神經元數達到 K 時，則啟動刪除機制，偵
測每個神經元的 Pr 值，當 Pr 值小於亂數
rnd 時，則將此神經元刪除。 
將 FAST 做以上的修正後，整個 ARM
的流程如下： 
1. 將輸入樣本 P 做正規化，如式(6)。 
2. 計算出輸入樣本 P與 j個神經元間的距
離差 ( )jWPD , 。 
3. 若 ( )jWPD , 與每個神經元的臨界值T均
比較過後，發現 ( ) jj TWPD >, ，則產生
出一個新的神經元 j + 1，並將此神經
元初始化它的權重值為Wj+1 = P、臨界
值Tj+1 = Tini、刪除機率初始值Prj+1 = 1 
 4
 
圖 4 實驗 10 次的平均結果 
 
表 1 結果比較  
 最佳 最糟 
BOX Q 351 >500 
ARM Q 7 82 
 6
 
ARM 根據環境狀態來產生類別，僅需
7 至 10 個神經元即可。而 BOX 已將環境
事先做好切割為 162 個 BOX。 
綜合以上模擬結果，我們驗證了 ARM 
Q 可產生較少的神經元，並且在較短時間
內使倒單擺成功站立。 
 
五、結論 
本研究提出一加強式學習架構，架構
中整合了 ARM(以 FAST 為基礎的演算法)
和 Q 學習演算法。ARM 可以降地分類震盪
發生的可能性，產生穩定且精確的分類結
果，提供 Q 學習系統更適當的輸入值，使
整個系統具有更佳的學習效果。由倒單擺
的模擬結果，成功驗證了此架構的可行
性，順利達成計劃中預定完成的目標。 
 未來可以嘗試將ARM與Q(λ)學習[11]
結合或是 ARM 與 SARSA(λ)[12]結合，會
是更有效率的加強式學習架構。 
 
六、參考文獻 
 
[1]  Watkins, C. J. C. H., and Dayan, P., 
Technical note: Q-Learning, Machine 
Learning, 8(3-4): pp. 279-292, 1992. 
[2]  Claude F. Touzet, “Q-Learning for 
Robot,” in M. A. Arbib, editor, Handbook 
of Brain Theory and Neural Networks, pp. 
934-937, 2003. 
[3]  L. E. Parker, C. Touzet and F. Fernandez, 
“Techniques for learning in multi-robot 
teams,” in Robot Teams: From Diversity 
to polymorphism (T Balch and L. E. 
Parker, Eds.), Natick, MA: A. K. Peters, 
2001. 
[4]  K.-S. Hwang, S.-W. Tan; C.-C. Chen, 
“Cooperative strategy based on adaptive 
Q-learning for robot soccer systems,” 
IEEE Transactions on Fuzzy Systems, Vol. 
12, Issue: 4, pp. 569-576 Aug. 2004. 
[5]  R. A. McCallum, “Instance-based state 
identification for reinforcement learning,” 
in Advances in Neural Information 
Processing Systems 7 (G. Tesauro, D. 
Touretzky, and T. Leen, Eds.), 
Cambridge, MA: MIT Press, 1995. 
[6]  A. Pérez-Uribe, Structure-adaptable 
digital neural networks, PhD Thesis 2052, 
Swiss Federal Institute of 
Technology-Lausanne, Lausanne, 1999. 
[7]  A. Perez and E. Sanchez, “The FAST 
architecture: a neural network with 
flexible adaptable-size topology,” 
Proceedings of Fifth International 
Conference on Microelectronics for 
Neural Networks, pp. 337–340, 12-14 Feb. 
1996 
[8]  G. A. CARPENTER and S. 
GROSSBERG, “The ART of adaptive 
pattern recognition by a self-organizing 
neural network,” IEEE Computer, 21(3), 
pp77-88, March 1988. 
[9]  A. E. Alpaydin, Neural models of 
incremental supervised and unsupervised 
learning. PhD thesis, Swiss Federal 
Institute of Technology-Lausanne, 
Lausanne, DPFL, 1990. Thesis 863.  
[10] Barto, Andrew G., Sutton, Richard S. 
and Anderson, Charles W., “Neuronlike 
adaptive elements that can solve difficult 
learning control problems.” IEEE 
Transactions on System, Man, and 
Cybernetics SMC-13: 834-846. 1983. 
[11] Peng, J. and Wiliams, R.J., “Incremental 
Multi-Step Q Learning,” Machine 
Learning, 22, pp. 283-290, 1996. 
