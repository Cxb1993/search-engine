可供推廣之研發成果資料表 
■ 可申請專利  ■ 可技術移轉                                      日期：98 年 6月 26 日 
國科會補助計畫 
計畫名稱：無線感測網路之分散式演算法設計分析與實作 
計畫主持人：陳震宇 
計畫編號：NSC 97-2218-E-259-003   學門領域：電信/網路 
技術/創作名稱 Optimal DRR-gossip 
發明人/創作人 陳震宇 
技術說明 
中文： 
隨傳演算法 (Gossip) 在現代的網路應用中，有非常廣泛且重要的
應用。 雖然隨傳演算法 (Gossip)本著它天生的分散式且隨機性，
有著強健性的優點， 然而、如何降低它的訊息複雜度(message 
complexity) 和時間複雜度 (time complexity) 一直是傳演算法 
(Gossip)是否能在資源有限的網路，例如無線感測網路 (wireless 
sensor network) 以及對等網路 (P2P network)，上有效執行的關
鍵。 
在這個計畫中，我們提出了一個在無線感測網路 (wireless 
sensor network) 以及對等網路 (P2P network)上用於計算“資料
集結統計值”(Aggregate) 的隨傳演算法 。我們並證明這個演算
法擁有最佳的訊息複雜度(optimal message complexity)最佳、和
最佳的時間複雜度 (optimal time complexity)。 
 
英文：Motivated by applications to modern networking 
technologies, there has been   interest in designing  efficient 
gossip-based protocols for computing aggregate functions. While 
gossip-based protocols provide robustness due to their 
randomized nature, reducing the message and time complexity 
of these protocols is also of paramount importance in the context 
of resource-constrained networks such as sensor and 
peer-to-peer networks. 
We present efficient gossip-based algorithms for aggregate 
computation that are both time optimal and almost 
message-optimal. Given a n-node network, our algorithms 
guarantee that all the nodes can compute the common 
aggregates (such as Min, Max, Count, Sum, Average, Rank etc.)  
of their values in optimal O(\log n) time and using O(n \log \log n) 
messages. 
Our result improves on the  algorithm  of Kempe et al. that is 
time-optimal, but uses O(n \log n) messages as well as on the 
algorithm of Kashyap et al. that uses O(n \log \log n) messages, 
but is not time-optimal (takes O(\log n \log \log n) time). We also 
show that our algorithms can be used to 
improve gossip-based aggregate computation in  sparse 
communication networks, such as in peer-to-peer networks.  
 
 
國科會專題研究計畫  
期末報告 
 
無線感測網路之分散式演算法設計
分析與實作 
 
NSC- 97-2218-E-259-003 
 
 
 
計畫主持人 
陳震宇 
 
國立東華大學 電機系 
中華民國九十八年六月二十五日 
 
1 Introduction
1.1 Background and Previous Work
Aggregate statistics (e.g., Average, Max/Min, Sum, and, Count etc.) are significantly useful for many ap-
plications in networks [2, 5, 6, 9, 11, 13, 24]. These statistics have to be computed over data stored at
individual nodes. For example, in a peer-to-peer network, the average number of files stored at each node
or the maximum size of files exchanged between nodes is an important statistic needed by system designers
for optimizing overall performance [22, 25]. Similarly, in sensor networks, knowing the average or max-
imum remaining battery power among the sensor nodes is a critical statistic. Many research efforts have
been dedicated to developing scalable and distributed algorithms for aggregate computation. Among them
gossip-based algorithms [1, 2, 4, 8, 9, 12, 16, 17, 20, 23] have recently received significant attention because
of their simplicity of implementation, scalability to large network size, and robustness to frequent network
topology changes. In a gossip-based algorithm, each node exchanges information with a randomly chosen
communication partner in each round. The randomness inherent in the gossip-based protocols naturally pro-
vides robustness, simplicity, and scalability [7, 8]. We refer to [8, 9, 7] for a detailed discussion on the
advantages of gossip-based computation over centralized and deterministic approaches and their attractive-
ness to emerging networking technologies such as peer-to-peer, wireless, and sensor networks. This paper
focuses on designing efficient gossip-based protocols for aggregate computation that have low message and
time complexity. This is especially useful in the context of resource-constrained networks such as sensor and
wireless networks, where reducing message and time complexity can yield significant benefits in terms of
lowering congestion and lengthening node lifetimes.
Much of the early work on gossip focused on using randomized communication for rumor propagation
[3, 21, 7]. In particular, Karp et al. [7] gave a rumor spreading algorithm (for spreading a single message
throughout a network of n nodes) that takes O(log n) communication rounds and O(n log log n) messages.
It is easy to establish that Ω(log n) rounds are needed by any gossip-based rumor spreading algorithm (this
bound also holds for gossip-based aggregate computation). They also showed that any rumor spreading
algorithm needs at least Ω(n log log n) messages for a class of randomized gossip-based algorithms referred
to as address-oblivious algorithms [7]. Informally, an algorithm is called address-oblivious if the decision to
send a message to its communication partner in a round does not depend on the partner’s address. Karp et
al.’s algorithm is address-oblivious. For non-address oblivious algorithms, they show a lower bound of ω(n)
messages, if the algorithm is allowed only O(log n) rounds.
Kempe et al. [9] were the first to present randomized gossip-based algorithms for computing aggre-
gates. They analyzed a gossip-based protocol for computing sums, averages, quantiles, and other aggregate
functions. In their scheme for estimating average, each node selects another random node to which it sends
half of its value; a node on receiving a set of values just adds them to its own halved value. Their protocol
takes O(log n) rounds and uses O(n log n) messages to converge to the true average in a n-node network.
Their protocol is address-oblivious. The work of Kashyap et al. [8] was the first to address the issue of
reducing the message complexity of gossip-based aggregate protocols, even at the cost of increasing the
time complexity. They presented an algorithm that significantly improves over the message complexity of
the protocol of Kempe et al. Their algorithm uses only O(n log logn) messages, but is not time optimal
— it runs in O(log n log log n) time. Their algorithm achieves this O(log n/ log log n) factor reduction in
the number of messages by randomly clustering nodes into groups of size O(log n), selecting representative
for each group, and then having the group representatives gossip among themselves. Their algorithm is not
address-oblivious. For other related work on gossip-based protocols, we refer to [8, 2] and the references
therein.
1
1.3 Organization
The rest of this paper is organized as follows. The network model is described in Section 2 followed by
sections where each phase of the DRR-gossip algorithm is introduced and analyzed separately. The whole
DRR-gossip algorithm is summarized in Section 6. Section 7 applies DRR-gossip to sparse networks. Sec-
tion A (in Appendix) gives a brief background on the main probabilistic tools used in our analysis — the
Doob martingale and Azuma’s inequality.
2 Model
The network consists of a set V of n nodes; each node i ∈ V has a data value denoted by vi. The goal is to
compute aggregate functions such as Min, Max, Sum, Average etc., of the node values.
The nodes communicate in discrete time-steps referred to as rounds. As in prior work on this problem [7,
8], we assume that communication rounds are synchronized, and all nodes can communicate simultaneously
in a given round. Each node can communicate with every other node. In a round, each node can choose
a communication partner independently and uniformly at random. A node i is said to call a node j if i
chooses j as a communication partner. (This is known as the random phone call model [7].) Once a call is
established, we assume that information can be exchanged in both directions along the link. In one round, a
node can call only one other node. We assume that nodes have unique addresses. The length of a message is
limited to O(log n+ log s), where s is the range of values. It is important to limit the size of messages used
in aggregate computation, as communication bandwidth is often a costly resource in distributed settings. All
the above assumptions are also used in prior work [9, 8]. Our algorithm can tolerate the following types of
failures (similar to the algorithms of [9, 8]). In particular, we assume two types of failures: (i) some fraction
of nodes may crash initially, and (ii) links are lossy and messages can get lost. Thus, while nodes cannot
fail once the algorithm has started, communication can fail with a certain probability δ. Without loss of
generality, 1/ log n < δ < 1/8: Larger values of δ, requires only O(1/ log(1/δ)) repeated calls to bring
down the call probability below 1/8, and smaller values only make it easier to prove our claims.
Throughout the paper, “with high probability (whp)” means “with probability at least 1−1/nα, for some
α > 0”.
3 Phase I: Distributed Random Ranking (DRR)
3.1 The DRR algorithm
The DRR algorithm is as follows. Every node i ∈ V chooses a rank independently and uniformly at random
from [0, 1]. (Equivalently, each node can choose a rank uniformly at random from [1, n3] which leads to
the same asymptotic bounds; however, choosing from [0, 1] leads to a smoother analysis, e.g., allows use of
integrals.) Each node i then samples up to log n − 1 random nodes sequentially (one in each round) till it
finds a node of higher rank to connect to. If none of the log n − 1 sampled nodes have higher rank then i
becomes a “root”. Since every node except root nodes connects to a node with higher rank, there is no cycle
in the graph. Thus this process results in a collection of (disjoint) trees which together constitute a forest F.
Algorithm 1 gives the pseudo-code of the DRR algorithm.
We next analyze the number of trees and the size of each tree produced by the DRR algorithm; these are
critical in bounding the time complexity of DRR-gossip.
3.2 Number of Trees
Theorem 1 The number of trees produced by the DRR algorithm is O(n/ log n) whp.
Proof:
Assume that ranks have already been assigned to the nodes. All ranks are distinct with probability 1.
Number the nodes according to the order statistic of their ranks: the ith node is the node with the ith smallest
3
that this set of k nodes form a tree. For the sake of analysis, we will direct tree edges as follows: a tree edge
(i, j) is directed from node i to node j if rank(j) < rank(i), i.e. i connects to j. Without loss of generality,
fix a permutation of S: (s1, . . . , sα, . . . , sβ, . . . , sk) where rank(sα) > rank(sβ), 1 ≤ α < β ≤ k. This
permutation naturally induces a spanning directed tree on S in the following fashion: s1 is the root and any
other node sα (1 < α ≤ k) connects to a node in the totally (strictly) ordered set {s1, . . . , sα−1} (as fixed
by the above permutation). For convenience, we denote the event that a node s connects to any node on a
directed tree, T , as s→ T . Note that s→ T implies that s’s rank is less than that of any node on the tree T .
Also, we denote a directed spanning tree induced on the totally (strictly) ordered set {s1, s2, . . . , sα, . . . , sh}
as Th where a node sα can only connect to its preceding nodes in the ordered set. As a special case, T1 is
the induced directed tree containing only the root node s1. In the following, we bound the probability of Tk
happening:
Pr(Tk) = Pr (T1 ∩ (s2 → T1) ∩ (s3 → T2) ∩ · · · ∩ (sk → Tk−1))
= Pr(T1) Pr(s2 → T1| T1) Pr(s3 → T2|T2) . . .Pr(sk → Tk−1| Tk−1). (1)
To bound each of the terms in the product, we use the principle of deferred decisions: when a new node
is sampled (i.e., for the first time) we assign it a random rank. For simplicity, we assume that each node
sampled is a new node — this does not change the asymptotic bound, since there are now only k = O(log n)
nodes under consideration and each node samples at most O(log n) nodes. This assumption allows us to
use the principle of deferred decisions to assign random ranks without worrying about sampling an already
sampled node. Below we bound the conditional probability Pr(sα → Tα−1| Tα−1), for any 2 ≤ α ≤ k as
follows. Let rq = rank(sq) be the rank of node sq, 1 ≤ q ≤ α; then
Pr(sα → Tα−1| Tα−1)
≤
∫ 1
0
∫ r1
0
∫ r2
0
. . .
∫ rα−1
0
logn−1∑
h=0
(
α− 1
n
)
rhα drα . . . dr1.
The explanation for the above bound is as follows: Since Tα−1 is a directed spanning tree on the first α − 1
nodes, and sα connects to Tα−1, we have r1 > r2 > · · · > rα−1 > rα. Hence r1 can take any value between
0 and 1, r2 can take any value between 0 and r1 and so on. This is captured by the respective ranges of the
integrals. The term inside the integrals is explained as follows. There are at most log n− 1 attempts for node
sα to connect to any one of the first α−1 nodes. Suppose, it connects in the hth attempt. Then, the first h−1
attempts should connect to nodes whose rank should be less than rα, hence the term rhα (as mentioned earlier,
we assume that we don’t sample an already sampled node, this doesn’t change the bound asymptotically).
The term (α− 1)/n is the probability that sα connects to any one of the first α− 1 nodes in the hth attempt.
Simplifying the right hand side, we have,
Pr(sα → Tα−1| Tα−1)
=
α− 1
n
∫ 1
0
∫ r1
0
∫ r2
0
. . .
∫ rα−1
0
[1 + rα + r2α + . . . r
logn−1
α ]drα . . . dr1
=
α− 1
n
(
0!
α!
+
1!
(α+ 1)!
+
2!
(α+ 2)!
+ · · ·+ (log n)!
(log n+ α)!
)
.
The above expression is bounded by an , where 0 < a < 1 if α > 2 and 0 < a ≤ (1− 1logn+2) if α = 2.
Besides, Pr(T1) ≤ 1logn (cf. Theorem 1); hence, the equation (1) is bounded by
(
a
n
)k−1
1
log n
.
5
After the Convergecast process, each root broadcasts its address to all other nodes in its tree via the tree
links. This process proceeds from the root down to the leaves via the tree links (these two-way links were
already established during phase 1.) At the end of this process, all non-root nodes know the identity (address)
of their respective roots.
Complexity of Phase II
Every node except the root nodes needs to send a message to its parent in the upward aggregation process
of the Convergecast algorithms. So the message complexity isO(n). Since each node can communicate with
at most one node in one round, the time complexity is bounded by the size of the tree (this is the reason for
bounding size and not just the height). Since the tree size (hence, tree height also) is bounded by O(log n)
(cf. Theorem 2) the time complexity of Convergecast and Broadcast is O(log n). Moreover, as the number
of roots is at most O(n/ log n) by Theorem 1, the message complexity for broadcast is also O(n).
5 Phase III: Gossip
In the third phase, all roots of the trees compute the global aggregate by performing the uniform gossip
algorithm on the graph G˜ = clique(V˜ ), where V˜ ⊆ V is the set of roots and |V˜ | = m.
The idea of uniform gossip is as follows. Every root independently and uniformly at random selects a
node to send its message. If the selected node is another root then the task is completed. If not, then the
selected node needs to forward the received message to its root (all nodes in a tree know the root’s address
in Phase II — note that here is where we use a non-address oblivious communication). (Thus, to traverse
through an edge of G˜, a message needs at most two hops of G.)
The algorithm 2, Gossip-max, and the algorithm 4, Gossip-ave (which is a modification from the Push-
Sum algorithm of [9, 8]) compute the Max and Ave aggregates respectively (other aggregates such as Min,
Sum etc., can be calculated by a suitable modification). Note that there is no sampling procedure in the
Gossip-ave algorithm. The algorithm 3, Data-spread, a modification of Gossip-max, is used by a root node
to spread its value. If a root needs to spread a particular value over the network, it sets this value as its initial
value of the Gossip-max algorithm and all the other roots set their initial value to minus infinity.
5.1 Performance of the Gossip-max and Data-spread Algorithms
Letm denote the number of the root nodes. By Theorem 1, we havem = |V˜ | = O(n/ log n) where n = |V |.
Karp, et al. [7] show that all the m nodes of a complete graph can know a particular rumor (e.g., the Max
in our application) in O(logm) = O(log n) rounds with high probability by using their Push algorithm, a
prototype of our Gossip-max algorithm, with uniform selection probability. Similar to the Push algorithm, the
Gossip-max algorithm needs O(m logm) = O(n) messages for all the roots to obtain Max if the selection
probability is uniform, i.e., 1/m. However, in the implementation of the Gossip-max algorithm on the forest,
the root of a tree is selected with probability proportional to its size (number of nodes in the tree). Hence,
the selection probability is not uniform. In this case, we can only guarantee that after the gossip procedure
of the Gossip-max algorithm, a portion of the roots including the root of the largest tree will possess the
Max. After the gossip procedure, roots can sample O(log n) number of other roots to confirm and update,
if necessary, their values and reach the consensus on Max.
We show the following theorem for Gossip-Max (proof in Appendix).
Theorem 4 Running the Gossip-max algorithm on G˜ = clique(V˜ ) of G(V,E) results in at least Ω( c·nlogn)
root nodes (whp) having the global maximum, Max, where n = |V | and 0 < c < 1 is a constant.
Sampling Procedure
From Theorem 4, after the gossip procedure, there are Ω( cnlogn) = Ω(cm), 0 < c < 1 nodes with the
Max value. For all the roots to reach the consensus on Max, all the roots then sample each other as in the
sampling procedure. It is possible that the root of a larger tree will be sampled more frequently than the roots
of smaller trees. However, this non-uniformity is an advantage, since the roots of larger trees would have
obtained the Max in the gossip procedure with higher probability due to this same non-uniformity. Hence,
7
ing the Max value whp.
Proof: After the sampling procedure, the probability that none of the max-roots being sampled by a
non-max-root is at most
(
m−cm
m
) 1
c
logn
< 1n . Thus, after the sampling procedure, with probability at least
1− 1n , all the roots will know the Max.
5.1.1 Complexity of the Gossip-max and the Data-spread algorithms
The gossip procedure takes O(log n) rounds and O(m log n)=O( nlogn log n)=O(n) messages. The sam-
pling procedure takes O(1c log n)=O(log n) rounds and O(
m
c log n)=O(n) messages. To sum up, this phase
totally takesO(log n) rounds andO(n) messages for all the roots in the network to reach consensus onMax.
The complexity of the Data-spread algorithm is the same as the Gossip-max algorithm.
5.2 Performance of the Gossip-ave Algorithm
When the uniformity assumption holds in gossip (i.e., in each round, nodes are selected uniformly at random),
then it has been shown in [9] that on an m-clique with probability at least 1− δ′, Gossip-ave (uniform push-
sum in [9]) needs O(logm + log 1 + log
1
δ′ ) rounds and O(m(logm + log
1
 + log
1
δ′ )) messages for all
the m nodes to reach consensus on the global average within a relative error of at most . When uniformity
does not hold, the performance of uniform gossip will depend on the distribution of the selection probability.
In the efficient gossip algorithm [8], it is shown that the node being selected with the largest probability
will have the global average, Ave, in O(logm + log 1 ) rounds. Here, we prove that the same upper bound
holds for our Gossip-ave algorithm, namely, the root of the largest tree will have Ave after O(logm+ log 1 )
rounds of the gossip procedure of the Gossip-ave algorithm. (Note that unlike the Gossip-max algorithm, the
Gossip-ave algorithm has no sampling procedure.) In this bound, m = O(n/ log n) is the number of roots
obtained from the DRR algorithm and the relative error  = n−α, α > 0. Thus, the root of the largest tree
will have Ave after O(logm + log 1 ) = O(log n) rounds of the gossip procedure. The upper bound of the
running time of the Gossip-ave algorithm is given by the following theorem (proof in Appendix.)
Theorem 6 Whp, there exists a time Tave = O(logm + α log n) = O(log n), α > 0, such that for all time
t ≥ Tave, the relative error of the estimate of average aggregate on the root of the largest ranking tree, z,
is at most 2nα−1 , where the relative error is
|xˆave,t,z−xave|
|xave| and the global Ave is xave =
∑
j xj , when all xj
have the same sign. (We can always offset the values to have the same sign.)
Complexity of Gossip-ave
The Gossip-ave algorithm needs O(logm+ log 1 ) = O(log n) rounds and m · O(log n) = O(n) messages
for the root of the largest tree to have the global average aggregate, Ave, within a relative error of at most
2
nα−1 , α > 0.
6 DRR-gossip Algorithms
Putting together our results from the previous sections, we present Algorithm 5, the DRR-gossip-max algo-
rithm, and Algorithm 6, the DRR-gossip-ave algorithm, for computing Max and Ave, respectively. In the
DRR-gossip-max algorithm, after the Gossip-max procedure, all the roots will knowMaxwhp. If necessary,
a root then broadcastsMax to its tree members, requiringO(log n) rounds andO(n) messages since the size
of a tree is at most O(log n) by Theorem 2 and the number of roots is O(n/ log n) by Theorem 1.
The DRR-gossip-ave algorithm is more involved than the DRR-gossip-max algorithm. Unlike the Gossip-
max algorithm which ensures that all the roots will haveMaxwhp, the Gossip-ave algorithm only guarantees
that the root of the largest tree in terms of tree size will have the Ave whp. To ensure that all the roots have
Ave whp, after the Gossip-ave algorithm, the root of the largest tree has to spread out its estimate, the Ave,
by using the Data- spread algorithm where the root of the largest tree sets its estimate, the Ave, computed
9
Proof: Fix any node u0. We first show that the path from u0 to a root is at most O(log n) whp. Let
u1, u2, . . . be the successive ancestors of u0, i.e., u1 is the parent of u0 (i.e., u0 connects to u1), u2 is the
parent of u1 and so on. (Note u1, u2, . . . are all null if u0 itself is the root). Define the complement value to
the rank of ui as Ci := 1 − rank(ui), i ≥ 0. The main thrust of the proof is to show that the sequence Ci,
i ≥ 0 decreases geometrically whp. We adapt a technique used in [15].
For t ≥ 0, let It be the indicator random variable for the event that a root has not been reached after t
jumps, i.e., u0, u1, . . . , ut are not roots.
We need the following Lemma.
Lemma 8 For any t ≥ 1 and any z ∈ [0, 1], E[Ct+1It|CtIt−1 = z] ≤ z/2.
Proof: We can assume that z 6= 0; since Ct+1 ≤ Ct and It ≤ It−1, the lemma holds trivially if z = 0.
Therefore, we have It−1 = 1 and Ct = z > 0. We focus on the node ut. Denote the set of neighbors of node
ut by U ; the size of U is at most n− 1. Let Y be the random variable denoting the number of “unexplored”
nodes in set U , i.e., those that do not belong to the set {u0, u1, . . . , ut−1}. If Y = 0, then ut is a root and
hence Ct+1It = 0. We will prove that for all d ≥ 1,
E[Ct+1It|((CtIt−1 = z) ∧ (Y = d))] ≤ z/2. (2)
Showing the above is enough to prove the lemma, because if the lemma holds conditional on all positive
values of d, it also holds unconditionally. For convenience, we denote the l.h.s. of (2) as Φ.
Fix some d ≥ 1. In all arguments below, we condition on the event “(CtIt−1 = z) ∧ (Y = d)”. Let
v1, v2, . . . , vd denote the d unexplored nodes in U . If rank(vi) < rank(ut) for all i (1 ≤ i ≤ d), then ut is a
root and hence Ct+1It = 0. Therefore, conditioning on the value y = miniCi = mini(1− rank(vi)) ≤ z,
and considering the d possible values of i that achieve this minimum, we get,
Φ = d
∫ z
y=0
y(1− y)d−1dy.
Evaluating the above yields
Φ =
1− (1− z)d(1 + zd)
(d+ 1)
.
We need to show that the r.h.s of above is at most z/2. This can be shown by a straightforward induction
on d.
Using Lemma 8, we now prove Theorem 7:
We have E[C1I0] ≤ E[C1] ≤ 1. Hence by Lemma 8 and an induction on t yields that E[CtIt−1] ≤ 2−t.
In particular, letting T = 3 log n, where c is some suitable constant, we get
E[CT IT−1] ≤ n−3.
Now, suppose uT = u and that CT IT−1 = z. The degree of node u is at most n; for each of these nodes
v, Pr(rank(v) > rank(u)) = Pr(1 − rank(v) < 1 − rank(u)) = Pr(1 − rank(v) < z) = z. Thus the
probability that u is not a root is at most nz; more formally, ∀z,Pr(IT = 1|CT IT−1 = z) ≤ nz. So,
Pr(IT = 1) ≤ log nE[CT IT−1] ≤ n/n3 = 1/n2.
Hence, whp, the number of hops from any fixed note to the root is O(log n). By union bound, the
statement holds for all nodes whp.
Similar to Theorem 1, we can bound the number of trees produced by DRR.
11
Proof: We lower bound the number of messages exchanged between nodes before a large fraction
of the nodes (say, 1 − o(1)) correctly knows the maximum value, among all nodes. Suppose nodes can
send messages that are arbitrary long. (The bound will hold regardless of this assumption.) Without loss
of generality, we will assume that a node can send a list of all node addresses and the corresponding node
values learned so far (without any aggregation). For any node i to have correct knowledge of the maximum,
it should somehow know the values at all other nodes. (Otherwise, an adversary — who knows the random
choices made by the nodes — can always make sure that the maximum is at a node which is not known by i.)
There are two ways that i can learn about another node j’s value: (1) direct way: i gets to know j’s value by
communicating with j directly (at the beginning, each node knows only about its own value); and (2) indirect
way: i gets to know j’s value by communicating with a node w 6= j which has a knowledge of j’s value.
Note that w itself may have learned about j’s value either directly or indirectly.
Let vi be the (initial) value associated with node i, 1 ≤ i ≤ n. We will assume that all values are distinct.
By the adversary argument, the requirement is that at the end of any algorithm, on the average at least half
of the nodes should know (in the above direct or indirect way) all of the vi, 1 ≤ i ≤ n. (Otherwise, the
adversary can make that value that is not known to more than half of the nodes, the maximum.) We want
to show that the number of messages needed to satisfy the above requirement is at least cn log n, for some
(small) constant c > 0.
We define a phase (consisting of one or more rounds) as follows. Phase 1 starts with round 1. If phase
t ends in round j, then phase t + 1 starts in round j + 1. Thus, it remains to describe when a phase ends.
We distinguish sparse and dense phases. A sparse phase contains at most n messages (for a suitably small
constant  > 0, fixed later in the proof). The length of these phases is maximized, i.e., a sparse phase ends
in a round j if adding round j + 1 to the phase would result in more than n messages. A dense phase
consists of only one round containing more than n messages. Observe that the number of messages during
the phases 0 to j is at least (j− 1)n/2 because any pair of consecutive phases contains at least n messages
by construction.
Let Si(t) be the set of nodes that know vi at the beginning of phase t. At the beginning of phase 1,
|Si(1)| = 1, for all 1 ≤ i ≤ n.
In phase t, we call a value as typical if is known by at most 6t log n nodes, i.e., vi is typical if |Si(t)| ≤
6t log n.
We have two claims.
Claim 1: In the beginning of phase t, at least (1/6)tn values are typical w.h.p., for all t ≤ δ log n, for a
small positive constant δ.
The above claim will imply the result since at the end of phase t = δ log n, |Si(t)| ≤ o(n) for at least
nΩ(1) values.
To show the above claim, it is helpful to show the following auxiliary claim. Let Vj(t) be the set of values
that node uj knows at the beginning of phase t. At the beginning of phase 1, |Vj(1)| = 1, for all 1 ≤ j ≤ n.
Let kt be the number of typical values (kt ≥ (1/6)tn by Claim 1). In phase t, we call a node as normal if it
knows at most (kt/n)(6)t log n typical values. We call a value special if it is known by only normal nodes.
Claim 2: In the beginning of phase t, at least 1/3 of the fraction of the typical values are special w.h.p.,
for all t ≤ δ log n, for a small positive constant δ. Furthermore, there are at least γkt nodes are normal w.h.p.
for a some constant γ > 0.
We prove using induction: If show that if the claims hold at the beginning of a phase then they hold at
the end of a phase. We show this regardless whether the phase is dense or sparse and thus we have two cases.
Case 1: The phase is dense. A dense phase consists of only one round with at least n/2 messages.
We first show that if Claim 1 holds at the beginning then it holds at the end of the phase. Fix a typical
value vi. Let Ui(t) = n − Si(t), i.e., the set of nodes that do not know vi at the beginning of phase t.
For 1 ≤ k(i) ≤ |Ui(t)|, let xk(i) denote the indicator random variable that denotes whether the k(i)th of
these nodes gets to know the value vi in this phase. Let Xi(t) =
∑|Ui(t)|
k(i)=1. Let u be a node that does not
13
a node has to send pull messages to satisfy the expansion criterion is at least n ∗ 5 ∗ 6t log n/(kt6t log n) =
5n/kt. Since there at most nmessages sent in this phase, the number of nodes that can satisfy the expansion
criterion is at most (n)kt/5n = kt/5. Since at least γkt nodes are normal, by choosing  small enough, we
satisfy the induction hypothesis.
9 Concluding Remarks
We presented a gossip-based protocol for computing aggregates that takes O(n log logn) messages and
O(log n) rounds. Our protocol is non-address oblivious. We also showed how our protocol can be applied
to arbitrary sparse networks. We also showed that Ω(n log n) messages are needed by any address-oblivious
algorithm regardless of the number of rounds in the random phone call model. An interesting question is to
establish whether Ω(n log log n) messages is a lower bound for gossip-based aggregate computation (in the
non-address oblivious random phone call model) if only O(log n) rounds are allowed.
References
[1] Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah, Randomized gossip algorithms, IEEE/ACM
Trans. on Netw. 14 (2006), no. SI, 2508–2530.
[2] Jen-Yeu Chen, Gopal Pandurangan, and Dongyan Xu, Robust aggregate computation in wireless sensor network:
distributed randomized algorithms and analysis, IEEE Trans. on Paral. and Dist. Sys. (TPDS) 17 (Sep. 2006),
no. 9, 987–1000.
[3] Alan Demers, Dan Greene, Carl Hauser, Wes Irish, John Larson, Scott Shenker, Howard Sturgis, Dan Swinehart,
and Doug Terry, Epidemic algorithms for replicated database maintenance, PODC, 1987, pp. 1–12.
[4] Alexandros G. Dimakis, Anand D. Sarwate, and Martin J. Wainwright, Geographic gossip: efficient aggregation
for sensor networks, IPSN, 2006, pp. 69–76.
[5] Jie Gao, Leonidas Guibas, Nikola Milosavljevic, and John Hershberger, Sparse data aggregation in sensor net-
works, IPSN, 2007, pp. 430–439.
[6] Ma´rk Jelasity, Alberto Montresor, and Ozalp Babaoglu, Gossip-based aggregation in large dynamic networks,
ACM Trans. Comput. Syst. 23 (2005), no. 3, 219–252.
[7] Richard M. Karp, Christian Schindelhauer, Scott Shenker, and Berthold Vo¨cking, Randomized rumor spreading,
FOCS, 2000, pp. 565–574.
[8] Srinivas Kashyap, Supratim Deb, K. V. M. Naidu, Rajeev Rastogi, and Anand Srinivasan, Efficient gossip-based
aggregate computation, PODS, 2006, pp. 308–317.
[9] David Kempe, Alin Dobra, and Johannes Gehrke, Gossip-based computation of aggregate information, FOCS,
2003, pp. 482–491.
[10] Valerie King, Scott Lewis, Jared Saia, and Maxwell Young, Choosing a random peer in chord, Algorithmica 49
(2007), no. 2, 147–169.
[11] Bhaskar Krishnamachari, Deborah Estrin, and Stephen B. Wicker, The impact of data aggregation in wireless
sensor networks, DEBS, 2002, pp. 575–578.
[12] Pradeep Kyasanur, Romit Roy Choudhury, and Indranil Gupta, Smart gossip: An adaptive gossip-based broad-
casting service for sensor networks, MASS, 2006, pp. 91–100.
[13] Samuel Madden, Michael J. Franklin, Joseph M. Hellerstein, and Wei Hong, Tag: a tiny aggregation service for
ad-hoc sensor networks, SIGOPS Oper. Syst. Rev. 36 (2002), no. SI, 131–146.
[14] Michael Mitzenmacher and Eli Upfal, Probability and computing, Cambridge University Press, 2005.
[15] Ruggero Morselli, Bobby Bhattacharjee, Michael A. Marsh, and Aravind Srinivasan, Efficient lookup on unstruc-
tured topologies, PODC, 2005, pp. 77–86.
15
B The Proof of Theorem 4
Proof: As per our failure model, a message may fail to reach the selected root node with probability ρ
(which is at most 2δ, since failure may occur either during the initial call to a non-root node or during the
forwarding call from the non-root node to the root of its tree). For convenience, we call those roots who
know the Max value (the global Maximum) as the max-roots and those who do not as the non-max-roots.
Let Rt be the number of max-roots in round t. Our proof is in two steps. We first show that, whp,
Rt > 4 log n after 8 log n/(1 − ρ) rounds of Gossip-max. If R0 > 4 log n then the task is completed.
Consider the case when R0 < 4 log n. Since the initial number of max-roots is small in this case, the chance
that a max-root selects another max-root is small. Similarly, the chance that two or more max-roots select
the same root is also small. So, in this step, whp a max-root will select a non-max-root to send out its gossip
message. If the gossip message successfully reaches the selected non-max-root, Rt will increase by 1. Let
Xi denote the indicator of the event that a gossip message i from some max-root successfully reaches the
selected non-max-root. We have Pr(Xi = 1) = (1 − ρ). Then X =
∑8 logn/(1−ρ)
i=1 Xi is the minimal
number of max-roots after 8 log n/(1− ρ) rounds. Clearly, E[X] = 8 log n. Here we conservatively assume
the worst situation that initially there is only one max-root and at each round only one max-root selects
a non-max-root. So X is the minimal number of max-roots after 8 log n/(1 − ρ) rounds. For clarity, let
n˜ = 8 log n/(1 − ρ). Define a Doob martingale sequence Z0, Z1, . . . , Zn˜ by setting Z0 = E[X], and, for
1 ≤ i ≤ n˜, Zi = E[X|X1, . . . , Xi]. It is clear that Zn˜ = X and, for 1 ≤ i ≤ n˜, |Zi − Zi−1| ≤ 1.
Applying Azuma’s inequality and setting  = 1/2:
Pr(|X − E[X]| ≥ E[X]) = Pr(|Zn˜ − Z0| ≥ E[X])
≤ 2 exp
(
− 
2E[X]2
2
∑n˜
i=1 12
)
= 2 exp
(
− 
2E[X]2
2(8 logn1−ρ )
)
= 2 exp
(
log n−(1−ρ)
)
= 2 · n−(1−ρ),
where ρ could be arbitrary small. W. l. o. g., let ρ < 1/4, then Pr(|X−E[X]| ≥ E[X]) ≤ 2 ·n− 34 . Hence,
whp, after 8 log n/(1− ρ) = O(log n) rounds, Rt ≥ R0 +X > R0 + 12E[X] = R0 + 4 log n > 4 log n.
In the second step of our proof, we lower bound the increasing rate of Rt when Rt > 4 log n. In each
round, there are Rt messages sent out from max-roots. Let Yi denote the indicator of an event that such a
message i from a max-root successfully reaches a non-max-root. Yi = 0 when either of the following events
happen: (1) The message i fails in routing to its destination; (2) The message i is sent to another max-root,
although it successfully travels over the network. The probability of this event is at most (1−ρ)Rt lognn since
whp the size of a ranking tree isO(log n) (cf.Theorem 2). (3) The message i and at least one another message
are destined to the same non-max-root. As the probability of three or more messages are destined to a same
node is very small, we only consider the case that two messages select the same non-max-root. We also
conservatively exclude both these two messages on their possible contributions to the increase of Rt. This
event happens with probability at most (1−ρ)Rt lognn .
Applying union bound [14],
Pr(Yi = 0) ≤ ρ+ 2(1− ρ)Rt log n
n
.
Since Rt ≤ cnlogn for any constant 0 < c < 1 (otherwise, the task is completed),
Pr(Yi = 0) ≤ ρ+ 2c(1− ρ) = c′ + (1− c′)ρ,
where c′ = 2c < 1 is a constant that is suitably fixed so that c′ + (1 − c′)ρ < 1. Consequently, we have
Pr(Yi = 1) > (1− c′)(1− ρ), and E[Y ] =
∑Rt
i=1E[Yi] > (1− c′)(1− ρ)Rt. Applying Azuma’s inequality
17
root and
∑
i∈V˜ P
2
i is the probability that two roots select the same root. The conditional expectation of
potential at round t+ 1 is
E[Φt+1|Φt = φ]
=
1
2
φ+
1
2
∑
i,j,k
(
yi,j − wi
m
)(
yk,j − wk
m
)
Pi
+
1
2
∑
j,k
∑
k′ 6=k
(
yk,j − wk
m
)(
yk′,j − wk
′
m
)∑
i∈V˜
P 2i
=
1
2
φ+
1
2
∑
i,j,k
(
yi,j − wi
m
)(
yk,j − wk
m
)
Pi
+
∑
i∈V˜ P
2
i
2
∑
k,j,k′
(
yk,j − wk
m
)(
yk′,j − wk
′
m
)
−
∑
i∈V˜ P
2
i
2
∑
k,j
(
yk,j − wk
m
)2
=
1
2
(1−
∑
i∈V˜
P 2i )φ
+
1
2
∑
i,j
(Pi +
∑
i∈V˜
P 2i )
(
yi,j − wi
m
)∑
k
(
yk,j − wk
m
)
=
1
2
(1−
∑
i∈V˜
P 2i )φ <
1
2
φ.
The last equality follows from the fact that∑
k
(
yk,j − wk
m
)
=
∑
k
yk,j −
∑
k
wk
m
= 1− 1 = 0.
Lemma 14 There exists a τ = O(logm) such that after ∀t > τ rounds of Gossip-ave, wt,z ≥ 2−τ at z, the
root of the largest tree.
Proof: In the case that the selection probability is uniform, it has been shown in [9] that on an m-clique,
with probability at least 1− δ′2 , after 4 logm+log 2δ′ rounds, a message originating from any node (through a
random walk on the clique) would have visited all nodes of the clique. When the distribution of the selection
probability is not uniform, it is clear that a message originating from any node must have visited the node
with the highest selection probability after a certain number of rounds that is greater than 4 logm + log 2δ′
with probability at least 1− δ′2 .
From the previous two lemmas, we derive the following theorem.
Theorem 15 (Diffusion speed of Gossip-ave) With probability at least 1 − δ′, there exists a time Tave =
O(logm + log 1 + log
1
δ′ ), such that ∀t ≥ Tave, the contributions at z, root of the largest tree, is nearly
uniform, i.e., ‖ yt,z,y‖yt,z‖1 − 1m‖∞ ≤ .
Proof: By Lemma 13, we obtain that E[Φt] < (m− 1)2−t < m2−t, as Φ0 = (m− 1). By Lemma 14,
we set τ = 4 logm + log 2δ′ and ˆ
2 = 2 · δ′2 · 2−2τ . Then after t = logm + log 1ˆ rounds of Gossip-ave,
19
D Convergecast algorithms
Algorithm 7: covmax =Convergecast-max(F,v)
Input: The forest F, and the value vector v over all nodes in F
Output: The local Max aggregate vector covmax over all the roots
foreach leaf node do send its value to its parent;
foreach intermediate node do
- collect values from its children;
- compare collected values with its own value;
- update its value to the maximum among all and send the maximum to its parent.
end
foreach root node z do
- collect values from its children;
- compare collected values with its own value;
- update its value to the local maximum value covmax(z).
end
Algorithm 8: covsum =Convergecast-sum(F,v)
Input: The forest F and the value vector v over all nodes in F
Output: The local Ave aggregate vector covmax over all the roots.
Initialization: every node i stores a row vector (vi, wi = 1) including its value vi and a size count wi
foreach leaf node i ∈ F do
— send its parent a message containing the vector (vi, wi = 1)
— reset (vi, wi) = (0, 0).
end
foreach intermediate node j ∈ F do
— collect messages (vectors) from its children
— compute and update vj = vj +
∑
k∈Child(j) vk, and wj = wj +
∑
k∈Child(j)wk, where
Child(j) = {j’s children nodes}
— send computed (vj , wj) to its parent
— reset its vector (vj , wj) = (0, 0) when its parent successfully receives its message.
end
foreach root node z ∈ V˜ do
— collect messages (vectors) from its children
— compute the local sum aggregate covsum(z, 1) = vz +
∑
k∈Child(z) vk, and the size count of
the tree covsum(z, 2) = wz +
∑
k∈Child(z)wk, where Child(z) = {z’s children nodes}.
end
21
