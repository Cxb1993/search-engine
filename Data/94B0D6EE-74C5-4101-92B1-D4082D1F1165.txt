 2 
Abstract 
 
With the flourishing of multimedia and internet, people can communicate with each 
other by sending images and sound, and can receive information form the others. 
How-ever, according to the internet quality varia-tion and the instability of internet 
bandwidth, how to maintain the quality of the 2D video encode images and to conceal 
the errors is still a very important task, and many re-searches have been involved in 
this field. 
 
In previous projects, we have established an integrated architecture by taking 
advantage of 2D video coding and 3D model-based coding to create video realistic 
avatars for a virtual confer-encing system, where the FAP parameters from the 
analyzed facial image and the dif-ference image between the synthesized and the real 
one are transmitted as base layer and enhancement layer, respectively. The data 
robustness was also increased with the error concealment and resilience techniques in 
the base layer of FAPs transmission. 
 
However, under a very-low bit rate band-width condition, the 2D difference image 
transmitted as an enhancement layer might be lost or not receivable. In this 3-year 
project, we will develop a new error concealment algorithm for the enhancement layer, 
which is expected to be more suitable in the video conferencing system. With the 
proposed algorithm, we can alleviate the defect caused by the lost of the enhancement 
layer and reconstruct a vivid 3D virtual face model and conceal the error with a proper 
method. This project includes: facial parameter transferring in wireless, error 
concealment for facial parameters, and audio-to-visual transform. 
 
 
Keywords: Error concealment, Error resil-ience, FAPs, 2D image-based coding, 3D 
model-based coding, Virtual conferencing system 
 
 4 
前言 
 
在近年來網路的蓬勃發展之下，幾乎所有資訊的傳遞都已數位化，人與人倚賴各
種網路溝通的比重也越來越高。但是在網路資料量急遽增加的同時，網路的頻寬
並沒有隨之增加到我們可以任意使用的地步，更突顯了資料壓縮的重要性。 
 
MPEG-4 標準於 1999 年年低被制訂後，其中提出的自然影像與合成影像的複合
編解碼功能，囊括了二維影像以及包含人體模型在內的三維物件的結合，定義了
臉部動畫參數，明顯的降低了在傳遞人臉影像時所需要的頻寬要求。而藉由在網
路上所建構用以代表使用者的虛擬人，亦提供了在網路上做低位元率的虛擬會議
等多媒體通訊應用的可能性。 
 
但是網路品質的不可預期，往往會造成資料接收端的錯誤，在資訊沒有壓縮之
前，錯誤的影響的範圖是很小的，但是隨著資料壓縮技術的使用，例如可變長度
編碼(Variable Length Coding)，錯誤往往會傳遞擴大，而讓使用者無法重建原來
的資訊。因此有許多關於錯誤忍容與錯誤隱藏的技術，一直在傳統的影像編碼上
被研究和探討，但是對於 MPEG-4 的臉部動畫參數方面卻鮮有所聞。 
 
因此在本計劃中，我們希望可以建立一個同時保有二維影像編碼以及三維物件模
型編碼優點的系統架構，在更低的頻寬要求下，提供更加逼真的呈現。並且整合
傳統影像編碼中錯誤容忍及錯誤隱藏工具，使其更適合於我們所提出的系統架
構，讓我們不僅可以提供一個超低位元率的編解碼系統，而且還能強化資料的強
健性。 
 
 
 6 
超低位元率傳輸下，當作加強層的二維影像資料發生遺失時，仍可利用當作基本
層傳送的臉部動畫參數讓虛擬代理人達到應有的生動表情。 
 
第三年 
在本計畫中，我們希望能使用多重資訊的輸入－影像及聲音－來驅動三維的人臉
模型。之前，我們已有發展出利用聲音的資訊加以轉換計算之後，便可以得到相
對應的嘴型動作。因此，如果臉部的 FAP 資訊遺失之後，我們也可以利用接收
到的聲音資訊，推導相對應的 FAP，進而利用第二年所發展出來的技術，估測大
概的影像。 
 
然而，每個人的聲音資訊有所不同，聲音與 FAP 的對應關係也有所差異，所以，
我們也體認到當系統將用於非特定使用者時，所發展之演算法必須能有學習與調
適之機制處理單使用者與多使用者之間表情變化之差異。因此，在第三年的計劃
中，我們便著眼於聲音與影像關係的調適上，讓即使是不同使用者的聲音，都可
以對應到正確的動作。 
 
 8 
型，因此英國劍橋大學的 P. C. Woodland 等學者提出 regression based model 
prediction (RMP)[8]以及 maximum likelihood linear regression (MLLR)[9]，藉由調
整模型之參數使得辨識率得以提升。同樣也是劍橋大學的 M. J. F. Gales 則假設
所有使用者的語音可以由少數的幾個群組模型來表示，因此提出 cluster adaptive 
training 的作法，對於每個使用者只要找出其語音特徵在各群組的分量即可達成
語者調適之目的。此外，另一作法則是藉由語音特徵的正規化(normalization)，
達成與語者無關 (speaker independent)之語音辨識。 
 
 
在目前執行的計畫中，我們已發展出人物偵測[23]、人臉模型建構[24]、頭部面
向估測[25]、人臉表情分析[26]以及音訊─視訊轉換[27]等演算法，建立了臉部表
情分析與合成之實驗平台，並正在執行細微臉部表情分析與高階表情辨識之實
驗。預期能順利於規劃之時程內完成所有實驗，並配合其他子計畫進行系統整合。 
 10
+ +
-
+ DCT Q
Q-1
DCT-1
Proj-1
TFAP-1
Visual
Analysis
TFAP
Proj
VLC
FAP
Encoder
Video In
TM Buf
Video
Bitstream
FAP
Bitstream
Q:  Quantization
Proj:  Projection from 3-D space
          to 2-D image
TFAP:  FAP TransformVLC:  Variable Length Coding
VLD:  Variable Length Decoding
DCT:  Discrete Cosine Transform
TM Buf:  Texture map buffer of
               the facial model
 
圖一、編碼器架構圖 
 
解碼器的架構圖則如圖二所示。相對應於編碼器的設計，解碼器則是將編碼器所
傳送過來的資料，做一個反向的動作，以還原影像。 
 
+
+
Video
Bitstream
FAP
Bitstream
FAP
Decoder
VLD Q-1 DCT-1 Proj-1 TFAP-1 TM Buf
TFAPProj
3-D
Synthesis
Realistic
Avatar
 
圖二、解碼器架構圖 
 
另一個重點是放在錯誤隱藏的工作上。必需先判斷出是否有錯誤發生，可能的作
法可以偵測解碼之後所得的臉部動畫參數值是不是在一個可以接受的範圍之
內？或是判斷人臉兩邊的對稱性是不是合理等等？當發現有錯誤發生時，我們可
以考慮前後的畫面，利用內插法，或者是依照人臉對稱的特性來調整得到的結果。 
 
對於臉部動畫參數的錯誤容忍度以及錯誤隱藏的技術，我們將臉部的參數分為位
元遮罩(bitmask)以及對應的數值(faps value)[28]，如此便可達到最有效率傳輸，
系統架構可參考圖三。 
 
在提高臉部參數錯誤容忍度時，與一般的保護方法存在極大的差異，對於如此低
位元的傳輸模式下，只要存在少許的錯誤，便使得參數無法回復，若考慮叢集錯
誤(burst error)或是封包遺失(packet loss)，一般的保護方法是無法回復參數。所以
必須先大輻的提高錯誤容忍度，然後加強錯誤隱藏，才能達到較為擬真的效果。 
 
 12
臉部參數之錯誤隱藏方法 
 
本系統架構之編碼器如圖一所示，我們首先分析目前輸入影像的臉部動作，並轉
換為臉部動畫參數，並套用到貼上紋理圖之三維虛擬人像上，作相對應之形變，
得到目前表情變化的人像，並經由投影後便可以得到合成的影像。將真實影像和
合成影像之間相減可得到傳送之差異值，並且為了模擬解碼器端的動作，並利用
此差異值用來更新暫存的紋理圖。 
 
 我們所希望建立的正是臉部動畫參數與用來更新之紋理差異值之間的關係
[32]，由此兩者之間的關係，我們將可以在解碼器端加入一錯誤隱藏的機制，當
解碼器端收到的錯誤資料或是收不到資料時，可以修正或填補此份資料，讓輸出
的三維虛擬人像仍能有著正確及生動的表情。加入此機制的解碼器如圖二。 
 
 建立臉部動畫參數與紋理差值之間的關係，主要可分為兩部份：（1）尋找表
情變化的紋理差異值，（2）建立臉部動畫參數與紋理差異值的轉換，以下分項述
之： 
（1）尋找表情變化的紋理差異值 
  
首先我們找出真實影像的人臉部份，並經過表情分析找出此表情的臉部動畫參
數，並套用到貼上原始紋理圖的三維虛擬人物，做相對應的形變，投影後可得到
合成的影像，真實影像與合成影像人臉部份相減的結果便是產生此表情在影像上
二維的紋理差異值。 
 
  
(a) (b) (c) 
圖四、（a）輸入影像，（b）三維模型編碼合成影像， 
（c）輸入影像與合成影像混合之影像 
 
另外，我們定義了定義三維模型編碼所無法表現出人臉細緻表情變化的區域（圖
五），也可說是在臉部動畫參數的形變後，使臉部發生不夠真實的區域，如嘴巴、
眉毛及額頭、眼睛和臉頰，因此我們可將原本整個臉部的紋理差異值分成幾個部
份的紋理差異值，並且根據臉部動畫參數的分組特性，每個部份的紋理差異值都
會跟著影響其區域的臉部動畫參數的組別，如嘴巴的臉部動畫參數影響了嘴巴部
份的部份紋理差異值。 
 
 14
時猜測的紋理圖，投影後便可得到此時的臉部表情影像。 
 
 16
[ ] ( )( )
( )
( ) ( )
1, ; ,,|
; ,
av
va aaa a
aa a
Nf
E d df N
−
= = = + −∫ ∫ a v μ Sa vv a v v v v μ S S a μa a μ S 。 
 
相同的，對一個高斯混合模型而言，a 的 marginal probability function 可以
由 
( ) ( ) ( ),,
1 1
, ; , ; ,
m m
i ia i i i i
i i
f N d Nπ π
= =
= =∑ ∑∫ aaa a v μ S v a μ S . 
得到。 
而在給定 a 時，v 的條件期望值則可以推導自 
[ ] ( )( )
( )
( ) ( )( ), 1, , ,, ,1
; ,,|
m aa ii a iav
va i aa iv i a i
ia a
Nf
E df f
π
−
=
⎛ ⎞⎜ ⎟= = + −⎜ ⎟⎝ ⎠
∑∫ a μ Sa vv a v v μ S S a μa a  
而這也是對高斯混合模型的最佳估測值。 
 
 
z 可調式機制之建立 
 
在調適與學習的機制上，我們擬採取廣泛用於語者調適的最大概似度線性迴
歸法(Maximum Likelihood Linear Regression, MLLR)來進行。在此作法中，
假設多使用者之間語音模型的差異可以線性轉換的關係加以近似，而其目的
則是使用線性迴歸的方式來調整高斯混合模型(Gaussian Mixture Model)中
各個高斯核心分布的中心向量(mean vector, iμ )，使得這個混合模型套用到
新的使用者資料時，其可能機率(likelihood)最大化。也就是說，希望找到一
個線性轉換矩陣(W )，計算新的中心向量( 'i
i
1⎡ ⎤
= ⎢ ⎥⎣ ⎦
μ W μ )，使得 
( ) ( )| , ', | , ,i ii i i iP Pπ π≥O μ S O μ S 。 
 
而其主要的方法則是定義了一個輔助的方程式 Q， 
( ) ( ) ( ), ' | log | 'Q P pλ λ λ λ= O O ， 
 
利用其一階微分為零的條件，便可以求得轉換矩陣(W )的最佳解。因此只需
新使用者少量的訓練資料即可找出線性轉換之參數，藉此調整原先系統中經
過原有使用者大量訓練資料所得之模型。 
 
然而，使用 MLLR 的方式會牽動到高斯分布中心向量( T⎡ ⎤= ⎣ ⎦T Ti a,i v,iμ μ μ )中
所有的數值，這樣子的表現並不符合我們應用中的需求。在我們的應用當
中，因為我們所能取得的語者調適資料僅限於聲音的部份，所以我們要修正
的也只有音訊部分的(
,a iμ )，而視訊部分的資訊( ,v iμ )則維持不變。 
 
 18
 
圖八、部份線線迴歸法的示意圖 
 
和圖七相較，我們知道如果一個音訊向量 (a)可以經過 Audioto-Visual 
Transform 而推導到其相對應的視訊向量(v)，則相反地，我們則可以使用一
個 Visual-to-Audio 的反轉換，從視訊向量(v)反推回其相對應的音訊向量(a)。 
 
而這也是部份線性迴歸的主要概念(如圖八圖所示)：希望可以用線性迴歸的
方式來調整高斯混合模型中的音訊中心向量(audio mean vectors, 
,a iμ )，使得
視訊向量(v)經過 Visual-to-Audio Transform 之後所得的音訊向量(a’)可以和
新的使用者所發出的音訊向量(a’)越接近越好。為了要能達到這樣子的效
果，新的使用者則必需先發出我們所要求的聲音，以做為調適的語料。 
 
假設我們現在擁有 J 筆的調適資料 a’j, j=1,2,…J.。則我們最後的目標是能夠
使得以下的式子最小化： 
1 1
|
J J
jj j j
j j
minimize minimize E
= =
⎡ ⎤− = − ⎣ ⎦∑ ∑a' a' a' a v
， 
 
也就是希可以找出一個轉換矩陣 A  和向量 b，最小化 
( )
( ) ( )( )
, 1,
, ,
, ,
1 1
; ,J m vv ii j v i
av i vv ij a i j v i
j i v j
N
f
π
−
= =
⎡ ⎤⎢ ⎥
− × + + −⎢ ⎥⎣ ⎦
∑ ∑ v μ Sa' Aμ b S S v μ
v
， 
其中 
( ) ( ) ( ), ,, ,
1 1
; , ; ,
M M
vv i vv iv j i j v i i j v i
i i
f N , d Nπ π
= =
= =∑ ∑∫v a v μ S a v μ S 。 
 
 20
研究結果 
 
無線網路中的臉部參數傳輸 
 
我們使用 NTT DoCoMo Error Pat-tern[29]進行測試，在 1kbps、BER = 0.01 以及
burst length =10ms 的情況下進行模擬，可以參考圖九及圖十，位元遮罩發生錯誤
的影響及保護後的結果，圖十表示完全的回復。 
 
 
 
圖九、位元遮罩發生錯誤的影響 
 
 
圖十、位元遮罩經過保護達到好的效果 
 
在保護相對應的數值，也在相同的環境之下進行模擬，由圖十一、圖十二、及圖
十三觀察出結果，圖十一是沒有經過任何的保護碼保護及時間、空間的動作限制;
圖十二是只經過保護碼保護，但對動作不加以限制，圖十三是加入保護後再加入
 22
 
 
 
 
 24
0
10
20
30
40
50
60
70
80
90
0 50 100 150 200 250 300 350 400 450
Select Group Number
Av
era
ge
 D
ist
ort
ion
 
十四、各個定義區域中選擇的分類數目和平均誤差的關係圖(續) 
 
0
10
20
30
40
50
0 20 40 60 80 100
Frame Number
PS
NR
 
圖十五、正常傳輸與發生錯誤漫延之比較 
 
0
10
20
30
40
50
0 20 40 60 80 100
Frame Number
PS
NR
 
圖十六、正常傳輸與利用臉部動畫參數推測的合成影像之比較 
 
圖十五表示在正常傳輸下，臉部動畫參數與影像差異值均收到的 PSNR 表現，以
及當影像差異值發生遺失造成錯誤漫延的結果。 
 
我們在嘴巴、眼睛、眉毛與額頭及臉頰部份分別選擇 29、16、11、11 個分類，
 26
利用聲音驅動臉部參數之研究 
 
我們利用紅外線攝影機，在使用者的臉上貼上特製的點之後，便可以每秒 120 張
的頻率取得特徵點的精確移動量值。並以麥克風取得 8kHz、16-bit 的單聲道語音
資訊。 
 
使用者分別錄制了 413個中文的字音(表一)做為實驗內容。聲音的資訊是以每 240
取樣資料來計算 10 維的 LSP 參數，影像則是取嘴部 8 個點的垂直及水平的運動
數值(共 16 維)，經過同步之後，我們對每個使用者會得到 19315 個聲音-視訊向
量。 
 
在實驗當中，我們使用的是 10 個核心的高斯混合模型。 
共有四類的模型： 
z 對每個使用者，分別利用自身的聲音和相同的動作參數，建立了使用者相依
的聲音到嘴部動作對應的模型(Modeli, i=1,2,3) 。 
z 利用每個使用者三分之一的資料，共同訓練建立使用者獨立的聲音到嘴部動
作對應的模型(Modelall)。 
z 利用我們所提出來的部份線性回歸(PLR)的方法，利用之前沒有用在建立
Modelall 以外的資料，調適出更適合各個使用者的高斯混合模型 
Modelall-i,i=1,2,3 。 
z 利用我們所提出來的部份線性回歸(PLR)的方法，直接由不包函使用者資料
的模型，來調適出更合適的混合模型 Modeli-j, i,j = 1,2,3, i ≠ j 
 
在驗証的步驟當中，我們則是使用 Audio-to-Visual Conversion 來計算使用者發出
某種聲音時，其所可能相對應的嘴型，並和實際錄得的數值作比較，在將嘴寬正
規化到 100 個畫素之後，其差異的平均值和標準差則分別列於表二和表三。由表
二我們可以看到，對第 i 個使用者而言， 
, 1, 2,3i all i allModel Model Model i−> > = ， 
也就是說使用者相依的模型會比使用者獨立的模型表現來得好，而由使用者獨立
模型調適而得的模型，也會比使用者獨立的模型好。另外由表三也可以驗証，由
某個使用者相依的模型，調適到另一個使用者之後的效果，也會比直接使用使用
者獨立的模型來得好。 
 
 
 28
 
表二、利用 GMM 推得的動作和原始資料的差值平均及標準差(I) 
Mean (Std.) 
GMM 
User 1 User 2 User 3 
Modelall 4.45(3.59) 4.57(3.61) 4.60(3.49) 
Modelall-1 3.72(3.57) 4.51(3.88) 4.55(4.14) 
Modelall-2 4.73(4.16) 4.22(3.97) 4.56(3.98) 
Modelall-3 4.41(4.04) 4.44(3.99) 4.21(3.85) 
Model1 2.95 (2.86) 4.28 (3.55) 4.34 (3.75) 
Model2 4.39 (3.59) 3.42 (3.21) 4.56 (3.91) 
Model3 4.67 (4.22) 4.35 (3.71) 3.30 (3.02) 
 
表三、利用 GMM 推得的動作和原始資料的差值平均及標準差(II) 
Mean (Std.) 
GMM 
User 1 User 2 User 3 
Modelall 4.45(3.59) 4.57(3.61) 4.60(3.49) 
Model1 2.95 (2.86) 4.28 (3.55) 4.34 (3.75) 
Model1-2 4.19(3.72) 4.18 (3.80) 4.20(3.49) 
Model1-3 3.92(3.62) 4.39(3.77) 3.94(3.47) 
Model2 4.39 (3.59) 3.42 (3.21) 4.56 (3.91) 
Model2-1 3.94(3.68) 4.71(4.36) 4.71(4.22) 
Model2-3 4.45(4.17) 4.85(4.55) 4.33(3.91) 
Model3 4.67 (4.22) 4.35 (3.71) 3.30 (3.02) 
Model3-1 4.02(3.77) 4.87(4.02) 4.83(4.34) 
Model3-2 4.96(4.46) 4.50(4.09) 4.57(3.93) 
 
 
 
 30
參考文獻 
 
[1]  K. Aizawa, H. Harashima and T. Saito, “Model-based analysis synthesis 
image coding (MBASIC) system for a person’s face,” Signal Processing: 
Image Communication, Vol. 1, pp. 587-590, Oct. 1995. 
[2]  Y. L. Tian, T Kanade, and J. F. Cohn, “Recognizing Action Units for Facial 
Expression Analysis,” IEEE Trans. Pattern Analysis and Machine 
Intelligence, Vol. 23, No. 2, Feb. 2001. 
[3]  G. Donato, M. S. Bartlett, J. C. Hager, P. Ekman, and T. J. Sejnowski, 
“Classifying Facial Actions,” IEEE Transactions on Pattern Analysis and 
Machine Intelligence, Vol. 21, No. 10, Oct. 1999. 
[4]  I. Essa and A. Pentland, “Coding, Analysis, Interpretation and Recognition 
of Facial Expressions," IEEE Transactions on Pattern Analysis and Machine 
Intelligence, Vol. 19, No. 7, July, 1997. 
[5]  P. Eisert, T. Wiegand, and B. Girod, “Model-Aided Coding: A New 
Approach to Incorporate Facial Animation into Motion-Compensated Video 
Coding,” IEEE Transactions on Circuits and Systems for Video Technology, 
Vol. 10, No. 3, pp. 344-358, April 2000. 
[6]  Chung J. Kuo and Meng H. Tsai, “Facial Expressions Synthesizing System 
Using Elastic Body Spline Transform,” Proceedings of 2000 Workshop on 
Consumer Electronics, pp.395-400, National Taiwan University, Taipei, 
Taiwan, Oct. 19-20, 2000. 
[7]  Chia-Yang Huang and Wen-Hsiang Tsai, “Knowledge-Based Tracking and 
Modeling of Facial Expressions by Stereo Vision Techniques,” Proceedings 
of the 13th IPPR Conference on Computer Vision, Graphics, and Image 
Processing, Vol. 1, pp. 286-298, Taipei County, Taiwan, Aug. 2000. 
[8]  S. M. Ahadi and P. C. Woodland, “Combined Bayesian and Predictive 
Techniques for Rapid Speaker Adaptation of Continuous Density Hidden 
Markov Models,” Computer Speech and Language, Vol. 11, pp. 187-206, 
1997. 
[9]  C. J. Leggetter and p. C. Woodland, “Maximum Likelihood Linear 
Regression for Speaker Adaptation of Continuous Density Hidden Markov 
Models,” Computer Speech and Language, Vol. 9, pp. 171-185, 1995. 
[10]  Redmill, D.W., and Kingsbury, N.G., “The EREC: an error-resilient 
 32
bit-rate mobile video,” IEE 3rd Int. Conf. 3G Mobile Comm. Techn. (3G 
2002), London, UK, 8-10 May 2002, pp. 371-375. 
[22]  Yao Wang, Stephan Wenger, Jiangtao Wen, and Aggelos K. Katsaggelos, 
"Error Resilient Video Coding Techniques, Real-Time Video 
Communications over Unreliable Networks," IEEE Signal Processing 
Magazine, July 2000, P61-82 
[23]  Chih-Ming Wang, Yao-Jen Chang, and Yung-Chang Chen, "Moving Object 
Extraction using Mosaic Technique and Tracking with Active Camera," 
Proceedings of the 14th IPPR Conference on Computer Vision, Graphics 
and Image Processing, Kenting, Taiwan, Aug. 19-21, 2001. 
[24]  Yao-Jen Chang and Yung-Chang Chen, “Textured Polygonal Model Assisted 
Facial Model Estimation from Image Sequence,” Proceedings of 2001 IEEE 
International Conference on Image Processing (ICIP-2001), Vol. 3, pp. 
106-109, Thessaloniki, Greece, Oct. 7-10, 2001. 
[25]  Yao-Jen Chang and Yung-Chang Chen, “Robust Head Pose Estimation 
Using Textured Polygonal Model with Local Correlation Measure,” 
Proceedings of the Second IEEE Pacific-Rim Conference on Multimedia 
(IEEE-PCM2001), pp. 245-252, Beijing, China, Oct. 24-26, 2001. 
[26]  Jen-Chung Chou, Yao-Jen Chang, and Yung-Chang Chen, “Facial Feature 
Point Tracking and Expression Analysis for Virtual Conferencing Systems,” 
Proceedings of 2001 IEEE International Conference on Multimedia and 
Expo (ICME2001), pp.415-418, Tokyo Japan, Aug. 22-25, 2001. 
[27]  Ju-Yu Yu, Yao-Jen Chang, and Yung-Chang Chen, “Frame-based Audio to 
Visual Conversion using Line Spectrum Pairs,” Proceedings of 2001 
Workshop on Consumer Electronics, Taipei, Taiwan, Oct. 17-19, 2000. 
[28]  Murat Tekalp and Jörn Ostermann, “MPEG4 FACIAL ANIMATION 
PROJECT”, 
http://www.cs.technion.ac.il/~gip/projects/s2002/Dmitry_Vadim_MPEG4/in
dex.htm . 
[29]  Toshio Miki, Toshiro Kawahara and Tomoyuki Ohya, “Revised Error 
Pattern Generation Programs for Core Experiments on Error Resilience ”, 
ISO/IEC JTC1/SC29/WG11 MPEG96/1492, 1996 
[30]  Di Giacomo,T ; Joslin, C.;Garchery, S ; Magnenat-Thalmann,N. 
“Ad-aptation of facial and body anima-tion for MPEG-based architec-tures”, 
Proceedings, 2003 Inter-mational Conference on, 2003, 21-228 ., 
