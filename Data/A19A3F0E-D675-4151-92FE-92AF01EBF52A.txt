Synthesis using Anechoic Chamber Recording.  
The mainstream view synthesis (VS) system uses image 
camera array to acquire multiple images and to 
synthesize a virtual images. Yet the ’passive depth 
estimation’ method using RGB images only is 
deficient in handling certain scene, such as texture-
less area. The depth sensor of Kinect fails to 
provide accurate depth map of slick surface, radiant 
object, and sharp object boundary. To improve the 
quality of synthesized image, we propose a depth 
refinement algorithm on processing the depth 
information. Experiment results show that synthesized 
images using our refined depth maps are noticeably 
improved in visual quality.   
The current focus of MPEG 3DVC project is on the 
parallel and dense camera array system. The distance 
between two nearby cameras is about of less than 10 
cm. In contrast, our focus in this study is on the 
so-called sparse multi camera systems, of which the 
cameras are located farther away. Our target is to 
synthesize a virtual view based on the recorded 
sparse camera pictures. We propose an instrumental 
backward warping algorithm on the depth map. It is 
able to reduce most artifacts in depth warping due to 
the more accurate geometric relations. 
 In the multi-channel sound system, the key 
technology is 3D acoustic signal synthesis at any 
virtual listening point. In this report, we 
demonstrate how to design and implement a virtual 
listening point audio system by constructing a 
physical testing environment in an anechoic chamber. 
Several techniques were employed in implementing this 
system. They are blind source separation (BSS), 
direction of arrival (DOA) estimation and denoising 
filtering. The final outcome is constructing an audio 
signal at the desired virtual listening position, 
which is called Virtual Listening Point Audio 
Synthesis. 
 
英文關鍵詞： DIBR, Kinect, Passive depth estimation, 3DVC, Forward 
depth warping, Occlusion, View synthesis, Blind 
1 
 
行政院國家科學委員會補助專題研究計畫■ 成 果 報 告   □期中進度報告 
 
自由點聲訊合成與多視點視訊表述技術研究(3/3) 
Free Viewpoint Audio Synthesis and Multi-view Video Representation (3/3) 
 
計畫類別：■個別型計畫  □整合型計畫 
計畫編號： NSC 98-2221-E-009-087 -MY3 
 
執行期間： 100 年 8 月 1 日至 101 年 7 月 31 日 
 
計畫主持人：杭學鳴 國立交通大學電子工程學系教授 
計畫參與人員：李讀修、張鈞凱、蔡長廷、吳崇豪、陳俊吉、詹家欣 
交通大學電子工程學系研究生 
 
 
成果報告類型(依經費核定清單規定繳交)：■精簡報告  □完整報告 
 
本成果報告包括以下應繳交之附件： 
■出席國際學術會議心得報告及發表之論文各 2 份 
 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
           
          
執行單位：國立交通大學電子工程學系 
 
中 華 民 國 101 年 10 月 15 日 
 
 
3 
 
 英文摘要 
This project, Multi-view Video/Audio Representation and Synthesis, investigates the 
topics of multi-view audio-visual data representation and synthesis and their related topics. 
The entire duration of this project is 3 years. This is the third-year report which concludes 
three individual parts in this report: (1) Depth Map Acquired by Depth Sensor and Color 
Camera, (2) View Synthesis Using Backward Warping, and (3) Virtual Listening Point Audio 
Synthesis using Anechoic Chamber Recording.  
The mainstream view synthesis (VS) system uses image camera array to acquire multiple 
images and to synthesize a virtual images. Yet the “passive depth estimation” method using 
RGB images only is deficient in handling certain scene, such as texture-less area. The depth 
sensor of Kinect fails to provide accurate depth map of slick surface, radiant object, and sharp 
object boundary. To improve the quality of synthesized image, we propose a depth refinement 
algorithm on processing the depth information. Experiment results show that synthesized 
images using our refined depth maps are noticeably improved in visual quality.   
The current focus of MPEG 3DVC project is on the parallel and dense camera array 
system. The distance between two nearby cameras is about of less than 10 cm. In contrast, our 
focus in this study is on the so-called sparse multi camera systems, of which the cameras are 
located farther away. Our target is to synthesize a virtual view based on the recorded sparse 
camera pictures. We propose an instrumental backward warping algorithm on the depth map. 
It is able to reduce most artifacts in depth warping due to the more accurate geometric 
relations. 
 Thirdly, the multi-channel audio system is an indispensable part of a multi-view 
multimedia system. In the multi-channel sound system, the key technology is 3D acoustic 
signal synthesis at any virtual listening point. In this report, we demonstrated how to design 
and implement a virtual listening point audio system by constructing a physical testing 
environment in an anechoic chamber. Several techniques were employed in implementing this 
system. They are blind source separation (BSS), direction of arrival (DOA) estimation and 
denoising filtering. The final outcome is constructing an audio signal at the desired virtual 
listening position, which is called Virtual Listening Point Audio Synthesis. In the Free Field 
Acoustic Room Chamber, each speaker represents a sound source and a microphone array 
records the received signals. 
 
Keywords:  
DIBR, Kinect, Passive depth estimation, 3DVC, Forward depth warping, Occlusion, View 
synthesis, Blind Source Separation, Direction of Arrival, SLAB, 3D audio signal synthesis 
 
 
5 
 
D.  研究方法 
     
圖 1 演算法流程圖 
 對於整個演算法可用圖 1 的流程圖來做說明。一開始從左右的 Kinect 取得各自的
景深影像以及彩色影像後，我們可以對於 Kinect 提出 4 個問題。 
 
1. 兩台 Kinect 之間的關係連結: 兩台感應器分別放在兩端拍攝場景，所以兩者拍
攝出的影像必存在某種幾何關係，且兩台感應器位於不同位置，接收到的光源
特性必定有差異，則拍攝出來的影像色彩必定有差異。 
2. Kinect 上彩色相機與景深相機的校準(Alignment)問題: 對於一台 Kinect 感應器
上，彩色相機與景深相機位於不同位置上，但我們目的是得到相同視角的彩色
照片與景深照片，所以必須考慮到校準問題。 
3. Kinect 無法偵測的破洞(Hole)區域: 對於 Kinect 感應器無法計算出合理景深資
訊的部分，會產生所謂「洞」的存在，且這些破洞也可稱為缺陷(Defect)。 
4. Kinect 偵測錯誤與雜訊問題: 部分區域會因為 Kinect 內部晶片計算錯誤，導致
相同物體但景深不一致;並且在物體的邊緣也被雜訊影響，導致該地方凹凸不
平。 
 
問題 1 在「校正處理」(Calibration Processing)利用左右的彩色影像，經過色彩校正、
相機校正、影像校正等處理後，除了可以獲得兩台 Kinect 的 3D 幾何關係，並可調整兩
台 Kinect 影像上的色彩差異。問題 2 在 Kinect 的感應器位置架構上是顯而易見的問題，
但又不易解決，在「校準處理」(Alignment Processing)的過程，可以解決彩色影像與景
深影像位於不同視角的狀況。問題 3 與問題 4 透過「景深影像改善」(Depth Image 
Refinement)的處理，可以做到消除雜訊、補洞、修補邊界的效果，改善景深影像的品質，
這部分可以在最後的合成影像結果看到明顯的差異。 
7 
 
整張景深影像投影到彩色相機的位置上。 
表 1 基線估測表 
Distance(mm)(Z) Disparity(pixel) Baseline(mm)(b) 
900 7 12.16 
1050 6 12.16 
1200 5 11.54 
1350 4 10.38 
1500 4 11.54 
Average Baseline Length : 11.56 
 
最後在 B4 步驟，將校準完成的景深影像投影到影像校正後的相機位置。 
D.3.  景深影像改善 
 
圖 4 景深影像改善詳細流程圖 
為改善景深影像的邊緣雜訊及遮蔽區域。這些雜訊以及遮蔽區域在最後的影像合成
步驟，都會影響最後合成影像的品質。所以我們將校準後的影像問題，再細分成 3 個子
問題。 
a. 景深影像的邊緣雜訊 
b. 補「洞」的問題 
c. 景深影像細部區域修補問題 
在問題 a 中，參考[6] 使用聯合雙向濾波器(Joint Bilateral Filter)來濾除邊緣雜訊(C1)。
在[6] 中對於景深影像考慮它們的域項以及範圍項以外，並額外考慮彩色影像的範圍項。
不同於[6] 只考慮灰階項，我們是參考 R、G、B 三個光度，其結果較只考慮灰階項來的
好。 
問題 b 中，同樣也是聯合雙向濾波器來補洞(C2)。參考[6] 的想法，我們先建立信
任表(Confidence Map)，依照「洞」內的像素周圍有景深資訊點的個數多寡做分類。再
依據信任表的資訊，使用遞迴的方式來填補洞。 
問題 c 中，因為部分區域會因為 Kinect 景深的計算錯誤，導致該區域會有錯誤的景
深資訊，而這些區域是聯合雙向濾波器所無法解決的，所以我們參考彩色影像的資訊
(C3)，利用相似顏色的彩色像素點位置，在景深影像中對應的位置，利用多數決的方式
來決定每個點的景深值。最後在 C4 步驟，我們使用中值濾波器來濾除景深影像中，邊
緣上所存在的胡椒鹽雜訊(Pepper and Salt Noise)。 
9 
 
 
圖 8 A: 原始景深合成影像 B 改善後景深合成影像 
圖 8為使用 MPEG 標準會議所提供的影像合成參考軟體合成出的影像。其中圖 8 A
為使用原始景深影像(未經過 C 步驟)所合成出的虛擬視點影像，圖 8 B 為改善後景深影
像所合成出的虛擬視點影像。從細部區域(包含熊腳、熊頭及冰箱邊緣等等)可以看到，
改善後景深影像所合成出來的虛擬視點影像較好。經由此結果可看出景深影像的品質，
對於合成影像的品質有著很大的影響。 
 
第二部份  基於正/逆向深度映射演算法的稀疏多相機虛擬視點
合成 
A.  前言 
近年來，隨著軟硬體不斷的進步，立體影像技術不斷推陳出新，例如立體電視系統
(3DTV)[7][8]。而 MPEG 國際標準會議 (ISO/IEC Moving Picture Expert Group) 也在近
幾年制定了 3DAV (3D audio-visual)[9]文件，其中有一項是互動式多視角視頻 (Interactive 
Multiple View Video)，也稱做自由視點視頻 (Free Viewpoint Video，簡稱 FVV)，目的是
希望利用新的視點合成技術，達到即時的互動視訊效果。 
目前在自由視點成像系統 (Free Viewpoint Television，簡稱 FTV [10][11]) 上也有了
初步的架構，包括影像校正  (Camera Correction)[12][13]、影像深度估計  (Depth 
Estimation)[14][15]、多視點壓縮 (Multiview Video Coding，簡稱 MVC)[16]、多視點影
像合成 (Multiple View Synthesis)[11]、以及多視點 2D/3D 顯像技術 (2D/3D Multiview 
Display)。本篇論文會以影像合成為目的加以探討。 
B.  研究目的 
現階段而言，影像合成主要可分為兩大區塊，分別是模型式合成 (Model-Based 
Rendering，簡稱 MBR) 和影像式合成 (Image-Based Rendering，簡稱 IBR)[7]。前者主
要將空間中的場景或物體做重建 (Reconstruction)，再投影到虛擬影像上。而在影像式合
成[18]中，它會直接利用影像間的相對關係，將虛擬影像中的色彩空間做內插 (View 
11 
 
 
 
 
圖 10  虛擬視點合成 (Virtual View Synthesis) 
 
在傳統的視點合成演算法上，會先對景深圖作平行正向投影至虛擬視點上。由於投
影完之後，會有許多不合理的小破洞，需要將這些破洞修補起來。目前現有的視點合成
參考軟體 (View Synthesis Reference Software，簡稱 VSRS [16])，它是採用中值濾
波器，以 3 3 大小的核心 (Kernel)將這些小破洞修補起來。若是破洞太大，可能需要
連續幾次的中值濾波效果才行。 
    有了虛擬視點的景深圖，再利用逆向紋理映射，從虛擬視點上的任意位置，回到參
考影像上擷取對應的紋理。最後再將兩張虛擬視點影像，配合遮蔽區域做合適的線性疊
合。 
D.  研究方法 
由於景深映射後修補方法，目前主流做法是採用中值濾波器修補因量化所造成的小
洞，很容易造成遮蔽區域輪廓被破壞，影響到最後的合成品質。據此我們發展一套逆向
景深映射演算法，以大量降低映射後所產生的小破洞。如圖 11 所示： 
 
 
圖 11  逆向景深映射演算法示意圖 
13 
 
(c) FDW (d) BDWIP-256 
(e) 虛擬視點合成( FDW ) (f) 虛擬視點合成( BDWIP-256 ) 
圖 12  正/逆向映射演算法對合成的影響 
 
    在主觀品質上，可發現逆向景深映射演算法所產生的瑕疵現象，比正向景深映射演
算法還要少，這是因為參考景深影像的平坦區域對於逆向映射而言，可確保其連續性；
反觀正向映射會造成很多不必要的量化誤差，而這些量化誤差又使用不完全正確的濾波
器來將之修復，造成瑕疵現象變多。圖 13 為合成後的部分放大示意圖。 
 
 Ground Truth FDW BDWIP-256 
ROI 1 
   
15 
 
C.  文獻探討 
語音訊號合成的主要程序可分成三大步驟。第一步驟為將接收到的混和聲源作語音
訊號處理，使個別聲音訊號分離。第二步驟為估測各個訊號在空間中的位置所在。第三
步驟為合成任意位置聆聽點語音訊號。 
對於第一步驟，我們使用盲訊號分離(BSS, blind source separation)的技術來分離錄製
的混和語音訊號，我們亦使用數學模型[19]來解決分離問題。在語音處理中，常使用主
要成分分析(principal component analysis，PCA)[20]來簡化訊號的處理。為了解決排序問
題(permutation problem)，我們採用[21]來解決此問題。我們也使用最小失真原則
(minimum distortion principle，MDP)[22]來解決比率問題(scaling problem)。在許多著名
盲訊號分離的方法中，我們採用獨立向量分析法(independent vector analysis，IVA)[21]。
而此獨立向量分析有別於其他傳統的獨立成分分析(independent component analysis，ICA)
演算法，它使用不一樣的學習規則(learning rules)[23]。對於第二步驟，我們使用到達方
向(direction of arrival，DOA)的技術來定位個別的聲音訊號。在錄製語音訊號時，必須
滿足一些條件來避免空間走樣(spatial aliasing)[24]。我們使用不變特性假設(invariant 
property assumption)[25]來解決方向偵測，並採用[26]來估測空間中三維聲源訊號。對於
第三步驟，我們使用步驟一、二所分離的語音訊號與個別的聲源的角度資訊，並且採用
NASA Ames Research Center [27]所研發的軟體來合成虛擬聆聽點的語音訊號。 
在真實的錄音語音訊號的環境中，空氣的吸收、麥克風本身的失真都會造成語音雜訊，
所以我們必須考慮雜訊的影響。在我們的系統中，我們採用[28]來解決去雜訊問題
(denoising problem)。 
D.  研究方法 
在此章節，我們回顧盲訊號分離的技術與到達方向的技術，此兩種技術分別使用[21]
與[26]兩種演算法。我們假定系統為多輸入多輸出(multiple-input multiple-output，MIMO)
混和模型，並且訊號模型為無回響與無雜訊。在此假設之下，我們使用短時傅立葉變換
(Short-Time Fourier Transform，STFT)將時域訊號轉換到頻域訊號處理，並且假定K 的輸
入訊號、 N 個輸入訊號，數學模型如下： 
 ( , ) ) ( , )f t f f tx A( s  (2) 
 ( , ) ( ) ( , )f t f f ty W x  (3) 
 1,2, ,f F   
在上式中 F 表示為頻點(frequency bin)。 ( , )f tx 與 ( , )f ty 表示為在每一頻帶 f 的聲源訊號
與分離訊號。 ( )fA 表示為 N K 混和矩陣(mixed matrix)或稱為導引向量矩陣(steering 
vector matrix)。 ( )fW 表示為解混和矩陣(demixing matrix)。 
最後我們採用[28]作為去雜訊的演算法來改善語音品質。 
 
D.1.  盲訊號分離 
我們採用的獨立向量分析，他假設聲源訊號在頻域下有某些依賴關係，原始來源訊
17 
 
z
x
y
Φ
θ
 
圖 14  麥克風陣列與聲源訊號的空間相對關係 
考慮在上述章節的混和訊號模型，我們將時域訊號轉變為頻域訊號，並且混和矩陣可
被表示為： 
  1 1 1( ) ( , , ) ( , , )K K Kf f f   A a a  (8) 
且 
  1( , , ) ( , , ) ( , , ) Tk k k k k k Nk k kf a f a f     a   (9) 
 2( , , ) exp ( , )Tnk k k nk k k
fa f g j r v
c
       
 
 (10) 
在上式中 ( )fA 表示為混和矩陣，它的第 k 個行向量代表第 k 個聲源訊號的轉移函數，即
被稱為導引向量矩陣。 nkg 表示為 nka 的增益。 ( , , )Tn n nr x y z

表示為第 n 個麥克風座標向
量。  ( , ) cos cos ,sin cos ,sink k k k k k kv        代表為第 k 個訊號源的方位向量，如圖 14
所示。我們使用[26]將兩參數做相除，並且得到以下方程式： 
  1 1 1 2 1 2
2 2
2exp sin cos ( )sink k k k k
k k
a a fj y y z z
a a c
            (11) 
  1 1 1 1 3
3 3
2exp 3 sin cos ( )sink k k k k
k k
a a fj y y z z
a a c
             (12) 
且 
  1 21/2 k k
angle a a
A
fc   (13) 
  1 31/2 k k
angle a a
B
fc   (14) 
最後我們可以得到所需角度、： 
19 
 
( )fW
 
圖 15  語音訊號合成的流程圖 
 
我們採用[27]研發語音軟體來合成虛擬聆聽點。我們從錄製的混和聲源中，使用盲
訊號分離分離各個原始聲源，並作為輸入訊號。圖 16(a)展示在 X-Y 平面分離訊號與麥
克風陣列的佈局。S1、S2 和 Po 分別表示第一個聲源、第二個聲源和原始麥克風陣列的
位置。θ1 和 θ2 分別表示第一個聲源和第二個聲源的水平角。d1 和 d2 分別表示第一個聲
源和第二個聲源離麥克風的距離。在此距離為實際值。最後我們可以合成 P1、P2、P3
等其他位置的語音訊號，即為虛擬聆聽點語音訊號。 
1 2
      
14o14o
 
(a)                                          (b) 
圖 16 語音合成示意圖 
E.  結果與討論 
在我們提出的語音合成系統，圖 16(b)展示我們最後合成在 X-Y 平面聲源訊號和麥克
風陣列的佈局。Female_1 和 Male_1 表示 BSS 分離訊號如上所示的 S1、S2。並且由 DOA
所估測的個別角度分別為 14o 和 14o。在此真實距離分別為 1.5 公尺和 1.5 公尺。由上述
的設置，我們可以使用 SLAB 合成虛擬聆聽點語音。圖 17 (a) ~ (d)展示在 P1、P2、P3
和 P4 各個位置的語音訊號合成。Table II 展示圖 16(b)中位置的座標資訊。 
21 
 
[5]  S. Lee and Y. Ho, “Real-time Stereo View Generation using Kinect Depth  Camera,” in 
Proc. APIPA Annual Summit and Conference(APSIPA ASC), Xi’an, China, pp. 
RS3.12(1-4), Oct 2011. 
[6]  M. Camplani, L. Salgado, “Efficient spatio temporal hole filling strategy for Kinect 
depth maps,” IS&T/SPIE Int. Conf. on 3D Image Processing (3DIP) and Applications , 
CA, USA, pp. 82900E 1-10, Jan. 2012. 
[7]  P. Kauff, N. Atzpadin, C. Fehn, M. Muller, O. Schreer, A. Smolic, and R. Tanger, “Depth 
map creation and image-based rendering for advanced 3DTV services providing 
interoperability and scalability,” Signal Process: Image Communication, Special Issue on 
3DTV, pp. 217-234, Feb. 2007. 
[8]  2007. A. Kubota, A. Smolic, M. Magnor, M. Tanimoto, T. Chen, and C. 
Zhang,“Multiview Imaging and 3DTV,” IEEE Signal Processing Magazine, vol. 24, no. 
6, pp. 10–21, Nov. 2007. 
[9]  A. Smolic´ and. McCutchen, “3DAV Exploration of Video-Based Rendering Technology 
in MPEG,” IEEE Trans. Circuits Syst. Video Technol., Special Issue on Immersive 
Commun., vol. 14, no. 9, pp. 348–356, March 2004. 
[10]  M. Tanimoto, “Overview of Free Viewpoint Television,” Signal Processing: Image 
Communication, vol. 21, no. 6, pp. 454–461, July 2006. 
[11]  S. Zinger, L. Do, and P.H.N. de, “Free-Viewpoint Depth Image Based Rendering,” 
Journal Visual Communication and Image Representation, 21(5-6), pp. 533–541, 2010. 
[12]  Y.-S. Kang and Y.-S. Ho, “An Efficient Image Rectification Method for Parallel 
Multi-Camera Arrangement,” IEEE Trans. Consum. Electron., 57(3), pp. 1041–1048, 
2011. 
[13]  F. M. Porikli, “Inter-camera color calibration by correlation model function,” IEEE Int. 
Conf. on Image Processing, pp. 133–136, 2003. 
[14]  A. Olofsson, “Modern Stereo Correspondence Algorithms: Investigation and evaluation,” 
Thesis from Dept. of Electrical Engineering, Linköping Univ., Linköping, Sweden, 2010. 
[15]  D. Scharstein and R. Szeliski, “A taxonomy and evaluation of dense two-frame stereo 
correspondence algorithms,” Int. J. Computer Vision, 2002. [Online]. Available: 
http://www.middlebury.edu/stereo 
[16]  ISO/IEC JTC1/SC29/WG11, MPEG document N11631, “Report on Experimental 
Framework for 3D Video Coding,“ Oct. 2010. 
[17]  ISO/IEC JTC1/SC29/WG11, MPEG document M15672, “View synthesis software and 
assessment of its performance,“ July 2008. 
[18]  R. Szeliski, “Image-Based Rendering,” Computer Vision Algorithms and Applications, 
chap. 13, August 2010. 
[19]  Alan V. Oppenheim, et al., discrete-time signal processing (Second Edition), Prentice 
Hall, 1999. 
23 
 
綜合評估：研究內容與原計畫進度與內容大致相符，已達成學術研究創新與人才
培育之預目標。整體成效良好。研究成果頗具學術與應用價值，承繼之前的同一系列
研究項目，已發表 3 篇學術會議論文，及碩士學位論文 4 冊如下表。因為此一系列題目
研究在本實驗室開始不久，期刊論文正在籌備中。 
 
 Publications (含前一年相關研究在本年發表) 
1. C.-H. Li, J.-J. Tsai, H.-M. Hang, “A	triangular‐warping	based	view	synthesis	scheme	
with	enhanced	artifact	reduction	for	FTV,” IEEE International Conf. on Image 
Processing '11, Brussels, Belgium, Sept 11-14, 2011. 
2. C.-Y. Luo, Y.-C. Lu, and H.-M. Hang, “基於聲源分離和定位技術所建構出虛擬聆聽點
音訊合成系統 (Virtual Listening Point Audio Synthesis using Sound Separation and 
Source Localization Techniques)”, in 2011 National Symposium on Telecommunications, 
Hualien, Taiwan, Nov. 18-19, 2011. 
3. S.-J. Chien and H.-M. Hang, “Virtual Listening Point Audio Synthesis using Anechoic 
Chamber Recording”, in 2012 National Symposium on Telecommunications, Changhua 
City, Taiwan, Nov. 16-17, 2012. 
4. 簡士傑, “Virtual Listening Point Audio Synthesis using Anechoic Chamber Recording,” 
MS Thesis, NCTU, July 2012. 
5. 李讀修, “Sparse Multi-Camera Virtual View Synthesis Using Forward and Backward 
Depth Warping Algorithms,” MS Thesis, NCTU, July 2012. 
6. 邱義文, “Depth Refinement for View Synthesis using Depth Sensor and RGB Camera,” 
MS Thesis, NCTU, July 2012. 
7. 楊復凱, “Acceleration and Improvement of MPEG View Synthesis Reference Software 
on NVIDIA CUDA,” MS Thesis, NCTU, March 2012 
 
2 
 
Prof. K. J. Ray Liu (University of Maryland, College Park, USA), “Information 
Anti-Forensics”. 
Mr. Longming Zhu (ZTE Corporation, China), “Perspective of wireless broadband 
technology”. 
Prof. Ray Liu是舊識，經常訪問台灣。Prof. Furui是 APSIPA主席，正逢其從 Tokyo 
Institute of Technology退休，因此有個特別儀式感謝他。 
除了 Keynote Speeches，Special Sessions是比較吸引人的。因為有六個左右平行議程，
分身乏術，Special Sessions通常拉走較多人。 
我們的論文是在週二下午 Video Streaming and Coding中。這個 Session包含五篇論
文，除我們的之外，還有 
New Prediction Techniques for Inter- and Intra-Frames of Advanced Video Coding  
(Chung-Cheng Lou, Szu-Wei Lee, Seung-Hwan Kim and C.-C. Jay Kuo)  
SVC-Based Scheduling Algorithm for Video in the Cloud  
(Song Xiao, Junmei Bai, Jianlong Zhang and Jianchao Du)  
Modeling Image Sparsity in Compressive Sensing  
(Shanzhen Lan, pin Xu and Qi Zhang)  
Quality Adaptation of SVC-Based P2P Streaming  
(Yan Pang, Jiaying Liu and Zongming Guo) 
會場人數不是很多，但討論頗為熱烈。  
此外， 韓國 Prof. Yo-Sung Ho擔任 Image, Video, and Multimedia Technical Committee 
(IVM TC)的主席，十數位這個領域的朋友擔任 Committee委員，我也列名其中開會，制
定 IVM Committee規程。 
 
二、與會心得： 
APSIPA為一年輕的研討會，目前仍算發展階段。主題的範圍相當廣，對於參與個人
來說，有性趣部分的論文數可能不是很多。但因此瞭解其他領域狀況。 
參加學術會議，可以了解最新學術研究趨勢與成果。專業之外也可同時認識了國內
外相關領域之學者，並與老朋友碰面，對於知識的擴展與友誼的增進均覺受益良多，達
到了學術交流目的。這次會議中，順利爭取到台灣 2013年 APSIPA主辦權。 
 
三、攜回資料名稱及內容 
會議手冊及論文 USB碟。 
 
98 年度專題研究計畫研究成果彙整表 
計畫主持人：杭學鳴 計畫編號：98-2221-E-009-087-MY3 
計畫名稱：自由點聲訊合成與多視點視訊表述技術研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 4 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
