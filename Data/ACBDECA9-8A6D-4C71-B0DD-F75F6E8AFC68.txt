in the TREC09 and TREC10 Web Track diversity tasks.  
The best performance our proposed algorithm achieves 
is α-nDCG@5 0.307, IA-P@5 0.121, and α#-nDCG@5 
0.214 on the TREC09, as well as α-nDCG@10 0.421, IA-
P@10 0.201, and α#-nDCG@10 0.311 on the TREC10.  The 
results conclude that the subtopic mining technique 
with the up-to-date users’ search query logs is the 
most effective way to generate the subtopics of a 
query, and the proposed subtopic-based 
diversification algorithm can select the documents 
covering various subtopics. 
英文關鍵詞： Diversified Retrieval, Subtopic Mining, Search Result 
Re-ranking 
 
2 
1. INTRODUCTION 
Users’ queries represented by a few keywords usually contain a certain extent of ambiguity 
(Spärck-Jones et al., 2007).  An ambiguous query may refer to multiple aspects or have more 
than one interpretation.  In this paper, different interpretations and aspects associated with a 
query are called subtopics.  For example, the query “dinosaur” may refer to three subtopics: (1) a 
paleontological science, (2) a multimedia recreation, and (3) a scenic spot.  Each subtopic may 
also contain several sub-sub-topics, for example, “comics”, “television shows”, “films”, and 
“music bands” with respect to the subtopic of “a multimedia recreation”.  In addition, even a 
query with a clearly faceted interpretation might still be under-specified, because it is not clear 
which subtopic of the interpretation is actually desirable for users.  For instance, the faceted 
query “air travel information” may contain different subtopics, such as (1) information on air 
travel, airports, and airlines, (2) restrictions for checked baggage during air travel, and (3) websites 
that collect statistics and report about airports.   
Traditional information retrieval models, such as the Boolean model and the vector space 
model typically only consider the relevance between a query and documents.  These retrieval 
models treat every input query as a clear, well-defined representation and completely neglect any 
sort of ambiguity.  This negligence results in the top ranked documents possibly containing too 
much relevant information on the same subtopic.  This might increase users’ search time for 
distinguishing whether the retrieved documents contain redundant information.  In addition, some 
retrieval models anticipate that the underlying meaning of a submitted query is always the most 
popular subtopic.  These models may focus the retrieval process on popular/particular subtopics 
too much.  This postulation may have risks with a wrong guess (e.g., the user information need is 
different from the most popular subtopic), which could leave users unsatisfied.  For maximizing 
the satisfaction of different users, a retrieval model has to select a list of documents that are not 
only relevant to some popular subtopics, but also covers different subtopics.  Users can quickly 
find relevant information that may be interesting if search results are diversified.  Nevertheless, 
how to balance the relevance and the diversity of search results is a trade-off.  On the one hand, 
too many subtopics may provide diversified information but introduce many irrelevant documents, 
which cause a relevance issue.  On the other hand, if only the similarity between a query and 
documents is considered, too many documents belonging to the same subtopics are retrieved, 
which causes a diversity issue. 
In recent years, using subtopics of a query for diversifying the retrieved documents has 
received considerable attention (Song et al., 2011).  The broad topic associated with an 
ambiguous or unclear query can be decomposed into a set of subtopics.  This provides an 
opportunity to deal with the problem of search result diversification, as we can employ the clues 
from the subtopics to rank a diverse ranking list based on optimizing the maximum coverage of the 
subtopics.  In this paper, we introduce a novel framework for search result diversification that 
exploits the subtopics embedded in queries and ranks the retrieved documents based on these 
discovered subtopics.  Several methods are proposed for mining subtopics from different aspects, 
such as the retrieved documents, the search query logs, and the related search services provided by 
the commercial search engines.   
Theoretically, the diversified retrieval models should provide a ranking list of documents that 
has the maximum coverage and minimum redundancy with respect to the possible subtopics 
underlying a query.  Moreover, the covered subtopics should also reflect their relative importance 
for the query, as perceived from most users (Yin et al., 2009)(Agrawal et al., 2009).  For example, 
a query “java” may have three subtopics (a programming language, coffee, and an island), where 
the subtopic of island attracts less interest than the subtopic of programming language.  The 
subtopic of programming language should be relatively more important than the subtopic of island 
for the query “java”.     
In this paper, we propose a subtopic-based diversified retrieval framework that first uncovers 
different subtopics embedded in a query, then assigns a weight for each mined subtopic to describe 
its importance, and finally estimates the relevance of the retrieved documents to each mined 
subtopic for diversifying search results.  The proposed framework not only keeps the quality of 
relevance, but also re-ranks the top-ranked retrieved results to cover multiple important subtopics. 
Specifically, there are three components in the proposed diversification framework, the richness of 
subtopics, the importance of subtopics, and the novelty of subtopics.  The richness part aims at 
measuring how many subtopics are covered by a document, the importance part estimates the 
4 
information between a new document and the previous selected documents without considering 
the subtopics of a query explicitly.  
For the implicit approaches, Carbonell and Goldstein (1998) proposed the maximal marginal 
relevance (MMR) to rank a retrieved document under a combination of a relevancy score with 
respect to a query and a dissimilarity score with respect to other similar documents selected at 
earlier ranks.  Zhai et al. (2003) modeled relevance and redundancy based on the KL-divergence 
measure and a simple mixture model.  Yue and Joachims (2008) maximized the word coverage to 
select the optimum set of diversified documents.  The learned model selected documents for 
covering maximum distinct words with the greedy search.  Chen and Karger (2006) presented a 
selection algorithm based on the Bayesian information retrieval framework for diversifying search 
results among the top ten previously visited results.  Their selection algorithm estimated the 
documents based on the probability ranking principle and used pseudo-relevance feedback to 
search result diversification by negative feedback on the redundant documents.  Vee et al. (2008) 
proposed several B+ tree based diversifying models to return a set of different answers on query 
answering diversification.  Gollapudi and Sharma (2009) developed a set of natural axioms for 
diversification and utilized several diversified functions in their diversification framework.  
Wang and Zhu (2009) used mean-variance analysis to search result diversification based on the 
economic portfolio theory.  Their algorithm estimated an uncertainty in terms of the “risk” in 
economic domain trade-off between the expected relevance of a set of retrieved documents and the 
correlation between them, and it selected the right combination of relevant documents under the 
uncertainty estimation. 
Different from the implicit approaches, subtopics can be modeled explicitly by queries for 
diversifying search results.  Subtopics may be mined from a predefined taxonomy (Agrawal et al., 
2009), related sub-queries, and suggested sub-queries (Santos et al., 2010a), etc.  Radlinski and 
Dumais (2006) used the query-query reformulation records in search query logs to discover 
subtopics, and they diversified the retrieved results for improving effectiveness on personalized 
search.  Agrawal et al. (2009) presented a systematic approach to diversifying search results 
through modeling subtopic from queries and documents by classification taxonomy for 
minimizing the risk of dissatisfaction of the average users.  Carterette and Chandar (2009) 
identified subtopics by topic models and used a probabilistic ranking model to maximize the 
subtopic coverage rate for search result diversification.  Santos et al. (2010a)(2010b) uncovered 
subtopics by the query suggestion from search engines and proposed a probabilistic framework 
that estimated the diversity based on not only the documents’ relevance to query subtopics, but 
also the relative importance associated with the query subtopics.  Rafiei et al. (2010) identified 
subtopics by existing taxonomic information and regarded the problem of diversifying results as 
expectation maximization.  They attempted to broaden the coverage of subtopics in the retrieved 
results.  Welch et al. (2011) employed WordNet and Wikipedia to discover subtopics and 
presented a diversification algorithm especially suitable for informational queries where users may 
take more than one page to satisfy their needs.  
Compared to the studies described above, the major contributions of this work are four-fold. 
 Exploring various subtopic mining methods: A total of six subtopic mining methods are 
explored in this paper.  These methods mine subtopics from different aspects, such as 
retrieved documents and external resources.  
 Analyzing the effectiveness of the subtopic mining methods in depth: We analyze the 
effectiveness of the subtopics derived from different subtopic mining methods.  A user study 
comparing the subtopics mined by different mining methods with the ground truth in the 
TREC09 and TREC10 is conducted.   
 A novel subtopic-based diversified algorithm: The proposed diversification algorithm 
optimizes the estimation from three aspects, the richness of subtopics, the importance of 
subtopics, and the novelty of subtopics for diversifying search results 
 Thorough evaluation of the document ranking experiments:  Through experiments on the 
ClueWeb09 Category A and Category B test collections with the topics of the TREC09 and 
TREC10, we demonstrate that our model significantly improves the performance of the state-
of-the-art models proposed in the TREC09 and TREC10 Web Track diversity tasks. 
6 
          
,
,
0.5(0.5 ) ,
max
i d
i d
d i
freq N
w logfreq n= + ×
 
(1) 
where freqi,d is the frequency of feature i in document d, maxd freq is the maximum feature 
frequency in document d, N is the total number of retrieved documents, and ni is the number of 
documents in which feature i appears. 
The k-means clustering algorithm (MacQueen, 1967) is performed on the retrieved documents, 
and the documents of similar representations are put together in a document cluster.  In this 
method, the cosine distance determines the similarity between two documents.  The number of 
clusters (e.g., k) has to be determined before the k-means clustering algorithm is applied.  The 
number of clusters is regarded as the number of subtopics within the given query.  Intuitively, the 
number of subtopics depends on the query itself.  For example, the query “Obama family tree” 
may have three subtopics like “TIME magazine photo essay”, “Barack Obama's parents and 
grandparents come from”, and “biographical information on Barack Obama”.  Another query 
“kcs” may have five subtopics, such as “Kansas City Southern railroad homepage”, “job 
information with the Kansas City Southern railroad”, “Kanawha County Schools in West Virginia 
homepage”, “Knox County School system” and “KCS Energy”.  Therefore, finding an 
appropriate k is an important issue.  
This paper considers three strategies to determine the k.  These strategies are based on 
empirical observation and two external resources, i.e., “Google Insight for Search1” (GIS) and 
“Open Directory Project2” (ODP).  For the empirical observation, we set k to 5, 10, and 20 
because our observation shows the number of subtopics of a query is unlikely to be more than 
twenty.  Furthermore, we employ external resources to determine the k.  The GIS provides 
information of a query based on users’ Internet search patterns on Google, such as search volume 
patterns across specific regions, geographic distribution, and categories for a query.  The GIS 
classifies queries on the Web into 27 categories.  We consult the GIS to collect all possible 
categories of a given query.  The number of categories is regarded as the number of its subtopics.  
The ODP is constructed and maintained by a vast, global community of volunteer editors.  It 
contains more than four million web pages that are organized into more than 500 thousand 
categories.  All websites in the ODP are classified into sixteen major categories.  We submit 
queries to the ODP to collect the major categories of the returned web pages.  The number of 
distinct major categories is regarded as the number of subtopics.   
Table 1 lists the collected categories based on the two external resources for the query 
“dinosaur”.  As shown in the table, the query “dinosaur” is classified into six major categories by 
the GIS and eleven major categories by the ODP.  This reflects the fact that the ODP provides 
more specific categories than the GIS. 
Table 1. Categories mined by using the GIS and the ODP for the query “dinosaur” 
Resource Category 
GIS Science, Entertainment, Games, Local, Lifestyles, Reference 
ODP Kids and Teens, Shopping, Science, Arts, Society, Games, World, Reference, 
Regional, Computers, Recreation 
 
After document clustering, critical terms in each document cluster are extracted to generate 
subtopics.  The tf-idf values of terms in each document cluster are calculated.  The top five 
terms of the highest tf-idf values in each document cluster are selected as a subtopic of a query.   
Table 2 lists the discovered subtopics by the clustering-based method with k=10 for the query 
“dinosaur”.  The mined subtopics demonstrate the user’s underlying search intent of the query.  
For example, users may have a search intent focused on dinosaur museums, tyrannosaurus rex, or 
dinosaur toys.  The subtopics mined from the clustering-based method may be duplicated in the 
surface form.  For example, No. 1, No. 9 and No. 10 subtopics refer to the same search intent of 
“find museum about dinosaur”.  After deep analysis, the search intents among the three subtopics 
are somewhat different.  The search intent of No. 1 subtopic is the dinosaur game in a museum, 
No. 9 subtopic is fossils in a museum, and No. 10 is dinosaur pictures in a museum.   
                                                 
1
 http://www.google.com/insights/search/ 
2
 http://www.dmoz.org/ 
8 
3.1.3. Concept-tag-based method 
The tags of a document provide a brief summary of the document.  Documents labeled with 
similar concept tags may infer a sort of similar concepts.  Folksonomy created by collaborative 
tagging and social tagging is a good resource to mine subtopics.   
In this method, we also employ the AlchemyAPI toolkit to analyze the retrieved documents 
and generate concept tags along with their confidence scores.  The confidence score represents 
the relevance between a concept tag and a document.  The concept tag of a higher confidence 
score for a document means the concept tag is more relevant to the document.  The generated 
concept tags are regarded as subtopics in this method.  Documents then are grouped to form a 
concept cluster if they contain the same concept tags.  Note that a document may belong to more 
than one concept cluster if it is labeled with more than one concept tag.  The size of a concept 
cluster is considered as the relative importance of the subtopic.  In addition, the documents in a 
sub-ranking list of a subtopic are sorted by the descending order of their original ranking score 
from the retrieval model multiplied by the confidence score of the subtopic.   
Table 4 lists the discovered subtopics for the query “dinosaur”.  As shown in the table, most 
of the subtopics are indeed associated with dinosaur.  Some interesting subtopics are generated, 
such as “Jurassic Park”, “Michael Crichton” and “Morrison Formation”.  All of these subtopics 
are relevant to dinosaurs: Jurassic Park is an American science fiction adventure film, Michael 
Crichton is the writer of the Jurassic Park novel, and many dinosaur fossils have been found at 
Morrison Formation in North America.  Comparing Table 3 and Table 4, we observe that the 
subtopics discovered from taxonomic information are more general than those from folksonomy. 
Table 4. Subtopics mined by the concept-tag-based method for the query “dinosaur” in the order of 
relative importance 
Importance Subtopic 
1 Dinosaur 
2 Jurassic Park 
3 Tyrannosaurus 
4 Museum 
5 Michael Crichton 
6 Paleontology 
7 Natural History Museum 
8 Morrison Formation 
 
3.1.4. Ontology-based method  
Queries are ambiguous because query terms may refer to more than one sense.  Identifying the 
senses of queries may be a possible method to uncover the subtopics of queries.  Wikipedia has 
become a source of sense annotations for word sense disambiguation.  The Wikipedia 
disambiguation pages provide a service to predict the correct sense of an input query.   After 
query sense disambiguation, a list of references pages is reported for each sense.  Each returned 
sense is regarded as a subtopic of the query.  Take Table 5 as an example for the query 
“dinosaur”.  The Wikipedia disambiguation page returns five senses: places, film and television, 
music, comics, and other uses.  For the sense “Film and television”, four snippets are reported to 
describe its meaning.  One of them is about a Disney computer animated film named Dinosaur, 
which was a popular film released in 2000.  
 
 
 
 
 
 
10 
Table 6. Subtopics mined by the query-log-based method for the query “dinosaur” 
No. Subtopic No. Subtopic 
1 dinosaur rex USA click 6 dinosaur student lesson web 
2 dinosaur bird science fossil 7 dinosaur fossil picture rex 
3 dinosaur lesson fossil student 8 dinosaur bed toy kid  
4 dinosaur fossil rex art prehistoric 9 dinosaur color plant print click 
5 dinosaur game available player check 10 dinosaur party birthday kid toy 
 
Recall that each session is represented as a pseudo document.  All of the sessions in the same 
cluster are merged together to form a pseudo document set and used to represent a subtopic of the 
query.  The cosine similarity between a retrieved document and each pseudo document set is 
computed.  A document is classified into the subtopic of the highest similarity score.  The 
documents in a sub-ranking list of a subtopic are sorted by the descending order of their similarity 
scores.  The size of a cluster (e.g., the number of sessions in a cluster) is taken into account in 
deciding the relative importance of the subtopic. 
 
3.1.6. Related-search-based method 
Most commercial search engines provide the related search mechanisms based on their up-to-date 
users’ search query logs, which record users’ long-term searching and query formulation behaviors.  
The related searching mechanism provides external knowledge sources for subtopic mining.  
Through such a mechanism, related search queries are expanded from the original query.  In this 
way, an expanded query describes an information need more precisely, based on global user search 
behaviors recorded in the query logs.  Given a query, we collect the related search queries and 
each related search query is regarded as a subtopic.  We utilize three major commercial search 
engines (e.g., Google, Yahoo, and Bing) for the reference searches.  The retrieved documents are 
assigned to some subtopic(s) as follows.  After obtaining the related search query (i.e., subtopics), 
we query the search engine again using the related search queries.  Each subtopic gets a subtopic-
ranking list.  The number of documents reported by the search engine is taken into account in 
determining the relative importance of a subtopic.  URLs of the original retrieved results for a 
given query are matched with those URLs in each subtopic-ranking list.  If a URL appears in the 
corresponding subtopic-ranking list, then the document is assigned to the subtopic.  Since some 
URLs may not be covered by any subtopic-ranking list, we propose an approximate approach to 
deal with the problem.  This approach condenses an uncovered URL one level at a time and looks 
up the subtopic-ranking lists to check if the condensed URL exists in any subtopic-ranking lists.  
If it exists, the document is assigned to the subtopic.  Otherwise, we condense one more level 
until either a match is found or a miss is reported.  Note that a document may be placed into more 
than one subtopic.  The documents in a sub-ranking list of a subtopic are placed by the ranks of 
the matched URLs. 
Table 7 lists the subtopics for the query “dinosaur” mined by the related-search-based 
methods of the three major commercial search engines.  The subtopics mined from Google are 
shorter and simpler than Yahoo and Bing after observation.  In contrast to the previous subtopic 
mining methods, the number of duplicate subtopics is decreased.  The subtopics mined from 
different search engines are not exactly the same, but the concepts are similar.  For example, the 
subtopic “Play Dinosaur Games” mined from Bing is similar to “dinosaur games” mined from 
Google.  Nevertheless, there is no game-related subtopic mined from Yahoo.  As a result, the 
related-search-based method takes into account various search engines to achieve the 
complementary effects. 
 
 
 
 
 
12 
Below, we describe how the selection process works.  The objective function, as shown in 
Equation (2), consists of two functions: Rel(q, D) and Div(q, D).  We use the Rel(q, d) function to 
measure a relevance score of a document d∈D, and we use the Div(q, d) function to estimate a 
diversity score of the document d∈D.  Previous studies (Carbonell & Goldstein, 1998)(Yin et al., 
2009) compute the relevance score of documents by the existing conventional retrieval models, 
such as the language model and the vector space model.  The limitation of these strategies is that 
relevance scores of the same document determined by different retrieval models may be unable to 
be compared.  Although relevance scores of documents are usually real numbers, they may be in 
different ranges, on different scales, and in different distributions.  Therefore, the relevance score 
may dominate the objective function if it is too large, and this may not be effective for diversifying 
search results.  In this paper, the relevance score of a document is computed and normalized as 
the reciprocal of the rank of a ranking list for generating a comparable relevance score between 
different retrieval models.  Given a document ranking list for q and a document d∈D, the Rel(q, 
D) function is defined as follows: 
( , ) ( , )
1
( , )
d D
d D
Rel q D Rel q d
rank q d
∈
∈
=
=
∑
∑
    
(4) 
where rank(q, d ) returns the rank of document d in the ranking list for q.  
We consider three dimensions for calculating the diversity function Div(q, D).  The three 
components include the richness of subtopics, the importance of subtopics, and the novelty of 
subtopics within the retrieved documents.  Let Sub(q) denote a set of mined subtopics for a given 
query q and D’ denote a document set selected from D.  The Div(q, D) function is defined as 
follows: 
( ) ( ), '
( , ) ( , )
( ( , ) ( , '))
d D
d s Sub q s Sub q D D
d D
Div q D Div q d
richness importance s q novelty s D
∈
∈ ∈ ⊂
∈
=
= ⋅
∑
∑
(5) 
where richnessd(•) measures the richness of subtopics covered by the document d, importance(s, q) 
measures the relative importance of the subtopic s, and novelty(s, D’) measures the novelty of the 
subtopic s in the selected document set D’.  In other words, the number of subtopics that d covers, 
the number of important subtopics that d contains and the number of novelty subtopics that d 
includes is directly related to the diversity score that d has.  
As mentioned before, the subtopics are mined by various subtopic mining methods, and a 
document belonging to more than one subtopic is more likely to satisfy more users.  Thus, the 
richness function ranks the documents of broad subtopics, i.e., documents with more subtopics 
move to the top positions of a ranking list.  Furthermore, the richness function considers clues 
from subtopics mined by various subtopic mining methods.  The richness function merges the 
coverage of subtopics mined by different subtopic mining methods by summing the covered 
subtopics and averaging by the number of subtopic mining methods.  We rewrite the Div(q, d) 
function in Equation (5) as follows: 
( ) ( ), '
1 1
1( , ) ( ( , ) ( , '))
inm
s S q s Sub q D D
i j
Div q d importance s q novelty s D
m
∈ ∈ ⊂
= =
= ⋅∑∑
 
(6) 
where m is the number of subtopic mining methods used and ni is the number of subtopics mined 
by the subtopic mining method i. 
As for the relative importance of a subtopic, we incorporate the factor into the Div(q, d) function 
directly.  The relative importance of subtopic s depends on different subtopic mining methods, as 
mentioned in Section 3.1.  With the incorporation, Equation (6) can be rewritten as follows: 
, ( ), '
1 1
1( , ) ( , '))
inm
i j s Sub q D D
i j
Div q d weight novelty s D
m
∈ ⊂
= =
= ⋅∑∑
  
(7) 
where weighti,j is a relative importance of the j-th subtopic generated by the subtopic mining 
method i. 
14 
 
Figure 2. An example topic along with its corresponding subtopics in the TREC09 
 
5. EXPERIMENTAL RESULTS AND DISCUSSION  
In this section, we first evaluate the performance of the different subtopic mining methods we 
propose.  Then, we compare the two proposed diversification algorithms, i.e., the RR-based 
diversification algorithm and the subtopic-based diversification algorithm.  We use topics on the 
TREC09 and TREC10 Web Track for testing, and we experiment on the ClueWeb09 Category B 
and Category A test collections.  In the experiments, three metrics are used for evaluation.  
Finally, we compare our best model with the state-of-the-art models in the TREC09 and TREC10 
Web Track diversity tasks and discuss the experimental results. 
Experiments were evaluated by three well-known metrics: (1) α-nDCG (Clarke et al., 2008), 
which measures the overall relevance across intents; (2) Intent-Aware Precision (Agrawal et al., 
2009) (abbreviated as IA-P), which measures the diversity; and (3) α#-nDCG, which is a linear 
combination of IA-P and α-nDCG.  Assume there are k subtopics for a given query.  The α-
nDCG metric considers the k subtopics for computing the gain vector.  The α-nDCG is defined as:  
- [ ]
- [ ]
- '[ ]
DCG k
nDCG k
DCG k
α
α
α
=     (9) 
where α-DCG[k] is a normalized discounted cumulative gain vector and α-DCG’[k] is the ideal 
discounted cumulative gain vector. 
Intent-aware precision (IA-P) at retrieval depth l is defined as follows: 
1 1 1
1 1 1
- @ ( , )tNM lt i j t
t
IA P l j i j
M N l= = =
= ∑ ∑ ∑
   
(10) 
where M is the number of topics.  Let Nt (1≤t≤M) be the number of subtopics associated with 
topic number t.  Let jt (i, j) = 1 if the document returned for topic t at depth j is judged relevant to 
subtopic i of topic t; otherwise 0.  We list the performance at depths of l = 5, 10, and 20 (i.e., the 
number of top ranked items to be evaluated), in order to comprehensively examine the effect of the 
proposed systems.  
5.1. Evaluation of the Subtopic Mining Methods 
A total of 100 topics, along with several example subtopics, were provided in the TREC09 
and TREC10 Web Track diversity tasks.  The average number of example subtopics per topic is 
4.83 and 4.36 for the TREC09 and TREC10, respectively.  The example subtopics for each topic 
were regarded as a ground truth for this topic.  We conducted a user study to evaluate the 
16 
Table 9. Performance of the subtopic mining methods on the TREC10 using the ClueWeb09 
Category B test collection 
TREC10 α-nDCG IA-P α#-nDCG 
 @3 @5 @10 @3 @5 @10 @3 @5 @10 
Indirect mining methods          
Topic-Category 0.288 0.292 0.297 0.088 0.095 0.093 0.188 0.194 0.195 
Concept-tag 0.307 0.290 0.277 0.086 0.079 0.067 0.197 0.185 0.172 
Clustering (GIS) 0.491 0.473 0.445 0.154 0.153 0.123 0.323 0.313 0.284 
Clustering (ODP) 0.507 0.441 0.369 0.160 0.129 0.068 0.334 0.285 0.219 
Clustering (k=10) 0.503 0.488 0.484 0.164 0.162 0.156 0.334 0.325 0.320 
Direct mining methods          
Ontology 0.292 0.265 0.254 0.069 0.061 0.045 0.181 0.163 0.150 
Query Logs (k =10) 0.451 0.451 0.448 0.136 0.144 0.140 0.294 0.298 0.294 
Query Logs (GIS) 0.494 0.442 0.378 0.137 0.113 0.064 0.316 0.278 0.221 
Query Logs (ODP) 0.505 0.482 0.456 0.159 0.156 0.118 0.332 0.319 0.287 
Related Search (Yahoo) 0.626 0.600 0.560 0.193 0.188 0.138 0.410 0.394 0.349 
Related Search (Google) 0.710 0.684 0.657 0.213 0.202 0.154 0.462 0.443 0.406 
Related Search (Bing) 0.719 0.664 0.644 0.221 0.207 0.184 0.470 0.436 0.414 
 
Since the subtopics of a TREC topic are just examples for the topic, some additional subtopics 
mined by the subtopic mining methods may be related to the test topic, but do not appear in the 
ground truth.  To clarify this point, we compare the strict and the lenient performance of the 
subtopic mining methods.  The strict performance is based on the original ground truth by the 
TREC only, and the lenient performance considers those additional subtopics that are regarded as 
correct by the assessors.  Figure 3 and Figure 4 show the strict and the lenient performance of 
different subtopic mining methods on the TREC09 and TREC10, respectively.  Intuitively, the 
lenient performance is better than the strict performance.  Take the ontology-based method as an 
example.  The ontology-based method achieves a significant improvement because more than 
half of the additional subtopics are discovered by the ontology-based method, compared with the 
number of subtopics in the TREC09 and TREC10 ground truth after observation.   
 
 
Figure 3. The strict and lenient performance of the subtopic mining methods on the TREC09 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
@
3
@
5
@
10 @
3
@
5
@
10 @
3
@
5
@
10 @
3
@
5
@
10 @
3
@
5
@
10 @
3
@
5
@
10 @
3
@
5
@
10 @
3
@
5
@
10 @
3
@
5
@
10 @
3
@
5
@
10 @
3
@
5
@
10 @
3
@
5
@
10
Topic-
Category
Concept-Tag Clustering
(GIS)
Clustering
(ODP)
Clustering
(k=10)
Ontology Query Logs
(GIS)
Query Logs
(k=10)
Query Logs
(ODP)
Related
Search
(Google)
Related
Search
(Yahoo)
Related
Search
(Bing)
Lenient Strict
18 
( ) ' '
( , , ) ( | ) ( | ) (1 ( ' | ))
xQuAD
s S q d D
Div q d D p s q p d s p d s
∈ ∈
= − −∑ ∏
  
(13) 
where D is a set of retrieved documents for a given query q, D’ is the set of previously 
selected documents, S(q) is the subtopic set of q, p(d|d’) measures the likelihood of a 
document d∈D and the selected document d’∈D’, p(d|s) measures the likelihood of the 
document d and the subtopic s∈S(q), and p(s|q) measures the likelihood of the subtopic s and 
the query q. 
5.3. Parameter Determination 
To determine a parameter k of the empirical strategy in the clustering-based and the query-logs-
based subtopic mining methods, along with a parameter ρ in the subtopic-based diversification 
algorithm, we performed 5-fold cross-validation over the fifty topics on both the TREC09 and 
TREC10, optimizing the evaluation metric α#-nDCG@5.  The parameter k was regarded as the 
number of clusters to represent the number of subtopics.  The parameter ρ affects how 
diversification is considered.  If the parameter ρ is equal to 1, then the ranking score will be 
generated according to the relevance part only.  Conversely, the diversity part is considered only 
if the parameter ρ is set to 0.  In other words, we put more weight on the diversity part to rank the 
initial retrieved documents if ρ is smaller than 0.5.  The interpolation parameter ρ employed by 
the three baselines, i.e., MMR, WUME, and xQuAD, was determined in the same way to balance 
both the relevance and the diversity.  
5.4. Evaluation on the ClueWeb09 Category B collection 
Table 10 and Table 11 list the performance of the four baseline models and the RR-based 
diversification algorithm with subtopics mined from various subtopic mining methods on the 
TREC09 and TREC10, respectively, using the ClueWeb09 Category B test collection.  The 
tendency of the two tables is similar.  The IRM model is the weakest baseline.  The experimental 
results show that MMR, WUME, and xQuAD perform better than IRM in both Table 10 and Table 
11.  This is because the Indri initial retrieval model concentrates on retrieving relevant documents 
and underestimates the search result diversification.  The baselines with the explicit approach, i.e., 
WUME and xQuAD, making use of subtopics mined by the related-search-based method with Bing, 
are more effective than MMR.  The xQuAD model performs the best among the four baseline 
models.  The α#-nDCG@5, α#-nDCG@10, and α#-nDCG@20 of xQuAD on the TREC09 are 
0.141, 0.172, and 0.195, respectively.  In addition, the α#-nDCG@5, α#-nDCG@10, and α#-
nDCG@20 of xQuAD on the TREC10 are 0.137, 0.161, and 0.180, respectively.  In the Wilcoxon 
signed rank tests, xQuAD performs significantly better than IRM and MMR on α#-nDCG at all 
measurement depths (p<0.05).  This reflects that using the explicit approach is better than the 
implicit approach, which is consistent with the study (Santos et al., 2010a).  Nevertheless, the 
performance of xQuAD cannot significantly outperform WUME. 
For the performance of the RR-based diversification algorithm, as shown in Table 10, the 
clustering-based method (k=10) performs the best among the indirect subtopic mining methods.  
It achieves the best α#-nDCG@5 0.186, significantly outperforming the strongest baseline (i.e., 
xQuAD) about by 27 percent and outperforming the other three baselines (p<0.05).  Using the 
GIS and the ODP to determine the number of clusters is lower than the empirical strategy.  The 
average number of subtopics determined by the GIS and the ODP is 4.16 and 8.86, respectively.  
If the number of clusters is decreased, then documents of different subtopics may be placed in the 
same cluster.  In such a case, the RR-based diversification algorithm will miss some subtopics if 
documents belong to those subtopics at lower ranks.  The topic-category-based and the concept-
tag-based subtopic mining methods suffer from the same problem. 
For direct subtopic mining methods, the related-search-based models using the up-to-date 
users’ search query logs from three commercial search engines (e.g., Bing, Yahoo, and Google) to 
generate subtopics are the most effective to diversify the search results.  The experiments show 
that their performance is very similar.  The related-search-based method using Bing, which 
performs the best among the direct subtopic mining methods, is significantly better than IRM and 
MMR (p-value<0.05).  The tendency of the RR-based diversification algorithm on the TREC10 
Category B test collection is the same as that on the TREC09 Category B test collection, except 
20 
As mentioned above, six different subtopic-mining methods are proposed.  The subtopic-
based diversification algorithm integrates clues of various subtopics mined by these methods.  
Table 12 lists the experimental results on the TREC09 using the ClueWeb09 Category B test 
collection, where DC(k=10) denotes the cluster-based method and k is set to a fixed number 10 in 
the k-means clustering algorithm, CT denotes the concept-tag-based method, TC denotes the topic-
category-based method, RS(B) denotes the related-search-based method using Bing, RS(Y) 
denotes the related-search-based method using Yahoo, RS(G) denotes the related-search-based 
method using Google, and RS(ALL) denotes the integration of the related search results of Bing, 
Yahoo, and Google.     
The performance of the top ten combinations in terms of α#-nDCG@5 is shown in Table 12.  
The related-search-based methods using Google combined with Bing or Yahoo achieve the best 
performance of α#-nDCG@5 0.200, which is significantly better than the strongest baseline, i.e., 
xQuAD (p<0.05).  The up-to-date users’ search query logs from commercial search engines are 
very important data sources for the subtopic-based diversification algorithm.  Integration of 
subtopics mined by other approaches, such as the topic-category-based and the concept-tag-based, 
does not improve the performance significantly. 
The subtopic-based diversification model is also better than the best RR-based diversification 
model, i.e., the RR-based diversification algorithm with subtopics mined by the Clustering (k=10) 
subtopic mining method.  The RR-based algorithm does not consider a document to be classified 
into more than one subtopic, so the performance may be decreased.  In contrast, the subtopic-
based diversification algorithm not only prefers documents with multiple subtopics (e.g., richness), 
but also considers the importance and novelty of subtopics, as mentioned at Section 3.2.2.  Table 
13 lists the experimental results on the TREC10 using the ClueWeb09 Category B test collection, 
whose tendency is similar to those on the TREC09 counterpart. 
Table 12. Performance of the subtopic-based diversification algorithm on the TREC09 using the 
ClueWeb09 Category B test collection  
TREC09 α-nDCG IA-P α#-nDCG 
 @5 @10 @20 @5 @10 @20 @5 @10 @20 
The best baseline (xQuAD) 0.196 0.246 0.287 0.085 0.097 0.103 0.141 0.172 0.195 
The best RR-based model 0.256 0.272 0.307 0.116 0.100 0.097 0.186 0.186 0.202 
RS(ALL)+DC(k=10)+CT+TC 0.269 0.286 0.326 0.108 0.084 0.083 0.189 0.185 0.205 
RS(ALL)+DC(k=10)+TC 0.269 0.283 0.328 0.110 0.082 0.086 0.190 0.183 0.207 
RS(ALL)+DC(k=10) 0.269 0.283 0.329 0.111 0.081 0.087 0.190 0.182 0.208 
RS(ALL)+CT+TC 0.276 0.304 0.335 0.109 0.096 0.085 0.193 0.200 0.210 
RS(B)+RS(Y) 0.277 0.311 0.344 0.109 0.101 0.098 0.193 0.206 0.221 
RS(ALL) 0.281 0.317 0.348 0.111 0.102 0.098 0.196 0.210 0.223 
RS(ALL)+TC 0.283 0.316 0.347 0.108 0.101 0.090 0.196 0.209 0.219 
RS(ALL)+CT 0.284 0.307 0.343 0.111 0.097 0.086 0.198 0.202 0.215 
RS(B)+RS(G) 0.285 0.305 0.337 0.114 0.100 0.095 0.200 0.203 0.216 
RS(G)+RS(Y) 0.280 0.301 0.338 0.119 0.100 0.095 0.200 0.201 0.217 
Table 13. Performance of the subtopic-based diversification algorithm on the TREC10 using the 
ClueWeb09 Category B test collection 
TREC10 α-nDCG IA-P α#-nDCG 
 @5 @10 @20 @5 @10 @20 @5 @10 @20 
The best baseline (xQuAD) 0.179 0.218 0.263 0.095 0.103 0.096 0.137 0.161 0.180 
The best RR-based model 0.250 0.285 0.317 0.148 0.131 0.114 0.199 0.208 0.216 
RS(ALL)+DC(k=10) 0.294 0.341 0.377 0.130 0.12 0.103 0.212 0.231 0.240 
RS(ALL)+DC(k=10)+TC 0.292 0.331 0.371 0.140 0.125 0.116 0.216 0.228 0.244 
RS(ALL)+DC(k=10) +CT+TC 0.293 0.311 0.362 0.139 0.108 0.097 0.216 0.210 0.230 
RS(ALL)+CT+TC 0.294 0.333 0.365 0.143 0.122 0.102 0.219 0.228 0.234 
RS(ALL) 0.299 0.337 0.370 0.147 0.123 0.100 0.223 0.230 0.235 
RS(ALL)+CT 0.308 0.322 0.371 0.141 0.102 0.095 0.225 0.212 0.233 
RS(ALL)+TC 0.302 0.330 0.369 0.150 0.119 0.101 0.226 0.225 0.235 
RS(B)+RS(Y) 0.312 0.348 0.387 0.147 0.128 0.117 0.230 0.238 0.252 
RS(B)+RS(G) 0.338 0.367 0.402 0.167 0.131 0.113 0.253 0.249 0.258 
RS(G)+RS(Y) 0.358 0.379 0.408 0.179 0.135 0.110 0.269 0.257 0.259 
22 
model significantly outperforms the Amsterdam and the ICTNET on α#-nDCG@5 and α#-
nDCG@10.  In addition, our proposed model is significantly better than the three state-of-the-art 
models on α#-nDCG@20.   
We next discuss the possible reasons for the improvement of our model over the state-of-the-
art models.  The MMR baseline is similar to the model of the Amsterdam team.  The main 
difference between the MMR baseline and their approach is that the subtopics are implicitly 
extracted by LDA and are encoded in their model.  Our model is better than the Amsterdam team 
because our diversity function combines subtopics mined by two direct methods.  The subtopics 
mined by the direct mining method are generally better than the indirect method, which is also 
demonstrated from the experimental results shown in Table 8 and Table 9.  The RR-based 
diversification algorithm with subtopics mined from the clustering-based method is similar to the 
model of the ICTNET team.  Our model performs better than the ICTNET team because their 
model does not consider the case where a document is classified into more than one subtopic and 
because the subtopics they used also are mined by the implicit method.  The xQuAD baseline 
model is the same as the diversification algorithm of the uogTr team.  Nevertheless, the initial 
document set and the subtopics used are different.  The reasons our model performs better than 
the uogTr team may be due to the initial document retrieval performance being improved.  
Moreover, our diversity function combines subtopics mined by different aspects and considers the 
relative importance of subtopics.   
 
Table 15. Comparison of the state-of-the-art models proposed in the TREC09 on the ClueWeb09 
Category B test collection  
TREC09 α-nDCG IA-P α#-nDCG 
 @5 @10 @20 @5 @10 @20 @5 @10 @20 
Amsterdam 0.232 0.250 0.281 0.086 0.079 0.071 0.159 0.165 0.176 
ICTNET 0.251 0.272 0.301 0.104 0.095 0.092 0.178 0.184 0.197 
uogTr 0.253 0.282 0.308 0.142 0.132 0.127 0.198 0.207 0.218 
Our Model 0.307 0.340 0.380 0.121 0.106 0.097 0.214*+ 0.223*+ 0.239*+▲ 
 
Table 16 lists the experimental results of the best diversification model we proposed and the 
three state-of-the-art models in the TREC10 Web Track diversity task (Clarke et al., 2010) for 
comparison.  In the TREC10, the performance of α-nDCG@5 and IA-P@5 were not reported.  
Moreover, the performance was computed over 36 of the 50 topics.  The held back topic numbers 
were 54, 61, 66, 68, 72, 78, 83, 86, 87, 90, 95, 98, 99, and 100.  The performance of the best 
model we proposed is better than the three state-of-the-art models in the TREC10.  For the 
UAmsterdam team, the documents were retrieved based on the similarity scores between the 
anchor texts of documents and a given query and were ranked by the similarity scores multiplied 
by the fusion spam percentiles (Kamps et al., 2010).  The uogTr team used the same model as 
that in the TREC09, but the subtopics were generated based on query reformulations of Bing and 
Google (Santos et al., 2010).  Unfortunately, the framework of the qirdcsuog team was not 
reported in the TREC10 proceedings.  Since the performance of each test topic for the three top 
models was not reported in the TREC10, the results shown in Table 16 are listed without 
significance tests.   
We further discuss the possible reasons our model is better than the three state-of-the-art 
models.  Our model is better than the UAmsterdam team because they only used the information 
from anchor texts to retrieve the initial document set.  The initial retrieval performance affects the 
performance of search result diversification significantly, as we discussed above.  The uogTr 
team used a similar approach as they employed in the TREC09.  The possible reasons are 
reported above.  
 
 
 
24 
Table 18. Performance of the subtopic-based diversification algorithm on the TREC10 using the 
ClueWeb09 Category A test collection 
TREC10 α-nDCG IA-P α#-nDCG 
 @5 @10 @20 @5 @10 @20 @5 @10 @20 
Baseline          
IRM 0.039 0.061 0.086 0.028 0.034 0.043 0.034 0.048 0.065 
MMR 0.044 0.067 0.093 0.029 0.036 0.046 0.037 0.052 0.070 
WUME 0.064 0.087 0.113 0.029 0.036 0.046 0.047 0.062 0.080 
xQuAD 0.085 0.108 0.135 0.034 0.039 0.050 0.060 0.074 0.093 
Subtopic-based diversification          
RS(ALL)+QL  0.246 0.269 0.290 0.119 0.101 0.078 0.183 0.185 0.184 
RS(ALL)+QL+DC(k=10)+CT  0.256 0.282 0.307 0.127 0.101 0.095 0.192 0.192 0.201 
RS(ALL)+DC(k=10)+QL  0.269 0.292 0.324 0.121 0.095 0.081 0.195 0.194 0.203 
RS(ALL)+DC(k=10)+CT  0.268 0.291 0.312 0.132 0.105 0.085 0.200 0.198 0.199 
RS(ALL)+DC(k=10)+TC  0.290 0.315 0.343 0.124 0.102 0.087 0.207 0.209 0.215 
RS(ALL)+DC(k=10) 0.280 0.306 0.332 0.135 0.114 0.097 0.208 0.210 0.215 
RS(ALL)+CT+TC 0.295 0.319 0.349 0.127 0.104 0.093 0.211 0.212 0.221 
RS(ALL)+CT  0.295 0.319 0.349 0.127 0.104 0.093 0.211 0.212 0.221 
RS(ALL)+TC 0.292 0.311 0.332 0.144 0.110 0.093 0.218 0.211 0.213 
RS(ALL) 0.326 0.343 0.365 0.158 0.123 0.100 0.242 0.233 0.233 
 
As mentioned before, the performance of the initial retrieval document set is a key factor for 
the search result diversification.  We employ the Indri search engine with the language model and 
pseudo-relevance feedback to retrieve an initial document set, and we diversify the initial 
document set via the subtopic-based diversification algorithm with subtopics mined by the related-
search-based method using Bing, Google, and Yahoo.  We further compare the best proposed 
diversification model with the three state-of-the-art models in the TREC09 Web Track diversity 
task (Clarke et al., 2009), as shown in Table 19.  The THUIR team used the BM25 retrieval 
model to retrieve relevant documents and clustered these documents.  Each cluster was taken as a 
probable subtopic, and the IA-SELECT algorithm (Agrawal et al., 2009) was employed for 
diversifying search results (Li et al., 2009).  The msrc team proposed a retrieval system that 
considered three features, BM25 score, PageRank, and the matching anchor count, and estimated 
ranking scores for documents by linear combination of the three factors.  The weights of the three 
features were trained by their own search engine logs.  Then, they diversified the top ranks 
documents based on “host collapsing” (Craswell et al., 2009).  The MSRAsia team proposed a 
search result diversification algorithm that used subtopics mined from anchor texts, search results 
clusters, and sites of search results.  They used the BM25 retrieval model to retrieve an initial 
document set and employed a greedy algorithm to iteratively select the best document from this set 
to maximize the coverage of subtopics for search result diversification (Dou et al., 2009).  The 
symbols *, +, and ▲, in Table 19 show that the improvement over the three state-of-the-art models, 
the THUIR, the msrc and the MSRAsia, respectively, is statistically significant (p<0.05).  As 
shown in Table 15, the best model we proposed significantly outperforms than the THUIR on α#-
nDCG at all measurement depths and the msrc on α#-nDCG@5.   
We next discuss the possible reasons for the improvement of our model over the state-of-the-
art models.  The used subtopics of the THUIR team were indirectly extracted by document 
clustering.  Our model is better than the THUIR team because our diversity algorithm combines 
subtopics mined by the direct mining methods.  The model of the msrc team concentrated on 
relevant documents retrieved, and diversified search results only considered the host names of the 
retrieved documents.  Our model performs better than the msrc team because our proposed 
diversification algorithm not only keeps the quality of relevancy, but also re-ranks the retrieved 
documents to cover multiple and important subtopics.  The MSRAsia used the BM25 retrieval 
model to retrieve the initial document sets.  Our model performs better than the MSRAsia team 
because of the initial document set and the parameter optimization.  As shown in Table 14, the 
initial document set by the language model and pseudo-relevance feedback is better than that using 
the BM25 retrieval model.  In addition, the parameters of the object function in the subtopic-
based diversification algorithm are optimized. 
26 
quality and less duplication.  The performance of the subtopic-based diversification algorithm is 
better than the RR-based diversification algorithm.  
Future directions include how to integrate other knowledge resources into the diversification 
models further, such as social information, and how to extend this work to diversify Web search 
results with different languages.  How to localize the diversified models to meet users’ needs 
from different areas/countries also has to be dealt with in the future.  Moreover, we plan to detect 
duplicate subtopics after the subtopic mining phrase.  Detection of duplicate subtopics is 
expected to refine the subtopic mining performance and then enhance the performance of search 
result diversification.  
 
7. REFERENCES 
Agrawal, R., Gollapudi, S., Halverson, A., & Ieong, S. (2009). Diversifying search results. In 
Proceedings of the 2nd ACM International Conference on Web Search and Data Mining (pp. 
5-14). 
Bi, W., Yu, X., Liu, Y., Guan, F., Peng, Z., Xu, H., & Cheng, X. (2009). ICTNET at Web Track 
2009 Diversity Track. In Proceedings of the 18th Text REtrieval Conference. 
Boldi, P., Bonchi, F., Castillo, C., Donato, D., Gionis, A., & Vigna, S. (2008). The query-flow 
graph: model and applications. In Proceeding of the 17th ACM Conference on Information 
and Knowledge Management (pp. 609-618).  
Broder, A. (2002). A taxonomy of web search. SIGIR Forum, 36(2), 3-10.  
Carbonell, J., & Goldstein, J. (1998). The use of MMR, diversity-based reranking for reordering 
documents and producing summaries. In Proceedings of the 21st Annual International ACM 
SIGIR Conference on Research and Development in Information Retrieval (pp. 335-336).  
Carterette, B. (2009). An Analysis of NP-Completeness in Novelty and Diversity Ranking. In 
Proceedings of the 2nd International Conference on Theory of Information Retrieval: 
Advances in Information Retrieval Theory (pp. 200-211).  
Carterette, B., & Chandar, P. (2009). Probabilistic models of ranking novel documents for faceted 
topic retrieval. In Proceedings of the 18th ACM Conference on Information and Knowledge 
Management (pp. 1287-1296).  
Chang, Y. S., He, K. Y., Yu, S., & Lu, W. H. (2006). Identifying User Goals from Web Search 
Results. In Proceedings of the 2006 IEEE/WIC/ACM International Conference on Web 
Intelligence (pp. 1038-1041).  
Chen, H., & Karger, D. R. (2006). Less is more: probabilistic models for retrieving fewer relevant 
documents. In Proceedings of the 29th Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval (pp. 429-436).  
Clarke, C., Craswell, N., & Soboroff, I. (2009). Overview of the TREC 2009 Web Track. In 
Proceedings of the 18th Text REtrieval Conference. (pp. 1-9).  
Clarke, C. L. A., Craswell, N., Soboroff, I., & Cormack, G. V. (2010). Overview of the TREC 
2010 Web Track. In Proceedings of the 19th Text REtrieval Conference (pp. 1-9). 
Clarke, C. L. A., Kolla, M., Cormack, G. V., Vechtomova, O., Ashkan, A., Büttcher, S., & 
MacKinnon, I. (2008). Novelty and diversity in information retrieval evaluation. In 
28 
Radlinski, F., & Dumais, S. (2006). Improving personalized web search using result 
diversification. In Proceedings of the 29th Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval (pp. 691-692).  
Radlinski, F., & Joachims, T. (2005). Query chains: learning to rank from implicit feedback. In 
Proceedings of the 11th ACM SIGKDD International Conference on Knowledge Discovery in 
Data Mining (pp. 239-248).  
Rafiei, D., Bharat, K., & Shukla, A. (2010). Diversifying web search results. In Proceedings of the 
19th International Conference on World Wide Web (pp. 781-790).  
Rose, D. E., & Levinson, D. (2004). Understanding user goals in web search. In Proceedings of 
the 13th International Conference on World Wide Web (pp. 13-19).  
Santos, R. L. T., Macdonald, C., & Ounis, I. (2010a). Exploiting query reformulations for web 
search result diversification. In Proceedings of the 19th International Conference on World 
Wide Web (pp. 881-890).  
Santos, R. L. T., Macdonald, C., & Ounis, I. (2010b). Selectively diversifying web search results. 
In Proceedings of the 19th ACM International Conference on Information and Knowledge 
Management (pp. 1179-1188).  
Santos, R. L. T., McCreadie, R. M. C., Macdonald, C., & Ounis, I. (2010). University of Glasgow 
at TREC 2010: Experiments with Terrier in Blog and Web Tracks. In Proceedings of the 
19th Text REtrieval Conference.  
Song, R., Zhang, M., Sakai, T., Kato, M. P., Liu, Y., Sugimoto, M., Wang, Q., (2011). Overview 
of the NTCIR-9 INTENT Task. In Proceedings of the 9th NTCIR Workshop Meeting.  
Spärck-Jones, K., Robertson, S. E., & Sanderson, M. (2007). Ambiguous requests: implications for 
retrieval tests, systems and theories. SIGIR Forum, 41(2), 8-17. 
Turtle, H., & Croft, W. B. (1991). Evaluation of an inference network-based retrieval model. ACM 
Trans. Inf. Syst., 9(3), 187-222. 
Vee, E., Srivastava, U., Shanmugasundaram, J., Bhat, P., & Yahia, S. A. (2008). Efficient 
Computation of Diverse Query Results. In Proceedings of the 24th IEEE International 
Conference on Data Engineering (pp. 228-236).  
Wang, J., & Zhu, J. (2009). Portfolio theory of information retrieval. In Proceedings of the 32nd 
International ACM SIGIR Conference on Research and Development in Information 
Retrieval (pp. 115-122).  
Welch, M. J., Cho, J., & Olston, C. (2011). Search result diversity for informational queries. In 
Proceedings of the 20th International Conference on World Wide Web (pp. 237-246).  
Yin, D., Xue, Z., Qi, X., & Davison, B. D. (2009). Diversifying Search Results with Popular 
Subtopics. In Proceedings of the 18th Text REtrieval Conference. 
Yue, Y., & Joachims, T. (2008). Predicting diverse subsets using structural SVMs. In Proceedings 
of the 25th International Conference on Machine Learning (pp. 1224-1231).  
Zhai, C., & Lafferty, J. (2004). A study of smoothing methods for language models applied to 
information retrieval. ACM Trans. Inf. Syst., 22(2), 179-214. 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                           100年 12月 22日 
報告人姓名  陳信希 
 
服務機構
及職稱 
 
國立台灣大學資訊工程學系教授
 
     時間 
會議 
     地點 
100年 12月 18日-12月 20
日 
杜拜 
本會核定
補助文號
 
會議 
名稱 
(中文)第七屆亞洲資訊檢索協會會議 
(英文) The Seventh Asia Information Retrieval Societies Conference 
發表 
論文 
題目 
(中文) 以區域模型研究查詢相關評分聚集 
(英文) Query-Dependent Rank Aggregation with Local Models 
 
附
件
三
 
          
                
              圖三、會場                         圖四、交流活動 
 
報告人這次以 AIRS steering committee member的身分，在第一天下午第一場議程結
束後，報告 AIRS 2012 bid 的結果。明年 AIRS 共有新加坡 Institute for Infocomm 
Research、和中國天津大學兩個組織提出規劃，參與投標(bid)，最後由天津大學勝出。
由於明年的主辦單位並未出席本次會議，報告人代為歡迎大家到天津大學。在隔天中午
與本次會議主席 Farhad Oroumchian 博士進行午餐會談，討論事項包括：今年論文投稿
數和投稿區域/國家統計、審稿流程檢討、論文接受區域/國家統計、以及經費預算開銷
等，建議提供正式文件給 AIRS steering committee，作為後續主辦會議參考。在最後一
天，會議結束時，也代表 AIRS steering committee向主辦單位致謝。會後並有一項交流
活動，圖四是報告人與兩位韓國 KAIST 教授、一位京都大學博士生、和一位外籍學者
合影。 
  
三、建議 
 AIRS 是資訊檢索領域亞洲區國際會議，報告人是台灣學術界在這項組織的代表，
建議能持續的參與，才能發揮具體影響力。 
 
四、攜回資料名稱及內容 
 Conference會議論文集隨身碟一個、和 LNCS論文集一冊。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
98 年度專題研究計畫研究成果彙整表 
計畫主持人：陳信希 計畫編號：98-2221-E-002-175-MY3 
計畫名稱：以機器學習法研究資訊檢索相關性、多樣性、多語性排列問題 
量化 
成果項目 
實際已達
成數（被接
受或已發
表） 
預期總達成
數(含實際
已達成數)
本計畫
實際貢
獻百分
比 
單位
備註（質化說明：如數個
計畫共同成果、成果列為
該 期 刊 之 封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報
告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次
 
期刊論文 1 1 100% 
(1) Ming-Feng Tsai, 
Yu-Ting Wang and Hsin-Hsi 
Chen (2011).  ’Learning a 
Merge Model for 
Multilingual Information 
Retrieval.’ Information 
Processing and Management, 
47(5), September 2011, 
635–646. 
(2) Chieh-Jen Wang, 
Yung-Wei Lin, Ming-Feng 
Tsai and Hsin-Hsi 
Chen, ’’Mining Subtopics 
from Different Aspects for 
Diversifying Search 
Results.’’ Information 
Retrieval, Major Revision.
研究報告/技術報
告 0 0 100%  
國外 論文著作 
研討會論文 3 3 100% 
篇 
(1) Chieh-Jen Wang, 
Hung-Sheng Huang, and 
Hsin-Hsi Chen 
(2012). ’Automatic 
Construction of An 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 5 5 100% 以共同發表論文人力計算 
博士生 2 2 100% 以共同發表論文人力計算 
博士後研究員 0 0 100%  
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次
 
其他成果 
(無法以量化表達之
成果如辦理學術活
動、獲得獎項、重
要國際合作、研究
成果國際影響力及
其他協助產業技術
發展之具體效益事
項等，請以文字敘
述填列。) 
(1) 主持人是台灣學術界在 AIRS (Asia Information Retrieval Societies)這項
國際組織的代表。 
(2) 主持人獲得 2012 Google Research Award。 
(3) 主持人是 ACL 2012 Area Chair。 
(4) 主持人是 ICADL 2012 Program Chair。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
