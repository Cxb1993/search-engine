 2 
 
文字式的關鍵字(Keyword)檢索也大部分針對單純
背景的投影片進行文字處理。本計畫即針對此一問
題，提出針對複雜背景教學視訊串列擷取文字之新
方法，以利教學視訊串列之文字關鍵字檢索。 
此一研究的相關主題有投影片轉換偵測、前景
擷取及文件二值化。投影片轉換一般發生在講者操
作鍵盤更換投影片，而投影片轉換偵測類同於場
景偵測(Shot boundary detection)。傳統上場景偵測
定義為影格間亮度或顏色分布產生差異[14-18] 。
但是投影片轉換一般發生在文字變化處，而背景模
板維持不變，所以傳統方法無法直接延用。文獻[19]
即提出以正旋轉換係數 DCT(DCT, Discrete Cosine 
Transform)為基準投影片轉換偵測法。 
在安全監控應用研究，常以傳統之移動物件偵
測[20-22]進行非法入侵者。而在數位學習視訊應用
研究，則以前景偵測擷取(Foreground extraction)進
行視訊與外部文件之同步定位。此一需求，可藉由
檢視內容差異、背景顏色及空間位置分布[23-25]
而達成。但是這種方式，只適用於內含圖像或圖表
之投影片之比對，但不適用於內含大部分是文字的
投影片。另一方面，視訊文字偵測[26-28]是一熱門
研究主題，而文字辨識則可視為文字偵測後之後處
理[26]。在文獻[27,28]中利用機器學習之分類器完
成文字切割之任務。Wang 等[19]發展一套包括有
主題式件偵測、視訊文字分析及則視訊與外部文件
之同步定位之數位學習視訊瀏覽系統。在該系統
中，視訊文字依其字體可分為標題及內文兩部分，
雖然針對標題文字該系統可得相當高的正確率
[19]，但針對字體較小之內文部分，該系統之文字
辨識率尚有改進的空間標。 
由於二值化(Binarization)是文字辨識成功與否
之關鍵技術，如何選取適當的門檻值則是二值化過
程必要的步驟。Otsu 法[29]是耳熟能詳方法，而二
均值法[30]也是常被引用之方法。然而上述方法都
無法有效解決在數位學習視訊中常遇到的光源背
景不均勻的問題。 
本計畫提出一針對複雜背景教學視訊串列擷
取文字之新方法，以利教學視訊串列之文字關鍵字
檢索。因為教學投影片的背景複雜且多樣化，甚至
與文字特性相近，所以設計針對教學視訊串列之前
景切割法，以擷取前景文字區塊，是本計畫研究主
題之一。另因教學視訊串列的解析度偏低，所以如
何提升文字品質，以利後續文字辨識，也是本計畫
的另一重要課題。 
 
三、研究方法 
1. 概述 
本計畫提出一針對複雜背景教學視訊串列擷
取文字之新方法，以利教學視訊串列之文字關鍵字
檢索。所提方法概述如下。首先因為教學視訊錄製
環境變異很大，特別是在光源照射的變異，所以光
源的正規化是進行教學視訊處理之必要前處理以
提升影像品質，及後續處理之穩定性。接下來，投
影片關鍵影格(Key frame)建置也有其必要性，因為
後續處理若只須在關鍵影格上執行，將大大縮減後
續處理的時間。 
在前景元件擷取上，本計畫提出以 SLEP 
(Structured Local Edge Pattern) [31]為區域特徵的
比對法，以區分前景與背景的差異，進而建置背景
模式，甚至分析時空的相關性，進一步修正背景模
式，以得更細緻的背景模式系。最後再以所提之適
性化二值化法，對可能之文字前景元件進行二值
化。最後再利用普被採用之商業文字辨識軟體
(Omni-Page) [32]進行文字辨識。 
值得一提的是，本計畫之所以採取以背景分析
進行文字擷取，而非採用一般之文字偵測法，是因
為如前所述教學視訊串列其背景有時相當複雜，甚
至具有文字特性，使得背景被視為文字，而降低文
字辨識精確度(Precision)，如圖一所示。另一方面
因為 Omni-Page 具有圖文分離的能力，所以本計
畫希望整合 Omni-Page 的功能，達到針對複雜背
景教學視訊串列正確擷取文字之目標。 
 
2. 投影片關鍵影格建置 
本計畫提出以 SLEP [31]為區域特徵的表示
法，以描述教學視訊中紋理變化情形，以區分前景
與背景區分前景與背景的差異，進而建置背景模
式。SLEP 之定義如圖二所示，是用以編碼邊緣點
之四個方向的分布結構，其特徵為度是 32。 
設一教學視訊Ft為一二維影格序列(a sequence 
of frames)，在此 t 為時間索引(index) 。現將每一
在時間軸上的影格，如圖三所示的，均勻切割為在
空間軸上的 BNsd × BNsd個區塊，在此計畫 BNsd是
設定為 4。針對每一區塊計算其 SLEP 之值方圖， 
SLEPh。計算空間上固定位置 區塊，但在時間軸
上前後(即索引 t 及 t+1)之 SLEPh 之個別差量，再
將 BNsd × BNsd個區塊的 SLEPh 個別差量累加，即
可得投影片轉換量測值，STDt。STDt 值若超過一
門檻值，THstd，即判定此影格 Ft 是一轉換投影片，
其數學公式表列於方程式(1)。 
 
    
 
 

 


 
 

otherwiseFALSE
 ifTRUE
2
1 1
,1,
2
stdt
t
sd
BN
i
FeatureDim
d
itit
t
THSTD
Transitive
BN
dd
STD
sd
F
BB
 (1)
 
將轉換投影片偵測出來後，轉換投影片 STs 與 
STs+1 的所有影格可取其平均值，作為關鍵影格
KFs。詳而論之，若轉換投影片 STs 與 STs+1所對
應的影格分別為 Fft 及 Fnt，則同屬關鍵影格 KFs
的連續影格個數，FrameNumbers，如方程式(2)所
示。而關鍵影格 KFs 之計算公式則如方程式(3)所
示。圖四是一關鍵影格建置之範例。 
 
ftntrFrameNumbe s  (2)
 4 
 
擷取。詳而論之，每張關鍵影格上各區塊之 SLEPh 
特徵，Ws,i，直接與背景模式之 SLEPh 特徵，Mi,j，
進行比對，比對公式詳方程式(9)，且若經由方程
式(9)計算出之相似值大於門檻值，THb，則判定此
區塊可能是前景文字區塊。圖九是前景分類之結
果，在此圖中前景及背景分別以原亮度值及紫紅色
加以標記。 
 
   
 

 
 

otherwiseFALSE
 ifTRUE ,
,
1
,,
bis
is
FetureDim
d
isis
THBD
Foreground
dBMdBD
W
W
(9)
 
4 文字擷取 
依據節 3 所述方法，擷取出前景部分，其中也
包括文字部分。接下來就必須將文字所在位置加以
確定，以利後續文字辨識。因為前景可能分散在關
鍵影格的不同位置，所以必須以連接物件標記法
(Connected component labeling)將各前景元件標記
出來，再將其作解析度提升，以提升影像品質，再
進行適調性之二值化法(Adaptive binarization)，以
增強文字辨識的正確率。 
 
4.1 文字元件擷取 
詳而論之，前述連接物件標記法是以相鄰區塊
標記法，將連接物件，CCn，標記出來。再針對
CCn，找出其外廓部分，亦即蓋括 CCn之最小方塊， 
但因為文字有諸多變形，有些文字部分仍會溢出外
廓小方塊。所以本計畫將外廓小方塊主動多增一區
塊，以確保文字部分完整蓋括在可能文字區塊內，
以提升文字辨識的正確率。圖十是文字元件擷取的
範例。 
 
4.2 解析度提升 
 
為節省教學視訊傳送之成本，教學視訊影像一
般而言都是低解析度，可能會造成文字辨識的困難
度。所以在進行文字辨識前，解析度提升有其必要
性。本計畫是採線性內插法達到解析度提升之需
求。 
 
4.3 適調性二值化法 
大部分的商用文字辨識系統皆以二值化影像
為輸入影像，而且二值化影像之品質是影響辨識率
之關現因素。所以本計畫提出一適調性二值化法，
針對各區塊自動選擇適宜之門檻值。首先本計畫先
針對各文字區塊，利用全域 2-均值法 (Global 
2-Means) (即將K-均值法之K值定為2)將所有文字
區塊內之影像點歸為黑白兩類，以達到二值化之目
的。圖十一(b)是文字區塊二值化結果，很明顯的
其成效不彰，主要的原因是教學視訊的背景光源不
均。所以本計畫提出一適調性二值化法，即區域
2-均值法(Local 2-Means)，進行二值化處理。詳而
論之，本計畫將 2-均值法在較小之 20×20 區塊內
執行。由圖十一(c)所示，所提區域 2-均值法確實
能提升二值化之成效。在第四項實驗結果部分，將
以辨識正確率之定量方式，進一步證實所提二值化
法確實有效。 
 
四、實驗結果與討論 
所提方法已實作在具有 2.67 GHz Core Intel 
Q8400 CPU 及 4 Gigabytes DDR2 SDRAM 之
GIGABYT 母版(Motherboard) 。所採作業系統為
Microsoft Windows 7 professional version。所有程
式皆以 C++ 語言編撰並使用開放原碼 (Open 
source)之 OpenCV 程式庫並以 Microsoft Visual 
Studio 2010 下之編譯器編撰。輸入視訊是以 SONY 
HDR-XR350 High Definition HDD Handycam 錄
製，並以 MTS 格式儲存。本計畫採現有商用軟體
OmniPage OCR system [32] 進行文字辨識。 
本計畫在相同地點但不同錄製環境下錄製 14
種教學視訊，只要整張投影片可完全入鏡，相機角
度可不同或光源變化可不同。每一視訊之投影片背
景將不相同，詳細資料見表二。為模擬壓縮視訊情
況，教學視訊是以低解析度之壓縮影像方式儲存。
教學視訊錄製時間不同，端視演講者報告時間而有
不同。一般而言，每一教學視訊包括有 10 到 30
張的投影片。錄製當中，可能會有異物干擾錄製內
容，例如講者的手可能入鏡。實驗設計是以驗證三
項主題為基準，即關鍵影格建置成效、前景分類成
效及文字辨識成效。 
就關鍵影格建置成效而言，若將縮減率
(Reducing rate)定義為關鍵影格個數與原視訊總影
格數之比值，則經由此一處理，縮減率可高達
98%。至於關鍵影格之偵測正確性，則以人工標記
之關鍵影格 (及對應每張投影片 )為正確基準
(Ground truth)，進行評估。召回率(Recall rate)、正
確率(Precision rate)及 F-分數(F-score)分別定義如
下。召回率(R)是正確偵測關鍵影格數與實際真實
關鍵影格數之比率; 正確率(P)是正確偵測關鍵影
格數與實際偵測關鍵影格數之比率;而 F-分數(F)
則是 P 與 R 之函數，其定義如方程式(10)。 
PR
PRF 
 2  (10)
 
表三列出在使用不同特徵及不同門檻值時，14
種教學視訊之平均召回率、正確率及 F-分數。由
表三發現，不同門檻值對於召回率、正確率及 F-
分數影響不大。所以本計劃選擇對應召回率較高之
門檻值，即 THstd=0.05 及 THsd =0.1。至於另一特
徵 LBP [33]是屬於亮度特徵(Intensity-based)，與本
研究採用之邊緣特徵(Edge-based) SLEP 性質不
同。由表三發現，所提 SLEP 特徵在關鍵影格建置
較 LBP 有較高的召回率、正確率及 F-分數。分析
其原因，應該是邊緣特徵對於不均勻背景有較高容
忍度;另外在表現文字特性上邊緣特徵 SLEP 較亮
度特徵有較高的分辨能力。 
 6 
 
http://penance.is.cs.cmu.edu/meeting_room/. 
[14] B.L. Yeo and B. Liu, “Rapid scene analysis on 
compressed video,” IEEE Trans. Circuits and 
Systems for Video Technology, vol. 5, pp. 
533-544, 2002. 
[15] A. Nagasaka and Y. Tanaka, “Automatic video 
indexing and full-video search for object 
appearances,” Proc. IFIP Second Working 
Conf. Visual Database Systems II, pp. 113-127, 
1992. 
[16] S.X. Ju, M.J. Black, S. Minneman, and D. 
Kimber, “Summarization of videotaped 
presentations: automatic analysis of motion 
and gesture,” IEEE Trans. Circuits and 
Systems for Video Technology, vol. 8, pp. 
686-696, 1998. 
[17] D. Zhang, W. Qi, and H.J. Zhang, “A new shot 
boundary detection algorithm,” Proc. IEEE 
Rim Conference on Multimedia, 
vol. 2195, pp. 63-70, 2001. 
[18] R. Zabih, J. Miller, and K. Mai, “A 
feature-based algorithm for detecting and 
classifying scene breaks,” Proc. ACM Int’l 
Conf. Multimedia, pp.189-200, 1995. 
[19] F. Wang, C.W. Ngo, and T.C. Pong, 
“Structuring low-quality videotaped lectures 
for cross-reference browsing by video text 
analysis,” Pattern Recognition, vol. 41, pp. 
3257-3269, 2008. 
[20] Y.T. Chen, C.S. Chen, C.R. Huang, and Y.P. 
Hung, “Efficient hierarchical method for 
background subtraction,” Pattern Recognition, 
vol. 40, pp. 2706-2715, 2007. 
[21] Y. Zha, D. Bia, and Y. Yang, “Learning 
complex background by multi-scale 
discriminative model,” Pattern Recognition, 
vol. 3, pp. 1003-1014, 2009. 
[22] M. Heikkila and M. Pietikainen, “A 
texture-based method for modeling the 
background and detecting moving objects,” 
IEEE Trans. Pattern Analysis and Machine 
Intelligence, vol. 28, pp. 657-662, 2006. 
[23] T.F. Syeda-Mahmood, “Indexing for topics in 
videos using foils,” Proc. IEEE Int’l Conf. 
Computer Vision and Pattern Recognition, vol. 
2, pp. 312-319, 2000. 
[24] B. Erol, J.J. Hull, and D.S. Lee, “Linking 
multimedia presentations with their symbolic 
source documents: algorithm and 
applications,” Porc. ACM Int’l Conf. 
Multimedia, 2003. 
[25] T. Liu, R. Hjelsvold, and J.R. Kender, 
“Analysis and enhancement of videos of 
electronic slide presentations,” Proc. IEEE 
Int’l Conf. Multimedia, vol. 1, pp. 77-80, 
2002. 
[26] R. Lienhart and A. Wernicke, “Localizing and 
segmenting text in images and videos,” IEEE 
Trans. Circuits and Systems for Video 
Technology, vol. 12, pp. 256-268, 2002. 
[27] H. Li, D. Doermann, and O. Kia, “Automatic 
text detection and tracking in digital video,” 
IEEE Trans. Image Processing, vol. 9, 
pp.147-156, 2000. 
[28] D. Chen, J.M. Odobez, and H. Bourlard “Text 
detection and recognition in images and video 
frames,” Pattern Recognition, vol. 37, pp. 
595-608, 2004. 
[29] O. Otsu, “A threshold selection method form 
gray-level histogram,” IEEE Trans. System, 
Man and Cybernetics, vol. 9, no. 1, pp. 62-66, 
1979. 
[30] J. Park, G. Lee, E. Kim, J. Lim, S. Kim, H. 
Yang, M. Lee, and S. Hwang, “Automatic 
detection and recognition of Korean text in 
outdoor signboard images,” Pattern 
Recognition Letters, vol. 31, no. 12, pp. 
1728-1739, 2010. 
[31] S.Z. Su, S.Y. Chen, S.Z. Li, and D.J. Duh, 
“Structured local edge pattern moment for 
pedestrian detection,” Proc. Int’l Conf. Image 
Analysis and Signal Processing, pp. 556-560, 
2010. 
[32] OmniPage Pro 17 
http://www.scansoft.com/omnipage/ 
[33] T. Ojala and M. Pietikainen, “Unsupervised 
texture segmentation using feature 
distributions,” Pattern Recognition, vol. 32, 
no. 9, pp. 477-486, 1999. 
[34] S. Su, S.-Y, Chen, S. Li, S.-A. Li, and D.-J. 
Duh, “Structured local binary Haar pattern for 
graphics retrieval,” Proc. of IEEE 
International Conference on Intelligent 
Computing and Intelligent Systems, Xiamen, 
China, pp. 670-674, 2010. 
[35] C.-Y. Shiao and S.-Y. Chen, “Text extraction 
for lecture videos,” Proc. of IPPR Conference 
on Computer Vision, Graphics and Image 
Processing, pp. , 2011 
[36] S. Su, S.-Y. Chen, S. Li, S.-A. Li, and D.-J. 
Duh, “Structured local binary Haar pattern for 
pixel-based graphics retrieval,” Electronics 
Letters, vol. 46, no. 14, pp. 996-998, July, 
2010. 
[37] C.-Y. Shiao and S.-Y. Chen, “Text extraction 
of lecture videos without text detection,” 
submitted to Journal of Electronic Imaging for 
possible publication. 
 
 
 8 
 
 
(a) (b) 
圖八、背景模式範例(a)未執行背景模式再修正前
之背景模式; (b) 已執行背景模式再修正後之背
景模式。 
 
 
(a) (b) 
圖九、前景擷取範例 (a) 原關鍵影格; (b)前景分
類結果。 
 
  
(a) (b) 
 
(c) (d) 
圖十、文字元件擷取範例(a) 關鍵影格上前景分類結
果;(b)連接前景物件標記結果;(c)蓋括前景物件之最
小方塊;(d)外廓小方塊多增一區塊之文字區塊。 
 
 
 
 
 
 
 
 
 
(a) (b) 
 
(c) 
圖十一、適調性二值化範例(a)原文字區塊; (b) 全域
2-均值法之二值化結果; (c)區域 2-均值法之二值化結
果。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 10 
 
表二、錄製教學視訊範例。 
視訊名稱 錄影內容 投影片影格個數
2001_Oliva 
 
2245 
2006_Ngo 
 
1600 
2006_Wu 
 
1605 
20 7_Chen 
 
2565 
2007_Yang 
 
 290 
2009_Shen 
 
1655 
200 _Zha 
 
3010 
2010_Amiri 
 
960 
2010_Bai 
 
2060 
2010_Chiu 
 
2185 
2010_Liu 
 
1010 
2010_Phan 
 
1025 
2010_Sidir 
 
1760 
2011_Roy 
 
2245 
 
 
表三、使用不同特徵及不同門檻值時之關鍵影格建
置平均召回率、正確率及 F-分數。 
 
所提方法 
LBP THstd=0.05
THsd =0.1
THstd=0.05 
THsd =0.15 
F-分數 96.40% 96.28% 78.75%
召回率 98.30% 97.45% 94.55%
正確率 94.57% 95.13% 67.47%
 
表四、執行背景模式再修正與否及使用不同特徵之
前景分類平均召回率、正確率及 F-分數。 
 所提方法 LBP 
Standard w/o refinement 
F-分數 92.58% 84.14% 76.38%
召回率 91.83% 79.96% 85.19%
正確率 93.35% 88.78% 69.22%
 
 
 
 
 
 
 
 
 
 
 
 
 
 
表 Y04 
 
二、 與會心得 
「智慧計算與智慧系統」一直以來皆是熱門研究主題。特別是在多媒體、數位化科
技、網際網路的興起及寬頻網路的普及，促成 4C（電腦 Computer、通訊 Communication、
內容 Content、有線電視 Cable）的匯合，進而帶動數位內容產業發展之時代。因為智慧
計算與智慧系統的納入，可使數位內容之研究，除運用資訊科技將圖像、文字、影像、
語音等多媒體加以數位化，並可以智慧計算與智慧系統進行整合，以衍生更具智慧的資
訊服務，使資訊服務更具人性化及貼適性之優點。 
 
此次會議，本人發表的論文為張貼論文，論文題目是「Structured Local Binary Haar 
Pattern for Graphics Retrieval」。 本論文主要是提出一結構化區域二元 Haar 模式，以進
行有效的圖像檢索(Graphics Retrieval)。事實上影像檢索(Image Retrieval)已有豐碩的研究成
果，甚至已有商業產品，但影像檢索的技巧無法直接應用至圖像 (Graphics)檢索，因為影像
與圖像性質並不完全相同。而現有圖像檢索或辨識方法大多採用向量化(Vectorization)或輪廓
軌跡(Contour Tracing)之特徵進行檢索，但就一般圖像或圖表而言並不一定存在有此種輪廓
特徵，更何況輪廓特徵的擷取相當費時，所以本論文直接針對邊緣點，設計構化區域二元
Haar 模式特徵，進行圖像檢索法。實驗結果證實所提方法確實較現有方法有較高的檢索正
確率。 
     
因為此次發表論文是本人與廈門大學「信息科學與技術學院之智慧科學與技術系」
之「智能多媒體技術實驗室」李紹滋教授及蘇松志博士生，共同合作之成果。故此次本
人除參加國際會議，還同時與廈門大學該系主管李紹滋教授及教師，進行學術交流，同
時參觀廈門大學「智能多媒體技術實驗室」，就雙方近期合作研究之主題，做進一步的
溝通交流。希望經過長期電子郵件往返討論的研究內容，能經由面對面的當面溝通，激
盪出更多智慧火花，以期有更進一步的研究成果。 
 
綜而論之，本人深覺此行收穫良多，對於國科會贊助此行經費，深感謝意。 
 
三、 攜回資料 
1. 研討會論文集 
2. 研討會光碟 
3. 大會議程手冊 
 
= ! 
 ,?3@	24&/ ?3@2/4
,#,	,
!/ 1
B !	,/, 
 ,,,, ",  
!$ /#  $  
! + !5G6, 7! ,
     ,   H   I 56 "
   ! ,  : ! ,  
H 1I /%   ,$0 , , 
  &  ,, +     
   ,     ,  ! 	 ,
  $1,-#  ,    ,
#      " ,       
 !+  !!  
"
	 ,  /  - 0#  ! 
 !  ! 56 
         ,
 !   ! $1    ! ,
 /   #  !     A
, ! ! , "
   1, !  ,   !   !  +
  !  ! # !  ,  " +
 ! $     "  /    ,
#,
/ )2-
	
!01+3+

= !7 ?3@/$ !/  ,/ #	
,
&  ,, !E1/ ?3@/
",$<
D
G
2  4 2  4
 



!0 4  -  4  
= 
= ∑
$ 7

- =   2  42  4
G




1 4  ,
 4 
	-
 
> ⎧ 
= ⎨ 
⎩
 B = ! 7 $
  , ?3@   #  / # 7 !
	,$ !  × A+! 
//  ,	,="
G :4$ 1 1 = +
7 $ 1 1 = +
$ 4$  $  !  ! " "   # "  $ 
,  5 76 −  5 76, −  +#



 &')&'A(?)B?3
B'C	BB'@B&&A'
)/ !10
= ! B",?3	@24=	,J2/4
 !	, $ + !J24"
?3	@+

   ?3 #	
@ 2?3	@4  /  ,  , ?3@ $  	
,?	3 #@2?	3@4/#
I 576   "1/0!
 +  , "   &  ?3	@
 , # , 	 , $   
! , !# + !       
+    !$ = !24
	$+ #   # , 	 ,    ++  
?3	@$ !   
 
/, !/,7;,
?3@,?3	@>+?3	@ ! 
,$%!!  ,1  
&?3@ ?3	@ !
       
 ,  ?  G E
 
 = L    !
671

" $, 1- /
   # ,  !1/  !  A 
  +$ = ! ;
&B3?A
 'A&'
AHB? B))'B)
A = A(A @
& 2A@4 ?3@ 	BB'B(?3	@ *
&		B?=1HA'?B@@
3?)Q	
)*!	 +*!	 ,*!
           
7"7 E;7 DG: ED EE F DF; FG F; F E7; F7; F;
7" E 7 EF EE; F: D:F DG F: F; DE DE; F;D
"7 EE  GE FG7 F7F DG ED F; F:7 EGG D77 FD
" E;G ;E7 7: EF: F7 E DG F:: F:: F D F;E
"E E7 :7G D: E EFE ;E :: FF F7 ;;; :;F FD
E" E :; 7FG E FG ;; ; F7; F7F ;DE :GD F:E
E"E DF G; 7F7 E7D EE F; :F EF EFE ::; F7 F7
&B3?A

 'A&'
AHB?B))'B)
A(A' B
B
A*
&	HB'
B)A ;GB(@A'&'3B&
R	
)*!	 +*!	 ,*!
           
7"7 EE DF EGF E7: D:; DEF FG:G FGE DDED E: F7:E FF;
7" D ;D E:E E;E DF;: D; D;DE F; E:D DF;: DED F:;D
"7 D7:: D77 GF EDG DF;: D: E7D F; ED7 E7 D7G F:FF
" DEGE ;FF7 7:7 EE E;F E7D F;7 FD: EF: D7; DDG F:FF
"E DFF : D;E EG EDEF ;7 ::7 F7GD EFFE ;;D: :;D7 FF;
E" DF7 :DG 7FG7 EE; EE ;:F  F F:: ;FGE :: F:FF
E"E EGG ;7 7F7 ED7 EDE :GGE :7: FG:G FG ::EF E7 F7:E
&B3?A


 'A&'
AHB? B))'B)
A(A'B?&B(@A@@A'
A*
&	@A'&'3B&
G;R	
)*!	 +*!	 ,*!
           
7"7 ;7: DGDD E; E:D FE DF FG7 F7:E 7;G; E7EE F7:E F:DE
7" 7G: :F E:E EG 7DFD D;;D D;DF F:; F7; DF DE;G F;7
"7 77;; D: GF EEG 7D; D7G E:E F:; : EGD D7:: F;7
" DD ;FD ;F EE;7 :FD DE; EEF FF; E DF D7E F;:
"E ;;F; :7EG D: E77 D77 ;7:G :G F77E DFG ;;F; :;G F7
E" ;:7E :DF 7EG ED7D D: ;:FG 7 F7 DEGE ;EED :GD F:;D
E"E DG;  7FG7 E; EGG :G7F :E EFDD E:;; :;GF F7; F77E
&B3?A
H 'A&'
AHB? B))'B)
A*
&	1HA'?B@@
 3?)Q
  ! "
           
7"7 E7G: DGFE FD EDE FG:G DD DE7F F:;D F7:F E:7 E7EE F;E7
7" DFD; :GF EG ED7D EEG D7; EEF F; EFFE D;;D D F:DE
"7 E77; E ;:: EF: EFDD D:F; D F:DE FGE DE;G :G F7:
" EGG ;D7G ;DE EEF: EEF: E ;D F F7 F FG F:
"E DE;G :: 7F7 E:FD EDG ;;D  F7 EF; ;:FG :GGE F7:E
E" DFF :G 7E EG EE;7 ;DD 7F:: F7 F; ;; 7;D F;
E"E D;FF 7F77 7DFD EG E:FD F7; E EE EE :7D 7 FG
H ))?

B+,?3	@/  ! ,
	?3@     &,, +
,?3	@/+/#+ " 
>+ !A@	
 ?3@ ?3	@   +    #
   =   /    " 
 !   + ,    +  1 !
+  + !! -#0#$
B)Q*?A(>A&
&  $0 $  /#     
=  , )  2GEDDF4 ( @!
= ,
  ,	 !A ,)  2
7GGFG7GG74     &!#
' =  2I)7GGFGEGGB
C37GGFGDGFB4      )  ,
& $2)FF17771A1;;1GD74
'A=A'A)A
673
表 Y04 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                          100 年 8 月 8 日 
報告人姓名  陳淑媛 服務機構及職稱 
 
元智大學資訊工程教授 
     時間 
會議 
     地點 
自 7 月 20 日～7 月 22 日，
共 3 天 
希臘雅典 
本會核定
補助文號
 
NSC 99-2221-E-155-072- 
會議 
名稱 
 (中文) 國際智慧互動多媒體系統與服務會議 
 (英文) International Conference on Intelligent Interactive Multimedia Systems 
and Services 
發表 
論文 
題目 
 (中文) 兼具有效率類別擴充性及有效分類能力之場景分類法 
  NSC 99-2221-E-155-072- 
 (英文) Scene Categorization with Class Extendibility and Effective 
Discriminative Ability 
附件
 
表 Y04 
 
二、 與會心得 
「智慧互動多媒體系統與服務會議」是近年來熱門研究主題。特別是在電腦普及，
而使用者的分佈寬廣，從精通資訊技術之專家到非專業之一般普羅眾生，所以「單一系
統滿足眾生」(One-fits-all)的服務模式已不府合使用者的需求。「智慧互動服務系統」之
研究就是在提供個人化(Personalization)及適性化(Adaptive)之動態服務。另一方面，多媒
體、數位化科技、網際網路的興起及寬頻網路的普及，促成「多媒體服務系統」的蓬勃
發展。本會議即結合「智慧互動服務系統」及「多媒體服務系統」，再加上「智慧決策
技術」，進行研討。 
 
此次會議，本人發表的論文為報告論文，論文題目是「Scene Categorization with Class 
Extendibility and Effective Discriminative Ability」。 本論文主要是提出一依各類別分別建
置詞彙字典與影像特徵之場景分類法，因所提方法具類別特異性，故有更高的分類能
力。同時因為各類別分別建置詞彙字典與影像特徵，則當有類別新增時，原類別影像特
徵皆不受影響，所以原資料庫不需重建，亦即原影像資料庫，不需相對更新，如此可免
除重建負擔，而達有效率擴增類別之目標。實驗結果顯示，在採用單一特徵前題下，不論
是單一尺度(Single-Scale)或多重尺度(Multi-Scale)機制，所提方法都較現有方法有較高的
正確率。 
     
因為此次其中一位 keynote 是來自台灣的曾國雄教授，同樣來自台灣所以倍感親
切，也曾向曾國雄教授請教有關「智慧決策技術」之問題，希望將其他相關領域之技術，
應用於本人之研究主題，激盪出更多智慧火花，以期有更進一步的研究成果。 
 
綜而論之，本人深覺此行收穫良多，對於國科會贊助此行經費，深感謝意。 
 
三、 攜回資料 
1. 研討會論文集 
2. 研討會光碟 
3. 大會議程手冊 
 
 2
Quelhas et al. [1] proposed Bags-of-Visterms to represent invariant local features and Probabilistic 
Latent Semantic Analysis (pLSA) to model scenes. Fei-Fei and Perona [2] represented a scene image 
by a collection of local regions, denoted as codewords obtained by unsupervised learning. They also 
modified the Latent Dirichlet Allocation to model scenes. Lazebnik et al. [3] proposed a spatial 
pyramid matching based on multi-spatial resolutions for recognizing natural scene categories. Zhou et 
al. [4] used Gaussianized vector representation rather than the distribution of visual words to represent 
scene images for categorization. A comparative study was performed in Ref. [5]. 
Some researchers [6-9] have considered contextual information to improve categorization accuracy. 
A detailed survey can be found in Ref. [6]. Some researchers [10-12] have focused on compact 
codebook construction. Some researchers [13-16] have proposed novel features or learning models to 
improve categorization accuracy. 
However, most approaches assume a fixed number of classes, and none categorize images with 
efficient class extendibility while preserving discriminative ability. This capability is crucial for an 
effective image categorization system. The proposed scene categorization method provides 
category-specific visual-word construction and image representation. The proposed method is effective 
for several reasons. First, since the visual-word construction and image representation are 
category-specific, image features related to the original classes need not be recreated when new classes 
are added, which minimizes reconstruction overhead. Second, since the visual-word construction and 
image representation are category-specific, the corresponding learning model for classification has 
substantial discriminating power. Experimental results confirm that the accuracy of the proposed 
method is superior to existing methods when using single-type and single-scale features. 
Section 2 introduces three strategies for codebook construction and image representation, including 
the proposed method. Section 3 gives the experimental results. Section 4 concludes the study and 
proposes future works. 
 
2. Category-Specific Approach 
The scene categorization problem can be formulated as follows. Given an image I, a set of C scene 
categories { }C,,2,1 L , and a training set of labeled images { }CcSsIT cs ,,1,,,,1 LL === , identify 
the category of image I. Here, S is the number of labeled images for each category, i.e., the number in 
the training set T is C×S. To solve the problem, this section describes three possible strategies. The 
former two are existing methods, and the latter is the novel proposed method for achieving class 
extendibility. 
 
2.1 Whole-construction/whole-representation strategy 
 4
( )
( )
( )( )
( ) ( )( ){ } MkSCcNnknLn
n
k
c
s
c
s
NnknL
c
s
c
s
c
s ,1,,,1,s ,,,1,
,,1,
,,1, LLL
L
L ======
∑
==
xx
x
f x  (2)
 
Note that the dimensions of the C-to-1 SVM classifiers are M and d×M when using the 
histogram-based approach and the vector-based approach, respectively. The experiments in Ref. [4] 
showed that the latter approach generally achieves a higher dimension and thus a higher categorization 
accuracy rate. This study therefore applied the vector-based approach. 
 
2.2 Category-specific-construction/whole-representation strategy 
A category-specific-construction/whole-representation (C-C/W-R) strategy (Figure 1) was recently 
proposed in Ref. [9]. Experimental results in that study confirmed that this strategy is more accurate 
compared to the conventional W-C/W-R strategy [9, 10]. In the training phase of this approach, each 
training image csI is divided into N local image patches of fixed size with local features ( )ncsx . 
However, the codebook is constructed individually for each category c. Restated, only the S×N features 
of images belonging to a specific category i are fed into K-Means clustering to construct a size M 
codebook { }Mkik ,,1L=w . The C codebooks, one for each category, are then combined into one 
codebook of size C×M, denoted as { }MkCiik ,,1,,,1 LL ==w . All subsequent processes in the training 
phase and in the testing phase are identical to those in the conventional W-C/W-R approach except that 
the codebook is { }MkCiik ,,1,,,1 LL ==w  rather than { }Mkk ,,1L=w . 
Specifically, each patch with features ( )ncsx is then labeled a codeword ( )( )( )( ) )( )(inL knL cscsxxw  from the 
codebook { }MkCiik ,,1,,,1 LL ==w  according to the Euclidean distance ( ) ikcs n wx − . Each 
training image csI with patch features ( )ncsx  is represented by the feature vector  
( ) ( ) ( ) ( )MCMM cscscscscs ×+= ffffF ,,1,,,1 LL  with ( )jcsf  as defined by the following equation. 
( )
( )
( )( ) ( )( )
( ) ( )( ) ( )( ){ }
MCjSCc
NnjremknLjinLn
n
j
c
s
c
s
c
s
NnjremknLjinL
c
s
c
s
c
s
c
s
×===
====
∑
===
,,1,,1,s ,,,1
,
,,1),()(),mod()(
,,1),()(),mod()(
LLL
L
L
xxx
x
f xx  (3)
where mod(j) and rem(j) are the quotient and remainder, respectively, after dividing j by M. The above 
two strategies use the same codebook to represent whole scene images regardless of category. 
 6
Each training image csI with patch features ( )ncsx is then represented by the mean feature vector  
( ) ( ) ( ) ( )MCCM cscscscscs ,,,1,,,,1,,1,1 ffffF LLL=  with ( )kics ,f  as defined by the following equation. 
 
( )
( )
( )( )
( ) ( )( ){ } MkCiSCcNnknLn
n
ki
c
s
ic
s
NnknL
c
s
c
s
c
s
i
,,1,,,1,,1,s ,,,1,
,,1,
, ,,1, LLLL
L
L =======
∑
==
xx
x
f x  (4)
 
Notably, each scene image is represented by the same dimension used in the C-C/W-R strategy, 
regardless of whether the approach is histogram-based or vector-based, i.e., C×M and d×C×M, 
respectively. 
The proposed method clearly achieves the goal of efficient class extendibility and effective 
discriminative ability. First, since the visual-word dictionary and image representation are 
category-specific, adding new classes does not require reconstruction of all image features related to 
those in the original class so as to minimize reconstruction overhead. Second, since the visual-word 
construction and image representation are category-specific, the corresponding learning model for 
classification should have sufficient discriminatory power. 
Notably, Expectation-Maximization (EM) in Gaussian mixtures [18] rather than K-means were used 
to construct codebooks and to represent images in Ref. [4], which improved performance [4, 18]. Thus, 
EM is also used in this study, and Eq. (4) should be updated as follows. Each visual word of the 
codebook { },,,1 Mkik L=w  ,,,1 Ci L=  is first updated as one of M uni-model Gaussian components 
by the following equation. 
 
( ) MkCiN ikikik ,1, ,,,1,,; LL === Σμxw  (5)
where ik
i
k Σμ  and denote the mean vector and covariance matrix of the kth Gaussian component and 
where x denotes the local patch feature. Each patch with features ( )ncsx is then labeled for each 
category i as a codeword ( )( )i nL csi xw  from the codebook { }Mkik ,,1L=w  according to the 
probability ( ) ,,,1,,);( CinN ikikcs L=Σμx  NnMk ,1,,,1, LL == . Each training image csI with patch 
features ( )ncsx is represented by the feature vector  ( ) ( ) ( ) ( )MCCM cscscscscs ,,,1,,,,1,,1,1 ffffF LLL=  
with ( )kics ,f  as defined by the following equation. 
 
 8
Experiments are performed to test the performance of the Scene-13 and Scene-15 datasets and the 
class extendibility of the Scene-15 dataset. For each issue, experiments are repeated ten times with 100 
randomly selected images per category for training and the rest for testing. 
Tables 1 and 2 show the categorization results for the Scene-13 and Scene-15 datasets for different 
features and codebook sizes and for different strategies, respectively. The tables show that the proposed 
method is more accurate compared to the C-C/W-R strategy. Moreover, for the Scene-13 dataset, the 
highest accuracies obtained in Ref. [4] are 83.6% with 512 visual words and 84.1% with 1024 visual 
words; in the current study, the accuracies are 85.42% with 13×35=455 visual words and 86.65% with 
13×80=1040 visual words. For the Scene-15 dataset, the highest accuracy in Ref. [4] is 83.5% with 
1024 visual words; that in the current study is 84.06% with 15×80=1200 visual words. However, Ref 
[4] adopted SIFT feature for the Scene-13 dataset but SIFT and coordinate information for the 
Scene-15 dataset. The proposed method adopted only SIFT feature for the Scene-13 and Scene-15 
datasets consistently. On the other hand, although Ref. [9] adopted C-C/W-R strategy, it obtains 
87.63% and 85.16% accuracies for the Scene-13 and Scene-15 datasets, respectively. However, Ref. 
[9] adopted multi-scale features rather than single-scale feature as in the proposed method. Thus, the 
proposed method has higher accuracy compared to all existing methods when single-type and 
single-scale features are used. 
 
Table 1 Categorization results for Scene-13 dataset. 
Features HOG SIFT 
M 35 50 80 35 50 80 
C-C/W-R 79.88±0.12 80.31±0.12 81.45±0.06 83.44±0.08 85.25±0.12 85.47±0.14 
C-C/C-R 83.25±0.06 83.58±0.06 84.03±0.09 85.42±0.04 86.35±0.03 86.65±0.48 
 
Table 2 Categorization results for Scene-15 dataset. 
Features HOG SIFT 
M 35 50 80 35 50 80 
C-C/W-R 78.49±0.11 79.45±0.19 79.81±0.13 79.45±0.16 79.95±0.14 80.21±0.06 
C-C/C-R 81.45±0.08 81.99±0.09 82.63±0.10 83.02±0.09 83.42±0.08 84.06±0.07 
 
Class extendibility is verified by simulating an increase in the number of classes from the original 2 
and 5 categories to 15 categories. The experiments were conducted only on Scene-15 dataset. The 
assumptions are 2 and 5 original categories and codebook construction based only on 2 and 5 
categories. Suppose that the number of categories is increased to 15. In this case, if image 
representations of the scene images in the original 2 and 5 categories are not reconstructed, the 
codebook categories are still M×2 and M×5, respectively, rather than M×15 in the original C-C/W-R 
strategy. Categorization accuracy is clearly degraded (Table 3). However, the proposed strategy can 
achieve efficient class extendibility easily since image representation is also category-specific. 
 10
[5] A. Bosch, X. Munoz, and R. Marti, “Which is the best way to organize/classify images by content?,” Image Vision 
Computing, vol. 25, no.6, pp. 778–791, 2007. 
[6] C. Galleguillos and S. Belongie, “Context-based object categorization: A critical survey,” Computer Vision and Image 
Understanding, vol. 114, no. 1, pp. 712–722, 2010.  
[7] N. V. Hoang, V. Gouet-Brunet, M. Rukoz, and M. Manouvrier, “Embedding spatial information into image content 
description for scene retrieval,” Pattern Recognition, vol. 43, no. 9, pp. 3013–3024, 2008. 
[8] Z. Lu and H. H. S. Ip, “Combining context, consistency, and diversity cues for interactive image categorization,” 
IEEE Trans. Multimedia, vol. 12, no. 3, pp. 194–203, 2010. 
[9] J. Qin and N. H. C. Yung, “Scene categorization via contextual visual words,” Pattern Recognition, vol. 43, no. 5, pp. 
1874–1888, 2010. 
[10] J. C. Van Gemert, C. G. M. Snoek, C. J. Veenman, A. W. M. Smeulders, and J.-M. Geusebroek, “Comparing 
compact codebooks for visual categorization,” Computer Vision and Image Understanding, vol. 114, no. 4, pp. 
450–462, 2010. 
[11] J. C. Van Gemert, C. J. Veenman, A. W. M. Smeulders, and J.-M. Geusebroek, “Visual word ambiguity,” IEEE 
Trans. Pattern Recognition and Machine Intelligence, vol. 32, no. 7, pp. 1271–71283, 2010. 
[12] L. Wu, S. C. H. Hoi, and N. Yu, “Semantics preserving bag-of-words models and applications,” IEEE Trans. Image 
Processing, vol. 19, no. 7, pp. 1908–1920, 2010. 
[13] Z.-L. Sun, D. Rajan, and L.-T. Chia, “Scene Classification using multiple features in a two-stage probabilistic 
classification framework,” Neurocomputing, vol. 73, nos. 16–18, pp. 2971–2979, 2010. 
[14] A. Abdullah, R. C. Veltkamp, and M. A. Wiering, “Fixed partitioning salient points with MPEG-7 cluster 
correlograms for image categorization,” Pattern Recognition, vol. 43, no. 3, pp. 650–662, 2010. 
[15] A. Bosch, A. Zisserman, and X. Munoz, “Scene classification using a hybrid generative/discriminative approach,” 
IEEE Trans. Pattern Recognition and Machine Intelligence, vol. 30, no. 4, pp. 712–727, 2008. 
[16] H. Cheng and R. Wang, “Semantic modeling of natural scenes based on contextual Bayesian networks,” Pattern 
Recognition, vol. 43, no. 12, pp. 4042–4054, 2010. 
[17] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,” Int. J. Computer Vision, vol. 60, no.2, 
pp.91–110, 2004. 
[18] E. Alpaydin, Introduction to Machine Learning, MIT Press, 2010. 
[19] C.-C. Chang and C.-J. Lin, “LIBSVM: A binary for support vector machine,” Software available at 
http://www.csie.ntu.edu.tw/cjlin/libsvm. 
[20] N. Dalal and B. Triggs, “Histogram of oriented gradient for human detection,” Proc. Computer Vision and Pattern 
Recognition, pp. 886–893, 2005. 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：陳淑媛 計畫編號：99-2221-E-155-072- 
計畫名稱：數位學習視訊檢索與摘要 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 0 100%  
博士生 1 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
