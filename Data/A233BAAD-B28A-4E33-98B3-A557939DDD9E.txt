 II
 
Title: Scene-Based Video Segmentation. 
 
Keywords: video segmentation, scene extraction, background reconstruction. 
 
Abstract: 
  A video scene is basically a story unit that reflects the dramatic and narrative 
structure of a video. Scene extraction is the first step toward semantic understanding 
of the video. Recently, DVDs are being made with options to view a particular scene 
in the movie by providing a ``chapter selection‚Äô‚Äô menu. To obtain such a 
representation, a human observer is required to sequentially watch the video and 
locate the scene boundaries. However, due to the large amount of data, manual 
content analysis is impractical, as it is slow and expensive. Therefore, there is a need 
to develop techniques for the automatic extraction of video scenes. 
 
  In this research project, we investigate an effective approach to video scene 
extraction based on the analysis of background images. Our approach exploits the fact 
that video fames belonging to one particular scene often have similar background. 
Although part of the video frame is covered by foreground objects, background scene 
still can be reconstructed by mosaic technique. In our approach, the original video is 
segmented into some basic components called shots. Next, the color and texture 
features of all background images are integrated to compute the similarity measure 
between two shots. Finally, using knowledge in cinematography, several shots are 
grouped into one semantic-related scene. The proposed technique also provides 
improved browsing and retrieval facilities to the user of video database. 
 
Constructing mosaic of each video shot. (3) Determina-
tion of similarity measure between two shots. (4) Group-
ing correlated shots into one scene. The first two com-
ponents are implemented by our previous algorithms[8-
9]. The last two components are described in the rest of
this paper. To build a static mosaic of the background
scene, one must be able to align frames from a video
shot. In Fig. 1, the original 20th, 40th and 60th frames
of ‚ÄúSkater‚Äù sequence are shown. The constructed mosaic
is shown in Fig. 2. It is noted that our mosaic technique
is able to recover the background scene even if the camera
is moving. Using the background mosaic, the background
scene image of each video frame is reconstructed. In the
subsequent processing, we will focus on these background
image sequences.
2. Shot Similarity Measure
Color is one of the most widely used visual features
in video content analysis. Most of the scene extraction
algorithms compare color histograms between keyframes
to determine the shot similarity measure. However, due
to the statistical nature, color histogram does not cap-
ture spatial layout information of each color. When the
image collection is large, two different content images are
likely to have quite similar histograms. To remedy this
deficiency, in our approach, the distribution state of each
single color in the spatial (image) domain is also taken
into account. The color histogram for an image is con-
structed by counting the number of pixels of each color.
Each bin with non-zero count corresponds to a color ob-
ject. HSV (hue,saturation,intensity) color space is cho-
sen since it is nearly perceptually uniform. As we are in-
terested in the whole shot rather than single image frame,
only one histogram is used to count the color distribution
of all background images within a shot. Then, each bin of
the resulting histogram is divided by the frame number
of a shot to obtain the average histogram. Next, several
spatial features are calculated to characterize the distri-
bution state of each color object in each image frame.
Assuming a set of pixels S = {(x1, y1), ¬∑ ¬∑ ¬∑ , (xn, yn)} be-
long to color object ci, k is the image size and m is the
total number of 4-connected pixels in S. Then, we define
(i) density of distribution: fi1 =
n
k
(ii) compact of distribution: fi2 =
m
n
(iii) scatter:
fi3 =
1
n
‚àö
k
‚àën
j=1
‚àö
(xj ‚àí x¬µ)2 + (yj ‚àí y¬µ)2 where x¬µ =
1
n
‚àën
i=1
xi and y¬µ =
1
n
‚àën
i=1
yi
To define the fourth feature, the image is equally
partitioned into p blocks of size 16√ó16. A block is active,
if it contains some subset of S. Let the number of active
blocks in the image frame be q, we define (iv) ratio of
active block: fi4 =
q
p
After the spatial features of all background images
within a shot are computed, we take average of these val-
ues respectively. Let fi1, fi2, fi3 and fi4 be the average
feature values of color object ci within a shot, for two
color objects ci and cj, the difference of spatial distribu-
tion within a shot is defined as
Ds(ci, cj) = |fi1 ‚àí fj1|+ |fi2 ‚àí fj2|+
|fi3 ‚àí fj3|+ |fi4 ‚àí fj4| (1)
Texture refers to the visual patterns that have prop-
erties of homogeneity that do not result from the presence
of only a single color or intensity. We define the coarse-
ness of texture in an image in term of the distribution
density of edges. The Canny edge detector is used to
extract edges from an image. Again, the detected edge
image is partitioned into a set of 16√ó 16 blocks. A block
is textured, if the number of edge points in this block is
greater than a threshold. Then, we can compute the ra-
tio of textured block of each image and its average value
over a shot. The texture similarity between two shots is
determined by the minimum of the two average values.
Let A,B be the set of all color objects in shot S1
and S2 respectively, for a given u ‚àà A, its similar color
object in B is some v ‚àà B such that ||u‚àí v|| < , where
||u‚àí v|| denotes the Euclidean distance between u and v
in the HSV color space and  is a threshold. Then, (u, v)
is called a similar color pair. Let ‚Ñ¶ = {(u, v)|(u, v) ‚àà
A √ó B, (u, v) is a similar color pair}, the shot similarity
measure between S1 (with the average histogram H1) and
S2 (with the average histogram H2) is defined as
ShotSim(S1, S2) =
1
k
‚àë
(u,v)‚àà‚Ñ¶
{W (Ds(u, v))√ó
min(H1(u),H2(v))} + wt √ómin(t1, t2) (2)
where k is the image size, t1 and t2 are the average ratios
of textured block for shot S1 and S2 respectively, wt is the
weight of texture feature, Ds is the difference of spatial
features as defined in equation (1) and W is a weight
function defined as
W (x) =
1
1 + e10√óx‚àí5
The weight function W is one form of sigmoid function
which is frequently used in neural networks computation.
In our work, it is used to fuse spatial distribution infor-
mation with histogram. The construction of this weight
function is motivated by the psychophysical observation:
the effect of spatial distribution on human perception is
progressive[10]. Only when the difference in spatial fea-
tures is greater than a threshold, human perceive signif-
icant visual variation. The property of sigmoid function
fulfills this requirement.
3. Scene Extraction
Movies directors, while filming the scenes, also con-
trol the pace of film in order to sustain the viewers inter-
est. One important factor known to influence the pace
of a movie is the Montage. In order to convey idea that
has strong resonance with the viewers, Montage is widely
used as the basis to model scenes. In most situations,
Montage can be simplified as a set of cinematic rules.
2
 4
References 
[1] M. Yeung and B.L. Yeo. Segmentation of video by 
clustering and graph analysis. Computer Vision and 
Image Understanding, 71(1):97--109, July 1998. 
[2] J.R. Kender and B.L. Yeo. Video scene segmentation via 
continuous video coherence. IEEE Conference on 
Computer Vision and Pattern Recognition, pages 
367--373, Santa Barbara, 1998. 
 [3] A. Hanjalic, R.L. Lagendijk and J. Biemond. Automatic 
high-level movie segmentation for advanced 
video-retrieval systems. IEEE Transactions on Circuits 
and Systems for Video Technology, 9(4):580-588, June 
1999. 
[4] C.-W. Ngo, T.-C. Pong , H.-J. Zhang, and R.T. Chin. 
Motion-based video representation for scene change 
detection. International Journal of Computer Vision, 
50(2):127-142, November 2002. 
[5] B.T. Truong, S. Venkatesh, and C. Dorai. Scene 
extraction in motion picture. IEEE Transactions on 
Circuits and Systems for Video Technology, 13(1):5--15, 
January 2003. 
[6] Z. Rasheed and M. Shah. Detection and representation of 
scenes in videos. IEEE Transactions on Multimedia, 
7(6):1097--1105, December 2005. 
[7] M. Irani and P. Anandan. Video indexing based on 
mosaic representations. Proceedings of the IEEE, 
86(5):905--921, May 1998. 
[8] L.-H. Chen, C.-W. Su, H.-Y. Liao, and C.-C. Shih. On the 
preview of digital movies. Journal of Visual 
Communication and Image Representation, 
14(3):357--367, September 2003. 
[9] L.-H. Chen, Y.-C. Lai, C.-W. Su, and H.-Y. Liao. 
Extraction of video object with complex motion. Pattern 
Recognition Letters, 25(11):1285--1291, August 2004. 
[10] B.B. Boycott et. al. Color Vision. Cambridge University 
Press, Cambridge, U.K.,  2001. 
[11] T.S. Chua and L.Q. Ruan. A video retrieval and 
sequencing system. ACM Transactions on Information 
Systems, 13(4):373--407, October 1995
   
Table 1: Performance comparison of scene extraction 
 
Fig. 1: The 20th , 40th and 60th frames of ‚ÄúSkater‚Äù sequence. 
 
Fig. 2: The static mosaic of ‚ÄúSkater‚Äù sequence. 
 
 
 
Fig. 3: Expanding scene and its two successive 
scenes, where each box denotes a shot. 
Our approach Yeung‚Äôs approach 
Video clip 
Duration 
in minutes Recall Precision Recall Precision 
Lgerca_lisa_1 15 100% 100% 100% 86% 
Dungeons & Dragons 107 82% 90% 74% 82% 
Little Voice 96 78% 82% 72% 72% 
CBB
expanding 
scene 
ÂèØ‰æõÊé®Âª£‰πãÁ†îÁôºÊàêÊûúË≥áÔ¶æË°® 
‚ñ° ÂèØÁî≥Ë´ãÂ∞àÔßù  ‚ñ° ÂèØÊäÄË°ìÁßªËΩâ                                      Êó•ÊúüÔºö95 Ô¶é 8 Êúà 1Êó• 
ÂúãÁßëÊúÉË£úÂä©Ë®àÁï´ 
Ë®àÁï´ÂêçÁ®±Ôºö‰ª•Â†¥ÊôØÁÇ∫Ê∫ñÁöÑË¶ñË®äÔ®ÄÂâ≤ÊäÄË°ì 
Ë®àÁï´‰∏ªÊåÅ‰∫∫ÔºöÈô≥Ô•ºËèØ 
Ë®àÁï´Á∑®ËôüÔºöNSC 94-2213-E-030-008 Â≠∏ÈñÄÔ¶¥ÂüüÔºöÂΩ±ÂÉèËôïÔß§ 
ÊäÄË°ì/Ââµ‰ΩúÂêçÁ®± ‰ª•Â†¥ÊôØÁÇ∫Ê∫ñÁöÑË¶ñË®äÔ®ÄÂâ≤ÊäÄË°ì 
ÁôºÊòé‰∫∫/Ââµ‰Ωú‰∫∫ Èô≥Ô•ºËèØ 
‰∏≠ÊñáÔºö 
Âú®ÈÄôÂÄãÁ†îÁ©∂Ë®àÁï´‰∏≠ÔºåÊàë ÂÄë Á†î Áôº ‰∏Ä Á®Æ ‰ª• ÂàÜ Êûê ËÉå ÊôØ ÂΩ± ÂÉè ÁÇ∫ Âü∫
Á§é ÁöÑ Ë¶ñ Ë®ä Â†¥ ÊôØ ÊäΩ Âèñ ÊäÄ Ë°ì„ÄÇÊàëÂÄëÁöÑÊñπÊ≥ïÊòØÂü∫ÊñºÂá°ÊòØÂ±¨ÊñºÂêå‰∏Ä
Â†¥ÊôØÁöÑË¶ñË®äÂΩ±ÂÉèÁöÜÊúâÁõ∏‰ººÁöÑËÉåÊôØ„ÄÇÈõñÁÑ∂ÈÉ®ÂàÜÁï´Èù¢Ë¢´ÂâçÊôØÁâ©È´î
ÊâÄÈÅÆ‰ΩèÔºå ‰ΩÜ Êàë ÂÄë ‰ªç ÂèØ Ôßù Áî® mosaic ÊäÄ Ë°ì Ô§≠ Èáç Âª∫ ËÉå ÊôØ ÂΩ±
ÂÉè„ÄÇÂú®ÊàëÂÄëÁöÑÊñπÊ≥ï‰∏≠ÔºåÈ¶ñ ÂÖà Âéü Âßã ÂΩ± Áâá Ë¢´ ÂàÜ Ââ≤ Êàê ‰∏Ä ‰∫õ Á®± ÁÇ∫
shot ÁöÑ Âü∫ Êú¨ Áâá ÊÆµ „ÄÇÊé•ËëóÊäΩÂèñÂá∫ÊâÄÊúâËÉåÊôØÂΩ±ÂÉèÁöÑÂΩ©Ëâ≤ÂèäÁ¥ãÔß§
ÁâπÂæµÔ§≠Ë®àÁÆó shot ‰πãÈñìÁöÑÁõ∏‰ººÁ®ãÔ®Å„ÄÇÊúÄÂæåÂÜçÈÅãÁî®ÈõªÂΩ±ÁöÑÊîùË£ΩÁü•
ÔßºÊääÔ•¥Âπ≤Áõ∏ÈóúÁöÑ shot Âêà‰ΩµÊàêÊúâÂØ¶Ë≥™ÊÑèÁæ©ÁöÑÂ†¥ÊôØÁâáÊÆµ„ÄÇÊàëÂÄëÁöÑ
Á†îÁôºÊàêÊûúÂèØÔ§ÅÈÄ≤‰∏ÄÊ≠•ÊáâÁî®ÊñºË¶ñË®äË≥áÔ¶æÂ∫´ÁöÑÁÄèË¶ΩÂèäÊ™¢Ô•™„ÄÇ 
ÊäÄË°ìÔ•ØÊòé 
Ëã±ÊñáÔºö  
  In this research project, we investigate an effective approach to 
video scene extraction based on the analysis of background images. 
Our approach exploits the fact that video fames belonging to one 
particular scene often have similar background. Although part of the 
video frame is covered by foreground objects, background scene still 
can be reconstructed by mosaic technique. In our approach, the original 
video is segmented into some basic components called shots. Next, the 
color and texture features of all background images are integrated to 
compute the similarity measure between two shots. Finally, using 
knowledge in cinematography, several shots are grouped into one 
semantic-related scene. The proposed technique also provides 
improved browsing and retrieval facilities to the user of video database.
 
ÂèØÔßùÁî®‰πãÁî¢Ê•≠ 
Âèä 
ÂèØÈñãÁôº‰πãÁî¢ÂìÅ 
Â§öÂ™íÈ´îÁî¢Ê•≠ÂèØÔßùÁî®Âà∞Êú¨ÊäÄË°ìÔ§≠ÁôºÂ±ïÂá∫‰∏ÄÂ•óÈõªÂΩ±ÁöÑÁÄèË¶ΩÁ≥ªÁµ±‰∏¶
Âä†ÈÄü DVD ÁöÑË£Ω‰Ωú„ÄÇ 
