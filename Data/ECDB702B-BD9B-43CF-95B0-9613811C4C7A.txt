2方面的研究成果，所提出的語音變換技術，大
體上可分成 6 類，分別是: (a)向量量化對映
(mapping)[7,8]、(b)共振峰(formant)頻率對映
[9,10]、(c)GMM (Gaussian mixture model)對映
[11,12]、(d)ANN (artificial neural network)對映
[13]、 (e)單元挑選 (unit selection)[14,15]、
(f)HMM (hidden Markov model)對映[16,17]。
三、研究方法
3.1四個因素
一般來說，語音變換系統的製作需考慮到
四個因素，分別是語料因素、頻譜特徵參數、
音色變換方法、信號合成方法。
關於語料因素，我們採取平行語料的方
式，就是要求來源語者和目標語者都對同一份
文句內容發音，如此在作系統訓練時，比較方
便建立兩語者之間的頻譜參數的對應關係。
關於頻譜特徵參數，過去常被採用的包括
了 LPC導出的 LSF (line spectrum frequency)線
頻譜頻率係數[18]，LPC 導出的倒頻譜係數
[19]，由 STRAIGHT[20]頻譜計算出的 MFCC
(mel-frequency cepstrum coefficient)梅爾倒頻
譜係數，及 DCC 離散倒頻譜係數。在本計畫
裡，我們採用了 DCC 係數來作為頻譜特徵參
數。
關於信號合成的方法，雖然使用
STRAIGHT [20] 來對修改過的頻譜參數作合
成，可以得到非常高品質的合成語音信號，但
是 STRAIGHT 作信號合成的計算量非常大，
而無法在一般個人電腦上達成接近即時處理
的要求，因此我們採用了基於 HNM之合成方
法。
關於音色變換的處理，觀察近期許多語音
變換的研究，都採取以 GMM作為基本的對映
機制，因此我們也決定以 GMM對映機制為基
礎，再加以改進。GMM對映機制的一個重要
問題，就是變換出的頻譜參數會有頻譜包絡
(spectral envelope)過於平滑(over-smoothed)的
現象[21, 22]，所以會有語音品質退化的感覺。
一個典型的基於 GMM的對映函數，其公
式如下[11]:
( ; , )y F x   
  1
1
1
( ; , )
( )
( ; , )
x xxM
y yx xx xm m m
m m m mM
x xxm
m m m
m
w N x
x
w N x
  




 
             
  


(1)
其中 x 表示來源語者的頻譜特微向量，y 表示
變換後得到的頻譜特微向量，M 是高斯混合
N(,,)的總數，而μ及Ψ分別表示平均向量
與共變異矩陣的集合。我們認為把全部語料放
在一起，去訓練一個 GMM模型的 M (如 128)
個高斯混合的參數，容易發生一種現象，就是
一個高斯分佈為了照顧兩種音素的頻譜特
性，而使得高斯分佈的中心落在兩音素
(phoneme)之間，也使得高斯分佈變得平緩。
因此，我們提出一個解決辦法，就是先把訓練
語料裡的各個音框依其來源發音的韻母作分
群，然後再分別對各群去訓練出自己的
GMM，如此各個 GMM 就可忠實地代表它所
對應韻母的頻譜特性。
3.2 模型訓練階段
在訓練階段的主要處理步驟如圖 1 所
示。我們邀請了三位錄音者，分別到隔音錄音
室來錄製 375 句之平行語料，取樣率設為
22,050Hz，其中二位是男性，在此以 M1 和
M2作代號，而另一位是女性，以 F1作代號。
在本計畫裡，我們把M1當作來源語者，而把
M2和 F1分別作為目標語者，也就是要把M1
的語音變換成M2及 F1的語音。375個訓練語
句共可擷取出 2,926個音節音檔，我們再依音
檔檔名中的韻母拼音符號，將這些音節音檔分
成 37群。
3.2.1 DCC 係數計算
在本計畫裡，我們採用離散倒頻譜之頻譜
包絡估計方法，並且以離散倒頻譜係數(DCC)
作為頻譜參數。對於一個語音音框，我們使用
先前發展的 DCC 估計程式來計算出 40 維的
DCC係數，在此一個音框的長度設為 512個樣
本點(23.2ms)，而音框位移則設為 110 個樣本
點(5ms)。
3.2.2 分段式 GMM 之訓練
在圖 1 中經由方塊 ”Grouping into 37 
classes” 的處理之後，對於各群的音節片段
(segment)，我們就分別拿去訓練出一個由 16
4HNM based
speech synthesis
Pitch
adjusting
Selecting a GMM
Mapping with
single mixture
Estimating DCC
Converted
voice
Detect pitch freq.
Unknown spoken
sentence
Framing
圖 2 變換階段之主要處理流程
框包夾)，當作一個批次來作單一高斯混合選
取的處理。接著在圖 2 裡左右流程合併之方
塊 ”HNM based speech synthesis”，我們使用一
個基於 HNM的信號合成方法，去依據變換出
的 DCC 係數及音高頻譜，把語音信號再合成
出來。
3.3.1 分段 GMM 之選取方法
對於一個線上處理的語音變換系統來
說，輸入語音的說話內容是事先不知道的，因
此當要對一個音框的 DCC 係數作對映時，我
們如何知道 37 個 GMM 當中的那一個應被選
取?這樣的問題必須先被解決，而該問題是一
種語音辨識的問題，不過它不需要像語音辨識
那樣嚴厲地被對待，因為選取到錯誤但近似的
GMM是可以容忍的。
在語音辨識領域，隱藏式馬可夫模型
(hidden Markov model, HMM)是最常被採用的
統計模型，不過在此我們希望以所訓練出的 37
個 GMM來取代 HMM的角色，如此就不需另
外去訓練 HMM。此外，我們觀察到一個非常
接近真實的現象是，一個人不可能在一個很短
暫的時間如 100ms之內，發出多於 2個的語音
片段(在此語音片段指的是音節)。所以，我們
決定把每 20個連續的有聲音框(含蓋 100ms之
時間範圍)作為一個批次，去作 20個音框整批
的 GMM選取之處理，如此一個批次裡就只需
選出一個或二個的 GMM。本論文研發了一個
DP 為基礎的 GMM 挑選之演算法，該演算法
會依據最大似然率(maximum likelihood)去選
出一個或二個 GMM。
令第 t 個輸入音框的 DCC 係數是由第 s
個 GMM所產生的機率是 Gt(s)，其詳細計算公
式為
 
1
( ) = ( ) ; ( ), ( ) ,

 
M
x xx
t m t m m
m
G s w s N x s s (3)
其中 Wm(s)表示第 m 個高斯混合的加權，xt表
示第 t 個音框的 DCC 向量。此外，令 R(t, s)
表示從時刻 1 到時刻 t 的音框都是由第 s 個
GMM所產生的似然率對數值，而令 D(t, s)表
示從時刻 1到時刻 t的音框是由 2個 GMM所
產生，並且第 t個音框是由第 s個 GMM所產
生的似然率對數值。依據前述的定義，我們可
以推導出如下的兩個遞迴公式:
 ( , ) log ( ) ( 1, ) ,tR t s G s R t s   (4)
 ( , ) log ( )tD t s G s 
 
0 37,
max max ( 1, ) , ( 1, ) ,
v v s
R t v D t s
 
   
 
(5)
其所需設定的邊界值是，D(1, s)=0 和 R(1,
s)=G1(s)，s=0, 1, ..., 36。接著，依據公式(4)和
(5)，我們可得到 T個音框整體的最大似然率為
    0 37 0 37( ) max max ( , ) , max ( , ) ,v vA T R T v D T v  (6)
其中 T 在本論文裡設為 20。在依據公式(4)，
(5)和(6)得到 A(20)之最大似然率數值之後，我
們可作回溯(backtrack)處理，去找出 A(20)數值
的最佳行走路徑，而得到 20 個音框各自所被
指派的 GMM編號。
6的DCC係數作對映。另外，在代號 SSG (system
using single Gaussian mixture for mapping)的系
統裡，我們仍然使用 350個語句所訓練出的一
個具有 256個高斯混合的 GMM，不過在變換
階段，3.3.2節裡說明的高斯混合選取方法被用
來為一序列的輸入音框選取出各音框的單一
高斯混合，然後各輸入音框的 DCC 係數就使
用所選出的單一高斯混合及公式(7)來作對
映。至於在代號 SLG (system using selected
GMM for mapping)的系統裡，我們首先以 350
個語句來訓練出 37個分段式 GMM，而各分段
式 GMM 都只有 16 個高斯混合，然後在變換
階段，我們採用 3.3.1 節裡說明的 GMM 選取
方法，來為每 20 個有聲音框選取出最大似然
率的一個或兩個分段 GMM，接著採用 3.3.2
節裡的高斯混和選取方法，來為各輸入音框選
取出單一個高斯混合，再依據公式(7)作對映。
當把一個來源語者的發音檔，分別輸入到
前述的三個語音變換系統，我們就可得到三個
變換出語音檔。然後使用變換出的音檔，我們
進行了兩種類型的聽測實驗，分別是音色相似
度之聽測、和語音品質之聽測。在這二類型的
聽測實驗裡，我們都邀請了 25 位人士來聆聽
音檔並給予相對分數，而在這 25 位人士中，
有 20位是不熟悉語音變換之研究的。
4.1 音色相似度測試
首先我們準備了 5個音檔，它們的代號分
別是 VS(由來源語者發音)，VT(由目標語者發
音)，VX1(經由 SOG系統變換得到)，VX2(經
由 SSG 系統變換得到)，VX3(經由 SLG 系統
變換得到)，其中 VS與 VT具有相同的說話內
容，而 VX1、VX2和 VX3三者也有相同的內
容，但不同於 VS和 VT的，這 5個音檔可從
網頁 http://guhy.csie.ntust.edu.tw/VoiceConv/去
下載。在進行聽測實驗時，我們以 ABX 的次
序來撥放前述的音檔，在此 A 固定為 VS，B
固定為 VT，而 X則隨機由 VX1、VX2和 VX3
三者中選出，每次以 ABX 次序播放完音檔
後，受測者就被要求給一個分數。在此分數的
定義是，9分(或 1分)表示 X的音色確定就是
B(或 A)的音色，7分(或 3分)表示 X的音色比
較接近 B(或 A)的音色，而 5分表示 X的音色
無法判斷是接近 A或接近 B。
做完聽測實驗之後，25 位受測者所給的
分數被用來計算出三個系統各自的平均分數
(AVG)和標準差(STD)，所得到的分數數值就
如表 1所列出的。由表 1的平均分數可知，不
同性別之間的語音變換(即從M1到 F1)，會比
同性別之間的(即從 M1 到 M2)獲得明顯較高
的分數。此外，拿三個系統的平均分數作比
較，可從表一的數值得知，SLG系統的表現明
顯比 SSG系統的好許多(7.05 vs 6.24，7.60 vs
7.24)，而 SSG系統的表現則是比 SOG系統的
稍微好一些(6.24 vs 6.08，7.24 vs 6.92)。所以
本計畫提出的分段式 GMM 之觀念及自動
GMM挑選之演算法，的確可幫忙改進所變換
出語音的音色相似度。
表 1、音色相似度聽測之平均分數與標準差
SOG SSG SLG
AVG 6.08 6.24 7.05M1=>M2
STD 1.11 1.09 0.93
AVG 6.92 7.24 7.60M1=>F1
STD 1.13 1.07 1.10
4.2 語音品質測試
在此我們使用三個系統變換出的語音檔
VX1、VX2 和 VX3，來進行語音品質的聽測
實驗。音檔撥放的次序為 AX，A 固定設為
VX1，而 X 則隨機由 VX2 和 VX3 兩者中取
出，每次以 AX次序播放完音檔後，受測者就
被要求給一個分數。在此分數的定應是，9分
(或 1 分)表示 X 的語音品質明顯比 A 的好(或
差)，7分(或 3分)表示 X的品質比 A的稍微好
(或差)一些，5分則表示 X和 A的語音品質無
法分辨優劣。
作完聽測實驗之後，我們收集 25 位受測
者所給的分數，來計算出 SSG和 SLG兩系統
各自的平均分數和標準差，結果得到的數值如
表 2裡列出的。依據表 2的平均分數可看出，
同性別之間(即從 M1 到 M2)的變換語音的品
質，會比不同性別之間(即從M1到 F1)的較好
約 0.5分，這顯示不同性別之間的變換語音的
品質，是比較難作改進的。此外，依據 SLG
和 SSG 兩系統的平均分數作比較，我們可看
出 SLG 的分數都比 SSG 的高約 0.7 分，並且
SLG 的平均分數都高於 5 分，所以分段式
GMM之觀念及自動挑選 GMM之演算法，確
實可用以改進所變換出語音的語音品質。
8Techniques for Discrete Cepstrum Estimation",
IEEE Signal Processing Letters, Vol. 3, No. 4,
pp. 100-102, April 1996.
[7] M. Abe, S. Nakamura, K. Shikano, and H.
Kuwabara, “Voice Conversion through Vector 
Quantization,” International Conference on
Acoustics, Speech, and Signal Processing, New
York, Vol. 1, pp. 655-658, Apr. 1988.
[8] S. Nakamura and K. Shikano, “Spectrogram 
Normalization Using Fuzzy Vector
Quantization”, J. Acoust. Soc., Japan, Vol. 45,
pp. 107-114, 1989.
[9] H. Mizuno and M. Abe, “Voice Conversion 
Algorithm Based on Piecewise Linear
Conversion Rules of Formant Frequency and
Spectrum Tilt”, Speech Communication, Vol. 16,
No. 2, pp. 153-164, 1995.
[10] 吳嘉彧、王小川，”不需平行語料而基於共振
峰與線頻譜頻率映對之語者特質變換系統”，
第二十一屆自然語言與語音處理研討會
(ROCLING 2009)，台中，第 319-332頁，2009。
[11] Stylianou Y., Capp´e O., Moulines
E, ”Continuous Probabilistic Transform for
Voice Conversion,” IEEE trans. Speech and
Audio Processing, Vol. 6, No. 2, pp.131–142,
1998.
[12] Min Chu, “Voice Conversion with Smoothed
GMM and MAP Adaptation”, Proc. of
EuroSpeech, Geneva, Switzerland, pp.
2413-2416, 2003.
[13] Srinivas Desaiy, et al.,“Voice Conversion Using 
Artificial Neural Networks,” ICASSP, Taipei,
Taiwan, pp.3893–3896, 2009.
[14] Zhiwei Shuang, Fanping Meng, and Yong Qin,
“Voice Conversion by Combining Frequency
Warping with Unit Selection”, ICASSP, Las
Vegas, U.S.A , pp.4661-4664, 2008.
[15] D. Sundermann, et al., “Text Independent Voice 
Conversion Based on Unit Selection”, ICASSP,
pp. 81-84, Toulouse, France, 2006.
[16] 劉德賢，應用雙可夫模型與聲音變換於情緒
語音合成之研究，碩士論文，國立成功大學
資訊工程研究所，2005。
[17] E. K. Kim, S. Lee, and Y. H. Oh, “Hidden 
Markov Model Based Voice Conversion Using
Dynamic Characteristics of Speaker”, Proc.
EuroSpeech, Vol. 5, Rhodes, Greece, 1997.
[18] A. Kain and M.W. Macon, “Spectral Voice 
Conversion for Text-to-speech Synthesis”, IEEE
ICASSP, Seattle, Vol. 1, pp. 285-288, May
1998.
[19] K. S. Lee, “Statistical Approach for Voice 
Personality Transformation,” IEEE trans. Audio,
Speech, and Language Processing, Vol. 15, No.
2, pp. 641-651, Feb. 2007.
[20] H. Kawahara, I. Masuda-katsuse and A. De
Cheveign, “Restructuring Speech Represen-
tations Using a Pitch-adaptive Time-frequency
Smoothing and an Instantaneous-frequency-
based F0 Extraction: Possible Role of a
Repetitive Structure in Sounds”, Speech
Communication, Vol. 27, pp. 187-207, 1999.
[21] T. Toda, H. Saruwatari, and K. Shikano, “Voice 
Conversion Algorithm Based on Gaussian
Mixture Model with Dynamic Frequency
Warping of STRAIGHT Spectrum”, ICASSP, 
Salt Lake City, pp. 841-844, May 2001.
[22] M. Zhang, J. Tao, H. Jia, and X.
Wang, ”Improving HMM Based Speech 
Synthesis by Reducing Over-Smoothing
Problems”, International Symposium on Chinese
Spoken Language Processing (ISCSLP),
Kunming, China, Dec. 2008.
[23] R. A. Redner and H. F. Walker, “Mixture 
densities, maximum likelihood and the EM
algorithm,” SIAM Review, vol. 26, no. 2, pp.
195-239, 1984.
[24] H. Y. Kim, et al., “Pitch detection with average 
magnitude difference function using adaptive
threshold algorithm for estimating shimmer and
jiter,” 20-th Annual Int. Conf. of the IEEE
Engineering in Medicine and Biology Society,
Hong Kong, China, 1998.
[25] H. Y. Gu and S. F. Tsai, “A discrete-cepstrum
based spectrum-envelope estimation scheme and
its example application of voice transformation,” 
International Journal of Computational
Linguistics and Chinese Language Processing,
vol. 14, no. 4, pp. 363-382, 2009.
2除了前述的論文發表 session 之外，我還參加了另外兩個 sessions，即 Array & Multi-channel
Signal Processing、Blind and Adaptive Signal Processing，其它信號處理的相關 sessions，則由於
平行 session 之安排方式而無法參與。
在行程方面，於 10 月 20 日搭乘 11:45 由松山機場直飛上海浦東機場的班機，然後搭乘長
途巴士前往位於蘇州城內的會場(即國際會議中心飯店)，到達會場時大約是 17:40。參加研討會
後，則於 23 日中午搭乘巴士前往上海虹橋機場第一航站，並且在航站附近的旅館住宿一晚，然
後在隔日清晨 8:05，趕搭由虹橋機場直航台北松山機場的班機。
二、與會心得
雖然 WCSP 2010 研討會接受投稿的領域包含了無線通信與信號處裡之相關領域，但是所邀
請的五位 keynote speaker 的演講題目，都集中於通信領域的議題。所以，個人感覺信號處理與
語音處理並不是此次研討會的焦點，不過投稿信號處理領域的論文數量相對地少很多，應也是
一個重要的原因 。
在 Speech and Audio Signal Processing 場次，共有 8 篇論文發表，其中有 7 篇是屬於語音處
理的。除了我的論文之外，其它論文中有兩篇是作語音編碼(speech coding)的，有一篇是作語音
強化(speech enhancement)的，一篇是作基週偵測的，及語者辨識等。雖然語音處理方面的篇數
不多，但是也含蓋了幾個語音研究的子領域，因此仍可相互了解不同子領域裡的研究情況。
inverse DFT (IDFT). Then, the logarithmic magnitude-
spectrum can be computed with the cepstrum coefficients as
2 1
0 / 2
1
2log ( ) 2 cos( ) cos( ),
0 1.
N
n N
n
X k c c k n c k
N
k N
 


  
  
 (1)
If most terms at the right side of (1) are cancelled except the
leading p+1 terms, the magnitude spectrum computed, log S(f),
would be a smoothed version of the original, log|X(f)|. Here, the
index variable, k, in (1) is replaced with f in order to change the
frequency scale from bins to the normalized frequency range
from 0 to 1. Accordingly, log S(f) is computed as
0 11 2
0
1
log ( ) 2 cos(2 ) , , , , ..., .
p
N
n N N N N
n
S f c c f n f 

    (2)
Based on (2), some researchers proposed to approximate the
spectral envelope of log|X(f)| with log S(f). Nevertheless, the
coefficients, cn, in (2) cannot be derived directly with IDFT.
One derivation method proposed by Galas and Rodet is to
define a set of envelope constraints, and find the values of the
coefficients, cn, that can best satisfy the envelope constraints. In
this manner, the derived coefficients, cn, n=0, 1, , p, are
called the discrete cepstrum for log|X(f)|.
The envelope constraints just mentioned are actually L pairs
of (fk, ak) for L representative spectral peaks selected from the
original spectrum log|X(f)|. Here, fk and ak represent the
frequency (already normalized to between 0 and 1) and
amplitude of the k-th spectral peak, respectively. Note that L is
usually larger than the cepstrum order, p. Hence, a least-
squares criterion is adopted to minimize the approximation
errors between S(fk) and ak, k L. In matrix form, the
optimal values of the DCCs is derived by previous researchers
[6, 7] to be
T 1 T( )C M M M A    (3)
where A = [log(a1), log(a2), , log(aL)]T, C = [c0, c1, , cp]T,
and
1 1 11 2cos(2 ) 2cos(2 2) 2cos(2 )
1 2cos(2 ) 2cos(2 2) 2cos(2 )L L L
f f f p
M
f f f p
  
  
  	

  
 

   

    

B. Regularization of Discrete Cepstrum
When (3) is used to derived DCCs, the spectral envelope
computed according to (2) may vibrate radically and have very
large approximation error at some frequencies slightly away the
selected spectral-peak frequencies, fk. This is because the direct
estimation method (i.e. Eq. (3)) may sometimes be ill-
conditioned. That is, slightly varying the frequency values of
the detected spectral peaks may result in a very different
spectral envelope curve being obtained. Therefore, Cappé and
Moulines proposed a regularization technique to prevent such
radical vibrations from occurring [7]. They add a curve-
sharpness penalty term, i.e.
 
2
0
( ) ( ) ,dR S f S f df
df
  	
 
 
 
 (4)
to the approximation error calculation equation, and the
resulted equation for deriving DCCs becomes
T 1 T( ) .C M M U M A    (5)
where  is a weighting parameter (suggested value is around
0.0001), and
2
2
2
0 0
1
8 .
0
U
p

 	

 

 

 

 
 

(6)
III. SELECTION OF SPECTRAL PEAKS
 are derived by minimizing the summation of squared
errors between the selected spectral peaks, ak, k=1, 2,, L, and
S(f). Therefore, selecting appropriate spectral peaks from a
DFT spectrum is an important preprocessing step. Consider a
simplest selection method, i.e. locate and select all the spectral
peaks on the spectrum as the final selected peaks. In this case,
the approximated spectral envelope would be very bad and of
large approximation error. When such bad spectral envelopes
are used to transform voice signals, the output obtained will
suffer significant voice-quality degradation.
Therefore, we studied this problem and found that the
concept of MVF (maximum voiced frequency) proposed in
HNM (harmonic-plus-noise model) [9, 10] is utilizable. The
MVF of a DFT spectrum is searched by testing the sharpness of
the spectral peaks one after another. After some low-frequency
spectral peaks pass the test, it will eventually occur that no
more spectral peak can pass the test. Then, the frequency of the
last spectral peak passing the test is defined to be the MVF. In
this paper, we first detect if a signal frame is voiced or
unvoiced. If it is detected to be voiced, the frame is further
searched for the MVF value, fv, by using the searching method
proposed by Stylianou [10]. According to fv, the DFT spectrum
of the frame is split into the lower-frequency harmonic part and
the higher-frequency noise part. Then, for the harmonic part,
the first spectral peak of a frequency within the range (0.5F0,
1.5F0), where F0 is the detected fundamental frequency, is
searched for. Let the obtained frequency and amplitude be f1
and a1. Next, the second spectral peak of a frequency within the
range (f1+0.5F0, f1+1.5F0) is searched for, and let the results
be f2 (frequency) and a2 (amplitude). When going on in this
manner, we can find the frequencies and amplitudes of the
other spectral peaks within the harmonic part. Sometimes, it
may occur that no spectral peak is found within a designated
frequency range. In this situation, we will right shift the
frequency range, i.e. adding 0.5F0, and try to find again.
For the noise part of a voiced frame, the searching method
explained above for the harmonic part cannot be adopted. Note
that the harmonic structure becomes obscure in the noise part,
and the frequency gaps between adjacent peaks become
randomly varied. For an example, inspect the DFT spectrum
curve beyond 5,800Hz in Fig. 2. Therefore, we adopt another
method to find the spectral peaks for the noise part. In this
method, a smoothed spectral curve is computed first by
truncating the real-cepstrum coefficients outside the leading 30
ones, and transforming (via DFT) the resulted real-cepstrum
( ) log(1 ) ,
1,750
fscl f   (8)
where f is in the unit Hz. This conversion function will have the
scaled frequency value, f , growing more slowly with f at the
low frequency end when it is used as the scl() function for (7).
The three curves shown in Fig. 5 are obtained by taking
Bark, mel, and our frequency conversions as the scl() function
for (7), respectively. From Fig. 5, it can be seen that our
frequency conversion as given in (8) can indeed grow the
scaled frequency f more slowly with the linear frequency f.
By using the frequency conversion function of (8), the
approximated spectral envelope in Fig. 3 will become the one
drawn in Fig. 6. According to the spectral envelope in Fig. 6, it
can be said that the frequency conversion function proposed
can indeed eliminate the over vibration phenomenon at the low
frequency end, and reduce the local approximation error around
3,000Hz. The reducing of the local approximation error we
think is due to the increased vibrating capability around
3,000Hz by using the proposed frequency conversion instead of
the mel-frequency conversion.
Hz
Fig. 5. Three curves of scaled frequencies by using Bark, mel, and our
frequency conversion functions, respectively.
Fig. 6. Envelop approximated in our freq. scale with 40 DCCs.
C. Approximation Error Comparsion
It may be queried whether our frequency conversion
function is just better than mel-frequency conversion for certain
signal frames. Therefore, we decide to compare the
approximation errors of the two frequency conversions in the
three frequency ranges, i.e. 0 ~ 2,000Hz, 0 ~ 4,000Hz, and 0 ~
6,000Hz. Here, approximation error is measured with the
formula,
-1 ( )
10 101=0
1 1 20 log 20 log ( , )
( )
Nr L t t
k kktEs a S t fNr L t 
 	
    
 
 
 (9)
where Nr is the total number of signal frames, and L(t) is the
number of spectral peaks, for the t-th frame, dynamically
determined to ensure that only the spectral peaks of frequencies
within the currently concerned frequency range are counted.
Here, 375 Mandarin sentences consisting of 2,925 syllables
recorded from a male are used as the testing speech. After all
frames of the testing speech are processed, the approximation
errors measured in different frequency ranges and different
discrete cepstrum orders are illustrated in Fig. 7.
Inspecting the error curve in Fig. 7, it can be seen that
across the cepstrum-order numbers from 30 to 50, our
frequency conversion and the mel-frequency conversion have
almost same approximation errors in the frequency range, 0 ~
2,000Hz. Nevertheless, in the other two frequency ranges, our
conversion function will apparently obtains smaller
approximation errors for different cepstrum-order numbers.
This decreasing of approximation error becomes more apparent
as the frequency range becomes wider.
(a) Frequency range: 0 ~ 2,000Hz
(b) Frequency range: 0 ~ 4,000Hz
(c) Frequency range: 0 ~ 6,000Hz
Fig. 7. Approximation errors measured for our frequency conversion
and mel-frequency conversion in the three frequency ranges.
V. AN EXAMPLE APPLICATION: TIMBRE TRANSFORMATION
Here, voice transformation is meant to change the timbre of
an input voice to a different timbre. For example, change the
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/18
國科會補助計畫
計畫名稱: 音質改進之語音變換系統
計畫主持人: 古鴻炎
計畫編號: 99-2628-E-011-107- 學門領域: 自然語言處理與語音處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
