 行政院國家科學委員會補助專題研究計畫 □ 成 果 報 告   
□期中進度報告 
 
藉由肋骨壓抑以提升胸腔 X 光影像肺臟腫瘤偵測效能之研究 
(第三年) 
 
計畫類別：■ 個別型計畫  □ 整合型計畫 
計畫編號：NSC 97-2221-E-024 -009 -MY3 
執行期間： 99年 8月 1日至 100年 10月 31日 
 
計畫主持人：李建樹 
計畫參與人員：黃飛翔、林志穎、吳品品、謝青穎 
 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  ■完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
■出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
          ■涉及專利或其他智慧財產權，■一年□二年後可公開查詢 
          
執行單位：國立臺南大學 
 
中   華   民   國 101 年 1 月 30 日 
1. 研究動機與目的 
Ginneken [1]等人提出以多尺度 Gaussian derivatives filter bank來擷取特徵，進而利用左
右肺部紋理對稱的特性來偵測腫瘤，在 2003 年 Coppini [2]等人提出以類神經網路
(Artificial neural network,ANN)為基礎的電腦輔助系統來判斷 X光中肺部腫瘤，其方法
和[1]一樣以多尺度 LoG filter和 Gabor filter來擷取特徵找出疑似腫瘤區域，將疑似腫瘤
區域之特徵輸入 ANN 進行分類，在 2006 Campadelli [3]等人提出以 KNN (k-nearst 
neighbours)分類器為基礎的肺部腫瘤偵測方法，一樣使用多尺度 LoG filter，且利用腫瘤
形狀、亮度等特徵進行腫瘤偵測，將疑似腫瘤區域之特徵以 KNN分類器進行判讀。在
2006年 Arnold[4]等人使用 multiscale LoG filter來偵測疑似腫瘤的物件，且根據所偵測
到物件的尺度、和 Hessian matrix所得到的六個屬性來當作特徵進一步使用 KNN分類器
來判讀此物件是否為腫瘤。上述論文的腫瘤偵測效能仍不夠理想，尤其是對人眼不易察
覺的早期腫瘤更是如此。SIFT 特徵點 [5-7]具有平移、旋轉及尺度等不變性，甚至在仿
射轉換或亮度改變下依然具有相當程度的不變性，因此，近年來受到大家的重視，也被
廣泛應用到影像處理和醫學影像等領域。由於早期肺臟腫瘤相當小，不易被偵測出來，
SIFT 具有的尺度、旋轉與亮度轉換不變性特性相當適合用來偵測這種不易察覺的早期
腫瘤 ，為了進一步提升腫瘤偵測效能，本論文提出一種利用 SIFT來偵測胸腔 X光影像
中腫瘤的方法。 
 
2. 研究方法 
本計畫第三年的目標為，在肋骨移除的基礎上開發出腫瘤偵測系統，其中包含：影像
強化、SIFT特徵擷取、特徵強化與特徵分類等四大步驟，以下分項加以說明： 
 
2.1 影像強化 
在本小節中將介紹如何把影像對比度增強，我們嘗試過多種方法，後來發現高頻強調的
成效最佳，高頻強調濾波器轉移函數為： 
),(),( vubHavuH hphfe +=        (1) 
其中 ),( vuHhfe 為高頻強調濾波器轉移函數， hpH 為高通濾波器的轉移函數，a≧0且 b>a，
在本研究中，所測試參數 a的數值範圍為 0.25~0.5，而參數 b的數值範圍為 1.5~2.0，圖
2. 特徵點定位(Accurate keypoint localization)。 
3. 方向的定位(Orientation assignment)。 
4. 特徵點描述(Keypoint descriptor)。 
 
2.3 特徵強化 
為強化 SIFT特徵對於腫瘤與非腫瘤的鑑別力，本研究採用正交子空間投影法(OSP)當
成特徵強化機制。正交子空間投影法可以用來排除掉不想要的干擾，強化出我們感興趣
之標的。其基本概念是將信號分成想要與不想要二類，藉由投射特徵向量到與不想要信
號特徵正交子空間(subspace)，來達到抑制不想要信號之目的，本文將 OSP應用在 SIFT
特徵向量上，分別嘗試腫瘤與非腫瘤的正交子空間投影，比較哪一種投影方式成效較
佳。以腫瘤正交子空間為例，分別藉由對腫瘤與非腫瘤的 SIFT特徵進行 PCA，找出代
表腫瘤與非腫瘤的前 k個 eigenvectors，由這些 eigenvectors展開成對應的腫瘤與非腫瘤
向量空間，再經過正交投影分別產生新的特徵向量。  
一開始本研究先利用腫瘤與非腫瘤物件SIFT特徵點的eigenvalue來計算到底需要使用
到幾個 eigenvectors 來代表整個分佈的資訊量，經實驗測試，發覺採用全部 eigenvalue
總和的百分之 99當成選擇 eigenvectors數量的依據，具有不錯的成效(如方程式(2))。將
這 k個 eigenvector做排列(如方程式(2)~(6)所示)形成矩陣Ｕ，我們計算Ｕ的正交子空間
投影 )( #UUIP −= ，並將 P與 v相乘得到投影後的特徵向量 q，其中 v為測試影像的每
個 SIFT特徵點的特徵向量。 
%95
....
....
128321
321
>
+++
+++
λλλλ
λλλλ k
       (2) 
[ ]kU νννν ......321 、、=        (3) 
TT UUUU 1# )( −=         (4) 
)( #UUIP −=          (5) 
Pvq =           (6) 
 
2.4 特徵分類 
由於不同訓練樣本的所訓練出來的分類器其所擅長的分類功能不太一樣，因此有多專
家分類器被提出用來整合多個分類器，以期獲致更理想的分類效能。本文採用 Philip[8]
3. 結果與討論 
在實驗二部份中，我們先進行實驗原始腫瘤與非腫瘤特徵點和經由 OSP之後腫瘤與非
腫瘤特徵點之間的差異程度，做為選擇腫瘤或非腫瘤特徵作為 OSP 的依據，如方程式
(7)~(8)所示， vni為原始腫瘤特徵向量，vnoni為原始非腫瘤特徵向量，qni為經 OSP投影
後的腫瘤特徵向量，qnoni為經 OSP 投影後的的非腫瘤特徵向量，mn為原始腫瘤的平均
向量，mnon為原始非腫瘤的平均向量，mn’為 mn經 OSP 投影後的結果，mnon’為 mnon經
OSP 投影後的結果，以 SB反應腫瘤與非腫瘤特徵向量之間的距離，以 SW反應腫瘤與
非腫瘤特徵向量的發散程度，並以 SB/SW 比值來度量腫瘤與非腫瘤特徵分佈的差異程
度，由表 1 可看出以腫瘤為底 99%資訊保存量中腫瘤與非腫瘤之間差異程度最大，作
為我們 OSP運算的依據。 
2)''( nonnB mmS −=        (7) 
2
1
2
1
)'()'( ∑∑
==
−+−=
i
non
i
non
i
n
i
nW mqmqS
     (8) 
 
表 1﹑腫瘤與非腫瘤之間差異程度。 
等級 原始腫瘤
與非腫瘤
資訊量 
非腫瘤為
底的 95%
資訊量 
腫瘤為底
的 95%資
訊量 
非腫瘤為
底的 99%
資訊量 
腫瘤為底
的 99%資
訊量 
WB SS /  1. 0 1.2 1.3 2.2 3.0 
 
我們針對五種等級的 SIFT特徵點作 OSP投影以拉大腫瘤與非腫瘤特徵差異，也分別
測試以腫瘤與非腫瘤為基底和分別以 95%和 99%資訊保存量進行實驗，將這些投影特
徵向量輸入 ANN做分類測試，以 Accuracy來表示: 
   (9) 
表 2為未投影之結果，表 3分別為以非腫瘤與腫瘤為底的正交子空間投影分類結果。
由表 3-2~3-4可得知以腫瘤為底，99%資訊量的投影效果最好，且比起原始未投影的效
果為佳，因此，本方法確實可以拉開特徵描述之間的差異來偵測出腫瘤與非腫瘤。 
 
表 2﹑投影前的 accuracy。 
等級 未投影的 Accuracy 
一 65.15% 
二 55.08% 
三 56.45% 
四 54.54% 
五 55.78% 
全部 57.41% 
 
表 3﹑投影後的 accuracy。 
 
表 4﹑有無肋骨移除之效能比較表。 
 
等級一~三 等級四和五 全部 
原始影像 81% 57% 73% 
肋骨移除之後影像 80% 63% 77% 
 
 
4. 成果自評 
本研究提出一種新的肺部腫瘤偵測方法，影像前處理是使用高頻強調來強化影像，接
著利用穩定、不受旋轉與尺度改變的 SIFT演算法，得到腫瘤與非腫瘤的 SIFT特徵點，
再利用正交子空間投影拉開腫瘤與非腫瘤 SIFT特徵點的差異性，並利用 DPB-DCS進一
步來判讀，由實驗結果得知，我們的系統效能在不顯著腫瘤的偵測效能上比論文[4]的方
法高出將近 15%，並且經由肋骨移除之後，我們系統確實能提高腫瘤偵測之正確率。針
對計畫執行成果，本人已將結果整理成二篇論文，一篇已刊登在 SCI-E 國際期刊
Biomedical Engineering-Applications, Basis & Communications，另一篇投稿至國際研討會
IS3C，並被推薦發表在 SCI期刊 Computers & Mathematics with Applications。 
參考文獻 
[1]. Bram van Ginneken, Shigehiko Katsuragawa, Bart M. ter Haar Romeny, Kunio Doi, And 
Max A. Viergever “Automatic Detection of Abnormalities in Chest Radiographs Using 
Local Texture Analysis,” IEEE Transactions on Medical Imaging, vol. 21, no. 2, Feb.2002 
[2]. Giuseppe Coppini, Stefano Diciotti, Student, Massimo Falchini, Natale Villari, and Guido 
Valli “Neural Networks for Computer-Aided Diagnosis:Detection of Lung Nodules in Chest 
Radiograms,” IEEE Transactions on Information technology in Biomedicine, vol. 7, 
no.4,Dec 2003 
[3]. Paola Campadelli, Elena Casiraghi and Columbano, Stefano, “Lung Segmentation and 
Nodule Detection in Postero Anterior Chest Radiographs,” Gruppo Italiano di Ricercatori in 
Pattern Recognition 2004(GIRPR2004), Perugia, Italy, Sep 2004 
[4]. Arnold M.R. Schilham, Bram van Ginneken, and Marco Loog. “A computer-aided diagnosis 
system for detection of lung nodules in chest radiographs with an evaluation on a public 
database,” Medical Image Analysis 10, pp. 247-258, 2006. 
[5]. David G. Lowe, “Object Recognition from Local Scale-Invariant Features,” International 
Journal of Computer Vision, Corfu, Greece, pp.1150-1157. 1999. 
[6]. David G. Lowe, “Distinctive Image Featuresfrom Scale-Invariant Keypoints,” International 
Journal of Computer Vision, no.60, pp.91–110. 2004. 
[7]. David G. Lowe, “Automatic Panoramic Image Stitching usingInvariant Features,” 
International Journal of Computer Vision, no.74(1), pp.59–73. 2006. 
影 
像 正 
確 
率 
 可供推廣之研發成果資料表 
■ 可申請專利  □ 可技術移轉                                      日期：101年 1月 31日 
國科會補助計畫 
計畫名稱：藉由肋骨壓抑以提升胸腔 X 光影像肺臟腫瘤偵測效能
之研究 
計畫主持人：李建樹 
計畫編號：NSC 97-2221-E-024 -009 -MY3 學門領域：醫學工程 
技術/創作名稱 藉由 SIFT特徵偵測肺臟腫瘤技術 
發明人/創作人 李建樹 
技術說明 
中文： 
針對胸腔 X光影像提出精確的肺癌檢測演算法。 
英文： 
We propose an accurate lung nodule detection technology for chest 
X-ray image. 
可利用之產業 
及 
可開發之產品 
醫療產業 
技術特點 精確 
推廣及運用的價值 讓胸腔科醫師得以更容易地發現隱身在肋骨後面的腫瘤。 
※ 本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
表 Y04 
 
三、考察參觀活動(無是項活動者省略) 
 
四、建議 
希望國科會能建立國內學者參加國際研討會的登錄窗口，以便讓參加同一研討會的
學者能彼此聯繫，比如：有人無法出席，可委託他人報告，以避免 no show 情形發生；
再者，學者們也能相約一同組團前往，一方面能節省差旅支出，同時也能彼此照應。 
 
五、攜回資料名稱及內容 
 
ICCSS Proceedings CD-ROM一片。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 A Visual Context-Awareness System for 
Computer Room Classes – an Automatic 
Roll-Call System 
 
Jiann-Shu Lee and Seng-Fong Lin 
Department of Computer Science and Information 
Engineering, National University of Tainan, 
Tainan, Taiwan, R.O.C. 
email: cslee@mail.nutn.edu.tw 
 
Abstract—Context-awareness is spreading into areas of 
computer and engineering sciences, with the goal of 
making systems more human-centric. Several systems 
have been developed for learning fields using 
context-aware models. However, the beneficiaries of these 
context-awareness systems are the learners. There are few 
context-aware computing systems designed for the 
instructor. In this paper, we propose an automatic 
roll-call system. We utilize a two-stage face recognition 
scheme to identify students. The experimental results 
show that our system efficiently and accurately identified 
students via webcams, and that it is promising for 
automatic roll-call in computer-room classes. 
Keywords: Context-awareness, roll-call, computer-room 
classes, chin detection. 
I. INTRODUCTION 
Pervasive computing, formerly only a concept, is 
now technologically possible with a series of novel 
techniques. Context-awareness, introduced by Schilit [1] 
in 1994, was important. Over the past two decades, 
context awareness has greatly evolved from its 
location-awareness roots to include properties such as 
situation, user, environmental properties, etc. Context is 
any information that can be used to characterize the 
situation of an entity. An entity is a person, place, or 
object that is considered relevant to the interaction 
between a user and an application, including the user 
and application themselves [2]. The context-awareness 
model allows contextual information to be collected, 
processed, inferred, and disseminated to spontaneous 
applications. Context awareness has spread to the 
computer and engineering sciences, with the goal of 
making systems more human-centric [3]. 
Several educational applications use context-aware 
models. Zancanaro et al. [4] applied the context 
awareness technique to museum guides. Jameson [5] 
argues for combining user modeling and context 
awareness, an example of which is the LISTEN system 
[6]: it offers audio presentations to art exhibition 
visitors by referring to their location and profile. Ogata 
and Yano [7] created a language learning system for 
Japanese polite expressions and personal attributes. For 
these and other examples [8-10], the primary entities in 
the context-awareness systems are learners. 
There are few context-aware computing systems 
designed for teachers. For classroom instruction, 
teachers visually determine their students' learning 
statuses. If they find one student distracted, the teacher 
may call the student's name to refocus the student's 
attention. If most students get distracted, the teacher 
may try another strategy, a joke, for example. However, 
because of the occlusion coming from the monitors, it is 
difficult for the teacher to assess the students' learning 
status in computer classrooms. Because of the 
popularization of webcams, the infrastructure for visual 
context awareness in teaching has become mature. 
Nowadays, most classroom computers are equipped 
with webcams. Hence, it is feasible to evaluate students' 
learning statuses using computer vision technology. To 
apply context-awareness computing to computer 
classrooms, there are two issues of concern: student 
identification and concentration tracking. We have 
developed a vision-based context-awareness system that 
automatically monitors learning status, takes attendance 
(roll call), and senses the computer-user's attention to 
the monitor. In this paper, we focus on the automatic 
roll-call subsystem. Fig. 1 shows the proposed 
automatic roll-call system diagram. 
This paper is structured as follows. In Section two, 
how to detect the face from the grabbed frame is 
introduced. The chin detection is presented in Section 
three. The face identification process is explained in 
Section four. Section five presents the experimental 
results. Finally, we make conclusions in Section six. 
 ii
OO
1=∪=
N
       (Eq. 4) 
where Oi is the i-th connected component in J. 
 
}...1:),({maxarg
),(
NiyxJm
iOyxi
=∑=
∈
   (Eq. 5) 
mp OFA =        (Eq. 6) 
)( pp FAholefillCFA =      (Eq. 7) 
pp CFAIBIIFA ×−•= )(     (Eq. 8) 
 
Because we require only the frontal face, we have to verify 
whether the skin area is a human face and whether it is frontal. 
For human face verification, eyebrows and eyes are significant 
and distinct features. Therefore, human face verification focuses 
primarily on the relative relationship of the eyebrows and eyes. 
Eq. (7) is used to get the complete potential face area by filling 
holes in FAp. The input image is then converted to grayscale, in 
which eyes, eyebrows, nose, and mouth are represented by 
somewhat darker areas. This attribute combined with bottom-hat 
operation and CFAp, as shown in Eq. (8), obtains IFAp, in which 
the facial features are enhanced. After thresholding IFAp, the 
initial facial features are acquired (Fig. 2). Subsequently, a series 
of filters is used to obtain the eye-and-eyebrow parts. Very small 
objects are filtered out by the area filter. The closing operation 
smoothes irregular shapes and connects neighboring objects. 
Objects with orientations far away from horizontal are discarded 
by the direction filter. The top four objects form the 
eye-and-eyebrow parts (Fig. 43). 
The face verification rules are based on the geometric 
relations between eyebrows and eyes. They are the shape 
similarity of the eyebrows and the distance and direction 
similarity between the eyes and eyebrows. First, the areas and 
centroids of the eyebrows and eyes are calculated. Let , 
, , and  represent the centroids of the left 
eyebrow, right eyebrow, left eye, and right eye, respectively. For 
shape similarity, because the eyes may be affected by the whites 
of the eyes, the eyebrows are used for this comparison. The 
perpendicular bisector of the line connecting the left and the right 
eyebrow centroids is used as the symmetric axis with the unit 
vector 
LBC
RBC LEC REC
⊥u
r
 (Fig. 4). The shape similarity (SS) between the left 
and the right eyebrows is calculated using Eqs. (9) to (14), in 
which X
r
 represents the coordinate vector of point X , and p~  
is the mirror point of p with respect to ⊥u
r
.
 
 
(a) 
 
(b) 
 
(c) 
(d) (e) (f) 
Fig. 2. The result of each step in the initial facial feature extraction for an example frame. (a) The original image; (b) The clustering results using the mean-shift 
algorithm; (c) the FAp; (d) the result after bottom-hat operation; (e) the result after bottom-hat operation restricted in CFAp; (f) the thresholding result of (e). 
 
 
 The protocol thus far described will verify that the webcam 
sees a human face; however, more information is necessary to 
verify that the face is a frontal view. Two main non-front 
conditions are considered, viz. in-plane and out-plane rotations. 
For in-plane rotation, we correct the image by rotating Θ degrees 
in reference to the angle between the line connecting both ECL 
and ECR and the horizontal line (Fig. 5). For out-plane rotation, 
we use the out-plane rotation index (OPRI), defined in Eq. (17), 
to judge this rotation, in which LM (left margin) and RM (right 
margin) denote the distances from ECL and ECR to the left and 
right skin edges, respectively. If OPRI > 1.2, then the image is 
considered to have out-plane rotation and skipped. 
⎪⎩
⎪⎨
⎧
<
≥
=
RMLMif
LM
RM
RMLMif
RM
LM
OPRI
,
,
    (Eq. 17) 
 
 
(a) 
 
(b) 
Fig. 5. An in-plane rotation correction example. (a) The original image; (b) the rotation-corrected image.
III. CHIN DETECTION 
Lee et al. [14] developed a chin detection method using gray 
intensity information. That method was designed for static and 
frontal face. To adapt it to meeting the requirements for the 
online roll-call system, skin-tone information is exploited here. 
First, a horizontal line from the nose center (Fig. 6(a)) is drawn 
to the left and right skin edges. The two skin edge points are the 
first pair of preliminary face boundary points, PFB1 (Fig. 6(b)). 
Because skin-tone segmentation is more conservative, the actual 
face boundary points will be a few pixels outside PFB1. 
Therefore, we fine-tune PFB1 by detecting the edge points 
around PFB1 in the grayscale image using a Gaussian first-order 
differential kernel. The new pair of edge points is viewed as the 
first pair of face boundary points, FB1 (Fig. 6(b)). The face 
width is thus defined as the distance between FB1. Next, a group 
of pixels 1/6 the width of the face are extracted from the left 
corner of the mouth along the slope slanting outward at 45 
degrees. The Gaussian first-order differential kernel convolves 
with these pixels to look for the maximum response 
corresponding to the face boundary point. The right boundary 
point is similarly obtained. These two points are denoted FB2 
(Fig. 6(b)). The 4 points FB1 and FB2 are then extended from 
both the left and right sides to the intersection, and sketching the 
triangle containing the area below the mouth and the chin tip 
(Fig. 6(b)). Because the chin tip is not at the lowest point of the 
triangle, we can limit the search range of the chin tip between the 
mouth and 1/2 of the face width below it. Subsequently, 
histogram equalization is applied to this trapezoidal area (Fig. 
6(c) and 6(d)). Taking the mean for every row to obtain the 
average brightness distribution (ABD). The chin tip is 
somewhere below the mouth and above the neck, where the 
brightness turns from bright to dark. Therefore, the minimum 
value of the first-order derivative of ABD will correspond to the 
chin tip (Figs. 6(b) and 6(e)). The profile of the chin outline is 
drawn with a curve (Fig. 6(f)) that fits the five already 
determined chin-boundary points (Fig. 6(b)). We can determine 
the chin features from the chin outline by first finding the two 
turning points of the chin. The tangent angle of every point on 
the chin outline is calculated, and then the maximum and 
minimum values of their second-order derivatives are calculated 
to determine the turning points. The distance between the two 
turning points is defined as the chin width (Fig. 6(g)). 
 The chin outline provides four sets of information: face 
width, chin curvature, face length (from the midpoint between 
the eyes to the tip of the chin), and chin width. The Pearson 
correlation coefficient is calculated to avoid highly correlated 
features. The chin curvature and chin width have a high 
correlation coefficient, which implies that the two features 
provide essentially the same information. Because the chin 
curvature is easier to obtain, we discard the chin width. The 
values of the remaining three features are distance-dependent; 
specifically, they are dependent upon the distance between the 
face and the camera. To eliminate the effect of distance, we 
created three distance-independent features: curvature × face 
width, curvature × face length, and face width/face length. We 
then use these three values to find the most effective feature 
combination. Because there are only three features, we test the 
seven possible combinations for feature selection. We adopt 
k-means to group the feature clusters. The performance of the 
k-means clustering algorithm is improved by using cluster 
validity techniques to estimate the number of clusters 
represented in the data. The silhouette method [17] is an efficient 
 example, Fig. 8 shows two students who were mismatched when 
the chin-shape classification stage was omitted and only GFs 
information was used. Using the chin-shape classification stage, 
however, eliminated this error. 
 
Fig. 7. The ROC curve of the proposed system. 
 
 
[7] Ogata, H., and Yano, Y., "Context-aware support for 
computer-supported ubiquitous learning," Proceedings of IEEE International 
Workshop on Wireless and Mobile Technologies in Education, IEEE Computer 
Society, pp. 27-34, 2004. 
Fig. 8. An example in which two students were misclassified because the 
chin-shape classification stage was omitted. 
VI. CONCLUSIONS 
In this paper, we proposed and tested an automatic roll-call 
system to help teachers take attendance in computer classrooms. 
The system consists of a two-stage student identification 
scheme—chin-shape classification and holistic face-recognition, 
inspired by how humans perceive faces. First, the system uses a 
mean-shift based skin-color modeling to segment the probable 
face areas. These areas are then verified by examining the 
relationship between the eyebrows and eyes and both in-plane 
and out-plane rotations. The face is then classified as a specific 
face shape by analyzing the chin shape. This face-shape 
classification step improves the face-recognition stage because it 
reduces the candidate number of possible faces. The system uses 
Gabor wavelets to extract the facial features, and 2D-PCA is 
applied to reduce feature dimensionality. Finally, the system uses 
SVM to identify the corresponding student for the face being 
processed. Our experimental results showed that our system 
correctly identified 96% of the students tested with 100% 
invader rejection rate. In addition, from the experimental results 
we also note that face-shape information improved the system's 
face-recognition performance. The invader test of students not in 
the system's face-recognition database showed that the system 
correctly rejected 98% to 100% of the invaders with less than a 
4% reduction of true-positives. Taken together, all these findings 
strongly indicate that our system is a good candidate for an 
automatic roll-call application in computer classrooms. 
ACKNOWLEDGEMENTS 
This work was partially supported by grant NSC 
96-2221-E-024-016 from the National Science Council, Taiwan. 
REFERENCES 
[1] Schilit, William Noah, A system architecture for context aware mobile 
computing. New York: Columbia University, pp. 66-94, 1995. 
[2] Dey, A.K., and Abowd, G.D., Towards a better understanding of context 
and context-awareness, GVU Technical Report GIT-GVU-99-22, College of 
Computing, Georgia Institute of Technology, 1999. 
[3] Godbole, A. and Smari, W.W., "A Methodology and Design Process for 
System Generated User Interruption Based on Context, Preferences, and 
Situation Awareness," The 2006 IEEE International Conference on Information 
Reuse and Integration, pp. 16-18, 2006. 
[4] Zancanaro, M., Stock, O., and Alfaro, I., "Mobile cinematic 
presentations in a museum guide," Book of Abstracts, Learning and Skills 
Development Agency, London, pp. 76-77, 2003. 
[5] Jameson, A., "Modelling both the context and the user," Personal 
Technologies, 5 (1), pp. 1-4, 2001. 
[6] Zimmerman, A., Lorenz, A., and Specht, M., "User modelling in 
adaptive audio-augmented museum environments," Proc. of 9th International 
Conference User Modelling, pp. 403-407, 2003. 
[8] Kolari, J., "Kontti - Context-aware mobile portal," ERCIM News, 54, p. 
14, 2003. 
[9] Chalmers, D., Sloman, M., and Dulay, N., Contextual mediation enables 
appropriate data selection, ERCIM News, 54, pp. 15-16, 2003. 
[10] Cui, Yanchun, and Bull, Susan, "Context and learner modelling for the 
mobile foreign language learner," System 33, pp. 353-367, 2005. 
[11] Rowley, H.A., Bluja, S., and Kanade, T., "Neural network based face 
detection," IEEE Transaction on Pattern. Analysis Mach. Int., vol. 20 (1), pp. 
39-51, 1998. 
[12] Burl, M.C., Leung, T.K., and Perona, P., "Face localization via shape 
statistics," Proceedings of the Ist International Workshop on Face and Gesture 
Recognition, Zurich, Switzerland, 1995. 
[13] Poynton, C.A., "A technical introduction to digital video," John Wiley & 
Sons, 1996. 
[14] Brand, J., and Mason, J., "A comparative assessment of three approaches 
to pixel-level human skin-detection," Proc. of the International Conference on 
Pattern Recognition, vol. 1, pp. 1056–1059, 2000. 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/01/31
國科會補助計畫
計畫名稱: 藉由肋骨壓抑以提升胸腔X光影像肺臟腫瘤偵測效能之研究
計畫主持人: 李建樹
計畫編號: 97-2221-E-024-009-MY3 學門領域: 醫學資訊
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
