一、中文摘要 
近日生物認證於安全維護方面被逐漸廣泛應用。尤其，以人臉為特徵而進行辨識，在
生物認證領域裡，於911恐怖行動後更成為該領域之主要研究問題之一。綜觀過去相關臉部
辨識之研究，多以單獨之2D或3D臉辨識為主，對於結合2D與3D臉部資訊研究極少。本計
畫為前ㄧ期國科會計畫(NSC94-2213-E-324-021)之延伸，其主要目的為建構一套有效之
2D+3D人臉辨識系統；本研究將使用雙影像對應法(Stereovision)取得3D人臉之深度資訊
(Disparity)，再結合原有2D人臉資訊完成人臉辨視。本計畫於特徵擷取方法上，採主成份分
析(Principal Component Analysis, PCA)及區域自相關係數(Local Autocorrelation Coefficient, 
LAC)等兩種特徵擷取方法，以改善人臉對於取像時位移改變之影響，並將此兩種特徵擷取
資訊進行整合。於分類方法上，採用倒傳遞類神經網路(Back-Propagation Neural Network, 
BP)，並將與最短歐氏距離法(Minimum Euclidean Distance)做比較。 
本計畫為三年計畫中通過之第一年期，第二期計畫於最近通過。於本期計畫中，已針
對一百五十位受測者進行取像，每位受測者均取十三種不同之表情影像，分別作為訓練影
像及測試影像，並將以 Borldand C++ Builder 之平台介面上實際建立此人臉辨識系統，藉以
評估實際之效用，且將對倒傳遞類神經網路及支持向量基之辨識結果與最短歐氏距離法相
比較，並做深入之探討。本計畫利用二維影像之資訊加上三維影像之深度資訊，突破僅採
用二維影像之平面資訊於辨識之瓶頸，進而獲得更佳之辨識能力。本計畫之主要成果，除
建立臉部辨識系統之外，並已將研究成果投稿且接受於 Expert Systems with Applications。 
 
關鍵字: 主成份分析法、 區域自相關係數、臉部辨識、雙影像對應系統、BPN
三 報告內容 
1. Introduction 
Biometric measurements received an increasing interest for security applications in the last 
two decades.  Owing to the growing number of real world applications, many face recognition 
techniques were developed.  In this area, principle component analysis (PCA) (Turk and 
Pentland, 1991), linear discriminant analysis (LDA) (Zhao et al., 2001), Elastic Graph Matching 
(EGM) (Wiskott et al., 1997), Support Vector Machine (SVM) (Wang et al. 2002) and artificial 
neural networks (Valentin et al., 1994; Zhang et al., 1997) are widely used in the literature as face 
recognition approaches.  Reviews of face recognition techniques can be found easily in literature 
such as Chellappa et al. (1995) and Zhao et al. (2003).  In addition, Local Autocorrelation 
Coefficient (LAC) is a widely used method in character recognition (Hotelling, 1993), time series 
classification (Keogh. and Pazzani, 1998), and face detection and recognition (Goudail et al., 
1993; Goudail et al. 1996; Hotta et al., 1998; Popovici, 2002) because of its economics of 
computation and translation-invariant property.  Goudail et al. (1993) applied LAC to recognize 
a face database with 116 faces and derived a 98% recognition rate.  Later, Goudail et al. (1996) 
used LAC and Linear Discriminant Analysis (LDA) to improve the performance of recognition.  
They found that LAC is fast and reliable method for face recognition, but requires high demand 
of memory.  Popovici (2002) applied PCA to autocorrelation feature vector to avoid the 
computation of the autocorrelation coefficients for pattern recognition. 
3D face recognition is a new research topic, but few studies exist on 3D or multi-modal 
2D+3D face recognition (Chang et al., 2003; Chang et al., 2005).  The early applications of 3D 
face recognition include Cartoux et al. (1989), Lee and Milios (1990), Gordon (1991), Nagamine 
of this project is to integrate the information of 2D and 3D (disparity) faces using different 
approaches of feature extraction and face recognition methods under a stereovision system.  A 
Synchronous Hopfield Neural Network is designed to derive disparity faces from two 
simultaneously scanned face images. Then, PCA and LAC methods are adopted to extract 
features of 2D and 3D faces; and Minimum Distance and Backpropagation Neural Networks are 
performed as face classifiers.  
2. The Proposed Method 
This study proposes a hybrid face recognition system that combines the face information of 
2D and 3D faces under a stereo vision system.  The proposed method is divided into two phases: 
learning and recalling processes, as shown in Figures 1 and 2.  Two face images (left and right) 
were digitized in a parallel stereovision system, and a designed Synchronous Hopfield neural 
network (SHNN) was used to match the left and right face to obtain the 3D face, called disparity 
face.  Two methods, Local Autocorrelation Coefficient (LAC) and Principle Component 
Analysis (PCA), were applied to left and disparity faces to obtain three sets of features, called 
PCA, LAC, and LAC+PCA.  Then, the derived face information, including 2D, 3D and 2D+3D, 
was saved into three databases and learned using Minimum distance classifier and 
Backpropagation neural networks (Figure 1).  After learning process, the testing face was 
digitized for recalling process (Figure 2).  In the following sections, the details of the learning 
and recalling processes are described. 
 Figure 2. Proposed recalling (testing) process 
3. Stereo Matching using Synchronous Hopfield Neural Networks  
3.1 Depth extraction from stereovision 
Figure 3 illustrates the geometry of a parallel stereovision system.  In a parallel stereovision, 
the optical axes are located parallel to each other and perpendicular to the baseline connecting the 
centers of two cameras.  Given a point (x, y, z) in the world coordinate system, the 
corresponding image points in image planes 1 and 2 are (X1, Y1) and (X2, Y2), and the 
three-dimensional information is determined, according to the triangulation in each camera 
 
 
Figure 4. Face alignment for acquisition 
3.2 Scanline-based Synchronous Hopfield Neural Networks (SHNN) 
The 3D face information is derived by matching scanline by scanline.  A general stereo 
matching model can be found in Sun’ (2006).  This study first derives the energy function of a 
given scanline as follows: 
∑ ∑∑∑ ∑ ∑
= == = = =
−⋅+−⋅+⋅=
n
j
n
i
ij
n
i
n
j
n
i
n
j
ijijij VCVBVwAE
1 1
2
1 1 1 1
2 )1()1( +∑∑    (2) 
=
−−
n
i
n
j
ijij dVxx
1
2
max ])[(
where the coefficients, A, B, C, and D are positive weights of component factors; n is the number 
of pixels in a scanline; xi and xj are the corresponding coordinates for the given matching pair in 
the left and right images; dmax is a pre-identified maximum disparity; and Vij and denotes 
a neuron, which represents the match (1) or unmatch (0) status between the left and right points.  
The values of the coefficients can be determined empirically.  
}1,0{∈
The Hopfield neural network is a single layer feedback network.  Its topology is shown in 
Figure 5.  Vij is used to denote a neuron representing the matching relationship defined in the 
domain property.  To find the minimum of the energy function, a stochastic activation rule is 
required when using the synchronous Hopfield neural network.  According to the postulates of 
netij = Ψij – Tij ,              (5) 
Ψij= ,      (6) ki
n
k
kiijkikiij VddVdda ,1
1
,1,1,13 )(]2)[( +
=
+++ ⋅−⋅+⋅−∑ η
   Tij = 4max21 2)2( adddaSa ijijij +−+⋅ .          (7) 
Then, the change in energy is calculated as 
ΔEij = netij × ,              (8) ijVΔ
where = Vijnew – Vijold.  Contrast to asynchronous Hopfield neural networks, synchronous 
Hopfield neural networks update a cycle of two states rather than a single state (Zurada, 1992).  
After a cycle of updating two states, the neuron Vij is updated only when 
ijVΔ
ΔE  is less than zero.  
The state of neurons is continuously updated and fed back to the network.  After a certain 
number of cycles, the network will eventually reach a steady state and an approximate optimal 
solution will be achieved.   
3.3 Procedure of SHNN 
The operational procedure of SHNN is as follows: 
Step (1) Construction of the energy function. 
Step (2) Initialization of SHNN.  For each feature point i on the left scanline, assign randomly a 
match (i.e., the indicator variable is equal to 1) to one of the neurons from Vi1 to Vin, and 
assign the rest of Vij as 0.  The initial solution s(0) at state t=0 is formed below as an 
example 
     s(0) =(V11, V12, ..., V1n;; V21, V22, ….., V2n; ….  Vij ,…; Vm1, Vm2, …,  Vmn), 
=(1, 0, 0, …, 0; 0, 1, 0, 0,…, 0; 0,…1, …;0, …1, … ; 0, 0, …, 1).  
Step (3) Synchronous update. SHNN updates two states for each cycle.  For the current state s(t), 
approximates the original patterns for pattern recognition.  This study first applies the PCA 
technique to 2D faces and disparity faces individually, and then integrates the recognition 
information of both to improve the face recognition rate. The details of training and recognition 
processes for 2D and 3D faces can be referred to Sun et al. (2007).  The eigenvectors with the 
largest k eigenvalues are chosen among the M eigenvectors to form a basis.  k is determined 
based on a pre-specified threshold θ, as follows: 
θ
λ
λ
M
1
i
k
1
i
>
∑
∑
=
=
i
i ,                                 (9) 
where λ1, λ2,.., λk represent the eigenvalues corresponding to the eigenvectors derived from the 
covariance matrix (Bishop, 1995), and θ represents the percentage of variation captured by the K 
largest eigenvectors.   
4.2 Feature extraction using LAC 
Local autocorrelation coefficients (LAC) own the advantage of shift-invariance and low 
computational complexity (Gudail et al. 1996). The feature extraction module computes 25 local 
autocorrelation coefficients from left face and disparity face images, using the kernels 
represented in Figure 6(a).  Each kernel is scanned over the entire face image, and the product of 
the pixels marked in black is computed for each possible position. All the products corresponding 
to a kernel are then summed as one of 25 coefficients. The entire operation is performed using the 
25 different kernels, thus providing a 25-dimensional feature vector. For an N by N face image, 
the Kth order LAC is calculated as below (Hotta et al. 1998). 
∑∑
= = =
++=
N
i
N
j
K
k
jjiik kk
fC
1 1 0
 ,C                          (10) 
where  is the order of the LAC;, which is the pixel marked in black;  is the { 2 ,1 ,0∈K } ) ,( ji
calculates the Euclidean distance (d) between the standard normalized feature vector (b) and the 
testing normalized feature vector (a).  
( ) 2/1
1
2 ⎟⎠
⎞⎜⎝
⎛ −= ∑
=
n
i
ii bad                              (11) 
where  is the number of feature extracted from PCA, LAC or PCA+LAC methods; ai is the 
i_th feature in the standard vector a; and bi is the ith feature in the testing vector b. For the 2D+3D 
cases, the product rule is adopted to fuse the information of 2D and 3D faces, and such that the 
face with the smallest product value is derived. 
n
Backpropagation neural networks (BP) are the most widely used networks and are considered 
the work horse of artificial neural networks (Rumelhart et al., 1986).  As shown in Figure 7, a 
BP consists of (1) an input layer with nodes representing input variables to the problem, (2) an 
output layer with nodes representing the dependent variables, and (3) one or more hidden layers 
containing nodes to help capture the nonlinearity in the data.  The neurons between layers can 
be fully or partially interconnected between layers with weight (wij).  The data are fed forward 
from the input layer, through hidden layer, to output layer without feedback.  Then, based on the 
feedforward error-backpropagation learning algorithm, BP will search the error surface using 
gradient descent for point(s) with minimum error. The fundamentals of BP can be found in 
Basheer and Hajmeer (2000). 
CPU and 512 MB SDRAM.  The stereovision system was constructed using the Alpha 2 frame 
grabber (Euresys Inc.) connected to two parallel aligned CIS VCC-870a B/W CCD cameras. The 
baseline distance (b) of the two parallel cameras was set to 13.5 cm, and the paired face images 
were acquired at approximately 75 cm from the stereovision system.  The face images were 
digitized at a size of 1376×1032 pixels and a window with 500×500 pixels covering the face area 
within a white background.  The stereo matching algorithm is implemented using the C++ 
language under Windows XP platform, as shown in Figure 4 previously.  According to Sun et al. 
(2007), their face images were acquired using the resolution of 640 by 480 pixels, which derived 
38 levels of depth maps as shown in Figure 8 (a) and (b), while using 1376 by 1032 pixels for 
face acquisition extends the depth of 3D faces to 64 levels as shown in Figure 13 (c) and (d). 
Obviously, more levels of depth maps lead to higher resolution of disparity for 3D face 
recognition. 
              
(a) Face image with 640 by 480 pixels        (b) Disparity face (32 levels) 
              
     
(a) Expression 1      (b) Expression 2     (c) Expression 3      (d) Expression 4 
 
       
(e) Expression 5       (f) Expression 6        (g) Expression 7 
Figures 9. Training faces -- seven different facial expressions  
        
(a) Expression 8        (b) Expression 9        (c) Expression 10      
            
       
(d) Expression 11        (e) Expression 12     (f) Expression 13 
Figures 10. Testing faces -- six different facial expressions 
5.3 Feature extraction using PCA 
For the learning process, the digitized face image (left face and disparity face) ware 
represented into the n-dimensional vector, and PCA method was used to reduce the 
 (a) 10 subjects 
 
(b) 100 subjects 
Figure 11. Determination of LAC kernel  
5.5 Face recognition using BP 
When using a Backpropagation neural network for pattern recognition, the parameters 
including learning rate, momentum, types of transfer function, and structure of networks (number 
of hidden layers, and number of nodes in hidden layers) must be determined in advance.  In this 
study, the nonlinear Sigmoid function (Figure 12), which is the most commonly used transfer 
  
(a) 2D face using LAC    (b) Disparity face using LAC 
   
(c) 2D+Disparity faces using PCA    (d) 2D face using PCA 
    
(e) Disparity face using PCA    (f) 2D+Disparity faces using PCA 
    
   (g) 2D face using LAC+PAC    (h) Disparity face using LAC+PAC 
PCA, LAC, and PCA+LAC feature vectors and recognizing with Minimum Euclidean Distance 
method.  Accordingly, minimum distance method (MD) worked well when the size of database 
was small for all three feature vectors (PCA, LAC and PCA+LAC) with 2D, 3D and 2D+3D 
faces, but the recognition rate gradually decreased when the size increased.  In average, the 
recognition rate of using 2D+3D feature vectors outperformed the other two’s in all methods, 
which implies that adding 3D information does offer more effective information for face 
recognition.  Moreover, combining LAC+PCA features, according to Figure 14(c), leads to a 
higher recognition rate (≧99%); and PCA method outperforms LAC method when using MD 
method. 
 
(a) PCA method 
 
     
 
 
 
 
 
 
Table 4. Comparison of classifiers with different feature extractors (100 subjects) 
6 Conclusions 
This study presents a hybrid face recognition method that combines the information of the 2D 
and disparity faces with different feature extraction methods, and an experiment using 100 
subjects was conducted for validation.  The proposed recognition method is based on the 
implementation of the Principle Component Analysis (PCA) and Local Autocorrelation 
Coefficient (LAC), which are both effective method to extract face features, and Backpropagation 
neural networks for face classification.  In addition, a quick and effective stereo matching 
method that uses Synchronous Hopfield neural network (SHNN) was developed and 
implemented.  According to our experiment, 2D face recognition still has better performance 
than 3D’s due to their rich features and high resolutions.  Although 2D face recognition with 
PCA has good recognition rate, using 2D+3D faces obtains best recognition performance.  
Using BP with PCA+LAC successfully recognizes different expression faces under the size of 
100 subjects. Overall, the following conclusions can be derived: (1) increasing the resolution of 
3D faces adds more information in images and leads to a higher recognition rate; (2) adding 3D 
face information improves the performance of 2D face recognition; (3) fusing the features of PCA 
 
 
2D 
faces 
3D 
face 
2D +3D 
face 
MD 0.9800 0.9383 0.9833 
PCA 
BP 0.9950 0.9683 0.9983 
MD 0.9603 0.8717 0.9750 
LAC 
BP 0.9905 0.9217 0.9867 
MD 0.9807 0.915 0.9900 
PCA+LAC 
BP 0.9983 1.0000 1.0000 
Faces
Methods Classifier 
Reference 
Achermann, B, Jiang, X., Bunke, H., 1997, Face recognition using range images, International 
Conference on Virtual Systems and MultiMedia, 129-136. 
Beumier, C., Acheroy, M., 2001, Face verification from 3D and grey level cues, Pattern 
Recognition Letters, 22, 1321-1329. 
Blanz, V., Vetter, T., 2003, Face recognition based on fitting a 3D morphable model, IEEE 
Transactions on Pattern Analysis and Machine Intelligence, 25 (9), 1063-1074. 
Cartoux, J.Y., LaPreste, J.T., Richetin, M., 1989, Face authentication or recognition by profile 
extraction from range images, Proceeding of the Workshop on Interpretation of 3D Scenes, 
194-199, November 1989. 
Chang, K.I., Bowyer, K.W., Flynn, P.J., 2005, An evaluation of multimodal 2D+3D face 
biometrics, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 27, No. 4, 
April 2005. 
Chang, S, Rioux, M, Domey, J., 2003, Face recognition with range images and intensity images, 
Optical Engineering, 36 (4), 1106-1112, April 1997. 
Chang, K.I., Bowyer, K.W., Flynn, P.J., 2003. Face recognition using 2D and 3D facial data. 
Workshop in Multimodal User Authentication, pp. 25-32, Santa Barbara, California, December, 
2003. 
Chellappa, R., Wilson, C.L., Sirohey, S.A., 1995, Human and machine 
recognition of faces: A Survey.  Proceedings of the IEEE, Vol. 83, No. 5, May, 1995, 
705-740 
spherical correlation principal directions for curved object recognition, Third International 
Conference on Automated Face an Gesture Recognition, 372-377, 1998. 
Tank, D., Hopfield, J., 1986, Simple 'Neural' optimization networks: An A/D converter, signal 
decision circuit, and a linear programming circuit. IEEE Transactions on Circuits and Systems, 
CAS-33 (5), 533-541. 
Tsalakanidou, F., Tzovaras, D., Strintzis, M.G., 2003, Use of depth and colour eigenfaces for face 
recognition, Pattern Recognition Letters 24, 1427-1435.  
Turk, M., Pentland, A., 1991, Eigenfaces for recognition, Journal of Cognitive Neuroscience, 3 
(1), 71-86 
Valentin, D., Abdi, H., O’Toole, A.J., Cottrell, G.W., 1994. Connectionist models of face 
processing: a survey. Pattern Recognition 27 (9), 1209-1230. 
Wang, Y., Chua, C., Ho, Y., 2002, Facial feature detection and face recognition from 3D and 3D 
images, Pattern Recognition Letters, 23, 1191-1202. 
Wiskott, L., Fellous, J., Kruger, N., von der Malsburg, C., 1997. Face recognition by elastic 
bunch graph matching. IEEE Transactions on Pattern Analysis and Machine Intelligence 19 (7), 
775-779. 
Wu, J., Zhou Z.H., 2002, Face recognition with one training image per person. Pattern 
Recognition Letters 23, 1711-1719. 
Zhang, J., Yan, Y., Lades, M., 1997. Face recognition: eigenface, elastic matching, and neural nets. 
Proceedings of IEEE 85 (9), 1423-1435. 
Zhao, W., Chellappa, R., Rosenfeld, A., 2003, Face recognition: a literature survey, ACM 
三、計畫成果自評部份 
本計畫主要之研究領域為臉部辨識系統開發與應用，計畫中提供一名研究生參予研究
機會，藉由原 2D 資訊加入 3D 資訊，並利用 PC 及 BPN 之方法探討於小型 Database (100~300
人)之可行性，參與計畫之研究生可實際學習解決實務性問題之方法，並達到理論於實務結
合之目的；本研究之成果簡述如下： 
(1) 臉部辨識之方法 review; 
(2) 3D 臉部深度圖之方法建立與實踐; 
(3) 2D 與 3D 臉部資訊之整合 
(4) PCA、BPN 等相關技術之程式發展; 
(5) 成功發展臉部辨識之系統，且辨識率達 99%; 
(6) SCI 論文發表一篇(已接受於 Expert System With Applications)。 
 
 
Program Committee Chair: Eugene Santos
下午則參加一場關於臉部辨識之報告，主要以 Rough Common Vector 之方法，將臉部影像進
行降維及線性轉換，經實驗得知該方法較 PCA、LDA 結果更佳，該方法由一日本學者與中國
學者合作，成果豐富，所得之演算法更有效率。
Face recognition 之報告
至下午 6:30 則參與個人之 Poster session，會場中參與之情形如下:
Poster Session 之現場及討論情形
Speaker I (日本學者) Speaker III (Keynote speaker)
並於當日參與該會所主辦之 Welcoming Banquet。
4. 於10月10日上午則參與Keynote speaker報告Artificial Intelligence In Transition: From Objects
And Rules To Networks And Dynamics，在參與三場報告後，即前去旅館check out，並往機
場準備回程。
二、與會心得
參與此次研討會之行程中，並順道參觀 Montreal、Quebec City 等成市之建設與風景，並於參
與研討會之過程中獲得以下心得：
1. 於 Image processing/pattern recognition session 中發現，日本之動態影像處理之研究相當活
潑，並不止於研究性之主題，相當注重其實用性，值得學習。
2. 日本學者於基礎數理之研究紮實，故於臉部辨識方面可以有所突破，但不知實務化之成
果，有待觀察。
3. 本次會議行程中，發現大陸之學者參加人數越來越多，而台灣之學者相當少，因此研討會
為 IEEE 之主流研討會之一，可見我國於學術之經費不足或不重視。
4. 於參觀加拿大之過程中發現，韓國產品之市占率相當高，包括汽車、電子產品等。
5. 加拿大之自然資源豐沛、景色優美，而國家對於該資源之保護、開發及利用得當，使得該
國之國力保持於世界強國之列。
6. 與會時發現本次參與會議之台灣學者數量不多，建議國科會相關單位，針對我國學者參與
機會及意願降低之原因深入探討。
7. 由於國內之教師多忙碌於教學及研究，參與國外研討會之機會不易，且目前國內所辦之研
討會多屬於區域性或國內之研討會，國際性不足，故建議應鼓勵國內之學會(IE 學會)或各
 
 
 
  
Abstract—This study presents a boundary-based corner 
detection method that achieves robust detection for digital 
objects containing wide angles and various curves using 
curvature.  The boundary of an object is first represented into 
curvature measured by K-cosine.  Then, by modifying the 
corner detection error [11], this study proposes a suitable K 
value and curvature threshold for robust corner detection.  
Finally, the K-cosine corner detection (KCD) algorithm is 
proposed and verified with several commonly employed digital 
objects.  The experimental results reveal that the proposed 
method is free from translation, rotation and scaling, and is 
superior to Tsai’s method [34] in computation speed in 
discriminating false targets. 
I. INTRODUCTION  
Corner detection plays a critical role in image processing 
and pattern recognition in which many different approaches 
having been developed. These approaches can be broadly 
classified into two major categories: gray-level and 
boundary-based approaches. Gray-level approaches match 
corners using gray-level corner templates [17][14] or 
compute the gradient at edge points [28][25] while 
boundary-based approaches analyze the properties of 
boundary pixels to identify corners [12][34]. Representing 
boundaries using different descriptors and then searching for 
the corner features is common in boundary-based approaches. 
Various boundary descriptors, such as chain code, curvature, 
and Fourier Descriptor (FD), have been developed. The 
Freeman chain code is easy to use and can be treated as a 
polygonal approximation of a contour, although it is less 
efficient and accurate [6]. Fourier descriptors have been 
successfully applied to contour enhancement and object 
inspection, providing position, scale and orientation invariant 
properties by normalization [3][26]. However, they require 
heavy computation when calculating the complex equations 
of the forward and backward transformations. 
Curvature, defined as the change rate of the slope, has been 
widely employed in different applications such as shape 
representation, feature extraction, corner detection and object 
 
T. H. Sun is currently an associate professor in Department of Industrial 
Engineering and Management at Chaoyang University of Technology, 
Taiwan. (e-mail: thsun@mail.cyut.edu.tw). 
C. C. Lo is with the Department of Informatics, Fo Guang University, 
Taiwan  (e-mail: locc@mail.fgu.edu.tw). 
T. H. Tsai, was graduate student in Department of Industrial Engineering 
and Management at National Taipei University of Technology.  Currently, he 
is working as a computer engineer in Utechzone Co., Ltd., Taiwan.  (e-mail: 
kio.adsl@msa.hinet.net). 
F. C. Tien* is with the Industrial Engineering and Management 
Department, National Taipei University of Technology, Taiwan. 
(Corresponding author, e-mail: fctien@ntut.edu.tw, tel: +886-2-27712171 
ext. 2343, fax: +886-2-27317168) 
recognition [3][4][9][15][17][21][32]. Different numeric 
curvature estimation approaches have been discussed.  
Rosenfeld and Johnston initially defined curvature as a 
K-cosine function, where K denotes a region of support on the 
boundary [23]. Mokharian and Mackworth [19], Teh and 
Chin [31], and Sohn et al. [29] expressed curvature with a 
formula involving its first- and second-order directional 
derivatives. Liu and Srinath evaluated the curvature by 
convolving the edge direction function with the first 
derivative of a Gaussian function at each pixel [16]. Fairney 
et al. experimented with several different measures of digital 
curvature and found them to be unreliable in the presence of 
noise [5]. Tsai and Chen computed directly the curvature by 
measuring the first- and second-order derivatives of the 
continuous functions[33]. Tsai measured the curvature by 
using neural networks to identify the included angles at 
boundary points [32]. Later, Tsai et al. employed the 
eigenvalue of covariance matrices to measure the curvature 
and detect the sharp corners in a contour [34]. Arrebola et al. 
evaluated the curvature as the correlation factor coefficient of 
the forward and backward histograms in a K-vicinity of a 
given point [1]. For simplicity, this study adopts the K-cosine 
to measure the magnitude of curvature. 
Many algorithms have been developed to locate corners 
using the local curvature maximum [2][7][8][19]. Rattarangsi 
and Chin found corners by transforming the coded digital 
scale space map into an organized tree by calculating the 
maximum of absolute curvature [22]. Sohn et al. applied a 
constrained regularization method to derive an optimal 
smoothing factor for curvature estimation, which facilitates 
the corner detection [29]. Sohn et al. later developed the mean 
field annealing strategy, which uses simulated annealing to 
improve the approximation of the curvature estimation for 
corner detection [30]. Lee et al. developed a multi-scale 
corner detection algorithm using wavelet transform of 
contour orientation, utilizing both the local extrema and the 
modulus of transform results to identify corners and arcs [13]. 
Sheu and Hu developed a two-phase corner detection scheme 
[27]. The first phase specifies the points with significant 
curvature as candidate corners, and then the second phase 
verifies them using a self-adjusting convolution window. 
Inesta et al. employed curvature to identify dominant points 
for polygonal approximations of real imaged objects, and 
defined K-curvature, K-angular bending and K-cosine [10]. 
Mokhtarian and Suomela  first extracted edges using a Canny 
edge detector, and then detected corners based on the 
maximum of absolute curvature in the curvature scale-space 
[20]. Urdiales et al. developed a look-up table to determine 
Boundary-based Corner Detection using K-Cosine 
Te-Hsiu Sun., Chih-Chung Lo, Po-Shen Yu, Fang-Chih Tien.  
11061-4244-0991-8/07/$25.00/©2007 IEEE
 
 
 
The chord property of a digital line stipulates that, “The 
line segment joining any two points of a digital straight line 
lies everywhere within a pixel distance,”[24]. According to 
this property, the corner (or joint) detection error [11] was 
defined as follows. 
<Definition > Corner Detection Error 
Given a corner with its angle θ, its corner detection error 
is defined as  
  ie =tan ( 2
θ ).                                                            (5) 
 
Fig 3. Corner Detection Error 
As depicted in Fig. 3, ab  denotes the bisector of adc∠ , and ei 
denotes the distance that the boundary moves along ac  until 
the distance of ab  equals one pixel length. Since ac  denotes 
a line, any two points on this line should lie within a pixel 
difference such that ab  is defined as one pixel length, 
indicating that K depends on the corner error ei. However, this 
definition does not address digitization errors, and produces 
unreliable corner detection in practice. 
B. Properties of K  
K is the region of support which smoothes the curvature 
functions. According to the corner detection error, ei, the 
lower bound is derived as follows. 
<Property 1> Lower Bound (KL) of K 
    Given an object with included angles ∠ A1, ∠ A2, …, ∠ An , KL∈ N exists such that  
Ki ≥  KL , ∀  i = 1, 2,…, m  
where  
KL=
⎪⎩
⎪⎨
⎧
+…
∈=}…
=
(6.b)                                                 . otherwise  1, ] }{Int[
(6.a)        }{      ),
2
},,,{
tan( 
321
n .., 2, 1,i
21
321
n
i
n
n
, e, , e, eeMax
NeMax
AAAMax
, e, , e, eMax {e
L
  In Eq. (6), m denotes the number of points on the boundary; 
n is the number of corners in the object, and Int[*] denotes a 
function converting a real number to a natural number (N). 
This study suggests that the value of K must be greater than 
the corner detection error of the largest angle (Max{Ai}) to 
compensate for the digitization errors. That is, the lowest 
value of K should be the maximum of all corner detection 
errors. When Max{ei} is not an integer, it is rounded up to the 
closest integer according to Eq. (6b). Meanwhile, the 
maximum of K should be the shortest segment of the 
boundary. The upper bound of K can be derived as follows. 
<Property 2> Upper Bound (KU) of K  
Given an object with its boundary S, the upper bound of the 
K-cosine curvature is given by 
KU =  } )({
...,2,1 ini
SLengMin
=
                                               (7) 
In Eq. (7), the function Leng(Si) denotes the number of 
points in segment Si.  The domain of K is located in [0, 
Leng(Si)]. )}({
,..,3,2,1 ini
SLengMin
=
 denotes the boundary segment 
with the lowest number of boundary points, which is the 
upper bound of K. 
As mentioned previously, Eq. (5) does not address the 
effect of digitization errors, making the corner detection 
process unreliable. This effect is clearly demonstrated in Fig. 
4. The distance ab  is over one pixel length when the corner 
point is missing because of digitization errors. Therefore, the 
corner detection error is modified by including a coefficient 
γ into the equation, and hence is derived below. 
)
2
tan(θ = 
ab
ac  = 
γ
ac   
Q 
γ
me = )
2
tan(θ  
∴  me = γ )2tan(
θ  .                                                       (8) 
 
Fig 4. Modified Corner Detection Error 
In accordance with the modified corner detection error, em, 
a suitable K is proposed as follows. 
<Property 3> Kp  
Given an object with angles ∠ A1, ∠ A2, …, ∠ An, and θ 
=Max{A1, A2, A3, …, An}, a Kp∈[KL,, KU] exists such that  
Kp =   , 1)]
2
( tan Int[γ +θ                                                 (9) 
where γ ∈  N. 
Accordingly, Kp is defined as the rounded-up value of em 
with various γ values. When the system has few digitization 
errors, γ is set to 1 (K = KL). Thus, a large γ provides reliable 
and consistent detection for wide-angle corners or corners 
with heavy digitization errors.  
C. Determination of Curvature Threshold 
The objective of the curvature threshold process is to 
extract the boundary points with K-cosine values larger than a 
pre-specified threshold, and then identify the corners as the 
points with the highest K-cosine curvature within the 
extracted areas. Figure 5a shows the boundary representation 
of a digital object in which four corner areas are detected 
because the curvatures of boundary points are above a 
threshold, T, as depicted in Fig. 5b. Accordingly, using a 
lower threshold discards more boundary pixels, leading to the 
γ 
1108
 
 
 
 
(a) Scale=100%      (b) K=3, T=-0.600, γ=2, and No. of pixel=914 
Fig 9. KCD in circular objects (8 corners) with different scales 
      
(a) Rotation with 30°                  (b)K=3, γ=2, and T= -0.600 
Fig 10. KCD in circular objects (7 corners) with different orientations 
       
(a) Rotation with 30°    (b) K=3, T =-0.600, γ=2, and No. of pixel=724 
Fig 11. KCD in circular objects (8 corners) with different orientations 
B. Experiment II — Benchmarks 
The proposed method was benchmarked using Tsai’s 
methods [34]. Tsai et al. utilized the eigenvalue of covariance 
matrices to estimate the curvature of a two-dimensional 
boundary point over a small support region [34]. The method 
measured curvature information by calculating the smaller 
eigenvalue, λs, of the covariance matrix of a given region of 
support and then identified corners by setting a 
self-determined threshold of the curvature information of 
boundary points at T−λs.  However, users must obtain the 
workable values of s and T−λs empirically.  A leaf digitized 
with various scales was used for verification and comparison 
of efficiency. The corners of the leaf image, depicted in Fig. 
12a, b, were both successfully identified by Tsai’s method 
and the proposed KCD, but KCD provides K’s and curvature 
thresholds as shown in Fig. 12c.  Considering computation 
speed, the complexity of Tsai’s method is O(N×s2), where N 
denotes the number of points on the boundary, and s denotes 
the number of points in the region of support, when 
calculating the covariance matrix and eigenvalues. When 
computing the K-cosine, the complexity of KCD is O(N×K), 
where K denotes the previously defined region of support.  
Figure 14 compares KCD and Tsai’s method in terms of 
computation speed given the same region of support and 
varying numbers of boundary points.  
 
                           
(a) Original image of a leaf        (b) Image after threshold 
 
(c) KCD with K=6, T=-0.600, γ=3 
Fig 12. KCD with a leaf image 
0
0.5
1
1.5
2
2.5
3
3.5
4
0 200 400 600 800 1000
C PU Time(sec .)
No. of points
Tsai's
KCD
 
Fig 13. Comparison of KCD and Tsai’s method in computation speed 
IV. CONCLUSIONS 
This study presents a position, orientation and scale 
invariant boundary-based corner detection method that 
reliably detects corners on two-dimensional digital objects 
with various curves and wide-angle corners. For 
effectiveness and simplicity, the boundary of an object is first 
expressed with curvature, which is measured using K-cosine. 
Then, the corner detection error is modified by including an 
effective coefficient γ, and recommends the use of Kp and the 
curvature threshold (T) to detect corners reliably.  
Additionally, the essential properties of curvature (K-cosine), 
including the lower and upper bounds, are discussed, and the 
KCD algorithm is proposed and implemented for practical 
use. 
Experiments were performed to demonstrate the 
effectiveness and efficiency of the proposed KCD method.  
KCD’s ability to detect wide-angle corner points was tested 
using a set of triangular objects. The experimental results 
show that the proposed method successfully identifies the 
corners with wide angles of up to 160°. Next, the proposed 
1110
