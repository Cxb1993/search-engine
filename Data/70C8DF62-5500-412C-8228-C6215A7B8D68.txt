 2
二、研究目的 
蛋白質的生物學功能訊息與其亞細胞的定位
有著非常緊密的聯繫，例如，如果知道某新蛋白
的功能與氧化磷酸化有關，那麼在細胞中它就很
可能是位於線粒體之內。目前確定蛋白質亞細胞
定位的實驗技術[Murphy, 2000]，除了傳統的亞細
胞分離技術外、融合綠色螢光蛋白、質譜和同位
素親和標籤、電子顯微鏡與螢光顯微鏡等實驗技
術提供了一些比較精確的亞細胞定位數據。但
是，基於實驗方法所獲得的定位結果具有較強的
主觀性與多變性，並且重複性也比較差，而且這
些技術多是昂貴且耗時的。相對於生物數據集中
蛋白質序列數據訊息的急遽膨脹，單純依靠這些
實驗技術來註釋蛋白質的亞細胞定位遠不能滿足
蛋白質體學研究的需要。鑒於研究需求與實驗所
得到的定位註釋之間巨大的空白，發展電腦計算
方法從蛋白質的一級序列出發，預測蛋白質亞細
胞定位變成日益重要的問題。一些電腦計算的方
法取得了不錯的預測結果[Donnes; 2004]，為功能
基因組註釋工作提供了一個重要的、可信賴的工
具。實際上，近年來一些新建立的功能資料庫都
已開始正式地收錄由電腦計算預測得到的亞細胞
定位訊息[Rey, 2005; Heazlewood, 2007]。而且隨
著某些特定領域的蛋白質功能研究的不斷深入，
出現了很多特定問題，如凋亡蛋白[Zhou, 2003]、
核蛋白亞定位[Lei, 2005]、線粒體亞定位[Du, 2006]
等的亞細胞分類預測，使亞細胞定位預測成為了
生物資訊學與實驗生物學結合的最為緊密的領域
之一。 
 
三、文獻探討 
3.1 蛋白質亞細胞定位預測 
　以實驗方法來注釋蛋白質亞細胞定位的方式，
其共同的缺點便是太過耗時，且常受限於蛋白質
的純化及分離技術的瓶頸，而且進行實驗所必須
花費的成本也是非常的大，因此在很多情況，必
須藉助生物資訊的方法來進行預測的工作。透過
電腦計算從蛋白質序列來進行亞細胞定位預測是
可經濟地確認某未知蛋白質的功能 (如圖二所
示)，也是生物訊息學研究的有力工具。Nakai等學
者最先使用“if-then”規則構建了一個專家系統來
進行亞細胞定位預測 [Nakai,1991]；Cedano等學
者則是對蛋白質的細胞定位和胺基酸組成做了相
關性分析[Cedano, 1997]。近年來，統計學和機器
學習方法廣泛地應用在蛋白質亞細胞定位的預測
問題中，機器學習方法的基本思想是根據已有生
物數據中發現有意義的生物學知識或者規律，通
過推理、模型匹配或樣本學習從中自動學習知識
和規則，然後利用這些規則去對未知數據庫進行
預測。最近鄰法(Nearest neighbor algorithm)、類神
經網路(neural networks)、隱藏馬可夫模型(hidden 
Markov model) 、 支 持 向 量 機 (support vector 
machine)和貝式網路(Bayesian network)等都是亞
細胞定位預測中常用的機器學習演算法。 
 
  
圖二、由蛋白質序列預測蛋白質亞細胞定位 
 
3.2 支持向量機 
支持向量機(Support Vector Machine， SVM) 
是最近被提出來的一種類神經網路架構[Cortes, 
1995; Vapnik, 1995]，它以 Vapnik 的統計學習理論
為基礎，而具有極優良的推理能力(Generalization 
ability)，SVM 不像傳統的圖訊識別技術以最小化
經驗風險(Empirical Risk)為目標 — 即使得訓練
資料的分類誤差最小，SVM 以最小化結構風險
(Structural Risk)為目標 — 即使得未知的資料(即
測試資料)的分類誤差在一個機率上界以下。這種
新的分類技術等同於最小化推理誤差的上界，雖
然支持向量機一開始是提出來解決二元分類問
題，但是也有學者提出單類別支持向量機
(one-class SVM)來解決單類別分類的問題[Tax, 
1999]，並且也有學者提出支持向量迴歸機來解決
迴歸的問題[Vapnik, 1995]。 
有越來越多的學者因為支持向量機具有優秀
 4
  
 
當所有 K 個超球都建立好後，我們可以由他們的
球心 ka 與球半徑 kR 來決定新進樣本點是屬於哪
一個類別，使用 ),( kSsim x 函數計算樣本點x屬於
第 k 個類別的程度(樣本點 x 與超球 kS 的相近程
度)，則我們可以由下方式決定新進樣本點 x 的類
別: 
class of ),(maxarg
1
k
Kk
Ssim xx
Κ=
≡ . 
),( kSsim x 函數將樣本點到第 k 類球心 ka 的距離
與第 k 類球半徑 kR 都納入計算，下一節我們將會
介紹不同的 ),( kSsim x 函數選擇。球狀支持向量機
的優點在於球心 ka 與球半徑 kR ，可以表示出該類
別 k 的平均值(mean)與變異量(variance)的資訊，
而這些資訊對於樣本分布不均勻(Imbalanced)的
情形特別重要，以圖三為例，SVM 的分類線是在
邊界(margin)的正中間，不過以貝式分類器的角
度，如果某一個類別分佈的變異量比較大，則樣
本屬於該類別的可能性越高，所以最佳決策線也
距離該類別比較遠，而使用球狀支持向量機所得
到的決策線會更接近最佳的貝式分類線。 
 
 圖三、球狀支持向量機的球心 ka 與球半徑 kR 可以表示出
該類別的平均值(mean)與變異量(variance)的資訊，所以球狀
支持向量機得到的決策邊界會更接近最佳的貝式分類線。 
 
 
傳統的球狀支持向量機，並沒有把邊界(margin)
的觀念納入考慮，根據 Vapnik 的統計學習理論，
最 大 化 邊 界 距 離 等 同 於 最 小 化 推 理 誤 差
(generalization error)的上界，所以在本計畫中，我
們將提出一個使用最大邊界的球狀多類別支持向
量機，並且把它應用在蛋白質亞細胞定位問題中， 
 
圖四、最大邊界的球狀多類別支持向量機。 
 
假設給定一組訓練資料 ),(),...,,( 11 NN yy xx ，
其中 ni R∈x  且 },...,1{ Kyi ∈  是樣本點 ix 的
類別標籤，對每一個類別 k，我們在特徵空間中建
立一個對應的超球 kS ，超球 kS 只包含類別 k 的樣
本點，並且將其他類別的樣本點剔除在超球外，
並且此超球使用最大的邊界(margin)來切割第 k類
與其他類別的樣本點，球狀支持向量機的邊界
(margin)定義為球殼與最靠近它的負樣本之間的
距離(如圖四所示)，為了最大化球狀支持向量機的
邊界，我們將使用一個邊界因子 kd 。超球 kS 的球
心 ka  與球半徑 kR 可經由下面的最佳化問題求
出:  
∑∑
≠=
++−
kyl
l
kkyi
i
k
kkdR
li
likkk N
C
N
CMdR
::
22
,,,,
minimize ξξξξa        
subject to    
kyiR iikki =∀+≤−Φ such that   )( 22 ξax ,     
kyldR llkkkl ≠∀−+≥−Φ such that   )( 222 ξax ,   
0,0 ≥≥ li ξξ    li,∀             
其中  iξ , lξ  是限制條件的差額變數 (slack 
variables)， kN 是屬於第k個類別的樣本點數目，
而 kN  是不屬於第k個類別的樣本點數目，i與l分
別是屬於與不屬於第k個類別的樣本點的索引
 6
 
4.2 蛋白質序列資料集 
本 計 畫 採 用 的 蛋 白 質 序 列 是 以
UniProtKB/Swiss-Prot database release 57.12 數據
庫為基礎，篩選出其中有明確的亞細胞定位註釋
的蛋白質條目，並且刪除有多個亞細胞位址的蛋
白質序列，最終的資料集包含人類、大鼠與小鼠
三個已經被研究者廣泛探討的物種，共計有 4738
個蛋白質序列，8 個不同的亞細胞位址，詳細資
料見表一。 
表一、蛋白質序列資料庫 
Subcellular localization No. of entries 
Cytoplasm 1046 
Endoplasmic reticulum 18 
Golgi apparatus 22 
Lysosome 86 
Mitochondrion 293 
Nucleus 1902 
Peroxisome 62 
Secreted 1309 
Total 4738 
 
4.3 蛋白質序列編碼方式 
胺基酸組成(amino acid composition, AAC) 
Nakashima 與 Nishikawa 在研究中最早發現
蛋 白 質 的 亞 細 胞 定 位 與 胺 基 酸 組 成 有 關
[Nakashima, 1994]，並最早提出了基於胺基酸組成
(amino acid composition, AAC)的編碼方法，AAC
簡單地表示 20 種胺基酸在蛋白質序列中出現的
機率，是一種基本的蛋白質序列編碼方法。AAC
將蛋白質序列映射成 20 維的向量: 
 TAAC vvvvSV )...,,,()( 20321=    
其中  ∑
=
=
20
1k
kii ffv  
而 if 為第 i 種胺基酸在蛋白質序列中出現的次數
(i=1,…,20)，顯然 1
20
1
=∑
=k
kv 。 
使用胺基酸組成編碼(amino acid composition, 
ACC)的優點是計算方便，因此在蛋白質亞細胞預
測定位中，胺基酸組成是應用最普遍的一種編碼
方式。然而 ACC 編碼僅僅用蛋白質序列中 20 種
胺基酸出現的百分比組成來表示一條蛋白質，不
可避免的會遺失一些重要的訊息，例如胺基酸出
現的順序等。因此，學者們提出下列不同的編碼
方式來提高預測能力。 
n 階耦聯組成(n-OCC) 
n 階耦聯組成(n-order coupling composition, 
n-OCC)編碼方式考慮鄰近的 n 個殘基對某個殘基
的耦聯作用[Feng, 2002]。當 n=0 時，n-OCC 編碼
方式退化為胺基酸組成(ACC) 編碼方式，可用一
個 20 維的向量表示；而當 n=1 時，耦聯組成表示
為一個 20×20 的條件機率矩陣 
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎣
⎡
=
)|()|()|(
)|()|()|(
)|()|()|(
)(
YYPYCPYAP
CYPCCPCAP
AYPACPAAP
S
Λ
ΜΟΜΜ
Λ
Λ
φ  
 
其中 ( )21 | aaP 表示蛋白質序列中胺基酸 1a 出現並
且胺基酸 2a 緊接其後的機率，所以我們知道 
( ) 1|20
1
20
1
=∑∑
= =i j
ji aaP ；當 n>2 時，n-OCC 用多維的條
件機率矩陣表示，n-OCC 在很多文獻中也稱為多
肽鍵組成(polypeptide composition) [Luo, 2002]。在
本 計 劃 中 ， 我 們 使 用 零 階 耦 聯 組 成 (zero 
intervening resides)，其編碼方式稱為 KGCC400，
共有 400 維的蛋白質向量；另外，我們也使用結
合 0, 1, 2, 與 3 階的耦聯組成，其編碼方式稱為
KGCC1600，共有 1600 維的蛋白質向量。 
 
4.3 蛋白質序列編碼的合成與特徵選取 
    根據以往的研究文獻顯示，混合不同的編碼
方式能夠有效的提高預測準確度，但是對哪些編
碼方法進行混合，採用鬆散型還是緊密型的結
合，這些都是本計畫中要考慮的問題。目前應用
最為廣泛的還是 ACC(amino acid composition)，主
要原因是 ACC 編碼雖然沒有考慮序列的順序與
殘基之間的相互作用，但是 ACC 的計算簡單，而
且對所有的胺基酸序列都能適用。而 n-OCC 編碼
中，向量的維數隨著 n 的增加呈現指數增加
(20n)，在現實應用中僅侷限於考慮 n<4 的情形；
所以針對 KGCC1600 共 1600 維的蛋白質向量，
 8
編碼方式，不同混合編碼方式的正確率顯示在表
三中，而最佳的正確率出現在使用混合編碼
AAC_info400_NOCC1600 時，我們將 NOCC1600
用 information gain 選取出 400 維的特徵，再加上
ACC 中 20 維的特徵，共有 420 維的特徵向量。
在此時，MSM-SVM 的預測正確率有 85.27%。 
 
表三、蛋白質序列編碼的預測正確率一覽表 
LibSVM MSM-SVM SVM 
 Data name 
C g Acc. Acc. C g M
AAC 9 6.7 78.76 80.86 9 2.8 40
NOCC400 8 0.5 79.35 82.41 9 -3.5 95
NOCC400_ACC 8 0.3 82.01 83.23 9 -4 82
info200_NOCC1600 8 0.6 82.11 82.27 10 -2.0 131
info400_NOCC1600 7 0.3 83.24 83.93 8 -3.7 80
info200_AAC_NOCC
1600 
9 0.6 83.24 84.28 9 -2.3 79
info400_AAC_NOCC
1600 
0.7 0.2 83.77 85.08 10 -3.6 80
gainRatio200_NOCC1
600 
6 0.7 82.51 83.87 10 -1.6 90
gainRatio400_NOCC1
600 
8 3.2 83.84 84.75 13 -3.4 82
gainRatio200_AAC_N
OCC1600 
8 0.8 81.81 82.38 9 -1.7 81
gainRatio400_AAC_N
OCC1600 
8 0.3 83.37 84.08 10 -3.1 100
AAC_info200_NOCC
1600 
8 0.7 83.22 84.71 11 -2.8 109
AAC_info400_NOCC
1600 
7 0.5 81.67 85.27 10 -3.6 110
AAC_gainRatio200_N
OCC1600 
8 0.6 82.99 84.37 10 -2.5 110
AAC_ 
gainRatio400_NOCC1
600 
7 0.3 84.11 85.04 10 -3.4 93
 
在本計畫中，我們開發了一套蛋白質亞細胞
定位預測系統，自動由大量的蛋白質中預測其亞
細胞定位。球狀支持向量機的優點在於球心 ka 與
球半徑 kR ，可以表示出該類別 k 的平均值(mean)
與變異量(variance)的資訊，而這些資訊對於樣本
分布不均勻(Imbalanced)的情形特別重要，SVM 的
分類線是在邊界(margin)的正中間，不過以貝式分
類器的角度，如果某一個類別分佈的變異量比較
大，則樣本屬於該類別的可能性越高，所以最佳
決策線也距離該類別比較遠，而使用球狀支持向
量機所得到的決策線會更接近最佳的貝式分類
線。以表一為例，類別 Secreted 為分布最廣大的
類別，當中共有 1309 條蛋白質序列，類別
Endoplasmic reticulum 為分布最小的類別，當中共
有 18 條蛋白質序列，蛋白質序列的數目比約為 1
比 72，在如此分布不平均的情況下，傳統 SVM
無法達到好的預測正確率，而球狀 SVM 可以達到
更佳的預測正確率，在實驗當中，不論使用何種
蛋白質序列的編碼方式，球狀 SVM 的預測正確率
都比傳統 SVM 來的優異。 
 
由本計畫贊助所發表文章有： 
1. Pei-Yi Hao*, “New Support Vector Algorithms with 
Parametric Insensitive/Margin Model,” Neural Networks, 
vol. 23, no. 1, pp. 60-73, January 2010. (SCI/EI, Impact 
Factor: 2.656. 
2. Wei-Ming Chen, I-Lin Wu, Jung-Hsien Chiang, and Pei-Yi 
Hao, “Prediction of Subcelluar Localization using 
Maximal-Margin Spherical Support Vector Machine,” 
Proceedings of the Ninth International Conference on 
Machine Learning and Cybernetics(ICMLC-2010), 
Qingdao, China, 11-14 July 2010. 
3. 郝沛毅, 陳偉銘, 吳易霖, 林彥伯, 何建霆, 莊足貞, 
“蛋白質亞細胞定位預測-使用最大化邊界球狀支持向
量機,”The 15th Conference on Artificial Intelligence and 
Applications (TAAI 2010), 新 竹 , 台 灣 , 18-20, Nov. 
2010. 
 
參考文獻 
[1] A. Ben-Hur and W. S. Noble, “Kernel methods for 
predicting protein-protein interactions,” Bioinformatics, 
21 suppl: i38-i46, 2005.  
[2] M. P. S. Brown, W. N. Grundy, D. Lin, N. Cristianini, C. 
W. Sugnet, T. S. Furey, Jr.M. Ares, D. Haussler. 
“Knowledge-based analysis of microarray gene 
expression data by using support vector machines.” Proc. 
Natl. Acad. Sci. USA, 97:262-267, 2000. 
[3] J. Cedano, P. Aloy, J. A. Perez-Pons, et al. “Relation 
between amino acid composition and cellular location of 
proteins,” J. Mol. Biol., vol. 266, no.3, pp. 594-600, 
1997. 
[4] J.-H. Chiang and P.-Y. Hao, 2003, "A New Kernel-Based 
 1
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                     日期： 99 年 10 月 25 日 
一、參加會議經過 
     過去幾年來，International Conference on Machine Learning and Cybernetics (ICMLC)研討會招集了
產官學界當中關於人工智慧、機器學習、控制理論的優秀學者共聚ㄧ起分享研究方面的心得，至今已
經是第十九個年頭了，在今年，LCMLC 與 International Conference on Wavelet Analysis and Pattern 
Recognition (ICWAPR)共同在大陸青島舉辦，也邀請了對於小波轉換在圖訊識別當中優秀的產官學者共
同與會，ICMLC 2010 邀請了世界各地一流的研究學者共聚ㄧ堂，彼此討論互動，分享最新的訊息與最
新的研究，並且彼此鼓舞，也邀請了世界ㄧ流的頂尖學者，尤其是 IEEE 學會當中的領導者，與 IEEE 
Systems, Man, and Cybernetics Society 學會的主持人擔任演講者，探討最新進的研究方向與主題，與會
者能有許多機會與世界ㄧ流的學者，近距離的討論他們的研究主題，同時也有許多機會能參考研究先
進寶貴的建議，對研究成果做進一步的修正，進而投稿到正式的期刊當中，ICMLC 2010 論文集被 EI
收錄，論文集也被 IEEE Xplore 索引，而過去 ICMLC 論文集當中部分優秀文章已被國際期刊接受。今
年 ICMLC 2010 於 2010 年 7 月 11-14 日，在大陸青島 InterContinental Qingdao 飯店舉辦，會議主席，
議程委員分別為 
 
Honorary Conference Chairs: 
-  Hongrui Wang, President, Hebei University, China  
-  Michael Smith, Past President, IEEE Systems, Man & Cybernetics Society, USA 
-  William A. Gruver, Simon Fraser University, Canada and Past President, IEEE Systems, Man & 
Cybernetics Society 
 
General Co-Chairs: 
-  Daniel S. Yeung, South China University of Technology, China and Junior President, IEEE Systems, Man, 
& Cybernetics Society, USA 
-  Xizhao Wang, Hebei University, China  
計畫編號 NSC  98－2221－E－151－047－ 
計畫名稱 蛋白質亞細胞定位預測-使用新的參數化邊界與球狀支持向量機 
出國人員
姓名 郝沛毅 
服務機構
及職稱 
高雄應用科技大學資管系副教授 
會議時間 
2010年 7月 11日
至 
2010年 7月 14日 
會議地點 大陸青島 
會議名稱 2010 International Conference on Machine Learnng and Cybernetics (ICMLC 2010) 
發表論文
題目 
Prediction of Subcelluar Localization using Maximal-Margin 
Spherical Support Vector Machine 
 3
 
 5
三、建議 
近年來，大陸地區積極舉辦大型的國際研討會，增加大陸地區學者與世界頂尖學者的互相交流
合作的機會，大陸學者的研究競爭力也是不斷提升，台灣地區的學者也應該有所警惕，不能鬆懈，
建議產官學界能多補助在國內舉辦大型的國際研討會，增加國內學者與世界一流學者的交流合
作，增加國內學者的國際能見度，進而組成研究社群，彼此分享資訊，交流最新研究成果，創意
激發新的研究主題，提昇國內學者的研究競爭力。 
 
四、攜回資料名稱及內容 
   『ICMLC 2010 論文集』收錄上百篇最新的學術研究論文，內容含括機器學習、智慧型系統、
模糊理論、圖訓識別、生物資訊、類神經網路等等研究主題，其中有目前最新的研究方向，最先
進的機器學習與圖訊識別技術，最新的研究成果與問題探討，ICMLC 2010 論文集當中最佳論文也
被推薦修改後投稿國際期刊發表。 
 
  
The total number of proteins in the final dataset was 4738 
proteins for the eight subcellular localizations: cytoplasm, 
endoplasmic reticulum (ER), Golgi apparatus, lysosome, 
mitochondrion, nucleus, peroxisome and secreted proteins. 
We also checked the OC field to remove prokaryotic proteins 
and the proteins annotated with two or more subcellular 
localizations which were excluded from the final dataset. The 
detail number of proteins in the final dataset for the eight 
subcellular localizations was summarized in Table 1. 
TABLE 1. THE NUMBER OF PROTEINS FOR EIGHT SUBCELLULAR 
LOCALIZATIONS 
Subcellular localization No. of entries 
Cytoplasm 1046 
Endoplasmic reticulum 18 
Golgi apparatus 22 
Lysosome 86 
Mitochondrion 293 
Nucleus 1902 
Peroxisome 62 
Secreted 1309 
Total 4738 
 
2.2. Amino acid composition 
Amino acid composition (AAC), which is a base encode 
method, denotes simply frequency of each amino acid in a 
protein. The protein sequences will be map into 
20-dimensional input spaces. The frequency of all 20 natural 
amino acids was calculated by (1), 
 
20 , ,1, acid amino ofy Probabilit 20
1
…==
∑
=
i
f
f
i
k
k
i  (1) 
 
where if  denotes the frequency of i-th amino acid in a 
protein sequence. 
2.3. Traditional Di-peptide Composition 
Another generalized sequence composition is the 
di-peptide composition (DPC) which takes account of two 
adjacent amino acids. The di-peptide composition was 
denoted as 20x20 conditional probability matrix and the 
frequency of two adjacent amino acids was calculated by  
 
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎣
⎡
=Φ
)|()|()|(
)|()|()|(
)|()|()|(
)(
YYPYCPYAP
CYPCCPCAP
AYPACPAAP
S
"
#%##
"
"
 (2) 
 
in where )|( ij aaP denotes the probability of some ja  
amino acid, given the occurrence of some other ia  amino 
acid 
2.4. 5-fold cross-validation test 
The performance of predicting subcellular localization 
was evaluated by 5-fold cross-validation test, in which the 
data set of 4738 proteins for 8 subcellular localizations was 
randomly divided into five subsets of approximately equal 
size. In other words, the data was divided into training and 
testing data in five different ways. After training the 
MSM-SVM with four subsets, the performance of 
MSM-SVM was tested against the fifth subset. The process 
was repeated five times so that every subset is once used as 
the test. 
3. Methodology  
3.1. MSM-SVM 
Inspired by the maximal-margin hyperplane-based 
SVM [13-14] and the support vector domain description 
(SVDD) [15], Wang et al. [16] first incorporated the concept 
of maximal-margin into the hypersphere-based SVM for 
two-class classification problem via a single sphere. In 
previous work, we proposed a modification of the Wang’s 
approach, called the maximal-margin spherical-structured 
multi-class support vector machine (MSM-SVM) [17]. The 
MSM-SVM finds several class-specific hyperspheres that 
each encloses all examples from one class but excludes all 
examples from the rest class. In addition, the hypersphere 
separates the positive examples from the negative examples 
with maximal margin.  
3.2. The Quadratic Programming Problem 
Given a set of training data ),(),...,,( 11 NN yy xx , where 
},...,1{ Kyi ∈  is the class of ix , we first map training points 
into a high-dimensional feature space via a nonlinear 
transform φ , and then find K class-specific hyperspheres 
with minimal radius in the feature space. 
1477
Proceedings of the Ninth International Conference on Machine Learning and Cybernetics, Qingdao, 11-14 July 2010
  
to the center ka  of the kth hypersphere by the following 
equation 
.),(),(2
),(),(2
),(2),()(
,,
,
2
∑∑
∑∑
∑
+−
++
−=−
ml
mlml
li
lili
ji
jiji
l
ll
i
iik
KK
KK
KK
xxxx
xxxx
xxxxax
αααα
ααα
αφ
 (5) 
 
Knowing ka , we can subsequently determine the spherical 
radius kR  and the margin factor kd  of the kth hypersphere 
by exploiting the Karush-Kuhn-Tucker (KKT) conditions:  ( ) 0)( 22 =−−+ kiiki R axφξα , (6) 
( ) 0)( 222 =+−−− lkkkll dR ξφα ax , (7) 
0=⎟⎟⎠
⎞
⎜⎜⎝
⎛
− ii
kN
C ξα , (8) 
0=⎟⎟⎠
⎞
⎜⎜⎝
⎛
− ll
kN
C ξα . (9) 
 
For some ),0( ki NC∈α and ),0( kl NC∈α , we have 
0== li ξξ  (using Eqs. (8) and (9)) and moreover the second 
factor in Eqs. (6) and (7) has to vanish. Hence, we can 
subsequently determine the spherical radius kR  and the 
margin factor kd  by setting 
 
22 )( kik aR −= xφ  i∀  such that 
k
i N
C
<< α0 , 
222 )( kklk Rad −−= xφ l∀  such that 
k
l N
C
<< α0 . 
3.3. The Decision Rule 
Given a collection of training example 
{ }),(),...,,( 11 NN yy xx  and },...,1{ Kyi ∈ , K class-specific 
hypersphere kS  can be constructed by solving the QP 2 for 
each class. For classifying a new test example x, we can use 
the following decision rule: 
class of ),(maxarg
1 kKk
Ssim xx
…=
≡ , (10) 
where ),( kSsim x  is the similarity function which 
determines the membership degree of the test example x 
belonging to the hypersphere kS . We say x  is in the class 
that has the largest value of the similarity function.  
In our previous work [18], we proposed the following 
fuzzy membership function to determine the membership 
degree of a new test example x belonging to each 
hypersphere in the feature space. 
( )⎪⎪
⎪⎪
⎩
⎪⎪
⎪⎪
⎨
⎧
⎟⎟⎠
⎞
⎜⎜⎝
⎛
−−+
×
≤−+
⎟⎟
⎟⎟
⎟⎟
⎠
⎞
⎜⎜
⎜⎜
⎜⎜
⎝
⎛
⎟⎟⎠
⎞
⎜⎜⎝
⎛ −
+
⎟⎟⎠
⎞
⎜⎜⎝
⎛ −
−
×
=
otherwise   , 
)(1
15.0
)( if  ,5.0
)(
1
)(
1
5.0
),(
2
1
kk
kk
k
k
k
k
k
R
R
R
R
Ssim
ax
ax
ax
ax
x
φλ
φφλ
φ
   
where 1λ  and 2λ  are user predefined parameters; and the 
parameters 1λ  and 2λ  satisfy 
        
)1(
1
1
2 λλ += kR
.  
Some important properties of the proposed similarity 
function are summarized as follows. The greater the distance 
between the new test point and the spherical center, the lower 
the degree of membership of the point belonging to that class 
is. Moreover, the membership value also takes into account 
the radius of the corresponding hypersphere in the feature 
space. The overall membership functions become S-type and 
upper-concave type when 1λ  < 0 and 1λ  > 0, respectively. 
It can be seen that the membership value remains higher than 
0.5 if the new test example is located inside the hypersphere 
in the feature space and smaller than 0.5 otherwise. In the 
following experiments, we set 1λ =0 and 
kR
1
2 =λ . 
4. Results 
4.1. MSM-SVM prediction accuracy 
Table 2 showed the result of the 5-fold cross-validation 
tests for the MSM-SVM classifiers using two different types 
of composition, amino acid composition (AAC) and 
traditional di-peptide composition (DPC) respectively. The 
AAC-based MSM-SVM has been able to achieve overall 
accuracy of 80.54% (cost function with 9=C , RBF kernel 
1479
Proceedings of the Ninth International Conference on Machine Learning and Cybernetics, Qingdao, 11-14 July 2010
  
[14] V. Vapnik, Statistical Learning Theory, Wiley, New 
York, 1998. 
[15] D. Tax and R. Duin, “Support Vector Data 
Description,” Machine Learning, vol. 54, pp. 45-66, 
2004. 
[16] J. Wang, P. Neskovic, and L. N. Cooper, “Pattern 
Classification via Single Spheres,” Lecture Notes in 
Artificial Intelligence, vol. 3735, pp. 241-252, 2005. 
[17] P.-Y. Hao, J.-H. Chiang and Y.-H. Lin, “A New 
Maximal-Margin Spherical-Structured Multi-class 
Support Vector Machine,” Applied Intelligence, vol. 30, 
no. 2, pp. 98-111, April 2009. 
[18] J.-H. Chiang and P.-Y. Hao, “A New Kernel-Based 
Fuzzy Clustering Approach: Support Vector Clustering 
with Cell Growing,” IEEE Trans. On Fuzzy Systems, 
vol. 11, pp. 518-527, 2003. 
[19] A. Garg, et al., "Support vector machine-based method 
for subcellular localization of human proteins using 
amino acid compositions, their order, and similarity 
search," J Biol Chem, vol. 280, pp. 14427-32, Apr 15 
2005. 
 
1481
Proceedings of the Ninth International Conference on Machine Learning and Cybernetics, Qingdao, 11-14 July 2010
了解蛋白質的亞細胞定位訊息，可以為推斷蛋白質的生物功能提供必要的幫助，因此
，發展電腦計算方法從蛋白質的一級序列出發預測亞細胞定位變成日益重要的問題。
在本計畫中，我們開發了一套蛋白質亞細胞定位預測系統，自動由大量的蛋白質中預
測其亞細胞定位。球狀支持向量機的優點在於球心與球半徑，可以表示出該類別的平
均值與變異量的資訊，而這些資訊對於樣本分布不均勻(Imbalanced)的情形特別重要
，在蛋白質亞細胞位址定位預測問題中，各類別的分佈資訊極度不平均，所以使用球
狀支持向量機能比傳統支持向量機獲得更佳的預測正確率，在實驗當中，不論使用何
種蛋白質序列的編碼方式，球狀SVM的預測正確率都比傳統SVM來的優異。
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
