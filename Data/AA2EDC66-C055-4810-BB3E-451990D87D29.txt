中文摘要 
本計畫旨在建立人與機器人於日常生活中協力合作之機制。計畫成果著眼於兩大課
題：（一）藉著建立人與機器人間方便使用之介面，可使人機互動更自然順暢；（二）以對
環境的更高階感知來增進機器人輔助人類的能力。此類研究之發展能夠協助機器人在日常
生活中發揮其最大效用；本計畫的主要具體成果包括：（一）以數位攝影機辨識使用者手部
不同手勢以執行其相對應命令及（二）以雷射測距儀偵測、追蹤與辨識市區中各式移動物
體，並進一步辨識出行車違規狀況，以幫助使用者更佳地掌握交通狀況。使用者之手勢辨
識系統利用 Scale Invariant Feature Transform (SIFT)特徵處理取得相機影像中之特徵，並
結合 Adaboost 學習演算法辨識出不同之手勢。在交通狀況的感測部份，我們提出以 Variable 
Structure Multiple Model Estimation Framework 為基礎，結合各式動態運動模型、靜止模型、
與場景互動模型(scene interaction model)、和鄰近物體互動模型(neighboring object interaction 
model)，來完成擁擠市區環境中各式移動物體之偵測與追蹤，並辨識出違規事件。 
 
關鍵詞：人機互動、手勢辨識、場景互動模型、鄰近物體互動模型 
 
 
 
 
英文摘要 
In this project, a mechanism for human robot coordination in our daily life is established 
with a smart human robot communication interface and a higher level scene understanding 
capability. Regarding the smart human robot communication interface, a camera-based hand 
posture recognition using SIFT with Adaboost is proposed to provide natural and smooth 
interaction between the robot and the users. With respect to high level scene understanding, a 
ladar-based interacting object tracking system using the variable structure multiple model 
estimation framework with the moving models, the stationary process model, the scene 
interaction model and the neighboring object interaction model is established to solve the 
challenging tracking problem in crowded urban areas. In addition, unusual activity recognition is 
accomplished straightforwardly.   
 
Keyword: human-robot interaction, hand posture recognition, scene interaction model, 
neighboring object interaction model 
 
 
 
 
當應用在物體辨識時，常由於對應的特徵區塊太少而有困難。此時引入Adaboost[2]分類演
算法，Adaboost演算法結合數個分類能力較弱的分類方法，藉由在訓練過程中，調整不同
的弱分類法所佔的比重，統整成一能力較強的分類方法。 
在手勢的辨識上，我們結合了SIFT和Adaboost的方法，整套系統的流程圖描述在圖2
中。首先藉由Adaboost訓練強分類法，藉由一系列經過標記的手勢或非手勢影像訓練，可
得到一具有較強能力的分類法。而當機器人透過相機擷取影像時，此影像會被找出SIFT特
徵，並和先前Adaboost訓練出的分類法比較，首先透過比較多個手勢共同的特徵，此時會
過濾掉不符合的影像，再藉由不同手勢所具有的不同特徵，來分辨出使用者的不同手勢。 
 
圖 2 手勢辨識流程圖 
 
3.2 人機互動-環境感知 
除溝通介面的建立之外，對於日常生活中，機器人如何輔助人類也是一個重要的方
向。本計畫亦藉由裝載於機器人正前方之雷射測距儀，利用機器人對於十字路口的行車
狀況及違規狀況加以分別，以幫助使用者掌握更佳的交通狀況。 
移動物體追蹤(Moving object tracking)一直是機器人領域一個重要的課題，透過移動
物體追蹤可讓機器人除了了解周圍的靜態環境外，對於會移動的物體，如人、車等，能
有更進一步的感知能力，也由於對於人或是人所駕駛的車輛具有追蹤能力，使得機器人
與人能有更好的互動。在本團隊所提出的同時定位、建地圖及移動物體追蹤演算法
(Simultaneous localization, mapping and moving object tracking)中[3,4]，假設了靜態環境
和移動物體間，以及移動物體彼此之間是互相獨立的，然而此一假設並不符合一般日常
生活中常見的狀況。在交通的狀況裡，車輛必須遵行一定的移動方向，此即靜態環境和
移動物體的關係；而在車輛行進時，後方的車輛的速度會受到前方車輛的速度限制，此
即為移動物體間的關係。根據此想法，進一步提出場景互動模型(scene interaction model)
和鄰近物體互動模型(neighboring object interaction model)，分別處理以上兩種狀況[5]。 
圖 3 中，分別顯示了車道與十字路口的格網地圖(grid map)中的一格，除了一般格
網地圖所具有的佔有機率在 z 軸方向表示外，在 x-y平面上表示了八個可能移動方向，
並在該方向上以長度代表速度。圖 4 中則顯示了機器人建出的格網地圖與實際照片比
較，在不同交通號誌的狀況下，路口有不同的車輛通行狀況，藉由解讀格網地圖可使機
器人了解現在的十字路口交通狀況。場景互動模型即利用此格網地圖來做移動物體追
  
圖 5 手勢偵測結果：正確(上方三張圖)，錯誤(下方三張圖) 
 
在圖 6 中，展示了對於不同視角所拍攝到了同一個手勢的偵測結果，我們的所提出
的方法可以成功的在 40 度的旋轉角度內仍能成功偵測出結果。 
 
圖 6 不同視角下的手勢偵測結果：(a)(f)無法偵測(b)(c)(d)(e)成功偵測 
 
讀者可在 [6]獲得更多演算法細節，並可在下列網址下載系統展示影片
http://www.csie.ntu.edu.tw/~bobwang/videos/HandPostureRecognition4HRI.wmv 
 
 
4.2 人機互動-環境感知 
在圖 7 中，方框代表現在時刻的物體位置，方框後面跟著的線條代表過去一定時間
內的軌跡。此結果利用上述所提到的，場景互動模型與鄰近物體互動模型的追蹤，因為
在複雜的路口，常會發生物體互相遮蔽的現象，而用傳統的追蹤方法常會誤判物體因遮
蔽而消失又出現，本實驗改善了此情形。 
5. 計畫成果自評 
透過本計畫之執行，我們成功地建立了能促進機器人在日常生活中之應用的人機互動
介面，包括手勢辨識系統及環境感知系統。 
我們能運用手勢辨識系統準確地分辨出影像中數種不同的使用者手勢，並能處理背景
中之雜訊問題，使得使用者能夠藉由變化手部動作與機器人進行溝通、下達命令並取得所
需之服務，藉此創造出方便使用且自然的人機介面以幫助機器人在日常生活中被接受及妥
善利用。 
除現階段採用之 SIFT 特徵辨識外，運用其它種特徵表示將替此辨識系統提供更高的準
確定，加強其可靠性。又為整合手勢辨識系統至機器人平台，完整建構出此一人機互動模
式，我們將發展此手勢辨識之即時操作系統。此外，更多不同類別的手勢也將被加入系統
的辨識中，以提供使用者更多不同的互動選項。 
而藉由場景互動模型及鄰近物體互動模型的引入，我們改善了傳統的動態物體追蹤方
法易造成誤判的弱點，成功地在複雜的路口感測交通情況並發現交通違規等異常狀況，有
效地輔助人類掌握交通狀況。 
未來我們將考慮更多不同的動態物體間互動關係以處理更複雜之環境感測問題；另外
藉由在不同環境下的資料蒐集及分析，我們將能得到其它種類之場景互動模型，使得此環
境感測系統能應用在如居家、辦公室等各式不同的環境下。 
 
6. 參考文獻 
[1] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,” International Journal of Computer 
Vision, vol. 60, no. 2, pp. 91–110, 2004. 
[2] P. Viola and M. J. Jones, “Robust real-time face detection,” International Journal of Computer Vision, vol. 57(2), 
pp. 137–154, 2004 
[3] C.-C. Wang, C. Thorpe, and S. Thrun, “Online simultaneous localization and mapping with detection and 
tracking of moving objects: Theory and results from a ground vehicle in crowded urban areas,” in Proceedings of 
the IEEE International Conference on Robotics and Automation (ICRA), Taipei, Taiwan, September 2003. 
[4] C.-C. Wang, “Simultaneous localization, mapping and moving object tracking,” Ph.D. dissertation, Robotics 
Institute, Carnegie Mellon University, Pittsburgh, PA, April 2004. 
[5] Chieh-Chih Wang, Tzu-Chien Lo and Shao-Wen Yang, Interacting Object Tracking in Crowded Urban Areas, 
Proceedings of IEEE International Conference on Robotics and Automation (ICRA'07), 10-14 April 2007, Roma, 
Italy. 
[6] Chieh-Chih Wang and Ko-Chih Wang, Hand Posture Recognition Using Adaboost with SIFT for Human Robot 
Interaction, Proceedings of the 13th International Conference on Advanced Robotics (ICAR), 21-24 August 2007, 
Jeju, Korea.  
 
