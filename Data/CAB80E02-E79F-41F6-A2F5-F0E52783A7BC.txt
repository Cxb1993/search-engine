proteins. If it is possible to know that if there are 
disulfide bonds in proteins and the exact positions 
of bonds, the correct rates of protein structure 
prediction would be higher. 
 
Studies related to disulfide bonds can be divided 
into two categories, state classification (to check 
that if there are disulfide bonds among the protein) 
and connectivity prediction (which is more difficult 
than the other). In this project we use a multi-phase 
approach to classify states first and then predict 
connectivities with prior results. It is known that 
the previous work in state classification are correct 
in 94.3% cases, so we try to improve the connectivity 
prediction in this project. 
 
In this project, we improve the connectivity 
prediction with SVM methods by two ways. First, 
proteins with different numbers of disulfide bonds 
should use different feature sets when applying SVM 
methods. We roughly divide our test set 
into two parts, the part with odd numbers of 
disulfide bonds and the other part with even numbers 
of disulfide bonds, then we have better prediction 
result by applying different feature sets to these 
two parts. Second, we use behavior knowledge space to 
fuse multiple classifiers to get or adjust our 
answers. These two ways make our prediction 
accurencies up to 70.8% and 71.5%. One of our results 
has been published in the conference and the other is 
waiting for submitting. 
 
英文關鍵詞： protein structure, disulfide bond, prediction, 
support vector machine, behavior knowledge space 
 
                                                             共 10 頁  第 2 頁 
而在2005前，沒有任何一個研究的準確率可以超過50%。2005年，[Tsai05]建立了以SVM為主的預測
系統，且準確率達63%。[Chen06a]則使用了2-level的SVM使得準確率達70%，這裡提到的準確率是
指預測有兩個鍵結的蛋白質部分。[Chun09]的方法是以前一階段的鍵結狀態機率為基礎進行
down-sampling，以減少正負樣本的比例差而增加準確率。[Jian07]提出了以multiple sequence 
feature vector 以及二級結構的方法，[Linz09]中則提出了以feature selection增進預測的準確
率。 
本研究主要使用SVM方法、並對features的選擇及使用多加著墨，主要兩個方法的的研究成果、
其一已於研討會發表，其二已完成碩士論文、正待修改投稿中。茲簡要說明如下： 
 
1. 以蛋白質雙硫鍵鍵結數目進行雙模預測：鍵結數目不同應使用不同的特徵集合(Feature 
Set)，在研究中、我們將蛋白質雙硫鍵鍵結數目粗分為偶數鍵結及奇數鍵結，利用兩組不
同的特徵集合做為SVM的參數、並得到較佳的預測，此方式將預測準確率提至70.8%。 
2. 利用行為認知空間法結合各分類器預測：我們利用行為認知空間法(Behavior Knowledge 
Space)結合各式已知分類器的結論、進而從中調整或求得較為正確之解答，此方式將預測
準確率提至71.5%。 
二、 研究成果： 
2.1 蛋白質雙硫鍵雙模預測 
 
初期、在利用 SVM 來進行蛋白質雙硫鍵預測時，由於多半計算機學家不明其機制、故多使用
猜測而得之各項特徵、用以做為 SVM 之參數、以區隔不同的類群。後繼者的研究、主要為加入新
推測的特徵、故 SVM 用以區隔類群的特徵集合便愈加愈多，然預測準確率卻不見得有顯著提昇。
在 2009 年[Linz09]提出了 feature selection 的概念，主張部份 feature 不僅無益於提昇預測準確率、
甚而可能干擾預測，而過多的 feature 亦減低預測之速度，故應過濾所選之 feature、以得最佳之預
測結果。 
在研究進行的過程、我們發現使用相同的 SVM 特徵集合，在蛋白質雙硫鍵鍵結數目不同的時
候、其預測結果表現有所差異。故而、除了嚴格揀選特徵集合外、應依該蛋白質鍵結數目不同、而
另外選用不同之特徵集合。在第一個方法中、我們僅僅將蛋白質大致分為兩類、一者為鍵結數目為
偶數者、另一者為鍵結數目為奇數者。表 1列出本方法中、所使用的特徵集合，可以發現偶數鍵結
所使用的特徵集合包含於奇數鍵結的特徵集合。此種現象可能是、相對於奇數鍵結而言、偶數鍵結
較為穩定故而影響因素較少、可使用較少之特徵進行預測。 
 
表 1：雙模預測方法，奇數鍵結、偶數鍵結使用之特徵集合 
FEATURE SIZE Model for Odd Model for Even 
Distance of cysteine 1 Y Y 
Cysteine order 2 Y N 
Protein weight 1 Y N 
Protein length 1 Y N 
Amino acid composition 20 Y N 
PSSM around cysteine (2k+1)x20x2 Y Y 
Secondary structure around cysteine (2k+1)x3x2 Y N 
                                                             共 10 頁  第 4 頁 
 
圖一為本研究提出方法之流程圖，表 2 為我們的方法準確度與[Chun09]的比較，可以看出在
各鍵結數目的預測表現上、本研究方法幾乎均優於前人之研究結果。 
 
表 2：雙模式蛋白質雙硫鍵預測方法準確率與[Chun09]比較 
方法 
B=2 B=3 B=4 B=5 B=2..5 
Qp Qc Qp Qc Qp Qc Qp Qc Qp Qc 
Down Sampling 80.8% 80.8% 57.5% 66.2% 54.5% 70.7% 42.2% 61.8% 63.5% 70.1% 
雙模式預測 84.0% 84.0% 60.2% 69.2% 55.6% 66.9% 44.4% 62.2% 65.9% 70.8% 
 
關於此段之研究已告一段落、並投稿研討會，其論文將隨附於本報告後方。 
 
2.2 行為認知空間法結合 SVM預測 
由前方法我們了解到鍵結數目不同、可能影響我們對於特徵集合的選取，之前單純由鍵結數目
的奇偶來判斷選用的特徵集合模組也許過於粗糙。可能的話、應為各種鍵結數目的蛋白質均建立一
種特徵集合、然此舉是否會過於細節而喪失預測之意義、亦應加入考量。故、為此我們加入了 BKS 
(Behavior Knowledge Space) [Raud03]，用以提高預測之準確率。 
 
BKS基本上是一種將多個分類的結果融合的方法，類似查表法、根據既知的各分類器提供的機
率及投票的組合、綜合判斷目標物應歸屬何種類群。舉例來說、假設我們有 m 個分類器、會將目標
分為 n 種類群，那麼在 BKS 的表中、我們就有 nm個記錄，即是 m 個分類器會產生的所有組合。對
一個需要被分類的目標來說，BKS會在表中尋找分類器的分類結果、並自其中選出最具代表性的結
果。部份表中沒有資料的決定方式則有可能隨機任選一個結果或採取多數決方式來選出最終結果。 
 
表 3我們製作了一個範例，例如我們有兩個分類器、分別將資料分為三個類群。那麼、假設分
類器計算後得到的結論分別是「第一群」及「第三群」、那麼表中顯示的資料是其三群的分類機率
依序是 2%、7%、1%，第二群為最高、因此雖然分類器並無將此結果歸在第二群，我們依 BKS 的選
取原則會得到歸類第二群的結論。另一個例子、若分類器得出結論依序為「第三群」、「第二群」，
那麼我們查表得知歸類在第三群機率為 5%、最高，故得出結論為第三群。 
 
表 3：BKS製表範例(2分類器、分為三個類群) 
預測結果 真正結果(機率、%) 
分類器一 分類器二 第一群 第二群 第三群 
第一群 第一群 23 8 2 
第一群 第二群 5 0 4 
第一群 第三群 2 7 1 
┇ ┇ ┇ ┇ ┇ 
第三群 第二群 1 1 5 
第三群 第三群 1 3 12 
                                                             共 10 頁  第 6 頁 
 
 
 
圖 二：行為認知空間法結合 SVM及 CSP預測流程圖 
                                                             共 10 頁  第 8 頁 
[03.] [Mirn96] L. A. Mirny and E. I. Shakhnovich, “How to derive a protein folding potential? a new 
approach to an old problem,” Journal of Molecular Biology, Vol. 264, No. 5, pp. 1164–1179, 1996. 
[04.] [Rubi08] R. Rubinstein and A. Fiser, “Predicting disulfide bond connectivity in proteins by 
correlated mutations analysis,” Bioinformatics, Vol. 24, No. 4, pp. 498–504, 2008. 
[05.] [Bald05] P. Baldi, J. Cheng, and A. Vullo, “Large-scale prediction of disulphide bond connectivity,” 
Advances in Neural Information Processing Systems 17, Cambridge, MA, pp. 97–104, MIT Press, 
2005. 
[06.] [Chen06] J. Cheng, H. Saigo, and P. Baldi, “Large-scale prediction of disulphide bridges using 
kernel methods, two-dimensional recursive neural networks, and weighted graph matching,” 
PROTEINS: Structure, Function, and Genetics, Vol. 62, pp. 617–629, 2006. 
[07.] [Fari99] P. Fariselli, P. Riccobelli, and R. Casadio, “Role of evolutionary information in predicting 
the disulfide-bonding state of cysteine in proteins,” PROTEINS: Structure, Function, and Genetics, 
Vol. 36, pp. 340–346, 1999. 
[08.] [Ferr05] F. Ferre and P. Clote, “Disulfide connectivity prediction using secondary structure 
information and diresidue frequencies,” Bioinformatics, Vol. 21, No. 10, pp. 2336–2346, 2005. 
[09.] [Mart02] P. L. Martelli, P. Fariselli, L. Malaguti, and R. Casadio, “Prediction of the 
disulfide-bonding state of cysteines in proteins at 88% accuracy,” Protein Science, Vol. 11, pp. 
2735–2739, 2002. 
[10.] [Vull04] A. Vullo and P. Frasconi, “Disulfide connectivity prediction using recursive neural 
networks and evolutionary information,” Bioinformatics, Vol. 20, No. 5, pp. 653–659, 2004. 
[11.] [Chen05] Y.-C. Chen and J.-K. Hwang, “Prediction of disulfide connectivity from protein 
sequences,” PROTEINS: Structure, Function, and Genetics, Vol. 61, pp. 507–512, 2005. 
[12.] [Chen04] Y.-C. Chen, Y.-S. Lin, C.-J. Lin, and J.-K. Hwang, “Prediction of the bonding states of 
cysteines using the support vector machines based on multiple feature vectors and cysteine state 
sequences,” PROTEINS: Structure, Function, and Genetics, Vol. 55, pp. 1036–1042, 2004. 
[13.] [Fras02] P. Frasconi, A. Passerini, and A. Vullo, “A two-stage svm architecture for predicting the 
disulfide bonding state of cysteines,” Neural Networks for Signal Processing, 2002. Proceedings of 
the 2002 12th IEEE Workshop on, pp. 25–34, 2002. 
[14.] [Shil05] J. R. G. L., A. P. Shilton, M. M. Parker, and M. Palaniswami, “Prediction of cystine 
connectivity using svm,” Bioinformation, Vol. 1, No. 2, pp. 69–74, 2005. 
[15.] [Liu07] H.-L. Liu and S.-C. Chen, “Prediction of disulfide connectivity in proteins with support 
vector machine,” Journal of the Chinese Institute of Chemical Engineers, Vol. 38, No. 1, pp. 63–70, 
                                                             共 10 頁  第 10 頁 
[28.] [Jian07] Jiangning Song, Zheng Yuan, Hao Tan, Thomas Huber and Kevin Burrage,” Predicting 
disulfide connectivity from protein sequence using multiple sequence feature vectors and secondary 
structure,” Bioinformatics, Vol. 23 no. 23, pages 3147–3154, 2007 
[29.] [Linz09] Lin Zhu, Jie Yang, Jiang-Ning Song, Kuo-Chen Chou, Hong-Bin Shen, “Improving the 
Accuracy of Predicting Disulfide Connectivity by Feature Selection,” Journal of Computational 
Chemistry, Vol. 31, No. 7, 2009 
[30.] [Cort95] C. Cortes and V. Vapnik, "Support-Vector Networks," Machine Learning, Vol. 20(3), 
pp.273-297, 1995. 
[31.] [Theo09] S. Theodoridis and K. Koutroumbas, "Pattern Recognition 4th Edition,” Academic Press, 
2009. 
[32.] [Alts97]S. F. Altschul, T. L. Madden, A. A. Schaffer, J. Zhang, Z. Zhang, W. Miller, and D. J. 
Lipman, “Gapped BLAST and PSI-BLAST: a new generation of protein database search programs,” 
Nucleic Acids Research, Vol. 25, No. 17, pp. 3389–3402, 1997. 
[33.] [Linc10] C.-Y Lin, C.-B. Yang, and C.-Y. Hor, “Disulfide Bonding State Prediction with SVM 
Based on Protein Types” 
[34.] [Raud03] S. Raudys and F. Roli, “The behavior knowledge space fusion method: Analysis of 
generalization error and strategies for performance improvement,” In Proc. Int. Workshop on 
Multiple Classifier Systems (LNCS 2709, pp. 55–64, Springer, 2003. 
 
五、 附錄 
國內研討會發表論文 
 
alignment method in order to get the score of each kind of
amino acid in each position within a protein. The sequence
alignment manages to determine the similarities between the
target sequence and the sequences in the queried database.
Consequently, it can find the homogeneous sequences with
highest similarity, and summarize the information from ho-
mogeneity among the queried sequences.
To get PSSM, there are two widely-used tools: BLAST (ba-
sic local alignment search tool)[1] and PSI-BLAST (position-
specific iterated-BALST)[2]. BLAST is an algorithm for query-
ing a protein or DNA sequence. With the database specified,
it can find the sequences similar to the target sequence by
the similarity score function. PSI-BLAST uses the result of
BLAST as input to run BLAST iteratively, so it is more
sensitive than BLAST to locate distantly homogeneous family.
The PSSM is calculated by PSI-BLAST. The score of each
entry in it represents the occurrence frequency of one residue.
If one score is higher than other scores in a row, it means
that it is more common for this residue in this position in the
homogeneous homology family.
B. Secondary Structure
In biochemistry and structural biology, the secondary struc-
ture is defined as the interaction of hydrogen bonds between
residues. Various hydrogen bonding states exhibit distinct
structural attributes, and these bonding states may further in-
fluence the three-dimensional structure of a protein. Therefore,
it is usually believed that the secondary structure provides
important information for protein structure prediction.
The secondary structure, as defined in DSSP (Dictionary
of Protein Secondary Structure)[11], includes several types
of hydrogen bonding mode, such as α-helix, β-sheet, 310-
helix, pi-helix, etc. Among these types, α-helix and β-sheet are
commonly found, and the others are relatively rare in nature
proteins. In protein sequences, the secondary structures are
roughly categorized into three states: helix, sheet and coil, each
of which is found to have its dominant kinds of amino acids.
Even though, it is still believed to be intractable to predict
secondary structures by means of only the information of an
amino acid sequence.
Many algorithms have been proposed for solving the sec-
ondary structure prediction, such as neural networks, hidden
Markov models, and support vector machines. Among these
methods, one of the most accurate is PSIPRED[10], which is
a tool based on neural networks. It first invokes PSI-BLAST
to locate homogeneous proteins to obtain the evolutionary
information, such as replacements, insertions, and deletions of
an amino acid. After that, the result of PSI-BLAST is served
as input of the neural network to generate prediction results.
The reason of its high accuracy is that it uses PSI-BLAST
to get homology information. Even though, it still works well
without PSI-BLAST.
PSIPRED roughly classifies each residue into three types
of secondary structure (helix, strand and coil), and outputs
the results into three kinds of file (.horiz, .ss and .ss2). The
.horiz file stores the predicted secondary structure type and
the confidence value of the prediction of each amino acid
in a protein. Both .ss and .ss2 files contain probabilities for
the three kinds of secondary structure associated with each
residue. These files contain the predicted secondary structure
and probabilities, belonging to coil, helix and strand states, of
each residue. The total probability of each residue in .ss file
is equal to 1, while that in .ss2 file is not equal to 1. In this
paper, we adopt three probability information from the .ss2
file.
C. Support Vector Machine
Support Vector Machine (SVM), is a machine learning
method. It is widely used in classification and regression
problems. The basic idea is to map the input data into a
higher dimensional space, and create a hyperplane to divide
different groups of samples. Furthermore, after an SVM model
is trained, the class corresponding to a data element can be
predicted by this model.
Assume that xi is a d-dimensional vector (data), and yi is
the label of xi, where yi ∈ {1,−1}. The main purpose of SVM
is to find an optimal decision hyperplane ωTx+ b = 0, where
ωT,x ∈ Rd and b ∈ R. The hyperplane is desired to separate
as many data elements as possible, by maintaining a maximal
margin between the two groups of data. The answers, which
are parameters of the hyperplane, are obtained by solving the
following optimization problem:
{
minimize
1
2
wTw
subject to yi(wTxi + b) ≥ 1 , i = 1 ≤ i ≤ m.
(3)
In the ideal situation, the hyperplane can separate all data
elements perfectly. However, it rarely happens in real worlds.
Consider the case that all kinds of data are mixed and scattered
within a 2-D space, and they could not be separated exactly
by any linear function. It turns out that we may separate them
by curves, not straight lines. To overcome the difficulty, SVM
provides nonlinear kernel functions to map the data into a
higher dimensional space. Then the problem can be solved
with an ordinal linearly separable scheme. In either linear or
nonlinear cases, SVM performs very well in most classification
problems.
In this paper, LIBSVM[3] is employed to perform the
SVC (Support Vector Classification) task. In addition, it also
provides SVR (Support Vector Regression) for regression.
The SVC function can classify a set of data elements with
their probabilities, while the SVR can generate a regression
value for an input data element. These values are helpful
to determine which class is more likely for each data ele-
ment. Furthermore, it provides several kernel functions for
various problems. Because of its outstanding performance and
extensive functionalities, LIBSVM is widely adopted in the
researches on data analysis.
III. ALGORITHMS FOR DISULFIDE BOND PREDICTION
In this section, we present our algorithm for solving the
connectivity prediction problem. We explain how to develop
2
TABLE I
THE FEATURE SETS USED IN OUR APPROACH.
Feature size Model for odd Model for even
Distance of cysteine 1 Y Y
Cysteine order 2 Y N
Protein weight 1 Y N
Protein length 1 Y N
Amino acid composition 20 Y N
PSSM around cystein (2k + 1)× 20× 2 Y Y
Secondary structure around cystein (2k + 1)× 3× 2 Y N
Input 
sequence
Check odd or 
even
Encode the feature 
set for odd model 
Encode the feature 
set for even model 
Predict with odd-
SVM model
Predict with even-
SVM model
Probabilities 
of pairs
Perform weighted 
graph matching
Predicted 
connectivity 
pattern
odd even
Fig. 1. The flowchart of our work for connectivity prediction.
IV. CONNECTIVITY PREDICTION EXPERIMENTS
In this chapter, we introduce the dataset used in our experi-
ments and then explain how we compute the accuracy. Finally,
we carry out the performance comparison of our work with
others.
A. Dataset
For comparing with other works in connectivity prediction,
we adopt the same dataset that was used by the previous works.
The dataset is given as follows:
SP39: The dataset was derived from SWISS-PROT release
No. 39. This dataset consists of 446 proteins and
the number of disulfide bonds in each protein ranges
from 2 to 5. To compare with Chung’s work [6],
we apply the same way to divide the dataset into
4 subsets for 4-fold cross-validation, in which the
sequence identity between any two subsets is less
than 30%. Moreover, to verify the correctness and
remedy randomness, we also divide this dataset into
4 folds randomly and perform experiments. Totally,
10 different 4-fold cross validations are carried out.
B. Performance Evaluation
We use k-fold cross-validation to evaluate our result. The
dataset D is split into k disjoint subsets D1, D2, . . . , Dk. We
take Di, 1 ≤ i ≤ k, for testing and the other k-1 subsets for
training. This process is repeated k times until all subsets are
tested.
We group the predicted results into 4 categories: TP (true
positive), TN (true negative), FP (false positive), and FN
(false negative). In pair-wise prediction, we adopt Qc as the
performance measure calculated as follows:
Qc =
Pc
Nc
=
TP
TP + FN
, (5)
where Pc denotes the number of bonded pairs that are correctly
predicted and Nc denotes the total bonded pairs in the dataset.
For pattern prediction, we use Qp to evaluate the perfor-
mance. The formula is given as follows:
Qp =
Pp
Np
, (6)
where Pp denotes the number of correctly predicted proteins
and Np denotes the total number of proteins in the dataset.
Finally, we perform k-fold cross-validation and calculate the
average accuracy of all folds, which is presented as follows:
R =
1
k
k∑
i=1
Ri. (7)
where Ri could be Qp or Qc.
C. Experiments of Connectivity Prediction
According to the literature, the down-sampling technique
is usually used to balance the negative samples and positive
samples in the training set so that it prevents the classification
4
TABLE II
PREDICTION ACCURACIES WITH THE DOWN-SAMPLING SCHEME.
Feature set B = 2 B = 3 B = 4 B = 5 B = 2 · · · 5
Qp Qc Qp Qc Qp Qc Qp Qc Qp Qc
D + P a 80.8 80.8 57.5 66.2 54.5 70.7 42.2 61.8 63.5 70.1
D + P + L+W + O + A+ S 76.9 76.9 54.1 63.5 57.6 67.7 37.8 57.8 61.2 66.8
150 features inZhu′swork 77.6 77.6 50.7 60.7 38.4 53.8 33.3 51.1 55.6 61.0
a From Chung et al. [6].
TABLE III
PREDICTION ACCURACIES CORRESPONDING TO RANDOMLY DIVIDED FOLDS WITH THE DOWN-SAMPLING SCHEME.
Feature set B = 2 B = 3 B = 4 B = 5 B = 2 · · · 5
Qp Qc Qp Qc Qp Qc Qp Qc Qp Qc
D + P a 87.7 87.7 68.8 75.3 79.9 85.5 50.9 63.6 76.0 79.2
D + P + L+W + O + A+ S 87.6 87.6 70.3 76.9 77.1 82.0 51.1 63.1 75.9 78.6
150 features inZhu′swork 84.4 84.4 67.1 74.2 73.9 80.3 46.0 59.7 72.5 75.9
TABLE IV
PREDICTION ACCURACIES OBTAINED BY THE TRAINING DATASET CONTAINING ALL OXIDIZED-OXIDIZED PAIRS.
Feature set B = 2 B = 3 B = 4 B = 5 B = 2 · · · 5
Qp Qc Qp Qc Qp Qc Qp Qc Qp Qc
D + P 84.0 84.0 53.4 63.7 55.6 66.9 46.7 60.4 63.9 68.7
D + P + L+W +O + A+ S 78.8 78.8 60.2 69.2 53.5 64.4 44.4 62.2 63.7 69.1
150 features inZhu′s selection 78.8 78.8 51.4 62.1 48.5 68.2 33.3 52.9 58.5 64.6
TABLE V
PREDICTION ACCURACIES CORRESPONDING TO RANDOMLY DIVIDED FOLDS OBTAINED BY THE TRAINING DATASET CONTAINING ALL
OXIDIZED-OXIDIZED PAIRS.
Feature set B = 2 B = 3 B = 4 B = 5 B = 2 · · · 5
Qp Qc Qp Qc Qp Qc Qp Qc Qp Qc
D + P 88.5 88.5 72.5 78.8 77.0 82.4 52.0 62.0 77.0 79.8
D + P + L+W +O + A+ S 88.9 88.9 70.5 77.5 77.6 82.9 52.2 66.1 76.7 79.8
150 features inZhu′s selection 86.5 86.5 71.8 76.8 79.2 83.6 49.1 61.4 76.2 78.4
might not guarantee to be fully correct. Consequently, the
provided information might be somewhat limited.
We finally propose a hybrid model in our work. Accord-
ing to different bonding configurations, we construct our
model with different feature sets and parameters for SVM.
We roughly divide the proteins into odd or even bonded.
For an odd-bonded protein, the predicted secondary structure
information is included to build the SVM model. But, the
information is not included for an even-bonded protein. By
this approach, we get better result in the SP39 dataset. In
this sense, sequences under various bonding configurations
may somewhat different in physical or chemical properties.
Consequently, proteins associated with different configurations
may have their individually preferential model for prediction.
REFERENCES
[1] S. F. Altschul, W. Gish, W. Miller, E. W. Myers, and D. J.
Lipmanl, “Basic local alignment search tool,” Journal of
Molecular Biology, Vol. 215, No. 3, pp. 403–410, 1990.
[2] S. F. Altschul, T. L. Madden, A. A. Schaffer, J. Zhang,
Z. Zhang, W. Miller, and D. J. Lipman, “Gapped blast
and psi-blast: a new generation of protein database search
programs,” Nucleic Acids Research, Vol. 25, No. 17,
pp. 3389–3402, 1997.
[3] C. C. Chang and C. J. Lin, LIBSVM: A library
for support vector machines. National Taiwan
University, No. 1, Roosevelt Rd. Sec. 4, Taipei,
Taiwan 106, ROC, 2001. Software available at
http://www.csie.ntu.edu.tw/∼cjlin/libsvm.
[4] Y.-C. Chen and J.-K. Hwang, “Prediction of disulfide
connectivity from protein sequences,” Vol. 61, pp. 507–
512, 2005.
[5] Y.-C. Chen, Y.-S. Lin, C.-J. Lin, and J.-K. Hwang,
“Prediction of the bonding states of cysteines using
the support vector machines based on multiple feature
vectors and cysteine state sequences,” Vol. 55, pp. 1036–
1042, 2004.
[6] W.-C. Chung, “A multi-phase approach for disulfide bond
prediction,” Master Thesis, Department of Computer Sci-
ence and Engineering, National Sun Yat-Sen University,
Kaohsiung, Taiwan, 2009.
[7] P. Fariselli and R. Casadio, “Prediction of disulfide
connectivity in proteins,” Vol. 17, No. 10, pp. 957–964,
2001.
[8] P. Frasconi, A. Passerini, and A. Vullo, “A two-stage svm
6
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/22
國科會補助計畫
計畫名稱: 使用SVM之雙硫鍵預測方法
計畫主持人: 曾國尊
計畫編號: 100-2221-E-242-003- 學門領域: 生物資訊
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
研討會論文發表詳細資料如下示： 
 
Chong-Jie Wang, Chang-Biau Yang, Chiou-Yi Hor and Kuo-Tsung 
Tseng, ＇＇＇＇＇＇＇＇Disulfide Bond Prediction with Hybrid 
Models,＇＇＇＇＇＇＇＇ Proc. of the 2012 International Conference on 
Computing and Security (ICCS＇12), Ulaanbaatar, Mongolia, July 8-11, 
2012. [NSYSU, NSC 100-2221-E-242-003]. 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
