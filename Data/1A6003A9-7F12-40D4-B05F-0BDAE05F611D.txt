dependency relationships among documents and program code are also stored. The benefit of this 
structure is that, when a document is reused, the documents and program code depending on the 
reused document can be reused accordingly. For example, when a requirement document is reused, 
the specification, design document, and program code depending on the requirements can be 
identified by tracing dependency relationships and then be reused. 
subSpec2
Spec
subDesign21
Design
subDesign2
subDesign22
subSpec1
Legend:
A B
“A” depends on “B”
The specification  “Spec” The design document “Design”
Figure 1. Product line structure
subDesign1
component
A
C
“A” is decomposed
into  “B” and “C”
B
 
.
.
.
.
.
.
.
.
.
.
.
.
Legend:
component
variation point (VP)
variant
A
B
C
VP “A”
provides
variants
“B” and “C”
Figure 2. Notation for a component
 
In our product line structure, every system is decomposed into subsystems. A subsystem can 
be further decomposed. Therefore, the structure of a product line looks like a tree. Figure 1 shows 
an example. In the figure, two simple product lines respectively named “Spec” and “Design” are 
shown. The decomposition and dependency relationships are also shown. Since variability may 
exist in a component, a component in a product line may contain VPs. For a VP, variants may be 
provided. Therefore, a component in the product line structure looks like that in Figure 2. According 
to VPs and variants, a tree in Figure 1 is not a system. Instead, a tree is a software product line. 
3. Classification 
We use features [8] to classify components in software product lines. Features are the abstraction of 
retrieved for users to justify whether the component can be reused. Our retrieval technique requires 
users to provide the threshold according to their experiences. A query is composed of features 
structured by relationships. Therefore, a query is a feature structure [3-4]. Relationships used in a 
feature structure are listed below: 
1. AND. Functions/data represented by the features within AND relationships are all necessary. 
2. OR. One or more functions/data represented by the features within an OR relationship are 
sufficient. 
3. NOT. The function/data represented by a feature with a NOT relationship must be excluded. 
4. ONE_ONLY. Only one function/data represented by the features within an ONE_ONLY 
relationship is necessary. This relationship is essential. For example, a person has only one 
birthplace. If birthplace is a feature in a system or subsystem, this relationship improves 
retrieval precision. 
The following syntax depicts a sample query for a car rental system. ONE_ONLY relationship is 
not shown in the query. Nevertheless, the relationship will be used in the example of section 4. 
Component features: “rent”, “return”, “car”, “add”, “discard”, “overdue notice”, “damage compensation”, 
“mortgage”, “member”, “non-member”; /* the feature “add” represents adding a new car and 
“discard” for discarding an old car. The feature “sell” is not included because a car in the 
car rental system is not for selling */ 
AND(“rent”, “return”, “car”, “add”, “discard”, “overdue notice”, “damage compensation”); 
OR(“mortgage”, “member”); /* a non-member need to mortgage something valuable to rent a car */ 
NOT(“sell”); /* a car in the car rental system is not for selling */ 
When retrieving a component, more than one query should be provided, in which one is for 
retrieving reusable components and the others for computing VP similarities within the components. 
We use the following equation to compute the similarity between a query and a component: 
Equation 1. Similarity = ComSim * ComWeight + VPSim * VPWeight, in which: (a) ComSim is the 
similarity between the component and the query for components, (b) ComWeight is the weight 
for ComSim, (c) VPSim is the similarity between VPs in the component and the queries for 
query. Then, if the number of features offered by the component appearing in the relationship 
is zero, the individual similarity of the relationship is zero. Otherwise, the individual similarity 
of the relationship is defined as “1/(number of features offered by the component that appear 
in the relationship)”. Since the ONE_ONLY relationship needs one and only one feature, more 
matched features decrease the similarity of the relationship. 
Section 3 states that VPs are not associated with features. To compute the similarity of a VP, the 
similarities of the VP’s variants are used. The similarity of a VP is the largest similarity value 
obtained from its variants. The computation of the similarity between a query and a VP’s variant is 
similar to that for computing the similarity between a query and a component described above. As 
to the similarity of VPs in a component (i.e., VPSim in Equation 1), it is computed using the 
equation below: 
Equation 8. VPSim = ∑
=
n
i
i nVPSim
1
/ , in which n is the number of VPs in the component and 
iVPSim  is the individual similarity of the i’th VP. 
Below we use a simple example to explain our retrieval technique. Suppose books in a book rental 
system can be rented, returned, reserved, or bought. In addition, new books can be added and 
obsolete books should be discarded. Moreover, members can obtain a special discount when renting 
books. Then, the book rental system can be classified using the following features: “rent”, “return”, 
“reserve”, “book”, “add”, “discard”, “overdue notice”, “damage compensation”, “member”, 
“non-member”, and “sell”. Moreover, suppose the system has two VPs in which one is for “overdue 
notice” and the other for “damage compensation”. We let the variants and the classification of the 
variants for the VP “overdue notice” be the same as those described in section 3. In addition, we let 
the variants of the VP “damage compensation” be “suspend renting right” and “fine”. The first 
variant is classified using the features “suspend”, “renting right”, and the second is classified using 
the features “fine” and “money”. 
Suppose a query that is the same as the sample query near the beginning of this section is used 
to retrieval a reusable component. To improve readability, we reproduce the query as follows: 
Query 1: Component features: “rent”, “return”, “car”, “add”, “discard”, “overdue notice”, “damage 
“overdue notice”. Moreover, the similarity should be 
3
2 , which is the largest among the four 
similarities mentioned above. When Query 3 is compared with the features in the VP “overdue 
notice”, two similarities 0 and 0 are obtained from the two variants, respectively. When Query 3 is 
compared with the features in the VP “damage compensation”, two similarities 0 and 
3
2  are 
obtained from the two variants, respectively. In this case, Query 3 should be considered matched 
with the VP “damage compensation”. Moreover, the similarity should be 
3
2 . According to Equation 
8 and the VP similarities obtained, the value VPSim in Equation 8 is 
3
2 . According to the ComSim 
and VPSim we computed, the similarity between the book rental system and the three queries is 
“
91
16 *ComWeight + 
3
2 *VPWeight”. 
Result of the Experiment
0
0.2
0.4
0.6
0.8
1
0.1 0.2 0.3 0.4 0.5 0.6
Threshold
Re
ca
ll/
Pr
ec
isi
on
Recall for ComWeight = 0.6 and
VPWeight = 0.4
Recall for ComWeight = 0.5 and
VPWeight = 0.5
Recall for ComWeight = 0.4 and
VPWeight = 0.6
Precision for ComWeight = 0.6 and
VPWeight = 0.4
Precision for ComWeight = 0.5 and
VPWeight = 0.5
Precision for ComWeight = 0.4 and
VPWeight = 0.6
 
Figure 3. Result of the Experiment 
5. Evaluation 
We set up experiments to evaluate the retrieval technique. We placed fifty systems in the product 
line repository. The systems were decomposed into 343 components. There are 391 VPs in the 
components and 805 variants for the VPs. During the experiments, we recorded the precisions and 
recalls of the retrieval using the values of ComWeight from 0.4 up to 0.6 (the value of VPWeight 
were 0.6 down to 0.4 because the summation of the two weights are one). We used the threshold 
values from 0.1 up to 0.6. Figure 3 shows the result of the experiment. As shown in the figure, the 
recall decreases as the threshold increases and the precision increases as the threshold increases. 
Using this figure, we can decide the threshold according to the precision and/or recall values. 
