2 
headway variance and in turn minimize the average waiting 
time of passengers. Basically, the traffic dynamics of metro 
lines underlies train motion dynamics and passenger’s 
access behavior; it is intrinsically continuous, nonlinear and 
stochastic. Hence linear quadratic ATR design, which solves 
a matrix Riccati equation for the regulator by assuming pure 
linear dynamics with Gaussian noise [3], does not assure 
optimality in real environment. Considering the complex 
traffic dynamics of metro lines and constrained by the train 
schedule, the minimum headway variance ATR over time 
can only be solved by dynamic programming. But practically 
exact dynamic programming cannot be used in stochastic 
environments; alternatively adaptive critic designs based 
upon online reinforcement learning can efficiently 
approximate the optimal solution. This paper addresses 
solving the minimum headway variance ATR by 
approximate dynamic programming (ADP) [5] [6].  The 
specific ADP method used here is known as the dual 
heuristic programming (DHP) adaptive critic design with 
multilayered perceptrons network (MLP) as the basic 
learning structure [7] [8]. The proposed adaptive critic ATR 
design considers general cases of metro lines but the 
verification uses experienced data drawn from Taipei Metro. 
2. Traffic dynamics of metro lines 
The traffic dynamics of metro lines can be described in 
the sense of departure time as follows:  
)()()()()()()()(1)( nnnnnnnnn wBDBuBtAt +++=+   (1)  
))(()( 0 nn TRSD ++=            (2)  
)()()( nnn mTtx −=   (3)  
where t, u, w denote the actual departure time, control action, 
and operation disturbance, respectively; S, R, T, x are the 
scheduled dwell time, scheduled run time, reference 
scheduled departure time, and schedule deviation, 
respectively. The vector components are defined as follows: 
TNn
N
nn
m TTTn ][)( 2211 −−−= LT  
THnn ]00[)(0 L=T  
TNn
N
nn tttn ][)( 2211 −−−= Lt  
TNn
N
nn uuun ][)( 11110 +−−−= Lu  
TNn
N
nn wwwn ][)( 11110 +−−−= Lw  
T
NSSSn ][)( 21 L=S  
T
NRRRn ][)( 21 L=R  
where njt = actual departure time of n
th
 train from jth station, 
1−n
jT = reference scheduled departure time of n
th
 train at jth 
station, jS = scheduled dwell time at station j, jR = 
scheduled runtime between stations j-1 and j, H= scheduled 
headway of the metro line, nju = control action (dwell and 
runtime) to nth train leaving (j-1)th station, njw = operation 
disturbance on nth train between (j-1)th and jth stations caused 
by driver or system abnormality, and )(tjϕ = passenger flow 
at jth station between two successive trains. 
In real metro lines, A(n) and B(n) in (1) are dependent of 
the stochastic effects of passenger’s action and flow, and 
may vary from system to system. Usually, we may assume 
these stochastic effects are characterized by Poisson 
distributions. But for simplicity, literature [2] and [3] use 
constant, nominal matrices A  and B  regressed from 
system operation data to reduce (1) into a deterministic, 
linear, time invariant system so that linear optimal control 
theory can be invoked to solve the train regulation problem. 
When the nominal model is used, the nominal schedule, T , 
is computed as 
)()(1)( nnn DBTAT +=+   (4)  
The schedule deviation, x(j), is simplified to                  
)()()()()()( nnnnnn wBuBxATtx ++=−=   (5)  
Using this simplified model and linear quadratic loss 
function, [3] solved approximately the train regulation 
problems of line and loop style metro networks. 
3. DHP adaptive critic ATR design for minimum 
headway variance 
Dynamic programming was developed in the field of 
operations research by Richard Bellman based on the 
principle of optimality [9]. The principle of optimality is 
formulated as the Bellman equation 
)1(())()((Opt            
))()((Opt))((
0
++=
++= ∑∞
=
nJn,nU
kn,knUnJ
)n(
k
k
)n(
xux
uxx
u
u
γ
γ
  (6)  
where the angle brackets denote the expectation value, x is 
the state vector, u is the control actions, 10, ≤≤ γγ  is the 
discount factor, n is an integer denoting the time tick, U is 
the primary utility function, J is called the value function or 
the secondary utility function, and “Opt” denotes 
optimization. The primary utility function is the 
measurement that expresses the interaction between the 
control actions and the plant (environment). When the 
primary utility function is defined to measure positive 
interaction, its value is called a “reward”. In contrary, when 
it is used to measure negative interaction, its value is called a 
“penalty”. While the primary utility function is the reward or 
penalty in present time, the value function is the sum of the 
reward or penalty from present to future time. Therefore 
optimization in the Bellman equation is either maximization 
or minimization depending on the primary utility function 
measuring positive or negative interaction. For the minimum 
headway variance, the primary utility function is defined as 
)()())()1(())()1((
)1()1()(
nnnnnn
nnnU
TT
T
RuuxxQxx
Pxx
+−+−++
++=
 (7)  
Since (7) is actually a quadratic loss function representing 
negative interaction, minimize the value function (6) over 
time subject to (7) may obtain the desired ATR. If the traffic 
dynamics of metro lines was purely linear, with Gaussian 
noise, then exact solution of the Bellman equation is possible 
[3]. Otherwise, ADP is a practical and efficient method to 
obtain the near-optimal solution. The basic idea of ADP is 
that the user supplies both a primary utility function U and a 
model of the plant then approximate the unknown value 
function J by reinforcement learning and then uses J to 
calculate the control actions u. ADP based on neural 
network learning has been developed in literature. The 
synonyms of this approach include “adaptive critic designs” 
4 
the training converges to sufficiently small error within 5 
epochs.  
Experiment 2: Schedule deviation subject to 240 sec. initial 
delay 
Fig. 3 shows that the adaptive critic ATR system and the 
linear quadratic ATR system perform equally well in 
schedule deviation subject to 240 sec. initial delay.  
Experiment 3: Schedule deviation subject to 240 sec. initial 
delay and disturbance on every train 
Fig. 4 shows when the operation is disturbed by normal 
distributed noise with mean delay of 20 sec. and standard 
deviation of 10 sec. The adaptive critic ATR system 
outperforms the linear quadratic ATR system.  
Experiment 4: Schedule deviation subject to 240 sec. initial 
delay and constraints on control actions 
In real metro lines, the control actions are constrained by 
physical conditions. Fig. 5 shows the schedule deviation 
when control actions are constrained by minimum run tine 
no less than 30% of standard run time and maximum run 
time of 120 sec. In this circumstance, the adaptive critic 
ATR system performs worse a little bit than the linear 
quadratic ATR system. The cause is that the adaptive critic 
ATR system has been trained by examples with 
unconstrained control actions. If the training examples take 
the constrained control actions into account, the adaptive 
critic ATR system should perform equally well.  
5. Conclusion 
Underlying dual heuristic programming, the method for 
designing the adaptive critic ATR system to obtain minimum 
headway variance has been built successfully. Comparing 
with the linear quadratic ATR system which assumed linear, 
deterministic traffic model, the adaptive critic ATR system 
was able to comply with un-deterministic disturbance. The 
adaptive critic ATR system relied on neural networks and 
reinforcement learning scheme to learn the traffic dynamics 
of metro lines. For ATR with online learning, the neural 
networks should be able to do sequential and fast learning. 
Future works would take this condition into consideration. 
6. Acknowledgements 
The authors gratefully acknowledge National Science 
Council of Taiwan for research grant under 
NSC95-2221-E-002-131. 
7. References 
[1] J-h Chen, R-L Lin, and Y-C Liu, Optimization of an 
MRT train schedule: reducing maximum traction 
power by using genetic algorithms, IEEE Trans. On 
Power Systems, vol. 20, no. 3, pp. 1366-1372, Aug. 
2005. 
[2]  H. Sasama and Y. Okawa, Floating traffic control for 
public transportation system, Planning and Control of 
Urban Transportation System, pp. 229-235, 1983. 
[3]  V. V. Breusegem; G. Campion, and G. Bastin, Traffic 
modeling and state feedback control for metro lines, 
IEEE Transactions on Automatic Control, vol. 
36,  Issue 7,  pp.770 – 784, July 1991. 
[4]  Y. Ding and S. I. Chien, Improving transit service 
quality and headway regularity with real-time control, 
Transportation Research Record, Issue 1760, pp. 
161-170, 2001.  
[5] P. Werbos, “Beyond regression: New Tools for 
Prediction and Analysis in the Behavioral Sciences”, 
PhD. thesis, Committee on Applied Mathematics, 
Harvard Univ., Cambridge, MA, 1974. 
[6]  P. Werbos, ADP: goals, opportunities and principles, 
Handbook of Learning and Approximate Dynamic 
Programming edited by J. Si, A. G. Barto, W. B. 
Powell and D. Wunsch II, IEEE Press Series on 
Computational Intelligence, pp. 3-44, 2004. 
[7]  D. White and D. Sofge (editors), Handbook of 
intelligent control, Van Nostrand, New York, 1992. 
[8]  D. Prokhorov and D. Wunsch, Adaptive critic designs. 
IEEE Trans. Neural Networks, vol. 8, pp. 997-1007, 
1997. 
[9]  Bellman, Dynamic programming, Princeton 
University Press, 1957. 
 
0 5 10 15 20 25 30 35 40 45 50
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
M
ea
n 
Sq
ua
re
 
De
v
ia
tio
n 
of
 
Sc
he
du
le
, 
se
c.
No of Training Eporch
Overal Performance, =0.33572
Schedule Cost Factor, p =1
Schedule Deviation =0.26848
Headway Cost Factor,q =1
Headway Deviation =0.14818
Run Time Cost Factor, r=1
Run Time Deviation =0.1362
Evolution of Performance in DHP Training
 
 
Run Time Deviation
Headway Deviation
Schedule Deviation
Overall Performance
 
Fig. 2 Training convergence of the adaptive critic ATR 
system 
2 4 6 8 10 12 14
-50
0
50
100
150
200
250
Schedule Deviation(DHP)
initial delay( first train departed from first station) 240 sec.
Overall system Performance=0.28335sec.
Schedule Deviation = 0.19362sec. (Cost Factor, p=1)
Headway Deviation =0.15236sec. (Cost Factor, q=1)
Run Time Deviation=0.13995sec. (Cost Factor, r=1)
Station No
De
v
ia
tio
n 
of
 
Sc
he
du
le
, 
se
c.
 
 
Train No.1
Train No.11
Train No.21
 
Fig. 3 (a) Schedule deviation subject to 240 sec. initial delay 
(The adaptive critic ATR design) 
  
Abstract—Autonomous drive of wheeled mobile robot (WMR) 
needs implementing velocity and path tracking control subject to 
complex dynamical constraints. Conventionally, this control 
design is obtained by analysis and synthesis of the WMR system. 
This paper presents the dual heuristic programming (DHP) 
adaptive critic design of the motion control system that enables 
WMR to achieve the control purpose simply by learning through 
trial. The design consists of an adaptive critic velocity 
neuro-control loop and a posture neuro-control loop. The neural 
weights in the velocity neuro-controller (VNC) are corrected with 
the DHP adaptive critic method. The designer simply expresses 
the control objective with a utility function. The VNC learns by 
sequential optimization to satisfy the control objective. The 
posture neuro-controller (PNC) approximates the inverse velocity 
model of WMR so as to map planned positions to desired velocities. 
Supervised drive of WMR in variant velocities supplies training 
samples for the PNC and VNC to setup the neural weights. In 
autonomous drive, the learning mechanism keeps improving the 
PNC and VNC. The design is evaluated on an experimental WMR. 
The excellent results make it certain that the DHP adaptive critic 
motion control design enables WMR to develop the control ability 
autonomously. 
 
Index Terms—Adaptive critic design, autonomous robot, 
neuro-control, dual heuristic programming, reinforcement 
learning 
 
I. INTRODUCTION 
utonomous wheeled mobile robot (WMR) relies on using 
sensors to percept its surroundings and using motion 
controller to drive to the destination. In the motion control, 
WMR should be capable of performing trajectory tracking, path 
following and stabilization. Since WMR is a nonholonomic 
dynamic system with intrinsic non-linearity, and commonly 
with unmodeled disturbance and unstructured, unmodeled 
dynamics [1]. Therefore, unless the mass is negligible [2], the 
motion control should consider the dynamics of WMR [3], [4]. 
Conventionally, this control design relies on engineers to 
analyze and synthesize the WMR system [5]-[7]. But usually 
 
 
1 Corresponding author, Wei-Song Lin is with the Department and Institute of 
Electrical Engineering, National Taiwan University; Address: No.1, Sec. 4, 
Roosevelt Rd., Taipei 106, Taiwan; E-mail: weisong@cc.ee.ntu.edu.tw, Fax: 
+886-2-23638247, Phone: +886-2-33663638 
2 Ping-Chieh Yang is with the army of Taiwan and was with the Department and 
Intitute of Electrical Engineering, National Taiwan University; Address: 
No.1, Sec. 4, Roosevelt Rd., Taipei 106, Taiwan; E-mail: 
r93921078@ntu.edu.tw 
difficulties arise from absence of accurate WMR model. Fuzzy 
control may skip the model but needs domain expert to 
construct the fuzzy rules [8], [9]. Controllers based on neural 
networks or neuro-fuzzy networks may construct the control 
function by learning the training samples [10]-[12]. But 
preparing appropriate training samples usually needs an 
existing controller. This limits the applications of neural and 
neuro-fuzzy learning control to a minimum. Alternatively, the 
adaptive critic motion control design presented in this paper 
enables WMR to build the control function by learning to 
optimize an objective function. No domain expert to setup the 
control rules and no existing controller to generate the training 
samples are required. In our laboratory, an experimental WMR 
was developed and its mathematical model was formulated and 
identified [13]. A hierarchical fuzzy control system was 
implemented and shown able to conduct the motion of WMR 
[14]. Furthermore, the experimental WMR was equipped with a 
stereovision system to enable autonomous path finding and 
collision avoidance [15]. In this paper, we assume the 
stereovision system foresees nearest path and the WMR system 
must construct the motion control function entirely through 
learning by trials. Essentially, this extends the definition of 
autonomous robot to autonomous setup of intelligence. But 
presently the autonomous setup is limited to the motion control. 
The idea is to obtain the motion control by learning to satisfy or 
optimize a specified objective function. Trials, actually 
supervised trials for the sake of safety, supply training samples 
to correct the neural networks. As a result, the neural networks 
can learn the motion control without reference to any existing 
controller. The dual heuristic programming (DHP) adaptive 
critic method [16], [17] is invoked to design the learning system. 
Multilayer perceptrons, a type of feedforward neural networks, 
are used as the basic learning structure to construct the posture 
neuro-controller (PNC) and velocity neuro-controller (VNC). 
The PNC learns to map planned positions to suitable desired 
velocity. The VNC learns to conduct the WMR motion so as to 
track the desired velocity. Supervised and autonomous drive of 
WMR in variant velocities supplies training samples for the 
PNC and VNC to correct the neural weights. The proposed 
design is evaluated with the experimental WMR. 
This paper is organized as follows: Section II presents the 
architecture of the interested autonomous WMR and illustrates 
the blocks in the adaptive critic motion control system. Section 
III formulates the adaptive critic motion control design. Section 
IV evaluates the control performance with the experimental 
WMR. Section V is the conclusions. 
 
DHP Adaptive Critic Motion Control of 
Autonomous Wheeled Mobile Robot 
Wei-Song Lin1, Member, IEEE and Ping-Chieh Yang2
Department and Institute of Electrical Engineering, National Taiwan University, Taiwan 
A 
311
Proceedings of the 2007 IEEE Symposium on Approximate 
Dynamic Programming and Reinforcement Learning (ADPRL 2007)
1-4244-0706-0/07/$20.00 ©2007 IEEE
under conditions of noise, uncertainty, and nonlinearity [16]. 
Heuristic dynamic programming (HDP), dual heuristic 
programming (DHP), and globalized dual heuristic 
programming (GDHP) are the main categories of adaptive critic 
designs [17]. They are differentiated by the critic output. HDP 
uses the critic to estimate the value function in the Bellman 
equation of dynamic programming. In DHP, the critic 
approximates the derivatives of the value function to facilitate 
the computation in the gradient descent correcting rule. The 
critic in GDHP estimates both the value function and its 
derivatives. DHP was shown to have a superior performance to 
HDP and no observable improved performance by GDHP [19], 
[20]. Therefore, DHP is chosen for the adaptive critic motion 
control design of autonomous WMR. 
 
 
 
Fig. 3 Architecture of the DHP adaptive critic velocity 
neuro-controller 
 
As shown in figure 3, the DHP adaptive critic VNC consists 
of neural networks to implement the action, critic, verification 
and even the plant model. In the figure, R(t) represents the 
state variable, u(t) is the control signal and U(t) which depends 
on R(t) is the primary utility function defined by the user for the 
specific application context. The neural weights in the action 
are corrected to minimize not only the present utility function 
alone but also the sum of all future values of U(t). The 
verification and critic approximate the derivatives of the 
secondary utility function J (the value function) with respect to 
its state variables at present and immediate future instances. In 
the Bellman equation of dynamic programming, J is expressed 
as 
)1()()()(
0
++=+= ∑∞
=
tJtUktUtJ
k
k γγ            (2) 
where γ , 10 ≤< γ  is a discount factor. Accordingly, the 
verification and critic outputs are respectively 
( ) ( )( )tR
tJt
s
s ∂
∂
=λ                                      (3) 
( ) ( )( )1
1
+∂
+∂
=
tR
tJt
s
s
Dλ                                  (4) 
The critic critiques )(tλ to be 
( ) ( ) ( ) ( )( )
( )
( )
( )
( )
( )
( )
( )
( )
( )
( )
( )
( )
( )
( )
( )
( )∑ ∑
∑∑
′ ′
′
′
′
′
′
′
′


 



∂
∂
∂
+∂
+∂
+∂
+




∂
+∂
+∂
+∂
+




∂
∂
∂
∂
+
∂
∂
=
++
∂
∂
=
s k s
k
k
s
s
s s
s
sk s
k
ks
s
s
tR
tu
tu
tR
tR
tJ
tR
tR
tR
tJ
tR
tu
tu
tU
tR
tU
tJtU
tR
t
1
1
1
1
1
1         
1
γ
γ
γλD
   (5) 
Therefore, the neural weights in the verification are corrected to 
minimize the following error function over time 
( ) ( )( )∑ −=
s
ss tttE
25.0)( Dλλ                          (6) 
Using the gradient descent method, the correcting rule of the 
verification is obtained as 
( ) ( ) ( )( )
)(
)(
)(
)(
tw
ttt
tw
tEtw
i
s
ss
i
i ∂
∂
−=
∂
∂
=
λλλαα∆ D         (7) 
where α  is the learning rate and iw  is a neural weight of the 
verification. The critic duplicates the neural weights in the 
verification and therefore no correcting rule is needed.  
The objective of the action is to perform control so as to 
minimize J. The correcting rule is 
( ) ( )( )
( )
( )
( )
( )
( )
( )
( )
( )
( )
( )
( )
( ) ( )
( )
( )
( )
( )tw
tu
tu
tRt
tu
tU
tw
tu
tu
tJ
tu
tU
tw
tu
tu
tJ
tw
tJtw
ik
k
k
s
s
s
k
ik
k
kk
ik
k
kik
ik
∂
∂




∂
+∂
++
∂
∂
=
∂
∂




∂
+∂
+
∂
∂
=
∂
∂
∂
∂
=
∂
∂
=
∑ 11
1
Dλγβ
γβ
ββ∆
     (8) 
where β  is a positive learning rate and )(twik  is a neural 
weight of the action. The plant model predicts the immediate 
future state )1( +tR  and calculates certain partial derivatives 
pertaining to the plant being controlled. The objective of the 
VNC is to track the desired velocities as closely as possible. 
Therefore the primary utility function is chosen as 
22 ))()((25.0))()((25.0)( tttvtvtU dd ωω −+−=         (9) 
B. Neural networks in the velocity neuro-controller  
Multilayer perceptrons (MLP) is used to implement the 
action, verification and critic of the VNC. The action has four 
inputs (v(t),w(t),vd(t),wd(t)) and two outputs corresponding to 
wheel’s driving torques (τl(t),τr(t)). Figure 4 shows a block 
diagram of the action neural network. The number of hidden 
neurons is an experienced choice and each hidden neuron has a 
hyperbolic-tangent activation function. The output layer has 
two neurons and each with a linear activation function. 
The critic neural network has four inputs (v(t+1), w(t+1), 
vd(t), wd(t)) and two outputs (λ°1(t+1), λ°2(t+1)). Figure 5 shows 
its architecture. The activation functions are chosen the same as 
that of the action network. The verification neural network has 
architecture identical to the critic neural network except the 
inputs and outputs are (v(t),w(t),vd(t),wd(t)) and (λ1(t),λ2(t)), 
respectively. The DHP method needs the plant model to predict 
the immediate future states and calculate the plant Jacobian 
quantities. Although neural modeling is possible, the analytic 
model of WMR [13] is adopted in this design. 
313
Proceedings of the 2007 IEEE Symposium on Approximate 
Dynamic Programming and Reinforcement Learning (ADPRL 2007)
 
Fig. 8 Architecture of learning the inverse velocity model 
D. The path planner 
The stereovision unit locates a target and finds a collision free 
path. According to the path and considering the physical 
constraints of WMR, the path planner modifies the path by 
smoothing and produces planed positions to approach the 
destination. The arc-line algorithm [22] is adopted to plan a 
smooth path. As illustrated in figure 9, this algorithm mainly 
replaces the line segments around the intersection of two 
straight lines with a smooth curve. First, the start point 
S(xs,ys,θs) on the first line, the end point E(xe,ye,θe) on the 
second line, the intersection point I(xi,yi,θi) of these two lines, 
and the angle ( dφ = θi - θe) between these two lines are found. 
Then we assume a value of curvature (γ ) to find the transition 
point T(xt,yt,θt) on the first line, the distance )2/tan( dφγ  to the 
intersection point, and the center point C(xc,yc). Finally, we use 
the arc starts at point T to replace the original straight line 
segments. 
Denote the physical limits of WMR in one-step displacement 
and steering-angle as maxd  and maxφ , respectively. Then by 
constructing a displacement vector from present position 
),,( ppp yx θ  to a target position (xb,yb) selected on the planed 
path, the desired displacement dp and steering angle pφ  can be 
calculated. When they violate the physical limits, the maximum 
allowable values are used.  Then the planed position and 
orienting angle are calculated as 
ppp
ppppp
ppppp
tt
dtyty
dtxtx
φθθ
θφ
θφ
+=+
++=+
++=+
)()1(
)sin()()1(
)cos()()1(                        (13) 
The above procedure is iterated once again to obtain planed 
positions and orienting angles for two sampling times.  
 
Fig. 9 Planning a smooth path by using the arc-line algorithm 
IV. PERFORMANCE OF THE DHP ADAPTIVE CRITIC MOTION 
CONTROL SYSTEM 
A. Performance of the adaptive critic velocity control system 
In this study, all neural weights in the neural networks of the 
action, critic and verification are initialized with values chosen 
randomly in the range [-0.1, 0.1]. The training samples are 
generated with (14) for 1000 samples (i=1~1000). 
  
)
40
1sin(]1)
1000
)1(6[cos()(
]1)
1000
)1(6[cos(5.0)( 001.0
−
++
−
=
++
−
=
−
iiiw
eiiv
d
i
d
π
π
π
π
              (14) 
These training samples are used to train the adaptive critic 
velocity control system for 500 epochs. Then the WMR system 
is tested to track a set of desired velocity patterns described by 
the following equation: 







>−
≤<−
≤<−
≤<−
≤−
=







>
≤<−
≤<
≤<
≤
=
9004.0
9007001.3003.0
7006001
6003006.0
300002.0
)(
9004.0
900700003.01.3
7006001
6003006.0
300002.0
)(
2
2
i
ii
i
i
ii
iw
i
ii
i
i
ii
iv
t
t
                     (15) 
Figures 10 and 11 show the results of velocity tracking. The 
accurate velocity tracking confirms the excellent performance 
of the adaptive critic velocity control system. 
B. Performance of the inverse velocity model 
In this study, all neural weights are initialized with values 
chosen randomly in the range [-0.1, 0.1]. The fuzzy posture 
controller used in [13] is connected to the adaptive critic 
velocity control system to conduct the WMR motion. During 
tracking a variety of trajectories, the desired velocities and the 
corresponding WMR positions at each sampling time are 
recorded as the training samples to train the PNC. Then the 
trained WMR system is commanded to track a sequence of 
positions calculated with 
)
1000
33cos( xy π=                                  (16) 
The desired velocity and the corresponding output position at 
every sampling time are recorded as test samples.  Then the 
position test samples are fed into the inverse velocity model 
sequentially and the outputs are compared with the velocity test 
samples. The results are presented in Figures 12 and 13. These 
excellent results show the inverse velocity model is well 
trained. 
C. Performance of the posture control system 
The trained WMR system is commanded to track a path 
containing a right turn. The following cases study the 
performance of the posture control system under dmax=0.035 
and maxφ =0.03. 
315
Proceedings of the 2007 IEEE Symposium on Approximate 
Dynamic Programming and Reinforcement Learning (ADPRL 2007)
