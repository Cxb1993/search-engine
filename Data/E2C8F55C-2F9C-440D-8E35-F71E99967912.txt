nodes in a cloud cluster. Therefore, in this project, 
the adaptive task assignment approach is proposed to 
assign appropriate mapper tasks no nodes according to 
the job types and nodes＇ capability and available 
resources. The objective is to achieving the load 
balancing among nodes which incurs high performance 
and resource utilization. Moreover, in order to 
realize such system, a prototype, which is able to 
distribute tasks to nearby mobile nodes for parallel 
processing, has been designed and implemented in this 
project. For ensuring the data security, the data 
transmission in this system is encrypted. According 
to users＇ security and performance requirements, 
this system can select the appropriate nodes and 
encryption algorithms for maximizing the data 
security and job performance. 
英文關鍵詞： Cloud computing, MapReduce, Load balancing, Task 
assignment, Data encryption 
 
 2
點數來進行工作的分散式處理才能夠縮短執行時間，而這是因為工作運算完成後還需要時間來進行各
節點運算結果的整合。舉例來說，如圖二所示，假設一個節點處理 200 MB的資料量需要花 20秒而且
網路傳輸速度是 5 MBps，當 1GB的資料由 5個節點來處理時需花費 20秒的運算時間與傳輸時間 160
秒(假設整合節點同時只能接收一個節點的運算結果)，所以執行時間總共需要 180秒；當 1 GB的資料
由 2個節點來處理時需花費 50秒的運算時間與傳輸時間 100秒，所以執行時間總共需耗費 150秒。所
以，如何根據網路環境狀態選擇節點的數量來做進行運算，會影響雲端運算的運算效能。 
 
圖二 執行節點數量與執行效能的關係 
 
第二，該如何根據節點的能力與可用資源分配適當的 mapper任務數量給執行工作的節點也會影響
雲端服務效能。例如，當 mapper任務數量已經超出節點本身能負載的工作量時，節點運算效能就會降
低，進而影響到整個工作的運算效能。舉例來說，如圖三所示，左圖是採用平均分配 mapper任務數量
的方式，因為節點可用資源所以會導致節點的負載不平均而影響到工作執行的時間；右圖則是考量節
點可用資源來分配不同的 mapper任務數量，因此可以平衡節點的負載。所以如何根據節點能力與可用
資源來配置 mapper任務數量使得節點間工作負載平衡，亦是影響MapReduce效能的議題之一。 
 
圖三 工作量對節點的負載比例圖 
 
第三，工作型態也會影響雲端服務效能，例如運算密集(computation intensive)的工作指的是在執行
工作時 CPU 負載大多是 100%，而資料的傳輸可以在很短的時間內完成，所以需要較多的運算資源；
而通訊密集(communication intensive)的工作指的是在執行工作時大部分的狀況是在進行資料的傳遞，
 4
3.1. 主要方法 
本計畫提出的 ATA方法共有五個步驟並且分成兩個階段進行改善，流程如圖五所示。第一個階段
(以下簡稱 ATA-1)有四個步驟，(1)分析工作特徵、(2)決定執行工作的節點數量、(3)決定執行工作的節
點、與(4)決定平均分配 mapper任務的數量；第二個階段(以下簡稱 ATA)有一個步驟，(5)根據節點能力
與可用資源重新配置 mapper任務。 
 
 
圖五 ATA優化流程圖 
 
在第一個階段(ATA-1)下，我們的實驗結果比 Hadoop預設方法更有效率，這是因為 Hadoop預設方
法不會根據檔案大小與工作的特徵來決定選取哪幾個節點來進行運算，而是平均分配給全部的節點來
運算。相反地，所提出的 ATA-1方法會根據檔案大小與工作特徵來決定選取幾個節點來運算以及每個
節點應該平均執行幾個 mapper任務以改善 MapReduce效能。然而，如果平均分配 mapper任務數量，
則在一個異質雲中資源較低的節點與資源較高的節點因為分配的工作量相同而會導致工作量不平衡的
狀況(資源較低的節點工作負載過高)，而造成運算效能下降。因此，本計畫所提出的 ATA 方法就是為
了改善上述問題。ATA會進一步根據實際節點的能力與可用資源，對在平均分配下的 mapper任務數量
作微調，使得運算能力與可用資源較高的節點執行較多的 mapper任務；反之，運算能力與可用資源較
低的節點執行較少的 mapper任務來得到一個能讓工作負載平衡的任務配置。 
 
3.1.1. 第一階段(ATA-1)方法說明 
A. 步驟一：分析工作特徵 
如上述，因為不同型態的工作應該以不同的節點數來進行分散式運算以提高雲端服務效能，所以
我們所提出的方法會先分析工作特徵。例如，由於 CPU-bound的工作需要較高的 CPU資源來運算，並
且不需要常常存取資料，所以 CPU-bound的工作可以在較多的節點數量來運算以得到較高的工作效能；
反之，若是 I/O-bound的工作，由於需要常常進行資料的存取與傳輸，並且不需要較高的 CPU資源來
運算，所以如果在較少的節點上進行分散式運算可以有效降低節點之間的資料傳輸時間。因此，分析
工作特徵來決定運算節點的數量是必要的。 
本計畫應用過去的文獻[2][7]所提出的方法來完成工作特徵的判定。由於一個MapReduce工作所需
 6
於 CPU-bound 但 mapper 任務輸出資料量卻很大的工作在網路傳輸速度很慢的雲端叢集中執行時，如
果在步驟二考慮工作特徵權重，即採用方程式(3)，由於 CPU-bound工作需要較多的節點來運算，但是
因為 mapper任務輸出資料量很大而網路傳輸的速度較慢，所以可能導致需要更多的資料傳輸時間，使
得整體運算時間的拉長。因此，在此情況下如果不考慮工作特徵權重，即採用方程式(4)，反而可以達
到更好的效能。 
 
C. 步驟三：決定選擇那些節點工作 
在步驟二決定出最佳的工作節點數′之後，ATA-1 會根據工作特徵與節點可用資源在雲端叢集中
所有 N個節點中選取′個節點來執行工作。這是因為如果節點選擇不好也會導致運算效能下降，例如
將 CPU 資源較低的節點分配去執行運算密集的工作，這樣導致運算時間拖長，若分配 CPU 資源較高
的節點去運算通訊密集的工作，則會導致資源的浪費。首先，系統必須先收集雲端叢集中所有節點的
可用資源(透過總計畫中其他子計畫)，再根據這些資訊以及工作的型態來決定來選擇適當的節點執行工
作。簡單來說，運算密集的工作，ATA-1會選取 CPU可用資源比例較高的節點；反之，通訊密集的工
作會選擇網路頻寬資源比例較高的節點，這樣就可以選擇能夠符合工作特徵且有效利用資源的節點來
執行工作。 
詳細作法說明如下。首先，必須根據方程式(6)計算在第	個節點中第種資源占所有資源的比例，
以,表示， 
, =

∗,
,

∑ 
∗	,
,


, ∀                  (6) 
其中,、,與分別為第 n個節點中第 k種資源可用量、第 n個節點中第 k種資源最大供應量、與
該工作對於第種資源的需求程度而 
∑  = 1                     (7) 
在方程式(6)中，因為每一種資源的單位都不相同而無法直接加在一起，所以我們會先將每種資源
進行正規化(normalization)，亦即將第 n節點第 k種資源的可用量除以第 k種資源的最大供應量。另外，
由於運算密集與通訊密集的工作對於不同種類資源的需求程度不同，所以可以透過來決定該工作對
於每種資源的需求程度，如圖六所示，運算密集、通訊密集或是平均的工作可以採用不同的權重值組
合。 
運算密集型工作 CPU HD RAM OTHER 
權重 0.5 0.2 0.2 0.1 
通訊密集型工作 CPU HD RAM OTHER 
權重 0.2 0.5 0.2 0.1 
平均型工作 CPU HD RAM OTHER 
權重 0.3 0.3 0.2 0.2 
 
可用資源 節點 A 節點 B 節點 C 節點 D 
CPU 30% 10% 20% 40% 
HD 20% 30% 40% 10% 
RAM 20% 30% 10% 30% 
OTHER 10% 10% 10% 10% 
 
 節點 A 節點 B 節點 C 節點 D 
運算密集型工作 62.5% 27.7% 47.6% 68.9% 
通訊密集型工作 47.6% 62.5% 74% 20% 
圖六 節點資源比例與工作負載的關係 
 
D. 步驟四：決定平均分配 mapper任務的數量 
當選取好適當的執行節點之後，ATA-1接下來會決定如何分配mapper任務給所選出來的節點執行。
 8
示： 
	 =
∑ 
∗	,
,


∑ ∑ 
∗	,
,






∗ ∑ 	                 (9) 
其中	與		分別為第
個被選出來執行工作的節點中第 k種資源的可用量與最大供應量；而	與	分
別是第
個被選出來執行工作的節點調整後與調整前的處理 mapper任務數量。根據方程式(9)，資源較
少的節點會被分配到較少的 mapper任務數量；反之，資源較高的節點則會被分配到較多的 mapper任
務數量以達到工作負載平衡。 
 
4. 結果與討論 
本計畫透過實驗來進行理論的驗證，實驗環境與實驗結果說明如下。 
 
4.1. 實驗環境 
本計畫施行之實驗環境主要建構於兩台以 Gigabit乙太網路連結的實體機器上，實體機器的硬體規
格 4核心且處理速度為 2.66GHz的處理器(CPU)、主記憶體(MEM)為 8 GB、以及裝載 1 TB的硬碟(HD)。
在兩台實體機器上我們安裝了 XEN開放原始碼的虛擬化系統用來建構虛擬節點，並在虛擬節點上安裝
Hadoop 來執行 MapReduce 工作。由於實體機器的硬體限制，所以每台實體機器最多可以模擬 4 個虛
擬節點。實驗主要在如表一、表二、與表三等三個不同的實驗環境下進行。 
 
表一 配置 2個虛擬節點的異質雲端叢集 
 Slave 1 Slave 2 
CPU 4 core 1 core 
MEM 1.5 GB 1.5 GB 
HD 200 GB 200 GB 
 
表二 配置 4個虛擬節點的異質雲端叢集 
 Slave1 Slave2 Slave3 Slave4 
CPU 3 core 1 core 2 core 2 core 
MEM 3 GB 3 GB 3 GB 3 GB 
HD 200 GB 200 GB 200 GB 200 GB 
 
表三 配置 8個虛擬節點的同質雲端叢集 
 Slave1 Slave2 Slave3 Slave4 Slave5 Slave6 Slave7 Slave8 
CPU 1 core 1 core 1 core 1 core 1 core 1 core 1 core 1 core 
MEM 1.5 GB 1.5 GB 1.5 GB 1.5 GB 1.5 GB 1.5 GB 1.5 GB 1.5 GB 
HD 200 GB 200 GB 200 GB 200 GB 200 GB 200 GB 200 GB 200 GB 
 
4.2. 實驗結果 
在下面的實驗中將比較 ATA、ATA-2、與 Hadoop預設方法的效能。我們使用 Grep與 Sort程式分
別代表運算密集與通訊密集的工作並以 10 GB、20 GB、40 GB、與 80 GB等不同的檔案大小來觀察不
同方法的效能。 
 
4.2.1. 比較 ATA-1 與Hadoop 預設方法執行運算密集工作的效能 
 10
 
圖十 以 8個節點執行 Grep程式處理不同大小的檔案時每個節點數執行之 mapper任務數量與工作執行時間的關係 
 
4.2.2. 比較 ATA-1 與Hadoop 預設方法執行通訊密集工作的效能 
接下來，我們比較執行 Sort程式時，ATA-1與 Hadoop預設方法的效能。同樣地，根據 ATA-1的
第一步驟，會先找出 Sort 程式的工作特徵，由於 Sort 的輸出等於輸入，所以其工作特徵值ω即為 1。
接下來，根據 ATA-1第二至第四步驟得到由 4個節點且每個節點執行 2個 mapper任務來執行 Srep程
式時為最佳。圖十一與圖十二分別為輸入資料量為 10 GB與 80 GB的實驗結果，從圖中可以發現當運
算的檔案越小時，效能的差異較不明顯，但是當運算的檔案越大時，不同的節點數量與 mapper任務數
量對工作執行時的影響越大。 
 
 
圖十一 執行 Sort程式處理 10 GB檔案時執行節點數量與工作執行時間的關係 
 
 
圖十二 執行 Sort程式處理 80 GB檔案時執行節點數量與工作執行時間的關係 
 
0
1000
2000
3000
4000
5000
10GB 20GB 40GB 80GB
s
e
c
o
n
d
s
Data size
Grep
hadoop-
map*4
hadoop-
map*6
hadoop-
map*8
ATA-1
0
5000
10000
15000
20000
Map*2 Map*4 Map*6 Map*8
s
e
c
o
n
d
s
Sort-10G
Node*2
ATA-1
Node*8
0
10000
20000
30000
40000
50000
60000
70000
Map*2 Map*4 Map*6 Map*8
s
e
c
o
n
d
s
Sort-80G
Node*2
ATA-1
Node*8
 12
 
圖十五 在 4個節點的異質雲端叢集下執行 Sort程式時 ATA、ATA-1與 Hadoop預設方法的效能比較 
 
5. 系統實作 
本計畫第一年主要完成了行動網路雲端運算基礎架構的建制以及基本的環境感知資料存取服務。
為了確保資料的安全性，透過資料加密技術，傳輸過程的資料都是加密過後的密文。當加密過後的資
料分散到不同的節點上後，如果有經過授權的節點即可將資料解密以進行運算。圖十六為目前所完成
系統之系統架構與資料/控制流程圖，說明如下： 
 
 Step 1. Service Request 
使用者透過行動裝置送出身分驗證訊息與服務品質要求。 
 Step 2. Service Profiling  
仲介器在驗證身分驗證訊息後，根據服務品質要求，包含網路、雲端運算中心、周遭行動裝置運
算資源與能力、與資料分布等狀態決定哪些資料由那些節點來執行運算。(環境感知存取) 
 Step 3. Job Submitting 
仲介器將運算工作與 Step 2所決定之運算裝置清單遞交給 jobTracker。 
 Step 4. Job Initialization 
jobTracker進行初始化。(例如，將資料加密後傳給其他裝置運算)。 
 Step 5. Mapper Assignment 
jobTracker將任務(與加密的資料)發送給所決定之運算裝置上的 taskTracker進行運算。 
 Step 6. Mapper Executing (在全部所決定之運算裝置上) 
多個行動裝置上的 taskTacker執行Mapper任務產生中間結果。 
 Step 7. Intermediate result submitting 
taskTracker將中間結果回傳給 jobTracker。 
 Step 8. Reducer Executing 
jobTacker執行 Reducer任務整合中間結果並得到最後結果。 
 Step 9. Return final result 
jobTacker將最後結果回傳給使用者。 
 
0
5000
10000
15000
20000
25000
30000
35000
40000
10GB 20GB 40GB 80GB
s
e
c
o
n
d
s
Data size
Sort-node*4
ATA
ATA-1
hadoop
map*2
hadoop
map*6
hadoop
map*8
 14
 
   
(a)服務要求      (b)網路上裝置狀態清單 
圖十九 根據服務要求在網路上找尋適合的運算節點 
 
 
 Step 3至 Step 9執行結果畫面 
 
圖二十 Broker端執行結果畫面 
 
 16
參考文獻 
[1] Hadoop. (2011). Welcome to Apache™ Hadoop™! [Online]. Available: http://hadoop.apache.org 
[2] Wikipedia. 2011. CPU-bound [Online] Available: http://en.wikipedia.org/wiki/CPU-bound 
[3] 董 的 博 客 . 2011. Hadoop 公 平 調 度 器 算 法 解 析 [Online] Available: 
http://dongxicheng.org/mapreduce/hadoop-fair-scheduler/ 
[4] Wikipedia. 2011. Apache Hadoop [Online]. Available: http://zh.wikipedia.org/wiki/Hadoop 
[5] J. Dean and S. Ghemawat, “MapReduce: Simplied data processing on large clusters,” in Proc. of 4th 
USENIX Symposium on Operating Systems Design and Implementation, 2004, pp. 137-150. 
[6] K. Kambatla, A. Pathak, and H. Pucha, “Towards Optimizing Hadoop Provisioning in the Cloud,” in Proc. 
of the First Workshop on Hot Topics in Cloud Computing, San Diego, 2009. 
[7] C. Tian, H. Zhou, Y. He, and L. Zha, “A Dynamic MapReduce Scheduler for Heterogeneous Workloads,” 
in Proc. of Grid and Cooperative Computing International Conference, China, 2009, pp. 218-224. 
[8] R. Maggiani, “Cloud Computing is Changing How We Communicate,” in Proc. of 2009 IEEE 
International Professional Communication Conference, Hawaiian, 2009, pp.1-4. 
[9] M. Zaharia, A. Konwinski, A.D. Joseph, R. Katz, and I. Stoica.,“Improving MapReduce performance in 
heterogeneous environments,” in Proc. of 8th USENIX Symposium on Operating Systems Design and 
Implementation, San Diego for, 2008, pp. 29-42. 
[10] R. Chen, H. Chen, and B. Zang, “Tiled-MapReduce: optimizing resource usages of data-parallel 
applications on multicore with tiling,” in Proc. of the 19th international conference on Parallel 
architectures and compilation techniques, Vienna, Austria, 2010, pp. 523-534. 
[11] J. Wei, V. T. Ravi, and G. Agrawal, “Comparing MapReduce and Freeride For Data-Intensive 
Applications,” in Proc. of IEEE International Conference on Cluster Computing and Workshops, New 
Orleans, 2009, pp. 1-10. 
[12] W. Hu, C. Tain, X. Liu, H. Qi, L. Zha, U. Liao, Y. Zhang, and J. Zhang, “Multiple-Job Optimization in 
MapReduce for Heterogeneous Workloads,” in Proc. of the Sixth International Conference on Semantics 
Knowledge and Grid, Beijing, China, 2010, pp. 135-140. 
[13] M. Zhou, R. Zhang, D. Zeng, W. Qian, and A. Zhou, “Join Optimization in the MapReduce Environment 
for Column-wise Data Store” in Proc. of the Sixth International Conference on Semantics Knowledge and 
Grid, Beijing, China, 2010, pp.97-104. 
[14] D. Jiang, B. C. Ooi, L. Shi, and S. Wu, “The performance of MapReduce: an in-depth study,” The Proc. 
of the VLDB Endowment VLDB Endowment, vol. 3, no. 1-2, pp. 472-483, September 2010. 
[15] M. M. Rafique, B. Rose, A. R. Butt, and D. S. Nikolopoulos, “Supporting MapReduce on large-scale 
asymmetric multi-core clusters,” ACM SIGOPS Operating Systems Review, vol. 43, no. 2, pp.25-34, 
April 2009. 
[16] L. A. Barroso , J. Dean , and U. Hölzle, ”Web Search for a Planet: The Google Cluster Architecture” 
IEEE Micro, vol. 23, no. 2, pp. 22-28, March-April, 2003. 
[17] C.-L. Chen, J.-W. Lee, W.-T. Su, M.-F. Horng, and Y.-H. Kuo, “Noise Referred Packet Length 
Adaptation and Energy-Proportional Routing for Clustered Sensor Network,“International Journal of Ad 
Hoc and Ubiquitous Computing, vol. 3, no. 4, pp. 224-235, June 2008. 
[18] W. Tom, Hadoop: The Definitive Guide. O'Reilly Media, Inc., 2009 
 1
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                     日期：101 年 9 月 1 日 
一、參加會議經過 
本次參加的是第 1屆 IEEE中國通訊國際會議(以下簡稱 IEEE ICCC)，其規模與網路通訊領域重
要會議 IEEE ICC相當，差別於舉辦地點在中國，而第 1屆當然就選在中國首都北京市中北京大學博
雅飯店來舉辦。IEEE ICCC是一個相當大型的會議，開幕式除了邀請到 IEEE Communication Society
主席 Vijay Bhargava 博士外，還包含中國工業與資訊科技部副部長、中國電信副主席、中國移動通
信副主席、與中國聯通副主席等重要人物，其議程包含了 3 場專題演講、2 個短期課程、與百餘篇
研究論文的發表，其議題涵蓋無線通訊、無線網路、綠能通訊、訊號處理、通訊理論、網路安全、
品質保證、新型網路服務、光纖網路等通訊相關領域。由於 IEEE ICCC同時進行的場次約有 8個，
所以無法參與所有場次，只能挑選幾個與計畫相關主題，如雲端運算與分散式運算，等議題相關之
論文發表的場次來參加。 
這次的專題演講邀請到 NYU-Poly的 T. R. Rappaport教授進行關於無線網路上新型的應用；中
國工業與資訊科技部通訊研究院主席 S. Cao博士進行關於 ICT在中國的發展；以及華為通訊科技實
驗室主任W. Tong博士進行關於行動寬頻網路的未來發展等議題。從 3場專題演講中可以發現未來
行動通訊網路與雲端運算結合已經勢不可擋。本人所發表的論文被安排在會議第 2天下午的「雲端
服務與資源管理」場次中進行報告，共有 4篇論文在此場次中進行報告。本人被安排在第 1順位，
報告結束後隨即有學者提出如何取得雲端叢集下所有節點的資訊，而在所提出的方法中必須要有一
個Master節點來進行蒐集節點資訊才能據以進行工作分配。 
 
二、與會心得 
在此次會議中，除了專題演講邀請到的都是在通訊領域內相當重要的人物外，所發表的研究論
文都相當具有水準，其中不乏相當知名大學，例如多倫多大學、北京大學、南加大、雪梨大學、
香港科技大學、台灣交通大學、台灣清華大學、伊利諾大學香檳分校、香港中文大學、復旦大學、
計畫編號 NSC100-2218-E-156-001 
計畫名稱 子計畫四：開發雲端仲介器閘道之環境感知資料加密與分散服務(I) 
出國人員
姓名 
蘇維宗 
服務機構
及職稱 
真理大學資訊工程學系/助理教授 
會議時間 
101年 8月 15日至 
101年 8月 18日 會議地點 中國北京 
會議名稱 
(中文)第 1屆 IEEE中國通訊國際會議 
(英文) First IEEE International Conference on Communication in China 
發表論文
題目 
(中文) 在異質雲中考量節點能力之資源配置方法 
(英文) Node Capability Aware Resource Provisioning in a Heterogeneous 
Cloud 
Node Capability Aware Resource Provisioning in a 
Heterogeneous Cloud 
 
Wei-Tsung Su and Sun-Ming Wu 
Dept. of Computer Science and Information Engineering 
Aletheia University 
New Taipei City, Taiwan (R.O.C.) 
 
au4451@au.edu.tw and small-teeth@hotmail.com 
 
 
Abstract—Although MapReduce, the core technology of cloud 
computing, lowers the barriers to enter the parallel computing, it 
introduces the other challenging research issue of improving its 
performance via properly resource provisioning. This issue is 
more complex in a heterogeneous cloud with multiple jobs since 
the nodes have various capability and workloads. In addition, the 
limited resources must be shared among all jobs. In this paper, 
this optimization problem, called Node Capability-aware 
Provisioning Problem (NCPP), is first formulated as a 
mathematical model. The purpose of NCPP is to minimize the job 
execution time which is influenced by node capability. However, 
NCPP is subject to the resource constraints on the nodes in a 
cloud. Moreover, the node Capability-Aware Resource 
Provisioner (CARP) is proposed based on Apache Hadoop to 
show its feasibility to solve NCPP in a systematic way. 
Keywords－ Cloud computing; MapReduce; Hadoop; Resource 
provisioning 
 
I.  INTRODUCTION 
In recent years, there are many companies, such as Google, 
Facebook, Amazon, and Dropbox, rise suddenly according to 
their innovation Internet services. Typically, after users issue 
requests, these services are expected to response in time by 
processing massive data. Therefore, the cloud computing 
technology is received significant attention due to its capability 
of parallel and distributed computing. Cloud computing is the 
technology of achieving similar computation power of super 
computers by coordinating a number of commodity computers. 
In 2004, the core technology of cloud computing, MapReduce, 
is first announced by Google [1]. In this computation model, 
the massive data is divided into several splits and is then stored 
in the multiple data nodes via underlying distributed file 
systems, such as GFS [2] or HDFS [3]. When a job is 
submitted, the master node will dispatch the map tasks to these 
nodes for processing partial data and returning the intermediate 
results. After that, these intermediate results will be aggregated 
by the reduce tasks as needed [4]. 
One of the most challenging research issues in cloud 
computing is efficiently allocating computation resources with 
maximizing resulting utility (e.g. minimizing job execution 
time) [5], since there are various utility factors. Because a 
cloud cluster is typically composed of a number of 
heterogeneous computer nodes with various workloads, these 
nodes have different computation capability and dynamic 
available resources [6]. Thus, it is important to dispatch the 
right jobs to the right nodes in a heterogeneous cloud. For 
example, a job with feature of CPU-bound requires more 
computation resources than communication resources. On the 
contrary, a job with feature of IO-bound requires more 
communication resources than computation resources. In 
addition, the resource allocation must consider the capability 
and available resources of nodes. For executing a job, if the 
same number of tasks is allocated to heterogeneous nodes, the 
execution time may be prolonged since the bottleneck is the 
nodes with lower capability and available resources. 
Although several approaches have been proposed to solve 
the resource allocation problem in a heterogeneous cloud [7], 
most of them focus on allocating resources to single job or 
overlook the resource constraints [8]. However, in practical, the 
problem is more complex since there must be multiple jobs 
simultaneously requested [9] by users. In this paper, we first 
formulate the optimization problem of allocating the limited 
resources to multiple jobs according to the job feature and node 
capability. The objective is to maximizing the aggregate 
resulting utility. Moreover, the node Capability-Aware 
Resource Provisioner (CARP) is proposed based on Apache 
Hadoop [10] to show its feasibility to solve above optimization 
problem. 
The rest of the paper is organized as follows. In Section 2, 
related work is surveyed. Then, the problem formulation is 
modeled in Section 3. In Section 4, the detail of CARP is 
described. Finally, the paper is concluded in Section 5. 
II. RELATED WORK 
Due to the emerging of cloud computing, more and more 
government, industry and academic organizations launch 
various kinds of related research projects, including the 
performance of MapReduce which is one of the core 
technologies in cloud computing. Although MapReduce 
technically lowers the barriers to enter the parallel computing, 
it introduces another challenging research issue of improving 
MapReduce performance via properly resource provisioning. 
In this section, we will introduce the basics of MapReduce and 
First IEEE International Conference on Communications in China: Advanced Internet and Cloud (AIC)978-1-4673-2815-9/12/$31.00 ©2012 IEEE 46
time is decreasing. Thus, the utility of job j is further 
formulated as 
j
j
e
u
1
= ,                (2) 
jCjPj TTe ,, +=                 (3) 
where jPT ,  and jCT ,  are the processing time and 
communication time, respectively. 
In addition, because the available resources of all nodes 
in a cloud must be shared among all jobs, the total allocated 
resources cannot exceed the available resources for each node 
in a cloud. Thus, NCPP is subject to the resource constraint as 
rn
J
j
njrnj Rxr ,
||
1
,,,
≤⋅∑
=
, ||,...,1 Nn =∀ , ||,...,1 Rr = .      (4) 
Moreover, since the tasks of job j will be dispatched to a node 
set Nj for executing, NCPP is also subject to the node 
constraint as 
||
||
1
, j
N
n
nj Nx =∑
=
, ||,...,2,1 Jj =∀              (5) 
By summarize Eq. (1) to Eq. (5), the NCPP is finally 
formulated as the following equations. 
Maximize                (6) 
∑
=
||
1
1J
j je
, 
Subject to 
rn
J
j
njrnj Rxr ,
||
1
,,,
≤⋅∑
=
, ||,...,1 Nn =∀ , ||,...,1 Rr = , 
||
||
1
, j
N
n
nj Nx =∑
=
, ||,...,2,1 Jj =∀  
where njx ,  is 1 if the node n is selected to run the tasks of job 
j and 0 otherwise. 
 
Table 1 Notation list 
Notation Description 
N The node set in a cloud 
J The job set in a cloud 
R The resource set in a node 
uj The utility of job j 
ej The execution time of job j 
Rn,r The available value of resource r in node n 
Nj The node set selected from N to run job j 
rj,n,r 
The allocated value of resource r in node n 
for running job j 
Sj The input data size of job j 
MSj The output data size from map tasks of job j 
SSj 
The split-size of job j. By default, the split-
size is 64MB in Hadoop. 
SIj 
The number of instructions per byte required 
to running job j. 
bwn The network bandwidth of node n 
cpn The instructions per second of node n 
 
IV. NODE CAPABILIRT AWARE RESOURCE PROVISIONER 
In order to solve NCPP systematically, the node 
capability-aware resource provisioner (CARP) is proposed in 
this section. In CARP, the resource provisioning will consider 
the job feature, the node capability and available resources. 
Moreover, CARP can be integrated into Hadoop. As shown in 
Fig. 3, the modification is that CARP will assist jobtracker in 
sharing resources among all jobs in an efficient way. 
 
Master Node
Jobtracker
1. Submit job
2. Store job 
data
Client JVM
3. Initialize 
job
4. Retrieve 
metadata
6. Retrieve 
data splitsHDFS
Namenode
TasktrackerDatanode
Slave Nodes
CARP
5. Dispatch 
tasks
 
Fig. 3. Proposed Hadoop MapReduce workflow 
 
The flow chart of CARP is shown in Fig. 4. There are 
four steps in CARP. In step 1, the job feature is first extracted. 
The job feature extraction is important since the resource 
requirements are different between a CPU-bound job and an 
IO-bound job. Then, the parallel degree of each job is 
determined according to the job feature and average node 
capability in a cloud in step 2. Finally, a two-stage procedure 
is employed for the purpose of solving NCPP. In coarse-
grained resource provisioning, NCPP is solved under the 
assumption of that tasks are uniformly allocated to nodes. 
Thus, in fine-grained resource provisioning, the task allocation 
will be further adjusted according to the node capability. The 
detailed description of CARP is described as follows. 
A. Job Feature Extraction 
The first step of CARP is job feature extraction. In this 
paper, we just consider the job feature which identifies a job is 
CPU-bound or IO-bound. The method of determining a job is 
a CPU-bound or an IO-bound job is beyond the scope of this 
paper. Based on the existing determination approaches 48
D. Fine-grained Resouce Provisioning 
In the coarse-grained resource-provisioning described as 
above, the job execution time is estimated based on the 
assumption of that all data splits are equally processed by the 
nodes selected to run this job. The reason is to decrease the 
complexity of NCPP. However, it is not practical in a 
heterogeneous cloud. For example, if there are node A and 
node B where cpA and cpB are 100MHz and 1GHz, 
respectively. If the same data splits are equally processed by 
node A and node B, then the processing time of node A will 
be 10 times of the processing time of node B. This will 
prolong the job execution time.  
Thus, the fine-grained resource provisioning is still 
required. In this step, the data-splits will be re-arranged to 
nodes according to node capability and workload. The re-
arrangement will be an iterative process which follows two 
heuristics. Firstly, the aggregate utility in NCPP must be 
higher than the previous arrangement. Secondly, the resource 
constraints in NCPP must still be met. 
 
V. CONCLUSION AND FUTURE WORK 
In this paper, the optimization problem for resource 
provisioning in a heterogeneous cloud with multiple jobs is 
first formulated as the Node Capability-aware Provisioning 
Problem (NCPP). In NCPP, the capability of nodes in a cloud 
will be considered in resource provisioning to minimize the 
job execution time. Moreover, the node capability-aware 
resource provisioner (CARP), which can be integrated into 
Hadoop, is designed to show its feasibility for solving NCPP 
in a systematic way. 
We focus on the problem formulation and system design 
in this paper. In the future, the algorithms in coarse-grained 
and fine-grained resource provisioning steps will be developed 
and integrated into Hadoop for further evaluation.  
 
Acknowledgement 
This paper is based on the work partially supported by 
National Science Council (NSC), Taiwan, R.O.C., under grant 
NSC100-2218-E-156-001 and NSC99-2632-H-156-001-MY3. 
 
REFERENCES 
[1] J. Dean. “Experiences with MapReduce, an abstraction 
for large-scale computation,“ in Proc. 15th International 
Conference on Parallel Architectures and Compilation 
Techniques, 2006, pp. 1. 
[2] S. Ghemawat, H. Gobioff and S.-T. Leung. “The Google 
File System,” in Proc. ACM Symposium on Operating 
Systems Principles, 2003, pp. 29-43. 
[3] K. Shvachko, “The Hadoop Distributed File System,” in 
Proc. IEEE 26th Symposium on Mass Storage Systems 
and Technologies, 2010, pp. 1-10. 
[4] T. White. Hadoop: The Definitive Guide, 2nd Edition. 
Sebastopol, CA: O’Reilly Media, Inc., 2010. 
[5] S. Khatua. “Optimizing the utilization of virtual 
resources in Cloud environment,“ in Proc. IEEE 
International Conference on Virtual Environments 
Human-Computer Interfaces and Measurement Systems, 
2010, pp. 82-87 
[6] T. Sandholm and K. Lai. “Dynamic Proportional Share 
Scheduling in Hadoop,” in Proc. 15th International 
Workshop on Job Scheduling Strategies for Parallel 
Processing, 2010, pp. 110-131. 
[7] J. U. Duselis. “Resource selection and allocation for 
dynamic adaptive computing in heterogeneous 
clusters,“ in Proc. IEEE International Conference, 
Cluster Computing and Workshops, 2009, pp. 1-9.  
[8] L. F. Bittencourt. “Scheduling Service Workflows for 
Cost Optimization in Hybrid Clouds,” in Proc. 
International Conference, Network and Service 
Management, 2010, pp. 394-397. 
[9] Y. O. Yazır, C. Matthews, R. Farahbod, S. Neville, A. 
Guitouni, S. Ganti, and Y. Coady. “Dynamic Resource 
Allocation in Computing Clouds using Distributed 
Multiple Criteria Decision Analysis,” in Proc. IEEE 3rd 
International Conference on Cloud Computing, 2010, pp. 
91-98. 
[10] Wikipedia. “Apache Hadoop.” Internet: 
http://zh.wikipedia.org/wiki/Hadoop [Dec. 30, 2011] 
[11] J. Dean and S. Ghemawat. “MapReduce: Simplied data 
processing on large clusters.” Communications of the 
ACM, vol. 51, pp. 107-113, Jan. 2008.  
[12] M. Tim. “Scheduling in Hadoop.” Internet: 
http://www.ibm.com/developerworks/linux/library/os-
hadoop-scheduling/index.html?ca=drs- [Dec. 6, 2011] 
[13] C. Tian, H. Zhou, Y. He, and L. Zha. “A Dynamic 
MapReduce Scheduler for Heterogeneous Workloads,” 
in Proc. 8th International Conference on Grid and 
Cooperative Computing, 2009, pp. 218-224. 
[14] E. Rosti, G. Serazzi, E. Smirni, and M.S. Squillante. 
“Models of Parallel Applications with Large 
Computation and I/O Requirements.” IEEE Transaction 
on Software Engineering, vol. 28, pp. 286-307, Mar. 
2002. 
[15] E. Rosti, G. Serazzi, E. Smirni, and M.S. Squillante. 
“The Impact of I/O on Program Behavior and Parallel 
Scheduling,” in Proc. SIGMETRICS Joint Conference 
on Measurement and Modeling of Computing Systems, 
1998, pp. 56-65 
[16] W. Lee, M. Frank, V. Lee, K. Mackenzie and L. 
Rudolph, “Implications of I/O for Gang Scheduled 
Workloads.” Lecture Notes in Computer Science, vol. 
1291, pp. 215-237, 1997. 
[17] Y. Wiseman. “Paired Gang Scheduling.” IEEE 
Transactions on Parallel and Distributed System, vol. 
14, pp. 581-592, Jun. 2003. 
[18] W. Su and W. Pan. “An Adaptive Task Allocation 
Approach for MapReduce in a Heterogeneous Cloud,” 
in Proc. 3rd International Conference on Computational 
Intelligence, Communication Systems and Networks, 
2011, pp. 287-291. 
[19] J. Wu. “Real-time scheduling of CPU-bound and I/O-
bound processes,” in Proc. Real-Time Computing 
Systems and Applications, 1999, pp. 303-310. 50
 2
三、 建議 
此行在澳洲的行程，因為是今年度第 2篇發表的論文，所以雖然國科會計畫仍有補助部分差旅
費，但是並不足以涵蓋本次會議的花費。由於發表會議論文一定要出國報告而且能夠就有機會與
國外學者交流，因此國科會是否可考量在特定領域之指標性會議可以再向國合處申請相關經費以
鼓勵老師出國發表論文。 
 
四、 攜回資料名稱及內容 
1. IEEE PIMRC 2012 會議議程一本    (Program) 
2. IEEE PIMRC 2012 論文集光碟一片 (Proceeding) 
 
五、 其他 
 
derive a secure routing path while considering system 
performance. 
The rest of the paper is organized as follows. In Section 2, 
some related works about network security are mentioned. 
Then, the proposed system, Cross-Layer Network Security 
Evaluation is described in Section 3 and a Secure Dynamic 
Routing Protocol based on Cross-Layer Network Security 
Evaluation is proposed in Section 4. Next, some simulation 
results are presented in Section 5. Finally, the paper is 
concluded in Section 6. 
II. RELATED WORKS 
A. Network Security Evaluation 
To evaluate network security, there are four security 
elements to be discussed: threats, vulnerabilities, 
countermeasures and security requirements. Vulnerabilities are 
threatened by threats. Countermeasures are applied to resist 
attacks from threats while security requirements represent the 
actual demands of security protection.  
A probabilistic-based system security evaluation approach is 
proposed in [7]. In this approach, vulnerabilities are assessed 
according to their probability. However, the countermeasures 
and security requirements are not considered. Another system 
security evaluation which adopts a decision-tree based 
approach to quantify system risk is proposed in [8]. In this 
approach, authors describe a general-purpose evaluation model 
which simultaneously consider about vulnerabilities, threats 
and countermeasures. However, the lack of security 
requirement consideration makes it impractical to real network 
environments. 
More vulnerability evaluation or threat analysis systems [9] 
for system security are enumerated in [10]. Most of these 
network security evaluations talk about the system security 
rather than communication security. Besides, none of them 
consider all the four security elements simultaneously. 
B. CVSS 
CVSS [1] is proposed by the US National Infrastructure 
Assurance Council (NIAC) to evaluate system vulnerabilities. 
In CVSS, score are derived from three metrics based metric, 
temporal metric and environmental metric, which are described 
as follows. 
1) Base Metric: for a vulnerability, the metric represents the 
attributes that will not change over time or in different 
environments. For example, how the vulnerability is 
exploited, the complexity of the attack required to exploit 
the vulnerability, the number of times an attacker must 
authenticate to exploit a vulnerability, and the impact of on 
confidentiality, integrity and availability, are included in 
the base metrics. 
2) Temporal Metric: for a vulnerability, the metric measures 
the attributes that might change over time, such as the 
current state of exploit techniques, the remediation level of 
a vulnerability, and the degree of confidence in the 
existence of the vulnerability, are temporal metrics. 
3) Environmental Metric: for a vulnerability, the metric 
measures the attributes that might change in different 
environments. For example, operation system type, data 
asset values, and security requirements, are environmental 
metrics. 
By combining the above metrics, the severity level of system 
vulnerability is evaluated. However, as CVSS is designed only 
for system vulnerability, it does not consider about threats and 
communication security. Some other research applies CVSS to 
evaluate security [5] or tries to improve CVSS [6]. However, all 
of them are applied to evaluate system security. 
C. Routing Security 
Distributed Dynamic Routing Algorithm (DDRA) [3] is an 
approach proposed for communication security. By using the 
multi-path routing technique, DDRA decreases the probability 
of that the communication between source and destination is 
eavesdropped by routers. The approach assumes that attacks 
can be triggered only by the compromised routers. In fact, most 
attacks are launched by outsider attackers, nevertheless. The 
approach proposed in [4] adopts similar concept. 
Several cross-layer design security evaluation approaches 
[11], [12] are proposed to assess network security level. By 
adopting the cross-layer concept, the security countermeasures 
applied over different layers are well considered in network 
security evaluation. Thus, the cross-layer evaluation 
approaches provide more accurate result of security level. Thus, 
we adopt cross-layer design concept to evaluate network 
security in the paper. 
 Fig 2 Cross-Layer Threat Danger Intensity Evaluation Flow 
III. CROSS-LAYER NETWORK SECURITY EVALUATION 
In this section, the proposed Cross-Layer Network Security 
Evaluation (CNSE) is introduced. There are two major steps in 
CNSE. Firstly Cross-Layer Threat Danger Intensity Evaluation 
(CTDIE) is applied to access the danger intensity of individual 
threat in a link. Then, Threat-based Link Risk Evaluation 
(TLRE) is applied to derive a Link Risk Score which indicates 
the security level of a link by considering the danger intensity 
of all threats on the link.  
304
influenced vulnerability (IV) in ITP and application-based 
security requirement (SR) in UTP, is described as follows. 
z Influenced Vulnerability: We define the network 
vulnerability as short of the protection for a specific 
security objective in the network. For example, short of 
confidentiality protection is one kind of vulnerability. 
There are several definitions made for security objectives. 
In X.800 [13], the main security objectives are 
authentication, access control, confidentiality, integrity, 
non-repudiation. In TIPHON [14], the security objectives 
include confidentiality, integrity, accountability, 
availability, and non-repudiation. In the paper, the network 
vulnerabilities are defined as short of protection for the 
security objectives including authentication, 
confidentiality, integrity, non-repudiation, and availability. 
A complete influenced vulnerability would be assigned a 
higher score which indicates it is more insecure. 
z Application-based Security Requirement: In the paper, 
the security requirement is defined as the demand of a 
specific security objective. Various applications might 
have different security requirements. For example, to 
provide confidentiality protection of personal credit card 
information is the most important requirement in 
E-Commerce. We have to analyze the property of an 
application to find its actual requirements. And then, we 
can pay more attention to protecting the critical security 
objective. The more attention we pay to a security 
objective, the higher score it should be assigned. 
Obtaining the subjective metrics is typically done by security 
professionals and is time-consuming. Thus, the advantage of 
representing a threat property as an objective metric is that it is 
easily derived in an automatic way. For examples, attack record 
(AR) and countermeasure (CM) in ETP can be derived as 
follows. 
z Attack Record: Attacked records represent the 
trustworthy level of a relay router. If a relay router is 
attacked before, it might be attacked again. We use the 
following formula to assess the trustworthy level of relay 
router at time t 
atttARtAR uu )1()1()( OO                                    (1) 
where O  is the weighted value to adjust the historical 
influence, and att represents the numbers of attack in the 
time period. Relay router will update the AR value 
periodically. 
z Cross-Layer Counter-Measure: Countermeasures are 
applied to protect security objectives from threats. In 
X.800, countermeasures are classified into several 
categories, which are key management, encipherment, 
digital signature, access control, data integrity, 
authentication, traffic padding, routing control, and 
notarization. For cryptographic-based mechanisms, the 
concept of cracking year proposed in [12] is adopted to 
evaluate the strength of a countermeasure. By employing 
the Infeasible Key Size function proposed in [15], we can 
derive the time when a key will be cracked and further use 
it to represent the strength of the countermeasure.  
3) Threat Danger Intensity Evaluation:  
After User Device obtains the threat profiles, Threat Danger 
Intensity Evaluator is launched to derive the threat exploit 
possibility (EP) in relay router R at time t by: 
Rthreat NStARDDADAOAPtREP xx )()(),( GJED   (2) 
1 w GJE                    
(3) 
where GJED ,,,  are the weighted values of exploit possibility 
parameters in OTP. )(tAR  is the attacked record at time t 
derived as (1). RNS  is the network status of relay router R. 
Furthermore, the threat impact in router R at time t for user U 
can be derived as follows: 
 ¦ 
 
V
UUVRVV
threat
IRAssetSRtCMIV
UtRimpact
,, )(
),,(
                              (4) 
¦  
V
UVSR 1,                        (5) 
where V represents the security objectives in the network since 
Influenced Vulnerability, Countermeasure, and Security 
Requirements are all related to security objectives. For a threat, 
VIV  indicates the level how security objective V is influenced 
by the threat;  tCM RV ,  represents the strength of 
countermeasure to defend the threat for security object V in 
router R at time t, and UVSR ,  can be denoted as the required 
protection of security objective V for the user U. 
Finally, the Threat Danger Intensity Evaluation Metric is 
defined as follows: 
 ),,(),(),,( UtRimpacttREPUtRDI threatthreatthreat           (6) 
where ),,( UtRDIthreat  is the danger intensity of the threat in 
relay router R, at time t, for the user U. As the danger intensity 
will be applied to derive the link risk, the higher score indicates 
higher danger.  
B. Threat-based Link Risk Evaluation 
After obtaining the danger intensity (DI) of all threats, TLRE 
is employed to evaluate the risk of transmitting data on a link. 
Since it is obvious that the link risk is significantly decided by 
the most dangerous threat on this link, TLRE derives the Link 
Risk Score (LR) by 
}:{ DBinlistthreatinthreatsDIMAXLR threatlink              (7) 
IV. SECURE DYNAMIC ROUTING PROTOCOL 
As in Fig 3, we assume that there is a Layer-based Threat 
Profile DB provided by the trusted third party. Every Router 
will query the latest Original Threat Profile from the DB. 
According to the environment status of routers, they will 
provide their Environment Threat Profile to every user. Users 
will then derive the link risk scores of all links and decide a 
secure path by employing the Secure Dynamic Routing 
Protocol. 
306
indicates there is no danger link in the path.  
 
Table IV. Results of SDRP with Security Consideration 
 RPS RRS MLR connect 
OSPF 0.4 1.2 0.8 100% 
B-SDRP 0.9 0.8 0.6 100% 
S-SDRP 0.8 0.9 0.5 90% 
E-SDRP 0.9 1 0.3 100% 
 
S D
R1
R2
R3
R4
R5
R6
R7
R8
R9
R10
0.2 / 0.3
0.2 / 0.2
0.3 / 0.2
0.3 / 0.2
0.5 / 0.3
0.3 / 0.2
0.2 / 0.3
0.9 / 0.2
0.1 / 0.3 0.1 / 0.3
0.6 / 0.3
0.7 / 0.3 0.6 / 0.3
0.3 / 0.1
0.3 / 0.2 0.3 / 0.2
0.2 / 0.2
OSPF
B-SDRP
S-SDRP
E-SDRP
Fig 5. The Derived Paths of SDRP with Security Consideration 
C. Simulation Results with Security and Performance 
Consideration  
The simulation results are represented in Table V and Fig6. 
As shown in Table V, the RPS and RRS in both B-SDRP and 
S-SDRP is 0.7 and 1.1, respectively. It means they select a path 
more efficient than they originally do. Although, the RRS is 
lower with considering performance, it is still better than OSPF. 
  
Table V. Results of SDRP with Security and Performance Consideration 
 RPS RRS MLR connect 
B-SDRP 0.7 1.1 0.3 100% 
S-SDRP 0.7 1.1 0.3 90% 
E-SDRP 0.9 1 0.3 100% 
 
S D
R1
R2
R3
R4
R5
R6
R7
R8
R9
R10
0.2 / 0.3
0.2 / 0.2
0.3 / 0.2
0.3 / 0.2
0.5 / 0.3
0.3 / 0.2
0.2 / 0.3
0.9 / 0.2
0.1 / 0.3 0.1 / 0.3
0.6 / 0.3
0.7 / 0.3
0.6 / 0.3
0.3 / 0.1
0.3 / 0.2 0.3 / 0.2
0.2 / 0.2
B-SDRP
S-SDRP
E-SDRP
 Fig 6. The Derived Paths of SDRP with Security and Performance 
Consideration 
VI. CONCLUSIONS 
In the paper, the Secure Dynamic Routing Protocols based 
on Cross-Layer Network Security Evaluation are proposed. By 
analyzing the properties of various threats, threat danger 
intensity is evaluated and quantified through our Cross-Layer 
Threat Danger Intensity Evaluation (CTDIE). Then, the 
transmission risk on a link can be derived by our Threat-based 
Link Risk Evaluation (TLRE) according to the danger intensity 
of threats. Base on this comprehensive network security 
evaluation, we propose several Secure Dynamic Routing 
Protocols due to various requirements. By providing a secure 
routing path, the communication security can be retained.  The 
simulation results show that we can obtain a secure routing path 
by the proposed SDRP even with the performance 
consideration. In the future, we will try to improve the 
evaluation framework by integrating system security evaluation. 
By the evolved evaluation, a routing path with complete 
security consideration can be obtained.  
ACKNOWLEDGMENTS 
This paper is based partially on work supported by the 
National Science Council (NSC) of Taiwan, R.O.C., under 
grant No. NSC98-2221-E-006-222-MY3 and NSC100-221
8-E-156-001. 
REFERENCES 
[1] M. Peter, S. Karen, and R. Sasha, “Common Vulnerability Scoring 
System,” IEEE Security and Privacy Magazine 2006 
[2] P. Dan, Z. Lixia, D. Massey, “A Framework for Resilient Internet 
Routing Protocols,” IEEE Network Magazine, 2004 
[3] K. Chin-Fu, P. Ai-Chun, C. Sheng-Kun, “Dynamic Routing with Security 
Considerations,” IEEE Transactions on Parallel and Distributed Systems, 
2009  
[4] P. Tague, D. Slater, J. Rogers, and R. Poovendran, “Evaluating the 
Vulnerability of Network Traffic Using Joint Security and Routing 
Analysis,” IEEE Transactions on Dependable and Secure Computing, 
2009 
[5] S. Petajasoja, H. Kortti, A. Takanen, and J. Tirila, “IMS Threat and 
Attack Surface Analysis Using Common Vulnerability Scoring System,” 
IEEE 35th Annual Computer Software and Applications Conference 
Workshops (COMPSACW), 2011 
[6] M. Peter, S. Karen, “Improving the Common Vulnerability Scoring 
System,” IET Information Security, 2007 
[7] M.S. Ahmed, E. Al-Shaer, L. Khan, “A Novel Quantitative Approach For 
Measuring Network Security,” IEEE INFOCOM 2008 
[8] M. Sahinoglu, “Security meter: a practical decision-tree model to quantify 
Risk,” IEEE Security and Privacy Magazine, 2005 
[9] C. Alberts, A. Dorofee, J.Stevens, C. Woody, "OCTAVE Method 
Implementation Guide", CERT Coordination Centre, Software 
Engineering Institute, Carnegie Mellon Institute, 
2001-2003, http://www.cert.org/octave/ 
[10] N.R. Prasad, “Threat Model Framework and Methodology for Personal 
Networks (PNs),” International Conference on Communication Systems 
Software and Middleware, 2007. COMSWARE 2007. 
[11] Avesh K. Agarwal, Wenye Wang and Janise Y. McNair, "An 
Experimental Study of Cross-Layer Security Protocols in Public Access 
Wireless Networks," IEEE GLOBECOM 2005 
[12] I-Hsun Chuang,  Chou-Ting Hsieh, and  Yau-Hwang Kuo, “An Adaptive 
Cross-Layer Design Approach for Network Security Management,” 
International Conference on Advanced Communication Technology 
(ICACT), 2011  
[13] The International Telegraph and Telephone Consultative Committee, 
“Security Architecture for Open Systems Interconnection for CCITT 
Applications Recommendation X.800,”  
[14] ETSI. "Telecommunications and internet protocol harmonization over 
networks (TIPHON) release 4; protocol framework definition; methods 
and protocols for security; part 1: Threat analysis", Technical 
Specification ETSI TS 102 165-1 V4.61.v1, 2003, 
[15] Arjen K, Lenstra, and Eric R. Verheul, “Selecting Cryptographic Key 
Sizes,” Journal of Cryptology, vol. 14, pp. 255-293, 1999 
308
100年度專題研究計畫研究成果彙整表 
計畫主持人：蘇維宗 計畫編號：100-2218-E-156-001- 
計畫名稱：具備多層次用戶情境感知、安全及服務品質協調能力之智慧型雲端服務仲介器閘道開發,--
子計畫四：開發雲端仲介器閘道之環境感知資料加密與分散服務(I) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 3 3 100% 
需求規格與專案
計畫、測試計畫與
測試報告、結案報
告 
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 1 100% 吳孫銘(目前服役中) 
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
詳見出席國際會
議報告 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 1 100% 黃傑翔(目前為碩士班二年級) 
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
