1 
 
行政院國家科學委員會專題研究計畫成果報告 
一個支援偵測跨語言與單一語言文件抄襲與相關性量測平台的開發研究 
Study on Development of a Unified System Platform for Supporting Plagiarism Detection 
and Relatedness Evaluation of Cross-lingual and Monolingual Texts 
計畫編號：NSC 97－2221－E－151－036 
執行期間：97 年 8 月 1 日至 98 年 7 月 31 日 
主持人：李俊宏 副教授 國立高雄應用科技大學電機工程系 
計畫參與人員：博士班研究生兼任助理：吳志鴻 碩士班研究生兼任助理：藍翊嘉、 
王士豪(國立高雄應用科技大學電機工程系博士班、碩士班) 
 
一、 中英文摘要 
 
本研究計畫開發出一個支援跨語言與
單一語言文件抄襲偵測與文件相關性量測
平台，此系統可以有效地學習由不同類型
文件中萃取出來的特徵，同時我們開發一
個支援數種異質性文件資料共構的向量空
間模型，希望藉此能夠克服多國語言文件
向量的複雜性，使其呈現在共同的向量空
間上，進而透過向量距離測量計算來實現
文件相關性評估與抄襲偵測的工作。實驗
結果顯示以SVM進行分類已達到可接受的
分類準確率，由此也證明本平台可有效的
應用於跨語言文件相關性量測與抄襲偵
測。透過設計一個『文件引用合理性』預
選機制，提供其他研究人員能快速有效的
找尋到與其研究領域或專業知識相關的文
件列表，做為『文件抄襲偵測與分析』或
『相關研究文件評估』的應用。 
 
關鍵詞：跨語言文件檢索, 文件探勘, 文件
檢索, 機器學習, 文件抄襲偵測 
 
Abstract 
 
In this project we developed an unified 
system platform for supporting plagiarism 
detection and relatedness evaluation of cross-
lingual and monolingual texts. This system 
can be used to learn the features extracted 
from several different types of texts 
effectively. Meanwhile, we developed an 
unified vector space model for supporting 
various heterogeneous types of text 
representation, in order to overcome the 
complexity of vectors of multilingual texts 
represented in a common vector space. Thus, 
we can utilize the vector system to measure 
the distance of vectors for implementing 
relatedness evaluation and plagiarism 
detection among texts. As experimental results, 
our platform is already attached an acceptable 
classification accuracy by performing SVM 
method; it is also means our platform can 
applied to cross-lingual text relatedness 
evaluation and plagiarism detection 
effectively. In addition, we also designed a 
pre-selecting mechanism for evaluating “the 
reasonability of document citation”, allowing 
other researchers to quickly find the lists of 
documents associated with their research 
fields or professional knowledge, and further 
apply them to perform the “text plagiarism 
detection and analysis “ or “relatedness 
evaluation of research documents”. 
 
Keywords: Cross-lingual text retrieval, Text 
mining, Text retrieval, Machine learning, Text 
3 
 
detection)的分析。在以文件段落為單位做
為文件抄襲偵測的研究部份，White等[35]
使用以句子段落為基礎(Sentence-Based)的
自然語言方法來做為文件抄襲的偵測。 
 
 Authorship identification 
 
Authorship identification是為了找出文
章真正的作者或是找誰是仿冒者，一些文
學及語言的研究裡也可稱為Authorship 
attribution，一般作法是在已給定特性條件
下之著作文章中，去辨識新進文章的作者
歸屬。 
Baayen[2]等人證明語法特性萃取比詞
彙特性萃取來的更有效率。 Koppel 與
Schler[13]證明在單獨對lexical features 和 
POS features下SVM準確性是優於DT，但
若是在考慮混合 features時，則DT優於
SVM。另外，單一特性萃取只針對特定文
章較有效率，增加特性萃取的種類也是種
增加準確率的方式，而透過文章標題方式
來區分如George與Eleni[6]提出文章標題對
著作歸屬的影響，以兩位作者在1997-2006
之間所寫文章中取兩種主題(政治、文化)共
200篇做研究，證明主題及作者的關係若是
在相同特性集合裡，可以較準確辨識出
來。Rong等人[31]提出線上文件裡各種寫
作風格的分類架構，將線上文件切成四種
特 性 (lexical 、 syntactic, 、 structural 、
content-specific features)，文件內容以兩種
語言做分析(英文、中文)，並使用三種分類
方法(DT、neural networks、SVM)測試，證
明特性萃取種類增加可以增進輸出的準確
率。 
 
(2) 語意相關性量測 
 
在相關性量測的研究中，Longman 
Dictionary of Contemporary English(LDOCE)
可 視 為 一 知 識 來 源 。 其 中 Kozima 與
Furugori[14]將headwords與其定義轉變成一
個網路模型，比較兩詞彙間相同相鄰結點
個數來計算他們之間的相關程度。Kozima
與 Ito[15]將LDV中的字彙轉化成向量表
示，計算向量與向量之間的距離，量測字
彙之間的語意相關程度。 
若以Peter Mark Roget 創造的Thesaurus
為知識來源，詞彙將依照其意義及語意相
關性分成數個群組。Hirst與Morris[8]將英
文Thesaurus中辭彙與詞彙之間的語意相關
性制定了五種型態，利用這五種型態來量
測 辭 彙 與 詞 彙 間 的 語 意 相 關 程 度 。
Okumura與Honda[27]將[8]的演算法應用在
日文的辭彙上。 
WordNet與MeSH為知識來源，藉由計
算所屬辭彙在語意網路中的節點位置(node)
與另一辭彙的節點位置 (node) 的距離
(semantic distance)來決定語意相近的程
度；這方面的研究方法有簡單的 edge-
counting[28] ， relative depth[16][33] 與
density[1]都是藉由語意網路如WordNet為主
要的知識來源。 
Resnik[30]利用特定語料庫作為知識來
源 ， 並 依 照 兩 個 相 似 的 概 念 (shared 
information，information content)建立字彙
的階層相關模型，計算字彙間的語意相關
程度。Lin[26]提出Similarity Theorem計算
兩字彙間語意相關程度，Jiang與Conrath[12]
利用類似Lin的方法在量測語意相關性。 
由於上述的幾種模式，因為所根據的
知識來源不同，無法判定何種模式成效較
高。所以Hirst[9]的研究針對WordNet知識
來源的五種演算法 (Hirst-St-Onge[10] 、
Leacock-Chodorow[16]、Resnik[30]、Jiang-
Conrath[12]與Lin[26])作一系列實驗與比
較。幾乎所有量測語意距離的相關計畫研
究中，必須依賴在嚴謹限制的辭彙知識來
源，而且大多只限於字(詞)彙間的語意相關
性量測，無法對文件與文件之間的語意相
關性作量測。因此，本計畫不使用上述特
殊網路模型架構，提出類別語意向量模
型，直接計算語意向量之間量測值代表文
5 
 
表4.1 文件配置 
類 別 數量(篇) 
已標註 中文 500 英文 500 
未標註 中文 100 英文 100 
 
輸入層：輸入層的工作主要是處理受
測文件輸入時的前置處理工作，計畫中文
件檢索與資訊過濾前置作業所採用之資訊
檢索技術為『Vector Space Model』；Vector 
Space Model (VSM) 的文件檢索模型是一
種古典而常用的資訊檢索方法，如圖4.2 
Vector Space Model示意圖所示，向量空間
模型是由詞彙(Term)與文件(Document)所構
成的矩陣，在此模型中詞彙與文件的均透
過向量的方式呈現，而詞彙與文件之間也
可以透過不同的權重值(Weight)來描述其重
要程度，亦可以藉由向量化轉化的計算來
獲得文件的向量空間模型，有利於分類器
的處理工作。 
 
圖4.2 Vector Space Model示意圖 
 
偵測層(Detection Layer)：偵測層的工
作主要是建立一個以針對抄襲偵測所開發
的分類器為基礎的平台 (Classifier-based 
Platform for Plagiarism Detection)，本計畫
主要是利用支持向量機 (Support Vector 
Machines)來進行本階段的工作；首先，不
同的領域知識在經過前置處理後被映射到
SVM中。接著，未知類別的文件經分類器
分類後所得的結果被轉化配製成新的文件
向量，此文件向量被用來當作是特定文件
的語意向量並用以評估與其他文件間的語
意相關度。利用SVM分類器最佳化演算
法，藉由調整所有訓練文件之權重值求得
最佳決策函數，即最佳分割超平面，在訓
練完成後將所有訓練文件之權重值輸出作
為測試階段決策函數建立之參數。 
評估層(Evaluation Layer)：本計畫透
過多類別分類的架構將原始文件詞彙特徵
轉變成文件間類別語意向量，藉由分群演
算法量測兩篇文件的類別語意向量，並透
過量化的方式呈現其語意相關程度；本計
畫利用自組織映射圖(Self-organizing map)
來量測文件向量間的相近程度。 
 
五、實驗結果與討論 
 
本計畫以Radial Basis Function (RBF) 
SVM進行分類並利用SOM來評估與其他文
件間的相關性。在RBF SVM的訓練階段，
本計畫透過參數設定(如表 5.1 RBF SVM 
參數設定所示)來調整所有訓練文件中的權
重值，並在訓練完成後將所有文件的權重
值輸出做為測試階段決策函數建立的參
數。接著，利用SOM來量測由SVM所產生
的決策結果，並以量化的結果呈現語意相
關程度。在實驗部分，本計畫以Linear 
SVM及Polynomial SVM作為對照組與RBF 
SVM進行比較，並透過Recall(檢出率)，
Precision(精確率)，與F1測量值來評估三者
的分類效能，實驗結果如圖 5.1 至圖 5.3 
所示；而相關性量測階段則透過量測SOM
的分群準確率(Accuracy)來做為兩文件之間
的相似程度，並與K平均(K-means)比較，
實驗結果如圖 5.4所示。 
表 5.1 RBF SVM參數設定 
核心函數參數(Gamma) 0.0025 
調和係數(C) 1 
7 
 
關性』(semantic relatedness)的研究，另一
個易被忽略的功能-『找出與自己研究領域
相關的研究者及其發表的學術文章』也可
透過該平台實現，並發掘出在概念上或想
法上相關的研究文獻，產生良性的學術互
動，而非只能提供競爭抄襲行為的偵測功
能。相關研究成果的學術或應用已發表期
刊論文1篇及會議論文7篇，請參照參考文
獻[17][18][19][20][21][23][24][25]。 
 
七、參考文獻 
[1] Agirre, E. and Rigau, G., (1996) "An 
Experiment in Word Sense 
Disambiguation of the Brown Corpus 
Using WordNet,” Memoranda in 
Computer and Cognitive Science, 
MCCS-96-291, Computing Research 
Laboratory, Mexico,1996 
[2] Baayen, R.H., et al, (2002) "An 
experiment in authorship attribution" In 
Proceedings of the 6th International 
Conference on the Statistical Analysis of 
Textual Data,UK, 2002 
[3] Stein, B., and Eissen, S., (2007) 
"Intrinsic Plagiarism Analysis with Meta 
Learning" In Benno Stein, Moshe 
Koppel, and Efstathios Stamatatos, 
editors, In Proceedings of SIGIR 
Workshop on Plagiarism Analysis, 
Authorship Identification, and Near-
Duplicate Detection (PAN 07), 2007. 
[4] Deerwester S., Dumais S.T., Furnas 
G.W., Landauer T.K., and Harshman R., 
(1990) "Indexing by latent semantic 
analysis,” Journal of the American 
Society for Information Science, vol.41, 
no.6, p.p.391-407,1990. 
[5] Eissen, S., Benno, S., and Marion, K., 
(2007) "Plagiarism Detection without 
Reference Colletions,” Decker and Lenz 
(Eds.): Advances in Data Analysis 
Selected Papers from the 30th Annual 
Conference of the German Classification 
Society (GfKl) Berlin, p.p. 359-366, 
Springer, 2007. (ISBN 978-3-540-
70980-0)  
[6] George, K.M. and Eleni, K.A.（2007）
"Investigating topic influence in 
authorship attribution,” Proceedings of 
SIGIR '07 Workshop ： Plagiarism 
Analysis, Authorship Identification, and 
Near-Duplicate Detection,2007, p.p. 29-
35 
[7] Gliozzo, A. and Strapparava, C.,(2005) 
"Cross-Language Text Categorization by 
acquiring Multilingual Domain from 
Comparable Corpora,” Proceedings of 
the ACL2005 Workshop on Building and 
Using Parallel Texts on 2005, p.p. 9-16. 
[8] Hirst ,G. and Morris, J.(1991) "Lexical 
Cohesion Computed by Thesaural 
Relations as an Indicator of the Structure 
of Text,” In Computational Linguistics, 
1991, vol. 17, no. 1, p.p. 21–48, 1991. 
[9] Hirst, G. and Budanitsky, A., (2005) 
"Correcting Real-Word Spelling Errors 
by Restoring Lexical Cohesion" Natural 
Language Engineering, Canada, 2005. 
vol. 11, p.p. 87-111, 2005. 
[10] Hirst, G. and St-Onge, D.,(1998) 
"Lexical Chains as Representations of 
Context for the Detection and Correction 
of Malapropisms,” WordNet: An 
electronic lexical database, USA, p.p. 
305–332,1998(MIT Press). 
[11] Hoad, T.C. and Zobel, J., (2003) 
"Methods for identifying versioned and 
plagiarized documents,” Journal of the 
American Society for Information 
Science and Technology, vol.54, no.3, 
p.p. 203–215, 2003. 
[12] Jiang, J.J. and Conrath, D.W., (1997) 
"Semantic Similarity based on Corpus 
Statistics and Lexical Taxonomy," In 
