materials or generalized binary trees. 
The number of narrowed binary trees grows 
exponentially with respect to the depth of the trees. 
Let Tn denotes the total number of distinct narrowed 
binary trees with depth n or less, then T0 =1 and 
Tn+1 = Tn2+1 for n > 0. Out preliminary study has 
completed the analysis of all the narrowed binary 
trees with depth 4 or less（T4 = 677）, and found out 
that all these trees can be expressed as a linear sum 
of 5 bases. This project will continue to explore 
deeper narrowed binary trees, and expect to complete 
the analysis of all the narrowed binary trees with 
depth 5 or less（T5 = 458,330）.  
 
英文關鍵詞： combinatorial games, binary tree, computer game, 
artificial intelligence. 
 
Kuo-Yuan Kao, I-Chen Wu, Yi-Chang Shan, 2011, Decomposition of Simple All Small 
games, Proceedings of the 28th Workshop on Combinatorial Mathematics and 
Computation Theory 
原文如下 
Decomposition of Simple All Small Games  
 
Abstract 
 
Simple all small games are games in which both players have exactly one option from every local 
non-terminal position. Each simple all small game born on day n corresponds to a binary tree with 
depth n where each none-terminal node has exactly two child nodes. We focus our study here on simple 
all small games with integer valued atomic weights. An all small game G is called a molecule if G has 
integer valued atomic weight. We found out there are 25 distinct molecules born on or before day 5, 
moreover, all the molecules can be decomposed as a sum of 13 elementary games with atomic weight 1 
or 0. 
 
 
1  Introduction 
 
We assume our reader is familiar with the basics of combinatorial game theory as presented in 
Winning Ways[1] or On Numbers and Games[2]. Specifically, our reader should be familiar with the 
following: the abstract definitions of the group of games, the partial order relation of games, the 
canonical form of games, all small games like the star(*), up(↑), and down(↓), and the atomic weight 
calculus. 
 
We will focus our discussion here on all small games in which both players have exactly one option 
from every local non-terminal position. These games are named simple all small games. Since both 
players have exactly one option from every local non-terminal position, each simple all small game 
corresponds to a binary tree within which each node is either a terminal node or a node with exactly 
one left and one right branch. For example, * ={0|0} and ↑={0|*} are simple all small games because 
each player has exactly one option from every local non-terminal position. As a counter example, *2 = 
{0, * | 0, *} is not a simple all small game, because the players have more than one option from *2.  
 
An all small game G is called a molecule if G has integer valued atomic weight, and an atom if G has 
atomic weight one. Many molecules can be decomposed as sums of atoms and *. One famous example 
is the up-star equality: {0|↑} =↑+↑+ *, note that {0|↑} is a molecule with atomic weight 2, the 
right-hand side is a sum of atoms ↑ and *. In this article, we shall examine all the simple molecules 
born on or before day 5, and check whether these molecules can be decomposed as a sum of atoms and 
star. 
 
The remaining part of this section introduces the notations and conventions used in this article. 
 
Let G be a game. We use the notation [G] to denote the atomic weight of G. 
 [ G ]   t h e  a t o m i c  w e i g h t  o f  G … … … . ( 1 ) ⇔
▽L(G) and ▽R(G) denote the Left and the Right incentive of G. 
 ▽ L ( G )  =  G  L  –  G … … … … … … . . . … … … . ( 2 ) 
▽ R ( G )  =  G  –  G  R … … … … … … … … … . ( 3 ) 
(17) and (18) implies twin-ups are well ordered. 
 
Definition 3: (star atom) 
An atom A is called a star atom if AR = *, and ▽R(A) > ▽L(A). 
Clearly, ups and twin-ups are star atoms. 
Definition 4: (monotonic molecule and maximum atom) 
A molecule M is monotonic if M can be written as: 
 M = +c∑
=i
ii Ac
1
.
n
0.*, ………..…….….(20) 
where A1 < A2 < … < An are star atoms, cn > 0, c0 = 0 or 1. An is called the maximum atom of M. 
 
Note that if M is monotonic then –M is not monotonic, because we require cn > 0. 
Since ▽R(An) > ▽R(Ak) > ▽R(–Ak), for k <n, we have ▽R(M) =▽R(An). Also since ▽R(An) > ▽L(An) 
and ▽R(An) > ▽L(–Ak)> ▽L(Ak), for k <n, We have ▽R(An) > ▽L(M). The next proposition 
summarizes the result. 
 
Proposition 3: (maximum atom incentive) 
If M be a monotonic molecule with maximum atom A, then  
▽R(M) =▽R(A) > ▽L(M). …..……….(21) 
According to proposition 3, when playing a sum of ordered star atoms, one should remove the 
opponent’s largest atom unless he has a larger atom. The next theorem is an important tool to analyze 
molecules. 
 
Theorem: (molecule calculus) 
If GL≧ 0, GR |> 0 is a monotonic molecule with maximum atom m, and  
G ≧ GR + m + *, 
then  
G = GR + x + *,………..………....….….(22) 
where 
x = {mL, GL－GR + * | *}.………..….….(23) 
Proof: We want show that the first player cannot win the sum S = GR + x + *－G.. 
(a) L cannot win S by a move from –G to –GR, because R can follow a move from x to *. 
(b) L cannot win S by a move from x to GL－GR + *, because R can follow a move from –G to –GL. 
(c) L cannot win S by a move from GR. By proposition 4, ▽L(GR) < ▽R(m) = m + * ≦ G－GR 
=▽R(G) = ▽L(－G)), so a move from GR will not be better than a move from –G. 
(d) L cannot win S by a move from x to mL, because mL－x ≦ mL－m = ▽L(m), so the move will 
not be better than a move from GR to GR－m + mL. 
(e) R cannot win S by a move from x to *, because L can follow a move from –G to –GR. 
(f) R cannot win S by a move from –G. to –GL, because L can follow a move from x to GL－GR + *. 
(g) R cannot win S by a move from GR, because ▽R(GR) = ▽R(m) = m+* ≦ x + * = ▽R(x), (x≧m), 
so the move will not be better than a move from x to *. 
Thus, S = 0 and G = GR + x + *.  
Moreover, if [GL－GR] ≦ 2 then [x] = 1, thus G is a monotonic molecule with maximum star atom x. 
□ 
GL\GR 0 * ↑ 
0 ～ ～ ～ 
↑ ～ ～ ～ 
↑2 {↑2|0} {↑2|*} {↑2|↑} 
↑↑* {↑↑*|0} {↑↑*|*} {↑↑*|↑} 
 
GL\GR ↑2 ↑↑* 
0 {0|↑2} {0|↑↑*} 
↑ {↑|↑2} {↑|↑↑*} 
↑2 {↑2|↑2} {↑2|↑↑*} 
↑↑* {↑↑*|↑2} {↑↑*|↑↑*} ～ denotes games born before day 4. 
By replacing with reversible move, one can get:  
{↑2 |0} = *, 
{↑↑*|*} = ↑. 
The molecule calculus can be applied to the columns GR = ↑, ↑2, and ↑↑*. We have 
 {↑2|↑}  = ↑↑*. 
 {↑↑*|↑}  = ↑2↑*. 
 {0|↑2} = ↑2↑2* 
{↑|↑2}  = ↑2↑2* 
{↑2|↑2} = ↑2↑2* 
{↑↑*|↑2} = ↑2↑2* 
 {0|↑↑*} = ↑↑↑ 
{↑|↑↑*} = ↑↑↑ 
{↑2|↑↑*} = ↑↑↑ 
{↑↑*|↑↑*} = ↑↑↑ 
So there are only 5 new molecules born on day 4: 
↑ 3  =  { ↑ 2 |  * } . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ( G - 5 ) 
⇑ *  =  { ↑ ↑ * | 0 } . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ( G - 6 ) 
↑ 2 ↑ * … … … … . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ( G - 7 ) 
↑ 2 ↑ 2 * … … … . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ( G - 8 ) 
↑ ↑ ↑ … … … … . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ( G - 9 ) 
5  Molecules Born on Day 5 
 
This section enumerates all the simple all small games born on day 5. Up to day 4, there are 10 
simple molecules. The right options of the possible all small games born on day 5 including, in addition 
to the 10 molecules, the molecule *. 8 of the 10 molecules born up to day 4 are greater than or equal 
to 0. These 8 molecules will be served as the left option of the possible molecules born on day 5. 
⇓
 
On day 5, there are 56 possible simple all small games (excluding games born up to day 4). 
 
In the above columns, there are 9 new molecules: 
↑ 3 ↑ * . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ( G - 1 0 ) 
↑ 3 ↑ 2 * . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ( G - 1 1 ) 
↑ 3 ↑ 3 * . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ( G - 1 2 ) 
{ ↑ 2 ↑ 2 ↓ | * } + ↑ * . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ( G - 1 3 ) 
↑ 2 ↑ ↑ … . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ( G - 1 4 ) 
↑ 2 ↑ 2 ↑ … . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ( G - 1 5 ) 
↑ 2 ↑ 2 ↑ 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ( G - 1 6 ) 
⇑ ⇑ ⇑ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ( G - 1 7 ) 
↑ ↑ ↑ ↑ * … . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ( G - 1 8 ) 
In the other columns, there are 7 new molecules. 
{ 0 | * } . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ( G - 1 9 ) ⇓
{ + ↑ | * } + ⇓ =  { ↑ | * } . . . . . . . . . . . . . . . . . . . . .  ( G - 2 0 ) ⇑ ⇓
{ + ↑⇑ 2 | * } + =  { ↑⇓ 2 | * } . . . . . . . . . . . . . . . . . .  ( G - 2 1 ) ⇓
{ + ↑⇑ 3 | * } + = { ↑⇓ 3 | * } . . . . . . . . . . . . . . . . . . .  ( G - 2 2 ) ⇓
↑ 4  =  { ↑ 3 |  * } . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ( G - 2 3 ) 
⇑ 2 2 * = { ↑ 2 ↑ 2 * | 0 } . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ( G - 2 4 ) 
⇑ 2 1 *  =  { ↑ 2 ↑ * | 0 } . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ( G - 2 5 ) 
Thus, the total number of none-zero molecules born on or before day 5 is 25.  
In addition to the 25 molecules, there are 6 none-molecule games (marked ◎) in table 3. All these 
none-molecule games are molecule switches. 
 {↑↑↑|0}...................................................(S-1) 
 {↑↑↑|*}...................................................(S-2) 
 {↑↑*|⇓ *}...............................................(S-3) 
 {↑2↑*|⇓ *}.............................................(S-4) 
 {↑2↑2*| *}............................................(S-5) ⇓
 {↑↑↑|⇓ *}...............................................(S-6) 
Since the incentive of any molecule switch is greater than the incentive of any molecule, when playing 
sums of molecules and molecule switches, molecule switches should always be played before 
molecules. 
6 Atoms Born on or before Day 5 
A close look at the molecules G-1 to G-25 will found out that these molecules are a sum of 13 
elementary games: 
{0| *}................................................. (E-0) ⇓
optimal strategy for playing sums of these atoms can be easily derived. 
 
References 
 
[1] E. R. Berlekamp, J. H. Conway, R. K. Guy, “Winning Ways”, Academic Press, New York, 1982 
[2] J. H. Conway, “On Numbers and Games”, Academic Press, New York, 1976 
[3] K. Y. Kao, “Sumbers – Sums of Ups and Downs ”, Integers, Electronic Journal of Combinatorial 
Number Theory, Vol.5  2005 
 
日期：民國 101 年 7 月 6 日～7 月 9 日 
 
本次出國前往香港參加 7 月 8 日於 Cosmopolitan Hotel(麗都)舉辦之 International 
Conference On Artificial Intelligence and Soft Computing(ICAISC) 。 
此次走訪行程如下： 
一、7 月 6 日，15：00──17：00，搭乘中華航空，從台灣桃園機場出發至香港，
下榻香港 Cosmopolitan Hotel。 
二、7 月 7 日，順道參訪香港市區，準備論文簡報。 
三、7 月 8 日，參加 ICAISC 研討會。 
四、7 月 9 日，14：30──16：30，搭乘中華航空，從香港返回台灣桃園機場。 
ICAISC研討會主要探討領域包括Fuzzy Logic (FL), Neural Computing (NC), 
Evolutionary Computation (EC) Machine Learning (ML) and Probabilistic Reasoning 
(PR)。本次研討會個人最大的目的在於交流並學習Monte Carlo Tree Search(MCTS)
相關技術及應用。近年來， MCTS已成為Machine Learning (ML) 領域中的重要
主題，並且成功地應用到電腦對局遊戲。最新的趨勢則包括二個面向，一方面各
種改良型的MCTS不斷被提出，另一方面則嘗試將MCTS應用到其它例如排程、
電力調度等最佳化問題。本人準備之研討會交流論文摘要(Incentive Learning In 
Monte Carlo Tree Search)如附件，這篇論文為個人近期與交通大學、東華大學合
作研究之成果，主要提出一種行動誘因(Action Incentive) 的機器學習(Machine 
Learning) 方法，可加速學習的效能。本論文為初步成果報告，完整版將投稿至
IEEE Transactions on Computational Intelligence and AI in Games (TCIAIG)。 
 
(攝於會場飯店、會場入口與中場茶敘) 
  2
 Incentive Learning in Monte Carlo Tree Search 
Kuo-Yuan Kao1, I-Chen Wu2, Member, IEEE, Shi-Jim Yen3, Member, IEEE, Yi-Chang Shan2  
 
Abstract—Monte Carlo tree search is a search paradigm that has been remarkably successful in 
computer games like Go. It uses Monte-Carlo simulation to evaluate the values of nodes in a search tree. 
The node values are then used to select the actions during subsequent simulations. The performance of 
MCTS heavily depends on the quality of its default policy, which guides the simulations beyond the search 
tree. In this article, we propose an MCTS improvement, called incentive learning, which learns the default 
policy online. This new default policy learning scheme is based on ideas from combinatorial game theory, 
and hence is particularly useful when the underlining game is a sum of games. To illustrate the efficiency of 
incentive learning, we introduce a game named Heap-Go and present the experiment results on the game.  
 
Index Terms—Artificial intelligence, combinatorial games, computational intelligence, computer games, 
reinforcement learning.  
 
I. INTRODUCTION - COMBINATORIAL GAMES 
INCE 1980’S combinatorial game theory has provided a common mathematical model for the 
analysis of many two-player, perfect information, zero-sum games. This section reviews the 
fundamentals of combinatorial game theory (see [1, 2] in more detail). In combinatorial game theory, a 
game is defined as an ordered pair of sets of games, 
S 
}|{ RL GGG =
                                                      
………………………………….……. (1) 
This work was supported in part by the National Science Foundation of Taiwan under Grant 100-2221-E-346-007.  
Kuo-Yuan Kao1 is with the Department of Information Management, National Penghu University, Taiwan 880. (e-mail: 
stone@ npu.edu.tw).  
I-Chen Wu2 and Yi-Chang Shan2 are with the Department of Computer Science, National Chiao Tung University, Hsinchu 
30050, Taiwan. (e-mail: icwu@csie.nctu.edu.tw and ycshan@java.csie.nctu.edu.tw) 
Shi-Jim Yen3 is with the Department of Computer Science and Information Engineering, National Dong Hwa University, 
Hualien, Taiwan (e-mail: sjyen@mail.ndhu.edu.tw). 
  4
temperature of a game G. 
The article [1] first introduced the concept of a game’s incentive. In this article, given a game G, 
▽L(G) and ▽R(G) are called the Left and Right incentive of G, which measures the precise move size of 
a game, as defined as follows.  
GGG LL −=∇ )( …………………………….……. (8) 
RR GGG −=∇ )( …………………………….……. (9) 
Incentive measures the precise game value difference between before and after a move at a game. Since 
the sum or difference of two games is also a game, incentive is game-valued. Note that the Left and 
Right incentives of a game may not be of the same value. One drawback of incentives is that incentives 
are not totally ordered, since games are not totally ordered. On the other hand, the temperature of a 
game G measures the difference between GL and GR. There is only one unique temperature of a game 
and it is simplified as a numerical value. Because a single numerical value cannot capture all the 
detailed information of a move, temperature is only a rough measure of a game’s move size. 
When playing a sum of games, players are always concerned about which move has the biggest 
move size. Although both temperature and incentive are measures of move size, the calculation of these 
values of a given game is not a simple task.  
The goal of this study is to design an automated learning algorithm that can learn incentive values in 
a relative manner. By relative manner, we mean, instead of learning the exact value of a move size, 
learning which move has relatively greater move size than others. This article first discusses how 
incentive learning works for combinatorial games. Then, a complete framework of incentive learning 
for state spaces of general groups is presented. 
This paper is organized as follows. Section II introduces Monte Carlo Tree 
Search researches. Section III presents incentive learning algorithm. Section IV 
introduces the combinatorial game Heap-Go. Section V describes the experiments. 
Section VI makes concluding remarks. 
II. MONTE CARLO TREE SEARCH 
Monte Carlo tree search (MCTS) [6] is a search paradigm for two-player, 
perfect-information, and zero-sum games. It has been applied to various computer 
games [7, 8, 9, 10, 11, 12, 13, 14 and 19]. This section reviews the basic ideas of 
MCTS and its related policy improvements, following the definitions of [16]. 
Let S denote the state space of a game, and st be the state of a game at time t. Let A 
denotes the space of actions and A(s) denotes the set of legal actions from state s. The 
two players alternate turns, at each turn t selecting an action at in A(st). The game 
  6
),(maxarg* asQa UCBa= …………….……..… (14) 
where c is a scalar exploration constant, N(s) is the number of visits to state s, and log 
is the natural logarithm. The underlying idea of UCT is to provide a balance between 
exploitation of the current best action and exploration of other potential better actions. 
RAVE [16] uses the all-moves-as-first (AMAF) heuristic to estimate the value of 
each action. The AMAF heuristic assumes there is one general outcome value z for 
each move, regardless of when it is played. The RAVE value function QRAVE(s, a) is 
the expected outcome from state s, given that a move a is taken at some subsequent 
turn. 
]..,|[),( auatstustszEas
RAVEQ =≥∃== …….. (15) 
RAVE provides a simple way to share knowledge between similar actions in the 
search tree, resulting in a rapid estimate of the action values. 
Compared to MCTS, RAVE learns very quickly, but its accuracy and convergence 
is not as good as MCTS, when a sufficiently large number of games are simulated. 
The MCTS–RAVE algorithm [16] overcomes this issue, by combining the rapid 
learning of the RAVE with the accuracy and convergence guarantees of MCTS.  
),(),()1(),(ˆ asQasQasQ RAVE×+×−= ββ …….… (16) 
where 10 ≤≤ β  has an initial value of 1 and gradually decreases as the number of 
simulations increases. 
We end this section by some remarks on the previous results. The underlying idea of 
RAVE is to have one general outcome value for each move, regardless of when it is 
played. Because of the property of “regardless of when it is played”, RAVE learns the 
outcome value by sharing knowledge between similar nodes in the search tree. On the 
other hand, for combinatorial games, we also know there is an incentive value of each 
action, regardless of when it is played. The next section studies how to learn these 
values of actions by sharing knowledge between nodes in the search tree. 
III. INCENTIVE LEARNING FOR COMBINATORIAL GAMES 
In this section, we focus on combinatorial games. From combinatorial game theory, 
we know each action has an incentive, as defined in Equations (8) and (9). Note that 
although Left’s goal is to maximize, while Right’s goal is to minimize the outcome, 
  8
Q(s0, b) = (10+20)/2 = 15 
 
So the RAVE thinks action b is better than a.  
Although action a and b happen twice in the simulations from state s0, there is only 
once the two actions starts from the same state s1. The RIDE value between actions a 
and b is: 
 
DRIDE(a, b) = (18-10)/1 = 8 
 
So the RIDE thinks action a is better than b. 
Although both RAVE and RIDE try to estimate the difference between the action 
values of action a and b by sampling from the simulations, their sampling methods are 
different. RAVE applies an independent sampling method, and any simulation 
contains the action a or b is collected into the samples. RIDE applies pair-wise 
sampling method, and only states that contain both actions a and b as the next actions 
are collected into the samples. 
From (17) and (18) we have 
babaDRIDE ∇∇⇒< |0),( p ……...……...….. (19)  
And,  
babaDRIDE ∇∇⇒> f|0),( ……...……..….. (20)  
Let L be a sequence of actions.  
  >=< naaL ,...,1 ……………………...………… (21)  
Ideally, we would like to order the actions in L such that 
 naa ∇∇ ff |...|1 ………………………………… (22)  
In reality, a sequence of actions in (21) may not be well ordered. So the goal is to 
find a sequence as close to (22) as possible. To define the term “close” more precisely, 
let DRIDE be the incentive difference function defined in (18), and L be a sequence of 
actions as in (21). We define the ordering cost of the sequence L under DRIDE as 
∑=
>
+
ji
ji
RIDE aaDDLCost ),(),( ……….…… (23)  
where 
⎩⎨
⎧ >=+
.otherwise0
,0),( if),(),( jaia
RIDEDjaiaRIDEDjaiaD … (24)  
  10
z When it is Left's turn to move, Left can choose any one of the heaps and 
repeatedly removes the top counter until either he removed a red counter or the 
heap has become empty.  
z When it is Right's turn to move, Right can choose any one of the heaps and 
repeatedly removes the top counter until either he removed a blue counter or the 
heap has become empty.  
The game is finished once all the counters in all the heaps were removed. At the end, 
the player who removed more total weights is the winner.  
Figure 3 shows the game tree of heap C in Figure 2. The numbers at the terminal 
nodes are the net scores of the paths from the root to these nodes. Left’s score are 
counted positive and Right’s negative. For example, consider the path LR. Left gets 8 
points for the first move (removed 2 counters); Right gets 5 points for the second 
move (removed 1 counter); the net score is 3.  
6
2
5
5 
5
5
2
13  3  ‐13 
91
L 
L
L
L  R 
R
R
R
re
blu
 
Figure 3: game tree of heap C 
Note that, in Heap-Go, each heap is a game with m states, where m is the number of 
counters in the heap and each state corresponds to a sub-heap of the heap. The global 
state is the sum of all the heaps. In Figure 2, the global state space has a size of 
3×2×3×4×2=144. Each action can only change the state of one heap. Although it 
seems there are many possible actions, the size of the space of actions is limited. In 
Figure 2, there are only 3+2+3+4+2=17 distinct actions, each action corresponds to a 
move at a state of a heap. In general, a sum of heaps with N counters total will have N 
distinct actions in the game tree of the sum. So, the memory requirement for incentive 
learning for Heap-Go is not a problem. One of the major reason we choose Heap-Go 
as the test bed is because the complexity of the game can be adjusted by simply 
changing the number of heaps and the number of counters in each heap.  
Our goal is to learn the partial order relations between the incentives in a Heap-Go 
game. We need a mechanism to evaluate the learning result. From combinatorial game 
theory, we know each heap has a temperature. Both incentive and temperature can 
  12
Table 1: RL match rate (%)  
(Default policy: maximum incentive) 
 (m×n)1 (m×n)2 (m×n)3no. of 
3×3 66.56 88.26 89.30 
game size 
4×4 62.51 84.98 93.68 
5×5 61.48 81.27 93.63 
6×6 60.77 78.70 91.59 
7×7 60.38 75.90 89.87 
The data in Table 1 is summarized as follows: 
z Even with a very small number, m×n, of simulations, the average match rate 
(between RL vs. RT) quickly runs above 60%. The average match rate increases 
as the number of simulation increase and almost reaches 90% within (m×n)3 
simulations. This implies the incentive learning converges fast and the agent’s 
default policy is getting closer and closer to the thermo-strategy, which selects 
the action with the maximum temperature. 
z The average match rate never reached 100%, even if a game tree is completely 
explored (in the case of game size 3×3). However, the high match rate, above 
89%, implies an action sequence ordered by action temperatures is very close to 
the action sequence with the minimum ordering cost. 
z The average match rate decreases as the action space size increases, under a 
fixed number of simulations. A low match rate implies the agent’s action policy 
is still far from the thermo-strategy. The match rate can also be used as an 
indicator of the performance of MCTS. 
Incentive learning (IL) with different default policy
(game size 5x5, 100 games)
(tree policy UCT)
50
60
70
80
90
100
100 1,000 10,000 100,000
simulations
m
at
ch
 ra
te
 (%
)
IL+random
default
IL+maximum
incentive default
 
Figure 4: Incentive learning with different default policy 
Figure 4 shows the comparison of the match rate between two different default 
  14
Indeed, if two players are both optimal, then the tie rate should be 100%. The increase 
of tie rate implies both players are playing better and better as the number of 
simulations increases. 
 
Winning rates with different default policies
(game size 5x5, 1000 games)
(tree policy UCT)
0
10
20
30
40
50
60
100 1,000 10,000 100,000
simulations
%
IL-default wins
random_default wins
Ties
 
Figure 6: Winning rates of different default policies 
VI. CONCLUSION AND FURTHER CONSIDERATION 
This article introduces the heuristic: “if an action can lead to a better reward than the rewards of 
other actions from the same state, then it is of great chance that the action has a greater incentive 
value than the ones of other actions”. We present an online incentive learning scheme equipped within 
MCTS. The outcome of incentive learning is a list of actions ordered by their incentives. This 
information is then used by the default policy to select the action with the maximum incentive.  
Each simulation in MCTS carries much information. The quality of MCTS depends on how much 
one can learn from each simulation. The majority of the past researches focus on the learning of action 
values. This article introduces the learning of incentive orders. Our experiment on Heap-Go has shown 
very positive results of incentive learning. 
We have seen in this article how incentive learning can be applied to the state space 
of combinatorial games. Indeed the general framework of incentive learning, as 
shown in Equations (18), (23), (24), and (25), can be applied to general state spaces.  
Both RAVE (Equation (15)) and RIDE (Equation (18)) use the same AMAF heuristic: there is one 
general action (incentive) value for each action, regardless when it is played. The major difference 
between RAVE and RIDE is their sampling strategy. RAVE applies independent sampling strategy, 
while RIDE applies pair-wise sampling strategy. In order to compare the action-value of two actions a 
and b. RAVE estimates the action-values of a and b independently and regardless of their starting states, 
and then compare their values. RIDE estimates the pair-wise action-value difference directly, as long as 
their starting states are the same. 
Bouzy and Chaslot also have the idea of learning by pair-wise comparison and 
  16
produces a complete incentive orders, for example L =<a, b, c, d, e, f>. If the results of 
the previous runs are not sufficient to determine a complete ordered list, one can 
continue additional runs. The divide-and-conquer feature of incentive opens a door for 
further research of more efficient incentive learning algorithms. 
Although our incentive learning scheme can be applied to general MCTS, further studies are needed 
to testify its performance at various problems. One way of measuring the feasibility of incentive 
learning for a problem is to check whether the match rate between the output action sequence L of 
incentive learning and the output action sequence of MCTS increases with increased simulations or not. 
The extension of incentive learning to other general problems deserves further research.  
REFERENCES 
[1] J. H. Conway, “On Numbers and Games”, Academic Press, New York, 1976.  
[2] E. R. Berlekamp, J. H. Conway, R. K. Guy, “Winning Ways”, Academic Press, New York, 1982. 
[3] John Milnor, “Sums of Positional Games”, in Kuhn and Tucker (eds.) “Contributions to the Theory of Games”, Ann. Math. 
Studies #28, Princeton, 1953 pp 291-301. 
[4] Olof Hanner, “Mean Play for Sums of Positional Games”, Pacific J. Math. 9, 1959, pp 81-99. 
[5] K. Kao, “Mean and Temperature Search for Go Endgame”, Information Sciences, Vol.122, Issue 1, pp. 77-90, January 
2000. 
[6] R. Coulom, “Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search”, in: 5th International Conference on 
Computer and Games, pp. 72–83. 
[7] S. Gelly, D. Silver, “Achieving master level play in 9 x 9 computer Go”, in: 23rd Conference on Artificial Intelligence, pp. 
1537–1540. 
[8] H. Finnsson, Y. Björnsson, “Simulation-based approach to general game playing”, in: 23rd Conference on Artificial 
Intelligence, pp. 259–264. 
[9] R. Lorentz, Amazons discover Monte-Carlo, in: 6th International Conference on Computers and Games, pp. 13–24. 
[10] M. Winands, Y. Björnsson, “Evaluation function based Monte-Carlo LOA”, in: 12th Advances in Computer Games 
Conference, pp. 33–44. 
[11] J. Schäfer, “The UCT algorithm applied to games with imperfect information”, Diploma Thesis. 
Otto-von-Guericke-Universit¨at Magdeburg, 2008. 
[12] N. Sturtevant, “An analysis of UCT in multi-player games”, in: 6th International Conference on Computers and Games, pp. 
37–49. 
[13] R. Balla, A. Fern, UCT for tactical assault planning in real-time strategy games, in: 21st International Joint Conference on 
Artificial Intelligence, pp. 40–45. 
[14] F. Teytaud, O.Teytaud, “Creating an Upper Confidence Tree program for Havannah”, in: 12th Advances in Computer 
Games Conference, pp. 65–74. 
[15] S. Gelly, “A Contribution to Reinforcement Learning; Application to Computer Go”, Ph.D. thesis, University of South Paris, 
2007. 
  18
for Artificial Intelligence (TAAI) Conference. He served as a Workshop Cochair of 2011 IEEE International Conference on 
Fuzzy Systems. He is the Chair of the IEEE Computational Intelligence Society (CIS) Emergent Technologies Technical 
Committee (ETTC) Task Force on Emerging Technologies for Computer Go in 2010-2012. 
 
 Yi-Chang Shan received the B.S. and M.S. degrees in Computer Science from National Taiwan 
Normal University. He is currently working toward the Ph.D. degree in Institute of Computer Science, 
National Chiao Tung University, Hsinchu, Taiwan. His research interests are computer game, 
combinatorial game theory, volunteer computing and cloud computing. 
 
 
  20
100 年度專題研究計畫研究成果彙整表 
計畫主持人：高國元 計畫編號：100-2221-E-346-007- 
計畫名稱：狹義二元樹加法運算之研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
