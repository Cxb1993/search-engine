will be implemented as 1) goal seeking (locking 
static target, static target with linear and circular 
path to avoid obstacle in the line of sight, and 
target in motion), 2) trajectory tracking (backward 
parallel parking and backward garage parking), 3) 
obstacle avoidance (open loop and close loop obstacle 
pattern) and 4) formation keeping (2 mobile robot 
with a virtual robot to form various pattern of 
geometry shape). Finally, the sub modules will be 
integrated as a whole to conduct a particular task 
which requires those four behaviors.  
This proposed research is also an extension of the 
previous international collaboration project between 
Graduate Institute of Automation and Control, 
National Taiwan University of Science and Technology 
(Taiwan) and Department of Mechanical Engineering, 
University of British Columbia (Canada). Three 
graduate students has been invited to Industrial 
Automation Laboratory, University of British 
Columbia, Vancouver, Canada in Jan. 2010 for 6 months 
as a visiting scholar to conduct the international 
collaboration research under the guidance of Dr. 
Clarence de Silva, the professor and the member of 
Royal Society of Canada. 
英文關鍵詞： Mobile Robot, Fuzzy Logic, Neural Network, Obstacle 
Avoidance, Goal seeking, Target switching strategy, 
Visual servoing； Autonomous Navigation； Omni-
directional camera； eye-to-hand camera configuration
 
I 
 
中文摘要 
 
本計劃目標在於智慧型自主式移動機器人之研究與發展。四種自主式行為將被分析及
實現 (1) 目標搜尋，(2)軌跡追蹤，(3)避障，(4)編隊。 透過不同的感測原理實現定位、
建圖及導航，如電腦視覺、聲納、雷射測距儀、慣性量測單元(包含陀螺儀及加速度計)
以建立感知感測演算法。動作規畫與控制之模型將被建立。一個有效率的追蹤導引演算法
之可行性分析將被研究，如 Kalman filter, particle filter, Bayesian model, Markov 
process 與軟性計算法（模糊邏輯、類神經網路及基因演算法）。在不同行為情境之下實
現(1) 目標搜尋（包括靜態目標鎖定、動態目標鎖定及在視線範圍內進行直線及曲線避障
路線下之靜態目標鎖定），(2)軌跡追蹤（倒退平行停車及倒車入庫），(3)障礙物躲避（開
迴路及閉迴路之障礙物模式），(4)隊形編排（兩個移動機器人與一個虛擬機器人保持不同
指定的幾何形狀）。最後，四種研究行為將整合為能執行特定任務的系統。 
本計劃同時為延續國立台灣科技大學自動化及控制研究所及加拿大溫哥華英屬哥倫比
亞大學機械系之國際合作專案，三位研究生已受邀以訪問學者身份至加拿大溫哥華參與英
屬哥倫比亞大學工業自動化實驗室，由加拿大皇家院士 Clarence de Silva 博士所領導
之研究團隊，於民國九十九年一月起進行為期六個月之國際合作研究。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
關鍵字: 移動式機器人、模糊控制、類神經網路控制、避障、目標搜尋、目標變換策略、
影像伺服、自主式導航、全景式攝影機 
1 
 
Introduction 
 
 
In the early 1970, the most widely accepted definition of an industrial robot is: "a reprogrammable 
multi-functional manipulator designed to move material, parts, tools or specialized devices through 
variable programmed motions to accomplish a variety of tasks", according to the Robotic Industries 
Association (RIA). Robot is a mechanical machine that is programmable to perform specific tasks 
assigned by humans. The powerful advantage of robot is that can be used to substitute humans to handle 
some very heavy, dirty, dangerous, or repetitive tasks. The various applications include cleaning of the 
main circulating pump housing in the nuclear power plant or repetitive jobs that are boring, stressful, or 
labor-intensive for humans. In the 1980s, the robot industry has higher level demand of robot. Mobile 
robot has been used for various purposes in many application fields including exploration, industry etc.  
The development of techniques for robots collaboration is one of the major trends in the 
autonomous robotics. This research is mainly focus on developing fully autonomous robot. The goal is 
to enable the robot to cope with its environment whether this is on land, underwater, in the air, 
underground, or in space. The goal is to meet the requirements of the task and be able to adapt to the 
unstructured environment that the robotic system interacted with, four fundamental behaviors are 
investigated and illustrated in Figure 1.  
 
Figure 1 Four behaviors of the autonomous robots 
 
 
With the following behavior-based approaches, one decomposes complex navigation tasks and 
different application of mobile robots into several, well-specified behaviors that are easier to design and 
carry out. The Robot behavior coordination structure illustrates in Figure 2.  
Goal 
Seeking 
Trajectory 
Tracking 
Obstacle 
Avoidance 
Formation 
keeping 
3 
 
Wang et al. [4] described the minimum risk method based on trial-return behavior phenomenon and 
demonstrated the robustness in traps. However, the research does not take into account the high power 
consumption in trial-return motion and the time efficiency for goal approaching. Xu et al. [10] proposed 
a local target-switching scheme solve the local trapping and wandering cycle problem. The concept of 
the memory state strategy [5] was implemented to solve a “dead cycle” problem.  
Motlagh et al. [6] proposed a target switching method instead of other virtual target approaches. The 
approach is to conduct a smooth clockwise or counter-clockwise shift from the actual target direction to 
a direction tangent to the wall to be followed. Therefore, the robot turns and follows the walls inside the 
dead-end trap. 
In this research we present sensor integration based soft computing methodology to the navigation 
of the mobile robot in a maze full filled with obstacles, the goals are summarized as follows: 
1. Traverse the maze and reach the exit. 
2. Collision free (obstacles or wall) navigation. 
3. Re-route to the collision free path will not affect the goal to exit of maze (e.g. getting trapped 
in the maze). 
Paper [8] would have been more convincing if the authors included the experimental validation and 
considered a more complicated scenario of maze including additional obstacles. This work addresses 
the weaknesses mentioned above as follows: 
1. Mobile robot is placed inside a complicated structured maze.  
2. Additional various characteristic obstacles are randomly located and in the maze.  
3. Non-uniform inter modal spacing membership function 
4. Experimental validation is conducted, and the data obtained in each experiment will provide 
useful information to modify our system design, and obtain better performance results for 
specific scenarios 
5. Sensor integration is used to achieve better navigation accuracy 
a. Laser range finder, better accuracy than sonar sensor used in [6]  
b. Omni-directional vision system generates the maze map including the posture of the robot 
and the target. 
 
  “Goal Seeking” behavior: 
Nowadays, visual servoing is widely used in various applications [11]. In mobile robot system, 
visual input is used to obtain environmental information and through perception to perform further 
action such as obstacle avoidance and goal seeking. One of the general approaches for image-based 
motion control is eye-in-hand camera for local mapping. 
Haoxiang and de Silva [12], Zhang and Ostrowski [13] proposed a visual servo control system to 
navigate the mobile robot in an unstructured environment with the kinematic models under an 
eye-in-hand camera configuration. It provided a direct motion planning solution based upon 
5 
 
Purpose 
There are three key questions for an autonomous mobile robot to answer when performing 
various tasks under a dynamic and unstructured environment: “Where am I?”, “Where am I 
going?”, and “How do I go there?”  The capabilities needed for a mobile robot to answer those 
questions are locomotion, sensing, perception, motion planning and control. 
In this research, a networked intelligent controlled autonomous robot based on the four 
behavior is proposed. By using distributed control and communication protocols, the global-level 
formation tasks are converted into local tracking problems. The control algorithms developed is 
based on the global positioning system or indoor localization system. The position measurement 
of each robot is obtained from both of the exteroceptive sensor and a vision system.  
We develop the fundamental behavior system for mobile robots. By global positioning 
systems and local sensor: Sonar, Laser range finder, the obstacle avoidance algorithm was 
developed. By global positioning system and PTZ camera, the goal seeking and reconnaissance 
system was developed. We also develop our own algorithm of the trajectory tracking and 
formation keeping behavior.  
Furthermore, the proposed methodologies have the advantage that it can minimize the 
computation of modeling whole mobile robot system and getting greater precision. In other 
words, the robot can directly change his behavior and interact with the environment with no 
knowledge on the environment. Compared with the current research works on the mobile robot 
application, this study explicitly integrates the dynamics of mobile robot with nonholonomic 
constraint into the mobile robot system. This integration approach can be used to many 
cooperative problems to yield the centralized coordinating control laws. 
 
 
7 
 
 
Figure 2.  Actual maze setup from the view of ceiling omni-directional camera 
 
Figure 3.  Layout of Experiment 
 
Figure 4.  Mobile Robot – AmigoBot 
The kinematic and dynamic models to drive the mobile robot are highly complex, nonlinear 
and uncertain. To overcome those disadvantages, knowledge and rule based control approach, 
Fuzzy Logic, is adopted for the proposed system. The advantages include (a) Simple structure: 
Less computational and cost than the conventional crisp control; (b) Intelligence: Incorporating 
the human knowledge, experience, and behavior; and (c) Robustness: the tolerance on the 
Exit 
Robot 
Wall 
Obstacle 
9 
 
However, before segmenting the data into five areas, it is necessary to pre-process them. 
This will assure that the system has valid and normalized values. 
In order to avoid false readings, N scans are done in the same robot’s position and the 
average is used as a single measurement.  
 𝐷𝑎𝑣𝑒 =
∑ 𝐷𝑖
𝑁
𝑖=1
𝑁
; 𝑁 = 10  (1) 
The laser may return invalid data values -1, which must be discarded. Then, the data is 
normalized, by (2) for each laser range finder measurement Dave(i), so that obstacles closer to 
the robot are more representative than the ones that are further. 
 𝐷𝑛𝑜𝑟𝑚(i) = 1 −
𝐷𝑎𝑣𝑒(i)
5000
;  i = [1, 1024] (2) 
After normalization, it was observed a noise with magnitude lower than 0.17.  This value 
was not considered significant for the system; therefore, readings lower than 0.17 were excluded 
from the valid data. 
It was also necessary to set up a threshold to avoid taking premature decisions and saving 
processing time. This threshold was determined by a 10cm obstacle at 30cm distance from the 
laser, which covers 64 laser reading points. For example, a 10cm obstacle further than 30cm 
covers less than 64 laser reading points and won’t be detected until it gets closer than 30cm. 
Likewise, bigger obstacles will be detected further than smaller obstacles, as showed in Figure 7. 
 
Figure 7.  Laser Range Finder Reading Data 
The formulation of the laser pre-processing can be seen in (3) and is represented in Figure 8. 
 𝐷𝑣𝑎𝑙𝑖𝑑 = {𝐷𝑛𝑜𝑟𝑚(𝑖) | [𝐷𝑛𝑜𝑟𝑚(𝑖), 𝐷𝑛𝑜𝑟𝑚(𝑖 + 64)] > 0.17} (3) 
11 
 
   =    
    
  
 
Substitute (5) into (4), the steering angle S is obtained as 
    =    
  ( 1   )
 1   
− 𝑟 
 
Figure 10.  Angles illustration 
Table I presented the 18 knowledge based if-then rules. The If section contains the two input 
sensor variables, robot and target orientation difference and robot and obstacle orientation 
difference. The then part contains the two output control variables, the steering angle and the 
speed of the robot. Those input and output variables have various grade in each fuzzy states of 
the membership designed. 
TABLE I.  RULE TABLE OF ROBOT’S BEHAVIOR 
Rule  
Number 
If Then 
Target Operator Obstacle Direction Operator Speed 
1 RF & R F & L 
2 RF & RF LF & L 
3 RF & F R & L 
4 RF & LF R & N 
5 RF & L R & H 
6 RF & No RF & N 
7 F & R F & H 
8 F & RF LF & L 
9 F & F F & L 
10 F & LF RF & L 
11 F & L F & H 
12 F & No F & N 
13 LF & R L & H 
14 LF & RF L & N 
15 LF & F L & L 
Y 
X 
W 
W 
X 
1 
Y 
1 
 
 
 
r 
 
S 
 
E 
Obstacle 
Centered at  P 
3 (x 3 , y 3 ) 
Exit 
Centered at  P 
2 (x 2 , y 2 ) 
Robot, Amigo 
Heading Direction  
Robot, Amigo 
Centered at  P 
1 (x 1 ,y 1 ) 
13 
 
Rule  
Number 
If Then 
Obstacle 1 Operator Obstacle 2 Direction Operator Speed 
19 F & R L & L 
20 F & L R & L 
21 F & R L & L 
22 F & L R & L 
23 F & R L & L 
24 F & L R & L 
25 RF & R L & L 
26 LF & L R & L 
27 RF & R L & L 
28 LF & L R & L 
29 RF & R L & L 
30 LF & L R & L 
 
  
Figure 12.  Input Membership Function; Left: Target angle; Right: Obstacle angle. L: Left ; LF: Front Left; F:Front; 
RF: Front Right; R:Right. 
  
Figure 13.  Output Membership; Left: Speed, Right: Steering angle. L: Left ; LF: Front Left; F:Front; RF: Front 
Right; R:Right. 
The trapezoid and triangular shaped membership function are used and can be formulated as 
follow 
   ( ) =
{
 
 
 
 
0            
  𝑎
  𝑎
          
1                  
𝑑  
𝑑  
          
0            
 
LF F RF L LF RF R F 
Low  Normal High  
L LF F RF R 
15 
 
Target Switching Strategy 
Motlagh et al. [6] suggested the use of the applied Fuzzy Logic Controller with target 
switching strategy to avoid obstacle and seek the goal. Their data from the simulation 
demonstrated the “loop path” problem shown in Figure 15 can be effectively avoided. Their 
proposed target switching strategy is to use a virtual target determined by the sum of every 
steering angle and the actual target angle when facing the obstacle. When robot faces free space, 
it adjusts to the actual target and so the loop path problem is avoided. 
However, two problems surfaced while applying their approach to P3DX robot. First, 
obstacle position sensing capability of the sonar sensors on the robot is limited. Secondly, when 
P3DX comes to a position facing the obstacle, it could not move or avoid the obstacle because 
the change of virtual target is not large enough. 
In the research, we focuses on extending and verifying Motlagh’s approach in the real robot 
platform.  Further findings from modifying and enhancing their approach are presented. 
Robot
Target
 
Figure 15.  The path trajectory looping problem 
To overcome the problem described above, two additional sensors, a laser ranger finder on 
robot and a CCD omnidirectional camera on the ceiling, are introduced. The fused sensory data 
inputs to the fuzzy logic controller (FLC) are the obstacle position and the target orientation. The 
FLC includes eighteen fuzzy rule inferences. Combined with the target switching strategy, the 
control action outputs, namely, the steering angle and velocity are applied to guide the robot out 
of the dead zone and reach the target. 
 
 
Figure 16.  The P3DX robot 
Laser 
Range 
Finder 
Sonar 
17 
 
Start
Get Sonar and Laser 
readings
Sonar > 2000 Laser>0
Use Laser 
readings
Yes Yes
Use Sonar 
readings
No
No
Laser > Sonar
Use Laser 
readings
No
Use Sonar reading
Yes
 
Figure 18. Sensor Fusion flowchart 
The sensor selection scheme is shown in Figure 18. The selected sensor reading is then used 
as the obstacle position input to the FLC. The pseudo code for the sensor selection strategy is as 
follows:  
If Laser reading > 0 
If Sonar Reading > 2000mm, then use Laser reading, 
Else use the Min (Sonar Reading, Laser reading). 
Else 
Use Sonar reading. 
Based on the size of our experiment setups, sonar readings over 2000mm are considered 
invalid. At each direction, the minimum reading between the sonar and laser sensor is selected as 
the obstacle position. 
B. Target switching strategy and Fuzzy logic controller 
Target switching [6] is a strategy of switching between actual target (real goal) and virtual 
19 
 
Start
Initialization
Load the Target 
position
Obstacle position fuzzification
L,LF,F,RF,R
Target orientation fuzzification
ul,uf,ur
Fuzzy set of 18 rules
Steering angle (Yt) Delta Velocity Calculate Velocity
Sensor Fusion
(sonar and laser)
Caculate the different 
angle of heading angle 
and target angle
Calculate target angle
Target_angle = target_ref + Syt
If any obstacle 
around
Syt = 0
Target_ref= 
actual target angle
Decrease Syt 10 
degrees
Increase Syt 10 
degrees
Heading angle
(from camera system)
Yes
Syt<0
Syt>0
Syt
Yes
Obstacle is less 
than 100mm
Safe rule 
( velocity = 0)
Yes
Last 
target_ref
Virtual target and reference target shift strategy
Distance 
is minimum
Yes
Target_angle= 
actual target angle
No
No
Increase target_ref 
10 degrees
Increase target_ref 
10 degrees
Target_angle
>=0Yes No
No
Last 
target_ref
 
Figure 20. Program flow chart; Red parts are our improvements 
After applying the 18 fuzzy rules, the FLC outputs the steering angle (Yt) and the delta 
velocity, then the total steering angle (Syt) and real velocity are calculated. The steering angle is 
feedbacked to the loop for further calculation. The loop interval is 0.2 second. 
 
Visual Servoing System for Autonomous Mobile Robot  
We proposed a visual servoing system for autonomous mobile robot navigation under an 
eye-to-hand camera configuration using omnidirectional vision. The proposed algorithm is based 
upon Position-Based Visual Servoing (PBVS) and uses only one fixed camera to provide the 
global path planning. The image is captured by the omnidirectional camera mounted on the 
ceiling followed by camera calibration, visual pose detection of targets, obstacles and robot, and 
21 
 
Item Value 
Model T2Z1816CS 
Focal length 1.8mm - 3.6mm 
Angle of View 
1/3" 
D 173.8° −  99.0° 
H 144.2° −  79.4° 
V 109.9° −  59.6° 
The ceiling camera is connected to the server PC via a USB repeater cable and a USB hub 
with its own power supply. The schematic system of the ceiling camera is shown Figure 23. 
 
Figure 23.  Ceiling camera connection diagram 
The server/client cloud computing architecture is adopted in this paper for the real-time 
visual servo control as shown in Figure 24. The camera calibration, computer vision and path 
planning processes are implemented in the server system to provide the client system the 
trajectory for mobile robot to follow via the Ethernet. Figure 25 shows the image captured by 
ceiling omnidirectional camera; the distortion is corrected through the calibration. 
 
Figure 24.  System of data exchange 
  
Figure 25.  (Left) Distorted image (Right) Undistorted with obstacle detecte 
Camera modeling is to mathematically approximating the internal camera geometry and the 
external position and orientation of the camera. Figure 26 shows the perspective projection and is 
 
23 
 
  (   ,  𝑣)   ( ,𝑣) +
  
  
( ,  )   +
  
  
( ,  )  𝑦 
 =  ( ,𝑣) +   ( ,𝑣) +   ( ,𝑣)𝑦 
Where Ix(u,v) and Iy(u,v) is the partial derivatives of I. The E(x,y) is approximated as 
  ( , 𝑦)  ∑ ∑  ( ,  )𝑣  (  ( ,𝑣) +   ( ,𝑣)𝑦)
  
Those corner points extracted are used as the 3D points in space to calibrate the camera. The 
corners of obstacles are detected using the vision algorithm and those corners are further 
expanded to enlarge the obstacle size as tolerance for a safety factor for the collision free path 
planning. Three steps are involved in path planning to generate collision free trajectory to reach 
the target and shown in following pseudo code. 
1. Connect all points among targets, corners of obstacles and start position of mobile robot. 
2. Exam all lines connecting those points and remove the ones passing the obstacle. 
3. Find the shortest path to reach target.  
 
The function CHECK is to check whether the line connecting two given nodes goes across 
any obstacles or not, and return 0 if it does, otherwise 1. A matrix adij contains the relationship 
between corner of enlarged obstacles, the targets, and the robot. This matrix stored the distance 
dij of the direct link between each node defined a graph, the formula shows in Equation (5). The 
size of this matrix is (N+M+P)×(N+M+P), where N represents the number of the corners of 
obstacles, M represents the number of targets (in this case M=3), P represents the number of 
mobile robot (in this case P=1). 
   𝑖𝑗 = {
 𝑖𝑗 , if  𝑖𝑗 ≠ 0 ∩ don
't cross obstacles
0, if 𝑖 = 𝑗
 𝑛 , o hers
 (5) 
 
 
 
Procedure PATH PLANNING; 
Begin 
for i=0,1,…, until i=N+M+P-1 do 
for j=i+1 until j=N+M+P-1 do 
if CHECK(node[i], node[j]) = 1 then 
begin 
ad[i][j]=distances between node[i] and node[j]; 
ad[j][i]=ad[i][j]; 
end; 
end; 
25 
 
obstacles. 
The linear control law from Siegwart et al [21] to guide robot to the goal position (, , 
)=(0, 0 , 0) is formulated as 
  = kρ𝜌,𝜔 = kα𝛼 − kβ𝛽 (6) 
They proved the condition for control parameters to have a locally exponentially stable 
system is as follows. 
kρ > 0; kβ < 0; kα − kρ > 0 
The nominal parameter [21] are (k, k, k) = (3, 9, -1.5) and various set of parameters is 
tested and the resulting trajectory is compared in this paper. The vision system using the 
omni-directional camera mounted on the ceiling provides the initial position {x, y, } of the 
target and the robot. The camera also recorded the robot pose every 0.01 second. The camera is 
connected to the server via a USB cable. The system setup is illustrated in Figure 28. 
 
 
Figure 28.  Structure of the IPC Server/Client model 
The AmigoBot [23], a differential-drive wheeled mobile robot platform, is used in the 
proposed experiment. The robot (33cm28cm) has two position encoders, a maximum translate 
speed at 100 cm/sec and rotational speed at 100 degree/sec. The transmission of control signal to 
the low level controller in the robot is via a wireless or the RS232 serial connection to PC. The 
mark on robot is to indicate the front side of the robot to the camera as shown in Figure 29. The 
experiment set-up is show in Figure 30. 
 
Figure 29.  The AmigoBot robot 
27 
 
Simulation and Experiment Result 
Figure 32 shows the result from sensor reading during the experiments which contained 1) 
the global map include maze location and robot pose via the ceiling omni-directional vision 
system; 2) the orientation of both obstacle and wall relative to robot, sensed via laser range finder; 
and 3) the orientation of the robot relative to the target, calculated through the robot and target 
pose obtained via ceiling omni-directional vision system. 
The experiments were divided in two scenarios: 
1. First, the wall following behavior and obstacle avoidance capability of the robot is 
analyzed in various corner setups. 
2. Second, different configurations of mazes are constructed to evaluate the performance in 
the proposed system. 
A. Scenarios – Corner Location 
Figures 33 shows various robot trajectory testing results under different corner scenarios, 
where we discovered that is the critical point when avoiding obstacles while simultaneously 
making turn around the corner.  The empirical results demonstrate the robustness of robot 
manipulation (wall following and obstacle avoidance) in corner location. 
C. Scenarios –  Different Configuration of Mazes 
However, in case of a dead-end, the robot failed to exit the maze as shown in Figure 34. It is 
due to the algorithm continually searching the path with shortest distance between the robot and 
the target, which will lead to an infinite loop. Figures 35 and 36 show the results under two 
different maze configurations to evaluate the effectiveness and efficiency of the proposed 
method. 
During the experiments, the robot was able to follow the maze shape, and avoid obstacles in 
the way. For the cases proposed, it was enough to successfully reach our target. However, there 
are still some points to improve such as smoothness of the path, speed and dead-end conditions. 
29 
 
 
Left turn with obstacle at right 
 
Left turn with obstacle at left 
Figure 33.  Corner Environment Scenario 
 
Figure 34.  Dead-End 
 
Figure 35.  First maze solved 
31 
 
(B, C, D, E, F) where the mobile robot made a sharp turn are illustrated in Figures 38. 
a. From point A(Starting point) to point B (Close to Wall): Linear path. Once robot 
reaches point B, the safe rule is applied and the virtual target is shifted to the right of 
robot.  
b. From point B to point C: The virtual target is shifted based on (1) and the robot follows 
the wall under the control of FLC. 
c. From point C to point D: At point C, the virtual target is shifted again, and under the 
control of FLC, the robot follows the wall until reaching the opening point D. At point 
D, the virtual target is on the front of the robot, but the Syt is positive and decreasing 
(shown in Figures 40), then the virtual target is shifted to the left of the robot, making 
the robot turn left instead of continuely turning right. 
d. From point D to point E and F: The robot keeps following the wall until point E, where 
it detects the opening point again. The same as at point D, Syt decreases, and virtual 
target is shifted to the left of the robot, then the robot follows the wall until reaching 
point F.  
e. From point F to point G(Goal): Point F is at real free space. Syt decreases 10 degree 
every step following “no obstacle and Syt > 0” conditions, and virtual target is shifted 
to the left of the robot. Meanwhile FLC keeps the robot going straight. After a short 
time, Syt converge to zero, virtual target, reference target and actual target become one. 
From this point on the P3DX goes straight to the goal. 
Robot
Target
A
B
CD
E F
G
X
Y
 
Figure 38. Robot’s path in U-shaped maze 
33 
 
c. From point D to point E: The virtual target is shifted based on (1) and the robot follow 
the wall to point E.  
d. From point E to point F: The virtual target is shifted again; the robot goes to point F 
under the effect of FLC. 
e. From point F to point G: The virtual target is still shifted; the robot keeps following 
the wall until point G. 
f. From point G to point H: Syt is positive and decreasing and the virtual target is shifted 
to the left of the robot, which make the robot turn to point H.  
g. From point H to point I:  The robot keeps following the wall 
h. From point I to point J: Point I is at the opening space and Syt is still positive, 
therefore the virtual target is once again shift to the left of the robot, which make 
the robot follow the wall and go straight to the goal. 
 
Figure 41. Top: Target Angle, Bottom: Total steering angle (Syt) 
The change of two parameters, target angle and Syt, are recorded and shown in Figures 41. 
The target switching strategy again successfully guided P3DX. 
 
Visual Servoing System for Autonomous Mobile Robot  
The corners of block are extracted from the calibration pattern image by Harris detection 
method as shown in Figure 42. 20 points are picked for camera calibration as red solid circles.  
-100
-50
0
50
100
150
200
250
1 51 101 151 201 251 301 351 401 451
D
eg
re
e
Time(second)
Total Yt
Total Yt
A
B
C
D
E
F G
H
I
J
-200
-150
-100
-50
0
50
100
150
200
1 51 101 151 201 251 301 351 401 451
D
e
gr
e
e
Time(second)
Target Angle
Target AngleA
B
C
D
E
F
G
H I
J
35 
 
Weng 3.934 4.0665 0.0674 16.58 
Color detection is used to identify multiple targets (red), obstacle (white), and robot (dark 
red) as shown in Figure 44. The dilation factor enlarge the obstacles sized to guarantee a safe 
space for mobile robots, the factor is 70 in this experiment. 
 
Figure 44.  (a) Original imag (b) Image processing to find targets and robot (c) Obstacle in binary image after being 
changed to rectangular shape (d) Obstacle image after being set in bigger for safety path 
The outcome of path planning is to locate the possible paths by connecting all points, and 
determine the order of nodes that the mobile robot will pass through. Dijkstra’s algorithm is 
applied to calculate the shortest path from starting point to all nodes. The sequence of node 
arrival is illustrated in Table VIII. 
TABLE VIII.  THE PATH ORDER OF NODES 
Path 
order 
1 2 3 4 5 6 7 
Path 
node 
1 22 11 23 6 9 24 
There are 24 nodes generated for the trajectories. The mobile robot is always located at the 
node 1. The first, second, and third target are positioned at node 22, 23, and 24 respectively. 
Meanwhile, node 11, 6, and 9 are the corners of enlarged obstacles. According to above process, 
the motion control is carried out so that the mobile robot moves along the shortest path. Figure 
45(a) shows all the possible paths. The final trajectory is shown in Figure 45(b). 
37 
 
set as 0.6 and ω is set as 50 rad/s. It can be seen from the data in Table X that the robot starts 
from the initial pose node No. 4 resulted with the highest time efficiency due to the fact that it is 
directly heading toward the target and no extra turning is required to aim at the target.  The 
robot starts from the initial pose nodes No. 1 and No. 7 had the lowest time efficiency due to the 
largest turning is required for heading toward the target. 
TABLE X.  THE TIME ARRIVE THE TARGET (SEC) 
No. 1 2 3 4 
Time 8.23 s 5.34s 5.24s 4.48s 
NO 5 6 7 8 
Time 5.24s 6.92s 9.51s 5.02s 
 
The third set of analyses examined the impact of the trajectory quality under the nominal 
control parameters, (k, k, k) = (3, 9, -1.5) through computer program simulation and at the 
real robot motion control in real-time. The results are shown in Figure 46 with blue lines 
represent simulation and green lines represent real robot motion.   
 
Figure 46.  Simulation and real trajectory, (k , k , k) = (3, 9, -1.5) 
The discrepancy between the simulation path and the real robot path indicates that using the 
nominal parameter from Siegwart et al [21] in our forward motion only approach does not yield 
the ideal results. It can be seen as well that the trajectory is not smooth. We tuned the control 
parameters and (k, k, k) = (1, 1.5, 0) results in smooth trajectories for all initial posed as 
illustrated in Figure 47. 
 
39 
 
 
Figure 49.  Errors of the target tracking 
 
TABLE XI.  DESCRIPTIVE STATISTICS ON FINAL POSITION ERROR (CM) 
Mean 10.03 
Standard Error 0.30 
Standard Deviation 2.65 
Minimum 4.15 
Maximum 16.44 
 
41 
 
has the ability to plan a path and follow this trajectory. However, blind spots might exist in the 
map, for example targets hidden under other objects. Local mapping could be taken into 
consideration to further improve this issue. A dynamic system can be developed if we can reduce 
the complexity of path planning so that the increasing of obstacle numbers would not impact 
run-time performance. 
The results of this study indicate that the forward and backward approach have significantly 
different trajectory character under same nominal control parameters and kinematics model. The 
experimental results demonstrated an optimal path of the wheeled mobile robot through the 
feedback control law to the goal with an average 10 cm error. However, the optimal parameter is 
obtained through experimental tries. The automatic parameter optimization algorithms should be 
implemented such as genetic algorithm. The real time path planning is also needed to obtain the 
dynamic parameters of k, k and k if there is the presence of obstacle. The real-time trajectory 
error estimation and velocity compensation can be further implemented to improve the system 
such as Kalman or Bastian filter. Lots of applications that need smooth movement towards the 
target, such as automatic parking system, search and rescue robot etc, can be further investigated 
based on our present approaches 
Future work should be aimed at incorporating higher level scene knowledge to enable 
obstacle avoidance and other behavior characterization, as well as connecting multiple robots to 
enable autonomous navigation between arbitrary points. 
43 
 
Robotics and Automation, IEEE Transactions on, vol. 9, pp. 837-842, 1993. 
[20] G. Campion, Georges Bastin, B. Dandrea-Novel,"Structural properties and classification of 
kinematic and dynamic models of wheeled mobile robots," Robotics and Automation, IEEE 
Transactions on, vol. 12, pp. 47-62, 1996. 
[21] Roland Siegwart and Illah R. Nourbakhsh, Introduction to autonomous mobile robots, Cambridge, 
Mass. MIT Press, 2004 
[22] F. O. Karray and C. W. De Silva, Soft Computing and Intelligent Systems Design: Theory, Tools 
and Applications Addison Wesley 2004. 
[23] http://mobilerobots.com/researchrobots/researchrobots/amigobot.aspx  
[24] http://www.hokuyo-aut.jp/02sensor/07scanner/urg_04lx_ug01.html 
[25] http://www.ptgrey.com/products/fireflymv/  
[26] C. Harris and M. Stephens, “A Combined Corner and Edge Detector,” Proc. Fourth Alvey Vision 
Conf., vol. 15, pp. 147-151, 1988. 
 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：李敏凡 計畫編號：99-2221-E-011-127- 
計畫名稱：智慧型移動機器人行為控制 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 2 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 7 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 5 0 100%  
博士生 1 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
