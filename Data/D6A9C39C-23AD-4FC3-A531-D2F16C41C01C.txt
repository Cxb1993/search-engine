行政院國家科學委員會補助專題研究計畫 
■ 成 果 報 告   
□期中進度報告 
 
植基於機器學習的物件類別辨識技術 
 
 
計畫類別：■ 個別型計畫  □ 整合型計畫 
計畫編號：NSC  97－2221－E－001－019－MY3 
執行期間： 97 年  8  月  1 日 至 100 年 7 月 31 日 
 
計畫主持人：劉庭祿 
共同主持人： 
計畫參與人員：林彥宇、張楷岳、張天龍、賴美雯 
 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  ■完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
■出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
          
執行單位： 中 央 研 究 院 資 訊 科 學 研 究 所 
 
中   華   民   國  100 年  10  月   31  日
 II 
Abstract 
 
This report summarizes the research outcomes of our three-year NSC research project, "Machine 
Learning Techniques for Object Categorization." In particular, we focus on three topics, including 
learning distance functions, training local classifiers, and combining multiple kernel learning 
(MKL) for dimensionality reduction. The three topics are closely related to each other, and are 
useful in tackling the problem of visual categorization. For learning (local) distance functions, we 
have introduced a new framework leveraging with a supervised learning technique called P-norm 
Push that is shown to be useful for object categorization. While the local learning techniques are 
flexible, their training time is usually a subject of concern. We thus address the issue of how to 
efficiently carry out learning a large number of local classifiers. Specifically, we propose to cast 
the local learning as a correlative multi-task learning problem, and develop a boosting-like 
scheme to accomplish the task. Finally, we describe our work on applying MKL to 
dimensionality reduction, and show its effectiveness for object categorization as well as a number 
of other computer vision applications. 
 
 
 
Keywords: Computer vision, machine learning, object categorization, local learning, multiple 
kernel learning, dimensionality reduction 
I. PREFACE
This NSC research report summarizes some of our recent research efforts and accomplishments
in carrying out the project supported by the NSC grant 97-2221-E-001-019-MY3. Our research
focus is on developing efficient machine learning techniques to address various computer vision
problems. In particular, we emphasize the area of object categorization, and present our results
in (i) learning an effective image distance measure, (ii) optimizing local learning efficiently, and
(iii) linking multiple kernel learning (MKL) with dimensionality reduction.
Learning a good image distance function is crucial in dealing with various vision problems. The
main idea of our approach is to look for right neighbors for the learning procedure. Our proposed
method combing the learning of a distance function with a supervised learning technique, called
P-Norm Push. That is, we consider a framework to rank neighbors by learning a local distance
function, and meanwhile to derive the local distance function by focusing on the high-ranked
neighbors. The two aspects of considerations can be elegantly coupled through a well-defined
objective function, motivated by P-Norm Push. While the local distance functions are learned
independently, they can be reshaped altogether so that their values can be directly compared.
When tackling the problem of object categorization, techniques based on local learning have
been shown to reasonably resolve the difficulties caused by the large variations in images from
the same object category. However, the high risk of overfitting and the heavy computational cost
in training local models often limit their applicability. To address these two unpleasant issues,
we propose to cast the multiple, independent training processes of local models as a correlative
multi-task learning problem, and design a new boosting algorithm to accomplish it.
Alternatively, adopting multiple image descriptors to more precisely characterize the data has
been a feasible way for improving performances in solving various visual learning tasks. These
representations are typically high dimensional and assume diverse forms. Thus finding a way to
transform them into a unified space of lower dimension generally facilitates the underlying tasks,
such as object recognition or clustering. In our research, we have established a new approach
that incorporates multiple kernel learning with dimensionality reduction (MKL-DR). While the
proposed framework is flexible in simultaneously tackling data in various feature representations,
the formulation itself is general and covers a broad range of dimensionality reduction techniques.
1
(a) (b) (c) (d) (e)
Fig. 2. Some examples of intraclass variations: Consider the Wild Cat class in Caltech-101. (a) A typical instance of a wild
cat for reference. (b) A wild cat in a different pose. (c) A wild cat under a different lighting condition. (d) A drawing of wild
cat with some unnatural features. (e) A wild cat example in which over 60% of the image belongs to the background.
as the L2 distance (Euclidean distance), or with a learned distance function that is optimized
over either the whole data or those within a manually specified region. Therefore, the selected
“neighbors” may include some undesired samples, as is illustrated in Figure 1. Our approach
toward addressing this problem is closely related to the “P-Norm Push” ranking formulation by
Rudin [35]. Specifically, for each labeled sample, the proposed technique learns a local distance
function and ranks its neighbors at the same time. As a result, we avoid manually predefining
the neighbors of a training sample in learning the respective local distance function, which will
turn out to be more precisely explaining the relationship between the sample and its high-ranked
neighbors (whose ranks are given by the distance function itself). These distance functions are
learned independently to fit local properties of each sample, and are subsequently reshaped
altogether to avoid data overfitting and achieve a unified effect. Classifying a testing sample can
then be conveniently done through these distance functions to find neighbors in training data
which are very likely in the same class with this testing sample.
B. Optimizing local learning efficiently
One of the major challenges in designing an object recognition system is to resolve the
difficulty arising from the large intraclass variations depicted in images from the same object
category. Such complications can be due to both intrinsic and extrinsic factors, and they signif-
icantly increase the complexity of separating data of one class from those of other classes or
the rest of the world. In Figure 3, examples of images with substantial intraclass variations are
shown to illustrate the scenario.
3
C. Multiple kernel learning and dimensionality reduction
The fact that most visual learning problems deal with high dimensional data has made dimen-
sionality reduction an inherent part of the current research. Besides having the potential for a more
efficient approach, working with a new space of lower dimension often can gain the advantage
of better analyzing the intrinsic structures in the data for various applications, e.g., [10], [22].
However, despite the great applicability, the existing dimensionality reduction methods suffer
from two main restrictions. First, many of them, especially the linear ones, require data to be
represented in the form of feature vectors. The limitation may eventually reduce the effectiveness
of the overall algorithms when the data of interest could be more precisely characterized in other
forms, such as bag-of-features [3], [28] or high order tensors [50]. Second, there seems to be
lacking a systematic way of integrating multiple image features for dimensionality reduction.
When addressing applications that no single descriptor can appropriately depict the whole dataset,
this shortcoming becomes even more evident. Alas, it is usually the case for addressing complex
visual learning tasks [14].
Aiming to relax the two above-mentioned restrictions, we introduce an approach called MKL-
DR that incorporates multiple kernel learning (MKL) into the training process of dimensionality
reduction (DR) algorithms. Our approach is inspired by the work of Kim et al. [23], in which
learning an optimal kernel over a given convex set of kernels is coupled with kernel Fisher
discriminant analysis (KFDA), but their method only considers binary-class data. Without the
restriction, MKL-DR manifests its flexibility in two aspects. First, it works with multiple base
kernels, each of which is created based on a specific kind of visual feature, and combines
these features in the domain of kernel matrices. Second, the formulation is illustrated with the
framework of graph embedding [50], which presents a unified view for a large family of DR
methods. Therefore the proposed MKL-DR is ready to generalize any DR methods if they are
expressible by graph embedding.
III. RELATED WORK
In this section, we discuss the current trends and the existing approaches related to our research.
In particular, we will divide them into three areas, including local learning, graph embedding
and multiple kernel learning.
5
are over Ω. An affinity matrix W = [wij ] ∈ RN×N is used to record the edge weights that
characterize the similarity relationships between training sample pairs. Then the optimal linear
embedding v∗ ∈ Rd can be obtained by solving
v
∗ = argmin
v⊤XDX⊤v=1, or
v⊤XL′X⊤v=1
v
⊤XLX⊤v, (1)
where X = [x1 x2 · · · xN ] is the data matrix, and L = diag(W ·1)−W is the graph Laplacian
of G. Depending on the property of a problem, one of the two constraints in (1) will be used
in the optimization. If the first constraint is chosen, a diagonal matrix D = [dij] ∈ RN×N is
included for scale normalization. Otherwise another complete graph G′ over Ω is required for the
second constraint, where L′ and W ′ = [w′ij ] ∈ RN×N are respectively the graph Laplacian and
affinity matrix of G′. The meaning of (1) can be better understood with the following equivalent
problem:
min
v
∑N
i,j=1 ||v
⊤
xi − v
⊤
xj ||
2wij (2)
subject to ∑Ni=1 ||v⊤xi||2dii = 1, or (3)∑N
i,j=1 ||v
⊤
xi − v⊤xj ||2w′ij = 1. (4)
The constrained optimization problem (2) implies that pairwise distances or distances to the origin
of projected data (in the form of v⊤x) are modeled by one or two graphs in the framework. By
specifying W and D (or W and W ′), Yan et al. [50] show that a set of dimensionality reduction
methods, such as PCA, LPP [22], LDA, and MFA [50] can be expressed by (1).
C. Multiple kernel learning
MKL refers to the process of learning a kernel machine with multiple kernel functions or
kernel matrices. Recent research efforts on MKL, e.g., [25], [34], [42] have shown that learning
SVMs with multiple kernels not only increases the accuracy but also enhances the interpretability
of the resulting classifier. Our MKL formulation is to find an optimal way to linearly combine
the given kernels. Suppose we have a set of base kernel functions {km}Mm=1 (or base kernel
matrices {Km}Mm=1). An ensemble kernel function k (or an ensemble kernel matrix K) is then
defined by
k(xi,xj) =
∑M
m=1 βmkm(xi,xj), βm ≥ 0 , (5)
K =
∑M
m=1 βmKm, βm ≥ 0 . (6)
7
is that for an arbitrary sample x, the exact value of f(x) is not important, but the order relation
between f(xm) and f(xn) is, where xm and xn are a positive-negative sample pair.
To achieve this purpose in P-Norm Push, a Height(·) function is introduced to each negative
sample xn, and is defined as the number of positive samples that are ranked beneath it. That is,
Height(xn) =
∑
m∈M
1[f(xm)≤f(xn)] . (8)
The idea of P-Norm Push is to push a negative sample with large height down from the top.
Hence a monotonically and rapidly increasing function from R+ to R+, g(r) = rp, p ≫ 1, is
adopted to give a proper price g(Height(xn)) on a negative sample xn, where p is adjustable
for different needs. With (8), we obtain the primal objective functional of P-Norm Push:
R(f) =
∑
n∈N
g(Height(xn)) (9)
=
∑
n∈N
(∑
m∈M
1[f(xm)≤f(xn)]
)p
. (10)
More details and discussions about Height(·) and the price function g can be found in [35].
2) Local learning with ranking: To explain how we use P-Norm Push to improve local
learning for object categorization, we first need to introduce some notations for the ease of our
discussions. We shall denote the ℓth training sample by Iℓ and the whole training set as {Iℓ}ℓ∈L.
The expression C(ℓ) is used to represent the function which extracts the index set such that
if m ∈ C(ℓ) then Im has the same class label of Iℓ. (We are now dealing with a multiclass
classification problem.) The notation Dℓ denotes the specific distance function learned for Iℓ,
and the distance from Iℓ to some Im is represented as Dℓ(Im). Note that in this setting Dℓ(Im)
is not necessarily equal to Dm(Iℓ), the distance from Im to Iℓ.
Consider now a given sample Iℓ. In our formulation, whenever the term “nearest neighbors”
is mentioned, it means that only a few samples that are in some sense close to Iℓ are considered.
In a classification task, these few neighbors are often assumed to have higher probabilities to
be in the same class with Iℓ. Furthermore, in local learning for classification, if the meanings
of the terms “neighbor,” “near,” and “far” with respect to Iℓ all come from a learned distance
function Dℓ, we in fact need not to worry about the exact distances from Iℓ to others, but we do
care about the relative magnitude between distances Dℓ(Im) and Dℓ(In) where m ∈ C(ℓ) and
n /∈ C(ℓ). One can easily check that this is indeed very similar to the ranking problem if we
9
1 2 3 4
5 6 7 8
9 10 11 12
13 14 15 16
17 18
19 20
21 22
(a) (b) (c) (d)
Fig. 4. We compute the HSV histograms in 22 overlapping regions in different scales. Each histogram is then normalized by
the total number of pixels in the region.
3) Distance function: The Dℓ in (11) is in a general form. If all samples in an application are
of the same size and an elementary distance function dℓi(Im) just returns the squared difference
between the two respective ith pixels of Iℓ and Im, then Dℓ is simply a squared Mahalanobis
distance with a diagonal covariance matrix. However, we choose to represent an image as a
bag-of-features, and set an elementary distance dℓi(Im) to be the smallest distance between the
ith feature of Iℓ and any detected feature of the same type in Im.
In our experiments we adopt two kinds of features, as they are often used in related work,
e.g., [17], [18], [52]. They are the geometric blur and HSV color histogram. We further apply
two different settings to each kind, and obtain four types of features. As in [18], the two types of
geometric blur features, termed as GB1 and GB2, are extracted under different scales with radii
of 42 and 70 pixels, respectively. The HSV histograms are also extracted under two schemes:
one is to compute the histograms on some 84 × 84-pixel patches (sampled as in GB1 and
GB2), the other is to compute the histograms in 22 regions extracted with a pyramid scheme
similar to that in [26] (see Figure 4). The two types of HSV histogram are named as HSV1 and
HSV2, respectively. We compute an HSV histogram in the same way as in [17] and extract the
geometric blur features by modifying the original version1 in [4]. Notice that for GB1, GB2, and
HSV1, the feature-to-set distance is calculated between features of the same type; for HSV2,
the feature-to-set distance is from distances between features at the same position.
4) Preliminary results: So far we have explained how to learn a distance function that is
suitable for ranking neighbors. To justify the formulation, we carry out an experiment for k-
1http://www.cs.berkeley.edu/∼aberg/demos/gb demo.tar.gz
11
fulfil the requirement of being capable of selecting a few good neighbors. We note that the
performance of the maximum margin method may be slightly underestimated since we have
not tuned the parameters λ in (15) exhaustively for each image. The other good property of
our method is that the learned distance functions are usually sparse. Although in our setting
an image Iℓ may be represented with up to 1222 features, the number of active features (i.e.,
with wℓi > 0) by our algorithm are usually less than 100. Some examples of active features are
illustrated in Figure 6, where the images have been transformed to gray level for display.
To further investigate the effectiveness of our neighbor ranking method, we also develop a
simple voting scheme for classification. Specifically, for each learned local distance function Dℓ,
ℓ ∈ L, we define a threshold
θℓ =
1
2
(Dℓ(Im) +Dℓ(In)), (16)
where, according to the neighbor list of Iℓ, In is the first sample not in the same class with Iℓ
and Im is the one ranked right above In. In testing with a new image I , each distance function
with its associated threshold behaves as a classifier. That is, if Dℓ(I) ≤ θℓ, then Dℓ supports that
I is in the same class with Iℓ. The final class label of I is voted by all classifiers. Images without
class label assigned are classified as background. We test this primitive method on the Caltech-
101 dataset by following the setting in Berg et al. [3]. Namely, for each class we randomly pick
15 images for training and another 15 for testing, and then switching their roles in the second
round. The results are presented in the first column of Table I. Even with such a rough rule,
the outcomes are already comparable with some of the recent novel techniques, e.g., [17], [26],
[27], [49].
5) Reshape distance functions for nearest neighbor classification: By far the distance func-
tions are learned in an independent manner, and are hardly compared with each other. Although
a learned distance function alone performs well in selecting the neighbors, it is not feasible to
decide if training sample Iℓ or Iℓ′ should be placed in a higher position on the neighbor list
of a new test sample I by comparing the values of Dℓ(I) and Dℓ′(I). In fact, these distance
functions act more like rankers, and it is not necessary to keep the exact values with them.
Considering that given a strictly increasing function q, transforming a distance function form
Dℓ(·) to D˜ℓ(·) = q(Dℓ(·)) would not affect the objective function (14). We can utilize such a
function to reshape these learned local distance functions altogether so that direct comparisons
13
Voting k-NN3 (k = 3) SVM-KNN50 (k = 50) SVM-KNN7 (k = 7)
GB1+GB2 59.75±1.55 67.87±0.42 (67.97±0.41) 70.05±0.31 68.94±0.72
GB1+GB2+HSV1 60.88±1.13 69.64±0.60 (68.43±0.33) 70.26±0.46 70.39±0.85
GB1+GB2+HSV1+HSV2 62.19±0.23 69.83±0.41 (69.15±0.25) 71.80±0.32 71.76±0.93
TABLE I
ACCURACY RATES ON CALTECH-101 DATASET WITH DIFFERENT FEATURES AND METHODS. VOTING: THE ROUGH
METHOD STATED IN OUR PRESENTATION. k-NN3: THE 3-NN CLASSIFIER BY A RESHAPED DISTANCE. SVM-KNN50:
SVM-KNN IN WHICH THE k = 50 NEAREST NEIGHBORS ARE SELECTED BY A RESHAPED DISTANCE. SVM-KNN7: THE
SAME WITH SVM-KNN50, BUT NOW k = 7 AND THE KERNEL USED IN THE SVM IS NOW DERIVED FROM OUR RESHAPED
DISTANCE. THE SCORES IN THE BRACKETS ARE RESULTED FROM THE SPEED-UP ALGORITHM. NOTICE THAT IN
SVM-KNN50 WE FOCUS ON THE BOOSTING ABILITY OF THE NEIGHBOR SELECTION SCHEME SO THAT IN SVM THE
KERNEL IS GENERATED BY ALL FOUR TYPES OF FEATURES.
minimized is
F˜ (a) =
∑
ℓ∈L
∑
n/∈C(ℓ)
 ∑
m∈C(ℓ)
1[amDm(Iℓ)≥anDn(Iℓ)]
p′ , (18)
where a = [a1, a2, ..., a|L|]T , and p′ ≫ 1 acts as the price function parameter like the p in (14).
6) Experiments: In this section we discuss some implementation details and the results derived
by the proposed neighbor ranking method. We again consider the Caltech-101 dataset, collected
by Fei-Fei et al. [15]. In all our experiments, from each class, we randomly pick 15 images
for training and another 15 images for testing. We then exchange their roles and calculate the
average recognition rate. Images with larger sizes are scaled down to around 60000 pixels while
preserving the aspect ratios. For each image, we extract the four types of features as described
in Section IV-A.3, and sample at most 400 features respectively for each type of GB1, GB2,
and HSV1. Hence an image is represented as a bag with at most 1200 + 22 features. Since the
Caltech-101 dataset contains 101 object classes and a background class, we thus obtain 1530
distance functions in the training procedure.
After the reshaping, we use a modified 3-NN classifier to assign class labels on test data.
Specifically, given a test image I , the classifier first arranges the order of the training images
15
B. Optimizing local learning efficiently
Our method focuses on efficiently learning sample-specific local models to improve the
accuracies on object recognition. In our framework, each local model is a classifier derived
by boosting, and has a special form such that the collection of them spreads as a manifold-like
structure in the resulting classifier space. By respecting this global structure of local classifiers,
we design a boosting algorithm to learn them jointly. The main contributions of our approach can
be characterized as follows: 1) We propose to cast the independent training procedures of local
classifiers as a multi-task learning problem. 2) We introduce a boosting algorithm that solves the
underlying multi-task problem with good efficiency, scales well to the data size, and produces
local classifiers with proper regularization and less redundancy. 3) We establish a new way of
carrying out multiple kernel learning (MKL) [25], [34] when several image descriptors are used
to account for the given data. It can yield recognition rates comparable to those reported by the
state-of-the-art MKL tools, e.g., [34].
The proposed approach is related to JointBoost [43] in the sense that multiple boosted clas-
sifiers are simultaneously generated under a specific form of multi-task learning. However, our
method can accomplish it more efficiently. We also note that our use of dyadic hypercuts,
introduced in Moghaddam and Shakhnarovich [31], as the weak learners for boosting is pivotal
to the formulation in that they can effectively capture useful information in a kernel matrix, and
elegantly connect multiple kernel learning with boosting.
We begin by specifying the notations used in this work, defining the problem of local learning
for object recognition, and describing its link to the multi-task learning.
1) Notations: Since the multi-class object recognition can always be reduced to an array of
two-class problems by adopting one-against-one or one-against-all rules, we assume a binary
dataset S = {xn ∈ X , yn ∈ ±1}Nn=1, where N is the data size and X denotes the input space.
In view of the complexity of visual object recognition, it is hard to find a universal descriptor
to well characterize the whole dataset. Instead, we consider representing each xn with totally
M kinds of different descriptors, i.e., xn = {xn,m ∈ Xm}Mm=1, and each descriptor is associated
with a distance measure dm : Xm ×Xm → R.
The useful data representations are often of high dimensional and in diverse forms, such as
vectors [32], histograms [7], bags of features [53], or pyramids [26]. To avoid the difficulties
caused by working with these varieties, we represent data under each descriptor by a kernel
17
and enjoys the convenience of working with different descriptors and distance measures. In our
discussions below, we will treat the training of each local classifier as a particular task. It follows
that the numbers of tasks and training samples are both N . For sake of clarity, hereafter we will
use subscript i as the index to tasks or classifiers, and n to the training samples.
3) Multi-task formulation: While local learning is effective for addressing complicated vision
problems like object recognition, care must be taken to ensure that the learning procedure has
been properly done for a given dataset. In particular, we pinpoint the following two critical
issues related to learning local classifiers {fi}Ni=1. First, each fi is learned with a small portion
of training data. When the number of weak learner candidates |H| is large or infinite, fi is at
the high risk of overfitting. Second, learning a local classifier for each training sample is indeed
an inefficient procedure. And the situation gets worse when dealing with a large dataset, but it
is often the case for vision applications.
To alleviate the two above-mentioned unfavorable effects, we view completing the independent
training processes of {fi}Ni=1 as a correlative multi-task learning problem. This is accomplished by
assuming these local classifiers share the same weak learners, but have their respective ensemble
coefficients, i.e.,
fi(x) =
T∑
t=1
αi,tht(x), for i = 1, 2, ..., N. (21)
With (21), all the classifiers will be learned jointly. We will later describe a systematic way for
constructing {fi}Ni=1 with multi-task learning in the next section. For now we give justifications
on why the two unfavorable effects can be eased by setting local classifiers in the form of (21).
Proper regularization. It is instructive to think as if all the local classifiers lie in a parametric
space induced by H. The space dimension is |H|, and the coordinates of a classifier (i.e., a
point) in the space are its ensemble coefficients over the weak learners. Consider now two local
classifiers fi and fi′ corresponding to two nearby samples xi and xi′ . According to (20), the
highly overlapping neighborhoods, wi and wi′ , should lead to the high similarity between fi
and fi′ . Hence, fi and fi′ are expected to be close in the classifier space. Extending the concept
to all the classifiers, it suggests that they would spread as a manifold-like structure. We instill
this property into regularizing the training process of each classifier. In other words, we learn all
the local classifiers simultaneously by respecting the underlying manifold structure. This could
lessen the instability (overfitting) problem when otherwise independently learning each classifier
19
Algorithm 1: Multi-task Boosting for Local Learning
Input : N tasks: task i involves weighted dataset
Si = {xn, yn, wi,n}
N
n=1.
Output: Local classifiers {fi}Ni=1, where
fi(x) =
∑T
t=1 αi,tht(x).
Initialize: w(1)i,n = wi,n, for i, n = 1, 2, ..., N .
for t← 1, 2, . . . , T do
1. Compute the cross-task data weights {w˜(t)n }Nn=1:
w˜
(t)
n =
∑N
i=1w
(t)
i,n.
2. Select the optimal dyadic hypercut ht:
ht = argminh
∑N
n=1 w˜
(t)
n · 1[h(xn)6=yn].
3. Compute task-wise weighted errors {ǫi,t}Ni=1:
ǫi,t =
∑N
n=1w
(t)
i,n · 1[ht(xn)6=yn].
4. Compute task-wise weighted accuracies {ci,t}Ni=1
ci,t =
∑N
n=1w
(t)
i,n · 1[ht(xn)=yn].
5. Set task-wise ensemble coefficients {αi,t}Ni=1:
αi,t = max(0,
1
2
ln
ci,t
ǫi,t
).
6. Update data weights {w(t+1)i,n }Ni,n=1:
w
(t+1)
i,n = w
(t)
i,n exp(−ynαi,tht(xn)).
where w(t)i,n denotes the weight of xn in task i at iteration t. In what follows, we describe the
details of running iteration t.
We start by defining the loss function of task i. At iteration t, fi =
∑t−1
τ=1 αi,τhτ is a linear
combination of the (t − 1) selected weak learners. With the exponential loss model [16], the
loss of fi with respect to Si is
L(fi, Si) =
N∑
n=1
wi,n exp(−ynfi(xn)). (23)
21
a) Novel sample prediction.: In the testing phase, given a new sample z, the proposed
technique will first find its nearest training sample, say, xi, and then use local classifier fi to
predict the label of z. Since z belongs to the neighborhood of xi, such a tactic is justified by
that fi is optimized to give good classification performances around xi.
Useful Properties. The proposed multi-task boosting is simple, easy to implement, and has
theoretic merit. At each iteration, it picks the optimal weak learner and computes its ensemble
coefficients by directly minimizing the total exponential loss as in (24). Although the selected
weak learners are shared cross tasks, it can be readily verified that the exponential loss of each
task is monotonically decreased in training. This property guarantees the convergence of our
algorithm.
Regarding the gain efficiency of leaning all the local classifiers, we elucidate this point with
two aspects of considerations. First, observe that according to (29), no matter how many tasks
sample xn is associated with, evaluating whether xn is misclassified by some weak learner h is
performed only once. Second, each selected weak learner is shared cross all the tasks, and the
degree of sharing in these tasks is adaptively controlled by the ensemble coefficients. It is in this
aspect that the proposed approach has the advantage over other related work, e.g., JointBoost
[43], where jointly training would quickly become infeasible due to an exponential growth in
the number of search hypotheses as the size of training dataset increases.
As is noted before, the algorithm maintains a 2-D weight array {wi,n} throughout the boosting
iterations. It records not only the difficulties of training data but also those of the tasks. With
iterative re-weighting, high weights will be gradually distributed to the difficult data and tasks,
and results in more emphases on them. The strategy ensures that all learning tasks will be
appropriately addressed.
One final remark about the multi-task boosting is that it iteratively learns classifiers by fusing
information from multiple kernels and thus inherently leads to an on-line and incremental way to
perform multiple kernel learning (MKL). In addition, compared with conventional optimization
techniques like semi-definite programming (SDP) [46], it does scale better when the size of data
is increased.
5) Experimental results: To evaluate the performances of the proposed method, we carry out
experiments of object recognition on two benchmark databases, Caltech-101 [14] and VOC 2007
23
1-NN SVM AdaBoost Ours
GB-Dist 42.4± 1.3 61.4± 0.8 61.5± 0.7 63.2 ± 0.8
GB 37.4± 0.9 57.6± 1.0 58.5± 0.7 59.7 ± 1.2
SIFT-Dist 49.6± 0.8 58.7 ± 0.9 57.5± 1.0 57.8± 0.7
SIFT-SPM 48.8± 0.7 56.1 ± 0.9 53.3± 0.8 55.2± 0.7
SS-Dist 31.7± 1.4 53.4± 1.0 56.1± 0.7 56.3 ± 0.9
SS-SPM 41.7± 0.9 53.9± 0.9 55.5± 0.7 56.9 ± 1.3
C2-SWP 22.0± 0.8 28.3 ± 0.8 26.0± 0.9 26.9± 1.0
C2-ML 37.7± 0.7 46.3± 1.0 44.8± 0.9 46.6 ± 0.6
PHOG 27.7± 1.0 43.9 ± 0.8 41.3± 1.0 42.9± 0.8
GIST 36.8± 1.1 48.7± 0.8 49.1± 1.0 51.2 ± 0.8
(a)
AdaBoost
SimpleMKL (local) Ours
All 74.3± 1.2 74.6± 1.3 75.8 ± 1.1
3.26× 102 sec. 1.87× 105 sec. 3.92× 103 sec.
(b)
TABLE II
RECOGNITION RATES [MEAN ± STD %] OF SEVERAL APPROACHES ON THE CALTECH-101 DATASET. (A) A KERNEL IS
TAKEN INTO ACCOUNT AT A TIME. (B) ALL KERNELS ARE JOINTLY CONSIDERED.
The distance matrices result from different parameter values of each descriptor are combined
in advance. It is used for ensuring that the resulting kernels individually reach their best perfor-
mances.
Quantitative results. Like in [3], [48], [53], we randomly pick 30 images from each of the 102
categories, and split them into two disjoint subsets: one contains Ntrain images per category, and
the other consists of the rest. The two subsets are respectively used for training and testing. The
whole evaluation process are repeated 20 times by using different splits between the training
and testing subsets. Recognition rates are measured in cases that Ntrain is set to 5, 10, 15, 20,
and 25.
We first investigate the effectiveness of each kernel by comparing our method with the one-
25
70.5± 0.7%, 77.5± 0.7%, and 79.1± 2.1% when Ntrain is set to 5, 10, 20, and 25 respectively.
For the case that Ntrain is 15, the recognition rate 75.8% by our method is either better or
comparable to those by other published systems, e.g., [5], [7], [18], [20], [26], [53].
b) VOC 2007: The second set of our experiments is performed on the dataset provided
by the VOC 2007 classification challenge, which serves as a benchmark for comparing image
classification methods in dealing with realistic scenes.
Dataset. The VOC 2007 dataset contains 20 object categories, including indoor objects, vehicles,
animals, and people. Each category consists of 195 ∼ 4192 images, and most of them are
of similar sizes. Unlike Caltech-101, objects in the images are neither centrally located nor
with similar scales. Besides, more than one kind of objects can be present in an image. The
overall learning task contains 20 binary classification problems, each of which is to predict the
presence/abcense of objects from a certain category.
Descriptors and kernels. We use 18 distance matrices generated from the combinations of six
kinds of descriptors and three kinds of spatial pyramids. The image descriptors are
• SIFT / GB / SS: We extract the SIFT features from a densely sampled grid over multiple
scales, and quantize them into 4000 visual words. The χ2 distance is adopted as the distance
function. Following the same procedure, we build the other two using the geometric blur
and self-similarity features.
• TC-SIFT: The same as SIFT, except that SIFT features are computed over three normalized
RGB channels separately [44].
• GIST: All images are resized to resolution 128× 128 before performing feature extraction.
Then the L2 distance is used as the distance function.
• C2-ML: This is constructed with the C2 feature by Mutch and Lowe [32], and the L2
distance function.
Three kinds of spatial pyramids, 1 × 1 (whole image), 2 × 2 (image quarters) [26], and 1 × 3
(horizontal bars), are considered to exploit the spatial information. Coupling the descriptors and
the spatial pyramids yields 18 distance matrices. However, using all the distance matrices will
lead to large memory consumption, and it can be resolved by using only one average distance
matrix for each descriptor. To address the possibly large variations due to using different distance
measures, the distance matrices are normalized by dividing their respective standard deviation.
27
precision-recall curve is built on the prediction values of all test images, and the performance is
measured by the average precision (AP), which is proportional to the area under the precision-
recall curve. Finally the average of 20 APs is used for the comparison.
In Table III, we report the results of comparing our method with SVM and SimpleMKL. When
training with one individual kernel at a time, our method performs slightly better than SVM on
the Train set, and comparably as SVM on the Train+Val set. It shows that the learned local models
give the same or better results than a global one does. When learning with multiple kernels, our
method achieves average AP 59.3% on the Train+Val set, and yields significant improvements
to those learned on each individual kernel. The performance gain in the classification accuracy
supports that the use of various distances can complement each other and the concept of local
models can better capture the structures of complex data. When SimpleMKL is applied to the
same kernels to learn a global model, the resulting average AP is 57.3%.
Table IV summarizes the per-class results on the Train+Val set of our method, SimpleMKL,
the top three (denoted respectively as INRIA, XRCE, and TKK) in VOC Challenge 2007 [13],
and van Gemert et al. [45], which has produced by far the best results for the dataset. Our method
achieves average AP 59.3%, and performs the best in 4 out of 20 categories. The performance by
the proposed approach is consistently better than that of SimpleMKL, while it is also comparable
with that by INRIA and meanwhile falls slightly behind the average AP 60.5% reported in [45].
C. Linking MKL with dimensionality reduction
To establish the proposed method, we first discuss the construction of a set of base kernels from
multiple features, and then explain how to integrate these kernels for dimensionality reduction.
Finally, we design an optimization procedure to learn the projection for dimensionality reduction.
1) Kernel as a unified feature representation: Consider a dataset Ω of N samples, and M
kinds of descriptors to characterize each sample. Let Ω = {xi}Ni=1, xi = {xi,m ∈ Xm}Mm=1,
and dm : Xm × Xm → 0 ∪ R+ be the distance function for data representation under the mth
descriptor. The domains resulting from distinct descriptors, e.g. feature vectors, histograms, or
bags of features, are in general different. To eliminate these varieties in representation, we
represent data under each descriptor as a kernel matrix. There are several ways to accomplish
this goal, such as using RBF kernel for data in the form of vector, or pyramid match kernel [21]
for data in the form of bag-of-features. We may also convert pairwise distances between data
29
φ(xi) and projection v would appear exclusively in the form of vTφ(xi). Hence, it suffices to
show that in MKL-DR, vTφ(xi) can be evaluated via the kernel trick:
v
Tφ(xi) =
∑N
n=1
∑M
m=1 αnβmkm(xn,xi) = α
T
K
(i)β where (34)
α =

α1
.
.
.
αN
 ∈ RN ,β =

β1
.
.
.
βM
 ∈ RM ,K(i) =

K1(1, i) · · · KM(1, i)
.
.
.
.
.
.
.
.
.
K1(N, i) · · · KM(N, i)
 ∈ RN×M .
With (2) and (34), we define the constrained optimization problem for 1-D MKL-DR as
follows:
min
α,β
∑N
i,j=1 ||α
T
K
(i)β −αTK(j)β||2wij (35)
subject to ∑Ni,j=1 ||αTK(i)β −αTK(j)β||2w′ij = 1, (36)
βm ≥ 0, m = 1, 2, ...,M . (37)
The additional constraints in (37) are included to ensure the the resulting kernel K in MKL-DR
is a non-negative combination of base kernels. We leave the details of how to solve (35) until the
next section, where using MKL-DR for finding a multi-dimensional projection V is considered.
3) Optimization: Observe from (34) that the one-dimensional projection v of MKL-DR is
specified by a sample coefficient vector α and a kernel weight vector β. The two vectors
respectively account for the relative importance among the samples and the base kernels. To
generalize the formulation to uncover a multi-dimensional projection, we consider a set of P
sample coefficient vectors, denoted by
A = [α1 α2 · · · αP ]. (38)
With A and β, each 1-D projection vi is determined by a specific sample coefficient vector αi
and the (shared) kernel weight vector β. The resulting projection V = [v1 v2 · · · vP ] will map
samples to a P -dimensional space. Analogous to the 1-D case, a projected sample xi can be
written as
V ⊤φ(xi) = A
⊤
K
(i)β ∈ RP . (39)
31
a) On optimizing A:: By fixing β, the optimization problem (40) is reduced to
min
A
trace(A⊤SβWA)
subject to trace(A⊤SβW ′A) = 1
(41)
where
S
β
W =
∑N
i,j=1wij(K
(i) −K(j))ββ⊤(K(i) −K(j))⊤, (42)
S
β
W ′ =
∑N
i,j=1w
′
ij(K
(i) −K(j))ββ⊤(K(i) −K(j))⊤. (43)
The problem (41) is a trace ratio problem, i.e., minA trace(A⊤SβWA)/trace(A⊤SβW ′A). A closed-
form solution can be obtained by transforming (41) into the corresponding ratio trace prob-
lem, i.e., minA trace[(A⊤SβW ′A)−1(A⊤S
β
WA)]. Consequently, the columns of the optimal A∗ =
[α1 α2 · · ·αP ] are the eigenvectors corresponding to the first P smallest eigenvalues in
S
β
Wα = λS
β
W ′α. (44)
b) On optimizing β:: By fixing A, the optimization problem (40) becomes
min
β
β⊤SAWβ
subject to β⊤SAW ′β = 1 and β ≥ 0
(45)
where
SAW =
∑N
i,j=1wij(K
(i) −K(j))⊤AA⊤(K(i) −K(j)), (46)
SAW ′ =
∑N
i,j=1w
′
ij(K
(i) −K(j))⊤AA⊤(K(i) −K(j)). (47)
The additional constraints β ≥ 0 cause that the optimization to (45) can no longer be formulated
as a generalized eigenvalue problem. Indeed it now becomes a nonconvex quadratically con-
strained quadratic programming (QCQP) problem, and is known to be very difficult to solve. We
instead consider solving its convex relaxation by adding an auxiliary variable B of size M ×M :
min
β,B
trace(SAWB) (48)
subject to trace(SAW ′B) = 1, (49)
e
T
mβ ≥ 0, m = 1, 2, ...,M, (50) 1 βT
β B
  0, (51)
33
Algorithm 2: MKL-DR
Input : A DR method specified by two affinity matrices W and W ′ (cf. (2));
Various visual features expressed by base kernels {Km}Mm=1 (cf. (31));
Output: Sample coefficient vectors A = [α1 α2 · · ·αP ]; Kernel weight vector β;
Make an initial guess for A or β;
for t← 1, 2, . . . , T do
1. Compute SβW in (42) and SβW ′ in (43);
2. A is optimized by solving the generalized eigenvalue problem (44);
3. Compute SAW in (46) and SAW ′ in (47);
4. β is optimized by solving optimization problem (48) via semidefinite programming;
return A and β;
multiple image descriptors for complex recognition tasks. Since the images in the dataset are not
of the same size, we resize them to around 60,000 pixels, without changing their aspect ratio.
To implement MKL-DR for recognition, we need to select some proper graph-based DR
method to be generalized and a set of image descriptors, and then derive (in our case) a pair of
affinity matrices and a set of base kernels. The details are described as follows.
Image descriptors. For the Caltech-101 dataset, we consider seven kinds of image descriptors
that result in the seven base kernels (denoted below in bold and in abbreviation):
GB-1/GB-2: From a given image, we randomly sample 300 edge pixels, and apply geometric
blur descriptor [3] to them. With these image features, we adopt the distance function, as is
suggested in equation (2) of the work by Zhang et al. [53], to obtain the two dissimilarity-based
kernels, each of which is constructed with a specific descriptor radius.
SIFT-Dist: The base kernel is analogously constructed as in GB-2, except now the SIFT
descriptor [28] is used to extract features.
SIFT-Grid: We apply SIFT with three different scales to an evenly sampled grid of each
image, and use k-means clustering to generate visual words from the resulting local features of
all images. Each image can then be represented by a histogram over the visual words. The χ2
distance is used to derive this base kernel via (31).
C2-SWP/C2-ML: Biologically inspired features are also considered here. Specifically, both
35
Quantitative results. Our experiment setting follows the one described by Zhang et al. [53].
From each of the 102 classes, we randomly pick 30 images where 15 of them are included for
training and the other 15 images are used for testing. To avoid a biased implementation, we redo
the whole process of learning by switching the roles of training and testing data. In addition,
we also carry out the experiments without using the data from the the background class, since
such setting is adopted in some of the related works, e.g., [17]. Via MKL-DR, the data are
projected to the learned space, and the recognition task is accomplished there by enforcing the
nearest-neighbor rule.
Coupling the seven base kernels with the affinity matrices of LDA and LDE, we can respec-
tively derive MKL-LDA and MKL-LDE using Algorithm 2. Their effectiveness is investigated
by comparing with KFD (kernel Fisher discriminant) [30] and KLDE (kernel LDE) [10]. Since
KFD considers only one base kernel at a time, we implement two strategies to take account
of the classification outcomes from the seven resulting KFD classifiers. The first is named as
KFD-Voting. It is constructed based on the voting result of the seven KFD classifiers. If there
is any ambiguity in the voting result, the next nearest neighbor in each KFD classifier will be
considered, and the process is continued until a decision on the class label can be made. The
second is termed as KFD-SAMME. By viewing each KFD classifier as a multi-class weak learner,
we boost them by SAMME [54], which is a multi-class generalization of AdaBoost. Analogously,
we also have KLDE-Voting and KLDE-SAMME.
We report the mean recognition rates and the standard deviation in Table V. First of all,
MKL-LDA achieves a considerable performance gain of 14.6% over the best recognition rate
by the seven KFD classifiers. On the other hand, while KFD-Voting and KFD-SAMME try to
combine the separately trained KFD classifiers, MKL-LDA jointly integrates the seven kernels
into the learning process. The quantitative results show that MKL-LDA can make the most of
fusing various feature descriptors, and improves the recognition rates from 68.4% and 71.2% to
74.6%. Similar improvements can also be observed for MKL-LDE.
The recognition rates 74.6% in MKL-LDA and 75.3% in MKL-LDE are favorably comparable
to those by most of the existing approaches. In [21], Grauman and Darrell report a 50%
recognition rate based on the pyramid matching kernel over data in bag-of-features representation.
By combing shape and spatial information, SVM-KNN of Zhang et al. [53] achieves 59.05%.
In Frome et al. [17], the accuracy rate derived by learning the local distances, one for each
37
except that by Varma and Ray [47], in which several robust features and a more sophisticated
learning scheme are used. As our framework has the flexibility in easily adopting more features
in the distance function, we would explore the effects of adopting those features used in [47]
for our future work.
On improving the efficiency of local learning, we have introduced an efficient local learning
approach to resolving the difficulties in object recognition caused by large intraclass variations.
We cast multiple, independent training processes of local classifiers to a correlative multi-
task learning problem, and develop a boosting-based algorithm to accomplish it. The proposed
technique is comprehensively evaluated with two benchmark datasets. The recognition rates in
both datasets are comparable to those yielded by the respective state-of-the-art approaches. Our
method can be considered as a general multi-task learning tool for vision applications where
multiple correlative classifiers are required, such as multi-view face detection, multi-part object
tracking, or user-dependent media ranking. The framework also provides a new way to carry out
multiple kernel learning in an incremental manner.
On combining MKL with dimensionality reduction, we have proposed the MKL-DR frame-
work, The MKL-DR technique is useful as it has the advantage of learning a unified space of low
dimension for data in multiple feature representations. Our approach is general and applicable to
most of the graph-based DR methods, and improves their performance. Such flexibilities allow
one to make use of more prior knowledge for effectively analyzing a given dataset, including
choosing a proper set of visual features to better characterize the data, and adopting a graph-based
DR method to appropriately model the relationship among the data points. On the other hand,
via integrating with a suitable DR scheme, MKL-DR can extend the multiple kernel learning
framework to address not just the supervised learning problems but also the unsupervised and
the semisupervised ones.
REFERENCES
[1] R. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data. JMLR,
2005.
[2] C. Atkeson, A. Moore, and S. Schaal. Locally weighted learning. Artificial Intelligence Review, 1997.
[3] A. Berg, T. Berg, and J. Malik. Shape matching and object recognition using low distortion correspondences. In CVPR,
pages I: 26–33, 2005.
[4] A. Berg and J. Malik. Geometric blur and template matching. In CVPR, pages I:607–614, 2001.
39
[33] A. Oliva and A. Torralba. Modeling the shape of the scene: A holistic representation of the spatial envelope. IJCV, 2001.
[34] A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet. More efficiency in multiple kernel learning. In ICML, 2007.
[35] C. Rudin. Ranking with a p-norm push. In COLT, pages 589–604, 2006.
[36] S. Salzberg, A. Delcher, D. Heath, and S. Kasif. Best-case results for nearest-neighbor learning. PAMI, 17(6):599–608,
June 1995.
[37] H. Schneiderman and T. Kanade. A statistical method for 3d object detection applied to faces and cars. In CVPR, 2000.
[38] M. Schultz and T. Joachims. Learning a distance metric from relative comparisons. In NIPS, Cambridge, MA, 2004. MIT
Press.
[39] T. Serre, L. Wolf, and T. Poggio. Object recognition with features inspired by visual cortex. In CVPR, 2005.
[40] G. Shakhnarovich, P. Viola, and T. Darrell. Fast pose estimation with parameter-sensitive hashing. In ICCV, pages 750–757,
2003.
[41] E. Shechtman and M. Irani. Matching local self-similarities across images and videos. In CVPR, 2007.
[42] S. Sonnenburg, G. Ra¨tsch, C. Scha¨fer, and B. Scho¨lkopf. Large scale multiple kernel learning. JMLR, 2006.
[43] A. Torralba, K. Murphy, and W. Freeman. Sharing visual features for multiclass and multiview object detection. PAMI,
2007.
[44] K. van de Sande, T. Gevers, and C. Snoek. Evaluation of color descriptors for object and scene recognition. In CVPR,
2008.
[45] J. C. van Gemert, C. J. Veenman, A. W. M. Smeulders, and J. M. Geusebroek. Visual word ambiguity. PAMI, 2009.
[46] L. Vandenberghe and S. Boyd. Semidefinite programming. SIAM Review, 1996.
[47] M. Varma and D. Ray. Learning the discriminative power-invariance trade-off. In ICCV, October 2007.
[48] M. Varma and D. Ray. Learning the discriminative power-invariance trade-off. In ICCV, 2007.
[49] G. Wang, Y. Zhang, and F. Li. Using dependent regions for object categorization in a generative framework. In CVPR,
pages II: 1597–1604, 2006.
[50] S. Yan, D. Xu, B. Zhang, H. Zhang, Q. Yang, and S. Lin. Graph embedding and extensions: A general framework for
dimensionality reduction. PAMI, 2007.
[51] G.-J. Zhang, J.-X. Du, D.-S. Huang, T.-M. Lok, and M. R. Lyu. Adaptive nearest neighbor classifier based on supervised
ellipsoid clustering. In FSKD, volume 4223 of Lecture Notes in Computer Science, pages 582–585. Springer, 2006.
[52] H. Zhang, A. Berg, M. Maire, and J. Malik. Svm-knn: Discriminative nearest neighbor classification for visual category
recognition. In CVPR, pages II: 2126–2136, 2006.
[53] H. Zhang, A. Berg, M. Maire, and J. Malik. Svm-knn: Discriminative nearest neighbor classification for visual category
recognition. In CVPR, 2006.
[54] J. Zhu, S. Rosset, H. Zou, and T. Hastie. Multi-class adaboost. Technical report, Dept. of Statistics, University of Michigan,
2005.
41
Co-occurrence Random Forests for Object
Localization and Classiﬁcation
Yu-Wu Chu and Tyng-Luh Liu
Institute of Information Science, Academia Sinica, Taipei 115, Taiwan
{ywchu,liutyng}@iis.sinica.edu.tw
Abstract. Learning techniques based on random forests have been lately
proposed for constructing discriminant codebooks for image classiﬁcation
and object localization. However, such methods do not generalize well to
dealing with weakly labeled data. To extend their applicability, we con-
sider incorporating co-occurrence information among image features into
learning random forests. The resulting classiﬁers can detect common pat-
terns among objects of the same category, and avoid being trapped by
large background patterns that may sporadically appear in the images.
Our experimental results demonstrate that the proposed approach can ef-
fectively handle weakly labeled data andmeanwhile derive amore discrim-
inant codebook for image classiﬁcation.
1 Introduction
Recent research eﬀorts on object classiﬁcation and localization have supported
that, especially for the latter, the bounding box or shape mask information is
useful for accomplishing the related tasks. However, while providing such infor-
mation for a large dataset is laborious, it has been noticed that the background
clutter within a bounding box encompassing a target object can be distracting
and cause misclassiﬁcations. The situation becomes even worse when the pro-
vided data are only weakly labeled. That is, an image is marked as positive
(without any bounding box information) for some object class if it does contain
such an object of interest. Otherwise, it is treated as a negative one. To alleviate
this unfavorable eﬀect, exploring the co-occurrence evidence has been proposed,
e.g., in [1] and shown to be a feasible approach. In this work, we propose a novel
implementation of random forests that eﬀectively incorporates the co-occurrence
information to perform proper splits and yield a discriminant codebook for both
object classiﬁcation and localization.
1.1 Related Work
A number of object localization schemes rely on that the target location in each
of the training data is annotated with a bounding box [2], [3], [4]. The proce-
dure is typically performed in the following three steps. First, interest points
are detected within object areas of all images. Second, for each object category,
a codebook is generated by partitioning the feature space into several regions.
H. Zha, R.-i. Taniguchi, and S. Maybank (Eds.): ACCV 2009, Part III, LNCS 5996, pp. 621–632, 2010.
c© Springer-Verlag Berlin Heidelberg 2010
Co-occurrence Random Forests for Object Localization and Classiﬁcation 623
1.2 Our Approach
Our method exploits the eﬃciency of random forests and the co-occurrence in-
formation between images to alleviate the eﬀects of noisy labels. The key idea
is to uncover common patterns among most of the images rather than speciﬁc
patterns in a few images, as illustrated in Fig. 1. As a result, we expect to de-
rive a codebook that is discriminant for both image classiﬁcation and object
localization on weakly labeled formulations.
2 Random Forests (RF)
In this section, we give a brief review of random forests. A random forest is an
ensemble of decision trees with randomness. Each decision tree is constructed by
recursively splitting each internal node into left and right nodes until the stop
criterion is met, e.g., there are too few examples in the node, all data in the node
have the same class label c, or the desired depth L is reached. At each internal
node n, a split function f partitions the training data set Sn into two subsets Sl
and Sr:
Sl = {v ∈ Sn|f(v) ≤ 0}, (1)
Sr = Sn\Sl. (2)
When growing the tree, we choose a split function at each node n to maximize
the score function that measures the degree of separation of the training data.
Following [12], the score function S is deﬁned as the normalized information
gain:
S(n, f) =
2IC,f (n)
HC(n) + Hf (n)
, (3)
where HC(n) is the class entropy of node n, Hf (n) is the split entropy of node
n, and IC,f (n) is the mutual information between the class distribution and the
given split function f of node n. They are deﬁned respectively as follows:
HC(n) = −
∑
c
p(c|n) log p(c|n), (4)
Hf (n) = −
∑
p∈{l,r}
p(Sp|n) log p(Sp|n), (5)
IC,f (n) =
∑
c
∑
p∈{l,r}
p(c, Sp|n) log p(c, Sp|n)
p(c|n)p(Sp|n) . (6)
Moreover, the randomness of random forests can be drawn from two aspects:
training data, and split functions. First, each decision tree is grown with diﬀerent
subset of the training samples. Second, the split function at each internal node
is generated randomly to divide the data into two subsets, and the best one of
them is chosen according to the score function in (3).
Co-occurrence Random Forests for Object Localization and Classiﬁcation 625
matched, otherwise m = 0. In other words, the mapping indicates co-occurrence
relations between two sets, Xi and Xj . The match function g is qualiﬁed by
a cost function C, for example, the sum of the distances between match pairs.
Assume the function g∗ is the minimum cost. We deﬁne a feature pair (A,B) ∈
{(C,D)|g∗i,j(C,D) = 1, C ∈ Xi, D ∈ Xj} as a co-occurrence feature pair (CoFP).
A CoFP (A,B) is the best mapping between two sets according to the cost func-
tion C. In this paper, we assume that CoFPs of two sets of features are one-to-one
mapping. A feature on one set maps at most one feature on another set, and the
number of match pairs is as many as possible.
As illustrated in Fig. 2a, each CoFP can be divided into three groups: self-
match pairs, intra-class-match pairs, and inter-class-match pairs. A self-match
pair is a CoFP from the same image. In other words, the interest point matches
to itself. An intra-class-match pair is a CoFP from two diﬀerent images which are
labeled as the same class. An inter-class-match pair is a CoFP from two diﬀerent
images which are labeled as diﬀerent classes. Therefore, CoFPs are labeled as
following. If a CoFP comes from images in the same class c, i.e. a self-match pair
or an intra-class-match pair, it is labeled as class c. Otherwise, it is labeled as φ,
which denotes a mismatch pair, i.e. an inter-class-match pair. The information,
i.e the CoFPs and their labels, is applied to any classiﬁcation model to determine
the CoFP belongs to the speciﬁc class c or the background clutter φ.
However, the cost function is aﬀected by the selection of the distance function
between features which would aﬀect CoFPs directly. The Euclidean distance is
a good choice, but it might suﬀer from the curse of dimensionality [13]. Besides,
ﬁnding the optimal function g∗ is time-consuming, while the feature dimension
and/or the number of features in each image are large. We instead incorporate
learning the distance function into the classiﬁcation model.
3.2 Implicit Co-occurrence Feature Pairs
As aforementioned, searching the optimal match function g∗ is time-consuming.
Therefore, we utilize tree structures to reduce search space and make this prob-
lem easier. We assume that a similarity or decision tree has been built. In the
tree, if features fall into the same node, we could say they are similar to each
other. In other words, each pair of features in the node is a candidate of the
CoFPs. Besides, it is easy to prove that the candidates in the node are the sub-
set of the candidates in its parent node. Because we assume that two sets of
features are one-to-one mapping, the candidates of the CoFPs can be removed
if they are the candidates in its descendant nodes. As a result, the CoFPs are
discovered by the bottom-up strategy. In addition, the number of CoFPs in the
node nˆ is computed eﬃciently by the following equation
|Snˆ| =
∑
i,j
min(|Sn ∩Xi|, |Sn ∩Xj |), (8)
where | · | is the number of elements in the set, Sn is the set of the features in
the node n, Xi is the set of the features in the image i, and Snˆ is the set of
the CoFPs in the node nˆ. Note that elements in the node n are features, and
Co-occurrence Random Forests for Object Localization and Classiﬁcation 627
p(c|nˆ) =
∑
pˆ∈{lˆ,rˆ,cˆ}
p(c, Spˆ|nˆ), (16)
p(Spˆ|nˆ) =
∑
c∈{1,··· ,C,φ}
p(c, Spˆ|nˆ), (17)
where | · | is the number of elements in the set which can be computed eﬃciently
by the equation (8), and wpˆ is the weight associated with the node pˆ. wpˆ is
proportional to the inverse of the diameter of the node. In our experiments, we
set wpˆ as wlˆ = wrˆ = 2wcˆ. Note that if we only consider self-match pairs, our
method is equivalent to the traditional random forests. After a co-occurrence
random forest is learned, the ﬁnal classiﬁcation result of a test sample x can
be computed using (7). In the next two sections, we describe how to apply the
CoRF to solve object localization and image classiﬁcation problems.
4 Object Localization
For object localization, we use the voting approach to generate the ﬁnal prob-
ability map. The probability of each patch x in test image I is calculated by
equation (7). Then, the probability of each pixel i is ﬁgured out by
p(c|i, I) = 1
N
∑
{j|i∈Rxj }
p(c|Nxj), (18)
where xj is a patch in the test image, Rxj represents the region that is covered
by the patch xj , and N is the number of patches that cover the pixel i. Therefore,
we could get the probability map.
5 Image Classiﬁcation
For image classiﬁcation, we consider support vector machines (SVMs) as the
classiﬁer with the linear kernel [10] or the pyramid match kernel [11], [14] to
measure the similarity between two images. Each node in the random forests
denotes a visual word, and each image is represented by the histogram of hier-
archical visual words. Note that, for the linear kernel, we only use the leaf nodes
as the visual words without utilizing the hierarchical structure of the trees.
The pyramid match kernel [14] is deﬁned as
K˜(Xi, Xj) =
K(Xi, Xj)√
K(Xi, Xi)K(Xj, Xj)
, (19)
where
K(Xi, Xj) =
L∑
l=1
wlNl (20)
Co-occurrence Random Forests for Object Localization and Classiﬁcation 629
Table 1. Pixel labeling results on Graz-02 (The pixel precision-recall EER)
Bike RF CoRF RF.GT CoRF.GT [17].GT [4].GT
L=8 53.9 56.7 60.0 58.9
61.8 66.4L=11 54.6 57.1 60.7 59.1
L=14 54.7 57.1 61.0 59.0
Car RF CoRF RF.GT CoRF.GT [17].GT [4].GT
L=8 44.0 44.8 52.4 54.9
53.8 54.7L=11 42.8 46.4 52.8 56.0
L=14 40.1 46.2 51.3 56.8
Person RF CoRF RF.GT CoRF.GT [17].GT [4].GT
L=8 30.5 35.1 39.4 39.1
44.1 51.4L=11 32.6 34.5 38.8 40.4
L=14 33.0 34.4 38.2 40.8
6.1 Object Localization
For object localization, train/test split is the same as the one in [4], [17]. For
each category, the ﬁrst 150 odd numbered images are used for training, and
the ﬁrst 150 even numbered images are used for testing. The performance is
measured as the pixel level classiﬁcation rate at equal error rate, which is the
point that the recall meets the precision. The results are shown in Table 1. The
number in the leftest column indicates the maximum tree depth L. The suﬃx
GT means that the method is trained with ground truth shape masks. In the
most of the cases, our method outperforms the traditional random forests. In
Fig. 3, we observe that the co-occurrence information guides the random forest
algorithm to select more meaningful features, and alleviates the eﬀect of weakly
labeled data. Moreover, the pixel level probability map which is produced by
our method is more compact than the conventional manner. However, there is
still a large gap of the accuracy between working with ground truth data and
weakly labeled data for object localization. More clues should be exploited, e.g.
spatial conﬁguration [18], to reduce the gap.
6.2 Image Classiﬁcation
For image classiﬁcation, we use the same train/test split as the one in [10]. For
each category, the ﬁrst 150 even numbered images are used for training, and
the ﬁrst 150 odd numbered images are used for testing. The performance is
measured as the image classiﬁcation rate at equal error rate, which is the point
that the false positive rate meets the false negative rate. We perform the image
classiﬁcation with diﬀerent kernels: the linear kernel (LK), the intersection kernel
(IK), and the pyramid match kernel (PMK). The intersection kernel is similar to
the pyramid match kernel. The diﬀerence between them is that the intersection
kernel only uses the leaf nodes as the codebook. The suﬃx p represents that
the histogram intersection is weighted by the class prior p(c|n). The PMK1 uses
the weight in equation (20) that is suggested by Shotton et el. [11], and the
Co-occurrence Random Forests for Object Localization and Classiﬁcation 631
Table 2. Image classiﬁcation results on Graz-02 (EER)
Bike LK IK IK.p PMK1 PMK1.p PMK2 PMK2.p
RF 83.3 82.7 83.2 80.8 81.3 82.2 82.7
CoRF 85.2 82.7 83.0 79.2 82.3 82.8 83.2
Moosmann et el. [10] 84.4 - - - - - -
Car LK IK IK.p PMK1 PMK1.p PMK2 PMK2.p
RF 80.8 80.8 81.6 78.1 78.8 80.8 81.6
CoRF 80.1 82.8 83.6 79.7 81.2 82.7 83.5
Moosmann et el. [10] 79.9 - - - - - -
Person LK IK IK.p PMK1 PMK1.p PMK2 PMK2.p
RF 88.5 87.0 87.2 86.0 86.7 87.2 87.3
CoRF 86.5 87.5 88.0 85.8 87.5 87.0 88.2
Moosmann et el. [10] - - - - - - -
PMK2 uses the weight we suggest in section 5. The results are shown in Table
2. Some conclusions could be derived from this table. In all cases, the class prior
improves the results as in [11]. This also veriﬁes the result mentioned in [19].
The results of PMK2 are similar to the results of IK and both of them are better
than the results of PMK1. In other words, the leaf node has more discriminant
power than the internal node. In most of the cases, our method is better than the
conventional random forests. Again, the co-occurrence information could guide
the random forest algorithm to discover the proper patterns.
7 Conclusions and Future Work
We have proposed a learning technique to exploit the co-occurrence information
when image data are only weakly labeled. As demonstrated, by more eﬀectively
identifying common patters among images of the same class, CoRF can learn
a more discriminant codebook, and locate the object position more accurately.
Consequently, our method can accomplish better accuracies in both object lo-
calization and image classiﬁcation. However, there is still a signiﬁcant gap of the
accuracy between working with ground truth data and weakly labeled data for
object localization than that for image classiﬁcation [10], [11]. To bridge this gap
would be the main focus of our future work.
Acknowledgments. The work is supported in part by NSC grants 95-2221-E-
001-031-MY3 and 97-2221-E-001-019-MY3.
References
1. Bosch, A., Zisserman, A., Munoz, X.: Image classiﬁcation using random forests and
ferns. In: ICCV (2007)
2. Lampert, C.H., Blaschko, M.B., Hofmann, T.: Beyond sliding windows: Object
localization by eﬃcient subwindow search. In: CVPR (2008)
2009 年國際電腦視覺會議出席報告 
 
報告人姓名(中文)：林彥宇 
(英文)：Yen-Yu Lin 
服務機構及職稱: 國立台灣大學資訊工程學研究所 博士生 
會議時間：民國 98 年 9 月 27 號至 10 月 4 號 
會議地點：日本京都 
會議名稱(中文)：第十二屆國際電腦視覺會議 
(英文)：The Twelfth IEEE International Conference on Computer Vision 
(ICCV) 
發表論文(中文)：基於有效率的區域學習之物件識別 
(英文)：Efficient Discriminative Local Learning for Object Recognition 
 
一. 參加會議經過 
 
這屆國際電腦視覺會議在日本京都(Kyoto, Japan)所舉辦。開會的日期是2009年9
月27號至10月4號。京都是一個環境十分優美的城市，它是日本的古都，因此擁
有相當豐富的歷史遺跡，也是日本傳統文化的重鎮。主要會議(main conference)
在京都國際會議中心(Kyoto International Conference Center)所舉行，這個會場也
就是著名的京都議定書的簽署地；而在主要會議前後的tutorial與workshop則是在
京都大學(Kyoto University)裡所舉行。 
 
這屆會議共收到1327篇投稿，共306篇為會議所接受，錄取率約為23.1%，其中48
篇錄取為口頭報告(oral)，260篇接收為海報貼示報告(poster)，算是一個大型的國
際研討會，也是電腦視覺領域中，最主要的國際會議之一。我們有一篇論文經審
查後，錄取為海報貼示報告，論文的題目為Efficient Discriminative Local Learning 
for Object Recognition。與會者來自世界各地，約一千三百多位專家學者，共同
探討電腦視覺和與其相關領域，如圖形識別(pattern recognition)與計算機圖學
(computer graphics)，的研究心得與未來發展趨勢。 
 
在八天的議程中，主要會議是在中間的四天所舉行，跟大多數會議類似，錄取為
口頭報告的論文，每篇有二十分鐘的報告時間，而接受為海報貼示報告的論文，
則依題目類別，平均分配在四天的下午報告。 
 
Tutorial是在會議的前二天所舉辦，有三至四場tutorial在同一時間平行進行，與
會者可自由參加；有許多的workshop與ICCV聯合舉行，這些workshop各自研討
的相關資訊，及在為報告作準備時所得到的經驗。 
 
這次大會的議程十分的豐富與緊湊，每天從早上九點開始，一直進行到晚上六點
半，如果能在這段期間內盡力去了解演講者的講演，相信會得到很多的收獲。雖
然每個演講者只有短短的二十分鐘來陳述自己的作品，但從投影片的製作與演說
的內容等方面來看，都可體會到作者們的用心，因此若能聽懂演說的內容，往往
會比自己看論文來的迅速有效率，並更容易認知到論文的重點。 
 
從過去參加國際會議的經驗，我認知到提過多細節的內容，通常會讓聽眾比較難
跟上，尤其是在五至八分鐘要完成的海報報告，重心更應該要放在講清論文概
念，和主要的貢獻表達，才能讓大多數與會者了解，有興趣想知道細節者，自然
就會針對想了解的內容進行詳問。因此在這次海報的製作上，論文的另一作者蔡
濬帆同學與我都試著將論文的概念，以圖表的方式去呈現，圖表的四周才放上具
體的數學模型。在會場報告時，我們就從圖表開始與來參觀的學者進行介紹，再
視情況決定是否參閱具體的數學式。 
 
這篇論文得以順利地發表，我很感謝中研院資科所劉庭祿博士與台大資訊系傅楸
善教授平日的指導，和研究討論。另外也感謝蔡濬帆同學在論文上的努力與貢
獻，包括一同參與程式撰寫，實驗設計，海報製作與會場上的報告。 
should most likely better characterize the distinctive prop-
erties shared by a small subset of data than a global one
does for the whole data. Consequently adopting multiple
locally adaptive models would achieve better performances.
For example, to enhance the nearest neighbor rule for clas-
sification, Domeniconi and Gunopulos [8] propose a local
flexible metric by adaptively reweighting dimensions of the
feature space according to the sample location.
An important issue in local learning is how to deploy the
local models among data points. In [16], Kim and Kittler
use k-means clustering to partition data into several clus-
ters, and train a linear discriminant analysis (LDA) classi-
fier for each cluster. Dai et al. [7], motivated by that more
local models should be located near data with high risks
of being misclassified, propose the responsibility mixture
model, in which an EM algorithm is adopted to place lo-
cal classifiers according to the distribution of data uncer-
tainty. However, for approaches of this kind, it is hard to
pre-determine the optimal number of local models. After
all, the Gaussian assumptions for the data distribution or
uncertainty distribution are not always valid.
Alternatively, in [12, 13, 21], a local model is specifi-
cally designed for each training sample. Thus, no assump-
tions about the data distribution are required. In addition,
local learning in these works is often coupled with feature
fusion. That is, not only the discriminant functions but also
the discriminant visual features are jointly selected to en-
hance the power of each local model. Nevertheless, training
the sample-specific models is time-consuming.
Pertaining to object recognition, the tasks of learning a
local model from a given dataset are typically correlated.
As is pointed out from the literature of multi-task learning,
e.g., [1, 6, 29], investigating related tasks jointly in most
cases can achieve a considerable performance improvement
than independently, since the extra knowledge from other
tasks may convey useful information to the completion of
the underlying task.
1.2. Our approach
Our method focuses on efficiently learning sample-
specific local models to improve the accuracies on object
recognition. In our framework, each local model is a clas-
sifier derived by boosting, and has a special form such that
the collection of them spreads as a manifold-like structure
in the resulting classifier space. By respecting this global
structure of local classifiers, we design a boosting algorithm
to learn them jointly. The main contributions of our ap-
proach can be characterized as follows: 1) We propose to
cast the independent training procedures of local classifiers
as a multi-task learning problem. 2) We introduce a boost-
ing algorithm that solves the underlying multi-task problem
with good efficiency, scales well to the data size, and pro-
duces local classifiers with proper regularization and less
redundancy. 3) We establish a new way of carrying out mul-
tiple kernel learning (MKL) [17, 25] when several image
descriptors are used to account for the given data. It can
yield recognition rates comparable to those reported by the
state-of-the-art MKL tools, e.g., [25].
The proposed approach is related to JointBoost [29] in
the sense that multiple boosted classifiers are simultane-
ously generated under a specific form of multi-task learning.
However, our method can accomplish it more efficiently.
We also note that our use of dyadic hypercuts, introduced in
Moghaddam and Shakhnarovich [22], as the weak learners
for boosting is pivotal to the formulation in that they can ef-
fectively capture useful information in a kernel matrix, and
elegantly connect multiple kernel learning with boosting.
2. Multi-task local learning
We begin by specifying the notations used in this work,
defining the problem of local learning for object recogni-
tion, and describing its link to the multi-task learning.
2.1. Notations
Since the multi-class object recognition can always be
reduced to an array of two-class problems by adopting one-
against-one or one-against-all rules, we assume a binary
dataset S = {xn ∈ X , yn ∈ ±1}Nn=1, where N is the data
size and X denotes the input space.
In view of the complexity of visual object recognition,
it is hard to find a universal descriptor to well characterize
the whole dataset. Instead, we consider representing each
xn with totally M kinds of different descriptors, i.e., xn =
{xn,m ∈ Xm}
M
m=1, and each descriptor is associated with a
distance measure dm : Xm ×Xm → R.
The useful data representations are often of high dimen-
sional and in diverse forms, such as vectors [23], histograms
[5], bags of features [34], or pyramids [18]. To avoid the
difficulties caused by working with these varieties, we rep-
resent data under each descriptor by a kernel matrix. And
it leads to M kernel functions {km}Mm=1 together with the
corresponding kernel matrices {Km}Mm=1:
Km(n, n
′) = km(xn,xn′) = exp(−γmd
2
m(xn,xn′)) (1)
where γm is a positive constant.
2.2. Local learning
Our goal is to carry out local learning by deriving a
classifier fi for each training sample xi such that fi is ex-
pected to give good performances for testing samples falling
around xi. To this end, we specify the neighborhood of xi
with a weight distribution wi = {wi,n}Nn=1 over S by
wi,n =
{
1/C, if xn ∈ C-NN of xi,
0, otherwise,
(2)
be accessed by multiple tasks, inefficiency can occur when
measuring the loss induced by the sample is evaluated in-
dependently throughout the relevant tasks. Our strategy is
to learn all local classifiers of (3) jointly so that information
redundancy among them can be reasonably circumvented
with a substantial speed-up in the training process.
3. A multi-task boosting algorithm
The main theme of this section is to detail the steps of
the proposed multi-task boosting algorithm, and to discuss
its justifications and useful properties.
3.1. Design of weak learners
We consider dyadic hypercuts [22] as the weak learners
in that they can achieve good classification performances,
and be generated by referencing only the kernel functions
{km}
M
m=1 or matrices {Km}Mm=1 of (1). A dyadic hypercut
h is specified by a kernel and a pair of training samples of
opposite labels. Specifically, h is parameterized by positive
sample xn, negative sample xn′ , and kernel function km,
and can be expressed by
h(x) = sign(km(xn,x)− km(xn′ ,x)− θ), (4)
where θ is for thresholding. The size of the resulting pool
of weak learners is |H| = N+ ×N− ×M , where N+ and
N− are the numbers of positive and negative training data.
To have an efficient boosting process, we randomly sample
a subset of weak learners from the pool at each iteration.
3.2. The boosting algorithm
The multi-task setting of local learning is accomplished
via a boosting algorithm, in which task i is to learn
classifier fi as in (3) with the weighted dataset Si =
{xn, yn, wi,n}
N
n=1. Steps of the proposed multi-task boost-
ing are listed in Algorithm 1. Note that the algorithm main-
tains a two-dimensional weight array {w(t)i,n}Ni,n=1 to link
successive iterations, where w(t)i,n denotes the weight of xn
in task i at iteration t. In what follows, we describe the
details of running iteration t.
We start by defining the loss function of task i. At iter-
ation t, fi =
∑t−1
τ=1 αi,τhτ is a linear combination of the
(t − 1) selected weak learners. With the exponential loss
model [11], the loss of fi with respect to Si is
L(fi, Si) =
N∑
n=1
wi,n exp(−ynfi(xn)). (5)
Since all the classifiers are trained jointly, a reasonable
choice of the joint objective function is the total loss in-
duced by these classifiers in all the tasks, i.e.,
Loss =
N∑
i=1
L(fi, Si). (6)
Algorithm 1: Multi-task Boosting for Local Learning
Input : N tasks: task i involves weighted dataset
Si = {xn, yn, wi,n}
N
n=1.
Output: Local classifiers {fi}Ni=1, where
fi(x) =
∑T
t=1 αi,tht(x).
Initialize: w(1)i,n = wi,n, for i, n = 1, 2, ..., N .
for t← 1, 2, . . . , T do
1. Compute the cross-task data weights {w˜(t)n }Nn=1:
w˜
(t)
n =
∑N
i=1 w
(t)
i,n.
2. Select the optimal dyadic hypercut ht:
ht = arg minh
∑N
n=1 w˜
(t)
n · 1[h(xn) 6=yn].
3. Compute task-wise weighted errors {ǫi,t}Ni=1:
ǫi,t =
∑N
n=1 w
(t)
i,n · 1[ht(xn) 6=yn].
4. Compute task-wise weighted accuracies {ci,t}Ni=1
ci,t =
∑N
n=1 w
(t)
i,n · 1[ht(xn)=yn].
5. Set task-wise ensemble coefficients {αi,t}Ni=1:
αi,t = max(0,
1
2 ln
ci,t
ǫi,t
).
6. Update data weights {w(t+1)i,n }Ni,n=1:
w
(t+1)
i,n = w
(t)
i,n exp(−ynαi,tht(xn)).
To decide the best weak learner ht shared by all classifiers
at iteration t, we minimize the total loss as in (6):
ht = argmin
h
N∑
i=1
L(fi + h, Si) (7)
= arg min
h
N∑
i=1
N∑
n=1
wi,n exp(−yn(fi(xn) + h(xn))) (8)
= arg min
h
N∑
i=1
N∑
n=1
w
(t)
i,n exp(−ynh(xn)) (9)
= arg min
h
N∑
n=1
w˜(t)n exp(−ynh(xn)) (10)
In (9), we have w(t)i,n = wi,n exp(−ynfi(xn)) from the ini-
tialization and step 6 of Algorithm 1. The cross-task data
weight of xn is defined to be its total weight in all tasks,and
is denoted by w˜(t)n =
∑N
i=1 w
(t)
i,n. Thus, (10) implies that
the optimal discrete h is the one with the minimal cross-
task weighted error, i.e.,
ht = arg min
h
N∑
n=1
w˜(t)n · 1[h(xn) 6=yn]. (11)
1-NN SVM AdaBoost Ours
GB-Dist 42.4 ± 1.3 61.4 ± 0.8 61.5± 0.7 63.2 ± 0.8
GB 37.4 ± 0.9 57.6 ± 1.0 58.5± 0.7 59.7 ± 1.2
SIFT-Dist 49.6 ± 0.8 58.7 ± 0.9 57.5± 1.0 57.8 ± 0.7
SIFT-SPM 48.8 ± 0.7 56.1 ± 0.9 53.3± 0.8 55.2 ± 0.7
SS-Dist 31.7 ± 1.4 53.4 ± 1.0 56.1± 0.7 56.3 ± 0.9
SS-SPM 41.7 ± 0.9 53.9 ± 0.9 55.5± 0.7 56.9 ± 1.3
C2-SWP 22.0 ± 0.8 28.3 ± 0.8 26.0± 0.9 26.9 ± 1.0
C2-ML 37.7 ± 0.7 46.3 ± 1.0 44.8± 0.9 46.6 ± 0.6
PHOG 27.7 ± 1.0 43.9 ± 0.8 41.3± 1.0 42.9 ± 0.8
GIST 36.8 ± 1.1 48.7 ± 0.8 49.1± 1.0 51.2 ± 0.8
(a)
AdaBoostSimpleMKL (local) Ours
All 74.3 ± 1.2 74.6± 1.3 75.8 ± 1.1
3.26 × 102 sec. 1.87× 105 sec. 3.92 × 103 sec.
(b)
Table 1. Recognition rates [mean ± std %] of several approaches
on the Caltech-101 dataset. (a) A kernel is taken into account at a
time. (b) All kernels are jointly considered.
• SIFT-SPM: With a densely sampled grid, SIFT fea-
tures are extracted and quantized into visual words.
The kernel is built by matching spatial pyramids [18].
• SS-SPM / SS-Dist: The same as SIFT-SPM and SIFT-
Dist respectively, except the self-similarity descriptor
[28] is used to replace the SIFT descriptor.
• C2-SWP / C2-ML: We adopt biologically inspired
features to depict images. Both the C2 features sug-
gested by Serre et al. [27] and by Mutch and Lowe [23]
are respectively used to establish the two RBF kernels.
• PHOG: The PHOG descriptor [5] is used to summa-
rize the distributions of edge orientations. Together
with the χ2 distance, the kernel is established.
• GIST: The images are resized to 128×128 pixels prior
to applying the gist descriptor [24]. The RBF kernel is
then constructed with the L2 distance.
The distance matrices result from different parameter
values of each descriptor are combined in advance. It is
used for ensuring that the resulting kernels individually
reach their best performances.
4.1.3 Quantitative results
Like in [3, 33, 34], we randomly pick 30 images from each
of the 102 categories, and split them into two disjoint sub-
sets: one containsNtrain images per category, and the other
consists of the rest. The two subsets are respectively used
for training and testing. The whole evaluation process are
repeated 20 times by using different splits between the train-
ing and testing subsets. Recognition rates are measured in
cases that Ntrain is set to 5, 10, 15, 20, and 25.
0 5 10 15 20 25 30 35 40 4510
20
30
40
50
60
70
80
90
Number of Training Samples per Class
R
ec
og
ni
tio
n 
Ac
cu
ra
cy
 (%
)
 
 
Ours
Boiman et al. 2008
Varma and Ray 2007
Bosch et al. 2007
Frome et al. 2007
Zhang et al. 2006
Lazebnik et al. 2006
Mutch and Lowe 2006
Grauman and Darrell 2005
Berg et al. 2005
Serre et al. 2005
Fei−Fei et al. 2004
Figure 3. The recognition rates of several approaches on Caltech-
101 dataset with different numbers of training data per class.
We first investigate the effectiveness of each kernel by
comparing our method with the one-nearest-neighbor (1-
NN) classifier, SVM, and AdaBoost. When the value of
Ntrain is 15, the recognition rates are reported in Table 1a.
The 1-NN serves as the baseline for the experiment. By
transforming a kernel to a set of dyadic hypercuts (weak
learners), AdaBoost and our approach can fuse these hy-
percuts into a global classifier and a set of local classifiers,
respectively. Observe that AdaBoost achieves comparable
performance to that of SVM. It implies that the hypercuts
can capture useful information in a kernel. Overall, the pro-
posed local learning consistently outperforms AdaBoost,
and gives the best recognition rates in most cases.
Focusing now on the case that multiple kernels are con-
sidered simultaneously, we evaluate the performances of
SimpleMKL [25], AdaBoost for local learning, and our
method. SimpleMKL is a well-known software for multi-
ple kernel learning, and can learn an optimal ensemble ker-
nel by searching the convex combinations of kernels. With
SimpleMKL, a global classifier is obtained with training
time 326 seconds, and achieves recognition rate 74.3%. We
then compare AdaBoost and our approach in realizing lo-
cal learning. While AdaBoost carries out local learning for
a total of N = 1530 classifiers one by one, our method
constructs them via a joint training. The recognition rates
and computational costs are shown in Table 1b. The results
demonstrate that our approach achieves not only a better
performance but also gives a two order speed-up.
In Figure 3, the accuracy rates of several recent tech-
niques, including ours, on Caltech-101 are plotted with re-
spect to different numbers of training data. The outcomes
of ours are 60.1 ± 1.1%, 70.5 ± 0.7%, 77.5 ± 0.7%, and
79.1 ± 2.1% when Ntrain is set to 5, 10, 20, and 25 re-
spectively. For the case that Ntrain is 15, the recognition
rate 75.8% by our method is either better or comparable to
those by other published systems, e.g., [4, 5, 13, 15, 18, 34].
avg. Aero. Bicy. Bird Boat Bott. Bus Car Cat Chair Cow Table Dog Horse Moto. Pers. Plant Sheep Sofa Train Tv
INRIA 59.4 77.5 63.6 56.1 71.9 33.1 60.6 78.0 58.8 53.5 42.6 54.9 45.8 77.5 64.0 85.9 36.3 44.7 50.6 79.2 53.2
XRCE 57.5 72.3 57.5 53.2 68.9 28.5 57.5 75.4 50.3 52.2 39.0 46.8 45.3 75.7 58.5 84.0 32.6 39.7 50.9 75.1 49.5
TKK 51.7 71.4 51.7 48.5 63.4 27.3 49.9 70.1 51.2 51.7 32.3 46.3 41.5 72.6 60.2 82.2 31.7 30.1 39.2 71.1 41.0
van Gemert et al. [31] 60.5 80.4 64.9 57.0 69.1 24.6 65.8 78.2 54.3 56.9 42.4 53.7 47.0 81.5 65.6 87.9 38.3 52.3 53.9 83.2 53.3
SimpleMKL 57.3 74.1 62.7 48.7 66.9 29.1 62.6 75.0 56.9 54.5 42.7 54.8 44.2 76.3 65.8 83.6 28.7 42.5 51.5 74.7 50.9
Ours 59.3 76.5 64.6 51.8 68.3 32.2 61.3 77.5 57.8 56.3 43.5 58.8 44.8 78.4 65.2 85.4 30.4 47.7 54.6 76.4 54.6
Table 3. Average APs (%) of several approaches on Train+Val set of the VOC 2007 dataset.
5. Conclusion
We have introduced an efficient local learning approach
to resolving the difficulties in object recognition caused by
large intraclass variations. We cast multiple, independent
training processes of local classifiers to a correlative multi-
task learning problem, and develop a boosting-based algo-
rithm to accomplish it. The proposed technique is com-
prehensively evaluated with two benchmark datasets. The
recognition rates in both datasets are comparable to those
yielded by the respective state-of-the-art approaches. Our
method can be considered as a general multi-task learn-
ing tool for vision applications where multiple correlative
classifiers are required, such as multi-view face detection,
multi-part object tracking, or user-dependent media rank-
ing. The framework also provides a new way to carry out
multiple kernel learning in an incremental manner.
Acknowledgements. The work is supported in part by
NSC grants 95-2221-E-001-031-MY3 and 97-2221-E-001-
019-MY3.
References
[1] R. Ando and T. Zhang. A framework for learning predictive struc-
tures from multiple tasks and unlabeled data. JMLR, 2005. 2
[2] C. Atkeson, A. Moore, and S. Schaal. Locally weighted learning.
Artificial Intelligence Review, 1997. 1
[3] A. Berg, T. Berg, and J. Malik. Shape matching and object recogni-
tion using low distortion correspondences. In CVPR, 2005. 5, 6
[4] O. Boiman, E. Shechtman, and M. Irani. In defense of nearest-
neighbor based image classification. In CVPR, 2008. 6
[5] A. Bosch, A. Zisserman, and X. Mun˜oz. Representing shape with a
spatial pyramid kernel. In CIVR, 2007. 2, 6
[6] R. Caruana. Multitask learning. ML, 1997. 2
[7] J. Dai, S. Yan, X. Tang, and J. Kwok. Locally adaptive classification
piloted by uncertainty. In ICML, 2006. 2
[8] C. Domeniconi and D. Gunopulos. Adaptive nearest neighbor clas-
sification using support vector machines. In NIPS, 2001. 2
[9] M. Everingham, L. V. Gool, C. K. I. Williams, J. Winn, and
A. Zisserman. The PASCAL Visual Object Classes Challenge 2007
(VOC2007) Results. 1, 5, 7
[10] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual mod-
els from few training examples: An incremental bayesian approach
tested on 101 object categories. In CVPR Workshop on Generative-
Model Based Vision, 2004. 1, 5
[11] Y. Freund and R. Schapire. A decision-theoretic generalization of
on-line learning and an application to boosting. In Computational
Learning Theory, 1995. 4
[12] A. Frome, Y. Singer, and J. Malik. Image retrieval and classification
using local distance functions. In NIPS, 2006. 1, 2
[13] A. Frome, Y. Singer, F. Sha, and J. Malik. Learning globally-
consistent local distance functions for shape-based image retrieval
and classification. In ICCV, 2007. 1, 2, 6
[14] M. Go¨nen and E. Alpaydin. Localized multiple kernel learning. In
ICML, 2008. 1
[15] K. Grauman and T. Darrell. The pyramid match kernel: Discrimina-
tive classification with sets of image features. In ICCV, 2005. 6
[16] T.-K. Kim and J. Kittler. Locally linear discriminant analysis for
multimodally distributed classes for face recognition with a single
model image. PAMI, 2005. 2
[17] G. Lanckriet, N. Cristianini, P. Bartlett, L. Ghaoui, and M. Jordan.
Learning the kernel matrix with semidefinite programming. JMLR,
2004. 2
[18] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features:
Spatial pyramid matching for recognizing natural scene categories.
In CVPR, 2006. 2, 6, 7
[19] Y.-Y. Lin, T.-L. Liu, and C.-S. Fuh. Local ensemble kernel learning
for object category recognition. In CVPR, 2007. 1
[20] D. Lowe. Distinctive image features from scale-invariant keypoints.
IJCV, 2004. 5
[21] T. Malisiewicz and A. Efros. Recognition by association via learning
per-exemplar distances. In CVPR, 2008. 1, 2
[22] B. Moghaddam and G. Shakhnarovich. Boosted dyadic kernel dis-
criminants. In NIPS, 2002. 2, 4
[23] J. Mutch and D. Lowe. Multiclass object recognition with sparse,
localized features. In CVPR, 2006. 2, 6, 7
[24] A. Oliva and A. Torralba. Modeling the shape of the scene: A holistic
representation of the spatial envelope. IJCV, 2001. 6
[25] A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet. More effi-
ciency in multiple kernel learning. In ICML, 2007. 2, 6
[26] H. Schneiderman and T. Kanade. A statistical method for 3d object
detection applied to faces and cars. In CVPR, 2000. 1
[27] T. Serre, L. Wolf, and T. Poggio. Object recognition with features
inspired by visual cortex. In CVPR, 2005. 6
[28] E. Shechtman and M. Irani. Matching local self-similarities across
images and videos. In CVPR, 2007. 6
[29] A. Torralba, K. Murphy, and W. Freeman. Sharing visual features for
multiclass and multiview object detection. PAMI, 2007. 2, 5
[30] K. van de Sande, T. Gevers, and C. Snoek. Evaluation of color de-
scriptors for object and scene recognition. In CVPR, 2008. 7
[31] J. C. van Gemert, C. J. Veenman, A. W. M. Smeulders, and J. M.
Geusebroek. Visual word ambiguity. PAMI, 2009. 7, 8
[32] L. Vandenberghe and S. Boyd. Semidefinite programming. SIAM
Review, 1996. 5
[33] M. Varma and D. Ray. Learning the discriminative power-invariance
trade-off. In ICCV, 2007. 6
[34] H. Zhang, A. Berg, M. Maire, and J. Malik. Svm-knn: Discriminative
nearest neighbor classification for visual category recognition. In
CVPR, 2006. 2, 5, 6
一頁有關論文的投影片至大會網站，大會則根據論文所屬之session依序收集成一
投影片，並由大會工作人員播放，在session開始時，每位作者逐一上台報告不超
過一分鐘(因為過一分鐘後，播放投影片的工作人員會強制換頁)。 
 
Tutorial是在主要會議的前一天所舉辦，有八場tutorial在同一天平行進行；有許
多的workshop與ECCV聯合舉行，這些workshop各自研討與電腦視覺相關，但較
為特定的議題，並分佈在議程的最後二天開會。 
 
在電腦視覺這個研究領域中，ECCV最主要的國際會議之一，因此在能在ECCV
獲獎者，是一分十分難得的殊榮，這屆的論文獎委員會(paper award committee)
由38篇錄取為口頭報告的論文中選出6篇作為candidate，並從中依序選出最佳論
文(best paper award)，最佳論文榮譽提名(runner-up paper award)與最佳學生論文
獎(best student paper award)，以下是今年ECCV的獲獎者與論文： 
 
Best Paper Award 
Graph Cut based Inference with Co-occurrence Statistics 
L. Laticky, C. Russell, P. Kohli and P.H.S. Torr 
Runner-up Paper Award 
Blocks World Revisited: Image Understanding Using Qualitative Geometry and 
Mechanics 
A. Gupta, A. Efros and M. Hebert 
Best Student Paper Award 
Ambrosio-Tortorelli Segmentation of Stochastic Images 
T. Pätz and T. Preusser 
 
另外，這ECCV亦選出兩篇於10年前的ECCV(2000年於愛爾蘭都柏林所舉行)中，
對這十年來電腦視覺領域貢獻與影響力最大之論文，以下為二篇得獎論文： 
 
Koenderink Prize 
Stochastic Tracking of 3D Human Figures Using 2D Image Motion 
H. Sidenbladh, M.J. Black and D.J. Fleet 
Unsupervised Learning of Models for Recognition 
M. Weber, M. Welling and P. Perona 
 
在這屈ECCV的晚宴中，也確定了後二屆的ECCV(2014年)在瑞士的蘇黎世(Zurich, 
Swiss)舉辦，2012年的ECCV在開會前已決定在義大利的佛羅倫斯(Florence, Italy) 
舉辦。 
 
spotlight的功用是讓聽眾能在有限的2~3小時poster session中，有效地將重心鎖定
在少數幾篇自己真正感興趣的paper上。所以以聽眾而言，我會希望能在這幾十
秒內，知道這篇paper的研究議題和主要貢獻，這有助於判斷，等會這篇paper是
不是主要需要去了解的目標。 
 
由這個角度出發，spotlight的設計就是要讓觀眾能在幾十秒內了解本篇paper的主
要貢獻，除了一些演說通則外，我覺得適度地把重點放在陳述自己論文的特色是
一個不錯的策略。例如論文主要貢獻是提出新方法，那重點就是新方法提出所依
據的motivation或新方法優於傳統方法的原因，又例如論文是提出computer vision
技術的新應用，那重點就是強調新應用的價值與適用性。這是因為在30~40篇的
spotlight session，每篇paper只有分配到一分鐘，這種情況下期待聽眾對paper有什
麼"了解"是不太可能的，反而是希望聽眾有個"印象"，比較像是一個合理的期
待，而強調論文的特色，是個讓人留下印象的合適方法。 
 
至於投影片的設計，文字，示意圖，動畫或三者的組合皆可，端看paper的貢獻
以哪種方式呈現比較清楚，並要能緊密地與講演內容搭配，另外考慮到解釋時間
有限，文字不要太多，示意圖不要太複雜，不然可能會有反效果。有些作者可能
是求好心切，想說把越多東西放在投影片上，觀眾可能會了解的越多，但以觀眾
而言可能就不是這樣想，例如當意識到字太小或字太多有可能看不完，觀眾可能
就會直接放棄整張投影片，直接用"聽"的，也就是說，設計的投影片是沒有達到
任何的效果。 
 
建議在設計spotlight的投影片和講稿時，可以先去參考過去別人的spotlight，目前
ECCV似乎還沒公開spotlight投影片，但NIPS (Neural Information and Processing 
Systems)近幾年的spotlight都有放在網頁上，參考別人的spotlight，有助於了解不
同設計的風格優缺點，並找出比較適合自己論文的設計型式。 
 
這篇論文得以順利地發表，我很感謝中研院資科所劉庭祿博士與台大資訊系傅楸
善博士平日的指導，在論文成形之前，劉老師與我常一起討論研究目標，訂定研
究議題，審核提出的研究方法之貢獻，並在實驗中出現問題或困難時，進行方法
的分析並加以修改至可行，傅老師與我一同去希臘開會，在旅館中，多次聆聽我
的報告，包含spotlight session的演講與海報的解說，並提供了很多實用的建議與
與會者可能會問到的問題。最後我要感謝傑出人才基金會與國科會所提供的出國
開會補助金，包括傑出人才基金會所補助的交通費與生活費，國科會所提供的會
議註冊費，這些補助讓我得以順利出國開會。 
 
三. 攜回資料名稱及內容 
Copyright c© The 11th European Conference on Computer Vision (ECCV-2010)
Clustering Complex Data with
Group-Dependent Feature Selection
Yen-Yu Lin1,2, Tyng-Luh Liu1, and Chiou-Shann Fuh2
1Institute of Information Science, Academia Sinica, Taipei, Taiwan
2Department of CSIE, National Taiwan University, Taipei, Taiwan
{yylin,liutyng}@iis.sinica.edu.tw fuh@csie.ntu.edu.tw
Abstract. We describe a clustering approach with the emphasis on de-
tecting coherent structures in a complex dataset, and illustrate its effec-
tiveness with computer vision applications. By complex data, we mean
that the attribute variations among the data are too extensive such that
clustering based on a single feature representation/descriptor is insuffi-
cient to faithfully divide the data into meaningful groups. The proposed
method thus assumes the data are represented with various feature rep-
resentations, and aims to uncover the underlying cluster structure. To
that end, we associate each cluster with a boosting classifier derived from
multiple kernel learning, and apply the cluster-specific classifier to feature
selection across various descriptors to best separate data of the cluster
from the rest. Specifically, we integrate the multiple, correlative training
tasks of the cluster-specific classifiers into the clustering procedure, and
cast them as a joint constrained optimization problem. Through the op-
timization iterations, the cluster structure is gradually revealed by these
classifiers, while their discriminant power to capture similar data would
be progressively improved owing to better data labeling.
1 Introduction
Clustering is a technique to partition the data into groups so that similar (or
coherent) objects and their properties can be readily identified and exploited.
While such a goal is explicit and clear, the notion of similarity is often not well
defined, partly due to the lack of a universally applicable similarity measure.
As a result, previous research efforts on developing clustering algorithms mostly
focus on dealing with different scenarios or specific applications. In the field
of vision research, performing data clustering is essential in addressing various
tasks such as object categorization [1, 2] or image segmentation [3, 4]. Despite the
great applicability, a fundamental difficulty hindering the advance of clustering
techniques is that the intrinsic cluster structure is not evidently revealed in the
feature representation of complex data. Namely, the resulting similarities among
data points do not faithfully reflect their true relationships.
We are thus motivated to consider establishing a clustering framework with
the flexibility of allowing the data to be characterized by multiple descriptors.
The generalization aims to bridge the gap between the resulting data similari-
ties and their underlying relationships. Take, for example, the images shown in
Clustering Complex Data with Group-Dependent Feature Selection 3
about cluster shapes or structures, and using various optimization techniques
for problem solving. Such variations however do not devalue the importance of
clustering being a fundamental tool for unsupervised learning. Instead, cluster-
ing methods such as k-means, spectral clustering [3, 8], mean shift [4] or affinity
propagation [9] are constantly applied in more effectively solving a broad range
of computer vision problems.
Although most clustering algorithms are developed with theoretic support,
their performances still depend critically on the feature representation of data.
Previous approaches, e.g., [5, 6], concerning the limitation have thus suggested
to perform clustering and feature selection simultaneously such that relevant
features are emphasized. Due to the inherent difficulty of unsupervised feature
selection, methods of this category often proceed in an iterative manner, namely,
the steps of feature selection and clustering are carried out alternately.
Feature selection can also be done cluster-wise, say, by imposing the Gaus-
sian mixture models on the data distribution, or by learning a distance function
for each cluster via re-weighting feature dimensions such as the formulations
described in [10, 11]. However, these methods typically assume that the under-
lying data are in a single feature space and in form of vectors. The restriction
may reduce the overall effectiveness when the data of interest can be more pre-
cisely characterized by considering multiple descriptors and diverse forms, e.g.,
bag-of-features [12, 13] or pyramids [14, 15].
Xu et al. [16] instead consider the large margin principle for measuring how
good a data partitioning is. Their method first maps the data into the kernel-
induced space, and seeks the data labeling (clustering) with which the maximum
margin can be obtained by applying SVMs to the then labeled data. Subse-
quently, Zhao et al. [17] introduce a cutting-plane algorithm to generalize the
framework of maximum margin clustering from binary-class to multi-class.
The technique of cluster ensembles by Strehl and Ghosh [18] is most relevant
to our approach. It provides a useful mechanism for combining multiple cluster-
ing results. The ensemble partitioning is optimized such that it shares as much
information with each of the elementary ones as possible. Fred and Jain [19]
introduce the concept of evidence accumulation to merge various clusterings to
a single one via a voting scheme. These methods generally achieve better clus-
tering performances. Implicitly, they also provide a way for clustering data with
multiple feature representations: One could generate an elementary clustering
result for each data representation, and combine them into an ensemble one.
However, the obtained partitioning is optimized in a global fashion, neglecting
that the optimal features are often cluster-dependent.
Finally, it is possible to overcome the unsupervised nature of clustering by
incorporating a small amount of labeled data in the procedure so that satisfac-
tory results can be achieved, especially in complex tasks. For example, Xing et
al. [20] impose side information for metric learning to facilitate clustering, while
Tuzel et al. [2] utilize pairwise constraints to perform semisupervised clustering
in a kernel-induced feature space.
Clustering Complex Data with Group-Dependent Feature Selection 5
by these classifiers. We resolve this difficulty by incorporating the learning pro-
cesses of the cluster-specific classifiers into the clustering procedure, and cast
the task as the following constrained optimization problem:
min
Y,{fc}Cc=1
C∑
c=1
Loss(fc, {xi, yic}
N
i=1) (2)
subject to Y ∈ {0, 1}N×C, (3)
yi,:eC = 1, for i = 1, 2, ..., N, (4)
ℓ ≤ e⊤Ny:,c ≤ u, for c = 1, 2, ..., C, (5)
yi,: = yj,:, if (i, j) ∈ S, (6)
yi,: 6= yj,:, if (i, j) ∈ S
′, (7)
where {fc}
C
c=1 are the cluster-specific classifiers. eC and eN are column vectors,
whose elements are all one, of dimensions C and N respectively.
We now give justifications for the above constrained optimization problem.
Our discussions focus first on the part of constraints. With (3) and (4), Y is
guaranteed to be a valid partition matrix. Since in practical applications most
clusters are rarely of extreme sizes, we impose the desired upper bound u and
lower bound ℓ of the cluster size in (5). The remaining constraints (6) and (7)
are optional so that our method can be extended to address semisupervised
learning. In that case, (6) and (7) would provide a set of pairwise instance-level
constraints, each of which specifies either a pair of data points must reside in
the same cluster or not. S in (6) and S′ in (7) are respectively used to denote
the collections of these must-links and cannot-links.
Assuming that all the constraints are satisfied, the formulation would look for
optimal data partitioning Y ∗ such that, according to (2), the total induced loss
of all the cluster-specific classifiers is minimized. That is, the proposed clustering
approach would prefer that data residing in each cluster are well separated from
the rest by the cluster-specific classifier (and hence yields a small loss), which is
derived by coupling a discriminant function with an optimal feature selection to
achieve the desired property. This implies that most of the data in an arbitrary
cluster c would share some coherent characteristics implicitly defined by the
optimal feature selection in forming f∗c . The proposed optimization elegantly
connects the unsupervised clustering procedure with the supervised learning
of the specific classifiers. By jointly addressing the two tasks, our method can
uncover a reasonable cluster structure even for a complex dataset.
4 Optimization Procedure
To deal with the cause-and-effect factor in (2), we consider an iterative strategy
to solve the constrained optimization problem. At each iteration, the cluster-
specific classifiers {fc}
C
c=1 and the partition matrix Y are alternately optimized.
More specifically, {fc} are first optimized while Y is fixed, and then their roles are
switched. The iterations are repeated until the loss cannot be further reduced.
Clustering Complex Data with Group-Dependent Feature Selection 7
4.2 On assigning data into clusters
Once the cluster-specific classifiers are fixed, we illustrate that how the partition
matrix Y in (2) can be optimized by binary integer programming (BIP) [28]. For
the ease of our discussion, the canonical form of a BIP problem is given below
min
z
d⊤z (10)
subject to Az ≤ b and Aeqz = beq, (11)
zi ∈ {0, 1}. (12)
It suffices to show the proposed constrained optimization can be transformed
to the above form. To rewrite the objective function (2) as the inner product
(10), we let z ≡ vec(Y ) = [y11 · · · y1C · · · yic · · · yNC ]
⊤, the vectorization of
partition matrix Y and set the column vector d = [dic] as
dic = ln (1 + exp (−fc(xi))) +
C∑
c′=1 & c′ 6=c
ln (1 + exp (fc′(xi))). (13)
The definitions of d and z would lead to
d⊤z = d⊤vec(Y ) =
C∑
c=1
N∑
i=1
ln (1 + exp (−y˜icfc(xi))). (14)
Indeed the derivation of (14) is based on (4). For each sample xi, there is one
and only one element whose value is 1 in the vector yi,: = [yi1 · · · yiC ]. And no
matter which element equals to 1, we have
C∑
c=1
dicyic =
C∑
c=1
ln (1 + exp (−y˜icfc(xi))). (15)
Now, summing over all the data on the both sides of (15) gives (14). We are left
to express the constraints (3)–(7) into (11) and (12). Since the derivations related
to (3)–(6) are straightforward, we focus on the reduction of constraint (7). To
represent yi,: 6= yj,:, we consider additional auxiliary variables, p ∈ {0, 1}
C×1
and q ∈ {0, 1}C×1, and the following three constraints
yi,: − yj,: = p− q, p+ q ≤ eC , and e
⊤
Cp+ e
⊤
Cq = 2. (16)
It can be verified that yi,: 6= yj,: if and only if the constraints in (16), which
are all conformed to (11), hold. Thus, our discussion justifies that when {fc} are
fixed, the constrained optimization problem (2) can be effectively solved by BIP
to obtain a new data partitioning Y .
4.3 Implementation details
In solving the constrained optimization, we begin by providing an initial Y de-
rived by randomly splitting the data into clusters of similar sizes. As it turns out
Clustering Complex Data with Group-Dependent Feature Selection 9
Table 1. The performances in form of [ACC (%) / NMI] by different clustering meth-
ods. Top: each kernel is considered individually. Bottom: all kernels are used jointly
kernel k-means Affinity Prop. Spectral Clus. Ours
GB 68.0 / 0.732 52.5 / 0.578 69.5 / 0.704 75.0 / 0.742
SIFT 62.5 / 0.680 59.8 / 0.638 62.5 / 0.668 69.6 / 0.706
SS 65.7 / 0.659 55.7 / 0.574 63.3 / 0.655 62.1 / 0.639
C2 37.8 / 0.417 47.5 / 0.517 57.7 / 0.585 51.2 / 0.550
PHOG 53.3 / 0.547 43.3 / 0.464 61.0 / 0.624 55.2 / 0.569
kernels CE + k-means CE + Affinity Prop. CE + Spectral Clus. Ours
All 73.8 / 0.737 63.3 / 0.654 77.3 / 0.758 85.7 / 0.833
– GB. For a given image, we randomly sample 400 edge pixels, and character-
ize them by the geometric blur descriptor [12]. With these image features, we
adopt the distance function suggested in equation (2) of the work by Zhang
et al. [22] to obtain the kernel.
– SIFT. The kernel is analogously constructed as is the kernel GB, except
that the features are described with the SIFT descriptor [13].
– SS. We consider the self-similarity descriptor [31] over an evenly sampled
grid of each image, and use k-means clustering to generate visual words from
the resulting local features of all images. Then the kernel is built by matching
spatial pyramids, which are introduced in [14].
– C2. Mutch and Lowe [21] have proposed a set of features that emulate the
visual system mechanism. We adopt these biologically inspired features to
depict images and construct an RBF kernel.
– PHOG. We also use the PHOG descriptor [15] to capture image features.
Together with the χ2 distance, the kernel is established.
Quantitative results In all the experiments, we set the number of clusters
to the number of classes in ground truth, and evaluate clustering performances
with the two criteria: clustering accuracy (ACC) [6], and normalized mutual
information (NMI) [18]. The output ranges of the two criteria are both [0, 1].
The larger the values, the better the clustering results are. Our approach starts
from a random initialization of data partitioning Y . We run our algorithm 20
times with different random partitionings, and report the average performance.
Besides, we respectively set ℓ and u in (5) as ⌊0.8k1⌋ and ⌈1.2k2⌉, where k1 and
k2 are the minimal and the maximal cluster sizes in the dataset respectively.
We first evaluate our method in the cases that each descriptor is used indi-
vidually, and compare it with three popular clustering methodologies: k-means,
affinity propagation [9], and spectral clustering [8]. The implementations for the
three techniques are as follows. k-means works on data in Euclidean space, so we
use multidimensional scaling [32] to recover the feature vectors of data from their
pairwise distances. Affinity propagation detects representative exemplars (clus-
ters) by considering similarities among data. We set the pairwise similarities as
the negative distances. Spectral clustering and our approach both take a kernel
matrix as input. The outcomes by the four clustering algorithms are shown in
Clustering Complex Data with Group-Dependent Feature Selection 11
(a) (b)
(c) (d)
Fig. 3. Four kinds of intraclass variations caused by (a) different lighting conditions,
(b) in-plane rotations, (c) partial occlusions, and (d) out-of-plane rotations
(a) (b)
φ ψψ′
Fig. 4. (a) Images obtained by applying the delighting algorithm [34] to the five images
in Fig. 3a. (b) Each image is divided into 96 regions. The distance between the two
images is obtained when circularly shifting causes ψ′ to be the new starting radial axis
rectory “lights”). For subjects in the second and third groups, the images with
near frontal poses (C05, C07, C09, C27, C29) under the directory “expression”
are used. While each image from the second group is rotated by a randomly sam-
pled angle within [−45◦, 45◦], each from the third group is instead occluded by
a non-face patch, whose area is about ten percent of the face region. Finally, for
subjects in the fourth group, the images with out-of-plane rotations are selected
under the directory “expression” and with the poses (C05, C11, C27, C29,
C37). All images are cropped and resized to 51× 51 pixels.
Descriptors, distances and kernels With the dataset, we adopt and design
a set of visual features, and establish the following four kernels.
– DeLight. The data representation is obtained from the delighting algorithm
[34], and the corresponding distance function is set as 1−cos θ, where θ is the
angle between a pair of samples under the representation. Some delighting
results are shown in Fig. 4a. It can be seen that variations caused by different
lighting conditions are significantly alleviated under the representation.
– LBP. As is illustrated in Fig. 4b, we divide each image into 96 = 24 × 4
regions, and use a rotation-invariant local binary pattern (LBP) operator [35]
(with operator setting LBP riu28,1 ) to detect 10 distinct binary patterns. Thus
an image can be represented by a 960-dimensional vector, where each dimen-
sion records the number of occurrences that a specific pattern is detected in
the corresponding region. To achieve rotation invariant, the distance between
two such vectors, say, xi and xj , is the minimal one among the 24 values
computed from the distance function 1−sum(min(xi,xj))/sum(max(xi,xj))
by circularly shifting the starting radial axis for xj . Clearly, the base kernel
is constructed to deal with variations resulting from rotations.
Clustering Complex Data with Group-Dependent Feature Selection 13
0M0C 1M1C 2M2C 3M3C30
40
50
60
70
80
a
cc
u
ra
cy
 ra
te
 (%
)
 
 
Ours+All
CE+All
Ours+DeLight
Ours+LBP
Ours+RsLTS
Ours+RsL2
0M0C 1M1C 2M2C 3M3C0.6
0.7
0.8
0.9
n
o
rm
a
liz
ed
 m
ut
ua
l in
fo
.
 
 
Ours+All
CE+All
Ours+DeLight
Ours+LBP
Ours+RsLTS
Ours+RsL2
Fig. 5. The performances of cluster ensembles and our approach w.r.t. different
amounts of must-links and cannot-links per subject and different settings of kernel(s)
6 Conclusion
We have presented an effective approach to clustering complex data that con-
siders cluster-dependent feature selection and multiple feature representations.
Specifically, we incorporate the supervised training processes of cluster-specific
classifiers into the unsupervised clustering procedure, cast them as a joint op-
timization problem, and develop an efficient technique to accomplish it. The
proposed method is comprehensively evaluated with two challenging vision ap-
plications, coupled with a number of feature representations for the data. The
promising experimental results further demonstrate its usefulness. In addition,
our formulation provides a new way of extending the multiple kernel learning
framework, which is typically used in tackling supervised-learning problems, to
address unsupervised and semisupervised applications. This aspect of general-
ization introduces a new frontier of applying multiple kernel learning to handling
the ever-increasingly complex vision tasks.
Acknowledgments. We want to thank the anonymous reviewers for their valu-
able comments. This work is supported in part by grant 97-2221-E-001-019-MY3.
References
1. Dueck, D., Frey, B.: Non-metric affinity propagation for unsupervised image cate-
gorization. In: ICCV. (2007)
2. Tuzel, O., Porikli, F., Meer, P.: Kernel methods forweakly supervised mean shift
clustering. In: ICCV. (2009)
3. Shi, J., Malik, J.: Normalized cuts and image segmentation. TPAMI (2000)
4. Comaniciu, D., Meer, P.: Mean shift: A robust approach toward feature space
analysis. TPAMI (2002)
5. Roth, V., Lange, T.: Feature selection in clustering problems. In: NIPS. (2003)
6. Ye, J., Zhao, Z., Wu, M.: Discriminative k-means for clustering. In: NIPS. (2007)
7. Lanckriet, G., Cristianini, N., Bartlett, P., Ghaoui, L., Jordan, M.: Learning the
kernel matrix with semidefinite programming. JMLR (2004)
8. Ng, A., Jordan, M., Weiss, Y.: On spectral clustering: Analysis and an algorithm.
In: NIPS. (2001)
（附表四） 
參加國際學術會議會後報告表 
REPORT FOR ATTENDING INTERNATIONAL CONFERENCES 
姓   名 
Name 
張楷岳 
服務單位及職稱 
Institute & Position 
中央研究院資訊科學研究所約聘研究助理 
會  議  名  稱 
Title of Meeting 
中文：IEEE 電腦視覺與模型辨識國際會議 
Chinese 
英文：IEEE Computer Society International Conference on 
Computer Vision and Pattern Recognition, CVPR 
English 
日   期 
Date of Conference 
100/6/20-100/6/25 地  點 
Location 
美國科羅拉多泉市(COS) 
經  費  來  源 
Source of Travel Funding ■申請補助 Applying for Funding（補助單位 Supported by: 國科會計畫–
植基於機器學習的物件類別辨識技(NSC97-2221-E-001-019-MY3)） 
□會方補助 Funded by Conference □自籌 Self-funding
 
2. 心得 
我這次海報演講感覺做得還不錯，吸引到了不少人來聽，在報告的過程中也沒有發生突然沒人
來聽的情形，主要的原因我想是因為我大部分的內容都是用圖來表示，海報上只有少量的文字，
在做海報前有參考了大會提供的指南，一個好的海報圖片應該占百分之四十到四十五，文字占
二十到二十五，剩下的部分保留空白，因為觀眾並不會花太多時間在一個海報上，用圖片呈現
的方式最容易讓觀眾馬上抓到重點，進而會想要聽你解釋。就我親身經歷的結果，一堆文字的
海報真的是看得很吃力，特別是有的海報直接把摘要的部分放上來，就算是論文的題目讓人感
興趣，碰到這種海報，如果我抓不到他論文的動機是什麼，我也會直接去看下一篇海報了。 
 
在這次會議中聽了一場口頭論文的演講，是由一個韓國人在台上報告，他的指導教授是 Kristen 
Grauman，我覺得他在台上的表現還不錯，是我少數聽得懂的幾場口頭報告之一，在講完之後，
碰到一位那教授的指導的台灣學生，他問我那位演講者的表現如何，我說他講得還蠻清楚的，
他告訴我說實際上那是被硬練出來的成果，那位教授十分重視在台上的表現，在會議前大概一
個月，那位學生幾乎天天都被叫到面前練習演講。另外在這次會議中，我發現似乎是比較有名
的學者或是教授，他們都比較注重在台上的表現技巧，不管是海報或是口頭發表，他們的內容
就是比較容易讓我了解。我覺得台上的表現真的很重要，都已經努力的完成一篇論文，而且被
接受了，在會議中是一個很好的機會去推廣自己的論文，如果觀眾就只有三三兩兩或是大部分
人都聽不懂，真的是很可惜。 
 
另外，應該來建個資料庫，來幫助我決定那些人的表演應該優先看，像是之後如果我再去參加
會議的話，可能統計方法的口頭演講就不會去看了，因為這方面的論文式子都很多，常常式子
都還沒看懂，就翻到下一頁投影片了，把時間花在這方面的演講，我的收獲都不大。或是應該
建議大會用一個投票的機制，來幫助大家判斷誰的表現比較好，誰的表現比較差，一方面，如
果像是在看海報的話，可以幫助觀眾較快決定出可看的海報來，如果是口頭演講的話，因為之
後會把錄好的影片放到網路上，把這類的訊息一起放上去，也可以節省大家看影片的時間；另
一方面，這樣也可以強迫演講者好好準備。 
填表人簽章 signature                         填表日期 Date：                                                                 
 
the areas that infrequently appear in most images. In the
second issue, we observe that an important factor in solv-
ing the co-segmentation problem is the establishment of a
suitable consistency measure between the foreground ar-
eas from any two different images. For those techniques
based on MRF, the consistency check is often carried out
by introducing a global term in the energy function, e.g.,
[11, 21, 26]. Such a global term is typically defined by the
similarity of two histograms from two potential foreground
areas. Since the number of possible foreground regions in
an image is two to the power of the number of superpix-
els (or pixels, in the extreme case), evaluating the global
term over all the possible foreground pairs becomes piv-
otal concerning efficiency. This aspect of consideration is
even more critical when more images are included for co-
segmentation. In our approach, we have proposed a new
and effective global term that satisfies the submodular con-
dition [17]. The resulting energy minimization can thus be
optimally solved by the graph-cut algorithm [4].
Another aspect of our effort is to investigate how to come
up with a good feature representation for co-segmentation.
In particular, we focus on the global energy term for the rea-
son just described. As in our formulation the potential fore-
ground regions evaluated by the global term are represented
as histograms of visual words, it is constructive to explore
whether enforcing the clustering criterion to consider the
property that the images share a common object would re-
sult in a more effective vocabulary. That is, we may prefer
that pixels (or sampled pixels) are grouped to form a visual
word owing to not only having similar descriptor values but
also spreading over different images. This point will be dis-
cussed in detail in Section 4. We summarize the main con-
tributions of this paper as follows:
• Introduce a co-saliency prior to make the unsupervised
co-segmentation possible.
• Establish a new global energy term to effectively solve
co-segmentation over multiple images.
• Propose a useful regularization term in K-means ob-
jective function to encourage gathering pixels with
similar appearance across different images.
2. Related work
Co-segmentation techniques most relevant to ours are
those heavily relying on the regularity of a global energy
term in their MRF model. Rother et al. [26] and Mukherjee
et al. [21] respectively use the L1 and the L2 norm to mea-
sure the dissimilarity between foreground histograms. The
main drawback of both is that solving the whole model be-
comes NP-hard. Hochbaum and Singh [11] subsequently
propose a “reward” model that satisfies the submodular
condition and therefore can be efficiently solved by graph
cuts. However, the inner product of two unnormalized his-
tograms representing the reward model is hard to give a
meaningful explanation of why it yields a suitable simi-
larity measure. Namely, a large inner product value by
their model does not imply that the two unnormalized his-
tograms are more similar. In [29], without directly com-
paring two histograms, Vicente et al. propose a new global
model to favor a co-segmentation result that all pixels asso-
ciated with a visual word are either uniformly from back-
ground or foreground. The criterion is reasonable when
the desired foregrounds of both images are indeed instances
of the same object with possible scale changes. It is not
clear if the model can be extended to handle more chal-
lenging backgrounds, viewpoint changes, and appearance
variations pertaining to the foreground object. While the
above methods [11, 21, 26, 29] all include a global term
in their MRF model and test on co-segmentation with only
two images containing identical or similar objects, Joulin et
al. [15] consider co-segmenting more than two images with
different instances from a more general concept of the same
object class. Their formulation treats co-segmentation as a
two-cluster problem, and yields impressive results. How-
ever, since the goodness of clustering depends on the ac-
curacy in evaluating the similarity between every two local
patches (or superpixels), the framework seems to require
fine over-segmentation, say, around 500 superpixels per im-
age to give satisfactory performances, and therefore results
in a less efficient implementation.
As is mentioned earlier, a number of co-segmentation
methods need user inputs to facilitate the process. The ap-
proaches by Mukherjee et al. [21] as well as by Hochbaum
and Singh [11] both require providing some scribbles (sim-
ilar to those in GrabCut [25]). Instead of suggesting the
scribbles at first, Batra et al. [1] propose to guide the user
to input additional strokes on the area that is the hardest
to decide the pixel labels. Without relying on the scribble
cues, Cui et al. [6] assume that one of the images is hand-
segmented. Rother et al. [26] add a constant penalty for
assuming the background label to avoid the trivial solution
that all pixels are labeled as background. Joulin et al. [15]
divide pixels into two clusters, and let the user choose which
cluster is more likely to be the common object cluster.
In passing, we notice that more recently Chen [5] has
proposed a scheme to find the common salient objects be-
tween a pair of images by enhancing the similar and pre-
attentive patches. However, it appears to be hard to gener-
alize the formulation to the case of handling more than two
images. Also, Rahtu et al. [22] and Ramanathan et al. [23]
both take account of the saliency information in segmenting
meaningful objects from a single image. Direct and feasi-
ble extensions of their approach to co-segmentation of two
or more images are not obvious in view of the difficulty in
sifting the saliency information from each image.
2130
3.2. Within-image MRF energy
We are now ready to define the within-image energy
Li(x
i) in (1) of binary labeling xi over superpixels {pij}
of Ii. Like in most of the conventional MRF models, Li
contains a data term and a pairwise smoothness term. To
specify the two terms, we need two additional definitions.
The first pertains to the cost of labeling a superpixel, say,
pij as foreground, and is given by
αij =
∑
k∈pi
j
τ − s˜ik (4)
where τ is a parameter to be adjusted and its discussions
will be provided in Section 5.1. The second definition con-
cerns the cost of assigning different labels to two adjacent
superpixels. Let E i be the edge set that encodes the ad-
jacency relations of {pij} and βij,k be the cost of different
labels between pij and pik, (j, k) ∈ E i. In particular, we
have
βij,k =
∑
(l,m)∈Bi
j,k
exp
(
−
‖vil − v
i
m‖
2
2σ2RGB
)
(5)
where vil and vim are the respective RGB values of pixels
l and m, and Bij,k includes all the pairs of adjacent pixels
across the boundary of superpixels pij and pik. (In our im-
plementation σRGB is set to 20/256.) With (4) and (5), the
exact form of Li(xi) can then be stated as follows:
Li(x
i) =
ni∑
j=1
αijx
i
j +
∑
(j,k)∈Ei
βij,kδ[x
i
j 6= x
i
k] (6)
where ni is the total number of superpixels in Ii, and δ is
an indicator function that outputs 1 when the statement is
true. The fact that βij,k > 0 for all (j, k) ∈ E i ensures the
following important regularity about Li(xi).
Property 1 The within-image MRF energy Li(xi) defined
in (6) is submodular.
3.3. Global energy term
In evaluating the global energy term E({xi}) in (1),
like [11, 21, 26], we represent each superpixel by an un-
normalized histogram h. It follows that the summation of
the histograms of all the superpixels within an area also
forms this area’s representation. Given a binary labeling
xi over image Ii, the implied foreground and background
can be respectively represented by
Hif =
ni∑
k=1
hikx
i
k and Hib =
ni∑
k=1
hik(1− x
i
k). (7)
We further denote the histogram of Ii as
Hi =
ni∑
k=1
hik = H
i
f +H
i
b. (8)
From (1), establishing the global term can be reduced to
specifying the between-image energy G(xi,xj , Ii, Ij). We
observe that good co-segmentation results often share two
important attributes—not only the foregrounds are similar
to each other but also each of them should be dissimilar to
its respective background. We thus define
G(xi,xj , Ii, Ij) = ‖Hif−H
j
f‖
2
2−
∑
k∈{i,j}
ck1‖H
k
f−c
k
2H
k
b‖
2
2
(9)
where c∗1 decides the influence of the dissimilarity, and c∗2 is
to balance the foreground and the background histograms;
otherwise, directly comparing these two un-normalized his-
tograms may not be reasonable, since their corresponding
areas can be of very different sizes. Note that the dis-
similarity measure in (9) is between the entire foreground
and background areas, which is different from the pairwise
terms of Li in (6) measuring only the local dissimilarities
between superpixels. For simplicity, we assume hereafter
c∗1 and c∗2 are respectively set to the same values c1 and c2.
By substituting Hib = Hi −Hif into (9), and taking the
definition of Hif in (7), we obtain
G(xi,xj , Ii, Ij) = C − 2
∑
l,m
〈
hil ,h
j
m
〉
xilx
j
m+
2c1c2(1 + c2)×
∑
k∈{i,j}
nk∑
l=1
〈
hkl ,H
k
〉
xkl +
(1− c1(1 + c2)
2)×
∑
k∈{i,j}
∑
l,m
〈
hkl ,h
k
m
〉
xkl x
k
m
(10)
where C is a constant term. Indeed the first three terms in
the RHS of (10) satisfy the submodular condition. Whether
G is a submodular function only depends on if the coeffi-
cient 1−c1(1+c2)2 of the last term is not greater than 0. We
let c1 = 1(1+c2)2 so that G can be submodular, and mean-
while assume a simpler form. Finally, by setting c = c21+c2 ,
G(xi,xj , Ii, Ij) becomes
C − 2
∑
l,m
〈
hil ,h
j
m
〉
xilx
j
m + 2c×
∑
k∈{i,j}
nk∑
l=1
〈
hkl ,H
k
〉
xkl .
(11)
From (11), we find that the global energy term in
Hochbaum and Singh [11] is a special case of our model
when c = 0 (i.e., c2 = 0). On the other hand, when c is set
2132
Num. of Without global term K-means K-means + L1,2Dataset
images
[15]
Saliency Co-Saliency Saliency Co-Saliency Saliency Co-Saliency {c2, τ}
Cars front 6 87.65% 77.01% 79.01% 83.27% 88.50% 88.04% 90.78% 90.46%
Cars back 6 85.10% 76.22% 77.63% 79.72% 81.86% 85.34% 85.76% 85.76%
Bike 30 63.30% 70.90% 72.38% 75.06% 76.67% 75.52% 76.76% 76.60%
Cat 24 74.40% 83.06% 79.80% 85.78% 86.36% 86.34% 86.68% 86.68%
Plane 30 75.90% 85.91% 86.22% 86.58% 86.80% 86.92% 87.66% 87.21%
Face 30 84.30% 78.54% 78.96% 84.41% 85.51% 85.08% 87.27% 85.76%
Cow 30 81.60% 88.40% 88.71% 91.25% 91.30% 91.10% 91.36% 90.92%
Horse 30 80.10% 78.72% 76.59% 85.30% 86.00% 85.57% 86.36% 84.36%
Gnome 4 89.29% 93.56% 93.28% 95.21% 95.00% 95.29% 95.12%
Table 1. Co-segmentation accuracy. The results by our method, measured in the pixel accuracy, are reported in the rightmost seven columns.
When the global energy term E in (1) is included, visual words can be obtained either by K-means or by K-means with L1,2 regularization.
Analogous to K-means clustering, we adopt an EM pro-
cedure to find {µk}Kk=1 andA in (12). In E-step, we first re-
lax Ai,j,k to [0, 1] so that an NP-hard problem can be trans-
formed into a convex optimization problem. We then use
the cvx toolbox [9] to solve A, and discretize its entries to
{0, 1} by setting Ai,j,k to 1 if k = argmaxl Ai,j,l and 0,
otherwise. In M-step, we compute µk by the mean of the
feature vectors of the pixels assigned to cluster k.
5. Experimental results
For the sake of comparison, we test our method with
the challenging datasets used in [15] which contain the
Weizman horses and MSRC database. We also include the
Gnome dataset in our experiments as it contains images
with large illumination and viewpoint changes of the same
object. Figure 4 shows some examples of these images and
the co-segmentation results. (Note that the images in Weiz-
man horses are resized to have the same larger dimension.)
5.1. Parameters
Our model has three parameters, {λ, τ, c}. τ appears in
(4), and its value is decided by running our algorithm with-
out the global term. λ and c are introduced in (1) and (11),
respectively. Recall that c = c2/(1 + c2) and from (9),
c2 can be thought of the ratio of the average area of fore-
grounds to the average area of backgrounds over {Ii}Mi=1.
For each dataset, we uniformly sample c2 from a given
range, adjust λ heuristically, and then report the best re-
sult. Indeed, the set of parameters can be reduced to {τ, c}
with slight decrease in accuracy, as λ can be tuned unsu-
pervisedly by checking whether the co-segmentation results
match the foreground-background ratio implied by c2.
5.2. Accuracy
We test our co-segmentation method in seven different
settings. First, the within-image energy Li(xi) can be im-
plemented with the cost of assigning a foreground label ac-
cording to either the co-saliency prior used in (4) or the
saliency prior by replacing s˜ik with sik. Second, to single
out the effect of the global term E in (1), the experiments
are also performed with or without this global term. In case
thatE is included, the histogram representation will be used
to describe a superpixel or an area of superpixels, and we
further consider the two ways of constructing visual vocab-
ularies described in Section 4. When the co-saliency prior
and the L1,2 regularization are used, we additionally test
our method by tuning only τ and c. The respective results
are reported in the rightmost seven columns of Table 1.
It can be inferred from the results in Table 1 that the
co-saliency prior tends to yield better co-segmentation per-
formances than the saliency prior, except implementing our
model without using the global term to test the two datasets,
Cat and Horse. And, such few exceptions are expected
since the between-image factors are not considered here.
We next look into the importance of using the global en-
ergy term E in co-segmentation. In Table 1, the results
by including E are those in the rightmost five columns,
and they are uniformly superior to those without using the
global term. However, considering the global term means
the necessity of the two parameters λ and c, where the
former controls its contribution, and the latter enables our
model to tackle the high complexity of co-segmentation
over more than two images.
The last factor discussed here that has a bearing on the
co-segmentation accuracy is how the visual words are ob-
tained. Recall that in Section 4, the L1,2 regularization term
is formulated based on the assumption that we have a region
Ri that has a higher probability of intersecting the underly-
ing foreground in image Ii. In practice, we have no access
to such knowledge in an unsupervised approach. Neverthe-
less, a reasonable way to yield Ri is as follows. Besides
the co-saliency map S˜i, we also apply the Gaussian center
prior [16] to Ii and generate, say, Oi. The superpixels inter-
secting the areas with the top 20% values of S˜i×Oi are then
included in Ri. The strategy is general in the sense that the
ratio between the area of the resulting Ri to the area of Ii
2134
Figure 4. Examples of the input images and the co-segmentation results.
[22] E. Rahtu, J. Kannala, M. Salo, and J. Heikkila. Segmenting
salient objects from images and videos. In ECCV, pages V:
366–379, 2010.
[23] S. Ramanathan, H. Katti, N. Sebe, M. Kankanhalli, and
T. Chua. An eye fixation database for saliency detection in
images. In ECCV, pages IV: 30–43, 2010.
[24] R. Rao and D. Ballard. Predictive coding in the visual
cortex: a functional interpretation of some extra-classical
receptive-field effects. Nature neuroscience, 2(1):79–87,
January 1999.
[25] C. Rother, V. Kolmogorov, and A. Blake. Grabcut: interac-
tive foreground extraction using iterated graph cuts. ACM
Trans. Graph., 23(3):309–314, 2004.
[26] C. Rother, T. Minka, A. Blake, and V. Kolmogorov. Coseg-
mentation of image pairs by histogram matching: Incorpo-
rating a global constraint into MRFs. In CVPR, pages I:
993–1000, 2006.
[27] A. Treisman and G. Gelade. A feature-integration theory
of attention. Cognitive Psychology, 12(1):97–136, January
1980.
[28] O. Veksler. Star shape prior for graph-cut image segmenta-
tion. In ECCV, pages III: 454–467, 2008.
[29] S. Vicente, V. Kolmogorov, and C. Rother. Cosegmentation
revisited: Models and optimization. In ECCV, pages II: 465–
479, 2010.
[30] J. Wolfe. Guided search 2.0: A revised model of visual
search. Psychonomic Bulletin & Review, 1(2):202–238,
1994.
2136
97 年度專題研究計畫研究成果彙整表 
計畫主持人：劉庭祿 計畫編號：97-2221-E-001-019-MY3 
計畫名稱：植基於機器學習的物件類別辨識技術 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 1 100%  
博士生 3 3 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 6 6 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
