  The traditional methods mentioned above require 
complex embedding and corresponding extraction procedures.  
In this paper we proposed a specific full counterpropagation 
neural network (FCNN) for watermarking. Different from the 
traditional methods, the watermark is embedded in the synapses 
of FCNN rather than the cover image. Hence the quality of the 
watermarked image is almost same as the original cover image. 
In addition, the quality of the extracted watermark image does 
not degrade after most attacks, because the watermark is stored 
in the synapses. [1] In our method, FCNN simplifies the 
complex embedding and corresponding extraction procedures 
and the FCNN has the capability for embedding and extracting 
watermark from images. The experimental results show that 
there was no need for these procedures in FCNN-based 
watermarking. Furthermore, the proposed method is able to 
achieve robustness, imperceptibility and authenticity in 
watermarking. 
 The following sections discuss the application of FCNN 
in watermarking in the proposed method. Section II introduces 
the FCNN, the algorithm of embedding, and the method of 
extraction. Section III summarizes the experimental results, and 
conclusions are given in Section IV.  
 
II. FULL COUNTERPROPAGATION NEURAL NETWORK FOR 
WATERMARKING 
 In this paper, a novel full counterpropagation neural 
network (FCNN) is proposed for image watermarking. The full 
counterpropatation neural network is a supervised-learning 
network with capacity of bidirectional mapping. Figure 1 
shows the schematic block diagram of the FCNN. 
 
 
(a) 
 
(b) 
Fig.1. The schematic block diagram of the FCNN. (a) embedding 
procedure. (b) extracting procedure. 
 
 The traditional watermarking methods require complex 
embedding and corresponding extraction procedures. However, 
the proposed watermarking method integrates the embedding 
and extraction procedure into a full counterpropagation based 
neural network. In order to ensure that the proposed 
watermarking method has capability of embedded and 
extracted watermarks, the cover images and watermark image 
are input to the FCNN simultaneously during the embedding 
process. After the network’s evolution, the watermarked 
images were generated. In addition, a FCNN could embed 
multiple images simultaneously. On other hand, the same 
FCNN was used for extracting the corresponding watermark 
from the watermarked images with or without being attacked. 
 Therefore the purpose of this paper is to present a full 
counterpropagation neural network for watermarking. The 
FCNN is designed to learn bidirectional mappings. Through the 
process of supervised training, the FCNN adaptively constructs 
a lookup table approximating the mapping between the 
presented input/output training pair: cover images Xs and 
watermark image Y.  After being trained, the FCNN can be 
used to extract the corresponding watermark Y* if the 
embedded image Xs* is known. Figure 2 shows the architecture 
of the proposed FCNN. 
 
Cover image X 
x1 
x2 
 
 
xn 
Input layer Output layer
 Watermark Y
y1 
y2 
 
 
ym 
x*1 
x*2 
 
 
x*n 
y*1 
y*2 
 
 
y*m 
Watermarked image 
Extracted Watermark Y*
W
U
V 
T 
Neuron 
Z1 
Z2 
Zl 
Fig. 2. The architecture of the full counterpropagation neural network 
for watermarking 
 
The s-th two-dimensional cover image Xs and watermark image 
Y can be written in vector form as: 
 { }snsss xxxX ,,, 21 L=   (1) 
 { }myyyY ,,, 21 L=   (2) 
where n is the number of pixels of the s-th cover image X, and 
m is the number of pixels of the watermark image Y. The input 
vectors X and Y are connected to neuron liZi ,,1, L=  with 
weights W and U, respectively.  
 
{ }11 12 1 21 2, , , , , , , , ,
1, ,
n n inW w w w w w w
i l
=
=
L L L
L
   (3) 
 
{ }11 12 1 21 2, , , , , , , , ,
1, ,
m m imU u u u u u u
i l
=
=
L L L
L
inw
nx im
my
( ) ( ) liuywxZ n
k
m
k
ikkik
s
ki ,,1,
1 1
22 L=−+−= ∑ ∑
= =
  (4) 
where  denotes the weight between i-th neuron and input 
. Similarly, u  denotes the weight between i-th neuron and 
input . 
Accordingly, the total input of the i-th neuron is defined as  
 (5) 
Full 
Counter-Propagation 
Neural Network 
Cover Images 
Watermark 
Watermarked Images
… 
…
Watermarked Images 
Watermark
Full 
Counter-Propagation 
Neural Network … 
 cover images including Jet, Baboon and Couple, and the image 
size is . In addition, the gray scale and binary 
watermark images are the school badge of National Yunlin 
University of Science and Technology (NYUST) with image 
size and , respectively. Figures 3(a-e) show 
the cover images and watermark images. 
256256×
32× 12832 128×
 
(a) 
 
(b) 
 
(c) 
 
 
 
 
(d) (e)  
A. Imperceptibility Testing 
During the embedding process, the three cover images are 
input to the FCNN sequentially, the initial learning rates α  
and β  were set to 0.7 and 0.5, respectively.  The number of 
neurons is 64 and the constant k0 was set to 10. The threshold 
value of 5 was established to terminate training. Figures 4(a-c) 
show the watermarked Jet, Baboon and Couple images 
respectively. Figures 5(a-c) show the extracted watermarks 
from the Figs.4(a-c). 
 
 
(a) 
 
(b) 
 
(c) 
Fig. 3. Cover images and watermark images. (a) Cover Jet image, (b) 
Cover Baboon image. (c) Cover Couple image (d) Gray Scale 
Watermark (e) Binary Watermark. 
 
Fig. 4.  Watermarked images. (a) watermarked Jet Image.  The Peak Signal to Noise Ratio (PSNR) and Normalized 
Correlation (NC) were used to evaluate the quality of 
watermarked image and extracted watermark, which can be 
represented as Eq.(20) and Eq.(22). 
(b) watermarked Baboon image.(c) watermarked Couple image. 
 
TABLE I 
THE PSNR OF THE WATERMARKED IMAGE AND THE 
NORMALIZED CORRELATION & PSNR OF  THE EXTRACTED 
WATERMARK  2
2
10log10)(
e
peakXdBPSNR σ=     (20) 
where  is defined as 2eσ
 ( )( )∑∑
= =
−⎟⎠
⎞⎜⎝
⎛=
M
i
N
j
e jiZjiXMN 1 1
22 ,),(1σ      (21) 
Watermarked 
Image 
PSNR of 
Watermarked 
image 
NC of 
Extracted 
Watermark
PSNR of 
Extracted 
Watermark
Jet 42.67 dB 0.9917 42.13 dB
Baboon 51.25 dB 0.9960 50.30 dB
Couple 43.98 dB 0.9919 42.35 dBwhere  is the size of cover image,  is the 
graylevel of (i,j)-th pixel of cover image. Similarly,  
denotes the gray level of (i,j)-th pixel of watermarked image. 
 denotes the squared peak value of cover image.  
NM × ),( jiX
),( jiZ
2
peakX
 
 
( ) (
( ) ( )
)
∑∑
∑∑
= =
= == M
i
N
j
M
i
N
j
jiWjiW
jiWjiW
NC
1 1
1 1
'
,,
,,
 (22) 
Observing Fig.3 and Fig.4, there is no visually distinguishable 
difference between the original images and their corresponding 
watermarked versions. The PSNR values of watermarked 
images, the Normalized Correlation and PSNR of the extracted 
watermark image are shown in table I. The high PSNR and NC 
values indicate that the watermarked images and extracted 
watermark are the most similar to the original image and 
watermark.  
 
where  is the size of watermark image,  is gray 
level of -th pixel of cover watermark,  is gray 
level of -th pixel of extracted watermark. The higher 
PSNR is the more similarity between the cover image and the 
watermarked image, and the higher NC value is the more 
similarity between the original watermark and the extracted 
watermark.  
NM ×
),( ji
),( ji
),( jiW
),(' jiW  
(a) 
 
(b) 
 
(c) 
Fig. 5.  The extracted watermark. (a)extracted from Fig.4(a). 
(b)extracted from Fig.4(b). (c) extracted from Fig. 4(c). 
 
Furthermore, we could also embed binary watermark in the 
proposed FCNN. The same parameters of the proceeding 
 TABLE IV 
THE PSNR OF EXTRACTED WATERMARK FROM THE ATTACKED 
WATERMARK JET IMAGE BY DIFFERENT METHOD 
Attacks \ Method FCNN PSNR 
Hsieh’s Method 
PSNR 
Cropped 42.13 dB 12.76 dB 
Sharpened 42.13 dB 6.72   dB 
Blurred 42.13 dB 7.71   dB 
Median Filter 42.13 dB 9.09   dB 
Mosaic 42.13 dB 22.27 dB 
JPEG (Q=80%) 42.13 dB 19.20 dB 
 
TABLE V 
THE NORMALIZED CORRELATION OF THE EXTRACTED 
WATERMARK FROM ATTACKED WATERMARKED JET IMAGE BY 
DIFFERENT METHOD 
Attacks    \   Method FCNN NC 
Hsieh’s Method 
NC 
Cropped 0.9917 0.8427 
Sharpened 0.9917 0.6527 
Blurred 0.9917 0.5741 
Median Filter 0.9917 0.6442 
Mosaic 0.9917 0.9168 
JPEG (Q=80%) 0.9917 0.8694 
 
Figures 10(a - f) exhibit the extracted watermarks from Hsieh’s 
Method with the watermarked Jet Image attacked by the same 
attacks. Obviously, all the watermarks were completely 
extracted by means of FCNN, which evidences robustness 
facilitated by our method. Table IV and Table V show the 
PSNR and NC values of the extracted watermarks after various 
attacks. As the results show, it indicates the proposed method is 
robust enough to resist attacks such as crop right-top 1/4, 3x3 
Laplacian mask, 3x3 averaging filter, median filter, 2x2 mosaic 
and jpeg compression. While Hsieh’s Method is fail to extract 
meaningful watermark under sharpened, blurred and median 
attacks. 
 
2) Robustness Testing for Binary Watermark 
 
(a) 
 
 
(b) 
Fig. 11. The watermarked image and extracted watermark by Deng’s 
method. 
 
 
(a) 
 
 
(b) 
Fig. 12. The watermarked image and extracted watermark by Hsu’s 
method. 
 
 
(a) 
 
 
(b) 
Fig. 13. The watermarked image and extracted watermark by proposed 
method. 
 
TABLE VI 
THE PSNR OF  WATERMARKED IMAGE  AND NC VALUES OF 
EXTRACTED WATERMARK BY DIFFERENT METHOD 
 To evaluate the robustness of embedding binary 
watermark image, the proposed FCNN is compare with Deng’s 
[5] and Hsu’s [8] methods.  The binary watermark as shown in 
Fig. 3(e) is embedded into the Jet image as shown in Fig. 3(a). 
Figures 11(a-b) show the watermarked image and extracted 
watermark by Deng’s method, respectively. Figures 12(a-b) 
show the watermarked image and extracted watermark by 
Hsu’s method, respectively. Obviously, there is no visually 
distinguishable difference between the cover image and their 
corresponding watermarked versions while there exists a lot of 
redundant noise in the extracted watermarks. Figures 13(a-b) 
show the watermarked image and the extracted watermark by 
proposed FCNN. Again, there is no perceptive difference 
between the cover image and the watermarked version. The 
corresponding watermark is extracted completely. 
Value \ 
Method FCNN 
Deng’s 
Method  
Hsu’s 
Method  
PSNR  49.62 dB 28.37 dB 39.12 dB 
NC  0.9960 0.7198 0.8108 
 
Table VI shows the PSNR value of the watermarked image and 
the NC value of the extracted watermark without any attack of 
the proposed FCNN, Deng’s and Hsu’s methods. As the results 
show, not only PSNR of watermarked image but also NC value 
of extracted watermark of the proposed FCNN is higher than 
the other two methods. Therefore, the FCNN has good 
capability to embed and extract the corresponding watermark in 
binary watermark, too.  
 
(a) (b) 
 
(c) 
 
(d) (e) (f) 
Fig. 14.  The extracted watermarks of watermarked Jet image by the 
proposed method. 
  
  
 
(a) 
 
(b) 
 
(c) 
 Fig. 21. The extracted watermarks. (a)extract from Fig. 20(a). 
(b)extract from Fig. 20(b). (c)extract from Fig. 20(c). 
 
TABLE VIII 
THE PSNR OF THE WATERMARKED IMAGE AND THE 
NORMALIZED CORRELATION & PSNR OF THE EXTRACTED 
GRAY SCALE WATERMARK IN MULTIPLE WATERMARKS  
Watermarked 
Image PSNR NC PSNR 
Jet 46.70 dB 0.9948 46.99 dB 
Baboon 45.13 dB 0.9908 40.39 dB 
Couple 43.96 dB 0.9905 40.35 dB 
 
E. Discussions 
There are many parameters including the learning rate α, β, 
the number of neurons and the terminative threshold value need 
to assign during the FCNN evolution. Especially, the number of 
neurons and the terminative threshold value play important 
roles in the proposed FCNN. Thus, empirical parameters 
suggestions are concluded in this section.  
Since the proposed FCNN has capability of multiple images 
watermarking, it needs enough neurons for embedding 
watermarks. The number of neurons of the proposed FCNN is 
highly depending on the number of cover images. Intuitively, if 
we want to embed cover images, the FCNN needs at least  
neurons. However, suppose that the number of neurons of the 
FCNN is equal to the number of cover images, the trained 
FCNN can extract watermark from watermarked image. 
Unfortunately, the FCNN extracted a wrong watermark if we 
input non-embed cover images into it. Hence, the number of 
neurons must be greater than the number of cover images. But 
the more quantity of neurons will be the more time complexity 
in training. On the other hand, it is easier to extract a wrong 
watermark if the number of neurons of the FCNN is smaller 
than the number of the cover images. There is a tradeoff 
between the training time and the correctness of extracted 
watermark. Accordingly, how to determine the number of 
neurons is a key point in FCNN method.  
n n
The normalized correlation (NC) value is used to measure 
the similarity between the original watermark and the extracted 
watermark. As mentioned, the higher NC value is the more 
similarity. Thus, it is worse to determine an acceptable NC 
value to represent the detection threshold. One hundred 
watermarks with size 32×32 selected randomly are used to 
compare the similarity with the original watermark. The 
compared NC values between the original watermark (Fig. 3(d)) 
and the 100 watermarks are shown in Fig. 22. Observing Fig. 
22, the 65-th watermark has the largest NC value of 0.64. It 
means that the 65-th watermark is the most similar to the 
original watermark. Hence, to obtain a more confidential 
watermarking result, the suggested detection threshold is no 
less than 0.8 (i.e., the extracted watermark is consider to be 
wrong or low similarity, if the NC value is less than 0.8).  
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 20 40 60 80 100
Number of watermarks
NC
 
Fig. 22 Normalized correlation results with 100 watermarks. 
 
 In order to determine the number of the neurons and the 
authenticity of FCNN method, 100 non-embedded raw cover 
images with size of 256×256 were selected randomly from 
image database.  In this experiment, we embedded one, two, 
and three watermarks into FCNN with different numbers of 
neuron. The 100 non-embedded raw cover images were input 
to the converged FCNN sequentially to extract corresponding 
watermark.  If the NC value of the extracted watermark is less 
than the suggested detection threshold of 0.8, the extracted 
watermark will be considered as insignificant watermark. 
Figure 23 shows the authenticity of FCNN under different 
neurons and watermarks. In which, the authenticity is defined 
as 
TC WWtyAuthentici / , (23) =
where WC denotes the number of insignificant watermarks 
(NC<0.8) and WT represents total number of non-embed raw 
cover images (100 in this experiment). Observing Fig. 23, we 
can find that the authenticity is rapidly increasing as the number 
of neuron is greater than 14.  Especially, the authenticity is 
approximating to 1 when the number of neuron is greater than 
64.  Since there is a tradeoff between the training time and the 
correctness of the extracted watermark, accordingly, we 
suggest the number of the neurons is 15 times to 25 times the 
number of cover images. 
Moreover, the terminative condition (threshold value) is also 
an important parameter in the proposed FCNN method. Table 
IX shows the simulation results under different threshold 
values. As Table IX shows, smaller threshold value will leading 
to longer training time. On the contrary, larger threshold value 
will cause the FCNN output lower quality watermarked cover 
image. To obtain reasonable quality of cover image and 
extracted watermark, the suggested threshold value is not 
exceed 30. 
 
 and System for Video Technology, vol. 10, pp. 974-979, 
2000. 
[7] Ming-Shing Hsieh, Din-Chang Tseng and Yong-Huai 
Huang, “Hiding digital watermarks using multiresolution 
wavelet transform,” IEEE Trans. Industrial Electronics, 
vol. 48, pp. 875 – 882, 2001. 
[8] Chiou-Ting Hsu and Ja-Ling Wu, “Hidden digital 
watermarks in images,” IEEE Trans. Image Processing, 
vol. 8, pp. 58-68, 1999. 
[9] K.J Davis and K. Najarian, “Maximizing Strength of 
Digital Watermarks Using Neural Networks,” Proc. Int. 
Joint Conf. Neural Networks, pp.2893–2898, 2001. 
[10] Shi-chun Mei, Ren-hong Li, H. Dang and Yun-kuan Wang, 
“Decision of image watermarking strength based on 
artificial neural-networks,” in Proc. 9th Int. Conf. Neural 
Information Processing, pp. 2430–2434, 2002. 
[11] Zhang Zhi-Ming, Li Rong-Yan, Wang Lei, “Adaptive 
Watermark Scheme with RBF Neural Networks,” Proc. Int. 
Conf.  Neural Networks and Signal Processing, pp. 
1517–1520, 2003. 
[12] R. Hecht-Nielsen, “Counterpropagation Networks,” Proc. 
of IEEE Int. Conf. on Neural Network, pp. 19-32, 1987.   
[13] Liu Jun and Wang Danwei, “Data Compression For Image 
Recognition,” in   Proc. International Joint Conference on 
Neural Network , pp. 333-338, 1992. 
[14] Zafar M.F and Mohamad D, “Counterpropagation neural 
networks for trademark recognition,” Proc. Int. Symp. 
Signal Processing and its Applications, pp. 751-752, 2001. 
[15] Chang, W., Soliman H.S. and Sung A.H., “Image data 
compression using counterpropagation network,” Proc. 
IEEE Int. Conf. on Systems, Man and Cybernetics, pp. 
405-409, 1992. 
 系統識別號 
公務出國報告提要 
 
出國報告名稱：ICICIC2007出席後感想 
                                            頁數     含附件：□是□否 
 
出國計畫主辦機關/聯絡人/電話 
Graduate School of Science and Technology, Tokai University 
出國人員姓名/服務機關/單位/職稱/電話 
張傳育/國立雲林科技大學/電腦與通訊工程系/副教授/05-5342601 ext. 4337 
出國類別：□1考察□2進修□3研究□4實習55其他（如出席會議、參觀訪問、
比賽、業務視察等） 
 
出國期間：96/9/4/~96/9/8         出國地區：日本熊本 
 
 
報告日期：96/10/2 
 
 
分類號/目： 
 
 
關鍵詞： 
ICICIC 2007、neural network。 
 
內容摘要：（200~300字） 
 
本出國報告敘述本人到日本熊本出席 ICICIC2007 會議，本人組織一特別
議程” Innovative Computing for Clustering”，擔任該議程主持人並發表兩篇論文
的經過及收穫。此次會議有上千篇論文的投稿，最後只接受 624篇論文的發表，
接受率約為六成。由於是高雄應用科技大學潘正祥教授擔任此次會議的議程主
席，因此國內學術界參與此會議的人數還不算少，也因此整個會議的與會人員
互動相當良好，彼此很多的機會可以深入的了解最新的研究成果。綜合而言，
藉由此會議的研討，獲致底下幾點收穫：(1)了解相關領域最新的發展狀況。(2)
應用在分類與分割等相關問題，所有作者均出席該 session並發表論文，會中討
論相當熱烈，是一個成功的論文發表會。 
 
心得： 
在會議進行過程，除了可以聆聽來自世界各國最新的研究成果外，更可利
用休息的時間與相關學者專家進行聯誼交流。此外經由這次會議，認識了許多
學者專家，對於國家及個人在國際能見度的提升確實有顯著的幫助。此次發表
的論文中”Learning Vector Quantization Neural Networks for LED Wafer Defect 
Inspection”已被邀請轉投到國際知名期刊 International Journal of Innovative 
Computing, Information and Control (IJICIC) 的 Special Issue "New Trends in 
Computational Intelligence and Applications"，此行成果豐富。 
 
 
建議： 
1. 國科會應多鼓勵年輕學者出席國際會議，給予較多的出國補助。 
2. 若國內有多人同時參加一個會議。可組團前往，一方面節省旅費，一方面可
相互照應。 
   
                           (a)                  (b)                 (c) 
Fig. 1. Three die images from LED wafer image: (a) a defect-
free die and its structure; (b) P-electrode defect and (c) light-
emitted area defect.  
33u median filter to reduce the influence of noise. 
Figure 3(a) shows the filtering result with the median 
filter. 
Second, according to the intensity difference 
between the P-electrode and light-emitted area, we use 
the k-means clustering method [6] to classify each die 
into three classes (P-electrode, light-emitted area and 
background) for getting the approximate P-electrode 
region. In order to inspect the P-electrode defects, we 
need to extract the accurate P-electrode for further 
processing. 
Because the shape of the P-electrode tends to be a 
circle, we can find the center of the circle and the 
radius to approximate the shape of the P-electrode.     
Using the center and the radius, we can plot a circle 
shape to outline the circular P-electrode accompanied 
with a probing scar (see Fig. 3(b)). Then the Otsu’s 
method [7] was performed to remove the probing scar. 
It can automatically select an optimal threshold value 
for separating objects of interest in an image from the 
background based on their gray level distribution. 
Using Otsu’s method, we can remove probing scars 
from circular areas (P-electrode part). For convenience, 
the pure P-electrode part is called “E-image” (see Fig. 
3(c)). Similarly, the region excluding the circular area 
and the probing scar is the light-emitted area. We name 
the light-emitted area “L-image” (see Fig. 3(d)). 
2.2. Feature Extraction 
By analyzing the distribution of the intensity, we 
extract the geometric features and texture features from 
the E-image and the L-image. These features are used 
for the proposed LVQ neural network later in section 
2.3.  
(a) Geometric Features Extraction: 
Transformation: We transform E-image and L-image 
into binary images (see Fig. 4(a)-(d)). The binary E-
image and L-image, which provide important shape 
information, will be processed further in geometric 
feature extraction. 
Geometric Features: In general, all the dies on the 
wafer have the same appearance. For instance, the  
Fig. 2.  Inspection flowchart.
shape of a die tends to be a square and the shape of P-
electrode tends to be a circle. Thus, we extract the 
geometric features to represent the characteristics of 
the die. The geometric features adopted for inspection 
are described in the following. 
1. The perimeter of P-electrode EP .
2. The perimeter of light-emitted area LP .
3. The area of P-electrode EA .
4. The area of light-emitted area LA .
5. The circularity of P-electrode region C (C = 1 
for a circle) is defined by 
2
4
E
E
P
A
C u S
Thus, the characteristics of the E-image and the L-
image can be represented as vectors EG and LG that are 
constituted by the geometric features respectively. 
Vectors EG and LG are defined in the following 
expressions: 
       > @TEEE CAPG ,, and > @TLLL APG , 
(b) Texture Features Extraction: 
Morphological processing: Observing Fig. 3(a) and 
(b), the boundary of the P-electrode is surrounds by a 
black circular mark. These pixels with lower intensities 
may affect the accuracy of the inspection algorithm. 
(1)
second LVQ for further detection.  In the second stage, 
analogous to the first LVQ, the texture features ( ET
and LT ) are used for the second LVQ to detect any 
tiny defects. The non-defective dies are identified if the 
potential non-defective dies pass the second LVQ; 
otherwise, the dies are classified as defective ones. 
3.  EXPERIMENTAL RESULTS 
In the experiments, we tested the feasibility and 
performance of the proposed method. The LED wafer 
image containing about thousands of dies was used for 
testing. The size of the LED wafer image was 10000 ͪʳ
˄˃˃˃˃ʳ pixels. Each pixel consists of 256 gray level 
values. The proposed method was implemented by 
C++ Builder 6.0 on an Intel Pentium IV 2.8GHz 
processor with 1GB RAM. 
Figure 5 shows the inspection results of the 
proposed method. Figure 5(a) is the test sample image. 
If the P-electrode or light-emitted area has defects, the 
inspection system will mark the die (see Fig. 5(b) and 
(c)). No matter if defects exist on the P-electrode or 
light-emitted area, the die is considered as defective. 
Figure 5(d) show the detection results of the proposed 
method. All the defective dies were outlined correctly. 
  To evaluate the performance of the proposed 
method, we adopt the criteria suggested in [9] to define 
sensitivity and specificity for performance evaluation. 
The defective dies which were inspected by the 
proposed method were compared with actual defective 
dies which were checked by people manually. Table I 
shows the quantitative evaluation results of the wafer 
detection results. The sensitivity and specificity of 
wafer images are 0.955 and 0.902, respectively. As a 
result, these values demonstrate the effectiveness of the 
proposed technique. 
4. CONCLUSIONS 
In this paper we have proposed an LVQ neural 
network based on an automatic inspection method for 
LED wafer images. By taking advantage of properties 
of distribution of gray level values and geometric 
features, we can find defective dies by the proposed 
method. The experimental results show the proposed 
automatic inspection method successfully identifies, 
with good performance, defective dies on wafer images. 
ACKNOWLEDGMENT 
This work was supported by the National Science 
Council Taiwan, R.O.C, under the grants NSC 94-
2212-E-224-008
REFERENCES 
[1] Chao-To Su, Taho Yang, Chir-Mour Ke, “Neural-
Network Approach for Semiconductor Wafer Post-
Sawing Inspection,” IEEE Transactions on 
Semiconductor Manufacturing, vol. 15, No. 2, pp.260-
266, 2002. 
[2] Kenneth W. Tobin, Jr., Thomas P. Karnowski, Fred 
Lakhani, “Integrated Applications of Inspection Data in 
the Semiconductor Manufacturing Environment,” SPIE, 
Metrology-based Control for Micro-Manufacturing, vol. 
4275, pp. 31-40, 2001. 
[3] Mital, D.P. Teoh, E.K. “Computer Based Wafer 
Inspection System,” Proceedings. IECON ’91., 1991 
Proceeding of International Conference on Industrial 
Electronics, Control and Instrumentation, vol. 3, pp. 
2497-2503, 1991 
[4] J.-M. Zang, R.-M. Lin, and M.-J. Wang, “The 
Development of an Automatic Post-Sawing Inspection 
System Using Computer Vision Techniques,” 
Computers in Industry, vol. 30, pp. 51-60, 1999 
[5] C.-Y. Chang, H.-J. Wang and S.-Y. Lin, “Simulation 
Studies of Two-layer Hopfield Neural Networks for 
Automatic Wafer Defect Inspection”, Lecture Notes in 
Computer Science, vol. 4031, (2006), 1119–1126 
[6] Jain, A.K., Dubes, R.C., Algorithms for Clustering Data,
Prentice Hall, 1988. 
[7] N. Otsu, “A threshold selection method from gray-level 
histograms,” IEEE Transactions on Systems, Man and 
Cybernetics, 9(1):62–66, 1979. 
[8] T. Kohonen, “Improved Versions of Learning Vector 
Quantization, “Proceedings of the International Joint 
Conference on Neural Networks, San Diego, CA, vol. 1 
1990, pp. 545-50.   
[9] Manuel G. Penedo, Maria J. Carreria, Antonio 
Mosquera, and Diego Cabello, “Computer-Aided 
Diagnosis: A Neural-Network-Based Approach to Lung 
Nodule Detection”, IEEE Trans. on Medical Imaging,
17, 872-880 (1998). 
TABLE I 
PERFORMANCE EVALUTION  
 Sensitivity Specificity 
Wafer image(contains 
thousands of dies) 
0.955 0.902 
(a)            (b)            (c)             (d)
Fig. 5. Inspection results: (a) die image for test; (b) inspection results of only P-electrode in die image; (c) inspection results of 
only light-emitted area in die image and (d) final inspection results of die image where the combined results in (b) and (c).
two specific designed masks are applied to segment the 
light-emitted area and P-electrode. 
Fig.2. The flow chart of LED defect inspection system 
2.2. Feature Extraction 
The purpose of the feature extraction is to obtain a 
set of measures that has ability to discriminate between 
defect and non-defect. For presentation convenience, 
the light-emitted area and P-electrode are denoted 
as LU and EU , respectively. Let LsizeU ,
L
averageF ,
L
sdF
denote the area size, average, and standard deviation of 
light-emitted area, respectively. Similarly, 
let EsizeU ,
E
averageF , and
E
sdF  denote the size, average, and 
standard deviation of P-electrode, respectively. On the 
other hand, EnumberF denotes the number of pixels with 
intensity lager than optimalT  in P-electrode 
and EpercentF denotes the number of pixels with intensity 
between EsdF and 
E
sdF in P-electrode. These features 
are defined as following. 
¦

 
LUyx
L
size
L
average yxsU
F
),(
),(1  (1) 
2/12
),(
))),((1( ¦

 
LUyx
L
averageL
size
L
sd FyxsU
F  (2) 
E
Tyxs
Uyxnumber
E
average UyxyxsF
F
optimal
E
 ¦
t

),(,),(1
),(if,
),(
 (3) 
¦
t

 
optimal
E
Tyxs
Uyx
E
average
number
E
sd FyxsF
F
),(if,
),(
2/12 ))),((1(  (4) 
¯
®
­
 
t 
 ¦

optimal
optimal
Uyx
E
number
Tyxsnumbr
Tyxsnumbr
numberF
E
),(if,0
),(if,1
,
),(
 (5) 
°¯
°
®
­
 
dd 
 ¦

otherwise,0
),(if,1,
),(
percent
FFyxsFpercent
percentF
E
sd
E
average
E
sd
Uyx
E
percent
E
 (6) 
where ),( yxs is gray level and optimalT is the optimal 
threshold. 
2.3. Otsu’s Method
Otsu proposed an automatic thresholding algorithm 
based on discriminant analysis which maximizes the 
inter-classes separability or minimizes the intra-classes 
inseparability in gray levels. In this paper, the optimal 
threshold value optimalT  is selected by searching for a 
value in the range > @1,0 L , which maximizes the inter-
classes separability. The optimal threshold is then 
adopted to separate the P-electrode and probing scar. 
2.4. Radial Basis Function Neural Network
In this paper, we applied the RBFNN to detect the 
defects on P-electrode. The network is presented with 
training pairs, each consisting of vector from an input 
space and a desire network response. The architecture 
of the RBF is presented in Figure 3. The network 
consists of three layers: an input layer, a single layer of 
nonlinear processing neurons, and an output layer. 
Therefore, using the stochastic gradient-based 
supervised learning algorithm, the error cost function 
is defined as 
2
1
2
2
2
2
1
2
])
)(
)()(
exp()()([
2
1
])}(),({)()([
2
1
)(
2
1)(
¦
¦
 
 

 
 
 
N
k k
k
desired
N
k
kdesired
n
nn
nwny
nnnwny
nenJ
V
I
dy
dy
(7) 
where 1u mRy  is an input vector, I  is the Gaussian 
function of the RBF neural network, w  are the weight 
vectors, N is the number of neurons in the hidden 
