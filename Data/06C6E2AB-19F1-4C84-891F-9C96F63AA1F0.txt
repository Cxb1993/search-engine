 2
計畫中文摘要 
我們發展一系列的調適性原型學習計算（APL）法則。它們使用同樣的法則架構來決
定原型的數量與位置，不同點在於使用樣本自身或樣本的加權平均作為原型，或者在於距
離量度的不同之設定。為了充分明白這些法則在理論上的意義，我們討論它們的收斂性質
以及在某些條件下的一致性。我們也呈現 APL 的軟態模式，在這種模式裡我們允許高於零
的訓練錯誤，用以提升所製造的分類器之推論效能。我們應用這些法則在一些 UCI 的數據
資料庫，證明它們的平均測試成效超過許多以個例型態的學習法則，k-近鄰法則，以及支
持向量機。 
關鍵詞：調適性原型學習，群聚型態的原型，一致性，個例型態的原型，模式分類 
 
ABSTRACT 
We develop a number of adaptive prototype learning (APL) algorithms. They employ the 
same algorithmic scheme to determine the number and location of prototypes, but differ in the 
use of samples or the weighted averages of samples as prototypes, and also in the assumption of 
distance measures. To understand these algorithms from a theoretical viewpoint, we address their 
convergence properties, as well as their consistency under certain conditions. We also present a 
soft version of APL, in which a non-zero training error is allowed in order to enhance the gener-
alization power of the resultant classifier. Applying the proposed algorithms to twelve UCI 
benchmark data sets, we demonstrate that they outperform many instance-based learning algo-
rithms, the k-nearest neighbor rule, and support vector machines in terms of average test accu-
racy. 
Keywords: adaptive prototype learning, cluster-based prototypes, consistency, in-
stance-based prototype, pattern classification 
 4
point is discussed further in Section 7.2), yet it achieves comparable accuracy to the latter algo-
rithms. 
Another method for finding prototypes can be categorized as cluster-based learning (CBL) 
algorithms, in which prototypes are not samples per se, but can be derived as the weighted aver-
ages of samples. The k-means clustering algorithm (Lloyd, 1982; Max, 1960; Linde et al. 1980), 
the fuzzy c-means algorithm (Bezdek, 1981; Höppner et al., 1999), and the learning vector quan-
tization algorithm (Kohonen, 1988, 1990) are examples of this method. Instead of representing 
prototypes as the weighted averages of samples, they can be represented as centroids of clusters 
(Devi & Murty, 2002), or as hyperrectangles (high-dimensional rectangles) (Salzberg, 1991). In 
the latter case, the distance between a sample and a hyperrectangle not containing the sample is 
defined as the Euclidean distance between the sample and the nearest face of the hyperrectangle. 
In their guidelines for the design of prototype learning algorithms, Devroye et al. (1996, 
Chapter 19) propose some sufficient conditions for the consistency of this kind of algorithm. The 
conditions stipulate that: (a) the algorithm should minimize the empirical error, which is the error 
in classifying training samples; and (b) the number of prototypes should grow as a lower order of 
the number of training samples. 
Support vector machines (SVM) can also be used for pattern classification. In this approach, 
objects are classified by maximizing the margins between samples with different labels, where 
the margin is defined as the gap between two parallel hyperplanes (Figure 1a). The consistency of 
SVM is assured if the samples are bounded and the margin between samples with different labels 
holds (Vapnik, 1995; Schölkopf et al. 1999; Cristianini & Shawe-Taylor, 2000). 
References 
P. Bartlett and J. Shaw-Taylor. Generalization performance of support vector machines and other pattern 
classifiers. In B. Schölkopf, C. Burges, and A. J. Smola, editors, Advances in Kernel Methods: Support 
Vector Learning. MIT Press, 1999. 
J. C. Bezdek, Pattern Recognition with Fuzzy Objective Function Algorithms. Plenum, New York, 1981. 
L. Bottou, C. Cortes, J. Denker, H. Drucker, I. Guyon, L. Jackel, Y. LeCun, U. Muller, E. Sackinger, P. 
Simard, and V. Vapnik. Comparison of classifier methods: A case study in handwriting digit recogni-
tion. In Proc. Int. Conf. Pattern Recognition, pages 77–87, 1994. 
H. Brighton and C. Mellish. Advances in instance selection for instance-based learning algorithms. In 
Data Mining and Knowledge Discovery, 6:153–172, 2002. 
C. Cortes and V. Vapnik. Support vector machines. In Machine Learning, 20:1-25, 1995. 
T. Cover and P. Hart. Nearest neighbor pattern classification. In IEEE Trans. Information Theory, 
13:21-27, 1967. 
N. Cristianini and J. Shawe-Taylor. An introduction to Support Vector Machines and other kernel-based 
learning methods. Cambridge University Press, 2000. 
F. Devi and M. Murty. An incremental prototype set building technique. In Pattern Recognition, 
35:505–513, 2002. 
 6
J. C. Platt, N. Cristianini, and J. Shawe-Taylor. Large margin DAG’s for multiclass classification. In Ad-
vances in Neural Information Processing Systems, 12:547-553, MIT Press, Cambridge, 2000. 
S. Salzberg. A nearest hyperrectangle learning method. In Machine Learning, 6:251-276, 1991. 
B. Schölkopf, C. J. C. Burges, and A. J. Smola. In Advances in Kernel Methods — Support Vector Learn-
ing. MIT Press, Cambridge, MA, 1999. 
C. Stone. Consistent nonparametric regression. In Annals of Statistics, 5:595-645, 1977. 
V. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of events to their 
probabilities. In Theory of Probability and Its Applications, 16:264-280, 1971. 
V. Vapnik and A. Chervonenkis. Ordered risk minimization I. In Automation and Remote Control, 
35:1226-1235, 1974. 
V. Vapnik and A. Chervonenkis. Ordered risk minimization II. In Automation and Remote Control, 
35:1403-1412, 1974. 
V. Vapnik. The Nature of Statistical Learning Theory. Springer Verlag, New York, 1995. 
D. R. Wilson and T. R. Martinez. Reduction techniques with instance-based learning algorithms. In Ma-
chine Learning, 38:257–286, 2000. 
Z.-D. Wu, W.-X. Xie and J.-P. Yu. Fuzzy c-means clustering algorithm based on kernel method. In Fifth 
Intern. Conf. Computational Intelligence and Multimedia Applications, pages 1–6, 2003. 
L. Zhao. Exponential bounds of mean error for the nearest neighbor estimates of regression functions. In 
Journal of Multivariate Analysis, 21:168-178, 1987. 
研究方法 
We illustrate APL for a version of it, called GCNN, which uses samples as prototypes. We 
assume there are n training samples (x1, y1), …, (xn, yn) drawn independently from the set Rd×Λ 
according to the same distribution, where Λ = {1, 2,…, m} is a set of labels or class types. We 
now describe the steps of GCNN as follows. 
G1 Initiation: For each label y, select a y-sample as an initial y-prototype. 
G2 Absorption Check: Check whether each sample is absorbed. If all samples are absorbed, 
terminate the process; otherwise, proceed to the next step. 
G3 Prototype Augmentation: For each y, if any unabsorbed y-samples exist, select one as a 
new y-prototype; otherwise, no new prototype is added to label y. Return to G2 to pro-
ceed. 
In G1, a y-sample is selected as follows. We let each y-sample cast a vote to its nearest 
y-sample, and select the one that receives the highest number of votes. In G3, an unabsorbed 
y-sample is selected as follows. Let Ψy = {xi: l(xi)=y & xi is unabsorbed}. We let each member of 
Ψy cast a vote for the nearest member in this set. The selected y-sample is the member of Ψy that 
receives the highest number of votes. 
The other versions of APL are similar to GCNN, but there are some critical differences. First, 
cluster centers are used as prototypes, instead of samples. Second, the resultant prototypes must 
maintain a certain separation between them. If this condition is not satisfied, then a fixation proc-
