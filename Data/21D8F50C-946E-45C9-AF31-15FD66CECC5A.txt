 - 1 - 
行政院國家科學委員會補助專題研究計畫 □ 成 果 報 告   
□期中進度報告 
 
類神經網路演算法之改良(III) 
 
 
計畫類別：□個別型計畫   □整合型計畫 
計畫編號：NSC 100-2221-E-032-070- 
執行期間：100 年 8月 1日至 101 年 7 月 31 日 
 
執行機構及系所：淡江大學土木工程學系 
 
計畫主持人：葉怡成 
計畫參與人員：詹翔安 廖仁傑   
 
 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  □完整報告 
 
本計畫除繳交成果報告外，另須繳交以下出國心得報告： 
□赴國外出差或研習心得報告 
□赴大陸地區出差或研習心得報告 
□出席國際學術會議心得報告 
□國際合作研究計畫國外研究報告 
 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
            □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
 
中   華   民   國 101 年 8月 10日 
 - 3 - 
壹、 前言 
 
徑向基底函數網路(RBFN)的核有形心與半徑二種參數，這二種參數可用監督式或
無監督式學習來決定，常用於分類問題。RBFN 採用 Gaussian 函數(圖 1 與圖 2)做為隱
藏單元的轉換函數： 
)exp( kk netH   (1) 
2
2)(
k
i
iki
k
CX
net

 

 (2) 
其中 kH 為隱藏層的第 k 個 Gaussian 單元的輸出值； knet 為隱藏層的第 k 個 Gaussian
單元的淨值； ikC 為第 k 個 Gaussian 單元的核的第 i 個輸入變數的中心值； k 為第 k 個
Gaussian 單元的核的半徑。 
 
並以線性迴歸方式連接隱藏單元以及輸出單元。RBFN 的困難在於決定隱藏單元的
數目以及 Gaussian 函數的形心以及半徑參數(Carse et. al. 1995)。其中形心與半徑這二種
參數可用監督式或無監督式學習來決定。RBFN 的監督式學習與 BPN 相似，可用最小
化誤差平方和與最陡坡降法導出學習規則；而無監督式學習是採用 k-means 演算法來找
出樣本的聚類中心做為每個隱藏單元的高斯函數的形心，然後利用最近鄰居法則決定其
半徑的適當大小。 
 
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
-3 -2 -1  0  1  2  3
ex
p(
-x
*x
)
x  
 
 
 
RBFN 在模式鑑別、時間序列預測以及故障診斷等領域都有廣泛的應用(Whitehead 
and Choate 1996)，其演算法也有許多不同的變形 (Xu 1993; Berthold 1994; Carse et. al. 
1995; Whitehead and Choate 1996; Cheng 1997; Bugmann 1998; Webb and Shannon 1998; 
Shibata and Ito 1999; Gomm and Dimg 2000; Han and Xi 2002; Gao and Yang 2002; Park 
and Sandberg 1991; Park and Sandberg 1993; Park et. al. 2004)。但它們有一個缺點是視所
X1       X2 …………………………..Xm 
    
 
圖 2 Gaussian 函數圖形 圖 1 徑向基底函數網路(RBFN) 
 - 5 - 
貳、 理論推導 
 
一、前向傳播 (回想階段) 
(一) 計算隱藏層輸出向量 ,...},,{ 321 HHH  
)exp()( kkk netnetfh                      (3) 
2
22 )(
k
i
ikiik
k
CxV
net

 
 Nk ,...,2,1        (4) 
其中 ix 為第 i 個輸入值； kh 為隱藏層的第 k 個 Gaussian 單元的輸出值； knet 為隱藏層
的第 k 個 Gaussian 單元的淨值； ikC 為第 k 個 Gaussian 單元的核之中心的第 i 個輸入變
數值； ikV 為第 k 個 Gaussian 單元的核的第 i 個輸入變數的加權值，代表輸入變數的重
要程度； k 為第 k 個 Gaussian 單元的核的半徑。 
 
在網路訓練過程中， k 有可能被調整至 0，造成分母為 0 的困擾，為了避免此一
困擾，(4)式改成下式 
 
i
ikiikkk CxVQnet
222 )( Nk ,...,2,1      (5) 
其中 kQ 為第 k 個 Gaussian 單元的核的半徑的倒數。 
 
(二) 計算推論輸出向量 ,...},,{ 321 yyy  
)exp(1
1
)(
j
jj net
netfy

                 (6) 
j
k
kkjj hWnet                       (7) 
其中 jnet 為輸出層的第 j 個單元的淨值； jy 為第 j 個輸出單元的輸出值； kjW 為隱藏層
的第 k 單元與輸出層第 j 個單元間的連結強度； j 為輸出層第 j 個單元的門限值。 
 
二、後向傳播 (學習階段) 
(一) 學習演算法 
因為監督式學習旨在降低網路輸出單元目標輸出值與推論輸出值之差距，所以一般
以下列誤差函數(或稱能量函數)表示學習的品質： 
   2
2
1
jj ytE                 (8) 
其中 jt 為訓練範例輸出層第 j 個輸出單元的目標輸出值； jy 為訓練範例輸出層第 j 個輸
出單元的推論輸出值。 
 - 7 - 
Gaussian 單元的核的中心值修正量可用偏微分的連鎖率得到： 
ik
k
kik
ik C
net
net
E
C
E
C








               (17) 
其中 
'
k
j k
j
jk
k
kk
f
h
net
net
E
net
h
h
E
net
E






















                              (18) 
因(14)定義 
j
j net
E


  
故
 
''
k
j
kjjk
j
kjj
k
fWfW
net
E





 




 


 
        (19) 
定義 
'
k
j
kjjk fW 





                       (20) 
則 
k
knet
E



              (21) 
))(2()( 22222 ikiikk
i
ikiikk
ikik
k CxVQCxVQ
CC
net





 





         (22) 
 
將(21)(22)代入(17)得
 
)(2))(2()( 2222 ikiikkkikiikkkik CxVQCxVQC            (23)
 
 
同理，Gaussian 單元的核的半徑之倒數修正量可用偏微分的連鎖率得到： 
k
k
kk
k Q
net
net
E
Q
E
Q







             (24) 
其中 
 




 





i
ikiikk
i
ikiikk
kk
k CxVQCxVQ
QQ
net 22222 )(2)(        (25)
 
將(21)(25)代入(24)得
 
 
i
ikiikkk
i
ikiikkkk CxVQCxVQQ
2222 )(2)(2)(          (26)
 
 
 - 9 - 
參、 示範例題 
 
本節以五個人為設計的題目展示 ARBFN 的特性，並與傳統的 RBFN 及多層感知器 
(Multi-Layered Perceptron, MLP)比較準確性。為了檢驗學習的效果，取五分之四的資料
做訓練範例，其餘做測試範例，並以其預測結果來評估各方法的準確性。 
在使用神經網路建模時，最重要的參數為學習速率、網路結構與學習循環。網路架
構是指隱藏單元的數目(一般為二個到數十個)，網路結構太小可能無法建立足夠複雜的
模型來配適資料，但太大也可能產生過度配適現象。學習循環是指將資料集反覆學習的
次數，其值太小可能無法建立足夠複雜的模型來配適資料，但太大也可能產生過度配適
現象。本研究的網路的參數設定原則如下：(1) 學習速率：採用在每一個訓練循環完畢
即將學習速率乘以一個小於 1.0 的係數的方式，逐漸縮小學習速率，但不小於預設的學
習速率下限值。本研究學習速率初始值=0.1，折減係數=0.95，下限值=0.01。(2) 隱藏
單元數目：考慮 2, 4, 8, 16, 32 個，並以嘗試錯誤方式決定每一個網路的最佳隱藏層單元
數目。(3) 學習循環設為 30000 次，然後取誤差均方根最小的學習循環做為採用的學習
循環。 
 
例題 1. 二維四球分類問題 
為探討前節的 Gaussian 核的中心、半徑、形狀參數學習規則是否真能正確找出應
有的值，本研究設計了一個「二維四球分類問題」，其分類公式如下： 
當     4.05.05.0 22  yx  或     4.05.05.0 22  yx   或  
    4.05.05.0 22  yx  或     4.05.05.0 22  yx   
則屬第一類，否則為第二類。                (35) 
    在值域 11  x , 11  y 的範圍內，以隨機取點方式取得 500 筆資料。此問題的範例分佈狀
況如圖 3 所示，其中屬第一類的範例形成各自以(-0.5, -0.5)，(-0.5, 0.5)，(0.5, -0.5)，(0.5, 0.5)為中心點，
半徑為 0.4 的四個完全封閉的圓形。屬於第二類的範例則是分佈於四個圓形以外的地方。 
 
例題 2. 二維四球分類問題：不同中心 
為探討前節的 Gaussian 核的「中心參數」學習規則是否真能正確找出應有的值，
在此設計了一個與例題 1 相似的題目，但其中心位置不同，其分類公式如下： 
當     34.05.00.0 22  yx  或     34.00.05.0 22  yx   或  
    34.00.05.0 22  yx  或     34.05.00.0 22  yx   
則屬第一類，否則為第二類。                (36) 
此問題的範例分佈狀況如圖 4 所示。 
 - 11 - 
-1 -0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
x
y
 
圖 3 例題 1.基準例題應有的分類邊界 
-1 -0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
-1
-0.7
-0.4
-0.1
0.2
0.5
0.8
x
y
 
圖 4 例題 2.不同中心應有的分類邊界 
-1 -0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
-1
-0.7
-0.4
-0.1
0.2
0.5
0.8
x
y
 
圖 5 例題 3.不同半徑應有的分類邊界 
-1 -0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
-1
-0.7
-0.4
-0.1
0.2
0.5
0.8
x
y
 
圖 6 例題 4.不同形狀應有的分類邊界 
-1 -0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
-1
-0.7
-0.4
-0.1
0.2
0.5
0.8
x
y
 
圖 7 例題 1 基準例題學習的分類邊界 
(形狀參數可以自適應調整) 
-1 -0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
-1
-0.7
-0.4
-0.1
0.2
0.5
0.8
x
y
 
圖 8 例題 2 不同中心學習的分類邊界 
 (形狀參數可以自適應調整) 
 - 13 - 
形狀參數值的相對大小。 
為了進一步證明本文提出的加入形狀參數的演算法優於沒有形狀參數的演算法，在
此將原演算法中的形狀參數值在學習過程中全部固定為 1.0，其模型的分類邊界如圖
11~圖 14 所示。因為例題 1 至例題 3 的分類邊界為圓形，故加入形狀參數的演算法
(ARBFN)應該與沒有加入者(RBFN)無太大區別；但在例題 4(不同形狀例題)這題上，
ARBFN 應該要遠優於 RBFN 才合理。仔細比較圖 3~圖 6、圖 7~圖 10、圖 11~圖 14 這
三組圖形可以發現： 
(1) RBFN 與 ARBFN 在例題 1 與 2 產生的邊界並無明顯的優劣之分，這與預期相符。 
(2) RBFN 在例題 3 產生的邊界中，右下方應有圓形未出現(圖 13)；而 ARBFN 可以產
生應有的四個圓形，且形狀亦較正確(圖 9)。這雖與預期不相符，但也顯示 ARBFN
優於傳統的 RBFN，對於具有不同半徑的分類問題有更好的分類能力。 
(3) RBFN 在例題 4 產生的邊界中，其右上方的橢圓橫向寬度大於縱向寬度，與應有的
形狀相反(圖 14)；而 ARBFN 的該橢圓橫向寬度小於縱向寬度，與應有的形狀相同(圖
10)。這顯示 ARBFN 的「形狀參數」確實發揮了效果，對於具有不同形狀的分類問
題有更好的分類能力。 
 
 
-1 -0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
-1
-0.7
-0.4
-0.1
0.2
0.5
0.8
x
y
 
圖 11 例題 1 基準例題學習的分類邊界
(形狀參數固定為 1.0) 
-1 -0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
-1
-0.7
-0.4
-0.1
0.2
0.5
0.8
x
y
 
圖 12 例題 2 不同中心學習的分類邊界
(形狀參數固定為 1.0) 
 - 15 - 
肆、 數值例題 
 
為了有系統的測試ARBFN網路之效能，並與MLP比較，以下以二個數值例題為例。 
 
 
例題 1：二維sin型曲面函數 
 
y x )sin(2 x ) +11
2 5 2sin(      其中 -0.8 < x1 < 0.7，-0.8 < x2 < 0.7 
 
這是一個相當複雜的函數 (Chen and Chang 1996)，圖16(a)為函數的理論三維曲面圖與
二維等高線圖。圖16(b)與圖16(c)為MLP與ARBFN預測三維曲面圖與二維等高線圖(學
習循環為10000次)。圖16(d)與圖16(e)為其誤差均方根收斂圖，可見ARBFN的收斂速度
遠高於MLP。不同方法的結果如表2所示，可見ARBFN是最佳方法，它比MLP及傳統
的RBFN還要準確許多。 
 
-0
.8
-0
.6
12
5
-0
.4
25
-0
.2
37
5
-0
.0
5
0.
13
75
0.
32
5
0.
51
25 0.
7 -0.8
-0.275
0.25
-4
-2
0
2
4
6
y
x1
x2
     
-0
.8
-0
.6
5
-0
.5
-0
.3
5
-0
.2
-0
.0
5
0.
1
0.
25 0.
4
0.
55 0.
7 -0.8
-0.65
-0.5
-0.35
-0.2
-0.05
0.1
0.25
0.4
0.55
0.7
-4206
x1
x2
 
(a) 函數的理論三維曲面圖與二維等高線圖 
 - 17 - 
表 2 數值例題一結果 
誤差均方根 
個案名稱 
網路 
架構 
學習 
循環 訓練範例 測試範例 
MLP 2-20-1 10000 0.2457 0.2723 
MLP 2-20-20-1 10000 0.3524 0.3993 
RBFN 2-20-1 10000 0.1584 0.1885 
ARBFN 2-20-1 10000 0.0915 0.1121 
 
例題 8：二維雙螺旋分類 
二維雙螺旋分類是一個很古典的難題 (Lang and Witbrock 1988)，它是一個在二維平面
上由二個互相纏繞成三圈螺旋形分佈的二類樣本構成的分類問題 (圖17(a))，因此具
有高度的非線性與高度的自變數 (直角座標) 交互作用。這個問題一般需上萬個學習
循環才能建立能完全區分二個分類的網路。 
圖17(b)~(i)分別為2-50-2的ARBFN網路在學習循環3, 10, 30, 100, 300, 1000, 3000, 
10000次下的預測二維等高線圖。它似乎是由內而外逐漸形成正確的分類邊界。可知
RBFN的建模過程十分精巧，並非只是靠最近鄰居法的策略，而是循序漸進的調整其
參數，產生許多正向、反向鐘形曲面，疊加成所需的曲面。圖17(j)與圖17(k)為ARBFN
與MLP的誤判率收斂圖，可見ARBFN的收斂速度遠高於MLP，且過程較穩定。MLP
需用二層隱藏層才能建構精確的二維雙螺旋分類模型，而ARBFN只有一層隱藏層就
能建構幾乎一樣精確的模型，可見ARBFN是一個很好的分類方法。不同方法的結果
如表3所示，可見ARBFN是最佳方法，它比MLP及傳統的RBFN還要準確許多。 
 
 
表 3 數值例題二結果 
誤判率 誤差均方根 
個案名稱 
網路 
架構 
學習 
循環 訓練範例 測試範例 訓練範例 測試範例 
MLP 2-20-2 30000 0.34375 0.33854 0.25495 0.25508 
MLP 2-20-20-2 30000 0.00000 0.00000 0.01769 0.02445 
RBFN 2-50-2 30000     
ARBFN 2-50-2 30000 0.00000 0.00000 0.03950 0.04726 
 
 - 19 - 
-1
.5
-1
.1
25
-0
.7
5
-0
.3
75 0
0.
37
5
0.
75
1.
12
5
1.
5 -1.5
-1.2
-0.9
-0.6
-0.3
0
0.3
0.6
0.9
1.2
1.5
-0.
00.511
x2 1-1.5
0.5-1
0-0.5
-1-0
 
(f) 學習循環 = 300 次 
-1
.5
-1
.1
25
-0
.7
5
-0
.3
75 0
0.
37
5
0.
75
1.
12
5
1.
5 -1.5
-1.2
-0.9
-0.6
-0.3
0
0.3
0.6
0.9
1.2
1.5
-0.
00.511
x2 1-1.5
0.5-1
0-0.5
-1-0
 
(g) 學習循環 = 1000 次 
-1
.5
-1
.1
25
-0
.7
5
-0
.3
75 0
0.
37
5
0.
75
1.
12
5
1.
5 -1.5
-1.2
-0.9
-0.6
-0.3
0
0.3
0.6
0.9
1.2
1.5
-0.
00.511
x2 1-1.5
0.5-1
0-0.5
-1-0
 
(h) 學習循環 = 3000 次 
-1
.5
-1
.1
25
-0
.7
5
-0
.3
75 0
0.
37
5
0.
75
1.
12
5
1.
5 -1.5
-1.2
-0.9
-0.6
-0.3
0
0.3
0.6
0.9
1.2
1.5
-0.
00.511
x2 1-1.5
0.5-1
0-0.5
-1-0
 
(i) 學習循環 = 10000 次 
0
0.1
0.2
0.3
0.4
0.5
0.6
0 10000 20000 30000
學習循環
誤
判
率
訓練誤判率
測試誤判率
 
(e) 2-20-20-2 MLP 的收斂曲線 
0
0.1
0.2
0.3
0.4
0.5
0.6
0 2000 4000 6000 8000 10000
學習循環
誤
判
率
訓練誤判率
測試誤判率
 
(e) 2-20-20-2 MLP 的收斂曲線 
圖 17 二維雙螺旋分類 
 
 
伍、 實際例題 
本節以四個 UCI 資料庫(Asuncion and Newman 2007)的實際應用的資料集進行實驗，以
比較 ARBFN 相對於傳統 RBFN 與 MLP 的效能，包括(1)垃圾郵件偵測 (2)遙測影像識
別 (3)森林樹種植生分類 (Blackard 1998) (4)混凝土強度預測(Yeh 1998)。為了檢驗學習
的效果，採用五段式交叉驗證(5-fold cross-validation)，即將範例隨機分成五個部份，每
 - 21 - 
Carse, B., Pipe, A.G., Fogarty, T.C. and Hill, T. “Evolving Radial Basis Function Neural 
Networks Using a Genetic Algorithm,” IEEE International Conference on Evolutionary 
Computation, Perth, WA, Australia, 1995, pp. 300-305. 
Chen, Chyi-Tsong and Chang, Wei-Der (1996) “A feedforward neural network with function 
shape automating,” Neural Networks, (9:4), 1996, pp. 627-641. 
Cheng, Y.M. “Adaptive Rival Penalized Competitive Learning and Combined Linear 
Predictor Model for Financial Forecast and Investment,” International Journal of 
Neural systems (8:5), 1997, pp. 517 -534. 
Gao, D. and Yang, G. “Adaptive RBF Neural Networks for Pattern 
Classifications,” Proceedings of the 2002 International Joint Conference on Neural 
Networks, 2002, pp. 846-851. 
Gomm, J.B. and Dimg, L.Y. “Selecting Radial Basis Function Network Centers with 
Recursive Orthogonal Least Squares Training,” IEEE Transactions Neural Networks 
(11:2), 2000, pp. 306-314. 
Han, M. and Xi, J. “Radial Basis Perception Network and Its Applications for Pattern 
Recognition,” International Joint Conference on Neural Networks, Honolulu, HI, 2002, 
pp. 669-674.  
Moody, J. and Darken, C. “Fast Learning in Networks of Locally-Tuned Processing Units,” 
Neural Computation (1:2), 1989, pp. 281-294.  
Park, J. and Sandberg, I. W. “Universal approximation using radial-basis-function networks,” 
Neural Computation, (3:2), 1991, pp. 246-257. 
Park, J. and Sandberg, I. W. “Approximation and radial-basis-function networks,” Neural 
Computation, (5:3), 1993, pp. 305-316. 
Park, J., Harley, R.G., and Venayagamoorthy, G.K. “Indirect adaptive control for 
synchronous Generator: comparison of MLP/RBF neural networks approach with 
Lyapunov stability analysis,” IEEE Transactions on Neural Networks (15:2), 2004, pp. 
460- 464. 
Shibata, K. and Ito, K. “Gauss-Sigmoid Neural Network,” International Joint Conference on 
Neural Networks, Washington, DC, 1999, pp. 1203-1208. 
Asuncion, A. and Newman, D. J. UCI Machine Learning Repository 
[http://www.ics.uci.edu/~mlearn/MLRepository.html]. Irvine, CA: University of 
California, School of Information and Computer Science, 2007. 
Webb, A.R. and Shannon, S. “Shape-Adaptive Radial Basis Functions,” IEEE Transactions 
on Neural Networks (9:6), 1998, pp. 1155-1166. 
Whitehead, B. A. and Choate, T. D. “Cooperative-Competitive Genetic Evolution of Radial 
Basis Function Centers and Widths for Time Series Prediction,” IEEE Transactions on 
Neural Networks (7:4), 1996, pp. 869-880.  
Xu, L. “Rival Penalized Competitive Learning for Clustering Analysis RBF Net, and Curve 
Detection,” IEEE Transactions on Neural Networks (4:4), 1993, pp. 636 -648. 
 - 23 - 
計畫成果自評 
研究計畫為「自適應 RBFN 神經網路」(Adaptive Radial Basis Function Network, 
ARBFN)。徑向基底函數網路(RBFN)的核有形心與半徑二種參數，這二種參數可用監
督式或無監督式學習來決定，常用於分類問題。RBFN 常視所有自變數有同等地位，
故分類邊界是圓形，但真實的問題精常是每一個自變數對分類的影響力不同，分類邊
界是應該是橢圓形。為克服此一缺點，本文提出具自適應核形狀參數的徑向基底函數
網路，並以監督式學習推導出其學習規則。為證明此一架構優於傳統的徑向基底函數
網路，本研究以多個人為的與真實的分類例題進行比較。結果顯示，此一架構確實比
倒傳遞網路及傳統的徑向基底函數網路更為準確，狀參數值的大小確實能表現出自變
數對分類的影響力高低。 
因此研究內容與原計畫完全相符程度。 
本研究的成果已寫成英文論文並投稿，目前正在審稿中。另外，這一年內作者發
表了相關著作期刊論文： 
1. I-Cheng Yeh and Kuan-Cheng Lin (2011). “Supervised learning probabilistic neural 
networks,” Neural Processing Letters, Vol. 34, No. 2, 193-208. (SCI) 
2. Yeh, I., Chen, C., Zhang, X., Wu, C., & Huang, K. (2012). Adaptive radial basis 
function networks with kernel shape parameters. Neural Computing Applications, Vol. 
21, No. 3, 469-480. (SCI) 
以及一篇國際研討會論文 
3. Yeh, I-Cheng, Tseng, Pei-Yen, Huang, Kuan-Chieh, and Yau-Hwang, Kuo (2012). 
“Minimum Risk Neural Networks and Weight Decay Technique,” Emerging Intelligent 
Computing Technology and Applications, Communications in Computer and 
Information Science, Volume 304, 2012, pp 10-16.  (8th International Conference, 
ICIC 2012, Huangshan, China, July 2012). (EI) 
因此已達成預期目標。 
本研究與其它文獻不同之處與創新的貢獻簡述如下： 
1. 統一的數學理論架構 
傳統的RBFN經常以無監督式學習規則來決定核函數的形心、半徑參數，以監督式
學習規則來調整隱藏單元到輸出單元之間的連結加權值、輸出單元之門限值，這種二階
段的方法可能無法建構最佳的模型。本研究雖然在可調參數上比傳統的RBFN還多了形
狀參數，但採用誤差平方和最小化原理統一導出了所有參數的監督式學習規則，包括核
函數的形心、半徑、形狀參數、以及隱藏單元到輸出單元之間的連結加權值、輸出單元
 1 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                   日期： 101  年 8 月 8 日 
一、參加會議經過 
7/26 上午聽三場主辦單位的演講，其中 Shun-Ichi Amari 提到稀疏資料的分析。 
7/26 下午報告發表論文 Minimum Risk Neural Networks and Weight Decay Technique. 
提問時間 Shun-Ichi Amari 教授建議 Weight Decay Technique 部份可考慮採用 weihght
絕對值總合，而非 weihght 平方值總合，做為懲罰函數。 
二、與會心得 
感覺上 Bio-inspired Computing 與 Feature Extraction Methods 仍然是熱門研究主題。 
三、考察參觀活動(無是項活動者略) 
無。 
四、建議 
計畫編號 NSC 100-2221-E-032-070- 
計畫名稱 類神經網路演算法之改良(III) 
出國人員
姓名 葉怡成 
服務機構
及職稱 淡江大學  土木工程學系 
會議時間 
2012 年 7 月 25 日
至 
2012 年 7 月 29 日 
會議地點 
中國安徽省黃山市 
會議名稱 
(中文)2012 智能計算國際會議 
(英文)2012 International Conference on Intelligent Computing 
發表論文
題目 
(中文)最小風險神經網絡和權重衰減技術 
(英文) Minimum Risk Neural Networks and Weight Decay Technique 
附件四 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/08/12
國科會補助計畫
計畫名稱: 類神經網路演算法之改良(III)
計畫主持人: 葉怡成
計畫編號: 100-2221-E-032-070- 學門領域: 人工智慧
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
