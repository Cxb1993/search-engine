 1 
行政院國家科學委員會專題研究計畫成果報告 
 
計畫編號：NSC 99-2221-E-194-034 
執行期限：99年 8月 1日至 100年 7月 31日 
 
主持人：劉興民 國立中正大學資訊工程學系 damon@computer.org 
共同主持人： 
計畫參與人員：鄭靜怡、鍾承軒、楊鈞豪、鄭廷威、謝侑錚、施沛宏、陳信融 
 
 
一、中文摘要 
 
擴增實境是一種能夠將虛擬畫面即時
地覆蓋於真實場景的技術，非常適合提供
使用者豐富的互動經驗。本研究的目標是
跨平台、整合桌上型與手機裝置擴增實境
之互動環境，讓桌上型與手機裝置使用者
們可以透過彼此不同的輸入裝置，在相同
的場景下來進行互動；並且加入物理模擬
讓互動更趨近真實。在此系統架構中桌上
型擴增實境環境下，提供直接用手對虛擬
物件來進行選取、移動等等操作的功能，
讓使用者能夠以更直覺的方式與虛擬物件
間做互動。 
為了使系統能夠完善地執行於兩種裝
置平台上，我們將以手機平台為開發基
準，並使用 Server/Client 的架構連接兩種
裝置平台；其中 Server 將維護與管理多個
使用者共享的虛擬場景。在擴增實境應用
程式開發上，我們採用以樣板為基礎的追
蹤技術，對影像中樣板來進行追蹤，藉此
獲得相機視角與樣板之間相對應之空間轉
換關係，並且利用 Scene Graph結構建立虛
擬場景，最後透過電腦繪圖應用程式介面
將結合虛擬與真實之畫面繪製於螢幕上。
在桌上型裝置環境下，加入手勢追蹤配合
遮蔽式互動技術，幫助估算平面影像的手
勢深度，達到在三維空間下自然手勢互動
之目的。最後透過一個多人擴增實境互動
遊戲來展現我們系統之應用。 
 
關鍵詞：擴增實境、實體使用者介面、行
動裝置擴增實境、手勢互動、遮蔽式互動
技術、整合型系統、物理模擬 
 
Abstract 
Augmented Reality (AR) applications 
can provide users with enhanced interaction 
experiences by allowing computer generated 
virtual imagery to overlay physical objects in 
real time. The purpose of this research is to 
integrate desktop and handheld AR 
environment, in which desktop and mobile 
users can collaborate with each other in a 
shared scene, enabling a physically realistic 
experience in the interaction. Users can 
intuitively pick up and move virtual objects 
using their hands in desktop AR 
environment. 
The development of our system is based 
on handheld platform in order to work 
between desktop and mobile devices. Our 
system uses Server/Client architecture to 
connect different devices, and the 
 3 
in real world during the course of interaction 
between users and virtual objects. 
In this work, we aim to present a 
cross-platform, physics-based, collaborative 
system for AR. Particularly in desktop AR 
environment, we design an intuitive and 
convenient bare-hand interface that can be 
utilized by users to easily interact with virtual 
objects. 
  
四、文獻探討 
 
To the best of our knowledge, there are 
few existing works that integrate personal 
computers and mobile devices into a 
cross-platform interactive AR environment. 
In this section, we discuss related projects 
using hand interaction with virtual objects as 
well as those research works for mobile AR 
applications. 
Buchmann et al. [3] present a technique 
for fingertip-based interface, called 
FingARtips, which allows for free hand 
interaction with virtual objects. It can be 
manipulated by natural gestures such as 
grabbing, pressing, dragging and releasing. 
To track fingers, users must wear a glove 
with small markers attached to two fingertips 
and the hand. Although markers which stick 
on gloves help in more reliably detecting 
hands and fingers, users can not interact with 
virtual objects using their bare hands. 
Lee et al. [4] develop a computer 
vision-based 3D hand tracking system for 
multimodal AR interface. Users manipulate 
augmented objects by finger pointing and 
direct touching. Furthermore, it allows users 
to change the color or shape of virtual objects 
using speech input. To get the 3D 
information of users’ fingertip and hand 
center, they use stereo camera as a video 
input device which has two lenses on one 
camera body to provide stereo color image 
with depth map. However, this system does 
not provide physics simulation when users 
interact with virtual objects. 
Fernandes and Fernandez [5] present a 
system that tracks the 2D position of users’ 
hands on a tabletop surface, allowing users to 
move, rotate and resize the virtual objects 
over this surface. They use statistical models 
which can be obtained by analyzing a set of 
training images and then be used to detect the 
hands. Compared to other approaches, the 
system is less sensitive to changes in 
illumination. However, this system also does 
not provide physics simulation with virtual 
objects. 
Seichter et al. [6] describe a touch-based 
interaction technique for Tangible User 
Interfaces (TUIs) in AR applications. This 
technique allows for direct access and 
manipulation of virtual content on a 
registered tracking target. It provides 
multiple finger touch interaction in the 
vicinity of markers, thus supporting spatial 
interactions with 3D content on desktop and 
mobile devices. The multiple touch input 
streams can be mapped in various ways to 
output parameters. However, this system 
does not provide natural hand interaction 
with virtual objects in 3D space. 
Wagner et al. [7] present a system for 
multi-user interactive AR applications 
running on handheld devices. Their 
architecture is based on Studierstube [8] 
framework to accelerate collaborative 
applications. To evaluate the usability of the 
system, they design a multi-player game 
which is called Invisible Train. In it, users 
can operate the virtual train through PDAs to 
accomplish purpose of interactive 
cooperation and competition with each other. 
Henrysson et al. [9] utilize mobile 
phones to support face-to-face collaborative 
AR gaming. They port ARToolKit library to 
Symbian phone operating system and then 
develop a collaborative AR tennis game to 
demonstrate their system. Users can use their 
mobile phones which act as tennis rackets to 
play tennis game. Unlike Invisible Train [7], 
AR Tennis is focus on face-to-face 
interaction between two users, so it does not 
support complicated Scene Graph 
architecture and Internet unit to manage 
consistency of the AR scene.  
 
五、研究方法 
 
Here we aim to integrate personal 
computers and mobile devices into a 
cross-platform collaborative AR system. 
Construction of the system is 
handheld-device biased because its 
computing resource is far inferior to that of 
desktop computer. Once the observed 
 5 
drag the virtual objects into AR scene using 
the picking command. The virtual object is 
attached on hand position in AR scene until 
users utilize the cancelling command by 
making a fist. Although, we currently provide 
three commands for hand interaction, the 
finger finding algorithm [10] can detect all 
fingers image on the screen, so more gesture 
commands can be extended. 
 
六、結果與討論 
 
To demonstrate the use of our 
collaborative framework, we developed an 
AR game – JengAR (Figure 4), which refers 
to the well-known tower building game 
Jenga. Furthermore, due to that the physics 
engine has been integrated into our system, 
the behavior of virtual objects in JengAR is 
just like those blocks which can be 
manipulated in the real world. 
 
 
Fig. 4. AR game – JengAR in initial stage. 
 
Gesture commands are applied to our 
JengAR game for intuitive manipulations. 
Players need to touch any building block first 
using a pointing command. After that, the 
selection is confirmed; a blending effect is 
also used to hint players that which block has 
been selected. Then players can change to 
picking gesture to pull the building block out 
of the tower. The direction and speed of the 
dragging operation may affect the tower 
structures, according to physics law (Figure 
5). 
 
 
Fig. 5. Pulling virtual block out of the tower 
using hand. 
 
In a handheld AR environment, players 
can add blocks into the scene and stack them 
to build a tower. They can specify the 
position of blocks and move the selected 
object to another desired position by 
physically moving of the mobile phone. In 
addition to adding blocks for stacking, the 
players also can remove blocks from 
anywhere below the top layer and put them 
on the top of the tower (Figure 6). 
 
 
Fig. 6. JengAR in handheld device. 
 
七、計畫成果自評 
 
In this work, we present a vision-based 
hand interaction for AR applications with 
physics simulation and create a simple 
application to demonstrate the use of our 
approach. 
The significant contributions of the 
work are described as below: 
 Providing a real-time natural hand 
interaction: In order to enable users to 
directly interact with the virtual objects 
using their bare-hand in real-time, we 
propose a hand tracking method to track 
possible gesture bare-hand in each video 
frame. It is further divided into 
skin-color detection and fingertips 
finding. First, we utilize the skin-color 
segmentation method that can 
efficiently distinguish hand region 
through a skin-color classifier which is 
operated in YCbCr colorspace from the 
input frame. Subsequently, we exploit a 
fingertips finding algorithm to extract 
the possible fingertips for intuitively 
hand interaction. Due to that the 
algorithm can run in real-time, it enable 
users to naturally manipulate the virtual 
object in our interactive AR 
environment. Furthermore, to robustly 
tracking the fingertip, the shape of the 
 7 
九、參考文獻 
 
[1]  Milgram, P., and Kishino, F.A.: 
Taxonomy of mixed reality visual 
displays. Institute of Electronics, 
Information, and Communication 
Engineers (IEICE) Trans. on 
Information and Systems (special issue 
on networked reality), vol. E77-D, no. 
12, pp. 1321—1929 (1994) 
[2]  Azuma, R.T., Baillot, Y., Behringer, R., 
Feiner, S., Julier, S., and MacIntyre, B.: 
Recent advances in augmented reality. 
IEEE Computer Graphics and 
Applications, vol. 21, no. 6, pp.34—47 
(2001) 
[3]  Buchmann, V., Violich, S., Billinghurst, 
M., and Cockburn, A.: FingARtips: 
gesture based direct manipulation in 
augmented reality. In: 2nd international 
conference on Computer graphics and 
interactive techniques in Australasia and 
SouthEast Asia, pp. 212—221 (2004) 
[4]  Lee, M., Green, R., and Billinghurst, M.: 
3D natural hand interaction for AR 
applications. In: 23rd International 
Conference on Image and Vision 
Computing, New Zealand (2008) 
[5]  Fernandes, B., and Fernandes J.: Bare 
hand interaction in tabletop augmented 
reality. International Conference on 
Computer Graphics and Interactive 
Techniques, ACM Special Interest 
Group on GRAPHics and Interactive 
Techniques (2009) 
[6]  Seichter, H., Grasset, R., Looser, J., and 
Billinghurst, M.: Multitouch interaction 
for tangible user interfaces. In: IEEE 8th 
International Symposium on Mixed and 
Augmented Reality, pp. 213—214 
(2009) 
[7]  Wagner, D., Pintaric, T., Ledermann, F., 
and Schmalstieg, D.: Towards massively 
multi-user augmented reality on 
handheld devices. In: 3rd International 
Conference on Pervasive Computing 
(2005) 
[8]  Schmalstieg, D., Fuhrmann, A., Hesina, 
G., Szalav´ari, Z., Encarna¸c˜ao, L.M., 
Gervautz, M., and Purgathofer, W.: The 
Studierstube augmented reality project. 
Teleoperators and Virtual Environments,  
vol. 11 (2002) 
[9]  Henrysson, A., Billinghurst, M., and 
Ollila, M.: Face to face collaborative 
AR on mobile phones. In: IEEE and 
ACM International Symposium on 
Mixed and Augmented Reality (2005) 
[10]  Lee, G.A., Billinghurst M., and Kim, 
G.J.: Occlusion based interaction 
methods for tangible augmented reality 
environments. Virtual Reality 
Continuum and its Applications in 
Industry, pp. 419—426 (2004) 
[11]  Yung, Chun-Hao. 2010. Designing a nature 
hand interface for physics-based augmented 
reality. Master Thesis. Computer Science 
Department, National Chung Cheng 
University. 
[12]  Chung, Cheng-Hsuan. Integrating mobile 
and desktop platforms for developing 
collaborative augmented reality. 2010. 
Master Thesis. Computer Science 
Department, National Chung Cheng 
University. 
   
 1 
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                   日期： 100 年 10月 17 日 
                                 
一、參加會議經過 
這個大規模的研討會是由大陸的虛擬實境委員會和杭州電子科技大學的計算機學院合
辦，它提供了相關學者專家在電腦繪圖和虛擬實境相關領域技術上有一個國際性交流的
機會。此次會議舉行的地點在風光明媚的杭州市西湖。 
 
議程期間大會安排了許多場次的 keynote speech，我也都儘可能的去聆聽學習。第一場為
美國 SIGGRAPH的 President講解有關電腦繪圖的教育方法；第二場為北京大學計算機
學院副院長講解 3D技術應用在龍門石窟；最後一場是一位加拿大國科會的學者，講解
3D模型的製作。每一場的演講都極具有啟發性。 
 
我的口頭報告場次被安排在第一天的下午。因為行程安排上的原因，只能在前一天的下
計畫編號 NSC  －  99  －2221  －  E  － 194   －034 
計畫名稱 
A cross-platform collaborative system for augmented reality applications 
with physics simulation 
〔基於物理模擬之整合桌上型與手機裝置互動技術的擴增實境系統〕 
出國人員
姓名 
劉興民 
服務機構
及職稱 
國立中正大學資訊工程學系副教授 
會議時間 
100年 5月 15日
至 
100年 5月 16日 
會議地點 
中國大陸杭州市 
會議名稱 
(中文) 
(英文) 2011 Workshop on Digital Media and Digital Content 
Management 
發表論文
題目 
(中文) 
(英文) A physics-based augmented reality Jenga stacking game 
 3 
題。相形之下，台灣同學們普遍被動與消極的態度，確實值得改進。 
 
三、考察參觀活動(無是項活動者略) 
 
四、建議 
國科會對於出席國際會議的補助，確實能夠鼓勵大家積極投稿和參與，和來自全球各地
的專家學者進行學術交流、聽取最新的研究成果和豐富的研究經驗。更可以構思將本身
的研究專長投射入不同的領域以取得更廣泛的應用，也可能因此建立起良好的學術合作
之路。 
 
五、攜回資料名稱及內容 
此行帶回了大會的光碟片，內容包涵了所有在會期間發表的論文。 
 
六、其他 
大會的網址是： 
http://computer.hdu.edu.cn/dmdcm/ 
A Physics-Based Augmented Reality Jenga Stacking Game 
 
Damon Shing-Min Liu, Chun-Hao Yung, and Cheng-Hsuan Chung 
Computer Science Department 
National Chung Cheng University 
Chiayi, Taiwan 
e-mail: {damon, ycha97m, cch97m}@cs.ccu.edu.tw 
 
 
Abstract—In this paper, we present a cross-platform 
environment in which handheld and desktop-computer users 
can collaborate with each other in a shared scene to 
accomplish physically realistic experiences during the course 
of interaction. In realizing the system, we: 1) exploit 
client/server architecture to connect user devices, where the 
server is responsible for maintaining and managing the 
virtual objects shared by users; 2) build the distributed scene 
graph structure according to X3D specification for 
presenting 3D computer graphics; 3) use mobile phones as 
tangible input object to provide an intuitive interaction 
interface; 4) design a natural hand interface which enables 
users to directly manipulate virtual object using their bare 
hands. To demonstrate the effectiveness of the system, we 
developed JengAR, a simulation of the well-known tower 
building game. 
Keywords-augmented reality; mobile phone; tangible 
interface; X3D; hand interaction; physics simulation 
I.  INTRODUCTION 
Augmented Reality (AR) seamlessly merges physical 
and virtual world through the accurate registration between 
image captured by a camera and the virtual objects. Over 
the past decade, there was an evolution in the types of AR 
interface being developed; moreover, interactive AR 
interfaces need to be as intuitive as possible to be accepted 
by users. Current design of such interfaces is considered 
deficient in two aspects. First, it does not fully exploit the 
use of bare-hand interaction to manipulate augmented 
objects. Bare-hand interfaces are more natural because 
hands are commonly used in real life [1]. It also provides a 
more natural and simple control type. Second, even though 
most AR designs allow users to interact with virtual 
objects, few actually simulate physical circumstances as in 
real world during the course of interaction between users 
and virtual objects. 
Furthermore, many collaborative AR applications were 
previously developed using desktop computers. However, 
owing to the technique and knowledge of AR have grown 
to maturity, the successful experience from several 
desktop-based AR research projects inspire researchers to 
port AR applications onto mobile platform as well. 
Nowadays mobile phones have full color displays, built-in 
cameras, fast processors, wireless communication, even 
dedicated 3D graphics chips, and are almost accessible for 
everyone. Through wireless communication, people can 
use their phones to access information or play 
entertainments, no matter where they are. On the other 
hand, the size of the mobile phones is well suited to be 
used as a tangible object, so the developers can naturally 
use them to design intuitive interactions. Therefore, the 
handheld devices have become an ideal platform for AR. 
Although a number of collaborative AR applications 
have been developed, few studies have integrated handheld 
and desktop AR environment into one where mobile and 
personal-computer users can collaborate with each other in 
a shared scene through PCs and mobile phone, 
respectively. In this paper, we aim to present a cross-
platform, physics-based, collaborative system for AR. 
Particularly in desktop AR environment, we design an 
intuitive and convenient bare-hand interface that can be 
utilized by users to easily interact with virtual objects. 
Besides, we developed JengAR, an AR game with physics 
simulation, to demonstrate our collaborative system and 
natural hand interface. 
The rest of the paper is organized as follows. Section II 
introduces development tools and technologies that are 
exploited in our system. Section III provides a hand 
interaction method which intuitively manipulates the 
virtual objects. Section IV describes architecture of our 
client/server-based collaborative system. Section V 
illustrates manipulation of our demonstrative JengAR 
game. Finally, we conclude the paper in Section VI. 
II. DEVELOPMENT TOOLS AND TECHNOLOGIES 
This section describes the tools and technologies used 
to develop our physics-based and marker-based AR system. 
A. Marker Tracking Library 
There are several computer vision tracking libraries, 
such as ARToolKit [2], ARTag [3], ARToolKitPlus [4], 
StudierStube Tracker [5], Simplified Spatial Target 
Tracker (SSTT) [6], and OSGART [7]. In our work, we 
exploit ARToolKitPlus which is a successor to ARToolKit 
pose tracking library for building the marker-based AR. 
Although ARToolKitPlus can calculate camera 
position and orientation relative to physical markers in real 
time, it does not read camera images or handle any 
rendering process. Nevertheless, we used Microsoft 
DirectShow [8] to open camera stream and developed a 
Frame Grabber filter for capturing incoming frames. 
Figure 1 shows the data flow of tracking and rendering in 
[19], which can rapidly and effectively distinguish skin 
region as segmented hand contour. It can be performed in 
several color spaces such as RGB, HSV or YCbCr [20]. We 
adopted YCbCr color space to perform the skin-color 
segmentation. Color in YCbCr is represented using 
luminance component (Y) and two chrominance 
components, blueness (Cb) and redness (Cr). The 
transformation simplicity and explicit separation of 
luminance and chrominance components make this color 
space suitable for skin color modeling. We also exploit an 
explicit skin classifier which was proposed by Garcia and 
Tziritas [21] for better skin-color segmentation. Figure 4 
shows the results of skin-color segmentation. 
 
Figure 4.  Results of skin detection. 
B. Fingertip Detection 
After accomplishing the skin segmentation, fingertips 
are then needed to be detected from the skin region for 
hand interaction. Our fingertip finding method is based on 
Hardenberg’s algorithm [22]. Their algorithm explores two 
properties of fingertips: 1) the center of the fingertips is 
surrounded by a circle of skin region; 2) along a square 
outside the inner circle, fingertips are surrounded by a 
short chain of filled skin pixels and a long chain of non-
skin pixels (see Figure 5). 
 
Figure 5.  A simple model of the fingertip. 
Given the fingertip candidates with relative pixel 
location (x, y) in frame image, we then need to obtain the 
number and interaction point of detected fingertips for 
hand interaction. First, we group all candidates by 
computing the distance between each candidate pair. If the 
distance between two candidates is less than a threshold, 
these two candidates are allocated to the same group; 
otherwise, they belong to different groups. Then we sort 
the list, which stores all fingertip candidates in it, to 
accomplish the grouping procedure. After grouping all 
fingertip candidates, we choose the center of pixel location 
from candidates as an interaction point in every group. 
Thus, the number and the interaction point of fingertips 
can be calculated. Figure 6 shows results of fingertip 
detection. 
 
Figure 6.  Results of fingertip detection. 
C. Finger Shape Filtering 
Hardenberg’s algorithm can reliably detect fingertip 
most of the times. However, it may produce false 
detections on the finger end that connects with one’s palm 
due to its similar shape. To eliminate these false 
detections, we referred to the finger shape detection 
method proposed by Song et al. [23]. As a fingertip always 
has a long chain of filled skin pixels connected to the 
palm, and the width of the chain of filled skin pixels 
abruptly changes at the connection from the finger to the 
palm. 
In our implementation, two steps are iterated to 
calculate the width of finger in each row: 1) we first 
choose a pixel location (x, y), which is the skin pixel on a 
chain along finger direction, as a searching pivot; 2) then 
we find out boundary points of the finger shape by shifting 
left and right from the searching pivot respectively. Thus, 
the width value in current row can be calculated for 
comparison process. 
After the width finding procedure, the width value in 
current row can be utilized to compare with the stored 
width value in previous row. As mentioned in Song’s 
algorithm, if there is no dramatically increase in length of 
finger, the fingertip is eliminated as a false detection. 
Through employing the finger shape filtering, we can 
effectively compensate some false fingertips detection. 
IV. PROPOSED COLLABORATIVE AR SYSTEM 
We design a component-based architecture framework 
which can accelerate the task of developing multi-user 
collaborative AR application where users can interact with 
each other in an augmented shared space through mobile 
phones and PCs respectively. To this end, we exploit 
client/server architecture to connect different devices, 
where server handles all communication. When server 
receives the modified requests, it will broadcast the update 
messages to all attached devices once it approves the 
modifications. In order to avoid losing update messages to 
Simulation Kernel is to perform collision detection, 
resolve collisions and constraints, and provide the updated 
world transform for all the objects. After each simulation, 
the Simulation Kernel determines whether the position 
and orientation of each object is changed or not and only 
transmits the update transform matrix of the moved 
objects to the buffer. 
B. Client 
Figure 9 illustrates the two threads and the connections 
between modules and Renderer in client. 
 
Figure 9.  Communication between modules in client. 
1) Rendering thread: Frame Grabber is initiated for 
acquiring a video stream from a camera. Afterward, 
Frame Grabber grabs a sample image which is then 
transmitted to Scene Renderer for rendering video 
background of display device and to ARTK+ Tracker for 
detection and pose estimation of 2D fiducial markers on 
each frame. As soon as the external device triggers the 
input event, the Event Handler will invoke the functions 
with parameters according to the event type, and pass the 
parameters to communication thread if the client is 
attached to server. For example, when a LeftKey (e.g., ←) 
event is triggered on the desktop platform, the 
InsertNewBox function will be invoked to add a box node 
to the scene graph. There is a queue buffer introduced 
between communication thread and rendering thread to 
match different processing speeds. This buffer is used to 
store update messages from server to change the shared 
scene. In addition, the scene graph is also updated 
according to the parameters stored in received update 
messages after the client has established a connection to 
server. The Scene Renderer creates the AR scene which is 
constructed using the tree of X3D nodes with the 
transformation matrix computed through the ARTK+ 
Tracker. Finally, this thread continuously runs through the 
infinite loop. 
 
2) Communication thread: In the client, the reception 
component and the transmission component are different 
from those in server. As soon as this thread initially starts, 
the Transmitter sends user information to the server 
immediately, and then waits for the parameters passed 
from the Event Handler. On the other hand, the main task 
of the Receiver is to receive the X3D file for initializing 
the shared scene by all clients and the update messages for 
keeping the shared scene synchronized. 
 
C. Client-Server Interaction 
In Section IV (A) and Section IV (B), we introduce the 
operation details of the server and the client. Here we 
describe the communication flow between the client and 
the server. After a client creates a workgroup, the server 
maintains and manages a shared scene for the workgroup 
and continues to wait for the client connection. At the 
beginning, the client program enters a render loop where 
the Renderer continuously manages all the rendering 
process and handles the augmented reality. When user tries 
to join the workgroup, the client connects to the server and 
the Renderer stops until the server accepts this connection. 
Then, the communication thread starts and the user 
information is sent to the server. The server converts a tree 
structure of X3D nodes that define the shared scene into an 
XML format file and sends it back to the client. Afterward, 
the Receiver notifies the Renderer that an X3D file is 
received. The Receiver then invokes the method on the 
Scene Manager to parse this X3D file for constructing a 
new scene graph and uses it to build the AR scene which is 
the same as the workgroup’s. 
V. JENGAR 
To demonstrate the use of our collaborative 
framework, we developed an AR game – JengAR (Figure 
10), which refers to the well-known tower building game 
Jenga. Furthermore, due to that the physics engine has 
been integrated into our system, the behavior of virtual 
objects in JengAR is just like those blocks which can be 
manipulated in the real world. 
 
Figure 10.  AR game – JengAR in initial stage. 
A. Desktop AR – Gesture Command 
We aim to provide a natural hand interface which can 
be intuitively utilized by users. Through the bare-hand 
interface, users can directly control the virtual object in 
AR scene without wearing other devices. We currently 
5) Insertion: We compute the point of intersection 
between a ray from the near clipping plane to the far 
clipping plane and the marker plane, and then add object 
into the scene according to the position of the point. 
 
6) Deletion: For removing virtual objects from the 
AR scene, we also read back the alpha value of the pixel 
under the crosshair’s center and then search through the 
list of objects looking for a matched alpha ID. 
 
7) Finger based user graphical interface: Currently, 
most smartphones have no physical keypad and joypad, 
instead, users use a touch-screen as an input device to 
quickly access the device's functions and applications. 
Therefore, we design a finger based graphical user 
interface (GUI) for collaborative AR applications. In 
addition, we make each menu item as virtual button to 
enable / disable specific functions, perform certain tasks, 
or interact with the virtual objects (Figure 15). Owing to 
that we place the GUI at the bottom of the screen (Figure 
16), users can use a simple touch of the thumb to select 
options from a hierarchical menu. 
 
Figure 15.  Menu items. 
 
Figure 16.  Graphical user interface. 
In a handheld AR environment, players can add blocks 
into the scene and stack them to build a tower. They can 
specify the position of blocks and move the selected 
object to another desired position by physically moving of 
the mobile phone. In addition to adding blocks for 
stacking, the players also can remove blocks from 
anywhere below the top layer and put them on the top of 
the tower (Figure 17). 
 
Figure 17.  JengAR in handheld device. 
VI. CONCLUSION 
In this paper, we present a cross-platform system 
which integrates desktop computers and mobile devices 
for developing collaborative augmented reality 
applications. Combining augmented reality, mobile 
computing, and computer supported collaborative work 
(CSCW) can bring realistic and mobile advantages over 
common virtual system. 
Moreover, we have designed a natural hand interface 
that enables users to directly touch virtual objects using 
their bare-hand in a desktop AR environment. Compared 
to traditional interfaces such as mouse and keyboard, our 
hand interface can be controlled in an intuitive way for 
AR, since people are accustomed to interact using their 
hands with objects in real life. 
We also developed a distributed scene graph structure 
according to X3D specification to maintain the AR scene 
shared by multiple users. In it, users are allowed to late 
join and save scene to a file for later collaboration 
sessions. The communication between each client is 
realized through client/server architecture, in which server 
handles all connections and each client needs not worry 
about the existence of other clients. Thus, the distributed 
features allow users to freely choose to utilize whether PCs 
or mobile devices to easily interact with each other. 
Finally, we implement a simple tower building AR 
game – JengAR to demonstrate our collaborative system 
and natural hand interface. Furthermore, we integrate a 
physics engine into our system for enriching more realistic 
behavior on virtual objects, so that when users pull a 
virtual block from the tower, they can see that the tower 
structures are affected by direction and speed of pulling 
operations according to physics law. 
 
REFERENCES 
[1] M. K. Lee and M. Billinghurst, “Space-based gesture classification 
in an augmented reality environment,” Proc. the International 
Workshop on Ubiquitous Virtual Reality (IWUVR 2009), pp. 36-
39, 2009. 
[2] ARToolKit – http://www.hitl.washington.edu/artoolkit/ 
[3] ARTag – http://www.artag.net/ 
[4] ARToolKitPlus – http://studierstube.icg.tu-graz.ac.at/handheld_ar/ 
artoolkitplus.php 
[5] StudierStubeTracker – http://studierstube.icg.tu-graz.ac.at/ 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/30
國科會補助計畫
計畫名稱: 基於物理模擬之整合桌上型與手機裝置互動技術的擴增實境系統
計畫主持人: 劉興民
計畫編號: 99-2221-E-194-034- 學門領域: 計算機圖學
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
