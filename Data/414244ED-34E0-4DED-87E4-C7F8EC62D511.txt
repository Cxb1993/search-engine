NSC 100-2221-E-126-015 RESEARCH REPORT 1
Analysis on a Dual Neural Network-Based 푘WTA
With Stochastic Output Nodes
Kevin Ho
Department of Computer Science and Communication Engineering
Providence University, Sha-Lu, Taiwan.
Abstract—Recently, a Dual Neural Network-based 푘WTA has
been proposed, in which the output nodes are defined as a heavi-
side step activation function. In this project, we extend this model
by considering that the output nodes are stochastic. Precisely,
the probability that a node will output one as a logistic function.
It is shown that the DNN-based 푘WTA with stochastic output
nodes is able to converge. But, the time taken for it to converge
to a stable equilibrium is infinite. In contrast to the original
model, the time taken is finite. Then, the convergence rate of the
network is analyzed. It is found that the convergence rates of the
network are three folds. Finally, the energy function governing
the dynamical behavior of the network is unveiled. Simulation
results are amended to illustrate the analytical findings and a
discussion on the convergence time is presented.
I. INTRODUCTION
In many classical design, a 푘WTA consists of 푛 nodes
and 푛2 connections [2], [11]. By re-formulating the 푘 win-
ners selection problem as a linear program, Wang and his
co-workers have recently proposed a Dual Neural Network
(DNN) structure to implement a 푘WTA which consists of only
푛 output nodes, and 2푛 connections [5], [17], [18]. The output
nodes are defined as heaviside step activation function, while
the hidden node behaves as a recurrent state variable1. As this
new structure is of much simpler design, this model is suitable
for hardware implementation.
However, as known in the studies of fault tolerant neural
network. Hardware implementation can never be perfect [8],
[14], [19]. The behavior of a electronic component can always
be affect by noise. In this regard, it is valuable to analyze the
dynamic behavior of this 푘WTA if the output nodes are of
stochastic behavior. We extend the work by Wang and his co-
workers by considering that the output nodes have stochastic
behaviors. A node with stochastic behavior is commonly used
to model the faulty behavior of a neuron [1], [6], [8], [15]
due to imperfect hardware implementation. Here, we define
the probability that the output of an output node is one by the
logistic function.
a) Contributions: The contributions of the project are
three folds.
∙ The exact convergence time of the DNN-kWTA model is
derived. With our result, we can study the convergence
The research work is supported by a research grant from Taiwan NSC,
number 100-2221-E-126-015.
1Note that an identical structure has very recently been proposed indepen-
dently in [10].
time without spending excessive time to simulate the net-
work dynamics. We also theoretically study the statistical
properties of the convergence time when the inputs are
uniformly distributed.
∙ The convergence of Wang’s DNN-based kWTA with
stochastic output nodes is proved. The convergence rate
of the network is analyzed and found that the convergence
rates of the network are three folds.
∙ The energy function governing the dynamical behavior
of the network is unveiled.
b) Publications: Those results have been published in a
journal paper, a conference paper and a book chapter.
∙ Yi Xiao, Yuxin Liu, Chi-sing Leung, John Sum, Kevin
Ho, Analysis on the Convergence Time of Dual Neural
Network-Based k-WTA, IEEE Transactions on Neural
Networks and Learning Systems, Vol.23(4), 676-682,
April 2012.
∙ Kevin I.J. Ho, Wei-Bin Lee, Siu-chung Lau, John Sum,
Convergence Time of Wang’s kWTA Network With
Stochastic Output Nodes, Proceedings of FTRA MUSIC,
Vancouver June 2012.
∙ John Pui-Fai Sum, Chi-Sing Leung, and Kevin Ho, Anal-
ysis on Wangs kWTA with Stochastic Output Nodes, in
B.-L. Lu, L. Zhang, and J. Kwok (Eds.): ICONIP 2011,
Part III, LNCS 7064, pp. 268V275, 2011.
In the rest of the report, the analytical results on this DNN-
based 푘WTA with stochastic output nodes is elucidated. In the
next section, the DNN-based 푘WTA network is presented and
the mathematical model of a stochastic output node is defined.
The convergence behavior of this 푘WTa with stochastic output
nodes is then analyzed and presented in Section III. Section IV
presents the energy function governing the dynamical behavior
of the network. The convergence time of the network is
discussed in Section V. Finally, we conclude the paper in
Section VI.
II. DNN-BASED 푘WTA
Figure 1 shows the structure of a DNN-based 푘WTA con-
sisting of three inputs. For a general 푛 inputs 푘WTA, the inputs
are denoted as 푢1, 푢2, ⋅ ⋅ ⋅ , 푢푛 and the outputs are denoted as
푥1, 푥2, ⋅ ⋅ ⋅ , 푥푛. Without loss of generality, we assume that the
values of 푢푖s are all distinct and bounded by zero and one.
Following Wang’s notation, we denote 푢¯푛, 푢¯푛−1, ⋅ ⋅ ⋅ , 푢¯푛−푘+1
as the 푘 largest numbers, and 0 ≤ 푢¯1 < ⋅ ⋅ ⋅ < 푢¯푛 ≤ 1.
NSC 100-2221-E-126-015 RESEARCH REPORT 3
푦(푡) further increases to a value close to 0.6, it increases in a
is very slow rate.
III. CONVERGENCE ANALYSIS
With the new definition on the output nodes, it is critical to
investigate if the network converges. If it converges, what will
be its convergence rate. To answer these questions, we need to
analyze the dynamical change of 푦(푡). In this section, we will
first show that 푦(푡) converges. Then, the convergent of 푦(푡)
will be derived. After that, we will show that 푦(푡) converges
in three different rates. The energy function governing the
dynamical behavior and the convergence time of the network
will be discussed in the next two sections.
Consider the change of 푦(푡) from 푡 to 푡+ 휏 , we get that
푦(푡+ 휏) = 푦(푡) +
1
휖
{
푛∑
푖=1
∫ 휏
0
푔˜(푢푖 − 푦(휂))푑휂 − 푘휏
}
. (8)
By (6), the above equation (8) can be rewritten as follows :
푦(푡+ 휏) = 푦(푡) +
휏
휖
{
푛∑
푖=1
푓(푢푖 − 푦(푡))− 푘
}
(9)
for 휏 is small and
푓(푠) =
1
1 + exp(−훼푠) . (10)
Theorem 1: For a DNN-푘WTA with stochastic output
nodes, there exists a unique equilibrium point. If furthermore,
∣푢¯푖+1− 푢¯푖∣ ≥ 2훼 for all 푖 = 1, ⋅ ⋅ ⋅ , 푛−1, the equilibrium point
푦∗ is given by
푦∗ = (푢¯푛−푘 + 푢¯푛−푘+1)/2.
Proof: For the uniqueness property, we let
퐹 (푦) =
푛∑
푖=1
푓(푢푖 − 푦(푡))− 푘. (11)
From (10), 푓 ′(푠) = 훼푓(푠)(1 − 푓(푠)). It is thus clear
that 퐹 ′(푦) < 0. In virtue of lim푦→−∞ 퐹 (푦) > 0 and
lim푦→∞ 퐹 (푦) < 0, 퐹 (푦) is therefore a monotonic decreasing
function with unique 푦∗ such that 퐹 (푦∗) = 0.
Rewrite the equation (9), we can have the following recur-
sive equation.
푦(푡+ 휏)
= 푦(푡) +
휏
휖
{
푛−푘−1∑
푖=1
1
1 + exp(−훼(푢¯푖 − 푦(푡)))
+
1
1 + exp(−훼(푢¯푛−푘 − 푦(푡)))
+
1
1 + exp(−훼(푢¯푛−푘+1 − 푦(푡)))
+
푛∑
푖=푛−푘+2
1
1 + exp(−훼(푢¯푖 − 푦(푡))) − 푘
}
. (12)
By the assumption that ∣푢¯푖+1 − 푢¯푖∣ ≥ 2훼 for all 푖 =
1, ⋅ ⋅ ⋅ , 푛 − 1, we can have the following approximations for
the case when 푢¯푛−푘 + 4훼 < 푦(푡) < 푢¯푛−푘+1 − 4훼 .
푛−푘−1∑
푖=1
1
1 + exp(−훼(푢¯푖 − 푦(푡))) = 0. (13)
1
1 + exp(−훼(푢¯푛−푘 − 푦(푡))) = exp(훼(푢¯푛−푘 − 푦(푡))). (14)
1
1 + exp(−훼(푢¯푛−푘+1 − 푦(푡))) = 1−exp(−훼(푢¯푛−푘+1−푦(푡))).
(15)
푛∑
푖=푛−푘+2
1
1 + exp(−훼(푢¯푖 − 푦(푡))) = 푘 − 1. (16)
By (13), (14), (15) and (16), the equation (12) can then be
written as follows :
푦(푡+ 휏) = 푦(푡) +
휏
휖
{exp(훼(푢¯푛−푘 − 푦(푡)))
− exp(−훼(푢¯푛−푘+1 − 푦(푡)))} . (17)
For 푡 → ∞, it is clear that the convergent 푦∗ is equal to
(푢¯푛−푘 + 푢¯푛−푘+1)/2. Hence, the proof is completed. Q.E.D.
Now, we can state the convergence theorem regarding the
convergence of 푦(푡) defined by (4), (5) and (6).
Theorem 2: For a DNN-푘WTA with stochastic output
nodes, 푦(푡) converges. If furthermore, ∣푢¯푖+1 − 푢¯푖∣ ≥ 2훼 for
all 푖 = 1, ⋅ ⋅ ⋅ , 푛− 1, lim푡→∞ 푦(푡) = (푢¯푛−푘 + 푢¯푛−푘+1)/2.
Proof: For the first part of the theorem, we let 푦∗ be the
equilibrium point such that lim푡→∞ 푦(푡) = 푦∗. So, we can
have the equality that
푛∑
푖=1
푓(푢푖 − 푦∗) = 푘. (18)
For large 푡, 푦(푡+휏) = 푦∗+Δ푦(푡+휏) and 푦(푡) = 푦∗+Δ푦(푡).
From (9), we can get that
Δ푦(푡+ 휏)
= Δ푦(푡) +
휏
휖
{
푛∑
푖=1
푓(푢푖 − 푦(푡))− 푘
}
= Δ푦(푡) +
휏
휖
{
푛∑
푖=1
(푓(푢푖 − 푦(푡))− 푓(푢푖 − 푦∗))
}
.(19)
Note that
∑푛
푖=1 푓(푢푖 − 푦) is a continuous function of 푦. By
Mean Value Theorem, there exists 휉(푡) ∈ [푦(푡), 푦∗] such that
푛∑
푖=1
푓(푢푖 − 푦(푡))− 푓(푢푖 − 푦∗)
=
푛∑
푖=1
푑푓
푑푦
∣∣∣∣
푦=휉(푡)
(푦(푡)− 푦∗)
= −훼
푛∑
푖=1
푓(푢푖 − 휉(푡))(1− 푓(푢푖 − 휉(푡)))(푦(푡)− 푦∗)
= −훼
푛∑
푖=1
푓(푢푖 − 휉(푡))(1− 푓(푢푖 − 휉(푡)))Δ푦(푡). (20)
NSC 100-2221-E-126-015 RESEARCH REPORT 5
0 0.2 0.4 0.6 0.8 1
1
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
Fig. 3. Energy function being minimized, with 훼 = 100. The number of
winners is 2 and the inputs are 0.5, 0.7, 0.8, 0.4, 0.1 and 0.3 respectively.
V. CONVERGENCE TIME
As mentioned in [13], [14], [16], convergence time is critical
for the design of a 푘WTA circuit. However, the convergence
time for this stochastic outputs 푘WTA is infinite. In this
regard, we turn our focus on the time when 푦(푡) has reached
[푢¯푛−푘, 푢¯푛−푘+1] and denote this time as 푇훼[푛−푘,푛−푘+1].
푇훼[푛−푘,푛−푘+1] = min푡 {푡∣푦(푡) ∈ [푢¯푛−푘, 푢¯푛−푘+1]} . (33)
Figure 4-6 show the convergence of 푦(푡) under three dif-
ferent conditions. The dotted lines shown in the top panels
corresponds to the times when 푦(푡) has just reached 푢¯푛−푘
if 푦(0) = 0. While the dotted lines in the bottom panels
corresponds to the cases when 푦(푡) has just reached 푢¯푛−푘+1
if 푦(0) = 1.
A. Estimation
Clearly, the value of 푇[푛−푘,푛−푘+1] is not easy to derive for
any finite 훼. For 훼 = ∞, which corresponds to the original
DNN-based 푘WTA, we have shown in [9] that4
푇∞[푛−푘,푛−푘+1] =
{
휖
∑푛−푘
푖=1
푢¯푖−푢¯푖−1
푛−푘−푖+1 if 푦(0) = 0
휖
∑푘
푖=1
푢¯푛−푘+푖+1−푢¯푛−푘+푖
푖 if 푦(0) = 1
(34)
for 푘 = 1, 2, ⋅ ⋅ ⋅ , 푛− 1.
So, our question is to ask if 푇훼[푛−푘,푛−푘+1] can be
estimated by 푇∞[푛−푘,푛−푘+1]. If it is the case, the analytical
results on convergence time of the original DNN-based 푘WTA
is able to estimate the convergence time of the DNN-based
푘WTA with stochastic output nodes. To proceed, we need to
have one assumption that
min{∣푢¯푖 − 푢¯푖−1∣} ≥ 2
훼
. (35)
This assumption ensures that 푢¯푛−푘−1, 푢¯푛−푘 and 푢¯푛−푘+1
are not too close to each other. Otherwise, as observed in
some simulations, the convergent of 푦(푡) are outside the range
[푢¯푛−푘, 푢¯푛−푘+1].
4For self-contained, the derivation is presented in the Appendix.
(a) 푦(0) = 0
10−8 10−6 10−4 10−2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
(b) 푦(0) = 1
10−8 10−6 10−4 10−2
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Fig. 4. Change of 푦(푡) against time. (a) 푦(0) = 0 and (b) 푦(0) = 1. Note
that the scale on the time is in log scale. The inputs are 0.5, 0.7, 0.8, 0.4,
0.1 and 0.3 respectively. 푘 = 2, 휖 = 0.0001 and 훼 = 500. The dotted lines
correspond to the convergence times obtained by (33).
B. Simulated studies
Figure 7-10 show the simulation results for the cases that
훼 = 50, 100, 200, 500 respectively. In each of the figure, the
upper panel shows the result when 푦(0) = 0. While the bottom
panel shows the result when 푦(0) = 1. In all the simulations,
the number of inputs and the number of winners are set to
20 and 10. In each simulation, we generate 100 samples in
which the inputs fulfill the condition that ∣푢¯푖+1− 푢¯푖∣} ≥ 0.02
for 푖 = 1, ⋅ ⋅ ⋅ , 19.
In accordance with the results as shown in Figure 8-10,
we can see that 푇훼[푛−푘,푛−푘+1] and 푇
∞
[푛−푘,푛−푘+1] are almost
identical for the case that 훼 = 500. For 훼 = 50, 100, 200,
푇훼[푛−푘,푛−푘+1] is slightly larger than 푇
∞
[푛−푘,푛−푘+1] but their
difference is insignificant. Thus, the estimated convergence
time given by (34) can be treated as a estimation on the lower
bound on the actual convergence time.
In summary, we can conclude from our simulation results
in the following statement. For min{∣푢¯푖 − 푢¯푖−1∣} ≥ 2훼 , the
convergence time of the DNN-based 푘WTA with stochastic
NSC 100-2221-E-126-015 RESEARCH REPORT 7
(a) 푦(0) = 0
10−6 10−5 10−4
10−6
10−5
10−4
Estimated
Ac
tu
al
(b) 푦(0) = 1
10−6 10−5 10−4
10−6
10−5
10−4
Estimated
Ac
tu
al
Fig. 7. Actual versus estimated convergence time for 푛 = 20, 푘 = 10 and
훼 = 50. (a) 푦(0) = 0 and (b) 푦(0) = 1. The actual time is obtained by the
definition of 푇훼
[푛−푘,푛−푘+1] in (33), while the estimated time is derived by
푇∞
[푛−푘,푛−푘+1] in (34).
[6] S. Judd and P. W. Munro, Nets with unreliable hidden nodes learn
error-correcting codes, in Advances in Neural Information Proceesing
Systems, Vol.5, S. J. Hanson, J. D. Cowan, and C. L. Giles, Eds. San
Mateo, CA: Morgan Kanfmanu, pp. 89-96, 1993.
[7] H.K. Khalil, Nonlinear Systems, MacMillan, 1992.
[8] Leung C.S., J. Sum, A fault tolerant regularizer for RBF networks, IEEE
Transactions on Neural Networks, Vol. 19 (3), pp.493-507, 2008.
[9] Leung C.S., J. Sum, K. Ho, Analysis on the Convergence Time of Dual
Neural Network-Based kWTA, in submission.
[10] Q. Liu, C. Dang and J. Cao, A novel recurrent neural network with
one neuron and finite-time convergence for 푘-winners-take-all operation,
IEEE Transactions on Neural Networks, Vol.21(7), 1140-1148, July
2010.
[11] E. Majani et al., On the k-winner-take-all network, Advances in Neural
Information Processing Systems, D.Touretzky, (Ed.) pp.634-642, 1989.
[12] C.A. Marinov, and B.A. Calvert, Performance analysis for a 푘-winners-
take-all analog neural network: Basic theory, IEEE Transaction on
Neural Networks, vol. 14(4), pp. 766-780, April 2003.
[13] C.A. Marinov, B.A. Calvert, R. Costea, and V. Bucata, Time evaluation
for analog KWTA processors, European Congress on Computational
Methods in Applied Science and Engineering, ECCOMAS 2004, P.
Neittaanmaki et al (eds)., Jyvaskyla, July 24-28, 2004.
[14] C.A. Marinov, and R.L. Costea, Time-oriented synthesis for a WTA
continuous-time neural network affected by capacitive cross-coupling,
IEEE Transactions on Circuits and Systems-I: Regular Papers, Vol.57,
No.6, 1358-1370, June 2010.
(a) 푦(0) = 0
10−6 10−5 10−4
10−6
10−5
10−4
Estimated
Ac
tu
al
(b) 푦(0) = 1
10−6 10−5 10−4
10−6
10−5
10−4
Estimated
Ac
tu
al
Fig. 8. Actual versus estimated convergence time for 푛 = 20, 푘 = 10 and
훼 = 100. (a) 푦(0) = 0 and (b) 푦(0) = 1. The actual time is obtained by the
definition of 푇훼
[푛−푘,푛−푘+1] in (33), while the estimated time is derived by
푇∞
[푛−푘,푛−푘+1] in (34).
[15] C.H. Sequin and R.D. Clay, Fault tolerance in feedforward artificial
neural networks, Neural Networks, Vol.4, 111-141, 1991.
[16] J. Sum, C.S. Leung, P.K.S. Tam, G.H. Young, W.K. Kan and L.W.
Chan, Analysis for a class of winner-take-all model, IEEE Transaction
on Neural Networks, vol. 10(1), pp. 64-71, Jan. 1999.
[17] J. Wang, and Z. Guo, Parametric sensitivity and scalability of 푘-Winners-
Take-All networks with a single state variable and infinity-gain activation
functions, in L. Zhang, J. Kwok, and B.-L. Lu (Eds), ISNN 2010, Part
I, LNCS 6063, pp.77-85, 2010.
[18] J. Wang, Analysis and design of a k-Winners-take-all model with a
single state variable and the Heaviside step activation function, IEEE
Transactions on Neural Networks, Vol.21(9), 1496-1506, Sep 2010.
[19] L. Wang, Noise injection into inputs in sparsely connected Hopfield and
winner-take-all neural networks, IEEE Transactions on Systems, Man,
and Cybernetics Part B: Cybernetics, Vol.27(5), 868-870, October 1997.
APPENDIX: CONVERGENCE TIME OF THE DETERMINISTIC
푘WTA
Now, let us describe the convergence behavior of 푦(푡) in the
DNN-based 푘WTA for any integer 푛 and 푘 < 푛. By (2) and
the condition that 푦(0) = 0, we can rewrite the state equation
NSC 100-2221-E-126-015 RESEARCH REPORT 9
By (36) and (38), we can further derive the convergence
times of this 푘WTA for the conditions that 푦(0) = 0 and
푦(0) = 1.
Theorem 4: If 0 ≤ 푢¯1 < ⋅ ⋅ ⋅ < 푢¯푛 ≤ 1, the convergence
time 푇∞[푛−푘,푛−푘+1] of the DNN-based 푘WTA with deterministic
output nodes is given by
푇∞[푛−푘,푛−푘+1] =
{
휖
∑푛−푘
푖=1
푢¯푖−푢¯푖−1
푛−푘−푖+1 if 푦(0) = 0
휖
∑푘
푖=1
푢¯푛−푘+푖+1−푢¯푛−푘+푖
푖 if 푦(0) = 1
(40)
for 푘 = 1, 2, ⋅ ⋅ ⋅ , 푛− 1.
Proof: For 푦(0) = 0, we let 푡1, ⋅ ⋅ ⋅ , 푡푛−푘 be the times such
that 푦(푡1) = 푢¯1, ⋅ ⋅ ⋅ , 푦(푡푛−푘) = 푢¯푛−푘. By (37), we can get
that
푡1 = 휖
푢¯1
푛− 푘 , 푡푖+1 = 푡푖 + 휖
푢¯푖+1 − 푢¯푖
푛− 푘 − 푖
for 푖 = 1, ⋅ ⋅ ⋅ , 푛 − 푘 − 1. As a result, the convergence time
푡푛−푘 is given by
푡푛−푘 = 휖
푛−푘∑
푖=1
푢¯푖 − 푢¯푖−1
푛− 푘 − 푖+ 1 , 푢¯0 = 0. (41)
For 푦(0) = 1, we let 푡′1, ⋅ ⋅ ⋅ , 푡′푛−푘 be the times such that
푦(푡′1) = 푢¯
−
푛 , ⋅ ⋅ ⋅ , 푦(푡′푛−푘) = 푢¯−푛−푘+1. By (37), we can get
that
푡′1 = 휖
1− 푢¯푛
푘
, 푡푖+1 = 푡푖 + 휖
푢¯푛−푖+1 − 푢¯푛−푖
푘 − 푖
for 푖 = 1, ⋅ ⋅ ⋅ , 푘 − 1. Repeat the same steps as for (41), the
convergence time 푡′푛−푘 can be derived and is given by
푡′푛−푘 = 휖
푘∑
푖=1
푢¯푛−푖+2 − 푢¯푛−푖+1
푘 − 푖+ 1 , 푢¯푛+1 = 1,
= 휖
푘∑
푖=1
푢¯푛−푘+푖+1 − 푢¯푛−푘+푖
푖
, 푢¯푛+1 = 1. (42)
The proof is completed. Q.E.D.
國科會補助計畫衍生研發成果推廣資料表
日期:2012/12/15
國科會補助計畫
計畫名稱: 分析利用Dual Neural Networks設計之具有隨機輸出神經元的k-Winners-
Take-All(kWTA)類神經網路
計畫主持人: 何英治
計畫編號: 100-2221-E-126-015- 學門領域: 人工智慧與仿生計算
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
N/A 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
