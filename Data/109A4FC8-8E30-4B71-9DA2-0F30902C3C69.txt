 2
摘要 
本報告說明本研究計畫在適應式影像訊號恢復處理之研究及結果。我們以隨機最佳化方
法作為機器學習之演算法並結合區塊式處理的技巧進行研究。本研究選用一個類神經系
統作為適應式濾波器，以隨機最佳化演算法訓練它，測試其恢復影像訊號之表現，並將
其結果與用平均最小平方法訓練此適應式濾波器之濾波結果比較，藉此知道本研究所使
用之方法之適應式影像訊號濾波的優劣。影像在擷取/傳送/接收之過程中會受到雜訊的
攻擊，致使影像訊號折損，本研究主要探討影像受到高斯雜訊攻擊後之復原。在影像訊
號處理之過程中，影像像素分成許多影像區塊(次影像)，再以區塊方式來處理影像訊
號。本研究使用之隨機最佳化方法具有計算量適當與快速收斂等優點。以本研究所提之
隨機最佳化方法訓練之適應式濾波器與以平均最小平方法訓練此濾波器之濾波結果比
較，前者之濾波結果明顯較佳。 
 
關鍵詞: 適應式影像處理，機器學習，區塊式處理，雜訊攻擊，隨機最佳化方法，平均最小平方法。 
 
Abstract 
Adaptive image signal processing using the RO-based machine learning method and the 
block-processing technique is presented in this report. With the machine-leaning-based 
adaptive filter, the study is focused on the image restoration from its Gaussian-type noisy 
version. The RO-based machine learning method is used for the training of the single-layered 
neural-network adaptive filter. For the block processing technique, the pixels of an image are 
divided into a finite number of blocks, and a moving window is designed to scan over the 
whole image block by block. With the learning method combined with the block processing 
technique, the proposed approach has shown the merit of moderate computation and fast 
convergence of learning for the filter. The proposed approach shows good filtering 
performance for the image-restoring problem, compared to the 2D Least Mean Square 
method. 
 
Keywords:  Image processing, Machine learning, Block processing, Noise corruption, Adaptive, Random 
optimization (RO), Least Mean Square (LMS). 
 
1. Introduction 
In wideband-based communication technologies, images can be acquired more easily 
than ever, especially with today’s Internet network and other computer-based communication 
networks. However, during acquisition, transmission or reception, images or image sequences 
might be blurred by noise with various distribution properties. The removal of noise is one of 
the most essential operations in image processing. In the case of impulse noisy corruption, a 
medium filter can be used to smash out the impulse noise easily. However, unlike impulse 
noise, a medium filter cannot remove the type of Gaussian noises, and it is usually tough to 
deal with this type of noise in image processing. In this paper, we will focus on Gaussian 
 4
3. ),( nmN and ),( nmη are related to each other, because of the noise channel function. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 1. Schematic diagram of noise canceling. 
 
The restored image ),( nmR can be expressed as follows. 
),(),(),( nmYnmDnmR −= ),()],(),([ nmYnmnmS −+= η            (1) 
Taking statistical mean-square operation of equation (1), based on assumption 2, we have: { } { } { }222 )],(),([),(),( nmYnmEnmSEnmRE −+= η                 (2) 
In the physical point of view, { }),(2 nmSE is the source signal power 
and { }2)],(),([ nmYnmE −η  is error signal power by the filter. If one can minimize this term of 
error signal (2), the restored signal power can be very close to the source signal power, and so  
 
 
 
 
 
 
 
 
 
 
 
Fig. 2. L×L block scanning over an M×M noise source image. 
 
clear image
S(m,n) +
nonlinear
channel
2-D adaptive 
filter
N(m,n)
noise source
+
),( nmη
+
noisy image
D(m,n)
Y(m,n)
–
E(m,n)
degradation image restoration system
restored 
image
estimated error
R(m,n)
M
ML
L
block
image
 6
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 4. Adaptive filtering for image restoration. 
 
 
in the next section. The two dimensional LMS filter [5] [6] is put in the block-based form in 
the following. For clarity, the following indices are defined first. 
 
M×M  : dimensions of the noise source image N  and the corrupted image D . 
L × L  : dimensions of a block (sub-image). 
F × F  : dimensions of a window. 
i      : index for the current block under processing, 2)/(,...,2,1 LMi = . 
k       : index for the current window scanning, 2,...,2,1 Lk = . 
( nm, )  : indices for the gray level of the m -th row and the n -th column in the image,  
( Mnm ,...,2,1, = ) 
( qp,  ) : indices for the p -th row and q -th column block in the image, 
( LMqp /,...,2,1, = ) 
( sr, )  : indices for the r -th row and s -th column gray level in a block, ( Lsr ,...,2,1, = ) 
( vu, )  : indices for the u -th row and v -th column window weight or windowed gray  
level in a block. ( Fvu ,...,2,1, = ) 
 
neural-network-based filter 
Σ
+
_noisy
image
noise
source
image
window
weights
D
N
pixel
M
M
L
F
filter
output
recovered
gray level
corrupted gray level
estimated error
F
L
 8
4. Random Optimization as Learning Algorithm 
The well-known Random Optimization (RO) is used as the learning method for the filter 
proposed in the paper. The RO is a derivative-free exploration in optimization problem [8-10]. 
It appeared in 1965s [7]. After that, many researchers were strived to strengthen its searching 
ability for global minimum. In the RO, the mechanisms for interpolation and jumping to 
avoid being entrapped at a local minimum are included.  
Suppose a real-valued cost function )(XJ  is given, and X consists of n-dimensional 
variables, TnxxxX ],...,,[ 21= . The objective is to find the vector ∗X  such that the cost 
value ][ ∗XJ  is optimal. The maximum search number maxk  is set to fasten the iterations 
for the RO learning. Gaussian random vectors with zero mean are generated in the RO 
learning for locally searching for better solution. A variance for generating random vectors is 
set for search-step. The local search number maxλ  is set to detect if the learning is entrapped 
at a local minimum. The procedure of the RO algorithm used in the study can be found in 
research literature. Basically, for the RO algorithm, random vectors are used to search for 
better solution continuously around the current point of parameters. The cost values of the 
points of parameters selected around the current point are competed to each other to 
determine the search for better solution locally around the current point. The procedure of the 
RO learning continues iteration by iteration. For the case of being entrapped of the learning, 
the RO is equipped with the mechanism of jumping away from the local minimum. Also, 
interpolation checking is involved in the RO for the case needed to increase the chance of 
obtaining better solution at iteration. The factors of 0.2 and 0.4 used in Steps 3 and 4 are 
experimental settings studied by Solis and Wets [11]. For much more details of the RO, it is 
suggested to read the paper cited in [8]. The RO is less likely to be entrapped at local 
minimum and is very suitable for machine learning based systems as long as cost function is 
given. For the adaptive image filtering problem as well as most engineering problems, one 
often hopes error signal to be as small as possible. The performance for the image processing 
is evaluated using the cost function defined as follows. 
∑∑
= =
−=
L
r
L
s
srYsrD
L
WJ
1 1
2
2 )],(),([
1)ˆ(                 (9) 
As the cost is minimized, the filter output ),( srY will be close to the noise ),( srη , while 
the ),( srS  is unrelated to both the ),( srY  and the ),( srη , and the learning process will 
not affect the source image ),( srS , as stated in section 2. Also, note that the learning process 
by the RO is done in the first block. The trained window weights are the same as the filter 
parameters, and are used in following blocks.  
 
 10
The learning constant in (8) is chosen to be γ = 4102.2 −× . 
3. Setting for the RO learning: 
Maximum learning iterations maxk =1500, 
Local search number maxλ = 20,  
Gaussian random vector with a fixed variance 2σ =0.03 and zero mean initially,  
Acceptable cost ε =1, and  
Jump factor β =3. 
4. Nonlinear channel transfer function is defined for experimental purpose as follows. 
),(40
),(2
1),(),( 2 nmN
nmN
nmNnm +−=η                        (10) 
The performance evaluated with the Mean-Square-Error (MSE) of the image is defined as 
follows.  
∑∑
= = ×
−M
m
M
n MM
nmSnmR
1 1
2)],(),([                          (11) 
After learning, the result by the 2D LMS filtering is shown in Fig.7, whose MSE is 176.42, 
and the recovered image by the RO filtering is shown in Fig.8, with MSE=9.7346. The 
performance has been improved significantly by the RO approach, compared to the 2D LMS 
filtering.  
 
 
Fig. 7. Restored image by the 2D LMS-based filter (L=8,F=2) with MSE=176.42. 
 
 12
blocks, and the filtering performance is great. With the same initial weight vector 0W
)
, the 2D 
LMS learning converges to TLMSW ]77.0,81.0,74.0,80.39[=∗
)
 after around 500-block iterations. 
After about 1400 epochs, the RO learning converges, and the weight vector reaches to 
T
ROW ]84.0,09.0,16.1,33.37[ −=∗
)
. The learning curve of the RO for the filter is given in Fig.10.  
0 500 1000 1500
1400
1600
1800
2000
2200
2400
2600
2800
3000
3200
learning epoch
co
st
 v
al
ue
 
Fig. 10. Learning curve of the RO learning for the filter. 
 
7. Conclusions  
   The proposed approach using the Random Optimization (RO) learning method to the 
problem of adaptive image processing has been presented. The objective of the proposed 
learning-based filtering approach is to use the simple single-layered neural-network filter to 
mimic the input-output behavior of the noise channel. In the study, we have illustrated the 
RO-learning for the filter to perform the task of adaptive filtering successfully, although the 
mapping ability for the single-layered neural-network filter is limited. The results by the 
proposed approach have been compared to those by the 2D LMS-based approach. Much 
better performance by the RO-based filter has been observed, compared to the LMS-based 
filter, as shown in Figs. 7,8, and 9.  
 
References 
[1]  R. C. Gonzalez and R. E. Woods: Digital Image Processing, Addison-Wesley, 1992. 
[2]  F. Russo, “Noise removal from image data using recursive neurofuzzy filters,” IEEE Transactions on 
Instrumentation and Measurement, vol. 49, n2, pp. 307-314, April 2000. 
[3]  B. Widrow, J. R. Glover, and J. M. McCool: “Adaptive noise canceling: principles and applications,” Proc. 
IEEE, vol. 63, pp. 1692-1730, Dec. 1975. 
[4]  C. Li, K.-H. Cheng: “Soft computing approach to adaptive noise filtering,” IEEE Proceedings Conference 
on Cybernetics and Intelligent Systems, Singapore, 1-3 Dec, 2004  
[5]  M. Ohki and S. Hashiguchi: “Two-dimensional LMS adaptive filters,” IEEE Trans. Consumer Electronic., 
vol. 37, pp.66-73, Feb, 1991.  
cost 
出席國際學術會議報告 
李俊賢老師 
國立臺南大學資工系, 臺南市中西區樹林街２段３３號. 
Phone: (06)2606123 ext.7700, Email: jamesli@mail.nutn.edu.tw 
會議名稱:  ICNC'07-FSKD'07, the 3rd International 
Conference on Natural Computation (ICNC'07) and 
the 4th International Conference on Fuzzy Systems 
and Knowledge Discovery (FSKD'07). 
會議地點:  Haikou, China 
會議日期:  2007年 8 月 24 日至 27 日. 
這次本人很高興出席此大型國際學術會議: 2007 ICNC'07-FSKD'07, the 3rd 
International Conference on Natural Computation (ICNC'07) and the 4th International 
Conference on Fuzzy Systems and Knowledge Discovery (FSKD'07).，會議地點在中
國海南島省海口市環島泰德大酒店舉行，整個會議期間共有 4天 (Aug 24 to 27, 
2007)，分 90 sessions 進行，皆為Oral Presentations (每一 session 有 6 至 12 篇發表)。
此次國際學術會議由海南大學主辦, Springer 及 IEEE Reliability Society 協辦，吸引
全世界許多國家之研究學者參與，包括美國、日本、台灣、中國、韓國及許多其
他的國家。這次國際學術會議全世界參與之 papers 約有 600 篇被接受、發表及
刊登入 IEEE Conference Proceedings。參與會議學者專家 4天估計超過 450 人，
為學術交流盛會，議程涵蓋了許多研究領域，包括 Neural Network Learning 
Algorithms, Ant Colony Optimization, Fuzzy Theory and Models, Text and Web 
Mining, Rough Sets, Pattern Recognition and Diagnostics, Cognitive Science, Image 
Processing, Pattern Recognition and Trend Analysis, Fuzzy Neural Systems, Hybrid 
Systems, Information Fusion, Robotics and Intelligent Control, Evolutionary 
Methodology, Classifier Systems, Theory and Foundations of Knowledge Discovery, 
Evolutionary Learning, Web-based Support Systems, Fuzzy Control, Mining Spatial 
and Time-Series Data, Multi-objective Optimization, Neural Networks, Knowledge 
