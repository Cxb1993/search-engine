 I
摘 要 
 
功能單元驗證(unit-level verification)在微處理器中是全晶片功能性驗證中最
重要的步驟。在功能單元驗證中,會先將此單元嵌入於一個用於模擬與週遭單元
運作行為的複雜性軟體，再使用一個模擬序列做為測量功能性收歛的依據。為了
產生這樣的序列，晶片設計人員必須了解此驗證單元和仿效軟體邊界的關聯。然
而，理解這樣的關聯是非常困難的。 
 
因此，這篇報告提出一個由有序二元決定森林演算法，實現漸近式的學習架
構。它可以自己評估單元內訊號的可控制性，以及提供晶片的資訊使晶片設計人
可以驅動這樣的訊號。我們並將這個架構應用在一個商用微處理器的核心，完成
了微處理器肉的讀／寫單元輸入訊號的可控制性，以及成功地發掘控制這些訊號
所須的資訊。 
 
為了確認二個不同版本的時序電路是否有相同的功能，最常見的方法是將電
路展到限定的數量進行驗證，此方法稱為有界時序等價驗證。雖然布林可滿足性
解答器的大幅進展已經使得組合電路的等價驗證上，可以應用至大型的電路，但
其解答器在解決時序電路或有界時序電路的問題時仍然非常沒有效率。因此，本
論文提出一個利用三階段的開發方法尋找電路的限制點，如此可以加速布林可滿
足性解答器在有界時序電路上的應用。這些限制點主要由時序電路上的正反器組
合而成。首先，我們會利用資料探勘的方法得到每個正反器的近似函數，再由這
些函數的組合找出不會同時發生的狀態組合，此則稱為限制點。被找出的限制點
會再被確認是否與模擬的資料相符。最後，有界時序電路會針對這些限制點逐一
驗證，通過驗證的限制點，才是有界時序電路上真實的限制點。完成三階段尋找
限制點的流程之後，所有的限制點會再被加回有界時序電路中，如此可以加速布
林可滿足性解答器的解答過程。實驗結果證明，對於ISCAS89 電路的有界時序驗
證可以達到平均40 倍的加速。 
 
關鍵字：功能性驗證 ; 學習演算法 ; OBDF; 布林可滿足性解答器; 有界時序
等效驗證 
 
 
 
 
 
 
 
 III
Contents 
 
List of Figures                 V 
List of Tables                 VI 
 
Chapter 1 Introduction………….………………………………………………….….1 
1.1  Research goal..……….………………………………………………….….2 
1.2  Method…………..….………………………………………………………2 
1.2.1 First year……………………………………………………………3 
1.2.2 Second year…………………………………………………………5 
 
Chapter 2 A learning framework for estimating signal controllability………………..7 
 2.1 Introduction………………………………………………………………….7 
 2.2 Incremental learning framework…………………………………………….8 
 2.3 Three steps of the framework………………………………………………..9 
 2.4 OBDF learning algorithm…………………………………………………..11 
 
Chapter 3  
A data mining algorithm to improve bounded sequential equivalence checking…….18 
 3.1 Bounded sequential equivalence checking………………………………….18 
 3.2 Boolean Satisfiability……………………………………………………….20 
 3.3 Data mining…………………………………………………………………22 
  3.3.1 Association rule mining……………………………………………...22 
  3.3.2 Support-confidence framework……………………………………...23 
 3.4 Learning framework………………………………………………………...24 
  3.4.1 Training data collection……………………………………………...24 
  3.4.2 Learning relaxed Boolean function………………………………….25 
 3.5 Constraint extraction method……………………………………………….29 
  3.5.1 Constraints in BSEC problem……………………………………….29 
  3.5.2 3-stage filtering method……………………………………………..32 
 
Chapter 4 Experimental results……………………………………………...……….35 
 4.1 Signal controllability in unit-level verification……………………………..35 
  4.1.1Experimental setup…………………………………………………...35 
4.1.2 Practical architectural issues………………………………………...36 
4.1.3 Experimental results…………………………………………………37 
 4.2 Bounded sequential equivalence checking…………………………………41 
 V
List of Figures 
 
Figure 1.1: Typical design flow overview [1]…………………………………………1 
Figure 1.2: Proposed incremental learning framework…………………………….….4 
Figure 1.3: OBDF vs. Venn diagram for covering space…………………………..,…5 
Figure 1.4: A learning-and-filtering framework for BSEC……………………………6 
 
Figure 2.1: An engineering flow for conducting signal controllability………………..8 
Figure 2.2: Testcase and the learning data transformation…………………………...10 
Figure 2.3: Step-by-step illustration of the OBDF learning algorithm………………13 
Figure 2.4: Impact comparison of splitting on  and …………………………..15 
 
Figure 3.1: Bounded sequential equivalence checking (BSEC) model………………18 
Figure 3.2: A sample circuit………………………………………………………….21 
Figure 3.3: Example of ruling cubes…………………………………………………23 
Figure 3.4: Flow of random simulation………………………………………………24 
Figure 3.5: Example of support-confidence learning………………………………...26 
Figure 3.6: Example for generating one ruling cube…………………………………27 
Figure 3.7: An example to illustrate constraint in SAT solving……………………...29 
Figure 3.8: Illustration for structural filtering………………………………………..33 
 
Figure 4.1: Block diagram of Freescale’s e200 microprocessor…………………..…35 
Figure 4.2: 10 stage pipeline in Freescale’s e200 microprocessor…………………...35 
Figure 4.3: Signal activities in K-instruction test templates…………………………38 
Figure 4.4: Supporting variable histogram…………………………………………..39 
Figure 4.5: Accuracy distribution of learning outputs……………………………….40 
Figure 4.6: Accuracy vs. confidence of output-27 as K=4…………………………..41 
Figure 4.7: Speedups of 4 large circuits with respect to 10 configurations…………44 
Figure 4.8: Bound for SAT solving of BSEC problems with and without constraints 
   ………………………………………………………………………….. .45 
Figure 4.9: SAT solving time with different # of constraints………………………..45 
 
 
 
 
 
 
 
 2
correctness of circuit. Formal verification could discover all the design problems by 
analyzing the circuit model. However, formal approach applicability in practice is 
limited due to the increase circuit size and complexity. On the other hand, 
simulation-based verification can be scable to larger circuit and it is the most common 
way to verify the designs in industrial cases. In simulation approach, it simulates the 
inputs patterns and then collects the output responses to analyze the circuit behavior. 
If all possible input patterns are simulated, it is easy to discover the corner cases in the 
design. However, if there are  inputs in circuit, the number of possible test patterns 
is . While  is large, it is impossible to simulate such amount of test patterns in a 
reasonable time. With large and complex circuits, simulation-based verification has 
become ineffective to finding bugs. 
 
In functional verification, formal-based verification can find all bugs but it is 
impractical in modern design, and simulation-based verification is scalable and 
practical but can not discover all bugs in circuit. Therefore, to complement strengths 
and weaknesses in formal and simulation techniques effectively, hybrid verification 
provides an immediate practical solution to overcome the verification problem. 
Hybrid verification is an approach which combines at least two techniques in formal 
and simulation verification method. 
 
1.1 Research Goal 
 
In this work, hybrid verification method is used to improve the signal 
controllability in unit-level verification and solve the bounded sequential equivalence 
checking (BSEC) [2] problem. In functional verification, signal controllability 
enhancement is used to explore hard-to-detect bug and improve the coverage of 
circuit. Equivalence checking involves ensuring that the current implementation of a 
design is functionally equivalent to an earlier version or the specification. Since 
improving the signal controllability and solving general problem in equivalence 
checking are difficult but important, the target in this work is solving the hard 
problem in functional verification by data mining techniques. 
 
1.2 Method 
 
Data mining technique is a technique to explore information from data. It utilizes 
statistical method to analyze the data and exploit the most frequent pattern to 
represent the data. Data mining method is widely used on many fields including 
marketing, science and engineering. Data mining techniques incorporated with 
 4
starts from generating multiple test sequences. Given a collection of test templates, 
each template is instantiated into a set of tests (each is a sequence of instructions) 
based on the constraints and biases specified in the template. After running simulation 
of these test sequences under each template, data observed on the boundaries at both 
full-chip level and unit-level are collected and sent to the later learning process. For 
each test template, the final learned result will summarize its signal controllability by 
including two kinds of useful information: (1) which inputs can be controlled under 
this template, and (2) how we can control these inputs when refining the template to 
instantiate tests. It is more interesting to note that essentially each template defines 
functional sub-space for the inputs of the UUV. Learning is to explore this sub-space 
based on the collected data in order to induce both kinds of information mentioned 
above. 
instruction
template 1
test 1
test 2
test i1
instruction
template 2
test 1
test 2
test i2
instruction
template K
test 1
test 2
test iK
unit under 
verification
Software C++
……… ………
Learning
simulation 
data
template 1 controllable:
signal 1 and how
…
signal n1 and how
template 2 controllable:
signal 1 and how
…
signal n2 and how
template K controllable:
signal 1 and how
…
signal nk and how  
Figure 1.2: Proposed incremental learning framework 
 
A data-mining engine based on a decision-diagram based learning approach was 
first proposed in [7]. The authors show that learning accuracy of their approach is 
comparable to other state-of-the-art learning techniques. However, they do not show 
how to quantify the confidence of their learning accuracy. In this work, we propose an 
ordered-binary-decision-forest (OBDF) based learning algorithm. This algorithm 
follows an approach of ensemble learning [8], which refers to building a collection of 
learners where each is trained with only a randomly selected portion of the dataset. 
Imagine that learning is to cover a space represented by the data. Figure 1.3 illustrates 
the basic principle of our OBDF algorithm (as well as ensemble learning). Each 
 6
random simulation
support-confidence
learning
constraint
candidates
functional filtering
historical filtering
structural filtering
learning
functions
simulation
data
constraint insertion
& SAT solving
le
ar
ni
ng
 p
ha
se
fil
te
rin
g 
ph
as
e
so
lv
in
g 
ph
as
e
  
Figure 1.4: A learning-and-filtering framework for BSEC 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 8
However, these new unit-level tests cannot be written on the unit-level boundary 
only. It is because typically the legal input space from the assembly instructions to the 
UUV is not explicitly specified in the design specification but implicitly defined 
within the emulation software. Hence, unit-level tests need to be written based on the 
inputs to the emulation software. Because designers (for the unit under verification) 
usually do not equip with the knowledge about the mapping from the inputs to the 
emulation software to the inputs of the UUV, it can be a difficult task to write tests 
which can satisfy the required values on the unit-level signals. 
 
These tests that provide the controllability to the unit-level input signals may not 
be complex. However, figuring out what kind of simple tests can be used to govern 
one unit-level input signal can be a tedious process. If this needs to be done manually, 
designers would need to (1) generate trial tests, (2) collect the simulation results on 
the unit-level signals of interest, (3) use a waveform viewer to examine the switching 
activities on the input, and (4) conduct a manual induction process to find a test that 
can indeed be used to control the target input signal. Figure 2.1 illustrates this 
engineering flow. 
 
Figure 2.1: An engineering flow for conducting signal controllability 
 
For each unit-level input signal, understanding what kind of tests can be used to 
control its value can be tremendously helpful to writing tests for the UUV. Hence, if 
the tedious process of figuring out this information can be automated, such a 
methodology will be valuable to designers for writing unit-level tests. Therefore, this 
work pursues this direction to develop such a learning framework. 
 
2.2 Incremental learning framework 
 
In order to assess the effectiveness of the learning framework, we further 
measure two indexes: (1) learning accuracy and (2) confidence of learning accuracy. 
Learning accuracy is measured by simulating an additional set of tests, and this index 
indicates how well the framework learns on the existing data. Besides, since it is 
impossible to exhaust the entire input space, current learning accuracy can only be 
evaluated on a sub-space explored by the additional set of tests. Therefore, another 
index is incorporated to quantify the confidence of the previous evaluation. 
 10
behaviors. It is because our objective is not to understand the circuit functionality but 
to understand what signals are controllable and how they are controlled. Simple 
instruction templates provide good controllability to prevent these unpredictable 
architectural behaviors from happening.  
 
Each simple instruction template, composed of the initialization sequence and 
core instruction selection, will instantiate a set of testcases. Learning boundaries are 
monitored to collect data during simple instruction testcase simulation. 
 
Data preprocessing: 
The preprocessing step transforms the simulation data into the format that data 
learning engine can take. Figure 2.2 illustrates the process. Since every input signal 
from the starting instruction in the initialization section to the current instruction in 
the core section may impact output signals at the current cycle, the sequence of those 
input signals in the simulation trace are concatenated into one bit stream. 
 
Later a pruning is performed on the bit stream to remove inputs with 
low-sensitivity activities. Sensitivity  is calculated by the total number of 1’s seen 
on input  over the total data size in the data set. Users can specify a threshold value 
 for low-sensitivity signals like 1%. If or , it means that the 0/1 
distribution on input  is too biased to provide information and thus can be reduced 
from the input dimension. The reduced bit stream will be sent to the learning 
algorithm as inputs. 
 
initialization
section
core
section
learning inputs
learning outputs
010…10101 | 10…10
100…10111 | 00…01
010…10101 | 10…10
… ……
concatenate
input
unknown 
constrained
function
output
assembly 
testcase
simulation 
trace
learning engine 
format
bit stream
prune
 
Figure 2.2: Testcase and the learning data transformation 
 
 12
 
The ensemble error can be written as  where 
 is the average of the errors of individual learners, and 
 is the average of their ambiguities, which stands for the variance 
of the output over the ensemble. 
 
Therefore, by averaging over the total Boolean space , the true generalization 
error is 
 
 
Here  and  stand for the (weighted) average of true errors of all ’s and 
the average of true ambiguities of all ’s over Bn. An important feature of 
ensemble learning is that it separates the generalization error into a term that depends 
on the generalization errors of individual learners and another term that contains all 
correlations between learners. The more the learners differ, the lower the error will be. 
That also infers the result of  since , which explicitly justifies the power of 
ensemble learning. 
 
To sum up, when designing an algorithm for ensemble learning, two major 
concerns need to be taken care of: 
 How to construct different but similar learners from the training data? 
 How to estimate the belief of each weak leaner? 
 
Our data learning algorithm resolves these two concerns by constructing an 
ordered-binary-decision-forest (OBDF). It employs the bootstrap method from [16], 
followed by ordered nearest neighbor (ONN) learning from [15] to construct 
individual learners. Then the out-of-bag evaluation helps us decide the weight for 
each OBDD learner. Figure 2.3 illustrates the step-by-step flow in our learning 
algorithm. The following sections will provide more details of individual techniques. 
 14
this error later. 
 
C. Selecting supporting variables 
One major difficulty mentioned in the previous decision-diagram-based learning 
algorithm in [15] is to find a good variable ordering for learning. However, since in 
the algorithm the OBDD sizes are determined by the size of training data , the 
worst-case OBDDs come from the complete binary trees and result in  
nodes at most. Therefore, instead of finding a good variable ordering directly, given 
the limited information provided by the training data, our first objective should be 
finding the variables that have the greatest impacts on the learning output among the 
large number of learning inputs. We call them supporting variables. 
 
Given the bootstrap samples generated from step 1, the current task is to select 
the supporting variables. Actually, this task can be transformed into finding the 
best-splitting attributes of the dataset for the tree-based algorithm. The impact of one 
variable can be defined upon the impurity difference of the dataset before and after 
splitting on the variable. The smaller the impurity is, the more clear trend the dataset 
can see. In this paper, Gini index proposed by Breiman et al. in [8] is applied to 
quantify the impact of one variable. 
 
Given the training sample dataset of  objects with its output class distribution 
, the Gini index of this dataset is defined as 
 
where  and  represent the numbers of 0’s and 1’s seen at the output from  
respectively. 
 
Given a variable ,  can be split into  with  objects and  
with  objects. The impact of  can be calculated by the difference from the Gini 
index of original sample of  objects and the weighted sum of Gini indices of two 
co-factor datasets,  and . That is, 
 
 
Figure 2.4 shows an example to compare the impact values of two variables  
and . Table I, II and III in Figure 8 represent the 0/1 class distributions of the 
original training sample, splitting on  and splitting on . The Gini indices of each 
node are also shown in Figure 8. Now we can compute the impacts of  and  as 
following 
 16
Algorithm 2.1 ONN learning algorithm: ONN( ) 
 
From [8], we know unstable learners like tree-based approaches 
characteristically have high variances and low biases. The original ONN learning 
algorithm also has similar characteristics. It always fits the training sample perfectly, 
and hence the derived learning models are vulnerable to small data changes. However, 
OBDF learning will mitigate this problem in the individual learner by tolerating errors 
during input dimension reduction and bootstrap sampling to prevent the model 
complexity from growing too high. 
 
E. Out-of-bag (OOB) weighting 
The typical ensemble learning uses uniform weights for each weak learner, i.e. 
 if , where  is the number of the weak learners in the 
ensemble and has the desirable property of , which guarantees the true 
generalization error of the ensemble learner will be less than and equal to the average 
true generalization error of individual learners. However, because not all learners are 
equally good in all parts of the problem space, weighting can help to improve the 
resolution of learning accuracy. Therefore, as shown in chapter 2.4-A the weight  
is introduced to reflect the generalization capability of each learner. 
 
For each bootstrap sample  as shown in Figure 2.3, only  objects are 
included from the training sample. Those objects are often called in-bag data. On the 
contrary, the remaining  objects are called out-of-bag (OOB) data. We can 
further re-use these OOB data to evaluate the quality of each weak learner since the 
out-of-bag data is drawn from the same distribution as the in-bag data used for the 
 18
Chapter 3  
A data mining algorithm to improve 
bounded sequential equivalence checking 
 
3.1 Bounded Sequential Equivalence Checking 
 
Typically, the problem of sequential equivalence checking (SEC) can be 
formulated as checking over time the output of the miter circuit which is composed of 
two finite state machines (FSMs). On the other hand, bounded sequential equivalence 
checking (BSEC) [2] as shown in Figure 3.1 is a special case of SEC problems and 
simplifies the problem formulation by limiting the timeframes under verification to an 
affordable number. 
PI0 PI1 PIk-1 PIk
C1
C2
C1
C2
C1
C2
C1
C2…
…
…
…
…
…
…
…
…
…
…
…
…
… … … …
…
 
Figure 3.1: Bounded sequential equivalence checking (BSEC) model 
 
Modeling a BSEC problem consists of two steps: miter construction and 
timeframe unfolding. The miter is constructed by linking every pair of two 
corresponding outputs from FSMs to one extra XOR gate. The miter is then unfolded 
to a user-specified number of timeframes, say k, to form the BSEC model. The 
combinational logic is duplicated into k copies. All flop-flops (FFs) are removed and 
inputs of FFs in one timeframe are connected to corresponding signals in the next 
timeframe. After unfolding the miter circuit, one big OR gate takes the disjunction of 
every output of added XOR gates from timeframe 1 to timeframe k. 
 
Since the BSEC model is purely combinational, its satisfiability can be solved by 
any formal engine. If the BSEC model is unsatisfiable (UNSAT), then two circuits are 
 20
strategies are required to reduce the total number of constraint candidates. 
 
3.2 Boolean Satisfiability 
 
Boolean Satisfiability (SAT) problem is a well-known constraint satisfaction 
problem. Given a propositional logic formula , determining whether there exists a 
variable assignment  that makes the formula evaluate to true. If such assignment _ 
exists, it indicates that  is satisfiable (SAT). Otherwise,  is unsatisfiable 
(UNSAT). For example, there is a satisfiable problem 
, there is a solution  satisfied the 
problem, which is . SAT is one of the central NP-complete 
problems [32]. 
 
Most solvers operate on problems for which  is specified in conjunctive 
normal form (CNF). This form consists of the logical AND of one or more clauses, 
which consist of the logical OR of one or more literals. The literal comprises the 
fundamental logical unit in the problem, being merely an instance of a variable or its 
complement denoted as . The advantage of CNF is that in this form, for  to be 
satisfied, each individual clause must be satisfied. If any clause is unsatisfied,  is 
unsatisfied. Table 3.1 lists the conjunctive normal form of some basic logic gates, 
where  and  denotes the inputs of a gate, and y denotes the output. 
 
Table 3.1 Conjunctive normal form of some basic logic gates 
 
 
Figure 3.2 shows a sample circuit, where ,  and  are inputs,  is output, 
and ,  and  are logic gates in this circuit. Table 3.1 is used to convert the 
circuit to CNF. The formula of the circuit is,  
 
, which is encoded by each logic gate. 
 22
3.3 Data mining 
 
3.3.1 Association Rule Mining 
Association rule mining (ARM) was first introduced by Agrawal et al. in [36], 
which aims to extract interesting correlations, frequent patterns, association or casual 
structures among sets of items in the transaction database or data repository. By 
definition, an association rule is an implication of the form  , where  and 
 are frequent itemsets in a transaction database and . Conventionally, 
 is called antecedent while  is called consequent. An association rule can be 
further classified into four different forms, , , and 
. The first form is called a positive rule while all of the others are called 
negative rules. 
 
When studying association rules, there are two important basic measures, (1) 
support and (2) confidence. Support, denoted as , is a statistical measure and 
 is defined as the fraction of records that contains the target itemset, , to the 
total number of records in the database. However, confidence (denoted as  ) of 
an association rule  is a measure of strength and defined as the ratio 
, where  means both  and  are present. 
 
The support-confidence framework proposed in [36] seeks positive rules of the 
form  with support and confidence greater than, or equal to, user-specified 
minimum support( ) and minimum confidence( ) thresholds, respectively, 
where 
  and  are disjoint itemsets; that is,  
  
  
 
The negation of an itemset  is indicated by . The support of , 
, is . To take a particular itemset, , for example, 
. Like positive rule, a negative rule (e.g. 
) also has a measure of its strength, , defined as 
. By extending the definition, negative association rule 
discovery seeks rules of the form  with support and confidence greater than, 
or equal to, user-specified minimum support ( ) and minimum confidence ( ) 
thresholds, respectively, where 
  and  are disjoint itemsets; that is,  
 , and  
 24
The ruling cubes are generated by taking the statistical information into account. 
First of all, a supporting variable method will explore the best splitting variable in 
each process. Next, the support-confidence measurement will calculate the  and 
 rate to quantify the ruling cube. If the sup and conf of ruling cube reach the 
minimal support ( ) and minimal confidence ( ) threshold, the ruling cube will 
be extracted. At the same time, the training data explained by the extracted ruling 
cube will be removed and the remaining data will continue to extract other ruling 
cubes. On the other hand, if the  and  of ruling cube do not reach the   
and  threshold, the supporting variable method will be applied to select another 
splitting variable until the ruling cube satisfying the ms and mc requirement. The 
detail algorithm will be introduced in Chapter 3.4. 
 
Return to the bounded sequential equivalence checking problem. The mined 
ruling cubes can be constructed the approximate function for each state (output of 
flip-flop) at some timeframe, the unreachable cross-timeframe can be examined by 
checking the intersection of functions for two arbitrary states. If the intersection is 
empty, then this state pair can be considered as a candidate of unreachable state pair. 
These unreachable state pairs will be set as constraints and apply to facilitate BSEC 
problem. 
 
3.4 Learning Framework 
 
3.4.1 Training Data Collection 
random
test
generation
input test
patterns
logic
simulation simulationdata
data 
partitioning
1-timeframe
database
2-timeframe
database
k-timeframe
database
…
 
Figure 3.4: Flow of random simulation 
 
The flow of random simulation is illustrated in Figure 3.4. First, a small number 
of test patterns are randomly generated and run through a logic simulator to collect the 
data from I/O and FFs. Then data partitioning prepares  (empirically, ) 
databases of different timeframes for later learning. For example, for a finite state 
machine  with 4 PIs, 1 PO and 3 FFs, Table 3.2(a) shows the simulation data after 
logic simulation. Since the FFs can be divided into PPOs and PPIs, each PPO value at 
 26
corresponding metrics, support and confidence (denoted as  and ), are used 
to quantify the importance of such a cube. If both  and  are larger than the 
threshold values,  and , respectively, then the cube  is a ruling cube and 
can be used to construct the relaxed Boolean function  later. In contrast, those 
Boolean cubes that fail to satisfy the support and confidence criteria will be excluded 
in . 
0 1 1 1 1 0 1
1 0 1 0 0 1 1
0 1 1 0 0 1 0
1 1 1 0 1 0 1
0 0 0 0 1 0 0
x x x x 1 0
x 1 0 x x x
x 1 x x 1 x
sup = 3/5   conf = 2/3
sup = 3/5   conf = 2/3
sup = 2/5   conf = 2/2
t1
t2
t3
t4
t5
c1
c2
c3
x1 x2x3 x4 x5 x6 y
 
Figure 3.5: Example of support-confidence learning 
 
Figure 3.5 shows an example of support-confidence learning. Given the database  
, the cube  satisfies ,  and  and  is  . However, 
since only  and  have the target output response ( ),  is  .  
and  of any other cube  can be computed in the same manner. Moreover, 
suppose that  and  are  and , only  satisfies the support and 
confidence criteria among three cubes and also is the only ruling cube on the basis of 
 in this example. 
 
Support-confidence learning algorithm is proposed to derive the set of ruling 
cubes for constructing the relaxed Boolean function for each flip-flip state. In Figure 
3.5,  = x1xx1x represents one ruling cube  where  and  are support 
literals which represent the most important variable states in such a ruling cube. One 
ruling cube is generated by adding the support literal one by one until no further 
support literal can be found. 
 
According to [37], the impact of one variable state can be achieved by comparing 
the impurity difference between the original database  and the new  split with 
respect to one variable state . In short, the support variable with maximum gain is 
the most important variable of the given database. For two literals (  and ) of one 
input variable and the database ,  and  can be formulated as 
 
 
 
 28
literal as shown in Figure 3.6(b). Note that the database  now becomes  
since the next support literal needs to be selected on the basis of all tests with  
in . 
 
Once the extracted Boolean cube  meets  and , it 
will be accumulated in the set of ruling cubes for constructing the approximate 
function of one flip-flop state later. However, if no other variable state can be selected 
and the current cube fails to meet the support and confidence criteria, the cube will be 
dropped. To avoid processing the same cubes, both the tests covered by ruling cubes 
and dropped cubes will be removed from the database. 
 
Algorithm 3.2 shows the overall algorithm to construct the approximate function 
for one flip-flop state. Given database ,  is the maximum number of support 
literals in one ruling cube since the maximum number of literal to split database  
is .  is the target function to be extracted and  is the set of current tests 
covered by . 
 
The algorithm starts from constructing a Boolean cube representing a 
sub-function f by adding one variable state one at a time.  is applied 
to select the next support literal  under . When both the frequency  and the 
accuracy  can meet the criteria,  is updated by conjuncting itself with . The 
algorithm keeps finding the next support literal to update f until the current cube  
has met the ruling cube criteria in line 9 or included more than  variable states in 
line 13.  continues accumulating ruling cube  for one flip-flop state until  
covers a percentage  of the total tests in the database . 
 
Note that according to the learning theory, the quality of a data-mining algorithm 
depends upon the data complexity, not the structural complexity underlying. Therefore, 
given a small number of simulation data, the FF state at the smaller -th timeframe 
seems relatively easy to learn its relaxed Boolean function. However, for those FF 
state at the large -th timeframe, more simulation may be needed but not necessary. 
Since learning for relaxed Boolean functions is one-time cost, the user can allocate a 
sufficient amount of time for his BSEC problems. 
 
 
 
 
 
 30
In DPLL-based algorithm, the assignment queue is used to record the assigned 
variables. Since all clauses in a SAT problem must be satisfable and the problem can 
be satisfiable. DPLL-based algorithm starts from the clause with minimum number of 
literals. If one variable v could be assigned the specific value, the variable could be 
pushed into the assignment and propagated to other clauses. The procedure would 
continue to assign other variables. If all variables have been assigned, the problem is 
satisfiable. If one implied variable conflicts the results in assignment queue, the 
problem is unsatisfiable. 
 
Figure 3.7(a) is the sample example and Figure 3.7(b) is the truth table of  
and  in sample circuit. From the truth table, it implies that the logic value of  is 
stuck at 0. If the circuit is modeled as a SAT problem and target , the SAT 
engine is expected to return an unsatisfiable answer. Table 3.3 lists the clauses of the 
sample circuit in Figure 3.7(a). 
 
 
Table 3.3: Clauses for Figure 3.7 
 
 
The procedure is used to solve the SAT problem, as follows: 
 
 step1: start from unit clause 14, imply , push  into assignment 
queue 
 step2: process , imply  in clause 11 and imply  in clause 
12, push  and  into assignment queue. 
 step3: process , imply  in clause 1 and imply  in clause 2, 
push  and  into assignment queue 
 step4: process , imply  in clause 4 and imply  in clause 5, 
push  and  into assignment queue. 
 step5: process , imply   conflict with V3 = 1, return UNSAT. 
 
In the procedure, it requires five steps to prove the SAT problem in Table 3.2 is 
unsatisfiable. Although the five steps seem not heavy, it would be difficult as solving 
 32
machine  is a conflict constraint if and only if  can never appear 
after  appears in  for all input sequences. 
 
Such constraints can be used to early stop the random walk during the SAT 
solving process, and their effectiveness will be demonstrated through our experiments 
later. The proposed method to exploit conflict constraint is introduced in the following 
section. 
 
3.5.2 3-stage Filtering Method 
Since previous studies [5] [26] that explore the constraints among internal nodes 
for SAT solving may suffer from a large number of constraint candidates, the 
proposed method instead considers cross-timeframe state-pairs as candidates and 
prunes the false cases on the basis of simulation data and the gate-level netlist of the 
circuit.  
 
Since each state-pair can be validated by running SAT solving on the BSEC 
model, one intuitive method is to enumerate all combinations of state-pairs for 
checking. However, given  and  are the numbers of flip-flops and the number of 
timeframe unrolling, respectively, the combinations for state-pairs will go up to 
, where 4 represents different cases of state-pairs including 
and . Running SAT solving for  times will be 
prohibitively time-consuming and even worse than solving the BSEC model directly. 
Therefore, a 3-stage constraint extraction shown in Figure 1.4 integrates multiple 
filtering strategies to help reduce the total number of state-pairs. 
 
The first stage is functional filtering. A data-mining algorithm called the 
support-confidence framework is developed to construct the approximate Boolean 
functions for each flip-flop state at one specific timeframe by learning the simulation 
data. Then, the cross-timeframe state-pair could be a constraint candidate if the 
conjunction of Boolean functions for such two flip-flop states is empty. Historical 
filtering in the second stage scans through the simulation data to prune the rare cases 
escaped from approximate functional learned in the first stage. The final stage is 
structural filtering which validates the candidate through SAT solving of the 
augmented miter circuit. Note that functional filtering plays an important role in the 
proposed method and needs generating as few candidates as possible to make the 
historical filtering and structural filtering efficient in time. 
 
The details of the proposed method, including 3-stage constraint extraction, will 
 34
timeframe  and the output of flip-flop  at timeframe  are connected by an 
extra AND gate. Such an example is illustrated in Figure 3.8. Next, SAT solving is 
performed on the corresponding ACC with enforcing 1 on the output of the extra 
AND gate. If the result is UNSAT,  is a true constraint; otherwise,  
should be removed. 
 
Constraint insertion 
Constraint insertion is the final step in the proposed framework. Given  as the 
number of timeframe unfolding in BSEC problems, each extracted constraint will be 
translated into multiple CNF constraint clauses of disjunction of inverting two FF 
states over  timeframes and appended to the CNF of the original BSEC model. For 
example, if  is one proven constraint, CNF clauses 
 will be appended to the original CNF for final 
SAT solving. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 36
of the load/store unit is the learning output boundary. Simulation data across 
timeframes at these two boundaries are collected. Then learning is to uncover the 
relationship between the signals at these two boundaries. 
 
4.1.2 Practical architectural issues 
Even though we know the signal boundary as the dotted lines indicate in Figure 
4.1, we are still far from understanding the relationship between these boundaries. It is 
because each testcase executes a sequence of instructions during simulation and a 
learning output may be affected by certain signals several timeframes ago. Therefore, 
the first issue that we need to resolve is the number of timeframes we need to watch at 
the output boundary. 
 
Many architectural techniques may worsen this problem: 
 (instruction pre-fetch): This technique increases operating speed by fetching 
the next instruction and starting to generate operand addresses before the 
current result has been calculated and stored. It may cause the same 
instruction to be fetched more than once, depending on the current machine 
state. 
 (pipeline stall): Hazards are the situations that prevent the next instruction 
from executing in its designated clock cycle. The most common solution is 
to stall the pipeline and wait for the completion of older instructions. 
Different kinds of hazards may induce different number of stalled cycles. 
Therefore, the activity lifespan (measured in cycles) of the testcases 
instantiated from the same template may vary. 
 (pipeline flush): Many situations such as branch mispredictions or exception 
handlings may cause the instruction pipeline to be flushed. Similarly, 
simulation data partitioning becomes harder because flushed cycles vary the 
activity lifespan of testcases. Moreover, those instructions before the flush 
is taken are not the true instructions in execution and become noise in the 
data. 
 
In our experiments, we resolve the issues mentioned above by constraining our 
test templates to avoid the unwanted situations happening. This greatly simplifies the 
learning process. Tests instantiated from these constrained test templates provide 
predictable controllability on the unit’s input signals. This gives designers another 
level of controllability if they decide to use these test templates later 
 
Suppose that we have successfully extracted the correct portion of the simulation 
 38
0
5
10
15
20
0 10 20 30 40 50 60
index of outputs
oc
cu
re
nc
es
0
5
10
15
20
0 10 20 30 40 50 60
index of outputs
oc
cu
rr
en
ce
0
5
10
15
20
0 10 20 30 40 50 60
index of outputs
oc
cu
rr
en
ce
0
5
10
15
20
0 10 20 30 40 50 60
index of outputs
oc
cu
rr
en
ce
(a) K=1 (b) K=2
(c) K=3   (d) K=4  
Figure 4.3: Signal activities in K-instruction test templates 
 
The first experiment is designed to observe signal activities of learning outputs 
with respect to different simple instruction templates. Signal activity is measured by 
the occurrences that one learning output has changed during the learning cycles. Since 
the clock cycles used in the core sections are 20 in our experiments, the maximum 
number of occurrences for one output is also 20. 
 
Figure 4.3(a) indicates the number of activity occurrences of each learning 
output. 22 learning outputs have no or low activities during 1-instruction testcase 
simulation, which also implies that these module inputs of load/store unit are not 
controllable through the current test template. Figure 4.3(b), (c) and (d) represents 
activity occurrence distributions for 2-, 3- and 4-instruction testcase simulation, 
respectively. Note that 3 more learning outputs (demarcated by stars) are specially 
marked out in Figure 4.3(b) because these outputs that cannot be controlled by 
1-instruction template now become controllable through 2-instruction template. 
Similarly, 3- and 4-instruction template can also excite one more learning output for 
each. 
 
The second observation compares the similarity of learners after applying ONN 
learning on bootstrap samples. Output 2 and output 23 are used as examples here. The 
average numbers of supporting variables used in an individual tree for output 2 and 
output 23 are 4 and 14, but the total numbers of possible supporting variables used in 
OBDFs are 7 and 22. No pair of learners in the OBDFs uses the same input variable 
 40
0
20
40
60
80
100
0 10 20 30 40 50 60
index of outputs
ac
cu
ra
cy
 (%
)
0
20
40
60
80
100
0 10 20 30 40 50 60
index of outputs
ac
cu
ra
cy
 (%
)
 
(a) output-2                     (b) output-23 
Figure 4.5: Accuracy distribution of learning outputs 
 
We further investigate the causes of these errors and observe that they are rooted 
from irreducible errors described in Section III. Fundamentally, the OBDF learning 
algorithm cannot avoid such errors. Instead, this just shows how the OBDF learning 
algorithm avoids the over-fitting problem by tolerating this small amount of errors on 
purpose in order to derive learning models using lower complexity. 
 
Table II further shows the learning accuracy and the average OBDD size of one 
tree in OBDF on those additional controllable outputs as demarcated by stars in 
Figure 9. High learning accuracy associated with small OBDD size on these outputs 
implies that their underlying behaviors in multiple instruction templates are simple. In 
other words, these signals are easy to control from the corresponding multiple 
instruction templates. 
 
Table 4.2: Accuracy on additional controllable outputs 
 
 
In the last experiment, we study the impact of the number of trees in the forest in 
terms of learning accuracy and predicted confidence.The ensemble learner  will 
output the predicted answer decided by , which ranges from 0 to 1. 
Predicted answers ranging from 0.3 to 0.7 is classified as high risk. High risk ratio is 
defined as the total number of answers in high risk range divided by the total number 
of validation testcases. We use high risk ratio to estimate confidence. The lower the 
high risk ratio, the higher the confidence of the ensemble learner. 
 42
Table 4.2: Comparison of numbers of constraint candidates 
 
 
Table 4.2 demonstrates the effectiveness of 3-stage filtering by reporting the 
numbers of constraint candidates across different timeframes after each filtering. 
Column 1 lists the name of the benchmark circuits while column 2 represents the 
initial number of constraint candidates. Column 3, 4 and 5 denote the numbers of 
candidates after functional, historical and structural filtering, respectively. 
 
Table 4.3: Comparison of numbers of constraint candidates 
 
 44
0
100
200
300
400
500
1 2 3 4 5 6 7 8 9 10
configuration index
sp
ee
du
p 
(X
)
0
50
100
150
200
250
300
1 2 3 4 5 6 7 8 9 10
configuration index
sp
ee
du
p 
(X
)
0
20
40
60
80
100
1 2 3 4 5 6 7 8 9 10
configuration index
sp
ee
du
p 
(X
)
0
200
400
600
800
1 2 3 4 5 6 7 8 9 10
configuration index
sp
ee
du
p 
(X
)
(a) s35932 (b) s38584
(c) b13 (d) b15  
Figure 4.7: Speedups of 4 large circuits with respect to 10 configurations 
 
To demonstrate the effectiveness of the extracted constraints, we compare the 
numbers of timeframe unfolding on 4 large benchmark circuits without and with the 
extracted constraints in a fixed time, say 1200 seconds in our experiments. In Figure 
4.8, the dotted lines denote the original benchmark circuits while the solid lines 
represent the benchmark circuits with extracted constraints. Results show that after 
adding constraints, the bound k can increase from 8X to 20X. It also means that the 
quality of BSEC can be improved greatly by applying our framework. However, 
different time limits may result in different improvements. 
 
We further investigate the relations between the number of constraints and 
runtime for SAT solving on three big ISCAS 89 circuits. Figure 4.9 shows the result 
where Y-axis represents the runtime for new SAT solving normalized to the original 
runtime used by SAT solving without any constraint. Obviously, s35932 and s38584 
converge fast and only require 500 constraints while s15850 may require 1900 
constraints to converge. However, since not each constraint has same contribution to 
SAT solving, the efficiency of solving BSEC may depend on the quality of constraints, 
not the number of constraints. Therefore, how to select enough good constraints to 
fast converge SAT solving is worth investigation and can be a topic for future 
research. 
 46
Chapter 5 Conclusion 
 
Full-chip functional verification for modern microprocessor designs usually 
adopts a divide-and-conquer strategy to save simulation cost and achieve better debug 
efficiency where unit-level verification is critical to this success. The unit under 
verification is typically embedded inside an emulation software which mimics the 
full-chip behavior. However, it can be very difficult to control signals at unit-level 
from full-chip boundary. Hence, this work proposes an ordered-binary-decision-forest 
(OBDF) algorithm to implement an incremental learning framework to automate 
estimating signal controllability and provide information to govern these unit-level 
signals. 
 
The proposed OBDF algorithm mathematically outperforms the previous ONN 
algorithm in terms of lower model complexity and lower error variance. Meanwhile, 
we also utilize Freescale’s e200 microprocessor design to demonstrate the 
effectiveness of the incremental learning framework. The learning results show that 
45 inputs of the load/store unit are highly controllable using only simple instruction 
templates. 
 
Our incremental learning framework can accurately estimate the controllability 
of unit-level inputs from full-chip boundary. In the future, several issues deserve 
further investigations : (1) Research on selecting best splits has become thriving in 
data mining area. Many other measures have been proposed and can be applied to 
decide supporting variables as well. (2) Searching the optimal number of trees to 
achieve best learning accuracy and stable high risk ratio can be formulated into an 
optimization problem where many existing algorithms such as genetic algorithm can 
be applied. (3) The learned information can be integrated into the pseudo-random test 
pattern generator (RTPG) and provides better guides during test generation. 
 
Functional verification in VLSI includes an important problem of determining 
equivalence for two circuit descriptions during the design process. The development 
of Boolean satisfiability (SAT) solvers has made great advances recently and makes 
combinational checking equivalence scalable and robust for large VLSI designs. 
However, with the increasing design complexity, checking the equivalence of two 
sequential circuits remains computationally inefficient. In this paper, we proposed a 
framework which integrates data mining, simulation and formal techniques to extract 
unreachable cross-timeframe state-pairs as constraints to facilitate SAT solving for 
 48
References 
[1] Charles H.-P. Wen, Li-Chung Wang and Kwang-Ting Cheng, "Chapter 9: 
Functional Verification," in Electronic Design Automation: Synthesis, Verification, 
and Testing, Elsevier/Morgan Kaufmann, Oct. 2008 
[2] D. Stoffel, M. Wedler, P. Warkentin and W. Kunz, "Structural FSM Traversal," in 
IEEE Trans. Computer Aided Design (TCAD), vol. 23, no. 5, pp. 598-619, 2004. 
[3] O. Guzey, L-C. Wang, J. Levitt and H. Foster, "Functional test selection based on 
unsupervised support vector analysis," in Proc. Design Automation Conf. (DAC), pp. 
262-267, 2008. 
[4] H-K. Peng, H-P. Wen and J. Bhadra, "On Soft Error Rate Analysis Beyond Deep 
Submicron - A Statistical Perspective," in Int'l Conf. Computer Aided Design 
(ICCAD'09), Nov. 2009. 
[5] W. Wu and M. S. Hsiao, "Mining Global Constraints for Improving Bounded 
Sequential Equivalence Checking," in Proc. Design Automation Conf. (DAC), pp. 
743-748, 2006 
[6] W. Wu and M. S. Hsiao, "Mining Global Constraints with Domain Knowledge for 
Improving Bounded Sequential Equivalence Checking," in IEEE Trans. CAD 
(TCAD), vol. 27, No.1, pp. 197-201, Jan. 2006 
[7] C. Wen, O. Guzey, L.-C. Wang and J. Yang, ”Simulation-based Functional Test 
Justification Using a Boolean Data Miner,” to appear in IEEE Int’l Conf. on 
Computer Design, 2006. 
[8] L. Breiman, Random forests. Machine Learning Journal, vol 45, pp. 5-32, 2001. 
[9] C. Roth, J. Tyler, P. Jagodik and H. Nguyen ”Divide and conquer approach to 
functional verification of PowerPC microprocessors,” Proc. Int’l Workshop on Rapid 
System Prototyping, pp. 128-133, 1997. 
[10] J. Monaco, D. Holloway and R. Raina, ”Functional verification methodology for 
the PowerPC 604 microprocessor,” Proc. Design Automation Conf., pp. 319-324, 
1996. 
[11] K. Albin, ”Nuts and bolts of core and SoC verification,” Proc. Design 
Automation Conf., pp. 249-252, 2001. 
[12] C. Scafidi, J. D. Gibson, and R. Bhatia, ”Validating the Itanium2 Exception 
Control Unit: A Unit-level Approach,” IEEE Design & Test of Computers, pp. 94-101, 
2004. 
[13] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning - 
Date Mining, Inference, and Prediction, Springer, 2001. 
[14] V. N. Vapnik, The nature of statistical learning theory, Springer, 1999. 
[15] C. Wen, O. Guzey, L.-C. Wang and J. Yang, ”Simulation-based Functional Test 
 50
Europe (DATE), pp. 1170-1175, 2007. 
[29] P.N. Tan, M. Steinbach and V. Kumar. Introduction to Data Mining, Addison 
Wesley, May 2005 
[30] A. Mishchenko and R. K. Brayton, "SAT-Based Complete Don't-Care 
Computation for Network Optimization," in Proc. Conf. Design, Automation and Test 
in Europe (DATE), pp. 412-417, 2005. 
[31] M. L. Case, V. N. Kravets, A. Mishchenko and R. K. Brayton, "Merging Nodes 
Under Sequential Observability," in Proc. Design Automation Cconf. (DAC), pp. 
540-545, 2008. 
[32] M. R. Garey and D. S. Johnson, "Computers and Intractability: A Guide to the 
Theory of NP-Completeness," W.H. Freeman, 1979. 
[33] M. Davis and H.Putnam, "A computing procedure for quantification theory," in 
Journal of the ACM, pp. 201-215, 1960. 
[34] M. Davis, G. Logeman, and D. Loveland, "A machine program for theorem 
proving," In Proceedings of the Communications of the ACM, pp. 394-397, 1962. 
[35] J. P. Marques-Silva and K. A. Sakallah, "A Search Algorithm for Propositional 
Satisfiability," In IEEE Transactions on Computers, Vol. 48, No. 5, pp. 509-521, 1999 
[36] R. Agrawal, T. Imielinski and Swami AN, "Mining Association Rules between 
Sets of Items in Large Databases," In Proc. of the ACM SIGMOD Int. Conf. on 
Management of data, Jun. 1993. 
[37] H. P. Wen, L. C. Wang and J. Bhadra, "An Incremental Learning Framework for 
Estimating Signal Controllability in Unit-Level Verification," in Proc. Int'l Conf. 
Computer Aided Design (ICCAD), pp. 250-257, 2007. 
