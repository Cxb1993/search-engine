 
 
detailed in Section 3.1, normalizes the brightness of an image 
in order to reduce the effects of environmental lighting 
variations. In the final color transform step, we calculate a 
number of chrominance values for each image pixel for 
locating skin regions in the next stage. 
In facial feature extraction stage, the face, eyes, and mouth 
are detected. There are four steps involved in this stage: skin 
detection, face localization, eyes and mouth detection, and 
feature confirmation. We first examine the chrominance 
values of each image pixel to see whether they satisfy the 
criteria of a predefined skin model defined in [10]. The largest 
connected skin region is preserved so as to ignore noisy 
patches. Moreover, the largest skin area usually corresponds 
to the face region. 
The located face is typically imperfect. Detecting eyes and 
mouth within the located face is unreliable. So, we search for 
facial features throughout the entire image (see Section 3.2). 
The imperfect face will later be used to confirm the detected 
facial features. Searching for facial features throughout the 
entire image is time consuming. However, the facial feature 
detection process is not necessarily applied to every video 
frame. We will apply Kalman filter [8] to track facial features 
over video images once they are located in a frame. The facial 
feature detection process is initiated only it receives a “no 
feature” signal from the system. 
The facial feature detection module often returns a number 
of eye and mouth candidates. The feature confirmation step is 
to select as the actual features among the candidates based on 
both a predefined face model and the face region located 
earlier. The face model specifies the relationships between 
facial features in terms of scale-invariant constraints. Face 
model will be detailed in Section 3.2.  
Two groups of the facial parameters will be estimated in the 
next stage. The first group, including head orientation (tilt, 
pan and rotation angles) and degree of gaze, are estimated 
every time the driver’s face is located. The second group, 
including percentage of eye closure over time, eye blinking 
frequency, eye closure duration, and mouth openness duration, 
are estimated every n times (n = 300 in our experiments) of 
face localization. The definition of the facial parameters will 
be discussed in Section 3.3. A fuzzy integral, addressed in 
Section 3.4, later combines the parametric values of facial 
features to obtain a value, which indicates a level of 
drowsiness.  
3. Implementations 
In this section, the implementation details of lighting 
compensation, color transform, face model, facial feature 
extraction, and fuzzy reasoning are addressed. 
3.1. Lighting Compensation 
Consider a color image C(R, G, B), where R, G, and B are 
the three color components of the image. First, we estimate the 
brightness level of the image by computing the intensity of the 
image as I = (R+G+B)/3. Let h(·) be the histogram of I. The 
distribution of h(·) gives a global description of the brightness 
of image C. Instead of calculating the skewness of h(·), we 
compute its distribution tendency. Both measure the 
asymmetricity of h(·) but with respect to its mean and 
mid-range, respectively. Let m denote the mid-range of h(·), 
where m=L/2 and L is the number of gray levels. The 
distribution tendency of h(·) is computed as 3 2 2/( )t M M M= , 
where M2 and M3 are the second and third moments of h(·) 
about m. The value of t can be positive or negative. A positive 
t indicates a relatively bright image, while a negative t 
indicates a relatively dark image.  
Having computed the histogram distribution tendency of an 
image, if 10t ≤ − , we calculate a fraction ( 138)tp e βα − += , 
where α = 0.713 and β = 0.013. Then, for each color 
component of the image we find the pixels with the top p 
fraction of values and average them to obtain the averages (aR, 
aG, aB). For each image pixel, we scale its (r, g, b) color values 
as 255
R
rr
a
′ = , 255
G
gg
a
′ = , and 255
B
bb
a
′ = , which will raise the 
brightness of the image. In a similar way, if 50t ≥ , we 
calculate p, where α =0.323 and β = 0.011. But, now we find 
the pixels in the bottom p fraction and average them to (aR, aG, 
aB). The image pixels are scaled by 
255255(1 )
255 R
rr
a
−′ = − − , 
255255(1 )
255 G
gg
a
−′ = − − , and 
255255(1 )
255 B
bb
a
−′ = − − . This results in a 
decrease of the image brightness. Finally, no operation is 
applied to the image if -10< t < 50. Figure 3 shows an example 
after compensating. 
   
Figure 3. Example illustrating lighting compensation; (a) 
input image, (b) histogram, distribution tendency, and fraction, 
and (c) resultant image. 
3.2. Facial Feature Detection 
In facial feature extraction, eyes and mouth are detected in 
an image using the Hsu et al. technique [4]. First, the image is 
transformed from the RGB to the YCrCb color spaces [3]. 
Some maps calculated from the new transformed image 
emphasize the chrominance and luminance characteristics of 
eyes and mouths. Finally, a number of eye candidates are 
detected. The example in Figure 4(a) and (b) shows the 
detected eye and mouth candidates. 
In Figure 4(c) of a face model, there are five constraints 
(two qualitative and three quantitative constraints) describing 
the face model: (1) the mouth is lower than the two eyes; (2) 
the horizontal position of the mouth is between those of the 
two eyes; (3) the tilt of the line connecting the two eyes from 
the horizontal is smaller than 10 degrees; (4) the angle 
between the line joining the two eyes and the line connecting 
the mouth and any eye is between 45 and 75 degrees; (5) lee < 
lrm, lee < llm and llm ≈ lrm. The lengths of lee, lrm, llm, and lem are 
determined from a few faces detected at the early stage of 
system processing. 
Let Se and Sm be the sets of eye and mouth candidates, 
respectively. For each pair of eye candidates in Se and a mouth 
candidate in Sm, we examine their relationships to see whether 
they satisfy the constraints of the face model. If so, the 
 
 
either the Lebesque or Riemann integral. In this study, the 
Sugeno fuzzy integral based on the Lebesque integral is 
considered. Let :  [0,1]f S →  be a function defined on a finite 
set S and : ( ) [0,1]g P S →  be a set function defined over the 
power set of S. Function g(·), referred to as the Sugeno fuzzy 
measure function, satisfies  
( ) ( ) ( ) ( ) ( ), 1g A B g A g B g A g Bλ λ= + + ≥  ∩ .    (1) 
The fuzzy integral of f(·) with respect to g(·) is defined as  
[0,1]
( ) sup { ( )}e f s g g Aαα
α
∈
= ⋅ = ∧∫
S
,                         (2) 
where  represents fuzzy intersection and 
{ | ( ) }A s S f sα α= ∈ ≥ .  
Suppose we have a collection of information sources, S = 
{si, i = 1, ···, n}, and a set of hypotheses, H = {hi, i = 1, ···, m}. 
A decision D is to be made from H based on S using the fuzzy 
integral approach. To this end, for each hypothesis h H∈  we 
calculate its fuzzy integral value eh according to Equation (2). 
The decision is then determined as 
* arg max hh H
D h e
∈
= = . To 
calculate eh, f(·) and g(·) are defined. Function f(·)  receives an 
information source s S∈  and returns the value that indicates 
the level of support of s to hypothesis h. Function g(·)  taking 
as input a subset A of S gives the value that specifies the 
degree of worth of subset A relative to the other sources in S. 
To calculate g(A), we have to first determine the degrees of 
worth of individual information sources, 
{ ( ),  1,  ,  }iW w s i n= = ⋅⋅⋅ , where ( ) ({ })i iw s g s= . g(A) can then 
be recursively calculated using Equation (1),   
( ) ( (1 ( )) 1) /
i
i
s A
g A w sλ λ
∈
= + −∏ .           (3)  
Since g(S) = 1 and Equation (3). Parameter λ can be 
determined by solving 1 (1 ( ))
i
i
s S
w sλ λ
∈
+ = +∏ . 
Let 1 2{ , , , }nS s s s′ ′ ′ ′= ⋅⋅⋅  be the sorted version of S such that 
1 2( ) ( )f s f s′ ′≥ ≥ ⋅⋅⋅ ( )nf s′≥ . Equation (2) can be rewritten as   
1[0,1]
( ) sup { ( )} [ ( ) ( )]i ii n
S
e f s g g A f s g Sαα
α ≤ ≤∈ ′ ′= ⋅ = ∧ = ∨ ∧∫ ,  (4)  
where  represents fuzzy union. 
B. Reasoning 
Let the eight facial parameters P = {PERCLOS, BF, D, α, β, 
γ, Dm, dG} involved in our drowsiness analysis have degrees of 
worth, W = {w1, w2, ···, w8}, which are determined a priori on 
the basis of three criteria: importance, accuracy and reliability. 
In this study, W ＝ {0.93, 0.8, 0.85, 0.5, 0.3, 0.3, 0.5, 0.9} to 
be discussed in the next section is used. Based on W, the 
degree of worth g(A) of any subset A P⊆  is readily computed 
by Equation (3).  
Let V = {v1, v2, ···, v8} be the parametric values estimated 
from a face. This set of values is transferred into S = {s1, s2, ···, 
s8} using the transfer functions listed in Table 1. Set S here 
serves as the set of information sources. Next, we determine 
the hypothesis set H. Let 10 min /10
i
is S
m s∈
⎢ ⎥= ×⎢ ⎥⎣ ⎦  and 10 max /10i is SM s∈
⎡ ⎤= ×⎢ ⎥⎢ ⎥ , 
in which ≤·⎦ and ϒ·⎤ denote the floor and ceiling operators. 
The hypothesis set is then defined as H = {m, m+0.1, m+0.2, 
···, M}, from which a hypothesis is to be selected as the 
drowsiness level of the face.  
For each hypothesis h H∈ , we calculate the levels of 
support of information sources is S∈  to the hypothesis h by 
( ) 1h i if s s h= − − . Clearly, the larger the difference between si 
and h the lower the level of support of si to h. We then sort the 
information sources in S according to their calculated levels of 
support. Let 1 2 8{ , , , }S s s s′ ′ ′ ′= ⋅⋅⋅  be the sorted version of S such 
that 1 2( ) ( )f s f s′ ′≥ ≥ ⋅⋅⋅ 8( )f s′≥ . Substituting ( )if s′  and ( )ig S ′  
into Equation (4), we obtain the fuzzy integral value eh of h. 
The above process repeats for every hypothesis in H. Finally, 
the drowsiness level of the face is determined as 
* arg max hh H
h e
∈
= . 
Table 1.Ttransfer functions for facial parameters 
 
4. Experimental Results 
The proposed driver’s drowsiness detection system has 
been developed using the Borland C++ Programming 
Language run on an Intel Solo T1300 1.66 GHz PC running 
under Windows XP Professional. The input video sequence to 
the system is at a rate of 30 frames/second. The size of video 
images is 320 x 240 pixels.  
A number of experimental video sequences of various 
drivers were taken under different illumination conditions, 
including shiny light, sunny day, cloudy day, twilight, 
underground passage, and tunnel. Table 2 summarizes the 
performances of the system on these video sequences. The 
system performance of our concern includes facial feature 
detection rate rd, face tracking rate rt, and system speed s, 
indicated in the top row of Table 2. Let n be the number of 
frame. The facial feature detection rate rd is defined as rd = 
n1/nd, where nd is the number of frames to which the facial 
feature extraction module is applied and n1 is the number of 
frames in which facial features are correctly detected. 
Likewise, the face tracking rate rt is defined as rt = n2/nt, where 
nt = n - nd is the number of frames to which the face tracking 
module is applied and n2 is the number of frames in which the 
face constituting facial features is correctly located by the 
tracking module. Let T be the total processing time of the 
video sequence. The system speed s is defined as s = t / n. 
As mentioned, the processing speed of our system is 
primarily governed by the facial feature extraction and face 
tracking modules. The former takes about 1/8 seconds to 
detect facial features in an image, while the latter takes about 
1/25 seconds to locate facial features in an image. The system 
 
 
 
國科會補助國內專家學者出席國際會議報告 
 
報告人姓名  陳世旺 
 
服務機構 
及職稱 
 
國立臺灣師範大學資訊工程系 
 
會議時間及  
地點 自 99 年 7 月 11 日至 99 年 7 月 14 日， 中國青島 
會議 
名稱 
(中文) 機器學習與控制國際研討會 
(英文)  International Conference on Machine Learning and Cybernetics 
發表 
論文 
題目 
(中文) 使用模糊屬性化圖論比對預測快速道路上危險駕駛事件 
(英文) Dangerous Driving Event Prediction on Expressways Using Fuzzy  
Attributed Map Matching 
 
 
 
二、 建議 
        感謝行政院國家科學委員會提供出席國際會議的相關補助，讓我們能在國際研
討會之交流上增廣見聞，並能降低經濟負擔。希望未來能持續提供相關補助方案以
供國內學者出國交流機會，增進台灣的國際觀。 
三、 攜回資料名稱及內容 
ICMLC 2010 大會議程手冊、論文集各一本、及隨身碟一片。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
Fig. 1. Relationships between sensing data, 
sensing information, and driving state. 
To represent driving events, graphical models, 
such as finite-state machines, hidden Markov 
models [HMM], Petri nets, and neural networks, 
have been well known to be elegant tools for 
describing time-series events. Helander [3] 
presented a two-state automaton to explore the 
relationship between driver braking behavior and 
electrodermal response. The two states of the 
proposed automaton represent braking and non-
braking actions, respectively. Pentland and Liu [7] 
reported a four-state HMM to infer the driver’s 
internal intention from his/her external 
movements of eyes, head, hands, and legs. 
Internal intentions are forwarded to a control 
system to adapt the remainders of driving actions 
to achieve optimal performances. In a similar 
vein, Kuge et al. [4] suggested a HMM model to 
detect lane change maneuver while driving. 
Mitrovi´c [5] introduced an assemble of HMMs, 
each of which recognizes one of the seven events 
of driving along a left/right curve, turning 
left/right on an intersection, with/without 
roundabout, and driving straight across an 
intersection with a roundabout. Each observation 
sequence is evaluated by the seven HMMs in 
parallel and the driving event with the highest 
probability is decided. 
Oliver and Pentland [6] extended the 
traditional HMM to a coupled HMM (CHMM) 
for recognizing the driving maneuvers of passing, 
changing lanes, turning, starting, and stopping. 
The CHMM differs from traditional HMMs by 
incorporating contextual information when 
modeling driver behaviors. Dagli et al. [1] 
reported a dynamic belief network (DBN) 
capable of interpreting six driver-intended actions: 
acceleration/deceleration, remaining longitudinal/ 
lateral, and lane-change right/left. The DBN 
evolving with time illustrates the causal 
dependencies between vehicle states. Vehicle 
states are described in terms of probabilistic 
variables, whose distributions are adjusted by an 
inference engine.  
In this study, we have improved a previous 
work [2] of the driving event prediction system. 
The weighting driving relational map (WDRM) 
in our previous paper has been replaced by the 
attributed driving relational map (ADRM). Both 
nodes and links are redefined to associate with 
attribute values characterizing the danger levels 
of driving conditions and their relationships. The 
membership functions of the previous, current, 
and following sets of a node for map matching 
are also redefined in this paper. Moreover, an 
activation mechanism and a novel learning 
process are proposed to increase the completeness 
of the prediction model. 
2. Dangerous driving event prediction system 
Figure 2 shows a flowchart for the 
proposed dangerous driving event prediction 
system. Considering an input sequence of 
driving conditions, the system initiates locating 
a driving event on the input sequence once a 
new driving condition with a high danger level 
is present. The new driving condition serves as 
the terminal condition of the driving event. To 
determine the starting driving condition of the 
event, we associate each incoming driving 
condition with an activation governed by an 
activation mechanism. With this mechanism, 
every incoming driving condition initially 
arises its activation for the duration of the 
driving condition. Thereafter, the activation 
  
effect. As a consequence, the maximum 
activation that the driving condition can gain is 
(1 )pdqa v e
p
−= − . If d is long enough, a approaches 
the equilibrium value /qv p . After duration d, v is 
no longer in effect. The activation gradually 
diminishes. Both the arise and decay times of the 
activation are about 1/p. In this study, it is 
desirable that the decay time can be longer than 
the arise time. We hence rewrite Equation (1) as 
( ) ( ( ) ( )),a t f pa t qg v= − +                       (2) 
in which functions ( )f ⋅  and ( )g ⋅  are defined as 
     if  0      if  0
( ) ,         ( ) ,
   if  0 0     if  0
x x x x
f x g x
cx x x
> >⎧ ⎧= =⎨ ⎨≤ ≤⎩ ⎩
 
where 0 1c< < . As a result, the arise time remains 
1/p, whereas the decay time becomes 1/pc. Since 
0 1c< < , the decay time is longer than the arise 
time. 
With the above activation mechanism, we 
determine the starting condition of a driving 
event by looking for the first driving condition 
from the beginning of the input sequence, 
which has its activation larger than a threshold. 
3.2. ADRM construction 
In this study, we introduce attributed 
driving relational maps (ADRM) to represent 
driving events. Figure 4 shows the configuration 
of an ADRM, in which nodes corresponding to 
driving conditions are interconnected according 
to their orders of occurrence. The horizontal line 
below the map specifies the time axis. The time 
intervals, 
1i i it t t −Δ = − , 2,  ,  i n= ⋅⋅⋅ , are not necessarily 
equal. 
 
Fig. 4. Configuration of ADRM. 
Recall that a driving condition is resulting 
from an alteration of the driving state. Any 
change in the driving state somehow involves risk. 
In addition, the relationship between two driving 
conditions could incur danger as well. In this 
section, we define the danger levels of driving 
conditions, relationships, and driving events. 
Considering a driving condition, the 
information available with the driving condition 
includes its attribute s, attribute value v, duration 
d, and activation a. Since different attributes have 
distinct ranges of attribute values, we transform 
attribute value v according to attribute s into the 
interval of [-1, 1] by ( )sS v , where ( )sS ⋅  is the 
sigmoid function defined as 1( ) 2(1 ) 1svsS v e− −= + −λ , in 
which 
sλ  characterized by attribute s is a positive 
constant. 
We then define the danger level lc of the 
driving condition as ( )sc
C
aS v
l S
d
⎛ ⎞= ⎜ ⎟⎝ ⎠
, where 
1( ) (1 )C xCS x e
− −= + λ  forces cl  between 0 and 1. The 
above definition states that the larger the 
activation a, attribute value v, and the shorter the 
duration d the higher the danger level of the 
driving condition. All the above cases contribute 
to the danger level of a driving condition. 
Moreover, we define the danger level lr of 
relationship R as sgn( )(2 )p f
c
f fv vr
R c
p p
l n
l S
l n t
−= ⋅ Δ
, where 
1( ) (1 )R xRS x e
− −= + λ  forces rl  between 0 and 1 and np 
and nf are the numbers of driving conditions 
occurring at times tp and tf, respectively. 
Having defined the danger levels 
1 2{ , , , }c
c c c c
nL l l l= ⋅⋅⋅  of nodes 1 2{ , , , }cnV v v v= ⋅⋅⋅  and the 
danger levels 1 2{ , , , }rr r r rnL l l l= ⋅⋅⋅  of links 
1 2{ , , , }rnE e e e= ⋅⋅⋅  of an ADRM ( , , , )c rM V L E L , we 
define the danger level of the ADRM in the 
following. Let link ej connect nodes vi and vk, 
whose occurrence times are ti and tk, respectively. 
We determine the occurrence time of link ej as 
  
( ) { }O On n
c
kS k S
n lμ ∀ ∈′ = ∧ .                                (6) 
Note that template map T is itself coming 
from an observation map due to its approval of a 
predefined danger criterion. Let TpnS , TnS , and TfnS  be 
the fuzzy versions of TpnS , TnS , and TfnS . The 
membership functions of any node n’ in T 
belonging to TpnS , TnS , and TfnS  are defined similar to 
Equations (4), (5) and (6).  
Let n be a node in map O, and n’ be a node in 
map T with the same attribute as node n. The 
match degree of these two nodes is defined as 
| | | || |1( , ) ( )
3 | | | | | |
O T O TO T
p n p n fn fnn n
O T O T O T
p n p n n n fn fn
S S S SS Sn n
S S S S S S
′ ′′
′ ′ ′
∧ ∧∧′ = + +∨ ∨ ∨
    
     ψ , (7) 
where |‧| represents the fuzzy scalar 
cardinality and ∨  indicates fuzzy union. The 
match degree between maps O and T can be 
defined as 
1 1( , ) { ( , ), ( , )}
T O
n nn S n Sn O n To t
O T n n n n
n n ′′∀ ∈ ∀ ∈′∀ ∈ ∀ ∈
′ ′Ψ = ∧ ∨ ∨∑ ∑ ψ ψ , (8) 
where T
nS  is the set including all the nodes in map 
T having the same attribute as node n, and O
nS ′  is 
the set including all the nodes in map O having 
the same attribute as node n′ . Equation (8) 
formulates the fuzzy attributed map matching 
process. 
Given an observation map O, the prediction 
process proceeds as follows.  
1. For each template map T in database B, 
calculate the match degree ( , )O TΨ  between O 
and T by Equation (8); 
2. Find the template map *T  in B, which has the 
maximum match degree with the observation 
map O, i.e., *
 in 
arg max { ( , )}
T
T O T= Ψ
B
; 
3. If *( , )O TΨ  is large enough, report the 
dangerous driving event associated with *T  and 
return;  
4. Otherwise, calculate the danger level L of 
map O by Equation (3); 
5. If L is large enough, store O and the 
associated driving event in database B and 
return. 
4. Experimental results 
In the first experiment, we examine the 
rationalities of the calculated danger levels of 
driving conditions, relationships between driving 
conditions, and driving events. Referring to Table 
1, let us consider the following scenario of a 
dangerous driving event. The event starts with the 
host vehicle entering a curved road with the 
counterclockwise curvature of 15 degrees/meter 
(driving condition C7 with attribute value c = 
15 degrees/meter). After 2.5 minutes of driving, 
the host vehicle approaches a left-front vehicle 
at the rate of 5 km/hr (C1 with dlf = -5 km/hr). 
The host vehicle further speeds up at the rate of 
10 km/hr (C6 with s = 10 km/hr) in order to 
overtake the vehicle. In the meantime, the driver 
turns the steering wheel to the left by 0.14 radian 
(C5 with w = 0.14 radian) due to the curved road. 
The above two actions take about 1.1 minutes to 
complete and result in the approach rate of 17 
km/hr (C1 with dlf = -17 km/hr) to the left-front 
vehicle. At this moment, a descending rate of 
0.37 of the driver’s alertness level (C9 with l = -
0.37) is reported. It is probable that the driver 
under a low alertness level overlooks a downhill 
ahead (C8 with r = -12 degrees) and still keeps 
speeding up (C6 with s = 5 km/hr). The host 
vehicle soon after 0.1 minute collides with the 
left-front vehicle. 
Tab. 1: The driving conditions considered in the 
experiments. 
Label Attribute Para. Driving Condition 
C1 distance dlf 
Approaching/departing the left-
front vehicle 
C2 distance drf 
Approaching/departing the right-
front vehicle 
C3 distance dlr 
Approaching/departing the left-
rear vehicle 
C4 distance drr 
Approaching/departing the right-
rear vehicle 
  
5. Concluding Remarks and Future Work 
In this paper, a template-based dangerous 
driving event prediction system was presented. 
There are three major tasks involved in the 
system: perception, representation, and 
interpretation of driving events. Accordingly, 
we developed three techniques: an activation 
mechanism, an attributed driving relational map 
(ADRM), and a fuzzy attributed map matching 
(FAMM) approach, to deal with the above three 
tasks, respectively. The FAMM applied to such 
maps has revealed its efficiency in experiments. 
Moreover, the FAMM conceptualized in terms 
of fuzzy set disciplines tolerates to some extent 
the uncertainties due to the imprecision of 
measurements, the inadequate perception of 
driving events, and the imperfection of map 
construction. On the other hand, both the map 
configurations and attribute values of nodes and 
links are considered by the FAMM during map 
matching. 
Acknowledgements 
This work was supported by the National 
Science Council, Republic of China, under 
Contract NSC-98-2221- E-003-014-MY2 and 
NSC 98-2631-S-003-002. 
References 
[1] I. Dagli, M. Brost, and G. Breuel, “Action 
Recognition and Prediction for Driver 
Assistance Systems Using Dynamic Belief 
Networks,” in Agent Technologies, 
Infrastructures, Tools, and Applications for 
E-Services, Eds. By R. Kowalczyk, J. P. 
Muller, H. Tianeld, and R. Unland, Lecture 
Notes in Computer Science, vol. 2592, pp. 
179-194, Springer, 2002. 
[2] C.Y. Fang, P. Y. Wu, S. L. Chang, and S. W. 
Chen, “Danger Prediction by Case-Based 
Approach on Expressways,” Proceedings of 
the 11th international IEEE Conference on 
Intelligent Transportation System, Beijing, 
China, pp. 1118-1123, 2008. 
[3] M. Helander. “Applicability of Drivers' 
Electrodermal Response to the Design of the 
Traffic Environment,” Journal of Applied 
Psychology, vol. 63, no. 4, pp. 481-488, 
1978.  
[4] N. Kuge, T. Yamamura, and O. Shimoyama, 
“A Driver Behavior Recognition Method 
Based on a Driver Model Framework,” 
Society of Automotive Engineering 
Publication, 1998. 
[5] D. Mitrovi´c, “Reliable Method for Driving 
Events Recognition,” IEEE Trans. on 
Intelligent Transportation Systems, vol. 6, 
no. 2, pp.198-205, 2005. 
[6] N. Oliver and A.P. Pentland, “Graphical 
Models for Driver Behavior Recognition in a 
Smartcar,” Proc. of the IEEE Intelligent 
Vehicles Symp. Dearborn, Michigan, pp. 7-
12, 2000.  
[7] A. Pentland and A. Liu, “Toward 
Augmented Control Systems,” Proc. of the 
IEEE Conf. on Intelligent Transportation 
Systems, Detroit, Michigan, pp. 350-355, 
1995. 
 
96年度專題研究計畫研究成果彙整表 
計畫主持人：陳世旺 計畫編號：96-2221-E-003-010-MY3 
計畫名稱：視覺式駕駛安全輔助系統--駕駛者眼睛的眨眼偵測子系統 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 2 100%  
博士生 0 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
