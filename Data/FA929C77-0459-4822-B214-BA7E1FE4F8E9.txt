一、計畫中文摘要
隨著個人化數位相機與數位攝影機的普及，在旅遊過程中以數位資料留下旅遊經驗已經
成為最普遍的應用方式。數位工具的簡便讓使用者易於分享交流以及儲存旅遊經驗。常見
的旅遊經驗以數位照片、視訊、語音、出遊前自行準備或旅行社提供的文字旅遊行程、旅
遊過後的遊記或觀光局提供的景點描述或地圖等等方式儲存下來。面臨這些龐雜的數位資
料，如何有效率地管理這些資料，甚至將這些資料整合呈現以增加其保留價值已經成為重
要的課題。
本計畫預計針對旅遊中的數位照片與視訊資料進行組織與呈現。我們分別對其提出新穎
的想法以建立一個實用且有趣的旅遊經驗組織與管理系統。在此系統內，本計畫預計以兩
年的執行期間建置以下多個模組：
在組織方面：
(1) 影像分群與代表性照片選取(image clustering and representative image selection)
(2) 影像自動註解 (automatic image annotation) 與類重複偵測 (near duplicate detection)
(3) 場景偵測與場景切換圖(scene transition graph)
(4) 基於瀏覽景點的視訊摘要 (spot-based video summarization)
在呈現方面主要以多模式呈現 (multimodality presentation)為研究目標。將各種旅遊經驗
資料，包括旅行社或自行準備的行程、旅遊觀光局提供的景點介紹或地圖、旅遊過程中記
錄下來的影像、視訊、語音等。這些不同的資料可藉由相互的整合呈現出多種不同風貌。
我們另外將延伸以往關於場景切換圖的研究，將之延伸為一種視訊資料的呈現方式。
關鍵詞：自動影像註解、場景切換圖、多模式呈現
二、計畫英文摘要
As the increasing popularity of personal digital camera and digital camcorder, recording
everything during trips in digital forms has been the most popular way to store travel experience.
The ease of using digitizing tools facilitates users to share, communicate, and store travel
experience. The most popular data for travel experience are digital photos, videos, speech, the
schedule prepared by users or provided by the travel agency, the travel notes written by travelers,
and the documents or maps provided by the tourism bureau. Facing this massive and complex
digital data, how to efficiently manage and effectively present so that the values of travel
experience can be well preserved has been a very important issue.
This project focuses on the organization and presentation for digital photos and videos
captured during trips. We respectively propose some novel ideas to build a practical and
interesting travel experience organization and management system. In this system, we subject to
develop the following modules:
From the perspective of organization
(1) image clustering and representative image selection
(2) automatic image annotation and near duplicate detection
(3) scene detection and scene transition graph
(4) spot-based video summarization
四、研究目的
本計畫的研究目的在於處理旅遊過程中拍攝的影像及視訊資料，自動進行組織與整合性
呈現。在旅遊經驗資料方面分成影像(相片)與視訊(在相關文獻中屬於家庭影片，Home
video，的一種)，在技術模組部份分成組織(organization)與呈現(presentation)兩部份。
在旅遊媒體(travel media)的組織方面，我們希望利用照片與影片的內容關聯性來輔助完
成場景偵測(video scene detection)以及照片跟影片的摘要(photo summarization 與 video
summarization)。因旅遊影片及照片常常包含類似的內容，我們利用此項特徵建立出兩媒體
間的跨媒體關聯性(cross-media correlation)，並根據建立方法的不同，另外將跨媒體關聯性
區分成全域跨媒體關聯性 (global cross-media correlation)及區域跨媒體關聯性 (local
cross-media correlation)。全域跨媒體關聯性的特點，在於它連接了兩媒體間視覺概念(concept)
相似的部分。我們利用照片內的時間資訊偵測照片的場景位置，並搭配全域跨媒體關聯性
完成影片的場景偵測。此外，區域跨媒體關聯性所連接的不只是概念相似的圖片，而是兩
媒體內容中包含同一物件的部份。我們在此假設當某物件同時出現在影片及照片當中，則
表示包含此物件的影片或照片相對重要。因此我們利用區域跨媒體的特性找出重要性較高
的影片片段及照片，來完成影片及照片摘要。在最後的實驗部分，我們分別呈現運用跨媒
體關聯性於場景分析、影片摘要以及照片摘要的效能結果，進一步證實了跨媒體關聯性的
確有助於旅遊影片分析。
在旅遊媒體(travel media)的呈現方面，我們希望利用類重複偵測(near-duplicate detection)
技術自動選取每個景點最具有代表性的照片，且在代表性照片中找出最具代表性的區域，
亦即興趣區(region of interest，ROI)。我們延伸既有的類重複偵測技術於新的應用中，並且
提出一個特徵分類(feature classification)的方法以提高類重複偵測的準確性。為了過濾特徵
點匹配時所產生的錯誤結果，我們提出三個特徵點過濾的方法，包括機率式潛在語意分析
(probability latent semantic analysis)、單一特徵點和區域特徵點過濾。這三種過濾方法將幫
助我們過濾落於自然景觀的特徵點(即雜訊)。對於一組使用者在相同景點所拍攝的旅遊照
片，為了決定任兩張照片內容是否相似，我們建構出一個支持向量機(support vector machine)
分類器將匹配圖案模組化。接著利用圖(graph)來描述近似照片之間的關係，並且選定擁有
最大中心性數值(centrality value)的照片為代表性照片。由於被匹配特徵點大部分落於重要
物件的內部或輪廓，因此將包含所有被匹配特徵點的區域當作照片的興趣區即可充分描述
一張照片內所要表達的內容。
五、研究方法
(一)旅遊媒體場景偵測與摘要
1. 全域跨媒體關聯性(Global Cross-Media Correlation)
為了得到照片場景偵測結果，我們利用照片的拍攝時間資訊。當某張照片 p 與前一張照
片 p-1 之間的拍攝時間差大於前後數張照片間的拍攝時間差的平均，代表 p 與 p-1 之間發生
了場景切換。之後藉由找出所有場景切換位置，來完成照片場景偵測。
接著我們利用袋字模型(bag of word models)的概念，首先利用非監督式的 global k-means
分群演算法，並依據鏡頭內所有影格的 HSV 色彩統計直方圖自動地決定單一鏡頭內最適合
的關鍵影格數量，在分群後選取每個群組內最靠近組中心點的影格當作關鍵影格。為了避
免某些模糊關鍵影格影響到最後的實驗結果，我們根據每張關鍵影格在不同解析度下的邊
程中往往會在旅遊景點中具有代表性的地點時同時使用數位相機和數位攝影機拍攝一些較
為重要的資訊，所以有許多相似的內容同時存在於影片及照片中。於是我們利用旅遊影片
和照片之間的區域跨媒體關聯性，當某段影片跟某張照片互相比對到，代表其皆拍攝於相
同地點並擁有相似的內容及物件時，我們則定義此段影片跟照片相對於其他拍攝內容來的
較為重要，並根據每段鏡頭的重要性及使用者想要的摘要長度來進行重要片段的選擇。
(二)代表性照片選取與興趣區偵測
圖 2 為系統整體架構，之後將對每一個部件做簡短介紹。
圖 2、系統整體架構。
1. 特徵擷取(Feature extraction)
這個系統主要針對使用者在某一特定景點及景點周遭所拍攝的旅遊照片。我們認為某一
景點的重要地標必定會時常出現於所拍攝的照片，因此希望藉由近似偵測來幫助代表性照
片的挑選。對於一組給定的旅遊照片，我們利用 Lowe 所提出的 SIFT(scale-invariant feature
transform)演算法[11]來偵測及描述特徵點。給定一張照片，首先使用 DoG(difference of
Gaussian)偵測器來偵測照片中的區域特徵點。接著利用一個 128 維的方向直方圖來描述每
一個特徵點。
2. 特徵過濾(Feature Filtering)
事實上，並非所有特徵點扮演正面的影響。雖然特徵點確實在顯著的位置上呈現不同的
特性，像是建築邊角和葉子尖端。但相較於落於自然景觀的特徵點，落於人造建築的特徵
點對於近似偵測提供重大的線索來判斷兩張照片是否近似。因此，我們希望過濾落於自然
景觀的特徵點(即雜訊)來減少特徵點匹配時所產生的錯誤匹配，進而提升近似偵測的準確
率。
2.1 機率式潛在語意分析過濾(PLSA-based filtering)
PLSA 演算法[12]傳統用於文章的分類。此方法的主要精神是將影像視為視覺化的文
章。我們各別收集人造和自然特徵點來建造兩本視覺字典(visual dictionary)，對於每一個特
徵點我們將找到其相對應的人造視覺字(artificial visual word)跟自然視覺字(natural visual
word)。接著建構兩個 PLSA 模組(人造和自然)，每一個特徵點分別利用人造模組及自然模
我們利用圖 3 來說明如何進行代表性照片挑選。在先前步驟中，我們將照片兩兩進行近似偵測，
因而得知照片之間是否擁有近似關係。我們利用社會網路分析的概念，將相似照片之間的關係用一
個無向圖來表示，如圖 3 所示。接著我們計算每一張照片在圖中的中心性數值，主要是採用程度中
心性(degree centrality)。根據每一張照片的連線個數來決定。圖 3 中，第二張照片擁有最大的中心
性數值(即 3)，因此我們選定這張照片為代表性照片。
4.2 興趣區判定
由於近似偵測是根據影像中人造特徵點匹配。因此代表性照片中被匹配的特徵點，常常落於重
要物件的內部或輪廓，如圖 4 的第一列所示。我們藉由被匹配特徵點的空間分佈資訊，幫助決定代
表性照片的興趣區(ROI)，如圖 4 的第二列所示。
圖 3、近似照片之間的關係。 圖 4、被匹配特徵點的空間分佈。
六、研究成果
(一)論文發表
此計畫相關論文發表如下：
 W.-T. Chu, C.-H. Lin, and J.-Y. Yu, “Feature Classification for Representative Photo 
Selection,” accepted by ACM Multimedia Conference, 2009.
 W.-T. Chu, Y.-L. Lee, and J.-Y. Yu, “Visual Language Model for Face Clustering in 
Consumer Photos,” accepted by ACM Multimedia Conference, 2009.
 W.-T. Chu, C.-C. Lin, and J.-Y. Yu, “Using Cross-Media Correlation for Scene Detection in
Travel Videos,” Proceedings of ACM International Conference on Image and Video 
Retrieval, 2009.
 W.-T. Chu, Y.-L. Lee, and J-.Y. Yu, “Using Context Information and Local Feature Points in 
Face Clustering for Consumer Photos,” Proceedings of IEEE International Conference on
Acoustics, Speech, and Signal Processing, pp. 1141-1144, 2009.
 W.-T. Chu, C.-T. Hung, and J.-J. Yu, “Object Segmentation Based on Common Information 
between Images,” accepted by the 22th Computer Vision, Graphics, and Image Processing 
Conference, 2009.
 W.-T. Chu and C.-H. Lin, “Automatic Selection of Representative Photo and Smart
Thumbnailing Using Near-Duplicate Detection,” Proceedings of ACM Multimedia
Conference, pp. 829-832, 2008.
7 所示，x 軸代表我們所使用的七組實驗資料，y 軸代表自動摘要結果和摘要標準內容相同
的圖片數量與摘要標準總數量的比率(accuracy)：
圖 6、照片摘要結果。
圖 7、影片摘要結果
由於資料 2 內影片及照片內容重覆的地方太少，且影像內容充斥著大量自然景觀，使
得擁有相同內容的影片及照片彼此難以相互比對到，造成不佳的實驗結果。除此之外，我
們的方法在其他資料中都得到相當不錯的結果，影片摘要及照片摘要中的最佳實驗結果皆
趨近於或甚至達到 0.8 以上。
(三)代表性照片選取與興趣區偵測
 特徵點過濾之成效
為了計算不同特徵點過濾方法的成效，我們各別收集 3182 人造特徵點和 5173 自然特徵點當作
實況資料(ground truth)。接著將這組資料各別利用三種不同的過濾方法(即 PLSA-based、Point-based
和Region-based)進行過濾。最後過濾準確率分別為PLSA-based 0.26、Point-based 0.81和Region-based
0.92。我們可以清楚地發現區域特徵點過濾擁有最高的過濾準確率，而 PLSA 為主過濾方法的結果
為最差。
 代表性照片挑選之成效
判定代表性照片挑選的成效是相當主觀的，因此我們邀請九位受測者給代表性照片評分。分數
範圍為 1 分到 5 分，5 分表示所挑選的代表性照片明確代表此景點。我們共收集 52 組在不同景點所
拍攝的旅遊照片，照片數量為 1024 張。每組旅遊照片藉由不同的特徵點過濾方法會個別挑選一張
代表性照片。我們將全部的評分結果平均，最後評分結果分別為 PLSA-based 3.32、Point-based 3.61
和 Region-based 3.63。從之前的特徵點過濾結果，我們得知 Region-based 方法最佳，而 PLSA-based
方法最差。因此我們由評分結果可得知有效的特徵點過濾方法有助於代表性照片挑選。
[3] Adams, B., Dorai, C., and Venkatesh, S. Toward automatic extraction and expression of
expressive elements from motion pictures: tempo. IEEE Transactions on Multimedia, vol. 4,
no. 4, 2002, 472-481.
[4] Vendrig, J. and Worring, M. Systematic evaluation of logical story unit segmentation. IEEE
Transactions on Multimedia, vol. 4, no. 4, 2002, 492-499.
[5] Li, Y., Lee, S.-H., Yeh, C.-H., and Kuo, C.-C. J. Techniques for movie content analysis and
skimming: tutorial and overview on video abstraction techniques. IEEE Signal Processing
Magazine, vol. 23, no. 2, 2006, 79-89.
[6] Chen, H.-W., Kuo, J.-H., Chu, W.-T., and Wu, J.-L. Action movies segmentation and
summarization based on tempo analysis. In Proceedings of the ACM SIGMM International
Workshop on Multimedia Information Retrieval, 2004, 251-258.
[7] Ahern, S., Naaman, M., Nair, R., Yang, J. World explorer: visualizing aggregate data from
unstructured text in geo-referenced collections. In Proceedings of International Conference
on Digital Libraries, 2007, 1-10.
[8] Snavely, N., Seitz, S.M., and Szeliski, R. Photo tourism: exploring photo collections in 3D.
ACM Transactions on Graphics, vol. 25, no. 3, 2006, 835-846.
[9] Chen, J.-C., Chu, W.-T., Kuo, J.-H., Weng, C.-Y., and Wu, J.-L. Tiling slideshow. In
Proceedings of ACM Multimedia Conference, 2006, 25-34.
[10]Cemerlang, P., Lim, J.-H., You, Y., Zhang, J., and Chevallet, J.-P. Towards automatic mobile
blogging. In Proceedings of IEEE International Conference on Multimedia and Expo, 2006,
2033-2036.
[11]Lowe, D. distinctive image feature from scale-invariant keypoints. International Journal of
Computer Vision, vol. 60, no. 2, 2004, 91-110.
[12]Hofmann, T. Unsupervised learning by probabilistic latent semantic analysis. Machine
Learning, vol. 42, no. 1-2, 2001, 177-196.
[13]Zhao, W.-L., Ngo, C.-W., Tan, H.-K., and Wu, X. Near-duplicate keyframe identification
with interest point matching and pattern learning. IEEE Transactions on Multimedia, vol. 9,
no. 5, 2007, 1037-1048.
[14]Sivic, J. and Zisserman, A. Video google: a text retrieval approach to object matching in
videos. Proceedings of the International Conference on Computer Vision, vol. 2, 2003,
1470-1477.
可利用之產業
及
可開發之產品
影像管理軟體、圖片搜尋引擎
技術特點
根據旅遊媒體的特性，我們將類重複偵測技術用於兩個傳統但重要
的問題中─代表性圖片偵測與興趣區偵測。我們可使用任一個類重
複偵測技術將照片之間的關係轉換成圖(graph)，並利用社群網路分
析(social network analysis)技術決定每張照片的重要性。
推廣及運用的價值
由於旅遊風氣盛行且數位相機普及，再加上網路相簿的便利與傳
播，大量的旅遊照片亟需有效的管理與瀏覽方式。本技術可應用於
此類媒體，確實符合使用者需求與相關產業發展。此外，我們在相
關論文中也提到此技術適合用於重新排列(re-ranking)圖片搜尋引
擎的搜尋結果，讓更重要的結果順位能更前面。
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位研
發成果推廣單位（如技術移轉中心）。
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
※ 3.本表若不敷使用，請自行影印使用。
around there, pedestrians, or something that is not directly related 
to this scenic spot.  
Figure 1 shows the content variations in the photos taken in the 
famous Rokuonji temple in Kyoto. From this example and many 
other web-based albums, we found that most travelers incline to 
take the landmark or famous views several times. Moreover, 
tourists usually take photos at some specific locations such that 
they can capture the canonical view as that in the postal card. 
According to these observations, we propose that we can 
approach the selection of representative photo based on near-
duplicate detection, which finds the near-duplicate pairs like the 
fifth to the eighth photos in Figure 1.  
(1) (2) (3)
(4) (5) (6)
(7) (8) (9)  
Figure 1. Photos taken around the same scenic spot.  
Applications of near-duplicate detection (NDD) have been 
proposed for many different purposes, such as sub-image retrieval 
[2] and automatic image annotation [4]. In various near-
duplication detection approaches, local image descriptors that 
capture the salient characteristics over different image scales are 
widely used. Among different descriptors, Lowe’s SIFT (scale-
invariant feature transform) feature [5] has been demonstrated to 
have the best performance and is used in this work.  
We exploit the SIFT-based NDD method proposed by Zhao et al. 
[6]. This method largely reduces the false alarms caused by 
conventional nearest-neighbor matching approaches and increases 
the matching speed with a multidimensional index structure. 
Moreover, as the near-duplicate photos are often highly localized 
and spatially smooth, the correspondence of SIFT matched points 
have coherent patterns, which can be modeled by support vector 
machines (SVMs). This method obtains good balance between 
matching speed and matching accuracy.  
2.2 Near-Duplication Detection Process 
Given a set of photos that are clustered 
together by using the time-based clustering method [1], we 
determine whether a pair of photos , is 
near-duplicate by the following steps, as illustrated in Figure 2.  
z SIFT-based matching: for any pair of photos in this cluster, 
the method in [6] that embeds a one-to-one symmetric 
criterion to filter out false matches is applied. Figure 3(b) 
shows the effectiveness of false alarms reduction, as 
compared to a conventional approach (Figure 3(a)).  
z Orientation feature extraction: due to the characteristics of 
local coherence and spatial smoothness, the orientation of the 
link connecting matched points in two photos are similar. We 
calculate the orientation of links and quantize it into 36 levels. 
A 36-bin orientation histogram is then constructed. In near-
duplicate pairs, the values of the orientation histogram would 
apparently concentrate.  
z SVM-based determination model: a SVM is used to model the 
characteristics of the orientation histogram. We estimate the 
model parameters based on 40 near-duplicate pairs and non-
near-duplicate pairs. At the test stage, we make a binary 
decision on each photo pair based on the SVM classifier.  
SIFT-based 
matching
Orientation 
feature 
extraction
SVM-based 
determination 
model
Near-
duplicate 
pairs
 
Figure 2. The process of near-duplicate detection.  
(a)
(b)
 
Figure 3. Sample results of (a) conventional SIFT-based matching 
and (b) one-to-one symmetric SIFT-based matching.  
2.3 Sub-Clustering Before Matching 
One of the critical issues in NDD is that there are tremendous 
pairs of photos should be examined. For example, if there are N 
photos in a set, totally  different pairs of photo are needed to 
be checked. To reduce the complexity, we further cluster the 
given set of photos based on content-based characteristics. We 
then perform NDD for each sub-cluster, i.e., any two photos that 
are in different sub-clusters would not be examined.  
Because the representative landmark or view would have similar 
appearance, we can reasonably assume that they would be 
categorized in the same sub-cluster. For example, if the set of N 
photos are categorized into M sub-clusters , the 
total number of pairs for NDD is 
, (1) 
where  is the number of photos in the ith sub-cluster. In the 
case of N = 10, M=2, , and , we need originally 
need to check  photo pairs. However, we only have to 
evaluate   photo pairs if we perform sub-clustering 
first. In this work, the sub-clustering process is implemented 
based on RGB histograms of photos.  
3. REPRESENTATIVE SELECTION 
With loss of generality, assume that the sub-cluster  in the set 
 contains the near-duplicate photos, i.e., the 
830
if the observer thinks a photo better represents a scenic spot. To 
spread out the scores, observers were asked to give five or one to 
at least and only one of the near-duplicate photos.  For each photo, 
the degree of representative is calculated by averaging the scores 
from observers.  
The performance of selection is measured by the corresponding 
score of the selected photo. The automatic selection method 
obtains higher score when the selected photo better matches 
human’s judgments. Due to the space limitation, Table 1 briefly 
lists the performance for five photo clusters, and the bottom row 
shows the overall performance for 47 photo clusters. It is not 
surprising that the performance would vary in different cases. 
Overall, the selection performance is satisfactory.  
We also show the variance of human judgments. In some photo 
cluster, there would be many different views for the landmarks. 
Different observers would have varied preference in selecting the 
most representative view. There is a trend that in the case of 
larger judgment variation, the performance of selection 
correspondingly degrades.  
Table 1. Performance of representative selection.  
Scenic spot Score Variance of score 
Notre Dame 3.14 0.14 
Statue of Liberty 4.86 0.14 
Space Needle 3.86 0.17 
Niagara Fall 3.28 0.57 
Gold Gate Bridge 2.29 2.90 
… … … 
Overall 3.21 0.75 
 
(a) (b)  
Figure 6. The results of ROI determination based on (a) filtered 
SIFT matched points and (b) saliency values.  
5.2 Performance of ROI Determination 
It is hardly to quantify the performance of ROI determination. 
Therefore, we compare the proposed method with the saliency-
based approach. We use the SaliencyToolbox [8] to generate the 
saliency map. On the basis of saliency values, the same method 
described in  Sec. 4 is used to determine the ROI.  
Figure 6 shows the comparison of the ROI determination results. 
The ROI determined based on filtered SIFT matched points is 
notably better. The reason is that the saliency-based approach 
only considers the contrast in color, intensity, and orientation. On 
the contrary, the features used in the proposed method are directly 
related to the region of interest, i.e., the near-duplicate object.  
5.3 Complexity Reduction 
Table 2 shows three examples about the number of photo pairs 
needed to be checked in NDD with and without the sub-clustering 
process described in Sec. 2.3. We can see that the times of NDD 
is largely reduced with this process. Note that the number of 
reduction depends on the content characteristics of a photo cluster. 
If photos in the same cluster have large variations, i.e., higher 
entropy, there may be more sub-clusters with similar sizes, and 
the number of reduction is larger.  
Table 2. Number of photo pairs needed for NDD.  
Scenic spot # photos in 
this cluster 
# pairs w.o. 
sub-clustering 
# pairs w. sub-
clustering 
Notre Dame 19 171 40 
Statue of 
Liberty 
24 276 14 
Rokuonji 15 105 48 
6. CONCLUSION 
With near-duplicate detection, we present automatic selection of 
representative photos and ROI determination. The relationships 
between near-duplicate photo pairs are described as a graph, and 
the representative photo is determined by checking the centrality 
value of each node. For the selected representative, the SIFT 
matched points are further used to locate the region of a landmark 
or a specific view. We design a scheme that not only quantifies 
the performance of the proposed selection method but also 
considers human’s subjective judgments. For ROI determination, 
we compare the proposed method with the saliency-based 
approaches to show its effectiveness.  
7. ACKNOWLEDGMENTS 
This work was supported by the National Science Council of the 
Republic of China under grants NSC 96-2218-E-194-005.  
8. REFERENCES 
[1] Platt, J.C., Czerwinski, M., and Field, B.A. 2003. PhotoTOC: 
automating clustering for browsing personal photographs. In 
Proc. of IEEE Pacific Rim Conference on Multimedia, 6-10.  
[2] Ke, Y., Sukthankar, R., and Huston, L. 2004. Efficient near-
duplicate detection and sub-image retrieval. In Proc. of ACM 
Multimedia, 869-876.  
[3] Zhang, D.-Q., and Chang, S.-F. 2004. Detecting image near-
duplicate by stochastic attributed relational graph matching 
with learning. In Proc. of ACM Multimedia, 877-884.  
[4] Wang, X.-J., Zhang, L., Jing, F., and Ma, W.-Y. 2006. 
AnnoSearch: image auto-annotation by search. In Proc. of 
CVPR, 1483-1490.  
[5] Lowe, D. 2004. Distinctive image features from scale-
invariant keypoints. IJCV, 60, 2, 91-110.  
[6] Zhao, W.-L., Ngo, C.-W., Tan, H,-K., and Wu, X. 2007. 
Near-duplicate keyframe identification with interest point 
matching and pattern learning. IEEE Trans. on Multimedia, 9, 
5, 1037-1048.  
[7] Itti, L., and Koch, C. 2001. Computational Modeling of 
Visual Attention, Nature Rev. Neuroscience, 2, 3, 194-203.  
[8] Walther, D., and Koch, C. 2006. Modeling attention to 
salient proto-objects. Neural Networks, 19, 1395-1407.  
 
832
module provides acceptable results, some objects that are
similar to face appearance would be mis-detected as faces.
Thus we further apply the eye detection process to the
detected faces, and filter out the ones that have no eye.
2.2. Eigenface
Following the standard eigenface approach, we project each
detected face region into the pre-trained eigenspace and
present each face region by the space¶s basis, i.e., eigenface.
The coefficients corresponding to this basis is concatenated
as a feature vector to describe a face region.
2.3. K-means clustering
Based on eigenface coefficients, we apply the k-means
algorithm to cluster face regions. Conceptually, the faces
that are grouped into the same cluster should correspond to
the same person. However, this method works badly in
consumer photos, where side-view faces or drastic lighting
conditions significantly harm the performance. To improve
the accuracy of face clustering, most works appeal to other
context information, such as clothes color [1][2] and group¶s
prior probability [3].
2.4. Re-clustering based on clothes information
People in the photos that were taken within a short duration
are assumed to be in the same dresses. Clothes information,
therefore, provides important clues to correlate faces. For
the faces that are grouped in the same cluster at the previous
stage, their corresponding clothes are further examined
based on clothes¶ color histograms. If a face¶s corresponding
clothes is significantly different from the average clothes
information of the same face cluster, this face is specially
selected to be re-assigned.
2.5. Re-clustering based on SIFT
Accurately finding clothes regions is still a challenging work.
Moreover, person¶s clothes are often occluded by other
persons in group photo. Therefore, we further extract SIFT
(scale-invariant feature transform) features [5] to represent
local feature points, and use them to compare detected faces.
We demonstrate that this feature works well in matching
faces and effectively enhance face clustering.
2.6. Postprocessing
It is obviously wrong if two or more faces in the same photo
are grouped in the same face cluster. They are re-examined
and surely assigned to different clusters in the post-
processing module.
Due to the space limitation, we would like to skip face
detection, eye detection, and the standard eigenface module,
which can be easily found in literature. In Sections 3 and 4,
we describe our major contributions ± re-clustering based on
clothes information and SIFT features.
Consumer
photos
Face
detection
Eigenspace
projection
K-means
clustering
eye
detection
Re-clustering
based on
clothes info.
Re-clustering
based on
SIFT
Face
clusters
Postprocessing
Figure 2. The proposed system framework.
3. RE-CLUSTERING BASED ON CLOTHES
INFORMATION
After k-means clustering based on eigenface coefficients, we
obtain face clusters, denoted by .
For each face cluster , , we select face images
whose corresponding clothes regions are significantly
different from the average characteristics of the same group
and re-cluster them. Because accurately segmenting each
face image¶s corresponding clothes region is still an on-
going research, we simply set the position and area of a
rectangle region according to the corresponding face image.
For a the ith face cluster , the
corresponding clothes regions are
extracted. A face image , , will be selected to be
re-clustered if
, (1)
where is the average clothes of the set ,
is the Euclidean distance of RGB color
histograms between and , and is the standard
deviation of the distances between clothes in .
The selected clothes regions are viewed as the ³outliers´
of the set . An outlier , which corresponds to the face
image , is re-assigned to the -th face cluster if
, (2)
where is the average clothes of the face cluster .
4. RE-CLUSTERING BASED ON SIFT
Although eigenface and clothes information provides helpful
clues for clustering face images, luminance changes and
pose variation often degrade the robustness of clustering.
Recently, local feature points are widely applied to match
images in different viewpoints, scales, and lighting
conditions. Therefore, we extract local feature points from
face regions, describe them by SIFT description, and
introduce them to the task of face clustering.
Figure 3(a) shows the SIFT-based matching situations
between the same persons. We can easily see that the
matched points distribute around the eyebrows, eyes, noses,
and mouths. This situation actually matches the cognition
theory [6], which describes how we understand and
distinguish people. We can also note that feature points can
be matched even two face images are in different lighting
conditions or in different poses. On the other hand, in Figure
1142
some examples that cause failed face detection. Such
extreme poses frequently occur in consumer photos.
precision
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
eigenface
eig+clothes
eig+cl+SIFT
Figure 5. Precision of face clustering in different datasets.
recall
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
eigenface
eig+clothes
eig+cl+SIFT
Figure 6. Recall of face clustering in different datasets.
Figure 7. Some examples that cause failed face detection.
In the task of face clustering, precision plays a more
important role than recall. Users are more likely to have
more small clusters, in each faces really belong to the same
person, than fewer large clusters, in each faces may belong
to different persons. Recently, Google¶s Picasa web album
[4] provides a function of automatic face clustering to
facilitate face annotation. It tends to provide many small
face clusters to users, and achieves high precision
performance. However, if a dataset actually contains k (e.g.,
four) different persons, Picasa often provides a much larger
number of clusters than k, e.g. twenty.
In our experiments, the actual number of face clusters is
decided by users and the proposed approach clusters faces
accordingly. To verify whether the precision values get
higher when we make more clusters, as Picasa does, we
evaluate precision values based on grouping face images
into k, k+1, k+2, and k+3 clusters. The value k
corresponding to each dataset is the actual number of
persons in it. Figure 8 shows the results in different
clustering situations. Overall, the average precision values
are 0.62, 0.62, 0.67, and 0.68. This result confirms the
tendency of more clusters, higher precision, and provides us
guidelines of designing a face-centric photo browsing
system.
precision
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
k
k+1
k+2
k+3
Figure 8. Precision in different clustering situations.
6. CONCLUSION
Face clustering for consumer photos is a challenging work
due to the data with extremely different characteristics
captured from uncontrolled environments. In this paper, we
introduce local feature points in face clustering. With the
help of number of SIFT matched points and their spatial
distribution, we can match faces in different poses or
lighting conditions. The experimental results demonstrate
the effectiveness of the proposed method, and further
confirm the design guidelines of a face clustering system. In
the future, we would investigate the fusion scheme of
different modules and compare different face clustering
methods based on the same dataset.
7. ACKNOWLEDGMENTS
This work was partially supported by the National Science
Council of the Republic of China under grants NSC 97-
2221-E-194-050.
8. REFERENCES
[1] L. Zhang, L. Chen, M. Li, and H.J. Zhang, ³Automated
annotation of human faces in family albums,´ Proceedings of
ACM Multimedia, 355-358, 2003.
[2] M. Zhao, Y.W. Teo, S. Liu, T.-S. Chua, and J. Ramesh,
³Automatic person annotation of family photo album,´
Proceedings of International Conference on Image and Video
Retrieval, 163-172, 2006.
[3] A.C. Gallagher and T. Chen, ³Using group prior to identify
people in consumer images,´ Proceedings of IEEE
International Conference on Computer Vision and Pattern
Recognition, 1-8, 2007.
[4] Picasa web albums, http://picasaweb.google.com
[5] D. Lowe, ³Distinctive image features from scale-invariant
keypoints,´ International Journal of Computer Vision, 60(2),
91-110, 2004.
[6] F. Nahm, A. Perret, D. Amaral, and T. Albright, ³How do
monkeys look at faces?´ Journal of Cognitive Neuroscience, 9,
611±623, 1997.
[7] The Gallagher Collection Person Dataset,
http://amp.ece.cmu.edu/people/andy/GallagherDataset.html
1144
clustering based on photos’shooting time. With these properties,
we can exploit cross-media correlation and the results of photo
clustering to achieve accurate scene detection for travel videos.
We design a matching scheme based on bags of visual words to
find the correspondence between two modalities. The reported
performance shows the superiority of the proposed scheme, and
further confirms the benefits of utilizing multi-modality context
information.
Contributions of this work are summarized as follows:
 We introduce the idea of using cross-media correlation to
perform scene detection. Cross-media alignment based on
approximate string matching has been designed to find the
optimal matching between two modalities.
 We perform comprehensive study about the performance of
the proposed scheme and conventional scene detection
approaches.
The rest of this paper is organized as follows. Section 2 reviews
related works. We describe the proposed cross-media alignment
framework in Section 3. To represent photos and video clips, bags
of visual words are extracted in Section 4. The scene detection
task is transformed into an approximating sequence matching
problem in Section 5. Section 6 givens experimental results and
discussion, and Section 7 concludes this paper.
2. Related Work
Because there is no standard benchmark and evaluation metrics
for home video analysis, studies in this field are diverse and rise
from different perspectives. We will briefly review the works on
home video structuring, automatic editing, browsing, and
intention analysis. Then, studies especially about scene detection
and cross-media correlation are reviewed as well.
Although there is no restriction in capturing home videos, Gatica-
Perez et al. [1] cluster video shots based on visual similarity,
duration, and temporal adjacency, and therefore find hierarchical
structure of videos. On the basis of motion information, Pan and
Ngo [2] decompose videos into snippets, which are then used to
index home videos. For the purpose of automatic editing,
temporal structure and music information are extracted, and
subsets of video shots are selected to generate highlights [3] or
MTV-style summaries [4]. Peng et al. [5] further take media
aesthetics and editing theory into account to perform home video
skimming. In [6], a system called Hyper-Hitchcock is developed
to semi-automatically edit videos and to facilitate hyperlink
properties. From the perspective of intention analysis on home
videos, [7] and [8] model user intention for video repurposing and
browsing.
Especially for scene detection, which is the main task of this paper,
Yeung and Yeo [9] proposed a classical work called scene
transition graph to describe relationships between video shots, and
achieved scene detection by analyzing links in the graph. For
movies and TV shows, Rasheed and Shah [10] developed a two-
pass algorithm based on motion, shot length, and color properties.
For the purpose of systematic evaluation, a method was proposed
in [11].
Most scene detection algorithms utilize convention in video
production, such as shot length in movies or TV shows, or visual
appearance in video capturing, such as motion and color
properties. However, there is no convention in producing home
videos, and motion and color information may make no sense in
scene detection. For example, drastic changes in motion activity
don’t imply scene changes, because motion may be caused by
hand shaking. Drastic color changes don’t necessarily imply scene
changes, because color may significantly change due to bad
lighting conditions or motion blur.
In this paper, we exploit multimodality correlation between
photos and videos captured in the same journey, and achieve
video scene detection with the aid of the results of image
clustering. Photos are often taken in much better quality, and
achieving accurate scene change detection is much easier than that
in corresponding video clips.
3. The Proposed Framework
3.1 Essence of the Idea
Due to low cost and popularity of video capturing devices, many
people used to capture travel experience by both cameras and
camcorders. This behavior is no longer limited to professional
photographers. Amateurs often do so by using digital cameras that
are able to capture videos, or even cell phones equipped with both
photo and video capturing functions. For example, to obtain high-
quality data, people often capture famous landmarks or human
faces by still cameras. To capture evolution of an event, people
prefer to record them by videos. The content in two modalities
often has high correlation. Therefore, the idea of this work is that
we want to utilize the correlation so that we can succeed the
works that are harder to be conducted in videos, but are easier to
be done in photos.
Scene detection for photos is much easier than that in videos. For
travel photos, we can accurately cluster photos taken at the same
place together by checking time information. As the example
illustrated in Figure 1, this journey can be separated into three
scenes based on photo clustering. Photos in the same cluster
usually present the content of the same scenic spot. To perform
scene detection in videos, we extract several keyframes for each
video shot, and find the optimal matching between photo and
keyframe sequences to find the correspondence between photo
scenes and video scenes. The essence of correlation between
travel photos and videos, and the elaborate design of visual-based
matching between image sequences constitute the major
contribution of this work.
Time
Photos
Video
Keyframes
Scene 1 Scene 2 Scene 3
Modality
Scene 1 Scene 2 Scene 3
Figure 1. The idea of scene detection based on cross-media
alignment.
the distribution of visual words in an image is described as a
normalized visual word histogram. Therefore, we finally
transform keyframes and photos as a sequence of normalized
visual word histograms, respectively.
The essence of this representation is that we view each image as a
document, and each document is composed of some basic visual
words. Each visual word conceptually represents a basic visual
element, such as a corner of a building, tips of a tower, tips of
leaves, and etc. We then estimate the similarity between two
images (documents) based on the distribution of visual elements.
5. Approximate Sequence Matching
5.1 Visual Word Histogram Matching
Because the content in videos and photos are not consistently the
same, we find the correspondence between two normalized visual
word sequences by approximate matching techniques. We find the
optimal correspondence between two sequences by determining
the longest common subsequence between them.
Given two visual word histogram sequences,
and , which are
corresponding to photos and keyframes, respectively. Each item in
these sequences is a visual word histogram, i.e., ,
, where is the number of visual words. The
longest common subsequence between two subsequences and
is described as follows.
(3)
where denotes the ith prefix of , i.e., ,
and denotes the length of the longest common
subsequence between and . This recursive structure
facilitates usage of the dynamic programming approach to find the
global optimal solution.
Based on visual word histograms, the equality in Eqn. (3) occurs
when the following criterion is met:
if , (4)
where and are the visual word histograms corresponding to
the images and . According to this measurement, if the
distribution of visual words is similar between a keyframe and a
photo, we claim that they are conceptually“common”and contain
similar content in images.
The algorithm mentioned above is a general sequence matching
formulation. However, it’s possible to design a formulation that
better fits the problem we meet. In a journey, we sequentially visit
different scenic spots, and take photos and videos in the same
time order. Because there is consistent time order in travel photos
and videos, we can impose time constraints in the matching
process.
Assume that there are scenes in the photo set
, where denotes the
number of photos in the scene . According to the proportion of
the number of a scene to that of all scenes, only the photo and
the keyframe meet the following time constraint are processed
in the matching algorithm.
Time constraint:
The photo and the order of the keyframe ranges from
to , where , and
there are totally photos and keyframes in the dataset. The
value is a tolerance parameter that slightly relaxes the
interpolation-based time constraint. In this work, the value of is
empirically set as the number of photo scenes.
With this idea, we reformulate the criterion defined in Eqn. (4) as
if
and the pair meets the time constraint. (5)
5.2 Postprocessing
Because the scene boundaries in photos have been determined by
the time-based clustering method, we can accordingly estimate the
scene boundaries in videos by utilizing the correspondence
between photo and keyframe sequences. For the keyframe , if it
is matched with a photo , denoted by , then the scene of
is determined as that corresponds to . If the keyframe
doesn’t match with any photo, find the two nearest keyframes
and that are matched with some photos. We check the
matched situations as follows.
1) If and , and both and belong
to the same scene , then is claimed to be in the scene .
2) If and , but belongs to the scene ,
and belongs to the scene .
 If and are in the same video shot, is claimed
to be in the scene .
 If and are in the same video shot, is claimed
to be in the scene .
 If don’t fall into the same shot as and , the
scene corresponding to is determined by interpolation.
The time differences between and , and that
between and , are calculated. The estimated
scene between and is claimed to contain
.
After this postprocessing, scene boundaries of travel videos are
determined.
6. Experiments
6.1 Evaluation Data
We evaluate the proposed method based on five data sets. Each
dataset includes a video clip and a set of photos. Lengths of these
video clips range from eight to fifteen minutes, and these videos
are stored in MPEG-1 format and 352 × 240 resolution. There are
20 to 126 corresponding photos in different datasets, stored as at
most 400 × 300 resolution. Photos are rescaled to smaller sizes
due to the efficiency of feature points processing and visual word
construction. Videos and photos are captured by different amateur
photographers, with different capturing devices. Figure 3 shows
snapshots of videos and some corresponding photos. We see that
these data are unconstrained and contain wide range of content.
Table 1 shows the detailed information of evaluation data,
including the number of extracted keyframes for each video clip.
Purity
Video
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 2 3 4 5
Vword+HSV
Visual words
HSV
Naive
Figure 5. Performance based on four different scene detection
approaches.
Comparing with the effectiveness of different features, visual
word histograms have better performance than HSV histograms in
Videos 1 and 5. Based on visual words, we describe what are in
an image. However, color information is eliminated in extracting
SIFT feature points, thus HSV histograms still work better in
Videos 2 and 4. We achieve the best scene detection performance
when visual words are combined with color information. We
averagely obtain a purity value of 0.95, which is very promising in
scene detection, especially in the uncontrolled travel media.
To show effectiveness of the sequence matching approach, we
compare the proposed method with a naïve approach, in which no
visual information is considered, and no matching is performed
between different modalities. From Figure 5, we can see that the
proposed method is much superior to the naïve one. The
performance of the naïve method depends on the consistency
between the distributions of number of photos and number of
keyframes. The naïve method has satisfactory performance in
Video 5, because the number of photos and keyframes in each
scene are proportionally similar.
To further show that the proposed method is more appropriate to
be applied in travel video, we compare it with the method
proposed in [16]. One of the major challenges in scene detection
is the over-segmentation problem. We measure this effect in two
methods and list the results in Table 2. In each cell of this table,
the value (m, n) denotes that the corresponding scene is
segmented into m and n scenes, by the method in [16] and our
method (visual word + HSV), respectively. For example, in the
second columns for Video 1, (4,1) means that the second scene in
Video 1 is segmented into 4 scenes and one scene by the method
in [16] and our approach, respectively. From Table 2, we see the
effect of over-segmentation is severe in the unsupervised
clustering approach, and our approach works much better from
this perspective.
Table 2. Over-segmentation situations in different videos.
S1 S2 S3 S4 S5 S6
Video 1 (1,1) (4,1) (7,2) (3,1) (9,2) (3,1)
Video 2 (2,2) (8,1) (1,1) (1,1)
Video 3 (6,1) (3,1) (1,1)
Video 4 (1,1) (1,1) (1,1) (3,1) (2,1)
Video 5 (1,1) (2,2) (1,1) (5,2) (1,1)
The major strength of the method in [16] is that there is no need
to predefine the number of targeted shot clusters, which are then
manipulated by some postprocesses to form the results scene
detection. Because there may be drastically different visual
appearance in the same scenic spot, the approach that just takes
color information into account often overly segments scenes. On
the other hand, we automatically determine the number of scenes
from the photos’time information, and then appropriately
segment videos into scenes by cross-media alignment. We
conclude that the proposed method takes advantages of the prior
knowledge of travel media, and is more appropriate to facilitate
effective management of travel photos and videos.
6.4 Discussion
Correlation between different modalities is not limited in travel
photos and videos. Similar ideas for reranking the results of visual
search has been proposed in [20], while they use conventional
visual features such as texture and color. Moreover, Chu and
Chen [22] exploited correlation between different teaching media,
such as HTML pages, teacher’s sound, and navigation objects.
Therefore, we argue that the same approach is possible to be
extended to other domains, such as topic tracking or detection in
news media.
Although we have demonstrated that visual word histogram
performs well, it’s possible to improve more if we take the most
recent variations of visual word representation, such as that in [18]
and [23]. In our work, we describe an image by a visual word
histogram, which is a representation derived from “hard”
quantization of SIFT descriptors. In [18], the authors describe
characteristics of SIFT descriptors by Gaussian mixture model.
The work in [23] poses similar idea that they describe images by
“softly”describing a SIFT feature point in terms of several visual
words. We would like to apply the same concept to construct
more flexible representation for images in the future.
7. Conclusion
We have presented a video scene detection method that is based
on the correlation between different modalities. A scene in travel
photos and videos represents a scenic spot, which can be easily
determined by the time information of photos. We segment travel
videos into shots, and extract keyframes for each shot by a global
k-means algorithm. After representing keyframes and a photo set
by a sequence of visual word histograms, we transform scene
detection into a sequence matching algorithm. By using a dynamic
programming approach, we find optimal matching between two
sequences, and then determine video scene boundaries with the
help of photo scene boundaries. We experiment on five different
travel videos, with different parameter settings, and demonstrate
that the proposed method achieves very promising performance
for unconstrained travel videos. This result shows that using
correlation between different modalities is an interesting and
effective approach in analyzing consumer multimedia content,
especially travel videos and photos.
8. ACKNOWLEDGEMENTS
This work was partially supported by the National Science
Council of the Republic of China under grants NSC 97-
2221-E-194-050.
Visual Language Model for Face Clustering in Consumer
Photos
Wei-Ta Chu
Department of CSIE
National Chung Cheng University,
Taiwan
wtchu@cs.ccu.edu.tw
Ya-Lin Lee
Department of CSIE
National Chung Cheng University,
Taiwan
lylin96m@cs.ccu.edu.tw
Jen-Yu Yu
Info. and Comm. Research Labs
Industrial Technology Research Inst.
Taiwan
KevinYu@itri.org.tw
ABSTRACT
For consumer photos, this work clusters faces with large
variations in lighting, pose, and expression. After matching face
images by local feature points, we transform matching situations
into a novel representation called visual sentences. Then, visual
language models are constructed to describe the dependency of
image patches on faces. With the probabilistic framework, we
develop a clustering algorithm to group the same individual’s face
images into the same cluster. An interesting observation about
evaluating face clustering performance is proposed, and we
demonstrate the superiority of the proposed visual language
model approach.
Categories and Subject Descriptors
I.5.2 [Pattern Recognition]: Design Methodology – feature
evaluation and selection, pattern analysis. I.2.10 [Artificial
Intelligence]: Vision and Scene Understanding. I.4.7 [Image
Processing and Computer Vision]: Feature Measurement –
feature representation.
General Terms
Algorithms, Experimentation.
Keywords
Face clustering, visual language model, agglomerative clustering.
1. INTRODUCTION
Recently, people take large amounts of photos to capture travel or
daily life experience, with the low-cost digital cameras or mobile
devices equipped with cameras. The increasing number of photos
rapidly incurs great challenges in media management, retrieval,
and browsing. For media management, in addition to annotate
where the photos were taken and what objects were in these
photos, human beings have special interests on annotating who
were in photos. Face annotation is, therefore, a fundamental issue
for consumer photo management. This trend can be confirmed by
the development of web-based albums embedded with face
annotation functions [1].
The special challenges of face recognition or clustering in
consumer photos are at least twofold. First, drastic pose and
lighting variations make the conventional eigenface approach fail.
Although techniques of face appearance model or face alignment
have been proposed for years, promising results haven’t reported
for consumer photos. Second, some studies were conducted to
annotate faces based on not only eigenface similarity, but also
some context information such as clothes [2][3]. However,
accurately finding clothes regions under uncontrolled capture
environments is rather an open issue.
In this paper, we propose a brave new idea that exploits visual
language models to describe face similarity, and accordingly
conduct face clustering by an agglomerative clustering approach.
This idea is motivated by that we often say two similar persons
have similar eyes, noses, mouths, etc. For example, we would
describe two brothers who both have thick eyebrows, almond eyes,
raised mouth, etc. For human beings, a sequence of similar parts
on faces drives the perception of similarity. In this work, we
develop a model that “visually”describes the similarity between
two faces, based on a visual sentence expressing the face
matching situation. With the help of visual language models, we
cluster similar faces in an agglomerative manner, given a set of
unconstrained consumer photos.
The rest of this paper is as follows. Section 2 reviews studies
related to face clustering and visual language models. Section 3
describes face matching based on local feature points. In Section 4,
face matching situations are transformed into visual sentence, and
visual language models are constructed to conduct clustering.
Experimental results are provided in Section 5, and Section 6
gives the concluding remarks.
2. RELATED WORKS
To specially tackle with face clustering for consumer photos, not
only standard face recognition techniques but also external
context information were utilized by previous researches. Zhang
et al. [2] extracted three features from the upper part of body, face,
and eyes, and proposed a Bayesian framework to describe and
predict the identification of each face. Zhao et al. [3] proposed a
graphical model to integrate face and clothes information. Further
post-processing was developed to eliminate identification errors.
In our previous work [4], we developed a module based on local
feature points matching to enhance clustering performance. In
addition to consumer photos, challenges derived from moving
faces in videos increasingly draw attention. Tao and Pan [5]
segment videos into sequences with face images in similar poses.
They then identify faces for each pose-constrained sequence.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
MM’09, October 19–24, 2009, Beijing, China.
Copyright 2009 ACM 978-1-60558-608-3/09/10…$10.00.
two closest visual words is modeled. These three models can be
respectively described as follows.
, (3)
, (4)
. (5)
To characterize different face matching situations, we construct
two visual language models. The first visual language model
describes the matching situations between faces of the same
individuals, in a form of conditional probability distributions. The
second visual language model describes the matching situation
between two distinct individuals.
 Unigram
The unigram model is constructed as
, . (6)
The symbols and denote matching situations between faces
of the same individuals and different individuals, respectively.
The function denotes the count of the visual word
appearing in the visual sentences collected from . In
language modeling, zero probability would harm the succeeding
classification process. Therefore, we assign a small prior
probability for this case. Handling the zero probability problem is
an age-old but important issue in natural language processing.
Detailed smooth methods please refer to [9]. In this work, we
utilize the toolkit provided by [9] to implement language models.
 Bigram
The bigram model describes the probability of a visual word that
conditionally depends on its previous closest neighbor:
, . (7)
Smooth methods for bigram may be more complicated than that
for the unigram model. However, detailed implementation is
beyond the scope of this paper, and readers are referred to [9].
 Trigram
Similarly, the trigram model is constructed as follows
, . (8)
The probability of a visual word that conditionally depends on its
previous two closest neighbors is described.
4.3 Face Clustering
With the visual language models, we first pick up outliers and
then cluster the remaining face images. Lastly, the outliers are
assigned to appropriate face clusters by a specially designed
method. Details of these processes are described as follows.
 Outlier Selection
Some face images may be captured in significantly varied poses,
or may be too blurred due to motion or bad lighting. This kind of
face image never resembles any else. Such images are viewed as
outliers, and we should conduct special process for them. For a
pair of faces and , the face likelihood ratio is defined as
, (9)
where is the visual sentence representing the matching
situation between and , is the visual language describing
matching situations between the same individual’s faces, and
describes matching situations between different individuals’faces.
A face image is selected as an outlier if its likelihood ratios to all
other face images are below a threshold:
(10)
 Clustering
For the face images other than outliers, they are clustered by an
agglomerative process. Each face image first forms a face cluster,
i.e., , , …, . Two face clusters
and are merged if
, (11)
, (12)
, (13)
where is a modified Hausdorff distance between the
clusters and . The distance between the face
in and the face in is evaluated counter to the
probability of being the same individual. The value denotes
the number of face images in .
The clustering process proceeds until the desired number of
clusters have been reached.
 Outlier Assignment
For each face image in the outlier set , we
assign each of them to one of the existing face clusters. The
outlier is assigned to the cluster if
, (14)
According to this equation, an outlier is assigned to a face cluster
by checking its average likelihood ratio to existing clusters and
finding the one that causes a maximum value.
5. EXPERIMENTS
We first verify how different language models affect clustering
performance. Four hundred face images from the AT&T face
database [11] are selected to train a unigram, a bigram, and a
trigram model, respectively. For testing, we evaluate these models
by clustering five sets of face images. Table 1 shows that these
faces are with different scales of lighting variations, expression
variations, and pose variations. The fourth and the fifth datasets
are captured by amateurs in family tours. Figure 3 shows the
clustering accuracy. The accuracy value of each face cluster is
calculated by dividing the number of correctly clustered faces by
the total number of faces in this cluster. For a dataset, the average
accuracy is calculated by averaging the accuracy values of all face
clusters in it, and is illustrated in Figure 3.
Figure 3 shows that we generally have the worst performance for
the fourth and the fifth datasets, and the first three datasets that are
captured in control environments have satisfactory performance.
These results confirm that consumer photos with large variations
in lighting, expression and pose harm the clustering performance.
The bigram and trigram models behaves much better than the
unigram model. The average accuracy values over these five
datasets for unigram, bigram, and trigram models are 0.58, 0.74,
and 0.73, respectively. Therefore, we adopt the bigram model for
the following experiments.
In performance evaluation, we collect consumer photos recording
travel or daily life, and also use a subset of photos from an open
photo collection [10]. There are 17 datasets containing totally
1409 face images. The number of persons in a dataset range from
two to seven. To verify the proposed method, we first investigate
Feature Classification for Representative Photo Selection
Wei-Ta Chu
Department of CSIE
National Chung Cheng University,
Taiwan
wtchu@cs.ccu.edu.tw
Chia-Hung Lin
Department of CSIE
National Chung Cheng University,
Taiwan
lchu96m@cs.ccu.edu.tw
Jen-Yu Yu
Info. and Comm. Research Labs
Industrial Technology Research Inst.
Taiwan
KevinYu@itri.org.tw
ABSTRACT
This paper points out that different local feature points provide
different impacts to near-duplicate detection and related
applications. Aiming to automatic representative photo selection,
we develop three feature classification methods, i.e., point-based,
region-based, and pLSA-based classification, to differentiate local
feature points described by SIFT descriptors. We investigate the
performance of these classification methods, and discuss how they
influence near-duplicate detection and extended applications.
Experiments show that, with effective feature classification, more
accurate representative selection results can be achieved.
Categories and Subject Descriptors
I.5.2 [Pattern Recognition]: Design Methodology –classifier
design and evaluation, feature evaluation and selection. H.3.1
[Information Storage and Retrieval]: Content Analysis and
Indexing–abstracting methods.
General Terms
Algorithms, Measurement, Experimentation.
Keywords
Feature classification, near-duplicate detection, probabilistic
latent semantic analysis, representative selection.
1. INTRODUCTION
Efficiently browsing and managing large amounts of digital
photos have been significant issues in recent years; especially
capturing, storing, and disseminating photos become extremely
popular and easy. Therefore, related researches have been
explosively proposed from many perspectives. For example, some
studies work on automatic tagging or semantic concept detection
to facilitate media management, and some works deal with finding
regions of interest or generating vivid presentation to enhance
user’s browsing experience.
In our previous work [1], we exploit near-duplicate detection
(NDD) to describe the relationships between photos, and then
discover the relationships to find the most representative photo
that conveys the most canonical landmark or view of a scenic spot.
Besides, we bring up an interesting idea that the matched feature
points, which are the intermediate information in the NDD
process, often lie on the contour or inside of the representative
object. This characteristic inspires us that the most prominent
region in a photo can be determined by utilizing the spatial
information of matched feature points.
Although promising results were reported in [1], we found that
different local feature points with varied characteristics may
discrepantly influence the performance of near-duplicate detection.
To human beings, the concept “duplication” often comes from 
that two images have the same artificial objects, such as building,
tower, and statute. This characteristic is especially convincing in
consumer photos taken in journeys. Although pieces of grass or
surface of waterfront in two images may be similar as well, they
pose little impact in near-duplicate detection and extended
applications. Therefore, it’s more reasonable to eliminate the 
influence of noisy feature points in near-duplicate detection,
which is not extensively studied in the literature. Figure 1 shows
examples of a photo marked with all feature points and only with
feature points on artificial objects, respectively. In this case, if
only the feature points on artificial objects are considered in near-
duplicate detection, more robust results can be obtained. In this
paper, we investigate three different feature classification methods
and conduct comprehensive experiments.
(a) (b)
Figure 1. An example of showing (a) all feature points and (b)
only the feature points on artificial objects.
The rest of this paper is described as follows. Section 2 gives the
system overview. Section 3 describes the major contribution of
this paper, i.e., feature extraction and classification. One
application, i.e., representative selection, that benefits efficient
photo management and browsing is described in Section 4.
Section 5 reports the experimental results, and Section 6 provides
concluding remarks.
2. SYSTEM OVERVIEW
The photos taken around the same place would include significant
content variations. Some of them may include the most famous
landmark or view, but some of them may include the shops
around there, pedestrians, or something that is not directly related
to this scenic spot. However, tourists usually take photos at some
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
MM’09, October 19–24, 2009, Beijing, China.
Copyright 2009 ACM 978-1-60558-608-3/09/10…$10.00.
difference between the region-based approach and the point-based
one is that the features put to training and testing are average
vectors of feature points in the same region. At the filtering stage,
each region is evaluated by the SVM classifier, and is then
categorized into an artificial region or a natural region. All feature
points in an artificial region are then claimed as artificial points.
3.3 PLSA-Based Classification
Another approach to consider the context information between
feature points was proposed in [4]. We modify their method as
follows. Feature points specifically from artificial objects and
natural objects are collected, respectively. For the set of artificial
feature points, we apply the k-means algorithm to group them into
a number of clusters. The set of clusters is called the visual
vocabulary for artificial objects, denoted by . Centroid of each
cluster is calculated by averaging all SIFT descriptors in this
cluster, and is called as a visual word that represents this cluster
of features. By the same method, we construct the visual
vocabulary for natural objects, denoted by .
Given a feature point , we determine its corresponding visual
words and in and by quantizing it into one of the
pre-trained visual vocabularies. That is,
, (2)
, (3)
where denotes the quantization function, denotes
the Euclidean distance between the feature point and the visual
word , and ( ) denotes the size of the visual vocabulary
for artificial (natural) objects.
The probability of a visual word corresponding to artificial
objects is estimated based on the co-occurrence information
between artificial feature points. We exploit probabilistic latent
semantic analysis (pLSA) model to build a joint probability model
over the image and the visual word :
, (4)
where is a latent concept subtly
embedded in the visual vocabulary . The pLSA model is
defined by the conditional probability that represents
the probability of observing the visual word given the concept
, and the condition probability of the occurrence of
in the image . The parameters of the model are estimated using
the Expectation-Maximization (EM) algorithm, using a set of
training data that includes artificial points. Construction of the
pLSA model for natural objects is in the same manner.
Given a feature point in the image , which corresponds to the
visual word with respect to artificial objects, we try to map the
visual word to the most likely concept that are learned from
artificial object training data. Based on the pLSA model, the most
likely concept can be determined as follows:
(5)
The same manner is applied to calculate the probability of the
most likely concept , based on the pLSA model for natural
objects. Finally, the probability of the feature point
corresponding to the artificial concept is , and
the probability of corresponding to the natural concept is
. The feature point in the image is claimed to
be an artificial feature point if
, (6)
where is a threshold that can adjusted to give different
preference in feature classification. If the ratio is less than the
threshold , the feature point is claimed to be a natural point. In
this work, we simply set the threshold as 1 so that no special
preference is applied.
4. REPRESENTATIVE SELECTION
Given a set of photos , we first filter out
feature points that are claimed as natural points by the methods
described above. Then, whether a pair of photos , ,
, is near-duplicate is determined by the method proposed
in [3]. We represent the relationship between near-duplicate
photos as a non-directed, non-weighted graph , where
any node (photo) in is at least once
determined as a near-duplicate to someone else. The edge is in
if and are detected as a near-duplicate pair. Given this
graph, we determine the most important node by checking the
“centrality value” of each node. From the idea of social network
modeling, the person who is “closest” to al others plays the most 
important role. Similarly, the photo that is mostly near-duplicate
to others is the most representative one. We evaluate the centrality
value of each node by the degree centrality [1]. The degree
centrality of a node is
, (7)
where if and are connected, and otherwise
.
5. EXPERIMENTS
5.1 Evaluation Dataset
 Training for the point-based classification: There are totally
3483 artificial feature points and 6170 natural feature points
for training, which are extracted from twelve photos. By
labeling artificial points as positive samples and natural points
as negative samples, we construct an SVM classifier [5] to
determine whether a feature point is artificial or natural.
 Training for the region-based classification: Each photo is
divided into 40×40 regions, and the feature vector for each
region is extracted. There are totally 846 artificial regions and
921 natural regions, which are extracted from forty photos.
Note that only the regions in which all feature points belong
to artificial or natural points are selected as the training data.
 Training for the pLSA-based classification: Four hundred
photos are used for training. There are totally 53655 artificial
feature points, which are clustered into 600 visual words.
Similarly, 63769 natural feature points are used to construct
600 visual words. In this work, we use the program provided
in [6] to implement the proposed approach.
5.2 Performance of Feature Classification
Based on manually labeled ground truths in which ten photos
include 3182 artificial feature points and 5173 natural feature
points, we calculate precision rate for each classification method
as , where is the number of artificial
OBJECT SEGMENTATION BASED ON COMMON INFORMATION
BETWEEN IMAGES
Wei-Ta Chu1 (朱威達), Chien-Ta Hung1 (洪建達), Jen-Yu Yu2 (游人諭)
1Dept. of Computer Science and Information Engineering, National Chung Cheng University
wtchu@cs.ccu.edu.tw, hct96m@cs.ccu.edu.tw
2Information and Communication Research Labs, Industrial Technology Research Instutite
KevinYu@itri.org.tw
ABSTRACT
We exploit common information between images to
construct data models and background models, and
accordingly segment major objects in images without
human intervention. This method can be applied to
images that consist of same foreground objects in varied
backgrounds, such as a person dressing the same in
different scenes, or a major object appearing with
different backgrounds. Experimental results show the
effectiveness of the automatic segmentation method, and
we provide discussion about the influence of common
information in object segmentation.
1. INTRODUCTION
Object segmentation in images has been an age-old
problem in computer vision and image processing
communities. One of the challenges in object
segmentation is the semantic gap between visual features
and human perception. An object may contain connected
pieces that have different visual appearance. Therefore,
the methods that cluster pieces with homogenous color
just segment an image into various regions rather than
meaningful objects. Nonetheless, more than a dozen of
studies have been proposed to conduct image
segmentation, such as the ones based on the mean shift
algorithm [1] and the ones based on graph cut [5][6][7].
Because automatic image segmentation is a
notorious problem that hasn’t been solved for several
decades, some researchers turn to develop friendly tools
to facilitate efficient manual segmentation. Li et al. [2]
developed a tool such that users can draw a few strokes
to roughly indicate foreground and background, and
then the system segments an image into foreground part
and background part. Wang et al. [3] further improved
this work to make the tool more reliable.
Recently, automatic segmentation based on
common information between images [8] or automatic
transduction based on a manual segment result [9] draw
contiguous attention. Gallagher and Chen [8] exploit the
graph cut framework to segment clothes regions from
background and parts of torso that are not covered by
clothes. Based on the fact that the same person wears the
same in a short period, their system automatically
constructs a clothes model and a background model
from a set of the same individual’s photos. Costs about
observed data and spatial discontinuity are respectively
calculated to be fed into the graph cut framework. In
contrast to automatic model construction, Cui et al. [9]
started from a manual segmentation result, and therefore
construct object models based on a more accurate
foundation. Given a photo that contain similar object
that has been manually segmented before, their system
propagates segmentation effect to the new photo.
In this work, we conduct automatic object
segmentation based the graph cut framework, which can
be formulated as an energy minimization problem. We
investigate how to automatically derive costs of
observed data and spatial discontinuity that are specific
to different applications. This work is developed under
the idea proposed in [8]; however, we try to extend its
feasibility in terms of more effective features and
general object segmentation. The goal of this work is to
utilize common information between images to achieve
meaningful object segmentation.
The contributions of this work are summarized as
follows.
 Using common information from a set of
images to construct foreground/background
models, and accordingly derive data cost and
spatial discontinuity cost.
 More effective features than that used in [8].
 Extending the method that is originally
designed for clothes segmentation to general
object segmentation, under some constraints.
The rest of this paper is organized as follows.
Section 2 describes the kernel component of this work,
i.e., the graph cut framework. We describe object model
Figure 2. Example results of normalized cut.
Figure 2 shows some results of normalized cut.
Parts of images in Figure 2 and in experiments are
downloaded from the open photo collection [10].
To characterize each superpixel, we extract HSV
(hue, saturation, value [intensity]) color histogram and
edge histogram of each superpixel. In the HSV
histogram, there are 16 bins for hue, 4 bins for
saturation, and 4 bins for value. After detecting gradient
of each pixel in a superpixel, an edge histogram is
constructed on the basis of five bins, which roughly
indicate orientation corresponding to , , , ,
and . Comparing with the features used in [8], we
extract more elaborate edge features, and represent
pixels in HSV color space.
Given a pair of training images and , we would
like to find rough regions that may present clothes.
Because the individual in both images wears the same,
this task is achieved by finding common information
between images. Before we enter the main process, we
first detect pixels with skin colors and put the aside from
the main process. Because faces and arms of the same
individual in different images are absolutely common
characteristics, we filter out them to avoid noises.
To check whether the pixels at the position in
and has “common”characteristics, we represent
the pixels at by the HSV color histogram and edge
histogram of the superpixel where the pixels belong to.
Accordingly, the th histogram of two pixels in
and are denoted as and , respectively.
This important trick attenuates noisy pixels and sensing
errors in a connected region (superpixel). All pixels in
the same superpixel share the same data characteristics.
Based on this representation, the distance
between these two pixels are calculated as
, (2)
where denotes the distance
between the th histogram, and the overall distance is
calculated by summing distance in terms of different
histograms.
Without loss of generality, we can conduct the same
process to any pair of images in the training data
. We have to emphasize again that
any two images and present the same individual in
the same dress. The overall distance at the position
is calculated by averaging all possible pairs in :
, , , . (3)
After calculating the overall distance between
correspond pixels, we can construct a discrete distance
distribution to show common characteristics
between images in . Figure 3 shows examples of
distance distributions from different training set.
The mask region is determined by finding the
distance such that
, (4)
where the value is set as 0.5 in this work.
To determine the mask covering clothes, we find the
positions in which average distances are smaller than
and assign them as the pixels in these positions as
foreground. Figure 4 shows examples of the obtained
foreground masks from different training data. In this
figure, the first three columns are training data, and the
fourth column shows the estimated foreground regions
(masks), which are displayed in white. Note that the
number of training images is not limited to three. The
same process can be applied generally.
Figure 3. Examples of distance distributions.
Figure 4. Examples of foreground masks.
(a) (b) (c) (d)
(e) (f) (g) (h)
Figure 6. Results of clothes segmentation.
Figure 7. Results of main object segmentation.
Figure 8. Comparison of segmentation results based on
100 superpixels and 50 superpixels.
It’s not surprising that accurate object segmentation is
harder to be achieved, especially appearance of the main
object may vary significantly, and objects in different
images may be captured from different viewpoints.
5. DISCUSSION
5.1. Influence of Superpixels
The basic unit for labeling is a superpixel. Based on the
normalized cut algorithm, we segment an image into
arbitrary regions. If we segment an image into a large
number of small pieces (superpixels), each superpixel
itself has high self similarity. However, the clothes may
be over-segmented into many small pieces because of
wrinkles or slight lighting variations. On the contrary, if
the number of extracted superpixels decreases, each
superpixel covers a larger region and may contain
content with larger variation. However, viewing the
clothes as a combination of a few large pieces matches
human perception because of the smoothness nature of
the human vision system. Therefore, setting of the
number of superpixels may influence construction of
data models and background models.
In the segmentation results shown above, we
segment each image into 100 superpixels, while Figure 8
shows the comparison of segmentation results based on
100 superpixels (the second row) and 50 superpixels
出席國際學術會議心得報告
計畫編號 NSC 97-2221-E-194-050
計畫名稱 旅遊經驗之組織與呈現系統
出國人員姓名
服務機關及職稱
國立中正大學資訊工程系 助理教授 朱威達
會議時間地點 加拿大溫哥華, 2008/10/27~2008/10/31
會議名稱 ACM Multimedia 2008 (2008年 ACM多媒體研討會)
發表論文題目
W.-T. Chu and C.-H. Lin, “Automatic Selection of Representative Photo and
Smart Thumbnailing Using Near-Duplicate Detection,”Proceedings of ACM
Multimedia Conference, pp. 829-832, 2008.
一、參加會議經過
2008年的 ACM Multimedia Conference在加拿大溫哥華舉辦，它是美國計算機協會(ACM)
在多媒體領域一年一度的盛會。與會者來自數十個不同的國家，與會者包括多媒體研究人員、
廠商、藝術家等，參加人數約七八百人，會議的規模、舉辦型式與論文品質為電腦科學領域
中最頂尖者。
ACM Multimedia素來以高品質的論文聞名，是世界上各著名研究機構多媒體團體亟欲攻
佔的灘頭堡。今年長篇論文(full paper)的接受率約 20%，短篇論文約 33%。此次會議台灣的
與會者大放異彩，台灣大學的研究團隊發表了五六篇長篇論文及三四篇短篇論文，而我們的
團隊也與有榮焉地首次以在本校研發的成果發表一篇短篇論文。
此會議除了常見的論文口頭報告(technical papers)、海報論文報告(poster)與演講(talk)之
外，還安排了媒體藝術展覽(art program)、博士生論壇(doctorial symposium)、以及相關議題的
workshop等。在會議進行的五天(10月 27日至 31日)當中，豐富的學術活動排得滿滿。
在學術論文的部份，此次會議包含以下主題：
(a) Multimedia content analysis, processing, and retrieval; (Content track)
(b) Multimedia networking and systems support; (Systems and Networking track)
(c) Multimedia tools, end-systems, and applications; (Applications track)
(d) Human-centered multimedia (Human Centered track).
雖然 ACM Multimedia 每年收的論文數不多，但幾乎每篇都擲地有聲，都代表在某個圖
學研究課題上的重要進展。此外，我們也可看到每位上台報告者嚴謹且準備完善的態度，這
在許多論文品質良萎不齊的研討會中是看不到的。
本人在 10月 27日報到，在熟悉場地後感受到會議規模與主辦單位之用心。會議地點選
擇在溫哥華市中心核心地區，讓人可充分領略溫哥華秋天的風情。
10月 28日早上在開幕式與演講過後隨即帶來第一個高潮─Best paper session。四篇由會
事實上這已經是幾年來我第五次參加 ACM Multimedia Conference，也見證幾年來它的變
化，包括會議內容越來越豐富，越來越與實際業界的需要結合，也新增了 human-centric這項
議題的探討等。然而，這次會議的硬體設備安排並不好。由於參與人數眾多，但會議會場不
夠大，所以往往有非常擁擠的感覺。論文品質與往年比起來似乎也略遜一籌，不曉得是否因
為今年取消 double blind review的關係。以上的參與經驗都可成為往後舉辦會議或發表論文的
重要的依據。
三、攜回資料
 Proceedings of ACM Multimedia, 2008.
 Call for paper, ACM Multimedia, 2009.
 Call for paper, ACM International Conference on Image and Video Retrieval, 2009.
 Call for paper, ACM International Conference on Multimedia Information Retrieval, 2010.
 Call for paper, IEEE International Conference on Acoustics, Speech, and Signal Processing,
2010.
around there, pedestrians, or something that is not directly related 
to this scenic spot.  
Figure 1 shows the content variations in the photos taken in the 
famous Rokuonji temple in Kyoto. From this example and many 
other web-based albums, we found that most travelers incline to 
take the landmark or famous views several times. Moreover, 
tourists usually take photos at some specific locations such that 
they can capture the canonical view as that in the postal card. 
According to these observations, we propose that we can 
approach the selection of representative photo based on near-
duplicate detection, which finds the near-duplicate pairs like the 
fifth to the eighth photos in Figure 1.  
(1) (2) (3)
(4) (5) (6)
(7) (8) (9)  
Figure 1. Photos taken around the same scenic spot.  
Applications of near-duplicate detection (NDD) have been 
proposed for many different purposes, such as sub-image retrieval 
[2] and automatic image annotation [4]. In various near-
duplication detection approaches, local image descriptors that 
capture the salient characteristics over different image scales are 
widely used. Among different descriptors, Lowe’s SIFT (scale-
invariant feature transform) feature [5] has been demonstrated to 
have the best performance and is used in this work.  
We exploit the SIFT-based NDD method proposed by Zhao et al. 
[6]. This method largely reduces the false alarms caused by 
conventional nearest-neighbor matching approaches and increases 
the matching speed with a multidimensional index structure. 
Moreover, as the near-duplicate photos are often highly localized 
and spatially smooth, the correspondence of SIFT matched points 
have coherent patterns, which can be modeled by support vector 
machines (SVMs). This method obtains good balance between 
matching speed and matching accuracy.  
2.2 Near-Duplication Detection Process 
Given a set of photos that are clustered 
together by using the time-based clustering method [1], we 
determine whether a pair of photos , is 
near-duplicate by the following steps, as illustrated in Figure 2.  
z SIFT-based matching: for any pair of photos in this cluster, 
the method in [6] that embeds a one-to-one symmetric 
criterion to filter out false matches is applied. Figure 3(b) 
shows the effectiveness of false alarms reduction, as 
compared to a conventional approach (Figure 3(a)).  
z Orientation feature extraction: due to the characteristics of 
local coherence and spatial smoothness, the orientation of the 
link connecting matched points in two photos are similar. We 
calculate the orientation of links and quantize it into 36 levels. 
A 36-bin orientation histogram is then constructed. In near-
duplicate pairs, the values of the orientation histogram would 
apparently concentrate.  
z SVM-based determination model: a SVM is used to model the 
characteristics of the orientation histogram. We estimate the 
model parameters based on 40 near-duplicate pairs and non-
near-duplicate pairs. At the test stage, we make a binary 
decision on each photo pair based on the SVM classifier.  
SIFT-based 
matching
Orientation 
feature 
extraction
SVM-based 
determination 
model
Near-
duplicate 
pairs
 
Figure 2. The process of near-duplicate detection.  
(a)
(b)
 
Figure 3. Sample results of (a) conventional SIFT-based matching 
and (b) one-to-one symmetric SIFT-based matching.  
2.3 Sub-Clustering Before Matching 
One of the critical issues in NDD is that there are tremendous 
pairs of photos should be examined. For example, if there are N 
photos in a set, totally  different pairs of photo are needed to 
be checked. To reduce the complexity, we further cluster the 
given set of photos based on content-based characteristics. We 
then perform NDD for each sub-cluster, i.e., any two photos that 
are in different sub-clusters would not be examined.  
Because the representative landmark or view would have similar 
appearance, we can reasonably assume that they would be 
categorized in the same sub-cluster. For example, if the set of N 
photos are categorized into M sub-clusters , the 
total number of pairs for NDD is 
, (1) 
where  is the number of photos in the ith sub-cluster. In the 
case of N = 10, M=2, , and , we need originally 
need to check  photo pairs. However, we only have to 
evaluate   photo pairs if we perform sub-clustering 
first. In this work, the sub-clustering process is implemented 
based on RGB histograms of photos.  
3. REPRESENTATIVE SELECTION 
With loss of generality, assume that the sub-cluster  in the set 
 contains the near-duplicate photos, i.e., the 
830
