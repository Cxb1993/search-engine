 II
中文摘要 
  
 本計畫之目的為沿續機器人之研發技術，並將虛擬實境技術導入，藉以進行兩項技術
之整合。在機器人或機械臂的研究上已經相當成熟也有相當多的國內外學者著墨於此，且
有相當卓越的成果。計畫主持人從事機器人與機械臂的研究多年，對於機器人的實作已相
當熟悉。所以對機器人的應用便有相當多的想法，因此提出將機器人與虛擬實境結合的整
合應用。 
  
我們於 96 年度提出兩年期研究計畫案，第一年已核准，在第一年的計畫中，我們依計
畫進度將計畫所需的機器人的運動控制模組設計製作完成，同時定義出與虛擬實境系統的
整合介面。 
關鍵字 ： 虛擬實境、直流馬達控制器、機器人 
 
Abstract 
With idea of continually study, we propose this project.  The virtual reality will be 
introduced to the project for integrating both robotic and virtual reality technology.  It could not 
be mentioned too much that the robotic study are popular all over the world. And there is a lot of 
excellent studies have been presented from time to time.  The project lead have worked on the 
robotic study for many years, he has a lot of experience on the robotic topic.   That why we 
propose the project which is called the integration of the robotic and virtual reality. 
 
We proposed a two year project to the NSC in 2006 and got the grant for the first year 
supporting. In the year one, we have designed a robotic motion controller and also define the 
interface of the virtual reality integrated system.  
 
Keywords: Virtual Reality, DC Motor Controller, Robotics 
1. 前言 
 
 本計畫 － 機器人系統於虛擬實境之系統整合與應用，初次提案為二年期之計畫，於
第一年之計畫中，我們主要的重點為設計開發機器人運動軸所需多軸驅動控制器以及建立
虛擬實境與機器人間的互動介面。在本年度中，我們已完成了機器人與虛擬實境的連結驅
動程式，並將虛擬實境與機器人的控制初步連線完成。另外，我們於多軸控制器的努力上，
已將五個 8 位元的微控制器，以一個 dsPIC 30F61015 整合在一起，並利用元件上的 I2C 介
面進行連線；除了可由各軸讀回編碼器的位置外，亦可利用 30F6015 下達控制命令，要求
各軸進行 PID 定位控制。在報告書中，將分別以多軸定位控制模組之定義、控制目標-二足
機器人、以 UBot 為動力平台之輪型機器人之 DLL 呼叫函數定義、機器人於 Virtools 平台
之實測結果與結論等主題來撰寫研究成果進度報告。 
  
2. 研究目的 
  
 本計畫之目的為沿續機器人之研發技術，並將虛擬實境技術導入，藉以進行兩項技術
之整合。在機器人或機械臂的研究上已經相當成熟也有相當多的國內外學者著墨於此，且
有相當卓越的成果。計畫主持人從事機器人與機械臂的研究多年，對於機器人的實作已相
當熟悉。所以對機器人的應用便有相當多的想法，因此提出將機器人與虛擬實境結合的整
合應用。我們於計畫的第一年中，將計畫所需的機器人的運動控制模組設計製作完成，同
時定義出與虛擬實境系統的整合介面。 
 
3. 文獻探討 
 
 首先我們先介紹虛擬實境的應用與研究現況[1-4]，虛擬實境乃是透過電腦圖學模擬物
理環境所建構出的三維數位化世界，這一部分已由專門的軟體，如Virtools軟體，可提供我
們進行程式建構。虛擬實境除了可讓使用者與與虛擬世界中人物、物體、場景互動，甚至
可以透過顯示及追蹤裝置讓使用者達到融入其中或身歷其境的感覺。而這個特性即可應用
於各種的場合中，例如復建與遊戲。在Michael Heim[1]在認為虛擬實境為一種技術，而構
成虛擬實境內容的形成必須滿足三項要求：融入、互動、與豐富資訊的傳達。而這三項要
求的英文皆以I開頭，又被稱為3I。由於各種輸入與輸出硬體技術的逐漸成熟，近年來虛擬
實境根據不同的介面設備與目的在業界被分類為五種類型：桌上型、融入型、投射型、模
擬機型、及混合型。 
 
 以桌上型而言，其結構單純乃是虛擬實境系統較簡易的系統，它可以鍵盤與滑鼠為輸
入設備，結果可螢幕直接輸出。由桌上型虛擬實境並無法達到融入的效果，因此採用多個
螢幕做為輸出顯示，便是一個克服此一缺點的方法。另外，在輸入裝置上還可選用軌跡球、
3D滑鼠、搖桿或偵測使用者頭部或身體位置動作的紅外線追蹤電腦滑鼠等設備，加強使用
者與虛擬實境空間更直覺化的互動性。例如利用置放於顯示器上的紅外線追蹤器感知使用
者的頭部的移動讓使用者的動作與與電腦中內容互動。 
 
 在融入型虛擬實境系統上，它的主要訴求為產生個人經歷。最佳的互動設備為頭盔顯
示器（Head Mounted Display)。這種顯示器用全罩式的方法將使用者視野包圍，除了講究視
13
4
5
2
第一軸
Axis 1
第二軸
Axis 2
第三軸
Axis 3
第四軸
Axis 4
第五軸
Axis 5
1
3
4
5
2
Encoder
Command
Encoder
Command
Encoder
Command
Encoder
Command
Command
Encoder
 
  
圖 2 各軸配線位置與輸入輸出訊號源 
  
圖 3 編碼器輸入測試 
 
圖 4 直流馬達驅動器 
 3
 5
平台的DLL呼叫函數名稱，每一個與機器人UBot互動的呼函數皆為機器人於運動所需的函
數。於本年度的計畫中，到目前為止我們已完成定義這些DLL函數，並完成DDL檔之製作。
另外將這些函數製作成Virtools所需的Building Block，於Virtools環境中進行功能測試。以下
我們所開發的跨平台呼叫函數這些函數亦為Virtools下的介面函數，以下的應用範例為
VB.Net的函數呼叫語法。我們將這些設計完成的DLL檔再封裝成Virtools所需的Building 
Block，並於Virtools中實際測試。我們目前的測試方式已完成， 
 
- 障礙物矩離偵測 
- UBot機器人於虛擬實境與真實環境中的同動實測 
- 整合OPEN CV之影像辨識功能，並將此函數庫之辨識資訊傳入Virtools平台 
 
在障礙物矩離偵測上，我先以紙箱進行測試，其測試結果於後文中說明。在機器人於
虛擬實境與真實環境中的同動實測，我們目前先以簡單的虛擬實境中移動虛擬機器人來導
引實體機器人之移動。 
 
5. 實驗結果 
 
機器人於Virtools平台之實測結果 
  
 我們以紙箱進行實驗測試，實驗步驟規劃如下： 
 
  步驟一： 測試連線，當Virtools啟動時，對機器發出連線命令。 
  步驟二： 查詢機器人16個聲納是否正常回應。 
  步驟三： 利用紙箱做為障礙物，量測實際距離是否正確。 
  步驟四： 於Virtools建置機器人本體，並利用現成的玩偶圖像做為障礙物。 
步驟五： 將聲納傳回值，輸入座標轉換矩陣，利用Virtools環境來表示目前機器
人的處境。 
  步驟六： 完成Building Block實測驗證。 
 
 根據以上的測試步驟，我們將實驗結果以圖像展示，我們利用無線的方式利用無線網
路連結UBot上的UART-LAN轉換模組，並透過該模組與UBot通訊並控制UBot。在測試的環
境中，我們以Sonar 5 與 Sonar 8來驗證系統的功能。圖4中二個障礙物距離機器人均為113
公分；圖5中將Sonar 5前的障礙物移至68公分處，Sonar 8前方的障礙物不動；圖6中，Sonar 
5前的障礙物不動，將Sonar 8前的障礙物移近，其量測距離為58公分；圖7中，Sonar 5前的
障礙物遠離機器人，Sonar 8前的障礙物不動，二者的量測距離分別為為115公分與58公分。
於虛擬實境中的紅外線偵測均為30公分，因為紅外線偵測的最遠距離為30公分，在此實驗
中，因為沒有將障礙物靠近機器人至30公分內所以機器人之紅外線感測器標示均為30公
分。除此之外，我們亦完成利用行走的基本函數，控制機器人於真實環境中行走，同時也
在虛擬實境中表現其運動狀態的實驗測試。 
 
 於虛擬實境中，我們已為機器人所建置的DLL檔，開發出機器人專屬的Building Block，
   
圖 7 虛擬實境與機器人互動結果，將 Sonar 5 前的障礙物移遠，其量測距離均為 115 公分 
 
 
圖 8 虛擬實境之 Building Block 測試程式方塊 
 
 7
 9
用的感測器為聲納或紅外線，無法辨識物體的特徵，因此在虛擬實境中僅能以玩偶代替。
我們以影像的方式來建置障礙物的特徵表，並利用該特徵表來找出環境中的特定障礙物，
接著應用內建於虛擬實境中的物品來表示障礙物。並利用該虛擬環境地圖，做為機器人的
記憶，來引導機器人於環境中行走。 
 
7. 參考文獻 
 
1. Heim M., Virtual Realism, Oxford University Press, 1998. 
2. Laurel B., Computers Theatre, Addison-Wesley Publishing Company, Inc, 1993. 
3. Oviatt S.L., Ten Myths of Multimodal Interaction, Communication of ACM, 42(11), 
74-81, 1999. 
4. Raskin J., The humane interface: new directions for designing interactive systems. 
Addison-Wesley, Reading, Mass, 2000. 
5. C.L. Golliday, Jr. And H. Hemami, “An Approach Analyzing Biped Locomotion 
Dynamics and Designing Robot Locomotion Control,” IEEE Transactions On 
Automation Control, Vol. 22, No. 6, pp. 963-972, 1977. 
6. F. Miyazaki, and S. Arimoto, “A Control Theoretic study on Dynamical Biped 
Locomotion,” ASME, Journal of Dynamic Systems, Measurement and Control, Vol. 
102, pp. 233-239, 1980. 
7. M.H.Raibert, Legged Robots that Balance, Cambridge, MA:MIT Press, 1986. 
8. Qinghua Li, Atsuo Takanishi, and Ichiro Kato, “A Bpied Walking Robot Having A ZMP 
Measurement System Using Universal Force - Moment Sensors,” IEEE/RSJ 
International Workshop on Intelligent Robots and Systems IROS ’91, pp. 1568-1573, 
1991. 
9. H. Miura, and I. Shimoyama, “Dynamical Walk of Biped Locomotion,” International 
Journal Robotics Research, Vol. 3, No. 1, pp. 60-74, 1984. 
10. J. Furusho, and A. Sano, “Sensor-Based-Control of a Nine Link Biped,” International 
Journal Robotics Research, Vol. 9, pp. 83-98, 1990. 
11. Y. F. Zheng, and F. R. Sias, “Design and Motion Control of Practical of Biped Robots, 
“International Journal Robotics and Automation, Vol. 3, No. 2, pp. 70-77, 1988. 
12. Ching-Long Shih, and William A. Gruver, “Control of a Biped Robot in the 
Double-Support Phase,” IEEE Transactions on System, Man, and Cybernetics, Vol. 22, 
No. 4, pp. 729-735, 1992. 
13. Ching-Long Shih, “Gait Synthesis for a Biped Robot,” Robotica, Vol. 15, pp. 599-607, 
1997. 
8. 計畫成果自評 
  
 本計畫於第一年中我們完成了 Virtools 這套軟體的虛擬平台建置，並利用自行研製的
機器人專屬 Building Block(BB)，透過這個 BB 我們可以將機器人當成是虛擬平台的物件，
透過 BB 中的輸入輸出介面可以操控機器人之運動行為。我們可利用無線網路將機器人的
障礙物資訊傳回虛擬平台，並利用虛擬平台的事件觸發程式下達機器人的動作命令。於本
年度我們已完成這些介面的建置，另外還包含具有網路傳訊的 BB，該 BB 可用於兩個電腦
平台間的訊息傳遞。 
 
 另外，我亦完成了二足機器人所需的多軸控制模組，我們研製了一個具有 500 瓦以上
驅動能力的 PWM 驅動型的直流馬達驅動器，並完成一個五軸的定位控制模組。我們將於
第二年將本年所開發的單元模組進一步整合在一起，完成二足機器人控制之目標。 
 
 11
 
可供推廣之研發成果資料表 
 
□ 可申請專利  ■ 可技術移轉                                      日期：97年9月26日 
國科會補助計畫 
計畫名稱：機器人系統於虛擬實境之系統整合與應用 
計畫主持人：李文猶         
計畫編號：96－2221－E －262－012   學門領域：控制 
技術/創作名稱 可用於 Virtools 平台之擴充介面 Building Block 建置技術 
發明人/創作人 李文猶 
中文： 
本項技術係為 Virtools 這個虛擬實境軟建立對外的連結技術。亦即
擴充可以整合於虛擬實境的週邊介面，例如利用建置 Building 
Block 我們可將操控機器人的控制碼植入 Building Block 內，再利
用 Virtools 這個虛擬實境平台的流程控制設計器將機器人的行為設
計於 Virtools 這個虛擬實境平台的事件處理流程中，當不同的事件
發生時可以透過機器人的 Building Block 經由無線網路來控制機器
人之運動行為 技術說明 
英文： 
This technique is about the virtual reality tool Virtools’s extension
interface.  We used the DLL module to develop the Building Blocks 
of the Virtools, which can be use to control a robot system or other 
hardware interfaces.  We can take the Robotics Building Block as a 
virtual reality module and use the event driven method to send control 
motion command to Robotics system through serial communication 
port or land port.    
可利用之產業 
及 
可開發之產品 
本項技術除了可以應用於一般的機器人場合或虛擬實境機電整合
場合，因已將串列通訊與網路技術加入 Building Block 內，其功能
與業界開發之輸入輸出介面之性能相當。 
技術特點 
利用 DLL 開發 Virtools 之 Building Block 可以在任何平台下運作
推廣及運用的價值 應用於一般的機器人場合或虛擬實境機電整合場合，可進行虛擬實境所需之相關硬體輸入輸出介面之開發 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位
研發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
2二. 與會心得
此會議是由台灣的 Chinese Automatic Control Society 組織所主辦，同時辦理了
CACS 2007 Conference 的 International Automatic Control Conference 與國科會的
2005 年計畫發表，因此參與此會議可以獲得相多相關的研究情報，同時由於議相
當的廣泛，因此對於聆聽相關論文發表的期待與規劃便早早開始。在 CACS 2007
的會場中大致可將其主要的議題區分為 Biomimetic Snake Locomotion and Its
Application to Advanced Platform Actuation Systems、Neural Networks、Robust
Control、Hands-on Contest、Design of multi-function intelligent nursing systems、Fuzzy
Systems and  Control、On the Study of Digital Homecare System、Design and
Application of Physiological Signal-Based Brain-Computer Interface— A Novel
Approach to Control Machine、Intelligent Systems in Ovarian Cancer with Microarray
Data for Molecular Evolution and Control、Image Processing and Control、Mobile
Robots、Robotics and Automation in Medical Application 以及 Intelligent Control 等分
組，綜合分析這些研討會的議題包含了適應性學習系統、電腦與人之互動、影像
與訊號處理、類神經網路控制、智慧型機器人與自動代理、模糊控制、家庭照護、
安全與保密系統、移動式機器人、以及智慧型控制等。如此多樣的議題同時在 3
天內進行，雖會有因同時間發表的遺珠之憾，但由於議豐富所以截長補短，整體
而言，參與此一會議獲益良多。
事實上，在會中還有多個專題演講也相當地精彩，其中 Prof. Atsuo Takanishi，
以Humanoid Robot Research and Its Applications為題針對人型機器人的研究與其應
用進行一個精彩的演說，另外 Jun Ho Oh 也以類似的題目 Humanoid Robot: A
Machine That Walks 探討人型機器步行的相關技術演進與成果。而 Prof. Zoran Gajic
則是 Power Control in Wireless Communication Networks: Challenges for Control
Engineers and Accomplished Results 為題分析不同的控制模式以及參數矩陣來使手
4的參考。例如相關於遠端除錯的 SOC/SOPC 系統想法，他們是利用 USB 與網
路以 JTAG 這些連線方式來設計 SOC/SOPC 的開發系統，這倒可以做為我們進
行 FPGA 設計時的一些參考。
2. 在智慧型家庭的議題上則是討論有關於無線感測網路、RFID 的應用，如何將一
個家庭內的設施自動化，遠距視覺化等。這些研究議題大概都是討論如何利用
現有的 PDA、手機、家電上網與 RFID 應用，結合網路將所有資料匯整在一起
後進行監控。
3. 在機器人的探討上則是以移動機器人為主，有輪型、二足以及四足機器人。每
個研究者所提出的系統架構與控制方法均不相同，值得我們參考與學習。
4. 在智慧型控制上的議題有模糊控制、類神經網路、適應性控制、強健控制以及
機器智能等。這些議題有實作亦有理論上的推演，雖然並不是所有的研究成果
都可應用在我們目前的研究題目上，但也提供了一些想法與方法，應有助於我
們的研究發展。
三. 建議
1. 本次能獲國科會之註冊費與交通費之補助，實感銘謝。由於國科會的補助方能
使得國內的學者可以參與這類的國際研討會並與其他國家的精英切磋學術研
究，藉由這樣的協助不但可以使國內的學者更上一層樓，亦可加速國際化的腳
步，對國家的助益著實良多。
2. 參加此類國際會議，除了參與者受益外，也使得參與者可以將與會的經驗，帶
回校內，並可加強本校教師在舉辦國際性及區域性的研討會，有一良好的參考
Proceedings of 2007 CACS International Automatic Control Conference
National Chung Hsing University, Taichung, Taiwan, Nov. 9-11, 2007
Image Guided Biped Robot Implementation
Wen-Yo Lee
Department of Computer Information
and Network Engineering
Lunghwa University of Science and
Technology
Taoyuan, Taiwan, ROC
TrisanWYLee@mail.lhu.edu.tw
Ching-Long Shih
Department of Electrical Engineering
National Taiwan University of
Science and Technology
Taipei, Taiwan, ROC
shihcl@mail.ntust.edu.tw
Tai-Feng Lin
Department of Electrical Engineering
National Taiwan University of
Science and Technology
Taipei, Taiwan, ROC
M9407216@mail.ntust.edu.tw
Abstract— the proposed paper is based on research on a
walking biped robot which can use a CMOS sensor to
measure the actual distance from itself to a target in its
sight. In terms of today’s technology, an intelligent robot
should be able to avoid running into obstacle along its de-
sired path. After the biped robot obtains its precise dis-
tance from the obstacle, it would walk to the front of the
obstacle and then kick the obstacle away. The biped robot
control system is an embedded kernel of NIOS FPGA sys-
tem, which including an inverse kinematics module, a
walking trajectory generation module, an image process-
ing module, as well as a mark-based distance measurement
module.
Keywords — Biped Robot, Image Guided Machine, NIOS,
Embedded Control Module
I. INTRODUCTION
People have been developing biped robots for the
past several decades.  In 1997, Gollidary and Hemani[1]
used the Langrangian dynamic equations to develop the
kneeless biped motion machine modle. Miyazaki and
Arimoto [2] applied the singular perturbation method to
model two types of dynamic behavior of the biped robot,
fast mode and slow mode. Examples for hand-on
practices are plenty: Kato and Takanishi built the WL-*
series biped robots [3], Miura and Simoyams developed
the Biper-* [4], Furusho and Sano created the BLR-*
biped robot [5], Zheng and Sias proposed the SD-*
biped robot [6], etc. A number of tech giants have been
developing of more sophisticate biped robots, such as
SONY (SDR), KAWADA INDUSTRIES (HRP-2P and
Isamu), HONDA (ASIMO). In further developments,
Yasuda used a PC-based controller and  PIC
microcontroller to construct a distribution motion
control system to drive a  mobile robot in 2000 [7];
Hannan built an intelligent robot with FPGA
implementation [8]. In most cases image sensors have
been used as the biped robots’ feedback sensors [9]:
Messom had introduced a vision tool-kit for humanoid
robots [10]; some applications that CCD or CMOS
sensors on a moible robot to detect obstacles in the
environment.
This paper chooses KONDO’s KHR1 biped robot as
the the machine of implementation. KRS-786ICS servo
modules are used as joints for the KHR1.  The
KRS-786ICS servo modules are controlled by RCB-1,
which can be commanded through RS232. That is, one
could send the joint angle commands to RCB-1 to
control KRS-786ICS, serving to keep each joint in
position.
A NIOS FPGA module is applied in the
development of this image guided biped robot control
systems, use a FPGA development board. The other key
modules implemented are the inverse kinematics module,
the working trajectory generation module, the image
processing module, and the mark-based distance
measurement module on a Stratix FPGA chipset [11].
After the integration of the FPGA-based control system,
the KHR1 biped robot can walk on flat horizontal planes
and avoid obstacles along its path.
II. SYSTEM ARCHITECTURE
The image guided biped robot system can be di-
vided into 3 parts: the KHR1 biped robot (body), the
CMOS image module (eyes), and the FPGA-based
controller (brain).  The biped robot system is shown in
Fig. 1. A I2C communication module, designed with the
FPGA development board, is used to connect the biped
robot with the CMOS image module.  The advantage of
using the FPGA to get images is faster the image proc-
essing.  A three-stage image processing algorithm has
been implemented on the Stratix FPGA chipset to de-
termine target position.  The first stage is preprocessing,
which transforms the colored images into gray-scale
images and eliminates noises. The second stage includes
setting the threshold of the matching process (the next
stage) and transforming the image into a two-valued
image.  Finally, a pattern matching process is used to
find the marks and obtain the target position.
Fig. 1. The biped robot system.
Proceedings of 2007 CACS International Automatic Control Conference
National Chung Hsing University, Taichung, Taiwan, Nov. 9-11, 2007
The biped robot’s inverse kinematics has been in-
plemented in the FPGA chipset.  Since the arms and legs
of the biped robot can be divided into independent parts,
the inverse kinematics equations have been derived
separately and hence easily. Then one should provide the
biped robot a walking gait. In this study, a cycloidal
contour is adopted as the biped robot’s walking gait
trajectory.  The biped robot walks statically stable by
keeping the vertically projected point of the center of
gravity inside the support foot.  The FPGA-based biped
robot control system is shown in Fig. 2.
III. THE INVERSE KINEMATICS OF THE BIPED ROBOT
The joints and links of the KHR1 biped robot are
defined as in Fig. 3. A plane geometry method is applied
to solve the inverse kinematics problem of the biped
robot’s legs. This method maps the three-dimensional
kinematics system onto the two-dimensional space, so it
can be viewed just like a 3-axis manipulator on a plane. 
The geometry figure mapping idea, featuring one leg of
the robot, is shown in Fig. 4.
Fig. 4. Plane geometry method.
To simplify the derivation, a constraint is given: the
waist and ankle of the biped robot lie vertical to the
ground plane. The results of the inverse kinematics
analysis are shown in the following equations,
26
261
2 tan zz
yyq , (1)
and
2
36
2
36 zzyya . (2)
Looking at the biped’s mechanical architecture, it can be
easily shown that
0
2
3
3 x
z
x
new
, (3)
and
azz newnew 36 ,
66
6
5
5
rz
x
z
x
newnew
.
Finally, the biped’s joint angle according to the foot
position can be represented as below,
54
2
5
2
4
2
1
4 2
cos
rr
rrdq (4)
53
531441
5 tan
sin
sin
newnew zz
xx
d
qr , (5)
where
454 q , (6)
and
2
35
2
35 newnew zzxxd , (7)
65
54
43
6
5
4
3
2 q
q
q
q
q
. (8)
Using this method, the inverse kinematics equations of
the biped robot’s arms and legs can be easily solved.
IV. THE GAIT PLANNING
Cycloidal contour trajectories are used to plan the
motion trajectories of the biped’s waist position (x0, y0,
z0), right foot (x7, y7, z7), and left foot (x14, y14, z14).  For
example, the motion trajectory equations for the right leg
can be expressed as
s
ss
f X
T
tt
T
ttX
tx 007 2sin22
)( , (9)
s
s
f Z
T
ttH
tz 07 2cos12
)( , sTttt 00 ,
(10)
where Xf is the length of each stride, Hf is the maximum
height the biped’s foot lifts up to, t0 is the initial time, Xs
is the initial position along the x-axis, Zs is the initial
position along the z-axis, and Ts is the cycloidal  period.
The biped robot’s center of gravity can be written
as
24
23
21
19
17
15
13
9
6
2
17
15
21
19
24
23
13
9
6
2
i
i
i
i
i
i
i
i
i
i
i i i
iiiiii
i
ii
i
ii
mmmmm
xmxmxmxmxm
COGx
(11)
24
23
21
19
17
15
13
9
6
2
17
15
21
19
24
23
13
9
6
2
i
i
i
i
i
i
i
i
i
i
i i i
iiiiii
i
ii
i
ii
mmmmm
ymymymymym
COGy
(12)
and
24
23
21
19
17
15
13
9
6
2
17
15
21
19
24
23
13
9
6
2
i
i
i
i
i
i
i
i
i
i
i i i
iiiiii
i
ii
i
ii
mmmmm
zmzmzmzmzm
COGz
. (13)
These equations are used to implement the biped robot’s
Proceedings of 2007 CACS International Automatic Control Conference
National Chung Hsing University, Taichung, Taiwan, Nov. 9-11, 2007
equation; it is the desired vision angle for the biped robot.
The distance-pixel relationship equation is repressed by
PP eeD 0035.0017.0 7.4617.171 , (14)
where P is the pixel value the targeted obstacle is located
at, and D is the distance between the biped robot and the
obstacle .
The term P in particular needs to be calibrated with
a mark. Here, a mark is placed at 180mm in front of the
biped robot, to help calibration. The mark’s pixel loca-
tion measured at the vision angle of 30 degrees is at
pixel= 273 in the image.  Now, P can be written as
)273( MpBpP , (15)
where the Bp is obstacle’s pixel location and Mp is the
mark’s pixel location. The real obstacle distance can
then be calculated using Eq. (14).
Fig. 7.  COMS sensor distance-pixel relationship
VI. EXPERIMENTS
Fig. 8 shows how the final experiment was setup. 
A ball along with a mark was placed on the walking path
and the mission of the biped robot in this experiment was
to locate the ball precisely and kick the ball away pre-
cisely.   The distance from the road mark to the biped
robot was 180mm as stated before, while the ball’s po-
sition could be selected arbitrarily. The default vision
angle was 30 degrees also as stated above. To begin, the
biped robot retrieved an image with the COMS sensor
and calculated the actual position of the ball relative to
itself. The image processing result are displayed in Fig.
9. In the image, Y represents pixel location.  The mark
was located at of pixel= 278, where the vision angle
distortion is -5 degrees.  The ball was located at pixel=
96. The distance from ball to the biped robot is then
373.7 mm.
Fig. 8. The experimental setup.
Then the biped robot used the calibrated distance to
generate its walking trajectory and followed the trajec-
tory to the front of the ball. As soon as the biped robot
reached the ball, it kicked the ball away. Thanks to the
system’s image calibration feedback, the biped robot did
not miss kicking the ball off its path every single trial.
Fig. 9. The image processing result.
The biped robot walking experiment’s continuous
picture sequence is shown in Fig. 10. In Picture 2 of the
figure, the biped robot bends forward from the waist up
to retrieve the image of the path. Next, the biped robot
walked toward the ball, and kicked the ball off the path,
(see Picture 12 in Fig. 10).
1Image Guided Biped Robot
Implementation
*Wen-Yo Lee , Ching-Long Shih and Tai-Feng Lin
*Lunghwa University of Science and Technology
National Taiwan University of Science and Technology
Taiwan, R.O.C.
Outline
Motivation
System Design
Kinematics
Gait Planning
Image Processing
Experiment Demonstration
Conclusion
Motivation
Approach a Low Wiring Working Robot
Use the Image Sensor to Avoid Running
into Obstacle
Simplify the Electrical Hardware
Implement a Humanoid Robot with
Vision Ability on a Programmable Chip
Former Research
Kato and Takanishi built the WL-* series
biped robots
Miura and Simoyams developed the Biper-*
Sung-Ui Lee and Kab-ll Kim they Used DSP
and FPGA to Design a Biped Robot
The Sophisticate Biped Robots, such as SONY
(SDR), KAWADA INDUSTRIES (HRP-2P and
Isamu), HONDA (ASIMO)
3The Gait Planning
Cycloidal Contour Trajectories
COG of the Robot
s
ss
f X
T
tt
T
ttXtx 007 2sin22
)( s
s
f Z
T
ttH
tz 07 2cos12
)(
sTttt 00 ,
24
23
21
19
17
15
13
9
6
2
17
15
21
19
24
23
13
9
6
2
i
i
i
i
i
i
i
i
i
i
i i i
iiiiii
i
ii
i
ii
mmmmm
xmxmxmxmxm
COGx
24
23
21
19
17
15
13
9
6
2
17
15
21
19
24
23
13
9
6
2
i
i
i
i
i
i
i
i
i
i
i i i
iiiiii
i
ii
i
ii
mmmmm
ymymymymym
COGy
The Motion Trajectory
Simulation
COG Simulation and Experiment
1               2           3               4                5              6 7 8 9 10
Image Recovery and Gray
Scale Conversion
Bilinear Interpolation
BGRY 114.0587.0299.0
Gray Scale Image
PAS106BCB283 COMS Layout
5The Experimentation Conclusions
A single Stratix FPGA can be used to implement
a complete control system for a 16-joint biped
robot
Inverse kinematics functions
Gait planning functions
Image process functions
We may use the natural landmarks to teach a
humanoid robot to walk in the right direction
It is easy for an embedded-based biped robot
controller to be implemented on a SOPC
Q & A Thank You
