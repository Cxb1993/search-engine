existing opening book, to judge the goodness and 
weakness of all moves in the opening book, and to 
promote the accurate rate of the opening book.  
 
This project main focuses on the management of 
opening books for the reason that it is a common 
issue for many chess-like games. Because there are no 
pieces captured in the opening phase, it is very 
difficult to judge the board situation by utilizing 
search algorithms. That is, a game-playing program 
cannot judge the quality of each move and therefore 
cannot make an excellent move because it has to 
balance the search depth and the accurate rate of 
evaluation functions. To overcome the above problem, 
an opening book for chess-like games is used, which 
is usually constructed by human-assigned moves, 
collections of massive game records, fine tuning 
manually by masters, such that programs have move 
sequences to follow in the opening phase.  
 
This project mainly applies alpha-beta search, proof-
number search and their extended algorithms to VCS to 
achieve the effect of verify the quality of an 
opening book rapidly and to let game-playing programs 
have more information to refer to. Besides, the 
information can also be utilized to judge the 
goodness and weakness of all the moves in an opening 
book. The strength of computer Chinese chess will 
improve when the weakness in the opening phase are 
extended.  
 
We finished the goals of this project including (1) 
collection and adjustment of existing game records 
and the opening book, which is then transformed to be 
a structure suitable for operating under the VCS 
environment, (2) applying the alpha-beta search 
algorithm to map-reduce system under the VCS 
environment, and then to integrate the result with 
our opening book, and (3) combining our research 
result of the opening book and the game-playing 
program under the VCS environment. Furthermore, the 
Chinese chess program, CHIMO, rewarded the silver 
1 
 
 
行政院國家科學委員會補助專題研究計畫 
■ 成 果 報 告   
□期中進度報告 
 
適用於電腦對局遊戲之志願型計算系統及其應用問題 
-- 運用志願型計算系統建構象棋開局庫 
 
計畫類別：□ 個別型計畫  ■ 整合型計畫 
計畫編號：NSC 99-2221-E-033-036-MY3 
執行期間： 99 年 8 月 1 日至 102 年 7 月 31 日 
 
計畫主持人：陳志昌 助理教授 
共同主持人：許舜欽 教授 
計畫參與人員： 曾汶傑、林柏瀚、周政緯、范綱宇、莊惟程、佘博玄 
 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  ■完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
■出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
          
執行單位：中原大學應用數學系 
 
中   華   民   國  102   年  9  月  6  日
II 
 
Keywords: Volunteer Computing, Chinese Chess, Opening Book, Alpha-Beta 
Search 
 
Huge computation and memory resources are required for many computer games and 
their applications, for instance, the opening book construction for Chinese chess. The 
volunteer computing system (abbr. VCS) can utilize a lot of idle computation 
resources. However, the existing VCS cannot apply to the opening book construction 
directly. Therefore, the main research goals of this project are to collect and adjust the 
existing opening book, to judge the goodness and weakness of all moves in the 
opening book, and to promote the accurate rate of the opening book.  
 
This project main focuses on the management of opening books for the reason that it 
is a common issue for many chess-like games. Because there are no pieces captured in 
the opening phase, it is very difficult to judge the board situation by utilizing search 
algorithms. That is, a game-playing program cannot judge the quality of each move 
and therefore cannot make an excellent move because it has to balance the search 
depth and the accurate rate of evaluation functions. To overcome the above problem, 
an opening book for chess-like games is used, which is usually constructed by 
human-assigned moves, collections of massive game records, fine tuning manually by 
masters, such that programs have move sequences to follow in the opening phase.  
 
This project mainly applies alpha-beta search, proof-number search and their 
extended algorithms to VCS to achieve the effect of verify the quality of an opening 
book rapidly and to let game-playing programs have more information to refer to. 
Besides, the information can also be utilized to judge the goodness and weakness of 
all the moves in an opening book. The strength of computer Chinese chess will 
improve when the weakness in the opening phase are extended.  
 
We finished the goals of this project including (1) collection and adjustment of 
existing game records and the opening book, which is then transformed to be a 
structure suitable for operating under the VCS environment, (2) applying the 
alpha-beta search algorithm to map-reduce system under the VCS environment, and 
then to integrate the result with our opening book, and (3) combining our research 
result of the opening book and the game-playing program under the VCS environment. 
Furthermore, the Chinese chess program, CHIMO, rewarded the silver medal in the 
Computer Olympiad tournament in Yokohama, Japan, 2013. 
  
IV 
 
圖目錄 
 
圖 1 對稱盤面.............................................................................................. 6 
圖 2 落著產生迴圈的修改方式.................................................................. 8 
圖 3 問題盤面.............................................................................................. 9 
圖 4 問題盤面............................................................................................ 10 
圖 5 棋盤座標示意圖................................................................................ 13 
圖 6 原始開局樹........................................................................................ 16 
圖 7紅方樹................................................................................................. 16 
圖 8黑方樹................................................................................................. 17 
圖 9 MPN 選取示意圖 ............................................................................... 17 
圖 10展開 MPN ......................................................................................... 18 
圖 11更新 Proof Number與 Disproof Number ........................................ 18 
圖 12初始 AND/OR 樹 ............................................................................. 19 
圖 13選取第一個 MPN ............................................................................. 19 
圖 14更新為 Virtual Loss .......................................................................... 19 
圖 15選取第二個MPN ............................................................................. 20 
圖 16 Hard Abortion 範例 .......................................................................... 26 
圖 17 Soft Abortion 範例 ............................................................................ 27 
圖 18 Speedup ............................................................................................. 28 
圖 19 Efficiency .......................................................................................... 28 
 
  
1 
 
（三）報告內容 
 
一、 前言 
 
第一年研究成果，由蒐集棋譜來建構開局庫，並統計此開局庫內部的各種落
著實際勝負數據，依此些資料來對開局庫處理分析，如分析強弱連結等。最終將
原始的資料轉為適用於志願型計算系統之開局庫系統單機版本。第二年則將上述
的電腦象棋開局庫單機版本，實際應用於網路版本，並研究 SSS* Search、
AB-SSS* Search、AB-Dual Search 等 Alpha-Beta Search 衍生演算法，實作應用於
志願型計算系統。第三年利用前兩年的研究經驗與成果，繼續研究第二年演算法
之延伸做法，解析其效率，最後再分析開局庫與中局程式整合所能得到的成效。 
 
二、 研究目的 
 
整體研究目標為，整合現有開局庫資料，並研究資料與各類演算法應用至志
願型運算系統。將兩者合併整理，研究分析將電腦象棋中局程式應用於開局庫中，
所能取得的效能。具體項目如下： 
 
 收集及研究搜尋演算法之相關資料。 
 收集、整理現有開局庫。 
 研究設計適用於志願型計算系統之開局庫系統。 
 研究應用於志願型運算系統之開局庫之演算法。 
 整合現有資料庫與演算法。 
 檢視、分析演算法效能。 
 分析所產生之開局庫。 
 應用中局程式分析開局庫。 
 
整體而言，本計畫之主要工作項目包括整理現有開局庫、研究設計並實作適
用於志願型計算系統之開局庫系統、把搜尋演算法應用於 MapReduce 系統、整
合現有資料庫與搜尋演算法的成果、分析開局庫效能、將開局庫應用於中局搜尋 
  
3 
 
 
 α < g < β 
 g為 game tree中實際的 minimax 值。 
 g < α 
 g為 game tree的 minimax 值之上界（upper bound）。 
 g ≥ β 
 g為 game tree的 minimax 值之下界（lower bound）。 
 
3.3 SSS* 
 
SSS*為 George C. Stockman於 1979年所提出，是一種搜尋固定深度Minimax 
tree的演算法[14]。SSS*為一種 Best First Search，因此搜尋相同的 game tree時，
需要評估的 Leaf Node 也會較 Alpha-Beta 少[1][14]。儘管 SSS*有此好處，但其卻
有以下數種缺點造成使用上的困難[1]。其一為演算法非常複雜難懂，除此之外，
執行 SSS*所需的記憶體亦非常龐大。造成使用上最大推力乃是因其效能上的問
題，由於 SSS*需要不斷維護一個有序的OPEN list以此來達成Best-First的效果，
而持續維護 OPEN list變造成 SSS*效能上的低落。 
 
因以上種種缺點，Plaat 與 Schaeffer 等人於 1994 年提出了一種改良 SSS*的
演算法，其名為 AB-SSS*[1]。 
 
3.4 AB-SSS* 
 
AB-SSS*為 Aske Plaat、Jonathan Schaeffer 等人於 1994年提出[1]，是一種改
良 SSS*的演算法，其中利用了 Alpha-Beta Search、Fail-Soft[39]、Transposition 
Table[24]與 Zero Window 等方法。雖然 Alpha-Beta Search 為 Depth-First Search
的演算法，但配合 Zero Window、Transposition Table，便能達到 Best-First Search
的效果[1]。 
 
 AB-SSS*使用Zero Window不斷重複呼叫Alpha-Beta Search，並配合Fail-Soft
修改 Alpha 值，直到 Alpha 值收斂為止。而 Alpha 的起始值為正無窮大，由於
Alpha-Beta Search 的特性，其值只會遞減，直到收斂於 MiniMax 值[1]。 
  
5 
 
四、 研究方法 
 
4.1. 收集、整理現有開局庫 
 
 本計畫收集網路上國內外象棋名手之對弈棋局，目前共計六十八萬餘筆象棋
開局譜、中局譜、殘局譜、對局譜等，乃為本計畫最佳素材。此六十八萬餘筆資
料，經處理與分析後以既定格式，完整地保存下來，以方便後續使用。 
 
 概觀現今世界上有名之電腦象棋對局程式所採用的開局庫，主要以棋局結束
結果的「勝負局數」的數量作為統計數據，用來評判盤面著手優劣，再輔以象棋
大師的專家知識進行人工微調。其主要方式可分項如下： 
 
1、下此著手後之勝局數、負局數、和局數。 
2、相同節點之子節點之間，彼此相對的著手次數。 
3、節點之子孫節點總數。 
 
然而，棋局結束盤面的「評分」，並沒有以 Mini-Max 方式回溯計算來提供
開局庫實際使用這項資訊。這是因為以蒐集棋譜的方式，會造成開局盤面的著手
選擇數量種類不完整，進而影響 Mini-Max 後分數的正確性。 
 
 本項目欲以現有開局庫為基礎，尋找需要再加以增添著手、重新排序著手優
劣順序的節點。方式是以系統化的方式，先以多個中局對局程式為現有開局庫的
葉節點（leaf node）進行審局評分，再以 Mini-max 方式整理所有內部節點（internal 
node）的分數。然後，比較節點中 Mini-max 與勝負局數統計資料的差異，將不
一致的節點選擇出來，作為志願型計算系統下需要特別判定的樣本，以及必須優
先展開的節點。 
 
4.2. 研究設計適用於志願型計算系統之開局庫系統 
 
此小節將介紹如何處理象棋開局庫中的盤面與走法，以及一些會遭遇到的問
題，譬如迴圈盤面。除此之外，也會說明後續將應用到的資料格式。還有電腦程
式中，對於盤面的評估方法與分類。 
  
7 
 
在儲存棋譜的資料庫內，盤面都以 64-bit 的 Zobrist Key 來代替。其實作方
法為，設立內容為固定亂數的三維陣列，對應棋盤、棋子與棋子顏色，另外再依
不同的落子方給予一組亂數。其變數設置如下： 
 
變數名稱 變數作用 
board_key[2][7][90] 紀錄固定亂數 
extra_key 分辨盤面輪替 
表 2 Zobrist Key之變數設置 
 
board_key[2][7][90] 儲存固定的 64-bit 亂數，對映棋子以及棋子在棋盤上的
位置，extra_key 則用來分辨盤面是輪替到紅方抑或是黑方落著。在取得盤面所
對應的 Zobrist Key 時，需以盤面上棋子位置來獲得 board_key[2][7][90] 中的亂
數，與輪替紅方與黑方的不同亂數 extra_key ，並對以上所有得出的數字做 XOR
運算，即可得出 Zobrist Key。 
 
4.2.2. 落著 
 
除了儲存盤面之外，需另外對每個盤面的合法步作處理，紀錄方法為父盤面
之 Zobrist Key 與子盤面 Zobrist Key 以及連接兩個盤面的落著，可以顯示如表 
3。 
 
變數名稱 變數作用 
ParentKey 父盤面之 Zobrist Key 
SonKey 子盤面之 Zobrist Key 
Move 落著 
表 3 落著紀錄之變數設置 
 
由於記錄了父盤面與子盤面的 Zobrist Key，對於左右對稱的落著，也可以藉
由檢查 Zobrist Key 來避免多次記錄等等錯誤的可能性，而同時記錄父盤面與子
盤面，亦能便利於後續的使用、查詢以及除錯。對於有可能造成迴圈的棋譜，將
會對產生迴圈的落著刪除，並將後續落著接續儲存。效果如圖 2。 
 
棋譜的落著依序造成 ABCDAE盤面，如圖 2 (a)。在紀錄棋局的過程，為了
避免迴圈，會將原始的 ABCDAE 改為圖 2 (b)的方儲存，以此儲存方法來排除儲
存資料內的迴圈。 
 
9 
 
4.2.4. 強弱連結 
 
若是單純只使用勝、負以及和局的數字來做為使用的依據，非常容易會造成
誤判，或是掉入陷阱而不自知的結果。如                           (b) 
圖 3 (a)之盤面，統計資料為黑方勝 225 局、敗 103局以及和 34局，單看這
三個數據很容易會認為                           (b) 
圖 3 (a)之盤面對黑方而言是勝率極大的優良開局盤面，但經過兩回合的落
著轉為                           (b) 
圖 3 (b)之盤面時卻變成黑方勝 35局、敗 74 局以及和 18局，變成對黑方非
常不利的盤面。 
 
   
(a)                            (b) 
圖 3 問題盤面 
 
                           (b) 
圖 3 (a)至                           (b) 
圖 3 (b)之落著： 
1.  象７進５  
2.  傌五進七  
3.  卒７平６  
4.  仕六進五 
 
會造成如此情況，乃是因為在第 3步也就是                          (b) 
圖 4(a)的盤面，紅方的落著有仕六進五與炮五平二兩個分歧，由於大多數
11 
 
 
1. 刪除統計資訊數量過少的盤面： 
由於需配合後續的處理，故要謹慎選擇下界。若是下界設定過低，雖然可以
保留較多盤面，卻可能會造成篩選過後所留下的盤面變得更少，導致最後得出的
盤面勝、負以及和的數據過低。若是下界設定太大，亦會刪除過多的盤面，造成
最後得出的數據不符合需求。 
 
2. 排除不具參考價值的落著： 
為了確保盤面統計資訊的可信程度，此一步驟必須非常嚴謹。每一個盤面皆
有多種可能的落著，故而判斷盤面時應由其盤面的所有子盤面給予評估。估計方
法由子盤面的勝率、和率以及勝、負、和的總和來決定，依以上數據給予估計值
(後稱 Rank)，並依 Rank 排序所有落著與子盤面，保留其中 Rank 值高的作為待
估計盤面的統計資訊。 
 
3. 由刪除後餘下的所有 Leaf Node盤面開始，一層層向上重新計算： 
經過前兩個步驟後，將保留下的盤面資訊都當成可信任的開局，由現存的
Leaf Node也就是沒有子盤面的父盤面，開始重新往上計算所有盤面的勝、負以
及和的數量。 
 
計算方法為： 
1. 由 Level 最大的盤面開始，找出所有能由可信任之落著連結到的 Leaf 
Node。 
2. 加總 Leaf Node 的勝、負以及和的數據。 
3. 找出 Level – 1的盤面。 
4. 對第 3步驟所找出的盤面，重複 1至 3步驟。 
 
重新計算完所有保留下的盤面後，便可以得到準確的勝敗資訊，不會再產生
誤入陷阱的情況。但此部分算完後，所有的資料只是盤面資訊以及勝敗資訊，較
不易對儲存的資訊做處理或應用，因此將會把現有的資料轉換格式，改變為能提
供程式編輯與應用的開局庫資訊。 
 
4.2.5. 輸出盤面與落著資料 
 
為了便利於編輯使用開局，將使用 Smart Game Format (後皆稱 SGF)來輸出
現有的所有盤面，並將處理過強弱連結的勝負資訊一併加入 SGF 檔案之中，如
此在後續使用時就免除了回到資料庫查詢的手續。 
 
13 
 
1. 位置： 
象棋之棋盤橫向共九格，縱向共十格，為了使用以及查看的便利性，將會使
用二維的方法來表示棋盤位置。表示方法如下圖 5 (a)，橫向座標由左至右，縱
向座標則由上至下。皆從 1開始，橫坐標至 9結束，縱向座標為了保持位數單一，
故採用 16進位來表示，所以座標由 1至 A為止。綜合的表示法，為先寫出縱向
座標再寫出橫向座標，並用大括弧將棋子的位置包含。如圖 5 (b)的帥，依上述
方法可表示為{A5}。 
 
(a)  (b) 
圖 5 棋盤座標示意圖 
 
2. 棋子顏色： 
棋子的顏色分為紅與黑兩種，故各給予一個代號，如表 5所示： 
 
棋子顏色 代號 
紅色 R 
黑色 B 
表 5 棋子顏色代號 
  
15 
 
4.2.6. 中局程式之分數分類 
 
開局庫的盤面會經過中局程式計算，並給予評估分數。為了能有較為明確的
局勢評估，這裡會把中局程式回傳的分數做區隔，給各個分數區間不同的定義。
以下是分數與局勢評估的對應表格： 
 
分數 評估 
10000 以上 必勝 
10000 至 800 極優 
800 至 400 大優 
400 至 200 優 
200 至 100 小優 
100 至 40 略優 
40至- 40 均勢 
- 40至- 100 略劣 
- 100至-200 小劣 
- 200至- 400 劣 
- 400至- 800 大劣 
- 800至-10000 極劣 
- 10000以下 必敗 
表 7 中局程式審局分數分類 
 
4.2.7. 依落著方區分開局庫 
 
開局庫中的盤面，幾乎都有複數個可信任的落著，但是在正式對弈時往往只
會使用其中一個著法，多餘的落著易容易造成形勢的誤判，故把建好的開局庫
SGF檔，依照不同的落著方，分為紅方樹與黑方樹。 
 
以下以紅方樹舉例，其條件如下： 
1. 由初始盤面開始處理。 
2. 若盤面的落著方為紅方，則僅保留 Rank 值最高的著法。 
3. 若盤面的落著方為黑方，則保留所有著法。 
  
17 
 
 
圖 8黑方樹 
 
4.3. 研究應用於志願型運算系統之開局庫之演算法 
 
此小節將 Proof Number Search 以及 Alpha-Beta Search 等演算法應用至志願
型計算系統上，並解釋其執行過程。 
 
4.3.1. Proof Number Search 
 
Proof Number Search利用 Proof Number與 Disproof Number尋找最佳的證明
節點Most Proving Node，後簡稱 MPN，再針對 MPN做向下展開的動作。而尋
找最佳的證明節點此一動作，若能改良為平行處理以用於志願型計算，便不需等
待向下展開所花費的時間，能減少整體的證明時間。 
 
 
圖 9 MPN選取示意圖 
 
19 
 
 
圖 12 初始 AND/OR 樹 
 
 
圖 13選取第一個 MPN 
 
 
圖 14更新為 Virtual Loss 
 
21 
 
 
依照此方法修改，除了能分辨新Node的狀態，亦有助於搜尋Most Proof Node，
故對於志願型計算的 Proof Number Search 也有幫助。 
 
4.3.2. MTD(f) 
 
MTD(f)演算法為 Best-First Search，僅須展開最少量的 Node就能完成開局庫
的驗證。除此之外，AB-SSS*與 AB-DUAL*亦有許多優點，譬如利用 Transposition 
Table紀錄各個 Node的分數上下界，以此來避免掉重複運算的可能。 
 
但由於原始的 AB-SSS*與 AB-DUAL*並無法運用於志願型計算系統上，因
此將演算法擴充，像是修改選點方法、分離更新方法等等，將其調整為適合於志
願型計算系統的演算法。 
 
 本子計畫之演算法運用於志願型計算系統，但未必隨時都有空閒的運算資源
可使用，因此為了使每個運算資源都能有效運用，應該要盡量找出最需要得知結
果的 Node來做運算。 
 
 將 AB-SSS*演算法處理擴充後，可以分為以下兩部分： 
 
 第一部分為選擇要運算的 Node，其步驟如下： 
1. 選點 
2. 傳送 Node資訊 
3. 給予選到的 Node暫時假設分數 
4. 以假設分數來更新開局庫 
 
第二部分為接收運算完成的結果，並以此數據更新開局庫。 
1. 接收與分析回傳結果 
2. 以新的資訊更新開局庫 
 
i. 第一部分: 選擇開局庫內需要運算的 Node 
 
原始的演算法執行過程可以用 Alpha、Beta 值來區分為多個 Pass，而一個
Pass 即為執行一次 Alpha-Beta Search。此乃因為 AB-SSS*會在每次選到 Node後
呼叫審局函數，並等待函數的回傳結果，以此結果繼續執行 Alpha-Beta Search，
因此其演算法可以在一次搜尋內就結束一個 Pass，這種作法會消耗十分多的時間
在等待審局函數運算。 
 
23 
 
在未得知真實分數時，會使用 Virtual Loss 來代替其分數，為了延續
Transposition Table 的作用，仍會使用 Virtual Loss 來更新開局庫，但用此分數來
更新，容易造成紀錄的錯誤。因此，我們多設定另一組參數來對 Virtual Loss 做
判斷，並免造成 Transposition Table 內的資訊錯誤。 
 
ii. 第二部分: 更新完成運算的結果 
 
當實際的運算結果回傳時，必須以真實分數來重新更新開局庫，並還原設定
Virtual Loss 所造成的影響。而將更新部分抽離出 Alpha-Beta Search 除了應對志
願型計算系統外，亦有些好處。 
 
 由於開局庫內的樹十分複雜，每個 Node 都可能會有多個 Parent，或是會產
生迴圈等等。若是如原始 AB-SSS*等方法，在 Alpha-Beta Search過程內更新 Node
的分數上下界，僅能處理搜尋下來所走的那一條路徑。但若是將更新部分完全切
離出 Alpha-Beta Search 後，即可針對 Node的所有 Parent 都處理更新，以此來加
大開局庫狀態更新的影響範圍，並加速整體執行的效率。 
 
 接收與分析回傳結果 
 
 由於演算法運用了志願型計算系統，故必須另外接收運算的結果，並分析回
傳內容，擷取出其中需要使用的資訊。 
 
 以新的資訊更新開局庫 
 
 以實際分數更新開局庫，則如同 AB-SSS*與 AB-DUAL*，依照回傳分數與
Alpha、Beta 值的關係來修改 Node 的分數上下界。應用於志願型計算系統的更
新方法，與原始方法不同之處為每一個 Node 都可以向所有 Parent 處理更新，此
方法能加速整體的執行效率。 
 
若是開局庫有加入新的棋譜或是刪減掉原始開局庫內的著手，都可以利用此
方法快速的更新與驗證這些改動是否讓開局庫更為準確。甚是修改做為審局函數
的 AI 程式時，亦可使用此方法快速地將整個開局庫重新檢驗，確認開局庫對於
AI程式來說，是否仍就保有相同的可信度。 
  
25 
 
利用 Minmax、Alpha-Beta、MTD(f)等演算法，取得各盤面下到最後，脫譜
時的最佳分數，並記錄在 SGF 的節點上。為了電腦程式讀取便利，除了將需要
的資料紀錄再 SGF 檔案上，還會再將其匯入資料庫之內。完成以上步驟的資料
庫，會儲存以下所列之資料。 
 
 盤面代表的 Hash Key 
 勝敗和的次數 
 合法步 
 和其他盤面的連結 
 Parent、Child 
 最佳的分數 
 
如此一來，使用這些資料便可以同時取得統計資料，以及電腦程式所給予的
評估分數。 
 
4.5. 演算法效能 
 
為了增加整體的計算效率，會希望每個正在使用運算核心的工作，都是確實
會被利用到的。倘若占用運算資源的工作，對於開局庫而言並不會被使用到，那
這一次的運算，可以說是一種浪費資源。為了達成計算必要工作的目的，當判斷
將來不會使用目前正在運算的工作時，便將此工作停止，讓運算資源進行別的運
算。 
 
為了即時判斷運算中的工作是否還需要，在更新開局庫分數後，便確認每項
工作的必要性。也就是每當有一個工作運算完成，便會確認所有正在運算的工作，
其是否仍需要繼續計算。判斷的方式依照目的可以分成兩種，其一為運算中的工
作在當下是不需要的便中斷，其二為確認該項工作的結果未來也不會運用到才中
斷。將其分別命名為 Soft Abortion 與 Hard Abortion。 
 
Abortion 利用了 AB-SSS*以及 AB-DUAL*的特性。AB-SSS*的特性是，執
行演算法的過程中，Alpha值只會遞減，依照此性質便能將Abortion分類為兩種。
根據Alpha-Beta Prune的效果，可以很容易的得知某些節點在目前的Alpha與Beta
區間內是會被剪裁的。配合 AB-SSS*等演算法的特性，便可以推估那些節點是
目前會被剪裁，並且在後續的 Alpha 及 Beta 底下仍然會被剪裁，或是不會被剪
裁掉。 
  
27 
 
 
圖 17 Soft Abortion 範例 
 
4.5.3. 實驗 
 
實驗總共分成兩部分，其一為演算法所能提供的效能，其二為運算核心所能
提供的效能。第一個部分的實驗，主要是為了驗證演算法的確能有效改善效率，
因此使用單一個核心來運算，作為比較的標準。第二部分的實驗，則是為了檢測
多核心時所能提高的效能，因此實驗中採用前面提出的 Soft Abortion 與 Head 
Abortion 於實驗中比較各自優劣。 
 
第一部分的實驗，在僅使用一個運算核心的前提下，比較Minimax Search、
Alpha-Beta Search 以及 AB-Dual*的效率，其實驗數據如下表格。 
 
 Minimax 
Search 
Alpha-Beta 
Search 
AB-Dual* 
#Jobs 16299 2042 1491 
Time(hr.) 306.06 50.83 33.34 
Speedup 
(w.r.t. AB1) 
0.16 1.00 1.52 
表 9 單核心運算時間 
 
實驗中所使用的開局總共 16299 個葉節點，在 Minimax Search 中會將這些
點全部運算過，總共耗時約三百小時，這是非常大量的時間消耗。改用 Alpha-Beta 
Search 後便有了極大的改進，所需計算葉節點數量降低至 2042 個，時間消耗也
只需約六分之一。而 AB-Dual*則將需要運算的葉節點數量降低到 1491個，時間
也幾乎是十分之一。此表格顯示出這類型的演算法，對於減少計算數量是非常有
幫助的。 
  
r
c d e h
i j
a b
29 
 
五、 結果與討論 
 
於本子計畫由蒐集棋譜開始，這些棋譜乃是由網路上高手所對弈的棋譜而來，
總計六十八萬筆。整理棋譜則利用了資料庫系統、Zobrist Key等等軟體工具或演
算法，並區分為紀錄數據與紀錄落著兩種。整理完成後的資料庫檔案，除了輸出
為 SGF檔亦能以多種方式利用。 
 
由於棋譜數據龐大，故採用區分強弱連結的概念，在精簡數據外，亦能避免
掉可能遭遇的陷阱或是誤判。而剔除弱連結後的資料，亦能與原始的資料相互比
對，作為新的偵錯方法。 
 
將開局庫分為紅方與黑方，除了可以屏除較不具參考價值的落著，亦有助於
針對某些特定盤面深入研究，且可與原始的開局庫交互比較。紅方樹或黑方樹利
用於對弈時也能取得更佳的棋譜資訊。 
 
利用已經整理完成的開局庫資料，並將第一年中所使用的 SGF 格式沿用至
網路版本，以此為基底，接續研究將 Alpha-Beta Search 等演算法應用於志願型計
算系統上。本子計畫第二年主要研究之演算法為 AB-DUAL*，以此演算法作為
驗證開局庫的方法。 
 
 演算法中，為了能夠隨時反應出目前開局庫的狀態，選點方式採用 Best-First 
Search 的模式，一次僅選擇一個最需要運算的 Node。將更新抽離出 Alpha-Beta 
Search 內另外處理的作法，除了因應志願型計算系統回傳資訊時更新狀態的需求，
也能夠讓更新變得容易處理，在需要更新所有 Parent 時，亦因為將其分離出了搜
尋過程從而簡化了各個步驟所需處理的事項。除了以上的改動，擴充後的演算法
仍保留了大多數AB-SSS*與AB-DUAL*演算法中的優點，如Transposition Table、
Zero Window 等。 
 
最後利用了前面所處理的各項數據，找出現有開局庫的弱點，例如弱點盤面，
在這裡指的是不適合電腦程式使用的盤面，即陷阱盤面或程式無法判斷優劣之盤
面。針對該些弱點進一步地審慎評估，將該些弱點避開。最後再將避過弱點盤面
的開局庫挖深或擴展，用以取得更加精準的評估。實際應用時，也可以取得更多
的資訊。 
 
 
  
31 
 
七、 參考文獻 
 
[1] A. Plaat, J. Schaeffer, W. Pijls, and A. de Bruin. SSS* = a-b + TT. Technical 
Report TR-CS-94-17, Department of Computing Science, University of Alberta, 
Edmonton, AB, Canada, 1994. 
[2] Allis, L.V., Searching for Solutions in Games and Artificial Intelligence. Ph.D. 
Thesis, University of Limburg, Maastricht, The Netherlands, 1994. 
[3] Allis, L.V., van der Meulen, M., van den Herik, H.J., Proof-number search. 
Artificial Intelligence, 66(1):91-123, 1994. 
[4] Auer, P., Cesa-Bianchi, N., Fischer, P., Finite-Time Analysis of the Multiarmed 
Bandit Problem. Machine Learning, vol. 47, no. 2/3, pp. 235-256, 2002. 
[5] Berliner, H. J., Chess as Problem Solving: the Development of a Tactics Analyzer. 
Ph.D. Dissertation, Carnegie-Mellon University, Pittsburgh, 1974. 
[6] Berliner, H. J., Computer Backgammon. Scientific American, vol. 242, no.6, 
pp.64-72, 1980. 
[7] Breuker, D.M., Allis, L.V., van den Herik, H.J., How to mate: Applying 
proof-number search. In H.J. van den Herik, I.S. Herschberg, and J.W.H.M. 
Uiterwijk, editors, Advances in Computer Chess 7, pages 251-272. University of 
Limburg, Maastricht, The Netherlands, 1994. 
[8] Breuker, D.M., Uiterwijk, J.W.H.M., van den Herik, H.J., The PN2-search 
algorithm. In H.J. van den Herik and B. Monien, editors, Advances in Computer 
Games 9, pp. 115-132. IKAT, Universiteit Maastricht, Maastricht, The 
Netherlands, 2001. 
[9] Bruegmann, B., Monte Carlo Go, 1993. 
[10] C.W. Chou, J.C. Chen, Hideki Kato and S.J. Yen, “The 5th GPW Cup 9x9 and 
13x13 Computer Go Tournaments,” ICGA Journal (SCI)(to be appeared) 
[11] Chaslot, G., Winands, M., Uiterwijk, J., van den Herik, H.J., Bouzy, B., 
Progressive strategies for Monte-Carlo Tree Search. New Mathematics and 
Natural Computation, 4(3), pp. 343-357, 2008. 
[12] Donald E. Knuth and Ronald W. Moore. An analysis of alpha-beta pruning. 
Artificial Intelligence, 6(4):293–326, 1975. 
[13] Gelly, S., Wang, Y., Munos, R., Teytaud, O., Modification of UCT with patterns 
in Monte Carlo Go. Technical Report 6062, INRIA, 2006. 
[14] George C. Stockman. A minimax algorithm better than alpha-beta? Artificial 
Intelligence, 12(2):179–196, 1979. 
[15] Ginsberg, M.L., GIB: steps toward an expert-level bridge-playing program. In 
Sixteenth Internal Joint Conference on Artificial Intelligence, pp. 584-589, 1999. 
33 
 
[32] Tesauro, G., Galperin, G.R., On-line policy improvement using Monte-Carlo 
search. In M.C. Mozer, M.I. Jorden, and T. Petsche, editors, NIPS 9, pp. 
1068-1074, 1997. 
[33] Winands, M.H.M., Uiterwijk, J.W.H.M., van den Herik, H.J., PDS-PN: A New 
Proof-Number Search Algorithm. Lecture Notes in Computer Science, CG2002, 
2003. 
[34] Winands, M.H.M., Uiterwijk, J.W.H.M., PN, PN2 and PN* in Lines of Action. In 
J.W.H.M. Uiterwijk, editor, The CMG Sixth Computer Olympiad 
Computer-Games Workshop Proceedings. Technical Reports in Computer 
Science CS 01-04, Universiteit Maastricht, Maastricht, The Netherlands, 2001. 
[35] 陳靖平、吳毅成（1999）。適用於六子棋應用的網格計算系統。 
[36] 陳昱維、吳毅成（2011）。適用於電腦遊戲之志願型計算系統仲介者。 
[37] 鄒忻芸、吳毅成（2000）。CGDG桌機格網之應用層框架一般化與資源分配
管理。 
[38] 韓尚餘、吳毅成（2011）。適用於電腦對局遊戲之志願型計算系統工作端。 
[39] http://chessprogramming.wikispaces.com/ 
[40] http://en.wikipedia.org 
碩的成果。 
 
四、攜回資料名稱及內容 
The 17th Game Programming Workshop (GPW-12) 國際會議論文集一份 
 
 
 
 
 
unit encodes the results and sends them 
to the computing unit master;  
(5) The computing unit master passes the 
results from the computing units to the 
User program;  
(6) The User program receives, decodes, and 
handles the results, as Figure 5 shows. 
 
At User program and Computing unit, we 
used encode at a simulation status to 
transmission. The method can add checksum 
to check the solved state is correct. At user 
program also add some information in code to 
send. At receive decode can also be sure that 
the integrity of the received data. In the LNPC 
system used the method of encode and decode 
can check the state surefire. 
 
 
 
 
The communication protocol of the LNPC 
system is described in Figure 4. For a 
successful request, it needs 8 operations to 
communicate information among the user 
program, the computing master unit, and the 
computing unit. If there are many jobs in the 
user program, we should make one operation, 
which is to repeat Step 3. Figure 4 presents the 
protocol of the LNPC system.  
 
In this paper, the experiment of MTU size is 
set to 9K, and used the same LNPC system of 
computing units. Table 1 compares our method 
and the standard TCP computing system. 
 
 
Figure 3. Protocol of standard TCP 
communication system 
 
 
Figure 4. Protocol of the LNPC system 
 
 
Figure 5.The Model for Paralleling Games 
 
 
 
Figure 1. Distribute jobs in the LNPC system 
 
 
 
Figure 2. Integrate results in the LNPC system
 
 
 
 
6) H. Kato and I. Takeuchi, “Parallel 
monte-carlo tree search with simulation 
servers”, The 13th Game Programming 
Workshop (GPW 2008), November 2008, 
Hakone, Japan. 
 
Acknowledgments   
This work is supported by the National Science Council 
of Taiwan under the grant NSC 
99-2221-E-259-009-MY3. 
 
The 27th Annual Conference of the Japanese Society for Artificial Intelligence, 2013 
- 1 - 
A Supervised Learning Method for Chinese Chess Programs 
Wen-Jie Tseng
1
, Jr-Chang Chen
2*
, I-Chen Wu
1
, Ching-Hua Kuo
1
, Po-Han Lin
23
 
1
 Department of Computer Science, National Chiao Tung University 
2
 Department of Information & Computer Engineering, Chung Yuan Christian University 
3
 Department of Computer Science & Information Engineering, National Central University 
The Elo-rating system and the Minorization-Maximization (MM) algorithm have been used for pattern learning in the 
game of Go. This paper proposes a learning method based on the Elo-rating and MM to help adjust feature weights in 
Chinese chess programs, enhancing position evaluation accuracy. The learning method automatically adjusts feature weights 
iteratively according to expert game records. In experiments, we demonstrate the method by adding three new features, 
including a different way of calculating piece mobility. The experimental results show the playing strength of our Chinese 
chess program, CHIMO, has improved, yielding a win ratio of 61.7% against the pre-learned version. In addition, we also 
justify the method by demonstrating the learned feature weights for piece mobility.  
 
1. Introduction 
Chinese chess is one of the most popular two-player chess-like 
games. It has a long history and very important cultural status. It 
was estimated in [CBCGAC 2013] that about 200 million people 
play Chinese chess worldwide. In game theory, Chinese chess 
has higher game-tree and state-space complexities than chess 
[Herik 2002]. 
Players in Chinese chess conventionally play as red and black 
sides, and the red side moves first. Each player has sixteen pieces 
on the board, including one king (K), two advisors (A), two 
elephants (E), two rooks (R), two horses (H), two cannons (C), 
and five pawns (P). In the game, players move alternately and 
their main goal is to capture the opponent’s king.  
Chess or chess-like game programs have been an important 
research topic in artificial intelligence [Allis 1994][Campbell 
2002]. Alpha-beta search (αβ search) and quiescence search 
(QS) [Marsland 1992] are the most common algorithms used in 
Chinese chess programs. An evaluation function [Knuth 1975], 
used in search algorithms to evaluate a position by giving it a 
score, may influence the search tree and even the search result.  
In many game-playing programs, when a position   is being 
evaluated, each feature of    is given a weighted score and 
eventually the sum of all feature weight scores is returned as the 
score of  . For example, if one rook is evaluated at 1000 points 
and one pawn is evaluated at 100 points, a position with one rook 
and two pawns is evaluated at 1×1000+2×100=1200 points. For 
accuracy, Chinese chess programs usually try to evaluate a large 
number of features. For example, our Chinese chess program 
CHIMO, which has won a silver medal in ICGA Tournaments, 
uses about 1500 features during evaluation.  
However, manually adjusting each feature weight accurately is 
time-consuming. For this problem, [Cheng 2006] proposed a 
method to automatically obtain weights for certain features using 
statistical methods. His method counts the appearance of pieces 
in different locations in many expert game records and converts 
the counted value to scores using self-defined formulas. However, 
the method can only be applied to specific features for evaluating 
pieces in different locations. [Kao 2009] also proposed a genetic 
algorithm method to automatically adjust feature weights, for 
which any feature can be applied to. However, the method 
requires considerable computation time for self-play testing after 
each generation in order to choose good feature weights into the 
next generation.  
In this paper, we propose a supervised learning method based 
on the Minorization-Maximization (MM) algorithm that adjusts 
feature weights of Chinese chess programs automatically by 
learning from expert game records. Moreover, when new features 
are added, the method can adjust new feature weights rapidly and 
accurately. Most importantly, the method uses QS to evaluate the 
moves in game records to avoid being affected by the horizon 
effect.  
In our experiments, we applied the method to CHIMO with 
three newly added features. The new features improved the 
playing strength of CHIMO, which was verified through self-
play games against its original version.  
2. Related Work 
Section 2.1 introduces the main algorithms used in computer 
Chinese chess programs. Section 2.2 introduces the design of the 
evaluation function in CHIMO. Section 2.3 explains how we 
applied the Elo-rating system and Bradley-Terry model to 
features and describes the MM algorithm used for learning.  
2.1 Alpha-beta search and quiescence search 
αβ  search is the main algorithm used in Chinese chess 
programs to look for the best move of a position. It is a kind of 
depth-first search and always with a limited depth because the 
game tree is too huge to traverse in a limit time. When the search 
reaches a leaf node of the limited depth, it will evaluate the node 
and assign a score.  
QS is a common technique used to evaluate leaf nodes for 
avoiding the horizon effect caused by depth limitation. 
Evaluating the leaf nodes only in the limited depth is very 
Contact: Jr-Chang Chen, Department of Applied Mathematics, 
Chung Yuan Christian University, No. 200, Chung Pei Rd., 
Chung Li, Taoyuan County 32023, Taiwan (R.O.C.), phone: 
+886-3-2653131, email: jcchen@cycu.edu.tw 
 
 
3C3-IOS-2-8 
The 27th Annual Conference of the Japanese Society for Artificial Intelligence, 2013 
- 3 - 
For each newly designed feature, we use the MM algorithm to 
update its weight. Every iteration of learning updates all feature 
weights once. Learning ends when all feature weights converge 
to its respective value. The adjustment allows the evaluation 
function to work more like experts within a collection of expert 
game records.  
3.2 QS for learning  
For reasons similar to the horizon effect, it is inaccurate for 
evaluation to take positions which are only one-depth away from 
the root into consideration because these positions may have 
noisy moves. For accuracy, we apply QS to evaluate candidate 
moves in our learning method.  
 
 
Figure 1. The evaluation of positions in game records. 
When using QS to evaluate a move, we use the features of the 
leaf positions returned by QS for learning. As illustrated in 
Figure 1, a’, b’, and c’ are the search results from QS. Therein, 
the leftmost move is chosen by experts. It is more reasonable that 
the competition (choosing a move) should be among a’, b’, and c’ 
instead of a, b, and c.  
4. Experiments and Results 
We performed experiments for all the new features mentioned 
above. This section describes them and their results. Section 4.1 
explains the experimental environment, including learning data, 
self-play, and some positions inappropriate for learning. Section 
4.2 describes the experimental results in different learning 
schemes, including single, sequentially, and simultaneously. 
Section 4.3 is the comparison between using and not using QS 
for evaluating candidate moves.  
4.1 Experimental environment 
Expert game records are collected as learning data from many 
websites, including Xiangqi Arena [Movesky 2013]. The records 
are played by players with Elo-rating higher than 2350.  
In our implementation, the iterations end when the update is 
less than 5%. After learning from game records, self-play 
between the learned version and the original version is conducted 
for comparison. The only difference between these two versions 
is the evaluation function. We chose 107 familiar testing 
positions from our opening database to initiate self-play. Each 
version of CHIMO played each position twice, taking red and 
black sides once respectively for a total of 214 played games. 
The game results are recorded and the win ratio is defined as 
follows.  
 
            
                     
  
Some positions of game records are inappropriate for learning 
because they are too unstable to be precisely evaluated by the 
evaluation function. In such positions, some moves can cause a 
win or loss and the evaluation function cannot account for that. 
The inappropriate positions which may cause noise are removed 
from learning.  
For example, checking or checked positions are removed 
because particular moves are forced regardless of the evaluation 
function. For checking positions, it is obvious that the most 
prioritized moves are those that capture the opponent’s king. On 
the other hand, in checked positions, the most prioritized moves 
are those that prevent the king from being captured. 
4.2 Experimental results 
In our experiments, features can be added sequentially or 
simultaneously. First, we added each feature individually. The 
self-play results are listed in Table 1 and the W-L-D column 
presents the game results against the original version for win, 
loss, and draw respectively. The enhancement of the win ratio of 
NM is more apparent than others.  
 
Features W-L-D Win ratio 
NM 68-41-105 56.3% 
CAE 44-42-128 50.5% 
PRP 54-41-119 53.0% 
Table 1. Self-play results of learning single features. 
Next, we tried adding features sequentially, as listed in Table 2. 
For example, for the learning sequence <NM, CAE, PRP>, the 
feature NM is learned first, then CAE, and finally PRP.  
Feature <NM, CAE> has the best result within Table 2 and 
outperforms even <NM, CAE, PRP>. The result shows that 
adding new features one by one does not necessarily improve the 
win ratio. This may be due to interaction between features. For 
example, a piece aiming to protect other pieces may need to 
sacrifice mobility to do so.  
 
Features W-L-D Win ratio 
<NM, CAE> 72-34-108 58.8% 
<NM, CAE, PRP> 63-42-109 54.9% 
<NM, PRP> 61-46-107 53.5% 
<NM, PRP, CAE> 59-36-119 55.4% 
<CAE, NM> 55-44-114 52.3% 
<CAE, NM, PRP> 65-40-109 55.8% 
<CAE, PRP> 52-58-104 48.6% 
<CAE, PRP, NM> 70-39-105 57.2% 
<PRP, NM> 68-49-97 54.4% 
<PRP, NM, CAE> 55-42-117 53.0% 
<PRP, CAE> 59-38-117 54.9% 
<PRP, CAE, NM> 74-52-88 55.1% 
Table 2. Self-play results of learning features sequentially. 
Finally, we tried learning combinations of two or three new 
features simultaneously. The self-play results are listed in Table 
3. The combination of {NM, PRP} outperforms other 
combinations of two and the combination of all three achieved 
the best result than other experiments in this paper. This implies 
that simultaneous learning of feature weights is superior. 
 
Features W-L-D Win ratio 
{NM, CAE} 67-51-96 53.7% 
{NM, PRP} 72-41-101 57.2% 
{CAE, PRP} 53-57-104 49.1% 
{NM, CAE, PRP} 89-32-93 63.3% 
Table 3. Self-play results of learning features simultaneously. 
4.3 Experiments for QS  
In order to validate that QS is better when evaluating candidate 
moves, we tried two versions of learning. One used QS and 
another used only the evaluation function. The results in Table 4 
國科會補助計畫衍生研發成果推廣資料表
日期:2013/09/06
國科會補助計畫
計畫名稱: 運用志願型計算系統建構象棋開局庫
計畫主持人: 陳志昌
計畫編號: 99-2221-E-033-036-MY3 學門領域: 人工智慧與仿生計算
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
電腦象棋程式 CHIMO 在日本橫濱舉行的 2013 Computer Olympiad 得到銀牌的
殊榮。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
