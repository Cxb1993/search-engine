an automatic scheme to discover the associations of 
Web pages and social tags, which associations can 
then be applied in various applications related to 
social Web and social tags. It is expected that the 
quality and usability of social tags will be improved 
through this research, as well as the feasibility of 
constructing social Web and the Semantic Web. 
英文關鍵詞： Social Tags, Social Network Mining, Text Mining, 
Self-Organizing Map 
 
II 
 
中文摘要 
 
社會性標記(social tag)為透過多使用者手動或半自動的為網頁內容所加註之資訊。透過
社會性標記，使用者可以容易的對網頁的內容進行歸類。社會性標記亦可提高網頁檢索的
正確性，且讓我們更容易的瞭解網頁的內容。目前社會性標記的建立大多是透過社會性標
籤(social bookmarking)網站由使用者手動進行加註。這樣的過程可能會產生標記具有多樣
性、冗餘性與不一致性的缺陷。本研究已發展一網頁內容與標記間關聯之探勘方法，並運
用所探勘之關聯進行垃圾標記偵測與標記建議等二應用。首先我們以自我組織圖為基礎，
發展一文本探勘方法以找出網頁內容與標記間之關聯。透過此關聯，我們可以針對上述標
記之缺點加以改善。之後我們運用此發掘成果於二個社會性標記相關之實際應用上。其一
為垃圾標記之偵測；其二為社會性標記建議。本研究之主要貢獻是發展一自動方法來獲取
網頁內容與標記間之關聯。運用此關聯，我們將可為許多相關應用提供可行的解決方案。
透過本研究，我們期望可以提升社會性標記之品質與使用性，並進一步促進其於建構社交
網站與語意網之實用性。 
1 
 
1. 前言 
 社交網站(Social Website)近年來已成為廣被使用之全球資訊網(World Wide Web, WWW)
之服務之一。目前已存在各種不同型態的社交網站，如交友網站、影音分享網站、書籤網
站等，各自吸引了許多使用者。其主要特性為來自使用者之協同貢獻。在此網路中的所有
使用者對此網站共同貢獻內容或註解，並加以利用。在內容的協同製作上，一般而言，使
用者在社交網站上皆會建立個人之側寫檔(profile)，以進行個人為主之社交行為，例如交友
網站或共筆網站，如Facebook11。另一種社交行為乃透過共同興趣而建立，例如共同議題
或同類型資源分享等，如Flickr22與Youtube33。另在註解的貢獻上，最大的應用即為社交性
書籤(social bookmarking)網站，如delicious44。透過社交性書籤網站，使用者可以分享某網
頁及其對該網頁加註之標記，我們稱此標記為社會性標記(social tag)。透過社交性標記過
程，我們可以較容易的獲取網頁（及其他受標記物件）之語意。然而，社交性標記過程亦
可以產生下列缺陷：  
1. 標記之多樣性(diversity)、冗餘性(redundancy)、與不一致性(inconsistency)過高。 
2. 標記間之關聯不明確。 
3. 產生垃圾標記(tag spam)問題。 
4. 標記之涵蓋性仍嫌不足。 
分析以上社會性標記之缺陷，可歸納出下列兩點主要原因：  
1. 社會性標記多為使用者手動進行註記，使用者之主觀認知與習慣主宰了標記之類
型、數量、精確度等，使的標記具有多樣、冗餘、不一致等特性。另一方面，社交
性書籤網站大多提供標記建議機制以輔助使用者進行註記，使得使用者常根據所推
薦之標記進行註記，又使得標記漸漸收斂，反向的限制了其涵蓋性。手動註記亦限
制了已標記網頁之涵蓋性。 
2. 網頁內容與其標記間之相關性難以準確判讀。因此我們很難判斷那些標記是垃圾標
記，也很難提供一較佳之標記建議予使用者。另一方面，標記與標記間之關聯亦難
獲得，使我們無法開發更深層之應用。 
為了克服社會性標記之缺陷，我們可以針對上述原因提出相對之解決方案。其一，我們
需要一自動化註記方法，以克服手動註記所產生之缺陷。其二，我們需要一網頁內容與標
記之關聯發掘方法，以判別網頁與標記間是否相關，解決垃圾標記的問題。此外，藉由發
掘此類關聯，我們亦可進一步應用於其他社會性標記相關應用上，例如標記建議與排序。 
本研究之目的，主要為提出一自動化社會性標記的分析方法，以發掘標記與網頁內容間
之關聯，並利用此關聯以改善相關應用之成效，即垃圾標記之偵測。 
2. 文獻探討 
垃圾標記之防範之主要策略有三種：垃圾標記偵測 (detection)、垃圾標記預防
(prevention)、與垃圾標記降級(demotion)[1]。以下將針對這三種策略介紹目前發展情況。 
1. 垃圾標記偵測：此策略之作法為利用手動或自動方式偵測出垃圾標記之存在，並加
以處理。許多網站，如Wikipedia與 Digg，仍仰賴使用者回報垃圾標記。通常人工
回報方式的結果可作為自動偵測方式之前置作業，因我們需要一訓練資料集來訓練
                                                 
1 http://www.facebook.com 
2 http://www.flickr.com 
3 http://www.youtube.com 
4 http://delicious.com 
3 
 
獲得資料間之關聯。 
3. 標示過程(labeling process)：經前置處理後所獲得之網頁向量與標記向量，將使用
一標示過程將各網頁與標記標示於訓練後之自我組織圖上。在此所謂標示即是將某
一網頁或標記依其與神經元鍵結向量之相似度加以標示，透過這個過程，我們可以
將相關的網頁或標記標示於同一神經元上，如此則可獲得網頁或標記之群集，完成
分群。 
4. 關聯發掘：針對自我組織圖訓練後所得到的網頁間關聯與標記間關聯，本文發展一
方法探勘網頁與標記間之關聯。基本上，網頁間之關聯由分群結果所表達。屬於同
一群組或鄰近群組的網頁即代表具有相關性。同樣的概念亦可用於標記關聯上。本
步驟的目的便在於發展一方法以找出一網頁分群與標記分群間之對應關係。令
Px={Pi}為一網頁分群，Ty = {Tj}為一標記分群，則我們可以定義其間之關聯度為： 
( )
( )( )
∑ ∑
∈ ∈
=
x yIi Ij
ji
yx
yx CS
P TTP
T,P ,
1
       (1) 
其中 I(Px)與 I(Ty)為 Px與 Ty之索引集(index set)，C為一矩陣，當網頁 Pi包含註解
Tj時，其第 i列第 j行之元素 Ci,j = 1，否則為0。對於一網頁分群 Px而言，與其最
相關之標記分群 Ty為符合下式者： 
( )
( )yx
Ij
Sy
T
T,P
M∈
= maxarg          (2) 
其中MT為利用標記訓練所得之自我組織圖。 
圖1 系統架構圖 
 垃圾標記偵測方法 
訓練網頁資料 訓練標記資料 
網頁／標記關聯 
前置處理 
網頁向量 標記向量 
標示過程 
網頁標示 標記標示 
垃圾標記偵測過程 垃圾標記 
新進網頁資料 新進標記資料 
自我組織圖神經元
權重 
5 
 
表1 自我組織圖訓練參數 
參數 
資料集 
標記發表 書籤網頁 
字彙集大小 19824 22990 
訓練資料筆數 141172 79806 
自我組織圖大小 50×50 40×40 
學習速率 0.4 0.4 
訓練最大週期數 800 1000 
訓練完成後，我們分別將標記發表與書籤網頁標示於其所對應之自我組織圖上，分別
得到標記發表分群圖與書籤網頁分群圖。隨後再進行關聯發掘過程。最後再根據關聯發掘
之結果進行垃圾標記發表之偵測。我們將所有的訓練資料用作測試資料，得到如表2之混亂
矩陣(confusion matrix)。 
表2 偵測結果之混亂矩陣 
 真實垃圾標記 真實正常標記 總計 
偵測垃圾標記 114628 280 114908 
偵測正常標記 23619 2645 26264 
總計 138247 2925 141172 
根據表2之結果，我們可以得到偵測結果之正確率為(114628+2645)/141172=83.07%。 
參考文獻 
[1] Heymann, P., Koutrika, G., and Garcia-Molina, H. (2007) “Fighting Spam on Social Web 
Sites: A Survey of Approaches and Future Challenges,” IEEE Internet Computing, vol. 11, 
no. 6, pp. 36-45. 
[2] Gyöngyi, Z., Garcia-Molina, H., and Pedersen, J. (2004) “Combating Web Spam with 
TrustRank,” Proc. 30th Very Large Databases Conf., pp. 576–587. 
[3] Graham, P. (2002) “A Plan for Spam,”; www.paulgraham.com/spam.html. 
[4] Urvoy, T., Lavergne, T. and Filoche, P. (2006) “Tracking Web Spam with Hidden Style 
Similarity,” Proc. 2nd Int’l Workshop on Adversarial Information Retrieval on the Web  
http://airweb.cse.lehigh.edu/2006/proceedings.pdf. 
[5] Mishne, G., Carmel, D. and Lempel, R. (2005) “Blocking Blog Spam with Language Model 
Disagreement,” Proc. 1st Int’l Workshop on Adversarial Information Retrieval on the Web 
(AIRWeb 05); http://airweb.cse.lehigh.edu/2005/#proceedings. 
[6] Gomes, L.H. et al. (2005) “Comparative Graph Theoretical Characterization of Networks of 
Spam,” Proc. 2nd Conf. Email and Anti-Spam (CEAS 05); www.ceas.cc. 
[7] von Ahn, L. et al. (2003) “CAPTCHA: Using Hard AI Problems for Security,” Proc. 
Eurocrypt, pp. 294–311; citeseer.ist.psu.edu/vonahn03captcha.html. 
[8] Templeton, B. (2007) “Proper Principles for Challenge/Response Anti-Spam Systems,”; 
www.templetons.com/brad/spam/challengeresponse.html. 
[9] Felten, E. (2003) “A Challenging Response to Challenge-Response,” Freedom to Tinker; 
www.freedom-to-tinker.com/archives/000389.html. 
[10] Levine, J.R. (2005) “Experiences with Greylisting,” Proc. 2nd Conf. Email and Anti-Spam 
(CEAS 05); www.ceas.cc. 
[11] Liu, D., Hua, X. S., Yang, L., Wang, M., and Zhang, H. J. (2009) “Tag Ranking,” Proc. 
WWW 2009, pp. 351-360. 
[12] Hotho, A., Jäschke, R., Schmitz, C., and Stumme, G. (2006) “Information Retrieval in 
Folksonomies: Search and Ranking,” Proc. ESWC ’06, pp. 411–426. 
[13] Jäschke, R., Marinho, L. B., Hotho, A., Schmidt-Thieme, L., and Stumme, G. (2007) “Tag 
表 Y04 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                          100年  10月  25日 
報告人姓名 楊新章 
 
服務機構 
及職稱 
國立高雄大學資訊管理學系 
教授兼系主任 
     時間 
會議 
     地點 
10/23/2011-10/24/2011 
 
Macau, China 
本會核定 
補助文號 
NSC 99-2221-E-390-034 
會議 
名稱 
 (中文) 
 (英文) The 3rd International Conference on Data Mining and Intelligent 
Information Technology Applications 
發表 
論文 
題目 
 (中文) 
 (英文) Identifying Spam Tags by Mining Tag Semantics 
一、參加會議經過 
 
於 10月 23日(日)自高雄小港機場搭機前往澳門，住宿一晚後，翌日前往會議地點
－澳門威斯汀酒店－參加會議。註冊時發覺該會議將本人報告時程由 10月 24日移至 10
月 26 日，然本人因行政業務與課程關係，早已排定 10 月 24 日返國。故與主辦者討論
後，將本人論文排定於當日 13:00之 session 3進行論文報告。該 session共排定九篇論文。
報告後即兼程趕往澳門機場搭機返抵高雄。 
 
二、與會心得 
 
本會議與另兩項會議－The 16th North-East Asia Symposium on Nano, Information 
Technology and Reliability與The 5th International Conference on New Trends in Information 
Science and Service Science－一同舉辦，共蒐錄 182篇論文。主要的投稿者來自於亞洲
各國，尤以中國與韓國居多，應是地緣與主辦單位關係。該會議在會議安排上尚稱用心，
場地安排符合一般國際會議水準。會議論文集共二冊，由 IEEE 出版，本人僅拿到論文
所在之第二冊。大會並邀請兩位學者進行 keynote speech，分別是美國杜克大學之 Paul P. 
Wang教授與澳門大學之 IEEE院士 Philip Chen教授。可惜該兩演講均安排於傍晚，本
人因時程關係無法參與。 
 
本人之議程原定於會議第一日（即 10月 24日）進行，但會議前三天所公布之最終
議程卻將本人之論文排於第三天（10月 26日）。由於本人之行程早已確定，故於會議第
一日當天與大會主辦單位協商，調整於適當之時段進行報告。由於第一日僅有一時段為
ICMIA會議議程，故將本人安排於 session 3。該議程原包含九篇論文，作者皆屬韓國學
者與大陸學者。其中數篇論文並無報告者出席。 
 
三、考察參觀活動(無是項活動者省略) 
 
因行程關係並無相關考察活動。 
 
 
四、建議 
附件三
 
表 Y04 
 
 
Identifying Spam Tags by Mining Tag Semantics
Hsin-Chang Yang
National University of Kaohsiung
Kaohsiung, Taiwan
Email: yanghc@nuk.edu.tw
Chung-Hong Lee
National Kaohsiung University of Applied Sciences
Kaohsiung, Taiwan
Email: leechung@mail.ee.kuas.edu.tw
Abstract—Social bookmarking or tagging Web sites, or folk-
sonomies, emerge recently since the result of traditional context-
driven search of Web pages could be improved by user-annotated
tags. People could retrieve, organize, and comprehend Web
pages thorough such tags. However, spam tags were significantly
increased and deteriorated the effectiveness of social tagging. Tags
unrelated to the content of the annotated Web pages were added
intentionally to improperly manipulate the ranking or retrieval
result for malicious purposes. Therefore approaches for filtering
and identifying spams in various granularity were proposed to
conquer such deficiency. Traditional approaches most focused on
user-level detection which tries to differentiate spammers from
normal annotators. Such approaches may suffer from the fact
that misclassification of spammers as well as spam tags often
happens since the intention and behavior of spammers change
dynamically. It will be better if we can detect spams in much
precise ways such as individual posts or tags. In this work we
propose a scheme to identify individual tag spams according to
the semantic relatedness between a tag and its annotated page.
We first cluster Web pages and tags separately to reveal the
relationships among pages and tags, respectively. A relationship
discovery process was then applied to find the relationships
between a page cluster and a tag cluster. We also devised
a measurement to measure the semantic relatedness between
individual tag and page. Individual spam tags were then identified
according to such relatedness. We conducted experiments on
ECML/PKDD RSDC 2008 dataset and obtained promising result.
I. INTRODUCTION
Nowadays, the gigantic amount of Web pages have caused
difficulties in finding materials regarding specific topics.
Search engines provide capabilities of retrieving and ranking
Web pages according to specified keywords thus play a major
role for users to obtain the needed pages amongst billions
of them [1]. However, keyword-based searching adopted by
traditional search engines suffers from the fact that these
keywords may not ideally reveal the underlying semantics of
the retrieved pages. Therefore pages regardless to the intent of
the query user were often retrieved. To bridge the gap between
the query and the page content, user generated annotations,
i.e. tags, were added to Web pages to summarize, explain,
reveal, remark, and redirect the contents. Retrieval of Web
pages according to such tags is recognized much precise
than traditional keyword-based approach. As a result, many
Web services have been established to allow users annotating
various types of contents, such as Web pages, photos, music,
or blogs. The term ’folksonomy’ was then created [2] to refer
to the categorization of contents using tags collaboratively
contributed by users.
User-generated tags have the desired property of being
semantically related to their annotated content. Content pro-
cessing is thus unnecessary for extracting meaningful key-
words that are expected to represent the semantics of the
content, which process is unlikely perfect. Search facilities
will be benefit from these precise tags. Tags are also ben-
eficial for users since they can comprehend and organize
Web pages much easier. Advantages such as those mentioned
above further encourage users to contribute Web pages and
their annotated tags, which attract more users to share and
use folksonomies. Meanwhile, the popularity of folksonomies
also suggests malicious users to contribute tags irrelevant to
annotated pages for commercial or breach purposes. Such tags
are known as spam tags or tag spams, which will be used
interchangeably in the remaining text. The existence of tag
spams will not only deteriorate the quality of folksonomies,
but also decrease the confidence of their users. Therefore,
approaches for identifying spam tags were devised to remedy
such deficiency.
Tag spams are usually used to alter the ranking of Web pages
in a social bookmarking site. For example, it is not rare that a
newly established company may want to receive more visits of
their company’s Web site for commercial purpose. She could
then bookmark her Web site, possibly using multiple accounts,
and tag it with some popular tags. These tags, although have
nothing to do with the nature of the company, may attract
incautious users to visit her Web site making them troublesome
or even in danger of being attacked by spam mails or viruses. It
will be helpful to filter out these spams to allow users obtaining
the real Web pages they interested.
The detection of tag spams could be divided into three levels
according to the granularity of detection. In the coarsest level,
namely user-level spam detection, we identify a user who
is regularly to post spam tags being a spammer. It is true
that most of the spam tags were posted by these spammers.
However, it is clear that not all spam tags were posted by
spammers and, vice versa, the posts from spammers may
not necessary be spams. Schemes for spam detection in finer
granularity will be beneficial. Post-level spam detection can
detect an individual post, which refers to the collection of
tags issued by a user to a Web page at the same time,
as a spam. Going further, tag-level spam detection tries to
identify individual spam tags which may probably co-exist
with normal tags in a post. The difficulty of both post- and
is no need to eliminate stopwords which were seldom used
as tags. However, stemming is necessary to correlate the tags
and their annotated Web pages. Note that a Web page may
be tagged by different users in multiple times. We simply
collected all tags annotated to this page to obtain its tag set.
We processed every tag set in the training corpus and collected
the tags into the vocabulary VT . We will refer a tag post to
the set of tags annotated to a Web page at a time and a tag
set to the set of tags annotated to a Web page throughout the
dataset, respectively.
We describe the vector encoding of Web pages and tag sets
here. For a page Pi and its tag set Ti, we first extracted their
keywords as mentioned above. Let Pi = {kij |1 ≤ j ≤ ni},
where kij and ni denote the j-th keyword and the number
of keywords in Pi, respectively. Similarly, let Ti = {kil|1 ≤
l ≤ mi}, where kil and mi denote the l-th keyword and the
number of keywords in Ti, respectively. Pi is then transformed
to a binary vector Pi = {pij |1 ≤ j ≤ |VP |} according to the
following rule:
pij =
{
1 if kj ∈ Pi,
0 otherwise. (1)
where kj is the j-th keyword in VP . Similarly, Ti is trans-
formed to binary vector Ti = {tij |1 ≤ j ≤ |VT |}. We did
not apply the classical term weighting scheme such as tf · idf
here since we concerned about the co-occurrence of keywords
rather than their significance during the text mining process.
We then constructed two self-organizing maps to cluster the
encoded Web page vectors and tag set vectors separately. We
should obtain two maps MP and MT which were trained by
the set of Web page vectors, Pi, and set of tag set vectors, Ti,
respectively. Standard SOM algorithm were used so we omit
the details here due to space limitation.
B. Labeling
The training process of SOM is always follow by a labeling
process to complete the clustering of data. Each input data will
be labeled to a neuron in the trained map after the labeling
process. By virtue of the SOM, similar input vectors will
be labeled to neighboring neurons and constitute a cluster.
Therefore we should obtain two labeled maps after labeling
Web pages and tag sets to their respective maps. The first map,
named page cluster map (PCM), contains a set of neurons
which each represents a cluster of Web pages. Similarly, the
second map, named tag cluster map (TCM), contains clusters
of tag sets. These maps should exhibit the relationships among
Web pages as well as among tag sets, respectively. It is
common to measure the association between two pages by
the geometric distance between their labeled neurons. Pages
labeled to close neurons are considered being more relevant
than those labeled to distant neurons. This is also true for tag
sets with respect to the TCM. Therefore, we can define metrics
to measure the relatedness between Web pages as well as tag
sets according to PCM and TCM.
IV. TAG-LEVEL SPAM DETECTION PROCESS
A. Association Discovery between Pages and Tags
We have shown the way to reveal the associations between
Web pages or tag sets. Here we will show how to obtain the
relationships across the PCM and TCM. That is, we should
discover the associations between a Web page and a tag set.
The relatedness between a Web page cluster Px = {Pi} and
a tag set cluster Ty = {Tj} could be measured according to
their constituent members. The relatedness between Px and Ty
can be measured by the following equation which resembles
to the average linkage clustering similarity function:
S(Px, Ty) = 1|Px||Ty|
∑
i∈I(Px)
∑
j∈I(Ty)
Ci,j , (2)
where I(Px) and I(Ty) are the index sets of Px and Ty , re-
spectively. C is a correlation matrix which each row represents
a page and each column represents a tag set. When a page Pi
is annotated with a tag set Tj , the corresponding element of
C, i.e. Ci,j , will be 1; otherwise the element will be 0. In fact,
C is the identity matrix in our current configuration since we
allow Pi being annotated by only Ti. S(Px, Ty) will have
maximum value of 1 when the pages in Px and tag sets in Ty
have one-to-one correspondence. On the other hand, S(Px, Ty)
will have minimum value of 0 when none of the pages in Px
is annotated by any of tag sets in Ty . Therefore, for a page
cluster Px, we then find the most related tag set cluster Ty by:
y = argmax
j∈I(MT )
S(Px, Tj), (3)
where I(MT ) is the index set of MT . Actually, we can also
reverse the direction of the association discovery process, i.e.
finding Px which is the most related page cluster of a tag set
cluster Ty. Equation 3 will be modified as follow:
x = argmax
i∈I(MP )
S(Pi, Ty). (4)
Sometimes the most related tag set cluster of a page cluster
is not the one that meets equation 3 due to imperfect training or
noises. We may amend such insufficiencies by aggregating and
averaging the relatedness comes from neighboring clusters.
Equation 2 is modified as follow:
S(Px, Ty) = 1|Nc(Ty)|
∑
t∈I(Nc(Ty))
1
|Px||Tt|
∑
i∈I(Px)
∑
j∈I(Tt)
Ci,j ,
(5)
where Nc(Ty) is the sets of neighboring neurons of Ty .
Equations 2-5 define the association between a page and
a tag set. We further modify equation 2 to incorporate tag
significance as follow:
S′(Px, kl) = S(Px, Ty)f(kl,Px)f(kl, Ty), (6)
where
f(kl,Px) = 1|Px|
∑
Pi∈Px
1Pi(kl), (7)
f(kl, Ty) = 1|Ty|
∑
Tj∈Ty
1Tj (kl), (8)
39987 to 22990 after removing terms appeared only once.
The tag sets and the bookmarked pages were then trans-
formed into binary vectors, as described in the Sec. III-A.
Both tag set vectors and page vectors were trained by self-
organizing map algorithm. The parameters of the SOMs and
the statistics of training data are shown in Table I. To de-
termine the sizes of the map, we used a rule of thumb that
the number of items labeled on a neuron should approximate
the dimension of the map. That is, the dimension of the map
should be about the third root of the number of training
data. For example, the size of the map for tag data is
50 × 50 since the third root of the number of training data,
3
√
141172 = 52.07 ≈ 50. The other parameters are determined
experimentally. Four types of neighborhood function ∆(j, t)
were tested in our experiments, namely Mexican hat, Gaussian,
linear, and uniform functions. The maximum training epoch
count η were chosen so good clustering results were achieved.
We tried values of η ranged from 500 to 2000 with interval
100.
TABLE I
THE TRAINING PARAMETERS AND STATISTICS OF DATA.
Parameters CorpusTags Pages
Size of vocabulary/Dimension of vectors 19824 22990
Number of training data 141172 79806
Size of map 50× 50 40× 40
Learning rate α(t) 0.4 0.4
Neighborhood function ∆(j, t) Uniform Uniform
Maximal training epoch count η 800 1000
The trained maps were then labeled by pages and tag sets to
obtain the PCM and TCM, respectively. Table II summarizes
the results of these maps. The unlabeled neurons are those
neurons without any data labeled on them. They will not affect
the computation of similarity metrics defined in equations 2, 7,
and 8. The PCM and TCM summarized in Table II is the one
producing the best result in the tag spam detection processes.
TABLE II
SUMMARY OF PCM AND TCM
PCM TCM
Number of neurons (clusters) 1600 2500
Number of unlabeled neurons 21 34
Average cluster size 49.88 56.47
Maximum cluster size 238 472
Minimum cluster size (excluding unlabeled neurons) 11 23
We performed the association discovery process after ob-
taining the PCM and TCM. The process can be applied in
bidirectional, i.e. finding the most related tag set cluster of a
page cluster (PCM→TCM) or finding the most related page
cluster of a tag set cluster (TCM→PCM). We performed both
processes on the PCM and TCM. The result is summarized in
Table III. In this table a dangling cluster is a cluster that is
not the most related cluster of any clusters in the other map.
Unlabeled neurons should not be associated with any clusters
and are all dangling clusters. The values of S(Px, Ty) were
computed using equation 2 where the neighborhood size is
5, i.e. the neighborhood consists of a 5 × 5 grid of neurons
centered at neuron y and x for PCM→TCM and TCM→PCM,
respectively.
TABLE III
SUMMARY OF THE ASSOCIATION DISCOVERY PROCESS.
PCM→TCM TCM→PCM
Average value of S(Px, Ty) 0.783 0.764
Number (ratio) of dangling clusters 153 (6.12%) 93 (5.81%)
Average MOR over all tags in PCM 0.732 0.711
Average MOR over all tags in TCM 0.689 0.703
Average MAOR over all tags in PCM 0.825 0.832
Average MAOR over all tags in TCM 0.813 0.846
The tag-level tag spam detection processes were performed
according to the PCM and TCM in Table II. Since we used all
the data for training, traditional k-fold cross validation is not
suitable. Instead, we used the entire training dataset for testing.
We adopted equation 11 as the decision criterion. Since RSDC
2008 dataset provides only user-level result, we decide the real
spam tags by the following rule. If a tag set Tx is posted by
a spammer, then all tag in Tx are spam tags. However, if a
tag was posted by both spammer and non-spammer, it will be
counted in both spam tags and non-spam tags. There are 7%
of tags that are both spams and non-spams.
The problem of these decision schemes suffers from the
fact that spams notably outnumber non-spams, as we stated
above. In RSDC 2008 dataset, there are 7034 spammers,
159458 spam posts, and 20310 spam tags, respectively. In
the mean time, it includes 171 non-spammers, 2592 non-
spam posts, and 2680 non-spam tags. To overcome such
deficiency, two approaches were adopted in our experiments.
In the first approach, namely extension configuration, we
extend the amounts of non-spams by duplicating them to
comparable amounts of spams. In the other configuration,
namely condensation configuration, we randomly selected
spams which number is the same as non-spams. Obviously,
the number of training data for condensation configuration
is prominently outnumbered by the extension approach. For
evaluation purpose, the confusion matrix is first computed.
Table IV summarizes the confusion matrices of the three tasks
under the two configurations. For extension configuration, the
numbers of non-spammers and non-spam posts are increased
41 and 61 times to meet the number of spammers and spam
posts, respectively. The number of non-spam tags remains
unchanged for both configurations since duplicating non-spam
data will not increase the size of vocabulary. For condensation
configuration, we always randomly selected the number of
spam data resembling the number of non-spam data.
The performance measurements are summarized in Table V.
Traditional recall, precision, and accuracy were used.
VI. CONCLUSIONS
Tag spams exist in social bookmark Web sites for commer-
cial and malicious purposes. Traditional tag spam detection
focused on user-level detection that would decide if a user
國科會補助計畫衍生研發成果推廣資料表
日期:2012/01/10
國科會補助計畫
計畫名稱: 基於自我組織圖之社會性標記之探勘與其應用
計畫主持人: 楊新章
計畫編號: 99-2221-E-390-034- 學門領域: WEB 技術
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
辦理國際會議 ASONAM 2011 並於其中發表論文 
辦理國際會議 GrC 2011 並於其中發表論文 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
