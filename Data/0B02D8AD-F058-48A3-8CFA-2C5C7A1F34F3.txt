The advent of media-sharing sites like Flickr and YouTube has drastically increased the 
volume of community-contributed multimedia resources on the web. However, due to their 
magnitudes, these collections are increasingly difficult to understand, search and navigate. 
With the kind support from NSC, we investigated methods to improve search quality (by 
investigating variant optimization methods for effectiveness and scalability in large-scale 
image collections.  
 
We investigated a real-time system (and one of very few in the world) that addresses three 
essential issues of large-scale image object retrieval: (1) image object retrieval ‚Äì facilitating 
pseudo-objects in inverted indexing and novel object-level pseudo-relevance feedback for 
retrieval accuracy [1][3]; (2) time efficiency ‚Äì boosting the time efficiency and memory usage 
of object-level image retrieval by a novel inverted indexing structure (also hash-based 
methods) and efficient query evaluation [2][3][1]; (3) recall rate improvement ‚Äì mining 
semantically relevant auxiliary visual features through visual and textual clusters in an 
unsupervised and scalable (i.e., MapReduce) manner [2]. We are able to search over 
million-scale image collection in respond to a user query in 121ms, with significantly better 
accuracy (+99%) than the traditional models. It is also an enabling technology for promising 
applications such as mobile landmark recognition [5], product query, touch-based object 
query during video playback [4], etc. We had experimented in several cloud platforms such as 
Amazon EC2, NCHC, NTU-TrendMicro, etc.  
 
We also exploited the developed technologies in variant applications. For example, we 
implemented an iPhone app for flower recognition, which won ¬õ¬©\ 2010¬†¬ï&
¬π  Y¬ê¬∑.¬â]m[7].The results also receive intensive reports from Taiwan-based 
news media (See the appendix). The technologies are being transferred to the industry. 
 
Through rigorously conducting the research for the project, we devised fruitful and significant 
results in terms of publications [1][2][3][4][5][6] in premier venues such as ACM Multimedia, 
IEEE CVPR, etc. 
 
We also receive important invitations for research community services [8], and joining 
prestigious international technical committee [9].  
 
We also thank the great support from NSC such that we can conduct advanced researches, 
which also help PI, Prof. Winston Hsu, winning the 2011 (NSC) Ta-You Wu Memorial 
Award (}i¬¢ 100 ;_E¬§4-jR¬±). Meanwhile, we also receive the Best 
Paper Award for 17th International Conference on Multimedia Modeling, January 2011 [6]. 
 
We append the accepted (also submitted) papers and related media reports in this report.  
 
  
pseudo-objects in inverted indexing and novel object-level pseudo-relevance feedback for 
retrieval accuracy; 2) time efficiency‚Äîboosting the time efficiency and memory usage of 
object-level image retrieval by a novel inverted indexing structure and efficient query 
evaluation; 3) recall rate improvement‚Äîmining semantically relevant auxiliary visual 
features through visual and textual clusters in an unsupervised and scalable (i.e., MapReduce) 
manner. We are able to search over one-million image collection in respond to a user query in 
121ms, with significantly better accuracy (+99%) than the traditional bag-of-words model. 
 
[4] Po-Nung Tseng, Yen-Liang Lin, Winston H. Hsu. Interactive Inquiry for Object of 
Interest in Video Playback by Motion-Augmented Graph Cut. In ACM Multimedia, Pages 
811-814, October 2010. (short paper, acceptance rate: ~30%) 
Abstract: The touch-based displays (devices) have entailed rich interactions between the 
videos and users. The objects appearing in videos usually interest users in wanting to know 
relative knowledge about them. In this paper, we proposed a video playback system for users 
to interactively query objects of interest in videos. Since the text information accompanied 
with videos might not be strongly related to the object of interest, we adopt visual 
appearances as features to retrieve similar objects from large image collections. The tags 
associated with the retrieved images are used to reveal related information of the object of 
interest for further exploiting related knowledge. Solely relying on single viewpoint of the 
object to query may suffer from different poses, occlusions and is not robust. So we present a 
novel video object segmentation approach to improve retrieval precision. The approach is 
based on a 3D graph cut framework. To ensure prompt response and effectiveness, we 
augment the algorithm with compressed-domain motion vectors; compared with the prior 
method, the processing speed of our approach is significantly faster. The experiments on 
community-contributed videos demonstrate the effectiveness of our approach based on multi- 
frame object region query and the improvement of retrieval precision. 
 
[5] An-Jung Cheng, Fang-Erh Lin, Yin-His Kuo, Winston H. Hsu. GPS, Compass, or Camera?: 
Investigating Effective Mobile Sensors for Automatic Search-Based Image Annotation. 
In ACM Multimedia, Pages 815-818, October 2010. (short paper, acceptance rate: ~30%) 
Abstract: Recently, more and more types of sensors are being equipped on the smart phones, 
which provide different aspects into consideration. When a user takes a photo, the 
information it provides like the image content, the location and even the direction the user 
faces can help us to understand the photo itself. Each factor mentioned above can be treated 
as an input to the image search system. However, most existing algorithms for image retrieval 
(or annotation) only focus on the content and location information of the images yet 
completely ignore the important direction-facing factor and lack of the insights of the 
capabilities for the sensors. In this paper, we propose a novel ranking algorithm that can 
leverage different sensors with traditional content-based image retrieval system, and further 
apply to annotate images. We evaluate different combinations of sensors and investigate how 
the geo-location, image content and compass direction influence on image retrieval. 
[6] Yu-Ming Hsu, Yen-Liang Lin, Winston H. Hsu, Brian Wang. Snap2Read: Automatic 
Author's personal copy
Boosting image object retrieval and indexing by automatically discovered
pseudo-objects
Kuan-Ting Chen b, Kuan-Hung Lin a, Yin-Hsi Kuo a, Yi-Lun Wu a, Winston H. Hsu a,b,*
aDepartment of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan
bGraduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan
a r t i c l e i n f o
Article history:
Available online 1 July 2010
Keywords:
Image retrieval
Object retrieval
Pseudo-object
Visual word
Local feature
Bundle feature
Indexing
Large-scale
a b s t r a c t
State-of-the-art object retrieval systems are mostly based on the bag-of-visual-words representation
which encodes local appearance information of an image in a feature vector. An image object search is
performed by comparing query object‚Äôs feature vector with those for database images. However, a data-
base image vector generally carries mixed information of the entire image which may contain multiple
objects and background. Search quality is degraded by such noisy (or diluted) feature vectors. To tackle
this problem, we propose a novel representation, pseudo-objects ‚Äì a subset of proximate feature points
with its own feature vector to represent a local area, to approximate candidate objects in database
images. In this paper, we investigate effective methods (e.g., grid, G-means, and GMM‚ÄìBIC) to estimate
pseudo-objects. Additionally, we also confirm that the pseudo-objects can significantly benefit
inverted-file indexing both in accuracy and efficiency. Experimenting over two consumer photo bench-
marks, we demonstrate that the proposed method significantly outperforms other state-of-the-art object
retrieval and indexing algorithms.
! 2010 Elsevier Inc. All rights reserved.
1. Introduction
With the prevalence of personal digital cameras and the popu-
larity of social media sites such as Flickr, means to manage large
collections of multimedia data become essential. Content-based
image retrieval (CBIR) is one of the techniques to manage exponen-
tially growing number of photos. CBIR is also the enabling tech-
nique for many emerging applications such as exploring photo
collections in 3D [1], question answering for photos [2], advertising
by image matching [3,4], annotation by search [5], estimating geo-
graphical information from images [6], etc.
Though promising, the success of the above applications heavily
depends on the image retrieval (matching) accuracy and efficiency.
Different from traditional CBIR techniques, the image target such
applications are to match might only cover a small region in the
database image; e.g., a logo in a movie or a landmark in the social
media, i.e., Flickr or YouTube. Meanwhile, the indexing server
needs to give a prompt response (less than a second) as searching
large-scale image collections. In this paper, we will first address
image object retrieval ‚Äì finding the occurrences of specific objects
in the image/photo database, as illustrated in Fig. 1, by estimating
candidate image objects (cf. Section 3); meanwhile, we will also
demonstrate that such effective representations can further benefit
efficient indexing methods boosting large-scale image object
retrieval (cf. Section 4).
1.1. Object retrieval
Conventional image retrieval systems (e.g., QBIC [7] and Visual-
SEEk [8]) aim to search for images that are similar to the query
image. Their feature vector is often designed to describe the image
as a whole. Thus, similarity scores computed according to the fea-
ture vectors represent the global similarity between two images.
Fig. 1(a) shows an example of image query. The user provides a
query image and expects to find images that look like it. The data
image on the right has very different color tone (mainly red and
blue compared to the query‚Äôs yellow and green) and texture distri-
bution (heavily textured on the right side while the query is heav-
ily textured on the left side) from the query image. The user does
not expect this particular data image to be retrieved.
With the exponentially growing number of images, it is not
hard to observe that images often contain multiple salient objects
and that even when there is only one salient object in an image, the
object may not occupy the entirety of the image. This raises the
new problem of object retrieval where users are interested in
specific objects inside images instead of the images themselves.
Fig. 1(b) shows an example of object retrieval. The user provides
a query image and marks the ‚Äò‚ÄòStarbucks logo‚Äù as the query object.
1047-3203/$ - see front matter ! 2010 Elsevier Inc. All rights reserved.
doi:10.1016/j.jvcir.2010.06.003
* Corresponding author at: Graduate Institute of Networking and Multimedia,
National Taiwan University, Taipei, Taiwan.
E-mail address: winston@csie.ntu.edu.tw (W.H. Hsu).
J. Vis. Commun. Image R. 21 (2010) 815‚Äì825
Contents lists available at ScienceDirect
J. Vis. Commun. Image R.
journal homepage: www.elsevier .com/ locate/ jvc i
Author's personal copy
objects in images. This is still an open research problem in the
computer vision community. Secondly, each feature point needs
to be assigned to segmented objects if available through manual
annotation. Besides, manual annotation for modern image data-
base comprised of billions of images is infeasible. We review some
related works for locating specific image objects.
2.1. Image segmentation for objects
The most intuitive way for locating specific objects within an
image is through image segmentation by Normalized Cut, proposed
by Shi and Malik in [16]. The goal is to consider the low-level
coherence of brightness, color, texture, etc., of neighboring pixels.
The segmentation is then treated as a graph partition problem
optimizing the dissimilarity between the different segments and
the total similarity within the segments. The example is illustrated
in Fig. 4(a). Apparently, the segments do not certainly correspond
to semantic-related objects and it is very time consuming; e.g.,
taking around 20 min for segmenting the example image. Instead,
the proposed pseudo-objects, generated by clustering few
thousands (or hundreds) of two-dimensional feature locations,
can be achieved in few seconds (in MATLAB). See more details in
Section 3.
2.2. Spatial pyramid matching
In [17], Lazebnik et al. proposed a spatial pyramid matching
scheme to improve upon the bag-of-words model. In their
approach, images are repetitively divided into finer grids to form
a spatial pyramid. Fig. 3 [17] illustrates the division scheme used.
Level 0 represents the undivided original image. At level 1, the im-
age is evenly divided both horizontally and vertically, resulting in
four equal-sized grids. At level 2, each of the grid in level 1 is again
divided in the same manner. As a result, there are sixteen grids at
level 2. Visual word histograms are computed for each grid at each
level. Finally, all histograms are concatenated into a long vector
that becomes the image‚Äôs feature vector. While concatenating, a
weighting parameter can be applied to histograms from different
levels to emphasize the importance of different levels. In [17], lev-
els with finer grids are given a higher weight. The goal of the long
feature vector is to ensure that images are deemed similar when
they not only look alike globally, but also have corresponding
sub-components that resemble each other.
2.3. Bundle features and visual phrases
Observing the problems in bag-of-visual-words representa-
tions, authors in [18] argue to bundle visual words into local
groups. Unlike a single large feature for the entire image, a bundled
feature provides a flexible representation that allows us to partially
match two groups of feature points, i.e., visual words. Each group
of bundled features becomes much more discriminative than a
single feature, and within each group simple and robust geometric
constraints can be efficiently enforced. The goal is similar to our
pseudo-object proposal in this work. However, the regions are
detected by isolated region detectors, e.g., maximally stable
extremal regions (MSER) [19], which does not consider the co-
occurrence with other feature points (or visual words). Generally,
the MSER regions are over-segmented and do not necessarily
correspond to object locations ‚Äì comparing Fig. 4(d) and Fig. 7.
Fig. 2. System architecture. Pseudo-objects are generated for all database images by considering the spatial consistency for those salient local features (cf. Section 3). Both
query image and pseudo-objects are represented by visual words as in standard bag-of-visual-words model. Retrieval is carried out to generate initial pseudo-object ranking
which is then analyzed to produce final image ranking (cf. Section 3.4). Essentially we add a pseudo-object layer between query and database images to better facilitate
retrieval of sub-image level objects. Meanwhile, we also investigate effective and efficient ways for inverted-file indexing over pseudo-objects (cf. Section 4).
Fig. 3. Spatial pyramid [17]. Level 0 is the original image undivided. At level 1 the image is evenly divided into 4 grids (16 grids for level 2). A visual word frequency histogram
is computed for each grid at each level. Finally, all histograms are concatenated into a long vector that becomes the image‚Äôs feature vector.
K.-T. Chen et al. / J. Vis. Commun. Image R. 21 (2010) 815‚Äì825 817
Author's personal copy
With pseudo-objects, in an automatic manner, we aim to better
represent candidate objects in the image database by additional
feature vectors. This unavoidably results in more storage space re-
quired and query processing speed will also take a hit. These prob-
lems can be mitigated by the use of modern multi-core and multi-
cluster parallel computing platforms [23]. Furthermore, our ap-
proach is fully compatible with and can further improve other
techniques (e.g., spatial verification [24], query expansion [11],
and reranking [12]), in the promising visual word paradigm.
In short, the paper aims to retrieve image objects in million-
scale image database in an efficient and effective way. We argue
to conduct the image object retrieval over the pseudo-objects1.
The proposed framework essentially includes two major parts: (1)
automatic pseudo-object discovery for million-scale images; (2)
inverted-file indexing and efficient online retrieval over the pseu-
do-objects. The main contributions of our work include:
! Identifying the deficiencies in modern object retrieval systems
based on object to full image comparison.
! Proposing pseudo-object representation that averts the problem
mentioned above and investigating effectivemethods (e.g., Grid,
G-means and GMM‚ÄìBIC) for estimating pseudo-objects.
! Demonstrating the significant effectiveness in the proposed
methods by experimenting over two large-scale consumer
photo benchmarks.
! Investigating the efficiency for inverted-file indexing over
pseudo-objects.
! The proposed approach is fully compatible with and can benefit
existing retrieval techniques (i.e., spatial verification, query
expansion, reranking, etc.).
3. Discovering pseudo-objects
As illustrated in the previous section, we propose pseudo-ob-
jects for approximating objects in an image. Pseudo-objects are
in fact subsets of feature points in an image. Therefore, the estima-
tion of pseudo-objects can be treated as a clustering problem on
the two-dimensional spatial coordinates (e.g., horizontal and verti-
cal positions in the image) of the feature points. The goal is to gen-
erate clusters that match true objects as well as possible. An ideal
cluster would be one whose feature points are all from the same
object. Visual word histogram computed using feature points in
this cluster is then a good representation of the object.
We propose three methods for estimating pseudo-objects by
clustering the proximate feature points, characterizing the salient
regions in the images. The first method is the Grid method inspired
by the spatial pyramid matching scheme [17]. The second method
is the G-means method [26] which is an improved version of k-
means that automatically determines the number of clusters (i.e.,
pseudo-objects), k. The third method is the GMM‚ÄìBIC method, a
Gaussian mixture model [27] with Bayesian information criterion
[28] to determine the suitable number of Gaussian components.
The latter two methods are able to estimate the number of clusters
and are desirable since the best number of pseudo-objects in an
image cannot be known. In the following sections we describe
the three proposed methods for estimating pseudo-objects.
3.1. The grid method
The Grid method is inspired by the spatial pyramid matching
scheme described in Section 2.2. An image is divided evenly
along each dimension, forming 4 grids of the same size. An extra
grid of the same size is placed at the center of the image assum-
ing that it is where people tend to place important objects.
Therefore, we define 5 grids for every image. Consequently, each
grid is considered as a pseudo-object and a visual word fre-
quency histogram can be computed by counting feature points
whose x- and y-coordinates fall inside the boundaries of the grid.
With this method, every image is considered as having five pseu-
do-objects. Fig. 4(b) gives an example of the Grid method. The
yellow2 dots are detected feature points for the image. Yellow lines
specify the boundaries of the grids, i.e., the pseudo-object. Note
that because of the overlapping center grid, some feature points
can belong to 2 different pseudo-objects. This is acceptable be-
cause pseudo-objects are approximation to true objects. Neither
true object boundaries nor hard membership to clusters is
necessary.
3.2. The G-means method
While estimating the pseudo-objects, the optimal number of
clusters to generate cannot be known beforehand. In the Grid
method, all images are assumed to have five pseudo-objects. This
is obviously a strong assumption. It is best if the clustering algo-
rithm can determine the optimal number of clusters automatically.
The G-means method [26] is one such algorithm that achieves this
goal with the assumption that data points in a cluster shall follow a
Gaussian distribution. In our case, this is equivalent to assuming
that an object in the image generates feature points around its
center according to a two-dimensional Gaussian distribution. In
reverse, for a feature point in an image, the further away it is from
an object‚Äôs center, the less likely it belongs to that object. Intui-
tively, this assumption is acceptable because it means that feature
points belonging to the same object fall within close proximity of
each other.
The algorithm starts by running k-means with a small k. Each of
the resulting cluster centers is statistically tested to detect whether
its member points are sampled from a Gaussian. A center is re-
tained if so or else split into two new centers. The new centers
are then used as initial centers for the next k-means routine. This
process repeats until no more cluster splits. The cluster number
is therefore automatically determined.
The Anderson‚ÄìDarling statistic [26] is used to test whether the
data assigned to a cluster is sampled from a Gaussian distribution.
The alternative hypotheses are:
H0: The data around the center are sampled from a Gaussian.
H1: The data around the center are not sampled from a
Gaussian.
Once the adjusted Anderson‚ÄìDarling statistic is in the range of
non-critical values, we accept the null hypothesis, H0, keeping the
original cluster. Otherwise, we reject H0 and split the cluster by
replacing the old center with the new centers.
Fig. 5 [26] illustrates the G-means method with a two-dimen-
sional dataset with two true clusters. At first, the dataset is as-
sumed to have only 1 cluster (k-means with k = 1). The initial
center is shown as a ‚Äò+‚Äô sign in the left plot. To test the normality
of this cluster, G-means splits the initial cluster in two (middle
plot) and computes the Anderson‚ÄìDarling statistic. The result is
significant and H0 is rejected. The two new centers are retained.
Each new center is split (right plot) again and tests for the new
splits are not significant. H0 is accepted and the new centers from1 We have the preliminary results in [25]. In this journal paper, we have further
investigated the effects of pseudo-object representation on indexing structure and
memory usage for large-scale image retrieval (cf. Section 4), provided more details for
the proposed approaches (cf. Section 3), experimental results, discussions (cf. Section
5), and more reviews for prior works (cf. Section 2).
2 For interpretation of colour in figures, the reader is referred to the Web version of
this article.
K.-T. Chen et al. / J. Vis. Commun. Image R. 21 (2010) 815‚Äì825 819
Author's personal copy
the hashing framework [33]. However, such efficient approaches
usually accommodate high-dimensional feature points [15] rather
than discrete visual words [9].
As working on bags-of-visual-word paradigm, most works make
use of the text retrieval technique ‚Äì inverted-file, which only consid-
ers documents that havewords in commonwith the query [9,10,22].
Conventionally inverted-file is a stream of postings, which store the
information of the occurrences of the term, such as the id of the doc-
uments that the termoccurs in, the frequency of the term in the doc-
ument, or even the location where the term occurs within the
document. In image retrieval, the visual words are analogous to
words in documents, also analogous to images here.
Recently, it had been shown that inverted-file indexing over
bundles (i.e., regions) of local features is more effective than a im-
age only [18]. Unlike a single large feature for the entire image, a
bundled feature provides a flexible representation that allows us
to partially match two groups of visual words. Each group of bun-
dled features becomes much more discriminative than a single fea-
ture, and within each group simple and robust geometric
constraints can be efficiently enforced. When retrieval is per-
formed, the ranking score is the sum of matched bundled features
for every (inverted) visual word specified by the query image.
Though originally used for duplicate detection, such framework
is promising for indexing image object retrieval.
However, the regions for the bundle features [18] are detected by
a region-based feature detector, MSER [19], whichmerely considers
low-level pixel intensity and does not necessarily correspond to
designated object locations. As shown in Fig. 7, MSER tends to
over-segment the object regions; someof themcontain none or very
few visual words and thus incur unreliable object-level matching.
Meanwhile, each visual wordmight be covered by several MSER re-
gions and thus incurs quite redundancy for inverted-file indexing
over the regions; each region is repeatedly indexed by several visual
words and in turn consumes extra memory space.
For inverted-file indexing, unlike [18], we propose to replace
MSER regions with the automatically discovered pseudo-objects.
We will show that the proposed pseudo-objects can remedy prior
indexing problems and outperform the bundled features [18] both
in retrieval accuracy and indexing efficiency (cf. Section 5.5). It is
natural since the pseudo-objects consider feature spatial correla-
tions in the images.
5. Evaluations
We experiment the proposed methods in two photo retrieval
benchmarks, Oxford buildings [10] and Flickr11K, a subset of
Flickr550 [12]. Some of the queries and their ground truth images
are exemplified in Fig. 8.
5.1. Datasets
5.1.1. Oxford buildings dataset3
The Oxford buildings dataset [10] consists of 5062 images col-
lected by issuing Oxford landmark names as search keywords on
the Flickr website. The authors defined 11 categories of Oxford
landmark queries. Every category contains 5 queries. We use the
cropped image objects from the authors for the 55 queries, two
of which are illustrated in Fig. 8(a) and (b). The images are down-
sized to quarter of the original to match the image dimensions in
our next dataset. The average image size is about 512 ! 374 pixels.
5.1.2. Flickr11k dataset
The Flickr11k dataset is a subset taken from the Flickr550 data-
set [12]. We modify the queries and ground truth defined by the
authors to suit our (content-based) object query criteria. The result
is a total of 1282 ground truth images in 7 query categories with 8
object queries in each category. The query categories are ‚Äò‚ÄòColos-
seum,‚Äù ‚Äò‚ÄòEiffel Tower,‚Äù ‚Äò‚ÄòGolden Gate Bridge,‚Äù ‚Äò‚ÄòTower de Pisa,‚Äù
‚Äò‚ÄòStarbucks logo,‚Äù ‚Äò‚ÄòTower Bridge, ‚Äù and ‚Äò‚ÄòArc de Triomphe.‚Äù We
add 10,000 images randomly sampled from Flickr550 to form our
Flickr11k dataset. The total number of images is 11,282. The aver-
age image size is about 500 ! 360 pixels. Note that the queries are
objects that might only occupy a smaller portion of the image,
illustrated in Fig. 8(c) and (d).
5.2. Performance metrics
In order to evaluate the retrieval performance, we use the aver-
age precision (AP) as the major performance metric. Widely
adopted in large-scale photo/video retrieval benchmarks such as
Oxford buildings [10] and Flickr550 [12], AP approximates the area
under a non-interpolated precision-recall curve for a query. Since
AP only shows the performance for a single image query, generally
we compute mean average precision (MAP) to represent the sys-
tem performance over the all queries; thereby, 55 query images
(across 11 categories) in Oxford dataset and 56 query images
(across 7 categories) in Flickr11K dataset.
Fig. 6. Query image is compared with the automatically discovered pseudo-objects. The L1 distance from the query to the pseudo-objects (marked by their corresponding
colors). Note that the entire image is treated as one of the pseudo-objects (white) for comparison. The distance among the objects are then utilized to rank the image they
belong to (cf. Section 3.4).
3 Note that we do not aim to optimize the retrieval performance for the benchmark
but investigate relative improvements for the proposed method.
K.-T. Chen et al. / J. Vis. Commun. Image R. 21 (2010) 815‚Äì825 821
Author's personal copy
outperform prior works. Table 1 summarizes search performance
by categories in the Oxford building dataset over the baseline
method ‚Äì one visual word vector only for the entire image.
Generally, G-means and GMM‚ÄìBIC gain the most improvements
over the baseline and spatial pyramid methods. The two proposed
pseudo-object estimation methods can generally catch local char-
acteristics and respectively improve up to 35.3% and 44.1% (on
the average for those benefit from pseudo-objects) over the
baseline method. The improvement is especially obvious when
the candidate objects in database images are relatively small com-
pared to the entire image; e.g., in the categories such as ‚ÄòBodleian‚Äô,
‚ÄòAshmolean‚Äô, ‚ÄòChrist_church‚Äô, etc. This is because the proposed
pseudo-objects-based methods can identify specific objects that
the user might be interested in. Note that the two methods are
adaptive and require no pre-fixed cluster (or pseudo-object)
number.
The spatial pyramid matching scheme only enhances query cat-
egories whose target objects usually appear at the center of ground
truth images, leaving little space for background noises and inter-
ference from other objects. However, if the set of ground truth
images exhibits enough intra-variation as query object appearing
in different sizes and at different locations, the spatial pyramid
method degrades the performance (Table 1). The Grid method
requires the least computing power and achieves satisfactorily.
However, it is inevitable that some objects are divided into multi-
ple grids and results in less informative feature vectors and waste
of storage space. As the image database grows more diverse, this
problem may become more serious.
The G-means method improves less than the GMM‚ÄìBIC method
in the Oxford buildings dataset. It is because the underlying k-
means algorithm tends to generate more circular clusters. For
example, tower-shaped objects are often broken down into smaller
round sections. The GMM‚ÄìBIC method generates clusters that bet-
ter match true objects.
Fig. 10 shows the results of one of the ‚ÄôStarbucks logo‚Äô queries in
Flickr11k dataset. The layout is similar to that of Fig. 9. The number
beside each row shows the number of corrects hits in top 10 results
for that method. The thumbnails are for images ranked from 11 to
14. We can see that even though the baseline method (a) succeeds
at at returning good top 10 results (8 out of 10 correct hits), it
Fig. 9. Example pseudo-object retrieval results. Retrieval results of the ‚ÄòChrist_church_3‚Äô query in the Oxford buildings dataset. On the left side, the query object is marked by
a red rectangle. The rows show top 4 results of (a) query object matching to global image histograms [9,10], (b) spatial pyramid matching [17], matching over pseudo-objects
estimated by (c) the Grid method, (d) the G-means method, and (e) the GMM‚ÄìBIC method. The last two methods, more flexible in estimating the number and location of
pseudo-objects, clearly outperform the others.
Table 1
Search performance on the Oxford buildings dataset. The right column for each method shows percentage change in performance over the baseline method [9,10]. Categories that
benefit significantly from our methods are highlighted in bold fonts. Their average gain is shown at the bottom line of each dataset. The ‚Äò‚ÄòMAP (!)‚Äù (bottom row) shows the MAP
over (selected) categories benefiting from pseudo-objects. Note that ‚Äò‚ÄòChrist‚Äù abbreviates for ‚Äò‚ÄòChrist_church‚Äù and ‚Äò‚ÄòRadcliffe‚Äù for ‚Äò‚ÄòRadcliffe_camera.‚Äù
Oxford buildings Baseline Spatial pyramid Grid G-means GMM‚ÄìBIC
All_souls 0.439 0.450 2.5% 0.461 5.1% 0.460 4.8% 0.453 3.1%
Ashmolean 0.228 0.219 "3.9% 0.307 35.1% 0.290 27.4% 0.321 40.9%
Balliol 0.305 0.316 3.8% 0.305 0.0% 0.305 0.0% 0.305 0.0%
Bodleian 0.061 0.070 16.3% 0.113 86.1% 0.117 93.7% 0.120 99.0%
Christ 0.201 0.201 0.1% 0.254 26.7% 0.293 46.0% 0.286 42.4%
Cornmarket 0.404 0.420 4.0% 0.412 2.1% 0.404 0.0% 0.416 3.0%
Hertford 0.442 0.458 3.7% 0.442 0.0% 0.442 0.0% 0.442 0.0%
Keble 0.284 0.291 2.7% 0.343 20.8% 0.347 22.2% 0.395 39.3%
Magdalen 0.051 0.051 "1.0% 0.069 34.6% 0.067 31.0% 0.065 26.7%
Pitt_rivers 0.512 0.537 5.0% 0.512 0.0% 0.512 0.0% 0.512 0.0%
Radcliffe 0.556 0.563 1.3% 0.556 0.0% 0.556 0.0% 0.556 0.0%
MAP 0.316 0.325 2.8% 0.343 8.4% 0.345 8.9% 0.352 11.2%
MAP (*) 0.165 0.166 1.0% 0.217 31.8% 0.223 35.3% 0.237 44.1%
K.-T. Chen et al. / J. Vis. Commun. Image R. 21 (2010) 815‚Äì825 823
Author's personal copy
the results in Table 3, which shows that inverted-file indexing over
pseudo-objects outperforms that over MSER regions in time per
query, memory consumption, and retrieval performance4.
For efficiency (retrieval time for each query), in each image each
visual word might need to consider less than 5 regions by the pseu-
do-object method. However, few hundreds are required for those
by MSER, where the postings for each inverted entry is much larger
since bundle features utilize lots of overlapped MESR regions; such
deficiency also incurs more memory requirements in the prior
work [18], as shown in Table 3.
For retrieval accuracy, pseudo-objects are more meaningful
than the MSER regions as corresponding to candidate objects, as
illustrated in Fig. 7 (comparing with the pseudo-objects in
Fig. 4(d)). MSER regions often contain none or very few visual
words and thus suffer from unreliable object-level matching and
lower retrieval accuracy.
6. Conclusions and future work
We proposed the novel idea of pseudo-objects for improving
object-level retrieval in consumer photos and investigated auto-
maticmethods (e.g., Grid, G-means, GMM‚ÄìBIC) for estimating pseu-
do-objects. We showed that our proposed methods boost search
performance significantly by experimenting on two large datasets.
The GMM‚ÄìBICmethod is shown effective for pseudo-object estima-
tion and significantly outperforms other state-of-the-art object
retrieval algorithms. We also further demonstrate that the pseudo-
objects are effective in benefiting inverted-file indexing methods
and outperform the prior method both in accuracy and efficiency.
In the future, we plan to investigate means to mitigate the pos-
sible side effects of a large number of additional feature vectors. A
possibility is to utilize powerful multi-core multi-computer plat-
forms such as the google cluster architecture [23]. We will also
study promising dimension reduction techniques which can be ap-
plied to our framework to reduce memory usage. To leverage the
novel utilities for pseudo-objets in large-scale consumer photos,
we are extending the framework in million-scale image collections
and investigating its further capabilities in query expansion for
improving retrieval quality.
References
[1] N. Snavely, S.M. Seitz, R. Szeliski, Photo tourism: exploring photo collections in
3d, in: SIGGRAPH, 2006.
[2] T. Yeh, J.J. Lee, T. Darrell, Photo-based question answering, in: ACM
Multimedia, 2008.
[3] Y. Li, K.W. Wan, X. Yan, C. Xu, Real time advertisement insertion in
baseball video based on advertisement effect, in: ACM Multimedia,
2005.
[4] W.-S. Liao, K.-T. Chen, W.H. Hsu, Adimage: video advertising by image
matching and ad scheduling optimization, in: ACM SIGIR, 2008.
[5] X.-J. Wang, L. Zhang, F. Jing, W.-Y. Ma, Annosearch: Image auto-annotation by
search, in: CVPR, 2006.
[6] J. Hays, A.A. Efros, Im2gps: estimating geographic information from a single
image, in: CVPR, 2008.
[7] M. Flickner, H. Sawhney, W. Niblack, J. Ashley, Q. Huang, B. Dom, M. Gorkani, J.
Hafner, D. Lee, D. Petkovic, D. Steele, P. Yanker, Query by image and video
content: the QBIC system, Computer 28 (9) (1995) 23‚Äì32.
[8] J.R. Smith, S.F. Chang, Visualseek: a fully automated content-based image
query system, in: ACM Multimedia, 1996.
[9] J. Sivic, A. Zisserman, Video google: a text retrieval approach to object
matching in videos, in: ICCV, 2003.
[10] J. Philbin, O. Chum, M. Isard, J. Sivic, A. Zisserman, Object retrieval with large
vocabularies and fast spatial matching, in: CVPR, 2007.
[11] O. Chum, J. Philbin, J. Sivic, M. Isard, A. Zisserman, Total recall: automatic
query expansion with a generative feature model for object retrieval, in:
ICCV, 2007.
[12] Y.H. Yang, P.T. Wu, C.W. Lee, K.H. Lin, W.H. Hsu, H.H. Chen, Contextseer:
context search and recommendation at query time for shared consumer
photos, in: ACM Multimedia, 2008.
[13] K. Mikolajczyk, C. Schmid, Scale & affine invariant interest point detectors, Int.
J. Comput. Vision 60 (1) (2004) 63‚Äì86.
[14] K. Mikolajczyk, T. Tuytelaars, C. Schmid, A. Zisserman, J. Matas, F. Schaffalitzky,
T. Kadir, L. Van Gool, A comparison of affine region detectors, Int. J. Comput.
Vision 65 (1‚Äì2) (2005) 43‚Äì72.
[15] D.G. Lowe, Distinctive image features from scale-invariant keypoints, Int. J.
Comput. Vision 60 (2) (2004) 91‚Äì110.
[16] J. Shi, J. Malik, Normalized cuts and image segmentation, IEEE Trans. Pattern
Anal. Mach. Intell. 22 (8) (2000) 888‚Äì905.
[17] S. Lazebnik, C. Schmid, J. Ponce, Beyond bags of features: spatial pyramid
matching for recognizing natural scene categories, in: CVPR, 2006.
[18] Z. Wu, Q. Ke, M. Isard, J. Sun, Bundling features for large scale partial-duplicate
web image search, in: CVPR, 2009.
[19] J. Matas, O. Chum, U. Martin, T. Pajdla, Robust wide baseline stereo from
maximally stable extremal regions, in: Proceedings of British Machine Vision
Conference, London, 2002.
[20] Y. Zheng, M. Zhao, S.-Y. Neo, T.-S. Chua, Q. Tian, Visual synset: towards a
higher-level visual representation, in: CVPR, 2008.
[21] S. Zhang, Q. Tian, G. Hua, Q. Huang, S. Li, Descriptive visual words and visual
phrases for image applications, in: ACM Multimedia, 2009.
[22] J. Zobel, A. Moffat, Inverted files for text search engines, ACM Comput. Surv. 38
(2) (2006) 6.
[23] L.A. Barroso, J. Dean, U. Holzle, Web search for a planet: the google cluster
architecture, IEEE Micro 23 (2) (2003) 22‚Äì28.
[24] O. Chum, J. Matas, S. Obdrzalet, Enhancing ransac by generalized model
optimization, in: ACCV, 2004.
[25] K.-H. Lin, K.-T. Chen, W. Hsu, C.-J. Lee, T.-H. Li, Boosting object retrieval by
estimating pseudo-objects, in: International Conference on Image Processing
(ICIP), Cairo, Egypt, 2009.
[26] G. Hamerly, C. Elkan, Learning the k in k-means, in: Neural Information
Processing Systems, 2003.
[27] L. Rabiner, B.-H. Juang, Fundamentals of Speech Recognition, Prentice Hall PTR,
1993.
[28] G. Schwarz, Estimating the dimension of a model, Ann. Stat. 6 (2) (1978) 461‚Äì
464.
[29] A.P. Dempster, N.M. Laird, D.B. Rubin, Maximum likelihood from incomplete
data via the em algorithm, J. Roy. Stat. Soc. Ser. B (Methodological) 39 (1)
(1977) 1‚Äì38.
[30] K. McDonald, A.F. Smeaton, A comparison of score, rank and probability-based
fusion methods for video shot retrieval, in: CIVR, Singapore, 2005.
[31] M. Datar, P. Indyk, Locality-sensitive hashing scheme based on p-stable
distributions, in: Symposium on Computational Geometry, 2004.
[32] Y. Ke, R. Sukthankar, L. Huston, An efficient parts-based near-duplicate and
sub-image retrieval system, in: ACM Multimedia, 2004.
[33] Y.-H. Kuo, K.-T. Chen, C.-H. Chiang, W.H. Hsu, Query expansion for hash-based
image object retrieval, in: ACM Multimedia, 2009.
Table 3
Inverted-file indexing experiments on two datasets. Bundle features (in MSER
regions) [18] require more processing time and more memory space due to their over
segmentation, where each visual word might link to multiple MSER regions.
Meanwhile, these low-level regions do not necessarily correspond to designated
object locations. Note that we use one million (1 M) visual words in this experiment
for the comparison with the prior work [18].
Regions Query time (ms) Memory (MB) MAP
Oxford MSER 80 140 0.318
Pseudo-object 6 90 0.378
Flickr11K MSER 140 290 0.193
Pseudo-object 6 180 0.210
4 Note that we do not aim to optimize the benchmarks but provide relative
performances for the designated methods.
K.-T. Chen et al. / J. Vis. Commun. Image R. 21 (2010) 815‚Äì825 825
tive AVWs across the visual and textual graphs since these
two modalities can boost each other (cf. Figure 3). The two
processes are formulated as optimization formulations itera-
tively through the subtopics in the image collections. Mean-
while, we also consider the scalability issues by leveraging
distributed computation framework (e.g., MapReduce).
Experiments show that the proposed method greatly im-
proves the recall rate for image object retrieval. Specifically,
the unsupervised auxiliary visual words discovery greatly
outperforms BoWmodels (by 111% relatively) and comple-
mentary to conventional pseudo-relevance feedback. Mean-
while, AVW discovery can also derive very compact (i.e.,
1.4% of the original features) and informative feature rep-
resentations which will benefit the indexing structure [14].
The primary contributions of the paper include,
‚Ä¢ Observing the problems in large-scale image object re-
trieval by conventional BoW model (Section 3).
‚Ä¢ Proposing auxiliary visual words discovery through vi-
sual and textual clusters in an unsupervised and scal-
able fashion (Section 4).
‚Ä¢ Investigating variant optimization methods for effi-
ciency and accuracy in AVW discovery (Section 5).
‚Ä¢ Conducting experiments on consumer photos and
showing great improvement of recall rate for image ob-
ject retrieval (Section 7).
2. Related Work
Most image object retrieval systems adopt the scale-
invariant feature transform (SIFT) descriptor [8] to capture
local information and adopt bag-of-words (BoW) model
[14] to conduct object matching [1, 11]. The SIFT descrip-
tors are quantized to visual words (VWs), such that index-
ing techniques well developed in the text domain can be
directly applied.
The learned VW vocabulary will directly affect the im-
age object retrieval performance. The traditional BoW
model adopts k-means clustering to generate the vocabu-
lary. A few attempts try to impose extra information for vi-
sual word generation such as visual constraints [13], textual
information [19]. However, it usually needs extra (manual)
information during the supervised learning, which might be
formidable in large-scale image collections.
Instead of generating new VW vocabulary, some re-
searches work on the original VW vocabulary such as [15].
It suggested to select useful feature from the neighboring
images to enrich the feature description. However, its per-
formance is limited for large-scale problems because of
the need to perform spatial verification, which is compu-
tationally expensive. Moreover, it only considers neighbor-
ing images in the visual graph, which provides very lim-
ited semantic information. Other selection methods for the
0
20
40
60
80
100
0 0.2 0.4 0.6 0.8 1 1.2
Images (%)
V
is
ua
l w
or
ds
 (%
)
Flickr11K (11,282 images)
Flickr550 (540,321 images)
Half of the visual words occur in
Flickr11K: 0.106% (12 images)
Flickr550: 0.114% (617 images)
Figure 2. Cumulative distribution of the frequency of VW occur-
rence in two different image databases, cf. Section 3.1. It shows
that half of the VWs occur in less than 0.11% of the database im-
ages (i.e., 12 and 617 images, respectively). The statistics rep-
resent that VWs are distributed over the database images very
sparsely.
useful features such as [6] and [10] are based on different
criteria‚Äîthe number of inliers after spatial verification, and
pairwise constraints for each image, thus suffer from lim-
ited scalability and accuracy.
Authors in [9] consider both visual and textual informa-
tion and adopt unsupervised learning methods. However,
they only use global features and adopt random-walk-like
process for post-processing in image retrieval. Similar limi-
tations are observed in [16], where only the similarity image
scores are propagated between textual and visual graphs.
Different from the prior works, we use local features for
image object retrieval and propagate the VWs directly be-
tween the textual and visual graphs. The discovered aux-
iliary VWs are thus readily effective in retrieving diverse
search results, eliminating the need to apply a random walk
in the graphs again.
To augment images with their informative features, we
propose auxiliary visual words discovery, which can effi-
ciently propagate semantically relevant VWs and select rep-
resentative visual features by exploiting both textual and vi-
sual graphs. The discovered auxiliary visual words demon-
strate significant improvement over the BoW baseline and
are shown orthogonal and complementary to conventional
pseudo-relevance feedback. Besides, when dataset size be-
comes larger, we can apply all the processes in a parallel
way (e.g., MapReduce).
3. Key Observations‚ÄîRequiring Auxiliary Vi-
sual Words
Nowadays, bag-of-words (BoW) representation [14] is
widely used in image object retrieval and has been shown
promising in several content-based image retrieval (CBIR)
tasks (e.g., [11]). However, most existing systems sim-
ply apply the BoW model without carefully considering the
sparse effect of the VW space, as detailed in Section 3.1.
906
(a) A visual cluster sample. (b) A textual cluster sample.
Figure 4. Sample image clusters (cf. Section 4.1). The visual clus-
ter groups visually similar images in the same cluster, whereas the
textual cluster favors semantic similarities. The two clusters facil-
itate representative VWs selection and semantic (auxiliary) VWs
propagation, respectively.
4.2. Semantic Visual Words Propagation
Seeing the limitations in BoWmodel, we propose to aug-
ment each image with additional VWs propagated from the
visual and textual clusters (Figure 5(a)). Propagating the
VWs from both visual and textual domains can enrich the
visual descriptions of the images and be beneficial for fur-
ther image object queries. For example, it is promising to
derive more semantic VWs by simply exchanging the VWs
among (visually diverse but semantically consistent) images
of the same textual cluster (cf. Figure 4(b)) .
We actually conduct the propagation on each extended
visual cluster, containing the images in a visual cluster and
those additional ones co-occuring with these images in cer-
tain textual clusters. The intuition is to balance visual and
semantic consistence for further VW propagation and se-
lection (cf. Section 4.3). Figure 5(b) shows two extended
visual clusters derived from Figure 5(a). More interestingly,
image E is singular in textual cluster due to having no tags;
however, E still belongs to a visual cluster and can still re-
ceive AVWs in its associated extended visual cluster. Sim-
ilarly, if there is a single image in a visual cluster such as
imageH , it can also obtain auxiliary VWs (i.e., from image
B and F ) in the extended visual cluster.
Assuming matrix X ‚àà RN√óD represents the N image
histograms in the extended visual cluster and each image
has D (i.e., 1 million) dimensions. And Xi stands for the
VW histogram of image i. Assume M among N are from
the same visual cluster; for example, N = 8 and M = 4
in the left extended visual cluster in Figure 5(b). The vi-
sual propagation is conducted by the propagation matrix
P ‚àà RM√óN , which controls the contributions from differ-
ent images in the extended visual cluster.1 P (i, j) weights
1Note that here we first measure the images from the same visual cluster
only. However, by propagating through each extended visual clusters, we
can derive the AVWs for each image.
C
F
A
E
Visual clusters
Textual clusters
H
A D
G
C
F
B H
E
(a) Visual and textual graphs.
C
F
A
E
From visual clusters
From textual clusters
G
D
B
H
H
B
F
(b) Two extended visual clus-
ters from the (left) visual and
textual clusters.
Figure 5. Illustration of the propagation operation. Based on vi-
sual and textual graphs in (a), we can propagate auxiliary VWs
among the associated images in the extended visual clusters. (b)
shows the two extended visual clusters as the units for propaga-
tion respectively; each extended visual cluster include the visually
similar images and those co-occurrences in other textual clusters.
the whole features propagated from image j to i. If we mul-
tiply the propagation matrix P andX (PX), we can obtain
a new M √ó D VW histograms, as the AVWs, for the M
images augmented by the N images.
For each extended visual cluster, we desire to find a bet-
ter propagation matrix P , given the initial propagation ma-
trix P0 (i.e., P0(i, j) = 1, if both i and j are semantically
related and within the same textual cluster). We propose to
formulate the propagation operation as
fP = min
P
Œ±
‚ÄñPX‚Äñ2F
NP1
+ (1‚àí Œ±)‚ÄñP ‚àí P0‚Äñ
2
F
NP2
, (1)
The goal of the first term is to avoid from propagating too
many VWs (i.e., propagating conservatively) since PX be-
comes new VW histogram matrix after the propagation.
And the second term is to keep the similarity to the orig-
inal propagation matrix (i.e., similar in textual cluster).
NP1 = ‚ÄñP0X‚Äñ2F and NP2 = ‚ÄñP0‚Äñ2F are two normaliza-
tion terms and Œ±modulates the importance between the first
and the second terms. We will investigate the effects of Œ±
in Section 7.2. Note that the propagation process updates
the propagation matrix P on each extended visual cluster
separately as shown in Figure 5(b); therefore, this method
is scalable for large-scale dataset and easy to adopt in a par-
allel way.
4.3. Common Visual Words Selection
Though the propagation operation is important to obtain
different VWs, it may include too many VWs and thus de-
crease the precision. To mitigate this effect and remove
those irrelevant or noisy VWs, we propose to select those
representative VWs in each visual cluster. We observe that
images in the same visual cluster are visually similar to each
other (cf. Figure 4(a)); therefore, the selection operation is
to retain those representative VWs in each visual cluster.
908
the matrices and get
(X‚äóIM )(XT‚äóIM )p = (X‚äóIM )vec(PX) = vec(PXXT )
Then,
‚àápf(p) = 2Œ±1vec(PXXT ) + 2Œ±2vec(P ‚àí P0)
= vec(2Œ±1PXXT + 2Œ±2(P ‚àí P0)).
That is, we can update pold as a matrix P old with the gra-
dient also represented in its matrix form. Coupling the up-
date scheme with an adaptive learning rate Œ∑, we get update
propagation matrix by
Pnew =P old ‚àí 2Œ∑(Œ±1P oldXXT + Œ±2(P old ‚àí P0)).(6)
Note that we simply initialize pstart to vec(P0).
For the selection formulation (Section 4.3), we can adopt
similar steps with two changes. And let Œ≤1 = Œ≤NS1 and
Œ≤2 =
1‚àíŒ≤
NS2
. First, Eq. (6) is replaced with
Snew =Sold ‚àí 2Œ∑(‚àíŒ≤1XTX(S0 ‚àí Sold) + Œ≤2Sold).(7)
Second, the initial point Sstart is set to a zero matrix since
the goal of selection formulation is to select representative
visual words (i.e., retain a few dimensions).
There is one potential caveat of directly using Eq. (7)
for updating. The matrix XTX can be huge (e.g., 1M √ó
1M ). To speed up the computation, we could keep only the
dimensions that occured in the same visual cluster, because
the other dimensions would contribute 0 to XTX .
5.3. Analytic Solver (AS)
Next, we compute the unique optimal solution p‚àó of
Eq. (1) analytically. The optimal solution must satisfy
‚àápf(p‚àó) = 0. Note that From Eq. (4),
‚àápf(p‚àó) = Hp‚àó ‚àí 2Œ±2p0,
where H is the constant and positive definite Hessian ma-
trix. Thus,
p‚àó = 2Œ±2H‚àí1p0.
Similar to the derivation in the gradient descent solver, we
can write down the matrix form of the solution, which is
P ‚àó = Œ±2P0(Œ±1XXT + Œ±2IM )‚àí1.
For the selection formulation, a direct solution from the
steps above would lead to
S‚àó = Œ≤1(Œ≤1XTX + Œ≤2ID)‚àí1XTXS0. (8)
Nevertheless, as mentioned in the previous subsection,
theXTX matrix in Eq. (8) can be huge (e.g., 1M√ó1M ). It
is a time-consuming task to compute the inverse of an 1M√ó
1M matrix. Thus, instead of calculating XTX directly, we
transform XTX to XXT which is N by N and is much
smaller (e.g., 100 √ó 100). The transformation is based on
the identity of the inverse function
(A+BBT )‚àí1B = A‚àí1B(I +BTA‚àí1B)‚àí1.
Then, we can re-write Eq. (8) as
S‚àó = Œ≤1XT (Œ≤1XXT + Œ≤2IN )‚àí1XS0. (9)
Note that the analytic solutions of Eq. (1) and (2) are of a
similar form to the solutions of ridge regression (Tikhonov
regularization) in statistics and machine learning. The fact
is of no coincidence. Generally speaking, we are seeking
to obtain some parameters (P and S) from some data (X ,
P0 and S0) while regularizing by the norm of the param-
eters. The use of the regularization not only ensures the
strict convexity of the optimization problem, but also eases
the hazard of overfitting with a suitable choice of Œ± and Œ≤.
6. Experimental Setup
6.1. Dataset
We use Flickr550 [20] as our main dataset in the exper-
iments. To evaluate the proposed approach, we select 56
query images (1282 ground truth images) which belong to
the following 7 query categories: Colosseum, Eiffel Tower,
Golden Gate Bridge, Tower de Pisa, Starbucks logo, Tower
Bridge, and Arc de Triomphe. Also, we randomly pick
up 10,000 images from Flickr550 to form a smaller sub-
set called Flickr11K.2 Some query examples are shown in
Figure 7.
6.2. Performance Metrics
In the experiments, we use the average precision, a
performance metric commonly used in the previous work
[11, 20], to evaluate the retrieval accuracy. It approximates
the area under a non-interpolated precision-recall curve for
a query. A higher average precision indicates better retrieval
accuracy. Since average precision only shows the perfor-
mance for a single image query, we also compute the mean
average precision (MAP) over all the queries to evaluate the
overall system performance.
6.3. Evaluation Protocols
As suggested by the previous work [11], our image ob-
ject retrieval system adopts 1 million visual words as the ba-
sic vocabulary. The retrieval is then conducted by compar-
ing (indexing) the AVW features for each database image.
To further improve the recall rate of retrieval results, we
apply the query expansion technique of pseudo-relevance
2http://www.cmlab.csie.ntu.edu.tw/%7Ekuonini/Flickr11K
910
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1MAP (M)
Alpha
0
0.2
0.4
0.6
0
20
40
60
80
100
MAP by PRF MAP #Features
(a) Alpha (Œ±, for propagation in Eq. (1))
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1MAP (M)
Beta
0
0.1
0.2
0.3
0.4
0
10
20
30
MAP by PRF MAP #Features
(b) Beta (Œ≤, for selection in Eq. (2))
Figure 8. Parameter sensitivity on alpha and beta. (a) shows that
propagating too many features is not helpful for the retrieval accu-
racy. (b) shows that only partial features are important (represen-
tative) to each image. More details are discussed in Section 7.2.
Note that we can further improve retrieval accuracy by iteratively
updated AVW by propagation and selection processes.
features needed to be propagated. Figure 8(a) shows that if
we propagate all the possible features to each image (i.e.,
Œ± = 0), we will obtain too many irrelevant and noisy fea-
tures which is helpless for the retrieval accuracy. Besides,
the curve drops fast after Œ± ‚â• 0.8 because it preserved few
VWs which might not appear in the query images. The fig-
ure also shows that if we set Œ± around 0.6we can have better
result but with fewer features which are essential for large-
scale indexing problem.
And for selection formulation, similar to Œ±, Œ≤ also influ-
ences the number of dimensions needed to be retained. For
example, if Œ≤ = 0, we will not select any dimensions for
each image. And Œ≤ = 1 means we will retain all the fea-
tures, and the result is equal to the BoW baseline. Figure
8(b) shows that if we just keep a few dimensions of VWs,
the MAP is still similar to BoW baseline though with some
retrieval accuracy decrease. Because of the spareness of
large VW vocabulary as mentioned in Section 3.1, we only
need to keep those important VWs.
8. Conclusions and Future Work
In this work, we show the problems of current BoW
model and the needs for semantic visual words to improve
the recall rate for image object retrieval. We propose to aug-
ment each database image with semantically related auxil-
iary visual words by propagating and selecting those infor-
mative and representative VWs in visual and textual clus-
ters. Note that we formulate the processes as unsupervised
optimization problems. Experimental results show that we
can greatly improve the retrieval accuracy compared to the
BoW model (111% relatively). In the future, we will fur-
ther look into the problem by L2-loss L1-norm optimiza-
tion which might preserve the sparseness for visual words.
We will also investigate different solvers to maximize the
retrieval accuracy and efficiency.
References
[1] O. Chum et al. Total recall: Automatic query expansion
with a generative feature model for object retrieval. In IEEE
ICCV, 2007.
[2] O. Chum et al. Geometric min-hashing: Finding a (thick)
needle in a haystack. In IEEE CVPR, 2009.
[3] J. Dean et al. Mapreduce: Simplified data processing on
large clusters. In OSDI, 2004.
[4] T. Elsayed et al. Pairwise document similarity in large col-
lections with mapreduce. In Proceedings of ACL-08: HLT,
Short Papers, pages 265‚Äì268, 2008.
[5] B. J. Frey et al. Clustering by passing messages between data
points. Science, 2007.
[6] S. Gammeter et al. I knowwhat you did last summer: Object-
level auto-annotation of holiday snaps. In IEEE ICCV, 2009.
[7] J. Hays et al. im2gps: estimating geographic information
from a single image. In IEEE CVPR, 2008.
[8] D. G. Lowe. Distinctive image features from scale-invariant
keypoints. IJCV, 2004.
[9] H. Ma et al. Bridging the semantic gap between image con-
tents and tags. IEEE TMM, 2010.
[10] P. K. Mallapragada et al. Online visual vocabulary pruning
using pairwise constraints. In IEEE CVPR, 2010.
[11] J. Philbin et al. Object retrieval with large vocabularies and
fast spatial matching. In IEEE CVPR, 2007.
[12] J. Philbin et al. Lost in quantization: Improving particu-
lar object retrieval in large scale image databases. In IEEE
CVPR, 2008.
[13] J. Philbin et al. Descriptor learning for efficient retrieval. In
ECCV, 2010.
[14] J. Sivic et al. Video Google: A text retrieval approach to
object matching in videos. In IEEE ICCV, 2003.
[15] P. Turcot et al. Better matching with fewer features: The se-
lection of useful features in large database recognition prob-
lems. In IEEE ICCV Workshop on WS-LAVD, 2009.
[16] X.-J. Wang et al. Multi-model similarity propagation and its
application for web image retrieval. In ACM MM, 2004.
[17] X.-J. Wang et al. Annosearch: Image auto-annotation by
search. In IEEE CVPR, 2006.
[18] X.-J. Wang et al. Arista - image search to annotation on
billions of web photos. In CVPR, 2010.
[19] L. Wu et al. Semantics-preserving bag-of-words models for
efficient image annotation. In ACM workshop on LSMMRM,
2009.
[20] Y.-H. Yang et al. Contextseer: Context search and recom-
mendation at query time for shared consumer photos. In
ACM MM, 2008.
[21] X. Zhang et al. Efficient indexing for large scale visual
search. In IEEE ICCV, 2009.
912
the end, we apply object-level pseudo-relevance feedback to 
refine the search result and improve the recall rate. Unlike its 
conventional counterpart, the proposed object-level pseudo-
relevance feedback places more importance on local objects 
instead of the whole image. 
3. OBJECT-LEVEL INVERTED INDEXING 
Inverted file is a popular way to index large-scale data in the 
information retrieval community [7]. Because of its superiority of 
efficiency, many recent image retrieval systems adopt the concept 
to index visual features (i.e. VWs). The intuitive way is to record 
each entry with <image ID, VW frequency> in the inverted file. 
However, to our best knowledge, most systems simply adopt the 
conventional method to the visual domain, without considering 
the differences between documents and images, where the image 
query is composed of thousands of (noisy) VWs and the object of 
interest may occupy small portions of the target images. 
3.1 Pseudo-Objects 
Images often contain several objects so we cannot take the whole 
image features to represent each object. Each object has its 
distinctive VWs. Motivated by the novelty and promising retrieval 
accuracy in [4], we adopt the concept of pseudo-object‚Äîa subset 
of proximate feature points with its own feature vector to 
represent a local area. An example shows in Figure 4 that the 
pseudo-objects, efficiently discovered, can almost catch different 
objects; however, advanced methods such as efficient indexing or 
query expansion are not considered. We further propose a novel 
object-level inverted indexing. 
3.2 Index Construction 
Unlike document words, VWs have a spatial dimension. 
Neighboring VWs often correspond to the same object in an 
image, and an image consists of several objects. We adopt 
pseudo-objects and store the object information in the inverted file 
to support object-level image retrieval. Specifically, we construct 
an inverted list for each VW t as follows, <Image ID i, ft,i, 
RID1, ... ,RIDf>, which indicates the ID of the image i where the 
VW appears, the occurrence frequency (ft,i), and the associated 
object region ID (RIDf) in each image. The addition of the object 
ID to the inverted file makes it possible to search for a specific 
object even if the object only occupies a small region of an image. 
3.3 Index Compression 
Index compression is a common way to reduce memory usage in 
textual domain. First, we discard the top 5% frequent VWs as stop 
words to decrease the mismatch rate and reduces the size of 
inverted file. We then adopt different coding methods to compress 
data based on their visual characteristics. Image IDs are ordinal 
numbers sorted in ascending order in the lists, thus we store the 
difference between adjacent image IDs instead of the image ID 
itself which is called d-gap [7]. And for region IDs, we adopt a 
fixed length bit-level coding of three bits to encode it (e.g., R2 √Ü 
010). On the other hand, we use a variant length bit-level coding 
to encode frequency (e.g., 3 √Ü 1110). Furthermore, we implement 
AND and SHIFT operations to efficiently decode the frequency 
and region IDs at query time. The memory space for indexing 
pseudo-objects can be saved about 54.1%.  
3.4 Object-Level Scoring Method 
We use the intersection of TFIDF, which performs the best for 
matching, to calculate the score of each region indexed by VW t. 
Besides the discovered pseudo-objects, we also define a new 
object R0 to treat the whole image as another object. We first 
calculate the score of every pseudo-object (R) to the query object 
(Q) as follows, 
,),min(),( ,,¬¶
¬è
u 
Qt
QtRtt wwIDFQRscore  (1)
where wt,R and wt,Q are the normalized VW frequency in pseudo-
object and in the query respectively. And then the pseudo-object 
with the highest score is regarded as the most relevant object with 
respect to the query, as suggested in [4]: 
score ( i, Q )  max{ score ( R , Q ) | R ¬è i}.  (2)
3.5 Efficient Query Evaluation (EQE) 
Conventional query evaluation in inverted indexing needs to keep 
track of the scores of all images in the inverted lists. In fact, it is 
observed that most of the scored images contain only a few 
matched VWs. We propose an efficient query evaluation (EQE) 
algorithm that explores a small part of a large-scale database to 
reduce the online retrieval time. The procedures of EQE are 
described below and illustrated in Figure 3. 
 
Figure 2: The system diagram. Offline part: We extract visual and textual features from images. Textual and visual image graphs 
are constructed by an inverted list-based approach and clustered by an adapted affinity propagation algorithm by MapReduce (18 
Hadoop servers). Based on the graphs, auxiliary visual features are mined by informative feature selection and propagation. 
Pseudo-objects are then generated by considering the spatial consistency of salient local features. A compact inverted structure is 
used over pseudo-objects for efficiency. Online part: To speed up image retrieval, we proposed an efficient query evaluation 
approach for inverted indexing. The retrieval process is then completed by relevance scoring and object-level pseudo-relevance 
feedback. It takes around 121ms to produce the final image ranking of image object retrieval over one-million image collection. 
 
Figure 3: Illustration of efficient query evaluation (cf. Section 
3.5). To achieve time efficiency, first, we rank a visual word 
by its salience to the query and then retrieve the designated 
number of candidate images (e.g., 7 images, A to G). After 
deciding the candidate images, we skip the irrelevant images 
and cut those non-salient VWs.  
1560
Figure 5(c), the middle image can then have representative VWs 
from the visual cluster it belongs to. We accumulate the number 
of each VW from the images of a cluster to form a cluster 
histogram. As shown in Figure 5(b), each image donates the same 
weight to the cluster histogram. We can then select the VWs 
whose occurrence frequency is above a predefined threshold (e.g., 
in Figure 5(b) the VWs in red rectangles are selected).  
4.3 Auxiliary Visual Word Propagation 
Due to variant capture conditions, some VWs that strongly 
characterize the query object may not appear in the query image. 
It is also difficult to obtain these VWs through query expansion 
method such as PRF because of the difference in visual 
appearance between the query image and the retrieved. Mining 
semantically relevant VWs from other information source such as 
text is therefore essential to improve the retrieval accuracy. 
As illustrated in Figure 5(e), we propose to augment each image 
with VWs propagated from the textual cluster result. This is based 
on the observation that images in the same textual cluster are 
semantically close but usually visually different. Therefore, these 
images provide a comprehensive view of the same object. 
Propagating the VWs from the textual domain can therefore 
enrich the visual descriptions of the images. As the example 
shows in Figure 5(c), the bottom image can obtain auxiliary VWs 
with the different lighting condition of the Arc de Triomphe. The 
similarity score can be weighted to decide the number of VWs to 
be propagated. Specifically, we derive the VW histogram from the 
images of each cluster and then propagate VWs based on the 
cluster histogram weighted by its (semantic) similarity to the 
canonical image of the textual cluster. 
4.4 Combining Selection and Propagation 
The selection and propagation operations described above can be 
performed iteratively. The selection operation removes visually 
irrelevant VWs and improves memory usage and efficiency, 
whereas the propagation operation obtains semantically relevant 
VWs to improve the recall rate. Though propagation may include 
too many VWs and thus decrease the precision, we can perform 
selection after propagation to mitigate this effect.  
A straightforward approach is to iterate the two operations until 
convergence. However, we find that it is enough to perform a 
selection first, a propagation next, and finally a selection because 
of the following reasons. First, only the propagation step updates 
the auxiliary visual feature and textual cluster images are fixed; 
each image will obtain distinctive VWs at the first propagation 
step. The subsequent propagation steps will only modify the 
frequency of the VWs. As the objective is to obtain distinctive 
VWs, frequency is less important here. Second, binary feature 
vectors perform better or at least comparable to the real-valued. 
The combination of selection, propagation and further OPRF 
brings 99% relative improvement over BoW model and reduces 
one-fifth of the features points. Note that the relative improvement 
of AVW (+44%) is orthogonal and complement to OPRF (+38%). 
5. DEMONSTRATION 
In the demonstration 1 , we cover five aspects of large-scale 
retrieval system: 1) image object retrieval in the one-million 
image collection2‚Äîresponding to user queries in 121ms, 2) the 
impact of object-level pseudo-relevance feedback‚Äîboosting 
retrieval accuracy, 3) image object retrieval based on effective 
auxiliary visual feature discovery‚Äîimproving the recall rate, 4) 
time efficiency with efficient query evaluation in the inverted file 
paradigm‚Äîcomparing with the traditional inverted file structure, 
and 5) a convenient tool for querying images from web directly‚Äî
fitting the reality of user queries. Note that the retrieval time 
reported excludes feature extraction and the latency in web pages. 
6. REFERENCES 
[1] J. Dean et al, ‚ÄúMapreduce: Simplified data processing on 
large clusters,‚Äù OSDI, 2004. 
[2] T. Elsayed et al, ‚ÄúPairwise document similarity in large 
collections with mapreduce,‚Äù ACL, 2008. 
[3] B. J. Frey et al, ‚ÄúClustering by passing messages between 
data points,‚Äù Science, 2007. 
[4] K.-H. Lin et al, ‚ÄúBoosting object retrieval by estimating 
pseudo-objects,‚Äù ICIP, 2009. 
[5] J. Sivic et al, ‚ÄúVideo google: a text retrieval approach to 
object matching in videos,‚Äù ICCV, 2003. 
[6] Y.-H. Yang et al, ‚ÄúContextSeer: context search and 
recommendation at query time for shared consumer photos,‚Äù 
ACM MM, 2008. 
[7] J. Zobel et al, ‚ÄúInverted files for text search engines,‚Äù ACM 
Computing Surveys, 2006. 
                                                                 
1 http://www.youtube.com/watch?v=AF9NKEBw4Ek 
2 We evaluate the proposed methods using a large-scale photo 
retrieval benchmark‚ÄîFlickr550 [6]. Besides, we randomly add 
Manhattan photos to Flickr550 to make it a 1 million dataset. 
 
(a) visual cluster example (b) representative VW selection (c) example results (d) auxiliary VW propagation (e) textual cluster example 
Figure 5: Image clustering results and mining auxiliary visual words. (a) and (e) show the sample visual and textual clusters; the 
former keeps visually similar images in the same cluster, while the latter favors semantic similarities.  The former facilitates 
representative VW selection, while the latter facilitates semantic (auxiliary) VW propagation. (b) and (d) illustrate the selection 
and propagation operations based on the cluster histogram as detailed in Section 4.2 and 4.3. And a simple example shows in (c).  
1562
knowledge, both are first proposed in this work. The experiments 
all confirm that our approach enhances the effectiveness and 
feasibility of video object segmentation for retrieval greatly. 
2. RELATED WORK 
Extracting foreground objects is considered to be a hard problem. 
The graph cut algorithm [4] has been demonstrated to be an 
effective optimization for interactive image segmentation [3][5]. 
[6] and [7] extend the idea to video sequences for interactive 
video object segmentation. Other works, like [13], address object 
motion estimation such as optical flow to find correspondence 
across frames. Somehow, too much user interaction and tedious 
motion estimation do not meet the expectation for our proposed 
system. Nowadays, motion vectors from compressed domain for 
moving object segmentation are shown to be capable of many 
real-time applications [8][9]. Since motion compensation is one of 
the most important techniques for the popular video coding 
standards, like MPEG series, motion vectors can be extended to 
the application of extracting user‚Äôs object of interest. 
3. 3D GRAPH CUT SEGMENTATION 
Here we give a short review for 3D graph cut algorithm proposed 
in [6], the base framework for our motion-augmented graph cut.  
A 3D graph G = (N, A) is built between each pair of successive 
key frames. Pre-segmentation (such as watershed) is applied to 
each frame in order to make the graph cut optimization process 
tractable in a large amount of video frames. The node set N 
contains all atomic regions generated by pre-segmentation. The 
set A contains two types of links: intra-frame links AI which 
connect the neighboring regions in the same frame and inter-
frame links AT which connect regions across adjacent frames. An 
inter-frame link is only built across adjacent frames if the two 
regions are similar in spatial location and color. The energy 
function for 3D graph cut algorithm is defined as follow: 
¬¶¬¶¬¶
¬è¬è¬è
 
TI Asr
sr
Asr
sr
Nr
r xxVxxVxUXE
),(
22
),(
11 ),(),()()( OO    
where xr (or xs) represents the object/background label on region r 
(or s). X is the set of the label ¬è {O, B} (O: object; B: 
background) to each region. 
The first term U, introduced as the data (unary) term, measures 
how the region r fits into the known object/background models. 
The second and the third term, V1 and V2, considered as the 
smoothness (pairwise) terms, describe the relation of the two 
neighboring regions and only give a penalty when the 
appearances of the two neighboring regions are alike. Each region 
r in the 3D graph construction connects to the object/background 
virtual node according to U. It connects its neighboring regions 
within the same frame with V1.  Candidate neighboring regions on 
adjacent frames are connected with V2. 
Suggested by [5], the Gaussian mixture model (GMM) is used to 
describe the color distribution for either the known object or 
background from successive sampling key frames. The data term 
is then defined as the color distance of each region to 
object/background GMMs. The smoothness term is defined with 
respect to color similarity of neighboring regions. The already 
labeled regions would be given zero to its energy value for correct 
estimation and a high penalty for wrong estimation. The 
optimization for the energy function simply follows [4]. 
4. PROPOSED METHOD 
For the original 3D graph cut algorithm, too much user 
interactions are involved to achieve better segmentation results. 
For our proposed system, the user may not be willing to do that 
since prompt responses are expected for interactions. Also, the 
system should return the acceptable retrieval result as fast as 
possible. Therefore, we must make a compromise in segmentation 
time and accuracy to achieve good retrieval results. 
4.1 Overview 
In our retrieval system, we assume that the user only gives a 
rough region on one frame to specify his/her interest object (cf. 
Figure 1).  We estimate the object region in that frame by other 
image segmentation work [5] and build the object/background 
models (using color GMMs) based on the specified frame. To 
make the segmentation process efficient, there are two ways to 
reduce the graph cut minimization time. One is to reduce the 
nodes of the graph; the other is to reduce the edges.  [6][7] apply 
pre-segmentation process beforehand and cluster similar pixels 
into a region. It reduces both nodes and edges and makes graph 
cut optimization time acceptable on dealing with a large amount 
of video frames. Though the processing time of pre-segmentation 
cannot match the purpose of our system, we still need to reduce 
the nodes to make graph cut optimization time acceptable. The 
alternative is to group pixels only considering in the similar 
locations, which is a rather simple version of traditional pre-
segmentation methods. For the number of edges, we use derived 
motion information from compressed domain in order to remove 
unnecessary edges between any neighbors who might not be 
considered into the same label region. In the following section, we 
will show the adaption for motion on 3D graph construction. 
  
(a) Temporal conherence shown by MV     (b) Graph construction with MV       (c) Fixed-neighboring graph construction 
Figure 2. The illustration of the effectiveness of MV (motion vector) in the graph construction. (a) MV presents a shift for 
prediction the location of the reference block, mostly kept the temporal coherence. (b)(c) The 3D graph construction with edge 
connection derived from MV reduces the graph complexity comparing to the prior fixed-neighboring methods. The motion-
augmented graph ensures the prompt response for the segmentation algorithm for user interactions. 
812
interactions are fixed on the same frame and the same specified 
region for each video taking on different segmentation methods. 
We compare our segmentation results with the manually defined 
ground truth mask by evaluating recall, precision and F1-
measurement. Here we define a true positive as the number of 
pixels which are correct detected as the object, a false positive as 
the number of pixels that are detected as the object but manually 
labeled as the background and a false negative as the number of 
pixels detected as the background but manually labeled as the 
object. Table 1 and Figure 3 show the processing time on each 
frame in average and the segmentation accuracy. We normalize 
the processing time by assuming all video frames as a 320x240 
resolution. 
The best time-efficiency method 3DGC-MB improves the 
processing time for 90 times comparing to 3DGC, which is based 
on traditional graph cut, though the segmentation accuracy face a 
little a drop. To achieve instant retrieval results, the best time-
efficiency method is adopted since the retrieval results shown 
similar behaviors among other segmentation methods in the 
following section. 
5.2.2 Retrieval Precision 
The expectation of the proposed system is to perform like a 
Question & Answering system in order to give relevant 
information from the highest rank retrieved images for the video 
object query. We evaluate precision at 10 (P@10) and 100 
(P@100) of retrieved images. We adopt visual words to represent 
each segmented object region among video frames. Feature points 
are obtained through by Hessian-affine detector and described by 
SIFT (quantized to 1 million dimensions) [11]. Since the video 
object might cover only a small region in the images from 
external database, we calculate similarity scores of each object 
region in a frame and pseudo object regions [10] in all the images. 
The similarity scores of all segmented object regions 
corresponding to an image are fused as the ranking score of that 
image. Finally, the system returns a list of images ranked by 
scores in descending. Here the retrieval precision of query by an 
un-segmented single frame in videos is used as the baseline to 
show the impact of query by multi-frame object regions. 
Table 2 shows that the retrieval precision of query by multi-frame 
object region has better performance in retrieval compared to 
query by an un-segmented single frame. Note that comparing to 
other segmentation methods, multi-frame object region segmented 
by 3DGC-MB has similar retrieval precision but shortest 
processing time. In the overall performance, multi-frame object 
region query improves 23.2% and 44.0% in P@10 and P@100. 
The performance of most query topics is improved both in P@5 
and P@10 except ‚ÄúPisa‚Äù query topic in P@10. The reason might 
be other buildings near ‚ÄúPisa‚Äù are considered to be good cues to 
search for ‚ÄúPisa‚Äù related images. 
6. CONCLUSION 
We propose a novel framework for users to exploit their object of 
interest during video playback. A modified 3D graph cut 
algorithm, enhanced by the efficient graph construction motivated 
by compress-domain motions, ensures the prompt responses for 
interactive segmentation. The segmented multi-frame objects 
                                                                 
1  The reason of ‚ÄúPisa‚Äù query topic has no improvement is 
described in Section 5.2.2. 
show its superiority in retrieving related information for 
exploiting further knowledge as experimented in community-
contributed videos. The motion-augmented algorithm is shown 
very efficient comparing with the prior 3D graph cut methods.  
We are now evaluating the algorithm in more diverse video 
objects of interest including people, vehicles, locations, etc. and 
evaluating in large-scale image and video collections for effective 
question and answering.   
7. REFERENCES 
[1] H. Miyamori and K. Tanaka. Webified video: media 
conversion from TV program to web content and their 
integrated viewing method. In WWW, 2005. 
[2] T. Yeh,  J. J. Lee, and T. Darrell. Photo-based question 
answering. In ACM Multimedia, 2008. 
[3] Y. Boykov and M.-P. Jolly. Interactive graph cuts for 
optimal boundary & region segmentation of objects in n-d 
images. In ICCV, 2001. 
[4] Y. Boykov and V. Kolmogorov. An experimental 
comparison of min-cut/max-flow algorithms for energy 
minimization in vision. IEEE Trans. PAMI, 26(9), Sep. 2004. 
[5] C. Rother et al. GrabCut: Interactive foreground extraction 
using iterated graph cuts. In SIGGRAPH, 2004. 
[6] Y. Li, J. Sun and H.-Y. Shum. Video object cut and paste. In 
SIGGRAPH, 2005. 
[7] J. Wang, P. Bhat, A. Colburn, M. Agrawala and M. Cohen. 
Interactive video cutout. In SIGGRAPH, 2005. 
[8] W. Zeng, J. Du, W. Gao and Q. Huang. Robust moving 
object segmentation on H.264/AVC compressed video using 
the block-based MRF model.  JRTIP, 11(4), Aug. 2005. 
[9] Z. Liu, Y. Lu and Z. Zhang. Real-time spatiotemporal 
segmentation of video objects in the H.264 compressed 
domain. JVCIR, 18(3), June 2007. 
[10] K.-H. Lin et al. Boosting object retrieval by estimating 
pseudo-objects.  In ICIP, 2009. 
[11] J. Philbin et al. Object retrieval with large vocabularies and 
fast spatial matching. In CVPR, 2007. 
[12] Y.-H. Yang et al. ContextSeer: Context search and 
recommendation at query time for shared consumer photos. 
In ACM Multimedia, 2008. 
[13] S. Khan et al. Object based segmentation of video using 
color, motion, and spatial information. In CVPR, 2001. 
 
Table 2. P@10 and P@100 for 5 query topics through 
Flickr11k image dataset.  The performance in most query 
topics (except P@10 of ‚ÄúPisa‚Äù1) is improved for multi-frame 
object region over single frame. The symbol ‚Äò%‚Äô indicates 
the relative improvement. Note that the performance of 
multi-frame object region is reported by using the most 
time-efficiency segmentation method, 3DGC-MB. 
 Un-segmented 
single frame Multiple-frame object region 
P@10 P@100 P@10 (%) P@100 (%) 
Pisa 0.72 0.30  0.71 (-1.4) 0.38 (26.7)
Triomphe 0.37 0.12  0.53 (43.2) 0.19 (58.3)
Golden 0.79 0.40  0.86 (8.9) 0.58 (45.0)
Eiffel 0.52 0.30  0.71 (36.5) 0.49 (63.3)
Colosseum 0.59 0.18  0.73 (23.7) 0.23 (27.8)
Overall 0.56 0.25  0.69 (23.2) 0.36 (44.0)
814
system (Section 3.3). In an online scenario, when a mobile user 
takes a picture of a landmark, the geolocation where he is, the 
facing direction and the image content will be kept. To get the 
textual descriptions of this photo, we adopt our real-time image 
retrieval system to get the top nearest visual neighbors as an initial 
possible image list (Section 4). Considering geolocation, compass 
direction and image content similarity, each result image will get 
its final score. We then propagate image‚Äôs score into the cluster it 
belongs, accumulating each potential POI‚Äôs possibility. Finally, 
the highest POI along with its representative tags will be returned 
back to the user. 
3. DATA PROCESSING 
For achieving real-time response to user queries, it is essential to 
build the backend reference database in preprocessing steps.  
3.1 Data Crawling and Indoor Image Removal  
For a given geographic area G, we first download geotagged 
photos from Flickr. The metadata of each image (textual tags, 
geolocation and owner) is fetched as well. Next, we need to 
remove indoor scenes. The idea of removing indoor images is 
motivated by the observation that the main subject of indoor 
photos are usually people and the meaning of them is mainly 
related to an event, rather than a geographic feature or a landmark. 
We employ a simple but effective approach for indoor/outdoor 
scene classification by training three two-class classifiers using 
Support Vector Machines with Gist descriptor, grid-based color 
moment, and Gabor texture, respectively, and combine the predic-
tions of these classifiers by taking the scene label with maximum 
prediction. The training and testing process involve a four-fold 
cross-validation for a consistent result. We achieve 83.65% 
average precision for the detector. 
3.2 Image Clustering in a Grid 
After removing the indoor images, we now turn to the task of 
automatically finding potential POIs given these large-scale 
consumer photos. We adopt the approach similar to [3]. We 
separate G into cells of the same size, assigning images to a cell 
according to their geolocation. Every four neighboring cells form 
a grid so that the grid has a high overlap (as shown in Figure 1). 
For each grid, we perform hierarchical agglomerative clustering 
algorithm based on the photos‚Äô visual features (i.e., BoW) and the 
similarity between two images is measured by cosine metric. The 
clusters with fewer images are discarded. We further merge the 
clusters if they are close enough and the degree of overlap in 
images is larger than a threshold. Each result cluster‚Äôs geolocation 
is the average of images it contains, and the mean and variance of 
geolocation are calculated by all images in this cluster. 
3.3 Representative Tags Extraction 
We then give the textual descriptions of POIs by finding the 
representative tags. Our premise is that the representative tags are 
frequent in this cluster and distinctive elsewhere. They should be 
adopted by many users as well. In this intuition, we propose a 
modified TF-IDF scheme considering photographer‚Äôs effect. The 
term frequency for a given tag x (i.e., TF(x)) in a cluster is 
counted by the number of times that x is labeled in this cluster.  In 
order to avoid the pathologies that a photographer who takes 
many photos using the same tags will unfairly bias the popularity 
of this tag, we further consider photographer‚Äôs effect that one 
photographer only contributes one count in this measurement. The 
inverse document frequency for a tag x is measured by the overall 
ratio of this tag among all clusters in the region G: IDF(x) = 
log(|G| / |{d: x d}|), where |G| is the number of result clusters in 
area G and |{d: x d}| is the number of documents (i.e., clusters) 
that contain tag x. Besides, it should be noted that a tag which is 
used by many photographers is more important. So we add 
another metric: UF(x) = |Ax|/|AG|, where |Ax| is the number of 
photographers tagging with x and |AG| is the number of 
photographers in the region G. 
Finally, the scoring function is defined as follows: 
)()()()(_ xUFxIDFxTFxtscore ‚àó‚àó=         )1(  
We rank tags in each cluster according the scoring function above 
and return the top three tags as the representative tags.  
4. IMAGE MATCHING 
In this section, we describe our real-time image matching method 
to get the initial image list. We adopt the inverted file scheme to 
achieve this goal. Inverted file is a popular way to index large-
scale data in the information retrieval community [6]. Because of 
its superiority of efficiency, many recent image retrieval systems 
adopt the concept to index visual features (i.e. VWs). 
Because image query is composed of thousands of (noisy) visual 
words and users only care about those images shown in the top-
 
Figure 1. System Diagram: In the offline part, we automatically mine important locations from online photo sharing websites using 
a grid-based clustering method and extract representative tags for further annotation process. In the online part, given a query 
image with its GPS and compass direction, we adopt our real-time retrieval system and re-rank the results considering content, 
GPS, and compass direction to get the final prediction result. 
816
crawled from Flickr. We select the top 7 famous landmark build-
ings in Taipei City for testing. For each topic, we take three to 
eight photos and record the GPS location and the compass direc-
tion we face when taking photos. The result is 39 photos in total. 
Because the final purpose of our system is to assign the correct 
topic (i.e. clusters with its representative tags) to the query image, 
we use precision at 1 (i.e., P@1) metric to evaluate the perfor-
mance, which if the cluster with the highest score corresponds to 
the right topic, we say that this query image gets correct tags. 
6.2 The Combinations of the Fusion Model 
As described in Section 5.4, we adjust the three parameters to 
generate different combinations with the three models and show 
the best result for each combination in Table 1. Ct, Cp and GPS 
are the baseline in which the score is merely from the respective 
single model. We can find that the GPS model performs better 
than other sensors and the compass model is the worst. It is 
actually probable, since our testing data focuses on famous 
landmarks and it is easy to find the related images nearby. For 
compass model, there is a case that the Œ∏ in Figure 3 may be small 
enough but the geographical distance between photographer and 
the target cluster is far way. In this situation, compass model will 
make a false positive. 
In columns 5-7 of Table 1, we consider the influence between 
each two sensors that the parameters are set from 0.1 to 0.9. The 
performance with GPS combined is still good. The best 
performance is 0.82, combining GPS and content, better than 0.69 
with GPS only and 0.59 with content only. It proves that sensors 
do help the traditional content-based retrieval system. 
In the last column of Table 1, we combine the three sensors that 
every parameter can be set from 0.1 to 0.8. Although the precision 
with the compass model only is the worst, it has about 30% 
weight in the fusion model. In Figure 4, we report the sensitive 
tests on three parameters. The curve goes down when the weight 
of content, Œ±, becomes bigger. It shows that the proportion of the 
GPS model cannot be too small. The best result shows when Œ± = 
0.1, Œ≤ = 0.6, Œ≥ = 0.3.  
We also tried the image-based and the cluster-based method on 
these combinations. The results show that the precision of the 
cluster-based is better than the image-based method on average. 
6.3 Results and Discussions 
6.3.1 GPS model and compass model 
The traditional mobile image search system uses only image 
content and user location information in usual. However, the 
compass model really helps in one specific condition. In Figure 3, 
we can see that user B stands near the landmark 2, but takes 
picture of the landmark 1. Under this condition, the GPS model 
will predict a wrong topic. However, with the help of the compass 
model, it can correct the prediction. This condition occurs when 
we want to take picture of a tall building or tower and stand far 
away from the target landmark. This is why we can see greater 
improvement when combining these three models than when only 
combining the content and GPS model. 
6.3.2 Image-based and cluster-based methods 
In the image-based method, we focus on the images returned by 
the image matching system individually. It may cause false 
negative. For example, we can take pictures of a tall building in 
any positions which are not too far away (e.g. a tower or the 
famous landmark such as Taipei101). It will cause the fact that 
while the same target is taken in the photos, the locations where 
they are taken are far from each other. The GPS score between 
them will be small. This problem can be solved by the cluster-
based method. The images with similar content will be clustered 
first and the average GPS location will be treated as the position 
of this POI. The GPS score we measure becomes the distance 
between the query image and the center of the POI. In this way, it 
can improve the precision for GPS and compass model. 
7. CONCLUSION 
We have proposed an image retrieval and automatic annotation 
system that combines with GPS, compass, and camera sensors. 
We have also investigated the related influence between those 
three factors. The results show that with the help of the GPS and 
compass model, we can obtain a relative improvement in terms of 
P@1 by 38%, in comparison with the content baseline. In 
summary, this work is the first attempt to integrate the compass 
direction sensor into the retrieval procedure. Together with the 
extraction of representative tags and inverted file indexing scheme, 
we achieve search-based image annotation with a considerably 
low response time. 
8. REFERENCES 
[1] D. Liu et al. Location sensitive indexing for image-based 
advertising. ACM Multimedia, 2009 
[2] J. Philbin et al. Object retrieval with large vocabularies and 
fast spatial matching. CVPR, 2007 
[3] T. Quack et al. World-scale mining of objects and events 
from community photo collections. CIVR,2008 
[4] J. Sivic et al. Video google: a text retrieval approach to 
object matching in videos. ICCV, 2003. 
[5] X. Yang et al. Mobile image search with multimodal context-
aware queries. IWMV, 2010 
[6] J. Zobel et al. Inverted files for text search engines. ACM 
Computing Surveys, 2006.
 
 
Figure 4. P@1 curve with Œ≥ fixed at 0.3. The combination with 
the best precision in both the image-based and cluster-based 
methods is Œ± = 0.1, Œ≤ = 0.6, Œ≥ = 0.3. 
Table 1. Comparisons under different combinations of 
cluster-based (CB) and image-based (IB) for P@1. We only 
show the best result in different parameters.  
 Ct Cp GPS Ct+ Cp 
Ct+ 
GPS 
Cp+ 
GPS 
Ct+Cp+ 
GPS 
CB 0.59 0.33 0.69 0.77 0.82 0.79 0.97 
IB                   0.51 0.15 0.74 0.69 0.74 0.82 0.95 
 
818
‚îÇ 

	 

	/ 2010.11.04 03:02 pm

√ù√æz¬∫j√æzƒÆ0ƒµS7+9ƒü√ó√ë√á√®ƒçX¬º≈Öƒæƒ¢√ß¬ª[ƒÜs^L
≈å√îƒò¬Åƒ¢ƒÆ0\#ƒñp+%w√à ƒø\DB√æzƒÆ0ƒµƒöxs√ñƒ∫w√à√âƒ¨

ƒôw√à¬ó3wƒàs√∑ƒê√Ñ¬£t¬≥¬≤ƒ¢¬±u
c?ƒ¢!¬Ü√µ¬Ωƒã¬áƒãL¬π¬®]¬ª¬ÜN(ƒ©¬ôƒ©;¬í¬πP0QRcodeƒ¢√å
¬â¬πel√©4ƒ∑¬£√¥¬é¬°¬∞¬ßƒ≤|
Iƒ¢s¬èƒ¢≈Çƒ´ƒÑ√∞√ü¬´ƒ∑¬£¬≠"¬∏{¬∂¬°√èƒ®¬æ¬ñkƒ¥ƒßsƒ≠√ç¬ì¬ô√´ƒ∂¬å¬©¬æ
√å¬âƒ¢¬ÖsNuNote ƒÆ0√òC√∏√òCP0s¬âGPS√ö:;ƒ∫ƒªƒîƒÉƒÇX¬º<ƒ¢
 
ƒù√†2s;s	d√å¬£ƒë≈Ä√é√ê9√•ƒÉ0√π¬•
),ƒ¢¬£√≤≈ãƒéƒº≈Üƒ¢?om≈ä¬¨
ƒé√å(ƒº≈ÜDƒ¢ 
¬ê¬±ƒØs
@\¬£<ƒ¢ƒüƒéHHƒé√ò¬Ñ
I¬∏<
√¶¬îTƒ§ƒ¢?oEW≈Ñ√®ƒ∏OƒÆ0√òC√å¬£≈åb0v¬ßJW√¶¬îƒä
¬∑≈åO√õƒÑ√≠V¬§≈àgTƒ§√¶¬î
Q¬Æ√¢ƒÖsw√à<√îƒò¬Åƒ¢(!"Z¬è≈ÉƒÆ0√òC

¬ú¬¶√ª7√≥2000=¬πsTaiwan FloraƒÄ√£√¢√óEƒú>rsMobile Flora &Zephyr
√òC¬æ¬ï√òR√ûO`√∂
ƒô=≈åO(!6ƒå¬ä
ƒô¬ã√°I√Çw√à¬ô<
¬ûZf8
M√¢¬¥ƒ¢√Ω√ª>n√Ñ√≠_4),ƒ∑EQ¬ô-q√™√ã.+*¬ë¬Æ-qm√±
√ß√å¬£Q¬ôREƒúsw&¬õƒà√∫ƒ∞√¨¬öƒ¢√å¬£g¬±¬üiƒà¬¢ƒà5K6≈á
ƒ¢¬£√¢"FFƒπw
w¬†Flora¬Ä(!ƒë≈Äƒ¶≈Åw&*¬ëƒ¢¬èƒ¢≈Çƒ´m≈àw~√ÉP0√§ƒâX¬ºsƒó√ø
ƒ¶≈Å$¬£w'√π¬òw&ƒÅ√ø√Üƒû:/As√Ø¬Ñƒçƒ≤ƒÇ√ô√π¬•√å(√é√ê<
w√à}ƒ™¬Ä√á¬ø"Uƒ°¬àa]¬ªsƒ•¬©√∫ƒ∞√Ä√ºƒïƒï}ƒ™c?ƒ∫ƒ±ƒ£ƒÖYƒΩƒΩ
¬ù√Æ√ôEƒú¬å¬©≈å¬Çƒë≈Äƒè≈Åw√à¬ç¬™ƒµ√£5ƒíƒá√ìG√ìƒá¬ÄH√°fw√à¬ó√ïa√Å+
≈ç√Öyh03Dƒó√ø1 ^√ú√íƒõ¬Øƒìb0v<¬É¬£ƒ†≈âw√à¬óƒ¨
2010/11/04 ƒ≥:¬µ√ä @ http://udn.com/
 	 
¬© udn.com. All Rights Reserved.
 1 
Âá∫Â∏≠ÂúãÈöõÂ≠∏Ë°ìÊúÉË≠∞ÂøÉÂæóÂ†±Âëä  
                                                             
Ë®àÁï´Á∑®Ëôü NSC 99-2221-E-002 -201 - 
Ë®àÁï´ÂêçÁ®± Êü•Ë©¢Êì¥Â±ïÊäÄË°ìÁî®ÊñºÊèêÊòáÈõúÊπäÂΩ±ÂÉèÁâ©‰ª∂ÊêúÂ∞ãÁ≥ªÁµ±	 
Âá∫Âúã‰∫∫Âì°ÂßìÂêç	 
ÊúçÂãôÊ©üÈóúÂèäËÅ∑Á®±	 
ÂæêÂÆèÊ∞ë	 
ÂúãÁ´ãËá∫ÁÅ£Â§ßÂ≠∏	 Ë≥áË®äÁ∂≤Ë∑ØËàáÂ§öÂ™íÈ´îÁ†îÁ©∂ÊâÄ	 
ÊúÉË≠∞ÊôÇÈñìÂú∞Èªû July 24 ~ 28, 2011 
ÊúÉË≠∞ÂêçÁ®± ACM SIGIR 2011, Beijing China 
ÁôºË°®Ë´ñÊñáÈ°åÁõÆ 
We thank the travel support for this project, which has helped the PI attend the 
premier conference in information retrieval, ACM SIGIR 2011. The goals for 
the visit include: 
‚Ä¢ We presented two papers (posters) in the premier conference and have 
great opportunities to interact with the worldwide researchers regarding 
our preliminary research results, which further help us polish the 
advanced researches. The two papers include: 
o Yen-Ta Huang, An-Jung Cheng, Liang-Chi Hsieh, Winston Hsu, 
Kuo-Wei Chang. Region-Based Landmark Discovery by 
Crowdsourcing Geo-Referenced Photos. In ACM SIGIR, Pages 
1141-1142, July 2011. 
o Wenyu Lee, Guan-Long Wu, Liang-Chi Hsieh, Winston Hsu. 
Multi-layer Graph-Based Semi-supervised Learning for 
Large-Scale Image Datasets using MapReduce. In ACM 
SIGIR, Pages 1121-1122, July 2011. 
‚Ä¢ We will also take this opportunity to visit Microsoft Research China 
(MSRA) to discuss collaborations in multimedia retrieval and 
recommendation. We will conduct a joint project starting in 2012. 
 
‰∏Ä„ÄÅÂèÉÂä†ÊúÉË≠∞Á∂ìÈÅé 
 
 
Please see the report in the following. 
 
 
 
‰∫å„ÄÅËàáÊúÉÂøÉÂæó 
 
 
Please see the report in the following.  
 3 
and textual information of image data during the learning process. Experimental 
results show the effectiveness of the proposed methods. 
 
2. Region-Based Landmark Discovery by Crowdsourcing Geo-Referenced Photos 
Yen-Ta Huang, An-Jung Cheng, Liang-Chi Hsieh, Winston Hsu (National Taiwan 
University), Kuo-Wei Chang (Chunghwa Telecom Co., Ltd.) 
 
We propose a novel model for landmark discovery that locates region-based 
landmarks on map in contrast to the traditional point-based landmarks. The proposed 
method preserves more information and automatically identifies candidate regions on 
map by crowdsourcing geo-referenced photos. Gaussian kernel convolution is applied 
to remove noises and generate detected region. We adopt F1 measure to evaluate 
discovered landmarks and manually check the association between tags and regions. 
The experiment results show that more than 90% of attractions in the selected city can 
be correctly located by this method. 
 
99 Âπ¥Â∫¶Â∞àÈ°åÁ†îÁ©∂Ë®àÁï´Á†îÁ©∂ÊàêÊûúÂΩôÊï¥Ë°® 
Ë®àÁï´‰∏ªÊåÅ‰∫∫ÔºöÂæêÂÆèÊ∞ë Ë®àÁï´Á∑®ËôüÔºö99-2221-E-002-201- 
Ë®àÁï´ÂêçÁ®±ÔºöÊü•Ë©¢Êì¥Â±ïÊäÄË°ìÁî®ÊñºÊèêÊòáÈõúÊπäÂΩ±ÂÉèÁâ©‰ª∂ÊêúÂ∞ãÁ≥ªÁµ± 
ÈáèÂåñ 
ÊàêÊûúÈ†ÖÁõÆ ÂØ¶ÈöõÂ∑≤ÈÅîÊàê
Êï∏ÔºàË¢´Êé•Âèó
ÊàñÂ∑≤ÁôºË°®Ôºâ
È†êÊúüÁ∏ΩÈÅîÊàê
Êï∏(Âê´ÂØ¶ÈöõÂ∑≤
ÈÅîÊàêÊï∏) 
Êú¨Ë®àÁï´ÂØ¶
ÈöõË≤¢ÁçªÁôæ
ÂàÜÊØî 
ÂñÆ‰Ωç 
ÂÇô Ë®ª Ôºà Ë≥™ Âåñ Ë™™
ÊòéÔºöÂ¶ÇÊï∏ÂÄãË®àÁï´
ÂÖ±ÂêåÊàêÊûú„ÄÅÊàêÊûú
Âàó ÁÇ∫ Ë©≤ Êúü Âàä ‰πã
Â∞Å Èù¢ ÊïÖ ‰∫ã ...
Á≠âÔºâ 
ÊúüÂàäË´ñÊñá 0 0 100%  
Á†îÁ©∂Â†±Âëä/ÊäÄË°ìÂ†±Âëä 0 0 100%  
Á†îË®éÊúÉË´ñÊñá 0 0 100% 
ÁØá 
 
Ë´ñÊñáËëó‰Ωú 
Â∞àÊõ∏ 0 0 100%   
Áî≥Ë´ã‰∏≠‰ª∂Êï∏ 0 0 100%  Â∞àÂà© Â∑≤Áç≤Âæó‰ª∂Êï∏ 0 0 100% ‰ª∂  
‰ª∂Êï∏ 0 0 100% ‰ª∂  
ÊäÄË°ìÁßªËΩâ 
Ê¨äÂà©Èáë 0 0 100% ÂçÉÂÖÉ  
Á¢©Â£´Áîü 0 0 100%  
ÂçöÂ£´Áîü 0 0 100%  
ÂçöÂ£´ÂæåÁ†îÁ©∂Âì° 0 0 100%  
ÂúãÂÖß 
ÂèÉËàáË®àÁï´‰∫∫Âäõ 
ÔºàÊú¨ÂúãÁ±çÔºâ 
Â∞à‰ªªÂä©ÁêÜ 0 0 100% 
‰∫∫Ê¨° 
 
ÊúüÂàäË´ñÊñá 1 1 100%  
Á†îÁ©∂Â†±Âëä/ÊäÄË°ìÂ†±Âëä 0 0 100%  
Á†îË®éÊúÉË´ñÊñá 4 2 100% 
ÁØá 
 
Ë´ñÊñáËëó‰Ωú 
Â∞àÊõ∏ 0 0 100% Á´†/Êú¨  
Áî≥Ë´ã‰∏≠‰ª∂Êï∏ 0 0 100%  Â∞àÂà© Â∑≤Áç≤Âæó‰ª∂Êï∏ 0 0 100% ‰ª∂  
‰ª∂Êï∏ 0 0 100% ‰ª∂  
ÊäÄË°ìÁßªËΩâ 
Ê¨äÂà©Èáë 0 0 100% ÂçÉÂÖÉ  
Á¢©Â£´Áîü 2 2 100%  
ÂçöÂ£´Áîü 2 2 100%  
ÂçöÂ£´ÂæåÁ†îÁ©∂Âì° 0 0 100%  
ÂúãÂ§ñ 
ÂèÉËàáË®àÁï´‰∫∫Âäõ 
ÔºàÂ§ñÂúãÁ±çÔºâ 
Â∞à‰ªªÂä©ÁêÜ 0 0 100% 
‰∫∫Ê¨° 
 
ÂúãÁßëÊúÉË£úÂä©Â∞àÈ°åÁ†îÁ©∂Ë®àÁï´ÊàêÊûúÂ†±ÂëäËá™Ë©ïË°® 
Ë´ãÂ∞±Á†îÁ©∂ÂÖßÂÆπËàáÂéüË®àÁï´Áõ∏Á¨¶Á®ãÂ∫¶„ÄÅÈÅîÊàêÈ†êÊúüÁõÆÊ®ôÊÉÖÊ≥Å„ÄÅÁ†îÁ©∂ÊàêÊûú‰πãÂ≠∏Ë°ìÊàñÊáâÁî®ÂÉπ
ÂÄºÔºàÁ∞°Ë¶ÅÊïòËø∞ÊàêÊûúÊâÄ‰ª£Ë°®‰πãÊÑèÁæ©„ÄÅÂÉπÂÄº„ÄÅÂΩ±ÈüøÊàñÈÄ≤‰∏ÄÊ≠•ÁôºÂ±ï‰πãÂèØËÉΩÊÄßÔºâ„ÄÅÊòØÂê¶ÈÅ©
ÂêàÂú®Â≠∏Ë°ìÊúüÂàäÁôºË°®ÊàñÁî≥Ë´ãÂ∞àÂà©„ÄÅ‰∏ªË¶ÅÁôºÁèæÊàñÂÖ∂‰ªñÊúâÈóúÂÉπÂÄºÁ≠âÔºå‰Ωú‰∏ÄÁ∂úÂêàË©ï‰º∞„ÄÇ
1. Ë´ãÂ∞±Á†îÁ©∂ÂÖßÂÆπËàáÂéüË®àÁï´Áõ∏Á¨¶Á®ãÂ∫¶„ÄÅÈÅîÊàêÈ†êÊúüÁõÆÊ®ôÊÉÖÊ≥Å‰Ωú‰∏ÄÁ∂úÂêàË©ï‰º∞ 
‚ñ†ÈÅîÊàêÁõÆÊ®ô 
‚ñ°Êú™ÈÅîÊàêÁõÆÊ®ôÔºàË´ãË™™ÊòéÔºå‰ª• 100 Â≠óÁÇ∫ÈôêÔºâ 
‚ñ°ÂØ¶È©óÂ§±Êïó 
‚ñ°Âõ†ÊïÖÂØ¶È©ó‰∏≠Êñ∑ 
‚ñ°ÂÖ∂‰ªñÂéüÂõ† 
Ë™™ÊòéÔºö 
2. Á†îÁ©∂ÊàêÊûúÂú®Â≠∏Ë°ìÊúüÂàäÁôºË°®ÊàñÁî≥Ë´ãÂ∞àÂà©Á≠âÊÉÖÂΩ¢Ôºö 
Ë´ñÊñáÔºö‚ñ†Â∑≤ÁôºË°® ‚ñ°Êú™ÁôºË°®‰πãÊñáÁ®ø ‚ñ°Êí∞ÂØ´‰∏≠ ‚ñ°ÁÑ° 
Â∞àÂà©Ôºö‚ñ°Â∑≤Áç≤Âæó ‚ñ°Áî≥Ë´ã‰∏≠ ‚ñ†ÁÑ° 
ÊäÄËΩâÔºö‚ñ°Â∑≤ÊäÄËΩâ ‚ñ†Ê¥ΩË´á‰∏≠ ‚ñ°ÁÑ° 
ÂÖ∂‰ªñÔºöÔºà‰ª• 100 Â≠óÁÇ∫ÈôêÔºâ 
3. Ë´ã‰æùÂ≠∏Ë°ìÊàêÂ∞±„ÄÅÊäÄË°ìÂâµÊñ∞„ÄÅÁ§æÊúÉÂΩ±ÈüøÁ≠âÊñπÈù¢ÔºåË©ï‰º∞Á†îÁ©∂ÊàêÊûú‰πãÂ≠∏Ë°ìÊàñÊáâÁî®ÂÉπ
ÂÄºÔºàÁ∞°Ë¶ÅÊïòËø∞ÊàêÊûúÊâÄ‰ª£Ë°®‰πãÊÑèÁæ©„ÄÅÂÉπÂÄº„ÄÅÂΩ±ÈüøÊàñÈÄ≤‰∏ÄÊ≠•ÁôºÂ±ï‰πãÂèØËÉΩÊÄßÔºâÔºà‰ª•
500 Â≠óÁÇ∫ÈôêÔºâ 
The advent of media-sharing sites like Flickr and YouTube has drastically increased 
the volume of community-contributed multimedia resources on the web. However, due 
to their magnitudes, these collections are increasingly difficult to understand, 
search and navigate. With the kind support from NSC, we investigated methods to 
improve search quality (by investigating variant optimization methods for 
effectiveness and scalability in large-scale image collections.  
 
We investigated a real-time system (and one of very few in the world) that addresses 
three essential issues of large-scale image object retrieval: (1) image object 
retrieval ‚Äì  facilitating pseudo-objects in inverted indexing and novel 
object-level pseudo-relevance feedback for retrieval accuracy Ôºõ  (2) time 
efficiency ‚Äì boosting the time efficiency and memory usage of object-level image 
retrieval by a novel inverted indexing structure (also hash-based methods) and 
efficient query evaluationÔºõ (3) recall rate improvement ‚Äì mining semantically 
relevant auxiliary visual features through visual and textual clusters in an 
unsupervised and scalable (i.e., MapReduce) manner. We are able to search over 
million-scale image collection in respond to a user query in 121ms, with 
significantly better accuracy (+99%) than the traditional models. It is also an 
