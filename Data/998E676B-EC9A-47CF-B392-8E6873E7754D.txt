1 
國科會補助專題研究計畫精簡報告 
計畫編號 NSC 100－2221－E－236－008－ 
計畫名稱 雲端運算系統中高效能的無硬碟查核點設計 
計畫主持人 邱展逢  東南科技大學助理教授 
 
一、前言： 
雲端運算服務具備以下幾條特徵：基於虛擬化技術快速部署資源或者獲得服務，
實施動態的、可伸縮的擴充功能，按需求提供資源、按使用量付費，透過網際
網路提供、面向海量資訊處理，使用者可以方便地參與，形態靈活，聚散自如，
減少使用者終端的處理負擔，降低了使用者對於電腦專業知識的依賴。這些都
是可降低企業和個人的成本，有著切實的市場需求，以提高競爭能力所必要的
發展趨勢。 
由於雲端運算系統是透過網路連接龐大的市售電腦所組成，規格和保固期很
難一致，這樣將造成設計、製造和操作上高機率的錯誤，因此零組件故障不是
例外而是常態，如果沒有加入容錯機制，雲端運算系統根本無法進行運作。尤
其客戶所輸入的資料或者計算到中間的結果的如何保證不流失，因此即時容錯
的需求是雲端運算系統不可或缺的機制。其中查核點(checkpoint)的製作[8]
是容錯最方便的方法，過去不少研究投入該項領域。查核點的製作是在平時運
轉時，系統定期或依照應用程式的需求，將回復的資料，稱為查核點
(checkpoint)，作備分在可靠的儲存裝置(stable storage)[8]，通常指硬碟，
是能夠容忍各種錯誤，包括抗雜訊、資料維持完整及不斷電的設施等等。如果
系統一旦發生錯誤，系統可以重新載入查核點，將系統還原至原來查核點儲存
的狀態，稱作返轉復原(rollback recovery)[8]。 
為了避免在硬碟上作寫入動作，一些研究學者利用電腦的主記憶體直接替代
硬碟的備份，稱為無硬碟查核點(diskless checkpointing)的方法[9]。大致
可分為兩類，鄰近方法(neighbor-based)和編碼元方法(encoding-based)。鄰
近方法是以鄰近處理器的記憶體作為製作查核點，每個處理器以鄰近的處理器
作為備份，頭尾相接，但是如果相鄰兩個處理器同時發生錯誤，系統就無法復
原。雖然鄰近方法在有些情況是可以容許多個錯誤，但是兩個以上的錯誤卻無
法完全保證復原。編碼方法是以外加的專用備份處理器，對所有工作處理器的
同步查核點，以記憶體的相對位址的資料，進行編碼，減少查核點的儲存空間，
又可分為同位元(parity)和 RS( Reed Solomon)編碼。同位元以互斥或
(exclusive-or)位元對位元的處理，如果系統中某一部處理器發生錯誤，可以
藉著其他所有存活的處理器和包括備份處理器進行互斥或處理，還原該錯誤處
理器的查核點。該方法又可分同位元查核點和二維(2D)同位元查核點。RS編
碼是將處理器的資料以矩陣方式將資訊編碼為 m 組方程式，如果處理器同時
3 
的時間會隨著容許同時錯誤的 k個數增長。如下列式所示： 
Trec 代表每個工作處理器回復原所花的時間。 
 
大多數現有的容錯設計是考量“最糟糕的情況”來處理，在某種意義上說，
在無故障運轉時是以最大容忍錯誤範圍的情況進行“高規格的備份”。以主
持人的方法[3]為例，因為單一或少數工作處理器同時發生錯誤的機率實際上
遠高於多數工作處理器同時發生錯誤的機率，系統卻為了要容許最多數工作
處理器同時的錯誤，不但在平時投入容許最多數同時發生錯誤的最多的通訊
及計算負荷；一旦出現錯誤，復原的花費卻較容許較少數目的機制還高，這
樣設計實在無法滿足雲端運算系統的處理器與日俱增，即時的需求和效能追
求。換句話說，較理想的設計應該是對高可能性的少數錯誤情況，較低負荷
的，以較多機會來處理；相反的，對低可能性的多數錯誤情況，較高負荷的，
以較少機會來處理。因此，越少數目的工作處理器同時發生錯誤，復原的時
間應該越短，使雲端運算系的平均負荷降低，以提昇整體的效益。 
 
大多數現有的容錯設計 較理想的容錯設計 
錯誤情況 可能性 
平時 
負荷 
復原 
負荷 
錯誤情況 可能性 
平時 
負荷 
復原 
負荷 
少數 高 高 高 少數 高 低 低 
多數 低 高 高 多數 低 高 高 
平均 高 平均 低 
 
這樣表示復原的資訊可能會有許多的容錯層次，系統如何將復原時間和同
時發生錯誤的數目成正比，不同個數目的處理器錯誤復原資訊的相互關係成
為重要的關鍵。 
 
二、研究目的： 
雲端運算系統主要是以服務導向，客戶要求的品質非常注重即時的反應，使得無
硬碟查核點方法研究成為重要的措施。尤其是雲端運算系統的規模將會逐漸倍增，
發生多數同時錯誤的機率將不可避免，除了必須能容許多個工作處理器同時發生
錯誤外，如何將復原時間和同時發生錯誤的數目成正比，以達到雲端運算系統的
低負荷需求。因此主持人將延續這些年無硬碟互助式查核點的研究成果，進一步
發展一套能夠為依工作處理器發生錯誤數目而調整復原時間的互助式查核點容
 
xor
ckp
net
ckp
rec
R
S
k
R
S
kT  1
5 
附件論文。 
其中Max_d 為最大間距，min_d為最小間距。 
 
 
圖(二) 
四、結果與討論： 
本研究主要解決雲端運算系統無硬碟查核點製作，要容許多個錯誤並且錯
誤少時能快速復原的難題。 
本研究所開發的雙層互助式查核點製作的優勢如下： 
1. 容錯能力增加，需要最小的行程個數為 3min__3  ddMaxn ，實際如表(一)
所示，同樣的容許最大錯誤個數 10，最小的行程個數為 121 比過去研究[3]
需要最小的行程個數為 167，少了 46個。 
表(一) 最少的行程個數 
容許最大
錯誤個數 
快速層的間距 容多層的間距 最大間距 最小間距 最少的行程
個數 
4 1 2 2 1 10 
5 2 1,3 4 2 17 
6 3 1,4,2 7 3 27 
7 6 1,3,5,2 11 6 42 
8 14 1,7,3,2,4 17 14 68 
9 11 1,3,6,8,5,2 25 11 89 
10 16 1,3,5,6,7,10,2 34 16 121 
 
7 
[7] T.-C. Chiueh and P. Deng, "Evaluation of Checkpoint Mechanisms for 
Massively Parallel Machines," Proc. IEEE Symp. Fault Tolerant 
Computing (FTCS’96), pp. 370-379, June 1996. 
[8] E. Elnozahy, L. Alvisi, Y. Wang, and D. Johnson, "A Survey of 
Rollback-Recovery Protocols in Message-Passing Systems, " ACM 
Computing Surveys, vol. 34, no. 3, pp. 375-408, Sep. 2002. 
[9] D. Hakkarinen and Z. Chen, “N-level diskless checkpointing,” High 
Performance Computing and Communications, 10th IEEE International 
Conference, vol. 0, pp. 384–391, 2009. 
[10] E. Elnozahy and J. Plank, "Checkpointing for Peta-Scale Systems: A Look 
into the Future of Practical Rollback-Recovery," IEEE Trans. 
Dependable and Secure Computing, vol. 1, no. 2, pp.97-108, Apr/June 
2004. 
[11] S. I. Feldman and C. B. Brown. "Igor: A System for Program Debugging 
via Reversible Execution," ACM SIPLAN Notices, Workshop on Parallel 
and Distributed Debugging (PADD'88), pp. 112-123, Jan. 1989. 
[12] J. Luo, L. Xu, and J. S. Plank, "An Efficient XOR-Scheduling Algorithm 
for Erasure Code Encoding," Proc. IEEE Symp. Dependable Systems and 
Networks (DSN’2009), pp. 504-513, June 2009.  
[13] A. Moody, G. Bronevetsky, K. Mohror, and B. R. de Supinski, “Design, 
modeling, and evaluation of a scalable multi-level checkpointing 
system,” Proc. ACM/IEEE Symp. Supercomputing (SC’10),, pp. 1–11,  
Nov. 2010. 
[14] Q. M. Malluhi and W. E. Johnston, "Coding for High Availability of a 
Distributed-Parallel Storage System," IEEE Trans. Parallel and 
Distributed Systems, vol. 9, no. 12, pp. 1237-1252, Dec. 1998. 
[15] J. S. Plank, Y. Kim, and J. Dongarra. "Algorithm-based Diskless 
Checkpointing for Fault Tolerant Matrix Operations," Proc. IEEE Symp. 
Fault-Tolerant Computing (FTCS’95), pp. 351-360, June 1995. 
[16] J. S. Plank, Y. Kim, and J. Dongarra, "Fault-Tolerant Matrix Operations 
for Networks of Workstations using Diskless Checkpointing," J. 
Parallel Distributed. Computing, vol. 43, no. 2, pp. 125-138, 1997. 
[17] J. S. Plank and K. Li. “Faster Checkpointing with N + 1 Parity,” Proc. 
IEEE Symp. Fault-Tolerant Computing (FTCS’94), pp. 288-297, June 
1994. 
Double Mutual-Aid Checkpointing for Fast Recovery 
Jane-Ferng Chiu 
Department of Information Technology and Communication, 
Tungnan University, 
 New Taipei 22202, Taiwan, R.O.C. 
E-mail: jfchiu@mail.tnu.edu.tw 
 
 
Abstract—Because of the enlarging system size and the 
increasing number of processors, the probability of errors and 
multiple simultaneously failures become the norm rather than 
the exception. Therefore, to tolerate multiple failures is 
indispensable. Normally, most diskless checkpointing need the 
maximum recovery overhead no mater how many failures 
happen at the same time. However, a small number of 
processors' failures happen more frequently than the worse 
case. This study resolves the dilemma between more fault 
tolerance and fast recovery by presenting a novel diskless 
checkpointing which makes use of double mutual-aid 
checkpoints. It not only gives the necessary and sufficient 
condition but also proposes a method for determination the 
setting of double mutual-aid checkpoints. 
Keywords-diskless checkpointing, tolerate multiple failures, 
fast recovery 
I.  INTRODUCTION 
It is fundamental that cloud computing systems can 
provide high performance and dependable services. Thus, the 
cloud platforms composed from a pool of computer 
resources typically deploying into clusters of a massive 
number of servers hosted in data centers. As high 
performance clusters continue to grow in size and the mean 
time between failures shrinks. With the increasing 
probability of fault occurrences for a large scale cloud 
computing, to tolerate multiple failures is essential. 
Checkpoint/restart [8] is the ability to save the state of a 
running application. Because of that it can later resume its 
execution from the time when it was checkpointed on the 
same or a different machine. However, taking checkpoint 
into disks needs unacceptable time for large number nodes.  
Diskless checkpoint [18] saves its checkpoint to the 
memory for recovery since memory accessing is much faster 
than disk accessing. The potentially low checkpoint 
overhead and faster restart should allow us to achieve better 
performance than traditional disk-based checkpoint schemes. 
However, the encoding time, the dedicated resources (the 
spares) and the memory overhead, which imposed by 
diskless checkpoint, are significant obstacles against its 
adoption. Using the results from [1], the overhead and the 
recovery time for a diskless checkpoint scheme increase 
linearly with the number of failures the scheme is capable of 
handling. 
There are many ways [5],[8] to reduce checkpoint 
overhead. For example, a processor takes a checkpoint by 
selecting idle time or by minimizing checkpoint size. 
However, failure time is random. In [13] experience, most 
failures only disable one or two nodes at a time, and multi-
node failures often disable nodes in a predictable pattern. 
[13] shows the probability of multiple failures on Tsubame, 
as we can see 95% of them are single failures. 
However, both tolerating multiple faults and fast 
recovery are two important features for availability. These 
work address one of the basic questions in diskless 
checkpointing studies: How can systems cope effectively for 
tolerating number failures and recover rapidly while a single 
or double node failure? 
The rest of the paper is organized as follows. Section II 
discusses some background and related work. The basics of 
our scheme are presented in Section III and details of its 
implementation in Section IV. Overhead analysis is provided 
in Section V. Finally, Section VI summarizes the 
contribution of our approach and thoughts for future work. 
II. RELATED WORK 
Checkpointing using stable storage has been studied 
extensively in the literature [8]. Since stable storage 
accessing is a considerable overhead, the number of 
checkpoints for taking is restricted.  
Diskless checkpointing approach was first proposed in 
[17] to avoid the excessive overhead associated with stable 
storage operation. The various diskless checkpointing can be 
broadly classified into three categories: neighbor-based [20], 
parity-based [2],[14],[16], and Reed-Solomon code-
based[18]. In the neighbor-based approach, each processor 
saves its checkpoints in the memory of another processor. 
However, a failed processor cannot recover its state if its 
partner or neighbor storing its checkpoint fails at the same 
time. The parity-based diskless checkpointing requires that 
all application processors coordinate to take checkpoints, 
with parity data of the checkpoints being saved in the main 
memory of a dedicated parity processor. Hence, the amount 
of diskless checkpoint data that is stored is small in the 
parity-based technique. However, the time overhead depends 
on the number of application processors. The Reed-Solomon 
code-based diskless checkpointing works by encoding the 
checkpoints of n application processors to generate an extra 
set of k distinct checksum data. These k pieces of encoded 
information are stored at k dedicated checkpoint processors. 
When some application processors fail, the system is able to 
decode the original checkpoints for each of the failed 
application processors as long as the total number of failed 
processors is no more than k.  
2012 IEEE 14th International Conference on High Performance Computing and Communications
978-0-7695-4749-7/12 $26.00 © 2012 IEEE
DOI 10.1109/HPCC.2012.148
1015
Proof. We prove the theorem by contradiction. 
Assume zxx PCCCC =∩
10 . If xP  and zP fail, the system can 
not tolerate k  faults since there only 2−k  copies of zP  in 
other nodes but xP . Thus contradiction arises.         Ű 
 
 
Figure 2.  5
1
0
0
0 PCCCC =∩  
For example, as Fig. 2, a system has 17 processors and 
installs double mutual-aid checkpoints. 
Since 5
1
0
0
0 PCCCC =∩ , 5P could not recover when 
131050 ,,, PPPP and 15P fail. Therefore, the tolerate failure 
number is not five. When 131050 ,,, PPPP and 15P fail, there are 
no recovery information for 5P . As result, the allocation 
could not tolerate five number failures.  
 
 
Figure 3.  { }7511000 , PPCCCC =  
 
Theorem 2. If the direct recovery criterion is met, then 
for all processor xP  and yP , 1≤jyix CCCC   for yx ≠  and 
ji ≠ .  
Proof. We prove the theorem by contradiction. Assume 
},{ ba
j
y
i
x PPCCCC = , other 2−k copies of aP in fP ; 
i
fa CCP ∈  and af ≠ or bf ≠ , if all fP , aP and bP  fail, aP  
can not be decoded from ixCC  and
j
yCC .         Ű 
For example, in Fig. 3, { }7511000 , PPCCCC = . When 
141375 ,,, PPPP and 15P fail, 5P can not be decoded from 0P or 
10P although both 0P and 10P aliveʿ since both 5P and 7P are 
not exit.  
Theorem 3. If the direct recovery criterion is met, then 
xP  and yP , φ=jyix CCCC  ; for yx ≠ , ixy CCP ∈  and 
10 ≤≤ i ; 10 ≤≤ j  
Proof. We prove the theorem by contradiction. 
Assume z
j
y
i
x PCCCC = . Therefore, since ixy CCP ∈ , we 
have ixzy CCPP ⊂},{ . When all fP where 
i
fz CCP ∈ and 
xf ≠  fail, if zP  also fail, zP  can not be decoded 
from ixCC since both zP and yP are not alive.          Ű 
 
For example, as Fig. 4, 5
1
7
0
0 PCCCC = ; 007 CCP ∈ . 
When 141375 ,,, PPPP and 15P fail, 5P can not be decoded 
although 0P alive since both 5P and 7P are not exit.  
 
 
Figure 4.  5
1
7
0
0 PCCCC = ; 007 CCP ∈  
D. Sufficient condition 
Theorem 4. If the conditions specified in Theorems 1, 2 and 
3 are all satisfied, then the direct recovery criterion is met. 
Proof.  Three cases are considered here. 
Case 1: There exists no processor xP such that
i
xf CCP ∈ ; 
10 ≤≤ i  such that FPx ∉ . 
We prove the theorem by contradiction. Since kF = , there 
are k-1 failure processors without fP , it means 
1−= kCS f there are two fxx PCCCC =∩
10  and xP ∈F. 
Hence, to satisfy the direct recovery criterion for all 
processor xP , φ=∩ 10 xx CCCC  is true. 
1017
spacing between two consecutive checkpoint coverage nodes 
of a processor. 00CC  is constructed as follows. The first 
checkpoint coverage node in 00CC  is 5P , whose subscript is 
one more than the value of dMax _ . The other checkpoint 
coverage nodes in 00CC  is obtained by 0d . That is, the other 
checkpoint coverage node in 00CC is 7P .The first checkpoint 
coverage nodes in 10CC  is one more than the value of the 
maximum of two checkpoint coverage node in 00CC is 8P . 
The second is obtained by 1d , and 2d  as respective 
increments. That is, the second checkpoint coverage node in 
1
0CC is 9P , and the last one is 12P . Eventually, we have 
},{ 75
0
0 PPCC =  and },,{ 1298
1
0 PPPCC = , whose elements 
follow the spacing requirement stated above. The proposed 
method requires that the total number of processors in the 
system is no less than 273__3 =++ dMindMax . The 
design is illustrates in Fig. 5, which meets the safe recovery 
criterion. Formally, this design is based on the following 
theorem. Here, we only consider 4≥k . 
 
Theorem 5. Suppose that 0d , 1d , 2d , 3−kd  is a PSR 
sequence of positive integers. Let ¦−
=
=
3
1
1_
k
i
ix dCCd ; 
( )10 _,_ xCCddMaxdMax = ; ( )10 _,min_ xCCdddmid = . If 
3min__3 ++≥ ddMaxn , the direct recovery criterion is 
met if };1_,{ 0
0
0 duvdMaxuPPCC vu +=+==  and 
};1;30|{
3
1
00
1
0 ¦+=+=−≤≤=
−
=
k
i
iim dmmvmkiPCC i  
 
 
 
Figure 6.  The design of Theorem 5 
Proof.  Fig. 6 shows the design of Theorem 5.  Because PSR 
sequence, 1_ +dMax and 3min__3 ++≥ ddMaxn , this 
method is  to meet the direct recovery criterion. It is not hard 
to show.              Ű 
 
For pragmatic purpose, we list the minimum n, along 
with one possible sequence of id  ’s, for k = 4 to 10 in 
TABLE I.  
TABLE I.  MINIMUN N AND ONE POSSIBLE PSR SEQUENCE 
k 0_ xCCd
1_ xCCd  dMax _  dMin _ NMin _
4 1 2 2 1 10
5 2 1,3 4 2 17
6 3 1,4,2 7 3 27
7 6 1,3,5,2 11 6 42
8 14 1,7,3,2,4 17 14 68
9 11 1,3,6,8,5,2 25 11 89
10 16 1,3,5,6,7,10,2 34 16 121
V. OVERHEAD ANALYSIS  
Double mutual-aid checkpointing can drastically increase 
fast recovery and overall reliability with little extra 
investment in memory. There are two major types of 
overhead for checkpointing and recovery operations: time 
and memory space. The following analysis assumes that the 
size of each checkpoint is ckpS words. Consider the operation 
of generating parity checkpoint data when a new checkpoint 
is taken by each of the n processors. In our scheme, each 
processor must send a local checkpoint to k checkpoint 
storage nodes, and receives k checkpoints from iP ’s 
respective coverage nodes. The entire process takes k steps. 
Again, iP  receives a checkpoint from a second checkpoint 
coverage node. iP  will then XOR this checkpoint into the 
one received in the first step.   iP  only keeps the resulting 
parity checkpoint data. Each iP performs a similar 
transmission-receiving-XOR operation for the remaining   
steps. Let ckpT denote the time required to complete the 
generation of the parity data for the entire system. Assuming 
that computation and transmission do not overlap at a 
processor, ckpT is given by 
xor
ckp
net
ckp
ckp R
S
k
R
S
kT )3( −+=  
where netR   represents the bandwidth of the network and   
xorR represents the rate of performing XOR operation. Note 
that each iP can perform the transmission of a checkpoint and 
XOR operation in parallel. 
Next, consider the failure recovery operation. Suppose 
that k failures occur simultaneously in the system. Since the 
proposed scheme satisfies the direct recovery criterion, each 
of the k failed processors can be recovered by a distinct 
surviving checkpoint storage node. Let  fP  be a failed 
processor and assume that  fr CSP ∈  is to help fP   recover 
its previous checkpoint. rP will ask all the processors, except  
fP , in 
0
rCC   to send their previous checkpoints, one by one. 
Upon receiving any of these checkpoints, rP  then XOR’s the 
checkpoint into the parity checkpoint data kept by itself. This 
operation can be completed in k-1 steps. rP   then sends the 
1019
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                       日期：101 年 7月 25日 
                                 
一、參加會議經過 
第 14屆高效能計算與通訊國際會議(2012HPCC)為每年舉辦一次的國際學術
研討會，今年為第十四屆於 2012年 7月 25日至 7月 27日在英國的利物浦的
Jurys Inn舉行，這是由 IEEE、IEEE Computer Society 和 IEEE TCSC 所贊
助，布拉福德大學 (University of Bradford) 所主辦。 
和本次會議同時同一地點舉行有 TrustCom-2012(The 11th IEEE 
International Conference on Trust, Security and Privacy in Computing 
and Communications)、IUCC-2012(The 11th IEEE International 
Conference on Ubiquitous Computing and Communications)、ICESS-
2012(The 9th IEEE International Conference on Embedded Software and 
Systems)等會議邀請了六位重量級的學者與會演講，內容提供先進的高效能計
算與通訊的新觀念，作為今後努力嘗試的方向。 
二、與會心得 
首先感謝國科會計畫對於這次國際會議研究計畫的經費補助和支持，除了
吸收世界各地優秀學者所提供高效能計算與通訊的研究資訊之外，許多專家學
者都以模擬或實際量測作為效能評估，本人提出的演算法，頗感興趣討論，對
於這種直接面對面交流與觀摩的機會，與會者提出的最新成果和交流思想對提
計畫編號 NSC 100－2221－E－236－008－ 
計畫名稱 雲端運算系統中高效能的無硬碟查核點設計 
出國人員姓名 邱展逢 
服務機構及職
稱 
東南科技大學助理教授 
會議時間 
2012年 6月
25日至 2012
年 6月 27日 
會議地點 
英國利物浦 
會議名稱 
(中文)第 14屆高效能計算與通訊國際會議 
(英文)2012 IEEE 14th International Conference on High 
Performance Computing and Communications 
發表論文題目 
(中文)快速復原的雙互助式查核點製作 
(英文) Double Mutual-Aid Checkpointing for Fast 
Recovery 
附件四 
論文被接受發表之大會證明文件 
 
寄件人: hpcc_2012@googlegroups.com 
日期: 2012/04/24 06:18 
收件人: jfchiu@mail.tnu.edu.tw;  
副本: hpcc_2012@googlegroups.com;  
主題: HPCC-2012 Status of paper 150 
 
 
Dear Jane-Ferng Chiu:  
 
First of all, thank you very much for submitting your paper to the 14th IEEE International Conference 
on High Performance Computing and Communications (HPCC-2012) to be held in Liverpool, UK, 25-
27 June 2012.  
 
HPCC-2012 has received a large number of submissions. All papers were carefully peer-reviewed and 
ranked according to their original contribution, quality, presentation and relevance to the themes of the 
conference. Due to the limited space, less than 30% excellent papers have been selected for the main 
conference.  
 
However, based on the reviewers' reports, your paper is considered as a good-quality paper. We are 
pleased to inform you that your paper  
 
Paper ID: AHPCN-150  
Title: Double Mutual-aid Checkpointing for Fast Recoery  
Authors: Jane-Ferng Chiu  
 
has been invited (only a very limited set of good quality papers from the submissions) for the Fifth 
International Symposium on Advances in High Performance Computing and Networking (AHPCN-
2012), which will be held in conjunction with HPCC-2012. The papers accepted for presentation at the 
Symposium AHPCN-2012 will be included in the HPCC-2012 main conference proceedings to be 
published by IEEE CS press.  
 
The URL of AHPCN-2012 Symposium is  
http://www.scim.brad.ac.uk/~hmibrahi/HPCC2012/AHPCN.htm  
 
For the convenience of registration and camera-ready paper submission, please note that your paper ID 
has been changed to AHPCN-150 from HPCC-150. Please quote this new paper number for the future 
communication.  
 
Please send an email to hpcc_2012@googlegroups.com as soon as possible (no later than 25 April 2012) 
if you do not want to have your paper included in the Symposium because we need submit the list of 
accepted papers to IEEE CS by the end of 25 April 2012.  
 
In order to achieve the highest quality proceedings, please consider carefully the reviewers' comments if 
any (attached at the end of this email) when preparing the final version of your contribution.  
Double Mutual-Aid Checkpointing for Fast Recovery 
Jane-Ferng Chiu 
Department of Information Technology and Communication, 
Tungnan University, 
 New Taipei 22202, Taiwan, R.O.C. 
E-mail: jfchiu@mail.tnu.edu.tw 
 
 
Abstract—Because of the enlarging system size and the 
increasing number of processors, the probability of errors and 
multiple simultaneously failures become the norm rather than 
the exception. Therefore, to tolerate multiple failures is 
indispensable. Normally, most diskless checkpointing need the 
maximum recovery overhead no mater how many failures 
happen at the same time. However, a small number of 
processors' failures happen more frequently than the worse 
case. This study resolves the dilemma between more fault 
tolerance and fast recovery by presenting a novel diskless 
checkpointing which makes use of double mutual-aid 
checkpoints. It not only gives the necessary and sufficient 
condition but also proposes a method for determination the 
setting of double mutual-aid checkpoints. 
Keywords-diskless checkpointing, tolerate multiple failures, 
fast recovery 
I.  INTRODUCTION 
It is fundamental that cloud computing systems can 
provide high performance and dependable services. Thus, the 
cloud platforms composed from a pool of computer 
resources typically deploying into clusters of a massive 
number of servers hosted in data centers. As high 
performance clusters continue to grow in size and the mean 
time between failures shrinks. With the increasing 
probability of fault occurrences for a large scale cloud 
computing, to tolerate multiple failures is essential. 
Checkpoint/restart [8] is the ability to save the state of a 
running application. Because of that it can later resume its 
execution from the time when it was checkpointed on the 
same or a different machine. However, taking checkpoint 
into disks needs unacceptable time for large number nodes.  
Diskless checkpoint [18] saves its checkpoint to the 
memory for recovery since memory accessing is much faster 
than disk accessing. The potentially low checkpoint 
overhead and faster restart should allow us to achieve better 
performance than traditional disk-based checkpoint schemes. 
However, the encoding time, the dedicated resources (the 
spares) and the memory overhead, which imposed by 
diskless checkpoint, are significant obstacles against its 
adoption. Using the results from [1], the overhead and the 
recovery time for a diskless checkpoint scheme increase 
linearly with the number of failures the scheme is capable of 
handling. 
There are many ways [5],[8] to reduce checkpoint 
overhead. For example, a processor takes a checkpoint by 
selecting idle time or by minimizing checkpoint size. 
However, failure time is random. In [13] experience, most 
failures only disable one or two nodes at a time, and multi-
node failures often disable nodes in a predictable pattern. [13] 
shows the probability of multiple failures on Tsubame, as we 
can see 95% of them are single failures. 
However, both tolerating multiple faults and fast 
recovery are two important features for availability. These 
work address one of the basic questions in diskless 
checkpointing studies: How can systems cope effectively for 
tolerating number failures and recover rapidly while a single 
or double node failure? 
The rest of the paper is organized as follows. Section II 
discusses some background and related work. The basics of 
our scheme are presented in Section III and details of its 
implementation in Section IV. Overhead analysis is provided 
in Section V. Finally, Section VI summarizes the 
contribution of our approach and thoughts for future work. 
II. RELATED WORK 
Checkpointing using stable storage has been studied 
extensively in the literature [8]. Since stable storage 
accessing is a considerable overhead, the number of 
checkpoints for taking is restricted.  
Diskless checkpointing approach was first proposed in 
[17] to avoid the excessive overhead associated with stable 
storage operation. The various diskless checkpointing can be 
broadly classified into three categories: neighbor-based [20], 
parity-based [2],[14],[16], and Reed-Solomon code-
based[18]. In the neighbor-based approach, each processor 
saves its checkpoints in the memory of another processor. 
However, a failed processor cannot recover its state if its 
partner or neighbor storing its checkpoint fails at the same 
time. The parity-based diskless checkpointing requires that 
all application processors coordinate to take checkpoints, 
with parity data of the checkpoints being saved in the main 
memory of a dedicated parity processor. Hence, the amount 
of diskless checkpoint data that is stored is small in the 
parity-based technique. However, the time overhead depends 
on the number of application processors. The Reed-Solomon 
code-based diskless checkpointing works by encoding the 
checkpoints of n application processors to generate an extra 
set of k distinct checksum data. These k pieces of encoded 
information are stored at k dedicated checkpoint processors. 
When some application processors fail, the system is able to 
decode the original checkpoints for each of the failed 
application processors as long as the total number of failed 
processors is no more than k.  
Proof. We prove the theorem by contradiction. 
Assume zxx PCCCC 
10 . If xP  and zP fail, the system can 
not tolerate k  faults since there only 2k  copies of zP  in 
other nodes but xP . Thus contradiction arises.         █ 
 
 
Figure 2.  5
1
0
0
0 PCCCC   
For example, as Fig. 2, a system has 17 processors and 
installs double mutual-aid checkpoints. 
Since 5
1
0
0
0 PCCCC  , 5P could not recover when 
131050 ,,, PPPP and 15P fail. Therefore, the tolerate failure 
number is not five. When 131050 ,,, PPPP and 15P fail, there are 
no recovery information for 5P . As result, the allocation 
could not tolerate five number failures.  
 
 
Figure 3.   75
1
10
0
0 ,PPCCCC   
 
Theorem 2. If the direct recovery criterion is met, then 
for all processor xP  and yP , 1
j
y
i
x CCCC   for yx   and 
ji  .  
Proof. We prove the theorem by contradiction. Assume 
},{ ba
j
y
i
x PPCCCC  , other 2k copies of aP in fP ; 
i
fa
CCP   and af  or bf  , if all
f
P , aP and bP  fail, aP  
can not be decoded from ixCC  and
j
yCC .         █ 
For example, in Fig. 3,  75
1
10
0
0 ,PPCCCC  . When 
141375 ,,, PPPP and 15P fail, 5P can not be decoded from 0P or 
10P although both 0P and 10P alive, since both 5P and 7P are 
not exit.  
Theorem 3. If the direct recovery criterion is met, then 
xP  and yP , 
j
y
i
x CCCC  ; for yx  , 
i
xy CCP   and 
10  i ; 10  j  
Proof. We prove the theorem by contradiction. 
Assume z
j
y
i
x PCCCC  . Therefore, since
i
xy CCP  , we 
have ixzy CCPP },{ . When all fP where 
i
fz CCP  and 
xf   fail, if zP  also fail, zP  can not be decoded 
from ixCC since both zP and yP are not alive.          █ 
 
For example, as Fig. 4, 5
1
7
0
0 PCCCC  ; 
0
07 CCP  . 
When 141375 ,,, PPPP and 15P fail, 5P can not be decoded 
although 0P alive since both 5P and 7P are not exit.  
 
 
Figure 4.  5
1
7
0
0 PCCCC  ; 
0
07 CCP   
D. Sufficient condition 
Theorem 4. If the conditions specified in Theorems 1, 2 and 
3 are all satisfied, then the direct recovery criterion is met. 
Proof.  Three cases are considered here. 
Case 1: There exists no processor xP such that
i
xf CCP  ; 
10  i  such that FPx  . 
We prove the theorem by contradiction. Since kF  , there 
are k-1 failure processors without fP , it means 
1 kCS f there are two fxx PCCCC 
10
 and xP F. 
Hence, to satisfy the direct recovery criterion for all 
processor xP , 
10
xx CCCC  is true. 
spacing between two consecutive checkpoint coverage nodes 
of a processor. 00CC  is constructed as follows. The first 
checkpoint coverage node in 00CC  is 5P , whose subscript is 
one more than the value of dMax _ . The other checkpoint 
coverage nodes in 00CC  is obtained by 0d . That is, the other 
checkpoint coverage node in 00CC is 7P .The first checkpoint 
coverage nodes in 10CC  is one more than the value of the 
maximum of two checkpoint coverage node in 00CC is 8P . 
The second is obtained by 1d , and 2d  as respective 
increments. That is, the second checkpoint coverage node in 
1
0CC is 9P , and the last one is 12P . Eventually, we have 
},{ 75
0
0 PPCC   and },,{ 1298
1
0 PPPCC  , whose elements 
follow the spacing requirement stated above. The proposed 
method requires that the total number of processors in the 
system is no less than 273__3  dMindMax . The 
design is illustrates in Fig. 5, which meets the safe recovery 
criterion. Formally, this design is based on the following 
theorem. Here, we only consider 4k . 
 
Theorem 5. Suppose that 0d , 1d , 2d , 3kd  is a PSR 
sequence of positive integers. Let 



3
1
1_
k
i
ix dCCd ; 
 10 _,_ xCCddMaxdMax  ;  10 _,min_ xCCdddmid  . If 
3min__3  ddMaxn , the direct recovery criterion is 
met if };1_,{ 0
0
0 duvdMaxuPPCC vu   and 
};1;30|{
3
1
00
1
0 


k
i
iim dmmvmkiPCC i  
 
 
 
Figure 6.  The design of Theorem 5 
Proof.  Fig. 6 shows the design of Theorem 5.  Because PSR 
sequence, 1_ dMax and 3min__3  ddMaxn , this 
method is  to meet the direct recovery criterion. It is not hard 
to show.              █ 
 
For pragmatic purpose, we list the minimum n, along 
with one possible sequence of id  ’s, for k = 4 to 10 in 
TABLE I.  
TABLE I.  MINIMUN N AND ONE POSSIBLE PSR SEQUENCE 
k 0_ xCCd  
1_ xCCd  dMax _
 dMin _  NMin _  
4 1 2 2 1 10 
5 2 1,3 4 2 17 
6 3 1,4,2 7 3 27 
7 6 1,3,5,2 11 6 42 
8 14 1,7,3,2,4 17 14 68 
9 11 1,3,6,8,5,2 25 11 89 
10 16 1,3,5,6,7,10,2 34 16 121 
V. OVERHEAD ANALYSIS  
Double mutual-aid checkpointing can drastically increase 
fast recovery and overall reliability with little extra 
investment in memory. There are two major types of 
overhead for checkpointing and recovery operations: time 
and memory space. The following analysis assumes that the 
size of each checkpoint is ckpS words. Consider the operation 
of generating parity checkpoint data when a new checkpoint 
is taken by each of the n processors. In our scheme, each 
processor must send a local checkpoint to k checkpoint 
storage nodes, and receives k checkpoints from iP ’s 
respective coverage nodes. The entire process takes k steps. 
Again, iP  receives a checkpoint from a second checkpoint 
coverage node. iP  will then XOR this checkpoint into the 
one received in the first step.   iP  only keeps the resulting 
parity checkpoint data. Each iP performs a similar 
transmission-receiving-XOR operation for the remaining   
steps. Let ckpT denote the time required to complete the 
generation of the parity data for the entire system. Assuming 
that computation and transmission do not overlap at a 
processor, ckpT is given by 
xor
ckp
net
ckp
ckp
R
S
k
R
S
kT )3(   
where netR   represents the bandwidth of the network and   
xorR represents the rate of performing XOR operation. Note 
that each iP can perform the transmission of a checkpoint and 
XOR operation in parallel. 
Next, consider the failure recovery operation. Suppose 
that k failures occur simultaneously in the system. Since the 
proposed scheme satisfies the direct recovery criterion, each 
of the k failed processors can be recovered by a distinct 
surviving checkpoint storage node. Let  fP  be a failed 
processor and assume that  fr CSP   is to help fP   recover 
its previous checkpoint. rP will ask all the processors, except  
fP , in 
0
rCC   to send their previous checkpoints, one by one. 
Upon receiving any of these checkpoints, rP  then XOR’s the 
checkpoint into the parity checkpoint data kept by itself. This 
operation can be completed in k-1 steps. rP   then sends the 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/07/23
國科會補助計畫
計畫名稱: 雲端運算系統中高效能的無硬碟查核點設計
計畫主持人: 邱展逢
計畫編號: 100-2221-E-236-008- 學門領域: 平行與分散處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
由於雲端運算系統逐漸龐大和處理器數量增多，發生錯誤的機率不再是意外而
且同時發生多個錯誤變成常態。因此，容許同時發生多個錯誤是不可或缺。通
常，大部分無硬碟查核點製作不論多少錯誤數量同時發生總需要最大復原負
荷。然而，發生少數錯誤的頻率遠超過最壞的狀況。 
本計畫成果以雙互助式查核點製作為新的無硬碟查核點製作解決容許許多錯誤
和快速復原的難題。這研究不但提供雙互助式查核點製作的充分及必要條件，
提供未來研究的基礎，並且發展的演算法決定如何安排雙互助式查核點製作以
達到直接復原的應用目的。研究成果完成撰寫「快速復原的雙互助式查核點製
作」Double Mutual-Aid Checkpointing for Fast Recovery 論文乙篇，依照
核定計畫投稿國際會議，成功的被接受，並刊登於 IEEE 2012 HPPC 第 105 頁
至第 110 頁，如附件。 
本計畫成果中雙互助式查核點製作的充分及必要條件，是未來多層互助式查核
點製作的發展重要依據，也是彈性調整復原速度和容錯的不可或缺的參考。成
果中推導的演算法，實際可以直接運用於雲端運算系統的各種平行或分散式的
高效能運算，或是多行程的即時性服務，降低復原的時間，提昇服務的品質。
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
