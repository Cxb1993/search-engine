 一、研究計畫內容： 
（一）前言 
網際網路在過去數年間已經快速成長為一個重要的商業平台，許許多多的公司都以提供網路服
務為主要的目標，原因即為網際網路平台的擴充性與多樣化，使得各式各樣的服務都能夠透過網際
網路來提供，如錯誤! 找不到參照來源。所示。然而，隨著服務的多樣化，各式各樣特別的需求也
跟著被提出，包括具有品質保證的網路服務與網路安全。上述兩類的需求都與此計畫要研究的主題
“封包分類＂有著極為深切的關連，本計劃即針對此一重要議題進行深入研究。 
（二）研究目的 
封包分類問題定義 
我們先定義封包分類演算法所要解決的問題，在一個規則資料庫中，包含了許多的規則與過濾
條件，每個規則(與過濾條件)都是由封包檔頭中的數個欄位中的值所組成，每個欄位的值可以是變
動長度的位址字首字串(Variable-length Prefix)、一個任意區間(Range)或是一個數值(Explicit 
Value)，對於一個d個欄位的規則Rj，我們可以定義成Rj=(Rj [1], Rj [2],…, Rj [d])。通常在封
包分類中，會使用到的欄位包括IP 起始位址、目標位址、協定類別、發起端與接收端的傳送層埠
號等。圖一舉例出有哪些封包的欄位可以當作分類的條件，值得注意的是這些欄位不限於網路層的
欄位(如起始位址與目標位址)， 資料連結層與傳送層的欄位也可包含在內。我們稱一個封包符合
一個規則的條件是每一個封包的d 個欄位值與該規則的d 個欄位定義值均相符。對於任一封包而
言，當然有可能同時符合數個政策，在傳統的封包分類中，每一個規則都被設定權重(如圖二所示)，
當封包符合數個規則時，最高權重規則的指定動作會被執行(如圖三所示)。但是針對防火牆所設計
之封包分類，則所有的符合規則都將被取出，因此無法使用權重簡化搜尋的步驟。此外，IP位址搜
尋（IP Address Lookup）在理論上可視為封包分類問題的簡化版本[18][19][20][21]，這個問題
是有關於路由器如何快速的針對IP 封包中目的位址來決定傳送的路由器埠，而決定的原則就是最
大符合的位址遮罩。對封包分類演算法而言，可視為將規則縮減成只有一個欄位(即變動長度的位
址字首字串)，因此許多在IP 轉接的技巧亦可用來改進封包分類演算法的效能。利用將多欄位搜尋
的問題分解成多個單一欄位的搜尋，進而簡化搜尋的複雜度[2][6][9][11][14][28]。 
 
圖一、封包分類欄位 
 
 1 
 
  3 
 
對封包分類，在[29]中，應用TCAM來處理多重配對封包分類的方法已被提出，然而其所需的TCAM
容量經常是規則數目的數十倍之譜。 
另一類型的硬體解決方案則是使用一個位元來代表一個規則，所有的規則進一步組成一個位元
向量(Bit Vector, BV)，每個不同欄位值內都放入一個BV，搜尋時只需找到相關的欄位值，並將相
關的BV取出後交集即可得到所有符合的規則。此方法的問題在於當規則數目很多時，位元向量將佔
很大的空間，並需要花很長的時間來存取。在[4]中，針對存取時間改善的方案已被提出，然而其
空間的問題並未解決。在[28]中，我們亦提出了針對時間與空間同時改良的方案。 
樹狀資料結構解決方案：在[9]中提出了利用Grid-of-tries的資料結構來處理封包分類，然而
此演算法只能處理二維規則。因此在[14]中另提出了將Grid-of-tries的資料結構延伸到多維規則
的演算法，但是搜尋效能較不理想。另一類針對二維的資料結構有有Area-Based-Quadtree [15]與
Fat Inverted Segment Tree [11]。有數篇論文針對只有兩維的情況作演算法設計，細節可參考[22]。 
決策樹解決方案：與決策樹有關的方案包括了[12][13][17]，這類型的方法透過事先計算出的
切割方式將規則資料庫切成許多小的資料庫，進而使用線性搜尋來尋找小資料庫內的規則。由於在
切割的過程中會造成規則的複製，因此此類演算法在最差情況下可能會需要巨大的空間來存放資料
庫。但在現有的環境中，決策樹演算法被公認為目前最有效的方案。 
欄位組合解決方案：另一類型的解決方案運用了欄位組合的概念，藉由事先運算將所有的可能
欄位組合找出，之後僅需搜尋每個符合的欄位並加以組合即可找出符合的規則[9]，此方法最大的
問題在於所需記憶體空間非常巨大，因此在[2]中，改變組合的過程並刪除不需要的組合，進一步
將所需的空間降低。 
雜湊表解決方案：在此類型方案中，所有的規則依據其使用的欄位值長度組合來將規則分類，
由於長度組合數目很明顯的會比規則數目少，因此搜尋時速度即可加快。搜尋的方式有利用多個一
維搜尋來篩選可能的長度組合或是利用額外的特殊標記(marker)來執行二維的運算[3]，在[30]與
[31]中，我們分別提出了改良的作法，然而這些方法都是針對單一配對封包分類所設計的，並無法
處理多重配對封包分類的問題。 
由目前的文獻中，我們可以看出現有針對多重配對封包分類的演算法上十分稀少，且無法處理
大量的規則，因此我們希望能夠提出一個高速的多重配對封包分類演算法以解決目前對於網路安全
的殷切需求。 
（四）研究方法 
我們的演算法以雜湊表資料結構為基礎，因為此類資料結構可以允許處理大量的規則，加上目
前我們在利用雜湊表資料結構方面已經發表了數篇重要的論文，因此延續先前的研究並規劃出下列
兩個演算法，透過結合此二演算法以得到改良的封包分類演算法。 
第一個演算法是利用將原有規則加以編碼，進而改變其不同的欄位長度，達到減少欄位長度組
合的目的，此方法是利用規則中的欄位值特性加以處理，因此不會造成任何的空間浪費[31]。但是
相對的效果只取決於規則的欄位值特性，因此在某些情況下可能無法得到任何改進。所以需要第二
個演算法的輔助，進一步減少欄位長度組合。 
第二個演算法是利用規則擴展(Filter Expansion)的方式來改變每個規則的長度組合，進而降
低長度組合的總數。此一作法是在[30]中由我們所提出的概念，藉由規則擴展，我們可以主動的改
變長度組合，然而此類方法最大的問題在於其所需的空間可能因為規則複製而產生較大的資料結
構，因此我們需要利用最佳化運算來將所需的空間降低。另一方面我們利用了編碼的特性進一步將
規則擴展所產生的語意相同規則壓縮成一條規則，藉此來降低空間成本，不過這樣的作法亦導致計
  5 
 
Multi-dimensional Range Matching,＂ in ACM SIGCOMM, September 1998, pp. 203–214. 
[11] Anja Feldmann and S. Muthukrishnan, “Tradeoffs for Packet Classification,＂ in IEEE INFOCOM, March 
2000, pp. 1193–1202. 
[12] Thomas Woo, “A Modular Approach to Packet Classification: Algorithms and Results,＂ in IEEE INFOCOM, 
March 2000, pp. 1213–1222. 
[13] Pankaj Gupta and Nick McKeown, “Packet Classification using Hierarchical Intelligent Cuttings,＂ 
in Hot Interconnects VII, August 1999. 
[14] Florin Baboescu, Sumeet Singh and George Varghese, “Packet Classification for Core Routers: Is there 
an alternative to CAMs?＂ in IEEE INFOCOM, March 2003, pp. 53–63. 
[15] M. Buddhikot, S. Suri and M. Waldvogel, “Space Decomposition Techniques for Fast Layer-4 Switching.＂ 
in IFIP Sixth International Workshop on Protocols for High Speed Networks, August 1999, pp. 25–41. 
[16] V. Sahasranaman and M. Buddhikot, “Comparative Evaluation of Software Implementations of Layer-4 
Packet Classification Schemes.＂ in IEEE ICNP, November 2001, pp. 220–228. 
[17] Sumeet Singh, Florin Baboescu, George Varghese and Jia Wang, “Packet Classification Using 
Multidimensional Cutting,＂ in ACM SIGCOMM ＇03, August 2003, pp. 213–224. 
[18] M. Waldvogel, G. Varghese, J. Turner and B. Plattner, “Scalable High Speed IP Routing Lookups,＂ 
in ACM SIGCOMM, September 1997, pp. 25–36. 
[19] V. Srinivasan and G. Varghese, “Fast IP lookups using controlled prefix expansion,＂ ACM Trans. On 
Computers, vol. 17, pp. 1–40, Febuary 1999. 
[20] P. Gupta, S. Lin, and N. McKeown, “Routing Lookups in Hardware at Memory Access Speeds,＂ in IEEE 
INFOCOM, March 1999, pp. 1240–1247. 
[21] S. Nilsson and G. Karlsson, “IP-Address Lookup Using LC-Tries,＂ IEEE JSAC, vol. 17, no. 6, pp. 
1083–1092, June 1999. 
[22] Priyank Warkhede, Subhash Suri and George Varghese, “Fast Packet Classification for Two-Dimensional 
Conflict-Free Filters,＂ in IEEE INFOCOM, March 2001, pp. 1434–1443. 
[23] F. Baboescu, G. Varghese, “Fast and scalable conflict detection for packet classifiers,＂ Computer 
Networks, vol. 42, no. 6, pp. 715–735, August 2003. 
[24] M. Degermark, A. Brodnik, S.Carlsson, and S. Pink, “Small Forwarding Tables for Fast Routing 
Lookups,＂ in ACM SIGCOMM, September 1997, pp. 3–14. 
[25] David E. Taylor and Jonathan S. Turner, “ClassBench: A Packet Classification Benchmark,＂ in IEEE 
INFOCOM ＇05, March 2005. 
[26] S. Bradner, “Next Generation Routers. Overview,＂ in Networld Interop, 1997. 
[27] H. Song, S. Dharmapurikar, J. Turner and J. Lockwood, “Fast Hash Table Lookup Using Extended Bloom 
Filter: An Aid to Network Processing,＂ in ACM SIGCOMM, September 2005, pp. 181–192. 
[28] Pi-Chung Wang, Hung-Yi Chang, Chia-Tai Chan and Shuo-Cheng Hu, “Scalable Packet Classification Using 
Condensate Bit Vector,＂ IEICE Transactions on Communications, Vol. E88-B, No. 4, April 2005, pp. 
1440-1447. 
[29] Fang Yu, Randy H. Katz and T.V. Lakshman, “Efficient Multimatch Packet Classification and Lookup 
with TCAM,＂ IEEE Micro, vol. 25, no. 1. 
[30] Pi-Chung Wang, Chia-Tai Chan, Shuo-Cheng Hu, Chung-Liang Lee and Wei-Chun Tseng, “High-Speed Packet 
Classification for Differentiated Services in NGNs,＂ IEEE Transactions on Multimedia, Vol. 6, No. 
出席國際學術會議心得報告 
                                                             
計畫編號 NSC 95-2218-E-005-010 
計畫名稱 針對高速防火牆之多重配對封包分類演算法設計與分析 
出國人員姓名 
服務機關及職稱 
王丕中/國立中興大學資訊科學系/助理教授 
會議時間地點 時間：2007.07.02~2007.07.04, 地點：Banff, Alberta, Canada 
會議名稱 The Fifth IASTED International Conference on Circuits, Signals, and Systems 
發表論文題目 
z Scalable Packet Classification for Network Intrusion Detection 
z An Efficient Algorithm of Huffman Decoder with Nearly Constant 
Decoding Time 
 
一、參加會議經過 
本人於 6/29 日出發前往溫哥華，並在溫哥華休息一天後前往會議地點 Banff，第一天議
程包括數位 Ketnote Speaker 的演講，我們的第一篇論文發表亦安排在當天下午，因此
於該議程中我們介紹了“Scalable Packet Classification for Network Intrusion 
Detection＂，並與參與的學者針對實作的議題進行了討論。第二天上午則繼續延續論文
報告的議程，我們的第二篇論文“An Efficient Algorithm of Huffman Decoder with Nearly 
Constant Decoding Time＂亦在此議程中報告，我們與來自加拿大、大陸清華大學的學者
進行簡短探討。第二天下午與第三天全天則參與其他學者的論文報告。議程在第三天下
午全部結束，投稿的論文總共為 123 篇，而接受的論文數則為 47 篇，因此錄取比例約為
三分之ㄧ。 
 
二、與會心得 
本次會議中我們一共發表了兩篇論文，雖然兩篇論文的研究主題截然不同，但是歸根究
底都是屬於搜尋資料結構與演算法的設計，第一篇是我們主要的研究議題，亦是此研究
計畫的主要成果，在此篇論文的報告中我們闡述了一個新的演算法以解決封包分類的問
題，而與會學者中亦有從事相關領域者，因此我們對實作的議題進行了相關的探討，但
由於方法的複雜度較高，因此似乎無法觸及較深入的議題。而第二篇論文是針對 Huffman 
Decoding 這一古老的研究議題進行的研究，相對則獲得較多的迴響，因此我們也發現發
SCALABLE PACKET CLASSIFICATION FOR NETWORK INTRUSION
DETECTION
Pi-Chung Wang
Institute of Networking and Multimedia
National Chung Hsing University
Taichung, Taiwan, R.O.C.
email: pcwang@cs.nchu.edu.tw
Chia-Ming Chang
Graduate Institute of Networking and Multimedia
National Taiwan University
Taipei, Taiwan, R.O.C.
email: solo@cmlab.csie.ntu.edu.tw
ABSTRACT
Network intrusion detection systems, which protect high-
speed networks, demand both high throughput and scala-
bility to handle new threats. In this paper, we propose a
scalable algorithm of multimatch packet classification for
network intrusion detection to handle the potentially in-
creasing filters resulted from new threats. The algorithm
utilizes the previous idea, which categorizes filters based on
distinct length combinations and corresponds each combi-
nation to one hash table. The classification procedure con-
sists of d one-dimensional lookups and T hash accesses.
We adopt ternary content addressable memory (TCAM) to
accomplish the one-dimensional lookups. As compared to
the existing schemes, the proposed scheme shows a better
leverage between speed and storage performance.
KEYWORDS
Packet classification, firewalls, and forwarding.
1 Introduction
Rules in a network intrusion detection system (NIDS) usu-
ally contain 5-tuple header filters (i.e. Source IP Address,
Destination IP Address, Protocol, Source Port and Desti-
nation Port) plus some strings. According to the prede-
fined rules, NIDS performs two critical functions: packet
header classification and content inspection. While the for-
mer classifies Internet packets based on the header fields,
the latter examines the malicious strings in the packet con-
tent. Due to the different nature of packet header fields and
strings, the processes of the header classification and the
string matching are usually separated, and their results are
cross-producted to determine whether the incoming packet
is a threat.
As regarding the scalability of the existing solutions,
they are limited by either dimensions, speed [1] or stor-
age [1, 2] in the worst case. Therefore, the major property
to enable an algorithm scalable is the capability to adjust
data structure according to the characteristics of different
databases, such as the adjustments done in RFC, HiCuts
and HyperCuts. Specifically, the way how wildcards spec-
ified in the filters affect the data structure can also signifi-
cantly influential on the speed and storage performance.
In this work, we focus on the problem of packet
header classification. In the last few years, the problem of
packet header classification has been studied extensively.
However, the existing algorithms are mainly designed for
finding out the highest-priority matching filter, while the
NIDS demands all matching filters [3] to decide the ade-
quate processing. Thus, some of the heuristics optimized
for single-match classification are not suitable for the mul-
timatch case [3]. Moreover, most of these algorithms are
designed to process the existing, relatively small, filter sets
[4]. The performance of these algorithms cannot be guar-
anteed with a reasonable storage as the number of filters in-
creases. The purpose of this study is therefore to provide a
scalable solution to the problem of multimatch packet clas-
sification.
Our Contributions: In this work, we introduce a scal-
able algorithm for the multimatch packet classification to
handle the potentially increasing filters resulted from new
threats. The resulted space complexity is O(T (N/T )d),
where T is the number of distinct combinations, N is the
number of filters and d is the number of fields inspected
in the filters. The classification procedure consists of d
one-dimensional lookups and T hash accesses. We adopt
ternary content addressable memory (TCAM) to accom-
plish the one-dimensional lookups. As demonstrated in the
experimental results, the proposed scheme shows a better
leverage between speed and storage performance. Also, a
speedup can be easily achieved by accessing T hash tables
in a hardware-parallelism design.
The rest of the paper is organized as follows. The
proposed scheme consists of two techniques. In Section 2,
the first technique depicts how to reduce the number of dis-
tinct length combinations by encoding the filters according
to their prefix nesting. In Section 3, the second technique
presents a novel filter-expansion skill to eliminate selected
combinations. Section 4 provides the experimental setup
and results. Finally, Section 5 states our conclusion.
2 Filter Encoding
In this section, we improve the previous idea based on hash
tables. In [1], the authors presented a clever idea to cat-
egorize the filters according to their field-length combina-
tions. The filters with identical length combination can be
stored in a hash table and accessed through a hash function.
573-123 64
Table 2. The encoded filters of the filters in Table 1. The numbers behind slash are the lengths of the encoded prefixes.
Filter f1 f2 f3 f4 f5 Filter f1 f2 f3 f4 f5
F0 ABC/3 ADF/3 AC/2 A/1 AB/2 F6 AB/2 ADF/3 AC/2 AC/2 AB/2
F1 ABC/3 ADF/3 AB/2 AC/2 AB/2 F7 AB/2 AD/2 A/1 A/1 AC/2
F2 ABC/3 ADE/3 A/1 AC/2 AC/2 F8 A/1 ABC/3 A/1 A/1 AC/2
F3 ABC/3 ADE/3 A/1 AB/2 AC/2 F9 A/1 AB/2 A/1 AB/2 AB/2
F4 ABC/3 ADE/3 AC/2 AD/2 AC/2 F10 A/1 A/1 A/1 A/1 AB/2
F5 AB/2 ADF/3 AC/2 AB/2 AB/2 F11 A/1 A/1 A/1 A/1 AC/2
Node mapping to preifxes
Node not mapping to any preifxes
Node generated for prefix expansion
Figure 2. The one-dimensional prefix expansion.
We use the example in Table 1 to explain further. To
expand the length combination of F2 from (3,2,0,2,8) to
(3,3,2,2,8), 8(= 20×21×22×20×20) filters in Fig. 3, are
generated. The number of generated filters grows to 256(=
22 × 22 × 22 × 22 × 20) for expanding F7 to (3,3,2,2,8). If
we expand all filters to the length combination (3,3,2,2,8),
the number of filters drastically increases to 2,719.
To efficiently reduce the number of distinct length
combinations without incurring explosive storage, we in-
troduce a new data structure, termed remainder nodes in the
specialized binary tries. Based on the remainder nodes, the
encoded prefix expansion is proposed. Our approach can
reduce the cost of prefix expansion significantly, as well as
filter expansion. Consequently, a greedy algorithm is pro-
posed to select the eliminated combinations with minimal
expansion cost.
3.1 Remainder Node
The native prefix expansion described above generates all
possible successive prefixes, no matter the prefixes are ex-
istent or not. For the instance in Fig. 3, expanding the zero-
bit prefix “∗” in f3 to two bits will generate four prefixes,
where two of them, “00” and “11”, are inexistent in the
filters. This is because the expanded prefixes must cover
with all possible values, which match to the original pre-
fix. Through the technique of encoding, we can represent
those inexistent successive prefixes by one encoded prefix.
Without generating inexistent successive prefixes, the cost
of prefix expansion is only related to the number of the ex-
istent successive prefixes, rather than the expanded length.
To generate the encoded prefixes for the inexistent
successive prefixes of all prefixes, we count on a new
data structure of binary trie by introducing pseudo nodes,
termed “remainder node”. We start the procedure from
the binary tries for prefix encoding in the previous section.
1. The first step is to label each prefix node with the num-
ber of nested prefixes, `. The maximal ` in the binary
trie is defined as `MAX . Note that the value ` can also
represent the length of the encoded prefix correspond-
ing to the prefix node.
2. Next, a remainder node is generated for each prefix
node, whose ` is smaller than `MAX . The ` value of
the remainder node is equal to that of the correspond-
ing prefix node plus one.
3. In the third step, the empty branch of each node, says
α, is pointed to the remainder node generated for the
nearest prefix node on the path from α to the root of
the trie.
4. The fourth step eliminates those remainder nodes,
which are not pointed by any branch.
5. Last, for each existing remainder node whose ` is
smaller than `MAX , we generate a remainder node
with `+1 and pointed by the existing remainder node.
Repeat this step until no remainder node can be in-
serted.
Figure 4 illustrates our idea stepwise. First, the ` val-
ues of the prefix nodes are labelled. Then, for the prefix
nodes, A and B, the remainder nodes, E and F, are gen-
erated. In the third step, the empty branches in the binary
trie are pointed to the remainder nodes corresponding to the
nearest prefix nodes. The unpointed remainder node, E, is
then removed in the fourth step. In the fifth step, another
remain node G with maximal ` value is inserted for node F.
3.2 Encoded Filter Expansion
We present the idea, “encoded prefix expansion”, before in-
troducing “encoded filter expansion”. The encoded prefix
expansion is defined as: each prefix is expanded to pre-
fixes with larger ` values. For the instance in Fig. 4, to
expand the length of encoded prefix “A” to two, two pre-
fixes, “AB” and “AF”, are generated. The prefix “A” can
also be expanded into three ` = 3 prefixes, “ABC”, “ABD”
and “AFG”. Since each prefix node might generate one re-
mainder node for each value of nested prefixes, the number
of prefixes generated for each prefix expansion is less than
or equal to the number of prefixes nodes successive to the
source prefix node plus one. As compared to the native pre-
fix expansion, the cost of the encoded prefix expansion is
66
Prefixes
*
0
000
f1
Prefixes
*
10
01
11
f4
C F
Prefixes
*
0
01
1
10
111
A
C
F
E
B D
HG
f2
Prefixes
*
10
01
B C D
A
f3 f5
Prefixes
UDP
TCP
D
A
CBB C DE
A
A
D
B E
Figure 5. The prefix tries of the filters in Fig. 1 for the proposed prefix expansion.
£ = {c1, c2, . . . , cm}, where m is the number of distinct
length combinations. The greedy algorithm acts as follows.
• For each combination ci ∈ £, the presumed target cj
is derived, where the number of generated filters from
ci to cj is minimal, i.e., ‖R′ci,cj‖ ≤ ‖R′ci,ck‖, ck ∈
£, ck 6= cj .
• Next, the filter expansion of cx whose expansion cost
vx is minimum is carried out. Since the filters in cx
are expanded, cx is removed due to that it is empty.
• For the combination cy whose presumed target is cx,
the presumed target and the cost is recalculated.
• The second and third items are repeated until the
‖£‖ < T or ‖R′x,y‖ =∞,∀x, y ∈ £.
The maximal number of calculation step is now reduced to
m(m−1)+(m−1)(m−2)+ . . .+(T +1)T < m3 (6)
In the following, we demonstrate the bound of the number
of generated filters.
Theorem 1 Given a filter set withN filters, the space com-
plexity for expanding N filters from m into T length com-
binations is less than O(T (N/T )d), wherem ≥ T .
Proof : The worst case occurs as the filters are evenly dis-
tributed in the m distinct length combinations, i.e., there
are N/m filters in each length combination. To expand
one combination into another, (N/M + 1)d filters are gen-
erated. Hence the total number of generated filters, C, to
reduce the length combinations fromm to T is
C = (
N
M
+ 1)d × (M − T ) + T × N
M
(7)
< M × (N
M
+ 1)d (8)
≤ T × (N
T
+ 1)d = O(T (N/T )d) (9)
If the number of filters in one or more target length com-
binations is larger than N/M , then we can always find an-
other target length combination with less than or equal to
N/M filters for expansion without increasing C.
4 Performance Evaluation
In this section, real and synthetic filter sets are used to ex-
amine the performance of the proposed scheme. Six real
filter sets are collected from different ISPs. The numbers
of filters vary from 333 to 3,065. We compare the proposed
scheme with the notable scheme, HyperCuts. We also im-
plement a multimatch version of HyperCuts based on our
reasoning to [2].
The major performance metrics include the required
storage in Kbytes and numbers of memory accesses in the
worst case. In addition to these metrics, we further exhibit
the numbers of distinct length combinations after prefix en-
coding and filter expansion. These values can help us to
further illustrate the effect of the encoded filter expansion
and the limitation of the proposed scheme.
Table 3 shows the performance results of storage by
setting T = 8. Originally, the numbers of distinct length
combinations vary from 58 to 212. However, with prefix
encoding, the filters can be effectively grouped into 7 to
16 distinct length combinations. Since F3 features only 7
combinations, the filters in F3 are not expanded. For F1
and F2, there are 1 and 3 length combinations eliminated;
nevertheless, the number of filters is not increased. This
is due to the fields of the expanded filters contain none
of the longer prefixes. Hence there is only one remain-
der node successive to each prefix nodes corresponding to
these fields. In such condition, only one filter is generated
for each filter expansion, i.e., there is no cost for filter ex-
pansion. For the other filter sets, the numbers of generated
filters for filter expansion are still less than 50. Hence, the
cost for filter expansion is low to the small filter sets.
Table 3. Performance evaluation for real filter sets by set-
ting T = 8.
Proposed Scheme HyperCuts
Filter TCAM Storage Memory Storage Memory
Sets (Kbytes) (Kbytes) Access (Kbytes) Access
F1 1.70 11.47 18 47.05 15
F2 1.73 8.88 18 22.05 11
F3 1.16 5.20 17 11.53 13
F4 5.19 48.61 18 191.53 21
F5 4.31 36.75 18 128.51 21
F6 2.88 18.86 18 57.22 11
68
AN EFFICIENT ALGORITHM OF HUFFMAN DECODERWITH NEARLY
CONSTANT DECODING TIME
Pi-Chung Wang
Institute of Networking and Multimedia
National Chung Hsing University
Taichung, Taiwan, R.O.C.
email: pcwang@cs.nchu.edu.tw
Chun-Liang Lee
Department of Computer Science and Information Engineering
Chang Gung University
Taoyuan, Taiwan, R.O.C.
email: cllee@mail.cgu.edu.tw
Hung-Yi Chang
Department of Information Management
National Kaohsiung First University of Science and Technology
Kaohsiung, Taiwan, R.O.C.
email: leorean@ccms.nkfust.edu.tw
ABSTRACT
This paper introduces a Huffman decoding algorithm based
on the single-side growing Huffman tree. The proposed
scheme exploits an intrinsic property of the single-side
growing Huffman tree to utilize the instructions of the mod-
ern processor. The data structure is proved to have lin-
ear space complexity O(d + 2n), where d is the maximal
length of the Huffman codes and n is the number of sym-
bols. More importantly, each Huffman decoding could be
carried out by a constant number of instructions and two
memory accesses, regardless the length of Huffman codes.
As demonstrated in the experimental results, the proposed
scheme yields a nearly constant decoding time and outper-
form the existing schemes. Our source code is publicly
available.
KEYWORDS
Huffman codes, decoding, and data compression.
1 Introduction
Since the Huffman encoding scheme is proposed by D.A.
Huffman in 1952, the Huffman code has been widely used
in text, image and video compression. The simplest data
structure used in the Huffman decoding is the Huffman
tree. Array data structure [1] has been used to implement
the corresponding complete binary tree. However, the spar-
sity in the Huffman tree causes a huge waste of memory
space for array implementation.
In [2], Hashemian presented a Huffman decoding al-
gorithm based on his newly proposed array data structure.
First, the Huffman tree is transformed into a single-side
growing Huffman tree (SGH-tree) while sustaining its size.
With that data structure, the ordering and clustering meth-
ods can alleviate the effect of sparsity of the conventional
Huffman tree and support quick search time, where the
memory requirement is ranged from O(n) to O(2d) [2],
where n is the number of symbols and d is the depth of
the Huffman tree. Recently, Hashemian also presented a
storage-efficient decoding algorithm which runs in O(d)
time [3]. In [4], Chung proposed a data structure with only
2n − 1 memory. Chen et al. [5] further reduce the mem-
ory requirement to d3n/2e + dn/2logne + 1. In short,
existing schemes aim to develop a compact data struc-
ture, while barely maintaining the decoding performance
as O(d) [2, 3, 5–8].
Our contributions: In this paper, a Huffman decoding
algorithm based on the single-side growing Huffman tree
is introduced. The proposed scheme exploits an intrinsic
property of the single-side growing Huffman tree to utilize
the instructions of the modern processor. Our data structure
is proved to have linear space complexity O(d+ 2n). Each
Huffman decoding could be carried out by a constant num-
ber of instructions and twomemory accesses, regardless the
length of Huffman codes. The experimental results demon-
strate that the proposed scheme can yield a nearly constant
decoding time. In addition, the decoding time outperforms
the existing schemes while maintaining a comparable re-
quired storage.
The rest of the paper is organized as follows. First, the
proposed data structure is introduced in Section 2. Section
3 presents the decoding procedure. The experiment results
are presented in Section 4. Finally, a summary is given in
Section 5.
2 The Proposed Data Structure
Consider a set of source symbols S = {s0, s1, . . . , sn−1}
with weights W = {w0, w1, . . . , wn−1} for w0 ≥ w1 ≥
. . . ≥ wn−1, where the symbol si has weightwi. The code-
word ci, 0 ≤ i ≤ n − 1, for symbol si can then be deter-
mined by traversing the path from the root to the leaf node
associated with the symbol si in the Huffman tree, where
the left branch is corresponding to ‘0’ and the right one is
corresponding to ‘1’. Let the level of the root be zero and
the level of the other node equal to the sum of its parent’s
level and one. Codeword length li for si can be known as
573-108 131
Index array
T0
s8
s3s2
s1s0
T9
T11
The ith entry in the index array records
the starting address of the symbol array
of Ti and depth of Ti.
The entry in the symbol array
records the matching symbol
and its codeword length.
The entries of the symbol arrays in memory
0
6
12
18
24
30
s7s6s5s4
s9 s11s10
s12s13
s14s15s16s17 s18
s19s20s21
s23s22 s24s25 s26
s27s28 s29
s30s31
T8
T12
T10
T7
T6
T5
T4
T3
T2
T1
6,1
0,1
2,2
34,1
33,0
31,1
30,0
28,1
24,2
20,2
12,2
8,2
T0
T4
T3
T2
T1
T12
T9
T8
T7
T6
T5
T11
T10
16,2
35
5
11
17
23
29
s28,12
s1,2 s5,4s4,4s3,4
s7,4 s9,6 s10,6s8,5
s15,8s13,7s12,7s11,6
s16,8 s17,8 s18,8 s19,9 s20,9
s22,10s23,10 s25,10
s31,13
s21,9
s0,2 s2,4
s6,4 s8,5
s11,6
s18,8
s21,9 s24,10
s26,10 s30,13s27,12 s29,12
s14,8
The first entry of each
symbol arraysj,A
Ti The root of subtree Ti
Figure 2. An 32-symbol example of the SGH-tree and the generated index and symbol arrays.
After removing the leaves which are left to the kth
leaf from the subtree Ti, i > 0, the number of the leaves in
the SGH-tree is reduced from (d +
∑j<d−1
j=1 2
d−j−1 + 2)
to ((d − 1) + ∑j<ij=1 2d−j−2 + k/2 + (2d−i−1 − k) +∑j<d−1
j=i+1 2
d−j−1 + 2), where (
∑j<i
j=1 2
d−j−2 + k/2 +
1) is the number of the newly generated leaves, i.e.,
nodes with their children removed. The number of
the required entries of the symbol arrays is also de-
creased from (
∑j<d−1
j=0 2
d−j−1 + 2) to (
∑j<i
j=0 2
d−j−2 +∑j<d−1
j=i 2
d−j−1 + 2) since the depths of the subtrees
Tj , 0 ≤ j < i, are decreased by 1. Therefore, the
number of the decreased entries in the symbol arrays
(
∑j<i
j=0 2
d−j−2) is no less than twice the number of the re-
moved leaves (1 +
∑j<i
j=1 2
d−j−2 + k/2).
Theorem 1 For a SGH-tree whose depth is d, the capacity
ratio satisfies γ < 2.
Proof : A given SGH-tree with depth d can be con-
structed by adding new leaves or removing original leaves
from the exemplified SGH-tree in Fig. 3. Nevertheless, the
new leaves can only be added to T0; therefore, the number
of the required entries in the symbol arrays is not affected,
and the γ value is further decreased. Now, we consider the
case of removing leaves.
The removing procedure starts from the bottommost
level of the SGH-tree. Assuming that there are nd leaves
in the dth level of the SGH-tree to be constructed. The first
(2+2d−1−nd) leaves from the left are removed and there
are ((d − 1) + 2d−2 + nd/2) leaves in the new SGH-tree.
According to lemma 1, the number of the decreased entries
in the symbol arrays is to be no less than twice the number
of the removed leaves. Therefore, the resulted γ is less than
2d/(d + 2d−1) since 2
d−sd
d+2d−1−ld ≤
2d−2ld
d+2d−1−ld <
2d
d+2d−1 ,
where sd is the number of the reduced entries in the dth
symbol arrays and ld is the number of the reduced leaves in
the dth level. This procedure is performed for the (d−1)th
level of the new SGH-tree. The nd leaves at level d can
be ignored in the following procedure because they are not
affected by the subsequently removed leaves. The leaves
beyond the nd−1 leaves and nd/2 parent nodes of nd leaves
at level d are then removed from the new SGH-tree. The
procedure is repeated until all leaves in the given SGH-tree
are generated in the exemplified SGH-tree. The value of
γ thus can be formulated as (2d −∑i<di=1 si)/(d + 2d−1 −∑i<d
i=1 li) and satisfies γ < 2
d/(d+ 2d−1) < 2.
Theoretically, the space complexity is O(d+2n). We
can further derive the exact storage requirement. The re-
quired bits for each entry in the index array and the sym-
bol arrays can be formulated as (dlog22ne + dlog2de) and
(dlog2ne+ dlog2de), respectively. Therefore, the entire re-
quired storage can be formulated as
d× (dlog22ne+ dlog2de) + 2n× (dlog2ne+ dlog2de)
= d+ (d+ 2n)× (dlog2ne+ dlog2de) (4)
If there are 256 symbols and the maximal length of Huff-
man codewords is 16 bits, the required storage is 796 bytes
in the worst case. However, in our experiments, the re-
quired storage is usually far less than the worst case.
3 The Decoding Procedure
The decoding procedure consists of two steps. The first
step determines the index of the fetched entry in the index
array. To achieve this, the accessed symbol array can be
decided by the position of the leftmost zero bit in the in-
coming bit stream. We utilize the bit scan reverse (BSR)
instruction, which derives the position of the first set bit in
a input stream [10], to generate the position of the first zero
bit by inputting a complement of the input stream. After
deriving the position of the first zero bit (says i), the ith
entry in the index array is accessed for the starting address
of the selected symbol array. In addition, the field of the
133
