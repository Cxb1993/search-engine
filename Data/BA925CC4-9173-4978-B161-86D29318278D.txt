計畫綱要 
在資料探勘的領域上，異常偵測 (Anomaly Detection) 是個很重要的議題且被應用在各種
領域之中，如國土安全、信用卡詐欺偵測、網路入侵偵測、網路內部威脅偵測，以及醫療上惡
性腫瘤診斷等方面都有著廣泛的應用。事實上在這些應用中，大多數的可疑行為或特殊跡象都
可能會產生「異常點」，而當前對於「異常點」最為普遍的認知是：在資料集中未遵循預期的
行為或資料分佈之資料點。而在實際的應用上，由於有標記的資料點數量常是少數，且人們感
興趣的這些異常點在資料集中更是稀少，甚至沒有出現過。也因此異常偵測在近期是受到愈來
愈多人的重視。 
在這個計畫中，我們研究了現今普遍的異常偵測演算法，如 Local Outlier Factor (LOF) 和 
Angle Based Outlier Detection (ABOD)，這兩種演算法皆為以資料點為基礎的演算法。但這類
型的演算法由於其過高的計算成本，對於大量的資料集將不敷使用。因此在本計畫中，我們提
出了執行速度更快、效能更優異的異常偵測演算法，以期能更有效地應用於偵測大量資料集。 
在實際的應用上面我們提出了一個架構，此架構分為兩階段，即「Data Cleaning」階段及
「On-Line Detection」階段。「Data Cleaning」階段意即給定一個資料集，先使用 “Leave One Out” 
的方式檢視個別資料點是否存在於資料集時對模型的影響，再對所有資料點依據其為異常點的
可疑性做排序，濾除掉所有的可疑點。而「On-Line Detection」階段意即利用我們在現有資料
點訓練出的模型進行線上異常點偵測。 
最後，我們透過多個數值實驗來驗證我們提出的方法，並且討論該方法的實際應用，如網
路入侵偵測系統，網路應用程式入侵偵測系統及資料庫安全等等。 
 
研究目的 
在先前異常偵測的方法中，如 LOF 及 ABOD 等以資料點為基礎的演算法，雖然在異常
偵測上有不錯的效果，但這兩個方法都要維持住所有訓練集內的資料，在線上偵測時納入計
算，因此計算複雜度高，不適用於 On-Line 的應用，僅適於「Data Cleaning」或可有充足時
間之 Off-Line 處理。 
因此我們提出一個新的異常偵測演算法，既能做到「Data Cleaning」，且未來在 On-Line 的
應用下，計算速度更快、效能更優異的演算法，並將之付諸於實際的應用如 IDS、 DB Security 
上。 
 
提出之技術 
我們對於異常點的詮釋為：該資料點的存在於資料集與否會對資料集之模型有很大的影
響。以這種詮釋為基礎，我們提出線上異常偵測的機制，對於新的測試資料點，先計算該資料
 計畫成果 
由於 Online Over-sampling PCA 在執行速度上有著非常明顯的改進，因此我們將此演算
法應用於網路入侵偵測之研究中，以期能降低 False Alarm Rate。在實務上，亦可加入正常點
作為 PCA 的主要方向之更新，如此便擁有調整 (Adaptive) 的能力，亦可符合 Real-Time的需
求。透過這樣的設計，我們提出一個效能足敷需求的 Online NIDS 架構。 
另外對於網路應用層 (Application Layer) 資料庫的查詢使用，我們也將針對呼叫頻率，
SQL 語法運用的異常偵測等特徵，運用機器學習及資料探勘的技術，提出一個資料庫不當查
詢的警示機制，如此可以防範內外部不同型式的入侵或不當使用行為，並可以解決資料庫使用
效能低落及嚴重的資料竊取問題。 
本計畫提出之異常偵測演算法及其架構整理成學術論文 (如下頁所示)，並發表至國際著
名研討會 First KES International Symposium on Intelligent Decision Technologies (IDT'09)，
2009。且目前送交至國際著名期刊 ACM Transactions on Knowledge Discovery from Data 審
核。另有相關研究成果「Periodic Step-Size Adaptation for Single-Pass On-line Learning」發表於
The Neural Information Processing Systems (NIPS)，2009。 
2 ·
1. INTRODUCTION
Anomaly (or outlier) detection aims to identify a small group of instances which
deviate remarkably from the existing data. A well-known definition of “outlier” is
given by Hawkins [Hawkins 1980]: “an observation which deviates so much from
other observations as to arouse suspicions that it was generated by a different mech-
anism,” which gives the general idea of an outlier and motivates many anomaly de-
tection methods [Breunig et al. 2000; Chandola et al. 2009; Hawkins 1980; Huang
et al. 2007; Kriegel et al. 2008; Lazarevic et al. 2003]. Practically, anomaly de-
tection can be found in applications such as homeland security, credit card fraud
detection, intrusion and insider threat detection in cyber-security, fault detection,
or malignant diagnosis [Huang et al. 2007; Lazarevic et al. 2003; Rawat et al. 2006;
Wang et al. 2004; Chandola et al. 2009]. However, since only a limited amount
of labeled data are available in the above real-world applications, how to deter-
mine and anomaly of unseen data (or events) draws attention from the researchers
in data mining and machine learning communities [Hawkins 1980; Breunig et al.
2000; Lazarevic et al. 2003; Huang et al. 2007; Kriegel et al. 2008; Chandola et al.
2009].
Despite the rareness of the deviated data, its presence might enormously affect
the solution model such as the distribution or principal directions of the data. For
example, the calculation of data mean or the least squares solution of the associated
linear regression formulation are both sensitive to outliers. As a result, anomaly
detection becomes a very challenging task, since one needs to solve an unsupervised
yet unbalanced data learning problem. In this paper, we propose an online outlier
detection algorithm based on principal component analysis (PCA). We observe
that adding (or removing) an abnormal data instance will cause a large variation
of principal directions than adding (or removing) a normal one does. Thus, by
applying the “Leave One Out” (LOO) procedure on the entire data set, we are able
to evaluate the outlierness of each individual data point by calculating the variation
of principal directions. More precisely, our idea is to first calculate the dominant
eigenvector of all data instances, then another PCA analysis is performed on the
entire data set without the target data instance present. The difference between
these two eigenvectors indicates the anomaly of the target instance. By ranking the
difference scores of all data points, one can identify the outlier data by a pre-defined
threshold or a pre-determined portion of the data.
We note that the above framework can be considered as a decremental PCA based
approach for anomaly detection. While it works well for applications with moderate
dataset size, the variation of principal directions might not be significant when
the size of the dataset is large. In such cases (e.g. real-world anomaly detection
problems dealing with large-scale data), adding or removing one target instance only
produces negligible difference in eigenvectors. To address this issue, we advance the
“over-sampling” strategy to duplicate the target instance, and we perform an over-
sampling PCA (osPCA) on such an over-sampled dataset. It is obvious that the
effect of an outlier instance will be amplified due to its duplicates present in the PCA
formulation, and this makes the detection of outlier data easier. However, this LOO
anomaly detection procedure with an over-sampling strategy will markedly increase
the computational load, since one needs to keep a dense covariance matrix and solves
ACM Journal Name, Vol. V, No. N, Month 20YY.
4 ·
of each data instance, the LOF determines the degree of outlierness, which provides
suspicious ranking scores for all samples. The most important property of the LOF
is the ability to estimate local data structure via density estimation. This allows
users to identify outliers which are sheltered under a global data structure. However,
it is worth noting that the estimation of local data density for each instance is very
computationally expensive, especially when the size of the dataset is large.
Besides the above work, some anomaly detection approaches are recently pro-
posed [Aggarwal and Yu 2001; Kriegel et al. 2008; Kriegel et al. 2009]. Among
them, the angle-based outlier detection (ABOD) method [Kriegel et al. 2008] is
very unique. They proposed to calculate the variation of the angles between each
target instance and the remaining data points, since they observed that an outlier
will produce a smaller angle variance than the normal ones do. It is not surpris-
ing that the major concern of ABOD is the computation complexity due a huge
amount of instance pairs to be considered. Consequently, a fast ABOD algorithm
is proposed to generate an approximation of the original ABOD solution. The dif-
ference between the standard and the fast ABOD approaches is that the latter only
considers the variance of the angles between the target instance and its k nearest
neighbors. However, the search of the nearest neighbors still prohibits its extension
to large-scale problems (batch or online modes), since the user will need to keep all
data instances to calculate the required angle information.
3. ANOMALY DETECTION VIA PRINCIPAL COMPONENT ANALYSIS
We first briefly review the PCA algorithm in Section 3.1. Based on the leave-
one-out (LOO) strategy, Section 3.2 presents our study on the effect of outliers on
the derived principal directions, followed by theoretical backgrounds provided in
Section 3.3.
3.1 Principal Component Analysis
PCA is a well known unsupervised dimension reduction method, which determines
the principal directions of the data distribution. To obtain these principal direc-
tions, one needs to construct the data covariance matrix and calculate its dominant
eigenvectors. These eigenvectors will be the most informative among the vectors
in the original data space, and are thus considered as the principal directions. Let
A = [x⊤1 ;x
⊤
2 ; · · · ;x⊤n ] ∈ Rn×p, where each row xi represents a data instance in
a p dimensional space, and n is the number of the instances. Typically, PCA is
formulated as the following optimization problem
max
U∈Rp×k,‖U‖=I
n∑
i=1
U⊤(xi − µ)(xi − µ)⊤U, (1)
where U is a matrix consisting of k dominant eigenvectors. From this formulation,
one can see that the standard PCA can be viewed as a task of determining a
subspace where the projected data has the largest variation.
Alternatively, one can approach the PCA problem as minimizing the data recon-
struction error, i.e.
min
U∈Rp×k,‖U‖=I
J(U) =
n∑
i=1
‖(xi − µ)−UU⊤(xi − µ)‖2, (2)
ACM Journal Name, Vol. V, No. N, Month 20YY.
6 ·
Remove an outlier
Add an outlier
Remove a normal data point
Add a normal data point
Fig. 1. The effect of adding/removing an outlier or a normal data instance on the principal
direction.
Once these eigenvectors u˜t are obtained, we use the absolute value of cosine simi-
larity to measure the variation of the principal directions, i.e.
st = 1− | 〈u˜t,u〉‖u˜t‖‖u‖ |. (7)
We note that, for the target instance xt, st can be considered as a distance metric,
or a “score of outlierness” to indicate the anomaly of xt. A higher st score (closer
to 1) means that the target instance is more likely to be an outlier. This process
can be considered as a decremental PCA with LOO scheme for anomaly detection.
In contrast with decremental PCA, we also consider the use of incremental PCA
for outlier detection. This strategy is preferable in online anomaly detection ap-
plications, in which we need to determine whether a newly received data instance
is an outlier. If the recently received data points are normal ones, adding such
instances will not significantly affect the principal directions (and vice versa). The
incremental PCA can be formulated as follows
ΣA˜u˜t = λu˜t, (8)
where A˜ = A ∪ {xt}. Again, µ˜ is the mean of A˜, and we have
ΣA˜ =
1
n+ 1
∑
xi∈A
(xi − µ˜)(xi − µ˜)⊤ + 1
n+ 1
(xt − µ˜)(xt − µ˜)⊤. (9)
Similarly, we check the score st of each newly received instance and determine its
ACM Journal Name, Vol. V, No. N, Month 20YY.
8 ·
we will discuss its limitations and why we advance a least squares approximation
of osPCA. Our proposed an online updating algorithm for osPCA is detailed in
Section 4.3.
4.1 Over-Sampling Principal Components Analysis (osPCA)
As mentioned earlier, when the size of the dataset is large, adding (or removing)
a single outlier instance will not significantly change the principal direction of the
data distribution. Furthermore, for problems where all data instances (normal and
deviated ones) are present (i.e. not in a streaming setting), it is not clear how to
apply an incremental PCA to address the anomaly detection problem. Furthermore,
Therefore, we integrate the over-sampling and LOO strategies together with the
incremental PCA, and propose an online over-sampling PCA (osPCA) algorithm
for large-scale anomaly detection problems.
Our osPCA algorithm will amplify the effect of an outlier instance and thus
make the detection of outlier data easier. Suppose that we over-sample the target
instance n˜ times, the associated PCA can be formulated as follows
ΣA˜u˜t = λu˜t, (11)
where A˜ = A ∪ {xt, . . . ,xt} ∈ R(n+n˜)×p. The mean of A˜ is µ˜, and thus
ΣA˜ =
1
n+ n˜
∑
xi∈A
(xi − µ˜)(xi − µ˜)⊤ + 1
n+ n˜
n˜∑
i=1
xtx
⊤
t − µ˜µ˜⊤. (12)
In this osPCA framework, we will duplicate the target instance n˜ times (e.g. 10%
of the size of the original data set), and we will compute the score of outlierness st
of that target instance, as defined in (7). If this score is above some predetermined
threshold, we will consider this instance as an outlier. With this over-sampling
strategy, if the target instance is a normal data (see Fig. 2a for example), we will
observe negligible changes in the principal directions and the mean of the data. The
case of over-sampling an abnormal instance is shown in Fig. 2b. It is clear that
our approach will amplify its effect on the derived principal direction. Therefore,
our method not only determines the anomaly of newly received data, it can also be
applied to detect outliers from existing data.
Clearly, the major concern is the computation cost of calculating or updating the
principal directions in large-scale problems. We will discuss this issue and propose
our solutions in the following sections.
4.2 The Power Method for osPCA
Typically, solutions to PCA is determined by solving an eigenvalue decomposition
problem. In the LOO scenario, one will need to solve the PCA and to calculate the
principal directions n times for a data set with n instances. This is very computa-
tionally expensive, and prohibits the practical use of such a framework for anomaly
detection.
It can be observed that, in the PCA formulation with the LOO setting, it is
not necessary to re-compute the covariance matrices for each PCA. This is because
when we duplicate a data point of interest, the difference between the updated
covariance matrix and the original one can be easily determined. Let Q = AA
⊤
n be
ACM Journal Name, Vol. V, No. N, Month 20YY.
10 ·
of the dominant eigenvector) is updated by
uk+1 =
ΣA˜uk
‖ΣA˜uk‖
. (15)
The sequence {uk} converges under the assumption that the dominant eigenvalue
of ΣA˜ is markedly larger than others. From (15), it is clear that the power method
only requires matrix multiplications, not decompositions; therefore, the use of the
power method can alleviate the computation cost in calculating the dominant prin-
cipal direction. We note that one could use the deflation process [Golub and Golub
1983] if other principal directions besides the dominant one need to be determined.
In our anomaly detection framework, we only consider the first principal component
and evaluate its variation in computing the score of outlierness of each sample.
4.3 Least Squares Approximation and Online Updating for osPCA
In the previous subsection, we apply a matrix update technique in (14) and the
power method to solve our over-sampling PCA for outlier detection. However, the
major concern of the power method is that it does not guarantee a fast convergence,
even if we use prior principal directions as its initial solutions. Moreover, the
use of power method still requires the user to keep the entire covariance matrix,
which prohibits the problems with high dimensional data or with limited memory
resources. Inspired by [Yang 1995; Papadimitriou et al. 2005], we propose an online
updating algorithm to calculate the dominant eigenvector when over-sampling a
target instance. We now discuss the details of our proposed algorithm.
Recall that, in Section 3, PCA can be considered as a problem to minimize the
reconstruction error
min
U∈Rp×k,‖U‖=I
J(U) =
n∑
i=1
‖x¯i −UU⊤x¯i‖2, (16)
where x¯i is (xi − µ), U is the matrix consisting of k dominant eigenvectors, and
UU⊤x¯i is the reconstructed version of x¯i using the eigenvectors in U. The above
reconstruction error function can be further approximated by a least squares form
[Haykin 1991]:
min
U∈Rp×k,‖U‖=I
Jls(U) =
n∑
i=1
‖x¯i −Uyi‖2, (17)
where U′ is the approximation of U, and thus yi = U′⊤x¯i ∈ Rk is the approx-
imation of the projected data U⊤x¯i in the lower k dimensional space. Based on
this formulation, the reconstruction error has a quadratic form and is a function of
U, which can be computed by solving a least squares problem. The trick for this
least squares problem is the approximation of U⊤x¯i by yi = U′⊤x¯i. In an online
setting, we update the current U⊤t x¯i by the previous solution U
⊤
t−1x¯i as follows
min
Ut∈Rp×k,‖U‖=I
Jls(Ut) =
t∑
i=1
‖x¯i −Utyi‖2, (18)
where yi = U
⊤
t−1x¯i. This projection approximation provides a fast calculation of
principle directions in our over-sampling PCA. Linking this least squares form to
ACM Journal Name, Vol. V, No. N, Month 20YY.
12 ·
Algorithm 1: Anomaly Detection via Online Over-sampling PCA
Input: The data matrix A = [x⊤1 ;x
⊤
2 ; · · · ;x⊤n ] and the weight β.
Output: Score of outlierness s = [s1s2 · · · sn]. If si is higher than a threshold,
xi is an outlier.
Compute first principal direction u by using (18);
Keep x¯proj =
n∑
j=1
yjx¯j and y =
n∑
j=1
y2j in (22);
for i← 1 to n do
u˜← βx¯proj+yix¯i
βy+y2i
by (18);
si ← 1− | 〈w˜,w〉‖u˜‖‖u‖ | by (7);
Table I. Comparisons of the power method and our proposed online osPCA for anomaly detection
in terms of computational complexity and memory requirements. Note that m indicates the
number of iterations.
Power Method Online Over-sampling PCA
Computation complexity O(nmp2) O(np)
Memory requirement O(p2) O(p)
−10 −5 0 5 10
−6
−4
−2
0
2
4
6
x1
2
(a)
 
 
Normal data:190 pts
Deviated data: 10 pts
The first PC of normal data
Outlier identified mark (5%)
0 50 100 150 200
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
0.045
indices of instances
sc
o
re
s 
o
f o
ut
lie
rn
es
s
(b)
Fig. 3. The result of identifying outliers in the 2-D synthetic data.
requirement of applying power method is O(p2) because the covariance matrix is
always needed. In our online updating approach, the updating of the principal
direction is done point-wisely, and thus the LOO procedure only results in O(np)
and O(p) for the computation complexity and memory requirement, respectively.
5. EXPERIMENTAL RESULTS
5.1 Anomaly Detection on Synthetic and Real-world Data
5.1.1 2D synthetic data set. To verify the feasibility of our proposed algorithm,
we conduct experiments on both synthetic and real data sets. We first generate a
2-D synthetic data, which consists of 190 normal instances (shown in blue dots in
ACM Journal Name, Vol. V, No. N, Month 20YY.
14 ·
Table III.Average CPU time (in seconds) of decremental PCA (dPCA), over-sampling PCA
(osPCA) with power method, our osPCA with online updating algorithm, fast ABOD, and
LOF on the pendigits data set.
Methods dPCA osPCA osPCA Fast ABOD LOF
(with power method) (with online updating)
Time (sec.) 0.0589 0.0892 0.0121 13.804 0.0789
curve (AUC) [Bradley 1997] to evaluate the detection performance, and the results
are shown in Tables 2 and 3. We note that, for over-sampling based methods such
as our online osPCA or osPCA with power method, the ratio r of the number of
duplicated target instances to the size of the data set size is chosen as 0.4. In
LOF and fast ABOD, we choose the numbers k of the nearest neighbors which
produce the best performances in Table 2 (k = 70 and 100 for ABOD and LOF,
respectively). It is worth repeating that the parameter β in our online osPCA can
be calculated with β = 1nr , as discussed in Section 4.2.
From these two tables, we see that our proposed osPCA achieved comparable
results with state-of-the-art methods such as LOF and fast ABOD, while ours is
the most computationally efficient approach among the methods considered. By
comparing the first and the second (or third) column in Table 2, it is interesting
to note that the AUC score of osPCA is significantly better than that of dPCA
(without over-sampling strategy). This confirms that the over-sampling strategy
indeed amplifies the outlierness of this type data, and makes the anomaly detection
problem easier. Finally, we observe that the performance of our osPCA with online
updating technique is comparable to that using power method in most scenarios. As
discussed in Section 4, this would allow us to calculate the approximated version of
the principal direction without sacrificing computation and memory requirements.
The second dataset we consider is the KDD intrusion detection dataset. There
are four categories of attacks in this data set:
—DOS: denial-of-service
—R2L: unauthorized access from a remote machine
—U2R: unauthorized access to local superuser (root) privileges
—Probe: surveillance and other probing
We use binary and continuous features (31 features) and focus on the 10% training
subset under the tcp protocol. The size of normal data is 76813. In this experi-
ment, data points from four different attacks are considered as outliers. Table 4
shows detection performance (in terms of AUC) and the numbers of test samples
of each attack category. Only LOF is used for comparison, since it is shown to
outperform the ABOD method in Table 2. From this table, we see that our osPCA
again achieved comparable performance with LOF, while the LOF required signifi-
cant longer computation time. Nevertheless, the effectiveness of our online osPCA
is verified by the experiments conducted in this section, and it is clear that our ap-
proach is the most computationally efficient one among the methods we considered
for comparison.
ACM Journal Name, Vol. V, No. N, Month 20YY.
16 ·
Table V. Online anomaly detection results on the KDD intrusion detection data set.
Attack Testing data size TP FP
type normal attack Rate Rate
Dos 2000 100 0.940 0.073
Probe 2000 100 0.980 0.022
R2L 2000 100 0.900 0.071
U2R 2000 49 0.816 0.038
target instance will be classified as an outlier. Our quick updating algorithm for the
first principal direction is able to meet the on-line detection requirement, and thus
makes our method unique. Prior PCA, ABOD, or LOF based methods cannot be
easily extended to online detection problems. Using our framework, one only needs
to decide the threshold for detecting an abnormal instance. In practice, we choose
to advance some statistics properties rather than blindly setting this threshold. We
first calculate the mean and standard deviation of the score of outlierness, which
are computed from all normal data points (after the data cleaning phase). Once
we collect this information, the user can easily decide whether to mark the newly
received data point as an outlier if its score of outlierness is above multiple times
of the standard deviation.
To simulate this on-line anomaly detection task, we first extract 2000 normal
instances points from the KDD intrusion data set for training, and filter the top
5% (100 points) to avoid noisy data. Next, we extract the first principal direction
and use our online osPCA to detect the received attacks (the numbers of normal
data and attacks from each category are shown in Table 5). Table 5 lists the
performance of our proposed method, and we achieved promising high true positive
and low false positive rates. It is worth noting that we did not re-implement the
ABOD or LOF methods for comparisons here, since we already showed that our
methods is able to achieve comparable performance while requiring significantly
less computatio time (and thus is mor suitable for online detection problems).
6. CONCLUSION
In this paper, we proposed an online over-sampling PCA (osPCA) algorithm for
anomaly detection. We showed that the osPCA with LOO strategy will amplify
the effect of outliers, and thus the variation of principal directions can be used as a
score of outlierness to identify abnormal data or rare events in anomaly detection
problems. When over-sampling a data instance, our online osPCA algorithm effi-
ciently updates the principal directions without solving eigenvalue decomposition
problems; furthermore, our method does not need to keep the entire covariance or
data matrices during the evaluation process. Therefore, compared with prior PCA
or anomaly detection methods, our approach is able to approximate the PCA so-
lutions while significantly reducing computational costs and memory requirements.
Thus, our osPCA is preferable for large-scale or streaming data problems.
Future research will be directed to the following anomaly detection scenarios:
normal data with multi-clustering structure, and data in a extremely high dimen-
sional space. For the former case, it is typically not easy to use linear models such as
PCA to estimate the data distribution if there exists multiple data clusters. More-
over, many learning algorithms encounter the “curse of dimensionality” problem
ACM Journal Name, Vol. V, No. N, Month 20YY.
18 ·
Wang, W.,Guan, X., and Zhang, X. 2004. A novel intrusion detection method based on principal
component analysis in computer security. In Proceeding of the International Symposium on
Neural Networks.
Yang, B. 1995. Projection approximation subspace tracking. IEEE Transaction on Signal Pro-
cessing 43, 95–107.
ACM Journal Name, Vol. V, No. N, Month 20YY.
Periodic Step-Size Adaptation for
Single-Pass On-line Learning
Chun-Nan Hsu1,2, Yu-Ming Chang1, Han-Shen Huang1 and Yuh-Jye Lee3
1Institute of Information Science, Academia Sinica, Taipei 115, Taiwan
2USC/Information Sciences Institute, Marina del Rey, CA 90292, USA
3Department of Computer Science and Information Engineering,
National Taiwan University of Science and Technology, Taipei 106, Taiwan
chunnan@isi.edu
Abstract
It has been established that the second-order stochastic gradient descent (2SGD)
method can potentially achieve generalization performance as well as empirical
optimum in a single pass (i.e., epoch) through the training examples. However,
2SGD requires computing the inverse of the Hessian matrix of the loss function,
which is prohibitively expensive. This paper presents Periodic Step-size Adapta-
tion (PSA), which approximates the Jacobian matrix of the mapping function and
explores a linear relation between the Jacobian and Hessian to approximate the
Hessian periodically and achieve near-optimal results in experiments on a wide
variety of models and tasks.
1 Introduction
On-line learning has been studied for decades. Early works concentrate on minimizing the required
number of model corrections made by the algorithm through a single pass of training examples.
More recently, on-line learning is considered as a solution of large scale learning mainly because
of its fast convergence property. New on-line learning algorithms for large scale learning, such as
SMD [1] and EG [2], are designed to learn incrementally to achieve fast convergence. They usually
still require several passes (or epochs) through the training examples to converge at a satisfying
model. However, the real bottleneck of large scale learning is I/O time. Reading a large data set
from disk to memory usually takes much longer than CPU time spent in learning. Therefore, the
study of on-line learning should focus more on single-pass performance. That is, after processing
all available training examples once, the learned model should generalize as well as possible so
that used training example can really be removed from memory to minimize disk I/O time. In
natural learning, single-pass learning is also interesting because it allows for continual learning from
unlimited training examples under the constraint of limited storage, resembling a nature learner.
Previously, many authors, including [3] and [4], have established that given a sufficiently large set
of training examples, 2SGD can potentially achieve generalization performance as well as empirical
optimum in a single pass through the training examples. However, 2SGD requires computing the
inverse of the Hessian matrix of the loss function, which is prohibitively expensive. Many attempts
to approximate the Hessian have been made. For example, one may consider to modify L-BFGS [5]
for online settings. L-BFGS relies on line search. But in online settings, we only have the surface of
the loss function given one training example, as opposed to all in batch settings. The search direction
obtained by line search on such a surface rarely leads to empirical optimum. A review of similar
attempts can be found in Bottou’s tutorial [6], where he suggested that none is actually sufficient to
achieve theoretical single-pass performance in practice. This paper presents a new 2SGD method,
called Periodic Step-size Adaptation (PSA). PSA approximates the Jacobian matrix of the mapping
function and explores a linear relation between the Jacobian and Hessian to approximate the Hessian
1
Therefore, we can update the step size component-wise by
eig(H−1) = 휂푖
1− eig(J) ≈
휂푖
1− 훾푖 ⇒ 휂
(푡+1)
푖 ∝
휂
(푡)
푖
1− 훾(푡)푖
. (5)
Since the mapping ℳ in SGD involves the gradient g(w(푡);B(푡)) of a randomly selected training
example B(푡), ℳ is itself a random variable. It is unlikely that we can obtain a reliable eigenvalue
estimation at each single iteration. To increase stationary of the mapping, we take advantage of the
law of large numbers and aggregate consecutive SGD mappings into a new mapping
ℳ푏 =ℳ(ℳ(. . .ℳ(w) . . .))︸ ︷︷ ︸
푏
,
which reduces the variance of gradient estimation by 1
푏
, compared to the plain SGD mapping ℳ.
The approximation is valid because w(푡+푖), 푖 = 0, . . . , 푏 − 1 are approximately fixed when 휂 is
sufficiently small [7].
We can proceed to estimate the eigenvalues of ℳ푏 from w(푡), w(푡+푏) and w(푡+2푏) by applying (2)
for each component 푖:
훾¯푏푖 =
w
(푡+2푏)
푖 −w(푡+푏)푖
w
(푡+푏)
푖 −w(푡)푖
. (6)
We note that our aggregate mappingℳ푏 is different from a mapping that takes 푏 small batches as the
input in a single iteration. Their difference is similar to that between batch and stochastic gradient
descent. Aggregate mappings have 푏 chances to adjust its search direction, while mappings that use
푏 small batches together only have one.
With the estimated eigenvalues, we can present the complete update rule to adjust the step size
vector 휂. To ensure that the estimated values of eig(J) ∈ (−1, 1) and to ensure numerical stability,
we introduce a positive constant 휅 < 1 as the upper bound of ∣훾¯푏푖 ∣. Let u denote the constrained 훾¯푏.
Its components are given by
푢푖 := sgn(훾¯푏푖 )min(∣훾¯푏푖 ∣, 휅), ∀푖. (7)
Then we can update the step size every 2푏 iterations based on u by:
휂(푡+2푏+1) = v ∙ 휂(푡+2푏), (8)
where v is a discount factor with components defined by
푣푖 :=
푚+ 푢푖
푚+ 휅+ 푛
, ∀푖. (9)
The discount factor is derived from (5) and the fact that when 푢 < 1, 11−푢 > 푒푢 ≈ 1 + 푢 to ensure
numerical stability, with 푚 and 푛 controlling the range. Let 훼 be the maximum value and 훽 be the
minimum value of 푣푖. We can obtain 푚 and 푛 by solving 훽 ≤ 푣푖 ≤ 훼 for all 푖. Since −휅 ≤ 푢푖 ≤ 휅,
we have 푣푖 = 훼 when 푢푖 = 휅 and 푣푖 = 훽 when 푢푖 = −휅. Solving these equations yields:
푚 =
훼+ 훽
훼− 훽 휅 and 푛 =
2(1− 훼)
훼− 훽 휅. (10)
For example, if we want to set 훼 = 0.9999 and 훽 = 0.99, then 푚 and 푛 will be 201휅 and 0.0202휅,
respectively. Setting 0 < 훽 < 훼 ≤ 1 ensures that the step size is decreasing and approaches zero so
that SGD can be guaranteed to converge [7].
Algorithm 1 shows the PSA algorithm. In a nutshell, PSA applies SGD with a fixed step size
and periodically updates the step size by approximating Jacobian of the aggregated mapping. The
complexity per iteration is 푂(푑
푏
) because the cost of eigenvalue estimation given in (6) is 2푑 and it
is required for every 2푏 iterations. That is, PSA updates 휂 after learning from 2푏 ⋅ B examples.
3
When we have the least possible step size 휂(푡+1) = 훽휂(푡) for all 푡 mod 2푏 = 0 in PSA, the
expectation of w(푡) obtained by PSA can be shown to be:
퐸(w(푡)) = w∗ +
푡∏
푘=1
(
퐼 − 휂(0)훽⌊ 푘푏 ⌋H(w∗;D)
)
(w(0) −w∗)
= w∗ + S(푡)(w(0) −w∗).
The rate of convergence is governed by the largest eigenvalue of S(푡). We now derive a bound of
this eigenvalue.
Theorem 1 Let 휆ℎ be the least eigenvalue ofH(w∗;D). The asymptotic rate of convergence of PSA
is bounded by
eig(S(푡)) ≤ exp
{−휂(0)휆ℎ푏
1− 훽
}
.
Proof We can show that
eig(S(푡)) =
푡∏
푘=1
(
1− 휂(0)훽⌊ 푘푏 ⌋휆ℎ
)
≤ exp
{
−
푡∑
푘=1
휂(0)휆ℎ훽
⌊ 푘
푏
⌋
}
= exp
{
−휂(0)휆ℎ
푡∑
푘=1
훽⌊
푘
푏
⌋
}
because for any 0 ≤ 푎푗 < 1, since 1− 푎푗 ≤ 푒−푎푗 ,
0 ≤
푛∏
푗=1
(1− 푎푗) ≤
푛∏
푗=1
푒−푎푗 = 푒−
∑푛
푗=1 푎푗 .
Now, since
푡∑
푘=1
훽⌊
푘
푏
⌋ ≈
⎛
⎝⌊ 푡푏 ⌋∑
푙=0
푏훽푙
⎞
⎠ = 푏 ⌊
푡
푏
⌋∑
푙=0
훽푙 −→ 푏
1− 훽 when 푡→∞,
we have
eig(S(푡)) ≤ exp
{
−휂(0)휆ℎ
푡∑
푘=1
훽⌊
푘
푏
⌋
}
→ exp
{−휂(0)휆ℎ푏
1− 훽
}
when 푡→∞.
□
Though this analysis suggests that for rapid convergence to 휃∗, we should assign 훽 ≈ 1 with a
large 푏 and 휂(0), it is based on a worst-case scenario and thus insufficient as a practical guideline
for parameter assignment. In practice, we fix (훼, 훽, 휅) = (0.9999, 0.99, 0.9) and tune 푏 as follows.
When the training set size ∣D∣ ≫ 2000, set 푏 in the order of 0.5∣D∣/1000 is usually sufficient.
This setting implies that the step size will be adjusted per ∣D∣/1000 examples. In fact, when 푏
is in the same order, PSA performs similarly. Consider the following three settings: (푏, 훼, 훽) =
(10, 0.9999, 0.99), (100, 0.999, 0.9) or (1, 0.99999, 0.999). They all yield nearly identical single-
pass F-scores for the BaseNP task (see Section 4). The first setting was used in this paper. To see
why this is the case, consider the decreasing factor 푣푖 (see (8) and (9)), which will be confined within
the interval (훼, 훽). Assume that 푣푖 is selected at ransom uniformly, then the mean of 푣푖 = 0.995
when (훼, 훽) = (0.9999, 0.99) and 휂푖 will be decreased by a factor of 0.995 on average in each PSA
update. When 푏 = 10, PSA will update 휂푖 per 20 examples. After learning from 200 examples, PSA
will decrease 휂푖 10 times by a combined factor of 0.9511. Similarly, we can obtain that the factors
for the other two settings are 0.95 and 0.9512, respectively, nearly identical.
5
Base NP Chunking BioNLP/NLPBA BioCreative 2
Method (pass) time F-score time F-score time F-score time F-score
SGD (1) 1.15 92.42 13.04 92.26 12.23 66.37 3.18 34.33
SMD (1) 41.50 91.81 350.00 91.89 522.00 66.53 497.71 69.04
PSA (1) 16.30 93.31 160.00 93.16 206.00 69.41 191.61 80.79
L-BFGS (batch) 221.17 93.91 8694.40 93.78 20130.00 70.30 1601.50 86.82
Table 2: CPU time in seconds and F-scores achieved after a single pass of CRF training.
0 50 100 150 200
90
90.5
91
91.5
92
92.5
93
93.5
94
94.5
BaseNP
Time(sec)
F−
sc
or
e
 
 
PSA
SMD
SGD
L−BFGS
0 200 400 600 800 1000 1200
90
90.5
91
91.5
92
92.5
93
93.5
94
94.5
Chunking
Time(sec)
F−
sc
or
e
 
 
PSA
SMD
SGD
L−BFGS
0 100 200 300 400 500 600 700 800
20
30
40
50
60
70
80
NLPBA04
Time(sec)
F−
sc
or
e
 
 
PSA
SMD
SGD
L−BFGS
0 100 200 300 400 500
30
40
50
60
70
80
90
BioCreative 2 GM Task
Time(sec)
F−
sc
or
e
 
 
PSA
SMD
SGD
L−BFGS
Figure 1: Comparison of CPU time; Horizontal lines indicate target F-scores.
We selected L2-regularized logistic regression as the loss function for PSA and Liblinear because
it is twice differentiable. The weight 퐶 of the margin error term was set to one. We kept SvmSgd
intact. The experiment was run on an Open-SUSE Linux machine with Intel Xeon E7320 CPU
(2.13GHz) and 64GB RAM. Table 3 shows the results. Again, PSA achieves the best single-pass
accuracy for both tasks. Its test accuracies are very close to that of converged Liblinear. PSA takes
much less time than the other two solvers. PSA (1) is faster than SvmSgd (1) for SVM because
SvmSgd uses the sparsity trick [15], which speeds up training for sparse data, but otherwise may
slow down. Both data sets we used turn out to be dense, i.e., with no zero features. We implemented
PSA with the sparsity trick for CRF only but not for SVM and CNN.
LS FD LS OCR
Method (pass) accuracy time accuracy time
Liblinear converge 96.74 4648.49 76.06 4454.42
Liblinear (1) 91.43 290.58 74.33 398.00
SvmSgd (20) 93.78 1135.67 - -
SvmSgd (10) 93.77 567.68 73.71 473.35
SvmSgd (1) 93.60 56.78 73.76 46.96
PSA (1) 95.10 30.65 75.68 25.33
Table 3: Test accuracy rates and elapsed CPU time in seconds by various linear SVM solvers.
7
[2] Michael Collins, Amir Globerson, Terry Koo, Xavier Carreras, and Peter L. Bartlett. Expo-
nentiated gradient algorithms for conditional random fields and max-margin markov networks.
Journal of Machine Learning Research, 9:1775–1822, August 2008.
[3] Noboru Murata and Shun-Ichi Amari. Statistical analysis of learning dynamics. Signal Pro-
cessing, 74(1):3–28, April 1999.
[4] Le´on Bottou and Yann LeCun. On-line learning for very large data sets. Applied Stochastic
Models in Business and Industry, 21(2):137–151, 2005.
[5] Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, 1999.
[6] Le´on Bottou. The tradeoffs of large-scale learning. Tutorial, the 21st Annual Conference
on Neural Information Processing Systems (NIPS 2007), Vancouver, BC, Canada, December
2007. http://leon.bottou.org/talks/largescale.
[7] Albert Benveniste, Michel Metivier, and Pierre Priouret. Adaptive Algorithms and Stochastic
Approximations. Springer-Verlag, 1990.
[8] Chun-Nan Hsu, Han-Shen Huang, and Bo-Hou Yang. Global and componentwise extrapola-
tion for accelerating data mining from large incomplete data sets with the EM algorithm. In
Proceedings of the Sixth IEEE International Conference on Data Mining (ICDM’06), pages
265–274, Hong Kong, China, December 2006.
[9] Han-Shen Huang, Bo-Hou Yang, Yu-Ming Chang, and Chun-Nan Hsu. Global and componen-
twise extrapolations for accelerating training of Bayesian networks and conditional random
fields. Data Mining and Knowledge Discovery, 19(1):58–91, 2009.
[10] Fei Sha and Fernando Pereira. Shallow parsing with conditional random fields. In Proceedings
of Human Language Technology, the North American Chapter of the Association for Compu-
tational Linguistics (NAACL’03), pages 213–220, 2003.
[11] Taku Kudo. CRF++: Yet another CRF toolkit, 2006. Available under LGPL from the following
URL: http://crfpp.sourceforge.net/.
[12] Burr Settles. Biomedical named entity recognition using conditional random fields and novel
feature sets. In Proceedings of the Joint Workshop on Natural Language Processing in
Biomedicine and its Applications (JNLPBA-2004), pages 104–107, 2004.
[13] Cheng-Ju Kuo, Yu-Ming Chang, Han-Shen Huang, Kuan-Ting Lin, Bo-Hou Yang, Yu-Shi
Lin, Chun-Nan Hsu, and I-Fang Chung. Rich feature set, unification of bidirectional parsing
and dictionary filtering for high f-score gene mention tagging. In Proceedings of the Second
BioCreative Challenge Evaluation Workshop, pages 105–107, 2007.
[14] Yann LeCun and Corinna Cortes. The MNIST database of handwritten digits, 1998.
http://yann.lecun.com/exdb/mnist/.
[15] Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal Estimated sub-
GrAdient SOlver for SVM. In ICML’07: Proceedings of the 24th international conference on
Machine learning, pages 807–814, New York, NY, USA, 2007. ACM Press.
[16] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines, 2001.
Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm.
[17] Thorsten Joachims. Training linear SVMs in linear time. In Proceedings of the 12th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’06),
pages 217–226, New York, NY, USA, 2006. ACM.
[18] Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
[19] Yann LeCun, Leon Bottou, Genevieve B. Orr, and Klaus-Robert Muller. Efficient backprop.
In G. Orr and Muller K., editors, Neural Networks: Tricks of the trade. Springer, 1998.
[20] Nicolas LeRoux, Pierre-Antoine Manzagol, and Yoshua Bengio. Topmoumoute online natural
gradient algorithm. In Advances in Neural Information Processing Systems, 20 (NIPS 2007),
Cambridge, MA, USA, 2008. MIT Press.
[21] Chun-Nan Hsu, Yu-Ming Chang, Han-Shen Huang, and Yuh-Jye Lee. Periodic step-size adap-
tation in second-order gradient descent for single-pass on-line structured learning. To appear in
Mchine Learning, Special Issue on Structured Prediction. DOI: 10.1007/s10994-009-5142-6,
2009.
9
98年度專題研究計畫研究成果彙整表 
計畫主持人：李育杰 計畫編號：98-2221-E-011-104- 
計畫名稱：異常偵測演算法之研究及其應用 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 0% 無 
研究報告/技術報告 0 0 0% 無 
研討會論文 0 0 0% 
篇 
無 論文著作 
專書 0 0 0%  無 
申請中件數 0 0 0% 無 專利 已獲得件數 0 0 0% 件 無 
件數 0 0 0% 件 無 
技術移轉 
權利金 0 0 0% 千元 無 
碩士生 0 0 0% 無 
博士生 0 0 0% 無 
博士後研究員 0 0 0% 無 
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 0% 
人次 
無 
期刊論文 0 1 100% 無 
研究報告/技術報告 0 0 0% 無 
研討會論文 1 0 50% 
篇 
 
論文著作 
專書 1 0 20% 章/本  
申請中件數 0 0 0% 無 專利 已獲得件數 0 0 0% 件 無 
件數 0 0 0% 件 無 
技術移轉 
權利金 0 0 0% 千元 無 
碩士生 3 0 80%  
博士生 1 0 30%  
博士後研究員 0 0 0% 無 
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 0% 
人次 
無 
 
