‐2‐ 
 
[4] Li-Wei Kang, Chih-Yang Lin, Jyh-Da Wei, Yu-Siang Chen, Hui-Chun Cheng, Yui-Cheng Lin, 
Ming-Hong Shen, and Chia-Hung Yeh, “Secure trans-sensing for compressively sensed 
multimedia signals over cloud environment,” submitted to National Symposium on 
Telecommunications, Hualien, Taiwan, Nov. 2011 (NST2011). 
[5] Li-Wei Kang, Chia-Wen Lin, and Yu-Hsiang Fu, “Automatic single-image-based rain streaks 
removal via image decomposition,” IEEE Trans. on Image Processing (on revision, Paper No. 
TIP-07387-2011R1, July 2011). 
[6] Li-Wei Kang, Chun-Shien Lu, and Chih-Yang Lin, “Low-complexity video coding via 
power-rate-distortion optimization,” Journal of Visual Communication and Image Representation 
(on revision, Paper No. JVCI-11-63R1, July 2011). 
[7] Yu-Hsiang Fu, Li-Wei Kang, Chia-Wen Lin, and Chiou-Ting Hsu, “Single-frame-based rain 
removal via image decomposition,” in Proc. of IEEE Int. Conf. on Acoustics, Speech, and Signal 
Processing, Prague, Czech Republic, May 2011 (ICASSP2011). 
[8] Chun-Yu Lin, Chih-Chung Hsu, Chia-Wen Lin, and Li-Wei Kang, “Fast deconvolution-based 
image super-resolution using gradient prior,” accepted and to appear in Proc. of IEEE Visual 
Communication and Image Processing Conf., special session on Advanced Techniques for 
Content-Based Image/Video Resizing, Tainan, Taiwan, Nov. 2011 (invited paper, VCIP2011). 
[9] Hung-Wei Chen, Li-Wei Kang, and Chun-Shien Lu, “Single-pixel camera compatible 
distributed compressive video sensing via dictionary learning,” in Proc. of IPPR Computer 
Vision, Graphics, and Image Processing Conf., Kaohsiung, Taiwan, Aug. 2010 (CVGIP2010). 
[10] Li-Wei Kang, Chao-Yung Hsu, Hung-Wei Chen, and Chun-Shien Lu, “Image copy detection and 
recognition via secure SIFT-based sparse representation,” in Proc. of IPPR Computer Vision, 
Graphics, and Image Processing Conf., Kaohsiung, Taiwan, Aug. 2010 (CVGIP2010). 
[11] Li-Wei Kang, Chia-Wen Lin, and Yu-Hsiang Fu, “Single-image-based rain removal via image 
decomposition,” accepted and to appear in Proc. of IPPR Computer Vision, Graphics, and Image 
Processing Conf., Chiayi, Taiwan, Aug. 2011 (CVGIP2011). 
[12] Chih-Yang Lin, C. C. Chang, W. W. Chang, M. H. Chen, and Li-Wei Kang, “Real-time robust 
background modeling based on joint color and texture descriptions,” in Proc. of IEEE Int. Conf. 
on Genetic and Evolutionary Computing, Shenzhen, China, Dec. 2010 (invited paper, 
ICGEC2010). 
[13] Kahlil Muchtar, Chih-Yang Lin, Li-Wei Kang, and Chia-Hung Yeh, “Hierarchical tree structure 
background construction method for motion detection,” accepted and to appear in Proc. of 
APSIPA Annual Summit and Conf., special session on Image/Video Processing and Analysis, 
Xi'an, China, Oct. 2011 (invited paper, APSIPA ASC 2011). 
[14] Chih-Yang Lin, Li-Wei Kang, Chun-Shien Lu, Jau-Hong Kao, and Yi-Ta Wu, “Multi-camera 
invariant appearance modeling for non-rigid object identification in a real-time environment,” 
submitted to Journal of Visual Communication and Image Representation, May 2011 (Paper No. 
JVCI-11-116). 
[15] Chih-Yang Lin, Chi-Shiang Chan, and Li-Wei Kang, “Left-object detection through background 
modeling,” submitted to Information Sciences, Apr. 2011 (Paper No. INS-D-11-596). 
 
DICTIONARY LEARNING-BASED DISTRIBUTED COMPRESSIVE VIDEO SENSING+
Hung-Wei Chen, Li-Wei Kang, and Chun-Shien Lu*
Institute of Information Science, Academia Sinica, Taipei, Taiwan 
E-mail: {hungwei, lwkang, lcs}@iis.sinica.edu.tw 
ABSTRACT 
We address an important issue of fully low-cost and low-complex 
video compression for use in resource-extremely limited 
sensors/devices. Conventional motion estimation-based video 
compression or distributed video coding (DVC) techniques all rely 
on the high-cost mechanism, namely, sensing/sampling and 
compression are disjointedly performed, resulting in unnecessary 
consumption of resources. That is, most acquired raw video data 
will be discarded in the (possibly) complex compression stage. In 
this paper, we propose a dictionary learning-based distributed 
compressive video sensing (DCVS) framework to “directly” 
acquire compressed video data. Embedded in the compressive 
sensing (CS)-based single-pixel camera architecture, DCVS can 
compressively sense each video frame in a distributed manner. At 
DCVS decoder, video reconstruction can be formulated as an l1-
minimization problem via solving the sparse coefficients with 
respect to some basis functions. We investigate adaptive 
dictionary/basis learning for each frame based on the training 
samples extracted from previous reconstructed neighboring frames 
and argue that much better basis can be obtained to represent the 
frame, compared to fixed basis-based representation and recent 
popular “CS-based DVC” approaches without relying on 
dictionary learning. 
Index Terms—Compressive sensing, sparse representation, 
dictionary learning, single-pixel camera, l1-minimization.
1. INTRODUCTION 
Conventional high-complexity video compression techniques [1] 
or recently popular low-complexity technique called distributed 
video coding (DVC) [2] all rely on the high-cost mechanism where 
video sensing and compression tasks are disjointedly performed. 
Most acquired raw pixel data in the sensing stage will be discarded 
in the (possibly) complex compression stage, which suffers from 
unnecessary memory wasting and power consumption, and is 
especially unfeasible for resource-extremely limited devices/ 
sensors. Recently, with the advent of the compressive sensing 
(CS)-based single-pixel camera architecture [3], based on the 
inherent sparse property of images, CS [4] can directly and 
efficiently acquire compressed image data via randomly projecting 
raw data to obtain linear and non-adaptive measurements. Image 
reconstruction can be formulated as solving an l1-minimization 
problem [5]-[6] based on the acquired data measurements. 
Recently, compressive video sensing integrating both video 
sensing and compression into a unified task has emerged as a new 
way to directly acquire compressed video data via random 
projection for each individual frame at a low-complexity encoder. 
Video reconstruction can be achieved via performing l1-
minimization together with exploiting correlations among 
successive frames at a high-complexity decoder [7]-[9]. In [7], we 
have proposed a distributed compressive video sensing (DCVS) 
framework, where an efficient initialization and several stopping 
criteria were designed to improve and speedup the employed l1-
minimization algorithm for video reconstruction. In [8]-[9], DVC 
algorithms using CS were proposed, where the major core is to 
assume each block in a frame can be sparsely represented with 
respect to the dictionary/basis formed from a set of spatially local 
neighboring blocks (without performing dictionary learning) of 
previous reconstructed neighboring frames, denoted as the “W/O 
dictionary learning”-based scheme in this paper. 
In this paper, a DCVS framework via “dictionary learning”-
based sparse representation is proposed. Our major contributions 
include: (i) Single-pixel camera-compatible low-complexity 
video encoder: only CS random projection will be individually 
performed for each frame, which can be fully compatible to the 
single-pixel camera [3]. In [8]-[9], it is required to support the 
H.264/AVC encoder to periodically encode each intra-frame, 
which is more complex. (ii) Dictionary-learning based sparse 
representation: a dictionary learned from a set of blocks globally 
extracted from the previous reconstructed neighboring frames 
together with the side information generated from them is used as 
the basis of each block in a frame. The major advantages are: (a) 
Extracting more blocks globally for dictionary learning can 
provide much better representation for blocks with large motions; 
and (b) Even if the qualities of the training blocks are not good 
enough (due to poorly reconstructed neighboring frames), the 
learned dictionary may still provide a good basis. The fact can be 
similarly explained by the image denoising approach via the 
dictionary learned from the patches extracted from a noisy image 
itself [10]. In contrast, the “W/O dictionary learning” approach [8]-
[9] may not work well for: (a) blocks with (very) large motions; 
and (b) the use of non-learned dictionary formed from (possibly) 
low-quality blocks. Other technical comparisons can be found in 
Table 1 of Sec. 4. 
2. COMPRESSIVE SENSING 
Assume that an orthonormal basis matrix (or dictionary)  
NN×∈ RȌ  (e.g., DWT basis) can provide a K sparse 
representation for a signal 1R ×∈ Nx , i.e., x = Ȍș, where 
1Rș ×∈ N  can be well approximated using only K << N non-zero 
entries. Compressive sensing (CS) [4] states that x can be 
accurately reconstructed by taking only M = O(K×log(N/K)), K < 
M << N, linear and non-adaptive measurements from the random 
projection as y = Ɏx, where 1R ×∈ My  is a measurement vector 
and NM ×∈ Rĭ  is a measurement matrix that is incoherent with Ȍ.
More specifically, the M measurements in y are random linear 
combinations of the entries of x, which can be viewed as the 
compressed version of x. The reconstruction of ș (or x) can be 
formulated as an l1-minimization problem. On the other hand, a 
basis matrix is actually not necessarily orthonormal. An 
overcomplete dictionary D learned from training some selected 
training samples [10] can be used as a basis for representing the 
original signal. In fact, by using a measurement matrix Ɏ randomly 
______________________________________________________________________________________________________________________________ 
+This work was supported in part by the National Science Council, Taiwan, under Grants NSC97-2628-E-001-011-MY3, NSC98-2631-H-001-013, NSC98-
2811-E-001-008, NSC99-2218-E-001-010, and NSC99-2811-E-001-006.  
*Corresponding author: lcs@iis.sinica.edu.tw. 
28th Picture Coding Symposium, PCS2010, December 8-10, 2010, Nagoya, Japan
978-1-4244-7135-5/10/$26.00 ©2010 IEEE - 210 -
decoder, where the parameter settings are described in Sec. 4. Fig. 
2(a) and (b) show, respectively, an original CS frame (the 32nd 
frame), and its dictionary with size 256×256, where each atom 
(column vector) with length 256 in the dictionary is displayed as a 
block. Fig. 2(c) and (d), respectively, show the reconstructed CS 
frame using the dictionary shown in Fig. 2(b) and the frame-based 
DWT basis (treat this frame as a key frame). It can be observed 
from Fig. 2 that using the learned dictionary can provide better CS 
frame reconstruction than using the DWT basis at the same MR.
(a)                           (b) 
(c)                                            (d) 
Fig. 2. Comparison of reconstructed CS frames with respective 
to learned and fixed dictionaries: (a) The original 32nd frame; 
(b) the dictionary learned for (a); (c) the reconstructed 32nd 
frame with respective to the dictionary shown in (b) (PSNR = 
31.49dB); and (d) the reconstructed 32nd frame with respect to 
the frame-based DWT basis (PSNR=27.83dB). 
4. SIMULATION RESULTS 
In this paper, several QCIF (frame size: 176×144) video sequences 
(51 Y frames for each) with GOP size = 2, and different 
measurement rates (MRs) were employed to evaluate the proposed 
DVCS scheme. For learning the dictionary for each CS frame 
consisting of several non-overlapping 16×16 blocks, the parameter 
settings are described as follows. The dictionary size was set to 
256×256, i.e., Nb = 16×16 = 256 and P = 256 (atoms). In K-SVD 
[10], the number of training iterations was set to 10 while the target 
sparsity, denoted by S (number of nonzero coefficients used to 
represent each signal/block) was set to 10. According to our 
simulations, the performances will not exhibit significant changes 
when the two above-mentioned parameters for K-SVD are 
increased, which will increase the complexity of dictionary 
learning. Currently, the MR of all the frames in a video sequence 
are set to be the same as the target MR. In addition, to keep the 
encoding complexity to be as low as possible, the available 
measurements for each CS frame are equally allocated to each 
block without considering the complexity or sparsity of the block. 
In this paper, two compressive video sensing schemes were 
used for comparison with our dictionary learning-based DCVS 
scheme (denoted by Proposed). The first one is the Frame-DWT
scheme, in which under our DCVS architecture, all frames are 
treated as key frame (reconstructed with respect to the frame-based 
DWT basis). The second one is the “W/O dictionary learning” 
scheme, in which based on our DCVS architecture, each block in a 
CS frame is reconstructed with respect to its corresponding 
dictionary without learning. The second type is similar to the major 
core in [8]-[9]. Here, based on [8], the dictionary of each block in a 
CS frame includes the blocks extracted from the two spatially 
corresponding square 17×17 windows, respectively, in the two 
neighboring reconstructed key frames. The characteristics of the 
“Proposed” and the “W/O dictionary learning” schemes are 
summarized in Table 1. Please note that we only implemented the 
major core of the schemes proposed in [8]-[9] instead of the full 
system for comparison. 
For reconstructing block bti in a CS frame xt using SpaRSA, the 
computational complexity is approximately O(Pȕ), where P is 
decided by the dimension of PMt tiA
×
∈ R , and ȕ is a constant. It 
has been shown that the complexity of SpaRSA is approximately 
linear (ȕ is close to 1) [6]. In our parameter settings (Table 1), the 
dimension P (256) used by Proposed scheme is smaller than that 
(578) used by “W/O dictionary learning” scheme. Nevertheless, 
additional complexity for performing K-SVD dictionary learning 
[10] (approximately Q×(S2×P + 2×Nb×P) per training iteration [11], 
where Q is the number of training patches, S is the target sparsity, 
and Nb×P is the size of each dictionary Dt) is required for each CS 
frame in our scheme, which is, however, usually acceptable for a 
high-complexity decoder supported in a server or in cloud. 
Table 1. Comparisons of the Proposed and “W/O dictionary 
learning” schemes. 
Scheme Proposed W/O dictionary learning 
Ingredients 
of dictionary
Learning based on 
the extracted patches 
from neighboring key 
frames and side 
information 
Spatially neighboring blocks 
from neighboring key frames 
without learning 
Dictionary 
size 256 atoms 
Size of spatially 
corresponding square 
window × Number of 
neighboring key frames 
(17×17×2 = 578 atoms) 
Number of 
dictionaries 
per CS 
frame 
1
Number of blocks per CS 
frame 
(99 dictionaries for a QCIF 
CS frame) 
Dictionary 
type Global with learning Local w/o learning 
Decoding 
complexity 
per CS 
frame
Dictionary learning 
by K-SVD + l1-
minimization solving 
256 coefficients per 
block 
l1-minimization solving 578 
coefficients per block 
The average PSNR performances at different MRs for the 
Foreman, Mobile, and Silent sequences are shown in Figs. 3(a), 
4(a), and 5(a), respectively, where it can be observed that the 
PSNR performances of the proposed DCVS can outperform or be 
comparable to the Frame-DWT and “W/O dictionary learning” 
schemes [8]-[9], especially at lower MRs and for sequences with 
large motion. It can also be observed from Fig. 4(a) that the PSNR 
performances obtained from the three schemes are somewhat poor. 
The major reasons include: (i) the frame contents of the Mobile
sequence are very complex, which may not be exactly sparse with 
respect to most bases, and (ii) the motions of the sequence are very 
large so that it is hard to learn a good dictionary for a CS frame 
from its neighboring key frames. It is worth noting that the 
dictionary learning of our DCVS can reveal some “denoising” 
- 212 -
  
 
 
附件二 
 
Li-Wei Kang, Chih-Yang Lin, Hung-Wei Chen, Chia-Mu Yu, Chun-Shien Lu, Chao-Yung 
Hsu, and Soo-Chang Pei, “Secure transcoding for compressive multimedia sensing,” in Proc. 
of IEEE Int. Conf. on Image Processing, Brussels, Belgium, Sept. 2011, pp. 933-936 
(ICIP2011). 
addressed in Sec. 2.2. In fact, by using a measurement matrix Ф 
randomly generated from some distribution, the incoherence 
between Ф and D should be usually high enough. 
 
2.2. Sparse Representation 
Given an overcomplete dictionary   PNPpp   RdD ,1,2, ,  
N < P, containing P prototype atoms 1Rd  Np , to find the 
sparse representation for a compressible signal xRN1 as a 
sparsely linear combination of these atoms to meet ||x  D||2  , 
where RP1 is the sparse representation coefficients of x and ε ≥ 
0 is an error tolerance, can be formulated as [10]: 
        0α αminargα
~   subject to 
2
Dα εx   ,       (1)
where ||α||0 counts the number of nonzero coefficients of α. The 
dimension of α is larger than that of x (P > N). Nevertheless, α is 
sparse and usually ||α||0 << N. Solving Eq. (1) can also be 
converted into a convex optimization problem [4], [10]. By 
combining CS and sparse representation, a sparse or compressible 
signal xRN1 being simultaneously compressed and encrypted as 
y = Фx can be further expressed as y = Фx = ФDα = Aα, where A 
= ФD and ARMP. The reconstruction of x can be formulated as a 
convex optimization problem as: 
1
2
2α αAα2
1minargα~  y , (2)
and αD~x~  , where ФRMN is a measurement matrix and τ is a 
non-negative parameter. 
 
3. PROPOSED SECURE TRANSCODING FOR CS 
 
In this section, we present the proposed secure transcoding scheme 
for compressive multimedia sensing. For simplicity, we will 
demonstrate the proposed scheme using image signals as an 
example, which can be naturally extended to video or other 
multimedia signals acquired or compressed via CS techniques. It 
should be noted that the proposed transcoding scheme is especially 
designed for CS-based multimedia compression paradigm, which 
is significantly different from those designed for traditional 
multimedia compression techniques (e.g., JPEG-2000 or 
H.264/AVC). As shown in Fig. 1, a sender acquires an image via 
CS with a certain number of measurements and transmits the 
measurement vector (compressed image data) to a transcoder 
which will securely transcode the received data into L 
measurement vectors of different number of measurements without 
reconstructing the original image, and transmit these vectors to the 
L legal receivers accordingly. 
 
3.1. Methodology 
At the sender, an image viewed as a column vector xRN1 can be 
jointly compressed and encrypted via CS using the measurement 
matrix ФRMN (controlled by the secret key S) to get its 
measurement vector yRM1, M < N, which is transmitted to the 
transcoder. The number of measurements (or the length of y) will 
decide the ratio for compressing x, which should be converted for 
fitting different requirements of multiple receivers. Here, the used 
measurement matrix Ф is the scrambled block Hadamard ensemble 
(SBHE) matrix [11], which takes the partial block Hadamard 
transform, followed by randomly permuting its columns. 
It is assumed that the transcoder can store the (L + 1) different 
matrices, ARMP and AiRMiP, i = 1, 2, …, L, where A = ФD, Ai 
= ФiD, ФiRMiN is generated using the same secret key S, 
DRNP is an overcomplete dictionary for sparsely representing x 
= Dα, and RP1 is the sparse coefficients of x, M, Mi < N < P, M 
≠ Mi, Mi ≠ Mj for i ≠ j. Here, we apply the K-SVD algorithm [10] 
to learn the dictionary D using several selected training images. It 
should be noted that the transcoder can know only A and Ai 
without knowing S (or Ф and Фi) and D. That is, it is hard for the 
transcoder to correctly decompose A into Ф and D (or decompose 
Ai into Фi and D), which will be discussed in Sec. 3.2. 
Because the decompression and decryption of a 
compressively sensed image will be jointly accomplished, for 
safety purpose, we cannot perform image reconstruction followed 
by compressively re-sensing at the transcoder. On the contrary, we 
propose to perform partial image reconstruction in a secure 
transform domain followed by re-sensing image in this domain 
with target number of measurements. The proposed secure 
transcoding scheme is illustrated in Fig. 1. In the following, we 
will formulate the three problems we want to solve and present 
corresponding solutions. 
 
Secure Transcoder
 PNNMPM DΦA  
  PNNM1PM1 DΦ)(A 11  
  PNNM2PM2 DΦ)(A 22  
  PNNMPM DΦ)(A   LL LL
1NNM1M Φ   xy
  1PPM1M αA  y
      1PPM11M1 αA 11  y      1PPM21M2 αA 22  y
      1PPM1M αA   LL LLy
        1P1PN1NM11M1 1111 βD  y

Stored inSender

Securely and 
Compressively 
Re-sensing x 
Securely Transform y to 
α without recovering x
        1P2PN2NM21M2 2222 βD  y

        1PPNNM1M βD   LLLL LLLLy
Receivers 1, 2, …, L
Attack point (i) Attack point (ii) Attack point (iii)

Attack point (iv)
Low bandwidth
Middle bandwidth
High bandwidth
 
Fig. 1. Proposed secure transcoding scheme for CS. 
 
The first problem, which is secure transform to be solved at the 
transcoder, can be converted to solve y = Aα and formulated as a 
convex optimization problem shown in Eq. (2). That is, the 
transcoder will transform a received measurement vector y into its 
secure coefficient domain α for further processing. Without 
knowing the dictionary D, the transcoder cannot reconstruct x via 
αD~x~  . In addition, without knowing the measurement matrix Ф, 
the transcoder cannot reconstruct x. Hence, the solution α~  of Eq. 
(2) can be viewed as the secure sparse representation of x. Here, 
we apply the “sparse reconstruction by separable approximation 
(SpaRSA)” algorithm [8] to solve the convex optimization 
problem due to its superior efficiency. 
 
The second problem, which is secure re-sensing to be solved at 
the transcoder, can be formulated as: 
α~Aiiy  , (3)
which can be implicitly further expressed as  α~Aiiy  
xii
~α~D  , where α~  is the solution of the above-mentioned 
first problem. Eq. (3) implies to compressively re-sense the 
2011 18th IEEE International Conference on Image Processing934
The dictionary of a CS frame (intra-encoded and inter-decoded 
frame) was learned using K-SVD with the training samples 
extracted from the neighboring reconstructed key frames. The 
sizes (number of atoms) of the dictionaries used in the transcoder 
and the i-th receiver, i = 1, 2, 3, were 1024, 512, 1024, and 2048, 
respectively. The size of dictionary will influence the 
reconstruction complexity solving the convex optimization 
problem. It is assumed that both the computational capabilities and 
receiver bandwidths for the 1st, 2nd, and 3rd receivers were from 
low to high, as illustrated in Fig. 1. 
Three baseline approaches used for comparisons include: (a) 
the sender compresses multiple data versions itself and directly 
transmit them to respective receivers without relying on 
transcoding (denoted by “W/O transcoding”), where the overhead 
of the sender is very heavy; (b) the transcoder reconstructs the 
original data and re-sense them to multiple versions for respective 
receivers (denoted by “W/O security”), which is insecure; and (c) 
based on the inherent robustness for measurement loss of CS, the 
transcoder randomly drops measurements to meet the desired 
number of measurements of each receiver except for the 3rd 
receiver (denoted by “random drop”). It should be noted that the 
proposed scheme is designed especially for CS-based compression 
techniques, which is unsuitable for comparisons with existing 
approaches for traditional compression techniques (e.g., JPEG-
2000 or H.264/AVC) [1]-[3]. 
The reconstruction performances (PSNR in dB) at the three 
receivers of the evaluated four approaches are shown in Tables 1-4. 
It can be observed that compared with the three approaches for 
comparisons, the proposed scheme can keep the three advantages: 
(a) secure transcoding; (b) the sender needs to send data only once; 
and (c) the reconstruction performances are comparable. The 
“W/O transcoding” approach directly transmitting the data to the 
respective receivers reveals the upper bounds of reconstruction 
performances. Nevertheless, when the number of receivers greatly 
increases, the overhead of the sender in this approach is 
unacceptable. The “W/O security” approach and the proposed 
scheme compressively re-sense the received data from the 
reconstructed pixel data and the transformed secure coefficients, 
respectively. Basically, the two approaches, respectively, perform 
insecure and secure re-sensing tasks, and exhibit very similar 
reconstruction performances. The performances of the “random 
drop” approach significantly suffer from severe measurement 
dropping even if CS is robust to slight measurement loss. 
Moreover, for the 3rd receiver with higher bandwidth, which can 
receive more measurements (60%) than those (50%) sent from the 
sender, the “W/O security”, “random drop,” and proposed 
approaches cannot satisfy this receiver because the performances 
have been bounded by the transcoder only receiving 50% (M/N = 
50%) of measurements from the sender. 
 
5. CONCLUSIONS 
 
In this paper, we have proposed a secure transcoding scheme for 
compressive multimedia sensing. The feasibility of our scheme has 
been verified via simulation results and security analysis. For 
future researches, we will provide the formal proof of the security 
analysis for the attack point (ii) as well as possible fingerprinting 
techniques for the attack point (iv) mentioned in Sec. 3.2. In 
addition, we will also investigate super-resolution techniques 
integrated with our scheme for further performance enhancement. 
 
 
Table 1. Reconstruction performances for the Lena image. 
PSNR (dB) 1st receiver 2nd receiver 3rd receiver
W/O transcoding 27.09 32.06 37.16 
W/O security 27.06 31.86 34.21 
Random drop 16.55 19.15 34.28 
Proposed 27.07 31.75 34.27 
 
Table 2. Reconstruction performances for the Pepper image. 
PSNR (dB) 1st receiver 2nd receiver 3rd receiver
W/O transcoding 26.36 30.01 35.94 
W/O security 26.33 29.87 33.46 
Random drop 15.56 17.97 33.48 
Proposed 26.35 29.82 33.43 
 
Table 3. Reconstruction performances for the Forman sequence. 
PSNR (dB) 1st receiver 2nd receiver 3rd receiver
W/O transcoding 23.88 25.56 27.56 
W/O security 22.77 24.01 26.50 
Random drop 12.56 14.91 26.59 
Proposed 22.77 23.95 26.56 
 
Table 4. Reconstruction performances for the Coastguard 
sequence. 
PSNR (dB) 1st receiver 2nd receiver 3rd receiver
W/O transcoding 25.67 26.79 28.67 
W/O security 24.35 25.18 27.61 
Random drop 14.46 15.67 27.69 
Proposed 24.29 25.17 27.67 
 
6. REFERENCES 
[1] J. Xin, C. W. Lin, and M. T. Sun, “Digital video transcoding,” 
Proceedings of the IEEE vol. 93, no. 1, pp. 84-97, 2005. 
[2] J. Apostolopoulos and S. Wee, “Secure media streaming and secure 
transcoding,” chapter in Multimedia Security Technologies for Digital 
Rights Management, edited by Zeng, Yu, and Lin, Elsevier, July 2006. 
[3] N. Thomas, D. Redmill, and D. Bull, “Secure transcoders for single 
layer video data,” Signal Processing: Image Communication, vol. 25, 
no. 3, pp. 196-207, Mar. 2010. 
[4] E. J. Candes and M. B. Wakin, “An introduction to compressive 
sampling,” IEEE Signal Processing Magazine, vol. 25, no. 2, 2008. 
[5] M. F. Duarte, et al., ”Single-pixel imaging via compressive 
sampling,” IEEE Signal Processing Magazine, vol. 25, no. 2, 2008. 
[6] J. Romberg, “Imaging via compressive sampling,” IEEE Signal 
Processing Magazine, vol. 25, no. 2, pp. 14-20, 2008. 
[7] H. W. Chen, L. W. Kang, and C. S. Lu, “Dictionary learning-based 
distributed compressive video sensing,” in Proc. of Picture Coding 
Symposium, Nagoya, Japan, 2010. 
[8] S. J. Wright, R. D. Nowak, and M. A. T. Figueiredo, “Sparse 
reconstruction by separable approximation,” IEEE Trans. on Signal 
Processing, vol. 57, no. 7, pp. 2479-2493, 2009. 
[9] Y. Rachlin and D. Baron, “The secrecy of compressed sensing 
measurements,” in Proc. of Allerton Conf. on Communication, 
Control, and Computing, 2008. 
[10] M. Aharon, M. Elad, and A. M. Bruckstein, “The K-SVD: an 
algorithm for designing of overcomplete dictionaries for sparse 
representation,” IEEE Trans. on Signal Processing, vol. 54, 2006. 
[11] L. Gan, T. T. Do, and T. D. Tran, “Fast compressive imaging using 
scrambled Hadamard ensemble,” in Proc. of EUSIPCO, 2008. 
[12] R. Blom, “An optimal class of symmetric key generation systems,” in 
Proc. of the EUROCRYPT’84 workshop on Advances in cryptology: 
theory and application of cryptographic techniques, 1984. 
[13] C. M. Yu, C. S. Lu, and S. Y. Kuo, “Non-interactive pairwise key 
establishment for sensor networks,” IEEE Trans. on Information 
Forensics and Security, vol. 5, no. 3, pp. 556-569, 2010. 
[14] J. Yang, J. Wright, T. Huang, and Y. Ma, “Image super-resolution via 
sparse representation,” IEEE Trans. on Image Processing, Nov., 2010. 
2011 18th IEEE International Conference on Image Processing936
Copyright (c) 2011 IEEE. Personal use is permitted. For any other purposes, Permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.
IEEE TRANSACTIONS ON MULTIMEDIA, 2011. 
 
1
 
Abstract—Assessment of image similarity is fundamentally 
important to numerous multimedia applications. The goal of 
similarity assessment is to automatically assess the similarities 
among images in a perceptually consistent manner. In this paper, 
we interpret the image similarity assessment problem as an 
information fidelity problem. More specifically, we propose a 
feature-based approach to quantify the information that is 
present in a reference image and how much of this information 
can be extracted from a test image to assess the similarity between 
the two images. Here, we extract the feature points and their 
descriptors from an image, followed by learning the 
dictionary/basis for the descriptors in order to interpret the 
information present in this image. Then, we formulate the 
problem of the image similarity assessment in terms of sparse 
representation. To evaluate the applicability of the proposed 
feature-based sparse representation for image similarity 
assessment (FSRISA) technique, we apply FSRISA to three 
popular applications, namely, image copy detection, retrieval, and 
recognition by properly formulating them to sparse 
representation problems. Promising results have been obtained 
through simulations conducted on several public datasets, 
including the Stirmark benchmark, Corel-1000, COIL-20, 
COIL-100, and Caltech-101 datasets. 
 
Index Terms—image similarity assessment, sparse 
representation, feature detection, image copy detection, image 
retrieval, image recognition. 
 
 
This work was supported in part by the National Science Council, Taiwan, 
under Grants NSC97-2628-E-001-011-MY3, NSC98-2631-H-001-013, 
NSC98-2811-E-001-008, NSC99-2218-E-001-010, NSC99-2811-E-001-006, 
and NSC 99-2221-E-468-023. A preliminary version of this manuscript was 
presented in the 2010 IEEE International Conference on Multimedia and Expo 
[8]. 
Li-Wei Kang is with the Institute of Information Science, Academia Sinica, 
Taipei, Taiwan. 
Chao-Yung Hsu is with the Institute of Information Science, Academia 
Sinica and Graduate Institute of Communication Engineering, National Taiwan 
University, Taipei, Taiwan. 
Hung-Wei Chen is with the Institute of Information Science, Academia 
Sinica and Graduate Institute of Communication Engineering, National Taiwan 
University, Taipei, Taiwan. 
Chun-Shien Lu is with the Institute of Information Science, Academia 
Sinica, Taipei, Taiwan (phone: 886-2-2788-3799 ext. 1513; fax: 
886-2-2782-4814; e-mail: lcs@iis.sinica.edu.tw). 
Chih-Yang Lin is with the Department of Computer Science and 
Information Engineering, Asia University, Taichung, Taiwan. 
Soo-Chang Pei is with the Graduate Institute of Communication 
Engineering, National Taiwan University, Taipei, Taiwan. 
I. INTRODUCTION 
MAGE similarity assessment is fundamentally important to 
numerous multimedia information processing systems and 
applications, such as compression, restoration, enhancement, 
copy detection, retrieval, and recognition/classification. The 
major goal of image similarity assessment is to design 
algorithms for automatic and objective evaluation of similarity 
in a manner that is consistent with subjective human evaluation. 
A simple and popularly used metric is the peak signal-to-noise 
ratio (PSNR) or the corresponding mean-squared-error (MSE), 
whose correlation with human judgment has been shown to be 
not tight enough for most applications [1]-[2]. Some advanced 
approaches, based on the human visual system (HVS), natural 
scene statistics (NSS), and/or some image distortion model, 
also have been proposed to improve the PSNR metric. They 
demonstrate that visual quality of a test image is strongly 
related to the relative information present in the image and that 
the information can be quantified to measure the similarity 
between the test image and its reference image [1]-[2]. 
 There is no doubt that these advanced similarity metrics are 
efficient to measure the “quality” of an image compared with 
its original version, especially for some image reconstruction 
applications. Nevertheless, they mainly focus on assessing the 
similarities between a reference image and its 
non-geometrically variational versions, such as decompressed 
and brightness/contrast-enhanced versions. Different from the 
above, in this paper, we emphasize on the “similarity” between 
two arbitrary images. In several applications, assessment of the 
similarities between a reference image and its geometrically 
variational versions, such as translation, rotation, scaling, 
flipping, and other deformations, is required. On the other hand, 
one could encounter appearance variabilities of images, 
including background clutter, different viewpoints, and 
different orientations. Even if some advanced approaches, such 
as the structural similarity (SSIM) index and visual information 
fidelity (VIF) [1]-[2], can tolerate slightly geometric variations, 
their goals still do not devote to the consideration of more 
comprehensive image variations. 
 In this paper, motivated by the concept addressed in Sheikh 
and Bovik’s scheme [2], we interpret the image similarity 
assessment problem as an information fidelity problem. More 
specifically, we attempt to quantify the information that is 
present in a reference image and how much of this information 
can be extracted from a test image to assess the similarity 
Feature-based Sparse Representation for 
Image Similarity Assessment 
Li-Wei Kang, Member, IEEE, Chao-Yung Hsu, Hung-Wei Chen, Chun-Shien Lu, Member, IEEE, 
Chih-Yang Lin, Member, IEEE, and Soo-Chang Pei, Fellow, IEEE 
I
Copyright (c) 2011 IEEE. Personal use is permitted. For any other purposes, Permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.
IEEE TRANSACTIONS ON MULTIMEDIA, 2011. 
 
3
The major novelties and contributions of this paper include: 
(i) a feature-based image assessment approach is proposed to 
quantify how much information present in a reference image 
can be extracted from a test image by integrating image feature 
extraction and sparse representation; (ii) the inherent 
discriminative characteristic of sparse representation is 
exploited to assess the similarity between two images by 
performing sparse coding with respect to the dictionary 
integrated from the two dictionary features of the two images, 
respectively; (iii) efficient feature representation can be 
achieved by representing features in terms of linear 
combination of dictionary atoms; (iv) the proposed FSRISA 
provides a bounded similarity score, i.e., [0, 1], for detected 
features to quantify the similarity between two images. A 
bounded similarity score should be more suitable for users to 
conjecture how similar two images are or for a vision system to 
decide a threshold for image comparison. This metric should be 
better than just using the number of matched keypoints which 
may range from zero to thousands of keypoints. 
E. Organization of This Paper 
The rest of this paper is organized as follows. The proposed 
FSRISA scheme is addressed in Sec. II. The applications of 
FSRISA to image copy detection, retrieval, and recognition are 
presented in Sec. III. The simulation results are shown in Sec. 
IV, followed by the conclusion presented in Sec. V. 
II. PROPOSED FEATURE-BASED SPARSE REPRESENTATION FOR 
IMAGE SIMILARITY ASSESSMENT (FSRISA) METHODOLOGY 
Sparse representation has resulted in significant impact on 
computer vision and pattern recognition, usually in 
unconventional applications where the goal is not just to obtain 
a compact representation of the observed signal, but also to 
extract semantic information. The selection of the dictionary 
plays a key role to achieve this goal. That is, overcomplete 
dictionaries consisting of (or learned from) the training samples 
provide the key to attach semantic meaning to sparse signal 
representations [15]. 
 
 
Fig. 1.  The concept of the proposed FSRISA framework. 
 
In this paper, we utilize the sparse representation and 
dictionary learning techniques to design our framework of 
feature-based sparse representation for image similarity 
assessment (FSRISA). As illustrated in Fig. 1, given a reference 
image, we first apply standard SIFT to detect the keypoints and 
extract the feature vector for each keypoint in this image. To 
make the SIFT features more compact, we propose to learn the 
dictionary consisting of the prototype SIFT atoms to form the 
“dictionary feature” of the reference image, as described in Sec. 
II-A. Similarly, we also extract the dictionary feature for an 
input test image. Then, we calculate the similarity value 
between the two images using the proposed FSRISA technique, 
as described in Sec. II-B. 
A. Dictionary Feature Extraction 
Here, we apply the K-SVD dictionary learning algorithm [16] 
to construct the dictionary for a set of SIFT feature vectors of an 
image to form its dictionary feature. To learn an overcomplete 
dictionary for a set of training signals, K-SVD seeks the 
dictionary leading to the best possible representation of each 
signal in this set with strict sparsity constraints. The K-SVD 
algorithm, which generalizes the K-means algorithm, is an 
iterative scheme alternating between sparse coding of the 
training signals with respect to the current dictionary and an 
update process for the dictionary atoms so as to better fit the 
training signals. 
Given a set of K SIFT feature vectors, 1R  Miy , i = 1, 2, …, 
K, we apply K-SVD to find the dictionary D of size M×N, M < 
N << K, by formulating the problem as: 
 
  


 

K
i
iiKix
xy
i 1
2
2,...,2,1,D,
Dmin subject to Lxi i  0, ,    (1) 
 
where 1R  Nix  is the sparse representation coefficients of yi, 
||xi||0, l0-norm of xi, counts the number of nonzero coefficients of 
xi, and L is the most desired number of nonzero coefficients of 
xi. We apply K-SVD to solve Eq. (1) via an iterative manner 
with two stages: (i) sparse coding stage: apply OMP 
(orthogonal matching pursuit) [17] to solve xi for each yi while 
fixing D; and (ii) dictionary update stage: update D together 
with the nonzero coefficients of xi. The two stages are 
iteratively performed until convergence. It should be noted that 
the l0-minimization formulation in Eq. (1) can be converted into 
an l1-minimization problem [18] and other dictionary learning 
algorithm, (e.g., the online dictionary learning algorithm [18]) 
can be also applied in the dictionary feature extraction stage. 
The obtained dictionary feature D is an overcomplete 
dictionary, where D = {[dn]M×1}n = 1, 2, …, N NM R , contains N 
prototype feature vector atoms as the column vectors in D. Each 
original feature vector 1R  Miy , i = 1, 2, …, K, can be 
sparsely represented as a linear combination of the atoms 
defined in D, satisfying ||yi – Dxi||2 ≤ ε, where ε ≥ 0 is an error 
tolerance. 
B. Sparse Representation-based Image Similarity 
Assessment 
After obtaining the dictionary feature for each image, we 
formulate the image similarity assessment based on dictionary 
feature matching as a sparse representation problem, described 
as follows. 
First, consider the two SIFT feature (column) vectors with 
length M, y1i, i = 1, 2, .., K1, and y2j, j = 1, 2, .., K2, extracted, 
respectively, from the two images, I1 and I2, where K1 and K2 
are the numbers of feature vectors of I1 and I2, respectively. The 
dictionary features of I1 and I2 are D1 (of size M×N1) and D2 (of 
Feature 
extraction
Feature 
vectors
Dictionary 
learning
Reference 
image
Test 
image
Information extraction via 
sparse coding
Feature 
extraction
Feature 
vectors
Dictionary 
learning
SSRISA 
similarity value
F RISA 
Copyright (c) 2011 IEEE. Personal use is permitted. For any other purposes, Permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.
IEEE TRANSACTIONS ON MULTIMEDIA, 2011. 
 
5
Otherwise, when I1 is visually different from I2, most atoms for 
reconstructing y2j will still be selected from D2, resulting in V1 
< V2 and smaller Sim(I1, I2). The proposed FSRISA technique 
is summarized in Algorithm I and illustrated in Fig. 2. 
 
 
(a) 
 
(b) 
Fig. 2.  An illustrated example of the proposed FSRISA framework: (a) 
extraction of dictionary features for the reference and test images, respectively; 
and (b) matching of the two images via sparse coding and voting. 
 
The major goal of performing sparse coding with respect to 
the dictionary consisting of D1 and D2, instead of only D1 can be 
addressed as follows. When I1 (reference image) is visually 
different from I2 (test image), D1 and D2 are significantly 
different. In this scenario, the idea behind our FSRISA is 
somewhat related to that of sparse coding-based image 
classification approach [15], [21] or sparse coding-based image 
decomposition approach [22]. We use the similar concept to 
quantify the similarity between I2 and I1, which may be 
interpreted as either (i) classifying I2 into I1 or I2 itself; or (ii) 
decomposing I2 into the components of I1 and/or those of I2 
itself. When I1 is visually similar to I2, D1 and D2 are similar, 
which is enforced to that D1 is finer than D2 in FSRISA. Hence, 
the above discussions are also valid in this scenario. Moreover, 
why we don’t perform sparse coding with respect to only one 
dictionary D1 can be explained as follows. When performing 
sparse coding for the feature vectors of I2 with respect to a 
dictionary D1 consisting of atoms which may be not suitable for 
sparsely representing them, the sparse coding procedure still 
attempts to minimize the reconstruction errors. Based on our 
experience, it is usually not well distinguishable from 
reconstruction errors obtained with respect to either related or 
unrelated dictionaries. On the other hand, it is not easy to define 
a bounded score based on reconstruction error obtained by only 
one dictionary. 
 
Algorithm I: Proposed FSRISA. 
Input: A reference image I1 and a test image I2. 
Output: The similarity value between I1 and I2, i.e., Sim(I1, I2). 
1. Extract the SIFT feature vectors y1i, i = 1, 2, .., K1, from I1, 
followed by learning the dictionary feature D1 sparsely 
representing y1i. 
2. Extract the SIFT feature vectors y2j, j = 1, 2, .., K2, from I2, 
followed by learning the dictionary feature D2 sparsely 
representing y2j. 
3. Perform l1-minimization by solving Eq. (3) for y2j, j = 1, 
2, ..., K2, with respect to D12 = [D1|D2]. 
4. Calculate the reconstruction errors, E1j and E2j, for y2j, j = 1, 
2, ..., K2, with respect to D1 and D2, respectively. 
5. Perform voting by comparing E1j and E2j, for y2j, j = 1, 2, …, 
K2, and get the percentages of votes, V1 and V2, with 
respect to D1 and D2, respectively. 
6. Calculate Sim(I1, I2) = (V1 - V2 + 1) / 2 (Eq. (4)). 
 
C. Computational Complexity Analysis of FSRISA 
The computational complexity of the proposed FSRISA can 
be analyzed as follows. The computational complexity for 
extracting the dictionary feature of an image includes the 
complexities of performing SIFT feature extraction and K-SVD 
dictionary learning. For an image I with K M-dimensional SIFT 
feature vectors, the computational complexity for learning a 
dictionary of size M×N, M < N << K, using K-SVD [16] can be 
derived to be around [23]: 
 
TKSVD(K, L, M, N, J) = [K×(L2×N + 2×M×N)]×J,               (5) 
 
where L is the target sparsity and J is the number of training 
iterations. Hence, the approximate computational complexity 
of the dictionary feature extraction for an image is obtained as: 
 
TDict_Feature(S, K, L, M, N, J) = TSIFT(S) + TKSVD(K, L, M, N, J), (6) 
 
where TSIFT(S) roughly denotes the computational complexity 
(proportional to S) of SIFT feature extraction, in terms of the 
size (S) of an image. 
 On the other hand, the computational complexity of 
performing l1-minimization using SpaRSA can be 
approximately derived as [20]: 
 
TSpaRSA(P) = O(Pβ),                                   (7) 
 
where P is the number of atoms in a dictionary and β is a 
constant. It has been shown that the complexity of SpaRSA is 
approximately linear, i.e., β is very close to 1. 
 Based on Eqs. (6)-(7), the overall computational complexity 
for assessing the similarity between two images, I1 and I2, by 
performing the proposed FSRISA can be derived as: 
SIFT 
descriptors 
extraction
K1 128-D 
feature 
vectors
Dictionary 
learning
N1 128-D 
atoms
(N1 < K1)
…
Reference/Original 
image (I1) D1 =
Test/Received 
image (I2)
SIFT 
descriptors 
extraction
K2 128-D 
feature 
vectors
Dictionary 
learning
N2 128-D 
atoms
(N2 < K2)
y1i
…D2 =
y2j
D = [D1 | D2]
Sparse coding via 
l1-minimization
(N1 > N2)
… …
D1
A column vector
… =
Dictionary D
Test image (I2)
Copy detected
Undetected
Voting
D2
Reference image (I1) Test image (I2)
SIFT feature vectors y2j
xj
…
D12  [ 1| ]
12 
Copyright (c) 2011 IEEE. Personal use is permitted. For any other purposes, Permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.
IEEE TRANSACTIONS ON MULTIMEDIA, 2011. 
 
7
SIFT feature vectors yQj, j = 1, 2, ..., KQ, where KQ denotes the 
number of SIFT feature vectors, and the dictionary feature DQ. 
Then, we apply FSRISA to assess the similarity between IQ and 
the i-th image class, i = 1, 2, …, C, by performing  the 
l1-minimization (similar to Eq. (3)) to obtain the sparse 
representation coefficients xQj for yQj of IQ, with respect to the 
dictionary DCi_Q consisting of DCi and DQ. Then, we calculate 
the reconstruction errors for yQj with respect to DCi and DQ, 
respectively, and perform voting for each yQj. Based on Eq. (4), 
we can calculate the similarity between IQ and the i-th image 
class, denoted by Sim(IQ, Class-i). Finally, the query image IQ 
can be determined to belong to the i-th class with the largest 
Sim(IQ, Class-i). 
Moreover, the computational complexity for assessing the 
similarity between the query image IQ and the i-th class of 
images can be also approximately analyzed based on Eq. (8), 
where the dictionary feature extraction for the i-th class of 
images can be performed in advance during database 
construction. In our image recognition approach, the sparse 
coding procedure should be performed for a query image and 
each image class, which is indeed computationally expensive. 
The complexity of our approach (slightly cheaper than or 
similar to that of the two-stage approach proposed in [28]) can 
be improved by applying more efficient sparse coding 
techniques, such as multi-core OMP [18]. 
 It is also worth noting that, if the feature size of a query 
image IQ is crucial for on-line applications, each SIFT feature 
vector yQj can be further compressed via compressive sensing 
[19]. Then, xQj can be also solved by performing the 
l1-minimization based on the received measurements for yQj 
(compressed yQj). 
IV. SIMULATION RESULTS 
In this section, we present simulations conducted on publicly 
available benchmarks or datasets for evaluation of the proposed 
FSRISA scheme in the fundamental issue of image assessment 
and three multimedia applications. Then, we address some 
experimental comparisons between sparse coding-based and 
traditional approaches to demonstrate the advantage of the 
proposed scheme. 
A. Evaluation of FSRISA-based Image Similarity 
Assessment 
To evaluate the efficiency of FSRISA for assessing the 
similarity between two images, we use several examples of 
image manipulations (including signal processing and 
geometric distortions) defined in the Stirmark benchmarks [26] 
and compare FSRISA of similarity range [0, 1] with 
well-known metrics, PSNR of range [0, ∞] and VIF of range [0, 
1] (for image contrast enhancement, VIF value may be larger 
than 1) [2]. In the three evaluated metrics, the larger the value is, 
the more similar the two evaluated images are. 
In our simulation, the size of each image is 280×280. 
Nevertheless, it should be noted that our scheme can work for 
images of different sizes. Based on the principle for tuning the 
KSVD parameters described in Sec. II-B, we set the following 
parameters to ensure that D1 is finer than D2. For a reference 
image I1 of size S1 = 280×280, the parameters are shown as 
follows. We set the number of atoms in the dictionary feature 
D1 to N1 = 0.5×K1, where K1 denotes the number of SIFT 
feature vectors for I1, the number of iterations K-SVD performs 
for learning D1 to J1 = 30, and the target sparsity to L1 = 0.3×N1. 
For a test image I2 of size S2 = 280×280, the parameters are N2 = 
0.3×K2, J2 = 10, and L2 = 0.1×N2. If N1 < N2, we force N1 > N2 
by properly adjusting the factors to be multiplied by K1 and K2, 
respectively. 
The similarity values obtained from the PSNR, VIF, and 
FSRISA for some examples of image manipulations for the 
Baboon and Lena images, respectively, are shown in Figs. 3-4. 
It can be observed from Figs. 3-4 that for image similarity 
assessment, FSRISA is more robust to several image 
manipulations, especially for geometrical manipulations. The 
similarity values between an image and its related manipulated 
versions for FSRISA are usually higher than 0.5, while the ones 
between an image and unrelated versions are usually far lower 
than 0.5. Nevertheless, the VIF value between an image and its 
manipulated versions are usually lower than 0.5, except for the 
brightness and contrast adjusting manipulation, which 
enhances the image quality. For some manipulations (e.g., 
scaling, cropping, and flipping), the VIF value is almost 
indistinguishable from that for an unrelated image. Moreover, 
PSNR is obviously not good for similarity assessment. Hence, 
the discriminability of FSRISA is usually better than the other 
two metrics used for comparisons. In this paper, we focus on 
“similarity” assessment between images and hence, the 
similarity scores for different kinds of manipulations are 
somewhat similar. The major goal is also the “discriminability” 
between different images. Nevertheless, for the “quality” issue, 
the differences between a test image and its reference image 
(ground truth) becomes more critical, which is not the focus of 
this paper. 
B. Evaluation of FSRISA-based Image Copy Detection 
To evaluate the proposed FSRISA-based image copy 
detection scheme, ten 280×280 images, Baboon, Boat, Clock, 
Girl, House, Lena, Monarch, Pepper, Splash, and Tiffany 
images were used. Each image was manipulated by 204 
manipulations defined in the Stirmark benchmarks [26]. These 
image manipulations are also very similar to the ones used to 
evaluate image copy or near-duplicate detection in [9], [10], 
[24]-[25]. We treat each of the ten images as a query image and 
its 204 manipulated versions as the test images. The parameter 
settings for applying FSRISA to each query image and each test 
image are the same as those settings for the reference image and 
test image, respectively, used in Sec. IV-A. 
To evaluate the true positive rate (TPR), the proposed 
scheme was conducted between each original image and its 204 
manipulated versions. To evaluate the false positive rate (FPR), 
the proposed scheme was conducted for each original image 
and the 204 manipulated versions of each of the other 9 images. 
The receiver operating characteristic (ROC) curves (TPR-FPR 
curve) obtained from the proposed scheme by adjusting the 
threshold λ, and the “feature points hash” scheme [25] with 
public source code available for the ten images are shown in Fig. 
Copyright (c) 2011 IEEE. Personal use is permitted. For any other purposes, Permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.
IEEE TRANSACTIONS ON MULTIMEDIA, 2011. 
 
9
sparse coding for each feature vector of the flipped image with 
respect to the dictionary, most of the feature vectors of the 
image can be still well sparsely and linearly represented by the 
dictionary feature of its original version. 
 
 
Fig. 5.  Comparison of ROC curves obtained using the proposed FSRISA-based 
image copy detection scheme and the “feature points hash” scheme [25]. 
 
 
Fig. 6.  The probability distributions of the FSRISA similarity values between 
the Lena image and its 204 manipulated versions and those between the Lena 
image and the 204 manipulated versions of each of the other night test images. 
 
C. Evaluation of FSRISA-based Image Retrieval 
To evaluate the proposed FSRISA-based image retrieval 
scheme, we construct an image database consisting of the ten 
test images together with their respective 204 manipulations 
used in Sec. IV-B (total 2050 images), and the Corel-1000 
image dataset [27] (total 1000 images from 10 classes), 
resulting in a total of 3050 images. The parameter settings for 
applying our FSRISA to each query image and each database 
image are the same as those settings for reference image and 
test image, respectively, used in Sec. IV-A. In this subsection, 
we conduct two kinds of experiments, including “copy image 
retrieval” and “general image retrieval.” 
In this paper, we just consider the simplest scenario for 
image retrieval, where the score between a query image and 
each database image is individually calculated using the 
proposed FSRISA. Then, we retrieve the top Q images with the 
largest scores, where Q is the desired number of retrieved 
images. Here, we neither consider performing any indexing or 
clustering techniques to re-organize an image database nor 
integrating multiple features for efficient image retrieval. 
The main reasons for constructing this database and 
conducting such two kinds of experiments to evaluate our 
scheme can be described as follows. We focus on investigating 
sparse representation of SIFT features and finding its 
usefulness in image retrieval application. Hence, the database 
includes several images and their variations for “copy image 
retrieval” evaluation (similar to the test dataset collected in [9], 
[10]). On the other hand, without integrating multiple features, 
we just want to test some “pure” query images (without overly 
complex scenes or with a clear background) to retrieve the 
images with the same semantic meaning, with different 
appearance for “general image retrieval” evaluation. 
For “copy image retrieval,” we use the ten original images as 
the query images and evaluate the precision rates for retrieving 
the top 205 database images with the largest scores for each 
query image, as shown in TABLE I, where the average 
precision is 99.01%. Such simulation settings are similar to the 
ones used for near-duplicate image retrieval performed in [10]. 
It can be observed from TABLE I that FSRISA is efficient for 
retrieving the copy versions of a query image. 
 
TABLE I 
THE PRECISION RATES FOR RETRIEVING THE TOP 205 DATABASE IMAGES WITH 
THE LARGEST SCORES FOR EACH QUERY IMAGE. 
 
Query image Precision Query image Precision 
Baboon 98.95% Lena 99.48% 
Boat 97.91% Monarch 98.43% 
Clock 98.95% Pepper 98.95% 
Girl 98.95% Splash 100% 
House 99.48% Tiffany 98.95% 
 
For “general image retrieval,” we randomly select 10 images 
from the “Dinosaurs” class of the Corel-1000 image dataset and 
evaluate the average precision for retrieving the top 10, 20, and 
40 images, respectively, with the largest scores from the dataset. 
We compare the results with the best ones reported in [29] 
(denoted by “visually significant point feature”), as shown in 
TABLE II. Some retrieved images for a query image are 
illustrated in Fig. 7. Moreover, the probability distributions of 
the FSRISA similarity values between a selected “Dinosaurs” 
image and the 100 images in the same class, and those between 
the image and the other 900 images in Corel-1000 dataset are 
displayed in Fig. 8. It can be observed from TABLE II and Figs. 
7-8, for images without overly complex scenes, FSRISA can be 
still efficient for retrieving images with similar semantic 
meanings. It should be noted that the FSRISA values between 
an image and its related images, shown in Fig. 8, are smaller 
than those shown in Fig. 6. This is because, in Fig. 8, FSRISA is 
used to assess the similarities between a query image and the 
images with the same semantic meaning, but different 
appearances in the same class, instead of the manipulated 
versions of the query image. 
D. Evaluation of FSRISA-based Image Recognition 
To evaluate the proposed FSRISA-based image recognition 
scheme, we used the COIL-20 [30] and COIL-100 [31] datasets. 
We followed the setup for simulations provided in [32], where 
Proposed FSRISA-based 
Feature points hash [25] 
Lena image vs. manipulated Lena images 
Lena image vs. unrelated images 
FSRISA similarity value
Pr
ob
ab
ili
ty
 
0          0.1        0.2         0.3        0.4         0.5         0.6        0.7        0.8        0.9         1 
Copyright (c) 2011 IEEE. Personal use is permitted. For any other purposes, Permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.
IEEE TRANSACTIONS ON MULTIMEDIA, 2011. 
 
11
 
TABLE V 
THE RECOGNITION RATES FOR EVALUATING CALTECH-101 DATASET. 
 
Schemes 5 training images 
15 training 
images 
30 training 
images 
Proposed FSRISA-based 58.09% 69.06% 74.51% 
ScSPM [7] - 67.00% 73.20% 
NBNN [39] 50.00% 65.00% 70.40% 
SVM-KNN [38] 45.10% 59.10% 66.20% 
 
Second, to evaluate the impact of sparse coding-based 
matching strategy, we compare two kinds of approaches 
denoted by: (i) “Sparse-Traditional;” and (ii) “Proposed:” 
sparse coding-based feature representation + sparse 
coding-based matching. Based on TABLE V, “Proposed” 
approach can slightly outperform “Sparse-Traditional” 
approach (denoted by “ScSPM” [7]). Another example for 
“Sparse-Traditional” also realized by using sparse coding and 
SVM for testing COIL-100 dataset can be found in [36]. Based 
on TABLE IV, “Proposed” approach can slightly outperform 
“Sparse-Traditional” approach (denoted by “Distributed sparse 
representation” [36]). Even if the improvement of the proposed 
scheme seems to be limited, a unique characteristic of our 
scheme is to define a similarity assessment metric, which can 
be widely applicable in several multimedia applications. 
V. CONCLUSIONS 
In this paper, we have proposed a scheme of feature-based 
sparse representation for image similarity assessment 
(FSRISA). The core is to propose a feature-based image 
similarity assessment technique by exploring the two aspects of 
a feature detector in terms of representation and matching in 
our FSRISA framework. Then, we properly formulate the 
image copy detection, retrieval, and recognition problems as 
sparse representation problems and solve them based on our 
FSRISA. The future works may focus on reducing the 
computational complexities for dictionary feature extraction 
and image matching by performing sparse coding, which can be 
further reduced via novel techniques, such as the online 
dictionary learning algorithm [18], efficient greedy algorithm 
[18], or fast l1-minimization algorithm [40]. On the other hand, 
for FSRISA-based image retrieval applications, further 
indexing techniques should be also studied. The proposed 
FSRISA-based image copy detection scheme may be extended 
to video copy detection by learning the “dictionary feature” for 
each video sequence/clip. Moreover, incorporated with our 
secure SIFT techniques [41]-[42], the three FSRISA-based 
applications can be performed in the encrypted domain and are 
suitable for privacy-preserving applications. 
REFERENCES 
[1] Z. Wang and A. C. Bovik, “Mean squared error: love it or leave it? - A 
new look at signal fidelity measures,” IEEE Signal Processing Magazine, 
vol. 26, no. 1, pp. 98-117, Jan. 2009. 
[2] H. R. Sheikh and A. C. Bovik, “Image information and visual quality,” 
IEEE Trans. on Image Processing, vol.15, no.2, pp. 430-444, Feb. 2006. 
[3] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,” 
Int. J. of Computer Vision, vol. 60, no. 2, pp. 91–110, 2004. 
[4] J. Sivic and A. Zisserman, “Video Google: a text retrieval approach to 
object matching in videos,” in Proc. of IEEE Int. Conf. on Computer 
Vision, Nice, France, Oct. 2003, vol. 2, pp. 1470–1477. 
[5] D. Nistér and H. Stewénius, “Scalable recognition with a vocabulary tree,” 
in Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, 
2006, pp. 2161–2168. 
[6] J. Zhang, M. Marszałek, S. Lazebnik, and C. Schmid, “Local features and 
kernels for classification of texture and object categories: a 
comprehensive study,” Int. J. of Computer Vision, vol. 73, no. 2, pp. 
213–238, 2007. 
[7] J. Yang, K. Yu, Y. Gong, and T. Huang, “Linear spatial pyramid 
matching using sparse coding for image classification,” in Proc. of IEEE 
Conf. on Computer Vision and Pattern Recognition, June 2009. 
[8] L. W. Kang, C. Y. Hsu, H. W. Chen, and C. S. Lu, “Secure SIFT-based 
sparse representation for image copy detection and recognition,” in Proc. 
of IEEE Int. Conf. on Multimedia and Expo, Singapore, July 2010. 
[9] Z. Xu, H. Ling, F. Zou, Z. Lu, and P. Li, “A novel image copy detection 
scheme based on the local multi-resolution histogram descriptor,” 
Multimedia Tools and Applications, Jan. 2010. 
[10] Y. Ke, R. Sukthankar, and L. Huston, “Efficient near-duplicate detection 
and sub-image retrieval,” in Proc. of ACM Multimedia, 2004. 
[11] W. L. Zhao and C. W. Ngo, “Scale-rotation invariant pattern entropy for 
keypoint-based near-duplicate detection,” IEEE Trans. on Image 
Processing, vol. 18, no. 2, pp. 412-423, Feb. 2009. 
[12] K. Mikolajczyk and C. Schmid, “A performance evaluation of local 
descriptors,” IEEE Trans. on Pattern Analysis and Machine Intelligence, 
vol. 27, no. 10, pp. 1615-1630, Oct. 2005. 
[13] V. Chandrasekhar, M. Makar, G. Takacs, D. M. Chen, S. S. Tsai, R. 
Grzeszczuk, and B. Girod, “Survey of SIFT compression schemes,” in 
Proc. of Int. Workshop on Mobile Multimedia Processing, Turkey, 2010. 
[14] O. Pele and M. Werman, “A linear time histogram metric for improved 
SIFT matching,” in Proc. of European Conf. on Computer Vision, vol. 
5304, pp. 495-508, 2008. 
[15] J. Wright, Y. Ma, J. Mairal, G. Sapiro, T. Huang, and S. Yan, “Sparse 
representation for computer vision and pattern recognition,” Proceedings 
of the IEEE, vol. 98, no. 6, pp. 1031-1044, June 2010. 
[16] M. Aharon, M. Elad, and A. M. Bruckstein, “The K-SVD: an algorithm 
for designing of overcomplete dictionaries for sparse representation,” 
IEEE Trans. on Signal Processing, vol.  54, no. 11, pp. 4311–4322, 2006. 
[17] S. Mallat and Z. Zhang, “Matching pursuits with time-frequency 
dictionaries,” IEEE Trans. on Signal Processing, vol. 41, no. 12, pp. 
3397-3415, Dec. 1993. 
[18] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online learning for matrix 
factorization and sparse coding,” Journal of Machine Learning Research, 
vol. 11, pp. 19-60, 2010. 
[19] E. Candes and M. Wakin, “An introduction to compressive sampling,” 
IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 21-30, March 2008. 
[20] S. J. Wright, R. D. Nowak, and M. A. T. Figueiredo, “Sparse 
reconstruction by separable approximation,” IEEE Trans. on Signal 
Processing, vol. 57, no. 7, pp. 2479-2493, July 2009. 
[21] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma, “Robust face 
recognition via sparse representation,” IEEE Trans. on Pattern Analysis 
and Machine Intelligence, vol. 31, no. 2, pp. 210–227, 2009. 
[22] J. M. Fadili, J. L. Starck, J. Bobin, and Y. Moudden, “Image 
decomposition and separation using sparse representations: an overview,” 
Proc. of the IEEE, vol. 98, no. 6, pp. 983-994, June 2010. 
[23] R. Rubinstein, M. Zibulevsky, and M. Elad, “Efficient implementation of 
the K-SVD algorithm using batch orthogonal matching pursuit,” CS 
Technical Report, Technion - Israel Institute of Technology, 2008. 
[24] C. Kim, “Content-based image copy detection,” Signal Processing: 
Image Communication, vol. 18, pp. 169–184, 2003. 
[25] V. Monga and B. L. Evans, “Perceptual image hashing via feature points: 
performance evaluation and tradeoffs,” IEEE Trans. on Image Processing, 
vol. 15, no. 11, pp. 3453–3466, 2006. 
[26] F. A. P. Petitcolas, “Watermarking schemes evaluation,” IEEE Signal 
Processing Magazine, vol. 17, no. 5, pp. 58-64, Sept. 2000. 
[27] J. Z. Wang, J. Li, G. Wiederhold, “SIMPLIcity: semantics-sensitive 
integrated matching for picture libraries,” IEEE Trans. on Pattern 
Analysis and Machine Intelligence, vol. 23, no.9, pp. 947-963, 2001. 
[28] Y. Liu, F. Wu, Z. Zhang, Y. Zhuang, and S. Yan, “Sparse representation 
using nonnegative curds and whey,” in Proc. of IEEE Conf. on Computer 
Vision and Pattern Recognition, CA, USA, June 2010, pp. 3578–3585. 
[29] M. Banerjee, M. K. Kundu, and P. Maji, “Content-based image retrieval 
using visually significant point features,” Fuzzy Sets and Systems, vol. 
160, no. 23, pp. 3323-3341, Dec. 2009. 
  
 
 
附件四 
 
Li-Wei Kang, Chih-Yang Lin, Jyh-Da Wei, Yu-Siang Chen, Hui-Chun Cheng, Yui-Cheng Lin, 
Ming-Hong Shen, and Chia-Hung Yeh, “Secure trans-sensing for compressively sensed 
multimedia signals over cloud environment,” submitted to National Symposium on 
Telecommunications, Hualien, Taiwan, Nov. 2011 (NST2011). 
 1
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                  日期：100 年 04 月 12 日 
一、參加會議經過 
2010 Picture Coding Symposium (PCS2010, http://www.pcs2010.org/) 於2010/12/08~ 2010/12/10於日
本名古屋 (Nagoya, Japan) 舉行，共計3日，由IEEE、IEICE、IPSJ、ITE等知名學術組織及日本Nagoya 
University共同主辦。另外亦於2010/12/07於同一地點舉行日本區域性的影像處理學術會議
-Workshop on Picture Coding and Image Processing (WPCIP)。 PCS為一個進階視覺資料編碼 
(visual data coding) 方面的主流國際學術會議，主要包含影像及視訊編碼 (image/video coding) 相
關領域。PCS為該領域的先驅且擁有最悠久的歷史 (PCS2010為第28屆)。自1969年起PCS便提供
一個visual data coding相關研究的交流園地且吸引到不少學術界及工業界的知名學者及技術研發
人員參與。PCS2010的投稿論文來自全世界超過27個國家，共有256篇論文投稿，經大會審查接受
152篇論文。 
PCS2010的論文主題涵蓋所有image/video coding相關的議題: (1) Coding of still and moving 
pictures、(2) Model-based and synthetic coding、(3) Distributed source coding、(4) Image and video 
processing、(5) Multimodal coding and processing、(6) Very high-resolution imaging, coding and 
processing、(7) Multi-view video processing and coding、(8) Representation, analysis and coding of 3D 
scenes、(9) Virtual/augmented/mixed reality、(10) Subjective and objective quality assessment metrics 
and methods、(11) Joint source and channel coding、(12) Error robustness, resilience and concealment、
(13) Transcoding and transmoding coding for mobile, IP and sensor networks、(14) Coding and 
計畫編號 NSC 99－2218－E－001－010－ 
計畫名稱 壓縮感測技術及其於多媒體應用之研究 
出國人員
姓名 康立威 
服務機構
及職稱 
中央研究院資訊科學研究所 
助理研究學者 
會議時間 2010 年 12 月 08 日至 2010 年 12 月 10 日 會議地點 日本名古屋 (Nagoya, Japan) 
會議名稱 
(中文) 2010 圖像編碼會議 
(英文) 2010 Picture Coding Symposium (PCS2010) (published by IEEE) 
發表論文
題目 
(中文) 基於字典學習之分散式視訊壓縮感測 
(英文)  Dictionary learning-based distributed compressive video sensing 
 3
複雜度。因此，我們率先以美國 Rice University 所發展的 compressive sensing-based single pixel 
camera 硬體架構為基礎發展可以直接擷取壓縮視訊資料 (compressed video data) 的低複雜度視訊
壓縮技術。我們的方法可以比先前的一個熱門研究議題-低複雜度視訊編碼: 分散式視訊編碼 
(distributed video coding (DVC)) 具有更低的計算複雜度。與會期間，我們的論文受到高度重視，
不少相關領域的學者皆來與本人討論及交換意見。例如 : DVC 研究領域知名學者-Leibniz 
Universität Hannover, Germany 的 Prof. Jörn Ostermann 及其博士生 Sven Klomp、北京清華大學電子
系的何芸教授、國立中央大學通訊系的詹佳琦同學 (唐之瑋助理教授的碩士生) 及多位日、韓學
者皆表示高度的興趣。 
在其它 session 方面，本人發現除了持續追求視訊壓縮效率的提升之外，其他屬於特殊應用或
提供特殊功能的相關壓縮技術也是熱門的研究議題。例如: free-viewpoint television (FTV)、3DTV
及 depth map coding 研究多重攝影機 (multi-camera 或 multi-view) 之 video coding 技術、evolutive 
video coding 則提出一種嶄新的 genetic programming (GP)-based image coding 技術，其中根據輸入
影像的特性來自動產生最適合的壓縮演算法，是一種非常新穎的概念。另外，尚有許多其它新的
壓縮技術於 Sessions-New techniques for video coding 及 Beyond H.264/MPEG-4 AVC 中討論。 
在與國際學者交流方面，在本次會議中，除了在各個 session 與多位學者交流之外，本人亦結
識了多位學者並與之交換意見。在國際學者方面有: Leibniz Universität Hannover, Germany 的 Prof. 
Jörn Ostermann (IEEE Fellow)、University of Southern California, USA 的 Prof. Antonio Ortega (IEEE 
Fellow) 及北京清華大學的何芸教授等國際知名學者。在本國學者方面有: 交大電子的杭學鳴教授 
(IEEE/IET Fellow) 及中央通訊的詹佳琦同學。 
二、與會心得 
本人自 2007 年至今已有 3 次於 PCS 發表論文 (PCS2007、PCS2009 及 PCS2010，約 18 個月舉行
一次)，感覺上雖然 PCS 的規模不如其它多媒體相關知名會議 (例如: ICIP、ICME、ICASSP 及
ISCAS) 且錄取率也不如其他知名低錄取率會議 (例如: ACMMM、ICCV 及 CVPR)，然而，PCS
確實是全球 image/video coding 知名學者 (例如: 本次有 Prof. Jörn Ostermann、Prof. Antonio Ortega
及杭學鳴教授等 video coding 界重量級學者) 必定投稿及參加的重要會議。PCS 亦經常有高原創
性的論文發表且經常被其它文獻所引用。因此，往後之 PCS 確實有繼續參加之必要。 
 總上所述，參與 PCS2010 使我們的論文在重要國際學術會議適時地發表並受到許多學者注意
及認同。藉由參加多場專題演講，我們瞭解到 image/video coding 相關領域研究的過去、現在與未
來。藉由參加多場 oral及 poster session，我們可以看到國際 image/video coding研究的現況與趨勢，
並確定我們的研究是邁向正確的方向及掌握其他可能與我們結合的相關主題。另外，藉由與國際
學者交流對我們及所屬單位知名度的拓展亦會有所幫助。 
三、考察參觀活動(無是項活動者略) 
無 
四、建議 
DICTIONARY LEARNING-BASED DISTRIBUTED COMPRESSIVE VIDEO SENSING+
Hung-Wei Chen, Li-Wei Kang, and Chun-Shien Lu*
Institute of Information Science, Academia Sinica, Taipei, Taiwan 
E-mail: {hungwei, lwkang, lcs}@iis.sinica.edu.tw 
ABSTRACT 
We address an important issue of fully low-cost and low-complex 
video compression for use in resource-extremely limited 
sensors/devices. Conventional motion estimation-based video 
compression or distributed video coding (DVC) techniques all rely 
on the high-cost mechanism, namely, sensing/sampling and 
compression are disjointedly performed, resulting in unnecessary 
consumption of resources. That is, most acquired raw video data 
will be discarded in the (possibly) complex compression stage. In 
this paper, we propose a dictionary learning-based distributed 
compressive video sensing (DCVS) framework to “directly” 
acquire compressed video data. Embedded in the compressive 
sensing (CS)-based single-pixel camera architecture, DCVS can 
compressively sense each video frame in a distributed manner. At 
DCVS decoder, video reconstruction can be formulated as an l1-
minimization problem via solving the sparse coefficients with 
respect to some basis functions. We investigate adaptive 
dictionary/basis learning for each frame based on the training 
samples extracted from previous reconstructed neighboring frames 
and argue that much better basis can be obtained to represent the 
frame, compared to fixed basis-based representation and recent 
popular “CS-based DVC” approaches without relying on 
dictionary learning. 
Index Terms—Compressive sensing, sparse representation, 
dictionary learning, single-pixel camera, l1-minimization.
1. INTRODUCTION 
Conventional high-complexity video compression techniques [1] 
or recently popular low-complexity technique called distributed 
video coding (DVC) [2] all rely on the high-cost mechanism where 
video sensing and compression tasks are disjointedly performed. 
Most acquired raw pixel data in the sensing stage will be discarded 
in the (possibly) complex compression stage, which suffers from 
unnecessary memory wasting and power consumption, and is 
especially unfeasible for resource-extremely limited devices/ 
sensors. Recently, with the advent of the compressive sensing 
(CS)-based single-pixel camera architecture [3], based on the 
inherent sparse property of images, CS [4] can directly and 
efficiently acquire compressed image data via randomly projecting 
raw data to obtain linear and non-adaptive measurements. Image 
reconstruction can be formulated as solving an l1-minimization 
problem [5]-[6] based on the acquired data measurements. 
Recently, compressive video sensing integrating both video 
sensing and compression into a unified task has emerged as a new 
way to directly acquire compressed video data via random 
projection for each individual frame at a low-complexity encoder. 
Video reconstruction can be achieved via performing l1-
minimization together with exploiting correlations among 
successive frames at a high-complexity decoder [7]-[9]. In [7], we 
have proposed a distributed compressive video sensing (DCVS) 
framework, where an efficient initialization and several stopping 
criteria were designed to improve and speedup the employed l1-
minimization algorithm for video reconstruction. In [8]-[9], DVC 
algorithms using CS were proposed, where the major core is to 
assume each block in a frame can be sparsely represented with 
respect to the dictionary/basis formed from a set of spatially local 
neighboring blocks (without performing dictionary learning) of 
previous reconstructed neighboring frames, denoted as the “W/O 
dictionary learning”-based scheme in this paper. 
In this paper, a DCVS framework via “dictionary learning”-
based sparse representation is proposed. Our major contributions 
include: (i) Single-pixel camera-compatible low-complexity 
video encoder: only CS random projection will be individually 
performed for each frame, which can be fully compatible to the 
single-pixel camera [3]. In [8]-[9], it is required to support the 
H.264/AVC encoder to periodically encode each intra-frame, 
which is more complex. (ii) Dictionary-learning based sparse 
representation: a dictionary learned from a set of blocks globally 
extracted from the previous reconstructed neighboring frames 
together with the side information generated from them is used as 
the basis of each block in a frame. The major advantages are: (a) 
Extracting more blocks globally for dictionary learning can 
provide much better representation for blocks with large motions; 
and (b) Even if the qualities of the training blocks are not good 
enough (due to poorly reconstructed neighboring frames), the 
learned dictionary may still provide a good basis. The fact can be 
similarly explained by the image denoising approach via the 
dictionary learned from the patches extracted from a noisy image 
itself [10]. In contrast, the “W/O dictionary learning” approach [8]-
[9] may not work well for: (a) blocks with (very) large motions; 
and (b) the use of non-learned dictionary formed from (possibly) 
low-quality blocks. Other technical comparisons can be found in 
Table 1 of Sec. 4. 
2. COMPRESSIVE SENSING 
Assume that an orthonormal basis matrix (or dictionary)  
NN×∈ RȌ  (e.g., DWT basis) can provide a K sparse 
representation for a signal 1R ×∈ Nx , i.e., x = Ȍș, where 
1Rș ×∈ N  can be well approximated using only K << N non-zero 
entries. Compressive sensing (CS) [4] states that x can be 
accurately reconstructed by taking only M = O(K×log(N/K)), K < 
M << N, linear and non-adaptive measurements from the random 
projection as y = Ɏx, where 1R ×∈ My  is a measurement vector 
and NM ×∈ Rĭ  is a measurement matrix that is incoherent with Ȍ.
More specifically, the M measurements in y are random linear 
combinations of the entries of x, which can be viewed as the 
compressed version of x. The reconstruction of ș (or x) can be 
formulated as an l1-minimization problem. On the other hand, a 
basis matrix is actually not necessarily orthonormal. An 
overcomplete dictionary D learned from training some selected 
training samples [10] can be used as a basis for representing the 
original signal. In fact, by using a measurement matrix Ɏ randomly 
______________________________________________________________________________________________________________________________ 
+This work was supported in part by the National Science Council, Taiwan, under Grants NSC97-2628-E-001-011-MY3, NSC98-2631-H-001-013, NSC98-
2811-E-001-008, NSC99-2218-E-001-010, and NSC99-2811-E-001-006.  
*Corresponding author: lcs@iis.sinica.edu.tw. 
28th Picture Coding Symposium, PCS2010, December 8-10, 2010, Nagoya, Japan
978-1-4244-7135-5/10/$26.00 ©2010 IEEE - 210 -
decoder, where the parameter settings are described in Sec. 4. Fig. 
2(a) and (b) show, respectively, an original CS frame (the 32nd 
frame), and its dictionary with size 256×256, where each atom 
(column vector) with length 256 in the dictionary is displayed as a 
block. Fig. 2(c) and (d), respectively, show the reconstructed CS 
frame using the dictionary shown in Fig. 2(b) and the frame-based 
DWT basis (treat this frame as a key frame). It can be observed 
from Fig. 2 that using the learned dictionary can provide better CS 
frame reconstruction than using the DWT basis at the same MR.
(a)                           (b) 
(c)                                            (d) 
Fig. 2. Comparison of reconstructed CS frames with respective 
to learned and fixed dictionaries: (a) The original 32nd frame; 
(b) the dictionary learned for (a); (c) the reconstructed 32nd 
frame with respective to the dictionary shown in (b) (PSNR = 
31.49dB); and (d) the reconstructed 32nd frame with respect to 
the frame-based DWT basis (PSNR=27.83dB). 
4. SIMULATION RESULTS 
In this paper, several QCIF (frame size: 176×144) video sequences 
(51 Y frames for each) with GOP size = 2, and different 
measurement rates (MRs) were employed to evaluate the proposed 
DVCS scheme. For learning the dictionary for each CS frame 
consisting of several non-overlapping 16×16 blocks, the parameter 
settings are described as follows. The dictionary size was set to 
256×256, i.e., Nb = 16×16 = 256 and P = 256 (atoms). In K-SVD 
[10], the number of training iterations was set to 10 while the target 
sparsity, denoted by S (number of nonzero coefficients used to 
represent each signal/block) was set to 10. According to our 
simulations, the performances will not exhibit significant changes 
when the two above-mentioned parameters for K-SVD are 
increased, which will increase the complexity of dictionary 
learning. Currently, the MR of all the frames in a video sequence 
are set to be the same as the target MR. In addition, to keep the 
encoding complexity to be as low as possible, the available 
measurements for each CS frame are equally allocated to each 
block without considering the complexity or sparsity of the block. 
In this paper, two compressive video sensing schemes were 
used for comparison with our dictionary learning-based DCVS 
scheme (denoted by Proposed). The first one is the Frame-DWT
scheme, in which under our DCVS architecture, all frames are 
treated as key frame (reconstructed with respect to the frame-based 
DWT basis). The second one is the “W/O dictionary learning” 
scheme, in which based on our DCVS architecture, each block in a 
CS frame is reconstructed with respect to its corresponding 
dictionary without learning. The second type is similar to the major 
core in [8]-[9]. Here, based on [8], the dictionary of each block in a 
CS frame includes the blocks extracted from the two spatially 
corresponding square 17×17 windows, respectively, in the two 
neighboring reconstructed key frames. The characteristics of the 
“Proposed” and the “W/O dictionary learning” schemes are 
summarized in Table 1. Please note that we only implemented the 
major core of the schemes proposed in [8]-[9] instead of the full 
system for comparison. 
For reconstructing block bti in a CS frame xt using SpaRSA, the 
computational complexity is approximately O(Pȕ), where P is 
decided by the dimension of PMt tiA
×
∈ R , and ȕ is a constant. It 
has been shown that the complexity of SpaRSA is approximately 
linear (ȕ is close to 1) [6]. In our parameter settings (Table 1), the 
dimension P (256) used by Proposed scheme is smaller than that 
(578) used by “W/O dictionary learning” scheme. Nevertheless, 
additional complexity for performing K-SVD dictionary learning 
[10] (approximately Q×(S2×P + 2×Nb×P) per training iteration [11], 
where Q is the number of training patches, S is the target sparsity, 
and Nb×P is the size of each dictionary Dt) is required for each CS 
frame in our scheme, which is, however, usually acceptable for a 
high-complexity decoder supported in a server or in cloud. 
Table 1. Comparisons of the Proposed and “W/O dictionary 
learning” schemes. 
Scheme Proposed W/O dictionary learning 
Ingredients 
of dictionary
Learning based on 
the extracted patches 
from neighboring key 
frames and side 
information 
Spatially neighboring blocks 
from neighboring key frames 
without learning 
Dictionary 
size 256 atoms 
Size of spatially 
corresponding square 
window × Number of 
neighboring key frames 
(17×17×2 = 578 atoms) 
Number of 
dictionaries 
per CS 
frame 
1
Number of blocks per CS 
frame 
(99 dictionaries for a QCIF 
CS frame) 
Dictionary 
type Global with learning Local w/o learning 
Decoding 
complexity 
per CS 
frame
Dictionary learning 
by K-SVD + l1-
minimization solving 
256 coefficients per 
block 
l1-minimization solving 578 
coefficients per block 
The average PSNR performances at different MRs for the 
Foreman, Mobile, and Silent sequences are shown in Figs. 3(a), 
4(a), and 5(a), respectively, where it can be observed that the 
PSNR performances of the proposed DCVS can outperform or be 
comparable to the Frame-DWT and “W/O dictionary learning” 
schemes [8]-[9], especially at lower MRs and for sequences with 
large motion. It can also be observed from Fig. 4(a) that the PSNR 
performances obtained from the three schemes are somewhat poor. 
The major reasons include: (i) the frame contents of the Mobile
sequence are very complex, which may not be exactly sparse with 
respect to most bases, and (ii) the motions of the sequence are very 
large so that it is hard to learn a good dictionary for a CS frame 
from its neighboring key frames. It is worth noting that the 
dictionary learning of our DCVS can reveal some “denoising” 
- 212 -
國科會補助計畫衍生研發成果推廣資料表
日期:2011/04/12
國科會補助計畫
計畫名稱: 壓縮感測技術及其於多媒體應用之研究
計畫主持人: 康立威
計畫編號: 99-2218-E-001-010- 學門領域: 影像處理
無研發成果推廣資料
期刊論文 1 1 100% 
(1) 1 篇期刊論文
已 被 IEEE 
Transactions on 
Multimedia 接受；
(2) 1 篇期刊論文
於 IEEE 
Transactions on 
Image Processing 
(on revision)；
(3) 3 篇期刊論文
送審中. 
研究報告/技術報告 0 0 100%  
研討會論文 6 6 100% 
篇 (1)PCS2010 ；
(2)ICGEC2010 
(invited 
paper) ；
(3)ICASSP2011 ；
(4)ICIP2011 
(accepted)； (5) 
APSIPA2011 
(invited paper, 
accepted), (6) 
VCIP2011 
(invited paper, 
accepted). 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
