where U and V are orthogonal matrices 
R
TT IVVUU ==  and Σ  
is a diagonal matrix. The first R columns of U and V and the first 
R diagonal elements of Σ  can be used to approach A with 
R=)rank(A  by TRRRR VUA Σ= , RA  is a representative matrix 
A. The result of SVD is a set of vectors representing the location 
of each term and document in the reduced R-dimensional LSA 
space [2]. 
 
3. LANGUAGE MODELING AND SMOOTHING 
METHODS 
 
3.1 LSA Parameter Modeling 
N-gram language models are useful for modeling local 
dependencies of word occurrences but lack of capturing global 
word dependencies. The modeling process leads to the estimation 
of conditional probability )Pr( 1 1
−
+−
q
nqq ww  that characterizes the 
linguistic regularity in the span of n words. When the windows 
size n is limited, the n-gram is weak to capture long distance 
dependencies. Long distance correlation between words is a 
common episode in language caused by the closeness of meaning, 
e.g. the word “stock” and “fund” are both likely to occur in 
financial news. To deal with the long distance modeling, LSA 
approach can be applied to extract large span semantic 
knowledge. Our motivation is that there exists some latent 
structure in the occurrence pattern of words across documents. 
Hence, the n-gram language model can be improved by 
integrating the LSA based large span prediction of word 
occurrence. 
Let word 
qw  denote the predicted word, 1−qH  mean the 
history for 
qw , and )Pr( 1−qq Hw  be the associated language 
model probability. Using n-gram language model 
},,,{ 1211 +−−−− = nqqqq wwwH L  is the relevant history comprises 
the preceding n-1 words. The LSA language model is expressed 
by 
)Pr(),Pr()Pr( 111 −−− == qqqqqq wSHwHw d ,       (3) 
where the conditioning on S reflects the fact that the probability 
depends on the particular vector space arising from the SVD 
representation, and )Pr( 1−qqw d  is computed directly from the 
closeness of 
qw  and 1−qd  in semantic space S. The vector 1−qd  
can be viewed as an additional pseudo-document vector for 
matrix A [3][4][5]. The representation 
1−qv  for pseudo-document 
vector 
1−qd  in space S is given by 
1
11
−
−− Σ= Udv Tqq .                                  (4) 
The pseudo-document vector 
qd  can be obtained recursively in 
the LSA space via [4][5] 
T
q
q
q
q
q
q nn
n
]00
1
00[
1
1 LL
ε−+−= −dd .             (5) 
However, at the beginning of a text document, it is difficult 
to capture long distance word dependencies for calculating 
)Pr( 1−qqw d  due to very short history 1−qH . To overcome this 
weakness, we present a new method to estimate the pseudo-
document vector
1−qd . Our method aims to retrieve the most likely 
relevance document 
1
ˆ −qd  from training documents Ndd ,,1 L  so 
as to represent the pseudo-document vector
1−qd . The LSA 
probability )Pr( 1−qqw d  is replaced by )ˆPr( 1−qqw d . Accordingly, 
the pseudo-document 
1
ˆ −qd  is estimated by 
Niqiq
i
,,1      ),Pr(maxargˆ 11 L== −− ddd
d
.              (6) 
Here, 
1−qd is obtained recursively from (5). The probability 
)Pr( 1−qi dd  is determined by finding cosine of the angle between 
vectors 
id  and 1−qd  in the latent semantic space, i.e. using the 
vectors Σiv  and Σ−1qv  in 
ΣΣ
Σ=ΣΣ=
−
−
−−
1
1
2
11  
),cos()Pr(
qi
T
qi
qiqi vv
vv
vvdd .          (7) 
When q is increased, the most likely document vector 
1
ˆ −qd  
moves around in LSA space. Assuming 
1
ˆ −qd  is semantically 
homogeneous, eventually we can expect the resulting trajectory 
to settle down in the vicinity of the document cluster 
corresponding to the closest semantic content. 
In this study, the LSA language model is exploited by 
integrating the effects of histories from the conventional n-gram 
component },,,{ 121
)(
1 +−−−− = nqqqnq wwwH L  and the LSA 
component 
1
)(
1
ˆ −− = qlqH d  [3][4]. The new language model is 
written by 
∑
∑
−+−−−
−+−−−
−−
−−
−−−
=
=
=
i
i
w
qinqqqi
qqnqqqq
w
n
q
l
qi
n
q
l
qq
l
q
n
qqqq
wwwww
wwwww
HHw
HHw
HHwHw
)ˆPr(),,,Pr(
)ˆPr(),,,Pr(
),Pr(
),Pr(
),Pr()Pr(
1111
1111
)(
1
)(
1
)(
1
)(
1
)(
1
)(
11
d
d
L
L
.         (8) 
)ˆPr( 1−qqw d  is computed from the representations of qw  and 
1
ˆ −qd  in semantic space S, which are provided by 
2/1Σqu  and 
2/1
1ˆ Σ−qv , respectively. The LSA probability is measured by 
2
1
2
1
2
1
2
1
1
1
11
ˆ 
ˆ
)ˆ,cos()ˆPr(
ΣΣ
Σ=ΣΣ=
−
−
−−
qq
T
qq
qqqqw
vu
vu
vud .   (9) 
 
 
3.2 LSA Parameter Smoothing 
In real world, the training corpus is not sufficient to estimate n-
gram model for all word occurrences },,,{ 11 qqnq www −+− L . To 
overcome the insufficient data problem, the parameter smoothing 
0 5 10 20 30 40 50 60
70
75
80
85
90
95
100
105
k nearest words
P
er
pl
ex
ity
bigram + LSA smoothing
LSA bigram + LSA smoothing
 
Figure 2: Perplexity versus k nearest neighbor words 
when applying LSA smoothing for standard bigram and 
proposed LSA bigram 
 
Furthermore, different language modeling and smoothing 
methods are compared as shown in Table 1. Except standard 
bigram, we implement the Bellegarda’s LSA bigram [4] and the 
proposed LSA bigram to evaluate the effect of language 
modeling. We can see that the baseline bigram model has the 
perplexity of 158.3. The perplexities are reduced to 128.7 and 
124.4 by applying the Bellegarda’s LSA bigram and proposed 
LSA bigram, respectively. However, when the Witten-Bell 
smoothing is incorporated, the perplexity is greatly reduced from 
158.3 without smoothing to 122.6 with smoothing. Using the 
proposed LSA bigram, the perplexity of combing Witten-Bell 
smoothing can be improved to 108.7. This indicates the 
importance of adopting smoothing algorithm in language model. 
Furthermore, when the proposed LSA smoothing is used, it is 
obvious to see that the perplexity is reduced to 102 which is 
better than 122.6 of using Witten-Bell smoothing. This is 
because that Witten-Bell smoothing method estimates n-gram 
model using (n-1)-gram while the proposed LSA smoothing 
always adopts nearest n-gram models without using (n-1)-gram. 
On the other hand, the computation times of different methods 
are compared. The results show that the computation overhead of 
using smoothing algorithm is slight. The computation load of 
LSA bigram is much higher than that of standard bigram. This 
result indicates that the smoothing algorithm has larger 
improvement in perplexity with smaller computation cost 
compared to modifying the language model. 
 
 
 
 
 
 
 
 
 
 
Table 1: Comparison of perplexity and computation time 
for different language modeling and smoothing methods 
Language Model 
Modeling 
 Method 
Smoothing 
 Method 
Perplexity 
Reduction 
Rate (%) 
Computation 
Time (minutes)
Bigram N/A 158.3 N/A 48.3 
Bigram WB Smoothing 122.6 22.6 51.3 
Bellegarda’s 
LSA Bigram 
N/A 128.7 18.7 176.7 
Proposed 
LSA Bigram 
N/A 124.4 21.4 161.2 
Proposed 
LSA Bigram 
WB Smoothing 108.7 31.3 163.3 
Bigram LSA Smoothing 102 35.6 52.2 
Proposed 
LSA Bigram 
LSA Smoothing 81 48.8 163.4 
 
 
5. CONCLUSION 
Statistical n-gram modeling is limited in its abilities to represent 
the historical words and estimate the unseen parameters from 
insufficient training corpus. In this project, we presented the new 
language modeling and smoothing methods based on the 
framework of latent semantic analysis. The concept of relevance 
retrieval was adopted so as to exploit a new language modeling 
approach where the most likely pseudo-document was retrieved 
to represent the historical words. The language model was 
estimated according to the closeness of the current word vector 
and the historical pseudo-document vector in the common LSA 
space. To overcome the problem of insufficient training data, we 
performed the LSA smoothing where the bigram of current word 
is computed by interpolating with the bigrams corresponding to k 
nearest words. In the experiments on evaluating Chinese news 
documents, we found that the language modeling performance 
was greatly improved by applying the proposed LSA parameter 
modeling and smoothing algorithms. The proposed methods 
outperformed Bellegarda’s LSA bigram and Witten-Bell 
smoothing. In the future, we are exploring the theoretical rule to 
determine SVD dimension for LSA. We will also investigate the 
effect of the amount of training data on the LSA framework. We 
are applying the proposed language model for the tasks of 
information retrieval and large vocabulary continuous speech 
recognition. 
 
 
 
6. REFERENCES 
[1] M. W. Berry, S. T. Dumais and G. W. O’Brien, “Using 
Linear algebra for Intelligent Information Retrieval”, Society 
for Industrial and Applied Mathematics (SIAM): Review, vol. 
37, no. 4, 1995, pp. 573-595. 
[2] M. W. Berry, “Large scale singular value computations”, 
International Journal of Supercomputer Applications, vol. 6, 
1992, pp. 13-49. 
出席國際學術會議心得報告 
                                                             
計畫編號 國科會計畫 NSC 95-2221-E-006-379 
計畫名稱 自發性語音辨識之強健性特徵擷取, 語音模型及語言模型 
出國人員姓名 
服務機關及職稱 
簡仁宗 國立成功大學資訊工程學系 教授 
會議時間地點 四月十五日至四月二十日 美國 夏威夷 
會議名稱 
(中文) 2007 年聲學語音暨訊號處理國際研討會 
(英文) International Conference on Acoustics, Speech, and Signal Processing 
(ICASSP 2007) 
發表論文題目 
(中文) 遞迴式貝氏迴歸模型及學習 
(英文) Recursive Bayesian Regression Modeling and Learning 
 
一、參加會議經過 
 
出國開會行程報告： 
四月十五日 從台南出發搭乘高鐵至桃園再搭巴士至中正國際機場, 搭乘華航班機直飛夏威
夷檀香山, 到達當地為四月十五日五時三十分, 隨即參加 City Tour, 到珍珠港
參觀, 下午參加小島觀光, 晚上下榻 Ambassador Hotel。 
四月十六日 白天沒有會議議程,即至檀香山國際會議中心辦理報到並領取會議相關資料, 
下午與幾位成大及中研院好友和學生一起至 Waikiki Beach 海灘游泳, 享受難得
的世界級海灘風光,隨即參加下午五時三十分舉行的 Welcome Reception, 與多
位海外學者寒喧問候。 
四月十七日 參加 ICASSP Open Ceremony 並於下午 4:30 至 6:30 之議程「Learning Algorithm 
and Modeling」發表論文「遞迴式貝氏迴歸模型及學習」。 
四月十八日 參加 ICASSP。 
四月十九日 參加 ICASSP。 
四月二十日 參加 ICASSP。 
四月二十一日 在夏威夷檀香山機場搭上午十時飛機。 
四月二十二日 下午五時到達中正國際機場再搭乘高鐵回台南。 
 
科會或教育部也應該繼續秉持此一信念，多鼓勵國內專家學者出國參加國際會議，我們國家
的形象及實力也會因此而提高。另外，我也建議往後我國應該要積極爭取舉辦大型的國際會
議，如此，對國家形象的提昇有很大的助益。這次會議攜回資料名稱及內容有一、會議論文
摘要集，二、會議論文集光碟片(儲存所有會議論文)，三、未來相關國際會議資料(可供國內
學者參考)，四、語音處理相關的期刊數本。 
2.1. Bayesian Support Vector Regression 
Using SVM, we are seeking the optimal hyperplane 
bf T += zwzw ),( , which has the maximum margin between 
support vectors of two classes. Vapnik [13] also extended 
SVM to kernel-based support vector regression (SVR). In 
SVR, training samples of a class are represented by 
ii
T
i by ξε +++= zw .                           (1) 
This ε insensitive loss function [13] was presented to 
obtain the sparseness property of SVM. Hence, we have 
εξξ ),(, * wx iiii fy −=  as a modeling error with value of ε  
being margin width. ε  is empirically defined. Using this 
loss function, the estimation accuracy of outliers will not 
hurt too much. The optimal parameters of w  and b  are 
obtained according to the least square error criterion 
∑
=
∗++ n
i
iiC
1
2 )(
2
1 ξξw .                          (2) 
How to automatically determine regularization parameter C  
for various datasets becomes a crucial issue. Kwok [5] 
illustrated the relationship between SVM and MacKay’s 
Bayesian framework. The hyperparameters of SVM were 
estimated by maximizing the evidence. 
In SVR model [6], the parameter w  was assumed to be 
random and αβ=C . Meaningfully, the hyperparameter α  
controls the model complexity. The hyperparameter β  
controls the training error. The prior distribution of w  is 
Gaussian distributed  
)
2
exp()( 2ww ⋅−∝ ααp .                      (3) 
Assuming training samples are i.i.d., the output distribution 
of training samples D  is [5]  
 )(),,(),,(),( 11 i
n
i ii
n
i ii pypypDp zwzwzw ∏∏ == == βββ ,  (4) 
where an exponential distribution is specified as 
)1(2
)exp(
),,( εβ
ξβββ ε+
−= iiiyp wz .                 (5) 
The posterior probability of w  can be formulated by 
∫= www
ww
w
dpDp
pDp
Dp
)(),(
)(),(
),,( αβ
αββα .            (6) 
The optimal parameter of w  is calculated by maximizing 
this posterior probability. Optimal MAP estimate MAPw  is 
used to approximate the evidence in (6). Optimal values of 
α  and β  are then obtained by iterating the process of 
finding MAPw  and maximizing the evidence. Parameter b  is 
determined correspondingly. The integral in evidence is 
done in accordance with MacKay’s evidence framework [8]. 
In [1], SVR was regarded as a classification problem in the 
dual space. No model learning was concerned in these 
works. 
  
2.2. Relevance Vector Machine 
Accordingly, Tipping [12] presented Bayesian learning 
via a relevance vector machine (RVM). Different from 
Bayesian SVR [6], RVM assumed that parameters α  and β  
are random. The training samples are represented as  
j
n
i
ijij bKwy ξ++= ∑
=1
),( xx .                        (7) 
where the relevance is revealed by kernel function ),( jiK xx . 
Let TT b],[~ ww =  denote model parameters. A normal 
distribution with adjustable variance α  is assumed for w~ . 
The gamma distribution over α  with hyperparameter αλ  is 
given by  
∏
=
= n
i
ip
0
)Gamma()( αλαα .                         (8) 
A zero-mean Gaussian prior distribution over w~  is given as 
∏
=
−− ⋅= n
i
iiwΝbΝp
1
11
0 ),0(),0()
~( αααw .               (9) 
The likelihood function of all the data conditional on 
unknown parameters is expressed by 
⎭⎬
⎫
⎩⎨
⎧
⎥⎦
⎤⎢⎣
⎡ −−−= )~()~(
2
exp
)2(
)(),~( 2/
2/
wΦywΦywy Tn
n
p βπ
ββ      (10) 
where Tn )](,),([ 1 xxΦ φφ K=  is the )1( +× nn  matrix and  
T
niii KK ]1 ),,(,),,([)( 1 xxxxx K=φ . Parameter β  is a precision 
in density (10). The gamma distribution over β  is also 
engaged in 
)Gamma()( βλββ =p .                         (11) 
These priors were assumed to be non-informative. Using (6), 
(9) and (10), we obtain the posterior distribution over w~  as 
a Gaussian distribution 
⎭⎬
⎫
⎩⎨
⎧
⎥⎦
⎤⎢⎣
⎡ −−−= −+
−
)~()~(
2
1exp
)2(
),,~( 12/)1(
2/1
μwΣμw
Σ
αyw Tnp πβ ,  (12) 
where the mean vector is yΣΦμ T β= , the covariance 
matrix is 1)( −+= αIΦΦΣ Tβ , and I  is the )1()1( +×+ nn  
identity matrix. We have the result of μw =MAP . Optimal 
α  and β  are obtained by MacKay’s evidence framework 
[8]. Finally, we can classify a test pattern according to 
likelihood function of (10). 
 
3. RECURSIVE BAYESIAN REGRESSION 
Basically, Bayesian SVR [5][6] provided Bayesian 
solution to SVM and extended SVM paradigm to deal with 
regression problem. Instead of merging support vectors, 
RVM [12] took relevance vectors into account and derive 
the predictive posterior distribution to activate Bayesian 
learning. Prior densities of parameters w~  and their 
hyperparameters β ,α  were defined. It was necessary to 
approximate the multivariate integral of evidence in 
MacKay’s framework [8]. In this study, we focus on 
developing Bayesian and incremental learning strategy for 
kernel regression model. Dummy variable is introduced in 
regression model so as to fit in with the maximum margin 
criterion adopted in SVM or SVR. 
 
