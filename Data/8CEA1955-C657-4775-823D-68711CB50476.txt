II
中英文摘要
中文摘要
早期的文字翻語音系統關心的是清晰度，即是讓聽者能將合成語音準確理
解成文字的能力。近期的文字翻語音系統已經具有很好的清晰度，研究的焦點已
經移至如何增進自然度，讓合成語音更具人性。因此，在合成系統中加入情感，
開發出更具人性的聲音是這項計畫的目標。在計畫中我們建立了一套情緒語音資
料庫，此資料庫使用語意中立的文字來錄製中性及各種(表演出來)的情緒語音。
這套情緒語音資料庫被用於情感分析和文字翻語音系統的建立。成果將可應用在
各式各樣的人機界面，例如輔助機器，娛樂系統，寵物機器人等。
英文摘要
A primary concern of early-generation text-to-speech systems was intelligibility,
or the ability of a listener to accurately transcribe the synthetic speech. As more recent
TTS systems have achieved very good intelligibility, research focus has largely
shifted to the notion of naturalness, or how “human” the synthetic speech sounds. 
Therefore, to incorporate emotions in the synthesis systems is a great lure to carry out
this project. An emotional corpus for TTS is built. A semantically neutral text corpus
is used for the recording of both neutral and emotional (acted) speech. The corpus is
applied to emotion analysis and TTS building. The achievement is useful for making
various man-machine interfaces, such as assistance machines, entertainment systems,
pet robots and so on.
中文關鍵詞
情緒、合成
英文關鍵詞
Emotion, Synthesis
2例如在[23]中便提到這樣的語音只是「讀」出來而非「講」出來的聲音，「讀」
出來的聲音一般具有不同的聲音特性。在[19]中也提到通常這樣「演」出來的聲
音會比較誇張一點。因此，如果用在語音辨識上，用誇張語料訓練出來的模型，
將來用來辨識自然語音時恐怕效果較差。但應用在語音合成時，雖然合成出來的
聲音可能會較誇張一些，但不影響情緒的表達，因此是可行的。
另外，若採用「演」的錄音方式，還須考慮錄音文字的選取。但要建立一
個適合用在資料庫語音合成系統或適用於韻律統計訓練的情緒語音資料庫本來
就是很耗時又耗力的工作，一般需要大量人力的長期介入來選取適當的文字內
容，使得語料能保有音素及韻律的豐富性，卻同時能保持適當的大小。如果要考
慮不同情緒的錄音文字收集，語料文字內容建立的複雜度將會大大提升。如果使
用中性語意的文字來錄製中性及情緒語音，在設計時就不需將文字語意列入考
慮，例如不需在錄製高興語音時特別設計帶有高興語意的文字，因此在設計上會
較為容易。在[19]中已經針對中性文字及帶有情緒語意的文字所錄製的語音加以
韻律上主觀及客觀的比較分析，並發現在情緒語音合成應用上，語料庫的文字設
計可以直接以中性文字即可，雖然「演」出來的效果可能略為誇張。
此外，要建立情緒語音合成系統，另一個很重要的主題為找到影響情緒的
聲學特徵，以及如何控制這些特徵來合成自然的情緒語音。最近的一些研究表
明，韻律成分，包含了聲音的音長、基頻曲線、音量及停頓等，具有一定的篇章
組織功能，並能負載說話者的情緒、態度等方面的副語言學信息。在[16]中經過
聽覺實驗證明僅靠著操控改變韻律即可得到情緒的通訊目的，亦即情緒的表達足
以傳達口語的內容及語者的態度。[3]中也確認音高的改變是情感訊息的重要暗
示。因此在先考慮情緒通訊目的時，先以韻律為分析的重點。則首先必須深入了
解人類的音量、音調、說話速度與重音，即韻律，在情緒上所造成的微妙效果。
在有關情感語調的研究中，韻律扮演了很重要的角色，因此通常會先比對
分析幾種主要情緒語音與相對應的“中性”語音的韻律特徵，包含音高、音長、停
頓、音量等。例如分析幾種主要情緒與對應中性語句重音模式之間的差異，考察
情感表達中基頻抖動現象及在情感語音合成中的作用問題。目前已有許多學者提
出情緒韻律的相關研究，其中[24]提出傳統的韻律特徵，包含音高、音長可以有
效區分出意圖，例如斷言、否定、要求重複等副語言資訊。而[25]中也發現基頻
的範圍比基頻的均值在情緒的表達中更為重要。[26]則發現即使聽的是外國語言
時，也可藉由韻律來偵測出情緒。也有研究[27]證實非言詞的資訊，如換氣、笑、
停頓、長時間無聲及哭等並不單單只是生理上的換氣需要，他們其實也是傳遞豐
富表情及態度的基本元素。[28]的研究顯示基頻的均值及範圍隨著情緒的活化程
度有強烈的變化，但基頻的曲線形狀則沒有證據顯示受到不同情緒的影響。
而在如何建立韻律模型上也有許多方法被提出。例如有學者提出利用主成
4圖 1: 情緒語音合成系統基本架構圖
1.5 結果與討論
首先，我們先針對部分資料庫做一些基本的統計觀察。例如圖 2 為資料庫
中男性語者前 80 句的七種情緒音長統計圖。利用基本的統計可以對情緒語音有
基本的認識。然後利用語音合成系統來做主觀音質的觀察。利用情緒語料庫針對
韻律所做的統計分析可以更加反應情緒在韻律上的表現，合成出來的聲音在主觀
測試上也更能表現情緒的起伏。但目前所採用的合成器及單一的合成單元使得聲
音必須作大幅的韻律調整，使得音質受到較大的破壞。在未來的工作上除了選用
更佳品質的合成器，使得聲音可以容忍更大的調整幅度外，採用更多的合成單元
及增加合成單元選取模組是可行的方向。單元選取技術的引進通常可以相當地提
升合成聲音的品質。此技術主要是從語言學特性或音韻特性上找到一些選擇的標
準以選出最符合目標值的最恰當單元。如果單元語料庫夠大，含有足夠多不同上
下文及韻律變化的組合，則調整單元就變得不需要或只要做小幅的調整。而調整
單元通常是造成聲音品質下降的重要原因。因此這種方式合成出來的語音通常會
比單一單元的合成語音更為清晰自然，從而改善當情緒韻律起伏很大時音質的問
題。
語言處理 音韻處理 語音合成器
語言資料庫 音韻資料庫 單元資料庫
62. 參考文獻
[1] Mohammad Shami, Werner Verhelst, “An evaluation of the robustness of
existing supervised machine learning approaches to the classification of
emotions in speech,” Speech Communication 49 (2007) 201–212.
[2] Zhihong Zeng, Jilin Tu, Brian M. Pianfetti, Jr., and Thomas S. Huang,”
Audio–Visual Affective Expression Recognition Through Multistream
Fused HMM,” IEEE Transactions on Multimedia, Vol. 10, No. 4, JUNE
2008.
[3] Malcolm Slaney, Gerald McRoberts, “BabyEars: A recognition system for
affective vocalizations,” Speech Communication 39 (2003) 367–384.
[4] Dimitrios Ververidis, Constantine Kotropoulos, “Emotional speech
recognition: Resources, features, and methods,” Speech Communication
48 (2006) 1162–1181.
[5] Donn Morrison, Ruili Wang, Liyanage C. De Silva, “Ensemble methods
for spoken emotion recognition in call-centres,” Speech Communication
49 (2007) 98–112.
[6] C. Clavel, I. Vasilescu, L. Devillers, G. Richard, T. Ehrette, “Fear-type
emotion recognition for future audio-based surveillance systems,” Speech
Communication 50 (2008) 487–503.
[7] Zoraida Calejas, Ramo’n Lo’pez-Co’zar,” Influence of contextual
information in emotion annotation for spoken dialogue systems,” Speech
Communication 50 (2008) 416–433.
[8] Tin Lay Nwe, Say Wei Foo, Liyanage C. De Silva,” Speech emotion
recognition using hidden Markov models,” Speech Communication 41
(2003) 603–623.
[9] Chul Min Lee, and Shrikanth S. Narayanan,“Toward Detecting Emotions
in Spoken Dialogs,” IEEE Transactions on Speech and Audio Processing,
Vol. 13, No. 2, March 2005.
[10] Akemi Iida, Nick Campbell, Fumito Higuchi, Michiaki Yasumura, “A
corpus-based speech synthesis system with emotion,” Speech
Communication 40 (2003) 161–187.
[11] Marc Schröder, “Expressing Degree of Activation in Synthetic Speech,” 
IEEE Transactions on Audio, Speech, and Language Processing, Vol. 14,
No. 4, July 2006.
[12] Sahar E. Bou-Ghazale, John H.L. Hansen, “Generating stressed speech
from neutral speech using a modified CELP vocoder,” Speech
Communication 20 (1996) 93-110.
8F0, duration and voice quality, “ Speech Communication 50 (2008)
531–543, 2008.
[25] Murtaza Bulut and Shrikanth Narayanan, “On the robustness of overall
F0-only modifications to the perception of emotions in speech,” J. Acoust.
Soc. Am. 123 (6), June 2008.
[26] Marc D. Pell, Vera Skorup, “Implicit processing of emotional prosody in a
foreign versus native language,” Speech Communication 50 (2008)
519–530.
[27] Chu Yuan, and Aijun Li, “The Breath Segment in Expressive Speech,” 
Computational Linguistics and Chinese Language Processing, Vol. 12,
No. 1, March 2007, pp. 17-32.
[28] Tanja Banziger, Klaus R. Scherer, “The role of intonation in emotional
expressions,” Speech Communication 46 (2005) 252–267.
[29] M. Bulut, S. S. Narayanon, and A. K. Syrdal, “ExpressiveSpeech
Synthesis Using a Concatenative Synthesizer,” Proc. ofICSLP’02, Denver,
pp. 1265-1268, Sep. 2002.
[30] 游恆山, “情緒心理學”, 五南出版社, 2002年
2面對實際問題時的解決方案，更能了解相關單位的實際研發動態。本人參加此次研討會可說受益
良多。
三、考察參觀活動(無是項活動者略)
無是項活動。
四、建議
出席國際學術會議能拓展研究視野，並有機會與來自世界各地相同領域的專家學者共同切
磋討論，國內應多多支持鼓勵參與這類國際性學術活動，以提升國內技術及國際能見度。
五、攜回資料名稱及內容
攜回資料為二零零九年第九屆電機電子工程師學會信號處理及信息技術國際研討會論文集
光碟一片。內容為於二零零九年第九屆電機電子工程師學會信號處理及信息技術國際研討會中
所發表的論文全集。
六、其他
無。
98年度專題研究計畫研究成果彙整表 
計畫主持人：賴玟杏 計畫編號：98-2221-E-327-038- 
計畫名稱：情緒語音合成 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 2 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
