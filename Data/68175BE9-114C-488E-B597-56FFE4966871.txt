?????? 
???????????????
?????????????????
?????????????????
????????????????
?????????????????
?????????????????
?????????????????
?????????????????
?????????????????
?????????????????
?????????????????
??????? 
 ?????????????????
Chi2 ????????????????
???(extended Chi2 algorithm)?????
??????????????????
??(Chi2???, ??? Chi2?????
??????)????????????
??? 
 
???????????????????
????????????? 
 
Abstract 
The Variable Precision Rough Sets (VPRS) 
model is a powerful tool for data mining, as it 
has been widely applied to acquire knowledge.  
Despite its diverse applications in many 
domains, the VPRS model unfortunately 
cannot be applied to real world classification 
tasks involving continuous attributes.  This 
requires a discretization method to pre-process 
the data.  Discretization is an effective 
technique to deal with continuous attributes 
for data mining, especially for the 
classification problem.  The modified Chi2 
algorithm is one of the modifications to the 
Chi2 algorithm, replacing the inconsistency 
check in the Chi2 algorithm by using the 
quality of approximation, coined from the 
Rough Sets Theory (RST), in which it takes 
into account the effect of degrees of freedom.  
However, the classification with a controlled 
degree of uncertainty, or a misclassification 
error, is outside the realm of RST.  This 
algorithm also ignores the effect of variance in 
the two merged intervals.  In this study we 
propose a new algorithm, named the extended 
Chi2 algorithm, to overcome these two 
drawbacks.  By running the software of See5, 
our proposed algorithm possesses a better 
performance than the original and modified 
Chi2 algorithms.  
 
Keywords: VPRS model, data mining, 
discretization, ?-reducts 
 
??????? 
  A number of methods based on the 
entropy measure establish a strong group of 
works in the discretization domain.  This 
concept uses class entropy as a criterion to 
evaluate a list of best cuts, which together 
with the attribute domain induce the desired 
intervals (Nguyen, 1998).  
The ChiMerge algorithm introduced by 
Kerber (1992) is a supervised global 
discertization method.  The user has to 
provide several parameters such as the 
significance level α , and the maximal 
intervals and minimal intervals during the 
application of this algorithm.  ChiMerge 
requires α  to be specified.  Nevertheless, 
too big or too small a α  will over-discretize 
or under-discretize an attribute.  Liu, et al. 
(1997) proposed a Chi2 algorithm that uses a 
ChiMerge algorithm as a basis, whereby the 
Chi2 algorithm improves the ChiMerge 
algorithm in that the value of α  is calculated 
based on the training data itself.  
Tay, et al. (2002) indicated that although 
the Chi2 algorithm automates the ChiMerge 
inconsistency rateξ . 
   Step 2. Calculate the chi-square value: 
        For each numeric attribute, sort data 
on the attribute and use formula (3.2) 
to compute the 2x  value. 
    Step 3. Merge: 
        For a comparison, compute the 2x  
value and corresponding threshold; 
merge the adjacent two intervals 
which have the maximal normalized 
difference and the computed 2x  
value is smaller than the 
corresponding threshold.  If no two 
adjacent intervals satisfy this 
condition, then go to Step 5. 
   Step 4. Check inconsistency rate for 
merger: 
        Check the merged inconsistency rate, 
and if the merged inconsistency rate 
exceeds the pre-defined 
inconsistency rate, then discard the 
merger.  Go to step 5.  Otherwise, 
go to step 2. 
   Step 5. Decrease the significance level: 
        Decrease 0α→α . 
   Step 6. Calculate finer the chi-square value: 
        For each numeric attribute, sort data 
on the attribute and use formula (3.2) 
to compute the 2x  value. 
   Step 7. Finer merge: 
        For a comparison, compute the 2x  
value and corresponding threshold; 
merge the adjacent two intervals 
which have a maximal normalize 
difference and the computed 2x  
value is smaller than the 
corresponding threshold.  If no two 
adjacent intervals satisfy this 
condition, then go to Step 9. 
   Step 8. Check the inconsistency rate much 
finer for a merger: 
        Check the merged inconsistency rate; 
if the merged inconsistency rate 
exceeds the pre-defined 
inconsistency rate, then discard the 
merger.  Go to step 9.  Otherwise, 
go to step 6. 
   Step 9. Decrease finer the significance 
level: 
        Decrease the significance level; then 
stop. 
The formula for computing the 2χ  value 
is: ∑∑
= =
−=χ
n
i
k
j ij
ijij
E
EA
1 1
2
2 )( ,       (3.2)             
where =n  2 ; 
  =k  number of classes; 
 =ijA  number of objects in the ith 
interval, jth class; 
 =iR  number of objects in the ith 
interval ∑
=
=
k
j
ijA
1
; 
 =jC  number of objects in the jth 
class ∑
=
=
n
i
ijA
1
; 
=N  total number of objects ∑
=
=
n
i
iR
1
; 
=ijE  expected frequency of 
N
CR
A jiij
*= . 
If either iR  or jC  is 0, then ijE  is set to 
0.1.  The degrees of freedom of the 2χ  
statistic are one less than the number of 
classes 
Five data sets are demonstrated to present 
the effectiveness of the proposed extended 
Chi2 algorithm.  The five data sets are taken 
from the University of California, Irvine’s 
repository of machine learning databases. 
We ran See5 on both the original data sets 
and the discretized data sets.  The parameters 
