processing of hardware, quickly to execute the training of 
various features, such as video, audio, information, etc, 
significantly reduce the time required for training. To 
compare with the integer data type of the AdaBoost 
algorithm. We used real data type of the AdaBoost 
algorithm that can be more clearly express its classification 
results. HAT has been implemented with VHDL hardware 
description language. After software simulation, HAT has 
been synthesized and downloaded into Xilinx Virtex-5 
FPGA to test. In addition, data sorting also is an important 
process, after analysis and comparison of some sorting 
algorithms, we decided to use the merge sort algorithm 
because of it computing behavior is more in line with the 
characteristics of parallel processing, and suitable for 
hardware implementation. 
The remainder of this paper is organized as follows. 
Section 2 explains the idea of AdaBoost in brief. Section 3 
presents the details of Gentle AdaBoost algorithm and the 
proposed parallel hardware architecture. Section 4 shows 
some experimental results and the corresponding analysis. 
The conclusion is provided in Section 5. 
 
Figure 1 : Haar-like features 
II. RELATED WORK 
A. AdaBoost algorithm 
Viola [1] proposed the application that AdaBoost 
algorithm combined with integral imaging to detect human 
faces in real time. In the paper, Viola use four kinds of 
simple Haar-Like features, through the change of enlarging 
and displacement to produce a large number of features, 
and combined with the application of integral images , it 
can be able to quickly calculate the required image 
information of training or detection. Figure 1 shows the 
four kinds of features. In addition, the paper also proposed 
an integer data type of image training algorithm, with the 
above-mentioned image information , it can produce some 
weak classifiers then weak classifiers will be compose of a 
strong classifier that use in image training. Hiroki Sugano 
[3] proposed an integer data type AdaBoost algorithm with 
integral images, Haar-Like features, and the LDA analysis 
method to train LDA weak classifier in hardware. Masayuki 
Hiromoto [4] proposed to use the pre-train classifier 
information by software, then Implement the architecture of 
image detection in hardware, in the paper, features will be 
divided into a number of parallel implementation of the 
hardware modules, this approach will improve the 
performance of image detection. [6-9] are some related 
applications of AdaBoost algorithm. 
B. Gentle Adaboost Algorithm 
Rainer Lienhart [2] rotated the feature of [1] and added 
new features to produce a new feature set. In order to be 
more accurate when executing face detection, he also 
proposed an algorithm, called Gentle AdaBoost. The 
difference with discrete AdaBoost is that Gentle AdaBoost 
uses real data type, the accuracy of computing result and 
classifier information is better than integer data type 
(discrete) AdaBoost algorithm. 
The base of executing algorithm is N training samples 
(X!,…,  X!), corresponding class labels (Y!,…,  Y!,Y=+1,-1), 
weights (W!=1/N, i=0,…,N), and used M features on all 
image N to create feature vectors that be a M x N size 
feature table. The following are the algorithm steps what we 
summarized: 
STEP 1: For each feature vector V, use of the variable 
threshold to find the expected value (L)(1) of all 
training images that feature value is less than 
 
Figure 2 : Block diagrams of proposed architecture. 
 
FSM will enter an initial state when HAT start, upon 
receiving the enable signal, then entered the Find Best 
Threshold state that used the feature table, class labels, and 
weights to find the minimum error rate of all features on all 
training samples, and its optimal threshold, then entered the 
strong classifier data generate state(SCDATA Generate), it 
using the information of weak classifiers to score all the 
training images, construct the data that every weak 
classifier determine each training image whether the human 
face (positive) or not. For each training image, if the score 
that sum of all the weak classifiers closer to +1, the 
probability of face in this training image is higher, on the 
contrary, the scores closer to -1 representing the training 
image is non-face (negative).  
 
Figure 3 : Finite state machine of HAT 
Because of the characteristic of parallel architecture, the 
data of the strong classifier start to sort at the same time. 
After sorting is completed, FSM entered the state of 
Calculate False Alarm Rate. This module will calculate the 
SC threshold to confirm that whether the classification 
results of weak classifiers correct or not, if the score of all 
weak classifiers to a non-face (negative) training image is 
greater than SC threshold, it represented misclassification. 
Using the total number of misclassification images divided 
by the total number of non-face images, we can get a false 
alarm rate to determine whether continue training or not. 
The classification results will be used to update the weight 
of training images and the factor. If the false alarm rate 
does not converge to the target, then enter the state of 
Normalize Weight , normalize all of weights that makes the 
sum of weights is 1 and continue to the next round of 
training, otherwise, enter the state of Training Done and 
output the information  of classifier. 
Due to the hardware resource of float point unit is more 
than fixed point unit and the computing speed has been 
slower, so the data in HAT used fixed-point notation to 
represent. Through the analysis of algorithm, the length of 
C. Hardware sorter 
Henry Barnor [5] proposed that merge sort is an 
algorithm based on divide and conquer, this characteristic is 
suited to implement a hardware sorter. Therefore, according  
to his architecture, we implement it in HAT, figure 6 shows 
the sort core architecture. Sort core contains one Data in  
 
Figure 6 : The structure of Sort core 
 
 
Figure 7 : The parallel architecture of Sorter 
 
FIFO, Splitter, two Data Buffer FIFO, and merger, in order 
to make sort flow pipelined, we use some sort cores to 
cascade into a complete hardware sorter. Figure 7 shows the 
sorter architecture. The number of sort core will change in 
accordance with the number of input data, if input N data, 
the number of sort core will be  logN. When sorting enable, 
sorter will automatically read N unsorted data, through the 
pipelined execution, then output N sorted data and strong 
classifier threshold (SC threshold). 
IV. EXPERIMENTAL RESULTS 
Table 1 shows the resource utilization of HAT on the 
Xilinx Virtex 5 FPGA, through the test of structure 
implementation, we get HAT that processing a single 
feature value needs 7.67 hardware clock cycles. in the 
experiment of executing AdaBoost algorithm with 85 
million Haar-like feature values, the PC (Intel I7 920，4 
cores，  2.66GHz， 6G DDR3 1333MHz) needs 17.3 
seconds, but HAT in a 100MHz FPGA only needs 6.52 
seconds. Table 2 shows the hardware clock cycles of each 
component. 
Table 1 Resource utilization of HAT 
 
Table 2 Clock cycles consumption 
 
V. CONCLUSIONS 
In this paper ,we propose a high-performance hardware 
AdaBoost trainer(HAT), through the parallel processing of  
hardware, implement the Gentle AdaBoost algorithm and 
quickly to execute the training of various features, In this 
paper, we simplified some part of the Gentle AdaBoost 
algorithm operation to make it more suitable for the 
hardware architecture. We also used a lookup table to 
replace hardware divider. Finally we show that in the 
comparison of computation time, HAT is much faster than 
pure software trainer. 
REFERENCE 
[1] PAUL VIOLA, MICHAEL J. JONES ,” Robust Real-Time Face 
Detection”, International Journal of Computer Vision 57(2), 137–
154, 2004. 
[2] Rainer Lienhart, Alexander Kuranov, Vadim Pisarevsky,” 
Empirical Analysis of Detection Cascades of Boosted Classifiers 
for Rapid Object Detection”, MRL Technical Report, May 2002. 
[3] Hiroki Sugano, Hiroyuki Ochi, Yukihiro Nakamura, Ryusuke 
Miyamoto,” Hardware Implementation of Discrete AdaBoost 
Algorithm for Run-Time Learning of Object Recognition”, 
A High-Performance Parallel Hardware Architecture for Gentle AdaBoost Trainer 
Abstract—AdaBoost (Adaptive Boosting) algorithm has 
been used widely in the domain of machine learning. 
Depends on feature vector length, the training time 
AdaBoost demands may leap from several hours to 
several days even weeks. For this reason, we have 
proposed a high-performance parallel AdaBoost trainer 
hardware architecture in this paper, which can quickly 
execute Gentle AdaBoost training algorithm for various 
types of features, e.g. visual objects. Besides parallelism, 
some of the key parts of the original algorithm, like best 
threshold searching, have been reduced and optimized 
to get further speeding up. According to our 
experimental results, for 85 million data points (1000 
face samples x 85000 Haar-like features), the 2.66GHz 4 
cores desktop PC requires 17.3 seconds for software 
training, but the FPGA implementation of our proposed 
hardware trainer only consumes 6.52 seconds at 
100MHz clock rate. 
 
Keywords-machine learning; AdaBoost; trainer; 
parallel architecture; FPGA; algorithm 
I.  INTRODUCTION  
Machine learning is one of the fields of artificial 
intelligence, the main research is to let the computer 
learning from the past experience, past data automatically. 
At present, this technology already has a very wide range of 
applications, such as biological feature recognition, search 
engine, voice recognition, handwriting recognition, 
computer vision, robots, and so on. For example, computer 
vision is using cameras and computers to replace the human 
eye that doing image processing on the target, when the 
images become suitable of detection ,then executing 
automated recognition, tracking, and surveying. In the 
present society, human beings increasingly focused on their 
own safety, home security and safety in the workplace, 
many criminal cases often need to be resolved by the 
surveillance system. Nevertheless, monitored around the 
clock brought a large number of image data, that is no 
longer only rely on manual methods to track the object in 
image, such as license plate, face, etc. Therefore, the 
automated image detection technology that combined with 
machine learning algorithms and image analysis techniques 
is gradually become the trend of future surveillance system. 
In the domain of machine learning, there includes several 
important algorithms, such as AdaBoost (Adaptive 
Boosting), etc. AdaBoost is a kind of classification 
algorithm, it composed of some selected classifiers, using 
these classifiers, the input data will be classified into two or 
more sets by updating it weights. AdaBoost has been used 
in many researches of image analysis. The domain of image 
analysis can be divided into the research of feature, image 
training and imaging detection. In the image training and 
detection, there is AdaBoost algorithm. When performing 
image analysis, it must go through image training to select 
the most suitable features for image detection. Then use 
these feature to perform image detection repeatedly. 
However, even if the image training only needs to execute 
one times before image detection, but if the number of 
features is too large, it will lead a lot of data operations 
during training and the execution time often takes a few 
days or even a few weeks. 
In this paper, we proposed a high-performance hardware 
AdaBoost trainer (HAT). We use hardware-based approach 
to achieve AdaBoost algorithm. Through the parallel 
threshold and the corresponding left error rate (ϵ!)(2), 
as well as the expected value (R)(3) of all training 
images that feature value is greater than threshold and 
the corresponding right error rate (ϵ!)(4) ,then ϵL and ϵR are added to find a feature that has the minimal 
total error rate(ϵ!)(5), making it a weak classifier. 
Suppose i is the current location of execution 
threshold, the equation as described below: L! = !!!!!!!!!!! Y!                                      (1) 
ϵ!! = !!!!!!!!!!! (L! − Y!)!                         (2) 
R! = !!!!!!!!!!! Y!                                    (3) 
ϵ!! = !!!!!!!!!!! (R! − Y!)!                       (4) ϵ!! = min  (ϵ!! + ϵ!!)                             (5) 
Which WL! is sum of all weights that feature value is 
less than threshold, otherwise is  WR!. WL! = W!!!!!!!                                    (6) WR! = W!!!!!!!                                   (7) 
STEP 2: Use the weak classifier information, ratings on all 
training images to generate strong classification 
information. 
STEP 3: Sort strong classifier information to determine 
strong classifier threshold (SC Threshold). 
STEP 4: Calculate false alarm rate and updated weights 
with the following equation: 
W! =   W! ∙ exp  (−Y ∙ H!)                  (8) 
          H! is the classification result of weak classifier, if the 
feature value of training image is greater than 
threshold, then H! sets to R!,otherwise,sets to L!. 
STEP 5: If the false alarm rate does not reach the target, 
then normalized all weights, make it total is 1 and 
return to Step 1.otherwise, training done. 
Since the large and complex operations, training process 
usually takes a long computing time. Therefore, the 
hardware trainer architecture we proposed will take the 
characteristics of parallel processing of hardware, pipelined 
executing image training to reduce the training time. In the 
algorithm part of HAT , we used Gentle AdaBoost 
algorithm to plan the overall structure, from the above 
description, the accuracy of operation result of Gentle 
AdaBoost is better than discrete AdaBoost.  
III. PROPOSED HARDWARE ARCHITECTURE 
Figure 2 shows an overview of the proposed 
architecture, which consists of algorithm circuit, sorter and 
data rams.  
A.  Algorithm circuit  
Algorithm circuit is an implementation of the Gentle 
AdaBoost algorithm with pipelined architecture. We used a 
pre-designed finite state machine (FSM) to combine with 
HAT, figure 3 shows the finite state machine of HAT.  
data are set to 16 bits, 1-bit indicating sign ,5-bit indicating 
integer part, and 10-bit indicating decimal part, this can be 
accurate to decimal point following the third place. 
B. Computational Cost Reduction 
According to our assessment, because of the loop and 
the division operation, the step 1 of Gentle AdaBoost will 
take up the most of hardware clock cycles of all algorithm 
circuit, if the arithmetic circuit implement according to 
equation (1) (2) (3) (4), it caused the operation path too 
long and leads to poor processing performance. Figure 4 
shows the arithmetic circuit before simplify the equation. 
For this reason, we simplified these equations: L! = !!!!!!!!!!! Y! = !!!! W!Y!!!!!!!                   (8) 
ϵ!! = !!!!!!!!!!! (L! − Y!)!  
= !!!! W!(!!!!!! L!! − 2L!Y! + Y!!)  
= !!!!!! W!!!!!!! − !!!!!! W!Y!!!!!!! + !!!! W!Y!!!!!!!!   
= L!! − 2L!! + !!!! W!Y!!!!!!!!   
= !!!! W!Y!!!!!!!! − L!!                        (9) 
R! = !!!!!!!!!!! Y! = !!!! W!Y!!!!!!!            (10) 
ϵ!! = !!!!!!!!!!! (R! − Y!)!  
= !!!! W!(!!!!!! R!! − 2R!Y! + Y!!)  
= !!!!!! W!!!!!!! − !!!!!! W!Y!!!!!!! + !!!! W!Y!!!!!!!!     
= R!! − 2R!! + !!!! W!Y!!!!!!!!  
= !!!! W!Y!!!!!!!! − R!!                       (11) 
From equation (8) (9), the simplified equation changes 
the original computing flow, it become easier to achieve 
parallel processing with hardware architecture, equation 
(10) (11) is the same. In addition, we found that the 
execution cycles of hardware divider still more than other 
operations, this will lead the processing cycle does not 
average between hardware components. To solve this 
problem, we used a 2K Bytes (1K*16bits) division table to 
perform division operations. As the weight fall in the range 
of values between 0 and 1, the address of division table 
only needs 10 bits wide for the fractional part. So, the 
processing cycle of all components can be more evenly to 
reduce waiting time. Figure 5 shows the arithmetic circuit 
after simplify the equation. When scan feature table 
completely, it will find a feature with minimal error rate, 
this feature is the best weak classifier of current training 
phase, at this time , the corresponding  feature id, threshold, 
L, and R will be write to the strong classifier information 
ram(SC Information Ram). 
 
 
Figure 4 : Arithmetic circuit before reduction 
 
 
Figure 5 :  Arithmetic circuit after reduction 
 
International Workshop on Smart Info-Media Systems ,December 
8, 2008. 
[4] Masayuki Hiromoto, Hiroki Sugano, Ryusuke Miyamoto,” 
Partially Parallel Architecture for AdaBoost-Based Detection With 
Haar-Like Features”, IEEE TRANSACTIONS ON CIRCUITS 
AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 19, NO. 1, 
JANUARY 2009. 
[5] Henry Barnor, ” Systolic Algorithm Design-Systolic Merge sort in 
Hardware”, October 11, 2004. 
[6] Jianxin Wu, S. Charles Brubaker, Matthew D. Mullin , James M. 
Rehg,” Fast Asymmetric Learning for Cascade Face Detection”, 
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND 
MACHINE INTELLIGENCE. 
[7] Hung-Chih Lai, Marios Savvides, Tsuhan Chen,” Proposed FPGA 
Hardware Architecture for High Frame Rate (>100 fps) Face 
Detection Using Feature Cascade Classifiers”, BTAS 2007,2007. 
[8] Huaifeng Zhang, Wenjing Jia, Xiangjian He, Qiang Wu,”Learning-
Based License Plate Detection Using Global and Local Features”, 
International Conference on Pattern Recognition - Volume 
02,2006. 
[9] Michael J. Jones,Paul Viola,” Face Recognition Using Boosted 
Local Features”, IEEE International Conference on Computer 
Vision 2003, April 2003. 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：蘇益慶 計畫編號：99-2221-E-214-066- 
計畫名稱：高效能硬體 AdaBoost 訓練器之設計與應用 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 1 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 2 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 1 100%  
研究報告/技術報告 1 1 100%  
研討會論文 0 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 1 1 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
