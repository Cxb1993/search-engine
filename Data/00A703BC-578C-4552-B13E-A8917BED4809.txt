等物體的定位方法，其中最普遍的方式包括
了推算航法（dead-reckoning）、導航時使用活
動信標（active beacons）、基於標的物的導航
方法、基於地圖的導航方法、或使用差分型
全球定位系統（differential global positioning 
system, DGPS）和基於影像及視覺的導航方
法。 
本計畫的研究內容則屬於上述方式中的
最後一項：基於影像及視覺的自主定位及導
航。以視覺為基礎的定位方式（vision based 
localization）通常利用攝影機進行資訊的採
集，透過即時的影像分析來取得定位所需的
景物特徵資訊，並以三角測量、校正後的相
機或是其它電腦視覺相關的三維測量技術來
決定物體的所在位置 [2]，進而提供導航及避
撞的相關訊息。對於靜止或是移動物體的定
位，包含以視覺影像為基礎的方式，在傳統
上為機器人學及自動控制的研究領域。其相
關的研究包括了 Leonard 與 Durrant-Whyte 所
提出的運用 extended Kalman filter（EKF）的
方式，利用已知的信標位置作為其基於模型
的定位演算法的根據 [3]。Betk 及 Gurvits 則
假設環境的地圖及標地物為已知，提出了與
標地物數量成線性運算量的演算法，但他們
僅提供了模擬的結果而無實際平台的移動導
航 [4]。Sim 與 Dudek 介紹了一個基於標的物
地圖的方法，此定位的方式結合了統計與特
徵擷取的理論，利用學習的方式對場景裡某
些可視的特徵來進行處理，並將某些影像中
對應的相同特徵以線性位置內插的方式來獲
得相機的估測位置 [5]。以上的方法皆依賴環
境中預先設置的已知資訊（如標地物等），因
此較不易應用於非結構性的戶外環境。 
近年來由於影像設備的價格滑落以及視
覺監控系統的廣泛運用，物體的運動分析及
行動機器人的自主導航逐漸成為電腦視覺研
究領域的重要方向之一。然而有別於一般機
器人研究上所著重的精確度以及對於不同資
料來源（如聲納、雷射測距、影像等）的整
合，電腦視覺研究上所強調的則為大範圍的
資料擷取以及不受限於特定的環境或是預先
架設輔助的標地物。因此，在於影像即時處
理的速度上及定位資訊精確度的改善方面，
硬體的設置及演算法的開發皆為相當具有挑
戰性的課題。 
因此，本研究擬開發一套可架設於移動
平台上的視覺系統，我們將著重於植基於視
覺的自主定位，以期能達到行動機器人的定
位、導航及避撞，以及與其它感測方式（如
聲納、雷射、DGPS 等）的結果進行分析比較，
並探討其資訊整合的必要性及可行性。不同
於一般辨識固定標的物或使用事前假設環境
模型的方法（皆需依賴辨識用的物體或對環
境的已知資訊），我們提出的方法將利用從影
像序列中自動擷取出的特徵對應點和從場景
中獲得的三維資訊來建立此一自主定位系
統。此一方式不僅具有電腦視覺及行動機器
人研究上的價值及挑戰性，在系統的設定上
也不需因不同的使用環境下而做大幅度的修
改或是增加額外的裝置。 
 
三、研究方法 
 
本計劃於移動平台上建構之基於視覺的
自主定位系統，在方法及步驟方面可分兩年
執行，第一年著重於影像序列中特徵點的擷
取、對應點的匹配與追蹤等演算法的開發，
以及立體視覺自主定位系統所需之三維重建
的相關研究；第二年則進行 HOPIS 系統的幾
何建構、校正與自主定位，以及與 DGPS 或
雷射掃描測距裝置的系統整合。圖 1 為本計
畫所提出之自主定位系統流程圖。 
 
 
圖 1：視覺定位系統流程圖 
 
3.1 特徵點的擷取與對應點的搜尋 
 
本計畫所提出的自主定位系統是以擷取
影像序列中的特徵來做為主要的處理資訊，
在一般的影像中“特徵點”通常具有較易處理
 
2
3.3 連續影像特徵點追蹤 
 
對於已校正的立體視覺系統而言，獲得
影像匹配點之後即可透過相機幾何模型計算
其對應的三維點座標。而對於移動中的視覺
影像系統，則須取得在不同時間所獲得的三
維特徵點之間的關係。一般而言，欲取得視
覺系統移動的三維距離及方向，我們必須知
道前後時間連續影像中特徵點的對應關係，
若僅考慮較為簡單的搜尋方法，此一特徵點
的追蹤可採用相同於上述左右影像對應點搜
尋的方式來進行。然而視覺系統在行進間其
前後時間連續影像的變化幅度通常較大，而
所需的搜尋範圍及計算量也相對提高。因此
除了上述的方式外，我們另外使用了運動向
量場（motion field）的觀念對搜尋區域進行可
變性的範圍限制。根據運動向量場的基本公
式 
f
y
f
xy
xf
Z
fTyT
v
f
x
f
xyyf
Z
fTxTv
xy
zx
yz
y
yx
zy
xz
x
2
2
ωωωω
ωωωω
−++−−=
−++−−=
 
影像平面上任一點(x, y)的移動向量(vx, vy)皆
可由ωx, ωy, ωz分別為對x, y, z軸的角速
度）、Tx, Ty, Tz（分別為x, y, z方向的移動速
率）、f（相機焦距長）以及Z（特徵點的實際
三維深度）決定。 
 
3.4 位置的測量與最佳化 
 
從立體視覺的理論可得知，獲得左右影
像的對應點和相機的內外部參數，就能計算
得到特徵點在三維空間中的位置。一般而
言，此一自主定位的方式可利用所追蹤到連
續影像中特徵點之間的關係，並運用移動估
測的演算法就能重獲相機在連續影像中三維
的相對移動距離。理論上移動估測演算法只
需三組三維的對應點就能計算出旋轉矩陣 R
和移動向量 t，但實際上由於影像雜訊或是取
得資料的正確性不夠高，單憑三組點來估計
相機的相對移動方位通常並不夠穩健。為了
解決此一問題，我們採用一般基於奇異值分
解（Singular Value Decomposition, SVD）的三
維移動估測演算法，再利用 RANSAC 的方式
[8]來去除受雜訊或不正確對應點的干擾，並
使用非線性最佳化的方式來獲得更加精確的
距離估計值。 
就最基本的物體移動估測而言，一般所
謂的三維移動估測也可稱之為疊合或姿態估
算，其目的在於找出不同座標系統之間的轉
動和移動關係。但由於三維重建所得之特徵
點座標未必完全正確，例如可能存在的匹配
點搜尋錯誤或是遠距離的特徵點無法精確得
知，因此我們將先前所獲得基本的移動估測
結果進行最佳化，其目的即是在於去除受到
雜訊、不穩定光源、或出現錯誤對應點等的
干擾，使得所獲得的移動估測結果能更加的
強健。本研究採用視差空間（disparity space）
的觀念及其特性 [9]為基本的架構，並運用基
於隨機取樣的方式（RANSAC）選取所有特
徵點中最佳的數十組三維對應點（實際對應
點數目的選取通常需視其所需的運算量而
定），在視差空間中進行相機移動估測的最佳
化。 
對於三維空間中的任一點M = (X, Y, 
Z)T，經過一轉動R和移動t的剛性運動，可得
到一新的位置M’ = (X’, Y’, Z’)T，且在齊次座
標中可表示成 
⎥⎦
⎤⎢⎣
⎡⎥⎦
⎤⎢⎣
⎡=⎥⎦
⎤⎢⎣
⎡
1101
' MRM t  
假設 M 投影在左影像上為點座標(x,y)相對應
的視差為 d，若令 Tdyx ),,(≡ω 為視差空間中
的一點，則上式可改寫為 
⎥⎦
⎤⎢⎣
⎡Γ⎥⎦
⎤⎢⎣
⎡
1
~
1
Mω
 
或是 
⎥⎦
⎤⎢⎣
⎡⎥⎦
⎤⎢⎣
⎡
1
),(~
1
' ωω
tRH  
其中 H(R,t)為單應性轉換函數(Homography) 
1
10
),( −Γ⎥⎦
⎤⎢⎣
⎡Γ= tt RRH  
因此在立體視覺的視差空間中，任意的
三維點都能透過單應性轉換函數 H(R,t)來進
行一剛性歐基里德轉換，所以給定歐基里德
的移動 R,t 和相機的內部參數，就能直接地推
導出此轉換函數 H(R,t)，反之亦然。 
根據上述的視差空間原理，我們開發了
 
4
其相對誤差皆小於 6%，而平均誤差亦僅約
3%。 
 
表 1：執行效能分析 
 
Step High  
Accuracy 
Low 
Computation
Feature 
Detection 
63 ms 35 ms 
Stereo 
Matching 
58 ms 25 ms 
Feature 
Matching 
47 ms 9 ms 
Motion 
Estimation 
158 ms 51 ms 
Nonlinear 
Optimization 
50 ms 16 ms 
Total Time 
Per Frame 
376 ms 136 ms 
 
 
圖 5：視覺定位精確度分析 
 
五、結論 
 
在本研究中我們提出了一個適用於行動
機器人的視覺自主定位系統及相關演算法，
並以定量的方式分析了其精確度、效能與可
行性。在為來的研究上我們將搭配全像鏡的
使用，以較為寬廣的視野來達成精確定位的
目的，並使用 SIMD (Single Instruction 
Multiple Data)等指令集來加速運算上的效能。 
 
六、參考文獻 
 
[1] B. K. P. Horn, Robot Vision, McGraw-Hill 
Book Company, 1986. 
[2] E. Trucco and A. Verri, Introductory 
Techniques for 3-D Computer Vision, 
Prentice Hall, 1998. 
[3] J. Leonard and H. Durrant-Whyte, “Mobile 
robot location by tracking geometric 
beacons,” IEEE Trans. Robotics and 
Automation, 7:376–382, 1991. 
[4] M. Betke and L. Gurvits, “Mobile robot 
localization using landmarks,” IEEE Trans. 
Robotics and Automation, 13:251–263, 
1997. 
[5] R. Sim and G. Dudek, “Mobile robot 
localization from learned landmarks,” In 
Proc. IEEE/RSJ Conf. on Intelligent Robots 
and Systems (IROS), pages 1060–1065, 
1998. 
[6] C. Harris and M. Stephens, “A combined 
corner and edge detector,” In Alvey 
Conference, pages 147–152, 1988. 
[7] R. Deriche, Z. Zhang, Q. Luong, and O. 
Faugeras, “Robust recovery of the epipolar 
geometry for an uncalibrated stereo rig,” In 
European Conference on Computer Vision, 
pages A:567–576, 1994. 
[8] M. A. Fischler and R. C. Bolles, “Random 
sample consensus: a paradigm for model 
fitting with applications to image analysis 
and automated cartography,” Commun. ACM, 
24(6):381–395, 1981.  
[9] M. Agrawal, K. Konolige, and L. Iocchi, 
“ Real-time detection of independent motion 
using stereo,” In IEEE Workshop on Motion 
and Video Computing, pages II: 207–214, 
2005.  
[10] C. L. Zitnick and T. Kanade, “A 
cooperative algorithm for stereo matching 
and occlusion detection,” IEEE Trans. 
Pattern Anal. Mach. Intell., 22(7):675–684, 
2000.  
[11] R. Tsai. “A versatile camera calibration 
technique for high-accuracy 3dmachine 
visionmetrology using off-the-shelf TV 
cameras and lenses,” IEEE Trans. Robotics 
and Automation, 3(4):323–344, 1987. 
 
6
