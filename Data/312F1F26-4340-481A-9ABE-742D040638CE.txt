 2
 
整合支援向量迴歸至小腦模式控制器與 ㄧ般基底函數之小腦模
式控制器之研究 
 
A study on the embedded SVR on CMAC and CMAC-GBF 
計畫編號：NSC 98-2221-E-197-020- 
執行期限：98 年 8 月 1 日 至 99 年 7 月 31 日 
主持人：莊鎮嘉          職稱：副教授 
電子信箱：ccchuang@niu.edu.tw 
執行機構：宜蘭大學電機系 
 
一、中文摘要 
在本計劃中, 我將提出一個新架構與學習方
式－整合支援向量迴歸 (SVR)至小腦模式控制器 
(CMAC)與  ㄧ般基底函數之小腦模式控制器 
(CMAC-GBF) ， 稱  SVR-CMAC 與 
SVR-CMAC-GBF。由於支援向量迴歸的性能與其
參數相關，在此計畫中支援向量迴歸的參數選擇
方式將被提出。另外，在我提出的 SVR-CMAC 
與 SVR-CMAC-GBF 架構中，修正漢明距離量測 
(Modified Hamming distance measure) 將被引入 
SVR-CMAC 與  SVR-CMAC-GBF 的核心函數 
(kernel function)。更重要是此核心函數極容易修
正現存支援向量迴歸的軟體來達成。除此之外，
SVR-CMAC 與  SVR-CMAC-GBF 所需記憶體
大小是與支援向量  (SVs) 有關。然而，傳統 
CMAC 與核心 CMAC ( kernel CMAC) 所需記
憶體大小是與輸入變數之維度有關。因此，傳統 
CMAC 與核心 CMAC ( kernel CMAC) 所需記
憶體大小會隨著輸入變數之維度呈指數方式增
加。最後，我將我提出新架構與學習方式 
(SVR-CMAC 與與 SVR-CMAC-GBF) 應用於動
態系統與控制上並與傳統  CMAC 與核心 
CMAC 做一比較與分析。 
 
關鍵詞：支援向量迴歸、小腦模式控制器、ㄧ般
基底函數之小腦模式控制器。 
 
英文摘要 
Recently, the support vector machine (SVM) 
is an important technique in the soft computing 
field. Basically, the support vector regression 
(SVR) is an extension version of SVM. That is, the 
SVM can be solves the regression problems, called 
support vector regression, when Vapnik’s 
ε -insensitive loss function is used. For the 
regression problem, the SVR tries to find the 
estimated function, which is expressed as a linear 
combination of a subset of training data (called 
support vectors) by solving a linearly constrained 
quadratic programming (QP) problem. Due to the 
SVR approach is also applied into various 
applications, the SVR approach is incorporated 
into CMAC and CMAC-GBF that named as 
SVR-CMAC and SVR-CMAC-GBF in this project. 
In the two approaches, the modified Hamming 
distance measure is used as the kernel function in 
the SVR. Furthermore, the existed SVR software is 
easily modified to implement the SVR approach 
with the new Gaussian kernel function. However, 
the selections of parameters are important matters 
in SVR approach. Then, the regularization 
parameter and epsilon must be determined and 
discussed in the SVR-CMAC. In the literature, 
some of selecting methods have been proposed to 
select those parameters. In this project, the 
selecting methods of those parameters are also 
proposed in the SVR-CMAC and 
 3
these new Gaussian kernel functions. However, the 
regularization parameter and epsilon must be 
determined in the SVR-CMAC and 
SVR-CMAC-GBF. Some of selecting methods 
have been proposed to select those parameters [20, 
21]. In [21], authors showed that the regularization 
constant and epsilon have a smooth effect on the 
results of SVR. For the selection of epsilon (ε ), it 
is well-known that the value of ε  should be 
proportional to the input noise level ψ , that is 
ψε ∝ [22,23]. However, the input noise level is 
unknown in general. In this study, the concept of 
repeated SVR approach [24] is used to estimate 
the input noise level. Additionally, the 
regularization constant controlled the tradeoff 
between the model complexity and approximation 
accuracy. In [25], the regularization constant was 
set as larger than the range of responsive values in 
the training data as possible is shown. In [20], the 
regularization constant is set to three times the 
standard deviation of the training response values 
(outputs). Moreover, in [21], the authors suggested 
that the regularization constant should be set to be 
sufficiently large. 
 
本計畫的主要目的是 
 
The main purpose of this project have 
(1) Development of the SVR and the CMAC 
and CMAC-GBF controller related 
software。 
(2) We will propose and analysis to SVR as a 
front mapping to deal with the CMAC and 
CMAC-GBF learning results. And then 
compared with the traditional CMAC and 
CMAC-GBF. 
(3) On the other hand, we will explore the 
SVR-CMAC and SVR-CMAC-GBF in the 
use of memory on the characteristics and 
thus provide a more suitable method. 
(4) By Matlab software analysis, to verify 
developed by the SVR to CMAC and 
CMAC-GBF system integration technology 
and the applicability of the accuracy. 
(5) On the whole, this project can enhance the 
study of intelligent systems, software 
development can be carried out on the 
exchange, in order to upgrade the 
intelligent control system based on 
software development research. 
 
三、研究方法及成果 
The new methods, SVR-CMAC and 
CMAC-GBF, utilize support vector regression as 
the learning scheme of CMAC and CMAC-GBF. 
Fig.1 and Fig. 2 show that the structure of 
proposed method (i.e. SVR-CMAC and 
CMAC-GBF), respectively. The memory element 
selection vector ia  is used as the input of the 
support vector regression model. Then, the output 
is computed from a support vector regression 
model for a given input. The construction of the 
support vector regression model and the 
computation of the output for a given input will be 
introduced as followed.  
The SVR approach is used to approximate an 
unknown function with a set of noisy data 
( ){ },...,pkyk 1 ,,ak = . Assuming that a set of basis 
functions ( ){ },..,lsgs 1 ,a =  is provided, a family of 
functions exists that can be expressed as a linear 
expansion of the basis functions. Restated, the 
problem of function approximation is transformed 
into that of finding the parameters of the following 
basis function linear expansion: 
( ) bgf l
s
ss +∑= =1 kk a),a( θθ
G ,                 (1) 
where ( )lθθθ ,...,1∈G  is parameter vector to be 
identified and b  is a constant. Hence, the solution 
for this problem is to find f that minimizes 
( ) ( )( )θθ GG ,a1 k
1
fyL
p
R k
p
k
−∑=
=
,                (2) 
subject to the constraint 
γθ ≤2G ,                            (3) 
where γ  denotes the regularization constant and 
 5
size M for the CMAC-GBF systems is 
)()2( vv Nbe
N
bev NNNNN ×+××× . It is obvious 
that the memory size M for SVR-CMAC-GBF 
systems and the CMAC-GBF systems can be 
expressed as )12(1 +×+ nsvk  and 
),(1 v
N
be NNk ×+ where 
,21 v
N
bev NNNk ×××= respectively. If 
vN
bev NNNk ×××= 21  is considered as a constant, 
the memory size M for the SVR-CMAC-GBF 
systems mainly depends on the nsv. Additionally, 
the memory size M for the CMAC-GBF systems 
depends on the number of input variables vN . 
In the SVR-CMAC and SVR-CMAC-GBF, 
the regularization constant γ  and epsilon value 
must be determined. Due to the regularization 
constant γ  is suggested as larger as possible [21], 
the regularization constant γ  is chosen as 1024 in 
this study. For the selection of epsilon (ε ), it is 
well-known that the value of ε  should be 
proportional to the input noise level ψ , that is 
ψε ∝ [22,23]. In this study, the parameter ε  is 
chosen as  
ψτε ×= .                   (14) 
Based on empirical results, the constant value 
2=τ  gives good performance. In general, the 
input noise level ψ  unknown. Hence, the 
concepts of repeated SVR approach is used [24]. 
According to these concept, the input noise level 
ψ  easily estimated as 
Step 1: The SVR-CMAC and 
SVR-CMAC-GBF with 0=ε  is 
applied to obtain the estimated output 
results. Consequently, the actual and 
expected output corresponding to the 
same input data were used to 
calculate the errors.  
Step2: According to the above errors, the 
noise level ψ  easily obtained by 
( )errorsstd  where std represented as 
the standard deviation. 
 
 
 
 
Figure 1: The structure of proposed SVR-CMAC 
is shown. 
 
 
Figure 2: The structure of proposed 
SVR-CMAC –GBF is shown. 
 
Simulation Results 
 
These simulations were conducted in the 
Matlab environment. The learning constant η  
and learning epoch are chosen as 0.1 and 100 for 
the conventional CMAC, respectively. In our study, 
the structure of the CMAC is denoted by a name 
such as 9e8b, which represents a structure with 
eight blocks for each variable and nine elements 
for each complete block. Then, the structure of the 
CMAC is considered as 5e8b and 9e8b that are 
often used in the literature. The index used for 
evaluating the performance is defined as 
 7
 
For the SVR-CMAC-GBF, the function 
modeling problem for the training data without 
noise is considered. These functions are defined as 
follows.   
1
sin( )xT
x
π
π=  for ]1,1[−∈x ,           (19) 
2 2
2 1 2 1( )sin(5 )T x x x= −  for ]1,1[1 −∈x  and 
]1,1[2 −∈x ,                         (20) 
3 1 2cos( )sin( )T x xπ π=  for ]1,1[1 −∈x  and 
]1,1[2 −∈x .                         (21) 
For function 1T , 201 training data are 
generated from (19). The number of the test data is 
401. For function 2T , the training data are 
obtained by equally sampling in the input variables 
are 441(21*21). The number of the test data is 
1681 (41*41). For function 3T , the training data 
are obtained by equally sampling in the input 
variables are 324 (18*18). The number of the test 
data is 1296 (36*36). Due to the regularization 
constant γ  in the SVR approach is suggested as 
larger as possible [9, 10], the regularization 
constant γ  in the SVR-CMAC-GBF is chosen as 
1000 in this study. The Gaussian kernel parameter 
ζ  in the SVR-CMAC-GBF is obtained by grid 
search method [11]. When the 5e8b structure is 
used, the Gaussian kernel parameter ζ  of the 
functions 1T , 2T  and 3T  are obtained as 1, 0.1 
and 0.004, respectively. When the 9e8b structure is 
used, the Gaussian kernel parameter ζ  of the 
functions 1T , 2T  and 3T  are obtained as 3, 
0.0035 and 0.0035, respectively. For the training 
data without noise, the epsilon ε  is chosen as 0 
in the SVR-CMAC-GBF. Then, nsv is equal to ntr. 
The memory sizes of the SVR-CMAC-GBF and 
the CMAC-GBF for the functions 1T , 2T  and 3T  
without noise are summarized as Table 3. The 
testing Errors of the SVR-CMAC-GBF and the 
CMAC-GBF for the functions 1T , 2T  and 3T  are 
summarized as Table 4.  
 
 
 
 
Table 3: The memory size of the SVR-based 
CMAC-GBF systems and the 
CMAC-GBF systems for the functions 
1T , 2T  and 3T  without noise are 
tabulated. 
 
 
 
Table 4: The testing Errors of the SVR-based 
CMAC-GBF systems and the 
CMAC-GBF systems for the functions 
1T , 2T  and 3T  without noise are 
tabulated. 
 
 
四、結論 
  
In this study, SVR-CMAC and 
SVR-CMAC-GBF are proposed. In the two 
approaches, the modified Hamming distance 
measure is used as the kernel function in the SVR. 
Furthermore, the existed SVR software is easily 
modified to implement the SVR approach with the 
new Gaussian kernel function. Additionally, the 
memory size of SVR-CMAC, SVR-CMAC-GBF, 
Kernel CMAC, the traditional CMAC and the 
traditional CMAC-GBF are discussed and 
compared in this project. Finally, from the 
simulation results, the performances of proposed 
methods are better than other methods. 
 
 
 
 
 9
“Support vector interval regression networks 
for interval regression analysis,” Fuzzy Set 
and Systems, vol. 138, vo. 2, pp. 283-300, 
2003. 
[25] D. Mattera and S. Haykin, Support Vector 
Machines for Dynamic Reconstruction of a 
Chaotic System, in: B. Schölkopf, J. Burges, 
A. Smola, ed., Advances in Kernel Methods: 
Support Vector Machine, MIT Press, 1999. 
 
 
本計劃成果已發表於相關期刊及研討會上，最
後主持人在此要感謝國科會在這一年之相關補
助
出 
國 
報 
告 
 
 
中華民國九十八年九月ㄧ日 
• Kansei, human-machine interface, brain-machine interface  
• Fuzzy image, speech and signal processing, vision and multimedia  
• Industrial, financial, and medical applications  
• Fuzzy control and robotics, sensors, fuzzy hardware, fuzzy architectures  
• Fuzzy optimization and design, decision analysis and support  
• Fuzzy systems design, modeling, identification, fault detection  
• Fuzzy data analysis - clustering and classifiers, pattern recognition, 
bio-informatics  
• Fuzzy information processing - information extraction and retrieval, 
fusion, text mining  
• Knowledge discovery, learning, reasoning, agents, knowledge 
representation  
• Type-2 fuzzy sets, computing with words, granular computing, rough sets, 
fuzzy human computer interaction  
• Fuzzy set theory, fuzzy measures, fuzzy integrals  
• Rough sets, grey systems  
• Optimization, decision analysis, decision making, multi-criteria decision 
making  
• Software and hardware applications  
 
此外本人也在這次會議中，發表一篇有關含有離異點的區域資料之強健模
糊聚集法則的研究論文。論文(如附件)名稱如下： 
 
Robust Clustering Algorithm for the Symbolic Interval-Values Data with 
Outliers 
 
本論文也投稿至 IJFS 期刊，未來也希望有很多成果的展現。 
此次本人很榮幸的獲得行政院國家科學委員會的補助(計畫編號
98-2221-E-197-020-)，來參加由 FUZZ IEEE 2009 所舉行的會議。會中除了
學習有關上列之模糊理論、Type II 模糊控制、模糊控制與模糊資料分析，
更能增加在神經網路、模糊系統與智慧型控制系統之論文研究能量的提
 
 
 
  
Abstract — In this study, the novel robust clustering 
algorithm, robust interval competitive agglomeration (RICA) 
clustering algorithm, is proposed to overcome the problems of 
the outliers and the numbers of cluster in the competitive 
agglomeration clustering algorithm for the symbolic 
interval-values data. The Euclidean distance measure is 
considered in the proposed RICA clustering algorithm. 
Moreover, the RICA clustering algorithm can be fast converges 
in a few iterations regardless of the initial number of clusters. 
Additionally, the RICA clustering algorithm is also converges to 
the same optimal partition regardless of its initialization. 
Experimentally results show the merits and usefulness of the 
RICA clustering algorithm for the symbolic interval-values 
data. 
I. INTRODUCTION 
HE Clustering algorithm, also known as unsupervised 
classification, is a process by which a data set is divided 
into different clusters such that elements of the same 
cluster are as similar as possible and elements of different 
clusters are as dissimilar as possible. Most existing clustering 
algorithms can be classified into the following two 
categories: hierarchical clustering algorithm and partitional 
clustering algorithm [1, 2]. The hierarchical clustering 
procedures provide a nested sequence of partitions with a 
graphical representation known as the dendrogram. The 
partitional clustering procedures generate a single partition 
(as opposed to a nested sequence) of the data in an attempt to 
recover the natural grouping present in the data. 
Prototype-based clustering algorithms are the most popular 
class of the partitional clustering algorithm. In the 
prototype-based clustering algorithms, each cluster is 
represented by a prototype, and the sum of distances from the 
feature vectors to the prototypes is usually used as the 
objective function. 
In the clustering analysis, the patterns to be grouped are 
usually represented as a vector of the quantitative or the 
qualitative measurements where each column represents a 
variable. Each pattern takes a single value for each variable. 
However, this model is too restrictive to represent complex 
data. In order to take into the account variability and/or the 
uncertainty inherent to the data, variables must assume sets of 
categories or intervals, possibly even with frequencies or 
weights. These kinds of data have been mainly studied in 
 
Chen-Chia Chuang and Chin-Wang Tao are with the Department of 
Electrical Engineering, National Ilan University, 1, Sec. 1, Shen-Lung Rd., 
I-Lan, 260, TAIWAN, R.O.C.(E-mail: ccchuang@niu.edu.tw). 
Jin-Tsong Jeng is with the Department of Computer Science and 
Information Engineering, National Formosa University, P.O. BOX 6-058, 
Huwei, Huwei Jen, Yunlin County, TAIWAN 632 (E-mail: 
tsong@nfu.edu.tw). 
Symbolic Data Analysis (SDA). The aim of SDA is to 
provide the suitable methods (clustering, factorial techniques, 
decision trees, etc.) for managing aggregated data described 
by the multi-valued variables, where the cells of the data table 
contain sets of categories, intervals, or weight (probability) 
distributions [3, 4].  
The SDA provides a number of clustering methods for the 
symbolic data. These methods differ in the type of the 
considered symbolic data, in their cluster structures and/or in 
the considered clustering criteria. With the hierarchical 
methods, an agglomerative approach has been introduced that 
forms composite symbolic objects using a join operator 
whenever mutual pairs of the symbolic objects are selected 
for agglomeration based on minimum dissimilarity [5] or 
maximum similarity [6]. In [7], authors defined generalized 
Minkowski metrics for mixed feature variables and presents 
dendrograms obtained from the application of standard 
linkage methods for the data sets containing the numeric and 
the symbolic feature values. In [8, 9], the divisive and 
agglomerative algorithms for the symbolic data based on the 
combined usage of similarity and dissimilarity measures are 
proposed. These proximity measures are defined on the basis 
of the position, span and content of symbolic data. In [10], 
author proposes a divisive clustering method that 
simultaneously furnishes a hierarchy of the symbolic data set 
and a monothetic characterization of each cluster in the 
hierarchy. In [11], a hierarchical clustering algorithm for the 
symbolic data based on the gravitational approach is also 
proposed. The agglomerative clustering algorithms based on 
the similarity [12] and dissimilarity functions [13] are 
introduced, respectively. 
A number of authors have addressed the problem of 
non-hierarchical (i.e. partitional) clustering algorithms for the 
symbolic data. In [14], a transfer algorithm is used to partition 
a set of symbolic objects into clusters that described by the 
weight distribution vectors. In [15], the classical k-means 
clustering algorithm is extended in order to manage data 
characterized by the numerical and the categorical variables. 
In [16], an iterative relocation algorithm is used to partition a 
set of symbolic objects into classes so as to minimize the sum 
of the description potentials of the classes. In [17], a dynamic 
clustering algorithm for the symbolic data is proposed. In 
[18], authors have proposed several clustering algorithms for 
the symbolic data described by interval variables, based on a 
clustering criterion and has thereby generalized similar 
approaches in the classical data analysis. In [19], authors 
proposed a dynamic clustering algorithm for the interval data 
where the class representatives are defined by an optimality 
criterion based on a modified Hausdorff distance. In [20], 
Robust clustering algorithm for the symbolic interval-values data 
with outliers 
Chen-Chia Chuang, Chin-Wang Tao and Jin-Tsong Jeng 
T
 
 
 
( ) ( )
( ) ( )
1
1
1
22
1
22
−
=
=
=
⎟⎟
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜⎜
⎜
⎝
⎛
⎟⎟
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜⎜
⎜
⎝
⎛
−+−
−+−
= ∑ ∑
∑c
h
p
j
j
h
j
k
j
h
j
k
p
j
j
i
j
k
j
i
j
k
ik
ba
ba
u
βα
βα ,  
for i=1,…,c, k=1,…,n.                                 (7) 
For the updating equation of G (i.e. j
iα  and jiβ ), equation 
(3) is minimized with respect to G. Then, we can be obtain  
( )
( )∑
∑
=
=
= n
k
m
ik
n
k
j
kik
j
i
u
au
1
1
2
α
  and ( )
( )∑
∑
=
=
= n
k
m
ik
n
k
j
kik
j
i
u
bu
1
1
2
β .           (8) 
III. ROBUST INTERVAL COMPETITIVE AGGLOMERATION  
CLUSTERING ALGORITHM 
To overcome the problems of the IFCM clustering 
algorithm (i.e. the initialization, the unknown clusters 
numbers and outliers), an RICA clustering algorithm that 
based on the concepts of the RCA clustering algorithm [24] 
are used to dealing with above problems. The proposed 
objective function for the RICA clustering algorithm is 
defined as  
( ) ( ) ( )[ ] ∑ ∑∑∑
= == =
⎟⎠
⎞⎜⎝
⎛
−=
c
i
n
k
ikik
c
i
n
k
ikik wuguXUGW
1
2
11 1
22 ,x,, ηφρ GG ,  (9) 
subject to 
1
1
=∑
=
c
i
iku , for k=1,…,n.                                        (10) 
In (10), 
iku  is the membership degree of feature 
(interval-values data) vector 
kx
G  in cluster 
ig
G , η  is balance 
parameter, ][⋅ρ  is the robust loss function, ikw  is the 
weighting function that has a major influence on the 
robustness of our RICA clustering algorithm and ( )ik gGG ,xφ  is 
the distance between the feature vector 
kx
G  and the prototype 
ig
G . The objective function of the RICA clustering algorithm 
has two components. The first component in (9) can be 
regarded as a generalization of M-estimator to detect c 
clusters simultaneously. The global minimum of this 
component is reached when the number of clusters c is equal 
to the number of “good” sample data points. The second 
component in (9) is used to maximize the number of “good” 
points in each cluster. The weights 
ikw  represent degrees of 
“goodness” or typicality of point 
kx
G  with respect to cluster i, 
and are used to generate robust estimates of the prototype 
parameters. The global minimum of this term (including the 
negative sign) is achieved when all points are lumped in one 
cluster, and all other clusters are empty. When both 
components are combined with a proper choice of the 
agglomeration parameter a, the final partition will minimize 
the sum of intra-cluster distances while partitioning the data 
set into the smallest possible number of clusters. 
To minimize (9) with respect to G for fixed U and set the 
gradient to zero, 
( ) ( ) 0
g
g,x
1
2
=
∂
∂∑
=
n
k i
ik
ikik wu G
GGφ ,                          (11) 
where 
( )[ ]
( )ik
ik
ikw g,x
g,x GG
GG
φ
φρ
∂
∂
=
.                           (12) 
Further simplification of this equation depends on the loss 
function ][⋅ρ  and the distance measure used, and will be 
discussed later in this section. It can be shown that the weight 
w in (12) is equivalent to the weight function of the 
W-estimator. The choice of the weight function has a major 
influence on the robustness of our RICA clustering algorithm, 
and will be discussed later in this section. 
In the RICA clustering algorithm, the Euclidean distance 
measure is used and defined as 
( ) ( ) ( )∑
=
−+−=
p
j
j
i
j
k
j
i
j
kik bag
1
22,x βαφ GG .         (13) 
To minimize the objective function in (9), the Lagrange 
multipliers method is applied and obtained as 
( ) ( ) ( ) ( )
.1-                        
,,
1 11
2
1
1 1 1
2222
∑ ∑∑ ∑
∑∑ ∑
= == =
= = =
⎟⎠
⎞⎜⎝
⎛
−⎟⎠
⎞⎜⎝
⎛
−
⎥⎦
⎤⎢⎣
⎡
−+−=
n
k
c
i
ikk
c
i
n
k
ikik
c
i
n
k
p
j
j
i
j
k
j
i
j
kik
uλwu
bauXUGJ
η
βαρ   (14) 
To obtain the updating equation 
stu , 
2J  is minimized with 
respect to U for fixed G and set to zero. That is, 
( ) ( )
02          
2
1
1
22
2
=−−
⎥⎦
⎤⎢⎣
⎡
−+−=
∂
∂
∑
∑
=
=
tsk
n
k
sk
p
j
j
s
j
t
j
s
j
tst
st
wu
bau
u
J
λη
βαρ           (15) 
for cs ,...,2,1=  and nt ,...,2,1= , to obtain an updating equation 
for the membership 
stu . The solution can be simplified 
considering by assuming that the membership degree values 
do not change significantly from one iteration to the next and 
by computing the term ∑
=
n
k
sksk wu
1
 in (15) using the 
membership values from the previous iteration. With this 
assumption, equation (15) can be rewritten as  
( ) ( ) ⎥⎦
⎤⎢⎣
⎡
−+−
+
=
∑
=
p
j
j
s
j
t
j
s
j
t
ts
st
ba
N
u
1
222
2
βαρ
λη ,              (16) 
where ∑
=
=
n
k
sksks wuN
1
 is the cardinality of cluster s. 
Substituting (16) into (10), then 
 
 
 
cardinality 
iN  is less than 4.5. Besides, in (27), the initial 
value 
0τ  and the time constant ζ  are set as 5 and 10, 
respectively. The initial partition matrix is randomly 
assigned. In this study, four data sets (ID1-ID4) are 
considered and shown in Figures 1~4.  
To illustrate the independence of the proposed algorithm 
from the choice of initial number of the clusters maxc , the 
RICA clustering algorithm with different initial number of 
clusters are considered. For the ID1 and the ID2, the RICA 
clustering algorithm can be converge to the same final 
partition when maxc  are chosen as 9, 12, 15, and 18. Figures 
5(a) and 5(b) show the reduction of the number of clusters in 
process for the RICA clustering algorithm with different 
maxc  values. For the ID3 and the ID4, the RICA clustering 
algorithm also converged to the same final partition when 
maxc  are chosen as 15, 20, 25, and 30. Figures 5(c) and 5(d) 
show the reduction of the number of clusters in process for the 
RICA clustering algorithm with the different maxc  values. 
These experiments illustrate the insensitivity of the RICA 
clustering algorithm for the interval-values data to the initial 
number of clusters. According the above results, the RICA 
clustering algorithm has fast converges in a few iterations 
regardless of the initial number of clusters. 
To illustrate the insensitivity of the RICA clustering 
algorithm to the initial prototypes, the above data sets are also 
considered. For the ID1~ID4, the initial prototypes are 
randomly assigned. Figures 6(a)-6(d) show the evolution of 
the algorithm versus the number of iterations for those data 
sets for the ID1, ID2, ID3 and ID4. In those figures, the 
RICA clustering algorithm can be converges to the same 
optimal partition regardless of its initialization. 
Based on the above results, the RICA clustering algorithm 
has following advantages. First, the RICA clustering 
algorithm has fast converges in a few iterations regardless of 
the initial number of clusters. Second, the initial number of 
clusters maxc  is not predetermined. Third, the results of the 
RICA clustering algorithm are not affected by the initial 
prototypes. 
V. CONCLUSIONS 
In this study, the RICA clustering algorithm is developed 
to overcome the problems of the outliers, the unknown 
clusters number and the initialization of prototypes in the 
clustering algorithm for the symbolic interval-values data. 
That is, the proposed RICA clustering algorithm extends the 
concepts of the RCA clustering algorithm that can deal with 
the symbolic interval-values data. Hence, the advantages of 
the RICA clustering algorithm are also liked to the RCA 
clustering algorithm and better than the IFCM clustering 
algorithm. Experiments with the simple interval-values data 
sets show the merits and the usefulness of the RICA 
clustering algorithm. Besides, the good properties of the RCA 
clustering algorithm for the crisp-values data are also 
expressed in the RICA clustering algorithm for the 
interval-values data. 
ACKNOWLEDGMENT 
The authors wish to thank that this work was supported by 
National Science Council Under Grant NSC 
97-2221-E-197-014. 
REFERENCES 
[1] A. K. Jain, M. N. Murty and P. J. Flynn, “Data clustering: A review,” 
ACM Computing Surveys, vol. 31, no. 3, pp. 264-323, 1999. 
[2] A. D. Gordon, Classification, CRC Press, 1999. 
[3] H. H. Bock and E. Diday, Analysis of Symbolic Data: Exploratory 
Methods for Extracting Statistical Information from Complex Data, 
Springer-Verlag, 2000. 
[4] L. Billard and E. Diday, “From the statistics of data to the statistics of 
knowledge: Symbolic data analysis,” Journal of the American Statistical 
Association, vol. 98, no. 462, pp. 470–487, 2003. 
[5] K. C. Gowda and E. Diday, “Symbolic clustering using a new 
dissimilarity measure,” Pattern Recognition, vol. 24, no. 6, pp. 567–578, 
1991. 
[6] K. C. Gowda and E. Diday, “Symbolic clustering using a new similarity 
measure,” IEEE Trans. on Systems Man Cybernetic, vol. 22, pp. 
368–378, 1992. 
[7] M. Ichino and H. Yaguchi, “Generalized Minkowski metrics for mixed 
feature type data analysis,” IEEE Trans. on Systems Man Cybernetic, vol. 
24, no. 4, pp. 698–708, 1994. 
[8] K. C. Gowda and T. R. Ravi, “Divisive clustering of symbolic objects 
using the concepts of both similarity and dissimilarity,” Pattern 
Recognition, vol. 28, no. 8, pp. 1277–1282, 1995. 
[9] K. C. Gowda and T. R. Ravi, “Agglomerative clustering of symbolic 
objects using the concepts of both similarity and dissimilarity,” Pattern 
Recognition Letter, vol. 16, pp. 647–652, 1995. 
[10] M. Chavent, “A monothetic clustering method,” Pattern Recognition 
Letter, vol. 19, pp. 989–996, 1998. 
[11] K. C. Gowda and T. R. Ravi, “Clustering of symbolic objects using 
gravitational approach,” IEEE Trans. on Systems Man Cybernetic, vol. 
29, no. 6, pp. 888–894, 1999. 
[12] D. S. Guru, B. B. Kiranagi and P. Nagabhushan, “Multivalued type 
proximity measure and concept of mutual similarity value useful for 
clustering symbolic patterns,” Pattern Recognition Letter, vol. 25, pp. 
1203–1213, 2004. 
[13] D. S. Guru and B. B. Kiranagi, “Multivalued type dissimilarity measure 
and concept of mutual dissimilarity value for clustering symbolic 
patterns,” Pattern Recognition, vol. 38, pp. 151–256, 2005. 
[14] E. Diday and P. Brito, Symbolic cluster analysis: Conceptual and 
Numerical Analysis of Data, Springer-Verlag, pp. 5–84, 1989. 
[15] H. Ralambondrainy, “A conceptual version of the k-means algorithm,” 
Pattern Recognition Letter, vol. 16, pp. 1147–1157, 1995. 
[16] A. D. Gordon, An iteractive relocation algorithm for classifying 
symbolic data. In: Gaul, W. et al. (Eds.), Data Analysis: Scientific 
Modeling and Practical Application, Springer-Verlag, pp. 7–23, 2000. 
[17] R. Verde, F. A. T. De Carvalho and Y. Lechevallier, A dynamical 
clustering algorithm for symbolic data. In: Tutorial on Symbolic Data 
Analysis held during the 25th Annual Conference of the Gesellschaft für 
Klassifikation e.V. University of Munich, March 13, 2001 
[18] H. –H. Bock, “Clustering algorithms and Kohonen maps for symbolic 
data,” Journal of the Japanese Society of Computational Statistics, vol. 
15, pp. 1–13, 2002. 
[19] M. Chavent and Y. Lechevallier, Dynamical clustering algorithm of 
interval data: Optimization of an adequacy criterion based on 
Hausdorff distance. In: Sokolowski, A., Bock, H.-H. (Eds.), 
Classification, Clustering and Data Analysis, Springer-Verlag, 
Heidelberg, pp. 53–59, 2002. 
[20] R. M. C. R.Souza and F. A. T. De Carvalho, “Clustering of interval data 
based on city-block distances,” Pattern Recognition Letter, vol. 25, no. 
3, pp. 353– 365, 2004. 
[21] F. A. T. De Carvalho, R. M. C. R.Souza, M. Chavent and Y. 
Lechevallier, “Adaptive Hausdorff distances and dynamic clustering of 
symbolic data,” Pattern Recognition Letter, vol. 27, no. 3, pp. 167–179, 
2006. 
[22] F. A. T. De Carvalho, “Fuzzy c-means clustering methods for symbolic 
interval data,” Pattern Recognition Letter, vol. 28, no. 6, pp. 423–437, 
2007. 
無研發成果推廣資料 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
已發表在 IJFS (SCIE) and Applied soft computing (SCIE) 期刊 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
