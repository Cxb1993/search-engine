2. RELATED ALGORITHMS 
Before describing the new discriminative transformation-based 
adaptation algorithm, we are introducing the MCE criterion and 
the linear regression adaptations using MLLR, MAPLR and 
MCELR. 
 
2.1 MCE Criterion 
Juang et al. [8] presented the MCE criterion with a three-step 
procedure. For the case of M-category classification, the first step 
is to determine the discriminant functions 
},,1),;({ MmXg mm L=λ . Second, a misclassification measure 
is introduced as follows 
 
[ ] ηληλ /1);(exp
1
1log);()( ⎥⎦
⎤⎢⎣
⎡
−+−= ∑≠mj jjmmm XMXXd gg ,  (1) 
 
where η  is a positive number and }{ mλ=Λ  is the model 
parameter. This function is continuous and flexible with varying 
η . Notably, all competing classes mj ≠  are considered during 
parameter learning. At the third step, the loss function measuring 
the classification errors is formulated by 
 
))(exp(1
1))(();( θγλ +−+== XdXdlXl mmmm .           (2) 
 
The sigmoid function )(⋅l  has parameters γ  and θ . In (1), the 
positive value of )(Xd m  reflects the misclassification while 
0)( <Xd m  implies the correct classification. Then, the 
generalized probabilistic decent algorithm is developed to 
iteratively fulfill the MCE criterion. The learning algorithm of Λ  
is given by 
);(1 iii XlU Λ∇−Λ=Λ + ε .                           (3) 
 
Here, i  is the iteration index, X  are the training samples, U  is 
the positive definite matrix and ε  is the learning rate. This study 
concerns the discriminative linear regression adaptation rather 
than discriminative HMM training. In what follows, we are 
describing several variants of linear regression adaptation and the 
conceptual evolution from MCE discriminative training to 
proposed discriminative linear regression adaptation. 
 
2.1 MLLR, MAPLR and MCELR 
The linear regression speaker adaptation aims to estimate the 
cluster-dependent regression matrices, which are used to 
transform/adapt the speaker-independent HMM parameters to a 
new speaker. By properly controlling the sharing of regression 
matrices, MLLR can effectively find the maximum likelihood 
estimate of regression matrices for adaptation of HMM mean 
vectors. Assume that a HMM distribution of mλ  having 1×D  
mean vector mμ , the adapted mean vector mμˆ  using )1( +× DD  
regression matrix )(mrW  is expressed by 
mmrm ξμ )(ˆ W= .                                      (4) 
 
Here, D  is feature dimension, )(mr  is the regression/cluster 
class of HMM label m  and mξ  is the extended mean vector 
[ ]TTmm μξ ,1= . The maximum likelihood estimate of regression 
matrices }{ML r(m)WW =  using adaptation data X  is determined 
by 
 
),(maxargML Λ= WW
W
Xp .                          (5) 
 
The expectation-maximization (EM) algorithm can be applied to 
find the optimal MLW . The regression matrices are calculated 
through solving a system of linear equations [9]. 
Further, when the amount of adaptation data is sparse, the 
estimated regression matrices are biased. It is helpful to achieve 
desirable adaptation performance by constraining the distribution 
shape of regression matrices using prior densities. In [3][5], the 
matrix-variate normal density served as the prior distribution for 
W  so as to perform the maximum a posteriori estimation 
 
)(),(maxarg),(maxargMAP WWWW
WW
gXpXp Λ=Λ= .   (6) 
 
The prior distribution of a regression matrix is defined by 
 
⎟⎟⎠
⎞⎜⎜⎝
⎛ −Σ−⋅Δ∝ ∑
=
−− D
d
T
dddddqg
1
12/1 )()()( mwmwW ,       (7) 
 
where dm  and dΣ  are mean vector and covariance matrix for 
dth row of regression matrix dw , respectively and Δ  is a block 
diagonal matrix ),,diag( D1 ΣΣ K . Usually, q  is an exponential 
function. The resulting maximum a posteriori linear regression 
(MAPLR) has better adaptation performance than MLLR. 
In [2], Chengalvarayan proposed the minimum classification 
error linear regression (MCELR) adaptation algorithm where the 
parameters of global regression matrix were estimated according 
to the GPD algorithm. Wu and Huo [12] further performed 
MCELR adaptation using multiple regression classes. In general, 
the learning algorithm of W  is similar to (3). Having the overall 
loss function );( iXl W  due to current regression matrices iW , 
the updating of W  is done iteratively by 
);(1 iii Xl WWW ∇−=+ ε .                             (8) 
 
3. AGGREGATE A POSTERIORI CRITERIA 
Subsequently, we are introducing the generalized minimum error 
rate (GMER) [10] criterion, which was proposed for 
discriminative training and done by Li and Juang. An aggregate a 
posteriori (AAP) probability was defined and rearranged to fit the 
formula of classification error criterion for discriminative training. 
Under certain assumptions, a closed-form solution to HMM 
training was obtained. Such solution allows us to perform 
efficient model training. In this study, we use AAP probability as 
an objective function to estimate the regression matrices for 
discriminative speaker adaptation rather than HMM parameters 
for model training. 
 
 
3.1 GMER Criterion 
In GMER training algorithm, the AAP probability is defined by 
 
4. EXPERIMENTS 
In the experiments, we carried out the linear regression speaker 
adaptation algorithms of MLLR, MAPLR, MCELR and AAPLR 
for continuous Mandarin speech recognition. Mandarin is a tonal 
and syllabic language. We conducted the base syllable 
recognition for comparative study. A broadcast news speech 
recognition task was performed to examine the performance of 
speaker adaptation. In this study, we prepared two speech corpora 
for HMM training and adaptation. The speaker-independent 
HMM’s were trained using the benchmark Mandarin speech 
corpus TCC300 [3] which was recorded in office environments 
using close-talking microphones. We sampled 14266 sentences 
(about 16 hours) recorded by 100 males and 100 females for 
training. The construction of context-dependent sub-syllable 
HMM’s for Mandarin speech was referred to [3]. The adaptation 
and testing data sets were sampled from the MATBN database 
[11]. MATBN database contained Mandarin Chinese broadcast 
news (PTSN) utterances, which were shared by the Public 
Television Service Foundation of Taiwan and collected by the 
Institute of Information Science at Academia Sinica, Taiwan. 
MATBN was a 120-hour broadcast news corpus. We performed 
two-pass adaptation prior to speech recognition; task adaptation 
and speaker adaptation. In task adaptation, we used 200 
utterances (about 30 minutes) randomly sampled from MATBN 
database to adapt the existing HMM’s to fit the broadcast news 
transcription task. In speaker adaptation, we collected sixty 
utterances (about 14 minutes) from one male reporter and one 
female reporter and performed linear regression adaptation. The 
other 40 utterances (about 9 minutes) from the same speaker were 
adopted for speech recognition. For all experiments, we used 26 
dimensional feature vectors consisted of twelve Mel-frequency 
cepstral coefficients, one log energy and their first derivatives. 
Several sets of experiments on supervised adaptation were 
reported. To evaluate the performance versus adaptation data 
length, we performed speaker adaptation using ten, thirty and 
seventy adaptation utterances. The number of regression classes 
was fixed to be four for all cases. Each utterance was about three 
seconds. In Table 1, the syllable error rates (%) are reported 
through five-fold cross-validation over the adaptation data set. 
 
 MLLR MAPLR MCELR AAPLR
10 32.5 31.5 31.1 30.6 
20 30.5 29.7 29.6 29.2 
Number of 
Adaptation 
Data 60 29.0 28.4 28.1 27.9 
Table 1 Syllable error rates (%) of supervised adaptation using 
various adaptation algorithms. 
 
 MLLR MAPLR MCELR AAPLR
10 42.7 43.4 52.5 46.6 
20 52.7 53.4 65.6 57.1 
Number of 
Adaptation 
Data 60 86.8 85.1 106.0 93.1 
Table 2 Adaptation time (second) of supervised adaptation with 
different adaptation algorithms. 
Without performing adaptation, the baseline syllable error 
rate (SER) is 53.3%. After performing task adaptation, SER is 
greatly reduced to 41.6%. This implies that the environmental 
mismatch between databases of TCC300 and MATBN is 
significant and can be compensated by task adaptation. Namely, 
it is important to perform environmental adaptation for a new task 
of broadcast news transcription. Further, when performing linear 
regression speaker adaptation, we find that the SER’s are reduced 
by using MLLR, MAPLR, MCELR and AAPLR under different 
numbers of adaptation data. At the case of ten adaptation 
utterances, AAPLR obtains SER of 30.6%, which is better than 
those of MLLR (32.5%), MAPLR (31.5%) and MCELR (31.1%). 
As the number of adaptation data increases to sixty, all 
recognition results are improved accordingly. The performance of 
AAPLR (27.9%) is still superior to those of MLLR (29.0%), 
MAPLR (28.4%) and MCELR (28.1%). The reasons are due to 
the incorporation of prior regression information and the 
discriminant capability when using AAPLR.  
Also, we investigate the computational costs of MLLR, 
MAPLR, MCELR and AAPLR adaptation algorithms. The 
processing time for different adaptation algorithms was measured 
on a personal computer with CPU Pentium 4 2.0 GHz and RAM 
256 MB. Table 2 reveals that MCELR spends additional 20% 
computation time relative to MLLR. However, the computation 
costs of AAPLR and MLLR are comparable. It is because that 
both algorithms realize the closed-form linear equations. These 
results show the superiority of AAPLR for speaker adaptation.
 
5. CONCLUSION 
We have presented a new AAPLR algorithm for rapid and 
discriminative speaker adaptation. The adapted speech HMM’s 
using discriminative regression matrices were able to enhance the 
speech recognition performance for broadcast news transcription. 
The AAP criterion was introduced to achieve model 
discriminability and simultaneously derive a closed-form solution 
for rapid parameter estimation. More importantly, we established 
AAPLR algorithm in which a closed-form solution to regression 
matrices was derived to achieve desirable adaptation performance. 
HMM parameters of all acoustic units could be effectively 
adapted. To improve system robustness, the prior information of 
transformation matrix was incorporated for constrained 
estimation. Such Bayesian approach enabled the proper 
estimation of regression matrices. From the experiments, we find 
that the recognition rate of using AAPLR for supervised speaker 
adaptation is higher than those of MLLR, MAPLR and MCELR 
for different numbers of adaptation data. Also, the computation 
cost of AAPLR is smaller than MCELR and moderate compared 
to MLLR and MAPLR. 
 
6. REFERENCES 
[1] L. Bahl, P. Brown, P. de Souza and R. Mercer, “Maximum 
mutual information estimation of hidden Markov model 
parameters for speech recognition”, in Proc. ICASSP, vol. 11, 
pp. 49-52, 1986. 
[2] R. Chengalvarayan, “Speaker adaptation using discriminative 
linear regression on time-varying mean parameters in trended 
HMM”, IEEE Signal Processing Letters, vol. 5, pp. 63-65, 
1998. 
[3] C. Chesta, O. Siohan, and C.-H. Lee, “Maximum a posteriori 
linear regression for hidden Markov model adaptation,” in 
Proc. EUROSPEECH, vol. 1, pp. 211-214, 1999. 
[4] J.-T. Chien and C.-H. Huang, “Bayesian learning of speech 
duration models”, IEEE Trans. Speech Audio Processing, vol. 
11, no. 6, pp. 558-567, 2003. 
