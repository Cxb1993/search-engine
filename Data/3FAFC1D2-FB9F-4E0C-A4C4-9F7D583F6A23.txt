i 
 
摘要 
    在本計劃中，我們實現了一套可模擬出人臉陰影 (self-shadow) 及環境光
遮擋 (ambient occlusion) 現象， 以應用於經典影片當中的人臉置換系統。在
打光估計的階段，我們藉由結合陰影圖 (shadow map) 和球諧函數 (spherical 
harmonics) 打光估計的臉部影像，重新模擬出目標人臉的陰影分布。在人臉混
合的階段，我們使用無接縫影像結合演算法 ─ Drag-and-Drop Pasting 找出最
佳接縫邊緣，使得來源人臉跟目標人臉結合的接縫邊緣差異降低。此外，我們
採用混合影像強度梯度 (mixing gradients) 讓使用者可以進一步選擇在替換人
臉的時候保存目標人臉上的肌理特徵，使得最後結果與目標人臉的表情更加一
致和生動。 
  
iii 
 
本計畫近兩年所有相關發表之論文列表如下 
1. Hong-Shiang Lin, Chih-Lin Zheng, Yu-Hsun Lin, and Ming Ouhyoung, “Opti-
mized Anaglyph Colorization”, to appear in Proceedings of Siggraph Asia, Tech 
Brief, Singapore, Nov. 2012. 
2. Yu Tu, Hong-Shiang Lin, Tsung-Hua Li, and Ming Ouhyoung, “Depth-based Re-
al Time Head Pose Tracking Using 3D Template Matching”, to appear in Pro-
ceedings of Siggraph Asia, Tech Brief, Singapore, Nov. 2012. 
3.  Mei-Yun Chen, Yung-Hsiang Yang, Chia-Ju Ho, Shih-Han Wang, Shane-Ming  
Liu, Eugene Chang, Che-Hua Yeh,  and Ming Ouhyoung, “Automatic Chinese 
Food Identification and Quantity Estimation”, to appear in Proceedings of  Sig-
graph Asia, Tech Brief, Singapore, Nov. 2012. 
4. Ming Ouhyoung, Hong-Shiang Lin, Yi-Ting Wu, Yi-Shan Cheng,  Domin-
ik Seifert, “Unconventional Approaches for Facial Animation and Tracking”, to 
appear in Proceedings of Siggraph Asia, Tech Brief, Singapore, Nov. 2012. 
5. Shih-Yen Hwang, Che-Hua Yeh, and Ming Ouhyoung, “Beautifying High-
Resolution 3D Facial Models by Example-Driven Aging Related Mesh Defor-
mation”, Siggraph 2012, Poster, August, LA, USA, 2012. 
6. Yi-Ting Wu (吳宜庭), Mei-Yun Chen (陳鎂鋆), Yao-Hui Yeh (葉耀輝), and 
Ming Ouhyoung (歐陽明), "Example-based Interactive Facial Animation Sys-
tem", in the 25th IPPR conference on Computer Vision, Graphics, and Image 
Processing(CVGIP), Nantou, Taiwan, Aug. 2012. 
7. Chih-Hsun Juan (阮治軒), Hong-Shang Lin (林宏祥), Gilbert Hsieh (謝宗宏), 
and Ming Ouhyoung (歐陽明), "Efficient Quadrilateral Simplification of Man-
made 3D Models", in the 25th IPPR conference on Computer Vision, Graphics, 
and Image Processing(CVGIP), Nantou, Taiwan, Aug. 2012. 
8.  Che-Hua Yeh, Pei-Ruu Shih, Kuan-Ting Liu, Huang-Ming Chang, Ming  Ou-
hyoung, Face Recognition and Clustering for Home Photos, Proc. Sketches, 
ACM SIGGRAPH ASIA, Hong Kong, Dec. 2011. 
9. Hong-Shang Lin, Chu-Tien Lee, Shuen-Huei Guan, Ming Ouhyoung, Stereo-
scopic 3D Experience Optimization Using Cropping and Warping, Proc. Sketch-
es, ACM SIGGRAPH ASIA, Hong Kong, Dec. 2011. 
10. Shih-Hao Hsiung, Hong-Shang Lin, Ming Ouhyoung, “Self-Shadow and Ambi-
ent Occlusion Recovery for Face Images in Face Replacement”, Proc. 
CVGIP2011, Jia-Yi, Oct. 2011. (論文獎佳作) 
11. Yu Tu (屠愚),  Chih-Lin Zeng (曾志霖),  Che-Hua Yeh (葉哲華),  Sheng-Yen 
Huang (黃勝彥 ) , Ting-Xin Cheng (鄭婷馨 ) , Ming Ouhyoung (歐陽
明) “REAL-TIME HEAD POSE ESTIMATION USING DEPTH MAP FOR 
v 
 
CONTENTS 
中文摘要............................................................................................................................ i 
Abstract ............................................................................................................................. ii 
本計畫近兩年所有相關發表之論文列表如下.............................................................. iii 
（一）研究計畫之背景及目標....................................................................................... 1 
（二）研究方法、進行步驟及執行成果....................................................................... 1 
      第一年研究方法與執行成果............................................................................... 1 
      第二年研究方法與執行成果............................................................................... 5 
（三）結論與討論........................................................................................................... 5 
參考文獻........................................................................................................................... 7 
附錄一、相關投稿論文................................................................................................... 8 
 
2 
 
從影像當中重建三圍模型是電腦視覺的重要研究 - Image-based modeling。[5]的
系統藉由使用者輔助降低影像和三維模型差距越小越好，得到不錯的結果，但
在大量的三維人臉建構時會變得比較沒有效率。[6]提出的方法是在連續的影片
中或是影像中，由人工標出幾個特徵點，藉由追蹤這幾個特徵點來建構人臉的
三維模型。[4]提出三維形變模型 (3D morphable model) 的人臉建構方法，透過
大量的三維人臉模型組合來逼近影片或是影像中的人臉三維結構。在本計畫我
們採用[4]的方法。三維形變模型會利用 117 個成人的三維臉部模型資料庫來建
立，每個三維模型包含 65536 個點和相對應的材質貼圖。接下來我們用 3D 
morphable model fitting method 去估計人臉相對於攝影機的方位，得到臉上面每
個點的法向量與三維位置。 
  
(2) 人臉校準    
第二個步驟為人臉特徵校準。為了精確取得影像上目標人臉的五官位置，
用以跟三維來源人臉模型進行對應以及後續人臉合成的區域判斷，系統會針對
目標影像做人臉校準  (face alignment)，也就是找出人臉的特徵點，包括眼角、
嘴巴、輪廓…等。有了這些特徵點，接著就可以根據這些特徵點來判別人臉的
表情，以及定義出人臉替換的範圍。  人臉校準的著名方法是 1991 年 Cootes
提出的 ASM (Active Shape Model) [7]，ASM 透過標記好的訓練資料建立人臉模
型樣板，透過計算樣板的形變對新的人臉影像做輪廓及五官對齊，得到新的特
徵點位置。2003 年[8]提出的 BTSM (Bayesian Tangent Shape Model)針對 ASM
的方法做加強，得到比 ASM 更好的對齊效果。我們採用 BTSM 的人臉校準方
法，找出人臉的 87 個特徵點，包含眼睛、眉毛、鼻子、嘴巴、臉的輪廓等部分。 
 
(3) 方位估計  
第三個步驟為三維來源人臉模型對應到目標影像人臉的方位估計。人臉替
換的過程中，需要將臉部的區域由原先的目標人臉（target face）換成來源人臉
（source face）。為了使替換結果變得自然，我們必須確保來源人臉模型在投影
到目標影像上以後，其五官特徵的位置與目標人臉一致。在方位估計的過程中，
系統逐步調整三維空間的旋轉平移參數，並將來源人臉三維模型投影到二維空
間上來計算與目標人臉的一致程度。一個合理的方位假設是投影至目標影像上
的來源人臉模型與目標人臉在五官及皮膚色彩重疊的部分一致。然而，因為在
這個時候光照環境未知，系統有可能由於不同光照或不同膚色而使得方位估計
產生失敗的結果。因此我們也藉由人臉校準模組輸出的每個畫面(frame)的人臉
特徵點二維座標，以及人臉生成模組輸出的人臉三維模型投影特徵點，協助我
們調整來源人臉模型方位和目標人臉方位的一致性。 
 
(4) 打光估計  
    人臉替換的過程中，由於來源人臉和目標人臉在膚色和打光上有所不同，
4 
 
 
圖一、人臉校準，以白色點表示偵測到的臉部特徵點。 
  
(a)                (b) 
圖二、目標人臉方位估計。(a)目標人臉。(b)來源人臉三維模型套用目標人臉方
位。 
  
(a)                (b)                (c)     
圖三、基於球諧函數之重新打光。(a)目標人臉。(b)未重新打光的來源人臉三維
模型。(c)重新打光過後的來源人臉三維模型。 
 
 
圖四、結合球諧函數與 shadow map 重新打光的結果。 
6 
 
 
 
圖六：人臉置換結果。左上圖：來源人臉影像。 下排：置換結果。每組左圖為
目標人臉影像，每組右圖為人臉置換結果。 
 
（三）結論與討論 
我們完成了一套完整的人臉置換系統，包含三維人臉模型建構、方位估計、
人臉校準、打光估計、表情估計、人臉合成六個模組。在過去方法中所提到的
臉部重新打光，無法對於臉部有較深的陰影或者過飽和的區塊作模擬，因此我
們特地針對這一部分，估算出會產生陰影的主要光源，並結合藉由此光源所產
生的臉部自身陰影與重打光影像，使得我們的人臉置換系統可以模擬出較之前
方法更好的照明效果。  
    此外，我們也加強了人臉合成部分的效果。過往在替換邊緣接縫以及內容
的處理上，常會產生不平滑或者是喪失目標資訊的情況。在我們的系統中也已
修正了這項問題。系統會自動找出最佳化的替換邊緣，使得邊緣兩側的顏色差
異總和最小。我們也提供梯度混和的方式，讓使用者可以選擇保留目標人臉影
像上的材質特徵。 
 
 
 
8 
 
pasting. ACM Transactions on Graphics (SIGGRAPH), 2006.  
[11] Patrick P e´rez, Michel Gangnet, and Andrew Blake. Poisson image editing. In  
ACM SIGGRAPH 2003 Papers, SIGGRAPH ’03, pages 313–318, New York,  
NY, USA, 2003. ACM.  
[12] T. Riklin-Raviv and A. Shashua. The quotient image: Class based re-rendering 
and recongnition with varying illuminations. In IEEE Conference on Computer 
Vision and Pattern Recognition, pages 566–571, June 1999. 
 
附錄一、CVGIP 2011 論文全文 
database, not with arbitrary user-input face. The
quality is thus dependent on the candidate selec-
tion. [6] provides a 3D morphable model based
method to reconstruct face model from single im-
age. And they also estimate rendering parameters
such as camera calibration, illumination, etc. [4]
records additional dataset of 35 static 3D laser
scans which form the vector space of mouth shapes
and facial expressions to capture the mouth move-
ment. [5] apply the estimated rendering parameters
of the target face to the synthesis source face model
for face exchanging. It also estimates one principle
direct light. However, the way of light source es-
timation is different from our method. And the
system in [5] produce artifacts when composite the
faces since it does not consider the difference be-
tween skin colors. [8] presents a system that re-
places the target face in a video. They clone the ex-
pression of the target face at each frame to the syn-
thesis source face model with a 3D face expression
database, and enforce the temporal coherence for
illumination and face poses. Our face replacement
framework is similar to those model-based meth-
ods, while it focuses on the improvement of two as-
pects: illumination simulation, and seamless com-
position, and expression detail preservation. The
replacement results are more natural and vivid.
2.2. Pose Estimation
Pose estimation in face replacement is required.
The techniques can also be categorized into image-
based and model-based methods.
Image-based methods like [15] consider this prob-
lem as a classification problem. They extract dis-
tinctive facial saliencies from Haar-like features to
determine the optimal pose range with classifiers.
Other image-based techniques are based on facial
characteristics, and they often assume that the
head pose is closed to a frontal pose. [7] uses the
colors on skin region and hair region of heads to
estimate head poses. All of the above methods
need uniform lighting condition on the face, be-
cause image-based techniques rely on color infor-
mation.
Most model-based techniques start from the mor-
phable model. [5] and [4] estimate lighting condi-
tion and pose estimation at the same time to handle
different illumination. Instead of using color infor-
mation, [10] extracts contours and feature points
of the target face. Those features are mapped onto
the source face model and the temporal coherence
of head pose is considered between near frames.
The problem of using feature points only is that
the facial saliency is not preserved well after face
replacement. We synthesize the source face by the
morphable model in our system, and then we com-
bine intensity and feature information to estimate
pose of the face model. The facial saliencies are
better preserved while the pose is accurately esti-
mated in our system.
2.3. Relighting
Instead of directly estimating positions and direc-
tions of light sources for the target face, most tra-
ditional techniques for face replacement use finite
linear subspace to embed the lighting condition in
function basis and generate the corresponding re-
lighting image. [1] and [17] suppose that the head
is a convex Lambertian object under distant and
isotropic light, and they approximate the lighting
conditions with spherical harmonics. To handle
objects which have different materials in different
area, [12] proposes a ratio image technique to re-
move material dependency in the radiance environ-
ment map. [16] and [2] design a framework to refine
the surface normal and albedo for using spherical
harmonics. [2] tries to segment human face into sev-
eral parts, and they iteratively computes surface
normal, albedo, and lighting. [18] incorporates spa-
tial coherence constraint of skin colors into their
energy function.
In our system, we refer [18] to get precise albedo
and spherical harmonics coefficients. Furthermore,
we combine the shadow map and spherical har-
monic relighting image to simulate the hard self-
shadow in target faces.
2.4. Seamless Composition
When pasting other objects to the target, the inten-
sity variation near the boundary must be smooth to
make results more natural. [13] optimizes the past-
ing boundary to both minimize color variation and
the replacing area. [9] aims to estimate the opacity
for each pixel in the foreground of the image. Both
of [9] and [13] cannot handle the case with very dif-
ferent skin colors. [14] proposes a blending method
to overcome this problem. It keeps gradient vari-
ation on the source image and enforces colors of
tion:
E =
∑
x,y
(
Itarget(x, y)− Imodel(x, y)
)2
+
∑
i
((ui
vi
)
−
(
xi
yi
))2
, where Itarget is color of target image. Imodel is
the image by projected source model to the image
plane with the estimated pose parameters . The
feature term can be set as 2-norm distance between
pairs of target image(xi, yi) and source model fea-
ture points(ui, vi).
5. Illumination
Because the relighting with spherical harmonics
cannot simulate hard self-shadow very well, the re-
placement will appear unnatural if the source face
and target face have very different illumination con-
dition.
Because the deep self-shadow on the target face
is often generated from a single principle light or
several lights in the closed location, we can ap-
proximate the lighting condition with a small num-
ber of light sources. In this paper, currently we
only estimate one principle light. We believe this is
enough for common scenes by arguing that: When
the scene has more than one light source and those
lights are located at different positions, the shadow
will be interactively influenced by different lights.
And the edge of shadows becomes softer or close to
the ambient occlusion effect, which can be approx-
imated well by spherical harmonics.
Based on the above reasoning, we combine the
spherical harmonica results and estimated shadow
map generated from the light source to simulate
harsh lighting conditions.
In Section 5.1, we briefly explain the method for
albedo and face normal estimation with spherical
harmonic functions. In Section 5.2, we introduce
our light source estimation method, and the corre-
sponding shadow map generation.
5.1. Robust Albedo Relighting
Using spherical harmonics function to synthesize il-
lumination model, the average skin color is taken
as the initial albedo. In the face with cast shadow
and color intensity saturated region, the average
skin color cannot approximate the true albedo. In
(a) (b) (c)
Figure 2: Shadow map generation. (a) Target
face (b) Synthesized source face model with same
pose(c) After adding self-shadow on the face model.
our work, we adopt the algorithm proposed in [18]
to refine albedos. The energy function consists of
the data term and smoothness term. Assuming
neighboring pixels with similar colors have sim-
ilar albedos, a spatial constraint is incorporated
into the smoothness term. The simultaneous over-
relaxation approach is utilized to minimize this en-
ergy function.
5.2. Self-Shadow in Face Image
First, the initial position of the light source is spec-
ified by users or given as a default value. Given
the position of the light source, we can use vol-
ume shadow method to generate the corresponding
shadow map. We propose an energy minimization
framework to estimate the optimal position of the
light source. Given the shadow map and relight-
ing source image, the goal is to minimize the Eu-
clidean distance over all pixels and color channels
between the target image and shaded relighting im-
age, which is a combination of the relighting image
and the shadow map. The energy function can be
written as:
E =
∑
x,y
(
S(lp)(x, y)− Ioriginal(x, y)
+ Irelight(x, y)− Itarget(x, y)
)2
(a) (b) (c)
(d) (e) (f)
Figure 4: Results of normal lighting condition. (a)
The source image (first author of this paper). (b)
The target image of Andy Lau. (c) Face replaced
result. (d) The target image of Bruce Lee. (e)
Non-typical skin color on relighting source image.
(f) Face replaced result.
ents of pixels (p, q) in the source and target image.
Gradients are avoided to mix on regions contain fa-
cial saliencies of two faces. Those regions are only
replaced by the source face. If we do not enforce
this constraint, those regions may generate ghost
or lost the characteristic of source face.
7. Experimental Results
We have done experiments using our system for
several characters. To demonstrate that replacing
shadowy face gets better results by our system, we
select the faces which contain self-shadow and am-
bient occlusion from public web pages. And we
show the result of each input with original image,
relighting face, optimal replaced region, and final
blending result.
In Figure 4 (a) and (b), we replace Subject A‘s
face into Andy Lau. This image does not have
complicated illumination environment, so there just
have a little saturation region and no wide-region
shadow. Because of slight changes in light and
shadow, the generated face replacement result looks
realistic in this case. We replace subject A‘s face
into Bruce Lee in Figure 4 (a) and (d). Figure 4
(a) (b) (c)
Figure 5: Hard self-shadow simulation. (a) The
target image of Ethan Ruan. (b) The blending face
without adding shadow. (c) The blending face with
adding shadow.
(a) (b) (c)
Figure 6: Case with strong ambient occlusion and
complicate facial geometry (a) The target image of
Jackie Chan. (b) Blending faces without mixing
gradient and self-shadow. (c) Blending faces with
mixing gradient and self-shadow.
(d) has similar lighting conditions as Figure 4 (b),
with slight changes in light and shadow, but the
spherical harmonic relighting image has unusual
skin color in the saturated region of Figure 4 (e).
We suppose that this situation results from relative
pale skin color of target face, but it still has similar
illumination model. When we blend two faces by
seamless composition, it can adjust the color tone
of relighting result. We can still generate realistic
result.
In Figure 5, we try to replace the face of a movie
star Ethan Ruan. This target image has wide self-
shadow region. We compare the replaced result
which is using spherical harmonics relighting only.
As mentioned above, we not only simulate self-
shadow well but also maintain ambient occlusion
effect on the target face.
In Figure 6, we replace subject A‘s face into the
3d-model-based face replacement in video. In
SIGGRAPH ’09: Posters, SIGGRAPH ’09, pages
29:1–29:1, New York, NY, USA, 2009. ACM.
[9] Y.-Y. Chuang, B. Curless, D. H. Salesin, and
R. Szeliski. A bayesian approach to digital mat-
ting. In Proceedings of IEEE CVPR 2001, vol-
ume 2, pages 264–271. IEEE Computer Society,
December 2001.
[10] P. Fitzpatrick. Head Pose Estimation Without
Manual Initialization.
[11] J. Jia, J. Sun, C.-K. Tang, and H.-Y. Shum. Drag-
and-drop pasting. ACM Transactions on Graphics
(SIGGRAPH), 2006.
[12] R. Kumar, M. Jones, and T. K. Marks. Morphable
reflectance fields for enhancing face recgonition. In
IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2010.
[13] V. Kwatra, A. Scho¨dl, I. Essa, G. Turk, and A. Bo-
bick. Graphcut textures: image and video synthe-
sis using graph cuts. ACM Trans. Graph., 22:277–
286, July 2003.
[14] P. Pe´rez, M. Gangnet, and A. Blake. Poisson im-
age editing. In ACM SIGGRAPH 2003 Papers,
SIGGRAPH ’03, pages 313–318, New York, NY,
USA, 2003. ACM.
[15] M. B. Vatahska, T. and S. Behnke. Feature-based
head pose estimation from images. pages pp.330–
335., 2007.
[16] Y. Wang, Z. Liu, G. Hua, Z. Wen, Z. Zhang, and
D. Samaras. Face re-lighting from a single im-
age under harsh lighting conditions. In CVPR’07,
pages –1–1, 2007.
[17] Z. Wen, Z. Liu, and T. Huang. Face relighting with
radiance environment maps. In Computer Vision
and Pattern Recognition, 2003. Proceedings. 2003
IEEE Computer Society Conference on, volume 2,
pages II – 158–65 vol.2, june 2003.
[18] M. H. Xuan Zou, Josef Kittler and J. R. Tena. Ro-
bust albedo estimation from face image under un-
known illumination. In Proc. SPIE 6944, 69440A,
page doi:10.1117/12.778599, 2008.
[19] Y. Zhou, L. Gu, and H.-J. Zhang. Bayesian tan-
gent shape model: Estimating shape and pose
parameters via bayesian inference. In Proceed-
ings of the 2003 IEEE computer society confer-
ence on Computer vision and pattern recognition,
CVPR’03, pages 109–116, Washington, DC, USA,
2003. IEEE Computer Society.
出席會議報告 
2012年電腦圖學與互動技術會議(SIGGRAPH ) 
 
今年的 SIGGRAPH 辦在洛杉磯，我這次去報告論文，也在這幾天參加了
SIGGRAPH 其他的論文發表會議，以及廠商展覽、演講跟動畫祭。會議期間為 8
月 5日到 8月 9日，茲將每日在會議的行程報告如下： 
 
1. 8/5 
 
(1) State-of-the-Art Stereoscopic Visual Effects: Stereoscopy and Conversion are 
“More Than Meets the Eye”.  
--Jonothan Karafin, Digital Domain 
 
主題在介紹 2D影片轉 3D影片的技術，從下午 2:00 到 3:30。現今有許多 3D
影片並非使用雙眼鏡頭去拍攝。而是在產生 2D影片之後，利用後製產生立體版
本。2D 轉換成 3D 的過程要考慮相當複雜的因素，希望產生出來的立體版本要
跟雙眼鏡頭拍起來一樣，而且要避免造成觀眾不舒服的視覺感受。我覺得在未來
2D轉 3D會是製作 3D 影片的主流，因為總成本相對較低，比起直接利用雙眼鏡
頭取得的 3D影片，2D轉 3D顯然會遇到如何取得精確深度圖的問題。包括Digital 
Domain,以及國內的兔將公司相信在這個問題上都有相當程度的著墨，這也是一
個重要的研究領域。 
 
(2) Technical paper & sketch paper fast forward 
 
時間從下午 6:00 到 8:00，所有技術論文發表者此時都會在講台上為自己的
論文主題做一段簡短的呈現，吸引觀眾在之後論文報告的時間去聽取作者正式的
報告。這個時段沒有其他項目的活動，所以大會的參與者都會聚集在一起，可以
說是 Siggraph 的重頭戲之一。呈現內容沒有任何限制。有的人準備了很簡要精
闢的技術介紹，有的人則準備了相當幽默風趣的影片，甚至還有作者群上台搭配
演出，相當精采。 
 
 
2. 8/6 
 
今天早上在準備海報呈現的時候，遇到一位重量級人物 ─ Roman Verostko，是
一位藝術家兼歷史學家。得過 Siggraph 藝術領域的終身成就獎。中文講得相當
深刻的是海浪視覺化這個階段，他們開發出了一套相當直覺的使用介面，可以讓
美術工作者根據視覺化的情形去編輯海浪動線，以及預覽動畫角色在海浪裡的行
進情形，相當厲害! 另一部分是 Pixar 的研究員來介紹勇敢傳說的溪流製作，據
說這部動畫電影製作的時間相當長，光是溪流的部分，研究員為了更生動地呈現
水花四濺的效果，納入了建模以及大量法向量的計算，看起來相當生動。 
 
(2) Art Gallery 
    第一次看 Siggraph 的美術展，今年的美術展場地不大，作品也沒有想像多，
但是從 8/6 以後全天候開放，所以可以慢慢觀賞。印象最深刻的是很多吊掛在空
中的錄音機，當你的手逐漸靠近(還沒碰到)，錄音機的音量會漸漸放大。還有一
個用金屬繞成的馬，上面某些節點放燈，當輪軸傳動以後，在遠方可從那些亮點
看到馬在跑動。 
 
(3) Exhibition 
    在洛杉磯，有很多大公司會來 Siggraph 舉辦展覽，可以看到各家廠商的得
意之作，五花八門，一個下午逛不太完。有的廠商本身是在製作影像處理軟體或
模型製作軟體，所以就會請專員在螢幕上操作生產的軟體或使用者介面給大家看。
有的廠商製作電影特效，會展示出它們所做過的電影作品，以及幕後配合的拍攝
過程，復仇者聯盟的浩克就吸引了大家的目光。 
 
(4) Animation Festival 
 
晚上 6:00 到 8:00，是一整天的放鬆時刻，一大群人聚集在電子劇院裡面看
電腦動畫祭，匯集了大量的入選動畫創意短片，在兩個小時播完。裡面有搞笑的
內容、有溫馨小品、也有尺度滿大的內容，呈現的手法都相當有趣。 
 
4. 8/8 
(1) Technical Papers – Displays 
    從早上 9點到 10 點半，這個 section 收集的論文在探討顯示器本身的發展。
第一篇 Resolution Enhancement by Vibrating Displays 提出在透過顯示器的振動
(vibration)使得生成的多張低解析度影像看起來像是一張高解析度影像。第三篇
Tensor Displays: Compressive Light-Field Synthesis Using Multilayer Displays With 
Directional Backlighting 提出結合了 light field 的時域分割顯示技術，讓顯示器呈
現的景深、視角範圍更大，效果更好。第四篇 Tailored Displays to Compensate for 
Visual Aberrations 提出了具有多重焦距的顯示器，希望可以做到讓近視者、遠視
者即使不用戴眼鏡，也能夠觀看到清楚的影像，我覺得這篇論文非常有潛力。 
(2)   
   在聽完 Displays 以後，跟兩位學長一起去 Exhibition 會場，現場有 Pixar 免費
    從早上 10:45 到中午 12:15 分，在聽電影 John Carter 的研究團隊演講。這個
演講有三個主要的重點：立體版電影的生成技術、Zondanga的 walking city的製
作，以及奈米視覺特效的研發。據說這部電影製作的成本相當高，研發時間也相
當長，聽完這次的演講過後，覺得奈米視覺特效那部分最有趣，一開始其實研發
人員自己也沒甚麼 idea，後來在 survey過後，想出了一個自己的演算法。我發現
視覺特效的研發過程當中，如何視覺化演算法呈現的結果還滿重要的，很多特效
都是那種看起來很酷炫，但如果亂做效果又會很差的情況，如果能夠抓準那一份
sense，應該有很大一個功勞在研發過程當中的視覺呈現。 
 
(2) Technical Papers – Faces and Hair 
    從下午 3:45 分到 5:15 分，這是我在 Siggraph 聽到的最後一個 section。第一
篇 Single-View Hair Modeling for Portrait Manipulation 提出用 2D sketch 為輔助，
在拍攝影像上產生頭髮的技術，只需單張影像輸入即可。第二篇 Coupled 3D 
Reconstruction of Sparse Facial Hair and Skin 提出利用多張固定視角的影像，生成
人臉模型，同時把人臉上的毛髮(包括鬍鬚)跟人臉表面拆解的技術。第三篇提出
一個可以完整模擬、生成動畫特效中的角色人臉以及表情的系統。 
 
 
                                                 林宏祥 2012/8/30 
 
99年度專題研究計畫研究成果彙整表 
計畫主持人：歐陽明 計畫編號：99-2221-E-002-160-MY2 
計畫名稱：用於經典影片中的人臉置換系統 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 1 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 2 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100字為限） 
(1) Shih-Hao Hsiung, Hong-Shang Lin, Ming Ouhyoung,＇Self-Shadow and Ambi-ent 
Occlusion Recovery for Face Images in Face Replacement＇, Proc. CVGIP2011,Jia-Yi, 
Oct. 2011.(論文獎佳作) 
(2) MingOuhyoung,Hong-ShiangLin, Yi-Ting Wu, 
Yi-ShanCheng,Domin-ikSeifert, ＇Unconventional Approaches for Facial Animation and 
Tracking＇,(4 pages) in Proceedings ofSiggraphAsia, Tech Brief, Singapore, Nov. 2012.
 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500字為限） 
In this paper, we present a face replacement sys- tem, which can simulate the harsh 
lighting con- dition such as self-shadow and ambient-occlusion in the target face. 
In the relighting step, we pro- pose a method that combines the relighting source 
face and a shadow map to simulate the self-shadow condition. Then in the composition 
step, we use Drag-and-Drop pasting to better maintain the fa- cial saliencies with 
optimal composition boundary. Furthermore, we incorporate the mixing gradient in 
the blending process to preserve the vivid expres- sion details. 
 
  We have done experiments using our system for several characters. To demonstrate 
that replacing shadowy face gets better results by our system, we select the faces
which contain self-shadow and am- bient occlusion from public web pages. And we 
show the result of each input with original image, relighting face, optimal 
replaced region, and _nal blending result. 
