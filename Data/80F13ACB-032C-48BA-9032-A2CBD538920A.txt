2effective features for development of inexpensive
diagnostic tests, 2) achieving accurate and robust
prediction of unknown samples using the selected
features, and 3) automatic design of efficient
classifiers without using domain knowledge and
heuristics for extensive applications. In this study, we
aim to develop an evolutionary SVM-based classifier
to cope with the three difficulties at the same time.
In the researches, the filter and wrapper
approaches (Kohavi and John, 1997) are two general
approaches to solving the feature selection problem. In
the filter approach, features are selected with regard to
some predefined relevance measure which is
independent of the actual generalization performance
of the learning algorithm. Zhou and Mao (2005)
suggested a filter-like evaluation criterion, derived
from the leave-one-out procedure of least square SVM.
The wrapper approach trains classifier systems with a
subset of given features as an input and return the
estimated generalization performance of the learning
machine as an evaluation of the feature subset.
Traditionally k-fold cross-validation is used as an
estimator of the generalization ability. Li et al. (2001)
proposed a hybrid method of the genetic algorithm
(GA) based gene selection and k-nearest neighbor
classifier to assess the importance of genes for
classification. Ooi and Tan (2003) proposed an
efficient hybrid approach based on GA and maximum
likelihood classification.
To use SVMs, a number of choices need to be
specified such as the type of kernel functions (linear,
polynomial, and radial basis) and its control
parameters. To maximize the classification
performance, it is better to take feature selection and
classifier design into account simultaneously (Ho et al.,
2002). The optimal design of the SVM-based
classifiers for achieving high prediction accuracy aims
to optimize the combination of feature selection,
parameter setting of SVM, and cross-validation
methods. However, it is intractable to efficiently
optimize the large number of parameters in features
selection and classifier design using conventional GAs.
To alleviate the load of GA, Peng et al. (2003) used
the three-stage feature selection method: 1) a pre-filter
method to remove features with lower standard
deviations among various classes, 2) iterative
SVM/GA for selecting the predictive features, and 3)
post-processing step for recursive feature elimination
using leave-one-out cross-validation.
Ho et al. (2004) proposed intelligent
evolutionary algorithms based on orthogonal
experimental design for solving large parameter
optimization problems. The intelligent genetic
algorithm (IGA) is one customized version of the
intelligent evolutionary algorithm for solving specific
problems. Ho et al. (2006) proposed an interpretable
gene expression classifier using IGA for microarray
data analysis. This project proposes an evolutionary
approach to designing an SVM-based classifier
(named ESVM) by simultaneous optimization of
automatic feature selection and parameter tuning of
SVM using IGA to achieve maximal classification
accuracy with strong generalization ability using k-fold
cross-validation.
To illustrate and evaluate the efficiency of
ESVM, a typical application to microarray
classification using 11 multiclass datasets is adopted.
For efficiently selecting a small number of relevant
genes from thousands of genes retaining high
classification accuracy, an efficient GA-chromosome
representation cooperated with an intelligent crossover
operation of IGA is presented. By considering model
uncertainty, a frequency based technique by voting on
multiple sets of potentially effective features is used to
identify the most informative subset of genes. It is
shown that ESVM can obtain high prediction accuracy
of 96.88% with a small number 10.0 of genes on
average for the 11 datasets using 10-fold
cross-validation. On the other hand, ESVM can also
select a prespecified number of genes while
maximizing classification accuracy. As a result,
ESVM can be used as a powerful tool for microarray
data analysis.
The merits of ESVM are threefold: 1) automatic
feature selection and parameter setting embedded into
ESVM can advance abilities of analysis and prediction,
compared to traditional SVMs; 2) ESVM can serve
not only as an accurate classifier but also as an
adaptive feature extractor, which is applicable to any
number of input features; and 3) ESVM is developed
as an efficient tool that various SVMs can be used
conveniently as the core of ESVM for bioinformatics
problems.
III. The results and the discussion
A. The results of study had been presented in the
below which will be show by figures and tables.
3.1 Evaluation of GeneSelect Classification
of microarray data
These datasets are often used in recent
literatures on the filed of tissue classification problems
from microarray data, which are available from
http://www.gems-systems.org for non-commercial use.
The 11 datasets are representative which have 2-26
classes, 50-308 samples, and 2308-15009 genes after
the data preparatory steps (Statnikov et al., 2005).
Fitness function and GA-chromosome
representation
Fitness function is the only guide for IGA to
optimize all system parameters encoded into a
GA-chromosome. There are two objectives for
designing ESVM using IGA. One is to maximize
classification accuracy Ca of the SVM classifier and
the other is to minimize the number Nf of selected
features. If S represents the set of parameters to be
evolved by IGA, the fitness function y(S) is defined as
follows:
max y(S)=Ca(S) - wfNf(S) (1)
where wf is a positive weight determined according to
the preference of individual objectives. Generally,
high classification accuracy is the major objective and
4Table 2.
Classification accuracy (%) and number of genes for ESVM using 10-CV. The results of MC-SVM
using a nested stratified 10-CV without gene selection are obtained from (Statnikov et al., 2005).
The abbreviation GS stands for gene selection. The results of GA/SVM using leave-one-out
cross-validation are obtained from (Peng et al., 2003).
Dataset
MC-SVM
(No GS)
MC-SVM
(with GS) GA/SVM ESVM
9_Tumors 65.10 74.86 (1000) 87.93 (27) 93.15 (6.9)
11_Tumors 94.68 96.12 (27.0)
14_Tumors 74.98 76.60(15009) 85.19 (26) 85.21 (39.9)
Brain _Tumor1 91.67 92.67 (500) 96.67 (6.1)
Brain _Tumor2 77.00 85.67 (500) 100.0 (4.0)
DLBCL 97.50 100.0 (3.3)
Leukemia1 97.50 100.0 (6) 100.0 (3.4)
Leukemia2 97.32 100.0 (3.5)
Lung_Cancer 96.05 95.75 (6.9)
Prostate_ Tumor 92.00 100.0 (3.1)
SRBCT 100.0 98.75 (6.2)
Mean 89.44 96.88 (10.0)
Table 3.
The selected feature sets and their associated control parameters of SVM which have 100%
accuracy of 10-CV on the Leukemia1 dataset.
No. of validation Selected feature set {gene indexes} C γ
1 {919, 923, 1334, 3299, 4389} 80 0.8
2 {1999, 2498, 4781} 800 8
3 {484, 2621, 2763} 400 4
4 {3791, 5048, 5142} 100 4
5 {927, 1958, 4444} 200 8
6 {233, 644, 919, 2573} 400 1
7 {1892, 2230, 2685, 4677} 800 2
8 {1163, 1770, 4715, 4729} 400 2
9 {43, 2862, 3991, 4021} 400 2
10 {1046, 2547, 3669, 4637} 80 4
Table 4.
The 10 top-ranked genes using ESVM in terms of selection frequency. The accuracy of SVM is
72/72=100% using the four (Ntop=4) top-ranked genes and parameters C =256.06 and = 0.754 while the
top three genes result in 71/72=98.61%. The W_rank means the rank in the Wilcoxon rank sum test.
Rank Index Name Description W_rank
1 1999 U05259 Human MB-1 gene, complete cds 14
2 919 L05148 Human protein tyrosine kinase related mRNA sequence 9
3 5142 M31523 Human transcription factor (E2A) mRNA 1
4 4688 M84371 Human CD19 gene, complete cds 37
5 1287 M13792 Human adenosine deaminase (ADA) gene, complete cds 27
6 1426 M27891 Human cystatin C (CST3) gene, exon 3 3
7 1770 M89957
Homo sapiens cell surface glycoprotein (IGB) mRNA,
complete cds 19
8 1271 M11722 Human terminal transferase mRNA 8
9 2143 U16954 Human (AF1q) mRNA 2
10 3256 X58529
Human rearranged immunoglobulin mRNA for mu heavy
chain enhancer and constant region 33
64. Self estimation of the project results
The project that I propose is finished smoothly
during carrying out in one year, reach goal in the
original plan already, research results contribute the
relevant scientific paper already.
5. References
Armstrong, S.A., Staunton, J.E., Silverman, L.B.,
Pieters R., den Boer, M.L., Minden, M.D.,
Sallan, S.E., Lander, E.S., Golub, T.R., and
Korsmeyer, S.J. 2002. MLL translocations
specify a distinct gene expression profile that
distinguishes a unique leukemia. Nature
Genetics 30(1), 41-47.
Chang, C.-C., and Lin, C.-J. 2003. LIBSVM: a
library for support vector machines. Software
available at
http://www.csie.ntu.edu.tw/~cjlin/libsvm.
Cortes, C., and Vapnik, V. 1995. Support-vector
network. Machine Learning 20, 273-297.
Ding, C., 2003. Unsupervised feature selection via
two-way ordering in gene expression analysis.
Bioinformatics 19(10), 1259-1266.
Dudoit, S., Fridlyand, J. and Speed, T.P. 2002.
Comparison of discrimination methods for the
classification of tumors using gene expression
data. Journal of the American Statistical
Association 97, 77-87.
Golub, T.R., Slonim, D.K., Tamayo, P., Huard, C.,
Gaasenbeek, M., Mesirov, J.P., Coller, H., M.
Loh, L., Downing, J.R., Caligiuri, M.A.,
Bloomfield, C.D., and Lander, E.S. 1999.
Molecular classification of cancer: class
discovery and class prediction by gene
expression monitoring. Science 286, 531-537.
Ho, S.-Y., Liu, C.-C., and Liu, S. 2002. Design of an
optimal nearest neighbor classifier using an
intelligent genetic algorithm. Pattern
Recognition Letters 23, 1495-1503.
Ho, S.-Y., Shu, L.-S., and Chen, J.-H. 2004. Intelligent
evolutionary algorithms for large parameter
optimization problems. IEEE Trans.
Evolutionary Computation 8(6), 522-541.
Ho, S.-Y., Hsieh, C.-H., Chen, H.-M., and Huang,
Hui-Ling. 2006. Interpretable gene expression
classifier with an accurate and compact fuzzy
rule base for microarray data analysis.
BioSystem (in press).
Hsu, C.W., and Lin, C.J. 2002. A simple
decomposition method for support vector
machines. Machines Learning 46, 291-314.
Khan, J., Wei, J.S., Ringner, M., Saal, L.H., Ladanyi,
M., Westermann, F., Berthold, F., Schwab, M.,
Antonescu, C.R., Peterson, C., Meltzer, P.S.
2001. Classification and diagnostic prediction
of cancers using gene expression profiling and
artificial neural networks. Nature Medicine 7(6),
658-659.
Kohavi, R. and John. G. 1997. Wrappers for Feature
Subset Selection. Artificial Intelligence 97(12),
273-324.
Li, L., Clarice, R., Darden, T.A., and Pedersen, L.G.
2001. Gene selection for sample classification
based on gene expression data: study of
sensitivity to choice of parameters of the
GA/KNN method. Bioinformatics 17(12),
1131-1142.
Li, W. and Yang, Y. 2002. How many genes are
needed for a discriminant microarray data
analysis. Methods of Microarray Data Analysis,
Lin, S.M. and Johnson, K.F., Kluwer Academic:
137-150.
Nutt, C.L., Mani, D.R., Betensky, R.A., Tamayo, P.,
Cairncross, J.G., Ladd, C., Pohl, U., Hartmann,
C., McLaughlin, M.E., Batchelor, T.T., Black,
P.M., Deimling, A.V., Pomeroy, S.L., Golub,
T.R. and Louis, D.N. 2003. Gene
expression-based classification of malignant
gliomas correlates better with survival than
histological classification. Cancer Res.
63(7),1602-1607.
Ooi, C.H. and Tan, P. 2003. Genetic algorithms
applied to multi-class prediction for the analysis
of gene expression data. Bioinformatics 19(1),
37-44.
Parkinson, H., Sarkans, U., Shojatalab, M.,
Abeygunawardena, N., Contrino, S., Coulson,
R., Farne, A., Garcia Lara, G., Holloway, E.,
Kapushesky, M., Lilja, P., Mukherjee, G.,
Oezcimen, A., Rayner, T., Rocca-Serra, P.,
Sharma, A., Sansone, S., and Brazma, A. 2003.
ArrayExpress—a public repository for
microarray gene expression data at the EBI.
Nucleic Acids Research 31(1), 68-71.
Peng, S., Xu, Q., Ling, X. B., Peng, X., Du W., and
Chen, L. 2003. Molecular classification of
cancer types from microarray data using the
combination of genetic algorithms and support
vector machines. FEBS Lett. 555(2), 358-362.
Statnikov, A., Aliferis, C.F., Tsamardinos, I., Hardin,
D., Levy, S. 2005. A comprehensive evaluation
of multicategory classification methods for
microarray gene expression cancer diagnosis.
Bioinformatics 21(5), 631-643.
Su, A.I., et al. 2001. Molecular classification of human
carcinomas by use of gene expression
signatures. Cancer Res. 61(20), 7388-7393.
Yeung, K.Y., Bumgarner, R.E. and Raftery, A.E. 2005.
Bayesian model averaging: Development of an
improved multiclass, gene selection and
classification tool for microarray data.
Bioinformatics 21(10), 2394-2402.
Zhou, X., and Mao, K.Z. 2005. LS Bound based gene
selection for DNA microarray data.
Bioinformatics 21(8), 1559-1564.
