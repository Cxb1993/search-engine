shorten time spending of search indexes space 
construction. As a result, users can accelerate the 
speed and find precise videos they want on the video 
websites. The project will use IaaS and PaaS to 
construct a feature with cloud computing, which is a 
profound video cloud service with distributed file 
storage and searching engine. The cloud video service 
can offer ordinary users to watch and search videos； 
moreover, registered users can upload videos, leave 
messages below videos, edit or delete videos which 
are uploaded by themselves, and connect to share with 
community websites such as Facebook, Plunk and 
Twitter. 
英文關鍵詞： Cloud computing, IaaS, Virtual Machine, PaaS, HDFS, 
Nutch Search Engine, Video cloud service 
 
計畫查核點自評表（請逐年填列） 
 
一、 本表之期程可視產學合作計畫執行情況予以設定（請逐年填列，例如按月別、季別、
半年別等均可）。 
 
重要工作項目 
查核內容概述（力求量化表示） 廠商參與情形概述 
第1季 第2季 第3季 第4季 第1季 第2季 第3季 第4季 
A 
需求與規格發展
階段 
完成系統訪談及需求文件 
參與需求訪談 
審查(Review)需求文件合適性 
100%    100%    
B 系統設計階段 
初步設計 
細部設計 
參與初步設計 
參與細部設計 
20% 100%   20% 100%   
C 系統開發階段 
  
 30% 100%   30% 100%  
D 系統整合階段 
 提供測試實例 
協助系統整合測試 
  30% 100%   30% 100% 
E 專案結案階段 
  
   100%    100% 
 
二、本產學合作計畫預估後續發展情形概述： 
（計畫執行及結束後之計畫如何配合追蹤管考，產品產出與開發規劃，預期可推廣至產業或市場之成果，
預估可授權商品，預估應用價值及產值，建立平台等） 
 
時程與進度審查監控機制說明 
 
本專案對於進行中的工作每週監控一次，由全體人員共同開會，對專案各部份負責相
關人員，針對所完成的工作比例進行進度來檢視審核。而需要實施矯正措施時，其基準為： 
(1) 100/07~100/10矯正基準定為25%，當進度落後超過25%必須實施矯正措施。 
(2) 100/10~101/04矯正基準定為15%，當進度落後超過15%必須實施矯正措施。 
矯正措施為由專案各部份負責相關人員先自行召開會議，討論如何修改時程規劃，並
經過全體人員討論整合，而後再實行修改。本專案的監控項目如下所列： 
(1) 系統規格分析完成 
監控時點 矯正基準 矯正機制 
100/09/15 網站需求分析完成 明確列出未完成的部分, 之後每隔1日確認完
成的進度 
101/01/01 網站整體設計完成 明確列出未完成的部分, 之後每隔1日確認完
成的進度 
(2) 網站各應用開發完成 
監控時點 矯正基準 矯正機制 
 
本產學合作計畫研發成果及績效達成情形自評表  
成果項目 
本產學合作計畫預估研究成果及績效指
標 
（作為本計畫後續管考之參據） 
計畫達成情形 
技術移轉 預計技轉授權  1  項  完成技轉授權  1  項 
專利 
國內 預估    件 提出申請    件，獲得   件 
國外 預估    件 提出申請    件，獲得   件 
人才培育 
博士   人，畢業任職於業界   人 
博士   人，畢業任職於業界    人 
碩士 3  人，畢業任職於業界 3  人 
碩士 3  人，畢業任職於業界 3 人 
其他   人，畢業任職於業界   人 
其他   人，畢業任職於業界    人 
論文著
作 
國內 
期刊論文    件 發表期刊論文    件 
研討會論文  2   件 發表研討會論文 2  件 
SCI論文    件 發表SCI論文    件 
專書    件 完成專書    件 
技術報告  1  件 完成技術報告  1  件 
國外 
期刊論文  2  件 發表期刊論文  2  件 
學術論文   2 件 發表學術論文   2 件 
研討會論文    件 發表研討會論文   件 
SCI/ SSCI論文  1  件 發表SCI/ SSCI論文  1  件 
專書    件 完成專書    件 
技術報告    件 完成技術報告    件 
其他協助產業
發展之具體績
效 
新公司或衍生公司   1   家 設立新公司或衍生公司(名稱)：   1        
其他  
註：其他實際完成之研究成果及績效請於「其他」欄內補充填寫。 
 
表 C012A-4                                                             
 
 
 研究摘要(500字以內)： 
雲端服務已經被視為繼 Web 服務之後，下一波科技產業的重要應用趨勢。雲端服務
整體架構包括了基礎實體設備、中介層(Middleware)與虛擬機器(VM)、雲端Web應用服務
以及存取雲端服務的終端設備(Cloud Device)。近年來由於全球網路頻寬的提升，大家開始
可以互相分享較大的影音檔案，加上雲端運算與儲存的普及，影音網站變得相當熱門。本
計畫將結合雲端運算中 Hadoop分散式檔案系統(HDFS)與 Nutch搜尋引擎，並結合影音串
流相關應用程式，利用雲端運算虛擬機器與的 HDFS 特性，將檔案分散儲存在多台電腦
內，連結多台電腦以MPI方式進行平行影像轉換運算，並以Map-Reduce方式進行雲端運
算，來有效縮短搜尋索引庫建置所需花費的時間，讓使用者在影音網站上可以更加快速而
精確地搜尋到所要找的影片。本計畫將使用開放源始碼軟體重雲端基礎設施(Iaas)、雲端
平台(Paas)以建置一個具備雲端運算特性，具有分散儲存索引搜尋引擎且功能完整的雲端
影音儲存服務系統(Video Cloud)。此雲端影音服務可供一般使用者觀看影片、搜尋影片，
而經註冊使用者還可上傳影片和在影片下方留言，並且可以編輯或刪除自己上傳的影片，
影片還可連結社群網站(Facebook、Plunk、Twitter)分享。 
 
關鍵詞：雲端運算、IaaS、虛擬機器、PaaS、Hadoop分散式檔案系統、Nutch搜尋引擎、
雲端影音服務系統 
 
Cloud Services have been regarded as the significant trend of technical industries and 
applications after Web Services. The framework of Cloud Services contains the Infrastructure, 
Meta OS, Virtual Machines, Cloud Web application services and Cloud Devices. Due to the 
improvement of global network bandwidth in the recent, people began to mutually share video 
files with larger space, in addition to the popularity of cloud computing and saving, video 
websites become fairly popular. The project will integrate both Hadoop distribute file system 
(HDFS) and Nutch Search Engine in cloud computing environment with video streaming related 
applications and make good use of the features of virtual machine (VM) and HDFS. In that, 
video files can be distributed storing in various multiple computers with parallel video 
converting in MPI programming. This system can perform distributed computation in 
Map-Reduced programming in order to sufficiently shorten time spending of search indexes 
space construction. As a result, users can accelerate the speed and find precise videos they want 
on the video websites. The project will use IaaS and PaaS to construct a feature with cloud 
computing, which is a profound video cloud service with distributed file storage and searching 
engine. The cloud video service can offer ordinary users to watch and search videos; moreover, 
registered users can upload videos, leave messages below videos, edit or delete videos which are 
uploaded by themselves, and connect to share with community websites such as Facebook, 
Plunk and Twitter. 
 
 
關鍵詞：Cloud computing, IaaS, Virtual Machine, PaaS, HDFS, Nutch Search Engine, Video 
cloud service 
 研發投入比提昇 
 功能開發流程能力提昇 
 以虛擬化平台為主體，建立整合伺服器設備、資料儲存空間、工作管理及資源監
控等之雲端計算基礎服務。 
 建立雲端服務中介平台，簡化軟體業者開發雲端應用系統門檻與限制。 
 建立雲端服務資料整合服務環境，提供具備安全性、私密性、可共享之雲端資料
服務。 
 
技術特點說明： 
(一)初始概念和規劃 
 
    一開始我們將在各自的電腦上架設多台虛擬機器，全部安裝 Linux作業系統，並且在
多台虛擬機器上安裝 Hadoop 叢集，藉以模擬多台電腦做平行叢集運算(雲端計算)的效
果。接著我們就開始尋找 Hadoop及包含於計畫中的 HDFS、MapReduce能做哪一方面的
應用，我們有和研究所的學長和教授討論這個問題，像是人臉辨識、聲音辨識、影片分段
載入與字幕搜尋…等想法都有被提出來討論過。而經過可行性、困難度與原創性等多方面
的考量後，最終我們決定設計一個影音網站，並將根基於 Hadoop的 Nutch搜尋引擎來修
改與整合，作為網站搜尋功能的應用，具體呈現具備雲端計算特性儲存和搜尋功能的影音
網站。 
 
(二)系統架設 
 
    有了明確的方向後，我們便開始收集相關資料並逐步架設整個系統，由於網站牽扯的
層面相當多，因此需要安裝許多各有用處的程式和套件。首先，網站需要一個網頁伺服器，
我們選擇 Lighttpd做為我們網站的伺服器，除了它是一套免費開放原始碼伺服器之外，它
不需耗用主機太多資源的「輕量」特性，也使得在測試用的主機不一定要高效運算能力就
可順暢的使用並實驗網站中各網頁的功能，因此，它的功能對於做專題研究的我們已經相
當足夠了。Lighttpd伺服器支援 PHP程式語言，於是我們網頁使用 PHP來進行開發，在
影片轉檔方面，我們選用 FFmpeg做為轉檔的軟體工具，當然和它是可自行修改的自由軟
體有關，如此我們可以用自己的方式使用它的轉檔功能，而在播放器方面，我們安裝了支
援影音串流播放的 Flowplayer，可在觀看影片時拉時間條進行播放，不一定要從頭看到尾。 
    在搜尋引擎方面，我們一開始是架設 Nutch 搜尋引擎來測試，期間也曾嘗試新的
Crawlzilla搜尋引擎，而在經過一連串的測試與比較後，我們發現 Nutch和 Crawlzilla的原
理大致相同，而 Crawlzilla的安裝和設定比較平易近人，相較 Nutch更具備了 GUI的管理
頁面。但也因為系統十分方便，許多地方有都幫使用者做自動設定，這樣讓我們想要修改
程式內部細節與參數時，反而造成了諸多不便。於是在經過詳細的討論後，我們最後決定
使用 Nutch搜尋引擎來做為我們影音網站的搜尋引擎。 
    最後在雲端分散儲存方面，我們運用的是 FUSE的虛擬資料夾技術，將供上傳影片用
的資料夾掛載到 HDFS中，確實完成雲端分散儲存，將資料分段儲存並擁有副本，可防止
資料損毀和遺失。 
而運用這種機制，儲存資料的 HDFS所用的 Hadoop部分可以和 Nutch共用，不需重新安
裝架設另一套 Hadoop，省去許多麻煩，而且經掛載後資料夾的存取使用和其它一般資料
夾相同，不需要使用特殊的控制指令或程序來做存取，只需在存取時注意一些權限問題即
可，使用起來十分方便。 
 
(三)規劃和測試 
 
    資料庫管理方面，我們使用 mySQL來做為資料庫，除了檢查其上傳資料是否正確的
進入指定的欄位，每當網站功能增加或改變，資料庫的種類和各自欄位都需經過適當的修
推廣及運用的價值：如增加產值、增加附加價值或營利、增加投資/設廠、增
加就業人數………等。 
 
「雲端運算」是一場正在進行中的資訊產業革命，由於管理簡易、成本低，可大幅縮
短建置時間及降低系統風險，又符合節能減碳及綠色環保優勢，是下一波科技產業重要商
機。「雲端運算」的發展已成為未來資訊產業的重點技術。近年來，國內外的專家學者，
亦逐漸把重心轉向範圍更大的雲端運算領域發展。日前，台灣雲端運算產業聯盟成立大會
上，指出在未來雲端服務應用運算型服務產值複合成長，可望從 34 億美金增加到明年的
945 億美金，其次是商用型年複合成長率為 38%，消費型為 26%。若台灣可善用雲端運算
的發展，積極帶動國內資通訊產值，預估到 2020年，台灣硬體資通訊產值約可達 3669億
美元、新台幣 10兆元的規模。軟體資通訊產值從 2009年的 200億美元到 2014年可增加到
500 億美元，軟體資通訊佔整體資通訊的比重將從 15%增加到 25%。雲端運算基本上，是
透過 Web 提供即時的 IT 基礎架構、服務與軟體的做法。以傳統的運算模式來說，需採取
在本機安裝硬體與軟體的方式，而雲端的解決方案不需要親自從頭到尾地建立，只需透過
網頁瀏覽器和高速的網際網路就可存取。 
預期效益： 
 雲端技術相關課程與技術的持續導入 
 對於雲端技術及架構有基礎認知。 
 雲端技術基礎服務(IaaS)建設及維護能力。 
 雲端技術平台服務(PaaS)建設及維護能力。 
 可有效爭取政府對雲端計算推動之相關資源。 
 提供給資訊業者提供開發平台並酌以收費。 
 可對跨領域、需要雲端計算技術之廠商提供諮詢及教育訓練服務。 
 
※ 備註：依規定，精簡報告係可供國科會立即公開之資料，並以 4至 10頁為
原則，如有圖片或照片請以附加檔案上傳，如因涉及專利、技術移
轉案或其他智慧財產權、影響公序良俗或政治社會安定等，而不宜
對外公開者，請勿將其列入精簡報告；原則上本會將公開精簡報告，
完整報告原則上不予公開。 
 
 Implementation of a DDS System with Resource Monitoring on Cloud Computing 65 
platform provides a space in which developers can build their software. Developers 
do not need to focus on the systems’ health. The final layer is the cloud infrastructure. 
Most of the cloud service aims to provide a mass of users, which requires high 
network traffic and much computing. Tens of thousands of racks must be connected 
together to fit the needs. 
Cloud storage is a service that provides storage resource service through remote 
storage servers based on cloud computing. Cloud storage can provide storage services 
at a low cost with high reliability and security. A cloud storage system is a 
cooperative storage service system with multiple devices, many application domains, 
and many service forms. The development of a cloud storage system benefits from 
broadband networks, Web 2.0, storage virtualization [2-4], storage networks, 
application storage integrated with servers and storage devices, cluster technology, 
grid computing, distributed file systems, content delivery networks, peer-to-peer 
networks, data compression, and data encryption. 
This paper focuses on cloud computing infrastructure, and particularly data 
services. The goal of this study is to implement a system that can provide data storage 
services for a private cloud used for a virtualization system and a public cloud for 
DaaS [5-8]. DaaS extends the functionality of a block distributed file system using 
HDFS [9-11, 45] and implements distributed data storage (DDS). The proposed 
approach implements a distributed resource monitor [12-16] of machine runtime 
factors such as CPU utilization, memory usage, and network flow. A block distributed 
file system enables web storage that can provide cloud software to store data. This 
system also requires high-performance data storage for creating redundant, scalable 
object storage using clusters of standardized servers to store petabytes of accessible 
data. It is not a file system or real-time data storage system, but rather a long-term 
storage system for more permanent, static data that can be retrieved, leveraged, and 
updated as necessary. This paper presents the details of the proposed design. To prove 
and extend its usability, a cloud program (app) running on Android used the proposed 
data storage services. This app can synchronize files, contact lists, messages, and 
phone settings between phones or PCs. Whenever the user uploads files, they are 
saved on the DaaS server. As soon as the file is modified, the app notifies the user or 
automatically updates the data. 
2 Background 
Data grid [17-23] or distributed storage systems focus on how storage can be used 
efficiently, and how users can share their resources in different regions. This type of 
system requires middleware to integrate and manage the distributed resources shared 
by users. Metadata are needed to record these distributed resources, and is used when 
a user queries a file or a resource. 
This study proposes a next generation storage service to solve this problem. The 
cloud storage model provides online data storage services. The data are stored in 
multiple virtual or real machines called clusters. The service is usually managed by 
third-party companies who own a large data center. People pay according to their 
 Implementation of a DDS System with Resource Monitoring on Cloud Computing 67 
Java APIs that provide some features for intensive I/O operations. NIO was 
announced by Sun Microsystems with the JDK 1.4 release to complement an existing 
standard I/O. The NIO APIs were designed to provide access to the low-level I/O 
operations of modern operating systems. To avoid busy loops, NIO usually use 
“select” to dispatch operations. Each channel must register operation to the selector. 
The selector monitors the availability of these operations. This programming model 
can use only one thread to complete all functions, limiting the number of threads.  
3 System Design and Implementation 
Figure 1 shows the main components of the proposed system. This system uses web-
based virtual machine management system to control the virtual machine cluster. 
Each node in the clusters has a daemon virtual machine controller. This system also 
contains a block distributed file system, distributed data storage (DDS), and file 
system management system. The file system management system controls the user 
account authentication and the space quota. The block distributed file system provides 
a web-based storage, and the DDS stores virtualization images. To monitor the whole 
system’s healthy and loading, the system also uses a resource monitor to gather 
information from each node. This paper takes over the file system, data storage, and 
resource monitor.  
3.1 Block Distributed File System 
We used HDFS to build the block distributed file system. Because of the authentication 
limitations of HDFS, it is unsuitable to connect to public cloud. Therefore, we designed 
an interface between HDFS and outside of the cloud. Developers may input or obtain 
their files through this interface. To be more efficient with HDFS, the system was 
optimized to speed up the transfer rates. HDFS was not designed for the public cloud. 
Thus, we developed an interface to exchange the data between private cloud and public 
cloud. The interface includes FTP protocol and JAVA library. However, most of the 
important commands were implemented in RFC 959. 
Figure 2 shows how the proposed system supports HDFS over FTP. During an FTP 
connection, two transmission channels are open: 
• A channel for commands (control channel). 
• A channel for data. 
Both the client and server have two processes that allow these two types of 
information to be managed: 
• DTP (Data Transfer Process) is the process in charge of establishing the connection 
and managing the data channel. The server side DTP is called SERVER-DTP, and 
the client side DTP is called USER-DTP 
• PI (Protocol Interpreter) interprets the protocol allowing the DTP to be controlled 
using commands received over the control channel. It is different on the client and 
the server. 
 Implementation of a DDS System with Resource Monitoring on Cloud Computing 69 
computing framework like MPI is because the scheduling is tight with data instead of 
managing tasks or jobs by grid. Lots of works remain to provide the data locality to 
map-reduce, and so on. The following sections introduce some special DDS 
strategies. 
4 Experimental and Results 
4.1 Experimental Environment 
The previous sections demonstrate the design principle and implementation methods. 
This section presents several experiments conducted on seven machines within two 
switches. Each nodes contained 2-GE NICs, but had different CPU and memory 
levels. Figure 3 shows the network topology of the testbed. This experiment 
compared six topologies: DDS, DDS-Green mode, DDS-Net optimize, NFS, iSCSI, 
and HDFS. The DDS-Green mode means enable green strategy and set the 
“leastTotalSize” to 2199023255552 bytes and set the “leastNodeSize” to 
107374182400 bytes. This experiment reduced the data node to three. The DDS-Net 
optimize step split the network into upload channel and replication channel. The NFS 
was ver. 4 and the mount option was “rw,sync,no_subtree_check”. This experiment 
used Open-iSCSI ver. 2.0-871 and Hadoop 0.20.203.0. We used Linux command 
“dd” [46] to generate test data from 1KB to 32 GB. We also used dd to test the disk 
write performance for each node. The first experiment compared the performance of 
zero-copy IO and traditional IO to illustrate the level of speed enhancement provided 
by the zero-copy IO. The second and the third experiments is compared the 
performance of DDS, HDFS, iSCSI, and NFS in terms of upload and download speed. 
Both of the DDS and the HDFS were set to have 3 replicas. 
 
Fig. 3. Experiment network topology 
4.2 Experimental Results 
Figure 4 shows the performance enhancement using zero-copy IO. The 
transferTo() API decreases the time by approximately 65% compared to the 
traditional approach. This has the potential to increase performance significantly for 
 Implementation of a DDS 
512MB. In this experimen
some overhead in splitting t
The third experiment te
assumed that the systems sh
once, read-many strategy, 
speed. The experiment res
20%~30% time in downl
difference for data less t
performance deteriorates. T
same speed. The HDFS is a
 
Fig. 7. Performance compa
DDS, HDFS and NFS on d
with small files 
5 Conclusions and
This study develops a 
distributed data storage m
system. The DDS approach
transfer time and split the 
strategy saves power cons
public cloud, the proposed 
FTP and REST-like proto
presents an Android app 
proposed system can help 
their output. 
References 
1. Cloud computing, ht
Infrastructure  
2. Milojičić, D., Llorente, I
IEEE Internet Computing
System with Resource Monitoring on Cloud Computing 
t, only the HDFS split the data, suggesting that it inc
he data.  
sted the download speed (Fig. 7-8). Before the test, 
ould have results similar to the upload test. With its wr
HDFS should have a faster download speed than upl
ults are close to these assumptions. The HDFS redu
oading when the data exceed 1GB, but shows li
han 512MB. When the data exceed 1GB, the NF
he DDS and DDS with green mode achieved almost 
 little slower and stable. 
  
rison between 
ownload data 
Fig. 8. Performance comparison betwee
DDS, HDFS and NFS on download da
with large files 
 Future Work 
high-speed, load-balanced, power-saving and relia
ethod to meet the needs of a virtualization managem
 can use some special strategy like zero-copy to red
network to avoid unnecessary noise, whereas the gr
umption and replication to increase reliability. For 
approach extends the functionality of HDFS by provid
cols for those who need cloud storage. This study a
that helps people synchronize their data. We hope 
administrators or developers to cut their work and dou
tp://en.wikipedia.org/wiki/Cloud_computin
.M., Montero, R.S.: OpenNebula: A Cloud Management T
 15(2) (2011) 
71 
urs 
we 
ite-
oad 
ces 
ttle 
S’s 
the 
 
n 
ta 
ble 
ent 
uce 
een 
the 
ing 
lso 
the 
ble 
g# 
ool. 
 Implementation of a DDS System with Resource Monitoring on Cloud Computing 73 
21. Hirofuchi, T., Nakada, H., Ogawa, H., Itoh, S., Sekiguchi, S.: A live storage migration 
mechanism over wan and its performance evaluation. In: Proceedings of the 3rd 
International Workshop on Virtualization Technologies in Distributed Computing, 
Barcelona, Spain, June 15 (2009) 
22. Bertino, E., Maurino, A., Scannapieco, M.: Guest editors’ introduction: Data quality in the 
internet aera. IEEE Internet Computing 14, 11–13 (2010) 
23. Carns, P., Lang, S., Ross, R., Vilayannur, M., Kunkel, J., Ludwig, T.: Small-File Access in 
Parallel File Systems. In: Proceedings of the 23rd IEEE International Parallel and 
Distributed Processing Symposium, pp. 1–11 (April 2009) 
24. Amazon S3, http://en.wikipedia.org/wiki/Amazon_S3 
25. Amazon Simple Storage Service, http://aws.amazon.com/s3/ 
26. EgeCast, http://www.edgecast.com/ 
27. Chang, F., Dean, J., Ghemawat, S., Hsieh, W.C., Wallach, D.A., Burrows, M., Chandra, 
T., Fikes, A., Gruber, R.E.: Bigtable: A Distributed Storage System for Structured Data. 
ACM Transactions on Computer Systems 26(2), Article 4 (June 2008) 
28. Ghemawat, S., Gobioff, H., Leung, S.-T.: The google file system. In: SOSP 2003: 
Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles, pp. 29–
43. ACM Press, New York (2003) 
29. Ceph, http://ceph.newdream.net/  
30. Gu, Y., Lu, L., Grossman, R., Yoo, A.: Processing Massived Sized Graphs using 
Sector/Sphere. In: 3rd Workshop on Many-Task Computing on Grids and Supercomputers, 
co-located with SC10, New Orleans, LA, November 15 (2010) 
31. Gu, Y., Grossman, R.: Sector and Sphere: The Design and Implementation of a High 
Performance Data Cloud. Theme Issue of the Philosophical Transactions of the Royal 
Society A: Crossing Boundaries: Computational Science, E-Science and Global E-
Infrastructure 367(1897), 2429–2445 (2009) 
32. SAN, http://en.wikipedia.org/wiki/Storage_area_network 
33. NAS, http://en.wikipedia.org/wiki/Network-attached_storage  
34. iSCSI, http://en.wikipedia.org/wiki/ISCSI 
35. NFS, http://en.wikipedia.org/wiki/Network_File_System_ 
(protocol) 
36. OpenNeBula, http://opennebula.org/ 
37. Ctrix XenServer, http://www.citrix.com/ 
38. Lo, C.-T.D., Qian, K.: Green Computing Methodology for Next Generation Computing 
Scientists. In: 2010 IEEE 34th Annual Computer Software and Applications Conference 
(COMPSAC), pp. 250–251 (2010) 
39. Giroire, F., Guinand, F., Lefevre, L., Torres, J.: Energy-aware, power-aware, and Green 
Computing for large distributed systems and applications. In: 2010 International 
Conference on High Performance Computing and Simulation, HPCS, pp. lv – lxvii (2010) 
40. Zhong, B., Feng, M., Lung, C.-H.: A Green Computing Based Architecture Comparison 
and Analysis. In: 2010 IEEE/ACM Int’l Conference on & Int’l Conference on Cyber, 
Physical and Social Computing (CPSCom), Green Computing and Communications 
(GreenCom), pp. 386–391 (2010) 
41. Richardson, L., Ruby, S.: Restful Web Services, 1st edn., O’Reilly Media (May 15, 2007) 
42. RFC 2616, http://tools.ietf.org/html/rfc2616  
43. Fielding, R.T., Gettys, J., Mogul, J.C., Nielsen, H.F., Masinter, L., Leach, P.J., Berners-
Lee.: RFC 2616: Hypertext Transfer Protocol – HTTP/1.1 
44. NIO, http://en.wikipedia.org/wiki/New_I/O 
45. Hadoop, http://hadoop.apache.org  
46. dd, http://en.wikipedia.org/wiki/Dd_(Unix) 
A Medical Image File Accessing System with Virtualization Fault Tolerance on Cloud 339 
scalability, availability and disaster recoverability, can we solve the long-term face 
the problem of medical image archive [5-9]. 
Build cloud computing in this large-scale parallel computing cluster is growing 
with thousands of processors [3-5, 9-14]. In such a large number of compute nodes, 
faults are becoming common place. Current virtualization fault tolerance response 
plan, focusing on recovery failure, usually relies on a checkpoint/restart mechanism. 
However, in today's systems, node failures can often be predicted by detecting the 
deterioration of health conditions [15-25]. 
For over a decade, the majority of all hospital and private radiology practices have 
transformed from film-based image management systems to a fully digital (filmless 
and paperless) environment but subtly dissimilar (in concept only) to convert from a 
paper medical chart to an HER. Film and film libraries have given ways to modern 
picture archiving and communication systems (PACS). And they offer highly 
redundant archives that tightly integrate with historical patient metadata derived from 
the radiology information system. These systems may be not only more efficient than 
film and paper but also more secure as they incorporate with safeguards to limit 
access and sophisticate auditing systems to track the scanned data. However, although 
radiologists are in favor of efficient access to the comprehensive imaging records of 
our patients within our facilities, we ostensibly have no reliable methods to discover 
or obtain access to similar records which might be stored elsewhere [6-8]. 
According to our research, there were few Medical Image implementations on 
cloud environment. However, a familiar research presented the benefits of Medical 
Images on cloud were: Scalability, Cost effective and Replication [9]. In the same 
study, they also presented a HPACS system but lacked of management interface. 
We build a HDFS as a platform for Medical Image File Access System (MIFAS), 
and to do fault-tolerant for HDFS. Instead of a reactive scheme for Virtualization 
Fault Tolerance (VFT), we are promoting a one where processes automatically 
migrate from “unhealthy” nodes to healthy ones. Our approach relies on operating 
system virtualization techniques exemplified by Xen. It leverages virtualization 
techniques combined with health monitoring and load-based migration. We exploit 
Xen’s live migration mechanism for a guest operating system (OS) to migrate a 
Namenode from a health-deteriorating node to a healthy one without stopping the 
Namenode task during most of the migration. 
Our VFT daemon orchestrates the tasks of health monitoring, load determination 
and initiation of guest OS migration. The results showed that the actual cost of 
relocation hidden cost of live migration has been seamless transfer can be done. 
Furthermore, migration overhead is shown to be independent of the number of nodes 
in our experiments indicating the potential for scalability of our approach. Overall, 
our enhancements make VFT a valuable asset for long running HDFS task, 
particularly as a complementary scheme to reactive VFT using full checkpoint/restart 
schemes. In the context of OS virtualization, we believe that this is the first 
comprehensive study of fault tolerance where live migration is actually triggered by 
health monitoring. 
A Medical Image File Accessing System with Virtualization Fault Tolerance on Cloud 341 
execute applications under many operating systems, manage IT more efficiently, and 
allot computing resources with other computers [2]. 
Virtualization has hardware imitate much hardware through a Virtual Machine 
Monitor, and each virtual machine functions as a complete individual unit. A virtual 
machine is composed of memories, CPUs, unique complete hardware equipment, and 
so on. It can run any operating system as Guest OS without affecting other virtual 
machines. In general, most virtualization strategies fall into one of two major 
categories: 
Full virtualization also called native virtualization is similar to emulation. As in 
emulation, unmodified operating systems and applications run within a virtual 
machine. Full virtualization differs from emulation because operating systems and 
applications run on the same architecture as the underlying physical machine. This 
allows a full-virtualization system to run many instructions directly on raw hardware. 
The hypervisor in this case monitors access to the underlying hardware and gives each 
guest operating system the illusion of having its own copy. 
For Para-virtualization, the hypervisor exports a modified version of the underlying 
physical hardware. The exported virtual machine has the same architecture, which is 
not necessarily the case in emulation. Instead, targeted modifications make it simpler 
and faster to support multiple guest operating systems. For example, the guest 
operating system might be modified to use a special hyper called application binary 
interface (ABI) instead of using certain architectural features. This means that only 
small changes are typically necessary in the guest operating systems, but any changes 
make it difficult to support closed-source operating systems that are only distributed 
in binary form, such as Microsoft Windows. As in full virtualization, applications are 
still in run without modifications. 
2.3 Related Work 
HDFS servers (i.e., Data nodes) and traditional streaming media servers are both used 
to support client applications that have access patterns characterized by long 
sequential reads and writes. As such, both systems are architected to favor high 
storage bandwidth over low access latency [34].  
Recently “Cloud” became a hot word in this field. S. Sagayaraj [9] proposes that 
Apache Hadoop is a framework for running applications on large clusters built of 
commodity hardware. The Hadoop framework transparently provides both reliability 
and data motion. Hadoop implements a computational paradigm named Map/Reduce, 
where the application is divided into many small fragments of work. Each fragment of 
work may be executed or re-executed on any node in the cluster. So, by just replacing 
the PACS Server with Hadoop Framework can lead to good, scalable and cost 
effective tool for the Imaging solution for Health Care System. In the same study, 
they also presented a HPACS system but lacked of management interface. 
It is complex for kind of performance issues, but J. Shafer, et al. [13] proposed 
“The Hadoop distributed file system: Balancing portability and performance” had a 
good view in this field. The poor performance of HDFS can be attributed to 
challenges in maintaining portability, including disk scheduling under concurrent 
A Medical Image File Accessing System with Virtualization Fault Tolerance on Cloud 343 
Virtualization Fault Tolerance (VFT) has three main phases: virtual-machine 
migration policy, information gathering, and maintaining constant service availability 
(Fig. 1). However, the physical host number must be bigger than three to achieve VFT 
methodology. Information Gathering presents a detection mechanism to retrieve all 
Hosts and checks whether Hosts are alive or not, using a “ping” command every five 
minutes by running Linux schedule through “crontab”.  
Maintaining constant service availability: assuming VM m is under a Heartbeat 
plus with DRBD mechanism, and then Host n becomes an unavailable physical 
machine. Once the Host n is shut down, if VM m is secondary node, then it moves to 
an on-line Host and boot automatically. If VM m is a primary node then the 
secondary node replaces the VM m to primary node immediately. Next pre-primary 
node boots on available host(s) and become secondary. In OpenNebula, command 
onevm is to submit, control and monitor virtual machines (Fig. 2). This helps control 
dead VM to deploy on other available physical hosts. 
This flow is one of the scheduled programs and deployed on the front end. It is 
reasonable to enhance this function on OpenNebula’s front end, because it controls all 
VM operations. One example can explain a single-failure event triggered in a VFT 
approach (Fig. 3). First, Host A is shut down by unexpected matters; a few minutes 
later, the front end detected it and also triggered VFT. Next, the secondary node VM 
2 becomes primary and hands over all services from pre-primary (FAIL-OVER). 
Finally, VM 1 boots on Host C automatically and becomes the secondary node 
(FAIL-BACK). 
 
Fig. 1. Virtualization Fault Tolerance Flow 
3.3 System Architecture 
MIFAS was developed on cloud environment, Detailed System Components, such as 
Figure 4. The distribution file system was built on HDFS of Hadoop environment. 
This Hadoop platform could be described as PaaS (Platform as a Service). We 
A Medical Image File Accessing System with Virtualization Fault Tolerance on Cloud 345 
In this research, we installed the Ganglia [44] in each member of Hadoop node to get 
the real-time state from all members. Therefore, we could get the best strategy of 
transmission data from Information Service which is one of the components of 
MIFAS Middleware.  
Co-allocation: Co-allocation mechanism could conquest the parallel downloading 
from Datanodes. Besides, it also sped up downloading and solved network faults 
problems. Due to user using MIFAS to access Medical Images, the co-allocation will 
be enabled automatically. In order to reached parallel downloading approaches, the 
system will split those file in to different parts and obtain data from different Cloud 
depend on Cloud health status. Therefore, we can get the best downloading strategy. 
In our earlier research [35-37] was also provided our co-allocation mechanism. 
Replication Location Service: In this research, we built three groups of HDFS in 
different locations, and each HDFS owned an amount of Datanodes. The Replication 
Location Service means that the Service would automatically make duplication from 
private cloud to one another when medical images uploaded to MIFAS. 
 
Fig. 4. System Components of MIFAS on cloud 
4 Experimental and Results 
In our MIFAS environment, there are three HDFS nodes (THU1, THU2 and CSMU). 
For each Namenode are done in two VMs configuration, and use the DRBD with 
heartbeat sync to do this part of the configuration, were configured for each 
Namenode four Datanodes. Figure 5 show the details and the environment. 
At this part we do stress testing with JMeter. We set 10 Threads, and Loop count 5 
times more physical machines and virtual machine on the environment were to 
download 1MB, 10MB and 50MB file sizes, etc., the resulting throughput and the 
ability to download data. The result of Figure 6 shows that the smaller of file size will 
enable greater throughput, and physical machines and VMs will be more obvious 
differences. Figure 7 shows we download a small file, VMs transmission performance 
will be better than physical machines. 
A Medical Image File Accessing System with Virtualization Fault Tolerance on Cloud 347 
 
Fig. 8. Compare of PACS and MIFAS Networking Performance  
 
Fig. 9. Network Fault Tolerance  
5 Conclusions 
At present the computer node failure can usually be detected through the monitoring 
mechanism for system health. Compared to passive solutions, the recovery has 
occurred in response to failure, we are actively promoting the virtualization fault 
tolerance (VFT). Systems that exhibit truly continuous availability are comparatively 
rare and higher priced, and most have carefully implemented specialty designs that 
eliminate any single point of failure and allow online hardware, network, operating 
system, middleware, and application upgrades, patches, and replacements. Zero 
downtime system design means that modeling and simulation indicates mean time 
between failures significantly exceeds the period of time between planned 
maintenance, upgrade events, or system lifetime. We use VFT mechanism to build a 
high availability of HDFS. Combining virtualization techniques, load balancing, 
health monitoring and live migration, to be able to run virtual machines from 
hardware failure on one machine and restart on another machine without losing any 
state. 
A Medical Image File Accessing System with Virtualization Fault Tolerance on Cloud 349 
23. Varghese, B., McKee, G., Alexandrov, V.: Implementing intelligent cores using processor 
virtualization for fault tolerance. Procedia Computer Science 1, 2197–2205 (2010) 
24. Villa, O., Krishnamoorthy, S., Nieplocha, J., Brown, D.M.J.: Scalable transparent 
checkpoint-restart of global address space applications on virtual machines over 
infiniband. Presetend at the Proceedings of the 6th ACM Conference on Computing 
Frontiers, Ischia, Italy (2009) 
25. Simons, J.E., Buell, J.: Virtualizing high performance computing. SIGOPS Oper. Syst. 
Rev. 44, 136–145 (2010) 
26. DICOM, http://medical.nema.org/ 
27. Yang, C.-T., Tseng, C.-H., Chou, K.-Y., Tsaur, S.-C., Hsu, C.-H., Chen, S.-C.: A Xen-
Based Paravirtualization System toward Efficient High Performance Computing 
Environments. In: Hsu, C.-H., Malyshkin, V. (eds.) MTPP 2010. LNCS, vol. 6083, pp. 
126–135. Springer, Heidelberg (2010) 
28. VMware, http://www.vmware.com/ 
29. Nagarajan, A.B., Mueller, F., Engelmann, C., Scott, S.L.: Proactive fault tolerance for 
HPC with Xen virtualization. Presetend at the Proceedings of the 21st Annual International 
Conference on Super Computing, Seattle, Washington (2007) 
30. Open Nebula, http://www.opennebula.org 
31. Apache Hadoop Project, http://hadoop.apache.org/hdfs/ 
32. DRBD Official Site, http://www.drbd.org 
33. Pla, P.: Drbd in a heartbeat. Linux J. 2006, 3 (2006) 
34. Reddy, A.L.N., Wyllie, J.: Disk scheduling in a multimedia I/O system. Presetend at the 
Proceedings of the First ACM International Conference on Multimedia, Anaheim, 
California, United States (1993) 
35. Yang, C.T., Wang, S.Y., Lin, C.H., Lee, M.H., Wu, T.Y.: Cyber Transformer: A Toolkit 
for Files Transfer with Replica Management in Data Grid Environments. Presetend at the 
Proceedings of the Second Workshop on Grid Technologies and Applications, WoGTA 
(2005) 
36. Yang, C.T., Wang, S.Y., Fu, C.P.: A Dynamic Adjustment Strategy for File 
Transformation in Data Grids. In: Li, K., Jesshope, C., Jin, H., Gaudiot, J.-L. (eds.) NPC 
2007. LNCS, vol. 4672, pp. 61–70. Springer, Heidelberg (2007) 
37. Yang, C.T., Chi, Y.C., Han, T.F., Hsu, C.H.: Redundant Parallel File Transfer with 
Anticipative Recursively-Adjusting Scheme in Data Grids. In: Jin, H., Rana, O.F., Pan, Y., 
Prasanna, V.K. (eds.) ICA3PP 2007. LNCS, vol. 4494, pp. 242–253. Springer, Heidelberg 
(2007) 
38. Yang, C.T., Yang, I.H., Wang, S.Y., Hsu, C.H., Li, K.C.: A Recursively-Adjusting Co-
allocation scheme with a Cyber-Transformer in Data Grids. Future Generation Computer 
Systems 25, 695–703 (2009) 
39. Yang, C.T., Yang, I.H., Li, K.C., Wang, S.Y.: Improvements on dynamic adjustment 
mechanism in co-allocation data grid environments. J. Supercomput. 40, 269–280 (2007) 
40. Yang, C.T., Wang, S.Y., Chu, W.: Implementation of a dynamic adjustment strategy for 
parallel file transfer in co-allocation data grids. The Journal of Supercomputing 54, 180–
205 (2009) 
41. Nanodicom, http://www.nanodicom.org/ 
42. Hadoop-over-ftp, http://www.hadoop.iponweb.net/Home/hdfs-over-ftp 
43. Curlftp, http://curlftpfs.sourceforge.net/ 
44. Ganglia, http://ganglia.sourceforge.net/ 
國科會補助計畫
計畫名稱: 整合Hadoop與Nutch搜尋引擎之雲端影音服務之設計與實作
計畫主持人: 楊朝棟
計畫編號: 100-2622-E-029-008-CC3 學門領域: 平行與分散處理 
損毀和遺失。 
而運用這種機制，儲存資料的HDFS所用的Hadoop部分可以和Nutch共用，不需重新安裝架
設另一套Hadoop，省去許多麻煩，而且經掛載後資料夾的存取使用和其它一般資料夾相
同，不需要使用特殊的控制指令或程序來做存取，只需在存取時注意一些權限問題即可，
使用起來十分方便。
技術移轉可行性及
預期效益
我們設計網頁的過程發現資料庫的資料表種類和欄位設置是一件要動腦筋且花不少功夫
的部分，要決定有多少資料表和其要對應多少欄位除了取決於網頁表面的功能外，還要
考慮到使用者的行為和其對應的解決方法，如檢舉不良影片和封鎖不肖的使用者等，這
真的算是一個「腦力激盪」的過程。 
除了網頁的程式碼製作外，資訊安全問題更是不容小覷，要防止別人惡意攻擊和取得使
用者資料等，在網頁中也對資料庫做了一些基礎的防範，讓使用者可以安心的瀏覽網頁。
 
其它如網頁美化、頁面顯示以及按鈕種類、位置和功能也需參考許多影音網站，並且要
有自己獨一無二的風格，如何在這當中取得平衡亦是一項挑戰。 
有關搜尋的部份是由HDFS儲存索引資料，經過連結多台主機後，可將資料分散儲存在各
個主機，並存在副本，降低因主機損壞所造成的資料遺失風險，而經設定後的Nutch搜尋
引擎每經過固定時間便更新索引庫，確保索引資料和更新的資料(新上傳的影片)有相對
應，使其盡量保持在最新的狀態，這正是雲端運算的優勢所在，使用者不必了解其結構，
而上傳的資料卻可因為其分散儲存的特性而不易損壞。 
此外，我們發現以多執行緒(Multi-Thread)的FFmpeg作上傳影片轉檔所需花費的時間明
顯較原先的FFmpeg影片轉檔要短，增進了上傳影片的效率，由此可見轉檔程序平行處理
是可行且有效的。除了減少使用者等待影片上傳後到可觀看的時間外，伺服器主機的運
算效能也可以做到較充分的利用，這對管理者而言，更是省去了不少麻煩。
註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
許家勝、王文瑋參加「2012年開放軟體創作競賽」，以麥菲斯-雲端計算平台上
醫療影像檔案存取系統榮獲學生組-雲端網際服務與其他應用組金牌並獲獎金
十萬元。 
研院國網中心 2012 全國學生叢集電腦競賽(TSCC)，楊朝棟老師領軍下的「簡訊
與他的好朋友們」團隊及「CTxHPCxTHU」團隊分別獲得此次 TSCC 的亞軍與季軍。
其中「CTxHPCxTHU」團隊也曾於去年度的全國學生叢集電腦競賽獲得佳作獎。
100年度學界協助中小企業科技關懷計畫，PC100171607 跨國貿易線上詢報價應
用於雲端行動商務 100/07/01~100/12/31 
100年度學界協助中小企業科技關懷計畫，PC100110283 雲端計算基礎環境之設
計與實作 100/08/01~100/12/31 
Chao-Tung Yang*, Bo-Han Chen, Wei-Sheng Chen: On Implementation of a KVM 
IaaS with Monitoring System on Cloud Environments. FGIT-FGCN (1) 2011: 
300-309 (Best Paper Award) 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
