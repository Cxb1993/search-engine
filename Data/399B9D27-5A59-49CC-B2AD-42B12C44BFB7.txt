2Keywords: Operating Systems, Embedded
Systems, Software Component, Dynamic
Update, Memory Protection
一、前言
近年來，隨著電子、通訊以及網路等技
術的進步，加上消費性電子產品市場的崛
起，帶動著嵌入式系統的急速發展，諸如
PDA 、 Smart Phone 、 Multimedia
Player/Recorder、…，等各式各樣的嵌入式
設備如雨後春筍般出現，功能也越來越強大
且多樣化，進而也帶動著國內外對嵌入式系
統（Embedded Systems）及其相關環境的研
究。而在嵌入式系統中，嵌入式作業系統
（Embedded Operating Systems）便是其中一
項非常重要的核心技術。有一個好的嵌入式
作業系統，才能做到有效的資源管理，完全
發揮嵌入式硬體的功能。此外，提供一個易
於開發的環境，讓設計嵌入式應用程式的發
展人員設計出較為完善的嵌入式應用程
式，亦是嵌入式作業系統的任務之一。
隨著近年來作業系統的持續發展，從 80
年代的微核心作業系統（Micro Kernel
Operating System），90 年代的可延展式核
心架構（Extensible Kernel Architecture）、
物件導向（ Object-oriented）及元件化
（Component-based）作業系統，到現在可
動態更新（Dynamic Update）核心功能的作
業系統等，帶給了作業系統設計上的不小變
革。其中，元件化作業系統更是近年來嵌入
式作業系統設計與發展上的趨勢。透過軟體
元件庫的置換，可依照嵌入式系統不同平台
及各種特殊應用的需求，快速的修改與發展
作業系統。
然而，要成為一個高延展性的嵌入式作
業系統，只有在編譯時期才能重組顯然是不
夠的。在提供擴充性的嵌入式硬體下，一旦
系統新增額外的裝置，嵌入式作業系統除了
需要能驅動該硬體的核心元件外，往往也需
要相對應的軟體元件才能使用它，例如新增
了一個儲存裝置時，除了要有該裝置的驅動
程式外，可能也需要相對應的檔案系統支
援，才能讀取到所需要的檔案內容。而在應
用程式有能夠自行擴充或置換需求的嵌入
式系統環境下，不同的應用程式也有可能會
需要作業系統不同的額外支援，例如不同的
應用程式可能需要不同的緩衝快取置換策
略、CPU排程演算法、磁碟排程演算法、…
等。如果遇到這些情況每次皆需要重新編譯
和安裝作業系統，勢必會對使用者造成非常
大的不便，使用者也不一定有能力自行更新
作業系統。因此，提供元件化嵌入式作業系
統一個動態更新的機制是有必要的。此外，
在嵌入式系統資源有限的情況下，為了不對
嵌入式設備造成負擔，需要一個存放各種元
件的元件伺服器，在系統有需要額外的元件
時，提供動態下載的功能，接著再透過動態
更新的機制，線上新增、置換、及移除不必
要的元件，如此一來，只有真正需要的元件
才會存在於嵌入式設備上。
除此之外，元件化作業系統中，由於元
件本身的獨立性，元件各自被封裝起來，彼
此間透過所釋出的介面來溝通，不同的元件
往往也是由不同的人所開發，在沒有任何保
護機制的運作下，一個設計不良的元件往往
會造成整個作業系統不正常，甚至無法運
作，這在僅提供靜態元件重組的嵌入式作業
系統下，還可盡量透過一些事前的測試來避
免，但在提供動態元件更新的環境下，便完
全無法預期是否會有惡意或是設計不良的
元件被載入，這時候就只能透過一個有效的
記憶體保護（Memory Protection）機制來預
防。
二、研究目的
本計畫的目的是在元件化嵌入式作業
系統上，探討適合於嵌入式環境下的動態元
件更新架構，及元件間的保護方式，並以本
實驗室現有的元件化嵌入式作業系統
LyraOS 為平台，研究與實作出一個具備動
態元件更新功能的高延展性元件化嵌入式
作業系統，並且針對元件間的記憶體保護問
題，提出一套有效的解決方式，同時設法解
決動態更新可能對系統安全所造成的危
害，以及對系統效能所造成的影響，及其它
衍生出的問題。並針對其它各種作業系統及
平台，探討此架構如何適用於不同的環境
下，及其可行的實作或移植方式。
4LyraOS 於目標嵌入式硬體平台的移植
LyraOS 是我們實驗室目前在發展使用
的元件化嵌入式即時作業系統，主要是使用
C++所撰寫，利用 C++物件導向的特性，依
照功能的不同將 OS分為各個獨立的物件，
如圖一所示，Vega 是 LyraOS Kernel 的名
稱，底下由各個元件（Components）所組成，
我們可以根據目標平台以及使用上的需
求，透過 Configuration Tool 將適當的
Components 挑出，並組合成符合我們需求
的作業系統，圖二為 LyraOS 系統架構圖。
圖一：LyraOS 元件化作業系統
圖二：LyraOS 系統架構圖
我們選用ARM Integrator/CP920T 來當
我們的嵌入式硬體平台，如圖三，其包含了
Integrator Compact Platform baseboard 及
Integrator ARM920T Core Module。選用
ARM920T 當核心處理器是因為我們利用
其MMU（Memory Management Unit）來達
到所需的記憶體管理及保護等功能。
圖三：ARM Integrator/CP920T
由於 LyraOS 最早是在 x86 平台上所
開發並在 Win32 環境下以 DJGPP 開發完
成，核心映像檔採用 DJGPP 所編譯出來的
a.out 格式，要將 LyraOS 移植到我們的嵌
入式目標平台上時，除了要針對硬體環境改
寫硬體抽象層的相關函式及元件外，尚會產
生一些額外的問題需要解決。
(1) bootloader
在移植 LyraOS 到 ARM 平台上時，為
了縮短開發的流程及增加系統開發時的便
利性，我們選用 Open Source 的 U-boot
（Universal Bootloader），只需針對 U-boot
做一些設定及修改一小部份的程式碼，便可
完全適用於我們的嵌入式硬體平台。
(2) 開發環境
在各種開發工具和環境組合的考量
中，我們在 Linux 下以 GNU 的 cross
toolchains 來做我們跨平台編譯的環境，原
因不外乎就是工具取得方便、開放原始碼、
大量的硬體平台支援、參考資料眾多、以及
編譯出來的核心映像檔使用了目前最常見
的 ELF 的檔案格式，可以使用 U-boot 直
接載入執行等。
從 Win32 環境下的 DJGPP 到 Linux
下的 GNU toolchains，我們必需針對不同環
境下檔名大小寫、Makefile、Link Script、…
等所造成的相容性問題做改寫。此外，由於
各家編譯器對 C++ 支援的完整度不同，不
Vega
Microkernel:
(HAL+minimum kernel core)
Configuration
tool
Small system
Medium system
Large systemcomponents
Hardware (ARM/ AMD Elan SC400 / x86 PC)
Kernel Core
Components
LyraNET
Networking
LyraDD Device Driver
LyraFILE
File System
Java Virtual Machine
POSIX subset API
Embedded Browser /Desktop
LyraOS Framework
API
Hardware Abstraction Layer
6虛擬記憶體的時候，採用一對一的映對方式
直接對應到實體記憶體，也就是說，讓系統
核心的記憶體管理元件直接管理系統的實
體記憶體。所以我們只需要先依照 ARM
Integrator/CP920T 中 Memory Map 的
Layout 去做修改即可。
(3.e) 中斷處理元件的移植
ARM 的中斷分為 IRQ（ Interrupt
Request）和 FIQ（Fast Interrupt Request）兩
種，我們先使用一般的 IRQ 方式，並將我
們的 Interrupt Handler註冊到 IRQ Exception
Vector 內，而 Interrupt Handler 會根據
IRQ Status Register 偵測所有目前所有發生
的 IRQ Number，接著呼叫 LyraOS 原先的
Interrupt Dispatcher，並傳入 IRQ Number。
如此一來，只要會產生 IRQ Interrupt 的硬
體週邊有先將 Interrupt Service Routine
（ISR） 註冊到 ISR Table 後，每當系統發
生 IRQ Interrupt，便會根據我們在 IRQ
Exception Vector 所 填 入 的 位 置 找 到
Interrupt Handler ， 緊 接 著 ， Interrupt
Dispatcher 根據 IRQ Number便可以從 IRQ
Table 內找到該週邊所對應的 ISR，並呼叫
該 ISR 處理目前所發生的 IRQ Interrupt。
(3.f) Context Switch / Thread Create
由於 ARM 不像 x86有 call 和 ret 的
指令，所有函式的呼叫皆需透過 branch（相
當於 jump），結束時還需要自行處理函式
的返回，ARM 微處理器會自動將返回位址
存放於 LR 暫存器中，所以我們自行撰寫的
組合語言程式便須靠 LR 暫存器才能正確
返回（也就是將 LR move 到 PC），但是
在每次函式呼叫時，ARM 微處理器便會自
動將返回位址存入 LR 暫存器中，若我們的
程式會呼叫其他函式，便會破壞 LR 暫存器
的值，這時我們除了藉助堆疊外，就只能另
外將 LR 的值自行存放在其他地方。於是，
在 LyraOS 做 Context Switch 或是 Thread
Create 時，我們利用一個叫做 jmp_buf 的
資料結構（包含了 R0 ~ R12、SP、LR、PC、
CPSR 等 Registers 的值），在 Context
Switch 或是 Thread Create 時，利用 setjmp()
儲存目前的 registers 狀態到 jmp_buf 中，
由於 ARM 平台下，GNU 編譯器會將函式
的參數透過 register 來傳遞，因此在
Thread Create 時，我們亦將相對應的參數塞
到 jmp_buf 內正確 registers 中，而
longjmp() 則是從 jmp_buf 中更新目前
registers 的狀態，並更新 PC，跳到指定的
記憶體位址執行。
(3.g) Timer Interrupt
ARM Integrator/CP920T 上提供了三個
Counter/Timers，其中 Timer 0 是 System
Bus Clock，而 Timer 1 和 Timer 2 則可讓
我們自由使用。我們使用 Timer 1 當作
LyraOS 的 Timer，讓 Timer 1 週期的發生
Interrupt，並將 Timer 1 的 Interrupt Service
Routine 註冊到 ISR Table 中，負責處理
LyraOS 的 CPU Scheduler， LyraOS 的
CPU Scheduler 會 去 判 斷 是 否 要 對
Process/Thread 作 Context Switch 的動
作，藉此來達到 OS Time-sharing 的功能。
(3.h) NIC（Network Interface Card）Driver
的製作或移植
為了讓 LyraOS 在目標嵌入式平台上
執行時，能具備動態元件載入及更新的功
能，網路是基本必需的功能，因此，我們製
作或移植 NIC driver，以及通訊協定組，使
LyraOS在目標嵌入式平台具備通訊的功能。
元件間記憶體保護機制的設計與實作
我們主要利用在各種處理器常見的
MMU所提供的 Paging以及記憶體保護等機
制，配合 OS的控制，將各個元件切割成不
同的 Protection Domain，讓各個元件間的
Address Space彼此獨立，設計出一個適用於
元件化嵌入式作業系統的記憶體保護機
制，並容易移植到其它硬體環境的方法。主
要內容包括了 Fast Context Switch Extension
(FCSE)、Page Translation、Access Permissions
Control、Domains、Dual Mode及 System Call
Interface等功能的實作。
ARM 硬體提供給 OS 關於的記憶體管
8Mode 的模式時，由 User Mode 跳到
Supervisor Mode 勢必需要透過系統呼叫，
此時，OS 在兩個 Modes 之間，便需要有
一層 System Call Interface，如圖六。
圖六：Dual Mode 與 System Call Interface
在 ARM的平台下，本身就有為 System
Call 提供一個 Software Interrupt 的指令
（SWI），在 User Mode執行 SWI這個指令
時，會產生軟體中斷並切換到 Supervisor
Mode，然後跳到我們開機時所填好的
Exception Vectors 中，執行 SWI Exception
的 Handler，我們可以利用 SWI 指令的後
24-bits Immediate Value 來選擇所要執行的
System Service，參數則經由一般的暫存器來
傳遞，最後透過 SPSR 和 LR 暫存器回到
User Mode 繼續執行。透過這層 System Call
Interface 的轉換，我們可以有效的確保核心
系統的安全。
動態元件更新機制的設計與實作
我們在元件化嵌入式作業系統上，探討
適合於嵌入式環境下的動態元件更新架
構，及元件間的保護方式，設法解決動態更
新可能對系統安全所造成的危害，及其它衍
生出的問題。實作的內容主要包括：
1. 設計並實作 LyraOS 的動態元件載入、
移除及置換等功能。包括元件的管理、
元件的下載、元件的動態載入、移除及
置換方式等。
2. 利用ARM920T所提供的記憶體管理單
元 (MMU)，結合 LyraOS 上的元件保
護機制，解決動態更新對系統安全所可
能造成的危害。
我們動態更新的的內容，便是以
LyraOS 內所切割好的元件為單位。除了單
純的新增元件外，動態更新需要的是提供系
統線上更新程式碼的功能，依照動態更新的
程度不同，我們將動態更新分成二個等級，
一個是針對新舊元件的程式碼做置換，並針
對全域、單一物件的資料內容做轉換。另一
個是針對新舊元件的程式碼做置換，並將所
有相同類別所衍伸的物件做更新。
關於更新的元件，由於元件化作業系統
中，元件各自被封裝起來，彼此間透過所釋
出的介面來溝通，通常為了開發以及元件間
彼此溝通的方便，相同功能的元件通常都會
釋出固定的介面（Interface），而 LyraOS 本
身相同功能的元件，也都會繼承同一個抽象
類別，元件間透過固定的 Interface 做溝
通。而目前有提供動態更新的元件化作業系
統，幾乎也都是基於此原因下，限制只能針
對有相同介面的元件做更新，簡化了不同介
面的元件間更新所可能造成的問題，然而，
雖然這樣提供了實作上的方便，相反的卻也
限制住了動態更新所能帶來的彈性。
目前我們所實作的動態更新的機制，可
以針對新舊元件的程式碼做置換，並可以針
對全域、單一物件的資料內容做轉換。同
時，我們也能夠讓元件所釋出的介面能夠自
由的做變更，藉由不同的 Protection
Domain，保護核心和其他元件的安全，防止
系統被不良或惡意的元件破壞。
五、結果與討論
我們已完成本計畫的目標，即元件動態
更新技術與元件間記憶體保護機制的研製，
並整合這些成果於本實驗室現有的元件化嵌
入式作業系統 LyraOS 內，詳細項目包括：
(1) 元件介面的研究分析、制訂及實作。
(2) 完成 LyraOS於嵌入式硬體平台的移植。
(3) 製作目標嵌入式硬體平台之驅動程式。
(4) 元件間記憶體保護機制的設計與實作。
(5) 元件動態更新技術的研究與製作。
(6) 系統整合、測試及效能評估。
關於元件動態更新機制，我們將來希望
達成不僅能針對新舊元件的程式碼做置
10
Asanovic, “Mondrian Memory
Protection,” 10th International Conference 
on Architectural Support for Programming
Languages and Operating Systems
(ASPLOS-X), San Jose, CA, Oct. 2002.
[13] Tzi-cker Chiueh, Ganesh Venkitachalam,
and Prashant Pradhan, “Integrating 
Segmentation and Paging Protection for
Safe, Efficient and Transparent Software
Extensions,” Symposium on Operating 
Systems Principles, December 1999.
[14] Gísli Hjálmtýsson and Robert Gray,
“Dynamic C++ Classes - A Lightweight
Mechanism to Update Code in a Running
Program,” Proceedings of the USENIX 
Annual Technical Conference, June, 1998.
[15] Michael Hicks, Jonathan T. Moore, and
Scot Netles, “Dynamic Software 
Updating,” Proceedings of the ACM 
SIGPLAN 2001 Conference on
Programming Language Design and
Implementation (PLDI'01), June 2001.
[16] Da-Wei Chang and Ruei-Chuan Chang,
“OS Portal: An Economic Approach for 
Making an Embedded Kernel Extensible,” 
the Journal of System and Software, 67,
2003, pp.19-30.
[17] Chi-Wei Yang, C. H. Lee, and R. C.
Chang, “Lyra: A System Framework in
Supporting Multimedia Applications,” 
IEEE International Conference on
Multimedia Computing and Systems'99,
Florence, Italy, June 1999.
[18] Z. Y. Cheng, M. L. Chiang, and R. C.
Chang, “A Component Based Operating 
System for Resource Limited Embedded
Devices,” IEEE International Symposium 
on Consumer Electronics (ISCE'2000),
HongKong, Dec. 5-7, 2000.
[19] Mei-Ling Chiang and Yun-Chen Lee,
“LyraNET: A Zero-Copy TCP/IP Protocol
Stack for Embedded Systems,” Real-Time
Systems, Vol. 34, No. 1, pp. 5-18, Sep.
2006.
[20] Mei-Ling Chiang and Ching-Ru Lo,
“LyraFILE: A Component-Based VFAT
File System for Embedded Systems,” 
International Journal of Embedded
Systems, Vol. 2, Nos. 3/4, pp. 248-259,
2006.
七、計畫成果自評
本研究已完成的具備動態元件更新功能
與元件間記憶體保護機制的元件化嵌入式作
業系統平台，除了可以支援後續發展相關的
動態元件更新與重組的研究，另外，本計畫
參與的學生藉由計畫的執行與參與，獲得了
嵌入式系統方面的實作經驗，並對作業系統
架構及元件動態更新技術與元件間記憶體保
護機制的相關技術有深入的了解和研究訓
練。同時，藉由一個元件化嵌入式作業系統
的實作，獲得對嵌入式作業系統以及作業系
統核心程式的訓練，深入了解其運作原理與
運作方式，同時獲得對嵌入式系統開發與效
能提昇進一步研究的基礎。
目前，相關本計畫的研究已有 1篇國際
會議論文已被 EUC’2007 (The 2007 IFIP
International Conference on Embedded and
Ubiquitous Computing) 所接受。另有 1篇碩
士論文，及在 2年度中 2次參加教育部大學
校院嵌入式軟體設計競賽中，各榮獲系統軟
體組優勝及佳作。列述如下：
1. 沈柏曄同學，“元件化嵌入式作業系統之
記憶體保護機制的設計與實作”，93學年
度教育部大學校院嵌入式軟體設計競賽
之系統軟體組優勝。
2. 沈柏曄同學，“嵌入式作業系統之動態元
件更新機制的設計與實作”，94學年度教
育部大學校院嵌入式軟體設計競賽之系
統軟體組佳作。
3. Bor-Yeh Shen, “Design and
Implementation of Dynamic Component
Update and Memory Protection
Mechanisms in an Embedded Operating
System,”Master Thesis, Department of
Information Management, National
Chi-Nan University, Nantou, Taiwan,
R.O.C., 2006. (元件化嵌入式作業系統之
元件動態更新與記憶體保護機制的設計
與實作)
4. Bor-Yeh Shen and Mei-Ling Chiang, “A
Server-side Pre-linking Mechanism for
Updating Embedded Clients Dynamicaly,” 
The 2007 IFIP International Conference on
Embedded and Ubiquitous Computing
(EUC’2007), Taipei, Taiwan, December 17 
- 20, 2007.
Improving the Performance of Log-Structured File 
Systems with Adaptive Block Rearrangement 
Mei-Ling Chiang 
Department of Information Management 
 National Chi-Nan University, Puli, Taiwan, R.O.C. 
joanna@ncnu.edu.tw
 Jia-Shin Huang 
Department of Information Management 
National Chi-Nan University, Puli, Taiwan, R.O.C. 
peterhuang1kimo@yahoo.com.tw
ABSTRACT
Log-Structured File System (LFS) is famous for its optimization 
for write performance. Because of its append-only nature, garbage 
collection is needed to reclaim the space occupied by the obsolete 
data. The cleaning overhead could significantly decrease the 
performance of file system. However, traditional cleaning policies 
do not consider the storage location where the valid data in the 
cleaned segments should be placed and rewritten to. In this paper, 
we propose a new method called R-LFS to dynamically 
reorganize data in disk to approximate the organ pipe heuristic 
that can place data in disk optimally. Basically, frequently 
accessed data are dynamically clustered and placed toward the 
center of disk, whereas less accessed data are moved and placed 
toward the edges of disk to reduce disk seek time. The essence of 
R-LFS is that R-LFS takes advantage of the chance of data 
reorganization during segment cleaning and data writing, no extra 
overhead is incurred for this data reorganization. Besides, because 
hot data and cold data are in nature separately clustered under R-
LFS, cleaning overhead can be substantially reduced as well. 
Performance evaluation under both trace-driven simulation and 
practical implementation on NetBSD/LFS shows that R-LFS can 
effectively improve the performance of LFS.  
Categories and Subject Descriptors
D.4.2 [Operating Systems]: Storage Management – 
allocation/deallocation strategies, garbage collection.
General Terms
Management, Performance, Design, Experimentation. 
Keywords
Log-Structured File System, Garbage Collection, Data 
Rearrangement.
1. INTRODUCTION
Disk performance has become one of the major system 
bottlenecks of a computer system. Log-Structured File System 
(LFS) [8] improves write performance by writing all new data to 
disk in a sequential structure called the log. The log is divided 
into segments by writing segment data sequentially to disk, thus 
disk seek times can be significantly reduced and faster crash 
recovery is permitted. To ensure enough free space for writing 
new data, it requires a garbage collection process called segment 
cleaner to reclaim the storage space occupied by obsolete data. 
However, the performance of the cleaner greatly affects the 
system performance. Therefore, many researches [2,6,8,11-13] 
aim at improving cleaning performance. 
To improve cleaning performance, several researches [4,11] show 
that separating hot data from cold data while performing garbage 
collection or writing data can significantly improve cleaning 
performance in log-based storage systems. This is because hot 
data have high possibility to be updated soon and the original 
space they occupy would become obsolete and invalid soon. 
Therefore, segments full of hot data would soon become garbage 
together. Cleaning these segments can be of low overhead. 
Among these researches, the Dynamic dAta Clustering (DAC) [3] 
technique is an efficient technique to dynamically cluster data 
according to their write access frequencies. It divides storage 
space into several regions. It moves hot data toward upper region 
during data writing and moves cold data toward lower region 
during garbage collection. In this way, data blocks with the 
similar write access frequencies would be effectively clustered in 
the same region. However, DAC is originally proposed for log-
based flash memory storage systems and the disk characteristic is 
different from flash memory that has no seek time and no 
rotational latency at all in accessing stored data. 
Researches [1,10] also show that clustering frequently referenced 
data and placing them near the center of disk can significantly 
reduce disk seek times. Especially, the organ pipe heuristic [5,14] 
can place data in disk optimally if data references are derived 
from an independent, random process with a known, fixed 
distribution. In this method, more frequently accessed data are 
placed toward the center of disk and less frequently accessed data 
are placed toward the edges of disk.  
In this paper, we propose a new method called Region-based LFS
(R-LFS) that combines the advantages of DAC and organ pipe 
heuristic methods to improve LFS performance. The R-LFS 
partitions disk space into several regions and the region 
organization in disk is an approximation to the disk layout of the 
organ pipe heuristic. The frequently accessed data clustered by 
DAC are placed toward the central region located at the center of 
disk. When a data block is updated, it will be promoted to the 
region near center of disk. When the cleaner selects a segment for 
garbage collection, the valid data in the selected segment will be 
demoted to the region toward edge of disk. In this way, hot data 
will be placed toward the central region and cold data will be 
placed toward the fringe region.
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
SAC’07, March 11-15, 2007, Seoul, Korea. 
Copyright 2007 ACM 1-59593-480-4/07/0003…$5.00. 
1136
3.2 Modification of the Cleaner 
In the R-LFS, cleaning process will copy the valid data in the 
victim segments to lower region. Therefore, the cleaner should be 
modified for this change. Mainly four parts need modification. 
First, the amount of empty segments should be separately counted 
for each region. If the amount of empty segments is not enough in 
one region, it starts cleaning. Secondly, the cost-benefit policy is 
used to calculate the cost of every segment in this region, and 
then selects the segment with lowest cost to clean. Thirdly, the 
valid blocks of those selected segments and the corresponding 
region number of those segments are contained in the valid blocks 
array which is then sent to the kernel. Finally, the kernel uses 
these information and starts the clean operation. 
4. SIMULATION STUDY 
4.1 The Simulator and Traces 
We have implemented a trace-driven simulator to simulate the 
read, write, and cleaning operations of a log-based file system. 
Each input trace record contains a block-level I/O request, 
including read or write, request length, block number, and time 
stamp. Table 1 summarizes the input and output parameters of our 
simulator. Table 2 shows all cleaning policies implemented in the 
simulator. “Cost-1” and “Greedy-1” mean R-LFS is not used, 
which represent the original LFS performance. 
Table 1. Simulator parameters 
Input Output
1. Disk parameters
2. Block size, segment size 
3. Cleaning policy (greedy, cost-benefit)[8] 
4. Initial disk utilization 
5. Trace (hplajw, snake) 
6. The number of regions 
1. The number of 
disk seek 
distance 
2. The number of 
blocks copied 
during cleaning
Table 2. Cleaning policies 
Cleaning policy Description 
Cost-1 Using cost-benefit policy 
Cost-2 Using R-LFS (2 regions) and 
cost-benefit policy 
Cost-3 Using R-LFS (3 regions) and 
cost-benefit policy 
Greedy-1 Using greedy policy 
Greedy-2 Using R-LFS (2 regions) and 
greedy policy 
Greedy-3 Using R-LFS (3 regions) and 
greedy policy 
Two real-world traces are used in our experiments. One is hplajw 
trace [9] which recorded two months of access behaviors in a 
personal workstation at HP Laboratories. The main jobs in this 
workstation were sending e-mail and editing text files. The 
system had two disks attached to it. The other is snake trace [9] 
which also recorded two months of access behaviors in a cluster 
server at UC Berkeley. The main use was for compilation and 
editing. The system had three disks attached to it.  
We evaluate the cleaning overhead in terms of the number of 
blocks copied during the cleaning process, which is equal to the 
amount of valid blocks in the selected segments that are moved 
and copied to other free storage space by the cleaner. The 
operation of copying blocks is the main overhead in executing 
garbage collection. The amount of disk seek distance is another 
metric used to measure the performance improvement. 
4.2 Effect on Cleaning Overhead 
When garbage collection occurs, the valid data in the cleaned 
segments should be read out and later written back to other empty 
disk segment. When the disk utilization increases, it needs to 
execute garbage collection more times, so the amount of blocks 
copied increases as well. Figures 3 and 4 show the performance 
comparison results of using different cleaning policies. The 
amount of blocks copied is significantly reduced by R-LFS. When 
R-LFS with three regions is used, the amount of blocks copied 
can be reduced by up to 90.48% for cost-benefit policy and 
93.22% for greedy policy under hplajw trace, and 99.89% for 
cost-benefit policy and 99.9% for greedy policy under snake trace. 
600000
1000000
1400000
1800000
2200000
50 60 70 80 90
disk utilization
nu
m
be
r 
of
 b
lo
ck
s 
co
pi
ed cost-1
greedy-1
cost-2
greedy-2
cost-3
greedy-3
(a) Number of blocks copied (root disk) 
0
100000
200000
300000
400000
500000
50 60 70 80 90
disk utilization
nu
m
be
r 
of
 b
lo
ck
s 
co
pi
ed
cost-1
greedy-1
cost-2
greedy-2
cost-3
greedy-3
(b) Number of blocks copied (swap disk) 
Figure 3. Cleaning overhead comparison (hplajw trace). 
0
100000
200000
300000
400000
500000
50 60 70 80 90
disk utilization
nu
m
be
r 
of
 b
lo
ck
s 
co
pi
ed cost-1
greedy-1
cost-2
greedy-2
cost-3
greedy-3
(a) Number of blocks copied (root disk) 
1138
