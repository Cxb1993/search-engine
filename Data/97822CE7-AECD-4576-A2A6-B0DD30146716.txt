 2
first ordering, critical-task-first ordering, and 
their hybrids.  For the scheduling phase, we 
propose a task-duplication algorithm with a 
look-ahead technique, so that the complexity 
of the new algorithm does not increase.  The 
experiment results show that our algorithm 
outperforms other algorithms for any feasible 
task sequences with respect to the average 
execution times and the average scheduling 
length ratios. 
 
Keywords: communication overhead, data 
alignment, data distribution, distributed 
memory parallel computing systems, indirect 
memory accesses, irregular scientific com-
putation, list-scheduling, load balance, tem-
poral and spatial dependence relations, un-
structured meshes. 
 
二、緣由與目的 
 
For most scientific applications, their 
solution methods have high degree of paral-
lelism, to use indirect memory accesses in 
their algorithms is for saving memory space; 
however, compilers fail to analyze unknown 
indices at compiling time. Therefore, the new 
challenge of a compiler is to understand the 
programming styles of implementing major 
algorithms.  For scientific applications, ma-
jor algorithms include domain decomposition, 
iterative method, greedy method, dynamic 
programming, divide and conquer, branch 
and bound, recursive doubling, program 
transformation, adaptive method, and 
multi-grain method. 
Fortunately, experienced programmers 
write their large-scale scientific application 
programs following certain good program-
ming styles.  For instance, each phase of a 
program segment is defined clearly as con-
secutive nested Do-loops, they may have 
some dependence relations between succes-
sive phases, but they are restricted to have as 
few as possible dependence relations within 
each phase.  Therefore, it has high parallel-
ism in each phase. 
It is our purpose in this project to ana-
lyze real application programs, so that we 
can develop effective compiler techniques to 
handle data alignment among data arrays and 
data distributions between successive pro-
gram segments in which each program seg-
ment may have its best data alignment and 
data distribution, and thus is to find a se-
quence of data alignments and data distribu-
tions which can maximize the load balance 
and minimize the communication overhead 
among processing elements for the whole 
application program. 
For those complex data structures which 
are not easily aligned at compiling time or 
for those complex reference indices which 
cannot be identified at compiling time, we 
suggest programmers to use programming 
language directives to select an appropriate 
data partitioner, which performs partitioning 
data structures in a preprocessing step. Then, 
we can reduce communication overhead us-
ing cheap communication primitives, based 
on run-time support techniques, such as in-
spectors and executors, on line during the 
execution. 
The whole project is propelled by real 
applications over structured or unstructured 
meshes, part of these benchmark programs 
studied in this preliminary report includes 
heat conduction equation, numerical wind 
tunnel for airplane fluid dynamics and nu-
merical engine combustion in reactive flow. 
 
三、研究方法及成果 
 
In this report, we illustrate our tech-
niques using real application programs based 
on their regular or irregular characteristics. 
 
3.1 Regular computation over a structured 
mesh 
Consider the program in Figure 1-(a), 
which solves a 2D heat condition equation 
using the alternating direction implicit (ADI) 
method, which reduces two-dimensional 
problems to a succession of one-dimensional 
problems.  The domain of the partial dif-
ferential equation ut = b1uxx + b2uyy is the unit 
square.  We used the Peaceman-Rachford 
algorithm to formulate the numerical solution 
of the partial differential equation as a sec-
ond-order approximation by solving two sets 
of tridiagonal systems of linear equations. 
The variables of the first set of tridiagonal 
systems correspond to elements from each 
column of an intermediate matrix, and the 
variables of the second set of tridiagonal  
systems correspond to elements from each 
 
Fig. 2 On a linear processor array with four processing elements, data arrays are evenly dis-
tributed according to their alignments within the program segment of the (a) column sweep 
and (b) row sweep. 
                             
row of a target matrix [13].  Using the 
Thomas algorithm, we reduce a tridiagonal 
system of linear equations to three sets of 
first-order recurrence equations. 
S
t x y
∂ ∂ ∂+ + =∂ ∂ ∂
Q E F  
where 
u
v
h
ρ⎡ ⎤⎢ ⎥⎢ ⎥= ⎢ ⎥⎢ ⎥⎣ ⎦
Q , xx
xy
x
u
uu p
uv
uh q
ρ
ρ τ
ρ τ
ρ
⎡ ⎤⎢ ⎥− +⎢ ⎥= ⎢ ⎥−⎢ ⎥+⎢ ⎥⎣ ⎦
E ,  In this benchmark program, all variable 
indices are known at compiling time, there-
fore, we can use compiler techniques to de-
termine data dependence and data alignment. 
Thus, compilers can determine an optimal 
load-balanced data distribution as shown in 
Figure 2. 
xx
yy
y
v
uv
vv p
vh q
ρ
ρ τ
ρ τ
ρ
⎡ ⎤⎢ ⎥−⎢ ⎥= ⎢ ⎥− +⎢ ⎥+⎢ ⎥⎣ ⎦
F ,  
 
0
0
0
xx xy xy yy
u u v v p Pu v
 4
3.2 Stencil computation over a structured 
mesh 
We use a program which implements 
the lid-driven cavity flow as our example. 
The lid-driven cavity flow is probably one 
of the most studied fluid problems in com-
putational fluid dynamics field. The physical 
configuration of a driven cavity flow con-
sists of a square container filled with a fluid. 
The lid of the container moves at a given, 
constant velocity, thereby setting the fluid in 
motion. Due to the simplicity of the cavity 
geometry, applying a numerical method on 
this flow problem in terms of coding is quite 
easy and straightforward. Despite its simple 
geometry, the driven cavity flow retains a 
rich fluid flow physics manifested by multi-
ple counter-rotating recirculating regions on 
the corners of the cavity depending on the 
Reynolds number. The simulation is based 
on Navier-Stokes equations and the most 
referenced solution available [26] in litera-
ture will be used for comparison. The gov-
erning equations of the two-dimensional 
driven-cavity flow are 
P
x y x y t x
τ τ τ τ
y
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥= ⎢ ⎥∂ ∂ ∂ ∂ ∂ ∂ ∂⎢ ⎥+ + + + + +⎢ ⎥∂ ∂ ∂ ∂ ∂ ∂ ∂⎣ ⎦
S
2 (2 )
3xx
u v
x y
τ μ ∂ ∂= −∂ ∂ , 
2 (2 )
3yy
v u
y x
τ μ ∂ ∂= −∂ ∂ ,  
( )xx
u v
y x
τ μ ∂ ∂= +∂ ∂ , 
x
Tq k
x
∂= − ∂ , y
Tq k
y
∂= − ∂ . 
And the calorically perfect gas is assumed, 
ph c T=  and p RTρ= . 
In the program, we adopt a structured 
mesh to represent the computing domain, 
where data variables are defined for each 
cell, edge, and vertex. Each vertex variable 
is updated based on its four adjacent cell 
variables; each edge variable is updated 
based on its two boundary vertex variables 
and its two adjacent cell variables; and each 
cell variable is updated based on its four 
boundary edge variables and eight adjacent 
solving this sparse linear system. 
 
Fig. 4  The unstructured mesh of NACA0012 airfoil, which is decomposed into eight parti-
tions using our quadtree spatial-based partitioning tool. 
 
Since we adopt an unstructured mesh to 
tessellate the computing domain, data struc-
tures are represented by graphs. Thus, in the 
program, in order to save memory space, we 
use array of arrays to implement indirect 
memory access. Therefore, compilers at the 
compiling time cannot determine neither data 
dependence relations no data alignment rela-
tions; compilers only can provide naïve data 
distributions, which may result in obtaining a 
load-imbalanced data partition and requiring 
expensive communication overhead. 
 6
 However, for this kind of unstructured 
mesh application, computation happened 
limitedly among logically local data. If pro-
grammers use an appropriate graph-based 
data partitioner to decompose the unstruc-
tured mesh, it can obtain a completely 
load-balanced data partition requiring less 
communication overhead than the naïve one.  
Figure 4 shows the unstructured mesh of 
NACA0012 airfoil, which is decomposed 
into eight partitions using our quadtree spa-
tial-based partitioning tool. Figure 5 shows 
the computation result of the density con-
tours for the NACA0012 airfoil, when the 
Mach number and angle of attack for incom-
ing flow are 0.8 and3o, respectively. 
 
3.4 Computation represented by task 
graphs 
 we adopt a list-scheduling mechanism 
whose performance depends on the task se-
quence and the scheduling technique.  We 
studied three popular sequences, the first fa-
vors the critical path, the second favors the 
critical task, and the third is a hybrid of the 
first two. In the scheduling phase, we use a 
task-duplication algorithm as a basis, and 
also implement the others for comparison. 
Based on the above-mentioned schedul-
ing algorithm, we propose a look-ahead 
method to improve the performance of list 
scheduling. This method takes the next task 
in the sequence into consideration when as-
signing an immediate task. Through experi-
 
Fig. 6  Description of a sample application in a heterogeneous computing system with three 
PEs. 
 
 
 
Fig. 7  Scheduling of the task graph in Figure 6 with: (a) DUP/LA using CPF ordering 
(makespan = 86), (b) DUP/LA using CTF ordering (makespan = 84), (c) DUP/LA using Hy-
brid ordering (makespan = 84), and (d) HCNFD/LA using HCN ordering (makespan = 92). 
 
In order to minimize data communica-
tion, high-dimensional data arrays should be 
avoided to do data migration, data distribu-
tions for different program segmentations 
should be reallocated dynamically [2], data 
alignments should follow axis alignments, 
and in addition, data distribution should take 
into account temporal dependence relations 
and spatial dependence relations so that 
communication overhead can be minimized. 
For irregular application program, when 
the subscript of a data array element is an ar-
ray element (which is an indirect memory 
access), compiler-time analysis fails to de-
termine data dependency and data alignment 
among data arrays, therefore, a compiler only 
can provide naïve data distributions among 
data arrays. Then it relies on run-time sup-
port techniques, such as inspectors and ex-
ecutors, to reduce communication overhead.   
 8
 10
五、參考文獻 
[1] P.-Z. Lee and Z.M. Kedem. Automatic data and 
computation decomposition on distributed mem-
ory parallel computers. ACM Trans. on Pro-
gramming Languages and Systems, 24(1):1--50, 
Jan. 2002. 
[2] P.-Z. Lee. Efficient algorithms for data distribution 
on distributed memory parallel computers. IEEE 
Trans. Parallel Distributed Syst., 8(8):825--839, 
Aug. 1997. 
[3] P.-Z. Lee. Techniques for compiling programs on 
distributed memory multicomputers. Parallel 
Computing, 21(12): 1895--1923, Dec. 1995. 
[4] P.-Z. Lee and W.-Y. Chen. Generating communica-
tion sets of array assignment statements for 
block-cyclic distribution on distributed memory 
parallel computers. Parallel Computing, 
28(9):1329—1368, Sep. 2002. 
[5] P.-Z. Lee and C.-H. Chang. Unstructured mesh 
generation using automatic point insertion and 
local refinement. Proc. National Computer Sym-
posium, pages B550--B557, Taipei, Taiwan, Dec. 
1999. 
[6] P.-Z. Lee, C.-H. Chang, and M.-J. Chao. A parallel 
Euler solver on unstructured meshes. Proc. ISCA 
13th International Conference on Parallel and 
Distributed Computing Systems, pages 171--177, 
Las Vegas, Nevada, Aug. 2000. 
[7] P.-Z. Lee, C.-H. Chang, and J.-J. Wu. Parallel im-
plicit Euler solver on homogeneous and hetero-
geneous computing environments. AIAA paper 
2001--2588, Proc. 15th AIAA Computational 
Fluid Dynamics Conference, Anaheim, CA, June 
2001. 
[8] P.-Z. Lee, J.-J. Wu, and C.-H. Chang. Partitioning 
unstructured meshes for homogeneous and het-
erogeneous parallel computing environments. 
Proc. International Conference on Parallel Proc-
essing, Vancouver, Canada, August 2002, pages 
315—322. 
[9] Y.-S. Chen, S.-D. Wang, and C.-M. Wang. Tiling 
nested loops into maximal rectangular blocks. 
Journal of Parallel and Distributed Computing, 
35(2):123--132, Feb. 1996. 
[10] Y.-M. Hou, C.-M. Wang, C.-Y. Ku, and L.-H. 
Hsu. Optimal processor mapping for lin-
ear-complement communication on hypercubes. 
IEEE Trans. Parallel Distributed Syst., 
12(5):514--527, May 2001. 
[11] G. Karypis and V. Kumar. METIS: A software 
package for partitioning unstructured graphs, 
partitioning meshes, and computing fill-reducing 
orderings of sparse matrices, version 4.0.Dept. of 
Computer Science, Univ. of Minnesota, Sep. 
1998. 
[12] P.-Z. Lee, C.-H. Yang and J.-R. Yang. Fast algo-
rithms for computing self-avoiding walks and 
mesh intersections over unstructured meshes. 
Advances in Engineering Software, 35(2):61--73, 
Feb. 2004. 
[13] J.~C. Strikwerda. Finite difference schemes and 
partial differential equations. Wadsworth & 
Brooks/Cole Advanced Books & Software, Pa-
cific Grove, CA, Chapter 7.3 The alternating di-
rection implicit (ADI) method, pages 142--153. 
[14] P.-Z. Lee, C.-M. Wang, and J.-J. Wu. Compiler 
and run-time parallelization techniques for scien-
tific computations on distributed memory parallel 
computers. In book High Performance Comput-
ing: Paradigm and Infrastructure, Laurence T. 
Yang and Minyi Guo, editor, pp. 135--181, John 
Wiley & Sons, Inc., 2006. 
[15] J. Wu, R. Das, J, Saltz, H. Berryman, and S. Hi-
randan. Distributed memory compiler design for 
sparse problems. IEEE Trans. Comput., 
44(6):737—753, June 1995. 
[16] J. Saltz, K.Crpwley, R. Mirchandaney, and H. 
Berryman. Run-time scheduling and execution of 
loops on message passing machings. Journal of 
Parallel and Distributed Computing, 8:303--312, 
1990. 
[17] S. Hiranandani, K. Kennedy, and C.-W. Tseng. 
Compiler optimizations for Fortran D on MIMD 
distributed-memory machines. In Proc. Super-
computing, pages 86--100, Nov 1991. 
[18] H. Berryman, J. Saltz, and J. Scroggs. Execution 
time support for adaptive scientific algorithms on 
distributed memory architectures. Concurrency 
and Computation: Practice and Experience, 
3:159--178, 1991. 
[19] High performance Fortran language specification. 
Scientific Programming, 2:1--170, 1993. 
[20] G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, 
U. Kremer, C. Tseng, and M. Wu. Fortran D lan-
guage specification. Technical Report TR90-141, 
Dept. Computer Science, Rice University, Dec 
1990. 
[21] C.-W. Tseng. An Optimizing Fortran D Compil-
ers for MIMD Distributed-Memory Machines. 
PhD thesis, Dept. of Computer Science, Rice Uni-
versity, 1993. 
[22] B. Chapman, P. Mehrotra, and H. Zima. Pro-
gramming in Vienna Fortran. Scientific Pro-
gramming, 1(1):31--50, 1992. 
[23] J. Li. Compiling Crystal for Distributed Memory 
Machines. PhD thesis, Dept. of Computer Science, 
Yale University, 1991. 
[24] C. Koelbel, P. Mehrotra, and J. V. Rosendale. 
Supporting shared data structures on distributed 
memory architectures. In Proc. ACM SIGPLAN 
Symp. on Principles and Practices of Parallel 
Programming, pages 177--186, March 1990. 
[25] M. Rosing, R. B. Schnabel, and R. P.Weaver. 
The DINO parallel programming language. 
Journal of Parallel and Distributed Computing, 
13:30--42, 1991. 
[26] U. Ghia, K. Ghia, and C.Shin. High-Resolutions 
for incompressible flow using the Navier-Stokes 
equations and a multigrid method, Journal of 
Computational Physics, 48:387--411, 1982. 
