可供推廣之研發成果資料表 
□ 可申請專利  █ 可技術移轉                                  日期：2009 年 10 月 22 日 
國科會補助計畫 
計畫名稱： 強健式視覺監控系統之建構與整合：以允許相機可變動性為
例 
計畫主持人： 謝君偉        
計畫編號：NSC97-2221-E-155-060  學門領域：資訊 
技術/創作名稱 允許相機可移動之前景物偵測與異常事件分析技術 
發明人/創作人 謝君偉 
中文：此技術的重點在於使用移動相機，可從自動從移動視訊中偵測出
前景物來，過去的方法都要求相機靜止不動，但此技術並不做此要求。
為了能有效地監控大區域環境，此技術首先利用分類技巧對視訊影像做
關鍵區塊分群，進而建構出整個要監控環境的場景，然後利用一個快速
比對的技巧，可對輸入的視訊比對出目前巡邏車的位置與相對應的場
景，接著我們提出一個蜘蛛網背景相減技巧，即使輸入影像跟場景有所
位移，也能偵測出前景物來，對這些前景物分析，可對大區域環境內所
發生的各種異常事件，做有效地偵測。 
技術說明 
英文： Different from traditional methods which require the cameras being 
static, a novel patch-based approach is proposed for foreground object 
detection and abnormal event detection from a mobile camera using 
concentric features. First of all, a novel scene representation scheme is 
proposed for large-scale surveillance using a set of corners and key frames. 
Then, a novel patch matching scheme is proposed for efficient scene 
searching and comparison. The scheme can reduce the time complexity of 
matching not only from the search space but also feature dimension in 
similarity matching.  Thus, desired scenes can be obtained extremely fast.
After that, a spider-web structure is proposed for missing object detection 
even though there are large camera movements between any two adjacent 
frames. 
可利用之產業 
及 
可開發之產品 
視訊監控系統、Home Care System、大賣場夜間監控 
技術特點 
此技術特點可有效地偵測在移動平台上所拍攝視訊裡所發生的相關異常
事件，如貴重物品的失竊，亦可進行視訊檢索的功能，例如給定一個事
件片段，即可在所有資料庫中，快速的找出相似的畫面。 
推廣及運用的價值 
可應用在許多方面，如安全監控、防盜、醫療診斷等方面，並可置於辦
公大樓、走廊、旅館大廳、停車場、銀行、超商與商務飯店之大廳櫃檯、
或行人步道等公共空間，對進出之人員作安全控管，以達到事件偵測警
示與犯罪預防等目標。 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位
研發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
附件二 
patterns stored in database and find whether it 
has variation. In this system, the robot 
positioning is the first task. Most robot 
positioning methods depend on analyzing the 
formulations of the relation between the 
geometric coordinates and the robot pose. 
However, our robot positioning system selects 
the measurement of ordinal distance which 
was defined in the paper by F. Serratosa and A. 
Sanfeliu [4]. The main advantage is that the 
time-complexity is reduced. Following that, 
the patterns matching technique is required. 
There are three major stages in pattern 
matching, i.e., feature extraction, feature 
representation, and pattern searching. For the 
first stage, various salient features can be 
extracted from images for matching like edges, 
corners, extreme points of pixel intensities or 
gradients, and so on. Recently, Lowe [5] 
proposed SIFT (Scale-Invariant Feature 
Transform) to extract scale-invariant features 
for pattern matching. Since the feature has 
higher tolerance to scale, illumination, and 
pose variations, it has been widely adopted in 
computer vision community for various 
applications like image retrieval, image 
mosaicing, object recognition, or surveillance. 
As to feature description, different robust and 
stable structures can be extracted from the 
surrounding intensities around a window mask 
of a feature point. For the correlation 
technique, the simplest representation of a 
feature point is directly its surround intensities. 
However, this representation is not invariant 
to different geometrical transformations like 
scaling and rotation. This transformation 
invariance can be achieved if the statistical 
properties of a feature are extracted from its 
surround pixels like moments, shape context, 
SIFT descriptor, and so on. Technically, for a 
feature descriptor, its size will be proportional 
to the dimension of its used window. In 
addition, if it has more invariance to 
geometrical transformations, its discriminated 
ability will also decrease more. For the third 
stage, it locates the targets of interest from 
images using the above feature descriptors and 
additional constraints through an exhaustive 
search. Usually, the search is highly 
computationally expensive and unstable when 
patterns have different geometrical changes. 
For speeding up the search process, Y. –S. 
Chen et al. [7] used a winner-update strategy 
to prune impossible candidates. In [6], S. 
Zokai and G. Wolberg used the polar 
transform for transform the rotation angle into 
a linear space so that the rotation parameters 
of a pattern can be efficiently found. When the 
search dimension is high, statistical methods 
like RANSAC and Monte Carlo will be 
preferable for getting the initial matches 
which will be finely tuned through a nonlinear 
optimization process. For example, T. 
Tamminen and J. Lampinen [8] proposed a 
sequential Monte Carlo method to sample and 
constrain the search space and then find the 
final solution through a Bayesian frame work. 
RANSAC [9] is another common approach to 
sample the search for estimating model 
parameters from a set of observed images. 
Technically, the sampling technique attempts 
to prune out the search space for significantly 
improving the efficiency of pattern matching. 
However, for real-time applications, this paper 
reduces the time complexity of this task not 
only from the search space but also the feature 
dimension in similarity calculation. We 
present a novel method which reduces not 
only the search space but also the feature 
space for similarity matching. 
三、研究方法 
The process flowchart of our surveillance 
system is shown in Fig. 1. In this system, the 
camera is mounted on a mobile robot and 
optical axis is vertical to the facing direction 
of the robot. The proposed approach consists 
of four steps in two phases. Training phase 
contains scene classification and feature 
points tracking. Scene matching and 
exceptional changes detection are in detection 
phase. In training phase, we classify the 
panorama into scenes according to select key 
frames and store the features patterns. In 
detection phase, scene matching is used for 
robot positioning. Compare feature points in 
before and after whether there are differences. 
Exceptional change is occurred if some 
feature points are missing in a region. 
 
Fig. 1  Flowchart of the system. 
p(x,y) is to be matched within an input image 
I(x,y) of size n×n. For one pixel c in the image, 
we can measure the distance between it and p 
as follows 
 ( ) ( ) ( )( )/2 /2 22
/2 /2
, , ,
m m
c c
i m j m
d c p I x i y j p i j
=− =−
= + + −∑ ∑ , (2) 
where c has the coordinates (xc,yc). Clearly, 
the time complexity of calculating d2(c,p) is 
O(m2). The time complexity can be reduced 
into O(1) if a kernel function is used based on 
the concept “integral image” for avoiding lots 
of redundant calculation. Given a point c in 
I(x,y), we define its kernel-based sampling 
function with the radius ρ as follows 
 ( ) ( )1,
p R
k c E p
R
ρ
∈
= ∫ , (3) 
where { }|R p p c ρ= − < and E is the edge 
map of I. The kernel-based sampling function 
forms the basic statistical measurement in our 
task for pattern matching. It is noticed that 
k(c,ρ) is rotation-invariant. When different 
radii ρ are used, a new descriptor can be 
defined for describing the visual 
characteristics of c, i.e., k(c,1), 
k(c,2),…,k(c,m). Then, a vector k(c) with m 
elements can be defined for describing c, i.e., 
k(c)=(k(c,1), k(c,2),…,k(c,m)). For the pattern 
p, we also use the similar idea to obtain its 
descriptor k(p). Then, the distance between c 
and p can be redefined as follows 
 ( ) ( ) ( )( )22
1
, , ,
m
k i
i
d c p w k c i k p i
=
= −∑ , (4) 
where wi is a weight to weight the term k(c, i). 
Compared with Eq.(2), the time complexity 
for calculating ( )2 ,kd c p  is O(m) which 
reduces one order of similarity calculation 
than Eq.(2). 
3.2.3 Integral Feature for Rotation 
Invariance 
To obtain the descriptor k(c), we use a circular 
kernel sampling function to calculate each 
term k(c,ρ) like Fig. 3(a). Technically, the 
circular kernel sampling function can be 
approximated with a square kernel sampling 
function π(c,ρ) defined as follows 
 ( ) ( )/2 /22
/2 /2
1, ,c c
i j
c E x i y j
ρ ρ
ρ ρ
π ρ ρ =− =−= + +∑ ∑ . (5) 
Like Fig. 3 (b), we use π(c, ρ) to 
approximate k(c, ρ). Since π(c, ρ) can be 
very efficiently obtained through an integral 
image structure, we use the descriptor π(c) to 
approximate k(c) for efficient pattern 
matching. 
 
(a)                 (b) 
Fig. 3 (a) A circular sampling function. (b) 
Its squared approximation centered at 
location c. 
 
Fig. 4  Calculation of integral image. 
Given an edge map E, its integral image 
S(x,y) contains the sum of edge points in E 
accumulated from the original (0,0) to the 
pixel (x,y), i.e., 
 ( ) ( )
0 0
, ,
yx
i j
S x y E i j
= =
= ∑∑ . (6) 
S(x, y) can be obtained using only one scan 
over E. Given a rectangle region Hi bounded 
by (l,t,r,b), its sum of edge magnitudes can be 
very efficiently achieved by taking advantages 
of the integral image S. In Fig. 4, sum(Hi) can 
be easily calculated as follows 
 ( ) ( ) - ( ) - ( )
             ( , ) ( , ) - ( , ) - ( , )
i isum H A B C H A A B A C
S r b S l t S l b S r t
= + + + + + +
= + . (7) 
Based on Eq.(7), sum(Hi) can be performed 
very efficiently using one addition and two 
subtractions. If we consider π(c,ρ) as a 
version of Hi, π(c,ρ) can be efficiently 
obtained with the time complexity O(1). Thus, 
the time for obtaining π(c,ρ) is independent 
of ρ. 
3.3 Ring Structure and Pyramid Process 
To more accurately match a pattern, a ring 
structure with layers will be proposed here for 
dealing with its inner localities. With different 
ρ, π(c,ρ) can form a ring structure for 
pattern matching as Fig. 3(b). It is a good 
rotation invariant feature for matching a 
pattern if it has different rotation changes. 
With π(c,ρ), we can perform the coarse 
matching process for getting a set CM of 
possible matching candidates. Then, like Fig. 
2, we enter the fine matching stage to get a set 
FM of fine matching candidates from CM 
using different ring structures for this filtering 
process. 
However, not all patterns can be matched 
successfully like the absence of patterns. We 
can solve this problem using the coordinate 
translation which presented in section 3.1.2. 
Therefore, all the coordinates of patterns can 
be recovered on input frame as Fig. 9(b) 
shows. Then, connect the points with each 
other and reject too long thread. We can build 
a “spider web” whose nodes are the points like 
Fig. 9(c). 
Assume SM and SI are two spider webs 
built from the patterns set of the matched 
scene and features of input frame, respectively. 
We use the distance transform to convert each 
spider web into the distance map. Assume 
MS
DT  is the distance map of SM. Then the 
value of a pixel g in 
MS
DT  is its shortest 
distance to all foreground pixels in SM as 
 
 
( ) min ( , )
M
M
S p S
DT g d g p
∈
=  (10) 
where d(g,p) is the Euclidian distance between 
g and p. The result of distance transform is 
shown in Fig. 9(d)When calculating ( )
MS
DT g , 
SM and SI must be normalized to an unit size 
and their centers must be set to the origins of 
MS
DT  and 
IS
DT , respectively. We can obtain 
the exceptional region by subtracting between 
MS
DT  and 
IS
DT . According to the number of 
features within the region, the exceptional 
object can be determined. 
 
四. 實驗結果 
 
(a) 
 
(b) 
Fig. 10 Results of exceptional changes 
detection with simple background. 
In all the experiments of our surveillance 
system, the video sequences (AVI) are with 
the dimension 320×240. All the testing videos 
are under different environments like 
simple/complex background and trajectory of 
robot. In our system, the exceptional change 
can be a(n) absence/presence of object even 
though the small size. Fig. 10 shows the 
results of exceptional object detection with a 
simple background. Fig. 11 shows the results 
of exceptional object detection with a complex 
background. There are lots of texture bring out 
lots of feature patterns. However, our system 
works very well. Fig.12 shows the results 
when the robot was turning at a corner.  All 
the above experimental results have proved 
that our proposed system is robust under 
different conditions. 
 
(a) 
 
(b) 
Fig. 11 Results of exceptional changes 
detection with simple background. 
 
Fig.12 Results of exceptional changes 
detection at a corner. 
 
四、結論 
This paper proposes a novel patch-based 
approach for abnormal event detection from a 
moving camera using concentric features. 
Detailed contributions of this paper are 
described in the following. 
(1) A novel scene representation scheme is 
proposed for large-scale surveillance using 
a set of corners and key frames.   
(2) A novel patch matching scheme is 
proposed for efficient scene searching and 
comparison.    
(3) A spider-web structure is proposed for 
missing object detection even though the 
camera movement is larger.     
Experimental results prove that our proposed 
system is efficient, robust, and superior in 
missing object detection and abnormal event 
analysis.  
 
