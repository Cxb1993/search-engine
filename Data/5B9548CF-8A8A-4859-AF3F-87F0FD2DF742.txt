 1
行政院國家科學委員會補助專題研究計畫成果報告 
在機器學習中有效選取特徵子集合以提升分類正確率新方法之研究 
The Study of A Novel and Effective Feature Subset Selection Method 
計畫編號： NSC95-2221-E-022-010 
執行期限： 95 年 8 月 1 日至 96 年 7 月 31 日 
主持人：黃淇竣副教授  國立高雄海洋科技大學資訊管理系 
計畫參與人員：王崧傑  國立中山大學電機工程研究所 
葉雯菁  國立高雄海洋科技大學資訊管理系 
楊順治  國立高雄海洋科技大學資訊管理系 
黃秋玲  國立高雄海洋科技大學資訊管理系 
 
一、中文摘要 
 
資料預處理(Data Preprocessing)是機
器學習或資料探勘技術得以成功的重要關
鍵 之 一 。 其 中 ， 特 徵 選 取 (Feature 
Selection)，又稱特徵子集合選取(Feature 
Subset Selection)是最重要且最常用的資料
預處理方法之一。特徵選取可謂是從原始
特徵集合中挑選一個特定的特徵子集合之
方法，並由特定的評估標準 (Evaluation 
Criterion)來衡量所挑選的特徵子集合是否
為最佳的。一般而言，選取最佳的特徵子
集合是非常困難的，許多與特徵選取或特
徵子集合選取相關的問題都被列為是
NP-hard 的問題。假設一分類問題有N個特
徵或屬性，在特徵空間(Feature Space)中則
會形成 2N 個不同的特徵子集合或狀態
(State)可供搜尋，換言之，由於搜尋整個特
徵空間所需要的時間將會隨著N 呈指數成
長，從特徵空間中選取與問題直接相關的
特徵，以提高探勘效能或分類正確率，將
是非常棘手且複雜的問題。 
基於此，本研究計畫旨在提出在機器
學習中有效選取特徵子集合以提昇分類正
確率的新方法，此一新方法將具有多項優
點，並得以有效解決諸多機器學習分類問
題中特徵或屬性個數過多、分類問題之資
料具有雜訊(Noisy)、不相關(Irrelevant)及重
複(Redundant)等特徵或屬性的問題，並藉
由有效決定搜尋起始點 (Search Starting 
Point)及選取特徵子集合提昇機器學習或
資料探勘演算法的速度，進而增加機器學
習或資料探勘的效能(Performance)，其中包
括預測正確率或分類正確率及學習或探勘
結 果 的 可 被 瞭 解 性 (Result 
Comprehensibility)。本計畫之相關研究成果
已發表於國際知名期刊與學術研討會，包
括[35-38]等。 
 
關鍵詞：特徵選取、機器學習、分類、真
實世界應用問題 
 
Abstract 
 
In recent years, feature selection (or 
feature subset selection) has gained great 
popularity and plays an important role for 
data preprocessing in machine learning and 
data mining.  Feature selection can be 
viewed as a process for selecting a feature 
subset from original features of a specific 
classification problem.  An evaluation 
criterion is used to measure the selected 
feature subset.  By using the selected 
feature subset of a classification problem, 
machine learning or data mining methods 
generally yield superior performance (i.e. 
high classification accuracy).  However, 
finding the optimal feature subset is usually a 
difficult problem because a classification 
problem may contain N features with 2N 
possible feature subsets. 
In this research project, a novel and 
effective feature subset selection method is 
proposed.  First, a new evaluation criterion 
is introduced to measure the optimality of 
each feature in a classification problem.  
Each feature with great importance will be 
included as the search starting point of 
 3
方法，藉由檢視對應的探勘效能以決
定每一個特徵子集合的優劣。探勘效
能較好的特徵子集合會被視為是較
佳的特徵選取解答。一般而言，此類
架構比起過濾模型，需花費更多的計
算時間與複雜度[2,9]。 
(3) 混合模型(Hybrid Model)[4,15] 
    在此類架構中，混合採用過濾模型與
封套模型，以評估和選取特徵子集
合 。 相 關 方 法 如 BBHFS[15] 及
Xing’s[4]等。 
在分類問題中，眾多的樣本特徵或屬
性形成一個特徵空間或特徵集合。不同的
特徵子集合則被視為特徵空間中不同的狀
態，而產生特徵子集合即是在特徵空間中
作啟發式搜尋(Heuristic Search)不同狀態
的過程。因此，在作搜尋特徵子集合的動
作前，必須先決定搜尋的起始點[3]，這也
主導了整個搜尋的方向。相關的選擇如下： 
(1) 前向式(Forward)：在此，搜尋的起始
點的決定，主要先由一個特徵空集合
(Empty Set)作為起始點開始搜尋，接
著依序加入不同的特徵，以找到最佳
的特徵子集合。 
(2) 後向式(Backward)：在此，搜尋的起
始點的決定，主要先由一個特徵完全
集合(Full Set)作為起始點開始搜尋，
接著依序移除不同的特徵，以找到最
佳的特徵子集合。 
(3) 雙向式(Bidirectional)：在此，搜尋的
起始點的決定，主要同時由特徵空集
合(Empty Set)及特徵完全集合(Full 
Set)等兩個起始點開始搜尋，接著依
序同時加入或移除不同的特徵，以找
到最佳的特徵子集合。 
(4) 搜尋起始點由亂數產生 (Randomly 
Generated)：在此，特徵子集合搜尋起
始點由亂數決定，這個方法將有機會
跳 離 局 部 的 最 佳 解 (Local 
Optima)[16]，但是其缺點是亂數決定
搜尋起始點會導致執行對應的搜尋
方 法 多 次 可 能 會 得 到 不 一 致
(Inconsistent)的「最終特徵子集合」
之問題。 
在決定特徵子集合搜尋的起始點之
後，接著即需針對特徵空間進行搜尋。各
類搜尋方法如下： 
(1) 完整式搜尋(Complete Search) 
在此方法中，可以保證由 2N個不同的
特徵子集合或狀態中找到最佳的特
徵子集合，如Focus[17]與ABB[18]。
與窮舉式的搜尋方法不同的是，此類
方法藉由各種啟發式的技巧(如有系
統 地 裁 剪 對 應 的 搜 尋 樹 (Search 
Tree))，減少搜尋次數，但卻得以保
證找到最佳的特徵子集合。 
(2) 循序式搜尋(Sequential Search) 
此類方法包括循序前向式選取法
[10]、循序後向式選取法[10]及雙向式
選取法[10]等等。這些方法藉由一次
增加一個特徵(遞增法)或一次減少一
個特徵(遞減法)的方式，試著找到最
佳或次佳的特徵子集合。在 take k add 
l 選取法[16]中，則是先一次增加或減
少 k 個特徵，接著一次減少或增加 l
個特徵(k > l)，進而找到最佳或次佳
的特徵子集合。 
(3) 隨機式搜尋(Random Search) 
    如前所述，特徵子集合搜尋起始點由
亂數決定，接著之後的每一個特徵子
集合搜尋點亦由亂數決定。由於加入
隨機選取的特性，這類方法將有助於
跳脫局部最佳解(Local Optima)，進而
有機會找到最佳的特徵子集合。但是
其缺點是執行該隨機式搜尋方法多
次可能會得到不一致的「最終特徵子
集合」之問題。 
在產生不同的特徵子集合之後，接著
即需透過各種明確的評估基準(Evaluation 
Criterion)，進行特徵子集合之評估(Subset 
Evaluation)，以汰弱留強，從中找出較佳的
特徵子集合。常用的評估基準整理如下： 
(1) 距離基準(Distance Measure) 
以兩類別分類問題而言，距離基準可
以測得任何一個特徵區分兩個不同
類別的能力高低。 
(2) 資訊基準(Information Measure) 
在這一類的評估基準中，大多先決定
分類問題中可從每一個特徵得到的
Information Gain，它被定義為是採用
該特徵所求得之事前不確定性(Prior 
Uncertainty) 與預期事後不確定性
其 中  )()(minminmin 0 kxkx jkj −=∆ ∀∀ , 
)()(maxmaxmax 0 kxkx jkj
−=∆
∀∀
, ζ∈[0,1] (一般而言, ζ
多半設為 0.5), i = j = 1, 2, …, m, and k = p = 
1, 2, …, n。 
在此，對於每一個樣本的單一屬性
(Attribute)p 而言，  代表了
 與 之間的相似程度(或關聯程
度，Similarity)。假如 大於 
，則  與   之間的
Similarity 大於  與  之間的
Similarity，反之亦然。基於此，樣本與樣
本之間的灰關聯程度 (Grey Relational 
Grade，GRG)可計算如下： 
( )(),(0 pxpxGRC i )
)(0 px )( pxi
( ))(),( 10 pxpxGRC
( ))(),( 20 pxpxGRC )(0 px )(1 px
)(0 px )(2 px
( ) ∑
=
= n
1
00 ))(),((n
1,
k
ii kxkxGRCxxGRG           (2) 
其中 i = 1, 2, …, m。 
在此，灰關聯分析(或灰關聯程度)的主
要特性如下： 
假如
 
大於 ，那麼
樣本x
( 10 , xxGRG ) )( 20 , xxGRG
0 與 x1 之間的差異(Difference)將小
於x0 與 x2之間的差異，反之亦然。 
藉由灰關聯分析所建構出來的樣本空
間，我們將之定義為灰關聯結構 (Grey 
Relational Structure)。本研究以此關聯結構
為基礎，以測量樣本與樣本之間的相似或
關聯程度，在研究方法上有其創新性。 
在[25]中，實驗研究結果已證明，應用
此處所提出的灰關聯模型測定樣本間相似
程度，將會得到比以 HOEM 、 Value 
Difference Metric 、 HVDM 、 IVDM 及
WVDM 等為相似度函數，有更佳的分類正
確率。同時，上述灰關聯模型擁有多項特
性及優點，詳見[26-27]。 
由於前述的距離基準、資訊基準、相
關性基準及一致性基準等特徵子集合評估
基準，均與探勘效能無直接關聯。以上述
幾種基準作為評估指標所得到的最終特徵
子集合在未來實際進行分類的效能表現上
往往不一定是最佳的[3]。在此，本研究所
提出的評估基準則是與探勘效能直接相
關，在進行特徵選取之前，以該分類法或
學習法則，針對每一個候選特徵子集合一
一作分類效能評估，從中找到分類效能表
現最佳的特徵子集合。一般而言，以此最
佳的特徵子集合針對未知類別(Unknown 
Class)的樣本進行分類時，所得到的探勘效
能也將會是極為優異的[3]。此外，此處所
採用的評估基準因為直接考慮到探勘演算
法或分類器及其分類效能，所得到的最終
特徵子集合其分類效能表現將遠優於前述
與探勘效能無關的評估基準所得到的最終
特徵子集合之分類效能表現。 
然而，這一類與探勘效能直接相關的
評估基準其缺點主要為每評估一個特徵子
集合都必須透過特定的探勘演算法或分類
器計算其相對應的分類效能，顯然這一類
的評估基準需花費較多的計算時間 [2]，亦
即具有較高的時間複雜度。而評估基準中
所選擇的探勘演算法或分類器本身完成學
習或探勘過程所需要的時間也將主導整個
特徵選取方法所需要的時間。而本研究所
採用的基於灰關聯結構之最近鄰居分類法
提供了多項優點，以解決上述 Wrapper 
Model 與時間複雜度相關的諸多問題。 
最近鄰居分類法的優點主要是讓人容
易瞭解及易於實現，同時也能產生良好的
分類效果[20-21]，同時目前也廣泛地運用
於各種有關樣本分類的問題。另外，不同
於類神經網路及決策樹等分類學習機制，
最近鄰居分類法不需要事前的訓練。相較
之下，類神經網路[29]或決策樹[30]等學習
或探勘過程較為耗時的探勘演算法或分類
器，將較不適於作為封套模型的評估基
準。以上這些重要優點說明了該方法非常
適合用在此處作為特徵子集合選取的評估
基準。 
假設一分類問題其樣本之特徵集合以
U表示，即U={S1, S2, S3,…, SN}，其中Si代
表第i個特徵或屬性，而|U|=N代表該分類
問題所有特徵的個數。此外，以Ui=U-Si代
表一特徵子集合，Ui子集合包括所有的特
徵或屬性，唯不包括Si，換言之，|Ui|=N-1。 
採用前述的基於灰關聯結構之最近鄰
居分類法，針對該分類問題中的所有樣本
並僅選取特徵子集合Ui，以每次只剔除一
個樣本的交互驗證方式 (Leave-One-Out 
Cross-Validation[31]，我們可以得到一個對
應的分類正確率，假設以Accuracy(Ui)表
示。在此，排除選取特徵Si後所得到的
Accuracy(Ui)值較低，代表該特徵Si是相對
重要、與分類問題直接相關、排除不選取
 5
 7
Shuttle、Segment。 
表一  9 個分類問題(包括其樣本個數、特徵個數及
類別個數) 
分題問題 樣 本 個
數 
特 徵 個
數 
類 別 個
數 
Car 1728 6 4 
Yeast 1484 8 10 
Postoperative 90 18 3 
Pageblocks 5473 10 5 
Primarytumor 339 17 21 
Nursey 12960 8 5 
Soybeansmall 47 21 4 
Shuttle 43500 9 7 
Segment 2310 18 7 
 
針對每個特徵，我們分別計算其評估
指標Accuracy(Ui)，進而得到每個特徵對於
分類效能之影響程度的排列順序(Ranked 
List)。 
如表二所示(特徵或屬性按阿拉伯數
字編號)，分別列出每一個分類問題的最佳
特徵子集合及在排列順序中重要程度或影
響程度前二分之一的特徵子集合(特徵或
屬性依Accuracy(Ui)之重要程度高低排
序)。我們發現在排列順序中重要程度或影
響程度前二分之一的所有特徵，絕大部份
也都出現於最佳特徵子集合中。換言之，
前述本研究所採用選取排列順序位於前二
分之一的所有特徵作為特徵子集合搜尋的
起始點，確實深具其有效性，並進一步獲
得驗證。 
表二  9 個分類問題的最佳特徵子集合及在排列順
序中重要程度或影響程度前二分之一的特徵子集
合之比較 
分題問題 最佳特徵子集
合 
在排列順序中
重要程度或影
響程度前二分
之一的特徵子
集合 
Car 1,2,4,5,6 6,1,4 
Yeast 1,2,3,4,5,6,7,8 3,4,1,8 
Postoperative 2,5,6,8,9,10,11,1
3,16,18 
16,17,10,5,2,13,
11,9,8 
Pageblocks 1,3,4,5,7,8,10 5,4,2,1,7 
Primarytumor 1,2,3,4,5,7,10,13
,15,16,17 
2,1,16,7,10,4,3,1
7,5 
Nursey 1,2,5,6,7,8 8,2,1,7 
Soybeansmall 11,12,13,14,15,1
6,17,18,19,20,21 
21,20,19,18,17,1
6,15,14,13,12,11
Shuttle 1,2,5,7,9 2,1,9,8,7 
Segment 1,2,4,6,7,9,10,13
,15,17,18 
2,1,17,15,10,12,
5,18,13 
 
由於本研究所提出的方法可快速而有
效地選擇特徵子集合搜尋的起始點，該搜
尋起始點可涵蓋最佳特徵子集合中大部份
重要的特徵或屬性，換言之，所提出方法
將有機會可快速地得到最佳特徵子集合，
同時大幅地減少搜尋次數，加快特徵子集
合選取的速度。 
在一開始有效地選擇搜尋起始點後，
接下來即需訂定特徵子集合搜尋策略。同
樣地，以前述分類問題為例，假設每一個
樣 本 共 有 八 個 特 徵 ， 分 別 以
{A,B,C,D,E,F,G,H}表示，若{A,B,C,D,E,}
是未來的最佳特徵子集合，而{ A,B,C,D}
為前述本研究所提出方法所得到的特徵子
集合搜尋起始點。在特徵子集合的表示方
法中，本研究所提出的特徵選取方法將以
一個序列11110000來代表此一特徵子集合
{A,B,C,D}，其中’1’代表對應的特徵被選取
作為特徵子集合，而’0’則代表對應的特徵
未被選取。相同地，以序列 11111000 來代
表特徵子集合{A,B,C,D,E}，以此類推。 
表三  初始的特徵子集合搜尋起始點與下一個可
能候選的特徵子集合搜尋點 
初始的特徵子集合搜尋
起始點 
下一個可能候選的特徵
子集合搜尋點 
11110000 即 代 表
{A,B,C,D} 
11100000 即 代 表
{A,B,C} 
11010000 即 代 表
{A,B,D} 
10110000 即 代 表
{A,C,D} 
01110000 即 代 表
{B,C,D} 
11111000 即 代 表
{A,B,C,D,E} 
11110100 即 代 表
{A,B,C,D,F} 
11110010 即 代 表
{A,B,C,D,G} 
11110001 即 代 表
{A,B,C,D,H} 
本研究所提出的特徵子集合搜尋策
略，即是讓初始的特徵子集合搜尋起始點
與所有下一個可能候選的特徵子集合搜尋
點，僅相異一個 bit；如表三中所舉的例子，
分別列出初始的特徵子集合搜尋起始點
(11110000)與所有下一個可能候選的特徵
子集合搜尋點，在依序以前述之特徵評估
指標(即 Accuracy)評選每一個候選特徵子
集合後，保留探勘效能或分類效能表現較
 9
(6) 本方法所採用的特徵評估指標同時
兼具了過濾模型[12-13]與封套模型 
[2,14]的重要特性與多項優點，以評估
和選取特徵子集合。 
(7) 本方法結合了循序前向式選取法
[10]、循序後向式選取法[10]及雙向式
選取法[10]等的概念與優點。 
(8) 本方法可避免隨機式搜尋方法執行
多次可能會得到不一致的「最終特徵
子集合」之缺點。 
在此，由嚴謹的理論探討至完整的實
驗設計過程，本計畫亦遭遇了一些困難，
包括相關理論之實現、特徵選取相關演算
法的實作與除錯、以及與其他特徵選取方
法的比較等等課題。在有效地掌握整個專
案計畫的執行進度下，方得以一一解決這
些困難，順利完成本計畫。 
 
五、成果自評 
 
根據本研究計畫內容，依序執行已完
成進度如下： 
(1) 已完成蒐集相關文獻，確定研究方
向，擬定研究目的、方法與步驟。 
(2) 已完成理論分析與文獻探討：蒐集最
近鄰居分類方法、特徵選取方法、分
類器設計、資料探勘理論、機器學習
理論、灰關聯結構及相似度函數、以
及各項理論之實務應用等相關重要
文獻，作為本研究的理論基礎與特徵
選取方法研究設計之參考。 
(3) 已完成建構所提特徵子集合選取方
法的實驗環境。 
(4) 已完成研究設計有效選取特徵子集
合以提升分類正確率的機制與整合
測試，同時尋找不同應用領域之分類
問題的訓練樣本及測試樣本等資料
集。將本研究所得到的特徵選取與分
類結果與人工處理所得的結果及其
他各種特徵選取方法進行比較與分
析，評估分類正確率及特徵選取效
能，並提出改進的策略，以供日後進
一步研究的基礎。 
(5) 已完成撰寫研究報告並以「在機器學
習中有效選取特徵子集合以提升分
類正確率的新方法」為研究主軸，將
相關研究結果發表在國際著名人工
智慧與機器學習領域 SCI 學術期刊與
研討會。本計畫之相關研究成果已發
表於國際知名期刊與學術研討會，包
括[35-38]等(如附件)。 
(6) 已將研究心得歸納結論，並提出未來
研究方向及建議。 
 
本研究之具體成果如下： 
 
(1) 已完成「在機器學習中有效選取特徵
子集合以提升分類正確率的新方法」
演算法及其應用程式。 
(2) 已完成特徵子集合選取技術之研究
設計報告書與建議。 
(3) 已完成撰寫研究報告並以「在機器學
習中有效選取特徵子集合以提升分
類正確率的新方法」為研究主軸，將
相關研究結果發表在國際著名人工
智慧與機器學習領域 SCI 學術期刊與
研討會。本計畫之相關研究成果已發
表於國際知名期刊與學術研討會，包
括[35-38]等(如附件)。 
 
六、參考文獻 
[1] I. Witten and E. Frank, Data mining - practical 
machine learning tools and techniques with java 
implementations, Morgan Kaufmann, San 
Francisco, CA, 2000. 
[2] R. Kohavi and G.H. John, “Wrappers for Feature 
Subset Selection,” Artificial Intelligence, vol. 97, 
nos. 1-2, pp. 273-324, 1997. 
[3] H. Liu and L. Yu, “Toward Integrating Feature 
Selection Algorithms for Classification and 
Clustering,” IEEE Trans. Knowl. Data Eng. 
17(4), pp. 491-502, 2005. 
[4] E. Xing, M. Jordan, and R. Karp, “Feature 
Selection for High-Dimensional Genomic 
Microarray Data,” Proc. 15th Int’l Conf. 
Machine Learning, pp. 601-608, 2001. 
[5] Y. Yang and J.O. Pederson, “A Comparative 
Study on Feature Selection in Text 
Categorization,” Proc. 14th Int’l Conf. Machine 
Learning, pp. 412-420, 1997. 
[6] A.L. Blum and R.L. Rivest, “Training a 3-Node 
Neural Networks is NP-Complete,” Neural 
Networks, vol. 5, pp. 117-127, 1992. 
[7] R. Agrawal, T. Imilienski, and A. Swami, 
“Mining association rules between sets of items 
in large databases,” In Proceedings of the ACM 
SIGMOD International Conference on 
Management of Database, pp. 207-216, 1993. 
[8] M. Dash and H. Liu, “Feature Selection for 
Classification,” Intelligent Data Analysis: An 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                            2006 年 11 月 12 日 
報告人姓
名 
黃淇竣 
 
 
服務機構
及職稱 
國立高雄海洋科技大學資訊管理系 
助理教授 
     時間 
會議 
     地點 
2006 年 11 月 3 日至 
2006 年 11 月 6 日 
大陸廣州 Ramada Pearl 
Hotel   
本會核定
補助文號
95-2914-I-022-003-A1 
會議 
名稱 
 (中文) 2006 年計算智慧與安全國際學術會議 
 (英文) 2006 International Conference on Computational Intelligence and 
Security (CIS) 
發表 
論文 
題目 
 (中文) 一個植基於灰色理論以選取特徵子集合的特徵排序新方法 
 (英文) A Novel Grey-Based Feature Ranking Method for Feature Subset 
Selection 
一、參加會議經過 
 
2006 年計 算 智慧與 安 全國際 學 術會議 (2006 International Conference on 
Computational Intelligence and Security, CIS2006)係美國電子電機工程師學會(IEEE -
the Institute of Electrical and Electronics Engineers)主辦的年度大型國際學術會議，
為期四天，是國際有關計算智慧與資訊安全研究領域的研討會。主辦地點位於大陸廣州
Ramada Pearl Hotel (去年則在大陸西安舉行)，協辦單位包括 IEEE (Hong Kong) 
Computational Intelligence Chapter，Guangdong University of Technology，Xidian 
University，IEEE Hong Kong Section，Hong Kong Baptist University 及 Jinan University
等。今年吸引來自 32 個國家與地區的領域專家學者共 2078 篇論文投稿，每篇經由三位專家
學者審稿人的嚴格審查，共有 399 篇論文被接受並收錄於會議論文集中，論文接受率僅
19.2%，其中包括 130 篇 Extended 論文及 269 篇 Regular 論文。本會議深受該領域專家學
者的重視，參與者來自世界各地，具有相當大的影響與重要性。 
在議程方面，11 月 3 日主要接受來自世界各地與會者報到與註冊，並領取會議議程及
論文集兩冊。11 月 4 日至 11 月 6 日接連三天的密集議程，共安排了 52 個議程場次。其中 4
日 15 個場次，5日 20 個場次，6日 17 個場次。主要的 Session 分類如下： 
 
1. Learning and Fuzzy Systems 4 場 
2. Bio-Inspired Computing 4 場 
3. Pattern Recognition 3 場 
4. Information Hiding and Authentication 2 場 
5. Cryptography 4 場 
6. Intrusion Detection 1 場 
7. Data Mining 2 場 
8. Application 6 場 
9. Image and Signal Processing 8 場 
10. Evolutionary Computation 5 場 
11. Security Model, Analysis and Management 4 場 
此次非常感謝國科會補助報告人參與 2006 計算智慧與安全國際學術會議的相關費用，
除發表相關研究成果外，更有助於研究視野的拓展，收穫良多。同時，非常期盼國內相關學
術機構或單位能承接此類大型國際學術會議的舉辦，積極策劃邀請國外相關領域專家學者參
與，除提供學術研究成果分享與意見交流的平台外，更得以與國際接軌，提昇台灣相關學術
研究在國際間的能見度。 
 
四、攜回資料名稱及內容 
 
1. 2006 International Conference on Computational Intelligence and Security (CIS) 
會議議程表  
2. 2006 International Conference on Computational Intelligence and Security (CIS) 
會議論文集兩冊(Part I and II), 2006/11/2-2006/11/6 
3. 大會註冊單  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
pattern classification model is computed and then each 
attribute can be ranked.  Features with higher 
classification effectiveness are more important and 
relevant for the specific classification task.  The set of 
these important and relevant features is thus considered 
as the final feature subset for feature subset selection.  
Experiments performed on various application 
domains are reported to demonstrate the performance 
of the proposed approach.  It can be easily seen that the 
proposed approach yields high performance and is 
helpful for pattern classification. 
The rest of this paper is organized as follows.  The 
concept of grey-based classification model is reviewed 
in Sections 2.  Section 3 proposes a novel grey-based 
feature ranking method with search starting point 
determination for feature subset selection.  In Section 4, 
experiments performed on various datasets are 
discussed.  Finally, Section 5 gives the conclusions. 
 
2. Grey-based Pattern Classification Model 
 
In [10], a grey-based pattern classification model 
(instance-based learning approach) is presented to 
identify new, unseen instances or examples.  In this 
approach, grey relational analysis (GRA) [11-13] is 
adopted as a similarity function to precisely represent 
the connections among all instances or examples in a 
specific classification problem.  Here, the relationships 
among a referential example and the compared 
examples will be determined by calculating two 
informative values: the grey relational coefficient 
(GRC) and the grey relational grade (GRG). 
Consider that a set of m+1 instances or examples 
{x0, x1, x2, …, xm} is given, where x0 is the 
referential example and x1, x2, …, xm are the 
compared examples.  Each instance xi has n features 
and is represented as xi = (xi(1), xi(2), …, xi(n)).  The 
grey relational coefficient regarding feature p between 
instances   and  can be computed as, 
( ) ,
max)()(
maxmin
)(),(
0
0 ∆+−
∆+∆
= ζ
ζ
pxpx
pxpxGRC
i
i         (1) 
where )()(minminmin 0 kxkx jkj
−=∆
∀∀
, 
)()(maxmaxmax 0 kxkx jkj
−=∆
∀∀
, ζ ∈ [0,1] 
(Generally, 5.0=ζ ), i = j = 1, 2, …, m, and k = p = 1, 
2, …, n. 
Here, ( ))(),(0 pxpxGRC i  is considered as the 
similarity or connection between instances 0x  and ix  
with respect to attribute p (i.e.,
 
)(0 px  and )( pxi ).  If 
( ))(),( 10 pxpxGRC
 
is larger than ( ))(),( 20 pxpxGRC  then 
the similarity between )(0 px  and )(1 px  exceeds that 
between )(0 px  and )(2 px ; otherwise the former is 
smaller than the latter.  Moreover, if instances
 
0x  and 
ix  have the same value for attribute p, ( ))(),(0 pxpxGRC i  
will be one.  By contrast, if instances
 
0x  and ix  are 
different to a great deal for attribute p, ( ))(),(0 pxpxGRC i  
will be nearly zero. 
The grey relational grade between instances 0x  and 
ix  is subsequently calculated as, 
( ) ∑
=
=
n
1
00 ))(),((n
1,
k
ii kxkxGRCxxGRG                         (2) 
where i = 1, 2, …, m. 
The principle property obtained by ordering all grey 
relational grades in the instance space, also known as 
grey relational analysis (GRA), is described as, 
If ( )10 , xxGRG
 
is smaller than ( )20 , xxGRG  then the 
difference between instances x0 and x1 exceeds that 
between instances x0 and x2; otherwise the former is 
smaller than the latter. 
For pattern classification, the grey-based instance 
space (i.e., the relationships among all instances) will 
be constructed based on the above grey relational 
analysis,  That is, each instance xi and its nearest 
neighbor xi* can be determined based on the degree of 
GRG.  Obvoiusly, 
*, ii xxGRG  is the smallest.  
Accordingly, a new, unseen instance can be classified 
by using the class label of its nearest instance in the 
grey-based instance space (This learning principle is 
derived from the nearest neighbor classification rule 
[14-15]).  Consequently, a grey-based pattern 
classification model, which can yield excellent 
classification accuracy [10], is stated clearly. 
 
3. Grey-based Feature Ranking Method 
 
In this section, a grey-based feature ranking method 
for feature subset selection is presented.  Firstly, the 
classification effectiveness of each feature of a specific 
classification task in the above–mentioned grey-based 
pattern classification model is stated. 
Consider that a set of m labeled training instances or 
examples V={v1, v2, …, vm} is given in a specific 
classification problem.  Each instance has n attributes 
or features, which are described as F = (f1, f2, …, fn).  
By using the above-mentioned grey-based pattern 
classification model with leave-one-out (LOO) cross-
validation method [17], a classification accuracy 
regarding the training set V and the feature set F 
(denoted by ACC(V,F)), can be determined.  Here, 
leave-one-out cross-validation means that each 
subset selection.  That is, features with high 
classification effectiveness of each classification 
domain are considered as the final feature subset and 
experimental results demonstrate that the obtained 
feature subset is helpful for pattern classification. 
 
Table 1. The classification abilities or 
accuracies (as mentioned in Section 3) of the 
above-mentioned grey-based pattern 
classification model while the proposed 
feature ranking method for feature subset 
selection is performed or not 
Classification 
task 
The proposed 
feature ranking 
method is not 
used for feature 
subset selection 
The proposed 
feature ranking 
method is used 
for feature 
subset selection 
Bridges  87.62% 100.00% 
Cpu 69.38%  69.38% 
Dis 97.89%  98.43% 
Echocardiogram 98.65%  98.65% 
Glass 73.83%  78.50% 
Lenses 75.00%  83.33% 
Tae 66.22%  66.22% 
Voting 92.87%  95.40% 
Waveform 76.14%  78.06% 
Average 81.96% 85.33% 
 
5. Conclusions 
 
In this paper, a novel grey-based feature ranking 
method for feature subset selection is proposed.  Here, 
the classification effectiveness of each attribute of a 
specific classification problem in the grey-based 
pattern classification model is computed and then each 
attribute can be ranked.  Features with higher 
classification effectiveness are more important and 
relevant for the specific classification task.  The set of 
these important and relevant features is thus considered 
as the final feature subset for feature subset selection.  
Experiments performed on various application 
domains are reported to demonstrate the performance 
of the proposed approach.  It can be easily seen that the 
proposed approach yields high performance and is 
helpful for pattern classification. 
 
6. References 
[1] R. O. Duda and P. E. Hart, Pattern classification and 
scene analysis, John Wiley & Sons, 1973. 
[2] T. M. Mitchell, Machine Learning, McGraw-Hill, New 
York, 1997. 
[3] R. Kohavi and G. H. John, Wrappers for feature subset 
selection, Artificial Intelligence, vol. 97(1-2), 1997, pp. 273-
324. 
[4] M. Dash, H. Liu, Feature selection for classification, 
Intelligent Data Analysis, vol. 1, 1997, pp. 131-156. 
[5] A. Blum and P. Langley, Selection of relevant features 
and examples in machine learning, Artificial Intelligence, vol 
97, 1997, pp. 245-271. 
[6] G.H. John, R. Kohavi, and K. Pfleger, “Irrelevant Feature 
and the Subset Selection Problem,” Proc. 11th Int’l Conf. 
Machine Learning, pp. 121-129, 1994. 
[7] H. Liu and L. Yu, “Toward Integrating Feature Selection 
Algorithms for Classification and Clustering,” IEEE Trans. 
Knowl. Data Eng. 17(4), pp. 491-502, 2005. 
[8] H. Liu and H. Motoda, Feature Selection for Knowledge 
Discovery and Data Mining. Boston: Kluwer Academic, 
1998. 
[9] I. Inza, P. Larrañaga, B. Sierra, Feature Subset Selection 
by Bayesian networks: a comparison with genetic and 
sequential algorithms, International Journal of Approximate 
Reasoning, vol.27, 2001, pp. 143-164.   
[10] C. C. Huang and H. M. Lee, "An Instance-based 
Learning Approach based on Grey Relational Structure," 
Applied Intelligence. (accepted) 
[11] J. Deng, “The theory and method of socioeconomic grey 
systems,” Social Sciences in China, vol. 6, pp. 47-60, 1984. 
(in Chinese) 
[12] J. Deng, “Introduction to grey system theory,” The 
Journal of Grey System, vol. 1, pp. 1-24, 1989. 
[13] J. Deng, “Grey information space,” The Journal of Grey 
System, vol. 1, pp. 103-117, 1989. 
[14] B.V. Dasarathy, Nearest Neighbor (NN) Norms: NN 
Pattern Classification Techniques, Los Alamitos, California: 
IEEE Computer Society Press, 1990. 
[15] T. M. Cover and P. E. Hart, “Nearest neighbor pattern 
classification,” IEEE Trans. on Information Theory, vol. 13, 
no. 1, pp. 21-27, 1967. 
[16] C. L. Blake and C. J. Merz, UCI Repository of machine 
learning databases 
[http://www.ics.uci.edu/~mlearn/MLRepository.html]. 
Irvine, CA: University of California, Department of 
Information and Computer Science, 1998. 
[17] G. C. Cawley and N. L. C. Talbot, “Efficient leave-one-
out cross-validation of kernel fisher discriminant classifiers,” 
Pattern Recognition 36(11), pp. 2585-2592, 2003. 
[18] D. W. Aha, D. Kibler, M. K. Albert, Instance-based 
learning algorithms, Machine Learning, 6 (1991) 37-66. 
[19] C. J. Watson, P. Billingsley, D. J. Croft and D. V. 
Huntsberger, Statistics for Management and Economics, 5th 
ed., Allyn and Bacon, Boston, 1993. 
[20] J. R. Quinlan, “Induction of decision trees,” Machine 
Learning, vol. 1, pp. 81-106, 1986. 
 
 
  Sunday, 5th November 
7:30a.m.-8:30a.m. Registration (Ramada Pearl Hotel) 
8:30a.m.-9:30a.m. Keynote 2  (Room A): Professor Chang Wen Chen,  The Florida Institute of Technology, USA 
9:35a.m.-10:35a.m.  Room A-Sun-01
Evolutionary Comput. 
 
CI06140014,CI06150010, 
CI06150123,CI06150084 
Room B-Sun-01 
Bio-Inspired Computing 
 
CI06110005,CI06110064,
CI06110065,CI06110074 
Room C-Sun-01 
Security Model, Analysis 
and Management 
IS06440001,IS06480036, 
IS06480046,IS06480041 
Room D-Sun-01 
Information Hiding and 
Authentication 
IS06540010,  IS06470017, 
IS06540013,  AP06780016 
 
Room E-Sun-01 
Image and Signal processing 
 
AP06810310,AP06810328, 
AP06720015,AP06810285 
10:35a.m.-10:50a.m. Tea/Coffee Break 
10:50a.m.-12:05p.m.  Room A-Sun-02:
Evolutionary Comput. 
 
CI06150129,CI06150017, 
CI06150022,CI06150070, 
CI06150117 
 
Room B-Sun-02 
Bio-Inspired Computing 
 
CI06130013,CI06130037,
CI06190002,CI06150059,
 
Room C-Sun-02 
Cryptography 
 
IS06410071,IS06410046, 
IS06410053,IS06410057, 
IS06410087 
Room D-Sun-02 
Network Systems and 
Security 
AP06840128,AP06840006,
AP06840050,IS06510059 
Room E-Sun-02 
 Image and Signal Processing 
 
AP06720024,AP06720034, 
AP06770010,AP06810317 
12:05p.m.-13:30p.m. Lunch 
13:30p.m.-15:15p.m.  Room A-Sun-03
 Learning and Fuzzy Systems
 
 
CI06180062,CI06180074 
CI06210007,CI06220006, 
CI06240017,CI06180071, 
CI06240036 
Room B-Sun-03 
 Multi-Agents & 
Autonomy-Oriented 
Computing 
CI06170087,CI06170089 
CI06170010,CI06170016 
CI06170030,CI06170045,
CI06120008 
Room C-Sun-03 
 Pattern Recognition 
 
 
AP06830075,AP06830090 
AP06830129,AP06830134,
AP06830135 ,AP06830054,
AP06830062 
Room D-Sun-03 
Application  
 
 
AP06850021,AP06850027,
AP06850046,AP06850060,
AP06850071,AP06800022,
AP06800039 
Room E-Sun-03 
Cryptography 
 
 
IS06420016,IS06420075 
IS06420007,IS06420008 
IS06420024,IS06410052, 
IS06410056 
15:15p.m.-15:35p.m. Tea/Coffee Break                                                                                                                  
15:35p.m.-17:45p.m.  Room A-Sun-04
 Learning and Fuzzy Systems
 
 
CI06150041,CI06110067, 
CI06220011,CI06160010, 
CI06160019,CI06240015, 
AP06830140 
Room B-Sun-04 
Multi-Agents and 
Autonomy-Oriented 
Computing 
CI06170079,CI06170081,
CI06170084,CI06260012,
CI06260029,CI06260030,
CI06150103,AP06750117
Room C-Sun-04 
 Pattern Recognition 
 
 
AP06830088,AP06830109 
AP06830155,AP06830160 
AP06830027,AP06830164, 
AP06830106 
Room D-Sun-04 
Application  
 
 
AP06850093,AP06850113 
AP06850120,AP06850147 
AP06850157,AP06850158 
 
Room E-Sun-04 
Cryptography 
 
 
IS06410017,IS06410019, 
IS06410021,IS06410088, 
IS06410085,IS06410036 
 
18:00p.m.-20:00p.m. Conference Banquet 
 ii
A Novel Grey-Based Feature Ranking Method for Feature Subset Selection 
 
 
Chi-Chun Huang 
Department of Information 
Management 
National Kaohsiung Marine 
University 
142 Hai Jhuan RD., Nanzih 
District, Kaohsiung 811, 
Taiwan, China 
cchuang@mail.nkmu.edu.tw 
Hsin-Yun Chang 
Department of Business 
Administration 
Chin-Min Institute of 
Technology 
110 Hsueh-Fu Road, Tou-
Fen, Miao-Li, Taiwan, China 
Department of Industrial 
Technology Education, 
National Kaohsiung Normal 
University  
116 Heping 1st RD., Lingya 
District, Kaohsiung 802, 
Taiwan, China 
ran_hsin@yahoo.com.tw 
 
Cheng-Hong Yang 
Department of Computer 
Science and Information 
Engineering 
National Kaohsiung 
University of Applied 
Sciences 
415 Chien Kung Road, 
Kaohsiung 80778, Taiwan, 
China 
chyang@cc.kuas.edu.tw 
 
Abstract 
 
In this paper, a novel grey-based feature ranking 
method for feature subset selection is proposed. 
Experiments performed on various application 
domains are reported to demonstrate the performance 
of the proposed approach.  It can be easily seen that 
the proposed approach yields high performance and is 
helpful for pattern classification. 
 
1. Introduction 
 
In pattern classification [1], a set of training 
instances or patterns (also known as training set) is 
given, where each instance is represented by n 
attributes (or features) and an output label.  Various 
pattern classification or machine learning methods [2] 
have been developed to classify new, unseen instances 
based on extracting useful knowledge from the training 
set.  Generally, all attributes of each instance will be 
considered for knowledge extraction.  However, real-
world classification tasks usually contain irrelevant or 
redundant attributes, which may result in performance 
degradation (i.e., degrade the classification accuracy).  
As a result, many feature subset selection approaches 
[3-8] have been investigated to reduce the 
dimensionality in a specific classification task.  That is, 
irrelevant or redundant attributes will be removed from 
the original feature set for pattern classification. 
Feature subset selection can be viewed as a process 
that selects important or relevant attributes from the 
original feature set.  In other words, it is also a search 
problem [9], where each search state in the search 
space specifies a possible feature subset.  Feature 
subset selection yields many advantages for pattern 
classification.  Firstly, the cost of gathering training or 
unseen instances can be reduced.  Secondly, pattern 
classification or machine learning models can be 
constructed faster.  Furthermore, the performance (i.e., 
classification accuracy) and the comprehensibility of 
the learning models can be improved.  If each instance 
in a specific classification problem contains n 
attributes, the search space will be composed of 2n 
candidate feature subsets.  Obviously, exhaustive 
search through the entire search space has a very high 
computational cost and thus is usually unfeasible in 
practice, even for medium-sized n [7].  Consequently, 
it is difficult to select a best feature subset for pattern 
classification from the entire search space with respect 
to the tradeoff between high classification accuracy 
and small number of selected features. 
In this paper, a novel grey-based feature ranking 
method for feature subset selection is proposed.  Here, 
the classification effectiveness of each attribute of a 
specific classification problem in the grey-based 
instance in V is used as the test instance once and other 
instances in V are used as the corresponding training 
instances.  In other words, the grey-based pattern 
classification model will be performed m times, with 
respect to m instances and n features in V.  
Accordingly, the average classification accuracy can be 
obtained.  As a result, ACC(V,F) is the overall, 
baseline and informative classification accuracy of the 
grey-based pattern classification model for the 
corresponding classification problem. 
Let Fi=F-{fi}.  Also, a corresponding classification 
accuracy ACC(V, Fi) can be determined.  The value of 
ACC(V, Fi) indicates the classification accuracy 
obtained by using the grey-based pattern classification 
model with the feature subset Fi (That is , feature fi is 
not considered in the feature space of the grey-based 
pattern classification model).  In other words, the 
difference between ACC(V,F) and ACC(V, Fi), denoted 
as DIF(fi), points out the classification effectiveness of 
each feature fi in the grey-based pattern classification 
model.  Here, feature fi is excluded from the original 
feature set F and the relationships or relevance among 
all other features in the grey-based pattern 
classification model are reflected.  If the difference 
between ACC(V,F) and ACC(V, Fi) (i.e., DIF(fi)) is big 
enough, feature fi can then be viewed as a relevant and 
important feature for the classification task.  This is 
because the exclusion of feature fi from the original 
feature set F will significantly degrade the overall and 
baseline classification accuracy.  In other words, 
feature fi should be firstly considered in feature subset 
selection.  Conversely, feature fi can be viewed as an 
irrelevant and meaningless feature if the difference 
between ACC(V,F) and ACC(V, Fi)(i.e., DIF(fi)) is 
slight.  In other words, feature fi should be lastly 
considered in feature subset selection. 
Restated, each attribute or feature in a specific 
classification task can be ranked according the 
corresponding classification effectiveness (i.e., DIF(fi)) 
in the grey-based pattern classification model.  
Consequently, the proposed grey-based feature ranking 
method has wrapper nature [3] (In wrapper feature 
subset selection methods, each candidate feature or 
feature subset is evaluated according to the 
classification ability obtained by the pattern 
classification model).  That is, features will be selected 
based on the special properties of the corresponding 
pattern classification model and thus the goal of feature 
subset selection or feature ranking method here is to 
maximize the classification accuracy.  Notably, in the 
above-mentioned grey-based pattern classification 
model, the nearest neighbor classification concept [14-
15] is adopted for pattern classification.  In other 
words, the proposed grey-based feature ranking 
method, with wrapper nature, uses the nearest neighbor 
classification rule in each feature or feature subset 
evaluation.  As a result, the proposed feature ranking 
process is an efficient method for feature subset 
selection since the nearest neighbor classification rule 
does not have learning time [18] and is simple and easy 
to implement.  Meanwhile, the proposed process is 
deterministic heuristic because the same solution will 
be obtained from different runs. 
Restated, consider that a set of training instances 
V={v1, v2, …, vm} with feature set F = {f1, f2, …, fn} is 
given.  Based on the concept of classification 
effectiveness of each attribute fi in F (i.e. DIF(fi)), with 
respect to the training set V in the grey-based pattern 
classification model, a novel method or algorithm for 
feature subset selection is proposed as follows. 
Step 1: 
Calculate the classification effectiveness of each 
attribute fi (i.e, DIF(fi)= ACC(V,F)-ACC(V,Fi)), where 
Fi=F-{fi}.  Here, ACC(V,F) and ACC(V,Fi) indicate the 
classification accuracy obtained by using the grey-
based classification model with the original feature set 
F and the feature subset Fi, respectively.  As a result, 
all features or attributes can be ranked. 
Step 2: 
Select the features with classification effectiveness 
(i.e., high DIF(fi)) greater than or equal to zero from 
the original feature set F.  These features, denoted as 
feature subset S, are used as the final feature subset for 
pattern classification. 
As stated in the above algorithm, the features in S, 
with higher classification effectiveness (DIF(fi)) are 
more important and relevant for the specific 
classification problem.  Thus, the exclusion of these 
features in S from the original feature set F will 
significantly degrade the overall and baseline 
classification accuracy.  In other words, these features 
in S should be firstly considered for feature subset 
selection. 
 
4. Experimental Results 
 
To demonstrate the power of the proposed feature 
ranking method for feature subset selection, various 
real datasets [16] (classification tasks) were used for 
performance comparison. 
Table 1 represents the classification abilities or 
accuracies (as mentioned in Section 3) of the above-
mentioned grey-based pattern classification model with 
respect to the classification tasks or datasets while the 
proposed feature ranking method for feature subset 
selection is performed or not.  Obviously, for these 
classification domains, the average classification 
abilities or accuracies can be increased when the 
proposed feature ranking method is used for feature 
A Novel Grey-Based Feature Ranking Method for Feature Subset 
Selection 
Chi-Chun Huang 
Department of Information Management, 
National Kaohsiung Marine University 
142 Hai Jhuan RD., Nanzih District, Kaohsiung 811, Taiwan 
Corresponding phone: +886-7-3617141 ext. 3907 
Fax number: +886-7-3617141 ext. 3907 
 E-mail: cchuang@mail.nkmu.edu.tw 
 
Hsin-Yun Chang 
Department of Business Administration,  
Chin-Min Institute of Technology 
110 Hsueh-Fu Road, Tou-Fen, Miao-Li, Taiwan 
Department of Industrial Technology Education, 
National Kaohsiung Normal University  
116 Heping 1st RD., Lingya District, Kaohsiung 802, Taiwan 
Corresponding phone: +886-37-605670 
Fax number: +886-37-605674 
E-mail: ran_hsin@ms.chinmin.edu.tw 
 
Cheng-Hong Yang 
Department of Computer Science and Information Engineering,  
National Kaohsiung University of Applied Sciences 
415 Chien Kung Road, Kaohsiung 80778, Taiwan 
Corresponding phone: +886-7-3814526 
Fax number: +886-7-3836844 
E-mail: chyang@cc.kuas.edu.tw 
 
Abstract 
In this paper, a novel grey-based feature ranking method for feature subset selection is 
proposed.  The classification effectiveness of each attribute of a specific classification problem 
is proposed and then each attribute can be ranked.  Features with higher classification 
effectiveness are more important and relevant and thus considered as the final feature subset for 
pattern classification.  Experiments performed on various application domains are reported to 
demonstrate the performance of the proposed approach.  The proposed approach yields better 
performance than other existing feature subset selection methods and is helpful for improving 
the classification accuracy in pattern classification. 
Keywords: Pattern Classification, Feature Subset Selection, Feature Ranking Method 
Subject Index: CO1 Artificial Intelligence 
and thus considered as the final feature subset for pattern classification.  As a result, the 
grey-based feature ranking method proposed here has wrapper nature (Kohavi and John, 1997) 
(In the wrapper models, each candidate feature or feature subset is evaluated according to the 
classification ability obtained by the pattern classification model).  That is, features will be 
selected based on the special properties of the corresponding pattern classification model and 
thus the goal of feature subset selection or feature ranking here is to maximize the classification 
accuracy.  The wrapper models, which focus mainly on improving the classification accuracy 
of pattern classification tasks, often yield better performance (i.e., high classification accuracy) 
than the filter models.  Notably, in the grey-based pattern classification model, the nearest 
neighbor classification concept (Dasarathy, 1990; Cover and Hart, 1967) is adopted for pattern 
classification.  In other words, the proposed grey-based feature ranking method, with wrapper 
nature, uses the nearest neighbor classification rule in each feature or feature subset evaluation.  
As a result, the proposed feature ranking process is an efficient method for feature subset 
selection since the nearest neighbor classification rule does not have learning time (Aha et al., 
1991) and is simple and easy to implement.  Experiments performed on various application 
domains are reported to demonstrate the performance of the proposed approach.  The proposed 
approach yields performance superior to other existing feature subset selection methods and is 
helpful for improving the classification accuracy in pattern classification. 
  The rest of this paper is organized as follows.  The concept of grey-based pattern 
classification model is reviewed in Section 2.  Section 3 proposes a novel grey-based feature 
ranking method for feature subset selection.  In Section 4, experiments performed on various 
datasets are discussed.  Finally, Section 5 gives the conclusions. 
2. Grey-based Pattern Classification Model 
A grey-based pattern classification model (instance-based learning approach) is presented in 
(Huang and Lee, 2006) to identify new, unseen instances or examples.  In this approach, grey 
relational analysis (GRA) (Deng, 1984; Deng, 1989a; Deng, 1989b) is adopted as a similarity 
 2
3. Grey-based Feature Ranking Method 
  In this section, the classification effectiveness of each feature of a specific classification task 
in the grey-based pattern classification model is firstly proposed and detailed.  Based on the 
concept of the proposed classification effectiveness of each attribute of a specific classification 
task, a novel method or algorithm for feature subset selection is proposed and summarized. 
Consider that a set of m labeled training instances or examples V={v1, v2, …, vm} is given in a 
specific classification problem.  Each instance has n attributes or features, which are described 
as F = (f1, f2, …, fn).  By using the grey-based pattern classification model with leave-one-out 
(LOO) cross-validation method (Cawley and Talbot, 2003), a classification accuracy regarding 
the training set V and the feature set F (denoted by ACC(V,F)), can be determined.  Here, 
leave-one-out cross-validation means that each instance in V is used as the test instance once 
and other instances in V are used as the corresponding training instances.  In other words, the 
grey-based pattern classification model will be performed m times, with respect to m instances 
and n features in V.  Accordingly, the average classification accuracy can be obtained.  As a 
result, ACC(V,F) is the overall, baseline and informative classification accuracy of the 
grey-based pattern classification model for the corresponding classification problem. 
Let Fi=F-{fi}.  Also, a corresponding classification accuracy ACC(V, Fi) can be determined.  
The value of ACC(V, Fi) indicates the classification accuracy obtained by using the grey-based 
pattern classification model with the feature subset Fi (That is , feature fi is not considered in the 
feature space of the grey-based pattern classification model).  In other words, the difference 
between ACC(V,F) and ACC(V, Fi), denoted as DIF(fi), points out the classification 
effectiveness of each feature fi in the grey-based pattern classification model.  Here, feature fi is 
excluded from the original feature set F and the relationships or relevance among all other 
features in the grey-based pattern classification model are reflected.  If the difference between 
ACC(V,F) and ACC(V, Fi) (i.e., DIF(fi)) is big enough, feature fi can then be viewed as a 
relevant and important feature for the classification task.  This is because the exclusion of 
 4
wrapper models, which focus mainly on improving the classification accuracy of pattern 
classification tasks, often yield performance superior (i.e., high classification accuracy) to filter 
models.  Notably, in the grey-based pattern classification model, the nearest neighbor 
classification concept (Dasarathy, 1990; Cover and Hart, 1967) is adopted for pattern 
classification.  In other words, the proposed grey-based feature ranking method, with wrapper 
nature, uses the nearest neighbor classification rule in each feature or feature subset evaluation.  
As a result, the proposed feature ranking process is an efficient method for feature subset 
selection since the nearest neighbor classification rule does not have learning time (Aha et al., 
1991) and is simple and easy to implement. 
4. Experimental Results 
  To demonstrate the performance of the proposed feature ranking method for feature subset 
selection, thirty-eight datasets (Blake and Merz, 1998) (classification tasks) were used for 
performance comparison.  Table 1 represents the classification abilities or accuracies (as 
mentioned in Section 3) of the above-mentioned grey-based pattern classification model with 
respect to the above thirty-eight classification tasks or datasets while the proposed approach is 
performed or not (In the experiments, the cross-validation technique (Stone, 1974) was used).  
Obviously, of these classification domains, the average classification abilities or accuracies can 
be increased from 82.35% to 85.46% when the proposed approach is used.  That is, features 
with high classification effectiveness for each classification domain are considered as the final 
feature subset and experimental results demonstrate that the obtained feature subset is helpful 
for pattern classification.   
In addition, the proposed approach was compared with many well-known feature subset 
selection methods, including SFS (Liu and Motoda, 1998), SBS (Liu and Motoda, 1998), BFSE 
(Liu and Yu, 2005) and BFSF (Liu and Yu, 2005) (bidirectional feature subset selection 
approach that starts with a full feature set).  Among these existing feature subset selection 
methods, two evaluation criteria, including correlation-based evaluation criterion (Hall, 1998) 
 6
performed.  Notably, the proposed approach performs better than eight other feature subset 
selection approaches.  In other words, the final feature subset obtained by using the proposed 
approach is also helpful for improving the classification accuracy in pattern classification, under 
the well-known decision tree pattern classification approach (Quinlan, 1986). 
Table 2 
The classification accuracies of the well-known decision tree pattern classification approach (Quinlan, 1986) while 
the proposed approach or other existing feature subset selection methods are performed. 
SFS : Sequential forward selection (Liu and Motoda, 1998), BFSE : Bidirectional feature subset selection approach 
that starts with an empty feature set (Liu and Yu, 2005), SBS : Sequential backward selection (Liu and Motoda, 1998), 
BFSF : Bidirectional feature subset selection approach that starts with a full feature set (Liu and Yu, 2005), COR : 
Correlation-based evaluation criterion (Hall, 1998), CON : Consistency-based evaluation criterion (Liu and Setiono, 
1996; Liu and Yu, 2005) 
 Feature Subset Selection Methods  
Classification 
task 
None SFS 
vs.  
COR 
SFS 
vs.  
CON 
BFSE 
vs.  
COR 
BFSE 
vs.  
CON 
SBS 
vs.  
COR 
SBS 
vs.  
CON 
BFSF 
vs.  
COR 
BFSF 
vs.  
CON 
Proposed
Approach
Allbp 97.09  96.90  96.99 96.90 96.99 96.90 96.96 96.90  97.09  97.18 
Allrep 99.12  99.28  99.26 99.28 99.26 99.28 99.26 99.28  99.15  99.29 
Anneal 97.87  97.68  98.56 97.68 98.56 97.68 98.71 97.68  97.87  98.32 
Australian 85.59  84.38  85.59 84.38 85.59 84.38 85.17 84.38  85.17  85.74
Autompg 78.79  83.71  78.07 83.71 78.07 83.71 78.07 83.71  78.07  82.71 
Breastw 95.01  95.01  95.31 95.01 95.34 95.01 95.34 95.01  95.35  95.25 
Bridges 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00
Cpu 67.35  67.35  67.35 67.35 67.35 67.35 67.35 67.35  67.35  72.32 
Dermatology 96.07  97.40  96.34 97.40 96.42 97.40 83.26 97.40  96.07  96.80 
Dis 99.01  98.39  98.85 98.39 98.85 98.39 98.85 98.39  99.07  99.11 
Echocardiogram 94.36  94.21  94.21 94.21 94.21 94.21 94.21 94.21  94.36  94.36 
Echoi 82.50  82.50  82.50 82.50 82.50 82.50 82.50 82.50  82.50  82.95 
Flag 72.36  71.84  71.76 71.84 71.76 71.84 61.69 71.84  58.89  75.23 
Glass 68.08  68.50  68.35 68.50 68.35 68.50 68.35 68.50  68.35  72.02 
Hcleveland 53.30  54.95  53.74 54.95 53.74 54.95 53.74 54.95  53.74  55.04 
Hepatitis 78.77  80.51  79.23 80.51 81.02 80.51 83.30 80.51  78.57  82.68 
Hhunggarian 77.87  79.37  79.26 79.37 79.26 79.37 79.26 79.37  79.26  77.58 
Horsecolic 84.78  82.12  85.48 82.12 85.48 82.12 75.35 82.12  75.19  84.99 
Hypothyroid 99.15  98.24  99.27 98.24 99.27 98.24 98.94 98.24  99.17  99.18 
Imagetrain 87.14  87.29  88.86 87.29 88.86 87.29 85.10 87.29  83.38  89.14 
Ionosphere 89.69  90.60  88.35 90.60 88.35 90.60 85.94 90.83  90.46  91.22 
Iris 94.73  94.80  94.80 94.80 94.80 94.80 94.80 94.80  94.80  94.80 
Lenses 83.50  56.50  83.50 56.50 83.50 56.50 83.50 56.50  83.50  83.50
Lungcancer 40.33  57.42  59.42 57.42 59.42 57.42 42.67 57.42  40.33  67.25 
Lymphography 76.41  75.20  77.82 75.20 77.90 75.20 75.92 75.20  77.70  77.06 
Musk 82.33  82.91  81.71 82.91 78.11 82.91 79.50 82.91  82.41  82.91
Pageblocks 96.96  97.08  96.96 97.08 96.96 97.08 96.96 97.08  96.96  97.14 
Promoters 79.00  87.96  90.43 87.96 90.88 87.96 56.09 87.96  78.99  79.00
Satelliteimage 83.25  83.37  83.77 83.37 83.77 83.37 82.26 83.37  83.21  83.61 
Sick 98.68  97.32  98.20 97.32 98.20 97.32 98.20 97.32  98.69  98.41 
Sponge 64.98  70.50  69.32 72.09 65.64 71.11 61.71 70.45  64.23  64.98
Tae 50.86  46.30  50.63 46.30 50.63 46.30 50.63 46.30  50.63  50.86 
Voting 96.27  94.99  96.29 94.99 95.90 94.99 96.09 94.99  96.06  96.50 
Water 70.42  71.08  72.08 71.08 72.08 71.08 63.64 71.08  70.29  70.82
Waveform 76.62  77.12  75.31 77.12 75.31 77.12 76.78 77.12  76.80  77.26 
Wine 93.14  93.09  94.76 93.09 94.76 93.09 92.76 93.09  93.75  94.55 
Yeast 56.37  56.33  56.33 56.33 56.33 56.33 56.33 56.33  56.33  56.37 
Zoo 92.61  93.18  94.03 93.18 94.03 93.18 96.05 93.18  91.61  93.58 
Average 82.64  82.77  83.76 82.82 83.62 82.79 80.93 82.78  81.98  84.20 
 8
7. Deng, J., 1984, “The Theory and Method of Socioeconomic Grey Systems,” Social 
Sciences in China, Vol. 6, pp. 47-60. 
8. Deng, J., 1989a, “Grey Information Space,” The Journal of Grey System, Vol. 1, pp. 
103-117. 
9. Deng, J., 1989b, “Introduction to Grey System Theory,” The Journal of Grey System, Vol. 
1, pp. 1-24. 
10. Duda, R. O., and Hart, P. E., 1973, Pattern Classification and Scene Analysis, John Wiley 
& Sons. 
11. Hall, M. A., 1998, “Correlation-based Feature Subset Selection for Machine Learning,” 
PhD Dissertation, University of Waikato. 
12. Huang, C. C., 2006, “A Novel Grey-Based Reduced NN Classification Method,” Pattern 
Recognition, Vol. 39, pp. 1979-1986. 
13. Huang, C. C., and Lee, H. M., 2006, “An Instance-based Learning Approach based on 
Grey Relational Structure,” Applied Intelligence, Vol. 25, pp. 243-251. 
14. Inza, I., Larrañaga, P., and Sierra, B., 2001, “Feature Subset Selection by Bayesian 
Networks: a Comparison with Genetic and Sequential Algorithms,” International Journal 
of Approximate Reasoning, Vol. 27, pp. 143-164. 
15. Kohavi, R., and John, G. H., 1997, “Wrappers for Feature Subset Selection,” Artificial 
Intelligence, Vol. 97(1-2), pp. 273-324. 
16. Liu, H., and Motoda, H., 1998, Feature Selection for Knowledge Discovery and Data 
Mining, Boston: Kluwer Academic. 
17. Liu, H., and Setiono, R., 1996, “A Probabilistic Approach to Feature Selection - A Filter 
Solution,” Proceedings of 13th International Conference on Machine Learning, pp. 
319-327. 
18. Liu, H., and Yu, L., 2005, “Toward Integrating Feature Selection Algorithms for 
Classification and Clustering,” IEEE Trans. Knowl. Data Eng. Vol. 17(4), pp. 491-502. 
 10
