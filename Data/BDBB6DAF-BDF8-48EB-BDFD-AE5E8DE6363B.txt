to access an underlying knowledge base.  However, people are 
interested in not only factual questions, but also opinions.  This 
report also deals with question analysis and answer passage retrieval 
in opinion QA systems, as an example application of opinion mining 
techniques.  For question analysis, six opinion question types are 
defined.  A two-layered framework utilizing two question type 
classifiers is proposed.  Algorithms for these two classifiers are 
described.  The performance achieves 87.8% in general question 
classification and 92.5% in opinion question classification.  The 
question focus is detected to form a query for the information 
retrieval system and the question polarity is detected to retain 
relevant sentences which have the same polarity as the question.  For 
answer passage retrieval, three components are introduced.  Relevant 
sentences retrieved are further identified as to whether the focus 
(Focus Detection) is in a scope of opinion (Opinion Scope 
Identification) or not, and, if yes, whether the polarity of the scope 
and the polarity of the question (Polarity Detection) match with each 
other.  The best model achieves an f-measure of 40.59% by adopting 
partial match for relevance detection at the level of meaningful unit.  
With relevance issues removed, the f-measure of the best model 
boosts up to 84.96%. 
 
1 Introduction 
Sentiment analysis has attracted much attention in recent years because a large 
amount of subjective information is disseminated through various platforms on the 
web such as review sites, forums, discussion groups, blogs, and news.  Web users are 
willing to share their thoughts or feelings with others after reading books, watching 
movies, buying products, and so on.  Consulting specific information sources and 
summarizing the newly-discovered opinions aids governments in improving services, 
companies in marketing products, and customers in purchasing items. 
In the past, extracted opinions have only focused on individual targets, with effects 
on multiple targets being left for users to interpret.  Few works touch on automatic 
comparison of opinions about multiple targets.  Opinions usually accompany specific 
events; this phenomenon makes event burst detection indispensable.  Given the 
detected events, opinion mining can further identify the relationship between opinion 
polarity and the correlated events:  two entities may be related if different events 
always result in similar opinions. 
We employ the results of opinion mining to relationship discovery, and compare 
the results with those of the traditional collocation model, which discovers 
relationships among terms based on their co-occurrences in physical contexts such as 
documents, sentences, and adjacent words.  The basic idea is that if entities involved 
in the same sequence of events yield similar opinion trends, they may be correlated.  
To minimize chance co-occurrences, two entities should be observed over a sufficient 
amount of time. 
We hence propose the opinion analysis system CopeOpi.  This system extracts 
opinions, providing the information necessary for relationship discovery.  To 
demonstrate its feasibility, listed companies are considered as the experimental targets 
for relationship discovery.  We propose several models and use the best model in 
CopeOpi.  For opinion-based models, we use original curves, digitized curves, and 
smoothened curves of tracking plots to examine the opinions’ effects on relationship 
discovery.  For collocation-based models, we extract collocations at the word, 
(2005) use machine learning techniques for classification, while Takamura et al. 
(2005) and Ghose et al. (2007) adopt regression models.  Riloff and Wiebe (2003), 
and Choi et al. (2005) employ linguistic or structural information as auxiliary cues.  
In addition to classification, opinion holders can be extracted (Choi et al., 2005; Kim 
and Hovy, 2005; Breck et al., 2007), and their opinions can be summarized (Hu and 
Liu, 2004b; Ku et al., 2005; Seki et al., 2005; Stoyanov and Cardie, 2006) and even 
tracked (Ku et al., 2006).  Opinions can be mined from texts of different genres, such 
as product reviews (Dave et al., 2003; Morinaga et al., 2002; Hu and Liu, 2004a; Hu 
and Liu, 2004b; Bai et al., 2005; Liu et al., 2007; Pang and Lee, 2005), news 
documents (Ku et al., 2006), blog articles (Ku et al., 2006; Mei et al., 2007; Yang, 
2007), and so on.  Many interesting applications (Hu and Liu, 2004b; Liu et al., 2005; 
Ku et al., 2007) have been developed. 
Among the opinion analysis systems that have been developed for alphabetic 
languages, OASYS (Cesarano et al., 2007) is the most famous.  Both OASYS and 
CopeOpi allow users input their queries and select preferred data sources, and then 
track opinions in a time zone.  However, OASYS analyzes opinions from each data 
source separately, while CopeOpi collects articles from all data sources and generates 
a summary.  OASYS lists related documents only by publication time, but CopeOpi 
ranks documents according to their significance by finding important events (Ku et al., 
2005).  For both systems, extracting opinions is the main focus, while holders and 
targets are identified implicitly when retrieving relevant documents. 
Relationship discovery is a hot research topic in the social network domain.  Lin 
and Chen (2008) mine relationships from features of entities and collocated terms 
from documents.  Mori et al. (2007) further mine predefined relationships.  We mine 
relationships from an opinion view, that is, from comments and evaluations people 
offer about these entities; this is very different from other works.  We have selected 
companies as example entities for mining relationships in CopeOpi.  From companies, 
the relationships mined by CopeOpi are financial, but are not specific template 
elements like employer, employee, or ownership, as defined in MUC (Chinchor, 
1998).  The company relationships are targeted more toward overall evaluations, such 
as stock prices.  We feel these relationships are useful, as CopeOpi manages to align 
companies whose financial performances are correlated.  Because CopeOpi does not 
apply any domain-specific methods to relationship discovery, it is able to mine 
relationships among any targets given relevant comments. 
It is also important to have an effective user interface for an opinion analysis 
system.  CopeOpi combines graphical and text media to present complex information 
like opinions.  It displays clear opinion polarities first using a tricolor opinion 
tracking line along the timeline, and then a tricolor visualization of texts, in which the 
text colors indicate the individual opinion polarity of each sentence or document.  
OASYS also uses multi-color tracking lines, but to distinguish different data sources.  
Carenini’s team proposed a graphical user interface for evaluative texts (2006), in 
which color blocks are used to present the evaluations for components of products.  
However, their blocks contain no time information for tracking and are more suitable 
for processing only reviews of specific products due to predefined features. 
Regarding QA systems, most of the research on them has been developed for 
factual questions, and the association of subjective information with question 
answering has not yet been studied much.  Despite the opinion analysis related 
research mentioned, some research has gone from opinion analysis in texts toward 
that in QA systems.  Cardie et al. (2003) took advantage of opinion summarization to 
support Multi-Perspective Question Answering (MPQA) system which aims to extract 
opinion-oriented information of a question.  Yu and Hatzivassiloglou (2003) 
we here replace green with bold gray and red with bold black, and there is no neutral 
date in this example.  Figure 2 below shows the tracking plot for the “typhoon” query 
submitted in Figure 1.   
 
FIG. 2. Typhoon tracking plot. 
 
The first and the last dates of the assigned time period are shown in the left and the 
right of the tracking plot.  The downward bar indicates a date with strong negative 
opinions, while the upward bar denotes a positive one. The length of each bar 
indicates the combined strength of opinion for that day, with longer bars signifying 
stronger opinions.  A positive date is a date in which the majority of opinions is 
positive, and vice versa.  When users move the cursor to a given bar, the summary for 
that day is displayed, as shown in the right window of Figure 2.  The date with the 
longest negative bar is October 26th, 2004, where a total of 53 documents were found 
to contain the query term “typhoon” on that day and the total opinion score is -124.97.   
Moreover, the number of bursts denotes the appearances of events or monitored foci.  
Figure 2 reflects the three major typhoons that hit Taiwan in October 2004, including 
No. 0418 typhoon “Aere”, No. 0423 typhoon “Tokage”, and No. 0424 typhoon 
“Nock-Ten”.  The second typhoon lasted longer, while the third one caused greater 
damage.  The later, smaller bursts are their consequences.  Since the focus is 
“typhoon”, most of the dates are negative.  However, even though a day is “negative” 
in the tracking plot, not all of the related documents need be negative.  Users can click 
“Details” to view the headlines and opinion polarities of these documents.  Figure 3 
shows the Chinese-English headlines of related documents on October 26th, 2004. 
 
3.2 Relationship Discovery 
In terms of relationship discovery, CopeOpi finds entities related to a given target.  
Users input the named entities that they are interested in, specify the maximum 
number of entities to be returned, and choose the number of retrieved documents to be 
examined, e.g., top 2000, 5000, 8000, 10000 or all. For instance, given the TOP2000 
setting, the top 2000 documents among the retrieved documents are mined for further 
opinions.  Figure 5(a) shows an example, where the analysis target is set to the 
company “力晶” (Powerchip Semiconductor Corp., or PSC), the number of the 
reported entities is set to 2, and the number of retrieved documents is set to the top 
8,000 documents. 
 
  
(a) 
 
(b) 
FIG. 5. An example of CopeOpi’s relationship discovery. 
 
After discovering relationships, CopeOpi lists entities possessing strong 
relationships with the target, together with a measure of the strength of their 
relationship.  The length of the horizontal line represents the strength of the 
relationship.  The details of this operation will be described in following sections.  
Figure 5(b) shows two most related companies as discovered by CopeOpi, “茂德” 
(ProMOS Technologies) and “南科” (Nanya Technology Corp.).  Two horizontal 
lines show that ProMOS has a stronger relationship with PSC than Nanya does.  
Indeed, these three companies are all DRAM manufacturers, and PSC and ProMOS 
are the two biggest DRAM companies in Taiwan.   
 
 
4 Implementation of the CopeOpi system 
Information retrieval, opinion extraction, opinion summarization and opinion 
tracking are the four major modules for opinion mining.  The retrieved documents 
must be relevant to a specific target; otherwise the mined opinions would lack focus.  
Then, the seed vocabulary is enlarged by consulting two thesauri, including 
tong2yi4ci2ci2lin2 (abbreviated as Cilin) (Mei et al., 1982) and the Academia Sinica 
Bilingual Ontological Wordnet3  (abbreviated as BOW).  Cilin is composed of 12 
large categories, 94 middle categories, 1,428 small categories, and 3,925 word 
clusters.  BOW is a Chinese thesaurus with a similar structure as WordNet4.  However, 
words in the same clusters may not always have the same opinion tendency.  For 
example, 「寬恕」 (forgive: positive) and 「姑息」 (appease: negative) are in the 
same synonym set (synset).  Nevertheless, they have opposite opinion tendencies.  
How to distinguish words with different polarities within the same cluster/synset is 
the major issue of using thesauri to expand the seed vocabulary and is addressed 
below.  
It is postulated that the meaning of a Chinese sentiment word is a function of the 
composite Chinese characters.  This is exactly how people read ideogram when they 
come to a new word.  A sentiment score is then defined for a Chinese word by the 
following formulas.  The equations not only tell us the opinion tendency of an 
unknown word, but also suggest its strength.  Moreover, using these equations, 
synonyms of different polarities are distinguishable by their scores while applying 
thesaurus expansion.  The discussion begins with the definition of the formula of 
Chinese characters. 
ii
i
i
cc
c
c fnfp
fp
P +=  (1) 
  
ii
i
i
cc
c
c fnfp
fn
N +=  (2) 
where fpci and fnci denote the frequencies of a character ci in the positive and 
negative words, respectively. 
Formulas (1) and (2) utilize the percentage of a character in positive/negative 
words to show its sentiment tendency.  However, there are more negative words than 
positive ones in the “seed vocabulary”.  Hence, the frequency of a character in a 
positive word may tend to be smaller than that in a negative word.  That is unfair for 
learning, so that Formulas (1) and (2) are normalized into Formulas (3) and (4), 
respectively. 
∑∑
∑
==
=
+
= m
j
cc
n
j
cc
n
j
cc
c
jiji
ji
i
fnfnfpfp
fpfp
P
11
1
//
/
 
(3) 
  
∑∑
∑
==
=
+
= m
j
cc
n
j
cc
m
j
cc
c
jiji
ji
i
fnfnfpfp
fnfn
N
11
1
//
/
 
(4) 
 
Where Pci and Nci denote the weights of ci as positive and negative characters, 
respectively; n and m denote total number of unique characters in positive and 
negative words, respectively.  The difference of Pci and Nci, i.e., Pci - Nci in Formula 
(5), determines the sentiment tendency of character ci.  If it is a positive value, then 
this character appears in positive Chinese words more often than in negative ones; 
and vice versa.  A value close to 0 means that it is not a sentiment character or it is a 
neutral sentiment character.   
)(
iii ccc
NPS −=
 
(5) 
                                                 
3 http://bow.sinica.edu.tw/ 
4 http://wordnet.princeton.edu/ 
Algorithm: Opinion Document Extraction 
1.   For every document d 
2.    Decide the opinion tendency of d by the function of the opinion tendencies of 
sentences insides d as follows.  
∑
=
=
m
j
pd SS
1  (8) 
Where Sd and Sp are sentiment scores of document d and sentence p, and m is the  
amount of evidence.  If the topic is anti type, reverse the sentiment type. 
 
 
 Such a bottom-up methodology is suitable for mining information of different 
granularities. The opinions extracted at different levels are used in opinion 
summarization and tracking.  However, merely summarizing opinions from 
documents does not give us sufficient information: we must further identify the events 
that engendered these opinions.  Therefore, we track the opinions of one target first to 
generate the tracking plot in which we attempt to find temporal hints for the latent 
events.   
Opinion tracking tells how people change their opinions over time.  Tracking the 
opinions of a single target is fundamental for analyzing the variation of the target’s 
reputation.  Calculating the overall opinion scores of relevant documents for a 
specific target every day and display them by their temporal order generates a 
tracking plot.  Whether a day is defined positive or negative for a target is determined 
by the opinion tendency of that target for that day.  Here positive and negative days 
are tracked separately to detect positive and negative events.  Figure 6 shows an 
example of the tracking plot for Taiwan Semiconductor Manufacturing Company 
(TSMC).  The black curve in the top figure illustrates the opinion scores on positive 
days. 
 
 
0
10
20
30
40
50
60
20
03
/0
8/
13
20
03
/0
8/
15
20
03
/0
8/
17
20
03
/0
8/
19
20
03
/0
8/
21
20
03
/0
8/
23
20
03
/0
8/
25
20
03
/0
8/
27
20
03
/0
8/
29
20
03
/0
8/
31
20
03
/0
9/
02
20
03
/0
9/
04
20
03
/0
9/
06
20
03
/0
9/
08
20
03
/0
9/
10
20
03
/0
9/
12
20
03
/0
9/
14
20
03
/0
9/
16
20
03
/0
9/
18
20
03
/0
9/
20
20
03
/0
9/
22
20
03
/0
9/
24
Opinion score
Burst state
A B C D E F G H I J
 
FIG. 6.  Bursts detected from the TSMC positive opinion tracking plot. 
Opinion Tracking Plot of TSMC
0
10
20
30
40
50
60
70
80
90
20
03
/08
/13
20
03
/08
/18
20
03
/08
/23
20
03
/08
/28
20
03
/09
/02
20
03
/09
/07
20
03
/09
/12
20
03
/09
/17
20
03
/09
/22
20
03
/09
/27
20
03
/10
/02
20
03
/10
/07
20
03
/10
/12
20
03
/10
/17
20
03
/10
/22
20
03
/10
/27
20
03
/11
/01
20
03
/11
/06
20
03
/11
/11
20
03
/11
/16
Opinion score
 
causes result in similar consequences for targets of a certain relationship.  For two 
related targets (e.g., they are the same business, or they behave similarly in the stock 
market) the occurrence of an event will result in similar behavior, as represented in 
their opinion tracking plots.  This effect is illustrated in Figure 7. 
 
(a) TSMC 
2003/8/7 2003/8/27 2003/9/16 2003/10/6 2003/10/26 2003/11/15 2003/12/5
 
(b) UMC 
2003/8/7 2003/8/27 2003/9/16 2003/10/6 2003/10/26 2003/11/15 2003/12/5
 
(c) Lucky Cement 
2003/8/7 2003/8/27 2003/9/16 2003/10/6 2003/10/26 2003/11/15 2003/12/5
 
(d) TSMC and UMC 
2003/8/7 2003/8/27 2003/9/16 2003/10/6 2003/10/26 2003/11/15 2003/12/5
TSMC
UMC  
(e) TSMC and Lucky Cement 
-300
0
300
2003/8/7 2003/8/17 2003/8/27 2003/9/6 2003/9/16 2003/9/26 2003/10/6 2003/10/16
TSMC
Lucky Cement
 
FIG. 7. Opinion Tracking for TSMC, UMC and Lucky Cement. 
 
The five curves in Figure 7 are tracking plots of the companies TSMC (Taiwan 
Semiconductor Manufacturing Company), UMC (United Microelectronics 
Corporation), Lucky Cement, a comparison of TSMC and UMC, and a comparison of 
TSMC and Lucky Cement, respectively.  The TSMC and UMC plots (d) are more 
similar than those of TSMC and Lucky Cement (e).  In fact, both TSMC and UMC 
are semiconductor companies, while Lucky Cement sells cement.  We therefore 
assume that if the opinion tracking plots of two companies are alike, they are more 
closely related than those with dissimilar plots.  In the next section we will discuss 
Curve Overlap Model: 
( ) ( )( )
n
SR
SR
SR
BACO
n
i ii
ii
ii∑
= ⎟
⎟
⎠
⎞
⎜⎜⎝
⎛ ⋅⋅
= 1 ,max
,min
sgn
),( , 
(11)
where Ri and Si are the opinion scores of targets A and B on a specific day i, 
respectively, and n is the number of days in the tracking period. 
 
Digitalized Curve Overlap Model: 
( ) ( ) ( )( )( ) ( )( )
n
SR
SR
SR
BADCO
n
i ii
ii
ii∑
= ⎟
⎟
⎠
⎞
⎜⎜⎝
⎛ ⋅⋅
= 1 sgn,sgnmax
sgn,sgnmin
sgn
),(
(12)
Only the sign of the opinion score is used to calculate the curve overlap in DCO.  
That is, only the opinion polarities are considered: the opinion degrees are not taken 
into consideration in this model. 
 
Curve Overlap with Burst Detection Model: 
First, is defined as the burst detection state on day i according to the 
tracking plot for target X.  Since positive opinions and negative opinions are 
processed separately in burst detection, variable t identifies the tendency of the 
analyzed plot.  When t is 1, function BD returns states from the positive tracking plot; 
when t is -1, function BD returns states from the negative tracking plot.  Returned 
states are used to calculate the curve overlap in BDCO. 
),,( itXBD
 
},{
),1,()),1,(),,1,(max(
),1,()),1,(),,1,(max(
1
1
)),1,(),,1,(max(
)),1,(),,1,(max(
BAX
iXBDiXBDiXBD
iXBDiXBDiXBD
fork
iBBDiBBDkS
iABDiABDkR
BDi
BDi
∈
⎩⎨
⎧
−=−
=−
−=
−⋅=
−⋅=
(13)
( ) ( )( )
n
SR
SR
SR
BABDCO
n
i BDiBDi
BDiBDi
BDiBDi∑
= ⎟
⎟
⎠
⎞
⎜⎜⎝
⎛ ⋅⋅
= 1 ,max
,min
sgn
),(
(14)
 
The plot of burst detection states is a smoothed tracking plot curve (see the black 
curve in the bottom figure in Figure 6).  In this model, relationships are discovered 
from the burst plot as opposed to the tracking plot. 
Chi-square Model: 
We adopt chi-square formula in the fourth opinion-based algorithm.  The chi-
square formula is defined in Formula (15), where is the exact observed value of 
frequency , and is the expected value of : 
o
ijf
ijf
e
ijf ijf
∑∑ −=
i j
e
ij
e
ij
o
ij
f
ff 22 )(χ . (15)
Daily opinion scores are extracted from the results of opinion tracking, and the 
opinion score signs are used to analyze target relationships.  An opinion score of 0 
means there are no documents retrieved on that day.  Therefore, as shown in Table 3, 
we use a chi-square contingency table with one degree of freedom.  For example, in 
 
Topic ID Topic Title # of Docs
CIRB010-OP 
ZH021 Civil ID Card 37 
ZH024 The Abolishment of Joint College Entrance Examination 55
ZH026 The Chinese-English Phonetic Transcription System 30
ZH027 Anti-Meinung Dam Construction 14
ZH028 Logging of Chinese Junipers in Chilan 23
ZH036 Surrogate Mother 33
Total 192
CIRB020/040-OP 
001 Time Warner, American Online (AOL), Merger, Impact 13
002 President of Peru, Alberto Fujimori, scandal, bribe 28
003 Kim Dae Jun, Kim Jong Il, Inter-Korea Summit 13
004 US Secretary of Defense, William Sebastian Cohen, Beijing 13
005 G8 Okinawa Summit 38
006 Wen Ho Lee Case, classified information, national security 41
007 Ichiro, Rookie of the Year, Major League 14
008 Jennifer Capriati, tennis 25
009 EP-3 surveillance aircraft, F-8 fighter, aircraft collision 95
010 History Textbook Controversies, World War II 13
011 Tobacco business, accusation, compensation 19
012 Tiger Woods, sports star 11
013 “Qiudou” (Autumn Struggle), Appeal, Laborer, Protest, Taiwan 28
014 Expert, Opinion, International Monetary Fund (IMF), Asian countries 21
015 Find articles dealing with a teenage social problem 19
016 Divorce, Family Discord, Criticisms 24
017 China, Reaction, Taiwan, Diplomatic Relations 14
018 China, Stationing, Weapons, Taiwan 17
019 Animal Cloning Technique 21
020 Sexual Harassment, Lawsuits 55
021 Olympic, Bribe, Suspicion 21
022 North Korea, Daepodong, Asia, Response 127
023 Joining WTO 89
024 China Airlines Crash 13
025 Province-refining 14
026 Economic influence of the European monetary union 32
027 President Kim Dae-Jung’s policy toward Asia 12
028 Clinton scandals 71
029 War crimes lawsuits 23
030 Nuclear power protests 13
031 College Admission Policy 28
032 Youth Counseling 13
Total 843
Table 4. Topics and number of documents in NTCIR corpora for opinion 
mining. 
 
Tag 
Level Attribute Value Description 
<DOC_ATTITUDE></DOC_ATTITUDE> 
Document TYPE 
POS 
NEG 
NEU 
Document Attitude: Define the opinion polarity of the 
whole document 
<SEN_ATTITUDE></SEN_ATTITUDE> 
Sentence TYPE 
SUP 
NSP 
NEU 
Sentence Attitude: Define the opinion polarity of one 
sentence 
<OPINION_SEG></OPINION_SEG> 
Sub- 
sentence TYPE PSV Opinion Segment: Define the scope of one opinion 
<OPINION_SRC></OPINION_SRC> 
Sub- 
sentence TYPE 
EXP 
IMP 
Opinion Source: Define the opinion holder of a specific 
opinion 
<SENTIMENT_KW></ SENTIMENT_KW > 
Word TYPE 
POS 
NEG 
NEU 
Sentiment Keyword: Define the opinion polarity of a 
single word 
<OPINION_OPR></OPINION_OPR> 
Word TYPE PSV Opinion Operator: Define the keyword of expressing an opinion 
Table 5. Tag descriptions 
 
Every element has an opening and a closing tag as the XML language.  For 
example, the pair of <DOC_ATTITUDE> and </DOC_ATTITUDE> denote 
document attitude.  The tag <OPINION_SEG> is especially useful in dealing with 
multi-perspective or opinion holder related issues.  Consider the following example: 
A says that B insists event C and D disproves event C. 
It is tagged as:  
FIG. 9.  A sample of nested tags  
 
Nested relations of opinion holders are critical to identify the belonging of opinions, 
that is, multi-perspective issues.  XML-like tags can easily represent nested relations.  
A Chinese and an English tagging examples are illustrated in the following figures.   
evaluated at first.  Because it is not cost effective to examine all the sentiments tagged 
by all annotators, only the words that are of parts-of-speech noun, verb, adjective and 
adverb, and co-occur with one of the seeds (defined in Section 4.1) are sampled for 
agreement test.  A total of 838 words were selected.  The metrics of the inter-
annotator agreement is shown in Formula 9. 
samples
BABAAgreement ∩=),(   (16) 
Three annotators denoted A, B and C examined the samples.  Tables 6 and 7 show 
the agreement of the three annotators under strict and lenient metrics.  Under lenient 
metrics, neutral sentiment words and positive sentiment words are considered to be in 
the same category.  Strict metrics treats all three categories (positive, neutral, and 
negative) as distinct.  The average agreements between two annotators under the two 
metrics are 68.62% and 69.69%, and the agreements among three annotators are 
54.06% and 55.13%, respectively. 
 
Annotators A vs. B B vs. C C vs. A Ave 
Percentages 78.64% 60.74% 66.47% 68.62% 
All agree 54.06% 
Table 6. Agreement of annotators under strict metrics 
 
Annotators A vs. B B vs. C C vs. A Ave 
Percentages 79.47% 62.05% 67.54% 69.69% 
All agree 55.13% 
Table 7. Agreement of annotators under lenient metrics  
 
The largest category of samples is “non-sentiment” (400 out of 838 words, i.e., 
47.73%).  The agreements of two annotators (68.62% and 69.69%) are much larger 
than the baseline percentage (47.73%).  In addition, the agreements of all annotators 
(54.60% and 55.13%) are still significantly higher.  Here, we do not define a strength 
tag as what Wiebe et al. did in English (2002), since the agreement decreases to an 
unacceptable degree when more annotators and more categories are involved. 
The annotations are called strongly inconsistent if positive polarity and negative 
polarity are assigned to the same word by different annotators.  In a total of 385 
inconsistent answers, only 16 words are strongly inconsistent (4.16%).  In contrast, 
annotations are highly inconsistent in weak opinion words of the form 
positive/negative vs. neutral (30) and sentiment vs. non-sentiment (339).  It also 
shows that deciding the opinion degree of a word is challenging for human annotators. 
The agreements at sentence level and document level are shown in Table 8 and 
Table 9, respectively.  The results are quite similar to that at word level.  Furthermore, 
agreements of annotations from news and blog articles are listed in Table 10 for 
comparison.  Table 10 shows that tagging news articles achieves lower agreement 
rates than tagging web blogs.  We observe that blog articles use more daily 
vocabulary and are easier to be understood by human annotators than news articles. 
 
Because sentences are the most reasonable units for expressing opinions, and because 
extracting opinion sentences has been shown to be the most challenging among all 
granularities (Ku and Chen, 2007), we focus on sentence-level opinion mining 
evaluation in this research.  That is, we annotated sentences in both CIRB010-OP and 
CIRB020/040-OP, and the latter were adopted in the NTCIR-6 pilot task.  There are 
two metrics for evaluation: under the strict metric, only sentences for which the three 
annotations are all consistent are counted. Under the lenient metric, only sentences 
with two or three consistent annotations are counted; the majority annotation is 
treated as the gold answer. 
 
 
5.3 Experiments and Results 
5.3.1 Ideal Performance of Opinion Extraction 
Before evaluating automatic opinion extraction algorithms, we have to know the 
ideal performance of human beings in the same task.  Tables 12-14 list the evaluation 
of the three annotators with respect to the golden standard.  Apparently, none of the 
annotators can assign 100% same answers with the golden standard, that is, the 
majority.  These results reveal an interesting observation.  In the opinion extraction 
task, one annotator cannot tell the opinion of the whole.  The statistics tell us that the 
agreement between one annotator’s opinions and the majority on average is around 
80%.  The other 20% is inconsistent because of annotators’ own perspective.  Such 
phenomenon makes the opinion extraction different from other research topics in 
information retrieval and extraction. 
 
Annotators A B C Average 
Recall 94.29% 96.58% 52.28% 81.05% 
Precision 80.51% 88.87% 73.17% 80.85% 
f-measure 86.86% 92.56% 60.99% 80.14% 
Table 12. Annotators’ performance referring to golden standard at word level 
 
Annotators A B C Average 
Recall 94.44% 38.89% 90.97% 74.77% 
Precision 71.20% 74.67% 50.19% 65.35% 
f-measure 81.19% 51.14% 64.69% 65.67% 
Table 13. Annotators’ performance referring to golden standard at sentence level 
 
 Testing 
C5 
A B 
A 83.60% 36.50% 
Training 
B 0% 41.50% 
Table 17. Performance of decision tree (Precision) 
 
As Tables 15-17 show, the proposed sentiment word mining algorithm achieves the 
best average precision 61.06% of Verb and Noun, while SVM achieves 46.81% 
(outside test, the average of 45.23% and 48.39%) and C5 does even worse (precision 
0% because of a small training set).  Our algorithm outperforms SVM and the 
decision tree methods in sentiment word mining.  The experiments indicate that the 
semantics within a word is not enough for a machine learning classifier.  In other 
words, machine learning methods are not suitable for word level opinion extraction.  
In the past, Pang, Lee and Vaithyanathan (2002) showed that machine learning 
methods are not good enough for opinion extraction at document level.  Thus we may 
conclude that opinion extraction is beyond a classification problem.  Compared to 
Table 12, our sentiment word miner achieves 85.44% of the performance of human 
annotators (i.e., 0.6847 in Table 15/0.8014 in Table 12). 
 
Source NTCIR BLOG 
Precision 34.07% 11.41% 
Recall 68.13% 56.60% 
f-measure 45.42% 18.99% 
Table 18. Opinion extraction at sentence level 
 
Source NTCIR BLOG 
Precision 40.00% 27.78% 
Recall 54.55% 55.56% 
f-measure 46.16% 37.04% 
Table 19. Opinion extraction at document level 
 
Table 18 and Table 19 further show the results of opinion extraction at sentence 
and document levels.  Results, including news and blog articles, tell that the precision 
rates are low at both sentence and document levels.  This is because the algorithm so 
far only considers opinions but not relevance.  Many sentences, which are non-
relevant to the topic “animal cloning”, are included for opinion judgment.  The non-
relevant rate is 50% and 53% for news articles and Web blog articles, respectively. 
The quantity of seeds also influences the performance of opinion extraction.  Figure 
12 shows their relationship.  The more seeds are used, the better performance is 
achieved.  However, Figure 12 further indicates that the improvement of performance 
converges when the quantity of seeds reaches around 8,000.  Recall that 10,542 
qualified seeds are used in this research.  Thus, we can neglect the issue of the lack of 
seeds.  
 CIRB020/040-OP-A CIRB020/040-OP-B 
Precision 0.335 0.664 
Recall 0.448 0.890 
f-measure 0.383 0.761 
Table 21. Opinion extraction performance for CIRB020/040-OP. 
 
Tables 22 and 23 show the performance of all of the systems in the NTCIR-6 pilot 
task (Seki et al., 2007).  All employ the CIRB020/040-OP corpus under the lenient 
metric in their evaluations.  For opinion sentence extraction, our system is in the high 
performance group.  For opinion and relevant sentence extraction, we ranked second. 
 
Opinion Group P R F 
UMCP-1 0.645 0.974 0.776 
UMCP-2 0.630 0.984 0.768 
Gate-1 0.643 0.933 0.762 
NTU 0.664 0.890 0.761 
Gate-2 0.746 0.591 0.659 
CHUK 0.818 0.519 0.635 
ISCAS 0.590 0.664 0.625 
Table 22.  Opinion sentence extraction in the NTCIR-6 pilot task. 
 
Opinion and Polarity Group P R F 
CHUK 0.522 0.331 0.405 
NTU 0.335 0.448 0.383 
UMCP-1 0.292 0.441 0.351 
UMCP-2 0.286 0.446 0.348 
ISCAS 0.232 0.261 0.246 
Gate-1 --- --- --- 
Gate-2 --- --- --- 
Table 23.  Opinion and relevant sentence extraction in the NTCIR-6 pilot task. 
 
6 Experimental Setup and Evaluation for Relationship Discovery 
We collected 1,282,050 economics-related documents as the knowledge base for 
relationship discovery.   We used 1,078 listed companies in Taiwan as the target 
candidates for discovering relationships, as well as the stock indices of these 
companies during the time period represented in the knowledge base to verify the 
correctness of the discovered relations. 
 
 
6.1 Experiment Materials 
For model training, a total of 1,282,050 economics-related documents were 
 
Stock Price of 
Company A Number of days 
↑ ↓ 
↑ f11 f12 Stock Price of 
Company B ↓ f21 f22 
Table 24.  Chi-square contingency table with one degree of freedom. 
 
Stock Price of Company ANumber of days 
↑ - ↓ 
↑ f11 f12 f13 
- f21 f22 f23 
Stock Price 
of Company 
B ↓ f31 f32 f33 
Table 25.  Chi-square contingency table with four degrees of freedom. 
 
With different degrees of freedom and significance levels, we extracted the 
correlated company pairs as the gold standard.  Table 26 shows the number of pairs in 
the gold standard for different conditions. 
 
Degree of 
freedom 
2
950.x
2
990.x
2
995.x   
1 7,815 4,239 2,008 
4 2,489 1,366 703 
Table 26.  Numbers of company pairs. 
 
In this case, we felt that specific company pairs with strong relationships were 
more informative than a large number of company pairs with weak relationships, that 
is, that precision was more important than recall.  Therefore, we used the strictest 
condition to generate the gold standard, and selected a total of 703 pairs as the gold 
standard under with four degrees of freedom. 2995.x
 
 
6.3 Experiments and Results 
Since the performance of companies is reflected in their stock prices, and because 
stock prices are public information, we chose companies as the entity type when 
evaluating methods for relationship discovery. 
6.3.1 Relationship Discovery Performance 
The relevance issue is also important in relationship discovery evaluation.  Only by 
working on relevant documents can we gauge the real performance of both 
collocation-based and opinion-based models.  Otherwise we cannot decide whether 
these models perform well, because extracting relationships from documents 
irrelevant to the target may result in too much noise.  However, retrieved documents 
are only those documents that mention the targets (companies).  Notably, many 
 
t-test Document  Level 
Sentence  
Level 
Word  
Level 
N P R F P R F P R F 
25 0.44  0.02 0.03 0.48 0.02 0.03 0.60†  0.02 0.04 
50 0.26  0.02 0.04 0.46 0.03 0.06 0.46†  0.03 0.06 
100 0.24  0.03 0.06 0.44 0.06 0.11 0.41†  0.06 0.10 
200 0.21  0.06 0.09 0.34 0.10 0.15 0.37† 0.08 0.13 
500 0.15  0.11 0.12 0.25 0.18 0.21 0.34*** 0.10 0.15 
Table 28.  The t-test model in relationship discovery  
including companies with general names. 
 
MI Document Level 
Sentence  
Level 
Word  
Level 
N P R F P R F P R F
25 0.28 0.01 0.02 0.96*** 0.04 0.08 0.52 0.02 0.04 
50 0.28 0.02 0.04 0.96*** 0.08 0.15 0.56 0.05 0.09 
100 0.25 0.04 0.07 0.75*** 0.12 0.21 0.49 0.08 0.14 
200 0.24 0.08 0.12 0.54‡  0.18 0.27 0.44 0.14 0.22 
500 0.18 0.15 0.17 0.32†  0.26 0.29 0.32 0.26 0.29 
Table 29.   The MI model in relationship discovery  
excluding companies with general names. 
 
t-test Document  Level 
Sentence  
Level 
Word  
Level 
N P R F P R F P R F 
25 0.44  0.02 0.04 0.48 0.02 0.04 0.64†  0.03 0.05  
50 0.26  0.02 0.04 0.44 0.04 0.07 0.54†  0.05 0.08  
100 0.26  0.04 0.07 0.43 0.07 0.12 0.47†  0.08 0.13  
200 0.23  0.08 0.11 0.34 0.11 0.17 0.41†  0.10 0.16  
500 0.16  0.13 0.14 0.26 0.21 0.23 0.42*** 0.14 0.21  
Table 30.   The t-test model in relationship discovery  
excluding companies with general names. 
 
Top 2000 Top 5000 Top 8000 Top 10000 ALL  
P R F P R F P R F P R F P R F 
CO 0.472 0.078 0.117 0.552 0.092 0.137 0.603 0.102 0.152 0.606 0.104 0.155 0.510 0.091 0.135
DCO 0.223 0.034 0.055 0.334 0.051 0.082 0.395 0.055 0.090 0.363 0.053 0.086 0.304 0.049 0.079
BDCO 0.090 0.012 0.020 0.049 0.006 0.009 0.097 0.014 0.023 0.092 0.014 0.023 0.107 0.018 0.029
χ2 0.550 0.069 0.114 0.584 0.072 0.118 0.621 0.078 0.128 0.625 0.079 0.130 0.600 0.077 0.126
Table 31.  Opinion-based models using different numbers of retrieved 
documents. 
their performance.  However, theχ2 precision rate drops rapidly when more answers 
are proposed.  Therefore, CO is the best of the opinion-based models. 
 
Top N MI∩CO MI-CO CO-MI
25 16 8 7 
50 27 21 16 
100 43 32 25 
200 67 40 29 
500 103 57 35 
Table 34.  Intersection and difference. 
 
Top N Average Rank of CO-MI in MI
Average Rank 
of MI-CO in CO
25   614.43   180.25 
50   806.75   439.29 
100 1305.12   722.03 
200 1487.86 1085.88 
500 2487.69 3663.12 
Table 35.  Average rank of CO-MI and MI-CO. 
 
We also examine the proposed answers of the collocation-based and opinion-based 
models.  The answers of the two best models, the collocation model MI and the 
opinion-based model CO, are selected as the targets for analysis.  Table 34 shows the 
intersection and difference of the two answer sets: only about half of the proposed 
answers of CO and MI intersect, and this quantity decreases when more answers are 
proposed.  This result shows that MI and CO propose different answers.  Table 35 
further shows the ranks of answers in the set differences.  The set difference CO-MI 
consists of answer pairs proposed by CO but excludes answer pairs in the top N of MI 
as well as pairs that were not proposed by the MI model.  Similarly, the set difference 
MI-CO consists of answer pairs proposed by MI but excludes answer pairs in the top 
N of CO as well as pairs that were not included by the CO model.  For example, if CO 
proposed the company pairs {1,2,7,5,3,8} and MI proposed the company pairs 
{1,2,4,6,8,3,9}, in the top 5 case, CO equals {1,2,7,5,3}, MI equals {1,2,4,6,8} and 
MI∩CO equals {1,2}.  CO-MI equals {3} because MI also finds company pairs {1,2}, 
and {7,5} are excluded by MI.  Likewise, MI-CO equals {8}.  All of the answers in 
CO-MI are checked to see which ranks they are in the answer set of MI; likewise for 
the MI-CO set.  If the ranks of the answers found by one model are low in the other 
model, it means that the other model does not propose the answers found by this 
model.  Table 19 shows that the MI ranks of the answers in CO-MI tend to be lower 
than the CO ranks in MI-CO.  In other words, the CO model finds company pairs 
which do not often co-occur and are thus not found by the MI model. 
To illustrate the idea that the CO and MI models complement each other, we 
further analyzed these company pairs and identified conditions in which related 
company pairs were found by the CO model but not by MI.  They are listed as follows: 
The company pair was never listed together in a single document in the news 
corpus, or when the two companies did co-occur in a document, it was with too many 
CO∩MI scans the MI and CO answers in a round-robin fashion to select common 
candidates.  Thus, CO+MI integrates two types of information using scores, while 
CO∩MI integrates based on ranks. 
Compared with the collocation-only and opinion-only models, both integration 
models perform better (results of two significance tests are shown in the P column).  
Table 20 shows the performance of CO+MI and CO∩MI.  CO+MI achieves precision 
rates of 1, 0.92 and 0.79, and CO∩MI achieves precision rate of 1, 0.92 and 0.76, 
respectively, when the top 25, 50 and 100 answers are proposed.  The overall 
performance of the eight models in Figure 8 shows that CO+MI is the best of all the 
models.  The fact that CO+MI slightly outperforms CO∩MI tells us indirectly that 
CO and MI are both good algorithms for selecting correct related pairs (precision-
based), as opposed to only collecting potential candidates (recall-based).  The 
symbols shown after the precision rates of CO+MI denote the significant levels when 
comparing CO+MI with CO and MI, respectively.  The CO+MI results improve 
significantly with the number of reported related pairs. 
 
 CO+MI CO∩MI 
N P R F P R F 
25 1.00†† 0.04 0.08 1.00 0.04 0.08
50 0.92†† 0.08 0.14 0.92 0.08 0.14
100 0.79†† 0.13 0.22 0.76 0.13 0.21
200 0.64***‡ 0.21 0.31 0.62 0.20 0.30
500 0.37***‡ 0.31 0.34 0.39 0.32 0.35
Table 36.  CO+MI and CO∩MI without general names. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 200 400 600 800 1000
Best N
Pr
ec
isi
on
MI
t-test
CO
DCO
BDCO
χ2
CO∩MI
CO+MI
 
MI 
FIG. 13.  Performances of all models. 
 
6.4 Mined Relationships 
We attempted to mine objects which led to opinions of the same polarities during a 
given time period, and extracted them with the assumption that they represented some 
kind of relationship.  People may be curious about these relationships: can the types 
have similar performance.  For example, not all banks are extracted in the gold 
answer set.  We may therefore say that the pre-defined relationships are different from 
the relationships extracted according to opinions on objects or according to their 
performance, and that we cannot find the pre-defined relationships from the 
performance-based ones easily, and vice versa. 
The characteristics of performance-based relationships are also distinct from those 
of pre-defined ones.  Pre-defined relationships give users information which may be 
acquired from other sources.  Performance-based relationships show users which 
objects behave similarly, i.e., perform similarly, thus yielding similar opinions.  As 
mentioned earlier, performance and opinions are highly related to events.  Therefore, 
relationships mined from performance and opinions provide information about which 
objects will also be influenced when events happen to another object. 
 
 
6.5 Opinion-based Prediction 
The last section deals with how to discover long-term relationships from opinions.  
Extracting correlated company pairs from tracking plots performs well in comparison 
to the gold standard extracted from the stock charts.  Here we further analyze if the 
opinion tracking plots for a company affect stock charts, that is, whether the stock 
price of the next day is affected by the opinions of the current day.  However, the 
experiment shows a precision of only 50.21%.   
Figure 14 illustrates the normalized tracking plot and the normalized stock chart of 
the company BenQ.  It shows that opinions have certain short-term relationships with 
the stock price.  However, the time to influence stock price varies.  The corresponding 
points in plots show that this time to influence varies from –1 to 4 days.  The time 
delay to events is because opinions are extracted from the articles published on the 
Web; the effect of events may persist for several days. 
Figure 15 shows the normalized stock chart of the Taiwan Stock Index (a large cap, 
large-volume indicator of the stock market performance) and Figure 16 shows a 
normalized randomized stock chart.  They are quite different from the tracking plot 
and the stock chart in Figure 14.  Together with the satisfactory performance of the 
curve-overlap related methods, we can infer that opinion tracking plots simulate stock 
charts.  However, opinions are only one of the factors for prediction.  Event duration 
should be considered, and therefore a more complicated model is necessary to predict 
the time to influence as well as the duration of influence. 
 
 
-0.2
-0.1
0
0.1
0.2
0.3
0.4
7
FIG. 14. Tracking plot vs. Stock chart (BenQ). 
2003/8/13 2003/8/18 2003/8/23 2003/8/28 2003/9/2 2003/9/7 2003/9/12 2003/9/1
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
2003 7/8/13 2003/8/18 2003/8/23 2003/8/28 2003/9/2 2003/9/7 2003/9/12 2003/9/1
1 4 1 0 2 1 0 2 1 1 0 -1 0 -1 
 
 
 
FIG. 17.  An opinion QA system framework. 
7.2 Experimental Corpus Preparation 
estions in OPQ, 304 are factual questions and the 
ot
ces.  These 3,732 questions, shown in Table 38, are used for our 
experiments. 
                                                
The experimental corpus comes from four sources, TREC1, NTCIR2, the Internet 
Polls, and OPQ.  TREC and NTCIR are two of the three major information retrieval 
evaluation forums in the world.  Their evaluation tracks are in natural language 
processing and information retrieval domains such as large-scale information retrieval, 
question answering, genomics, cross language processing, and so on.  We collected 
500 factual questions from the main task of QA Track in TREC-11.  Since the 
documents for answer extraction are in Chinese, the English questions were translated 
into Chinese manually for the experiments.  A total of 1,577 factual questions are 
obtained from the developing question set of the CLQA task in NTCIR-5.  Questions 
from public opinion polls in three public media websites – say, China Times, Era, and 
TVBS, are crawled.  OPQ is developed for this research, and it contains both factual 
and opinion questions.  To construct the question corpus OPQ, annotators are given 
titles and descriptions of six opinion topics selected from NTCIR-2 and NTCIR-3.  
Annotators freely ask any three factual questions and seven opinion questions for 
each topic.  Duplicated questions are dropped and a total of 1,011 questions are 
collected.  Within these 1,011 qu
her 707 are opinion questions.   
In total, we collected 2,443 factual questions and 1,289 opinion questions from four 
different sour
 
 
 
1 http://trec.nist.gov/ 
2 http://research.nii.ac.jp/ntcir/index-en.html 
based on the above seven challenges.  Table 39 lists the number of questions (#Q) 
with respect to the number of challenges (#C).   
 
T
 
#C 1 2 3 4 5 6 7 otal 
#Q 19 47 39 30 13 12 0 160
Table 39. Challenge of opinion questions. 
m construction, hewing down of 
Chinese junipers in Chilan, and surrogate mother. 
7.3 Two-layered Question Classification 
ions from 
er determines the types of opinion questions. 
Ty
ason, majority, and 
yes/no types concern the answer format of opinionated questions. 
(1) Holder (HD) 
 specific opinion is. 
Answer: Entities and the corresponding evidence. 
(2) Target (TG) 
Example does the public think should be responsible for the airplane 
Answer: Entities and the corresponding evidence. 
 
Due to the heavy manual effort of annotations, only a total of 60 questions, which 
include one to three challenges, are selected for further annotation.  Sentences are 
annotated as to whether they are opinions (Opinion), whether they are relevant to the 
NTCIR topic of the document in which they are (Rel2T), whether they are relevant to 
the question (Rel2Q), and whether they contain answers (AnswerQ).  If sentences are 
annotated as relevant to the question, annotators further annotate the text spans which 
contribute answers to the question (CorrectMU).  A total of 1,952 sentences are 
annotated.  These documents are relevant to six opinionated topics, including civil ID 
card, the abolishment of Joint College Entrance Examination, the Chinese-English 
phonetic transcription system, anti-Meinung Da
 
 
A two-layered classification, i.e., the first classifier Q-Classifier and the second 
classifier OPQ-Classifier, is proposed.  Q-Classifier separates opinion quest
factual ones, and OPQ-Classifi
pes of Opinion Questions 
As mentioned, the holder, the target and the opinion expressed are three important 
factors in an opinion expression.  Besides, opinion questions could be asked in the 
same way as factual questions.  Considering these factors and the answer format in 
both opinion questions themselves and their corresponding answers, we define six 
opinion question types as follows.  Among these types, holder, target, and attitude 
types are related to the opinionated factors of questions, while re
 
Definition: Asking who the expresser of the
Example: Who supports the civil ID card? 
 
Definition: Asking whom the holder’s attitude is toward.   
: Who 
crash? 
Section 3 mentions 2,443 factual questions and 1,289 opinion questions are 
collected from four different sources.  To keep the quantities of factual and opinion 
questions balanced, 1,289 factual questions are randomly selected from the 2,443 
questions.  Together with 1,289 opinion questions, a total of 2,578 questions are 
employed in the experiments of question classification.  We adopt See5 to generate 
th
ll but feature x” shows the error rate of using 
all features except the specified feature. 
 
 
e decision tree based on different combinations of features. 
With a 10-fold cross-validation, See5 outputs the resulting decision trees for each 
fold, and a summary with the mean of error rates produced by these 10 folds.  Table 
40 shows experimental results.  The symbol “only with feature x” shows the error rate 
of using one single feature, while “with a
feature x PTY OPR POS NEG TOW TSR MSR ALL 
only with feature x  19.6 38.5 34.9 35.3 21.9 26.6 29.6 12 2 .
with all but featu 12.8  re x 16.3 12.7 13.7 12.2 14.8 12.4
Table 40. Error rates of Q-Classifier. 
stion.  After all the 
dered together, the best performance is 87.8%.  
O
on.  If yes, 
the rule for the pattern is applied.  Otherwise, a scoring function is applied. 
he heuristic rules are listed as follows. 
er 
(6) pretype (PTY) is Selection: Majority 
 questions.  Then the 
di
ervation probability of a feature i in the question type j is defined in 
Formula (19):  
 
 
The features pretype (PTY) and totalow (TOW) perform best in reducing errors 
when used alone.  Moreover, they cannot be ignored since the error rate increases 
when they are excluded.  The feature totalow shows that if a question contains more 
opinion keywords, it is more possible that it is an opinion que
features are consi
PQ-Classifier 
OPQ-Classifier categorizes opinion questions into the corresponding opinion 
question types. We first examine if there is any specific pattern in the questi
T
 
(1) The pattern “A-not-A”: Yes/No 
(2) End with question words: Yes/No 
(3) “Who” + opinion operator: Hold
(4) “Who” + passive tense: Target 
(5) pretype (PTY) is Reason: Reason 
 
A scoring function deals with those questions which cannot be classified by the 
above patterns.  Unigrams, bigrams, and trigrams in training questions are selected as 
feature candidates.  These feature candidates are separated into the topic dependent 
type and the general type.  A topic dependent feature is only meaningful in questions 
of some topics, while general features may appear in questions of all kinds of topics.  
If a feature is topic dependent (e.g., human cloning and Clinton), it is dropped from 
the feature set.  Only general features (e.g., is or is not, whether, and reason) are kept.  
Finally, a set of features is obtained from the training
scriminate power of these features is calculated as follows. 
First, the obs
 
Opinion question type i % HD TG AT RS MJ YN
HD 87.1 0.0 0.0 4.4 0.0 0.0
TG 0.0 62.5 0.0 0.0 0.0 0.0
AT 0.0 0.0 78.2 0.0 0.0 0.0
RS 3.2 0.0 4.6 73.9 0.0 0.0
MJ 0.0 0.0 0.0 0.0 61.5 0.0
YN 9.7 37.5 17.2 21.7 38.5 100
C
la
ss
ifi
ed
 a
s t
yp
e 
j 
Total 100 100 100 100 100 100
Table 42. Confusion matrix (percentage). 
7.4 Answer Passage Retrieval 
Figure 18 shows the framework of answer passage retrieval in an opinion QA 
system.  The question focus (Q Focus) supplied by the question analysis serves as the 
input to an Okapi IR system (Reberson et al., 1998) to retrieve relevant sentences 
from the knowledge base.  From the relevant sentences, we can tell whether the focus 
(Focus Detection) is in a scope of opinion text spans or not (Opinion Scope 
Identification), and, if yes, (Opinion Toward Focus), whether the polarity (Detecting 
Polarity) of the scope matches the polarity of the question (Same Polarity).  The 
details are discussed in the following sections. 
 
 
Opinion operators and negation words are removed as well since they represent the 
question polarity instead of the question focus.  Once we have the question focus, we 
use the Boolean OR operator rather than the AND operator to form a query.   This is 
because we prefer the IR system to return sentences that have any relevancy to the 
question. 
 
 
7.4.2 Question Polarity Detection 
The polarity of the question is useful in opinion QA systems to filter out query-
relevant sentences which have different polarities from the question.  If the question 
polarity is positive, the sentences containing answers ought to be positive, and vice 
versa.  The polarity detection algorithm is shown as follows. 
 
(1) Determine the polarity of the opinion operator.  1 is for positive, 0 is for 
neutral, and -1 is for negative. 
(2) Negate the polarity of operator if there is any negation word anterior to the 
operator. 
(3) Determine the polarity of the question focus.  1 is for positive, 0 is for 
neutral, and -1 is for negative. 
(4) If one of the operator polarity and question focus is 0 (neutral), output the 
sign of the other; else output the sign of the product of the polarities of the 
opinion operator and the question focus. 
 
We consider the polarity of the question focus together with the polarity of the 
opinion operator, because the opinion operator primarily shows the opinion tendency 
of the question and different polarities of the question focus can affect the polarity of 
the entire question.  A positive opinion operator stands for a supportive attitude such 
as “agree”, “approve”, and “support”.  A neutral opinion operator stands for a neutral 
attitude such as “state”, “mention”, and “indicate”.  A negative opinion operator 
stands for a not-supportive attitude such as “doubt”, “disapprove”, and “protest”.  In 
the question “Who approves of the Joint College Entrance Examination?”, “approve” 
is a positive operator, and “the Joint College Entrance Examination” is a neutral 
question focus.  The overall polarity of this question is positive, so the opinion QA 
system needs to retrieve sentences that express a positive attitude to “the Joint 
College Entrance Examination.”  In contrast, in the question “Who agrees with the 
abolishment of the Joint College Entrance Examination?”, the question focus “the 
abolishment of the Joint College Entrance Examination” becomes negative because of 
“the abolishment”.  Even though the operator is positive, opinion QA systems still 
have to look for sentences that contain negative opinions toward “the Joint College 
Entrance Examination.” 
 
 
7.4.3 Opinion Scope Identification  
In Chinese, a sentence ending with a full stop may be composed of several sentence 
fragments sf separated by commas or semicolons as follows: ”sf1，sf2，sf3，…，
sfn。”.  Chen and Yan (1995) show that about 75% of Chinese sentences contain 
important issue.  Two approaches are adopted.  The opinion word approach employs a 
sentiment dictionary NTUSD 1 , which contains 2,812 positive words and 8,276 
negative words, to detect whether words in this dictionary appear in a certain scope.  
The score of an opinion scope is the sum of the scores of these words (Ku and Chen, 
2007).  
People sometimes imply their feelings or beliefs toward a particular target or event 
by actions.  For example, people may not say “Objection!” to disagree an event, but 
they may try to abolish or terminate it as possible as they could.  On the contrary, 
people may not say “I love it!” to show their delight with an event, but they may try 
to fight for it or legalize it.  In both circumstances, what people take in action 
expresses their opinions.  Action words are those which indicate a person’s willing of 
doing or not doing some behaviors.  For example, carry out, seek, and follow are 
words showing willingness to do something, and we name these words as do’s; 
substitute, stop, and boycott are words showing unwillingness to do something, and 
we name these words as don’ts.  We manually collect action words from materials 
other than those used in this research.  A total of 69 action words are collected, 
including 54 do’s and 15 don’ts.  In the action word approach, we detect opinions in 
scopes with the help of do’s and don’ts together with a sentiment dictionary.  
 
 
7.4.6 Experiments on Answer Passage Retrieval 
The f-measure metric is used for evaluation for the answer passage retrieval.  With 
recall (R) and precision (P), f-measure is defined as 2RP/(R+P).  To answer an 
opinion question, all answer passages have to be retrieved for opinion polarity 
judgment.  Therefore, the conventional evaluation metric that uses the precision and 
recall at a certain rank, e.g., top 10, may not be suitable for this task.  Since all answer 
passages, sentence fragments and meaningful units which provide correct answers are 
already annotated in the testing bed, the f-measure metric can be applied without 
questions.  Tables 43 and 44 show the f-measures of answer passage retrieval using 
the opinion word approach and the action word approach, respectively.  In these two 
approaches, adopting meaningful units as opinion scopes is better than adopting 
sentences and sentence fragments.  Considering both opinion and action words is 
better than opinion words only.  The best f-measure 40.59% is achieved when 
meaningful units and partial match are used. 
Although meaningful units are the most reasonable units for opinionated question 
answering, exact match is better than partial match when using opinion word 
approach, while it is the opposite when adopting action word approach.  This is 
because the number of opinion words is much greater than the number of action 
words.  Although opinion words are useful in extracting opinion evidence as well as 
action words, they may bring in noise.  Applying exact match is more helpful than 
applying partial match in the aspect of expelling noise. 
 
 
 
 
 
 
                                                 
1 http://nlg18.csie.ntu.edu.tw:8080/opinion/index.html 
shows the performance of using correct opinion fragments, which are relevant to the 
question focus, to decide opinion polarities.  Rel2T is similar to the relevant sentence 
retrieval, which was shown to be tough in the TREC novelty track (Soboroff and 
Harman, 2003).  From Rel2T to Rel2Q and CorrectMU, the best strategy for matching 
the question focus switches from partial match to lenient.  This is reasonable, since 
the contents of Rel2Q and CorrectMU are already relevant to the question focus.  In 
Rel2Q, doing focus detection doesn’t benefit or harm much (50.37% vs. 53.06%).  It 
shows that the question focus will appear exactly or partially in the relevant sentences.  
However, focus detection lowers the performance in CorrectMU (72.84% vs. 
84.96%).  It tells that the question focus and the correct meaningful units may appear 
in different positions within the sentence.  For example, the first meaningful unit talks 
about the question focus, while the third meaningful unit really answers the question 
but omits the question focus since it is mentioned earlier.  From Rel2T to Rel2Q, the 
f-measure does not increase as much as that from Rel2Q to CorrectMU.  This result 
shows that finding the correct fragments of passages to judge the opinion polarity is 
very crucial to answer passage retrieval.  The f-measure of CorrectMU shows the 
performance of judging opinion polarities without the relevant issue.  Using either the 
opinion word approach or the action word approach achieves an f-measure greater 
than 80%.  As a whole, including action words is better than using opinion words 
only. 
 
 
8 Conclusion and Future Work 
Our approaches facilitate Chinese opinion mining.  Approaches at word, sentence, 
and document levels function in a bottom-up fashion and can be easily replaced with 
other approaches to improve system performance in the development phase.  Taking 
into account temporal information when organizing mined opinions allows us to 
capture and track opinion variations over time. 
The way in which we discover relationships between entities leads us to view 
relationships in terms of related events; our mining of these relationships using 
opinions about latent events is achievable exactly because of our ability to track 
opinions.  Our experiments extend the objective knowledge base of relationship 
discovery to the subjective, and we show that even minor changes of opinions – short-
term variations of tendencies and weights – are helpful information.  This work 
demonstrates that collocation-based and opinion-based models are complementary, 
and can be integrated into algorithms that draw from the best of both worlds. 
We further gauge the usability of opinions for stock price prediction, under the 
assumption that events and opinions influence stock prices.  Our analysis shows that 
opinion variation for a company is related to its stock price variation: this is 
encouraging, although we are still far from a good prediction.  We have shown that 
opinions should be taken into consideration when predicting stock prices. 
The opinion analysis system CopeOpi was developed to illustrate applications of 
the proposed algorithms in opinion mining and relationship discovery.  The multi-
colored and temporal representations of opinions for its IR-system-like user interface 
provide a clear and friendly visualization for mined opinions and relationships.  
Moreover, using online news articles as the knowledge base demonstrates the 
applicability of our proposed algorithms in the real world.  Even though the original 
design of CopeOpi was for mining Chinese opinions, these methodologies can be 
extended to other languages, which takes us to the broader research challenges and 
Chen, H.-H. and Yan, S.-J. (1995). Dealing with Very Long Chinese Sentences in a Robust 
Parsing System. In Proceedings of National Science Council, Part A: Physical Science and 
Engineering, 19(5), (pp. 398-407). 
Chen, K.-H. (2002). Evaluating Chinese Text Retrieval with Multilingual Queries. 
Knowledge Organization, 29(3/4), (pp.156-170). 
Choi, Y., Cardie, C., Riloff, E. and Patwardhan, S. (2005). Identifying sources of opinions 
with conditional random fields and extraction patterns. In Proceedings of the Conference 
on Human Language Technology and Empirical Methods in Natural Language Processing 
(pp. 355-362). Association for Computational Linguistics . 
Chinchor, N. A. (1998). Overview of MUC-7. In Proceedings of Message Understanding 
Conference. http://www.itl.nist.gov/iaui/894.02/related_ projects/muc/index.html.  
Dave, K., Lawrence, S., & Pennock, D.M. (2003). Mining the peanut gallery: Opinion 
extraction and semantic classification of product reviews. In Proceedings of the 12th 
International World Wide Web Conference (pp. 519-528). Budapest, Hungary. 
Ghose, A., Ipeirotis, P. and Sundararajan, A. (2007). Opinion Mining using Econometrics: A 
Case Study on Reputation Systems. In Proceedings of the 45th Annual Meeting of the 
Association of Computational Linguistics (pp. 416-423). Association of Computational 
Linguistics. 
Hu, M. and Liu, B. (2004a). Mining Opinion Features in Customer Reviews. In Proceedings 
of the 19th National Conference on Artificial Intelligence, (pp. 755-760). 
Hu, M., & Liu, B. (2004b). Mining and summarizing customer reviews. In Proceedings of the 
2004 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 
(pp. 168-177). Seattle, USA.  
Jindal, N. and Liu, B. (2006). Identifying Comparative Sentences in Text Documents. In 
Proceedings of the 29th annual international ACM SIGIR conference on Research and 
development in information retrieval (pp. 244-251). ACM Press 
Kim, S.-M. and Hovy, E. (2004). Determining the sentiment of opinions. In Proceedings of 
the 20th International Conference on Computational Linguistics (pp. 1367-1373). Geneva, 
Switzerland. 
Kim, S.-M. and Hovy, E. (2005). Identifying Opinion Holders for Question Answering in 
Opinion Texts. In Proceedings of AAAI-05 workshop on Question Answering in Restricted 
Domains. Pittsburgh, Pennsylvania. 
Kleinberg, J. (2002). Bursty and Hierarchical Structure in Streams. In Proceedings of the 8th 
ACM SIGKDD International Conference on Knowledge Discover and Data Mining (pp. 
91-101). ACM Press. 
Ku, L.-W., Lee, L.-Y., Wu, T.-H., & Chen., H.-H. (2005). Major topic detection and its 
application to opinion summarization. In Proceedings of 28th Annual International ACM 
SIGIR Conference on Research and Development in Information Retrieval (pp. 627-628). 
Salvador, Brazil. 
Ku, L.-W., Liang, Y.-T. and Chen, H.-H. (2006). Opinion extraction, summarization and 
tracking in news and blog Corpora. Proceedings of AAAI-2006 Spring Symposium on 
Computational Approaches to Analyzing Weblogs, AAAI Technical Report (pp. 100-107). 
Ku, L.-W., Lo, Y.-S. and Chen, H.-H. (2007). Using Polarity Scores of Words for Sentence-
level Opinion Extraction. In Proceedings of the 6th NTCIR Workshop Meeting on 
Evaluation of Information Access Technologies (pp. 316-322). Japan. National Institute of 
Informatics. 
Ku, L.-W., and Chen, H.-H. (2007). Mining Opinions from the Web: Beyond Relevance 
Retrieval. Journal of American Society for Information Science and Technology, Special 
Issue on Mining Web Resources for Enhancing Information Retrieval, 58(12), (pp. 1838-
Meeting on Evaluation of Information Access Technologies: Information Retrieval, 
Question Answering and Cross-Lingual Information Access (pp. 265-278). Japan. National 
Institute of Informatics. 
Soboroff, I. and Harman, D. (2003). Overview of the TREC 2003 novelty track. In 
Proceedings of the 12th Text REtrieval Conference, National Institute of Standards and 
Technology (pp. 38-53). NIST, Maryland, USA. 
Stoyanov, V. and Cardie, C. (2006). Toward Opinion Summarization: Linking the Sources. In 
Proceedings of the Workshop on Sentiment and Subjectivity in Text, ACL-06 (pp. 9–14). 
Association for Computational Linguistics. 
Takamura, H., Inui, T. and Okumura, M. (2005). Extracting semantic orientations of words 
using spin model. In Proceedings of the 43rd Annual Meeting of the Association for 
Computational Linguistics (pp. 133-140). Ann Arbor, Michigan, USA. 
Wiebe, J. (2000). Learning Subjective Adjectives from Corpora. In Proceedings of the 
Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on 
Innovative Applications of Artificial Intelligence (pp. 735-740). AAAI Press/The MIT 
Press. 
Wiebe, J., Breck, E., Buckly, C., Cardie, C., Davis, P., Fraser, B., Litman, D., Pierce, D., 
Riloff, E., and Wilson, T. (2002). NRRC Summer Workshop on Multi-perspective Question 
Answering, Final Report.  In ARDA NRRC Summer 2002 Workshop. 
Wiebe, J., Wilson, T. and Bell, M. (2001). Identify collocations for recognizing opinions. In 
Proceedings of ACL/EACL2001 Workshop on Collocation: Computational Extraction, 
Analysis, and Exploitation. Toulouse, France, July. 
Wilson, T., Wiebe, J. and Hoffmann, P. (2005). Recognizing contextual polarity in phrase-
level sentiment analysis. Proceedings of the conference on Human Language Technology 
and Empirical Methods in Natural Language Processing (pp. 347-354). Association for 
Computational Linguistics. Morristown, NJ, USA. 
Yang K., Yu, N., Valerio, A., Zhang, H. and Ke, W. (2007). Fusion Approach to Finding 
Opinions in Blogosphere. In Proceedings of the International Conference on Weblogs and 
Social Media. Boulder, Colorado, USA. 
Yu, H., and Hatzivassiloglou, V. (2003). Towards Answering Opinion Questions: Separating 
Facts from Opinions and Identifying the Polarity of Opinion Sentences, In Proceedings of 
HLT/EMNLP, (pp. 129-136). 
 
表 Y04 
報告內容應包括下列各項： 
一、參加會議經過 
 第八屆美洲機器翻譯學會會議(The Eighth Conference of the Association for Machine 
Translation in the Americas)、以及 2008 年自然語言處理實驗法會議(2008 Conference on 
Empirical Methods in Natural Language Processing)，10 月 21 日到 10 月 27 日在夏威夷威
基基(Waikiki) The Hilton Prince Kuhio Hotel 舉行。AMTA 和 EMNLP 今年的投稿數非常
多，AMTA 比前幾年成長 2.5 倍，顯示大家對機器翻譯研究的興趣。EMNLP 共有 385
篇論文投稿，最後接受 81 篇一般論文(regular paper)和 35 篇壁報論文(poster paper)，本
屆首開在宣布接受與否前，由投稿人回覆審查意見的記錄。每篇論文有三位審查委員以
blind review 方式審查，相當嚴謹，筆者和博士生林欣毅君的共同著作是被接受為 regular 
paper，接受率僅有 21%。 
 筆者 10 月 21 日由台北出發，經由東京成田機場轉機至夏威夷參加這項會議，10 月
28 日由夏威夷經東京返回台北。 
 
二、與會心得 
機器翻譯由於網際網路所帶來的實用性，以及 Google、Microsoft 等大公司的
promotion，吸引很多研究人員投入，機器翻譯研究邁入另一高峰。今年探討的主要議題
規納成以下 6 項： 
 (1)  機器翻譯新技術。  
 (2)  研究人員正從事的下一代機器翻譯研究。  
 (3)  機器翻譯商業模式的創新性和市場分析。  
 (4)  使用者的機器翻譯需求。  
 (5)  商業使用者以機器翻譯解決的問題。  
 (6)  政府單位以機器翻譯完成的事務。 
 三場機器翻譯邀請演講談論下列議題： 
(1) 機器翻譯三項優勢：翻譯支出、翻譯所發的時間、和翻譯的品質。 
Intel 研製機器翻譯系統的動機是：採用編輯過翻譯結果可以有效降低 30%到
90% localization 的負擔；增加 10%到 100%顧客可以存取的內容；顧客可以獲
得本身語言的協助；提供快速的翻譯支援，由兩週降低到 24 小時；市場擴張。
(2) Trasl´an 翻譯系統人機互動模式。 
Trasl´an提升翻譯速度、準確性、和一致性。機器翻譯的挑戰之ㄧ是如何讓翻譯
人員採用機器翻譯技術，並充分運用這項技術。Trasl´an處理這個問題的方式
是：開發系統時就緊密的與內部翻譯人員合作，這些人員直接融入翻譯系統的
開發過程。因此，不僅可以協助訓練系統使用者，同時也可以與其他翻譯人員
有效的溝通，充分發揮翻譯系統的效能。 
(3) 自動機器翻譯 2008：科學遇到解決方案。 
機器翻譯相較於其他產業具有其獨特性，機器翻譯對於一般使用者、政府使用
者、和內容提供者的需求差異性。 
在論文發表方面，有 21 篇機器翻譯研究論文、6 篇學生研究論文、和 26 篇機器翻
譯在政府和商業上的應用。 
自然語言處理實驗法會議邀請到三位研究人員作專題演講，談論下列議題： 
(1) 過去 25 年網際網路資訊擷取經驗 
介紹 KnowItAll 計畫，嘗試解決人工智慧研究知識擷取瓶頸，以及建立智慧型
搜尋引擎。目前已經研究出資訊擷取法，可以處理任意網頁文件，和無預期的
 
