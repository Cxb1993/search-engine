A Novel Data Mining Algorithm for Searching the Relationship 
Between Environment and the Outage Incidents of Taiwan Power Company 
Abstract 
Taiwan Power Company (Taipower), the major electricity provider in Taiwan, is 
committed to offering consistently high quality services to ensure user satisfaction. To 
facilitate timely and efficient maintenance work, Taipower has developed a maintenance 
system that keeps track of all maintenance procedures. Accessing this maintenance 
database via data mining, this study seeks to predict accidents by investigating causes of 
past incidents. A data cutting and inner product method (DCIP) is proposed to optimize 
the scan function. DCIP demonstrates higher search efficiency than the traditional Apriori 
algorithm in tests with different amounts of data and at different support counts. The 
association rules discovered in this research can help Taipower to determine causes of an 
accident when it occurs. In the future, a disaster prediction system can also be constructed 
based on the data models established in this study to provide immediate relief and prompt 
maintenance.  
 
Keywords: Data mining; Data cutting and inner product method; Mining methods and 
algorithms; Association rules 
 
1.  Introduction 
 Computer programming and database technology have been widely applied in 
industrial processes and business management. The vast amounts of data generated by 
corporate computerization however are often left unharvested and wasted. With proper 
data mining techniques, corporations can turn these data into valuable assets that yield 
important information for production process readjustment and customer service 
improvement. Such information apparently plays a crucial role in any corporate success. 
Since data mining was first introduced, many new algorithms have been developed to 
increase the speed in extracting large datasets. Apriori identifies and measures association 
rules by the support of attributes or itemsets. Though often used to establish correlation 
among items, Apriori is inefficient and time-consuming, especially in a world where 
speed and efficiency are paramount, because it works by recursively scanning the entire 
database.  
This study proposes a new technique called the Data Cutting and Inner Product 
Method (DCIP). To increase search efficiency and shorten computation time, DCIP 
divides data into smaller segments to accelerate computation, eliminates superfluous 
items, and reduces the number of processing steps by sorting them into a fixed order. This 
study compares the efficiency of Apriori and DCIP by applying them to Taipower 
maintenance records. In terms of the correlations derived from computation, both Apriori 
and DCIP could find the accurate correlations; this shows that the two algorithms are 
equally capable of identifying the same association rules. In terms of computation time, 
are then applied to mine frequent patterns in the target database. Two itemsets are 
generated in the mining process: candidate itemsets and large itemsets. All large itemsets 
from the previous stage (LK-1) form the candidate itemsets of the new stage. Large 
itemsets are identified as candidate itemsets are scanned with the minimum support. It is a 
rather time-consuming process, however, as Apriori scans the database all over again 
every time. 
Many scholars presented some other algorithms to improve the calculation processes. 
Compared with Apriori, these improved algorithms, such as the ant colony algorithm, can 
shorten operation time and reduce operation resources [22]. The frequent-pattern tree 
(FP-tree) algorithm was proposed to infer the association rules based on the characters of 
tree structure [23]. The Apriori algorithm is also improved by the Mining Sequential 
Pattern Algorithm, which integrates the time attribute into the data mining process [24]. A 
Top-Down FP-Growth algorithm was utilized to improve the efficiency of data mining by 
reducing the quantity of the processed data and deleting unavailable transactions [25]. The 
efficiency of frequent itemset selection is enhanced with the concept of tree data 
structures [26]. Combined with the greedy algorithm, Apriori not only identifies more 
accurate associated rules but also shortens the scan time [22]. Deleting redundant 
candidate items and selecting accurate candidate rules thus prove to be crucial steps to 
modify association rules [27]. To improve the efficiency of Alpriori, scholars have 
proposed a number of alternative algorithms [27, 28]. In addition, the Direct Hashing and 
Pruning algorithm (DHP) scans primarily by means of hash functions [29] that is, the 
second candidate itemset (C2) are calculated with a hash table. Therefore, DHP 
significantly reduces the execution time of forming the second candidate itemset. DHP, 
however, leaves something to be desired. For example, in the case of large data sets, it 
might take a long time to create the hash table. Besides, the itemsets pruned in the hash 
address might likely to include large itemsets. Savasere et. al., [30] on the other hand, 
proposed the partition algorithm which consists of two phases: in the first phase, the 
database is divided into N partitions, each of which is then scanned separately to find all 
the datasets with support counts greater than the minimum support threshold. In the 
second phase, the combination of all these local large itemsets form candidate itemsets. To 
create the global large itemsets, the candidate itemsets are scanned again to remove those 
items lower than the minimum support count. Dynamic Itemset Counting (DIC) 
represents yet another improved algorithm [31]. DIC partitions the database into blocks 
with a given number of datasets. After the first pass, the frequent itemsets of each block 
are retained to serve as the starting point of the next pass, thus effectively reducing the 
size of the database processed in each pass. Though all the algorithms mentioned above 
divide up the database into partitions, they do not process the data within the partitions to 
reduce the data size. Unlike them, the method proposed in this paper further sorts the data 
within each partition based on their support counts. By deleting items lower than the 
minimum support, the data size to be processed is reduced, thereby considerably 
increasing the mining efficiency. 
3. The Data Cutting and Inner Product Method  
9:  endfor 
 
Step2: Establish the first large itemset, L1 
10:  NT1=NT0 
11:  for  x =1 to NT0  
12:    if  SP0,x ＜ Smin     
13:       delete I0,x 
14:       NT1= NT1 – 1    
15:    endif 
16:   endfor 
/* the remained items become the elements of the first large itemset, H1,x ,  
   /*where x= 1, 2,  …., NT1  
17:   NI1=NI0 
18:   for x =1 to NI0 
19:      if  SI0,x = 1    
20:         delete T0,x 
21:         NI1= NI1 – 1     
22:      endif 
23:    endfor 
24:    I1,j =sort( H1,x)  /* recalculate the support of each item, 
/* the items of BM1 are arranged in the order of their supports 
 
Step3: Establish the second large itemset, L2 
25:  for x=1 to NT1 - 1  /* extract the item which has the minimum support 
26:     NT2=NT1 
27:     for y=1 to NT1 - x 
28:        if SP1,y < Smin     
29:          delete I1,y 
30:          NT2= NT2 – 1    
31:        endif 
32:      endfor 
/* ( xNTI 1,1 1 , I1,y(remainded)) become the elements of the second large itemset, H2,x ,  
 
Step4: Establish the third large itemset, L3 
33:      for m=1 to NT2 - 1  /* extract the item which has the minimum support 
34:         for n=m+1 to NT2  
35:            if ( nm VV

 ) ≥ Smin     
36:           ( xNTI 1,1 1 , I2,m, I2,n) become the elements of the third large itemset, H3,x 
Step 1 Data conversion 
Firstly, the items are converted into a Boolean matrix, as shown in Table 2. The 
right column represents the item number of each event. The bottom row, SPi, is the 
support count of each item.  
 
 
 
 
 
 
 
 
 
Step 2: Establish the first large itemset, L1 
Items F and I are deleted from Table 2 because their support counts are less than the 
minimum support, Smin. Sequentially the rest items are arranged by support count in 
descending order, as shown in Table 3. 
 
Step 3: Establish the first Reduction Matrix, BM1 
There are no pairs when the item number of each event is less than 2. Therefore 
event 10 is deleted from Table 3 because its item number is 1. Item E, which has the 
lowest support, is selected to be the conjunctive item. Events 1, 3, 5 and 6, which have 
item E, are selected to establish a new reduction matrix, as shown in Table 4.  
 
 
 
 
 
 
 
 
 
Step 4: Establish the second large itemset, L2 
We delete items A and H whose support counts are less than Smin. Therefore, the 
second large itemset, L2, is composed of itemsets (E, B), (E, C), (E, D), (E, G), and (E, 
J), as shown in Table 5. 
Table 3  the First Large Itemset 
Event B C D J A G H E Total 
1 1 1 1 0 1 0 1 1 6 
2 1 1 1 1 0 1 1 0 6 
3 1 1 1 1 1 1 0 1 7 
4 0 1 1 0 1 0 1 0 4 
5 1 0 0 1 0 1 0 1 4 
6 1 1 1 1 0 1 0 1 6 
7 1 1 1 1 1 1 0 0 6 
8 1 0 1 0 0 0 1 0 3 
9 1 1 1 1 1 0 1 0 6 
10 0 1 0 0 0 0 0 0 1 
SPi 8 8 8 6 5 5 5 4  
 
Table 4  the First Reduction Matrix 
E 
Event B C D J A G H Total 
1 1 1 1 0 1 0 1 5 
3 1 1 1 1 1 1 0 6 
5 1 0 0 1 0 1 0 3 
6 1 1 1 1 0 1 0 5 
SPi 4 3 3 3 2 3 1  
 
efficiency of the data mining algorithm. The research design that consists of five 
procedures is shown in Figure 1. The six research procedures are: problem definition, data 
preparation, variable selection, algorithm efficiency analysis, and data mining and result 
explanation. These procedures help to extract the most important association rules that 
satisfy the minimum support and confidence from the database. Then, expert views and 
suggestions are sought to shed light on the association rules and the condition attributes of 
accidents. The efficiency of the improved algorithm is analyzed and evaluated with the 
maintenance database. Finally, based on the association rules derived from maintenance 
database, the engineers improve the maintenance efficiency.  
4.1 Problem Definition 
This work centers upon Taipower’s annual accident and disaster data. The DCIP 
algorithm is adopted to find out possible accident causes and any unusual condition 
signals that might have contributed to the accidents. After these causes and signals are 
carefully analyzed, the research results, in conjunction with expert opinion on the 
association rules, provide valuable information to Taipower’s decision makers. In 
addition the research also assists the maintenance crews in prompt identification of the 
accident cause and in having the problem solved in the shortest time possible, so that the 
electricity company could maintain a high-quality electricity supply service and enhance 
customer satisfaction. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1 the procedures of the research design 
 
4.2 Data Preparation 
Before performing data analysis, items in the data file are divided into two categories: 
time attributes and space attributes. Useless information, or noise, is removed to facilitate 
data consolidation or re-encoding. This procedure increases the accuracy of data analysis 
Problem Definition 
Data Preparation 
Variable Selection 
Algorithm 
Efficiency Analysis 
 
Data Mining 
Result Explanation 
 
This study examines the 2006 and 2007 Taipower repairs and maintenance records, which 
have 346 entries in 2006 and 238 entries in 2007. Each entry has 23 condition attributes, 
such as weather, month, day of the week, start/stop times, time spent, and so on. In 
addition, each entry has 19 decision attributes, including operation mistakes, lightning 
strikes, construction defects of another department, and damage caused by fallen trees. 
The algorithm is written with Microsoft Visual Studio 2008 C
＃
. Three approaches are 
adopted to prove that DCIP has higher search efficiency than the traditional Apriori 
algorithm.  
Approach 1: Comparing algorithm efficiency at varying support levels  
First, the two algorithms are applied to the 2006 data with varying support thresholds 
to observe the effect on efficiency. Figure 2 indicates the performance time of both 
algorithms at the minimum support levels of 0.2, 0.18, 0.16, 0.14, 0.12, 0.10, 0.08 and 
0.06 respectively.  
 
 
Figure 2  Performance Time Comparison with the 2006 Data 
Figure 2 clearly shows that as the minimum support becomes lower, Apriori’s 
performance time increases. To be exact, when the support level is set at 0.12, the running 
time escalates quickly. At such a low support level, the computational burden of the 
algorithm grows exponentially. The growing number of high frequency items leads to 
larger candidate itemsets, as well as longer computation time, which eventually results in 
lower efficiency. Instead of a steep rise, the performance time of the DCIP algorithm 
increases steadily as the minimum support gets lower. Therefore, even at a low support 
level, DCIP still achieves higher efficiency. 
Approach 2: Comparing algorithm efficiency with a larger database 
 Next, the 2006 and 2007 data are merged, as such the data size increases from 346 to 
584 entries. Figure 3 shows the results in respect to performance time at different support 
thresholds. When the minimum support is set at 0.16, there is a three-fold difference 
between DCIP and Apriori, namely, the latter taking triple the amount of the former’s time 
to accomplish the task. Therefore, compared with Apriori, the DCIP algorithm shows 
take too long to run. Therefore, how to enhance the efficiency of association rule mining 
has become a primary focus of current research. This study identified three major flaws of 
the original algorithm (i.e., Apriori) in computation: inconsistent efficiency in computing 
large amounts of data, too many invalid pairs, and many invalid transactions in unfiltered 
itemsets. To correct these flaws and improve the original algorithm, the Boolean 
algorithm was adopted in combination with the matrix model. The results clearly indicate 
that the DCIP algorithm is a lot more efficient than Apriori. Table 8 shows the differences 
between the two algorithms. 
 
 
Table 8  Comparison of the two algorithms 
 Apriori DCIP 
Computation Method Scan the database 
directly 
Boolean algorithm  
matrix model  
Search Method Scan the database 
directly 
Boolean algorithm is 
database is partitioned 
E
x
ecu
tio
n
 T
im
e 
Larger Data 
Size 
Longer execution 
time 
Shorter in comparison with 
Apriori 
Lower 
Support 
Longer execution 
time 
Shorter in comparison with 
Apriori 
4.5 data mining and result explanation 
Table 9  2006 & 2007 Accident Association Rules 
 
This study uses Taipower’s 2006 and 2007 accident and maintenance records as the 
empirical basis of research. There are 584 entries in total, each of which has 19 accident 
cause variables. With the support count thus set at 31 (584/19≒31), and confidence set at 
50%, the DCIP algorithm successfully identifies the association rules of the five highest 
support values, as shown in Table 9. The five leading accident causes all turn out to be 
 Association Rules Target Variable Support confidence 
1.  
Weather—thunderstorm  
Interruption Costs—none 
Lightning strikes 74% 67.89% 
2.  
Weather—thunderstorm 
Oscillograph—no record taken 
Lightning strikes 60% 68.18% 
3.  
Interruption duration—instant  
Power supply—no influence 
Lightning strikes 58% 59.79% 
4.  
Interruption duration—instant  
Interruption Costs—none  
Lightning strikes 57% 61.29% 
5.  
「345kV」 
Interruption Costs—none  
Lightning strikes 50% 59.52% 
 References 
[1]  M.J.A. Berry, G.S. Linoff, Data Mining Techniques For Marketing, Sales and 
Customer Support. John Wiley & Sons, New York. 1997. 
[2]  U.M. Fayyad, Making Sense Out of data. Data Mining and knowledge Discovery, 11, 
(1996), 20-25. 
[3]  W. J. Frawley, G. Piatetsky-Shapiro, C. J. Matheus, Knowledge discovery databases: 
An overview, in Knowledge Discovery in Databases, AI Magazine. 13, (1992), 57-70. 
[4]  E. Vazquez, H.J. Altuve, O.L. Chacon, Neural network approach to fault detection in 
electric power systems. IEEE International Conference on Neural Networks(ICNN). 
4, (1996), 2090-2095. 
[5]  M. Sforna,. Data mining in a power company customer database, Electric Power 
Systems Research. 55, (2000), 201–209.  
[6]  C. Rygielski, J. C. Wang, D. C. Yen, Data mining techniques for customer 
relationship management, Technology in Society.24(4), (2002), 483-502.  
[7]  C. T. Su, Y. H. Chen, D.Y. Sha, Linking innovative product development with 
customer knowledge: a data-mining approach. Technovation, 26(7), (2006),784-795. 
[8]  I. S.Y. Kwan, J. Fong, H. K. Wong, An e-customer behavior model with online 
analytical mining for internet marketing planning, Decision Support Systems. 41(1), 
(2005), 189-204. 
[9]  D. R. Carvalho, A. A. Freitas, A hybrid decision tree genetic algorithm method for 
data mining, Information Sciences. 163, (2004), 13-35. 
[10]  S. S. R. Abidi, Knowledge management in healthcare: towards knowledge- driven 
decision-support services, International Journal of Medical Informatics. 63, (2001),  
5-18. 
[11]  C. F. Chien, A. Hsiao, I. Wang, Constructing semiconductor manufacturing 
performance indexes and applying data mining for manufacturing data analysis, 
Journal of the Chinese Institute of Industrial Engineers. 21(4), (2004), 313-327. 
[12]  C. F. Chien, H. C. Li, A. Jiang, Data Mining for Improving Solder Bumping Process 
in the Semiconductor Packaging Industry, International Journal of Intelligent Systems 
in Accounting, Finance and Management. 14, (2006), 1-15.  
[13]  C. F. Chien, W. Wang, J. Cheng, Data mining for yield enhancement in 
semiconductor manufacturing and an empirical study, Expert Systems with 
Applications. 33(1), (2007), 192-198. 
[14]  S. Hsu, C. F. Chien, Hybrid Data Mining Approach for Pattern Extraction from 
Wafer Bin Map to Improve Yield in Semiconductor Manufacturing, International 
Journal of Production Economics. 107, (2007), 88-103.  
[15]  C. Keissner, Data Mining for the Enterprise, Proc. of the 31st Annual Hawaii 
International Conference on System Science (HICSS 98). (1998), 295-304. 
[16] J. R. Richard, W. G. Michael, Data Mining a Tutorial-Based Primer, Addison Wesley, 
New York. 2002. 
  
國科會補助專題研究計畫項下出席國際學術會議心得報告 
 
一、參加證書 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
二、會議內容 
                          
 計畫編號 NSC 100-2221-E-020-026- 
計畫名稱 資料探勘在台電停電事故關連性法則搜尋之應用 
出國人員
姓名 
黃怡詔 
服務機構及
職稱 
屏東科技大學工業管理系 
助理教授 
會議時間 
100年12月21日 
至 
100年 12月23日 
會議地點 
Patong Beach Hotel, Phuket, 
Tailand 
會議名稱 ICDM 2011 : International Conference on Data Mining, Thailand 
發表論文
題目 
An Improved Data Mining Method Applied to the Search of Relationship 
between Metabolic Syndrome and Lifestyles 
 
  
內容：研討會議程與所有發表論文內容 
六、其他 
  
similar attempt to enhance the performance of Apriori. Most significantly, DCSM modifies the data processing technique to remove 
the problem of repetition that is chiefly responsible for slowing down the calculations.  
III. DATA CUTTING AND SORTING METHOD 
Notations: 
DB：the original trade database  
Ndb: the quantity of items of the original trade database  
Oi:the items of the original trade database，i = 1…Ndb  
NR:the quantity of the trade of the original trade database  
Ri: the set consisted of the items of the i
th
 trade  
Ii,j：the j
th
 item of the i
th
 Boolen Matrix  
Hi,j：the j
th
 item of the i
th
 large itemset  
Li：the ith large itemset  
NTi：the quantity of items of the i
th
 Boolen Matrix   
NIi：the quantity of trades of the i
th
 Boolen Matrix  
Smin：the minimum support  
Ti,j：the j
th
 trade of the i
th
 Boolean matrix  
Eijk：the Boolean value of the k
th item of the jth trade of the ith Boolean matrix  
Eijk=



 
 
SIi,j：the sum of the j
th
 item of the i
th
 Boolean matrix  
SPi,j：the support of the j
th
 item of the i
th
 Boolean matrix  
BM0：the original Boolean matrix 
BM1：the Boolean matrix of the first large itemset, L1 
BM2：the Boolean matrix of the second large itemset, L2 
A. The Procedures of DCSM 
To improve the efficiency of the Apriori Algorithm, this paper proposes the data cutting and sorting method, DCSM, 
to verify the relationship between Metabolic Syndrome and Lifestyles. DCSM reduces the calculation time by getting 
rid of redundant data during the data mining process. In addition, DCSM minimizes the computational units by 
splitting the database and sorting data with support counts. The procedures of DCSM are illustrated as follow: 
 
Step1: Transfer the original database into the original Boolean 
matrix, BM0 
/ * DB  BM0 
1:  for i = 1 to NR  
2:     for j= 1 to Ndb  
3:        if Oj   Ri 
4:          E0ij = 1 
5:        else 
6:          E0ij = 0 
7:       endif 
8:     endfor 
9:  endfor 
 
Step2: Establish the first large itemsets, L1 
 
10:  NT1=NT0 
11:  for  x =1 to NT0  
1, the kth item of the jth trade of the ith Boolean matrix exists 
0, the kth item of the jth trade of the ith Boolean matrix does not exist 
  
B. An Illustration of the Data Cutting and Sorting Method 
The following example, as shown in Table I, illustrates the deduction procedure via DCSM. The minimum support count, Smin, 
is set at 2 in this example. 
Step 1 Data conversion 
Firstly, the items are converted into a Boolean matrix, as shown in Table II. The right column represents the item number of 
each event. The bottom row, SPi, is the support count of each item.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Step 2: Establish the first large itemsets, L1 
Items E and F are deleted from Table II because their support counts are less than the minimum support, Smin. Sequentially the 
remaining items are arranged by support count in descending order, as shown in Table III. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Step 3: Establish the first Reduction Matrix 
There are no pairs when the item number of each event is less than 2. Therefore event 5 is deleted from Table III because its item 
number is 1. Item H, which has the lowest support, is selected to be the conjunctive item. Events 1 and 2, which contain H, are 
selected to establish a new reduction matrix, as shown in Table IV. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
TABLE II 
ORIGINAL BOOLEAN MATRIX 
TID A B C D E F G H Count 
1 1 1 0 1 0 0 0 1 4 
2 1 1 1 1 0 1 1 1 7 
3 1 1 1 0 0 0 1 0 4 
4 1 1 0 0 1 0 1 0 3 
5 1 0 0 0 0 0 0 0 1 
Support 5 4 2 2 1 1 3 2  
 
TABLE III 
 THE FIRST LARGFE ITEMSETS 
TID A B G C D H Count 
1 1 1 0 0 1 1 4 
2 1 1 1 1 1 1 6 
3 1 1 1 1 0 0 4 
4 1 1 1 0 0 0 3 
5 1 0 0 0 0 0 1 
Support 5 4 3 2 2 2  
 
TABLE IV  
THE FIRST REDUCTION MATRIX FOR H 
H 
TID A B D G C Count 
1 1 1 1 0 0 3 
2 1 1 1 1 1 5 
Support 2 2 2 1 1  
 
  
 
 
 
 
 
 
 
 
 
I. CONCLUSION 
DCSM speeds up the search in the mining process by partitioning the database (Ye and Chiang, 2006) and by deleting items 
whose support counts are lower than the minimum support (such as items E and F in Step 2) and transactions whose item number is 
lower than 2 (such as event 5 in Step 3). Besides, DCSM arranged items in the order of their support value (such as Step 2). In such 
a way, the items to be processed are reduced considerably, so is the search time due to deletion of repetition. The association rules 
derived by DCSM and by Apriori respectively are the same, but it is evident that DCSM has better efficiency than Apriori. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
ACKNOWLEDGMENT 
This research was partially supported by National Science Council, Taiwan, ROC. (NSC 98-2221-E-020-006 ; NSC 
99-2221-E-020-021). 
REFERENCES 
[1] R. Agrawal, R. Srikant, “Fast Algorithms for Mining Association Rule,” in  Proceedings of 20th International Conference on Very Large Data Bases, pp.  
487-499, 1994. 
[2] W. J. Frawley, G. Piatetsky-Shapiro, C. J. Matheus, “Knowledge discovery databases: An overview,” AI Magazine. vol. 13, no. 3, pp. 57-70, 1992. 
[3] F. H. Grupe,; M. M. Owrang, “ data base mining discovering new knowledge and competitive advantage,” Information systems management, vol. 12, no. 4, pp. 
26-31, 1995. 
[4] M.H. Smith, and W. Pedrycz, “Expanding the meaning of and applications for data mining,” International Conference on Systems, Man, and Cybernetics, vol. 
3, 2000, pp. 1874. 
[5] J. S. Park, M. Chen and P. S. Yu,  “An effective hash-based algorithm for mining association rules,” in Proceedings of the 1995 ACM SIGMOD international 
conference on Management of data. 
[6] U. Takeaki, A. Taisuya , U. Yuzo, A. Hiroki “LCM：AN Efficient Algorithm for Enumerating Frequent Closed Item Sets,” In Proc. IEEE ICDM99 Workshop 
FIMI’03,2003. 
[7] K. M. Yu, J. Zhou, T. P.  Hong, J. L. Zhou, “A Load-Balanced Distributed Parallel Mining Algorithm,” Expert Systems with Applications, vol. 37, issue 3, 
pp.2459-2464, 2010. 
 
Fig. 1 Calculation time of DCSM and Apriori 
Support 
TABLE IX 
ASSOCIATION RULES 
Association Rules (L3) Support(%) Confidence(%) 
BMI>27,  Exercise≦1,                        MS 11.89 61.36 
BMI>27,   Shift-Work,                         MS 10.57 55.81 
BMI>27,    Male,                                MS 10.13 58.97 
Male,         No Coffee,                         MS 8.81 57.14 
BMI>27,    with family history of MS,   MS 7.05 100.00 
 
100 年度專題研究計畫研究成果彙整表 
計畫主持人：黃怡詔 計畫編號：100-2221-E-020-026- 
計畫名稱：資料探勘在台電停電事故關聯性法則搜尋之應用 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 5 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 2 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
