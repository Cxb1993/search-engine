(CCPs) is a critical issue in SPC. The cost or impact 
of incorrect pattern recognition becomes very 
prominent when a subsequent search is made to 
determine the assignable cause(s). Hence, the 
recognition accuracy is an important performance 
index for a CCP recognition (CCPR) model. Various 
machine learning (ML) approaches have been applied 
into SPC since 1990s. In the pertinent literature, 
the ML-based approach has been proved to be a 
superior alternative to traditional control chart 
schemes. However, almost all the ML-based CCPR models 
in the literature employed a single classifier 
approach to recognize CCPs. It is difficult for a 
single classifier to complete a large and complicated 
CCPR task, such as on-line recognizing concurrent 
(mixed) CCPs. Ensemble methodology imitates human 
being＇s second nature to seek several opinions 
before making a crucial decision. Creating effective 
component classifier is very important for 
implementing ensemble classification. This research 
not only applied classifier ensemble approaches to 
develop effective ML-based models for on-line 
monitoring and diagnosing concurrent CCPs, but also 
focused on creating effective component classifiers 
for the classification ensemble used in on-line CCPR 
tasks. Three typical classifier ensemble methods—
Bagging, AdaBoost, and Arcing—are employed and 
enhanced using a diversity measure. Neural networks 
were applied for building component classifiers in 
this research. The simple majority voting method was 
used to combine the outputs of component classifiers. 
The performance of the proposed ML-based on-line CCPR 
model using ensemble classifier was compared to those 
of the single classifier CCPR model in the 
literature. Numerical results obtained using 
extensive simulation show that the proposed 
classifier ensemble model can significantly 
outperform the single classifier model in the 
literature for on-line recognition of concurrent CCPs 
with respect to both recognition accuracy and speed. 
These results confirm the great potential of 
classifier ensemble methods in the field of CCP 
1 
On-line recognition of concurrent control chart patterns using 
artificial neural network-based classifier ensemble methods 
 
Abstract 
 
Statistical process control (SPC) has been well recognized as an effective method for 
improving and maintaining quality and productivity of industrial products. Recognition of control 
chart patterns (CCPs) is a critical issue in SPC. The cost or impact of incorrect pattern recognition 
becomes very prominent when a subsequent search is made to determine the assignable cause(s). 
Hence, the recognition accuracy is an important performance index for a CCP recognition (CCPR) 
model. Various machine learning (ML) approaches have been applied into SPC since 1990s. In the 
pertinent literature, the ML-based approach has been proved to be a superior alternative to 
traditional control chart schemes. However, almost all the ML-based CCPR models in the 
literature employed a single classifier approach to recognize CCPs. It is difficult for a single 
classifier to complete a large and complicated CCPR task, such as on-line recognizing concurrent 
(mixed) CCPs. Ensemble methodology imitates human being’s second nature to seek several 
opinions before making a crucial decision. Creating effective component classifier is very 
important for implementing ensemble classification. This research not only applied classifier 
ensemble approaches to develop effective ML-based models for on-line monitoring and 
diagnosing concurrent CCPs, but also focused on creating effective component classifiers for the 
classification ensemble used in on-line CCPR tasks. Three typical classifier ensemble 
methods—Bagging, AdaBoost, and Arcing—are employed and enhanced using a diversity 
measure. Neural networks were applied for building component classifiers in this research. The 
simple majority voting method was used to combine the outputs of component classifiers. The 
performance of the proposed ML-based on-line CCPR model using ensemble classifier was 
compared to those of the single classifier CCPR model in the literature. Numerical results obtained 
using extensive simulation show that the proposed classifier ensemble model can significantly 
outperform the single classifier model in the literature for on-line recognition of concurrent CCPs 
with respect to both recognition accuracy and speed. These results confirm the great potential of 
classifier ensemble methods in the field of CCP recognition. 
 
Keywords: Statistical process control; control chart; pattern recognition; concurrent pattern; 
machine learning; classifier ensemble; neural network. 
 
中文摘要 
 
將產品的某些品質特徵(quality characteristic)維持在一個穩定的水準上，一直是品質管理
的一個基本目標，而統計製程管制(statistical process control, SPC)是最常被用來達成這個目標
的工具。管制圖上所出現的異常形狀(pattern)通常和某些造成製程失控(out-of-control)的特定
原因(assignable cause)有關，因此正確且迅速地辨識管制圖形狀(control chart pattern, CCP)是
SPC 的一個重要課題。在辨識 CCP 時所發生的形狀誤判，有時會嚴重地誤導後續框正失控
製程的行動，從而發生不必要的成本損失，因此準確率是 CCP 辨識模式的重要績效指標。
從 90 年代以來，有許多學者陸續利用各種機器學習(machine learning, ML)技術辨識 CCP，
研究結果顯示以 ML 為基的 CCP 辨識模式，在準確度及速度上均優於傳統的管制圖，但是，
文獻中幾乎所有以 ML 為基的 CCP 辨識模式均使用單一分類器(single classifier)來辨識
CCP，單一分類器常因分類偏差或變異(classification bias/variance)問題，很難對複雜的 CCP
辨識任務，如辨識同時發生(concurrent)或混合(mixed)的 CCP，具有良好的績效。集成式
(ensemble)方法模仿人類在作重大決策前，會先諮詢多位專家意見的行為，其核心原理在於
整合多位專家的意見後，所作的決策，常比只聽單一專家的意見好。本研究應用集成式分類
技術構建以 ML 為基的混合 CCP 線上辨識與分析模式，而有效的成份分類器(component 
3 
trend pattern that may be concurrent with a cycle pattern (resulting from another assignable cause 
that comes and goes on a regular basis, e.g., unstable material). In order to effectively identify each 
cause in real time, it is essential that each ‘target’ pattern be separately and accurately identified. 
The cost or impact of incorrect pattern identification becomes prominent when a subsequent action 
is taken to try to remove the assignable cause(s). For example, if a trend pattern is incorrectly 
recognized as a shift pattern, the corrective action for a shift pattern could be wrongly applied to 
the trend pattern. Then, a trend pattern resulting from loose fittings could be incorrectly identified 
as a shift pattern, which may have been caused by a tool breakage. The remedy taken for a shift 
pattern might then be to replace an expensive part whereas only a simple adjustment is needed to 
correct the problem. Hence, quickly and accurately recognize each target pattern is a substantial 
issue in on-line SPC. 
On the other hand, the ‘correlated patterns’ are often defined as CCPs of more than one quality 
variables, which may be provoked by one assignable causes or more than one dependent 
assignable causes in a multivariate scenario. Therefore, these CCPs are statistically correlated. 
Recognition of such correlated CCPs are often referred to as multivariate SPC procedures. This 
study focuses mainly on the on-line recognition of concurrent CCPs in a univariate scenario only. 
Among the seven control chart concurrent pattern-related works in the literature, Guh and 
Tannock (1999) primarily used the direct (raw) data as the input to ANN-based CCP recognizers, 
while other six works adopted a hybrid approach to recognize concurrent CCPs. Al-Assaf (2005) 
and Chen et al. (2007) used the concurrent CCP features extracted using wavelet transform as the 
input to ANN classifier. Wang et al. (2009) and Lu et al. (2011) used the CCP features extracted 
using independent component analysis (ICA) as the input to decision tree (DT) and support vector 
machine (SVM) classifiers, respectively. Gu et al. (2012) and Xie et al. (2013) applied 
independent components, which were decomposed from original concurrent CCP data using 
singular spectrum analysis (SPA), as the input to ANN and SVM classifiers, respectively. The 
reported performances of these recognition models for control chart concurrent patterns were 
promising. The average classification rates (CRs)—the ratio of the number of correctly identified 
examples to the total number of examples—achieved by these models were all higher than 80%. 
Gu et al. (2012) further presented a real case study in an aluminium smelting process to show the 
usefulness and robustness of their proposed method. Unfortunately, almost all the works referred 
here evaluated the CR performances in an off-line recognition scenario, in which complete (full) 
patterns are shown in the recognition data vector to the CCP recognizer. In the pertinent literature, 
only Guh and Tannock (1999) work evaluated and reported the performances of the CCPR model 
in an on-line recognition scenario. The average CR performance of the proposed model on control 
chart concurrent patterns was relatively low (56.5%) in an on-line process monitoring scheme. 
Note that among the seven related works in this research topic, six of them have used the term 
‘concurrent’ patterns. Only the work by Lu et al. (2011) used the term ‘mixture’ patterns. To avoid 
the possible confusion of the potential readers, this study used the term ‘concurrent’ CCPs to be 
consistent with most of the researches in this field. 
Concurrent patterns are more difficult to recognize than single patterns due to the pattern 
interaction and resultant complexity. Moreover, this recognition problem is more serious when the 
CCPR is being carried out in an on-line real-time mode. For example, the average accuracy of 
on-line recognition of concurrent shift and cycle patterns was only 24.7% in Guh and Tannock’s 
work (1999). In an on-line recognition scenario, the monitored process data will, in practice, 
appear as a continuous stream of partially developed patterns. Together with the common cause 
variation inherent in the manufacturing process, such developing patterns are difficult to recognize 
since their structure are normally vague and dynamic. Consequently, the recognition ability of the 
existing ANN-based on-line CCPR models needs to be improved in order to overcome this 
problem. 
With a finite number of addressed out-of-control situations (i.e. CCP types), the CCPR 
problem can be essentially formulated into a classification problem. Due to the fact that it is 
difficult for a single classifier to complete a large, complicated task (Liu et al., 2000; Zhou et al., 
2002), the performances of the on-line CCPR models in this study are enhanced using classifier 
On the basis of the discussions detailed above, this study has two major purposes. First, this study 
intends to further examine the feasibility of applying classifier ensemble methods in the on-line CCPR field. 
Second, this study aims to apply classifier ensemble techniques to build an effective ANN-based 
model for on-line detection and diagnosis of concurrent CCPs. Three common classifier ensemble 
methods—Bagging (Breiman, 1996a), AdaBoost (Freund & Schapire, 1996), and Arcing (Breiman, 
1998)—are used and enhanced using a diversity measure to create useful component classifiers. 
The performance of the proposed on-line CCPR model using ensemble classifiers is compared 
with those of other CCPR systems in the literature by simulation to demonstrate the superiority of 
the proposed model over on-line CCPR systems that use single classifiers. 
 
2. The classifier ensemble method 
 
Empirical studies conducted by the machine learning community have shown that combining 
the outputs of multiple classifiers (e.g. classifier ensembles) reduces the generalization error of the 
individual classifiers. The ensemble method is a learning paradigm in which a collection of several 
learners (classifiers) are trained for the same task. The main idea underlying the ensemble 
methodology is to weigh several individual pattern recognizers, and combine them in order to 
obtain a classifier that outperforms every one of them. It has been demonstrated that no ensemble 
method can produce the best performance in all possible domains (Rokach, 2010). Thus, given a 
certain application, the practitioner needs to decide which ensemble method should be used. 
 
2.1. Diversity in classifier ensembles 
 
Ensemble methods are very effective, mainly due to the phenomenon that different classifiers 
have different “inductive biases” (Mitchell, 1997). In order to make the ensemble more effective, 
there should be some sort of diversity between the component classifiers (Kuncheva, 2005). It has 
been proved that the ensemble error can be divided into a term measuring the average 
generalization error of each component classifier and a term called diversity that measures the 
disagreement among the classifiers (Krogh & Vedelsby, 1995). The procedure is concisely stated 
as follows. 
Formally, if fi(x) is the output generated by each of the ensemble members and each 
component classifier is assigned a weight Wi (i = 1, 2, …, n). The output of the ensemble is 
obtained using: 



n
i
ii xfWxF
1
)()(                                            (1) 1 0iW
1


n
i
iW
if Wi = 1/n (i = 1, 2, …, n), the output of ensemble is simple averaging, i.e. 
 


n
i
i xfnxF
1
)(/1)( .
The weighted average of the individual classifiers’ generalization error is given by 



n
i
ii xEWxE
1
)()(                                                         (2) 
where  (y(x) is the expected target value of the input x) is the squared 
error of the ith classifier on the input x. Then the generalization error of the ensemble on the 
input x is  
2( ) [ ( ) ( )]iE x F x y x 
 2)()()(ˆ xyxFxE                                                         (3) 
The generalization error of an ensemble can be decomposed by analysis of the ambiguity 
(diversity) of individual classifiers that make up the ensemble. The diversity of the classifier i with 
5 
jiijjiji
jiji
ji mmmm
mm
dv 
,                                                    (8) 
where ij  specifies the number of instances in which both classifier i and classifier j are 
correct while 
m
i j  indicates the number of instances that are misclassified by both classifiers. 
Similarly, 
m
i j  and m i j  indicate the number of instances in which one classifier has correctly 
classified the instances but its counterpart has misclassified these instances. 
m
 
2.3. Bagging with diversity 
 
Diversity improves the performance of a classifier ensemble. As a result, methods for creating 
ensembles center around producing classifiers that disagree on their predictions (classifications). 
Bagging (Breiman, 1996a), a “bootstrap” (Efron & Tibshirani, 1993) ensemble method, creates 
individuals for its ensemble by training each classifier on a random redistribution of the training 
set. Specifically, each classifier in the ensemble is trained on a sample of examples taken with 
replacement (allowing repetitions) from the training set. It is common to set the size of each 
training example set to the size of the original training set. Bagging tries to generate disagreement 
(diversity) among the classifiers by altering the training set each classifier sees. The Bagging with 
diversity (BWD) proposed in this study is designed based on Bagging by considering the diversity 
among the component classifiers in order to achieve better performances. The BWD algorithm is 
summarized in Figure 3. Bootstrapping builds k replicate training data sets by randomly 
re-sampling, but with replacement, from the original training data set, D, repeatedly. Each example 
in the original training set may appear more than once or not at all in any particular replicate 
training data set. Each replicate training set is used to train a certain classifier. After each new 
component classifier (Mi) is created, the diversity of the entire ensemble is calculated and checked 
in steps (4)–(8) using Eq. (7). The new component classifier, which does not contribute the total 
diversity of the ensemble, is discarded. The purpose of this algorithm is to assure that each newly 
created component classifier can more or less contribute some diversity to the classifier ensemble. 
The bagging classifier often has significantly greater accuracy than a single classifier derived from 
D, the original training set. It is also more robust to the effects of noisy data. The increased 
accuracy occurs because the ensemble model reduces the variance of the individual classifiers. 
 
2.4. AdaBoost with diversity 
 
Boosting is a general method for improving the performance of a weak learner. Similar to 
Bagging, the classifiers are generated by resampling the training set. The classifiers are then 
combined into a single strong composite classifier. Unlike Bagging, the probability of selecting an 
example is not equal across the training set. This probability depends on how often that example 
was misclassified by the previous k classifiers. Hence, Bagging can be treated as a special case of 
the boosting method, in which each training example’s probability of being selected is always 
equal (= 1/h) across the original training set. Breiman (1996a) refers to the boosting idea as the 
most significant development in classifier design of the Nineties. Both bagging and boosting is 
effective on “unstable” learning algorithms where small changes in the training set result in large 
changes in predictions. ANNs and decision tree (DT) learning (Quinlan, 1986) are examples of 
unstable learning algorithms. Unstable classifiers can have low bias on a large range of data sets. 
Their problem is high variance. Ensemble either through bagging or boosting reduces variance 
significantly. Boosting sometimes is better than bagging; sometimes it increases generalization 
error due to its sensitivity to noise. This may be caused by outliers in the data. An outlier will be 
consistently misclassified, so its probability of being sampled will continue to increase as the 
boosting continues. It will then start appearing multiple times in the resampled data sets. Therefore, 
the classifier will tend to over-learn from that outlier. In small data sets, this may be enough to 
warp the classifiers. 
7 
9 
.6. Ensemble size 
It has been shown that, for ensemble methods, most of the reduction in error occurs with the 
first
. An ANN-based classifier ensemble model for the recognition of concurrent CCPs 
The on-line CCPR model adopted here is an extended version of the model proposed by Guh 
(200
.1. Generation of concurrent CCP data 
Advances in manufacturing and measurement technology have made possible real-time, rapid, 
inte
 knowledge of a manufacturing process, the commonly 
enc
2
 
 few additional classifiers (Freund & Schapire, 1996; Quinlan, 1996). In most cases, 
ensembles containing ten classifiers are sufficient to reduce the error rate (Rokach, 2010). 
However, Opitz and Maclin (1999) reported in their empirical study paper that, for bagging and 
boosting methods applied to ANNs, much of the reduction in error appeared to have occurred after 
ten to fifteen classifiers. They further suggested that 25 would be a sufficient ensemble size. 
Therefore, in this study, the ensemble size was set to 25, i.e. an ANN learning algorithm was used 
to create 25 component classifiers in an ensemble. Instead of using the simple majority voting 
method, the distribution summation method (Clark & Boswell, 1991) was used to combine the 
outputs of the 25 ANN classifiers. The output data vectors obtained from each ANN classifier were 
summed up. Then the selected pattern type was chosen according to the highest value in the total 
vector. 
 
3
 
8). This model is designed to be applied to an automated manufacturing process where 
process data are collected automatically and monitored by a computer-based system, without the 
need for human intervention. The manufacturing process is monitored on-line by the CCPR model 
using a moving window recognition approach (Figure 6). A sequence of process data vectors are 
presented to the CCPR model in the moving window, which is incremented forward by one 
process observation at a time, denoting a single sampling interval. 
 
3
 
grated measurement of process and production quality. One hundred percent inspection is 
becoming increasingly common and the need for the analysis of individual measurements is also 
increasing. The individual (X) chart, which plots individual measurement values rather than 
subgroup averages, has attracted the interest of SPC practitioners. Although the use of X charts 
usually results in a higher Type I error (i.e. shorter in-control ARL), it can reveal an out-of-control 
situation quickly (i.e. shorter out-of-control ARL or lower Type II error). In-control ARL means 
the average number of observations needed for a process monitoring system to give an 
out-of-control signal when the process is actually in-control. In-control ARL and out-of-control 
ARL are referred to as ARL0 and ARL1, respectively, in this study. In a high-speed automatic 
production scenario, detecting and correcting the inceptive problem as early as possible is essential 
to the prevention of the possible high-speed manufacturing of defects. Hence, the X chart is the 
primary chart considered in this study. 
With appropriate experiences and
ountered types of the unnatural CCP in the process are usually known to the quality practitioner. 
In other words, the number of CCP types, which need to be identified and analyzed in the CCPR 
scheme, will usually be limited. To monitor a specific process, the CCPR model needs only to be 
trained and tested by the CCPs, which are commonly encountered in that process. Therefore, 
although limited types of simulated CCP process data are used here to train, test, and evaluate the 
proposed CCPR model, the proposed methodology can still be applied to a practical manufacturing 
process. For example, a concurrent CCP recognition model (Gu et al., 2012) developed with 
simulated CCP data was applied in an aluminium smelting process in a plant in Australia. The 
major target concurrent CCP type, which occurred in this smelting process, was the mixed 
downward trend and cycle pattern. In this application, both the concurrent CCP example sets 
collected from the real process (20 examples) and generated using simulation (300 examples) were 
tried to train the CCPR model. The model trained with simulated CCP example set performed 
11 
betw
.2. Model architecture 
This on-line CCPR model consists of two modules: the off-line learning module that uses 
ense
ns are made here: (1) Only univariate quality characteristics are used; 
(2) 
.3. Design and training of the component classifier 
Many different ANN structures and learning algorithms have been adopted to develop CCPR 
syst
omponent classifier for concurrent CCPs, the BPN comprised four 
laye
een natural and cyclic patterns. Skewness (SKEW) and Kurtosis (KURT) contain information 
on the shape of the distribution of the time series data (Kandall & Stuart, 1976). SKEW can be 
used to detect the difference between increasing and decreasing trends, while KURT can be 
employed to detect the difference between upward shifts and increasing trends as well as 
downward shifts and decreasing trends. Slope (SLP) and Pearson correlation coefficient (Pearson) 
were reported to be useful in the classification of increasing trends, decreasing trends, and natural 
patterns, where the gradients of trend patterns are small and the signal-to-noise ratio is low 
(Lavangnananda & Piyatumrong, 2005). The equations for the six features employed in this study 
are given in the Appendix (Section A.4). The SLP used here was calculated using the least squares 
method (Neter et al., 1996). 
 
3
 
mble methods and the on-line process monitoring module (Figure 7). The function of the 
learning module is to off-line build the required ensemble classifiers that will be used in the 
on-line process monitoring module. The methodology and procedure used to create the ensemble 
classifiers for recognition of concurrent CCPs in the learning module are described in Section 2. 
The primary function of the process monitoring module is to apply the built ensemble classifiers to 
on-line recognize concurrent CCPs. During the monitoring process, a data window formed with a 
sequence of process data sets Yi is presented to the ensemble classifier in the moving window, i = 
t-w+1, t-w+2, …, t-1, t, where t is the time of sampling and w is the recognition window size. If 
no unnatural CCP is identified in the current recognition window (i.e. the process is in-control), the 
next process data set, Yt+1, is included in the recognition window, and Yt－w+1 is excluded from the 
window. This monitoring procedure is repeated until unnatural CCPs are detected. The potential 
assignable causes towards the detected CCPs are then diagnosed and eliminated based on the 
relevant process knowledge. 
Three research assumptio
Only two concurrent patterns are present at any one time, and they are assumed to begin 
simultaneously; and (3) The common-cause standard deviation () does not change during the 
process. 
 
3
 
ems. These include multilayer perceptron (MLP) with back propagation learning (also known 
as BPN) (Guh, 2010; Hwarng & Wang, 2010); learning vector quantization (LVQ) networks (Guh, 
2008); probabilistic neural networks (PNN) (Plummer, 1993); and adaptive resonance theory 
network (ART) (Hwarng & Chong, 1995). The BPN was considered to be the most appropriate 
ANN algorithm for constructing the component classifier here for two reasons. Firstly, BPNs have 
been used successfully in many CCP applications according to the literature. Secondly, each output 
value from the output nodes of a BPN can represent a specific pattern type. The output value of a 
BPN is continuous, normally scaled within [0, 1], where 1 means that the data are wholly fitted to 
a specific CCP type and 0 means that the pattern data are not fitted to a CCP type at all. The value 
between 0 and 1 represents the belonging degree of the data to a specific CCP type. This unique 
characteristic was used to tackle the concurrent pattern recognition problem, as more than one 
output value must be analyzed when more than one pattern is sought simultaneously. This 
characteristic was also employed to adjust the Type I error rate of the proposed ensemble classifier 
model using a threshold value. 
In the construction of the c
rs with 30 neurons in the input layer that were used as input data for 24 consecutive 
observations and six statistical features extracted from the 24 observations (Figure 8). The output 
layer comprised four neurons, each of which was used for the natural (random) pattern, together 
13 
 is essentially a machine 
lear
01]. The 
initi
sult herein represents only the performance of the BPN ensemble 
crea
e CR performances between the proposed BPN ensemble model and the 
con
the BPN used, concurrent CCP examples were also included in the training example set here. 
There were 4000 concurrent CCP examples, comprising eight groups of comprehensive concurrent 
cases (Table 2), in the training example set. The parameter settings in Tables 1 and 2 are 
reasonable ranges of magnitudes encountered in practice. In general, patterns smaller than these 
ranges are very difficult to detect. Patterns with parameters larger than these ranges can be 
detected easily and quickly by traditional methods (e.g. control limits). 
The proposed concurrent CCP recognition method in this study
ning-based method. One of the main advantages of a learning-based CCPR method is that it is 
relatively easy to add other types of CCP (e.g. systematic pattern) or to change the parameter 
ranges of the CCP addressed (e.g. by training for shifts with 1.0  s  5.0 rather than 1.0  s  
3.0 as used herein) in the control scheme, by just adding or modifying the relevant CCP examples 
in the training sets. This approach is considered to be easier and more flexible than the 
development of new analytical approaches to various types of CCP (Guh, 2005). With respect to 
practical applications, although the BPN-based classifier ensembles in the proposed model take 
considerable computer time to retrain, the recall process (i.e. recognition) is very fast. This feature 
enables the proposed CCPR model to be applied in an on-line mode, and trained off-line. 
The initial network connection weights were randomly set in the range [–0.01, +0.
al learning rate and momentum factors were set to 0.5 and 0.4, respectively. These two 
parameters decreased gradually during the training process. A hyperbolic tangent function was 
employed as the activation (transfer) function for the hidden and output layers. These training 
parameters were set primarily according to the experiments from the previous study (Guh & 
Tannock, 1999), as well as through simple trail-and-error. They are not necessarily optimal here. 
The connection weights were updated by the Delta-rule (Rumelhart et al., 1986) and convergence 
condition was established when the CR was greater than 0.95. As discussed in Section 2.6, 25 
BPNs were created for each classifier ensemble in this study. Each BPN here was trained using the 
training and testing example sets established through the ensemble methods detailed in Section 
2.3–2.5. The BPNs converged within 15 learning cycles with an average final CR of 0.9663 and 
root-mean-squared (RMS) error of 0.0472. The overall testing result (CR = 0.9517) indicated that 
the training was successful. 
Note that the testing re
ted in the off-line learning stage, in which the BPN ensemble analyzed each testing example 
CCP (fully developed pattern) only once. In this study, the manufacturing process is monitored 
on-line by the BPN-based classifier ensemble model using a moving window recognition approach. 
During the monitoring scheme, once unnatural CCPs start, the patterns will enter into the moving 
window gradually as the process goes on. In this regard, the CCPR models need to recognize 
‘dynamic’ CCPs. BPNs do not have explicit architecture built into the network to deal with 
dynamic patterns, and therefore require an input pattern to be described by a single vector that is 
most representative to the pattern being addressed. In this study, the fully developed patterns were 
used in the training and testing examples of the BPNs (i.e. all the patterns in an observation 
window are complete). However, ANNs have the capability to generalize. For instance, the trained 
CCPR model can detect a trend pattern that occurs at the middle of the monitoring window, 
although the trend pattern always occurs at the beginning of the window in the training examples. 
Hence, the ANN-based CCPR model trained with fully developed patterns can be applied in an 
on-line CCPR scenario. For the ANNs that can represent relationships between dynamic patterns 
in a time sequence using special architecture, interested readers are referred to Guh and Shiue 
(2010), in which the time delay neural network is utilized to improve the performance of an 
on-line CCP recognizer. 
Table 3 compares th
current CCP recognition models in the literature. In Table 3, no CR performances are provided 
in Wang et al. (2009) work. Table 4 indicates that the proposed BPN ensemble model 
outperformed all the concurrent CCP recognition models in the literature, except the Guh and 
Tannock (1999) model. However, it should be noted that the CR performances listed in Table 3 
were evaluated in an off-line recognition scenario, in which the complete CCPs were used to test 
15 
vec
 using Eq. (A1). 
Xi, i = t-23, t-22, …, t-1, t, from the 
Step 4. I, which contains the direct process data (Zt-23, Zt-22, …, 
Step 5. ector VI to the CCP classifier ensemble and obtain an output vector 
Step 6. CP has been detected in the current recognition window 
Step 7.
. Evaluation results of the proposed model 
The performance of the proposed BPN-based CCPR model using classifier ensemble methods 
was
vements in ARLIDX of ensemble BPN approaches for 
on-l
valuation results for different directions of shift (upward and downward) and trend 
(inc
tor from the classifier ensemble is the average of the output vectors obtained from the 25 
component classifiers. The cut-off value () in Step 6 was set as 0.85 in accordance with the 
discussion in section 3.3. 
Step 1. Simulate a process data stream
Step 2. Set t (time of sampling) to 24. 
Step 3. Take the most recent 24 sample observations, 
generated process stream. 
 Generate an input vector V
Zt-1, Zt) and the six features extracted from the current 24 direct process data, using 
Eqs. (A2)–(A9). 
 Present an input v
VO (VO = (O1, O2, O3, O4)). 
 Conclude that an unnatural C
if Oj (j = 2–4)  . Otherwise, increment t by 1, and go to Step 3.  Repeat this 
procedure until all target CCPs are detected. 
 Compute the performance indices. 
 
4
 
 evaluated with 1000 independent simulation runs for each pattern parameter combination 
setting. The average evaluation results for on-line single CCP recognition are summarized in Table 
4. The ARL-related indices here were calculated using the difference between the alarm point and 
the CCP starting point (t = 25). It can be seen from Table 4 that the proposed model using 
enhanced BPN ensemble methods outperformed the single BPN model (Guh & Tannock, 1999) 
with respect to both the ATPRL (recognition speed) and ROT (recognition accuracy) indices. 
Among the three enhanced ensemble methods, ABWD and BWD yielded the highest and lowest 
performances, respectively. The ROT performance of BWD (69.9%) was only slightly better than 
that of the single BPN approach (69.7%). 
Figure 9 plots the performance impro
ine single CCP recognition as a percentage of the performance of the single BPN approach. As 
can be seen, classifier ensemble methods notably improved the ARLIDX performance of the 
on-line BPN-based model for single CCP recognition. Figure 9 also shows that all the enhanced 
ensemble methods performed better than the corresponding typical ensemble methods for on-line 
recognition of the single CCPs, except in the case of BWD for trend patterns. The percentage 
improvement in ARLIDX of BWD was lower by 2.0% than that of Bagging for trend patterns. 
Overall, the enhanced ensemble methods and typical ensemble methods on average improved the 
ARLIDX performance by 26.1% and 23.7%, respectively, compared to the single BPN approach. 
The boosting methods (ABWD, AdaBoost, AWD, and Arcing) were more effective than the 
bagging methods (BWD and Bagging), especially in the recognition of single shift and single trend 
patterns. 
The e
reasing and decreasing) patterns can be observed to be similar in the simulation experiments, 
indicating that the proposed BPN-based CCPR model possesses the directional invariance property 
(Cheng, 1997) for shift and trend pattern recognition. Therefore, in the following evaluation 
scheme for concurrent CCPs, only upward shift and increasing trend patterns were addressed. The 
evaluation results for on-line concurrent CCP recognition are shown in Table 5. Note that the two 
unnatural patterns are assumed to start at the same time (t = 25). For a comprehensive performance 
evaluation, 25 categories of evaluation CCPs with different pattern parameters combinations were 
used for each kind of concurrent CCP. For instance, “s = 1.0, a = 1.0” means a shift pattern with a 
mean displacement of 1.0, concurrent with a cycle pattern having amplitude of 1.0. It can be 
seen in Table 5(a) that the proposed CCPR model using enhanced ensemble methods performed 
17 
ge training strategy proposed in Guh and Tannock (1999) and the boosting 
ense
5. Conclusions 
Individual CCPs that are embedded in a concurrent CCP can be related independently to 
mul
classifier 
ense
search work. (1) Only one type of learning 
Tables 1 and 2). During the learning process of the boosting ensemble method, the weak points (i.e. 
the concurrent CCP examples that were easily wrongly recognized) were marked and re-learned by 
the algorithm itself. 
Both the two-sta
mble method employed in this study have actually applied a similar reinforcement learning 
notion. The difference is that the procedures used in deciding which training examples should be 
re-learned and how many times the selected training examples should be re-learned were 
implemented ‘automatically’ in the boosting ensemble method in this study, while those 
procedures were implemented ‘manually’ in the single BPN model of Guh and Tannock (1999). 
Additionally, instead of using only one BPN, the wrongly recognized CCPs were re-learned by 
different BPNs in the learning procedure of the boosting ensemble methods. The experimental 
results indicated that the boosting ensemble methods were simpler but more efficient and effective 
than the single BPN model for on-line recognition of single and concurrent CCPs. With the 
bagging ensemble method, relatively similar example sets were built for the training of component 
classifiers and therefore component classifiers with similar recognition capabilities were created 
during the ensemble learning process. This may explain why the results of this study show that the 
bagging ensemble method always giving performances that are inferior to those of the boosting 
ensemble method. 
 
 
tiple assignable causes that adversely affect a manufacturing process simultaneously but give 
rise to different unnatural CCPs. Hence, real-time recognition of concurrent CCPs is an important 
issue for accelerating a diagnostic search when a process is out-of-control. However, almost all 
studies in the CCPR field have focused on identification and analysis of single CCPs (e.g. shift or 
cycle patterns). Little attention has been paid to the recognition of concurrent CCPs, especially 
when the manufacturing process is assumed to be monitored in an on-line scenario. Classifier 
ensembles have been examined in many pattern recognition problems. They have shown better 
performances than single classifiers in many fields. However, very few studies have utilized 
ensemble classifiers to recognize CCPs. In this study, classifier ensemble methods were applied to 
build an effective BPN-based model for on-line recognition of concurrent CCPs. Three typical 
classifier ensemble methods (Bagging, AdaBoost, and Arcing) were employed and modified using 
a diversity measure to create useful component classifiers for the proposed CCPR model. 
Numerical results obtained using extensive simulation show that the proposed 
mble model can significantly outperform the single classifier model in the literature for 
on-line recognition of concurrent CCPs with respect to both recognition accuracy and speed. On 
average, the recognition accuracy performance (ROBT) was improved from 56.4% (single BPN) 
to 79.2% (ensemble BPN with boosting methods). The recognition speed performance (ABTPRL) 
was improved from 10.15 (single BPN) to 7.53 (ensemble BPN with boosting methods). The 
improvement effect of the classifier ensemble methods upon the concurrent CCPs was larger than 
that upon the single CCPR. The experimental results also indicate that bagging ensemble methods 
are almost always more accurate than the single BPN approach. Moreover, bagging ensemble 
methods are much less accurate than the boosting ensemble methods. It can be concluded that the 
boosting ensemble method can perform much better than the bagging ensemble method in on-line 
CCPR tasks. The enhanced classifier ensemble methods proposed in this study, by considering the 
diversity measure among the created component classifiers, perform better than the typical 
classifier ensemble methods. By utilizing the enhanced classifier ensemble methods, the 
percentage improvement in ABRLIDX (when compared to the single BPN approach) can be 
further increased by 5.1%, which is the difference between the 34.8% of typical ensemble methods 
and the 39.9% of enhanced ensemble methods. Results from this study confirm the great potential 
of classifier ensemble methods in the field of CCPR. 
There are several useful directions for future re
19 
k                     (A3) 
, 3, …, 49. 
A.4.
MEAN = 
–6.125 + 0.25(k–1)  Yt  –6.125 + 0.25k         Zt = –25 +
Yt  –6.125                                 Zt = –25 
where Zt denotes the process value coded from Yt, and k = 1, 2
 Feature extraction 
n
Z
n
t
t
1                                                          (A4) 
where Zt is the coded process value at time t and n is the number of observed values. n was 
STNDV = 
24 in this study. 
 
n
MEANZ
n
t
t


1
2
                                               (A5) 
SKEW = 
 
3
1
3
nSTNDV
MEANZ
n
t
t


                                                (A6) 
KURT = 
 
34
1
4



nSTNDV
MEANZ
n
t
t
                                               (A7) 
SLP = 
  
 





n
t
n
t
t
tt
MEANZtt
1
2
1                                                (A8) 
where t  denotes the mean of t of the (t, Zt) pairs of sample observations. 
Pearson = 



 





 



 nnn 




2
11
2
2
11
2
111
n
t
t
n
t
t
n
t
n
t
t
t
tt
t
ZZnttn
ZttZn
                             (A9) 
References 
lti-resolution wavelets analysis approach for the recognition of concurrent 
izing the applicability of classification 
redictors. Machine Learning, 24(2), 123-140. 
port 460, Statistics 
stics, 26(3), 801-849. 
rowing based on node 
., Harris, R., & Yao, X. (2005). Diversity creation methods: a survey and 
). Detection of land-cover transitions by combining 
ed ensembles. International 
Al-Assaf, Y. (2005). Mu
control chart patterns. Quality Engineering, 17(1), 11-21. 
Brazdil, P., Gama, J., & Henery, B. (1994). Character
algorithms using meta-level learning. In Machine Learning: ECML-94, Lecture Notes in Computer 
Science, Volume 784, 83-102, 
Breiman, L. (1996a). Bagging p
Breiman, L. (1996b). Bias, variance, and arcing classifiers, Technical Re
Department, University of California, Berkeley, CA. 
Breiman, L. (1998). Arcing classifiers. Annals of Stati
Brodley, C. E. (1995). Automatic selection of split criterion during tree g
selection. Proceeding of the Twelfth International Conference on Machine Learning, 73-80, 
Morgan Kaufmann. 
Brown, G., Wyatt, J
categorisation. Information Fusion, 6(1), 5-20. 
Bruzzone, L., Cossu, R. & Vernazza, G.. (2004
multidate classifiers. Pattern Recognition letters, 25(13), 1491-1500. 
Carney, J. G., & Cunningham, P. (2000). Tuning diversity in bagg
21 
). Improved SPC 
ng rough sets theory and database operations to construct a good ensemble of 
ern recognizer for cyclic data. IIE 
W. (1995). Detecting process non-randomness through a fast and 
Hubele, N. F. (1993), Back-Propagation Pattern Recognizers for X-bar Control 
riate 
ondon: C. 
, J., & Eberhart, R. (1997). A discrete binary version of the particle swarm optimization. 
 Matas, J. (1998). On combining classifiers. IEEE 
(1), 3-16. 
and active learning. 
sion, 
 L. I., & Whitaker, C. J. (2003). Measures of diversity in classifier ensembles and their 
l issues. In Kittlerand, J., 
, A. (2005). Image processing approach to features extraction 
s. New York: McGraw-Hill. 
 of control chart patterns in 
ng, P. (2006). Content-based image retrieval trained by Adaboost 
& Tetsuya, H. C. (2000), Evolutionary ensemble with negative correlation learning. 
 chart patterns recognition using 
 
, R. (2004). Model selection for medical diagnosis decision 
Hassan, A., Shariff Nabi Baksh M., Shaharoun, A. M., & Jamaluddin, H. (2003
chart pattern recognition using statistical features. International Journal of Production Research, 
41(7), 1587-1603. 
Hu, X. (2001). Usi
classifiers for data mining applications. ICDM01, pp. 233-240. 
Hwarng, H. B. (1995). Proper and effective training of a patt
Transactions, 27(6), 746-756. 
Hwarng, H. B., & Chong, C. 
cumulative learning ART-based pattern recognizer. International Journal of Production Research, 
33(7), 1817-1833. 
Hwarng, H. B., & 
Charts: Methodology and Performance. Computers & Industrial Engineering, 24(2), 219-235. 
Hwarng, H. B., & Wang, Y. (2010). Shift detection and source identification in multiva
autocorrelated processes. International Journal of Production Research, 48(3), 835-859 
Kendall, M., & Stuart, A. (1976). The Advanced Theory of Statistics, Volume I, 4th Ed. L
Griffin. 
Kennedy
Proceedings of IEEE International Conference on Computational Cybernetics and Simulation, 
12-15 October, Piscataway, NJ, 4104-4108. 
Kittler, J., Hatef, M., Duin, R. P. W., &
Transactions on Pattern Analysis and Machine Intelligence, 20(3), 226-239. 
Kohonen, T. (1988). An introduction to neural computing. Neural Networks, 1
Kohonen, T. (2001). Self-Organizing Map, 3rd ed. New York: Springer Verlag. 
Krogh, A., & Vedelsby, J. (1995). Neural network ensembles, cross-validation, 
In: Advances in Neural Information Processing Systems, Vol. 7, MIT Press, MA, pp. 231-238. 
Kuncheva, L. I. (2005). Diversity in multiple classifier systems (Editorial). Information Fu
6(1), 3-4. 
Kuncheva,
relationship with ensemble accuracy. Machine Learning, 51(2), 181-207. 
Lam, L. (2000). Classifier combinations: implementations and theoretica
& Roli, F. (Eds.), Multiple Classifiers Systems, Vol. 1857 of Lecture Notes in Computer Science, 
Cagliari, Italy, Springer, pp. 78-86. 
Lavangnananda, K., & Piyatumrong
in classification of control chart patterns. Proceedings of 2005 IEEE Mid-Summer Workshop on 
Soft Computing in Industrial Applications, Espoo, Finland, pp. 85-90. 
Law, A. M., & Kelton, W. D. (1982). Simulation Modeling and Analysi
Leigh, W., Purvis, R., & Ragusa, J. M. (2002). Forecasting the NYSE composite index with 
technical analysis, pattern recognizer, neural networks and genetic algorithm: a case study in 
romantic decision support. Decision Support Systems, 32(4), 361-377. 
Lin, S. Y., Guh, R. S., & Shiue, Y. R. (2011). Effective recognition
autocorrelated data using a support vector machine based approach. Computers & Industrial 
Engineering, 61(4), 1123-1134. 
Lin, H., Kao, Y., Yang, F., & Wa
for mobile application. International Journal of Pattern Recognition and Artificial Intelligence, 
20(4), 525-541. 
Liu, Y., Yao, X., 
IEEE Transaction on Evolutionary Computation, 4(4), 380-387. 
Lu, C. J., Shao, Y. E., & Li, P. H. (2011), Mixture control
independent component analysis and support vector machine. Neurocomputing, 74(1), 1908-1914. 
Maimon, O., & Rokach, L. (2004). Ensemble of decision trees for mining manufacturing data sets.
Machine Engineering, 4(1-2), 32-57. 
Mangiameli, P., West, D., & Rampal
support systems. Decision Support Systems, 36(3), 247-259. 
 -1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
2.0
2.5
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
Sample No.
M
ea
su
re
m
en
ts
 
Fig. 1. A control chart showing concurrent pattern (trend concurrent with cycle). 
 
............
........... ..
...
........... ..
...
............ . . . ........... ..
...
............
ANN classifier 1 ANN classifier 2 ANN classifier k
Input instance
Combining outputs
Final classification
Input instance
Output
Input instance
Output
Input instance
Output
 
Fig. 2. An ANN-based classifier ensemble. 
 
 
 
 
 
 
23 
Algorithm: Arcing with diversity (AWD). The AWD algorithm creates an ensemble of 
models (classifiers) for a learning scheme where each model gives an equally-weighted 
prediction. 
Input: 
 D, a set of h category-labeled training examples; 
 k, the number of models in the ensemble; 
 An ANN learning algorithm (BPN). 
Output: An ensemble classification model, M＊ 
Method: 
(1)    initialize the sampling probability pj for each example in D to 1/h; 
 (2)    for i = 1 to k do // create k models: 
 (3)       if i ≧ 2 do 
 (4)          Calculate sampling probability pj (j = 1 to h) for each example in D; 
 (5)       endif 
(6)       sample D with replacement according to the sampling probability pj of each 
example to obtain Di ; 
 (7)       use training set Di to derive a model, Mi; 
 (8)       if i ≧ 2 do // check diversity contribution: 
 (9)          calculate the ensemble diversity DVtotal(i); 
 (10)         if DVtotal(i) ≦ DVtotal(i-1); 
 (11)         go back to step (6) and try again; 
 (12)      endif 
(13)   endfor 
To use the ensemble model (M＊) on an unseen example, X: 
(1) Let each of the k models classify X ; 
(2) sum up output data vectors from each of the k models; 
(3) return the type with the highest value in the total vector. 
 
Fig. 5. Arcing with diversity algorithm. 
Recognition window 
Fig. 6. The notion of process monitoring by moving window recognition. 
 
25 
....
....
....
30 input neurons
18 hidden neurons
4 output neurons
Input data vector from the 
recognition window
Output data vector for 
pattern type
18 hidden neurons
 
Fig. 8. BPN topology. 
 
 
 
0 10 20 30 40 50 60 70 80 90 100
Average of single CCPs
Single cycle
Single trend
Single shift
Percentage improvements in ARLIDX (%)
BWD
Bagging
ABWD
AdaBoost
AWD
Arcing
 
Fig. 9. Performance improvements in ARLIDX of ensemble BPN approaches for on-line single 
CCP recognition as a percentage of the performance of the single BPN approach (Guh & 
Tannock, 1999). 
 
27 
29 
 
Table 1 
Single CCP training set. 
Pattern type Parameter description Example quantity of each parameter setting Example quantity
Upward shift (US) s = (1.0, 3.0, 0.5) 80 400 
Downward shift (DS) s = (-3.0, -1.0, -0.5) 80 400 
Increasing trend (IT) d = (0.10, 0.26, 0.04) 80 400 
Decreasing trend (DT) d = (-0.10, -0.26, -0.04) 80 400 
Cycle (CY) a = (1.0, 3.0, 0.5) 80 400 
Natural pattern   400 
The notation (a, b, c) corresponds to (initial value, final value, increment). 
 
 
 
 
 
Table 2 
Concurrent CCP training set. 
Pattern type Parameter description Example quantity of each parameter setting Example quantity
US & IT s = (1.0, 3.0, 0.5) d = (0.10, 0.26, 0.04) 
20 500 
US & DT s = (1.0, 3.0, 0.5) d = (-0.10, -0.26, -0.04) 
20 500 
US & CY s = (1.0, 3.0, 0.5) a = (1.0, 3.0, 0.5) 
20 500 
DS & IT s = (-3.0, -1.0, -0.5) d = (0.10, 0.26, 0.04) 20 500 
DS & DT s = (-3.0, -1.0, -0.5) d = (-0.10, -0.26, -0.04) 20 500 
DS & CY s = (-3.0, -1.0, -0.5) a = (1.0, 3.0, 0.5) 20 500 
IT & CY d = (0.10, 0.26, 0.04) a = (1.0, 3.0, 0.5) 
20 500 
DT & CY d = (-0.10, -0.26, -0.04) a = (1.0, 3.0, 0.5) 
20 500 
The notation (a, b, c) corresponds to (initial value, final value, increment). 
 
 
 
 
Table 5(a) 
Evaluation results of on-line concurrent CCP recognition (shift & trend) using enhanced BPN ensemble methods proposed in this study. 
 ABPRL ABTPRL ROBT(%) ABRLIDX 
 Proposed BPN  
ensemble methods 
Proposed BPN  
ensemble methods 
Proposed BPN  
ensemble methods 
Proposed BPN  
ensemble methods 
Shift & Trend BWD ABWD AWD 
Single 
BPN a 
BWD ABWD AWD 
Single 
BPN a 
BWD ABWD AWD 
Single 
BPN a 
BWD ABWD AWD 
Single 
BPN a 
s = 1.0, d = 0.10 12.19 11.32 11.57 12.58 12.19 11.32 11.57 12.58 100.0 100.0 100.0 100.0 12.19 11.32 11.57 12.58 
s = 1.0, d = 0.14 9.89 9.08 9.38 10.33 9.89 9.08 9.38 10.33 100.0 100.0 100.0 100.0 9.89 9.08 9.38 10.33 
s = 1.0, d = 0.18 9.51 8.75 8.96 9.91 9.51 8.75 8.96 9.91 100.0 100.0 100.0 100.0 9.51 8.75 8.96 9.91 
s = 1.0, d = 0.22 8.68 7.92 8.13 9.11 8.68 7.92 8.13 9.11 100.0 100.0 100.0 100.0 8.68 7.92 8.13 9.11 
s = 1.0, d = 0.26 8.15 7.20 7.55 *** 8.15 7.20 7.55 *** 100.0 100.0 100.0 *** 8.15 7.20 7.55 *** 
s = 1.5, d = 0.10 10.82 10.06 10.25 11.19 10.82 10.06 10.25 11.19 100.0 100.0 100.0 100.0 10.82 10.06 10.25 11.19 
s = 1.5, d = 0.14 8.86 8.09 8.23 9.30 8.86 8.09 8.23 9.30 100.0 100.0 100.0 100.0 8.86 8.09 8.23 9.30 
s = 1.5, d = 0.18 9.10 8.25 8.61 9.54 9.10 8.25 8.61 9.54 100.0 100.0 100.0 100.0 9.10 8.25 8.61 9.54 
s = 1.5, d = 0.22 7.56 6.74 7.01 7.95 7.56 6.74 7.01 7.95 100.0 100.0 100.0 100.0 7.56 6.74 7.01 7.95 
s = 1.5, d = 0.26 7.89 6.99 7.23 *** 7.89 6.99 7.23 *** 100.0 100.0 100.0 *** 7.89 6.99 7.23 *** 
s = 2.0, d = 0.10 7.88 7.10 7.26 8.27 7.88 7.10 7.26 8.27 100.0 100.0 100.0 100.0 7.88 7.10 7.26 8.27 
s = 2.0, d = 0.14 7.18 6.39 6.56 7.61 7.18 6.39 6.56 7.61 100.0 100.0 100.0 100.0 7.18 6.39 6.56 7.61 
s = 2.0, d = 0.18 8.52 7.87 8.01 8.95 8.52 7.87 8.01 8.95 100.0 100.0 100.0 100.0 8.52 7.87 8.01 8.95 
s = 2.0, d = 0.22 6.11 5.31 5.44 6.52 6.11 5.31 5.44 6.52 100.0 100.0 100.0 100.0 6.11 5.31 5.44 6.52 
s = 2.0, d = 0.26 6.87 6.00 6.23 *** 6.87 6.00 6.23 *** 100.0 100.0 100.0 *** 6.87 6.00 6.23 *** 
s = 2.5, d = 0.10 8.21 7.37 7.61 8.57 8.19 7.37 7.61 8.57 99.8 100.0 100.0 100.0 8.21 7.37 7.61 8.57 
s = 2.5, d = 0.14 6.88 6.15 6.22 7.28 6.88 6.15 6.22 7.28 100.0 100.0 100.0 100.0 6.88 6.15 6.22 7.28 
s = 2.5, d = 0.18 8.25 7.46 7.67 8.66 8.25 7.46 7.67 8.66 100.0 100.0 100.0 100.0 8.25 7.46 7.67 8.66 
s = 2.5, d = 0.22 7.14 6.20 6.45 7.53 7.14 6.20 6.45 7.53 100.0 100.0 100.0 100.0 7.14 6.20 6.45 7.53 
s = 2.5, d = 0.26 7.37 6.52 6.82 *** 7.37 6.52 6.82 *** 100.0 100.0 100.0 *** 7.37 6.52 6.82 *** 
s = 3.0, d = 0.10 7.99 7.14 7.25 *** 7.96 7.14 7.25 *** 99.6 100.0 100.0 *** 7.99 7.14 7.25 *** 
s = 3.0, d = 0.14 6.51 5.75 5.96 *** 6.51 5.75 5.96 *** 100.0 100.0 100.0 *** 6.51 5.75 5.96 *** 
s = 3.0, d = 0.18 7.97 7.27 7.42 *** 7.97 7.27 7.42 *** 100.0 100.0 100.0 *** 7.97 7.27 7.42 *** 
s = 3.0, d = 0.22 6.53 5.81 5.96 *** 6.53 5.81 5.96 *** 100.0 100.0 100.0 *** 6.53 5.81 5.96 *** 
s = 3.0, d = 0.26 6.64 5.74 5.98 *** 6.64 5.74 5.98 *** 100.0 100.0 100.0 *** 6.64 5.74 5.98 *** 
a From Table 9(a) of Guh and Tannock (1999). 
 
31 
 
Table 5(c) 
Evaluation results of on-line concurrent CCP recognition (shift & cycle) using enhanced BPN ensemble methods proposed in this study. 
 ABPRL ABTPRL ROBT(%) ABRLIDX 
 Proposed BPN  
ensemble methods 
Proposed BPN  
ensemble methods 
Proposed BPN  
ensemble methods 
Proposed BPN  
ensemble methods 
Shift & Cycle BWD ABWD AWD 
Single 
BPN a 
BWD ABWD AWD 
Single 
BPN a 
BWD ABWD AWD 
Single 
BPN a 
BWD ABWD AWD 
Single 
BPN a 
s = 1.0, a = 1.0 13.24 10.58 11.83 14.00 9.23 6.31 8.08 10.00 26.1 52.4 31.7 13.0 35.37 12.03 25.49 76.92 
s = 1.0, a = 1.5 13.26 10.05 11.84 14.00 8.73 6.10 7.38 9.60 68.3 92.5 73.9 56.0 12.79 6.59 9.99 17.14 
s = 1.0, a = 2.0 12.15 9.42 10.88 12.91 10.21 7.34 9.06 11.00 67.6 92.2 76.7 54.0 15.12 7.96 11.82 20.37 
s = 1.0, a = 2.5 8.70 5.86 7.35 9.49 8.59 5.85 7.15 9.34 53.8 82.5 59.3 42.0 15.97 7.09 12.06 22.24 
s = 1.0, a = 3.0 7.10 4.27 5.87 *** 6.90 3.96 5.49 *** 50.3 74.9 58.3 *** 13.72 5.29 9.41 *** 
s = 1.5, a = 1.0 15.19 12.03 13.94 16.00 9.29 6.72 8.22 NA 12.1 37.5 22.5 0.0 76.91 17.89 36.47 NA 
s = 1.5, a = 1.5 11.03 8.09 9.72 11.77 10.56 7.41 9.00 11.25 60.4 84.0 67.6 48.0 17.48 8.83 13.30 23.44 
s = 1.5, a = 2.0 8.80 6.00 7.53 9.65 8.51 5.83 7.04 9.23 56.1 78.9 63.7 43.0 15.18 7.39 11.05 21.47 
s = 1.5, a = 2.5 9.30 6.11 7.79 10.00 7.13 4.32 5.86 8.00 38.4 65.8 46.8 26.0 18.54 6.56 12.52 30.77 
s = 1.5, a = 3.0 9.60 6.44 8.14 *** 6.11 2.92 4.92 *** 35.1 59.2 42.3 *** 17.39 4.94 11.63 *** 
s = 2.0, a = 1.0 13.03 10.22 11.88 13.82 10.25 7.58 8.73 NA 11.7 41.0 20.8 0.0 87.24 18.50 41.98 NA 
s = 2.0, a = 1.5 12.95 9.94 11.38 13.65 10.60 7.37 9.11 11.29 32.7 58.3 43.4 21.0 32.43 12.65 21.01 53.76 
s = 2.0, a = 2.0 7.47 4.46 6.29 8.33 7.36 4.37 6.02 8.05 36.1 62.1 43.3 23.0 20.38 7.03 13.89 35.00 
s = 2.0, a = 2.5 9.35 6.40 7.89 10.07 7.85 4.72 6.45 8.56 33.3 60.1 41.7 20.0 23.60 7.86 15.45 42.80 
s = 2.0, a = 3.0 10.07 6.94 8.93 *** 8.22 5.25 7.15 *** 31.5 53.8 40.3 *** 26.07 9.75 17.77 *** 
s = 2.5, a = 1.0 14.25 11.26 12.72 15.00 8.87 5.82 7.74 NA 10.9 40.6 21.4 0.0 81.64 14.36 36.09 NA 
s = 2.5, a = 1.5 8.86 5.72 7.44 9.58 7.20 4.36 5.98 8.00 25.6 51.7 33.8 13.0 28.09 8.42 17.68 61.54 
s = 2.5, a = 2.0 9.78 6.76 8.34 10.53 8.37 5.70 7.12 9.17 21.2 46.4 33.3 11.0 39.59 12.29 21.34 83.36 
s = 2.5, a = 2.5 7.65 5.22 7.09 8.52 8.19 5.09 7.06 9.00 38.6 65.8 45.0 25.0 21.19 7.74 15.70 36.00 
s = 2.5, a = 3.0 8.44 5.69 7.07 *** 8.12 5.23 6.78 *** 36.7 59.1 45.8 *** 22.13 8.86 14.79 *** 
s = 3.0, a = 1.0 13.36 10.43 11.91 *** 8.28 5.27 7.24 *** 14.2 40.4 26.9 *** 58.16 13.04 26.86 *** 
s = 3.0, a = 1.5 11.91 8.87 10.41 *** 6.56 3.51 5.41 *** 23.7 49.6 30.9 *** 27.65 7.07 17.50 *** 
s = 3.0, a = 2.0 8.86 6.13 7.79 *** 7.77 4.96 6.53 *** 25.1 52.3 34.0 *** 30.98 9.49 19.21 *** 
s = 3.0, a = 2.5 7.88 4.87 6.56 *** 7.50 4.48 6.19 *** 42.5 70.4 53.0 *** 17.64 6.36 11.67 *** 
s = 3.0, a = 3.0 7.46 4.71 6.48 *** 7.40 4.49 6.13 *** 41.4 65.0 49.0 *** 17.88 6.90 12.51 *** 
a From Table 9(c) of Guh and Tannock (1999). 
 
 
 
 
33 
35 
計畫成果自評 
管制圖的異常形狀可被用來尋找引發製程異常的原因(assignable cause)，因此有效且及
時地辦識異常管制圖形狀(control chart pattern，CCP)是統計製程管制(statistical process control, 
SPC)中的重要任務，在辨識 CCP 時所發生的形狀誤判，有時會嚴重地誤導後續框正失控製
程的行動，從而發生不必要的成本損失，因此準確率是 CCP 辨識模式的重要績效指標。從
90 年代以來，有許多學者陸續利用各種機器學習(machine learning)技術辨識 CCP，研究結果
顯示以機器學習為基的 CCP 辨識模式，在準確度及速度上均優於傳統的管制圖，但是，文
獻中幾乎所有以機器學習為基的 CCP 辨識模式均使用單一分類器(single classifier)來辨識
CCP，但是單一(獨立)的辨識器往往不足以學習複雜辨識環境下所需的知識(如辨識同時發生
(concurrent)的 CCP)，因此分類的績效往往不佳。集成式(ensemble)方法模仿人類在作重大決
策前，會先諮詢多位專家意見的行為，其核心原理在於整合多位專家的意見後，所作的決策，
常比只聽單一專家的意見好。本計畫應用集成式分類技術(classifier ensemble)構建以機器學
習為基的混合 CCP 線上辨識與分析模式，同時本計畫也應用分散性(diversity)指標，改良現
有的集成式學習方法，提出三種新型的集成式學習演算法，包括 BWD(Bagging with 
diversity)、ABWD(Adaboost with diversity)及 AWD(Arcing with diversity)，使集成式分類器具
有更佳的 CCP 分辨能力，以解決線上即時 CCP 的形狀誤判問題，而在構建各成份分類器時，
則應用類神經網路(neural network)學習方法，並以簡單的多數決投票法(majority voting)整合
各成份分類器的輸出。本計畫將所發展的以集成式分類技術為基線上即時 CCP 辨識模式之
績效表現，應用模擬與文獻中其他應用單一分類器的 CCP 辨識模式之績效相比較，結果顯
示，集成式分類方法不但能有效地提升以機器學習為基的管制圖混合型態辨識模式的辨識速
度，也可大幅提升辨識模式的辨識精度(準確率)，平均可提升 22.8%的辨識分類準確率及
25.8%的辨識分類速度。本計畫的成果，對欲將此技術應用於監控實際複雜製程的品管人員，
將有極大的助益。 
本計畫研究結果與計畫預期成果完全相符，本計畫的研究成果已寫成三篇期刊論文並
投稿至國際學術期刊，第一篇巳投稿至 Computers & Industrial Engineering 期刊，目前在審
查中，並己修訂(revised)過兩次，第二篇巳投稿至 Applied Soft Computing 期刊，目前在審查
中，並己修訂過一次，第三篇則巳投稿至 Journal of Intelligent Manufacturing 期刊，目前在
審查中；本研究成果並己在四場學術研討會中發表。 
 
100年度專題研究計畫研究成果彙整表 
計畫主持人：顧瑞祥 計畫編號：100-2221-E-150-039-MY2 
計畫名稱：應用集成式分類技術於機器學習方法為基之線上即時製程監控與分析模式之研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 4 4 75% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 2 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 3 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：□已發表 ■未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100字為限） 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500字為限） 
在執行線上(on-line)統計製程管制(statistical process control, SPC)時，正確而即
時地偵測出異常的管制圖形狀(control chart pattern, CCP)，可以幫助品管人員及早發
現製程的異常狀況，而且管制圖上所出現的異常 CCP 通常和某些造成製程失控
(out-of-control)的特定原因(assignable cause)有關，因此正確且迅速地辨識異常 CCP
是統計製程管制(statistical process control)的一個重要課題。文獻中已有許多學者
陸續利用各種機器學習(machine learning)技術辨識 CCP，研究結果顯示以機器學習為基
的 CCP 辨識模式，在準確度及速度上均優於傳統的管制圖，但是，文獻中幾乎所有以機器
學習為基的CCP辨識模式均使用單一分類器(single classifier)來辨識 CCP，但是單一(獨
立)的辨識器不足以學習複雜辨識環境下所需的知識(如辨識混合(concurrent)CCP)，因此
分類的績效往往不佳。集成式(ensemble)方法模仿人類在作重大決策前，會先諮詢多位專
家意見的行為，其原理在於整合多位專家的意見後，所作的決策，通常比單一專家的決策
好。本計畫應用集成式分類技術(classifier ensemble)構建以機器學習為基的混合 CCP
線上辨識與分析模式，同時本計畫也應用分散性(diversity)指標，改良現有的集成式學
習方法，提出三種新型的集成式學習演算法，包括 BWD(Bagging with diversity)、
ABWD(Adaboost with diversity)及 AWD(Arcing with diversity)，使集成式分類器具有
更佳的 CCP 分辨能力，以解決線上即時 CCP 的形狀誤判問題，而在構建各成份分類器時，
則應用類神經網路(neural network)學習方法，並以簡單的多數決投票法(majority 
voting)整合各成份分類器的輸出。本計畫將所發展的以集成式分類技術為基線上即時 CCP
