 I. Introduction 
 
With the popularity of the Internet, multimedia services such as e-learning, 
videoconferencing and video on demand in heterogeneous network frameworks 
become more and more important. However, the tedious content often results in 
video’s emphasis difficulty to understand. To solve this problem, one of the 
methods is to extract a coded video bitstream by a video annotation from video 
content to reduce the complexity of users’ requirements. Video annotation is widely 
used to retrieve and browse content-based video so far. It is still a challenging issue 
to transmit an effective annotating multimedia stream to various receivers on 
networks. The merit of video annotation is to find appropriate region directly from 
the original stream without whole bitstream frame. Moreover, an annotation can 
dynamically adjust the attractive area of video stream without additional auxiliaries 
of the end decoder. Therefore, video annotating technology has become an essential 
component in the system of multimedia content. 
Because video content contains more essential information than other types of 
media, the annotated content can be employed in some fields, such as video 
surveillance systems and entertainment applications. Among the different types of 
video annotation approaches, detecting visual salience in a video is a good way to 
understand the video’s content. Moreover, to detect visual saliency successfully can 
lower the complexity of the video annotation process effectively. Several visual 
attention models have been proposed in the last decade [1-7]. Based on the type of 
attention method, the models can be roughly schooled into two classes: bottom-up 
methods, which extract image using saliency cues; and top-down approaches, 
which extract task-dependent cues. Usually, extracting task-dependent cues needs a 
priori knowledge of the targets. However, a priori knowledge of interesting objects 
is usually difficult to obtain. Therefore, a bottom-up approach is focused in this 
paper. 
 In this paper, a video annotating method is proposed by considering the visual 
attention model to obtain the visual attention region. To do this, the complexity can 
be reduced since we only focus the attention region instead of the whole frame. 
Through extracting the visual attention region of original frame, the complexity is 
reduced to promote the retrieval and browsing efficiency. 
 
A. Visual Attention Model 
 
Contrast value is significant information for human vision. Visual acuity 
measurement in the clinic uses high contrast, such as black letters embedded in a 
white background. Therefore, the relationship between visual acuity and contrast 
allows a more detailed understanding of HVS. In Fig. 1(a), a white cloud is inside 
the black background and obviously is a visual attention region in this picture. This 
phenomenon shows that the color contrast plays an important role in human’s 
perception. Figure 1(b) shows a texture picture and the weak texture area is 
surrounded by strong texture patches. However, the texture objects of similar color 
cannot catch users’ attention. The similar result can also be seen in Fig. 1(c) and the 
complex of shape is also not the major factor for humans’ perception. 
 
(a) (b) (c)  
Figure 1. Contrast behind color, texture and shape perception. 
 
Because high contrast in a frame is rich informative with viewers’ attention, Ma et 
al. presented a typical architecture that consists of visual, aural and linguistic 
attention models to analyze video content for video summarization [13]. By 
employing the visual attention model to measure contrast value, viewers can further 
understand what ROI of frames try to catch. Grounding this concept, the 
characteristic of frames can be captured, creating a faithful representation of the 
significance of frames for the proposed algorithm. The framework of the visual 
attention model includes three parts: 1) color quantization; 2) color space 
transformation; 3) contrast value calculated. The three steps are described in detail 
as follows. 
 
 The XYZ color space is then transformed to the L.u.v. color space by the following 
equations. 
3
' '
' '
116 16
13 ( )
13 ( )
r r
r r
r
r
y y
L
ky y
u L u u
v L v v


 


 
 



,                                        (2) 
where 
' '
' '
0.008856 903.3
4 9
15 3 15 3
4 9
15 3 15 3
r
r
r r
r r
r r r r r r
Y
y k
Y
X Y
u v
X Y Z X Y Z
X Y
u v
X Y Z X Y Z
  
 
   
 
    . 
 
3) Contrast Value Calculation 
The contrast value is calculated pixel by pixel on an image after color quantization 
and color space transformation. If each perception unit contains one pixel, a frame 
of size M N  can be viewed as perceiving field with the corresponding M N  
perception units. Moreover, the contrast value ,i j
C
 can be defined as 
, ,
( , )
i j i j
q
C d p q


,                                            (3) 
where ,
( [0, ], [0, ])
i j
p i M j N 
 and q  indicates the perception unit.   is the 
neighborhood of perception unit ( , )i j  and d  is Eulidean distance between ,i j
p
 
and q . The size of   can control the sensitivity of the perceiving field. The 
smaller the size of   is, the more sensitivity the perceiving field becomes. 
After finding the contrast values ,i j
C
, these values are normalized between 0 and 255 
for the illustration purpose to generate a content-based saliency map. This 
content-based saliency map not only consists of contrast information, but also 
reserves the strength of edge. Meanwhile, the regions which close to the boundary 
of major object have the same or similar contrast values. Therefore, the saliency 
map provides enough information such as color, texture and approximate shape for 
visual attention analysis. Figure 4 shows the result of the content-based saliency 
map. 
 
example of visual attention region extraction. After the visual attention region is 
determined, the region is acquired from the original frame. By comparing the 
original frame and the visual attention region, we can see that the annotated region 
convey the same concept as that of the original frame. 
 
 
Figure 5. Comparison of the original frame and the visual attention region. 
 
As mentioned above, the ROI data can be reduced while only the visual attention 
region is needed. Due to the decrease of the ROI data, the required complexity is 
reduced. To find the area of attended regions, we take the visual attention region 
that corresponds to the maximum entropy as extracted size. Entropy is an indicator 
showing the degree of disorder. When the internal energy of a system is increased, 
its corresponding entropy is increased. Usually, a system will be stable when the 
molecules within the system are uniformly distributed. Meanwhile, the value of 
entropy can reach its maximum.  
In the proposed method, we calculate the S value [16] of entropy function shown in 
Eq. 5. For each motion attention area, the most appropriate scale, S, for each region 
centered at the coordinate (x0, y0) is obtained by 
                                                                (5) 
 where 
                                                                      (6) 
and 
               
                                                                                                           (7) 
 
D is the set that contains the values of visual attention region, which correspond to 
the histogram distribution in a local region r centered at the coordinate (x0, y0) in an  
attention area at time t. Eq. 7 indicates the exponential entropy in a local region r 
and the probability mass function,                 ,is acquired from the histogram of 
pixel values at time t for scale r centered at position (x0, y0)  and the value v, 
In this paper, a video annotating algorithm with entropy optimization is proposed to 
focus the interesting area to fit user’s requirement. The proposed annotating system 
consists of the spatial resolution reduction method via visual attention model 
analysis. The goal of visual attention model analysis is to find a user interesting 
object in images or video sequences. Visual attention region can be extracted from 
the original frame based on visual attention model and the proposed entropy 
function. The attention region contains major object and partial background; 
therefore, the region still can convey the same concept as that of the original frame. 
After extracting the visual attention region, the interesting data can be efficiently 
reduced since we only need to focus the visual attention region. From experimental 
results, we can observe that the visual attention region can obtain good 
performance. 
 
技術特點說明： 
可幫助龐大的視訊資料庫提供精簡的檢索功能，以減少搜尋目標的困難。 
 
可利用之產業及可開發之產品： 
校園安全監控、社區安全監控、園所嬰幼兒行為監控、遠距醫療。 
 
推廣及運用的價值： 
可增加業者推廣本身業外的監控產業的產值、增加監控產業的附加價值或營利、
增加監控產業的投資/設廠、增加就業人數………等。 
 
 
98年度專題研究計畫研究成果彙整表 
計畫主持人：吳明德 計畫編號：98-2622-E-244-006-CC3 
計畫名稱：應用於監控系統中之有效視訊註解技術 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 1 1 100% 件  
技術移轉 
權利金 34380 34380 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 3 3 100% 
人次 
 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
