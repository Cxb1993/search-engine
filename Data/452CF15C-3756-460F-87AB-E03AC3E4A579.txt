 1
 
 
 
 
 
?????????????? 
Probabilistic Algorithms for Solving the Multi-dimensional 
Knapsack Problems 
 
 
???? 
Abstract 
0-1 multidimensional knapsack problem (0-1 MKP) is the problem of finding a subset of 
items that yields maximum profit under several knapsack constraints. It is proven to be NP-hard. 
When the dimension increases, the computing time needed for exact methods increases rapidly. 
In this research, we propose a hybrid genetic algorithm (GA) for solving the large-scale 0-1 MKP. 
We fix a large portion of variables to be 0 or 1 before applying GA which is made up of several 
hybrid operators to make the search more flexible and intelligent. 
     Computational study is conducted on the medium-scale problem sets taken from literature. 
We also test the proposed GA on several sets of randomly generated large-scale problems (up to 
10000 variables and 1000 constraints). Computational results show that: (1) The proposed GA is 
superior to all the GAs so-far designed for the 0-1 MKP. (2) The GAs proposed by Chu and Raidl 
respectively can also be improved significantly in solution quality and computing time if the 
proposed variable reduction procedure is incorporated into their algorithms. (3) The proposed GA 
also outperforms Chu and Raidl’s GAs in which the proposed variable reduction procedure is 
incorporated. 
 
Keywords: 0-1 multidimensional knapsack problem, Probabilistic Algorithms, Greedy algorithm, 
Surrogate constraint, Variable reduction, Genetic algorithms. 
 
 
 
???? 
0-1??????????????????,????????????????,
?????????????????????,?????????????????
??,?????????????????,?????,??????????????
?????,???????????????????? NP-hard,??????????
???????????????????????,?????????,???????
?????????,????????,???????????????,??????
???? 1?? 0,???????(1) Chu? Raidl???????????????,??
?????????????(2)????????????? Chu? Raidl???,???
?????(3)????????????? Chu? Raidl???,???????????
????? 
???: 0-1????,??????,???,Surrogate???,?????,?????? 
 
 3
observation of evidence without regard to theory. For example, Eilon et al. [Eilon 1971] and 
Senju-Toyoda [Senju 1968] considered the efficiency score of item j (denoted by ej) as 
1
( )
j
j
m ij
i
i
c
e a
b=
=
∑
.                            (6) 
Greedy heuristics for the 0-1 MKP have been widely studied because of their simplicity 
and efficiency. In addition, the greedy concept has also been incorporated into metaheuristics for 
the 0-1 MKP in order to improve the solution on hand or handle the infeasible solutions, 
especially in the domain of GAs. For more empirical-based greedy heuristics, see [Koche 1974] 
[Loulo 1979] [Kelle 2004]. 
On the other hand, the theoretical-based efficiency score is established upon theoretical 
results of the 0-1 MKP. For example, Pirkul [Pirku 1987] proposed a primal greedy heuristic 
which makes use of a surrogate relaxation approach and the resulting efficiency score for item j is 
1
jj
SR m
i iji
c
e
s a=
= ∑                             (7) 
where surrogate multipliers si are predetermined by using a descent procedure. Usually, we adopt 
the optimal dual solution of LP relaxation of the 0-1 MKP as the surrogate coefficients (si). The 
main idea of Equation (7) is to transform m constraints into a single one by using surrogate 
multipliers and make use of the ratios as the efficiency scores. For more theoretical-based greedy 
heuristics, see [Frevi 2004] [Kelle 2004]. 
GAs was first proposed by Holland [Holla 1975] and its usual form is given by Goldberg 
[Goldb 1989]. It initializes with a set of individuals called a population. Each individual in the 
population is called a chromosome, standing for a possible solution to the given problem. The 
quality of each chromosome called fitness is evaluated by a fitness function which is usually 
defined based on the objective function of the given problem. The process of creating new 
populations from old ones is called a generation. To form a new population, it uses three 
operators: crossover, mutation and selection. First, crossover operator selects two chromosomes 
to exchange information between them. Then, mutation operator changes some genes of some 
chromosomes in order to introduce new information. After applying these two disruptive 
operations, new chromosomes, called offspring, are produced and should be evaluated again to 
assign proper fitness. A certain portion of offspring are generated through crossover-mutation 
process. Finally, selection operator copies some chromosomes from parents and offspring based 
on their fitness to form a new population. The crossover-mutation-evaluation-selection cycle is 
repeated until the user-specified termination criterion is reached. 
 
3.  A variable reduction procedure 
In order to identify the effective and ineffective items, we adopt the optimal dual solution of 
LP relaxation of the 0-1 MKP as the surrogate coefficients (si). Suppose xj has surrogate ratio 
j
SRe which is defined by equation (7) and without loss of generality, we assume that 
1 2  ... nSR SR SRe e e≥ ≥ ≥ . By the theory of linear program, we have the following properties: (i) the 
items with 1jSRe =  are mainly the basic variables in the optimal solution of the LP-relaxed MKP. 
(ii) the items with 1jSRe ≤  are the nonbasic variables at lower bound in the optimal solution of 
the LP-relaxed MKP. (iii) the items with 1jSRe ≥  are the nonbasic variables at upper bound in the 
optimal solution of the LP-relaxed MKP. So we will divide the items (variables) of a given 0-1 
MKP into three parts: The first part (denoted by P1) includes the items with larger surrogate ratios 
( 1jS Re >  , the second part (P2) is composed of the items with 1jSRe ≅  and the third part (P3) is 
made up of the items with smaller surrogate ratios ( 1jSRe < ). The basic idea is to find two 
positions (CP1 and CP2) to divide {1, 2, ... , n} into three parts such that P1 = { j | j < CP1}, P2 = 
{ j | CP1 ≤ j ≤ CP2} and P3 = { j | j > CP2} . We will let xj =1, j∈ P1 and xj = 0, j∈ P3 before 
executing the proposed genetic algorithm. A procedure for identifying CP1 and CP2 is given as 
 5
(MP repairing operator) [Pirku 1987] and repairing operator based on the ranking information 
of two heuristic algorithms proposed by Senju-Toyoda [Senju 1968] and Kochenberger 
[Koche 1974] (ST&K repairing operator). We draw a random number q. If q ≤ static% (= 
0.75), we use the static ranking information (i.e. MP repairing operator) to repair a solution. 
Otherwise, we use the dynamic ranking information (ST&K repairing operator) to repair a 
solution. 
(7) Selection operator: We form new populations by using a hybrid selection operator which 
hybridizes the roulette wheel selection and the (µ+λ) selection. 
We consider three parameters, pop_size, pc, and pm. Based on the settings in the past studies 
and preliminary computational results, we let pop_size = 0.1n, pc = 0.6 and pm = 0.05. 
 
5.  Computational Results 
     In this section, we compare the proposed GA with the most effective GAs so-far for the 0-1 
MKP which was proposed by Chu and Beasley [Chu 1998] and that by Raidl and Gottlieb [Raidl 
2005]. The computational study is first performed on two medium-scale problem sets (up to 2500 
items and 100 constraints) from literature. Then, in order to better test the effectiveness of our 
algorithm, we generate several sets of large-scale test problems (up to 10000 items and 1000 
constraints) based on the procedure suggested by Gavish and Pirkul [Gavis 1985]. We outline the 
settings of the these algorithms as follows.  
Table 1. Outline of the three genetic algorithms 
Algorithm CGA [Chu 1998] RGA [Raidl 2005] HGA 
Preprocessing None None Variable reduction procedure
Initialization Randomization Based on the optimal solution of LP-relaxed MKP
Based on surrogate 
information 
Crossover Uniform Uniform Fitness-guided 
Mutation Uniform Uniform Heuristic 
Repairing Based on surrogate information 
Based on surrogate 
information 
Hybridize surrogate 
information with greedy 
heuristics 
Selection (µ+λ) selection (µ+λ) selection 
Hybridize roulette wheel 
selection with (µ+λ) 
selection 
Remark: HGA is the proposed GA. 
We code HGA by using C language and develop it on the integrated development 
environment called Dev C++. The LP-relaxed MKP is solved by using the mathematical 
programming solver called LINGO (Version 8.0). For Chu and Raidl’s algorithms, since the 
original codes are not available to us, we code them ourselves based on the descriptions 
mentioned in the corresponding papers. All of the computational study are ran on the PC with 
Pentium 4 2.8G Hz CPU. 
To illustrate the computational results, we define the following notations and performance 
indexes. 
(1)   α: the tightness ratio, which is used to set bi as in Equation (8). 
1
,  1, 2, ... , . ni ijjb a i mα == =∑                      (8) 
(2)   β: the density ratio, which is the percentage of nonzero values in the constraint matrix [aij].  
(3)   z*: the best objective value found. 
(4)   Avg z*: average of z*. 
(5)   lpgap = 100*( LPz − * z )/ LPz (in %) where LPz  is the optimal objective value of the 
LP-relaxed MKP. 
(6)   Avg lpgap: average of lpgap (in %). 
(7)   rgap = 100*( *maxz − * z )/ *maxz (in %) where *maxz  is the highest objective value among the 
 7
As shown in Table 3, HGA also outperforms CGA and RGA in which the variable 
reduction procedure is incorporated. 
We compare HGA with CPLEX and branch and bound algorithm [Osori 2002] in Table 4. 
Table 4. Comparison with CPLEX and branch and bound algorithm [Osori 2002] 
Problem set ORL-0.25 ORL-0.5 ORL-0.75 
Method CPLEX B&B-O HGA CPLEX B&B-O HGA CPLEX B&B-O HGA 
Avg z* 115497  115520  115564 216151 216180 216224 302366  302373  302412 
Remark 1: Each set has 10 problems. 
 2: The version of CPLEX is 6.5.2. The results are from [Osori 2002]. In all of the         
30 problems, it terminated because of reaching 250 MB treesize memory. 
 3: B&B-O is the branch and bound algorithm developed by [Osori 2002]. The         
results are from [Osori 2002]. In all of the 30 problems, it terminated because of 
reaching 10800 seconds solution time (CPU: Pentium III 450 MHz). 
 
5. 2  Medium-scale problems from Hearin center for enterprise science 
The second medium-scale problem set contains 5 largest 0-1 MKP from Hearin center for 
enterprise science (available at http://hces.bus.olemiss.edu/tools.html). Each of them has different 
(n, m) and the largest one contains 2500 items and 100 constraints. They were generated by 
Glover [Glove 1996]. For each problem, we derive its reduced version based on the proposed 
variable reduction procedure (W = 0.15). The resulting problem set is denoted by HCES-R and 
the original one is denoted by HCES. 
Table 5. Experimental results of the three algorithms 
Problem set HCES 
Method CGA RGA HGA 
Avg z* 49484 49717 49727 
Avg lpgap(%) 0.479 0.090 0.067 
Avg rgap(%) 0.412 0.017 0.000 
Avg t*(secs) 2622 1775 3045 
Avg G 517867 507961 1209297
#best 0 0 5 
Remark 1: The running time of each algorithm for each problem is 5400 seconds. 
 2: CGA, RGA solve HCES; HGA solves HCES-R. 
 3: Each set has 5 problems and the average terms are the average of the results from 5 
problems. 
Table 6. Experimental results of the three algorithms for reduced problems 
Problem set HCES-R 
Method CGA RGA HGA 
Avg z* 49694 49719 49727 
Avg lpgap(%) 0.132 0.084 0.067 
Avg rgap(%) 0.065 0.017 0.000 
Avg t*(secs) 794 443 3045 
Avg G 1566768 1564033 1209297 
#best 0 0 5 
Remark 1: The running time of each algorithm for each problem is 5400 seconds. 
 2: Each set has 5 problems and the average terms are the average of the results from 5 
problems. 
 
5. 3  Randomly generated large-scale problems 
 9
 
Table 10. Experimental results of the three algorithms 
Problem set β = 0.25 β = 0.5 β = 0.75 
Method CGA RGA HGA CGA RGA HGA CGA RGA HGA 
Avg z* 680854 681799 683760 835946 836660 837974 993060 993614 994522 
Avg lpgap(%) 1.299 1.162 0.878 0.760 0.675 0.519 0.500 0.445 0.354
Avg rgap(%) 0.425 0.287 0.000 0.242 0.157 0.000 0.147 0.091 0.000
Avg t*(secs) 8395  7402  3500 6823 7317 5640 7342  6613  6536 
Avg G 4525  4438  16571 5031 6193 18024 5008  4947  19081 
#best 0  0  10  0  0  10  0  0  10  
Remark 1: For each problem in these problem sets, n = 5000, m = 500, α = 0.25. 
 2: CGA and RGA solve non-reduced problems. HGA solves reduced problems. 
 
6.  Conclusions 
     In this research, we propose a hybrid GA for solving the large-scale 0-1 MKP. We perform 
a variable reduction procedure before applying GAs. We fix a portion of variables to be 0 or 1 to 
form a reduced 0-1 MKP before the proposed GA is applied.  
The reduced 0-1 MKP has some characteristics which may be hard to be solved by using 
the conventional GAs. Therefore, the proposed hybrid GA is made up of several hybrid operators 
which are used to let the search more flexible and intelligent. To evaluate the effectiveness of the 
proposed GA, we conduct some computational study based on two medium-scale problem sets 
from literature and several randomly generated large-scale problem sets (up to 10000 variables 
and 1000 constraints). 
 
References 
[Alaya 2004] Alaya, I., C. Solnon, and K. Ghedira. (2004). “Ant algorithm for the 
multi-dimensional knapsack problem,” In B. Filipic and J. Silc (eds.), Bioinspired Optimization 
Methods and their Applications, Jozef Stefan Institute, Ljubljana, 63-72. 
[Beasl 1990] Beasley, J.E. (1990). “OR-library: Distributed test problems by electronic mail,” 
Journal of the Operational Research Society 41, 1069–1072 (OR-Library is available at web site 
http://people.brunel.ac.uk/~mastjjb/jeb/info.html). 
[Chu 1998] Chu, P. and J. Beasley. (1998). “A genetic algorithm for the multidimensional 
knapsack problem,” Journal of Heuristics 4, 63–86. 
[Drexe 1988] Drexel, A. (1988). “A simulated annealing approach to the multiconstraint zero-one 
knapsack problem,” Computing 40, 1-8. 
[Eilon 1971] Eilon, S., C.G. Watson, and N. Christofides. (1971). Distribution management: 
mathematical modeling and practical analysis, Hafner, New York. 
[Frevi 2004] Freville, A. (2004). “The multidimensional 0-1 knapsack problem: An overview,” 
European Journal of Operational Research 155, 1-21. 
[Garey 1979] Garey, M.D. and D.S. Johnson (1979). Computers and Intractability: A Guide to 
the Theory of NP-Completeness, W. H. Freeman, San Francisco. 
[Gavis 1985] Gavish, B. and H. Pirkul. (1985). “Efficient algorithms for solving multiconstraint 
zero-one knapsack problems to optimality,” Mathematical Programming 31, 78-105. 
[Glove 1996] Glover, F. and G.A. Kochenberger. (1996). “Critical event tabu search for 
multidimensional knapsack problems,” In I.H. Osman and J.P. Kelly (eds.), Meta-heuristics: 
Theory and Applications, Kluwer Academic, Boston, 407–427. 
[Glove 1997] Glover, F. and M. Laguna. (1997). Tabu Search, Kluwer Academic, Boston. 
[Goldb 1989] Goldberg, D.E. (1989). Genetic algorithms in search, optimization and machine 
learning, Addison-Wesley, New York. 
[Gottl 2000] Gottlieb, J. (2000). “Permutation-based evolutionary algorithms for 
multidimensional knapsack problems,” In J. Carroll., E. Damiani., H. Haddad., and D. 
Oppenheim (eds.), ACM symposium on Applied Computing 2002, ACM Press, New York, 
408-414. 
