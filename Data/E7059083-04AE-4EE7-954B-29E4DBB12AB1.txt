行政院國家科學委員會補助專題研究計畫成果報告 
 
手指指頭掌形識別使用摩門慣性不變特性,訓號強化,幾何位
置特性比對 
 
計畫類別：個別型計畫 
計畫編號： NSC 97-2221-E-212-021 
執行期間： 97 年 8  月 1  日至 98  年  7 月 31  日止 
計畫主持人：蘇慶良 
共同主持人： 
計畫參與人員：廖進德, 楊隆盛, 陳逸夫, 廖唯舒, 劉冠鋐, 李駿清, 張建豪, 詹益嘉, 郭
建宏, 曾偉銓, 賴囿任, 蔡培剛, 洪瑋聖, 馬忠良, 繆宜辰 
 
 
成果報告類型(依經費核定清單規定繳交)：精簡報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
□出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
          
執行單位：大葉大學工業工程與科技管理系 
 
中   華   民   國 98    年  10   月   31   日 
 
 
INFORMATICA, Vol. 18, No. 3, 447–456, May 2007, NSC 
95-2221-E-212-003, EI, SCI (COMPUTER SCIENCE, 
INFORMATION SYSTEMS, 79/92, IF 0.329) 
Ching-Liang Su, “Finger Extraction, Finger Image Automatic 
Registration, and Finger Identification by Image Phase Matching,” 
Applied Mathematics and Computation, Volume 188, Issue 
1, 1 May 2007, Pages 912-923, NSC 95-2221-E-212-003 EI, SCI 
(APPLIED MATHEMATICS 59/150, IF 0.816) 
Ching-Liang Su, “Hand Shape Recognition by Hand Shape 
Scaling, Weight Magnifying and Finger Geometry Comparison,” 
Lecture Notes in Computer Science, Springer, MIRAGE, March 
2007, LNCS 4418, pp. 516–524, NSC 95-2221-E-212-003 EI, SCI 
(impact factor 0.402) 
Ching-Liang Su, “The Extraction and Comparison of Finger 
Edges for Person Identification,” JOURNAL OF INTELLIGENT 
MANUFACTURING, USA, April 2006, pp. 233-241, Vol. 17, No. 
2, NSC 93-2213-E-212-011, EI, SCI (COMPUTER SCIENCE, 
ARTIFICIAL INTELLIGENCE 78/93, IF 0.419) 
Ching-Liang Su, “Original Finger Image Extraction by 
Morphological Technique and Finger image Comparisons for 
Persons’ Identification,” Journal of Intelligent and Robotic 
Systems, Netherlands, January 2006, pp. 1-14, vol. 45, No. 1, NSC 
93-2213-E-212-011, EI, SCI (COMPUTER SCIENCE, ARTIFICIAL 
INTELLIGENCE 75/93, IF 0.459) 
Ching-Liang Su, “Technique for Person’s Identification: Using the 
Extracted Index Finger Image to Identify Individuals,” Journal of 
Intelligent and Robotic Systems, Netherlands, July 2003, pp. 
337-354, vol. 37, No. 3, EI, SCI (COMPUTER SCIENCE, 
ARTIFICIAL INTELLIGENCE 75/93, IF 0.459) 
 
 
 
 
 
 
et al., 2006; Ravikanth et al., 2007] and finger-to-finger seam recognition [Zheng et al., 2007] are 
new approach to recognize hands. In the previous research, palm-prints [Han et al., 2004], iris, 
fingerprints, facial features, and vein patterns [Lin et al., 2004] were also used to identify 
different individuals. Vein patterns [Lin et al., 2004; Wang et al., 2008; Zhao et al., 2007] are 
susceptible to body temperature and are unreliable to identify the hands. 
In this study, the new technique [Su, 2003] is used to extract: thumb, index, middle, ring, and 
small fingers and to perform finger identification. The geometrical descriptor is used to transfer 
geometrical features of the finger to another feature domain for image comparison. For reducing 
the number of files in the system, all the fingers of one person are placed in one file. Image 
subtraction is used to examine the difference of the two fingers. The hand is fixed each time when 
photographing and it is assumed that each time when the hand image is taken, it presents the 
same shape as the previous acquired ones; subsequently, after the fingers are extracted, they can 
be used to identify different persons. This report consists of four sections. Section 2 acquires the 
hand image; section 3 extracts the finger images; section 4 describes the geometrical descriptor 
and image subtraction. section 5 concludes this report. 
 
2. Acquiring hand image. 
 
Figure 2.1 shows the acquired hand images. After further processing, one can obtain the 
hand-edge images. The hand-edge images are shown in figure 2.2. Figure 2.3 shows the geometry 
features of the hand image. By the edge thinning direction and the hand geometry features, one 
can extract the fingertips and finger-valleys. The results are shown in figure 2.4.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2.1: The acquired hand images.
Figure 2.2: The hand-edge images.
taken for each group. To each group, one hundred and five comparisons are conducted to find the 
accurate-identification-rate of the developed identification algorithm. Within those one hundred 
and five comparisons, fifteen comparisons are conducted for self-comparison – since one person 
taken three different photos. The data of these fifteen comparisons are shown inside the 
rectangular boxes. The other ninety comparisons are conducted for comparisons between two 
different sources. Figure 5.1 shows parts of the data. The comparisons of the same person’s finger 
images yield much less values; however, the comparisons of the different person’s finger images 
yield much greater values. For the first group test, the error rate for the index finger is 22%, the 
middle finger is 9%, and the ring finger is 11%. For the second group, the error rate for the index 
finger is 35%, the middle finger is 29%, and the ring finger is 15%. 
 
 
 
 
 
 
 
 
Figure 3.1: The extracted fingers. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
∑∑ ⋅∑∑ ⋅
= == =
−= 128
1
128
1
2128
1
128
1
2 )()( tantan2
j i
ij
j i
ij WeightjWeighti cediscedisVar
2
1
21 22
1
VarVar
Sin Var
ObjO
+=
−
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
+−
+
=
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
1
cossin
sincos
1
θθ
θθ
differencedifference
differencedifference
f
f
ji
ji
j
i
Weightji ijcediscedis
j i
Var ⋅⋅⋅= ∑∑
= = tan
tan
128
1
128
1
21 4.1
4.3
4.2
4.4
References 
1. Gholamreza Amayeh, George Bebis, Ali Erol, and Mircea Nicolescu, “A Component-Based Approach to Hand 
Verification,” IEEE conference, 2007  
2. Miguel A. Ferrer, Carlos M. Travieso, and Jesus B. Alouso, “Using hand knuckle texture for biometric identifications,”
IEEE A&E Systems Magazine, June 2006 
3. Chin-Chuan Han, “A hand-based personal authentication using a coarse-to-fine strategy,” Image and Vision Computing,
Volume 22, 2004, pp 909–918 
4. 
 
D. G. Joshi, Y. V. Rao, S. Kar, V. Kumar, “Computer vision based approach to personal identification using finger crease 
pattern,” Pattern Recognition, 31, 1998, pp 15-22  
5. Chih-Lung Lin and Kuo-Chin Fan, “Biometric Verification Using Thermal Images of Palm-Dorsa Vein Patterns,” 
IEEE Trans Circuits System Video Tech, Vol. 14, No. 2, 2004  
6. YingLiang Ma, Frank Pollick and W. Terry Hewitt, “Using B-Spline Curves for Hand Recognition,” Proceedings of the 
17th International Conference on Pattern Recognition (ICPR’04), 2004 
7 Sotiris Malassiotis, Niki Aifanti, and Michael G. Strintzis, “Personal Authentication Using 3-D Finger Geometry,” IEEE 
Transaction on Information Forensics and Security, Vol. 1, No. 1, 2006 
8. Aya Mitome and Rokuya Ishii, “A Comparison of Hand Shape Recognition Algorithms,” The 29th Annual Conference 
of the IEEE Industrial Electronics Society, Nov 2-6 2003 
9. 
 
Cenker Oden, Aytul Ercil, and Burak Buke, “Combining implicit polynomials and geometric features for hand 
recognition,” Pattern Recognition Letter, Volume 24, Issue 13, September 2003, Pages 2145-2152 
10. Ch. Ravikanth and Ajay Kumar, “Biometric Authentication using Finger-back Surface,” IEEE Conference, 2007  
11. 
 
R. Sanchez-Reillo, C. Sanchez-Avila, and A. Gonzales-Marcos, “Biometric identification through hand geometry
measurements,” IEEE Trans Pattern Anal Mach Intell, Volume 22, No. 10, 2000, Pages 1168-1171 
12. Tadej Savic and Nikola Pavesic, “Personal recognition based on an image of the palmar surface of the hand,” Pattern 
Recognition 40 (2007) 3152 – 3163  
13. Ching-Liang Su, “Technique for Person’s Identification: Using the Extracted Index Finger Image to Identify 
Individuals,” Journal of Intelligent and Robotic Systems, Vol. 37, No. 3, 2003, pp. 337-354  
14. Lingyu Wang, Graham Leedham, and David Siu-Yeung Cho, “Minutiae feature analysis for infrared hand vein pattern 
biometrics,” Pattern Recognition 41 (2008) 920 – 929 
15. Wei Xionga, Kar-Ann Toha, Wei-Yun Yaua, and Xudong Jiangb, “Model-guided deformable hand shape recognition 
without positioning aids,” Pattern Recognition, Volume 38, 2005, Pages 1651 –1664 
16. Wei Xiong, Changsheng Xu, and Sim Heng Ong, “Peg-free Human Hand Shape Analysis and Recognition,” ICASSP, 
2005, Pages 77 –80 
17. Xiaoming Yin and Ming Xie, “Finger identification and hand posture recognition for human–robot interaction,” Image 
and Vision Computing 25 (2007) 1291–1300 [Hand finger  extraction and identification, Error find the finger edge]
18. Erdem Yoruk, Helin Dutagaci, and Bulent Sankur, “Hand biometrics,” Image and Vision Computing 24 (2006) 483–497
19. Erdem Yörük, Ender Konuko˘glu, and Bülent Sankur,, “Shape-based hand recognition,” IEEE Transaction on Image 
Processing,, Vol. 15, No. 7, 2006 
20. Shi Zhao, Yiding Wang and Yunhong Wang, “Extracting Hand Vein Patterns from Low-Quality Images: A New 
Biometric Technique Using Low-Cost Devices,” IEEE Fourth International Conference on Image and Graphics, 2007, 
pp 667-671 
21. Gang Zheng, Chia-Jiu Wang, and Terrance E. Boult, “Application of Projective Invariants in Hand Geometry 
Biometrics,” IEEE Transaction on Information Forensics and Security, 2007, Pages 1–11 
234 J Intell Manuf (2006) 17:233–241
Fig. 1 The designed shelves for
taking the hand images
Fig. 2 Obtain the thinned-edges
of the hand images
Extract the finger images
Figure 1 shows the shelves for taking the hand-images. The
illuminations are from the left lateral and the bottom of the
shelves. By adjusting the lights one can control the illumina-
tions to the hand-image. In the middle shelf, several pegs are
used to peg the person’s hand to a certain position. This will
make the hand inertial when the hand-image is taken. Figure 2
shows the hand images and the extracted hand-edge images.
Figure 3 shows the position-features of the fingertips and fin-
ger-to-finger-valleys. By using these position-features, one
can extract the entire—thumb, index, middle, ring, and small
fingers. The entire images are shown in Figs. 4, 5, and 6. The
finger extracting technique is shown in reference Su (2003).
Geometry descriptor
For every pixel (i, j) in the object, after the transformation
of the geometry descriptor, the final destination of (i, j) is
(i f , j f ). The geometry descriptor will transfer and interpo-
late the geometry features of every pixel inside the object
to another feature-domain. The original geometry feature of
the finger image will be preserved, after the object is trans-
fer to the new feature-domain. In the new feature-domain,
every object will have the same straight orientation and every
object’s centroid is aligned to position (64, 64). Since every
Fig. 3 Features of the fingertips and finger-to-finger-valleys
object has the same orientation and every object is aligned
to the same centroid, one can perform the image subtrac-
tion to perform the person’s identification. Equation 1 shows
the function of the geometry descriptor. In Eq. 1, “θ” repre-
sents the object’s orientation and “Centroid” represents the
object’s centroid. By using Eqs. 2–4, one can find the param-
eter θ . In Eqs. 2 and 3, idistance represents the i—distance
of a specific pixel g(i, j) to the object’s centroid, jdistance
represents the j—distance of a specific pixel g(i, j) to the
object’s centroid, and Weighti j represents the weight of a
specific pixel g(i, j).
236 J Intell Manuf (2006) 17:233–241
Fig. 9 The result of the
finger-edge after performing the
geometry descriptor
Fig. 10 The result of the
finger-edge after performing the
geometry descriptor
Fig. 11 The result of the
finger-edge after performing the
geometry descriptor
(i, j, θ, centroid) →
Geometry
Descriptor
(i f , j f ) (1)
Var1 =
N∑
j=1
N∑
i=1
(i−distance)2 · Weighti j (2)
Var1 =
N∑
j=1
N∑
i=1
2 · idistance · jdistance · Weighti j (3)
θ =
Sin − 1
∑N
j=1
∑N
i=1 2·idistance· jdistance·Weighti j√
Var22+Var12
2
(4)
Hybrid geometry signals and image subtraction
Figures 12 and 13 show the hybrid geometry signals. The
hybrid geometry signals are the composed signals of—the
thumb, index, middle, ring, and small fingers. The signals
of—the thumb, index, middle, ring, and small fingers are
added together to generate the hybrid geometry signals. Since
the separate finger cannot perform well to perform the per-
son’s identification, the hybrid geometry signals would pos-
sess more fingers’ geometry features to perform the person’s
identification. Finger 14 shows the various hybrid geometry
signals. As mentioned earlier, because of the illumination and
the fingers stretch at different positions, one cannot precisely
extract the fingertip and finger-root. For coping these errors,
the images are fine-tuning to different positions to obtain the
better recognition rate—during the image subtraction stage.
238 J Intell Manuf (2006) 17:233–241
Fig. 16 Sub-pattern template convolution to find the minimum pixel
value
First, the image is shifted at different positions—to which
are—left four pixels, left three pixels, left two pixels, . . .
etc, right four pixels. Also, the image is shifted—top four
pixels, top three pixels, top two pixels, . . . etc, bottom four
pixels. When the images are fine-tuning to one certain po-
sition, the image is also fine-tuning to different orientations
for comparison. The rotated degrees are—counter-clockwise
2 degrees, counter-clockwise 0.8 degrees, counter-clockwise
0.6 degrees, . . .etc, to clockwise 1.8 degrees. Figure 15 shows
the rotated-images, which are rotated from −2.0 degrees to
+1.8 degrees.
After the image rotation, the software will perform the im-
age subtraction. The image subtractions are shown in
Figs. 16 and 17 In the top of Fig. 16, one can find a 5 by 5 sub-
pattern template. This 5 by 5 sub-pattern template will extract
the sub-pattern images of two persons’ geometry hybrid sig-
nals and, consequently, the image convolution is performed
to these two 5 by 5 sub-pattern template images. Fine-tuning
of the image is also performed in this stage and the min-
imum value of the convolution results will be recorded in
the resultant-image. The subtract result image is shown in
the right-hand side picture of Fig. 16. By calculating the
difference of the subtracted-result, one can recognize differ-
ent fingers.
Results and conclusions
In Fig. 17, one can find the original hand-edge images and
the hybrid finger geometry signals. The image subtraction
is performed to the hybrid finger geometry signals and the
results are shown in the right-hand side pictures of the Fig.
17. Figure 17 also shows the value of the difference of the
two subtracted-images. Figure 17 shows four different cases
of image subtractions. All these four cases involve the im-
age subtraction of one person’s hand-image subtracting to
the same person another hand-image. After performing the
image subtraction, the differences of these four cases are—
886850, 825250, 763852, and 834800. Figures 18 and 19
show the partial comparison data of the image difference after
performing the image subtraction. The data inside the rectan-
gular boxes in Figs. 18 and 19 are the image subtraction-result
of one person’s hand-image subtracting to the same person’s
another hand image. The other data, which is not circled
by the rectangular boxes, show the difference between two
different persons’ hand images subtractions. In Fig. 20, one
can find that the error range of the subtracted value of differ-
ent person comparison are 834725–674625. When conduct-
ing one person’s hand-image subtracting to the same person
another hand-image, the error range of the subtracted-value
are 1135550– 834800. In this research, 57 persons’ hand
images are taken. Each person takes three different hand im-
ages. As shown in Fig. 1, the illuminations are adjusted to
provide different illuminations to each hand image. After the
hand images are taken, several problems are found in the
hand images—the middle finger and the ring finger of sev-
eral people hands are stick together or the thumb finger of one
person’s hand-image are stretching too wide. The algorithm
developed in this research cannot correctly extract the middle
and ring fingers correctly when the middle and ring fingers
are sticking together and the algorithm will not extract the
wider-stretched thumb finger either. Since these two prob-
lems will cause the algorithm unable to extract the thumb,
middle, and ring fingers correctly, several images are dis-
carded from the test. Actually, the hand-photo-images used
in this system are only 120, which are belonged to 40 persons.
Regard these 120 photos as whole and run these 120 pho-
tos in one batch—which will take a very long time to get
the job done. Due to the software glitch, the running process
might break before the job completed. Consequently, one
might endlessly run and run the procedure again and again
and every time from the square one. Furthermore, the data-
base might have no enough memory to accommodate the
entire obtained-data. Thus, in this system, five persons are
designated as one group, i.e., five people are regarded as a
group and these five persons are tested in one step. Since one
person has been taken three different photos, totally 15 photos
are tested in each step. As mentioned earlier, in this system,
there are 40 persons participating the test. Thus, the system
totally will run eight different batches. Totally, 120 hand-
photos are tested. For each hand-photo-image, the following
steps are performed (1) the hand-edge is found, (2) the fin-
gers are extracted separately, (3) geometry descriptor is per-
formed to each finger, (4) mixed-finger-signal is generated,
(5) image shifting and rotation are performed, (6) genuine
and imposter comparisons are conducted. To each group, one
photo is compared against the other 14 photos. Totally, 105
comparisons are conducted to test the accuracy of the devel-
oped identification algorithm. Within those 105 comparisons,
15 comparisons are conducted for genuine-comparison, the
comparisons of the same person’s fingers — since one person
taken three different photos. The other ninety comparisons
are conducted for imposter-comparisons — the comparisons
of different person’s fingers. In this research, there are eight
different test batches. Thus, there are 120 genuine tests and
720 imposter tests. Figure 21 shows the comparison results
240 J Intell Manuf (2006) 17:233–241
Fig. 19 The comparison-data
Range ofthe
subtracted-value
Error range of
the subtracted-
value
Error
(Times)
Error
rate
Different person
comparison
Same person
comparison
2022075
~
674625
1135550
~
544150
834725
~
674625
113550
  ~
834825
123
12
840:123
840:12
120 different photographs; 840 comparisons; 834800 asthethreshold
Fig. 20 Experimental data
Fig. 21 Response character of
genuine and imposter
comparisons-120 different
photographs; 840 comparisons;
834800 as the threshold
Subtracted-Value
2022075
1135550
544150
834800
834725
674625
Threshold Threshold
Correct
(Times)
597
Error
(Times)
123
Imposter Comparison
Error
(Times)
12
Correct
(Times)
108
Imposter Comparison
(Total Times)
720
Genuine Comparison
(Total Times)
120
Genuine Comparison
Original Finger Image Extraction by Morphological
Technique and Finger Image Comparisons for
Persons’ Identification
j
CHING-LIANG SU
Department of Industrial Engineering & Technology Management, Da Yeh University,
112 Shan-Jeau Road, Da-Tsuen, Chang-Hua, Taiwan 51505; e-mail: cls2@mail.dyu.edu.tw
(Received: 14 August 2004; in final form: 16 June 2005)
Abstract. This research uses the object extracting technique to extract the index, middle and ring
fingers from the hand images. The algorithm developed in this research can find the precise
locations of the different fingers’ fingertips and the finger-to-finger-valleys. After finding the
positions of the fingertips and finger-valleys, the index, middle and ring fingers can be extracted
from the hand images by using morphological technique. The extracted index, middle and ring
fingers contain many useful geometry features. One can use these features to do the person’s
identification. The orientations of the index, middle and ring fingers are found in this research.
Image rotating, image shifting, and image interpolating techniques are used to align different
persons’ index, middle and ring fingers. Image subtraction is used to exam the difference of two
index, middle and ring finger images. In this research so far only use the index, middle and the ring
fingers as the features to identify different persons.
Key words: dilating, finger shape extraction, finger shape identification, morphology, object curve
comparison.
1. Introduction
In the past 20 years, researchers invested a lot of effort to develop different
techniques to identify the hand images. This past work includes Y hand geometry
[2, 5, 12, 16], middle finger crease pattern matching [6], various finger size
measurements [7, 14], various finger lateral view size measurements [14], vein
pattern [9], eigenpalm [10], implicit polynomials [13], algebraic invariants [13],
Karen invariant computation [13], line interception and slope comparisons [17],
control point selection [8, 17], coarse to fine strategy [3], B-Spline [11], wa-
tershed transform [9], HMM [16]; However, some are very sensitive to the noise
[6, 17]; some have very complicated mathematical models [13] and some have
very complicated neural training algorithms. In this research, the new technique
is used to extract the index, middle and the ring fingers to do the person’s
identification. The orientations of the index, middle and ring fingers are cal-
j This work was supported by National Science Council under grant NSC 93-2213-E-212-011.
Journal of Intelligent and Robotic Systems (2006) 45: 1Y14 # Springer 2006
DOI: 10.1007/s10846-005-9007-3
are fixed with interval of five pixels. When A is moved, the distance BC between
points B and C is calculated. Consequently, various distances BC of points B
and C will be found. By checking the center picture in Figure 1, one can
find the distances BC are less than 13 Y since A is located in the positions around
the finger-to-finger-valley. Various distances BC are compared; the point A with
the minimum distance of BC will represent the index-middle-fingers-valley. The
center and right-hand-side pictures in Figure 1 are extracted from Microsoft
Access database. The center picture shows how to find the index-middle-fingers-
valley. The right-hand-side picture shows how to find the middle-ring-fingers-
valley.
In this research, the finger-ends are used to identify different persons. Figure 2
shows by using the finger root Y (i.e., finger-to-finger-valley) as the beginning
searching point and performing the bottomYup searching, one can obtain the dif-
ferent fingers’ finger-ends. When the algorithm performs the bottom-up searching
and when the algorithm detects the straight finger edge, the algorithm will mark
this point as the extracting point Y which late the algorithm will use this point to
extract finger-ends. In this research, when the distance BC is the greater than 20,
the algorithm will mark this point as the beginning point of the straight-finger-
edge. As contrast to Figure 1, one can find that when the finger edge is straight,
the distances BC between points B and C would be greater than those distances
of BC Y when A is located in the positions of the bending finger edge in
Figure 1. The center and right-hand-side pictures in Figure 2 are also extracted
from the Microsoft Access database. By using the beginning points, which are
Figure 2. Find the beginning points with straight finger edges.
FINGER IMAGE EXTRACTION BY MORPHOLOGICAL TECHNIQUE AND FINGER IMAGE 3
The extracted middle finger images are shown in Figures 5Y7, respectively. Di-
lating circles with radii 2, 3, and 4 generate the results of Figures 5Y7, re-
spectively. The error rates for Figures 5Y7 are 4.7%, 2.8%, and 4.7%. The error
rates are calculated by using the misidentification cases divided by the total test
cases. Since the dilating circle with radius 3 generates more accurate result, the
circle with radius 3 is used in this research to extract the original finger images.
Figure 5. The extracted middle finger images with dilating radius 2.
Figure 6. The extracted middle finger images with dilating radius 3.
Figure 7. The extracted middle finger images with dilating radius 4.
Figure 8. The extracted ring finger images with dilating radius 3.
Figure 9. The extracted index finger images with dilating radius 3.
FINGER IMAGE EXTRACTION BY MORPHOLOGICAL TECHNIQUE AND FINGER IMAGE 5
the second group, the error rate for the original middle finger is 2.8%, the
original ring finger is 5.7%, and the original index finger is 1.9%. In the previous
research [15], the middle, ring, and index finger edges are extracted from
different persons for conduct the persons’ identification too. The subtracted-re-
sults of the middle, ring, and index finger edges are shown in Figures 13Y15,
Figure 11. The subtracted-results of different persons’ ring-finger images.
Figure 12. The subtracted-results of different persons’ index-finger images.
FINGER IMAGE EXTRACTION BY MORPHOLOGICAL TECHNIQUE AND FINGER IMAGE 7
By the experimental test, one can conclude that the original finger images
extracted by the dilated finger edges will produce the more accurate identification
rates.
4. Fine-tuning the Extracted Images to Increase the
Identification-Accuracy-rates and Speed Up the
Identification Process
For one specific person, one should expect one unique shape of the finger image
is extracted from the person’s different hand photos. However, the illumination
Figure 15. The subtracted-results of different persons’ index-finger images.
Figure 16. Shift the image to different positions for comparison.
FINGER IMAGE EXTRACTION BY MORPHOLOGICAL TECHNIQUE AND FINGER IMAGE 9
the topYbottom position shifting are top two pixel, top one pixel, no move,
bottom one pixel, and bottom two pixels. The shifting positions are shown in
Figure 16.
When the images are shifted at one certain position, the image is also rotated
to different orientations for comparison. The rotated degrees are counter-clock-
wise 2-, counter-clock-wise 0.8-, counter-clock-wise 0.6-. . . clock-wise 1.8-. The
image rotating figures are shown in Figure 17. After this fine-tuning, the
misidentification rate is drop from 30% to 20%, for the pure thinned middle
finger edge test in Figure 13.
As mentioned in Section 3, in this research, totally 10 persons participate
in the experimental test. In each group, totally 15 photos are taken. For each
group, totally 105 comparisons are required for each photo to be compared to the
other 14 remaining photos. Figure 18(a) shows 225 comparisons, which contains
120 trivial comparisons Y (since C2
15 is equal to 105). Figure 18(b) shows the
essential 105 comparisons. The algorithm, developed in this research, only
perform the 105 essential comparisons and the other non-essential comparisons
are skipped.
As checking Figure 19(a), one can find that the extracted middle finger image
only occupies a small portion of the image. In order to speed up the comparison
process, the algorithm only needs to compare the useful image to identify the
person. The useful image range Y Imin, Imax, Jmin, and Jmax are found in this
research and are saved in the end of the image’s original BMP file. The content
of the BMP file with saved Y Imin, Imax, Jmin, and Jmax is shown in Figure 19(b).
Figure 19. Save the image range into the BMP file. (a) The useful image occupies only a
small portion of the image. (b) The BMP file records the image’s range Y Imin, Imax, Jmin, and
Jmax.
FINGER IMAGE EXTRACTION BY MORPHOLOGICAL TECHNIQUE AND FINGER IMAGE 11
respectively, the algorithm will locate the beginning points, which begin with the
straight-finger-edges. The picture is shown in Figure 21(b). By further pro-
cessing, the point with minimum distance near the thumb-index-fingers-valley
will be located. This point will be treated as the thumb-index-fingers-valley. The
result is shown in Figure 21(c).
6. Results and Conclusions
As mentioned in Section 3, the original finger images extracted by the dilated
finger edges will produce more accurate results than the pure thinned finger
edges. If one can more effectively control the zoom-in and zoom-out effect of the
camera lens and if one also can control the illumination well when one person’s
hand image is taken, the algorithm might reduce the misidentification error to
below 2%. In order to cope the above lens and illumination problems, one might
consider the 3-D laser scanner to acquire the hand images.
References
1. Editorial: Hand-based biometrics, Biom. Technol. Today 11(7) (July 2003), 9Y11.
2. Egiazarian, K. O. and Pestana, S. G.: Hand shape identification using neural networks, SPIE
4667 (2002), 440Y448.
Figure 21. Find the thumb-index-fingers-valley. (a) Find the finger tips. (b). Locate the
beginning points, which begin with the straight edges. (c) Find the point with the minimum
distance, which will be treated as the thumb-ndex-fingers-valley.
FINGER IMAGE EXTRACTION BY MORPHOLOGICAL TECHNIQUE AND FINGER IMAGE 13
Hand image recognition by the techniques of hand shape scaling
and image weight scaling
Ching-Liang Su *
Department of Industrial Engineering and Technology Management, Da Yeh University, 112 Shan-Jeau Road, Da-Tsuen, Chang-Hua 51505, Taiwan
Abstract
This research uses the object extracting technique to extract the – thumb, index, middle, ring, and small ﬁngers from the hand images.
The algorithm developed in this research can ﬁnd the precise locations of the ﬁngertips and the ﬁnger-to-ﬁnger-valleys. The extracted
ﬁngers contain many useful geometry features. One can use these features to do the person identiﬁcation. The geometry descriptor is
used to transfer geometry features of these ﬁnger images to another feature-domain for image-comparison. Image is scaled to make
the ﬁnger image has more salient feature. Image is also magnifying by the basis of distance-multiplying-pixel-gray-level. After the image
magnifying, the ﬁnger-image will possess more salient feature. Image subtraction is used to exam the diﬀerence of the two images.
 2007 Elsevier Ltd. All rights reserved.
Keywords: Finger shape information extraction; Finger shape computation and identiﬁcation
1. Introduction
In the past 20 years, researchers invested a lot of eﬀort
to develop diﬀerent techniques to identify the hand images.
This past work includes – hand geometry (Editorial, 2003;
Egiazarian & Gonzalez Pestana, 2002; Han, Cheng, Lin, &
Fan, 2003; He, Qiu, & Sun, 2002; Mitome & Ishii, 2003; Su,
2003; Sun & Qiu, 2004; Xionga, Toha, Yaua, & Jiangb,
2005; Xiong, Xu, & Ong, 2005), middle ﬁnger crease
pattern matching (Joshi, Rao, Kar, & Kumar, 1998), vari-
ous ﬁnger size measurements (Kumar, Wong, & Shen,
2003; Sanchez-Reillo, Sanchez-Avila, & Gonzales-Marcos,
2000), various ﬁnger lateral view size measurements (San-
chez-Reillo et al., 2000), vein pattern (Lin & Fan, 2004),
eigenpalm (Lua, Zhang, & Wanga, 2003), implicit polyno-
mials (Oden, Ercil, & Buke, 2003), algebraic invariants
(Oden et al., 2003), Karen invariant computation (Oden
et al., 2003), line interception and slope comparisons
(You, Li, & Zhang, 2002), control point selection (Lia,
Zhang, & Xub, 2003; You et al., 2002), coarse to ﬁne strat-
egy (Han, 2004), B-Spline (Ma, Pollick, & Terry Hewitt,
2004), watershed transform (Lin & Fan, 2004), HMM
(Sun & Qiu, 2004); however, some are very sensitive to
the noise (Joshi et al., 1998; You et al., 2002); some have
very complicated mathematical models (Oden et al.,
2003) and some have very complicated neural training
algorithms.
In the previous research, when performing the hand
geometry matching (Editorial, 2003; Egiazarian & Gonz-
alez Pestana, 2002; Han et al., 2003; He, Bing, Qiu,
Zheng-Ding, & Sun, 2002; Mitome & Ishii, 2003; Su,
2003; Sun & Qiu, 2004; Xionga, Toha et al., 2005; Xiong,
Xu et al., 2005), in order to recognize the hand image, every
time the hand needs to place in a precise certain ﬁxed posi-
tion – thus the camera can capture the same hand image. In
this research the hand needs not to place in a precise certain
ﬁxed position. Wavelet technique is used to ﬁnd the ﬁnger-
to-ﬁnger valley in the past research (Han, 2004; Han et al.,
2003). In this research, the developed recognition algorithm
automatically calculate and check the ﬁnger-edge energy
response signals and selected the high energy response sig-
nals to ﬁnd the ﬁnger-to-ﬁnger valleys of the hand image
automatically. The algorithm developed in this research
ﬁnds the ﬁnger-to-ﬁnger valleys more accurately and more
0957-4174/$ - see front matter  2007 Elsevier Ltd. All rights reserved.
doi:10.1016/j.eswa.2007.05.040
* Tel.: +886 4 851 1888x4121; fax: +886 4 851 1270.
E-mail address: cls2@mail.dyu.edu.tw
www.elsevier.com/locate/eswa
Available online at www.sciencedirect.com
Expert Systems with Applications 34 (2008) 2976–2987
Expert Systems
with Applications
between (kz+q and kzq) is shorter than the distance
between (ky+q and kyq) too. In Eq. (2.1), knq represents
the (n  q)th pixel residing in the obtained palm’s edge
image. The variable n varies from 1 to palm-edge-total-pix-
els subtracting q. The response energy Energy(kn) is
weighted by the distance of the two points knq and kn+q.
As one can see from Fig. 4, one can ﬁnd Energy(kn) will
have the less values in the ﬁngertip or the ﬁnger-to-ﬁnger
valley areas. In the straight-line area of the ﬁnger edge,
Energy(kn) will have the greater values. In this research,
q is set to 10. By this concept, one can extract the entire ﬁn-
gertips and ﬁnger-to-ﬁnger-valleys. Furthermore, one can
extract the entire ﬁngers. Since the ﬁngertip and the ﬁn-
ger-roots might cause some errors during the ﬁnger-recog-
nition process, the ﬁngertips and ﬁnger-roots are taken out.
The ﬁnal ﬁnger images are shown in Fig. 5. In the previous
research, the wavelet transform (Han, 2004; Han et al.,
2003) is used to ﬁnd the positions of ﬁnger-to-ﬁnger-val-
leys. This paper purposed method work more eﬃciently
and correctly to ﬁnd the position of the ﬁngertips and ﬁn-
ger-to-ﬁnger-valleys.
EnergyðknÞ
PalmOutline Pixel Numberq
n¼1
¼ Distance
PalmOutline Pixel Numberq
n¼1
kðknqÞ to ðknþqÞk
ð2:1Þ
3. Geometry descriptor and image subtraction
For every pixel (i, j) in the object, after the transforma-
tion of the geometry descriptor, the ﬁnal destination of
(i, j) is (if, jf). The geometry descriptor will transfer and
interpolate the geometry features of every pixel inside the
object to another feature-domain. The original geometry
feature of the ﬁnger image will be preserved, after the
object is transfer to the new feature-domain. In the new
feature-domain, every object will have the same straight
orientation and every object’s centroid is aligned to posi-
tion (64,64). Since every object has the same orientation
and every object is aligned to the same centroid, one can
perform the image subtraction to perform the person’s
identiﬁcation. Eq. (3.1) shows the function of the geometry
descriptor. In Eq. (3.1), ‘‘h’’ represents the object’s orienta-
tion and ‘‘Centroid’’ represents the object’s centroid. By
using Eqs. (3.2)–(3.4), one can ﬁnd the parameter h. In
Eqs. (3.2) and (3.3), idistance represents the i-distance of a
speciﬁc pixel g(i, j) to the object’s centroid, jdistance repre-
sents the j-distance of a speciﬁc pixel g(i, j) to the object’s
centroid, and Weightij represents the weight of a speciﬁc
pixel g(i, j). The images in the second row of Fig. 6 shows
the resultant images after the geometry descriptor is
applied to the ﬁnger images. During the image-comparison
stage, to allow the ﬁnger geometry to possess more salient
feature, the ﬁngers are scaled to various sizes. The images
are shown in the third row of Fig. 6.
In this research, in order to allow the ﬁnger geometry to
possess more salient feature when performing the ﬁnger
image-comparison, the ﬁngers are also magniﬁed to vari-
ous images. The images are named ‘‘distance-weighted-
images’’. One can obtain the ‘‘magniﬁed ﬁnger-image’’ by
processing the ﬁnger images by the following steps:
Assume one existing pixel T, T 2 ﬁnger image, and
assume f(i, j) is the gray level of point (i, j), and also assume
f(i, j) 2 ﬁnger image. One use the symbol kT  f(i, j)k to
represent the distance between point T and point (i, j)
and one denote the distance as d. The construction of
Fig. 4. Show the positions of various feature points.
Fig. 5. The hand image and the extracted ﬁngers.
2978 C.-L. Su / Expert Systems with Applications 34 (2008) 2976–2987
better recognition rate. First, the image is shifted at diﬀer-
ent positions – to which are – left four pixels, left three pix-
els, left two pixels, etc., to right four pixels. Also, the image
is shifted – top four pixels, top three pixels, top two pixels,
etc., to bottom four pixels. The picture is shown in Fig. 12.
When the images are ﬁne-tuning to one certain position,
the image is also ﬁne-tuning to diﬀerent orientations for
comparison. The rotated degrees are – counter-clockwise
20, counter-clockwise 15, counter-clockwise 10, coun-
ter-clockwise 5, etc., to clock-wise 20. Fig. 13 shows the
rotated-images, which are rotated from 5 to +5.
As mentioned earlier, for reducing the number of ﬁles in
the system, one person’s entire ﬁnger images are placed in
one ﬁle to perform the process of the image shifting and
image-rotation. After the image shifting and rotation, the
individual ﬁnger image needs to be extracted from the
hybrid geometry signal separately to perform the pattern
recognition. The hybrid geometry signals and the extracted
ﬁnger images are shown in Figs. 14 and 15. After obtaining
the individual ﬁnger image, one can perform the image sub-
traction to the ﬁnger images and to perform the pattern
recognition. The image subtractions are shown in Figs.
16–19. In the top of Fig. 16, one can ﬁnd a 5 by 5 sub-pat-
tern-template. This 5 by 5 sub-pattern-template will extract
the sub-pattern images of two persons’ hand geometry sig-
nals and, consequently, the image convolution is performed
to the 5 by 5 sub-pattern-template images. Image-ﬁne-tun-
ing is also performed in this stage and the minimum value
of the convolution results will be recorded in the resultant-
image. The subtract result image is shown in the right-hand
side picture of Fig. 16. By calculating the diﬀerence of the
subtracted-result, one can recognize diﬀerent persons. In
Figs. 17–19, one can ﬁnd the hybrid geometry signals and
the extracted ﬁnger images. The image subtraction is per-
formed to the extracted ﬁnger images and the subtracted-
results are shown in the right-hand side pictures of Figs.
17–19. Fig. 17 shows the palm image, Fig. 18 shows thumb
ﬁnger, and Fig. 19 shows the index ﬁnger.
Fig. 8. The images before and after image-magnifying.
Fig. 9. The results of the ﬁnger images after performing the geometry descriptor, imagescaling, and image-magnifying.
2980 C.-L. Su / Expert Systems with Applications 34 (2008) 2976–2987
ði; j; h;Centroid )
Geometry Descriptor
ðif ; jfÞ ð3:1Þ
Var1 ¼
XN
j¼1
XN
i¼1
ðidistanceÞ2 Weightij

XN
j¼1
XN
i¼1
ðjdistanceÞ Weightij ð3:2Þ
Var2 ¼
XN
j¼1
XN
i¼1
2  idistance  jdistance Weightij ð3:3Þ
h ¼
Sin1
PN
j¼1
PN
i¼12idistancejdistanceWeightijﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Var22þVar12
p
2
ð3:4Þ
Fig. 13. The image is rotated to diﬀerent positions.
Fig. 15. Separating-operation to separate the diﬀerent ﬁngers.
5º0º
Subtract =
sultSubtract Re
TemplateSubPattern _
SubPattern _Template SubPattern_Template
MinimumGetnConvolutioTemplateSubPattern __ ⇒
Fig. 16. Sub-pattern template convolution to ﬁnd the minimum pixel value.
Fig. 14. Separating-operation to separate the diﬀerent ﬁngers.
2982 C.-L. Su / Expert Systems with Applications 34 (2008) 2976–2987
correctly, several images are discarded from the test. Actu-
ally, the hand-photo-images used in this system are only
120, which are belonged to 40 persons.
Regard these 120 photos as whole and run these 120
photos in one batch – which will take a very long time to
get the job done. Due to the software glitch, the running
process might break before the job completed. Conse-
quently, one might endlessly run and run the procedure
again and again and every time from the square one. Fur-
thermore, the database might have no enough memory to
accommodate the entire obtained-data. Thus, in this sys-
tem, ﬁve persons are designated as one group, i.e. ﬁve peo-
ple are regarded as a group and these ﬁve persons are tested
in one step. Since one person has been taken three diﬀerent
photos, totally ﬁfteen photos are tested in each step. As
mentioned earlier, in this system, there are 40 persons par-
ticipating the test. Thus, the system totally will run eight
diﬀerent batches. Totally, 120 hand-photos are tested.
For each hand-photo image, the following steps are per-
formed – (1) the hand edge is found, (2) the ﬁngers are
extracted separately, (3) geometry descriptor is performed
to each ﬁnger, (4) image shifting and rotation are per-
formed, (5) image magnifying are performed, (6) genuine
and imposter comparisons are conducted. To each group,
one photo is compared against the other 14 photos.
Totally, 105 comparisons are conducted to test the accu-
racy of the developed identiﬁcation algorithm. Within
those one hundred and ﬁve comparisons, ﬁfteen compari-
sons are conducted for genuine comparison, the compari-
sons of the same person’s ﬁngers – since one person
taken three diﬀerent photos. The other ninety comparisons
are conducted for imposter-comparisons - the comparisons
of diﬀerent person’s ﬁngers. In this research, there are eight
diﬀerent test batches. Thus, there are 120 genuine tests and
720 imposter tests. Fig. 20 shows the partial comparison
data of the image diﬀerences after performing the image
Fig. 19. Image subtraction of diﬀerent orientations and diﬀerent positions.
Comparisons of the same
person's index fingers
13606 9344 32648 35920 32057 19731 19951 20553 21518 22666 24366 23012 23208 24767
9075 36050 39984 35514 16802 18862 17742 27583 27046 25050 19072 19944 23275
32330 36490 31081 17812 20023 17886 21973 22145 22144 21070 21091 23797
15355 6971 39351 40742 41120 27115 28159 34670 42610 45765 41568
14295 45178 45510 47317 29593 29584 34317 45483 49224 43513
39852 41553 40999 27351 26778 32646 42803 45542 42197
8055 11227 28340 28836 28194 12836 12500 17419
11585 29746 31098 30431 14746 15300 18198
30076 31020 30898 16496 12981 20415
11496 18355 32092 34529 31320
14401 32070 34369 31665
28564 31604 27848
10206 11121
14594
Fig. 20. The comparison-data of index ﬁnger images.
2984 C.-L. Su / Expert Systems with Applications 34 (2008) 2976–2987
person’s another ring ﬁnger image. The other data, which is
not surrounded by the rectangular boxes, show the sub-
tracted-results of the diﬀerences of two diﬀerent persons’
ring ﬁnger images. Fig. 27 shows the summarized-results
of Fig. 26. Fig. 27 shows the gap between the imposter
and genuine comparisons. Fig. 27 depicts ﬁve diﬀerent
persons’ comparison results. For each person the top line
with arrow represents the value-range of the imposter com-
.
.
.
.
.
.
.
First
Person
Imposter
Comparison
Genuine
Comparison
Imposter
Comparison
Genuine
Comparison
Second
Person
Imposter
Comparison
Genuine
Comparison
Third
Person
Genuine
Comparison
Fourth
Person
Genuine
Comparison
Fifth
Person
13.0
8.9
13.9
9.0
9.4
8.8
12.0
.
.
.
Imposter
Comparison
10.0
7.1
9.9
14
15
16
17
10
11
12
13
7
8
9
Imposter
Comparison
Fig. 27. The gap between the imposter and the genuine comparisons for
ring ﬁnger.
Comparisons of the same
person's ring fingers
8 912 7236 1 3 0 9 1 1 4 4 7 2 1 3 9 1 0 2 0 4 0 9 2 0 3 4 0 1 8 8 5 8 1 8 8 2 6 1 5 4 5 5 1 6 1 3 5 2 3 2 5 8 2 4 2 7 2 2 3 9 7 0
8304 1 6 2 2 7 1 4 8 8 4 1 5 5 5 4 1 8 6 8 5 1 7 8 9 8 1 6 4 0 8 1 7 2 3 2 1 6 4 4 0 1 4 0 7 3 1 9 5 8 2 2 0 7 3 0 2 0 1 4 6
1 3 3 5 7 1 4 8 9 6 1 3 9 2 0 1 7 8 6 5 1 8 0 3 4 1 6 5 1 6 1 7 5 8 6 1 4 7 4 3 1 5 1 3 2 2 1 2 9 0 2 2 1 3 2 2 1 8 3 0
1 2 0 8 3 8 7 1 4 2 5 6 6 2 2 5 6 9 5 2 4 0 2 7 2 1 7 9 1 2 1 4 4 8 2 2 0 8 7 3 0 1 1 2 3 1 0 1 4 3 0 7 7 4
8 3 4 0 2 8 6 9 0 2 9 0 3 1 2 7 3 8 7 2 6 7 9 7 2 5 3 4 1 2 4 4 3 6 3 1 8 8 9 3 2 9 4 0 3 2 2 3 1
2 7 1 0 4 2 7 5 6 9 2 6 0 3 7 2 4 9 7 3 2 3 6 6 1 2 3 4 4 5 3 1 5 6 1 3 2 1 4 2 3 1 9 3 6
8 8 2 3 7 9 6 9 1 0 9 5 1 1 2 8 2 0 1 2 3 0 9 1 2 1 1 4 1 2 6 1 2 1 3 5 2 3
7944 9 1 5 0 1 1 3 2 2 1 1 0 5 5 9 0 8 6 1 0 1 3 0 1 0 7 2 8
9 2 2 0 1 0 1 1 2 9 4 3 4 1 0 0 9 9 9 9 9 4 1 1 3 7 6
8 5 9 8 1 0 0 0 1 1 3 9 7 0 1 4 4 3 0 1 4 5 8 4
8 3 5 1 1 3 6 1 2 1 4 4 1 1 1 4 2 6 5
1 2 9 2 9 1 3 5 0 4 1 3 4 3 0
7 1 7 5 6 8 8 6
6 5 9 8
Fig. 26. The comparison-data of ring ﬁnger images.
.
.
.
.
.
.
.
First
Person
Imposter
Comparison
Genuine
Comparison
Imposter
Comparison
Genuine
Comparison
Second
Person
Imposter
Comparison
Genuine
Comparison
Third
Person
Genuine
Comparison
Fourth
Person
Genuine
Comparison
Fifth
Person
15.6
11.5
18.2
11.4
12.9
10.1
13.4
.
.
.
14
15
16
17
18
10
11
12
13
19
20 Imposter
Comparison
16.1
10.1
11.4
Imposter
Comparison
Fig. 24. The gap between the imposter and the genuine comparisons for
middle ﬁnger.
.
.
.
.
.
.
.
First
middle finger
FAR
.
.
.
90
95
100
0
5
10
Second
middle finger
Third
Middle finger
Fourth
Middle finger
Fifth
Middle finger
FRR
Accuracy Rate
Fig. 25. The accuracy rate, FRR, and FAR for the middle ﬁnger.
.
.
.
.
.
.
.
First
Ring finger
FAR
.
.
.
90
95
100
0
5
10
Second
Ring finger
Third
Ring finger
Fourth
Ring finger
Fifth
Ring finger
FRR
Accuracy Rate
Fig. 28. The accuracy rate, FRR, and FAR for the ring ﬁnger.
2986 C.-L. Su / Expert Systems with Applications 34 (2008) 2976–2987
Finger extraction, ﬁnger image automatic registration,
and ﬁnger identiﬁcation by image phase matching q
Ching-Liang Su
Department of Industrial Engineering and Technology Management, Da Yeh University, 112 Shan-Jeau Road,
Da-Tsuen, Chang-Hua 51505, Taiwan
Abstract
In this research, a new technique is used to extract the thumb, index, middle, ring, and small ﬁngers and to perform a
person’s identiﬁcation. To allow the ﬁnger geometry to be more salient when performing the ﬁnger image comparison, the
ﬁngers are scaled to various sizes. For reducing the number of ﬁnger-image-ﬁles in the system, a person’s entire ﬁnger-
images are placed in one ﬁle. The hand is ﬁxed each time when a picture is taken and one can assume that each time when
the hand image is taken, the acquired ﬁnger images are the same as the previously acquired ones. Since the pictures are the
same, after the ﬁngers are extracted from the hand image, one can use the acquired ﬁngers to identify diﬀerent people. In
this research, the developed algorithm of the auto-registration technique can ﬁnd the precise location of the ﬁnger image –
including the centroid of the ﬁnger image and the orientation of the ﬁnger image. The ﬁnding of the position and the ori-
entation of the ﬁnger image are conducted automatically and without any further human eﬀort. After ﬁnding the positions
of the ﬁnger images, image rotating, image shifting, and image interpolating techniques are used to align diﬀerent ﬁnger
images to the same position and the same orientation for comparison. The extracted ﬁnger image contains many useful
geometrical features. One can use these features to do ﬁnger image identiﬁcation. Since the entire ﬁnger images are aligned
to the same position and the same orientation, the image phase-matching technique is used to examine the diﬀerence
between two ﬁnger images. The image phase-matching technique involves complex number manipulation and also ﬁnds
the most salient feature of the resultant images.
 2006 Elsevier Inc. All rights reserved.
Keywords: Finger shape information extraction; Image phase matching; Identiﬁcation intelligence
1. Introduction
During the past 20 years, researchers invested a lot of eﬀort to develop diﬀerent techniques to identify hand
images. This past work includes hand geometry [1,2,4,5,12,15–18], middle ﬁnger crease pattern matching [6],
various ﬁnger size measurements [7,14], various ﬁnger lateral view size measurements [14], vein pattern [9],
eigenpalm [10], implicit polynomials [13], algebraic invariants [13], Karen invariant computation [13], line
0096-3003/$ - see front matter  2006 Elsevier Inc. All rights reserved.
doi:10.1016/j.amc.2006.10.074
q National Science Council, Taiwan, supported this work under grant NSC 95-2221-E-212-003.
E-mail address: cls2@mail.dyu.edu.tw
Applied Mathematics and Computation 188 (2007) 912–923
www.elsevier.com/locate/amc
Fig. 2.1. The designed shelves for taking the hand images.
Fig. 2.2. The 128 by 128 gray scale hand images.
Fig. 2.3. The extracted hand edge images.
kx ρ ρ+
kx
kx−
ky ρ
ρ
ρ ρ
+
ky
ky−
kz+
kz
kz−
Fig. 2.4. The positions of various feature points.
914 C.-L. Su / Applied Mathematics and Computation 188 (2007) 912–923
The images in the third row of Fig. 3.1 show the resultant images after the geometry descriptor is perform
to the ﬁnger images. To allow the ﬁnger geometry to possess more salient features when performing the ﬁnger
image comparison, the ﬁngers are scaled to various sizes. The images are shown in the fourth row of Fig. 3.1.
Fig. 3.2 shows the overlapped geometry signals. The overlapped geometry signals are the overlapped signals
of the palm, thumb, index, middle, ring, and small ﬁngers; i.e. the signals of the palm, thumb, index, middle,
ring, and small ﬁngers are added together to generate the overlapped geometry signals. Since one ﬁnger-image
needs one ﬁle to record the ﬁnger image, more ﬁnger-images need more ﬁles to record these ﬁnger-images. In
this overlapped scheme, several ﬁngers’ images are recorded in one ﬁle. Since several ﬁnger images are
recorded in one ﬁle, the total number of the ﬁles in the system would be reduced. Since the total number
of the ﬁles is reduced, the system complexity is reduced when performing the person identiﬁcation. Fig. 3.3
shows the various overlapped geometry signals.
Fig. 3.1. The results of the ﬁnger-images after performing the geometry descriptor and image scaling.
Fig. 3.2. The overlapped geometry signal.
916 C.-L. Su / Applied Mathematics and Computation 188 (2007) 912–923
4. Image phase matching
Eq. (4.1) shows the function which transfers the function f(x,y) to function F(u,v) and Eq. (4.2) shows the
function which transfers the function g(x,y) to function G(u,v). Fig. 4.1 shows the various f(x,y) and its
corresponding F(u,v) images. Eq. (4.3) shows the relationship of image phase matching between – F(u,v),
conjugated G(u,v), f(x,y), and g(x,y). Symbol  represents the image phase matching. Fig. 4.2 shows the phase-
matching images of the two images f(x,y) and g(x,y). By checking Fig. 4.2, one can ﬁnd that by using the
image phase-matching technique, one can correctly identify the position of g(x,y). Fig. 4.3 shows the various
phase-matching images which are generated by the images f(x,y) and g(x,y). The top image is the palm image,
the center image is the thumb image, and the bottom image is the index ﬁnger image. The algorithm developed
in this research will examine the most salient pixel in the phase-matching image in Fig. 4.3 to identify whether
or not image f(x,y) and image g(x,y) are the same images. Fig. 4.4 shows the extracted palm images and the
phase-matching images of the two extracted palm images, Fig. 4.5 shows the extracted thumb images and the
phase-matching images of the two extracted thumb images, and Fig. 4.6 shows the extracted index ﬁnger
images and the phase-matching images of the two extracted index ﬁnger images. As mentioned before, the
algorithm developed in this research will identify the most salient pixel of the phase-matching image to identify
whether or not image f(x,y) and image g(x,y) are the same images.
F ðu; vÞ ¼ 1
128
X127
x¼0
X127
y¼1
f ðx; yÞ  exp½j2pðuxþ vyÞ=128; ð4:1Þ
Gðu; vÞ ¼ 1
128
X127
x¼0
X127
y¼1
gðx; yÞ  exp½j2pðuxþ vyÞ=128; ð4:2Þ
F ðu; vÞ  Gðu; vÞ ¼ f ðx; yÞ  gðx; yÞ: ð4:3Þ
Fig. 4.1. The f(x,y) and F(u,v) images.
918 C.-L. Su / Applied Mathematics and Computation 188 (2007) 912–923
have insuﬃcient enough memory to accommodate the entire obtained data. Thus, in this system, ﬁve persons
are designated as one group, i.e. ﬁve people are regarded as a group and these ﬁve persons are tested in one
step. Since one person has three diﬀerent hand images being taken, totally 15 hand images are tested in each
step. As mentioned earlier, in this system, there are 40 persons participating in the test. Thus, the system will
Fig. 4.4. Image phase matching of diﬀerent orientations and diﬀerent positions.
Fig. 4.5. Image phase matching of diﬀerent orientations and diﬀerent positions.
920 C.-L. Su / Applied Mathematics and Computation 188 (2007) 912–923
various ﬁnger images. By analyzing the entire test result of this research, one can conclude that the accuracy
rate is 95%. The false accepted rate is 2.5% and the false rejected rate is 3%.
References
[1] Hand-based biometrics, Editorial, Biometric Technology Today, 11(7) (2003) 9–11.
[2] Karen O. Egiazarian, S. Gonzalez Pestana, Hand shape identiﬁcation using neural networks, The International Society for Optical
Engineering 4667 (2002) 440–448.
.
.
.
.
.
.
.
First
Person
Imposter
Comparison
Genuine
Comparison
Imposter
Comparison
Genuine
Comparison
Second
Person
Imposter
Comparison
Genuine
Comparison
Third
Person
Imposter
Comparison
Genuine
Comparison
Fourth
Person
Genuine
Comparison
Fifth
Person
16.8
13.6
27.1
12.5
14.5
18.3
11.5
15.3
.
.
.
.
.
.
14
15
16
17
18
10
11
12
13
19
20
21
22
Imposter
Comparison
22.1 23.2
Fig. 5.2. The gap between the imposter and the genuine comparisons for an index ﬁnger.
.
.
.
.
First
person
Second
person
Third
person
Fourth
person
Fifth
person
FAR
.
.
.
0
5
10
FRR
Index finger
FAR
.
.
.
0
5
10
FRR
Middle finger
FAR
.
.
.
0
5
10
FRR
Ring finger
Fig. 5.3. The FRR and FAR for the various ﬁnger images.
922 C.-L. Su / Applied Mathematics and Computation 188 (2007) 912–923
Palm extraction and identiﬁcation q
Ching-Liang Su *
Department of Industrial Engineering and Technology Management, Da Yeh University, 112 Shan-Jiau Road,
Da-Tsuen, Chang-Hua 51505, Taiwan
Abstract
This study uses an extracting technique to obtain palm images. The algorithm proposed in this study can determine the precise loca-
tions of ﬁngertips and ﬁnger-to-ﬁnger-valleys on a hand. After locating these positions, palm images can be extracted. Such images con-
tain many useful geometrical features, which can be used to identify palms. The orientations and centroids of palms are identiﬁed in this
study. Image rotation, shifting, and interpolation techniques are used to align diﬀerent palms to the same position for geometrical
comparison.
 2007 Elsevier Ltd. All rights reserved.
Keywords: Finger extraction; Palm comparison; Image location identiﬁcation
1. Introduction
In the past 20 years, researchers have devoted much
attention to identify the hand images. The past works used
the following techniques to identify hands: hand-geometry
matching (Egiazarian, 2002; Ferrer, Travieso, & Alouso,
2006; Han, Cheng, Lin, & Fan, 2003; He, Qiu, & Sun,
2002; Malassiotis, Aifanti, & Strintzis, 2006; Mitome &
Rokuya Ishii, 2003; Su, 2003; Sun & Qiu, 2004; Xionga,
Toha, Yaua, & Jiangb, 2005; Xiong, Xu, & Heng Ong,
2005; Yoruk, Dutagaci, & Sankur, 2006), middle-ﬁnger
crease pattern matching (Joshi, Rao, Kar, & Kumar,
1998), various ﬁnger size measurements (Kumar, Wong,
& Shen, 2003; Sanchez-Reillo, Sanchez-Avila, & Gonza-
les-Marcos, 2000), various ﬁnger lateral view size measure-
ments (Sanchez-Reillo et al., 2000), vein pattern (Lin &
Fan, 2004), eigenpalm (Lua, Zhang, & Wanga, 2003),
implicit polynomials (Oden, Ercil, & Buke, 2003), algebraic
invariants (Oden et al., 2003), Karen invariant computa-
tion (Oden et al., 2003), line interception and slope com-
parisons (You, Li, & Zhang, 2002), control point
selection (Lia, Zhang, & Xub, 2003; You et al., 2002),
coarse to ﬁne strategy (Han, 2004), B-spline (Ma, Pollick,
& Terry Hewitt, 2004), principal component analysis
(Erdem Yoruk et al., 2006; Yo¨ru¨k, Glu, & Sankur, 2006),
Hausdorﬀ distance (Yo¨ru¨k et al., 2006), watershed trans-
form (Lin & Fan, 2004), and HMM (Sun & Qiu, 2004).
However, some of these techniques are very sensitive to
noise (Joshi et al., 1998; You et al., 2002) some have very
complicated mathematical models (Oden et al., 2003) some
have very complicated neural training algorithms.
In the aforementioned research, when performing hand-
geometry matching for recognizing images, the hand must
be placed in a precise ﬁxed position before the camera can
capture the same image. In the present study the hand does
not need to be placed in a deﬁnite position; still, the hand
recognition algorithm can recognize hands. In past
research (Han, 2004; Han et al., 2003) a wavelet technique
was used to locate the ﬁnger-to-ﬁnger valley; whereas, in
the present study, the proposed recognition algorithm
automatically calculates and checks the ﬁnger-edge energy
response signals and selects the high-energy signals to
locate the ﬁnger-to-ﬁnger-valleys. The proposed algorithm
in this study locates the ﬁnger-to-ﬁnger-valleys more
accurately and more eﬃciently; it amputates ﬁngers from
0957-4174/$ - see front matter  2007 Elsevier Ltd. All rights reserved.
doi:10.1016/j.eswa.2007.11.001
q The National Science Council, Taiwan, supported this work under
Grant NSC 96-2221-E-212-004.
* Tel.: +886 4 851 1888x4121; fax: +886 4 851 1270.
E-mail address: cls2@mail.dyu.edu.tw
www.elsevier.com/locate/eswa
Available online at www.sciencedirect.com
Expert Systems with Applications 36 (2009) 1082–1091
Expert Systems
with Applications
Fig. 4. Database for locating middle-ring-ﬁnger-valley.
Fig. 5. Database for locating thumbtip.
1084 C.-L. Su / Expert Systems with Applications 36 (2009) 1082–1091
ﬁnger-truncated images. Fig. 13 shows how the thumb can
be dilated and truncated. The fully truncated image is
shown in Fig. 14. Fig. 15 shows all the fully truncated
images. The images in Fig. 15 are the gray scale images.
4. Image registration and subtraction
To compare two images, one needs to locate the cen-
troids of the ﬁnger images. The images used in this study
have 128 · 128 frames. After the centroids have been
located, these should be moved to the centers of frames,
i.e., the images must be shifted to allow each centroid to
be moved to the (64, 64) location in the frame. Furthermore,
the major axes of palm images should be located and moved
to a straight position. When each image has been shifted to
the center of the frame and each image has been aligned to
the same straight position, subtraction of two images can be
performed to determine the diﬀerence.
In Eqs. (4.1) and (4.2), it is assumed that one pixel g(i, j)
exists and it represents a speciﬁc pixel that is part of the ﬁn-
ger image. From point g(i, j) to the centroid of the palm,
one can generate one vector. By the Cartesian rule, one
can separate this vector into vertical and the horizontal
vectors. The term idistance represents the vertical distance
of a speciﬁc pixel g(i, j) to the centroid of the object; jdistance
represents the horizontal distance of the same pixel to the
centroid of the object; weightij represents the weight or gray
level of that pixel. By using Eq. (4.3), the orientation of the
object can be obtained. In Eq. (4.4), hdiﬀerence represents the
diﬀerence between the orientations of two images. After
solving Eq. (4.4), a speciﬁc pixel in location (i, j) is rotated
to position (if, jf). After the image rotation, every palm
image will have the same straight orientation, and every
centroid of the palm image would be aligned to the same
(64, 64) position in frame. Since both ﬁnger images overlap,
subtraction can now be applied to compare the diﬀerences
between the two. By subtracted result, one can identify
the palms. Figs. 16 and 17 show subtractions of palm
images.
Truncate small finger
Fig. 10. Location of small-ﬁngertip and ring-smallﬁnger- valley positions.
Fig. 11. Image after truncation of small-ﬁnger.
Fig. 12. Images with only thumb remaining.
Truncate thumb
Fig. 13. Area for dilation and truncation of thumb. Fig. 14. Image obtained after dilation and truncation of thumb.
1086 C.-L. Su / Expert Systems with Applications 36 (2009) 1082–1091
were sticking together or the thumbs and the index ﬁngers
were stretching too wide. The algorithm cannot correctly
extract the middle and ring ﬁngers when these stick
together; neither can it extract a wider stretched thumb.
Since these two problems prevent the algorithm from
extracting the thumb, middle, and ring ﬁngers, several
images were discarded from the test. Actually, the photo-
graphs used in this system are only 120 and depict only
40 diﬀerent hands.
Considering the 120 photographs altogether and run-
ning them in one batch would require a very long process-
Fig. 17. Image comparison.
Fig. 18. Erosion of small-ﬁnger with two additional pixels.
Fig. 19. Erosion of thumb with two additional pixels.
Fig. 20. Erosion of bottom of hand image.
Fig. 21. Obtained hand images.
1088 C.-L. Su / Expert Systems with Applications 36 (2009) 1082–1091
ing time. There is also the possibility of malfunctioning
software and/or insuﬃcient memory to accommodate the
database. Therefore, the 40 participants were divided into
eight groups of ﬁve persons each for testing. Since three dif-
ferent photographs were taken for each participant, 15
images were tested in each step. Thus the system ran eight
diﬀerent batches. For each of the 120 photographs, the fol-
lowing steps were implemented: (1) hand-edge is located,
(2) ﬁngers are truncated, (3) geometry descriptor is per-
formed, (4) image shifting and rotating are performed,
and (5) genuine and imposter comparisons are conducted.
In each group, one photograph was compared with the
other 14 photographs. Altogether, 105 comparisons were
conducted to test the accuracy rate. Within these compar-
isons, 15 were conducted for genuine comparison, since
three diﬀerent photographs were taken for each person.
The other 90 comparisons were conducted to identify
imposters among diﬀerent palms. Among the eight diﬀerent
test batches, there were 120 genuine tests and 720 imposter
tests. Figs. 25 and 27 show partial comparison data on
image diﬀerences after performing image subtraction. The
data inside the rectangular boxes in both tables were the
results of subtraction for genuine palms. The other data
in these tables comprise the subtracted results from two dif-
ferent palms. Figs. 26 and 28 present the summarized
results of Figs. 25 and 27, showing the gap between the
imposter and genuine comparisons for ﬁve diﬀerent per-
sons. For each person the upward-pointing arrow repre-
sents the value-range of imposter comparisons; the
36992 20694 99877 68924 88036 176718 172981 161739 65346 56110 63719 179185 192596 202135
31467 109722 78969 101873 177194 182123 168160 79986 68442 53043 183933 193425 203734
101773 69576 87175 174997 176522 162569 63539 54899 59268 182882 195051 204868
44130 29646 236598 198803 188936 66088 68829 102040 218580 227712 240850
32126 212172 184983 174393 50020 38209 70536 196544 207824 219629
226559 189154 177882 58203 54736 92107 205639 216951 227984
37182 46208 193483 210588 208665 143642 177167 148869
28278 193293 183369 186864 107919 126665 122884
174133 176119 177664 108673 127309 123009
56718 61759 210182 221134 233212
47812 187862 197300 210802
196671 206908 218856
45381 37102
39205
Comparisons of same
person's palm images
Fig. 25. Comparison data.
Comparisons of same
person's palm images
47975 31848 223069 220379 222395 62827 83740 91201 249235 278546 269964 240465 242687 227065
38753 209066 208838 210170 54129 63239 65612 234710 260205 253205 247728 241478 224516
198275 199093 200927 63660 80435 86772 247071 274780 268078 243837 227243 205137
37126 27074 223799 232683 230720 180295 265935 219847 120736 72401 58620
23889 214834 223328 223721 170874 267020 214100 115877 65193 59657
217457 225419 224980 168271 262301 211399 113756 66386 58625
33983 45269 229090 254794 246364 232947 230635 234460
31855 225602 249382 241433 246086 242778 249719
220876 240980 237555 252511 244367 251304
66882 50892 142907 160861 190976
45810 223307 267827 286652
163860 208742 236253
71794 79577
34053
Fig. 27. Comparison data.
.
.
.
.
.
.
.
First
Person
Imposter
Comparison
Genuine
Comparison
Imposter
Comparison
Genuine
Comparison
Second
Person
Imposter
Comparison
Genuine
Comparison
Third
Person
Genuine
Comparison
Fourth
Person
Genuine
Comparison
Fifth
Person
5.3
3.6
4.4
3.8
4.6
.
.
.
Imposter
Comparison
6.1
4.5
12.2
9
10
11
12
5
6
7
8
2
3
4
Imposter
Comparison
3.8
10.7
Fig. 26. Gap between imposter and genuine comparisons for palm images
(·10,000 to obtain actual number).
1090 C.-L. Su / Expert Systems with Applications 36 (2009) 1082–1091
可供推廣之研發成果資料表 
□ 可申請專利  □ 可技術移轉                                      日期：98 年 7 月 31 日 
國科會補助計畫 
計畫名稱：手指指頭掌形識別使用特性點擷取,指頭取出,指頭外型
特性轉換比對 
計畫主持人：蘇慶良         
計畫編號：NSC 97-2221-E-212-021 學門領域：影像與圖形辨識 
技術/創作名稱 手掌掌型識別機 
發明人/創作人 蘇慶良 
技術說明 
中文： 
1. 將手掌, 置於特定的位置拍照. 
2. 控制照明光度,拍照手掌圖片 
3. 找到手指指尖位置,找到手指與手指間凹槽真正位置 
4. 翠取出食指,中指,無名指,母指,小指,手掌型 
5. 使用二次元慣性理論找出食指的影像方位 
6. 做影像位移和影像旋轉,做影像定位,使兩個不同的手指影像調
到同一位置,做影像比對 
7. 把非整數點做影像詮述. 
8. 手指影像放大,使手指手掌特性曲線更明顯 
9. 手指影像權值放大,使手指手掌影像幾何特性更明顯 
10. 集中一個人的所有手指指頭及手掌影像成單一 file 
11. 手指指頭及手掌影像從單一 file中分離,做單一手指指頭及手掌
的比對 
12. 小波轉換擴大手指指頭訊號. 
13. 做影像 Small Window Convolution比對及取最佳值,並考慮到影
像旋轉,影像漂移的影響,依影像旋轉,影像漂移的影響,順其影響
而算出最佳比對值. 
14.做影像 Correlation做影像識別.對所有手指指頭及手掌影像比
對值排序及做排序值統計做手指指頭及手掌影像識別 
附件二 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
報 告 人 
姓   名 
 
蘇慶良 
服務機構 
及 職 稱 
大葉大學工業工程與科技管理系暨研究所 
教授 
      時間 
會議 
      地點 
自 2008 年  2 月  5 日 
至 2008 年  2 月  8 日 
Lisbon - Portugal. 
本會核定 
 
補助文號 
 
A-DIT-9709 
NSC 97-2221-E-212-021 
 
會 議 名 稱 
(中文) 第 4屆電腦視覺原理與運用國際研討會 
 
 (英文) The 4rd International Conference on Computer Vision Theory and Applications 
發表論文題目 
Member of International Program Committee; participate member board meeting during the conference 
period. 
報告內容應包括下列各項： 
 
一、 參加會議經過 
VISAPP is organized by INSTICC - Institute for Systems and Technologies of Information, Control and 
Communication. This international conference is a very big international computer vision event. It takes 
four days to complete this conference and it covers another computer vision workshop and several other 
conferences. Several important keynote speakers address their research interesting in this event. 
This is a very huge international conference. There are about 1000 speakers from the world attend this 
conference.  
 
二、 與會心得 
I am very glad to join this conference. I talked with many scientists, who were coming from different 
countries. Many interesting papers are presented in this conference and I came out the following new 
things after I attended this conference:  
 
1. Searching the similar butterfly or fish in another picture by the simple symmetry method, relying 
on symmetrical axis, works; however, it will not work on asymmetrical object. 
2. Image is separated as the simple fore- and complicated back-image, which works fine to extract the 
fore image. However, it needs to deal with the changing circumstances of the background such as 
brighter or darker light. 
3. Work well to represent the terrain of the terrain description project, which is presented by one 
researcher. 
4. To the topic of improve face lighting circumstance, the presenter uses 4 lights to demonstrate the 
effectiveness; however, the lightness seems no relationship or improperness with these 4 lights.
