 
Fig. 2 Photo showing the different cars in 
different lanes 
 
2. Theory of distance measuring 
Fig. 3 Configuration of the proposed measuring 
system. 
Figure 3 is the configuration of the proposed 
measuring system, depicting both the vertical and 
horizontal views, where the top of Fig. 3 shows 
the vertical view and the button shows the 
horizontal view. A CCD camera is mounted at 
height mH  with its optical axis parallel with the 
ground. By this structure, the taillights’ image will 
appear around the scan line 1/ 2 (max)VN  in the 
images. DN is the minimum measuring distance. 
The distance between two taillights, Lamps L and 
Lamp R, is a constant SD . The position of Lamp 
L and Lamp R in images is denoted as PL and PR, 
respectively, in Fig. 4. Pixel number between LP  
and RP  is ( )LR KN H . As demonstrated in our 
previous work [9]-[12], there is a direct 
proportional relationship between the horizontal 
pixels’ counting and horizontal distance. For 
distance KH  with a view angle of 2 Hθ , the 
maximal horizontal distance ( )max KD H  covered 
by the view angle can be expressed as: 
 
( ) ( )( ) SKLR
H
K DHN
NHD ×= maxmax   (1) 
 
In Fig. 4, the shooting distance HK can be 
expressed as: 
 
( )
( )
( )
max
1 cot
2
max1 cot
2
K K H
H
S H
LR K
H D H hs
N
D hs
N H
θ
θ
= × −
= × × −
  (2) 
 
 
Fig. 4 Diagram illustrating the IBDMS. 
 
The shooting distance KH  indicating the 
distance between two vehicles can be determined by 
(2) . Hθ  is half of the horizontal view angle 2 Hθ . hs 
is the distance between the optical origin and the 
front end of the CCD when the view angle is known 
as 2 Hθ . The parameter Hθ  can be determined 
through a proposed mechanism previously revealed 
in our researches [13]-[14]. By this mechanism, the 
view angle Hθ  and hs can be ascertained for any 
brands of CCD cameras. When the position of the 
two taillights is identified, pixel number between 
two taillights in the image is ( )LR KN H . By (2), the 
distance between two vehicles can be calculated. 
Note that the image size of the taillights in images 
varies depending on the distance and the brightness 
of the taillights in images might be up to the 
saturation level. Furthermore, the image might be 
affected by other cars or streetlamps. In these cases, 
pixel numbers 1( )N W  and 2( )N W  might be 
different. To solve this problem, pixel counting 
between LP  and RP  is decided by their image 
center.  
 
3. Identification of taillight position     
in images 
As shown in Figs. 1 and 2, image size of the 
taillights in the image frame varies depending on 
different distances. However, the taillight’s image is 
( ) ( ) ( )[ ]21212
1
RRLLKLR NNNNHN +−+⋅=    (3) 
 
4. Separate different cars from 
different lanes 
For vehicles moving in different lane, the 
alarm should not be activated. So we have to 
eliminate the image signals of cars which are not 
in the same lane from the image, focusing only on 
the vehicle ahead in the same lane. By this 
algorithm, the proposed measuring system is more 
suitable for fitting into practical situations. 
 
Fig. 11 Diagram illustrating vehicles in different 
lanes. 
 
With reference to Fig. 11, there are three 
cars in the image. 1( )RP C  stands for a taillight of 
a car in the left lane driving in a short distance 
from our car. Therefore, only the right taillight is 
captured by the camera. 2( )LP C  and 2( )RP C  are 
images of the two taillights of the car ahead in the 
same lane. 3( )LP C  and 3( )RP C  are images of the 
two taillights of a car driving in a longer distance 
from our car in right lane. There are five taillight 
regions in total shown as N(1), N(2), N(3), N(4) 
and N(5), respectively. 
 
( ) ( ) ( ) ( )iNiNiN H δ≤





−
++
2
2
max
2
1    (4) 
 
Ni ....1=  
 
( ) ( ) ( )
2
2
1





 −+
=
iNiNiδ  
 
As long as (4) is proved valid, we have 
 
( ) )()1( iNiNHN KLR −+=     (5) 
 
As shown in Fig. 11, ( )iδ  has minimum 
value when i=2, which means 
( ) (3) (2)LR KN H N N= −  by discarding N(1), N(4) 
and N(5). The implication is that taillights in 
different lane in the images will not be included in 
the procedure for counting ( )LR KN H . 
 
5. Experiment results 
CCD camera adopted: PANASONIC Lumix 
DMC-FZ30. 
Distance between Laser A and Laser B: min( )D  = 20 
cm. 
Camera resolution: (max)HN = 3264 pixels, 
(max)VN = 2448 pixels. 
Intrinsic parameters: cot Hθ  = 2.11 , cot Vθ = 2.81 , 
0h = 1.121 cm 
 
Table 1 The measuring results 
Distance 
between 
two cars 
( )LR KN H
Measured 
distance Error % 
244 2096 260.3599 6.7 
415 1221 442.7949 6.7 
634 801 674.8938 6.5 
931 542 994.2719 6.8 
1244 406 1332.507 7.1 
1572 329 1643.524 4.5 
2000 250 2160.074 8.0 
2370 212 2547.902 7.5 
*In this experiment, we use Ford Monde0 as our test 
car. The distance between two taillights is SD = 157 
cm. 
 
Pic1 Pic2 
Image of taillights 
(Pic1) 
Image of taillights   
(Pic2) 
 
Intensity signals of 
Pic9 after operation 
Intensity signals of 
Pic10 after operation 
 
 
 
 
Table 2. Symmetrical identification 
 Symmetrical identification 
Pic1 correctly 
Pic2 correctly 
Pic3 correctly 
Pic4 correctly 
Pic5 correctly 
Pic6 correctly 
Pic7 correctly 
Pic8 correctly 
Pic9 correctly 
Pic10 correctly 
 
 
 
6. Conclusions 
In this paper a novel distance measuring 
method incorporating the CCD-based distance 
measuring system and partial image analysis 
technique is presented for preventing car 
accidents at night. Based on the proposed method 
with a special structure, any brands of CCD 
camera can be used to measure distance between 
two vehicles. The partial analysis technique used 
in this paper has been proved to be effective in 
reducing the processing time. The problem of 
different lanes has been solved by our proposed 
method. Experiment results show that the error 
rate via the proposed approach is acceptable for 
car accident prevention. There are a lot of 
potential applications of the nighttime image 
based distance measurement. In the future, we 
shall consider fitting the method into real situation, 
such as numerous brands of cars, and 
implementing the proposed system on a SOPC 
embedded system to allow a faster and more 
accurate measuring.  
 
 
 
References: 
[1] Kai-Tai Song, Chih-Hao Chen and Chen-Hsien 
Chiu Huang, “Design and Experimental Study of 
on Ultrasonic sensor System for Lateral collision 
avoidance at Low Speed”, Proc. of 2004 IEEE int. 
Symposium on Intelligent Vehicles, pp.647-652, 
2004. 
[2] Zou Yi, Ho Yeong Khing, Chua Chin Seng, Zhou 
Xiao Wei, “Multi-ultrasonic sensor fusion for 
mobile robots,” Proc. of 2000 IEEE Int. 
Symposium on Intelligent Vehicles, pp.387-391, 
2000. 
[3] N. Tamiya, H. Mandai, T. Fukae, “Optical spread 
spectrum radar for lateral detection in vehicles,” 
Proc. of 1996 IEEE Int. symposium on Spredd 
Spectrum Techniques and Applications, 
pp.195-198, 1996. 
[4] S. Tanaka et al., “Development of a compact 
scan laser radar,” in Proc. SAE World Congr., no. 
1999-01-1044, 1999, pp. 101-106. 
[5] K. Hanawa and Y. Sogawa, “Development of 
stereo image recognition system for active driving 
assist,” in Proc. IEEE Intelligent vehicles Symp., 
2001. pp. 177-182. 
[6] C. Kn〞oppel, A. Schanz, and B. Michaelis, 
“Robust Vehicle Detection at Large Distance 
Using Low Resolution Cameras,” Proceedings of 
IEEE Intelligent Vehicles Symposium, pp.267-272, 
Oct. 2000. 
[7] Jeong Kwan Kang, Sam Yong Kim, Se-Young 
Oh, “All-around vehicle detection using vision 
sensors”, Proceedings 2004 ITS World Congress, 
Nagoya, Oct. 2004. 
[8] Alguel Angel Sotelo and Jose E. Naranjo, 
“Vision-based Adaptive Cruise Control for 
Intelligent Road Vehicles,” Proceedings of 2004 
IEEE/RSJ International Conference on Intelligent 
Robots and Systems, pp.64-69, September, 2004. 
[9] Ming-Chih Lu, Wei-Yen Wang, and Hung-Hsun 
Lian, “Image-Based height measuring system for 
liquid or particles in tanks,” IEEE International 
Conference on Networking, Sensing and Control, 
Vol.1, pp.24-29, 2004. 
[10] Ming-Chih Lu, Wei-Yen Wang, and Chun-Yen 
Chu, “Optical-Based Distance Measuring System 
(ODMS),” The Eighth International Conference 
On Automation Technology, pp.282-285, 2005. 
[11] W.-Y. Wang, M.-C.Lu, T.-H. Wang, C.-P. Tsai 
and Y.-Y. Lu, “Image-and Robot-Based Distance 
and Area Measuring,” Proceedings of the 2007 
WSEAS Int. Conference on Circuits, Systems, 
Signal and Telecommunications (CISST’07), Gold 
Coast, Queensland, Australia, January 17-19, 
2007. 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                            97 年 6  月  30  日 
報告人姓名  
王偉彥 
服務機構
及職稱 
 
國立台灣師範大學應用電子工程
系教授 
     時間 
會議 
     地點 
97年 6月 1日~ 6月 6日 
香港 
本會核定
補助文號
NSC 96-2221-E-003-019 
會議 
名稱 
 (英文) 2008 IEEE World Congress on Computational Intelligence (WCCI 2008). 
發表論文題
目 
 A Dynamic Hierarchical Fuzzy Neural Network for A General Continuous Function
會後感想及建議： 
 
一、 參加會議經過 
2008 IEEE World Congress on Computational Intelligence (WCCI 
2008) 是IEEE年度最大型國際學術研討會之一，其中包括三個子研
討會：2008 International Joint Conference on Neural Networks (IJCNN 
2008), 2008 IEEE International Conference on Fuzzy Systems 
(FUZZ-IEEE 2008), and 2008 IEEE Congress on Evolutionary 
Computation (CEC 2008)。此研討會係由 IEEE Computational 
Intelligence Society 所資助。舉辦地點香港交通非常便利，研討會一
連舉行6天，來自全世界的學者專家約2400人齊聚一堂，共同研討所
涵蓋之相關主題，共發表學術論文2000餘篇。 
 
此次會議有以下特色: 
1. 由五位世界權威學者擔任大會演講者 
者卻有愈來愈多之趨勢，值得我們特別注意。另外相關國際論文研討
會，各國學者的參與非常踴躍，該研討會在國際學術界的越來越受重
視，地位相當的重要。研討會中發表的學術論文大膽新穎，揭櫫了國
際學術界的未來研究趨勢。若能鼓勵同仁多多參與，對於研究工作的
精進以及本校學術地位的提昇，必能有實質的幫助，提昇。 
 
四、攜回資料名稱及內容 
全文論文 CD光碟片。 
 
 
 
 
 
where ),...,,( 21 nxxxX =  is the input variable vector, and 
y  is the output variable. According to the Kolmogorov’s  
theorem [9, 10], a general continuous function )(XG  can 
be represented as 1+M  continuous functions with natural 
hierarchical structure as [9]: 
 
1 2 3 1( ) ( ( ), ( ),..., ( ))My G X G G X G X G X+= =       (2) 
 
where the qth continuous function with natural hierarchical 
structure ( 1, 2,..., 1)qG q M= +  is 
 
1 2 1 2,1 2,2 2,( ) ( , , ..., )lq q q q q q qy G Y g y y y= =            (3) 
 
and  
 
,2,
2, 2, 2, 2, ,2, ,2, ,2,
1 2( ) ( , ,..., ), 1,2,...,q j
j j j j q j q j q j
q q q q my g X g x x x j l= = =  (4) 
 
In equations (3) and (4), 1
qg  and 2, 1,2,..., )jqg j l=（  are 
lower dimensional functions (i.e., nl <  and ,2, )q jm n< . 
A natural approach to approximate a hierarchical 
function )(XG  is to use a matching hierarchical structure, 
that is, to design a function )(XH  to have the same 
hierarchical structure as )(XG . In the next section, we will 
explain the structure of the hierarchical fuzzy neural 
networks used to approximate the hierarchical function 
)(XG . 
 
B. Constructing a hierarchical fuzzy neural network 
(HFNN) with the merged fuzzy neural networks 
(Merged-FNNs) 
Let ))(),...,(()( 121 XHXHHXH M +=  be the hierarchical 
fuzzy neural network (HFNN) matching the hierarchical 
function )(XG , where )(XH  consists of several 
merged-FNNs [7]. Each merged-FNN of the HFNN (i.e 
)(XH q ) is used to approximate one of the hierarchical 
functions )(XGq , where 1,...,1 += Mq . Fig. 1 is the 
configuration of the merged-FNN, which is a two level 
fuzzy neural network with a hierarchical structure. Each 
subsystem of the merged-FNN is a typical fuzzy-neural 
network.  
For the qth merged-FNN, the ith fuzzy IF-THEN rule is 
given by equation (5) at the bottom of this page, where 
,j
q
2FNN  ),...,2,1;,...1( ljMq ==  represents the FNN in 
Level 2, 1FNNq  is the FNN in Level 1, 
),...,1( ,2,
,2,
jq
jq
k mkA =  and ),...,1(
1, lkAqk =  are fuzzy sets 
and jqw ,2,  and 1,qw are singleton fuzzy sets. In the rest of 
this paper, we adopt A  to represent jqkA
,2, and 1,qkA , 
 
which neglects the number of levels and subsystems. That 
is, A  stands for the entire antecedent fuzzy set no matter 
what the level and subsystem are. By using product 
inference, center-averaging and singleton fuzzification, the 
output of FNN1 at Level-1 is 
 
)(
)(
)(
21,1,
1 1
,2
1 1
,21,
)(
1
1
1
q
qTq
h
i
l
k
k
qA
h
i
l
k
k
qA
q
i
q Yw
y
yw
y
q
i
j
q
i
k
ϕ
µ
µ
=






=
∑ ∏
∑ ∏
= =
= =       (6) 
 
where Tq
h
qqq
q
wwww ]...[ 1, )(
1,
)2(
1,
)1(
1,
1=
 is the weighting vector 
for the merged-FNN in Level 1 and Tl
qqqq yyyY ]...[
,22,21,22
= . 
The outputs of ),...,2,1;,...1(FNN2 ljMq,jq == , at Level-2,  
which are the inputs of 1FNNq  at Level-1, can be 
expressed as 
 
)(
)(
)(
,2,2,,2,
1 1
,2
1 1
,2,2,
)(
,2
,2
,2,
,2
,2,
k
q
jqTjq
h
i
m
k
k
qA
h
i
m
k
k
qA
jq
i
j
q Xw
X
Xw
y j
q jq
i
k
j
q jq
i
k
ϕ
µ
µ
=






=
∑ ∏
∑ ∏
= =
= =    (7) 
 
where Tjq
h
jqjqjq
j
q
wwww ]...[ ,2, )(
,2,
)2(
,2,
)1(
,2,
,2=
 is the weighting 
vector for the merged-FNN in Level 2, 1
qh  and 
j
qh
,2 are 
the number of fuzzy rules in Level-1 and Level-2, 
respectively, 
jqm ,2,  represents the number of input 
variables for the jth FNN at Level-2, 
TTlqTqTqTqq wwwww ]...[ ,,,, 2,2,22,11=  is an adjustable 
parameter vector for the qth merged-FNN, and )(1, ⋅qϕ  and 
Fig. 1 the configuration of the qth merged-FNN 
Subsystem(      )
Subsystem(      )
Subsystem(     )
1,2
qh
2,2
qh
j
qh
,2
1,2
qX
2,2
qX
j
qX
,2
… 1
qh
…
Subsystem(      )l
qh
,2l
qX
,2
Subsystem(      )
1,2
qy
2,2
qy
j
qy
,2
l
qy
,2
1
qy…
…
_____________________________________________________________________________________



11,
)(
11,,21,
1
1,2
2,2,
)(
,2,2,,2,,2,
1
,2,
1)(
FNNfor,isthen is  and ... andisIF 
FNNfor,isthen isand... andisIF 
   :
)()(
)(
,2,,2,
)(
q
q
iq
q
l
l
q
q
q
,j
q
jq
i
j
q
jq
m
jq
m
jqjq
i
q
wyAyAy
wyAxAx
R
ii
i
jqjq
i
               (5) 
3) Crossover 
Crossover is an operation to generate a new chromosome 
(i.e., child) from two parent chromosomes. The one-point 
crossover used in our paper is illustrated in Fig. 2. One 
point is randomly selected for dividing one parent. The set 
of genes on one side (each side is chosen with the same 
probability) is passed from one parent to the child, and the 
other genes are placed in the order of their appearance in 
the other parent. In Fig. 2, genes 1x , 2x  and 3x  are 
inherited from Parent 1, and the other genes (i.e. genes 
nxxx ,...,, 54 ) are placed in the order of their appearance in 
Parent 2. 
 
 
4) Mutation 
Mutation is an operation to change the order of n genes 
in each chromosome generated by a crossover operator. 
Such a mutation operation can be viewed as a transition 
from a current solution to a neighboring solution in the 
local search space. We adopt the adjacent two-gene change 
shown in Fig. 3. The adjacent three genes to be changed are 
randomly selected. 
 
 
 
5) Random grouping 
After mutating, a random process is used to construct the 
structure of the qth merged-FNN. As Fig. 4(a) shows, we 
randomly divide l  groups form the thγ chromosome, 
where the genes in group j means the input variables j
qX
,2  
for the jth subsystem in Level-2 of the qth merged-FNN. 
The corresponding merged-FNN is shown in Fig. 4(b). 
 
6) Fitness function  
The fitness function is defined as follows: 
 
2,...,1,)(1
1
κγγγ =+
=
sE
fitness           (14) 
 
 ∑
=
−=
p
k
kykysE
1
21
1 ))()(()(
γ is the thγ  error function, 
where p  is the number of training data pairs. 
 
 
7) Sorting 
The newly generated population is sorted by ranking the 
fitness of chromosomes within the population, resulting in 
)()()( 221 κsEsEsE ≤≤ " . The first chromosome of the 
sorted population TsssS ][ 221 κ"=  has the highest fitness 
value (or smallest error). 
 
B. Optimizing the HFNN with RGA 
The reduced-form genetic algorithm (RGA) [4] was 
proposed to deal with a vast number of adjustable 
parameters in the fuzzy-neural networks. The key 
properties of the RGA are as follows: 1. It uses a 
sequential-search-based crossover point (SSCP) method to 
determine a better crossover point is determined. 2. The 
population size is fixed and can be reduced to a minimum 
size of 4. 3. The crossover operator is simplified to be a 
single gene crossover. After GA_FSP, the HFNN usually 
contents a vast number of adjustable parameters, so we 
adopt RGA to optimize the HFNN. However, we don’t use 
single gene crossover, because single gene crossover limits 
the speed of convergence during the learning process. 
Instead, we use one point crossover. 
 
C. Recursive learning method 
Unfortunately, a problem comes along with the 
hierarchical structure (2). That is, the representation of the 
two-level hierarchical structure, which is composed of 
1+M  continuous functions )(XGq , is so complicated 
that the function )(XH  matching )(XG  will tend to be 
non-useful. Hence, a recursive learning method (from 
1=q  to 1+M ) is applied to solve this problem. We 
3x 4x 5x 6x 7x 8x 9x10x 11x 12x1x 2x
Group 1 … Group l
Group j
…
nx…
Group 2
(a) 
Fig. 4 (a) the random grouping (b) the corresponding 
structure of the merged-FNN 
Subsystem(      )
Subsystem(      )
Subsystem(     )
1,2
qh
2,2
qh
j
qh
,2
T
q xxxX ][ 321
1,2
=
T
q xxX ][ 104
2,2
=
7
,2 xX jq =
… 1
qh
…
Subsystem(      )l
qh
,2T
n
l
q xxX ]...[ 12
,2
=
Subsystem(      )
1,2
qy
2,2
qy
j
qy
,2
l
qy
,2
1
qy…
…
(b)  Fig. 2 One-point crossover operator 
…
Parent 1
Parent 2
Child
Crossover point
1x 2x 3x 4x 5x 6x 7x 8x 9x 10x 11x 12x nx…
1x 2x 3x 4x 5x 7x 8x9x 11x12x10x 6x nx…
4x 5x 10x 2x 8x6x7x 1x 9x 3x 11x12x nx…
…
 Fig. 3 A mutation operator: Arbitrary three-gene change 
Mutation point Mutation point
3x 4x 5x 6x 7x 8x 9x 10x 11x 12x1x 2x
1x 2x 3x 4x5x 6x7x 8x9x 10x 11x 12x
nx…
nx…
  
 
 
 
 
 
 
Fig. 11 Output of the HFNN trained by two-stage GA when 
training data pair is 80. 
Fig. 10 Error curve of HFNN trained by two-stage GA 
when training data pair is 80.
Fig. 6 Error curve of HFNN trained by two-stage GA when 
training data pair is 100. 
the next day’s 
closing price 
the output of the 
HFNN
Fig. 7 Output of the HFNN trained by two-stage GA when 
training data pair is 100. 
Fig. 8 Error curve of HFNN trained by two-stage GA when 
training data pair is 90. 
Fig. 9 Output of the HFNN trained by two-stage GA when 
training data pair is 90 
