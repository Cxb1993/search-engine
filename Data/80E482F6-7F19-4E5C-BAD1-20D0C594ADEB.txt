 I
中文摘要 
 
 本報告之內容包含過去三年來本實驗室執行「流形學習在電腦視覺及多媒體相關應用
之研究」的主要研究成果。在計畫的第一年度，我們以探討流形學習的理論及方法為主，
並著重在能有效地利用旁資訊(side information)以增進學習效能。我們提出了一個新的影像
表示方式 glocal image representation，能適當地分別表示出影像全域以及區域的特徵，在後
續的處理過程中，大幅降低流形分析的難度，並且搭配我們所提出的學習方法，能夠只利
用少量的訓練資料即學得資料在其流形上的相對關係。另外我們亦建立一可利用相關回饋
(relevance feedback)資訊的演算法，同時利用影像本身的內容以及使用者在線所給予的少量
標記資料，實際完成一內容式影像檢索系統(content-based image retrieval)。此系統可以在線
(online)快速地學習到資料的意涵流形(semantic manifold)，而不需花費大量人力將整個資料
庫內容標記出不同的意涵(semantics)或類別。  
  
計畫中第二個年度我們在流形學習的理論及方法上做進一步的探討改進，提出了一個學習
方法，利用核技巧(kernel trick)及核調準(kernel alignment)技術，加上我們所設計的馬可夫隨
機場(Markov random fields)，可以在影像流形上不同之處選擇適當的影像特徵組合，因此可
同時處理在一般情況下常是分開處理的影像分類問題，例如在影像類別中包含有材質、人
造物、或是生物影像等。另外我們亦試著從一個更基本的角度來探討如何改進流形學習方
法。我們探討流形學習上關於區域性的概念，重新定義資料鄰居與區域性距離量測的關係，
並利用最近一個新發展的關於排序(ranking)的技術，來為每個訓練樣本排序其鄰居以做為
區域學習之用，同時並訓練出一距離量測最符合這些鄰居與樣本間的關聯性。我們將其應
用到物件類別辨識的問題上，可得到目前最好的辨識效果並能改進現有的區域性學習方法。 
 
計畫的第三年度我們亦提出兩項研究成果。在第一項成果裡，我們建立了一套將多核學習
(multiple kernel learning)融入於流形分析過程中的演算法。在一方面，由於演算法立基於多
核學習，因此資料的多種不同特徵表示可以同時被考慮；在另一方面，演算法是實現在圖
形嵌入(graph embedding)的模型上，因此任何流形學習的方法，一旦它能為圖形嵌入的模型
所表示，我們的演算法就可將多核學習融入於其中。在第二項的成果中，我們將流形的概
念，從資料點延伸至分類器，並發展一套植基於這個概念的多任務提昇(multi-task boosting)
演算法，來解決第二年度的研究裡，區域學習(local learning)所可能導致的過適(overfitting)
與計算量過高等問題。 
 
關鍵詞：電腦視覺、流形學習、物件識別、區域性學習 
 
 
 III
目錄 
 
摘要………………………………………………………………………………..………I 
 
一、前言……….……………………………………………………………….………….………1 
 
二、研究目的……….……………………………………………………….…………….………1 
 
三、文獻探討……….…………………………………………………….……………….………2 
 
四、結果與討論…….………………………………………………….……….………….……..11 
 
1. 有效影像距離之學習.................…………...……………………………….……..……11 
 
2. 半監督式流形學習.................…………...………………………………….……..……15 
 
3. 結合不同影像特徵之區域性合成核學習.………………...……………….……..……17 
 
4. 利用排名問題來改進流形學習中的區域性學習………………………….……..……20 
 
5. 融入多核學習於流形分析......…………...………………………...……….……..……23 
 
6. 分類器流形之學習..…………...…………………………………………………..……30 
 
五、計畫成果自評…………………………………………………………….………….……..37 
 
六、參考文獻……………………………………………….…………………………….……..38 
 
會議心得報告及論文……………………………………………………………………………43 
 
 
 
 2
如在許多電腦視覺或多媒體等相關領域的實際應用中，因訓練資料的取得不易，或人工標
示訓練資料之類別的成本過高，我們通常僅會擁有少量的具類別標示的訓練資料，而同時
大部分的資料可能並無類別標示資訊。在此情況下一般監督式(supervised)學習演算法由於
所需資訊不足將大幅降低其準確性。而改善的關鍵即在於需能充份有效地利用少量有類別
標示的訓練資料，並結合不具類別資訊的訓練資料本身的特性，發展出新的演算法。在計
畫的第一年我們即針對此議題發展出新的流形學習演算法，並分別以有效影像距離學習，
與半監督式設定下之內容式影像檢索作為二個實際應用，測試其可行性。 
 
在計畫的第二年我們延續第一年中有關於流形學習理論與方法的研究成果，將流形學
習的概念與現有其他機器學習的法做結合，並將之應用在實際的電腦視覺問題上面，如多
類別物體的影像辨識等。鑑於近年來相關問題的發展趨勢，我們特別著重於在影像變異性
極大的情況下，如何正確地辨識出不同種類的物體。其中影像的變異性可能源自於不同種
類影像間性質差異過大，例如交通工具及手寫文字的辨識，在既有的文獻中，其所適用的
影像特徵(image feature)即大為不同[8], [76]；或是在同樣的一類物體中，因為光線、環境、
姿勢、或是影像擷取方式的不同，使得同一種類甚至是同一物體的影像產生極大差異。在
這類情況下不論是影像特徵的選取或是物體影像距離的量測，目前單一適用於整體影像空
間的方法都難達到良好的辨識效果。有鑑於此，我們即考量以區域性學習方法在影像資料
流形上來探討影像辨識問題。我們從兩個角度分別來進行，其一是利用機器學習方法，在
影像資料流形上不同位置學習不同的影像特徵組合來做影像辨識工作。另一方面，我們則
從如何學習影像流形空間上的距離測度著手，設法改善現有的區域性距離量測方式，在避
免預先人工指定區域性範圍的情況下學習進行區域性學習。 
 
在計畫的第三年我們繼續推廣流形學習的理論，以期使計畫前兩年之研究成果更具一
般性且能被運用於不同的複雜問題中。我們發展一套演算法，讓許多流形學習的方法(包括
監督式的與非監督式的)，皆能成功地被運用在以多種特徵表示(feature representation)之資料
的分析；另外，我們進一步將流形的概念，從資料點(data points)延伸至分類器(classifiers)，
並發展一套植基於這個概念的 boosting 演算法，來解決一般流形研究裡，區域學習(local 
learning)所可能導致的過適(overfitting)與計算量過高二個主要問題。 
 
 
三、 文獻探討 
 
流形(manifold)的定義為``a topological space that is locally Euclidean(i.e., around every 
point, there is a neighborhood that is topologically the same as the open unit ball in RD)’’。從定
義上可知，流形為一拓撲空間(topological space)，其在空間中任一區域性(local)空間為歐氏
空間(Euclidean space)，亦即為距離空間(metric space)，而歐氏距離(Euclidean distance)可適
用於其區域性空間距離衡量。歐氏空間本身即為流形空間中最簡單的一個例子，因為歐氏
空間不管是區域性(local)或全域性(global)範圍的空間均為歐氏空間。 
 
在一個流形空間中，給予有限個資料點後，我們可假設這些資料點延伸在一個維度為
d(在這裡 d 遠小於原全域空間之維度 D)的平滑子流形(smooth sub-manifold)中，流形學習的
 4
1
s.t.  0,   ( ) 
      1
ij j i
l
ijj
w if x x
w


 
  
         很明顯地，這是個解最小平方誤差(least square)的問題。 
3. 固定步驟 2 所解出的最佳 w，解下列的最佳化問題：   li jlj iji ywy1 21 ||||  
其中變數 iy 就是資料點 ix 降維後所投影到的位置，在假設所有 iy 的平均是 0 向量，變
異是 I 的情況下，此最佳化問題可經由特徵值問題解出。在步驟 2 中，針對每個資料
點 ix ，使用變數 w 的第 i 列去記錄其與其相鄰點 )( ix 的鄰近關係，在步驟 3 中，使用
低維度的變數 iy 去近似 ix 在高維度中的鄰近關係。［圖 二］顯示人臉資料集，經 LLE
演算法所學習出的二維子空間。 
 
圖 二：使用 LLE 在人臉資料中所學習出的子空間。圖片引用自[79]。 
  
（a） （b） 
  
（c） （d） （e） 
圖 一：（a） & （b） 針對人臉與數字資料集，Isomap 所學習出的二維子空間。 （c） Swiss 
roll 的在三維空間中的分佈，藍線為二點間的 geodesic distance。（d） 所建立的鄰近關係圖，
紅線為二點間的最短路徑。 （e） 使用 MDS 學習出的二維子空間。圖片引用自[96]。 
 6
的技術。在 SNE 中，給定流形空間中的資料 lxxx ,...,, 21 ，對任一對資料均定義如下的
機率 
  

ik ik
ij
ij
d
d
p
)exp(
)exp(
2
2
 where 2
2
2
||||

ji
ij
xx
d
  
針對每一筆資料點 ix ，我們用 iy 去表示 ix 在低維度空間中的座標，並定義以下的機率
式： 
  

ik ki
ji
ij
yy
yy
q
)||||exp(
)||||exp(
2
2
. 
直覺地，若我們希望高維度空間中資料點間的性質可以被保持的越好， ijp 與 ijq 的分
佈上自然也是越接近越好，因此作者以 Kullback-Leibler 距離的總合 (sum of 
Kullback-Leibler divergences)作為目標函式，以共軛梯度下降為方法去解最佳解問題，
目標函式定義如下： 
     li lj
ij
ij
ij
l
i ii q
p
pqpKL
1 11
log)||( , 
將上式對變數 iy 微分雖然十分地複雜，但推導簡化後的結果卻相當簡單又合乎直覺： 
  lj jiijjiijji qqppyy1 ))((2 . 
SNE 就是以這樣的方式，對每一變數 iy 進行共軛梯度下降，並以遞迴的方式去執行至
收斂。相較前面所介紹的三個流形學習演算法，雖然 SNE 的學習過程最為耗時，但由
於學習出的子空間擁有機率型式上的涵意，因此有著許多後續的應用與研究。［圖 四］
(a)為 SNE 針對數字辨識資料上學習出的子空間，(b)為針對 NIPS(Neural Information 
Processing Systems)這個學術會議中，歷年來作者與作者間研究領域的差異性，將之投
射至二維空間，圖中越近的作者表示質研究領域越相近，是一個十分有趣的例子。 
 
（a） （b） 
圖 四：（a） SNE 針對數字資料上所學習出的子空間。（b） 針對 NIPS 這個會議，作
者與作者間研究領域的差異圖。圖片引用自[49]。 
 8
變動因素可能為數極少，如在影像空間中旋轉的人臉其變動維度可能只有一維，即是攝影
機的旋轉角度。［圖 五］顯示了二組不同旋轉角度的人臉影像，由於變動的維度只有旋轉
角度一項，在高頻率的拍攝下，二組資料應該在高維度空間(維度為影像的像素數量)中，各
自形成一維的連續曲線。作者以這個觀點去說明，流形空間的定義是十分符合人類的視覺
系統。 
 
我們在本計畫中即探討了流形學習應用在不同的電腦視覺問題上時的效果，包括了人
臉辨識、追蹤、多媒體檢索等。以下便將就各應用近年來的發展作一概述。以人臉辨識為
例，要比較兩張人臉影像的差異，牽涉到兩個層面，一是該使用什麼樣的特徵做為比較依
據，另一則是該使用何種分類器來做比較。早期的人臉辨識技術通常使用五官特徵，例如
兩眼距離、鼻子的長度寬度、嘴巴位置等等，但這些數值的取得容易受到影像解析度，雜
訊，以及偵測技術的影響。後來在大多數的研究中則直接使用影像本身做為特徵。在 Brunelli 
& Poggio [14] 的論文中，作者比較了這兩種不同類型特徵在人臉辨識上的效果，他們得到
直接使用影像本身當作特徵來源，可以得到較好的效果的結論。以影像本身為特徵的研究
中，最常被提到的方法之一，是由 Turk & Pentland [100]，以及 Moghaddam & Pentland[69]
提出的 Eigenfaces 方法，其基本原理是將 Principal Component Analysis (PCA) 用在人臉影
像資料上。 PCA 會試圖在較低的維度中找到最好的投影方向，使得影像資料在投影之後的
變異最大、最具代表性。決定投影方向後，每張人臉影像都先經過投影再來比較遠近。這
種做法的好處除了使資料盡量分散外，由於維度降低，計算速度也因此加快。 Eigenfaces 方
法之後，另一個被廣泛採用的技術是 Belhumeur et al. 提出的 Fisherfaces [5]方法。這個方
法主要是把統計中的 LDA 應用在人臉影像辨識上。 在這裡 LDA 方法會假設每個人的人
臉資料在影像空間中各自形成高斯分佈，然後類似 PCA 方法，它也嘗試在較低的維度中找
到適當的投影方向，但是要求的條件更多；LDA 要求資料經過投影後，同一類資料要盡量
靠近，而不同類的資料彼此要離得越遠越好。於是資料在經過這樣的投影之後，能夠更容
易的被線性平面依照類別分開。在大多數實驗中都發現，使用 Fisherfaces 來做人臉辨識所
得到的效果會比使用 Eigenfaces 更好。Etemad & Chellappa [26]、Swets & Weng [95]、以及 
Moghaddam et al. [68] 等人也曾提出類似的方法，使用在人臉辨識問題上。另外在統計學習
理論中，SVMs 技術也是近來相當常用且效果很好的分類方法。因此，Heisele et al. [45]把 
SVM 分類技術用在人臉辨識問題上，開發出基於 SVM 的人臉辨識系統，可以辨識半側面
及有旋轉的人臉，而且可以每秒達到 15 個畫面的執行速度。隨著 SVM 方法備受重視，各
種以Kernel為基礎的學習理論也陸續被提出。 PCA 和 LDA 兩種方法都被已拓展成Kernel
的形式，皆由 Mika et al. [66]所發表，分別稱作 Kernel PCA 以及 Kernel LDA 方法。同樣
的，這兩種方法也被應用到人臉辨識問題上，開發出了  Kernel Eigenfaces 和  Kernel 
Fisherfaces 兩項技術。Kernel 形式的方法是想像先將資料用非線性函數映到更高維度，然
後在高維度空間中以線性平面來分類；使用 Kernel 的好處在於可將高維度中的內積轉換成
低維度中容易計算的 Kernel 函數，因此不需真正去算高維度空間的內積。另外， Bartlett [4]
採用 Independent Component Analysis (ICA) 概念，應用 Infomax 方法來作人臉辨識。 
 
在動態的視訊追蹤方面，國內外的相關研究亦相當豐富，而綜觀視訊追蹤系統發展的
過程，其中以貝氏機率模型為主要架構(Bayesian framework)的方法，在許多研究中都扮演
了重要的角色。其中最為有名的要算是 Isard 與 Blake [52]所提出的 CONDENSATION 演算
 10
供的線索中，去學習出使用者的搜尋意涵，而是在於如何有效率地從資料庫提取出相關資
料。另外在處理多媒體資料時，資料本身並不具電腦可判讀的意涵，在資料數量十分龐大
的情況下，使用人工去標記資料的意涵幾乎是不可行的方法，此時自動化地以資料本身內
容為基礎的特徵(content-based feature)去描述物件就成了取而代之的作法。當檢索介面通常
無法直接地跟意涵相關連時，例如影像、錄像以本身的視覺特性為特徵向量的表示，超文
件以多型態的特徵向量呈現，均無法直接跟意涵作關連，這種情況又被稱意涵隔閡(semantic 
gap)，此時在檢索這個應用中，最主要的挑戰便是如何從使用者所提供的少量線索(如尋索
集 query set)中，去找出使用者意涵，再根據學習出的意涵，從資料庫存取相關資料。 
 
建構於相關回饋的檢索有許多相關的研究文獻，如[41], [42], [51], [82], [83], [97], [98]，
在這些文獻中，檢索型式都是以 QBE(Query-By-Example)進行，也就是使用者檢索時，是
以一個物件為例子當成尋索線索。我們以這些研究的發表時間點為序，分敘如下。 
 
調整使用者尋索例子物件特徵向量是實現相關回饋學習模式的一個方法，在 Rui et al. 
[82], [83]的論文中，便是於特徵向量每一維度中設定一權重(weight)，藉由正負相關回饋例
子(relevance and irrelevance feedback examples)作為更新權重(re-weighted)的依據，再使用學
習後的權重去加權計算歐氏空間的每一維度，作為物件間衡量距離的標準。Ishikawa 等人[53]
的論文中，是以移動(moved)尋索例子物件特徵向量作為回饋學習方式，移動的方向與強度
是經由解最佳化問題而決定。在 Porkaew 與 Chakrabarti [77]的論文中，則認為不管是更新
權重或移動特徵向量的方式，均難以去表示較複雜的正負相關回饋例子，因此他們建議可
由分裂(expanded)尋索例子，以多個尋索例子的型態去模擬正負相關回饋例子的分佈。隨著
機器學習領域的蓬勃發展，支持向量機(SVMs)與提昇演算法(boosting)亦可被應用於相關回
饋學習模式下，最著名的例子為 Tieu 與 Viola [97]使用 AdaBoost 去學習一個區隔正負相關
回饋例子的決策邊界(decision boundary)，作為檢索相關內容的依據，Tong 與 Chang [98]以
類似的作法，使用支持向量機去學習決策邊界。在實驗結果上顯示，這一類使用分類器作
為檢索依據的方式，效果通常比調整尋索例子物件特徵向量的方式來得好，但卻也有嚴重
的限制，因為支持向量機與提昇演算法所學習出的分類器，往往需要較多的訓練資料，才
能學習出良好的決策邊界。這違反相關回饋學習模式下，只有少量訓練資料的原則。 
 
在最新的檢索研究趨勢上，流形學習是另一個可行的方式，不同以上所述二類主流方
式，基於流形學習的檢索模式通常同時考慮了二種訊息來源，一為資料在資料空間中的內
部結構(intrinsic structure)，一為使用者所提供正負相關回饋例子；在某種程度上，第一種訊
息似乎可用於彌補少量資料原則上的限制。X. He [42]使用其所發表的 LPP 演算法於影像檢
索上，J. He [41]則使用流形排序(manifold ranking)[116]演算法。這二個方法雖然開了流形學
習在檢索應用上的先例，但仍舊有其缺點，首先以二種訊息來源的數量和重要性的嚴重不
對稱情況為例，這類問題中資料內部結構訊息數量多但相對不重要，正負相關回饋例子數
量少但卻十分重要，但論文[41]中未考慮這個不對稱性，因此檢索效果並不理想；另一方面，
在執行相關回饋時，系統在反應時間上須為即時(real-time)系統，在 X. He 的論文[42]中，
尋索例子必須在加入鄰近關係圖後(也就是必須計算尋索例子與資料庫中所有資料在高維
度空間中的距離)，再執行流形學習，這增加了系統的反應時間，同時也限制了一些計算量
大但效果良好的距離函數的使用，如 EMD(Earth Mover’s Distance)[80]。 
 12
的 3h 。若對 A做探討我們可以發現 A的行向量空間都是由影像 AI 的區域性影像特徵
(local image features)所組成的；而 A的每一個列向量中的元素皆來自於影像 AI 中不同 
的小方塊，因此 A列向量空間可說是由影像 AI 的全域性特徵所組成。於是我們很簡單
地便建立一個新的影像表示法，同時表現出原本影像的全域及區域性質，而且由於我
們分別將全域及區域性質表現在行向量及列向量空間上，在概念上我們將面對的是會
是兩個維度較小的向量空間。例如在我們之後的實驗中，一張影像的大小為 3333 ，
而我們設定 3h (即 121)3/33()3/33(,933  nd )，在流形學習的過程中，不
論是處理 9 維的空間或是 121 維的空間都會比原本 10893333  維的空間來的簡單許
多，這不但可以避免掉常見的維度的詛咒(the curse of dimensionality)的問題，也可因
為只需較少的訓練資料而可大幅增加應用的範圍。接下來我們將描述如何可利用
glocal 表示法在只有少量的訓練資料情況下快速地學習到影像資料流形上的影像距離
測度(metric)。 
 
學習影像測度(Image Metrics)：我們提出了一個方法可以利用少許帶有旁資訊(side 
information)的訓練資料習得影像的流形空間並取得其上的距離測度。假定我們有一些
成對的訓練資料    ', SSII BA  ，其中 S 與 'S 是互斥的兩個集合，在 S 中每組成對的
影像皆為彼此相似的影像，在 'S 中的每組成對影像則為彼此不相似的影像： 
 
 ' ' '
,  and  are similar,
                  and
, '  and  are disimilar.
A B A B
A B A B
I I S I I
I I S I I
 
 
 
令矩陣 , d nA B R  為兩個相近的 glocal 表示矩陣，則給定任意兩個矩陣  d lU R l d 
以及  n mV R m n  ，我們都可以定義一個影像測度為： 
 
 
, ; ,
                      
T T
F
T
F
A B U V U AV U BV
U A B V
  
  。 
我們並且希望影像測度  可以表現出和旁資訊一致的性質，即當  ,A BI I S 時， 
 
圖 六： AI 為原始影像， A為經轉換後之 glocal 影像表示矩陣。 
 14
 
 
 
 
 
 
 
 
 
圖 八：同類的影像資料在數值上可能差異極大。 
表格 1：相對於不同影像測度的最近鄰居分類方法之正確率 
Method Dimensions Number 
of Pairs
ARlight ARlight+exp ARall YaleB Caltech ARlight 
Gender
Euclidean 1089  98.51 64.04 80.44 13.90 40.00 10.80 
RCA 80 10 44.50 52.20 67.32 10.03 26.56 8.87 
BiGL(1) 2x40 10 23.08 33.13 52.44 6.78 16.72 5.57 
BiGL(2) 2x40 10+1 24.39 35.16 55.44 5.71 15.20 5.38 
RCA 150 10 38.03 44.81 61.16 7.81 20.72 7.45 
BiGL(1) 3x50 10 18.91 27.16 44.83 3.17 13.60 4.70 
BiGL(2) 3x50 10+10 20.24 29.17 48.26 4.45 13.52 4.21 
RCA 80 20 23.24 35.41 57.26 5.75 13.20 5.80 
BiGL(1) 2x40 20 17.34 24.83 44.68 4.54 8.32 5.93 
BiGL(2) 2x40 20+20 15.15 24.07 41.87 3.70 8.32 4.66 
RCA 150 20 20.65 31.05 51.57 5.01 11.20 5.21 
BiGL(1) 3x50 20 14.72 22.35 37.48 2.81 8.08 4.55 
BiGL(2) 3x50 20+20 14.13 21.89 34.89 2.81 6.64 3.91 
 
 
 
圖 九：一些本實驗中用到的影像資料。 
 16
,
1,  if ,
0,  otherwise,
i jP
i j
x F x F
W
     
 
,
1,  if ,
1,  if ,
0,  otherwise,
i j
N
i j i j
x F x F
W x F x F
 
 
      
 
相對於 SG 用於記錄無類別標示訓練資料間的關係， PG 與 NG 記錄具類別標示訓練
資料間的二種關係。 
3. 將 n維的影像空間映至 l維( nl  )的意涵流形(semantic manifold)：可藉由解廣義
特徵值問題(generalized eigenvalue problem)求得。現將廣義特徵值問題列於下 
vXXLvXLLX TSTPN   ][ , 
其中 SSS WDL  ，而 SD 為一對角矩陣，  j SjiSii WD ,, ，另一方面，我們也相似
地定義 PL 與 NL 。而 是用於控制回饋集合中的正與負相似關係的權重，一般來說，
在影像檢索的應用中，正相似關係通常比較重要，所以我們令 1 。在上述的廣
義特徵值問題中，我們找出相對於最大 l 個特徵值的特徵向量 lvvv ,...,, 21 ，並令
]...[ 21 lvvvV  為將影像空間投影至的意涵流形的投影矩陣。最後，我們將每一張資
料庫中的影像投影至意涵流形，亦即 iTi xVz  。 
4. 在意涵流形上進行檢索：當使用者給定任一影像 x ，將其投影至意涵流形中
xVz T ，再使用歐式距離(Euclidean distance)與最近鄰居原則，將離 z 最近的數張
影像，作為檢索的結果。 
 
實驗結果：我們使用 Corel 影像資料庫來測試，並使用廻圈式相關回饋，亦即不斷地
增加具標示類別資料的數量(一次增加八個，四個屬於 F ，四個屬於 F )，我們將檢
索的精確度(precision)與回饋廻圈的關係記錄於［圖 十］中。其中三個子圖分別代表
使用三種不同的影像表示方式，每個子圖中，則以三種方法進行比較，LPP(Locality 
Preserving Projection)[42]是由 He 所提出的半監督式流形學習演算法，ARE 是由我們
所提出的演算法，kernel ARE 是 ARE 的 kernelization 版本。從圖中，我們可得知隨著
具標示類別資料數量的增加，檢索的精確度在三種不同的半監督式流形學習演算法中
均能有所提昇，相較於 LPP，ARE 使用三個圖形分別去記錄資料的內部結構關係與回
饋關係(LPP 將所有關係記錄在同一圖形中)，因此有較高的檢索精確度。 
 
圖 十：在回饋廻圈的檢索精確度。 
 18
kernel)，而衡量此一合成核好壞的方式是利用 Cristianini[21]等人提出的一個用 kernel 
alignment 的方法來判斷，假設有 筆訓練資料 ，其中 為
每一個資料的類別標籤，則在[21]中，兩個核心矩陣 和 的相似度可以用下面這個
式子來衡量： 
, 
其中 為 Frobenius 內積(Frobenius inner product)： 
. 
若令 為定義在 上的理想核心(ideal kernel)，則對於 最適於分
類問題的核心 的好壞即可用 來衡量。在本計畫中我們欲處理多類別的影像
分類問題，假設影像資料分別屬於 個類別，而我們要利用 個核心
來合成一個最好的合成核 ，於是將目標核心定義如下： 
  
並最佳化 
  
以求得最好的 。 
 
學習區域性合成核心：我們已經知道如何結合不同的核心成一最好的合成核，但這是
對所有的資料一起做最佳化，忽略了不同的影像可能適用不同的影像特徵來做分類的
事實。因此我們提出一個區域性核心調準(local kernel alignment)方法來進行區域性合
成核學習。我們首先利用區域權重向量 為每一筆訓練資料定義其鄰
居以及區域性的目標核心： 
  
  
其中不同的 表示採用不同的影像特徵。然後我們便可利用前述的方法在每個訓練資
料所在之處學習出一個向量 以之組成區域性合成核 。這樣的做法的缺
點是有可能因為相近的訓練資料太少而產生過適(overfitting)的問題，於是我們提出使
用馬可夫隨機場(MRF)利用相近合成核的交互作用來改善這個問題。作法是先建立一
個圖型模型(graphical model) ：每個訓練資料 會產生一個觀察節點(observation 
node) 及一個狀態節點(state node) ，相對應的 與 間會有一個邊(edge)相連，另外
 20
(四) 利用排名問題來改進流形學習中的區域性學習： 
 
在本報告的前一部分我們著重於以區域性的學習方法在資料流形上學習不同的影像
特徵組合方式；在計畫的這個部分我們則是從另一個角度來探討改進一般流形學習中
的區域性學習方法。區域性(local)是流形學習中的一個重要概念，由於自然影像資料
的複雜性，其在影像空間中的分布常是非線性且難以簡單地被描述，一般流形學習方
法多假設資料在較小的區域中會呈現較簡單的結構而可以被分析，但目前區域性的範
圍該如何訂定則尚無一個明確且一致的作法，較常見的作法為在一個固定的影像特徵
空間選定後，利用歐氏距離選定 個最接近的資料或選擇一個固定距離 內的所有資料
來代表該區域的資料分布性質，但這樣的作法常會因 或 的選取適當與否而影響其學
習效果[17], [44], [79]。另外，一個預先指定的距離量測也可能並不適合利用在所要分
析的資料上來選擇鄰近區域的範圍，雖然有相關研究是著重在學習一最適當的距離量
測，但大多數的研究都是對全域性的資料做分類或是視覺表現(visualization)最佳化，
且多是在人工指定的一個範圍上學習距離量測，本計畫的前一部分亦是利用一預先給
定的距離量測定出每一資料的鄰居。我們觀察到這樣的設定在起始條件給的不好的情
況下仍會影響最後的學習效果［圖 十二］。於是我們提出一個新的方法，引用由 Rudin
所提出”P-Norm Push” [81]排序方法的概念，來學習一區域性距離量測並同時對其鄰居
做排序。我們的方法中會先針對每一個訓練資料分別訓練出一區域性的距離量測，再
在不變動每一距離量測所給予的鄰居排序情況下，對所有的距離量測一起做調整，使
得它們的距離量測值能被放在一起做比較。我們的方法避免了預先以人工指定鄰居的
缺點並且使得在每一訓練資料附近所得的距離量測及鄰居排序有更高的相關性。 
 
鄰居排序在分類問題中的應用：我們利用一個影像分類問題來說明所提出來的方法。
我們採用近年來廣被使用的一個影像資料庫，Caltech-101[29]，其中包含了一百零一
類不同生物及人造物的影像，加上一個背景(background)影像類別［圖 十三 (a)］。
Caltech-101 不只在影像類別間有極大的變異性，在同類別之中的不同影像也因不同的
姿勢、光源、表現方示(繪畫或照像)、背景等原因而有極大的變異性［圖 十三 (b)］
在這樣的情況下區域性的演算法常能較全域性的演算法有更好的效果。接下來我們先
簡短介紹 P-Norm Push”演算法，再詳細說明如何運用在區域性學習上： 
 
圖 十二：在(a)中紅色的楕圓形及在(b)中藍色的楕圓形分別代表了以黑色方塊為中心，依照兩個不同的
距離量測所畫出的一條等高線。而這兩種距離量測是利用不同的區域範圍內的訓練資料所訓練出來，
我們可以發現(a)中的距離量測試圖描述大多數的資料分布情形，但在小尺度上，(b)中的距離量測則能
更準確的描述資料分布。 
 22
   其中 代表所有被標為正的資料集合，而 代表所有被標為負的資料的集合。 
 
2. 利用排序方法學習區域性量測：由於在探討影資料流形時，我們通常只針對每個
資料與附近的資料分布做分析，因此當學習一個區域性距離量測時，我們只需要
其在較近的距離時能準確描述資料間的關係，而對較遠的資料則可不需有太精確
的量測。在影像分類問題中，即是當我們在影像資料 上學習一區域性距離量測
時，會希望被 指為與 較近的資料能盡量與 處於同一個類別，而較遠的資料則
無需理會。若是將 看作是一個排序函式，而距離 愈近的資料會被排在愈前面，
此時我們可發現這個問題與前面”P-Norm Push”部分所描述的問題是極為類似的。
在訂定區域性距離量測的目標函式前，我們先說明一下這裡所採用的距離函式的
形式，一個距離函式 是由數個基本距離函式(elementary distance function) 所組
成： 
 。 
為了簡化算式，我們以向量式表示如下： 
 。 
然後對於每一個與 不同的類別的影像 ，以一函式記錄其於 的距離(以 量測)
小於多少個與 同類別的影像到 的距離： 
 。 
最後我們可以訂出一個目標函式： 
 ， 
使得我們所要學的距離函式 會讓比較接近 的影像都是處於與 相同的類別。 
 
3. 距離函式選取：在前一節中我們並沒有詳細說明距離函式是什麼。由於本計畫中
採用了 bag of feature 的形式來表示一張影像[61], [22], [103], [68]，因此距離函式的
定義可以簡單的依照影像特徵的不同來給定，我們使用的影像特徵包含 Geometric 
Blur 以及顏色直方圖，均可以一固定長度的向量表示，而任兩個影像區塊的差異
性即由其上的影像特徵經歐氏距離決定。一個基本距離函式定義的距離 則是
上的第 個影像區塊與 上所有影像區塊間最小的距離。 
 
4. 重塑(reshape)距離函式：目前為止我們所學習到的每一個距離函式 雖然適於為
選取鄰居，但在得到一個新的無標籤的測試資料時，我們將無法根據現有的區域
性距離函式來為新的影像挑選其鄰居，因為這些距離函式是分開學習的。於是我
們利用以下這個目標函式來一起調整所有的距離函式，使其值能互相比較： 
 ， 
 24
同的平台下進行： 
k(xi;xj) =
MX
m=1
¯mkm(xi;xj); ¯m ¸ 0 or ;
K =
MX
m=1
¯mKm; ¯m ¸ 0 :
 
其中合成系數¯m就可直覺地理解為第m種特徵表示的重要性。 
 
多核學習[39], [57], [78], [94]是目前機器學習領域中一個重要的研究題目，它是指同時
使用多個核函數或核矩陣，去學習一個核機器(kernel machine)，如SVMs (support vector 
machines)或 GP (Gaussian processes)，一般來說，在此情況下訓練出來的核機器會以下
列的形式來表示： 
f(x) =
NX
i=1
®iyik(xi;x) + b
=
NX
i=1
®iyi
MX
m=1
¯mkm(xi;x) + b:
 
多核學習的任務就是對係數f®igNi=1與f¯mgMm=1進行最佳化，而我們的目標就是將多核
學習融入於流形分析的訓練過程中。 
 
流形學習演算法的一致表示式： 
 
由於許多流形學習演算法都在處理資料點兩兩之間的關係，因此這些演算法通常可被
圖狀結構(graph structure)所表示。在 2007 年，Yan 等人[112]發表了一個降維分析的一
致表示式，名為圖形嵌入，其式子如下： 
v¤ = arg min
v>XDX>v=1, or
v>XL0X>v=1
v>XLX>v;
 
其中X = [x1 x2 ¢ ¢ ¢ xN ]為資料矩陣，L與L0為 graph Laplacian，D是一個對角矩陣，而
v¤就是要進行最佳化而得到的投影向量，而這個模型亦等同於下式 
min
v
NX
i;j=1
jjv>xi ¡ v>xjjj2wij
subject to
NX
i=1
jjv>xijj2dii = 1; or
NX
i;j=1
jjv>xi ¡ v>xjjj2w0ij = 1:
 
其 中 affinity matrix W = [wij ] 與 graph Laplacian L 之 間 的 關 係 為
L = diag(W ¢ 1)¡W ，同樣的關係亦存在於 affinity matrix W 0 = [w0ij ]與 graph 
Laplacian L0之間。從這個式子，我們可以理解到圖形嵌入使用圖狀結構來處理投影後
的資料點(v>x)，兩兩之間的關係。 
 
 26
min
A
trace(A>S¯WA)
subject to trace(A>S¯W 0A) = 1
where
S¯W =
PN
i;j=1 wij(K(i) ¡K(j))¯¯>(K(i) ¡K(j))>;
S¯W 0 =
PN
i;j=1 w
0
ij(K(i) ¡K(j))¯¯>(K(i) ¡K(j))>: 
因此變數A = [®1 ¢ ¢ ¢ ®P ]之行向量的最佳解，可由經解下列特徵值問題而得到： 
S¯W ® = ¸S
¯
W 0®: 
 
2. 固定A，最佳化¯，則原有限制之最佳化問題可表示成下式： 
min
¯
¯>SAW ¯
subject to ¯>SAW 0¯ = 1 and ¯ ¸ 0
where
SAW =
PN
i;j=1 wij(K(i) ¡K(j))>AA>(K(i) ¡K(j));
SAW 0 =
PN
i;j=1 w
0
ij(K(i) ¡K(j))>AA>(K(i) ¡K(j)): 
額外的限制式¯ ¸ 0讓這個問題不再能用特徵值問題來解，而這個問題就是標準的
non-convex QCQP (quadratically constrained quadratic programming)，通常我們都是
解它的 SDP (semi-definite programming) relaxation [102]，它的 SDP relaxation 如下所
列： 
min
¯;B
trace(SAWB)
subject to trace(SAW 0B) = 1;
e>m¯ ¸ 0; m = 1; 2; :::; M;·
1 ¯>
¯ B
¸
º 0;
 
並用 SDP [102]來解出最佳之¯。 
 
我們將發展的演算法稱為 MKL-DR，用以代表這個方法是將多核學習(MKL)融合在降
維(或流形)分析(DR)的訓練過程。於［圖 十四］中我們總結整個演算法的流程，包括
訓練與測試，圖中上半部為訓練流程，下半部為測試流程。在訓練的過程裡，演算法
的輸入為核矩的集合與一個能用圖形嵌入模型所表示的流形學習方法，經個了交替式
最佳化過程，最佳化後的變數A與¯則傳入測試過程，在測試時，一張影像的所有特
徵表示會同時被使用，經一連串昇維與結合後，最終被投影至所訓練出的低維空間中。 
 28
實驗結果： 
 
為了驗證 MKL-DR 的效能，我們將其套用在 Caltech-101 資料庫[29]上來進行效能衡
量，在應用方面，我們分別選擇了監督式的物件識別(supervised object recognition)與
非監督式的影像分群(unsupervised image clustering)。 
 
Caltech-101 資料庫包含了 101 類的物件與 1 類背景影像，所以共有 102 個類別，在監
督式的物件識別的應用中，所有 102 類的資料均會被使用，我們在每一類別中隨機取
出 30 張影像，15 張用於訓練，15 張用於測試；在非監督式的影像分群的應用中，我
們採用 Dueck 與 Frey [25]等人的設定，在其中僅 20 類的影像會被使用到。我們這個
資料庫呈現在［圖 十五］裡，在圖中，102 類的資料，我們每一類取一張作為代表，
所有類別均使用在監督式物件識別之應用中，而在紅框中的 20 類，則在非監督式的
影像分群之應用中使用。 
 
我們共使用七種不同的特徵表示來形容這個資料庫中的影像，因此有七個核矩陣衍生
而出，我們將其簡稱和介紹分述如下： 
GB-1/GB-2：對應到 geometric blur (GB) descriptor [8]所產生的特徵表示與其變形。 
SIFT-Dist/SIFT-Grid：對應到 scale invariance feature transform (SIFT) descriptor [61]所產
生的特徵表示與其變形。 
C2-SWP：對應到 Serre 等人[89]所發明的特徵表示方式。 
C2-ML：對應到 Mutch 與 Lowe [71]所發明的特徵表示方式。 
PHOG：對應到 pyramid histogram of oriented gradients (PHOG) descriptor [11] 所產生的
特徵表示。 
 
最後我們將實驗結果分述如下： 
 
1. 監督式的物件識別 
 
在這個監督式的應用中，我們選用監督式的流形學習演算法 LDE [17]，我們比較了
 
圖 十五：Caltech-101 資料庫。共 102 類，每一類取一張影像顯現，所有類別均使用在監督式物件識別
之應用中，而在紅框中的 20 類，則在非監督式的影像分群之應用中使用。 
 
 30
 
(六) 分類器流形之學習： 
 
在今年度的第二項成果裡，我們將流形的概念，從資料流形(data manifold)延伸至分類
器流形(classifier manifold)。也就是說，我們可先根據分類器的參數建立一個分類器空
間(classifier space)，在這個空間中的每一個點，都代表著一個特定的分類器，在某些
電腦視覺的應用中會同時需要多個具高度相關性的分類器，如多個局部物件追蹤器
(multi-part object tracker)或多類型人臉偵測器(multi-view face detector)，而這些分類
器，極可能會因為其間的高度相關性，因而在分類器空間中形成一個流形結構
(manifold structure)，我們稱之為分類器流形。 
 
這類電腦視覺應用的文獻中，這些分類器大多是一個個獨立學習出來的，不但費時，
且可能因忽略其間的相關性，而導致效能未能如預期，在本計畫的研究裡，我們提出
了一個有效率且可同時得到這些分類器的方法，簡單地來說，我們將多個獨立分類器
學習過程，轉換成一個多目標學習問題(multi-task learning problem)，而這個多目標學
習問題，就是直接完成分類器流形的學習(classifier manifold embedding)，不但可以加
速學習過程，且可以引入適當的正規項(regularization term)，以提高分類器的正確性。 
 
具體來說，我提供了一套植基於這個概念的多目標提昇(multi-task boosting)演算法，
去完成分類器流形的學習，並以之來解決前一年本研究計畫裡，區域學習(local learning)
所可能導致的過適(overfitting)與過高計算量等二個問題。以下我們就針對這項研究成
果進行說明。 
 
區域學習的問題： 
 
物件識別在目前電腦視覺領域中，是一個極具挑戰性的問題，而造成物件識別困難的
最主要因素，就是同類別的物件之間，存在很大的變異性(large intra-class variation)，
在［圖 十六］中，我們可看出在人臉與飛機這個二個類別中，同類別影像之間所存
表格 5：各種不同的方法在 Caltech-101 資料庫的分群效能 (NMI / ERR (%)) 
affinity propagation 
kernel(s) preprocessing method without data 
preprocessing 
with data 
preprocessing 
GB-1 0.553 / 50.8 0.609 / 38.3 
GB-2 0.577 / 48.0 0.624 / 43.7 
SIFT-Dist 0.627 / 43.7 0.651 / 31.0 
SIFT-Grid 0.598 / 41.3 0.631 / 45.7 
C2-SWP 0.383 / 70.3 0.379 / 60.5 
C2-ML 0.499 / 54.5 0.488 / 56.0 
PHOG 
KLPP 
0.455 / 57.3 0.482 / 52.7 
All MKL-LPP     - 0.714 / 25.0 
 32
 
最直接達成區域學習的方法就是獨立地進行一個個的區域分類器之學習，雖然合理，
但這個方式，至少會有以下兩個缺點。第一，要學習的區域分類器的數量等同於訓練
資料的數量，這會造成一個過高的訓練成本。第二，我們只用一部分的訓練資料(C  out 
of N )去學習區域分類器，如再搭配到較複雜的分類器模型，如 SVMs，則過適問題會
有比較高的發生可能性。為了避免這兩個缺點的發生，在這項研究成果中，我們會將
多個獨立區域分類器學習過程，轉換成一個多任務學習問題。 
 
我們觀察到在許多分類器的模型下，分類器是由一組可調控之參數所決定，如 SVMs
模型下的分類器，是由所有 support vector 上的權重與一門檻值(threshold)所定義： 
f(x) =
NX
n=1
®nynk(xn;x)¡ b; 
又例如在提昇演算法模型下的分類器，是由所有弱學習器(weak learner)的權重所定義： 
f(x) =
jHjX
t=1
®nht(x): 
因此我們可以想像這些分類器模型分別衍生出各自的分類器空間，在空間中的每一個
點，就代表著一個特定分類器，而這個分類器空間的維度，在 SVMs 模型的架構下是
(N + 1)，在提昇演算法模型的架構下是所有可能的弱學習器之總數。 
 
在這項研究中，我們假設所有區域分類器都是經由提昇演算法所學習出(boosted 
classifier)，所以這N個分類器ffigNi=1將會對應到分類器空間中的N個點。而我們的觀
點是：這N個點將會在分類器空間中形成一個流形狀的結構，至於原因可由［圖 十七］
來說明，首先我們考慮二個相鄰訓練資料xi與xi0，他們各自的C-NN將會有高度的重
複，因此，區域分類器fi與fi0會因為訓練資料的類似，而存在高度相似的分類行為，
所以我們覺得fi與fi0會處在分類器空間中相近的位置，把這個概念延伸至全部資料
點，所有的區域分類器將在分類器空間H中形成一個流形狀的結構。 
 
a b 
圖 十七：(a) 二個相鄰訓練資料xi與xi0，以及他們各自的鄰近範圍。(b) 所有的區域分類器將在
分類器空間H中形成一個流形狀的結構。 
 34
經過迴圈式的學習過程，所有區域分類器ffigNi=1的訓練將同時被完成，由於流形的概
念被融入在訓練的過程中，可以彌補因訓練資料量少而造成的過適問題，在另一方
面，由於分類器的共同學習，他們之間的重複性，如一筆訓練資料同時出現在多個區
域訓練集中或一個弱學習器的好壞程度在多個區域分類器間十分類似，可以被妥善地
利用，而大幅降低訓練時間。最後我們將所提出多任務提昇演算法的流程列在［圖 十
八］中。 
 
 
 
實驗結果： 
 
為了衡量所提出的方法之效能，我們將它應用在物件識別的任務上，並分別使用二個
標竿資料庫，Caltech-101 與 Pascal VOC 2007，進行識別率的計算，分述如下： 
 
1. Caltech-101 資料庫 
 
如前所述，Caltech-101 資料庫包含 102 類的影像，我們從每個類別中，隨機取出 30
 
圖 十八：多任務提昇演算法的流程。 
 36
表格 7：三種不同的方法，在多核同時使用的情況下之識別率(mean ± std %)與訓練時間。 
kernels SimpleMKL Local AdaBoost Ours 
All 
74.3 ± 1.2 
3:26£ 102 sec. 
74.6 ± 1.3 
1:87£ 105 sec. 
75.8 ± 1.1 
3:92£ 103sec. 
 
最後，我們將所提出的方法在Ntrain = 5; 10; 15; 20; 25的設定下而得到的識別率畫於［圖 
十九］中，在圖中同時也呈現其他所發表的系統之識別率，包括[8], [10], [11], [29], [35], 
[40], [58], [71], [89], [114]。 
 
 
圖 十九：數個已發表的系統，在 Caltech-101 資料庫上，使用不同數量的訓練資料
而得到的識別率。 
 
2. Pascal VOC 2007 資料庫 
在 Pascal VOC 2007 資料庫上，共有六個核矩陣被用來形容資料，我們將這些核矩陣
分列如下： 
SIFT / GB / SS / GIST / C2-ML：建立的方式如前所述。 
TC-SIFT：分別在 RGB 三個色系使用 SIFT descriptor 所建立的核矩陣。 
 
我們以平均精確度(average precision)來衡量系統的效能，表格 8 記錄了六種不同方法
所得到的效能，其中 INRIA，XRCE，TKK 為參加 VOC 2007 前三名的參賽隊伍，另
外三個方法分別是[101]，SimpleMKL 與我們的方法。 
 
 38
六、參考文獻 
 
[1] V. Athitsos, J. Alon, S. Sclaroff, and G. Kollios, “BoostMap: a Method for Efficient Approximate Similarity Rankings,” 
CVPR, vol. 2, pp. 268–275, 2004. 
[2] C. Atkeson, A. Moore, and S. Schaal. Locally Weighted Learning. Artificial Intelligence Review, 1997. 
[3] S. Avidon, Support Vector Tracking, CVPR01, vol. 1, pp. 184-191, 2001. 
[4] M. S. Bartlett, H. M. Lades, and T. J. Sejnowski. Independent Component Representations for Face Recognition. Proc. of 
SPIE, volume 2399, pages 528–539, 1998. 
[5] P. Belhumeur, J. Hespanha, and D. Kriegman. “Eigenface vs. Fisherfaces: Recognition using Class-specific Linear 
projection,” IEEE Trans. Pattern Analysis and Machine Intelligence, 19(7):711–720, 1997. 
[6] M. Belkin and P. Niyogi. “Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering,” In Neural 
Information Processing Systems, 2001. 
[7] Y. Bengio, J.-F. Paiement, P. Vincent, O. Delalleau, N. Roux, and M. Ouimet. “Out-of-sample Extensions for LLE, Isomap, 
MDS, Eigenmaps, and Spectral Clustering,” In Neural Information Processing Systems, 2003. 
[8] A. Berg, T. Berg, and J. Malik.  Shape matching and object recognition using low distortion correspondences. In CVPR, 
pages 26–33, 2005. 
[9] S.T. Birchfield, Elliptical Head Tracking Using Intensity Gradients and Color Histograms, Proc. Conf. Computer Vision 
and Pattern Recognition, pp. 232–237, Santa Barbara, CA, 1998. 
[10] O. Boiman, E. Shechtman, and M. Irani. In Defense of Nearest Neighbor based Image Classification. In CVPR, 2008. 
[11] A. Bosch, A. Zisserman, and X. Munoz. Image Classification Using Random Forests and Ferns. In ICCV, 2007. 
[12] Y. Boykov, O. Veksler, and R. Zabih.  Fast approximate energy minimization via graph cuts. PAMI, 23(11):1222–1239, 
2001. 
[13] G.R. Bradski, Computer Vision Face Tracking for Use in a Perceptual User Interface, Intel Technology Journal, 1998. 
[14] R. Brunelli and T. Poggio, Face Recognition: Features versus Templates, IEEE Trans. PAMI, vol. 15, no. 10, pp. 
1042–1052, October 1993. 
[15] D. Cai, X. He, and J. Han. Semi-supervised Discriminant Analysis. In ICCV, 2007. 
[16] C. Carson, M. Thomas, S. Belongie, J. Hellerstein, and J. Malik. “Blobworld: A System for Region-based Image Indexing 
and Retrieval,” In Visual Information Systems, pages 509–516, 1999. 
[17] H.-T. Chen, H.-W. Chang, and T.-L. Liu. “Local Discriminant Embedding and Its Variants,” In Int’l Conference on 
Computer Vision and Pattern Recognition, pages II: 846–853, 2005. 
[18] D. Comaniciu, V. Ramesh, and P. Meer, Real-Time Tracking of Non-Rigid Objects using Mean Shift, Proc. Conf. 
Computer Vision and Pattern Recognition, vol. 2, pp. 142–149, Hilton Head Island, South Carolina, 2000. 
[19] T. Cormen, C. Leiserson, R. Rivest, and C. Stein. Introduction to Algorithms. The MIT Press, 2nd edition, 2001. 
[20] T. Cox and M. Cox. Multidimentional Scaling. Chapman & Hall, London, 1994. 
[21] N. Cristianini, J. Shawe-Taylor, A. Elisseeff, and J. Kandola. On kernel-target alignment. In NIPS, 2001. 
[22] G. Csurka, C. Dance, L. Fan, J. Willamowski, and C. Bray. Visual categorization with bags of keypoints. In ECCV, 2004. 
[23] J. Dai, S. Yan, X. Tang, and J. Kwok. Locally Adaptive Classification Piloted by Uncertainty. In ICML, 2006. 
[24] C. Domeniconi and D. Gunopulos. Adaptive Nearest Neighbor Classification Using Support Vector Machines. In NIPS, 
2001. 
[25] D. Dueck and B. Frey. Non-metric Affinity Propagation for Unsupervised Image Categorization. In ICCV, 2007. 
[26] K. Etemad and R. Chellappa, “Face Recognition Using Discriminant Eigenvectors,” ICASSP96, 1996. 
[27] M. Everingham, L. V. Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 
 40
[52] M. Isard and A. Blake, “Contour Tracking by Stochastic Propagation of Conditional Density,” Proc. Fourth European 
Conf. Computer Vision, vol. 1, pp. 343–356, Cambridge, England, 1996. 
[53] Y. Ishikawa, R. Subramanya, and C. Faloutsos. “Mindreader: Querying Databases through Multiple Examples,” In 
International Conference on Very Large Data Bases, pages 218–227, 1998. 
[54] I. T. Jolliffe. Principal Component Analysis. Springer-Verlag, New York, 1986. 
[55] Y. Ke, R. Sukthankar, and L. Huston. “Efficient Near-Duplicate Detection and Sub-image Retrieval,” In ACM Conference 
on Multimedia, pages 869–876, 2004. 
[56] T.-K. Kim and J. Kittler. Locally linear Discriminant Analysis for Multimodally Distributed Classes for Face Recognition 
with A Single Model Image. PAMI, 2005. 
[57] G. Lanckriet, N. Cristianini, P. Bartlett, L. Ghaoui, and M. Jordan. Learning the Kernel Matrix with Semidefinite 
Programming. JMLR, 2004. 
[58] S. Lazebnik, C. Schmid, and J. Ponce. Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene 
Categories. In CVPR, 2006. 
[59] E. Levina and P. Bickel. Maximum Likelihood Estimation of Intrinsic Dimension. Neural Information Processing Systems 
17, 2004. 
[60] Y.-Y. Lin, T.-L. Liu, and C.-S. Fuh. Local Ensemble Kernel Learning for Object Category Recognition. In CVPR, 2007. 
[61] D. Lowe. “Distinctive Image Features from Scale-invariant Keypoints,” Int’l Journal of Computer Vision, 60(2):91–110, 
2004. 
[62] W.-Y. Ma and B. Manjunath. “Netra: A Toolbox for Navigating Large Image Databases,” In Multimedia Systems, volume 
7, pages 184–198, 1999. 
[63] S. Mahamud and M. Hebert, “Minimum Risk Distance Measure for Object Recognition,” ICCV, pp. 242–248, 2003. 
[64] T. Malisiewicz and A. Efros. Recognition by Association via Learning Per-exemplar Distances. In CVPR, 2008. 
[65] A.M. Martinez and R. Benavente, “The AR Face Database,” Tech. Rep. CVC Technical Report #24, Computer Vision 
Center at the U.A.B, June 1998. 
[66] S. Mika, G. Ratsch, J. Weston, B. Scholkopf, and K.-R. Muller. Fisher Discriminant Analysis with Kernels. Neural 
Networks for Signal Processing IX, pages 41–48, 1999. 
[67] K. Mikolajczyk and C. Schmid. “A Performance Evaluation of Local Descriptors,” In Int’l Conference on Computer Vision 
and Pattern Recognition, pages 275–263, 2003. 
[68] K. Mikolajczyk, B. Leibe, and B. Schiele.  Local features for object class recognition. In ICCV, pages II: 1792–1799, 
2005. 
[69] B. Moghaddam and A. P. Pentland, Probabilistic Visual Learning for Object Representation, IEEE Trans. PAMI, vol. 19, 
no. 7, pp. 696–710, July 1997. 
[70] B. Moghaddam and G. Shakhnarovich. Boosted Dyadic Kernel Discriminants. In NIPS, 2002. 
[71] J. Mutch and D. Lowe. Multiclass Object Recognition with Sparse, Localized Features. In CVPR, 2006. 
[72] S.G. Narasimhan, C.Wang, and S.K. Nayar, “All the Images of an Outdoor Scene,” ECCV, vol. 3, pp. 148–162, 2002. 
[73] A. Ng, M. Jordan, and Y. Weiss. On Spectral Clustering: Analysis and An Algorithm. Neural Information Processing 
Systems 14, 2001. 
[74] A. Oliva and A. Torralba. Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope. IJCV, 
2001. 
[75] A. Opelt, M. Fussenegger, A. Pinz, and P. Auer. “Weak Hypotheses and Boosting for Generic Object Detection and 
Recognition,” In Euro. Conference on Computer Vision, pages 71–84, 2004. 
[76] G. Pass, R. Zabih, and J. Miller. “Comparing Images using Color Coherence Vectors,” In ACM Conference on Multimedia, 
 42
[103] V. Vapnik. Statistical Learning Theory. Wiley, 1998. 
[104] M. Varma and A. Zisserman.  A statistical approach to texture classification from single images. IJCV, 62(1-2):61–81, 
2005. 
[105] K. Wagstaff, C. Cardie, S. Rogers, and S. Schr¨odl, “Constrained K-Means Clustering with Background Knowledge,” 
ICML, pp. 577–584, 2001. 
[106] J. Wang, J. Liu, and G. Wiederhold. “Simplicity: Semantics-sensitive Integrated Matching for Picture Libraries.,” IEEE 
Trans. Pattern Analysis and Machine Intelligence, 23(9):847–963, 2001. 
[107] E. W. Weisstein et al. Geodesic. From MathWorld--A Wolfram Web Resource. 
http://mathworld.wolfram.com/Geodesic.html. 
[108] L. Wolf and A. Shashua, “Feature Selection for Unsupervised and Supervised Inference: the Emergence of Sparsity in a 
Weighted-Based Approach,” ICCV, pp. 378–384, 2003. 
[109] Y. Wu and T.S. Huang. A Co-Inference Approach to Robust Visual Tracking. In Proc. Int. Conf. Computer Vision, pages II: 
26–33, Vancouver, Canada, 2001. 
[110] E.P. Xing, A.Y. Ng, M.I. Jordan, and S.J. Russell, “Distance Metric Learning with Application to Clustering with 
Side-Information,” NIPS 15, pp. 505–512, 2002. 
[111] Yang, J., et al., Two-Dimensional PCA: A New Approach to Appearance-Based Face Representation and Recognition. IEEE 
Transactions on Pattern Analysis and Machine Intelligence, 2004. 26(1): p. 131-137. 
[112] S. Yan, D. Xu, B. Zhang, H. Zhang, Q. Yang, and S. Lin. Graph Embedding and Extensions: A general Framework for 
Dimensionality Reduction. PAMI, 2007. 
[113] L. Zelnik-Manor and P. Perona. Self-Tuning Spectral Clustering. Neural Information Processing Systems 16, 2004. 
[114] H. Zhang, A. Berg, M. Maire, and J. Malik. Svm-knn: Discriminative Nearest Neighbor Classification for Visual Category 
Recognition. In CVPR, 2006. 
[115] J. Zhang, S. Z. Li, and J. Wang. Manifold Learning and Applications in Recognition. In Intelligent Multimedia Processing 
with Soft Computing. Springer-Verlag, 2004. 
[116] D. Zhou, J. Weston, A. Gretton, O. Bousquet and B. Schölkopf. Ranking on Data Manifolds, Neural Information 
Processing Systems 16, 2003. 
[117] W. Zhu, S. Wang, R.S. Lin, and S. Levinson, “Tracking of Object with SVM Regression,” Proc. Conf. Computer Vision and 
Pattern Recognition, vol. 2, pp. 240–245, Kauai, Hawaii, 2001. 
[118] J. Zhu, S. Rosset, H. Zou, and T. Hastie. Multi-class Adaboost. Technical report, Dept. of Statistics, University of Michigan, 
2005. 
 
(a) (b) (c) (d) (e)
Figure 2. Some examples of intraclass variations: Consider the Wild Cat class in Caltech-101. (a) A typical instance of a wild cat for
reference. (b) A wild cat in a different pose. (c) A wild cat under a different lighting condition. (d) A drawing of wild cat with some
unnatural features. (e) A wild cat example in which over 60% of the image belongs to the background.
more precisely explaining the relationship between the sam-
ple and its high-ranked neighbors (whose ranks are given by
the distance function itself). These distance functions are
learned independently to fit local properties of each sam-
ple, and are subsequently reshaped altogether to avoid data
overfitting and achieve a unified effect. Classifying a test-
ing sample can then be conveniently done through these dis-
tance functions to find neighbors in training data which are
very likely in the same class with this testing sample.
Throughout this work we focus on the classification
problem of visual object categories, and show that the pro-
posed method can compete with other related techniques as
well as the ability to improve them. The paper is organized
as follows. In Section 2 we discuss related work, and then
explain the P-Norm Push formulation in Section 3. The de-
tails of our approach are presented in Section 4, while some
experiments and discussions are included in Section 5.
2. Related work
Despite a long history of research on visual object clas-
sification and its related problems, some noteworthy rapid
progress has been made in recent years, especially on those
applications involving numerous object classes. For exam-
ple, the first reported recognition rate in 2004 on the popular
102-class image dataset Caltech-101 is 17% [6]. In 2006 a
number of methods, e.g., [7, 26], have boosted the accuracy
rate to near 60%. More recently, the march of recognition
rate on this dataset is now around 87% [23].
Among the many efforts on solving this particular appli-
cation, one important issue is often discussed: the intraclass
variations due to the sparsely distributed objects for describ-
ing a general concept. In Figure 2, we show some exam-
ples of intraclass variations, which are caused by differ-
ent poses, lighting conditions, representations (painting or
photographing), and backgrounds. These variations make
the visual object classification a very challenging problem,
since it is hard to find an overall applicable criterion to sep-
arate objects from different classes. Previous approaches
to addressing intraclass variations generally proceed on two
fronts: one is to develop a good image representation so
that it becomes easier to distinguish different classes by
simple calculations on the representation, and the other is
to learn some sophisticated classifiers by machine learning
techniques. Although the emphasis could be on one or the
other, current methods mostly have their formulations es-
tablished by investigating both the two aspects. After all,
a more meaningful image representation can often improve
the efficiency of a fine classifier, and vice versa.
Representing an image for object categorization appears
frequently in the form of a set of patches. (We also adopt the
representation.) While such a representation is already flex-
ible, adding various types of image feature to record differ-
ent properties of a patch have been further proposed to han-
dle issues caused by, say, the changes in an object’s shape
and pose. The SIFT framework by Lowe [12] is one of
the most successful and widely used appearance feature de-
scriptor. In its original version, Lowe first uses a difference-
of-Gaussian (DoG) function to find scale-space extrema in
an image, and then calculates a 3-D histogram over gradient
locations and orientations within each patch covering an ex-
tremum. Subsequently, Csurka et al. [5] describe a bag-of-
keypoints model, based on the SIFT descriptor, to quantize
the collected features into a finite dictionary, and represent
each image as a histogram over this dictionary. They then
apply the SVM classification method [22] to the new repre-
sentation to achieve better scores than those from applying
SVM to the original image data. While the SIFT feature
seems to be effective in object recognition, related studies
on feature comparisons [14, 20] have pointed out that SIFT
is more conducive in the context of matching than in classi-
fication. On the other hand, the shape descriptor geometric
blur (GB) by Berg and Malik [2] has also attracted much
attention lately. Like the SIFT feature, geometric blur cap-
tures the gradient information in a patch. The main differ-
ences to SIFT are that in geometric blur the gradient infor-
mation is blurred according to the distance to the patch cen-
ter, and the information is sampled sparsely. In this work,
the geometric blur is adopted as the main feature on patches,
and the details will be explained later.
Besides finding a robust feature, there are efforts to cor-
relate the relationships between features to generate a more
appropriate image representation, such as the pyramid rep-
neighbors are often assumed to have higher probabilities to
be in the same class with Iℓ. Furthermore, in local learning
for classification, if the meanings of the terms “neighbor,”
“near,” and “far” with respect to Iℓ all come from a learned
distance functionDℓ, we in fact need not to worry about the
exact distances from Iℓ to others, but we do care about the
relative magnitude between distances Dℓ(Im) and Dℓ(In)
where m ∈ C(ℓ) and n /∈ C(ℓ). One can easily check
that this is indeed very similar to the ranking problem if
we also list the samples according to the distances Dℓ(Iℓ′),
ℓ′ ∈ L \ {ℓ}. The only difference is that in a ranking prob-
lem the samples listed in the top portion are with higher val-
ues, but here the samples in the top portion of a neighbor list
are with smaller values. In view of that only a few nearest
neighbors are used, we therefore need to make sure that the
top portion of the neighbor list is correctly constructed. And
this aspect of consideration is identical to the main property
in the P-Norm Push framework.
Similar to the formulation of a distance function de-
scribed by Frome et al. [7], we define the distance function
Dℓ for Iℓ to be a weighted sum of several elementary dis-
tance functions dℓis:
Dℓ(Im) =
Eℓ∑
i=1
wℓidℓi(Im), (4)
where Eℓ is the number of elementary distance functions
introduced on Iℓ, and is indeed the number of features de-
tected in Iℓ. For notation simplicity, we further let wℓ be the
weight vector [wℓ1 wℓ2 · · · wℓEℓ ]T and dmℓ denote the vec-
tor [dℓ1(Im) dℓ2(Im) · · · dℓEℓ(Im)]
T
. Thus equation (4)
can be rewritten in an inner product form:
Dℓ(Im) = wℓ · d
m
ℓ . (5)
Henceforward what we have to learn is a distance function
parameterized by wℓ to sort samples other than Iℓ. We now
introduce a cost on each sample not in the same class with
Iℓ (cf. the Height function defined for negative samples in
[15]), and the definition is given by
Cost(In:n/∈C(ℓ)) =
∑
m∈C(ℓ)
1[wℓ·dmℓ ≥wℓ·dnℓ ] . (6)
That is, the cost of a particular sample In:n/∈C(ℓ) is the num-
ber of samples that are in the same class with Iℓ and lo-
cated further than In according to Dℓ. A sample In:n/∈C(ℓ)
with a large cost is expected to be pushed far away from Iℓ.
Here the price function g(r) = rp in P-Norm Push is again
adopted and the objective function to be minimized is
F (wℓ) =
∑
n/∈C(ℓ)
 ∑
m∈C(ℓ)
1[wℓ·dmℓ ≥wℓ·dnℓ ]
p . (7)
1 2 3 4
5 6 7 8
9 10 11 12
13 14 15 16
17 18
19 20
21 22
(a) (b) (c) (d)
Figure 3. We compute the HSV histograms in 22 overlapping re-
gions in different scales. Each histogram is then normalized by the
total number of pixels in the region.
While in [15] Rudin points out that the indicator function
in (7) can be replaced by any non-negative, monotonically
increasing function which is an upper bound of the indicator
function for easier optimization, in our case we still use the
original objective in that we can quickly neglect useless dℓis
in the optimization algorithm. The details will be explained
in Section 5.1.2.
3.3. Distance function
The Dℓ in (4) is in a general form. If all samples in
an application are of the same size and an elementary dis-
tance function dℓi(Im) just returns the squared difference
between the two respective ith pixels of Iℓ and Im, then
Dℓ is simply a squared Mahalanobis distance with a diago-
nal covariance matrix. However, we choose to represent an
image as a bag-of-features, and set an elementary distance
dℓi(Im) to be the smallest distance between the ith feature
of Iℓ and any detected feature of the same type in Im.
In our experiments we adopt two kinds of features, as
they are often used in related work, e.g., [7, 8, 26]. They
are the geometric blur and HSV color histogram. We fur-
ther apply two different settings to each kind, and obtain
four types of features. As in [8], the two types of geometric
blur features, termed as GB1 and GB2, are extracted under
different scales with radii of 42 and 70 pixels, respectively.
The HSV histograms are also extracted under two schemes:
one is to compute the histograms on some 84 × 84-pixel
patches (sampled as in GB1 and GB2), the other is to com-
pute the histograms in 22 regions extracted with a pyramid
scheme similar to that in [9] (see Figure 3). The two types
of HSV histogram are named as HSV1 and HSV2, respec-
tively. We compute an HSV histogram in the same way as
in [7] and extract the geometric blur features by modifying
the original version1 in [2]. Notice that for GB1, GB2, and
HSV1, the feature-to-set distance is calculated between fea-
tures of the same type; for HSV2, the feature-to-set distance
is from distances between features at the same position.
3.4. Preliminary results
So far we have explained how to learn a distance function
that is suitable for ranking neighbors. To justify the formu-
lation, we carry out an experiment for k-nearest-neighbor
1 http://www.cs.berkeley.edu/∼aberg/demos/gb demo.tar.gz
strictly increasing function q, transforming a distance func-
tion form Dℓ(·) to D˜ℓ(·) = q(Dℓ(·)) would not affect the
objective function (7). We can utilize such a function to re-
shape these learned local distance functions altogether so
that direct comparisons among them can be achieved.
The reshape process is carried out as competitions over
training samples between any two distance functions of dif-
ferent classes. Given a reference sample Iℓ, we prefer that
if Im is in the same class with Iℓ and In is not, then the
reshaped distance D˜m(Iℓ) should be smaller than D˜n(Iℓ).
That is, the competition between D˜m and D˜n for Iℓ depends
on the class labels of Iℓ, Im and In. Furthermore, suppose
we make a sorted list of reshaped distances (in increasing
order) {D˜ℓ′(Iℓ)}ℓ′ 6=ℓ from the training data and the learned
distance functions in Section 3. It is reasonable to pay more
attention to the samples in the top portion, since in testing
a given I , we usually consider those training samples with
smaller distances to I (under the assumption that I is re-
lated to some training image(s) Iℓ). Hence for each fixed
reference sample Iℓ, we can define a cost to each reshaped
D˜n(Iℓ), n /∈ C(ℓ) by
Cost(D˜n(Iℓ)) =
∑
m∈C(ℓ)
1[D˜m(Iℓ)≥D˜n(Iℓ)]. (10)
Although any strictly increasing function could be our re-
shape function, in this work we restrict it to be a scaling
function, q(r) = ar, where a ∈ R+. Now the reshaped
function D˜ℓ can be parameterized by a single scaler aℓ as
D˜ℓ = aℓDℓ and the overall objective function to be mini-
mized is
F˜ (a) =
∑
ℓ∈L
∑
n/∈C(ℓ)
 ∑
m∈C(ℓ)
1[amDm(Iℓ)≥anDn(Iℓ)]
p
′
,
(11)
where a = [a1, a2, ..., a|L|]T , and p′ ≫ 1 acts as the price
function parameter like the p in (7).
5. Experiments
In this section we discuss some implementation details
and the results derived by the proposed neighbor ranking
method. We again consider the Caltech-101 dataset, col-
lected by Fei-Fei et al. [6]. In all our experiments, from each
class, we randomly pick 15 images for training and another
15 images for testing. We then exchange their roles and cal-
culate the average recognition rate. Images with larger sizes
are scaled down to around 60000 pixels while preserving
the aspect ratios. For each image, we extract the four types
of features as described in Section 3.3, and sample at most
400 features respectively for each type of GB1, GB2, and
HSV1. Hence an image is represented as a bag with at most
1200 + 22 features. Since the Caltech-101 dataset contains
BA
CK
G
RO
UN
D_
G
oo
gl
e
Fa
ce
s
Fa
ce
s_
ea
sy
Le
op
ar
ds
M
ot
or
bi
ke
s
a
cc
o
rd
io
n
a
irp
la
ne
s
a
n
ch
or an
t
ba
rre
l
ba
ss
be
av
er
bi
no
cu
la
r
bo
ns
ai
br
ai
n
br
on
to
sa
ur
us
bu
dd
ha
bu
tte
rfl
y
ca
m
e
ra
ca
n
n
o
n
ca
r_
si
de
ce
ilin
g_
fa
n
ce
llp
ho
ne
ch
ai
r
ch
an
de
lie
r
co
u
ga
r_
bo
dy
co
u
ga
r_
fa
ce
cr
a
b
cr
a
yf
ish
cr
o
co
di
le
cr
o
co
di
le
_h
ea
d
cu
p
da
lm
at
ia
n
do
lla
r_
bi
ll
do
lp
hi
n
dr
ag
on
fly
e
le
ct
ric
_g
ui
ta
r
e
le
ph
an
t
e
m
u
e
u
ph
on
iu
m
e
w
e
r
fe
rry
fla
m
in
go
fla
m
in
go
_h
ea
d
ga
rfi
el
d
ge
re
nu
k
gr
am
op
ho
ne
gr
an
d_
pi
an
o
ha
w
ks
bi
ll
he
ad
ph
on
e
he
dg
eh
og
he
lic
op
te
r
ib
is
in
lin
e_
sk
at
e
jos
hu
a_
tre
e
ka
ng
ar
oo
ke
tc
h
la
m
p
la
pt
op
lla
m
a
lo
bs
te
r
lo
tu
s
m
a
n
do
lin
m
a
yf
ly
m
e
n
o
ra
h
m
e
tro
no
m
e
m
in
ar
et
n
a
u
til
us
o
ct
op
us
o
ka
pi
pa
go
da
pa
nd
a
pi
ge
on
pi
zz
a
pl
at
yp
us
py
ra
m
id
re
vo
lv
er
rh
in
o
ro
o
st
er
sa
xo
ph
on
e
sc
ho
on
er
sc
is
so
rs
sc
o
rp
io
n
se
a
_
ho
rs
e
sn
o
o
py
so
cc
e
r_
ba
ll
st
ap
le
r
st
ar
fis
h
st
eg
os
au
ru
s
st
op
_s
ig
n
st
ra
w
be
rry
su
n
flo
we
r
tic
k
tri
lo
bi
te
u
m
br
el
la
w
a
tc
h
w
a
te
r_
lill
y
w
he
el
ch
ai
r
w
ild
_c
at
w
in
ds
or
_c
ha
ir
w
re
n
ch
yin
_y
an
g
 
 
BACKGROUND_Google
Faces
Faces_easy
Leopards
Motorbikes
accordion
airplanes
anchor
ant
barrel
bass
beaver
binocular
bonsai
brain
brontosaurus
buddha
butterfly
camera
cannon
car_side
ceiling_fan
cellphone
chair
chandelier
cougar_body
cougar_face
crab
crayfish
crocodile
crocodile_head
cup
dalmatian
dollar_bill
dolphin
dragonfly
electric_guitar
elephant
emu
euphonium
ewer
ferry
flamingo
flamingo_head
garfield
gerenuk
gramophone
grand_piano
hawksbill
headphone
hedgehog
helicopter
ibis
inline_skate
joshua_tree
kangaroo
ketch
lamp
laptop
llama
lobster
lotus
mandolin
mayfly
menorah
metronome
minaret
nautilus
octopus
okapi
pagoda
panda
pigeon
pizza
platypus
pyramid
revolver
rhino
rooster
saxophone
schooner
scissors
scorpion
sea_horse
snoopy
soccer_ball
stapler
starfish
stegosaurus
stop_sign
strawberry
sunflower
tick
trilobite
umbrella
watch
water_lilly
wheelchair
wild_cat
windsor_chair
wrench
yin_yang 0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 6. Confusion table by 3-NN+neighbor ranking+reshaping.
101 object classes and a background class, we thus obtain
1530 distance functions in the training procedure.
After the reshaping, we use a modified 3-NN classifier
to assign class labels on test data. Specifically, given a test
image I , the classifier first arranges the order of the train-
ing images according to their reshaped distances to I . If
a prediction on the class label of I cannot be determined
by the top three, then the succeeding samples will be con-
sidered one at a time until a decision can be made. We
have tested this method in several settings with different
features. The resulting confusion table is plotted in Fig-
ure 6, and the outcomes are listed in the second column of
Table 1. Despite the use of a simple classifier and basi-
cally two kinds of features, the accuracy rate by our method
achieves 69.83±0.41%, which is better than those reported
in all previous work on the same setting. However, it is still
behind the 87% accuracy rate by Varma and Ray [23], in
which they exploit several other robust features.
We also apply the neighbor ranking to the SVM-KNN
framework [26]. Given a test image I , the reshaped dis-
tances from each training sample to it are calculated first,
and then the k nearest neighbors are determined to train a
multiclass SVM classifier for I . (We use libSVM [3] for the
implementation here.) The overall recognition rate in this
case can be improved to 71.8 ± 0.32%. Table 1 shows the
related results.
5.1. Details and discussions
5.1.1 Price function
The parameters p in (7) and p′ in (11) are the price func-
tion parameters in the P-Norm Push, and some discussions
where B(It) includes the 3 nearest neighbors of It in the
training data.
We obtain similar outcomes with these two settings in
very different k values. With the first setting we get the best
result when k is around 50 and the score does not change
much as k varies in 30 ∼ 60, while with the second we set
k = 7 to achieve the best score. The later result seems that
the classification is dominated by the neighbor selection.
This is probably due to the crudely produced kernel from
the more asymmetric distance functions.
6. Conclusion
We have presented a novel approach to improving lo-
cal learning by incorporating supervised neighbor ranking
in distance function learning. The effectiveness of the pro-
posed technique is demonstrated by dealing with a challeng-
ing multiclass classification problem, visual object catego-
rization. We show that together with a simple k-nearest-
neighbor classifier, our method can yield satisfactory re-
sults, as well as has the ability to improve some existing
localized learning methods, e.g., SVM-KNN [26]. The clas-
sification rates by the simple settings described in our exper-
iments can compete with those in most related work, except
that by Varma and Ray [23], in which several robust features
and a more sophisticated learning scheme are used. As our
framework has the flexibility in easily adopting more fea-
tures in the distance function, we would explore the effects
of adopting those features used in [23] for our future work.
Acknowledgements
This work is supported in part by grants 95-2221-E-001-
031, 96-2221-E-001-021 and 96-EC-17-A-02-S1-032.
References
[1] A. Berg, T. Berg, and J. Malik. Shape matching and object
recognition using low distortion correspondences. In CVPR,
pages I: 26–33, 2005. 5
[2] A. Berg and J. Malik. Geometric blur and template matching.
In CVPR, pages I:607–614, 2001. 2, 4
[3] C.-C. Chang and C.-J. Lin. LIBSVM: a library for
support vector machines, 2001. Software available at
http://www.csie.ntu.edu.tw/
˜
cjlin/libsvm.
6
[4] O. Chum and A. Zisserman. An exemplar model for learning
object classes. In CVPR, pages 1–8, 2007. 3
[5] G. Csurka, C. Dance, L. Fan, J. Willamowski, and C. Bray.
Visual categorization with bags of keypoints. In ECCV,
2004. 2
[6] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative
visual models from few training examples an incremental
bayesian approach tested on 101 object categories. In Pro-
ceedings of the Workshop on Generative-Model Based Vi-
sion, Washington, DC, June 2004. 2, 6
[7] A. Frome, Y. Singer, and J. Malik. Image retrieval and classi-
fication using local distance functions. In NIPS, pages 417–
424. MIT Press, Cambridge, MA, 2007. 1, 2, 3, 4, 5
[8] A. Frome, Y. Singer, F. Sha, and J. Malik. Learning globally-
consistent local distance functions for shape-based image re-
trieval and classification. In ICCV, 2007. 4, 5
[9] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of
features: Spatial pyramid matching for recognizing natural
scene categories. In CVPR, pages II: 2169–2178, 2006. 3, 4,
5
[10] M. Leordeanu, M. Hebert, and R. Sukthankar. Beyond local
appearance: Category recognition from pairwise interactions
of simple features. In CVPR, pages 1–8, 2007. 3
[11] Y.-Y. Lin, T.-L. Liu, and C.-S. Fuh. Local ensemble kernel
learning for object category recognition. In CVPR, pages 1–
8, 2007. 3, 5
[12] D. Lowe. Distinctive image features from scale-invariant
keypoints. IJCV, 60(2):91–110, November 2004. 2
[13] M. Marszaek and C. Schmid. Spatial weighting for bag-of-
features. In CVPR, pages II: 2118–2125, 2006. 3
[14] K. Mikolajczyk, B. Leibe, and B. Schiele. Local features
for object class recognition. In ICCV, pages II: 1792–1799,
2005. 2
[15] C. Rudin. Ranking with a p-norm push. In COLT, pages
589–604, 2006. 1, 3, 4, 7
[16] S. Salzberg, A. Delcher, D. Heath, and S. Kasif. Best-case
results for nearest-neighbor learning. PAMI, 17(6):599–608,
June 1995. 1
[17] B. Scho¨lkopf. The kernel trick for distances. In NIPS, pages
301–307, 2000. 7
[18] M. Schultz and T. Joachims. Learning a distance metric from
relative comparisons. In NIPS, Cambridge, MA, 2004. MIT
Press. 5
[19] G. Shakhnarovich, P. Viola, and T. Darrell. Fast pose es-
timation with parameter-sensitive hashing. In ICCV, pages
750–757, 2003. 1
[20] M. Stark and B. Schiele. How good are local features for
classes of geometric objects. In ICCV, October 2007. 2
[21] E. B. Sudderth, A. B. Torralba, W. T. Freeman, and A. S.
Willsky. Describing visual scenes using transformed dirich-
let processes. In NIPS, 2005. 3
[22] V. Vapnik. Statistical Learning Theory. Wiley, 1998. 2
[23] M. Varma and D. Ray. Learning the discriminative power-
invariance trade-off. In ICCV, October 2007. 2, 6, 8
[24] G. Wang, Y. Zhang, and F. Li. Using dependent regions for
object categorization in a generative framework. In CVPR,
pages II: 1597–1604, 2006. 3, 5
[25] G.-J. Zhang, J.-X. Du, D.-S. Huang, T.-M. Lok, and M. R.
Lyu. Adaptive nearest neighbor classifier based on super-
vised ellipsoid clustering. In FSKD, volume 4223 of Lecture
Notes in Computer Science, pages 582–585. Springer, 2006.
1, 7
[26] H. Zhang, A. Berg, M. Maire, and J. Malik. Svm-knn: Dis-
criminative nearest neighbor classification for visual cate-
gory recognition. In CVPR, pages II: 2126–2136, 2006. 1,
2, 3, 4, 6, 7, 8
（附表四） 
參加國際學術會議會後報告表 
REPORT FOR ATTENDING INTERNATIONAL CONFERENCES  
姓   名 
Name 
張楷岳 
服務單位及職稱 
Institute & Position 
中央研究院資訊科學研究所約聘兼任助理 
會  議  名  稱 
Title  of Meeting 
中文：IEEE 電腦視覺與模型辨識國際會議 
Chinese 
英文：IEEE Computer Society International Conference on 
Computer Vision and Pattern Recognition, CVPR 
English 
日   期 
Date of Conference 
97/6/23-97/6/28 地  點 
Location 
美國阿拉斯加安克拉治 
經  費  來  源 
Source of Travel Funding ■申請補助 Applying for Funding（補助單位 Supported by:
 
國科會計畫–
流形學習在電腦視覺及多媒體相關應用之研究 
(NSC95-2221-E-001-031-MY3)） 
□會方補助 Funded by Conference □自籌 Self-funding
 
Copyright c© 2009 The Neural Information Processing Systems (NIPS).
Dimensionality Reduction for Data in Multiple
Feature Representations
Yen-Yu Lin1,2 Tyng-Luh Liu1 Chiou-Shann Fuh2
1Institute of Information Science, Academia Sinica, Taipei, Taiwan
{yylin, liutyng}@iis.sinica.edu.tw
2Department of CSIE, National Taiwan University, Taipei, Taiwan
fuh@csie.ntu.edu.tw
Abstract
In solving complex visual learning tasks, adopting multiple descriptors to more
precisely characterize the data has been a feasible way for improving performance.
These representations are typically high dimensional and assume diverse forms.
Thus finding a way to transform them into a unified space of lower dimension
generally facilitates the underlying tasks, such as object recognition or cluster-
ing. We describe an approach that incorporates multiple kernel learning with
dimensionality reduction (MKL-DR). While the proposed framework is flexible
in simultaneously tackling data in various feature representations, the formulation
itself is general in that it is established upon graph embedding. It follows that
any dimensionality reduction techniques explainable by graph embedding can be
generalized by our method to consider data in multiple feature representations.
1 Introduction
The fact that most visual learning problems deal with high dimensional data has made dimension-
ality reduction an inherent part of the current research. Besides having the potential for a more
efficient approach, working with a new space of lower dimension often can gain the advantage of
better analyzing the intrinsic structures in the data for various applications, e.g., [3, 7]. However,
despite the great applicability, the existing dimensionality reduction methods suffer from two main
restrictions. First, many of them, especially the linear ones, require data to be represented in the
form of feature vectors. The limitation may eventually reduce the effectiveness of the overall al-
gorithms when the data of interest could be more precisely characterized in other forms, such as
bag-of-features [1, 11] or high order tensors [19]. Second, there seems to be lacking a systematic
way of integrating multiple image features for dimensionality reduction. When addressing applica-
tions that no single descriptor can appropriately depict the whole dataset, this shortcoming becomes
even more evident. Alas, it is usually the case for addressing complex visual learning tasks [4].
Aiming to relax the two above-mentioned restrictions, we introduce an approach called MKL-DR
that incorporates multiple kernel learning (MKL) into the training process of dimensionality reduc-
tion (DR) algorithms. Our approach is inspired by the work of Kim et al. [8], in which learning an
optimal kernel over a given convex set of kernels is coupled with kernel Fisher discriminant anal-
ysis (KFDA), but their method only considers binary-class data. Without the restriction, MKL-DR
manifests its flexibility in two aspects. First, it works with multiple base kernels, each of which
is created based on a specific kind of visual feature, and combines these features in the domain of
kernel matrices. Second, the formulation is illustrated with the framework of graph embedding [19],
which presents a unified view for a large family of DR methods. Therefore the proposed MKL-DR
is ready to generalize any DR methods if they are expressible by graph embedding. Note that these
DR methods include supervised, semisupervised and unsupervised ones.
3.1 Kernel as a unified feature representation
Consider a dataset Ω of N samples, and M kinds of descriptors to characterize each sample. Let
Ω = {xi}
N
i=1, xi = {xi,m ∈ Xm}
M
m=1, and dm : Xm ×Xm → 0 ∪ R+ be the distance function for
data representation under the mth descriptor. The domains resulting from distinct descriptors, e.g.
feature vectors, histograms, or bags of features, are in general different. To eliminate these varieties
in representation, we represent data under each descriptor as a kernel matrix. There are several ways
to accomplish this goal, such as using RBF kernel for data in the form of vector, or pyramid match
kernel [6] for data in the form of bag-of-features. We may also convert pairwise distances between
data samples to a kernel matrix [18, 20]. By coupling each representation and its corresponding
distance function, we obtain a set of M dissimilarity-based kernel matrices {Km}Mm=1 with
Km(i, j) = km(xi,xj) = exp
{(
−d2m(xi,m,xj,m)/σ
2
m
)} (8)
where σm is a positive constant. As several well-designed descriptors and their associated distance
functions have been introduced over the years, the use of dissimilarity-based kernel is convenient in
solving visual learning tasks. Nonetheless, care must be taken in that the resulting Km is not guar-
anteed to be positive semidefinite. Zhang et al. [20] have suggested a solution to resolve this issue.
It follows from (5) and (6) that determining a set of optimal ensemble coefficients {β1, β2, . . . , βM}
can be interpreted as finding appropriate weights for best fusing the M feature representations.
3.2 The MKL-DR algorithm
Instead of designing a specific dimensionality reduction algorithm, we choose to describe MKL-DR
upon graph embedding. This way we can derive a general framework: If a dimensionality reduction
scheme is explained by graph embedding, then it will also be extendible by MKL-DR to handle
data in multiple feature representations. In graph embedding (2), there are two possible types of
constraints. For the ease of presentation, we discuss how to develop MKL-DR subject to constraint
(4). However, the derivation can be analogously applied when using constraint (3).
It has been shown that a set of linear dimensionality reduction methods can be kernelized to nonlinear
ones via kernel trick. The procedure of kernelization in MKL-DR is mostly accomplished in a
similar way, but with the key difference in using multiple kernels {Km}Mm=1. Suppose the ensemble
kernel K in MKL-DR is generated by linearly combining the base kernels {Km}Mm=1 as in (6).
Let φ : X → F denote the feature mapping induced by K . Through φ, the training data can be
implicitly mapped to a high dimensional Hilbert space, i.e.,
xi 7→ φ(xi), for i = 1, 2, ..., N . (9)
By assuming the optimal projection v lies in the span of training data in the feature space, we have
v =
∑N
n=1 αnφ(xn). (10)
To show that the underlying algorithm can be reformulated in the form of inner product and accom-
plished in the new feature space F , we observe that plugging into (2) each mapped sample φ(xi)
and projection v would appear exclusively in the form of vTφ(xi). Hence, it suffices to show that
in MKL-DR, vTφ(xi) can be evaluated via the kernel trick:
v
Tφ(xi) =
∑N
n=1
∑M
m=1 αnβmkm(xn,xi) = α
T
K
(i)β where (11)
α =
 α1..
.
αN
 ∈ RN ,β =
 β1..
.
βM
 ∈ RM ,K(i) =
 K1(1, i) · · · KM (1, i)..
.
.
.
.
.
.
.
K1(N, i) · · · KM (N, i)
 ∈ RN×M .
With (2) and (11), we define the constrained optimization problem for 1-D MKL-DR as follows:
min
α,β
∑N
i,j=1 ||α
T
K
(i)β −αT K(j)β||2wij (12)
subject to ∑Ni,j=1 ||αT K(i)β −αT K(j)β||2w′ij = 1, (13)
βm ≥ 0, m = 1, 2, ...,M . (14)
The additional constraints in (14) are included to ensure the the resulting kernel K in MKL-DR is a
non-negative combination of base kernels. We leave the details of how to solve (12) until the next
section, where using MKL-DR for finding a multi-dimensional projection V is considered.
Algorithm 1: MKL-DR
Input : A DR method specified by two affinity matrices W and W ′ (cf. (2));
Various visual features expressed by base kernels {Km}Mm=1 (cf. (8));
Output: Sample coefficient vectors A = [α1 α2 · · ·αP ]; Kernel weight vector β;
Make an initial guess for A or β;
for t← 1, 2, . . . , T do
1. Compute SβW in (19) and SβW ′ in (20);
2. A is optimized by solving the generalized eigenvalue problem (21);
3. Compute SAW in (23) and SAW ′ in (24);
4. β is optimized by solving optimization problem (25) via semidefinite programming;
return A and β;
On optimizing β: By fixing A, the optimization problem (17) becomes
min
β
β⊤SAW β
subject to β⊤SAW ′β = 1 and β ≥ 0
(22)
where
SAW =
∑N
i,j=1 wij(K
(i) −K(j))⊤AA⊤(K(i) −K(j)), (23)
SAW ′ =
∑N
i,j=1 w
′
ij(K
(i) −K(j))⊤AA⊤(K(i) −K(j)). (24)
The additional constraints β ≥ 0 cause that the optimization to (22) can no longer be formulated as
a generalized eigenvalue problem. Indeed it now becomes a nonconvex quadratically constrained
quadratic programming (QCQP) problem, and is known to be very difficult to solve. We instead
consider solving its convex relaxation by adding an auxiliary variable B of size M ×M :
min
β,B
trace(SAWB) (25)
subject to trace(SAW ′B) = 1, (26)
e
T
mβ ≥ 0, m = 1, 2, ...,M, (27)[
1 βT
β B
]
 0, (28)
where em in (27) is a column vector whose elements are 0 except that its mth element is 1, and the
constraint in (28) means that the square matrix is positive semidefinite. The optimization problem
(25) is an SDP relaxation of the nonconvex QCQP problem (22), and can be efficiently solved
by semidefinite programming (SDP). One can verify the equivalence between the two optimization
problems (22) and (25) by replacing the constraint (28) withB = ββT . In view of that the constraint
B = ββT is nonconvex, it is relaxed to B  ββT . Applying the Schur complement lemma,
B  ββT can be equivalently expressed by the constraint in (28). (Refer to [17] for further details.)
Note that the numbers of constraints and variables in (25) are respectively linear and quadratic to
M , the number of the adopted descriptors. In practice the value of M is often small. (M = 7 in
our experiments.) Thus like most of the other DR methods, the computational bottleneck of our
approach is still in solving the generalized eigenvalue problems.
Listed in Algorithm 1, the procedure of MKL-DR requires an initial guess to either A or β in the
alternating optimization. We have tried two possibilities: 1) β is initialized by setting all of its
elements as 1 to equally weight each base kernel; 2) A is initialized by assuming AA⊤ = I . In
our empirical testing, the second initialization strategy gives more stable performances, and is thus
adopted in the experiments. Pertaining to the convergence of the optimization procedure, since
SDP relaxation has been used, the values of objective function are not guaranteed to monotonically
decrease throughout the iterations. Still, the optimization procedures rapidly converge after only a
few iterations in all our experiments.
Novel sample embedding. Given a testing sample z, it is projected to the learned space of lower
dimension by
z 7→ AT K(z)β, where K(z) ∈ RN×M and K(z)(n,m) = km(xn, z). (29)
Table 1: Recognition rates (mean ± std %) for Caltech-101 dataset
number of classes number of classeskernel(s) method 102 101 method 102 101
GB-1 57.3± 2.5 57.7± 0.7 57.1± 1.4 57.7± 0.8
GB-2 60.0± 1.5 60.6± 1.5 60.9± 1.4 61.3± 2.1
SIFT-Dist 53.0± 1.4 53.2± 0.8 54.2± 0.5 54.6± 1.5
SIFT-Grid 48.8± 1.9 49.6± 0.7 49.5± 1.3 50.1± 0.3
C2-SWP 30.3± 1.2 30.7± 1.5 31.1± 1.5 31.3± 0.7
C2-ML 46.0± 0.6 46.8± 0.9 45.8± 0.2 46.7± 1.5
PHOG
KFD
41.8± 0.6 42.1± 1.3
KLDE
42.2± 0.6 42.6± 1.3
- KFD-Voting 68.4± 1.5 68.9± 0.3 KLDE-Voting 68.4± 1.4 68.7± 0.8
- KFD-SAMME 71.2± 1.4 72.1± 0.7 KLDE-SAMME 71.1± 1.9 71.3± 1.2
All MKL-LDA 74.6 ± 2.2 75.3 ± 1.7 MKL-LDE 75.3 ± 1.5 75.5 ± 1.7
Affinity matrices for LDE: In LDE, not only the data labels but also the neighborhood relationships
are simultaneously considered to construct the affinity matrices W = [wij ] and W ′ = [w′ij ]:
wij =
{
1, if yi = yj ∧ [i ∈ Nk(j) ∨ j ∈ Nk(i)],
0, otherwise, (31)
w′ij =
{
1, if yi 6= yj ∧ [i ∈ Nk′(j) ∨ j ∈ Nk′(i)],
0, otherwise. (32)
where i ∈ Nk(j) means that sample xi is one of the k nearest neighbors for sample xj . The
definitions of the affinity matrices are faithful to those in LDE [3]. However, since there are now
multiple image descriptors, we need to construct an affinity matrix for data under each descriptor,
and average the resulting affinity matrices from all the descriptors.
4.4 Quantitative results
Our experiment setting follows the one described by Zhang et al. [20]. From each of the 102 classes,
we randomly pick 30 images where 15 of them are included for training and the other 15 images
are used for testing. To avoid a biased implementation, we redo the whole process of learning
by switching the roles of training and testing data. In addition, we also carry out the experiments
without using the data from the the background class, since such setting is adopted in some of the
related works, e.g., [5]. Via MKL-DR, the data are projected to the learned space, and the recognition
task is accomplished there by enforcing the nearest-neighbor rule.
Coupling the seven base kernels with the affinity matrices of LDA and LDE, we can respectively de-
rive MKL-LDA and MKL-LDE using Algorithm 1. Their effectiveness is investigated by comparing
with KFD (kernel Fisher discriminant) [12] and KLDE (kernel LDE) [3]. Since KFD considers only
one base kernel at a time, we implement two strategies to take account of the classification outcomes
from the seven resulting KFD classifiers. The first is named as KFD-Voting. It is constructed based
on the voting result of the seven KFD classifiers. If there is any ambiguity in the voting result, the
next nearest neighbor in each KFD classifier will be considered, and the process is continued until
a decision on the class label can be made. The second is termed as KFD-SAMME. By viewing each
KFD classifier as a multi-class weak learner, we boost them by SAMME [21], which is a multi-class
generalization of AdaBoost. Analogously, we also have KLDE-Voting and KLDE-SAMME.
We report the mean recognition rates and the standard deviation in Table 1. First of all, MKL-LDA
achieves a considerable performance gain of 14.6% over the best recognition rate by the seven KFD
classifiers. On the other hand, while KFD-Voting and KFD-SAMME try to combine the separately
trained KFD classifiers, MKL-LDA jointly integrates the seven kernels into the learning process. The
quantitative results show that MKL-LDA can make the most of fusing various feature descriptors,
and improves the recognition rates from 68.4% and 71.2% to 74.6%. Similar improvements can
also be observed for MKL-LDE.
The recognition rates 74.6% in MKL-LDA and 75.3% in MKL-LDE are favorably comparable to
those by most of the existing approaches. In [6], Grauman and Darrell report a 50% recognition
2008 年神經資訊處理系統會議出席報告 
 
報告人姓名(中文)：林彥宇 
          (英文)：Yen-Yu Lin 
服務機構及職稱: 國立台灣大學資訊工程學研究所 博士生 
會議時間：民國 97 年 12 月 8 號至 13 號 
會議地點：加拿大卑詩省，溫哥華 
會議名稱(中文)：第二十二屆神經資訊處理系統會議 
        (英文)：22nd Annual Conference on Neural Information Processing Systems 
(NIPS) 
發表論文(中文)：多類型特徵表示下之資料的降維分析 
( 英 文 ) ： Dimensionality Reduction for Data in Multiple Feature 
Representations 
 
一. 參加會議經過 
這屆神經資訊處理系統會議在加拿大卑詩省的溫哥華和惠斯勒(Vancouver and 
Whistler, British Columbia, Canada)所舉行。溫哥華是一個環境十分優美的城市，
惠斯勒位於溫哥華以北約一百多公里的地方，它是加拿大的一個滑雪勝地，也是
2010 年冬季奧運的舉辨地點。會議的前四天是在溫哥華的凱悅飯店(Hyatt 
Regency)所舉行，內容為 tutorial 與 main conference；後兩天在惠斯勒舉行，內
容為 workshop。 
這屆會議共收到 1016 篇投稿，共 250 篇為會議所接受，錄取率約為 24.6%，其
中 28 篇錄取為口頭報告(oral)，95 篇接收為簡介報告(spotlight)，127 篇接收為
海報貼示報告(poster)，算是一個十分大型的國際研討會，也是機器學習領域中，
最主要的國際會議之一。我們有一篇論文經審查後，錄取為海報貼示報告，論文
的題目為 Dimensionality Reduction for Data in Multiple Feature Representations。與
會者來自世界各地，約五、六百位專家學者，共同探討機器學習和與其相關領域
的研究心得與未來發展趨勢。 
在六天的議程中，第一天包含了六場 tutorial 和部分的主要議程(main 
conference)，接下來的三天是主要議程，最後二天是專題研討會(workshop)。議
題主要是 algorithms and architectures; applications; brain imaging; cognitive science 
and artificial intelligence; control and reinforcement learning; emerging technologies; 
learning theory; neuroscience; speech and signal processing; and visual processing 等
相關或延伸的研究與應用。 
最後二天的 workshop 是在惠斯勒所舉行，這屆共有二十四個不同的 workshop，
有些舉行一天，有些二天。所有 workshop 都是平行進行，我選擇參加的 workshop
為 Kernel Learning: Automatic Selection of Optimal Kernels，因為這是我比較感興
趣的議題，也與這次投稿論文的內容最相關。 
二. 與會心得 
 
在這次的開會過程中，我覺得最大的收獲是在為報告作準備時所得到的知識，及
在會議中所聽到的一些有興趣之議題的相關資訊，和報告時的經驗。雖然這並不
是我第一次在會議上報告，但這是在機器學習之學術會議的第一次，因為我本身
主要的研究是電腦視覺。 
 
對於報告，我事先已演練過許多次，因此在克服了剛開始的緊張後，就順利進入
了狀況。我的論文比較屬於電腦視覺或機器視覺這個領域，雖然這也是屬於 NIPS
的範圍，但它並不算主流的議題，再加上同一時間，有約八十篇 poster 正同時進
行，因此我預期只有少數與會者有興趣花比較多的時間在了解論文中的細節，多
數與會者應該只會有興趣了解論文的主要貢獻，為了反應此一的情況，在海報的
製作上，我試著``可視化＂論文的內容(請見文件所附的海報之中央區域)，並用
簡潔的句子去描述它，另外對於有興趣了解細節的與會者，也準備了較詳細的介
紹版本，最後，針對可能會被提及的問題，作些準備。 
 
在實際的報告過程中，我認知到，通常提到了過多細節的內容，會讓聽眾比較難
跟上，而在七，八分鐘的海報報告，重心應該要放在講清概念，和主要的貢獻表
達，才能讓大多數與會者了解，有興趣想知道細節者，自然就會針對其想了解的
內容進行追問。 
 
這篇論文得以順利地發表，感謝劉庭祿博士與傅楸善教授平日的指導，和研究討
論。另外我也感謝陳嘉平同學對於海報設計與印製上的協助，以及卓奕宏同學陪
同我至加拿大開會。最後感謝傑出人才基金會提供出國獎助金，讓我順利出席國
際會議，尤其是在遠地的加拿大。 
 
三. 建議 
 
參加了這次的電腦視覺與圖形識別會議後，我有以下二個建議與分享： 
 
(一) 這次的會議共接收了 250 篇論文，但台灣被接收的論文數量十分的少，與
我國的科技發展或學校素質來說並不相稱，且今年也並不是個案。我想這
跟大多的實驗室都將主要的心力放在期刊的投稿上有很大的關係。然而就
我所知，機器學習這領域許多頂尖的學者，實驗室或研究機關，都有將投
Copyright c© 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR).
Learning Partially-Observed Hidden Conditional Random Fields
for Facial Expression Recognition
Kai-Yueh Chang1,2 Tyng-Luh Liu1 Shang-Hong Lai2
1Institute of Information Science, Academia Sinica, Taiwan
2Department of Computer Science, National Tsing Hua University, Taiwan
Abstract
This paper describes a novel graphical model approach
to seamlessly coupling and simultaneously analyzing facial
emotions and the action units. Our method is based on the
hidden conditional random fields (HCRFs) where we link
the output class label to the underlying emotion of a facial
expression sequence, and connect the hidden variables to
the image frame-wise action units. As HCRFs are formu-
lated with only the clique constraints, their labeling for hid-
den variables often lacks a coherent and meaningful config-
uration. We resolve this matter by introducing a partially-
observed HCRF model, and establish an efficient scheme
via Bethe energy approximation to overcome the resulting
difficulties in training. For real-time applications, we also
propose an on-line implementation to perform incremental
inference with satisfactory accuracy.
1. Introduction
For humans, the making of a particular facial expres-
sion is a continuous and often short event, typically trig-
gered by the associated emotion and put together through
a series of muscle motions (see Figure 1). These subtleties
have caused designing computer vision algorithms to au-
tomatically detect and recognize facial expressions a chal-
lenging task. To tackle this problem, most of the exist-
ing techniques, e.g., [2, 14] have converged to investigate
the action units (AU) of the Facial Action Coding System
(FACS) proposed by Ekman and Friesen [7]. In this work,
our primary goal is to establish a new graphical model ap-
proach of which classifying facial expressions and identi-
fying action units can be elegantly coupled and simultane-
ously analyzed. As a result, the framework can lead to a
more effective implementation for both technical and prac-
tical concerns. (The term “facial expressions” hereafter is
restricted to the six basic ones, including joy, sadness,
surprise, anger, fear, and disgust.)
Instead of recognizing the facial expression per image
Figure 1. From left to right, these face images show a transition
from neutral to peak for the making of a joy facial expression.
frame, we consider casting the problem as a classification
task over an image sequence. That is, our aim is to de-
termine the class label specifying the emotion of a given
sequence. Meanwhile, observe that knowing the combina-
tions of action units (especially those in the peak images)
generally provides useful evidence for distinguishing differ-
ent emotions. It is therefore insightful to know what action
units are activated in each facial image. To this end, we
consider hidden conditional random fields (HCRF) [16] for
facial expression recognition, and establish a useful connec-
tion between the hidden states of a CRF [11] and the action
units. A key distinction between our framework and HCRF
is that learning the proposed graphical model utilizes infor-
mation from some partially-observed hidden states. We will
show that such a deviation usually yields better predictions
for the sequence and the hidden-state labels, with the price
of requiring a more delicate learning/training process.
1.1. Related work
An extensive review on facial expression analysis can be
found in Pantic and Rothkrantz [15]. While the survey is a
bit outdated, it still provides a comprehensive overview on
the related topics. In what follows, we briefly describe tech-
niques that deal with facial activity, and then discuss those
focusing on predicting the emotion of a facial expression.
On analyzing facial activity and action units, Donato et
al. [6] show that the Gabor wavelet representation and the
independent component analysis are useful for classifying
action units. They conclude with a surmise that combin-
ing motion and gray-level information may give the best
facial expression recognition performance. Kapoor et al.
[10] have constructed a shape model of the upper face, and
used the model parameters as the inputs to SVMs for rec-
ognizing action units within the upper face. Different from
Observe that, through (1), an interest point k gives rise to
two distributions: one for the positive images of {Ji} and
the other for the negative ones. Treating each interest point
as a weak classifier would produce a weighted misclassifi-
cation error ǫt(k). It follows that the selected interest point
at iteration t satisfies
k∗t = argmin
k
ǫt(k). (2)
Suppose that upon the completion of AdaBoost, K inter-
est points have be chosen. (K = 60 in all our experiments.)
Then, repeatedly performing the interest point selection by
setting the target class to each of the emotion categories in
turn would yield a set of n = 7×K interest points, denoted
as {k∗1 , . . . , k
∗
n}. To construct a feature vector x (of dimen-
sion n) for an arbitrary image I from its bag-of-features rep-
resentation, we first detect the interest points of I , and then
define the ith component of x by
xi = d(k
∗
i , I). (3)
3. Partially-observed HCRFs
The HCRF model has been applied to object recognition
[16] and gesture recognition [20]. In essence, the main idea
behind HCRFs is to enrich CRFs by adding hidden states to
capture complex dependencies or implicit structures in the
training samples. The effect can be achieved by using more
hidden variables, or by increasing the number of possible
hidden states. Either way would lead to a graphical model
with a large number of hidden-state configurations.
Unlike other graphical models with hidden states,
HCRFs lack an explicit formulation (such as the transition
probabilities in HMMs) on correlating the hidden variables
other than the clique relations. Under such a general setting,
it is difficult to foresee useful regularities from the hidden-
state outputs by HCRFs. In our experiments we observe that
applying HCRFs to image sequences of similar appearances
may give rise to rather different hidden-state configurations.
Our use of HCRFs for facial expression recognition has
a good analogy here. While recognizing the underlying
emotion is our goal, uncovering the action units in each im-
age frame turns out to be crucial for making the prediction.
Also, in our training data, the action unit information is al-
ready provided in the peak (last) image of each sequence.
These two aspects of consideration have prompted us to
develop a new generalization for HCRFs—by introducing
partially-observed hidden state variables. As we will ex-
plain that the modification does not affect the graph struc-
ture (see Figure 2), and requires no extra data labeling.
3.1. Energy function and data likelihood
In a partially-observed HCRF model, the hidden vari-
ables of a training sequence s are divided by h = ho ∪ hu,
h1 h2 he
hu
y
s
Figure 2. The shaded node corresponds to an observed sequence
s. h1 and he are the starting and the ending hidden variables re-
spectively, and hu includes the remaining hidden variables.
where ho and hu denote the hidden variables whose (dis-
crete) state values are respectively observed or unknown
during training. Analogous to [16], the conditional prob-
ability of class label y and hidden variables h is given by
p(y,ho,hu | s; θ) =
exp {−E(y,ho,hu, s; θ)}∑
y′,h′
o
,h′
u
exp {−E(y′,h′o,h
′
u, s; θ)}
(4)
where θ includes the parameters of the probabilistic model,
and E is the energy function. It implies that
p(y,ho | s; θ) =
∑
hu
p(y,ho,hu | s; θ). (5)
And the data log-likelihood ofD = {(s(i), y(i),h(i)o )}, with
partially-observed hidden states, is given by
L(θ) =
∑
i
log p(y(i),h(i)o | s
(i); θ)−
‖θ‖2
2σ2
(6)
where we have assumed a zero-mean Gaussian prior on θ.
Learning a partially-observed HCRF model can now be ac-
complished by solving
θˆ = arg max
θ
L(θ). (7)
In our implementation, scaled conjugate gradient [13] is
used to find θˆ in (7). We emphasize that the partially-
observed information about the hidden variables is provided
only in training. Hence probability inference with the pro-
posed model is exactly the same as with a regular HCRF.
That is, given a new test sequence s, we have
p(y | s; θ) =
∑
h
p(y,h | s; θ). (8)
3.2. Approximation with Bethe free energy
By far it may appear that adding partially-observed hid-
den variables to the learning of HCRFs is straightforward.
The definition of L(θ) in (6) indicates that solving (7) re-
quires evaluating p(y,ho | s; θ) for each training sequence.
When applying the belief propagation algorithm, the joint
probability whose variables belong to a clique can be di-
rectly approximated by the corresponding belief. In our
one facial expression from the on-line streaming data. In
practice such a restriction is not reasonable. To relax the
limitation, we need some mechanism to ensure that proba-
bility inference of the current facial expression can be made
without including the effects from the previous expressions.
One way to achieve this is to detect the conclusion of an ex-
pression and reset the recognition system. However, there
still lacks an efficient way for detecting the ending of an
expression peak in real-time applications [5]. Here we dis-
cuss what conditions can assure the HCRF framework to
make inference without considering the past information.
Assume that frame t is the start of an expression. From (14)
and (15), we denote those terms related to the past by
ut(y, ht; θ) =
∑
ht−1
exp {−θ3(y, ht, ht−1)}mt−1(y, ht−1; θ).
(17)
As is implied in Section 3, we can compute the Bethe
free energy for each y. If (17) yields the same value for all
ht given any y, the information prior to frame t causes no
effect at all since the message from t− 1 acts as a uniform
distribution. To facilitate this condition, we consider
f(x) = xq, for x > 0 and 0 ≤ q < 1. (18)
When f is repeatedly applied to any real value, the outcome
will converge to 1. We call the parameter q in (18) the prun-
ing factor that controls the rate to approach 1. At frame t,
we “relax” the past information ut(y, ht; θ) by f(ut) and
approximate mt(y, ht; θ) by
f(ut(y, ht; θ))× exp (φ(s, t) · θ1(ht) + θ2(y, ht)). (19)
Whenever a new expression starts, one can set the value
of q to be close to 0 (to disregard the past information), and
otherwise to be close to 1. In our experiments, the values
are 0.1 and 0.9, respectively. The reason we choose 0.9
instead of 1 is mainly because such a tactic can result in
a more noticeable drop in the inference probability when
an expression is completed and image frames with neutral
expression are reached. Nevertheless, detecting the end of
an expression is still a hard problem. In our experiments,
we use a heuristic way to decide. We compute the dif-
ference between the values maxy,r;1≤r≤5 p(y |x1:t−r−1; θ)
and maxy p(y |x1:t−1; θ). If the difference is larger then
0.05, we say that a new expression starts and set q = 0.1.
5. Experimental Results and Discussion
We test our method with three sets of experiments. The
first is to compare the sequence-wise classification out-
comes derived by the partially-observed HCRF model (PO-
HCRF for abbreviation) and other implementations. The
second is to investigate the effects of using an extensive set
of hidden labels to account for all action unit combinations
in the training dataset. And for the last, we demonstrate
that satisfactory real-time recognition performances can be
achieved via the on-line incremental inference.
5.1. Dataset
The facial expression database used in our experiments
is Cohn and Kanade’s DFAT-504 dataset [9]. It contains 486
sequences produced from 97 subjects. There are about 1 to
9 sequences for each subject. In this dataset, the action unit
information is provided only for the last frame (peak image)
of each sequence, and occasionally it may not be sufficient
for identifying the emotion category. We have labeled 392
sequences by referencing Table 2 in Zhang and Ji [24].
5.2. Sequence-wise classification
In this set of experiments, we are to demonstrate the ef-
ficiency of using PO-HCRFs to analyze sequences of facial
expressions. We begin by setting the total possible hidden
labels/states to 14, and further consider two cases. For case
one, the hidden labels range from 1 to 7 (1 will be reserved
for neutral), and each corresponds to some combina-
tion(s) of action units. And for the other case, the mean-
ingful labels are from 1 to 9. In both cases the remaining
labels are not explicitly defined so that it leaves some de-
gree of freedom for the training process since certain image
frames are hard to be labeled. The purpose of adding two
more hidden labels is to enable our system to distinguish
whether (i) a joy face is with mouth open (i.e., AU25), and
(ii) a fear face is with a “shock” (AU2: outer brow raiser
or AU5: upper lid raiser). (See Table 1 for details.)
Besides comparing the results derived by HCRFs and
PO-HCRFs, we have implemented the method of Bartlett
et al. [2] for image sequences. It is done by applying their
classifier to each image frame, and the label of a sequence
is decided by a majority vote. (Note that those classified as
neutral are not counted.) For a more insightful study, we
also consider adapting their method by using our feature se-
lection scheme. In Table 2, we report the recognition accu-
racies. The notations PO-HCRF7 and PO-HCRF9 indicate
the number of meaningful hidden labels used in their imple-
mentation. Examples of illustration on the labeling results
are provided in Table 1. A confusion matrix by PO-HCRF9
is given in Table 3. Overall, the experimental results show
that PO-HCRF can achieve better accuracy rates, and output
more coherent hidden labels.
Indeed the fact that all test sequences start with a neu-
tral frame is not required by our method. To verify this
claim, we apply PO-HCRF to all the last half sequences,
and obtain a slightly better recognition rate, 93.11%. Two
such examples are plotted in Figure 3, where the broken-
line graphs show the probabilities of different emotions for
the given (last half) facial expression sequences.
1 2 3
(a)
0 2 4 6 8 10 12 14 16 18 20 22
0
0.2
0.4
0.6
0.8
1
 
 
joy sadness surprise anger fear disgust
1 8 3
1 8 3
(b)
1 8
(c) (d)
Figure 4. (a) A joy sequence. (b) The plot for the inference probabilities for the six emotion c1asses. Below the plot are the hidden-node
labeling results respectively derived by processing the whole sequence (top) and by incremental inference (bottom). In both results, despite
some of the images have been labeled as fear, the output emotion class label is still correct. (c) and (d) Some examples of the fear faces
with action unit AU12 (mouth corners pull up), which also could occur in a joy facial expression.
image frames. Thus, we use an extensive set of 100 hidden
labels, and carry out PO-HCRF100 to achieve a five-fold
cross-validation rate of 90.05%.
We consider the labeling results at the first and the last
frames to evaluate the recognition rate and the false alarm
rate for each action unit (see Figure 5). Most of our results
are better than or comparable to those in [2] except for AU1
and AU4. The main reason for the degradation is due to that
the distributions of AU1 and AU4 are dominated by those of
other action units for the fear and sadness sequences.
Finally, we note that the better action unit recognition rates
in Tong et al.’s work [18, 19] are derived by using action
unit information from all training image frames, while in
our method only the two end frames are used.
5.5. Implementation for real-time applications
Five-fold cross-validation is adopted to estimate the on-
line recognition accuracy by our real-time implementation.
At each fold, we first run PO-HCRF9 and PO-HCRF100
separately on each training sequence, and frame-wise de-
rive the inference probabilities for the six emotion classes
(like the plot in Figure 4b). We also compute the entropy
and its first derivative at each time instant. The six prob-
abilities and the two entropy-related quantities form a fea-
ture vector of dimension 8. Suppose now we want to learn
a decision function for the emotion of joy via the Percep-
tron algorithm. The positive/negative data are those image
frames from the joy/non-joy sequences that the inference
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
1
2
4
5
6
7
9
12
15
17
20
23
24
25
27
 
 
Bartlett et al. [1] Ours
0 0.02 0.04 0.06 0.08 0.1
1
2
4
5
6
7
9
12
15
17
20
23
24
25
27
 
 
Bartlett et al. [1] Ours
(a) Recognition rate (b) False alarm rate
Figure 5. Evaluation for classifying each action unit.
probability for joy in their feature vector is the largest. The
remaining cases can be analogously learned.
For testing, we perform incremental inference by treat-
ing a test sequence as an image stream. At each time instant,
the decision function corresponding to the emotion class
that currently has the highest inference probability will be
applied, and a decision on the emotion label will be made
if the response is positive. With this setting, the accuracy
rate we achieve is 80.10% with 9.18% false alarm rate for
PO-HCRF9 and 80.36% with 8.93% false alarm rate for
PO-HCRF100. For justifying the advantage of using the
pruning factor q in our on-line implementation, we further
simulate image streams by concatenating all the sequences
of the same person in the training dataset. An example of
such comparison results is illustrated in Figure 6.
（附表四） 
參加國際學術會議會後報告表 
REPORT FOR ATTENDING INTERNATIONAL CONFERENCES  
姓   名 
Name 
張楷岳 
服務單位及職稱 
Institute & Position 
中央研究院資訊科學研究所約聘兼任助理 
會  議  名  稱 
Title  of Meeting 
中文：IEEE 電腦視覺與模型辨識國際會議 
Chinese 
英文：IEEE Computer Society International Conference on 
Computer Vision and Pattern Recognition, CVPR 
English 
日   期 
Date of Conference 
98/6/20-98/6/25 地  點 
Location 
美國邁阿密 
經  費  來  源 
Source of Travel Funding ■申請補助 Applying for Funding（補助單位 Supported by: 國科會計畫–
流形學習在電腦視覺及多媒體相關應用之研究 
(NSC95-2221-E-001-031-MY3)） 
□會方補助 Funded by Conference □自籌 Self-funding
 
