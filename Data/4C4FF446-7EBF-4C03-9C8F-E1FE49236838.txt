 1
行政院國家科學委員會專題研究計畫成果報告 
IC 封裝產品自動分類模式資料庫之建立(I) 
Establishment of IC Encapsulation Product Automatic 
Classification Model Database (I) 
計 畫 編 號：NSC 97-2221-E-167 -014 
執 行 期 限：97 年 8 月 1 日至 98 年 7 月 31 日 
主 持 人：洪永祥  國立勤益科技大學 工業工程與管理系 
計畫參與人員：余光正  國立勤益科技大學 冷凍空調系 
柳馨喬  國立勤益科技大學 工業工程與管理系 
張錦煌  矽品工業股份有限公司 研究發展部 
一、中文摘要 
本研究主要目的，藉由整理現有 IC 封
裝型式的相關資訊，如尺寸、特性、設計準
則和應用準則等等，應用資料採礦技術(Data 
mining)建立 IC 封裝產品的分類準則，協助
IC 封裝廠商建立一套 IC 封裝型式分類系
統的雛形，使客戶、業務、客服人員或管理
階層人員能夠迅速且準確的得到想要獲得
的訊息。經與 R&D 專家討論後選定 PBGA, 
QFP, PBGA, LGA, FCBGA, FCLGA , 
FCCSP ,QFN 及 SOP 等 9 種產品族及其子
類，應用粗集合(RST)及精準變數(VPRS)等
演算法建立 IC 封裝產品自動分類器及其分
類規則，藉以提供專家快速汲取知識庫內的
分類資料，及執行決策時的輔助工具。 
關鍵詞：：IC 封裝產品、自動分類器、類
神經網路、決策準則、Rough Set、SVM 
Abstract 
The main purpose of the present study is, 
by integrating currently available relevant 
information regarding IC encapsulation 
pattern including size, features, design 
principles and application principles, to set up 
IC encapsulation product classification 
principles by application of data mining and 
help IC encapsulation product manufacturers 
to establish a prototype of IC encapsulation 
classification system, which may render 
clients, business personnel, customer service 
staffs or the management personnel able to 
rapidly and accurately obtain desired 
information. After discussion with R&D 
experts, PBGA, QFP, PBGA, LGA, FCBGA, 
FCLGA, FCCSP, QFN and SOP product 
families and their sub-categories are selected. 
By application of algorithms such as RST and 
VPRS, set up IC encapsulation product 
automatic classifier and the classification rules 
for experts to rapidly obtain classified data 
from the knowledge bank and serve as an 
assistant tool in decision-making. 
Keywords: IC Package Product, Automatic 
Classifier, Neural Network, Decision 
Rule, Rough Set, SVM. 
二、緣由與目的 
在 IC 的產品設計開發過程中，IC 封
裝型式的選擇對於 IC 設計人員的重要性，
猶如建築師在設計建物時決定主結構體建
築工法之決策般的重要。換言之，IC 設計
人員必須選擇適當的 IC 封裝型式，才能依
此選擇 IC 晶片後續的設計方式及其製造程
序。事實上，IRIT 的預估，未來幾年 IC 設
計業之成長會高於測試、代工及封裝業，IC 
 3
格點狀散列(Area grid Array)的方式排列，並
以銲錫球作為接腳及焊點，克服導線架腳位
數目、散熱及訊號干擾等問題。BGA其接腳
數可輕易達到768pins~1156pins，而相對的
腳 距 為 0.8mm~1.27mm ， 相 較 QFP 
0.4mm~0.8mm。晶片級封裝 CSP而是泛指
封裝後晶片面積在裸晶面積的1.5倍以內。
覆 晶 封 裝  FC 覆 晶 接 合 技 術 又 名 為
C4(Controlled Collapse Chip Connection) 最
顯著的優點是因晶片的整面都可當作輸出
輸入的接點，因此其構裝的密度可大幅增
加。而且因為沒有導線，距離短，銲料體積
小，除了構裝密度可提高外，散熱性與電氣
性質都會改善。較通用的製程為錫球直徑
125微米，間距250 微米，其輸出/輸入接點
數可達1000 以上[3]。FCLGA之封裝形式採
用薄的核心、高密層壓基板、焊錫可濕性I/O
焊盤以及很薄的晶片，因而使封裝厚度僅為
0.65mm。而這種無鉛封裝適用於行動電
話、無線區域網路模組、PDA、數位相機和
可攜相機等高頻與 /或高傳輸率應用。而
QFN封裝不但體積小、成本低、生產良率
高，還能為高速和電源管理電路提供更佳的
共面性以及散熱能力等優點。 
3.2 資料探勘技術在分類問題的應用 
資料探勘技術經過多年的發展，已有許
多 技 術 及 方 法 被 提 出 ， 包 括 關 聯 性
(Association)、資料分類和預測(Classification 
and Prediction)、推理(Estimation)、集群分析
(Clustering Analysis)。文獻中常被提及的分
類技術如：統計方法、類最近鄰居法
(k-Nearest Neighbor) [3]、神經網路 (Neural 
Network)[4]、決策樹 [5] 、粗集合理論及
SVM 等各適用在不同的情況與資料性質。 
3.3 研究方法及步驟 
IC 封裝族群與產品種類暨多且雜繁，
且歷史資料更是龐大。因此面對大量 IC 設
計人員在產品諮詢的需求，又缺乏專職且合
適的的專業人員，使得 IC 封裝廠商在回應
能力及反應速度上更顯得肘襟見拙。 
分類就是將不同的資料對應或歸屬到
數個事先定義好的資料類別中，同時也可以
個別找出不同類別的屬性特徵，相反的，除
了找出屬性特徵外，它也可以反向檢驗現存
的分類方式並發展出預測模式。因此要建立
一個有效率準確性又高的多產品自動分類
器(如做好分類的工作)，首要的認識是特徵
的種類愈多，愈能夠照顧單一特徵所不能照
顧到的區別。比如說，「四條腿」只是某種
椅子的特色，並不是所有椅子所必備的特
徵，甚至不能將殷商的器皿排除在範疇之
外，因為後者也有四條腿。為了提升分辨的
精確度，我們必須收集更多的特徵，如「具
有凹洞」、「可以儲存液體」等。多種特徵所
帶來的問題是，可以經由機器學習的方法形
成分類器(Classifier)及建立分類的邏輯規則
來協助建立 IC 封裝型式知識庫達到分類的
工作。 
本研究計畫經與 R&D 專家討論後蒐集
PBGA, QFP, PBGA, LGA, FCBGA, FCLGA , 
FCCSP ,QFN 及 SOP 等 9 種產品族及其子
類，應用粗集合及精準變數等演算法建立 IC 
封裝產品自動分類器及其分類規則，藉以提
供 IC 設計工程師快速汲取知識庫內的分類
資料縮短工作時程。研究架構如如圖 2 所
示。 
 
圖 2  IC 封裝資料庫自動分類研究架構 
3.4粗集合理論的擴展模型－精準變數
VPRS 
粗集合理論是由 Pawlak 教授於 1982 
年所提出，應用在含糊和不確定為特性的環
境。在資料前置處理上首先需要產生資訊表
(Information table)用以表示資料的組成。一
般係定義為 S = < U, A, V, f>，其中 U 為有
限 物 件 的 集 合 (Universe) ， 如
U={x1,x2,…,x5}，A 為屬性(Attributes)的集
合，V 是屬性值，而 f 為方法；如果 P 為一
個屬性子集合， 且 ，則 稱為「不可區別
關係」；其次需定義資料所在之「上界」
(Upper approximation)與「下界」 (Lower 
approximation)，上、下邊界(Boundary)是約
略集合理論中兩個極重要的觀念。基本粗集
 5
  
 
 
The weight values are commonly used to represent the neuron connection strengths. Classified by the network neuron learnin
g algorithm strength, it can be roughly divided into the supervised learning and the unsupervised learning. These two are differ
ent: the supervised neural networks have the actual correct value for comparison in the supervised learning process(4). In su
m, the ANNs learning training process is to continuously adjust the weight vales, biased weight vales to let network work out th
e output values in line with the target values. The purpose of the operation is to let the network reflect the correct input/output r
elationship mode. 
 
2.2 Back-propagation networks 
 
Multilayer feed-forward networks are very important in the development history of ANNs. The “multilayer perceptions”, also 
known as “back-propagation networks”, is the best known [6]. The concept of BPNN algorithm originated from the BPNN al
gorithm concept proposed by Harvard graduate Dr. Werbos in his graduation paper. Later on, DDP panel led by two psycholog
ists Rumelhart and McClellend (5,7) developed the same technologies and applied the training of the multilayer perception ma
chine, making the method widely known. 
“BPNN algorithm” network training method includes two stages: feed-forward stage and the propagation stage. At the feed-f
orward stage, input vector is introduced by the input layer and transmitted to the output layer by feed-forward through the hidd
en layer, and work out the network output value. At this time, network chaining value is fixed; while at the back-propagation net
work, network chaining value is modified according to the error correction rule. By the correction of the chaining value, the net
work output value approaches the expected output value. In fact, it is expected that the error signal by subtracting the network 
output value from the correct output value. Such error signal will be in the back-propagation network. Therefore, this algorithm i
s known as the “BPNN algorithm”, as shown in Figure 1. 
 
The artificial neurons of each hidden layer and output layer will execute two computations: 
 
1. Calculate ANNs signal output, namely, after multiplying the input signals or the output of artificial neurons of the previous lay
er and the vectors of the artificial neuron chaining values, input the result into the activation function. The output at this point wi
ll be the output value of the artificial neuron. This computation is implemented at the feed-forward stage. 
 
2. Calculate the transient estimation of the artificial neuron on the gradient vector of the error function. The transient estimation 
of the error function on gradient vector is the differentiation of the error function on the neuron chaining vector. This computatio
n is implemented at the back-propagation network stage. 
 
In a back-propagation network, at the time of n-th network learning, the i-th artificial neuron error function of the network output 
layer is defined as: 
 
Define the i-th artificial neuron’s quadratic difference transient value and the transient error square function asE(n). 
Nnamely, the total of all the output layer artificial neurons’ quadratic difference transient values is represented as  
 
where, Concentration C includes all the sets of the output layer artificial neurons.; Nis the number of the input training data, an
 4. BPNN Experimental results 
 
The BPNN method of the ANNs with TFBGA and LGA was applied on the TFBGA and LGA products, each containing 16 and 
17 items, respectively. The number of TFBGA data was 632 with 16 items. The item data details are as shown in Table 1. The 
number of LGA data was 424 batches with 17 categories. The details of the categories are as shown in Table 2. The total num
ber of data was 1056 batches with 33 categories. 
 
This experiment simulated and trained each category by using BPNN, with the parameters set with learning rate at 0.2 per gri
d, 0.1~0.9 for testing, simulated for 1000 times. Under different learning rates, the number of convergence of the network in th
e simulation process varied from time to time. Therefore, the best ten results from the simulation results were selected for reco
rding, and the best result of the 10 times was the representative. As shown in Figure 3, the horizontal axis learning rates of the 
TFBGA Family are 0.9, 0.7, 0.5, 0.3, 0.1 respectively, and the vertical axis is the scope of distribution of the testing and training 
accuracy rates. The simulation was set in 1000 times. The results showed that the highest classifying testing accuracy rate wa
s 82.81% and the highest training accuracy rate was 80.4%. Figure 4 shows the results of different learning rates of LGA. As s
een, the highest classifying testing accuracy rate was 83.72%, while the training accuracy rate was 81.42%. 
