networked game) has been popular for several years. 
On the other hand, international organizations such 
as MPEG have scheduled a series of standardization 
for multi-viewpoint and free viewpoint audio/video 
technology which present 3D interactivity. 
Accordingly, integrating the above- mentioned 
technology (i.e., 3D stereo display, 3D 
interactivity, and networked transmission) to 
establish a real/virtual-mixed environment for 3D 
interactivity will be heavily potential in the near 
future. 
In the first year, we focus on the calibration of 
color/depth camera, depth image improvement, DIBR 
quality improvement and the detection and tracking of 
the human 3D pose. Besides, in order to insert the 
human 3D model into the depth image data to get the 
human 3D pose parameters, we use the depth image from 
a 3D video capturing system to do motion capturing. 
With the human pose parameters, 3D video data can be 
linked with the virtual 3D model at the server side 
to achieve augmented reality. 
In the second year, in addition to integrating the 
low resolution depth video with the high resolution 
color video, we also integrate the pre-established 3D 
virtual backgrounds with the 3D video captured from 
the two high resolution color cameras and one low 
resolution depth camera. Finally, we process the 
depth image captured from the 3D camera to establish 
the user’s skeleton model.         
In the third year, we proposed a rate control 
technique based on 3D quality optimization focusing 
on 3D video in terms of video plus depth format. 
First, we evaluate the training sequences to find the 
best color/depth rate allocation percentage for each 
frame by using 3D quality metric. These percentage 
data are then used as training data to establish a 
support vector regression (SVR) model. Then, we 
analyze the color and depth frames’ image features 
and use them as the input to SVR model to predict the 
bit allocation percentage between the color and 
depth. Therefore, we can accurately control the bit 
rate to result in the best 3D perceivable quality. 
 1 
行政院國家科學委員會專題研究計畫成果報告  
多點 3D 模型與 3D 視訊之整合與其任意視角 3D 立體影像合成
之品質最佳化技術 (3/3) 
計畫編號：NSC 99-2221-E-194-003-MY3 
執行期限：99 年 8 月 1 日至 102 年 7 月 31 日 
   主持人：賴文能教授   中正大學電機系 
計畫參與人員：陳韋志、廖有朋、陳羿霖、卓政翰、佘東穎、盧彥亨、何佳哲、郭俊廷、
徐筱婷 中正大學電機所 
 
一、 中文摘要 
3D 立體電影、3DTV 在 2010 年已經引起視
聽科技界的一股熱潮，而互動遊戲 (例如 Wii 及 
Second Life 網路虛擬互動遊戲) 則早已在過去數
年創造了一股風潮。另一方面，國際上相關組織 
(例如 MPEG) 則針對多視域 (Multi-view) 及自
由視角 (Free View-point) 此種帶有『互動』的音/
視訊進行一系列標準化的制定。據此，整合上述技
術 (即 3D 立體顯示、3D 互動、及網路傳輸)，
也就是建立一個讓多方使用者能夠透過網路雲端
伺服器在虛擬/真實混合的環境中進行 3D 互動的
網路多媒體平台，在不久的未來顯然有很大的應用
潛力。 
在本研究的第一年中，我們把主要焦點放在彩
色/深度攝影機校正、深度影像改善、DIBR 品質
改善、和人體 3D 姿態偵測與追蹤上。除此之外，
為了將一人體 3D 模型嵌入深度影像資料中以求
得人體 3D 姿態參數，我們利用 3D 視訊擷取系
統中深度攝影機所獲得的深度影像來進行使用者
的 motion capturing。有了人體姿態參數，3D 視訊
資料便可以在 server 端與虛擬 3D 模型資料進行
整合，而達到擴增實境的目的。     
第二年，我們除了延續第一年度的初步成果
將低解析深度視訊與高解析彩色視訊加以整合，
以得到完全高解析的 3D 視訊 (彩色+深度)之
外，我們利用兩台高解析彩色攝影機搭配一台低
解析深度攝影機所擷取的 3D 視訊資料與預先建
立的3D 虛擬背景場景進行整合。最後，我們利用 
3D 視訊擷取系統中體感攝影機所獲得的深度影
像來進行使用者的骨架模型建立。 
第三年，本計畫針對彩色搭配深度資訊的 3D 
視訊格式，提出一種基於 3D 品質最佳化的位元
率控制技術。本系統先以 3D 品質評估演算法對
訓練視訊 (training sequence) 進行評估，找出能夠
得到最佳 3D 品質的彩色/深度位元率分配比例，
以此當作訓練資料建立支持向量迴歸模型 
(Support Vector Regression, SVR)。實際編碼時，透
過分析彩色畫面與深度畫面的特性，並利用所建立
的支持向量迴歸模型來預測彩色與深度間的位元
分配比例。如此一來便能達到精確地控制位元率與 
3D 品質最佳化的目的。 
 
關鍵詞：多視域視訊、自由視角視訊、深度繪圖
法、擴增實境、3D 品質度量、位元率控制、支持
向量迴歸模型 
Abstract: 
3D stereo movie and 3DTV has gained 
increasing attention in the past year 2010, whereas 
interactive game (e.g., the famous Wii and the 
“Second Life” networked game) has been popular 
for several years. On the other hand, international 
organizations such as MPEG have scheduled a series 
of standardization for multi-viewpoint and free 
viewpoint audio/video technology which present 3D 
interactivity. Accordingly, integrating the above- 
mentioned technology (i.e., 3D stereo display, 3D 
interactivity, and networked transmission) to 
establish a real/virtual-mixed environment for 3D 
interactivity will be heavily potential in the near 
future. 
In the first year, we focus on the calibration of 
color/depth camera, depth image improvement, 
DIBR quality improvement and the detection and 
tracking of the human 3D pose. Besides, in order to 
insert the human 3D model into the depth image data 
to get the human 3D pose parameters, we use the 
depth image from a 3D video capturing system to do 
motion capturing. With the human pose parameters, 
3D video data can be linked with the virtual 3D 
model at the server side to achieve augmented 
reality. 
In the second year, in addition to integrating the 
low resolution depth video with the high resolution 
color video, we also integrate the pre-established 3D 
virtual backgrounds with the 3D video captured 
from the two high resolution color cameras and one 
low resolution depth camera. Finally, we process the 
depth image captured from the 3D camera to 
establish the user’s skeleton model.         
In the third year, we proposed a rate control 
technique based on 3D quality optimization focusing 
on 3D video in terms of video plus depth format. 
 3 
填補，對於角度距離較近的新視角而言影響不大 
(因該 disocclusion 區域屬於細長型)，然若欲合成
的新視角與原視角的距離較遠，則因 disocclusion 
區域較寬，簡單的內插通常帶來不佳的合成影像
品質。而本計畫中，C1 與 C2 攝影機通常在安
排上距離較遠 (以便可以拍攝使用者較廣的角度
範圍)，因此預合成的新視角與原視角 C1,C2 的距
離亦較遠，此顯然掉入 DIBR 技術的缺失中。本
計劃嘗試線性內插以外的新方法，以改善未來可
能的品質缺失。 
 
圖 2.3 3D Image Warping [1] 
3D 模型與 3D 視訊資料的整合 
為了達成讓多個使用者能同時在虛擬場景中
進行互動，我們必須將各個使用者端拍攝到的 3D
視訊 (包括 2D 彩色與 3D 深度) 與伺服器中的
虛擬場景進行結合，使得每一個使用者可以看得
到所建構的虛擬環境及其他使用者的影像。由於
本計劃的應用在於互動的虛擬遊戲，如下棋、麻
將等桌上遊戲，因此在伺服器端我們可以先建立
好多種不同的場景，以西洋棋為例，我們需建立
西洋棋棋盤的三維模型以及棋子等，而虛擬物體
的建立可以使用市售的 3D 繪圖軟體輔助，如
Maya [2]、3Dmax 等。 
欲整合場景道具的 3D 模型與使用者端所量
測的 3D 深度影像，我們需要克服兩個問題：1) 
資料型態上的不同，2) 資料點參考座標的不同。
由於使用者端深度影像是以深度攝影機為中心的
量測結果，因此，我們必須將它轉換為參考「物
體中心」的資料型態，如此才能與虛擬環境結合。
雖說深度影像測量時是以使用者的正面為主，但
我們無法保證此一結果，因此除了改變座標參考
中心外，還必須對 3D 深度影像進行 3D model 
fitting，也就是將一個三維人體模型套入 3D 深度
影像中 (學術上或稱 3D motion capturing)，如此
便可得知使用者的四肢姿態。這樣的好處是可以
對為數眾多的深度影像點進行較佳的處理，例如
將人體正面 “靠” 到桌面正緣、把手放在桌面上 
(而不致穿越虛擬的桌面或躲到桌子下) 等等。 
雖然使用 3D 繪圖軟體來繪製虛擬角色可以
很容易合成各角度的立體影像，但使用者看到的
並非真正自我的影像，較難產生參與感 (這也是
為何許多遊戲如 Second Life 皆會設計多個不同
的人體三維模型，讓使用者能夠創造出更符合自
己的角色)，而我們的系統使用真實的影像來進行
互動，應是未來的趨勢。 
3D 立體影像/視訊品質評估 
相比於傳統的 2D 視訊傳輸，3D 立體視訊傳
輸需要更為龐大的資料量。然而，在傳輸頻寬有限
的情形下，就必須透過壓縮技術將資料減量以進行
傳輸，但相對會造成影像品質的降低。所以，如何
在保持影像品質與傳輸頻寬有限間取得平衡便成
為一個很重要的課題。然而，在傳統位元率控制技
術中，對於影像品質的評估，各個壓縮標準皆以 
PSNR (Peak Signal-to-noise Ratio) 當作品質好壞
的界定標準。不過，在 V+D 的立體視訊格式中，
即使彩色與深度資訊經由壓縮過後都有不錯的
PSNR 值，卻無法確保合成影像的 3D 品質為最
佳。所以，本計畫提出基於 3D 品質最佳化之位
元率控制技術，以期能精準地達到位元率控制並保
有最佳的 3D 品質。 
三、研究方法  
3.1 第一年度研究方法 (99 年 8 月~100 年 7 月) 
攝影機校正 
本計畫首先建構一個 3D 視訊擷取系統，它
是由兩個彩色攝影機及一個 ToF 深度攝影機所組
成。如圖 3.1 中所示，ToF 深度攝影機位於兩個
彩色攝影機中間，三個攝影機不一定要呈一直線 
(高度可各不同)，各個攝影機可以有獨立的俯仰
角。其中彩色攝影機為高解析度  (640x480 
pixels)，而 ToF 深度攝影機則由於目前技術問
題，其解析度僅達 176x144 pixels；彩色攝影機的
畫面率為標準的 30 fps，然 ToF 深度攝影機的畫
面率則不固定，最高可達 50 fps 以上。目前在我
們 的 系 統 中 ， 三 支 攝 影 機 並 未 進 行 同 步 
(synchronization)。 
由於兩支彩色攝影機與 ToF 深度攝影機之
間並不需要有固定的幾何位置關係，因此他們的攝
影機內外參數需要被校正，以進行不同攝影機相互
之間的轉換。攝影機校正 [3] 在於找出攝影機
內、外部參數，由參數得知攝影機彼此之間的關係
供攝影機做 3D warping。深度影像改善是將低解析
度到高解析度深度影像做 3D warping 之後，並加
以處理深度影像不精確的部分。 
 
圖 3.1. 攝影機校正環境 
 5 
以 DIBR 合成的影像才能有高品質)，投影得到的
深度影像如果直接以內插法取得 (類似前述的直
接放大)，其得到的深度影像並不理想，與原始彩
色影像在 edge 上將無法完全匹配。 
本計畫採用色彩分割 (color segmentation) 的
方法 (如圖 3.4)，先將兩邊攝影機的彩色影像分割
成好幾個色塊區域。先前把低解析度深度影像以 
depth warping (即所謂 3D warping，但被投影的量
為 depth，非 color) 投影到彩色攝影機平面上，
對投影後的深度影像做二值化 (如圖 3.5) 以分出
前景和背景。我們對每一個色塊區域依其所對應到
的深度影像的屬性 (前景或背景) 來判斷該色塊
區域屬於前景或背景，以把前景和背景的色塊區分
出來 (也就是說，色彩資訊中的前景/背景分類係
以深度影像資訊來做輔助)。因此，往後在進行深
度影像內插時，可參考色塊區域分類結果的資訊。
內插深度影像中的空點時會由每一個色塊區域內
的深度值加總取平均。內插的深度值 D(x,y) 如下
式表示( kR :為第 k 個色塊區域，ｗ為搜索範圍)。 
       
w
wi
w
wj kk
jiDR
n
yxDR ,
1
,    (3-4) 
   
圖 3.4  (左)為攝影機拍的彩色影像 
           (右)為經過色彩分割後的影像 
    
圖 3.5  (左)為 3Dwarping 完後的深度影像 
           (右)為二值化後的深度影像 
三維人體姿態估測與追蹤 
為了讓使用者達到遠端視訊會議的視覺上互
動效果，因此需先對使用者的姿態做進一步偵測與
追 蹤 。 如 文 獻 [6] ， 在 這 裡 我 們 也 是 利 用
Time-of-Flight 攝影機 (SR4000) 所擷取到的資料
來做三維人體上半身姿態追蹤，整個追蹤系統架構
圖如下圖 3.6 所示。我們將得到的 SR4000 資料做
為系統輸入，經過 Image preprocessing、Human pose 
initialization、3D model construction、Tracking 這
些步驟後，生成相對應於使用者姿態的人體模型並
顯示。 
 
圖 3.6 人體三維姿態之追蹤系統架構圖 
(a) Image pre-processing 
經由 SR4000 拍攝得到的場景，我們可得到此
場景的 Intensity map與 Range map，如圖 3.7所示。 
  
(a) Intensity map      (b) Range map 
圖 3.7 
但礙於 SR4000的資料是整個場景的三維點雲
資訊，並且在拍攝過程中受到不確定的環境干擾影
響，含有大量的雜訊 (圖 3.8 所示)。而實際上我
們只需要用到前景(人體上半身)的資訊來做追
蹤，因此在第一個步驟會先做去除背景與雜訊的預
處理，所以我們可以利用 SR4000 所提供的信心指
數 (confidence value) 與使用者跟攝影機的相對距
離做為去除背景之依據，進而得到前景的三維點雲
資訊，去除前後的比較如圖 3.9 所示。 
 
圖 3.8 (左)正面觀看角度  
   (右)側面觀看角度 
 
  圖 3.9 (左)去除背景與雜訊前 
        (右)去除背景與雜訊後 
 7 
大，其中 ),( yxpc ji  ，代表著在這個遮罩內符合
候選點資格的點雲 ),( yx 為影像的像素位置，由於
我們是用影像處理的方式，以二維遮罩來選取候選
雲點，所以也必須對 ),( yxp j 有更明確的定義，如
下： 
),( yxp j  subject to  ),(),( ,, yxMPyxp zkzj   (3-5) 
),(, yxp zj 為相對於影像位置的距離資訊， ),(, yxMP zk
為上一刻模型關節點的距離資訊，ζ 為一個誤差範
圍，這個定義是為了把雜訊或是與前一刻距離相差
太多的點雲給濾掉。 
最後要定義的是觀察函數，在這邊我們分成兩
個部份：1) 每一個關節點的候選點各擴張成一個
遮罩範圍，並且計算每個候選遮罩內有效的點雲數
量，以脖子的遮罩為例，如圖 3.14 所示；2) 觀察
每一個候選關節點間的肢節長度與一開始校正動
作時所紀錄肢節長度之間的比例關係，避免相鄰關
節點的距離過度分開或靠近 (因人體的關節基本
上屬於剛性運動)。綜合上述兩部份，我們將得到
一個觀察值 iw ，並將其正規化在 0~1 之間，即
}),...,,{,|( 21, nkttti cccCXZPw  ，其中 ni ....1 ，表
示把目前使用者的點雲狀態
tX 與候選點集合 ktC ,
考慮進來的情況下所觀察到的條件機率分佈， iw
越大就代表 ic 越有可能最佳的候選關節點。 
 
3.2 第二年度研究方法 (100 年 8 月~101 年 7 月) 
3D 虛擬背景模型與人體 3D 視訊資料的整合 
本計畫中，我們為了讓使用者達到遠端互動效
果，在伺服器端會預先建立一 3D 背景模型，其
乃藉由 3D 實體模型建構軟體來達成，其格式為
一般 3D 網格形式。然而從使用者端之傳輸子系
統傳送過來的 3D 視訊資料 (彩色+深度) 則為視
訊形式。因此，必須加以整合 (包括真實人物資料
點與虛擬物品間座標的對正、資料一致化等)，以
便進行後續整體場景 (背景+人物) 顯示時的視角
影像重取樣。由於本計劃目標係應用在互動的虛擬
遊戲，如打沙包、拳擊等，因此在伺服器端我們可
以先建立好背景場景 (例如三維場景以及沙包
等)，其中虛擬物體的建立係使用開放的 3D 繪圖
軟體繪製，整個 3D 背景模型與 3D 人物視訊資
料的整合系統架構如圖 3.15 所示。 
(a) 深度影像後處理與背景刪除 
由於 TOF 受到拍攝物體之材質影響，容易在
影像上產生雜訊，造成估計出的深度值與原本實際
深度有些差異，所以都會配合一些影像的後處理，
使得深度影像不受到雜訊的影響而造成嚴重的誤
差。 
      
深度影像後處理 
背景刪除 
3D 視訊與 3D 虛擬場景整合 
整體場景之任意視角
影像合成與顯示 
多視角彩色影像與深度資訊 
3D 互動輸出 
3D 虛擬場景建立 
 
圖 3.15 3D 背景模型與 3D 人物視訊資料整合的
系統架構 
 
我們利用 Trilateral Filter (三項濾波器) 來進
行深度影像濾波，利用影像上鄰近像素的色彩、深
度的相似性、與鄰近像素的幾何距離做為權重值的
依據，進行非線性的深度修正。除了可對深度影像
做平滑處理，減少其雜訊，並可維持與色彩資訊類
似而對正的邊緣。基於這個概念，我們將要處理的
影像設定為深度影像，參考原始影像上的色彩資
訊，藉此對深度影像做空間域上的濾波。 Trilateral 
Filter 如下式表示 (深度值: D(x,y)，n 為搜索範
圍， ),( jiw 為參考鄰近像素的色彩相似性 ),( jiI
和鄰近深度的相似性 ),( jid )。 
),().(),( jyixDjiwyxD
n
ni
n
nj
tt  
 
 








else         ,0
,      ,2).(
)
125.0125.0
(
njinjiw
Id
 
),(),(),( jyixDyxDjid tt   
),(),(),( jyixIyxIjiI tt         (3-6) 
 
 
 
  
                      
 
  
 (a)初始深度影像        (b)經三項波器處理 
後之深度影像 
圖 3.16 
利用修正後的深度影像，設定一門檻值，判斷
拍攝物體為前景或背景，若為前景的區域，則保留
該區域的色彩資訊，若為背景的區域，則刪除該區
域的色彩資訊，如圖 3.17 所示。 
 
 
 9 
(b) Initialization of human pose and 3D model  
construction 
 
所謂的姿態校正就是令使用者進行一個預先
指定的動作 (圖 3.23，例如請使用者將雙手舉
起)。因為該動作為事先定義始系統可以較容易辨
識出來而得到使用者的初始姿態參數。在使用者建
立人體骨架之前，須先做一姿態校正動作，以符合
事先定義好的骨架模型，使用者在校正姿態後所得
到的初始姿態參數較為準確。 
關於人體三維模型，我們定義了此模型的關節
點 (joint point)，關節點分別為頭、脖子、胸腔、
左右肩、左右手肘以及左右手共 9 點，而關節點之
間的連結我們稱肢體 (limb)，如圖 3.24 所示。每
個關節點以 3D 座標來表示。 
在姿態校正過程當中，若無法校正成功時，使
用者資訊會資訊在校正程序中循環，此時使用者需
稍微移動手臂以符合事先定義好的姿態模型。當姿
態校正完畢後就能得到 9 點關鍵點 (key point) 
如圖 3.25 (d)，這 9 點就是各關節點位置，之後各
關節點會進入追蹤程序。 
 
 
  
  圖 3.23 校正用姿態     圖 3.24 三維人體模型 
  
(a)                   (b) 
  
(c)        (d) 
圖 3.25 (a)(b)(c)(d)校正過程 
(c) Candidate Points Selection For Tracking 
 
追蹤系統中，我們要在三維點雲集合中挑選出 
9 個符合各個關節點特徵的點雲，並建立關節點彼
此之間的連線關係，以產生人體上半身的骨架做追
蹤。我們以校正姿態完成後的初始骨架參數為基礎
來預測下一時刻的骨架參數，所以我們可以假設在
某些區域範圍內，搜尋到理想關節點位置的機率較
高，然後針對該範圍做搜尋，我們可以利用一開始
校正出的各個關節點資訊鄰近範圍以一個三維遮
罩做三維空間的搜尋，如圖 3.26 所示。 
 
     
圖 3.26 三維遮罩示意圖 
圖 3.26的搜尋範圍是以 Wi×Wi大小的二維影
像遮罩與距離 z 資訊遮罩 Di，所建立出來的三維
遮罩 3Dmaski，而紅點表示上一個時間點中，關
節點 i 的位置。注意的是，由於每個關節點的運
動量不同 (例如手部關節點運動範圍大)，因此，
D、W 遮罩大小的設定是因關節點而異。所以骨
架模型中各關節點的遮罩集合  M 可以如下式 
(3.7)表示： 
}3,...,3,3{ 621 DmaskDmaskDmask   (3.7) 
經由上述六個關節點遮罩所選出的候選點依
照不同部位用不同顏色表示，如圖 3.27 所示。 
 
圖 3.27 經由三維遮罩所挑選出的各關節候選點 
(D) Tracking 
一般的追蹤方法對於姿態參數定義往往是關
節點的自由度，文獻[8][9][10]中的姿態參數在 24
至 40 個維度不等，再加上人體的姿態是多變，所
以如果在追蹤的過程中以關節點的角度做為最佳
的姿態參數是會是比較費時的，為了達到即時性的
要求，如文獻 [6] 我們的姿態參數並不包含關節
點的角度，只包含關節點的位置與各肢節的長度。 
追蹤的主要精神就是利用假設前一刻是正確
的姿態來預測目前的姿態，而且可以確定的是，我
們可以由上一刻姿態參數的附近範圍，預測出目前
最佳的姿態參數。 
本計劃所提出的追蹤法則是基於動態規劃最
佳化演算法，動態規劃 (dynamic programming，簡
稱 DP)。動態規劃在有很多重疊子問題的情況的最
 11 
Ωa
Ωb
 
      圖 3.33 遮罩 Ω 框選示意圖 
距離特徵 --當完成使用者校正用姿態辨識
後，我們可以紀錄初始骨架中各相鄰關節點間的三
維空間距離。從前景點雲中找到相鄰兩個關節點各
自的最佳候選點雲集合，如圖 3.34，並計算彼此集
合間任二個點雲的三維歐幾里德距離，最後再去與
相對應的初始骨架肢幹做比較，兩者間的差值就是
距離特徵的觀察值。 
點雲集 },...,1{ niaA i  的相鄰點雲集為
},...,1|{ mjbB j  ，記錄 A, B 兩個點雲集間
距離資訊的集合 },...1;,...,1{ mjnidD ijAB  ，其
中 jiij bad  表示 A 集合中任一點 ia 所對應的
各假設肢幹。 
依據前述所計算出來的邊緣特徵及距離特
徵，作為 DP 處理時的 node cost 與 edge cost，
再依據 DP 的計算法則，即可找出一條最小 cost 
的路徑，即對圖  3.29 的每一個關節點的 
candidate point 中找出一個最佳點為所追蹤到的
點。 
我們建立的多階段拓樸架構有可能會發生某
個階段的節點數目為零的情形，如圖 3.35 (即最佳
候選點雲集為空集合) 所示，這主要是因為某個關
節點被遮蔽或是移動太快，以至於在上一個時間得
到的關節點附近找不到目前輸入的前景點雲，即在
該搜尋空間內沒有辦法得到任何解的資訊，此時就
利用上一個時間保存下來的最佳關節點位置來取
代。 
    
圖 3.34 距離特徵示意圖 
 
 
 
圖 3.35 關節點資訊遺失示意圖 
 
 
3.3 第三年度研究方法 (101 年 8 月~102 年 7 月) 
本計畫所提出的聯合位元率控制方法架構於 
H.264/SVC 編碼標準上 [17]。我們將彩色序列作
為基本層 (Base Layer) 編碼，深度序列作為加強
層 (Enhancement Layer) 編碼。本計畫方法主要可
以分為三個重點部分：(1) 支持向量迴歸 (Support 
Vector Regression, SVR) 模型 [18][19]的建立。(2) 
彩色影像與深度影像的位元率分配。(3) 彩色與深
度影像序列的聯合位元率控制。在 SVR 模型建立
的過程中，需要先行建立訓練資料集。在本計畫
中，我們對每一個時刻的彩色影像與深度影像進行
索貝爾邊緣偵測 (Sobel Edge Detection)，以邊緣資
訊當作訓練資料中的輸入特徵值，接著嘗試各種不
同的彩色/深度位元分配比例來進行編碼並以參考
文獻 [11] 所提 3D 品質評估度量的最佳化為目
標尋找每一個時刻畫面最佳的彩色/深度位元分配
比例，當作訓練資料的正確輸出值。最後，透過收
集到的全部訓練資料建立 SVR 預測模型，其流程
如圖 3.36 所示。在 SVR 預測模型建立之後，編碼
時就可以針對每個時刻的彩色影像與深度影像進
行特徵分析，利用所建立的 SVR 模型預測當前時
刻最佳的彩色/深度位元分配比例，並進行位元率
控制。 
本計畫採用彩色/深度聯合位元率控制，也就
是說，彩色與深度影像編碼時所須的位元數係來自
一個相同的，而非獨立的。在一般的多層式編碼系
統中，每一畫面先估算一個總位元數，基本層先編
碼，加強層後編碼。若採用基本層/加強層的聯合
位元率控制，很可能導致畫面總位元率都被基本層
耗盡，而加強層編碼時因為位元數所剩不多而導致
較差品質。然而，在本論文中，由於有配套措施，
即在基本層編碼前預估其所占畫面之位元比例，而
使得基本層/加強層 (或彩色/深度) 的聯合位元率
控制變得可行。聯合位元率控制的好處是可以使得
位元率控制更加精確。圖 3.37 為本論文位元率控
制流程圖。 
 13 
depthcolor
color
color BitBit
Bit
Percent

     (3-10) 
其中 colorBit  為彩色影像實際編碼所用位元數；
depthBit 為深度影像實際編碼所用位元數。最後得
到的實際比例值便為當前時刻 (訓練資料) 的目
標輸出值。 
 
步驟五：調整不同的總位元率並回到步驟一。對於
不同的總位元率，我們收集不同的訓練樣本，以建
立不同的 SVR 模型。 
 
圖 3.39 目標輸出值求值流程圖 
輸入特徵向量 
一般來說，物體邊緣資訊的好壞對於合成影像
的 3D 品質有很大的影響。邊緣資訊越完整，其
合成影像的 3D 品質越好，反之則越差。在文獻 
[12]、[13]、[14]、[15] 中，也將邊緣資訊做為 3D 
品質評估的依據。因此，我們以邊緣資訊當作輸入
特徵，進行 SVR 模型的建立。在本論文中，我們
使用索貝爾運算子 (Sobel Operator) [16] 進行邊
緣資訊的計算，其運算公式如下： 
              

 

a
as
b
bt
tysxftswyxg ),(),(),(    (3-11) 
其中 ),( yx  為像素位置； g 為經由索貝爾運算
後的強度值； f 為原始影像的像素值； w 為索貝
爾運算遮罩，其中包含水平與垂直兩種方向，如圖 
3.40 所示。 
-1 0 1
-2 0 2
-1 0 1
       
-1 -2 1
0 0 0
-1 2 1
 
(a)索貝爾水平遮罩       (b)索貝爾垂直遮罩 
               圖 3.40 
如同目標輸出值，每一時刻的畫面皆為一筆訓
練資料，所以，每一時刻的彩色影像與深度影像均
會進行索貝爾邊緣偵測。在進行索貝爾運算之後，
以索貝爾強度值的最小值0與最大值255為界分為
4 個區域。在本計畫中進行不等分分區，如圖 3.41
所示，以索貝爾強度值 65 為一界限，強度值小於
等於 65 的區域再平均分為 3 區；強度值大於 65
的區域統整為一區，以此進行實驗。 
 
圖 3.41 本論文索貝爾強度值分區示意圖 
在進行分區之後，個別統計在 4 個區域中所佔
的像素數目比例。與前一個步驟相同，彩色影像與
深度影像皆須進行統計。因此，在彩色影像中可以
得到 4 個比例值，以 iC  表示，其中 4~1i  且 
C1+C2+C3+C4=1.0，另外，在深度影像亦可得到 4
個比例值，以  jD  表示，其中  4~1j  且 
D1+D2+D3+D4=1.0。除此之外，同時分析相同位
置的像素點在彩色影像與深度影像各屬於哪個區
域。以彩色影像與深度影像各有 4 個分佈區域來
說，共有 16 種組合情形。以圖 3.42 為例，若相同
位置像素點在彩色影像的索貝爾強度值屬於第一
區，在深度影像中亦屬於第一區，則此點像素屬於
組合 ( 1C , 1D )，以此類推。最後一樣統計每個組
合內像素數目的所占比例，共可以得到 16 個比例
值，以 ji DC  表示，其中 4~1i 、 4~1j ，
且
 

4
1
4
1
1
i j
ji DC 。 
最後我們利用這 24 個比例值 (彩色影像的 4
個比例值、深度影像 4 個比例值以及聯合分析彩色
影像與深度影像的 16 個比例值)當作訓練資料的
輸入特徵向量進行實驗。 
 15 
四、實驗結果 
4.1 第一年計畫成果 (99 年 8 月~100 年 7 月) 
4.1.1 深度影像改善結果 
圖 4.1 為兩支彩色攝影機所拍攝的彩色影
像，我們以彩色影像中色塊區域的前景/背景分類
資訊去輔助深度影像進行內插，得到的結果如圖
4.2(d)，我們可以發現在邊緣的地方都與實際的彩
色影像匹配，可以有效提升後續任意視角影像合成
的品質，避免合成影像變形。 
    
圖 4.1 拍攝的左、右眼影像 
    
(a)                   (b) 
    
 (c)                  (d) 
圖 4.2  (a)以像素為單位投影的高解析深度影像 
       (b)以 33 的區塊投影的高解析深度影像 
(c)以色彩分割輔助內插後的高解析深度
影像 
       (d)同個色塊區域取平均的高解析深度影
像 
4.1.2 三維姿態追蹤結果 
在人體姿態估測與追蹤這一部份的實驗，本計
畫以 SR4000 深度攝影機 (與前一部份使用相同的
深度攝影機) 所拍攝的一組影像序列來進行測
試，此影像序列包含了 1200 張畫面。完成校正程
序後，後續的動作主要分成三個部份，如圖 4.3 的 
(a)、(b)、(c ) 所示，底下的數字為影像編號。 
 
 
    609        645        681        686 
               (a) 
 
    743        783        839        894  
                    (b) 
 
    992       1030       1056       1120 
                    (c) 
圖 4.3 人體姿態追蹤實驗結果 
4.2 第二年計畫成果 (100 年 8 月~101 年 7 月) 
4.2.1 深度影像改善結果 
圖 4.4 為兩支彩色攝影機所拍攝的彩色影
像，我們以彩色影像中色塊區域的前景/背景分類
資訊去輔助深度影像進行內插對應像素放大，得到
的結果如圖 4.4 (c)，我們可以發現在邊緣的地方都
與實際的彩色影像匹配，可以有效提升後續任意視
角影像合成的品質，避免合成影像變形。 
    
(a)                   (b) 
    
(c)                  (d) 
圖 4.4  (a)右眼彩色影像 
   (b)以像素為單位投影的高解析深度影像 
(c)以 55 的區塊投影的高解析深度影像 
       (d)同個色塊區域取平均的高解析深度像 
4.2.2 3D 模型與 3D 視訊資料的整合結果  
在虛擬場景與 3D 視訊資料整合這一部分的
實驗，我們利用前一部分拍攝之彩色影像資訊與深
度影像資訊，藉由 3D 繪製軟體產生虛擬的三維
空間，並且依照實際比例的大小，將 3D 視訊資
料與虛擬場景結合，其結果如圖 4.5 所示。 
 
 17 
影像的整體目標位元數進行縮減以達到控制的目
的，而在個別控制中，彩色影像序列與深度影像序
列為獨立進行，即使有一序列當前時刻發生超用太
多的情形，也不會影響到另一序列在下個時刻的目
標位元數。 
表 4.2  Undodancer 位元率控制精準度結果 
 
表 4.3  Kendo 位元率控制精準度結果 
 
表 4.4  Ballons 位元率控制精準度結果 
 
表 4.5  Breakdancer 位元率控制精準度結果 
 
 
 
 
表 4.6  Ballet 位元率控制精準度結果 
 
表 4.7  Poznan_Hall1 位元率控制精準度結果 
 
表 4.8  GT_Fly 位元率控制精準度結果 
 
從表 4.2~4.8 結果可以看出，即使是對 out-lier 測
試序列的 Poznan_Hall 及 GT_Fly，位元率控制與
其他 in-lier 測試序列的控制精確度相同。  
4.3.1 3D 品質評估 
在客觀評估方面，我們使用文獻 [11] 的 3D
品質評估度量進行評比。在此實驗的比較對象有四
個： 
(1)JSVM+ 的編碼 3D 品質結果   
(2)參考文獻 [20] 的編碼 3D 品質結果 
(3)MPEG 建議量化參數組合的編碼 3D 品質結
果 
(4)最佳 3D 品質 ground truth：我們以每一筆 SVR 
訓練資料目標輸出值所對應的解碼影像進行 3D 
品質計算，並作為 ground truth。對於 out-lier 測
試影像序列，我們以類似於訓練序列的方式進行試
驗，找出每個畫面的最佳 3D 品質，作為後續比
較分析。 
 19 
驗相同。透過訓練資料的 SVR 建立方法，找出最
佳 3D 品質所對應的實際彩色/深度位元分配比例
值當作 ground truth，以進行誤差計算。誤差計算
如下式： 
         gtSVR pperror           (4.1) 
其中 SVRp  為 SVR 預測的彩色/深度位元分配比
例值； gtp  為 ground truth。 
表 4.16 為 SVR 平均誤差結果，圖 4.8 為每
組測試序列的誤差分佈情形。每組測試序列皆有
258 筆資料 (每組序列在每種總位元率皆編碼 100
張畫面，扣除 I 畫面與第一張 P 畫面不進行 
SVR 預測，在每種總位元率有 86 筆資料，共進行
3 種總位元率實驗)，透過分佈圖可以發現每組測
試序列的預測誤差主要落在  0%~20% 的區間
內，在 out-lier 測試序列 “Poznan_Hall1” 中會有
預測誤差過大的情形發生，而另一個 out-lier 測試
序列 “GT_Fly” 的 SVR 預測誤差相對於 in-lier 
測試序列亦較大。  
表 4.16  SVR 平均預測誤差 
 平均誤差 
Undodancer 7.26% 
Kendo 8.38% 
Ballons 10.31% 
Breakdancer 8.93% 
Ballet 13.19% 
Poznan_Hall1 25.03% 
GT_Fly 14.67% 
  
         (a)                   (b) 
  
         (c)                   (d) 
  
         (e)                   (f) 
 
(g) 
圖 4.8 測試序列之 SVR 預測誤差值分佈情形 
 
五、結論 
在本計劃第一年度中，我們為了讓使用者實
現遠端視訊會議的視覺互動，採用了高解度彩色和
低解析度深度攝影機 (Time-of-Flight) 來架構我
們的系統。攝影機對使用者上半身進行拍攝，得到
的資料可用於立體影像合成 (DIBR) 和人體姿態
追蹤系統上。透過 Image preprocessing、human pose 
Initialization、3D model construction、Tracking 等
程序，將輸出最佳的姿態參數套入模型達到追蹤的
效果。 
在第二年度中，我們為了讓使用者實現遠端
視訊會議的視覺互動，採用了 Microsoft Kinect 
sensor 來架構我們的深度感測系統。以深度攝影
機對使用者上半身進行拍攝，得到的資料可用於
立體影像合成 (DIBR) 和人體姿態追蹤系統上。
透 過 Image preprocessing 、 human pose 
Initialization、3D model construction、Tracking 等
程序，將輸出最佳的模型姿態參數以套入深度影
像中，達到姿態估測與追蹤的效果。而本項目所
得到的人體姿態參數將有助於將人體與虛擬背景
模型的整合 (例如把得到的人體模型置於虛擬桌
子旁，猶如視訊會議般)。 
在第三年度中，我們提出一種基於 3D 品質
最佳化的 3D 視訊 (彩色+深度)編碼聯合位元率
控制技術。本計畫將彩色影像序列與深度影像序
列進行整體的位元率控制，並利用事先建立 SVR 
預測模型，根據每個時刻的彩色影像與深度影像
特徵分佈，預測當前時刻的最佳彩色/深度位元分
配比例值，以達到立體合成影像的 3D 品質最佳
化。根據實驗結果可以發現在位元率的控制效果
方面，比起彩色/深度個別的位元率控制方式可以
精加 %83.2~%4.0  的精確度，並且相對於其他
編碼方法在大部分的 3D 視訊中都可以獲得較佳
的 3D 品質。 
本計畫至目前的研究成果投稿如下： 
1. Wen-Nung Lie, Hung-Wei Shiu, Chieh Huang, 
“3D Human Pose Tracking Based on Depth 
Camera and Dynamic Programming Optimiza- 
tion,” Proc. Of IEEE International Symposium on 
Circuits and Systems, ISCAS-2012, Seoul, Korea, 
May 2012. 
2. Jui-Chiu Chiang, Zheng-Feng Liu, and 
Wen-Nung Lie, “High Quality Novel View 
Synthesis Based on Low Resolution Depth Image 
and High Resolution Color Image,” Proc. Of Int’l 
 21 
Video Compression,” Proc. of Picture Coding 
Symposium (PCS), Nov. 2007. 
[21] Heiko Schwarz, and Dmytro Rusanovskyy, 
“Common Test  Conditions for 3DV 
Experimentation,” ISO/IEC JTC1/SC29/WG11, 
Doc. MPEG-N12745, Geneva, Switzerland, 
May 2012. 
 
 
 
2其餘 2 篇皆為 poster，第一篇係由本人親自發表，而第二篇則由共
同作者林國祥副教授 (本人的畢業博士生) 負責發表。
在參加會議期間，本人亦藉機會參觀了筑波市，感覺上，筑波
市是一個新城市，位於東京市郊，比較沒有古蹟類的東西，有點像是
台灣的新竹或是桃園，做為台北大城市的衛星城市一般。以下是本人
在發表 poster 論文時與林國祥副教授及彰化秀傳醫院劉楷哲博士的
合影。
五、與會心得:
由於開會期間與柳金章教授同住一房，因此知悉其參加 IAPR 會議
的經過。我國在影像方面的研究人口眾多，也成立了 IPPR 學會二
十餘年。因此，有必要在這個世界舞台上爭取亮相的機會 (例如爭取
未來數年中 ICPR 在台灣舉辦的機會)，而參加 ICPR 正式創造這個
機會的最好方式。
48. Visual recognition and processing
9. Image and vision sensors
10.Multimedia 3D
11.Multiview and 3D visual signal processing
此外，大會亦安排了 2 個 live demo session，讓參加者可以看到實
際的技術展品，非常值得。
在參加會議期間，本人亦藉機會參觀了北京市的古蹟，例如故宮。最
後一天大會議安排了一個 Farewell party，是到八達嶺登上長城。
以下是本人在發表 poster 論文時請長榮大學李素玲教授幫忙拍攝
的。
五、與會心得:
IEEE ISCAS 的主要探討方向為 Hardware、SoC、Multimedia、
Power system。ISCAS 歷史比 ICASSP 還要悠久，近年論文的接受
率已降至 45% 左右，可以說是大型會議中投稿量頗多，接受率不高
的，它匯集了不少有名的研究團體在這個會議上發表最新的成果，因
此可以有機會與其他專業人士作更切身的討論與交流。
以往很多 MPEG 的最新技術除了在每年的 MPEG 會議外，即是
在這個會議中發表。在 IEEE 中有很多關於多媒體視訊壓縮標準的
國科會補助計畫衍生研發成果推廣資料表
日期:2013/10/31
國科會補助計畫
計畫名稱: 子計畫三：多點 3D 模型與 3D 視訊之整合與其任意視角 3D 立體影像合成
之品質最佳化技術
計畫主持人: 賴文能
計畫編號: 99-2221-E-194-003-MY3 學門領域: 訊號處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
