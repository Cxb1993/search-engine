 1. 前言 
現代資訊技術已經見證不同使用者對數位資料的需求，雲端計算和行動平台的持續發展，不
論使用者在任何時間(anytime)、任何地點(anywhere)、任何終端設備(anydevice)皆可輕易地使
用所需的資料和應用軟體，預期未來得資訊使用者對即時的知識和智慧 將有高度的需求；
以知識為基礎的擴增實境(augmented reality, AR)技術就是一項越來越普及的例子，它提供自
然環境中所儲存物體的人工資訊，並重新得到在現實環境中的資訊。資料探勘是從大量的資
料記錄中擷取規律的樣版或模式資訊，它已經是現代資訊技術不可或缺的一環，資料探勘能
夠將混亂的資料轉換成有用的訊息，探索與分析這些資訊能夠進一步整合成為各種不同形式
的知識或智慧，因此可以預見未來的資訊服務將倚重更多更有效能的資料探勘技術。 
在衡量歸納學習(inductive learning)效果時，資料探勘技術的執行效率(efficiency)和準確
度(accuracy)同樣扮演重要的角色，然而執行效率獲得資料庫使用者更多的青睞；舉例來說，
為了協助企業獲取更高的利益，快速的決策(doing quickly)比正確的決策(doing accurately)更
具有吸引力，快速的決策可以基於目前為止收集到的資料進行推論，然而正確的決策卻必須
在收集足夠的資料之後才能進行資料探勘。決策樹是現代重要資訊技術之一，它廣泛地應用
於資料探勘和知識挖掘(knowledge discovery)領域，為了探索資料的輸入(特徵)屬性和輸出屬
性之間的交互關聯，傳統決策樹開發了許多不同的分類策略，這些策略常常遭遇到無法同時
達到高效率和高準確度的問題。 
為增進歸納學習系統的分類效能，主成份分析(Principal Component Analysis, PCA)已廣
泛應用於現今的資料分析上，利用無參數的數值統計方法，主成份分析可以從大量的混亂資
料集合中提取有用的資訊；換言之，隱藏於高維度資料背後卻具規律性的資訊，可藉由降低
維度的方式被有效地提取出來。若一組輸入屬性與輸出屬性之間存在的高度相關性，表示這
些輸入屬性在歸納學習的過程中，扮演重要的決策角色[13] ，但不幸的是，這些輸入屬性之
間大都存在高度的相關性，當這些屬性用於歸納學習時，類似的分類準則將重覆被使用，因
而無法有效提升資料的分類準確度，本研究首先將專注於鑑別輸入屬性的分類能力
(classification capability)，篩選一組有效的輸入屬性，然後再利用主成份分析消除輸入屬性間
的關聯性，並且整合這些輸入屬性，產生一個新的合成屬性(hybrid attribute)，此合成屬性可
以在不喪失決策樹準確度的情況下，有效提升決策效率。 
 
2. 研究目的 
決策樹的效率可以運用三項要點進行評估，它們分別為規則簡易性(rule simplicity)、決策準
確度(decision accuracy)及樹的成長時間(tree growing time)，為了讓這些評估值達到很好的水
準，必須正確地鑑別輸入屬性的分類能力(classification capability)。分類能力的意義是指資料
基於某些準則進行分類之後，能夠獲得的歸納改善效果；換言之，若根據特定屬性進行資料
分類之後，造成的每一個子集合(subclass)內的目標屬性值能夠得到較高的相似性和一致性，
我們稱該輸入屬性對目標屬性具有高度分類力。許多研究對資料的相似性和一致性提出不同
的測量方法，例如，goodness score考慮分類資料中的大多數成分(overall majority)和類別數
量(class number)兩項因素，一般將大多數成分視為正確的 score。資訊獲利(information gain)
及熵值(entropy)首先計算子類別中的資訊量，接著將個別的資訊量整合為整體資訊量(overall 
information amount)。除了資訊理論(information theory)中已考慮的因素外，資訊獲利率
3. 文獻探討 
最早的歸納學習方法是由 Hunt 等人[6]所提出的概念式學習系統(concept learning system, 
CLS)，他們採用向前搜尋極大極小機制(MinMax mechanism)的方法，促使資料在搜尋空間上
有固定的深度，此方法的目地是使決策樹達到最小的分類成本。Shannon 於 1949年提出資訊
理論(information theory)並利用熵函數(entropy function)測量隨機變數的不確定性。Quinlan 所
提出 ID3 演算法[8, 10]決策樹演算法是基於 CLS 的設計問題和熵值所發展而來，ID3 演算法
明顯地改善歸納學習的效果，大幅降低決策樹的規模，因而產生的決策規則較先前的方法更
簡單且易於明瞭。 
ID3 演算法的兩項缺點是過度配適(over fitting)的結構和無法處理連續性數值資料，C4.5
採用資訊獲利率(gain ratio)來改善過度配適的問題，資訊獲利率的設計增加了一項重要因素
－群體在分類之前的整體資訊量。為使 ID3 演算法穩定地執行歸納學習，過去許多文獻專注
於資料的前置處理[5, 11, 12]；然而，資料前置處理的目的在於方便區別輸入屬性值(或特徵
值)，前置處理後的數值無法用於推估輸出屬性值(或目標值)，因此資料前置處理無法有效提
升屬性的分類能力。 
C5.0 演算法[9]是 C4.5 演算法的修訂版，它採用 boosting 方式提高分類的準確率並適用
於處理大型資料庫，所以又稱作 boosting tree，C5.0 演算法可以利用較少的記憶體資源，來
達成資料的快速運算。另外，分類與迴歸樹(Classification And Regression Tree, CART)[2]與
C4.5/C5.0 主要的差異在於二元的分類策略，CART 分類樹是處理類別變數的輸出屬性，CART
迴歸樹是處理連續變數的輸出屬性。卡方自動互動檢視法(Chi square automatic interaction 
detector, CHAID)[4]僅限於處理類別變數使用，在每次分割時利用卡方檢定(chi-squared)檢驗
對於輸出變數是否產生最大的類別差異，表 1 整理上述方法的分類策略。這些方法的共同點
是採用單一屬性對資料進行逐步的分類，因此決策樹下層的歸納學習效果完全依賴於上層分
類結果的良窳。 
 
表 1： 各種分類器之分類策略 
演算法 分類方法 是否修剪 
ID3 entropy, information gain Yes 
C4.5/C5.0 gain ratio Yes 
CART gain ratio Yes 
CHAID Chi-square detector No 
 
4. 研究方法 
我們決策樹的分類策略基於三個步驟：步驟一、挑選一組具有高分類能力且彼此之間具有低
相關的輸入屬性；步驟二、藉由統計分析方法以消除所挑選屬性間的相關性；步驟三、最後
將這些屬性整合成為一個合成屬性，並將此合成屬性作為決策樹樹根。 
一、選擇有效的輸入屬性 
ID3 演算法藉由資訊獲利的計算，挑選資訊獲利高的屬性，但在挑選的過程中，卻容易
傾向資料分佈較廣的屬性；舉例來說，一個特定屬性 X 在每筆記錄中都呈現不同屬性值，那
外的其他輸入屬性，處理流程如下列演算法中步驟 5.4.1至 5.4.2。 
1. 設 T 為一組訓練案例。 
2. 基於凝聚度的計算，挑選對於 T 的輸出屬性來說，擁有最佳區別能力的 k個輸入屬
性。 
3. 將挑選的 k個輸入屬性進行整合，以產生合成屬性 H。 
4. 以 H作為最初資料分類的依據，為決策樹的樹根制定分類法則。 
5. 為了決定樹根最佳的分支度，使用凝聚度公式(1)的計算方法，對於不同的分支數進
行比較，挑選出最佳的凝聚度值。進行流程如下： 
5.1 初步先設定分支度為二(br=2)， 
5.2 計算 )(TConbrH 和 )(1 TConbrH + 。 
5.3 若 )()(1 TConTCon brHbrH >+ 那麼 br=br+1 並回至步驟 5.1。 
5.4 對於步驟 5.2 的每個子集合 ),,,( 21 brTTT L 進行下列判斷： 
5.4.1 若有一個子集合的所有案例都有相同的輸出屬性時，則不再進行進一步的
分類，反之則進行步驟 5.4.2。 
5.4.2 在此子集合中，在未挑選到的輸入屬性中，運用凝聚度公式(1)選擇最有鑑
別力的輸入屬性。  
6 傳回產生的決策樹。 
在第二層分類的過程中，若賸餘輸入屬性的凝聚度非常相近，若執意再進行分類將不會有明
顯的助益，為防止過度配適造成預測的偏差，故建議不再持續的進行分類工作，本研究所建
構的決策樹至多在二階層內即可完成歸納學習。 
 
5. 結果與討論 
本研究使用取自 UCI機器學習知識庫(UCI machine learning repository)[1]的三個資料庫，分別
為皮膚科資料庫(DB1)、泥土抗壓強度資料庫(DB2)及玻璃識別資料庫(DB3)，表 3 分別列出
三個資料庫的全部案例個數(標示為 N)、訓練案例個數(標示為 T)、輸入屬性個數(標示為 A)
及輸出屬性的資料範圍(標示為 R)，其中 T 的構成案例來源自 75%的全部案例(N)。另外，輸
出屬性值基於不同應用領域而有不同的整數值，三個資料庫均只有一個輸出屬性，且資料為
整數型態。 
 
表 3： 三個資料庫 
 DB1 DB2 DB3 
N 365 1030 214 
T 274 773 161 
A 33 8 10 
R [1,6] [1,9] [1,7] 
 
基於熵值(C4.5)及凝聚度的計算，第一項實驗針對被選取的輸入屬性，觀察它們之間的
相關性，在表 4 中，pick3、pick4、與 pick5 分別表示有三個、四個、與五個輸入屬性被挑選
表 5：分類器的準確度 
 C4.5 our 
準確度 
達成率 
DB1 50.40%(6) 37.74%(1) 0.75 
DB2 42.41%(7) 32.30%(1) 0.76 
DB3 60.38%(4) 37.74%(1) 0.63 
 
由於數位資料庫的數量快速成長且多樣性，使得應用領域更為廣泛，正因為如此，為了
產生滿足使用者需求的答案，往往消耗大量的時間在分析資料上，因此任何方便於資料探勘
的方法，對於現代資訊技術來說愈來愈具有吸引力。就歸納學習研究領域而言，我們提出對
於資料評估上的一些新觀點，及對於資料處理的一種新分類策略，本研究的主要貢獻是使歸
納學習的表現更有效率，未來研究方向是集中於同時兼顧並增進資料分類的準確度及效率。 
 
   
Abstract—Classification by using of multiple variables is a 
frequently encountered data mining problem. Conventional 
classification models can either suffer from insufficient data 
collection or be burdened with overabundant data. In this paper, 
we propose a novel model in generating a robust multivariate 
classifier to solve the overabundance case. The classification 
problems with multiple objectives can be supported by a subset 
of effective variables identified by our scheme. Traditional Gini 
index and principal component analysis (PCA) are integrated to 
complete our classification model. Some experiments based on 
practical databases are conducted to verify the robustness of our 
method. 
 
Index Terms—Multi-objective classification, Gini index, PCA, 
multivariate classifier 
 
I. INTRODUCTION 
he ubiquitous information tools and technology allows us 
to collect and store a variety of data for various 
application domains. The ripe development of 
multi-dimensional database management, artificial 
intelligence, expert system, and data mining techniques makes 
easy implementation of various observation, examination, 
study, or analysis on the stored data. Based on these 
advancements of computer technologies and computational 
techniques, decision makers have gained more momentum to 
tackle difficulties and even to make precise predictions. 
Classification is a frequently encountered problem where a 
categorical dependent variable needs to be predicted based on 
a subset of independent variables. Many classification 
problems including web page classification [25], web spam 
detection [3, 6], intrusion detection [7], mobile commerce 
behavior [19], fraud detection [5], bankruptcy prediction [29], 
medical diagnosis [9, 12], and crime activity analysis [10], 
have attracted many attentions and encouraged new research 
stream.  
In order to simplify data processing procedure and in turn 
promote classification performance, data dimension 
reduction is essential before data analysis is preceded. 
Especially when high dimensional features are considered, 
 
This work was supported by the Nation Science Council of ROC under 
Grant No. 99-2221-E-025-012. Hung-Yi Lin is now an associate professor 
with the Department of Distribution Management, National Taichung 
Institute of Technology, Taiwan R.O.C., (e-mail: linhy@ntit.edu.tw). 
Yu-Han Lai is now a graduate student with the Department of 
Distribution Management, National Taichung Institute of Technology, 
Taiwan R.O.C., (e-mail: guiwui77@gmail.com). 
the use of proper features is crucial to subsequent handles.  
Feature extraction and feature selection are two familiar 
strategies in achieving data dimension reduction. The 
methods of feature extraction transform or arrange some 
original features into a single new feature which is more 
capable of classification task [20, 22, 27]. The methods of 
feature selection generally adopt some specific criteria to 
evaluate original features and then select a subset of proper 
features for classification task [17, 23, 28, 30]. In this paper, 
feature selection and feature extraction are integrated in our 
two-stage scheme for efficient classification.  
Dataset size and data diversity in modern digital 
applications keep growing and changing [13], the handle of 
multi-objective problems necessitates accurate data analysis 
and effective data processing. Many studies are dedicated to 
analyze various types of data and develop new split criteria 
for achieving adequate categorization. Multivariate analysis 
methods with the integration of the effective use of multiple 
independent variables and the collaboration of suitable 
classifying scheme have verified their validations [11, 21, 24, 
34]. A new PCA-based multivariate classification method 
which takes data granularity into account is proposed to 
manipulate multi-objective problems in this paper. 
The remainder of this paper is organized as follows. In 
Section 2, some related studies are discussed. The third 
section proposes a new model with two stages for the 
generation of new multivariate classifier. The experimental 
results are given in Section 4. We conclude this paper in 
section 5. 
II. RELATED WORK AND BACKGROUND 
Complexity, efficiency, and accuracy are three principles in 
assessing classifiers. The number of variables selected and the 
association handles based on these selected variables are two 
important factors impact the complexity of classifiers. And 
typically, the complexity of one classifier should be 
disproportional to its classification efficiency. So, one 
classifier with simple handle can process classification with 
efficiency. Nevertheless, efficiency does not grantee accuracy. 
Many classification methods devote so many efforts on data 
processing and analyzing that they sacrifice classification 
efficiency in return for higher classification accuracy. 
Identifying a subset of useful variables and taking effectual 
handles are no doubt the way to promoting efficiency and 
accuracy. 
On another hand, in order to collect sufficient information, 
every instance usually includes a lot of categorical and/or 
Construction and Evaluation of a Robust 
Classification Model for Multi-objective 
Problems 
Hung-Yi Lin and Yu-Han Lai, Member, IAENG 
T 
 In the following, we propose an algorithm which proposes 
a heuristic scheme in arranging a compact subset of features 
for high classification quality. The following notations are 
used in the algorithm. 
 A: The set contains all original independent attributes 
for a specific dataset. 
 B1, B2: The sets used to collect the effective attributes. 
 α, β, γ: The first, second and third effective attributes 
extracted from set A. 
 r(β, γ): The correlation coefficient between attributes β 
and γ. 
 PCA(S): After applying principle component analysis 
over a set S, the eigenvalue of the first component is 
outputted.  
 
Algorithm 
Input: The attribute set A. 
Output: A subset of attributes extracted from A.  
1. B1←φ; B2←φ. 
2. Pick the attribute “α” with the least Gini index among all 
attributes. Remove it from A and add to B1.  
3. Choose the next attribute “β” from A which 
minimizes |),(|)(Gini βαβ r× . Remove β from A and add 
to B1. 
4. Similarly, attribute γ is picked from A for the reason that 
it minimizes |),(),(|)(Gini γβγαγ rr ⋅× . Delete γ from A 
and B2 ← B1∪{γ}. 
5. While PCA(B2) > PCA(B1) do 
6. Begin 
7.      Choose attribute x from A which minimizes     
|),(|)(Gini
2
yxrx
By∈
∏× . 
B1 ← B2; B2 ← B2∪{x}. 
8. End 
9. Return B2. 
 
We notice that |),(|)(Gini
2
yxrx
By
Π
∈
×  in step 7 is used to 
measure the enhanced effect of feature x on B2. For instance, 
suppose set A contains xp, xq and xr and set B contains yi and yj, 
then three measurements    |),(),(|)(Gini jpipp yxryxrx ⋅× , 
|),(),(|)(Gini jqiqq yxryxrx ⋅×  and 
|),(),(|)(Gini jrirr yxryxrx ⋅×  are compared for the next 
authenticated attribute. 
 
B. Feature Extraction 
To promote the classification quality as far as possible, 
principal component analysis (PCA) is applied on the 
authenticated attributes in B2. All dta corresponding to the 
authenticated attributes are preprocessed into quantitative 
values for PCA. The first component with the maximum 
variance (eigenvalue) is taken to extract the major essence of 
attributes. We neglect other components for their minor data 
variance explanation rates. Continue the previous example 
with table I, y=－0.1858 x1’+0.9826 x3’ is generated after 
PCA is applied. Table III lists the preprocessed data and 
transformed data corresponding to every original data.  
  
Final inductive learning completes this example, the 
classification rule is concluded as that entities with 
component values fall in [0, 1], [1, 2], or [2,3] are respectively 
categorized into class “bus”, “train”, or “car”. The accuracy 
of this example is 0.9 since the eighth entity is the unique 
mismatched record. More datasets are employed in the next 
section for the verification of our model.    
IV. EXPERIMENTAL RESULTS 
Three multi-objective datasets are adopted in our 
experiments. The first dataset is German credit approval 
(DB1) which is extracted from the UCI [2]. The second and 
third dataset are residential quality (DB2) and wealth (DB3) 
which are taken from [1].  Table IV lists the related 
information including the total number of instances (N), the 
number of input attributes (A) and the number of outcomes for 
the target attribute (R). We adopted 10-fold cross-validation 
to train and test these datasets for 10 times. So, the 
experimental results with numeric measures were averaged to 
produce a single estimation. In following subsections, 
statistical analyses, classification effectiveness and 
classification performance are measured for detail 
verification. 
 
TABLE IV THREE DATASETS 
 DB1 DB2 DB3 
N 1000 329 252 
A 20 9 15 
R 4 4 5 
 
A. Statistical analyses 
Our first experiment measures the correlation 
coefficients between attributes in order to observe the 
reduplication degree from different attributes. In Table V, the 
higher correlations are found in the entropy-based model C4.5 
while this suffering has less impact on our model. 
 
 
TABLE II MEASURE FOR INTEGRATED GINI 
(1) 
(xi∪xj) 
(2) 
|rij| 
(3) 
Gini(xi)×Gini(xj) 
(4) 
(2)×(3) 
(5) 
Gini(xi∪xj) 
(x1∪x2) 0.1429* 0.2700 0.0386 0.23 
(x1∪x3) 0.2294 0.0960 0.0220 0.10 
(x1∪x4) 0.3162 0.2160 0.0683 0.25 
(x2∪x3) 0.6227 0.0720 0.0448 0.13 
(x2∪x4) 0.6776 0.1620 0.1098 0.25 
(x3∪x4) 0.7255 0.0576 0.0418 0.13 
 
 
TABLE III MEASURE FOR INTEGRATED GINI 
Attributes Component Classes 
x1 x3 x1’ x3’ 
y=－0.1858 
x1’+0.9826 x3’ 
 
male cheap 2 1 0.6110 bus 
male cheap 2 1 0.6110 bus 
female cheap 1 1 0.7968 bus 
male cheap 2 1 0.6110 bus 
female expensive 1 3 2.7620 car 
male expensive 2 3 2.5762 car 
female expensive 1 3 2.7620 car 
female cheap 1 1 0.7968 train 
male standard 2 2 1.5936 train 
female standard 1 2 1.7794 train 
 
 [3] L. Becchetti, C. Castillo, D. Donato, R. Baeza-Yates and S. Leonardi, 
“Link analysis for Web spam detection,” ACM TWEB Transactions on 
the Web, vol. 2, no. 1, 2008. 
[4] T. Bellotti and J. Crook, “Support vector machines for credit scoring 
and discovery of significant features,” Expert Systems with 
Applications, 36(2), 3302-3308, 2009. 
[5] R. J. Bolton and D. J. Hand, “Statistical fraud detection: a review,” 
Statistical Science, vol. 17, no. 3, pp. 235–255, 2002. 
[6] C. Castillo, D. Donato, A. Gionis, V. Murdock and F. Silvestri, “Know 
your neighbors: web spam detection using the web topology,” in Conf. 
Annual ACM Conference on Research and Development in 
Information Retrieval, Netherlands Amsterdam, 2007, pp. 423-430. 
[7] V. Chandola, A. Banerjee and V. Kumar, “Anomaly detection: A 
survey,” ACM CSUR Computing Surveys, vol. 41, no. 3, 2009.  
[8] L. Chen, Q. Ye and Y. Li, “Research on GA-based bank customer's 
credit evaluation,” Computer Engineering, 32(3), 70-72, 2007 
[9] D. Delen, G. Walker and A. Kadam, ”Predicting breast cancer 
survivability: a comparison of three data mining methods,” Artificial 
Intelligence in Medicine, vol. 34, no. 2, pp. 113-127, 2005. 
[10] V. Estivill-Castro and I. Lee, “Data mining techniques for autonomous 
exploration of large volumes of geo-referenced crime data,” in Proc. 
6th International Conf. on Geocomputation, 2001. 
[11] J. A. Etzel, V. Gazzola and C. Keysers, “Testing simulation theory with 
cross-modal multivariate classification of fMRI data,” PLoS ONE, vol. 
3, no. 11, 2008. 
[12] Y. Feng, Z. Wu, X. Zhou, Z. Zhou and W. Fan, “Knowledge discovery 
in traditional Chinese medicine: State of the art and perspectives,” 
Artificial Intelligence in Medicine, vol. 38, no. 3, pp. 219-236, 2006. 
[13] J. F. Gantz. (2007, March). A Forecast of Worldwide Information 
Growth Through 2010. Available: 
http://www.emc.com/collateral/analyst-reports/expanding-digital-idc-
white-paper.pdf. 
[14] A. Genkin, D.D. Lewis and D. Madigan, “Large-scale Bayesian 
logistic regression for text categorization,” Technometrics, 49(3), 
291-304, 2007. 
[15] C. Gini, “Variabilità e mutabilità,” Reprinted in Memorie di 
metodologica statistica (Ed. Pizetti E, Salvemini, T), Rome: Libreria 
Eredi Virgilio Veschi, 1912. 
[16] X. Hao, W. Deng-sheng and X. Yang-qun, “Study on enterprise credit 
evaluation based on PCA/FCM,” Technology Economics, 3, 2007. 
[17] M. M. Kabira, M. M. Islamb and K. Murase, “A new wrapper feature 
selection approach using neural network,”, Neurocomputing, vol. 73, 
pp. 3273-3283, 2010.  
[18] A. Khashman, “Neural networks for credit risk evaluation: 
Investigation of different neural models and learning schemes,” Expert 
Systems with Applications, 37(9), 6233-6239, 2010. 
[19] H.-W. Kim, H. C. Chan and S. Gupta, “Value-based adoption of 
mobile internet: An empirical investigation,” Decision Support Systems, 
vol. 43, no. 1, pp. 111-126, 2007. 
[20] N. Kwak and J.-W. Lee, “Feature extraction based on subspace 
methods for regression problems,” Neurocomputing, vol. 73, pp. 
1740-1751, 2010. 
[21] S. K. Lee, “On generalized multivariate decision tree by using GEE,” 
Computational Statistics & Data Analysis, Vol. 49, Issue 4, no. 15, pp. 
1105-1119, Jun. 2005. 
[22] G.-C. Luh and C.-Y. Lin, “PCA based immune networks for human 
face recognition,” Applied Soft Computing, 2010. 
[23] S. Maldonado, R. Weber and J. Basak, “Simultaneous feature selection 
and classification using kernel-penalized support vector machines,” 
Information Science, vol. 18, pp. 115-128, 2011. 
[24] C. Orsenigo and C. Vercellis, “Multivariate classification trees based 
on minimum features discrete support vector machines,” IMA Journal 
of Management Mathematics, vol. 14, no. 3, pp. 221-234, 2003. 
[25] X. Qi and B. D. Davison, “Web page classification: Features and 
algorithms.” ACM CSUR Computing Surveys, vol. 41, no. 2, 2009. 
[26] L. E. Raileanu and K. Stoffel, “Theoretical Comparison between the 
Gini Index and Information Gain Criteria,” Annals of Mathematics and 
Artificial Intelligence, vol. 41, no. 1, pp. 77-93, 2004. 
[27] M. Šušteršič, D. Mramor and J. Zupan, “Consumer credit scoring 
models with limited data,” Expert Systems with Applications, vol. 36, 
no. 3, pp. 4736-4744, 2009. 
[28] C. F. Tsai and Y. C. Hsiao, “Combining multiple feature selection 
methods for stock prediction: Union, intersection, and 
multi-intersection approaches,” Decision Support Systems, vol. 50, pp. 
258-269, 2010. 
[29] C. F. Tsai, “Feature selection in bankruptcy prediction,” 
Knowledge-Based Systems, vol. 22, no. 2, pp. 120-127, 2009. 
[30] C. M. Wang and Y. F. Huang, “Evolutionary-based feature selection 
approaches with new criteria for data mining: A case study of credit 
approval data,” Expert Systems with Applications, vol. 36, pp. 
5900-5908, 2009.  
[31] C. Wu and H. Xia, “Study of personal credit evaluation under C2C 
environment based on support vector machines ensemble,” 
International Conference on Management Science and Engineering 
(pp. 25-31). CA: Long Beach, 2008. 
[32] L. Yu, W. Yue, S. Wang and K. K. Lai, “Support vector machine based 
multiagent ensemble learning for credit risk evaluation,” Expert 
Systems with Applications, 37(2), 1351-1360, 2010. 
[33] H. Zhao, “A multi-objective genetic programming approach to 
developing Pareto optimal decision trees. Decision Support Systems, 
43(3), 809-826, 2007. 
[34] M. Zucknick, S. Richardson and E. A. Stronach, “Comparing the 
characteristics of gene expression profiles derived by univariate and 
multivariate classification methods,” Statistical Applications in 
Genetics and Molecular Biology, vol. 7, no. 1, 2008. 
 
 
 
 出席國際學術會議心得報告 
                                                             
計畫編號 NSC 99-2221-E-025-012 
計畫名稱 運用高效能與高準確度決策樹於多元目標問題之研究 
出國人員姓名 
服務機關及職稱 
林泓毅 臺中技術學院 流通管理系  副教授 
會議時間地點 Hong Kong, 16-18 March, 2011. 
會議名稱 IAENG International Conference on Data Mining and Applications (ICDMA'11) 
發表論文題目 
Construction and Evaluation of a Robust Classification Model for 
Multi-objective Problems 
 
一、參加會議經過 
本人於 2011/3/18  14:00 - 15:45pm ICDMA II [Seminar Room C ]進行 Oral presentation, 
Session Chair 為 Mr. Baris Yildiz,其間分別聆聽來自日本、中國與印度等學者的研究報告如下： 
 Feature Selection and Boosting Techniques to Improve Fault Detection Accuracy in the 
Semiconductor Manufacturing Process / Dr. Kittisak Kerdprasop  
 Mining Interesting Patterns and Rules in a Time-evolving Graph / Mr. Yuuki Miyoshi  
 A Two-layer Model for Interactive Mining of Frequent Patterns / Dr. M.H. Nadimi-Shahraki   
 A Comparative Study on Data Perturbation with Feature Selection / Prof. Jun Zhang。 
本人投稿之論文 Construction and Evaluation of a Robust Classification Model for 
Multi-objective Problems，獲得熱烈討論，其中中國大陸學者Prof. Jun Zhang 提供許多寶貴建
議，會後本論文獲得 The Best Paper Awards and Best Student Paper Awards of the IMECS 2011。
大會亦舉辦盛大晚宴，參加晚宴的學者與研究相關人員眾多，氣氛融洽。  
 
二、與會心得 
參與此次研討會本人有三個極深刻的感觸：第一、許多開發中國家的學生素質均有亮麗
的表現，不僅俱備英文溝通能力，也有優秀的學術溝通能力。第二、中國大陸的學者與研究
人員在國際研討會的表現愈加活躍，不僅在論文能見度方面，亦或研究生的參與度方面，以
至於舉辦國際研討會的能力，皆令人印象深刻；交流結識許多與會的大陸學者後，瞭解大陸
正朝向鼓勵學術研究，並支持學者往國際舞臺發展，大陸的教育品質與學術能量正不斷往上
提升。第三、此次研討會的主辦單位成功地吸引全世界學者持續參與研討，論文品質穩定並
獲得Engineering Index 收錄，不僅吸引優秀學者持續支持 IAENG 工程學會，提高主辦單位知
名度，更成功地行銷了城市觀光價值。 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：林泓毅 計畫編號：99-2221-E-025-012- 
計畫名稱：運用高效能與高準確度決策樹於多元目標問題之研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 3 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
