Z`Ł
ÌßÆÝÝxŁýÝJóÝ!õºÕ9Î@®¢Yi`aÛ
DÙ(ECC)Yi`a°5Ł.ó(ECM)ÝxŁP0ECMÎ*^à¼<
\RSAÛDÙôµÎ5ŁJóÝ.óÝ×ó °(GNFS)ªt`
 ÝTPÌßè¿àî(%+«)îÝ%§ (GPU)¼BÃ
Jó!õºÕ, ¬@×ÍÊ)ECMÝJóP0ÌGECCBAL, h
ÝêGECM3×é\îÝ>t&S!`ôÎt·ÝP}f	ECMÝ
¢óB1 = 8192, GECCBAL |3×¬PC îNJÖÕ605 f`a, 3Intel ã
ÝQ6600 °mT§ î, êGàGNU9¥ÞºÕP0(GMP)X@®ÝECM N
JÖG§172 f`aÍ@~I	ÑJ§7FÕºÈÎ
n"C: Yi`aÛDÙ, Yi`a°, %§ , !õºÕ, RSAÛDÙ
Abstract
Modular arithmetic of big integer numbers lies at the core of the elliptic curve cryp-
tography (ECC). It is also at the center of the elliptic curve method (ECM) of integer
factorization, an important subroutine in factoring RSA numbers with general number
ﬁeld sieves. Needless to say, high performance modular arithmetic is key to wider de-
ployment of ECC as well as success of ECM. In this project, we explore the possibility of
speeding up modular arithmetic using graphics processing units (GPUs). We develop an
arithmetic library, called the GPU-accelerated elliptic curve cryptography big-number
arithmetic library (GECCBAL), with which we are able achieve record-setting perfor-
mance for ECM. For example, we report a throughput of 604.99 curves per second for
ECM stage 1 with B1 = 8192 for 280-bit integers on a single PC. State-of-the-art ECM
software implementation using the GMP (GNU Multi-Precision) library handles 171.42
curves per second for the same ECM stage 1 setup using all four cores of an Intel 2.4
GHz Core 2 Quad Q6600 CPU. These results are being prepared/revised for submission.
Keywords: ECM, modular arithmetic, graphic processing units, RSA, ECC
2
GECCBAL: A GPU-accelerated
Elliptic Curve Cryptography
Big-number Arithmetic Library
October 31, 2008
Abstract
Modulo arithmetic of big integer numbers lies at the core
of the elliptic curve cryptography (ECC). It is also at the
center of the elliptic curve method (ECM) of integer factor-
ization, an important subroutine in factoring RSA numbers
with general number field sieves. Needless to say, high-
performance modulo arithmetic is key to wider deployment
of ECC as well as success of ECM. In this project, we ex-
plore the possibility of speeding up modulo arithmetic using
graphics processing units (GPUs). We develop an arithmetic
library, called the GPU-accelerated elliptic curve cryptography
big-number arithmetic library (GECCBAL), with which we are
able achieve record-setting performance for ECM. For exam-
ple, we report a throughput of 604.99 curves per second for
ECM stage 1 with B1 = 8192 for 280-bit integers on a single
PC. The state-of-the-art GMP-ECM software handles 171.42
curves per second for the same ECM stage 1 setup using all
four cores of an Intel 2.4 GHz Core 2 Quad Q6600 CPU.
1 Motivations and Objectives
Elliptic curve cryptography (ECC), independently suggested
by Koblitz and Miller in 1985, is a general approach to public-
key cryptography. It can be used in encryption and key-
exchange schemes, as well as digital-signature schemes. ECC
has emerged as a readily available public-key cryptographic
standard that is replacing aging cryptosystems such as RSA.
For example, the National Security Agency of the United
States has determined that, rather than increasing key sizes be-
yond 1024 bits, a switch to elliptic curve technology is war-
ranted for beyond-1024-bit public-key cryptography in com-
mon use today.
Similar algebraic structures on elliptic curves can also be
used in other aspects of cryptography. For example, Lenstra’s
elliptic curve method (ECM) of integer factorization [1] is an
important subroutine in factoring RSA numbers using general
number field sieves (GNFS). First introduced as a generaliza-
tion of Pollard’s p−1 andWilliams’ p+1-method, many speed-
ups and good choices of elliptic curves were suggested, and
ECM is now the method of choice to find factors in the range
1010 to 1060 of general numbers. The largest factor found by
ECM was a 222-bit factor of the 1266-bit number 10381 + 1
found by Dodson [2].
The core of ECC computation is modulo arithmetic of large
integer numbers, typically of hundreds of bits long. How to
efficiently perform such arithmetic operations is of crucial im-
portance to wider acceptance and adoption of ECC in various
cryptographic application scenarios. It is certainly no less im-
portant, to say the least, to cryptanalysis application scenarios
such as the use of ECM in the general number field sieves, in
which there is a huge volume of factorization that needs to be
executed as fast as possible.
Graphic processing units (GPUs) have recently gained con-
siderable attention as a general-purpose computing machinery
for highly data-parallel computations. Modern GPUs can de-
liver a raw computational power that is several orders of mag-
nitudes higher than the general-purpose CPUs. GPUs achieve
their high throughput through massive parallelism—typically
tens to hundreds of processors running at clock frequencies
slightly lower than that of state-of-the-art CPUs. For example,
the NVIDIA GeForce 8800 GTS has 128 processors running
at 1.625 GHz. GPUs are particularly suitable for executing
data-parallel algorithms because the architectural design de-
votes large silicon areas to computation rather than complex
control of execution, or cache memory and its management.
In this project, we have exploited these opportunities af-
forded by modern GPUs and built a fast, GPU-accelerated el-
liptic curve cryptography big-number arithmetic library (GEC-
CBAL). We report on our implementations on several NVIDIA
GPUs and show that they can achieve a better performance-to-
cost ratio, even compared with the most competitively-priced
Intel CPUs. Through GECCBAL, we have demonstrated that,
in contrast to the general perception that GPUs are only good at
graphics applications, decent integer-arithmetic performance
can also be obtained with inexpensive, off-the-shelf graphics
hardware. Hence, a cluster of hybrid computers consisting
of both CPUs and GPUs can have significant contribution to
cryptanalysis challenges such as the factorization of RSA num-
bers.
2 Design and Implementation
2.1 CUDA: The Working Platform
Our choice of development platform is NVIDIA’s CUDA,
which provides an environment in which software program-
mers can program GPUs using a high-level, C-like language.
A CUDA program, called a “kernel” starts with a source file
foo.cu, which is first compiled by nvcc, the CUDA com-
piler, into code for a virtual machine (foo.ptx), then con-
verted into actual machine code by the NVIDIA driver, and
finally loaded and run.
CUDA adopts a super-threaded, massively parallel compu-
tational model, in which computation is broken down and exe-
cuted by a large number (typically thousands) of threads. The
seemingly concurrent execution of these threads is mapped
onto to a pool of streaming processors (SPs), e.g., 128 in G80.
Those threads mapped onto the same physical processor time-
share the underlying processor. Such time-sharing is necessary
for effectively hiding instruction latency, similar to the reason
why modern superscalar CPUs search for independent instruc-
tions to issue in a program stream. As a result, the parallelism
1
The CPU first prepares the curve parameters (including the
coordinates of the starting point) in an appropriate format and
pass them to the GPU for scalar multiplication, whose result
will be returned by the GPU. The CPU then does the gcd com-
putation to determine whether we have found any factors.
Our implementation of modular arithmetic in essence turns
a multiprocessor on a GPU into an eight-way modular arith-
metic unit (MAU) that is capable of carrying out eight modular
arithmetic operations simultaneously. How to map our elliptic
curve computation onto this array eight-way MAUs on a GPU
is of crucial importance.
We have explored two different approaches to use the eight-
way MAUs we have implemented. The first one is straightfor-
ward: we compute on eight curves in parallel, each of which
uses a dedicated MAU. This approach results in 2 kilobytes of
working memory per curve, barely enough to store the curve
parameters (including the base point) and the coordinates of
the intermediate point. Besides the base point, we can not
cache any other points, which implies that the scalar multipli-
cation can use only a non-adjacent form (NAF) representation
of s. So we need to compute log
2
s doublings and on average
(log
2
s)/3 additions to compute the scalar multiplication [s]P˜ .
In the second approach, we use two MAUs to compute the
scalar multiplication on a single curve. Because our imple-
mentation uses Montgomery representation of integers, it does
not benefit from multiplications with small values. In particu-
lar, multiplications with the curve coefficient d take the same
time as general multiplications. We assume that all curve ad-
ditions are mixed additions, i.e., the base point is given in
affine coordinates. Inspecting the explicit formulas, one no-
tices that both addition and doubling require an odd number
of multiplications/squarings. In order to avoid idle multiplica-
tion cycles, we developed parallel formulas that pipeline these
two operations. The scalar multiplication can be composed
of the building blocks DBL-DBL (doubling followed by dou-
bling), mADD-DBL (mixed addition followed by doubling)
and DBL-mADD. Note that there are never two subsequent ad-
ditions. At the very end of the scalar multiplication, one might
encounter a single DBL or mADD, in that case one processor
is idle in the final multiplication.
We note that in our current implementation, concurrent exe-
cution of a squaring and a multiplication does not result in any
performance penalty since squaring is implemented as multi-
plication of the number by itself. Even if squarings could be
executed somewhat faster than general multiplications the per-
formance loss is minimal—e.g., instead of needing 3M+4S per
doubling, the pipelined DBL-DBL formulas need 4M+3S per
doubling [3].
We also kept the number of extra variables to a minimum.
The pipelined versions need one extra variable compared to
the versions on a single processor but since now two proces-
sors share the computation, which frees up enough memory
so that we can store the eight points P˜ , [3]P˜ , [5]P˜ , . . . , [15]P˜
per curve. We store these points in affine coordinates—this
way they consume only two Z/n elements’ worth of storage
space, not to mention that mixed additions are faster than full
ones. With these precomputations, we can use a signed-sliding
window method to compute [s]P˜ . This reduces the number of
mixed additions to an average of (log
2
s)/6 (and worst case of
(log
2
s)/5).
3 Results and Discussions
We summarize the experimental results in Tables 1 and 2. Our
experiments consist of running stage-1 ECM with B1 ranging
from 210 to 220 on integersm of 280 bits on various CPUs and
GPUs. For CPU experiments, we run GMP-ECM, the state-of-
the-art implementation of ECM. Note that for some GPUs, we
have serial and parallel ECM implementations, in which case
both are presented in the table. We are unable to make parallel
ECM to run on G80 and G92 because they do not have enough
registers to accommodate the more complicated control code.
The bottommost row represents the situation in which we use
CPUs and GPUs simultaneously for ECM computations.
For each implementation we derive two sets of performance
numbers based on cycle-accurate measurements of ECM ex-
ecution time: the per-second throughput of modular multipli-
cation, and the per-second throughput of elliptic-curve scalar
multiplication. In some GPU experiments we are unable to
obtain such cycle-accurate measurement when B1 is too large
because of the overflow in built-in clock cycle counter. How-
ever, we have verified that for a fixed coprocessor, the modular-
multiplication throughput roughly remains the same for dif-
ferent B1’s and can be used to accurately predict the scalar-
multiplication throughput when multiplied by the number of
modular multiplications executed in each scalar multiplica-
tion. Therefore, in Table 1 we only report the result obtained
with B1 = 8192, the largest B1 for which we have obtained
cycle-accurate measurements in all experiments—except for
the modular-multiplication throughput of 8800 GTS (both G80
and G92), where the measurement is done using the clock cy-
cle counter on the CPU, and therefore it also includes the over-
head of setting up the computation and returning the computed
result. For elliptic-curve scalar multiplications, this overhead
is negligible. But for modular multiplications, this overhead
can be significant, so we use B1 = 1048576 when calculating
the modular-multiplication throughput.
In Table 1, the first column lists the coprocessors. The
next three columns list their specifications: number of cores,
clock frequency, and theoretical maximal arithmetic through-
put. Note that this throughput figure tends to underestimate
CPU’s computational power while overestimating GPU’s be-
cause CPUs have wider data paths while GPUs suffer from
lower degree of instruction-level parallelism. The next two
columns give the actual performance numbers derived from
experiment measurement.
We note that the first row in Table 1 does not correspond to
any experiments we have performed. It is extrapolated based
on the result of Szerwinski and Gu¨neysu, published in CHES
2008 [4]. In their result, the scalar in the scalar multiplications
is 224 bits long, whereas in our experiments, it is 11797 bits
long. Therefore, we have scaled their result by 11797/224 to
fit into our context. We also note that their modulus is a special
prime, which leads to faster modular reduction and that it has
only 224 bits, as opposed to 280 in our implementations. We
did not account for this difference in the performance figure
stated. In spite of that, our implementation on the same plat-
form achieves a significantly higher throughput of more than
twice the number of curves per second.
Here we give a back-of-envelope analysis on the perfor-
mance achieved by an NVIDIA GeForce GTX 280 graphics
3
