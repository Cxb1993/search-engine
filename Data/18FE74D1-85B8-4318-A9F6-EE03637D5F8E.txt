1 
 
(一) 計畫中文摘要 
本研究所提架構的確能為多媒體服務之保護機制帶來新希望。本計畫探討多媒體辯證
(multimedia forensics)中的兩大問題：影像版權追溯與影像認證(image authentication)。
我們以影像雜湊技術(image hashing)解決影像認證問題，並且將其應用於保密型基於內容的
影像擷取(private content-based image retrieval)。 
本研究探討的主題為影像認證，且以影像雜湊技術實施。以往影像認證利用嵌入浮水印
以檢測影像是否被惡意修改，此法容易被使用者經由比對而得以偽造新影像卻仍保有認證記
號，因此，引入與內容相關的訊息，可以讓比對的成功率大為降低而增加認證的可信度，雜
湊技術便可提供一個有效將影像內容與認證結合的方式。然而，傳統雜湊技術並不全然適合
於影像雜湊，必須可以抵抗使用者合理的修改且仍能讓惡意竄改的影像認證無效。本研究考
慮旋轉、縮放、平移、裁切等多項破壞，提出能抵抗劇烈但合法破壞的特徵萃取演算法，並
結合多項安全機制，使得雜湊機制得以有效運用於影像認證。 
此外，本研究亦將影像雜湊技術應用於保密型基於內容的影像擷取，在此新興應用上，
必須能更放寬對影像雜湊脆弱性(fragility)的要求，使得相近的影像都能被找到，同時又能
保持其資料隱密性(privacy)。本研究考量影像雜湊在影像擷取上的特殊要求，讓影響不同要
求(強韌性或脆弱性)的特徵比對結果根據權重調整並整合，尋找出更為合理的搜尋結果。 
本計畫所提研究架構的確能為目前串流視訊之數位權利保護研究與雜湊技術應用於影像
認證與影像擷取之研究帶來創新的思考方向，並且初步的研究成果與理論探討顯示這個方向
的確能為此二影像辯證重要議題帶來新希望。 
(二) 計畫英文摘要 
This proposal addresses two major problems in multimedia forensics: forged image tracing and 
image authentication. For the first problem, we focus on the designing of a traceable fingerprinting 
scheme; and for the second one, we develop a robust and secure image hashing mechanism for 
image authentication, and also apply it to the private content-based image retrieval.  
All the three main trends of fingeprinting codes (the orthogonal noise-like signals, 
combinatorial designs, and ECCs) have trade-offs among the size of customer base, the collusion 
resilience, and codeword length. We propose a multi-level fingerprinting scheme based on the user 
grouping, which disperse the decoding efforts over multiple code levels by the grouping structure. 
Both theoretical analyses and the practical implementation show good performances of error rates 
and collusion-resiliency of the proposed methods. 
This proposal brings up new methodologies for multimedia forensics, including digital rights 
protection of video streaming, a high-speed video encryption algorithm, a bandwidth-saving 
fingerprint embedding approach, a scalable fingerprinting scheme with high collusion resiliency 
and the short code length, a robust and secure hashing scheme which is RST-invariant and could be 
applied to both image authentication and private content-based image retrieval. Current 
experimental results encourage us to continue the research and carry out the proposed systems.  
3 
 
 
圖二 
然而，使用者在搜尋多媒體資料時，必須從資料庫中的多媒體做 query，然而此一動
作會洩漏多媒體內容中的秘密，在現今多媒體內容盛行的資訊時代，如何保護多媒體內容
的私密訊息而仍能做 data retrieval，亦為一值得研究的新興課題。 
(四) 研究目的 
I. 針對影像資料設計對於雜湊效能好的特徵萃取演算法 
發展一個合適的影像特徵萃取演算法，可以考量影像資料允許些許修改的特質，同時
也必須保持傳統雜湊 collision‐free的特性，此外，對於影像幅度較大的修改，如縮放、旋
轉、裁切等，也希望具備一定的抵抗能力。 
II. 研發壓縮率高的特徵編碼方式 
包含特徵的壓縮與量化，使編碼過後的雜湊值長度小且仍然具備 robustness 與
collision‐free的特質。 
III. 探討安全機制如何使用於特徵萃取與特徵編碼過程 
欲將雜湊應用於影像認證與 private image retrieval上，security是一項重要的要求，因
此必須研究如何將 security 機制從特徵萃取到編碼過程中適切地加入。 
IV. 研究如何以影像雜湊技術達成影像認證 
綜合以上工作，以影像雜湊技術設計一套完整的影像認證機制，使之能認證合理修改
的版本，而拒絕竄改內容的版本。 
V. 研究如何以影像雜湊技術達成 private content‐based image retrieval 
除了影像認證之外，同時以所發展的技術，探討如何達成影像搜尋與 indexing，以利
Image  Hash value 
1000111111 
101100001 
010101000 
000000111 
010101100 
Best match 
Query Image 
5 
 
matrix，以 singular value decomposition (SVD)取影像之 low‐rank matrix approximation
作為特徵，此法對於幾何破壞具有好的抵抗能力，但必須付出碰撞機率高的代價；
[MONG07]進一步利用 non‐negative matrix factorization，大量減少碰撞的問題。 
D. 低階影像表示(low‐level image representation) 
[DITT99]取出影像中的邊緣(edge)特徵作為雜湊依據，但對於縮放(scaling)、降
低解析度、高度量化(high  quantization)等破壞非常敏感。[MONG06]利用可保留幾
何特性的特徵點(feature point)做雜湊，將 corner或 high curvature的點取出，此法
對於非惡意的影像處理具有好的抵抗力，且能成功偵測出惡意破壞。 
II. 安全機制 
用於多媒體認證或者私密保留之 image retrieval 的應用時，影像雜湊演算法除了影像
特徵的萃取之外，尚須引入安全機制，否則使用者很容易由比對影像資料與 hash value 而
偽造不同於原影像的偽造版本，卻仍擁有相同的特徵值，此外，如前所述，一般使用者的
影像處理將對影像造成些許破壞，因此安全機制也必須能容忍這些破壞，因此所使用的安
全機制必須能兼顧 security 與 robustness。  [BHAT98]用 public‐key encryption 的方式，但加
密演算法對於所萃取出來的特徵之些微差別相當敏感，許多研究[MONG06][MALK04]以亂
數(randomness)方式達成安全性的要求：[MONG06]根據特徵的機率分佈上做quantization，
利用特徵向量正規化(normalization)過後的 histogram估測機率分佈，增加一些 randomness，
此外  ，將影像切割成許多互相重疊的區塊分別萃取特徵值，並根據 key 值決定取哪些區
塊，使 randomness更為增加；[MALK04]將 randomness放在 hash function 的選取上，利用
Gaussian 與 Curvelet  wavelet 作為 basis  function，且用 Randlet  Transformation 引入
randomness，使攻擊者難以觀察 hash值。 
III. 效能評估 
影像雜湊技術的幾項要求：robustness、fragility、randomness、localization與 hash值
的差距等等可作為評估影像雜湊技術優劣的依據， [SWAM06]提出一種 normalized 
Hamming distance 作為估測 hash良劣的依據，亦即，量測同一影像經過 content‐preserving
處理過後的版本其 hash值與為處理過的值之差距，其定義為 
        1 2 1 2
1
1( , ) | ( ) ( ) |
L
k
d h h h k h k
L 
  ， 
其中 h1與 h2分別為兩個 hash  values。若為兩影像為同一張，則希望接近 0，若為不同影
像，則希望接近 0.5。以圖十一的三張影像(分別為 A、B與 C 影像)為例，所得到的結果如
表一。 
7 
 
[ALGH04] (statistics 
based) 
B  高  中  中  有 
[LIN01] (relation 
based) 
A  低  低  低  有 
[FRID00] (Coarse 
Representation) 
C  中  低  中  無 
[SWAM06] (Coarse 
Representation) 
C  中  中  中  無 
[DITT99] (Low‐level 
feature) 
A  中  中  中  有 
[MONG06] 
(Low‐level feature) 
B  高  中  中  無 
Security mechanism 
  A:  取出特徵後直接轉成 hash value 而沒有加入 randomization 
  B:  將取出的特徵用 secret key做 randomization 
  C:  在原影像上先做 randomization，再對應到 feature值 
  D:  亂數產生一些函數依此去對應影像的特徵 
由上可知，要設計一種符合各項要求的雜湊技術，實屬不易，因為各項需求彼此之間
有 trade‐offs，以往的研究也無法一次兼顧所有要求，因此如何找尋一個好的雜湊機制，
使其能同時滿足各項要求，為一具研究價值之課題。   
影像雜湊技術除了可應用於影像認證之外，也可以用在 content‐based image retrieval 
上，  如此可藉由壓縮影像特徵與雜湊機制以縮短比對時間。在此問題上，如何將低階的
影像特徵與高階的使用者搜尋條件中間做轉換，為一個很有挑戰的議題。Image  retrieval
大致可分為兩種研究方向：(a) the weighting approach,  與(b) the probabilistic approach。前
者如[RUI99]不斷利用使用者給的回饋重新估測相似度，系統得以逐漸調整每種特徵的重要
性；後者如[COX00]則藉著估測相關影像的機率分佈，將 query往相近的影像移動。[HUAN06]
將雜湊技術加入，提出一種 shape‐based image retrieval model，先抓取影像中物件的輪廓
特徵，再利用 fuzzy hash table 與關連回饋(relevance feedback)，找出適當的匹配，圖六為
搜尋結果其中一例。 
9 
 
In order to guarantee the invariance of salient feature points and simplify the SIFT computation, we embed 
the watermark only in the area in which users are interested. This is because even though users may remove 
or modify  the  image,  they will never destroy  the  significant parts. Therefore, a  region of  interest  (ROI)  is 
extracted for embedding in order to prevent users from removing or modifying the embedded watermark.   
A human visual system has been introduced in many watermarking‐related researches. In [28] and [29], an 
HVS was used to improve the quality of a watermarked image. In [30], an HVS was used to skip bits without 
influencing  the  visual  perceptibility  of  video  encoding  applications.  We  apply  the  user‐attentive  model 
proposed  in  [31]  to build an evaluation  function  that  is used as  the criterion  for deciding  the ROI.  In  this 
user‐attentive model, regions with mid‐gray  levels will have a high score for selection because regions with 
very high or low gray levels are less noticeable to human beings. In addition, the strongly textured segments 
will have  low  scores. The distances  to  the  image  center are also  considered because human beings often 
focus on the area near the center of an image. Therefore, we evaluate the user‐attentive levels of areas using 
the values of CR, LR, and TR, which estimate the closeness to the center points, the moderate of gray  levels, 
and the roughness of region R, respectively. The following steps are followed when deciding the embedding 
region. 
1. Decide the region sizeεand the checking interval s. 
2. For each checking area R withε  and s, compute   
R R RE C L T     ,                                                             
            where                                                                                                                                                                             
2 2( ) ( )
( , )
( ) ( )
1
c cx x y y
w h
x y R
RC R
 


 

,
2,
( , )
128( )128
1
m
i j
i j R
R
I
L
R

    

, and   
2 2
, 0,0
2
( , ) 0,0
( ) ( )
(  )
( )
k l
R
k l R
C C
T
C
 
,
   
in which  Ii,j  is  the  gray  level of  the pixel  (i,  j), Ci,j  is  the DCT  coefficient of  the positions  (k,  l)  after 
applying  the DCT  to  the  region  block  R, m  is  an  empirically  determined  constant  that  adjusts  the 
degree  of  the  gray  level’s  influence  (a  large  m  increases  the  distance  among  values  of  the  base 
number), and |R| denotes the number of pixels  in the region R.   ,   , and     are also empirically 
decided constants and result in different weights for CR, LR, and TR. In our experiments,     is set to 0.6, 
   to 0.2, and     to 0.2. 
3. Select the area Ra with the largest value E as the embedding region; this region is the most attractive 
area (the ROI), as shown in Fig. 2.   
 
Scale‐Invariant Feature Transform 
1. Scale‐space extrema detection   
The image I is first preprocessed by an unsharp filter in order to stabilize the SIFT features: 
( , ) ( , ) ( , )
( , ) ( , ) ( , )
sharp
smooth
I x y I x y k g x y
g x y I x y I x y
  
  ,                                                                                                                   
11 
 
In order to enhance the robustness of SIFTW features, we remove the feature points that are not sufficiently 
robust to RST attacks:   
1. Apply  RST  operations with  different  parameters  to  I  in  order  to  derive  different  destroyed  images: 
{ | 1,..., }
RST
i
RSTI I i k  , where k is the number of RST operations. 
2. Extract feature points from each image 
RST
iI . 
3. Count the frequency of each feature point extracted from 
RST
I
.
 
4. Select  feature  points  with  a  high  frequency  for  watermark  embedding.  The  remaining  points  are 
supposed to be resilient to RST attacks. 
In addition  to RST attacks, we  consider  the  influences of  common  signal processing  tasks  such as noise 
addition and compression. Similar steps are used for obtaining destroyed images  { | 1,..., }
G
j
GI I j m    (j 
is  the  number  of  Gaussian  operations)  with  Gaussian  attacks.  The  salient  feature  points 
},...,1|{ liSPSP i    (l  is the number of salient feature points) are then produced by selecting the points 
with frequency F > TF (TF is the threshold) for both attacks. One example of the feature points is illustrated in 
Fig. 3. 
Keypoint-Based Watermarking 
Since  the  spatial domain of  images has  limited  features,  it  is difficult  to  find  robust  spatial  features  for 
hiding messages, particularly for resisting geometric transformations or other attacks. Hence, spatial‐domain 
image watermarking is still a challenging problem. However, in this section, we devise a robust approach for 
quantization watermarking in the spatial domain; this approach has a high resistance to RST and many other 
attacks. 
 
4.1.1. Watermark Embedding   
The ROI is first extracted. Then, the keypoints are detected in this area, and the corresponding features are 
derived    in order to obtain the principal axis of the image. After aligning the  image with the principal axis, 
the watermark sequence is embedded according to the aligned direction.   
The principal axis is computed as follows:   
1. Compute the gradients of salient points. 
2. Use the gradients of salient points to obtain the orientation histograms. 
3. Derive the main orientation for each 8  ×  8 block from the local orientation histograms.   
4. Obtain the principal axis (PA) of this image by averaging the gradient directions of all blocks.   
1
( ) /
l
i
i
PA A l

  ,                                                                                                                                                 
where  iA   is the gradient direction of  iSP . Then, a circle that centers on iSP   is drawn, and the radius is 
  m in ( , ) /e i jd d C C  ,                                                                                                                           
13 
 
watermarking (DCTW) schemes. The imperceptibility is good because we do not embed an excessive amount 
of information in the image; however, recently proposed RST‐invariant watermarking methodologies still 
have low capacities (see Table 2).     
Table 1. PSNR (dB) of watermarked images. 
      
Length 
Method 
8 bits 16 bits 32 bits 64 bits 128 bits 
SW   46.64 46.63 46.62 46.61 46.58 
DCTW 50.52 50.49 50.49 50.48 50.45 
Table 2. Watermarking capability of existing approaches. 
 
 
 
 
 
Watermarking Robustness 
Various attacks including rotation, scaling, translation, cropping, and warping are applied to the 
watermarked image using Checkmark Benchmark 1.2 [33] to test the watermarking robustness. Some 
examples of these attacks are illustrated in Figs. 15–18, and 20.   
  There are two types of watermark detection schemes: one involves detection of the existence of the 
watermark pattern, for which the output is yes/no. The other involves the extraction of all the watermark 
bits. The proposed method belongs to the second type. Since few existing watermarking schemes have a high 
extraction correctness, we select the method proposed in [28] for comparing the extraction correctness; this 
method is a newer keypoint‐based method and is superior to the methods reported in many other 
researches. However, in [28], larger test images (512  ×  512) and rectangular watermarks (32  ×  32) were 
used. For comparison, we use the method proposed in [34], which is also a keypoint‐based watermarking 
method that has high robustness but uses larger (512  ×  512) test images. (More information can then be 
embedded using this method than using our proposed method, so that the robustness can be higher if the 
same methods are applied). Moreover, this scheme does not extract the exact watermark bits but only 
detects the existence of the watermark pattern. Therefore, the comparisons are not very fair but provide 
considerable information for analyzing the proposed watermarking methodology. Since [5] is a milestone 
Method Watermarking capacity  PSNR (dB) 
Proposed method  128
256 256   bits/pixel
2  48.52 
[5], 2001  74
512 512 numbers/pixel
2 40 
[26], 2009  8
512 512   patterns/pixel
2  42 
[28], 2006  32 32
512 512

   bits/pixel
2  51.97 
[34], 2008    25
512 512 bits/pixel
2  44.58 
15 
 
   T(1, 1)  T(5, 1) T(17, 5) T(1, 5)  T(5, 17) T(25, 20)  T(50, 50)
SW‐Lena    0.903  0.903 0.835  0.938  0.822  0.781    0.792 
SW‐Airplane  0.878  0.871 0.770  0.861  0.776  0.759  0.799 
SW‐Barbara  0.891  0.877 0.790  0.891  0.795  0.771  0.803 
DCTW‐Lena    0.742  0.695  0.641  0.742  0.758  0.734    0.688 
DCTW‐Airplane  0.753  0.713 0.691  0.721  0.721  0.713  0.691 
DCTW‐Barbara  0.775  0.742 0.687  0.753  0.753  0.719  0.723 
[5]          0.6–0.8 0.6–0.8   
[28]‐Lena  0.705  0.674 0.555  0.666  0.558  0.53  0.561 
 
It can be observed that the proposed methods show higher performance than many existing works with 
smaller test images, more reasonable watermark capacities, and better fidelity. Although [34] shows 
considerably better performance in terms of RST resilience, our method has a considerably higher watermark 
capability and a lower time complexity. Instead of using several well‐known test images in most researches, 
[5] used a lot test images. Besides, a lot of correlation coefficients have to be computed and drawn from the 
distribution of detection values, so the probability of false positive is high. If larger images are used and only 
the watermark existence (yes/no) has to be reported, our proposed method can provide better experiment 
results. 
  Table 6 lists the results after cropping; C(cx, cy) is used for cropping the image with the cropping ratio cx 
and cy in the x and y directions, respectively. We can find that even if some part of the image is sheared, most 
of the watermark bits can still be extracted correctly. 
Table 6. Similarities after cropping attacks. 
 
 
 
 
 
 
In 
addition to rotation, scaling, translation, and cropping, the proposed algorithms are resilient to warping, 
which can lead to considerable destruction of the images. The experimental results for SW and DCTW are 
shown in Fig. 19, in which W(f) is used for warping the image with a factor f. The experimental results also 
reveal that the proposed methods can resist warping to a considerable extent, even though the perceptual 
  C(5%, 
5%) 
C(10%, 
10%) 
C(15%, 
15%) 
C(20%, 
20%) 
C(25%, 
25%) 
C(30%, 
30%) 
SW‐Lena    0.875    0.875  0.844  0.844  0.781    0.750 
SW‐Airplane  0.813  0.853  0.817  0.801  0.738  0.712 
SW‐Barbara  0.891  0.891  0.835  0.826  0.791  0.763 
DCT‐Lena    0.883  0.883  0.851  0.844  0.797  0.781 
DCT‐Airplane  0.823  0.823  0.817  0.811  0.758  0.733 
DCT‐Barbara  0.881  0.879  0.847  0.806  0.771  0.753 
[34]‐Lena            1 
[34]‐Airplane            0.96 
[34]‐Barbara            1 
17 
 
Schemes and Frameproof  Codes”, SIAM J. Discr. Math., Vol. 11, pp.41-53, 1998. 
TRAP03   W.  Trappe,  M.  Wu,  J.  Wang  and  K.  J.  R.  Liu,  “Anti‐Collusion  Fingerprinting  for 
Multimedia”, IEEE Trans. S.P., Vol. 51, pp.1069‐1087, 2003. 
WANG03   Z.  J. Wang, M. Wu, H.  Zhao,  and  K.  J.  R.  Liu,  “Resistance  of Orthogonal Gaussian 
Fingerprints to Collusion Attacks”, pp.IV‐724‐IV‐727, ICASSP 2003. 
WANG04   Z.  J. Wang, M. Wu, W. Trappe, and K.  J. R. Liu, “Group‐Oriented Fingerprinting  for 
Multimedia Forensics”, EURASIP, pp. 2153‐2173, 2004. 
ZHAO04   H.  Zhao  and  K.  J.  R.  Liu,  “Bandwidth  efficient  fingerprint  multicast  for  video 
streaming,” IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 2004. 
Yu-Tzu Lin1 Bai-Jan Yen2 Chia-Hu Chang3 Huei-Fang Yang4 Greg C. Lee2  
1Inst. of Curriculum Instruction & Technology, National Chi Nan University 2Dept. of Computer Science & Information Engineering, 
National Taiwan Normal University 3Graduate Inst. of Networking & Multimedia, National Taiwan University 4Dept. of Computer Science 
& Engineering, Texas A&M University  
Dec. 14, 2009  
 
Outline  
Introduction Lecture video indexing Teaching focus mining Experimental results Conclusion  
 
Outline  
Introduction  
Lecture video indexing  
Teaching focus mining  
Experimental results  
Conclusion  
 
Introduction  
Organizing lecture videos  
�  
is helpful for learners to retrieve lecture information.  
�  
needs technologies for slide structure reconstruction and teaching content understanding.  
Experimental results  
Conclusion  
 
Preprocessing  
Lights aﬀect the color and contrast of the lecture video. Histogram equalization is to enhance the image contrast. Niblack 
binarization method is to diﬀerentiate slide content from the background.  
 
(a) turn on light (b) turn oﬀ light (c) turn on light (d) turn oﬀ light Figure 1: Preprocessing results under diﬀerent light 
conditions.  
 
Occlusion object detection  
Two possible abrupt changes in lecture videos: slide changes: true slide shot change. object occlusions: not true slide 
shot change.  
 
(a) slide change (b) object occlusion Figure 2: Abrupt changes in the slide video.  
 
Occlusion object detection (cont.)  
 
β = Fi (x, y) ∨ Fj (x, y) , 
(i,j)∈Fi,j 
x  
 
P = Fi (x, y) ⊕ Fj (x, y) . 
(i,j)∈Fi,j (i) penalty mask  
 
Outline  
Introduction Lecture video indexing  
Teaching focus mining  
Experimental results Conclusion  
 
Gesture analysis  
Finding the teaching focus is equivalent to analyzing the instructor’s gesture.抄 to find the possible finger area.  
 
The frame image.  
 Computing the Euclidean distances between the human center and regions.  
Finding the maximum distance.  
The teaching focus position is:  
Pfocus = argmax Dist 
C
human
, A
i 
Ai  
 
Gesture analysis example  
 
(j)  
instructor on the left side (k) teaching focus positions  
(l)  
instructor on the right side (m) teaching focus positions  
 Outline  
Introduction Lecture video indexing Teaching focus mining  
Experimental results  
Conclusion  
 
Experiment I: shot change detection  
10 videos were recorded under diﬀerent light conditions. Each video is about 1 − 2 mins long.  
  Lamplight  Sunlight Sudden change 
Front  Middle  Back  Open curtain Light on Curtain open 
I-a  ×  O  ×  × × O 
I-b  ×  O  ×  × × × 
I-c  ×  ×  ×  × × × 
I-d  ×  ×  ×  O × × 
I-e  O  O  O  O × × 
I-f  O  O  O  × × × 
I-g  O  ×  ×  O × × 
I-h  ×  ×  O  O × × 
I-i  ×  O  ×  × O × 
I-j  ×  O  ×  × O O 
Table 1: Test videos with diﬀerent light conditions.  
Experimental results 19 / 28  
Experiment II-a: shot change detection with occlusions  
Video no.  Video length  # of slides  Precision Recall 
II-a  19'06” 15  81% 86% 
II-b  48'55” 38  93% 84% 
II-c  48'32” 6  94% 92% 
II-d  38'07” 31  84% 94% 
II-e  30'29” 39  75% 100% 
II-f  38'07” 31  92% 83% 
II-g  30'29” 39  100% 97% 
II-h  17'32” 19  90% 94% 
II-i  41'58” 32  87% 90% 
II-j  53'13” 98  90% 90% 
Avg.    98% 89% 
Table 3: Test results for videos with occlusions.  
Even if occlusions appear, the proposed method still has high precision.  
 
Experiment II-b: shot change detection without occlusions  
5 videos without any occlusions were chosen for testing.  
Video no.  Video length  # of slides  Precision Recall 
III-a  23 ' 20” 17  100% 94% 
III-b  35 ' 40” 22  100% 100% 
III-c  38 ' 06” 27  100% 100% 
III-d  19 ' 33” 26  100% 92% 
III-e  47 ' 19” 29  100% 100% 
Avg.    100% 97% 
Table 4: Test results for videos without occlusions.  
The precision rate is 100%. The proposed method would fail when the shot changes happen at the slides with 
similar layouts.  
Outline  
Introduction Lecture video indexing Teaching focus mining Experimental results  
Conclusion  
 
Conclusion  
Our indexing and teaching focus mining system  
provides students an eﬃcient way to access the lecture content.  
accurately rebuilds the slide structure by introducing a new edge-based shot  
boundary detection algorithm.  
successfully resists the unwanted influences induced from the variant illumination conditions and occlusions. 
extracts teaching focus that is useful for learning.  
 
References  
S. G. Deshpande and J.-N. Hwang. A real-time interactive virtual classroom multimedia distance learning system. IEEE Trans. Multimedia, 3(4):432–444, 2001.  
C. Fredembach, M. Schr¨oder, and S. S¨usstrunk. Eigenregions for image classification. IEEE Trans. Pattern Anal. Mach. Intell., 26(12):1645–1649, 2004.  
J. Ha, R. M. Haralick, and I. T. Phillips. Recursive x-y cut using bounding boxes of connected components. In ICDAR ’95: Proceedings of the Third International 
Conference on Document Analysis and Recognition (Volume 2), 1995.  
O. Ikeda. Estimation of speaking speed for faster face detection in video-footage. In ICME, pages 442–445, 2005.  
R. A. Kirsch. Computer determination of the constituent structure of biological images. In Computers in Biomedical Research, volume 4, pages 315–328, 1971.  
Y.-T. Lin, B.-J. Yen, and G. C. Lee. Structuring and analyzing low-quality lecture videos. In ICASSP ’09: Proceedings of the 2009 IEEE International Conference on 
Acou 
 
2. 現場提問: 
國科會補助計畫衍生研發成果推廣資料表
日期:2010/12/18
國科會補助計畫
計畫名稱: 多媒體辯證研究:1.串流視訊指紋碼嵌入與廣播加密機制 2.應用於影像認證與
影像擷取之雜湊技術
計畫主持人: 林育慈
計畫編號: 98-2221-E-260-039- 學門領域: 影像處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
