行政院國家科學委員會專題研究計畫完整報告 
 
建構新的高效能資料分群技術以支援電子商務及影像辨識實務應
用 
Development of New Effective Data Clustering Technique to 
Support E-Commerce and Image Recognition 
計畫編號：NSC 95－2221－E－020－036－ 
執行期限：95年 08月 01日至 96年 10月 31日 
主持人：蔡正發   國立屏東科技大學資訊管理系 
計畫參與人員：林宗聖、嚴家成、陳政緯、李俊璋、陳冠宇、趙士元            
國立屏東科技大學資訊管理系 
 
一、中文摘要 
本研究計畫主要係改善資料分群演算
法的效能及效率並將其應用於實務。藉由 
K-Means執行分群快速的效率優點，來產
生本研究之初始形心點。但是，K-Means
常會陷入區域最佳解而降低分群的效能，
所以本研究即加入最小平方差(Minimum 
of Square-Error)的觀念，以避免求解能力
被區域搜尋所局限，作為本研究第一階段
求取初始形心點的依據。本研究計畫提出
proportional distance距離公式，對於處在
分群形心點週邊的核心區域的物件之距離
予以減少權重計算距離，而核心區域外則
予以增加權重計算距離，以有效的改善各
資料維度的權重關係。依此觀念及原則，
本研究計畫提出 Weighted K-Means 
Algorithm (WKM)，考量各子群中各個資
料維度之表現強度差異，藉以計算物件相
似度高低。經由筆者實際模擬結果，本計
畫所提之方法在分群效率上均較 K-means 
algorithm、FSOM+K-means approach、GKA, 
Max One +K-means (GMA)、 KGA、及筆
者近兩年所提出且已發表之數個新的資料
分群方法如 ACODF、ICA、 MSGKA、
CCA、ECA、TKM 等方法為佳。在執行
速度上亦與K-Means非常接近並且優於以
上所提及之所有方法。此法不僅可以修正
k-means 較適合處理圓形分布之資料集之
缺點，並且適合處理大量的資料集，在時
間成本與 k-means 幾乎相同或相近，幾乎
感受不到時間成本之差異性，因為兩者執
行資料分群之速度均極為快速。 
本研究計劃亦已發展數個資料分群的
應用系統，並測試所提出之演算法在真實
資料集之實際應用情形。例如，我們測試
此演算法在數萬筆資料的多個維度中對於
消費者在使用信用卡之行為分析，以幫助
信用卡公司據此進行決策分析。此外，我
們亦開發以下數個資料分群的應用系統：
1. 智慧型多重物流配送資訊系統之設計
與實作，2.分群演算法應用於車牌辨識系
統等。 
 
關鍵詞：資料挖掘,資料分群,資料庫, 啟發  
式方法 
英文摘要 
In this project, we propose a new 
algorithm called Weighted K-Means 
(WKM) for data clustering. The k-means 
algorithm is a well-known and powerful 
algorithm and it may implement 
 2
分群以增強企業效率。由於資料分群可應
用於趨勢分析、顧客剖析、相似性搜尋、
分類、影像處理、圖型辨識及多重物流配
送等應用[1]-[9]，所以此一研究領域也隨
之成為熱門且實際可應用於企業決策與工
業技術上之重點科技。   
三、文獻探討 
目前已有許多頗具效率且實用性高之
智慧型啟發式方法被發展出來。其中較著
名的 K-means、自我組織圖(Self Organizing 
Map; SOM) 及 基 因 演 算 法 (Genetic 
Algorithm; GA) 已被成功應用於資料分群
問題上。SOM是最廣為人知的一個方法，
但不幸的是 SOM 計算所需花費的時間非
常的長，因此有許多學者們遂對於如何減
少 SOM 的計算時間極感興趣，其中的研
究包含將 SOM 結合 Fuzzy、K-means、
Genetic Algorithms等或是在 SOM的計算
過程中加入一些技巧。此外，近來有三個
極為有效率之方法稱為 FSOM+K-means、
KGA及 GKA被提出。此三方法在資料分
群問題上之效率均較前面三者為佳。 
最近二年筆者亦提出數個已在 IEEE
國際會議或國際期刊發表之新的資料分群
方法，例如 ICA (Intuitional Clustering 
Algorithm) 、 ACODF (Ant Colony 
Optimization with Different Favor) 、
MSGKA (Multiple Searching Genetic 
K-Means Algorithm) 、 CCA (Cobweb 
Clustering Algorithm) 、 ECA (Elite 
Clustering Algorithm) 及 TKM (Twice 
k-means)等。經由理論分析與實際的應用
結果，這些方法在許多資料分群問題的錯
誤率上均較前述方法為低。其中尤以 ICA
在資料分群問題的錯誤率上及時間成本上
成效最佳。 
四. 研究方法 
本研究計畫提出之資料分群演算法，
叫做Weighted K-Means Algorithm，簡稱為
WKM。『群』乃是群體當中所有資料物件
的一個集合。『群』乃是多個物件集結而成
的聚合體，而當由不同方面解讀集結而成
的『群』時，亦將呈現不同的表徵，正是
「橫看成峰側成嶺、高低遠近各不同」，
K-Means 就是藉由不斷的重複計算各群
所屬物件、調整形心點, 達到最適分群的
目的。由此可知『群』乃是由許多不同物
件所集結而成的一個整體。然而 K-Means
在利用歐式距離(Euclidean Distance)計算
後，因為該公式係為各資料維度的最後合
併值，故分群結果大致上是呈現圓形狀
態，並且可能因為某一維度的資料值為極
大，產生了對其他資料維度的支配性，造
成了分群的不恰當。因此在計算距離時，
應該考量『群』的形狀，考量『群』中所
有物件的分佈情形，不可忽略了群體的形
狀對物件的影響，因此計算物件與各形心
點之距離時，各維度的資料值不應該只是
單純的視為相同比重，而應該依物件與各
分群間之相關性，分別考量物件中各資料
維度在各分群的表現強度，再則利用
conceptual clustering的概念，考量物件之
週邊及鄰近點對分群的影響，將物件分配
到概念上較為接近的群，是一種更直接而
且更有效的作法。例如圖 1 之二維空間
中，Cluster A群呈現圓形而 Cluster B群則
呈現橢圓形，直覺上物件 D在分群時應指
派到與其形狀較為一致的 Cluster B群，但
是若經由 K-Means分群方法，因採用歐式
距離(Euclidean Distance)計算，將造成物件
D歸屬到 Cluster A群的錯誤情形。 
 
 4
K-Means資料分群演算法分別執行五次，
並 以 五 次 執 行 結 果 之 最 小 平 方 差
(minimum of SE)為最後之K-Means資料分
群演算法整體最佳解。經過實際實驗測試
結果顯示，經過五次的 K-Means資料分群
演算法執行後，達到整體最佳解的機率接
近百分之百。根據以上的分析，本研究提
出分群演算法如下： 
Clustering Algorithm(Data，K)  
 Input：Data {Xi ，i = 1,…,N}，K 
 Output：K clusters 
Step1：執行 K-Means 資料分群演算法五
次分別執行 K-Means 資料分群演算法五
次，並記錄個別執行 K-Means資料分群演
算法的平方差、各群組形心點。平方差
Square-Error (SE)之計算如公式 2所示： 
(SE)=                (2) ∑ ∑
= ∈
−
K
j CX
ji
ji
ZX
1
2)(
其中， X i  { i = 1,…, N }為所有資料點，
{j =1,…, K}為群組，K為群組數，ｚC j j屬
於該Cj群之形心點。 
Step2：求最小平方差(minimum of SE)之形
心點根據Step1求取最小平方差(minimum 
of SE)之最佳分群組合，讓資料分群之各
群組形心點以最小平方差之形心點{ Z1，
Z2，…，ZK}取代之，做為本研究之初始形
心點。 
Step3：執行分群動作。從所有點 X i  { i = 
1,…,N }分配到群組 {j =1,…, K}中，其
分群規則如下：
C j
−iX jZ  ≤ −iX pZ  ，  
其中 j = 1，2，…，K ;  p = 1，2，…，K 
且 j ≠ p，當該點Xi與形心點Zj的距離比其
它形心點Zp之距離小於或等於者，則該點
Xi分配到該群Zj中。其中距離公式如下：
ji ZX −  = 
2/12
1
))/)/((( jkjkjkik
d
k
jkik WCSZXZX −×−∑
=
 
CS jk： ∑
∈
−
jj CX
jkjk
j
ZX
N
)(1   其中Nj屬於該
Cj群之資料點總數，W jk：  ∑
=
d
i
jijk CSCS
1
/
Step4：重新計算新的形心點。計算新的形
心點{Z *1，Z *2，…，Z K* }如下：
=Z j* ∑∈ ji CX ij XN
1  ，j = 1，2，…，K 且Nj 屬
於該Cj群之資料點總數。 
Step5：分群終止條件。假如產生的K個新
形心點與舊的形心點相同時，即終止分群
動作。亦即 Z=Z j* j ， j = 1，2，…，K 則
終止分群；否則再從Step3繼續做分群。 
五、結果與討論 
   為了証明我們提出的方法之效率，我們
利用實際的電腦模擬去和 k-means、
GMK、KGA、DBSCAN及 ACODF、ICA、
ECA、CCA、TKM 等方法作比較，我們
採用四組人造資料(Artificial data)及三組
真實資料(Real life data sets)進行實驗，並
利用前述所定義的公式求得各項演算法的
各種效能評估值。而實驗分為二部份，第
一部份描述測試資料的部份，第二部份將
測試資料放置於 k-means、GMK、KGA、
GKA、Fast SOM及 ACODF、CCA、ECA、 
TKM 及本研究計劃所提之 WKM 演算法
中以進行分群處理。  
Data sets 
測試資料可分為人造資料(Artificial 
data)及真實資料(Real life data set)，同時以
人造資料及真實資料進行實驗之模擬能確
保本研究計劃所提之方法WKM確實是可
 6
  
圖 2資料 579點以 K-Means執行結果 
 
圖 5資料 579點以 KGA執行結果 
 
圖 3資料 579點以 GMA執行結果 
 
圖 6 資料 579點以WKM執行結果 
 
圖 4 資料 579點以 GKA執行結果 
表 2 資料 579點各分群演算法群組內評估模式參考結果 
 TWCV CSa CSnn CSc
K-Means 360.56357964 0.96879165 0.07925806 0.67552306 
GMA 360.56357964 0.96879165 0.07925806 0.67552306 
GKA 360.56357964 0.96879165 0.07925806 0.67552306 
KGA 360.56357964 0.96879165 0.07925806 0.67552306 
WKM 364.23128003 0.97187434 0.0772568 0.67779374 
 
 
表 3 資料 579點各分群演算法群組間評估模式參考結果 
 8
 
圖 10 資料 60000點以GKA執行結果 
 
圖 11 資料 60000點以KGA執行結果 
 
圖 12 資料 60000點以WKA (WKM)
執行結果 
 
 
表 5 資料 60000點各分群演算法分群錯誤率及耗費時間成本 
 錯誤點數 錯誤率 耗費時間(單位：秒)
K-Means *** *** *** 
GMA 4589 7.648% 26005 
GKA 5713 9.522% 24266 
KGA 13155 21.925% 9192 
WKM 0 0 53 
由圖 7、圖 8 可以看出在資料 60000
點若以 K-Means執行結果會有二種情形，
這是 K-Means執行資料分群時，有時會陷
入區域最佳解而非整體最佳解的原故。因
此在評估分群演算法之優劣時，我們將
K-Means資料分群演算法之各項評估值均
 10
訊。 
二、路口尋找 
當使用者的運送站點選完畢後，即開
始進行各運送站的路口找尋。在找尋路口
的過程中，首先先將路口資訊組成一個網
路模式，也就是將所有路口的網路模式配
置完成。由於使用者可隨意在任二路口之
間，點選一個以上的運送站，在找尋各運
送站路口時，每找完一運送站之路口時，
須把此一運送站資訊加入此網路模式裡。
因此每當加入一運送站至網路模式後，此
運送站將被視為一個新的路口節點，以便
做為其它運送站的連結。換言之，當找尋
使用者所點的運送站相鄰的路口時，使用
者所點的運送站可能會是介於路口之間，
或是介於路口及其他新的運送站之間。 
輸入
找尋可走的方向
累加此方向的X或Y
值
是否超出邊界
是否遇到路口
輸出
是
否
否
是
 
圖 14 路口尋找方法 
 
首先，讀入路口的相關資訊，並以各
路口座標值，上下左右各加上某值，使路
口擴大成一方形範圍，並使此範圍的四角
碰到建築物。於圖 14 中的流程圖，其每個
流程皆代表一個步驟。 
Step1：於地圖上點選欲送貨的運送站，並
取得各運送站的 XY 座標值。 
Step2：取得各座標值後，依序做處理。以
某一一運送站之座標值開始，四個
方向各加 1，找尋可行進的方向；
若有三個方向，取對稱的兩方向，
所以本研究皆假設每個運送站一
開始只有兩個方向可走。 
Step3：各累加此兩方向 X或 Y值，若碰到
建築物，再重新判斷行進方向，行
進方向不可往回走，也不可往運送
站一開始的另一方向走(前提，假
設每一運送站一開始只有兩方
向)。 
Step4：判斷是否超出地圖邊界。若是，承
接 Step2，運送站此方向沒有路
口，改以另一方向找路口；若否，
則往 Step5 執行。 
Step5：判斷是否行進至路口範圍，若是，
記錄該路口的編號(Step6)；若
否，則返回Step3直到離開判斷式。 
Step6：記錄此運送站的路口資訊，並把此
運送站加入原先的路口網路模式
中。且此流程圖為一完整迴圈，當
使用者點選 8個運送站時，此流程
圖也將被執行 8 次。 
以下將以圖 15 的實例說明： 
 
圖 15 
 12
碼方式為「唯一編碼」法，利用隨機的方
式挑選母體(初始化)，以進行選擇Æ交配
Æ突變等三大循環。本研究將執行 1000
代，以演化出最佳的路徑。 
4.1 選擇、交配、突變 
在本研究中，將所行走，其適應值愈
高在選擇步驟是採用三選一的方式，即指
隨機取出三條，並挑選其中適應值
(Fitness)最低者，即為該路線組合之距離
成本為最低者，以成為交配的父代。 
本方法預設交配的機率為 0.7，當在
進行交配的步驟前，會先隨機產生一個
值，當該值小於所預設的交配機率 0.7
時，即進行交配，反之亦反。本研究的交
配 方 法 是 採 用 是 Partially-Mapped 
Operator (PMX)，假設某一對父代的基因
排 列 分 別 為 0,1,2,3,4,5,6,7 和
1,3,5,7,6,0,4,2，並且隨機產生出，從第
三個基因到第六個基因中的基因須要互
換，其結果就會為 2,1,5,7,6,0,4,3 和
1,7,2,3,4,5,6,0。 
Example： 
p1、p2：父代 
o1、o2：交配後的子代 
︱ ︱ ：將交配(交換)的基因 
p1 = (0 1︱2 3 4 5︱6 7) 
p2 = (1 3︱5 7 6 0︱4 2)  
 
 
相同基因
不同基因
method2
End
method1
 
圖 17  Partially-Mapped Operator(PMX)
交配方法 
input： 首先取出不需做交換的基因。
於 p1 的範例中，則分別為
0,1,6,7，於 p2 中則分別為
1,3,4,2。 
method1：和將進行交配的基因做比較，
當發現有相同基因，則進行
method2，不同，則結束。在
p1 中不需做交換的基因，如
0,1,6,7 分別和在 p2 當中的
5,7,6,0 做比較，而在 p2 中不
需做交換的基因，如 1,3,4,2
分別和在p1當中的2,3,4,5做
比較。 
method2：記錄發生相同基因的位置 j，
並被在本身的第 j 個位置的基
因所取代，再重複 method1。
p1 中的基因 0 被 p1 中第 6 個
位置的基因 5 所取代，且重複
method1。 
End： 當 所 有 input 的 值 經 過
method1 或 method2 後，再將
p1 和 p2 要交換的基因互換
後，即完成了 PMX 交配方法。
 
經過以上的程序後，則可以得到下列
的結果 
o1 = (0-2 1︱5 7 6 0︱6-4 7-3) = 2 
1 5 7 6 0 4 3 
o2 = (1 3-7︱2 3 4 5︱4-6 2-0) = 1 
7 2 3 4 5 6 0 
 14
六、方法一之初步模擬結果 
圖 18為系統一開始執行的畫面，其中
加入了使用者所點選的運送站，以紅色小
點表示，總計有十九個運送站。按下「路
徑」的功能鍵後，會呈現經過方法一的運
算結果，如圖 19，其中於三角形旁的紅色
小點，代表起始點。 
 
                            
圖 18 系統執行畫面 
 16
一、WKM分群方法 
此等方法和方法一不同的地方是在利
用標籤法來找最短路徑之前，利用 WKM 分
群方法將使用者所點的運送站作分群，再
將每一群分別執行標籤法和基因演算法；
由於預設運送車數量為三台，故分成三
群，所以在方法二的流程圖當中，標籤法
和基因演算法都必須執行三次。在WKM
分群方法中，首先先從所有運送站中隨機
抽取三個運送站，當為第一次分群的中心
點，之後進行分群，每一個運送站和最靠
近的中心點分為一群，結束分群後，記錄
每一群當中所有運送站的 x和 y值，分別
累加後除以該群的總數，即可得到該群新
的中心點，再重複分群、中心點-分群的步
驟，直到每一個運送站所歸屬的群組，不
再變動為止，若中心點不再變動，此時只
須再做一次分群即可。 
依這三條最佳路線，並利用三種不同
顏色來交替每一條路線，並在呈現每一條
路線之後，皆讓系統停止 1000 毫秒，再繼
續呈現後續的路線直到系統執行完畢，以
方便使用者瞭解，整個路線的跑動。 
 
二、方法二之初步模擬實驗結果 
圖 21 為方法二的執行結果，和方法一的執
行結果來比較(藉由圖形的比較)，可得知
方法二的結果優於方法一。 
 
圖 21  方法二執行結果 
分群演算法用於即時車牌辨識系統 
近年來學者們紛紛把智慧形系統應用於各
種複雜的問題，例如無線通訊網路
(Wireless Communication Network)、資
料探勘(Data Mining)、多媒體傳輸、旅行
推 銷 員 問 題 (Traveling Salesman 
Problem ; TSP)、資源分配(Resource 
Allocation) 、 排 程 問 題 (Scheduling 
Problem)及辨識系統⋯等。因此這個部份
將以分群方法應用於車牌辨識，一般車牌
辨識系統有以下幾個步驟：  
 18
 圖 24車牌 SOBEL邊緣結果 
 
在圖 24 中，可以發現許多雜訊點，
同時參考三原色的分布曲線可以發現，受
到光源的影響，較暗的區域三原色值普遍
都偏低，較亮的區域卻是極端的高而且數
量很多，所以有必要在 Sobel 的處理動作
中加入亮度補償的程序。 
 
B. 三原色平均考慮 
除了亮度補償的缺失外，對於三原色
的 Color Space不加考慮也是缺點之一；
基於對於車牌定位以及複雜背景的過濾需
要，必須要同時考慮三原色分布，從其中
擷取跟車牌顏色條件接近的區塊進行重點
分析。本節最後以一張普通室內生活照來
說明三原色平均考慮的重要性，並列出該
照三原色的分布曲線： 
從以上的三原色分布曲線圖可以看
出，一般現實環境三原色的訊號是非常尖
銳而不平滑，根據背景和物體本身的顏色
分布，可能有非常懸殊的比重與差異，這
張是在光源穩定的教室內拍攝尚且如此，
可以想見如果是在室外複雜背景以及光源
不穩定的情形下，分布曲線一定是更為尖
銳而極端。所以必須思考有別於灰階值的
顏色值判斷指標，以改善灰階值為基礎的
邊緣偵測演算法的盲點。 
 
3. 新分群演算法之設計 
在完成了邊緣偵測以及目標偵測的
運算過程後，仍然需要一個可以進行字元
切割、樣本比對的演算法來完成整個辨識
系統「辨識」的工作，根據筆者新提之
WKM分群演算法，確實可以進行字元切
割以及任意形狀的分群；但是在即時的環
境下，本研究需要設計一個新的分群演算
法以滿足以下的需求： 
A. 自動判斷分群數 
在即時車牌辨識系統中，無法預期會
有多少目標個數，但是目前大部分的分群
演算法皆需要輸入參數，例如：分群數、
半徑、點數、門檻值⋯等，但是在即時的
環境中，光線、物體形狀、背景空間⋯等
因素都會影響參數的適用性，所以需要設
計一個不需要參數或是可以自動計算最適
合參數的分群演算法。 
B. 適用於任意圖形 
除了無法預期目標個數，還要考慮車
牌本身的扭曲、傾斜、遠近⋯等因素，因
此系統中所採用的分群演算法本身對於任
意形狀圖形，要有良好的辨識率，而且不
能有太多的預設條件跟資料分布限制。 
C. 動態以及即時需求 
由於現實環境的變動迅速，像素點跟
邊緣點的變化也會很明顯，以 500×500 
WebCam 畫面來計算，像素點高達二十五萬
點，即使經過邊緣偵測技術進行測邊，也
會保留約數萬點的邊緣點，在高速變動且
大量的邊緣點資料集合中，要迅速分出正
 20
七、計畫成果自評 
 
本研究計畫就研究內容而言，與原計畫可謂
相符，並已達成預期目標。此研究之成果已
投稿至國際 SCI 著名期刊，預期應有很大的
機會將被接受發表。此外，與本方法相關之
分群技術及相關應用計有 5個，業已通過屏
東科技大學之全額經費補助獎勵，正分別透
過揚正與五洲專利事務所向經濟部專利標
準局申請中華民國與美國發明專利中。 
與本研究計畫相關之論文已發表於如下之
會議與期刊中： 
 
1.Cheng-Fa Tsai , Shun-Bin Jhuang and 
Chen-An Jhuang, “A Novel Edge Detection 
Technique for Real-Time Image Processing 
System,” International Conference on 
Information Science Technology and 
Management (CODE/CISTM 2006), in CD 
paper no: 105, Chandigarh, India, 16-18, 
July. 2006. 國科會經費補助出國發表 
 
2.. Cheng-Fa Tsai, Deng-Chiung Shih and 
Chih-Wei Liu, “FICA: A New Data 
Clustering Technique Based on Partitional 
Approach for Data Mining,” 2007 
International Conference on Machine 
Learning and Cybernetics, pp. 739-744, 
Hong Kung, 19-22, Aug. 2007. 國科會經
費補助出國發表. 
3. Cheng-Fa Tsai, Chih-Wei Liu, 
“KIDBSCAN: A New Efficient Data 
Clustering Algorithm for Data Mining in 
Large Databases,” Lecture Notes in 
Computer Science, vol. 4029, pp. 702-711, 
2006.        
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 22
then the distances between each point connecting to the two 
virtual points V1(min_x, min_y) and V2(Max_x, Max_y) will 
be computed. Take average distance multiply α as initial 
clustering distance and segment these points into new cluster 
sets P{P1, P2, …, Pm}.Notably, α=0.04, which is taken as the 
initial clustering distance as a pre-process step. These cluster 
sets are transformed to form the cluster centers and replace X 
{X1, X2, …, Xn} as input. We take a two-dimensional data set 
as example. Notably, step 1 is depicted as Fig. 2. 
 
Fig. 1.  Idea of initial clustering. 
 
Fig. 2.  Sketch of step 1. 
Step 2: From P {P1, P2, …, Pm}, each point connects its 
nearest point one by one in the forward direction but these 
lines can not be repeated and form loops. Compute the total 
distance of generated lines and retain the result lines which are 
smaller than the average distance of generated lines multiply β. 
Notably, β=1.25, which denotes reserve those lines whose 
distance is smaller than average distance multiply β. Fig. 3 
represents the connected lines of step 2. 
 
Fig. 3.  Sketch of step 2. 
 
Step 3: From P {P1, P2, …, Pm}, each point connects its 
nearest point one by one in the backward direction but these 
lines can not be repeated and form loops. Compute the total 
distance of generated lines and retain the result lines which are 
smaller than the average distance of generated lines multiply β 
as shown in Fig. 4. 
 
Fig. 4. Sketch of step 3. 
 
Step 4: As Fig. 5 shows, from P {P1, P2, …, Pm}, each point 
connects its nearest point one by one in the forward direction 
but these lines can not be repeated. Compute the total distance 
of generated lines and retain the result lines which are smaller 
than the average distance of generated lines multiply β. 
 
Fig. 5.  Sketch of step 4. 
 
 
 
 
 
 24
3 Simulations and Analysis 
Several experiments have been performed to verify the 
correctness and efficiency of the proposed FICA. Experiments 
are set as follows: (1) the benchmark dataset, (2) test the 
correctness of FICA with K-means and DBSCAN, (3) Time 
cost of FICA, K-means and DBSCAN, (4) the robustness of 
input parameters. 
All these algorithms have been implemented in JAVA and 
executed on a computer with Intel Pentium 4 2.8GHz and 
512M RAM. 
 
3.1 Benchmark datasets 
 
  There are three datasets adopted as benchmark datasets. 
DS1-DS3 datasets are shown in Table 1. DS1, from Su [5], 
consists of 3 Gaussian clusters with 579 points. There are four 
circular clusters in DS2 which contains 14799 points and four  
arbitrary clusters in DS3 which exists 14290 points, by Jörg  
Sander [6]. These different kinds of clusters are used to test 
the correctness of FICA. 
 
Table 1: Benchmark datasets 
Benchmark 
dataset 
Points Clusters Source 
DS1 579 3 Su and Chang, 2000
DS2 14799 4 Jörg Sander,1998 
DS3 14290 4 Jörg Sander,1998 
 
 
3.2 Correctness experiments on FICA 
 
  In order to prove the clustering performance of FICA, we 
compare the error rate with K-means [1], DBSCAN [10] and 
the proposed FICA. Fig. 8 demonstrates the clustering results 
of K-means, DBSCAN, and the proposed FICA. K-means has 
error rate of 3.45% and DBSCAN has error of 2.07%. 
However, FICA can correctly generate three clusters. 
  
 
Fig. 8. Comparison of correctness with K-means, DBSCAN 
and our FICA using DS1 dataset: (1) the original DS1 dataset, 
(2) the clustering result by K-means, (3) the clustering results 
by DBSCAN, (4) the clustering results by FICA. 
   
This DS2 dataset comes from GDBSCAN generated by 
edge detection to transform an image into two-dimensional 
data points with four circular clusters [6]. As Fig. 9 revelation, 
it is difficult to tell the exact error rate of clustering results, 
but by taking a look at the clustering results, we can recognize 
that DBSCAN and FICA can show better classification. 
 
Fig. 9.  Comparison of correctness with FICA, K-means and 
DBSCAN with DS2: (1) the original DS2 dataset, (2) the 
clustering result by K-means, (3) the clustering results by 
DBSCAN, (4) the clustering results by FICA. 
   
 26
 Fig. 11.  Flow chart of face feature detection system. 
 
                       (1) 
 
(2) 
Fig. 12. Face feature detection application: (1) transformation 
after edge detection, (2) FICA Clustering result. 
 
5 Conclusion 
This paper proposes an FICA clustering algorithm to solve 
the problem of clustering. In the beginning, FICA generates 
small clusters as a pre-process step to lower the size. Then 
FICA utilizes the idea of nearest neighbor to attain local 
search. Besides, referring to the distribution states of input 
datasets, global search is met. Our simulation results confirm 
that the robustness and speed of FICA is better than K-means 
and DBSCAN. A face recognition application is implemented 
to reveal the robustness of FICA. In the future work, we would 
like to apply FICA on decision making and license plate 
recognition. 
References 
[1] McQueen, J.B., “Some Methods of Classification and 
Analysis of multivariate Observations,” Proceedings of the 
5th Berkeley Symposium on Mathematical Statistics and 
Probability, pp. 281-297, 1967. 
[2] Kaufman, L. and Rousseeuw, P.J., Finding Groups in Data: 
An Introduction to Cluster Analysis, John Wiley & Sons, 
1990. 
[3] Frigui, H., and Krishnapuram, R., “A Robust Competitive 
Clustering Algorithm with Applications in Computer 
Vision,” IEEE Transactions of Pattern Analysis and 
Machine Intelligence, vol. 21, no. 5, pp. 450-465, May 
1999. 
[4] Tsai, C.F., Liu, C.W., “KIDBSCAN: A New Efficient Data 
Clustering Algorithm,” Lecture Notes in Artificial 
Intelligence, vol. 4029, pp.702-711, 2006. 
[5] Su, M.C., and Chang, H.T., “Fast self-organizing feature 
map algorithm,” IEEE Transactions on Neural Networks, 
vol. 11, no. 3, pp. 721-733, May 2000. 
[6] Sander J., Ester M., Kriegel H., and Xu X., “Density-based 
Clustering in Spatial Databases: The Algorithm 
GDBSCAN and its Applications,” Data Mining and 
Knowledge Discovery, vol. 2, no. 2, pp. 169-194, 1998. 
[7] Dasarathy B., Nearest Neighbor: Pattern Classification 
Technique, IEEE Press, Dec. 1990. 
[8] Tsai, C. F., Yen, C. C., “ANGEL: A New Effective and 
Efficient Hybrid Clustering Technique for Large 
Databases,” Lecture Notes in Artificial Intelligence, vol. 
4426, pp. 817-824, May 2007. 
[9] Tsai, C. F., Tsai, C. W., Wu, H. C., Yang, T., “ACODF: A 
Novel Data Clustering Approach for Data Mining in 
Large Databases,”  Journal of Systems and Software, 
vol. 73, pp. 133-145, 2004. 
[10] Ester, M., Kriegel, H.P., Sander, J., and Xu, X., "A 
density-based algorithm for discovering clusters in large 
spatial databases with noise," 2nd International 
Conference on Knowledge Discovery and Data Mining, 
pp. 226-231, 1996.
  
 28
 摘  要 
This paper adopts the idea of nearest neighbor and proposes a 
new approach called Fast Intuitive Clustering Approach (FICA). 
Besides, FICA also adds the concept of data compression to lower the 
operating times and coordinates with parameters to reach global 
search. A series of experiments have been conducted on FICA and 
other clustering algorithms, like K-Means and DBSCAN. According 
to the simulation results, it is observed that the proposed FICA 
clustering algorithm outperforms K-Means and DBSCAN. FICA can 
not only to perform good efficiency and correctness but also be 
applied in large number of data sets. Finally, the proposed FICA is 
applied in face recognition problem. 
 
 
 
 2
 正文 
一、目的 
本人的一篇論文被著名的計算機學術會議所接受，此學術會議為
2007 機器學習與電腦控制學國際學術會議(2007 International 
Conference on Machine Learning and Cybernetics; ICMLC 2007)，
本屆假香港的美麗華酒店(Miramar Hotel)召開，自 2007 年 8 月 19 日
~8 月 22 日，共計四日。此會議每年在全球尋覓一著名景點召開一次。
本次會議係由國際著名的國際電機與電子工程師協會(IEEE)所主
辦，由 IEEE SMC Society 所支持。本人遂前往香港的美麗華酒店出
席此學術會議並發表論文-- FICA: 一個為資料探勘應用之植基於分
割式之新的資料分群技術。 
 
 
 
 
 
 
 
 
 
 
 4
Networks 、Towards Understanding Images of the Mind、Evolving 
Connectionist Systems: Methods, Tools, Applications 、 An 
Overview of VC Learning Theory and its Applications 、
Unification of Neural and Wavelet Networks and Fuzzy Systems
及 Independent Component Analysis 等多項熱門議題，發表論文甚
多。其主題涵蓋: Economics and Finance、Data Mining and 
Knowledge Discovery、Network Optimization、Communications、
Mobile Business、Economics and Finance、Image Processing、
Data Classification、Vision、Medical Applications、Pattern 
Recognition、Face Recognition、e Government、Rule Extraction、
Speech Processing、Manufacturing、fuzzy Neural Systems、
Applications to Optimization 、Image and Video processing 、
Speech and Handwriting Recognition 、Optical Network、Neural 
Network on Pattern Recognition and Control、Mobile Agent 等。 
而筆者之論文被安排於 8 月 20 日 11:45-13:15 之 regular 
session 中第一篇論文發表，Session Number 為 ME-1,Session Chair
是 Dr. Witold Pedrycz。其中主要探討在資料探勘研究領域中，資
 6
Management、data Mining、Rule Extraction 及 Economics and 
Finance 有關的議題：其中有 Network Flow and Congestion 
Control、Computational Intelligence in Adaptive Educational 
Hypermedia、Chaos Generators for CDMA Communication Using 
Neural Networks、Multi-User Detector with an Ability of Channel 
Estimation Using RBF Network in an MC-CDMA System及 DBNN, FDNN, 
Discriminative Learning, E-Business 等，給予筆者許多寶貴的經
驗與啟發。特別是 Data Mining and Knowledge Discovery 受到了
廣泛的討論，有許多公司與研究單位提出不同的解決方案，預期將
對知識探勘與知識管理的技術產生重大的變革。 
 
三、心得 
資訊技術之進步可謂瞬息萬變，一日千里。它是現代科技的整
合，也是一種計算系統，包括軟體與硬體。影像與通訊系統是最近
非常熱門的研究領域，許多學者投入了此方面的鑽研，所以最近與
此方面相關的國內或國際學術會議層出不窮，與通訊及資料庫相關
 8
文化藝術資產卻也不遺餘力，如此也為他們帶來了大筆的觀光財
產。香港的進步極其快速，是以在發展科技、經濟的同時，也要做
好環境美化及保護珍惜可貴的文化藝術及土地資源，這部分也將是
我國應加以深思的課題。透過此次會議的論文發表,筆者有幸認識許
多國際上從事影像通訊及資料探勘方面研究的專家學者，不但介紹
自己給大家認識之外，也簡略告訴與會國際學者台灣目前在此方面
的發展與應用。筆者透過此次的論文發表，深切體會到專業知識與
技能及英文溝通的重要性，可謂受益菲淺。日後唯有不斷地汲取新
知，並不斷在研究領域上研發創新，加強英文的溝通能力，始能在
國際學術領域內受到重視。 
 
 
 
 
 
 
 
 10
then the distances between each point connecting to the two 
virtual points V1(min_x, min_y) and V2(Max_x, Max_y) will 
be computed. Take average distance multiply α as initial 
clustering distance and segment these points into new cluster 
sets P{P1, P2, …, Pm}.Notably, α=0.04, which is taken as the 
initial clustering distance as a pre-process step. These cluster 
sets are transformed to form the cluster centers and replace X 
{X1, X2, …, Xn} as input. We take a two-dimensional data set 
as example. Notably, step 1 is depicted as Fig. 2. 
 
Fig. 1.  Idea of initial clustering. 
 
Fig. 2.  Sketch of step 1. 
Step 2: From P {P1, P2, …, Pm}, each point connects its nearest 
point one by one in the forward direction but these lines can 
not be repeated and form loops. Compute the total distance of 
generated lines and retain the result lines which are smaller 
than the average distance of generated lines multiply β. 
Notably, β=1.25, which denotes reserve those lines whose 
distance is smaller than average distance multiply β. Fig. 3 
represents the connected lines of step 2. 
 
Fig. 3.  Sketch of step 2. 
 
Step 3: From P {P1, P2, …, Pm}, each point connects its 
nearest point one by one in the backward direction but these 
lines can not be repeated and form loops. Compute the total 
distance of generated lines and retain the result lines which are 
smaller than the average distance of generated lines multiply β 
as shown in Fig. 4. 
 
Fig. 4. Sketch of step 3. 
 
Step 4: As Fig. 5 shows, from P {P1, P2, …, Pm}, each point 
connects its nearest point one by one in the forward direction 
but these lines can not be repeated. Compute the total distance 
of generated lines and retain the result lines which are smaller 
than the average distance of generated lines multiply β. 
 
Fig. 5.  Sketch of step 4. 
 
 
 
 
 
 12
3 Simulations and Analysis 
Several experiments have been performed to verify the 
correctness and efficiency of the proposed FICA. Experiments 
are set as follows: (1) the benchmark dataset, (2) test the 
correctness of FICA with K-means and DBSCAN, (3) Time 
cost of FICA, K-means and DBSCAN, (4) the robustness of 
input parameters. 
All these algorithms have been implemented in JAVA and 
executed on a computer with Intel Pentium 4 2.8GHz and 
512M RAM. 
 
3.1 Benchmark datasets 
 
  There are three datasets adopted as benchmark datasets. 
DS1-DS3 datasets are shown in Table 1. DS1, from Su [5], 
consists of 3 Gaussian clusters with 579 points. There are four 
circular clusters in DS2 which contains 14799 points and four  
arbitrary clusters in DS3 which exists 14290 points, by Jörg  
Sander [6]. These different kinds of clusters are used to test the 
correctness of FICA. 
 
Table 1: Benchmark datasets 
Benchmark 
dataset 
Points Clusters Source 
DS1 579 3 Su and Chang, 2000
DS2 14799 4 Jörg Sander,1998 
DS3 14290 4 Jörg Sander,1998 
 
 
3.2 Correctness experiments on FICA 
 
  In order to prove the clustering performance of FICA, we 
compare the error rate with K-means [1], DBSCAN [10] and 
the proposed FICA. Fig. 8 demonstrates the clustering results 
of K-means, DBSCAN, and the proposed FICA. K-means has 
error rate of 3.45% and DBSCAN has error of 2.07%. 
However, FICA can correctly generate three clusters. 
  
 
Fig. 8. Comparison of correctness with K-means, DBSCAN 
and our FICA using DS1 dataset: (1) the original DS1 dataset, 
(2) the clustering result by K-means, (3) the clustering results 
by DBSCAN, (4) the clustering results by FICA. 
   
This DS2 dataset comes from GDBSCAN generated by 
edge detection to transform an image into two-dimensional 
data points with four circular clusters [6]. As Fig. 9 revelation, 
it is difficult to tell the exact error rate of clustering results, but 
by taking a look at the clustering results, we can recognize that 
DBSCAN and FICA can show better classification. 
 
Fig. 9.  Comparison of correctness with FICA, K-means and 
DBSCAN with DS2: (1) the original DS2 dataset, (2) the 
clustering result by K-means, (3) the clustering results by 
DBSCAN, (4) the clustering results by FICA. 
   
 14
 Fig. 11.  Flow chart of face feature detection system. 
 
                       (1) 
 
(2) 
Fig. 12. Face feature detection application: (1) transformation 
after edge detection, (2) FICA Clustering result. 
 
5 Conclusion 
This paper proposes an FICA clustering algorithm to solve 
the problem of clustering. In the beginning, FICA generates 
small clusters as a pre-process step to lower the size. Then 
FICA utilizes the idea of nearest neighbor to attain local search. 
Besides, referring to the distribution states of input datasets, 
global search is met. Our simulation results confirm that the 
robustness and speed of FICA is better than K-means and 
DBSCAN. A face recognition application is implemented to 
reveal the robustness of FICA. In the future work, we would 
like to apply FICA on decision making and license plate 
recognition. 
References 
[1] McQueen, J.B., “Some Methods of Classification and 
Analysis of multivariate Observations,” Proceedings of the 
5th Berkeley Symposium on Mathematical Statistics and 
Probability, pp. 281-297, 1967. 
[2] Kaufman, L. and Rousseeuw, P.J., Finding Groups in Data: 
An Introduction to Cluster Analysis, John Wiley & Sons, 
1990. 
[3] Frigui, H., and Krishnapuram, R., “A Robust Competitive 
Clustering Algorithm with Applications in Computer 
Vision,” IEEE Transactions of Pattern Analysis and 
Machine Intelligence, vol. 21, no. 5, pp. 450-465, May 
1999. 
[4] Tsai, C.F., Liu, C.W., “KIDBSCAN: A New Efficient Data 
Clustering Algorithm,” Lecture Notes in Artificial 
Intelligence, vol. 4029, pp.702-711, 2006. 
[5] Su, M.C., and Chang, H.T., “Fast self-organizing feature 
map algorithm,” IEEE Transactions on Neural Networks, 
vol. 11, no. 3, pp. 721-733, May 2000. 
[6] Sander J., Ester M., Kriegel H., and Xu X., “Density-based 
Clustering in Spatial Databases: The Algorithm 
GDBSCAN and its Applications,” Data Mining and 
Knowledge Discovery, vol. 2, no. 2, pp. 169-194, 1998. 
[7] Dasarathy B., Nearest Neighbor: Pattern Classification 
Technique, IEEE Press, Dec. 1990. 
[8] Tsai, C. F., Yen, C. C., “ANGEL: A New Effective and 
Efficient Hybrid Clustering Technique for Large 
Databases,” Lecture Notes in Artificial Intelligence, vol. 
4426, pp. 817-824, May 2007. 
[9] Tsai, C. F., Tsai, C. W., Wu, H. C., Yang, T., “ACODF: A 
Novel Data Clustering Approach for Data Mining in 
Large Databases,”  Journal of Systems and Software, vol. 
73, pp. 133-145, 2004. 
[10] Ester, M., Kriegel, H.P., Sander, J., and Xu, X., "A 
density-based algorithm for discovering clusters in large 
spatial databases with noise," 2nd International 
Conference on Knowledge Discovery and Data Mining, 
pp. 226-231, 1996. 
 16
