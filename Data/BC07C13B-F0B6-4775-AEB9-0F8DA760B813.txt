 中文摘要 
關鍵字:視訊編碼、H.264/AVC、計算察覺運動偵測、平行處理、內嵌式系統、視訊檢索、3 維矩量保
持技術、視訊立方格分析、速率-失真度-計算最佳化、視訊資料索引。 
快速發展的網際網路引進包括視訊資料在內之大量多媒體資訊在網路上傳送，視訊資料處理的
高計算複雜度特性限制了在網際網路建構即時視訊應用系統的可能性，常見的效能改善方式是運用
超大型積體電路加速高計算複雜度的即時視訊資料處理模組，然而一些重要的視訊應用系統如
H.264/AVC 編碼系統及視訊檢索系統往往不具備細微平行化因子，因此純硬體化的加速方法的效果
往往受到極大的限制。本計畫捨棄硬體化的加速方法轉而設計軟體為主的視訊系統加速方法，其原
因在於軟體為主的視訊處理架構更適合用於設計快速之可調適視訊應用。 
快速軟體為主的視訊處理架構的核心為計算察覺技術，計算察覺技術基於一計算-失真最佳化方
法尋找影響視訊處理架構效能的關鍵工作，並優先執行具有最大效能改善效果的步驟。具有即時限
制特性的視訊應用往往無法保證當程式執行時間用盡時，系統已達最佳狀態，計算察覺技術優先執
行具有最大效能改善效果的步驟的特性可有效解決這個問題，因此具計算察覺特性的視訊應用系統
往往可以在系統尚未檢驗所有可能解之前的情況下就已達一定程度的系統效能。 
本計畫為一二年期計畫，第一年，我們將提出一具計算察覺特性的即時軟體式的 H.264/AVC 編碼
架構，H.264/AVC 需要大量記憶體存取頻寬的問題也因引進區塊索引結構而獲得解決，區塊索引技
術提供本視訊編碼系統內容導向記憶體隨機存取能力，具計算察覺特性的即時軟體式視訊處理架構
開創了設計即時 H.264/AVC 編碼系統的新方法。本計畫的第二年，我們投注精神在基於 3 維視訊資
料向量量化技術的視訊立方格為主的檢索系統設計，一個視訊片段首先被切割成多個畫面群，每個
畫面群更進一步被切割成多個視訊立方格，然後我們使用 3 維矩量保持技術抽取每個視訊立方格內
包含時間-空間的重要特徵，進而我們設計了一個視訊片語意標示系統。最後，我們可擴展第一年所
使用的區塊索引技術為視訊立方格索引技術，應用第一年所發展的基於索引架構的計算察覺技術，
我們設計了一個具計算察覺特性的即時軟體式視訊檢索系統，同時我們也可利用 GPU 多重處理器平
行化處理本計畫所提出之視訊檢索系統，實驗結果驗證本計畫提出的方法的可行性。 
報告內容 
Part I: Computation-aware fast motion estimation for H.264/AVC using image indexing 
Abstract 
The key to designing a real-time video 
coding system is efficient motion estimation, 
which reduces temporal redundancies. The 
motion estimation of the H.264/AVC coding 
standard can use multiple references and multiple 
block sizes to improve rate-distortion 
performance. The computational complexity of 
H.264 is linearly dependent on the number of 
allowed reference frames and block sizes using a 
full exhaustive search. Many fast block-matching 
algorithms reduce the computational complexity 
of motion estimation by carefully designing 
search patterns with different shapes or sizes, 
which have a significant impact on the search 
speed and distortion performance. However, the 
search speed and the distortion performance often 
conflict with each other in these methods, and 
their high computational complexity incurs a 
large amount of memory access. This paper 
presents a novel block-matching scheme with 
image indexing, which sets a proper priority list 
of search points, to encode a H.264 video 
sequence. This study also proposes a 
computation-aware motion estimation method for 
the H.264/AVC. Experimental results show that 
the proposed method achieves good performance 
and offers a new way to design a cost-effective 
real-time video coding system. 
1. INTRODUCTION 
As the Internet continues to develop, the 
demand for new ways to represent, integrate, 
store, and exchange multimedia information 
(such as text, image, audio, and video) is 
increasing. The increasing role of synthetic 
information and two-way communication systems 
has created a remarkable change in video-driven 
applications such as teleconference, videophone, 
and image-based multimedia services. Advanced 
video coding techniques, which yield images with 
good subjective image quality and make efficient 
use of available bandwidth, facilitate the 
development of video-based applications over the 
Internet. Among these coding methods, H.264 is 
an emerging international video coding standard. 
This standard is the product of Joint Video Team 
(JVT), consisting of ITU-T Video Coding Experts 
Group (VCEG) and ISO/IEC MPEG Video 
Coding Group [1]. Compared to MPEG-4 [2], 
H.263 [3], and MPEG-2 [4], H.264 achieves a 
bit-rate reduction of 39%, 49%, and 64%, 
respectively [5]. 
The performance improvement achieved by 
H.264 comes mainly from the prediction part [1], 
[6] involving the quarter-pixel motion estimation 
(ME) with variable block sizes and multiple 
reference frames. These novel techniques greatly 
reduce prediction errors. H.264 has seven block 
sizes (16x16, 16x8, 8x16, 8x8, 8x4, 4x8, 4x4) and 
up to 5 reference frames for motion compensation 
(MC). Simulation results from H.264 [7] 
reference software show that it achieves more 
than a 15% reduction in bit rate reduction using 
variable block sizes compared with using block 
size 16x16, and a 5%-10% bit rate reduction 
using multiple reference frames [8]. However, the 
complexity of ME in developing H.264 is high. 
Hence, reducing the complexity of ME for 
H.264-based real-time video applications is a 
crucial issue. In H.264, several factors affect ME 
performance: (1) the complexity of the mode 
decision in motion estimation [9]; (2) the 
complexity of reference frames in motion 
estimation [10]; and (3) the number of search 
points required to complete a motion estimation 
process. As Huang et al. [10] reported, encoding a 
video sequence accounts for more than 80% of 
the execution time in motion estimation for the 
reference software of H.264/AVC, JM [7]. 
H.264/AVC adopts block-matching algorithm 
(BMA). Among all BMAs, the well-known 
full-search block-matching algorithm (FSBMA) 
aims to find the position with minimal block 
distortion from all possible candidate motion 
vectors in a predetermined neighborhood search 
window. Although FSBMA produces the best 
quality, it demands the most computation. 
Researchers have proposed many fast BMAs, 
such as three-step search (TSS) [11], Diamond 
Search (DS) [12], and Hexagon Based Search 
(HEXBS) [8], [13] to speed up the FSBMA with 
acceptable distortion performance. By carefully 
matching process. When the stop condition 
occurs, the matching process stops processing the 
remaining un-processed search points since the 
current best match is optimal in terms of a 
similarity measure. The image-indexing scheme 
offers a new way to analyze the workload of a 
block to complete the process of block matching 
for motion estimation. Workload information is 
highly related to the problem of computation 
allocation, which decides the deadline for each 
frame in encoding a video sequence.  
Fast motion estimation are based on 
multilevel successive elimination algorithms [32, 
33] for block matching motion estimation, which 
in turn are variants of the well-known successive 
elimination algorithm (SEA) [34]. Compared to 
the fast block matching methods mentioned 
above, SEA uses a triangle-equality filter to 
reduce the computational complexity of FSBMA 
without sacrificing the optimal characteristics of 
FSBAM in terms of block distortion. One of the 
disadvantages of SEA-like algorithms is that all 
search points must be checked to decide if the 
corresponding blocks should be skipped to find 
the optimal match block. The proposed approach 
uses an image indexing scheme to solve this 
problem. Thus, the proposed block matching is 
also a variant of the SEA, which is fast and 
optimal. This study examines the time consuming 
problem of the inter mode decision in 
H.264/AVC coding standard by extending the 
image indexing scheme into a multilevel structure. 
Computer simulation results show that the 
proposed method achieves good performance and 
offers a new way to design a cost-effective 
real-time video coding system. 
The remainder of this paper is organized as 
follows. Section 2 discusses the structure of the 
image block-indexing method, etc. Section 3 
presents the proposed optimal motion estimation 
using block indexing. Section 4 presents the 
extension of the computation aware motion 
estimation algorithm for H.264. Section 5 shows 
experimental results. Finally, Section 6 gives a 
brief conclusion. 
2. The Structure of Block Indexing  
Let ),( yxI t and ),(1 yxI t denote the pixel values 
at (x, y) in the current frame and the reference 
frame, respectively. Block matching for motion 
estimation is an approach to shift or warp image 
blocks relative to each other and to compute the 
block similarity according to certain criteria. This 
function, often used in video coding because of 
its speed, contains the minimum square error 
(MSE) metric, i.e., 
2
1
1
0
1
0
, |),(),(|),( jvyiuxIjyixIvuMSE t
n
i
n
j
tyx  





 (1) 
where ),( vuu 

 is the displacement and 
|),(),(| 1 jvyiuxIjyixI tt   is the 
displaced frame difference and n is the size of the 
block for matching. Given a block Bt with the 
top-left pixel located at (x,y) in the current frame, 
)12()12(  yx distortion computations must 
determine the best match, where x and y are 
search ranges along the x-axis and y-axis, 
respectively. For a large search window, this 
determination process is very time consuming. 
    To maintain the same distortion achieved by 
FSMBA and speed up the motion estimation, this 
paper proposes an indexing scheme that takes a 
filter-based approach to skipping irrelevant 
blocks based on the projections on orthogonal 
bases. The indexing structure should be simple to 
ensure low complexity of indexing data. Let the 
first basis for computing a response (projection) 
be the following 2D mask: 













1111
1111
1111
1111
1L      (2) 
for n = 4. Given a block Bt-1 with the top-left 
pixel located at (u, v) in the reference frame, the 
projection p1 of Bt-1 is  
)(1 111 LBnp t       (3) 
where 11 LBt  is the inner product between Bt-1 
and L1 if Bt-1 and L1 are vector forms. Figure 1 
shows that 11 LBt   projects 1tB  into 1tB  on 
a hyper-plane H that is perpendicular to L1. The 
vector 1tB is easy to obtain by 
1111 LpBB tt   .     (4) 
 
According to Fig. 1, it is simple to prove that 
2
1
2
1
2
1 |||| pBB tt      (5) 
points in the search window can be checked to 
skip irrelevant search positions and reduce 
computation costs.  
  The proposed fast motion estimation based 
on the block indexing (FMEBI) method uses an 
upper bound for a block sum difference as the 
criterion to eliminate impossible candidate blocks. 
The FMEBI method uses the predicted motion 
vector (PMV) as the initial motion vector and 
then calculates the MSE between the current 
block and the reference block corresponding to 
the PMV as the upper bound to improve the 
filtering performance. Constructing a tighter 
upper bound can improve the efficiency of the 
filter-based approach when using multiple 
orthogonal bases and multiple projections. 
Let L1, L2, and L3 be three orthogonal vectors, 
where 













1111
1111
1111
1111
1L

















1111
1111
1111
1111
2L















1111
1111
1111
1111
3L
 (15) 
for n = 4. Given a block Bt-1 with the top-left 
pixel located at (u,v) in reference frame, the 
projections of Bt-1 on the three bases are 
)(1 1 iti LBnp  .   (16) 
Following the procedure mentioned above, we 
can obtain the following formula: 


 
3
1
11
i
iitt LpBB   (17) 


 

3
1
2
,,
2
1
2
1 )(|||| 1
i
BiBitttt tt
ppBBBB , (18)
2
1
3
1
2
,,min1 |)||(|)(|| 1 tt
i
BiBitt BBppBB tt  

   , (19) 
||
3
1
, t
i
Bit BpS t 

,  (20) 
|| 1
3
1
,1 1 


  t
i
Bit BpS t ,   (21) 
),(|||),(|
4
1
,
2
min1
2
1 vuMSEBBvuSS yxtttt   . (22) 
3. Optimal Block Matching Using Image 
Indexing 
Given a current block, for most fast motion 
estimation algorithms, the search points 
corresponding to reference blocks with less 
distortion may not be checked at the beginning of 
the search point list. This leads to a problem: the 
optimal motion vector might not be found before 
the deadline in a real-time video application. 
Adding an image-indexing scheme to the system 
can facilitate the random access function in terms 
of MSE values. 
…
…
…
…
… ……
… ……
Projection
Reference frame It-1 Projection frame 1
~
tI
Hashing
… …
Projection norms
F
req
u
en
cy Indexing
.
.
.
P
ro
jectio
n
 n
o
rm
s
…
…
…
Inverting List 
 
Fig. 3. The process to generate indexing data for 
fast motion estimation. 
   A straightforward approach to adding the 
image indexing to the proposed FMEBI algorithm 
is to sort the projection norms of blocks 
corresponding to search points in the reference 
frame. Let the size of the block for matching be 
nxn. Considering a reference image It-1 of size 
MxN, and transform It-1 into 1-t
~
I of size 
)()( nNnM  using the following projection 
process: 
|]),(|,,,[),(
~
13211- jiBpppjiI tt     (23) 
where p1, p2, and p3 are the projections of a block 
),(1 jiBt which is an nxn block with the top-left 
coordinates (i,j) in It-1. Note that each location in 
1
~
tI  is characterized by a projection vector of 
size 4. Given a block Bt with the top-left pixel 
located at (x,y) in current frame, every pixel in 
1
~
tI corresponds to a search point in It-1  for 
matching Bt. To speed up the block matching 
process, hash the pixel values of 1
~
tI  using  Eq. 
(21). This step organizes the search points as a 
projection norm histogram Ht-1, which plays a 
role as the indexing structure. Let the projection 
norm of Bt be St. A reference block in It-1 with the 
projection norm St-1 is a neighbor of Bt when its 
coordinates (i,j) are within the search window of 
Fig. 4. Step 6 of the FMEBI algorithm can be 
stopped at the corner point of the curve to obtain 
a computation-aware motion estimation scheme. 
One of the major contributions of the FMEBI 
algorithm is that the workload to obtain the 
optimal motion vector is predictable, and thus the 
algorithm does not waste additional time on 
irrelevant block matching. Step 6 is an iterative 
loop that simultaneously reduces the MSE
* 
value 
and the workload. In designing 
computation-aware motion estimation, the stop 
condition of Step 6 can be changed to the corner 
point of computation-distortion plot occurs, as 
Fig. 4 shows. Furthermore, employing the 
FMEBI algorithm on multiple blocks for motion 
estimation allows blocks with larger gradients of 
computation-distortion curves to have higher 
priorities in motion estimation.  
4. Extension to H.264/AVC 
   H.264 adopts the variable block size (VBS) 
and multiple reference frames (MRF) techniques, 
which include up to 259 different decision modes, 
to reduce the prediction error of motion 
compensation. VBS-MRF is the chief driver of 
massive computation, which is in direct ratio to 
the product of reference frames and decision 
modes. Many algorithms can reduce the inter 
mode [9] and the reference frame number [10], 
[36-38]. These algorithms efficiently reduce 
30-80% of redundant computations in the 
software. However, as pointed out in [39], the 
order of coded blocks limits methods 
emphasizing rate-distortion performance. In other 
words, only local and coded blocks can serve as a 
reference to carry on the remainder of the process 
of bit allocation. This drawback limits the 
real-time applications of these methods. 
Another promising scheme is to reduce the 
search areas on inter modes and reference frames 
using global variations. Some algorithms can 
speed up the mode decision procedure by 
referring to the motion vectors‘ strong 
correlations in consequent pictures [36, 37]. 
However, a shot change in the video sequence 
causes the performance to degrade dramatically 
in the initial stage. In fact, the variable block size 
and multiple references frame techniques adopted 
by H.264 aim to compensate the prediction error 
caused by aliasing problems [38]. To solve this 
problem, the proposed indexing structure 
searches for blocks with a larger matching 
workload that  leads to larger distortion. By 
focusing on high distortion blocks, a 
computation-aware mode decision strategy can 
reduce the search areas of reference frames by 
analyzing the global workload variations.  
A. Structure of  Multilevel Block Indexing 
      
44
00
u
 44
01
u
 44
02
u
 44
03
u

44
10
u
 44
11
u
 44
12
u
 44
13
u

44
20
u
 44
21
u
 44
22
u
 44
23
u

44
30
u
 44
31
u
 44
32
u
 44
33
u

1616u

816
uu

816
bu

168
lu

168
ru

88
00
u
 88
01
u

88
10
u
 88
11
u

48
00
u
 48
01
u

48
10
u
 48
11
u

48
20
u
 48
21
u

48
30
u

48
31
u

84
01
u
 84
02
u
 84
03
u
84
00
u

84
11
u
 84
12
u
 84
13
u

84
10
u

 
Fig. 5. The correlations between motion vectors 
of variable block sizes. 
At the beginning of the mode decision in the 
reference software baseline encoder, the FMEBI 
algorithm calculates the motion vectors of the 16 
4x4 blocks for a macroblock by checking all 
search points in the previous reference frame. The 
algorithm then reuses these motion vectors to 
predict the motion vectors of the block of larger 
size, as the blocks of variable block sizes in the 
same macroblock have high correlation. Figure 5 
shows the correlation between motion vectors of 
variable block sizes. The motion vectors of the 16 
44  blocks in a macroblock can help predict the 
motion vectors of larger blocks. For a block size 
of 1616 , use all the motion vectors of 44  
















1
44
,1
84
,1
1
44
,1
48
,1
30,10,
2
1
10,30,
2
1
i
is
itij
j
jt
itij
jipp
jipp
 . (35) 
After obtaining the values of the first projections 
for each level, it is possible to obtain the values 
of the norm of the projected vectors for each 
index level using Eq. (4). Adding together the 
projections and the norms of the projected vectors 
for all blocks using Eq. (12) constructs the 
multilevel indexing structure.  
B. The Computation Aware  Inter mode 
Decision Using Computation-Distortion 
Optimization 
The H.264 mode decision dealing with the 
kernel rate-distortion function compromises 
between distortion and the coding bit-rate. In 
general, the coding order of blocks limits the 
improvement in rate-distortion performance by 
including workload variations in searching for the 
optimal motion vectors of blocks in the current 
frame. The proposed mode decision algorithm 
conducts workload analysis using multilevel 
image indexing. The motion vectors of the 16 4x4 
blocks in a 16x16 block can be used to calculate 
the predictive motion vector of the block, which 
is then used to compute the initial MSE value. As 
mentioned above, given a block with initial MSE, 
the workload for searching for the optimal motion 
vector can be computed using Eq. (25) based on 
the indexing data. The ratio of MSE to workload, 
which corresponds to the slope of the 
MSE-workload curve, is a useful measure of 
whether or not a block is computation-distortion 
efficient. Blocks with large MSE values but small 
workloads are computation-distortion efficient 
since they are cost effective in computing power. 
On the contrary, blocks with small ratios are not 
computation-distortion efficient. However, a 
potential disadvantage of the MSE-workload ratio 
is that it might not distinguish the ratio of a large 
MSE block with a large workload from the case 
--a small MSE block with a small workload. We 
would like to spend more time to reduce the MSE 
values for the blocks belonging to the former case 
to obtain better coding quality. Thus, the MSE 
value should work as a second index to determine 
the block order for motion estimation.  
  The proposed computation aware mode 
decision process maintains a priority list whose 
head block is the most computation efficient 
block for improving the distortion-rate 
performance. Initially, all the 16x16 blocks in the 
current frame are sorted and kept as a priority list 
PL in the descending order using the ratio values 
as the first index and MSE values as the second 
index. The core of the mode decision process is 
an iterative loop that consists of three steps: Step 
1 removes the head block H from PL; Step 2 
obtains the optimal motion vector of H using the 
FMEBI algorithm with the corresponding 
indexing data; Step 3 divides H into multiple 
sub-blocks, and inserts each of them into PL after 
computing its initial MSE and MSE-workload 
ratio. Notice that it is not necessary to perform 
the FMEBI algorithm on H of size 4x4 since the 
preprocessing stage of the mode decision process 
obtains its optimal motion vectors. The mode 
decision process stops when PL is empty or the 
process reaches the deadline. The following 
section summarizes the fast mode decision 
algorithm. 
Algorithm 2. Fast Computation Aware Mode 
Decision (FCAMD) Using Multilevel Block 
Indexing 
Input: Current frame It, reference frame It-1, the 
deadline to encode It, and indexing data 
for 4x4 blocks of It-1 
Output: Motion vectors of blocks of variable sizes 
Method: 
1. Perform the FMEBI algorithm on It to obtain 
the motion vectors of 4x4 blocks. 
2. Construct multilevel block indexing of It-1 
based on the indexing data of the 4x4 blocks. 
3. for each 16x16 block B in It, compute its 
predictive motion vector using Eq. (31), and 
then compute the distortion-workload 
ratio BBB WLMSER  using Eq. (1) and (25). 
4. Construct the priority list PL by sorting 16x16 
blocks Bs in It in descending order in terms 
of BR . 
5. Remove the head block B from PL. 
6. while )( NULLB  and the deadline is not 
reached 
switch (size of B) 
 case 16x16:  
Perform the FMEBI algorithm on 
B to obtain the optimal motion 
indexing data constructed from the reference 
frame i. This paper assumes that each block mode 
in a macroblock has the same number of 
reference frames. On the contrary, macroblocks in 
the current frame may have different numbers of 
reference frames (1 or 5). Moreover, the indexing 
data for 4x4 blocks for each reference frame 
determines the number of reference frames of a 
macroblock to reduce the size of indexing data. 
Thus, we have 16x5 workload information 
16,...,1,5,...,1,)(  jiWL iB j for a macroblock B.  
Let 80
5,...,1 16,...,1
)()(    i j
i
B
i
B
j
WLWL be the 
average workload value for B. Compute the 
workload variance VarB of B using the following 
equation: 
2
5
1
16
1
)( )( B
i j
i
BB WLWLVar j
 
 .  (36) 
A higher value of VarB represents a higher 
variation in workload using different indexing 
data; on the contrary, a lower value of VarB means 
that B contributes less distortion improvement 
using multiple reference frames. The rule to 
determine the number of reference frame RF is 
then  


 

otherwise,5
 if,1 B
B
Var
RF     (37) 
where is the threshold. In practice, we set as 
the average of VarB. This implies that half of the 
macroblocks in the current frame perform motion 
estimation using multiple reference frames. After 
determining the value of RFB, the steps to 
perform motion estimation of B in the FCAMD 
algorithm should use multiple reference frames 
RFB = 5.  
5. Experimental Results 
This study evaluates the proposed approach 
using a series of experiments on a 1.8 GHz PC 
with 960 MB of memory. For motion estimation, 
the search window is from -16 to 16, the number 
of reference frames is 5, and the number of block 
types is 7. The simulations in this study compare 
the performance of the proposed method with 
various computation-aware approaches CA-FS, 
CA-PDS, and CA-3SS, all presented by Tai et al. 
[18], and two methods presented Chen et al. [20]. 
These simulations implement all the methods in 
the H.264/AVC reference software JM.17 [7]. 
This section compares the proposed method with 
the simulated computation aware schemes 
mentioned above using eight test video 
sequences –‗Forman,‘ ‗Salesman,‘ ‗Mother & 
Daughter,‘ ‗Carphone,‘ ‗Grandma,‘ 
‗Clair,‘ ‘News,‘ and ‘Silence.‘ All the test 
sequences were in the QCIF format. 
Figures 7 and 8 depict the relationships 
between the average PSNR value and the average 
number of search points per macro block (MB) 
for test video sequences using the compared 
methods. To ensure a fair comparison, the JM 
reference software used the same parameters to 
compress test video sequences using the 
compared methods. For block matching, the 
average number of search points required for each 
MB determines the computational complexity of 
motion estimation. Furthermore, the order of 
evaluating search points has a significant impact 
on the final compression quality. This is because 
the deadline constraint of real-time video 
applications limits the number of search points 
per MB for motion estimation. Compared with 
other computation-aware schemes, the proposed 
scheme produces the highest quality of 
reconstructed video sequences among the 
compared methods under the constraint of search 
points per MB. In addition, the proposed method 
uses relatively few search points per MB to find 
the best match for a macro block in motion 
estimation compared with other computation 
aware motion estimation. The proposed method is 
better able to filter unnecessary search points, 
implying that the proposed computation aware 
motion estimation is ideally suited to real-time 
video applications.  
Figures 9 and 10 depict the relationships 
between the bit rate and the average number of 
search points per macro block (MB) for test video 
sequences using the compared methods. The 
proposed method generates smaller bit rates but 
higher PSNR values than the simulated methods 
under the same number of search points per MB 
and compression parameters. Thus, the proposed 
method outperforms the compared methods in 
constructing real-time video applications with 
low bit rate but high quality. According to the 
simulation results in Fig. 7 and 8, the average 
PSNR values of the reconstructed video 
5 10 15 20 25 30 35 40 45 50
38.8
38.9
39
39.1
39.2
39.3
39.4
39.5
39.6
Claire
Search points/MB
A
v
er
a
g
e 
P
S
N
R
Proposed method
CA FS[14]
CA PDS[14]
CA 3TS[14]
Method 1 of Chen et al.[16]
Method 2 of Chen et al.[16]
 (b) 
5 10 15 20 25 30 35 40 45 50
36
36.1
36.2
36.3
36.4
36.5
News
Search points/MB
A
v
er
a
g
e 
P
S
N
R
Proposed method
CA FS[14]
CA PDS[14]
CA 3TS[14]
Method 1 of Chen et al.[16]
Method 2 of Chen et al.[16]
  (c)                                                                         
5 10 15 20 25 30 35 40 45 50
35.56
35.58
35.6
35.62
35.64
35.66
35.68
Silence
Search points/MB
A
v
er
a
g
e 
P
S
N
R
Proposed method
CA FS[14]
CA PDS[14]
CA 3TS[14]
Method 1 of Chen et al.[16]
Method 2 of Chen et al.[16]
 (d) 
Fig. 8. The relationships between the average 
PSNR value and the average number of search 
points per macro block (MB) using the compared 
methods for test sequences (a) ―Grandma,‖ (b) 
―Claire,‖ (c) ―News,‖ and (d) ―Silence.‖ 
5 10 15 20 25 30 35 40 45 50
100
150
200
250
300
350
400
450
500
Foreman
Search points/MB
B
it
 r
a
te
s 
(k
b
p
s)
Proposed method
CA FS [14]
CA PDS [14]
CA 3SS [14]
Method 1 of Chen et al.[16]
Method 2 of Chen et al. [16]
 
(a)                                                                              
5 10 15 20 25 30 35 40 45 50
65
70
75
80
85
90
Salseman
Search points/MB
B
it
 r
a
te
s 
(k
b
p
s)
Proposed method
CA FS [14]
CA PDS [14]
CA 3SS [14]
Method 1 of Chen et al.[16]
Method 2 of Chen et al. [16]
 (b)
5 10 15 20 25 30 35 40 45 50
100
110
120
130
140
150
160
170
180
190
Mother and daughter
Search points/MB
B
it
 r
a
te
s 
(k
b
p
s)
Proposed method
CA FS [14]
CA PDS [14]
CA 3SS [14]
Method 1 of Chen et al.[16]
Method 2 of Chen et al. [16]
(c)                                                                             
0 10 20 30 40 50 60 70 80 90 100
34.5
35
35.5
36
36.5
37
Frame No.
P
S
N
R
(Y
)
Foreman
Proposed method
CA FS [14]
CA PDS [14]
CA 3SS [14]
Method 1 of Chen et al.[16]
Method 2 of Chen et al. [16]
 
Fig. 12. The PSNR distribution of the 
reconstructed frames for the video sequence 
―Foreman‖ using the compared compression 
methods. The compression parameters for all 
methods are the same in this experimental work. 
Tables 1 and 2 compare the execution speed 
of video sequences using the compared methods, 
where all methods generated similar compression 
quality in terms of PSNR. In most cases, the total 
time to encode a video sequence is slightly longer 
than that for the compared methods. This is 
because the proposed method requires extra time 
to maintain the indexing structure. However, the 
proposed motion estimation method is more 
suitable for implementation on a parallel machine. 
Performing motion estimation on multiple blocks 
concurrently can greatly reduce the maintenance 
overhead of indexing data. Furthermore, the 
average number of search points per macro block 
required to achieve a specific image quality in the 
proposed method is much lower than that in the 
compared methods. In this case, achieve the 
specific PSNR value requires an average of 10,  
27.5, 15, 18.125, 16.875, and 16.875 search 
points per macro block for the proposed method, 
CA FS [18], CA PDS [18], CA 3SS [18] , Method 
1 of Chen et al. [20], and Method 2 of Chen et al. 
[20], respectively. These results again 
demonstrate the effectiveness of the filtering 
capability for the proposed indexing structure. 
Figure 11 plots the relationship between the 
number of search points visited and the test video 
sequences using the compared methods. Figure 
12 shows the PSNR distribution of the 
reconstructed frames for the video sequence 
―Foreman‖ using the compared methods. The 
proposed method outperforms other simulated 
methods for almost all reconstructed frames 
under the same compression parameters for 
H.264/AVC reference software. Notice that the 
indexing structure would results in noticeable 
computation time if it is not well designed. To 
solve the problem, a fast method proposed by 
Viola and Jones [40] is used to construct the 
index data and the encoding time of the proposed 
method, shown in Tables 1 and 2, is highly 
reduced. 
The methods in JM 17 [7] including Full 
Search, Uneven Multi-Hexagon Search 
(UMHexagon Search) and Enhanced Predictive 
Zonal Search (EPZS) and the method proposed 
by Kuo & Chan [14] are also simulated for 
performance comparison to make sure the 
efficiency of the proposed method. Tables 3 and 4 
shows the experimental results using different 
methods and test video sequences. In order to 
have a fair comparison, the PSNR values of the 
reconstructed frames using the compared methods 
are almost the same. As compared with the full 
search algorithm, the proposed method performs 
equally well in image quality and 80% time 
reduction in total encoding time. Although the 
proposed method takes extra time to construct 
indexing for each frame, the execution time of the 
proposed method is comparable to UMHexagon 
Search and Kuo & Chan‘s method. EPZS is the 
fastest method among all compared methods, but 
it suffers from relatively bad quality. 
UMHexagon Search, EPZS, and Kuo & Chan‘s 
method does not make sure to obtain optimal 
motion vectors; however the proposed does. Thus, 
the proposed method can accurately encode video 
sequences with low computational complexity.  
6.  Conclusion 
Motion estimation is the most critical aspect of 
H.264, which is the latest standard for video 
compression that supports video applications with 
high video quality and low bit rates. This paper 
presents a fast block matching for video coding 
based on image block indexing. A video encoding 
strategy based on block indexing also increases 
the random access capability of the system 
without the disadvantage of high-computational 
complexity for traditional block motion 
estimation. The proposed method applies a simple 
scheme to generate and maintain indexing data. 
Communication and Image 
Representation,   Vol. 17   No.  4,    pp. 
767-782, 2006. 
[20] C.-Y. Chen, Y.-W. Huang, C.-L. Lee, and L.-G. 
Chen, “One-Pass Computation-Aware Motion 
Estimation with Adaptive Search Strategy,” 
IEEE Trans. Multimedia, vol. 8, no. 4, pp. 
698-706, August 2006. 
[21] C.-Y. Cho, S.-Y. Huang,  and J.-S. Wang, 
“Design of computation-aware mode decision 
scheme for H.264/AVC,” in Proc. Of 31st 
IEEE International Conference on Acoustics, 
Speech and Signal Processing, Vol. II, pp. 
565-568, May 14-19, 2006 Toulouse, France. 
[22] M. Jakubowski and G. Pastuszak, “A new 
multi-path scheme for adaptive 
computation-aware motion estimation,” 
Opto-electronics Review, Vol. 15,   No. 2,   pp. 
118-124,    2007. 
[23] S. Theodridis and K. Koutroumbas, Pattern 
Recognition, second. Ed., Academic Press, 
2003. 
[24] H. Murase and S. K. Nayar, “Visual Learning 
and Recognition of 3D Objects from 
Appearance,” International J. of Computer 
Vision, vol. 14, no.1, pp. 5-24, 1995. 
[25] A. K. Jain and R. C. Dubes, Algorithms for 
Clustering Data, Prentice Hall, 1988. 
[26] Jim Z. C. Lai and Y.-C. Liaw, “Fast Searching 
Algorithm for Vector Quantization Using 
Projection and Triangular Inequality,” IEEE 
Trans. Image Processing, vol. 13, no. 12, pp. 
1554-1558, Dec. 2004. 
[27] S.-C. Cheng and T.-L. Wu, ”Fast Indexing 
Method for Image Retrieval Using k Nearest 
Neighbors Searches by Principal Axis 
Analysis,” Journal of Visual Communication 
and Image Representation, vol. 17, pp. 42-56, 
Feb. 2006. 
[28] T. Kim, “Side Match and Overlap Match 
Vector Quantizers for Images,” IEEE Trans. 
Image Processing, vol. 1, no. 2, pp. 170-185, 
1992. 
[29] J. McNames, “A Fast Nearest-Neighbor 
Algorithm Based on a Principal Axis Search 
Tree,” IEEE Trans. PAMI, vol. 23, no. 9, pp. 
964-976, 2001. 
[30] Jim Z. C. Lai, Y.-C. Liaw, and J. Liu, “Fast 
k-Nearest-Neighbor Search Based on 
Projection and Triangular Inequality,” Pattern 
Recognition, vol. 40, pp. 351-359, 2007. 
[31] S.-C. Cheng, “Content-based Image Retrieval 
Using Moment-preserving Edge Detection,” 
Image and Vision Computing, vol. 21 no 9, pp. 
809-826, Sep. 2003. 
[32] X. Q. Gao, C. J. Duanmu, and C. R. Zou, “A 
Multilevel Successive Elimination Algorithm 
for Block Matching Motion Estimation,” IEEE 
Trans. On Image Processing, Vol. 9, No. 3, pp. 
501-504, 2000. 
[33] Shou-Der Wei, Shao-Wei Liu, and 
Shang-Hong Lai, “Fast and Optimal Block 
Motion Estimation Via Adaptive Successive 
Elimination,” in Proc. of IEEE International 
Conf. on Image Process, pp. 2361-2364, 2006. 
[34] W. Li and E. Salari, “Successive Elimination 
Algorithm for Motion Estimation,” IEEE Trans. 
On Image Processing, Vol. 4, pp. 105-107, 
1995. 
[35] M. R. Spiegel, Mathematical Handbook of 
Formulas and Tables, McGraw-Hill Book 
Company, 1981. 
[36] Y. P. Sun and M. T. Sun, “Fast Multiple 
Reference Frame Motion Estimation for 
H.264/AVC,” IEEE Trans. Circuit and Systems 
Video Technol., Vol. 16, No. 3, pp. 447-452, 
2006. 
[37] M. J. Chen, G. L. Li, Y. Y. Chiang, and C. T. 
Hsu, “Fast Multiframe Motion Estimation 
Algorithm by Motion Vector Composition for 
the MPEG-4/AVC/H.264 standard,” IEEE 
Trans. Multimedia, Vol. 8, No. 3, pp. 478-487, 
2006. 
[38] Z. Liu, S. Goto, and T. Ikenaga, 
“Content-Aware Fast Motion Estimation for 
H.264/AVC,” IEICE Trans. Fundamentals, Vol. 
E91-A, No. 8, pp. 1944-1952, 2008. 
[39] C.-Y. Cho, N. S.-Y. Huang, and J.-S. Wang, 
“Design of Computation-Aware Mode 
Decision Scheme for H.264/AVC,” in Proc. 
IEEE Intl. Conf. Acoustics, Speech, and Signal 
Processing (ICASSP), Vol. II, pp.565-568, 
2006. 
Paul Viola and Michael J. Jones, “Rapid  Object 
Detection  using a Boosted  Cascade of Simple 
Features,” in Proc. IEEE Intl. Conf. on Computer 
Vision and Pattern Recognition (VCPR), 
pp.511-518, 2001. 
 
Table 4. Average PSNR per frame and the total encoding time in micro second (ms) with different methods 
for video sequences ―Silent,‖ ―Paris,‖ ―Mobile,‖ and ―Tempete.‖ The search range is from -16 to 16, the 
number of reference frame is 5, and the number of block types is 7. 
 Silent Paris Mobile Tempete 
PSNR Encoding 
time 
PSNR Encoding 
time 
PSNR Encoding 
time 
PSNR Encoding 
time 
Full Search [7] 35.71 2454158 34.92 8795171 33.19 9715372 33.68 9283706 
UMHexagon Search [7] 35.70 352663 34.91 2793489 33.11 3267793 33.61 3151528 
EPZS [7] 35.70 297287 34.89 2451192 33.04 2586948 33.62 2511043 
Kuo & Chan‗s method [14] 35.71 359410 34.91 2640642 33.16 3314065 33.65 3092342 
Proposed 35.71 364873 34.92 3061093 33.19 3234823 33.68 3102437 
 
those pixels at the corresponding pixel position 
among the frames in a video shot [3]. However, 
the TMOF scheme does not consider the motion 
feature of the video shot concerned. Furthermore, 
the performance of key frame based video 
retrieval is sensitive to the outlier frames within a 
video shot. 
Spatial-temporal feature extraction is a crucial 
step for adhering semantic information on image 
object relationship to improve the quality of 
content-based video retrieval. The approaches for 
extracting such features are twofold –to extract 
spatial-temporal features from a 3D volume with 
(x,y) as the spatial coordinates in each frame and 
(t) as the third temporal dimension, and to 
separate the extraction of spatial features in 
individual frames from the extraction of temporal 
features based on detecting the variance among  
these frames. The former extracts different types 
of spatial-temporal features including object 
trajectories, shape, motion, and activities through 
the analysis of this 3D data space [8], whereas the 
latter concatenates spatial and temporal feature 
vectors to represent the content of a video 
sequence [9].  However, all these approaches to 
handling spatial-temporal motion-based 
recognition involve a variant of models, leading 
to heavy computation and to a domain-specific 
processing of real-world scenes. 
Spatial-temporal information representation 
offers a video retrieval framework which uses 
pattern recognition techniques to analyze and 
recognize visual appearance. In [4], Ren et al. 
categorize spatial-temporal recognition as either 
trajectory-to-trajectory-based approach or 
sequence-to-sequence-based approach. The 
former assumes that object motion trajectories 
can be used to recognize moving objects‘ 
behavior, activities, human gait, etc, whereas the 
latter represents video content by extracting 
features from pixel changes frame-by-frame. The 
time-consuming problem of estimating object 
motion trajectories is slightly solved by using 
spatial-temporal slice from image sequence 
volume (x,y,t) [8,10,11]. Another approach to 
motion trajectory matching for video retrieval is 
to represent the trajectory of an object as a 2D 
curve on a plane, which is typically approximated 
as a polygon or splines [12,13]. More recently, 
several research results for activities analysis 
based on spatial-temporal features and motion 
trajectories are also presented [14-17]. However, 
all these approaches to handling motion analysis 
are sensitive to false alerts due to tracking failure 
or creation of false targets including 
fragmentation of targets, shadows, and object 
occlusion. 
In contrast to trajectory-based approaches that 
track moving objects only, 
sequence-to-sequence-based matching extract 
spatial-temporal features from all pixel values in 
video frames. In this approach, video similarity 
can be computed based on motion feature 
comparison [5,18], string matching [19] , and 
knowledge-based modeling [20]. 
Knowledge-based modeling approach relies on 
the rules derived from training video sequences to 
enhance the accuracy of video retrieval [21]. 
Although the gap between the low-level and 
high-level features limits the accuracy of 
sequence-to-sequence-based approach, it is 
simple, direct and suitable to general purpose 
video retrieval. 
Following the sequence-to-sequence-based 
approach, we propose an original and general 
method to extract online spatial-temporal features 
from (x,y,t) video volume. In this paper, each 
video shot is first segmented into multiple video 
cubes. The spatial-temporal features within video 
cubes are then extracted with a set of analytical 
formulas using the proposed 3D 
moment-preserving principle [27]. Other 
interesting approach to modeling spatial-temporal 
features based on video cubes is also applicable 
to the video retrieval framework [28]. The content 
of a video cube is finally approximated by three 
blocks corresponding to the cube projections on 
xy, yt and tx planes. Based on the visual patterns 
of these three blocks, a fast video shot retrieval 
scheme is proposed. As compared with other 
histogram-based key-frame representations, the 
proposed cube-based video retrieval improves the 
retrieval accuracy without sacrificing the 
execution speed. Experimental results show the 
efficiency and effectiveness of the proposed video 
retrieval. 
2. Video Cube Analysis using 3D 
characteristics than for spatial mechanisms. In 
general, the maximum value of the contrast 
sensitivity function versus temporal frequency 
occurs at 8~25 Hz and falling off both at higher 
and lower frequencies. Considering an image 
block, it can be decided as a static block or a part 
of background if the block shows little variation 
in pixel values along the time axis. Otherwise, the 
block might be a part of a moving object. The 
size of time window for a video cube that 
contains a visible moving pattern in the human 
visual system is about 4 if the temporal frequency 
is 25 Hz. And hence, the size of a video cube is 
determined to be 444  to achieve the goal of 
accurate representation for cube content 
according to human perception. In practice, it is 
possible to use T44 (T > 4) cubes in 
spatial-temporal feature analysis as the motion in 
frame images of a video shot is relatively slow. 
2.2 Spatial-Temporal Feature Extraction 
A given video shot is partitioned into a set of 
non-overlapping cubes. Each cube is classified as 
either a stationary cube or a moving cube. The 
edge transition in each moving cube is detected 
by the proposed feature detection technique. The 
frame images can be reconstructed according to 
the parameters of these cubes.  
Figure 1(b) shows the aspects used for feature 
detection in this approach, including a continuous 
three-dimensional plane model specified by six 
parameters, two representative gray (color) values 
h1 and h2, an edge translation l, and the 
3-dimensional principal axis of the 3D cube 
specified by three directional numbers cos, cos 
and cos , where  , and  are the angles 
between the principal axis and the axes x, y and t, 
respectively. The transition is approximated as 
simply a step edge transition from representative 
value h1 to representative value h2. The edge 
translation l is defined as the length from the 
center of the edge model to the transition, and is 
confined within the range of -1 to +1. cos, cos 
and cos are the three direction numbers of L and 
satisfy the following relationship: 
1coscoscos 222   .   (4) 
The values of  , and can be computed 
individually first, as described next. 
 Let  tyx ,,  be the coordinates of the center 
of gravity of the gray (color) values within the 
sphere S inscribed the cube C. Then  
),,(),,(
000 M
M
M
M
M
M
tyx t
yx     (5) 
where M0 , Mx , My and Mt are the mean, the 
x-mass moment, the y-mass moment and the 
t-mass moment of S, respectively, and are 
computed by 





































S
S
S
S
t
y
x
dydxdttyxtf
dydxdttyxyf
dydxdttyxxf
dydxdttyxf
M
M
M
M
),,(
),,(
),,(
),,(
0
     (6) 
where f(x,y,t) is the gray value and the vector 
norm at (x,y) and frame t for a gray-level image 
and a color image, respectively. The vector norm 
of the pixel at (x,y,t) of a color image is computed 
as 
,),(),(),(),,( 222 yxByxGyxRtyxf ttt   (7) 
where (Rt(x,y), Gt(x,y), Gt(x,y)) denotes the color 
value (R, G, B) of the pixel at (x,y) of frame t for a 
color image. 
x
y
t
-1
1
-1
1
-1
1
h1
h2
S’
C’
l
x
y
t

(cos,cos,cor)
d
cor
cos 
t
d
x
y
1
cos

(a)
(b)
(c)
 
Fig. 2. Given the orientation  , and of a 3D 
edge transition of a cube, the normal of the 
transition plane will coincide with t axis with 
rotating the cube about x axis by and rotating 
the cube about y axis by . (a) The edge transition 
if the value of p2 is less than 0.5. 
  An algorithm is given below to summarize the 
proposed 3D moment-preserving feature detector. 
Algorithm 1. Proposed 3D moment-preserving 
feature detection technique (3DMPFD). 
Input. A video cube C containing an edge 
transition. 
Output. Parameters of the transition plane. 
Method. 
Compute the values of Mx, My, Mt and Mo from 
the sphere S inscribed inside C. 
Calculate the values of  , and using 
equation (8). 
Apply the moment-preserving technique to find 
the solutions of the parameters h1, h2, p1, 
and p2 of the transition plane. If C is a 
gray-level cube, apply the 
moment-preserving threshlding [23] 
technique, else use the quaternion 
moment-preserving thresholding [25] 
technique. 
If )5.0( 2 p , apply equation (17) to compute 
the value of translation l, else use equation (18). 
 
Fig. 3. Applying the proposed 3D 
moment-preserving principle to a gray-level cube: 
(a) the original cube and its computed values of 
moments M0, Mx, My, Mt and three directional 
numbers cos, cos and cos of the plane; (b) the 
detected plane and the values of parameters h1, h2, 
p1, p2, and l. 
Employing the cube-based feature analysis 
to an input video cube, we obtain the parameter 
set (cos, cos cos, l, h1,h2) which can be used 
to reconstruct the content of the cube. To verify a 
cube containing an edge or not, we could simply 
check the difference between h1 and h2. Given a 
predefined threshold , the cube can be classified 
as a uniform cube if  )( 21 hh ; otherwise, the 
cube contains an edge. If a cube contains an edge, 
the pixels in a cube can be classified into two 
parts – one of them is represented as h1 and the 
other is represented as h2. As an example, 
suppose a gray-level cube with size 4 x 4 x 4 (as 
shown in Fig. 3(a)) is given and the detected 
plane and its parameters are shown in Fig. 3(b).  
 
Fig. 4. Projecting the pixel values of a quantized 
video cube onto xy, yt and tx planes to obtain 
three projection blocks. 
2.3 Video Shot Representation 
   Based on the parameters of a video cube C, 
the volumetric cells of C can be classified into 
two parts – one of them is represented as h1 and 
the other is represented as h2. The plane to 
characterize the transition of pixel values in the 
cube can be described as 
,coscoscos ltyx  
1,,,1  ltyx .          (19) 
As shown in Fig. 4, the cube can be 
represented as xy, yt, and tx blocks through 
projecting the quantized video cube onto xy, 
yt, and tx planes, respectively. The xy block 
reveals the spatial changes on local visual 
similarity of two video shots with a lower MSE, 
indicating a greater degree of similarity. 
Let the size of a video shot be TYX  . 
Given a query video shot Vq and a database video 
shot Vd, we need to compute the similarity 
between Vq and Vd according to certain distance 
function. In order to speed up the process of 
video shot matching, in this paper, the minimum 
square error (MSE) is used: 
   
 
  

aX
i
bY
j
cT
k
CCVV d
V
ijk
qV
ijk
dq
MSEMSE
1 1 1
,,
)()(  (22) 
where cba  is the size of a video cube. The 
computation complexity of video shot similarity 
is high as a video shot might consist of a large 
amount of cubes.  Fortunately, the performance 
can be improved by transforming the cube 
matching into block matching using the 
projection blocks mentioned above. That is, the 
cube difference in (22) is approximated as  
txtxytytxyxy BBBBBBCC
MSEMSEMSEMSE
21212121 ,,,
,     (23) 
where xyB1 (
xyB2 ), 
ytB1 (
ytB2 ), and 
txB1 (
txB2 ) are 
the xy, yt, and tx projection blocks for the video 
cube C1 (C2), respectively. Based on the fast 
method, proposed in our previous work [26], for 
visual pattern matching, the value of 
21 ,CC
MSE in 
(23) is computed quickly. 
3.2 Matching Strategy for Video Shot Retrieval 
In order to perform video retrieval we must be 
able to efficiently compare two video shots, 
where one is a database video shot and the other 
is the query video shot to determine if they have 
similar content. As mentioned above, based on 
video cube representation, the MSE measurement 
between two video shots using (22) can be used 
to determine their similarity. However, the time 
complexity to compute the MSE value remains a 
challenge for the application of the proposed 
method to a large-scale video database. Although 
the efficiency of the video shot retrieval is much 
improved based on the cube model, the time 
complexity of the system is directly proportional 
to the size of the database. To answer a query, 
un-relevant video shots in the database should be 
skipped using a proper indexing structure to 
further enhance the performance. A 
quick-and-dirty approach is applied to quickly 
filter the impossible matches of a query video 
shot based on the cube model. The database video 
shots in the candidate set is then evaluated and 
ranked one by one in terms of MSE. 
A video cube to be coded A common sub-video shot
1V 2V
x
y
t
 
Fig. 6. Aligning video shot V2 to V1 for computing 
the matching rate according to their common 
sub-video shot. 
As mentioned above, a video shot can be 
modeled as a (x,y,t) volume and a video cube 
corresponds to a sub-volume of the space. To 
represent the content of a video sequence based 
on the cube model provides two advantages: 
lower computational complexity and less storage 
space. However, it also generates problems in two 
situations described as follows. 
(1) A process to align the common video cubes 
of a query video with those of a database 
video must be done, as shown in Fig. 6. The 
representation of a video shot with cube 
model is not rotation or scale invariant, but 
is translation invariant if the identical 
sub-volume of the two video shots is aligned 
properly. 
(2) The retrieval by matching the video cubes of 
database video shots and a given query shot 
directly may receive responses from 
irrelevant matching pairs because if two 
video shots have the same sub-volume, then 
parts of the video cubes for both two shots 
are overlapping. The problem is how to 
efficiently find the highly co-related parts. 
To overcome the problems from both 
situations above, the 1-D histograms of 
visual-pattern types are computed from a 
video shot to eliminate irrelevant matching. 
database. 
Method. 
(1) For each video segment V in D, encode the 
content of V using the proposed video cube 
modeling. 
(2) Tailor Q into several segments by removing 
front and rear frames so that the size of Q is a 
multiple of the size of a segment. 
(3) For each query segment Qs, do the following 
steps. 
(3.1) Encode the content of Qs using the video 
cube modeling. 
(3.2) Compute the visual-pattern histogram of 
Qs. Match all the segment histograms of 
D with that of the query segment Qs using 
equation (24) to obtain the segments in D 
whose values of similarity are larger than 
a predefined threshold (i.e. 0.97). 
(3.3) Compute the MSE values between Qs and 
segments of D obtained from Step (3.2). 
 (3.4) For each database shot, find the segment 
with the smallest value of MSE with Qs. 
(4) Average the values of MSE of all query 
segments as the matching score between Q and 
a database shot in D. 
(5) Rank the matching scores and output the top k 
database video shots to the user. 
As mentioned above, Steps (3.3) and (3.4) can be 
speeded up by screening un-relevant computation 
of MSE values using the filtering rule (26). 
4. Experimental Results 
A video database with more than 1000 video 
shots is constructed to evaluate the performance 
of the video shot retrieval system. The selected 
video shots in the test database are extracted from 
different categories including a complete movie 
program, a news program, and an edited home 
video. The shot boundaries have been identified 
manually, and 100 queries are randomly selected 
from the database. The TMOF [2] algorithm is 
also implemented for performance comparison in 
order to verify the effectiveness of the proposed 
method. 
 The size of a video cube affects the 
efficiency and effectiveness of the proposed video 
retrieval. In practice, we set the cube size to be 
4x4x4. However, it is possible to adapt the cube 
size for the variation of voxel values. To evaluate 
the respective performances of the simulated 
methods, the average recall (AR) and the average 
normalized modified retrieval rank (ANMRR) [2] 
are used. AR is defined as: 
 



Q
q qng
qnr
Q
AR
1 )(
)(1             
where )(qnr and )(qng denote the number of 
correctly retrieved items in the top K retrievals 
and  the number of ground truth, respectively to 
answer a query q.  GTMqngK  2),(4min , 
where  )(max qngGTM  . To obtain the ANMRR, 
the modified retrieval rank (MRR) should be 
computed first:  
5.0
2
)(
)(
)(
)(
)(
1
 

qng
qng
ir
qMRR
qng
i
   
where r(i) is either the rank of each of the ng(q) 
in the top K retrievals or K + 1. The ANMRR is 
then calculated by 



Q
q
qNMRR
Q
ANMRR
1
)(
1 .  
 
Fig. 4. A query example: (a) partial frames of the 
input query, (b) and (c) are the top 12 results 
using the cube-based video retrieval and, 
respectively. 
A higher value of AR implies a better retrieval 
performance. On the other hand, a lower value of 
ANMRR implies a higher retrieval rate, with the 
relevant items ranked at the top positions. Table 1 
shows the performance comparison between the 
proposed method and TMOF in terms of AR and 
ANMRR. Notice that the queries contain object 
motion or luminance variation among video 
frames will decrease the performance of TMOF. 
On the contrary, the proposed scheme sustains 
good performance under the same query 
surveillance video content modeling, Pattern 
Recognition, 41 (7) (2008) 2309-2326. 
[15] A. Briassouli and N. Ahujia, Extraction and 
analysis of multiple periodic motions in video 
sequences, IEEE Trans. Pattern Anal. Mach. Intell. 
29 (7) (2007) 1244-1251. 
[16] T. Goodhart, P. Yan, and M. Shah, Action 
recognition using spatial-temporal requlaity based 
features, in Proc. IEEE International Conference 
on Accoustics, Speech and Signal Processing 
(ISASSP 2008) (2008) 745-748. 
[17] J.-W. Hsieh, Y.-T. Hsu, H.Y.M. Liao, and C. 
C. Chen, Video-based human movement analysis 
and its application to surveillance systems, IEEE 
Trnas. Multimedia, 10 (3) (2008) 372-384. 
[18] X. Ma, F. Bashir, A.A. Khokhar, and D.  
Schonfeld, Event analysis based on multiple 
interactive motion trajectories, IEEE Trans. 
Circuits and Systems for Video technology, 19 (3) 
(2009) 397-406. 
[19] T. Liu and J. R. Kender, Computational 
approaches to temporal sampling of video 
sequences, ACM Transactions on Multimedia 
Computing, Communications, and 
Applications (TOMCCAP),  3 (2) (2007)  1-23. 
[20] A. Anjulan and N. Canagaraiah, A unified 
framework for object retrieval and mining, IEEE 
Trans. Circuits and Systems for Video technology, 
19 (1) (2009) 63-76. 
[21] K.-H. Liu, M.-F. Weng, C.-Y. Tseng, Y.-Y. 
Chuang, and M.-S. Chen, Association and temporal 
rule mining for post-filtering of semantic concept 
detection in video, IEEE Trnas. Multimedia, 10 (2) 
(2008) 240-251. 
[22] S. Winkler, Issues in vision modeling for 
perceptual video quality assessment, Signal 
Processing, 78 (1999) 231-252.  
[23] W.-H. Tsai, Moment preserving thresholding: 
A new approach, Comput. Vis., Graph., Image 
Process., (1984) 377-393. 
[24] S.-C. Cheng and T.-L.Wu, Subpixel edge 
detection of color images using 
moment-preserving technique and principal axis 
analysis, Pattern Recognition, 38 (4) (2006) 
527-537. 
[25] S. C. Pei and C. M. Cheng, Color image 
processing by using binary 
quaternion-moment-preserving thresholding 
technique, IEEE Trans. Image Process.,  8 (1999) 
614-628. 
[26] S.-C. Cheng, Content-based image retrieval 
using moment-preserving edge detection, Image 
and Vision Computing, 21 (9) (2003) 809-826. 
[27] W.-K. Huang, C.-H. Chuang, S.-C. Cheng, 
and J.-W. Hsieh, A fast cube-based video shot 
retrieval using 3D moment-preserving technique, 
in Proc. IEEE International Conference on Image 
Processing (ICIP), (2009).  
[28] K. Yan, Volumetric Features for Video Event 
Detection, Ph. D. Thesis, CMU-CS-08-113, 
Carnegie Mellon University, USA, March 2008
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
         98 年  11  月  15  日 
報告人姓名 鄭錫齊 服務機構 
及職稱 
國立台灣海洋大學教授 
會議期間及地點 2009/11/7~2009/11/11 
埃及開羅 
本會核定 
補助文號 
NSC 98-2221-E-019 -036 
-MY2 
會議名稱 (中文)2009 年 IEEE 國際影像處理處研討會 
(英文)2009 IEEE International Conference of Image Processing 
(ICIP  2009) 
發表論文題目 (中文)基於 3 維矩量保持技術之快速立方體為主的視訊片段檢
索 
(英文) A Fast Cube-Based Video Shot Retrieval Using 3D 
Moment-Preserving Technique 
報告內容： 
一、參加會議經過 
2009年IEEE國際影像處理研討會於98年11月7日至11日在埃及開羅舉行，大會舉辦單
位為美國亞利桑那大學，區域協辦單位為埃及開羅大學。IEEE國際影像處理研討會已經
舉行多年，前幾屆分別在全世界各地舉辦，舉凡美國、義大利、西班牙、新加坡等全球
重要國家均曾舉辦過，本屆由埃及開羅大學承辦，會議大會收到來自超過64個不同國家
超過2442篇投稿論文，其中包含Special Sessions共有1101投稿論文被接受參與盛會，
論文錄取率大約4成，ICIP提供一全世界影像處理專家共同討論最新的技術發展趨勢的
重要平台，來自全世界各知名大學超過600個學者，齊聚一堂共同討論影像處理的最新
技術。 
今年度本研討會著重在電腦影像、視訊訊號處理及視覺壓縮與通訊等相關領域之先
進研究討論，議程概分為 7 個 special sections, 23 個 lecture sections,及 70 個
poster sections,廣泛就影像與視訊處理技術進行討論。除此之外，大會邀請 Dr. Zahi 
Hawass, Secretary General of the Supreme Council of Antiquities Carol, Egypt
探討影像處理技術如何幫助進行古埃及文明考古研究，另外大會也邀請 Prof. Takeo 
Kanade, Robotics Institute, Carnegie Mellon University,USA 進行專題演講，探
討顯微影像技術於細胞發展追蹤之最新研究趨勢與方向。 
個人多次參加此類型的國際研討會，在 5月 10日收到論文接受信後，即開始規劃參
加研討會事宜。本人與博士班學生莊季翰，於 11月 5日晚上自桃園國際機場塔乘中華
航空公司飛機出發，直達泰國國際機場，轉搭埃及航空於當地 11月 6 日上午到達位於
埃及開羅，投宿大會配合之 Hilton旅館，隔天到大會會場 Grand Hyatt, Cario，隨即
Summarization。 
第五天(11 月 11日) 一早起床利用空檔時間撰寫出國報告，隨後準備離開埃及開國，
結束為期四天的研討會行程。 
 
三、考察參觀活動(無是項活動者省略) 
無 
四、建    議 
這個研討會集合的世界各國的研究學者聚集在一起，討論主題是影像與視訊處理的最
新技術發展，多年來ICIP聚集這方面全世界的傑出學者一起研討最新的研究議題，投稿
的學者來自全球64 個國家。影像及多媒體處理技術對於發展新一代的多媒體服務、數
位內容開發、影像顯示技術、及影像/視訊通訊系統扮演關鍵的角色，然而相關的研究
領域雖然已發展多年，但是世界各國仍積極地發展最新技術，台灣自然不能置身度外，
積極參與對提升台灣競爭力有極大幫助。國際研討會對擴展研究視野、避免閉門造車有
相當大的助益，研究學者應多多參與相關的國際研討會。 
首先感謝國科會補助教師出國參加研討會的機票及相關費用，然而補助經費確實不
足，如果國科會補助的額度能適度提高，讓老師或學生多一點參與專業議題的機會。相
信亦可拓展學生研究的深度及國際觀。 
五、攜回資料名稱及內容 
研討會行程表及研討會論文集之隨身碟。 
六、其    他 
無。 
xy
t
(a) (b)
… D
x
y
t
-1 1
-1
1
1
-1
h1
h2l
EJ
S
C
…
the retrieval accuracy without sacrificing the execution speed. 
Experimental results show the efficiency and effectiveness of the 
proposed video retrieval.
2. VIDEO CUBE ANALYSIS USING 3D MOMENT-
PRESERVING TECHNIQUE 
The cube-based approach to video retrieval described in this paper 
operates by estimating spatial-temporal features of a cube using 
the proposed 3D moment-preserving principle. In Fig. 1(a), an 
input video sequence is segmented into a number of 3D video 
cubes. Each of them is constructed by arranging each frame close 
to the other and forming a parallelepiped, where the first two 
dimensions are determined by the frame size and the third one is 
the time. Feature parameters of a cube describe the variation of the 
image content along the x-y plane and t-axis and are estimated by a 
3D moment-preserving feature detector. 
The voxel values of a video cube are classified into two classes 
by a 3D plane shown in Fig. 1(b). Based on the 3D plane model, 
the content of a video cube is approximated by six parameters, 
namely, two representative gray (color) values h1 and h2, plane 
translation l, and the 3D principal axis specified by three 
directional numbers (cosD, cosE  cosJ, where ED , and J are the 
angles between the principal axis and the axes x, y, and t,
respectively. The transition is approximated as simply a step 
transition from representative value h1 to representative value h2.
The translation l is defined as the length from the center of the 
video cube to the plane, and is confined within the range of -1 to 
+1. The directional numbers of the plane should satisfy the 
following relationship: 
Fig. 1. Creating video 3D cubes. (a) A given video shot is 
partitioned into a set of non-overlapping cubes. (b) A 3D step 
transition model in a cube C, which is inscribed by a sphere S.
Once the values of ED , and J are obtained, the solutions for h1 ,
h2 , p1, and p2 can be found by employing the moment-preserving 
technique [8]. The remaining unknown parameter l is simple to 
calculate from the fact that p2 = V2 / V, where V2 is the volume of 
S covered by the voxel value h2, V is the total area of S and is 
equal to 34S . The value of V2 can be computed by 
)2()1(
3
1 2
2 llV  S               (5) 
Then, we get 1coscoscos 222   JED .                  (1) 
0423 2
3   pll                   (6) 
Let  tyx ,,   be the coordinates of the center of gravity of the 
sphere S inscribed the cube C. Then 
The solutions of (6) are 
)cos(2 31 T l ,                           (7) 
),,(),,(
000 M
M
M
M
M
M
tyx tyx                   (2) )120cos(2 31 q Tl ,                 (8) 
)240cos(2 31
q Tl                (9) where M0 , Mx , My, and Mt are the mean, the x-mass moment, the 
y-mass moment, and the t-mass moment of S, respectively, and are 
computed by 
where 12cos 2  pT . The solution for l can be determined by 
imposing the constraint –1 < l < 1 on l.
»»
»»
»»
»»
¼
º
««
««
««
««
¬
ª
 
»»
»»
¼
º
««
««
¬
ª
³³³
³³³
³³³
³³³
S
S
S
S
t
y
x
dydxdttyxtf
dydxdttyxyf
dydxdttyxxf
dydxdttyxf
M
M
M
M
),,(
),,(
),,(
),,(
0                  (3) 
Based on the parameters of a video cube C, the voxels of C
can be quantized into two parts – one of them is represented as h1
and the other is represented as h2. The plane to characterize the 
transition of voxel values in the cube can be described as 
,coscoscos ltyx  uuu JED 1,,,1 dd ltyx .       (10) 
Based on the classification plane, we can project the voxels of the 
quantized video cube onto the xy, yt, and tx planes to obtain three 
projection blocks, shown in Fig. 2. Each block might contain a step 
transition to reflect the variation of pixel values within the 
corresponding group of frames. The line edges for three projection 
blocks can be characterized as 
where f(x,y,t) is the gray value and the color vector norm of (x,y,t)
voxel for a gray-level video cube and a color video cube, 
respectively. Notice that each coordinate of the 3D voxels is 
normalized to the interval (-1, 1). It has been found that the 
classification plane P is perpendicular to the principal axis from 
the origin to  tyx ,,   [7]. So, the normal 
)cos,cos,(cos JED n&  of P can be calculated by 
lyx c  TT sincos  (11)
where l’ and T  are defined as 
 
°°
°
¯
°°
°
®
­



 c



block  for )
coscos
cos(cos,
coscos
(
block for )
coscos
cos(cos,
coscos
(
block  for )
coscos
cos(cos,
coscos
(
,
22
1
22
22
1
22
22
1
22
txl
ytl
xyl
l
DJ
E
DJ
JE
J
JE
ED
D
ED
T.
cos
cos
cos
222
222
222
222
222
222
»»
»»
¼
º
««
««
¬
ª



 
»»
»»
¼
º
««
««
¬
ª



 
»»
»
¼
º
««
«
¬
ª
tyxt
tyxy
tyxx
MMMM
MMMM
MMMM
tyxt
tyxy
tyxx
J
E
D  (4) 
242
where and denote the number of correctly retrieved 
items in the top K retrievals and  the number of ground truth, 
respectively to answer a query q.
)(qnr )(qng
^ GTMqngK `uu 2),(4min ,
where . To obtain the ANMRR, the modified 
retrieval rank (MRR) should be computed first:
^ (max ng `)qGTM  
5.0
2
)(
)(
)()(
)(
1
 ¦
 
qng
qng
irqMRR
qng
i
  (18) 
where r(i) is either the rank of each of the ng(q) in the top K
retrievals or K + 1. The ANMRR is then calculated by 
¦
 
 
Q
q
qNMRR
Q
ANMRR
1
)(1 . (19) 
Fig. 4. A query example: (a) partial frames of the input query, 
(b) and (c) are the top 12 results using the cube-based video 
retrieval and, respectively. 
Table 1. Performance comparison between the proposed cube-
based video retrieval and TMOF in terms of AR and ANMRR. 
Cube-based 
video retrieval 
   TMOF Video
Category 
AR ANMRR AR ANMRR
Movie 0.9518 0.0932 0.9421 0.0974
News 0.8963 0.1857 0.5386 0.4594
Home video 0.9674 0.0597 0.8366 0.3077
Average   0.9385 0.1129 0.7724 0.2882
A higher value of AR implies a better retrieval performance. 
On the other hand, a lower value of ANMRR implies a higher 
retrieval rate, with the relevant items ranked at the top positions. 
Table 1 shows the performance comparison between the proposed 
method and TMOF in terms of AR and ANMRR. Notice that the 
queries contain object motion or luminance variation among video 
frames will decrease the performance of TMOF. On the contrary, 
the proposed scheme sustains good performance under the same 
query conditions. Accordingly, the proposed method outperforms 
TMOF. Fig. 4 illustrates a query example: part of a query shot is 
shown in Fig. 4(a); Figs. 4 (b) and (c) are the top 12 video shots 
using the proposed method and TMOF, respectively. Based on the 
query, we have 53 relevant video shots in the database, and AR = 
1.0 and ANMRR = 0.10967 for the cube-based video retrieval, and 
AR =  0.75472 and ANMRR = 0.47146 for TMOF. This 
demonstrates that the proposed cube-based video retrieval  
outperforms TMOF in terms of retrieval accuracy.  
Table 1. Performance comparison between the proposed cube-
based video retrieval and TMOF in terms of AR and ANMRR. 
5. CONCLUSIONS
In this paper, we present a fast cube-based video retrieval using the 
proposed 3D moment-preserving technique. The contributions of 
the approach includes: (1) a set of analytical formulas is derived to 
extract spatial-temporal features from video cubes; (2) a fast video 
shot matching is proposed based on the video cube model; (3) a 
quick-and-dirty approach for fast video retrieval is proposed. 
Experimental results show the efficiency and effectiveness of the 
proposed method. Future work will be on extending the size of the 
test database and designing a better indexing structure in order to 
achieve the goal of optimal video shot matching. 
REFERENCES
[1] C.-L. Huang and B.-Y. Liao, “A Robust Scene-Change 
Detection Method for Video Segmentation,” IEEE Trans. Circuits 
Syst. Video Technol., vol. 11, no. 12, pp. 1281–1288, Dec. 2001. 
[2] K.-W. Sze, K.-M. Lam, and G. Qiu, “A New Key Frame 
Representation for Video Segment Retrieval,” IEEE Trans. 
Circuits and Systems for Video Technology, vol. 15, no. 9, pp. 
1148–1155, Sept. 2005. 
[3] W. Ren, S. Singh, M. Singh, and Y. S. Zhu, “State-of-the-Art 
on Spatial-Temporal Information-Based Video Retrieval,” Pattern
Recognition, vol. 42, no. 2, pp. 267-282, 2009. 
[4] C.-W. Su, H.-Y. Mark Liao, H.-R. Tyan, C.-W. Lin, D.-Y. 
Chen, and K.-C. Fan, “Motion Flow-Based Video Retrieval,” IEEE
Trans. Multimedia, vol. 9, pp. 1193–1201, Oct. 2007. 
[5] A. J. T. Lee, R.-W. Hong, and M.-F. Chang, “An Approach to 
Content-Based Video Retrieval,” in Porc. IEEE International 
Conference on Multimedia and Expo (ICME), vol. 1, pp. 273–276, 
June. 2004. 
[6] Z. Min, “Key Frame Extraction from Scenery Video,” in Proc.
IEEE International Conference on Wavelet Analysis and Pattern 
Recognition (ICWAPR), vol. 2, pp. 540–543, Nov. 2007. 
[7] S.-C. Cheng and T.-L.Wu, “Subpixel Edge Detection of Color 
Images Using Moment-Preserving Technique and Principal Axis 
Analysis,” Pattern Recognition, vol. 38, no. 4, pp. 527-537, 2006. 
[8] W.-H. Tsai, “Moment Preserving Thresholding: A New 
Approach,” Comput. Vis., Graph., Image Process.,: pp. 377-393,  
1984.
[9] Y. Ke, R. Sukthankar, and M. Hebert, “Efficient Visual Event 
Detection Using Volumetric Features,” in Proc. ICCV 2005.ʳ
244
 2 
配合的因素，必須提早於 8月 25日晚上就搭機離開土耳其，錯過最會一天的議程。 
二、與會心得 
第 20 屆 IAPR 國際圖型識別研討會是國際圖型識別學會(International Association for Pattern 
Recognition, IAPR)所協辦，自 1970年起舉辦在智慧型訊號處理暨圖型識別領域中就理論發展及應用最
具聲望且歷史悠久的研討會之一。 
第一天(8月 23 日)註冊報到，大會開幕，展開隨即參加歡迎晚會，利用這個機會與與會各國專家學
者討論智慧型訊號處理暨圖型識別的最新研究趨勢與方向。除了一整天的議程外，當天大會也安排 Prof. 
Horst Bunke 進行 
K.S. Fu Prize Lecture: Towards the Unification of Structural and Statistical Pattern Recognition: 
Abstract: Statistical pattern recognition is characterized by the use of feature vectors for pattern representation, 
while the structural approach is based on symbolic data structures, such as strings, trees, and graphs. Clearly, 
symbolic data structures have a higher representational power than feature vectors because they allows one to 
directly model relationships that may exist between the individual parts of a pattern. However, many 
operations that are needed in classification, clustering, and other pattern recognition tasks are not defined for 
graphs. Consequently, there has been a lack of algorithmic tools in the domain of structural pattern 
recognition since its beginning. This talk gives an overview of the development of the field of structural 
pattern recognition and shows various attempts to bridge the gap between statistical and structural pattern 
recognition, i.e. to make algorithmic tools originally developed for feature vectors applicable to symbolic data 
structures. 
   第二天(8 月 24 日)一早大會就利用 9 個會議廳平行進行各個主題的論文發表與技術研討，同時也
邀請到電腦視覺界的大師 Professor Chris Bishop, Microsoft Research Cambridge, UK 進行專題演講，題
目是”Embracing Uncertainty: The New Machine Intelligence,”一席的演講勾勒出在結合圖形理論與圖
樣識別技術，機器智慧領域之新研究的議題及方向。第二天我也參加了 Section TuAT3:Image Analysis –III,
由 Section chair: Kittler, Josef (Univ. of Surrey)主持，發表的論文題目如下: 1.Canonical Image Selection by 
Visual Context Learning;2.Exposing Digital Image Foegeries by using Canonical Correlation Analysis – 
China; 3. Adding Affine Invariant Geometric Constraint for Partial-Duplicate Image Retrieval; 
4.Outlier-Resistant Dissimilarity Measure for Feature-Based image Matching; 5.The University of Surrey 
Visual Concept Detection System at Image –CLEF@ICPR: Working Notes。 
第三天(8月 25 日) 一早大會就利用 9個會議廳平行進行各個主題的論文發表與技術研討，同時也
邀請到電腦視覺界的大師 Shree K. Nayar,Columbia University, USA 進行專題演講，題目是 
Computational Cameras: Redefining the Images 
Abstract: The computational camera embodies the convergence of the camera and the computer. It uses new 
optics to select rays from the scene in unusual ways, and an appropriate algorithm to process the selected rays. 
This ability to manipulate images before they are recorded and process the recorded images before they are 
presented is a powerful one. It enables us to experience our visual world in rich and compelling ways. 
第四天(8月 26 日)搭機回到台灣，結束多天的行程。 
三、考察參觀活動(無是項活動者省略) 
無 
四、建    議 
Human Smoking Event Detection Using Visual Interaction Clues 
Pin Wu1, Jun-Wei Hsieh2*, Jiun-Cheng Cheng1, Shyi-Chyi Cheng2, and Shau-Yin Tseng3 
1Dep. of Electrical Engineering 2Dep. of Computer Science and Eng. 3Inform. and Com. Res. Lab. 
Yuan-Ze University National Taiwan Ocean University Industry Tech. Res. Institute 
135 Yuan-Tung Road, Chung-Li, Taiwan 2 Pei-Ning Road, Keelung ,Taiwan 
*shieh@ntou.edu.tw 
Chutung, Hsinchu, Taiwan 
tseng@itri.org.tw 
 
Abstract 
This paper presents a novel scheme to automatically 
and directly detect smoking events in video. In this 
scheme, a color-based ratio histogram analysis is 
introduced to extract the visual clues from appearance 
interactions between lighted cigarette and its human 
holder. The techniques of color re-projection and 
Gaussian Mixture Models (GMMs) enable the tasks of 
cigarette segmentation and tracking over the 
background pixels. Then, a key problem for event 
analysis is the non-regular form of smoking events. 
Thus, we propose a self-determined mechanism to 
analyze this suspicious event using HHM framework. 
Due to the uncertainties of cigarette size and color, 
there is no automatic system which can well analyze 
human smoking events directly from videos. The 
proposed scheme is compatible to detect the smoking 
events of uncertain actions with various cigarette sizes, 
colors, and shapes, and has capacity to extend visual 
analysis to human events of similar interaction 
relationship. Experimental results show the 
effectiveness and real-time performances of our scheme 
in smoking event analysis. 
 
1. Introduction 
 
Smoking is one of long-history human events in 
common activities, and noticeable for many modern 
aspects, such as fire prevention and public safety, 
environmental and health effects, behavior studies, and 
so on. Comprising the varying visual appearance of 
smoker and smoke, and the action sequences based on 
non-uniform individual habits, human smoking event 
detection becomes a challengeful problem for vision-
based analysis.  
Detecting specific human events by certain patterns 
automatically is the goal of intelligent vision systems, 
widely in many applicaiton aspects[1]-[3]. Recently, in 
[4]-[7], the use of motion and functionality relations 
between human and objects is introduced for analyzing 
human actions, and enables the spatial trajectories of 
the objects as the key roles in event recognition 
processes.  State-of-the-art visual tracking techniques 
have shown the adaptive mechanisms against dynamic 
changes in appearance, however its trajectory 
information has the upper bound for complex human 
event analysis. To detect specific human events, one 
traditoinal method is to build the predefind general 
model of appearance or shape features for representting 
this object against dynamic conditions. For example, 
Spengler [8] used a blob-based object detection 
technique to identify abandoned objects, and then 
proposed a Bayesian multi-people tracker to track these 
objects.  Tian et al. [9] used the mixture of Gaussian 
models to model background and then extracted 
moving objects from backgrounds. In addition, 
Haritaoglu et al. [10] presented a silhouette-based 
approach to analyze the symmetry of body shape for 
determining whether a person carrys an object, e.g., 
backpack.  However, silhouette feature is ambiguous 
in many cases for detecting other objects (such as 
handbags slung over shoulder).  For the smoking 
event analysis, the uncertainties in cigarette size, 
orienation, and colors make it very challenging if 
without any human efforts.  In this paper, a novel 
histogram-based method will be proposed to detect 
different kinds of cigiratee object and then analyze their 
related smoking events directly from videos. 
 
2. Overview 
 
 
Fig. 1: Flowchart of our proposed scheme. 
This proposed method consists of two stages: event 
cue detection and event classification. The cue stage is 
a self-determination scheme to extract cigarettes and 
human hand motions, and then nominates time point 
candidates for smoking event analysis.  For 
recognizing the event characteristcs of intentional 
actions, we introduce three conceptual terms into our 
system, i.e., human, object, and smoke, to represent the 
state of interaction clues.  As shown in Fig. 1, this 
paper first uses a background subtraction technique to 
extract the regions of motion.  In addition, we 
examine the motion energy on each pixel to extract 
smoke regions from the result of background 
subtracition.  Then, the face detector [13] is applied 
for exploring face regions from videos.  Based on the 
obtained face, a ratio histogram analysis is then applied 
for highlighting all possible cigarette candidates.  
Once in support of the cigarette and smoke reigons 
2010 International Conference on Pattern Recognition
1051-4651/10 $26.00 © 2010 IEEE
DOI 10.1109/ICPR.2010.1056
43285244
( , )( , ) e C x yG x y ε−= ,            (7) 
where Cε  denotes the color distance between a pixel 
(x, y) and the color mean Cμ  of f , i.e.,  
( ) ( )1( , ) ( , ) - ( , ) -TC C C R C Cx y I x y I x yε μ μ−= Σ ,. 
Here, ( , )CI x y  represents the color vector of the pixel 
(x, y) and RΣ  is the color variance of f . ( , )G x y  is 
a skin model and can be used for hand detection.  Fig. 
3 shows a result example of hand region detection.   
 
4. Smoking Event Classification 
  
(a) H           (b) X           (c) S 
Fig. 4: State transitions of three observations H, 
X, and S in the smoking event analysis. 
  
1H
1C
1X 1S 2H
2C
2X 2S nH
nC
nX nS
 
Fig. 5: The visual signatures of cigarette smoking 
event in detection scope (left) and their graphical 
presentation as spatial nodes on each primitive (right). 
In the smoking event analysis, we focus on its event 
nature of three visible signatures: hand position 
(distance to face), cigarette (event property), and smoke 
(visual suspension) in the probability model machines 
of interaction relationship.  As illustrated in Fig. 4, 
where the state of hand, H = {body:−1, face:+1},  in 
terms of hand position, the state of cigarette, X = 
{(approaching: −1, leaving:+1)},  in terms of distance 
to face; the state of smoke, S = {firming: −1, dispersing: 
+1}  as visual suspension of smoke.  A smoking 
event E is defined as a temporally ordered set of frame 
primitives { }1,..., ,...,t nC C C .  The graphical model for 
this event E is illustrated in Fig. 5. A frame primitive 
tC  is associated with a hand node tH  for the hand 
status, a cigarette node tX  for the state of cigarrete, 
and the smoke node tS  for the status of smoke region.  
The primitive tC  in E follows a Markovian model, so 
that the probability of E under the observation 
{( , , )}O H X S=  can be presented as:  
1
2 1
( | ) ( | ) ( | ) ( | )
( | ) ( | ) ( | ) ( | ).
n n
t t t t t t t t
t t
P O E P H E P X E P S E
P C C P H C P X C P S C
−
= =
≅
= ∏ ∏ (8) 
Here, 1( | )t tP C C −  is the transition probability from 
primitive tC  to 1tC − . ( | )t tP H C , ( | )t tP X C , and 
( | )t tP S C  denote a hand likelihood model, a cigarette 
likelihood model, and a smoke likelihood model for 
tC , respectively. Then, the probability of an 
observation O belonging to an event E can be defined 
as follows: 
( | ) ( ) ( | )P E O P E P O E≅ , 
where ( )P E  is the prior probability.  Let Ω  denote 
the event space.  Then, the event analyzer to analyze 
O can be formulated an optimization problem as 
follows: 
arg max ( | )optimal EE P E O∈Ω= .   (9) 
When Ω  is known, Eq.(9) can be well solved using 
the famous Viterbi algorithm [14].  
 
5. Experimental Result 
To analyze the performance of our proposed approach, 
a real-time system to analyze different abnormal events 
at face zones from different views was implemented. 
To analyze the efficiency and effectiveness of the 
proposed approach, we created a large database under 
different scenes and lighting conditions. The dimension 
of each frame is 360 240× . The frame of our system is 
faster than 25 fps. 
Fig. 6 shows three results of cigarette detection under 
different lighting conditions.  The environment in (a) 
indoor and the one in (b) is outdoor.  In (c), even 
though the head orientation is slanted, the proposed 
method still works with correct results.  Table 1 
presents the accuracy rate of cigarette detection, where 
the total number of analyzed frames is 2196, and the 
average accuracy is about 93.2%.  Fig. 7 shows the 
result of smoking event analysis.  (b) lists all the 
probability of event voting.  Even though the outdoor 
scene is analyzed, our method still works well to detect 
this abnormal event.  Table 2 lists the accuracy 
analysis of smoking event detection.  Fig. 8 shows 
other examples of smoking event analysis at different 
places ((a)-(d)). The overall average accuracy of 
smoking event analysis is about 82.1%.  
     
(a)                  (b) 
 
(c) 
Fig. 6: Results of cigarette detection. 
43305446
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
         98 年  11  月  15  日 
報告人姓名 鄭錫齊 服務機構 
及職稱 
國立台灣海洋大學教授 
會議期間及地點 2009/11/7~2009/11/11 
埃及開羅 
本會核定 
補助文號 
NSC 98-2221-E-019 -036 
-MY2 
會議名稱 (中文)2009 年 IEEE 國際影像處理處研討會 
(英文)2009 IEEE International Conference of Image Processing 
(ICIP  2009) 
發表論文題目 (中文)基於 3 維矩量保持技術之快速立方體為主的視訊片段檢
索 
(英文) A Fast Cube-Based Video Shot Retrieval Using 3D 
Moment-Preserving Technique 
報告內容： 
一、參加會議經過 
2009年IEEE國際影像處理研討會於98年11月7日至11日在埃及開羅舉行，大會舉辦單
位為美國亞利桑那大學，區域協辦單位為埃及開羅大學。IEEE國際影像處理研討會已經
舉行多年，前幾屆分別在全世界各地舉辦，舉凡美國、義大利、西班牙、新加坡等全球
重要國家均曾舉辦過，本屆由埃及開羅大學承辦，會議大會收到來自超過64個不同國家
超過2442篇投稿論文，其中包含Special Sessions共有1101投稿論文被接受參與盛會，
論文錄取率大約4成，ICIP提供一全世界影像處理專家共同討論最新的技術發展趨勢的
重要平台，來自全世界各知名大學超過600個學者，齊聚一堂共同討論影像處理的最新
技術。 
今年度本研討會著重在電腦影像、視訊訊號處理及視覺壓縮與通訊等相關領域之先
進研究討論，議程概分為 7 個 special sections, 23 個 lecture sections,及 70 個
poster sections,廣泛就影像與視訊處理技術進行討論。除此之外，大會邀請 Dr. Zahi 
Hawass, Secretary General of the Supreme Council of Antiquities Carol, Egypt
探討影像處理技術如何幫助進行古埃及文明考古研究，另外大會也邀請 Prof. Takeo 
Kanade, Robotics Institute, Carnegie Mellon University,USA 進行專題演講，探
討顯微影像技術於細胞發展追蹤之最新研究趨勢與方向。 
個人多次參加此類型的國際研討會，在 5月 10日收到論文接受信後，即開始規劃參
加研討會事宜。本人與博士班學生莊季翰，於 11月 5日晚上自桃園國際機場塔乘中華
航空公司飛機出發，直達泰國國際機場，轉搭埃及航空於當地 11月 6 日上午到達位於
埃及開羅，投宿大會配合之 Hilton旅館，隔天到大會會場 Grand Hyatt, Cario，隨即
Summarization。 
第五天(11 月 11日) 一早起床利用空檔時間撰寫出國報告，隨後準備離開埃及開國，
結束為期四天的研討會行程。 
 
三、考察參觀活動(無是項活動者省略) 
無 
四、建    議 
這個研討會集合的世界各國的研究學者聚集在一起，討論主題是影像與視訊處理的最
新技術發展，多年來ICIP聚集這方面全世界的傑出學者一起研討最新的研究議題，投稿
的學者來自全球64 個國家。影像及多媒體處理技術對於發展新一代的多媒體服務、數
位內容開發、影像顯示技術、及影像/視訊通訊系統扮演關鍵的角色，然而相關的研究
領域雖然已發展多年，但是世界各國仍積極地發展最新技術，台灣自然不能置身度外，
積極參與對提升台灣競爭力有極大幫助。國際研討會對擴展研究視野、避免閉門造車有
相當大的助益，研究學者應多多參與相關的國際研討會。 
首先感謝國科會補助教師出國參加研討會的機票及相關費用，然而補助經費確實不
足，如果國科會補助的額度能適度提高，讓老師或學生多一點參與專業議題的機會。相
信亦可拓展學生研究的深度及國際觀。 
五、攜回資料名稱及內容 
研討會行程表及研討會論文集之隨身碟。 
六、其    他 
無。 
xy
t
(a) (b)
… D
x
y
t
-1 1
-1
1
1
-1
h1
h2l
EJ
S
C
…
the retrieval accuracy without sacrificing the execution speed. 
Experimental results show the efficiency and effectiveness of the 
proposed video retrieval.
2. VIDEO CUBE ANALYSIS USING 3D MOMENT-
PRESERVING TECHNIQUE 
The cube-based approach to video retrieval described in this paper 
operates by estimating spatial-temporal features of a cube using 
the proposed 3D moment-preserving principle. In Fig. 1(a), an 
input video sequence is segmented into a number of 3D video 
cubes. Each of them is constructed by arranging each frame close 
to the other and forming a parallelepiped, where the first two 
dimensions are determined by the frame size and the third one is 
the time. Feature parameters of a cube describe the variation of the 
image content along the x-y plane and t-axis and are estimated by a 
3D moment-preserving feature detector. 
The voxel values of a video cube are classified into two classes 
by a 3D plane shown in Fig. 1(b). Based on the 3D plane model, 
the content of a video cube is approximated by six parameters, 
namely, two representative gray (color) values h1 and h2, plane 
translation l, and the 3D principal axis specified by three 
directional numbers (cosD, cosE  cosJ, where ED , and J are the 
angles between the principal axis and the axes x, y, and t,
respectively. The transition is approximated as simply a step 
transition from representative value h1 to representative value h2.
The translation l is defined as the length from the center of the 
video cube to the plane, and is confined within the range of -1 to 
+1. The directional numbers of the plane should satisfy the 
following relationship: 
Fig. 1. Creating video 3D cubes. (a) A given video shot is 
partitioned into a set of non-overlapping cubes. (b) A 3D step 
transition model in a cube C, which is inscribed by a sphere S.
Once the values of ED , and J are obtained, the solutions for h1 ,
h2 , p1, and p2 can be found by employing the moment-preserving 
technique [8]. The remaining unknown parameter l is simple to 
calculate from the fact that p2 = V2 / V, where V2 is the volume of 
S covered by the voxel value h2, V is the total area of S and is 
equal to 34S . The value of V2 can be computed by 
)2()1(
3
1 2
2 llV  S               (5) 
Then, we get 1coscoscos 222   JED .                  (1) 
0423 2
3   pll                   (6) 
Let  tyx ,,   be the coordinates of the center of gravity of the 
sphere S inscribed the cube C. Then 
The solutions of (6) are 
)cos(2 31 T l ,                           (7) 
),,(),,(
000 M
M
M
M
M
M
tyx tyx                   (2) )120cos(2 31 q Tl ,                 (8) 
)240cos(2 31
q Tl                (9) where M0 , Mx , My, and Mt are the mean, the x-mass moment, the 
y-mass moment, and the t-mass moment of S, respectively, and are 
computed by 
where 12cos 2  pT . The solution for l can be determined by 
imposing the constraint –1 < l < 1 on l.
»»
»»
»»
»»
¼
º
««
««
««
««
¬
ª
 
»»
»»
¼
º
««
««
¬
ª
³³³
³³³
³³³
³³³
S
S
S
S
t
y
x
dydxdttyxtf
dydxdttyxyf
dydxdttyxxf
dydxdttyxf
M
M
M
M
),,(
),,(
),,(
),,(
0                  (3) 
Based on the parameters of a video cube C, the voxels of C
can be quantized into two parts – one of them is represented as h1
and the other is represented as h2. The plane to characterize the 
transition of voxel values in the cube can be described as 
,coscoscos ltyx  uuu JED 1,,,1 dd ltyx .       (10) 
Based on the classification plane, we can project the voxels of the 
quantized video cube onto the xy, yt, and tx planes to obtain three 
projection blocks, shown in Fig. 2. Each block might contain a step 
transition to reflect the variation of pixel values within the 
corresponding group of frames. The line edges for three projection 
blocks can be characterized as 
where f(x,y,t) is the gray value and the color vector norm of (x,y,t)
voxel for a gray-level video cube and a color video cube, 
respectively. Notice that each coordinate of the 3D voxels is 
normalized to the interval (-1, 1). It has been found that the 
classification plane P is perpendicular to the principal axis from 
the origin to  tyx ,,   [7]. So, the normal 
)cos,cos,(cos JED n&  of P can be calculated by 
lyx c  TT sincos  (11)
where l’ and T  are defined as 
 
°°
°
¯
°°
°
®
­



 c



block  for )
coscos
cos(cos,
coscos
(
block for )
coscos
cos(cos,
coscos
(
block  for )
coscos
cos(cos,
coscos
(
,
22
1
22
22
1
22
22
1
22
txl
ytl
xyl
l
DJ
E
DJ
JE
J
JE
ED
D
ED
T.
cos
cos
cos
222
222
222
222
222
222
»»
»»
¼
º
««
««
¬
ª



 
»»
»»
¼
º
««
««
¬
ª



 
»»
»
¼
º
««
«
¬
ª
tyxt
tyxy
tyxx
MMMM
MMMM
MMMM
tyxt
tyxy
tyxx
J
E
D  (4) 
242
where and denote the number of correctly retrieved 
items in the top K retrievals and  the number of ground truth, 
respectively to answer a query q.
)(qnr )(qng
^ GTMqngK `uu 2),(4min ,
where . To obtain the ANMRR, the modified 
retrieval rank (MRR) should be computed first:
^ (max ng `)qGTM  
5.0
2
)(
)(
)()(
)(
1
 ¦
 
qng
qng
irqMRR
qng
i
  (18) 
where r(i) is either the rank of each of the ng(q) in the top K
retrievals or K + 1. The ANMRR is then calculated by 
¦
 
 
Q
q
qNMRR
Q
ANMRR
1
)(1 . (19) 
Fig. 4. A query example: (a) partial frames of the input query, 
(b) and (c) are the top 12 results using the cube-based video 
retrieval and, respectively. 
Table 1. Performance comparison between the proposed cube-
based video retrieval and TMOF in terms of AR and ANMRR. 
Cube-based 
video retrieval 
   TMOF Video
Category 
AR ANMRR AR ANMRR
Movie 0.9518 0.0932 0.9421 0.0974
News 0.8963 0.1857 0.5386 0.4594
Home video 0.9674 0.0597 0.8366 0.3077
Average   0.9385 0.1129 0.7724 0.2882
A higher value of AR implies a better retrieval performance. 
On the other hand, a lower value of ANMRR implies a higher 
retrieval rate, with the relevant items ranked at the top positions. 
Table 1 shows the performance comparison between the proposed 
method and TMOF in terms of AR and ANMRR. Notice that the 
queries contain object motion or luminance variation among video 
frames will decrease the performance of TMOF. On the contrary, 
the proposed scheme sustains good performance under the same 
query conditions. Accordingly, the proposed method outperforms 
TMOF. Fig. 4 illustrates a query example: part of a query shot is 
shown in Fig. 4(a); Figs. 4 (b) and (c) are the top 12 video shots 
using the proposed method and TMOF, respectively. Based on the 
query, we have 53 relevant video shots in the database, and AR = 
1.0 and ANMRR = 0.10967 for the cube-based video retrieval, and 
AR =  0.75472 and ANMRR = 0.47146 for TMOF. This 
demonstrates that the proposed cube-based video retrieval  
outperforms TMOF in terms of retrieval accuracy.  
Table 1. Performance comparison between the proposed cube-
based video retrieval and TMOF in terms of AR and ANMRR. 
5. CONCLUSIONS
In this paper, we present a fast cube-based video retrieval using the 
proposed 3D moment-preserving technique. The contributions of 
the approach includes: (1) a set of analytical formulas is derived to 
extract spatial-temporal features from video cubes; (2) a fast video 
shot matching is proposed based on the video cube model; (3) a 
quick-and-dirty approach for fast video retrieval is proposed. 
Experimental results show the efficiency and effectiveness of the 
proposed method. Future work will be on extending the size of the 
test database and designing a better indexing structure in order to 
achieve the goal of optimal video shot matching. 
REFERENCES
[1] C.-L. Huang and B.-Y. Liao, “A Robust Scene-Change 
Detection Method for Video Segmentation,” IEEE Trans. Circuits 
Syst. Video Technol., vol. 11, no. 12, pp. 1281–1288, Dec. 2001. 
[2] K.-W. Sze, K.-M. Lam, and G. Qiu, “A New Key Frame 
Representation for Video Segment Retrieval,” IEEE Trans. 
Circuits and Systems for Video Technology, vol. 15, no. 9, pp. 
1148–1155, Sept. 2005. 
[3] W. Ren, S. Singh, M. Singh, and Y. S. Zhu, “State-of-the-Art 
on Spatial-Temporal Information-Based Video Retrieval,” Pattern
Recognition, vol. 42, no. 2, pp. 267-282, 2009. 
[4] C.-W. Su, H.-Y. Mark Liao, H.-R. Tyan, C.-W. Lin, D.-Y. 
Chen, and K.-C. Fan, “Motion Flow-Based Video Retrieval,” IEEE
Trans. Multimedia, vol. 9, pp. 1193–1201, Oct. 2007. 
[5] A. J. T. Lee, R.-W. Hong, and M.-F. Chang, “An Approach to 
Content-Based Video Retrieval,” in Porc. IEEE International 
Conference on Multimedia and Expo (ICME), vol. 1, pp. 273–276, 
June. 2004. 
[6] Z. Min, “Key Frame Extraction from Scenery Video,” in Proc.
IEEE International Conference on Wavelet Analysis and Pattern 
Recognition (ICWAPR), vol. 2, pp. 540–543, Nov. 2007. 
[7] S.-C. Cheng and T.-L.Wu, “Subpixel Edge Detection of Color 
Images Using Moment-Preserving Technique and Principal Axis 
Analysis,” Pattern Recognition, vol. 38, no. 4, pp. 527-537, 2006. 
[8] W.-H. Tsai, “Moment Preserving Thresholding: A New 
Approach,” Comput. Vis., Graph., Image Process.,: pp. 377-393,  
1984.
[9] Y. Ke, R. Sukthankar, and M. Hebert, “Efficient Visual Event 
Detection Using Volumetric Features,” in Proc. ICCV 2005.ʳ
244
 2 
配合的因素，必須提早於 8月 25日晚上就搭機離開土耳其，錯過最會一天的議程。 
二、與會心得 
第 20 屆 IAPR 國際圖型識別研討會是國際圖型識別學會(International Association for Pattern 
Recognition, IAPR)所協辦，自 1970年起舉辦在智慧型訊號處理暨圖型識別領域中就理論發展及應用最
具聲望且歷史悠久的研討會之一。 
第一天(8月 23 日)註冊報到，大會開幕，展開隨即參加歡迎晚會，利用這個機會與與會各國專家學
者討論智慧型訊號處理暨圖型識別的最新研究趨勢與方向。除了一整天的議程外，當天大會也安排 Prof. 
Horst Bunke 進行 
K.S. Fu Prize Lecture: Towards the Unification of Structural and Statistical Pattern Recognition: 
Abstract: Statistical pattern recognition is characterized by the use of feature vectors for pattern representation, 
while the structural approach is based on symbolic data structures, such as strings, trees, and graphs. Clearly, 
symbolic data structures have a higher representational power than feature vectors because they allows one to 
directly model relationships that may exist between the individual parts of a pattern. However, many 
operations that are needed in classification, clustering, and other pattern recognition tasks are not defined for 
graphs. Consequently, there has been a lack of algorithmic tools in the domain of structural pattern 
recognition since its beginning. This talk gives an overview of the development of the field of structural 
pattern recognition and shows various attempts to bridge the gap between statistical and structural pattern 
recognition, i.e. to make algorithmic tools originally developed for feature vectors applicable to symbolic data 
structures. 
   第二天(8 月 24 日)一早大會就利用 9 個會議廳平行進行各個主題的論文發表與技術研討，同時也
邀請到電腦視覺界的大師 Professor Chris Bishop, Microsoft Research Cambridge, UK 進行專題演講，題
目是”Embracing Uncertainty: The New Machine Intelligence,”一席的演講勾勒出在結合圖形理論與圖
樣識別技術，機器智慧領域之新研究的議題及方向。第二天我也參加了 Section TuAT3:Image Analysis –III,
由 Section chair: Kittler, Josef (Univ. of Surrey)主持，發表的論文題目如下: 1.Canonical Image Selection by 
Visual Context Learning;2.Exposing Digital Image Foegeries by using Canonical Correlation Analysis – 
China; 3. Adding Affine Invariant Geometric Constraint for Partial-Duplicate Image Retrieval; 
4.Outlier-Resistant Dissimilarity Measure for Feature-Based image Matching; 5.The University of Surrey 
Visual Concept Detection System at Image –CLEF@ICPR: Working Notes。 
第三天(8月 25 日) 一早大會就利用 9個會議廳平行進行各個主題的論文發表與技術研討，同時也
邀請到電腦視覺界的大師 Shree K. Nayar,Columbia University, USA 進行專題演講，題目是 
Computational Cameras: Redefining the Images 
Abstract: The computational camera embodies the convergence of the camera and the computer. It uses new 
optics to select rays from the scene in unusual ways, and an appropriate algorithm to process the selected rays. 
This ability to manipulate images before they are recorded and process the recorded images before they are 
presented is a powerful one. It enables us to experience our visual world in rich and compelling ways. 
第四天(8月 26 日)搭機回到台灣，結束多天的行程。 
三、考察參觀活動(無是項活動者省略) 
無 
四、建    議 
Human Smoking Event Detection Using Visual Interaction Clues 
Pin Wu1, Jun-Wei Hsieh2*, Jiun-Cheng Cheng1, Shyi-Chyi Cheng2, and Shau-Yin Tseng3 
1Dep. of Electrical Engineering 2Dep. of Computer Science and Eng. 3Inform. and Com. Res. Lab. 
Yuan-Ze University National Taiwan Ocean University Industry Tech. Res. Institute 
135 Yuan-Tung Road, Chung-Li, Taiwan 2 Pei-Ning Road, Keelung ,Taiwan 
*shieh@ntou.edu.tw 
Chutung, Hsinchu, Taiwan 
tseng@itri.org.tw 
 
Abstract 
This paper presents a novel scheme to automatically 
and directly detect smoking events in video. In this 
scheme, a color-based ratio histogram analysis is 
introduced to extract the visual clues from appearance 
interactions between lighted cigarette and its human 
holder. The techniques of color re-projection and 
Gaussian Mixture Models (GMMs) enable the tasks of 
cigarette segmentation and tracking over the 
background pixels. Then, a key problem for event 
analysis is the non-regular form of smoking events. 
Thus, we propose a self-determined mechanism to 
analyze this suspicious event using HHM framework. 
Due to the uncertainties of cigarette size and color, 
there is no automatic system which can well analyze 
human smoking events directly from videos. The 
proposed scheme is compatible to detect the smoking 
events of uncertain actions with various cigarette sizes, 
colors, and shapes, and has capacity to extend visual 
analysis to human events of similar interaction 
relationship. Experimental results show the 
effectiveness and real-time performances of our scheme 
in smoking event analysis. 
 
1. Introduction 
 
Smoking is one of long-history human events in 
common activities, and noticeable for many modern 
aspects, such as fire prevention and public safety, 
environmental and health effects, behavior studies, and 
so on. Comprising the varying visual appearance of 
smoker and smoke, and the action sequences based on 
non-uniform individual habits, human smoking event 
detection becomes a challengeful problem for vision-
based analysis.  
Detecting specific human events by certain patterns 
automatically is the goal of intelligent vision systems, 
widely in many applicaiton aspects[1]-[3]. Recently, in 
[4]-[7], the use of motion and functionality relations 
between human and objects is introduced for analyzing 
human actions, and enables the spatial trajectories of 
the objects as the key roles in event recognition 
processes.  State-of-the-art visual tracking techniques 
have shown the adaptive mechanisms against dynamic 
changes in appearance, however its trajectory 
information has the upper bound for complex human 
event analysis. To detect specific human events, one 
traditoinal method is to build the predefind general 
model of appearance or shape features for representting 
this object against dynamic conditions. For example, 
Spengler [8] used a blob-based object detection 
technique to identify abandoned objects, and then 
proposed a Bayesian multi-people tracker to track these 
objects.  Tian et al. [9] used the mixture of Gaussian 
models to model background and then extracted 
moving objects from backgrounds. In addition, 
Haritaoglu et al. [10] presented a silhouette-based 
approach to analyze the symmetry of body shape for 
determining whether a person carrys an object, e.g., 
backpack.  However, silhouette feature is ambiguous 
in many cases for detecting other objects (such as 
handbags slung over shoulder).  For the smoking 
event analysis, the uncertainties in cigarette size, 
orienation, and colors make it very challenging if 
without any human efforts.  In this paper, a novel 
histogram-based method will be proposed to detect 
different kinds of cigiratee object and then analyze their 
related smoking events directly from videos. 
 
2. Overview 
 
 
Fig. 1: Flowchart of our proposed scheme. 
This proposed method consists of two stages: event 
cue detection and event classification. The cue stage is 
a self-determination scheme to extract cigarettes and 
human hand motions, and then nominates time point 
candidates for smoking event analysis.  For 
recognizing the event characteristcs of intentional 
actions, we introduce three conceptual terms into our 
system, i.e., human, object, and smoke, to represent the 
state of interaction clues.  As shown in Fig. 1, this 
paper first uses a background subtraction technique to 
extract the regions of motion.  In addition, we 
examine the motion energy on each pixel to extract 
smoke regions from the result of background 
subtracition.  Then, the face detector [13] is applied 
for exploring face regions from videos.  Based on the 
obtained face, a ratio histogram analysis is then applied 
for highlighting all possible cigarette candidates.  
Once in support of the cigarette and smoke reigons 
2010 International Conference on Pattern Recognition
1051-4651/10 $26.00 © 2010 IEEE
DOI 10.1109/ICPR.2010.1056
43285244
( , )( , ) e C x yG x y ε−= ,            (7) 
where Cε  denotes the color distance between a pixel 
(x, y) and the color mean Cμ  of f , i.e.,  
( ) ( )1( , ) ( , ) - ( , ) -TC C C R C Cx y I x y I x yε μ μ−= Σ ,. 
Here, ( , )CI x y  represents the color vector of the pixel 
(x, y) and RΣ  is the color variance of f . ( , )G x y  is 
a skin model and can be used for hand detection.  Fig. 
3 shows a result example of hand region detection.   
 
4. Smoking Event Classification 
  
(a) H           (b) X           (c) S 
Fig. 4: State transitions of three observations H, 
X, and S in the smoking event analysis. 
  
1H
1C
1X 1S 2H
2C
2X 2S nH
nC
nX nS
 
Fig. 5: The visual signatures of cigarette smoking 
event in detection scope (left) and their graphical 
presentation as spatial nodes on each primitive (right). 
In the smoking event analysis, we focus on its event 
nature of three visible signatures: hand position 
(distance to face), cigarette (event property), and smoke 
(visual suspension) in the probability model machines 
of interaction relationship.  As illustrated in Fig. 4, 
where the state of hand, H = {body:−1, face:+1},  in 
terms of hand position, the state of cigarette, X = 
{(approaching: −1, leaving:+1)},  in terms of distance 
to face; the state of smoke, S = {firming: −1, dispersing: 
+1}  as visual suspension of smoke.  A smoking 
event E is defined as a temporally ordered set of frame 
primitives { }1,..., ,...,t nC C C .  The graphical model for 
this event E is illustrated in Fig. 5. A frame primitive 
tC  is associated with a hand node tH  for the hand 
status, a cigarette node tX  for the state of cigarrete, 
and the smoke node tS  for the status of smoke region.  
The primitive tC  in E follows a Markovian model, so 
that the probability of E under the observation 
{( , , )}O H X S=  can be presented as:  
1
2 1
( | ) ( | ) ( | ) ( | )
( | ) ( | ) ( | ) ( | ).
n n
t t t t t t t t
t t
P O E P H E P X E P S E
P C C P H C P X C P S C
−
= =
≅
= ∏ ∏ (8) 
Here, 1( | )t tP C C −  is the transition probability from 
primitive tC  to 1tC − . ( | )t tP H C , ( | )t tP X C , and 
( | )t tP S C  denote a hand likelihood model, a cigarette 
likelihood model, and a smoke likelihood model for 
tC , respectively. Then, the probability of an 
observation O belonging to an event E can be defined 
as follows: 
( | ) ( ) ( | )P E O P E P O E≅ , 
where ( )P E  is the prior probability.  Let Ω  denote 
the event space.  Then, the event analyzer to analyze 
O can be formulated an optimization problem as 
follows: 
arg max ( | )optimal EE P E O∈Ω= .   (9) 
When Ω  is known, Eq.(9) can be well solved using 
the famous Viterbi algorithm [14].  
 
5. Experimental Result 
To analyze the performance of our proposed approach, 
a real-time system to analyze different abnormal events 
at face zones from different views was implemented. 
To analyze the efficiency and effectiveness of the 
proposed approach, we created a large database under 
different scenes and lighting conditions. The dimension 
of each frame is 360 240× . The frame of our system is 
faster than 25 fps. 
Fig. 6 shows three results of cigarette detection under 
different lighting conditions.  The environment in (a) 
indoor and the one in (b) is outdoor.  In (c), even 
though the head orientation is slanted, the proposed 
method still works with correct results.  Table 1 
presents the accuracy rate of cigarette detection, where 
the total number of analyzed frames is 2196, and the 
average accuracy is about 93.2%.  Fig. 7 shows the 
result of smoking event analysis.  (b) lists all the 
probability of event voting.  Even though the outdoor 
scene is analyzed, our method still works well to detect 
this abnormal event.  Table 2 lists the accuracy 
analysis of smoking event detection.  Fig. 8 shows 
other examples of smoking event analysis at different 
places ((a)-(d)). The overall average accuracy of 
smoking event analysis is about 82.1%.  
     
(a)                  (b) 
 
(c) 
Fig. 6: Results of cigarette detection. 
43305446
國科會補助計畫衍生研發成果推廣資料表
日期:2011/08/08
國科會補助計畫
計畫名稱: 應用於即時多媒體應用之快速軟體為主的視訊計算方法研究
計畫主持人: 鄭錫齊
計畫編號: 98-2221-E-019-036-MY2 學門領域: 影像處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
