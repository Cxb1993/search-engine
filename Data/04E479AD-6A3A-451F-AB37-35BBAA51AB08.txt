 2
一、中文摘要 
文字辨識(OCR)技術是影像處理及圖形識別最成功的應用案例之一，然而傳統之文字辨
識卻無法解決日益急迫的三維文字辨識問題。因為全球化所造成各國間交流越趨頻繁，各種
語文間翻譯的需求量越來越高，但翻譯人員並非隨處皆有，只能借助自動化翻譯機器的協助，
但三維文字的輸入與辨識本身就是一個極大的問題與挑戰，除非是翻譯文字檔案，否則根本
不易於進行出國時日常生活隨處可見的三維場景中招牌、菜單、路標等外國文字上的輸入，
此時問題的核心，就在於如何進行三維場景中文字的偵測、切割與識別。在本研究計畫中；
第一年我們將建立一套 web-camera 平面文字辨識系統，這套系統將提供使用者中英文辨識的
功能。第二年將建構一套規則立體表面上文字辨識的模型。第三年將嘗試在第二年的基礎上
更往前一步，建構一套不規則曲面上文字辨識的模型，以將辨識範圍由規則曲面擴展至非規
則曲面上。本計畫擬建立一套以電腦視覺為基礎之智慧型的文字辨識系統，希望能夠使用現
今科技帶來的便利性與機動性，來提供使用者一種隨身翻譯秘書的功能。 
關鍵詞：三維場景、文字切割、文字辨識 
二、英文摘要 
Optical Character Recognition (OCR) is admitted as one of the most successful and mature 
applications in the areas of image processing and pattern recognition. However, the recognition of 
3D characters in 3D scenes is still a challenging problem which is much difficult than traditional 2D 
character recognition problem. Due to the internationalized and frequent visit between different 
countries, the demand for the real-time translation of different languages gradually becomes a must. 
However, the input of 3D texts, such as road signs, campaigns, and manus, are infeasible using 
traditional 2D scanning devices. The problems encountered are then the solving of those unexpected 
effects incurred by the 3D image capturing devices in order to successfully extract the characters 
embedded in the real scene environments. In this research, we build a web-camera based character 
recognition system which can recognize 3D English/Chinese characters captured by 3D capturing 
devices. Name card like planar objects are the simplest 3D texts to be resolved in the first year of 
this project. In the second year, we plan to extend the objects to regular non-planar objects and 
recognize the embedded characters accordingly. As to the third year, the objects to be dealt with are 
further lifted to irregular non-planar objects. The handling of non-uniform lighting, perspective 
distortion, and camera vibration are the challenging issues to be conquered in the execution of this 
project which have seldom been addressed or even ignored in traditional OCR/DA research. With 
the ongoing of this project, we hope to develop a personal portable assistant to resolve the machine 
translation problem. 
Keywords: 3D scene, character segmentation, character recognition 
因此本計畫提出以LBP方法作為文字影像的特徵並利用最近特徵空間轉換法進行文字特徵空
間的轉換，由於 LBP 方法是一種描述 pixel 與 pixel 之間大小關係的紋理資訊，因此對於雜
訊的敏感度較低，且相當適用於描述文字特性，而最近特徵空間轉換法則是可以虛擬文字的
變化使樣本空間更具有代表性，因此本計畫以 LBP 為特徵結合最近特徵空間轉換法作為新的
OCR 系統。 
 
去除影像受
外在光線影
響模組 
於複雜背景
下進行文字
區域偵測模
組 
對非共平面
或曲面文字
區域進行校
正模組 
文字切割與
辨識模組 
圖一：系統區塊圖 
三、相關研究 
本計畫是以視覺為基礎的實景文字偵測系統，我們在本計畫中共完成了(1) 去除影像受
外在光線影響、(2) 於複雜背景下進行文字區域偵測、(3) 對非共平面或曲面文字區域進行校
正、(4) 文字切割與辨識等四個模組，而在相關研究方面我們則可以分為(a)影像中的光線影
響與經驗模態分解法、(b)物件偵測、(c)影像失真校正、(d)物件辨識等幾個方向來探討： 
(a)影像中的光線影響與經驗模態分解法： 
本計畫的目標是針對實景影像中的文字進行偵測與辨識，首先面對的即為實景環境中影
像受光線影響的問題，因此在相關研究方面我們首先探討影像中的光線影響以及本計畫中採
用的經驗模態分解法。 
1. 影像中的光線影響:在自然界中，由於外在環境是我們無法預料與掌握的，取像時遇到不
同光影影響的機會非常多，所以也有許多文獻對這方面問題做研究與解決。最常見的光
場偏暗現象，Pei and Chang針對自然影像光影不足做亮度調升的色彩恆常性矯正，提出
一個以基準的RGB值，運用Von Kries mode中的色適應公式對原始影像做光影的調整，
達到光影矯正後影像低色偏現象且執行速度快的成果，但是演算法中必須手動決定閥
值。Pei and Tzeng使用二維經驗模態分解法(BEMD)解決彩色影像光影不足問題，但是二
維內插與經驗模態分解法本身皆需耗比較大的計算量。我們也常遇到對一個場景或一個
文件取像，取像目標與取像裝置之間隔了一層平滑面，造成不同場景影像的混合成一張
混合影像，Farid與Adelson描述如何運用獨立成分分析(Independent Components Analysis, 
ICA)，做數學上空間的旋轉，將混影影像分解成兩張不同場景的來源。Chen與Xu不只
針對混影問題 做討論，也描述取像裝置經過非線性的轉換才成像，在影像分析與量化
上可能會造成計算不精確的現象，所以他們使用Kernel-ICA來解決這些問題。但是Farid
與Chen的方法均需要取得兩張以上影像進行分析才能獲得量好的效果。Chen and Chung
 4
(extrema) 具有相同數量。為了使EMD解析出來的IMF分量都滿足上述限制條件，IMFs
必須滿足兩項限制條件，第一項是在訊號中，極值(局部極大值與局部極小值)的數目總
和與訊號跨零點的數目必須相等或是最多差一，第二項是局部極大值包絡線(maximum 
envelope)及極小值包絡線(minimum envelope)在訊號任一點其兩者平均也就是均值包絡
線(mean envelope)其值為零的常數函數。第一項條件類似傳統平穩高斯過程(stationary 
Gaussian process)之窄頻寬的要求。第二項條件為將整體性的要求轉變為局部性的要求。
分析訊號的過程中，針對非平穩性的訊號方面，局部均值需要有一個局部的時間尺度
(local time scale)定義，但尋找局部時間尺度非常困難。為了達到此要求，使用局部極大
值所定義的極大值包絡線及局部極小值所定義的極小值包絡線強迫局部對稱，這是一種
對信號可分析的必要近似。如圖2為典型的IMF，橫軸為取樣點，縱軸為訊號值。 
 
 
圖二：IMF 訊號 
(b)物件偵測： 
現行常見的物件偵測方法有相當多的研究成果，主要可以分為：(1)特徵式及(2)影像式。
其中特徵式(Feature-based)演算法又可分為動態活動模型(Active shape models)、低階分析
(Low-level analysis)等，低階分析包含邊緣(Edges)、灰階(Gray-level)、顏色(color)、動態資訊
(Motion)、影像測量(Generalized measure)等方式。影像式 (Image-based) 演算法為基礎的演算
法則包含線性子空間法(Linear subspace)如PCA，類神經網路如幅射基底函式類神經網路，以
及統計類的方法如支援向量機等。 
1. 動態活動模型：動態活動模型是利用影像的特徵，依據物件在影像中如亮度、邊緣
等差異性擷取活動式範圍，通常利用動態輪廓(Active contour)或曲線(Snake)方式實
作，找出物件的邊界，而後續的改良方式除了提升了型態在邊緣的地標點位置
(Landmark point)準確性，也加入了分群法，避免型態差異過大，另外在搜尋的機制
上有較佳的延展性，然而此一類型偵測器必須限制特定偵測範圍，且無法滿足及時
偵測之要求。 
2. 低階分析：此一類型偵測器著重於資訊融合，色彩，動態位移量，物件特徵與幾何
資訊等等，皆是資訊融合的最佳資訊，因此適用於多專家系統，然後針對個個結果
 6
主成份分析取得特徵向量，輸入至 One-Class 支援向量機來判定是否為物件。支援向
量機的方法基本上皆是決定最合適的支持向量(support vector)，此類型偵測時耗費時
間與類神經網路架構相似，許多論文往往提出簡易的過濾器，以減少檢視視窗數目，
然而，預先過濾器往往有設計者依據其環境，利用簡易的色彩或動態位移量進行過
濾，往往無法達到預期效果 
(c)影像失真校正： 
由於目標文字影像存在三度空間中，以相機所擷取出來的文字影像可能會有扭曲、傾斜
與透視失真，因此需要將文字區塊予以校正才能利於後端的OCR進行。針對這種三度空間的
非共平面影像，Agam等與Brown 等將影像資料輸入，並建立一個3D表面模型做為矯正的參
考點。3D 表面模型的建立在於先將資料分布到立體投射網格當中，透過三角形網格的評估，
將表面一步一步地逼近出來，這需要極大量的計算，優點則是可以解決不平坦的非共平面的
問題。關於2D 平面的影像透視失真矯正，最常見也最可行的是單一平面的透視矯正，只要
將原始影像的空間Io透過轉換參數H來達到我們所預期的影像空間It，表示成：It=IoH。根據
Hartley等在轉換參數H 上的討論，各種平面轉換的類型主要分為：歐幾里德(Euclidean)轉換、
相似(Similarity)轉換、仿射(Affine)轉換及投影(Projective)轉換，如圖三所示: 
 
圖三：平面轉換類型示意圖 
 
透過歐幾里德轉換函數能做平移及旋轉，相似轉換函數除了平移與旋轉加上縮放，仿射
轉換函數則是再加上水平切變與垂直切變，投影轉換函數綜合以上動作再加上了透視變形。
由於投影轉換函數對於平面的轉換後角度比較具有自由度，同時也需要較多的計算，如圖三
中ABCD對應到A’B’C’D’，而計算投影轉換函數矩陣H的公式為：H=A-1R。其中H是投影轉換
矩陣，R是影像四個參考點的對應座標及輸入參數座標點矩陣A。在矯正文字影像的研究中，
早期處理方式大多依靠影像中文件紙張所提供的邊緣或是文件的段落排版格式，求得平面矯
正的參考點；近來也有研究者利用文字本身的特徵做分析，X. Chen 等針對文字的排版做了
分析，得出文字內部(intrinsic)與文字外部(extrinsic)特徵；內部特徵包含顏色、字型、對比，
內部特徵不受取像位置角度影響；外部特徵則包含字形大小、邊界形狀、文字排列，外部特
徵受相機位置與角度的影響。而這些外部特徵可對投影轉換提供一些參數資訊交換。早期做
 8
徑、文字詞彙的字串及文字行底部的底線。文字行間距方法是指在印刷排版或手寫文字時，
相鄰文字行間的空白。利用文字行與行之間的空白間隙，以水平走向尋找空白路徑的方式，
將行與行以空白路徑給分隔開，或是透過水平投影方式將黑色像素點連結起來。Tseng等使用
Viterbi演算法來分割手寫中文字的路徑藉以找出文字行，也屬於此類方法。文字行底線方法，
以英文字為例，在書寫英文字時存在著文字行的底線，Zramdini等提出了以Typoline (圖七)分
析文字的概念，透過找出四條水平文字的基準線來分析文字特性。後續許多相關研究人員也
利用Typoline觀念來標定文字行。而字串方法是指將偵測出單一文字的連通元件，運用塗抹的
方式找出具有關連性的連通元件，並將之串連起來。Friedrich等提出運算平滑長度演算法(Run 
Length Smoothing Algorithm)透過往垂直、水平方向的像素探訪將白與白之間的像素點，用預
先定義好的閥值來判斷關連性，若距離小於閥值，則將之變成黑點，形成文字行。 
 
  
圖六：文字行特性描述：路徑，底線和字串 
 
  
圖七：Typoline 定義 
 
(d)物件辨識： 
物件辨識是目前生物認證中的熱門課題，在門禁系統等商業應用上更是有相當大的需
求，又由於目前攝影機的成本相當低，更促成了相關應用的發展，概觀目前的相關研究，都
是將其物件影像拍攝下來再經過系統直接作辨識，可是物件影像卻容易受到光線或是外在環
境的影響而改變辨識結果。物件辨識方法可分為以下四類：(1)演算法式 (algorithm approach)，
主要是以物件某種特徵為基礎，例如物件特徵的形狀(Shape)或特徵的亮度關係進行辨識。(2)特徵
空間轉換(eigen space transform)的演算法，將物件影像投影至事先取得的物件特徵轉換空間中進
行比較。(3)類神經網路式(Neural network based approach)，利用類神經網路的學習能力，調整每
 10
 
(1) 去除影像受外在光線影響模組： 
過去取像的方法，大多是用傳統的掃描影像裝置所取得，這種掃描裝置不僅設備笨重且
取像時間較長，照成諸多不便；而科技的進步使得電子產品的樣式越來越多，體積也越來越
小，因而讓我們可以把取像功能的裝置帶在身上活動，不必受限於掃描器，如數位相機取像、
網路攝影機取像、手機照相甚至近年來漸漸盛行的平板式電腦也有取像的功能，這些裝置帶
給人們多元方便的取像方式。但是相較於傳統掃描裝置取像時光源充足、機身穩定，這些便
利的裝置取像時，卻容易遭受外在環境光線的干擾或無法掌握取像環境的光場強度，如圖八
即是常見影像上因光線問題所造成的現象，包括(1)針孔成像：某些針孔式取像裝置因為硬體
的關係，取像時雖然環境光場是均勻的，仍造成離鏡頭較遠的部分取像出來的結果光場較弱。
(2)方向光：在三維空間中，取像目標受到單一方向的光源照射，造成取像目標的其他部位因
此形成陰影。(3)陰影影響：取像面因為遮蔽物阻擋其他光源，造成取像結果光場強度不一的
狀況。(4)反光問題：因為取像裝置與取像目標之間隔一層透明平滑面，平滑面容易倒映其他
場景的像，取像後的結果也因而照成影響。(5)透光問題：常發生在文件影像上，因為在取像
目標文件的背後有較強烈的光照，使得雙面列印文件的背面文字資訊顯露出來。(6)非共面光
影影響：因為取像時是三維實景取像，所以取像目標不見得在同個平面上，而現實世界的光
源強弱不一且多方向，所以很可能造成不同平面的實景有不同受光的強度。(7)曝光過度問
題：因為閃光燈或取像時間太長，造成影像每個點的灰階強度皆上升，而失去紋理與細節。
這些光影影響不僅可能造成某些實景紋理的消失，應用在OCR辨識上效果也會大打折扣。 
 
 
  
(a) (b) (C) 
   
(d) (e) (f) 
圖八：各種光影問題影像。(a) 針孔成像。(b) 方向光。(c) 陰影影響。(d) 反光問題。
(e) 透光問題。(f) 非共面光影影響。 
 
 12
圖九上包絡線在光影斷層處很明顯偏離實際光場，再觀察實際光場與上包絡線之間的關
係，我們可以發現在光場改變處，很明顯有一段區域是上包絡線皆大於實際光場(圖九紅色圈
處)，而實際光場線必定大於或等於該原始灰階值。基於以上的觀察我們定義「偏離區間」如
下：如果找到某個夠大的區間，該區間其上包絡線的值皆大於原始灰階訊號，則該區間稱為
「偏離區間」。當輸入訊號含不同光場時，其光場改變處一定有此「偏離區間」。如果我們找
到多個偏離區間，就選擇訊號端點差值最大的區間為「最佳的偏離區間」，如圖十(a)所示，圖
中我們找到偏離區間的結果有2個，以紅色偏離區間而言，端點差為 21 XXX diff −= ，而另一
個紫色偏離區間端點差為 21 YYYdiff −= ，很明顯可看出來 ，因此我們選擇紅色偏離
區間為我們選定的最佳偏離區間。之後，比較選定的偏離區間兩端點的灰階值較大者，該端
點即為分段點，如圖十(a)所示， ， 點為最佳的分段點；圖十(b)為圖十(a)輸入訊號
的反向光場情況時所計算的上包絡線，以同樣的機制我們依然可以在圖十(b)找到最佳分段點
。當順利找到分段點，並計算出各區段的局部上包絡線後，對於某些特定的灰階分佈，局
部區域的上包絡線依然無法描述的很貼近，而產生新的偏離區間，如圖十一，所以我們對整
段訊號依新的上包絡線，再找出那些夠大的偏離區間，然後只對這些偏離區間做小區域的上
包絡線計算。 
diffdiff YX >
21 XX > 1X
2Y
 
 
(a) 
 
(b) 
圖十：多段偏離區間與分段點選擇。(a)範例光場。(b)反向光場。 
 
 
圖十一：新偏離區間 
 14
本模組去除光線影響的實驗結果如下： 
 
  
(a) (b) 
圖十三：彩色影像做ABEMD實驗結果1。(a) 原始影像。(b) ABEMD實驗結果
 
  
(a) (b) 
圖十四：彩色影像做ABEMD實驗結果2。(a) 原始影像。(b) ABEMD實驗結果
 
 
(a) (b) 
圖十五：彩色影像做ABEMD實驗結果3。(a) 原始影像。(b) ABEMD實驗結果
 16
器做分類的特徵，而其中Garcia 在2004年所提出的以CNN (Convolutional Neural Network) 做
人臉偵測中的特徵映像(Feature Map)即扮演此角色，如圖十七所示。因此CNN在本模組中是
作為文字偵測器中的影像前處理以及特徵強化使用，而其中FMc和FMs分別為CNN經過
Convolution和Sub-Sampling運算後的結果，如圖十八所示。由於在一張影像中做物件偵測時，
系統必須在各個不同位置偵測各種不同大小的物件，是一種非常需要大量運算的工作，因此
如果直接利用訓練出來的物件偵測器去針對整張影像做偵測將會非常地耗時，所以為了讓偵
測速度能有效的提升，本系統採用了Coarse-to-Fine的架構，來加快物件的偵測，本模組的系
統架構如圖十九所示。 
 
 
圖十七：以 CNN 為基礎的物件偵測器架構圖[Garcia] 
 
 
圖十八：單一特徵映像的 CNN 物件偵測器 
 
 
圖十九：文字區域偵測模組系統架構圖 
 18
限於印刷字體的特徵，同時將來應用至其他非印刷字體上的容忍性較高。找出文字行之後，
再以文字行的形狀分佈來替平面區分類別。 
 
 
圖二十：文字行角度變化圖 
 
 
 
圖二十一：文字行間距與字元間距定義 
 
本模組改良使用由 Basu 等所提出的流水(Water flow)法進行文字行抽取。流水法原理是
透過察訪影像中的背景資訊，藉由找出在文字影像中屬於背景空白的部分，最終通過探訪的
路徑，將文字行隔離抽取出來，Basu 等在手寫的文字影像處理中提出此方法。通常手寫的文
字並不如印刷體般的整潔，而且手寫文字行距也不如印刷字體整齊，所以找尋方法必頇能夠
容忍文字行的空白間隙在角度上的變化。模擬流水可以避開障礙物的特性，將流水一分為二
往障礙物的兩側方向流去，最終繞過障礙物後又合在一起的現象，Basu 等發現到此一流水擬
態可應用到手寫文件影像的背景資訊探訪。在概念上如何將流水的擬態應用到文字行掃描
上，以圖二十二說明之。圖二十二(a)中的黑色部分為障礙物，流水從左至右方向流過，產生
流水未浸溼的部分如圖二十二(b)中障礙物的陰影。 
 
圖二十二：流水擬態示意(a)目標障礙物(b)從左而右流水結果 
 20
 
接著用型態學的侵蝕(Erosion)，對標記值為－‟的像素點(非文字行)去做侵蝕的動作，用
大於文字之間的空白間隙，又小於文字行之間的空白間隙做為侵蝕半徑，可以將文字行斷開
的部分結合在一起。其用意是彌補在流水擬態做掃描後，其一是某些文字行之間流水無法通
過，使我們得到一大片的文字區塊而非文字行區塊，其二是某些字元字形或標點符號特性導
致字形高低不同狀況，使得流水文字行的邊緣中，夾在文字間的非文字像素點無法被流水過，
讓應該是同一條文字行卻發生斷裂的情況，造成一條文字行成為不同的兩條文字行。本模組
實作與原作上稍有不同，原作上執行完流水演算法後，以侵蝕的方式解決一條文字行斷裂與
一大片文字區塊中分割出文字行的問題；本論文的文字行因為在文件表面的曲折上而產生形
變，原本的流水演算法只針對一種流水方向並不適用在此。同時我們希望保留文字行裡的用
流水遮罩得出的文字外形，因此提出改善的方式為：影像首次對左右流水方向做完流水演算
法後，將得到的影像上下旋轉再做一次左右流水方向後再結合成一張影像標記。這樣的方式
可以分割出文字行同時保有流水遮罩產生的文字行外形，但仍有產生一條文字行斷裂的可能
性，所以再對流水文字行的外形做膨脹(Dilation)後再做侵蝕，即為形態學的閉合，來連接斷
層而不只是侵蝕。從圖二十五(a)中可看到仍有許多獨立點的出現，在這邊做一個判斷來去除
獨立點。閥值的選擇是，當獨立點的面積小於一開始所有連通元件的平均面積，則視為雜訊
將之捨棄如圖二十五(b)。 
 
(a) (b) 
圖二十五：流水偵測說明(a)流水演算法(b)過濾資訊不足點 
 
將做完流水後的影像做標記的動作，將流水過的地方標記值為”255”，未曾被水浸濕的標
記點”0”，透過快速連通元件標記演算法，以標記出文字行範圍。考慮合併標記時，再次由左
至右、由上至下掃描，若存在相鄰的像素點被分別標記為不同的值，表示相鄰的像素點屬於
等價類別如圖二十六(a)。則重新將標記設定為相鄰點中標記值中最小的值如圖二十六(b)。在
影像中所有文字行標記處理結束完後，再與原影像做標記，只對影像中文字部分重新標記，
即為文字行文字部分的抽取，得到如圖二十七。 
 
 22
平面，反之便是雙平面。 
 
圖二十八：直線文字行匯集趨勢線圖(a)單一平面文字行(b)單一平面文字行(c)雙平面
文字行 
 
圖二十九：判斷匯集讀序圖(a)單一平面文字行(b)單一平面文字行(c)雙平面文字行
 
在本論文影像中，文字行類型分為直線、曲線、折線三種，平面類別則可以分為非共平面、
圓柱體、共平面等三種，平面類別偵測則是透過平面中頂端與底端的文字行形狀進行判斷，
最後對各種分類做不同方式的矯正。最後透過迴歸分析得出的文字行做逼近方程式，逼近方
程式可分為幾種：第一種直線，將文字行趨勢線的座標，用一次線性，逼近一條直線方程式。
第二種曲線，將文字行趨勢線的座標點，用二次多項式逼近而得曲線方程式。第三種折線，
則是以折線文字行趨勢線的分界轉折座標為界，將切點的左右側像素座標點分別逼近獲得兩
條端點匯接的直線方程式。將文字行像素的XY 座標點，以簡單線性迴歸(Simple linear 
regression)方法求其直線方程式係數估計參數截距與斜率。概念是，由許多畫在散佈圖
(Scattergram)裡的直線裡頭，希望選一條最適配(Best fits)的直線，能夠最小化誤差平方和
SSE(Sum of Squares Error)的數值。此時即可獲得截距與斜率。而曲線文字的逼近則可以採用
二次多項式的曲線模式逼近。而折線逼近則可以先找出折線中的切點，利用切點將折線切割
成兩條直線，由於輸入的文字行有向上、下開口的不同情形；而當直線文字行是一條接近水
平直線時，對極值判斷形成多種不同情況，使折點的判斷困難。因此我們取一直線通過趨勢
線左右端點，透過計算趨勢線每個點與此一直線的距離，作為極值衡量因子，即此直線與趨
勢線間距離。而文字行影像中假如文字行較接近水平，在文字行形狀判斷時無法提供有效的
 24
 26
 
(4) 文字切割與辨識模組 
 
4.1 文字切割模組 
車牌文字的切割，是利用車牌區域影像二值化得的垂直投影值來進行切割的處理，這個
部分比較適用於清晰的影像上，如果偵測到的車牌區域之影像有歪斜發生，或是區域中文字
不清晰的情況時，容易有錯誤的出現。文字切割的流程如下： 
1. 對車牌區域的影像進行二值化處理，這個部分是利用Otsu 二值化的方法進行處理，在完
成二值化處理後對結果進行垂直投影的計算。 
2. 對垂直投影的結果，找出垂直投影量之間投影量低的部分，當部分為零則不進行處理，
而當投影量低的部分，與兩側的投影量相減後的差異量大於6 時，則把該投影量投為零。
而進行完處理後找不到為零的向量值時，則將整個垂直投影減去最小的投影量，使車牌
二值化的垂直投影出現投影量為零的部分，以切割出文字。 
3. 我們所用的類神經網路的輸入樣本大小寬度為11，但是在完成垂直投影處理後的結果，
每個車牌文字的寬度有可能大於或是小於11，所以針對這個部分，需要先對文字的寬度
進行判斷。首先是垂直投影寬度小於11 時，這個部分需要先判斷是於投影量過小，投影
大於4 的數目小於2 個時，則視為雜訊。反之以此段文字的中心當成基準，往左右擴張，
直到寬度為11 時。接著是寬度大於11 又小於20 的部份，這個部分取出文字部分的方式
是以最左端為基準，直接取下影像寬度為11。最後是投影量大於20 者，這個部分中，可
能包含兩個以上的文字，所以先找出中間投影量低的部分，將這個區域給切割開來，分
成兩左右兩個部分，而左邊的部分直接取下寬度為11 的影像，剩餘右邊的部分則重覆之
前的動作，將影 像重新切割。後來經過實驗發現上述寬度大於11 而小於20 的部分，如
果直接以最左端為起點，則可能會發生將邊緣雜訊給包含進去的問題，所以根據實驗結
果發現，如果改以先去除左右兩端投影量較低的部分，再重新切割文字。而在車牌區域
的最左與最右端，如果以上述的方式進行處理，很容易把邊緣不屬於車牌的影像給切割
出來，所以直接以最左與最右端為起點，直接取下寬度11 的區塊當成是第一個與最後一
個的文字。 
進行完文字的切割後，將文字輸入辨識模組，求取最接近的樣板，將其當成是文字辨識
的結果。 
 
4.1 文字辨識組基本概念： 
文字辨識一直是一個相當熱門的研究領域，它的困難之處在於文字影像具有高度的非線
性分佈，因此難以線性的方式將不同類別的資料分割，此外，文字資料的變化性為無窮大，
因此相對上只能收集到少量樣本，因此如何以少量樣本達到良好的辨識率亦是一個熱門的研
究議題，本模組的目的便在於提出一新的空間轉換方法最近特徵空間轉換(Nearest Feature 
Space Embedding，以下稱 NFSE)，傳統線性區別分析方法皆是以點與點之間的向量為基礎計
算共變異數矩陣，而最近特徵空間轉換法則提出了以點到點、點到線、點到面乃至點到超平
成點線最短向量，而此目標函數代表的意義為將點到線之間的最短向量予以最小化如圖三十
二所示，而此式可以轉換為以 Laplacian Matrix 的形式表示。 
 
iy
nymy )(
)2(
, inmf y
nms , mns ,
圖三十二：文字行的文字抽取示意圖 
 
最後再以 Fisher's Criterion 將類別資訊加入，如下式： 
 
( )( )( )∑ ∑∑
= ∈= ⎟
⎟
⎠
⎞
⎜⎜⎝
⎛ −−= c
i
j
P
K
P
iN
i
Ti
j
TPi
j
Ti
j
TPi
j
T
xFf
N
j
P
W ff
1
)()(
)(1
)( )xw(xw)xw(xw
)(
1
)(
S     (3) 
 
( )( )( )∑ ∑∑
= ∈= ⎟
⎟
⎠
⎞
⎜⎜⎝
⎛ −−= c
i
j
P
K
P
iN
i
Ti
j
TPi
j
Ti
j
TPi
j
T
xBf
N
j
P
B ff
1
)()(
)(1
)( )xw(xw)xw(xw
)(
2
)(
S      (4) 
 
其意義為將同一類別的點到線的距離予以最小化，而不同類別之間的點到線的距離予以最大
化，如此即可以求得一特徵空間，而此一特徵空間為以點到線的距離為基礎的區別分析所得
到的特徵空間，不僅可以保留區域資訊，亦將類別之間的區別能力最大化。 
 
  
(a) (b) 
圖三十三：最近特徵空間轉換法中的 fisher’s criterion 示意圖 
 28
原拍攝影像 文字區域偵測、校正、
切割結果 
文字辨識結果 
 
6H7227 
 6H7227 
 6H7227 
 6H7227 
圖三十五：車牌偵測與辨識結果 
 30
 32
[9] Shijian Lu, Chew Lim Tan and Bolan Su "Document image binarization using background 
estimation and stroke edges", International Journal on Document Analysis and Recognition , 
Volume 13 Issue 4, December 2010 
[10] Jean Claude Nunes, Yasmina Bouaoune, Eric Delechelle, Oumar Niang, Philippe Bunel, 
"Image Analysis by Bidimensional Empirical Mode Decomposition", Image and Vision 
Computing, vol. 21, pp.1019- 1026, 2003. 
[11] Shuen-De Wu , Liang-Jung Chen , Chiu-Wen Wu,Guan-Yu Chen and Chia-Chi Lin , 
"Improving the computational efficiency of empirical mode decomposition algorithm by down 
sampling ", International Journal of Advanced Information Technologies (IJAIT) , Vol. 4 , No. 
2 , 2010。 
[12] 吳順德,陳思予,陳虹伯“經驗模態分解法之研究趨勢探討與問題分析” ,臺北科技大學學報, 
No.42-1, 2009。 
[13] 吳政憲，「希爾伯特阻尼頻譜於高樓損傷評估之應用」，國立中央大學土木工程研究所，
碩士論文，民國 90 年 
[14] Ron Larson and Bruce H. Edwards, Elementary Linear Algebra 4th ed., Houghton Mifflin 
Company, 2000. 
[15] Marc Ebner, "Color constancy base on local space average color" ,Machine Vision and 
Applications, vol. 11, No. 5, July, 2009. 
[16] Joost van de Weijer, Theo Gevers, and Arjan Gijsenij, "Edge-Based Color Constancy" ,IEEE 
Trans. on image processing, Vol. 16, No. 9, September 2007. 
[17] G. Agam and C. Wu, “Structural Rectification of Non-planar Document Images:Application to 
Graphics Recognition,” Lecture Notes in Computer Science, pp.2390, 2002. 
[18] M. S. Brown , M. Sun , R. Yang , L. Yun and W. B. Seales, “Restoring 2D Content from 
Distorted Documents,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.29, 
n0.11, pp.1904-1916, 2007 
[19] R. Hartley and A. Zisserman, “Multiple View Geometry in Computer Vision,” Cambridge 
university press, 2ed, 2003. 
[20] X. Chen, J. Yang, J. Zhang and A. Waibel, “Automatic detection and recognition of signs from 
natural scenes,” IEEE Trans. Image Process., vol. 13, no. 1, pp.87 - 99, 2004. 
[21] N. Stamatopoulos, B. Gatos, I. Pratikakis and S. J. Perantonis, “Goal-Oriented Rectification of 
Camera-Based Document Images,” IEEE Trans. Image Process., vol. 20, no. 4, pp.910 - 920, 
2011. 
[22] S. J. Lu, B. M. Chen and C. C. Ko, “A partition approach for the restoration of camera images 
of planar and curled document,” Image and Vision Computing, vol. 24, no. 8, pp. 837-848, 
2006. 
[23] L. Likforman-Sulem, A. Zahour and B. Taconet, “Text line segmentation of historical 
documents: a survey,” International Journal on Document Analysis and Recognition, vol. 9, no. 
2, pp. 123–138, 2007. 
[24] Y. H. Tseng, H. J. Lee, “Recognition-based handwritten Chinese character segmentation using 
a probabilistic Viterbi algorithm,” Pattern Recognit. Lett., vol. 20, no 8, pp. 791–806, 1999. 
 34
[44] C. Liu, “A Bayesian discriminating features method for face detection,” IEEE Transactions on 
Pattern Analysis and Machine Intelligence, vol. 25, pp. 725–740, 2003. 
[45] P. Shih and C. Liu, “Face detection using discriminating feature analysis and support vector 
machine,” Pattern Recognition, pp. 260–276, 2006. 
[46] C. A. Waring and X. Liu, “Face detection using spectral histograms and svms,” IEEE 
Transactions on Systems, Man, and Cybernetics-Part B: Cybernetics, pp. 467–476, 2005. 
[47] Y. Li, S. Gong, J. Sherrah, and H. Liddell, “Support vector machine based multi-view face 
detection and recognition,” Image and Vision Computing, pp. 413–427, 2004. 
[48] R. Xiao, M.-J. Li, and H.-J. Zhang, “Robust multipose face detection in images,” IEEE 
Transactions on Circuits and Systems for Video Technology, pp. 31–41, 2004. 
[49] B. Heisele, T. Serre, S. Prentice, and T. Poggio, “Hierarchical classification and feature 
reduction for fast face detection with support vector machines,” Pattern Recognition, pp. 
2007–2017, 2003. 
[50] H. A. Rowley, S. Baluja, and T. Kanade, “Neural network-based face detection,” IEEE 
Transactions Pattern Analysis and Machine Intelligence, vol. 20, pp. 23–38, January 1998. 
[51] S. Milosevic, 1997, Driviers' fatigue studies. Erognomics, 40(3), pp381-389. 
[52] Lisper H.O., Laurell H., van Loon J., 1986, Relation between time to falling asleep behind the 
wheel on a closed track and changes in subsidiary reaction time during prolonged driving on a 
motorway. Ergonomics, 29, pp445-453 
[53] Garcia C., and M. Delakis, “A Neural Architecture for Fast and Robust Face Detection,” 
Proceedings IEEE International Conference on Pattern Recognition, pp. 44-47, 2002. 
[54] Chang C. Y., and S. Y. Fu, “Image Classification using a Module RBF Neural Network,” 
Proceedings of the First International Conference on Innovative Computing, Information and 
Control, 2006. 
[55] Burges C. J. C., “A Tutorial on Support Vector Machines for Pattern Recognition” Data 
Mining and Knowledge Discovery, pp. 1–47, 1998.  
[56] Jin H., Q. Liu, and H. Lu, “Face Detection Using One-Class-Based Support Vectors,” 
Proceedings of the Sixth IEEE International Conference on Automatic Face and Gesture 
Recognition, 2004. 
[57] Waring C. A., and X. Liu, “Face Detection Using Spectral Histograms and SVMs,” IEEE 
Transactions on System, Man, and Cybernetics—Part B: Cybernetics, Vol. 35, No. 3, 2005. 
[58] Soriano M., S. Huovinen, B. Martinkauppi, and M. Laaksonen, “Using the Skin Locus to Cope 
with Changing Illumination Conditions in Color-Based Face Tracking,” Proceedings of the 
IEEE Nordic Signal Processing Symposium, pp. 383-386, 2000. 
[59] R. J. Howarth and B. Hilary,“ A analogical representation of space and time, Image Vision. 
Computing, vol. 10, no. 7, pp.467-478, 1992. 
[60] F. Bocick and A. D. Wilson, “A state-based technique to the representation and recognition of 
gesture＂ IEEE Trans. Pattern Anal. Machine Intell., vol. 19, pp.1325-1337,Dec, 1997 
  
1 
 
出席國際學術會議報告 
                                                            100 年 7 月 25 日 
 
報告人姓名 高巧汶 
服務機構及職稱 中央大學資工系博士生 
會議時間 自民國 100 年 7 月 10 日至 100 年 7 月 13 日 會議地點 大陸桂林 
經費來源 NSC97-2221-E-008-086-MY3 
會議名稱 
(中文) 2011 年 ICMLC 國際研討會 
(英文) International Conference on Machine Learning and Cybernetics (ICMLC) 
2011 
發表論文題目 
(英文) An adaptive eye gaze tracker system in the integrated cloud computing and 
mobile device 
參與性質 Oral 
報告內容： 
一、參加會議經過 
此次參加 ICMLC 2011 國際學術研討會，係於 2011 年 7 月 10 日至 7 月 13 日在中國桂林市舉
行，會議地點是在桂林市的 Sheraton Guilin hotel，除會中發表一篇研究論文，並參加多場專
題演講。 
2011/07/09：上午 12:00 搭乘中華航空 CI0913 班機飛往香港轉機。並於下午 18:00 抵達桂林 
機場，並前往預定飯店登記住房。 
2011/07/10：上午至會場報到，並參與第一場 Tutorial，題目為 Essentials of research              
methodology and an effective dissemination of research results. 
2011/07/11：早上 8:30 大會正式開始，今日有 10 場 sessions、2 場 Keynote speech。大會開幕 
吸引相當多學者的參與，並由大會邀請 Vladimír Marik 及 Seong-Whan Lee 兩位 
學者做精彩的演講。學生所報告的 session 為下午 OMB2 Intelligent Systems II。 
2011/07/12：今日大會安排一場 Keynote speech 由 Shyi-Ming Chen 教授主講 Fuzzy Forecasting 
           Based on High-Order Fuzzy Time Series and Genetic Algorithms。晚間，大會舉行大 
   會晚宴，在晚宴中把握機會與各國相關領域學者交流。 
  
3 
 
三、此次研討會中所攜回之資料 
1.研討會會議議程及論文摘要集一本。 
2.研討會會議論文全文光碟片一片(圖一)。 
     
圖一 研討會會議論文全文光碟片一片 
四、其它 
附件一、論文接受通知函 
附件二、論文全文 
 
 
b. We will need the LaTeX or WORD version of your paper (WORD
document is preferable) for English grammar and/or usage editing. Please
revise your paper according to the reviewer comments (shown on the
ICMLC Submission System). Pay special attention to your paper format
and make sure your paper follows the Formatting Guidelines as stated in
the ICMLC website. The LaTeX or WORD version of the paper should be
submitted via the ICMLC Submission System.
c. Fill in the Online Registration Form. The Registration Form can be found
in the ICMLC Submission System. The Oral or Poster presentation will be
assigned according to your preference. Please state your preference for your
paper presentation. You will be notified whether your paper will be
presented in an Oral Session or Poster Session in June 2011.
d. Fill in the IEEE Copyright Form The IEEE Copyright Form can be signed
via the ICMLC Submission System.
Steps b, c and d will need to be completed when you logon to the ICMLC
submission system. (http://www.icmlc.com/icmlcSystem/author/)
(4) After the completion of your registration, you will receive an email attached with
a registration confirmation letter. You need to print it and bring it to the
conference as proof. If you cannot attend the conference and would like to ask
your colleague to pick up the conference proceeding CDROM and one volume
of the proceeding, your colleague must present your registration receipt to the
conference organizer.
(5) You will be informed whether your paper will be invited to participate in the
Conference Awards and/or Ph.D. Colloquia Series in June 2011.
(6) We will confirm your attendance in ICMLC 2011 in June 2011.
(7) If you have any enquiry, please contact the Conference Secretary, Patrick Chan,
at patrickchan@ieee.org. Please indicate your paper ID.
Thank you again for your contribution to the ICMLC, and we look forward to seeing you in
Guilin.
Best regards,
ICMLC 2011 Program Committee
Page 2/2
  
hardware. IR-bases approach utilizes active illumination 
from infrared light source to enhance the contract between 
the pupil and the iris [7-8]. After grabbing the features of 
eye image, pupil and glint (a small bright point on the 
corneal surface) use a second order polynomial mapping 
function to calculate the gaze point on the screen. The 
calibration can be got by gazing at nine points on the 
screen in polynomial time. Therefore, IR-based system is 
prevalent in commercial and research environment.   
The neural network methods have become more 
popular for eye gaze tracker. Baluja and Pomerleau 
proposed a neural network method without explicit 
features [9]. Each pixel of the image is considered as an 
input parameter of the mapping function. Once the eye is 
detected, the image of the eyes is cropped and then used 
as the input of ANN (Artificial Neural Network). A 
method for human eye sclera detection and tracking based 
on a modified time-adaptive self-organizing map 
(TASOM) is proposed [10]. In [11], authors proposed 
remote eye gaze tracker based on eye feature extraction 
and tracking by combining neural mapping (GRNN) to 
improve robustness, accuracy and usability under natural 
conditions. 
For 3D model-based approaches, gaze directions are 
estimated as a vector from the eyeball center to the iris 
centers [12-14]. A stereo camera system is constructed for 
3D eye localization and the 3D center of the corneal 
curvature in world coordinates. Points on the visual axis 
are not directly measurable from the image. By showing 
at least a single point on the screen, the offset to the visual 
can be estimated. The intersection of the screen and the 
visual axis yield the point of regard [1]. 
In contrast to traditional computing network, a cloud 
computing user need only pay for the computing services. 
With cloud computing, new services can be developed 
without capital acquisitions of expensive hardware. Cloud 
computing is a new paradigm for distributed computing 
that delivers infrastructure as a service (IaaS), platform as 
a service (PaaS), and software as a service (SaaS). SaaS 
changes the previous use of habits, do not need to install 
the required software on the local side of the device, only 
a single username and password link cloud software that 
can use these services. PaaS is to provide users of the 
service in cloud platform; users can take advantage of 
cloud-platform application development platform or 
operating system platform to write software programs, 
and development in cloud. IaaS is the underlying structure 
of the cloud computing for the construction base of the 
cloud platform and cloud software. Cloud can be used in 
the enterprise IT departments to reduce equipment costs.  
A particle swarm optimization (PSO) methods is 
proposed, considering the cost of computing and data 
transmission to optimize scheduling [15]. The authors 
proposed BIP (binary integer program) method to 
describe the scheduling problem and assess the 
computational cost [16]. In order to meet the different 
service requirements, and the management for the 
deployment effectively, they proposed a requirements to 
meet service quality, reduce deployment costs [17]. An 
optimal allocation approach of virtual servers to 
appropriate data centers is proposed for a network made 
up of collections of data centers and user groups [18]. 
They proposed an approach for optimally organizing the 
distributed component computing in the Cloud computing 
environments [19].  
3. Proposed Method 
The purpose of this paper is proposed an adaptive 
method of eye gaze tracking system for the integrated 
cloud computing and mobile device environment. There 
are two problems. The first is user involuntary movement 
of the head .The second is that mobile device doesn’t 
have enough computing power; therefore, an integrating 
cloud computing system is proposed to overcome this 
limit. The procedure of tracking eye gaze is divided into 
multiple modules for the purpose of achieving good 
performance and adapting for heterogeneous 
environment.  
3.1. Modular Function 
Based on Figure 1, the proposed method is composed 
of three blocks. The first one is initialization and face 
detection. The image processing algorithm is used to 
extract the features in the second functionality block. The 
finally block accomplishes by using multilayer perceptron 
(MLP) to mapping the geometric visual features with the 
gaze pointer on the screen.   
In feature tracking procedure, estimation of the gaze is 
as following. First, blink detection procedure and eye 
locations are estimated through image difference. Second, 
the eye features are analyzed. Eight features are extracted 
from the input images as Eq. (1). 
     ]  [ yxy x yx  GGGGLG LG LCLCF      (1) 
where γ is the ratio between the major and the minor 
axes of a fitted ellipse. θ is the orientation of the ellipse. 
LC is the left corner position. LG is used as a local gaze 
vector by subtracting the left corner position from the eye 
center position. GG is a global gaze vector by subtracting 
the left corner position from the right corner position. The 
input dimension of input features reduced to eight. These 
geometric features are used to train the neural network. In 
  
 
Figure 3. Adaptive Feedback System 
4.2. Eyes Enlarge Screen 
An eyes enlarge screen is adopt to experiment the 
outcome. According to the coordinates of focus, the 
screen is magnified in the square area. This work is of 
help to elderly and disabled people to get information 
more easily. The eyes gaze coordinates is shown in Figure 
5. Figure 6 shows using the eyes gaze coordinates move 
the magnifying glass.  
 
 
Figure 4. Face detection in mobile device 
5. Conclusions and Future Works 
Neural network can effectively find the coordinates 
of eye sight; however, a mobile device must be 
considered the capacity of mobile devices such as 
computing power and memory space. Combining cloud 
computing and proposed method can get good 
performance by performing utility function. Besides, the 
proposed method decides that functionality is performed 
in cloud or mobile device adaptively based on device and 
network conditions. 
 
 
Figure 5. Eye Gaze Tracking  
 
 
Figure 6. Using Eye Gaze Enlarge Screen  
In the experimental scenario, the face and eyes are 
detected by mobile device and eye gaze is performed in 
the PC. The utility function, implementing neural network 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/25
國科會補助計畫
計畫名稱: 三維空間實景文字之偵測,切割與辨識
計畫主持人: 范國清
計畫編號: 97-2221-E-008-086-MY3 學門領域: 圖形辨識
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
