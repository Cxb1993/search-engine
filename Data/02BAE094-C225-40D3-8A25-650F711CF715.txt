子計畫在本年度之具體研究成果，第六節介紹與日方
合作所共同發表的論文內容。第七節是本計畫今年度
所舉辦之跨國研討會的相關介紹，第八節則為本計畫
國內同仁所成立之核心實驗室(core-lab)相關說明，第
九節為精簡結論。 
 
   II. 同仁赴日參訪深入交流 
 
本年度本計畫同仁赴日參訪深入交流的工作共分成兩
個部份，一是與東京大學就韻律模型的合作研究，一
是與 ATR 就相關領域作深入廣泛交流。此二部份分述
於下。 
 
2.1 與東京大學合作中文韻律模型研究 
在中文語音韻律模型及聲調辨認研究方面，主要是與
東京大學 Prof. Fujisaki 與 Prof. Hirose 合作，共
實際安排兩次長期（一個月以上）赴日訪問研究，分
別為： 
 
（1）2006 年暑假六月底至九月初，由王逸如教授帶
領博士班學生江振宇訪問日本東京大學，江振宇總共
待在東大兩個半月，並實際與東京大學 Prof. Hirose
之博士班研究生王曉東，共同合作研究以 tone 
nucleus 與韻律模型為基礎之中文連續語音聲調辨
認。此合作結果並已與日方聯名（co-author）投稿
兩篇會議論文，包括一篇以潛在韻律模型為基礎之自
動韻律狀態標記與聲調辨認，發表在 ICASSP＇2007 
[1]，與一篇以 tone nucleus 為基礎之聲調辨認，發
表在 IEICE-technology report [2]（詳細研究內容
與成果於第 VI 節關於聯合發表之部分另外詳細敘
述）。王逸如與江振宇在赴日訪問期間並順道拜訪東
京工業大學 Prof. Furui 與 ATR Prof. Nakamura 各
一天，互相做 presentation 與討論相關研究問題。 
 
(2) 2007 年五月，由鄭秋豫教授研究助理蘇昭宇訪問
研究東京大學 Prof. Hirose 一個月，主要研究題目
為 Fujisaki model 對應階層性語流韻律架構 HPG 
（Hierarchical Framework of Discourse Prosody)
在中文的分析與應用。 
 
 
2.2 與 ATR 就相關領域作深入交流 
陳柏琳教授於民國 95 年 8 月 1 日至 9 月 8 日間帶領李
琳山教授博士學生許長文、宮嵊益兩位同學赴日本
National Institute of Information and Communications 
Technology (NiCT)轄下 Advanced Telecommunication 
Research Institute(ATR)進行研究訪問，在這段期間與
ATR 的 Satoshi Nakamura 博士、Hideki Kashioka 博
士、Xinhui Hu 博士以及其他研究人員透過演講與研討
方式廣泛且深入地交流台日雙方在語音文件檢索與組
織、語音強健處理等研究領域的研究成果，以及規劃
將來在語音摘要研究的可能合作方式與題材。陳教授
一行人並且在這段期間訪問了 University of Tokyo 的 
Keikichi Hirose 和 Shigeki Sagayama 兩位教授、Tokyo 
Institute of Technology 的 Sadaoki Furui 教授、Kyoto 
University 的 Tatsuya Kawahara 教授以及 Nara Institute 
of Science and Technology 的 Kiyohiro Shikano 教授，
進一步深入瞭解日方近年來在語音相關研究的進展。 
 
 
III. 子計畫一：韻律、聲調及文句翻語音(Prosody, 
Tones and Text-to-Speech) 
 
今年度本子計畫的研究方向與成果主要分為三個主
題，包括（1）建立以潛在韻律模型為基礎之韻律狀
態自動標記核心技術，預備將來應用此核心技術於語
音辨認，合成與語者辨認，（2）中文語流階層式韻
律架構模型，與（3）基於潛在韻律分析之語音與語
者辨認，以下簡述各主題的研究內容。 
 
3.1.以潛在韻律模型為基礎之韻律狀態自動標記 
在這方面的研究主要是提出以一聯合（joint）潛在
韻律模型描述韻律標記、韻律特徵與語言特徵間的相
互關係[3]，其完整數學模型如下式所示： 
* *
,
,
,
1 2
,
, argmax ( , | , , , , )
argmax ( , , , , | , )
argmax ( , , | , , , ) ( , | , )
argmax ( | , , ) ( , | , ) ( | ) ( | )
P
P
P P
P P P P
=
=
=
≈
B P
B P
B P
B P
B P B P SP PD PE L T
B P SP PD PEL T
SP PD PEB P L T B PL T
SP B P T PD PEB L P B BL
 
 
其具有四個子模型，包含（1）音節基頻軌跡模型
 ， （2）停頓長度
 , （ 3 ） 韻 律 狀 態 轉 換
, , , 1 , , 1 , , 1( | , , , , ,k n k n k n k n k n k n k nP p B B t t t− −sp )+
)
)
1
, , , ,( , | ,k n k n k n k nP pd pe B l
, , 1 , 1( | ,k n k n k nP p p B− −  與（4）停頓語法模型 。
其中： 
2
, ,( |k n k nP B l )
 
（1）音節基頻軌跡模型，假設音節基頻軌跡的分佈
受前後文,聲調(t)，韻律狀態(p)與停頓(b)等潛在影
響因素（affecting factor,β）影響，如圖 1 所示： 
 
並以下列加成性數學模型表示： 
 
, , , , 1 , 1
, ,
, ,
,
r
k n Bk n k n k n k n
Bk n k n
f
t pk n tp
b
tp
− −
+
= + + +
+ µ
sp sp β β β
β
 
此外，我們並整合 HPG 韻律模型與 TTS 系統，製作一
中文氣象預報 TTS 合成展示系統，以展示 TTS 系統考
慮 HPG 架構的必要性。 
 
 
 
圖 4. A schematic representation of the 
Hierarchical linear model where residual 
between prediction and original values is 
regarded as the government from higher level 
instead of error. 
 
 
 
 
(a) 
 
(b) 
圖 5.Comparison of the synthesized pitch 
contour considering (a) without and (b) with 
PG-effect. 
 
 
3.3.基於潛在韻律分析之語音與語者辨認 
在韻律模型應用方面，我們主要是以潛在韻律分析應
用到語音與語者辨認方面，其方法是在傳統以頻譜為
主之語音與語者辨認系統中，考慮如下式之頻譜與韻
律之聯合模型如圖 6 所示，並進一步將韻律模型拆解
成數個子模型，尤其是包含 tone 與 break 模型，每
個 子 模 型 並 用 probabilistic latent semantic 
analysis（PLSA）方式萃取與韻律相關之潛在因素
（latent factor）之建立，並同時濾掉與韻律無關
之潛在因素。 
 
argmax ( | , )
argmax ( ) ( , | )
argmax ( ) ( | ) ( | )
argmax ( ) ( | ) ( | , )
W
W
W
W
W p W X F
p W p X F W
p W p X W p F W
p W p X W p F T B
∗ =
=
≈
=
 
 
 
 1
( | ) ( | ) ( | ) , ,
L
k i k l l i
l
P f d P f z P z d i k
=
= ∀∑
d1
d2
di
dN
…
…
z1
z2
zl
zL
.
f1
f2
fk
fK
..
..
P(zl| di)
P(fk| zl)
prosodic 
documents prosodic terms
prosodic 
states
.
…
…
. ..
...
…
…
. ..
..
…
…
. ..
..
…
…
.. ..
..
..
...
 
圖 6. The block diagram of the probabilistic 
latent semantic analysis (PLSA). 
 
我們將此方法應用到大字彙連續語音辨認，自發性語
音辨認，與語者辨認與確認上，分別處理語言模型，
講話不流利現象（disfluency），與通道強健性上，
皆有不錯之效果。 
 
 
 
 
 
IV. 子計畫二：語音辨認之強健性 
(Robustness in Speech Recognition) 
 
本子計畫主要針對語音辨認中所遭遇的不匹配問題，
在辨認系統中各個重要階段如訊號層面、參數層面、
模型層面（包含語音模型和語言模型等）、最佳路徑
蒐尋層面、以及系統層面，皆加以考量強健性的議
題，並發展相對應的技術以提高語音辨認的效能與穩
且驗證了所提出的倒頻譜高階動差正規化方法在環境
不匹配的條件下，能得到更穩健之語音辨識效能。 
 
4.3. 語音模型與語言模型 
在傳統語音辨識系統中，語音模型和語言模型常被假
設為互相獨立，分別去估測其對應之參數，再加以結
合計算分數。然而語音特徵與語言特徵相互之間有著
高度的相關性，在此我們提出以最大熵法則，於單一
事後機率模型中同時考慮語音及語言特徵，使得語音
和語言模型參數可於相同的訓練法則之下，同時達到
最佳化，而其間的相依特性也將被結合入參數之中
[7]。 
最大熵法則的基本概念是-盡量完整的描述我們所觀察
到的資訊，對於我們不知道的，將不對其做任何假設
而使其保持均勻分佈。利用最大熵法則同於結合語音
及語言模型，首先我們定義特徵函數(Feature Function, 
)來反應我們希望擷取的特性，在此我們延續傳統隱
藏式馬可夫模型與統計式 n-gram 語言模型，分別為其
定義特徵函數，其中包含狀態初始特徵、狀態轉移特
徵、混合權重特徵、K 階統計徵與 n-gram 特徵。利用
定義之特徵函數，由訓練資料中擷取初期個別對應之
統計量，並利用限制條件使所求的最大熵模型能符合
該統計特性，在滿足此限制條件下，我們將模型的熵
值最大化，最後求得整合式最大熵(Unified ME)模型如
下: 
f
∑∑ ∑
∑
′ =
=
′= LS W Fi ii
F
i ii
LSXWf
LSXWf
XWP
, 1
ALAL
1
ALAL
),,,(exp
),,,(exp
)( λ
λ  
其中 S、L 分別為狀態和混合成分序列， 為此最大
熵模型參數。 
AL
iλ
在此提出以最大熵法則同時結合語音及語言特徵，以
估測出一整合的事後機率模型，語音和語言模型間的
交互資訊也將被融入參數之中，同時利用最大熵法
則，我們也可將更複雜的語音或語言特性，簡單並有
效的集合到此整合性模型之中，以達到更強健的語音
辨識效能。 
 
4.4. 最佳路徑蒐尋 
在語音辨識中，給定一組語音樣本的特徵向量，我們
希望找出其對應的文字，然而該特徵向量中，不同維
度對於辨識理應擁有不同的鑑別能力，因此我們利用
熵(Entropy)值的計算對不同維度加以不同權重，以提
高辨識的可靠性。 
在本研究中[8]，對於某一個時間點的觀察樣本的某個
維度，對於不同類別都將有一相似度分數，若這些分
數彼此很相近，及其對應的熵值較大，反映出該時間
點下，此維度對於辨識有較高的不確定性，難以對不
同類別加以區別，因此我們給定一較小的權重值；相
反的，若其對應的熵值較小，則給定較大的權重。如
此動態的對不同維度加以權重調整，用以反應其鑑別
能力，將可有效的的提升辨識的強健性與正確性。 
4.5. 系統層面考量 
在大詞彙連續語音辨識中，利用不同模型參數或是搜
尋演算法，都會得到不同的辨識結果，而各種方法都
有其優缺點，因此在本研究中我們提出一系統整合的
方法，有效的將不同的系統的設定空間 (Hypothesis 
Space)加以整合，其中我們利用多種不同的結合法
則，包括 Consensus Score、Expected Phone Accuracy 
Score 以及 Time Frame Error 來擷取出一整合的辭圖
(Word Graph)，並以設定空間之概念合併於系統當
中，如此一來所有時間資訊都將被保留並使得重計分
數(Rescoring)更有彈性[9]。 
 
V. 子計畫三：新一代語音系統  
(Next Generation Spoken Language Systems) 
 
本子計畫執行的重點在發展新一代的語音系統技術，
包括了語音文件自動轉寫、多媒體內涵的理解與組
織、語音文件檢索、口語對話、以及分散式語音辨識
等。在本年度裡，我們主要研究成果包含(1)分散式語
音辨識、(2)自動語音文件摘要、(3)自動音素分段、(4)
語者確認與分群、(5)動態語言模型調適、(6)鑑別式聲
學模型訓練，分別於下面各次章節裡作介紹。 
 
 
5.1 分散式語音辨識 
分散式語音辨識(Distributed Speech Recognition)系統所
面臨的問題為：隨身攜帶的手持設備面臨多變且無可
預知的環境，環境雜訊(Environmental Noise)與壓縮帶
來的信號失真(Quantization Distortion)以及無線通道傳
輸所帶來的位元錯誤(Bit Errors)會互相加成起來，嚴
重影響分散式語音辨識的效能。因此，探討如何減低
碼本不匹配(Codebook Mismatch)、環境不匹配和無線
通道傳輸錯誤(Transmission Errors)對辨識率影響的強
健性研究便成為一個很重要的研究方向。本計畫針對
訓練聲學模型與量化碼本的語音取得環境和實際進行
語音辨識環境不匹配的問題，提出一套強健性的量化
法，可以根據使用者手持設備的計算能力與頻寬限
制，以及所處環境的背景訊噪比不同，動態調整資料
傳輸率，使得辨識系統效能達到最佳化。這種新發展
出來的以分佈統計為基礎的量化法(Histogram-based 
Quantization) [10]跳脫了以往傳統分割式向量量化法
(Split Vector Quantization)使用固定碼本計算距離的概
念，而是根據語音的分佈統計特性，動態訂出量化區
間，完全解決了雜訊環境下碼本不匹配的問題，同時
量化本身吸收了雜訊的干擾，對於極嘈雜環境和特性
詞與其歷史詞序列在機率潛藏主題空間的相近關係，
以估測候選詞出現機率。在中文大詞彙連續語音辨識
的實驗結果顯示，所提出的詞主題混合模型較現有潛
藏語意分析、機率式潛藏語意分析以及觸發對模型
(Trigger-Based Language Model, TBLM)在語言模型調
適上有較佳的表現 [18]。 
 
5.6 聲學模型鑑別式訓練 
聲學模型鑑別式訓練(Discriminative Training)旨在最小
化訓練語句的分類錯誤，而不僅是去逼近聲學模型參
數的真實密度分佈。因為鑑別式訓練同時考慮到聲學
模型的分類錯誤資訊，所以其語音辨識效能會比傳統
的最大化相似度訓練(Maximum Likelihood Training)來
的好。另一方面，近年來以邊際基礎(Margin-Based)的
模型估測法則在機器學習領域中已有高度的發展，例
如：支撐向量機(Support Vector Machine, SVM)，其以
邊際基礎的模型估測法則的中心思想，嘗試選出離決
定邊界 (Decision Boundary) 較近訓練樣本 (Training 
Samples)、利用最大邊際法則來訓練模型參數；最近
幾年，以邊際基礎的聲學模型估測在語音辨識上已有
不錯的成效。有別於傳統邊際基礎的聲學模型估測方
法，本研究提出不同層次的語音訓練樣本選取方法應
用在鑑別式模型訓練上，期望解決鑑別式訓練法則缺
少推廣能力的問題。傳統邊際基礎的模型估測方法是
建立在相似度定義域(Likelihood Domain)，而本研究
則是嘗試基於詞錯誤率定義域 (Word Error Rate 
Domain)來選取語句(Utterance)、音素(Phone)以及音框
(Frame)等三層次的語音訓練樣本。在中文大詞彙連續
語音辨識的實驗結果顯示所提出的方法能提升傳統最
小化音素錯誤(Minimum Phone Error, MPE)及最大化相
互資訊(Maximum Mutual Information, MMI)等兩種聲
學模型鑑別式訓練的效能 [19]。 
 
 
VI. 與日方合作完成之聯合發表(co-authored)的論文 
 
本年度本計畫共完成兩篇與日方聯合發表之論文，包
括一篇在 ICASSP＇2007 [1]，探討以聯合潛在韻律與
tone 模型為基礎之自動韻律狀態標記與 tone 辨認應
用，與一篇 IEICE-technology report，研究以 tone 
nucleus 為基礎之聲調辨認 [2]，分述於下。 
 
6.1. 以聯合潛在韻律與聲調模型為基礎之自動韻律
狀態標記與聲調辨認 
在此我們嘗試將潛在韻律模型應用到自動韻律狀態標
記與聲調辨認，首先我們將觀察到之 pitch contour
以下式之線性加成公式表示成具有數個潛在影響因素
（affecting factor）的公式： 
 
, ,, , k n k nk n k n t p
= + + +x y χ γ µ  
 
其中 與 都是二維向量 (mean and slope of 
logF0 contour of a syllable) 分別用來表示在 
utterance k 中 第 n-th syllable 的  原 始
（observed） 與正規化後（normalized） 的 pitch 
contours; ,  and 
,k nx ,k ny
µ ,k ntχ ,k npγ 則是語者，tone，
與 prosodic state，, {1, 2, 3, 4, 5}k nt ∈ , {1,2, , }k np P∈ "
的影響因素。 
 
我 們 將 正 規 化 後 的 參 數 以  Gaussian 
distribution 描述,則 可用下式描
述： 
,k ny
,( ; ,k nN y 0 R)
)
)
,k nx
 
, ,, , , ,( | , , ) ( ; ,k n k nk n k n k n k n t pP t p Nλ = + +x x µ χ γ R  
 
此 外 若 進 一 步 考 慮 韻 律 狀 態 轉 移 機 率
, , 1 , 1( | ,k n k n k nP p p B− − ，則此聯合模型可以定義出一
likelihood function 如下： 
 
, , ,
1 1
,1 , , 1 , 1
2
log ( | , , )
( ) ( | , )
k
k
NK
k n k n k n
k n
N
k k n k n k n
n
L P t p
P p P p p B
λ
= =
− −
=
=
⎡⎧⎪ ⋅⎢⎨⎪ ⎢⎩ ⎣
⎤⎫⎬⎥⎦⎭
∏∏
∏
x
 
 
在此式中 K 表示 total number of utterances; 而 
kN 表示 total number of syllables in utterance 
k. 
 
此 聯 合 模 型 之 訓 練 則 以 上 式 之 likelihood 
funciton ， 以 遞 迴 式 expectation and 
maximization（EM）演算法訓練，估出各影響參數
之數值。 
 
表 1、2 為訓練完成後，模型自動找出之 tone 與
prosodic state 的個別 affecting factor 與每個
prosodic state 所代表的意義，皆符合語言學
（linguistic）的意義。 
 
表 1. The learned affecting factors of the five 
tones. 
Tone 1 2 3 4 5
Mean (LogF0) 0.17 -0.09 -0.21 0.09 -0.13
Slope 0.00 0.01 -0.03 -0.03 -0.01  
圖 11 為我們所提出之以 maximum likelihood 與
Viterbi search 為基礎之 tone nucleus 萃取方法。 
 
圖 11. Illustration of tone nucleus extraction 
algorithm. 
 
藉由此自動切割所求得之 tone nucleus 則進一步求
取特徵參數後分別與以 multiple layer perceptron 
（MLP）與 hidden Markov model（HMM）方式辨認，
並與傳統方式做比較。圖 12 為我們所提出方法的與
傳統特徵參數在香港中文大學中文語料庫上做 tone
辨認的效能比較，可以看出以 tone nucleus 方式求
取特徵參數可以有較低的錯誤率。 
 
 
圖 12. Tone recognition performances in error 
rates. 
 
 
VI.中日跨國研討會 
(Taiwan-Japan Joint Workshop) 
 
今年本計畫於三月底邀請日本十位從事語音研究的專
家學者來台舉辦中日跨國研討會，以與國內語音相關
研究人士進行學術交流。此次中日跨國研討會由本計
畫主辦、中華民國計算語言學會及台灣大學電機系協
辦，於三月 29、30 日兩天於台灣大學博理館國際會議
廳舉辦，會中共安排了 2 個 keynote speech、12 個
invited talk、poster/demo session 及座談會各一個。研
討會共有 165 名國內學界及業界人士報名參加，其中
業界參與人士包括國內各大研究機構及從事語音研究
之公司，如中華電信研究所、工業技術研究院、台達
電等 50 人。 
此次研討會所邀請的日方專家學者計有：東京大學的
Hiroya Fujisaki 教授、東京理工大學的 Sadaoki Furui
教授、國家資訊研究所的 Shuichi Itahashi 教授、東京
大學的 Keikichi Hirose 教授、京都大學的 Tatsuya 
Kawahara 教授、NTT 通信實驗室的 Shoji Makino 博
士、ATR 暨 NICT 的 Satoshi Nakamura 博士、東京大
學的 Shigeki Sagayama 教授、奈良科技研究所的
Kiyohiro Shikano 教授、早稻田大學的 Yoshinori 
Sagisaka 教授，每位都是在國際上享有盛名之語音相
關研究專家學者。在台灣學者方面，除有本計畫中三
個子計畫的報告之外，還邀請了成功大學吳宗憲教授
代表本計畫以外的國內學者於會中發表專題演講。 
會中專題演講之題目包含：(1)日本學界之研究成果；
如：東工大 Furui 教授之 Question and Answering 
system 、 東 京 大 學 Hirose 教 授 之 Synthesizing 
Fundamental Frequency Contours in the Framework of 
Generation Process Model for Flexible Control of 
Prosody 、 京 都 大 學 Kawahara 教 授 之 Automatic 
Detection of Sentence and Clause Units from Spontaneous 
Speech，這些內容讓聽眾能瞭解日本學界語音方面在
QA、spontaneous speech recognition 及 prosody 方面的
成果，而東京大學 Fujisaki 教授所講授的＂ Fujisaki 
Model＂ 是從事語音合成研究人員之重課題、 早稻田
大 學 Sagisaka 教 授 之 Prosody Generation for 
Communicative Speech Synthesis 是極具前瞻性的新方
向 、 東 京 大 學 Sagayama 教 授 之 Seeking New 
Directions Toward Speech, Music, and Acoustic Signal 
Processing 是聲學模型在音樂信號分析之應用、國家
資訊研究所 Itahashi 教授之 On the Activities of Oriental 
COCOSDA and NII Speech Resources Consortium 則是
讓聽者充分瞭解亞洲地區語料庫之蒐集狀況，(2)日本
業界的研究成果及其發展之實用系統，如：ATR 暨
NICT Nakamura 博 士 之 Corpus-based Speech 
Translation、NTT 通信實驗室 Makino 博士之 Audio 
Linear Discriminant Analysis (C-LDA) For 
Constructing Robust Features In Speech Recognition”, 
in Proc. IEEE Int. Conf. Acoustics, Speech, Signal 
processing (ICASSP2007), Hawaii, USA, April 2007. 
[7] Chuang-Hua Chueh and Jen-Tzung Chien, 
“Maximum Entropy Modeling of Acoustic and 
Linguistic Features”, in Proc. IEEE Int. Conf. 
Acoustics, Speech, Signal processing (ICASSP2006), 
Toulouse, France, May 2006. 
[8] Yi Chen, Chia-yu Wan and Lin-shan Lee, “Entropy-
based Feature Parameter Weighting for Robust 
Speech Recognition,” in Proc. IEEE Int. Conf. 
Acoustics, Speech, Signal processing (ICASSP2006), 
Toulouse, France, May 2006. 
[9] I-Fan Chen, Lin-shan Lee, “A New Framework for 
System Combination Based on Integrated Hypothesis 
Space,” ICSLP 2006, pp. 533-536. 
[10] Chia-yu Wan, Yi Chen and Lin-shan Lee, “Three-
Stage Error Concealment for Distributed Speech 
Recognition (DSR) with Histogram-Based 
Quantization (HQ) under Noisy Environment” in Proc. 
IEEE Int. Conf. Acoustics, Speech, Signal processing 
(ICASSP2007), Hawaii, USA, April 2007. 
[11] Sheng-yi Kong and Lin-shan Lee, “Improved spoken 
document summarization using probabilistic latent 
semantic analysis (PLSA),” in Proc. IEEE Int. Conf. 
Acoustics, Speech, Signal processing (ICASSP2006), 
Toulouse, France, May 2006. 
[12] Sheng-yi Kong and Lin-shan Lee, “Improved 
summarization of chinese spoken documents by 
probabilistic latent semantic analysis (PLSA) with 
further analysis and integrated scoring,” in Proc. 
IEEE Workshop on Spoken Language Technology 
(SLT 2006), Aruba, Dec 2006. 
[13] Yi-Ting Chen, Suhan Yu, Hsin-min Wang, Berlin 
Chen, “Extractive Chinese Spoken Document 
Summarization Using Probabilistic Ranking Models,” 
in Proc. International Symposium on Chinese Spoken 
Language Processing (ISCSLP 2006), Singapore, 
December 2006. 
[14] Jen-Wei Kuo and Hsin-min Wang, “A Minimum 
Boundary Error Framework for Automatic Phonetic 
Segmentation,” in Proc. International Symposium on 
Chinese Spoken Language Processing (ISCSLP2006), 
Singapore, December 2006. 
[15] Jen-Wei Kuo, Hung-Yi Lo, and Hsin-min Wang, 
“Improved HMM/SVM Methods for Automatic 
Phoneme Segmentation,” submitted to 
Interspeech2007-EUROSPEECH. 
[16] Yi-Hsiang Chao, Wei-Ho Tsai, Hsin-min Wang, 
Ruei-Chuan Chang, “Improved Methods For 
Characterizing The Alternative Hypothesis Using 
Minimum Verification Error Training For LLR-Based 
Speaker Verification,” in Proc. IEEE Int. Conf. 
Acoustics, Speech, Signal processing (ICASSP2007), 
Hawaii, USA, April 2007. 
[17] Wei-Ho Tsai and Hsin-min Wang, “Speaker 
Clustering Based on Minimum Rand Index,” in Proc. 
IEEE Int. Conf. Acoustics, Speech, Signal processing 
(ICASSP2007), Hawaii, USA, April 2007. 
[18] Hsuan-Sheng Chiu and Berlin Chen, “Word Topical 
Mixture Models for Dynamic Language Model 
Adaptation,” in Proc. IEEE Int. Conf. Acoustics, 
Speech, Signal processing (ICASSP2007), Hawaii, 
USA, April 2007. 
[19] Shih-Hung Liu, Fang-Hui Chu, Shih-Hsiang Lin, 
Berlin Chen, “Investigating Data Selection for 
Minimum Phone Error Training of Acoustic Models,” 
in Proc. IEEE International Conference on 
Multimedia & Expo (ICME 2007), Beijing, China, 
July 2007. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
10:30 
10:30-
11:20 
Prosody Generation for Communicative 
Speech Synthesis 
Yoshinori Sagisaka,  
Waseda University 
11:20-
12:10 
Seeking New Directions Toward Speech, 
Music, and Acoustic Signal Processing 
Shigeki Sagayama,  
University of Tokyo 
Chair: 
Sin-Horng Chen, 
National Chiao Tung 
University 
12:10-
13:10 Lunch 
13:10-
14:00 
Synthesizing Fundamental Frequency 
Contours in the Framework of Generation 
Process Model for Flexible Control of 
Prosody 
Keikichi Hirose,  
University of Tokyo 
14:00-
14:50 
Sub-Project (Ι): Prosody, Tones and Text-
to-speech Synthesis 
Sin-Horng Chen, 
National Chiao Tung 
University 
Chiu-yu Tseng, 
Academia Sinica 
Yuan-Fu Liao, 
National Taipei 
University of 
Technology  
Chair: 
Jen-Tzung Chien,  
National Cheng Kung 
University  
14:50-
15:20 Break 
15:20-
16:10 Corpus-based Speech Translation 
Satoshi Nakamura,  
ATR and NICT 
16:10-
17:00 
New Speech Media Applied to Universal 
Communication 
Kiyohiro Shikano,  
Nara Institute of 
Science and 
Technology  
17:00-
17:50 
Sub-Project (II): Robustness in Speech 
Recognition 
Jen-Tzung Chien,  
National Cheng Kung 
University  
Chair: 
Yuan-Fu Liao, 
National Taipei 
University of 
Technology 
March 30 (Fri) 
09:00-
10:00 
Keynote Speech: Modeling the Control 
Process of Fundamental Frequency of 
Speech with Applications to Analysis and 
Representation of the Tone Systems of 
Some Tone Languages 
Hiroya Fujisaki,  
University of Tokyo 
10:00-
10:50 
On the Activities of Oriental COCOSDA 
and NII Speech Resources Consortium 
Shuichi Itahashi,  
National Institute of 
Informatics 
Chair: 
Chiu-yu Tseng, 
Academia Sinica 
10:50-
11:10 Break 
these evidences, a command-response model is derived that is capable of generating F0 contours of tone 
languages with an extremely high accuracy. Experimental results are shown to support the validity of the model 
for several languages including Mandarin, Cantonese, Thai, Vietnames, etc. Apart from it usefulness in phonetic 
studies and techonological applications, the model leads to a novel way of representing the phonological 
structure of the tone system of a tone language in terms of timing and polarity of tone commands.  
Invited Speakers from Japan and Their Lectures 
Prosody Generation for Communicative Speech Synthesis 
Yoshinori Sagisaka (Waseda University) 
A corpus-based prosody modeling is proposed aiming at F0 control for communicative speech synthesis. For 
input information in communicative speech synthesis, input word attributes are employed. Two experimental 
results are introduced to show the possibility of communicative speech prosody control from input word 
attributes. The results showed that F0 height, F0 dynamic patterns and duration could be consistently controlled 
by the word attributes. The positive-negative characteristics can be controlled by F0 height, while confident-
doubtful, allowable-unacceptable characteristics were reflected in F0 dynamic patterns and duration. Through 
these analyses, the correlations between perceptual impressions on output speech and input word attributes are 
turned to be useful for F0 characteristics prediction from input word attributes.  
Seeking New Directions Toward Speech, Music, and Acoustic Signal Processing 
Shigeki Sagayama (University of Tokyo) 
One of most important issues in speech recognition include robust speech recognition in noisy and reverberant 
environments. We present a model-based approach to this issue studied at the Department of IPC (Information 
Physics and Computing), Graduate School of Information Science and Technology, The University of Tokyo. 
Another issue is the usability of spoken dialog systems. We conducted "Galatea Project" to provide with a 
license-free open-source toolkit for building anthropomorphic spoken-dialog agent with face animation. On the 
other hand, speech technology has been highly developed and ready to be applied to other interesting areas. 
Particularly, music contains rich research areas to which major concepts in speech recognition can apply 
including spectral analysis, HMM, CSR, training algorithm, language models, and other related items. 
Harmonically constrained GMM is successfully utilized in multipitch analysis for automatic audio-to-MIDI 
conversion. HMM is used in rhythm analysis in combination of language model of rhythm vocabulary and 
unsupervised training for iteratively finding the fluctuation tempo. Ergodic HMM is suitable to represent the 
chord transition for finding the music tonality (key). Dynamic Programming is useful in automatic counterpoint 
to find a melody matching to the given melody with music theoretic constraints. Automatic fingering decision, 
automatic accompaniment and automatic harmonization to the given melody and automatic music composition 
for a given text are also well formulated with speech technology such as HMM. Statistical estimation algorithms 
such as EM algorithm is also useful in acoustic signal processing using microphone arrays especially in 
presence of background noise. The CSR framework best matches online recognition of handwritten Kanji and 
math expressions exploiting good combination of HMM and language models.  
Synthesizing Fundamental Frequency Contours in the Framework of Generation Process Model for 
Flexible Control of Prosody 
Keikichi Hirose (University of Tokyo) 
Introduction of firm criteria when searching optimum parameters has lead corpus-based methods to a great 
success in various applications. Currently, almost all speech synthesis systems adopt a scheme based on corpus-
based selection and concatenation of waveform (or acoustic parameter) segments. Although synthetic speech 
from these systems sounds quite natural, it still includes "strange" prosody. The major reason is that prosodic 
features such as fundamental frequency (F0) are processed in a similar way as segmental features are: frame-by-
frame manner. This may cause sudden F0 undulations, which are not characteristic of human speech. Prosodic 
features cover a wider time span than segmental features, and should be treated differently. Taking these into 
account, we have developed a corpus-based method of synthesizing F0 contours in the framework of the 
generation process model (henceforth F0 model) and realized speech synthesis in reading and dialogue styles, 
and with neutral and several basic emotions. By predicting the model commands instead of frame-by-frame F0 
values, sudden undulations in F0 movements do not occur in the generated F0 contours, still keeping acceptable 
(SRC) with the goal of creating future value in information media, particularly speech media. SRC aims at 
collection, distribution, investigation, research, and standardization of electronic data and software tools that are 
necessary for the development of science, education, and industry concerning speech. The consortium will 
contribute to the development of the information society through these activities. Based on these activities, it 
facilitates the distribution, promotion, and publicity of speech resources. The talk also explains the background 
of SRC's establishment and the outline of speech corpora development in Japan.  
Automatic Detection of Sentence and Clause Units from Spontaneous Speech 
Tatsuya Kawahara (Kyoto University) 
Detection of sentence and clause units in spoken documents such as lectures and meetings is vital in improving 
the readability and post-processing such as translation and summarization. This talk first presents two different 
approaches based on statistical language model (SLM) and support vector machines (SVM) for sentence 
boundary detection. The SVM-based approach realizes more effective and robust performance, achieving the 
best F-measure for the CSJ (Corpus of Spontaneous Japanese). Then, for more robust detection of clause units, a 
novel cascaded chunking strategy is presented. The method incorporates local syntactic dependency, and 
realizes even better performance in ASR outputs.  
Audio Source Separation based on Independent Component Analysis 
Shoji Makino (NTT Communication Science Laboratories) 
This paper introduces the blind source separation (BSS) of convolutive mixtures of acoustic signals, especially 
speech. A statistical and computational technique, called independent component analysis (ICA), is examined. 
By achieving nonlinear decorrelation, nonstationary decorrelation, or time-delayed decorrelation, we can find 
source signals only from observed mixed signals. Particular attention is paid to the physical interpretation of 
BSS from the acoustical signal processing point of view. Frequency-domain BSS is shown to be equivalent to 
two sets of frequency domain adaptive microphone arrays, i.e., adaptive beamformers (ABFs). Although BSS 
can reduce reverberant sounds to some extent in the same way as ABF, it mainly removes the sounds from the 
jammer direction. This is why BSS has difficulties with long reverberation in the real world. If sources are not 
"independent," the dependence results in bias noise when obtaining the correct separation filter coefficients. 
Therefore, the performance of BSS is limited by that of ABF. Although BSS is upper bounded by ABF, BSS 
has a strong advantage over ABF. BSS can be regarded as an intelligent version of ABF in the sense that it can 
adapt without any information on the array manifold or the target direction, and sources can be simultaneously 
active in BSS.  
Project Presentations by Taiwanese Speakers 
Sub-Project (Ι): Prosody, Tones and Text-to-speech Synthesis 
Sin-Horng Chen (National Chiao Tung University), Chiu-yu Tseng (Academia Sinica), and Yuan-Fu Liao 
(National Taipei University of Technology) 
The main concerns of this subproject are the prosody modeling of Mandarin speech and its applications. Three 
recent studies are reported in this presentation. They are: (1) An automatic prosody labeling method, (2) 
Hierarchical modeling of different prosody styles and boundaries, and (3) Applications of prosody modeling - 
explicit and latent prosodic modeling for speech/speaker recognition. They are summarized as follows. 
    (1) An automatic prosody labeling method: The new automatic prosody labeling method first introduces four 
models to describe the relationships of the prosody tags to be labeled (i.e., inter-syllable break type and syllable 
prosodic state), the prosodic features (i.e., syllable pitch contour and inter-syllable pause duration), and 
linguistic features of various levels. It then employs a sequential optimization procedure to estimate parameters 
of these four models and find all prosody labels. Experimental results on the Sinica Tree-Bank corpus showed 
that most prosodic tags labeled were meaningful and the estimated parameters of these four models matched 
well with our a priori knowledge about Mandarin prosody. 
    (2) Hierarchical modeling of different prosody styles and boundaries: The top-down HPG framework (Tseng, 
2004, 2005, 2006) found with quantitative evidences that hierarchical constrains from higher level discourse 
information that forms the necessary cross-phrase semantic cohesion is key to fluent speech prosody. Mandarin 
prosody is in fact layered and cumulative contributions from lexical (tone), syntactic (intonation) to semantic 
(discourse) prosody. Cross-phrase intonation modifications are systematic and predictable instead of random 
Posters and Demos by Taiwanese Presenters 
Posters:  
P1: From One Base Form (HPG) to Multiple Prosody Outputs - Systematic but Dynamic Higher-Level 
Contributions and Distributions 
Chiu-yu Tseng and Zhao-yu Su (Academia Sinica) 
We proved from earlier corpus analyses of Mandarin (Tseng et al, 2004; 2005; 2006) that fluent speech prosody 
contains higher level discourse information above IU that could not be overlooked. Our Hierarchical framework 
of Prosodic Phrase Grouping (HPG) specifies how layered contributions cumulatively make up cross-phrase 
output prosody, and accounted for how individual phrase intonations adjust systematically to yield fluent speech 
prosody. Modular acoustic simulation models correlating to HPG were also constructed for speech synthesis 
applications. To test whether distribution of contribution from each prosodic layer is fixed or dynamic within 
and across speakers and speaking styles, and whether the HPG framework serves as a base form on which 
various styles of fluent speech prosody could be built, we further analyzed read speech of different literary 
styles from rigid (classical poetry) to semi-rigid (Chinese classics) surface prosodic formats into three categories, 
namely, 1.regular 2.semi-regular and 3.irregular. The results showed that the distribution of contribution 
patterns from the PPh (Prosodic Phrase) level to the PG (Prosodic Phrase) level varied systematically with 
respect to different literary style. The more regular the style is, the bigger the planning templates are used; and 
the more contributions from higher level information to output prosody. Systematic but significantly distinct 
distribution patterns are found across prosodic layers, thus proving how varied distribution patterns of layered 
contributions on one base form, i.e., the HPG framework, would be sufficient to generate various output 
prosody styles.  
P2: Boundaries and Boundary Properties in Mandarin Fluent Speech Prosody - the HPG Perspectives 
and Implications 
Chiu-yu Tseng and Chun-Hsiang Chang (Academia Sinica) 
The aim of the present study is to investigate whether (1.) acoustic properties in relation to boundary breaks are 
also boundary cues in Mandarin fluent speech, (2.) the HPG (Hierarchical Prosody Group) perspectives (Tseng 
et al, 2004; 2005; 2006) could account for boundary variations found in speech data, and (3.) the results 
obtained bear any technological implications. Note in our fluent speech corpora COSPRO (Tseng et al, 2005), 
the annotation design stressed perceived boundary breaks between prosodic units. The rationale being that hand 
annotated outcomes would enabled us to study the inconsistencies/gaps between human speech perception and 
the physical data. We found as expected relatively wide distribution of pause durations for all labeled breaks, 
most notably B3's (from 0 to over 300 msec), and were therefore interested why some very short pauses were 
consistently judged as a B3 across transcribers. We further analyzed duration and intensity of the immediate 
neighboring prosodic units PW (Prosodic Word) of B3's in detail, compared them with B2's; and found different 
corresponding patterns in these acoustic parameters. Thus it became clear that to the human ear, B2's and B3's 
are distinguished from each other not by the duration of the following pauses alone, but by the duration and 
intensity patterns of the preceding PW as well. Accordingly, we included factors of duration and intensity to 
revise and fine-tuned the linear regression model used, and recalculated contribution predictions from the 
Prosodic Word (PW) layer to final prosody output under our HPG framework (Tseng et all, 2005, 2006). Our 
results are not only consistent with the actual distribution of breaks, but also improve the total correlation by 
3%-10%. Our HPG framework could now better account for layered-and-cumulative contributions that make up 
output prosody, and further explain why in fluent speech boundary breaks involve more than simply pause 
duration, why acoustic properties of the immediate neighboring prosodic units are to be included, why 
extremely short pauses are consistently labeled as B3's by humans, and finally why B3's are now predictable. 
We believe these significant findings could be directly applied to automatic annotation as well as speech 
recognition.  
P3: Latent Prosody Model of Continuous Mandarin Speech 
Chen-Yu Chiang (National Chiao Tung University), Xiao-Dong Wang (University of Tokyo), Yuan-Fu Liao 
(National Taipei University of Technology), Yih-Ru Wang (National Chiao Tung University), Sin-Horng Chen 
(National Chiao Tung University), and Keikichi Hirose (University of Tokyo) 
beamforming so that the interference is suppressed. Following the microphone array, a post-processor performs 
the speech enhancement to further reduce the additive noise. Techniques of minima controlled recursive 
averaging (MCRA) and log-spectral amplitude (LSA) estimation are employed for noise estimation and speech 
enhancement. The MCRA process is repeated twice to perform a two-cycle noise estimation. The experiment 
shows that this two-stage process is effective in removing the additive noise.  
P8: Silence Energy Normalization for Robust Speech Recognition in Additive Noise Environments 
Chung-fu Tai and Jehi-weih Hung (National Chi Nan University) 
The energy parameter has been widely used as an extension to the basic features of mel-frequency cepstral 
coefficients (MFCCs) to improve the recognition accuracy in speech recognition. In this paper, a simple and 
effective approach for energy normalization for silence (non-speech) portions in an utterance is proposed. This 
approach, named as silence energy normalization (SEN), sets the log-energy of the non-speech frames to be a 
small constant, while keeps that of other frames unchanged. In the experiments conducted on AURORA2 
database, we showed that SEN provides an averaged word error rate reduction of 34.9% and 44.6% for Test 
Sets A and B when compared with the baseline processing. It was also shown that SEN outperforms similar 
approaches like energy subtraction (ES) and feature vector selection (FVS). Finally, we showed that SEN can be 
integrated with cepstral mean and variance normalization (CMVN), to achieve further improved recognition 
performance.  
P9: A Reference Model Weighting-based Method for Robust Speech Recognition 
Yuan-Fu Liao (National Taipei University of technology) 
In this poster a reference model weighting (RMW) method is proposed for fast hidden Markov model (HMM) 
adaptation which aims to use only one input test utterance to online estimate the characteristic of the unknown 
test noisy environment. The idea of RMW is to first collect a set of reference HMMs in the training phase to 
represent the space of noisy environments, and then synthesize a suitable HMM for the unknown test noisy 
environment by interpolating the set of reference HMMs. Noisy environment mismatch can hence be efficiently 
compensated. The proposed method was evaluated on the multi-condition training task of Aurora2 corpus. 
Experimental results showed that the proposed RMW approach outperformed both the histogram equalization 
(HEQ) method and the method proposed in European Telecommunications Standards Institute (ETSI) 
distributed speech recognition (DSR) standard ES 202 212.  
P10: Latent Prosody Analysis for Robust Speaker Identification 
Yuan-Fu Liao (National Taipei University of technology) 
Handsets that are not seen in the training phase (unseen handsets) are significant sources of performance 
degradation for speaker identification (SID) applications in the telecommunication environment. In this poster, a 
novel latent prosody analysis (LPA) approach to automatically extract the most discriminative prosody cues for 
assisting in conventional spectral feature-based SID is proposed. The concept of the LPA approach is to 
transform the SID problem into a full-text document retrieval-like task via (1) prosodic contour tokenization, (2) 
latent prosody analysis, and (3) speaker retrieval. Experimental results of the phonetically balanced, read-speech, 
handset-TIMIT (HTIMIT) database demonstrated that the proposed method of fusing the LPA prosodic feature-
based SID systems with maximum likelihood a priori handset knowledge interpolation (ML-AKI) spectral 
feature-based SID outperformed both the pitch and energy Gaussian mixture model (Pitch-GMM) and the 
bigram of the prosodic state (Bigram) counterparts for both cases of counting all and only unseen handsets.  
P11: Factor Analyzed Subspace Modeling and Selection 
Chuan-Wei Ting and Jen-Tzung Chien (National Cheng Kung University) 
We will present a novel subspace modeling and selection approach for noisy speech recognition. In subspace 
modeling, we develop factor analysis (FA) representation of noisy speech, which is a generalization of signal 
subspace (SS) representation. Using FA, noisy speech is represented through the extracted common factors, 
factor loading matrix and specific factors. The observation space of noisy speech is accordingly partitioned into 
a principal subspace containing speech and noise and a minor subspace containing residual speech and residual 
noise. We minimize the energies of speech distortion in principal subspace as well as minor subspace so as to 
estimate clean speech with residual information. More attractively, we explore optimal subspace selection via 
solving hypothesis test problems. We test the equivalence of eigenvalues in minor subspace to select subspace 
dimension. To fulfill FA spirit, we also examine the hypothesis of uncorrelated specific factors/residual speech. 
P16: A Minimum Boundary Error Framework for Automatic Phoneme Segmentation 
Jen-Wei Kuo and Hsin-min Wang (Academia Sinica) 
This paper presents a novel framework for HMM-based automatic phoneme segmentation that improves the 
accuracy of placing phoneme boundaries in speech utterances. In the framework, both training and segmentation 
approaches are proposed according to the minimum boundary error (MBE) criterion, which tries to minimize 
the expected boundary errors over a set of possible phoneme alignments. This framework is inspired by the 
recently proposed minimum phone error (MPE) training approach and the minimum Bayes risk decoding 
algorithm for automatic speech recognition. To evaluate the proposed MBE framework, we conduct automatic 
phoneme segmentation experiments on the TIMIT acoustic-phonetic continuous speech corpus. MBE 
segmentation with MBE-trained models can identify 80.53% of human-labeled phoneme boundaries within a 
tolerance of 10 ms, compared to 71.10% identified by conventional ML segmentation with ML-trained models. 
Moreover, by using the MBE framework, only 7.15% of automatically labeled phoneme boundaries have errors 
larger than 20 ms.  
P17: Phonetic Transcription Using Speech Recognition Technique Considering Variations in 
Pronunciation 
Min-Siong Liang (Chang Gung University, Taiwan), Ren-Yuan Lyu (Chang Gung University, Taiwan), and 
Yuang-Chin Chiang (National Tsing Hua University) 
We propose a new approach for performing phonetic transcription of speech and text that combines automatic 
speech recognition (ASR) and grapheme -to- phoneme (G2P) techniques. By augmenting the text with speech 
and using automatic speech recognition with a sausage searching net constructed from multiple text 
pronunciations corresponding to human speech utterance, we are able to reduce the effort for phonetic 
transcription. By using a multiple pronunciation lexicon, a transcription error rate of 12.74% was achieved. 
Further improvement can be achieved by adapting the pronunciation lexicon with pronunciation variation (PV) 
rules and an error rate reduction of 17.11% could be achieved.  
Demos:  
D1: A Mandarin TTS System for Weather Forecast with Integrated HPG Prosody Model 
Yong-cheng Chen, Hsin-min Wang, and Chiu-yu Tseng (Academia Sinica) 
HPG (Hierarchical Prosody Group) is essential to characterize the prosody for Mandarin fluent speech. 
Evidence of HPG prosody model has been found both in adjustments of F0 contours and temporal allocations 
within and across phrases. We demonstrate a Mandarin TTS system for weather forecast with an integrated 
HPG prosody model for duration, F0, intensity, and break predictions. The database consists of a weather-
forecast corpus with 7062 syllables and 1292*3 syllable-tokens chopped off specially designed three-phrase 
carrier sentences. Since temporal allocations and rhythmic structure in speech flow are carefully dealt with, the 
TTS system is capable of converting long paragraph text input into natural synthesized speech output.  
D2: Mandarin Singing Voice Synthesis with Improved Harmonic-plus-noise Model 
Hung-Yan Gu and Huang-Liang Liao (National Taiwan University of Science and Technology) 
Here, we study to improve and extend the harmonic plus noise model (HNM) in order to synthesize more 
natural and clearer Mandarin singing voice. A few improvements are made. For examples, the determination of 
the maximum voiced frequency (MVF) is now more robust, and the estimation of the parameter values of the 
harmonics are now more precise. In addition, the model is extended to provide more abilities. For examples, 
synchronization of phase delay is added to synthesize more natural voice, phoneme duration determination is 
made with ADSR concept to promote fluency, and vibrato and portamento processing are added to synthesize 
more expressive singing voice. With the improvements and extensions mentioned, a real-time Mandarin singing 
voice system has been built now.  
D3: SoVideo - A Mandarin Chinese Broadcast News Retrieval System 
Hsin-min Wang, Shih-Sian Cheng, Yong-cheng Chen, and Yi-Ting Chen (Academia Sinica) 
SoVideo is a Mandarin Chinese broadcast news retrieval system. The system is based on technologies such as 
large vocabulary continuous speech recognition for Mandarin Chinese, automatic story segmentation, and 
information retrieval. Currently, the database consists of more than 400 hours of broadcast news, which yielded 
10,343 stories by automatic story segmentation.  
D7: Network Communication Platform using Distributed Speech Recognition (DSR) frontend, VoIP, Text 
Chat & Galatea Face Simulation 
Yuan-Fu Liao (National Taipei University of technology) 
We currently are developing a network communication platform using distributed speech recognition (DSR) 
frontend, UDP-based VOIP, TCP/IP-based text chat and Galatea face simulation module. In fact, this is the first 
step to take the advantage of the open-source Galatea toolkit to develop a Mandarin spoken dialog system over 
the internet.  
D8: Speaker Verification-based Security System 
Yuan-Fu Liao (National Taipei University of technology) 
We recently developed a speaker verification-based security system. It combines the technologies of keyword 
spotting and speaker verification to give an authorized user a convenient way to quickly (in about 10 seconds) 
pass security controls.  
 
3. 中日跨國研討會(Taiwan-Japan Joint Workshop)活動照片 
 
z 2007/3/28 參觀活動 
 
新竹地區參訪--參訪工研院電通所 
 
 
新竹地區參訪--參訪聯發科技公司 
 
 
中日研討會之 Keynote speech -- Professor Hiroya Fujisaki 
 
 
中日研討會的 keynote speech -- Professor Sadaoki Furui 的演講 
 
 
Invited talk -- Professor Keikichi Hirose 
 
 
Invited talk -- Professor Tatsuya Kawahara 
 
 
Invited talk -- Professor Shuichi Itahashi 
 
 
Invited talk -- Professor Shigeki Sagayama 
 
  
會場聽眾 
 
 
coffee break 
 
 
午餐 
 
