1具平面運動接頭複重疊物件之辨認 (2/2)
Recognition of Multiple Occluded Objects having Planar Kinematic Joints (2/2) -
Homomorphic Graph Matching of Articulated Objects
期末成果報告
計畫編號：NSC 94－2212－E－110－003
執行期間：94 年 8 月 1 日至 95 年 7 月 31 日
主持人：何應勤 中山大學機械與機電所
中文摘要
近年來，在仿射轉換下之物體辨識文獻陸續被提
出。然而這些文獻僅止於探討圖形同型單物問題，在
仿射轉換下求解圖形同型多物問題至今尚未被討論。
因此本計劃提出結合遺傳演算法與霍菲爾類神經網路
優點的方法來解此條件下之物體辨識問題。首先，利
用遺傳演算法來找出在影像中包含所有模型姿態之近
似解，然後重覆執行霍菲爾類神經網路以找出模型在
影像中之每一姿態。此法可用來解重疊物體辨識問
題，並可求得多個相同模型在一張影像中之同型多物
比對問題。本系統可用來辨識關節物體，即使我們事
先不知影像中是否有關節物體。藉由實驗結果可驗證
本提議法是具強健性的。
關鍵字：仿射轉換、霍菲爾類神經網路、遺傳演算法，
物體辨識、圖形同型多物問題
1. Abstract
In the last few years, several attempts have been
made to the study of object recognition under affine
transformation, but all these studies have concentrated on
graph isomorphism. There has not been any discussion of
solving the graph homomorphism problem under affine
transformation to date. Therefore, in this project, an
integrated approach, which combines the advantages of
both the genetic algorithm and Hopfield neural network,
is proposed for solving object recognition under this
condition. The Genetic algorithm is first used, to find the
near-optimal solution including all the poses of the
model in the scene. Then the Hopfield network is
implemented and repeated to find each pose of the model.
This method can solve occluded object recognition
problems, and it can also obtain the homomorphic
mapping indicating multiple occurrences of a model in
the scene. The system is used to recognize articulated
objects, and we do not need to know in advance that
there is one in the scene. Through experiments, we can
demonstrate that the proposed method is robust.
Keywords: Affine transformation; Hopfield network;
Genetic algorithms; Object recognition; Graph
homomorphism.
2. Project Motives and Goals
Object recognition has been of considerable interest
in the field of industrial automation. The task of
recognizing objects will be more difficult as multiple
objects overlap each other, and this problem is often seen
in robotic applications. In such cases, multiple objects or
overlapping objects cause many problems in identifying
and locating the objects in the workcell of a robot.
Moreover, most of the investigations to date have
concentrated on the recognition of rigid objects.
However, many of the standard tools, as well as the
industrial robots, possess kinematic degrees of freedom.
Thus, in order to have a practical object recognition
system, one has to step further to acquire the ability of
handling articulated rigid objects. An articulated object is
an object composed of a set of rigid components which
are connected by joints and allow certain kinematic
degrees of freedom. These joints can be, for example,
prismatic joints that allow relative translation between
components, or revolute joints that allow relative rotation
of the components about the joint. The appearance and
structure of an articulated object may change from view
to view. Most of the object recognition systems do not
have the capability of recognizing articulated objects.
The homomorphic mapping, which permits multiple
occurrences of the model appearing in the scene, was
considered by Suganthan et al. [1-3]. In Suganthan’s
method, the uniqueness constraint in the energy function
was relaxed to obtain relational homomorphism.
Consequently, many spurious corresponding pairs were
generated. Recently, Li and Lee [4] have presented an
accumulative Hopfield matching to deal with the graph
homomorphism problem under similarity
transformations. In that paper, the authors first divided
the scene graph into many subgraphs, and then
performed Hopfield network to obtain the subgraph
isomorphism. The final results were deduced by
accumulating the solutions of all small sub-networks.
However, their methods cannot recognize the object
under affine transformations.
As far as the study of object recognition is
3gene in 2-D GA corresponds to a neuron in the Hopfield
network. The 2-D GA procedure can be found in [9]. For
solving homomorphic graph matching, the fitness
function of the GA needs to be modified. The fitness
function is constructed based on the compatibility
measure. Because the goal of our Hopfield-GA algorithm
is to search the point correspondences of all poses to see
if there are multiple occurrences in a scene, the
uniqueness constraint in the second term of Eq. (1) can
be relaxed. Besides, our GA enforces that each node in
the scene needs to match one node in the model.
Therefore, the uniqueness constraints in the first and the
third terms of Eq. (1) can be also omitted. Thus, the
fitness function F of the GA in this project is simplified
as follows:
   





















i k
ij
ij
ij
kl
kl
kl
jlikikjl VVCF
1
1
1
1
. (5)
3. 2 An Integrated Scheme for Matching
The flow chart for integrating the genetic algorithm
and the Hopfield neural network by our scheme is shown
in Fig. 1. The procedures of the proposed method are as
follows. At first the GA is performed. Now, the result of
the GA, referred to as the initial candidate matrix,
includes almost all point correspondences of each pose.
We then choose some active neurons in the initial
candidate matrix as the seeds of the initial state of
Hopfield network. We call this procedure DHN
preprocessing. These seeds were chosen based on the
individual fitness values, Fik, of corresponding active
neurons in the initial candidate matrix. Once the initial
state of the Hopfield network is determined, the network
can be immediately started. The output of the DHN
represents the matching result of one pose. In this project,
we use a post-DHN refinement to enhance the matching
result. The method used here is similar to that in Ref. [9].
After applying the post-DHN process, the refined
result may represent one pose of the model in the scene.
Because the objective of the proposed method is to
recognize multiple occurrences of the model in the scene,
DHN must be repeated to find other possible poses of the
model until a stopping criterion is reached. In this paper,
the stopping criterion is constructed based on the
following two conditions.
1. We stop when the resulting active neurons of the
post DHN refinement have no more than three
consecutive matching neurons.
2. Similarly, if the resulting active neurons have no
more than three matching neurons that are at the
corresponding positions of the active neurons in the
current initial candidate 2-D matrix, we also stop.
If a result of the DHN does not meet the above
criteria, we say the model and the scene are matched and
the whole scheme continues. Of course we need to
record the output of the post DHN refinement as one
pose of matching result, and then reinitialize the initial
candidate matrix to search for other poses. The initial
candidate matrix is reinitialized by setting the
corresponding neurons, which are at the same positions
of the matched neurons, to zero. Then we implement
DHN again. The entire process runs until one of the
stopping conditions is satisfied. Note that the last output
of this DHN refinement is a spurious solution, and is to
be discarded.
3.3 Recognition of the Articulated Objects
Multiple poses of a model appearing in the scene can
be interpreted as simply different occurrences of the
model or subparts of an articulated object with different
articulation. Sometime, we need to identify between
these two cases. For an articulated object with two
subparts, the relative geometric relation between the
subparts may be different in the model image and the
scene image. The articulated object may be extracted
from the scene by two poses of the model. These two
poses of the model can be represented by two affine
transformation matrices T1 and T2. Thus, for a given
point P in the model, the transformed point P1 and P2 in
the scene can be formulated as
P1 = T1P, P2 = T2P. (6)
If the model is an articulated object with a rotary joint or
a prismatic joint, there is a simple transformation relation
between P1 and P2. Thus, the transformed point P2 also
can be formulated as
P2 = T1T3P, (7)
where T3 is a translation transformation matrix or a
rotation transformation matrix. From Eq. (6) and (7), we
obtain










 
100
333
333
2212
2111
2
1
13 y
x
taa
taa
TTT . (8)
If the relationship between the two parts is a prismatic
one, then











100
10
01
3
3
3 y
x
t
t
T , (9)
where (
3xt , 3yt ) is the linear relative translation between
the two parts of the articulated model. An articulated
model with the prismatic joint can be identified when
elements, {a11, a12, a21, a22}, of the two affine
transformation matrices T1 and T2 are all the same.
If the kinematic relationship between the two parts
of an articulated object is a rotary one, T3 can then be
represented as a transformation matrix describing a
rotation around a fixed revolute joint located at Pc(cx, cy).
5they all will meet the stopping criterion in the first output
stage of our Hopfield network. Using the model in Fig.
4(a) and the scene in Fig. 5(e), we can find three poses of
the model in the scene, as shown in Table 2. From the
second and the third poses we can know that the model
object is actually an articulated one. The coordinates of
the revolute joint are (294, 136), and the angle
displacement is 46.9° . In the same way, we can easily
find that the model in Fig. 4(a) is an articulated object
with revolute joint in Fig. 5(a) and the model in Fig. 4(c)
is an articulated object with prismatic joint in Fig. 5(b),
respectively. Fig. 6 shows the transformed contours of
the matched model overlaid onto the scene. The cross‘+’
shown in Figs. 6(a) and 6(e) mark the computed
positions of the revolute joints. Note that there is some
error in calculating cx, cy for Figs. 5(a) and 5(e). This is
because we use a 2-D affine transformation to
approximate the 3-D perspective transformation.
Therefore, the locations of the feature points in the scene
are somewhat distorted. Moreover, in some cases, some
excessive feature points are introduced and some others
are missing in the scene images. However, regardless of
all these difficulties, our method still provides the
accurate answers. The proposed integrated scheme has
proven itself to be a robust and useful approach for
object recognition.
4. Conclusions
We have just presented a methodology of
implementing the GA-Hopfield integrated approach for
homomorphic graph matching. Since the Hopfield
network is sensitive to its initial state, so we use GA to
search globally a good starting point for the DHN
learning algorithm. The GA part of our methodology
runs only once, no matter how complex the scene is, to
obtain a near-optimal solution including all possible
point correspondences. Then, the DHN may need to run
several times if multiple occurrences of a model are
detected in the scene. The DHN is used to extract exact
point correspondences of each pose solution from the
approximate near-optimal solution. The proposed
method is capable of recognizing multiple occurrences of
a model in the scene. Moreover, it can identify
articulated objects, without knowing in advance their
existence. In our experiments, feature points have been
extracted based on relative curvatures, and they may be
slightly distorted under affine transformations. However,
the results on hand-tool recognition show that our
method is robust enough to tolerate most of these errors,
while providing by far the most accurate answers.
5. References
[1] P.N. Suganthan, E.K. Teoh, D.P. Mital, Hopfield
network with constraint parameter adaptation for
overlapped shape recognition, IEEE Trans. Neural
Networks 10 (1999) 444–449.
[2] P.N. Suganthan, E.K. Teoh, D.P. Mital, Pattern
recognition by homomorphic graph matching using
Hopfield neural networks, Image and Vision
Computing 13 (1995) 45-60.
[3] P.N. Suganthan, E.K. Teoh, D.P. Mital, Pattern
recognition by graph matching using the potts mft
neural networks, Patten Recognition 28 (1995)
997-1009.
[4] W.J. Li, T. Lee, Object recognition and articulated
object learning by accumulative Hopfield matching,
Pattern Recognition 35 (2002) 1933-1948.
[5] A. Beinglass, H.J. Wolfson, Articulated object
recognition, or: how to generalize the generalized
Hough transform, in: Proceedings of IEEE
Conference on Computer Vision and Pattern
Recognition, 1991, pp. 461-466.
[6] Y. Hel-Or, M. Werman, Recognition and
localization of articulated objects, in: Proceedings
of IEEE Workshop on Motion of Non-rigid and
Articulated Objects, 1994, pp.116-123.
[7] Y. Hel-Or, M. Werman, Constraint-fusion for
interpretation of articulated objects, in:
Proceedings of IEEE Conference on Computer
Vision and Pattern Recognition, 1994, pp. 39-45.
[8] B. Bhanu, J. Ahn, A system for model-based
recognition of articulated objects, in: Proceedings
of International Conference on Pattern
Recognition, Vol. 2, 1998, pp. 1812-1815.
[9] C.C. Huang and I. Her, An integrated
approach containing genetic algorithm and
Hopfield network for object recognition
under affine transformations, IEICE Trans.
Inf. & Syst., E87-D (2004) 2356-2370.
Fig. 1 The flow chart of our integrated approach.
Hopfield’s
DHN
Output all poses of
the matching result
DHN
Post DHNReinitialize the initial
candidate matrix
Satisfy stopping
criterion?
70
1112
8
2 4
610
9
20
16
14
24
26
1822
28
30
32
34
36 38
40
42
0
11
12
8
2
5
4
6
10
1
20
17
16
14
24
26
18
2228
30 32
12
8
2
46
10
20
17
16
24
26
18
22
28
0
30
32
311
(a) (b) (c)
0
12
8
2
4 6
10
20
17
16
14
24
26
18
22
28
30
32
34
36
38
40
41
42
43
44
46
48
50 52
54 0
12
8
2
4
6 10
20
16
14
24
22
28
30 3234
36
38
40
42 46
48
50
52
12
8
2
4
6
10
9
1920
14
18
22
28
0
30
31
32 34
36
37
(d) (e) (f)
Fig. 5 Contours and feature points of the scenes.
(a) (b) (c)
(d) (e) (f)
Fig. 6 Matching results of the scenes, with gray lines denoting the scene contours, and the dashed lines denoting the
transformed contours of the matched models.
