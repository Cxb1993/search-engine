 1
行政院國家科學委員會專題研究計畫完整報告 
基於超音波醫學影像即時分析診斷系統的數位服務之研發 
E-service research and development based on an ultrasonic medi-
cal imaging real-time analyzer-diagnosis system 
計畫編號：NSC 96-2221-E-155-057-MY3 
執行期限：96 年 8 月 1 日至 99 年 7 月 31 日 
 主持人：陳永盛 元智大學電機工程學系 
E-mail：eeyschen@saturn.yzu.edu.tw 
計畫參與人員：吳瑞珍、朱奎丞、郭奕廷、盧台威、 
       杜文豪、李保憲、饒哲維 
元智大學電機工程學系 
 
一、中文摘要 
 
 基於超音波影像診斷使用日趨頻繁，
醫院對於這方面的電腦輔助判讀與數位服
務需求日殷，本研究報告介紹基於超音波
醫學影像即時分析診斷系統的數位服務之
研發成果。在電腦輔助判讀方面，目前腫
瘤全自動偵測驗算法具備自動偵測多個腫
瘤的能力是公認的難題之一，欲建置有用
的腫瘤診斷醫療數位服務系統，全自動偵
測演算法的開發是必須的。除了靜態超音
波影像偵測演算法開發外，序列超音波影
像處理也是相當重要，其有效的比對與追
蹤有利於立體超音波影像顯示演算法的開
發，其亦能有助於醫生臨床的分析與診
斷。在數位服務方面，則探討規劃系統之
相關模組設計，以期所設計的系統可以對
臨床超音波影像進行即時分析診斷、並可
透過網際網路進行生醫資訊的流通以達到
健康管理與醫療照護的目的。雖然研究成
果尚未完全達到預期目標，但整體而言，
相關基礎已經建立，且具有學術價值；而
其應用價值，加以時日，亦能有所期待。 
 
關鍵詞：影像處理、超音波影像、電腦輔
助診斷、電腦圖學、數位服務、網站技術 
 
Abstract 
 
Due to the use of ultrasonic medical 
imaging diagnosis is more frequently at pre-
sent, the needs of computer-aided diagnosis 
and e-service are more significant at hospital. 
We reports the results of this project: 
“e-service research and development based 
on an ultrasonic medical imaging real-time 
analyzer-diagnosis system.” On the aspect of 
computer-aided diagnosis, one of the most 
difficult topics is to design a fully automatic 
algorithm which can detect multiple tumors 
from an ultrasonic medical image. It is nec-
essary to develop such a fully automatic al-
gorithm for building up a useful medical tu-
mor diagnosis e-service system. Besides the 
development of tumor detection algorithm 
from a single ultrasonic image, it is also of 
great importance for the processing of a se-
quence of ultrasonic images. The effective 
matching and tracking for an ultrasonic im-
age sequence can make the algorithm devel-
opment of three-dimensional display of ul-
trasonic image more easily, and thus be 
helpful for the clinic diagnosis and analysis. 
On the aspect of e-service, we have investi-
 3
模組為我們未來要持續進行的工作。 
在下一章的研究方法中，我們將分別
介紹：演算法程式開發、特定影像區域偵
測演算法之 DE 法、特定影像區域偵測演算
法之 BTS 法、序列與三維醫學影像顯示、
伺服器系統架構與相關模組等。結果與討
論則列在最後。 
 
三、研究方法 
 
3.1 演算法程式開發 
 本研究計畫之演算法程式大部分均在
Microsoft Visual Studio 6.0 C++的 MFC 環
境開發。為了提升信號處理的反應能力，
對於需要大量運算與記憶體資料複製的部
分，我們採用 IPP 函式與相關指令為之[8]。 
在進行基礎超音波之實驗時，我們使用清
大葉秩光教授之超音波影像實驗室的高頻
超音波系統進行介面設計。所設計的程式
介面如圖一所示。此圖之上半部是超音波
原始 data 呈現結果，而下半部則是經過解
調後的超音波信號成像結果。 
 
 其他相關演算法亦均在 C++環境開
發。由於我們所開發之演算法亦可以應用
在類似超音波影像的醫學影像，如：核子
醫學影像等。因此，本研究計畫所開發之
方法的應用將更為廣泛。 
 
3.2 特定影像區域偵測演算法：DE 法 
 偵測演算法的開發為本研究計畫最主
要的核心之一。我們提出一個以 Disk Ex-
pansion (DE) 為主的方法設計[9, 10]，其可
以有效的偵測腫瘤的輪廓，實驗證明非常
有效且好用。此部分的研究成果已經以「A 
disk expansion segmentation method for ul-
trasonic breast lesions」為題發表在 2009 年
之 Pattern Recognition 國際期刊中[2]。此
法的主要做法概述如下（參考圖二）。 
 
 
 
圖一 超音波系統取得之原始信號與解調
後之影像。 
 
本法可以圈選出不規則狀的腫瘤形狀，而
且在同一個 ROI 當中可圈選出多個腫瘤形
狀、輪廓。 
1. 適應性二值化（Adaptive threshold-
ing）：由於在光斑影像中，常有光度
不均勻（Uneven-brightness）的問題，
所以不能使用固定臨限值做二值化處
理。我們探討腫瘤區、非腫瘤區、腫
瘤區與非腫瘤區的邊界，經由統計累
積分佈分析，求得適應性閥值。由於
腫瘤的區域一般趨近黑色，所以在二
值化轉換過程，像素低於此臨限值者
設為黑色。因為每一個 ROI 的影像不
同，所以臨限值也會不同。這樣的作
法處理直到每一個像素都被轉換完畢
為止。所得到的結果記錄在原始二值
 5
期刊 Journal of Digital Imaging 第二次修定
稿中[4]。圖四顯示所得到的結果。 
 
 
圖四 顯示甲狀腺醫學影像診斷的結果。 
 
3.3 特定影像區域偵測演算法：BTS 法 
為了探討影像腫瘤嚴重性的次序，我
們也設計了一種以 Binary Tree Structure 
(BTS) 的偵測法，以便應用在伺服器端之
影像分析模組中。這項研究成果「Automatic 
lesion detection in ultrasonic images」也已收
錄在 Image Processing 一書之 Chapter 17 中
[3]，本人為此書之 Invited Editor。以下介
紹 BTS 法的運作原理。 
我們所提出之方法的構想來自於地表
等高線（Contour Map in Earth Surface），
其中山（Mountain）與圍繞此山的谷（Valley 
around the Mountain）可以由一等高線區分
出來。等高線一般看來均是平滑的。在超
音波影像中，一個可能的腫瘤可以想像是
一座山。如前所述，由於腫瘤的品質並不
好而且形狀各有不同，因此一個中值濾波
器（Median Filtering）首先被用來平滑超音
波影像，其中的操作窗為可估測且是圓形
的（estimated circular-window size）。採用
此種作法的原因是：腫瘤有大有小，必須
選用合適的操作窗才能反映出腫瘤的形
狀，如果小腫瘤採用大操作窗，則此小腫
瘤無法被反映出來；另外，如果使用的是
方形操作窗，會使濾波過的影像有明顯的
鋸齒狀，所以我們選用圓形操作窗。 
接著，考慮腫瘤一般較暗的特性，如
果將影像使用平均值（Mean）或中值
（Median）區分成兩個區域，其中較暗的
可能為腫瘤，反之則不是。這種作法，我
們簡稱為 Mean-Cut 或 Median-Cut，統稱為
M-Cut。然而，腫瘤有大有小，其有可能在
較暗的區域，也有可能在較亮的區域，因
此 M-Cut 的作法會繼續下去。其中終止條
件定義如下：我們訂一個簡單的臨限值
（TH，如：TH=5），如果較暗區域與較亮
區域的 M 值差<TH，則此區域不繼續下
去。為了能自動檢查每一種情況，我們採
用二分樹的結構（BTS, Binary Tree Struc-
ture）。在未滿足終止條件前，每一個被分
割的影像區域，均被視為新影象，再進入
BTS 流程中進行分析。圖五顯示此演算法
流程圖。BTS 的節點順序示意圖如圖六所
示。其中節點 0 為開始影像。經過此種作
法，所有可能的腫瘤區域會呈現在末端節
點（Ending Node）中。 
 
 
圖五 分割流程。 
 
 
圖六 3-level BTS 二分樹結構。 
 
 
圖七為一含有腫瘤的超音波影像，我們先
估測可能腫瘤的影像點數以及影像上半部
的總點數（此項考慮主要在於腫瘤之影像
泰半位於此處，以此區域預估尚屬合理），
以此數據以及選定之操作窗範圍估測此影
像之中值濾波器的圓形操作窗半徑為 18。
經過 BTS 的處理流程，我們得到以下十七
個 Ending Nodes：3, 4, 12, 23, 24, 56, 111, 
112, 57, 118, 235, 236, 30, 59, 121, 245, 
 7
之結果，以證明此 BTS 法的可行性。 
 
 
圖十 依序可能為腫瘤之結果。 
 
 
圖十一 其他之單一腫瘤偵測結果。 
 
 
圖十二 多個腫瘤偵測結果。 
 
3.4. 序列與三維醫學影像顯示 
 我們首先介紹以高頻超音波系統擷取
之影像，嘗試進行三維影像的重組。我們
以二維全聚焦影像的 B/D 掃描模式為基
礎，在第三維度上長  2mm 的距離每隔 
50μm 進行共 40 次相同的 B/D 掃描，並
對每次掃描的影像資訊疊合出深度  5.3 
mm 寬度 4.1 mm 的全聚焦影像，最後將 
此 40 張疊合全聚焦影像使用三維影像處
理軟體 Amira (Mercury Computer Systems, 
Inc., MA, USA) [5] 進行重組，即可得到
5.3mm  4.1mm  2mm 的三維影像如圖十
三所示。從圖中我們可以看到四個不同角
度的圓形孔洞隧道。 
 
 
圖十三 重組之三維影像。 
 
在進一步之三維醫學影像顯示研究，我們
採用具有影像多重格式的 SPECT/CT 影像
對於 Sentinel Lymph Node (SLN)之標定進
行相關演算法的設計。我們採用以 C++為
 9
的處理表單狀態序列為之，處理結果有
自動產生之相對應檔案資料夾作為儲
存，方便下載與再利用。影像資料的入
口透過唯一碼與圖形驗證碼作為資訊
安全的把關機制。此數位服務系統，可
以透過一般瀏覽器在任何地方將所欲
處理的超音波影像上傳，系統處理完畢
後，會顯示完成狀態，同時 e-mail 通知
使用者。這樣的設計，對於有需要進行
影像診斷者而言，是非常便利的。 
 
 
圖十六 演算法執行模組之關聯圖。 
 
以上之架構，我們使用特定之資料進行測
試實證，但尚未應用於所開發之演算法。
為了能使研究成果能夠被使用，現階段我
們將部分成果以DEMO介面來展現未來的
樣貌 ( 如 : 圖十七顯示研究成果 [11] 之
DEMO 介面)。也就是，使用者將執行程式
下載後即可執行，只要待分析之特定醫學
影像送入此程式，即可自動得到分析結
果。這個過渡研究成果與顯示方式，有助
於我們未來將本系統研發的更臻完全。 
 
 
圖十七 DEMO 使用者介面。 
 
四、結果與討論 
 
 本研究計畫經過三年的執行，雖然尚
未臻完全，但已有不錯的研究成果(包括：
腫瘤自動偵測演算法及相關延伸應用、醫
學影像診斷三維顯示之階段研究成果、數
位服務伺服器系統初步架構相關模組建
立、以及跨入網路服務前之使用者介面模
組初探等)。由於所研究的內容兼顧學術性
與實用性，本研究之學術性成果初步已陸
續發表在學術刊物中。實用性部分，由於
經費之侷限以及相關技術待突破，因此系
統之雛型建置期待未來有更充足的人力經
費時，再將其完備之。 
 從學術價值面向，本研究計畫雖然原
始目標定在超音波影像，但在研究計畫執
行期間，我們所提之方法應用在其他醫學
影像也能得到不錯的結果，特別是在影像
分割與特別症狀區域之自動偵測。在實作
價值面向，我們已架構伺服系統，並建立
了幾個基本模組，目前應用在特定網頁中
進行測試：其中包括後台管理模組、網頁
顯示模組、網頁程式與資料庫計算模組、
快取更新模組、以及演算法執行模組。另
外，自動啟動執行與更新模組為我們未來
要持續進行的工作。 
 經過三年的研究觀察，在經費、人力、
時間、以及中間所遇到難題的因素下，我
們發現系統整合並不是件容易的事。但透
過國科會的補助，我們已經有很好的基
礎，期待未來，在此基礎上，有更多的成
果發表出來。整體而言，本研究計畫所建
立之基礎，已具有學術價值；而其應用價
值，加以時日，亦能有所期待。 
 本研究計畫兼具學術性與實用性，目
前在學術論文上已有六篇文章發表/審稿
中：兩篇國際期刊論文刊登[2,11]、一篇修
訂審稿中[4]；一篇國際版 Book Chapter (本
計畫主持人為 Invited Editor)[3]；以及兩篇
Pattern Recognition 42 (2009) 596 -- 606
Contents lists available at ScienceDirect
Pattern Recognition
journal homepage: www.e lsev ier .com/ locate /pr
A disk expansion segmentationmethod for ultrasonic breast lesions
Chih-Kuang Yeha,∗, Yung-Sheng Chenb, Wei-Che Fanb, Yin-Yin Liaoa
aDepartment of Biomedical Engineering and Environmental Sciences, National Tsing Hua University, 101, Section 2, Kuang-Fu Road, Hsinchu, Taiwan 30013, ROC
bDepartment of Electrical Engineering, Yuan Ze University, Chungli, Taiwan, ROC
A R T I C L E I N F O A B S T R A C T
Article history:
Received 21 July 2007
Received in revised form 2 September 2008
Accepted 9 September 2008
Keywords:
Speckle noise
Lesion contour
Disk expansion method
Computer-aided diagnosis (CAD)
Automatically extracting lesion boundaries in ultrasound images is difficult due to the variance in shape
and interference from speckle noise. An effective scheme of removing speckle noise can facilitate the
segmentation of ultrasonic breast lesions, which can be performed with an iterative disk expansion
method. In this study, a disk expansion segmentation method is proposed to semi-automatically find
lesion contours in ultrasonic breast image. To evaluate the performance of the proposed method, the
simulations with seven types of cysts, three in vitro phantom images and 10 clinical breast images are
introduced. The mean normalized true positive area overlap between simulated contours and contours
obtained by the proposed method is over 85% in simulation results. A strong correlation exists between
physicians' manual delineations and detected contours in clinical breast images. In addition, the method
is also verified to be able to simultaneously contour multiple lesions in a single image. In comparison with
the conventional active contour model, our proposed method does not require any initial seed within a
lesion and thus, it is more convenient and applicable.
© 2008 Elsevier Ltd. All rights reserved.
1. Introduction
Medical ultrasound image has become a popular tool for early di-
agnosis of breast tumor. Unfortunately, the speckle brightness varia-
tions inherent in ultrasonic imaging degrade the image quality; and
thus limit the detection of low contrast lesions and human interpre-
tation. Therefore, many studies focused on development of speckle
noise reduction technique to improve the image contrast resolution
and the lesion texture extraction ability [1–6].
The lesion contour extraction provides significant clinical infor-
mation to discriminate between benign andmalignant tumors. Previ-
ous studies primarily focused on the lesion classificationwithmanual
delineations of the tumor boundaries [7–9]. However, the malignant
tumor often embeds the surrounding tissue. As a result, the bound-
ary with fine linear strands extending irregularly outward from the
main tumor becomes obscure. Consequently, the work of manual de-
lineation lesion by physicians is highly subjective judgment. In order
to increase reliability of ultrasound lesion delineation and reduce
the unnecessary operations such as biopsy and fine needle aspira-
tion, the topic of computer-aided diagnosis (CAD) for breast lesion
detection has gained wide interest [10–14]. CAD refers to the use of
computerized analysis in helping physician to recognize abnormal
∗ Corresponding author. Tel.: +88635715131x34240; fax: +88635718649.
E-mail address: ckyeh@mx.nthu.edu.tw (C.-K. Yeh).
0031-3203/$ - see front matter © 2008 Elsevier Ltd. All rights reserved.
doi:10.1016/j.patcog.2008.09.004
areas in a medical image. One goal of CAD is to increase the effi-
ciency and effectiveness of breast cancer screening by means of the
computer as a second reader, and the other goal is to classify the
lesion as benign or malignant [13].
The active contourmodels (also called snakes) [15], themost pop-
ular CAD technique, have beenwidely applied to segment the bound-
aries in ultrasound images for the cortex of the brain [11], ovarian
follicles [12], and left-ventricular boundaries [16]. These models re-
quired an initial seed point and utilized a closed contour to approach
object boundary by iteratively minimizing an energy function. Since
inherent speckle interference obscures the ultrasound breast image,
the active contour models have poor convergence to lesion bound-
ary concavity; and thus they cannot accurately contour the irregular
shape malignant tumor. A modified model based on involving an ex-
ternal force, called gradient vector flow (GVF) snake was proposed to
try to resolve the above mentioned problem [17]. The external force
is a main contribution to make GVF functioning. It means that the
result will depend whether the image quality can support an effec-
tive external force or not. Moreover, recent general active contour
approaches were proposed to achieve higher segmentation accuracy
and faster convergence [18,19].
In our previous study, we considered the image having degraded
paint characters (DPC) on the surface of road to demonstrate the clo-
sure noise property and showed a disk expansion (DE) method for
the removal of closure noise [20,21]. From the viewpoint of image
processing, when a thresholding process is applied on a gray-scale
ultrasound image embedding speckle noises, the obtained binary
598 C.-K. Yeh et al. / Pattern Recognition 42 (2009) 596 -- 606
Fig. 2. (a) A simulated ultrasound image G with the size 256×256. The three ROIs represent the speckle (ROI 1), lesion edge (ROI 2), and lesion (ROI 3) regions, respectively.
(b) The CDF rising rate of lesion region (solid line) is larger than that of either lesion edge (dashed line) or speckle region (dashed-dot line). (c) The obtained binary image B
after applying the adaptive thresholding process. (d) The radius information R converted into gray range (0–255) for visualization, where the coordinate (124, 139) having
the maximum radius 16 is plotted with a white-cross mark. (e) The corresponding gray information under the region A2 of the extracted significant object, where gobject =96.
(f) The outer band region A3 displayed with the gray information, where gbackground = 179. (g) The finally found region, where the contour of the lesion is plotted with a
white curve on G.
In Fig. 2(b), the three selected ROIs gray level thresholds (denoted
by circle symbols) are 154, 132 and 157, respectively. The pixels
within the ROI below the gray level threshold are assigned to be a
black pixel. The adaptive thresholding procedure is executed until
the entire original image is transformed to a binary image. Note that
to obtain an entire binary image from the original one, the process
has to array multiple ROIs to cover the entire image. The ROIs with
the size of 8×8 pixels were arrayed adjacent to each other with 87.5%
overlap. Fig. 2(c) shows the obtained binary image as a matrix B after
performing the adaptive thresholding procedure.
2.2. Extract significant object using DE scheme
Theoretically, this segmentation work can also be done by the
morphological method using close and open operations. Although
the close and open operations are strongly dependent on the struc-
turing element, e.g., a 3×3 square array, for the fast computation, the
iterations of closing and opening are generally decided in heuristic.
This is because the a priori knowledge of noise involved in an image
is usually unknown. To overcome this problem, the classical con-
cept of maximal disk, which is well-known for the computation of
600 C.-K. Yeh et al. / Pattern Recognition 42 (2009) 596 -- 606
100
80
60
40
20
0
50
40
30
20
10
0
FP
 (%
)
TP
 (%
)
25
20
15
10
5
0
FN
 (%
)
10
8
6
4
2
0
sh
or
te
st
 d
is
ta
nc
e 
(p
ix
el
s)
Sm
oo
th
Ma
cro
-
lob
ula
te
Mi
cro
-
lob
ula
te
Irr
eg
ula
r
Ps
eu
do
-
po
d Zig
za
g
Sp
icu
lat
e
Sm
oo
th
Ma
cro
-
lob
ula
te
Mi
cro
-
lob
ula
te
Irr
eg
ula
r
Ps
eu
do
-
po
d Zig
za
g
Sp
icu
lat
e
Sm
oo
th
Ma
cro
-
lob
ula
te
Mi
cro
-
lob
ula
te
Irr
eg
ula
r
Ps
eu
do
-
po
d Zig
za
g
Sp
icu
lat
e
Sm
oo
th
Ma
cro
-
lob
ula
te
Mi
cro
-
lob
ula
te
Irr
eg
ula
r
Ps
eu
do
-
po
d Zig
za
g
Sp
icu
lat
e
10dB
20dB
30dB
Fig. 5. Statistic analyses of detected contours are shown in: (a) true positive (TP), (b) false negative (FN), (c) false positive (FP) and (d) shortest distance estimates and their
standard deviations. The simulated images of seven cyst types with different SNRs ranging from 10 to 30dB were tested.
the coordinate (124, 139) having the maximum radius 16 is found
and plotted with a white-cross mark.
In order to eliminate some small holes in this region, morpho-
logical operations, dilation and erosion, are suggested and applied
on L. In our algorithm, we performed dilation following erosion two
times on the image for eliminating small holes, where the structuring
element is with a size of 3×3. Thereafter, the most significant ob-
ject in G can be extracted based on the “2” region in L as shown in
Fig. 2(e).
2.3. Refine the extracted object
In addition to the significant region located, the contour of the
lesion detection is also of great importance. However, due to speckle
noise effect and the original “1” labels in L determined by THp, it is
possible that some pixels are misclassified. Therefore, referred to the
“2” region in L, the original gray image G is involved in this stage to
refine the extracted object. Let A2 be the “2” region representing the
previously extracted significant object in L. Within the same region
A2 in G, we compute the mean object's gray value gobject as follows:
gobject =
1
NA2
⎡
⎣ ∑
∀(x,y)∈A2
G(x, y)
⎤
⎦ , (4)
where NA2 represents the total number of pixels in A2. In the current
case of Fig. 2(e), gobject = 96.
Further, since some pixels outside A2 may belong to the object's
ones and some pixels in A2 may belong to the non-object's ones,
we dilate several times (typical 10 to 12 times, which are empir-
ically determined according to the contrast between object and
background. The higher the contrast is, the less the dilation times
are.) for the A2 region to obtain an outer band region denoted as A3
which does not include A2. All the pixels of A3 in L are labeled by “3”.
Because the most pixels in A3 belong to the background pixels, we
602 C.-K. Yeh et al. / Pattern Recognition 42 (2009) 596 -- 606
Fig. 7. Three clinical images analysis: (a) original images, (b) detected contours by the proposed method, and (c) physician's manual delineations.
5.3. Use (3) to update L.
5.4. Extract the most significant object in G based on the “2” region
in L.
6. Refine the extracted object (some morphological operations are
included).
6.1. Use (4) to compute gobject in region A2.
6.2. Use (5) to compute gbackground in region A3.
6.3. Use (6) to obtain the object matrix O.
6.4. Regard O as a binary image and reperform steps 5 and 6 with
p = 0 (the second round) to find the finally refined object.
7. Remove the part of the extracted object from B, and do steps 5
and 6 for next object extraction until all the desired objects are
contoured.
3. Results and discussion
3.1. Simulation results
To evaluate the performance of the proposed algorithm, simu-
lated images of varying types of cysts with different contours were
used [23]. For emulating speckle characteristic, the backscattered
amplitude from each scatterer is random, and the scatterers have
an independent and identical distribution in 2-D space. The center
frequency of transducer was 5MHz. The cysts with varying shapes
were also embedded in speckle background with SNR of 20dB. Ac-
cording to the classification of contours of breast lesions proposed
by Chou et al. [9], seven types of cysts were designed to describe
the benignancy and malignancy of lesions. In addition to the ir-
regular type illustrated in Fig. 2, other six types of cysts includ-
ing smooth, macro-lobuate, micro-lobuate, pseudo-pod, zigzag, and
speculate shown in Fig. 3(a) are also adopted in our experimentation.
Fig. 3(b) represents the simulated images. The contour extraction
results obtained by our algorithm are shown in Fig. 3(c), where the
white color contours represent the boundary of lesions and demon-
strate the feasibility of our approach.
The boundary error and estimation area error between the
simulated contours and the delineations obtained by the proposed
algorithm were used for further evaluations [13]. The boundary er-
ror is defined as the mean shortest distance between the simulated
contour and our algorithm's delineation. We denote the simulated
contour boundary as M = {m1,m2, . . . ,m} and the result obtained
by our algorithm as P = {p1,p2, . . . ,p}, where each element of M or
P is a point on the corresponding contour. We find the distance of
every point in P from all points in M, and define the distance to the
closest point for pj ( ∀ pj ∈ P) to the contour M as
d(pj,M)=minw ‖pj −mw‖ (7)
where ‖ · ‖ is the 2-D Euclidean distance between any two points.
Then, all the d values are calculated to obtain the statistics of mean,
minimum, and maximum values.
The area error is represented by the three parameters including
true positive (TP), false negative (FN) and false positive (FP). They are
604 C.-K. Yeh et al. / Pattern Recognition 42 (2009) 596 -- 606
Fig. 6(a) shows the irregular tumor pattern image; and the
B-mode contrast relative to glandular background tissue is −10dB.
The image size is 201×199 pixels. Fig. 6(c) shows the image compris-
ing a background of glandular material with three different contrast
spheres with a radius of 4mm. The upper-right, upper-left and low-
est sides are the cyst, high-attenuation pattern, and fat sphere; and
their B-mode contrasts relative to glandular background tissue are
−14, −12 and −14dB, respectively. The contour extraction results in
white solid lines as shown in Fig. 6(b) and (d) demonstrate the su-
perior performance of the proposed algorithm. In the two cases, the
mean normalized TP area overlap is over 85%. The mean normalized
FN is lower than 12% and the mean normalized FP is lower than 5%.
The mean shortest distance error between actual phantom image
Fig. 10. Receiver operating characteristic (ROC): relationship between true positives
(TP) versus false positives (FP) in clinical images.
Fig. 11. Contours results from gradient vector flow (GVF) snake algorithm in simulated images with (a) irregular- and (b) spiculate-cyst patterns and (c)–(e) clinical images
from Fig. 7(a).
contours and contours obtained by the proposed algorithm is 3 pix-
els. The standard deviation and maximum value results are 4.5 and
7.2 pixels, respectively.
3.3. Clinical data analysis
The clinical breast images were also used to test the proposed
algorithm. The images were acquired using a commercial imaging
system (ATL HDI 5000, Bothell, Washington, USA) and a linear ar-
ray transducer (ATL L12-5, 50mm, Bothell, Washington, USA) at Na-
tional Taiwan University Hospital. Each pixel in the acquired image
had a resolution of 8 bits. The original breast images are shown in
Fig. 7(a), where the white dotted-line boxes indicate the selected
ROIs being transferred to binary images prior to the contouring pro-
cess. The contour results obtained by the proposed algorithm are
shown in Fig. 7(b). Themanual delineations by an experienced physi-
cian are shown in Fig. 7(c). There are high correlations between the
physician manual contours and the contours obtained by our algo-
rithm. The mean normalized TP area overlap is over 90%. The mean
normalized FN is lower than 5% and the mean normalized FP is 10%.
Themean shortest distance error between the physicianmanual con-
tours and contours obtained by the proposed algorithm is 8.5 pixels.
The standard deviation andmaximum value results are 10.5 and 15.2
pixels, respectively. The result of multiple lesions extraction shown
in Fig. 8 obtained by the propose algorithm further confirms the ef-
fectiveness of the proposed approach.
We also invited 10 experienced physicians to manually contour
the lesion boundaries in other six clinical breast images and then
compared the results with our proposed method did. The statis-
tic analyses between the 10 physicians' manual delineations and
detected contours by our proposed method are shown in Fig. 9.
The symbols from “A” to “J” in lateral axis represent the physi-
cians order. Note that since there is no ground truth of the 10
physicians' manual delineations, the contour results by our proposed
method and physicians' manual delineations were designated as the
terms of As and Ap in Eqs. (8)–(10), respectively. In other words,
there were six independent estimates of TP, FN, FP and shortest dis-
tance in each physician's manual delineations. The mean shortest
distance error between the physician manual contours and contours
obtained by the proposed algorithm is 1.4 pixels. The standard devi-
ation and maximum value results are 11 and 17 pixels, respectively.
606 C.-K. Yeh et al. / Pattern Recognition 42 (2009) 596 -- 606
About the Author—YUNG-SHENG CHEN was born in Taiwan, ROC, on June 30, 1961. He received the B.S. degree from Chung Yuan Christian University, Chung-Li, Taiwan, in
1983 and the M.S. and Ph.D. degrees from National Tsing Hua University, Hsinchu, Taiwan, in 1985, and 1989, respectively, all in Electrical Engineering. In 1991, he joined
the Electrical Engineering Department, Yuan Ze Institute of Technology, Chung-Li, where he is now a Professor. His research interests include human visual perception,
computer vision and graphics, circuit design, and website design. Dr. Chen received the Best Paper Award from the Chinese Institute of Engineers in 1989 and an Outstanding
Teaching Award from Yuan Ze University in 2005. He has been listed in the Who's Who of the World since 1998 and awarded with The Millennium Medal from The Who's
Who Institute in 2001. He is a member of the IEEE, and the IPPR of Taiwan, ROC.
About the Author—WEI-CHE FAN was born in 1981 in Taiwan, ROC. He received his B.S., M.S. degrees in Electrical Engineering from National United University and Yuan
Ze University, Taiwan in 2004 and 2006, respectively. His research interest is medical image processing.
About the Author—YIN-YIN LIAO was born in 1985 in Taiwan, ROC. She is now a Master student in the Department of Biomedical Engineering & Environmental Sciences,
National Tsing Hua University, Hsinchu, Taiwan. Her research interests include ultrasound image processing and pattern recognition.
 Image Processing 
 
312 
2. Proposed approach 
The main idea of our approach is from the concept of contour map onto earth surface, where 
a mountain and the valley around the mountain may be distinguished with a contour line. 
In ultrasonic image, a possible lesion may be imaged as a mountain. Since it is usually with 
a poor quality and the lesion may be with a various size, in our approach a median filtering 
with estimated circular-window size is first used for forming a smoothed image. A proper 
circular-window for median filter is necessary. That is, a proper small window should be 
used in the case of small lesions, and vice versa. Furthermore, to avoid the zigzag effect, a 
squared window should not be used. 
The darker region belonging to a lesion is another useful cue in ultrasonic image processing. 
Hence intuitively, if an ultrasonic image is divided into two regions: darker and brighter 
regions, assuming a lesion located in the darker region is reasonable. To perform such 
segmentation, a mean or median value of the given image can be used as a threshold, which 
can be referred to as a mean-cut or median-cut scheme, respectively, therefore M-cut named 
generally in this paper. Since a lesion may be of large or small; and it may be located in the 
newly divided darker region or brighter region, the M-cut scheme will perform 
continuously until a stopping condition satisfies. In our study, the stopping condition is 
defined as follows. 
 brighter darkerM M TH− <   (1) 
Where brighterM  and darkerM  represent the M-value of brighter and darker region, respectively; 
and TH is a defined threshold value. If a stopping condition occurs, then the previous 
undivided one may represent a possible lesion. 
Because the stopping condition may occur at either darker or brighter region, to check the 
stopping condition a binary tree structure (BTS) having several nodes is adopted in our 
approach. Based on the BTS, each segmented part will be regarded as a new image and fed 
into the BTS for analysis, and thus be regarded as a new node. A 3-level node sequence 
starting at node 0 for a BTS is illustrated in Fig. 1. After the BTS segmentation, all possible 
lesions’ regions will be located and indicated by the ending nodes. To facility the final 
possible lesions’ regions easily identified, a postprocessing will be useful. In the following 
subsections, the main parts of our approach namely, segmentation, binary tree structure, 
and postprocessings will be detailed.  
 
 
Fig. 1. Illustration of a 3-level BTS. 
 
 
Fig. 2. Segmentation procedure. 
 Image Processing 
 
314 
   
                            (a)                                               (b)                                              (c) 
   
                            (d)                                               (e)                                              (f) 
Fig. 3. (a) An ultrasonic image containing a lesion. (b) The estimated object’s region from the 
upper image. (c) Binary result. (d) The largest labelled region. (e), (f) Segmented R1 and R2 
subimages. 
Let k be the node number. Refer to Fig. 1, for node 0 (original image), we will have two son 
nodes denoted node 1 and node 2. It can be formulated as: for node k, it has two son nodes: 
node 2k+1 and node 2k+2. Based on this formulation, for example, two son nodes of node 2 
(k=2) are node 5 and node 6 as addressed in Fig. 1. 
Because of the recursive operations of BTS, the backward node indexing is also needed. That 
is, for node k, its father node will be 
 
( 1) / 2 if  is odd,
node 
( 2) / 2 otherwise.
k k
k
−⎧⎨ −⎩  (3) 
In the BTS algorithm of Fig. 4, the tc[k] is used to count the passing times for the k-th node. 
Each node can be visited two times at most, one is forward and the other is backward. 
After the BTS processing, for the current example, we obtain the following ending nodes: 3, 
4, 12, 23, 24, 56, 111, 112, 57, 118, 235, 236, 30, 59, 121, 245, 246. The tree structure of these 
ending nodes denoted by black colour is depicted in Fig. 5. The detailed information 
including size and mean grey value of each node (subimage) are given in Table 1(a), which 
will be further used in postprocessings. Here size means the total non-white pixels of the 
corresponding subminage. 
 Image Processing 
 
316 
 
Fig. 5. The tree structure of ending nodes 3, 4, 12, 23, 24, 56, 111, 112, 57, 118, 235, 236, 30, 59, 
121, 245, 246, denoted by black colour. 
      
(a)                                                  (b) 
Table 1. (a) Size and mean grey information of each node for tree structure in Fig. 5. Here 
size means the total non-white pixels of the corresponding subminage. (b) Reduced nodes’ 
information after applying Rule 1. They are ordered by size in ascent for the further use of 
Rule 2.  
 Image Processing 
 
318 
 
Fig. 7. Contouring all the regions in an image for illustrating the inclusion results. 
The final steps of postprocessings are to sort the nodes depending on the possibility of a 
node belonging to a lesion and to display the lesion detection result. It is reasonable that if a 
region in the given image showing a higher possible lesion, it should have a higher contrast; 
otherwise to identify a lesion is somewhat difficult. Hence we define a so-called contrast 
ratio (cr) to index the possibility of a lesion. Given a node 1, its father node is easily 
addressed according to (3) and thus indexing to the node 2 which complements to node 1 
since node 1 and node 2 are two segmented regions from their father node. Let 1g , 2g , and 
fg  be the mean grey values of node 1, node 2, and their father node, respectively. Three 
parameters 1 1| |fd g g= − , 2 2| |fd g g= − , and 1 2( ) / 2avg g g= +  are defined to formulate the 
following contrast ratios. 
1
1
2
2
1 2
3
dcr
avg
dcr
avg
d dcr
avg
=
=
+=
 
Here 1cr  considers the contrast between node 1 and the father node; 2cr  considers the 
contrast between node 2 and the father node; and 3cr  considers the contrast between two 
son nodes and the father node. Thus our totally contrast ratio ( totalcr ) is combined by the 
above three terms. 
 1 2 1 2total 1 2 3 3
( )( ) d d d dcr cr cr cr K K
avg
+= × × =   (5) 
Since the lesion tends to a darker region and possesses a higher contrast, the higher of 
numerator and the lower of denominator in (5) will derivate a higher totalcr  and thus show a 
higher lesion possibility at this node. Here constant K is used to facilitate the numeric 
manipulation, 65536 is used in our program. Take node 12 at the current example as an 
illustration, its mean grey value is 1 63g = , we can find its father node, node 5 ( 58fg = ), 
 Image Processing 
 
320 
property and the influence of speckle information. Traditionally, it needs a manual ROI 
selection prior to contour detection for a lesion. After performing our approach to this 
image, we finally obtain 20 nodes to represent all possible lesion’s regions, where an image 
including the inclusion results like Fig. 7 is given in Fig. 10(b). Obviously, the real lesions 
should be in the detection list even many non-lesion regions are located. Because of the 
inherently un-uniform brightness property and the influence of speckle information, the real 
lesion may not be placed in front of the significance order. In this case, three most possible 
lesions placed in order 1, 8, 20 are shown in Fig. 10(c), 10(d), and 10(e) respectively. 
 
   
                            (a)                                               (b)                                              (c) 
   
                            (d)                                               (e)                                              (f) 
Fig. 9. Some other results. Here only the first place of detected regions in each image is 
displayed. 
4. Conclusion 
In this article, we have presented a simply but effectively fully automatic segmentation 
method for detecting lesions in ultrasonic images without the constraint of ROIs or initial 
seeds given. Based on the use of a binary tree structure and some postprocessings, multiple 
lesions can be detected and displayed in order for further visualization and inspection. Since 
experiments have confirmed the feasibility of the proposed approach, an e-service for 
ultrasonic imaging CAD system is worthy of being developed. In addition, the strategy of 
reducing non-lesion regions may also be an interesting topic; and will be further 
investigated and involved in this approach as a near future work. 
 Image Processing 
 
322 
Hamarneh, G. & Gustavsson, T. (2000). Combining snakes and active shape models for 
segmenting the human left ventricle in echocardiographic images, In: Proceedings of 
IEEE Computers in Cadiology, Vol. 27, pp. 115-118, Cambridge, Massachusetts, USA, 
September 24-27, 2000. 
Ladak, H.; Downey, D.; Steinman, D. & Fenster, A. (1999). Semi-automatic technique for 
segmentation of the prostate from 2D ultrasound images, In: Proceedings of IEEE 
BMES/EMBS Conference Serving Humanity, Advanced Technology, Vol. 2, p. 1144, 
Atlanta, GA, USA, October 13–16, 1999. 
Ladak, H.; Mao, F.; Wang, Y.; Downey, D.; Steinman, D. & Fenster, A. (2000). Prostate 
segmentation from 2D ultrasound images, In: Proceedings of International Conference 
Engineering in Medicine and Biology, Vol. 4, pp. 3188-3191, Chicago, IL, USA, July 23-
28, 2000. 
Richard, W. & Keen, C. (1996). Automated texture-based segmentation of ultrasound images 
of the prostate, Computerized Medical Imaging and Graphics, Vol. 20, pp. 131-140. 
Yeh, C. K.; Chen, Y. S.; Fan, W. C. & Liao, Y. Y. (2009). A disk expansion segmentation 
method for ultrasonic breast lesions, Pattern Recognition, Vol. 42, No. 5, pp. 596-606. 
 
Author's personal copy
computer methods and programs in b iomed ic ine 9 8 ( 2 0 1 0 ) 15–26
journa l homepage: www. int l .e lsev ierhea l th .com/ journa ls /cmpb
Automatic computer-aided sacroiliac joint index analysis for
bone scintigraphy
Jia-Yann Huanga, Ming-Fong Tsaib, Pan-Fu Kaoc, Yung-Sheng Chena,∗
a Department of Electrical Engineering, Yuan Ze University, 135 Yuan-Tung Road, Chung-Li 320, Taiwan, ROC
b Department of Nuclear Medicine, Chang Gung Memorial Hospital, Chiayi, Taiwan, ROC
c Department of Nuclear Medicine, Buddhish Tzu Chi General Hospital, Taipei, Taiwan, ROC
a r t i c l e i n f o
Article history:
Received 24 June 2008
Received in revised form
25 June 2009
Accepted 30 July 2009
Keywords:
Sacroiliitis
Bone scintigraphy
Sacroiliac joint-to-sacrum uptake
ratios
Fuzzy sets
Morphological operations
a b s t r a c t
Bone scintigraphy helps to detect sacroiliitis before the radiographic changes. Nuclear
medicine physicians interpret lesion on sacroiliac joint by the aid of sacroiliac joint-to-
sacrum uptake ratios (sacroiliac joint index, SII). Usually, the SII is measured by manually
drawing regions of interest (ROI) over the sacroiliac joints and sacrum, which is a tedious,
time consuming, and highly operator dependent procedure. In this approach, we devel-
oped an automatic SII measurement program based on fuzzy sets histogram thresholding,
anatomy-based image segmentation method and pelvis reference points located by mor-
phological operations. To validate the program, the results of automatic SII measurement
from 33 patients were compared with the results of manual SII measurement. For each
patient, six SIIs representing the upper, middle, and low portions of the right and left SIIs
were obtained. Totally, 198 SIIs were included for linear regression, bias, and precision anal-
yses. The coefﬁcient of determination (R2) between our approach and 2 medical experts’
manual measurements are 0.923 and 0.917 respectively. The predictive performance (bias,
precision) shows a good agreement result. From the result of this study, we expect that this
automatic computer-aided approach may be applied to help nuclear medicine physicians
in sacroiliitis interpretation.
© 2009 Elsevier Ireland Ltd. All rights reserved.
1. Introduction
Nuclear medicine bone scintigraphy is valuable in the evalua-
tion of sacroiliac joint and may reveal evidence of sacroiliitis
severalmonths or even years before the development of radio-
graphic changes [1]. Nuclear medicine bone scintigraphy can
overcome the difﬁculty in recognizing early sacroiliac abnor-
malities on planar radiography [2].
Usually, SII analysis is performed on a static bone scintigra-
phy, but the pelvis image from whole body bone scintigraphy
at a low scan speed can also be adopted for quantitative anal-
ysis [3]. Besides, the abnormalities of the whole body skeleton
∗ Corresponding author. Tel.: +886 3 4638800x7113; fax: +886 3 4639355.
E-mail address: eeyschen@saturn.yzu.edu.tw (Y.-S. Chen).
can be surveyed simultaneously. After acquisition by a dual-
head scintillation gamma camera [4], the whole body bone
scintigrams are displayed in anterior view (IorgAP ) and posterior
view (IorgPA ) (Fig. 1). Currently, most of the gamma cameras are
equipped with a commercial program for SII measurement,
a semi-quantitative analysis by manually placing 3 measure-
ment areas at upper, middle, and lower portions of bilateral
sacroiliac joints and sacrum on the posterior view of pelvis
image from bone scintigraphy. Each measurement area is 3–5-
pixel thickness (Fig. 2a). Computer then generates 3 horizontal
proﬁles from the aforementionedmeasurement areas anddis-
plays the total counts in each horizontal 1-pixelwidth (Fig. 2b).
From separative proﬁle, operators then manually select those
0169-2607/$ – see front matter © 2009 Elsevier Ireland Ltd. All rights reserved.
doi:10.1016/j.cmpb.2009.07.010
Author's personal copy
computer methods and programs in b iomed ic ine 9 8 ( 2 0 1 0 ) 15–26 17
Fig. 3 – Flowchart of the preprocessing stage.
11–76-year-old), who visited Chang Gung Memorial Hospital
at Chiayi in 2007. The hospital’s medical ethics committee
approved this investigation.
3.2. Bone scintigraphy
At 3h after intravenous injection of 740 MBq 99mTc-MDP, the
whole body bone scan images were acquired by a dual-head
gamma camera model Siemens E-CAM equipped with low
energy high-resolution (LEHR) collimators. Scan speed setting
was 10 cm/min with no pixel zooming. Energy discrimina-
tion was provided by a 20% window centered at 140keV of
99mTc-MDP. The whole body ﬁeld was used to record anterior
and posterior views digitally with resolution 1024×256 pixels.
Images represented the detected counts of gamma ray in each
pixel with a 16-bit gray-level scale.
3.3. Preprocessing
The ﬂowchart of the preprocessing stage in our approach
is shown in Fig. 3. Following, the detailed algorithms of
Fig. 4 – The histogram of an original whole body bone scan
image.
noise removing, true body height determination, and rough
pelvis segmentation based on human anatomical knowledge,
enhancement processing in pelvis region, Gaussian smooth-
ing, and histogram equalization will be presented.
The histogram of a whole body bone scan image revealed
a deep valley between two peaks at the low gray-level area
(Fig. 4). A threshold value at valley of the histogram was
applied to eliminate the background noise outside the body
frame. The true body frame height can be obtained by detect-
ing the topmost and bottommost extremities of the body
frame. In image IorgAP , starting from 35% to 55% of the true
body frame height. The pelvis region was segmented roughly
(Fig. 5a), and was denoted as IrpelAP . In image I
org
PA , the rough
posterior pelvis region was obtained in a same way, and was
denoted as IrpelPA (Fig. 5b).
Due to the distances from the collimators, the iliac activity
is more prominent in anterior view (Fig. 5a) and the sacral
Fig. 5 – Illustrations of a rough segmented pelvis images in (a) anterior view (IrpelAP ) and (b) posterior view (I
rpel
PA ). After the
pelvis enhancement and smoothing procedures, the image is displayed in (c) IrpelSMO. Finally, (d) shows the histogram
equalization result (IrpelEQU).
Author's personal copy
computer methods and programs in b iomed ic ine 9 8 ( 2 0 1 0 ) 15–26 19
around typical reference points were reserved. By using fuzzy
sets on histogram thresholding, Tobias and Seara demon-
strated successfully in segmentation background from object
in multi-modal and bi-modal histogram images [9]. Our pre-
vious work [10] used this algorithm to segment bone regions
from a whole body scintigram, and showed a satisﬁed result.
In this approach, we adopted the same algorithm to classify
soft tissue and bone regions; the thresholding algorithm can
be summarized as follows.
Let U be the universe of discourse (all pixels in an image),
a fuzzy set A in U is deﬁned as
A = {(xi, A(xi))}, xi ∈ U (3)
Themembership functionA(xi) characterizes the fuzzy set
A. The S-function is deﬁned as
As (x) = S(x;a, b, c)
=
⎧⎪⎪⎨
⎪⎪⎩
0, x ≤ a
2{(x− a)/(c− a)}2, a < x ≤ b
1 − 2{(c− x)/(c− a)}2, b < x ≤ c
1, x > c
(4)
The parameter b is deﬁned as b= (c+a)/2, with AS (b) = 0.5.
The range of the function is deﬁned as b= c−b=b−a. The
Z-function is obtained from the S-function as below:
AZ (x) = Z(x;a, b, c) = 1 − S(x;a, b, c) (5)
The nearest ordinary set of A is denoted as A and its mem-
bership function is deﬁned as
A(xi) =
{
0, if A < 0.5,
1, if A ≥ 0.5.
(6)
The index of fuzziness (IF) is obtained by calculating the
distance between A and A [11]. IF can be simpliﬁed as
 k(A) =
2
n
n∑
i=1
|A(xi) − A(xi)|
= 2
n
n∑
i=1
min(A(xi),1 − A(xi))
= 2
n
t∑
k=s
(min(A(k),1 − A(k))) · h(k)
(7)
where h(k) means the image histogram and s and t are the
intensity count limits of the subset being considered. This
algorithm applies the concept of ‘similarity’; a fuzzy set with
a low index of fuzziness indicates that its elements are very
similar to each other.
Two linguistic variables {Soft Tissue, Bone} model was
deﬁned by two fuzzy subsets, denoted as S and B, respectively.
The fuzzy subsets S and B were associated with the histogram
intervals [xmin, xj] and [xr, xmax] respectively, where xj and xr
were the last and ﬁrst intensity count limits for these subsets,
and xmin and xmax were the lowermost and highest intensity
count of the image, respectively. For the ﬁrst IF measuring,
we set xj = xmin + 10, and xr = xmax −10. We then modiﬁed the
Fig. 7 – The histogram and the characteristic functions for
the seed subsets (black color) of a preprocessed image,
normalization step of the indices of fuzziness (blue color)
and decision of the threshold value (red color). (For
interpretation of the references to color in this ﬁgure legend,
the reader is referred to the web version of the article.)
value of xmax to the intensity count where pixels accumula-
tion from xmin to the 97% of total pixels. The intensity counts
in each of these subsets have the intuitive property of belong-
ing with certainty to the ﬁnal subsets background (G) or object
(O). That means, S⊂G and B⊂O. Those subsets were located
at the beginning and the end regions of the imagehistogram.A
fuzzy region placed between S and B was deﬁned as delineated
in Fig. 7. For classiﬁcation procedure, an intensity count picked
from the fuzzy region was added to each of the seed subsets.
By calculating the IF’s of the subsets S∪{xi} and B∪ {xi}, the xi
was appointed to the subset with lower IF. After applying this
procedure for all gray levels of the fuzzy region, each pixel was
classiﬁed into bone or soft tissue. The classiﬁcation method is
a comparison of IF calculations, we have to normalize those
calculations. This is archived by ﬁrst computing the IF’s of
the seed subsets S and B, then by computing a normalization
factor ˇ in accordance with the following relationship:
ˇ = k(S)
k(B)
(8)
where k(S) and k(B) mean the IF’s of the subsets S and B,
respectively. Fig. 7 shows how the normalization works (blue
color). The result of the fuzzy subset histogram thresholding
processing in image IrpelEQU was demonstrated in Fig. 8a, and
denoted as IrpelSEG. After the fuzzy subset histogram threshold-
ing processing, the soft tissue regions were removed and the
major bone regions were reserved.
3.4.2. Morphological operations
In this procedure,weusedmorphological operations to beneﬁt
the locating reference points in next step. First, we convert
image IrpelSEG to a binary image denoted as I
rpel
BIN , and then a 3×3
structure element B was used to erode image IrpelBIN by
IrpelBINB = {z|(B)z ⊆ I
rpel
BIN } (9)
Author's personal copy
computer methods and programs in b iomed ic ine 9 8 ( 2 0 1 0 ) 15–26 21
3.4.3.2. Locate pelvis reference points. In image IverrCON, down
from the reference point pverlCON to 40% image height in ver-
tical direction, the horizontal distances from left image
border to the ﬁrst non-zero pixel were calculated for every
column. If the distance difference between current dis-
tance and previous distance exceeded 3 times than previous
one, then the left-top of pelvis was located. In a similar
way, this was applied to locate the right-top of pelvis. We
denoted the left-top reference point as ppelltCON (labeled ‘c’ in
Fig. 10), and right-top reference point as ppelrtCON (labeled ‘d’ in
Fig. 10). If the vertical coordinate difference between reference
points ppelltCON and p
pelrt
CON was greater than 5 pixels, a rota-
tion transformation was performed to correct the tilt image
by:
[
x′
y′
]
=
[
cos  sin 
− sin  cos 
][
x
y
]
(11)
where  is the angle formed by the vertical coordinate dif-
ference between reference points ppelltCON and p
pelrt
CON, x and y
are coordinates of a source image, and x′ and y′ are coor-
dinates of an object image. Reference points pverlCON, p
verr
CON,
ppelltCON, and p
pelrt
CON must be re-located, if rotation transforma-
tion was performed. Beginning from reference point ppelltCON
to the 50% image height in vertical direction, by calcu-
lating horizontal distances between the left image border
and the left-ﬁrst non-zero pixel in each column. The right-
end of the minimum distance column was the position of
pelvis left-extremity and denoted as ppelleCON (labeled ‘e’ in
Fig. 10). The similar way was applied to locate the right-
extremity of pelvis and denoted as ppelreCON (labeled ‘f’ in
Fig. 10).
In image IrpelCON, downward from 50% to 80% of the image
height in vertical direction, by calculating horizontal dis-
tances between the left image border and the ﬁrst non-zero
pixel in each column. The right-end of the minimum dis-
tance column was the position of pelvis left lower extremity
and denoted as ppelljCON (labeled ‘i’ in Fig. 10). By scanning each
horizontal line from reference point ppelleCON to reference point
ppelljCON in vertical direction, the horizontal widths between the
left and right pelvis boundaries were measured in every col-
umn, the coordinates of both ends of the minimum width
were denoted as ppellbCON and p
pelrb
CON (labeled ‘g’ and ‘h’ in
Fig. 10).
3.4.4. Locate measurement region based on reference
points
The reference points ppelltCON, p
pelrt
CON, p
pelle
CON , p
pelre
CON , p
pellb
CON and p
pelrb
CON
in image IrpelCON were mapped back to image I
rpel
PA , and then the
measurement region in image IrpelPA was segmented by these
reference points. The segmented measurement region con-
tained the portions of sacrum and left/right sacroiliac joints.
Then we divided the measurement region into 3 equal parts
in vertical direction, the vertebra reference points pverlCON and
pverrCON were used to identify the area of sacrum. Finally, we
obtained 9 sub-regions (Fig. 11) for beneﬁting the automatic
SII measurement in next step.
Fig. 11 – The segmented 9 sub-regions in the measurement
region.
3.5. Fully automatic SII measurement
As mentioned above, we obtained 9 sub-regions for automatic
SII measurement. A 3-pixel height measurement area was
placed over the mid-part in each sub-region automatically
(Fig. 12a). Then, the upper, middle, and lower horizontal pro-
ﬁles were created by our approach (Fig. 12b). The 3 horizontal
proﬁleswere then inspected to ﬁnd thehighest count accumu-
lation in each sub-region. The maximum count accumulation
value in each sub-region was selected and denoted as m1, m2,
m3, m4, m5, m6, m7, m8, and m9. Finally, 3 sets of SII data were
obtained by
m1 : m2 : m3 (upper portion)
m4 : m5 : m6 (middle portion)
m7 : m8 : m9 (lower portion)
(12)
Linear regression analysis was applied to assess the cor-
relations between our approach and 2 medical experts. The
difference-versus-mean method was performed for the com-
parison of measurements [13]. The predictive performance
of the various tests was described by the bias and precision
[14]. All statistical analyses were performed with Excel 2000
(Microsoft, Troy, NY).
4. Results
In order to evaluate the proposed algorithm in this
approach, we implemented a program constructed by
C++ Builder 6.0 Enterprise Suite (Borland, Austin, TX),
and performed on the 33 patients. The current system
includes removing noise outside the whole body scintigram,
locating pelvis region, enhancing pelvis region, Gaussian
ﬁltering, histogram equalization, histogram thresholding,
morphological operations, locating reference points, and
automatic SII measurement. Fig. 13 shows the user inter-
face of our program. A demonstration can be found in
http://yschen.ee.yzu.edu.tw/Academic/CVLab/demo sii.asp.
For validating our approach, the results of our automatic
SII measurement were compared with the manual SII mea-
surements obtained by 2 medical experts. For this approach,
Author's personal copy
computer methods and programs in b iomed ic ine 9 8 ( 2 0 1 0 ) 15–26 23
18
21
M
1.
39
3/
1.
11
4/
1.
24
8
1.
38
8/
1.
06
7/
1.
38
8
1.
45
1/
1.
31
5/
1.
07
5
W
ri
st
p
ai
n
(N
eg
at
iv
e
bo
n
e
sc
an
).
1.
32
7/
1.
03
3/
1.
27
1
1.
35
8/
1.
03
9/
1.
22
5
1.
77
4/
1.
25
7/
1.
04
1
19
50
F
1.
32
8/
0.
98
9/
1.
66
8
1.
42
3/
0.
96
3/
1.
67
1
1.
47
1/
1.
16
8/
1.
55
6
T
h
ig
h
p
ai
n
(N
eg
at
iv
e
bo
n
e
sc
an
).
1.
22
0/
0.
99
4/
1.
67
5
1.
28
8/
0.
94
0/
1.
48
0
1.
56
5/
1.
16
8/
1.
63
6
20
53
M
1.
31
3/
1.
33
0/
1.
32
5
1.
33
9/
1.
37
5/
1.
33
6
1.
39
9/
1.
35
1/
1.
48
2
Fe
ve
r
of
u
n
kn
ow
n
or
ig
in
(N
eg
at
iv
e
bo
n
e
sc
an
).
1.
24
1/
1.
26
1/
1.
26
6
1.
28
3/
1.
28
5/
1.
31
2
1.
29
4/
1.
24
9/
1.
33
9
21
14
M
1.
63
7/
1.
16
0/
1.
51
0
1.
63
6/
1.
19
8/
1.
53
1
1.
64
5/
1.
20
2/
1.
32
6
Lo
w
ba
ck
p
ai
n
(S
ac
ro
il
ii
ti
s)
.
1.
74
3/
1.
38
9/
1.
67
4
1.
78
9/
1.
43
0/
1.
67
1
1.
78
0/
1.
43
6/
1.
55
4
22
41
F
0.
93
5/
1.
10
1/
1.
46
0
0.
96
4/
1.
13
9/
1.
46
8
0.
95
8/
1.
13
4/
1.
43
9
Lo
w
ba
ck
p
ai
n
(N
eg
at
iv
e
bo
n
e
sc
an
).
0.
89
0/
1.
18
2/
1.
48
7
0.
93
4/
1.
16
5/
1.
43
3
0.
88
4/
1.
10
8/
1.
45
2
23
51
M
1.
53
4/
1.
06
6/
1.
28
3
1.
51
2/
1.
04
7/
1.
24
7
1.
49
9/
1.
11
2/
1.
31
0
Lo
w
ba
ck
p
ai
n
(N
eg
at
iv
e
bo
n
e
sc
an
).
1.
43
6/
1.
03
1/
1.
18
6
1.
40
9/
1.
03
6/
1.
18
1
1.
49
2/
1.
08
7/
1.
19
9
24
69
M
1.
19
7/
1.
10
7/
1.
31
3
1.
11
1/
1.
07
1/
1.
22
6
1.
22
3/
1.
00
8/
1.
34
2
Li
m
bs
p
ai
n
(N
eg
at
iv
e
bo
n
e
sc
an
).
1.
06
1/
1.
07
6/
1.
17
8
0.
99
8/
1.
00
5/
1.
15
2
1.
12
3/
1.
12
1/
1.
19
8
25
57
M
1.
33
9/
1.
03
8/
0.
98
9
1.
23
0/
1.
04
5/
1.
01
7
1.
34
1/
1.
11
2/
1.
02
3
Lo
w
ba
ck
p
ai
n
(N
eg
at
iv
e
bo
n
e
sc
an
).
1.
30
5/
1.
11
7/
0.
96
2
1.
26
1/
1.
09
8/
0.
96
9
1.
28
7/
1.
05
6/
1.
11
2
26
48
F
1.
26
7/
1.
11
0/
1.
82
6
1.
31
9/
1.
17
9/
1.
74
7
1.
50
6/
1.
17
1/
1.
79
9
Lo
w
ba
ck
p
ai
n
(S
ac
ro
il
ii
ti
s)
.
1.
42
5/
1.
19
2/
1.
76
9
1.
49
5/
1.
24
0/
1.
80
2
1.
60
6/
1.
20
2/
1.
78
8
27
39
F
1.
23
3/
1.
04
6/
1.
31
1
1.
20
9/
1.
07
7/
1.
00
0
1.
36
7/
1.
15
4/
1.
00
0
Li
m
p
in
g
ga
it
(N
eg
at
iv
e
bo
n
e
sc
an
).
1.
16
0/
1.
03
3/
1.
30
5
1.
16
8/
1.
06
0/
1.
36
4
1.
07
4/
1.
17
6/
1.
26
7
28
11
M
1.
30
2/
1.
04
4/
0.
98
0
1.
28
9/
1.
10
2/
0.
99
6
1.
28
4/
1.
11
2/
1.
02
3
Fo
ot
p
ai
n
(N
eg
at
iv
e
bo
n
e
sc
an
).
1.
18
7/
1.
09
6/
0.
83
6
1.
28
5/
1.
12
0/
0.
85
3
1.
21
9/
1.
13
2/
0.
91
0
29
44
F
1.
22
3/
1.
18
0/
0.
97
3
1.
20
2/
1.
05
1/
0.
98
6
1.
19
8/
1.
19
7/
0.
99
7
Lo
w
ba
ck
p
ai
n
(N
eg
at
iv
e
bo
n
e
sc
an
).
1.
21
1/
1.
13
9/
0.
97
0
1.
21
6/
0.
08
0/
0.
99
3
1.
18
7/
1.
16
3/
0.
98
3
30
52
F
1.
30
2/
1.
15
9/
1.
19
3
1.
33
9/
1.
11
4/
1.
19
1
1.
30
1/
1.
02
7/
1.
30
4
Li
m
p
p
ai
n
(N
eg
at
iv
e
bo
n
e
sc
an
).
1.
46
7/
1.
21
8/
1.
27
9
1.
50
5/
1.
27
2/
1.
27
3
1.
39
8/
1.
15
6/
1.
37
6
31
32
M
1.
36
9/
1.
03
7/
1.
20
4
1.
34
2/
1.
03
5/
1.
19
0
1.
38
5/
1.
02
1/
1.
30
1
Lo
w
ba
ck
p
ai
n
(N
eg
at
iv
e
bo
n
e
sc
an
).
1.
31
1/
1.
03
1/
1.
09
4
1.
28
7/
1.
02
9/
1.
15
4
1.
32
1/
1.
00
8/
1.
18
7
32
32
M
1.
17
1/
1.
39
8/
1.
14
4
1.
20
7/
1.
38
1/
1.
35
8
1.
13
0/
1.
30
3/
1.
03
2
Lo
w
ba
ck
p
ai
n
(N
eg
at
iv
e
bo
n
e
sc
an
).
1.
14
3/
1.
30
5/
1.
19
9
1.
12
6/
1.
28
8/
1.
24
2
1.
10
4/
1.
28
0/
1.
12
0
33
76
F
1.
22
2/
1.
10
0/
1.
68
7
1.
24
9/
1.
22
3/
1.
65
4
1.
23
9/
1.
11
3/
1.
67
5
Lo
w
ba
ck
p
ai
n
(N
eg
at
iv
e
bo
n
e
sc
an
).
1.
22
3/
1.
04
6/
1.
59
9
1.
28
3/
1.
04
5/
1.
61
1
1.
22
3/
1.
02
4/
1.
62
8
Author's personal copy
computer methods and programs in b iomed ic ine 9 8 ( 2 0 1 0 ) 15–26 25
Fig. 15 – Plots of difference against mean for SII measurements with our approach versus (a) medical expert 1, (b) medical
expert 2. Reference lines are mean+2SD and mean−2SD.
Table 2 – Correlation, bias, and precision for expert 1 and
expert 2 versus our approach. In the linear regression
equation, x1 is the SII measured by expert 1, x2 is the SII
obtained by expert 2, and y is the SII obtained by our
approach.
Expert 1 Expert 2
L y=0.8478x1 + 0.211 y=0.8512x2 + 0.1693
R2 0.9231 0.9171
Bias −0.0203 −0.0202
Precision 0.1088 0.2028
L: linear regression equation; R2: coefﬁcient of determination.
correlated closely (R2 =0.923, 0.917). The agreement between
our approach and 2 medical experts was found to be good as
shown in Table 2 and Fig. 15.
5. Discussion
Clinical diagnosis of early phase sacroiliitis is very difﬁcult.
Laboratory ﬁndings are nonspeciﬁc. The radiological changes
may appear several years after onset of the disease [1,2].
However, early diagnosis is import to start an appropriate
treatment.
Quantitative sacroiliac scintigraphy has been used to mea-
sure the SII more objective. In some studies, a quantitative
analysis was performed for high uptake in sacroiliac joints
in normal subjects, but these studies yielded disagreeing
results [8]. Some studies reported that SII is a sensitive index
for detecting the early phase disease, while others have
not shown [6,15–17]. Many explications for this have been
offered, including differences in ethnicity, gender, and the
case selection method [7]. Besides, differences between bone
agents used and imaging time also inﬂuence the SII’s value
[18,19]. Different ROI selections between operators would
seem particularly important because of uptake variability
within reference regions [16]. In addition,manual ROI drawing
would cause intra- and inter-operator variations artiﬁcially. In
this approach, we performed a set of digital image processing
techniques to avoid the drawbacks ofmanual ROI drawing.We
consider that automatic computer-aided SII measurement in
bone scintigraphy is promising for better consistency in SII
counting evaluation for either normals or abnormals.
6. Conclusion
In this approach, a fully automatic computer-aidedSII analysis
system for bone scintigraphywas developed. The results of SII
from the automatic measurement correlated well with those
from 2 medical experts’ manual measurements (R2 =0.923,
0.917), and the predictive performance analysis (bias, preci-
sion) presented a satisﬁed result. It is a manpower and time
saving program. The results of this work achieved the initial
goals of fully automatic processing, and consisted well with
clinical interpretation from manual works. Based on this pro-
posed approach, these satisfactory results encourage us to
further develop a new and more efﬁcient approach that can
also be applied to single photon emission computed tomogra-
phy (SPECT) images.
Acknowledgement
Thisworkwas supported inpart by theNational ScienceCoun-
cil, Taiwan, Republic of China, under the grant number NSC
96-2221-E-155-057-MY3.
r e f e r enc e s
[1] A. Hadi, P. Hickling, M. Brown, A. Al-Nahhas, Scintigraphic
evidence of effect of inﬂiximab on disease activity in
ankylosing spondylitis, Rheumatology 41 (2002) 114–116.
[2] N.A. Dunn, B.H. Mahida, M.V. Merrick, G. Nuki, Quantitative
sacroiliac scintiscanning: a sensitive and objective method
for assessing efﬁcacy of nonsteroidal, anti-inﬂammatory
drugs in patients with sacroiliitis, Ann. Rheum. Dis. 43
(1984) 157–159.
[3] B.C. Lentle, A.S. Russsell, J.S. Percy, F.I. Jackson, The
scintigraphic investigation of sacroiliac disease, J. Nucl. Med.
18 (6) (1977) 529–533.
[4] A. Ho, Scintillation camera with multichannel collimators, J.
Nucl. Med. 5 (1964) 515–531.
[5] L. Sˇajn, M. Kukar, I. Kononenko, M. MilCˇinski, Computerized
segmentation of whole-body bone scintigrams and its use in
automated diagonosis, Comput. Methods Programs Biomed.
80 (2005) 47–55.
[6] S.D. Miron, M.A. Khan, E.J. Wiesen, I. Kushner, E.M. Bellon,
The value of quantitative sacroiliac scintigraphy in
Visual Enhancement for Sentinel Lymph Node Mapping in Breast Cancer by 
Multiple Display Formats of SPECT/CT Images 
 
 
Jia-Yann Huang 
Department of Electrical 
Engineering, Yuan Ze 
University, Chungli, Taiwan 
john@adm.cgmh.org.tw 
 
Pan-Fu Kao 
Department of Nuclear 
Medicine, Buddhist Tzu Chi 
General Hospital, Taipei, 
Taiwan 
kaopanfu@tzuchi.com.tw 
Yung-Sheng Chen 
Department of Electrical 
Engineering, Yuan Ze 
University, Chungli, Taiwan 
eeyschen@saturn.yzu.edu.tw 
 
 
Abstract 
 
Lymph node status is a main factor in determining 
the stage, suitable therapy and outcome in patients 
with breast cancer. It is therefore of clinical 
importance to accurately identify all sentinel lymph 
nodes (SLNs) for each individual tumor before surgery. 
Composite Single-Photon Emission Computed 
Tomography and Computed Tomography (SPECT/CT) 
scanner has the great potential to enhance 
topographic orientation and diagnostic sensitivity of 
SLN imaging. In this paper, a visual enhancement of 
sentinel lymph node mapping in breast cancer of 
SPECT/CT images was proposed. Our approach is 
based on the isodata histogram thresholding, image 
fusion, and 3-dimensional (3D) volume rendering. By 
improving the visual impression of SPECT/CT images, 
it is useful to help surgeons to precisely locate all 
SLNs in patients with breast cancer. 
 
1. Introduction 
 
SLN is the first draining node in lymph node basin 
from the primary tumor [1]. It is most likely the site of 
early metastasis. The detection and biopsy of the SLN 
has been implemented in the surgical treatment of 
breast cancer [2-3]. Conventional lymphoscintigraphic 
planner imaging was a common means for sentinel 
node imaging prior to sentinel node biopsy [4]. 
Nevertheless, SLNs close to the injection site were 
hard to detect on planer images due to scatter radiation 
[5]. SPECT of sentinel lymph nodes has been 
introduced as a means of improving spatial resolution 
and orientation. However, SLNs detected with this 
technique still cannot be imaged in relation to 
anatomical information [6]. Kretschmer et al. [7] 
performed SPECT and the image fusion technique 
with CT in 29 patients with malignant melanoma, the 
fusion procedure was tedious, time consuming, and 
highly dependent upon the technologist’s experience. 
Recently, a hybrid imaging system consisting of a low-
dose CT and a gamma camera installed in a single 
gantry has been introduced. The SPECT/CT scanner 
allows transmission (CT) and emission (SPECT) scans 
to be performed without changing the patient’s 
position, thereby allowing for direct and correct fusion 
of images obtained with the two image modalities. An 
axial SPECT image and an axial CT image are shown 
in Fig. 1(a) and 1(b), respectively. 
Based on such a hybrid imaging system, an 
approach is presented for improving the visual 
impression of SPECT/CT images with multiple display 
formats to help surgeon accurately identifying all 
SLNs. 
  
   
(a)                                  (b) 
  
Figure. 1. Illustrations of the axial images of (a) 
SPECT and (b) CT, respectively. 
 
2. Imaging protocol 
 
At 40 minutes after peritumoral tissue injection of 
the radiotracer, SPECT/CT was initiated on an 
integrated SPECT/CT scanner (Hawkeye, GE 
2008 International Conference on BioMedical Engineering and Informatics
978-0-7695-3118-2/08 $25.00 © 2008 IEEE
DOI 10.1109/BMEI.2008.167
1210286
max
max 1
( ) ( )
h
f g p u du= ∫A AI                    (2) 
Where AI  is the input image that was obtained after 
the noise removing processing, maxg  is the maximum 
gray level value (255 in an 8-bit gray level scale), maxh  
is the maximum gray level of AI , and ( )p uA  is the 
probability mass function of AI . Figure 4(a) shows an 
equalized CT slice. After aforementioned processing, 
there was no noise outside the body area, the skin 
information can be found by searching the body 
contour from Fig. 4(a), and obtained as shown in Fig. 
4(b).  
   
(a)                                    (b) 
 
Figure 4.  Illustrations of (a) an equalized CT 
axial image, and (b) a body contour image. 
 
3.2. Axial CT images thresholding 
 
By using isodata algorithm, John and Michael [8] 
successfully separated soft tissue and bone from a CT 
axial image. The iterative technique (isodata) for 
selecting a threshold was developed by Ridler and 
Calvard [9]. The histogram is initially segmented into 
two parts using a beginning threshold value as 
1
0 2
Bθ −= , half the maximum dynamic range. The 
sample mean , 0( )fm  of the gray values associated with 
the foreground pixels and the sample mean , 0( )bm  of 
the gray value 1θ  are then calculated as the average of 
these two sample means. If change in threshold is 
small enough (0.04%), then algorithm stops. Such an 
algorithm is therefore defined as 
, 1 , 1 1( ) / 2 untilk f k b k k km mθ θ θ− − −= + ≈ .      (3) 
In our approach, we adopted this algorithm to suppress 
soft tissue in all equalized axial CT images, where a 
processed-slice image combined with body contour 
image can be shown in Fig. 5. 
 
 
Figure 5. A segmented CT axial image 
combined with body contour image. 
 
3.3. Fusion of SPECT and CT images 
 
SPECT and CT slices were acquired by an 
integrated SPECT/CT scanner without changing the 
patient’s position, thereby there was no registration 
processing needed before fusion, allowing for direct 
and correct fusion of images obtained with the two 
image modalities. A fusion result of SPECT and CT 
slices can be shown in Fig. 6. 
 
 
 
Figure 6.  A fusion result of SPECT/CT slices. 
 
3.4. 3D volume rendering 
 
Our approach adopted OpenGL [10] as the graphics 
application program interface (API). The fused slices 
formed the volume data, and then an opaque 3D 
volume rendering image was obtained by our program, 
and can be displayed as in Fig. 7(d). 
 
4. Experimental result 
 
In order to evaluate the proposed approach, we 
implemented a program using C++ Builder 6.0, and 
performed experiments on an image database from the 
Department of Nuclear Medicine, Buddhist Tzu Chi 
General Hospital, Taipei, Taiwan. The five random 
1212288



Page 3-1 
 
行政院國科會補助專家學者出席國際會議報告 
                                                 98年 10月 31日 
報 告 人 姓 名 陳永盛 服務機關名稱（請註
明系所）及職稱 
元智大學 電機系 教授 
會議期間及地點 
自 98 年 10 月 17 日
至 98 年 10 月 19 日
中國‧天津 
國科會專題研究計畫
核定補助文號 
NSC 96-2221-E-155-057-MY3
會 議 名 稱 
（ 中文 ）2009生醫工程與資訊國際研討會 
（ 英文 ）BMEI2009: The 2nd International Conference on BioMedical 
Engineering and Informatics 
發 表 論 文 題 目 
Automatic Thyroid Volume Estimation in Graves’ Disease Using Planar 
Scintigraphy 
 
 本次蒙國科會專題研究計畫（出席國際會議經費）補助，得以參加在中國‧天津舉行之
BMEI2009: The 2nd International Conference on BioMedical Engineering and Informatics 
（Oct 17-19, 2009），在此報告之前先行感謝。本人於10月16日早上8:00搭乘港龍航空(KA 489)
從台北到香港，再搭乘 12:50 港龍航空 (KA1104)於下午 16:00 抵達天津。大會地點位於天津
市華苑產業園區中心的天津賽象酒店（圖 1, 2）。回程於 10 月 19 日早上 8:35 搭乘港龍航空
(KA1103)經香港轉 13:15 國泰航空(CX 564)返回桃園，結束此次與會的行程。 
  
                   圖 1                                     圖 2 
 
我們此次發表之論文為 Automatic Thyroid Volume Estimation in Graves’ Disease Using 
Planar Scintigraphy，主要介紹一個自動影像強化與輪廓偵測演算法以利計算甲狀腺的體積來
幫助格拉夫葡萄酒疾病的診斷。此法也採用先前 2009 年我們發表在 Pattern Recognition 的 disk 
expansion (DE) 方法(A disk expansion segmentation method for ultrasonic breast lesions)，經過本
論文的研究，DE 法確實很適合如超音波或類似閃爍造影術之醫學影像的臨床應用。預計於會
議結束後開始整理或後續實驗以便投稿於相關領域的期刊中。本次論文是以 Poster 發表，與
會者和我們有需多的互動與討論 (圖 3, 4)。 
Page 3-3 
 
圖 6 
 
論文集： 
紙本論文集 (Vol. 1)、CD 論文集、研討會摘錄手冊 
 
次頁開始為所發表的論文影本。 


Page 3-1 
 
行政院國科會補助專家學者出席國際會議報告 
                                                 99年 7月 19日 
報 告 人 姓 名 陳永盛 服務機關名稱（請註
明系所）及職稱 
元智大學 電機系 教授 
會議期間及地點 
自 99 年 7 月 11 日 
至 99 年 7 月 14 日 
中國‧山東‧青島 
國科會專題研究計畫
核定補助文號 
NSC 96-2221-E-155-057-MY3
會 議 名 稱 
（ 中文 ）2010機器學習與控制學國際研討會 
（ 英文 ）International Conference on Machine Learning and Cybernetics 2010
發 表 論 文 題 目 
1. Human subject-based video browsing and summarization 
2. An intelligent positioning approach: RSSI-based indoor and outdoor 
localization scheme in zigbee networks 
 
 本次蒙國科會專題研究計畫（出席國際會議經費）以及元智大學部分經費補助，得以參
加在中國‧山東‧青島舉行之 ICMLC: International Conference on Machine Learning and 
Cybernetics （July 11-14, 2010），在此報告之前先行感謝。本人與數位教授（本校工管系陳
以明老師、海洋大學謝君偉老師、以及中華大學兩位教授）於 7 月 11 日早上 7:10 搭乘國泰
班機（CX465）從台北到香港，再搭乘 10:40 港龍班機（KA950）於中午 13:55 抵達青島。大
會地點在金灣路上的青島海爾洲際酒店（圖 1, 2）。大會結束後，便參觀山東的一些歷史文物
景點，如：嶗山、蓬萊、泰山、曲阜、博物館等。回程於 7 月 18 日下午 14:55 搭乘港龍班機
（KA951）經香港轉 19:55 國泰班機（CX468）返回桃園，結束此次與會的行程。 
  
                   圖 1                                     圖 2 
 
我們此次發表之論文為 Human subject-based video browsing and summarization 與 An 
intelligent positioning approach: RSSI-based indoor and outdoor localization scheme in zigbee 
networks，前者在探討於影片中建立並找尋特定的片段，其對於影片的 summarization 有一定
的助益；後者則探討使用無線感測網路於室內與室外之定位的策略與效能。這兩篇論文，預
計於會議結束後開始整理或後續實驗以便投稿於相關領域的期刊中。由於安排在 Intelligent 
Page 3-3 
 
 
圖 7 
 
論文集： 
紙本論文集 (Vol. 6)、CD 論文集、研討會摘錄手冊 
 
次頁開始為所發表的兩篇論文影本。 
  
color features is extracted from the part of torso. In addition, 
to overcome the problem of the nature of non-rigid body 
movement, an online learning classifier which is based on the 
measure of Mahalanobis distance is proposed to recognize 
the individuals appeared in the video sequence.   
The remainder of this paper is organized as follows.  In 
Section 2, the method used for face detection is described. In 
Section 3, the feature set used for human subject recognition 
is introduced. In Section 4, the online training classifier based 
on the Mahalanobis distance is shown. We then detail our 
experimental results in Section 5, and some concluding 
remarks in Section 6. 
2. Face detection 
In this phase, we first use the support vector machine 
based (SVM-based) face detector developed by Kienzle et al. 
[24] to perform the face detection task. Then, we apply the 
proposed filtering process to remove false positives detected 
by the above face detector. 
2.1  SVM-based face detector 
To count the number of people watching an 
advertisement on a TV billboard, it is necessary to detect 
frontal part of faces because the people are “really watching” 
the advertisement. Therefore, face detection is the first 
important step to be accomplished. To satisfy the real-time 
requirement, we adopt the SVM-based face detector 
developed by Kienzle et al., which can provide fast 
approximations of support vector decision functions. The 
detail of the face detector can be found in [24]. 
Most of the faces can be detected correctly based on this 
approach, but there are some false positives. Removing false 
positives from the detected frontal face set is necessary since 
the detected results (no matter whether they are correct or not) 
will be further analyzed to compute the number of viewers. 
Since the subsequent face recognition module will consume a 
great deal of computational power, a pre-processing step to 
filter out false positive faces is very important. In the next 
section, we describe an effective approach for removing false 
positives from the set of detected face candidates. 
2.2  Filtering false positive faces 
Face detection has attracted a great deal of attention in 
the past decade. Well-developed face detection techniques, 
such as OpenCV and Kienzle et al. [24], can achieve success 
rates of 80%-90%. However, their detection rate of false 
positives is in the 10%-20% range. In our system, we adopt a 
temporal filter to remove those detected candidates that 
appear and disappear suddenly. Detected non-faces usually 
behave in this manner and would not continually appear in a 
short duration. Therefore, a probability that measures the 
spatiotemporal characteristic of a candidate is as follows: 
( ) ( ) ( ), ,i jMSSD o o t i jiP o e− ⋅∆=                       (1) 
where MSSD(oi, oj) denotes the minimum sum of square 
difference between candidates oi and oj in different time 
instant in a short duration. ∆t(i, j) means the time interval in 
terms of the number of frames between oi and oj. The 
measure not only captures the appearance similarity between 
detected candidates in different time instant but also evaluates 
the temporal consistency between them. 
3. Feature extraction 
When summarizing video content based on the number 
of people actually watching toward the camera in the 
captured video, we encounter many difficult problems.  First, 
one has to make sure that the people are really “watching” 
camera. Second, some people may watch it for a long while, 
but they cannot be double counted. In these situations, the 
extracted faces should have a frontal orientation and the 
chosen feature set should have strong discriminative power 
so that correct assessments can be made. Since each person 
only has a few distinct facial features, the features are not 
enough. We propose extracting a separate set of features from 
part of the torso so that the complete feature set will contain 
richer information. We extract features from the front of each 
individual’s torso. The features extracted are the 
quadtree-based color and edge features. We describe these 
features in Section 3.1 and Section 3.2, respectively. 
3.1  Quadtree-based color features 
The general quadtree [19][21] is a tree data structure in 
which each internal node has up to four children. Quadtree 
are most often used to partition a two dimensional space by 
recursively subdividing it into four quadrants or regions. It 
enables an efficient representation of the resulting 
decomposition. 
Color features are commonly used now in human visual 
applications [1][3]. We used the YUV color space, because of 
the following reasons: (1) the RGB color space is 
intensity-sensitive, which does not help image processing or 
object recognition; (2) all three RGB components need to be 
of equal bandwidth to generate any color within the RGB 
color cube. Also, processing an image in the RGB color 
space is usually not the most efficient method. 
A quad tree is a tree whose nodes either are leaves or 
with four children. We then use Y component to represent an 
image by a quad-tree representation [19-20]. The image is 
2797
Proceedings of the Ninth International Conference on Machine Learning and Cybernetics, Qingdao, 11-14 July 2010
  
determine if a template iO  is reliable enough to be selected 
as a training sample for the same person.  The similarity 
( , )i jM O O  between two templates is measured by:  
1 2-( )( , ) s tD Di jM O O e
α α+
= ,                      (4) 
where iα  is the weight for its 
thi  corresponding distance, 
sD is the spatial distance defined by 1 1 2 2sD D Dβ β= + , which 
is a linear combination of the distance of the quadtree-based 
color features 1D  and quadtree-based edge features 2D  
with weights 1β  and 2β  respectively. The weight sets α  
and β  are determined empirically based on extensive 
experiments and, tD is the time interval between iO  and 
jO . If (.,.)M  is larger than a pre-defined threshold, an 
unknown object is found. 
4.2  Online learning algorithm 
In every classifier, we need to collect enough positive 
training samples, adaptively determine the weight for each 
feature, and adaptively learn the region confidences for each 
captured template. We then use the method based on 
Mahalanobis distance criterion [13] to determine a projection 
orientation so that the two classes will be properly separated. 
If the covariance matrix is the identity matrix, the 
Mahalanobis distance reduces to the Euclidean distance. 
Mahalanobis distance can be applied to discrete random 
variable or data set [16]. The standard deviation of a discrete 
random variable is the root-mean-square deviation of its 
values from the mean. Our feature variable MiO takes on Q 
values 1,..., NO O which are equal probability, and then their 
standard deviation Moσ  can be calculated as follows: 
1. Find the mean 
M
O , of the values. 
2. For each value MiO  calculate its deviation ρ  from 
the mean. 
3. Calculate the squares of these deviations. 
4. Find the mean of the squared deviations. This quantity 
is the variance 2 MOσ . 
5. Take the square root of the variance. 
The standard deviation MOσ  is defined by 
 ,M MiO Oρ = −                              (5) 
2
1
1 ( ) .M
Q
M M
iO
i
O O
Q
σ
=
= −∑    (6) 
The method based on Mahalanobis distance equation is 
represented as follow: 
2
2
( )( , ) ,
M
M
M new
c new
O
O OD O O
σ
−
=                 (7) 
where  is the diversity of categories.cD  
We use the criterion to adaptively determine the weight for 
each feature. We then use the proposed method based on 
Mahalanobis distance to calculate the similarity between two 
classifiers, and update new feature the most similar classifier. 
If diversity is larger than a pre-defined threshold, we do not 
update the classifier. 
5. Experimental Results 
In this section, we present the experimental results of the 
proposed system. In order to evaluate the performance of our 
system, we capture real videos under different environments. 
The details and results of experiments will be explained 
clearly in the following sections. We implemented the 
proposed system using Visual C++ 6.0 with a 2.8 GHz Intel 
CPU. All the tested video sequences is of frame size 640 × 
480 and frame rate 30 fps. 
5.1 Performance evaluation of human subject detection 
We take several video to test the performance of our 
system. Our experimental result including two types; a quad 
tree based features and proposed online learning classifier.  
We use three videos to test the human subject detection.  
Sequence 1 is a 5 minutes indoor video containing 5 human 
subjects, and sequence 2 is a 5 minutes outdoor video 
containing 5 human subjects, and sequence 3 is a 10 minutes 
mix video containing 8 human subjects. In this video 8 
human subjects moved frequently in the field of view and 2 
of them left the field for a short time and then returned. 
System error occurred in following situation; human subjects 
changed his appearance by removing his coat. The system 
was affected by this action because it relies on the color and 
edge of the torso. The experimental results are shown in 
following:  
5.2 Performance evaluation of scene change detection 
To evaluate the performance of a scene change detection 
algorithm, we need to consider these two rates 
simultaneously. To maximize the recall and precision rates 
means to minimize the number of misses and the number of 
false detection, respectively. A good shot change detection 
algorithm needs to have small numbers of misses as well as 
false detection. In our system, we have proposed a scene 
2799
Proceedings of the Ninth International Conference on Machine Learning and Cybernetics, Qingdao, 11-14 July 2010
  
IEEE International Conference on Acoustics, Speech, and 
Signal Processing, vol.2, pp.1228-1231, 1996. 
[5] Y. Zhuang, “Adaptive key frame extraction using 
unsupervised clustering,” Proc. of IEEE International 
Conference on Image Processing, pp.866-870, 1998. 
[6] D. Roqueiro and V. A. Petrushin, “Counting People using 
Video Cameras,” International Journal of Parallel, Emergent 
and Distributed Systems 22 (3),   pp.193-209,2007 
[7] A. B. Chan, Z. S. Liang, and N. Vasconcelos, “Privacy 
Preserving Crowd Monitoring: Counting People without 
People Models or Tracking,” Proc. of IEEE Conference on 
Computer Vision Pattern Recognition, Jun 2008. 
[8] S. Y. Cho, T. W. S. Chow, and C. T. Leung, “A Neural-Based 
Crowd Estimation by Hybrid Global Learning Algorithm,” 
IEEE Transactions on Systems, Man, and Cybernetics - Part 
B, vol. 29, no. 4, pp.535-541. Aug 1999. 
[9] D. Kong, D. Gray, and H. Tao, “Counting Pedestrians in 
Crowds Using Viewpoint Invariant Training,” British 
Machine Vision Conference, 2005. 
[10] R. Ma, L. Li, W. Huang, and Q. Tian, “On Pixel Count Based 
Crowd Density Estimation for Visual Surveillance,” Proc. of 
IEEE International Conference on Cybernetics and 
Intelligent Systems, Singapore, pp.170-173, Dec 2004. 
[11] C. S. Regazzoni, A. Tesei, “Distributed data fusion for 
real-time crowding estimation,” Signal Processing, vol. 53, 
pp47-63, 1996. 
[12] A. N. Marana, L. F. Costa, R. A. Lotufo, and S. A. Velastin, 
“Estimating Crowd Density with Mikowski Fractal 
Dimension,” Proc. of IEEE International Conference on 
Acoustics, Speech, and Signal Processing, vol.6, 
pp.3521-3524, 1999. 
[13] P. Viola and M. Jones, “Rapid object detection using a 
boosted cascade of simple features,” Proc. of IEEE 
Conference on Computer Vision and Pattern Recognition, 
pp.I511-I518, 2001. 
[14] R. Ma, L. Li, W. Huang, and Q. Tian, “On Pixel Count Based 
Crowd Density Estimation for Visual Surveillance,” Proc. of 
IEEE International Conference on Cybernetics and 
Intelligent Systems, vol.1, pp.170-173, Dec 2004. 
[15] P. Viola and M. Jones, “Robust Real-Time Face Detection,” 
International Journal of Computer Vision, pp.137-154, 2004. 
[16] I. Haritaoglu, D. Harwood, and L. S. Davis, “W4: Real-Time 
Surveillance of People and Their Activities,” IEEE 
Transactions on Pattern Analysis and Machine Intelligence, 
vol. 22, no.8, pp.809-830, Aug 2000. 
[17] P. Viola, M. Jones, and D. Snow, “Detecting Pedestrians 
Using Patterns of Motion and Appearance,” Proc. of IEEE 
International Conference on Computer Vision, vol.2, 
pp.734-741, 2003. 
[18] Y. Freund and R. E. Schapire, “A decision-theoretic 
generalization of on-line learning and an application to 
boosting,” Journal of Computer and System Sciences, 
pp.23-37, 1997. 
[19] H. Y. Mark Liao, D. Y. Chen, C. W. Su, and H. R. 
Tyan,“Real-time Event Detection and Its Application to 
Surveillance Systems,” Proc. of IEEE International Symposium 
on Circuits and Systems, art, no. 1692634, pp.509-512, 2006.  
[20] T. Markas and J. Reif, “Quad tree structures for image 
compression applications,” Information Processing & 
Management, vol. 28, no. 6, pp.707-721, 1992. 
[21] A. Martínez-Usó, F. Pla, and P. García-Sevilla, “A 
Quadtree-based Unsupervised Segmentation Algorithm for 
Fruit Visual Inspection,” Lecture Notes in Computer Science, 
pp.510-517, 2003.   
[22] Robinson, Guner S, “Edge Detection by Compass Gradient 
Masks,” Comput Graphics Image Process 6 (5), pp.492-501, 
1977. 
[23] C. Papageorgiou and T. Poggio, “Trainable Pedestrian 
Detection,” Proc. of IEEE International Conference on Image 
Processing, pp.35-39, 1999. 
[24] W. Kienzle, G. Bakir, M. Franz and B. Scholkopf, “Face 
Detection - Efficient and Rank Deficient,” Advances in 
Neural Information Processing Systems, Vol. 17, pp. 673-680, 
2005. 
 
2801
Proceedings of the Ninth International Conference on Machine Learning and Cybernetics, Qingdao, 11-14 July 2010
 In this work, we propose two localization algorithms 
which are for indoor and outdoor positioning of sensor 
networks. Basing on the RSSI of the signal, these algorithms 
can find the position of the transmitting node. The proposed 
approach needs no fingerprinting process and pattern 
database, thus a localization of sensor network can be 
conducted in a feasible and efficient way. 
The remainder of the paper is structured as follows. In II, 
we detail the two proposed algorithms. In III, the numerical 
results are presented and discussed. Finally, the paper is 
concluded in IV.  
2 Indoor and Outdoor localization Schemes 
2.1  System Architecture 
In both indoor and outdoor environment, a common 
architecture which gathers the information of the signal 
strength is needed. As shown in Fig. 1, in a sensor network, 
several reference nodes that have known positions are placed 
into the environment, which we call “sensing nodes” 
hereafter. When the node whose position is to be detected 
(called “sensed node” hereafter) appear in the network, all 
sensing nodes measure the signal strength from the sensed 
node, then report the data to the coordinator node which 
controls the network. The coordinator reports all gathered 
information to the PC in its back end, where the localization 
process is performed. 
With the existing AODV (Ad hoc On-Demand Distance 
Vector) routing protocols, this architecture can be easily 
implemented in a Zigbee network. When a sensed node joins 
the network, it must negotiate with the coordinator and be 
given a unique ID. After the initialization process, all other 
nodes in the network is informed the new-coming node. The 
coordinator can then order the sensing node to sense the 
signal from the sensed node. In a Zigbee packet, there is the 
information field called LQI (link quality index), which 
records the link quality (i.e., signal strength) of the packet in 
8 bytes. When receiving the periodical broadcast message of 
the sensed node, the strength of the receiving signal is 
provided by the physical layer of the device and written in 
the field. So the sensing nodes can periodically report the 
latest LQI to the coordinator. Having gathered the signal 
strength from all reference nodes, the coordinator can 
provide them to the PC for analysis. 
Typically, the representation of LQI is different by the 
device vendor. Before analyzing the strength of signals, the 
value of the LQI must be transferred into dB. Then, the 
system in the back end can use different positioning scheme 
to do the positioning job. 
 
 
Figure 1. System Architecture. 
2.2  Indoor Positioning Method 
In the indoor environment, the signal strength of the 
receiving signal may be severely affected by the 
intermediate barrier, such as doors and walls. Therefore, the 
receiving strength can effectively reflect the position only 
when there are no barriers between the sensing nodes and 
the analyzing node. 
 
Figure 2. An example of the node placement. 
 
Therefore, given the structure of the building, our Indoor 
Positioning Method places the sensing node in the ends of 
the corridor so that no matter the position of the node, at 
least two sensing nodes can directly receive the signal and 
provide useful information. An example is shown in fig. 2, 
in a building with four corridors, four sensing nodes are 
placed in each end of the corridors. 
To decide the position of the node basing on the gathered 
information of the signal strength, we propose three methods. 
First and most intuitively, we classify the positioning plane 
into several zones by the position of the sensing nodes. 
2755
Proceedings of the Ninth International Conference on Machine Learning and Cybernetics, Qingdao, 11-14 July 2010
  
(a)  CIA 
 
(b)  MIA 
Figure 4. Two Positioning Algorithms 
3 Numerical Results 
3.1  Experiment Configuration 
In the experiment, we use Zigbee devices FT6250 and 
FT6251 provided by Fontal Technology [11]. We rewrite the 
protocol so that the sensing node can periodically report the 
LQI of the sensed node to the coordinator.  
3.2  Indoor Results 
The indoor positioning experiments are conducted in the 
7th building of Yuan-Ze University. Two difference 
placements and the respective results are shown in Fig. 5(a) 
and (b). We move the sensed node and measure the signal 
strength using method 1. To compare the actual position to 
the sensed position, we move the sensed node to the door of 
each room and see what area it is placed in by the algorithm. 
Each position answered by the algorithm is marked by the 
color of the coordinator. However, for positions which have 
same receiving strength from two sensing nodes, they are 
given two colors. As shown in Fig. 5(a), we find that under 
this placement of the sensing nodes, some positions are 
placed into wrong area. The rooms in the bottom are close to 
either SN1 or SN3. However, they are put into area 2 since 
SN2 senses the strongest receiving strength. This is because 
although it takes longer to walk from SN2 to these rooms, 
their physical distance is shorter, and the building is hollow, 
which lets the signal can directly pass through. To fix this 
problem, we adjust the position of sensing nodes and 
conduct another test, whose results are shown in Fig. 5(b). 
By moving the position of SN2, the rooms in the bottom are 
not placed to area 2 anymore, and the building can be 
perfectly clustered into four areas. The difference between 
Fig. 5(a) and (b) show that the position of the sensing nodes 
is critical and may severely affect the accuracy. 
 
(a) 
 
(b) 
 
Figure 5. Indoor results of different sensing positions 
 
Next we study the method 2 which also considers the 
second strongest receiving strength, which is shown in Fig. 
6(a). By the placement of Fig. 5, we can further cluster each 
2757
Proceedings of the Ninth International Conference on Machine Learning and Cybernetics, Qingdao, 11-14 July 2010
  
 
Figure 7. The measured RSSI and the analytical value. 
 
TABLE 8. THE RESULTS OF CIA AND MIA 
Error(m) 
 
Area(m)/Method 
Minimal 
error 
Maximal 
error 
Average 
error 
CIA 0.33 1.82 0.89 4x4 MIA 0.24 0.82 0.36 
CIA 0.74 1.66 1.16 6x6 MIA 0.93 1.39 1.12 
CIA 0.89 3.40 1.98 8x8 MIA 0.58 2.34 1.68 
CIA 0.97 6.08 2.59 10x10 MIA 0.32 3.46 1.94 
4 Conclusion 
In this paper, we study the strength-based positioning 
methods in sensor network. Two kinds of methods, which 
are indoor and outdoor methods, is separately studied and 
discussed in this paper. In indoor methods, the receiving 
strength cannot be directly transferred into the distance since 
the obstacles conditions are difference all over the building. 
Therefore, the building is clustered into many sectors, and 
the position is decided by the several nodes with strongest 
receiving strength. On the other hand, we use signal strength 
to estimate the distance in open area. 
We also conduct experiments to observe the accuracy of 
the proposed methods. The results show that our approaches 
can provide satisfying accuracy no matter in indoor or 
outdoor environment. Since no fading effect is caused by 
buildings in outdoor environment, it provides better 
accuracy. In the future, we will extend the research to more 
complicated environment such as 3-D positioning (e.g., 
multi-floor environments.) 
Acknowledgment 
This work was supported by National Science Council 
(NSC), Taiwan, under Grant Number 
NSC98-2221-E-155-027. 
References 
[1] Jon, “Designing with 802.15.4 and ZigBee,” Industrial 
Wireless Applications Summit San Diego, pp.11, Mar. 
2004.  
[2] I.F. Akyyiliz, S. Weilian, Y. Sankarasubramaniam, E. 
Cayirci, “A Survey on Sensor Networks,” IEEE 
Communications Magazine, pp.102-114, Aug. 2002. 
[3] The ZigBee alliance website. http://www.zigbee.org 
[4] B. Hofmann-Wellenhof, H. Lichtenegger, and J. Collins, 
Global Positioning System: Theory and Practice, 
Springer-Verlag, 4th edition, 1997. 
[5] S. Capkun, M. Hamdi, and J. Hubaux, “GPS-free 
positioning in mobile ad-hoc networks,” Proceedings of 
the 34th Annual Hawaii International Conference on 
System Sciences, pp.10, Jan. 2001. 
[6] Balogh, G., Lédeczi, A., Maróti, M., and Simon, “Time 
of arrival data fusion for source localization”. In 
Proceedings of The WICON Workshop on Information 
Fusion and Dissemination in Wireless Sensor Networks, 
Budapest, Hungary, 2005. 
[7] K. Kaemarungsi, “Design of Indoor positioning system 
based on location fingerprint technique”, University of 
Pittsburgh, 2005. 
[8] K. Pahlavan, X. Li, and J. P. Makela, “Indoor 
geolocation science and technology,” IEEE Commun. 
Mag., vol. 40, no. 2, pp. 112‐118, Feb. 2002. 
[9] M. Srbinovska, C. Gavrovski, V. Dimcev, “Localization 
estimation system using measurement of RSSI based on 
ZigBee standard”. ELECTRONICS’ 2008, pp. 46-47, 
Sozopol, Bulgaria, Sep. 2008. 
[10] X. Li, “RSS-Based Location Estimation with Unknown 
Path loss Model”, IEEE Transactions on Wireless 
Communications, vol. 5, no. 12, Dec. 2006. K. Aamodt, 
“Application Note AN042 (Rev. 1.0)”, pp. 7-8, Jul. 
2006. 
[11] http://www.fontaltech.com/ 
[12] K. Aamodt, “Application Note AN042 (Rev. 1.0)”, pp. 
7-8, Jul. 2006. 
 
2759
Proceedings of the Ninth International Conference on Machine Learning and Cybernetics, Qingdao, 11-14 July 2010
96年度專題研究計畫研究成果彙整表 
計畫主持人：陳永盛 計畫編號：96-2221-E-155-057-MY3 
計畫名稱：基於超音波醫學影像即時分析診斷系統的數位服務之研發 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 2 0 75% 
已刊登之ㄧ篇為
兩個計畫共同成
果。 
另外有一篇國際
期刊論文第二次
修訂審稿中。 
此外，尚有一篇剛
投稿，故未列入本
次計畫的成果報
告中。 
研究報告/技術報告 0 0 100%  
研討會論文 2 0 75% 
篇 
 
論文著作 
專書 1 0 100% 章/本 
一篇國際版 Book 
Chapter (本計畫
主持人為 Invited 
Editor) 
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 5 0 100%  
國外 
參與計畫人力 
（外國籍） 博士生 2 0 100% 
人次 
 
 
