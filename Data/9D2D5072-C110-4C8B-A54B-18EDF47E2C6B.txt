sensors with amplitude and phase errors. Let gs
and gj be the array gain error vectors at signal
direction and interference direction, respectively.
The array elements are i.i.d., zero-mean, complex
Gaussian random variables with the same variance
σ2a. Let a(θ) be the steering vector with an arrival
angle of θ, it can be expressed as
a(θ) = [edw
2π
λw sin θ, e2dw
2π
λw sin θ, · · · , eNdw 2πλw sin θ]T
(1)
where dw and λw are the sensor distance and wave-
length, respectively, and the superscript T denotes
matrix transpose. The received vector of the de-
sired signal can be written as
a˜s = (I+ diag(gs))a(θs) (2)
where I is the identity matrix, and θs is the ar-
rival angle of the desired signal. Similarly, a˜j =
(I+diag(gj))a(θj) is the received vector of the in-
terferer with the interference angle θj . Then, the
received data vector at the time sample k is given
by
x(k) = ss(k)a˜s + sj(k)a˜j + n(k) (3)
where ss(k) is the envelope of the desired signal
with power σ2s , and sj(k) is the envelope of the
interfering signal with power σ2j . In addition, n(k)
is the additive received noise with power σ2n.
Let w represent the weights of the adaptive
beamformer. The beamformer output can be ex-
pressed as y(k) = wHx(k), where the superscript
H denotes the Hermitian transpose. The beam-
former determines the weightw by minimizing the
output power under linear constraints
CHw = f (4)
where C and f are an N×S constraint matrix and
an S × 1 response vector, respectively. The struc-
ture of the traditional GSC [1] is shown in Fig. 1.
The GSC implements eﬃciently the beamformer
by decomposing the overall weight vector w into
two components. The first componentwf = C(C
HC)−1f
denotes the fixed weight vector. If the look-direction
constraint is considered, then the fixed weight vec-
tor can be expressed as wf =
1
N [1, 1, · · · , 1]T . The
second component, denoting the adaptive part,
can be represented as −Bwa, where B is a block-
ing matrix which satisfies CHB = 0 and wa is an
adaptive weight vector. The weight vector wa is
obtained as the solution to the minimization prob-
lem:
min
wa
(wf −Bwa)HRx(wf −Bwa) (5)
where Rx
∆
= E{x(k)x(k)H} = Rs + Rj + Rn is
the data correlation matrix with E{·} denoting
the expectation operator, Rs = σ2s a˜sa˜Hs is the re-
ceived signal correlation matrix, Rj = σ2j a˜j a˜Hj is
the received interference correlation matrix, and
Rn = σ2nI is a white noise correlation matrix.
In the presence of array gain errors, the opti-
mal weight vector nulls out not only the interfer-
ence but also the desired signal. To prevent this,
[2] presented a leaky algorithm using an artificially
injected noise. Although the algorithm considered
there can preserve the desired signal by applying
a leakage elimination constraint ,h(wa),2 = 0,
where , · , is the Euclidean norm and h(wa) is
given by
h(wa) = σswHa BH (6)
it can also increase the noise power. Therefore, the
resulting output SINR degrades as injected noise
increases. To alleviate this problem, a null con-
straint algorithm is presented in the next subsec-
tion.
B. Deep Null Algorithm:
The structure of the proposed null-constrained
GSC is shown in Fig. 2. In most practical situa-
tions, the interference angle θj in a˜j is known. Ac-
tually, it can be easily estimated via the subspace-
based techniques in [6]. The expected output of
the traditional GSC due to interference only is
g(wa) = σj(wf −Bwa)H a˜j (7)
For a small σ2a, an estimate of g(wa) can be made
as follows:
gˆ(wa) = σj(wf −Bwa)Ha(θj) (8)
Therefore, the weight computation block of the
proposed adaptation chooses wa to minimize the
output power |(wf −Bwa)Hx(k)|2 subject to the
constraints ,h(wa),2 = 0 and |gˆ(wa)|2 = 0. The
cost function for this problem is
L(wa) = |(wf −Bwa)Hx(k)|2 + δ1,h(wa),2
+δ2|gˆ(wa)|2 (9)
where δ1 and δ2 are Lagrange multipliers. The in-
jected noise δ1,h(wa),2 is used to preserve the de-
sired signal and the pseudo-interference δ2|gˆ(wa)|2
is used to obtain a high interference suppression.
Note that if δ2 = 0, then the adaptation based on
(9) degenerates into a leaky algorithm addressed
in [2]. Then, the weights update is
wˆa(k + 1) = wˆa(k) + ξe(k) (10)
where ξ is the step-size used in the least mean
squares (LMS) algorithm, and
e(k) = BH(x(k)x(k)H + δ1σ2sI+ δ2σ2j a˜j a˜j)wˆa(k)
+BH(x(k)x(k)H + δ2σ2j a˜j a˜j)wf (11)
where ζj
∆
= (1 + δ2)σ2j/σ2na denotes the equivalent
interference-to-noise ratio (INR) and
γj = a˜Hj B(BHB)−1BH a˜j (19)
The overall output noise power Pon = σ2n(wf −
Bwoa)
H (wf −Bwoa) can also be expressed as
Pon ≈ σ2n(
ζ2j γj
(1 + ζjγj)2
|a˜Hj wf |2
+
ζ2sγs
(1 + ζsγs)2
|a˜Hs wf |2 +
1
N
) (20)
where we have used the fact that ,wf,2 = 1/N .
The output SINR can then be written as
SINR
∆
=
Pos
Poj + Pon
(21)
≈ αs|a˜
H
s wf |2
αj |a˜Hj wf |2 + αn|a˜Hs wf |2 + 1N
(22)
where
αs =
ηs
(1 + ζsγs)2
,
with ηs = σ2s/σ2n being the input SNR,
αj =
ηj + ζ2j γj
(1 + ζjγj)2
with ηj = σ2j/σ2n being the input INR, and
αn =
ζ2sγs
(1 + ζsγs)2
To achieve a high interference suppression, the La-
grange multipliers δ1 and δ2 should be adjusted to
maximize the output SINR. The maximum can
be obtained by taking derivative of SINR with re-
spect to the Lagrange multiplier δ2 and setting it
to zero. It yields
δ2 = ηsδ1 (23)
Moreover, we may assume that δ1 is large enough,
i.e.,
δ1 >> max{γs,
0
Nγs|a˜Hs wf |} (24)
so that 1 + ζsγs ≈ 1 and ζ2sγs|a˜Hs wf |2 << 1/N .
Using (17)∼(24), the output SINR can be reduced
to
SINR ≈ ηs|a˜
H
s wf |2
1
N +
1
(1+ηjγj)
|a˜Hj wf |2
(25)
It follows from [7] that (25) is approximately equal
to the output SINR of an ideal array. Moreover,
using |a˜Hs wf | ≈ 1 and 1 + ζsγs ≈ 1, it can be
verified from (17) that the output signal power
Pos for small values of σ2a is approximately equal
to σ2s . Therefore, the proposed approach is in-
sensitive to array gain errors. The simulation re-
sults in the next section also verify this property.
Also note that the value of γs in (24) can be es-
timated. A subspace method for estimating the
vector a˜s in γs by employing two identical ar-
rays whose corresponding elements are spaced at
a constant distance can be found in [6]. More-
over, Jablon [2] presented an element misplace-
ment method to obtain the variance σ2a. With the
expression E{γs} = σ2atr(B(BHB)−1BH), where
tr(·) denotes the matrix trace operator, an esti-
mate of γs can be made as
γˆs = σ2a(N − 1) (26)
Using |a˜Hs wf | ≈ 1, the multiplier δ1 addressed in
(24) can be determined directly from
δ1 = µmax{γˆs,
0
N γˆs} (27)
where µ is a large constant.
4. SIMULATION RESULTS
In this section, we examine the performance of the
proposed GSC. For comparison, three beamform-
ing algorithms are considered:
(a) the traditional GSC.
(b) the leaky algorithm with δ1 = 1σ2s (η
2
sηjN2
(N − 1)σ2a/|wHf a˜j |2)1/4 as suggested in [2].
(c) the proposed approach with µ = 102 and
γˆs = σ2a(N − 1) in the multiplier δ1.
Example 1:
The array considered in this example consists
of 32 sensors. The look-direction constraint is used
for the simulations. Figure 3 shows the normalized
output signal power Pos, for various input SNR,
v.s. the array gain error variance σ2a. The inter-
ference angle is fixed at θj = 15o with ηj = 20
dB. From this figure, we see that Pos of the pro-
posed GSC is equal to σ2s for all SNR’s. It clearly
shows that the proposed approach oﬀers the valu-
able property of preserving the desired signal. The
comparison of the output SINR, for various input
SNR, v.s. σ2a is shown in Fig. 4. We can observe
that the proposed GSC yields the highest SINR
for all SNR’s. Figure 5 also shows the array out-
put beam-pattern based on an ensemble average
of 500 independent trials for a fixed array gain er-
ror σ2a = 10−4. The input INR and SNR are 20dB
and 40dB, respectively. We can observe from this
figure that the proposed approach yields signifi-
cantly superior performance than the other two
methods.
Example 2:
The array considered in this example consists
of 64 sensors. The first-order derivative constraint
is used for the simulations. Figure 6 shows the
