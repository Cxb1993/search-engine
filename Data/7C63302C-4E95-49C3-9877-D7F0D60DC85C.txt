 2
  
目錄 
 
前言                 3 
 
1. 國語關鍵詞辨認系統之製作            4 
2. 麥克風陣列之通道效應補償            7 
3. Corpus-based 國語語音合成器之製作          8 
4. 口語對話系統之設計與製作            12 
5. 台語語音辨認系統之製作            22 
6. 語者辨認系統               24 
7. 語者確認門禁展示系統             26 
8. 嵌入式系統語音辨認系統            28 
9. 老年人的語音資料庫之蒐集            32 
10. 老人語音辨認器之研究             33 
11. 使用聲道正規化技術(VTLN)及最大可能性線性回歸(MLLR)調適技術製作老人語音
辨認器                35 
 
 
 
 
 
 4
 
1. 國語關鍵詞辨認系統之製作 
 
在本計畫中製作一個國語關鍵詞辨認(keyword spotting)系統，已配合居家照護計畫
作為語音人機介面的第一步： 
 
此系統工作環境為 PC windows 環境，可以跟應用程式結合；所訂定的關鍵詞組也
由使用者可隨時更改設定，系統之反應時間為語音開始後偵測到 0.8 秒之靜音確定語音
結束，就可立即獲得辨認結果。 
 
在此將此系統建立的步驟簡述如下： 
 
(1) 首先使用TCC-300國語語料庫[1]建立國語 100 final-dependent initial及 40 final HMM
語音辨認模型。 
 
(2) 製作一連續語音辨認器，為了簡化計算量以達及時辨認的目的，在系統中使用了光
束搜尋法(beam search)[2]以降低搜尋空間增加速度，並加入 look-ahead 方法[3]以提
升在刪除部分搜尋空間後之辨認效能，如圖一所示。 
 
 
 
圖一、語音辨認系統中之光束搜尋(beam search)及 look-ahead 方法示意圖。 
 
 
(3) 在關鍵詞辨認系統中，必須訂定待辨認之詞彙。在所製作之系統中使用一個簡單的
文字檔來定義系統中所能辨認的關鍵詞，如下所示： 
 
 
居家看護 4  /* task name, no. of keywords */ 
電腦  電腦  /* keyword, output answer */ 
開燈  開燈 
關燈  關燈 
打電話 打電話 
 
 
在關鍵詞定義檔中所有關鍵詞均以 BIG -5 碼格式輸入，辨認器中會將其轉換成
 
圖二、國語關鍵詞辨認系統。 
（下方左、右子視窗為 batch 測試用，中間下方視窗之結果為線上辨認結果）。 
 
 6
 3. Corpus-based 國語語音合成器之製作[4] 
 
子計畫四之居家看護機器人除需要語音輸入外，還需要語音合成器能讓機器人『說
話』。在現階段，將使用交大語音實驗室所發展的國語語音合成器；因為它是使用國語
基本 411 音節來合成所有國語語音，所以所需之記憶體會較小，適合在機器人上。 
 
現今的語音合成器多使用例如：PSOLA(Pitch Synchronous OverLap adding)等語音剪
接方式來產生所需之合成語音，在語音合成器中任何對波型有做修飾都會造成合成音質
下降的情況之下，使用大型語料庫所儲存的語音來做合成單元，如果可以找到較長之詞
或詞組來當合成單元，當然是一個比較良好的選擇，因為在這樣的合成單元內，就已經
包含本身的音韻，因此對訊號的修飾就可以盡量避免，在串接時，對於合成語音的自然
度當然有一定的效果提升。但所需付出的代價是必須儲存數十 Gbyte 的語音波形檔。現
今我們所錄製的大型語料庫其文字內容為中央研究院現代漢語語料庫(Sinica Corpus)[1]
中的文章，至目前為止計有 8,017 個詞，語音波型檔僅數十 Mbyte。 
 
(1) 合成器語音資料庫之文字及波形前處理 
 
因為所使用之語料使用中央研究院現代漢語語料庫之文字資料所錄製，所以使具有
斷詞結果乃至於語法結構之資料，所以不需對文字資料再做處理。而波形資料則是使用
HMM 辨認模型做 force-alignment 後再經微調獲得－例如：將因節切割位置移至能量之
local minimum 點 
我們也提出 HMM 切音位置之 refinement 方法是以一個語句(utterance)為單位做音節
起使與終止點之校正。其演算法則為 
 
a. 利用一語句之 HMM 音節切割位置，對 silence 語料建立一個 GMM 模型。 
b. 對各音節切割點前後見建立一個 final 及 initial 之 Gaussian model，對各音節切割點
加減 5 個音框範圍使用圖三所示之 HMM 模型找最佳切割位置。 
 
 
圖三、節起始與終止點之校正時在音節切割點所使用之模型。 
 
圖三所示即為一國語語句切音位置自動調整演算法則之例子；由例子中，由例子中
我們可以看到我們所提出之 HMM 切音位置之 refinement 方法可以有效的切出音節間之
短靜音。經 HMM 切音位置之 refinement 後各發音方法之平均長度均有下降，尤其是 Stop
及 Affricate 音，並且可以切割出音節間的短靜音。 
 
 
(2) corpus-based 語音合成器核心之製作 
 8
對於在語料庫出現次數頻繁的中文字，例如：『的』，上述連續相關比對方法會導致
比對時間的拉長，所以我們必須將這些字找出並做一些特殊的比對處理。我們使用了另
一個表 FCLT (Frequent Character Location Table) 來紀錄這些常用字在句子中出現位置
資訊的標記檔，用以代替要紀錄在 CLT 中的相同資訊；以便搜尋時可以跳過這些常見
字，如：『的』，在比對時之處理流程則如圖六所示。如此將可減少搜尋所有可能的候選
詞組所組成的合成單元 lattice 之流程所需之時間。 
 
在獲得所有可能的候選詞組所組成的合成單元 lattice 後，我們對將各候選詞組串的
聲學參數與由 prosodic information generator 產生之合成語句之聲學參數做比對，決定一
組作佳合成詞組串，其聲學及語言參數與所需者合成語句經斷詞及國語語音合成器之
RNN 韻律產生器[5]所產生之聲學及語言參數最接近，所採用的比對條件是 
( ) ( )( ) ( , )
i i
i il l k s s k
intra k p k t
l s
w D N w D SC L D P P
Q Q
= + +  
其中考慮了各詞串之字數 ，語意參數 及韻律參數 iNk iSk iPk 。 
 
 
 
圖五、連續相關比對法之示意圖。 
  
 10
 4. 口語對話系統之設計與製作 
 
在計畫中，為配合其他子計畫，製作一個語音對話系統(Speech Dialogue System)系
統。對人們來說，最便利的溝通管道便是透過聲音，架設對話系統的目的就是建立一個
透過語音操作的人機介面。要想達到此目標就得使電腦能直接透過語音理解使用者的語
言和語義，並且能準確地完成使用者交代的工作。[7] 
 
 
 
圖七、 對話系統的系統圖。 
 
 
整個語音對話系統的核心就是對話管理員（Dialog Manager），它整合了由即時語音
辨認器辨認出來的結果，透過會話分析（Discourse Analysis）來分析語義，再由得到的
語義和系統目前的狀態從對話策略（Dialog Strategy）裡尋得系統下一步該做的動作，並
且可藉由搜尋外部的資料庫（Database）來完成任務，再將需要告知使用者的訊息藉由
語音合成的技術（Text-To-Speech）用語音通知使用者。 
 
在計畫中我使使用強化學習（Reinforcement Learning）學習法則讓對話系統和外在
環境互動，藉由 trial-and-error 的方式學習最佳的行為模式。下圖就是一個以對話系統為
例子的強化學習標準模型： 
 12
 
 
圖九、 使用 slot filling 表示一個對話系統。 
 
在計畫中我們使用 simulated annealing Q-learning (SA-Q)學習方法[9]，也就是估計
所處狀態(state)與下一個問話內容(action)之間的關係。Q(s,a) 用來定義狀態(state) s 與下
一個問話內容(action) a 所獲得之獎勵。我們希望找到一個問話內容(action) a，其至對話
完成時可獲得最高之獎勵值。所以最佳之問話內容(action) a 被定義為使得至對話完成時
Q 函數值最高，也就是 
* *
' '
( , ) ( , ) ( , , ') max ( ', ')
s S a
Q s a R s a T s a s Q s aγ ∈= + ∑             
(1) 
其中 T(s,a,s’) 為狀態轉換機率 R(s,a) 為在這個狀態的獎勵函數。γ 則是對接著問話獎
勵值的 discount 參數。所以我們要找一串問話(action)使的 
( ) ( )* *max ,
a
V s Q s a=
                      
(2) 
而最佳對話策略即 
( ) ( )* arg max ,
a
s Qπ = * s a
,
                
(3) 
我們可用下列遞迴方式來定義 Q 函數: 
 
(i) 從現在狀態 s， 選擇一個問話 a.，將使系統進入新狀態 s' 且獲得 γ 之獎勵； 
 
(ii) 利用下列式子更新 Q(s,a)  
( ) ( ) ( ) ( )( )
( ) ( ) ( )( )
, : 1 , max ,
, max ,
a
a
Q s a Q s a r Q s a
Q s a r Q s a Q s a
α α γ
α γ
′
′
′ ′= − + +
′ ′= + + −
           
 14
 
 如果我們分析系統所得到之最佳對話策略，如圖十一所示，也符合人類所會採取之
對話策略。 
 
 
圖十一、使用 SA-Q 方法自動學習之對話策略。 
 
 
[實驗二] 
 
 在此實驗中我們模擬了 4 種使用者特性，以測試 SA-Q 方法自動學習之對話策略之
不同，4 種使用者分別為 
 
(i) User1 – 僅回答系統一個問題，不管系統在一次問話中問幾個問題； 
(ii) User2 –最多回答系統三個問題，不管系統在一次問話中問幾個問題 
(iii) User3 –回答系統所有問題，不管系統在一次問話中問幾個問題 
(iv) User4 –不但回答系統所有問題，連系統未問之問題也一併回答 
 
在模擬中我們針對 4 種不同的使用者特性各自訓練了一套最佳對話策略，使用所獲
得之 4 種不同對話策略，對不同使用者特性作效能分析。其結果如表一致表四所示。表
中 SR 代表對話成功率（Success rate）, NQ 代表系統平均的詢問次數，NQ 直當然越小
越好， O 代表我們評估對話系統時所設定的目標函式(objective measure) 之平均值。 
 
 
表一. Performance of different of dialogue strategies under user behavior model User1. 
 
Task1 Task2 Task3 Task4 Strateg
y 
Used 
NR NQ O NS NQ O NS NQ O NS NQ O 
1 0.98 6.18 -164 0.95 8.73 -242 0.99 8.34 -264 0.98 9.07 -246
2 1.00 6.92 -180 0.95 9.54 -301 0.99 8.08 -254 0.97 9.03 -253
 16
 
 
 
[實驗三] 
 
 在對話系統中事實上還有一影響系統效能的重要因素需要考量那就是語音辨認的
辨認效能；皆下來我們就考量語音辨認效能對對話系統之影響。下面實驗中我們假設語
音辨認系統的辨認率及對辨認結果確信程度(confidence measure)不同的對話模擬。(表五
~表九中 R 為辨認率、C 為辨認結果確信程度) 
  
結論是顯而易見的語音辨認率如果太低則會嚴重影響對話系統效能，但對話系統也
可容忍部分的語音辨認錯誤而對話次數不會明顯的上升。 
 
 
表五. Performance of strategy 3 (trained under recognition rate (R)=1.00 and confidence 
measure (C)= 0.90) under different conditions. 
 
C=0.9 C=0.65 C=0.3 
 
SR NQ O SR NQ O SR NQ O 
R=1.0 0.990  5.08  -160 0.940 7.44 -408 0.555 15.44  -1509  
R=0.9 0.985  5.38  -173 0.938 7.83 -419 0.515 15.80  -1555  
R=0.8 0.993  5.75  -189 0.903 8.51 -496 0.473 16.13  -1608  
R=0.6 0.985  6.59  -233 0.913 9.38 -571 0.448 16.91  -1680  
 
 
表六. Performance of strategy 3 (trained under recognition rate (R)=1.00 and confidence 
measure (C)= 0.65) under different conditions. 
 
C=0.9 C=0.65 C=0.3 
 
SR NQ O SR NQ O SR NQ O 
R=1.0 0.995  4.73  -120 0.993 6.70 -186 0.823 12.96  -426  
R=0.9 0.995  5.05  -141 0.990 6.97 -213 0.835 13.19  -429  
R=0.8 1.000  5.57  -161 0.993 7.33 -233 0.833 13.67  -476  
R=0.6 0.990  6.34  -216 0.993 8.56 -325 0.745 14.84  -618  
 
 
表七. Performance of strategy 3 (trained under recognition rate (R)=0.9 and confidence 
measure (C)= 0.9) under different conditions. 
 
C=0.9 C=0.65 C=0.3 
 
SR NQ O SR NQ O SR NQ O 
R=1.0 0.978  5.04  -156 0.853 7.84 -456 0.480 13.97  -1421  
R=0.9 0.973  5.22  -168 0.855 8.20 -476 0.478 14.46  -1414  
R=0.8 0.965  5.49  -179 0.858 8.47 -463 0.398 14.61  -1480  
 18
 
圖十二. The block diagram of the in-vehicle speaking assistant prototype system. 
 
利用前述的對話系統設計流程，我們設計了下列 function 及其所需之 18 個 slots。 
 
 
 
 
圖十二. The assignment between the functions and slots. 
 
利用 SA-Q 方法自動學習之對話策略所設計的結果如下圖所示。 
 
 20
 5. 台語語音辨認系統之製作 
 
在台灣高齡人口群中，使用台語的人口數目十分多。所以在本計畫中，我們也製作
了台語語音辨認器。 
在台語語音中共有 877 個基本音節，18 個聲母(包含空聲母)及 84 韻母，韻母中有
28 個是入聲音調(entering tone)。台語中有入聲音調音節這也是與國語最大不同之處。而
且台語之音節數也是國語的兩倍多，所以音節辨認就變成較為困難。計畫中首先我們整
理並擴充我們已有之台語語料庫。以下台語語音辨認系統之製作我們所使用之台語語料
庫是由 197 個語者；91 男性及 106 女性語者所錄製。我們將整個語料庫分成兩部分；
(1)訓練語料：包含語料庫中 9/10 語者共 105,687 個音節，(2) 測試語料：共包含 19 個男
女語者，12,211 個音節。 
 
 我們所製作的台語語音辨認器與國語語音辨認器相同是使用次音節 HMM 節模型。
我們所使用的是 101 個右相關聲母模型及 84 個韻母模型； 其中右相關聲母模型狀態數
為 3 而韻母模型狀態數為 5，以及 3/1 個狀態的靜音/短靜音模型。經實驗，台語基本音
節辨認率如表十所示。 
 
表十、台語 877 基本音節辨認率。 
 
Correct del sub ins total recog 
rate 
5967 353 5891 341 12211 46.1%
 
 
經錯誤分析，入聲韻母之辨認率較平均辨認率為低。在台語中之變調規則[10、11]
較國語複雜，事實上幾乎所有音節都會變調。尤其 *h 韻尾之入聲音，不但會變調，而
且韻尾會消失。如此不但聲調會改變，連基本音節都會改變。在台語 87 個韻母(28 個入
聲音調韻母)中共有 17 個是*h 韻尾的入聲音，所以我們將考慮 *h 韻尾變調規則後之語
料作重新訓練及辨認後，辨認結果如表十一所示，可將台語 877 基本音節辨認率提高
3.5%。 
 
表十一、台語 877 基本音節辨認率(考慮 *h 韻尾變調規則)。 
 
Correct del sub ins total recog 
rate 
6439 355 5417 314 12211 50.2%
 
接著我們再加入 syllable unigram 和 syllable bigram 的 language model(LM)，並檢
視辨識率提升的情形。再求取 syllable unigram 和 syllable bigram 時我們所採用的訓練
資料除原語音訓練資料外另加入了一些訓練文字檔，如表十二所示。再加入 syllable 
unigram 和 syllable bigram 後之台語 877 基本音節之辨識率則如表十三及十四所示。 
 
表十二、建立台語語言模式之資料庫。 
 22
 6. 語者辨認系統 
 
我們在 prosodic level 進一步解決對話筒或通道 mismatch 問題，並提出以 latent 
prosodic analysis（LPA）方法用於語者識別，主要是以特徵分析方式，分析韻律訊息，
萃取有用之主要韻律特徵成分，以輔助傳統以頻譜參數為主之語者辨認系統。其中我們
所用的 LPA 方法，或使用 latent prosodic semantic analysis (LPSA)方法如圖十五所示，系
統方法與架構圖如圖十六所示，韻律參數的求取則如圖十七所示。 
 
所提出方法以 LDC 出版之 HTIMIT corpus 測試[19]，其效能如圖十八所示，在有限
語料下（7 句訓練，三句辨認語料），使用 prosody information 可大幅幫助提升語者辨認
率，明顯優於傳統方法。 
 
 
 
圖十五，Block diagram of the LPA speaker identification. 
 
 
 
圖十六，Block diagram of the LPSA. 
 24
 7. 語者確認門禁展示系統 
 
我們利用計畫所開發之技術，製作一語者確認門禁系統，實際裝置在實驗室門上使
用（見圖十九）。系統包含PC上之keyword spotting與speaker verification軟體介面（見圖
二十），觸控螢幕，透過RS232之電磁鎖控制電路與電磁鎖。使用者只要透過觸控螢幕啟
動程式，說出他的姓名，即可同時輸入speaker ID與做speaker verification，大約可在9秒
內完成整個驗證程序並開鎖進入，相當方便。實際展示影片請見： 
http://www.cc.ntut.edu.tw/~yfliao/nsc/demo/SpeakerVerification2005.wmv
 
 
 
 
圖十九、語者確認門禁展示系統。 
 
 
 26
 8. 嵌入式系統語音辨認系統 
 
計畫中我們也建立了一套嵌入式系統上的國語獨立詞辨認器，系統是建構 Window CE
之 Pocket PC 下。下面就是詳細系統架構： 
 
Automatic speech recognition system (ASR) in embedded systems is expected to be a 
major product differentiator in the upcoming decade. The embedded automatic speech 
recognition system (eASR) can be used in lots of valuable applications, such as home 
entertainment, in-vehicle information systems, personal communication, and elder-care 
environment. In last century, due to the computation and memory capabilities of a mobile 
device, usually a simple speech recognition algorithm, dynamic time warping (DTW), was 
used in mobile device. Thus, the eASR system will be speaker-dependent and the 
performance was not good enough. So, the eASR could only be treated as a toy. In the past 
decade, the capability of the portal smart device likes a mobile phone/PDA makes the ASR 
system using the hidden Markov model (HMM) became possible. And the performance of the 
ASR system using HMM technique such as isolated Mandarin-word recognition system was 
better enough for the real application. Due to the speed and size constraint for the mobile 
device such as a mobile phone, the development of an eASR system was still a big challenge 
for engineers. 
In this project, an isolated Mandarin word eASR system was implemented in the 
PDA-based device. The system specifications or development environment are listed below: 
 
 OS: WinCE platform 
 Speech signal sampling rate : 8KHz 
 Memory: ROM 1.2M bytes, RAM 2M bytes 
 
And, the target was to build an isolated Mandarin word eASR system which can recognize 
1000 words. The system response time will be less than two times of real time. And the 
recognition rate will be higher than 95% for clean speech and 90% fro in-car environment. 
The basic block diagram of the HMM-based eASR system is displayed in Fig. 21. It 
consists of four modules. Input speech signal is firstly pre-processed to extract some spectral 
features. Then a beam-search algorithm is applied to generate top-N recognized words. The 
acoustic modeling adopts 100 right-dependent initial HMM models and 39 final HMM 
models. A simple lexicon tree is formed from all the words to be recognized and used to guide 
the recognition search. 
 
 28
The recognition rate degradation is 2.4%, and this system can run smoothly in the 32bit 
embedded system, the response time about 1.5 times real time. 
 
[Experiment 3]  
In this experiment, further computational reduction was made in order to faster the system. 
In the front-end feature extraction was 100 frames per second. Since, the recognition feature 
had been smooth out by the RASTA filter. Thus, we down-sampling the recognition feature 
found in feature extraction block in a factor of 2.5. Down sample speed up the system, and the 
same time maybe reduce recognition rate, but as shown in Table 1, the recognition rate was 
still hold good. And, the system response time is less than 1.1 times real time. This type 
system is the better solution than previous system.  
 
[Experiment 4]  
In order to check the robustness of HMM models, training speech database is corrupted by 
environment noisy, the noise we used was the car noise.   As shown in Table 1, the system 
performance was degenerated. The recognition rate was 1.9% lower than using clean speech 
training. Although the performance was degenerated, but the system should be more 
robustness when the system was operated under noise environments. So, experiment 5 was 
used to the robustness of the system under noise environments. 
 
[Experiment 5]  
 In this experiment, the performance of our recognizer was evaluated under corruption of 
real car-environment noise with different SNR. The real car-environment noise contained 
engine noise, windshield wiper noise, car concussion noise, and outside noise.  The car noise 
used in this experiment was collected by PDA in a real car driving environment. The above 
car noises were added to the testing corpus with different SNR level. The performances of our 
Mandarin isolated word recognizer were shown in Table 20.  We can see that the 
performance of 1the speech signal with 18 and 12dB car-environment noise were better than 
the one get from clean test and training data. And 91.3% recognition rate can be achieved 
under 6dB car-environment noise.  So, the system will be robust under the real application 
environments. 
 
Table 20. Performance for Mandarin isolated word recognition under different 
SNR level with real car environment noise corruption. 
HMM model type Total Sentence SNR
Top1 
Correct 
number 
Top5 
Correct 
number 
CLaplacian 
(Fixed-point) 
(I2,F4)(8,8,16) DS 2.5 
1268 18db 1220(96.2%) 1259(99.3%) 
CLaplacian 
(Fixed-point) 
(I2,F4)(8,8,16)  DS 
2.5 
1268 12db 1212(95.6%) 1260(99.4%) 
CLaplacian 
(Fixed-point) 
L(I2,F4)(8,8,16) DS 2.5 
1268 6db 1158(91.3%) 1237(97.6%) 
CLaplacian 
(Fixed-point) 1268 0db   848(66.8%) 1045(82.4%) 
 30
 9. 老年人的語音資料庫之蒐集 
 
到目前為止，我們的語音辨認系統還是使用年輕人的聲音做 HMM 升學模型的訓練
及測試，對於老人的語音則需做語料之蒐集。對於一個語料庫蒐集的工作，一向是個繁
瑣且耗費人力的工作，現今我們已做了所有準備工作：(1)錄音工具之選擇：我們改寫了
工研院電通所尖端科技中心(ATC)為進行蒐集台灣之英文語料庫所製作之錄音介面；(2)
錄音語料之選擇：本計畫已獲得清大王小川老師之授權，使用 MAT (Mandarin Across 
Taiwan)語料庫[1]所製作之語料文字，因為該語料內容為短詞、成語及短句，較適合老
人錄製。在計畫中，我們蒐集了一個老人語料庫，其語料錄音者是由台北市與新竹市市
民所構成，並限定錄音者的年紀必須在六十歲以上，當中包含了十九名女性和三十五名
男性，總共有五十四名錄音者。此錄音的語料屬於朗讀式語音(read speech)，而我們取
用的文字轉寫(transcription)源自於MAT-2000及MAT-2500中四個音節(syllable)以上的句
子，使得每位語者皆具有六十句，並將其文字轉寫修正為每句中只包含十個音節左右或
十個音節以內，希望以此來減少因年紀增長導致記憶力下降的因素，以提高老人錄音的
品質，因此每位語者所錄的句數將變成六十句至八十多句不等。。 
 
 32
 
 
    由上表的統計值可看出老人聲道的確會隨著年齡的增長而變短，針對母音/a/而言，
Formant 1 也會變得較低，而 Formant 2 與 Formant 3 的統計資料中老人的標準差(SD)與
範圍(Range)都比年輕人來的寬廣，而平均值卻相去無幾，這顯示了老人語音特性中 F2、
F3 是無法明確去做計算的。以下是已被語言學家證實的老人語音現象： 
 
    ◆聲門變長，老人語音較低沉，尤其是男性 
    ◆聲道長度變短，First Formant 下降，其他 Formant 變異量極大 
    ◆說話速率(speech rate)較緩慢 
 
在計畫中，我們蒐集了一個老人語料庫，其語料錄音者是由台北市與新竹市市民所
構成，並限定錄音者的年紀必須在六十歲以上，當中包含了十九名女性和三十五名男
性，總共有五十四名錄音者。此錄音的語料屬於朗讀式語音(read speech)，而我們取用
的文字轉寫(transcription)源自於 MAT-2000 及 MAT-2500 中四個音節(syllable)以上的句
子，使得每位語者皆具有六十句，並將其文字轉寫修正為每句中只包含十個音節左右或
十個音節以內，希望以此來減少因年紀增長導致記憶力下降的因素，以提高老人錄音的
品質，因此每位語者所錄的句數將變成六十句至八十多句不等。表二十二為訓練老人語
音辨認器時所使用之訓練及測試語料中的音節數，而 filler 為語音中一些呼吸聲等非語
音之聲音信號。我們使用上述語料所建立之老人語音辨認器之辨識率為 44.72%，則如
表二十三示。 
 
 
表二十二、訓練及測試語料中的音節數、filler 的統計 
 
 
411 音節數
訓練/測試 
Filler 
音節數 
訓練/測試 
女性語者 8841/1901 29/13 
男性語者 16145/3619 33/7 
全部語者 24986/5520 62/20 
 
表二十三、老人語音辨識系統之辨識率 
%Del %Sub %Ins %Acc
4.4 47.72 3.2 44.72
 
 34
  
Fourier 
Transform 
Filter Bank 
Integration 
Warping Factor 
Frequency 
Warping 
圖二十二、 VTLN Frequency Warping 示意圖與方塊圖 
 
0
2
4
6
8
10
0.9 0.92 0.94 0.95 0.96 0.99 1.01 1.02 1.04 1.05 1.06 1.08 1.1 1.12 1.14 1.18
warping factor
# 
of
 sp
ea
ke
r
male
female
 
圖二十三、 男、女語者VTLN調適之α 值與人數的統計 
 
 
 
 
表二十五、 加入VTLN後的MLLR調適 
Regression
Classes %Del %Sub %Ins %Acc
32 5.08 47.1 2.65 45.18
64 5.12 45.64 2.51 46.73
128 4.96 43.98 2.51 48.55
256 5.38 40.91 2.23 51.47
 
我們所訓練之老人 411 音節辨認器之音節辦認率為 51.5%，該老人 411 因音節辨認
模型已可應用於老人居家看護環境下之語音人機介面之小詞彙之特定應用如
voice-command 了。 
 
 36
 參考文獻 
 
[1] 中華民國計算語言學學會， 
http://rocling.iis.sinica.edu.tw/ROCLING/corpus98/index_cf.htm. 
[2] H. Ney, D. Mergel, A. Noll, and A. Paesler, “ Data driven search organization for 
continuous speech recognition,”, IEEE Transactions on Signal Processing, 
40(2):272--281, February 1992. 
[3] Ortmanns, Stefan; Ney, Hermann 2000, “ Look-ahead techniques for fast beam search,” , 
Computer Speech and Language 14, 15.32, 2000. 
[4] C. J. Leggetter and P. C. Woodland, “ Maximum Likelihood Linear Regression for 
Speaker Adaptation of Continuous Density Hidden Markov Models,”, Computer Speech 
and Language, pp. 171–185, 1995. 
[5] 吳佩穎，『以語料庫為基礎之中文文句翻語音系統中合成單元之選取』，交大電信所
碩士論文，民國 94 年。 
[6] Sin-Horng Chen, Shaw-Hwa Hwang, and Yih-Ru Wang, “ An RNN-based Prosodic 
Information Synthesizer for Mandarin Text-to-Speech, “, IEEE Trans. Speech and Audio 
Processing, Vol. 6, No. 3, pp. 226-239, May, 1998. 
[7] E. Levin, R. Pieraccini, W. Eckert, G. Fabbrizio,  S. Narayanan, “Spoken Language 
Dialogue: from Theory to Practice,”,  Proc. of ASRU99, IEEE Workshop, Keystone, 
Colorado, Dec. 1999. 
[8] E. Levin, R. Pieraccini, and W. Eckert, “Using Markov decision process for learning 
dialogue strategies,” Proc. ICASSP 98, Seattle, WA, May, 1998.C. Watkins, ‘Learning 
from Delayed Rewards.’ Ph.D. Thesis, Psychology Department, Cambridge University, 
Cambridge, England, 1989. 
[9] Maozu Guo, Yang Liu, and Jacek Malec, “A new Q-learning algorithm based on the 
metropolis criterion,” in IEEE Transactions on systems, man, and cybernetics-part B, 
VOL.34, NO.5, Oct 2004. 
[10]  R Lyu, M Liang, D Lyu, Y Chiang CHAPTER 17, TAIWANESE MIN-NAN SPEECH 
RECOGNITION AND SYNTHESIS - Advances in Chinese Spoken Language Processing, 
2007. 
[11]  鄭良偉，台語的語法與詞性，遠流出版社，1997。 
[12] 林川傑，1997，《國語-閩南語機器翻譯系統之研究》，台北，台灣大學資訊工程系(碩
士論文)。 
[13] Min-siong Liang、Jui-Cheng Yang、Yuang-Chin Chiang、Ren-Yuan Lyu，” A Taiwanese 
Text-to-Speech System with Applications to Language Learning ”，Proc. of the 4th IEEE 
Int. Conf. on Advanced Learning Technologies , p91-95，Finland, 2004. 
[14] Chin-Han Tsai, Yih-Ru Wang, Yuan-Fu Liao, ' Reinforcement Learning-based Spoken 
Dialog Strategy Design for In-Vehicle Speaking Assistant, ', Proc. of CACS 2005, Tainan, 
ROC. pp. i-4 12-17, Nov. 2005. 
[15] Kemal Sonmez, Elizabeth Shriberg, Larry Heck, Mitchel Weintraub, “Modeling 
Dynamic Prosodic Variation For Speaker Verification,” In Proc. of ICSLP, Vol. 7, pp. 
3189-3192, 1998. 
[16] D. A. Reynolds et. al., “The superSID project: exploiting high level information for 
high-accuracy speaker recognition,” Proc. ICASSP’03, vol. IV, pp.784-787, 2003. 
[17] NIST - Speaker Recognition Evaluations, 
 http://www.nist.gov/speech/tests/spk/index.htm
[18] Thomas Hofmann, “Unsupervised Learning by Probabilistic Latent Semantic Analysis,” 
 38
出席國際學術會議心得報告 
                                                             
計畫編號 95-2218-E-009-006- 
計畫名稱 數位化居家照護系統研究-子計畫三：語音人機介面於居家照護系統之應用
(3/3) 
出國人員姓名 
服務機關及職稱
王逸如 
交通大學電信工程學系 
會議時間地點 美國夏威夷 
會議名稱 32nd IEEE International Conference on Acoustics, Speech, and Signal Processing 
發表論文題目 
Chen-Yu Chiang, Xiao-Dong Wang, Yuan-Fu Liao, Yih-Ru Wang, 
Sin-Horng Chen, Keikichi Hirose, “ Latent Prosody Modeling Of 
Continuous Mandarin Speech, “ 
 
一、參加會議經過 
 此次 ICASSP (IEEE International Conference on Acoustics, Speech, and Signal 
Processing)會議於 4月 17日至 20日在夏威夷 Honolulu的國際會議中心舉行，本
次大會共有 1344 篇 paper 發表。此次大會共有兩千餘人與會，會議中共有十幾
個 parallel sessions 同時舉行，其中語音信號處理方面共有 3 至 4 個 parallel 
sessions。兩年後的 ICASSP-2009 會議將由台灣主辦，屆時希望有更多國哪專家
學者可以在會議中發表論文。 
 
二、與會心得 
 
本次大會國內共有二十餘篇論文發表，國內從事數位信號處理及語音研究
之各單位，如：中研院、台大、清大、交大、成大等專家學者及學生有三十餘人
與會。 
4月 18日大會 plenary talk 是由日本東京理工的 Sadaoki Furui教授主講，
題目為『50 years of progress in speech recognition technology -- Where we are, and 
where we should go』，光由題目就可以看出語音辨認技術研究今日的處境，回顧
五十年來，雖然語音辨認之技術已有很大的進展；直至今日，語音辨認系統已走
向 state of art，但辨認率仍與人類可以達到的有一段差距，要如何提升語音辨認
之效能已變成是一個系統整合的問題，我們必須進一步結合各種不同領域的技術
以提升語音辨認的效能。 
在大會中看到國外一些進行語音辨認技術發展之公司及研究單位，如：
IBM、Microsoft、Cambridge University… 等單位或一些歐美重要大學所發表之
