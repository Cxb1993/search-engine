Candide-3上。由於 PSM產生高度密集的三維資訊，為了降低後續計算負擔，我們再施以 Quadratic Edge 
Metrics Edge Collapse（QEM-based ecol.）網格簡化技術將網格端點減少並保留臉部幾何特徵。最後將結
果輸出，建立相容於MPEG-4的臉部網格模型。 
 
 
圖 1. 系統流程圖 
Surface Recovery 
本系統使用 PSM 方法，使用燈泡光源做為安全的掃描能量，依序將三盞光源投射至待測表面上並
擷取影像，以 Lambertian全漫射模型在無環境光源的干擾下逆解表面梯度向量，再以 Frankot-Chellappa
全域積分演算法，在頻率域將表面起伏轉換為深度資訊，取得待測表面的深度圖。重建得到的網格為
M scan=(V scan ,G scan)
 
 
  
(a) (b) (c) (d) 
圖 2. (a)-(c) 使用 PSM以三盞光源照明與重建之(d)深度圖 
Locating Facial Feature Points 
使用 PSM 得到的表面僅是由端點與三角面構成的網格，並不能直接作為臉部模型使用。文獻中的
研究多將臉部動畫模型以特定的參數化（parametrization）方式描述，也就是使用固定的控制點（control 
points）配合可調整的參數改變臉部表情，而這些控制點即為特徵點。在 MPEG-4 臉部動畫（Face 
Animation, FA）標準中制定了 84個特徵點（如圖 3），為了將重建的表面與臉部模型建立關聯，我們必
須先取出特徵點。 
 圖 4. Candide-3臉部網格模型 
 由於 Candide-3端點包含MPEG-4臉部動畫之特徵點（即 V mpeg⊂V candide），因此可將重建的網格 M scan
內嵌到臉部模型 M candide 中。在網格嵌入方面，本研究嘗試以 barycentric coordinate參數化技術達到網格
嵌入（mesh embedding）的目的（）。在 barycentric coordinate的座標表示法中，每端點 v i之幾何位置是
由相鄰端點線性組合而成，即 
 
Gi= ∑
v j∈adj (vi )
w ij G j
, 
(1) 
 
其中 G 為 ∣V∣×3 幾何矩陣（geometry matrix）而 w ij 為權重，滿足 
 
∥wi∥=1
. 
(2) 
 
若已知所有權重與部分端點的位置，則 (1) 式成ㄧ線性方程組，未知的端點位置即為欲求之解。由 (1) 式
右項左移可得 
 
Gi− ∑
v j∈adj (vi )
w ij G j=0
, 
(3) 
 
設ㄧ矩陣 L 定義為 
 
l ij={
1 , v i=v j
−wij , v j∈adj (vi)
0 , otherwise
  
(4) 
 
 Gt+ 1=(1− η )G t+  η A Gt
  
(12) 
 
以逼近 (9) 式之解， η 為可控制之更新速率。當 dG /dt 趨緩時停止疊代，此時以 G 收斂值為解，作為
網格 M scan 在 M candide 的嵌入 M embed ，而 V b 則作為臉部表情動畫的控制點。 
Mesh Simplification 
在 M embed 中包含稠密的拓譜與幾何資訊，為了降低模型計算複雜程度，我們以邊折疊（edge collapse, 
ecol.）的方式簡化模型。邊摺疊是常用的網格操作之ㄧ，其原理是將邊的兩端點合併成ㄧ端點，消去ㄧ
個端點、ㄧ個邊與兩個三角面（如圖 5所示），藉此簡化網格結構（）。 
 
圖 5. 邊折疊操作 
 文獻中有多種邊摺疊的網格簡化法，其中主要的差異在於摺疊邊的選擇（見）。ㄧ般在折疊前會評
估對各個邊施以折疊操作所引入的誤差，而不同的評估策略可在簡化的過程中保留不同的特性，在本研
究中我們以二次誤差（Quadratic Error Metrics, QEM）評估簡化誤差。對於端點 v i其二次誤差定義為 
 
E i= ∑
p j∈ plane(v i)[ p jT(G i1 )]
2
,  
(13) 
 
其中 plane (v i) 是 v i 相鄰之三角面的集合，其元素 p j=(a j b j c j d j)
T
是三角面的平面方程式，而
(a j , b j , c j) 為三角面單位法向量。改寫 (13) 式整理得 
 
E i=(Gi1 )
T
( ∑p j∈ plane(v i) p j p j
T
)(G i1 )=(Gi1 )
T [ ∑p j∈plane (vi)(
a j
2
a j b j a j c j a j d j
a j b j b j2 b j c j b j d j
a j c j b j c j c j2 c j d j
a j d j b j d j c j d j d j
2 )](Gi1 )
, 
(14) 
 
定義 Q j 為 4×4對稱矩陣 
 
References 
[1] I.S. Pandzic and R. Forchheimer, “MPEG-4 Facial Animation: The Standard, Implementation and 
Applications,” Wiley, 2022. 
[2] M. Rydfalk, “CANDIDE, a parameterized face,” Technical Report LiTH-ISY-I-866, Dept. of Electrical 
Engineering, Linköping University, Sweden, 1987.  
[3] J. Ahlberg, “CANDIDE-3 - an updated parameterized face,” Technical Report LiTH-ISY-R-2326, Dept. of 
Electrical Engineering, Linköping University, Sweden, 2001. 
[4] M.S. Floater, “One-to-one Piecewise Linear Mappings Over Trian-gulations.” J. Mathematics of 
Computation, vol. 72, issue 242, pp.685-696, 2003. 
[5] M.S. Floater and  K. Hormann, “Surface parameterization: a tutorial and survey,” In Advances in 
Multiresolution for Geometric Modelling, pp.157-186, 2005. 
[6] E. Hoppe, ”Progressive Meshes,” Proc. ACM SIGGRAPH 96”, pp.99-108. 1996. 
[7] P Lindstrom, “Fast and Memory Effcient Polygonal Simplifcation,” Proc. Visualization'98, pp.279-286, 
1998. 
[8] M. Garland and P. S. Heckbert, "Surface simplification using quadric error metrics," Proc. ACM 
SIGGRAPH'97, pp.209-216, 1997. 
 2 
刊出於 IEEE Explore 之影像處理與視覺計算，以及電腦視覺方面之論文之優良品質
亦為一誘因，我國過去幾年均有各大學及研究單位之人員參與該二國際會議發表論
文。今年有中央研究院陳教授、清華大學賴尚宏教授、高雄大學陳佳妍教授、宜蘭
大學黃于飛教授、暨南大學洪教授、以及義守大學本人參加，本人亦榮幸獲邀為該
二國際會議之論文議程委員。 
此次出席會議，成果豐碩。會議之第一天，本人於上午完成報到手續，並利用報到
空檔時間與來自各地之出席者寒暄，其中有不少是常在有關會議見過幾次面之熟悉
面孔，倍感親切，例如來自德國的 Ralf Reulke 博士、大會主辦主席 Canterbury 大
學 Andrew Bainbridge-Smith 教授、當地奧克蘭大學之 John Morris，Patrice Delmas
教授等人，尤需一提，其中奧克蘭大學 John Morris 教授係於去年會議晚宴中同意
本人初步邀請，而於今年 10 月 18 日下午至電機系講演。本人過去幾年均獲邀為國
際影像與視覺計算會議之議程委員，與大會主席 Canterbury 大學之 Andrew 
Bainbridge-Smith 教授與相關工作人員互動良好。大會開始之主題講者為奧克蘭大
學 Reinhard Klette 教授主講具前瞻性之ㄧ主題"Progress and Challenges in 
Vision-Based Driver Assistant Systems"，內容精彩實用，頗具有參考價值;就個
人記憶所及，Klette 教授與台灣數所大學互動持續且良好，曾獲邀為義守大學電機
系 2005 年主辦之 ICSS 國際會議之主題講者，且隨後數次藉訪問台灣之便至本校演
講，並於 2008 年與清華大學陳永昌教授合辦 PSIVT 國際會議，且於 2009 年為宜蘭
大學主辦之國際會議之主題講者，為與台灣學術與研究界互動頻繁之國際知名學者;
我國駐奧克蘭辦事處亦於今年 11月 23日特別以晚宴答謝 Klette 教授於雙方學術與
研究之努力，Klette 教授雖於會議期間抽空邀請本人同往，惟因行程關係，錯失良
 4 
值得稱讚。國際影像與視覺計算會議註冊費不高，為紐幣 370 元，約為台幣 8800 元;
惟其可租用 Rydges Hotel，且提供良好晚宴，預算之應用相當有效率;且因與會者領
域有很大之同質性，彼此容易建立溝通管道與興趣，且全程參與會議一討論，使會
議氣氛熱絡與融洽，可謂一成功之會議，其各優點可作為以後本系舉辦相似會議之
借鏡。國際影像與視覺計算會議明年預定將在北島風景優美之地點舉行，相信仍將
吸引許多國際參與者。本人於此次會議中收集一些相關資料、照片、與會議論文集，
歡迎有興趣者借閱與指教。 
三、考察參觀活動(無是項活動者略) 
四、建議 
五、攜回資料名稱及內容 
六、其他 
(a) (b)
(c) (d)
Figure 1: (a) A checkerboard referenced to calib-
rate projector and (b) improper correspondences
decoded from extracted patterns. (c) Another
checkerboard (d) with the projection of checker
markers.
ric calibration of a camera-projector system treats
the projector as an inverse camera and calibrates
both camera and projector using the same target
(e.g. [8, 9]). The acquisition of dense stereo cor-
respondences needs to be carried out to establish
a mapping of control points to calibrate the pro-
jector. To achieve better accuracy, an implemented
strategy usually collects multiple sets of calibration
data from different viewpoints. In this case the
acquisition of stereo correspondences is performed
repeatly, using multiple images and, as a result,
largely increases the calibration time. In the exper-
imental results [8]and [9] report reprojection errors
of 0.2243 and 0.1133 pixels, respectively.
The recent work [10] proposed a flexible calibration
method that uses only a few images to estimate the
projection matrix of the projector. Their two-shot
method is based on projective invariants, and es-
tablishes sparse world-projector point correspond-
ences without the projection of sequences of en-
coded light patterns. Although the method as-
sumes that the projector follows inverse linear pin-
hole camera model, a nonnegligible radial distor-
tion of projector lens (especially for a cheap port-
able projector) is found in our experiments. The
reported reprojection error of projector is 0.4282
pixel.
Some proposed methods are to use another projector-
friendly object (e.g. a white board) from which the
projected calibration patterns are easy to extract
(e.g. [11, 12]). The major drawback of these meth-
ods is that two different objects are required to
calibrate a camera-projector system. Furthermore,
since the devices are separately calibrated from
different sets of control point, a derivative-based
optimization procedure cannot directly integrate
Jacobian matrices of the camera and the projector.
There are also proposals that suggested the use
of special devices to overcome the interference of
calibration pattern. For instance, Zhan et. al. [6]
uses a LCD monitor as the calibration target. The
panel is turned on to display a checkerboard pat-
tern to calibrate a camera, and turned off during
the projection of light patterns. They achieve an
accuracy of arround 0.4 pixel in reprojection error.
In the next section we propose a calibration method
that uses one planar checkerboard to calibrate both
the camera and projector with comparable accur-
acy.
3 Proposed Method
3.1 Overall Strategy
Before we go into details of the proposed method,
an overview is given as follows. First, an image
of a checkerboard is taken and analyzed to ex-
tract control points, which are used to calibrate
the camera. Afterwards, rough camera-projector
correspondences are established by the projection
of encoded light patterns (3.3). The decoded map-
ping is looked up to generate the first calibration
pattern that contains features aimed to hit the cen-
ters of white squares on the checkerboard (3.4).
Second image of the calibration target at the same
position is taken, but this time with the projection
of the generated light pattern. By identifying the
residuals of feature points, a set of accurate world-
projector correspondences is acquired (3.5). The
projector is then calibrated using refined data.
For a new calibration viewpoint (by altering the
position of the checkerboard while other devices
stay stationary), the world-projector correspond-
ences of control points can be synthesized directly
using previous results, without the projection of
a sequence of encoded light patterns (3.6). In this
manner, the number of frames required to complete
a calibration for each new viewpoint is reduced to
two, one for camera and one for projector. The
overall procedure is depicted in figure 2. The de-
scribed method has the following features:
Fast. The encoded light patterns are only needed
for the first viewpoint. In subsequent view-
points, the calibration patterns are generated
on-the-fly, enabling the system to be calib-
rated in two-shots.
Adaptive. The light patterns are computed from
the position of the calibration target, there-
fore the problematic extraction of markers
can be avoided.
Close-looped. The system has an opportunity to
fix errors in calibration data through image
(a) (b)
(c) (d)
Figure 3: (a)(b) Two examples of 2-d coloured
Gray pattern and (c)(d) their projections onto
checkerboard. Note the black squares do not
preserve much colour information.
black areas is nearly impossible unless the lighting
condition is improved.
The identification of patterns is robustly done by
an adaptive thresholding algorithm. The auto-
matic classification mechanism first masks out re-
gions that have bad responses to projection of light
in the image. The pixel values in green and blue
channels are extracted and processed separately in
subsequent processes. By applying Otsu’s method
[15], an optimal threshold value that achieves min-
imal intra-class variance in each channel is found.
Once the binarization is done, the remaining pro-
cedure is simply the decoding of ordinary Gray
patterns. This method is adaptive to the distortion
of colours throughout the projector-scene-camera
chain of energy propagation.
3.4 Generation of Calibration Patterns
The proposed method does not use a predefined
pattern to calibrate projector. Instead the calibra-
tion patterns are generated according to the image
of calibration target. The following elements are
taken into account to generate the initial calibra-
tion pattern:
1. A set of world coordinates of control points
Iwc , as used to calibrate the camera in the
previous stage. These coordinates are labeled
using an automatic homography-based algorithm,
which was developed in previous work [16].
2. A plane-to-plane mapping that denotes camera-
projector correspondences. This mapping is
acquired from 2-d Gray patterns.
A set of world coordinates Iwp are computed from
Iwc according to the grid structure of checkerboard.
The estimated location of each new control point
is supposed to be at the center of a targeted white
square. A set of image pixels Icp corresponding
to Iwp is then estimated using homography Hwc
which is computed from the mapping Iwc → Icc as
used to calibrate the camera. Since the camera
has been calibrated prior to this stage, the image
coordinates can be optionally undistorted to derive
the homography, although the removal of radial
distortion is not essential, as the errors will be
compensated later.
The system is ready to generate calibration pattern
once Iwp and I
c
p are obtained. The mapping of
camera-projector correspondences is looked up to
find Ipp , the pixels on projector screen that cor-
respond to Icp. The pattern can be synthesized in
different ways depending on the choice of feature
markers. This work adopts the checker marker.
The pattern generated in such manner is target-
adapted since the features are controlled by geo-
metrical configuration. The stated process is illus-
trated in figure 4.
3.5 Marker Tracking
Once the generated pattern is projected, an im-
age is captured by the camera for further ana-
lysis. The routine designed to extract and cluster
feature points for calibration of camera is reused
here. However, the labeling of projected markers
is different as the expected locations of markers are
already known.
Using given values Icp, the labeling procedure be-
gins to search for each control point from expec-
ted location in the captured image. The search is
bounded by an adjustable radius, which controls
the tolerance to mismatch of points. The observed
image pixels of control points are denoted by Icp
′,
by which the updated world coordinates Iwp
′ are
retrieved. The update is performed through the
inverse of Hwc, which is the homography previ-
ously calculated. In particular, the updated con-
trol points are
Iwp
′ = H−1wc I
c
p
′. (5)
This update is the key to improve calibration ac-
curacy from image feedback. The refined world-
projector correspondences Iwp
′ → Ipp are then input
to Tsai’s linear calibration process. The image
pixels referred in this stage are radially undistor-
ted.
The result of residual tracking on a region is illus-
trated in figure 5. The green circles are expected
locations, and red crosses are actual observations.
The residuals are depicted by lines. Each red line
optimization during the calibration process. Since
the device-specific optimization does not model the
dependency between devices, a final system-scope
optimization is suggested to be carried out at the
end of calibration. The system-scope optimization
also takes into account the linkage defined in equa-
tion (5).
4 Experimental Results
Two camera-projector systems have been set up
to evaluate proposed method. The system Clas-
sical has a digital single lens reflex (DSLR) cam-
era with high image resolution, and a modest data
projector. Another system, namely Portable,
is equipped with a pocket size projector and a
cheap CCD USB camera. This system has a noisy
image sensor that works at relatively low resolution
as well as limited brightness of light projection,
making it suitable to test how the proposed calib-
ration works under bad imaging conditions. These
components are listed in table 1.
For each system nine views of a checkerboard with
different poses have been taken. Figure 6 shows
first four viewpoints through the calibration pro-
cess of system Portable. The performance is
measured by mean reprojection error (RPE), which
is obtained by calculating the Euclidean distance
between expected and reprojected pixels of a con-
trol point. The results are plotted in figure 7. The
calibrated projectors of systemClassical andPort-
able respectively achieves average RPE of 0.09
and 0.22 pixels.
The systems are also evaluated in Euclidean space
using triangulated 3-d coordinates. The results are
compared with conventional method that uses 14
Gray-coded patterns plus 16 phase shifted sinus-
oidal patterns to establish world-projector corres-
pondences for each viewpoint. The conventional
method used a total of 270 frames, which is more
than ten times the proposed method needed, to
finish the calibration. The average triangulation
errors of control points of system Classical and
Portable are 0.035mm and 0.157mm respectively.
Compared to the conventional method which achieves
0.344mm and 0.298mm in two experimental sys-
tems, the improvements are 89.83% and 49.47%.
For each view the triangulated 3-d control points
are further fitted to a plane by orthogonal regres-
sion. The residuals are measured for evaluation,
as shown in table 2 and table 3. The planarity de-
viations of system Classical and Portable are
0.144mm and 0.483mm respectively. Compared to
the conventional method which achieves 0.879mm
and 0.933mm, the improvements are 87.03% and
48.32%.
5 Conclusion
Our proposal allows a camera-projector system to
be calibrated in a more accurate way using less
frames. The method described in this paper offers
a convenient solution that utilizes one planar target
to calibrate both the camera and the projector
from one or multiple viewpoints. The procedure is
designed to be fully automated. All that the user
has to do is to change the pose of reference tar-
get to begin a calibration of new viewpoint. This
operation can also be automated when a computer-
controlled shifting device or turntable is available.
The experimental results have shown a significant
improvement in accuracy is attainable over con-
ventional method. The results also show that the
calibrated parameters are satisfactory for precision
applications that require accurate geometrical cal-
ibration.
6 Acknowledgements
This work is sponsored by National Science Coun-
cil of Taiwan under grant number NSC 99-2631-H-
390-001.
References
[1] J. C. Lee, J. C. Lee, P. H. Dietz, P. H. Dietz,
D. Maynes-Aminzade, D. Maynes-aminzade,
S. E. Hudson, and S. E. Hudson, “Auto-
matic projector calibration with embedded
light sensors,” IN PROCEEDINGS OF UIST
2004, vol. 1, pp. 123—126, 2004.
[2] R. R. Jeroen, J. Vanbaar, and P. Beardsley,
“Display grid: Ad-hoc clusters of autonomous
projectors,” in Proceedings of Society for In-
formation Displays International Symposium,
May 2004.
[3] R. Furukawa and H. Kawasaki, “Dense 3D
reconstruction with an uncalibrated stereo
system using coded structured light,” in Com-
puter Vision and Pattern Recognition Work-
shop, vol. 0. Los Alamitos, CA, USA: IEEE
Computer Society, 2005, p. 107.
[4] D. G. Aliaga and Y. Xu, “Photogeometric
structured light: A self-calibrating and multi-
viewpoint framework for accurate 3d model-
ing,” in CVPR08, 2008, pp. 1–8.
[5] H. Chien, C. Chen, C. Chen, and Y. Su, “Ad-
aptive pixel classifier for binary structured
light: A probabilistic kernel approach,” in
Proceedings of Image and Vision Computing
New Zealand, 2009, Wellington, New Zealand,
Nov. 2009, pp. 367–372.
Table 2: Planarity errors of system Classical (mm)
Calibration
Viewpoint
1 2 3 4 5 6 7 8 9 avg.
Conventional 1.441 1.617 0.520 0.543 0.669 0.880 0.986 0.624 0.582 0.879
Proposed 0.175 0.123 0.111 0.104 0.142 0.166 0.130 0.175 0.170 0.144
Table 3: Planarity errors of system Portable (mm)
Calibration
Viewpoint
1 2 3 4 5 6 7 8 9 avg.
Conventional 1.366 0.876 0.841 0.805 0.849 0.902 0.817 0.992 0.948 0.933
Proposed 0.456 0.514 0.404 0.573 0.443 0.413 0.466 0.554 0.513 0.483
[6] Z. Song and R. Chung, “Use of LCD panel
for calibrating Structured-Light-Based range
sensing system,” IEEE Transactions on In-
strumentation and Measurement, vol. 57,
no. 11, pp. 2623–2630, 2008.
[7] S. Audet and M. Okutomi, “A user-friendly
method to geometrically calibrate projector-
camera systems,” in Computer Vision and
Pattern Recognition Workshop, vol. 0. Los
Alamitos, CA, USA: IEEE Computer Society,
2009, pp. 47–54.
[8] H. Cui, N. Dai, T. Yuan, X. Cheng, and
W. Liao, “Calibration algorithm for struc-
tured light 3D vision measuring system,” in
2008 Congress on Image and Signal Pro-
cessing, Sanya, China, 2008, pp. 324–328.
[9] X. Chen, J. Xi, Y. Jin, and J. Sun, “Accurate
calibration for a camera-projector measure-
ment system based on structured light pro-
jection,” Optics and Lasers in Engineering,
vol. 47, no. 3-4, pp. 310–319, 2009.
[10] W. Gao, L. Wang, and Z. Hu, “Flexible
calibration of a portable structured light sys-
tem through surface plane,” Acta Automatica
Sinica, vol. 34, no. 11, pp. 1358–1362, Nov.
2008.
[11] M. Kimura, M. Mochimaru, and T. Kanade,
“Projector calibration using arbitrary planes
and calibrated camera,” in Proceedings of
Projector-Camera Systems workshops 2007
(PROCAM07), 2007, pp. 1–2.
[12] F. Gabriel, H. Natalia, and M. Joan,
“Plane-based calibration of a projector-
camera system,” Tech. Rep., 2008. [Online].
Available: http://procamcalib.googlecode.
com/files/ProCam Calib v2.pdf
[13] R. Klette, K. Schluns, and A. Koschan, Com-
puter Vision: Three-Dimensional Data from
Images, 1st ed. Springer, Sep. 1998.
[14] J. Salvi, S. Fernandez, T. Pribanic, and
X. Llado, “A state of the art in structured
light patterns for surface profilometry,” Pat-
tern Recognition, vol. 43, no. 8, pp. 2666–2680,
Aug. 2010.
[15] N. Otsu, “A threshold selection method from
gray-level histograms,” IEEE Transactions on
Systems, Man and Cybernetics, vol. 9, no. 1,
pp. 66, 62, Jan. 1979.
[16] H. J. Chien, C. Y. Chen, and C. F. Chen, “Re-
construction of cultural artifact using struc-
tured lighting with densified stereo corres-
pondence,” in Arts and Technology, 2010, pp.
239–246.
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/19
國科會補助計畫
計畫名稱: 快速擷取個人化臉部三維資訊並合成擬真三維臉部動畫之系統以及方法
計畫主持人: 陳基發
計畫編號: 99-2221-E-214-064- 學門領域: 影像處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
