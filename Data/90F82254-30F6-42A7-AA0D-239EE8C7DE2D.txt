II 
 
摘要 
在 911 事件後的分析發現所謂危險份子之間其實是隱藏著某種連結（例如
說他們曾經在同一間餐廳出現）。然而，這些人可能也與一般普通民眾有著連結。
一般的社群網路之間通常隱含少數危險份子以及大部分的普通民眾。所以，如何
能夠自動在這個網路中找出少數可疑的人物變成了一個重要的研究議題。這個研
究題目的挑戰有二：首先那些危險人物的行為模式並不是固定，可能會隨著時間
經驗改變，所以利用他們過去的行為來預測未來的行為，不一定能達到目的。第
二個挑戰來自資料複雜度：在很大且複雜的社群網路中，人的行為模式會成指數
成長，增加計算上的負荷。本研究主要目的就是提出一個新穎的「非指導式」學
習機制，來找出複雜網路中特殊而且可疑的個體。這個系統將不只可以應用在國
土安全相關的問題，也可用在生物或化學等科學發明相關應用。 
Abstract 
The post-event analysis of 9/11 tragedy reveals that there are certain hidden 
social connections among terrorists (for example, some of them happened to show up in 
the same restaurant occasionally). However, those terrorists also have many connections 
with other normal persons. They together can form a large social network which 
contains mostly unharmful individuals and a small amount of dangerous ones. An 
important research problem in knowledge discovery data mining will then arise as how 
an intelligent program can automatically detect suspicious individuals such as the 
potential terrorists from such a huge network. This problem is challenging in two aspects. 
First, the threatening people usually change their crime pattern over time to avoid being 
detected; therefore simply learning their previous behaviors cannot be very effective in 
terms of predicting the future. The second challenge comes from complexity. In a 
complex social network, there could be enormous combinations of connected instances 
which makes search very time demanding. To handle the above challenges, we propose 
a novel and unsupervised framework to find interesting or suspicious instances from 
social networks. Since the proposed system is unsupervised and domain independent, it 
can be applied to not only homeland security related domain but also other scientific 
discovery problems such as biology and chemical discovery. 
2 
 
semantic graphs such as relational Bayesian networks, relational Markov networks, and 
relational dependency networks (Jaeger 1997; Bunescu and Mooney 2004; Neville and 
Jensen 2004).  However, these frameworks aim at exploiting the graph structure to learn 
the dependency (i.e. joint probability) or the posterior probability of events (nodes) or 
relations between them based on training examples. Our goal is very different, since we 
are focusing on identifying suspicious instances and we want to do it without introducing 
training bias. Also our framework has an ultimate goal as being able to generate human 
understandable explanations for its findings. 
Methodology: 
We propose a social network mining system that is capable of identifying suspicious 
nodes in a social network. The first step is to extract the environment of each individual 
node and determine the types of paths that surround it. A path type is a group of common 
paths that can be represented by a sequence of links or nodes with certain variables. Once 
the path types are determined, the system can utilize some sampling methods (e.g. 
pointwise mutual information) to learn the statistical dependency between the node and 
the path types to produce the semantic profile. The semantic profile is essentially a 
representation of this node’s social role based on its neighbor structure.  
 The flow of the proposed system can be seen in Figure 2.  First, the system generates the 
set of path types to use as feature values in the semantic profiles of nodes. Once the set of 
path types is determined, the system can then compute the dependency as the feature 
values to generate a semantic profile for each node. Finally, the system applies an 
existing outlier ranking algorithm to find nodes with abnormal semantic profiles.  
meta constraints
 
There are three major classes of outliers: clustering-based, distribution-based and 
distance-based outliers. Each has its associated detection algorithms. Clustering-based 
outlier detectors such as CLARANS (Ng and Han 1994), BIRCH (Zhang, Ramakrishnan 
4 
 
algorithm with a selection of state-of-the-art unsupervised network algorithms such as 
PageRank, social network analysis, random walks, etc.  
The data we used came from a suite of simulated datasets designed by Information 
Extraction & Transport, Inc. as part of DARPA’s Evidence Extraction and Link 
Discovery program. The data simulates a Russian organized crime domain with a large 
number of entities involved in activities such as contract murders, gang wars and industry 
takeovers.  For each dataset we are given an answer key containing the information of 
participants in the high-level events of interest. Those participants are not explicitly 
described in the data but need to be inferred from lower-level, incomplete and noisy 
evidence.  With these answer keys we can test our program by checking if the suspicious 
nodes it discovered match the hidden higher-level event participants in the data.  
There are two different types of top-level events: gang wars and industry-takeovers.  
Gang wars occur between two rivaling Mafiyas, and industry takeovers are attempts by 
one Mafiya to take over an industry controlled by another. Both events are composed of 
various levels of lower level events such as murders for hire (or contract murders), 
communication transactions, financial transactions, etc.  
We perform this experiment by applying our system to find the most suspicious Mafiyas. 
UNICORN first generates the semantic profile for all Mafiyas and then applies 
Ramaswamy’s distance outlier algorithm to rank them. Our goal is to see if the top three 
suspicious Mafiyas our program identified match the crime participants (two for gang 
war and one for industry takeover) described in the answer key. We did a similar 
experiment for industries, where we want to know if the industry on top of our list is the 
one being taken over. We compare our results with other network ranking algorithms 
including PageRank (Page, Brin et al. 1998), HITS (Kleinberg 1999) and Betweeness 
Centrality (Wasserman and Faust 1994). The results are shown in Table 1. 
The schema of the Contract Murder Dataset looks like Figure 4 and the results are 
displayed in table 1. 
 
6 
 
dataset UNICORN ranks Mafiya_gang1 as the 4th in its list of suspicious individuals, 
while HITS rank it as 15th. A perfect algorithm should rank the three target Mafiyas as 
the top three and the target industry as the top one. 
The results show that the top three most suspicious nodes identified by UNICORN 
are consistently the ones that are supposed to be found in the answer key, while this is not 
the case for the other three algorithms.  We believe the major reason that UNICORN 
outperforms the other algorithms by a large margin is twofold: first, it has the capability 
to utilize information provided by different types of links, while the other algorithms do 
not take different semantics of links into account.  Second, using abnormality as a 
heuristic for finding suspicious instances seems to be a better option than using centrality 
or importance of nodes. 
Conclusion 
In this paper we described a general unsupervised framework for identifying suspicious 
individuals in large and complex social networks.  Our contribution is the design of a 
novel framework called UNICORN that can summarize the semantics of nodes into 
propositional semantic profiles.  Given such profiles we can compare and contrast nodes 
and exploit distance-based outlier algorithms to identify abnormal and suspicious nodes.  
Since the method is unsupervised and does not require training examples or user-defined 
features, it has the potential to discover truly novel instances without being biased by 
human analysts or training examples. 
Our algorithm have been successfully applied to networks with around 40000 nodes and 
400,000’s of links.  However, an important future direction for us is to further improve 
the scalability of the system.  What is most expensive is the computation of contributions, 
since it requires the system to search and count specific types of paths in a potentially 
very large graph.  Since contributions define a conditional probability distribution for 
paths based on nodes, we are investigating different sampling methods to approximate 
these values without having to search all of the data, which should enable us to further 
improve the scalability of UNICORN.  
References: 
Adafre, S. F. and M. d. Rijke  (2005). Discovering missing  links  in wikipedia. Workshop on Link 
Discovery: Issues, Approaches and Applications (LinkKDD‐2005). 
