162 ၦଊᆔ౪Ᏹൣ! ಒΫѳڣ! ಒѳ෉
Using Independent Component Analysis and 
Support Vector Regression for Financial Time Series 
Forecasting 
Chi-Jie Lu 
Department of Industrial Engineering and Management, Ching Yung University 
Tian-Shyug Lee 
Graduate Institute of Management, Fu-Jen Catholic University 
Hsueh-Chun Chen 
Graduate Institute of Applied Statistics, Fu-Jen Catholic University 
Abstract 
As financial time series are inherently noisy, non-stationary and deterministically 
chaotic, it is one of the most challenging applications of modern time series forecasting. 
Due to the advantages of the generalization capability in obtaining the unique and global 
optimal solution, support vector regression (SVR), has also been successfully applied in 
time series prediction, especially in the financial time series forecasting. In the modeling 
of financial time series using SVR, one of the key problems is the inherent high noise. 
Therefore, detecting and removing the noise are important but difficult tasks when 
building an SVR forecasting model. To alleviate the influence of noise, a two-stage 
approach by integrating independent component analysis (ICA) and support vector 
regression is proposed in this research for financial time series forecasting. ICA is a novel 
statistical signal processing technique that was originally proposed to find the latent source 
signals from observed mixture signal without knowing any prior knowledge of the mixing 
mechanism. The proposed approach first uses ICA to the forecasting variables for 
generating the independent components (ICs). After identifying and removing the ICs 
containing the noise, the rest of the ICs are then used to reconstruct the forecasting 
variables which contain less noise. The SVR is then applied to use the filtered (or denoised) 
forecasting variables to build the forecasting model. In order to evaluate the performance 
of the proposed approach, the Nikkei 225 opening index is used as the illustrative example. 
The experimental results show that the proposed model outperforms the SVR model with 
non-filtered forecasting variables and random walk model. 
Key words: Independent component analysis, Support vector regression, Financial time 
series forecasting, Stock index 
164 ၦଊᆔ౪Ᏹൣ! ಒΫѳڣ! ಒѳ෉
є֥НԆϸ᜹ (Texture classification)ȃኈ჌ᜌր (Image recognition)ȃКቹ኶Ԇᒲᜌ
(Hand-written digit recognition)ȃၦਠ௥ଢ଼(Data mining)ІҢޑၦଊ(Bioinformatics)๊
ȞBurbidge et al. 2001; Kim et al. 2002; Chang et al. 2003; Norinder 2003; Li et al. 2003; 
Shin et al. 2005ȟȄՅଷΠᔗңܼϸ᜹ୱᚡѵȂDrucker et al. (1997)І Vapnik et al. (1997)
ᏳΤε-insensitivity ཭ѷړ኶(Loss function)ϟ྆܉ܼ SVM ϜȂණяΠМනӪ໕ଠᘫ
(Support vector regression, SVR)ዂԓٿ೏౪ߩጤܓޠଠᘫୱᚡȄҦܼМනӪ໕ଠᘫ૗
਑ਕ੬ኊޫ໣ޠංե੬ܓІਣ໣זӗၦਠϜޠ੬ኊȂϑഃᅛޠೞңܼөᆎႲขୱᚡϜ
ٯᕖூًԂޠႲขᕽਞ(Thissen et al.2003; Mohandes et al. 2004; Koike & Takagi 2004; 
Karras & Mertzios 2004)ȂӱԫҐःف஡ٻң SVR࣐࡛ᄻଓଡ଼ਣ໣זӗዂ࠯ϟϏڏȄ
Ӷ࡛Ҵଓଡ଼ਣ໣זӗႲขዂ࠯ਣȂ඾೾८ᖞϟୱᚡ൸࢑ଓଡ଼ਣ໣זӗၦਠє֥೩
ӼޠᚖଊȄ࿌ၦਠϜԇӶᚖଊਣȂཽٻூႲขዂԓӱӶଌጜႇโϜᏱಭژ೼ٳᚖଊޠ
ၦଊȂՅ౱Ңႇ࡚ପᎍ(Over-fitting)ܗପᎍϛ٘(Under-fitting)ޠୱᚡȂໍՅӶขၑ໧
ࢳോѷႲขᕽਞޠѠᎭܓᇅྦጃ࡚ȂӱԫԄե୏ขпІџଷଓଡ଼ਣ໣זӗၦਠϜޠᚖ
ଊȂп७մڐᄈ SVRϟႲข๗ݏޠኈ៫Ȃ࢑Κ໷२्ޠឋᚡȄ࣐ӱᔗࠊख़ϟୱᚡȂΚ
ٳ੬ੇޠ SVR ׭೛ȂԄ robust SVR(Chuang et al. 2002)І weighted Least Square 
SVR(Suykens et al. 2002)Ȃϑစೞණяңп஽Ͼ SVRӶ८ᄈႲขၦਠє֥ᚖଊਣޠႲ
ข૗ΩȂดՅ೼ٳঔҔޠ SVR׭೛ϛ࢑ሰ्τ໕ޠၽᆘਣ໣ᇅԪ኶Ȃ൸࢑ณݳጃۢ૗
஋ூژၷٺޠႲข๗ݏȄ
࣐ΠණЁଓଡ଼ਣ໣זӗႲขϟᕽਞІ၍؛ࠊख़ SVRѠ૗८ᖞϟୱᚡȂҐःفᏳΤ
ߗԒٿץഁึ৥ңܼଊဵϸᚕ (Signal separation)ϟᑀҴԚӌϸݚ (Independent 
component analysis, ICA)׭೛ܼଓଡ଼ਣ໣זӗႲขୱᚡαȂණяΚঐ๗ӬᑀҴԚӌϸ
ݚᇅМනӪ໕ଠᘫϟڎ໧ࢳਣ໣זӗႲขዂԓȂ஡ւң ICAٿ୏ขІᘯଷଓଡ଼ਣ໣ז
ӗၦਠϜޠᚖଊȂпණЁ SVRϟႲขྦጃ࡚ȄܛණႲขዂԓϟ࡛ᄻࢻโ࣐Ӓٻң ICA
ܼႲขᡑ኶ϜϸᚕяঐրᑀҴޠ዗Ӷٿྜଊ Ȟဵ֊ᑀҴԚӌȟȂ௦޲௄೼ٳᑀҴԚӌϜ
џଷхߓၦਠϜᚖଊޠᑀҴԚӌϟࡤȂ஡ߴ੽ޠᑀҴԚӌ२࡛ਣ໣זӗၦਠȂூژᘯ
ଷᚖଊࡤϟႲขᡑ኶Ȃഷࡤӕւң SVRпᘯଷᚖଊࡤޠႲขᡑ኶࡛ᄻႲขዂԓȄҐः
فܛණРݳ૗џଷࠊख़ঔҔ SVRዂԓሰτ໕ၽᆘਣ໣ܗ࢑ณݳூژഷٺ၍ޠુᘉȂη
૗෶Ў SVRӶዂԓ࡛ᄻਣȂӱڨᚖଊኈ៫Յ౱Ңႇ࡚ପᎍܗପᎍϛ٘ޠୱᚡȂໍՅණ
Ё SVRዂԓႲข๗ݏޠྦጃ࡚Ȅ࣐ᡜᜍܛණРݳϟԥਞܓȂҐःف஡пСစ 225౫ೳ
໡ዻࡿ኶Іѯ޷౫ೳԞዻࡿ኶ϟႲขໍ՘ᄃᜍःفȂٯᇅ൑હٻң SVRዂԓІᓎᐡᅟ
ؐዂԓޠႲข๗ݏձЩၷȄ
ҐНө࿾ୣϸԄίȈಒΡ࿾஡Ӳ៬ᑀҴԚӌϸݚІМනӪ໕ଠᘫϟࣻᜱНᝧȇಒ
ή࿾ଭᄈᑀҴԚӌϸݚІМනӪ໕ଠᘫໍ՘ᙐ൑ޠϮಞȇಒѳ࿾ࠍᇴ݃ܛණϟڎ໧ࢳ
Ⴒขዂ࠯ϟ࡛ᄻРԓȇಒϥ࿾࣐Ⴒขዂ࠯ϟᄃᡜ๗ݏᇅଇ፤Ȃٯᇅ൑હٻң SVRዂԓ
ІᓎᐡᅟؐРݳޠ๗ݏໍ՘Щၷȇഷࡤϟಒϳ࿾ࠍ࣐Ґःفϟ๗፤ᇅ࡛ឋȄ
166 ၦଊᆔ౪Ᏹൣ! ಒΫѳڣ! ಒѳ෉
ΡȃМනӪ໕ଠᘫ(Support vector regression, SVR) 
МනӪ໕ᐡ(Support vector machine, SVM)࢑пಜॏᏱಭ౪፤(Statistical learning 
theory)࣐ஆᙄȂၽң๗ᄻॴᓏഷϊϾ঩౪ (Structural risk minimization inductive 
principle)пІഷτϾӶထᡞ໣ϟᘉޠᜟࣩ(Margin)ܛණяϟᐡᏣᏱಭقಜ(Vapnik et al.
1997; Vapnik 2000)ȄSVMл्ңٿ၍؛ጤܓΡԪ೤დ(Quadratic programming, QP)ୱ
ᚡȂٯйϑစኅހޠᔗңܼ၍؛ϸ᜹ޠୱᚡȄՅӶ೏౪ႲขୱᚡαȂDrucker et al.(1997)
І Vapnik et al.(1997)ւңε-insensitivity཭ѷړ኶஡ SVMޠ྆܉ၽңܼଠᘫୱᚡϜȂ
ՅණяМනӪ໕ଠᘫ(SVR)ዂ࠯ȄSVR Ҧܼ૗਑ਕ੬ኊޫ໣ޠංե੬ܓІਣ໣זӗၦ
ਠϜޠ੬ኊȂӱԫഃᅛޠೞңܼөᆎႲขୱᚡϜȂٯᕖூًԂޠႲขᕽਞ(Koike & 
Takagi 2004; Karras & Mertzios 2004)ȄThissen et al. (2003)ւң SVRȃANNІ ARMA
୉࣐Ⴒขዂ࠯ȂᄃᜍڎಣዂᔤၦਠІΚಣળ઴ႇโϜᘯఽᏣ኶অϟਣ໣זӗၦਠȂ๗
ݏึ౫ SVRϟႲข๗ݏၷ ARMAІ ANN࣐ٺȄMohandes et al. (2004)ւң SVRІӼ
ቺདޤᏣ(Multilayer Perceptrons, MLP)ܼॴഁႲขȂഇႇ RMSEձ࣐ຠզࡿዀȂ๗ݏ
ึ౫ SVRϟߓ౫ᓻܼMLPዂ࠯ȄPai & Lin(2005)ւң SVRȃSARIMA(Seasonal ARIMA)
ІኅဏӲᘫઢစᆪၰ(General Regression Neural Network, GRNN)Ⴒขڏԥ۠࿾ܓޠϏ
ཿᐡఢᖃ౱অȂ๗ݏᡘұ SVRᓻܼ GRNNІ SARIMAዂ࠯Ȅ
ՅӶଓଡ଼ਣ໣זӗႲขР८ȂTay & Cao(2001)ٻңМනӪ໕ᐡႲขଓߝҀൠϟ኶
ᐄȂٯйໍ՘୥኶ϟఄདܓϸݚпᒶᐆႲขዂԓϟഷٺ୥኶Ȃᄃᡜ๗ݏᡘұМනӪ໕
ᐡዂ࠯ᓻܼঈ༉ሏ᜹ઢစᆪၰȄKim(2003)ࠍւң׭೛ܓࡿዀձ࣐ᒰΤᡑ኶ȂପӬ
SVRȃBPNNІ਱پ௱౪ݳ(Case-based reasoning)୉࣐Ⴒขዂ࠯ȂᄃᜍؑСᗻ୾䶵Ӭ޷
ቌࡿ኶(KOSPI)ԞዻቌޠᅜໂȂ๗ݏᡘұ SVRϟߓ౫ഷٺȂ૗ྦጃႲข 57.83%ޠᅜໂȄ
Cao(2003)ණяᐍӬ SVR ϟਣ໣זӗ஠ঢ়ႲขقಜȂ၏قಜ࣐ڎ໧ࢳޠᆪၰ࢝ᄻȂॷ
ӒഇႇՍשಣᙒ੬ኊ࢏ৣᆪၰ(Self-organizing feature map network)ձ࣐໲ထϸݚᅌᆘ
ݳȂ஡ᒰΤᡑ኶ޫ໣ୣϸԚ኶ঐณᜱᖓޠ໲ထȂಒΡ໧ࢳࠍഇႇ SVRଭᄈөထᒶᐆഷ
ٺႲขዂ࠯ȂᄃᜍЋ໩༄φၦਠІဒ༲ຆःفଲ(Santa Fe Institute)ϟຏᔤ޷ಊҀൠၦ
ਠޠ๗ݏᡘұȂٻңԫᐍӬ SVRႲขقಜϟႲข๗ݏᓻܼ SVRȄ
୥ȃःفРݳ
ΚȃᑀҴԚӌϸݚ
ᑀҴԚӌϸݚ(ICA)࢑Κᆎңٿ׳яᓎᐡᡑ኶ϜᗵᙡӱφޠಜॏРݳ(Comon 1994; 
Lee 1998; Hyvärinen & Oja 2000)ȄICA୆೪ᢏᄇژޠᓎᐡᡑ኶࢑Ҧґޤޠ዗Ӷᡑ኶
(Latent variables)пጤܓРԓಣӬՅԚȂՅಣӬޠᐡښґޤȄ೼ٳ዗Ӷᡑ኶Ӷ୆೪Ϥ࣐
ᑀҴޠ௒ݸίȂೞᆏ࣐ᢏᄇၦਠޠ዗Ӷٿྜ(Latent sources)ȄICA ൸࢑ӶѬԥᢏᄇژ
168 ၦଊᆔ౪Ᏹൣ! ಒΫѳڣ! ಒѳ෉
)()()( yyy HHJ gauss −= Ȟ3ȟ
ڐϜ gaussy ߓұڸ yڏԥࣻӤᡑ౵኶ޠାලᡑ኶Ӫ໕Ȅ॓ⰒޠঅҘϛ࣐॓অȂ֊
0)( ≥yJ ȂѬԥ y࣐ାලϸոਣ )(yJ Ϙཽ࣐ႮȂܛп ICA ޠҭዀړ኶൸ᡑԚ्ഷτϾ
ᓎᐡӪ໕ yޠ॓ⰒȂ֊ )(yJMaximize Ȅ
ดՅȂ॓Ⱂഷτޠୱᚡ൸࢑ॏᆘЋႇፓᚖȂӱ࣐਴ᐄۢဏȂ्զॏя॓Ⱂ൸҇໹
Ӓզॏ yޠᐡ౦ஞ࡚ړ኶ )(yf Ȅ࣐Π၍؛೼೼ঐୱᚡȂHyvärinen(1999)ึ৥я॓Ⱂޠ
ߗծړ኶
2)}]({)}({[)( νGEyGEyJ −∝ Ȟ4ȟ
ڐϜν࢑҂ְঅ࣐Ⴎйᡑ౵኶࣐ 1 ޠାලϸոϟᓎᐡᡑ኶ȄGѠ࣐ӉեߩΡԪР
ړ኶(Non-quadratic function)Ȃӱ࣐Gस࣐ΡԪРړ኶Ȃ )(yJ ࣐҇ႮȄ
ӶಁӼңٿؒ၍ ICA ዂԓޠᅌᆘݳϜȂHyvärinen (1999) ܛණя FastICAᅌᆘݳ
ҦܼڏԥًԂޠၽᆘਞ౦Ȃй࢑೏౪ ICA ୱᚡαഷளೞٻңޠᅌᆘݳϟΚȂӱԫҐः
ف஡ٻң FastICAᅌᆘݳ೏౪௄ଓଡ଼ਣ໣זӗၦਠϜϸᚕяᑀҴԚӌޠϏձȄ
ΡȃМනӪ໕ଠᘫ(Support vector regression) 
࣐ᇴ݃ SVRዂԓȂॷӒՄኍΚঐڑ࠯ޠଠᘫዂԓȂڐҭޠ࢑զॏΚঐґޤРโԓ
)(xr ٿႲขΚঐґޤޠঅ qȂڎ޲ޠᜱ߾ԄίȈ
δ+= )(xrq
ڐϜ δ ࢑ᓎᐡᑀҴٯй҂ְ኶࣐Ⴎޠᓎᐡᇳৰ(Random error)Ȃ x࢑Ӽᡑ኶ޠᒰΤ
ᡑ኶Ȃ q࢑൑Κ(Scalar)ޠᒰяঅȄ೼ঐРโԓѠпңԥ४ঐ኶ޠଌጜኻҐٿ࡛ҴȈ
),( ii qx , )...,,1( ni = ȄӶଠᘫୱᚡϜѠϸ࣐ጤܓଠᘫᇅߩጤܓଠᘫୱᚡȂጤܓଠᘫୱᚡ
࢑ၷܿ೏౪Ȃٯйϑԥࣻ࿌Ӽޠ׭೛ೞණяٿᇅԚѓޠᔗңȇࣻᄈՅّȂߩጤܓଠᘫ
ࠍၷϛৡܿ೏౪ȄSVRл्൸࢑ೞึ৥ٿ೏౪ߩጤܓଠᘫޠឋᚡޠϏڏȂڐл्྆܉
࢑஡ΚঐӶմᆱ࡚ᒰΤޫ໣(Input space)ϜߩጤܓଠᘫϟୱᚡȂᙾඳԚӶାᆱ࡚੬ኊޫ
໣(Feature space)ϜޠጤܓଠᘫୱᚡȄӱԫӶ SVRϜȂᒰΤᡑ኶ xॷӒೞᙾඳ(Map)ژ
Κঐାᆱ࡚ޠ੬ኊޫ໣( F )ϜȂӶ೼੬ኊޫ໣ϜȂᒰΤᡑ኶ѠпೞঔҔ࣐ᇅᒰяᡑ኶
ԥጤܓޠᜱ߾Ȅп኶ᏱዂԓߓұȂSVRዂԓѠпߓұ࣐(Vapnik 2000) 
bf +Φ⋅= ))(()( xȦx Ȟ5ȟ
ڐϜȂȦ࣐᠍२Ӫ໕Ȃ b࢑࣐୒ৰঅ(bias)Ȃ )(xΦ ࣐Κᙾඳړ኶Ȃңп஡ᒰΤᡑ኶
xпߩጤܓޠРԓᙾඳژାᆱ࡚ޠ੬ኊޫ໣ϜȂ ))(( xȦ Φ⋅ ࠍңٿඣख़Ӷ੬ኊޫ໣ FϜ
ޠঅȄ
ӶଠᘫୱᚡϜȂѠпӶଌጜၦਠңഷϊϾᄃᡜॴᓏ(Minimization of empirical risk)
ޠྦࠍȂզॏଠᘫዂԓϜޠ୥኶Ȅڑ࠯ೞңӶഷϊϾᄃᡜॴᓏϜޠ཭ѷړ኶є֥ԥ๙
ᄈঅᇳৰІ҂Рᇳৰ๊ȂկӶ SVR ϜȂVapnik et al. (1997)ࠍٻңΠΚঐᆏ࣐
ε-insensitivityޠ཭ѷړ኶( εL )Ȉ
170 ၦଊᆔ౪Ᏹൣ! ಒΫѳڣ! ಒѳ෉
ϵԓȞ6ȟ࣐ΡԪ೤დȞQPȟޠഷٺϾୱᚡȂѠпңܝԓॹ኶(Lagrange Multipliers)
ᙾඳԚᄈୌୱᚡٿؒ၍྄τঅȂϵԓ(6)Ѡпᙾඳ࣐ίԓ
MaximizeȈ
),()()(
2
1)()(),( *
1,
*
1
*
1
**
jijj
n
ji
iii
n
i
ii
n
i
iid qL xxKααααααααεαα −−−−++−= ¦¦¦
===
Subject to 
niC
niC
i
i
n
i
ii
,...,1,0
,...,1,0
0)(
*
1
*
=≤≤
=≤≤
=−
°¯
°
®
­ ¦
=
α
α
αα
Ȟ7ȟ
ڐϜȂα І *α ࣐ܝԓॹ኶ȂѬԥЎ኶ϛ࣐ႮȂՅϛ࣐Ⴎޠα І *α ܛᄈᔗޠၦਠᘉ
֊ᆏ࣐МනӪ໕(Support vectors)ȇ ),( ji xxK ࣐ਰЗړ኶(Kernel function)Ȃңٿ஡ᒰΤ
ޫ ໣ ޠ ၦ ਠ ᙾ ඳ Ԛ ੬ ኊ ޫ ໣ ޠ ၦ ਠ Ȃ ڐ ๊ ܼ ڎ ঐ Ӫ ໕ ޠ ϲ ᑗ Ȃ
)()(),( jiji xxxxK Φ×Φ= Ȃй҇໹ሰᅗ٘ Mercer’s ఩ӈ(Vapnik 2000)ȄளңޠਰЗړ
኶ԥӼ໷ԓ(Polynomial)ړ኶Іᒮৣஆۼړ኶(Radial basis function, RBF)(Lin et al.
2003; Cherkassky & Ma 2004)Ȅ
ଌጜΚঐ SVR ዂԓ๊Ӥܼ਴ᐄϵԓ(7)ޠ४ښԓίȂഷٺϾܝԓॹ኶α І *α п׳
яᎍ࿌ޠМනӪ໕ঐ኶ٿ࡛ҴዂԓȂҭࠊϑԥ೩ӼޠРݳೞණяٿ೏౪ԓ(7)ޠഷٺϾ
ୱᚡ(Platt 1999; Joachim1999; Vapnik 2000; Collobert & Bengio 2001; Trafalis & Ince 
2002)ȂڐϜ SMOᅌᆘݳ(Sequential Minimum Optimization)࢑ഷளңޠ׭೛ϟΚȂၐ
ಡޠၦଊ፝୥Մ Platt(1999)І Vapnik(2000)Ȅ
နȃ๗Ӭ ICAᇅ SVRႲขዂԓϟ࡛ᄻ
ҐःفණяΚঐ๗ӬᑀҴԚӌϸݚᇅМනӪ໕ଠᘫϟଓଡ଼ਣ໣זӗႲขዂԓȄԫ
Ⴒขዂԓϟ࡛ᄻѠпϸԚւң ICA ᄈႲขᡑ኶џᚖଊޠࠊ೏౪ȂІп SVR ٻңџଷ
ᚖଊࡤޠႲขᡑ኶࡛ᄻႲขዂԓ๊ڎഌϸȄॷӒӶ ICAഌϸȂӶ஡ Mঐᆱ࡚࣐ N×1 ޠ
Ⴒขᡑ኶ ix ಣԚెӬઑଳࡤȂւң ICA ዂԓ௄ઑଳ XϜզॏя M ঐᆱ࡚࣐ N×1 ޠᑀ
ҴԚӌ( iy )Іᆱ࡚࣐ MM × ޠ၍ెӬઑଳWȄ௦຀Ȃ࣐׳яхߓᚖଊޠᑀҴԚӌ(IC)Ȃ
Ґःفւң Cheung & Xu (2001)ܛණяޠขၑ௦ڨݳ(Testing-and-Acceptance, TnA)ᄈ
ICӶᗎ༗੬ኊᘞڦϟߓ౫ໍ՘ຠզȄTnAݳ࢑ஆܼၦਠᗚ঩(Data reconstruction)ޠ௷
זРݳ(ICAϜᗚ঩ޠϵԓԄί८ԓ(8)ܛұ)Ȃٻң Relative hamming distance(RHD)ᗚ
঩ᇳৰঅ࣐ࡿዀȂᒌ໕ᗚ঩ࡤޠזӗၦਠᇅ঩ۗזӗ໣ޠᗎ༗ࣻծโ࡚ȄRHDঅ຺ϊ
хߓڎঐזӗၦਠϟᗎ༗຺ࣻծȂ࿌ᗎ༗׈ӓΚयਣڐঅ࣐ 0Ȃ׈ӓࣻЇࠍ࣐ 4ȞRHD
ϟϵԓ፝୥Ꭸ Cheung & Xu 2001)Ȅ
ٻң TnA ݳ௷זޠؐ᡾࣐ӒॏᆘؑΚঐᑀҴԚӌөՍໍ՘ᗚ঩ࡤᇅ঩ۗၦਠϟ
໣ޠ RHDঅȂԥഷϊ RHDঅޠᑀҴԚӌ࢑ഷ૗஋਑ਕژ঩ۗזӗၦਠϜл्ᗎ༗๗
172 ၦଊᆔ౪Ᏹൣ! ಒΫѳڣ! ಒѳ෉
1 151 301 451 601 751
ᇷறរ
ᗑ
م
ګ
ٝ
ΰ
IC
α
შ 3Ȉშ 2ϟଓଡ଼ਣ໣זӗଊဵޠ 4ঐᑀҴԚӌ
Ӷᗚ঩ޠႇโϜȂ२्ޠ࢑ᒶᐆᎍ࿌ޠᑀҴԚӌঐ኶ٿхߓႲขᡑ኶ၦਠϜޠл
्ᗎ༗๗ᄻȂпႁژഷٺޠᚖଊџଷਞݏȂӱᒶᐆႇӼᑀҴԚӌཽٻூᚖଊޠᘯଷਞ
ݏϛٺȂЇϟᒶᐆЋЎࠍѠ૗ཽോѷႲขᡑ኶ޠᗎ༗๗ᄻȄҐःف਴ᐄٸזᗚ঩௷ז
ಒΚޠᑀҴԚӌȃࠊڎঐᑀҴԚӌІܛԥᑀҴԚӌޠ RHD অϟᡑϾȂᒶᐆᎍ࿌ޠᑀ
ҴԚӌঐ኶ȄҦܼᗚ঩ޠᑀҴԚӌ຺ӼȂᗚ঩ޠଊဵཽᇅ঩ۗଊဵ຺ࣻծȂܛпԫΚ
RHD অޠᡑϾ࢑஡࢑Ҧτژϊޠ௷זȄՅᒶᐆޠྦࠍ࣐स RHD অӶ࢛ঐᑀҴԚӌђ
Τᗚ঩ಣӬࡤϟί७ൾ࡚ϛτȂߓұԫΚᑀҴԚӌє֥ၷЎޠᗎ༗੬ኊၦଊȂђΤᗚ
঩ࡤٯณݳٻᗚ঩ࡤଊဵ؂ߗծ঩ۗଊဵȂܛпԫΚІ௷זӶڐϟࡤޠᑀҴԚӌ஡ೞ
ңٿхߓ঩ۗၦਠϜޠᚖଊȄЇϟȂस RHD অӶ࢛ঐᑀҴԚӌђΤᗚ঩ಣӬࡤԥᡘ
຀ޠί७ȂࠍԫΚᑀҴԚӌхߓႲขᡑ኶Ϝޠл्ᗎ༗๗ᄻȄଷԫϟѵȂसԥM ঐᑀ
ҴԚӌȂࠍഷӼᒶᐆ 1−M ঐᑀҴԚӌхߓႲขᡑ኶Ϝޠл्ᗎ༗๗ᄻȂܗ࢑ഷЎཽᒶ
ᐆΚঐᑀҴԚӌ(Ҽ֊௷זഷࡤᑀҴԚӌ)хߓႲขᡑ኶ϜޠᚖଊȂӱ࣐ଓଡ଼ਣ໣זӗ
ၦਠԇӶᚖଊ(Deboeck 1994; Yaser & Atiya 1996)Ȅ
ߓ 1Ȉขၑ௦ڨݳ௷זႇโޠ RHDᗚ঩ᇳৰঅ
ЩၷԪ኶ ᗚ঩ϟᑀҴԚӌȞICȟ RHDᗚ঩ᇳৰঅ
IC1 1.7531 
IC2 1.8528 
IC3 1.9779 
ಒ 1Ԫ
IC4 1.9044 
IC1ȃIC2 1.7137 
IC1ȃIC3 1.3346 ಒ 2Ԫ
IC1ȃIC4 1.7373 
IC1ȃIC3ȃIC2 0.7662 ಒ 3Ԫ
IC1ȃIC3ȃIC4 1.4294 
ಒ 4Ԫ IC1ȃIC3ȃIC2ȃIC4 0 
IC1( 1y )
IC2( 2y )
IC3( 3y )
IC4( 4y )
174 ၦଊᆔ౪Ᏹൣ! ಒΫѳڣ! ಒѳ෉
Ӷւң ICA ዂԓூژᚖଊᘯଷϟႲขᡑ኶ࡤȂ௦຀ SVR ஡пࠊख़ؐ᡾ܛூϟᡑ
኶࡛ҴႲขዂԓȄӶٻң SVR ਣॷӒሰໍ՘ਰЗړ኶ޠᒶᐆȂҦܼ RBFړ኶࢑ SVR
ϜளңޠਰЗړ኶ϟΚ(Lin et al. 2003; Cherkassky & Ma 2004)ȂҐःف஡ٻң RBFړ
኶࣐ SVR ϜޠਰЗړ኶Ȅ௦຀ȂҦܼ SVR ޠႲขࠣ፵л्ڨڐዂԓϜޠ።ᐍள኶ C
І཭ѷړ኶ İޠኈ៫(Lin et al. 2003; Cherkassky & Ma 2004)Ȃӱԫٻң SVRਣഷл्
ޠឋᚡ࢑Ԅեᒶᐆᎍ࿌ޠ።ᐍள኶ CІ཭ѷړ኶ İ ޠঅпூژًԂޠႲข๗ݏȄ
Ӷ።ᐍள኶ CІ཭ѷړ኶ İޠ؛ۢαȂҭࠊٯณΚૢܓޠРݳѠпٻңȂ೾ளཽ
ٸୱᚡޠ੬ܓޣ௦ցᘟܗٻңၑᇳݳ؛ۢഷᎍޠ୥኶অȄҐःف୥Մ Lin et al.(2003)
ܛණяޠᆪੀཫ൷ݳ(Grid search)؛ۢ୥኶ CІ İޠ኶অȂڐ࡛ឋӶ SVRޠ୥኶ᒶᐆ
ႇโϜȂٻңࡿ኶Ԛߞޠזӗཫ൷૗ூژԂޠ୥኶๗ݏȂ(پԄȂ 15135 2,...,2,2,2 −−−=C )Ȅ
կҦܼᆪੀཫ൷ݳ࡛ឋޠၑᇳԪ኶ႇӼٯйءԥጓ൝Ȃ࣐Π෶Ў؛ۢ୥኶ޠཫ൷ਣ໣
ٯூژၷٺޠ๗ݏȂҐःف஡Ӓւң Cherkassky & Ma(2004)ܛ࡛ឋϟРݳூژ SVR
ϟ୥኶ C І İޠ࡛ឋঅ(֊୥኶ಣӬ)ࡤȂӕ਴ᐄԫ࡛ឋঅ୉ᎍ࿌Ԫ኶ޠࡿ኶זӗཫ൷Ȃ
ٯп૗౱ҢഷϊְРᇳৰ(Mean Square Error, MSE)ϟႲข๗ݏޠ୥኶ಣӬ࣐ഷᎍޠಣ
ӬȄԫРݳ஡Ѡџଷ൑Κ୥኶ಣӬ࡛ឋঅޠॴᓏȂٯ७մၑᇳݳॏᆘԪ኶ႇӼޠુᘉȄ
Ӄȃᄃᜍ๗ݏ
ΚȃၦਠԞ໲Іᕽਞᒌ໕ྦࠍ
ӶᄃᜍഌϸȂ࣐ᡜᜍҐःفܛණϟ๗Ӭ ICA ᇅ SVR ϟڎ໧ࢳႲขዂԓ(ᙐᆏڎ໧
ࢳዂԓ)ϟႲข૗ΩȂ஡пСစ 225ࡿ኶࣐ःفዀޠȂᄈСစ 225ϟ౫ೳ໡ዻࡿ኶ໍ՘
ႲขȄٯйڎ໧ࢳႲขዂԓϟႲข๗ݏ஡ᇅ൑હٻң SVRዂԓІᓎᐡᅟؐРݳޠႲข
๗ݏໍ՘ЩၷȂпᡜᜍܛණРݳޠԥਞܓȂڐϜҐःفܛٻңޠᓎᐡᅟؐዂԓ࣐ւң
ዀޠޑࠊΚСቌੀ(t-1)ȂႲข࿌Сޠዀޠޑቌੀ(t)ȄՅӶڎ໧ࢳዂԓІ SVRዂԓޠႲ
ขᡑ኶Р८ȂҦܼ෉ೳҀൠЇ࢏Ҁൠၦଊޠഁ࡚ၷ౫ೳ࣐ץȂٯйҀൠޠԞዻԚһቌ
ཽ౦ӒЇ࢏яዻࡤଊਁ(Lee & Chen 2002; Lee & Chiu 2002)ȂӱԫҐःفᒶᐆήঐпС
စ 225ࡿ኶࣐ዀޠ෉ೳࡿ኶Ȃϸր࣐τ٭ȃུђۄڸ߁ђ঱୧ࠣһܿܛϟСစ 225෉
ೳȂІࠊΚС౫ೳԞዻࡿ኶࣐ 4ঐႲขᡑ኶Ȅ
ӶᄃᜍၦਠഌϸȂᇕ໲ޠၦਠ෉໣࣐ 1999Ԓ 10У 4СՎ 2004Ԓ 9У 30СӔϥ
Ԓ 1144ঐһܿСȂᄃᜍၦਠϟٗ༗შԄშ 6ܛұȄᄃᜍၦਠϜ௄ 1999Ԓ 10У 4СՎ
2003Ԓ 3У 27СӔ 794์ၦਠ஡ձ࣐ଌጜ኶ᐄ໲Ȃѫߴ੽ 2003Ԓ 3У 28СՎ 2004
Ԓ 9У 30СӔ 350์ၦਠձ࣐ขၑၦਠ໲ȄӶႲขᕽਞޠᒌ໕ྦࠍР८ȂҐःفпְ
Р਴ᇳৰ(Root Mean Square Error, RMSE)ȃዀྦϾ҂ְ҂Рᇳৰ(Normalized mean 
square error, NMSE)ȃ҂ְ๙ᄈᚕৰ(Mean Absolute Difference, MAD)ȃРӪᄈᆏܓ
(Directional Symmetry, DS)ȃҔጃαᅜᗎ༗౦(Correct Up trend, CP)ІҔጃίໂᗎ༗౦
176 ၦଊᆔ౪Ᏹൣ! ಒΫѳڣ! ಒѳ෉
Ρȃᄃᡜ๗ݏ
ॷӒӶ൑હٻң SVRዂԓР८Ȃשউٻң Chang & Lin (2001)ܛึ৥ޠ LIBSVM
࡛ҴႲขዂԓȂႲขዀޠޑޠ኶অ(֊Сစ 225 ໡ዻቌ)஡ӒစႇА࡚Ͼ(Scaling)ޠ೏
౪ȄՅӶ SVRޠ୥኶ Cޠ İ೪ۢαȂւң Cherkassky & Ma (2004)ܛ࡛ឋϟРݳூژ
0019.0=ε ȂC=1.25ȂҦܼ ε௦ߗ 92− ȂC௦ߗ 12 Ȃӱԫשউ஡п( )2,2 19 == − Cε ޠ୥኶
ಣӬ࣐କᘉໍ՘ᆪੀཫ൷Ȃпၑᇳяഷٺޠ୥኶ಣӬ౱ҢഷԂޠႲข๗ݏȄߓ 3࣐൑
હп SVRϏڏӶϛӤ୥኶ಣӬίޠଌጜІขၑ๗ݏȄҦߓ 3ϜѠޤ࿌ 19 2,2 == − Cε ਣ
ԥഷϊޠขၑၦਠ MSEঅȂ࢑൑હ SVRዂԓޠഷٺ୥኶ಣӬȄ
ՅӶڎ໧ࢳႲขዂԓР८ȂҦܼܛٻңޠ 4ঐႲขᡑ኶ϟၦਠ֊࣐შ 3 Ϝޠ 4 ঐ
ଓଡ଼ਣ໣זӗଊဵȂӱԫւң ICAዂԓᘯଷႲขᡑ኶Ϝᚖଊ๊Ӥܼᘯଷშ 3Ϝଓଡ଼ਣ
໣זӗଊဵޠᚖଊȂڐႇโᇅ๗ݏϑᇴܼ݃ಒή࿾ϜȂӶԫ൸ϛᙶख़ȄՅӶ SVRႲข
ዂԓޠ࡛ҴαȂ( )2,2 19 == − Cε ޠ୥኶ಣӬϬ࢑ᆪੀཫ൷ޠକۗᘉȄߓ 4 ࣐ڎ໧ࢳႲ
ขዂԓӶϛӤ୥኶ಣӬίޠႲข๗ݏȄҦߓ 4 ϜѠпᢏᄇژȂ 37 2,2 == − Cε ԥഷϊޠ
ขၑၦਠ MSEঅȂ࢑ڎ໧ࢳႲขዂԓޠഷٺ୥኶ಣӬȄ
ߓ 5࣐ᓎᐡᅟؐȃ൑હٻң SVRІڎ໧ࢳዂԓήᆎႲขዂԓޠႲข๗ݏཋᖃߓȄ
Ҧߓ 5 ѠޤȂܛණڎ໧ࢳዂԓޠ RMSEȃNMSE І MAD অϸր࣐ 56.76ȃ0.0026 І
40.86Ȃְϊܼ SVR ዂԓІᓎᐡᅟؐዂԓȂхߓڎ໧ࢳዂԓܛႲขޠ኶অᇅᄃርঅഷ
௦ߗȂႲขᇳৰഷϊȄଷԫϟѵȂࣻᄈ SVRዂԓІᓎᐡᅟؐዂԓȂڎ໧ࢳዂԓҼԥഷ
ାޠ DSȃCD І CP অȂϸր࣐ 87.53%ȃ88.77%І 86.09%ȂхߓڐӶ޷ቌᗎ༗ޠႲ
ขαҼၷڐуڎঐРݳྦጃȄՅ࣐ΠໍΚؐ၍មႲขዂ࠯໣ޠৰ౵Ȃშ 6࣐ 3ঐႲข
ዂ࠯ӶขၑኻҐϜޠഷࡤ 50์ϟႲขঅᇅᄃርঅЩၷშȄҦშ 6ϜѠпᢏᄇژȂڎ໧
ࢳዂԓᇅ൑હٻң SVRዂԓӶᗎ༗ޠ਑ਕαְၷᓎᐡᅟ࣐ؐٺȂկڎ໧ࢳዂԓӶΚٳ
ᙾ׸ᘉޠᆡጃ࡚έЩ SVRዂԓ࣐ٺ(ԄኻҐᘉ 307, 322ᇅ 349)Ȃй SVRዂԓࣻၷՅّ
ܿାզႲขঅȄറພᇴ݃ޠ࢑ᗷดშ 6Ѭ࢑ขၑኻҐϜޠ 50์ၦਠȂկңᐍঐขၑၦ
ਠ໲ٿᢏᄇϬѠᕖூࣻӤޠ๗፤ȄᐍᡞՅّȂҐःفܛණϟ๗Ӭ ICA ᇅ SVR ϟڎ໧
ࢳႲขዂԓȂࣻၷܼ SVRዂԓІᓎᐡᅟؐዂԓȂ૗ණٽഷϊޠႲขᇳৰпІഷାޠᗎ
༗Ⴒขྦጃ౦Ȅ
178 ၦଊᆔ౪Ᏹൣ! ಒΫѳڣ! ಒѳ෉
10700
10800
10900
11000
11100
11200
11300
11400
11500
301 308 315 322 329 336 343 350
ᇷறរ
Ꮭ
௑
Actual values
Random walk
SVR
ICA+SVR
შ 6Ȉ3ঐႲขዂԓϟႲขঅᇅᄃርঅЩၷშ-пขၑኻҐϜޠഷࡤ 50์࣐پ
࣐ໍΚ֖ؐ౫ܛණڎ໧ࢳႲขዂԓϟႲขਞݏȂҐःفѫпѯᢋђ᠍޷ቌࡿ኶
(Taiwan stock exchange capitalization weighted stock index, TAIEX)࣐ዀޠȂᄈ TAIEXϟ
౫ೳԞዻቌໍ՘ႲขȄ௵ңޠၦਠ෉໣࣐ 2003Ԓ 1У 2СՎ 2006Ԓ 2У 27СӔѳԒ
781ঐһܿСȂڐϜӶଌጜኻҐޠഌϸ࣐ 2003Ԓ 1У 2 СՎ 2005Ԓ 3У 17СӔ 546
์ၦਠȞխၦਠ෉໣ 69.9%ȟȂѫߴ੽ 2005Ԓ 3У 18СՎ 2006Ԓ 2У 27СӔ 235์
ၦਠ(խၦਠ෉໣ 30.1%)ձ࣐ขၑኻҐȂშ 7 ࣐ၦਠ෉໣ޠ TAIEX Ԟዻࡿ኶ٗ༗შȄ
ӶႲขᡑ኶ഌϸȂҦܼ׭೛ࡿዀІп၏౫ೳҀൠ࣐ዀޠϟ෉ೳࡿ኶࢑޷ቌႲขਣளң
ޠႲขᡑ኶(Balachandher et al. 2002; Leigh et al. 2005)ȂӱԫҐःفٻң౫ೳࡿ኶ϟࠊ
ΚСഷାቌȃࠊΚСഷմቌȃࠊΚСԚһঅȃࠊΚСԚһ໕ȃ౫ೳ࿌С໡ዻቌȃϳС
ࣻᄈ஽৶ࡿዀ(6 days-relative strength indicator, RSI6)ȃ10Сࡿ኶ᘉԚһঅࡿዀ(Total 
amount weight stock price Index, TAPI)ȃུђۄһܿܛኟѯࡿ໡ዻ෉ೳࡿ኶(SGX-DT 
MSCI Taiwan opening index futures)пІѯᢋ෉ೳһܿܛѯࡿ෉໡ዻ෉ೳࡿ኶(TAIFEX 
TAIEX Taiwan opening index futures)๊ 9ঐႲขᡑ኶࡛ᄻ TAIEXԞዻቌϟႲขዂ࠯Ȅ
ߓ 6࣐๗Ӭ ICAᇅ SVRዂԓȃ൑હٻң SVRᇅᓎᐡᅟؐዂԓܼ TAIEXԞዻቌޠ
Ⴒข๗ݏȄҦߓ 6ϜөຠզࡿዀϟঅѠпึ౫Ȃϛ፤Ӷᚕৰᒌ໕ࡿዀܗᗎ༗Ҕጃ౦ࡿ
ዀР८Ȃܛණ๗Ӭዂԓϟ๗ݏְၷ൑હٻң SVRᇅᓎᐡᅟؐዂԓ࣐ٺȂԥၷմޠႲข
ᇳৰІၷାޠႲขྦጃ࡚Ȅ
180 ၦଊᆔ౪Ᏹൣ! ಒΫѳڣ! ಒѳ෉
यᗃ
Ґःفഌϸܜ՘࢈ଲ୾ऌཽȞॏฬጢဵȈNSC 95-2221-E-030-002ȟစຳ᜔ֆȂ੬
ԫयᗃȄ
ࢯȃ୥ՄНᝧ
1.  Antoniou, A. and Holmes, P. “Futures Trading, Information and Spot Price Volatility: 
Evidence for the FTSE-100 Stock Index Futures Contract Using GARCH,” Journal of 
Banking and Finance (19) 1995, pp: 117-129. 
2.  Back, A. and Weigend, A. “Discovering Structure in Finance Using Independent 
Component Analysis,” Proceeding of 5th International Conference on Neural 
Networks in Capital Market 1997, pp: 15-17. 
3.  Balachandher, K.G., Fauzias, M.N. and Lai, M.M. “An Examination of the Random 
Walk Model and Technical Trading Rules in the Malaysian Stock Market,” Quarterly 
Journal of Business & Economics (41) 2002, pp: 81-104. 
4.  Barlett, M. and Sejnowski, T.J. “Viewpoint Invariant Face Recognition Using 
Independent Component Analysis and Attractor Networks,” Advances in Neural 
Information Processing Systems (9) 1997, pp: 817-823. 
5.  Bartlett, M.S., Movellan, J.R. and Sejnowski, T.J. “Face Recognition By Independent 
Component Analysis,” IEEE Transactions on Neural Networks (13) 2002, pp: 
1450–1464. 
6.  Beckmann, C.F. and Smith, S.M. “Probabilistic Independent Component Analysis for 
Functional Magnetic Resonance Imaging,” IEEE Transactions on Medical Imaging
(23) 2004, pp: 137-152. 
7.  Burbidge, R., Trotter, M., Buxton, B. and Holden, S. “Drug Design by Machines 
Learning: Support Vector Machines for Pharmaceutical Data Analysis,” Computer & 
Chemistry (26) 2001, pp: 5-14. 
8.  Cao, L.J. “Support Vector Machines Experts for Time Series Forecasting,” 
Neurocomputing (51) 2003, pp: 321-339. 
9.  Cao, L. and Tay, F.E.H. “Financial Forecasting Using Support Vector Machines,” 
Neural Computing & Applications (10) 2001, pp: 184-192. 
10.  Chang, C.C. and Lin, C.J. “LIBSVM: A Library for Support Vector Machines,” 2001, 
Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.
11.  Chang, R.F., Wu, W.J., Moon, W.K. and Chen, D.R. “Improvement in Breast Tumor 
Discrimination by Support Vector Machines and Speckle-Emphasis Texture 
Analysis,” Ultrasound in Medicine and Biology (29) 2003, pp: 679-686. 
12.  Cheung, Y.M. and Xu, L. “Independent Component Ordering in ICA Time Series 
Analysis,” Neurocomputing (41) 2001, pp: 145-152. 
13.  Chuang, C.C., Su, S.F., Jeng, J.T. and Hsiao, C.C. “Robust Support Vector 
Regression Networks for Function Approximation with Outliers,” IEEE Transactions 
on Neural Networks (13) 2002, pp: 1322–1330. 
14.  Cichocki A. and Amari, S.I. Adaptive Blind Signal and Image Processing: Learning 
Algorithms and Applications, John Wiley & Sons, New York, 2002. 
182 ၦଊᆔ౪Ᏹൣ! ಒΫѳڣ! ಒѳ෉
35.  Kim, T.K., Kim, H., Hwang, W. and Kittler, J. “Independent Component Analysis in A 
Local Facial Residue Space for Face Recognition,” Pattern Recognition (37) 2004, pp: 
1873-1885. 
36.  Kiviluoto, K. and Oja, E. “Independent Component Analysis for Parallel Financial 
Time Series,” Proceedings of the Fifth International Conference on Neural 
Information, Tokyo, Japan, 1998, pp: 895–898. 
37.  Koike, A. and Takagi, T. “Prediction of Protein-Protein Interaction Sites Using 
Support Vector Machines,” Protein Engineering Design & Selection (17) 2004, pp: 
165-173. 
38.  Kwon, K.Y. and Kish, J.R. “Technical Trading Strategies and Return Predictability: 
NYSE,” Applied Financial Economics (12) 2002, pp: 639-653. 
39.  Li, S., Kwok, J.T., Zhu, H. and Wang, Y. “Texture Classification Using The Support 
Vector Machines,” Pattern Recognition (36) 2003, pp: 2883-2893. 
40.  Lin, Q.H., Zheng, Y.R., Yin F. and Liang, H.L. “Speech Segregation Using 
Constrained ICA,” Lecture Notes in Computer Science (3173) 2004, pp: 755-760. 
41.  Lin C.J., Hsu, C.W. and Chang, C.C. “A Practical Guide to Support Vector 
Classification,” Technical Report, Department of Computer Science and Information 
Engineering, National Taiwan University, 2003. 
42.  Lee, T.W. Independent Component Analysis: Theory and Application, Kluwer 
Academic Publishers, Boston, 1998. 
43.  Lee, T.S. and Chen, N.J. “Investigating the Information Content of Non-Cash-Trading 
Index Futures Using Neural Networks,” Expert Systems with Applications (22) 2002, 
pp: 225-234. 
44.  Lee, T.S. and Chiu, C.C. “Neural Network Forecasting of An Opening Cash Price 
Index,” International Journal of Systems Science (33) 2002, pp: 229-237. 
45.  Lee, T.S., Chen, N.J. and Chiu, C.C. “Forecasting the Opening Cash Price Index 
Using Gray Forecasting and Neural Networks: Evidence from the SGX-DT MSCI 
Taiwan Index Futures Contracts,” Computational Intelligence in Economics and 
Finance (Wang, P and Chen, S. S., Eds), Springer, 2003. 
46.  Mohandes, M.A., Halawani, T.O., Rehmam, S. and Hussain, A.A. “Support Vector 
Machines for Wind Speed Prediction,” Renewable Energy (29) 2004, pp: 939-947. 
47.  Norinder, U. “Support Vector Machine Models in Drug Design: Applications to 
Transport Processes and QSAR Using Simplex Optimisations and Variable Selection,” 
Neurocomputing (55) 2003, pp: 337-346. 
48.  Oja, E., Kiviluoto, K. and Malaroiu, S. “Independent Component Analysis for 
Financial Time Series,” Proceedings of the IEEE 2000 Adaptive Systems for Signal 
Processing, Communications, and Control Symposium, Lake Louise, Canada, 2000, 
pp: 111-116.  
49.  Pai, P.F. and Lin, C.S. “Using Support Vector Machines in Forecasting Production 
Values of Machinery Industry in Taiwan,” International Journal of Advanced 
Manufacturing Technology (27) 2005, pp: 205-210. 
50.  Parisi, F. and Vasquez, A. “Simple Technical Trading Rules of Stock Returns: 
Evidence from 1987 to 1998 in Chile,” Emerging Markets Review (1) 2000, pp: 
152-164. 
51.  Park, H.M., Jung, H.Y., Lee, T.W. and Lee, S.Y. “On Subband-based Blind Signal 
Separation for Noisy Speech Recognition,” Electronic Letters (35) 1999, pp: 
2011-2012. 
184 ၦଊᆔ౪Ᏹൣ! ಒΫѳڣ! ಒѳ෉
 2
國際商業與經濟國際學術研討會較一般研討會特別之處在於，亦安排個案
之論文發表，讓與會專家學者有機會接觸第一手個案之機會。此外，IBEC 亦頒
贈最佳論文獎，對傑出之論文給予肯定。主辦單位除用心安排專題演講、論文發
表之外，亦於會場安排出版社舉辦書展，本人意外發現一本相當適合所任教課程
“顧客關係管理及資料探勘”及與本人研究領域相關之教科書及相關之財金資
料，此部份可以作為日後舉辦學術研討會之參考。此外，本人亦受邀擔任該研討
會明年審查之 reviewer工作，對提高輔仁大學及台灣之能見度應有正面及積極的
幫助，則是意外的收獲。 
整體而言，這是一個相當成功的研討會，超過六年之舉辦歷史與超過幾百
名來自於全球各地的學者參與，更顯示其重要性。感謝國科會提供往返機票以及
輔仁大學提供生活費及註冊費的經費補助，使得未學在研究工作上能不擔心經費
部分的負擔，而能將全部心力將研究工作做的更好，最後期待明年的研討會再次
參與！ 
二、與會心得 
從在美國唸奧斯汀德州大學作業研究與工業工程博士班一年級時，參加第
一次國際學術研討會至今，已經不記得曾參加過多少國內外的學術研討會。但很
少有如這次研討會的經驗，同時遇到就讀大學時社團之同學，及博士班時之同
窗。他們非常關心本人目前之狀況，也關心目前台灣的教育及國際情勢。並告之
自從我畢業，同學們分道揚鑣後，他們因為我來自台灣的關係，就十分關心台灣
的一舉一動。趁此難得的機會，他們也詢問了許多台灣目前的現況，特別是大學
國際化、國內經濟的發展、及與中國大陸的微妙關係等等。很訝異雖然只是大學
及研究所的同窗，但於分別十數年之後，他們仍然非常關心本人及台灣。由此可
知至國外留學，除了吸收新知、拓展國際視野、增進台灣之國際能見度外，還有
促進國外高級知識份子對台灣的瞭解及支持台灣的加分效果。此外，任職瑞士銀
行舊金山分行之張敏郁投資副總裁，慷慨允諾將協助輔仁大學管理學院未來到瑞
士銀行舊金山分行進行國際學術參訪，分別數十年之老同學，能於管理教育國際
化之工作伸出援手，更為行前意想不到之收穫，也確定了參加這次研討會的大方
向是正確的。 
除了之前提到的收穫之外，本人與張敏郁投資副總裁皆對目前出國留學人
數大幅降低，特別是到美國一流學府進修人數持續下跌之現象感到十分憂心，特
別是相較於中國大陸、南韓及印度赴美留學人數持續成長，這個問題顯得更加嚴
重。教育主管機關雖也有注意到此現象，並也採取一些作法，試圖刺激國人出國
進修，但成效似乎仍不顯著，也仍然有改進之空間。本人深刻認為此關係到台灣
未來之國際競爭力及能見度，教育主管機關應有更積極的作法，鼓勵出國(特別
是一流之學府)進修，以繼續維持台灣在高等教育、人力資源及國際能見度方面
之競爭力。 
