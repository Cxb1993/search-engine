 2
中文摘要 
 
    傳輸控制協定 TCP（Transmission Control Protocol）是目前網際網路上最重要
的壅塞控制方法，相當高比例的網際網路訊務是透過 TCP 在傳遞，因此 TCP 的傳輸效
能直接影響網際網路的整體使用效率。 
目前網際網路上廣被採用的 TCP 版本 Reno 是以 AIMD (Additive Increase and 
Multiplicative Decrease)演算法調整其壅塞窗口。AIMD 演算法在高頻寬網路環境下，其
壅塞窗口增加的速度太慢，對封包遺失事件（收到三個 dup-ACK 或是發生 Timeout）所
減少的大量壅塞窗口更要花許多時間才能回復（recovery），這種行為不適合逐漸趨於
寬頻化的網際網路、因其壅塞控制機制本身將成為網路效能上的瓶頸。 
就解決上述議題而言，直觀的研究朝向改良 TCP 壅塞控制機制著手，其目的在加
快壅塞窗口增加以及回復的速度；另一種解決策略為平行化 TCP（parallel TCP），平行
化 TCP 以同時開啟多條 TCP 連線進行傳輸的方式著手。相較於一條 TCP 連線，多條
TCP 連線可以以數倍的速度增加其壅塞窗口，其對於封包遺失之後的回復速度也較迅
速，因此平行化 TCP 對於高頻寬網路的資源使用率較佳，進而可提升整體的網路效能。 
平行化 TCP 在高頻寬網路上的傳輸效率較佳已眾所周知，目前已有越來越多網路
傳輸應用軟體將平行化 TCP 實作其上，但我們認為在固定的網路資源情況下，若是每
個網路使用者無限制的開啟大量 TCP 連線進行傳輸，屆時必會造成網路極度壅塞、大
量封包遺失，整體網路效能低落甚至是網路癱瘓的嚴重狀況，這是利用平行化 TCP 進
行傳輸所必須重視的急迫議題。因此我們認為，在有限的網路資源前提下，對於平行化
TCP 的產能（throughput）以及連線數量之間的關聯性有進行詳細研究的必要。 
本計畫即著眼於此，期望經由平行化 TCP 產能數學模式的探討，推導出平行化 TCP
基於特定網路資源參數下之最大平行度公式，並希望藉由連線路徑上網路設備器的協
助，取得必要的網路參數，以便在實際的傳輸行為中換算並開啟適當數目的 TCP 連線，
適切且充分的善用網路資源。我們將從兩個不同方向切入研究以求達到目標，首先在第
一年的研究裡，我們假設網路資源參數是可取得的，例如：端點對端點間訊號來回的傳
遞時間、網路瓶頸點可用的頻寬、緩衝區大小，配合各連線壅塞窗口值，推得平行化
TCP 之產能數學公式，並以逼近法求得最佳產能下的最適連線數量 M 值（最大平行度），
並進一步調校平行化 TCP 的壅塞控制方法，使其已存在的 N 條連線傳輸總和等同於 M
條連線的傳輸行為；在第二年的研究中我們將研究並設計機制，在考慮可行性的前提
下，透過連線路徑上路由器的協助，由網路端點取得平行化 TCP 最大平行度公式所需
的網路參數，讓平行化 TCP 的最大平行度計算成為可能。 
 
關鍵詞：平行化 TCP、壅塞控制、網際網路、傳輸層協定 
 
 
 
一、計畫緣由及目的 
 
傳輸控制協定 TCP (Transmission Control Protocol) [1]是現今網際網路上最重要的傳
輸層協定之一，它提供了網際網路端對端(end-to-end)間一個可以自我調適速度的可靠性
傳輸，避免網路資源因過度被使用而造成的崩潰。網路應用層中被廣泛利用的協定例
如：TELNET、FTP、SMTP、HTTP…，皆是以 TCP 作為其下層的傳輸協定(如圖 1.1)，
TCP 的重要性以及其效能改善對於網際網路傳輸的影響甚鉅由此可見。 
 
圖 1.1：TCP connection in the Internet protocol stack. 
網際網路發展至今早已和當年 TCP 草創之初截然不同，網際網路的巨大成功帶動
了各種網路技術的快速發展。過去，網路頻寬侷限於實體線路技術與成本，一般終端使
用者所能應用的頻寬十分狹小，但明顯的網路已有逐漸寬頻化的趨勢。面對日益普及的
高頻寬網路環境以及使用者的多元需求，許多原本適用於網際網路的技術面臨已不再適
任的瓶頸，TCP 就是極需提升的技術之一。自 1981 年 TCP 被正式定義在 IETF RFC793
以來，許多 TCP 版本相繼被提出，試圖改良 TCP 的壅塞控制技術，以因應不同底層網
路技術的發展及限制，維護整體網路的效能。其中，如何在高頻寬網路下突破傳統 TCP
所帶來的自我產能限制，是近年來隨著網路頻寬大幅增加後所面臨到的問題。 
傳統 TCP 會受限於自己本身的壅塞控制(Congestion Control)機制，即便在無人競爭
的高速網路環境中，其頻寬利用度亦不高[2,3]。目前被廣泛使用的 TCP 版本 Reno，是
一種以封包遺失為基礎(loss-based)的網路壅塞控制技術，其壅塞控制機制，是以 AIMD 
(Additive Increase and Multiplicative Decrease)演算法來調整其壅塞窗口 (congestion 
window)，作為傳輸速率控制機制。在 AIMD 機制下 TCP Reno 線性增加其壅塞窗口值
直至察覺發生封包遺失，在成功重傳遺失的封包後就立即將壅塞窗口值調整為原本的二
分之ㄧ，如圖 1.2 所示。 
 4
 圖 1.4：高速網路環境下改善 TCP 傳輸效能的兩個方向。 
第 一 類 研 究  (HS-TCP[5], Scalable TCP [6], CUBIC [7], FAST TCP [8], 
Westwood+[9]…)著眼於 TCP 壅塞控制機制本身，企圖突破單條 TCP 連線(connection)
在高速網路下產量(throughput)的自我限制，有效地傳輸資料。相關研究分述如下： 
HS-TCP（HighSpeed TCP）[5]調整傳統 TCP 的壅塞控制機制，讓壅塞窗口上升的速度
快過線性成長，下降的速度慢過指數衰減，藉以讓傳送速度盡量維持在高速；
Scalable TCP [6]是 HS-TCP 的特例，它將壅塞窗口變化的演算法侷限於適合在專線網路
下傳送大量資料；CUBIC [7]則是以一個三次方的時間變化係數來加快壅塞窗口上升的
速度，時間變化係數會去監控上次壅塞窗口減小的時間點到現在要傳送的時間點的差
距；FAST TCP [8]利用 RTT 的變化觀察路由器上佇列延遲時間，並在次每收到 ACK 時
以延遲時間的大小，大幅度的更新壅塞窗口來達到控制傳輸速度的目的；
TCP  Westwood+ [9]以 ACK 回傳的情形作為評估計算壅塞視窗臨界值（ssthresh）之依
據，加長緩啟動(Slow-Start)成長的時間，藉此讓傳送速度盡量維持在高速。 
傳統 TCP 在高速網路下的適應議題的第二類研究，著眼在同時使用多條 TCP 連線
進行資料傳輸，稱為平行化 TCP（parallel TCP）。在高速網路下，一對要進行大量資料
傳送的端點以平行化 TCP 結合 N 條 TCP 連線同時作傳輸，如圖 1.5 所示，此種同時開
啟 N 條連線的傳輸行為，方法簡易且可有效改善傳統 TCP 壅塞窗口以線性成長指數衰
退的缺點，大幅提昇產能，改善傳統 TCP 壅塞控制機制成為效能瓶頸之自我限制。 
 
圖 1.5：利用多條 TCP 連線同時進行傳輸示意圖。 
列舉兩個平行化 TCP 的相關研究說明如下：TCP/DAC-C (TCP/Delay-based 
Congestion Avoidance Collaborative)[14]之傳輸行為虛擬碼如圖 1.6 所示，其精神在於利
用多條 TCP 連線形成 Group 進行傳輸，在建立連線之初，連線必須註冊成為某 Group
的一員，資料傳輸的過程中，每條 Parallel TCP 連線依照 RTT(round-trip time)的變化做
自我壅塞機制的調整，並把將訊息傳送給同在此 Group 的其他連線，以達到充分運用網
路資源、提高產能。在 A-TCP-P(Adaptive TCP-P)[23]中，各個平行化 TCP 連線彼此分
享依 Smooth RTT 變化值所預測的臨界壅塞窗口值等參數，作為每條連線更新網路參數
 6
 8
FlashGet[27]，以及 BitComet[33]、BitTorrent[32]等著名的 P2P（Peer-to-Peer）互傳軟體
等，在傳輸層實做平行化 TCP 來提昇高速網路下頻寬的利用度，以增進傳輸效能。在
此風潮之下，網路使用者勢必會不斷提高平行化 TCP 的連線數目、欲藉此以達到提昇
自我傳輸效能的目的。因此，一個十分急迫而重要的議題如下：在網路頻寬資源非無限
大的情況下，一個固定頻寬的網路鏈結上可同時存在的 TCP 連線數目(平行度)是否越多
越好？過大的連線數目是否會造成網路資源的過度競爭？該如何取得其平衡？因此我
們認為有必要對於平行化 TCP 多條連線同時傳輸的作法進行更深入的研究，避免眾多
網路使用者一味的擴大其平行度，反而導致整體網路環境應無法負荷而陷入嚴重的封包
遺失狀態，甚至造成網路癱瘓的嚴重情況。 
 
本計畫即著眼於此，期望經由平行化 TCP 產能數學模式的探討，推導出平行化 TCP
基於特定網路資源參數下之最大平行度公式，並希望藉由連線路徑上網路設備器的協
助，取得必要的網路參數，以便在實際的傳輸行為中換算並開啟適當數目的 TCP 連線，
適切且充分的善用網路資源。我們將從兩個不同方向切入研究以求達到目標，首先在第
一年的研究裡，我們假設網路資源參數是可取得的，例如：端點對端點間訊號來回的傳
遞時間、網路瓶頸點(bottleneck)可用的頻寬、緩衝區(buffer)大小，配合各連線壅塞窗口
值，推得平行化 TCP 之產能數學公式，並以逼近法求得最佳產能下的最適連線數量 M
值（最大平行度）；此外並進一步調校平行化 TCP 的壅塞控制方法，使其已存在的 N 條
連線傳輸總和等同於 M 條連線的傳輸行為，讓網路資源的使用率達到最佳化。在第二
年的研究中我們將研究並設計機制，在考慮可行性的前提下，透過連線路徑上路由器的
協助，由網路端點取得平行化 TCP 最大平行度公式所需的網路參數，讓平行化 TCP 的
最大平行度計算成為可能。 
TCP 在網路傳輸中所扮演的地位十分重要且歷史悠久，自有網路以來不斷有 TCP
壅塞控制機制的相關研究持續進行，目的就是為了藉由調整 TCP 的傳輸行為，以因應
隨著底層網路技術逐漸改變的網際網路狀況，充分利用網路頻寬，降低封包遺失機率以
及減低延遲時間，發揮出更高的傳輸效能。假使能夠透過本研究計畫推導出一個更適合
於目前高頻寬網路的傳輸模式，例如本計畫所研究的 parallel TCP 多條連線同時傳輸之
模式，進而觸發網路傳輸層傳輸模式的改善，那麼這個研究計畫的影響將是重大且深遠
的。 
 
 
 
 
 
 
 
 
 
TCP 公式都是根據它。因為封包遺失速率 p跟最大的 window size W 有關係，這個公式
顯示 W, RTT 和 p 是重要的參數可以影響 TCP 的效能。然而這個公式也有兩個缺點，首
先，它們只有考慮壅塞避免時期，這並不完整，而且在緩啟動以及快速重傳和復原時期
缺少 TCP 的行為。第二，對於一個 TCP 來源端來說，要得到現在網路上封包遺失速率的
值是不容易的。 
 Padhye 等人在[23]提出了其他單一 TCP 穩定狀態的產能公式，叫做 PFTK 公式，
如圖 2.2，它可以獲得壅塞避免狀態的行為。在 PFTK 公式裡 TCP 壅塞控制的行為是根
據每一回合來模擬的，每一回合的持續時間相等於一個訊號來回的傳輸時間。這個公式
有兩個假設如下，首先，一個回合裡面的封包遺失與其他回合的封包遺失是無關的。第
二，在一個回合裡面的封包遺失是互相關聯的。 
 
圖 2.2：在壅塞避免的階段，PFTK 公式封包的傳送情形 
 
在 PFTK 公式裡，封包遺失可以表示成收到連續三個重複 ACK 或是 timeout。在
timeout 發生時，T0代表 TCP 來源端等待重傳沒有認可的封包的時間，此外，這個公式
也包括 cwnd 被 TCP 終點端限制的例子。它的產能公式大致可以表示如下 
    (2.2) 
b 為收到 ACK 的封包數目。 
PFTK 公式考慮的非常仔細，它可以表示成最大 window size W, RTT, T0和封包遺
失速率 p的函數。儘管如此，對於一個 TCP 來源端來說，要得到封包遺失速率的值仍然
是不容易的。 
        
平行化 TCP 傳輸的產能公式 
根據 Bollinger[28]研究的結果，Hacker 等人提出了 N個平行化 TCP 連線產能上限
的公式。作者假設對於所有的 TCP 連線，MSS 和 RTT 是固定的。所有的連線是界於兩個
連線之間，而且當網路仍然壅塞時，所有的連線都有相同的 RTT 值。產能公式可以表示
 10
要網路的封包遺失速率，它對於 TCP 來源端去預測產能也許是可行的。 
接下來先介紹我們公式的假設，再下來就是我們公式的詳細推導。 
 12
假設 
TCP Sack [31]，我們假設 TCP 來源端
同時
是因為壅塞的影響，我們不考慮隨機遺失的影響。此外，
為了
行化 TCP 產能公式，我們使用的四個變數或參
數定
行化 TCP 在三個時期的行為：緩啟動(slow-start)、壅塞避免
(con
                               
我們的公式是根據廣泛運用在現今網路上的
開啟所有的連線，而且總是有資料在傳。我們也在每一回合模擬 TCP 的行為，一個
回合開始會隨著現在 TCP 的 cwnd 大小傳送，在我們的公式，每一回合的時間相等於訊
號來回的傳輸時間(RTT )。 
在我們的公式，封包遺失
簡化分析，我們假設所有平行化 TCP 來源端的連線都在同一回合察覺遺失事件，TCP
來源端偵測到封包遺失是根據接收到 V個重複的 ACK，V 通常設為 3。根據上述的假設，
我們導出平行化 TCP 產能的公式如下。 
我們想要導出使用實際網路參數的平
義如下。B是 bottleneck link 的頻寬，T是端點對端點間訊號的來回的傳遞時間(不
包括排隊時間)，drop-tail 路由器的緩衝區大小是在 bottleneck link，N 是平行化 TCP
的連線個數。 
我們考慮平
gestion avoidance)、以及快速重傳和復原(fast retransmit and recovery)。我
們定義 Y是整個傳輸時間傳送的封包總數，A是傳輸的時間，然後產能可以表示如下 
                     (2.6)  
期藉由逐漸地增加進入網路的資料總數使 TCP 連線能夠發現可用的頻
寬，
緩啟動時期 
緩啟動時
一但接收到一個 ACK，cwnd 會增加一個封包，如圖 2.4。 
 
圖 2.4：TCP 連線在緩啟動階段的封包傳送情形 
讓 USS是在緩啟 合)，我們使用 YSS
表示
                    
動時期的回合總數(從第 0 個回合到第 USS-1 個回
在緩啟動時期傳送的資料總數。平行化 TCP 的連線以指數方式增加它們的 cwnd，
所以我們得到   
      (2.7) 
 14
      
              …… 
      
在我們的公式，我們假設在第 UCA – 1 個回合網路充滿了封包，所以在第 UCA – 1
的 cwnd 必須是 B × T + Q。因此，我們可以得到 
 
個回合
                                          (2.11) 
在壅塞避免時期傳送的資料總數是在每一回合的 cwnd 總和，它可以表示成  
                         (2.12) 
的傳送期間，
      te：佇列是空的期間 
  
送的期間，當平行化 TCP 的 cwnd 從
          我們考慮 TCA是在壅塞避免時期平行化 TCP TCA包括以下參數： 
     tq：佇列增大的期間
      tQ：佇列已經滿的期間 
在壅塞避免時期資料傳 成長到 B × T 時，
。所以在佇列大小等於零的期間，回合的總數等於bottleneck 佇列應該是空的
，而每一回合的時間長度等於 T。 整個壅塞避免時
期佇列可能不會是空的，在這個例子 te 可能等於零，我們可以得到 
考慮在
                           (2.13) 
當平行化 TCP 的 cwnd 從 B × T 成長到 B × T + Q，仍然有一些封包累積在佇列裡。
在緩衝區裡面的封包排隊期間當時的回合總數是 。每一回合的時
間長度包括 T和封包排隊時間，也就是說 
 
藉
                         
由上面已知的算式，我們可以得到 
                   (2.14) 
在壅塞避免時期最後一回合的緩衝區是滿的，所以 
 16
們使用啞鈴型網路拓墣執行所有的實驗，如圖 2.6，我們運用 TCP SACK 於所有
P 連線，平行化 TCP 流量從來源節點 S 到終點節點 D，R0和 R1是 drop-tail
路由
我
的平行化 TC
器，Link L2 是 bottleneck link，L1 和 L3 是末端節點和路由器之間的 acess links，
每一次的模擬時間都長的足以得到穩定狀態的產能，所有的平行化 TCP 連線在開始時都
同時啟動。 
 
圖 2.6：模擬的拓墣 
 
我們公式的設定參數描述如 等於 1KB，而且 SSTHRESH 等於
，我們把 X設為 1，也就是在每一條連線的 CA 會有一個封包遺失，接著把 V設成 3，
它是
節如下， 
在不同
NS2 模擬的結果，為了
測量在不同長度的 RTT(0.03 秒、0.05 秒、0.08 秒、0.1 秒、0.2
秒)下
的頻寬都是 1000Mbps，bottleneck 緩衝區大小等於
BDP，
結果和改善比例如表
2.1，
所有遺失的封包都在同一回合重傳，以簡化分
析。
下，讓 segment 大小
32KB
指當接受 3個重複的 ACKs 就視為封包遺失。 
我們在 3個典型的 bottleneck links 如 T1(B=1.54Mbps)、T3(B=45Mbps)和寬頻網
路(B=100Mbps)執行平行化 TCP。每一個實驗結果細
RTT 下的模擬 
接下來介紹了我們的公式和 Wu 的公式在不同的 RTT 下使用
評估 RTT 的影響，我們
的效能，這些實驗結果隨著不同數目的 N 而改變，圖 4.2 到 4.4 是利用 NS2 模擬
的產能，並且使用 N當做函數。 
圖 2.7 說明在 100Mbps bottleneck link 上，不同 RTT(0.03 秒、0.05 秒、0.08 秒、
0.1 秒、0.2 秒)的產能，L1 和 L3
N 從 1 到 100，圖 2.7(a)和 2.7(b)的模擬時間是 300 秒，2.7(c)和 2.7(d)的模擬
時間是 500 秒，2.7(e)設定為 1500 秒以獲得穩定狀態的產能。 
圖 2.7 清楚的顯示平行化 TCP 勝過單一 TCP，特別是當 RTT 大的時候，為了繼續觀
察平行化 TCP 和單一 TCP 的效能，單一 TCP 和平行化 TCP 的模擬
跟平行化 TCP 相比，當 RTT 變大時，單一 TCP 的產能會減少，在 RTT 大的寬頻網
路上，平行化 TCP 增加的改善比例優勢更加明顯。從結果來看，我們得知在高 BDP 網路
下，平行化 TCP 比單一 TCP 還有效率。 
在圖 2.7，我們平行化 TCP 的結果類似於模擬的結果(除了 N=1)，原因是違反了模
擬的假設，因為在我們的公式，我們假設
然而，當平行化 TCP 的連線少時，每一條連線的 cwnd 會變大，在一個遺失事件裡
遺失的封包數目會變大，所以對於一個 TCP 來源端來說，在同一回合重傳成功是困難的。 
 18
圖 2.7 顯示在寬頻網路下，兩種公式和模擬的效能，明顯地，Wu 的公式和模擬的
結果有很大的誤差，我們把它歸因於計算傳輸期間的偏差。讓 uq 是佇列建立時的回合
數，在 Wu 的公式，TCA的時間等於 UCA × tQ  - uq × T，缺少 tq 的考量導致誤差發生。從
圖得知，當 RTT 變大時(除了 N=1 外)，我們公式和模擬結果的誤差會減少。當 N 等於
50 時，我們公式和模擬結果的誤差比例分別等於 4％、1.7％、0.7％、0.3％、0.04％，
當 RTT 增加時，預測的值和模擬結果非常接近，所以可以得知我們平行化 TCP 產能公式
在高 BDP 網路上更有效率。 
圖2.8顯示 ps，bottleneck 
緩衝
遺失事件發生，因為其他的連線已經對於遺失
事件做出反應並且把
，我們公式的預測更加準確，在圖 2.8，Wu 的公式和模擬結果的誤差仍然很
大就
在T3 bottleneck link上的產能，L1和L3的頻寬是100Mb
區大小等於 BDP，N 從 1 到 100。我們測量在不同長度的 RTT (0.03 秒、0.05 秒、
0.08 秒、0.1 秒、0.2 秒)下的效能，圖 2.8(a)、2.8(b)、2.8(c)和 2.8(d)的模擬時間
都是 300 秒，2.8(e)設定為 1000 秒。 
使用平行化 TCP 的好處在圖 2.8 也非常明顯，表 2.2 顯示單一 TCP 和平行化 TCP 的
模擬結果和改善比例，當 RTT 變大時，平行化 TCP 比單一 TCP 有更好的產能，單一 TCP
減少的產能類似於表 2.1，這個結果支援了在高 BDP 網路上，平行化 TCP 的產能勝過單
一 TCP。 
在 RTT 等於 0.03 秒的例子，當 N 變大時，我們公式的預測和模擬的結果出現了誤
差，我們把它歸因於下面了理由。為了簡化分析，我們假設當封包遺失時，所有平行化
TCP 連線都在同一回合察覺遺失事件發生，事實上並非所有的連線都在相同回合偵測到
遺失事件，一些平行化 TCP 連線並不會有
window 值減半。然而當 RTT 變大時，我們公式的預測和模擬結果
非常接近。在 RTT 是 0.03 秒而 N等於 50 的誤差比例是 3％，而當 RTT 等於 0.05 秒 0.08
秒、0.1 秒和 0.2 秒的誤差比例分別是 2％、0.7％、0.3％、0.01％。由此可以得知當
RTT 增加時
像圖 2.7。 
為了比較在圖 2.7 和圖 2.8 我們公式和模擬的結果，我們可以找到它們都有相同的
特徵，也就是說我們的公式在高 BDP 網路上有穩定的預測，對於 TCP 來源端來說，使用
我們的公式預測平行化 TCP 的產能是很重要的。 
 
圖 2.9 顯示在 T1 bottleneck link 上不同 RTT 的產能，L1 和 L3 的頻寬是 100Mbps，
bottleneck 緩衝區大小等於 BDP，N 從 1 到 10。每一張圖的模擬時間是 300 秒。 
在圖 2.9(a)和 2.9(b)當N變大時，我們公式預測的結果有反常增加的現象，我們把
它歸因於違反了round-based的模擬。在我們提出的公式裡，我們是以回合為單位模擬
TCP的行為，算式(3.6)的Q設為BDP，UCA等於 BT  N +1，在圖 2.9(a)和 2.9(b)的例子裡，
BT的值分別是 5.775KB和 9.625KB，當N大於BT時，UCA的值會被偵測為有小數，所以對於
我們round-based的公式在一個回合模擬平行化TCP的行為是很嚴重的。 
 
 
 
圖 2.9：在 T1 的 bottleneck link 上，不同 RTT(0.03 秒、0.05 秒、0.08 秒、0.1 秒、
0.2 秒)的產能 
除了上述的不尋常情況外，在圖 2.9(c)和 2.9(d)裡當 N 變大時，我們的公式和模
 20
Wu 的公式預測的結果並沒有呈現跟在低頻寬環境裡相同的使用度。 
 
 
 
圖 2.10：在 100Mbps bottleneck link 上，不同緩衝區大小的產能 
 
圖 2.11 顯示在T3 bottleneck link上，不同緩衝區大小的產能，L1 和L3 的頻寬是
1000Mbps，RTT等於 0.1秒，N從 1到 50，圖 2.11每一張小圖的緩衝區大小分別等於50KB、
 1 
 2 BDP和BDP，每一次模擬的時間設定為 300 秒。 
圖 2.11 呈現了跟圖 2.10 類似的結果，單一 TCP 在圖 2.11 (a)、2.11 (b)和 2.11 (c)
分別等於 36.539Mbps、39.861Mbps 和 41.108Mbps，可以清楚地得知平行化 TCP 比單一
TCP 連線更有效率。圖 2.11 (a)和 2.11 (b)顯示了我們的公式和模擬結果的誤差跟平
行化 TCP 連線的數目有關，這是因為違反了我們公式的假設，因為我們假設所有平行化
TCP 連線都在同一回合察覺遺失事件發生，由於缺乏緩衝區大小導致遺失事件更頻繁地
發生而且影響了預測。 
在圖 2.11 可以看到平行化 TCP 連線在緩衝區大時，可用頻寬的使用率更有效率。
在圖 2.11 (a)、2.11 (b)和 2.11 (c)的頻寬使用率分別是 86％、98％和 99％，緩衝區
不夠可能會導致遺失視線頻繁地發生而且減少產能，在圖 2.11，Wu 的公式跟模擬的結
果同樣地跟圖 2.10 一樣有很大的誤差，這是因為 Wu 的公式並不適合運用於高 BDP 網路
而且它也不能顯示跟在低頻寬使用率一樣的趨勢。 
 22
 24
圖 2
TCP 產能，在圖 2.12 (b)的高 BDP 網路
上，W
在低
 
.12 (b)平行化 TCP 的模擬結果比跟圖 2.12 (a)更穩定，這是因為它有足夠的緩衝
區。在圖 2.12 (a)低頻寬和緩衝區小時，我們的公式和模擬的結果出現了誤差，這是
因為我們的公式在低 BDP 網路上是不利的。 
Wu 的公式在圖 2.12 (a)準確地預測平行化
u的公式和模擬的結果出現了誤差，這是因為 Wu 的公式在高 BDP 網路上是不利的。 
我們上述利用 NS2 比較我們的公式和 Wu 的公式的效能，從結果可以得知 Wu 的公式
BDP 網路上執行的比較好，而我們的公式比較適合在足夠的緩衝區和 RTT 大的環境
上執行。此外，我們的公式並不需要用到封包遺失速率。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 26
oric, “Improving Congestion Control in IP-based Networks by Information 
[14] l TCP Flows”, in 
[15] ngestion control algorithms for the 
[16] yd. “Connections with Multiple Congestion Gateways in Packet-Switched 
[17] ke, J. Mahdavi and T. Ott, “The Macroscopic Behavior of the TCP 
[18] ey, and J. Kurose, “Modeling TCP Reno performance: A 
[19] anwar, “Analysis of TCP congestion control 
[20]  comparative 
[21] -End Performance Effects of 
[22] Jadwiga Indulska, Sylvie Perreau and Liren Zhang, “Exploring TCP 
[23] llel TCP Flows in High 
[24] ummynet,” http://info.iet.unipi.it/˜luigi/ip_dummynet/
Fairness using Parallel TCP,” in Proc. of INFOCOM 2004. vol.4, pp. 2480–2489, Mar. 
2004. 
[13] M. Sav
Sharing,”  Doctoral Thesis, Berlin University, Germany, Nov. 2004, 
S. Cho and R. Bettati, “Collaborative Congestion Control in paralle
Proc. of IEEE ICC, pp. 1026–1030, May 2005. 
S. Floyd. “SACK TCP ： The sender’s co
implementation sack1 in LBNL’s ns simulator (viewgraphs),” Technical report, Mar. 
1996. 
S. Flo
Networks, Part 1: One-Way Traffic,” ACM Computer Communications Review, vol.21, 
no.5, Oct. 1991. 
M. Mathis, J. Sem
Congestion Avoidance Algorithm”, ACM Computer Communications 
Review, pp.67–82, Jul. 1997. 
J. Padhye, V. Firoiu, D Towsl
simple model and its imperical validation,” IEEE/ACM Transactions on Networking, 
vol.8, Issue 2, pp. 133–145, Apr. 2000. 
R. Roy, R. C. Mudumbai and S. S. P
using a fluid model,＂ in Proc. of IEEE ICC, pp. 2396-2403, Mar. 2001. 
B. Sikdar, S. Kalyanaraman and K. S. Vastola, “Analytic models and
study of the latency and steadystate throughput of TCP Tahoe, Reno and SACK,＂ in 
Proc. of IEEE GLOBECOM, pp. 1781-1787, Nov. 2001. 
T. J. Hacker, B. D. Athey and B. Nobel, “The End-to
Parallel TCP Sockets on a Lossy Wide-Area Network,” in Proc. of IPDPS, pp. 434-443, 
Apr. 2002. 
Qiang Fu, 
Parallelisation for performance improvement in heterogeneous networks”, Computer 
Communications, vol. 30 , no. 17, pp. 3321-3334, Sep. 2007. 
S. Cho, “Congestion Control Schemes For Single and Para
Bandwidth-Delay Product Networks”, Doctoral Thesis, Texas A & M University, USA. 
May 2006. 
L. Rizzo, “D . 
isi.edu/nsnam/ns/[25] UCB/LBNL/VINT Network Simulator – ns (version 2), http://www. . 
 
[27] et, http://www.flashget.com/
[26] S. Cho, R. Bettati, “Adaptive Aggregated Aggressiveness Control on Parallel TCP
Flows Using Competition Detection,” in Proc. of IEEE ICCCN, pp. 237–244, Oct. 
2006. 
FlashG . 
/[28] Net Transport, http://www.xi-soft.com . 
[29] Orbit, http://www.orbitdownloader.com/. 
[30] FasterFox, http://fasterfox.mozdev.org/. 
