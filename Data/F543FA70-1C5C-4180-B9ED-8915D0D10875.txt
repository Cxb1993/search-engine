II 
 
 
中文摘要 
微陣列基因資訊在癌症分類與診療中是一個逐漸受重視的領域。如何從腫瘤所能提供的
有限資訊裡正確的判斷癌症種類，進而針對此癌性的特性給予正確的診療，是一個很重要的研
究方向。癌症種類通常是由醫師依據其形態或是組織來判斷；然而，有些不同的癌症在組織病
理上與形態上相當類似，誤判的可能性也隨之提高。隨著基因科技的進展，由微陣列基因資料
中來進行癌症種類的判別與分析，甚至於是預測，己經有相當的研究成果。然而，目前的方法
存在以下幾個缺點： 
1. 準確率有進步空間。 
2. 分類結果僅為簡單答案或機率值，不容易供後續分析 
3. 因特徵數量過大所引發的效率問題。 
本計劃將發展一個可以利用微陣列基因表現資料進行自動癌症分類的方法，並實作此方
法成為一個分類系統。利用基因規劃法做為分類器的模型，以微陣列資料與癌症類別進行訓練，
得出適用於各種不同癌症的判別式。而我們將以多層架構式基因歸劃法為基礎，研發新的基因
歸劃法分類器模型。多層架構式基因歸劃法所建立之分類器具有準確率高與高效率的特性。此
方法所產生之判別式是由基因所組成，可以方便進行後續分析，例如分析出現在同一判別式中
的基因，是否具有某種關係等等。 
本計劃完成之後，預期將具有以下優點： 
1. 對己知的癌症種類可以先行建立資訊庫節省時間。若有新的樣本資料，可將原判別
式進行再訓練以得到更佳準確度； 
2. 針對某一判別式，可分析其所使用之基因的關聯性； 
3. 對不同的判別式之間，可用於探勘基因與不同癌症的關聯性； 
4. 本計劃所研發之高效率基因規劃法工具可應用於不同問題。 
IV 
 
研究成果 
本計劃除了培養多位大學生參與專題研究計劃案，也將研究成果發表於國際研討會。由本計劃
支援的基因規劃法軟體 LAGEP (Layered Architecture GEnetic Programming)也開始於網路上開放下載
使用。LAGEP 的使用者來自多個國家，包含美國與加拿大等地，許多研究生以 LAGEP 做為他們研
究基因規劃法的工具。 
在本計劃案中，我們將多種癌症微陣列資料庫以多層式基因規劃法進行分類判斷。由基因規劃
法的優異學習能力，雖然癌症微陣列資料的特徵數極多，仍然可以有效率的找到分類函數，並依此函
數對測試資料進行分類。除了分類的功能之外，我們認為，在分類函數中出現的基因代碼，應該是較
具有代表性的基因，此一方面的研究，將是我們未來的研究方向。 
附上發表於 2009年Genetic and Evolutionary Computation Conference (GECCO 2009)的研究論文
乙篇，做為本計劃案之研究報告。 
 
great number of features. In this paper, we not only devote to 
build an accurate classification model but also try to propose a 
feature selection mechanism. 
In this paper, in order to enhance the accuracy of the diagnosis 
and to resolve the efficiency reduction due to mass amount of 
gene data, we use the layered genetic programming, LAGEP, as 
an alternative classifying tool. The utilization of LAGEP enables 
us to derive a set of discriminant functions in a more effective 
way. It can also shorten the training time of recognizing cancer 
from its microarray gene expression profiles. Moreover, it can 
select significant features and generate new features by 
compositing original features. LAGEP demonstrates several 
merits includes (1) Dealing with multi-populations instead of 
single ones (2) The ability of feature selection (3) Diminishing the 
length of the entities as well as the evolution time (4) Distributive 
computation (5) Comparing with multi-parameters. 
2. METHODOLOGY 
Genetic programming [30][31] is a branch of evolutionary 
computation. It mimics natural selection mechanism to 
approximate the optimal solution by picking individuals with 
highest fitness value within a simulated environment. 
Traditionally, genetic programming utilizes single population that 
diminishes variety of individuals. Multi-population GP [32][33] 
employs several populations to keep high variety via evolving 
individuals separately located in different populations. Fernández 
et al. [33] performed several experiments with parallel and 
distributed GP (PADGP), isolated multi-population GP (IMGP), 
where "isolated" means that there is no migration between 
populations, and traditional single population GP. Their 
experiments show that PADGP and isolated multi-population GP 
usually obtain better performance than traditional single 
population GP.  
Layered architecture genetic programming (LAGEP) is different 
from PADGP [34][35]. LAGEP does not arrange populations in 
neither a circle topology nor a grid topology but a multi-layer 
structure, as shown in Figure 1. LAGEP is constructed by L layers. 
Each layer that consists of a number of populations could have 
particular configuration, for example, layer 1, L1, could have 3 
populations while L2 have 2 populations. 
The connection between layers is different from neural networks. 
The output of a population in Li does not directly connect to a 
population in Li+1. Instead, LAGEP collects all outputs of 
populations Pi1, Pi2, …, Piki, where ki is the number of populations 
in Li, to generate a new training set Ski. To accomplish such goal, 
a individual forms a functional expression tree. Considering a 
given m-dimensional training instance t, 
t =(a1, a2, …, am, category), ai∈R, m∈N. 
Individual idv is defined as a function mapping training instances 
from the feature space into a real number:  
idv(t): (Rd∪ C) ÆR,  
where C is the space of constants and t∈Rd. An individual is 
implemented by a binary tree structure. The length of an 
individual is controlled by its depth, for example, an individual 
has at most 211 nodes if its depth is 10; it also has at most 210 
leaves that are variables (genes) and constant values. 
Training instance t is recognized to target class when idv(t) ≥ 0. 
The dimension of an instance is d rather than m because the 
dimension of it will be changed by the LAGEP. A training set Ski 
is a collection of training instances whose data are ki-dimensional: 
Ski = {t1, t2, …, tn}, ti=(ai1, …, aij, ..., aiki, categoryi),  
aij∈R, m, n∈N. 
The original training set is denoted by Sm. The training set Ski+1 
generated by Li is constructed as follows. Once a population Pij 
completes its evolutionary processes, its output is an individual 
idvij having highest fitness value. The training set Ski is 
transformed by those individuals by: 
for every ti∈Ski, ti=(ai1, …, aij, aiki, categoryi), 
we have ti’=(idvi1(ti), idvi2(ti), ..., idviki(ti), categoryi). 
The number of instance in the new training set Ski+1 is the same to 
Ski. The dimension of an instance of Ski+1 is reduced to ki, the 
number of populations in Li. Each feature of an instance in Ski+1 is 
a composition of features in Ski. Furthermore, since idvij provides 
discriminating ability for most of training instance in Ski, for a 
positive instance t∈Ski it is expectable that most features of ti’ is 
positive. Also, most features of ti’ should be negative if ti is a 
negative instance. Such property makes training instances easier 
to be classified at next layers. Li+2 evolves with Ski+1 could obtain 
better classification performance. Notice that the last layer LL 
contains single population only, i.e., kL=1. While LL includes two 
or more populations, the instance in SkL would have more than one 
feature to indicate its predicted category that could be confused 
especially when the kL is even. Instead of proposing a mechanism 
to solve such problem, we restrict kL=1 in the LAGEP. 
3. EXPERIMENTS AND RESULTS 
3.1 Datasets and Evaluation Method 
In this paper we completed experiments with two microarray 
datasets: colon[1] and leukemia[36]. Table 1 shows the 
summarization of these two datasets. 
In order to evaluate the classification error rate for the microarray 
datasets we use leave-one-out cross-validation (LOOCV) method. 
For each training instance ti of a dataset S, LAGEP trains 
remaining |S|-1 training instances and predicts the category of ti. 
LOOCV performs |S| times for a training set S and evaluates 
classification error rate by the fraction of all training instances 
that are correctly classified. The classification performance is 
measured by performing LOOCV 10 times. 
As mentioned in Section 2, an instance t is either recognized or 
rejected by an individual idv. For a binary classification problem, 
we do not need to train the classifier with two categories but only 
one of them. In this paper, we select the major class as the target 
class. 
 
which would raise a question about interpreting the functions. 
This is a great challenge and we will investigate this problem with 
biology models in future. 
 
90.00
91.00
92.00
93.00
94.00
95.00
96.00
97.00
98.00
99.00
100.00
1st Layer 2nd Layer 3rd Layer
 
Figure 2. Comparison of average fitness values obtained from 
the three layers. 
 
5. ACKNOWLEDGMENTS 
This work was supported by National Science Council under 
grant NSC 98-2218-E-231-001. 
6. REFERENCES 
[1] T. R. Golub, D. K. Slonim, P. Tamayo, C. Huard, M. 
Gaasenbeek, J. P. Mesirov, H. Coller, M. L. Loh, J. R. 
Downing, and M. A. Caligiuri, et al., Molecular 
classification of cancer: class discovery and class prediction 
by gene expression monitoring, Science, 286(1999), 531-537 
[2] A. Dupuy and R. M. Simon, Critical review of published 
microarray studies for cancer outcome and guidelines on 
statistical analysis and reporting, Journal of National Cancer 
Institute, 99, 2 (2007), 147-57 
[3] J. W. Lee, J. B. Lee, M. Park and  S. H. Song, An extensive 
comparison of recent classification tools applied to 
microarray data, Computational Statistics & Data Analysis, 
48 (2005), 869-885 
[4] M. H. Asyali, D. Colak, O. Demirkaya and M. S. Inan, Gene 
expression profile classification: a review, Current 
Bioinformatics, 1 (2006), 55-73 
[5] Y. Lu and J. Han, Cancer classification using gene 
expression data, Information Systems, 28 (2003), 243-268 
[6] S. Dudoit, J. Fridlyand and P. Speed, Comparison of 
discrimination methods for classification of tumors using 
gene expression patterns, Journal of American Statistical 
Association, 97, 457 (2002), 77-87 
[7] L. Li, T. A. Darden, C. R. Weingberg, A. J. Levine and L. G. 
Pedersen, Gene assessment and sample classification for 
gene expression data using a genetic algorithm/k-nearest 
neighbor method, Combinational Chemistry and High 
Throughput Screening, 4, 8 (2001), 727-739 
[8] A. Statnikov, C. F. Aliferis, I. Tsamardinos, D. Hardin and S. 
Levy, A comprehensive evaluation of multicategory 
classification methods for microarray gene expression cancer 
diagnosis, Bioinformatics, 21, 5 (2005), 631-643 
[9] H. Zhang, C. Y. Yu, B. Singer and M. Xiong, Recursive 
partitioning for tumor classification with gene expression 
microarray data, in Proceedings of the National Academy of 
Sciences, 98, 12 (2001), 6730-6735 
[10] L. Breiman, J. H. Friedman, R. Olshen, and C. J. Stone, 
Classification and regression trees, Belmont, CA, 
Wadsworth, 1984 
[11] L. Breiman, Random forests, Machine Learning, 45 (2001), 
5-32 
[12] A. Statnikov, L. Wang and C. F. Aliferis, A comprehensive 
comparison of random forests and support vector machines 
for microarray-based cancer classification, BMC Informatics, 
9, 319, 2008. 
[13] B. Wu, T. Abbott, D. Fishman, W. McMurray, G. Mor, K. 
Stone, D. Ward, K. Williams and H. Zhao, Comparison of 
statistical methods for classification of ovarian cancer using 
mass spectrometry data, Bioinformatics, 19 (2003), 1636-
1643 
[14] R. Diaz-Uriarte and S. Alvarez de Andres, Gene selection 
and classification of microarray data using random forest, 
BMC Bioinformatics, 7 (2006), 3 
[15] J. Gehrke, G. Ganti, R. Ramakrishnan, and W. Loh, Boat-
optimistic decision tree construction, in Proceedings of the 
1999 ACM SIGMOD, Philadephia, PA, 1999, 169-180 
[16] G. Natsoulis, L. E. Ghaoui, G. R. G. Lanckriet, A. M. 
Tolley, F. Leroy, S. Dunlea, B. P. Eynon, C. Pearson, S. 
Tugendreich, and K. Jarnagin, Classification of a large 
microarray data set: algorithm comparison and analysis of 
drug signatures, Genome Research, 15 (2005), 724-736 
[17] L. Zhu, B. Han, L. Li, S. Xu, H. Mou, and Z. Zheng, A novel 
two-stage cancer classification method for microarray data 
based on supervised manifold learning, in Proceedings on 
the 2nd international conference on Bioinformatics and 
biomedical Engineering (ICBBE), (16-18, May, 2008), 1908-
1911 
[18] Y. Lee, and C. Lee, Classification of multiple cancer types 
by multicategory support vector machines using gene 
expression data, Bioinformatics, 19, 9 (2003), 1132-1139 
[19] J. J. Liu, G. Cutler, W. Li, Z. Pan, S. Peng, T. Hoey, L. 
Chen, and X. B. Ling, Multiclass cancer classification and 
biomarker discovery using GA-based algorithms, 
Bioinformatics, 21, 11 (2005), 2691-2697 
[20] J. Khan, J. S. Wei, M. Ringner, L. H. Saal, M. Lananyi, F. 
Westermann, F. erthold, M. Schwab and C. R. Antonescu, C. 
Peterson, and P. S. Meltzer, Classification and diagnostic 
prediction of cancers using gene expression profiling and 
artificial neural networks, Nature Medicine, 7, 6 (June, 
2001), 673-679 
出席國際會議心得報告 
單位名稱 清雲科技大學 
資訊工程系 
職稱 助理教授 姓名 林忠億 
會議名稱 Genetic and Evolutionary Computation Conference 2009 
舉辦時間 2009年7月8日 地點 Montreal, Canada 主辦單位 ACM SIGEVO 
GECCO 是演化式計算領域的最大研討會，許多重量級的研
究學者們，如 Memorial University of Newfoundland 的 Wolfgang 
Banzhaf、University of Missouri的Martin Pelikan、Michigan State 
University的 Erik Goodman與MIT的Una-May O’Reilly等人皆有出
席參加。本次參加 GECCO2009 發表論文，也參加了多場 Tutorial
與演講，獲益良多。從本次研討會中，知道了許多最新的研究發展，
也在幾個論文的發表場合中，得到了解決目前研究困難點的線索，
可以說是不虛此行。 
本身有在進行基因規劃法的程式工具開發，在 poster展中，
與來自西班牙的研究學者們互相討論，他們使用 Java來開發基因規
劃法的工具，並應用在軟體品質分析，是個很有趣的應用。他們也
對我開發的軟體提出一些建議，相當有幫助。 
台灣的學者中，從事演化式計算的人並不多。很高興地，這
次參加 GECCO時，結識了同研究領域的多位人士，像是台灣大學
電機系的于天立博士，交通大學資訊工程系的陳穎平博士與中華大
學資訊工程系的陳健宏博士等等。他們都可說是我的前輩，也不嗇
於給我關於論文發表與研究的指引與建議。在這次研討會中，大家
相處愉快，未來有希望進一步進行研究工作上的合作。 
 
 
 
 
 
 
