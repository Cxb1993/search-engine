  2 
出一具備高效能、高延展性、具備以服務需
求內容為基礎的需求分配  (Content-ware 
Request Dispatching) 的  LVS-CAD Web 
Cluster，將來自客戶端的需求封包中的內容
如 URL 資訊作為需求分配排程的基礎。藉
者使用封包中的需求內容以及伺服器的負
載資訊，我們可以發展更複雜且更具智慧的
需求分配排程法，更有效率的利用整體叢集
式系統的資源，也更適合於各類型的網頁應
用。 
而隨著網際網路相關技術的蓬勃發展
與寬頻網路的普及，網路的應用更加的多元
化，多媒體影音技術的應用讓網頁內容更加
的豐富，也讓許多網站提供了更多元化的服
務，如無名小站、YouTub、…都是相當成
功的網站，許多網路的服務如網路購物、線
上即時影音新聞、遠距教學、視訊會議、…
等等，都使用了多媒體影音技術。 
一個動態影音資料如影片、電影檔案通
常都很龐大，如果當我們在網頁上點選下載
一個影音檔案時，是如傳統的方式是由
client 與伺服器端建立連線後，伺服器開始
檔案的傳輸，直到檔案傳輸完畢關閉此一連
線後，client 端才播放此影音檔案，如此對
於動輒上百 MB 的動態影音資料來說，使用
者需等待很長的時間來下載後才能播放，這
樣的網路服務並不符合使用需求。因此，在
國際組織 ISO 和 IEC 所訂定的 MPEG-4 標
準中，引入串流（Streaming）概念，透過串
流技術，允許 client 不需將遠端伺服器上的
檔案資料全部下載，即可播放，因此，串流
傳輸已被廣泛的應用。 
雖然 MPEG-4 提高了壓縮比例，伺服器
仍需強大的運算能力來處理使用者需的大
量需求，與大量的儲存設備來存放影音檔
案，以保證在大量的使用者需求下仍能提供
一定品質的影音服務，透過叢集式系統，使
用者的服務需求可以被分散至各伺服器處
理來達到負載共享。而透過適當的 request 
dispatching 策略，可以使各伺服器的負載平
衡，並具有高度的可擴充性，而儲存設備之
間也可以相互連結，叢集式系統在設計上亦
可有高可靠度（High Availablity)之優點，在
伺服器發生故障時，可將 requests 由一台伺
服器轉移至另一台伺服器，且不影響 client
端的服務進行，動態地調整負載平衡。因
此，個人電腦叢集系統相當適合當做提供多
媒體影音視訊服務的平台。 
然而，以串流傳送資料後即時播放影音
資料的模式，不同於傳統下載整部影音檔案
後觀賞的方式，使用者的預期可以平順不間
斷地欣賞整部影片。過於緩慢的傳送速率，
會導致使用者對於服務失去耐性；而忽快忽
慢的資料傳送方式，同樣的會讓使用者失去
耐性也失去注意力，如此則失去了即時播放
的意義。所以如何能讓影音服務保持一個穩
定的速度傳送資料，並且不因伺服器故障而
發生影片中斷，成為提供串流服務的主要議
題。 
 
二、研究目的 
 
本計畫的目的即是基於我們在 Linux 
Kernel 2.6 版本的 Module-based LVS-CAD 
Web cluster 做為我們研究的實驗基礎平
台，進行相關課題如 Web QOS 與差異性服
務 (Differentiated Service)、多媒體影音串流
服 務 的 支 援 、 Content-aware Request 
Distribution Policies、Web Content Placement 
與 Resource Management 等研究，讓我們的
叢集式系統所能提供的服務更臻完善。 
由於網際網路使用者與各式需求的快
速增加，傳統「盡力而為(best-effort)」的服
務規範已無法滿足現有需求，提供具有服務
品質 (QoS：Quality of Service) 的架構已成
為趨勢之一，QoS 是一種控制機制，主要針
對不同的使用者需求或是不同數據串流對
應不同的優先順序，或是提供保證服務 
(Guaranteed Services)。透過事先與網路服務
業者  (ISP) 訂定的服務等級協議  (SLA: 
Service Level Agreement)，以在不違反服務
等級協議的前提下，提供使用者最佳的服務
品質。 
在過去的相關研究中，有大量的研究專
注於網路基礎架構上 QoS 應用，如頻寬配
置、封包延遲、需求控制、降低服務品質。
在此，有學者認為提供 QoS 服務的架構有
四個重要概念，分別是允入控制 (Admission 
Control)、服務分類 (Service classification)、
效能獨立 (Performance isolation) 以及高度
  4 
現允入控制 (Admission Control)與服務分
類  (Service classification) 。在 Voigt 與 
Tewari 等人的研究中，提出一個 TCP SYN 
policing 的機制，講求盡可能地提前丟棄過
多的 SYN 請求連線封包。而 Andreolini 等
人則提出一 Switch Admission 演算法，考
慮每台伺服器的能力，限制最大連線數量來
進行允入控制，此種方法簡單且容易實作，
但是卻有不夠精確的問題存在，因為每條連
線並非無時無刻皆在傳輸資料，也有可能處
於閒置狀態。Aweys 等人提出一種應用在 
Web cluster 上的允入控制機制，稱為 AC 
演算法，在此每一台後端伺服器定期回送本
身資訊給 Web switch，由 Web switch 計算
出在這個週期中拒絕新的 TCP 連線建立
的機率，並設定一個門檻值 (Threshold)，若
大於此門檻值，則暫時關閉允入控制機制，
反之則開啟。 
至於在服務分類 (Service classification) 
機制方面，Andreolini 等人提出 Dynamic 
Partitioning 演算法，根據分類使用者連線數
量，將 Web cluster 中的伺服器動態地分成
兩組，稱為 High set 和 Low set，分別服務
高優先權與低優先權的使用者。至於 Shen
的 研 究 中 ， 則 使 用 一 種  Two-level 
Scheduling 的方式，將排程的機制分成外部 
cluster-level 和單一伺服器 node-level 兩部
份，而  node-level 的部份則在內部佈署 
multi priority queue，將各個 queue 給予不同
的優先權，再透過複雜的函式預測下一個該
處理的需求。類似於 Shen 的研究，Voigt
等人則在核心傳輸層中實作一個 Prioritized 
listen queue，一律優先處理高優先權的服務
請求。而在 Luo 的研究中，採用一個參考 
Boyer-Moore string matching 的變形演算
法，來搜尋使用者連線中的 Cookie 資訊，
在根據此  cookie 資訊去比對已建立好的 
URL 表格，找尋適合服務此種類型需求的伺
服器。 
在多媒體影音串流服務的支援方面，
ISO 和 IEC 在 1999 年訂定的 MPEG-4 標準
中引入串流(Streaming)的概念，使得使用者
接收一部份的影音檔案的情況下，即可開始
同步播放，並能夠隨機播放，擁有大量、同
時且不同影音資料區段存取的能力。 
Apple 公司於 1999 年發表了其第一套
串流伺服器 QuickTime Streaming Server
（QTSS），讓使用者能夠在網絡上串流傳
輸多媒體資料和提供現場直播的服務。
QTSS 是基於 Real-Time Streaming Protocol 
(RTSP)、Real-Time Transport Protocol (RTP)
及 Real-time Transport Control Protocol 
(RTCP) 即時網路傳輸標準而設計的，其作
業的平台為 Mac OS X Server。另外，在其
他的作業系統如 Linux、Solaris、FreeBSD
和 Windows NT/2000/XP 等，相同的技術亦
有 open source 的串流伺服器版本：Darwin 
Streaming Server（DSS）。QuickTime 利用
MPEG-4 規格中的壓縮方法與串流概念，提
出一種符合 MPEG-4 的媒體規格 MP4，利
用 MP4 媒體規格，在同樣的影音品質下，
大大降低播放時所需頻寬。 
在利用叢集系統提供串流服務的相關
研究中，國內的學者提出了一個相容於
QTSS 的叢集式串流伺服器。主要包含了前
端的負載平衡伺服器，與後端的服務提供伺
服器。負載平衡伺服器用來分配使用者的
requests 給真正提供服務的後端伺服器，負
載平衡伺服器是控制整個叢集系統。後端伺
服器處理完 requests 後直接回傳給使用者，
而後端伺服器可隨時地加入此一叢集系
統，以達成系統的可擴充性，並在後端伺服
器無法提供服務時，由負載平衡伺服器將其
自叢集系統中刪除，以確保服務的穩定性。 
關於如何提供一個不間斷串流服務系
統的研究中，國內的學者針對 MP4 叢集式
隨選視訊伺服器設計並實作了一個具容錯
能力的伺服器架構，並提出一個影片配置策
略，來提高系統負載平衡效能。以影片配置
策略搭配負載平衡演算法及容錯機制，改善
MP4 叢集隨選視訊伺服器效能，及提供伺服
器錯誤回復機制，達到完整隨選視訊服務。
在提高系統效能方面，結合影片配置策略及
負載平衡演算法，將不同點閱率的影片複製
成不同數量，依據 Chained-Declustering 方法
分散放置在後端伺服器上，搭配能平均分配
使用者需求的負載平衡策略，使系統資源能
得到最大使用率。在系統容錯能力方面，分
別針對負載平衡伺服器及視訊伺服器提出
錯誤回復機制，在此容錯機制中，當前端的
  6 
模組的  Traffic Control 機制，所謂的 
Traffic Control 機制主要包含幾個功能，除
了可以將服務依照不同類型予以分類，另一
是 Policing，用來監控連線的狀態，或是丟
棄違反 SLA 的封包，二是 Traffic shaping，
用來維持資料連線傳輸的速率，並將過剩的
封包進行標記或是延遲的動作。 
如圖二所示，在傳往後端伺服器的網路
請求資料流，在經過核心資料傳輸層時，先
檢查資料結構 dev->qdisc->enqueue 是否為
空值 (若無啟用 QoS 設定即為空值)，若是
空值則依照一般流程呼叫 qdisc_run() 繼續
處理，否則就將封包進行分類處理。 
圖二：Linux Traffic Control 與服務分類 
 
我 們 採 用  Linux kernel 2.6 中 的 
Traffic Control framework，利用 Queuing 
discipline、Filter 與 Class 等資料結構來進
行服務分類的過程，Queuing discipline 可以
依照需求實作各種 Queue，例如使用相關的
演算法如 HTB、GRED、SFQ…等演算法或
是其變形式，來處理內部動作與決定 
dequeue 的順序，並透過多個 Filter 根據不
同的條件來分類收到的封包進入不同的 
Class，而不同的 Class 中又可以再次實作
各種 Queuing discipline。我們將經由先前實
作於  LVS-CAD 的機制改寫相關封包欄
位，透過 Filter 找出優先權較高的網路請
求，然後 enqueue 進入 Queuing discipline，
再根據上述機制調整請求封包在 Queue 中
的位置，以確保高優先權的網路請求可以在
最低的延遲時間內轉送到後端伺服器。 
我們實作在前端伺服器的 Web-Based 
QoS 機制的架構如圖三，其中  FILTER 
module 進行 admission control，MARKER 
與  Multi-Qdisc 模組進行服務分類與控
管，而 Policy Agent 進行 QoS 機制的策略
性控管。而在 LVS-CAD 平台中加入了此
Web-Based QoS 機制後，封包流程如圖四。 
 
 
圖三：實作的 Web-based QoS 機制 
 
圖四：LVS-CAD 加入實作的 Web-based 
QoS 機制的封包流程圖 
 
(二) 支援 Web Cluster 之多媒體影音串流
服務 
在我們的 LVS-CAD Cluster 已能針對
  8 
該影片之後端視訊伺服器來接續提供服
務，且力求客戶端在觀賞影片的過程中並不
會有任何的感覺，如此才能在串流服務中做
到真正的負載平衡管理。  
我們的作法如圖七，由前端伺服器
（Front-end server）所挑選出來的後端伺服
器 (Real server），提供一定時間長度的串
流服務後，可向前端伺服器發出影片調度
（Scheduling）的請求封包，該封包必頇含
有目前服務的客戶端相關資訊（如 IP 位址)
以及影片目前播放時間點（Normal Play 
Time; npt）、封包編號（Sequence Number; 
seq）及時間戳記（Time Stamp; tstamp）等
接續播放影片所需資訊。之後被挑選出來的
後端伺服器便可透過該封包內所提供的相
關資訊來接續提供串流影音的服務，藉此達
成影片調度的機制，讓叢集式串流伺服器系
統整體的負載平衡能達到更好的效果。 
圖七：影片調度(Scheduling)機制 
 
(2) 影音檔案儲存機制 
先前學者的研究顯示，大多數的使用者
會選取所有影片中前 20％的熱門影片觀
賞。所以，影片如同網頁一樣，有不同的熱
門程度，大多數的使用者會選取熱門影片觀
賞 。 在 我 們 先 前 的 研 究 中 我 們 提 出 
CWARD/FR 策略，依網頁的熱門程度的不同，
分配給不同個數的伺服器群來負責服務，即
愈熱門的網頁它的伺服器集合就愈大，就會
有愈多的伺服器會被選中來服務，如此可以
有效地利用整體叢集式系統的記憶體，同時
兼顧服務熱門網頁伺服器的負責平衡。 
我們參考我們的 CWARD/FR 策略，將影
片依熱門程度的不同，配置在後端伺服器的
數目也會不同，因此，影片依熱門程度的不
同，負責服務的後端伺服器的個數也會有不
同。由於影片依熱門程度，複製不同份數分
散儲存於不同的後端伺服器上，因此一部影
片若同時有多個使用者需求，系統可分散到
不同台後端伺服器處理，改善系統整體負載
情形，而且增加系統 throughput，提高同一
部影片同時可服務客戶量。 
然而此種作法的缺點在於，後端伺服器
必頇與客戶端保持持續連線直到影片播放
完畢或是客戶端要求結束該視訊服務，造成
被挑選中的伺服器之負載及服務影片的位
元率都會隨著客戶端連線數成線性成長，即
其負載平衡是依影片播放為單位，如此無法
達到良好的負載平衡，這是我們必需解決的
問題。 
因此針對此議題，我們研究將影片作分
段儲存及分段播放的機制，考慮儲存空間將
依據所負責的後端伺服器的數量將影片做
切割的動作，並個別儲存在所負責的後端伺
服器上；或者是分段播放，即把原本是一小
時影片長度的影片傳輸負載量，分攤到所負
責的後端伺服器上，如此一來可達到較 
fine-grained 的負載平衡，也就不會單一
影片傳輸的工作量都要由同一台後端伺服
器來處理，造成負載平衡不均的情況產生。 
結合影片調度(Scheduling)的機制，由 
Content-aware 的前端伺服器所挑選出來的
後端伺服器，在提供完其系統內儲存的影片
片段之後，便必頇向前端伺服器發出影片調
度的請求封包，該封包必頇含有目前服務的
客戶端資訊及接續播放影片所需資訊，之後
便將客戶端的影音串流需求交由其他後端
伺服器接續提供服務。 
 
(3) 服務品質保證(Quality of Service) 
在QoS的研究可分為以使用者分級機制
及以服務類型分級機制兩種議題來探討。在
使用者分級機制上，可將客戶端的使用者區
分為會員制以及非會員制兩種，屬於會員身
份的使用者，其所發出的要求會具有較高的
  10 
而，此法只求負載平衡而未如 LARD/R 策略
考慮到 access locality，提昇 cache hit 
ratio 以減少 disk I/O，並且並未考慮到
後端伺服器硬體效能的差異。 
而有些策略只有考慮到硬體配備同質
性的叢集，即每個後端伺服器都是相同的硬
體配備，未考慮到後端伺服器硬體效能的差
異。即使有些策略考慮到後端伺服器硬體效
能的差異，使用一權重值 (weight) 的設定
來影響 requests 的分配，然而如在 LVS 的 
Weighted Least Connection 與 Weighted 
Round-Robin，此 weight 是靜態的，並不
會隨伺服器的負載狀況動態的調整。 
而有些策略只有考慮到每個後端伺服器
所提供的服務是同質性服務，即伺服器都是
提供相同的服務，然而，在 Web Cluster同
時提供許多類別的服務時，在後端伺服器硬
體配備或效能有差異時，也許更適合讓後端
伺服器提供的服務是異質性服務，即有些伺
服器可能更適合某種特別的服務。 
因此，我們發展新的 Content-aware 
Request Distribution Policies，針對當
Web Cluster 提供許多類別的服務時，考量
伺服器硬體效能的差異，讓後端伺服器提供
異質性服務，在 request dispatching 時，
考量以下的因素： 
(1) access locality、load balancing、
Web page 屬性，如 file size、access 
frequencies等等。 
(2) 後端伺服器硬體配備效能不同時，權重
值會隨伺服器的負載狀況動態的調整。 
(3) 考量來自客戶端的服務需求有分動靜
態之不同類型，以及多媒體影音串流的
服務需求。 
(4) 考量後端伺服器所提供的服務是異質
性服務。 
 
五、結果與討論 
 
我們基於我們於 Linux kernel 2.6.18 所
實 作 的 具 備 Content-ware Request 
Dispatching 的  Module-based LVS-CAD 
Web cluster，完成研究與實作相關的研究課
題，包含支援  Web QoS 與 Differentiated 
Service 的機制、多媒體影音串流服務的支
援 、 Content Placement 與 Resource 
Management 的 研 究 ， 以 及 發 展 
Content-aware Request Distribution Policies。
詳細項目包括： 
 
(1) 完成 Web QoS 的  Admission Control 
Mechanism 的製作與研究。 
(2) 完成 Web QoS 的 Service classification 
Mechanism 的製作與研究。 
(3) 完成研究與製作適合 Web-based 的多
媒體影音串流服務需求的  request 
distribution policy、Quality of Service 機
制、Fault Tolerant 機制、與影音檔案儲
存機制。 
(4) 完成研究與實作新的 Content-aware 
Request Distribution 策略，使 Web Cluster
提供差異性服務。 
(5) 完成 Web Cluster 之 Content Placement
與 Resource Management 的研究與實作。 
(6) 完成實驗平台的建置、系統整合與效能
提昇的研究與實作。 
 
六、計畫成果自評 
 
我們已完成本計畫的研究目標，基於我
們於 Linux kernel 2.6 所實作的高效能、高
延 展 性 、 具 備 Content-ware Request 
Dispatching 的 Web Cluster，研究與實作智
慧型的 Content-aware Request Distribution 
Policies，藉由有效的負載共享以提升整體叢
集式系統的效能，並研究與實作Web QoS 的
支援、差異性服務 (Differentiated Service)、
多媒體影音串流服務的支援、Web Content 
Placement 與 Resource Management，並整合
整體研究成果於 LVS-CAD 研究平台上。 
此外，本計畫參與的研究人員藉由執行
本計畫，獲得叢集式系統方面的實作經驗，
並對叢集式系統架構、Content-ware Request 
Dispatching 機制與策略、QoS、多媒體影音
串流等相關技術，有深入的了解和研究訓
練。同時，藉由在 Linux 系統上的實作，獲
得對 Linux 系統以及 Linux 核心程式的訓
練，深入了解網路系統的運作原理與運作方
式，同時獲得對叢集式系統效能提昇進一步
研究的基礎。 
目前，相關本計畫的研究已有 2 篇會議
 1 
Efficiently Supporting Differentiated Services and Content-aware Request 
Distribution in Web Clusters Providing Multiple Services 
 
Chun-Hung Wu 
Dep. of Information Management 
National Chi-Nan University 
 Puli, NanTou 545, Taiwan, RoC 
yhung124@gmail.com 
 
Mei-Ling Chiang 
Dep. of Information Management 
National Chi-Nan University  
Puli, NanTou 545, Taiwan, RoC 
joanna@ncnu.edu.tw 
 
Tsung-Lin Lu 
Dep. of Information Management 
National Chi-Nan University  
Puli NanTou 545, Taiwan, RoC 
s97213501@ncnu.edu.tw 
 
ABSTRACT—In this paper, we have designed and implemented a 
kernel-level Web-based QoS (WQoS) mechanism that could 
efficiently support differentiated services when serving multiple 
diverse types of Web requests in a cluster-based Web server 
system. Our mechanism is implemented at kernel level to 
effectively reduce the number of protection domain switches and 
data copying between kernel space and user space to accelerate 
response time. We also use a new content-aware request distribution 
policy to dispatch requests efficiently in Web clusters providing 
multiple types of services. Experimental results demonstrate that 
the Web cluster with our proposed WQoS mechanism can not 
only efficiently serve multiple types of service requests but also 
ensure that all requests with high priority can conform to Service 
Level Agreement (SLA) by dropping acceptable percentage of 
requests with minor importance during system overload.  
Keywords-Web Cluster; Quality of Service; Content-aware 
Request Distribution; Cluster-based Systems; Web Server. 
1. INTRODUCTION 
In recent years, Web-based services have occupied a great 
portion of the Internet services. A Web cluster system is 
composed of a front-end request dispatching server also called 
a Web switch and several back-end request-handling servers. 
By distributing requests from clients to separate servers for 
load sharing and load balancing, Web clusters have been 
proved to be a cost-effective solution to serve the huge 
amount of service demands.  
Nowadays, electronic transactions and on-line services are 
important applications in the Internet. It’s essential to design a 
Web cluster that can provide differentiated services and treat 
important requests with preferred order. Traditionally, QoS 
mechanisms are used to achieve this goal. 
A lot of QoS-related researches have focused on the 
network framework. Nevertheless, traditional network QoS is 
not enough to support end-to-end QoS in Web servers because 
its mechanisms generally focus on traffic control of network 
devices such as router and switch; and it causes high-priority 
requests are sent to a Web server with a preferred sequence 
but dropped finally if the Web servers do not support end-to-
end QoS mechanisms. Hence, the Web servers should be 
enhanced with some mechanisms to provide end-to-end QoS 
and efficiently distinguish the contents of Web services. 
Besides providing end-to-end QoS, Web servers also have 
to handle more complex requests, since nowadays types of 
requests from clients have become much more diverse and 
requests may be mixed with dynamic Web pages, database 
processing, and multimedia stream data.  
In order to deal with a great amount of requests from 
Internet simultaneously and support differentiated services, 
it’s important to design a Web system that can not only deal 
with a huge amount of versatile requests simultaneously but 
also provide end-to-end QoS mechanism. 
In this paper, we have proposed and implemented our 
kernel-level WQoS control mechanisms on the LVS-CAD 
Web cluster [1] which is a Linux-based cluster-based Web 
server system including a front-end request-dispatching server 
and multiple back-end request-handling servers. LVS-CAD 
can analyze HTTP headers of request packets from clients and 
adopts intelligent or sophisticated dispatching policies. This 
platform is efficient because back-end servers can respond 
data directly to clients, bypassing the front-end server. 
Especially, the ingressive packets can be forwarded to the 
selected back-end server without any modification. 
With our proposed WQoS, LVS-CAD could effectively 
distinguish requests of Internet users with different priorities 
and give preferred requests better handling than normal 
requests to ensure lower response time even during system 
overload. Besides, to effectively support Web clusters providing 
multiple services, we apply a content-aware dispatching policy 
named Locality-Aware Request Distribution with Replication 
and Classification (LARD/RC) [3] in which requests of different 
types are dispatched to certain set of servers to consider the 
different server capabilities and to achieve performance isolation. 
Our mechanism and policy are implemented at kernel level, 
because kernel-level implementation can effectively reduce the 
number of protection domain switches and data copying between 
kernel space and user space and thus accelerate response time. 
We have deployed an E-commerce Web site on LVS-CAD 
Web cluster for experiments. Performance evaluation shows 
that the LVS-CAD with our proposed WQoS control 
mechanism with LARD/RC policy could efficiently serve 
multiple types of service requests and effectively distinguish 
requests of users from Internet with different priorities and 
ensure guaranteed and differentiated services.  
2. BACKGROUND AND RELATED WORK 
2.1 LVS and LVS-CAD Web Clusters 
Linux Virtual Server (LVS) [2] is a Web cluster consisting 
of a set of independent Linux-based servers, which acts as a 
 3 
 
Figure 2. The packet flow of LVS-CAD Web cluster with our WQoS 
mechanism. 
 
The Policy Agent implemented as a shell script is used to 
initialize the kernel modules in network layer and data-link 
layer. The rule table consists of many rules which record the 
acceptance rate of each request. The Policy Agent will be 
performed periodically to collect the connection statistics 
information and checks whether the system is overloaded. In 
our implementation, the Policy Agent will check whether the 
sum of the number of active connections of all back-end 
servers exceeds the threshold-over (THR-O) or not. If the 
value exceeds the THR-O, then the Policy Agent will enable 
WQoS mechanism and load the FILTER module and 
MARKER module into the corresponding location of Netfilter. 
If later the value is lower than the THR-O, the Policy Agent 
will disable the WQoS mechanism. The FILTER module is 
used to drop the surplus requests of low priority to prevent 
from system overloaded. 
Figure 2 shows the packet flow of LVS-CAD with the 
proposed WQoS mechanism. In the LVS-CAD, the fast 
handshaking module is hooked in front of the IPVS-CAD 
module which is modified from the IPVS module [2] of the 
LVS and used by the front-end server for scheduling and 
dispatching requests from clients to back-end servers at kernel 
level. So, when the incoming SYN packets are received by the 
front-end server, the fast handshaking module has to deal with 
three-way handshaking with the client before the SYN packets 
are forwarded by IPVS-CAD module. After the connections 
are established, when the back-end servers receive the HTTP 
requests from the front-end server for the first time, the back-
end servers will perform a fake three-way handshaking. Then 
the subsequent HTTP requests are handled by our WQoS 
mechanism in the front-end server. Once the WQoS 
mechanism is enabled, our FILTER module will also be 
hooked in front of the IPVS-CAD module. Then the MARKER 
module will be hooked after the IPVS-CAD module. A Multi-
Qdisc architecture is created by the QoS module and is 
located in the data-link layer to handle the outgoing HTTP 
requests forwarded to the target back-end servers later. 
 
Figure 3. The packet flow of our WQoS mechanism. 
 
Figure 3 shows the flow of our WQoS mechanism. At first, 
FILTER module fetches an incoming packet and checks 
whether the packet is a HTTP request. If it is not a HTTP 
request, then it is ignored. Otherwise, if it is a static-type 
request, such as images or static Web pages, then the system 
accepts it and passes it to the IPVS-CAD module directly. If it 
is a dynamic-type request, such as PHP, ASP, and CGI Web 
pages, we further examine whether the predefined acceptance 
rate is exceeded by current acceptance rate of requests. If the 
dynamic-type requests exceed the predefined acceptance rate, 
the FILTER module will drop them. If not, the system will 
accept them and pass them to IPVS-CAD module. 
After all requests are scheduled by IPVS-CAD module to 
proper back-end servers, the MARKER module will fetch 
requests on the way to the outgoing interface and examine the 
URL in HTTP headers of requests. Then according to the 
keyword of URL in HTTP headers of requests, the MARKER 
module assigns specific priorities to requests. When the 
requests arrive to data-link layer, they are enqueued to the 
corresponding queues of Multi-Qdisc data structures which 
are created by the QoS module. The requests are dequeued 
from Multi-Qdisc according to the feature of different queuing 
discipline to outgoing interface. 
3.2 Proposed Admission Control Mechanism 
For achieving admission control, we implement a FILTER 
module which can be used as a match module hooked in the 
filter table of Netfilter subsystem [10]. Netfilter is created  for 
filtering, mangling, and translating network packets. It 
consists of five hooks at various points of network layer. 
As shown in Figure 4, incoming packets pass the 
NF_IP_PRE_ROUTING (shortly called PREROUTING) 
hook point before being processed by the routing of the 
network layer. If the destination of packets is the local host, 
Netfilter is called again and packets are passed to the local host. 
Otherwise, they will pass the NF_IP_FORWARD (shortly 
called FORWARD) hook point. The NF_IP_LOCAL_OUT 
(shortly called OUTPUT) hook point is called for packets that 
are created locally. The packets then pass the final Netfilter 
hook point, the NF_IP_POST_ROUTING (shortly called 
POSTROUTING) hook point, before they leave network layer. 
 5 
 
Created Server Set of each request  
Figure 7. Dispatching Result of LARD/RC. 
 
3.4 LARD/RC Request Dispatching Policy 
Locality-Aware Request Distribution with Replication and 
Classification (LARD/RC) policy [3] based on LARD/R [8] is 
a content-aware request distribution policy. The goals of 
LARD/RC is to achieve better performance and load balancing 
among back-end servers especially for a Web cluster providing 
multiple types of services. Besides, LARD/RC policy can be 
used in both homogeneous and heterogeneous systems. 
In the implementation, we use Weighted Least-Connection 
(WLC) scheduling to assign a request to the proper server in 
the request’s server set. LARD/RC policy will maintain a table 
to record the relationship between each request and its 
responsible back-end servers. Only the server that has offered 
this type of service can be added into the server set of this 
request as the request-handling server.  
Figure 7 illustrates how LARD/RC scheduling works. We 
assume that the Web cluster has three back-end servers 
offering two different types of requests, both server A and 
server B can serve type-1 request and server C can serve only 
type-2 request, and the threshold of LARD/RC scheduling is 
set to 3. If the active number of connection exceeds the 
threshold, then the subsequent requests will be dispatched to 
another back-end server in which the number of active 
connections does not exceed the threshold. 
4. Performance Evaluation 
4.1 Experimental Environment 
 
Table 1. Hardware and software environment.  
Table 2. Three phases of E-commerce transactions. 
 
 
Table 1 shows that our experimental Web cluster includes 
front-end server, back-end servers and simulator servers. In 
addition, ten computers are used as clients to issue Web 
requests to our Web cluster. We use a commercial benchmark 
named SPECweb2005 [13] which is designed to measure the 
performance of a Web server that serves static and dynamic 
Web requests. We use the SPECweb_Ecommerce 
configuration as our workload. This workload is designed to 
simulate a popular Web site that sells computer systems, and 
this Web site allows clients to search, browse, customize, and 
purchase products. Table 2 shows three distinct phases involved 
in E-commerce transactions of clients in SPECweb_Ecommerce 
workload and the proportion of requests. 
4.2 Performance evaluation of WQoS mechanism 
We demonstrate that LVS-CAD Web cluster with our 
proposed WQoS mechanism can effectively provide 
differentiated services to different requests of clients and 
make sure our Web cluster can deal properly with the requests 
of high priorities. When Web cluster performs with our WQoS 
mechanism, all requests with high priorities are treated as a 
guaranteed service and the response times of them have to 
conform to SLA. The SLA of our experiments is set to two 
seconds, so the response times of all requests with high 
priorities have to be lower than two seconds.  
We need to determine the situation of system overloaded 
and then set the Threshold-Overload (THR-O) value as 
described in Section 3.1. In order to get THR-O value, we 
observe the response time of each kind of request during the 
whole experimental phases. Besides, two back-end servers 
handle only requests of dynamic type and six back-end servers 
handle only requests of static type.  
 
0
1
2
3
4
5
6
7
8
2
0
0
4
0
0
6
0
0
8
0
0
1
0
0
0
1
2
0
0
1
4
0
0
1
6
0
0
1
8
0
0
2
0
0
0
2
2
0
0
Number of Clients
R
es
p
o
n
se
 T
im
e 
(S
ec
) index
search
browse
product
billing
confirm
customize
 
Figure 8. Response time of various types of requests in LVS-CAD. 
 
 
Front-end 
server 
Back-end 
servers 
Simulators 
Clients 
Processor Intel P4 3.4GHz 
Intel P4 2.4GHz 
Intel P3 800 MHz 
Memory 
DDR 
1GB 
DDR 
384MB 
DDR 1GB DDR 256 MB 
NIC (Mbps) 
Intel Pro 
100/1000 
Intel Pro 
100/1000 
D-Link 
DGE-
530T 
Reltek RTL8139 
Intel Pro 100/1000 
D-Link DGE530T 
OS / Kernel Fedora core 6 Linux  2.6.18.1 Windows XP SP2 
IPVS 1.21 X X X 
Web Server X Apache 2.2.3 X 
Benchmark X X 
SPECweb
2005 
SPECweb2005 
Number of PCs 1 8 2 10 
                                                                             1
An Efficient Web Cluster with Content-Aware 
Request Distribution for Streaming Service 
 
Cheng-Wei Lin 
Department of Information Management 
National Chi Nan University 
Email: s96213514@ncnu.edu.tw 
 
Mei-Ling Chiang 
Department of Information Management 
National Chi Nan University 
Email: joanna@ncnu.edu.tw 
 
Chen-Yu Yang 
Department of Information Management 
National Chi Nan University 
Email: s97213503@ncnu.edu.tw 
Abstract―As the advance of the web and multimedia 
technologies, the web sites usually can provide not only static 
and dynamic web page services but also multimedia stream-
ing services. To serve various types and huge amount of ser-
vice demands, nowadays the web cluster system has been 
popularly deployed because of the advantages of cost effec-
tiveness, load sharing, scalability, and high availability.  
Traditionally, for providing the multimedia streaming ser-
vice, the server needs to establish a streaming connection with 
the client at first and then transmits the streaming data. The 
servers need to maintain the connection until the entire 
streaming service is finished. In the process of providing 
streaming service, the servers cannot handoff the existing 
connection to the other server to continue the streaming ser-
vice. This might cause load unbalance among servers and 
degrade the performance of the whole web cluster. In this 
paper, we have proposed and implemented the RTSP Handoff 
mechanism. Using our proposed mechanism, a long runtime 
film can be logically partitioned into several sections and 
each section of the film can be served by different real server. 
During the process, the client would not sense the change of 
servers. In this way, a web cluster can achieve better load 
balance when providing the streaming service. 
We have implemented our mechanism in the Linux kernel 
of the LVS-CAD web cluster. Experimental results demon-
strate that the LVS-CAD web cluster with our proposed RTSP 
Handoff mechanism can reduce 34.35% average response 
time and achieve 37.19% better throughput than the one 
without our proposed mechanism when providing multiple 
web services. 
Index Terms: Web Cluster, Multimedia Streaming, Con-
tent-aware Request Distribution 
1. Introduction 
With the fast development of internet and the 
advance of the web and multimedia technologies, 
the web services become more diverse. The web 
services are not limited to static pages, dynamic 
pages, and streaming services. To provide multiple 
web services needs more hardware resources, so 
the traditional single server is not sufficient to han-
dle such heavy workload. The web cluster that is 
composed of a front-end request dispatching server 
and several back-end request-handling servers has 
become a cost-effective way to serve huge amount 
of service demands, because of the advantages of 
load sharing and load balance, high performance, 
scalability, and high availability.     
For providing the multimedia streaming service, 
the streaming server needs to establish a streaming 
connection with the client and then starts to trans-
mit the streaming data. In general, no matter how 
long the movie runtime is, the server has to main-
tain the connection with the client until the entire 
streaming service is finished. For example, if a 
movie runtime is an hour, the streaming server 
needs to provide the streaming service and main-
tain the connection for an hour. However, the 
streaming server might be overloaded with heavy 
streaming workload from lots of clients. In the 
process of providing streaming service, the servers 
cannot handoff the existing connection to the other 
server to continue the streaming service. This might 
cause load unbalance among servers and degrade 
the performance of the whole web cluster. 
In this paper, we have proposed a new mechan-
ism named RTSP Handoff. Using our proposed 
mechanism, a long runtime film can be logically 
partitioned into several sections and each section of 
the film can be served by different real server. 
During the process, the client would not sense the 
change of servers. In this way, a web cluster can 
                                                                             3
(5) TEARDOWN request can terminate a 
streaming connection. 
2.1.2 Real Time Transport Protocol (RTP) 
This protocol is defined by the Audio-Video 
Transport Working Group of the IETF and first 
published in 1996 as RFC 1889 and superseded by 
RFC 3550 in 2003. It carries streaming media 
based on the UDP protocol to the client by the way 
of unicast, multicast or broadcast. Using the RTP 
protocol can obtain better efficiency and avoid the 
delay of movie playing. The RTP protocol does not 
provide the control mechanism for transmitting the 
streaming data. For this reason, the RTP Control 
Protocol (RTCP) could provide the control me-
chanism for the RTP packet flow. The client can 
send the RTCP packets periodically to inform the 
streaming server to change transfer rate dynamical-
ly. Figure 2 illustrates the RTP header format. 
 
Figure 2. The RTP header format 
 
Some important fields are listed as follow: 
(1) Payload type field  
This field indicates the data format of RTP pack-
et and determine the decoding method of RTP 
packet. The payload type of the video streaming 
data is 96 and the payload type of the audio 
streaming data is 97. When receiving the RTP 
packet, the multimedia player will distinguish the 
payload type corresponding to its value to decode 
it. 
(2) Sequence number field 
This field presents the order of RTP packets sent 
by streaming server. The initial value of the se-
quence number is generated in random and incre-
mented by one when each RTP packet is sent. The 
clients could use the sequence number to detect lost 
packets and reorder the order of RTP packets. 
(3) Timestamp field  
This field is used to make the client to playback 
the received RTP packet at appropriate time inter-
vals. The streaming server gives the timestamp 
corresponding to the time value in the film for each 
RTP packet. Because the RTP is a real-time transfer 
protocol, all received RTP packets must be 
processed immediately. If the timestamp value of 
the received RTP packet is delayed, the RTP packet 
would be dropped by client. 
2.1.3 Real Time Control Protocol (RTCP) 
The RTP Control Protocol (RTCP) is used to 
control the RTP packet flow. Its basic functionality 
and packet structure are defined in RFC 3550 su-
perseding its original standardization in 1996 (RFC 
1889). When the streaming connection is estab-
lished, the client will open two ports, one is used to 
receive the RTP packets, and the other is used to 
receive the RTCP packets. The client will periodi-
cally send RTCP packets to inform DSS the status 
of receiving video data, which includes the amount 
of packets received and the amount of lost packets. 
DSS can use the information to change transfer rate 
dynamically. 
2.1.4 MPEG-4 Part 14 (MP4) File Format 
MPEG-4 Part 14 is a multimedia container for-
mat standard specified as a part of MPEG-4 by 
ISO/IEC, which is based on Apple’s QuickTime 
container format. A MP4 file is basically comprised 
of video and audio streams.  
Note that before the streaming server start to 
transmit the media data, it must be encapsulated to 
streaming packets with a hint track. A hint track 
stores corresponding information into the packet 
with appropriate size and then sends to the client. 
Figure 3 shows the MP4 file structure. 
 
                                                                             5
2.3.2 Layer-7 Web Switch Mechanism 
The layer-7 web switch which works at applica-
tion level can support content-aware routing which 
is more sophisticated than content-blind request 
distribution to make the back-end servers achieve 
better load balancing. This routing mechanism is 
less efficient than the layer-4 web switch because it 
has to parse the content of the requests. In order to 
conduct content-aware request distribution, the 
front-end server needs to do three-way handshaking 
with the client for receiving the packet containing 
the HTTP content. 
 
Client
Front-end 
Server
Back-end 
Server
SYN
SYN
SYN,AC
K
ACK
ACK
PSH,ACK
PSH,ACK
ACK
PSH,AC
K
ACK
ACK
 
Figure 5. Packet forwarding flow of LVS with 
direct routing mechanism 
 
2.4 LVS-CAD Web Cluster 
2.4.1 Content-Aware Request Distribution 
Policy 
Our front-end server uses the content-aware dis-
patching policy named Grouped Client-Aware Pol-
icy (GCAP) [3] to select a back-end server to pro-
vide the streaming service. When the front-end 
server receives the notification packet which is sent 
by the original back-end server to handoff the cur-
rent streaming service, it uses the same dispatching 
policy to select the next back-end server for con-
tinuing the streaming service. 
The concept of GCAP is based on CAP [7]. Be-
cause different types of requests need different re-
sources, CAP classifies the requests from clients 
into four types including normal (N), CPU bound 
(CB), disk bound (DB), and disk and CPU bound 
(DCB) services. Each type of requests from clients 
will be dispatched to the proper back-end server by 
using the Round-Robin (RR) policy, so the 
back-end servers would handle each type of re-
quests evenly. In order to distinguish the processing 
capabilities of the back-end servers, the GCAP was 
proposed in our early work to limit the request 
types that each server could process. GCAP dis-
patches different types of requests to back-end 
servers with Weighted Round-Robin (WRR) sche-
duling, and each back-end server processes only its 
own types of requests.  
Figure 6 shows an example of GCAP policy. We 
assume there are three back-end servers in the 
cluster, and both server A and server B can serve 
type-1 requests while server C can serve only 
type-2 requests. When the front-end server receives 
the type-1 requests, it dispatches the type-1 re-
quests to the server A and server B based on WRR 
policy. Therefore, the server A and server B would 
handle type-1 requests evenly. Because only server 
C can serve type-2 requests, so the seventh, eighth, 
and ninth requests are handled by back-end server 
C. 
A
B
C
BE
FE
2
Clients
Request Sequence
Dispatch Result
11112 12 1
1
1
2
1
1
2 2
1
1
BA
C
Type of Requests 
that Servers serve
1
2
12
1
3456789
1
2
3
4
5
6
7 8 9
 
Figure 6. Request sequence and dispatching re-
sult of GCAP 
2.4.2 The architecture of LVS-CAD 
Figure 7 shows the architecture of our LVS-CAD 
                                                                             7
 
Figure 9. Processing Flow of RTSP Request 
3.1.2 Processing Flow of RTP Request 
After the streaming connection is set, the client 
would send the RTSP PLAY request to inform the 
back-end server to start providing the streaming 
service, and the back-end server replies the RTSP 
PLAY response packet to the client. At last, the 
back-end server transmits the video and audio data 
in RTP packets separately (Figure 10). 
 
Client
Front-end 
Server
Back-end 
Server
RTP-Video
RTP-Audio
RTSP DESCRIBE
RTSP DESCRIB
E reply
RTSP DESCRIBE
RTSP PLAY
RTSP  PLAY rep
ly
RTSP PLAY
RTSP SETUP
RTSP SETUP re
ply
RTSP SETUP
Figure 10. Processing Flow of RTP Request 
3.2 Front-end Server Implementation 
Due to the streaming connection cannot be dis-
connected before entire film finished, this may 
cause load imbalance when the back-end server is 
in heavy workload. For this reason, we design a 
method of the front-end server is to logically parti-
tion a film into several parts and handoff among 
back-end servers for lightening their workload. 
In principal, establishing a streaming connection 
only depends on the RTSP DESCRIBE and RTSP 
PLAY packets. The front-end server uses the client 
source IP address and port number to insert entries 
into the RTSP hash table, and determines whether 
the streaming connection needs to be migrated by 
the length of the selected film, i.e. whether the film 
needs to be partitioned. 
Figure 11 demonstrate the operation of migrating 
the existing streaming connection. Before the han-
doff occurs, the old back-end server sent a notifica-
tion packet to inform the front-end server to select 
a new back-end server to continue the streaming 
service. The notification packet includes the current 
client source IP address and source port number. 
Then, the front-end will copy the two packets and 
send them to the new back-end server for rebuild-
ing the existing connection. According the content 
of RTSP PLAY packet, the new back-end server 
can transmit the film with the modified time range 
of the playing movie to the client. The client would 
not feel any interrupt during the whole process. The 
workflow of RTSP Handoff mechanism shows in 
Figure 12. 
 
Internet
Requests
Web cluster
Front-end Server 
Internal Network
Real Server 2
Real Server 3
Real Server n
Real Server 1
Clients
Responses
1. 
RT
SP
 re
qu
est
2 H
an
do
ff 
de
ma
nd
3. The copied 
RTSP packets
Web cluster
cli
en
t IP
, p
ort
 
nu
mb
er
 
Figure 11. The operation of migrating the exist-
ing streaming connection 
                                                                             9
We also added the checkpoint by modifying 
function ip_finish_output() in Linux kernel for 
RTSP rebuilding as shown in Figure 14. The in-
coming packet (step 1) has to be examined to 
whether it is a duplicate RTSP reply packet. If it’s 
not, the packet is delivered to the client (step 2.a). 
Otherwise (step 2.b), if it is the RTSP SETUP reply 
packet (step 3.b), the back-end server should create 
a RTSP hash table entry then store the new session 
key and RTP port number. In order to avoid return-
ing the duplicate reply packet to the client, the 
back-end server drop it at all (step 3.a or 4). 
 
Kernel Layer
1
Is a duplicated 
reply packet?
Is the RTSP SETUP 
reply packet?
Creat a RTSP 
hash table entry, 
store new session key and 
RTP port number 
Drop the 
reply packet
Yes
Yes
3.b
Yes
4
Modified ip_finish_output()
No
2.a
No
3.a
2.b
drop
Multimedia Player
InternetInternet
Client
Back-end
server
 
Figure 14. Packet flow in ip_finish_output() 
function in the back-end server 
3.3.1 Handoff the Existing Streaming Ser-
vice 
As the back-end server initializing a RTP 
streaming connection to transfer the film, it will 
generate a corresponding initial sequence number 
in the first sent RTP packet. Because of the length 
of films are fixed, we can easily know the end se-
quence number of the last RTP packet. Actually, the 
client could get the initial number by the RTSP re-
ply packet. Thus, the back-end server fetches the 
RTSP PLAY reply packet at the kernel layer, it uses 
the RTP port number to create the RTP hash table 
entry and then store initial sequence number of the 
RTP packet into the RTP hash table.  
The back-end server uses the end sequence 
number of the RTP packet to determine whether it 
should send a notification packet to the front-end 
server or not. When the sequence number of the 
packet approaches the end sequence number of the 
RTP packet, the back-end server would send the 
notification packet to the front-end server to han-
doff the existing streaming connection. The content 
of the notification packet includes the client IP ad-
dress and the RTSP port number. 
4. Performance Evaluation 
4.1 Benchmark and Workload of SPECweb2005 
SPECweb2005 is a performance benchmark tool 
developed by the Standard Performance Evaluation 
Corporation (SPEC). It is composed of three sys-
tems: a web server, an application server, and over 
than one web client simulators. The benchmark is 
run on many clients that use port 80 to send HTTP 
requests to the web server. All clients are controlled 
by one prime client that is also a normal client. In 
our experiment, we construct a web cluster instead 
of a single server to serve clients’ requests. In addi-
tion, we use the simulator servers to simulate the 
application servers, such as database servers.  
SPECweb2005 benchmark generates a number 
of client connections that correspond to the setting 
of the number of simultaneous sessions in the con-
figuration files. The clients will continuously send 
requests to web cluster system. A new user session 
starts as soon as the previous user session is over, 
and this process continues until the whole bench-
mark is completed. 
There are three frames of SPECweb2005 work-
load: Banking, e-Commerce, and Support. They are 
designed to measure the performance of static and 
dynamic web services, and we selected SPEC-
web_Ecommerce as our workload. This workload 
was developed by analyzing log files as actual 
E-commerce sites, as well as browsing popular web 
stores to gather statistics such as average page size, 
access frequencies, and capturing form data that a 
customer typically fills out when purchasing prod-
ucts. As using the E-commerce workload in 
SPECweb2005, the server has to hold session in-
formation. For this purpose, we set up a cache 
server by Memcached software on the front-end 
server to share the session information with 
                                                                             11 
cluster provides all films with our proposed RTSP 
Handoff mechanism. The X-axis denotes the num-
ber of clients simultaneously issuing requests in 
each phase of experiment. The Y-axis denotes the 
total amounts of web files transferred on MBytes in 
each phase of experiment. 
In this experiment, the web cluster using our 
proposed RTSP Handoff mechanism with “50% 
Handoff” can achieve the best performance, and 
outperforms the one without using RTSP Handoff 
mechanism by 12.85-29.55%. We also found that 
excessive RTSP Handoffs could degrade the per-
formance of the whole web cluster. 
8000
9000
10000
11000
12000
13000
14000
15000
16000
17000
1
0
0
0
1
2
0
0
1
4
0
0
1
6
0
0
1
8
0
0
Number of Clients
T
o
ta
l 
a
m
o
u
n
t 
o
f 
w
e
b
 f
il
e
s 
tr
a
n
sf
e
rr
e
d
 (
M
B
)
without RTSP Handoff 50% Handoff 100% Handoff
 
Figure 15. Performance of the LVS-LAD under 
synthetic workload 
 
 
0
2
4
6
8
10
12
14
1
0
0
0
1
2
0
0
1
4
0
0
1
6
0
0
1
8
0
0
Number of Clients
R
e
sp
o
n
se
 T
im
e
(s
e
c
)
without RTSP Handoff 50% Handoff 100% Handoff
 
Figure 16. Average response time of LVS-CAD 
under synthetic workload 
Figure 16 shows the average response time of 
LVS-CAD using the RTSP Handoff mechanism, 
CAP policy, and different settings. The X-axis 
means the number of clients simultaneously issuing 
requests in each phase of experiment. The Y-axis 
means the average response time (second) in each 
phase of experiment. In this experiment, the web 
cluster using our proposed RTSP Handoff mechan-
ism with “50% Handoff” can obtain the smallest 
average response time, and outperforms the one 
without using RTSP Handoff mechanism by 
6.94-34.59%. Excessive RTSP handoffs could also 
increase the average response time of requests dur-
ing the experiment. 
4.3.2 Heavy Streaming Workload 
For demonstrating the web cluster can gain bet-
ter performance using our RTSP Handoff mechan-
ism while processing the heavy streaming workload, 
we increase the amount of streaming requests in the 
synthetic workload used in previous section in this 
experiment. Each client watches two streaming 
movies at the same time, and it will send a new 
streaming request to our web cluster as soon as the 
previous streaming service is finished. Therefore, 
each client will send a total of twenty-eight 
streaming requests during one phase of experiment. 
All films’ runtime which a client requests is two 
minutes. We set the playing time of the movie 
which needs handoff to be one minute.  
Figure 17 shows the experimental results of the 
LVS-CAD using RTSP Handoff mechanism, GCAP 
policy, and different settings. In this experiment, 
there are one third of all films which need to be 
handed off in the web cluster. In this experiment, 
the web cluster using our proposed RTSP Handoff 
mechanism can achieve the better performance, and 
outperforms the one without using RTSP Handoff 
mechanism by 10.99-37.19%. 
Figure 18 shows the average response time of 
LVS-CAD with the RTSP Handoff mechanism, 
CAP policy, and different settings. In this experi-
ment, the web cluster using our proposed RTSP 
Handoff mechanism can achieve the less average 
response time, and outperforms the one without 
using RTSP Handoff mechanism by 15.54-34.35%. 
出席國際學術會議心得報告 
                                                             
計畫編號 NSC 98-2221-E-260-010- 
計畫名稱 叢集式網路伺服器支援 Web 服務品質與多媒體影音串流服務的研究與製作 
出國人員姓名 呂宗霖  
服務機關
及職稱 
國立暨南國際大學資訊管理學系  
碩士生兼任助理 
會議時間地點 99/04/20~99/04/23 會議地點 Perth, Australia 
會議名稱 
(中文) 第 24 屆 IEEE 前瞻資訊網路與應用國際研討會 
(英文) 24th IEEE International Conference on Advanced Information Networking 
and Applications Workshops 
發表論文題目 
(中文)有效率的支援差異性服務及瞭解內容分配機制於提供多樣性服務的
Web叢集系統 
(英文)Efficiently Supporting Differentiated Services and Content-aware Request  
Distribution in Web Clusters Providing Multiple Services 
 
一、參加會議經過 
此會議行程共計 4 天, 參加會議詳細經過如下: 
日期 說明 
4/20 上午至會場完成 registration 手續，並參加 industry Keynote，中午參加餐敍，下
午參加Wireless Sensor Networks 1 Session及Mobile Networks and Applications 1 
Session。 
4/21 上午參加 Keynote Speech 與 Industry Plenary，中午參加餐敍，下午參加
Advanced Network Systems 1 Session 與 Internet Computing and Application 2 
Session，晚上參加 Reception Party，與各國友人聯誼，並交換心得。 
4/22 上午參加 Keynote Speech 與 Ad Hoc and Sensor Networks 2 Session，中午餐敍，
下午參加Ad Hoc and Sensor Networks 2 Session以及Wireless Sensor Networks 3 
Session，晚上參加大會舉辦的 Gala dinner，與各國友人聯誼，並交換心得。 
4/23 上午參加 Keynote Speech 與 WAMIS 3 Session，中午餐敍，下午參加 WAMIS4 
Session，並於會議中發表 1 篇論文，與會討論熱烈。 
 
二、與會心得 
IEEE Advanced Information Networking & Applications (IEEE AINA)會議主要專注在電腦
網路與應用的議題上，並且與許多的 Workshop 合辦，像是 Frontiers of Information Systems and 
Network Applications(FINA)和 Web and Mobile Information Services (WAMIS)等等。其會議的主
題包括近年來熱門的無線感測網路、RFID、雲端運算、通訊協定、資訊安全等等，同時並邀
請多位學界、業界人士進行專題演講。另外此會議還舉辦了 Reception Party 和 Gala dinner，
讓各國友人可以互相聯誼，並交換想法。 
 
在這為期四天的會議中，學生受益良多。在專業知識方面，此會議中出席了許多的專家
     Efficiently Supporting Differentiated Services and Content-aware Request 
       Distribution in Web Clusters Providing Multiple Services  
 
Chun-Hung Wu 
Dep. of Information Management 
National Chi-Nan University 
 Puli, NanTou 545, Taiwan, RoC 
yhung124@gmail.com 
 
Mei-Ling Chiang 
Dep. of Information Management 
National Chi-Nan University  
Puli, NanTou 545, Taiwan, RoC 
joanna@ncnu.edu.tw 
 
Tsung-Lin Lu 
Dep. of Information Management 
National Chi-Nan University  
Puli NanTou 545, Taiwan, RoC 
s97213501@ncnu.edu.tw 
 
ABSTRACT—In this paper, we have designed and implemented a 
kernel-level Web-based QoS (WQoS) mechanism that could 
efficiently support differentiated services when serving multiple 
diverse types of Web requests in a cluster-based Web server 
system. Our mechanism is implemented at kernel level to 
effectively reduce the number of protection domain switches and 
data copying between kernel space and user space to accelerate 
response time. We also use a new content-aware request distribution 
policy to dispatch requests efficiently in Web clusters providing 
multiple types of services. Experimental results demonstrate that 
the Web cluster with our proposed WQoS mechanism can not 
only efficiently serve multiple types of service requests but also 
ensure that all requests with high priority can conform to Service 
Level Agreement (SLA) by dropping acceptable percentage of 
requests with minor importance during system overload.  
Keywords-Web Cluster; Quality of Service; Content-aware 
Request Distribution; Cluster-based Systems; Web Server. 
1. INTRODUCTION 
In recent years, Web-based services have occupied a great 
portion of the Internet services. A Web cluster system is 
composed of a front-end request dispatching server also called 
a Web switch and several back-end request-handling servers. 
By distributing requests from clients to separate servers for 
load sharing and load balancing, Web clusters have been 
proved to be a cost-effective solution to serve the huge 
amount of service demands.  
Nowadays, electronic transactions and on-line services are 
important applications in the Internet. It’s essential to design a 
Web cluster that can provide differentiated services and treat 
important requests with preferred order. Traditionally, QoS 
mechanisms are used to achieve this goal. 
A lot of QoS-related researches have focused on the 
network framework. Nevertheless, traditional network QoS is 
not enough to support end-to-end QoS in Web servers because 
its mechanisms generally focus on traffic control of network 
devices such as router and switch; and it causes high-priority 
requests are sent to a Web server with a preferred sequence 
but dropped finally if the Web servers do not support end-to-
end QoS mechanisms. Hence, the Web servers should be 
enhanced with some mechanisms to provide end-to-end QoS 
and efficiently distinguish the contents of Web services. 
Besides providing end-to-end QoS, Web servers also have 
to handle more complex requests, since nowadays types of 
requests from clients have become much more diverse and 
requests may be mixed with dynamic Web pages, database 
processing, and multimedia stream data.  
In order to deal with a great amount of requests from 
Internet simultaneously and support differentiated services, 
it’s important to design a Web system that can not only deal 
with a huge amount of versatile requests simultaneously but 
also provide end-to-end QoS mechanism. 
In this paper, we have proposed and implemented our 
kernel-level WQoS control mechanisms on the LVS-CAD 
Web cluster [1] which is a Linux-based cluster-based Web 
server system including a front-end request-dispatching server 
and multiple back-end request-handling servers. LVS-CAD 
can analyze HTTP headers of request packets from clients and 
adopts intelligent or sophisticated dispatching policies. This 
platform is efficient because back-end servers can respond 
data directly to clients, bypassing the front-end server. 
Especially, the ingressive packets can be forwarded to the 
selected back-end server without any modification. 
With our proposed WQoS, LVS-CAD could effectively 
distinguish requests of Internet users with different priorities 
and give preferred requests better handling than normal 
requests to ensure lower response time even during system 
overload. Besides, to effectively support Web clusters providing 
multiple services, we apply a content-aware dispatching policy 
named Locality-Aware Request Distribution with Replication 
and Classification (LARD/RC) [3] in which requests of different 
types are dispatched to certain set of servers to consider the 
different server capabilities and to achieve performance isolation. 
Our mechanism and policy are implemented at kernel level, 
because kernel-level implementation can effectively reduce the 
number of protection domain switches and data copying between 
kernel space and user space and thus accelerate response time. 
We have deployed an E-commerce Web site on LVS-CAD 
Web cluster for experiments. Performance evaluation shows 
that the LVS-CAD with our proposed WQoS control 
mechanism with LARD/RC policy could efficiently serve 
multiple types of service requests and effectively distinguish 
requests of users from Internet with different priorities and 
ensure guaranteed and differentiated services.  
2. BACKGROUND AND RELATED WORK 
2.1 LVS and LVS-CAD Web Clusters 
Linux Virtual Server (LVS) [2] is a Web cluster consisting 
of a set of independent Linux-based servers, which acts as a 
2010 IEEE 24th International Conference on Advanced Information Networking and Applications Workshops
978-0-7695-4019-1/10 $26.00 © 2010 IEEE
DOI 10.1109/WAINA.2010.181
473
  
Figure 2. The packet flow of LVS-CAD Web cluster with our WQoS 
mechanism. 
 
The Policy Agent implemented as a shell script is used to 
initialize the kernel modules in network layer and data-link 
layer. The rule table consists of many rules which record the 
acceptance rate of each request. The Policy Agent will be 
performed periodically to collect the connection statistics 
information and checks whether the system is overloaded. In 
our implementation, the Policy Agent will check whether the 
sum of the number of active connections of all back-end 
servers exceeds the threshold-over (THR-O) or not. If the 
value exceeds the THR-O, then the Policy Agent will enable 
WQoS mechanism and load the FILTER module and 
MARKER module into the corresponding location of Netfilter. 
If later the value is lower than the THR-O, the Policy Agent 
will disable the WQoS mechanism. The FILTER module is 
used to drop the surplus requests of low priority to prevent 
from system overloaded. 
Figure 2 shows the packet flow of LVS-CAD with the 
proposed WQoS mechanism. In the LVS-CAD, the fast 
handshaking module is hooked in front of the IPVS-CAD 
module which is modified from the IPVS module [2] of the 
LVS and used by the front-end server for scheduling and 
dispatching requests from clients to back-end servers at kernel 
level. So, when the incoming SYN packets are received by the 
front-end server, the fast handshaking module has to deal with 
three-way handshaking with the client before the SYN packets 
are forwarded by IPVS-CAD module. After the connections 
are established, when the back-end servers receive the HTTP 
requests from the front-end server for the first time, the back-
end servers will perform a fake three-way handshaking. Then 
the subsequent HTTP requests are handled by our WQoS 
mechanism in the front-end server. Once the WQoS 
mechanism is enabled, our FILTER module will also be 
hooked in front of the IPVS-CAD module. Then the MARKER 
module will be hooked after the IPVS-CAD module. A Multi-
Qdisc architecture is created by the QoS module and is 
located in the data-link layer to handle the outgoing HTTP 
requests forwarded to the target back-end servers later. 
 
Figure 3. The packet flow of our WQoS mechanism. 
 
Figure 3 shows the flow of our WQoS mechanism. At first, 
FILTER module fetches an incoming packet and checks 
whether the packet is a HTTP request. If it is not a HTTP 
request, then it is ignored. Otherwise, if it is a static-type 
request, such as images or static Web pages, then the system 
accepts it and passes it to the IPVS-CAD module directly. If it 
is a dynamic-type request, such as PHP, ASP, and CGI Web 
pages, we further examine whether the predefined acceptance 
rate is exceeded by current acceptance rate of requests. If the 
dynamic-type requests exceed the predefined acceptance rate, 
the FILTER module will drop them. If not, the system will 
accept them and pass them to IPVS-CAD module. 
After all requests are scheduled by IPVS-CAD module to 
proper back-end servers, the MARKER module will fetch 
requests on the way to the outgoing interface and examine the 
URL in HTTP headers of requests. Then according to the 
keyword of URL in HTTP headers of requests, the MARKER 
module assigns specific priorities to requests. When the 
requests arrive to data-link layer, they are enqueued to the 
corresponding queues of Multi-Qdisc data structures which 
are created by the QoS module. The requests are dequeued 
from Multi-Qdisc according to the feature of different queuing 
discipline to outgoing interface. 
3.2 Proposed Admission Control Mechanism 
For achieving admission control, we implement a FILTER 
module which can be used as a match module hooked in the 
filter table of Netfilter subsystem [10]. Netfilter is created  for 
filtering, mangling, and translating network packets. It 
consists of five hooks at various points of network layer. 
As shown in Figure 4, incoming packets pass the 
NF_IP_PRE_ROUTING (shortly called PREROUTING) 
hook point before being processed by the routing of the 
network layer. If the destination of packets is the local host, 
Netfilter is called again and packets are passed to the local host. 
Otherwise, they will pass the NF_IP_FORWARD (shortly 
called FORWARD) hook point. The NF_IP_LOCAL_OUT 
(shortly called OUTPUT) hook point is called for packets that 
are created locally. The packets then pass the final Netfilter 
hook point, the NF_IP_POST_ROUTING (shortly called 
POSTROUTING) hook point, before they leave network layer. 
475
  
jGzGzGGG  
Figure 7. Dispatching Result of LARD/RC. 
 
3.4 LARD/RC Request Dispatching Policy 
Locality-Aware Request Distribution with Replication and 
Classification (LARD/RC) policy [3] based on LARD/R [8] is 
a content-aware request distribution policy. The goals of 
LARD/RC is to achieve better performance and load balancing 
among back-end servers especially for a Web cluster providing 
multiple types of services. Besides, LARD/RC policy can be 
used in both homogeneous and heterogeneous systems. 
In the implementation, we use Weighted Least-Connection 
(WLC) scheduling to assign a request to the proper server in 
the request’s server set. LARD/RC policy will maintain a table 
to record the relationship between each request and its 
responsible back-end servers. Only the server that has offered 
this type of service can be added into the server set of this 
request as the request-handling server.  
Figure 7 illustrates how LARD/RC scheduling works. We 
assume that the Web cluster has three back-end servers 
offering two different types of requests, both server A and 
server B can serve type-1 request and server C can serve only 
type-2 request, and the threshold of LARD/RC scheduling is 
set to 3. If the active number of connection exceeds the 
threshold, then the subsequent requests will be dispatched to 
another back-end server in which the number of active 
connections does not exceed the threshold. 
4. Performance Evaluation 
4.1 Experimental Environment 
 
Table 1. Hardware and software environment.  
Table 2. Three phases of E-commerce transactions. 
 
 
Table 1 shows that our experimental Web cluster includes 
front-end server, back-end servers and simulator servers. In 
addition, ten computers are used as clients to issue Web 
requests to our Web cluster. We use a commercial benchmark 
named SPECweb2005 [13] which is designed to measure the 
performance of a Web server that serves static and dynamic 
Web requests. We use the SPECweb_Ecommerce 
configuration as our workload. This workload is designed to 
simulate a popular Web site that sells computer systems, and 
this Web site allows clients to search, browse, customize, and 
purchase products. Table 2 shows three distinct phases involved 
in E-commerce transactions of clients in SPECweb_Ecommerce 
workload and the proportion of requests. 
4.2 Performance evaluation of WQoS mechanism 
We demonstrate that LVS-CAD Web cluster with our 
proposed WQoS mechanism can effectively provide 
differentiated services to different requests of clients and 
make sure our Web cluster can deal properly with the requests 
of high priorities. When Web cluster performs with our WQoS 
mechanism, all requests with high priorities are treated as a 
guaranteed service and the response times of them have to 
conform to SLA. The SLA of our experiments is set to two 
seconds, so the response times of all requests with high 
priorities have to be lower than two seconds.  
We need to determine the situation of system overloaded 
and then set the Threshold-Overload (THR-O) value as 
described in Section 3.1. In order to get THR-O value, we 
observe the response time of each kind of request during the 
whole experimental phases. Besides, two back-end servers 
handle only requests of dynamic type and six back-end servers 
handle only requests of static type.  
 
0
1
2
3
4
5
6
7
8
20
0
40
0
60
0
80
0
10
00
12
00
14
00
16
00
18
00
20
00
22
00
ŏŶŮţŦųġŰŧġńŭŪŦůŵŴ
R
es
po
ns
e 
Ti
m
e 
(S
ec
) index
search
browse
product
billing
confirm
customize
 
Figure 8. Response time of various types of requests in LVS-CAD. 
 
 Front-end server 
Back-end 
servers 
Simulators Clients 
Processor Intel P4 3.4GHz Intel P4 2.4GHz Intel P3 800 MHz 
Memory DDR 1GB 
DDR 
384MB DDR 1GB DDR 256 MB 
NIC (Mbps) Intel Pro 100/1000 
Intel Pro 
100/1000 
D-Link 
DGE-
530T 
Reltek RTL8139 
Intel Pro 100/1000 
D-Link DGE530T 
OS / Kernel Fedora core 6 Linux  2.6.18.1 Windows XP SP2 
IPVS 1.21 X X X 
Web Server X Apache 2.2.3 X 
Benchmark X X SPECweb2005 SPECweb2005 
Number of PCs 1 8 2 10 
477
國科會補助計畫衍生研發成果推廣資料表
日期:2010/11/03
國科會補助計畫
計畫名稱: 叢集式網路伺服器支援 Web 服務品質與多媒體影音串流服務的研究與製作
計畫主持人: 姜美玲
計畫編號: 98-2221-E-260-010- 學門領域: 計算機結構與計算機系統
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
