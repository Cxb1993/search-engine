  
A REAL-TIME VEHICLE COUNTING ALGORITHM FOR TRAFFIC 
SURVEILLANCE SYSTEM 
 
Shen-Chuan Tai (戴顯權), Zhi-Hua Xu (許芝華), Chih-Huan Wu(吳志桓) 
 Institute of Computer & Communication, Dept. of Electrical Engineering, National Cheng 
Kung University 
E-mail:sctai@ee.ncku.edu.tw 
 
 
ABSTRACT 
 
Many surveillance and vehicle counting researches 
propose complex equation to solve the problem of 
occluded vehicles at a high accuracy rate of 95%. 
However, it costs too much time to do this job in real 
time. In this paper, a surveillance system with correctly 
detecting and segmenting the occluded vehicles is 
proposed to count vehicles in multiple-vehicle 
occlusions in real time. First of all, the system gets 
rough foreground by pre-process. According to the 
difference of saturation and brightness between the 
foreground pixel and the corresponding background 
pixel, self-shadows in foreground are removed. If the 
occlusion is happened, the system will segment the 
foreground object immediately. Finally, the relation of 
successive frames and centric matching strategy is used 
to find out the correspondence of regions. Experimental 
results show that the system can not only get correct 
vehicle count even in vehicle occlusion but also in real 
time. 
 
1. INTRODUCTION 
 
The surveillance systems are in great demand in many 
places such as plaza, roads, office buildings, hospitals, 
banks, and prison. The surveillance system basically is 
classified into background estimation, foreground 
region detection, and moving object tracking. There are 
a lot of methods to train the background model, such as 
averaging the images over time, median filter [1][2], 
Gaussian [3][4], and Kalman filter-based adaptive 
background modeling [5].  
Averaging the images over time is the most 
common way to create a background model. However, 
if there are vehicles moving slowly, the vehicle will be 
classified into background image. Pixel-wise median 
filter [2] preserves fixed pixels from the video sequence 
to generate background by assuming that vehicles 
should not stay at the same place for a long time. The 
background needs to be updated every time. Gaussian is 
performed wonderful while it has a period of empty 
scene for training. Nevertheless, it cannot deal with 
background noise such as electronic noise of video-
capture devices, the light illumination, and noisy motion 
which is moving element of scene. As for Kalman filter-
based adaptive background modeling, the background 
update quite relies on the previous one, so the slowly 
moving object may be included in background and the 
object is enlarged obviously by tailing phenomena. 
Background subtraction [6] [7], frame difference 
[8], and optical flow [9][10] are methods to extract the 
foreground region from the background. Background 
subtraction is the most common way for detecting the 
foreground region in video sequence because of its low 
complexity and good result. Frame difference is a pixel-
based subtraction in adjacent frames method, and it set a 
difference threshold to separate foreground pixels from 
background pixels. Optical flow is a popular method of 
motion segmentation, which achieves very good result 
in crowded scene. However, with the large 
computational quantity, much time to find out an 
optimal solution, and cannot be implemented in real-
time, the optical flow is not fit for the traffic jam. 
Changick Kim and Jenq-Neng Hwang [11] 
proposed an object tracking algorithm according to 
smoothness of motion trajectory and assuming the 
motion of each moving object is piecewise linear. Harini 
Veeraraghavan et al. [12] present a multi-level that 
combines low-level image based blob with high-level 
Kalman filter to estimate position and shape of new 
moving objects. 
In this paper, we presented a real time surveillance 
algorithm that solves vehicle occlusion and counts 
vehicle correctly. This paper is organized as follows. 
Section 2 introduces the related algorithms applied in 
surveillance system including background estimation, 
foreground region detection, and the application of 
surveillance systems. Section 3 describes the detail of 
the proposed algorithm. Section 4 presents the 
experimental results of the proposed system. The 
conclusion is given in Section 5. 
 
2. RELATED WORKS 
 
  
 
 
Fig. 2: Flowchart of proposed method. 
 
The HSI color domain is considered due to hue and 
saturation represent chromaticity and intensity stands 
for brightness. Because shadow must be darker than the 
corresponding background model, the system only 
chooses saturation to represent chromaticity for 
reducing computing time. The computing formula is as 
follows 
 
         || boI IID −=                      (3.1) 
|| boS SSD −=                         (3.2) 
 
where ܫ௢ is intensity of foreground object pixel and ܫ௕ is 
intensity of the corresponding background model pixel. 
ܵ௢  is saturation of foreground object pixel, and ܵ௕  is 
saturation of the corresponding background model pixel. 
For each pixel of foreground object, the difference 
values, ܦூ and ܦௌ, are compared with the corresponding 
background model. If ܦூ and ܦௌ of a pixel are all lower 
than specific threshold, the pixel is considered as 
shadow.  Otherwise it is classified into moving object. 
 
  
Fig. 3: Flowchart of occlusion processing. 
 
3.2. Occlusion processing 
 
First, the system normalizes each moving object. Then, 
according to physical property, the system can decide 
whether each moving object is in occlusion or not. If the 
moving object is occluded, the system finds out which 
kind of occlusion does it has. Finally, all occluded 
moving object is segmented. The flowchart of occlusion 
processing is shown in Fig. 3. 
 
3.2.1. Normalization 
 
Due to the position of camera, the width and length of a 
moving object are changed according to the location of 
the object in the scene. Hence, normalizing width and 
length is needed to avoid error measurement. In the 
proposed system, the width and length of each moving 
object is multiplied with the coefficient generated by the 
location of moving object in the scene. 
 
3.2.2. Detection of Occlusion 
 
According to normalized width and length, the system 
can know which one is motorcycle and which one is car. 
After all moving objects are classified into two 
categories, the system begins to detect occlusion of each 
foreground region. It is noticed that the area ratio of an 
occluded moving object is different from the area ratio 
of an individual moving object. In other words, the 
density of an occluded moving object is smaller than 
that of an individual moving object. Therefore, the 
system adopts area ratio as first condition to determine 
whether the moving object is occluded or not. The 
function is given as follows 
 
AreaBoxBounding
AreaForegroundAR
   
  )(Area Ratio =            (3.3) 
⎩⎨
⎧ <=
.otherwise      0
      1 a
ar
TAR
f                                 (3.4) 
 
If ܣܴ  is smaller than threshold ௔ܶ , the moving 
object is occluded. Otherwise it is individual. Certainly, 
AR of motorcycle differs from AR of car.  
 
3.2.3. Classification of occlusion 
 
The first kind of occlusion is that two or more vehicles 
line up vertically. The length of occluded moving 
objects is longer than width in this situation, so the ratio 
of length to width is different from the original ratio. 
The function is given as follows 
 
Width
LengthLRRatio LW =)(                    (3.7) 
⎩⎨
⎧ >=
else      
TLR      
f llw 0
1 .                               (3.8) 
 
  
one frame splits into two or more objects in the current 
frame. Then, the system calculates the distance between 
the object which is not matched in the current frame and 
those objects which have merged in i successive frames. 
Similarly, if the minimum distance is less than a 
specified threshold, the object is matched. 
The third situation is that the object is new in the 
current frame. If the object which is not matched in the 
current frame does not fit in with two situations above, 
the system classifies the object into new category. The 
flowchart is shown in Fig. 4. 
 
3.3.3. Delayed backward tracking 
 
After confirming the situation of every moving object in 
the current frame, the system needs to know the 
situation of every moving object in the previous one 
frame. Similarly, the object which is not matched in the 
previous one frame is in one of the following three 
situations. 
 
 
Fig. 5: Flowchart of delayed backward tracking. 
 
The first situation is that the object is outside the 
detected scene in the current frame. Because the 
information of far sight object is lack, the system sets up 
a detected region. If the moving object is not in detected 
scene, the system does not have to track it. Therefore, 
the moving object which is not matched in the previous 
one frame is determined in this situation. 
The second situation is that there are two or more 
objects merged in the current frame. According to the 
difference of length between the object which is 
matched in the current frame and the corresponding 
object in the previous one frame, it shows that there are 
some objects merged in the current frame. 
The third situation is that the object disappears in 
the current frame. It means that the object disappears in 
the detected region. Since the foreground region is not 
extracted perfectly or the object is covered with other 
thing, such as vehicle, tree, and bill board, the system 
cannot detect the corresponding object in the current 
frame. The flowchart of delayed backward tracking is 
shown in Fig. 5. 
 
4. EXPERIMENTAL RESULTS 
 
In this paper, the proposed traffic surveillance system 
can remove shadow, track moving objects and segment 
occluded moving objects. The frame size of tested 
sequences is SIF (352*240). The proposed surveillance 
system is implemented with C++ code and tested at the 
environment of Windows XP, 1GB RAM, and 2.4GHz 
CPU. 
 
 
Fig. 6: Shadow removal result. (a) Input frame. (b) 
Shadow detected by Zeng. (c) Shadow detected by the 
proposed method.  
 
Fig. 7: Proposed segmentation result. (a) Track occluded 
vehicles without segmentation. (b) Successfully 
segment occluded vehicles by proposed method. (c)(d) 
Another example of segmenting occluded vehicles. 
 
Fig. 6 shows the example of shadow detected by 
the proposed method and Zeng[13]. One can see that the 
false-positive detection inside the object is reduced. The 
comparison results are shown in Table 1 while the 
Current Rate is the rate of correctly detecting shadow. 
 Fig. 7 shows the results of the proposed 
segmentation method that segments the occluded 
vehicles correctly. As shown in Table 2, our 
surveillance system is also tested in five video 
sequences and shows the high accuracy of vehicle 
counting and processing speed which demonstrates the 
real-time ability.  
(c) (d) 
(b) (a) 
(a)
(b) (c) 
   
 
 
 
 
出席國際學術會議心得報告 
 
一、會議時程  
–ORAL SESSION– , Multimedia Processing and Multimedia Interaction 
10:20-12:10, Aug. 19, 2009. 
二、與會心得 
本次參加在西安舉辦的2009年信息安全與保證國際研討會(The Fifth International Conference 
on Information Assurance and Security (IAS-2009))，主辦單位為西安電子科技大學(Xidian 
University, China)以及高雄應用科技大學(Kaohsiung University of Applied Sciences, Taiwan)，
由國際電子電機工程師協會(The Institute of Electrical and Electronics Engineer, IEEE )協辦。 
本篇報告安排於19日上午。論文主題為 ” Analysis and Evaluation of Contrast Enhancement methods 
in Digital Images”，並且於中場休息時與國內外參與本會的學者交換研究心得，收穫良多。會議上討論
到不少影像相關的處理技術，並且與其他國家的教授學生交換意見，本次會議的主軸範圍很廣，會議
內容涵蓋 Information Assurance, Security Mechanisms, Methodologies and Models、Secure System 
Architectures and Security Application 、 Image Engineering, Multimedia Signal Processing and 
Communication Security等三大領域，對於了解影像處理或訊號處理相關領域的研究有相當大的幫助。  
總結此次會議包含了網路通訊及訊息安全等各個相關研究領域，既滿足個人研究領域上的需要，
也開拓了研究視野，是非常值得參與的一個會議。本人收穫良多，也期待未來有機會再次參與相關的
國際會議。 
計畫編號  NSC 97-2221-E-006-187- 
計畫名稱 全天候.全自動交通監視追蹤系統 
出國人員姓名服務
機關及職稱 
戴顯權 國立成功大學電機系教授 
蔡定洲 碩士班學生 
會議時間地點  Aug. 15-21, 2009, Sanya, Hainan, China  
會議名稱  The Fifth International Conference on Information Assurance and Security(IAS-2009)  
發表論文題目  Analysis and Evaluation of Contrast Enhancement methods in Digital Images 
  
The surveillance system first extracts the interesting 
regions from the original frame. The system needs a 
robust background model estimation method so as to 
obtain complete interesting foreground regions. To 
segment the background, the proposed system adapts 
background subtraction which is simple and costs less 
time to recognize background region and foreground 
region. 
 
2.1. Background estimation 
 
If any moving object exists in the scene, the noises are 
caused and these noises are classified into background 
image. Therefore, pixel-wise mode filter background 
image estimation is considered. Pixel-wise mode filter is 
based on modeling the statistics of every pixel in each 
frame. Every ninety frames as a group is a training data 
set and one frame is taken in every three frames for 
building the background model. Totally thirty frames 
are extracted in each group. The pixels arrange in the 
same group according to their position (x,y) in each 
frame, as shown in Fig. 1. The pixel value from 0 to 255 
is equally divided into 32 bins. The system counts the 
number of pixel in the same group that divide into the 
same bins. The bin with the maximum counting is 
selected. 
 
 
Fig. 1: Background estimation. 
 
Then, the value of pixels in the same group of the 
selected bin is averaged. This final pixel value in each 
position (x,y) is used to build a background model. The 
function is given by 
1
2
3
1
01
2
02
3
03
1( , ) ( ), ( ), 0,...,
1( , ) ( ), ( ), 0,...,
1( , ) ( ), ( ), 0,...,
n
b i i p
i
n
b i i p
i
n
b i i p
i
R x y x x S R i n
n
G x y x x S G i n
n
B x y x x S B i n
n
=
=
=
⎧ = ∀ ∈ =⎪⎪⎪⎪ = ∀ ∈ =⎨⎪⎪⎪ = ∀ ∈ =⎪⎩
∑
∑
∑
 
 
where ܵ௣ሺܴሻ, ܵ௣ሺܩሻ, ܵ௣ሺܤሻ are the selected bins and ݊ଵ, 
݊ଶ, ݊ଷ are the number of counting values of the selected 
bins. ݔ௜ is the value of pixels in the same group of the 
selected bin. ܴ௕ሺݔ, ݕሻ ,  ܩ௕ሺݔ, ݕሻ ,  ܤ௕ሺݔ, ݕሻ  are the 
average RGB of the selected bin and they are used to be 
the value of background pixel of position ሺݔ, ݕሻ. 
 
2.2. Foreground region detection 
 
Color feature is a way to find out the foreground regions. 
Since the color of foreground regions usually is 
different from background model, the system calculates 
color different between foreground regions and 
background model to discriminate which one the pixel 
belongs to. It is noted that human have different 
sensitivity to the RGB colors, especially the color green, 
which is the most sensitive color to human eyes. 
According to different sensitivity of each color, the 
system assigns different weights. The difference value is 
defined as follows 
 
0.25 R R 0.5 G G 0.25 B Bc b f b f b fdiff = − + − + −  
 
where ݂݀݅ ௖݂  is the difference value. When ݂݀݅ ௖݂  is 
greater than a specific threshold, the pixel is classified 
to foreground region. Otherwise it is considered as 
background. ܴ௕ , ܩ௕ , and ܤ௕  represent RGB color of 
background while ௙ܴ , ܩ௙ , and ܤ௙  represent RGB color 
of foreground, separately. 
 
2.3. Shadow removal 
 
The extracted foreground regions may not be good 
enough sometimes. Therefore, the system needs to do 
some morphological operations such as size filtering, 
shadow removal, opening, and closing to make the 
foreground regions more precise and more complete.  
Because of the small foreground regions caused by 
the noise, tree swaying, small trash, or other conditions 
change, a size filter is used to remove these small 
foreground regions. In addition, the foreground regions 
must include shadow which is caused by sun light and 
streetlamps. Therefore, shadow removal is also needed. 
Zeng et al. [13] use R, G, and intensity to find out 
the shadow region. They use difference of R, G, and 
intensity separately between foreground and background 
to decide if the pixel belongs to shadow region. But they 
cannot get high accuracy and cost lots of time, we 
improve this method. 
 
3. PROPOSED METHOD 
 
In Fig. 2, the blocks with color are our proposed method. 
We divide proposed method into three parts to discuss: 
shadow removal, occlusion processing and object 
tracking.  
 
3.1. Proposed shadow removal 
 
Removing shadow in foreground regions, chromaticity 
and brightness are important variations between moving 
objects and shadow. The proposed shadow removal that 
is introduced as follows is modified from Zeng[13].  
  
If ܮܴ  is larger than threshold ௟ܶ , the occluded 
moving object is classified as the first kind of occlusion. 
Similar to AR, ௟ܶ  is different between motorcycle and 
car. The second kind of occlusion is that two or more 
vehicles move side by side. This case is like the first 
kind of occlusion but different from that vehicles are in 
parallel line. Similarly, the system calculates the ratio of 
width to length. The function is given as follows 
 
Length
WidthWRRatio WL =)(                    (3.9) 
⎩⎨
⎧ >=
else      
TWR      
f wwl 0
1 .                           (3.10) 
 
If WR is larger than threshold ௪ܶ , the occluded 
moving object is classified as the second kind of 
occlusion. Similarly, motorcycle’s ௪ܶ  is different from 
car’s. The third kind of occlusion is that two or more 
vehicles are in neither vertical nor parallel line. 
 
3.2.4. Segmentation of occlusion 
 
According to different kinds of occluded moving object, 
the system adopts individual method for each case to 
separate occluded foreground regions into individual 
one. By observing the first kind of foreground region, 
we find out that the connection between two vehicles is 
separable. Therefore, the system counts the number of 
pixel of each row in the three fourths middle foreground 
region, and searches out which row has least pixels. 
Then, the system assigns this row as demarcation line to 
divide this region into two parts. Furthermore, the 
system renews a bounding box of each part of region for 
getting their accuracy width and length. 
The second situation of occlusion is like the first 
one. The difference is that the first situation is vertical 
occlusion and the second one is parallel occlusion. 
Therefore, the corresponding methods of the two 
situations are almost the same. But it counts the number 
of pixels of each column in the three fourths middle 
foreground region. 
The third situation of occlusion is not like first one 
and second one, so we adopt individual method to deal 
with the problem. The system finds the boundary of 
moving object from bottom to top in every column. The 
system compares the vertical position of every two 
adjacent boundary pixels. According to the difference of 
vertical position of adjacent pixels, the system decides 
do these two pixels belong to different moving object. 
Thus, the system obtains one boundary lines between 
two moving objects. Then, the boundary line is used for 
separating this region into two areas. After getting the 
bottom boundary of upper moving object, the system 
needs to find out the left and right boundary line of 
upper moving object. The system counts the number of 
pixels of each column of upper bounding box from the 
top to down in the three fourths middle foreground 
region. Then, finding least number of pixels of column 
to be left or right boundary and the system attains new 
bounding box of moving object. The other bounding 
box of moving object is obtained by the rest foreground 
region. 
 
3.3. Object tracking 
 
After moving objects are obtained, the system begins to 
find out the correspondence between the detected 
objects frame by frame. Consequently, when the moving 
object appears in the detected region, the system tracks 
it and obtains its trajectory of moving object. 
 
3.3.1. Centroid matching 
 
The concept of this method is that the position of object 
in the next frame is in the neighborhood of its current 
position. Accordingly, the system measures the distance 
between the center point of bounding box of current 
object and all objects in the last frame 1−nF . If the 
minimum distance is less than a specified threshold, the 
matching object is found. 
 
3.3.2. Forward tracking 
 
The system uses centroid matching in each frame, so it 
knows the moving object in the current frame is 
matched or not. If the object is matched, the system gets 
the trajectory of moving object and knows this object is 
static. If the object is not matched, it may be in one of 
the following three situations. 
 
 
Fig. 4: Flowchart of forward tracking. 
 
The first situation is that the object has disappeared 
in i successive frames and turns out in the current frame. 
The system firstly finds out whether there have some 
objects in this situation or not if there is an object cannot 
be matched in this current frame. Then, the system 
calculates the distance between the object which is not 
matched in the current frame and all objects which has 
disappeared in i successive frames. If the minimum 
distance divides i frames is less than a specified 
threshold, the object is matched. 
The second situation is that the object splits in the 
current frame. Before forward tracking, the system 
compares the length of the object which is matched in 
the current frame and in the previous one frame. If the 
length of the object in the previous one frame is more 
than that in the current frame, the object in the previous 
  
Table 1: Comparison of shadow removal methods.  
 Current 
Rate 
(%) 
Processing 
Speed 
(frame/sec)
Zeng[13] 71.8 54 
Proposed 78.3 70 
 
Table 2: The results of vehicle counting in video 
sequences by the proposed surveillance system. 
 Sequence 
Length 
(frames) 
Accuracy 
Rate 
(%) 
Processing 
Speed 
(frame/sec)
Seq. 1 329 100 24.298 
Seq. 2 135 100 22.388 
Seq. 3 367 96.15 25.120 
Seq. 4 280 100 25.271 
Seq. 5 371 91.67 24.472 
5. CONCLUSION 
 
In this paper, an automatic traffic surveillance system is 
proposed to segment occlusion and track moving objects. 
After obtaining the foreground region, shadows in 
foreground regions are detected by the difference of 
saturation and brightness between the foreground pixel 
and the corresponding background pixel. Therefore, 
shadows are discarded from foreground regions. Then, 
according to physical property of vehicles, the occluded 
moving objects are detected and segmented. The system 
tracks each moving object through comparing their 
centroid of bounding box frame by frame. The 
experimental results show that the proposed system has 
high accuracy.  
 
6. ACKNOWLEDGMENT 
 
This work was supported by the National Science 
Council, Taiwan, R.O.C. under the Grant No. NSC 97-
2221-E-006-187 
 
7. REFERENCES 
[1] B.P.L. Lo and S.A. Velastin, “Automatic Congestion 
Detection System for Underground Platforms,” Proc. 
Int’l Symp. Intelligent Multimedia, Video, and Speech 
Processing, pp. 158-161, 2000. 
[2] Q. Zhou and J.K. Aggarwal, “Tracking and Classifying 
Moving Objects from Video,” Proceedings of IEEE Int. 
Workshop on PETS, 2001. 
[3] C. Stauffer, W. E. L. Grimson “Adaptive background 
mixture models for real-time tracking,” 1999. IEEE 
Computer Society Conference on Computer Vision and 
Pattern Recognition, Volume 2, June 1999. 
[4] I. Haritaoglu, David Harwood, and L. S. Davis, 
“W4:Real-time surveillance of people and their 
activities,” IEEE Trans. Pattern Analysis and Machine 
intelligence, 22(8):809–830, August 2000. 
[5] Klaus-Peter Karmann, Achim von Brandt, “Moving 
Object Recognition Using an Adaptive Background 
Memory,” Time-Varying Image Processing and 
Moving Object Recognition, 2, Elservier, Amsterdam, 
The Netherlands,1990. 
[6] Monnet A., Mittal A., et al, “Background modeling and 
subtraction of dynamic scenes,” Proceedings of Ninth 
IEEE International Conference on Computer Vision, 
2003. 
[7] Wang L., Tan T., “Silhouette analysis-based gait 
recognition for human identification,” IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence, 2003. 
[8] Suchendra M., Bhandarkar and Xingzhi Luo, “Fast and 
Robust Background Updating for Real-time Traffic 
Surveillance and Monitoring,” Computer Society 
Conference on Computer Vision and Pattern 
Recognition, 2005. 
[9] Juan L, Mattthew M, Naveed S. Performance of passive 
ranging from image flow. IEEE, ICIP, 2003. 
[10] Sasa G, Loncaric S. Spatio-temporal image 
segmentation using optical flow and clustering 
algorithm. Proceedings of the First International 
Workshop on Image and Signal Processing and 
Analysis, 2000. 
[11] C. Kim and J-N Hwang, “Fast and automatic video 
object segmentation and tracking for content-based 
applications,” IEEE transaction on circuits and systems 
for video technology, vol. 12, Feb. 2002.  
[12] Harini Veerarahavan, Osama Masoud, and Nikolaos P. 
Papanikolopoulos, “Computer Vision Algorithm for 
Intersection Monitoring,” IEEE transaction on 
intelligent transportation systems, 2003. 
[13] Hui-Chi Zeng and Shang-Hong Lai, “Adaptive 
Foreground Object Extraction for Real-time Video 
Surveillance with Lighting Variations,” IEEE 
international conference, 2007 
