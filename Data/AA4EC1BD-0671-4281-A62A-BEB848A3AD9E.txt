II
中文摘要
在此三年期的計畫的最後一年中,我們發展了一些從影像重建三維物體模型的方法。這些方
法中包含了從環狀運動的物體(或相機)所拍攝的多張影像，利用物體輪廓重建物體的三維
模型以及從單張人臉影像，重建三維人臉模型並同時估測人臉表情變化。其中，還發展了
三維非剛體配準的演算法。以及，利用重建三維人臉模型的理論，發展了以影片為基礎的
人臉識別系統。實驗結果充份驗證了這些演算法的表現，而這些演算法也已分別被國際會
議接受。
關鍵詞：三維重建, 相機自動校正, 環狀運動, 三維人臉重建, 三維人臉表情變形, 非剛性
三維對位, 人臉識別。
Abstract
In the final year of this project, we have developed some methods for 3D modeling. These
methods include silhouette-based reconstruction from multiple views taken under circular motion,
reconstruction 3D face geometry and estimation the facial expression from a single 2D face
image, and the 3D non-rigid registration algorithm. Besides, based on the face modeling
algorithm, we also develop a video-based face recognition system. The experimental results
demonstrate the performance of these proposed algorithms. Also, these works have been
accepted by international conferences.
Keywords: 3D Reconstruction, Auto-Calibration, Circular Motion, 3D Face Modeling, Facial
Expression, Manifold, Non-rigid Registration, Face Recognition.
41. Introduction
The main goal of this project is to develop
robust and efficient algorithms for 3D
reconstruction from multi-viewed images. In
the final year of this 3-year project, we have
developed two 3D reconstruction systems,
which are the silhouette-based 3D
reconstruction system under circular motion
and the face modeling system with facial
expression, and one 3D non-rigid
registration algorithm. Besides, based on the
face modeling algorithm, we also developed
a video-based face recognition system.
These works have been accepted by the
international conferences.
The rest sections of this report are
organized as follows: In the section 2, we
will introduce the first system that is to
reconstruct 3D object from multiple images
taken under circular motion. The second
system, which is to model 3D face and
estimate the facial expression, will be
presented in Section 4. The algorithm of 3D
non-rigid registration is shown in Section 3.
We introduce the video-based face
recognition system, which is based on the
3D face modeling algorithm of Section 4, in
Section 5. Final, the conclusion is given in
Section 6.
2. Silhouette-Based Camera Calibration
from Sparse Views under Circular
Motion
Camera calibration from uncalibrated image
sequences without using any specific
calibration patterns can be classified into
two camps; namely, the feature-based and
silhouette-based approaches. In the
feature-based approach, structure from
motion (SfM) algorithm determines the
camera information and the 3D structure of
the object simultaneously from the feature
correspondences. In the silhouette-based
approach, the cameras are first calibrated
from the object silhouettes and then the
volumetric description, intersection or the
image-based visual hull (IBVH) techniques
are applied for object modeling.
It takes more efforts to calibrate cameras
in general motion from silhouettes than
feature points. Additional constraints on
camera motion, such as circular motion, can
facilitate the camera calibration. It may seem
to be a restriction to assume the camera
motion to be circular motion. However, in
real applications, such as 3D digitization in
digital museum, 3D object reconstruction
from a circular motion sequence is a
practical and widely used approach for
generating 3D models and a number of 3D
reconstruction methods focused on circular
motion have been proposed, including the
feature-based and silhouette-based methods.
In this paper, we deal with the problem
of camera calibration from silhouettes under
circular motion. Different from previous
silhouette-based methods based on surface
of revolution (SoR) [H. Zhang et al.,
2005][G. Zhang et al., 2006] , the proposed
approach can be applied with very few
sparsely located views to calibrate the
cameras. We introduce the concept of mirror
symmetry property [K. Forbes et al.,
2006][P.-H. Huang and S.-H. Lai, 2006] into
circular motion and derive the property that
images under circular motion share a
common homography that relates silhouettes
with epipoles. With this property, epipoles
can be located directly from the silhouettes
and then the remaining camera parameters
can be recovered. This work is focused on
6and SV2.
Similarly, camera V2 can be regarded as
the reflection of camera C2 according to
another miror plane Πn. To be more specific,
we enlarge this part in Figure 2.2(b). In this
figure, Πc and Πv are the image planes of
cameras C and V, respectively. Because of
the coincident camera center of cameras C
and V, silhouettes SC and SV can be
transformed with each other via a
non-singular 3x3 homography H [R. Hartley
and A. Zisserman, 2000].
From mathematical derivation, the
camera projection matrices for cameras C
and V can be expressed as:
 ]|[ TRKRP yC   (2.2)
 ]|[ TRKRP yV   (2.3)
where Σ = diag([-1 1 1]).
Suppose the projections of an object
point X onto cameras C and V are denoted
by xc and xv, respectively. The relation
between the corresponding image points can
be expressed as:
cc
T
v HxxKRKRx  1~ (2.4)
The homography H only involves the
constant camera intrinsic parameters and
camera pose. Thus, all cameras under
circular motion share the common
homography.
Substituting equation (2.4) into
equation (2.1) leads to
  0212  CCTC Hxxe (2.5)
Similarly, we can derive the following
relation
  0121  CCTC Hxxe (2.6)
For any two views under circular
motion, a pair of correspondence points with
two epipoles can provide two linear
constraints on H according to equation (2.5)
and (2.6). With enough constraints, H can be
linearly determined by the least-square
estimation. On the other hand, given the
homography H, a silhouette can be
transformed to its reflected silhouette.
Therefore, the epipoles of any two views can
be determined from the bi-tangent lines of
one of the silhouettes and the transformed
silhouette of the other one.
2.2. Camera Calibration
If the epipoles are available, the tangent
points of silhouettes induced by the epipoles
are considered as point correspondences, and
equation (2.8) and (2.9) are used to compute
the homography H. One way to extract the
epipoles from object silhouettes can be
found in [P. R. S. Mendonca, and R. Cipolla,
1999].
If the epipoles are not available, a
low-dimensional search of the optimal H in
a bounded region is performed. As given in
equation (2.4), H contains four parameters
which are focal length, denoted by f, and
three rotation angles, denoted by θx, θy and
θz, respectively, of the camera pose. In fact,
H is independent of the rotation angle of
x-axis because of the following relation:
     Txxxx RR  (2.7)
Thus, the number of unknowns of H is
reduced to three.
Under circular motion, all cameras are
positioned on the same plane and their
projections, which are epipoles, are co-linear.
The homography H can hence be determined
by minimizing the distance from the
epipoles to the co-linear line, lh, and the cost
8and then minimized the epipolar tangency
constraints [K.-Y. K. Wong et al., 2002] for
all pairs of views to refine the camera
parameters. The first real-data experiment is
on the Fox sequence, which contains 12
images of size 640x480 pixels and the angle
between images is about 30 degrees (may
deviate a little bit due to mechanical error).
Some example views are depicted in Figure
2.3. In this case, the epipoles are first
determined from the method in [P. R. S.
Mendonca, and R. Cipolla, 1999], and then
used to compute the homography and the
remaining camera parameters. Sample views
of the reconstructed 3D model and the
estimated angles are shown in Figure 2.4
and 2.5, respectively
The second experiment is on the crystal
apple sequence as depicted in Figure 2.6.
Notice that the object is made of glass and it
is almost impossible to establish feature
correspondences in this case. In this part, six
images of size 640x480 pixels with different
interval angles are chosen. The estimated
results are shown in Figure 2.7 and 2.8.
Figure 2.8 shows the ground truth angles in
black and the reconstructed values in white.
We can see the estimated angles are quite
close to the true angles.
The proposed method is also compared
with the methods proposed in [K.-Y. K.
Wong et al., 2002] and [H. Zhang et al.,
2005] in the following experiments. First,
the head image sequence in [K.-Y. K. Wong
et al., 2002] was used for reconstruction.
The sample views of the reconstructed
model and the estimated angles are shown in
Figure 2.9(a) and 2.10, respectively. In [K.-Y.
K. Wong et al., 2002], the focal length was
calibrated off-line, while the proposed
method auto-calibrated the camera. The
RMS error of interval angles in [K.-Y. K.
Wong et al., 2002] is 0.2131o and it is
0.1015o by using the proposed method.
Figure 2.8: The estimated interval angles between
images.
Figure 2.7: Sample views of the reconstructed Apple
model.
Figure 2.6: Example images in the crystal apple
sequence.
Figure 2.5: The estimated angles for the Fox sequence.
Figure 2.4: Sample views of the reconstructed Fox
model.
Figure 2.3:Example images in the Fox sequence.
10
functions from two implicit surfaces over a
source cell. For the MPU implicit surface [Y.
Ohtake et al., 2003], the local shape function
Q(x) may be a general quadric, a bivariate
quadratic polynomial or a piecewise
quadratic function, depending on the local
surface structure. The quadric function is
given by
cQ TT  xbAxxx)( (3.1)
where the parameter A is a symmetric 3x3
matrix, b is a 3x1 matrix, and c is a constant.
Based on the original octree structure of
the source model, a local affine transform
p(x) is used as the transformation for a leaf
cell of the source surface. The affine
transformation p(x) is represented by Kx + h,
where K is a 3x3 matrix and h is a 3x1
vector. The data energy is composed of
many integral terms with each measuring the
difference between the local shape function
of the transformed source surface in one cell
and that of the target surface in the
corresponding cell given as follows:
   
i
S
i
T
icorrdiff dQpQpE xxx
2
)( )()( (3.2)
In addition, the difference between
affine transformations for adjacent cells is
combined into the energy function for
smoothness. The smoothness energy Es is
simply a weighted summation over the
norms of the differences between adjacent
affine transform parameters, i.e. (Ki - Kj)
and (hi - hj), given by
 


ACji
FjiFjis
pE
),(
22
hhKK  , (3.3)
where AC is the set of all pairs of adjacent
cells, and  and  are positive constants.
A modified gradient descent
minimization method is employed to
minimize this energy for non-rigid
registration. The local affine transform
parameters that minimize the total energy
are integrated to determine the deformation
function. In the final step, we blend all local
affine transformations into a continuous
deformation function.
There are several advantages for the
proposed 3D non-rigid registration
algorithm for MPU implicit surfaces. First
of all, our algorithm is directly applied to
implicit surfaces. It does not need to sample
Figure 3.2: Surface registration example. The first row
shows source surface and target surface. The initial pose is
shown in the second row. The third row shows the
registered result
(a) (b)
Figure 3.1: Two examples, (a) Geom1 and (b) Geom2, of
registration on open surfaces. The first row shows source
surfaces and target surfaces. The initial poses are shown in
the second row. The third row shows the registered results.
12
very popular topic with many applications,
such as facial animation, face recognition,
model-based facial video communication,
etc. Previous works on 3D head modeling
from a single face image utilized prior
information on 3D head models. However, it
is difficult to accurately reconstruct the 3D
face model from a single face image with
expression since the facial expression
induces 3D face model deformation in a
complex manner. The main challenge is the
coupling of the neutral 3D face model and
the 3D deformation due to expression, thus
making the 3D model estimation from a
single face image with expression very
diffcult. In this paper, we propose a 3D face
model reconstruction algorithm that can
recover the 3D face model as well as the 3D
expressional deformation from a single face
image with expression. Our algorithm
integrates the linear and non-linear subspace
representations for a prior 3D neutral
morphable model and the probabilistic
manifold-based 3D expressional
deformation.
In this work, we propose an algorithm to
reconstruct a 3D face model with the
associated expressional deformation directly
from a single 2D face image with expression
based on linear and non-linear subspace
analysis. We propose a novel approach
which combines probabilistic manifold
embedding and prior 3D eigen-head model
to perform this task. The manifold is not
constructed to represent the whole
expressional 3D models, but only model the
deformation from neutral 3D face to the
corresponding expressional 3D face. The
parameters estimated during the
reconstruction process are sampled from the
probability distribution on the neutral
eigenface space as well as the
low-dimensional expression manifold space
and they minimize the total-image
re-projection error simultaneously.
In summary, the contributions of this
proposed approach are listed as follows:
First, we propose to reconstruct the
complete 3D face model with expression
deformation from a single image without
existing action unit, motion field, or any
expression style assumption. Second, a
probabilistic nonlinear 3D expression
manifold is learned from a large set, more
than 1000, of 3D facial expression models to
represent the general deformations from
facial expressions. The manifold reduces the
complexity of the reconstruction problem
and therefore makes it easier and more
robust to track the continuous expression
deformation in image sequence. Third, from
the learned expression manifold, the
reconstructed 3D face model can be
re-targeted to any expression content, style
or even mixture of them in a more general
way. The global view of our training and
reconstruction process is shown in Figure
4.1.
4.1. Morphable Model and Spherical
Harmonics Illumination
We first describe the morphable model used
for 3D face model reconstruction from a
single face image. The mophable model
Figure 4.1: The framework of our 3D expression
reconstruction.
14
There are several techniques proposed
recently to infer the intrinsic structure of the
non-linear data. After some experiments, our
results show that locally linear embedding
(LLE) and Laplacian eigenmaps are suitable
for modeling the expression deformations.
For our training data set, LLE outperforms
the Laplacian eigenmaps. In this section, we
present how to build a probabilistic model in
a non-linear low-dimensional manifold for
representing of the 3D deformation due to
facial expression.
We employ the LLE [S. Roweis and L.
Saul, 2000] to achieve a low-dimensional
non-linear embedding of M expression
deformations fpis obtained by registering
between the 3D models of a subject with and
without expression in the BU-3DFE
database [L. Yin et al., 2006]:
(4.4)
where
is a set of feature points representing the i-th
3D face geometry with facial expression,
and similarly, fpNiS denotes the set of feature
points of the corresponding 3D neutral face.
In our experiment, we used 83 feature
landmarks for each face scan obtained from
the database BU-3DFE, as shown in Fig.
4.2(a). The collection of M 3D expression
deformations, denoted by fpis for i =
1,…,M, including the happy, sad and
surprise expressions, are embedded to M 2D
points on the manifold space. Fig. 4.3(a)
shows the embedded two-dimensional
manifold space.
As described above, M training data sets
are projected onto a 2D manifold, including
different magnitude, content, and styles of
expressions. Each expression of a certain
individual forms a trajectory in the manifold
2D space. In order to represent the
distribution of expression deformations,
Gaussian Mixture Model (GMM) is used to
approximate the probability distribution of
the 3D expression deformation in this
low-dimensional expression manifold as
follows:
(4.5)
where ωc is the probability of being in
cluster c, 0 < ωc < 1 and 11 Cc c , μc
and C are the mean and covariance
matrix for the c-th Gaussian distribution.
The expectation maximization (EM)
algorithm is employed to compute the
maximum likelihood estimation of the
model parameters from the training data. We
apply the EM-based Gaussian mixtures
estimation [J. Bilmes and A. Gentle, 1998]
to build the unsupervised GMM in the LLE
manifold space for 3D expression
deformations. The colorful dots shown in
Fig. 4.3(a) indicate the distribution of the
manifold and the probability distribution of
the estimated GMM in the expression
Figure 4.3: Low-dimensional manifold representation of
expression deformations. (a) 3D expression deformations
are projected onto the 2D expression manifold. (b) The
probability distribution of the estimated GMM.ampling
and smoothing.
16
Since the magnitude, content, and styles of
expressions are all embedded into the
low-dimensional expression manifold, the
only parameters for facial expression are the
coordinate of sLLE. The initial sLLE is set to
(0, 0.01), which is located at the common
border of different expressions on the
expression manifold.
After the initialization step, all the
parameters are iteratively optimized in two
steps which will be described in details
subsequently.
4.3.1. Texture and Illumination Update
To recover the texture and illumination, we
need to estimate texture coefficient vector β
and then determine the illumination bases B
and the corresponding spherical harmonic
(SH) coefficient vector . From Eq. (4.2),
spherical harmonic bases B are determined
by the surface normal n and texture intensity
T(β). Therefore, with the surface normal n
determined from the current 3D face model,
the texture coefficient vector βand the SH
coefficient vector  can be estimated by
solving the non-linear optimization problem:
(4.10)
According to different reflection
properties in the face feature area and skin
area, we define these two areas for more
accurate texture and illumination estimation.
Since the feature area is less sensitive to
lighting variations, texture coefficient vector
βare estimated based on minimizing the
intensity errors for the vertices in the face
feature area. On the other hand, the SH
coefficient vector  is determined by
minimizing the image intensity errors in the
skin area.
4.3.2. 3D Face Shape Update
In this step, the inter and intra face
deformation is estimated from the
photometric approximation with the
estimated texture parameters obtained from
the previous step. Since we have the statistic
models of facial geometry and deformation,
the shape parameters α, expression
parameters LLEsˆ and pose vector
 tRf ,, can be estimated by
maximizing the posterior probability(MAP)
with a given input image Iinput and the
texture parametersβ. Similar with [V. Blanz
and T. Vetter, 2003], we neglect the
correlations between these parameters and
the posterior probability can be written as
follows:
(4.11)
where
(4.12)
where I is the standard deviation of the
image synthesis error, and
  NeLLEs 3:ˆ  is a nonlinear mapping
function that maps the estimated LLEsˆ from
the embedded space with dimension e = 2 to
the original 3D deformation space with
dimension 3N. Although the embedded
manifold is globally nonlinear, it is
constructed based on a neighbor-preserving
mapping. Therefore, we use the nonlinear
mapping function of the following form:
(4.13)
where NB( LLEsˆ ) is the set of nearest
neighbor training data points to LLEsˆ on the
expression manifold, ks , is the 3D
deformation vector for the k-th facial
18
happiness, sadness, and surprise, under
arbitrary illumination conditions. The
images are rendered with OpenGL, as
depicted in Fig. 4.5, and used as the input
images for the 3D reconstruction.
For more quantitative measurement, we
calculate the average error from the 3D
differences of all vertices and normalize the
error by the size, edgecube, of the bounding
cube containing the 3D face model:
(4.15)
where jvˆ is the j-th 3D vertex of the
reconstructed face model, and vjg is the j-th
3D vertex of the ground truth. In this
experiment, the proposed algorithm is also
compared with the PCA-based method. For
a fair comparison, the PCA model is built
from the face models with neutral, happiness,
sadness, and surprise expressions and the
same optimization scheme is applied for the
PCA method.
The reconstructed expressional 3D faces
and the models after expression removal (i.e.
the neutral part S(α)) are shown in Fig. 4.5.
The average errors between the
reconstructed 3D model and the ground truth
are computed with Eq. (4.15) at every
updating step. By optimizing the
photometric approximation in the 2D image,
the error of our 3D reconstruction is
monotonically decreased as the updating
steps increase. Comparatively, the results of
the PCA method shown in Fig. 4.5 are not
stable and the reconstructed 3D face models
sometimes are not satisfactory. The unstable
phenomenon of the PCA-based method in
Fig. 4.5(b) may be caused by the unsuitable
linear modeling of the manifold structured
expression deformation, which leads to
ambiguity in the optimization process.
4.4.2. Experiments on Real Images
We used the CMU-PIE data set [T. Sim et
al., 2003], which contains face images under
different poses, illumination and facial
expressions, to test the proposed algorithm.
The facial expressions in CMU-PIE include
smile, blink and talk, and all the face images
with smile expressions are used in our
experiments. For sadness and surprise
expressions, we selected some frames of
these two expressions in FG-Net database
[FG-Net] as the input images in our
experiments.
Most of the testing images got high
probabilities for the corresponding
expressions. Some of the reconstructed
models and the corresponding probability
distributions of different expressions are
shown in Fig. 4.6. The bar graphs in Fig. 4.6
show the estimated probabilities for the
expression modeling on the learned
manifold, which also demonstrate the
accuracy of facial expression estimation.
The second and third rows show the
Figure 4.5: 3D expression reconstruction from a single
simulated face image. (a) The first column shows the input
images and the second column are the results of PCA-based
method. The third and the fourth columns represent the
reconstructed 3D face models with expression, and those
after expression removal by our proposed algorithm. (b)
The curves are the average differences between
reconstructed 3D face model and the ground truth along the
updating steps with two different methods.
20
techniques have been widely used for robust
human face modeling. Most of the previous
3D face reconstruction techniques require
more than one face image to achieve
satisfactory 3D human face modeling. When
it comes to face recognition, most of the
previous methods relied on a large quantity
of training examples to cover different face
variations.
In this work we employ 3D morphable
model to reconstruct the 3D face model
from one single face image and synthesize a
large set of 2D face images under different
head poses for modeling pose variations in
the training dataset. Therefore, only a single
face image is needed in the training phase,
nevertheless, we can well model the
non-linear pose variations in 2D space.
Besides the 3D reconstruction technique in
training phase, the proposed face recognition
algorithm presents a probabilistic approach
to temporally integrate recognition results
from video sequences to resolve the inherent
ambiguities of single-shot based recognition.
Besides, based on the idea of locally linear
embedding [S. Roweis and L. Saul, 2000],
the proposed system integrates the rejection
ability more than behaving classification
only inside the training classes.
5.1. Three-Dimensional Morphable Model
The morphable model used for 3D face
model reconstruction from a single face
image is the same as described in Section
4.1.
The 3D geometry is initialized by the
feature points shown in Fig. 4.2(a) and
approximated by the photometric error
minimization. The cost function is defined
as:
),,,,( 2modelinput dortho f tRPαII  (5.1)
where Iinput is the input image, Portho is an
orthographic projection matrix, f is the
scaling factor, R denotes the 3D rotation
matrix, and t2d is the translation vector. The
function minimization problem given in Eq.
5.1 can be solved by using the
Levenberg-Marquart optimization [K.
Levenberg, 1994] to find the 3D face shape
vector and the pose of the 3D face model.
To train the face recognition classifier,
Figure 5.1: Two examples of 3D face reconstruction and
view synthesis from a single-shot in FG-Net video
database. The central image is the input real image and the
outer 8 images are the synthesized novel views.
Figure 4.7: 3D face reconstruction from video sequences in
FG-Net video database.
22
  trejtrejtrej PPP    1ˆˆ 1 , (5.4)
where cthreshold are the parameters controlling
scale. It turns out It can not belong to a
system user if it has a large trejPˆ . We then
sum up the conditional probability Pit along
with trejPˆ for normalization, and obtain the
posterior probability tiPˆ that this image
sequence belongs to class i up to time t:
  
     ,1ˆ1ˆ
1ˆˆ
11
1
 
 

i
t,SVC
i
t
i
tt
rej
t
rej
t,SVC
i
t
i
t
t
i PPPP
PP
P


(5.5)
where ni ,...,2,1 , and trejt Pˆ1  is the
acceptance rate at time t.
 Ttrejtnttt PPPP ˆ,ˆ...,,ˆ,ˆˆ 21P is a posterior
probability distribution. Once a tiPˆ exceeds
the threshold
thP , we can surely decide the
identity of a video sequence, either rejection
with 1ˆmaxarg nP tii (class n+1 for imposter
class) or acceptance and further recognize its
identity (i.e. if c=1,2,…,n, tii Pc
ˆmaxarg ).
We illustrate the fusion system of face
recognition framework in Figure 5.2.
5.3. Experimental Results
In this section, we validate the proposed
method by conducting experiments on the
data sets from FG-Net Video database
[FG-Net], which consists of videos of 18
identities. In the experiments, we randomly
choose 10 people as system users and
reserve the rest as imposters.
To train the SVM model for probability
output SVCtiP , , in the beginning we need to
synthesize face images for SVM training.
Here we use an SVM with a quadratic kernel
embedded. As introduced in section 5.2, we
take one and only one face image for each
system user and synthesize images to cover
the 2D non-linear variations under different
head poses. In the experiments, different
views of 121 different head poses (every 1°
from -5° to 5° along the direction of pan and
tilt) are synthesized for each system user and
applied for SVM model training. Some
examples of synthesized face images are
shown in Fig. 5.1.
To locate the face regions in video
frames, we simply apply an eigen-based
algorithm to detect interesting areas for
recognition. In order not to bias the results
of face recognition, this step is
accomplished under a semi-automatic
process with some manual interaction. Then
the trained SVM classifier for face
recognition is applied with confidence
accumulated as described in section 5.2. The
performance of face recognition is evaluated
by using the videos from FG-Net exclusive
of the training samples. Note that, in the
experiments, two concerns of a real
authentication system are used: (1) None of
the imposters can be accepted, i.e. type II
error should be zero. In this case we need to
select a smaller cthreshold so the rejection rate
1ˆt
rejP 
tPˆ
t
rejPˆ
1
1ˆtP
1ˆt
rejP
t
rejP
1t
rejP
1t
rejP
1ˆtP
…
1

SVCtP ,1
SVCtP ,2
SVCt
nP
,
Figure 5.2: A conceptual framework of fusing temporal
information for face recognition.
t+2
24
confidence, we trade only a tiny time for
more reliable face recognition results.
6. Conclusion
In this project, we have presented two 3D
reconstruction systems, which include
silhouette-based 3D reconstruction system
under circular and face modeling system
with facial expression, one video-base
recognition system and one 3D non-rigid
registration algorithm. Experimental results
demonstrate the performance of our
proposed algorithms and systems.
These works have been accepted by
international conferences and the details are
listing as follows:
1) The silhouette-based 3D reconstruction
system. [P.-H. Huang and S.-H. Lai,
2008]
2) The 3D non-rigid registration algorithm.
[T.-Y. Lee and S-.H. Lai, 2008]
3) The face modeling system with facial
expression. [S.-F. Wang and S.-H. Lai,
2008]
4) The video-based face recognition system.
[C. T. Liao et al., 2008]
7. Reference
[A. Sharf et al., 2004] A. Sharf, M. Alexa, and D.
Cohen-Or, Context-based Surface Completion, In
ACM Trans. on Graphics (TOG), 2004.
[A. W. Fitzgibbon et al., 1998] A. W. Fitzgibbon, G.
Cross, and A. Zisserman, Automatic 3D model
construction for turn-table sequences, In Proc. 3D
Structure from Multiple Images of Large-Scale
Environments, 155-170, 1998.
[C. T. Liao et al., 2008] C.-T. Liao, S.-F. Wang, Y.-J.
Lu, S.-H. Lai, Video-based face recognition based on
view synthesis from 3D face model reconstructed
from a single image, In Proc. Int’l. Conf. on
Multimedia and Expo, 1589-1592, 2008.
[FG-Net] FG-Net:http://www.mmk.ei.tum.de/~waf/
fgnet/feedtum.html
[G. Zhang et al., 2006] G. Zhang, H. Zhang, and K.-Y.
K. Wong, 1D camera geometry and its application to
circular motion estimation, In Proc. British Machine
Vision Conference, 1:67-76, 2006.
[H. Xie et al., 2004] H. Xie, K. T. McDonnel, and H.
Qin, Surface reconstruction of noisy and defective
data sets, In Proc. of IEEE Visualization, pages
259–266, 2004.
[H. Zhang et al., 2005] H. Zhang, G. Zhang, and K.-Y.
K. Wong, Auto-calibration and motion recovery from
silhouettes for turntable sequences, In Proc. British
Machine Vision Conference, 1:79-88, 2005.
[J. Bilmes and A. Gentle, 1998] J. Bilmes and A.
Gentle, Tutorial of the em algorithm and its
application to parameter estimation for gaussian
mixture and hidden markov models, International
Computer Science Institute, 1998.
[K. Forbes et al., 2006] K. Forbes, F. Nicolls, G. de
Jager and A. Voigt, Shape-from-silhouette with two
mirrors and an uncalibrated camera, In Proc. 9th
European Conference on Computer Vision,
2:165-178, 2006.
[K. Levenberg, 1994] K. Levenberg, A method for
the solution of certain non-linear problems in least
squares, In Quarterly of Applied Mathematics,
2(2):164–168, Jul. 1944.
[K.-Y. K. Wong et al., 2002] K.-Y. K. Wong, P. R. S.
Mendonca, and R. Cipolla, Structure and motion
estimation from apparent contours under circular
motion, Image and Vision Computing, 20(5):441-448,
2002.
[L. Yin et al., 2006] L. Yin, X. Wei, Y. Sun, J. Wang,
and M. Rosato, A 3D facial expression database for
facial behavior research, FG, p211–216, 2006.
[L. Zhang and D. Samaras, 2006] L. Zhang and D.
Samaras, Face recognition from a single training
image under arbitrary unknown lighting using
spherical harmonics. PAMI 28(3), 351-363, 2006.
26
可供推廣之研發成果資料表
■ 可申請專利 □ 可技術移轉 日期：97 年 9月 22 日
國科會補助計畫
計畫名稱：從多重角度影像重建三維物體模型之研究(3/3)
計畫主持人：賴尚宏 博士
計畫編號：96-2221-E-007-043 學門領域：
技術/創作名稱
VIDEO-BASED FACE RECOGNITION BASED ON VIEW SYNTHESIS
FROM 3D FACE MODEL RECONSTRUCTED FROM A SINGLE IMAGE
發明人/創作人 賴尚宏，廖家德
中文：在此計畫中我們發展了一個人臉辨識系統。這個人臉辨識系
統可使用單張人臉影像，經由 morphable model 以重建使用者的三
維人臉模型，並利用此重建回的人臉模型來模擬產生不同人臉變化
（如光影變化、角度變化…等等）之訓練資料。這些合成資料經由
訓練程序將有助於增進人臉辨識系統之強健性。除此之外，本人臉
辨識系統基於機率的表現方式合理的正規化及累積各個時間點上
的人臉辨識結果，改善了單張影像人臉辨識系統的不穩定性。基於
Locally Linear Embedding 方法，此人臉辨識系統也具有拒絕不合法
入侵者的能力。
技術說明
英文： The proposed face recognition system applies the 3D face
model reconstructed from the single face to synthesize different views
for effectively training, thus leading to robustness against poses
variations. In addition the proposed face recognition algorithm presents
a probabilistic approach to temporally integrate recognition results
from video sequences to resolve the inherent ambiguities of single-shot
based recognition. The proposed system also integrates the rejection
ability based on locally linear embedding.
可利用之產業
及
可開發之產品
諸如監視系統，門禁系統… 等等
可經由取像設備辨識使用者之應用
技術特點
Most of the face recognition algorithms were proposed based on
training numerous still examples of face images to accommodate
different face variations; however, it is not practical to collect lots of
face images under different variations for each subject in a real
authentication system. We propose a novel face recognition system
with only one single image for each individual in the training dataset.
Moreover, the proposed system integrates the temporal face
recognition results from the video in a probabilistic framework to make
附件二
28
可供推廣之研發成果資料表
■ 可申請專利 ■ 可技術移轉 日期：97 年 9月 22 日
國科會補助計畫
計畫名稱：從多重角度影像重建三維物體模型之研究(3/3)
計畫主持人：賴尚宏 博士
計畫編號：96-2221-E-007-043 學門領域：
技術/創作名稱 Estimating 3D Face Model and Facial Deformation from a SingleImage Based on Expression Mainfold Optimization
發明人/創作人 賴尚宏、王書凡
中文：臉部表情的估測在表情變識以及臉部動畫的表情合成都是相
當核心的問題。先前的研究中提出相關的報告指出使用「低維度的
流形」比「線性子空間」更為適合模型化臉部表情的變化。在此篇
論文中，我們提出了一個以流形為主的三維人臉的重建方法從單張
人臉影像估測其三維立體結構以及其真實臉部表情變化。在訓練階
段，我們從超過 1000 筆大量的三維人臉表情三維模型建構了一個
非線性的三維表情流形來表示臉部可能的所有變化。另外，我們並
用高斯混合機率模型來估測在此流形上的機率分佈。透過結合線性
無表情三維人臉模基以及非線性三維表情流形各自的優點，我們提
出了一個新的演算法從單張影像估測其三維立體結構以及其真實
臉部表情變化。在 CMU-PIE 影像資料庫以及 FG-Net 影片資料庫的
實驗結果中證實了我們方法的有效性以及準確性，此類技術也將對
各類廣泛的應用有相當的貢獻。
技術說明
英文：Facial expression modeling is central to facial expression
recognition and expression synthesis for facial animation. Previous
works reported that modeling the facial expression with
low-dimensional manifold is more appropriate than using a linear
subspace. In this paper, we propose a manifold based 3D face
reconstruction approach to estimating the 3D face model and the
associated expression deformation from a single face image. In the
training phase, we build a nonlinear 3D expression manifold from a
large set of 3D facial expression models to represent the facial shape
deformations due to facial expressions. Then a Gaussian mixture model
in this manifold is learned to represent the distribution of expression
deformation. By combining the merits of morphable neutral face model
and the low-dimensional expression manifold, we propose a new
algorithm to reconstruct the 3D face geometry as well as the 3D shape
deformation from a single face image with expression in an energy
minimization framework. Experimental results on CMU-PIE image
database and FG-Net video database are shown to validate the
effectiveness and accuracy of the proposed algorithm.
可利用之產業
及
可開發之產品
1. 三維人臉表情之分析
2. 提高人臉辨識的準確性
3. 模擬表情變化之應用
附件二
行政院國家科學委員會補助國內研究生出席國際學術會議報告 
                                                                 2008 年  07 月  09 日 
報告人姓名  
黃柏豪 
 
就讀校院
（科系所）
                     □博士班研究生 
國立清華大學資訊工程學系 
                     □碩士班研究生 
     時間 
會議 
     地點 
2008/06/23~2008/06/28 
美國安哥拉治 
本會核定
補助文號
NSC-97-2922-I-007-128 
會議 
名稱 
 (中文) 電腦視覺與樣式辨識 2008 
 (英文) Computer Vision and Pattern Recognition 2008 
發表 
論文 
題目 
 (中文) 利用少數環狀運動物體的影像輪廓資訊達成相機校正 
 (英文) Silhouette-Based Camera Calibration from Sparse Views under Circular 
Motion 
一、參加會議經過 
2007 年 12 月，我們將當時之研究成果投稿至此次電腦視覺與樣式識別國際會議(IEEE CVPR
2008)，經過近三個月的審查，順利獲得主辦單位之肯定，並於 2008 年 6 月 26 日當天於會議上
發表此研究報告。我們搭乘離會議時間最接近的班機，於 6 月 22 日下午出發前往安哥拉治，抵
達時間為 6 月 22 日當地早上 9 點。於 6 月 23-28 日參與會議。於當地時間 7 月 5 日凌晨 4 點半
由安哥拉治出發返回台北，返抵時間為台北時間 7月 6日早上 5點。 
 
二、與會心得 
此次國際會議(IEEE CVPR 2008)一直以來是電腦視覺與樣型辨識領域熱門且公認為 Rank-1 的國
際會議，所有來自世界各國的相關領域研究員、廠商齊聚一堂，發表近期研究成果，以期達到學
術交流之目標；另有許多廠商與會，以期促進最新科技產品之推廣與方向。有此機會與國際友人
之切磋下，增廣許多寶貴的見識，也得到不少寶貴的建議。值得注意的是，近期各大知名國際會
議中，華人參與的比例上升了不少，尤其是對岸中國、香港的研究團隊。相較之下，台灣在該重
量級國際會議的發表量還是非常非常的少。期許未來能有更多來自台灣本土的研究，致力發表在
知名的國際會議。當然，也期望能有相當的產學合作或是政府補助來投資這個未來的產業。 
 
三、考察參觀活動(無是項活動者省略) 
(無) 
 
四、建議 
在此特別感謝指導教授賴尚宏博士的鼓勵與照顧，清大資工系的支持，和國科會所設置之出國補
助，才得以將最好的研究成果，呈現在國際舞台上，使台灣本土的研究與世界接軌。也期許未來
能的經費補助能夠更為充足，往往機票要價動輒五、六萬，而補助僅 1/3，更遑論註冊費等其他
開支，常見有學生因為補助不夠，而放棄與會，實在非常可惜。尤其是各領域的 Top Conference，
更應該給於相對應的補助，鼓勵國內研究多致力於頂尖會議的發表。 
ˇ 
