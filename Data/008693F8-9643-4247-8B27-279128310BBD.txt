(a) equally distributed over local optima (b) distributed according to the quality 
Figure 1: Concept of niching techniques.
to compute all dependencies among k variables in a problem of size ℓ, it makes more sense to
start with only pairwise linkages which are computationally limited to O(ℓ2). Therefore, most of
multivariate EDAs establish linkage models by starting with pairwise dependency detection. For
example, ECGA [18] builds the marginal product models by merging two sets of variables at one
time according to the minimum description length(MDL) [3, 24, 31, 32] metric. Similarly, BOA [29]
exploits pairwise dependency to decide the addition or deletion of an edge between two variables in
the Bayesian networks.
Nevertheless, a quick approach may be generally deficient in precision for those allelic pairwise
independent problems. The parity functions, which do not have relation in any pairs, is a typical
example. Hence, it is theoretically impossible to correctly identify the dependencies of parity func-
tions by starting with pairwise detection. Coffin and Smith tried the hierarchical BOA (hBOA) [28]
, one of the most successful EDAs, on a series of mix of the parity and trap functions, and found
that number of function evaluation scales exponentially [7]. It was then believed that these allelic
pairwise independent functions are generally difficult for EDAs, because this problems are able to
deceive those linkage-learning mechanisms that starts with pairwise dependency detection.
1.2 Multimodal Problems and Niching
In the field of optimization, researchers have paid attention to so-called multimodal problems.
A multimodal problem consists of many local optima. To prevent search algorithms from being
trapped in one local optimum, niching techniques have been adopted to spread search points over
many local optima. Another reason behind this idea is the feasibility concern. In many real-world
applications, feasibility check is possible only after the solution is given. A traditional GA gives
only one best-ever-seen solution; however, if that solution is infeasible we would need to re-do the
whole search procedure for other feasible solutions, which is time consuming.
Niching techniques are then developed to resolve the above issue for multimodal problems.
The main idea is to keep competitions in local such that the GA would keep a certain number
of individuals for each local optimum. Ideally, the number of individuals that the GA maintains
should be proportional to the quality of the corresponding local optima (Figure 1).
Niching can be used as a replacement mechanism to preserve alternative solutions for GAs and
EDAs. Niching methods can be classified into three categories : (1) fitness sharing [21, 17], (2)
crowding [5, 9, 26], and (3) spatial separation [37, 11]. RTR belongs to the crowding category and
will be discussed later.
Here we discuss the interaction between selection and niching. Selection keeps better solutions
2
CP/TF is defined as a mixture of the parity and trap functions:
CP/TF (X) =
m−1∑
i=0
{
parity(xik . . . xk(i+1)−1) if i is even
trap(xik . . . xk(i+1)−1) otherwise.
(4)
They investigated the performance of hBOA [28] on these problems and found that hBOA
scales exponentially on both problems. Since hBOA has shown robustness and reliability on most
nearly decomposable problems, it was believed then that CPF and CP/TF are difficult for EDAs
in general. Based this belief, more investigations have been done and more EDA designs have been
proposed. Echegoyen et al. [10] investigated the model building on Bayesian networks based EDAs
and verified that the linkages in CPF can be learned by building an exact model. Emmendorfer and
Pozo’s work [12] also employed CPF as a test function for their EDA design.
The similar part of preview research is that these EDAs usually both employ the linkage learning
and RTR niching mechanism. The unsolved problem may be both of them or one of these two.
Hence, it is necessary to test different EDAs separately. CGA do not have linkage learning and RTR,
ECGA has linkage learning but without RTR, and DSMGA can both employes linkage learning and
RTR as same as hBOA.
2.2 Restricted Tournament Replacement
For each offspring, RTR defines a window size w and randomly chooses w individuals from the
parental population into the tournament pool. The offspring is then compete with the most similar
parental individual in the pool. The previous work [25] suggested a windows size w = min(ℓ,n/20),
where ℓ is the problem size and n is the population size. They also used Hamming distance as the
similarity metric. For examples, consider the following three individuals:
A = [000 001 011] B = [000 101 111] C = [000 001 000].
The Hamming distance between A and B (2) is shorter than the distance between A and C (4).
Hence, A and B will be chosen for competition. The pusedo-codes of RTR is given by Algorithm 1.
3 Spurious Linkage
We suspect that RTR adopted by most EDAs causes the exponential scalibility on parity functions,
but the reason is yet unclear. Here, we investigate the changes of mutual information over gen-
erations with and without RTR. Mutual information is based on the concept of entropy [33] and
defined by kullback-leibler distance [22]. It has been used as the linkage-learning metric in many
EDAs. The general form of mutual information is as follows:
I(X;Y ) =
∑
x
∑
y
p(x, y) log
p(x, y)
p(x)p(y)
. (5)
In this equation, X and Y are two random variables, x and y are the outcomes of X and
Y respectively, and p(.) is the probability of the corresponding outcome. The value of I(X;Y ) is
between 0 and 1. A greater value indicates a stronger relation between X and Y . If the two variables
have greater mutual information, it is suggested to arrange them as one BB from being disrupted.
Furthermore, we utilize a 3-bits parity function as an example to discuss linkage detection.
4
0   0   0 
0   0   0 
0   1   1 
0   1   1 
0   0   0           2 
0   0   1           0 
0   1   0           0  
1   0   0           0 
0   1   1           2 
1   0   1           2 
1   1   0           2 
1   1   1           0 
1   1   0 
1   1   0 
1   0   1 
1   0   1 
After 
selection 
Fitness (even =2, odd = 0) 
I (X,Y) = 0   
   X  Y   Z          X  Y   Z    
Figure 2: An example of 3-bits parity function. If the outcomes of 1 is even, individual of fitness is
given 2, otherwise given 0. EDAs detect linkage after selection operators.
0   0   0 
0   0   0 
0   1   1 
0   1   1 
1   1   0 
1   1   0 
1   0   1   
1   0   1 
0   0   0            
0   0   1          
0   1   0            
1   0   0            
0   1   1            
1   0   1            
1   1   0            
1   1   1            
After 
RTR 
I (X,Y) = 0.15   
Parent  
Population 
Offspring  
Population 
Window 
size = 4 
0   0   0            
0   0   0           
0   1   1            
1   1   0            
0   1   1            
1   0   1            
1   1   0            
1   1   1            
X    Y    Z 
Figure 3: RTR operator forms new population from parent population and offspring population.
6
two individuals. Therefore, it is needed to define another similarity metric in substitution for the
Hamming distance metric to conquer the drawback of RTR.
We adopt the new detect metric but still preserve some flow of RTR, because RTR still has
some advantage on maintain diversity of population. For most niching mechanism, like crowding
and fitness sharing, it is necessary to define the size of niche and number of niche. If the niche
size is too large, it will decrease diversity and may result to converge to local solution, not global
optimum. The random mechanism, adopted by RTR, have ability to adapt different problems and
still has better performance than traditional niching algorithm. This is why RTR is employed by
most EDAs.
Take more detail on the principle of similarity detection. For examining the similarity, RTR
utilizes Hamming distance to detect the closest pair. It is institutive way to detect similarity by
using distance metric, because the closest pair in distance sometimes do not mean the similarity
for all problems. The other question is that distance metric can not reflect the preserved solution
quality, where we anticipates to keep more better solutions. If the preserved solution is no meaning
in proportion to solution quality, these method should be replaced to the new version.
Hence, according to the binomial distribution if variables are independent, we can assume that
every individuals should have a equivalent distance by their solution quality. The goal of our new
niching mechanism is to use another estimated metric to keep population into binomial distribution.
This method has more meanings and intuition than RTR on the linkage learning. We hope the
preserved solutions can form population into expected distribution according to correct linkage.
Before discussing the estimated metric, we should define what is the ideal distribution for linkage
learning.
4.1 Ideal Distribution
EDAs employed the model building after selection, because the selection operator can keep meaning-
ful distribution of current population from the original population. After replacement mechanism,
RTR may disrupt the these meaningful distribution. Hence, the best way to design a new niching
mechanism is that the preserved solution still maintain the characteristic of distribution. As the
example of one max problems, binomial distribution is our goal for the preserved solutions. The
other question is how to determine the parameter probability of binomial functions. Here, we can
determine the average fitness of population as the parameter probability. We discuss the relation
between distance and fitness of each individuals. First, we discuss how to involve the fitness assign-
ment into the similarity metric . According to fitness function, each individual will be assigned a
fitness values. However, the fitness values have bias, which means the larger fitness values will take
more important on the population. For example, the bias between fitness 1001 and 1000 will be
affected by noise. It should be avoided because this results will decrease the resolution for EDAs
to recognize the different individuals. Hence, we use a new estimation, fitness ranked, as our new
fitness assignment not fitness value. For example, if we have 2N individuals in the pool including
parent and offspring population, the new fitness assignment is subsequently as 1
2N
, 2
2N
, ..., 2N
2N
. We
use the symbol of r for rank value in the latter discussion.
At next step, we define the relative occurrence rate to explain the method. We define two
parameter p and q. p means the occurrence rate of schema for each individuals, where schema is
assumed with most occurrence in the population. For example, if the BB size is single bit and 1
presents the most occurrence BB, the pi of this i-th individual 01100 00100 is
3
10
= 0.3. q is the same
definition as p, but the difference is that p is for current population and q is for the expected rate
after replacement. p can be calculated according to observation on population, but q is unknown
8
The idea of this work is to obtain those characteristics from the current population. In the
preview section, we observed the rate p of occurrence schema and anticipated q of every individual.
These two parameter will help us to define the ideal size of each niching.
By using estimated qr, we define what is the ideal distance given the binomial distribution. dr
is defined as the ideal distance of the rth individual to its nearest individual. Given qr, we can
estimate dr by calculating the distance between qr and average pa of population. The equation is
as follows:
dr = [qr(1− pa) + pa(1− qr)] ·m · k, (8)
where m is the problems size and k is the bounded of sub-problems. Because we have already
obtained the qr from normal approximation, the equation can be reduce as follows:
dr = [2pa(1− pa) + αr
√
pa(1 + pa)
l
(1− 2pa)] · ℓ. (9)
Finally, according to the fitness rank, we can obtain ideal distance for every niche. In our definition,
each chosen individuals can be seem as a independent niche and the distance is defined as dr. This
section describes how we define the ideal distance if we have a estimated binomial distribution.
4.3 The Flow of BRTR
Now we can estimate the ideal distance according to observing occurrence rate p. However, in
implementation, it is difficult to decide ideal distance due to finite individuals. Hence, we use the
binomial distribution to calculate the probability of matching between real distance and equivalent
distance. If the equivalent distance of any pair of individuals is closer to the real distance, the
probability is high. This probability is more meaningful than Hamming distance.
Before discussing the new method, we first look back RTR. Although RTR’s distance detection
has some weakness on detecting similarity of individuals, RTR has been proven to success in many
deception problems. The goal of work is the change the Hamming distance as our probability
metric. As the success of RTR, we will keep some characteristic form RTR. Like RTR mechanism,
we compete the most similar pair to compete. Furthermore, we would not use the distance detecting
metric as RTR. RTR uses the Hamming distance as the detecting metric and compare the most
similar pair with fitness. However, the distance metric will throw too more better solutions if those
solutions are too close. Hence, according to our probability, even if better solutions have closest
distance, they are still preserved according the equivalent distance. We still use the restricted
tournament replacement to replace the population. Hence, we call our new niching method as
binomial restricted tournament replacement (BRTR), because we use binomial distribution as our
estimation.
The flow of BRTR is similar as RTR. Fisrt, we begin as a offspring to tournament replacement.
Second, we use the probability metric to define the most equivalent distance matching between
offspring and parent population. Finally, use tournament mechanism to decide which individual
should be keep. The only different is that we choose the new probability metric of equivalent. Here,
we define the parameter Q as the probability of similarity. It means if we have two individual
estimated qi and qj. Q is the probability of estimated distance by two individual i and j.
Q = qi · (1− qj) + (1− qi) · qj. (10)
Now that we have obtained the values of qi and qj, Q can be evaluated. Here, Q presents the ideal
distance if individuals i and j are sampled from the binomial distribution. Then, we define dtheory as
10
Algorithm 2 Fitness Normalization
Input: Parent population A = {a1, a2, ..., an}, offspring population B = {b1, b2, ..., bn}
, and model structure C = {c1, c2, ..., cm}, where m is the size of sub-problems
Output: Next generation population O with n individuals.
1: Assume {α1, α2, ..., αn} are the normalized value of {a1, a2, ..., an}
2: Assume {β1, β2, ..., βn} are the normalized value of {b1, b2, ..., bn}
3: for i = 1→ n do
4: for j = 1→ m do
5: if ai has most occurrence schema at position cj then
6: αi+=
1
m
7: end if
8: if bi has most occurrence schema at position cj then
9: βi+=
1
m
10: end if
11: end for
12: end for
Algorithm 3 Binomial Restricted Tournament Replacement(BRTR)
Input: Parent population A = {a1, a2, ..., an} with normalized values {α1, α2, ..., αn}
and offspring population B = {b1, b2, ..., bn} with normalized values {β1, β2, ..., βn}
Output: Next generation population O with n individuals.
1: Set window size w = min(l,n/20), where l is the length of problems
2: for i = 1→ n do
3: Choose random subset Aw with w size from A
4: {s1, s2, ..., sw} ←− Aw, where {σ1, σ2, ..., σw} are their normalize values
5: for j = 1→ w do
6: Distance dj ←− Hamming-Distance(bi, sj)
7: Equivalent distance ej ←− Equivalent(βi, σj)
8: Probability pj ←− binomial cdf(l, dj , ej)
9: end for
10: Find Minimal x←− argx∈{1,2,...,w} min{p1, p2, ..., pw}
11: if Fitness{bi} ≥ Fitness{sx} then
12: sx ←− bi
13: end if
14: Aw ←− {s1, s2, ..., sw}
15: end for
16: O ←− A
12
104
105
106
60 84 120 171 240
Problem Size
BRTR
RTR
N
f
e
(a) CP/TF problems with k = 3
104
105
106
107
108
40 56 80 112 160
Problem Size
BRTR
RTR
N
f
e
(b) CP/TF problems with k = 4
Figure 6: EDA solves CP/TF problem. y axis is the number of function evaluation
0
0.2
0.4
0.6
0.8
1
10 15 20 25 30
FR
RTR
BRTR
N
u
m
b
er
o
f
D
iff
er
en
t
O
p
ti
m
a
Generation
(a) k = 3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
20 25 30 35
FR
RTR
BRTR
N
u
m
b
er
o
f
D
iff
er
en
t
O
p
ti
m
a
Generation
(b) k = 4
Figure 7: Assume sub-problems have many global optima. The experiment is to test the ability of
maintaining diversity. y axis is percent of different kinds of global optima. If y = 1.0 means all of
population are all global optima and different with others
5.2 Experiments of CP/TF
In Figure 6, the case of k = 3 shows that RTR and BRTR scale polynomial, although BRTR is
better than RTR. RTR scales polynomially with parity function, because the k = 3 is an easy case
even if RTR induce more spurious linkage. However, the case of k = 4 shows that RTR scales
in the exponential time as we anticipated. It is clear that BRTR remains polynomially with both
deceptive problems and pairwise independent problems.
5.3 Experiments of Niching Testing
We have proven that BRTR can solve CPF and CP/TF in the polynomial time, where RTR fails
on this kind of problems. Although BRTR has ability to reduce spurious linkage, it still needs to
prove the ability of maintaining diversity. RTR can keep population diversity well, so our proposed
method should compare with RTR in this situation. For more meaningful comparison, we add a full
14
0.7
0.75
0.8
0.85
0.9
0.95
1
20 80 140 200 260
  FR
  RTR
  BRTR
Population Size
P
ro
b
a
b
il
it
y
o
f
C
o
n
v
er
g
en
ce
Figure 8: DSMGA with and without RTR solve a 100 bits one max. The example shows the average
mutual information on the independent pair in every iteration of trap function. The higher average
value mean the more serious spurious linkage produced.
The contribution of this work is to explain the mysterious failure of EDAs on allelic pairwise
independent problems. We observed that the drawback of hBOA is manly on its replacement
mechanism, RTR. It is an important discovery that RTR is the famous niching mechanism and
rare research discuses its drawbacks. We discuss RTR on the view of linkage learning and find that
RTR is no meanings for linkage learning of most EDAs. It gives us a hint that we should design
a new niching mechanism really suited for most EDAs. The design of the new niching algorithm
will become the important issue in the future — what kind of characteristic of niching mechanism
should be really suitable for linkage learning of EDAs.
The work also introduces a new niching mechanism, BRTR, which is derived from the binomial
distribution. If we want to keep preserved local optima and still maintain the characteristic of
distribution, the replacement should remain such binomial distribution which all schema will be
independent to others. At the same time, we propose the equivalent distance to determine the
anticipated distance for linkage learning to reduce the spurious linkage. As the result, our new
niching algorithm has proven that BRTR can solve such allelic pairwise independent problems than
RTR and define the ideal distance to linkage learning.
As empirical results, BRTR can solve both deceptive problems and allelic pairwise independent
problems. It reveals the ability that BRTR preserves sufficient local optima to search optima and
prevent the extra spurious linkage to fail EDAs. The most important key is that BRTR acquire
the knowledge of linkage structure from DSMGA. In future work, some EDAs has implicit linkage
information which is hard to be observed. According this case, the next approach to niching
algorithm is to use binomial concept to construct a ideal niching algorithm without considering
explicit linkage information.
16
[16] D. E. Goldberg, K. Deb, and J. H. Clark. Genetic algorithms, noise, and the sizing of popula-
tions. Complex Systems, vol. 6:331–362, 1991.
[17] D. E. Goldberg and J. Richardson. Genetic algorithms with sharing for multimodal function
optimization. In Proceedings of the Second International Conference on Genetic Algorithms,
pages 41–49, 1987.
[18] G. Harik. Linkage learning via probabilistic modeling in the ecga. Technical report, IlliGAL,
University of Illinois at Urbana-Champaign, 1999.
[19] G. Harik, F. Lobo, and D. Goldberg. The compact genetic algorithm. Evolutionary Computa-
tion, IEEE Transactions on, 3(4):287–297, 1999.
[20] G. R. Harik. Finding multimodal solutions using restricted tournament selection. In Proceedings
of the 6th International Conference on Genetic Algorithms, pages 24–31, 1995.
[21] J. H. Holland. Adaptation in natural and artificial systems. Ann Arbor, MI: University of
Michigan Press, 1975.
[22] A. Kullback and R. A. Leibler. On information and sufficiency. Annual Mathematical Statistics,
vol. 22:79–86, 1951.
[23] P. Larran˜aga and J. A. Lozano, editors. Estimation of Distribution Algorithms: A New Tool
for Evolutionary Computation. Kluwer Academic Publishers, 2002.
[24] R. Lutz. Recovering high-level structure of software systems using a minimum description
length principle. In Proceedings of the 13th Irish International Conference on Artificial Intel-
ligence and Cognitive Science (AICS), pages 61–69, 2002.
[25] K. S. M. Pelikan and E. Cantu´-Paz, editors. Scalable Optimization via Probabilistic Modeling
from Algorithms to Applications. Springer-Verlag, 2006.
[26] S. W. Mahfoud. Crowding and preselection revisited. Parallel problem solving from nature,
2:27–36, 1992.
[27] H. Mu¨hlenbein and D. Schlierkamp-Voosen. Predictive models for the breeder genetic algo-
rithm: I. continuous parameter optimization. Evolutionary Computation, 1:25–49, 1993.
[28] M. Pelikan and D. E. Goldberg. Escaping hierarchical traps with competent genetic algorithms.
In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO2001), pages
511–518, 2001.
[29] M. Pelikan, D. E. Goldberg, and E. Cantu´-Paz. BOA: The bayesian optimization algorithm.
In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO-99), pages
525–535, 1999.
[30] M. Pelikan and H. Mu¨hlenbein. The bivariate marginal distribution algorithm. In Advances in
Soft Computing - Engineering Design and Manufacturing, pages 521–535, 1999.
[31] J. Rissanen. Modeling by shortest data description. Automatica, 14:465–471, 1978.
18
出席國際學術會議心得報告
                                                            
計畫編號 NSC 99-2221-E-002-175
計畫名稱 基因演算法中支援鏈結學習的小生境技術發展
出國人員姓名
服務機關及職稱
于天立
國立台灣大學 助理教授
會議時間地點 7/12~7/16, 2011, Dublin, Ireland
會議名稱 Genetic and Evolutionary Computation Conference (GECCO)
發表論文題目 The essence of Real-valued Characteristic Function for Pairwise Relation in Linkage  
Learning for EDAs
A Test Function with Full Controllability over Overlapping
1、 參加會議經過
2. The Estimation of Distribution Algorithm Track
Our poster is in this secession of EDA, hence there are many related issues that can give us new 
idea with our research.  There was a presentation impressing me most, named :Use of infeasible 
individuals in probabilistic model building genetic network programming.
Classical EDAs in evolutionary algorithms use those feasible or better individuals to estimate the 
distribution of the problem.  This paper tries to exploit the information brought by infeasible or worse 
individuals.  This paper studies infeasible individuals to modify the sub-structures rather than the 
entire structures.  The idea is to use those “worse information” to filter out some “bad sub-
structures”.  Reinforcement learning problems are used to implement the idea.  The paper wisely 
uses the Q-value of reinforcement learning as the fitness.  Results show that infeasible individuals do 
help evolutionary algorithms.  I think the future work is “how to exploit the method to any kind to 
problem”, but as the speaker said it might be hard.  The idea to exploit Q-value as fitness is brilliant 
and can be further studied. 
3. VIZGEC WORKSHOP
In GECCO2011, it was the 2nd GECCO Workshop on Visualization Methods for Genetic and 
Evolutionary Computation. Jason H. Moore hosted this section. The main idea of developing 
visualization methods is to extract more knowledge from the massive data. This technique is 
generally applied to many realms such as science, information, and medical. However, it’s pretty 
hard to represent the dynamics of evolution visually. In this workshop, Jason presented a 3D model 
built by Unity3D, which is a famous engine for 3D game. This model can show evolutionary 
information in five dimensions. The x-axis is the population; the y-axis shows the generation, and the 
z-axis represents the frequency among the population at any time. The other one dimension is the 
costs. Our field benefits a lot from GPU computation because there’s a parallel nature in Genetic and 
Evolution computation. Plus, the amount of computation required in Genetic and Evolution 
computation is usually large for practical problems. Researchers around the globe have 
implemented their Evolutionary Computation (EC) and Genetic Algorithm (GA) on GPUs and acquired 
tremendous success. In GECCO 2011 some scholars shared their experience on GPU computation. 
Due to the characteristics of GPU (which is very different from CPU), a lot of common operators in EC 
and GA have to be modified to fit the structure of GPU. When this is done, we can solve some 
problems, which were too time-consuming to be solved practically, in reasonable amount time. One 
of the most interesting topics is a Sudoku solver implemented on GPU. A Japanese researcher 
presented this work on Sudoku. It solves Sudoku REALLY FAST! Well, this is not really useful in life 
but it is surely interesting. At the end of the workshop, the host of this section, William Langdon, 
summarized some guidelines for adopting GPU computation into evolutional computation research. 
The talk was very concise, but very helpful.
6. Graduate Student Workshop
Different from other workshops or tutorials that are presented by “masters” in this field, the 
presenters are graduate students. It’s quite refreshing to hear from other young researchers in the 
world. Though they may not be as professional as professors, they do provide some valuable new 
thoughts and some interesting ideas of research. In the term of Genetic Algorithm, “mutations” of 
thought into the converging thought pool. The talks in this session are relatively simple, sometimes 
rough, but full of creativity. It’s exciting that I might be witnessing the beginning of some future 
great works. It’s even more interesting that these presentations are pretty fun to watch. One speech 
that I’m particularly interested in was “Evolving Board-Game Players with Genetic Programming”. 
Even though Genetic Programming is a totally different branch of research from Genetic Algorithm 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/26
國科會補助計畫
計畫名稱: 基因演算法中支援鏈結學習的小生境技術發展
計畫主持人: 于天立
計畫編號: 99-2221-E-002-175- 學門領域: 人工智慧
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
