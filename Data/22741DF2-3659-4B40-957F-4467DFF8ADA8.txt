 
行政院國家科學委員會補助專題研究計畫成果報告 
※※※※※※※※※※※※※※※※※※※※※※※※※
※                      ※ 
※ 省電與性能最佳化技術：從應用面至系統面之探討─總計劃     ※ 
※                  ※ 
※                  ※ 
※※※※※※※※※※※※※※※※※※※※※※※※※ 
 
計畫類別：□個別型計畫  ■整合型計畫 
計畫編號：NSC95－2221－E－002－095-MY3 
執行期間： 95 年 8 月 1 日至 98 年 10 月 31 日 
 
計畫主持人：郭大維 教授 國立台灣大學資訊工程學系 
計畫共同主持人: 洪士灝 助理教授  國立台灣大學資訊工程學系 
                楊佳玲 教授  國立台灣大學資訊工程學系 
                施吉昇 副教授  國立台灣大學資訊工程學系 
                逄愛君 副教授  國立台灣大學資訊工程學系 
                    黃泰一 副教授  國立清華大學資訊工程學系 
                廖世偉 副教授  國立台灣大學資訊工程學系 
 
計畫參與人員：楊川岳，黃柏鈞，蕭舒方，蔡承祐，吳柏良，黃昶竣，莊
豐旭，王俊文，陳冠儒，廖彧宏，涂嘉恆，陳江睿，周錦彥，廖瑋星，涂
漢興，許文昌，楊文隆，林業峻，王穎杰，陳建華，曾賢舜，林武頡，蘇
建豪，黃禮俊，歐陽銘康，張家榮，林怡賢，陳宏信，張哲維，陳膺正，
張閎翔，曾學文，余亞儒，鄒志鴻，戴邦炘，陳雋中 
 
執行單位：國立台灣大學資訊工程學系 
 
 
中 華 民 國  98 年 06 月 28 日 
英文摘要 
 
As the multimedia, communication and IC technology progress steadily, we are now in the 
multimedia and communication era.  Integrated multimedia & communication applications 
are becoming popular, such as video conferencing, digital TV, video on demand etc.  For this 
type of applications, how to improve system performance (including QoS guarantee and 
efficient resource utilization) and reduce power consumption in the multimedia 
communication system is critical. 
 
Based on the trend of hardware technologies and software applications, the goal of this 
project is to develop high-performance/low-power techniques for each layer of the system 
design, including network/application layer, operating system layer and architectural platform.  
Moreover, we focus on the design of cross-layer techniques considering both the performance 
and power factors. In the first year of this project, we have developed fundamental technology 
in energy/performance optimization and designs, and develop simulation/emulation platforms. 
A set of scheduling algorithms is also proposed to manage the number of task preemptions so 
that the energy consumption resulting from the devices can be reduced. In the area of energy 
optimization, we have developed a composite low-power real-time scheduling framework that 
includes DVS in a DPM policy to maximize the energy reduction of the system. In the second 
year of this project, we looked at heterogeneous networking systems, continued the 
development on the energy-efficient techniques from the first year by considering external 
devices, and developed adaptive power management mechanism and high performance/low 
power techniques for the multi-core architecture. Moreover, we designed the QoS control 
mechanism for P2P networks including the resource management and streaming scheduling 
algorithm, and proposed an algorithm to support interference-aware routing and link 
scheduling to maximize the system throughput for WRNs 
 
In the third year, we have integrated techniques developed at each layer and evaluate the 
proposed techniques with a complete system and real-world applications. In the development 
of system simulation/emulation tools, we have utilized and verified our tools for 
characterizing MPSoC systems and applications, including performance profiling, application 
tracing, platform simulation, and power analysis. Based on the tools, we further use the slack 
time in energy-efficient real-time scheduling algorithm. In wireless application domain, we 
have conducted a complete analysis and developed integrated routing and scheduling 
mechanisms. Based on the developed simulation model and theoretical analysis, it is shown 
that this work is especially suitable for multimedia application in mobile WiMAX networks. 
Targeting at mobile multimedia systems, we build the platform for energy saving, design the 
interface for performance tuning and develop the framework for WiMAX application routing. 
Many optimal algorithms are proposed, and software techniques are completed in this project. 
The corresponding papers are also published in important conferences and journals. 
 
Keywords: high-performance, low-power, mobile multimedia 
In the literature of energy- efficient 
scheduling for real-time tasks, most studies 
assume that the worst-case execution time of 
every task instance that belongs to the same 
task is the same. However, for any practical 
applications, the average execution time of a 
task might be much shorter than its 
worst-case execution time, and the actual 
execution times of the task instances 
conform to a particular pattern, e.g. 
decoding of MPEG. To model such 
applicaions properly, Mok and Chen [Mok 
96, Mok 97] developed the Multiframe Task 
model.  
The contributions of the project are as 
follows: Two types of ap- proaches are 
proposed for multiframe real-time tasks with 
different strategies to exploit the regular 
execution patterns of tasks. The first type, 
called task-based approach, allocates the 
same time length for the executions of task 
instances belonging to the same task. The 
second type, called frame-based approach, 
can approximate the op- timal solution in 
terms of energy savings with the space that 
is poly- nomial in the input size. By 
referring to the allocated time length or 
using a lookup table, the on-line (run-time) 
scheduling overhead for speed determination 
is independent on the number of jobs in the 
ready queue in both types of approaches. 
Simulations show that our proposed 
approaches sacrifice some optimality in 
terms of energy savings, compared to the 
optimal solutions, but require less space and 
less run-time overhead. 
The project also explores how to efficiently 
cope with multiframe real- time tasks for 
energy-efficient schedule- ing in DVS 
systems. We have shown that existing 
approaches use too much space for storing 
the solution [Yao 95] or are too conservative 
by applying slack reclamation [Aydin 01, 
Pillai 01]. By storing only one speed for 
each task frame to reduce the space 
overhead, we propose algorithms to derive 
energy- efficient schedules without 
sacrificing too much performance in energy 
optimization. Simulations show that our 
approach can improve the slack reclamation 
algorithms by 5~16%. Moreover, the gap 
between the energy consumption of our 
solutions and the lower bound is less than 
6%. 
 
 
子計畫二 - 多處理器系統晶片最佳化方法與工具設計 
To establish a general framework to address 
these challenges and to speed up the 
optimization work flow for MPSoC’s, we 
have developed performance tools and 
methodologies in the past three years. In the 
first year, we built a tools suite for 
characterizing MPSoC systems and 
applications. The suite includes performance 
profiling, application tracing, platform 
simulation, and power analysis. In the 
second year, we focused on the integration 
of the tools and the methodologies for 
characterizing realistic applications in the 
areas of network, multimedia, and security. 
In the third year, we dedicated to the 
development of methodologies for 
optimization hardware-software interactions 
for aforementioned applications via 
techniques such as minimizing on-chip 
shared-memory communication. Within the 
three years, we successfully integrate our 
subproject with other subprojects by 
carrying out experiments on realistic 
applications, which has also proven our 
tools and optimization methodologies. The 
results are categorized into three categories 
below: 
1. Performance tools and optimization 
methodologies for MPSoC applications: 
We proposed a trace-based 
performance analysis framework and 
applied it on storage server 
[Hung&Huang 08] and IBM Cell 
platform [Hung&Tu 10]. We developed 
a efficient inter-processor 
communication mechanism on 
embedded multicore environment 
[Lin&Tu 09]. We proposed algorithms 
to minimize the overhead introduced by 
context switches on the dynamically 
reconfigurable platforms [Perng&Hung 
07, Perng&Hung 09]. We developed an 
automatic compiler flag selection 
framework to find good combinations 
of compiling options and improve the 
performance of a product level storage 
server [Hung&Tu 09]. 
2. Performance optimization for 
application-specific NoCs. One critical issue 
in a memory-aware NoC design is how to 
utilize the available die area to achieve a 
balanced design between memory and 
computation subsystems. That is, on one 
hand, we want to dedicate as many die 
resources to PEs as possible to fully utilize 
the available task parallelism in the target 
applications, and on the other hand, we need 
to incorporate a significant amount of 
on-chip memory to alleviate memory 
bottleneck. Therefore, the goal of 
PM-COSYN is to decide the PE and on-chip 
memory allocation for NoCs such that 
system performance is maximized and the 
area constraint is met. The research achieved 
in the third year is going to be published in 
the proceedings of the Design, Automation 
and Test in Europe (DATE 2010).  
 
子計畫四 - 集合式感測控制網路之效能最佳化 
In this project, we target the problems 
from the viewpoint of gateways. Typical 
network devices such as network routers and 
switches aim at providing high average 
throughput and eliminating the quality of 
service control mechanism to reduce the 
run-time overhead. However, the goal of the 
gateways in digital home network is 
different. In digital home networks, the 
number of network connections is limited 
but it is essential for providing the quality of 
service for various home applications. In the 
three year periods, we designed and 
implemented several resource management 
protocols for digital home networks to 
provide predictable performance.  
In the first year, we focus on the 
workload modeling and design the clock 
synchronization protocols over federated 
sensors and actuator networks. We design a 
Clock Free Data Streams Alignment for 
Sensor Networks [Li 07]. The developed 
mechanism makes use of the built-in 
counters on sensors and external 
synchronization signals to align data streams. 
The data server broadcasts out-of-channel 
synchronization signals with constant or 
variable intervals. The sensor data streams 
are aligned when received on the server. 
Only one way communication is used so as 
to reduce the communication overhead and 
clock synchronization overhead on sensors 
nodes. In addition, the developed 
mechanism is scalable thanks to one way 
communication. The synchronization error is 
bounded by the maximum sampling period 
of the sensors, and is independent of the 
number of the nodes in the network. Our 
analysis also shows the required awake time 
for sensors to tolerate different clock drifts 
and signal lost. 
Based on the accomplishment on the 
first year, we continue to work on the 
resource management for multimedia 
streaming in peer-to-peer home networks. In 
a peer-to-peer media streaming system, we 
need bandwidth that satisfies playback rate 
to transmit media data from supplying peers. 
There are two challenges to provide high 
quality playback over wireless network: (1) 
available bandwidth of each client is 
time-varying. It is difficult to implement a 
peer-to-peer media streaming system with 
QoS support if we treat the bandwidth as 
constant value. (2) Users might use other 
network applications and network traffic of 
these applications might affect media 
streaming. In this project, we investigate two 
problems: (1) how to dynamically estimate 
available bandwidth and (2) how to 
dynamically manage bandwidth according to 
QoS parameters. We propose a bandwidth 
management system to solve these problems. 
The system is an underlying part of a 
peer-to-peer media streaming system. It 
provides approximate bandwidth 
information and bandwidth management 
service to upper layer QoS management. By 
managing bandwidth according to available 
bandwidth dynamically, a peer-to-peer 
system can select appropriate supplying 
peers dynamically to provide QoS support. 
It also prevents media streaming being 
affected by other network traffic. Thus, high 
quality playback can be achieved over 
wireless network. 
P2P network can be employed in a 
video streaming system to improve the 
system scalability with lower cost. In the last 
year of the project, to achieve jitter-less 
video playback on the requesting peer, it 
requires both the requesting policy on the 
requesting peer and the scheduling policy on 
each sending peer. Earlier works for 
that no mobility is supported for SS. It can 
be employed in backbone networks, 
backhaul and last-mile transmission, 
enabling new fixed network operators to 
provide Digital Subscriber Line (DSL) grade 
services to compete with existing operators. 
IEEE 802.16-2004 can enhance basic 
telecommunication constructions quickly 
without installing fixed lines into every 
building, making it more applicable for 
developing countries than for developed 
countries. 
The IEEE Task Group j is defining the 
802.16j standard [IEEE 802.16j2007, IEEE 
802.16j2008] to enhance system throughput 
and maintain low system construction cost 
through IEEE 802.16j multipoint relay 
networks. 802.16j is currently in the draft 
stage, and the latest version is P802.16j/D3. 
It uses the tree-based topology, and is 
compliant with the 802.16 Point to 
MultiPoint (PMP) mode. Therefore, 
deploying an 802.16j MR network does not 
involve the change of existing MSs. The 
goal of 802.16j is to enhance the coverage, 
throughput and capabilities of 802.16 
networks by specifying 802.16 multihop 
relay capabilities and functionalities of 
interoperable RS and BS. The mesh mode in 
the existing 802.16 standard can also be 
adopted to achieve the same goal as 802.16j. 
Additionally, the MAC of the current 802.16 
is also similar to that of the 802.16j MR 
network, except that it allows MSs in mesh 
network to communicate directly without 
BSs. The 802.16 mesh mode is thus more 
efficient than 802.16j MR network, but has a 
more complicated MAC mechanism that is 
not compliant with the PMP mode, which is 
used by most MSs worldwide. Thus, 802.16j 
is more suitable for actual applications than 
the mesh mode of 802.16. This study 
investigates how the 802.16j MR network 
enhances coverage, throughput and system 
capabilities. 
 
C、 Joint Routing and Link Scheduling 
in WRNs 
Recently, IEEE 802.16j task group has been 
devoted to the development of multi-hop 
relaying technology as an enhancement for 
IEEE 802.16 point-to-multipoint networks. 
While IEEE 802.16j subscribers are 
expected to experience performance 
improvement through low-cost relay stations 
(RSs), the degradation of system throughput 
would occur instead if multi-hop radio 
resources can not be well-utilized. 
Consequently, the routing and 
link-scheduling algorithms, which are of the 
important resource control functions in 
multi-hop networks, shall be specifically 
re-designed to facilitate the effective use of 
network resources for WRNs. 
It is still an open and interesting question: 
whether there exists a time-efficient joint 
routing and link scheduling scheme that 
provides near-optimal system throughput for 
WRNs. In this study, we present an 
algorithm to support interference-aware 
routing and link scheduling to maximize the 
system throughput for WRNs. The algorithm 
accommodates a general workload model 
for data requests, and fits many other 
polling-based wireless systems as well as 
IEEE 802.16. The achieved throughput of 
our proposed algorithm is proven to be 
within a factor of three of that of any 
optimal solution in the worst case. To the 
best of our knowledge, the 3-approximation 
algorithm for joint routing and link 
scheduling is one of the first works for 
WRNs that can guarantee the worst-case 
performance within a small-constant bound. 
Through our simulation experiments, the 
numerical results show that our proposed 
algorithm outperforms the previously 
proposed routing and link-scheduling 
algorithms. Furthermore, the algorithm 
effectively achieves near-optimal 
performance, and provides much better 
throughput than that of the theoretical 
worst-case bound in the average case. 
 
 
 
 
 
 
policy of COLORS, called OPADS, 
carefully considers all power parameters of a 
device, including its break-even time and 
shutdown and wake-up latency. The DVS 
extension of OPADS makes use of both 
static and dynamic slack time to create 
additional switch-off opportunities as well as 
reduce the processor energy consumption. 
The efficient use of slack time allows 
COLORS to switch off idle devices for 
longer periods. COLORS determines a 
switching decision of a device according to 
its predicted earliest-access time. An 
accurate prediction requires large 
computational overhead. Finally, we present 
F-COLORS, a simplified version of 
COLORS, that significantly reduces its 
run-time complexity. Many system 
parameters may impact the effectiveness of 
a real-time DPM policy. These parameters 
include task workloads, processor profiles, 
and device characteristics. Previous studies 
considered only a small subset of these 
parameters in their performance evaluation. 
Instead, we take a systematic approach to 
analyze all parameters and identify a key 
metric that primarily determines their 
performance. We compare COLORS with 
four algorithms, each of which is derived 
from an existing work, Swaminathan and 
Chakrabarty [Swaminathan 03], Jejurikar 
and Gupta [Jejurikar 04], Cheng and 
Goddard [Cheng 05, Cheng 06]. These 
algorithms well represent the state-of-the-art 
technology in the integration of DPM and 
DVS for hard real-time systems. The 
simulation results show that COLORS 
delivers significantly more energy reduction 
in all system configurations we tested. The 
reduction is as much as 20%. The 
performance of F-COLORS is only slightly 
worse than COLORS even at its largely 
reduced complexity. 
 
 
四、結論 
本計畫執行三年期滿，成果豐碩。已有多篇嵌入式系統及無線網路領域之期刊論文
與研討會論文發表，並於重要之嵌入式系統競賽，獲得冠軍之殊榮。計畫第一年著重於
平台建置與環境設計，第二年開發對應之演算法並且實作系統軟體與應用程式，第三年
強調整合，各子計畫的成果相互輔助、驗證與延伸。硬體及系統軟體之進展開啟了嵌入
式系統多媒體應用之濫觴，於其中無線網路必然扮演著重要的資訊傳遞管道，而手持式
裝置市場也已進入戰國時期，各研討會中對省電及效能調教技術之探討依然方興未艾，
本計畫著力於此，以期為嵌入式系統省電暨性能最佳化技術扮演承先啟後之一員。 
 
 
五、References 
[Chen 01]H. Aydin, R. Melhem, D. Mossé, and P. Mejía-Alvarez. Determining optimal 
processor speeds for periodic real-time tasks with different power characteristics. In 
EuroMicro Conference on Real-Time Systems, pages 225–232, 2001. 
[Chen 05]J.-J. Chen, T.-W. Kuo, and C.-S. Shih. 1+o approximation clock rate assignment 
for periodic real-time tasks on a voltage-scaling processor. In the 2nd ACM Conference on 
Embedded Software (EMSOFT), pages 247–250, 2005. 
[Mejia 04]P. Mejía-Alvarez, E. Levner, and D. Mossé. Adaptive scheduling server for 
power-aware real-time tasks. ACM Transactions on Embedded Computing Systems, 
3(2):284–306, 2004. 
[Pillai 01]P. Pillai and K. G. Shin. Real-time dynamic voltage scaling for low-power 
embedded operating systems. In Proceedings of the 18th ACM Symposium on Operating 
Systems Principles, pages 21–24, 2001. 
[Yao 95]F. Yao, A. Demers, and S. Shenker. A scheduling model for reduced CPU energy. In 
Proceedings of the 36th Annual Symposium on Foundations of Computer Science, pages 
374–382. IEEE, 1995. 
2007. 
[Chen 09] Y.-J. Chen, C.-L. Yang and Y.-S. Chang, “An Architectural Co-Synthesis Algorithm 
for Energy-aware Network-on-Chip Design”, Journal of Sysems Architecture,  Vol. 55, Issue 
5-6, page 299-309, May-June 2009.  
[Chen 10] Y.-J. Chen, C.-L. Yang and P.-H. Wang, “PM-COSYN: PE and Memory 
Co-Synthesis for MPSoCs“, to appear in the proceedings of DATE 2010. 
[Li 07] Guo-Liang Li and Chi-Sheng Shih, Clock Free Data Streams Alignment for Sensor 
Networks , in Proceedings of the 13th IEEE International Conference on Embedded and 
Real-Time Computing Systems and Applications (RTCSA 2007), Daegu, Korea, August 
21-23, 2007. 
[Jun03] J. Jun and M. L. Sichitiu, “The nominal capacity of wireless mesh networks,” IEEE 
Trans. Wireless Communications., vol. 10, pp. 8–14, Oct. 2003. 
[Bruno05] R. Bruno, M. Conti, and E. Gregori, “Mesh networks: commodity multihop ad hoc 
networks,” IEEE Communications Magazine, vol. 43, pp. 123-131, Mar. 2005. 
[Akyildiz05] I. F. Akyildiz and X. Wang, “A survey on wireless mesh networks,” IEEE 
Communications Magazine., vol. 43, pp. 23–30, Sept. 2005. 
[IEEE 802.16j2007] IEEE 802.16j-06/026r4, “Baseline Document for Draft Standard for 
Local and Metropolitan Area Networks Part 16: Air Interface for Fixed and Mobile 
Broadband Wireless Access Systems Multihop Relay Specification”, 2007. 
[IEEE 802.16j2008] IEEE P802.16j/D3, “Draft Amendment to IEEE Standard for Local and 
Metropolitan Area Networks Part 16: Air Interface for Fixed and Mobile Broadband Wireless 
Access Systems Multihop Relay Specification”, 2008. 
[Alexandropoulos 05] T Alexandropoulos, S Boutas, V Loumos, E Kayafas. “Real-time 
change detection for surveillance in public transportation,” in AVSS, 2005. 
[Bergasa 06] LM Bergasa, J Nuevo, MA Sotelo, R Barea, ME Lopez. “Real-time system for 
monitoring driver vigilance,” IEEE Trans Intell Transp Syst, 7(1):63–77, 2006. 
[Cheng 05] H Cheng, S Goddard. “Integrated device scheduling and processor voltage scaling 
for system-wide energy conservation,” in PARC, 2005. 
[Cheng 06] H Cheng, S Goddard. “Online energy-aware I/O device scheduling for hard 
real-time systems,” in DATE, 2006. 
[Im 04] C Im, S Ha, H Kim. “Dynamic voltage scheduling with buffers in low-power 
multimedia applications,” ACM Trans Embed Comput Syst, 3(4):686–705, 2004. 
[Jejurikar 04] R Jejurikar, RK Gupta. “Dynamic voltage scaling for systemwide energy 
minimization in real-time embedded systems,” in ISLPED, pp 78-81, 2004. 
[Khatib 06] IA Khatib, D Bertozzi, F Poletti, L Benini, A Jantsch, M Becara, H Khalifeh, M 
Hajjar, R Nabiev, S Jonsson. “MPSoC ECG biochip: a multiprocessor system-on-chip for 
real-time human heart monitoring and analysis,” in CF, pp 21-28, 2006. 
[Kim 01] M Kim, S Ha. “Hybrid run-time power management technique for real-time 
embedded system with voltage scalable processor,” in LCTES, pp 11-19, 2001. 
[Kim 02] W Kim, D Shin, HS Yun, J Kim, SL Min. “Performance comparison of dynamic 
voltage scaling algorithms for hard real-timesystems,” in RTAS, pp 219-228, 2002. 
[Lu 00] YH Lu, L Benini, G DeMicheli. “Low power task scheduling for multiple devices,” in 
CODES, pp39-43, 2000. 
[McIntire 06] D McIntire, K Ho, B Yip, A Singh, W Wu, WJ Kaiser. “The low power energy 
aware processing (LEAP) embedded networked sensor system,” in IPSN, pp 449-457, 2006. 
[Pillai 01] P Pillai, KG Shin. “Real-time dynamic voltage scaling for low-power embedded 
operating systems,” in SOSP, pp 89-102, 2001. 
[Swaminathan 03] V Swaminathan, K Chakrabarty. “Energy-conscious, deterministic /O 
device scheduling in hard real-time systems,” in IEEE Trans Comput-Aided DesIntegr 
CircSyst, 22(7):847–858, 2003. 
第十五屆 IEEE即時暨嵌入式技術與應用研討會 
(The 15th IEEE Real-Time and Embedded Technology and 
Applications Symposium (RTAS)) 
郭大維 
國立台灣大學資訊工程系 
 
一.  參加會議經過與心得 
 
The 15th IEEE Real-Time and Embedded Technology and 
Applications Symposium (RTAS) was held between April 13 and 16 at 
the San Francisco, USA. RTAS 2009 is co-located with the 8th 
ACM/IEEE Conference on Information Processing in Sensor Networks 
(IPSN'09) and the International Conference on Hybrid Systems 
(HSCC'09) as part of the second series of Cyber-Physical Systems Week 
(CPSWEEK) during April 13-16, 2009. The General Co-Chairs of RTAS 
2009 are Chris Gill and Chenyang Lu of Washington University in St. 
Louis, and the Program is Neil Audsley of University of York, UK. The 
conference is sponsored by the IEEE Technical Committee on Real-Time 
Systems as a major event of the community.   
 
There are 11 oral presentation sessions, one poster session, one forum, 
and three plenary sessions. Attendants come from many countries, such 
as USA, Italy, United Kingdom, Canada, Hong Kong, China, and Taiwan. 
In each plenary session, one major topic is presented. For example, the 
first one is on Cyber-Physical Systems, that receives a lot of attention in 
recent years. In this conference, I not only has a paper to present in the 
conference but also serve as a program committee member. As an 
Executive Committee member of the IEEE Technical Committee on 
Real-Time Systems, I am also required to attend the Executive 
Committee meeting on April 14. Note that the Executive Committee 
conference. It helps me to identify scientific foundations and technologies 
that integrate cyber-concepts with the dynamics of physical and 
engineered systems. I am also interested in physical processes that 
include HW/SW co-design and inter-discipline research topics. 
Interesting topics in the conference includes dynamic reconfiguration of a 
large-scale battery system. There are also a number of researcher working 
on integration frameworks for CPS. Example research directions are 
languages to specify systems and their behaviors and programming models 
for CPS systems.  
 
We have a paper entitled “EMWF for Flexible Automation and 
Assistive Devices.” It was presented in the afternoon of April 14. Despite 
tremendous progress in quality of health care worldwide, recent 
publications on this subject still report alarming statistics on occurrences 
and consequences of preventable medication errors. This fact has 
motivated our work on identifying and preventing medication errors and 
the advent and use of an increasingly broader spectrum of information 
systems and smart devices as tools for prevention of medication errors 
and improvement in compliance. We are interested in intelligent devices 
and tools designed specifically for prevention of administration error. 
Under the leadership of Prof. Jane Liu, we proposed the design and 
prototyping of intelligent medication carts without the above mentioned 
deficiencies. Our current work include tools and devices for error-free 
medication process specifically and on component-based design, 
architecture and development of configurable automation and assistive 
devices in general. More information of the work can be found at project 
SISARL (Sensor Information Systems for Active Retirees and Assisted 
Living) homepage (URL: http://sisarl.org). 
 
EMWF for Flexible Automation and Assistive Devices  
 
   T. S. Chou, Y. C. Wang           
and J. W. S. Liu 
Institute of Information Science 
Academia Sinica, Taiwan 
{tschou, wych, 
janeliu}@iis.sinica.edu.tw 
S. Y. Chang, Y. F. Lu, M. K. 
Ouyang, T. W. Kuo and C. S. Shih 
Dept. of Computer Science and 
Information Engineering 
National Taiwan University, Taiwan 
{ktw, cshih}@csie.ntu.edu.tw 
J. S. Hu 
Department of Electrical and Control 
Engineering 
National Chiao-Tung University, 
Taiwan 
jshu@cn.nctu.edu.tw
 
 
Abstract—This paper describes an embedded workflow 
framework (EMWF) that enables flexible personal and home 
automation and assistive devices and service and social robots 
(collectively referred to as SISARL) to be built on workflow 
architecture. The process definition language supported by 
EMWF is called SISARL-XPDL. It consists of a subset of the 
WfMC standard XML Process Definition Language (XPDL) 
2.0, together with elements that implement common 
mechanisms for robot behavior coordination. EMWF provides 
workflow engines for Linux and Windows CE platforms. The 
engines are written in C in order to keep their memory 
footprint and runtime overhead small. Performance data show 
that the overheads introduced by the engine and workflow 
data are tolerable for most SISARL devices. 
Keywords-embedded Middleware; workflow architecture, 
component-based design, automation and assistive devices 
I.  INTRODUCTION 
Thanks to steady advances in embedded systems and 
robotic technologies over the years, a diverse spectrum of 
smart devices targeted for elderly individuals and 
functionally limited users have become economically 
feasible today. Examples include personal and home 
automation and assistive devices (e.g., automatic medication 
dispenser, smart storage pantry, robotic housekeeping aids, 
and mobility assistants [1-11]) and service and social robots 
(e.g., object fetchers and delivery robots, robotic pets, and 
intelligent physical-therapy companions [12-17]). We call 
them collectively SISARL (Sensor Information Systems for 
Active Retirees and Assistive Living) [18]. Such devices are 
increasingly more essential for an increasingly larger aging 
segment of the world population.  
As tools designed to help their users stay well and live 
independently or to improve the quality and reduce the cost 
of medical and long-term care, SISARL must be flexible: A 
flexible device can be easily configured to work with a 
variety of sensors and controllers, rely on different support 
infrastructures and operate in different environments. It can 
be easily customized to suit its user. Furthermore, the device 
can adapt to changes in its user’s needs and skills over the 
years while it is in use.  
This paper describes an embedded workflow framework 
(EMWF) as architectural foundation for the design and 
implementation of flexible SISARL. Even today, all but the 
simplest SISARL devices are handcrafted, and handcrafted 
devices are difficult and costly to configure and customize. 
EMWF was motivated by this fact. Its primary purpose is to 
reduce the level of expertise and effort required to design 
and implement from reusable components flexible SISARL 
devices in general and behavior-based robots in particular. 
As its name implies, EMWF is based on the try-and-true 
workflow approach [19] that is widely used for automation 
of business processes of enterprises worldwide. 
Components of an application based on this approach are 
workflows. Each workflow is composed from elementary 
steps called general activities (or simply activities). In 
workflow-based embedded devices, some activities are done 
by executable code running on a CPU; others are by 
hardware components. A semi-automatic device also 
contains user activities carried out manually. The orders and 
conditions under which activities in a workflow are 
executed, the resources used for their execution, and 
interactions and communications among activities are 
specified either by textual workflow definitions or 
graphically by one or more workflow graphs. Some 
workflow graphs resemble task graphs used to represent 
workloads in real-time systems. Each node represents an 
activity. Each directed edge represents a transition from the 
source activity to the sink activity, and consequently a 
precedence relation between the activities. Each workflow 
has a start and one or more stop. They are special activities 
that have no predecessor and successor, respectively, in the 
workflow. Many workflows in embedded devices are event-
driven. Such a workflow is often defined as a state machine 
in which device and user activities cause the machine to 
transition from state to state. 
A key element of a platform for workflow applications is 
the workflow engine (or engine for short). The engine 
executes activities implemented by software components 
and commands and coordinates activities carried out by the 
user(s) and hardware components. In addition, the engine 
provides and executes built-in activities that start and stop 
workflows, sequence and synchronize general activities in 
each running workflow and facilitate their communications 
as specified by the definition of the workflow. By managing 
control and data flows among general activities, scheduling 
and allocating resources among competing workflows and 
enforcing rules and policies on behalf of the applications, 
the engine dynamically integrates the application 
components that are implemented by workflows.  
15th IEEE Real-Time and Embedded Technology and Applications Symposium
1080-1812/09 $25.00 © 2009 IEEE
DOI 10.1109/RTAS.2009.21
243
Authorized licensed use limited to: National Taiwan University. Downloaded on February 8, 2010 at 00:28 from IEEE Xplore.  Restrictions apply. 
one exception, they meet this requirement naturally. The 
exception is when behaviors share sensor devices; a behavior 
needs to send the data it reads to receiving behavior(s). 
Having the sender blocked, waiting for the receiver to be 
ready is not acceptable. ESAIR provides a middleware 
component, called (behavior) supervisor, to facilitate their 
communication: A behavior can send data asynchronously to 
the supervisor. The supervisor holds the data and delivers the 
data to the receiver when the receiver is ready. The EMWF 
workflow engines also provide this kind of service. 
Table 1 lists examples of built-in activities. Some of the 
symbols here are from Windows Workflow Framework [24]. 
A block arrow directed to or from an activity represents 
multiple transitions (edges) in or out of the activity.  
TABLE I. EXAMPLE OF BUILT-IN ACTIVITIES
G
en
er
ic
 b
ui
lt-
in
s 
Start Stop While
Route If else (2-way XOR split)
Split Merge
Throw Exception
Execute workflowInvoke workflow
B
ui
lt-
in
s 
fo
r B
C
 
S
P R
A V
+

Superposition Arbiter Voter
Mode change
Pull 
data
Push 
data
Wait for events / timers / workflow triggers
Delay /timeout Set events / timers
The top half of the table lists generic built-ins required by 
typical embedded applications. The names of some of them 
(e.g., if-else and wait) more or less tell what they do.  The 
more common names of built-in activities Split and Merge
listed in the first row are branch and join, respectively. The 
former branches an activity into multiple successor activities, 
and latter joins multiple predecessor activities into a single 
successor. They are special cases of the Route activity. A
Route can have arbitrary numbers of incoming and outgoing 
transitions. When used with transition restrictions, Route can 
implement arbitrary complex flow logic, including 
combinations of conditional XOR and AND merges of 
incoming transitions and splits of outgoing transitions. We 
will return to provide an illustrative example in Section V 
and discuss the behavior coordination (BC) activities listed 
in the bottom half of the table. BC activities are specifically 
for behavior-based robotic applications.  
As an example, Fig. 1 shows the workflow-based 
structure of an intelligent medication cart, highlighting its 
application components. The device is designed to assist its 
user in administration of medications to patients under the 
user’s care. The cart has microcontrollers and motors for 
propulsion, an ultrasound range finder and other sensors for 
guidance and navigation, a RFID reader and a bar-code 
scanner for medication and patient identification, and so on. 
Some device drivers, like the workflow engine itself, are not 
built from workflows; these parts are shown in dark color in 
the figure. The other functions are provided by workflows 
components. 
Workflow   engine
Ha
rd
w
ire
d 
D
riv
er
s 
Environment 
Interaction
Sense edgeSense edge
Sense contactSense contact
Check the routeCheck the route
on  edge?
No
contacted? No
Yes
on route?
Yes
No
EdgeEdge
20Hz
ContactContact
10Hz
TrackTrack
2Hz
Yes
OS  kernel
A
Component workflows
Figure 1. Structure of a workflow-based device 
Fig. 2 shows the guidance and propulsion module of the 
medication cart in its entirety to highlight the distinguishing 
characteristics of embedded workflows. In this and 
subsequent workflow graphs, we use rectangular boxes to 
represent general activities of the application. When there is 
no need to be specific, we use dotted circles to represent all 
built-in activities. External activities are in dotted boxes 
labeled environment interaction and cart components. This 
module operates automatically; it has no user activities. 
Environment 
Interaction Software Workflows Cart Components
Sense edgeSense edge
Sense contactSense contact
Check the routeCheck the route
On  edge? No
Yes
contacted? No
Yes
on route?
Yes
No
EdgeEdge
20Hz
ContactContact
10Hz
TrackTrack
2Hz
Track maneuverTrack aneuverm
Move the cartove the cartM
Back & random 
move
(Contact 
maneuver)
Back & rando  m
ovem
(Contact 
aneuverm )
Follow edge
(Edge 
maneuver)
Follow edge
(Edge 
aneuver)m
A
Figure 2.  Guidance and propulsion workflows 
B. Engine Structure 
Fig. 3 shows a different view of the workflow-based 
structure of a device. Focusing on the engine, the figure 
depicts applications simply as workflow scripts. We will 
provide further details on EMWF internal representation of 
workflow definitions and attributes in Section V. The XPDL 
term participant in the callout at upper right corner refers to 
a resource that is managed and allocated to workflows by the 
engine. Participants required by workflows are declared in 
245
Authorized licensed use limited to: National Taiwan University. Downloaded on February 8, 2010 at 00:28 from IEEE Xplore.  Restrictions apply. 
the scripts. These components form the core of the engine 
and contribute the bulk of memory footprint and context 
switch overhead of the engine. Needless to say that it is 
important to keep these overheads small.  
A. Design Rationales 
We ruled out having single-threaded workflow processor 
by default even through having a single thread execute all 
software and built-in activities is a way to minimize context 
switch and synchronization overheads. Rather, the maximum 
number of threads is a configuration parameter of EMWF 
engines, allowing the developer of a device to choose the 
configuration best for the device. 
1) Alternative Structures: There are two strategies to 
structure a multi-threaded workflow processor: 
 Workflow-level assignment (WLA): Each thread is 
dedicated to a workflow.
 Activity-level assignment (ALA): Activities from 
multiple workflows are queued as work items and 
executed by worker threads serving the queues.  
We call engines using these strategies WLA engine and ALA
engine, respectively.  
Existing workflow engines typically used the WLA 
strategy. The Linux EMWF engine, called LIWWE (Light-
Weight Workflow Engine) in [42], uses the WLA strategy as 
it suits the Linux thread model and APIs.  
We implemented both the WLA strategy and the ALA 
strategy on Windows CE. The ALA engine can take better 
advantage of the Windows thread model and APIs. We 
implemented the WLA strategy on Windows CE so we can 
make fair comparison of the alternatives strategies. 
2)  Pros and Cons: We note that in a WLA engine, a 
thread assigned to a workflow executes both general and 
built-in activities in it. Hence, most of the transitions 
between activities incur no context switch. Another 
advantage of this strategy is that the workflow manager does 
not need to handle blocking built-in activities (e.g., delay and 
wait in Table 1) specially. It can simply let the thread wait 
when a workflow processing thread executes such a built-in. 
It is expensive for threads to change priority, however. This 
is why the current versions of WLA engines do not support 
varying priority within workflows: Priority increments of 
activities are ignored.  
As we will see shortly that in an ALA engine, varying 
priority in a workflow is accomplished at no cost by having 
threads of differing fixed priorities execute activities in it. 
Similarly, coherent time order for all tasks within a device is 
accomplished naturally with no additional cost by having all 
timing events handled by a single thread. On the other hand, 
every transition from one general activity to another incurs at 
least one context switch. As we will see in Section VI, this is 
a disadvantage when compared with WLA engines.  
3)  Memory Allocation: We note that many low-level 
embedded components (e.g., the workflows in Fig. 2) have 
stringent timing requirements. This is why we try to keep 
runtime overheads low, sometimes at the expense of memory 
footprint. As stated earlier, the engine manager loads .wfs 
files of all workflows needed for all modes and adaptation 
during initialization. This allows the workflow manager to 
dynamically allocate memory for all instances of activities 
and workflows during initialization.
B. WLA Engines 
Again, the workflow manager creates and initializes 
threads needed to execute software and built-in activities 
when the engine is initialized, a group of POSIX threads in 
the Linux engine and user-mode threads in the Windows CE 
versions. All threads execute at fixed priorities. Fig. 5(a) 
depicts the structure of LIWWE and the WLA version(s) of 
the Windows CE engine. The dotted box below the engine 
manager encircles the workflow manager and processor. 
Executables (.dll or .exe)
Engine Manager
Workflow 
Scheduler
Thread 
Dispatcher
Workflow scripts
D
ev
ic
e 
D
riv
er
s
(a)
Executables (.dll or .exe)
D
ev
ic
e 
D
riv
er
s 
Activity Processor
Workers
Work 
queues
Result queues
Workflow Manager
GAS (General 
activity 
scheduler) 
BAA (Built-in activity accelerator)
Non-blocking queue Blocking queue
To BAA 
& GAS
Engine Manager
Workflow scripts
(b)
Figure 5. WLA and ALA engine structures 
1) Basic Version: The workflow manager attaches a 
thread to each workflow when it initializes the workflow and 
schedules the thread to execute both general and built-in 
activities in the workflow. The thread inherits the priority of 
the workflow. A workflow may contain Split and Merge 
activities. When a split occurs, the manager selectively 
attaches additional threads to execute the successors. When 
multiple threads join for a merge activity, the last thread that 
reaches the merge executes the merge operation.
When a thread executes an end activity, the manager 
detaches it from the workflow and returns it to the thread 
pool. On the other hand, after a thread executes the
EndTrigger of a workflow, the workflow manager may 
terminate the thread and other threads attached to the 
workflow since the activity signals the termination of the 
workflow process.  
2) Enhanced Version: Before performance data became 
available, we conjectured that since the number of context 
247
Authorized licensed use limited to: National Taiwan University. Downloaded on February 8, 2010 at 00:28 from IEEE Xplore.  Restrictions apply. 
each workflow definition is the declaration of participants 
needed by the workflow. Examples here are the user needed 
by the manual workflow, the function sched ( ) needed by 
the schedule workflows, and alarm and clock needed by the 
notify workflow. 
Dispense 
dose(s) & 
set timer
notify_workflow
Notify 
user by 
alarm
timer_workflow
schedule_workflowSend
email
trg_1
Compute 
next dose time &
medication(s) 
alarm
Follow direction
timer_workflow
notify_workflow
schedule_workflow
Update 
record
S
ch
ed
( )
late
W
or
kf
lo
w
P
ro
ce
ss
di
sp
en
se
r
al
ar
m
, c
lo
ck
 …
Set trg_1
Figure 6. Workflows in a medication Dispenser 
The term SubFlow in Table 2 refers to a workflow that is 
called synchronously by another workflow. In the example in 
Fig.6, schedule_workflow is a subflow of notify_workflow. A
workflow may also be called asynchronously; the interaction 
between notify workflow and timer workflow is an example. 
The example has only a two-way XOR split (i.e., an if-
else built-in activity) depicted as a circle labeled R. XPDL
Event activity is a general primitive for alternating the 
courses and timing of a workflow process. An event trigger 
can be time, timer, rule, results, result errors, and so on. The 
manual process illustrates the use of StartEvent. In this 
example, it is an alarm event. When sounded, it triggers the 
start of the manual workflow: The user reports to the 
dispenser, which in turn sets the intermediate event (trg_1)
waited for by notify_workflow.
Finally, XPDL schema provides a standard way for 
introducing user specific extensions. We have not yet fully 
exploited this aspect to add workflow attributes, such as rate 
and latency, which can be used to help the engine better 
service workflows with rate and deadline constraints.  
B. Built-Ins for Behavior Coordination 
EMWF supports large grain workflow-based design of 
behavior robots: In a robot with this architecture, behaviors 
are implemented as activities. Once starts, each behavior 
runs to completion without requiring engine attention.  
Some transitions between activities are for coordination 
of behaviors. As Table 1 indicates, built-in behavior-
coordination (BC) activities provided by SISARL-XPDL are 
arbiter for fixed-priority arbitration and superposition and 
voter for command fusion. We choose to support these 
simple mechanisms mostly because they are commonly used. 
One may argue that the advantages of more general 
mechanisms (e.g., variable priority, multiple objective 
coordination and fuzzy fusion) cannot offset their 
disadvantages in higher complexity and variability [44, 45]. 
Another reason is that we can implement these special-
purpose built-ins from standard XPDL elements together 
with simple library functions. 
As an example, Fig. 7 shows the workflow that 
implements a 3-way fixed-priority arbiter. The input I_1 has
the highest priority, and I_3 the lowest. In other words, the 
output O is the command generated by activity (behavior) 
I_1 if data are present at the output of the activity when it 
completes. Otherwise, it is from I_2 if data are present on I_2.
The output is the command generated by I_3 only when both 
activities I_1 and I_2 generated no data. 
A
I_1
I_2
I_3
O
Deadline 
10 ms
I_1
I_2
I_3
Enable_1
Enable_2
Enable_3
Move_2
Move_1
Move_3
JoinAND_
SplitXOR
JoinXOR O
Figure 7. Workflow implementing a 3-way fixed priority arbiter 
The 3-way JoinAND_SplitXOR and JoinXOR routers in 
the figure are standard XPDL elements. The workflow also 
contains activities implemented by Enable and Move
functions. For sake of legibility, we omit all edge labels that 
specify the conditions under which the corresponding 
transitions are enabled. An Enable activity (say Enable_i)
sets the condition of its outgoing transition to TRUE, and 
thus enables the transition, when it finds data generated by 
its predecessor; otherwise, it disables its outgoing transition. 
Fixed priority arbitration is accomplished by the router 
JoinAND_SplitXOR. Starting from the instant when one of its 
incoming transitions is enabled, the router scans in fixed 
order all of its incoming transitions for an interval of length 
specified by deadline (e.g., 10 ms as shown in Fig. 7). Upon 
finding the first enabled transition (say from Enable_k), the
condition (I_k == TRUE) of the corresponding outgoing 
transition of the router becomes TRUE. This causes the 
successor Move_k activity to be executed, moving the data 
from I_k to the output and enabling the transition from 
Move_k to the JoinXOR router. Being an exclusive-OR 
merge activity, the outgoing transition of the router is 
enabled and its successor becomes ready for execution. For 
example, suppose that the JoinAND_SplitXOR router finds 
the transition from Enable_2 enabled, but the transition from 
Enable_1 remains disabled during its 10 ms scan. Because 
the transition from Enable_2 is scanned before the transition 
from Enable_3, the condition of the outgoing transition to 
Move_2 becomes TRUE and the transition enabled regardless 
whether the transition from Enable_3 is enabled. The 
command sent to the robot is from activity I_2.
C. Workflow Scripts 
For relative simple applications, it suffices for us to parse 
SISARL-XPDL definitions of workflows directly to binary 
scripts. We anticipate, however, the need for intermediate 
scripts in C. They can offer developers of large complex 
workflow applications added convenience during debugging 
and for experimentation and evaluation purposes. 
249
Authorized licensed use limited to: National Taiwan University. Downloaded on February 8, 2010 at 00:28 from IEEE Xplore.  Restrictions apply. 
Parts (a), (b) and (c) of Fig. 11 show how RTBU changes 
with granularity for branch, split and iteration patterns. We 
leave out plots for sequential and merge patterns because 
they exhibit similar trends as branch and split, respectively. 
The most noteworthy plot is the one on iteration pattern. For 
this pattern, we were able to measure the RTBU for loads 
with very small granularity. The data validate what we 
expected: The disadvantage of ALA engine in terms of 
context switches can be serious sometimes. In this instance, 
the iteration pattern has 2000 transitions; each transition 
incurs a context switch on the ALA engine. In contrast, no 
context switch incur on WLA and WLA-HT engines and for 
hardwired code.  
190
192
194
196
198
200
202
204
206
208
210
Sequential Branch Split Merge Iteration
ALA
WLA
WLA-HT
Hardwired
Test pattern
R
es
po
ns
e 
tim
e 
pe
r b
as
ic
 u
ni
t (
ns
)
(Granularity = 10000)
Figure 10. Response time per basic unit for different test patterns  
Finally, to be sure that memory footprint is not an issue 
for majority of SISARL devices, we ran Roomba workflows 
on the ALA engine. The engine consumes approximately 
524 KB after it starts. It consumes 1114 KB after 
Roomba .wfs file is loaded. Footprint of this order is but a 
small fraction of available memory for typical service robots. 
VII. SUMMARY AND FUTURE WORK
We described in earlier sections the design and 
implementation of the embedded workflow framework 
EMWF. EMWF provides light-weight engines for Linux and 
Windows CE platforms. It also provides a small but 
extensible language, called SISARL-XPDL, for defining 
workflow processes in embedded automation and assistive 
devices and service robots. The SISARL-XPDL preprocessor 
translates special-purpose built-in activities into standard 
XPDL. EMWF engines cannot execute XPDL directly. This 
is why EMWF parses XPDL workflow process definitions 
either into intermediate C scripts or directly into binary 
workflow scripts for execution by the engine.  
EMWF engines are available under GPL license at 
http://of.openfoundry.org/projects/emwf. Runtime overheads 
introduced by all the engines are tolerable for most types of 
workloads. An exception is when workflows have small 
granularities. For that kind of workloads, ALA engine 
performs poorly. WLA-HT or WLA engine should be used.  
Current versions require that all workflow scripts a 
device needs to operate in different modes be in the system 
and initialized before the device starts to run. This clearly 
limits the ability of the device to adapt while running. 
Removing this limitation is one of our near term goal.  
As middleware, a workflow engine duplicates many 
functions (e.g., scheduling and synchronization) of the 
underlying operating system. The extra runtime and memory 
overheads thus consumed can be further reduced (and 
eliminated all together in some cases) by having the engine 
runs on a thin hardware abstraction layer, replacing the OS. 
This is our goal beyond the immediate work needed to make 
EMWF ready for industrial use. 
0
50
100
150
200
250
300
350
400
450
1000 10000 100000 1000000
ALA
WLA
WLA-HT
Hardwired
Granularity (multiples of basic unit)
R
es
po
ns
e 
tim
e 
pe
r b
as
ic
 u
ni
t (
ns
) (a) Branch
0
50
100
150
200
250
300
1000 10000 100000 1000000
ALA WLA
WLA-HT Hardwired
Granularity (multiples of basic unit)
R
es
po
ns
e 
tim
e 
pe
r b
as
ic
 u
ni
t (
ns
) (b) Split
Granularity (multiples of basic unit)
R
es
po
ns
e 
tim
e 
pe
r b
as
ic
 u
ni
t (
ns
)
0
500
1000
1500
2000
2500
3000
10 100 1000 10000
ALA
WLA
WLA-HT
Hardwired
(c) Iteration
Figure 11. Response time per basic unit as function of granularity 
ACKNOWLEDGMENT
This work was partially supported by the Taiwan 
Academia Sinica thematic project SISARL, Sensor 
Information Systems for Active Retirees and Assisted Living. 
251
Authorized licensed use limited to: National Taiwan University. Downloaded on February 8, 2010 at 00:28 from IEEE Xplore.  Restrictions apply. 
出席國際學術會議報告 
 
報 告 人 
姓 名 楊佳玲 
服 務 機 構
及 職 稱
台 灣 大 學 資 工 系
副教授 
會 議 時 間 
地 點 
2009/08/19 – 2009/8/21 
會議名稱 IEEE/ACM International Symposium on Low Power Electronics and 
Design (ISLPED 2009) 
發表論文題目 PPT: Joint Performance/Power/Thermal Management of DRAM 
Memory for Multi-Core Systems 
 
一、參加會議經過 
 
 IEEE/ACM International Symposium on Low Power Electronics and 
Design 為當今最著名之國際低功率研討會。 本人很榮幸發表論文於2009
ISLPED，故於八月赴美參與ISLPED 並報告研究成果。 
  
此次會議在circuits, architecture, system/software 各層之最新
power/thermal研究。 共含12 paper session, 2 poster session, 及 4 
tutorials。本人於Session 2.2.1: Dynamic thermal management發表論文 
“PPT: Joint Performance/Power/Thermal Management of DRAM Memory for 
Multi-Core Systems”，此篇論文之發表於會中廣獲好評,獲得最佳論文獎。
本人除參與論文發表外，並擔任Green at the Micro-Scale: Towards 
Self-Powered Embedded Systems 之session chair.  
 
二、與會心得 
此次會議之主題為”Green Data Centers and Computing”。會議議程含4  
PPT: Joint Performance/Power/Thermal Management of
DRAM Memory for Multi-Core Systems
Chung-Hsiang Lin, Chia-Lin Yang
Department of Computer Science and
Information Engineering
National Taiwan University, Taipei,
Taiwan(R.O.C).
{f94040,yangc}@csie.ntu.edu.tw
Ku-Jei King
Taiwan Systems and Technology Lab
IBM Corporation
kujei@tw.ibm.com
ABSTRACT
With the popularity of multi-core architecture, to sustain the
memory demands from diﬀerent cores, the memory system is ex-
pected to grow signiﬁcantly in both speed and capacity. This
will lead to increasing power consumption in the memory system.
Therefore, it is critical to address the power issue in the mem-
ory subsystem. In designing a power-aware memory system, due
to the interplay among power, thermal and performance, all the
three factors need to be taken into account. In this paper, we
propose the ﬁrst joint performance, power and thermal manage-
ment framework (PPT) through orchestrating task execution and
page allocation. The PPT framework adapts to system loading
to maximize power saving and avoid memory hotspot at the same
time whiling sustaining the system bandwidth demand.
Categories and Subject Descriptors: C.4 [Performance of
systems]: Design studies
General Terms: Measurement, Performance, Design, Experi-
mentation.
Keywords: DRAM memory, Power, Temperature, Scheduling,
Page allocation.
1. INTRODUCTION
Optimizing power consumption has become a critical design
issue not just for battery-operated mobile devices but also for high
end systems due to the reliability issue and cooling/packaging
cost. To achieve an energy-eﬃcient design, all system components
need to be considered. It has been observed that the memory
system is one of the main contributors to the overall system power
consumption[9, 10, 11, 16]. Recently, multi-core processors are
widely adopted. To sustain the increasing bandwidth demand in
multi-core systems, the capacity and speed of DRAM memories
are also expected to grow signiﬁcantly. This will lead to increasing
power consumption in the memory system. Therefore, it is ever
increasingly important to design a power-aware memory system.
In designing a power-aware memory subsystem, power, per-
formance and thermal factors all need to be taken into account.
Various data allocation policies have been proposed to improve
memory system performance or reduce memory power consump-
tion. Most of them look at only an individual factor. A purely
performance-driven memory system uses data striping to explore
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
ISLPED’09, August 19–21, 2009, San Francisco, California, USA.
Copyright 2009 ACM 978-1-60558-684-7/09/08 ...$10.00.
maximum parallelism among memory accesses to gain perfor-
mance[13, 17]. In contrast, a power-driven memory system tries
to cluster data accesses in a small number of memory modules and
put the remainings into a low-power operating mode [4, 5, 9]. Al-
though popularity data layout is good for power, it is not able
to expose memory parallelism thereby impairing memory system
performance. Recently, due to the high power consumption of
the memory system, memory thermal management is becoming
a critical issue[6]. To protect the memory system from thermal
emergency, several dynamic thermal management (DTM) tech-
niques have been proposed[6, 2, 12, 7]. Although reducing power
in general can lower average temperature, it may cause adverse
eﬀect on peak temperature since clustering memory requests in
few modules causing rapid temperature rise in active modules.
This may consequently incur more frequent DTM invocation.
Due to the interplay among performance, power and thermal,
a technique optimized for one factor only often causes uncontrol-
lable degradation at other design metrics. To tackle this issue,
in this paper, we propose the ﬁrst joint performance, power and
thermal (PPT) management framework for DRAM memory in
multi-core systems. PPT orchestrates task execution and page al-
location to achieve desirable tradeoﬀ between performance, power
and temperature. In the PPT framework, both threads and mem-
ory resources are partitioned into groups. Threads of the same
group are scheduled concurrently and only one group is active
at each scheduling interval. Memory modules allocated to non-
active groups are put into the low-power mode to save energy.
From thermal aspect, alternating active groups periodically al-
lows memory modules to cool down in their idle periods. The
main challenge in the proposed PPT framework is to determine
how much memory resources should be allocated to a group to
sustain the bandwidth demand of current workloads. To this end,
we propose an Adaptive Grouping mechanism which dynamically
adjusts PPT conﬁguration (i.e., number of groups, and resources
per group) to satisfy the bandwidth demand while minimizing
power consumption.
We compare the proposed PPT framework with two task schedul-
ing/page allocation policies, performance-driven and power-driven
policies. Both policies schedule threads in round-robin fashion.
The performance-driven policy distributes pages to all memory
modules, and the power-driven policy allocates pages in the order
they are accessed. The experimental results show that compared
to the performance-driven policy, PPT reduces power consump-
tion by 24.2% and delivers comparable throughput; while the
power-driven policy achieves 33.7% power reduction at the cost
of peak throughput reduction of almost 50% (9% on the average).
From the thermal aspect, PPT has the lowest peak temperature.
The power-driven policy is 10◦C higher than PPT.
The rest of this paper is organized as follows. Section 2 intro-
duces the basics of DDR2-SDRAM. Section 3 explains our joint
performance, power, thermal management of DRAMmemory sys-
tem. Section 4 describes experimental methodology and Section 5
presents the results of our experiments. We discuss the related
work in Section 6. Finally, Section 7 concludes this paper.
93
Figure 3: Adaptive Grouping mechanism.
Figure 4: 1-group PPT configuration.
If Adaptive Grouping is triggered with a new thread, we need
to examine {g1 ∼ gi+1}, i.e., all the group numbers smaller than
gi due to higher system bandwidth, and one level higher than gi
because we may be able to partition threads into ﬁner granular-
ity due to more threads in the system now. Similarly, if Adaptive
Grouping is triggered because a thread exits the system, the pos-
sible g is {gi−1 ∼ gk}. Given g, the number of channels which can
be allocated to a group (c(g)) is given by the following formula:
c(g) =
{ m·n
g
(g ≥ n)
m (g < n)
(3)
For a memory system withm channels and n ranks per channel,
when we partition the memory system into g groups, each group
has at most m·n
g
ranks. To maximize the number of channels
allocated to a group, the m·n
g
ranks of a group are distributed
to as many channels as possible. For example, in a 4-channel, 2-
rank per channel system, if g is 2, each group can access 4 ranks
( 4·2
2
), and these ranks are distributed to 4 diﬀerent channels.
Therefore, c(2) = 4. In the case when g < n, then each group is
allocated more than m ranks; that means each group could access
all channels, therefore, c(g) = m.
So after the ﬁrst step, we have a new PPT conﬁguration, {gj , c′, r′},
where c′ = c(gj), and r′ = c′.
Step 2: In this step, we examine if we could reduce the chan-
nels and ranks allocated to a group. For example, in a 4-core
system with only 7 threads, we are not able to partition these
threads into multiple groups. Therefore, in the ﬁrst step, gj is
equal to 1, and c’ is the total number of channels in the system.
If BD(gj) < c
′, then we could reduce the channels and ranks allo-
cated to this group. Therefore, in the second step, we decide the
minimal amount of channels c” to satisfy BD(gj) ≤ c′′. There-
fore, in this step, we get a new PPT conﬁguration {gj , c′′, r′′},
where r′′ = c′′.
With the new PPT conﬁguration {gj , c′′, r′′}, the PPT system
will trigger either Merge (X, Y ) → (Z) or Split (Z) → (X, Y )
operations. If gj < gi , we perform Merge which merges threads
in two diﬀerent groups into one group. Let M(X) represent the
memory ranks which can be used by group X, and T (X) repre-
sent the threads assigned to group X. After Merge (X, Y )→ (Z),
T (X) and T (Y ) could allocate pages on both M(X) and M(Y ).
The pages originally allocated in M(X)(M(Y )) do not need to be
re-distributed to M(Y )(M(X)). Merge operations are performed
repeatedly until gj = gi. If gj > gi, a Split operation is per-
formed. After Split (Z)→ (X, Y ), T (Z) is partitioned into T (X)
and T (Y ) with balanced bandwidth between groups in mind. Af-
ter a Split operation, T (X)(T (Y )) could only allocate pages on
M(X)(M(Y )). Split operations are also performed repeatedly
until gj = gi. Note that the Merge and Split operations are
performed on all groups.
A Split operation is more complicated than Merge. After Split
(Z) → (X, Y ), T (X) may still access M(Y ). If most of the
memory accesses of T (X) do not map to M(X), the new PPT
conﬁguration does not take any eﬀect. Therefore, after a Split
operation, page migration may be required to make the access
pattern conform to the new PPT conﬁguration. However, page
migration should be managed carefully since it incurs both per-
formance and energy overheads. We propose a Deferred Page
Migration approach. That is, after Split (Z) → (X, Y ), we do
not move all the pages of group X from M(Y ) to M(X) immedi-
ately. Since after a Split operation, those threads might bring in
new pages, which are very likely to be hot data due to temporal
locality. Therefore, we monitor how many accesses of T (X) miss
inM(X). Page migration is triggered when the miss ratio reaches
a predeﬁned threshold (i.e., page migration threshold). For each
page migration, we move n recently accessed MRU pages of T (X)
from M(Y ) to M(X). Note that, we also monitor how many ac-
cesses of T (Y ) hit in M(Y ) to trigger page migration in the same
way. Deferred Page Migration requires one hardware counter for
each rank to record the number of accesses in the ranks. Please
note that in addition to a Split operation, page migrations could
also be triggered in the case when the number of ranks in a group
is reduced as explained in the second step of Adaptive Grouping.
In the conﬁguration selection process of Adaptive Grouping de-
scribed above, we mainly consider bandwidth demand for resource
allocation. Threads in a group use only one rank per channel. So
if there are not enough threads for partitioning into groups, there
may exist ranks which are not utilized at all. For example, for
a {1, 4, 4} conﬁguration as illustrated in Figure 4, rank1 of all
channels are not allocated to any group. In this case, we will al-
low a thread to allocate new pages to those unutilized ranks when
the designated ranks are full.
3.2 Thermal Control Policy
Thermal control in the PPT framework is achieved through
Group Switching and Activity Migration. When the system has
enough threads for partitioning into groups, balanced tempera-
ture among ranks could be achieved through periodically schedul-
ing threads in diﬀerent groups as illustrated in Figure 2. The
question remains to be answered is how long the group switch
interval should be. To achieve power saving and prevent mem-
ory ranks from overheating, the group switch interval needs to
be long enough to transit idle ranks into a low-power mode, and
short enough to balance the temperature among memory ranks.
When a rank is idle, according to the power management policy
of DDR2, it takes several microseconds to perform a refresh op-
eration and enter the precharge power-down state, which is the
state that consumes the least power. Therefore, the group switch
interval should be longer than several microseconds to transit the
rank into the precharge power-down state. As for thermal con-
trol, we stressed the memory system to see how fast temperature
rises. We found that with the heaviest loading, a DRAM chip
only increases at most 1◦C in 0.2 second2. Therefore, we know
that if group switch interval is shorter than 0.2 second, temper-
ature could be kept balanced among ranks to avoid hotspot. In
modern operating system, context switch interval is usually set to
1ms, which is much longer than several microseconds, and shorter
than 0.2 second. Therefore, setting group switch interval equal to
context switch interval could achieve power savings while main-
taining balanced temperature at the same time without causing
extra OS overheads.
2We use the simulation setup described in Section 4 to perform
this experiment.
95
Figure 6: Rank Miss Rate, new page rate and number
of page migrations for Workload-1.
the metric to quantify if the memory system behavior conforms to
the new PPT conﬁguration. The higher Rank Miss Rate means
that memory behavior deviates more from the desired pattern of
the new PPT conﬁguration. We also show the new page rate
(i.e., the ratio of new pages to total accessed pages in a sample
period), and the number of page migrations. In Figure 6(a), the
Rank Miss Rate does not increase right after the Split operation,
because a lot of new pages are brought in during the time inter-
val 1.5s ∼ 1.6s. The accesses to the new pages conform to the
new conﬁguration. Therefore, Rank Miss Rate is low and only
a few page migrations are triggered. However, in the time in-
terval 1.6s ∼ 1.8s, threads scheduled at that time access more
old pages, therefore, Rank Miss Rate is higher and more page
migrations are triggered. After 1.9s, Rank Miss Rate drops be-
low 10%, and maintains stable thereafter. Figure 6(b) shows the
similar behavior to Figure 6(a) except that the Rank Miss Rate
increases right after the Split operation and it triggers more page
migrations since less new pages are brought in. The average num-
ber of page migrations in Figure 6(b) is about twice as that in
Figure 6(a). The experimental results show that the power over-
head of page migrations is no more than 4% in the overall power
consumption, and the performance overhead is negligible, because
page migrations only consume less than 1% of the bandwidth per
channel.
In Figure 7, we show the memory throughput and power con-
sumption of the performance-driven, power-driven and Adaptive
Grouping mechanisms for Workload-2. Recall that Workload-2
has very light loading, and contains only 6 threads, so there is
only one group in the system. Since the bandwidth demand of
Workload-2 is quite low, all three mechanisms achieve comparable
throughput. The performance-driven policy has the most power
consumption since it still distributes pages to all ranks. Because
Adaptive Grouping could cluster pages of threads into a subset
of ranks, it has similar power consumption to the power-driven
policy.
5.2 Thermal Evaluation
The proposed PPT scheme controls memory temperature through
Group Switching and Activity Migration. We ﬁrst show the tem-
perature proﬁle of Workload-1 to show how group switching per-
forms. We then demonstrate the thermal eﬀect of activity migra-
tion using Workload-2. Please note that the temperature in the
following discussion is the highest temperature among ranks at a
time instance.
Figure 8(a) shows the temperature of performance-driven, power-
driven and PPT mechanisms for Workload-1. We could see that
PPT maintains the lowest temperature among three schemes most
of the time. The power-driven policy results in signiﬁcantly higher
temperature than PPT (e.g., 7.5◦C higher at 1.6s). Its peak tem-
perature even exceeds 85◦C. The performance-driven has pretty
Figure 7: Throughput and power of DRAM system for
Workload-2.
good thermal proﬁle but it is still slightly higher than PPT. The
temperature advantage of PPT over the performance-driven pol-
icy comes from lower power consumption of PPT as discussed in
Section 5.1.
Figure 8(b) shows the temperature of performance-driven, power-
driven and PPT mechanisms with and without activity migra-
tion for Workload-2. Since the system has only one group for
Workload-2, memory temperature cannot be kept balanced through
group switching. So PPT has higher temperature than the perfor-
mance-driven policy, which distributes pages to all ranks. But
with activity migration, we could control the temperature under
a pre-deﬁned threshold (i.e., 85◦C in this case). In contrast, the
power-driven policy and PPT without activity migration have no
way to control temperature, therefore, their temperatures exceed
85◦C. Activity migration consumes less than 1% of the bandwidth
per channel, and incurs about 6% power overheads.
5.3 Summaries of Performance, Power and Tem-
perature of PPT
Table 2 shows the average throughput, peak throughput, power
and peak temperature of the performance-driven, power-driven
and PPT schemes for Workload-1. Throughput and power are
normalized to those of the performance-driven scheme.
We can see that PPT can achieve comparable peak and av-
erage throughput to the performance-driven policy by utilizing
suﬃcient channels to sustain bandwidth demand. The power-
driven policy only achieves about half of the peak throughput of
PPT and performance-driven mechanisms. For the power aspect,
PPT achieves 24.2% power saving, while the power-driven pol-
icy achieving 33.7% power saving compared to the performance-
driven policy. For temperature, PPT has the lowest peak tem-
perature. The power-driven policy is 10◦C higher than PPT.
6. RELATED WORK
To reduce DRAM power consumption, several works are pro-
posed to prolong the idle time of a DRAM bank by reducing
the number of idle memory bank access [8, 15], or by reordering
memory access pattern to prolong memory bank idle time [8, 15,
9, 4, 5]. To reduce the number of access to idle memory banks,
Koc et al. [8] proposed to perform extra computations if doing
so makes it unnecessary to reactivate a bank which is in the low-
power operating mode. Ozturk et al. [15] proposed to replicate
data at several banks to prevent re-activating an otherwise idle
memory bank. Idle time of a DRAM bank can also be length-
ened by concentrating hot data into a subset of memory modules
by proper data allocation or dynamic data migration. Lebeck et
al. [9] propose to concentrate hot data pages on certain memory
banks to prolong the idle time of memory banks in the low-power
mode. Huang et al. [4] propose to allocate data pages of a pro-
cess on certain memory modules to minimize the energy footprint
of each process. In [5], Huang et al. proposed to coalesce short
idle periods by allocating frequently-accessed data pages to hot
97
出席國際學術會議心得報告 
                                                             
計畫編號 
95-2221-E-002-095-MY3(總計劃) 
95-2221-E-002-097-MY3(子計劃) 
計畫名稱 
省電與性能最佳化技術:從應用面至系統面之探討 
子計畫二：多處理器系統晶片最佳化方法與工具設計 
出國人員姓名 
服務機關及職稱 
洪士灝 
台大資訊工程系助理教授 
會議時間地點 2008/12/17~2008/12/20, Shanghai, China 
會議名稱 2008 IEEE/IFIP Internal Conference on Embedded and Ubiquitous Computing (EUC2008) 
發表論文題目 
網路儲存系統上的快取及預先提取機制之最佳化 
Optimizing the Embedded Caching and Prefetching Software on a 
Network-Attached Storage System 
 
一、參加會議經過 
本次 EUC 主要是由 IEEE 與 IFIP 等 Societies 所贊助。總計有 233 篇論文投稿，其中
70 篇文章被接受及收入論文集中，接受率約 30%。另外有 8 個 workshops 也各有論文發
表。Workshops 與 EUC 會議的註冊費是分別收費的，雖然大會並不介意與會人士去聽任
何一邊的演講，但是論文集則是只給有註冊的人員，因此，我們只帶回 EUC 會議的論文
集。 
 
本次整個 EUC 會議加上 Workshops 的舉辦日期是由 12 月 17 日進行至 12 月 20 日，其
中第一天的議程是 Workshops，共分為 8 個不同的主題，包括 End-User Virtualization、
Embedded Software Optimization 、 Security for Ubiquitous Computing 、 Ubiquitous 
Underwater Sensor Network、Ubiquitous Wireless Multihop Networking 、Network Centric 
Ubiquitous Systems、Trustworthiness, Reliability and services in Ubiquitous and Sensor 
neTworks、Trust, Security and Privacy for Pervasive Applications、以及 Trust, Security and 
Privacy for Pervasive Applications。 
 
EUC 的主會議於 12 月 18 日登場，同時有兩個平行的議程(parallel tracks)，所以無法聽
到所有演講。不過，Embedded Systems 和 Ubiquitous Computing 雖然相關，基本上還是
兩個分開的社群，因此議程上是以 Embedded Systems 的主題搭配 Ubiquitous Computing
的主題形成平行的議程。本人參加的都是本人專長領域的 Embedded Systems Track。以
下摘要敘述本人在此會議收穫較大的論文: 
 我們發表的論文是 Optimizing the Embedded Caching and Prefetching Software on a 
Network-Attached Storage System，這是我們國科會的成果。利用我們針對嵌入式系統所
開發的效能分析工具，我們成功的分析上的軟體運作以及效能瓶頸，並且提出新的快取
(caching)和預先提取(prefetching)的機制。由於我們所探討的嵌入式應用的軟體，網路儲
存系統建構於 Linux 作業系統之上，其複雜度遠超過傳統的嵌入式軟體。因此，幾位與
會人士對於我們的效能分析工具相當感興趣，在會後我們也與他們做進一步的討論與交
流。 
 
大會除論文發表之外，還安排幾個 Keynote Speeches，以及晚間餐會及活動。來自 Ohio 
State University 的 Prof. Xiaodong Zhang 給的 Keynote Speech 的題目是 Research Issues and 
Challenges to Advance System Software，與我的研究方向相當接近，重點放在目前的多核
心系統設計上所遇到的問題及瓶頸，包括硬體平台如何規劃，軟體如何平行化，以及硬
軟體統合設計的挑戰。除了在研究上的努力，人才的培育也相當重要。演講後，我有幸
和 Prof. Xiaodong Zhang 同席進餐，除了交換學術心得之外，也聊到美國中西部的生活，
由於 Ohio State University 與我的母校 University of Michigan 就在隔壁州，彼此是多年來
在美國大學中最著名的競爭對手，在學術和運動上都爭的激烈，在談論許多趣事中，拉
近彼此的距離。 
圖一 Keynote Speech 摘要投影片(Prof. Xiaodong Zhang) 
 
本次 2008 年的第五屆 EUC 會議由上海交通大學主辦，也由此可看出中國大陸在這個領
域的崛起。前次在台北舉行時，已有不少大陸學者來訪，並且積極的與國際接軌，令人
印象深刻。此次由大陸主辦，在有東方之珠之稱的上海市舉行，啟程之前，不免對此次
會議多了一些期待。 EUC 的歷史雖然只有幾年，但是與會者人數日益增多，可以看出
這個會議在嵌入式系統的領域的影響力。本次會議中，中國大陸人佔大多數，雖然是佔
著地利之便，但是大陸在這個領域教學研究所投入的資源與努力，以及高科技產業的崛
起，也有非常顯目的表現。台灣在此領域也投入甚多，得到 Best Paper Award，也是非常
突出的成績。剛好在會議之前，兩岸正式直航，大幅減少我們往返上海的時間，單程步
道兩小時的飛機，對兩岸的交流有莫大的助益。兩岸的政治不提，但學術文化的交流，
以及經濟產業的合作，已經對兩岸人民的生活產生了重大影響，希望今後能有良好的發
展。 
 
Section 4 describes a two-level caching mechanism that
we developed to reduce the cache access latency. Section
5 evaluates the performance of the original and improved
caching mechanisms. Section 6 discusses the cost and
benefit of prefetch and introduces our adaptive prefetch
algorithm. In Section 7, we evaluate the prefetch
strategies of sequential prefetch and our adaptive prefetch
via trace-driven simulation. We show the simulation
results over the TPC-C benchmark. Finally, Section 8
states the conclusion.
2. Related Work
Disk cache can improve I/O throughput and avoids
repeated physical I/Os for a storage server [1, 5, 9-11].
Smith [1] has described and evaluated the performance of
adding a disk cache in storage system. Some work [9, 12]
has proposed that cache system consists of two-level
caches using volatile and non-volatile memory devices to
enhance the cache capacity and cache hit-ratio. Another
work [10] showed that a storage system would achieve
better performance if the first level cache is designated
with low set-associativity and a small block size while
the second level cache is designated with fully-
associativity and a large block size.
Various prefetching techniques have been proposed
[1-7, 13-15]. Some studies showed that how to predict
future file accesses via the hints from applications [4, 13]
or the access patterns from the past [5-7]. Let and
Duchamp’s work [7] builds access trees which capture
dependencies between referenced files and prefetches
files according the access trees. Tran and Reed’s work
[14] builds time series models to predict temporal access
patterns, and then prefetches I/O requests during
computation intervals to hide I/O latency. Vellanki and
Chervenak’s work [6] uses a probability tree to record
past access patterns to predict future accesses and
analyzes the cost-benefit of prefetching.
However, aggressive prefetch can cause problems.
Methods [6, 14] pay attention to figure out the optimal
amount of prefetching, but the management of tree
structures in their algorithm is both time and space
consuming. The adaptive prefetch scheme in this work
we proposed dynamically finds an appropriate prefetch
size for a disk volume with little CPU and memory
overhead.
3. The Storage System under Evaluation
In this section, we give an overview of the system
architecture of our target storage server. For
confidentiality, we call the storage server QSS, short for
Quanta Storage Server. QSS is a mid-range direct-
attached storage server which supports RAID5
technologies and fiber channel connections to the host
computers. The embedded processor on the system runs
RAID and cache software on top of the Linux operating
system, along with the network and disk controller
drivers to handle the entire operation of the storage
system. The design of QSS represents a typical design of
storage server in its class, and thus our performance
evaluation and proposed enhancement techniques are
also applicable to many other designs.
3.1. The Target System Platform
Our simulation specifications are based on the
specifications of the QSS platform, as shown in
Table 1,
Table 1.  Target storage system configuration.
3.2. The Disk Caching Mechanism
In this paper, we use the term disk cache to refer to the
cache memory in the storage server. We define three
terms: stripe, stripe-unit size, and cache block. A stripe is
a collection of data across an array of hard drives; the
granularity stored in one hard drive of a stripe of the
array is called the stripe-unit size. A stripe unit is divided
into one or several cache blocks, where a cache block is
the granularity of data brought into memory (disk cache).
Figure 1 shows the relations of the stripe, stripe-unit size,
and cache block with a disk array which contains four
disks, where a stripe unit contains four cache blocks.
Figure 2 shows the environment of QSS storage
system, where multiple computers (clients) connect to the
storage server (host) through a fiber channel network.
The original request data format contains three fields of
information: (1) LBA (logical block address), (2) the
amount of requested size, and (3) LUN (logical unit
number). The storage system first does striping by
computing the disk stripes based on the LBA. After that,
CPU CPU PowerPC SP 440
Operating
System Linux Kernel 2.6.14
Front-end
Interface Tachyon fiber channel card, PCI-X 133 MHz
Back-end
Interface LSI SCSI SATA adapter, PCI-X 133 MHz
Disk
Array
4 Seagate ST3250624NS disks using raid0,
Hard drive size 250 GB , Buffer size 16 MB,
Spindle speed 7200 rpm, Data transfer rate 95
MBps, seek time 8 ms(average), 2.6 ms (33%
of seek time) Average latency 4.16 ms
Cache
Block Size
128 * 512 bytes = 64 KB
Disk transferring a cache block needs 0.67 ms
Stripe-
Unit Size
4 cache blocks = 256 KB
Disk transferring a stripe unit needs 2.7 ms
Stripe Size 16 cache blocks = 1 MB
153
Authorized licensed use limited to: National Taiwan University. Downloaded on February 7, 2010 at 22:13 from IEEE Xplore.  Restrictions apply. 
L1 cache is first searched and results in one of the two
possible scenarios: (1) the data is a hit in the L1 cache
and responded back to the client immediately, or (2) the
data is  a miss in the L1 cache and the system has to do
the regular data-transfer path: striping, hashing, and
indexing before responding the data to the client.
Figure 5. Two-level cache management.
Figure 6. L1 cache line indexed with the request.
Figure 7. Request transfer with two-level caching.
We use the terms L1 cache to refer to the fast caching
scheme, and L2 cache to  refer  to  the  existing  caching
mechanism in the QSS caching architecture. The data
block is found using its block address (LBA), and each
block frame has a tag to check the legal mapping, the
associativity of the caches ranges from direct- mapped to
fully-associative, depending on the system configuration.
Only if the data block has been cached in the disk cache,
it can be found in the L1 cache. For example, in a direct-
mapped cache, a referenced LBA, is divided into:
1. a cache index, which is used to select the block
2. a tag field, which is used to compare with the
value of the tag field of cache
Each cache block contains a CCR pointer field. The
system uses the CCR data structure to access data in the
disk cache. Figure 6 illustrates the data mapping
manipulated in our L1 cache. LBA is indexed into the L1
cache directory, and the tag is compared. If the tags are
matched, the CCR pointer is used to get the referenced
data from the data cache. Figure 7 illustrates the two-
level caching scheme mentioned above.
LBA is indexed into the L1 cache directory, and the
tag is compared. If the tags are matched, the CCR pointer
is used to get the referenced data from the data cache.
Figure 7 illustrates the two-level caching scheme
mentioned above.
5. Performance Evaluation for Caching
In this section, we describe two methodologies which
we used to evaluate our proposed two-level caching
technique before it was implemented. The evaluation
results eventually led us to implement a prototype of the
two-level caching mechanism in the storage server.
Section 5.1 presents the analytical evaluation based on
performance profile data. Section 5.2 shows a trace-
driven simulation environment that we put together to
model and evaluate the performance of our QSS storage
server system. The detailed simulation includes the
transactions waiting in the queues, system data-transfer,
disk cache and prefetch mechanisms with accurate timing
model. The timing factor affects the performance results
in many ways for the simulation, so it is very important
to model the system timing carefully.
5.1. Analytical Evaluation for L1 Cache
The average time serving a transaction can be derived
from three parts: L1 cache access time, L1  cache  miss
rate * (L2 cache access time + L2 cache miss rate * disk
access time), and average request wait time in queue. We
use the following equations to compute the system time:
Tsystem = TL1 + MissL1 * (TL2 + MissL2 * Tdisk) + Tqueue
(1)
without L1 cache:
155
Authorized licensed use limited to: National Taiwan University. Downloaded on February 7, 2010 at 22:13 from IEEE Xplore.  Restrictions apply. 
prefetch strategy may hurt the performance as the
prefetch operations which do not follow the rules would
consume processor resources unnecessarily. We will
discuss the cost of prefetch operations in our storage
server in the following section.
6.2. The Disk Access Time
A disk access time is defined as the time required for
the storage server to fetch a block from the disk. A disk
access time can be broken into three parts: waiting time
for disk I/O, waiting time for bus I/O, and execution time
to process the transaction in the driver.  Here we assume
that the time of disk I/O and bus I/O are overlapped. In
our study, based on our profile, the execution time in the
driver time is relative minor, thus we can ignore it in our
estimation. Therefore, based on the above, we estimate
the cost of an I/O access, Ti/o, with the equation below:
/ ,diskSUi o latency busSU
d T
T T p MAX d * T
parallelism of a stripe
 
    
	 

(5)
where TdiskSU is the disk transfer time of a stripe
unit size, TbusSU is the bus traffic time of a stripe unit
size, d is the number of disks, p is the amount of prefetch
stripes, and Tlatency is the time of average disk latency
plus disk seek time and data transfer time.
6.3. Performance of the Cache Prefetch
We use equation (5) to estimate the cost-benefit of
prefetch in our simulation, with the following parameters
extracted from the platform specifications shown in
Table 1: (a) bus bandwidth is 1GB/s; (b) disk transfer
time is 95 MB/s, ignoring cache buffers; (c) stripe-unit
size is 256 KB with 4 disks using raid 0, 1MB per stripe.
We assume the parallelism of a stripe is always across
all disks so that the term parallelism of a stripe equals to
d.  Thus,  the  disk  access  time  for  prefetching  1  stripe  is
(4.16+2.6+0.67) + 1 * max (2.7, 1) = 10.13 ms. A
prefetch of 2 stripes takes 12.83 ms, while non-
prefetching I/O takes 7.43 ms. Transferring three stripes
costs  almost  an  disk  I/O  access  time.  The  benefit  of  a
prefetch is only achieved if it reduces cache misses, for
example, prefetching 3 stripes to eliminate more than 2
cache misses is a worthy case under our estimation.
However, prefetch also causes the side-effect of evicting
the cached blocks might be referenced in the future. Thus
it is a double-edged sword that should be used carefully.
6.4. Adaptive Prefetch
Unlike a fixed sequential prefetch strategy, the
adaptive prefetch algorithm we proposed adaptively
chooses the amount of blocks to prefetch in order to
achieve the benefit of prefetching. The algorithm
dynamically adjusts its prefetch size using statistics
information of the cache hit/miss and prefetch history.
We applied this scheme on a per-disk-volume basis since
the storage server usually managed the volumes
(partitions) with specific purposes. For example, different
databases resided in the different partitions.
Requests from a client may contain additional data
requests injected by the prefetch policy of its file-system.
Upon receiving these requests, our adaptive prefetch
scheme may additionally prefetch more data blocks from
a disk volume for possible future uses. In practice, over
the fiber channel, the size of the requests from a client is
limited to 2Mbytes. The limitation is usually dependent
of the transmission control protocol supported by the
hardware, and the client has to send multiple requests
instead of one, which is less efficient. Our adaptive
prefetch strategy helps solve this issue by prefetching
multiple blocks with one I/O operation and thus reduces
the average access time seen by the client.
We use  a sliding-window buffer to collect the history
of the most recently occurred cache-hits and prefetch-hit.
Additionally, a flag is attached to the prefetched block to
identify whether it is a prefetch-hit or not. The flag is
cleared, if it has been actually referenced after the
prefetch. The state machine has a set of five states. Each
state  is  associated  with  a  prefetch  policy  and  two
thresholds for cache-hit ratio and prefetch-hit ratio, as
shown in Table 2. The term prefetch-hit ratio is defined
as the ratio between the count of prefetch-hits and that of
cache-hits. The input set for the state machine is based on
the cache-hit ratio and prefetch-hit ratio:
The finite-state machine modeling adaptive
prefetching is defined as a sextuple ( , , , , , )S So    ,
where:
 ={HH, HL, LH, LL}. (Input alphabets)
HH = high cache-hit rate, high prefetch-hit rate
HL = high cache-hit rate, low prefetch-hit rate
LH= low cache-hit rate, high prefetch-hit rate
LL = low cache-hit rate, low prefetch-hit rate
 = S . (output alphabet refers to next state)
S = {A, B, C, D, E}. (set of states)
So = {A}. (initial state)
: S S  (state-transition function)
: S  (output function)
For input alphabets,  , high cache-hit rate means the
cache-hit rate is over the cache-hit threshold; high
prefetch-hit rate means the prefetch-hit rate is over the
prefetch-hit threshold. Figure 8 shows the state diagram
for our adaptive prefetch strategy. The initial state is A,
which has the threshold of cache-hit ratio set to 50% and
the threshold of prefetch-hit ratio set to 20%, as shown in
157
Authorized licensed use limited to: National Taiwan University. Downloaded on February 7, 2010 at 22:13 from IEEE Xplore.  Restrictions apply. 
Simulation Configuration Parameters (us)
L1 cache hit CPU spending time
L1 cache miss CPU spending time
L2 cache hit CPU spending time
L2 cache miss CPU spending time
Interrupt handling CPU spending time
Disk latency time
Disk seek time
Disk transfer time 5 us per sector
2
2
27
54
3.5
4160
2666
5 us/sector
Max concurrent requests 20
Table 4.  Memory model configurations in the simulation.
7.2. Performance Results
Using the TPC-C traces as the workload, we first
evaluated the performance of QSS with and without
(baseline) two-level caching scheme, and then added
prefetch strategies on top of the two-level caching. The
cache miss rates are plotted against cache sizes in Figure
9. We also plotted the transaction per second (TPS) in
Figure 10. The miss rates are almost identical with or
without our two-level caching. The miss rates dropped
from about 70% to 5% between 768 MB and 1024 MB.
This is where our two-level caching reveals its benefits
by improving the transaction rate (TPS), e.g. 10%
improvement at 1024 MB and 13% at 1280 MB. On the
other hand, the benefits were not obvious for small
caches when cache miss ratio is high, because the
execution time saved by the L1 cache is relatively small
compared to the execution time required by the cache
misses to go through the stripping-hashing-indexing path
that is done by the L2 cache.
We broke down the execution time on the embedded
processor in the case study, as shown in Figure 11, where
two-level caching has effectively reduced processor
execution time for large caches when the cache-hit rate is
high. The reduction of execution time is up to a factor of
3.4. The processor execution time reduced by 1%, 3%,
3%, 339%, and 340% at cache size set to 256 MB, 512
MB, 768 MB, 1024 MB and 1280 MB, respectively.
Figure 12 shows that basic sequential prefetching
(Prefetch 1 stripe) reduced the miss rate to below 5%,
and that aggressive prefetching further reduced the cache
miss rates. Figure 13 shows that the performance with
prefetch strategies is complicated trade-offs between
reducing cache misses and increasing unhelpful prefetch
operations. The sequential average series shown in
Figure 14 was derived by averaging the TPS of the five
fixed-size sequential prefetch strategies. Adaptive
prefetch delivered the best performance as cache sizes set
to 256 MB, 512MB, 768 MB, and 1280 MB. However,
sequential prefetching 4 stripes had the best performance
on 1024 MB. As we described previously, the prefetch
size needs to be carefully chosen as prefetching too
aggressively may reduce the miss rate but also increases
access time. This was illustrated by observing that
sequential prefetching 16 stripes always had the lowest
miss rates, but its performance was not the best.
In our case study, sequential prefetching of 4 stripes
has outperformed our adaptive prefetch by 14% on a
1024 MB cache. However, our adaptive prefetch still
won the competition for most of the other cache sizes and
it outperformed the sequential average as shown in
Figure 13 and Figure 14.  Table 5 lists  TPS and average
response time for TPC-C traces, and, in the last row, it
shows that the speedups of adaptive prefetch over the
average performance of sequential prefetch range from
9% to 20% in terms of TPS, and 12% to 29% in terms of
average response time.
Figure 9. Comparisons of miss rates over TPC-C.
Figure 10. Comparisons of TPS over TPC-C.
159
Authorized licensed use limited to: National Taiwan University. Downloaded on February 7, 2010 at 22:13 from IEEE Xplore.  Restrictions apply. 
Cache Size (MB) Transactions Per Second Average Response Time (ms)
256 512 768 1024 1280 256 512 768 1024 1280
Baseline 350 362 366 4833 4835 59.89 57.97 57.29 4.34 4.34
Two-level Caching 350 363 366 5363 5505 59.89 57.73 57.32 3.92 3.81
Prefetch 1 stripes 2706 2798 2844 30401 30581 7.76 7.5 7.38 0.69 0.69
Prefetch 2 stripes 3885 3942 4101 39801 39948 5.4 5.33 5.12 0.53 0.53
Prefetch 4 stripes 4836 4913 5155 46416 46504 4.34 4.27 4.07 0.45 0.45
Prefetch 8 stripes 5163 5317 5601 40348 48972 4.06 3.95 3.75 0.52 0.43
Prefetch 16 stripes 4947 5215 5531 29320 48089 4.24 4.02 3.79 0.72 0.44
Sequential Average (SA) 4307 4437 4646 37257 42818 5.16 5.01 4.82 0.58 0.51
Adaptive Prefetch (AP) 5169 5322 5606 40729 49804 4.06 3.94 3.74 0.52 0.42
AP Speedup over SA 20% 20% 20% 9% 16% 27% 27% 29% 12% 21%
Table 5.  Performance values of Transactions Per Second and the average response time.
9. Acknowledgements
This work was supported in part by a grant from the
Quanta Computer Inc., a grant from the National Science
Council (95C2443-2), and a grant from Excellent
Research Projects of National Taiwan University
(96R0062-AE00-07).
10. References
[1]    A. J. Smith. Disk Cache: Miss Ratio Analysis and Design
Considerations, ACM Trans. Comput. Syst., vol.3, no.3,
pp. 161-203, 1985.
[2] P. Cao, E. W. Felten, A. R. Karlin, and K. Li.
Implementation and Performance of Integrated
Application-Controlled File Caching, Prefetching, and
Disk Scheduling, ACM Trans. Comput. Syst., vol.14, no.4,
pp. 311-343, 1996.
[3]   P. Cao, E. W. Felten, A. R. Karlin, and K. Li. A Study of
Integrated Prefetching and Caching Strategies, In
Proceedings of the 1995 ACM SIGMETRICS joint
international conference on Measurement and modeling of
computer systems, pp. 188-197, 1995.
[4]   R. H. Patterson, G. A. Gibson, E. Ginting, D. Stodolsky,
and J. Zelenka. Informed Prefetching and Caching, In
Proceedings of the fifteenth ACM symposium on
Operating systems principles, pp. 79-95, 1995.
[5]  C. L. Chee, H. Lu, H. Tang, and C. V. Ramamoorthy.
Adaptive Prefetching and Storage Reorganization in a
Log-Structured Storage System, IEEE Trans. on Knowl.
and Data Eng., vol.10, no.5, pp. 824-838, 1998.
[6]   V. Vellanki, and A. Chervenak. A Cost-Benefit Scheme
for High Performance Predictive Prefetching, In
Proceedings of the 1999 ACM/IEEE conference on
Supercomputing, pp. 50-50, 1999.
[7]   H. Lei, and D. Duchamp. An Analytical Approach to File
Prefetching, In Proceedings of the annual conference on
USENIX Annual Technical Conference, pp. 21-21, 1997.
[8]    TPC-C, http://www.tpc.org/tpcc/.
[9]  J.-h. Huh, and T.-m. Chang. Hierarchical Disk Cache
Management in Raid 5 Controller, J. Comput. Small Coll.,
vol.19, no.2, pp. 47-59, 2003.
[10]  C.  Yun,  Y.  Genke,  and  W.  Zhiming.  The  Application  of
Two-Level Cache in Raid System, In Proceedings of the
4th World Congress on Intelligent Control and
Automation, pp. 1328-1332, 2002.
[11]  J.  L.  Baer,  and W. H.  Wang.  On the Inclusion Properties
for Multi-Level Cache Hierarchies, In Proceedings of the
15th Annual International Symposium on Computer
Architecture, pp. 73-80, 1988.
[12] J. Huh. Two-Level Cache for Distributed System in Raid 5
Disk Controller, In Proceedings of the 18th International
Conference on Systems Engineering, pp. 64-69, 2005.
[13] A. Tomkins, R. H. Patterson, and G. Gibson. Informed
Multi-Process Prefetching and Caching, In Proceedings of
the 1997 ACM SIGMETRICS International Conference on
Measurement and Modeling of Computer Systems, pp.
100-114, 1997.
[14] N. Tran, and D. A. Reed. Arima Time Series Modeling and
Forecasting for Adaptive I/O Prefetching, In Proc. of the
15th Int'l Conf. on Supercomputing, pp. 473-485, 2001.
[15]  A.  R.  Butt,  C.  Gniady,  and  Y.  C.  Hu.  The  Performance
Impact of Kernel Prefetching on Buffer Cache
Replacement Algorithms, In Proceedings of the 2005
ACM SIGMETRICS Int'l Conf. on Measurement and
Modeling of Computer Systems, pp. 157-168, 2005.
[16] Open Systemc Initiative (Osci), http://www.systemc.org/.
[17] The Disksim Simulation Environment,
http://www.pdl.cmu.edu/DiskSim.
[18] P. M. Chen, and E. K. Lee. Striping in a Raid Level 5 Disk
Array, SIGMETRICS Perform. Eval. Rev., vol.23, no.1, pp.
136-145, 1995.
[19] Storagereview.Com,
http://www.storagereview.com/map/lm.cgi/str.
[20] Postgresql, http://www.postgresql.org/.
[21] Trace Distribution Center, Brigham Young University,
http://tds.cs.byu.edu/tds/.
[22] Transaction Processing Performance Council,
http://www.tpc.org/.
[23] J.-S. Chen. Performance Optimization on a Raid System:
Design and Implementation of a Fast Indexing Table for
Disk Caching, Master Thesis, National Taiwan University,
Taipei, Taiwan, 2007.
161
Authorized licensed use limited to: National Taiwan University. Downloaded on February 7, 2010 at 22:13 from IEEE Xplore.  Restrictions apply. 
 2
六、會議經過與心得： 
我本次參加的學術會議是國際上在「計算機結構」領域，最頂尖一流的研
討會，來自世界各國最傑出的學者專家、業界研發單位裡最優秀的工程師，以
及一流知名大學中最聰明的研究生，聚在一起討論最拔尖、最具挑戰性的研究
問題。對於來自台灣的我而言，一方面是能夠增加自己的眼界，與國外大師級
學者教授們交流討論的好機會；另一方面是提高台灣「計算機結構」相關領域
的能見度，與國際學術界、產業界之間接軌溝通的管道，使不亞於日本、韓國、
印度、大陸等眾多亞洲國家。由於接近暑假的關係，去程我搭了台北東京
紐約休斯頓奧斯汀的飛機，在紐約轉換度過一晚，剛好提前一天到達會議
在奧斯汀舉辦的現場(希爾頓飯店)，也是我們與會人士此期間住宿的地方，事先
進行研討會註冊以及準備各項前置工作。 
這場會議的 Keynote 演講有兩場。第一天請到了 UC Berkeley 的 Katherine 
Yelick 教授，她是 MIT 的博士，同時也是 NERSC 在 Lawrence Berkeley 國家實
驗室的 director，主講題目是「Ten Ways to Waste a Parallel Computer」：由於硬體
設計在晶片 scale parallelism 碰到的困難麻煩，使軟體研發者往往必須在不清楚
architecture target 下寫出 portable 的 application，這是一大挑戰；在硬體系統端
的考量幾乎取決於 energy 因素，因此限制了系統設計與應用的發展，這將成為
exascale computing 下個主要的里程碑。硬體在乎設計上的 efficiency，特別是
battery-operated device，而軟體設計往往卻無此考量，因此造成時間上的浪費而
使得 energy 大量消耗，只為了等待 communication、synchronization、和與其他
使用者或系統的 interaction，比如並非計算問題的 data movement。內容包括提
出"Believe Moore's rule but ignore Little's rule."、"Don't think more on 
algorithms."、"Don't take memory issues into account."等要項，結合了硬體、演算
法、軟體作最佳化產生更有效率的使用，這需要從軟體和硬體兩個面向再次思
考 programming model、architecture、algorithm 等議題並作設計上的合作。 
第二天的演講者 James Hamilton 是 Amazon Web Services 的 vice president
與 distinguished engineer。演講內容主要是在討論 Internet-Scale Service 
 4
        (第四天) 
 Load and Stores 
 DRAM and SSD 
 Power in Chip Multiprocessors 
 Hardware Support for Monitoring and Debugging 
 Potpourri 
 Memory System Reconfiguration and Acceleration 
        (第五天) 
 On-Chip Interconnection Networks 
 Speculative Threading and Parallelization 
由上述議程可看出 ISCA 研討會在 computer architecture 研究領域所細分涵
蓋的範圍非常廣，也包含了一些較新穎拔尖的研究技術和主題，這也是此類大
型學術研討會的特性，不少研究是評估分析並改善硬體及應用程式的效能、作
出最佳化，或是實作出有效而應用廣泛的效能工具，從中可以掌握相關領域的
研究現況，並使我獲得研究上的啟發。每篇論文台上講者被分配到 25 分鐘的時
間，其中 20 分鐘是 presentation，剩下 5 分鐘是 Q&A，從其他不少學生上台報告
論文流利的台風與回答問題的從容不迫，可以看出報告前的充分準備與清楚的
思維觀念，還有臨場機敏的反應能力，這是國內小型研討會比較看不到而需要
改進的地方。另外，從與論文發表者們的事後討論更能了解這些研究成果背後
的經歷過程、所需要的資源、原始想法及實驗細節，包括每個步驟考量的因素
還有遭遇面臨的各種問題，從別人的想法觀點中吸收對方的經驗，並提出問題
討論，一邊思考對照自己研究領域的相關問題，著實讓我受益匪淺，其中對方
不少是大師級知名學者和規模龐大的業界研究團隊如 Intel、IBM、Microsoft、
Google、Sun、AMD 等。 
ISCA 的主辦單位在會議第一、二天同時也附帶辦了許多 workshops 來針對
第五十一屆 IEEE 全球通訊會議 
(The 51th IEEE Annual Global Telecommunications 
Conference (Globecom)) 
黃昱愷 
國立台灣大學資訊網路與多媒體研究所 
 
IEEE Globecom 2008 是每年一度的國際知名的通訊領域研討
會，為 Global Communications Conference的縮寫，是一個針對網路領
域研究的國際盛事，主要目的為連結產學界相關領域的領導者與研究
者，近年來，此會議已經針對語音、資料、影像、與多媒體網路等領
域提出具體的進展。 
今年的 Globecom 2008所舉辦的地點是在美國的路易斯安那州的
紐澳良(New Orleans, LA, USA)的 Hotel Hilton旅館內舉行，在會議中
除了主要以口頭報告方式的技術性會議之外，另外還邀請產業界與學
術界的知名人士就相關議題發表主題演說(Keynote Speech & 
Tutorials)，此次的會議總共有一百多個個技術性議程，我所報告的論
文被歸屬於 Resource Management in WLANs的部份，時間是在會議
的第四天下午，參加此議程的總共有六組，每組由一人上台報告在。
在議程中，大家主要針對無線網路的特性做分析與討論，討論的議題
wireless sensor network，提出了一個全新的概念來最小化能量消耗，
同時能夠保證傳輸延遲以及可靠性。這個問題主要的挑戰在於 IEEE 
802.15.4上的MAC是以 random的方式 access頻帶，因此造成傳輸延
遲與可靠性都無法預測。另外一方面，資料的流量，網路拓樸，以及
此論文所探討的 duty-cycle，都更加深了 random的程度。此論文建立
了一個模型解決這個問題，並能夠達到最佳化，這個模型可以最小化
所有的能量消耗，包含了 transmit，receive，listen，與 sleep模式，限
制是傳輸延遲以及可靠度，變數是 sleep與 wake時間，另外一方面，
本論文亦提出了一個可以輕易用於 IEEE 802.15.4設備上的 light 
lookup方法，可以達到最好的效果。實驗結果顯示，這個方法的確可
以大幅提升既有的效能。 
另外一篇我也感興趣的論文題目為「Modified Beacon-Enabled 
IEEE 802.15.4 MAC for Low Latency」，這篇探討了 IEEE 802.15.4這
個規格上面傳輸延遲的問題，這篇論文的作者們注意到，在 IEEE 
802.15.4的 superframe架構上，CAP與 CFP的前後位置會嚴重影響
系統傳輸的延遲，依照原先的架構，CFP裡面所傳輸的即時資料若是
失敗的話，必須要等一段 inactive的時間才能夠再下一次的 superframe
傳輸，而這樣的方式會大大增加資料的傳輸延遲，另外一方面，這些
資料若是完全依照 CFP傳輸，也會讓頻寬的使用無法達到最有效的
Asian Workshop of Ubiquitous and Embedded Computing August 23‐26, 
2009 Beijing, China 
The Fifth International Symposium for Ubiquitous Computing Systems 
(UCS 2009) 
The 15th IEEE International Conference on Embedded and Real‐Time 
Computing Systems and Applications (RTCSA) 2009 
Hung‐Hsiang Chang 張閎翔 
國立台灣大學資訊工程研究所 
參訪經過及會議參加心得 
 
  From 8/22 to 8/27, I had gone Beijing to participate in AWUEC 2009 (Asian 
Workshop on Ubiquitous and Embedded Computer), there was 32 members joined 
this workshop. They came from Japan and Taiwan. The Workshop had four sessions. I 
was scheduled in the second session to present. 
The topic I talked about is “Run‐Time Environment for Heterogeneous 
Multi‐Core Virtualization Platform” ‐ Nowadays, as the demand of the multimedia 
applications in the embedded system increases greatly, not only the popularity of the 
heterogeneous multi‐Core architecture grows, but also the applications of the 
embedded systems become complicated day by day. So that we face several 
challenges for designing and implementing embedded software on such platforms, 
first on run‐time environment, the system should provide real‐time guarantee no 
matter hard real‐time or soft real‐time, and system environment should guarantee 
robustness and security, second with heterogeneous multi‐core architecture, the 
multimedia program have frequently inter‐core data change. Third, if the system is 
not well designed for portability, it will spend heavy overhead porting a system to 
different platforms. Last but not the least, the demand for multimedia applications 
using DSP increases gradually because they want to enhance performance and 
shorten the product development process. 
According to the above observation, we propose run‐time environment 
architecture for heterogeneous multi‐core SOC platform. For MPUs, we put micro 
kernel, and we integrated Inter‐Core Process Communication protocol into the 
run‐time environment. And for DSPs, we put the light‐weight kernel, and the kernel 
has thread management to support multi‐threading, and memory management to 
support memory allocation. 
First we use virtualization framework on embedded System. The rationale is to 
time. For Scheduler, the scheduling algorithms support several features, including 
priority‐based scheduling, preemptive scheduling, dynamically priority scheduling, 
and cost constant time to find the task with the highest priority. 
We combined ICPC Protocol to Virtual framework and set up the multi‐core 
programming model, we turn ICPC library into the server of L4 ENV, offers API for the 
usage of upper layer. When a transmission is needed, the application invoke ICPC API, 
L4Linux calls ICPC library, and ICPC will allocate memory region of both sides, After 
Setup, ARM ends will send data to DSP, finish calculation, DSP end sends back data to 
ARM end. This structure offered high Portability, User Applications and L4linux 
doesn’t need to be revised, just revise L4: Fiasco for porting. 
After finished presentation, I had concentrated the presentation of other 
members. The topics presented by the members from Japan are about ubiquitous 
computing. They tried to bring computing in daily life for helping upgrading the 
quality of life. That’s interesting. 
UCS conference and RTCSA conference held in 8/24~8/26. There were lots of 
interesting ideas in RTCSA. Like Architecture and Practice Session, Real‐time 
Scheduling session and Real‐time Multiprocessor Systems session, the discussions are 
very enthusiastically.   
For demo session in UCS, I saw that they proposed idea about pelagic fishing. 
They tried to use the technique of Ubiquitous‐computing to solve the pelagic fishing 
problem. The contribution is very practical. 
For this trip, I have learned the new knowledge about Ubiquitous‐computing 
and real‐time technique. And this time is my first time presentation in English. I was 
so nervous, but I know I have those shortcomings. It will help me to give a better 
talk. 
 2
z 2009/6/14 མᐒӣѠǶ  
 
Ϥǵୖೖ࿶ၸǺ 
  Ԝԛ཮᝼ޑӦᗺӧફऊǴךॺӧᙑߎξᙯᐒǴ٠Ъӧ϶ΓޑϟಏΠளډୖ
ᢀ GoogleǵCiscoǵStanfordεᏢ฻ᐒᄬޑᐒ཮Ǵ٠ЪӧୖೖၸำύǴΨவ΋٤
ᇡ᛽ډޑ๮Γπำৣܻ϶ॺαύǴΑှډΑ΋٤ޖكޑπբ࿶ᡍǴаϷ΋٤ࣽ
מ཰ޑᖿ༈وӛǶ 
  Googleࢂϣ೽Ўϯ࣬྽੝ਸޑ΋໔ϦљǴ࣬ჹܭ΋૓ᆅ౛ޣЬᏤޑϦљǴ
GoogleКၨႽࢂҗπำৣॺٰЇᏤϦљޑوӛǴКБᇥπำৣॺ೏Ϣ೚ѝ޸
80%ޑਔ໔ӧೀ౛΢ભҬжޑπբǴԶഭᎩޑ 20%ਔ໔ёаѐ଺ԾρԖᑫ፪ޑ
ЬᚒǴӵ݀೭٤ᚐѦޑЬᚒ೏ᇡࣁࢂёаჹϦљ׳Ԗᔅշޑ၉Ǵ൩཮ᙯᡂԋϦ
љЬाޑ஑ਢǴπำৣ൩ᡂளёа޸ 80%ޑਔ໔ӧགᑫ፪ޑЬᚒ΢य़ΑǴӧ೭
ኬޑπำৣЬᏤޑᕉნϐΠǴ΋٤όѸाޑࡹݯᏹբǴλიᡏჹҥޑ௃ݩ൩ё
а෧ډനϿǴԶֹӄаჴΚࣁᏤӛǴჴΚமޑπำৣ൩ёаКၨԖᐒ཮ЬᏤϦ
љޑБӛǴΨᜤ܁ Googleёаӧ೭ሶอޑਔ໔ϣᘺଆԶԋࣁᆛၡШࣚޑ៙Ь
ΑǶ 
Զ Cisco࣬ჹܭ GoogleٰᇥǴ׳ख़ຎ࠼Њޑሡ؃Ǵ΋຾Εௗࡑޑε᡺ǴCisco
ࡐܴᡉၨ Google׳ࣁᇬ๮ǴགྷѸࢂࣁΑௗࡑ࠼Њޑਔংёа๏࠼ЊԖၨӳޑᢀ
གǴCiscoϣ೽ޑᒤϦ࠻ଛ࿼Ψག᝺Кၨಷख़ǴؒԖ Googleٗኬࢲዃޑག᝺Ǵ
ךॺӧ౜൑࣮Α΋঺ջਔᇻຯ཮᝼س಍ޑ demoǴࢬᄣޑຬଯှ݋ࡋฝय़کߚத
ᙁൂᏹբޑϟय़ᡣΓག᝺ډ CiscoჹౢࠔޑҔЈำࡋǴаϷ୯ሞޕӜޑεϦљ
ޑ॥ጄǶ 
ӧᆶStanfordޑ௲௤Monica Lamޑፋ၉ύǴӴϟಏΑӴᆶځд΋٤Stanford
ޑ௲௤ȐDr. Dan Boneh, Dr. Monica Lam, Dr. Nick McKeown, Dr. Guru 
Parulkar, Dr. Arogyaswami Paulraj, Dr. Mendel Rosenblumȑ҅ӧ๱Ћ຾Չޑ
