 
 2
不深入。然而不論是在家庭活動或其它空間
中，不同區域可因使用者的需求賦予不同的
重要性。而攝影機擺放的方位也需顧及光線
照射位置，以求拍攝清楚。在我們的研究中，
將這些因素進行綜合的考慮。發展出相關的
演算法，當給定空間平面圖以及固定數量攝
影機後，能夠透過自動漸進  (incremental) 
的方式，在兼顧拍攝清楚與死角盡量減少的
情況下，進行攝影機的佈建。這些佈建位置
可提供在已知平面圖的空間中進行攝影機
架設之參考。 
 另一個在攝影機網路方面過去較少觸
及的課題，是互相不重疊之攝影機之間的相
連性估測。在過去相互重疊攝影機間的方位
估測已被探討得很完整。然而在一般情況下，
為了節省攝影機數量，通常只針對重要性較
高的地方布置攝影機，且這些攝影機的監控
區域並不一定會互相重疊。對不重疊的攝影
機而言，一種可能的方式是藉由其間物體進、
出入的狀況，透過最大可能性的估測，來發
掘其間的連接關係。以三台攝影機 A，B，
C 為例，倘若真正的連接是 A-B, B-C,凡由
A 至 C 者必定經過 B。但困難點在於除了
A-B 與 B-C 之間的出入交通狀況外，A-C 
間也可能存在著由 B 作中介所引入的交通
連接情況，因而產生虛假關連性。此時，如
何由這些攝影機視訊中的進出入狀況，發掘
出攝影機之間的連接關係，是過去尚未被探
討的問題。針對此問題，我們也發展了一個
解決方法。我們並可藉由攝影機的追蹤嬰幼
兒的移動軌跡，在家庭布置的場景中嬰兒進
入進行警戒區域之事件偵測。 
 在第二部分中，我們對於嬰幼兒的居家
影片 (home video) 進行事件偵測。無論是
攝影機網路或者一般家庭常用的DV直接拍
攝錄製，都可能成為嬰幼兒生活影片的來源。
事件可分成單人事件以及群體事件。由於嬰
幼兒的歡笑時光十分珍貴，因此我們嘗試發
掘嬰幼兒歡笑與悲傷的片段，來作為事件偵
測的項目。此外，由於嬰幼兒經常相處的對
象包括了父母與朋友等，我們針對這些可能
的人物關係進行事件偵測。所發掘出的事件
不僅是檢索上很好的參考，也可協助經典回
憶畫面的開發。此課題包含了性別與年齡分
類，以及表情的判斷等。而家長的表情通常
對於嬰幼兒有撫慰的作用。因此我們也研發
關於表情合成之技術，以期能夠由單一的家
長影像合成出其不同表情之影像。 
此外，家庭錄影由於並非專家所拍攝，
通常品質參差不齊，如何協助居家能夠拍攝
出更好美感的影片，或者如何自動判斷一般
使用者所拍攝影片的美感品質，成為一個重
要課題。為了協助拍攝出更好的居家影片，
我們也進行了自動美感評估的方法開發，有
助於經點回憶畫面的產生與評估。 
基於此，我們將研究分成兩個部分：攝
影機網路部分以及視訊內容分析與合成部
分，共涵蓋四個子題。第一部分包含子題一
及二，其中子題一為攝影機網路的佈建，子
題二為跨攝影機物體追蹤與攝影機相連性
的發掘，以及空間死角偵測與警示。第二部
分包含了子題三及四，其中子題三為家庭視
訊嬰幼兒事件偵測與家長表情合成，子題四
為攝影機拍攝畫面美感評估。透過這些技術
的發展與整合，期能有助於提供更好的畫面
回憶系統與安全空間。 
二、 研究目的、方法、與成果 
第一部分：攝影機網路相關研究課題 
 
子題一: 最佳化多攝影機佈建 
1-1 研究背景 
在科技快速發展下，越來越多人選擇安置攝
影機來維護在公共區域與出入口或是居家
保全。然而因缺乏需求與應用方面的知識，
使得工程師無法安裝攝影機至最佳的拍攝
位置與角度，因而無法達到預期攝影系統所
應有的拍攝效果，更導致資源的浪費。因此，
如何有效佈置攝影機之擺放與角度對攝影
機的應用是重要的課題。 
 看似容易的攝影機擺放問題中隱藏著
多因素最佳化問題。如：攝影機俯角設定？
 4
款式價位為優先考量以達到最佳涵蓋率為
目標；若是在已知場景涵蓋率範圍的需求下，
則需擺放最少攝影機數，並使各攝影機擺放
能有效達到最佳涵蓋率，一旦目前總攝影機
數之涵蓋率不能達到已知涵蓋需求，則須增
加攝影機數量。不論以何種需求為前提，攝
影機的佈建皆為了達到使用者需求所設計，
因此，如何依據需求與條件設計出最佳化匹
配為攝影機佈建的首要步驟。 
 至今，使用不同的演算法解最佳化問題，
其所獲得的結果卻不盡相同。許多最佳化演
算法皆可應用在此問題。例如：利用粒子群
優化(Particle Swarm Optimization，PSO)模擬
鳥群不可預測行為來放置攝影機的最佳位
置 [Conci et al., 2009]；貪心演算法(Greedy 
Algorithm)一步步將攝影機擺放置最佳的位
置；基因演算法 (Genetic Algorithm)找出最
適合的基因(攝影機參數)交配以達到最佳擺
放效果 [Wang et al., 2009]。 
 先前研究主要著重於二維平面上的模
擬，尚未考慮三維場景的拍攝，不過在三維
場景模擬的計算量與複雜度都極高，故我們
將以 2.5D 設計攝影機的涵蓋區域 (FOV) 
[ Lobaton, et al., 2009 ] ，並合併所提出的光
的影響與感興趣重點區域涵蓋，來實現所提
出的多攝影機佈建模擬系統。 
 
1-3 研究方法 
1-3-1 系統規劃 
本計畫將系統規畫為符合多種安全類型的
使用需求，對於不同安全類型需求，家長們
對於危險空間的設定也會不同。我們將介面
以半手動的方式提供家長們可以自行給予
危險空間的比重，使得攝影機的擺放可以根
據危險空間佈建以便達到最佳的拍攝效益
而不造成硬體資源的浪費。此外，使用者可
以給定場景中的光源位置，以利於所架設之
攝影機盡量取得高品質(順光拍攝)的拍攝
內容而非低畫質(逆光)的拍攝資訊。 
 針對於不同安全顧慮，所要實現的目的
也不盡相同，家長們可選擇攝影機擺放盡可
能涵蓋所有的危險區域，或只針對嬰兒的全
身看護避免拍攝死角或遮蔽，抑或是設定好
硬體價位條件，讓系統自動計算出最佳建議
的攝影機擺放角度與支數，並使攝影機間彼
此涵蓋區域可以達到家長們的需求。 
1-3-2  2.5D 攝影機模型 
過去對於攝影機的模擬，通常在簡單的二維
平面考慮，然而對真實攝影機拍攝到的三維
空間，使用三角形來模擬攝影機涵蓋區域的
考量不夠扎實與精確，而真正以三維空間去
計算，其複雜度與計算量又過高，故我們架
構於 2.5D 模型下 [Lobaton et. al, 2009]，俾
便以較二維更精確的方式模擬攝影機的拍
攝範圍。 
 攝影機通常架設在距地板一定的高度，
且以某個角度向下俯視。對透視投影模型而
言，其所拍攝的範圍在三度空間中形成一個
具四個面的角錐，如 圖 1-2 所示。假若我
們以 (x,y,z) 來表示三度空間中的點，其中 z
軸代表高度，地板與天花板的高度分別為 0
與 h。我們定義角錐內部所有的點所成的集
合為 G，且 F={(x,y,z)| z≥0, (x,y,z)∈G}。當我
們將 F 中的點投影到地板平面上，將形成一
個五邊形。如圖 1-2 所示。所以我們使用此
2.5D 模型推導出的五邊形來模擬攝影機拍
攝的涵蓋範圍。 
另一個說明圖例如圖 1-3 所示。更進一
步而言，倘若我們考慮實際上所監控的對象，
例如人的身高平均大都在 1.7m 以下，且距
攝影機太近的涵蓋範圍無法有效取像。因而
涵蓋範圍可更進一步減少，成為一個六邊形，
如圖 1-3(b)所示。如此可讓攝影機所要負
責的任務範圍更加精確。無論是五邊形或六
邊形，皆比一般使用的三角形更能在投影平
面上模擬實際的涵蓋範圍。 
 我們稱此涵蓋範圍為攝影機的FOV。在
FOV內，一般而言拍攝的中心點較重要，越
往旁邊其監控功能逐步降低，而降低的程度
與攝影機的俯角也有關連。為了反應FOV內
不同的點宜具有不同的監控強度之特性（參
見圖1-4），此處我們採用[Conci and L. Lizzi, 
 6
, 
其中 , 是第 i 個攝影機的旋轉角。 
1-3-4 模糊集合涵蓋 
我們推導出一個漸進式強化演算法，依序地
擺置攝影機。當前面的攝影機已經安置好方
位後，我們的方法能夠自動地將其已經監控
的區域的比重降低，讓新加入的攝影機著重
在原本較為忽視的區域，達到較好的整體佈
建效果。 
給予場景的光源能量大小、場景空間以
及危險區域比重圖之後，假設過去的 M1
支攝影機已經擺放好，我們推導如何加上第
M 支攝影機使其能夠繼續最佳化。 
假設 ( )pg  為家長們給予場景的每個
座標點 p 之比重；f 是當第 i 支攝影機的方
位與俯角參數為α 時，根據 2.5D 攝影機模
型，在位於場景內的涵蓋區域內的所產生的
比重；h 為場景的光源對於第 i 支攝影機所
產生的影響；w 為調整涵蓋區權重影響與光
線影響的比重參數。對於場景中的點 p，這
M1 台攝影機對於其的顧及程度為 
)(X)(
1
11
i,ppF
M
iM
−
=
−
∪=  , 
其中 X(i,p)為第 i 台攝影機對於場景中第 p
個點的顧及程度，如下式： 
( ) ( ) ( ) ( )( )iiii phwppwfpgi,p αα ,1,)(X −+−=  
而“∪” 是廣義的模糊集合聯集運算。 
總和所有場景中的點，倘若最好的M1
台攝影機已根據下式選定。 
∑ −−
−
−
=
p
MPPM
pFE
M
M
)(max 1
,...
,...
1
11
11
αα
 
我們將繼續找出第 M 台攝影機的最佳擺放
方位。 
( ) )X(max 1
,
M,ppF
p
MP MM
∑ ∪−
α
 
此處對於 ∪ 的計算採用代數積 (algebraic 
product)作為模糊聯集的定義，則 
( ) ( ) )Monitor()Monitor(max 11
,
M,ppFM,ppF M
p
MP MM
−−
−+∑
α
因上式中 1−MF 為常數，故相當於 
( )
∑
∑
−
−
−=
−
p
MP
p
MP
M,ppF
M,ppFM,p
MM
MM
)X())(1(max
)Monitor()Monitor(max
1
,
1
,
α
α
(1)
 
以上的推導可視為解決模糊集合涵蓋問題
的 一 種 漸 進 式 強 化 演 算 法 (boosting 
algorithm)。當前面 M1 台攝影機對於場景
中的點 p 顧及程度 )(1 pFM − 已經很強的話，
則在擺放第 M 台攝影機時，其重要性會降
低為 1 − )(1 pFM − ；反之，當前面 M1 台攝
影機對於場景中的點 p 顧及程度還不夠的
話，其重要性相對會增強。每擺放一支新的
攝影機時，會考慮所有可能位置與角度參數，
找出使得(1)式最大的 pM 與 αM。 
我們以此漸進式演算法架構，根據使用
者需求的假設前提設定參數去計算匹配方
程式，逐步找出最佳的攝影機參數與位置。 
 
1-4 實驗結果 
 根據所設計的漸進式強化演算法，經模
擬與給定場景重要性及光源位置後，其結果
如下。圖 1-7(a)場景中光源位置，(b)為根
據使用者給定的場景比重圖。(c)-(h)為擺
放的結果。圖 1-8 與 1-9 分別為另外兩個場
景之結果。 
 
(a)            (b)            (c) 
 
(d)           (e)             (f) 
 
(g)            (h) 
圖 1-7：針對場景與燈光位置圖進行漸進式
攝影機擺放實驗結果。 
 
 8
中的哪一個，或是一個未曾出現過的目標物。
由[Chen et. al, 2008]，我們可將此配對的機
率，整理為以下式子： 
))(
)((maxarg
hPpw
hPpwh
appapp
stst
Hh s
=×
+=×=
∈
∗
 
其中h*為在handover list中最有可能與P為
同一個的目標物，而 )( hPpst = 為時空特徵
的配對機率， )( hPpapp = 為外觀特徵的配對
機率。因此該問題即為學習不同攝影機間的
目標物，在此二特徵下的配對機率分布。 
 
2-1-2-2 時空關係之自動學習 
首先，我們會先針對每台攝影機的個別畫面，
進行出入口的自動學習。藉由收集一段時間
的目標物出現與消失座標，我們將出入口學
習成Gaussian Mixture Model (GMM)，接著
使用EM algorithm來求得其參數。 
 接著，我們將所有不同攝影機的出入口
進行相連，產生可能連結(possible link)，並
針對所有可能連結學習通過時間機率分布
(transition time probability distribution)。假設
pab(t)為目標物由攝影機1之出入口a經過時
間t秒後出現於攝影機2之出入口b。則我們
可學習出入口a, b之通過時間機率分布為： 
0,
,
0
)(1)(
max ≥<



 =−
= ∑∑
tTt
otherwise
tttifS
C
tp
i j
ijij
ab
 
其中Tmax為最大允許通過時間，Sij為目標物
於兩攝影機畫面之外觀相似機率。 
 而當我們有足夠多的目標物通過所學
習的可能連結。此時，如果該可能連結之通
過時間機率分布的最大值比兩倍中位數值
還大的話，則我們判斷此可能連結為合法連
結(valid link)。 
 
2-1-2-3 亮度轉換函式之自動學習 
攝影機1, 2間的亮度轉換函式為，可將攝影
機1中拍攝的目標物之亮度分布轉換為該
目標物在攝影機2的畫面時的亮度分布。在 
[Javed et. al, 2008]中已證明，任二攝影機
拍攝相同的目標物，其亮度轉換關係會在
一較低維度的子空間中 (low-dimensional 
subspace)。 
 藉由此特性，即正確的配對會在低維度
子空間表現得很好，可是錯誤的配對則無
法學到好的結果。我們提出一自動學習演
算法[Chen et. al, 2008]。基於時空關係的學
習結果，我們可得到一超過60%正確率的
追 蹤 結果 ，接 著使 用 MCMC 演 算 法
[Dellaert, 2000]進行取樣，則可學出正確的
亮度轉換函式之低維度子空間。 
 
2-3 自動去除錯誤連結 
如我們先前所示，在上述的合法連結學習
過程中，會有許多誤錯連結產生，因此我
們提出一自動錯誤連結去除演算法。 
 
 
圖2-2 點A, C, and { }kBBBB ,,, 21 …=• 為
出入口。紅線表示正確的合法連結，藍線
表示錯誤連結。
•
P  表示所有在時間點t(PA)
與t(PC)間出現的目標物。 
 
以圖2-2為例，由錯誤連結的定義，我
們可得到，連結LAC的平均通過時間會大於
所對應的所有正確連結的平均通過時間總
和，即： 
Proposition 1. Suppose a weak link LAC is 
between node A and C, and there are 
corresponding valid links between node A 
and B1, B1 and B2,..., Bk-1 and Bk, and Bk and 
C. Then we have 
 10
 
(a) 
 
(b) 
圖2-5 實驗場景之攝影機監控網路拓撲之
自動學習結果。(a) Makris的方法，(b)我們
的方法。 
 
 接著，我們也展示亮度轉換關係的自動
學習，對我們的方法與其他自動學習的方法
[Gilbert and Bowden, 2008]進行比較。結果
如圖 2-6所示，我們的方法可迅速降低重建
誤差，可是他們方法雖然有進行逐漸學習，
不過依然無法學到正確的結果。 
 
 
圖2-6 亮度轉換關係之重建誤差的比較結
果：實驗場景之Cam 3與Cam 4。 
 
 最後，我們也比較不同方法的追蹤結果：
baseline method (人工決定攝影機連結情形，
直接比較外觀顏色)、ST only(僅使用自動學
習的時空關係)、baseline + BTF(人工決定攝
影機連結情形，不過會先進行亮度轉換再做
外觀比較)、ST + BTF(同時使用時空關係與
外觀關係之學習結果)。多攝影機追蹤之實
驗結果如圖 2-7。 
 
圖2-7 追蹤結果之正確率。 
 
由以上實驗結果可知，我們所提之自動學習
演算法，不論在時空關係的學習或是亮度轉
換關係的學習，都優於過去的方法。而且由
追蹤之正確率顯示，所提出之方法，藉由結
合兩個特徵，確時能有效提升多攝影機追蹤
之正確率。 
我們除了發展出相關的演算法進行多
攝 影機 的 布置 問 題 (camera placement 
problem)，在給定空間平面圖以及固定數量
攝影機後，透過自動漸進 (incremental) 
的方式並兼顧拍攝清楚與死角盡量減少的
情況下進行攝影機的佈建。同時我們也發展
了跨攝影機追蹤與自動連結性發掘的演算
法。 
 
2-6 空間死角偵測與警示 
在實務上，攝影機不論如何的配置，在有限
監視範圍下，死角仍可能是無法避免的問題。
當佈建完畢時，每個攝影機所監視的範圍，
可視為一個節點，由每個節點之間的空間相
對關係，可建置出一個有空間關係的拓撲建
置圖(topological map)，如圖一。配合攝
影機佈建圖，其在空間平面圖上的位置及監
控範圍可被標示出來，可協助死角的標定與
偵測。當嬰幼兒進入死角區域時，在這些無
法藉由影像直接監視的地方，即需要先前的
時間點的影像來判斷是否所欲追蹤的目標
進入了該無法看見的範圍裡。而當發現嬰幼
兒進入死角區域時，即可透過其他技術整合
 12
  
(a)              (b) 
 
(c) 
圖 2-10：嬰幼兒進入死角區域偵測結果。(a)
嬰幼兒在客廳爬行到接近死角區域警示偵
測，(b)嬰幼兒在客廳爬行到攝影機偵測不
到的死角警報，(c)嬰幼兒在廚房爬行到攝
影機偵測不到的死角警報。 
 
  
         (a)                (b) 
圖 2-11：嬰幼兒爬行到廚房之偵測結果。(a)
嬰幼兒在廚房爬行到危險區域警示偵測，(b)
嬰幼兒在廚房爬行到爐火旁之危險警報。 
如圖 2-9所示，首先將攝影機所擷取到
的視訊做背景學習，取出前景的物體，針對
前景的物體進行區塊追蹤，當嬰幼兒持續被
追蹤的同時也會偵測期在各場景可能的危
險事件，若非爬行至其他攝影機場景範圍，
也會判斷嬰幼兒是否爬行至死角區域。 
目前的系統我們包含了居家不同地點
與事件偵測，對於嬰幼兒來到死角區域附近
時，將給予通知給其他整合智慧型產品，如
圖 2-10(a)於客廳的場景實例，系統將可通
知機器人帶著嬰幼兒喜歡的玩具過去，避免
嬰幼兒進入死角區域而長時間無法被家長
得知情況。當嬰幼兒進入死角區域，如圖
2-10(b)的客廳場景及圖 2-10(c)的廚房場
景，系統將可景報通知家長與褓母，以免在
死角發生不尋常事件。    
嬰幼兒進入廚房的場景時，如果接近爐
火附近如圖 2-11(a)所示，系統亦將給予警
示以避免嬰幼兒更接近爐火，若嬰幼兒更接
近了爐火危險區域，如圖 2-11(b)所示，則
系統將發出警報給家長與褓母。 
 
  
(a)                (b) 
圖 2-12：嬰幼兒爬行到床緣之偵測結果。
(a)嬰幼兒爬行到床緣危險區域警示偵測，
(b)嬰幼兒由床緣摔下之危險警報。 
    在臥室場景時，我們模擬了兩個事件。
當嬰幼兒爬行到床緣，可能會危險墜落床下
時，我們將偵測並給予警示，如圖 2-12(a)
所示。若真正跌落床下，如圖 2-12(b)，則
系統將立即通知家長或褓母已發生嚴重事
件。當嬰兒在嬰兒在嬰兒車內，企圖爬出時，
若可能發生墜落情況將發出警示，如圖
2-13(a)，若從嬰兒車墜落，如圖 2-13(b)，
則系統亦立即通知家長或褓母已發生嚴重
事件。 
 
(a)                (b) 
圖 2-13：嬰幼兒爬出嬰兒車之偵測結果。
(a)嬰幼兒欲爬出嬰兒車之偵測警示，(b)
嬰幼兒爬出嬰兒車摔下之危險警報。 
 
當嬰幼兒在客廳場景時，接近沙發邊緣
與樓梯時可能會跌落，系統將偵測並給予警
示，如圖 2-14(a)與圖 2-15(a)。若嬰幼兒
 14
這個做法可以大幅度降低人工校正的作業
時間。 
連續修改的使用方式如下：使用者首先
選取一個特定的畫面（以下稱為基準畫面），
對其中的人臉位置作人工校正。之後，再選
一張畫面做作為連續校正範圍的終止畫面
（此終止畫面可以在基準畫面的之前或之
後）。使用者選擇預覽後，系統會自動從基
準畫面開始，透過追蹤的演算法標定範圍內
其他畫面中的人臉。使用者可以選擇確定，
將追蹤的演算法標定的人臉記錄下來。 
連續修改功能中的追蹤演算法說明如
下：我們將參考畫面中的人臉區域作為一個
樣本（template），在下一個畫面中，在擴大
5%的範圍中，找出最相似的區域。相似程
度的判定方式為計算兩區域間像素值的歐
氏距離，距離越小則越相似。 
在以下圖 3-1 中是人工校正程式的螢幕
截圖。其中黃色圓圈代表未校正（即 OpenCV
結果）之人臉位置，紅色圓圈代表過去人工
校正之位置，連續綠色方框代表連續修改的
範圍，而綠色圓圈代表連續修改結果之預覽
（經確認後將轉為紅色）。 
 
3-1-2 人臉五官標記 
我們利用[Everingham et. al, 2006]中的
演算法進行人臉五官自動標記，所標記的臉
部特徵點有 9 個：右眼外側、右眼內側、左
眼內側、左眼外側、鼻子右側、鼻尖、鼻子
左側、嘴巴右側、嘴巴左側。五官的位置對
後續分析有很大的價值。以下圖 3-2 中所標
記 9 個特徵點的範例（數字代表的是特徵點
的順序）。這部分結果尚未經過人工校正。
我們預期未來將以臉部人工校正程式為基
礎，透過類似的演算法，增加人工校正五官
位置的功能。 
人臉轉正: 
為了讓辨識用的資料能夠更一致，我們
利用五官擷取中得到的眼睛部分的四個點
（圖二中的點 1-4），利用這四個點所構成的
近似直線算出與水平線的夾角，再以影像中
心將整張臉部影像旋轉。以下圖 3-3 是人臉
轉正的一個範例（左右影像分別為轉正前與
轉正後）。 
 
 
以上我們描述了對一組大量視訊資料進行
人臉與五官標記的系統，以及所開發的進行
人工校正的工具。此類工具對於開發可用於
發展基於視訊的人臉資訊分析系統具有幫
助。所蒐集之部分資料被應用於後續的嬰幼
兒視訊事件偵測上。 
3-2 家庭視訊研究背景與簡介 
近年來，居家生活影片於網路上分享的
成長，提升了影片事件發掘的必要性，如此
使用者可以快速的索引及取得有趣或值得
懷念的片段。在眾多的居家影片中，嬰幼兒
成長過程的生活影片或許是大多數家庭最
重要的收藏，這可從 YouTube 對這類影片的
 
 
 
 
圖 3-3 
圖 3-2 
 16
模組中，藉以進行年齡判斷和表情識別。 
單一畫面可能會隸屬於多個事件。當一
部影片中大多數畫面只包含一個臉孔，我們
即假設其為嬰兒的臉，此假設滿足大多數的
嬰幼兒生活影片，且標註此為第一類影片
(type-1 videos)；其餘的影片，因其大多數的
畫面偵測到兩張以上的臉孔，我們註記其為
第二類影片(type-2 videos)。第一類影片的畫
面將直接進行臉部表情識別(3-4-2 節)以判
別表情狀況，無表情(neutral)、笑(smile)或
哭(cry)；對於第二類影片，則進行年齡分類
(3-4-3 節)及表情識別，以推測人物的年齡狀
況(嬰幼兒或成人)。圖 3-5 為事件判別之流
程圖。 
 
3-4-1 臉部及座標偵測 
居家影片的拍攝品質有限，使用單一臉
部偵測器很難偵測到每一個畫面中的每一
張臉。在此研究中，我們設計一個結合偵測
及雙向追蹤 (bi-directional tracking) 的方法
來定位臉部的區域。在偵測階段，我們只保
留可信度非常高的臉部偵測偵測，亦即須擔
保大部分在此階段所保留者的確是臉。此處
利用較嚴格的門檻值選擇來達成。 
偵測階段所找到的臉可視為”種子”，並
藉由其在該影片中尋找更多的臉。我們使用
物件追蹤的技術，試圖將這些種子連結成一
個片段，藉此找到因姿勢或光線變化而較難
偵測的臉。然而在時間上單一方向的追蹤可
能會因物件飄移而失敗，使用雙向追蹤較能
確保所追蹤到的臉是正確的：對畫面 i 的一
個臉部偵測種子，假設另一個偵測種子於畫
面 i+n，且時間 i+1 到 i+n-1 之間沒有偵測
到任何臉，則我們執行兩次追蹤工作：其一
做順向追蹤從畫面 i 到 i+n，另一個做反向
追蹤從 i+n 到 i。我們察看中間畫面 i+(n/2)
的追蹤結果，若兩個追蹤可於中間畫面相遇，
則合併兩個追蹤成一個單一片段，藉此利用
種子偵測的延伸找到更多的臉。但若雙方向
追蹤的結果無法在中間畫面相遇，我們則忽
略此片段的偵測，也就是，影片中還是會有
一些臉無法被偵測。此方法的目的是尋找更
多可信賴的臉而減少錯誤的偵測。 
在偵測階段，我們先使用 Viola-Jones 
臉部偵測 [Viola & Jones, 2001] 定出近乎
正面的臉，並藉由嚴格的門檻來避免錯誤的
偵測 [Everingham et al, 2006]，接著我們除
去時間上孤立出現的臉。然而，儘管謹慎的
選用門檻值，還是會包涵許多非人臉的區域，
為移除這些不正確的偵測，我們更進一步的
使用 active shape model (ASM) [Milborrow 
& Nicolls, 2008] 去驗證其是否為臉部區域 
─ 因一個可判別的臉部區域通常具有清楚
的臉部座標。儘管此假設會找到較少的臉，
在此階段，我們以找到可信度較高的臉部區
域為目標，遺漏的臉希望可由後續雙向追蹤
階段補回。 
在雙向追蹤階段，每一個單向追蹤都使
用 multiple-instance-learning (MIL) 追蹤器 
[Babenko et al., 2009]。MIL 追蹤器是一個基
於學習的追蹤方法，其可以學習物件於以前
追蹤時出現的樣式，達成有效的追蹤。此外，
雙向追蹤所發掘之臉部區域也會被代入 
ASM [Milborrow & Nicolls, 2008] 以進行確
認及臉部特徵點萃取。 
我們將此方法應用在網路上收集的多
部嬰幼兒生活影片中，得到了可接受的結果。
在 10 部平均長度 48.3 秒的居家影片中，我
們偵測到 10975 張臉部區域，其中只有 9 張
錯誤的偵測。 
3-4-2 臉部表情分析 
在實驗中，我們將分辨 3 種嬰幼兒的表
情，笑、哭及無表情。其中笑跟哭皆可歸類
為較誇張的表情。經驗上，將此問題直接看
臉部及座標偵測 
第二類
影片 
第一類影片 
 嬰幼兒與年長者 
 嬰幼兒與朋友們 
年紀判斷 
 歡樂時光 
 悲傷時光 
表情鑑定 
圖 3-5. 事件判別流程圖 
 18
1758 個畫面正確分到無表情，其平均正確
率分別為 86.2%、78.7%及 84.8%。對於第
二類影片，201、0 及 2222 個畫面分別被標
註歡樂時光、悲傷時光及無表情，其中有
192、0、1624 個畫面被正確偵測，其正確
率分別為 95.5%、100%及 73.1%。第二類影
片因其拍攝品質較差而較有挑戰性，其結果
可反映出較多無表情的案例被分類錯誤。 
人與人之間的事件只適用於第二類影
片。我們手動標註部分多於一張臉孔的影片
畫片，分別有 2481 及 1347 個畫面被標註嬰
幼兒與年長者及嬰幼兒與朋友們，其中有
1345 及 488 個畫面被正確偵測，正確率分
別為 54.2%及 36.2%。歸究其原因在於臉部
偵測器使正確率不如預期。由於專注於信賴
度較高的臉孔，對於拍攝品質較差的第二類
影片而言，許多原本多張臉之畫面中只偵測
到一張臉。然而，若我們只考慮於偵測到多
張臉的情形，則對於人與人之間事件的平均
正確率分別可達 76.3%及 59.3%。 
3-5 視訊事件偵測結果討論 
在本項目中，我們展現如何使用人臉相
關的分析及辨識技術來建構一套嬰幼兒生
活影片檢索系統。我們提出一個有系統的架
構來檢索嬰幼兒生活影片中的事件，並探討
某些人臉相關技術以呈現此規劃之可行
性。 
我們也對視訊資料中的性別辨識與年
齡（成人/兒童）辨識進行了初步分析。實
驗結果顯示這樣的辨識是可行的。儘管在目
前的研究中只使用表情及年齡分類器，我們
的架構能擴展到更多人臉相關的識別方法，
例如性別識別，如此可判別更多的事件，如
嬰幼兒與父母或祖父母。若人臉相關的辨識
技術有被廣泛地研究而持續改進，這些進步
皆可幫助此系統提升執行的結果。 
3-6 父母親表情模擬 
我們更進一步提出了一個以影像為基
礎之臉部表情模擬系統。給定一張不帶表情
的正臉當作來源人臉影像，此系統能模擬出
來源人臉在帶有表情時的影像。在臉部動畫
和人機互動的領域，以影像為基礎的臉部表
情模擬技術已經是相當普遍的研究主軸。即
使已經有很多相關的研究，但大部分的研究
仍是建立在二維空間上，因此容易忽略了由
表情改變所造成的肌肉變化。不同於之前只
著重在二維空間的研究，所提出的整合的系
統結合了二維空間方法和三維空間方法的
優點。一方面，我們利用一個省時的臉部模
型重建方法取得三維空間中較準確的臉部
幾何資訊。另一方面，利用從三維空間取得
的資訊在二維空間上模擬表情，產生出一個
自然的模擬結果。為了取得帶有表情的影像，
我們提出了以位移為基礎的表情模擬方法。
此方法只利用臉部的少數特徵點位移，就能
估計出所有臉的位移。實驗結果可以證明所
提出的系統可以產生出令人滿意的模擬結
果。 
臉部表情是最直接地反映出一個人的
情緒，例如，開心的時候會笑，難過的時候
會哭，生氣的時候會皺眉。也就是說，心理
的反應會影響到臉部的表情變化，進而影響
到外觀。因此，表情對一個人外表而言是相
當重要的。 
在日常生活中拍攝照片時，常常會出現
瑕疵。這些瑕疵大致上可以分成兩類：一種
是因為相機設定不良或外在條件不佳而造
成的瑕疵，如影像模糊、光線不足；另一種
是來自相片中的人物，對於拍攝出來的表情
很不滿意，像是覺得自己笑的時候很不上相
或是捕捉到怪異的表情等。所以我們很常羨
慕一些明星像是林志玲或言承旭一般的迷
人笑容。因此，我們提出了一個以影像為基
礎的臉部表情模擬系統，希望能夠模擬出使
用者套用別人表情的樣子。 
3-7 表情合成研究方法 
整個表情模擬系統流程如圖 3-9 所示。
在訓練的階段，先將三維人臉模型資料庫
[Yin et. al, 2006]分成兩個部分：不帶表情的
三維人臉模型和帶有表情的三維人臉模
 20
上特徵點位置，R 則是投影矩陣。利用縮小
三維特徵點投影後的座標和二維影像特徵
點座標之間的誤差，這個方法會疊代地找出
來源影像的三維模型。將這個不帶表情的三
維模型，套上在訓練階段取得的表情變形，
即可以達到表情模擬的效果，並取得帶有表
情的來源三維模型。這裡我們提出了下述式
子的位移估算能量函數，此能量函數能利用
從資料庫取得的少數特徵點表情變形位移

ivd 估算出臉部所有點的表情形變位移 ivd ，
並把估算出來的位移套用在來源影像的三
維模型。最後，將此三維模型做二維投影即
可取得帶有表情的來源影像。 

2 2 2
2( ) i i iii i
ne ne
i fp i
v v v
vdisp v v disp
v V v V
d d d
E d d d
x y z
λ
∈ ∈
 ∂ ∂ ∂     
= − + + +      ∂ ∂ ∂       
∑ ∑
 
3-8 表情合成實驗結果 
首先，我們先針對不同表情做模擬，如
圖 3-12 所示，最上排是不帶表情的來源影
像，下排是表情模擬的結果。我們模擬了四
種表情，分別是生氣、害怕、快樂和傷心。
從模擬結果可以看到，生氣的時候眼睛的形
狀會變細、眉毛上提；害怕的時候眼睛會變
大，嘴巴會往下抿；快樂的時候嘴角會上揚；
傷心的臉部變化跟害怕類似，但是眼睛不會
變大(圖 3-13)。 
此外，我們也針對同一種表情做不同強
度的模擬。圖 3-14 所呈現的是我們對快樂
的表情做不同強度的模擬。上排是從資料庫
裡面取出表情變形的三維人臉模型，下排是
模擬的結果圖。由結果圖可看出，隨著表情
強度増大，肌肉的變形也越明顯。一個人在
笑的時候，嘴巴和鼻子的擴張會導致附近肌
肉的變動，這個部分的細節放大在圖 3-15。 
從圖 3-15 可以看出本系統的限制。當
要模擬的對象在笑的時候有露齒會造成模
擬的困難，嘴巴在垂直方向上的變動並不能
模擬出來。這是因為牙齒本身的質地跟嘴巴
其他部分差異太大，這會讓訓練階段的對位
步驟結果有落差，但是本系統在嘴巴水平方
向上變動(嘴巴往外擴、嘴角往上提)的模擬
結果是令人滿意的。 
在計算效能的部分，因為我們只處理臉
部區域的部分，一張解析度為150 150× 的人
臉影像，本系統模擬出一種表情的時間為 1
分鐘。這樣的計算效能和模擬結果說明了本
系統的實用性。 
 
 
 
圖 3-12:生氣、害怕、快樂、傷心，四種表
情模擬結果 
 
圖 3-13:害怕、傷心眼睛模擬結果比較 
 22
子像是鮮明色彩或者巧思構圖的影片則很
輕易的就能觸動觀眾的心。為了能夠拍出有
美感的照片，攝影專家門採用了許多特別的
技巧來讓他們拍攝的相片在美感上達到一
定的水準，像是景深，或是著名的三分之一
法則，同樣的，利用這些技巧，並將這些技
巧給設計成評分自動化人們可以拍出一個
更好更有美感的影片。 
 自動美學評比擁有許多的應用，其中之
一就是用在拍攝家用影片時，能夠即時的反
應現在的美感程度，如此一來就可以避免拍
出令自己懊悔不已的影片，像是過曝或者手
震。美學評比能在未來拍攝影片時，提供許
多的輔助功能，這對於家用攝影，或是業餘
攝影，將會有不少的幫助以及樂趣。從以上
的優點我們得知，美學評比變得越來越重要，
過去就有許多的研究做在相片上，使得電腦
足以用機器學習的方式，判斷出照片是否有
美感。隨後，[Luo et al. 2008]中試圖將現有
的美學特徵(aesthetic feature)直接應用在影
片上，畢竟影片是由許多的照片所組合而成，
他們還提出了不同的見解 ─ 專業攝影的
主題，多半是在焦點上，並且背景多半是非
對焦。然而一般消費者攝影的模式不一定會
取景深。因此，用他們方法抓出來的主角可
能會錯誤。此外，沒考慮到時間變化，單純
只分析組成影片的圖片群，會讓評比有失公
允。最近 [Moorthy et al. eccv 2010] 提出了
一 個分層 整合方法 (hierarchical pooling 
method) 將美學特徵和時間變化整合起來，
用以模型化影片的美學品質，然而因為沒有
考量到重要的動態特性(motion property)，這
些美學特徵對影片的評比能力仍需加強。 
 本研究除了提出更多動態特性，更進一
步研究美學特徵的語義特性，使得我們可以
客觀的評比。有些美學特徵擁有語義獨立特
性，像是影片的晃動，不論使用者拍的物體
或內容是什麼，觀看者所接收到的美感或不
適感都是一樣的，這種情況我們就稱作語義
獨立，反之，若觀看的美感或不適感會隨著
拍攝的物體改變，像是亮度，那麼這樣的美
學特徵我們說它有語義非獨立的特性。事實
上，這二種特性同樣重要，因為它們在不同
的地方有其特別的應用，語義獨立適合應用
多樣化的影片研究，而非獨立的應用範圍則
比較屬於是同質性高的影片。 
4-2 研究方法 
4-2-1 定義問題 
首先我們將第 i 部影片定義為  ，從 
  拆解出m張圖，那麼  {, , … , }, 
 代表著第 j 張圖。同樣的道理，一張圖可
以 找 出 n 個 美 學 特 徵 ， 那 麼
  { ,  , … ,  !},	裡頭 # ∈ ℝ 並且 #代表
著第 k 種類型的特徵。有了這些美學特徵以
後，我們採用[Moorthy et al. ECCV 2010]所
提的整合方法，將影片裡每一小段給整合成
起來，整合方法(&, &)包含了許多種運算子
像是：平均值、中間值、最大值、最小值…
等等，更確切的描述這個方法，我們把一秒
內的所有圖透過整合方法合在一起。假設
'  [, ), … ,)*],, 其中 N是攝影取圖
頻率 (frame per second)，把'裡頭元素透過
& 整 合 起 來 成 秒 合 特 徵 (second-pooled 
feature)，得到B  &'，接下來，再透過
&將所有的秒合特徵整合起來，得到整部影
片的整合特徵X  &/;∀。至於影片中的
主角要怎麼定義，我們則採用[Rahtu et al. 
2010] 中所提出來的 saliency segmentation
方法，將影片中的 saliency 排名前 25% 之
像素設為攝影主角。 
4-2-2 美學特徵計算方法 
(1)語義獨立特徵 
移動空間(motion space): 
根據專業攝影技巧的分析，當使用者拍
攝一段主角在移動的影片時，必須注意到主
角移動方向的前方要留下足夠的空間，因為
這個留下來的空間，會讓觀眾多了一份想像，
由圖 4-1、圖 4-2 我們即可看出移動空間所
帶給人們想像空間的差別。 
 
 24
合諧度設為K  LK, K , … , KMN
,，那麼最
後的計算公式如下： 
 J)E  |ℎ ∗Q K  KE ∗Q K|, ∀R ∈ [1,2,… ,7] 
其中∗Q為回旋積分取最大值。 
 
構圖(composition): 
除了顏色以外，構圖也是影響圖片美感的重
要因素之一，例如:三分之一法則，景深以
及凸邊形。三分之一法則敘述的是：如果一
張畫面以九宮格的方式切割，那麼攝影主題
應該放在九宮格切割線的那四個交點的其
中一點。關於景深的部份，則是強調主角和
背景的清晰度對比，背景若是模糊的，則有
助於突顯主角的存在，也有助於美感的提升。
而凸邊形是指人們會傾向攝影主體是凸多
邊形存在，根據 [Luo et al. 2008]，藉由以上
三種美學特性，我們可以得出三個美學特徵
 B、 F、 J分別表示三分之一法則，景深
以及凸邊形。 
 (2)語義非獨立特徵 
動向熵(motion direction entropy): 
在過去的應用中，熵常被用來衡量狀態的不
穩定度，在本研究中，我們採用熵來衡量每
個畫面中每個畫素的移動方向亂度，簡而言
之，畫面中的物體如果很散亂，那麼動向就
會遍佈各個方向，在此我們先統計各方向，
而這些方向大致分為五個，也就是上(UVW)
右UVW)下UVWB)左UVWF)，以及針對移動量
很小的值或者不沒有動的值，設為不動
UVWJ)，統計這些方向的數目後，我們代入
以下算動向熵的公式： 
 X  ∑ D lnDJD[ ，D 
D!\
∑ D!]
^
]_`
 
然而，不同的攝影主題會得到不同的美學特
徵，舉例來說，拍攝一段球類比賽跟風景就
會有很大的動向熵美學特徵差異，所以這個
特徵我們把他歸類在語義非獨立特徵中。 
 
飽合度及明度(color saturation and value): 
在 HSV 空間中，除了前面討論的色相(Hue)
以外，還有 S(飽合度)及 V(明度)應列為美感
特徵，因此我們針對這二個特徵算出 M ,  a。
此外，根據三分之一法則，最中間區塊對於
觀眾是特別有吸引力的，所以我們又特別計
算中間區塊的平均明度及飽合度，得到二個
新的特徵 b,  c。 
 
亮度(Lightness): 
對於亮度我們希望能考慮的點是它和背景
的對比，在 中我們算主角和扣掉主角的
背景亮度對比，而另一個則是算主角和整個
畫面的亮度對比，稱為 。之所以要這樣
區分，是因為 是考慮了主角亮的情況，
如果主角過亮，會讓整個平均亮度大幅度提
高，如此一來，便難以看出對比度，而 還
是需要，因為如果這部影片沒有主角，那麼
扣掉主角會變成雜訊干擾。 
 最後，我們得到這 22 種美學特徵，也
就是一開始定義問題中的 n=22。既然 n=22
是一張圖會得到的美學特徵數，那麼一部影
片有很多圖片，就需整合起來，先從整合方
法&開始，&  {mean, median, min, max	,
first	quartile, third	quartile}一共六種運算，
而&  {mean, std}一共兩種運算，所以一部
影片會有 22*6*2=264 個可能的美學特徵。 
  
表 4-1： 
Type Motion Color Lightness Comp. 
Dependent 1 4 2 0 
Independent 5 7 0 3 
 
表 4-2： 
D.      I. Motion Color Lightness Comp. 
Motion 72±1.3 63±2.1 60±2.3 66±1.5 
Color 74±1.4 64±2.5 56±2.2 60±2.2 
Lightness 71±1.7 70±2.1 59±2.4 65±2.4 
Comp. 72±1.3 63±2.7 N/A 65±2.5 
 
4-3 視訊拍攝美感評估結果 
為了分析我們的評比方法是否可靠，我
們用了 [Moorthy et al. 2010] 所提供的資料
 26
表於[Chu et. al, ICPR’10]，[Chang et. al, 
ICPR’10]。在光線變化之人臉比對與辨認研
究方面，發表於[Chen et. al, CVPR’11] 以及
期刊 IEEE TSMCB [Chen et. al., 2012]。關
於攝影機網路之自動連結性發掘發表於期
刊 IEEE TMM ([Chen et. al, 2011]) 。除應用
性課題方法開發外，也伴隨基礎方法的探討：
關於影像分割之研究，包含了運動分割以及
多影像共分割，分別發表於期刊 IJCV ([Jian 
and Chen, 2010]) 以 及  [Chu et. al, 
ACCV’10]。在機器學習之分群方法探討方
面 ，發表 於 [Huang et. al, ICASSP’12], 
[Huang et. al, CVPR’12], 以及 期刊 IEEE 
TFS [Huang et. al, 2012]。在視訊追蹤與亮度
變化之前處理方面，分別發表於 [Yeh et. al, 
ICIP’10]以及 [Chen et. al, ICIP’11]。其中
CVPR 為電腦視覺領域頂尖的國際會議(top 
conference), 國內每年能在此會議刊登之論
文數量甚為稀少。 
 
參考文獻 
1. B. Babenko, M. H. Yang, S. Belongie, 
“Visual Tracking with Online Multiple 
Instance Learning,” CVPR 2009. 
2. V. Blanz and T. Vetter, “A morphable 
model for the synthesis of 3d faces,” 
SIGGRAPH, 1999. 
3. W. Y. Chang, C. S. Chen and Y. P. Hung, 
“Analyzing Facial Expression by Fusing 
Manifolds,” ACCV 2007. 
4. K. W. Chen, C. C. Lai, Y. P. Hung, and C. 
S. Chen, "An Adaptive Learning Method 
for Target Tracking across Multiple 
Cameras," IEEE Conference on Comp 
Vision and Pattern Recognition, CVPR, 
June 2008. 
5. W. H. Cheng, Y. Y. Chuang. Y. T. Lin, et al.  
“Semantic Analysis for Automatic Event 
Recognition and Segmentation of Wedding 
Ceremony Videos,” IEEE Trans. Circuits 
and Systems for Video Technology, Vol. 18, 
No. 11, pp. 1639-1650, 2008. 
6. C. D. Cerosaletti and A.C. Loui, 
“Measuring the perceived aesthetic quality 
of photographic images,” in QoMEx, 2009. 
7. D. Cohen-Or, O. Sorkine, R. Gal, T. 
Leyvand, and Y.Q. Xu, “Color 
harmonization,” ACM Trans. Graph., vol. 
25, no. 3, 2006. 
8. N. Conci and L. Lizzi,“Camera placement 
using particle swarm optimization in visual 
surveillance applications,” Proceedings of 
the International Conference on Image 
Processing, pp. 3485-3488, 2009. 
9. T. Cormen, C. Leiserson, R. Rivest, and C. 
Stein, “Introduction to Algorithms,”  the 
MIT Press, Cambridge, Massachusetts, 
1999. 
10. N. Dalal and B. Triggs, “Histograms of 
oriented gradients for human detection,” 
CVPR 2005. 
11. R. Datta, D. Joshi, J. Li, and J. Z. Wang, 
“Studying aesthetics in photographic 
images using a computational approach,” 
in ECCV, 2006. 
12. F. Dellaert, “Addressing the 
Correspondence Problem: A Markov 
Chain Monte Carlo Approach,” Tech. 
Report, Carnegie Mellon Univ. School of 
Computer Science, 2000. 
13. M. Everingham, J. Sivic, and A. Zisserman, 
“"Hello! My name is... Buffy" - Automatic 
naming of characters in TV video,” BMVC 
2006. 
14. A. Gilbert and R. Bowden,“Incremental, 
Scalable Tracking of Objects Inter 
Camera,” Computer Vision and Image 
Understanding, 111(1), 2008, pp. 43-58. 
15. A. Gupta, P. Srinivasan,  J. Shi, and L. S. 
Davis, “Understanding Videos, 
Constructing Plots, and Learning a 
Visually Grounded Storyline Model from 
Annotated Videos,” CVPR, 2009. 
16. E. Hörster, R. Lienhart, “On the optimal 
placement of multiple   visual sensors,” 
Proceedings of the third ACM 
International Workshop on Video 
Surveillance and Sensor Networks, pages 
111-120, 2006. 
17. X. Hua, L. Lu, and H. J. Zhang, “AVE - 
Automated Home Video Editing,” ACM 
MM 2003. 
18. C. R. Huang, H. P. Lee, and C. S. Chen, 
“Shot Change Detection via Local 
Keypoing Matching,” IEEE Trans. 
Multimedia, Vol. 10, No. 6, 2008. 
19. C. R. Huang and C. S. Chen, “Video Scene 
Detection by Link-Constrained Affinity 
Propagation,” ISCAS 2009. 
 28
Recognition, ICPR, August 2010. 
45. Kuan-Wen Chen, Chih-Chuan Lai, 
Pei-Jyun Lee, Chu-Song Chen, and 
Yi-Ping Hung, "Adaptive Learning for 
Target Tracking across Multiple 
Non-Overlapping Cameras," IEEE Trans. 
on Multimedia, Vol. 13, No. 4, pp. 625-638, 
August 2011. 
46. Lu-Hung Chen, Yao-Hsiang Yang, 
Chu-Song Chen, and Ming-Yen Cheng, 
“Illumination Invariant Feature 
Extraction – Taking Face Images as an 
Example,” IEEE Computer Society 
Conference on Computer Vision and 
Pattern Recognition, CVPR, June 2011. 
(top conference in computer vision) 
47. Lu-Hung Chen, Yao-Hsiang Yang, and 
Chu-Song Chen, “A Monotonic 
Constrained Regression Framework for 
Histogram Equalization and Specification,” 
to appear in IEEE Intl. Conf. on Image 
Processing, ICIP, 2011. 
48. Chia-Ping Chen and Chu-Song Chen, 
"Intrinsic Illumination Subspace for 
Lighting Insensitive Face 
Recognition," IEEE Trans. on Systems, 
Man, and Cybernetics, Part B: 
Cybernetics, Vol. 42, No. 2, pp. 422-433, 
April 2012. 
49. Wen-Sheng Chu, Chun-Rong Huang, 
Chu-Song Chen, “Identifying Gender from 
Unaligned Facial Images by Set 
Classification,” Intl Conf on Pattern 
Recognition, ICPR, August 2010. 
50. Wen-Sheng Chu, Chia-Ping Chen, and 
Chu-Song Chen, "MOMI-Cosegmentation: 
Simultaneous Segmentation of Multiple 
Objects among Multiple Images," Asian 
Conference on Computer Vision, ACCV, 
October 2010. 
51. Hsin-Chien Huang, Yung-Yu Chuang and 
Chu-Song Chen, "Multiple Kernel Fuzzy 
Clustering," IEEE Transactions on Fuzzy 
Systems, volume 20, number 1, pages 
120-134, February 2012. 
52. Hsin-Chien Huang, Yung-Yu Chuang, and 
Chu-Song Chen, "Affinity Aggregation for 
Spectral Clustering," IEEE Computer 
Society Conference on Computer Vision 
and Pattern Recognition, CVPR, June 
2012. (top conference in computer vision) 
53. Hsin-Chien Huang, Yung-Yu Chuang, and 
Chu-Song Chen, "Multi-affinity Spectral 
Clustering," Intl. Conf. on Acoustics, 
Speech, and Signal Processing, ICASSP 
2012. 
54. Yong-Dian Jian and Chu-Song Chen, 
“Two-View Motion Segmentation with 
Model Selection and Outlier Removal by 
RANSAC-Enhanced Dirichlet Process 
Mixture Models,” International Journal of 
Computer Vision, volume 88, number 3, 
July 2010.  
55. Chun-Yu Yang, Hsin-Ho Yeh, and 
Chu-Song Chen, “Video Aesthetic Quality 
Assessment by Combining Semantically 
Independent and Dependent Features,” Intl. 
Conf. on Acoustics, Speech, and Signal 
Processing, ICASSP, May 2011. 
56. Hsin-Ho Yeh, Jiun-Yu Chen, Chun-Rong 
Huang and Chu-Song Chen, “An Adaptive 
Approach for Overlapping People 
Tracking Based on Foreground 
Silhouettes,” IEEE Intl. Conf. on Image 
Processing, ICIP, September 2010. 
表 Y04 
再在非剛性三維重建上。這是一般化的模型，可取代過去的模型，在相關運用到 latent 
variable model 的問題上更能有所幫助。 
在本屆的會議中，也伴隨著許多相關研討會(workshop)的舉辦。其中在 OpenCV 
workshop 中，由來自 OpenCV 的開發團隊人員親自擔任演講者。他們介紹了 OpenCV 
的發展走向，以及其以非營利為目的的宗旨（甚至可以商業使用）。我們知道 OpenCV 已
經是目前世界上最被廣泛運用的開放式電腦視覺程式碼，能夠讓大家很有效率的開發電
腦視覺方面相關的應用。此次 OpenCV workshop 特別針對行動裝置，如 Android 系統
以及 Apple 上面的 IOS 系統的開發作說明。吸引現場許多人聆聽。 
在 coffee break 中，遇到了不少教授、學生、與公司派來參加的人員。其中遇到了 
Canon Research 團隊的人員。由於此研究中心才剛設立，也因而瞭解到 Canon 也開始
在美國設立研究團隊與實驗室，並希望挖角美國 CMU 的著名教授擔任負責人。這些國
際級的大公司紛紛在美國設立電腦視覺領域設立研發團隊，代表了電腦視覺領域的日漸
興盛。由大公司來設立這些兼顧學術發展的研究團隊 (如 IMB 的 TJ Watson)， 除可以
吸引剛國際人才加入外、也有利於擴大研究成果與掌握先期技術。相較於國外而言，台
灣的大公司在美國設立相關性質研究團隊的情況似乎較不普及。未來也許可考慮學習美
國、日本、歐洲等大公司做這類相關的長遠投資，以累積長期競爭力。 
在晚宴中，頒發了論文獎以及相關終身成就獎。其中 Brown University 的 David 
Cooper 教授也是獲獎人。由於他是本人指導教授的指導教授，成就斐然而獲獎，讓我
們也倍感高興。並也跟他道賀與合影。 
參與此會議可獲得電腦視覺最新資訊，對計畫執行甚有幫助。所發表之論文如附
件。 
 
 
 
Affinity Aggregation for Spectral Clustering
Hsin-Chien Huang1,2 Yung-Yu Chuang1,3 Chu-Song Chen2,3
1 Dept. of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan.
2 Institute of Information Science, Academia Sinica, Taipei, Taiwan.
3 Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan.
E-mail: sean@cmlab.csie.ntu.edu.tw, cyy@csie.ntu.edu.tw, song@iis.sinica.edu.tw
Abstract
Spectral clustering makes use of spectral-graph struc-
ture of an affinity matrix to partition data into disjoint
meaningful groups. Because of its elegance, efficiency and
good performance, spectral clustering has become one of
the most popular clustering methods. Traditional spectral
clustering assumes a single affinity matrix. However, in
many applications, there could be multiple potentially use-
ful features and thereby multiple affinity matrices. To ap-
ply spectral clustering for these cases, a possible way is to
aggregate the affinity matrices into a single one. Unfortu-
nately, affinity measures constructed from different features
could have different characteristics. Careless aggregation
might make even worse clustering performance. This paper
proposes an affinity aggregation spectral clustering (AASC)
algorithm which extends spectral clustering to a setting with
multiple affinities available. AASC seeks for an optimal
combination of affinity matrices so that it is more immune
to ineffective affinities and irrelevant features. This enables
the construction of similarity or distance-metric measures
for clustering less crucial. Experiments show that AASC
is effective in simultaneous clustering and feature fusion,
thus enhancing the performance of spectral clustering by
employing multiple affinities.
1. Introduction
Clustering is an important unsupervised learning method
for dividing data into a set of disjoint subsets with high
intra-cluster similarity and low inter-cluster similarity. It
has been addressed in many contexts and widely used for
computer vision, pattern recognition, and multimedia anal-
ysis. Among many clustering algorithms proposed before,
spectral clustering (SC) is one of the best. It often outper-
forms other methods by transforming data points into an-
other space in which their cluster properties are enhanced.
The success of spectral clustering algorithms depends
heavily on the choice of the metric [3]. However, spectral
clustering has no built-in mechanism for discovering good
metrics for better clustering results. Therefore, it is often
necessary to use other feature selection or feature weight-
ing methods as a precursor before invoking spectral cluster-
ing. The problem is aggravated for many real-world clus-
tering problems in which there are multiple potentially use-
ful cues. For example, for face clustering, cues such as hair
appearance, global positioning information and clothing ap-
pearance have been used for boosting the performance [1].
For clustering images, a variety of visual descriptors have
been proposed for colors, textures and bag-of-word mod-
els. Each type of the descriptors defines an affinity matrix
in association with its similarity metric. For such applica-
tions, to apply spectral clustering, it is often necessary to
aggregate similarity measures from different features into
a single affinity matrix by feature selection or feature fu-
sion. Otherwise, performance of spectral clustering could
degrade dramatically in the presence of irrelevant, ineffec-
tive or unreliable features.
This paper proposes affinity aggregation spectral clus-
tering (AASC), a method for aggregating affinity matrices
for spectral clustering. The proposed method shares simi-
lar ideas with multiple kernel learning (MKL) [23, 35, 8]
that aggregates several kernels to construct a better one. We
propose a framework for learning the similarity matrix of
spectral clustering, which attempts to make spectral cluster-
ing more robust by alleviating the impact of unreliable and
irrelevant features. However, the method is different from
MKL in the following two aspects: (1) our method is un-
supervised, i.e. no labels are available for data; and (2) the
affinity matrix composed by the pairwise similarity between
data is not necessarily positive semi-definite. We only as-
sume that affinity matrices are symmetric.
The rest of the paper is organized as follows. Section 2
discusses related work. In Section 3, we briefly review spec-
tral clustering. Section 4 introduces our affinity aggregation
spectral clustering algorithm. Experiments are described in
Section 5. Finally, Section 6 concludes the paper with di-
rections for future work.
a n × C matrix, the i-th row of the stacked matrix corre-
sponds to the indicator fi for xi. The above method is called
the unnormalized spectral clustering.
Shi and Malik [25] proposed a normalized spectral clus-
tering algorithm, in which the indicators are constructed
by finding the eigenvectors v of the generalized eigenprob-
lem Lv = λDv. In normalized spectral clustering, when
minimizing Equation 1, the constraint employed becomes
fTDf = 1 instead of fT f = 1. It is equivalent to mini-
mizing gT (D−1/2LD−1/2)g when transforming the vari-
ables with g = D1/2f and constrained by gTg = 1. Many
studies [25, 3] have shown that normalized spectral cluster-
ing performs considerably better than unnormalized spec-
tral clustering for various problems.
In practice, spectral clustering often serves as a prepro-
cessing step of other clustering algorithms such as k-means.
The main trick of spectral clustering is to transform the rep-
resentations of the data points xi into the indicator space in
which the cluster characteristics become more prominent.
Because cluster properties are enhanced in this new rep-
resentation space, even simple clustering algorithms, such
as k-means clustering, have no difficulty on distinguishing
clusters. Main reasons for spectral clustering’s success in-
clude: (1) it does not make any assumptions on the form
of the clusters (as opposed to k-means, where the clusters
are always convex sets); and (2) it can be implemented effi-
ciently even for large data sets as long as the affinity matrix
is sparse. However, one of its limitations is that choosing a
good affinity measure is not trivial for the application. For
real-world clustering problems, the affinities wij could be
obtained in multiple ways. They could be determined with
different types of extracted features, or be constructed by re-
producible kernels when xi are vectors in some Euclidean
space. We show how to find a weighted combination of the
affinities so that a better similarity measure can be learned
for spectral clustering in an unsupervised fashion.
4. Affinity aggregation spectral clustering
Assume that there are m affinity matrices Wk(k =
1 . . .m) available. The k-th matrix’s ij-th element wij;k
represents the similarity between xi and xj when measur-
ing with the k-th type of affinity metric. Since the affini-
ties wij;k are non-negative, we can denote wij;k = s2ij;k
to reflect this nature. As mentioned, the goal is to find a
proper weight assignment to these affinities. Let v =
[v1, v2, · · · , vk]T be a weight vector in association with
these affinities. The k-th weighted affinity can be denoted as
σij;k = vksij;k. We can then formulate the AASC problem
as
min
f1,...,fn
v1,...,vm
∑
k
∑
i,j
σ2ij;k||fi − fj ||2
= min
f1,...,fn
v1,...,vm
∑
k
∑
i,j
v2kwij;k||fi − fj ||2
= min
f1,...,fn
v1,...,vm
∑
k
v2kf
T (Dk −Wk)f
≡ min
v1,...,vm
∑
k
βkv
2
k (2)
whereDk−Wk is the Laplacian matrix associated with the
k-th affinity metric, and
βk = f
T (Dk −Wk)f .
Note that applying an affinity aggregation vector v makes
the representation of new data easy. Hence, it avoids the
out-of-sample problem of previous works such as Cai et
al. [6]. Besides, we minimize the clustering error directly in
the representation space, making the results better than find-
ing an averaged representation of the single-affinity outputs
in their approach (c.f. the experimental validation).
The objective is minimized under the constraint that
weighted sum of vk is normalized,
∑m
k=1 tkvk = n,
where tk = tr(Sk) and Sk is the matrix constituted
of sij;k. This implies the trace of the aggregated affin-
ity matrix is bounded. It is because from Cauchy-
Schwartz inequality, the aggregated affinity matrix satisfies
that tr(W) =
∑m
k=1 tr(v
2
kWk) =
∑m
k=1 v
2
ktr(Wk) >
1
m (
∑m
k=1 vktr(Sk))
2 = n
2
m , yielding a lower bound of the
trace of W. Without loss of generality, since the diagonal
element of an affinity matrix is always set as 1, which im-
plies tk = n. The constraint
∑m
k=1 tkvk = n thus becomes
a simpler form,
∑m
k=1 vk = 1.
In addition, to satisfy the normalized spectral clustering,
the constraint fTDf = 1 is also required. That is,
1 = fTDf = fT (v21D1 + · · ·+ v2kDk)f ≡
∑
k
αkv
2
k
where
αk = f
TDkf .
To solve the above problem, there are two sets of vari-
ables, the indicator vector f and the affinity aggregation
weights v. It becomes much easier to solve if we solve
one set of variables at a time while fixing the other set of
variables. If the weights vk are given, the problem becomes
a standard spectral clustering problem (Equation 1) and the
affinities are set as wij =
∑
k v
2
kwij;k. This can be done by
finding the eigenvectors of the Laplacian matrix as reviewed
in the previous section.
is ensured since the objective in Equation 2 is minimized by
solving the weights v under the constraints (Equation 3 and
4), and also minimized by solving indicator f under the con-
straint that f is orthogonal to the constant-one-vector. Thus,
alternatively finding v and f keeps reducing the error, en-
suring convergence of the iterative process. Algorithm 1
summarizes the proposed AASC algorithm.
5. Experiments
We have implemented and tested the proposed AASC al-
gorithm on a variety of clustering problems including image
clustering, face clustering and text clustering. This section
starts by describing the procedure for calculating similar-
ity and the adopted metrics for comparing clustering results
(Section 5.1). For image clustering (Section 5.2), we used
two benchmark datasets, Caltech-101 [11] and Microsoft
Research Cambridge Volume 1(MSRC-v1) [30]. Two well-
known face databases from ORL [24] and CMU-PIE [26]
were used for face clustering (Section 5.3). As for text
clustering (Section 5.4), we adopted two data sets from 20
Newsgroups and Reuters-21578. Statistics of these data
sets are summarized in Table 1, including the number of
instances, dimensionality of data and the number of clus-
ters. For each set of experiments, we describe the data sets,
the experimental settings, the choice of pairwise affinities,
the experimental results and comparisons to other methods.
5.1. Settings and measures
We first describe how to obtain the affinity matrix for
each type of feature. Given the raw data in the data set,
the first step is to extract features for each instance. Each
feature can be represented as a vector. These feature vec-
tors were substituted into the Gaussian kernel to calculate
pairwise distances,
κ(xi,xj) = exp(−(xi − xj)T (xi − xj)/σ).
Assume that the minimal value of the Gaussian kernel over
the data set is γ. We then obtain the corresponding σ as
σ = min
i,j
(−(xi − xj)T (xi − xj)/log(γ)).
and we set γ to 0.005.
For comparing clustering results, clustering measures
were used to evaluate how well data are grouped in
comparison with the ground truth. Clustering measures
can be roughly categorized into pair-counting-based mea-
sures (e.g. Rand index (RI) and adjusted Rand index
(ARI) [13]), set-matching-based measures (e.g.H criterion)
and information-theoretic-based measures (e.g. mutual in-
formation and normalized mutual information (NMI) [28]).
Several papers have attempted to evaluate these clustering
measures. Unfortunately, there is no definite answer on
Table 1. Statistics of the data sets used in the experiments. The
first two data sets are adopted from Caltech-101 with seven and
twenty classes. Along with the third data set MSRC-v1, these three
sets were used for image clustering. For face clustering, two face
databases from ORL and CMU-PIE were used. For CMU-PIE, we
used the frontal images (Pose 27) with 22 different lightings. The
last two are text data sets from 20 Newsgroups and Reuters-21578.
For 20 Newsgroups, we randomly chose 100 instances from each
class in the training set. For Reuters-21578, we used the test set of
R52.
ID Name #instances #dimensions #classes
I1 Caltech-101 441 100,000 7
I2 Caltech-101 1,230 100,000 20
I3 MSRC-v1 210 100,000 7
F1 ORL 360 7,744 40
F2 CMU-PIE 1,496 7,744 68
T1 20 Newsgroups 2,000 25,753 20
T2 Reuters-21578 2,568 8,575 52
which measure is the best yet. Vinh et al. [29] reported that
some popular measures do not facilitate informative cluster-
ing comparisons because they either do not have a predeter-
mined range or do not have a constant baseline value. For
those measures, a poor clustering could yield a very high
performance index, especially when there are many clus-
ters. They suggested that ARI is a faithful measure that
does not have these drawbacks. They also proposed another
fair measure, adjusted mutual information (AMI). However,
Wu et al. [32] reported that, when clustering performances
are hard to distinguish, the normalized variation of mutual
information, i.e. NMI, could still work the best. For fair
comparisons, this paper uses AMI, NMI and ARI as met-
rics for reporting clustering performance.
5.2. Image clustering
In order to compare AASC to MMSC [6], we used the
same data sets Caltech-101 and MSRC-v1. For Caltech-
101, we followMMSC to choose the same 7 and 20 classes.
For MSRC-v1, the same 7 classes were obtained in the same
way as MMSC. AsMMSC, five types of features were used,
LBP [21], GIST [22], CENTRIST [31], Dog-SIFT [18], and
HOG [9]. We denote SCL, SCG, SCC , SCD and SCH
as the single-affinity spectral clustering methods with five
different affinity matrices derived from the above five fea-
tures (LBP, GIST, CENTRIST, Dog-SIFT, and HOG), re-
spectively. In addition, we also combined the above five
affinity matrices by equal weights and denoted it as EASC.
Tables 2, 3 and 4 show AMI, NMI and ARI values for
different algorithms on Caltech-101 (7 classes), Caltech-
101 (20 classes) and MSRC-v1. As the results show, the
proposed AASC method has better performance than other
methods. Note that, the performance of our single-affinity
spectral clustering methods are not exactly the same with
the ones listed in the MMSC paper [6] due to implemen-
(a) Eigenface
(b) Gabor texture
(c) LBP
(d) EASC
(e) AASC
Figure 1. The visual clustering performance of different methods
for ORL data set. AASC correctly grouped photos of a subject
into a cluster while other methods either wrongly included photos
of other subjects or left out some photos of the subject.
(a) Eigenface
(b) Gabor texture
(c) LBP
(d) EASC
(e) AASC
Figure 2. The visual clustering performance of different methods
for CMU data set. AASC correctly grouped photos of a subject
into a cluster while other methods either wrongly included photos
of other subjects or left out some photos of the subject.
train-stemmed and r52-test-stemmed to evaluate AASC. Let
D = {d1, · · · , dn} be the set of documents and T =
{t1, · · · , tm} the set of distinct words occurring in D. We
denote the frequency of word t ∈ T in the document d ∈ D
as tf (d, t). tf -idf is a weighting scheme which weights the
frequency of a word t in the document d with a factor that
discounts its importance with its occurrences in the whole
document collection, which is defined as
tf -idf (d, t) = tf (d, t)× log( |D|
df (t)
),
where df (t) is the number of documents in which the word
t appears. Thus, the feature vector representation of a doc-
ument d is defined as
−→
td = (tf -idf (d, t1) , · · · , tf -idf (d, tm)) .
Table 7. Comparisons of different methods on text data set 20
Newsgroups in terms of AMI, NMI and ARI.
AMI NMI ARI
SCed 0.5230 0.5749 0.2868
SCcs 0.5212 0.5723 0.2816
SCjc 0.5237 0.5762 0.2854
SCpcc 0.5152 0.5661 0.2779
EASC 0.5199 0.5690 0.2814
AASC 0.5340 0.5840 0.3003
Table 8. Comparisons of different methods on text data set
Reuters-21578 in terms of AMI, NMI and ARI.
AMI NMI ARI
SCed 0.3585 0.5151 0.1854
SCcs 0.3519 0.5086 0.1828
SCjc 0.3671 0.5208 0.1967
SCpcc 0.3506 0.5085 0.1805
EASC 0.3555 0.5104 0.1861
AASC 0.3695 0.5213 0.2096
After normalizing the vectors to a unit length, we used the
following four metrics to calculate the pairwise distances
between two documents: Euclidean distance, Cosine sim-
ilarity, Jaccard coefficient and Pearson correlation coeffi-
cient, which measure distance between feature vectors of
the i-th and j-th documents
−→
tdi and
−→
tdj in different ways.
We denote as SCed, SCcs, SCjc and SCpcc the spectral
clustering with these four affinity matrices, respectively. Ta-
bles 7 and 8 show the AMI, NMI and ARI for 20 News-
groups and Reuters-21578, respectively. Note that doc-
uments are represented with the bag-of-word model and
these four affinity metrics only define different ways to mea-
sure distances. Thus, they have similar clustering capabil-
ity. Nevertheless, AASC is still able to assign the weights
appropriately to improve the clustering performance.
6. Conclusions
We have extended the spectral clustering algorithm to
the setting where there are multiple affinities available. Our
method can explore strengths of different features automat-
ically and weight them properly. Experiments show that it
effectively incorporates multiple affinities and yields bet-
ter performance compared to spectral clustering with only a
single affinity or naive feature fusion strategies. In addition,
it outperforms the previous method such as [6], and does
not suffer from the out-of-sample problem. Furthermore, it
is easy to implement, since only the computation of eigen-
vectors and 1-D search are involved. These characteristics
make it useful for real-world applications. In the future, we
will work on strategies for choosing the basis kernels to cal-
culate pairwise distances.
按：本計畫共出國開會兩次。以下此份 ICPR2010 出國報告早已於
過去本三年期計畫的期中報告時繳交上傳。但發現於國科會網站繳
交最後期末整合報告時，此檔案並不會隨之整合。為完整起見，故
將此報告亦彙整於檔案內。 
 
 2 
今年度的會議總共有 2140篇投稿論文，共計錄取 1147篇論文，其中口頭報告論文 
(oral papers) 有 385篇，壁報論文 (posters) 有 762篇。口頭報告論文錄取率約
18％左右，而整體的論文錄取率在 54％左右。本人在此會議共計發表兩篇論文，皆
為口頭報告論文。除了論文發表外，本人也應邀擔任其中一場 poster session 的 
session chair，協助大會進行 no-show 統計等工作。 
 
本人發表的第一篇論文 “Identifying Gender from Unaligned Facial Images by Set 
Classification” 是有關於性別辨識的研究課題。由於這類的辨認問題常要仰賴人臉偵
測及眼睛對位，倘若偵測或對位不甚準確，則在實際情況中辨認率會受到影響。在
報告中，首先就過去性別辨識的方法作一回顧，並解釋對位不準確所引發的問題。
接著提出我們的作法，藉由將偵測區塊附近重新抽樣成多張區塊的方式，透過影像
集合間的比對，能降低對位不準的影響，達到更高的準確率。報告後有不錯迴響。
其中 Univ. Washington 的 Linda Shapiro教授詢問人的年紀是否對於性別辨識有
影響，本人回答並提出看法：會有影響，比如年紀小時（例如幼童）其性別很難區
分，因此年紀的確會有影響，而整合此兩者的辨認應該是未來有趣的課題。 
 
本人發表的第二篇論文 “A Ranking Approach for Human Ages Estimation Based on 
Face Images” 為利用 learning to rank 的方式來進行年紀估測的論文。在同場次
還有另一篇論文同時也採用 learning to rank 進行年紀估測，但可惜的是其作者
缺席並未到場報告。這兩篇論文也同時是此領域中第一個採用 learning to rank 作
法的論文。在報告中，本人首先針對年紀分類的特性進行分析：由於年紀類別是一
個有順序的數列，而順序之間的絕對大小並不一定反映人臉外觀的變化，因此使用
相對大小的類別(即序位)可能比實際的值更為合理；也因而利用 learning to rank 
的方法可能比 multi-class 或 regression 的方法效果來得好。我們採用 
multiple parallel-hyperplane 的 ranking 方法，並將其與 multi-class 及 
regression 相比較，發現在同樣的特徵(active appearance model)下，
learning-to-rank 的方式的確具有比較高的準確率。由於此想法較不尋常，當報告
完畢時，有聽眾詢及所採用的 learning-to-rank 方法，本人作更詳細的解釋以助
於其瞭解。 
 
在本人擔任的 poster session chair 的場次中，主辦單位要求需巡察是否有未出
現的作者。為求慎重，並避免有人因為暫時離開而被紀錄，本人特地環視了三次以
上，確定是否有未張貼的海報，以及張貼後報告者是否在場。最後將紀錄上交給 PC 
chair。倘若投稿並獲發表但未出現報告的話，一定要有合理原因並事先請假，否則
將留下 no-show 紀錄。 
 
二、與會心得 
大會的專題演講相當精采。其中 MIT 的 Antonio Torralba 教授在大量 web 蒐集之
影像資料上的分析有相當的成果與經驗。他的團隊建立了許多大型的資料庫，可提
供相關研究工作者作為 benchmark。而目前在 web-scale 的大型資料上進行開發與
Identifying Gender from Unaligned Facial Images by Set Classification
Wen-Sheng Chu†, Chun-Rong Huang† and Chu-Song Chen†‡¶
†Institute of Information Science, Academia Sinica, Taipei, Taiwan
‡Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan
¶Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan.
{wschu, nckuos, song}@iis.sinica.edu.tw
Abstract—Rough face alignments lead to suboptimal perfor-
mance of face identification systems. In this study, we present
a novel approach for identifying genders from facial images
without proper face alignments. Instead of using only one input
for test, we generate an image set by randomly cropping out a
set of image patches from a neighborhood of the face detection
region. Each image set is represented as a subspace and
compared with other image sets by measuring the canonical
correlation between two associated subspaces. By finding an op-
timal discriminative transformation for all training subspaces,
the proposed approach with unaligned facial images is shown
to outperform the state-of-the-art methods with face alignment.
Keywords-gender identification; set classification; face align-
ment; subspace learning; discriminative analysis
I. INTRODUCTION
Gender identification, i.e., inferring female or male from
facial images, is a useful preprocessing step for face ap-
plications, such as facial expression recognition and age
estimation. Previous studies on gender identification have
relied on careful alignment of a facial image into a standard
template. The studies in [1], [2] conduct experiments on
many combination of the state-of-the-art face alignment and
pattern classification methods for gender identification. The
authors found that the classification rates increase with the
accuracy of face alignment because such alignment could
reduce the variability during building the model in the
training phase.
Since face detection can provide a rough face region for
each subject, most works of gender identification regard
the detected image patches as desired facial images. This
approach, however, suffers from the problem that the detec-
tor could not specify a robust result of face location. The
performance of gender identification, therefore, is somewhat
limited to the accuracy of face alignment. In practice,
false recognition may occur frequently because of variations
of human poses or difficult alignments. Two widely used
methods for aligning faces are affine warping for geometric
shape alignment [3], and a statistical model built for facial
shape and appearance [4]. Nevertheless, these alignment
processes require feature detection or manual efforts of
labeling specific facial features, e.g. eyes and mouth for
affine warping or plenty facial landmark points for the AAM
method [4]. When applied to a new test face set under
different variation settings, such as illumination or pose, the
face model must be rebuilt from a newly collected training
set to identify the variation. Moreover, rough alignment of
a single input is likely to downgrade the results.
In this paper, we aim to discover the gender correlations
from unaligned facial images to avoid alignment processes.
We generate a facial image set by randomly cropping
facial image patches around the detected face position of
a single image. Rather than using the scattered distribution
of these unaligned facial images, a subspace is constructed
for each image set to extract the joint information from
these randomly cropped facial patches. Because the facial
image set contains more facial appearance of the same
subject, a subspace is capable of representing more integral
facial information than a single input. Thus, we employ sets
of multiple unaligned facial images and represent them as
subspaces to achieve better results for gender identification.
After each facial image set is represented as a subspace
to express non-aligned facial variations of the same sub-
ject, we measure the similarity between two subspaces by
canonical correlation, which has been successfully applied
to several image-set-based applications such as object and
face recognition [5], [6], [7], [8]. It is a notable property that
the intersection between two subspaces of image sets are less
sensitive to variations compared to that between a vector and
a subspace [7]. Namely, a subspace constructed by unaligned
facial images is a considerable representation to reduce the
variation caused by face alignment. In addition, we find an
optimal gender discriminative transformation for all training
subspaces by exploiting the method of discriminant-analysis
of canonical correlation (DCC) [6]. The transformation
maximizes the correlation within subspaces of the same gen-
der and simultaneously minimizes the correlation between
subspaces of different genders. Finally, our approach is
demonstrated to perform better gender identification results
without face alignment over the approaches that assume the
face is well aligned.
II. UNALIGNED FACIAL IMAGE SETS
Due to the fact that face detection as well as face align-
ment algorithms does not align a face perfectly, we intend
to discover the gender correlations among sets of unaligned
facial images. To capture more facial variations for each
2010 International Conference on Pattern Recognition
1051-4651/10 $26.00 © 2010 IEEE
DOI 10.1109/ICPR.2010.646
26284036
(a) (b)
Figure 2. Example facial patches of female (top two rows) and male (bottom two rows) from (a) the FERET database and (b) the MORPH
database, where the later database can be regarded as a more challenging one because of more variation in aging and ethnicity.
the FERET database [13] and the MORPH database [14],
were used for the experiments. While FERET is one of the
mostly used database with good quality facial images for
gender identification, MORPH is a notable illustration of
much larger number of facial images with more challenging
variations in terms of aging and ethnicity.
In order to compare our results with those reported in [1],
we followed their experimental setups as follows. We used
fa- and fb-subsets from the FERET database by removing
duplicate images of the same subject; at the end there are 450
facial images for both females and males. For the MORPH
database, which is not used in [1], we collect 8,033 female
and 47,810 male images; there are totally 418 subjects of
age ranging from 18-69 and of races of Caucasian, African-
American and Asian. To the best of our knowledge, we are
the first tackling the gender identification problem over such
variations with wide range of age and races. Example images
from both databases are shown in Figure 2.
We collected 75 facial image patches as the image set
for each subject. A subspace is constructed for each subject
using Eq. (1) with 98 percentage of data energy preserved.
Being proved a stable method against the dimensionality
of transformation T [6], the dimension is fixed at 160 for
the following experiments. The solution of T was ensured
nonsingular by computing the pseudo-inverse of the within-
gender scatter matrix in Eq. (6).
We compared the performance of the proposed subspace-
based approach to discriminative sample-based methods, i.e.,
k-Nearest Neighbor (kNN) of images transformed by “PCA
plus LDA” [11]. The number of nearest neighbors is set to
1 and 10. Mutual subspace method (MSM) [8], a simpler
accession of canonical correlations, was also implemented
for verifying the discriminative power of the transformation
T. Our performance was reported by classification accuracy
versus the number of training subspaces. Note that each
subspace represents a subject; the same number of female
and male subjects for both training and test were chosen
from different image sets.
As shown in Figure 3, it is easier to obtain a better result
for the FERET than the MORPH database, because the
facial images are frontal and easier to classify even though
alignment processes are ignored. The accuracy increases
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
180 202 225 248 270 315 360
Ac
cu
ra
cy
No. Training Subspaces
DCC
MSM
1-NN PCA+LDA
10-NN PCA+LDA
(a)
0.7
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
0.88
0.9
100 200 300 400 500 600 700
Ac
cu
ra
cy
No. Training Subspaces
DCC
MSM
1-NN PCA+LDA
10-NN PCA+LDA
(b)
Figure 3. Classification accuracy between different set classication
methods of (a) the FERET database and (b) the MORPH database
over different numbers of training subspaces.
with the number of training subspaces in both databases
since the larger number of training subspaces are selected,
the more discriminatory gender information from different
subjects can be learned. It can be seen from Figure 3 (a)
that both subspace-based methods, DCC and MSM, out-
perform sampled-based methods. Although the input facial
images are not aligned in advance, subspaces are capable
of providing more general information of a subject than a
single image regardless of alignment processes. In Figure 3
(b), however, the performance of MSM is inferior to 10-
NN PCA+LDA. Even 1-NN PCA+LDA performs better
2630428
A RANKING APPROACH FOR HUMAN AGE ESTIMATION BASED ON FACE
IMAGES
Kuang-Yu Chang 1,3, Chu-Song Chen 1,2,4, and Yi-Ping Hung 1,3,4
1 Institute of Information Science, Academia Sinica, Taipei, Taiwan.
2 Research center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan.
3 Dept. of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan.
4 Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan.
{kuangyu, song}@iis.sinica.edu.tw, hung@csie.ntu.edu.tw
Abstract—In our daily life, it is much easier to distinguish
which person is elder between two persons than how old a
person is. When inferring a person’s age, we may compare his
or her face with many people whose ages are known, resulting
in a series of comparative results, and then we conjecture
the age based on the comparisons. This process involves nu-
merous pairwise preferences information obtained by a series
of queries, where each query compares the target person’s
face to those faces in a database. In this paper, we propose a
ranking-based framework consisting of a set of binary queries.
Each query collects a binary-classification-based comparison
result. All the query results are then fused to predict the
age. Experimental results show that our approach performs
better than traditional multi-class-based and regression-based
approaches for age estimation.
Keywords-Age estimation; ranking; binary classification; face
recognition.
I. INTRODUCTION
Inference of human ages has become an important topic in
recent studies. To build an age estimator, many approaches
formulate the training problem as a regression problem or a
multi-class classification problem. Given a set of training
examples tpxi, yiq|i  1, . . . , nu, where xi is the i-th
training person (face image) and yi is the age of this
person (in most cases is an integer), the regression approach
basically learns a function f that can fit the mapping from
xi to yi with appropriate regularization, and the multi-class
approach simply treats yi as discrete labels and learns a
classifier for inferring the age.
For age estimation problem, multi-class approaches could
overlook some essentially useful information because they
assume that the labels are independent or have no inherent
relationship to each other. However, the labels (i.e., ages)
of this problem indeed have a strong interrelationship since
they form a well-ordered set. For example, if a face image
is 10 years old, this face is also near to the label of 9 or
11 years old, but a multi-class approach cannot reflect this
property because their labels are basically irrelevant to each
other.
By contrast, the regression approaches take advantage of
the ordering information for estimation, and are expected to
have better performance than classification to this problem.
Typical nonlinear regression approaches such as Gaussian
Process (GP) or Support Vector Regression (SVR) have been
used to solve the age estimation problem. However, human
face may be aging in different forms, shape and texture
variations during different ages [1]. This property makes the
random process formed by the aging patterns non-stationary
in general, and so the kernel functions used to measure the
pair-wise similarities between ages could be shift- or time-
varying. Nevertheless, learning non-stationary kernels for a
regression problem is usually difficult because it will easily
cause over-fitting in the learning process.
Intuitively speaking, human aging processes show diver-
sity in different age ranges. This means that the difference
between ages, say, 52 and 51 years old, is not necessarily
equivalent to that between, say, 22 and 21 years old, due
to the fact that facial aging process is a non-stationary
procedure. Hence, the reliable information we can use would
be the relative order among the age labels, instead of their
exact values.
The above observations inspire us to propose a ranking-
based approach for age estimation. Note that the labels
considered in age estimation are usually dense enough when
we need to estimate an integer by the relative order. In our
approach, the age labels are only treated as a well-ordered
set regardless their exact values. This approach learns the
rank of ages by using preferences from binary decisions via
a series of queries. Each query reduces the age estimation
problem to an easier one of distinguishing which object is
ranked higher. According to this kind of preferences, the
rank of each object can be inferred.
The rest of this paper is organized as follows. First, we
review the relative works in Section 2. The framework of
ranking-based age estimation is introduced in Section 3.
Experimental results are presented in Section 4. Section 5
gives conclusions.
II. RELATED WORK
Previous researches in human age estimation via face
images can be roughly divided into two categories, the multi-
2010 International Conference on Pattern Recognition
1051-4651/10 $26.00 © 2010 IEEE
DOI 10.1109/ICPR.2010.829
338440096
(a)
(b)
Figure 1. Example images in (a) MORPH Album 2 database and (b) FG-NET aging database.
Age Range MORPH (%) FG-NET (%)
0-9 0 37.03
10-19 8.94 33.83
20-29 26.04 14.37
30-39 32.16 7.88
40-49 24.58 4.59
50-59 7.37 1.50
60-69 0.82 0.80
70-77 0.09 0
Table I
AGE RANGE DISTRIBUTIONS IN MORPH AND FG-NET
DATABASE
feature extraction method [3], [7]. For both databases, AAMs
extracts the number of features that preserves 95 percent of
the variability.
SVM and SVR are probably the most popular methods
for classification and regression, respectively. Many age
estimation approaches [6], [7], [12] suggested SVM and
SVR in their frameworks. In the experiments, the methods
that we compare include SVR, SVM, k-Nearest Neighbors
(kNN), Back Propagation neural network (BP), and Binary
decision Tree (BT). The parametric configurations of the
above methods are as follows. In SVR and SVM learning,
RBF kernel function is used and related parameters, C and
γ, are selected using 5-fold cross validation. LIBSVM [13]
is used for SVR and SVM evaluation. The k in kNN is
30. BP has a single layer with 100 neurons and the number
of output neurons is the same as that of the classes. We
use 80% of images for training and select the parameters
via cross validation. The rest of images (20%) are used for
testing over 30 trails of random splits for the final results.
Two popular measures are used to evaluate the perfor-
mance of age estimation, Mean Absolute Error (MAE) and
Cumulative Score. MAE 
°N
i1 |li  li|{N , where li
is the predicted age, li is the ground truth, and N is the
MORPH Rank SVR SVM kNN BP BT
MAE 6.49 6.99 7.55 9.39 10.03 11.97
std 0.17 0.07 0.08 0.28 1.00 0.24
Table II
MEAN ABSOLUTE ERROR IN MORPH DATABASE
FG-NET Rank SVR SVM kNN BP BT
MAE 5.79 6.05 6.72 8.34 10.25 15.73
std 0.61 0.50 0.83 0.69 1.71 1.10
Table III
MEAN ABSOLUTE ERROR IN FG-NET DATABASE
number of testing images. Cumulative score is defined as
CumulativeScorepLq  pNe L{Nq  100%, where Ne L
is the number of testing images with absolute error less than
the error level L.
Tables 2 and 3 show the experimental results by using
MAE and standard derivation. We find that SVR has better
performance than the multi-class classification methods such
as SVM, kNN, BP and BT. This is because regression has
the advantage of employing the relationship among labels.
Our proposed ordinal ranking age estimation framework
performs better than regression and achieves the lowest
MAEs of 6.49 and 5.79 years in MORPH and FG-NET
databases, respectively. Figure 2 and 3 show the results
of cumulative score at different error levels, which are
increasing with error level. When the cumulative score
is fixed, the smaller error level is the better. It can be
seen that the proposed ranking-based framework has the
highest accuracy compare to other algorithms at different
error levels. According to the experimental results, ordinal
ranking consistently outperforms all the other comparative
algorithms in both databases.
338640298
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/28
國科會補助計畫
計畫名稱: 以智慧型攝影機網路協助建立嬰幼兒之安全活動空間與回憶系統
計畫主持人: 陳祝嵩
計畫編號: 98-2221-E-001-012-MY3 學門領域: 圖形辨識
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
