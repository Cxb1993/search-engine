行政院國家科學委員會補助專題研究計畫 ■ 成 果 報 告   □期中進度報告 
 
非監督式中文寫作自動評閱系統之研究 
 
 
計畫類別：■ 個別型計畫  □ 整合型計畫 
計畫編號：  
執行期間： 2010 年 8 月 1 日至 2011  年 7 月 31 日 
 
計畫主持人：李嘉晃 
共同主持人： 
計畫參與人員： 劉建良、陳俊憲、郭宗勳、蔡岳洋 
 
 
成果報告類型(依經費核定清單規定繳交)：■精簡報告  □完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
■出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、列
管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年■二年後可公開查詢 
          
執行單位：國立交通大學資訊工程學系(所) 
中   華   民   國  100   年   11  月   1   日 
工評定分數來當訓練資 料，僅需要一定的同
主題文章數，便可藉由文章特徵的資訊、文章
間的相似度進行自動評分的，再利用前半段已
完成評分之文章集合，做高分與低分間文章特
徵 之比對，與一般文章寫作常注意事項的偵
測，最後綜合這些文章特徵給予可改進方向，
與應注意事項的評語回饋。 
 
四、想法與討論 
此章節中，將描述整個系統的架構與流
程，首先用ㄧ張系統主架構圖來了解系統整
個運作的流程，圖中各個部份的執行內容將
在後續幾小節中做詳細的介紹。 
 
(一)系統架構 
 
本系統裡面共包含４個部份： 
1. 前置處理：先將作文語料庫做一些規劃與 
處理，並取出一些評語系統所需的特徵，
讓後續系統的進行能更有效率。  
 
2. 評分系統：將測試文章與作文語料庫一起 
做為評分系統的輸入，放進評分系統得到
測試文章的級分評估。  
 
3. 評語系統：針對各評語特徵進行評估與比 
較，設定各特徵面向的評語結果。  
 
4. 評語整合彙出：將評分系統與評語系統的 
結果整合輸出。 
 
(二)前置處理 
    前置處理的程序主要包含以下兩個部份：
第一部份為作文語料庫的斷詞、詞性、結構
化，此處是將現有的語料庫的資訊重新匯整
成系統所需與可用的資訊，並將這些資訊使
用有效率的資料結構方式建構，讓系統在處
理後續資料時能更有效率。 
    第二部份為紀錄文章常用詞語組合，以
及高分文章常用名詞與頻率。 
 
●作文語料庫斷詞、詞性、結構化 
    文章中，最能代表一篇文章的字詞，就
是名詞(N)和動詞(Vt)，所以將作文 語料庫
內的名詞與動詞全部先取出來做為關鍵字詞
的基底，將每篇作文的文章都 做完處理後，
可得到 1 關鍵詞下接 1 填充詞上再接 1 
關鍵詞…等等的結構，做好 這樣的結構後，
在後續的運作上將更方便。如下的例子。 
 
原始文章：  
中午(N)   用餐(Vi)   的(T)   時候(N)   
都(ADV)   是(Vt)   叫(Vt)   別人  
(N)   打飯(N)  ，(COMMACATEGORY)   打
好(Vt)   之(T)   候(N)   之(T)   很多 
(DET)   男生(N)   在(P)   陽台(N)   上
(N)   吃飯(Vi)   聊天(Vi) …… 
 
結構化：  
中午(N) 時候(N) 別人(N) 打飯(N) 打好
(Vt) 男生(N) 陽台(N)……  
-------------------------------------- 
 
用餐(Vi)   的(T)  
都(ADV)   是(Vt)   叫(Vt)  
 
，(COMMACATEGORY)  
之(T)   候(N)   之(T)   很多(DET)  
在(P) 
 
●文章常用詞語組合、高分常用名詞 
    文章分數由一到六級分，評估發現三級
分的文章，詞語使用已達一般水準，故文章
常用詞語組合的語料庫採用的是三分以上的
文章。  
    首先利用[6]中所提及的評分系統，對整
份作文語料庫進行評閱，得到各文章的初閱
分數，使用這次評分的結果中三分以上的文
章，做為文章常用詞語組合的材料。常用詞
語組合分為兩個部份，一個是核心字所構成
的組合。有關核心字，將在後續章節介紹，
此處僅以作法講解；另一個則是所有提取出
語料庫來合併，對於投票演算法的運行，每
篇待測文章都會以同樣的作文語料庫給予評
分，因此在評分的標準上，可稱為是同一標
準的。 
 
(四)評語系統 
    評語系統，一共分成九個不同的面向，
來對待測文章進行評估，包含了情緒分析、
分段技巧、標點運用、佳句讚賞、錯別字…
等九項，將在後續小節逐一介紹。 
 
●情緒分析 
    由於一般中學作文多數為記敘文，在情
感流露上，可較為顯見。因此利用陳信希[11]
所得到的各情緒代表詞來做為評估基準。  
    以下表中各情緒代表詞為資料，統計待
測文章中使用的情緒詞總個數，再除 以待測
文章長度做正規化，若超過平均值，會給予
「情感豐富」之評語；而使用 較少或無使用
到情緒代表詞，會給予「情感內斂」之評語。 
 
 
●分段技巧 
    作文常見分段寫作技巧為起、承、轉、
合，四個部份。一般一個部份會分配一個段
落來負責敘述，也可能因版面問題而將承與
轉兩段合併。因此，在寫作分段技巧方面，
我們判定若文章段落少於三段，或大於五段，
則表示在分段技巧上，需多加強改進。  
    作文語料庫在建立時，已有特別處理過，
在文章的每個段落前標記有「**」。 所以以
此符號數量為基準，小於三，大於五，均判
定為分段技巧有待加強，會分 別給予「段落
過少」、「段落過多」的評語。 
 
●標點符號運用 
    此面向主要針對段落數為 1，而長度未
低於平均文章長度的一半的文章做回 饋，假
設為標點符號使用不當，造成應分段而未分
段的結果。  
 首先，利用新酷音輸入法的詞庫，統計
出常用詞語的使用頻率，可以得到如 下方格
式的資訊： 
 
 
 
    經觀察發現，設定門檻值 50000 可約略
區隔出常用詞語 stoplist。  
    接著，使用中研院平衡語料庫[14]，統
計句號前的常出現詞彙，輔以常用詞語 
stoplist，歸納出句號前常用詞彙集。 
    對於段落數為 1 的文章，系統會進一步
搜尋在文章中是否出現句號常用詞彙 集中
的詞語，除了給予「標點符號使用不佳」的
評語外，還加上建議可分段處的 位置，讓使
用者做為參考。 
 
●注音符號使用 
    經觀察「下課十分鐘」、「用餐時分」兩
份作文語料庫發現，仍有少部份學生 在寫文
章時，會使用注音符號來表示不會寫的中文
字。  
 
範例：  
有肉有采 我看了就想吃 可是要進去火ㄍㄨ
ㄛ 裡面 煮一煮才可以吃  
 
    在這一部份，系統也將給予提醒的回饋，
利用注音符號表，檢查待測文章中是否出現
注音符號，若有出現，則會給予「盡量減少
使用注音符號」的評語。 
 
●文章長度 
    常見中學作文長度為 600 字，在此，假
設三級分以上文章，乃因為內容已達一定 水
準，所以可以得到三級分以上的成績。因此，
以三級分以上的文章平均長度， 做為文章長
度是否足夠的判別條件，若待測文章長度小
於平均長度，則判定長度不足，系統會給予
 
第一次補充：  
    針對[觀點]一詞，在核心 bigram 中尋
找{[觀點] [A]}、{[B] [別有]}此兩 種組合
並記錄下 A、B 詞與其出現次數，最後挑選
最高出現次數的詞做為填充核 心字。接著處
理最後一個核心字[別有]，在核心 bigram 
中尋找{[別有] [C]}記 錄C詞及出現次數，
同樣取出最高次數的詞來填充。  
    假設結果為  [觀點]  [專題]  [別有]  
[餐廳]。  
    因為第一次結果未超過平均長度，所以
第二次補充即以第一次結果再做一次 核心
字填充處理，直到完成後結果大於平均長
度。  
 
範例二：  
原始核心字串：  
[溫暖] [做好] [東西] [時候] 
 
第一次補充：  
    分別針對[溫暖]，在核心 bigram 中查
詢{[溫暖] [A]}、{[B] [做好]}此兩種組  
合並記錄下 A、B 詞與其出現次數，挑選出
最高出現次數的詞做為填充核心字；接著針
對[做好]、[東西]做相同的處理；[時候]則
以最後核心字做另外的處理。  
    補充後結果為[溫暖]  [屬於] [做好] 
[範圍] [東西] [規定] [時候] [那裡]
此結果已超過平均長度，故不需再做第二次
的處理。 
    第二部份為對於已新增之核心字串，利
用前處理的一般常用詞組合(以下稱  
常用 bigram)擴充其前後關鍵字，形成新的
一段的關鍵字串列。經計算，平均關  
鍵字串的長度為 9，故針對每一個新增的字
串，最多將擴充到 9 個關鍵字。擴充  
方式如下：  
1.隨機決定核心字在新字串中的位置。  
2.以核心字為索引，查詢常用 bigram 中核 
  心字前方常出現詞彙，並記錄出現次數， 
  挑選最常出現的詞做為擴充關鍵字，若挑 
  選出來的詞與現有詞相同時，將不做 
  擴充；或常用 bigram 中無法找到擴充關
鍵字候選詞時亦然。  
3.以核心字為索引，查詢方式改為核心字後 
  方常出現詞彙，同樣的擴充方法將核心字 
  後方關鍵字串填充完。  
 
範例：  
新增核心字： [規定]  
 
    假設位置決定為 4，表示核心字前方需
擴充 3 個，後方 5 個關鍵字。先對前方做
擴充的動作，在常用 bigram 中查詢 {[A] 
[規定]}的出現次數，並把 A 詞彙記錄下來，
最後取出最常出現的詞，與[規定]比對是否
相同，不同方將詞加入[規定]前方。假定尋
找到的詞彙為[餐廳]，則現有字串更新為[餐
廳] [規定]。接著以[餐廳]為索引，在常用 
bigram 中查詢{[B] [餐廳]}的出現次數，以
此方法逐一擴充，直到挑出的詞與索引詞相
同，或是已達到欲擴充詞數為止。後方關鍵
字的擴充方式與前方相似，僅改變查詢的索
引詞組為{[索引字] [候選字]}。  
    此例結果可擴充為[東西] [覺得] [外面] 
[餐廳] [規定] [喜歡] [感覺]。  
    經過第二部份的擴充，可以得到完整的
關鍵字串列表，第三部份將由此結果中，挑
選出一段做為建議回饋的依據，來給予使用
者可補充內容的資訊。首先，會對每個新增
的關鍵字串，再做一次各關鍵字間的關係程
度計算，重新擷取出核心字，並以核心字在
各段落中與其他關鍵字關係程度的分數，做
為依據取出最高的一個，來做為建議方針。
並以該段落中所有的關鍵字做為 term 
vector 與三級分以上，未超過 100 個字的句
子做相似度計算，取出相似度最高的一句做為
提示句。相似度的計算方式採用 cosine 相似
度算法，範例如下： 
 
關鍵字串列： 
[天氣] [郊遊] [野餐] [傍晚]  
 
備選句：我們 昨天 傍晚 才 去 了 郊外 野
餐 ，現在 好 累。  
 
    備選句轉換成向量之後為〈0,0,1,1〉，
與本來的 term vector〈1,1,1,1〉做 cosine
運算。 
    利用 cos(A,B) = A‧B / |A||B|，得到
此備選句與關鍵字串列的相似度 為 2/2√2 
= 1/√2。 
 
●佳句讚賞 
    先將前置處理時紀錄的高分常用詞語設
4.實驗結果 
 
 
●評語整合系統 
1.實驗流程 
    從作文資料庫中選取三十篇含一到六級
分的文章，做為系統的輸入文章，利 用評語
整合系統得到各篇文章的評語回饋，並請六
位使用者幫忙評估系統評語回 饋的實用
性。 
 
2.評鑑方法 
針對評語通順程度、評語面向準確程度
及評語建議良好程度三個方面來做為評估的
基準，評語通順程度為評語回饋的語句通順
程度；評語面向準確程度為系統給予的面向
是否準確；評語建議良好程度為各面向評語
所給予的建議，是否有切中要點。程度的基
準分為五個等級，分別以 1~5 來表示，1 為
最低，最不準確；5 為最高。最後將六人在
各面向評估大於 3 與大於 4 的比例計算出
平均值。 
 
3.實驗結果 
 
 
 
由結果可以看出，在 3 以上的比例，已
有九成以上的滿意度；在 4 以上，在 準確
度與良好度方面仍可以達到約六成的滿意
度。 
 
五、計畫成果與自評 
●學術貢獻 
    本計畫從 2008 年到 2011 年總計發表六
篇期刊論文於 3 個國際期刊上，分別是 IEEE 
Intelligent Systems 、 Expert Systems with 
Applications 和 IEEE Trans. On SMC-PartC 
[15] [16][17][18][19][20]。另外還有 3 篇國際
會議論文[21][22][23]。 
 
●研究總結 
    在本計畫中，我們所提出的中文作文評分
及評語系統，有別於以往的評分系統，本系統
除給予評估之後的分數外，更針對文章各方面
的缺失及優點，給予建議與回饋，讓使用者可
藉由此回饋資訊得知在寫作上有哪些需要改
進的部份，以 提升寫作能力。 
 
●未來工作 
    本計畫目前雖然已針對作文寫作上各方
面能給予建議與改進的回饋，但系統尚未包含
中文作文評分的全部標準，仍有未能包刮到的
部份，如對文章結構上的評論、文意上的探討
等，因此希望未來能加入這些方面的考量，使
系統更能符合人工批閱的模式。 
 
六、參考文獻 
[1] Jill Burstein. “The E-rater Scoring 
Engine: Automated Essay Scoring With 
Natural Language Processing.＂Automated 
Essay Scoring: A Cross-Disciplinary 
Perspective. pp. 113-121,2003. 
[2] 蔡沛言，「自動建構中文作文評分系統：
產生、篩選與評估」，國立交通大學，碩士
論文.( 2005) 
[3] 林信宏，「基於貝氏機器學習法之中文
自動作文評分系統」，國立交通大學，碩士
論文.( 2006) 
[4] 粘志鵬，「基於支援向量機之中文自動
作文評分系統」，國立交通大學，碩士論
文.( 2006) 
[5] 張佑銘，「中文自動作文修辭評分系統
設計」，國立交通大學，碩士論文.( 2005) 
[6] 陳彥宇，「非監督式中文寫作自動評閱
系統」，國立交通大學，博士論文.(2007) 
[7] 張 道 行 ， 「 Conceptualization 
2011 Second ETP/IITA Conference on 
Telecommunication and Information (TEIN 2011)  
研討會 
出國報告書                      
姓名：劉建良 
會議舉辦地點：泰國普吉島 
會議日期：April 3-4, 2011 
 
 
 
 
Database-driven Framework for RDF Statements
Generation
Chien-Liang Liu
Department of Computer Science
National Chiao Tung University
1001 University Road, Hsinchu, Taiwan 300, ROC
Email: clliu@mail.nctu.edu.tw
Chia-Hoang Lee
Department of Computer Science
National Chiao Tung University
1001 University Road, Hsinchu, Taiwan 300, ROC
Email: chl@cs.nctu.edu.tw
Abstract—The web has become the biggest database in the
world. However, most web content today is designed for humans
to read, not for computer programs to manipulate meaningfully.
The first step to realize semantic web is putting data on the
Web in a form that machines can naturally understand. The
main objective of the work reported in this paper is to provide
a database-driven framework to automatically extract relational
database schema and transform the data into RDF statements
representation. Moreover, we employ the Wikipedia to obtain
meta-information about the concept. Then, the NGD score is
used to measure the relatedness of the terms and the hyponym
patterns are further used to discover “is a” relationship. As our
experience with a first simple prototype has shown that this
approach could be applied to the environment where the web
page content is stored in the database.
I. INTRODUCTION
The semantic web is an evolving extension of the World
Wide Web in which the semantics of information and services
on the Web is defined, making it possible for the web to
understand and satisfy the requests of people and machines
to use the web content [1]. The first step to realize semantic
web is putting data on the Web in a form that machines
can naturally understand, or converting it to that form [2].
In practice, transform existing web pages into semantic web
content manually is impractical and automatic transformation
tools remains largely undeveloped in the past couple of years.
Wikipedia, which is currently the largest knowledge repos-
itory on the Web, provides entries on a vast number of named
entities and very specialized concepts. Cucerzan [3] performed
the recognition and semantic disambiguation of named entities
based on information extracted from the Wikipedia and web
search results. Bunescu and Pasca [4] employed several of
the disambiguation resources and built a contextarticle cosine
similarity model. In addition to the Wikipedia, search engines
have become essential tools in the Internet. Some researchers
started to employ search engine result hits to measure the
semantic relatedness of terms, a measure of how related two or
more concepts are. The PMI (Pointwise Mutual Information)
which is proposed by Turney [5] estimates the relatedness by
issuing queries to a search engine (AltaVista) and noting the
number of hits (matching documents). The NGD (Normalized
Google Distance) which is proposed by Cilibrasi and Vi-
tanyi [6] measure the relatedness based on NGD formula. The
main contribution of this paper is that we propose a framework
combining database and Internet services to transform existing
web pages that are database-driven into semantic web content.
II. PRELIMINARY
Resource Description Framework (RDF), which is proposed
by W3C, adopts XML [7] as interchange format and provides
interoperability between applications that exchange machine-
understandable information on the Web. RDF uses simple
graph model to represent resources, properties and property
values. The resource described by RDF is anything that could
be named via a Uniform Resource Identifier (URI). In essence,
RDF could provide a more precise description about the
resource available on the Web and that could elevate the status
of the web from machine-readable to machine-understandable.
Currently, most websites employ database as their stor-
age system to store web contnet. Practically, the structural
view of the web application is mapped to the schema of
a storage repository [8], so database schema could always
represent the skeleton of the web applications. Hence, the
above observation offers us the motivation to transform the
existing database into corresponding RDF statements. Many
researchers have been attempted to transform database data
into RDF representation [9][10][11]. Declarative languages
are used by Bizer [9] and Barrasa et al. [10] for expressing
mappings between ontologies and databases. D2R MAP [9] is
a declarative, XML-based language to describe such mappings.
In practice, the database schema should reflect application
requirement. For example, Fig. 1 shows a typical database
schema in an online shopping web site. The table fields such as
“ProductName” and “Price” indicate that each product should
have these properties. The primary key “ProductID” is used for
identification of products and its value will not be duplicated.
As for foreign key, it could not only represent the reference
constraint, but also the relationship between tables. Generally
speaking, there are “1:1”, “1:N” and “M:N” relationships be-
tween tables. In practice, “1:1” relationship could combine two
tables into one table when performing database normalization.
The other two relationships are described in the following
sections.
“ProductOrder” table could be regarded as an aggregate of
corresponding products for each order.
RDF containers, which could be used to indicate that the
value of a property is a group of things, is employed in this
paper to represent the aggregate association. Similarly, two
semantic information could be discovered from “M:N” rela-
tionship. Unlike “1:N” relationship, no table dependence rela-
tionship exists explicitly between the tables that have “M:N”
relationship, so manual judgment is required to determine
the dependence relationship. In this paper, the transformation
process pre-assigns predicates to the results and if they are
incorrect, the predicates for “M:N” relationship should be
exchanged manually. The transformation process is described
as follows:
1) Discover the foreign keys of intermediate table and
assume that the foreign keys are FK1 and FK2.
2) Use FK1’s value as the basis to group FK2’s value and
transform the grouping list into RDF:Bag representation.
3) Use FK1’s value and the primary key name of its
referenced table to compose RDF resource.
4) Link the resource created in Step (3) and the RDF:Bag
container created in Step (2) by using predicate “Has”.
5) Use FK2’s value as the basis to group FK1’s value and
transform the grouping list into RDF:Bag representation.
6) Use FK2’s value and the primary key name of its
referenced table to compose RDF resource.
7) Link the resource created in Step (6) and RDF:Bag con-
tainer created in Step (5) by using predicate “BelongTo”.
TABLE IV
ORDER TABLE SAMPLE DATA
OrderID OrderDate
100 2008-01-01
101 2008-01-02
102 2008-01-03
TABLE V
PRODUCTORDER TABLE SAMPLE DATA
ProductOrderID ProductID OrderID
1 1 100
2 2 100
3 1 101
4 3 101
IV. SEMANTIC RELATIONSHIP DISCOVERY
As mentioned above, the Wikipedia contains a vast amount
of information, which is 100% edited by users. Hence, the
Wikipedia content structure is not identical. Since May 2004
Wikipedia allows for structured access by means of categories.
Each concept in Wikipedia can belong to one or more cate-
gories and these categories form a graph which can be taken
to represent a conceptual network with unspecified semantic
relations [14].
Figure 2 shows the semantic relationship discovery flow.
The input is the entity name and the entity name will be sent
ŊůűŶŵġŏŢŮŦ
ŘŪŬŪűŦťŪŢ
ńŢŵŦŨŰųźġŊůŧŰųŮŢŵŪŰů
ġŔŦŢųŤũġņůŨŪůŦ
ŒŶŦųź ŘŪŬŪűŢŨŦ
ŒŶŦųźġŶŴŪůŨġŊůűŶŵġŏŢŮŦġŢůťġńŢŵŦŨŰųź
ȾŪŴŢȿġųŦŭŢŵŪŰůŴũŪűġ
ťŦŵŦŤŵŪŰů
ȾųŦŭŢŵŦťȿġ
ųŦŭŢŵŪŰůŴũŪűġťŦŵŦŤŵŪŰů
ŔŦŮŢůŵŪŤġœŦŭŢŵŪŰůŴũŪűġŊťŦůŵŪŧŪŤŢŵŪŰů
Fig. 2. Semantic Relationship Discovery Flow
to the Wikipedia to obtain the page whose title containing
the entity name. As mentioned above, the category section
of the Wikipedia page contains category information and the
categories can be regarded as the meta-information about the
concept. Similarly, everyone can assign a category to the con-
cept, so the relationship between the category and the concept
may be tiny. Take “iPod” as an example, the category section
of “iPod” includes “IPod”, “2001 Introductions”, “Portable
Media Players”, “Industrial Designs”, “IPhone OS Software”
and “ITunes”. Obviously, not all categories provide clear
information about the relationship. For example, “iPod” is
launched on October 23, 2001, so some people say that “iPod”
concept belongs to “2001 Introductions” category. However, it
is not easy for people to infer the relationship directly without
iPod launching date information. On the other hand, “iPod” is
clearly a “Portable Media Players”, so their relationship can be
represented using “is a” relationship. Therefore, a mechanism
which can determine the strength of the relationship is needed
to discover the semantic relatedness of terms.
We employ the Internet search engine to determine the
strength of the relationship between concept and category. The
mechanism includes two phases. In the first phase, we employ
Normalized Google Distance (NGD) [6] to obtain the semantic
relatedness scores of terms. One of the advantages of using
search engine to compute the semantic relatedness of terms
is that it can be applied to any language and domain, since
search engines index a large amount of web pages and the
web pages cover different languages and domains. Equation 1
shows the NGD formula:
NGD(x, y) =
max{logf(x), logf(y)} − logf(x, y)
logM −min{logf(x), log(y)} (1)
where M is the total number of web pages searched by
Google; f(x) and f(y) are the number of hits for search
terms x and y, respectively; and f(x, y) is the number of
web pages on which both x and y occur.
If both terms always occur together, their NGD is zero and it
2) Link RDF statements based on the relationship among
tuples.
Besides individual tuple transformation, the relationship be-
tween tuples are determined from the foreign key information.
In essence, the transformation is based on the relationship type
and the following heuristics are used in this paper.
1) If there is only one foreign key FK in table Tc and
FK references to PK in table TP , “1:N” relationship
transformation would be performed.
2) If there are two foreign keys in the table Ti and they ref-
erence to two tables, “M:N” relationship transformation
would be performed.
C. RDF Statements Generation
In RDF statements generation phase, Jena [16] is employed
to generate the final RDF statements representation. Jena is
a Java framework for building Semantic Web applications. It
provides a programmatic environment for RDF, RDFS and
OWL. Since Jena and JDBC are both JAVA framework, the
integration is expected to be seamless. Moreover, the whole
system could be applied to the environments where JAVA
Virtual Machine (JVM) and the database JDBC driver are
available.
D. Semantic Relationship Discovery
As mentioned above, the system relies on the Internet
service for the semantic relationship discovery. Thus, if the
administrator wants to extend the semantic information of
products, the system can use the product name as query term
and issue the query term to the Wikipedia. If the product name
can be found in the Wikipedia title page, the system can obtain
its category information. Then, the NGD score is employed in
measuring the semantic relatedness and the one whose NGD
score is below the threshold value will contain “related to”
relationship. Meanwhile, the system will discard the categories
whose NGD scores exceed the threshold value.
Furthermore, the system issues the query to the Altavista
search engine with “NEAR” operation to obtain the description
of the links. Then, the system applies hyponym patterns to
the description to verify whether hyponym relationship exists.
If the number of results matching the patterns exceeds the
threshold value, the system will infer that there should have
“is a” relationship between the product name and its category.
After the discovery process, the administrators can verify the
results.
VI. CONCLUSION
In this paper, a database-driven framework is proposed to
transform database data into RDF statements representation.
The framework includes database transformation module and
semantic relationship discovery module. In database transfor-
mation module, the system automatically extracts relational
database schema and transforms the data into RDF statements
based on schema information. In semantic relationship discov-
ery module, we employ the Wikipedia to discover the semantic
information about the entities. Meanwhile, we further use
search engine and hyponym patterns to discover the semantic
relationships, which are “related to” and “is a” relationships.
The framework could be applied to the environment where
the web page content is stored in the database and it can be
applied to different languages with a little modification on the
hyponym patterns.
ACKNOWLEDGMENT
This work was supported in part by the National Science
Council under the Grants NSC-99-2221-E-009-150 and NSC-
099-2811-E-009-041.
REFERENCES
[1] T. Berners-Lee, J. Hendler, and O. Lassila, “The semantic web,” Scien-
tific American, vol. 284, no. 5, pp. 34–43, 2001.
[2] T. Berners-Lee, Weaving the Web : The Original Design and Ultimate
Destiny of the World Wide Web by its Inventor. HarperOne, 1999.
[3] S. Cucerzan, “Large-scale named entity disambiguation based on
Wikipedia data,” in Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL). Prague, Czech Repub-
lic: Association for Computational Linguistics, June 2007, pp. 708–716.
[Online]. Available: http://www.aclweb.org/anthology/D/D07/D07-1074
[4] R. C. Bunescu and M. Pasca, “Using encyclopedic knowledge for named
entity disambiguation,” in EACL. The Association for Computer Lin-
guistics, 2006. [Online]. Available: http://acl.ldc.upenn.edu/E/E06/E06-
1002.pdf
[5] P. D. Turney, “Thumbs up or thumbs down?: semantic orientation
applied to unsupervised classification of reviews,” in ACL ’02: Pro-
ceedings of the 40th Annual Meeting on Association for Computational
Linguistics, 2002, pp. 417–424.
[6] R. L. Cilibrasi and P. M. B. Vitanyi, “The google similarity distance,”
IEEE Trans. on Knowl. and Data Eng., vol. 19, no. 3, pp. 370–383,
2007.
[7] W3C, “Extensible markup language (xml),” March 2004. [Online].
Available: http://www.w3.org/XML/
[8] P. Fraternali and P. D. Milano, “Tools and approaches for developing
data-intensive web applications: A survey,” ACM Computing Surveys,
vol. 31, pp. 227–263, 1999.
[9] C. Bizer, “D2r map a database to rdf mapping language,” in Proceedings
of the 12th International World Wide Web Conference, WWW 2003,
2003.
[10] J. Barrasa, scar Corcho, and A. Gmez-Prez, “R2o, an extensible and
semantically based database-to-ontology mapping language,” in Second
Workshop on Semantic Web and Databases (SWDB2004), Toronto,
Canada, 2004.
[11] L. Stojanovic, N. Stojanovic, and R. Volz, “Migrating data-intensive web
sites into the semantic web,” in ACM Symposium on Applied Computing
(SAC 2002), Madrid, Spain, March 2002.
[12] W3C, “Xml schema,” 2010. [Online]. Available:
http://www.w3.org/XML/Schema
[13] P. P.-S. Chen, “The entity-relationship model-toward a unified view of
data,” ACM Trans. Database Syst., vol. 1, no. 1, pp. 9–36, 1976.
[14] M. Strube and S. P. Ponzetto, “Wikirelate! computing semantic relat-
edness using wikipedia,” in AAAI’06: proceedings of the 21st national
conference on Artificial intelligence. AAAI Press, 2006, pp. 1419–1424.
[15] R. C. Wang and W. W. Cohen, “Automatic set instance extraction
using the web,” in Proceedings of Joint Conference of the Association
for Computational Linguistics and the International Joint Conference
on Natural Language Processing of the Asian Federation of Natural
Language Processing (ACL-IJCNLP 2009), 2009, pp. 441–449.
[16] Jena, “Jena a semantic web framework for java,” 2009. [Online].
Available: http://jena.sourceforge.net/
99 年度專題研究計畫研究成果彙整表 
計畫主持人：李嘉晃 計畫編號：99-2221-E-009-150- 
計畫名稱：非監督式中文寫作自動評閱系統之研究與設計 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 4 4 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 3 100%  
博士生 0 0 100%  
博士後研究員 1 1 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
