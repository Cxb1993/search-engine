2 
 
position of the robot and compasses are often used to 
detect the orientations of the robot. Based on the sensor 
information, motion control is done and then the 
localization of the robot are estimated [6], [7]. Despite 
some limitations of an encoder, most researchers agree 
that an encoder is a vital part of the robot’s navigation 
system and the tasks will be simplified if the encoder 
accuracy is improved. Besides, the additional sensor, a 
camera, allows a robot to perform a variety of tasks 
autonomously. The use of computer vision for 
localization has been investigated for several decades. 
The camera has not been at the center of robot 
localization while most of researchers have more 
attention to other sensors such as laser range-finders and 
sonar. However, it is surprising that vision is still an 
attractive choice for sensors because cameras are compact, 
cheaper, well-understood and ubiquitous. In this study, 
the algorithm is achieved by combining different types of 
sensors including optical wheel encoders, an electrical 
compass and a single camera.  
Mobile vehicles such as robots, UGVs and UAVs are 
becoming fully intelligent robotic systems which can 
handle more complicated tasks and environments. It is 
necessary to provide them with higher autonomy and 
better functionalities. For instance, robot localization and 
target tracking are mandatory. For these tasks, some of 
sensors such as GPSs, IMUs, laser range-finders, 
encoders and compasses are possibly combined to 
describe the time evolution of the motion model. 
Additionally, visual sensors, such as electro-optic or 
infrared cameras, have been included in the on-boarded 
sensor suit of many robots to increase their value. 
Therefore, it is a popular research topic regarding how to 
combine hybrid sensors to achieve a special task. Recent 
researches have shown that the way to improve sequential 
estimation and achieve repeatable motion with 
multi-sensors is to adopt the probabilistic method. The 
Kalman filter has been widely used for data fusion such 
as navigation systems [8], virtual environment tracking 
systems [9] and 3D scene modeling [10]. The Kalmna 
filter is a mathematical method to provide an iterated 
procedure for a least-square estimation of a linear system. 
The procedure includes predicting state by using the 
motion model and then correcting the state by the 
measurement innovation. The EKF is a varied type of the 
Kalman filter for a non-linear system such as the target 
tracking system in this study. More details about the 
Kalman filter are introduced in the material of [11]–[14]. 
In this study, the algorithm represents a new approach of 
EKF based multi-sensor data fusion. The information 
from different types of sensors is combined to improve 
the estimations in the system state. In order to make our 
system able to deal with the variety of tasks such as 
navigation for a mobile robot or ground target tracking 
for UAVs, we are determined to choose monocular vision 
rather than binocular vision as one type of our sensors. 
Based on the monocular vision, simultaneous robot 
localization and target tracking become more complex 
with a higher computational loading due to unknown 
depths of targets in only one image observation. Thus, it 
is an important issue regarding how to make use of 
computational tricks or adopt more efficient algorithms to 
reduce the running time. In this study, Chelesky 
decomposition [15] and forward-and-back substitution are 
used to invert the covariance matrix for improving the 
computational efficiency because the property of 
covariance matrix is semi-positive definite. 
II. EXPLANATION OF THE ALGORITHM 
The algorithm is mainly divided into five parts 
including motion modeling, new target adding, 
measurement modeling, image matching and EKF 
updating. By sequential measurement from sensors, the 
EKF is capable of improving the initial estimate for the 
unknown information while simultaneously updating the 
localization of targets and the robot pose. Finally, 
Chelesky decomposition and forward-and-back 
substitution are presented to calculate the inverse 
covariance matrix efficiently. 
A. Proposed Algorithm 
A single camera is mounted on our system as one of 
the sensors. The monocular vision infers that the depth of 
the target is not able to be measured by only one image 
but estimated by the sequential images. Therefore, the 
single camera has to estimate the depth by observing the 
target repeatedly to get parallax between different 
captured rays from the target to the robot. The 
orientations of target are estimated in the world 
coordinate system only by one image.   
  is a six 
dimension state vector and used to describe the position 
of the ith target in 3D space. Its equation is addressed as 
 
  
     
    
  
 
  
 
  
 
  (1) 
 
  
  is the location of the robot when the robot detects 
the ith target in the first time.   
  is defined as the 
position of the target with respect to the camera 
coordinate system and denoted by 
 
  
         
    
     
 
      
   
 
     (2) 
 
  
 
 and  
 
  are the orientations of the ith target and 
calculated by the pixel location of the target in only one 
image. The relationship between the position of the target 
  
  and the current robot localization   
  is presented in 
Fig. 1. 
4 
 
     
   and      
   respectively. 
C. Adding New Target 
It is remarkable about our algorithm that new targets 
are initialized by using only one image. The initialization 
includes the state initial values and the covariance 
assignment.  
 
(1) Target Initialization 
Equation (1) is used to define the location of the new 
target and estimate the six parameters of   
  with the 
EKF prediction-update loop. If the robot senses the 1st 
target at the state k for the first time, the new target 
information is added and then the process state vector is 
modified. The expanded process state vector is 
represented by the following equation: 
 
  
   
   
 
  
   (8) 
 
where 
  
     
    
  
 
  
 
  
   
 
   
 
The first time observation of the 1st target is done at the 
current camera location   
 .     
   
 
   is defined as an 
unit vector from the location   
  to the 1st target with 
respect to the world frame. Fig. 2 illustrates the 
relationship about     
   
 
  ,   
 
 and  
 
 .    
      is defined as the pixel location of the 1st target in 
an undistorted image.    is the location of the 1st target 
with respect to the camera frame. The location of the 1st 
target with respect to the world frame    is addressed as 
 
    
  
 
  
 
  
 
            
          
          
 
   
  (9) 
 
where 
  
    
 
  
    
    
    
                         
              
    
    
    
             
                          
    
    
    
 
                                                  
 
Then we get 
 
 
  
 
  
 
  
 
    
      
          
          
 
   (10) 
 
Only   
   
   and   
   
   rather than   
  and   
  are 
computed by using   . It is impossible to know   
  by 
only one image. However, it is possible to make use of 
  
   
   and   
   
   to calculate   
 
 and  
 
  which are 
shown as 
 
 
  
 
 
 
   
 
 
 
 
       
  
 
   
 
  
 
   
 
      
   
 
   
   
   
   
    
   
   
 
 
 
 
 
 
 
 
  (11) 
 
The unit vector     
   
 
   is derived as a function of 
the orientations of the target and described as 
 
    
   
 
    
    
 
      
 
     
 
 
    
 
      
 
   (12) 
 
The final parameter   
 
   is not able to be measured by 
only one image but estimated by the sequential images 
with EKF prediction-update loop. As a result, it is 
assumed to be an initial value   
 
 . 
 
(2) New Covariance Assignment 
The system covariance is modified after adding a new 
target. By using equation (42) in appendix, the new 
covariance       
   and the function of   
  are 
denoted by 
 
      
    
          
 
    
  (13) 
  
      
    
             
   (14) 
where 
  
 
 
 
 
 
  
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
  
  
      
  
 
   
 
  
 
   
 
      
   
 
   
   
   
   
    
   
   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   
 
Fig. 2.  The orientations of the target with respect to the world frame can be 
estimated as a function of its undistorted pixel location   . 
  
6 
 
E. Image Matching by Using Cross Correlation 
 The image patch information of the targets whose 
variables are set in the process state vector has been 
stored in the data base when the robot senses them in the 
first time. The predicted pixel locations of the targets 
within distortion are estimated by using equation (20). 
Next important issue is how to search for the actual pixel 
locations of the targets in a 320×240 image. Image 
matching is a fundamental and vital aspect of many 
problems in computer vision including motion tracking, 
object recognition and so on. The image features are 
invariant to rotation, image scaling, change in 
illumination and 3D camera viewpoint. It is still a popular 
research topic regarding how to allow a single candidate 
image to be correctly matched with high probability. 
Based on the estimation of measurement covariance, the 
small area where the target lies with high probability is 
able to be predicted. The measurement covariance of the 
ith target is defined as 
 
                
      
       
 
      
       
    (24) 
 
The template match technique is used to search the 
actual image pixel location of the ith target in the small 
search area which of width and length are         and 
       , respectivelhy. The coefficient k is a constant and 
defines how large the search area is. The larger k is, the 
more the search time is. There are two advantages 
regarding the search area which is estimated by the 
measurement covariance. One advantage is that a lot of 
time is saved because the image pixel location of the ith 
target is detected in a small search area rather than in a 
320×240 image. The other one is that the successful 
search rate increases dramatically because the search area 
allows the image pixel location to be correctly matched 
with high probability. As a result, it is not necessary to 
use a complicated object recognition algorithm such as 
Scale Invariant Feature Transform (SIFT) [17] for image 
matching in our system. Cross-correlation search is 
applied to be our template match algorithm and the 
computing loading is lower due to its simplification. This 
approach uses cross-correlation of image to search a 
suitable image patch.    is defined as the template image 
patch for the ith target and stored in the database.    is 
defined as a candidate image patch in the search area 
whose width is         and length is        . The cross 
correlation value of    with    is given by 
 
 
    
 
                          
        
  (25) 
 
   is the number of pixels in an image patch and     
and     are the stand deviations of    and    
respectively.     and     are the average values of    
and   , respectively. The maximum value of cross 
correlation of    with    is the best template matching 
and it is seen as the matching target pixel patch for the ith 
target. 
 
F. Iterated EKF Updating 
The EKF is one of the most widely used non-linear 
estimators due to its similarity to the optimal linear filter, 
its simplicity of implementation and its ability to provide 
accurate estimates in practice. We employ the iterated 
EKF to update the state. At each iteration step k, the prior 
process state vector is computed by using equation (4) 
and then      is calculated as a function of    
  by 
using equation (22). Next, the measurement innovation 
and the measurement Jocobian matrix are computed as 
         and                respectively. The 
measurement covariance and Kalman gain can be 
performed as 
 
             
         
                   
  
(26) 
                   
    
           
    (27) 
 
Finally, the process state vector and its covariance are 
updated at each iteration state and presented as 
 
                
              
             
(28) 
                      
       
             
   
(29) 
 
where     is the actual measurement and detected by 
using image matching search.      is the predicted 
measurement and computed by the sensor model. The 
error in the estimation is reduced by the iteration and the 
unknown depths of the targets converge toward the real 
values gradually. 
 
G. Fast Inverse Transformation for Covariance 
Matrices 
 The inverse matrix of the measurement covariance 
has to be computed at each iteration step. It needs plenty 
of running time by using the transpose of the matrix of 
cofactors to invert the measurement covariance. In order 
to deal with the large size of the inverse matrix within the 
variations of N targets efficiently, Cholesky 
decomposition is applied to invert the measurement 
covariance and reduce the running time at each iteration 
step. The measurement covariance is factored in the form 
           
  for a lower-left corner of the matrix   
because the measurement covariance is positive 
semi-definite. The matrix   is not unique, so multiple 
factorizations of a given matrix          are possible. 
A standard method for factoring positive semi-definite 
matrices is the Cholesky factorization. The element of   
8 
 
where 
         
      
      
  
   
    
 
Therefore,    
      is computed without   
  and   
  
by using equation (37) and applying equation (10) to 
calculate   
   
   ,   
   
   and   
   
  . 
 
Based on the results of calculating    
    
  , 
   
      and equation (10), it infers that we have solved 
a problem that the depth information,   
  and   
 , are not 
able to be measured only by one image. This is a very 
important characteristic of the proposed algorithm. 
 
B. Analysis Regarding Using Cross-Correlation 
Search 
Generally speaking, cross-correlation search is though as 
a simple template match algorithm and it is not as robust 
as SIFT. However,    is still detected correctly in the 
search area by using cross-correlation search because the 
small area is estimated by an iterated EKF and it includes 
the actual pixel location of the target with high probability. 
As shown in Fig. 3, the predicted pixel locations of the 
targets are estimated by using the sensor model and 
denoted by the green crosses. The pixel locations of the 
red crosses are the corresponding target pixel locations 
and detected by applying cross-correlation search. Based 
on the testing result that the red crosses are closer to the 
actual target pixel locations, it has proved that it is 
feasible for the proposed algorithm to apply 
cross-correlation search as its template match algorithm. 
 
 
IV. EXPERIMENTAL RESULTS 
In order to validate the proposed algorithm for 
localizing targets when the ground truth is available we 
have performed a number of experiments. The algorithm 
is implemented by C++ and performed by PC with 2.0 
GHz microprocessor. The monocular sensor is a single 
camera with wide angle lens because we hope that more 
targets can be observed in one image and tracking rate 
can be higher. The camera’s field of view is 1700 with a 
focal length of 1.7 mm. The image measurements 
received at a rate of 15 Hz are distorted with noise  =20 
pixel. The addressed experimental results are tested under 
the high speed robot motion because the average velocity 
of each case is higher than 20 cm/sec and the maximum 
velocity of all cases is 69.11 cm/sec. For the duration of 
experiments, the initial distance between the camera and 
the targets ranges from 1.68 m to 5.76 m.  The unknown 
depth of the target is estimated by sequential images with 
EKF and six cases (3.0 m, 3.5 m, 4.0 m, 4.5m, 5.0 m, and 
5.5 m) are assumed to be default depths for each 
experiment case. All of the sensors mounted on our 
system are shown in Fig. 4.  
 
There are some measurement errors caused by the 
camera distortion when using the camera with wide angle 
lens. Before validating the proposed algorithm, we 
performed an experiment to estimate the distorted noise 
by making use of the artificial landmarks. We chose the 
corners of the artificial landmarks as targets. The 
undistorted pixel locations of the corners are estimated by 
the pinhole model and then the image correction is 
applied to compute their predicted pixel locations with 
distortion. Owing to using a camera with wide angle lens, 
there is an error between the predicted pixel location with 
distortion and its actual pixel location which is detected 
by the cross-correlation search. In the proposed algorithm, 
the predicted pixel location without distortion is 
estimated in terms of the prior process state vector. The 
distorted error is produced if transforming the undistorted 
pixel locations of targets to the distorted pixel locations. 
Therefore, the distorted error should be taken into 
consideration very carefully. Based on the experimental 
result, the distorted noise is assumed to be 20 pixels. 
 
A. Results of Target Localization 
The way of the experiments shown in Fig. 5 is 
designed to track the top bar’s corners of the door as 
targets. The developed system does not require the initial 
setting at the first iteration. Several cases of different 
moving paths with varied linear velocities and 
orientations are examined. The average localization error 
of all experiments is 0.1260 m. The experimental result is 
presented in Fig. 6.  
 
We also performed an experiment about the 
comparison between encoders and IMU. We choose an 
encoder as our sensor instead of IMU in indoor 
 
(A)                  (B)                 (C) 
Fig. 4.  The encoder shown in (A) is used to measure the linear velocity 
and simulate the acceleration for the robot. The electrical compass 
shown in (B) is applied to measure the angular velocity of the robot. As 
shown in (C), the camera with wide angle lens is the additional 
measurement sensor. The three sensors are combined to estimate the 
localization of targets.  
 
Fig. 3.  Detect the actual target pixel location by using cross-correlation 
search. 
  
10 
 
Artificial Intelligence vol. 15, no. 3, 2003, pp. 
281-297. 
[2] S. Sohn, B. Lee, J. Kim and C. Kee, “Vision-based 
real-time target localization for single-antenna 
GPS-guided UAV,” IEEE Transactions on Aerospace 
and Electronic Systems, vol. 44, no. 4, 2008, pp. 
1391-1401. 
[3] P. Perez, J. Vermaak, and A. Blake, “Data fusion for 
visual tracking with particles,” Proceedings of the 
IEEE, vol. 92, 2004, pp. 495-513. 
[4] F. M. Mirzaei, and S. I. Roumeliotis, “A Kalman 
filter-based algorithm for IMU-camera calibration: 
observability analysis and performance evaluation,” 
IEEE Transactions on Robotics, vol. 24, no. 5, 2008, 
pp. 1143-1156. 
[5] S. B. Kim, S. Y. Lee, J. H. Choi, K. H. Choi and B. T. 
Jang “A bimodal approach for GPS and IMU 
integration for land vehicle applications,” 58th IEEE 
Vehicular Technology Conference, 2003, pp. 
2750-2753. 
[6] K. Park, H. Chung, J. Choi and J. G. Lee, “Dead 
reckoning navigation for an autonomous mobile 
robot using a differential encoder and a gyroscope,” 
8th International Conference on Advanced Robotics 
Proceedings ICAR'97, 1997, pp. 441-446. 
[7] S. Suksakulchai, S. Thongchai, D. M. Wilkes, and K. 
Kawamura, “Mobile robot localization using an 
electronic Compass for corridor environment,” 
Proceedings of the IEEE International Conference on 
Systems, Man and Cybernetics, vol. 5, 2000, pp. 
3354-3359. 
[8] J. Schroeder, S. Galler, K. Kyamakya, and K. 
Jobmann, “Practical considerations of optimal three 
dimensional indoor localization,” IEEE International 
Conference on Multi-sensor Fusion and Integration 
for Intelligent Systems, 2006, pp. 439-443. 
[9] S. Emura and S. Tachi, “Sensor fusion based 
measurement of human head motion,” 3rd IEEE 
International Workshop on Robot and Human 
Communication, 1994, pp. 124-129. 
[10] P. Grandjean and A. Robert De Saint Vincent, “3-D 
modeling of indoor scenes by fusion of noisy range 
and stereo data,” IEEE International Conference on 
Robotics and Automation, vol. 2, 1989, pp. 681-687. 
[11] P. S. Maybeck, “Stochastic models estimation and 
control volume 1,” Academic Press, Inc., 1979. 
[12] S. Thrun, W. Burgard and D. Fox, “Probabilistic 
robotics,” The MIT Press, 2005, pp. 191–217. 
[13] R. Siegwart and I. R. Nourbakhsh, “Introduction to 
autonomous mobile robots,” The MIT Press, 2004, 
pp. 227– 243. 
[14] R. Smith, M. Self and P. Cheeseman, “Estimating 
uncertain spatial relationships in robotics,” IEEE 
International Conference on Robotics and 
Automation, vol.4, 1987. 
[15] D. Dereniowski and M. Kubale, "Cholesky 
factorization of matrices in parallel and ranking of 
graphs," 5th International Conference on Parallel 
Processing and Applied Mathematics, Lecture Notes 
on Computer Science, 3019, Springer-Verlag, 2004, 
pp. 985–992. 
[16] J. Heikkila and O. Silven, “A four-step camera 
calibration procedure with implicit image correction,” 
IEEE Computer Society Conference on Computer 
Vision and Pattern Recognition, 1997, pp. 1106-1112. 
[17] S. Se, D. G. Lowe and J. Little, “Vision-based mobile 
robot localization and mapping using scale-invariant 
features,” Proceedings 2001 ICRA. IEEE 
International Conference on Robotics and 
Automation, vol. 2, 2001, pp. 2051-2058. 
 
機會發表了數百篇最新最先進的相關研究論文，對於各領域之互相交流，激盪出
更先進的研究構想，以及提高體學術風氣有十分正面的影響。本次研討會邀請了
多個領域知名學者發表演說，對於所有參與此次研討會的專家學者們是一次難得
的學習機會。 
 
三、考察參觀活動 
個人偕同於會場結識，國內教授群，如文化大學、高雄應用科技大學，5/3
早上，於青島市區參觀訪問，同時本研討會主辦單位–青島大學參觀，這是一次
非常難得的文化經驗。 
 
四、建議 
 建議國內各個學術單位能積極爭取主辦大型國際研討會，增進與國際學者學
術交流，此外政府相關單位亦能充分支持國內的學術研究與交流機會，如此不但
可以增進國際能見度，亦能給予國內學者及學生一個學習、討論的機會。 
 
五、攜回資料名稱及內容 
    本次帶回了一片 CD 會議論文集，除了大會所邀請的專題演講之外，亦收集
了各個分組討論所發表的論文，十分具有參考價值。對於從事相關研究上會有極
大的助益。 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：吳炳飛 計畫編號：99-2221-E-009-160- 
計畫名稱：導護與運輸機器人於智慧型路口安全防護之研究--子計畫一:結合車路人之車路資通整合服
務研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 7 7 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100 字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：□已發表 □未發表之文稿 ■撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100 字為限） 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
目前機器人正經歷著從產業型機器人時代走向服務型機器人時代的轉變，這些具備智慧的
機器人也為文化、福利、健康等領域帶來了新的市場。同時，機器人是跨領域的產物，隨
著機器人在生活領域的不斷普及，本計畫在研究方向與目前國外相關系統的概念相符，整
體發展上，除了在安全保護警示方面有所突破，更創新的設計了運輸機器人，將弱勢用路
人安全保護進一步提升至實際輔助行為，在弱勢用路人安全保護方面，有重點突破。另外，
這個計畫將完全運用國人自行開發的人工智慧、影像處理之技術、嵌入式平台技術、無線
通訊技術、機器人設計研製的研發能量，在提升國內相關產業以及人才培訓方面，將有很
大助益。 
    本計畫以多種感測與控制技術研製運輸機器人，運輸機器人主要是以自動搭載的方
式，協助行動不便或是行動緩慢之身障人士與老年人安全通過馬路。基於無線區域網路的
訊號，機器人可以連上網路以及訊號的強度指標，我們將透過此一訊號強度指標資訊來實
現機器人於定位機制，透過無線訊號強度的接收與判斷，機器人可以知道自己的位置，再
搭配距離型的感測器如雷射雷達、超音波雷達等等，我們開發機器人感知環境的智慧並且
能聰明地規劃出最佳路徑，接著設計控制演算法，使機器人準確無誤地移動到指定的目標
位置；此外，我們透過架設攝影機於機器人之上，可隨時抓取機器人前方的影像，可自行
偵測障礙物以及停車位，使用完畢後能自行停回機器人待機處，利用我們所提供的視覺化
監控界面，即可享受方便運送服務。這項成果以作品「室內無線定位與導覽運送之輪椅機
器人」獲得第九屆旺宏金矽獎半導體設計與應用大賽 --應用組評審團銅獎。 
