2 
 
目錄 
計畫中文摘要...………………………………………………………………………………………………. .4 
計畫英文摘要...……………………………………………………………………………………………….. 4 
1. Introduction..,………………………………………………………………………………………………5 
2. Overview of machine learning-based classifiers for RTS……………………………………………….6 
3. Review of literature regarding the enhancement of the generalization ability of machine 
learning-based classifiers………………………………………………………………………………….8 
4. Problem statement and formulation…………………………………………………………………….10 
5. Development of ensemble classifier RTS……………………………………………………………….13 
6. Experiment and results……………………………………….……………………………………..…..18 
7. Conclusion and future work………………………………….………………………………………....21 
References……………………………………………………….…………………………………………..22 
Figure 1…………………………………………………………….…………………………………………26 
Figure 2…………………………………………………………….…………………………………………26 
Figure 3……………………………………………………………………………………………………….27 
Figure 4……………………………………………………………………………………………………….27 
Figure 5……………………………………………………………………………………………………….28 
Table 1………………………………………………………………………………………………………..28 
Table 2………………………………………………………………………………………………………..29 
Table 3………………………………………………………………………………………………………..30 
Table 4………………………………………………………………………………………………………..30 
4 
 
計畫中文摘要 
在機器學習為基的分類器中有兩項因子會影響分類正確率：屬性選擇（含參數調整）及分類器的
集成。因此本研究開發以集成式方法開發即時排程系統，而基底分類器則以基因演算法進行屬性選擇。
本研究所提出的方法所建構的即時排程系統與以往三種經典的機器學習方法並以基因演算法進行屬性
選擇所建構即時排程系統經由十摺交叉驗證的分類正確率比較，本研究計畫所開發以基因演算法為基
集成分類器其分類正確率優於上述三種機器學習方法。並經由模擬實驗證明本研究方法相對於文獻中
其他的機器學習為基即時排程系統與及啟發式單一派工法則，在彈性製造系統環境下之各種不同的生
產績效指標下進行比較亦可顯示有其優越性。 
關鍵詞：即時排程，現場監控，彈性製造系統控制，機器學習，集成，屬性選擇。 
 
計畫英文摘要 
 
There are two items that significantly enhance the generalization ability (i.e., classification accuracy) of 
machine learning-based classifiers: feature selection (including parameter optimization) and an ensemble of 
the classifiers. Accordingly, the objective in this study is to develop an ensemble of classifiers based on a 
genetic algorithm (GA) wrapper feature selection approach for real time scheduling (RTS). The proposed 
approach can better enhance the generalization ability of the RTS knowledge base (i.e., classifier) in 
comparison with three classical machine learning-based classifier RTS systems, including the GA-based 
wrapper feature selection mechanism, in terms of the prediction accuracy of ten-fold cross validation as 
measured according to all the performance criteria. The proposed ensemble classifier RTS also provides better 
system performance than the three machine learning-based RTS systems, including the GA-based wrapper 
feature selection mechanism and heuristic dispatching rules, under all the performance criteria, over a long 
period in a flexible manufacturing system (FMS) case study. 
Keywords: Real time scheduling; Shop floor control; FMS control; Machine learning; Ensemble; Feature 
selection. 
 
 
6 
 
of possible features to identify a subset that is optimal or near-optimal with respect to performance measures 
such as accuracy. The second item results from the fact that an ensemble of classifiers is a collection of 
several classifiers whose individual decisions are combined in some way to classify the test examples.  
No research has combined feature subset selection and an ensemble of various classifiers such as back 
propagation neural networks (BPNNs), decision tree (DT) learning, and support vector machines (SVMs) to 
enhance the performance of the resulting ensemble classifier with regard to the RTS problem. Hence, 
developing an ensemble classifier in RTS that includes a selection mechanism for a subset of features (i.e., 
manufacturing attributes) is highly desirable. 
This study seeks to develop an ensemble of classifiers based on the genetic algorithm (Goldberg 1989) 
wrapper feature selection for an RTS system. The proposed approach will enhance the generalization ability 
of the RTS KB in terms of the prediction accuracy for unseen data under a variety of performance criteria. 
Furthermore, we believe that the proposed approach to building an RTS system can improve system 
performance, according to various criteria, over a long period of time, compared to the results obtained with 
classical machine learning-based approaches and the heuristic individual dispatching rule. This is the main 
contribution of this study. 
The remainder of this paper is divided into six sections. Section 2 introduces background information 
about machine learning-based classifiers for RTS issues. Section 3 reviews the relevant literature regarding 
enhancing the generalization ability of machine learning-based classifiers in relation to the two 
above-mentioned issues (feature subset selection and the ensemble of classifiers). Section 4 formulates the 
RTS problem and states the research objectives. Section 5 describes the methodology of the proposed 
ensemble of classifiers based on the GA wrapper feature selection approach for RTS systems. Section 6 
presents experimental results from a case study and an analysis of the proposed approach, as well as analyses 
of other approaches. Finally, Section 7 concludes the paper with a summary and suggests some topics for 
future research.  
2. Overview of machine learning-based classifiers for RTS 
The major advantages of using the machine learning approach in constructing an RTS system can be 
8 
 
the ID3 algorithm (Quinlan 1986, 1987) and its successor C4.5 (Quinlan 1993), is the primary focus of 
research in the DT learning field. The learned knowledge is presented in the form of either DTs or sets of 
if-then rules. The major advantage of the DT learning approach is that knowledge can be easily coded into a 
machine cell dynamic dispatching selection mechanism, which can improve its readability. 
An SVM was developed by Vapnik (1999). This type of learning machine is based on statistical learning 
theory. A special property of SVMs is that they minimize the empirical classification error and maximize the 
geometric margin simultaneously; hence, they are also known as maximum margin classifiers. Recent 
studies (Shiue 2009a, Priore et al. 2010) have demonstrated that the use of the SVM approach for building 
an RTS system KB can yield good system performance compared to heuristic individual dispatching rules 
and most of the other classical machine learning approaches under various performance criteria and over a 
long period. 
3. Review of literature regarding the enhancement of the generalization ability of machine 
learning-based classifiers 
3.1  Feature selection 
Various methods of performing feature subset selection are available (Yang and Honavar 1998). Based 
on their relations with the base learner, these methods can be classified into two categories: filter and wrapper. 
The filter approach usually chooses features independently of the base learner through human experts or 
statistical methods such as principal component analysis (PCA), and it is generally more computationally 
efficient than a wrapper approach. Its major drawback is that an optimal selection of features may not be 
independent of the inductive and representational biases of the learning algorithm that is used to construct the 
classifier. The wrapper approach, on the other hand, evaluates candidate feature subsets by training a base 
learner such as an ANN or SVM, with a given training data set, using each feature subset under consideration 
and then testing it against a separate validation data set. Because the number of feature subsets is usually very 
large, this scheme is feasible only if the training is relatively fast (Yang and Honavar 1998). Furthermore, 
because the wrapper approach usually involves a search, choosing an appropriate search method is also an 
important task. A simple exhaustive search is computationally impractical. Consequently, a suboptimal, yet 
practical, search method is desired. The well-known stepwise regression is a wrapper approach based on a 
10 
 
Bagging (Breiman 1996) and AdaBoost (Freund and Schapire 1996) are two of the most popular 
methods of creating ensembles based on sampling. Both of these methods operate by taking a base learning 
algorithm and invoking it many times with different training sets. In Bagging, each classifier’s training set is 
generated by randomly drawing, with replacement, n examples, where n is the size of the original training set; 
many of the original examples may be repeated in the resulting training set, while others may be left out. Each 
individual classifier in the ensemble is generated with a different random sampling of the training set. The 
Adaboost algorithm maintains a set of weights over the original training set and adjusts these weights after 
each classifier is learned by the base learning algorithm. These adjustments increase the weights of examples 
that are misclassified by the base learning algorithm and decrease the weights of examples that are correctly 
classified. 
Earlier studies (Opitz and Maclin 1999, Dietterich 2000) compared the effectiveness of Bagging and 
AdaBoost for improving the performance of an ensemble of classifiers. The experiments showed that, in 
situations with little or no classification noise, AdaBoost is often more accurate than Bagging. In situations 
with substantial classification noise, Bagging is much better than AdaBoost. Moreover, analyses indicate that 
the performance of the boosting method is dependent on the characteristics of the data set being examined. In 
fact, further results show that ensembles with boosting methods may over fit noisy data sets and thus decrease 
performance. The results of previous studies (Chen et al. 1999, Kwak and Yih 2004, Shiue et al. 2009) 
indicate that the classification accuracy of RTS system classifiers is only around 60% to 70% in each 
performance criterion. The reason for this is that the training data set generated by a simulation often contains 
noise data in a dynamic manufacturing environment. Based on the above-mentioned points, using the Bagging 
approach to create ensembles based on sampling is preferable to using an RTS system in this study. 
4.  Problem statement and formulation 
4.1  Real time scheduling system 
Previous studies have confirmed that, because the status of a manufacturing system changes continuously, 
it is possible to improve the system performance by implementing a multi-pass scheduling policy rather than 
using a single dispatching rule for an extended planning horizon; the policy is based on the system status at 
12 
 
no machine in the family is idle, the lot goes to the least-utilized machine. 
3. Each machine is subject to random seed failures; processing times are assumed to be predetermined. 
4. The model assumes that no rework takes place and there is no yield loss. 
5. All material movements that do not involve the automated material handling system (AMHS) are 
assumed to be negligible. 
6. The part with a pallet travels to each machine or load/unload station to achieve operational 
flexibility, and the part-type match for one specific pallet problem is ignored. 
7. When each step of the process for the part is completed, it must return to one of the available 
load/unload stations for reorientation. Otherwise, it will go to the central WIP buffer to wait for the 
next operation (part reorientation in load/unload stations). 
8. An AGV can carry only one part at a time, and it moves only in the counter-clockwise direction. 
4.3  Training examples specification 
A set of training examples is provided as information for the system to learn the concepts that represent 
each class. Each given training example comprises a vector of attribute values and the corresponding class. It 
is known that the relative effectiveness of a dispatching rule strongly depends on the status of the system. 
Hence, in order to construct a scheduling knowledge base, training examples must have sufficient information 
to reveal this property.  
In the classifier requiring an RTS mechanism, let X denote the set of training examples generated by the 
training sample generation mechanism. A training example Xx  can be denoted as {f,  
 }, where   
  
represents the optimal control strategy (dispatching rule) for the following scheduling period under 
performance criterion o. Three kinds of performance criteria are typically studied in scheduling problems: 
throughput-based, flow time-based, and due date-based. Table 1 presents the three performance criteria used 
in this study. 
[Insert Table 1 about here] 
This research seeks to determine the optimal subset of system features under various performance 
criteria. Thus, all possible system features are exhaustively examined. Table 2 lists 30 candidate attributes. 
The criteria for selecting system features are based on earlier research (Park et al. 1997, Chen et al. 1999, Arzi 
14 
 
as described in Section 4.1. Moreover, each individual base classifier is trained independently using its own 
replicated training data set via a Bagging method, and all constituent base classifiers are aggregated by a 
majority voting strategy, as explained in Section 4.2. 
[Insert Fig. 2 about here] 
During the on-line operational phase, the physical component of the shop floor is responsible for 
operating the manufacturing system by implementing an RTS mechanism at each scheduling decision point 
(i.e., the end of a scheduling period). At each decision point, the physical component records the status of the 
system. Then, the proposed ensemble classifier RTS system generates the decision rules for the next 
scheduling period. 
5.1  GA-based wrapper feature selection mechanism 
For the purposes of our study, we first develop three types of base classifiers: BPNN, DT, and SVM, 
using the GA-based wrapper feature selection mechanism. The GA-based wrapper feature selection 
mechanism can determine the optimal subset of system features and learning parameters for three types of 
base classifiers. Figure 3 shows the proposed GA-based wrapper feature selection mechanism. The 
chromosomal representations of the three types of base classifiers are discussed below. 
[Insert Fig. 3 about here] 
5.1.1 Back propagation neural network 
In the BPNN-based classifier, the GA was utilized to optimize the configuration of the BPNN (i.e., the 
input layer configuration that is determined with the optimal subset of features and the optimal number of 
hidden layers and neurons). The parameters of the BPNN algorithm (learning rate, momentum factor, and 
range of initial weights) are used for training the RTS classifiers. Hence, every BPNN is coded as a binary 
string of finite length. A GA population consists of B binary strings. Each binary string, Bb , consists of 
lc  bits and encodes the ANN architecture. These lc  bits are divided into seven parts. The first part of length 
1lc  is used to encode the input neurons (system attributes) that will be used. The second part of length 2lc  is 
used to encode the number of hidden layers. The third and fourth parts of lengths 3lc  and 4lc  are used to 
16 
 
feature-value test is denoted as the M parameter and the confidence level of pruning is denoted as the CF 
parameter) were used for training the RTS system classifiers. Hence, every C4.5 was coded as a binary string 
of finite length. Each binary string encodes the system features and DT parameters. Each binary string 
consists of lc  bits. The lc  bits are divided into three parts. The first part, with a length of 1lc , is used to 
encode the system features that will be used. The second and third parts, with lengths of 2lc  and 3lc , are 
used to encode the values of the M and CF parameters, respectively. 
The M parameter determines the minimum number of instances represented by a node. A high value of M 
leads to an increase in the level of abstraction and thus to less recoverable information on individual instances. 
Setting M ＞ 1 (the default value is M = 2) allows C4.5 to avoid the creation of long paths involving a small 
number of obscure individual instances that are likely to represent noise. The CF parameter denotes the 
confidence level of pruning, which ranges from 0% to 100%. No pruning occurs if CF = 100% is set (the 
default value is CF = 25%). As more pruning is performed, less information about the individual examples is 
retained in the abstracted decision tree. When the number of bits is given, 42 lc  and 53 lc  for the 
second and third parts of the string, respectively; the binary encoding of the second and third parts of the 
string is summarized in Table 5. 
[Insert Table 5 about here] 
5.1.3 SVM algorithm  
In the SVM-based classifier RTS presented in this study, a radial basis function (RBF) is used as the 
basic kernel function of the SVM. The penalty parameter, C, and the kernel parameter, r, for the RBF kernel 
play crucial roles in the accurate performance of the SVM classification (Hsu et al. 2003). Hence, these two 
parameters, C and r, should be optimized by the GA. 
In this study, on the basis of the above discussion, the GA was utilized to optimize the subset of system 
attributes and the SVM parameters. Hence, every training SVM was coded as a binary string of finite length. 
Each binary string encodes the system features and SVM parameters. The binary string comprises lc bits. 
These lc bits are divided into three parts. The first part, with a length of lc1, is used to encode the system 
attributes that will be used. The second and third parts, with lengths of lc2 and lc3, are used to encode the 
18 
 
j
j
mv Nf Max arg)( x .                                    (3) 
The Bagging algorithm used to create the ensemble classifier in this study is summarized in Table 6. 
[Insert Table 6 about here] 
6. Experiment and results 
6.1  Construction of simulation model and training example generation  
In order to verify the proposed method, this study uses a discrete-event simulation model to generate 
training examples. The simulation model is built and executed using Tecnomatix Plant Simulation (2009), an 
object-oriented simulation language, run on a Xeon E5530 2.4 GHz Quad Core CPU with the Windows 7 
operating system. 
The proposed approach is expected to achieve the desired dynamic scheduling performance. Several 
parameters were determined from a preliminary simulation run. The time between jobs follows an exponential 
distribution with a mean of 31 min. The due date for each job is randomly assigned from 6 to 10 times the 
total processing time, and the due dates are uniformly distributed. The maximum number of pallets (jobs) that 
are permitted in the FMS system is 100. Table 7 shows five product-mix ratios used to generate the training 
examples. The proportions of part types are changed every 20,000 minutes. 
[Insert Table 7 about here] 
To generate a large number of different training examples, we use 100 different random seeds to generate 
100 different job arrival patterns. The warm-up period for each run is 10,000 min, and this is followed by 60 
multi-pass scheduling periods. The time window (i.e., scheduling period) of the multi-pass simulation is 2,000 
min. Hence, a total of 6,000 training examples are collected for the two case studies. 
6.2 Ensemble classifier RTS implementation  
On the basis of the concept illustrated in Table 6, it is necessary to create the ensemble model by 
Bagging in advance. The applied Bagging algorithm is encoded in advance in WEKA release 3.6.4 with 
default parameter settings (i.e., create k = 10 training data sets); WEKA (Witten et al. 2011) is a Java-based 
machine learning tool. Then, each replicate training data set is trained using GA+BPNN, GA+DT, and 
20 
 
random seeds to investigate whether the proposed ensemble classifier RTS is more effective over a long 
period compared to an RTS that uses the classical machine learning-based approaches, including the GA 
wrapper feature selection as with GA+BPNN, GA+DT, and GA+SVM (Shiue et al. 2009). Here, GA+BPNN, 
GA+DT, and GA+SVM RTSs, which involve the use of GA, evolve optimal subsets of the shop floor features 
and simultaneously determine a learning parameter for the classification algorithm. The simulation results 
indicate that the GA-based feature selection approach yields a system performance over a long period of time 
that is better than the performance obtained using the classical machine learning approach (Shiue et al. 2009). 
Moreover, a single heuristic dispatching rule is also compared to the proposed ensemble classifier RTS in 
various scenarios based on three performance criteria. 
Table 11 shows the mean and standard deviation of 30 simulation runs—carried out using 30 random 
seeds—resulting from different scheduling strategies. The proposed ensemble classifier approach has been 
shown to be capable of achieving better results, as measured by all the performance criteria, because of its 
superior efficiency.  
[Insert Table 11 about here] 
The authors are also interested in determining which of the 10 scheduling strategies have superior 
performance in relation to three different performance criteria. A one-way ANOVA for a randomized block 
design with 30 replications is constructed, where the scheduling strategies are defined as ―factors‖ and the 30 
simulation runs carried out using 30 random seeds are ―blocks.‖ The results of this ANOVA are given in Table 
12; they reveal that there are significant differences between the 10 scheduling strategies for the three 
different performance criteria. Fisher’s least significant difference, a multiple comparison method, is 
implemented on each pair of the 10 scheduling strategies for the three different performance criteria, and the 
p-value is listed in Table 13.  
[Insert Table 12 here] 
[Insert Table 13 here] 
It can be found that the proposed ensemble classifier RTS is significantly superior to the GA+BPNN, 
GA+DT, and GA+SVM approaches and the heuristic dispatching strategies, as measured by all the 
22 
 
created (trained), then the meta-classifier is created. In the prediction of an unseen instance, the base 
classifiers will output their classifications, then the meta-classifier will make the final classification 
(i.e., as a function of the base classifiers) (Rokach 2010). 
 In this study, the knowledge base of the RTS was static. Although this approach is feasible, it would 
be interesting to establish a procedure that maintains the increment in the knowledge base, even 
when significant changes occur in the manufacturing system. One possible solution incorporates a 
reinforcement learning mechanism (Stutton and Barto 1998) that can learn to select the appropriate 
actions for achieving its goals through interactions with the system environment and that can 
respond to rewards or penalties received based on the impact of each action. Future research could 
investigate and examine this idea. 
References 
Arzi, Y. and Iaroslavitz, L., 1999, Neural network-based adaptive production control system for flexible 
manufacturing cell under a random environment, IIE Transactions, 31 (3), 217-230.  
Arzi, Y. and Iaroslavitz, L., 2000. Operating an FMC by a decision-tree-based adaptive production control 
system. International Journal of Production Research, 38 (3), 675-697. 
Baker, K.R., 1984. Sequencing rules and due-date assignments in a job shop. Management Science, 30 (9), 
1093-1104. 
Breiman, L., 1996. Bagging predictors. Machine Learning, 24 (2), 123–140.  
Chang, C.C. and Lin, C.J., 2010. LIBSVM 3.0: A library for support vector machines. 
Available from: http://www.csie.ntu.edu.tw/~cjlin/libsvm/. 
Chen, C.C., Yih, Y., and Wu, Y.C., 1999. Auto-bias selection for learning-based scheduling systems. 
International Journal of Production Research, 37 (9), 1987-2002.  
Cho, H. and Wysk, R.A., 1993. A robust adaptive scheduler for an intelligent workstation controller. 
International Journal of Production Research, 31 (4), 771-789.  
Dietterich, T.G., 2000. An experimental comparison of three methods for constructing ensembles of decision 
trees: Bagging, Boosting and randomization. Machine Learning, 40 (2), 139–157.  
Freund, Y. and Schapire, R., 1996. Experiments with a new Boosting algorithm. Proceedings of the 13th 
international conference on machine learning, 3–6 July 1996, Bari, Italy. San Francisco: Morgan Kaufmann 
Publishers, 148–156.  
Goldberg, D.E., 1989, Genetic Algorithms in search, Optimization and Machine learning. Reading, MA: 
24 
 
19 (3), 247-255.  
Priore P, Parreno, J, Pino, R, Gomez, A, and Puente, J., 2010. Learning-based scheduling of flexible 
manufacturing systems using support vector machines. Applied Artificial Intelligence, 24 (3),194-209.  
Qiu, R., Wysk, R., and Xu, Q., 2003. Extended structured adaptive supervisory control model of shop floor 
controls for an e-Manufacturing system. International Journal of Production Research, 41 (8), 1605-1620.  
Quinlan, J. R., 1986, Induction of decision trees. Machine Learning, 1, 81-106.  
Quinlan, J. R., 1987, Simplifying decision trees, International Journal of Man-Machine Studies, 27, 221-234.  
Quinlan, J.R., 1993. C4.5: Programs for Machine Learning. San Manteo: Morgan Kaufmann.  
Rokach, L. 2010. Pattern Classification Using Ensemble Methods. Singapore: World Scientific Publishing.  
Rumelhart, D.E., Hinton, G.E., and Williams, R.J., 1986, Learning internal representations by error 
propagation. Parallel Distributed Processing Cambridge: MA Press.  
Shiue, Y.R., 2009a. Data mining-based dynamic dispatching rule selection mechanism for shop floor control 
systems using support vector machine approach. International Journal of Production Research, 47, 
3669-3690.  
Shiue, Y.R., 2009b. Development of two-level decision tree-based real-time scheduling system under product 
mix variety environment. Robotics and Computer-Integrated Manufacturing, 25 (4-5), 709-720.  
Shiue, Y.R. and Guh, R.S., 2006. Learning-based multi-pass adaptive scheduling for a dynamic manufacturing 
cell environment. Robotics and Computer-Integrated Manufacturing, 22 (3), 203-216.  
Shiue, Y.R., Guh, R.S., and Tseng, T.Y., 2009. GA-based learning bias selection mechanism for real time 
scheduling systems, Expert Systems with Applications, 36 (9), 11451-11460. 
Shnits, B. and Sinreich, D., 2006. Controlling flexible manufacturing systems based on a dynamic selection of 
the appropriate operational criteria and scheduling policy. International Journal of Flexible Manufacturing 
Systems, 18 (1), 1-27. 
Son, Y., Rodrı g´uez-Rivera, H., and Wysk, R.A., 1999. A multi-pass simulation-based, real-time scheduling 
and shop floor control system. Transactions: The Quarterly Journal of the Society for Computer Simulation 
International, 16 (4), 159–172.  
Son, Y., Wysk, R.A., and Jones, A., 2003. Simulation based shop floor control: formal model, model 
generation and control interface. IIE Transactions, 35 (1) 29–48.  
Stutton, R.S., and Barto, G.B., 1998. Reinforcement Learning: An Introduction. Cambridge: MIT Press. 
Su, C.T. and Lin, H.C., 2011. Applying electromagnetism-like mechanism for feature selection. Information 
Sciences, 181 (5), 972–986. 
Su, C.T. and Shiue, Y.R., 2003. Intelligent scheduling controller for shop floor control systems: a hybrid 
26 
 
 
 
Data 
M1 Classifier 
M2 Classifier 
． 
． 
Mk Classifier 
New data 
sample 
Prediction Combine 
votes 
Figure 1. The ensemble approach in the classifier system 
 
 
 
Combination rule 
according to Majority 
Voting 
Figure 2. The general architecture of the proposed approach 
Training examples X 
X1 X2 Xk 
…      ... 
    ... 
GA+BPNN1 
Classifier …   .. 
    ... 
GA+DT1 
Classifier 
GA+SVM1 
Classifier 
GA+BPNN2 
Classifier 
GA+DT2 
Classifier 
GA+SVM2 
Classifier 
GA+BPNN2 
Classifier 
GA+DT2 
Classifier 
GA+SVM2 
Classifier 
Sampling by Bagging 
approach 
Bagging
GAEC
Classifier 
…  .
. 
    ... 
Create base classifier by GA 
wrapper feature selection 
 
28 
 
 
(a) SVM parameter C 
Minimum value of the C parameter is      0.0 
Maximum value of the C parameter is  13107.1 
111111is13107.2.0 
9.1676)12121212(
12
0.01.13107 07814
17



C  
(b) SVM parameter r 
Minimum value of the r parameter is   0.000 
Maximum value of the r parameter is  32.767 
706.4)1212121212(
12
000.0767.32 156912
15



r
Figure 5. Examples of encoding SVM parameters for the GA 
0   0   1   0   0   0   0   0   1   1   0   0   0   0   0   0   1 
0   0   1   0   0   1   0   0   1   1   0   0   0   1   0 
 
 
 
Table 1. Performance criteria used in this study 
Performance criteria Description 
TP Throughput 
MCT Mean cycle Time 
NT Number of the tardy parts 
  
 
  
30 
 
Table 3. Dispatching rules used in this study 
 
Dispatching Rule Description 
    DS Select the job with minimum slack time  
    EDD Select the job with the earliest due-date  
SIO Select the job with the shortest imminent operation time 
SPT Select the job with the shortest processing time 
SRPT Select the job with the shortest remaining processing time 
 
Table 4. 
The binary encoding form second parts to seventh parts of the string for BPNN 
parameter 
 
 Bit length Number of 
setting encoded 
Bits Encoding 
Number of hidden 
layers 
lc2 = 1 2 0, 1 1, 2 
Max. neuron number 
in hidden layer 1 
lc3 = 6 64 000000, 000001, 00010, 
000011,……………, 
111111 
0, 1, 2, 3, ……, 63 
Max. neuron number 
in hidden layer 2 
lc4= 6 64 000000, 000001, 00010, 
000011,……………, 
111111 
0, 1, 2, 3, ……, 63 
Initial learning rate lc5= 2 4 00, 01, 10, 11 0.2, 0.3, 0.4, 0.5 
Initial momentum 
factor 
lc6= 2 4 00, 01, 10, 11 0.3, 0.4, 0.8, 0.9 
Initial weights range lc7= 2 4 00, 01, 10, 11 [-0.01, +0.01] , [-0.125, +0.125], 
[-0.25, +0.25], [-0.5, +0.5],  
 
Table 5.  
The binary encoding form second parts to third parts of the string for C4.5 
parameter  
 
 Bit 
length 
Number of 
setting encoded 
Bits Encoding 
The value of M 
parameter 
lc2 = 4 16 0000, 0001, 0010, 
0011,0100,…………
………,1100, 1101 
2, 3, 4, 5,…..14, 15 
The value of CF 
parameter  
 
lc3 =5 32 00000, 00001, 00010, 
………………………
…….., 11110 
10%,11%, 22%,……40% 
 
32 
 
Table 8.  
The values of the optimal subset of system features and the learning parameters in the 
GA+BPNN classifier under three performance criteria. 
 
 
Performance 
criterion 
Sampling 
no. 
No. 
features 
Network 
architecture 
Initial 
learning rate 
Initial momentum 
factor 
Initial weight 
range 
 
 
 
 
 
TP 
1 12 12-6-54-5 0.5 0.95 [-0.5, +0.5] 
2 12 12-6-54-5 0.5 0.95 [-0.5, +0.5] 
3 12 12-16-25-5 0.4 0.95 [-0.125, +0.125] 
4 20 20-16-25-5 0.3 0.80 [-0.5, +0.5] 
5 12 12-16-25-5 0.4 0.95 [-0.125, +0.125] 
6 15 15-42-8-5 0.4 0.95 [-0.25, +0.25] 
7 10 10-38-30-5 0.4 0.90 [-0.125, +0.125] 
8 16 16-38-13-5 0.2 0.90 [-0.25, +0.25] 
9 9 9-22-45-5 0.3 0.90 [-0.125, +0.125] 
10 13 13-38-6-5 0.3 0.95 [-0.125, +0.125] 
 
 
 
 
MCT 
1 12 12-32-10-5 0.2 0.85 [-0.1, +0.1] 
2 12 12-5-33-5 0.4 0.80 [-0.1, +0.1] 
3 15 15-13-30-5 0.5 0.95 [-0.125, +0.125] 
4 17 17-18-7-5 0.3 0.85 [-0.125, +0.125] 
5 10 10-64-5-5 0.3 0.90 [-0.25, +0.25] 
6 15 15-34-10-5 0.3 0.85 [-0.25, +0.25] 
7 12 12-6-24-5 0.2 0.80 [-0.25, +0.25] 
8 21 21-3-45-5 0.5 0.90 [-0.25, +0.25] 
9 12 12-5-5-5 0.3 0.95 [-0.125, +0.125] 
10 17 17-59-10-5 0.2 0.90 [-0.1, +0.1] 
 
 
 
 
 
NT 
1 15 15-2-2-5 0.4 0.90 [-0.5, +0.5] 
2 14 14-61-2-5 0.2 0.90 [-0.125, +0.125] 
3 16 16-10-19-5 0.4 0.80 [-0.25, +0.25] 
4 17 17-6-29-5 0.4 0.95 [-0.25, +0.25] 
5 15 15-4-49-5 0.4 0.90 [-0.125, +0.125] 
6 14 14-1-19-5 0.4 0.80 [-0.25, +0.25] 
7 16 16-1-19-5 0.4 0.80 [-0.1, +0.1] 
8 16 16-9-19-5 0.4 0.95 [-0.5, +0.5] 
9 16 16-1-48-5 0.2 0.85 [-0.25, +0.25] 
10 16 16-2-2-5 0.4 0.90 [-0.125, +0.125] 
 
34 
 
Table 10. The ten-fold cross validation results of the ensemble classifier RTS and the 
GA-based wrapper feature selection classifier RTSs. 
Performance 
criterion 
 
Fold 
Classifier classification accuracy (%) 
Bagging
GAEC  GA+BPNN GA+DT GA+SVM 
 
 
 
 
 
TP 
 
1 74.833 72.000 71.267 72.500 
2 73.333 66.800 67.000 67.500 
3 76.667 69.500 71.000 69.500 
4 73.667 70.000 72.267 70.333 
5 71.000 66.000 68.267 67.167 
6 74.500 66.700 66.267 67.167 
7 75.833 67.700 68.267 68.500 
8 70.000 69.700 69.267 69.833 
9 73.333 68.700 70.000 69.000 
10 73.500 68.700 69.267 68.333 
Avg. accuracy 73.667 68.580 69.287 68.983 
 
 
 
 
 
MCT 
1 67.000 59.333 66.667 66.333 
2 64.000 55.467 62.500 59.500 
3 64.167 56.467 62.767 63.667 
4 66.667 55.667 65.500 64.667 
5 65.333 53.000 59.500 64.167 
6 55.833 52.667 55.267 56.000 
7 63.000 55.000 56.667 61.500 
8 66.333 57.667 61.767 62.333 
9 64.333 54.467 62.267 64.333 
10 62.333 50..767 59.767 62.333 
Avg. accuracy 63.899 55.049 61.267 62.483 
 
 
 
 
 
NT 
1 68.000 53.700 62.667 64.833 
2 67.833 59.700 68.167 66.333 
3 70.000 57.500 63.167 65.833 
4 69.500 58.700 65.767 66.333 
5 67.500 56.800 65.667 66.000 
6 68.000 60.200 69.167 65.500 
7 73.167 63.200 66.167 67.833 
8 68.833 54.700 67.267 66.000 
9 74.167 57.800 63.767 65.500 
10 67.000 56.200 63.767 65.667 
Avg. accuracy 69.449 57.850 65.947 65.983 
  
36 
 
Table 12. ANOVA table under three performance criteria. 
 
Performance 
criterion 
Source DF SS MS F test p-value 
 
 
 
TP 
Treatments 
(scheduling 
strategy) 
9 22874.35 2541.59 9.71 0.0000 
Blocks 
(simulation runs) 
29 226263.48 7802.19 29.00 
 
Error 261 68319.45 261.76   
Total 299 317457.28    
 
 
 
MCT 
Treatments 
(scheduling 
strategy) 
9 35934355.76 3992706.20 59.16 0.0000 
Blocks 
(simulation runs) 
29 36732920.06 1266652.42 18.77  
Error 261 17614449.20 67488.31   
Total 299 90281725.02    
 
 
 
NT 
Treatments 
(scheduling 
strategy) 
9 285979633.95 31775514.88 257.38 0.0000 
Blocks 
(simulation runs) 
29 19010708.02 655541.66 5.31  
Error 261 32222426.95 123457.57   
Total 299 337212768.92    
DF: degree of freedom 
SS: sum of squares 
MS: mean squares (variance)  
38 
 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。 
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■  達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 □無 
技轉：□已技轉 □洽談中 □無 
其他：（以 100 字為限） 
1. Yeou-Ren Shiue*, Ruey-Shiang Guh and Ken-Chun Lee (2011/12). Study of SOM-based 
intelligent multi-controller for real-time scheduling. Applied Soft Computing, Vol. 11, No. 8, 
pp. 4569-4580. (SCI, EI) (NSC 98-2221-E-211-007-MY2) (2010 IF=2.084; Computer 
Science, Interdisciplinary Applications, Rank=18/97=18.56%) 
（審查及修訂中之論文） 
2. Yeou-Ren Shiue*, Ruey-Shiang Guh, and Tsung-Yuan Tseng (2011). Study of shop floor 
control system in semiconductor FAB by SOM-based intelligent multi-controller approach. 
Computers & Industrial Engineering. (SCI, EI) (NSC 98-2221-E-211-007-MY2) (2010 
IF=1.543; Engineering, Industrial, Rank=12/38=31.58%) 
3. Yeou-Ren Shiue*, Ruey-Shiang Guh (2011). Development of machine learning–based real 
time scheduling systems: using ensemble based on wrapper feature selection approach. 
International Journal of Production Research. (SCI, EI) (NSC 98-2221-E-211-007-MY2) 
(2010 IF=1.033; Operations Research & Management Science, Rank=31/75=41.33%) 
 
附件二 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/07
國科會補助計畫
計畫名稱: 使用集成式機器學習方法開發即時排程系統
計畫主持人: 薛友仁
計畫編號: 98-2221-E-211-007-MY2 學門領域: 資訊技術及系統整合
無研發成果推廣資料
and Tsung-Yuan 
Tseng (2011). Study 
of shop floor 
control system in 
semiconductor FAB 
by SOM-based 
intelligent 
multi-controller 
approach. 
Computers &amp ；
Industrial 
Engineering. (2010 
IF=1.543 ；
Engineering, 
Industrial, 
Rank=12/38=31.58%)
21. Yeou-Ren 
Shiue*, 
Ruey-Shiang Guh 
(2011). 
Development of 
machine 
learning–based 
real time 
scheduling 
systems: using 
ensemble based on 
wrapper feature 
selection 
approach. 
International 
Journal of 
Production 
Research.(2010 
IF=1.033 ；
Operations 
Research &amp ；
Management 
Science, 
Rank=31/75=41.33%)
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100%  
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
