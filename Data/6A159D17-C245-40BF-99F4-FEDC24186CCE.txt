 行政院國家科學委員會補助專題研究計畫 ■成果報告   □期中進度報告 
 
基於強化學習具滿足服務品質要求之集能無線感測網路動
態電源管理方法及其可重組式硬體實現與模擬 
 
計畫類別：■個別型計畫   □整合型計畫 
計畫編號：NSC   99-2221-E-415-013- 
執行期間：  2010 年 8 月 1日至 2011 年 11 月 30 日 
 
執行機構及系所：國立嘉義大學資訊工程學系 
 
計畫主持人：徐超明 
共同主持人：章定遠 
計畫參與人員：劉政廷、賴建宏、林智全、廖孟哲、簡志偉 
 
 
成果報告類型(依經費核定清單規定繳交)：■精簡報告  □完整報告 
 
本計畫除繳交成果報告外，另須繳交以下出國心得報告： 
□赴國外出差或研習心得報告 
□赴大陸地區出差或研習心得報告 
■出席國際學術會議心得報告 
□國際合作研究計畫國外研究報告 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
            □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
 
中   華   民   國  101 年 2 月 28 日 
附件一 
1 
 
報告內容 
INTRODUCTION 
Embedded systems are widely used in our daily life and for scientific purpose. Embedded systems such as sensor nodes in a 
wireless sensor network usually rely on battery as a primary energy source, and if without a secondary renewable energy source the 
energy source can only be charged before deployment or off-line. The operation life of such wireless sensor node is then decided by 
the pre-stored energy in the battery which limits the comprehensive utilization of wireless sensor network. Even if power saving 
techniques can conserve energy in extending the operational lifetime of the sensor node, sustainable operation for sensor node still 
seems impossible with such limited energy source [1]. 
Owing to the modern technology of renewable energy, harvest energy from environment has been widely applied to the 
embedded system which requires perpetual operation such as wireless sensor network and recently power management techniques 
were designed for the energy harvesting wireless sensor network [2-7]. Most power management techniques not only consider the 
conservation of energy usage but also the balance between harvested and consumed energy. Based on an embedded system model, 
Moser et al. [2, 3] considered the maximization of overall executed time for the periodically tasks on time using the 
greedy-incremental scheduling algorithm. On the other hand, [4-7] considered duty cycle of adjustment of sensing task in wireless 
sensor node as the management to optimize the performance of system and extend its operation life. 
Energy neutrality theorem is proposed by Kansal et al. [4]. With the suitable capacity of energy storage and the proper evaluation 
of the renewable source, the operation life time of the system can be perpetual theoretically. They reported that the consumed 
energy can be compensated by the harvested energy as soon as possible, the net outgoing and incoming of energy thus can be 
maintained at zero level within a given period. With the sufficient capacity of energy storage and the sustainable operation in energy 
neutrality, perpetual operation of wireless sensor node can be expected. And the adaptive duty cycle control (ADC) method is 
applied to dynamically change the exercised duty cycle rate based on the estimation of incoming energy so that the outgoing energy 
can be managed to maintain energy neutrality. 
In this paper, a dynamic power management (DPM) utilizing reinforcement learning with fuzzy reward (RLFR) is proposed for 
energy harvesting wireless sensor network. The proposed RLFR first consider energy harvesting environment as a fuzzy 
environment [8, 9], and use the fuzzy decision processes to model such environment [10]. The modified Q-learning of 
reinforcement learning [11] is then used to learn from the fuzzy environment and a sequence of duty cycle is decided in maintaining 
energy neutrality and improving the energy efficiency. This paper is organized into six sections.  Section II gives the theory of fuzzy 
decision process with fuzzy reward. The system model and the proposed algorithm are discussed in Section III. Configurations for 
simulation are described in Section IV. Section V shows the experimental results with comparing with the ADC method. Finally, 
conclusions are drawn in Section VI. 
FUZZY DECISION PROCESS WITH FUZZY REWARD  
In [10], Yoshida has formulated the Markov decision process (MDP) into the fuzzy decision process (FDP). The state space and 
action space are denoted as S and A respectively, and s is fuzzy state, that is, s:S→[0, 1], and s∈F̃(S), where F̃(S) is the set of all 
fuzzy states. And the α-cut of s̃ is defined as  
sα≡ {x∈S  | s(x)≥ α},  0<α≤1                                (1) 
 
where s(.) is a membership function. The related fuzzy action, a∈F̃(A), is similarly defined. Considering a triple (x, a, x’)∈S×A×S, a 
fuzzy function h ̃(x, a, x’) is used to represent the degree of fuzzy relation between a given x∈S and an another x’∈S associated with 
a executed action a∈A within the given model.  
In MDP, a policy pi can be defined as a function, pi: S×A→[0,1], which associates each (x, a) tuple to a probability value to select 
action a on state x [12]. According to the policy, the state transition probability from x to x’ within time step t on a given a policy pi 
can be formulated as ppit(x, x’). If x and x’ follow the definition of (1), ppit(x, x’) can be rewritten by ppit(x, x’) for constructing the 
FDP. Thus, the structure of the FDP can be modeled by a transition from a fuzzy state to another fuzzy state by mapping ppit: F̃(S) 
→F̃(S) for a given policy, also the degree of fuzzy relation between x and x’ associated with a for the given spaces [10].  
The decision processes consist of five objects which are current state, action, reward, next state and next action, and in the FDP 
which can be rewritten by 
  (s, a, r, s’, a’),  s∈F̃(S), a∈F̃(A), and r∈F̃(R)                     (2) 
 
where R represents real value space, and r= r(s, a) is a prototype of fuzzy reward function based on S×A, which means that the 
fuzzy reward can be obtained by giving a fuzzy state and a fuzzy action using fuzzy reward mapping function.  
3 
 
environment and also has the rewarding for each executed action or discovered state [14]. The decision procedures are that observing 
a current fuzzy state system, deciding an action, receiving fuzzy reward, and going to next fuzzy state, then repeating these 
procedures. We modified the Q-learning algorithm for exploring the model given in previous section as shown in Fig. 2. 
 
 
Fig. 2. The RLFR algorithm applied to the proposed ENPM management 
 
In this algorithm, the round and the step of reinforcement learning are defined as every day and every hour, respectively. The 
function ADC(i) is an action selective function to decide an action ai in the action space, here we adopt soft-max strategy for this 
function which also being used in [15]. In line 6, the reward ri+1 uses (5) to obtain the value. For the Q function, we use Q-learning 
update rule which also being discussed in [16]. The environment setup and important specifications for DPM and RLFR are 
described in the following section. 
CONFIGURATIONS OF SIMULATION 
A.  The environment configuration 
Before the simulation, the environment for energy harvesting system and the available configuration should be established first. In 
this study, the solar panel is adopted as energy harvested device. In modeling solar energy, three major concerns have been considered 
in this study, which are the incident angle, the sunlight intensity, and the current-voltage function. In this study, the energy harvested 
device in used is BSP-112, which can produce power up to 1W [6]. Stochastic profile of atmosphere circumstances, in which the 
stochastic atmosphere variation will decrease the sunlight power exposing onto the solar panel, is also adopted. Fig. 3 shows the 
modeled harvested energy of solar panel for the first week at N23.29°. 
 
Fig. 3. The harvested energy by solar panel of the first week on N23.29° 
The average harvesting power is equal to 102 mW. The average operational power is constrained by the average harvesting power at 
the given voltage supply of 3.6V, thus the average current at the certain operational voltage is 28.3mA.  According to the condition of 
energy neutrality [4], the capacity of the energy storage calculated is 5944mAH.  Thus, a commercial battery with capacity of 
7000mAH at 3.6V is a good choice for simulation. The possible duty cycle to complete the node’s sensing task is among the four crisp 
levels of 0%, 30%, 65%, and 100%, respectively, for consuming   powers at 0mW, 173mW, 374mW, and 576mW. 
B. RLFR Configuration 
The aim of energy neutrality is to find a balance between the harvested energy and the consumed energy such that sustainable 
operation of wireless sensor network can be obtained. To achieve the energy neutral operation, a serial of data sensing duty cycle over 
a certain time period must be adaptively determined by the DPM unit in responding to the harvested energy [4]. In this study, three 
fuzzy state variables, SHE, the harvested energy from solar panel, SDE, the difference energy between the harvested and the consumed 
energy, and SRE, the remained energy in the energy storage, are designed for the energy harvesting wireless sensor node.  The range of 
SHE, SDE, and SRE, are in [0, 1200] joule, [-2100, 1200] joule, and [0, 7000] mAh, respectively. The three state variables are fuzzified 
according to the remained energy in the storage by assigning among high (H), medium (M), and low (L), in describing their fuzzy 
membership and reward functions as shown in Fig. 4.  Fig. 4 (a) shows the remained level of energy storage for deciding which 
suitable fuzzy reward, which is shown in Fig. 4 (b), to be applied in the learning process. According to Fig. 4 (b), the best reward 
will occur while the SRE is low/high and SDE is positive high/negative low, respectively,  and the SRE is medium and SDE is closed to 
zero. 
1 Configure all fuzzy state, fuzzy action, fuzzy reward, 
           learning parameter, and Q(s,a) 
2 For every day, initialize state si 
3 Repeat every timeslot i 
4    Assign duty cycle action ai:=ADC(i) using soft-max 
        and keep the action until timeslot i ends 
5    Receive ri+1 and Observe si+1 
6    Update Q(si , ai) using below iteration formula 
)),(max(),()1(),( 111
1
+++
+
++−= ii
a
iiiii asQrasQasQ
i
γηη  
7    Update state si = si+1 
8 Go to line 3 until i = T 
9 Go to line 2 until the given days have been done 
 
5 
 
 
 
(a) (b) 
Fig. 6. Results of long term simulation showing day 180 to 360. Daily average (a) RBE and (b) EDC of the sensor node. 
By examining Fig. 6 (a), it can be seen that the RBE of RLFR maintains with 55%-60% in the 2nd half year, while the RBE of 
ADC is around 50%.  Fig. 5 (b) shows that the daily average EDC of both RLFR and ADC are cycling according to the seasonal 
change of solar strength, where day 230 to 330 are during the winter season with weaker solar strength, yet both methods maintain 
RBE at certain levels. Table II shows the quantitative comparison of experimental results for the 2nd half year in term of average RBE 
and EDC. By inspecting table II,  one can see that RLFR achieves better average RBE of 57.4% in comparing with that of ADC, 
which is 49.9%, while maintains similar average EDC around 45%. Table II shows that after long term of experiment, the RLFR 
outperforms the ADC in terms of average RBE, which indicates that the  RLFR not only can maintain energy neutrality operation 
but it achieves better energy utilization as well. 
TABLE II 
PERFORMANCE COMPARISON DURING DAY 180 TO 360 
Parameter 
Method 
Average EDC Average RBE 
RLFR 45.2% 57.4% 
ADC 45.4% 49.9% 
CONCLUSION 
In this paper, a dynamic power management utilizing reinforcement learning with fuzzy reward (RLFR) for energy harvesting 
wireless sensor network has been proposed to maintain energy neutrality operation, and to improve the energy utilization. 
According to the simulation results, the average residue energy of the proposed method is higher than other existing method by 
7.5% under almost the same average exercised duty cycle at 45%, and under the same simulation environment. Simulation results 
exhibit that the RLFR not only satisfies the sensing requirement in maintaining energy neutrality, but it also achieves better energy 
utilization in terms of residual battery energy in comparing with another existing dynamic power management method. 
 
REFERENCES 
1. K. S. Jeong, W. Y. Lee, and C. S. Kim, “Energy management strategies of a fuel cell/battery hybrid system using fuzzy 
logics,” J. of Power Sources, vol. 145, pp. 319-326, 2005. 
2. Moser, L. Thiele, D. Brunelli, and L. Benini, “Adaptive power management in energy harvesting systems,” in Proc. of Design, 
Automation & Test in Europe Conf. & Exhib., pp. 1–6, 2007. 
3. Moser, J.-J. Chen, and L. Thiele, “Reward maximization for embedded systems with renewable energies,” in Proc. of IEEE 
Inte. Conf. on Embedded and Real-time Computing Systems and Applications, Design, Automation & Test in Europe Conf. & 
Exhib., pp. 247–256, 2008. 
4. Kansal, J. Hsu, S. Zahedi, and M. B. Srivastava, “Power management in energy harvesting sensor networks,” ACM Trans. on 
Embedded Computing System, vol. 6, no. 4, article 32, 2007. 
5. Vigorito, D. Ganesan, and A.G. Barto, “Adaptive control of duty-cycling in energy-harvesting wireless sensor networks,” in 
Proc. of IEEE Communications Society Conference on Sensor, Mesh, and Ad Hoc Communications and Networks, 2007. 
6. R. C. Hsu, C.-T. Liu, W.-M. Lee, “Reinforcement learning-based dynamic power management for energy harvesting wireless 
sensor network,” Lecture Notes in Artificial Intelligence, vol. 5579, pp. 399-408, 2009. 
7. R. C. Hsu, C.-T. Liu, K.-C. Wang, and W.-M. Lee, “QoS-aware power management for energy harvesting wireless sensor 
network utilizing reinforcement learning,” in Proc. of Inte. Conf. on Computational Science and Engineering, pp. 537-542, 
2009. 
8. L. A. Zadeh, “Fuzzy Sets,” Information and Control, vol. 8, pp. 338-353, 1965. 
7 
 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。 
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明： 
本計畫中已完成了基於強化學習具滿足服務品質要求之集能無線感測網路動態
電源管理方法實現與模擬，並發表一篇於 IEEE IECON2011 研討會論文，其中
並投稿於 IEEE Transactions on Industrial Informatics 期刊，經第一次審查，後
修改後再第二次送審中。 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：□已發表 ■未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 □無 
技轉：□已技轉 □洽談中 □無 
其他：（以 100 字為限） 
本研究已投稿於 IEEE Transactions on Industrial Informatics 期刊，經第一次審
查，後修改後再第二次送審中。 
附件二 
9 
 
 
國科會補助計畫衍生研發成果推廣資料表 
日期：101 年 2 月 28 日 
國科會補助計畫 
計畫名稱：基於強化學習具滿足服務品質要求之集能無線感測網
路動態電源管理方法及其可重組式硬體實現與模擬 
計畫主持人：嘉義大學資工系 徐超明         
計畫編號：NSC 99-2221-E-415-013-  領域：資訊 
研發成果名稱 
（中文）基於強化學習具滿足服務品質要求之集能無線感測網路
動態電源管理方法 
（英文） 
成果歸屬機構 國立嘉義大學/徐超明 
發明人 
(創作人) 
徐超明、劉政廷 
技術說明 
（中文） 
 
不可揭露中 
 
（200-500 字） 
（英文） 
不可揭露中 
產業別 
資訊業、工業 
技術/產品應用範圍 
無線感測網路 
技術移轉可行性及預期
效益 
極大 
     註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
 
附件三 
11 
 
轉機費用，另外也為了節省膳宿費用，我們選擇跨夜航班 7日晚上出發 8日上午抵
達，也選擇與會議中心緊臨之旅館住宿，以節省當地往來通勤時間。抵達旅館後隨
即前往會議中心報到及與會。下午發表論文與參與相關議程後，於市區略作遊覽至
傍晚。9日亦參與相關議程，敝人並於當晚即返程，前往機場搭機返國，於 11 日上
午返抵國門，結束本次的學術旅程。 
二、與會心得 
IECON 會議為 IEEE 工業電子學會的旗艦型會議，與會之學者與先進大多為硬體與控
制等的專門，而本研究的專門領域是在軟體與演算法設計方面。從聆聽發表的過程
中也敝人亦同時在思考，別的研究如何使用軟體的方式來達成或進行實驗，以及本
研究如何以硬體方式達成目標。因為大部份的研究都以昂貴的硬體平台為基礎，但
是如以資訊工程模擬方式來實現，可以省下較多之硬體設備費用，但卻缺乏無具體
之實施，在這方面是較為缺乏的。如本次計畫，因硬體模擬需採購之設備甚多，而
鈞會僅核定不到 1/4 之經費，因而僅能購置些許設備，而考量經費不足以購置 
硬體模擬環境，因此捨棄原規劃之硬體模擬，而以軟體模擬完成計畫，並達成目標。
未能如因受到硬體與設備的限制，如何讓研究持續進行，採軟體模擬方式完成目標
未嘗不是未來可以進行的方向。 
IECON2011 的會議議程相當廣泛，從電力電子、自動控制、自動化、機器人的領域皆
有含蓋，總結來說這次參與 IECON 研討會是有相當豐富的收穫。至於本次會議的發
表，因為行程短促，無法配合大會原訂之口頭發表時間，故寫信請大會安排海報張
貼，原本預期可能會因此流失了與同領域之學者先進討論交流的機會，但仍有二位
13 
 
論文摘要： 
Abstract-This paper considers a scenario that wireless sensor node is powered by harvesting energy with 
the characteristics of ambiguity and uncertainty. Reinforcement learning with fuzzy reward (RLFR) is 
used in this study for the dynamic power management of such energy harvesting wireless sensor node. 
By interacting with the given environment, the RLFR adjusts the duty-cycle in data sensing task 
according to the variable incoming energy related signals. The outcomes of these interactions are 
evaluated by fuzzy reward that express how well the duty-cycle adjustments in satisfying given 
requirement of energy neutrality. Simulation results show that the RLFR not only satisfies the sensing 
requirement in maintaining energy neutrality, but it also achieves better energy utilization in terms of 
residual battery energy in comparing with another existing dynamic power management method.   
Keywords—Dynamic Power Management, Energy Harvest, Fuzzy Reward, Reinforcement Learning, 
Wireless Sensor Network. 
即前往會議中心報到及與會。下午發表論文與參與相關議程後，於市區略作遊覽至
傍晚。9日亦參與相關議程，敝人並於當晚即返程，前往機場搭機返國，於 11 日上
午返抵國門，結束本次的學術旅程。 
二、與會心得 
IECON 會議為 IEEE 工業電子學會的旗艦型會議，與會之學者與先進大多為硬體與控
制等的專門，而本研究的專門領域是在軟體與演算法設計方面。從聆聽發表的過程
中也敝人亦同時在思考，別的研究如何使用軟體的方式來達成或進行實驗，以及本
研究如何以硬體方式達成目標。因為大部份的研究都以昂貴的硬體平台為基礎，但
是如以資訊工程模擬方式來實現，可以省下較多之硬體設備費用，但卻缺乏無具體
之實施，在這方面是較為缺乏的。如本次計畫，因硬體模擬需採購之設備甚多，而
鈞會僅核定不到 1/4 之經費，因而僅能購置些許設備，而考量經費不足以購置 
硬體模擬環境，因此捨棄原規劃之硬體模擬，而以軟體模擬完成計畫，並達成目標。
未能如因受到硬體與設備的限制，如何讓研究持續進行，採軟體模擬方式完成目標
未嘗不是未來可以進行的方向。 
IECON2011 的會議議程相當廣泛，從電力電子、自動控制、自動化、機器人的領域皆
有含蓋，總結來說這次參與 IECON 研討會是有相當豐富的收穫。至於本次會議的發
表，因為行程短促，無法配合大會原訂之口頭發表時間，故寫信請大會安排海報張
貼，原本預期可能會因此流失了與同領域之學者先進討論交流的機會，但仍有二位
外國籍教授前來表示我們的研究令他感到有興趣，另也有 1位新加坡的教授，他是
再生能源的專家，對我們的研究提出很多建設性的意見，有助於我們在未來深入研
究時可以加以考量，他也提到或許會有合作的機會。另外並與幾位中國前來的研究
生彼此交流研究心得，發現他們幾乎是整個實驗室的研究生都一起出來發表，這也
論文摘要： 
Abstract-This paper considers a scenario that wireless sensor node is powered by harvesting energy with 
the characteristics of ambiguity and uncertainty. Reinforcement learning with fuzzy reward (RLFR) is 
used in this study for the dynamic power management of such energy harvesting wireless sensor node. 
By interacting with the given environment, the RLFR adjusts the duty-cycle in data sensing task 
according to the variable incoming energy related signals. The outcomes of these interactions are 
evaluated by fuzzy reward that express how well the duty-cycle adjustments in satisfying given 
requirement of energy neutrality. Simulation results show that the RLFR not only satisfies the sensing 
requirement in maintaining energy neutrality, but it also achieves better energy utilization in terms of 
residual battery energy in comparing with another existing dynamic power management method.   
Keywords—Dynamic Power Management, Energy Harvest, Fuzzy Reward, Reinforcement Learning, 
Wireless Sensor Network. 
 Conference accommodation and travel information will be soon be updated 
on the conference website. Please check the website regularly for these 
and other program updates. 
 
Thank you very much for your contribution.  We look forward to our 
meeting in Melbourne, Australia! 
 
Sincerely, 
 
IEEE IECON 2011 Organizing Committee 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：徐超明 計畫編號：99-2221-E-415-013- 
計畫名稱：基於強化學習具滿足服務品質要求之集能無線感測網路動態電源管理方法及其可重組式硬
體實現與模擬 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 4 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
