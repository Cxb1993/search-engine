 2 
ύЎᄔा 
ࣁΑှ،εࠠၗ਑܌़ғޑୢᚒǴךॺගрΑ΋ঁБݤǴ٬Ҕ،฼ᐋٰϩപၗ਑ޜ໔Ǵ
٠Ъӧϩപޑ୔༧ύ૽ግЍ࡭ӛໆᐒȐSVMȑǶᗨฅԖ೚ӭБݤ೿ёаϩപၗ਑ޜ໔Ǵךॺ
ᡉҢ،฼ᐋёаࣁ SVMӧεࠠၗ਑ޑ૽ግ஥ٰኧ໨ӳೀǶ२ӃǴ،฼ᐋёа٬ҔԾρޑБ
Ԅٰϩᜪࢌ٤ၗ਑ǴӢԶᕭ෧Α SVMӧഭᎩၗ਑ޑ૽ግਔ໔ǶಃΒǴ،฼ᐋёаԖਏӦ،
ۓന٫ୖኧॶǴӢԶගϲෳ၂ޑ҅ዴ౗ǶಃΟǴ،฼ᐋϩപБݤёаᏤ़р၀ϩᜪᏔޑ௢
ቶᒱᇤ΢ज़Ƕଞჹ྽Ϟߚጕ܄ SVMёаೀ౛ޑၗ਑Ǵךॺ܌ග᝼ޑБݤёаගϲኧί७ޑ
૽ግೲࡋǴ٠Ъ٩ฅᆢ࡭࣬྽ޑෳ၂҅ዴ౗Ƕ 
ᜢᗖຒǺΒϩݤ،฼ᐋǴ௢ቶᒱᇤ΢ज़Ǵεࠠၗ਑ᐒᏔᏢಞǴࣚጕࣁҁޑ౛ፕǴኳԄ
ϩᜪǴ،฼ᐋϩപǴЍ࡭ӛໆᐒǴVC౛ፕ 
मЎᄔा 
To handle problems created by large data sets, we propose a method that uses a decision tree 
to decompose a given data space and train SVMs on the decomposed regions. Although there are 
other means of decomposing a data space, we show that the decision tree has several merits for 
large-scale SVM training. First, it can classify some data points by its own means, thereby re-
ducing the cost of SVM training for the remaining data points. Second, it is efficient in determin-
ing the parameter values that maximize the validation accuracy, which helps maintain good test 
ac-curacy. Third, the tree decomposition method can derive a generalization error bound for the 
classifier. For data sets whose size can be handled by current non-linear, or kernel-based, SVM 
training techniques, the proposed method can speed up the training by a factor of thousands, and 
still achieve comparable test accuracy.!
Keywords: binary tree, generalization error bound, large-scale machine learning, mar-
gin-based theory, pattern classification, tree decomposition, support vector machine, VC theory 
ൔ֋ϣ৒ 
1. Introduction 
We present here a new method for solving large-scale SVM problems. The presented method 
decomposes a large data set into a number of smaller ones and trains SVMs on each of them. 
Since this method uses a decision tree to decompose a data set, we refer to it as the decision-tree 
support vector machine (DTSVM) method. For data sets whose size can be handled by current 
non-linear SVM training techniques, DTSVM can speed up the training by a factor of thousands, 
and still achieve comparable test accuracy. 
The role of the decision tree as a decomposition scheme can have the following benefits 
when dealing with large-scale SVM problems. First, the decision tree may decompose the data set 
so that certain decomposed regions become homogeneous; that is, they contain samples of the 
 4 
to DTSVM, except it decomposes a data set by a random partition.  
To measure speedup factors, we took LIBSVM’s training time as the baseline. The speedup 
factor of DTSVM on the medium-size data sets was between 4 and 3,691 for one-against-one 
(1A1) training, which were significantly higher than that of all the compared methods, except 
CART. However, CART achieved poor test accuracy rates in many data sets. The results are 
shown in Tables 2-4. 
TABLE 1. The medium-size data sets used in our experiments. 
Data set No. of LabelsNo. of SamplesNo. of Features 
Pen Hand Written (PHW) 10 10,992 16 
Letter 26 20,000 16 
Shuttle 7 58,200 9 
Poker 10 25,010 10 
Census Income (CI) 2 45,222 14 
News20 20 19,927 62,060 
KDD CUP 10% (KDD-10%) 5 494,021 41 
 
TABLE 2. Training times of the seven methods, expressed in seconds. Training type = 1A1. 
CART, DTSVM and RDSVM outperformed the other methods. 
 PHW  Letter  Shuttle Poker CI  News20  KDD-10% 
CART 0.4 4.3 0.3 12.4 13.1 306.0 10.3 
DTSVM 275 768 23 3,533 5,209 3,053 371 
RDSVM 278 841 542 9,234 3,174 5,223 4,014 
CBD 697 2,598 303 106,819  215,219 39,590 57,789 
Bagging   2,204 7,575 2,100 7,979 6,100 35,176 17,123 
LASVM 924 4,001 5,670 546,417 492,764 23,339 1,261,639 
LIBSVM 1,192 4,157 5,096  1,307,667 315,130 27,071 1,369,600 
 
TABLE 3. Speedup factors of all the seven methods, except LIBSVM that is taken as the base-
line. Training type = 1A1. CART, DTSVM and RDSVM outperformed the other methods. 
 PHW Letter Shuttle Poker CI  News20   KDD-10% 
CART  3,320.3   974.4 ʳ 19,157.9 105,143.3  24,066.7 88.5 133,009.6 
DTSVM 4.3 5.4 221.6 370.1 60.5 8.9 3,691.6 
RDSVM 4.3 4.9 9.4 141.6 99.3 5.2 341.2 
CBD 1.7 1.6 16.8 12.2 1.5 0.7 23.7 
Bagging 0.5 0.5 2.4 163.9 51.7 0.8 80.0 
LASVM 1.3 1.0 0.9 2.4 0.6 1.2 1.1 
 
TABLE 4. Test accuracy rates of the seven methods. Training type = 1A1. CART performed 
poorly on several data sets; while CBD and Bagging lagged behind DTSVM on some data sets. 
 PHW Letter Shuttle Poker CI News20   KDD-10% 
CART 95.51%ʳ 87.18%ʳ 99.95% 49.72% 80.97%   46.50%ʳ 99.95%ʳ
DTSVM   99.42%ʳ  97.60%ʳ  99.93%  57.56%   84.81% 83.22%ʳ 99.96%ʳ
RDSVM 99.10%ʳ 97.54%ʳ 99.61% 57.18% 83.48% 83.10%ʳ 99.43%ʳ
CBD 99.63%ʳ 95.25%ʳ 99.85% 56.75% 84.08% 75.23%ʳ 99.91%ʳ
 6 
DTSVM is not simply built on the decomposition of a data set, but also on using a decision tree 
as the decomposition scheme. Moreover, comparing the kernel-based DTSVM with the li-
near-based LIBLINEAR demonstrates the value of non-linear SVM as a classification method. 
One noteworthy result is that LIBLINEAR achieved the best test accuracy rate on “Webspam”, 
presumably because a linear model fits this data set rather well. However, to verify this assump-
tion, we need to compare the test accuracy rates of linear and non-linear models. DTSVM offers 
us an opportunity to make such a comparison. 
ୖԵЎ᝘ 
[1] F. Chang, C.-Y. Guo, X.-R. Lin, and C.- J. Lu, Tree decomposition for large-scale SVM 
Problems, Journal of Machine Learning Research, to appear. 
[2] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, Classification and Regression 
Trees. Chapman and Hall, 1984. 
[3] N. Panda, E. Y. Chang, and G. Wu, Concept boundary detection for speeding up SVMs, Proc. 
International Conference on Machine learning, pp. 681-688, 2006. 
[4] L. Breiman, Bagging predictors, Machine Learning, vol. 262, pp.123-140, 1996. 
[5] A. Bordes, S. Ertekin, J. Weston, and L. Bottou, Fast kernel classifiers with online and active 
learning, Journal of Machine Learning Research, vol. 6, pp. 1579-1619, 2005. 
[6] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin, LIBLINEAR: A library for 
large linear classification, Journal of Machine Learning Research, vol. 9, pp.1871-1874, 
2008. 
[7] A. Rida, A. Labbi, and C. Pellegrini, Local experts combination through density 
decomposition, Proceedings of International Workshop on AI and Statistics, 1999. 
[8] L. Breiman, Bagging predictors. Machine Learning, 262:123-140, 1996. 
[9] V. Tresp, A Bayesian committee machines, Neural Computation, 12:2719-2741, 2000. 
[10]R. Colbert, S. Bengio, and Y. Bengio, A parallel mixture of SVMs for very large scale 
problems, Neural Computation, 14(5): 1105-1114, 2002. 
[11]H. P. Graf, E. Cosatto, L. Bottou, I. Dourdanovic, and V. Vapnik, Parallel support vector 
machines: the cascade SVM, L. K. Saul, Y. Weiss and L. Bottou, editors, Advances in Neural 
Information Processing Systems, MIT Press, 2004. 
[12]S. Knerr, L. Personnaz, and G. Dreyfus. Single-layer learning revisited: A stepwise procedure 
for building and training a neural network, J. Fogelman, editor, Neurocomputing: Algorithms, 
Architectures and Applications, Springer-Verlag, 1990. 
[13]L. Bottou, C. Cortes, J. Denker, H. Drucker, I. Guyon, L. Jackel, Y. LeCun, U. Müller, E. 
Sackinger, P. Simard, and V. Vapnik, Comparison of classifier methods: A case study in 
 8 
decision tree is constructed in a recursive manner; hence, obtaining a tree with a larger size of   
does not require the reconstruction of a decision tree corresponding to that size of  . 
Using a decision tree also reduces the cost of searching for the optimal values of 
SVM-parameters. Searching for these values is important, but it takes a tremendous amount of 
time, especially when training non-linear SVMs. To the best of our knowledge, no data-reduction 
method has attempted to reduce the cost of this operation. Our strategy involves training SVMs 
with all combinations of SVM-parameter values, but only for decomposed regions with an initial 
 -level. The optimal values of the SVM-parameters obtained at this level are not necessarily the 
same as those obtained at higher levels. However, we observe that the best values for a higher 
level are usually among the top-ranked values for the initial level. Therefore, when we want to 
train SVMs for a higher  -level, we only train them with the top-ranked values obtained for the 
initial level. Given the n
2
-complexity of SVM training, restricting the full search of 
SVM-parameter values to regions with the initial  -level certainly reduces the SVM training time. 
In fact, our experiments showed that such savings were possible even when the optimal  -level 
was higher than that of the full size data set. 
In the experimental study, we divided each data set into training, validation and test compo-
nents. We then used the training component to build DTSVM classifiers, the validation compo-
nent to determine the optimal parameters, and the test component to measure the test accuracy. 
We adopted two types of SVM training: one-against-one (1A1) [12] and one-against-others (1AO) 
[13]. Furthermore, we built non-linear SVMs on the data sets. When evaluating the DTSVM me-
thod, we found it could train DTSVM classifiers that achieved comparable test accuracy rates to 
those of SVM classifiers. For seven medium-size datasets, in which the largest number of sample 
was 494K and the largest number of feature was 62K, DTSVM achieved speedup factors between 
4 and 3,691 for 1A1 training, and between 29 and 5,775 for 1AO training. We also found that 
DTSVM achieved much higher speedup factors than several data reduction methods and numeri-
cal methods. To demonstrate that DTSVM can train classifiers efficiently for larger data sets, we 
applied it to four large-size data sets in which the largest number of samples was 4.9M and the 
largest number of features was 16.6M. For all the datasets, DTSVM could complete 1A1 training 
and 1AO training within 18.25 hours. Note that the training time included the time required to 
build a decision tree, the time to train SVMs on all the leaves, and the time to search for the op-
timal parameters. 
99年度專題研究計畫研究成果彙整表 
計畫主持人：張復 計畫編號：99-2221-E-001-017- 
計畫名稱：提升學習機器的可擴展性 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
F. Chang, C.-Y. 
Guo, X.-R. Lin, 
C.-C. Liu, and 
C.-J. Lu, Tree 
decomposition 
for large-scale 
SVM problems, 
TAAI 2010, 
Hsinchu, Taiwan.
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 1 100% 
F. Chang, C.-Y. 
Guo, X.-R. Lin, 
and C.- J. Lu, 
Tree 
decomposition 
for large-scale 
SVM Problems, 
Journal of 
Machine Learning 
Research, to 
appear.此期刊的
impact factor 為
5.952. 
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
國外 
論文著作 
專書 0 0 100% 章/本  
 
