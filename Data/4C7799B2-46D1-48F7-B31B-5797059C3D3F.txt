 2 
image processing are important researches 
owing to their wide varieties of application. 
Among all of these applications, visual 
tracking has become a great interest in this 
domain. The speed of capturing an image 
must be as fast as possible to detect the 
movement of the object in real-time. 
However, the camera has a constraint on the 
number of capturing image per second. The 
purpose of this project is to deal with this 
problem and implement the visual servo 
control system. This project introduces an 
efficient approach of a real-time moving 
object tracking by the two eyes of the MHER 
and describes the design of an algorithm 
based on the neural network structure.  
 
II. Hardware 
   
The MHER tracking system is built 
with two cameras and five motors to emulate 
human eyeballs as shown in Fig. 1. The 
MHER adopts five FAULHABER DC 
servomotors  to steer the MHER in 
tracking system. With RS-232 interface, the 
controller of DCservomotors is executed 
by the motion control card, MCDC 3006S, in 
a positioning resolution of 0.18°. Therefore, 
the MHER has two pan-direction video 
cameras, a conjugated tilt motor and a 
pan-tilt neck. The range of pan is 
approximately 120 degrees, and tilt is 
approximately 60 degrees. The size of the 
head is about 25 cm width and 10 cm height 
for the eye part. 
 
III. Intelligent Hand Gesture Recognition 
   
The system structure of the proposed 
hand gesture recognition system [4] includes 
image data retriever, posture data generator, 
posture state encoder, and gesture analyzer. 
The main goal of the proposed hand gesture 
recognition system is to recognize hand 
gestures captured by a CCD camera. This 
project will focus on nine gestures, named 
upward, downward, turn left, turn right, left 
around, right around, follow, learning and 
warming as shown in Fig. 2. 
 
Fig. 1. HMER. 
 
Fig. 2. Hand gestures. 
To describe a hand gesture by a posture 
state sequence, it is often based on a 
fixed-length sequence of posture states. 
However, a hand gesture usually happens 
during an uncertain time; for example one 
hand posture can change to another hand 
posture quickly or slowly and thus it is 
difficult to represent a hand gesture by a 
fixed-length state sequence. To solve this 
problem, the hand gesture analyzer contains a 
trigger net and a gesture classifier,  
developed to learn a posture state sequence 
unfixed length. The single state eliminator 
and the repeated state processor are 
implemented by the feedforward neural 
network, while the trigger net is implemented 
by the recurrent neural network. 
There are two kinds of clock mode, 
synchronous and asynchronous, used in this 
gesture recognition system. If repeated state 
retriever is applied for feed-forward classifier, 
then the gesture recognition system adopts 
synchronous clock mode for recurrent 
classifier; otherwise, it adopts asynchronous 
clock mode. 
 4 
 
Fig. 8. Set-point control of the neck. 
The object tracked by the Eye-robot is 
projected by an overhead projector in a 
screen. The speed of the object motion 
velocity is 28 cm/sec. The distance between 
the Eye-robot and screen is set as 240 cm and 
the proportion of the vision object in the 
Eye-robot is designed as 10cm x 10cm 
shown in Fig. 5. 
To detect the object on the screen, the 
image of the left-eye camera is first 
transformed into a gray level image with gray 
level intensity distribution shown in Fig. 6. 
Then the object can be extracted. 
The neural-network-based controller is 
achieved via offline training. The input of the 
neural network is the object center p and the 
output is the tracking index V related to the 
driving voltage of the Eye-robot. The neural 
network controller receives the error e 
between the current horizontal position xc 
and the desired position xd of the target and 
provides a tracking index V to drive the 
Eye-robot. The neural network controller 
design will emphasize on the training pattern 
driving the Eye-robot to trace the object. 
With the patterns shown in Fig. 7, the 
human visual motion can be mimicked. The 
left eye is selected as the dominant eye in this 
project and is designed to have the ability to 
find the object in searching mode. Once the 
object is detected, the eyes and the neck will 
work together to trace the object. It is clear 
that the tracking motion is mainly executed 
by the neck when the object appears at the 
position far away the vision center xc, while 
the dominant eye and right eye play the roles 
of concentration on the object. 
After the offline training, the trained NN 
will be applied to the Eye-robot tracking, i.e., 
the object is kept in the image center. During 
the object tracking, the maximal velocity of 
the neck is set to be vmax=1500rpm/min. It is 
clear from Fig. 8 that the motors employed to 
steer the two cameras have also reached the 
control goal to locate the object around the 
visual center with an error near to zero. 
Reference: 
 
[1]  Y. Wu and T. S. Huang, “Hand modeling 
analysis and recognition for vision-based human 
computer interaction,” IEEE Signal Process 
Mag.—Special Issue on Immersive Interactive 
Technology, vol. 18, no. 3, pp. 51–60, 2001. 
[2]  N. Badler, “Temporal Scene Analysis - 
Conceptual Descriptions of Object Movements,” 
Report TR 80, 1975. 
[3]  R. Brooks, “Symbolic reasoning among 3D 
models and 2D images,” Artificial Intelligence, 
pp. 285-348, 1981. 
[4]  洪新光，智慧型手勢辨識系統設計，交通大學
碩士論文，2009。 
[5]  徐傳源，應用於眼球機器人之智慧型多軸追蹤
控制，交通大學碩士論文，2010。 
無衍生研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
