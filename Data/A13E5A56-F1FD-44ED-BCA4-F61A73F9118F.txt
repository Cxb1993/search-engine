  1 
 
行政院國家科學委員會專題研究計畫成果報告 
低延遲叢集計算於潤滑分析之性能研究 
Performance Study of a Low Latency Cluster Computing for Lubrication Analysis  
計畫編號：NSC 96-2221-E-182-037 
執行期限：96年 8月 1日至 97年 7月 31日 
主持人：王能治   長庚大學 機械工程學系 
 
摘要     
本研究利用一個低延遲的電腦叢集來
有效的降低潤滑分析的計算時間。在計算過
程中，平行計算的加速率和可擴充性受限於
系統分派工作量的大小。能有效的降低分派
工作量的大小，卻不影響平行計算的加速
率，則取決於叢集電腦間之計算節點的通訊
延遲的長短。 
在一個共享記憶體處理的系統，通常氣
體軸承的分析可以經由平行計算來處理多
執行緒的程式，而降低運算時間。然而，單
機的共享記憶體處理的系統，因為受限於硬
體因素，其擴充能力有很大的限制。而一般
採用以標準網路介面 (Gigabit Ethernet)連
接計算節點的電腦叢集，則可能因為節點間
的訊號延遲過大，在有些應用上不容易取得
良好的平行效率。 
本研究採用多台低延遲介面(InfiniBand)
的電腦，以簡易的叢集套件(Cluster OpenMP)
將 OpenMP語法延伸使用於電腦叢集。比較
低延遲介面與標準網路介面的電腦叢集，前
者在小工作量上的平行效率有明顯的優
勢。在氣體軸承的分析上，當分配的工作量
較大時，平行效率在前述二系統中則無明顯
的區別。低延遲介面加上使用簡易的叢集套
件，能將遲叢集計算容易且有效的應用於許
多與磨潤相關的研究。 
 
關鍵字 
    叢集運算、氣體軸承、低延遲 
 
ABSTRACT 
This study presents an air bearing 
analysis by means of a low latency computer 
cluster to reduce the execution time in the air 
bearing analysis. In a parallel computing, the 
speedup and scalability of the computing is 
influenced by the size (granularity) of divided 
tasks. And the size of task granularity (the 
smaller the better) to be used to obtain useful 
speedup is determined by the communication 
latency among the computing nodes in the 
cluster.  
In a shared memory processing (SMP) 
system, the execution time of air bearing 
analysis usually can be minimized by running 
multithreaded application in parallel. However, 
the computing power in a standalone SMP 
system is usually not easy to scale up. On the 
other hand, a cluster with standard Ethernet 
connection may have poor parallel efficiency 
due to a relatively large latency in nodal 
communication.  
In this study, a user-friendly Cluster 
OpenMP software is used to extend the 
OpenMP programming model to a cluster with 
low latency InfiniBand interface. The 
InfiniBand cluster delivers a marked 
improvement over a Gigabit Ethernet cluster 
in the cases of fine-grain applications. 
However, the difference in parallel efficiency 
in coarse-grain applications is negligible in the 
illustrated air bearing computation. It is also 
noted that a cluster running Cluster OpenMP 
with low latency connection can be easily 
extended to a general application of cluster 
computing in many tribological studies.  
 
KEY WORDS 
 Cluster Computing, Air Bearing, Cluster 
OpenMP, Low Latency  
 
INTRODUCTION 
In order to reduce the execution time of a 
numerical simulation a tribological study may 
require a cluster of computers to carry out the 
computation in parallel. The use of cluster 
  3 
 
THE CLUSTER OPENMP (CLOMP) 
The OpenMP programming paradigm is 
designed to support multi-platform SMP 
parallel programming in Fortran or C/C++. In 
the past, the OpenMP has been confined to 
SMP machines and teamed with MPI 
technology to make use of a system of 
multiple commodity SMP machines. Recently, 
the Cluster OpenMP (CLOMP) developed by 
Intel company to be used with Intel's compiler 
is an implementation of OpenMP that can 
make use of multiple network-connected SMP 
machines without resorting to a means of 
message passing. The CLOMP configured 
compiler in most cases can turn an OpenMP 
program into a CLOMP compatible execution 
code when the CLOMP option is turned on 
during the compilation phase. In some cases, 
some variables may have to be declared 
sharable among the threads in the cluster in 
order to execute the CLOMP program 
correctly. A step-by-step documentation of the 
variable-made-sharable operation can be 
found in the CLOMP user guide [3]. 
The parallel efficiency of the CLOMP 
cluster will almost certainly be worse than that 
of ordinary OpenMP system due to 
communication latency among the nodes. 
However, the scalability (the ability to scale 
up the number of processors) of the CLOMP 
is the main advantage of using such software, 
which is important for a complex problem. 
Therefore, the latency of networking plays an 
important role in evaluating the parallel 
computing performance. The speedup defined 
in the following equation, Eq. (5), can be used 
to identify the parallelization performance of a 
program executed in a cluster, in which each 
node may have multi-processor or multi-core 
setup. In a cluster having n computing 
processors/cores, the speedup can approach n 
in ideal conditions. 
 
)(
)(  
processorsnTimeExecution
processoroneTimeExecutionSpeedup =   (5) 
 
THE COMPUTER CLUSTER 
In this study, the cluster consists of four 
nodes (Fig. 1), and each node has a 2.8 GHz 
quad-core processor and 4 GB main memory. 
Thus, the maximum number of available 
threads in a parallel computing concurrently is 
16. The four nodes in the cluster are linked 
either by InfiniBand (Fig. 2) or Gigabit 
Ethernet (1 Gbps) interface. Fig. 3 shows the 
switches for networking the PCs to form the 
cluster. Only one interface was used in one 
instance during the comparison tests.  
The IBA specification defines a 
connection between processor nodes and high 
performance (low latency and wide bandwidth) 
I/O nodes. The IBA can be used to build a 
parallel system from a small size cluster to a 
supercomputer. The IBA is a standardized 
method which would benefit greatly to handle 
IP on IB fabrics (IPoIB).  
In a cluster computing, latency is the 
time for memory to respond to a read or write 
request. The IBA's ultra-low latencies, with 
measured delays of one to three ms, which are 
much lower than that of the standard Ethernet 
connection (50 ms) [1]. Therefore, many 
supercomputers are built with the IBA to 
realize high performance distributed systems. 
Bandwidth is the number of bytes that can be 
read or written per second in a network 
communication. The bandwidth of IBA can be 
from 2.5 up to 30 Gbps under different linking 
modes [4]. The ideal bandwidth of Gigabit 
Ethernet, as its name suggested, is 1 Gbps [4]. 
   
RESULTS & DISCUSSION 
The result shows the speedups obtained 
by the IBA are better or equal than those of 
the Gigabit Ethernet for all tested cases. Fig. 4 
shows the speedup distribution under various 
task loads in the four-node cluster. It can be 
seen that the InfiniBand system exhibits better 
speedup when the task execution time is 0.7 
ms or smaller. However, the speedup for both 
configurations are less than 12 (75% parallel 
efficiency) when the task execution time is 
less than 1.0 ms. Note that the gain obtained 
by using InfiniBand interface in the current 
configuration, four-node cluster running the 
CLOMP, is not markedly better that the 
Gigabit Ethernet connection. 
Fig. 5 shows the overhead time (the 
elapsed time minus the CPU time required) 
under various number of threads. It can be 
seen that the IBA has lower overhead time 
(better performance) when the threads size is 
greater than four. More importantly, the 
overhead time in the IBA is not increased as 
the size of thread is increased. Therefore, 
when an IBA cluster is scale up the parallel 
efficiency is less affected due to increased 
communication latency.  
  5 
 
 
Figure 1. The Linux cluster of InfiniBand 
connected PCs. The center switch is for 
keyboard-video-mouse connection. 
 
 
Figure 2. The rear view of a computing node. 
The InfiniBand interface is the card connected 
to the bottom optical-fiber cable. 
 
 
 
Figure 3. The switches (top: gigabit Ethernet; 
bottom: InfiniBand) for connecting the PCs to 
form the clusters. 
 
0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 2
0
2
4
6
8
10
12
14
16
Sp
ee
du
p
Ave. Task Execution Time (ms)
 Gigabit Ethernet
 InfinBand
 
Figure 4. The speedup distribution for various 
task loads in the InfiniBand cluster (only four 
nodes were used). The maximum number of 
simultaneous computing threads is 16.  
 
0 2 4 6 8 10 12 14 16 18
0.000
0.001
0.002
0.003
0.004
0.005
O
ve
rh
ea
d 
Ti
m
e 
(s
)
Number of Threads
 Gigabit Ethernet
 InfiniBand
Figure 5. The distribution of overhead time for 
various number of computing threads. 
 
 1 
席國際學術會議心得報告 
                                                             
計畫編號 NSC 96-2221-E-182-037 
計畫名稱 低延遲叢集計算於潤滑分析之性能研究 
出國人員姓名 
服務機關及職稱 
王能治 
長庚大學 機械系 教授 
會議時間地點 2007/5/6 - 2007/5/10, Philadelphia, PA, USA 
會議名稱 美國磨潤工程師學會年會(STLE 62
nd Annual Meeting) 
發表論文題目 
l An assessment of neural network metamodels for thermohydrodynamic 
lubrication analysis 
l Cluster computing of the direct algorithm to the thermohydro- dynamic 
lubrication optimization 
 
 
一  、 參加會議經過 
    2007年美國磨潤工程師學會（Society of Tribologists and Lubrication 
Engineers, STLE）之年度會議於5月6日至10日在美國費城的Marriott Hotel
舉行。此會議為美國磨潤工程師學會的年度會議，今年會議參與的人數、
論文發表數及系列的工程課程為例年僅見的規模。5月份的STLE年會和每年
10月份舉辦的國際磨潤學研討會（International Joint Tribology Conference，
由STLE及American Society of Mechanical Engineers -Tribology division共同
主辦）比較，此年度會議，不僅參與的人數眾多，而且有多家的磨潤相關
的廠商參與攤位的展出，會場活動較多且熱鬧。此次會議期間論文均以口
頭發表的方式進行，另外亦有多場的討論會（Panel Discussion）及收費之教
育課程，在會期中論文發表同時在多個會議廳舉行，共有400篇的論文以口
頭發表，每篇論文發表及討論的時間為30分鐘，大會統計約有1,200與會者
參加論文發表會議、短期教育課程、大會展覽等活動。 
