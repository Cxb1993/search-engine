the robotic head. Finally, a set of experiments are 
conducted to verify the performance of the humanoid 
robot. Specific experimental demonstrations include 
material handling and object tracking. 
英文關鍵詞： humanoid robot, vision-guided, coordinated arms, 
visual tracking, material handling 
 
 2
足人們的需求，人們對機器人充滿著幻想，
總希望其無所不能，若能將許多功能整合於
一部機器人，使用者將能省下許多空間與金
錢，因此，擁有多功能的機器人逐漸受到重
視。 
在多功能的服務用機器人中，以「輪式
移動機器人」、「輪式移動型機械臂機器人」
與「輪式人形機器人」最受到重視；其中「輪
式移動機器人」主要由輪式移動平台與安裝
在平台上的攝影機所組成，例如: 韓國 Yujin
的數位家用機器人 Nettro，可藉由身上的感測
器導航於家中，進行吸塵與保全的工作，而
日本日立之家用清潔機器人與台灣微星∕松
騰的 RV-13ipc 機器人，皆擁有監控與吸塵的
功能；而「輪式移動型機械臂機器人」主要
由輪式移動平台、安裝在平台上的一部機械
臂與攝影機所組成，例如:韓國的 PSR 機器人
[8]，可藉由身上的感測器導航於平地上，進
行物件的傳送、清潔、導覽與巡邏的工作，
而德國的 Care-O-Bot II 機器人可幫忙收拾餐
具，並可以協助行動不便者，藉由其支撐的
力量在室內自由的走動；另外，「輪式人形機
器人」則主要由輪式移動平台與安裝在平台
上的人形半身(包括一個身軀、雙手臂∕手與
機械頭 )所組成，例如 :日本三菱重工的
Wakamaru（牛若丸）機器人，主要具備保健、
看護、看家、異常通報與接待等五個功能，
而日本早稻田大學的 Twendy-One 機器人，可
幫使用者做早餐(煎蛋、烤土司等)，並以雙手
將餐盤端給使用者，另可以雙手協助使用者
從床鋪轉移到輪椅上。由以上的介紹，不難
發現當機器人的能力(與光機電元件數量有關)
增加時，較容易賦予它新的功能(或工作)，因
而得以擁有較多的功能，此乃因有較多元件
可供重複使用，然而光機電元件數量越多表
示機器人系統越複雜，因此，在追求機器人
擁有多功能的目標時，亦須考量到機器人本
身硬體上的複雜性。 
機械臂在製造過程的自動化方面貢獻良
多，它不但可增加生產力、降低製造成本，
尚可增進產品品質，然而大部份的機械臂大
多只執行簡單的重覆式工作，例如：取放
(Pick-and-Place)、噴漆(Spray Painting)、與點
焊(Spot Welding)，此乃因單一機械臂可獨立
完成的工作種類有限，因此，為了增加其用
途，在同一工作空間同時使用兩機械臂是必
要的，日本的 FANUC 公司就曾將其所製造二
部或四部機械臂，於生產線上同時用以握持
並運送大型工件。近十年來，機器人技術正
由製造領域擴展至我們的日常生活，為滿足
消費者的需求，也已開發出各式各樣的人形
服務用機器人，雖然目前能同時以雙手完成
一件工作的人形機器人尚屬少見，但是以其
來實現人類的技能，一直是機器人學的一大
目標，而人類的技能往往需要同時使用雙手
臂/手來完成，因此，為充分利用人形服務用
機器人所擁有的視覺系統與雙手臂/手，有必
要對視覺導引雙手臂/手協調控制進行研
究，藉以賦予人形服務用機器人更多的功能。 
計畫目的 
在日常生活中，人們會使用雙手去搬運
一些體積過大或重量較重的物件，機器人在
遇到相同的狀況時，亦會採取相同的方式—
雙機械臂/手協調搬運，尤其當物件體積過大
時，手無法夾住物體，雙手僅能以握持的方
式進行搬運的工作，相較於夾著的方式，握
持的方式較為困難。因此，本計畫的目的在
於賦予「輪式人形服務用機器人」以頭部之
視覺系統進行視覺追蹤與導引雙手臂協調搬
運物件的功能；所能執行的搬運工作包含物
件之握持、運送與堆疊。所使用的機器人是
屬於簡易型之「輪式人形服務用機器人」，其
擁有人形機器人之親和力，但沒有一般人形
機器人之複雜性，該機器人主要是由一全方
向輪式移動平台、一個架於輪式移動平台上
的固定式身軀、兩個各七自由度的機械臂、
兩個機械手及一個安裝著全景式攝影機與二
維旋轉式攝影機之機械頭所組成。在實際應
用上，使用者可要求機器人到家中的某個位
置去搬運物件至另一個位置，機器人在藉由
其導航策略前往物件所在處的過程中，將隨
時凝視著該物件，直到到達所指定的位置，
機器人會以視覺系統導引雙手臂/手握持物
件，並將其運送至目的地，再將物件放至所
指定的位置，若須做堆疊的動作，機器人亦
可將物件進行堆疊。  
 4
3.2.2 Vision-Guided Control Strategy 
Let C X  be an m-dimensional relative pose 
vector of a workpiece with respect to the camera 
frame C , and let eF  be an l -dimensional 
feature vector of the workpiece image. Then, the 
forward kinematic relationship between C X  
and eF  can be represented as, 
( )Ce CF I X , (1) 
where ( )CI   is a mapping from C mX R  to 
l
eF R . If the mapping from C X  to eF  is 
one-to-one, then the inverse kinematic 
relationship is given by 
1( )C C eX I F
 , (2) 
where 1( )CI    is a mapping from leF R  to 
C mX R . Using the known head/eye 
transformation, HCT , the above inverse 
kinematic relationship can also be represented 
as 
1( )H H eX I F
 , (3) 
where H X  denotes an m-dimensional relative 
pose vector of the workpiece with respect to the 
head frame H ; 1( )HI    is a mapping from 
l
eF R  to H mX R . Furthermore, using the 
known coordinate transformation from the head 
frame to the robot base frame, RBHT , the above 
inverse kinematic relationship can also be 
expressed as 
1 ( )RB RB eX I F
 , (4) 
where RB X  denotes an m-dimensional relative 
pose vector of the workpiece with respect to the 
robot base frame RB ; 1 ( )RBI    is a mapping from 
l
eF R  to RB mX R . 
The robot can use the visual feedback 
through the PTZ camera to estimate the position 
and orientation of a workpiece relative to its 
own base coordinate frame RB , by knowing the 
relative location between the head and the robot 
base, between the camera and the head, and 
between the workpiece and the camera. These 
three sets of information can be obtained 
through the robot calibration, the robotic 
eye-to-head calibration, and the camera 
calibration. These three calibrations regularly 
demand large-scale nonlinear optimization, 
special setting up, and expert skills. 
Additionally, the traditional method of 
position-based “look-and-move” visual 
feedback [3] uses the geometric optics model 
specified by (3) to determine the position of the 
workpiece with respect to some fixed reference 
frame from image features. However, only 
under ideal conditions does the geometric model 
yield accurate results. 
Therefore, no any calibrations are 
performed in this project. A vision-guided 
control strategy is proposed to determine the 
pose of the workpiece to be held, transported 
and stacked by using the uncalibrated PTZ 
camera. The proposed control strategy is based 
on the back-propagation neural network (NN), 
which is applied to achieve the mapping 
between image features of the workpiece and 
the pose of the workpiece. Although the number 
of layers may be as many as desired, the 
three-layer NN is used in this research since 
using only one hidden layer has been shown to 
be sufficient. In the training stage, the 
workpiece held by two coordinated hands is 
conducted to move randomly inside a training 
space. Consequently, the input of the NN 
consists of six selected image features of the 
workpiece, while the output of the NN 
comprises the six-dimensional pose of the 
workpiece, including the position and 
orientation of a workpiece relative to its own 
base coordinate frame RB . The Marquardt 
back-propagation algorithm [5] is adopted in 
this study. The flowchart for the learning of 
weights and biases of each layer in the training 
process is shown in Fig. 4. 
3.3 Robust Visual Tracking with PTZ  
 Camera by an Active Contour Method 
The overall architecture of the developed 
visual tracking algorithm can be represented in 
the flow diagram shown in Fig. 5. The algorithm 
of this tracking system is composed of three 
major parts, including target detector, target 
information extractor, and the target tracker. 
In the beginning, the PTZ camera platform is 
kept stationary and the target detector is set to 
detect possible moving targets. After the target 
has been detected, the target information 
extractor will gather necessary target 
information, includes target contour, target 
illumination template, and target color template. 
 6
of the bounding box. Then, we can obtain the 
coordinates of the initial ellipse contour 
points, ( , )i ix y , by the following equations: 
2cos cos( )  
, 0,1,...,
2sin sin( ) 
i E i E
i E i E
x X A X A i
N i N
y Y B Y B i
N


           
 (6) 
, where 
EX  and EY  represent the coordinate of 
the ellipse center (or the bounding box center), 
N  represents the total number of the contour 
points, and i  denotes the incremental angle. 
After this, the target contour would be extracted 
by the attractable snake model, which is able to 
converge to the real target contour in several 
iterations without overrun. 
3.3.3 Target Tracker 
In a visual tracking system, “tracking” 
usually refers to the process of continuously 
locating the position of a specific target in a 
video sequence. Therefore, after gathering 
necessary target information, the target tracking 
method is only to search the most similar image 
region in the following image frames by specific 
target matching methods. Besides, in order to 
reduce the processing time, a target searching 
area is defined at the same center as the target 
bounding box obtained in the target detector. 
Then, the target matching method searches over 
the whole searching area, to locate the new 
position of the target in the following image 
frames. 
After extracting the target contour by the 
attractable snake model, the target position can 
be located via contour matching methods. 
Different to the traditional contour matching 
method described in [1][2][4], a more intuitive 
method is developed in this project. The new 
method simply performs edge detection in the 
subsequent image frame, and obtains a binary 
edge image with a suitable threshold. Then, by 
calculating the number of the light pixels along 
the target contour in the binary edge image 
through the searching area, a contour that 
resembles the target contour most would be 
denoted as the new target position. This concept 
can be formulated as followed: 
 
( , )
arg max ( , )contour contourx y SS x y   (7) 
, where contourS  represents the best normalized 
matching score in the searching area, ( , )x y  
represents the center position of the contour 
model, S denotes the searching area, and 
( , )contour x y  represents the normalized matching 
score at position ( , )x y , which is normalized to 
the range of [0,1] by the following equation: 
( , )
( , ) contourcontour
x y
x y
N
   (8) 
Here, N represents the total number of the 
ellipse perimeter points, and ( , )contour x y  is the 
number of the light pixels (or the matched 
points) along the elliptical contour. In this way, 
this simple and intuitive matching method can 
increase the robustness of the tracking system in 
the complex background and cluttered 
environment. 
From previous researches, we know that 
the template matching methods such as NCC, 
SAD, and SSD are useful for target tracking; 
however, these methods fail if the image 
contains repeated patterns of the same 
grey-level intensity. Similarly, the contour 
matching method would also fail if the image 
contained several objects with the same contour 
or the target passes through low-contrast 
background. Therefore, in this study, we would 
integrate the illumination, color and contour 
matching methods to formulate a hybrid 
matching method, where these sub matching 
methods compensate each other. Herein, the 
illumination template and the color template 
matching techniques both adopt the SAD 
template matching method based on the 
templates extracted from the target information 
extractor. However, since the SAD method and 
the contour matching method has different 
measuring standard, the matching score of the 
SAD method should also be normalized to the 
range of [0,1] by the following equation: 
( , )
1 ,   ( , ) ( )
( , )  
             0          ,   ( , ) ( )
I
ISAD
SAD T T II
T T ISAD
I
SAD T T I
x y if x y W H
W Hx y
if x y W H
  
 
          
  (9) 
, where ( , )ISAD x y  represents the original SAD 
matching score of the illumination template; 
TW  and TH  denote the width and the height of 
the template; I  is the pixel tolerance of 
illumination, and ( , )ISAD x y  represents the 
normalized SAD matching score of the 
 8
of the experiment. Besides, the target’s 
maximum image velocity is about 20 
pixels/frame. Then, from Fig. 9, we can see that 
the visual tracking system can continuously 
locate the moving object when the target 
changes its pose. Also, the system can still 
locate the object even it suffered from changes 
in environment illumination (Fig. 10), low 
contrast background (Fig. 11), temporary partial 
occlusion by a human hand (Fig. 12), or similar 
target interference (Fig. 13). 
四、結論 
Following the preceding two projects, this 
project has completed the reconstruction of two 
arms of a humanoid robot. The weight of each 
robot arm is reduced to about 8.5 kg. To equip 
the humanoid robot with more capabilities, an 
intelligent vision-guided control strategy has 
been proposed for the robot to hold, transport 
and stack workpieces by two coordinated arms. 
Additionally, a novel visual tracking algorithm 
has been presented for the robot to track any 
single moving target with the PTZ camera of the 
robotic head. Experimental results demonstrate 
that the robot can drive its two coordinated arms 
to hold, transport and stack workpieces, and the 
PTZ camera of the robotic head can track any 
single moving target with 320x240 image 
sequences in real time (less than 34ms). 
五、參考文獻 
[1] S. Birchfield, “Elliptical Head Tracking 
Using Intensity Gradient and Color 
Histograms,” IEEE Conference On 
Computer Vision and Pattern Recognition, 
July 1998. 
[2] S. Birchfield, “An Elliptical Head 
Tracker,” 31st Asilomar Conference on 
Signals, Systems and Computers, Nov. 
1997. 
[3] F. Chaumette and S. Hutchinson, “Visual 
servo control Part I: basic approaches,” 
IEEE Robotics and Automation Magazine, 
pp.82-90, Dec. 2006. 
[4] P. Y. Chen, “A Robust Visual Servo 
System for Tracking an Arbitrary Shaped 
Object by a New Active Contour Method,” 
Master Thesis, Dept. of Electrical 
Engineering, National Taiwan University, 
2003. 
[5] M. T. Hagan and M. B. Menhaj, “Training 
Feedforward Networks with the Marquardt 
Algorithm,” IEEE Transactions on Neural 
Networks, Vol. 5, Issue 6, pp. 989-993, 
Nov. 1994. 
[6] J. H. Jean and R.Y. Wu, “Adaptive Visual 
Tracking of Moving Objects Modeled with 
Unknown Parameterized Shape Contour,” 
IEEE International Conference on 
Networking, Sensing and Control, Vol. 1, 
pp.76 – 81, Mar. 2004. 
[7] L. Ji and H. Yan, “An Attractable Snake 
Model for Contour Extraction in MRI 
Images,” Proceedings of the 20th Annual 
International Conference of the IEEE , Vol. 
2, pp.609-612, Oct. 1998. 
[8] G. Kim and W. Chung, “Tripodal 
Schematic Control Architecture for 
Integration of Multi-Functional Indoor 
Service Robots,” IEEE Transactions on 
Industrial Electronics, Vol. 53, No. 5, Oct. 
2006. 
 
Fig. 1 Reconstructed wheel-type humanoid robot 
U
V 1 11 ( , )Corner u v
2 22 ( , )Corner u v
3 33 ( , )Corner u v
4 44 ( , )Corner u v
1L 2L
3L
4L
Image Plane
2 2
1 1 3 1 3
2 2
2 2 4 2 4
2 2
3 1 2 1 2
2 2
4 3 4 3 4
( ) ( )
( ) ( )
( ) ( )
( ) ( )
L u u v v
L u u v v
L u u v v
L u u v v
   
   
   
   
 
Fig. 2 Quadrangle in the workpiece image 
 10
 
 
 
 
 
Fig. 8 Snapshots from a material handling video 
 
Fig. 9 Changes in target pose 
 
Fig. 10 Changes in environment illumination 
 
Fig. 11 Low contrast background 
 
 
Fig. 12 Temporary partial occlusion by a human hand 
 
Fig. 13 Similar target interference 
 
 
 
 
 
 
 2
是以一個輪式移動平台為主，其上安裝有陣列式的麥克風與一些攝影機，目標
是有效整合此二種感測器，使機器人能與人類同居一室，展示的內容是一些目
前該實驗室的初步成果(實驗室參觀過程嚴禁拍攝)。 
                   
                博覽會會場外之告示牌                       會場內之展示攤位 
二、 研究成果 
於參觀博覽會的過程中，主要是想了解目前世界上有哪些感測器能應用於
機器人，並能與其充分結合在一起；最大的收穫除了得到感測器發展的趨勢外，
莫過於看到筆者比較感興趣的壓力感測器、小型力量感測器、加速度感測器、小
型遙控器、客製化之人體姿態偵測裝置與無線感測網路，對於未來的研究頗有幫
助。在日本第四回機器人大賞中，共頒發了「第四回機器人大賞(經濟產業大臣
賞)」、「最優秀中小企業賞(中小企業廳長官賞)」、「日本機械工業連合會會長
賞」、「中小企業基盤整備機構理事長賞」、「日本科學未來館館長賞」與七個「優
秀賞」，本次的獲獎機器人都有到展覽會場展出，參觀者皆能於現場近距離看到
實體之動態操作與靜態展示，並能與設計者交流，筆者能親眼看到這些機器人，
除讚嘆其功能外，更佩服設計者的功力與巧思，頗令人感動。 
                  
         美女機器人 HRP-4C(優秀賞)           自動採草莓機器人(優秀賞)    
國科會補助計畫衍生研發成果推廣資料表
日期:2012/01/31
國科會補助計畫
計畫名稱: 人形機器人雙手臂於視覺導引物件搬運協調控制之發展
計畫主持人: 蔡清元
計畫編號: 99-2221-E-006-172- 學門領域: 人形狀機器人
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
