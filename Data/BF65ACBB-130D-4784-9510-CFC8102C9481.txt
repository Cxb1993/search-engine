                                                      未來生活互動式溝通環境 
行政院國家科學委員會補助專題研究計畫 成果報告 
 
未來生活互動式溝通環境之建置 
子計畫四：個人化認知知識庫之建構(1/2) 
 
計畫類別：□ 個別型計畫  ■ 整合型計畫 
計畫編號：NSC 98-2220-E-224-006 
執行期間：98 年 08 月 01 日至 99 年 07 月 31 日 
 
計畫主持人：黃胤傅 教授 
共同主持人：無 
計畫參與人員：楊宗憲、林亨、邱欽祥、黃俊彰 
 
 
成果報告類型(依經費核定清單規定繳交)：■精簡報告  □完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
□出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
□ 涉及專利或其他智慧財產權，□一年■二年後可公開查詢 
 
執行單位：國立雲林科技大學 資訊工程學系暨研究所 
 
中華民國   九十九   年   六   月   二十二   日 
                                                      未來生活互動式溝通環境 
Knowledge Base)」，將可針對使用者提供更
趨於個人化之服務，取得使用者腦中認知
之知識領域，將提供真正可令使用者感到
興趣以及有幫助之服務。藉由本計畫所提
供之個人化認知知識庫，將可獲得更多之
人性化服務與應用，並依此個人化認知知
識庫，提升未來人類生活之便利性。 
有關「個人化認知知識庫之建構
(Constructing the Personal Perception 
Knowledge Base)」必須具備之功能如以下
所述： 
 使用者需可透過本計畫所建構之介面
進行資訊之查詢與取得 
 領域知識分類資料庫須能夠從網頁中
取得並儲存各個領域之分類以及其涉
及之相關關鍵字 
 網頁擷取器須達到與各類搜尋引擎結
合，以取得各種所需資料之功能 
 針對使用者提供之資料，進行探勘，以
找尋出有用之網頁標籤 
 針對有用之網頁標籤進行探勘，以進一
步篩選出更符合使用者需求之網頁 
 建構個人化認知知識庫，作為未來針
對使用者探勘之依據 
 個人化認知知識庫藉由不斷自我學習
修正，達到真正的個人化 
「 個 人 化 認 知 知 識 庫 之 建 構
(Constructing the Personal Perception 
Knowledge Base)」之架構圖如圖一所示，
本研究計畫之實行，將其區分為二個主要
部份，分別為資訊擷取及使用者探勘平台
之建構、建立個人化認知知識庫之實現，
關於內部詳細執行流程及運作程序，如以
下所列。 
 
圖一、整體架構圖 
¾ 資訊擷取及使用者探勘平台之建構 
 使用者溝通介面(User Interface) 
 網頁擷取器(Homepages Extractor) 
 使用者資料探勘模組(User Data Text 
Mining) 
¾ 建立個人化認知知識庫之實現 
 關 係 摘 要 擷 取 器 (Relationship 
Summary Extractor) 
 相 關 性 矩 陣 產 生 器 (Correlation 
Matrix Generator) 
 概念階層建立器(Concept Hierarchy 
Constructor) 
 精 密 關 係 擷 取 器 (Sophisticated 
Relationship Extractor) 
 自我學習模組(Evaluation Module) 
三、研究結果與討論 
本研究計畫將透過「未來生活互動式溝
通 環 境 (Interactive Communication 
Environments)」為背景，收集個人化資料，
建構出「個人化認知知識庫 (Personal 
Perception Knowledge Base)」，並利用此知
識庫提供各種有用之資訊給各子計畫進行
使用。 
本研究計畫主要分為兩個部份，分別為
資訊擷取及使用者探勘平台之建構以及建
                                                      未來生活互動式溝通環境 
給概念階層建立之依據。 
(3) 概念階層建立器 (Concept Hierarchy 
Constructor)：利用上一步所提供之關聯
矩陣為依據產生初步概念階層給下一
步驟做更進一步使用。 
(4) 精 密 關 係 擷 取 器 (Sophisticated 
Relationship Extractor)：利用先前所建
立之概念階層，以線上免費字典找尋出
關鍵詞間之 Is-A 與 Parts-Of 關係，提
出更精確之分析並建立出個人化認知
知識庫。 
(5) 自我學習模組(Evaluation Module)：記
錄每次搜尋的結果，並改善自我下次搜
尋之方向，以提升準確度以及可信度。 
 
圖三、建立個人化認知知識庫架構圖 
四、計畫成果自評 
在本研究計劃中，我們將計畫切割成兩
部份來同步執行，最後再將其整合為一完
整系統。首先為「資訊擷取及使用者探勘
平台之建構」，此平台主要提供一使用者溝
通介面，負責提供使用者一操作平台，收
集使用者所有的資訊並且將其做整理、分
析。當使用者第一次進入我們的系統，必
須進行登錄系統的動作，以建立個人的基
本資訊，在系統中，使用者能夠透過本系
統所提供的整合式搜尋引擎介面，選擇個
人感興趣的領域以及網頁，本系統將會把
使用者的選擇記錄起來，並將這些網頁做
處理，以文字探勘技術找尋出網頁與網頁
之間的關聯性，在本研究計畫中，我們利
用文字探勘技術篩選出各個網頁間重要的
文字，並從這些文字中挑選出在此領域使
用者可能真正感興趣的文字，以此為輸入
資料，提供給「建立個人化認知知識庫之
實現」區塊使用。 
在「建立個人化認知知識庫之實現」區
塊中，我們一共規劃了五個模組，從一開
始的關係摘要擷取器、相關性矩陣產生
器、概念階層建立器、精密關係擷取器到
自我學習模組每個模組皆用來取得各種文
字與文字間的各種關係與使用者行為紀錄
以及計算文字間的關聯度，並且搭配使用
四部網路免費辭典(Dictionary/thesaurus、
Acronyms、Computing Dictionary、Wikipedia 
Encyclopedia)以取得文字的其它意義，如同
義詞、反義詞、相似詞以及縮寫詞等等，
並再透過Wikipedia以及WordNet這兩部辭
典找出文字間 Is-A 以及 Part of 的關係以建
立出這些文字的知識庫(Ontology)，而所產
生的知識庫(Ontology)即為使用者個人所
專屬的。 
接下來，我們將這兩個區塊整合成一完
整的系統供使用者使用，以建立出個人的
知識庫。在每個人專屬的知識庫中涵蓋著
各種領域的知識，而這些領域的產生即為
一開始使用者在資訊擷取及使用者探勘平
台中所選擇的類別所產生，使用者在資訊
擷取及使用者探勘平台中，只會選擇自己
感興趣的類別，並且勾選自己感興趣的網
頁，因此，我們將可透過收集此一類資訊，
進而透過建立個人化認知知識庫區塊而達
到建立專屬使用者在此一領域中的知識
庫，而透過此一步驟的循環工作下，我可
以建立專屬使用者在各種領域下的知識
庫，最後，將這些知識庫整合起來，則專
屬於使用者個人的知識庫即產生。 
                                                      未來生活互動式溝通環境 
Proceedings of the 38th SIGCSE 
technical symposium on Computer 
science education ACM, 2007, pp. 
410-414. 
[9]. Christian Wallraven, Kathrin Kaulard, 
Cora Kurner, Robert Pepperell, Heinrich 
HBulthoff, ”Psychophysics for 
Perception of (In) determinate Art,” 
Proceedings of the 4th symposium on 
Applied perception in graphics and 
visualization, ACM, 2007, pp. 115-122. 
[10]. Peter Vangorp, Jurgen Laurijssen, Philip 
Dutre, ”The Influence of Shape on the 
Perception of Material Reflectance”, 
ACM Transactions on Graphics, Vol. 26, 
No. 3, Article 77, Publication date: July 
2007. 
[11]. Shinyoung Park, Akira Harda, Hiroya 
Igarashi, ”Influences of Personal 
Preference on Product Usability,” CHI 
2006, April 22–27, 2006, Montréal, 
Québec, Canada. ACM 
1-59593-298-4/06/0004. 
[12]. Tim J. Smith, Martyn Whitwel, John 
Lee, ”Eye Movements and Pupil 
Dilation During Event Perception,” 
ETRA 2006, San Diego, California, 
March 27–29, 2006 ACM 
1-59593-305-0/06/0003. 
[13]. Sin-Jae Kang and Jong-Hyeok Lee, 
“Semi-automatic practical ontology 
construction by using a thesaurus, 
computational dictionaries, and large 
corpora,” Proc. Workshop on Human 
Language Technology and Knowledge 
Management, 2001, pp. 1-8. 
[14]. Alexander Maedche and Steffen Staab, 
“Mining ontologies from text,” Proc. 
12th European Workshop on Knowledge 
Acquisition, Modeling and Management, 
2000, pp. 189-202. 
[15]. Huang Yin-Fu and Huang Yu-Yu, “A 
Framework Automating Ontology 
Construction on Computer Science 
Documents,” Proc. of the 4th 
International Conference on Web 
Information Systems and Technologies, 
Funchal, Madeira - Portugal, May 4-7, 
Vol. 2, 2008, pp. 16-25. 
[16]. The Free Dictionary, 
http://www.thefreedictionary.com/. 
[17]. Wikipedia Categories, 
http://en.wikipedia.org/wiki/Special:Cat
egories. 
[18]. WordNet, http://wordnet.princeton.edu/. 
[19]. 張真誠, 蔡文輝, 林敏惠, “挑戰資料
庫管理系統 A Challenge to Database 
Management Systems,” 旗標出版股份
有限公司. 
[20]. 曾守正, 周韻寰, “資料庫系統之理論
與實務 ,” 華泰文化事業股份有限公
司. 
[21]. 葉良川 譯, “CRM Data Mining 應用系
統 建 置 Building Data Mining 
Applications for CRM,” 美商麥格羅.
希爾國際股份有限公司 台灣分公司. 
[22]. 曾新穆, 李建億 譯, “資料探勘（Data 
Mining A TUTORIAL-BASED 
PRIMER）,” 東華書局. 
[23]. 曾憲雄, 蔡秀滿, 蘇東興, 曾秋蓉, 王
慶堯, “資料探勘（Data Mining）,” 旗
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                           99 年  4 月  8 日 
報告人姓名  黃胤傅 服務機構 及職稱 
 雲林科技大學 
 教授 
時間 
會議 
地點 
 99/3/22 ~ 99/3/26 
 瑞士 Sierre 
本會核定
補助文號  98-2220-E-224-006 
會議 
名稱 
 (中文) 第二十五屆 ACM 應用計算國際研討會 
(英文) the 25th Annual ACM Symposium on Applied Computing 
發表 
論文 
題目 
 (中文) 架構在 MPEG-7 規格下之棒球運動影片場景偵測系統 
 (英文) Semantic Scene Detection System for Baseball Videos Based on the 
MPEG-7 Specification 
附件三
 
Semantic Scene Detection System for Baseball Videos 
Based on the MPEG-7 Specification 
Yin-Fu Huang 
National Yunlin University of Science and Technology 
123 University Road, Section 3, 
Touliu, Yunlin, Taiwan 640, R.O.C. 
+886-5-5342601 Ext. 4314 
huangyf@yuntech.edu.tw 
Lien-Hung Tung 
National Yunlin University of Science and Technology 
123 University Road, Section 3, 
Touliu, Yunlin, Taiwan 640, R.O.C. 
+886-5-5342601 Ext. 4390 
g9517706@yuntech.edu.tw 
 
 
ABSTRACT 
In this paper, we proposed a content-based multimedia 
analysis/retrieval system mainly based on the MPEG-7 
specification which is capable of handling the high-level content 
analysis such as the semantic scene detection for baseball 
broadcast videos. Here, eight semantic scene classes were 
predefined for baseball videos. First, an effective shot boundary 
detection scheme based on scalable color histogram was proposed 
to segment a video into many shots. Then, various visual features 
including field color features, skin color features, and camera 
motion information were extracted to analyze the semantics for 
each shot. According to the visual properties of different scenes, 
we developed a two-stage classification strategy for the semantic 
scene detection. Finally, the experimental results showed that the 
proposed framework identifies eight semantic baseball scenes 
with 81% of precision rates and 84% of recall rates.   
Categories and Subject Descriptors 
I.4.8 [Image Processing and Computer Vision]: Scene 
Analysis – color, motion.  
General Terms 
Algorithms, Performance, Design, Experimentation, and 
Standardization. 
Keywords 
CBVR, Baseball Broadcast Video, Semantic Scene Detection, 
SVM, MPEG-7. 
1. INTRODUCTION 
Over the past decade, the developments of multimedia 
technologies grew rapidly. With the increasing amount of audio-
visual information, people prefer actively accessing information 
which they are interested in. Therefore, video indexing retrieval is 
strongly required for video searching and summarization. The 
content-based video retrieval system (CBVR) has been studied 
extensively to efficiently manipulate videos based on their 
contents. With the CBVR, people can search and retrieve 
interesting contents conveniently and effectively in a huge 
amount of videos. 
As motivated by the above observations, we applied the concepts 
of CBVR to baseball broadcast videos, and developed an efficient 
shot-based baseball scene detection system. For the semantic 
scene detection [2, 6], by extracting the semantics of successive 
segmented shots, various kinds of video scenes could be identified. 
However, the major challenge of video content analysis is how to 
bridge the gap between low-level features and high-level 
semantics. In [2], the authors proposed a new scene classification 
technique for baseball videos based on spatial and temporal 
features. In [6], a shot-based baseball scene classification was 
proposed where four scenes were detected by using low-level 
features. 
In this paper, we extended and improved the methods [2, 6] by 
further considering mid-level features for more scene classes. The 
extracted mid-level features such as field color features, skin 
color features, and camera motion information are expected to act 
as an effective link between low-level video processing and high-
level video content analyses. Furthermore, a semantic scene 
classifier was designed based on a two-stage classification. We 
first classified shots roughly into several basic types using field 
color features by SVMs. Then, the mid-level features would be 
utilized as the criteria to further classify them. Finally, we 
integrated the video contents and MPEG-7 specification to 
provide automatic techniques to access a video based on its 
contents through video indexing and retrieval, summarization, and 
understanding. 
The remainder of the paper is organized as follows. In Section 2, 
we give a brief overview of the proposed semantic scene 
detection system. Section 3 presents the video analysis and 
feature extraction techniques, including shot boundary detection, 
camera motion extraction, and key-frame extraction schemes. 
Then, based on each extracted key-frame, Section 4 introduces the 
extraction techniques for mid-level features such as field color 
features and skin color features. In Section 5, an effective two-
stage classification strategy was proposed to detect and classify 
semantic scenes in baseball videos. In Section 6, we present a 
content-based multimedia analysis/retrieval system to characterize 
baseball videos by integrating the MPEG-7 descriptions. The 
experimental results are presented to show the accuracy of the 
shot boundary detection and scene classification in Section 7. 
Finally, we make conclusions in Section 8. 
 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
SAC’10, March 22-26, 2010, Sierre, Switzerland. 
Copyright 2010 ACM 978-1-60558-638-0/10/03…$10.00. 
 
941
the motion information from each shot. Finally, also based on the 
extracted motion information, a key-frame extraction strategy was 
proposed. 
3.1 Shot Boundary Detection 
A shot is a sequence of frames generated during a continuous 
camera operation, and it represents a continuous action in time 
and space. The histogram-based approach compares the 
color/intensity histograms of two consecutive frames, and 
identifies a shot boundary if their difference exceeds a certain 
threshold. In our approach, we would extend the method for 
detecting shot boundaries using color histograms certified by the 
MPEG-7 standard [3]. 
First, one of MPEG-7 color descriptors (i.e., Scalable Color 
Descriptor) is used as a low-level feature. The SCD is a color 
histogram in the HSV color space where the triple-color 
components (H, S, V) is uniformly quantized with 16 bins in H, 
and 4 bins in each S and V (i.e., 256 bins in total). Therefore, the 
normalized color histogram in bin c of frame fi is represented as : 
256
1
1( ( )) ( )*
( )
i i
i
j
Norm Hist c Hist c
Hist j
=
=
∑
 
Afterward, the abrupt boundary detection is done simply by using 
the Bhattacharyya Coefficient [1] to compute the color histogram 
similarity between two consecutive frames, CHS is expressed as 
follows. 
  
1
1
( ( )) ( ( ))
number of bins
i i
c
CHS Norm Hist c Norm Hist c−
=
= ∗∑  
The CHS represents the similarity of 1 and i iHist Hist − which 
are two consecutive frames fi and fi-1, respectively. The CHS 
value is in the range [0, 1], and the more the CHS value is, the 
more similar two consecutive frames are. Given a fixed threshold 
for the CHS value, if the CHS value is less than the threshold, the 
shot boundary would be detected since an abrupt transition occurs 
between two consecutive frames. 
3.2 Motion Feature Extraction 
The purpose of extracting camera motion information is to 
acquire the semantic meaning so that the intention of a video shot 
could be inferred. Thus, the camera motion characterization plays 
an important role in content-based video representation and 
indexing, especially for sports videos. The proposed multimedia 
content description MPEG-7 [3] has adopted several descriptors 
(D) to characterize various aspects of camera motions. 
To extract motion information, the method proposed by Th. 
Zahariadis et al. [8] was applied to estimate reliable motion vector 
fields (MVFs) from raw video data. By computing block-based 
motion vectors, the Mean Absolute Different (MAD) was defined 
to measure the difference between the macro-block of current 
frame and the macro-block of the reference frame. Then the 
simplest search is the exhaustive search (ES), where all possible 
displacements are evaluated within a specific range. Thus, the ES 
computes the MAD at all locations of the search window in order 
to find the best motion vector. After we derived the MVFs from 
raw video data, they can be divided into several non-overlapping 
regions (sub-MVFs); i.e., either background regions or object 
regions. The example of background sub-MVFs is shown in 
Figure 2. 
 
Figure 2. An object region in the dark area and four 
background regions located on the corners. 
In general, background regions contain the most camera 
operations so that we reserve the four corners for the camera 
motion analysis. As a result, the camera motion analysis only on 
the background sub-MVFs avoids heavy computation and 
increases the effectiveness of camera motion information 
extraction. The background sub-MVFs are denoted as [ ],s n , 
where { }, , ,s LT LB RT RB∈ in the thn frame. Then, its 
magnitude can be expressed as: 
2 2
1 1
1[ , ] ( [ , ]) ( [ , ])  
mb mb
k k
N N
k kmb
M s n mvx s n mvy s n
N = =
= ⋅ +∑ ∑  
where  and mvx mvy are the horizontal and vertical components 
for the thk  macro-block of the background sub-MVF. A camera 
movement only occurs when the motion magnitude is more than a 
threshold. For the camera movement in the four background sub-
MVFs, if at least three motion magnitudes of sub-MVFs are more 
than a threshold magτ , a camera movement occurs; otherwise the 
camera motion is still. Afterward we defined six basic templates 
as shown in Figure 3, which are used to classify camera motions 
in the subsequent process. 
 
Figure 3. Camera motion templates. 
The orientation of background sub-MVFs where [ , ]M s n  is more 
than a given threshold magτ  is used to determine the moving 
direction of a camera. For each [ , ]M s n , an angle is denoted as 
[ , ]s nθ for the orientation, and divided the angle into eight 
different orientations as shown in Table 1. 
Pan (Left) 
Pan (Right) 
Tilt (Up) Zoom (In) 
Zoom (Out) Tile (Down)
LT 
LB RB 
RT 
943
where xij is a pixel in a block, H and W are the height and width 
of a block, and typeHist are the histograms for different field color 
types in each block. The field color histograms of all blocks in a 
key-frame concatenate a 48-D feature vector which contributes to 
the field color feature of the key-frame as shown in Figure 4(b). 
 
Figure 4. Field feature histograms extracted from a pitching 
scene. 
4.2 Skin Color Feature Extraction 
Because the faces of players occupy a certain percentage in the 
“player” and “close-up” scenes, the skin color is an essential 
feature used for classification. To extract skin color features, skin 
detection [7] is applied to obtain skin regions in each key-frame. 
In the paper, we combined two skin color methods based on an 
HSV color space and GMMs. First, we built the skin color model 
trained by seven GMMs. Then, according to the skin color model, 
the probability distributions determine whether a pixel is in a skin 
or non-skin. In the HSV color space, we adopted the range of skin 
colors proposed in [7]. A pixel is labeled as the skin if it conforms 
to 340 359 0 50H H≤ ≤ ∨ ≤ ≤ , 0.23 0.68S≤ ≤ , and0.3 1V≤ ≤ . 
 
Figure 5. Example of skin region detection. 
Furthermore, we applied morphological operations such as 
erosion and dilation to eliminate noise and we used two 
constraints to refine the skin region, and constructed a face mask. 
The first one is that for the shape, we created a flat and disk-shape 
structural element with size 3x3. The second one is that the aspect 
ratio of the height to the width of the detected skin region is 
between 0.8 and 2.6. Then, after using a connecting operation to 
find out all connected skin regions, each skin region is labeled so 
that each group of pixels can be identified at a single region. The 
results of color labeled regions are illustrated in the Figure 5(c). 
Finally, three skin color features are derived from the skin regions 
in each key-frame. These features are 1) the number of skin 
regions in each key-frame, 2) the percentage of skin colors in the 
red-block, and 3) the size of the dominant skin region in the red-
box. They would be used to classify a shot into the close-up or 
player scene. 
5. SEMANTIC SCENE DETECTION 
Through a series of video analyses and feature extractions, we 
obtained three types of information including field color 
distributions, skin color distributions, and camera motions. In the 
paper, we have defined eight types of semantic scenes including 
pitching, infield-hitting, outfield-hitting, fielding, player, 
walking/running, close-up, and others which occur frequently in a 
baseball game. Here, a two-stage classification strategy was 
proposed to detect and classify semantic scenes of which the 
decision tree as shown in Figure 6 reveals the processing flow. 
First, we defined three extra basic types namely “field color 
dominant”, “field with players”, and “non-field color dominant” 
in classification stage 1 using the spatial characteristics of each 
key-frame. The field color distributions of each key-frame would 
be used as features to discriminate a pitching scene from these 
three basic types via a one-against-one SVM classifier [4]. The 
input features of an SVM are 48-dimension feature vectors, and 
the radial basis function (RBF) is selected for the kernel function. 
After classification stage 1, we have derived pitching scenes and 
three types using field color features. In stage 2, we took skin 
color features and camera motion information as the criteria to 
further classify these three basic types. The rest of three basic 
types can be further classified step by step through the 
hierarchical decision tree. As shown in Figure 6, once a leaf is 
reached, one specific semantic scene can be obtained for a shot. 
1. For a shot with type “field color dominant”, we classified it 
into “hitting” or “non-hitting” using zooming motion. According 
to our observation, hitting scenes are obtained by controlling 
cameras (via zooming operations) to track a moving ball, whereas 
non-hitting scenes only contain slight or few camera motions to 
capture the actions of fielders. When the duration of zooming 
motion is more than a specified threshold zoomτ  , the shot is 
classified as a hitting scene; otherwise, as a “fielding” scene. For 
a hitting scene, we could further classify it into an “outfield-
hitting” scene or “infield-hitting” scene according to the ratio of 
grass and soil gsrT . The ratio of grass and soil can be determined 
by where the ball falls or rolls (i.e., the outfield or infield). Here, 
the thresholds zoomτ and gsrT are set to 2.5 seconds and 70%, 
respectively. 
2. For a shot with type “field with players”, skin color features 
can be applied to check whether it is a player scene or not. In 
general, the common characteristics in player scenes are that the 
lead role in the image is a player, and the background is 
composed of field colors or others. Thus, if the number of skin 
regions is more than a specified threshold Tns  and the total 
number of skin color pixels in the red-box as mentioned in 
Section 4.2 is also more than a specified threshold Tts , the shot 
must contain players’ skin in the field; otherwise, it is an “others” 
scene. For the shot containing players’ skin, we could further 
classify it into a “walking/running” scene or “player” scene 
according to the duration of panning motion. The duration of 
panning motion more than a specified threshold panτ  means the 
camera is tracking a moving player. Here, the thresholds Tns , Tts , 
and panτ are set to 1, 240 pixels, and 7 seconds, respectively. 
(b) 
(c) (a) 
(a (b) 
945
parameters   with the best cross validation rate 95.2% for SVMs. 
For testing, 28 out of 81 video clips were used to test the whole 
system. Two performance measures of precision and recall were 
applied to analyze the accuracy of the scene classification. 
C
C F
NPrecision = 
N +N
 
C
C M
NRecall = 
N +N
 
where NC is the number of scenes that were retrieved and 
correctly detected, NF is the number of scenes that were retrieved 
but false detected, and NM is the number of missed scenes. The 
accuracy analysis of the scene classification is shown in Table 2. 
Table 2. Performances on semantic scene detection. 
 Groundtruth
Detected
scenes NC NM NF Rec.(%) Prec.(%) 
Pitching 403 389 385 18 4 95.5 99 
Infield-hitting 71 84 66 5 18 93 78.6 
Outfield-hitting 47 47 36 11 11 76.6 76.6 
Fielding 114 112 99 15 13 86.8 88.4 
Player 141 156 127 14 29 90.1 81.4 
Walking/Running 86 89 70 16 19 81.4 78.7 
Close-up 826 804 673 153 131 81.5 83.7 
Others 189 274 124 65 150 65.6 45.3 
 
According to the experimental results, we have come up with 
several points as follows. 
1. Overall, the system leads to an average classification success 
rate of 81%. Among them, major scenes exhibit higher accuracy 
than minor scenes, especially for pitching scenes (i.e., recall 
96% and precision 99%). This is due to the regularity of 
characteristics in baseball videos and the uniform field color 
distributions within pitching scenes. The promising 
performances on major scenes would be significant for event 
detection. 
2. The accuracy of infield-hitting and outfield-hitting scenes is 
comparatively lower than the average rate. The ambiguous arose 
when we tried to use the grass and soil ratio to distinguish 
infield from outfield. Because of the change of the field color in 
different baseball stadiums, the threshold for judging infield and 
outfield might be unsuitable anymore. Therefore, more effective 
criterion is needed. 
3. When we distinguished close-up scenes, sometimes it is 
difficult to detect player faces due to background soil or some 
CG effects. Thus, we judged all these scenes as “others”, so that 
the retrieval number of “others” scenes increases quickly. This 
incurs the poor precision for “others” scenes. 
8. CONCLUSIONS 
In the paper, we presented an effective framework for semantic 
scene detection of baseball videos. To achieve our goal, various 
visual features specific to baseball videos including field color 
features, skin color features, and camera motion information 
were extracted to detect semantic scenes in the classification. 
The experiments showed that the scene classification can be 
successfully accomplished, especially for those important scenes 
with good performances, such as pitching, player, and close-up. 
Moreover, we also integrated several description schemes based 
on MPEG-7 to describe videos effectively for browsing, 
indexing, and annotation. The proposed tool enables users to 
access videos easily through a generated summarization index. 
In addition, it also supports other operations that are helpful and 
interesting to users. 
9. ACKNOWLEDGMENTS 
This work was supported by National Science Council of R.O.C. 
under Grant NSC 98-2220-E-224-006. 
10. REFERENCES 
[1] Devroye, L., Gyorfi, L., and Lugosi, G. A Probabilistic 
Theory of Pattern Recognition, Springer-Verlag Inc., 1996. 
[2] Hung M. H., Hsieh, C. H., and Zhu, Y. C. Scene 
classification for baseball video using spatial and temporal 
features. In Proceedings of the 9th Joint Conference on 
Information Sciences. 2006, 1053-1056. 
[3] ISO/IEC JTC1/SC29/WG11, Doc.N6828, MPEG-7 
Overview (version 10), Oct. 2004. 
[4] Krebel, U. Pairwise classification and support vector 
machines. Cambridge, MA: MIT Press, 1999, 255-268. 
[5] Lee, J. H., Lee, G. G., and Kim, W. Y. Automatic video 
summarizing tool using MPEG-7 descriptors for personal 
video recorder. IEEE Transactions on Consumer 
Electronics, 49, 3, 2003, 724-748. 
[6] Pei, S. C. and Chen, F. Semantic scenes detection and 
classification in sports videos. In Proceedings of the 16th 
IPPR Conference on Computer Vision, Graphics, and 
Image Processing, 2003, 210-217. 
[7] Vezhnevets, V., Sazonov, V., and Andreeva, A. A survey on 
pixel-based skin color detection techniques. In Proceedings 
of the International Conference on Computer Graphics, 
2003, 85-92. 
[8] Zahariadis, T. and Kalivas, D. New fast algorithms for the 
estimation of block motion vectors. IEEE Transactions on 
Circuits and Systems for Video Technology, 3, 2, 1996, 
716-719. 
947
98年度專題研究計畫研究成果彙整表 
計畫主持人：黃胤傅 計畫編號：98-2220-E-224-006- 
計畫名稱：未來生活互動式溝通環境之建置--子計畫四:個人化認知知料庫之建構(1/2) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 3 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 4 4 100%  
研究報告/技術報告 0 0 100%  
研討會論文 4 4 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
