 
中文摘要 
 
 本報告說明一年期研究計畫「可動態化篩選物件特徵的機率測度追蹤」所獲致之研究
成果。本計畫的主要目標在於以 particle filter 為基礎，結合可自動篩選物件特徵(feature)之
演算法，發展一套可適應物體變化之即時視訊追蹤系統。在過去一年的研究過程中，我們
已發展出了一套完整的追蹤系統，利用我們所提出的新的物件表示模型 glocal image 
representation，不僅可快速地提取描述目標物件，亦可在後續過程中有效地根據追蹤問題中
全域及區域的變化做分析。 
  
  針對我們所提出的物件表示模型，我們亦提出了一個演算法，利用少量帶有旁資訊(side 
information)的訓練資料 (training data)，快速地得到一線性變換，可將 glocal image 
representation 映射到對於訓練資料較有意義的空間，並給予不同影像間一相似度，亦即距
離，上的量測。可快速根據少量訓練資料求得一距離函數的性質，讓我們得以成功地結合
particle filter 實做出一可動態篩選物件特徵的追蹤系統。 
 
 
 
關鍵詞：追蹤、particle filter、電腦視覺、表示模型、特徵選取 
 
1 
一、前言 
 
隨著各項軟硬體技術的進步發展，利用著電腦視覺的相關技術取代人力，在自動化的視
覺處理系統，如人員或交通流量的監控系統、居家看護系統、門禁安全系統、以及裝載於
機器人或運輸工具上的自動巡航系統，都是目前學術界及產業界所積極開發的新領域。而
有關自動化視訊追蹤之研究，實為上述各項應用的核心技術之一。在影像序列(image 
sequence)中，欲對特定物件進行辨識、行為或活動的分析、異常事件或狀態的偵測等進階
處理過程，對目標物件追蹤的準確與否，實具有決定性的影響。 
 
在電腦視覺研究初期所開發的追蹤系統，大多從固定式攝影機取得影像序列，亦即假設
追蹤的場景，在某種程度內可視為是固定的。在此假設下，欲找出影像中可能的目標物，
只需將目前攝影機所擷取出來的即時影像序列，和一個已知的參考背景作相減運算，經過
影像處理後即可得知目標物位置；此即為一般背景相減式（background subtraction）的追蹤
系統所採用之方法。儘管這類背景相減式追蹤系統具有演算法簡單與執行快速的優點，卻
面臨實用上的限制，如大幅度光線變化、背景物體擾動（如戶外樹葉搖曳）、或當追蹤物件
的顏色與背景相似等，都會影響追蹤結果之正確性。 
 
較近期的追蹤系統，為免於背景相減式追蹤的限制，則大多專注探討物件 appearance 
model 的表示模型，如形狀(shape)，顏色分佈直方圖(color histogram)；並搭配不同的追蹤演
算法(object tracking scheme)，如 Kalman filter， particle filter，mean-shift，trust-region 來進
行處理。這一類的追蹤演算法，在一些特定的問題上，有優於背景相減式演算法的準確度；
並可應用於非固定式背景的追蹤。然而這些追蹤實驗，大多數仍是在可控制之理想環境下
進行測試，往往忽略了一個在長期追蹤或實際情況下，所需考慮的關鍵因素，即目標物件
的 appearance model 和以及場景環境均可能在追蹤過程中發生變動，如攝影機視角的轉變、
追蹤物本身的運動、或光線照明條件(lighting condition)的改變，而這通常就是造成一般即
時追蹤系統失敗的主要原因。 
 
本計畫主要即針對上述問題，並希望能増進追蹤系統之效能。為解決目標物件和場景在
整個追蹤的過程中，動態產生變化所引起之種種難題，我們所建立的追蹤系統中物件的
appearance model 和追蹤演算法亦能即時動態地反應出物件和背景的變化。 
 
二、研究目的 
 
自動化電腦視覺處理系統因其應用的廣泛性，是目前相當熱門且亟待開發的一個領域，
即時視訊追蹤技術即是其中一個重要的題目。然目前大多數的追蹤系統在實際應用上，其
3 
分布，在某些需以花色樣式(pattern)去分別物件與背景的應用中便會顯得捉襟見肘。 
(4) 利用小波轉換(wavelet transform)或Gabor filter處理後的特徵值萃取([15][16])。此類
appearance model可區分出目標物高頻及低頻特徵，為物件偵測、辨識、追蹤時常用的
物件表示方式。其主要缺點在於計算上稍嫌耗時。 
(5) 特徵子空間 (eigenspace)([1][12])。利用降維 (dimension reduction)技術在低維度 (low 
dimensional)空間中有效地表示目標物。優點是在降維後能大幅減少硬體成本並增進運
算之速度，缺點則是繼承了所使用之降維方法，如PCA、LDA，中的一些不足之處，例
如對outlier敏感等等。 
(6) 3D模型(3D model)([15][16][18])。這類appearance model完整精確地描述目標物的 3D狀
態。缺點是需要複雜耗時的最佳化演算法來求解，因此在實用上有其限制。 
(7) 經由機器學習方式得到分類器 (classifier)並依分類器分數 (classification score)作為
appearance model([1])。利用機器學習法的好處是可自動化的訂定，減少需要人為操作設
定的部份，直接對訓練資料(training data)進行預測的最佳化。缺點則是難以分辨分類分
數是否真實的反映物件的相似程度。 
 
針對不同的應用問題，我們會選用不同的appearance model來代表物件，也可能對基本
的appearance model做進一步的調整或是組合不同的appearance model，來增強系統的穩定
度。我們可能利用自訂的先備知識(prior knowledge)來做調整，例如在追蹤人臉時利用橢圓
區塊來標示目標區[2]。對於較複雜或非剛性的物件，則可先對物體不同的組成區塊訂定
appearance model，再經由描述不同區塊間的相對位置關係來訂定物件整體的appearance 
model。我們通常可經由：區別性(discriminant)、效率(efficiency)、強健性(robustness)、以及
通用性(generality)，來判斷一個appearance model是否適用。以下便就這四點來分別做說明： 
區別性：好的 appearance model 應該追蹤應用中，應該在不同的序列影像中於描述物體
時有較高的相似度，而對目標物體與背景的描述之間則有低相似度，以便能有效的在追蹤
程式中區別出目標物件。 
效率：由於追蹤問題需處理大量的影像畫面，程式各方面的速度是一個需要被考量的重
點，特別是在某些需要即時處理的問題中，會更希望使用較不複雜、易於擷取的 appearance 
model，以便增進程式整體的運作速度。 
強健性：強健性代表了對物件的變動具有高容忍性。由於物體在追蹤過程中可能產生變
動，如攝影機視角的改變、物件本身的運動等，強健的 appearance model 可有效地降低追
蹤失敗的可能性。 
通用性：有些 appearance model 適用範圍較為侷限，如只適用於追蹤特定的物件類別，
或只適用在特殊設備或需要額外前處理過程，如立體視覺的攝影器材或追蹤前對追蹤物件
類別須進行額外的學習過程。 
 
 
圖 一：Particle Filter 的執行步驟示意圖 
的表示模型及相似度衡量上，此類的演算法仍有其侷限之處。此類演算法的相關論文可見
諸於[4][11][9]。 
Bayesian-based approaches：基於貝氏(Bayesian)機率架構下所提出演算法，一直在視訊
追蹤的領域中佔有舉足輕重的角色。其中最廣為人知為 Isard & Blake[14]所提出的
CONDENSATION演算法，他們使用Monte Carlo演算法在狀態向量空間(state space)中進行因數
化取樣(factored sampling)，可有效地表示非線性(nonlinear)，非高斯分佈(non-Gaussian)的機
率分佈函數。假設在影像序列中， t時間的狀態向量與影像資料分別為 tX 及 tZ ，且
1 1 2{ , ,..., }t tX X X X=： 及 1 1 2{ , ,..., }t tZ Z Z Z=： ，則在t+1 時間的狀態密度機率分佈(state density)
可以遞迴地以下式表示： 
               
其中 為likelihood或稱為observation model，
1 1 1 1 1 1 1( | ) ( | ) ( | ) ( | )
t
t t t t t t t t
X
p X Z p Z X p X X p X Z dX+ + + + +∝ ∫： ： t
1) )1( |t tp Z X+ + 1( |t tp X X+ 為dynamic model。有關
particle filter的細節可從以下步驟及圖 一來檢視： 
1. 在影像序列的 t 時間，狀態密度機率分佈 可以由 K 個 particles 與其權重
(weight)，亦即 ，來表示。 
1( | )t tp X Z：
1{ , }
k k K
t t kX π =
2. 使用 factored sampling，產出同權重(uniform weight)的 particles， 。 
~
1{ }
k K
t kX =
3. 經由 dynamic model 產生 t+1 時間的 particles， 。 1 1{ }k Kt kX + =
4. 由 t+1 時間的 particles 及 observation model，找出 t+1 時間最可能的物件狀態向量。
計算 K 個 particles 個別的權重， ，用於表示 t+1 時間之狀態密度機率分
佈， 。 
1 1{ , }
k k K
t t kX π+ + =1
)1 1 1( |t tp X Z+ +：
除了Isard & Blake的論文外，我們亦可在Sidenbladh & Black[25]以及Wu & Huang[27]等人的
5 
                       
( , ; , )
( )
T T
F
T
F
A B U V U AV U BV
U A B V
ρ = −
= −  
其中
F
⋅ 為”Frobenius matrix norm”。距離函數 ρ 相當於利用U 與V 將影像映射到另一個
空間再使用我們熟悉的Frobenius norm來量測影像之間的距離。要求得最適當的距離函數
ρ ，即是改變U 及V 使得 ρ 最能被用來描述訓練資料之間的關係，因此根據訓練資料 及
，我們導出下列最佳化問題： 
                    
S
S ′
2
, ( , )
2
( , )
min ( , ; , )
subject to ( , ; , )
A B
A B
U V I I S
I I S
A B U V
A B U V c
ρ
ρ
′ ′
∈
′∈
′ ′ =
∑
∑ ， 
其中c為一常數。由於這個最佳化問題是一個”general two-sided Procrustes problem”[13]，
因此可以利用”flip-flop”演算法來[25]求解。我們先將Frobenius norm寫成矩陣的跡(matrix 
trace)： 2
F
V ( ) ( )T T T T T T TU A U AVV A U tr V A UU AV= =
min ( ( ) ( )
T
A B
T
A B
T T T
V V I I I S
T T
U U I I I S
tr V A B UU A B V
tr U A B VV A B U
= ∈
= ∈
− −
− −
∑
∑
% %
% %
)
tr ，再反覆的令U 及V 分別為當下的U
以及V 的估計值，求解以下兩個最佳化方程式： 
                     。 
先看第一個式子，若令
% %
( , )
( , )
min ( ( ) ( ) )
)T
( , )
( ) (
A B
T T
u
I I S
D A B UU A B
∈
= −∑ % % − ，利用拉格朗日乘數法(Lagrange 
multiplier method)可知，我們事實上只需求解矩陣特徵值問題： uD v vλ= 。取出其對應到最
小的 個特徵值的特徵向量 便可組成新的矩陣V 的估計值 。如果要同時
考慮到不相似的影像訓練資料即 的資訊時，只需依照與先前一樣的方法先得到矩陣 及
，接著反覆地求解下列兩個一般化特徵值問題(generalized eigenvalue problem)： 
                        
m 1{ ... }mv v 1[ ... ]mV v v=%
S ′ uD′
vD′
  and  u u v v uD v D D u Dλ λ′ ′= = 。 
分析以上的步驟，我們可以發現 將TU A A的行向量投影到了一個較低維度的空間，在那
個空間上只保留了比較重要(significant)及比較具有區別性(discriminative)的特徵，相同地，
也將AV A 的列向量投影到了對於訓練資料較有意義的低維度空間。若是不使用 glocal 
image representation，而使用原本的影像，我們仍然可以使用一樣的分析方法求得一距離函
式，但這樣的距離函數卻無法同時適應追蹤問題中有可能發生的全域(光影變化)或區域(部
分遮蔽)的變化情形。 
7 
了一個方式及演算法，利用包含了旁資訊的訓練資料建立影像的距離函式，可將影像投影
到較有意義的特徵空間(feature space)計算影像間的差別，並利用 particle filter 實做出一可動
態化選取物件特徵的機率測度追蹤系統。我們以一極端環境下的追蹤應用證明這個系統的
可靠性。 
在實驗中我們在有光影及姿勢變化的情況下同時追蹤三張人臉，我們在追蹤過程中持
續的根據 particle filter 的追蹤結果取樣建立 及 ， 包含了一些成對的影像，每一對影
像都是同一個目標物在不同畫面中的取樣， 中的每對成對影像則分別來自不同的目標
物，或是靠近目標物的背景影像。在實驗過程中，我們不斷地取樣利用較新的資料更新 及
，並保持 及 中的資料數量(
S S ′ S
S ′
S
S ′ S S ′ 10S S′= = )。追蹤過程中起始的幾個畫面是用 Euclidean 
distance 來做追蹤，一旦收集到足夠的訓練資料，我們即改用學習到的距離函式來做追蹤。
圖 三為我們的多物體追蹤實驗中的兩個範例畫面，其中上排為影像序列的其中兩幅畫面及
追蹤結果，下排則為相對應的目標物影像。為了方便在本報告中顯示說明，目標物影像已
經過亮度方面的強化處理，但在追蹤過程中，則沒有使用任何的影像強化技術。由圖中可
見，我們的方法可有效處理亮度，表情，以及姿勢變化的情形。 
圖 三：極端情況下之追蹤實驗 
 
 
四、計畫成果自評 
在為期一年的研究計畫中，我們提出了一個完整的視訊追蹤系統，包含以下幾點特性： 
1. 利用新的物體表示模型 glocal image representation 來表示目標物件，因其僅是將原
有的影像矩陣做一重新排列，因此可被快速的提取。同時因為 glocal image 
representation 將影像全域及區域的資訊分開表示在列向量空間及行向量空間，因此
可方便同時考慮因環境如光線變化造成的整幅影像的變化以及因表情或遮蔽所造
9 
11 
[11] D. Comaniciu, V. Ramesh, and P. Meer. Real-Time Tracking of Non-Rigid Objects using Mean Shift. In Proc. 
Conf. Computer Vision and Pattern Recognition., pages II:142–149, Hilton Head, SC, 2000. 
[12] G.D. Hager and P.N. Belhumeur. Efficient Region Tracking with Parametric Models of Geometry and 
Illumination. IEEE Trans. Pattern Anal. Machine Intell., 20(10):1025–1039, 1998. 
[13] N. Higham, “Matrix Procrustes Problems,” Tech. rep., Department of mathematics, University of Manchester, 
1994. 
[14] M. Isard and A. Blake. Condensation–Conditional Density Propagation for Visual Tracking. Int. J. Computer 
Vision, 29(1):5–28, 1998. 
[15] A. Jepson, D. Fleet, and T. El-Maraghi. Robust Online Appearance Models for Visual Tracking. IEEE Trans. 
Pattern Anal. Machine Intell., 25(10):1296–1311, 2003. 
[16] V. Krüger, A. Happe, and G. Sommer. Affine Real-Time Face Tracking using Gabor Wavelet Networks. 
In Proc. Int. Conf. Pattern Recognition, pages I: 1127-1130, Barcelona, Spain 2000. 
[17] H.T. Nguyen and A. Smeulders. Tracking Aspects of the Foreground against the Background. In Proc. Euro. 
Conf. Comp. Vision, pages 446-456, Prague, Czech Republic, 2004. 
[18] H.T. Nguyen, M. Worring, and R. van den Boomgaard. Occlusion Robust Adaptive Template Tracking. In 
Proc. Int. Conf. Computer Vision, pages I: 678–683, Vancouver, Canada, 2001 
[19] N.P. Papanikolopoulos, P.K. Khosla, and T. Kanade. Visual Tracking of a Moving Target by a Camera 
Mounted on a Robot: A combination of control and vision. IEEE Trans. Robotics and Automation, 9:14–35, 
1993. 
[20] P. Perez, C. Hue, J. Vermaak, and M. Gangnet. Color-Based Probabilistic Tracking. In Proc. Euro. Conf. 
Comp. Vision, pages I:661–675, Copenhagen, Denmark, 2002. 
[21] N. Peterfreund. The Velocity Snake: Deformable Contour for Tracking in Spatiovelocity Space. Computer 
Vision and Image Understanding, 73(3):346–356, 1999.  
[22] A. Rahimi, L.P. Morency, and T. Darrell. Reducing Drift in Parametric Motion Tracking. In Proc. Int. Conf. 
Computer Vision, pages I: 315–322, Vancouver, Canada, 2001. 
[23] D. Ross, J. Lim, and M.H. Yang. Adaptive Probabilistic Visual Tracking with Incremental Subspace Update. In 
Proc. Euro. Conf. Comp. Vision, pages 470-482, Prague, Czech Republic, 2004. 
[24] P. H. Schonemann, “On Two-Sided Orthogonal Procrustes Problems,” Psychometrika, pp. 19-33, 1968. 
[25] H. Sidenbladh and M.J. Black. Learning Image Statistics for Bayesian Tracking. In Proc. Int. Conf. Computer 
Vision, pages II:709–716, Vancouver, Canada, 2001. 
[26] M. Spengler and B. Schiele. Towards Robust Multi-Cue Integration for Visual Tracking. In Int. Workshop on 
Computer Vision Systems, Vancouver, Canada, 2001. 
[27] Y. Wu and T.S. Huang. A Co-Inference Approach to Robust Visual Tracking. In Proc. Int. Conf. Computer 
Vision, pages II: 26–33, Vancouver, Canada, 2001. 
[28] Y. Wu and T. Huang. Color Tracking by Transductive Learning. In Proc. Conf. Comp. Vision Pattern 
Recognition, pages I:133–138, Hilton Head, SC, 2000. 
