0A Sports Video Highlight Extraction System based
on Transition Effect Detection
Po-Chyi Su
Abstract—In this research, we present a practical high-
light extraction scheme for sports videos. The approach
relies on precise detections of transition effects inserted
preceding and following the slow motion replays, which
demonstrate highlights of sports videos. Since the replays
are selected manually by experienced broadcasters, the
highlights extracted according to the replays will better
fulfill the audiences’ need than the ones decided by pure
audiovisual analysis. In the proposed system, the features
of MPEG compressed videos are used for subsequent pro-
cessing to achieve efficiency. The properties of transition
effects are exploited so that the effects can be accurately
retrieved for locating the video segments of replays. Next,
the contents of the replays can be analyzed and classified
to further determine their types or exciting levels. Finally,
appropriate starting positions of highlight segments will
be located for the viewers so that they can fast-forward to
watch the highlights played in the normal speed. We use
baseball videos as examples to illustrate the feasibility of
this approach.
Index Terms— Sport, baseball, video, highlight, transition
effect, slow motion, MPEG, SVM.
I. INTRODUCTION
Sports games appeal to a huge number of au-
dience worldwide and watching sportscast has be-
come an essential past time activity. In additional
to watching the live sportscast, many viewers may
prefer to record their favorite games for archiving
or time-shifting purposes, especially when many
sports events are held concurrently. Thanks to the
advanced multimedia technologies, the users nowa-
days prefer to store the videos in digital formats
by increasingly cheaper yet more powerful digital
video recorders. The superior perceptual quality
offered by digital videos along with the convenience
of storage, transmission, retrieval and even process-
ing of visual content contributes to the widespread
acceptance of digital recording facilities.
When the users set to enjoy their archived sports
games, they may be more interested in watching
only the game highlights, instead of the whole
matches from the beginning to the end. Watching
the game highlights will save viewers substantial
amount of time without sacrificing too much excite-
ment. It could be an even more agreeable experience
to watch video highlights if they are selected in
accordance with the viewers’ requests. Besides, the
viewers may prefer to record several games a day
and browse them quickly afterwards. A highlight
extraction scheme will be helpful in this aspect.
Hence, there have been a lot of research activities
on extracting highlights from sports videos [1]–[7]
in recent years.
The existing approaches to extracting highlights
from sports videos can be roughly classified into
four categories. The first approach is to employ
the visual characteristics of the video content to
help identify the important scenes that may appear
in game highlights. Such visual features as the
color histogram, types of camera shots, motions
and even segmented moving objects are calculated
from the video for further processing [8]. Generally
speaking, when an impressive performance occurs
in the sportscast, a typical scene in a particular
type of sports or a pattern of camera shots will
be shown. Therefore, by combining these visual
features with the domain-specific knowledge, we
can obtain a semantic-level or a better understanding
of the video content in a sport event. For example,
in soccer games, the scene covering the goalpost
may show a dramatic offensive or defensive play
and the appearance of a close-up view of a referee
may inform us that a penalty may be issued [9]. In
baseball games, the combination of certain court-
views may be useful in determining the play of a
homerun or a hit [6]. The higher-level understanding
of the baseball game for highlight detection can also
be achieved by the scene analysis [10]. In tennis
games, the player’s position that is found to be
close to the middle of the court may reveal the
event that a player advanced to the net for a volley
[11]. It should be noted that the results generated
from these methods usually have to be refined to
2(a) (b)
Fig. 1. Two examples of artificially imposed transition effects: (a)
a baseball pitcher animation and (b) a moving baseball.
the viewers’ attention. These characteristics make
the artificial effects look very different from the
scenes of the sports events and should thus be able
to be distinguished. We believe that the detection of
transition effects will help to achieve a more precise
and reliable slow motion detection, which may lead
to successful highlight extractions. In addition, the
volume of video data is so large that efficient
analysis becomes a challenging issue. By locating
the replays correctly and then using them as the
anchor points for subsequent processing to extract
video highlights, we can substantially reduce the
computational load. In other words, a trustworthy
replay detection scheme can serve as the baseline
framework and be coupled with other techniques,
such as sound analysis, video OCR or advanced
visual analysis, to acquire a better performance.
In this paper, we propose to detect transition
effects for highlight extraction in sports videos. We
will illustrate the idea by applying the proposed
scheme to baseball videos. Extension to other kinds
of sport videos should be achievable. The paper is
organized as follows. In Section II, we will show the
overall system of the proposed method and briefly
explain the idea of each step. Then we detail and
analyze the individual procedures in the system,
which include the feature extraction in Section III,
the generation of the processing units in Section
IV, the construction of transition effect template in
Section V and the process of locating highlights
in Section VI. Experimental results will be shown
in Section VII to demonstrate the feasibility of the
proposed scheme. Finally, conclusive remarks and
discussions will be given in Section VIII.
II. SYSTEM OVERVIEW
Fig. 2 shows the block diagram of the pro-
posed scheme. The input of the system is a MPEG
compressed video since all the commercial video
recorders nowadays store the video in MPEG-1 or
MPEG-2 format. The first step is to extract rep-
resentative features from the video for subsequent
processing. Many existing video-analysis schemes
adopt the pixel-domain approach, in which consec-
utive video frames are expanded and reconstructed
from the compressed bit-stream before further anal-
ysis. The advantage of doing so is its more accurate
and abundant content information, which may result
in a better performance. However, a clear drawback
of pixel-domain methods is the large amount of data
to be processed, which may increase the complexity
or the cost of the hardware in consumers’ video
recorders. It should be noted that the highlight
extraction is viewed as an accessory function of a
video recorder so this process should not consume
too much computational power. Since MPEG video
compression is a sophisticated procedure and the
essential part of the video has been extracted by it,
efficiency can thus be achieved by making good use
of this already processed information in the users’
side.
Feature
extraction
Constructing the 
transition effect template
Locating appropriate 
starting positions
MPEG
video
Highlight
position
Locating and analyzing
slow-motion replays
Generating
processing units
Fig. 2. The block diagram of the proposed scheme.
The next step is to divide the video into meaning-
ful segments for effective processing. We calculate
the extracted features from MPEG videos to gen-
erate the so-called “processing units,” which will
be investigated to see whether a transition effect
is included. Hence, the objective of this step is to
produce the short video segments that cover a com-
plete transition effect. Since an effect usually causes
large variations in the contents of frames, scene
changes will always be detected in the duration of
the effect. The compressed-domain shot-boundary
detection is thus applied to determine the frame of
scene-change and then a forward and a backward
extensions are made to establish the processing unit
with several frames. With the processing units at
hand, we can use the statistical information of the
extracted features to filter out those video segments
that do not possibly contain a transition effect.
The third step is to construct a transition effect
template in this specific game. As the transition
4and Ad respectively, DCBp′ is estimated by
DCBp′ =
1
64
∑
n∈{a,b,c,d}
{DCBn × An}. (1)
Special care has to be paid on boundaries of a
frame. The DC coefficients of the four 8×8 residual
blocks in Bp are then extracted from the MPEG-
compressed bit-stream and added onto DCBp′ to
form the estimated DC value, DCBp . After applying
this process to all the blocks in inter-coded frames,
we acquire estimated DC frames of the whole video.
Ba Bb
Bc Bd
Aa Ab
Ac Ad
Bp
Reference frame Current frame
Bp’
Fig. 4. Constructing DC frames for inter-coded frames.
In B- and P-frames, the encoder will seek a
good match for a macroblock in a predefined search
region for inter-coding. If a good match can not be
found in the reference frame, the intra-coding mode
will be enforced and the macroblock will be coded
independently. Thus, the number of macroblocks
coded with the intra-coding mode in B- and P-
frames may serve as a measurement of how fast
the video content changes.
IV. GENERATING PROCESSING UNITS
Processing units are the basic elements in our
system. The objective of this step is to generate
video segments covering transition effects so that
they can be later recognized. From our observation,
transition effects always come with scene changes.
The scene-change detection in our scheme is shown
as follows. We first extract the I-frames, Ii and
Ij , from the two adjacent GoP’s, GoPi and GoPj ,
respectively. We compute the difference by
D(Ii, Ij) =
K∑
k=1
∣∣∣∣DCBIi
k
−DC
B
Ij
k
∣∣∣∣ , (2)
where BIik (B
Ij
k ) are the k
th 8×8 block of Ii (Ij) and
K is the number of blocks in a frame. If D(Ii, Ij)
is larger than a threshold TI , a scene change is
identified as occurring between Ii and Ij . Next,
we calculate the percentage of macroblocks that are
intra-coded, denoted by Pr(I)p , in all the P-frames in
GoPi. The P-frame with the largest Pr(I)p , denoted
by Pm, is chosen and Pr(I)pm is compared to the other
threshold TP . If Pr(I)pm > TP , Pm will be chosen as
the frame with scene change, Fc. Otherwise, Ij will
be chosen as Fc. We don’t process B-frames at this
stage as the accuracy is already good enough and
the complexity will thus be reduced. In other words,
the percentage of intra-coding in P frames serves as
a good indication of content fluctuations with little
computational cost. Fig. 5 shows the percentage of
intra-coding in P frames in a typical video segment
containing transition effects and slow motion replay.
There are 7 scenes in the video with the scenes (1),
(6) and (7) showing the normal plays in the baseball
game and the scenes (3) and (4) demonstrating
two different views in a slow-motion replay. The
surges of intra-coding percentage between (3), (4)
and between (6), (7) clearly indicate scene changes.
0 20 40 60 80 100 120 140 160
0
10
20
30
40
50
60
70
80
P-frame number
P
er
ce
n
ta
g
e 
o
f 
in
tr
ac
o
d
in
g
(%
)
(1) (2) (3) (4) (5) (6) (7)
Fig. 5. The percentage of intra-coding in P frames in one typical
video segment containing transition effects and slow-motion replay.
The scenes (2) and (5) in Fig. 5 illustrate the
transition effects. We can see that the surges of intra-
coding percentage occur during the appearance of
transition effects. This may be explained by Fig. 6,
which shows consecutive frames of an effect. When
the effect just comes into sight, it may only cover a
small portion of a frame as shown in Fig. 6(b) and
the number of intra-coded macroblock is thus small.
This number will increase along with the emerging
effect and hit the maximum value when the full
logo is shown. The other observation is that there
are more P frames with a large number of intra-
6into the processing unit. Therefore, the first step
of refinement is to determine the exact starting
and ending frames of processing units. To achieve
this, we utilize the color information, i.e. the DC
values of 8 × 8 luminance blocks in I/P/B-frames,
since the frame containing an effect will be quite
different in colors from the scenes of ball game.
Assume that the first two frames of a processing
unit are F1 and F2, the difference of DC color
histograms in these two frames, i.e. HF1 and HF2 ,
is calculated. We delete F1 from the processing unit
if the difference is not large and then use F2 as the
starting frame to repeat the refining procedure. The
same process is applied at the end of the processing
units to search in the reversed order so that the
first and last frames of the processing unit are
visually different from the preceding and following
frames, respectively. It should be noted that this step
can help to further remove some wrongly included
video segments. These segments may contain a few
zoom-in/out shots that cannot be predicted well
so many macroblocks will be intra-coded. Besides,
some close-up views of a moving player may also
be mistakenly identified as a transition effect in the
previous step. By checking the color differences
at the two sides of processing units, we can trim
these wrongly included segments short and filter
them out by examining the lengths. Here we show
a numerical example of a one-inning professional
baseball game. The number of processing units
generated from the steps described in Section IV
is 118 and reduced to 35 after applying the refining
process.
We may choose to further reduce the number
of processing units that need to be investigated
in the template training process by utilizing the
characteristics of transition effects as they always
contain fast-moving scenes or overlaying objects.
The average number of intra-coded macroblocks in
P- and B- frames should be a good indication of the
fast content changes and the average value should
be high in the units containing transition effects.
Besides, the motion vector is another good refer-
ence. Although the magnitudes of motion vectors
directly reflect fast moving scenes in the video,
camera operations and the players’ actions may
also contribute to large movements and we have to
differentiate these regular scenes from the transition
effects. We found that in effects with fading in/out,
the number of intra-coded macroblocks may not
be that large. Nevertheless, the motion vectors in
the whole frame or around the center of the frame
appear in a rather disordered way, which is quite
different from the normal scenes of a ball game.
We may measure the variance of motion vectors
and keep only the processing units with adequate
amounts of variations in motion vectors as candi-
dates.
Many transition effects are overlaying objects or
logos imposed on the video frames. Besides the
artificial logo, the scene of ball game will also
reveal in the duration of the transition effect. These
background scenes will complicate the identification
of effect and it will be very helpful if only the
visual data associated to the effect are left for
analysis. We found that, in most of the cases, the
background scenes before or after the effect are
usually kept steady or only have small and smooth
movements as shown in Fig. 6. This may also be
the reason why the intra-coding percentage curves
shown in Fig. 7 are similar in the same effect.
Therefore, given that the starting and ending frames
of the processing unit are Fs and Fe respectively,
we will pick the frame preceding Fs and the frame
following Fe as the “background” frames. We then
compare the luminance DC values of all frames in
the processing unit and the background frames. If
the DC difference at the same location in a frame
and the background frames is large, we mark this
location as being covered by the transition effect.
We can thus form a binary mask called “effect
mask”, which indicates the pixel associated with
the effect. Figs. 8(a) to (j) demonstrate the resulting
effect mask applied on 10 frames of the processing
unit shown in Figs. 6(c) to (l). We can see that the
transition effect is discriminated and the background
is successfully removed, which will substantially
facilitate the effect extraction.
With a few processing units left as candidates,
we can use the idea of majority voting to determine
the template of transition effects in this sportscast.
After marking the spatial locations of an effect in
each frame of all the candidate processing units, we
calculate the cross-similarity among the candidate
units for grouping. To be more specific, for each
two candidate units, we first synchronize them by
finding the maximum number of position-match in
consecutive frames. The units may have different
numbers of frames since some longer processing
units may be formed by including an effect and its
8the background interference is suppressed and the
matching can be done effectively.
(a) (b) (c)
(d) (e) (f)
(g) (h) (i)
(j)
Fig. 9. The constructed template.
VI. LOCATING THE HIGHLIGHTS
After the template is constructed, detecting all the
transition effects for locating slow motion replays
can be done in a simple manner. Processing units
are formed by the similar procedures used in the
template training phase. The refined units are com-
pared with the template based on the similarity of
colors and effect locations in the reconstructed small
DC frames according to the binary effect mask.
Although the transition effects in the training set can
be retrieved right after the template is formed, we
may rather reapply the detection process from the
beginning. The reason can be explained as follows.
In the template training process, an accurate tem-
plate model is required so we tend to set up stricter
thresholds at this stage to include fewer processing
units and most of them contain the transition effects.
In the highlight-locating process, identifying every
transition effect is important so we may set up
relaxing thresholds so that more processing units
can be compared with the template to avoid miss
detections.
After the transition effects before and after the
slow motion replay can be detected correctly by
the above-mentioned procedures, the replay video
segments can be retrieved reliably and we will
then analyze the content to determine whether it is
exciting to the audience. The types of slow motion
replays may be limited since they belong to only a
small subset in the entire game and the classification
of replay contents may thus be simplified. In our
scheme, we will first apply the shot-boundary de-
tection to divide the replay segments into shots. The
procedure basically follows the concept used in the
generation of processing units and is based on the
similarity of I frames and the number of intra-coded
macroblocks in P and B frames. Because scene
changes play a more important role in the content
classification, we further improve the accuracy by
checking the color information in the P/B frames
that have their intra-coded percentage larger than
the threshold, which may be caused by fast camera
movements, to avoid over-segmentations.
The number and types of shots in the slow motion
segments sandwiched by two transition effects are
examined. It has been reported that the types of
camera shots are very helpful in analyzing sports
videos so we classify shots into several categories
including zooming, panning, tilting and still shots.
The still shots are further classified into medium,
long and pitching views. We first extract the re-
play video segments from training videos that are
broadcasted by different TV stations and classify
the frame types manually. Next, we select such
features as DC values, coding modes and motion
vectors as the input to the Support Vector Machine
(SVM) [27]. Then we use this model to classify
the frame types of the investigated video and adopt
the method of majority voting to decide the type
of shot that these frames belong to. After shot
classification, we apply Hidden Markov Model [6]
to classify the replay segments into highlights and
non-highlights. Here we define the highlights as
the scenes that are directly related to the scoring
situations, including a good defense and offense
while the non-highlights may contain such cases as
a foul ball or a strike, which the users may not be
that interested in watching in their archived video.
The final step of the proposed scheme is to
find the appropriate starting point for an identified
highlight. In baseball games, we use the pitching
view frame before the slow motion replay as the
10
TABLE II
RESULTS OF HIGHLIGHT IDENTIFICATION.
No. of Replay Highlight Hit False Miss
140 43 35 12 8
improved by more sophisticated content analysis
algorithms with more computational costs.
VIII. CONCLUSION
We propose to make use of transition effects
inserted by broadcasters for sports videos highlight
extraction. The MPEG compressed-domain features,
including motion vectors, coding modes and the
color information, are used to differentiate the shots
containing transition effects from others. The tem-
plate of transition effects in the investigated game
is obtained by examining only partial video data
and then can be used to detect the effects in the
entire game. After the transition effects are identi-
fied, the slow motion replays can be retrieved and
their contents are analyzed to determine if they are
highlights of a game. The suitable starting points of
video highlights before the slow motion replay will
be selected. Experimental results demonstrate this
promising research direction. We believe that the
proposed scheme will be beneficial to many existing
content analysis algorithms in sports videos to either
speed up their executions or increase their accuracy.
A cost-effective implementation, which is a strict re-
quirement of consumers’ digital video recorders, of
our scheme is achievable since the proposed scheme
only utilizes the features extracted/calculated from
the MPEG bitstream and the data to be processed
have been limited to replay segments only. The
feasibility of the research is illustrated by baseball
videos and the same methodology can be applied to
other sports videos with little extra effort.
REFERENCES
[1] A. Hanjalic, “Adaptive extraction of highlights from a sport
video based on excitement modeling,” IEEE Transactions on
Multimedia, vol. 7, no. 6, Dec. 2005.
[2] J. Assfalg, M. Bertini, C. Colombo, A. del Bimbo, and
W. Nunziati, “Semantic annotation of soccer videos: Automatic
highlights identification,” Computer Vision and Image Under-
standing, vol. 92, no. 2-3, Nov. 2003.
[3] D. Tjondronegoro, Y.-P. Chen, and B. Pham, “Integrating high-
lights for more complete sports video summarization,” IEEE
Multimedia, vol. 11, no. 4, Oct.-Dec. 2004.
[4] M. Petkovic, V. Mihajlovic, W. Jonker, and S. Djordjevic-
Kajan, “Multi-modal extraction of highlights from tv formula
1 programs,” in IEEE International Conference on Multimedia
and Expo., Lausanne, Aug. 2002.
[5] J. Assfalg, M. Bertini, A. D. Bimbo, W. Nunziati, and P. Pala,
“Soccer highlights detection and recognition using HMMs,”
in IEEE International Conference on Multimedia and Expo.,
Lausanne, Aug. 2002.
[6] P. Chang, M. Han, and Y. Gong, “Extract highlights from
baseball game video with hidden markov models,” in IEEE
International Conference on Image Processing, 2002.
[7] C.-C. Cheng and C.-T. Hsu, “Fusion of audio and motion
information on HMM-based highlight extraction for baseball
games,” IEEE Transactions on Multimedia, vol. 8, no. 3, Jun.
2006.
[8] L.-Y. Duan, M. Xu, Q. Tian, C. Xu, and J. S. Jin, “A unified
framework for semantic shot classification in sports video,”
IEEE Transactions on Multimedia, vol. 7, no. 6, Jun. 2005.
[9] A. Ekin, A. M. Tekalp, , and R. Mehrotra, “Automatic soc-
cer video analysis and summarization,” IEEE Transactions on
Image Processing, vol. 12, no. 7, Jul. 2003.
[10] H.-C. Shih and C.-L. Huang, “MSN: Statistical understanding
of broadcasted baseball video using multi-level semantic net-
work,” IEEE Transactions on Broadcasting, vol. 51, no. 4, Dec.
2005.
[11] A. Kokaram, N. Rea, R. Dahyot, M. Tekalp, P. Bouthemy,
P. Gros, and I. Sezan, “Browsing sports video (trends in sports-
related indexing and retrieval work),” IEEE Signal Processing
Magazine, vol. 23, no. 2, Mar. 2006.
[12] L. Lu, H. Jiang, and H. Zhang, “A robust audio classifica-
tion and segmentation method,” in ACM Multimedia, Ottawa,
Canada, 2001, pp. 203–211.
[13] Y. Rui, A. Gupta, and A. Acero, “Automatically extracting high-
lights for tv baseball programs,” in The 8th ACM International
Conference on Multimedia, 2000.
[14] Z. Xiong, R. Radhakrishnan, A. Divakaran, and T. S. Huang,
“Audio events detection based highlights extraction from base-
ball, golf and soccer games in a unified framework,” in IEEE
International Conference on Acoustics, Speech, and Signal
Processing, vol. 5, Apr. 2003.
[15] D. Zhang and D. Ellis, “Detecting sound events in basketball
video archive,” in Technical Report, Electrical Engineering
Department of Columbia University, 2001.
[16] Y. Zhong, H. Zhang, and A. K. Jain, “Automatic caption lo-
calization in compressed video,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 22, no. 4, Apr. 2000.
[17] D. Zhang, R. K. Rajendran, and S.-F. Chang, “General and
domain-specific techniques for detecting and recognizing su-
perimposed text in video,” in IEEE International Conference
on Image Processing, Rochester, NY, USA, Sep. 2002.
[18] D. Zhang and S.-F. Chang, “Event detection in baseball video
using superimposed caption recognition,” in Proceeding ACM
Multimedia, Juan Les Pins, France, Dec. 2002.
[19] J. C. Boulton, “Two mechanisms for the detection of slow
motion,” Journal of the Optical Society of America: Optics,
Image Science, and Vision, Oct. 1987.
[20] L. Wang, X. Liu, S. Lin, G.-Y. Xu, and H.-Y. Shum, “Generic
slow-motion replay detection in sports video,” in IEEE Inter-
national Conference on Image Processing, Oct. 2004.
[21] H. Pan, P. V. Beek, and M. I. Sezan, “Detection of slow-motion
replay segments in sports video for highlights generation,” in
IEEE International Conference on Acoustics, Speech and Signal
Processing, 2001.
[22] H. Pan, B. Li, and M. I. Sezan, “Automatic detection of replay
segments in broadcast sports programs by detection of logos
出席國際學術會議心得報告 
                                                             
計畫編號 97-2221-E-008-072- 
計畫名稱 基於串場效果偵測之實用型運動比賽精華擷取系統 
出國人員姓名 
服務機關及職稱 
蘇柏齊 
國立中央大學資訊工程系助理教授 
會議時間地點 Oct 8th to Oct 10th, 2008, Cairns, Australia 
會議名稱 IEEE 2008 International Workshop on Multimedia Signal Processing (MMSP) 
發表論文題目 A Digital Video Watermarking Scheme for Annotating Traffic Surveillance Videos 
 
 
一、參加會議經過 
 
On Oct. 8th, 2008, I arrived in Cairns, Australia to attend IEEE 2008 International 
Workshop on Multimedia Signal Processing (MMSP). , where we presented a paper “A Digital 
Video Watermarking Scheme for Annotating Traffic Surveillance Videos” in the session of 
“Multimedia Networking and Security.”  
 
I attended the key note speeches and a few sessions in MMSP. In our section, there were 4 
papers. I was the first author to present. In this paper, we proposed a new application of 
exploiting digital watermarking techniques for annotating traffic surveillance videos. Our work 
is an integrated part of a joint research project related to the intelligent transportation system. 
We make use the techniques of digital watermarking to facilitate data management. In our 
opinions, the research of information hiding or watermarking should have a target application 
and then fine tune it with the associated limitation. It is not possible to develop a digital 
watermarking scheme that can serve any kind of applications. However, this mistake appears 
quite frequently. In our scheme, the traffic scene is analyzed and the vehicles, viewed as 
Region of Interest, will be extracted for watermark embedding. The watermark embedding and 
extraction processes are applied in H.264/AVC to ensure the efficiency of the proposed scheme. 
The issues of payload, effective embedding/detection and rate/distortion are taken into account 
to fulfill the requirements of such an application.  
 
Some questions were raised. A Korean student asked about the processing of vehicles and 
then I explained the details of our procedures. The other researcher had a question related to 
the payload of our watermarking scheme and challenged that using the meta data should be 
more appropriate. In our opinions, digital watermarking techniques ensure that the data be 
closely tied to the video content. We believe that the removal of the meta data without 
Chen from University at Bfffalo, SUNY. Dr. HongJiang Zhang is an expert in the field of 
multimedia searching and has a lot of experience. He gave a good overview and some 
perspectives regarding to this topic. Some interesting opinions and insights are helpful to our 
project. The title of Prof. Chen’s talk is “Technical Challenges in Video Coding and Processing 
for Future Digital Entertainment.” From his talk, we know that the integration of several 
state-of-the-art audio-visual codecs is the key to the success of bringing the high-quality 
entertainment to consumers. This talk validates some of my own opinions in video coding 
technologies. 
 
In addition, I met the colleagues in the research group in University of Southern 
California and other distinct researchers in the world. We thus have the opportunities to 
discuss the related research with other scholars with a similar academic background.  
 
After attending the conference, I walked around 
Cairns. Cairns is a famous traveling resort in the 
world. I got the feeling of the living style of 
Australians as this is my first time visiting this great 
country. 
 
 
 
 
 
 
 
 
The street of Cairns along the coast near 
 Shangri-la Hotel, which hosts MMSP 2008. 
 
viewed as an object-based methodology since a video frame
will be segmented into the background and foreground, i.e.
vehicles, for subsequent information embedding and detection.
Most of the existing researches on object-based watermarking
[4], [5] are related to copyright protection in MPEG-4 videos,
which explicitly addresses the object coding. The existing
works on digital watermarking in H.264/AVC focus on robust
embedding/detection [6], [7], high bit-rate information hiding
[8] and the efﬁciency issues [9]. In our opinions, the robustness
of digital watermark in a speciﬁc standard for annotation
purposes may not be required. The payload should be high
to carry the appropriate amount of information. The efﬁcient
execution is important since the coding process of H.264/AVC
is already computationally expensive so the digital water-
marking procedure should not bring in further heavy burden.
Furthermore, the target bit-rate and video quality should be
well preserved to fulﬁll the requirements of the applications.
The proposed system is composed of three parts, i.e. the
background model construction, information embedding and
detection. We will ﬁrst describe the background model in
Section II. The processes of information embedding are pre-
sented in Section III. The information detection procedures
and the strategies to achieve a reliable watermarking scheme
are described in Section IV. Experimental results are shown
in Section V. Conclusive remarks will be given in Section VI.
II. THE BACKGROUND MODEL
The captured frame in our scheme is the raw data with
CIF format. The background model includes the constructed
background of the camera scene, the trafﬁc lane information,
and the pixels covering the highway roads. Since the roadside
cameras are usually kept still, the background of the scene
is established for subsequent processing. The background
construction is achieved by iteratively updating:
Bi+1 = Fi × α +Bi × (1 − α), (1)
where Fi and Bi are the current frame and the background
image respectively and Bi+1 is the newly constructed back-
ground image, which is a linear combination of Fi and Bi by
using a small weighting factor α. The background image can
thus be adaptive to the gradual change of environment, such as
the light condition. The vehicles can then be extracted by sub-
tracting the background image from the captured video frame.
Advanced background models [10] should help to generate
more decent results. In addition, the highway lane information
on the video frame can be identiﬁed by training the video
frames for a while after setting up the camera. Moving vehicle
trajectories are analyzed to determine the location of each lane
[11]. The pixels of road in the video frame will be located
successfully and so will be the background in which no vehicle
passes at any time and we call it “no-vehicle regions.” An
example of the constructed background is shown in Fig. 1(a)
and the trafﬁc lanes and the “no-vehicle regions” are identiﬁed
in Fig. 1(b).
(a) (b)
Fig. 1. (a) The constructed background image and (b) the background with
trafﬁc lanes marked and “no-vehicle regions” highlighted.
III. INFORMATION EMBEDDING
The block diagram of the encoder side is shown in Fig.
2, which consists of H.264/AVC video compression and the
information embedding. The inputs to the information em-
bedder are the captured video frame, the background model,
the vehicle information and the global information. With the
background model at hand, the trafﬁc surveillance camera can
apply the information embedding as follows. Two types of
information will be embedded, i.e. the global and vehicle
information. The global information may specify the data
regarding to the camera and/or video, including the serial
number of camera and/or video, the date/time of video record-
ing, the sequence number of frames and even the secure
hash of video, etc. By embedding the global information into
the compressed video, the authenticity of the recorded video
can be further ensured. The vehicle information indicates the
data of individual car collected by either the sensor near the
camera and/or by visual analysis of the recorded video. In this
research, we consider the speed and license plate of a vehicle.
Input
frame
Intra-
prediction
Background
model
Intra-slice?
Inter-
prediction
Foreground
extraction
Transform
Quantization
Transform
Quantization
Kalman
Filtering
Vehicle Info.
embedding
Vehicle
identification
Vehicle information
Global info.
embedding
Background model
& Global information
H.264/AVC
compressed
video
Yes
No
Prediction
residue
Quant.
indices
Fig. 2. The block diagram of information embedder.
First of all, vehicles in the input captured frame will
be extracted. The procedure of vehicle extraction including
subtracting the background image from the current frame,
acquiring vehicle mask by grouping foreground pixels, re-
moving isolated noises by morphological operations such as
opening/closing, removing shadows and resolving occlusions
of vehicles. Advanced techniques can help to extract the
individual vehicle accurately. Then, we apply Kalman ﬁltering
to video frames to track the movement of a vehicle so the
743
Authorized licensed use limited to: IEEE Xplore. Downloaded on December 30, 2008 at 05:20 from IEEE Xplore.  Restrictions apply.
and some global settings. In the encoding process, we can
calculate the average value in the pixel domain of a 4×4 sub-
block to determine the DC value and the luminance masking
can be derived. Our JND value, ai,j,k, will be transferred to
the corresponding quantization value, aqi,j,k, by dividing it with
Quality Parameter (QP), which is changeable for controlling
the bit-rate in H.264/AVC, and then truncating it to an integer
for indicating the maximum allowable amount of modiﬁcation.
That is, aqi,j,k = ai,j,k/QP .
If we have to modify an index in a subblock in the
information embedding process, we ﬁrst trace back from the
last of the 10 selected quantization indices according to the
zig-zag scan to ﬁnd an index Ii,j,k > 0 and its corresponding
aqi,j,k ≥ 1. We always try to subtract Ii,j,k by 1, which may
also help reduce the bit-rate, but add 1 to Ii,j,k instead in
the case that Ii,j,k is the only nonzero selected index and
is equal to 1, to avoid confusing the watermark detector. If
we cannot ﬁnd a nonzero index with its JND larger than 0,
we have two options. The ﬁrst option is to add 1 to a zero
index that has the largest nonzero JND value. The drawback
of this method is that we may create a subblock that has
many zeros between two nonzero indices and the resulting bit-
rate may be severely increased according to the CAVLC table
deﬁned in H.264/AVC. The second option is to directly modify
the nonzero index with the higher frequency. We choose the
second option since we assume that the contrast masking,
which is not calculated in our scheme though, may help to
make the watermark embedding less obvious.
The vehicle information embedding is very similar to the de-
sign of global information embedding. In our scheme, we will
embed 34 bits for a vehicle to indicate the information of car
speed (7 bits) and the license plate number (27 bits). The speed
of car will be embedded ﬁrst so this value can be shown at the
beginning of information detection in a vehicle followed by
the plate number. The macroblocks covering the vehicle will
be chosen to embed the associated information. The reason of
choosing 16× 16 macroblocks, instead of 4× 4 subblocks, is
detailed in Section IV. The vehicle information embedding is
not restricted to the I-slice but applied on several frames in
which the corresponding vehicle appears. In most of the trafﬁc
surveillance videos, the emergence of a car will always result
in intra-coded macroblocks. Besides, it is quite common that
the size of vehicle will become larger or smaller (depending
on the location of camera) in consecutive frames and this case
violates the assumption of linear movements of a rigid body
in motion estimation/compensation mechanism. Therefore, the
intra-coding is applied quite often in the duration of a vehicle’s
appearing in video frames. Fig. 4 is one typical inter-coded
frame of a trafﬁc surveillance video encoded at 350Kbps by
H.264/AVC. The intra-coded macroblocks are highlighted and
we can see that most of them cover the moving vehicles
and visually verify our argument. We still embed one bit
information into one intra-coded subblocks to maintain a
decent visual quality. The major difference is that we will
try to embed the vehicle information in all the subblocks with
nonzero sum of selected indices, instead of sparsely selected
subblocks only, to ensure that all the necessary data related to
a vehicle be embedded successfully.
Fig. 4. The intra-coded macroblocks in a typical inter-coded frame of a
trafﬁc surveillance video.
IV. INFORMATION DETECTION AND THE ISSUES OF
RELIABLE WATERMARKING
The ﬂowchart of information detection, which is almost the
reverse operation of information embedding, is shown in Fig.
5. One major exception is that the encoder will extract the
vehicle masks from the raw video while the detector will use
the reconstructed, lossy-compressed and watermarked video to
identify the vehicles in the video. To detect the watermark in a
frame, the detector has to store all the quantization indices of a
frame ﬁrst. After reconstructing the video frame and retrieving
the vehicles, the detector extracts the hidden information from
the buffered quantization indices accordingly.
In global information detection, the sum of 10 quantization
indices, Isum, of the selected subblocks in I-slices will be
calculated. The subblocks with Isum = 0 will be skipped. An
even value of Isum generates a bit “0” while an odd value
of Isum generates a bit “1”. The process continues until we
acquire the total (64) bits of the global information.
Input bit-
stream
Inter slice
decoding
Intra slice
decoding
Inverse
Transform &
Quantization
Constructing
video frames
Prediction
residue Foregroundextraction
Background
model
Store 
quantization
indices
Store
quantization
indices
Kalman
filtering
Global info.
detection
Vehicle info.
detection
Vehicle
identification
Intra-
slice?
Yes
No
Vehicle
information
Global
information
Fig. 5. The block diagram of information detector.
In vehicle information detection, the macroblocks covering
the vehicle mask will be used to extract the information
associated with the vehicle. The reason of choosing 16 × 16
macroblocks comes from the unstable vehicle-mask extrac-
tions in the reconstructed frames from lossy-compressed and
watermarked video. Although 4 × 4 subblocks can depict a
vehicle mask much better, a slight difference in the shapes
745
Authorized licensed use limited to: IEEE Xplore. Downloaded on December 30, 2008 at 05:20 from IEEE Xplore.  Restrictions apply.
