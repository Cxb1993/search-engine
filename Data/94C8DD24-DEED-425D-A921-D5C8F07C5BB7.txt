2 
 
 
 
Contents 
 
I、 ABSTRACT ……………………………………………………………3 
II、 INTRODUCTION…………………………………………………3 
III、 JOINT ANGLE REPRESENTATION FOR 
HUMAN AND ROBOT SHOULDER 
JOINTS……………………………………………………………………4 
IV、 PROPOSED APPROACH…………………………………5 
V、 COMPUTER SIMULATIONS AND 
EXPERIMENTAL WORK…………………………………7 
VI、 CONCLUSIONS……………………………………………………7 
REFERENCE 
VII、 SUPPLEMENTS………………………………………………9 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
4 
 
simulations and experimental work on a robot arm with 
two or three DOFs in the shoulder and an DOF in the 
elbow are presented. Discussions and conclusions are 
summarized in Section V. 
 
 
III. JOINT ANGLE REPRESENTATION 
FOR HUMAN AND ROBOT 
SHOULDER JOINTS  
A. Sequence-dependent joint angle representation 
Figure 1 shows a human arm with three rotational 
degrees in the shoulder and a HiTECH Robotics 
Robonova robot with two rotational degrees in the 
shoulder. For the human shoulder joint, its reference 
coordinate system (X0, Y0, Z0) is shown in Fig. 1(a) with 
the x-axis directing downward, the y-axis directing 
forward, and the z-axis directing outward from the 
shoulder and perpendicular to x and y axes. The initial 
upper arm pose is defined as the orientation when the 
reference coordinate system coincides with the local 
anatomical coordinate system (Xr, Yr, Zr). Since a human 
shoulder is a ball-in-socket joint with three rotational 
degrees, the rotation sequence of the local anatomical 
coordinate system determines the pose of the human 
upper arm; reversely, only when the rotation sequence is 
known, the joint angles can be determined. On the other 
side, Fig. 1(b) shows a robot arm with two rotational 
degrees in the shoulder. The reference and local 
anatomical coordinate systems of the robot shoulder are 
also shown in Fig. 1(b). However, due to the mechanical 
structure, the robot is only able to rotate about the 
Zr-axis followed by the Yr-axis of the local anatomical 
coordinate system. These two Euler angles result in 
different robot arm poses by different orders of rotation 
sequence. 
 
In Fig. 1, obviously, the robot arm is not capable of 
imitating the human movements involving the rotation 
about the Xr-axis which is the degree of freedom that the 
robot lacks. Thus, the insufficiency of the robot imitation 
capability motivates us to evaluate how possible the 
robot can imitate the human movements. For example, 
not every rotation matrix R of the human upper arm 
obtained from the human demonstration data can be 
expressed by a sequence of the basic rotation matrices 
representing a rotation of ψ angle about the Zr-axis 
followed by a rotation of θ angle about the Yr-axis. 
Thus,  ,, rr yZ RRR   and there exists R  between 
the human-arm and robot-arm rotation matrices. Once 
R  occurs, it is difficult for us to derive the similarity 
metric using R  because apparently, R  does not 
provide us a direct picture of the difference between the 
human and robot postures. 
B. Sequence-independent joint angle representation 
To acquire the human-arm posture without knowing 
the rotation sequence of the three DOFs in the human 
shoulder, we utilize the sequence-independent joint 
angle representation to determine the human and robot 
postures. Cheng [9] proposed the sequence-independent 
joint representation using two spherical rotation angles, 
longitude α and β latitude, and proved that these two 
angles are sequence independent. α represents the 
rotation of the long axis of the humerus about the 
vertical axis of the trunk coordinate system. β represents 
the elevation and descending of the humerus. Figure 2 
shows that the human arm swings from the starting pose 
to the ending pose where α is the difference angle about 
the Xr-axis between the starting and virtual poses, β is 
the difference angle about the axis normal to the virtual 
pose and the Xr-axis between the virtual and ending 
poses, and γ is the angle about the long axis (the segment 
of limb). A complete human arm pose can be achieved 
by two cases: first, an axial rotation with α and β angles 
followed by a long axis rotation with γ angle ; second, 
along axis rotation with γ angle followed by an axial 
rotation with α and β angles. Cheng proved that the three 
dimensional rotation of a human shoulder joint (denoted 
as a rotation matrix R(α, β, γ) satisfies R(α, β, γ)= R(α, 
β)R(γ, a)= R(γ, b)R(α, β) where a and b are the long axes 
at the starting and ending poses, respectively. 
By utilizing the sequence-independent joint 
representation with α, β, γ, angles on both human and 
robot arms, it allows us to measure their postural 
similarity with/without γ angle in particular situations. 
For example, the movements involving different γ angles 
in different trials of reaching may be considered as the 
same movements. On the contrary, γ angle does matter 
in the movements of rotating. Additionally, α, β, γ angles 
provide us a direct and understandable means to define 
the similarity metric because it is easy for us to 
recognize the arm posture using α, β, γ angles in the 
three dimensional space. 
6 
 
 
,
,
,
,,,
2,1,3,
1,3,2,
1,2,1,
3,2,1,
CCC
CCC
CCC
CCCChestr
VVV
MMV
MMV
VVV




             (1) 
After we acquire all the rotation matrices about the 
rigid-body parts of a human arm, we utilize them to 
derive the transformation matrices about the shoulder, 
elbow, and wrist joints. Here, the transformation matrix 
describes the coordinate relationship between the two 
human rigid-body parts connected by a joint. The 
transformation matrix is changed due to the angle 
change of the joint. For example, CRU represents the 
transformation matrix between the upper arm coordinate 
system with respect to the chest coordinate system. Once 
we obtain the transformation matrix, we can derive the 
corresponding robot joint angle. 
  For example, to derive the transformation matrix 
about the shoulder, the proposed approach is as follows. 
We denote every marker system and its marker vectors 
by right-hand side superscripts s and c when the human 
arm is in the starting and current positions, respectively. 
The local anatomical coordinate system (xr, yr, zr) 
coincides with the reference coordinate system (x0, y0, z0) 
when the human arm in the starting position. Thus, the 
upper-arm marker system is expressed as  
   sUpperlsUCsCsUsUsUsUpperr RRVVV  03,2,1, ,,
                                           (2) 
When the human arm moves to the current location, the 
upper-arm marker system is expressed as 
   cUpperlcUCcCcUcUcUcUpperr RRVVV  03,2,1, ,,
                                           (3) 
As we assume )( sUpper
c
Upper rRr  , R is derived as 
         10110
1,)(




s
C
s
U
Cs
Upperl
c
Upperl
c
U
Cc
C
s
Upper
c
Upper
RRRR
rrR
 
(4) 
Since ,cUpper
s
Upper ll   
     1010 )())()((  sCsUCcUCcC RRRRR         (5) 
To derive the joint angles about the shoulder, we have to 
obtain cU
C R  from Eq. (5), which is derived as 
))()(()0( 01 sU
Cs
C
c
C
c
U
C RRRRR                 (6) 
In addition, the local coordinate system of the chest 
coincides with the one of the upper arm, 3IR
s
U
C  , by 
the initial calibration and alignment. Thus Eq. (6) 
becomes  
))(()0( 01 sC
c
C
c
U
C RRRR                      (7) 
C. Particle-swarm optimization for solving α, β, and γ 
angles  
According to the sequence-independent joint 
representation mentioned in section II, we utilize 
particle-swarm optimization (PSO) to solve α, β, and γ 
for human and robot transformation matrices. Once we 
obtain the transformation matrix about the human 
shoulder joint calculated by Eq. (7), we equal this matrix 
to the rotation matrix, R(α, β, γ), shown in Cheng’s paper. 
Thus, we formulate the optimization problem and solve 
it numerically by the particle-swarm optimization. The 
problem is formulated as Eq. (8) where || · || denotes the 
Frobenius norm, C denotes the cosine function, and S 
denotes the sine function. 
 
D. Posture similarity metric 
From Section II, the Frobenius norm of the matrix 
difference R between human and robot arm 
transformation matrices cannot be the fitness value in the 
PSO to acquire the optimal solution (φandθ). Since 
several non-similar robot postures may result in the same 
R , the robot posture found in the PSO may not be the 
most similar robot posture compared to that of the 
human. For example, we assume the human posture as 
αh = 0.785, βh = 0.785, γh = 0.785(radian). Figure 5 
shows || || with respect to different robot postures 
represented by α, β, γ. From Fig. 5, it shows there are 
some non-similar robot postures with different α, β, and 
γ angles but they have the same R . Thus, R  
cannot be utilized to evaluate how a robot faithfully 
imitates a human posture. To resolve the problem, we 
derive the similarity metric which truly reflects the 
spatial relationship between human and robot postures. 
The similarity metric is expressed as  









 






 


 
222
)1(1
N
h
N
h
N
h





                                           (9) 
where αh, βh, and γh represent the human posture α, β, 
and γ represent the robot posture, ω represents the 
weighting scaler of γ , and αN, βN, and γN are the ranges 
of α, β, and γ angles, respectively. The existence of ω 
illustrates some different postures with the similar α and 
β angles, but different γ are regarded as the similar 
postures. In Eq. (9), a larger ρ represents a more similar 
robot posture compared to that of the human. From the 
perspective of spatial relationship, (α-αh)2+(β-βh)2 in Eq. 
(9) explains that the similarity metric treats the robot 
postures within a small cone    22    is less than a 
small value) as the similar postures compared to the 
human posture. Figure 6 shows how ω influences the 
metric. When ω=0.5,   ,  and   are equally 
important to the metric. When ω= 0,   does not 
affect the similarity metric. 
8 
 
 
 
 
 
REFERENCES 
[1] Y. Kuniyoshi, M. Inaba, and H. Inoue, “Learning by 
watching: Extracting reusable task knowledge from 
visual observation of human performance,” IEEE 
Trans. Robot. Autom., vol. 10, no. 6, pp. 
799–822,1994. 
[2] K. Ikeuchi, M. Kawade, and T. Suehiro, “Assembly 
task recognition with planar, curved and mechanical 
contacts,” in Proc. IEEE Int. Conf. Robot. Autom., 
vol. 2, May 1993, pp. 688–694. 
[3] S. Nakaoka, A. Nakazawa, K. Yokoi, H. Hirukawa, 
and K. Ikeuchi,“Generating whole body motions 
for a biped humanoid robot from captured human 
dances,” in Proc. IEEE Int. Conf. Robot. Autom., Sep 
2003, pp. 3905–3910. 
[4] C. L. Nehaniv and K. Dautenhahn, “The correspond- 
dence problem,”Imitation in Animals and Artifacts(K. 
Dautenhahn and C. L. Nehaniv, eds., MIT Press), pp. 
41–61, 2002. 
[5] Y. Sakagami, R. Watanabe, C. Aoyama, S. 
Matsunaga, N. Higaki, and K. Fujimura, “The 
intelligent asimo: System overview and 
integration,”in Proc. IEEE/RSJ Int. Conf. Intell. 
Robot. Syst., Dec 2002, pp. 2478–2483. 
[6] V. Potkonjak, D. Kostic, S. Tzafestas, M. Popovic, M. 
Lazarevic, and G. Djordjevic, “Human-like behavior 
of robot arms: general considerations and the 
handwriting task- part ii: the robot arm in 
handwriting,”Robotics and Computer Integrated 
Manufacturing, vol. 17, pp. 317–327,2001. 
[7] J. Huang, B. T. Quan, M. Harada, and T. Yabuta, 
“Emulating the motion of a human upper limb: 
Controlling a finger-arm robot by using the 
manipulability of its finger,” in Proc. IEEE Int. Conf. 
Robot Biomimetics, Dec 2006, pp. 607–612. 
[8] G. S. Dordevic, M. Rasic, D. Kostic, and V. 
Potkonjak, “Representation of robot motion control 
skill,” IEEE Trans. Syst., Man, Cybern. C,vol. 30, no. 
2, pp. 219–238, May 2000. 
[9] P. L. Cheng, “Joint rotation between two attitudes in 
the spherical rotation coordinate system,” J. 
Biomechanics, vol. 37, no.10, pp. 1475–1482, Oct 
2004. 
I 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
□ 達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：□已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 □無 
技轉：□已技轉 □洽談中 □無 
其他：（以 100 字為限） 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
This research provides a unified method to study robot skill learning. The impacts of 
the research work include similarity evaluation and a framework of imitation learning 
by robots. The research is novel and promising. The ability of skill learning makes 
robots become versatile. The broader impacts of this research can be seen in three 
areas: societal, research and education, and outreach impacts. The proposed research 
will have a major impact on imitating human skills, capability and limitation, and its 
application to human-robot interaction. We also plan to organize workshops and 
tutorials for graduate students and encourage minority graduate students from other 
universities to participate in these workshops/tutorials. Also, we encourage students 
to attend robot competitions using the skills learned from this project. This project 
will also benefit education and outreach. The research results will be integrated into 
our graduate robotics course. Our research provides an excellent vehicle to educate 
high-school students and encourage them to select engineering /robotics/computer 
science for their higher education and career. 
 
 
 
A Hybrid Control Policy of Robot Arm Motion for
Assistive Robots
Hsien-I Lin† and Chi-Li Chen
Graduate Institute of Automation Technology, National Taipei University of Technology, Taipei, Taiwan
E-Mail:soﬁn@ntut.edu.tw†
Abstract—Tele-operated robotics has been widely-used to
manipulate heavy or hazardous objects in an unknown or
dangerous environment. Nowadays, it is adopted in assistive
robots to help the elderly in accomplishing household tasks. This
paper proposes a hybrid control policy of robot arm motion to
semi-automatically control a remote robot arm, which differs
from previous work in its hybrid control policy. The motivations
of the proposed hybrid control policy are rooted from (1) the
safeness in controlling a robot arm in an uncertain environment
by tele-operating the robot far from the target position; (2) the
quickness in automatically controlling the robot arm close to
the target position. By doing this, the robot arm is manipulated
to quickly and safely reach to the target position. The proposed
hybrid control policy tele-operates a robot by utilizing the
joint angles of a human arm to remotely control the robot
arm posture where the human joint angles are captured by
the proposed motion capture method. To validate the proposed
method, we conducted the experimental work on an 6 degree-
of-freedom humanoid robot arm. The results showed that the
human was able to safely and quickly reach to the manipulative
objects by the proposed hybrid control policy of robot arm
motion.
Index Terms—Tele-operated robotics, assistive robots, hybrid
control policy, robot arm motion, motion capture, humanoid
robot arm.
I. INTRODUCTION
Recently, the demand for healthcare services is rising due
to the increasing elderly population. Thus, assistive robots
become crucial since they alleviate the labor burden for
healthcare specialists. With the aid of an assistive robot, the
elderly are able to do household tasks [1], [2]. An assistive
robot is usually composed of an arm with multiple joints
to manipulate objects, a mobile platform to travel in an
environment, and a head with a stereo vision system to
recognize human faces and localize manipulative objects.
For manipulative tasks, the quality of service provided by
an assistive robot depends on (1) a good vision system and
its well-established object database for accurately recognizing
and localizing objects; (2) a robust algorithm for localizing
objects; (3) collision-free trajectory planning for reaching
to an object. For the safety concern in healthcare services,
This work was supported in part by the National Science Council under
Grant NSC 99-2221-E-027-117. Any opinion, ﬁndings, and conclusions or
recommendations expressed in this material are those of the authors and do
not necessarily reﬂect the views of the National Science Council.
any troubles caused by the above-mentioned parts are not
allowed. Especially, when the robot is far from the target,
the environmental uncertainty makes the robot vision system
tend to introduce errors. Thus, this paper proposes a hybrid
control policy to manipulate a robot arm in a semi-automatic
mode that does not heavily rely on the vision system. For
the safety concern, this proposed hybrid control policy in an
assistive robot is especially preferable to healthcare services.
In conventional methods, the end-effector of a robot arm
was planned to move in a desired trajectory speciﬁed by the
positions of via points in the Cartesian space. To program a
robot arm, these via points were converted into the robot
joint positions (knot points) by the inverse kinematics of
the robot. Here, the knot points were used to navigate the
robot arm to avoid obstacles. To navigate the entire robot
arm, vision systems [3], skin sensors [4], and range ﬁnders
[5] were commonly adopted to detect nearby obstacles for
conﬁguring the environment. However, once the environment
was not accurately recognized by the sensors and obstacle-
avoidance algorithms, collisions might happen and result in
serious problems especially to healthcare services.
The alternative method to tele-operate a robot arm was to
make the robot imitate human motions. Schaal [6] pointed
out that the traditional methods for controlling a robot were
not sufﬁcient to resolve uncertain situations; on the contrary,
if the robot was taught by human demonstration, it would not
perform any strange postures and the robot programmer could
save much time on programming its complicated motions.
Motion imitation was done through either the trajectory of
the end-effector [7] or the trajectories of all the joints [8]
of a human demonstrator. For the end-effector trajectory
imitation, the obstacle-collision problem still existed since
it is a complicated task of taking the entire arm into account.
For the joint trajectory imitation, the joint angles of a human
demonstrator were measured by a heavy and customized
exoskeleton device [9], [10] or a vision-based motion capture
system [8].
To healthcare services, safety is a critical issue. Automatic
robot arm trajectory planning based on obstacle location is
not sufﬁciently satisfactory in a complicated environment
because its vision system tends to fail. To keep safety the
ﬁrst priority, we propose a semi-automatic mode to switch
                      Proceeding of the IEEE 
International Conference on Information and Automation 
                   Shenzhen, China June 2011
          978-1-61284-4577-0270-9/11/$26.00 ©2011 IEEE 163
B. Humanoid robot arm
The robot used in our system is an 6 degree-of-freedom
(3 for the shoulder, 1 for the elbow, 2 for the wrist, and 2
for the gripper) robot arm manufactured by Dr Robot, Inc.
Its payload is up to 1 kg. The maximum reaching distance is
0.6m. Figure 3 shows the conﬁguration of the 6 degrees of
freedom of the robot. The robot in our system is controlled
by wireless communication. The time interval between two
robot commands is 1ms. With the hardware of the proposed
system, the proposed system is able to achieve almost real-
time whole-arm tele-opration.
Z1
Z2
Z3
Z4
Z5
Z6
Fig. 3. The humanoid robot arm and its conﬁguration of the 6 degrees of
freedom
III. PROPOSED METHOD OF HYBRID CONTROL POLICY
A. Hybrid control policy
Due to the difﬁculties of vision navigation for obstacles, we
implement a hybrid control policy to tele-operate a robot arm
without vision navigation. Once the robot approaches close
to a given target, the hybrid control policy inactivates the
human tele-operation and commands the robot to approach
to the target by itself. The robot can rapidly reach to the
target because there do not exist any obstacles except of the
target in front of the robot. By doing this, the robot arm safely
and quickly moves to the target. Figure 4 shows the distance
margin of safety governed by the hybrid control policy.
Object
z
f
Infrared sensor r
o
o r
Robot Arm
f z
o r
(a) (b)
(Radius of rotation)
Wrist joint center
r z

Fig. 4. The distance margin of safety governed by the hybrid control policy.
(a) The points deﬁned by the hybrid control policy; (b) the distance margin
of safety represented by the deﬁned points in (a). Blue line: |Po −Pr|; red
line: |Pf − Pz |; brown line: |Pr − Pz |.
In Fig. 4(a), Pz is the joint center of the robot wrist, Pr
is the location of the infrared sensor, Po is the closest point
on the object to Pr, and Pf is the tip location of the robot
gripper. |Pf −Pz| is the radius of rotation (RR), |Po−Pr| is
the shortest distance from the infrared sensor to the object,
and |Pr−Pz| is the distance from the wrist joint center to the
infrared sensor. We formulate the rules of the hybrid control
policy as follows:
Tele-operation mode 1 :
RR+Δδ ≤ |Po − Pr|+ |Pr − Pz| (1)
Automatic mode :
RR ≤ |Po − Pr|+ |Pr − Pz| < RR+Δδ (2)
Tele-operation mode 2 :
|Po − Pr|+ |Pr − Pz| < RR (3)
where Δδ represents the distance margin of safety. When
Δδ is larger, the robot enters into the automatic mode earlier.
When the robot moves very fast, the condition of Eq. (2) may
be accidentally bypassed, resulting in that the robot is too
close to orientate the gripper for grasping the object. To avoid
this situation, Eq. (3) drives the robot to the tele-operation
mode 2. At this moment, the human can tele-operate the
robot to move either forward to the object or backward to
somewhere for re-entering into the automatic mode. The state
machine of these modes is expressed in Fig. 5. In Fig. 5, the
start state represents the initial position of the robot. Once the
robot enters into the automatic mode, the robot automatically
approaches to the target itself and will not enter the tele-
operation modes 1 and 2 anymore. After the robot reaches
to the object and grasps it, the robot will re-synchronize its
arm with the human arm. Once the robot and human arms
are synchronized, the robot goes to the tele-operation mode
3 to freely manipulate the object.
Tele-operation 
mode 1 
Automatic 
mode
Tele-operation 
mode 2
Start
Checked by 
Eq. (1)
Checked by 
Eq. (2)
Checked by Eq. (1)
Checked by 
Eq. (2)
Checked by 
Eq. (3)
Checked by 
Eq. (2)
Checked by Eq. (3)
Reach to the
Target and 
Grasp
Resync
Tele-operation 
mode 3
Fig. 5. The state machine of the proposed hybrid control policy.
B. Tele-operation with whole-arm motion imitation
To tele-operate robot motion, there is a great deal of
research working on the end-effector trajectory imitation.
165
A. Performance comparison between the end-effector trajec-
tory planning and the hybrid control policy
In this part, we used an apple as the target object and
a gray box in the experiment A shown in Fig. 7(a) and
a transparent box in the experiment B shown in Fig. 7(b)
are the obstacles. In each experiment, we tested 5 different
object locations and recorded the execution time. An infrared
sensor was placed on the robot hand shown in Fig. 4. For
both methods, the robot used its vision system to recognize
the obstacle location initially. In the end-effector trajectory
planning, the robot automatically planned its path according
to the obstacle location. But in the hybrid control policy, the
human demonstrator tele-operated the robot in the beginning
and the robot automatically approached to the apple when it
was close to the apple.
(a) (b)
Fig. 7. Task environment. (a) the experiment A: to avoid the gray box and
pick up the apple; (b)the experiment B: to avoid the transparent box and
pick up the apple.
1) The experiment A: avoided the gray box and picked
up the apple: In this experiment, once the robot slightly
touched the obstacle, we regarded this test as failure. We
conducted 5 tests with different obstacle locations for the
end-effector trajectory planning ﬁrst and applied the same 5
tests to the hybrid control policy as well. Table II shows
the performance comparison between these two methods.
In Table II, (Ox, Oy, Oz) represents the location of the
apple and (Rx, Ry, Rz) represents the location of the end-
effector. From Table II, it shows that the proposed hybrid
control policy was superior to the end-effector trajectory
planning because the latter failed mostly. Figure 8 shows the
experimental results.
2) The experiment B: avoided the transparent box and
picked up the apple: Table III shows the performance com-
parison between these two methods. Apparently, the hybrid
control policy was superior to the end-effector trajectory
planning because the latter did not ever succeed the task.
From the result of the experiment B shown in Table III, it
demonstrated that once the vision system could not accurately
recognize the obstacle, the robot failed the task. Figure 9
shows the experimental results.
(a) (b)
Fig. 8. The results of the experiment A. (a) the end-effector trajectory
planning; (b) the hybrid control policy.
(a) (b)
Fig. 9. The results of the experiment B. (a) the end-effector trajectory
planning; (b) the hybrid control policy.
B. Performance comparison between the tele-operation and
hybrid modes
In the second part, we demonstrated and compared the
performance between the tele-operation and hybrid modes.
From this experiment C, we tested whether the automatic
mode affected in the hybrid control policy. Figure 10 shows
that the robot was asked to pick up the ping-pong ball on the
right-handed side and place it in the cup on the left-handed
side. Between the ball and cup, there was an obstacle. We ran
10 tests in this experiment and controlled these tests by the
hybrid or tele-operation mode in a random order. Also, we did
not tell the human demonstrator which mode the robot was
going to run because the human’s forethought of the control
mode might inﬂuence the result. The average execution time
of the hybrid mode and tele-operation mode was 22.26 and
40.68 seconds, respectively. It shows the hybrid mode was
much faster than the tele-operation mode.
V. CONCLUSIONS
In this paper, we have presented the hybrid control policy
of robot arm motion imitation without heavily relying on
vision navigation. Thus, the proposed method works well
even in a complicated environment with many obstacles in
it. The proposed hybrid control policy switches between the
tele-operation and automatic modes depending on how close
the robot is to the target position. In the tele-operation mode,
the whole-arm motion imitation is conducted by a means
of capturing the joint angles of a human demonstrator and
converting them into the robot joint angles. In the automatic
167
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/27
國科會補助計畫
計畫名稱: 子計畫一：機器人技能學習設計與研究
計畫主持人: 林顯易
計畫編號: 99-2221-E-027-117- 學門領域: 智慧型機器人
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
林顯易 改進國際技能競賽機器人國手訓練中心計畫 2010/12/29 2011/08/31 
4,000,000 教育部 
林顯易 下肢行動障礙老年人之歩態分析專家系統開發 2010/01/01 
2010/12/31 641,000 馬偕醫院 
 
林顯易 自動水果檢測系統軟體開發 除了先期技轉金新臺幣二十萬元外，合約
中註明衍生利益金高達新台幣一百萬元。 
林顯易 改進國際技能競賽機器人國手訓練中心計畫 所訓練的選手將於八月底
參加 FIRA 機器人足球比賽。 
林顯易 99 年度學界協助中小企業科技關懷計畫 輔導協助欣拓科技股份有限
公司開發「汽車電子開關之自動檢測系統」。 
林顯易 99年度工業區廠商轉型再造升級計畫診斷輔導 輔導協助一銘科技股份
有限公司開發「高線數花紋印刷滾輪之自動光學檢測系統設計」。 
林顯易 99年度工業區廠商轉型再造升級計畫診斷輔導 輔導協助勇泰科技股份
有限公司開發「自動化塑品噴漆加工系統之評估分析」。 
林顯易、劉育成 應用於移動裝置上的平台平衡結構 (99 年申請中) 中華民國
專利 
林顯易、鍾炤宇 重新建構物件三維模型的裝置 (99 年申請中) 中華民國專利
林顯易、鍾炤宇 以單一影像擷取裝置提供距離量測的系統及其方法(99 年申請
中) 中華民國專利 
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
