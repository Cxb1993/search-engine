 1
行政院國家科學委員會專題研究計畫成果報告 
 
0B高準確性之自動化視訊去背的研究 
 
計畫編號：NSC100-2221-E-346-008 
執行期限：100 年 8 月 1 日至 101 年 7 月 31 日 
主持人：胡武誌   國立澎湖科技大學資訊工程系 
 
一、中文摘要 
視訊去背方法是視訊合成、視訊創作、
電影(電視)特效製作上非常重要且具高價值
的技術，透過視訊去背方法可以保有視訊中
前景物件的透明度並讓後續的視訊處理所
得到的結果更為真實。由於目前所提出有關
視訊去背的方法絕大多數均屬於非自動化
或半自動化的視訊去背方法，也就是需由使
用者由人工方式加入三區域圖或人工塗鴉
來標示出前景區域及背景區域才能獲得透
明圖層遮罩，因此對於使用者而言是件非常
耗時且繁瑣的工作。所以，本研究提出了一
個植基於混合式視訊物件分割及封閉型去
背法的自動化視訊去背方法，主要目的是要
提出不需藉由使用者人工給定三區域圖或
人工塗鴉來標示出前景區域及背景區域而
能達到自動化的視訊去背結果且保有高準
確性的目的。實驗模擬結果可證實本研究所
提出的方法效能優於其他類似的方法。 
 
關鍵詞：視訊去背、視訊物件分割、透明圖
層遮罩、封閉型去背法 
 
Abstract 
Video matting is an important and 
high-value technology in video composition, 
video editing, and special effects of film and 
TV. Using video matting can maintain the 
foreground object’s transparency (alpha matte) 
in order to obtain more realistic result in the 
subsequent video processing. Most of the 
suggested approaches of video matting obtain 
alpha matte using users’ specified trimaps or 
scribbles to mark the foreground and 
background, thus they are non-automatic or 
semi-automatic video matting. User’s 
specified trimaps or scribbles is a 
time-consuming and troublesome process for 
users. Therefore, this project proposes an 
automatic video matting based on hybrid 
video object segmentation and closed-form 
matting. The main goal is to propose the 
accurate and automatic matting without users’ 
specified trimaps or scribbles Experimental 
results show that the proposed method 
outperforms state-of-the-art methods. 
 
Keywords: video matting, video object 
segmentation, alpha matte, closed-form 
matting. 
 
二、緣由與目的 
視訊去背方法是視訊合成、視訊創作、
電影(電視)特效製作上非常重要且具高價值
的技術，透過視訊去背方法可以保有視訊中
前景物件(例如主角演員)的透明度並讓後續
的視訊處理(例如場景置換)所得到的結果更
為真實。以往電影科技中特效合成問題的解
決方法是在攝影棚內建藍幕(blue screen)的
背景，再將前景物件置於其中來拍攝影片後
再將前景物件擷取出來，以達到視訊去背的
目的；但此方法需花費昂貴的攝影棚設置成
本。此外，視訊去背與傳統的視訊物件分割
(video object segmentation)有所不同，視訊物
件分割屬於二元分割(binary segmentation)
方法，無法保有視訊物件原有的透明度，故
該方法並不適合應用在具透明度的物件
上，例如髮絲、毛髮、雲彩等的分割。因此，
保有物體透明度的視訊去背方法在隨著視
訊合成、創作、壓縮…等應用領域的發展及
需求下更顯得其必要性與重要性。 
視訊去背方法是影像去背方法的延伸
技術，影像去背方法的理論背景是建構在若
影像中的像素受到光線折射的影響，則同時
會接收到前景和背景的光線，此種混合的比
 3
在擷取前景物件遮罩上，所提出的混合
式視訊物件分割方法中是先採用移動偵測
(motion detection) [7]來求算視訊物件的移
動資訊。在本研究中只探討移動的前景物
件、靜態的背景及固定的攝影機的案例。圖
2 為移動偵測的結果，圖 2(a)為第 k 張影格、
圖 2(b)為第 k-1 張影格、圖 2(c)則為移動偵
測的結果。 
 
 
    (a)           (b)          (c) 
圖 2. 移動偵測的示意圖。 
 
梯 度 變 化 偵 測 (gradient-variation 
detection) [10]則是使用 Sobel 運算分別對第
k 張影格和 k-1 張影格代入式子(2)進行運
算，其中 ),( yxkG 和 ),(1 yxk−G 分別為第 k 張影
格和 k-1 張影格的梯度大小。圖 3(a)為圖 2
的梯度變化偵測結果。 
Gkk Tyxyx ≥− − ),(),( 1GG  (2) 
 粗糙的移動物件遮罩 (coarse moving 
object mask)則是對移動物件偵測結果和梯
度變化偵測結果進行 OR 運算來獲得，如
),(),(),( yxyxyx Gk
M
k
C
k MOMOMO U= 所示，其中
),( yxCkMO 、 ),( yxMkMO 和 ),( yxGkMO 分別代表
粗糙的移動物件遮罩、移動物件偵測結果、
梯度變化偵測結果。圖 3(b)為利用圖 2(c)和
圖 3(a)所獲得的粗糙的移動物件遮罩的結
果。由於粗糙的移動物件遮罩可能會有雜
訊，故需再進一步進行去除雜訊的處理來獲
得精緻的移動物件遮罩(fine moving object 
mask)。 
 
  
        (a)                (b) 
圖 3. 粗糙的移動物件遮罩。 
 
背景相減法則是利用角度模組法
(angle-module rule) [8]來獲得前景物件，接
著再利用式子(3)和(4)的陰影及光影反射去
除法[8]來去除相關的陰影及光影反射來獲
得最後的前景物件遮罩。 
⎩⎨
⎧ ⋅≥≥⋅≤≤=
otherwise ,0
)),((),()),(( and ),( if ,1
),( 2121
yxhyxyxhyxΘ
yx kshkkshshkshk
BIΒ
Sh
φφ
 
 (3) 
⎩⎨
⎧ ⋅≥≥⋅≤≤=
otherwise ,0
)),((),()),(( and ),( if ,1
),( 2121
yxhyxyxhyxΘ
yx krfkkrfrfkrfk
BIB
Rf
φφ
 (4) 
在背景建立及更新上，在第一張影格中
背景影像是被設定為空的，接著在後續的影
格中背景影像再由相關的像素(非移動物件
像素)來取代；也就是利用式子(5)來進行背
景建立及更新，其中 ),( yxkMO 是移動物件遮
罩、 ),( yxMk 是標記影像 ( 0),( =yxMk 和
1),( =yxMk 分別代表該像素是未標記及已標
記) 
⎪⎩
⎪⎨
⎧
==
==×−+×
=
=
−
−
1),(  1),(  ,),(
0),(  1),(  ,),()1(),(
0),(  ,),(
),(
1
1
yxandyxifyx
yxandyxifyxyx
yxifyx
yx
kk
kkk
k
k
MOMkB
MOMkBI
MkI
B λλ
 (5) 
當背景相減法所獲得的物件遮罩及移
動物件偵測法所獲得的物件遮罩均被獲得
後，接著便利用式子 (6)進行遮罩合併
(merging)，其中 ),( yxkBS 是背景相減法所獲
得的物件遮罩、 ),( yxkFD 是移動物件偵測法
所獲得的物件遮罩。圖 4(a)則為遮罩合併的
結果圖。 
⎪⎩
⎪⎨
⎧
∈
∈
=
else
Ryxifyx
Ryxifyx
yx movingt
stillt
Merge
t
 ,0
),(  ,),(
),(  ,),(
),( FD
BS
MO  (6) 
實務上，有可能會有多餘的背景區域留
在最後的遮罩合併圖中，所以最後還要進行
移動物件精煉，移動物件精煉可分為洞區域
的填充及物件邊緣精煉，在洞區域的填充部
分，當洞區域在前景物件遮罩內時，先判定
洞 區 域 內 沒 有 背 景 像 素 的 區 域 ( 即
0),( =yxkB 之區域)，並將其補為前景像素，
再判斷剩餘的洞區域，當洞區域的面積小於
洞區域周長時則判定為前景物件，最後再使
用連通標記刪除過小的前景物件區域和洞。 
物件邊緣精煉是為了使前景物件的邊
緣更加潔淨，從而獲得最後的前景物件遮
罩，其前景物件邊緣上每一點的精煉則是使
用式子(7)來判斷該像素點要不要從前景物
件遮罩中刪除，其中 ),( yxEkI 是前景物件邊緣
 5
  
(a) 
  
(b) 
  
(c) 
圖 8. Gupta 等人所提出方法的模擬結果。 
 
 
  
(a) 
  
(b) 
  
(c) 
圖 9. Hu 等人所提出方法的模擬結果。 
 
 
 
  
(a) 
  
(b) 
  
(c) 
圖 10. 本研究所提出方法的模擬結果。 
 
實驗模擬中是採用自行拍攝的大廳視
訊(Lobby sequence)來估算所提出方法的效
能，大廳視訊共含有 250 張影格，每張影格
大小為 480640× 像素。大廳視訊的基準樣板
(ground truth)是採用手工給定人工塗鴉資訊
並透過封閉型去背法來獲得，如圖 7 所示。
圖 7(a)是大廳視訊序列的第 48 張及第 141
張影格，圖 7(b)為相關的手工給定人工塗鴉
示意圖，圖 7(c)是所獲得的透明圖層遮罩，
圖 7(d)則是利用透明圖層遮罩提取出的具
透明度前景物件並合成到一個純綠色背景
的示意圖。 
另外，本研究分別和 Gupta 等人[4]及
Hu 等人[6]所提出的方法進行效能比較，其
相關為模擬結果如圖 8~圖 10 所示。 
另外，為了進一步評量方法的效能指
標，故採用了平均絕對誤差(mean absolute 
error, MAE)來進行評估，其公式如式子(8)
所示，其中 ),( jitα 為所獲得的透明圖層遮
罩、 ),( jitα 則為基準樣板的透明圖層遮罩。
經計算其視訊影格的平均 MAE 後，獲得
Gupta 等人的方法為 210512.7 −× 、Hu 等人的
方 法 為 210992.4 −× 、 本 研 究 的 方 法 為
210423.0 −× 。從上述的模擬結果圖及平均
MAE 值均可證明本研究所提出的方法具有
很好的效能。 
0BAUTOMATIC VIDEO MATTING USING CLOSED-FORM MATTING BASED 
ON AUTOMATIC GENERATING TRIMAPS 
 
Wu-Chih Hu (胡武誌)1, Jung-Fu Hsu (許榮富)2, Deng-Yuan Huang (黃登淵)3 
 
1,2Department of Computer Science and Information Engineering, 
National Penghu University of Science and Technology, Taiwan 
{wchu, d98523002}@npu.edu.tw 
3Department of Electrical Engineering, 
Dayeh University, Taiwan 
kevin@mail.dyu.edu.tw 
 
ABSTRACT 
 
This paper proposes an automatic video matting scheme 
that uses closed-form matting based on automatic 
generating trimaps. In the proposed method, foreground 
masks are obtained using the hybrid video object 
segmentation. Next, the trimaps are automatically 
generated using two different scanning windows from 
the obtained foreground masks. Finally, closed-form 
matting is used with the automatically generated trimaps 
to yield the alpha mattes and the foreground objects 
with an opacity estimate. Experimental results show that 
the proposed method has good performance in 
automatic video matting. 
 
Keywords video matting; closed-form matting; alpha 
matte; opacity estimate; trimap. 
 
I. INTRODUCTION 
 
In video matting, the background is removed from a 
video sequence to obtain the foreground object along 
with an opacity (alpha) estimate for each pixel covered 
by the object. Therefore, video matting is not a binary 
segmentation. Video matting plays an important role in 
video editing applications, especially for commercial 
television and film production. 
Typically, the alpha matte is computed using a blue 
(or green) background for easier segmentation of the 
foreground object in video matting. However, this 
technique is not suitable in all situations since it requires 
a calibrated studio setup with special equipment. 
Video matting is an extension of image matting. 
Image matting was first mathematically established by 
Porter and Duff [1]. Image matting considers the 
problem of estimating the opacity of each pixel in a 
given image. The given image is assumed to be a 
composite of a foreground image and a background 
image using the compositing equation, where the color 
of the given image is assumed to be a convex 
combination of corresponding foreground and 
background colors with the associated opacity value 
(alpha value). 
For each pixel in a color image, the compositing 
equation produces 3 equations (RGB channels) in 7 
unknowns. Consequently, image matting is a highly 
under-constrained problem. In order to solve this 
problem, most existing methods of image matting 
require the user to provide additional constraints in the 
form of a trimap or a set of scribbles (brush strokes). 
Most existing methods of video matting are 
supervised video matting (non-automatic matting or 
semi-automatic matting) methods [2-6] that are time-
consuming and troublesome for the user. Therefore, 
automatic video matting (unsupervised video matting) is 
desirable. For automatic video matting to solve the 
alpha matte from a highly under-constrained problem 
without any user input is a great challenge. 
Object segmentation-based trimap estimation can be 
used for automatic video matting. Object segmentation 
can extract a foreground object from the background. A 
trimap is automatically constructed based on the 
obtained foreground object and background to solve the 
alpha matte. Therefore, automatic video matting can be 
performed using video object segmentation [7-10] and 
image matting. 
Closed-form matting [11] does not require exact 
estimates for the foreground and background; a few 
scribbles are sufficient for extracting a quality alpha 
matte. Methods that use video object segmentation and 
closed-form matting have been proposed to achieve 
automatic video matting [12-14]. 
Jain et al. [12] proposed an automatic scribbling 
approach based on the motion of the foreground object 
in a video sequence to obtain automatic video matting. 
A motion analysis technique [15] is used to determine 
the probability map, which defines the pixel as a 
foreground pixel or a background pixel. The corrupted 
probability map is then refined and reconstructed using 
a noise filter and morphological techniques to obtain a 
clear segmentation between the foreground and the 
background. Then, each frame is automatically 
15=GT  is set from experience. Fig. 3(a) shows the 
gradient-variation detection of Fig. 2(a). 
Gkk Tyxyx ≥− − ),(),( 1GG  (1) 
The coarse moving object mask is obtained using Eq. 
(2), where the operator U  is the Boolean operator “OR”; 
),( yxCkMO , ),( yx
M
kMO , and ),( yx
G
kMO  are the 
coarse moving object mask, motion detection, and 
gradient-variation detection of point ),( yx  in the thk  
frame, respectively. Fig. 3(b) shows the detection of the 
coarse moving object mask using Fig. 2(c) and Fig. 3(a). 
),(),(),( yxyxyx Gk
M
k
C
k MOMOMO U=  (2) 
 
    
                  (a)                                          (b) 
Fig. 3. Coarse moving object mask. (a) Gradient-
variation detection; (b) detection of coarse moving 
object mask. 
 
The coarse moving object mask has background 
noise, thus the probability of noise distribution [10] is 
used to eliminate noise. 
In background subtraction, the angle-module rule [7] 
is used to obtain the foreground, where 00 8=ω  and 
100 =h  are set from experience. Then, shadows and 
reflections are removed using Eqs. (3) and (4), 
respectively [7], where ),( yxkB  is the background 
image; 01 0=shφ , 02 2=shφ , 11 =shh , 5.02 =shh , 
0
1 0=rfφ , 02 2=rfφ , 21 =rfh , and 12 =rfh  are set from 
experience. 
⎩⎨
⎧ ⋅≥≥⋅≤≤=
otherwise ,0
)),((),()),(( and ),( if ,1
),( 2121
yxhyxyxhyxΘ
yx kshkkshshkshk
BIΒ
Sh
φφ  
 (3) 
⎩⎨
⎧ ⋅≥≥⋅≤≤=
otherwise ,0
)),((),()),(( and ),( if ,1
),( 2121
yxhyxyxhyxΘ
yx krfkkrfrfkrfk
BIB
Rf
φφ  
 (4) 
In background initialization and background 
updating, the background image is void in the initial 
frame; that is, every pixel in the background image is 
unmarked. Then, the pixels in the background image are 
marked and replaced by the associated pixels in the 
current image except for the moving object mask.  
Background initialization and updating is performed 
using Eq. (5), where 5.0=λ  is set from experience. 
),( yxkMO  is the fine moving object mask. ),( yxMk  
is the marked image, where 0),( =yxMk  and 
1),( =yxMk  denote that pixel ),( yx  is unmarked and 
marked in the background image, respectively.  
When pixels in the background image are all marked 
or not all marked in the last frame, the feedback scheme 
is used with the constructed background image to return 
to the first frame in a video sequence to obtain high-
accuracy video object segmentation. 
 
⎪⎩
⎪⎨
⎧
==
==×−+×
=
=
−
−
1),(  1),(  ,),(
0),(  1),(  ,),()1(),(
0),(  ,),(
),(
1
1
yxandyxifyx
yxandyxifyxyx
yxifyx
yx
kk
kkk
k
k
MOMkB
MOMkBI
MkI
B λλ  
 (5) 
When the fine moving object mask and the 
foreground extraction are obtained using the frame 
difference scheme and background subtraction, 
respectively, the merging processing is performed. In 
the merging processing, the result obtained using 
background subtraction (foreground extraction mask) is 
used if the pixel is in the still region; the result obtained 
using the frame difference scheme (fine moving object 
mask) is used if the pixel is in the moving region; 
otherwise the pixel is marked in black, as defined in Eq. 
(6), where ),( yxkBS  is the background subtraction 
mask; ),( yxkFD  is the frame difference mask; stillR  is 
the still region; and movingR  is the moving region. Rules 
of object region determination [8] are used to detect the 
still region and moving region. Fig. 4(a) shows the 
result obtained using the merging processing. 
⎪⎩
⎪⎨
⎧
∈
∈
=
else
Ryxifyx
Ryxifyx
yx movingt
stillt
Merge
t
 ,0
),(  ,),(
),(  ,),(
),( FD
BS
MO  (6) 
 
    
                  (a)                                          (b) 
Fig. 4. Foreground mask. (a) result obtained using the 
merging processing; (b) result obtained using moving 
object refinement. 
 
In practice, a residual background region appears in 
the result obtained using the merging processing. 
Therefore, moving object refinement is proposed to 
remove the residual background region and to store 
hole-regions with the background information inside the 
foreground object. Moving object refinement mainly 
consists of hole-region filling, object boundary 
refinement, and residual region removal. In the hole-
region filling, the point in the hole-region inside the 
foreground object mask is first remarked in white if it is 
not the background pixel (namely, 0),( =yxkB ). Next, 
the residual hole-region inside the foreground object is 
filled if its area is less than its circumference. Finally, 
connected component labeling is used to label each 
respectively; the figures show the results of the th48  
and th141  frames of the Lobby sequence.  
 
    
(a) 
    
(b) 
    
(c) 
   
(d) 
Fig. 7. Ground truths of the Lobby sequence. (a) 
Original frames, (b) manual scribbles, (c) alpha mattes, 
(d) foreground objects with an opacity estimate with a 
green background. 
 
Fig. 8(a)-Fig. 10(a) show the automatic scribbling 
or trimaps. Fig. 8(b)-Fig. 10(b) show the obtained alpha 
mattes. Fig. 8(c)-Fig. 10(c) show the obtained 
foreground objects with an opacity estimate with a 
green background. In the methods proposed by Gupta et 
al., the check boundary r was set as 10 pixels; scribble 
size m was set as 3 pixels; and the number of 
normalized cuts was set as 5. 
The experimental results show that the proposed 
method obtains better automatic video matting, 
especially the matting of hole-regions with the 
background information inside the foreground object. 
Besides, the results obtained using the methods 
proposed by Gupta et al., and Hu et al. are very easily 
affected by the reflection. 
 
    
(a) 
    
(b) 
    
(c) 
Fig. 8. Video matting using the method proposed by 
Gupta et al. (a) Automatic scribbling, (b) alpha mattes, 
(c) foreground objects with an opacity estimate with a 
green background. 
 
    
(a) 
    
(b) 
    
(c) 
Fig. 9. Video matting using the method proposed by Hu 
et al. (a) Automatically generated trimaps, (b) alpha 
mattes, (c) foreground objects with an opacity estimate 
with a green background. 
 
algorithm based on change detection and 
background updating,” International Journal of 
Innovative Computing, Information and Control, 
Vol. 5, No.7, pp. 1797-1810, 2009. 
[9] L. Wang and N.H.C. Yung, “Extraction of moving 
objects from their background based on multiple 
adaptive thresholds and boundary evaluation,” 
IEEE Transactions on Intelligent Transportation 
Systems, Vol. 11, pp. 40-51, 2010. 
[10] W.-C. Hu, “Real-time on-line video object 
segmentation based on motion detection without 
background construction,” International Journal of 
Innovative Computing, Information and Control, 
Vol. 7, No. 4, pp. 1845-1860, 2011. 
[11] A. Levin, D. Lischinski, and Y. Weiss, “A closed-
form solution to natural image matting,” IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence, Vol. 30, No. 2, pp. 228-242, 2008. 
[12] A. Jain, M. Agrawal, A. Gupta, V. Khandelwal, “A 
novel approach to video matting using automated 
scribbling by motion analysis,” Proceedings. of 
IEEE International Conference on Virtual 
Environments, Human-Computer Interfaces, and 
Measurement Systems, Istambul, Turkey, pp. 25-30, 
2008. 
[13] A. Gupta, S. Mangal, P. Nagori, A. Jain, and V. 
Khandelwal, “Video matting by automatic 
scribbling using quadra directional filling of 
segmented frames,” Proceedings of 2nd IEEE 
International Conference on Computer Science and 
Information Technology, Beijing, China, pp. 336-
340, 2009. 
[14]W.-C. Hu, D.-Y. Huang, C.-Y. Yang, and J.-F. Hsu, 
“Automatic video object segmentation with opacity 
estimate,” Proceedings of the 4th International 
Conference on Genetic and Evolutionary 
Computing, Shenzhen, China, pp. 683-686, 2010. 
[15] C.K. Williams and M.K. Titsias, “Greedy learning 
of multiple objects in images using robust statistics 
and factorial learning,” Nerual Computation, Vol. 
16, No. 5, pp. 1039-1062, 2004. 
[16] J. Shi and J. Malik, “Normalized cuts and image 
segmentation,” IEEE Transactions on Pattern 
Analysis and Machine Intelligence, Vol. 22, No. 8, 
pp. 888-905, 2000. 
 2
(Speaker: Prof. Hideyuki Takagi, Japan)、Learning from Ants and Bees: Nature 
Inspired Problem Solving (Speaker: Prof. Saman Halgamuge, Australia)及
Hybrid ICT Platform for Intelligent Systems (Speaker: Prof. Dickson Lukose, 
Malaysia)。考量個人研究領域與興趣，故只參與了 Prof. Hideyuki Takagi 及
Prof. Saman Halgamuge 前兩個的演講場次，這兩個場次的講題內容對於個
人在機器學習領域的研究有更進一步的了解及省思，再加上會中有多位與
會學者的提問，著實讓我受益良多。 
本人所發表的論文則是被安排在 12/6日(第二天)上午 11:45~13:15進行
的 Session : Intelligent Data Analysis And Applications 的第 3 篇論文發表。在
該 section 中共有六篇論文，所有作者均出席該 session 並發表論文，會中討
論相當熱烈，發表過程順利，也同時和與會學者交換研究心得與相關問題
討論，是一個成功的論文發表會。 
 
二、 與會心得 
本次會議涵蓋的主題範圍非常廣泛，包含有混合智慧系統(hybrid 
intelligent systems)、影像與訊號處理軟性計算(soft computing)、智慧型網路
建模與通訊、智慧型資料採礦(data mining)、控制與自動化軟性計算、多重
代理系統與應用(multi agent systems and applications)…等主題，是一個探討
混合智慧系統的國際研討會。此次研討會除了台灣的學者外，亦有不少日
本、英國、美國、南非、澳大利亞…等地約 40 幾國的學者與會。除個人論
 4
五、 攜回資料名稱及內容 
會議摘要論文集、論文光碟片及會議手冊。 
Figure 1. The proposed method of gait recognition for different people 
groups 
A. Background Modeling 
The principle of background modeling is simple in that 
we can consider the pixel to be a background point if the 
change of gray intensity of that pixel between adjacent 
frames is small or approaching to zero. Therefore, the 
temporal difference of frames is utilized to find the pixels 
with small change in gray intensity between consecutive 
frames. The pixel in background is determined as. 
 11 ( , ) ( , ) ( , )( , )     1
0
t t
t
if F x y F x y Th x y
BP x y t
otherwise
­  ° t  ®
°¯
where  represents that the pixel of  in 
current frame t  is a background pixel if the intensity change 
is less than the threshold  of which the value is 
different pixel by pixel, and it is determined adaptively, 
t
( , ) 1tBP x y  
)
( , )p x y
( , )Th x y
( ,F x y  and 1( ,t )F x y
t

( , )x y
 are gay intensities in current frame 
t  and previous frame 1 , respectively. Therefore, the gray 
intensity of pixels in background for current frame t , i.e., 
, is calculated as. 

tBG
  ),(),(),( yxFyxBPyxBG ttt u 
The average of T frames for each pixel is further 
carried out to represent the image of absolute background 
.
( , )p x y
( , )TBG x y
  
1 1
( , ) ( , ) / ( , )
T T
T t t
t t
BG x y BG x y BP x y
  
 ¦ ¦
Figure 2. Result of background modeling. (a) Original image, and 
modeling absolute background using (b) 5 frames, and (c) 10 frames. 
To determine the threshold  adaptively, 
temporal update of this parameter is given as. 
( , )Th x y
 1 1( , ) ( 1) ( ( , ) ( , ))( , )   1t t tt
Th x y t F x y F x y
Th x y t
t
 u   ª º t« »« »
 
where the initial value is set to 0. The result of 
background modeling is shown in Fig. 2. 
0 ( , )Th x y
B. Motion Detection by Background Subtraction 
The purpose of motion detection is to segment the gait 
silhouette of a walking people from their absolute 
background. The idea of background subtraction is quite 
direct in that the change in gray intensity of pixels is 
noticeable if objects appear rightly in these pixels. Therefore, 
the shape of gait silhouette is detected as. 

1 ( , ) ( , )
( , )
0
t B
t
if F x y B x y Th
D x y
otherwise
­  t
 ®
¯
 
where  is a gray level of absolute background at 
pixel ,
( , )B x y
( , )p x y BTh
t
 is a threshold value set to 23 by 
experiments, and D x  is a resulting binary image for 
extracting the gait silhouette of a walking people. However, 
since the segmented clusters in  are not always 
complete, morphological processing, including the methods 
of erosion and dilation, is further performed. The result of 
motion detection using background subtraction is shown in 
Fig. 3. 
( , )y
( , )tD x y
Figure 3. Result of motion detection. (a) Absolute image, (b) current 
image, (c) result of background subtraction, and (d) result after 
morphological processing in (c). 
C. Shadow Removal 
The foreground pixels can be considered as shadow 
pixels if their gray intensities are darker than those in 
background [12], which is primarily caused by non-uniform 
illumination on motion objects.  Shadow often leads to the 
deformation of the shape of human gait silhouette, and thus it 
unavoidably increases the erroneous recognition rate of 
human gait. Hence, the method inspired by [12] is proposed 
to effectively remove the influence of shadow on motion 
objects as. 
602 2011 11th International Conference on Hybrid Intelligent Systems (HIS)
gait silhouettes. The highest CRR of 81.0% in the case of 
n=20 were obtained, indicating the feasibility of the 
proposed method. 
Figure 5. Video sequences of 5 different people groups. (a) The pregnant, 
(b) the child, (c) the adult, (d) people with a walking stick, and (e) the aged. 
To analyze the misclassification of gait silhouettes for the 
5 different people groups, a confusion matrix of CRR is 
analyzed, as listed in Table I, from which the three groups of 
the adult, the people with a walking stick, and the aged are 
most confused of having the highest misclassification rate of 
16.6% due to their similar silhouettes in shape. For the 
people group with a walking stick, when the walking stick is 
occluded by walker’s body, this group can be easily 
recognized as the groups of the adult and the aged. Thus, 
high misclassification rates can arise. Moreover, the two 
groups of the adult and the children are similar in shape but 
different in scale. This result causes a misclassification rate 
of 13.3% between them since the shape of the two groups is 
similar described by the FDs that has the favorable property 
of scale invariance. 
TABLE I. CONFUSION MATRIX OF CORRECT RECOGNITION RATE FOR 
THE CASE OF n=20 FOURIER DESCRIPTORS
The 
pregnant 
The 
children
The 
adult
People/w
walking
stick
The aged
The 
pregnant 
93.4% 0 6.6% 0 0 
The 
children
0 86.7% 13.3% 0 0 
The 
adult
0 0 75.1% 16.6% 8.3% 
People/w
walking
stick
0 0 16.6% 66.8% 16.6% 
The aged 5.0% 0 11.6% 0 83.4% 
IV. CONCLUSION AND FUTURE WORK
In this paper, we have proposed a novel method of 
recognizing the gait silhouettes for different people groups. 
The correct recognition rate of 81% is obtained using only 
the first 20 coefficients of FDs, indicating the feasibility of 
the proposed method. However, the three groups of the adult, 
the people with a walking stick, and the aged for the test 
video sequences have a similar shape, especially when the 
walking stick is occluded by walker’s body, to cause an 
erroneous recognition rate of 20% between them. Hence, the 
increase of discriminability between the three groups can 
leave as the future work to improve the recognition rate of 
gait silhouettes for different people groups. 
ACKNOWLEDGMENT
This work is partly supported by a grant from National 
Science Council, Taiwan, under contract NSC-100-2221-E-
212-020. 
REFERENCES
[1] Z. Hong, Z. Jun, and Zhijing, “A new method of pedestrian gait 
classification,” in: Proc. IEEE Int. Conf. on Educational and 
Information Technology, Chongqing, China, 2010, Vol. 3, pp. 268-
272. 
[2] E.H. Zhang, H.B. Ma, J.W. Lu, and Y.J. Chen, “Gait recognition 
using dynamic gait energy and PCA+LPP method”, in: Proc. IEEE Int. 
Conf. on Machine Learning and Cybernetics, Baoding, China, 2009, 
pp. 50-53. 
[3] X.T. Chen, Z.H. Fan, H. Wang, and Z.Q. Li, “Automatic gait 
recognition using kernel principal component analysis”, in: Proc. 
IEEE Int. Conf. on Biomedical Engineering and Computer Science, 
Wuhan, China, 2010. 
[4] S.D. Mowbray, and M.S. Nixon, “Automatic gait recognition via 
Fourier descriptors of deformable objects,” LNCS 2688, pp. 566–573, 
2003. 
[5] Z. Ling, C. Zhao, Q. Pan, Y. Wang, and Y. Cheng, “Analyzing 
human movements from silhouettes via Fourier descriptor,” in: Proc. 
IEEE Int. Conf. on Automation and Logistics, Jinan, China, 2007, pp. 
231-236. 
[6] D. Xiao, and L. Yang, “Gait recognition using Zernike moments and 
BP neural network,” in: Proc. IEEE Int. Conf. on Networking, 
Sensing and Control, Sanya, China, 2008, pp. 418-423. 
[7] B. Ye, and Y.M. Wen, “Gait recognition based on DWT and SVM,” 
in: Proc. IEEE Int. Conf. on Wavelet Analysis and Pattern 
Recognition, Beijing, China, 2007, Vol. 3, pp. 1382-1387. 
[8] X. Yang, J. Dai, Y. Zhou, and J. Yang, “Gabor-based discriminative 
common vectors for gait recognition,” in: Proc. IEEE Int. Conf. on  
Image and Signal Processing, Sanya, China, 2008, Vol. 4, pp. 191-
195. 
[9] J. Wu, J. Wang, and L. Liu, “Feature extraction via KPCA for 
classification of gait patterns,” Human Movement Science, Vol. 26, 
No. 3, June 2007, pp. 393-411. 
[10] J. Wu, “A novel approach for discrimination of human gait using 
kernel learning algorithm,” in: Proc. IEEE Int. Conf. on Natural 
Computation, Fuzhou, China, 2010, Vol. 6, pp. 3253-3256. 
[11] P.R.G. Harding, and T.J. Ellis, “Recognizing hand gesture using 
Fourier descriptors,” in: Proc. IEEE Int. Conf. on Pattern Recognition, 
Cambridge, UK, 2004, Vol. 3, pp. 286-289. 
[12] R. Cucchiara, C. Grana, M. Piccardi, A. Prati, and S. Sirotti, 
“Detecting moving objects, ghosts, and shadows in video streams,” 
IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 
25, Oct. 2003, pp 1337-1342. 
[13] R.E. Fan, P.H. Chen, and C.J. Lin. “Working set selection using 
second order information for training SVM,” Journal of Machine 
Learning Research, Vol. 6, 2005, pp. 1889-1918. 
604 2011 11th International Conference on Hybrid Intelligent Systems (HIS)
 2
Computation for Future Internet: Internet of Things (Speaker: Prof. Han-Chieh 
Chao, Taiwan)及 Hot Issues in Evolutionary Many-Objective Optimization 
(Speaker: Prof. Hisao Ishibuchi, Japan)。考量個人研究領域與興趣，故只參與
了 Prof. Han-Chieh Chao 及 Prof. Hisao Ishibuchi 後兩個的演講場次，這兩個
場次的講題內容對於個人在物聯網及多目標最佳化等領域的研究有更進一
步的了解及省思，再加上會中有多位與會學者的提問，著實讓我受益良多。 
本人所發表的論文則是被安排在8/28日(第四天)上午10:30~12:20進行
的 Session : Multimedia Innovative Computing 的第 1 篇論文發表。本人為該
session 的 chair，該 section 中共有六篇論文，所有作者均出席該 session 並
發表論文，發表過程相當順利，同時也和與會學者交換研究心得與相關問
題討論，其中第一位 Keynote Speaker Prof. Valentina E. Balas 全程參與本
session，故是一個成功的論文發表會。 
 
二、 與會心得 
本次會議涵蓋的主題範圍非常廣泛，包含有多媒體的進化計算、雲端
計算的資安、人工智慧應用於電子服務、混合式進化演算法、行動服務、
電腦網路的進化計算…等主題，是一個探討遺傳與進化計算國際研討會。
此次研討會除了台灣的學者外，亦有不少日本、美國、羅馬尼亞、大陸…
等地的學者與會。除個人論文發表場次外，亦參與了其他場次的論文發表，
會中除了與論文作者有交談、互換研究心得。本次出席會議的台灣學者(含
 Novel Detection of Image Forgery for Exchanged Foreground and Background Using 
Image Watermarking Based on Alpha Matte 
 
Wu-Chih Hu 
Department of Computer 
Science & Information 
Engineering, 
National Penghu University 
of Science and Technology, 
Penghu, Taiwan 
wchu@npu.edu.tw 
Wei-Hao Chen 
Graduate Institute of 
Electrical Engineering and 
Computer Science, 
National Penghu University 
of Science and 
Technology,Penghu, Taiwan 
d99523005@npu.edu.tw  
Deng-Yuan Huang 
Department of Electrical 
Engineering, 
Dayeh University, 
Changhua, Taiwan 
kevin@mail.dyu.edu.tw 
 
Ching-Yu Yang 
Department of Computer 
Science & Information 
Engineering, 
National Penghu University 
of Science and Technology, 
Penghu, Taiwan 
chingyu@npu.edu.tw 
 
 
Abstract—This paper proposes an novel method to detect image 
forgery for exchanged foreground and background by using image 
watermarking based on alpha matte. In the proposed method, the 
DWT-DCT-SVD-based image watermarking is used and applied 
on the alpha mattes obtained using image matting algorithms. The 
difference of singular values is used to determine the exchanged 
foreground or exchanged background. Experimental results show 
that the proposed method can obtain accurate detection of image 
forgery for exchanged foreground or background. Therefore, the 
proposed method is a high-value tool of the image forgery 
detection. 
Keywords-image watermarking; alpha matte; image matting; 
exchanged image;image forgery 
I.  INTRODUCTION 
With the rapid development of image processing 
technology and powerful editing of multimedia tools, it has 
become easier to duplicate and manipulate images without 
degrading the quality and leaving any obvious visual clues. 
Abusive use of image forgery has become a serious problem, 
thus the authentication of images is an important study issue. 
Therefore, it is also a challenging task to obtain accurate and 
robust detection of image forgery. 
Image forensics can be divided into active forensics and 
passive forensics [1, 2]. The active forensic approach is a 
non-blind approach. Image watermarking [3-5] is a popular 
technique among the non-blind approaches. Image 
watermarking embeds a watermark at the recording time and 
extracts hidden message later to verify the image 
authenticity. 
The passive forensic approach is a blind approach. No 
supplementary information is used in the passive forensic 
approaches, such as copy-move forgery detection [6-8] and 
tampering detection of composite images [1, 2, 9]. Therefore, 
the passive forensic approaches are more realistic than the 
active forensic approaches. 
Image matting is the process of extracting the foreground 
object from an image along with an opacity estimate for 
each pixel covered by the object. Although image matting 
has been studied for more than two decades, image matting 
has received increasing attention in the last decade and many 
methods have been proposed for image matting [10]. It is 
worth mentioning that state-of-the-art matting algorithms 
were proposed in the last five years, such as easy matting 
[11], closed-form matting [12], spectral matting [13], and 
modified spectral matting [14, 15]. 
Using image matting can obtain the more realistic image 
composition of the new background image and the extracted 
foreground object with the alpha matte. However, the copy-
move forgery detection can not work well for the above 
composite images. Furthermore, the tampering detection of 
composite images uses the content consistency of the image 
to obtain the forgery detection, thus it also can not obtain the 
accurate forgery detection of the composite image based on 
the alpha matte. Moreover, the image watermarking is used 
to obtain authentication of the whole image, thus it is not 
suitably to detect image forgery for exchanged foreground or 
exchanged background. 
In this paper, we propose a novel image forensic method 
to detect the exchanged foreground and background using 
image watermarking based on alpha matte. In the proposed 
method, the DWT-DCT-SVD-based image watermarking [4] 
is used to embed the watermark, and the alpha matte is 
obtained using the known image matting algorithm. Two 
different watermarks are embedded into the foreground 
image and background image based on obtained alpha matte. 
Then, the difference of singular values is used to determine 
the exchanged foreground or exchanged background of an 
image. Experimental results show that the proposed method 
has good performance on image forgery detection. 
The rest of this paper is organized as follows. The 
proposed image forensic method of exchanged foreground 
and background detection is described in Section II. Section 
III presents experimental results and their evaluations. 
Finally, the conclusion is given in Section IV. 
II. IMAGE FORENSIC METHOD OF EXCHANGED 
FOREGROUND AND BACKGROUND DETECTION 
In the DWT+DCT+SVD-based watermarking method 
[4], the cover image is transformed from the RGB color 
space into the YCbCr color space to obtain the gray-level 
image. The DWT is then applied to the gray-level image to 
obtain the LL subband image. The DCT is then applied to 
2012 Sixth International Conference on Genetic and Evolutionary Computing
978-0-7695-4763-3/12 $26.00 © 2012 IEEE
DOI 10.1109/ICGEC.2012.114
245
 III. EXPERIMENTAL RESULTS 
Experiments were conducted on a computer with an 
Intel(R) Core(TM)2 Quad Q8200 @2.33 GHz CPU and 
2GB of RAM. The algorithms were implemented in Matlab 
R2011a. The watermarks NPU-logo and CSIE-logo were 
used to embed into the foreground and background images, 
respectively, as shown in Fig. 3(a) and Fig. 3(b). The 
threshold 10=Th  was set by experience. The alpha matte of 
the tested image was obtained using the component-hue-
difference-based spectral matting [15]. 
 
       
                                 (a)                                              (b) 
Figure 3. Used watermarks: (a) NPU-logo, (b) CSIE-logo. 
Six tested images were used to evaluate the performance 
of the proposed image forgery detection. Figs. 4(a)-4(f) 
show images of a face, woman, bear, Amira, Kim, and 
pharos, respectively. Figs. 5(a)-5(f) show the alpha mattes of 
Figs. 4(a)-4(f), respectively. 
Figs. 6-11 are the results of image forgery detection 
using the proposed method, where Figs. 6(a)-11(a) are the 
detected results of image forgery for exchanged background, 
and Figs. 6(b)-11(b) are the detected results of image forgery 
for exchanged foreground. Experimental results show that 
using the proposed method can obtain the accurate detection 
of image forgery for exchanged foreground or background. 
Therefore, the proposed method has good performance on 
image forgery detection. 
 
   
               (a)                                     (b)                                       (c) 
   
              (d)                                       (e)                                          (f) 
Figure 4. Tested images: (a) face, (b) woman, (c) bear, (d) Amira, (e) Kim, 
(f) pharos. 
   
               (a)                                     (b)                                       (c) 
   
              (d)                                       (e)                                          (f) 
Figure 5. Alpha mattes: (a) face, (b) woman, (c) bear, (d) Amira, (e) Kim, 
(f) pharos. 
 
  
Figure 6. Forgery detection of the face image: (a) result of exchanged 
background, (b) result of exchanged foreground. 
 
  
Figure 7. Forgery detection of the woman image: (a) result of exchanged 
background, (b) result of exchanged foreground. 
 
  
Figure 8. Forgery detection of the bear image: (a) result of exchanged 
background, (b) result of exchanged foreground. 
247
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/08
國科會補助計畫
計畫名稱: 高準確性之自動化視訊去背的研究
計畫主持人: 胡武誌
計畫編號: 100-2221-E-346-008- 學門領域: 影像處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
