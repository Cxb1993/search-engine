摘要 
對於裝設在移動車輛擋風玻璃後的攝影機，
玻璃上的水滴很容易影響攝影機影像的清晰度，也
會導致相關功能的失效，如：車道偏移警示。有鑑
於此，這裡提出一個車用攝影機影像之水滴偵測與
移除的方法，偵測的步驟包含直方圖均化、影像相
減、二值化與形狀檢測。這些偵測出的雨滴接著依
照其所在建築物或者馬路區域的不同，運用不同方
法來進行移除。實驗結果顯示，雨滴可以被明顯的
移除，模擬結果也顯示其移除後影像的 PSNR 可以
達到 33dB，這些結果顯示提出的方法可以有效地
移除車用攝影機影像上的水珠。 
 
Abstract 
For a camera installed behind the windshield 
window of a moving vehicle, water drops easily 
influence the clarity of the camera view and cause the 
malfunction of related functions, such as lane 
departure warning. Therefore, a water drop detection 
and removal method was proposed. The detection 
steps consist of histogram equalization, frame 
intersection, binarization, and shape checking. Those 
detected water drops are then removed by using 
different methods according to their locations to 
achieve a desirable result. In the experimental studies, 
water drops of test videos can be removed obviously. 
Simulation results also shows that the method can 
achieve a maximum PSNR (Peak Signal to Noise 
Ratio) of 33 dB. These results show that the proposed 
method is useful and effective. 
 
1. Introduction 
Currently, cameras are widely installed in 
vehicles for various purposes, such as rear view 
camera for parking; front view camera for lane 
departure warning, front collision warning, traffic 
light or sign recognition, or vehicle digital video 
recorder (DVR); surrounding cameras for video 
monitoring and recording. They all have the same 
ultimate goal: increasing the driving safety. 
In this study, we assume a camera is installed 
behind the front windshield window of a vehicle. 
When the vehicle is driving straightly under rainy 
condition or water splash on the windshield window, 
the water drops on the window is easily to influence 
the clarity of the camera image. Of course, the 
windshield wiper is assumed unable to clean the 
water drops on the area in front of the camera view. 
Those water drops in the camera image may cause 
the malfunction of related functions mentioned in 
previous paragraph. An example is shown in Figure 1. 
The clarity of the camera image is obviously 
influenced by the water drops. Therefore, the main 
purpose of this paper is the detection and removal of 
water drops from the camera view. Here, several 
video clips were recorded by an in-vehicle DVR 
system to evaluate the performance of the proposed 
method. 
 
Figure 1. The water drops influence the clarity of 
the camera view 
For a fixed camera installed in a moving vehicle, 
those objects in the camera view are also moving 
quickly. Only the water drops exist in the video 
frames for a period of time. It means that the 
foreground object detection methods are unable to 
detect the water drops, such as background 
subtraction, temporal image differencing, optical 
flow, or Gaussian feature extraction and matching. 
Besides, the brightness change or vehicle vibration 
also influences the water drop detection in the video 
frames. The above conditions are depicted by the 
example in Figure 2. The sizes of the signs are 
changing quickly. The water drops also influence the 
clarity of the signs. It shows that the removal of these 
water drops from video frames is a challenging task. 
 
(a)                   (b) 
 
          (c)                   (d) 
Figure 2. A series of video frames with water drops 
(a) Frame 1 (b) 6 (c) 11 (d) 16 
In order to detect and remove water drops from 
the video frames, there are three key points must be 
resolved in turns:  
1. Water drop detection: Water drops on the 
windshield window have two characteristics. 
One is that they exist in the video frames for a 
period of time. The other is that their brightness 
is high caused by the condensation effect of the 
water drop. However, there are many objects 
may have the same brightness, such as sign, 
window, traffic light, and so on. Therefore, a 
shape checking step is used to prevent the 
misdetection of water drops. 
2. Patch locating: When a water drop is detected, 
the area must be replaced by the original image 
covered by the water drop. It is also called a 
patch. A patch can be located from the previous 
 The shape of a water drop is close to an 
ellipse. 
 When a water drop is more obvious, the 
change of the gradient around the boundary 
is more apparent.  
(1) Lane detection 
In this step, the video frame is divided into 
roadway and building areas according to the result of 
lane detection. In order to detect the leftmost and 
rightmost traffic lanes, the bottom half area of the 
frame is processed by dilation and subtract from the 
original frame [25]. Then, an optimal binarization 
method is used to get clear image of traffic lanes. A 
Hough transform is also used to estimate the leftmost 
lane by finding the desired traffic line that its strength 
is larger than  [24].  is set to 0.3 for ensuring the 
strength of straight line. The same step is performed 
to find the rightmost lane. The Hough transform 
algorithm is listed as below: 
Step 1: Obtain a binary mask of the image by using 
optimal binarization. The white pixels are 
deemed as the feature points for Hough 
transform. 
Step 2: Perform the following computation for every 
feature point: 
Step 2.1:  A feature point (xi, yi) is calculated by 
using Eq. (1) in polar coordinates to obtain 
the straight line (', ') passing (xi, yi). 
'sin' cos   )'('  ii yxr   (1) 
Step 2.2:  The accumulator of the line (', ') is plus 
one. 
Step 3: After all the feature points are processed, the 
accumulator of the line with the maximum 
value is the traffic lane. 
According to the above steps, the leftmost and 
rightmost traffic lanes can be detected, an example is 
shown in Figure 4. 
   
(a)           (b)            (c) 
Figure 4. An example of traffic lane detection (a) 
original frame (b) detection result (c) the 
separation of roadway and building areas 
(2)  Image pre-processing 
In the building area, the brightness of water 
drops may be not so obvious but still influences the 
clarity of frames for visual perception. Therefore, a 
histogram equalization method is used to increase the 
difference between the water drops and the gray 
background. The method is presented as follows. 
For each pixel value x, 0xL1, its probability 
density function, p(x), is computed by using Eq. (2). 
1,1,0,)(  Lx
N
n
xp x   (2) 
The nx is the count of the pixels with value x in 
L (L equals to 256 for a gray scale image). N is the 
total number of pixels. Cumulative distribution 
function c(x) is mapped from p(x) by using Eq. (3). 



x
m
mpxc
0
)()(  (3) 
Finally, the pixel value x is reallocated as I(x) 
by using Eq. (4).  
)()( xcLxI   (4) 
Oppositely, the detection of those water drops in 
the roadway area is much easier. A Sobel edge 
operator is used to locate water drop areas with large 
gradient change [25]. Some objects may also be 
detected by the edge operator, such as traffic light, 
front vehicle, as shown in Figure 6(b). They will be 
filtered by the following steps. 
(3) Intersection 
According to the characteristic of a water drop 
appearing in the same location of a series of frames, 
N successive frames are intersected to preserve the 
possible water drops in this area. An example of the 
intersection result is shown in Figure 6(c) for N=20. 
Expect to the sky area with high brightness, the water 
drops are detected clearly. The more frames are 
intersected, the more clear the water drops will be. 
However, a new water drop will take time to be 
detected. Here, N set to 20 is about 4 seconds at 5 
FPS (frame per second). 
(4) Binarization 
In this step, binarization is used to extract the 
possible water drop areas. Two kinds of binarization 
methods are used: simple image statistics (SISB) [27] 
and adaptive (AB) [28-29]. They are presented as 
follows: 
SISB method: Assume the size of a Blob is n by m. 
The value of a pixel at (x, y) is I(x, y). The steps of 
SISB method are listed below.  
Step 1: Two gradients values, denoted as Ex and Ey, 
are calculated for every pixel by using the 
following Eqs. (5) and (6). The maximum 
value of Ex(x, y) and Ey(x, y) is denoted as 
MaxG(x, y). 
|)1()1(| =)( , yx - - I, yx + Ix, yEx  (5) 
|x, y -  - Ix, y + = |Ix, yEy )1()1()(  (6) 
Step 2: For all the pixels, their MaxG(x, y) are 
summarized and denoted as TotalMG.  

 

n
i
m
j
jiMaxGTotalMG
1 1
),(  (7) 
Step 3: MaxG(x, y) is then multiplied with I(x, y). 
The summation of the product of all the 
pixels is calculated and denoted as 
TotalMGI. 
   
 

n
i
m
j
jiIjiMaxGTotalMGI
1 1
,,  (8) 
Step 4: The result threshold, denoted as TSISB, equals 
the TotalMGI divided by TotalMG. 
AB method: Each pixel is thresholding using the 
following steps: 
Step 1: The intensity, i.e., value, of a pixel at (x, y) is 
denoted as f(x, y). The sum of all f(x, y) 
terms to the left and above the pixel (x, y) is 
denoted as I(x, y). I(x, y) is computed using 
the difference result with the total number of pixels. 
The Blob passes the shape checking if its ratio, , is 
less than a pre-defined threshold, =0.3. Two 
examples of shape checking are shown in Figure 7. 
There are three images in Figure 7(a). They are the 
water drop, ellipse mask, and difference result from 
left to right. The ratio of the difference result equals 
0.18. Oppositely, the ratio of the difference result in 
Figure 7(b) equals 0.45. It cannot pass the shape 
checking. 
If a Blob passes the shape checking, the next 
checking is the mean pixel value. The mean pixel 
value of a Blob is computed and denoted as μ. Then, 
the mean pixel value of its surrounding 
eight-connection areas with the same size of the Blob 
is also computed and denote as M. A Blob passes the 
mean pixel value checking if its μ is larger than M. 
Two examples are shown in Figure 7(c) and (d).  
  
(a)                   (b) 
  
(c)               (d) 
Figure 7. The example of shape/mean value 
checking (a) and (b) shape checking (Blob, ellipse 
mask, difference result) (c) and (d) two 
eight-connection examples 
3.2 Water drop removal phase 
After those water drop areas are detected, they 
will be removed in this phase. The removal consists 
of two main steps. One is the patch locating of a 
water drop in the previous frame. The other is the 
replacement of the water drop area. For a vehicle 
driving straightly, the buildings or roads in the video 
frames are moved radially away from the vanishing 
point. Besides, the greater the distance between the 
object and the vanishing point is, the larger the 
displacement between two successive frames is.  
The following steps are designed to overcome 
the above mentioned problems. Template matching is 
used to remove the water drops in the building area. 
Morphological operation and image inpainting 
methods are used to remove the water drops in the 
roadway area. 
(1) Template matching 
Several methods are tried to locate the patch of 
a water drop in the previous frame, such as optical 
flow or Gaussian feature matching. However, they 
are failed caused by some reasons, including the 
video shaking, brightness and size change. A template 
matching method is used here to locate the patch [1]. 
The template matching method requires a template 
and a region of interest (ROI) for matching. For every 
detected water drop area, the template is 
eight-connection area as shown in Figure 7(d). The 
size of ROI in the previous frame is set to 80150. If 
the water drop area is in the right side of the building 
area, the desired area to be located is in the left side 
of the template. So, the right margin of the ROI is 
aligned along the left margin of the template. After 
the template and ROI are determined, they are resized 
to one fourth of the original size to shorten the 
matching time. The area in ROI with the highest 
matching similarity, i.e., patch, is used to replace the 
water drop in current frame. A popular method, called 
sum of absolute difference (SAD) [30], is used here 
to compute the matching difference. 
An example of template matching and 
replacement is shown in Figure 8. The original video 
frame is shown on the left-hand side. The 
replacement result is shown on the right-hand side. In 
Figure 8(a), the water drops are not detected yet. 
Therefore, the original and replacement frames are 
the same. After the template matching and 
replacement is performed, the replacement result is 
obviously better than the original frame. 
  
(a) 
  
(b) 
  
(c) 
Figure 8. An example of template matching and 
replacement (left: original frame, right: 
replacement result) (a) Frame 1 (b) 5 (c) 9 
(2) Morphological operation and inpainting 
In the roadway area, the template matching 
method is easily failed since the roadway area is 
similar. Therefore, morphological operation and 
image inpainting methods are used to remove the 
water drops. 
The difference between morphological 
operation and image inpainting is shown in Figure 9. 
The original water drop image is shown in Figure 
9(a). The water drop area detected in the previous 
phase is shown in Figure 9(b). By observing two 
examples in Figure 9(c) and (d), the result of 
morphological operation is better than that of image 
inpainting. However, the morphological operation is 
not suitable for all the cases in the roadway area. 
Therefore, the removal of a water drop is depended 
on whether the drop is covering the traffic lane. For a 
Figure 12. An example with small number of 
water drops (a) original (b) intersection (c) 
detected water drops (d) result 
 
(a)                    (b) 
 
      (c)                    (d) 
Figure 13. A water drop removal example with 
medium number of water drops (a) original (b) 
intersection (c) detected water drops (d) result 
 
(a)                    (b) 
 
      (c)                    (d) 
Figure 14. A water drop removal example with 
large number of water drops (a) original (b) 
intersection (c) detected water drops (d) result 
4.2 PSNR evaluation  
PSNR is used to evaluate the image quality after 
removing water drops. However, the video frames 
without water drops are needed for computing PSNR. 
We tried several ways to record and found that it was 
difficult to record the videos with and without water 
drops at the same time. Therefore, water drops are 
placed in the video frames manually using the image 
editing tool (Photoshop) to generate the video frames 
with water drops. The numbers of water drops for 
small, medium, and large conditions are 15, 45 and 
75, respectively. The results of the simulated frames 
are shown in Figure 15. Figure 15(a) is one of the 
video frames without water drops. The frames with 
small, medium and large number of water drops are 
shown in the left image of Figures 15(b)~(d). The 
corresponding results of water drop removal are 
shown on the right-hand side. The small number of 
water drops is removed obviously. As the increasing 
the water drops, partial drops are not removed clearly. 
It is because the artificial water drops are not very 
similar.  
 
(a) 
  
(b) 
  
(c) 
  
(d) 
Figure 15. Manual water drops results (a) a 
sample frame (b)~(d) the manual placement of 
small, medium, large number of water drops and 
the removal result 
The PSNR of the frame after removing water 
drops and the original frame without water drops is 
computed and depicted in Figure 16. There are 90 
frames. The PSNRs of frames with small number of 
water drops are higher than that with medium and 
large number of water drops. The average PSNRs are 
33.0, 29.1, and 26.9 dB for small, medium, and large 
number of water drops, respectively. The PSNR over 
30 dB is considered as an acceptable frame quality. 
Although only the PSNRs of the frames with small 
number of water drops satisfy such criterion, the 
PSNRs under the actual condition are expected to be 
higher than the current simulation results since the 
method is designed to detect and remove the real 
water drops. 
Frame No.
81716151413121111
P
S
N
R
 (
dB
)
40
35
30
25
20
Water drops:
Small
Medium
Large
 
Figure 16. The PSNR results of different number 
of water drops 
2006. 
[11] H. Zhang, Y. Sun, X. Duan, and Z. Yu, “A 
Removal Algorithm of Rain and Snow from 
Images Based on Fuzzy Connectedness,” 
ICCASM, Taiyuan, 2010. 
[12] B. R. Chang, C.-P. Young, and H. F. Tsai, 
“Intelligent Data Fusion System for Predicting 
Vehicle Collision Warning Using Vision/GPS 
Sensing,” Expert System Applications, Vol. 37, 
pp. 2439–2450, March, 2010. 
[13] N. El-Sheimy, C. Wang, and H. Sun, 
“Automatic Traffic Lane Detection for Mobile 
Mapping Systems,” In Proc. of Multi-Platform/ 
[14] Multi-Sensor Remote Sensing and Mapping 
(M2RSM), Xiamen, pp. 1, Jan. 2011. 
[15] B. T. Morris and M. M. Trivedi, “Learning, 
Modeling, and Classification of Vehicle Track 
Patterns from Live Video,” IEEE Transactions 
on Intelligent Transportation Systems, Vol. 9, 
pp. 425-437, 2008. 
[16] J. C. Halimeh and M. Roser, “Raindrop 
Detection on Car Windshields Using 
Geometric-Photometric Environment 
Construction and Intensity-Based Correlation,” 
In Proc. of IEEE Intelligent Vehicle Symposium, 
pp. 610-615, 2009. 
[17] L. Li, Yi Zhang, and Z. Li, “Future Research in 
Vehicle Vision Systems,” IEEE Intelligent 
Systems, Vol. 24, pp. 62-65, 2009. 
[18] A. Yamashita, K. T. Miura, T. Kaneko, and Y. 
Tanaka, “Restoration of Images Stained with 
Waterdrops on a Protection Glass Surface by 
Using a Stereo Image Pair,” Proceedings of 
IAPR Conference on Machine Vision 
Applications, pp. 152-155, 2005. 
[19] A. Yamashita, K. T. Miura, T. Harada, and T. 
Kaneko, “Virtual Wiper –Removal of Adherent 
Noises from Images of Dynamic Scenes by 
Using a Pan-Tilt-Zoom Camera,” Advanced 
Robotics, Vol. 19, pp. 295-310, 2005. 
[20] G. Sapiro, L. Vese, M. Bertalmio, and S. Osher, 
“Simultaneous Structure and Texture Image 
Inpainting,” IEEE Transactions on Image 
Processing, Vol. 12, pp. 882-889, 2003. 
[21] F. Chan, and J. Shen, “Variational Image 
Inpainting,” Communications on Pure and 
Applied Mathematics, Vol.58, pp. 579-619, 
May 2005. 
[22] A. Yamashita, I. Fukuchi, K. T. Miura and T. 
Kaneko, “Removal of Adherent Noises from 
Image Sequences by Spatio-temporal Image 
Processing”, Robotics and Automation, pp. 
2683, May 2008. 
[23] A. Criminisi, K. Toyama, and P. Perez, “Region 
Filling and Object Removal by Exemplar-Based 
Image Inpainting,” IEEE Trans. Image 
Processing, Vol. 13, pp. 1200-1212, 2004. 
[24] A. Geiger and M. Roser, “Video-based 
Raindrop Detection for Improved Image 
Registration,” In Proc. of IEEE Computer 
Vision Workshops, 2010. 
[25] P. E. Hart and R. O. Duda, “Use of the Hough 
Transformation to Detect Lines and Curves in 
Pictures” Communications of the ACM, Vol. 15, 
pp. 11-15, Jan. 1972. 
[26] R. C. Gonzalez and R. E. Woods, “Digital 
Image Processing,” Pearson, 2002. 
[27] F. Gasparini and R. Schettini, “Color Balancing 
of Digital Photos Using Simple Image 
Statistics,” submitted to Pattern Recognition, 
Vol. 37, pp. 1201-1217, 2004. 
[28] D. Bradley and G. Roth, “Adaptive 
Thresholding using the Integral Image,” Journal 
of Graphics, GPU, and Game Tools, Vol. 12, pp. 
13-21, 2008. 
[29] P. D. Wellner, “Adaptive Thresholding for the 
DigitalDesk,” Technical Report EPC-93-110, 
Rank Xerox Research Centre, Cambridge, UK, 
pp.18, 1993. 
[30] J. Vanne, E. Aho, T.D. Hamalainen, K. 
Kuusilinna, “A High-Performance Sum of 
Absolute Difference Implementation for Motion 
Estimation,” Inst. of Digital & Comput. Syst., 
Tampere Univ. of Technol., Vol. 16, pp. 876-883, 
July, 2006. 
 
份文件其與前後各階段文件的相關性，例如：某個模組的實作是來自於哪個設計文件亦
或者某個設計文件是來自於哪個分析規格等。在目前 traceability 的研究方面，對於可以
追溯到需求規格的部分比較不足，也就是從需求規格到分析規格的部分沒有很好的追溯
性管理機制，但這卻是目前快速變遷的環境下，需求規格的變動也同樣相當的頻繁，而
作者透過分析目前各種電腦輔助軟體工程工具(CASE tool)在這方面的支援狀況，也比較
他們所開發的工具，可以有機會提供較好的需求規格追溯性的功能，只是大約還需要二
年左右的時間來實現。 
而一位來自西班牙的學者 Luis Gracia 則是發表一個方法來進行花卉、水果或者蔬菜
辨識的應用，其簡報中運用電腦視覺的技術來辨識類似蘋果的圓形物體，不過個人的想
法中覺得就各種不同的花卉、水果與蔬菜的類型中，水果是最容易辨識的，因為其形狀
最為固定，若是要辨識花卉或者蔬菜應該是會有相當的難度，除非是有設定比較標準的
辨識環境。在當時，另一位與會者則是提出類似的問題，表示大家都有相同的疑問，而
這也是需要進一步加強的地方。 
還有一位泰國的學者 Pijitra Jomsri 則是針對資訊檢索的領域提出其研究成果，其針
對目前較為熱們的 social networking 方面的研究論文，來進行相關度的排序，資料來源主
要以 citeseer 網站的論文資訊為主，接著進行相關度的計算與排序，不過其計算模型還算
比較簡單。 
在參與不同國家主辦的國際會議時，都可以了解該國家人民的做事步調及態度，這
次的會議算是比較特別的，首先在會議報到的現場，參加的人員必須自己先去找名牌，
接著排隊領取會議資料，原本看到排隊的人大概只有十來位，想說應該還蠻快的，後來
足足等了約一個小時才完成報到，原因是只有兩位工作人員，而且拿名牌給工作人員看
了之後，她才開始檢查報名繳費的資料，然後填寫收據，接著收據給另一位人員時，他
才開始拿大會手冊、論文集與相關資料，這當中還出現這位人員發現資料不足，還離開
好一陣子才拿資料回來補發的狀況，但是所有與會者都還是很有耐心的依序排隊等待，
相較於台灣常出現謾罵服務人員的景象，真是值得給這些與會人員鼓鼓掌。 
 在會議過程中除了上面這些內容之外，還吸收了相當多其他學者在特定議題上的研
究成果，對於個人在這方面的研究上有相當大的助益。 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：廖珗洲 計畫編號：99-2221-E-324-039- 
計畫名稱：無所不在攝影機環境中車用情境感知之智慧服務與服務動態卸載機制的研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 1 1 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 4 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 2 100% (投稿審查中) 
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
