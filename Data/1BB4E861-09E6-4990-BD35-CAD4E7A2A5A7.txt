 2
關鍵字 – 小樣本、機器學習、模糊類神
經網路、資料趨勢、資料擴散 
 
 
1. Introduction  
 
A common situation in the manufacturing 
industry today shows that product life cycles 
are getting shorter and shorter. The company 
that can survive serious competition usually 
is innovative and produces new products 
efficiently. Many manufacturers employ the 
dynamic flexible manufacturing system 
(FMS) to promote new products as fast as 
possible. Manufacturing techniques include 
data mining to create useful management 
knowledge. However, the data are often rare 
during the early FMS stages and the built 
knowledge is not always reliable. How to 
make decisions using such small data sets 
seems to be attracting more and more 
attention. This research aims at improving 
learning accuracy for FMS scheduling by 
incorporating small data sets. 
 
Artificial neural network is widely 
utilized to recognize a non-linear system. 
Several researchers have investigated the 
properties of universal approximations 
about using neural networks [1-4]. They, 
however, all assume that they have as many 
samples as they need to train a neural 
network. Basically, researchers believe that 
without sufficient training samples, neural 
networks cause non-negligible errors. In 
fact, when the assumption is not obeyed, the 
results will not be indisputably accepted 
anymore. In the case of learning with small 
samples or insufficient samples, it implies 
that the conventional back-propagation 
neural network (BPNN) needs to be 
remodeled. Suggested approaches include to 
fill the gaps within the incomplete data set, 
and to establish a new topology for a neural 
network. 
 
Under a static and stable 
manufacturing environment, many 
scheduling models or algorithms have been 
developed to raise system performance and 
reduce production costs. However, in FMS, 
it is difficult to form a manifest scheduling 
model. In recent decades, using synthesis 
approaches, machine learning methods have 
been widely applied to scheduling in this 
situation [5–8]. In numerous simulated 
academic research models, after mimicking 
a system and generating examples as 
experience data, machine learning 
techniques were applied to determine 
system scheduling rules [9]. Several 
inductive learning approaches have been 
proposed in the published literature: the 
pattern directed scheduling system [10], 
using the ID3 learning algorithm [11] for 
FMS scheduling; a learning-aided dynamic 
scheduler introduced by Nakasuka and 
Yoshida [12] to solve scheduling problems; 
 4
neuro-fuzzy learning. Following a data 
fuzzification procedure, the domain range 
expansion technique was administered to 
enhance the domain range allowance in 
dynamic and non-enough data environments. 
The learning accuracy using fuzzified input 
proved being preferable to that using crisp 
data from traditional neural learning. 
However, their research embodied three 
critical drawbacks: 
1. The possible attribute domain range 
must be predetermined. It is clear that to 
require production managers to set all 
inputs’ range in advance will seriously 
constrain the feasible solution space and 
its applicability. 
2. The domain range expansion is 
essentially a trial-and-error procedure. 
The trial-and-error approach is usually 
computationally tedious work in 
achieving answers. 
3. The domain range expansion does not 
consider data behavior (trend). 
Understanding the trend of emerging data 
is uniquely important in small data set 
learning since the juvenile data set is 
growing like a tree. Identifying data 
sprouting direction would eventually 
result in a correct portray of the true 
population. 
 
1.3 Remodeling BPNN with Information 
Diffusion 
 
Huang and Moraga [21] combined the 
principle of information diffusion [20] with 
a traditional neural network, named 
diffusion-neural-network (DNN), for 
function learning. According to their results 
of numerical experiments, the DNN 
improved the accuracy of the BPNN. The 
information diffusion approach partially 
fills the information gaps caused by data 
incompleteness via applying fuzzy theories 
to derive new samples but the research does 
not provide clear indications for 
determining the diffusion functions and 
diffusion coefficients. Besides, symmetric 
diffusion technique sometimes over 
simplifies a generation of new samples and 
could cause overestimation of the domain 
range. Either under-estimating or 
over-estimating the ranges would lead to a 
poor accuracy. Thus, asymmetric domain 
range estimation, named data trend 
estimation, is developed as an important 
work. This research proposes a strategy 
combining data diffusion, data trend 
estimation and a fuzzy neural net in small 
data set learning to by improve non-linear 
function recognition accuracy. 
 6
2 2
2 2 2 2
If  is  and  is , 
             then 
x A y B
f p x q y r= + +  
 
where 1,A 2 ,A 1B  and 2B  are fuzzy set 
values and 1,p 2 ,p 1,q  2 ,q 1r  and 
2r are constants. Explained graphically, 
Figure 1 shows this Sugeno model and the 
corresponding neural net structure with a 
five-layer artificial neural network. 
 
Denote that the output of the i th node of 
layer l is ,l io . Therefore the outputs of 
layer 1 of the model are 
 
2
,
( ),         1, 2
( ),       3, 4
i
i
A
l i
B
x i
o
y i
µ
µ −
=⎧⎪= ⎨ =⎪⎩
 
 
where 
iA
µ  and 
2iB
µ −  are fuzzy 
membership functions that can represent 
any membership style, such as triangular or 
generalized bell functions (this study uses 
gauss functions). 
 
For nodes in layer 2, the outputs iw  are 
the product of the outputs of layer 1 and 
will be employed as the weights in layer 3: 
 
2, ( ) ( ),    1, 2i ii i A Bo w x y iµ µ= = =  
 
In layer 3, the outputs of nodes are 
normalized by the following calculations: 
 
( )3, 1 2 ,    1, 2i i io w w w w i= = + =  
 
Next, layer 4 is the defuzzy layer which 
adapts node values with the equation: 
 
( )4, ,    1, 2i i i i i i io w f w p x q y r i= = + + =  
 
where ip , iq and ir  are consequent 
parameters of the nodes. Finally, the fifth 
layer computes the output for all inputs 
using the equation: 
 
5, ,    1, 2i i i i i i
i i i
o w f w f w i= = =∑ ∑ ∑  
 
The learning tool was chosen for this 
study because it adapted to the developed 
data diffusion technique (will be depicted in 
Section 3.1), which fuzzified data in a group 
view. 
 
3.  The Proposed Methodology 
 
The proposed method involved in this 
research includes a unique data set 
fuzzification procedure, named data 
diffusion, and a data expansion and trend 
estimation technique based on data 
appearing behavior. 
 
 
3.1 Data diffusion and Data Expansion 
 
 8
diffusion method fuzzified a set of data 
using a common membership function. For 
example, given two data displayed in Figure 
2, these two data are fuzzified using one 
common fuzzy membership function with 
limits of a and b. 
 
Also, for adapting to the usage of the 
neural net, as shown in Figure 1(b), each 
attribute value in layer 1 is formed through 
combining two functions, Functions 1A  
and 2A , having limits a and b respectively. 
These two functions, Functions 1A  and 
2A , are shown in Figure 3 and these 
functions can be triangle, trapezoid, or 
gauss; this research use gauss functions. 
The main objective of this unique 
fuzzification process seeks to diffuse the 
data within the estimated set domain 
coverage in a group-consideration view. 
 
Huang and Moraga [21] proposed an 
information diffusion technique filling the 
gaps caused by incomplete samples. The 
diffusion formula was listed as: 
 
( )2 ln ( )i i ix u h xϕ= ± − × ×  (1)
 
where ix  was the estimated sample, iu  
was the collected sample, h was the 
diffusion coefficient and ( )ixϕ  was the 
value of membership function of ix . 
 
According to their computer simulation 
results, h was suggested as: 
 
0.6841(max min)                5
0.5404(max min)                6
0.4482(max min)                7
0.3839(max min)                8
2.6851(max min) ( 1)      9
for n
for n
h for n
for n
n for n
− =⎧⎪ − =⎪⎪= − =⎨⎪ − =⎪ − − ≥⎪⎩
 
where min and max was the minimum and 
maximum values of the data set, 
respectively; n was the sample size. 
Besides, ( )ixϕ  was set as: 
 
2
 9's
( ) ( ) (0.9 10 )
            0.9.........9
        {0.91, 0.92, , 0.99}
i
m
x r m
r
ϕ ϕ ϕ −= = + ×
=
∀ ∈ "
 
 
where r was the linear correlation 
coefficient of input and output data, and m 
was the second number of digit after the 
decimal point of r and it implies that ( )ixϕ  
has m 9’s after the decimal point. 
 
In their research, the sample diffusion 
process was implemented individually for 
each sample without considering the 
position of the sample in the set. In contrast 
to it, we believe that, in general, doing it in 
a global view covering the whole data set 
would provide the work of data 
transformation with a better shape in the 
sense of data set completeness. Therefore, 
 10 
 
3.3 The Method in Steps 
 
The detailed steps of the procedure are 
described as follows: 
 
Step 1. Let min and max be the 
minimum and maximum values of the 
data set, respectively. LN  and UN  
represent the number of data less and 
larger than (min max) 2+ , 
respectively. There are two reasons 
using LN  and UN  as the measure of 
skewness in the distribution of the data. 
One is to simplify the calculation of 
skewness in the distribution of the data; 
the other is because the data set 
available is small and the sample 
regression line is usually not reliable. 
Based on these two reasons, this 
research uses LN  and UN  as the 
measure of skewness in the distribution 
of the data. 
 
Step  2. The left end a is calculated 
as: 
( )2 20ˆ2 ln (10 )x
set L
L
s
a u Skew
N
ϕ −−= − (2)
and the right end b is calculated as: 
( )2 20ˆ2 ln (10 )x
set U
U
s
b u Skew
N
ϕ −−= + (3)
for 1 LN< < ∞  and 1 .UN< < ∞  
In equations (2) and (3), when LN  and 
UN  approach ∞  (when sufficient 
data are collected), a will be greater 
than min , and b will be smaller than 
max (This situation leads to a data 
range shrinkage instead of data 
expansion). Obviously, this situation is 
not acceptable. Once this situation 
takes place, a and b is set to be min and 
max , respectively. 
 
The used machine learning system 
employs the neural net software. After input 
data set has been fuzzified and expanded, a 
neuro-fuzzy model is then established using 
the software. After forming the model, the 
testing data set is used to estimate the 
learning accuracies. 
 
 
4. The Numeric Examples 
 
In this section, we shall provide two 
numerical examples adopted from Huang 
and Moraga [21] to explain the proposed 
procedure and to evaluate the method 
performance. First, we introduce the 
generation of simulation data sets and the 
used evaluation criterion. Next, we describe 
in detail for each step and compute the 
results for the data sets of multinomial and 
exponential functions. After a satisfying 
performance is confirmable, we shall apply 
the proposed method to a real case of 
 12 
 
 
 
 
flexible manufacturing system. 
 
4.1 The Generation of Simulation Data 
Sets 
 
This research adopts the generating 
approach of Huang and Moraga [21]. The 
sizes of training data sets are specified as 5, 
7 and 9. Each data set contains 10 randomly 
generated problems. Training and testing 
samples are generated randomly between 0 
and 1. For each training data set, another 
100 random samples are generated as the  
testing data set. The evaluation criterion 
used in this research is mean square error 
(MSE) and the experiment result is the 
average MSE of 10 problems for each data 
set. 
 
4.2 Detailed Steps and Simulation 
Functions 
 
In this section, two simulation functions 
adopted from Huang and Moraga [21] are 
described. One is the multinomial function: 
2 30.01 0.02 0.9y x x x= + + ; the other is the 
exponential function: 421 xy e−= − . 
 
4.2.1 Multinomial Function 
 
Table 1 presents the 10 simulation problems 
with 5 training samples. According to Table 
1 and equations (1), (2), (3), and (4), we 
constructed the values of (min max) 2+ , 
LN , UN , a , b and MSE for each problem, 
as listed in Table 2. Figure 4 shows the real 
function 2 30.01 0.02 0.9y x x x= + +  in‧notes 
and estimated function in * notes. 
 
The average of the last column of Table 
2 equals 0.00019423. The error in DNN of 
 14 
 
Figure 5. FMS simulation model layout 
 
 
Table 3. Operations of four machines 
Machine Operation 
M1 M2 M3 M4 
op1   10   
op2  12 15   
op3    13 17 
op4   18 8  10 
op5  7   12  
op6  5     
 
 
 
Table 4. Required operations of parts 
Part Type Required operations Due date (unit time) 
p1 op3 → op5 → op2 → op1 1000 
p2 op6 → op3 → op2 → op5 → op4 1400 
p3 op1 → op2 → op4 1200 
 
 
 
simulation model are as follows:  
(1) The load/unload station capacity is 
unlimited.  
(2) Parts are transported to CNC machines by 
AGVs, one part at a time.  
(3) Only one part can be processed in each 
machine at a time.  
(4) The processing time of the machine 
includes set-up time.  
(5) The breakdown of machines and AGVs is 
not considered.  
 16 
 
Table 5. 200 data for training and testing 
Buffer 
size 
Arrival rate 
of parts 
AGV 
speed
FCFS performance 
in machine use (%)
SPT performance 
in machine use (%)
EDD performance in 
machine use (%) 
The best 
scheduling rule 
12 32 84 83 73 64 FCFS 
12 38 116 68 80 85 EDD 
9 24 81 70 89 81 SPT 
11 38 105 69 88 80 SPT 
9 30 102 67 79 83 EDD 
13 35 83 80 77 65 FCFS 
10 34 98 64 74 82 EDD 
12 37 92 81 89 70 SPT 
10 36 111 75 91 92 EDD 
11 28 71 77 83 66 SPT 
12 36 113 66 85 76 SPT 
9 28 93 65 75 82 EDD 
13 38 93 89 80 70 FCFS 
9 34 101 70 90 80 SPT 
12 36 94 78 83 66 SPT 
10 36 115 82 69 85 EDD 
11 27 66 75 84 66 SPT 
11 34 110 65 75 82 EDD 
11 30 80 78 85 67 SPT 
12 38 118 65 83 75 SPT 
8 25 93 65 84 74 SPT 
12 37 102 66 82 79 SPT 
9 32 109 69 83 84 EDD 
12 31 74 83 82 68 FCFS 
11 34 101 68 80 86 EDD 
8 31 101 71 89 82 SPT 
9 28 94 66 77 84 EDD 
8 33 101 64 82 74 SPT 
8 33 100 66 84 75 SPT 
9 34 100 65 84 74 SPT 
12 38 118 70 83 88 EDD 
10 34 99 66 77 84 EDD 
10 34 99 65 76 82 EDD 
10 37 115 68 82 84 EDD 
11 34 102 66 77 82 EDD 
12 31 83 77 85 67 SPT 
11 34 106 65 76 82 EDD 
11 34 108 66 77 84 EDD 
11 34 111 67 78 85 EDD 
 18 
 
Table 6. A training data set randomly chosen from Table 5 
Buffer 
size 
Arrival rate 
of parts 
AGV 
speed 
FCFS performance 
in machine use (%)
SPT performance in 
machine use (%) 
EDD performance 
in machine use (%) 
The best 
scheduling 
rule 
12 32 84 83 73 64 FCFS 
12 38 116 68 80 85 EDD 
9 24 81 70 89 81 SPT 
10 38 105 69 88 80 SPT 
9 30 102 67 79 83 EDD 
13 35 83 80 77 65 FCFS 
10 34 98 64 74 82 EDD 
12 37 92 81 89 70 SPT 
10 36 111 75 91 92 EDD 
12 28 71 77 83 66 SPT 
 
 
 
 
Figure 7. Membership functions of arrival rate of parts 
 
 
  
 
Figure 8. Comparisons of computational results 
 20 
provide a strategy in small data set learning for 
FMS scheduling knowledge. While only 
5training data sets are used, our testing accuracy 
increased to 70%, and 95% with 100 training 
data. These results indicate that our approach 
not only offers a degree of having high learning 
accuracy, but also is reliable, applicable, and 
certainly flexible in a very competitive business 
marketplace. 
 
Future studies might delineate more 
specifically matters such as determining 
membership function type and appropriate 
height for min and max in the triangular. These 
research subjects are believed to be critical for 
learning quality. Noting that the height of min 
and max in the triangular do have a direct 
relationship with the attribute domain range 
estimation for either under-estimating or 
over-estimating the ranges would lead to a poor 
learning. Thus, providing suitable methods or 
developing a solid theoretical basis for the 
determination of domain ranges is also an 
important work in future research. 
 
References 
 
[1] Cybenko, G. 1989. Approximation by 
super positions of a sigmoidal function. 
Mathematics of Control, Signals, and 
Systems 2 303-314. 
 
[2] Funahashi, K. 1989. On the approximate 
realization of continuous mappings by 
neural networks. Neural Networks 2 
183-192. 
 
[3] Hornik, K., Stinchcombe, M., White, H. 
1989. Multilayer feed forward networks 
are universal approximators. Neural 
Networks 2 359-366. 
 
[4] Wray, J., Green, G.G.R. 1995. Neural 
networks, approximation theory, and 
finite precision computation. Neural 
Networks 8 31-37. 
 
[5] Li, D.C., Chen, L.S., Lin, Y.S. 2003. 
Using functional virtual population as 
assistance to learn scheduling knowledge 
in dynamic manufacturing environments. 
International Journal of Production 
Research 41 4011-4024.  
 
[6] Li, D.C., Wu, C.S., Tong, K.Y. 1997. 
Using an unsupervised neural network 
and decision tree as knowledge 
acquisition tools for FMS scheduling. 
International Journal of Systems Science 
28 977-985.  
 
[7] Li, D.C., Han, K.L., Tong, K.Y. 1996. A 
strategy for evolution of algorithms to 
increase the computational effectiveness 
of NP-hard scheduling problems. 
European Journal of Operational 
Research 88 404-412.  
 
[8] Li, D.C., She, I.S. 1994. Using 
unsupervised learning technologies and 
