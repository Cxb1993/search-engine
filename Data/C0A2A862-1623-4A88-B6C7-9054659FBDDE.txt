2行政院國家科學委員會專題研究計畫成果報告
利用採取修正之 Bagging 和 Boosting 演算法的多分類器系統來改善遙測影像
分類的準確率
Improvement of remote sensing image classification accuracy by using a
multiple classifiers system with modified Bagging and Boosting algorithms
計畫編號：NSC 95-2221-E-239-034
執行期限：95 年 8 月 1 日至 96 年 7 月 31 日
主持人：曾裕強 國立聯合大學電子工程學系
共同主持人：陳錕山 國立中央大學太空及遙測研究中心
計畫參與人員：邱順興 國立聯合大學電子工程學系碩士班
陳靜儀 國立聯合大學電子工程學系碩士班
Abstract—In this paper, a multiple classifiers system
is utilized to improve the classification performance
by integrating the outputs of several classifiers. A
weighting policy is applied to fuse knowledge
acquired by classifiers to arrive at an overall decision
that is supposedly superior to that attainable by any
one of them acting alone. The most popular weighting
policies, Bagging and Boosting, are introduced. By
adopting the concept of confidence index, which
accounts for the ambiguities among classes, the
modified Bagging and Boosting weighted multiple
classifiers systems are proposed. The classification
performances of utilizing the original and modified
Bagging and Boosting weighted multiple classifiers
systems to the application of remote sensing image
classification are demonstrated and compared.
Experimental results show that the classification
accuracy is considerably improved by making use of
the multiple classifiers system. In addition, the
multiple classifiers systems of using modified
Bagging and Boosting algorithms are superior to those
of using original Bagging and Boosting algorithms.
Moreover, the effectiveness of using confidence index
is obviously, especially when the ambiguities among
classes are high.
Keywords:multiple classifiers system; confidence
inde
I. Introduction
A multiple classifiers system, which combining or
integrating the outputs of several classifiers has led to
improved performance [1]. Its aim is to determine an
effective combination method that makes use of the
benefits of each classifier but avoids the weaknesses.
Wolpert [2] introduced the stacked generalization
scheme for minimizing the classification error rate for
one or more classifiers. The outputs from classifiers
are combined in a weighted sum with weights that are
based on the individual performance of the classifiers.
The expectation of using the multiple classifiers
system is that the differently trained classifiers
converge to different local minima on the error
surface and overall performance is improved by
combining the outputs in some way. Consider first the
case of a single classifier that has been trained on a
given training data set. Let x denote an input random
vector of the classifier, D denote the corresponding
desired response, and F(x) denote the input-output
function realized by the classifier. The effectiveness
of F(x) as a predictor can be measured by the
mean-square error between F(x) and the desired
response D which is defined by
          222 xFExFEDxFEDxFE  (1)
The first term on the right hand side is the bias
squared of the average value of the approximating
function F(x). The second term on the right hand side
is the variance of the approximation function F(x).
The main goal of using the multiple classifiers system
is therefore determine an effective combination
method that reduces the bias and/or the variance of the
approximation function F(x). A simple ensemble
averaging combiner, where the outputs of different
classifiers are linearly combined, reduces the overall
error due to varying initial condition [3]. As the
individual classifiers are concerned, the bias is
reduced at the cost of variance. Subsequently,
however, the variance is reduced by ensemble
averaging (weighted sum) of the classifiers’ outputs, 
leaving the bias unchanged.
4lkk ywywc 1111  and kll ywywc 2222  (4)
The higher the value ck is the more confidence the
class k is predicted by classifier 1. Also, the higher the
value cl is the more confidence the class l is predicted
by classifier 2. Upon applying the confidence index,
the fused output of class k for the modified Bagging
and/or Boosting algorithms becomes
  ikk yki ilii ikik ywywZ argmax: (5)
The subscripts of the second term are defined as l =
argmaxl yjl, ij  and kl  . Finally, a
winner-takes-all approach was adopted to select a
proper class. This enabled us to use Kappa coefficient
for accuracy evaluation. This statistic has been shown
to be an efficient estimator of the classification
accuracy [6]. The kappa coefficient is indeed a
measure of how well the classification agrees with the
reference data.
III. EXPERIMENTAL RESULTS
In this section, a plantation area in Au-Ku on the
east coast of Taiwan was chosen as an investigated
test site. Test data was obtained during the Pacrim II
campaign flying on Taiwan areas [7]. Three spectral
bands (bands 3, 5, and 8 which are similar to the R, G,
and IR bands of SPOT) of the MASTER data were
chosen as the data source to be classified. The test site
mainly contains six ground cover types which are
sugar cane A, sugar cane B, bare soil, rice, grass, and
seawater. The total number of training and verification
pixels for each class was given in Table I.
TABLE I. DATA SET SUMMARY
Class
Label
Ground Cover
Types
Number of
Training Pixels
Number of
Verification Pixels
1 Sugar Cane A 1110 660
2 Sugar Cane B 288 126
3 Bare Soil 540 568
4 Rice 300 310
5 Grass 343 269
6 Sea Water 595 225
A dynamic learning neural network (DL) [8] was
adopted as the base classifier. The classification
results of using various approaches mentioned in the
previous section with this data set were given and
compared in the following. Being a comparing
reference, the classification performance of using a
single classifier only was computed at first. When
applied to the test data, DL was configured to have 3
input nodes (R, G, and IR), 6 output nodes, and 2
hidden layers with 40 hidden nodes each. The
classification matrix was given in Table II. It was seen
that the overall accuracy was 65.01% and the kappa
coefficient was only 0.55. The classification accuracy
was so low mainly because that some pixels of the
class 1 (sugar cane A) were misclassified as class 5
(grass), some pixels of the class 4 (rice) were
misclassified as class 1 (sugar cane A), and almost all
of the class 5 (grass) were misclassified as class 1
(sugar cane A).
TABLE II. CLASSIFICATION MATRIX (SINGLE CLASSIFIER)
class 1 2 3 4 5 6 Producer's
1 422 65 0 9 164 0 63.94
2 14 112 0 0 0 0 88.89
3 43 0 506 1 18 0 89.08
4 96 2 41 137 34 0 44.19
5 268 0 0 0 1 0 0.37
6 0 0 0 0 0 225 100.0
User's 50.06 62.57 92.5 93.2 0.46 100.0
overall accuracy = 65.01%; kappa coefficient = 0.55
To apply the multiple classifiers system, the
number of classifiers utilized is selected to be 3. All
the base classifiers (DL) are arranged to have identical
configuration, which are the same as what used for the
single classifier case. All the classifiers were trained
on the same data set, but were initialized differently.
Also, for both the original and modified Bagging
weighted multiple classifiers systems, the training set
for each classifier was collected by randomly and
uniformly resampling about 63.2% of the original data
set. Table III demonstrates the classification result of
using the original Bagging weighted multiple
classifiers system. Its overall accuracy is 84.20% and
its kappa coefficient is 0.8. Table IV displayed the
classification result of using the original Boosting
weighted multiple classifiers system. Its overall
accuracy was 83.92% and its kappa coefficient was
0.79. Great improvements in classification accuracies
were obtained of using both original Bagging and
Boosting algorithms. Almost all the class 1 (sugar
cane A) pixels that had been misclassified as class 5
(grass) were recovered. However, a large amount of
class 4 (rice) pixels which had been misclassified as
class 1 (sugar cane A), and class 5 (grass) pixels that
had been misclassified as class 1 (sugar cane A) were
still unable to be recovered. Even worse, the class 3
6[2] D. H. Wolper, “Stacked generalization,”Neural
Networks, vol. 5, pp. 241-259, 1992.
[3] U. Naftaly, N. Intrator, and D. Horn, “Optimal 
ensemble averaging of neural networks,” Network, 
vol. 8, pp. 283-296, 1997.
[4] L. Breiman, “Bagging Predictors”, Machine 
Learning, vol. 24, pp. 123-140, 1996.
[5] Y. Freund and R. E. Schapire, “A
decision-theoretic generalization of online
elearning and an application to boosting,” Journal 
of Computer and System Sciences, vol. 55, pp.
119-139, 1997.
[6] T. M. Lillesand and R. W. Kiefer, Remote Sensing
and Image Interpretation, NY: Wiley , 1993.
[7] Y. C. Tzeng and K. S. Chen, “A variance
reduction technique applied to multisource remote
sensing data classification,”Optics Engineering,
accepted, 2005.
[8] Y. C. Tzeng, K. S. Chen, W. L. Kao, and A. K.
Fung, “A dynamic learning neural network for
remote sensing applications,”IEEE Transaction
on Geoscience and Remote Sensing, vol. 32, pp.
1096-1102, 1994.
