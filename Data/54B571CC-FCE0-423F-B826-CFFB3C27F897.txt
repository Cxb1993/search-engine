 1
語意網路分類知識自動建構技術之研究(II) 
 
摘要 
「語意網路」的提出希望能夠為下一代的知識網頁架構提供一個解決方案，其主
要基本概念是建立在語意網路本身的領域本體知識上。然而，領域本體知識建構
是一件相當具專業與煩瑣的工作，因此如何快速的學習與建構有效的本體知識是
一個相當重要的問題。在我們前一年度的計畫中已經成功的發展出一個以模糊概
念階層為架構的階層概念知識自動學習方法，並設計不同的實驗來證實其效能。
本年度計畫主要運用前一年度計畫所發展的模糊概念階層並透過對概念階層知
識樹的產生與知識概念的標定，用以自動建構不同領域本體知識。其步驟為由具
有語意網路資料格式的網頁中，經由語意標記與全文處理，擷取適當的關鍵字
詞，用以代表分類階層概念，並利用所自動產生概念階層知識，來判別網頁內容。
實驗規劃以 ImageCLEF2004 資料集，應用於問答資訊擷取的效能來驗證所提出
的領域本知識自動建構模型的效能。實驗結果發現，領域本體知識的確對語意網
頁的判別有重大影響，採取不合適的本體知識將會有更差的結果，結合適當的本
體知識將有助於資訊擷取的正確性，而以我們所提出的自動模糊概念階層建構方
法均有正面的表現。 
 
關鍵詞：語意網路、本體知識、模糊概念階層、資訊擷取、資料探勘 
 
 
Automated Knowledge Acquisition and Construction for 
Classification on Semantic Web (II) 
 
ABSTRACT 
The Semantic Web and the techniques of web intelligence is proposed to try to solve 
the problem of building knowledge structures on web contents. However, creating 
ontology is a professional and tedious work. It is important and interesting work for 
researchers to learn and construct ontology efficiently and effectively. In the previous 
project of last year, we had proposed automatic clustering algorithm for learning 
hierarchical concept knowledge from databases based on fuzzy concept hierarchy. The 
experiments also showed that the proposed approach is effective. In this project, the 
automatic clustering algorithm is applied to generate distinct ontology automatically 
by mapping appropriate keywords to the generated concept hierarchies. The procedure 
starts from the textual process of semantic web page including the meaning detection 
of tags. After the keywords extraction, the fuzzy concept hierarchy is generated and 
ontology is built. The data sets of ImageCLEF2004 and their Q/A benchmarks are 
used to verify the effetiveness of constructed ontology. The experiments show that 
ontology will influence the retrieval results seriously. An appropriate ontology will 
improve the effectiveness; on the contrary, a bad ontology may degrade the results. 
However, the ontology constructed by the proposed automatic construction method 
generally demonstrates good results. 
 
Keywords: Semantic web, ontology, fuzzy concept hierarchy, information retrieval, 
data mining. 
 3
 
圖 1：領域本體知識為基礎的資訊擷取系統架構 
 
有關系統(圖 1)中所使用的領域本體知識包括有以下幾類： 
1. 一般性的語意本體知識：在此部份主要使用的是 WordNet  語意詞彙本體知識與其所定
義的詞彙相似評估方法[27]。 
2. 領域知識(Domain knowledge)：此一部份的領域本體知識主要是由語意網頁本身的 XML
或 OWL 標記結構所分析而得。 
3. 自動建構的領域知識：此一部份主要配合第二部份的領域知識，並由語意網頁中的無結
構資料萃取相關語意關鍵字，之後應用前一年度計畫中所發展的自動模糊概念階層群聚
學習演算法所產出來的領域本體知識。 
 
而處理有關語意網頁中半結構化資料的流程敘述如下：  
一、 全文處理與字詞擷取(Textual preprocess & term extraction)： 
語意網頁資料可視為半結構化資料，這些資料將會針對非結構化的資料部份進行前置
處理，流程如圖 2 範例所示，詳細步驟說明如下： 
1. 詞性標記(POS, Part of speech)：為了萃取出文件中文字跟文字之間的語意關係，針
對句子進行語句分割與關鍵字詞性標記是不可缺少的重要步驟，我們利用
Tsuruoka[36]所提出的 SS Tagger 對英文文件進行詞性標記。 
2. 贅字過濾(Stop list)：大量文件的處理過程中，有許多的字不但無法提供資訊，反而
會因此而造成文件分析的阻礙。為了與 WordNet 整合，我們參考 WordNet 所提供
的贅字清單，將不必要的字，如：on, with, … 等過濾刪除。 
3. 詞形還原(Stemming)：為了避免不同詞形的關鍵字在文件分析的過程中遭到誤判，
因此我們在這個步驟將清除大小寫(First 與 first)、不同時態動詞(Site 與 sited)、不
同數量級名詞(Hut 與 huts)、不同比較級名詞(Big 與 bigger)之間的差異。我們使用
前一步驟所得結果，根據 Porter[29]所提出的 Porter stemmer[30]進行處理。 
二、 字詞相似度比對與擴展(Term similarity & extension)： 
經由前置處理取得初步的關鍵字詞後，依照圖 3 的流程，對非結構文字字詞結合語意
本體知識，半結構化部分與領域本體知識做文件語意概念的映對與計算，相關處理方
法如下： 
 Domain 
Knowledge 01
WordNet 
Ontology Base
Relevant 
Documents 
Document 
Analysis 
Term Similarity 
Processing 
Hierarchical 
Clustering 
Algorithm 
Ontology Construction Method
Query 
Textual 
Preprocessing 
 Imported 
Ontology 
Domain 
Knowledge 02
Documents 
Ontology 
Selection and 
Mapping 
Similarity 
Measure 
Matching Method 
Query processing 
Document processing 
 5
 
圖 3：字詞相似度比對與擴展 
 
圖 4：文件分析和概念程度計算 
 
三、 文件分析與語意篩選(Document analysis & semantic filter)： 
最後的步驟則是分析文件，如圖 4，對每一關鍵字詞所產生的可能語意篩選出適當的
領域本體知識中的概念來相對應，用以產生概念程度(Concept factor)分析，概念程度
頻數 cf(Dn, cl)為文件 Dn 在各領域本體知識語意概念 cl出現的強度，若無法歸類的關鍵
字詞本身則成為一個可能的新概念。概念程度矩陣 T 如下表： 
 
 
 
 
 
 
 
 
 c1 c2 … cl 
D1 cf(D1, c1) cf(D1, c2) … cf(D1, cl) 
D2 cf(D2, c1) cf(D2, c2) … cf(D2, cl) 
… … … … … 
Dn cf(Dn, c1) cf(Dn, c2) … cf(Dn, cl) 
步驟 1. 計算關鍵字詞間的相似度。 
步驟 2. 挑選出重要的文件特徵。 
- 非結構化資訊：將關鍵字詞以模糊分群演算法分成相似概念群 
- 半結構化資訊：利用領域本體知識挑選出數個代表性的概念特徵 
Term/pattern 
Similarity 
Processing 
Textual preprocess 
+ 
Term extraction 
Semi-structured 
Data 
 
… 
WordNet 
 
… 
Domain knowledge 
步驟 1. 計算關鍵語意概念程度。 
步驟 2. 對可能代表該文件可能的語意概念進行篩選。 
Document analysis 
+ 
 Semantic Filtering 
Term/pattern 
Similarity 
Processing 
Textual preprocess 
+ 
Term extraction 
Semi-structured 
Data
 
… 
 
… 
Word Net 
Domain knowledge 
 7
 
表 1：The list of designed experiments. 
 WordNet Fuzzy 
Concept 
Hierarchy 
Domain 
Knowledge 
TF-IDF Query 
Processing 
TF-IDF_01    Y  
TF-IDF_02 Y   Y  
Onto_01  Y D+O+L  Y 
Onto_02  Y D+O+L+C  Y 
Onto_M1  Y M  Y 
Onto_M2  Y M+ D+O+L  Y 
Onto_M3  Y M+ D+O+L+C  Y 
C – Category.     D – Date.     L – Location.     O – Originator.      M – Unstructured data. 
 
實驗的進行以 ImageCELF2004[6]所公佈的標準規範(Benchmark)來進行，如表 2，其問答標
準查詢共分五類，25 個問題，答案則由所有文件集中取出前 1000 相似文件來做評比，並
計算其 MAP(Mean average precision)來做為效能評比依據。如表 3 所示，前二行 TF-IDF_01
與 TF-IDF_02 為使用傳統 TF-IDF 模型與加上使用 WordNet 處理的擷取結果，其他則為我
們使用所提出的方法結合不同領域本體知識所得到的結果。 
 以平均準確率來看，我們發現 Onto_M2 與 Onto_M3 有相當不錯的結果，以 MAP 的值
來看則是 Onto_02 為最高，因此不同的 Ontology 明顯的會導致不一樣的擷取結果，另外最
明顯的是 TF-IDF_02 加上 WordNet 後的結果明顯變差(圖 7)，這與之前的認知上認為加上語
意解釋與相似度後的結果會便好的預測有相當的落差，同時這也顯示不適當的 Ontology 是
會嚴重影響資料結取的結果。至於何謂適當的 Ontology 則與其問答查詢問題的分類有關，
不同類別的問題對不同 Ontology 有著不同程度的影響。 
 
表 2：問答標準規範 
 
 
 9
表 4：於 25 topics 實驗之 MAP 結果值 
  TF-IDF_01 TF-IDF_02 Onto_01 Onto_02 Onto_M1 Onto_M2 Onto_M3
Overall 52.33% 47.41% 54.91% 55.84% 52.91% 55.04% 55.04% 
1 82.94% 85.09% 82.94% 79.19% 70.17% 70.17% 70.17% 
2 95.48% 8.59% 97.20% 97.20% 95.48% 97.20% 97.20% 
3 64.05% 76.84% 64.05% 64.05% 64.05% 64.05% 64.05% 
4 53.22% 53.22% 53.22% 56.70% 56.70% 56.70% 56.70% 
5 23.49% 24.69% 45.53% 60.65% 81.40% 81.40% 81.40% 
6 94.40% 94.40% 91.82% 91.75% 91.52% 91.52% 91.52% 
7 39.88% 39.88% 39.88% 56.46% 56.46% 56.46% 56.46% 
8 77.13% 77.13% 77.13% 77.13% 57.28% 77.13% 77.13% 
9 3.16% 0.00% 3.16% 14.76% 14.76% 14.76% 14.76% 
10 45.16% 42.60% 45.16% 37.70% 47.70% 47.70% 47.70% 
11 32.87% 32.87% 64.09% 64.09% 64.09% 64.09% 64.09% 
12 45.24% 32.88% 48.02% 45.04% 45.24% 48.02% 48.02% 
13 74.60% 32.76% 69.36% 75.79% 64.95% 67.55% 67.55% 
14 84.80% 98.09% 84.80% 100.00% 88.13% 100.00% 100.00% 
15 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 
16 64.76% 60.89% 64.76% 56.66% 56.66% 56.66% 56.66% 
17 38.02% 38.69% 38.02% 55.14% 38.38% 38.38% 38.38% 
18 2.54% 1.00% 15.45% 17.90% 3.51% 17.90% 17.90% 
19 68.62% 68.62% 68.62% 32.21% 32.21% 32.21% 32.21% 
20 57.19% 55.68% 57.19% 55.53% 55.53% 55.53% 55.53% 
21 20.50% 20.50% 20.50% 23.31% 28.86% 28.86% 28.86% 
22 13.01% 12.99% 13.01% 1.91% 1.91% 1.91% 1.91% 
23 41.51% 41.51% 41.51% 44.69% 44.69% 44.69% 44.69% 
24 34.21% 25.96% 34.21% 30.37% 10.54% 10.54% 10.54% 
25 51.58% 60.46% 53.16% 57.69% 52.48% 52.48% 52.48% 
 
參考文獻 
[1] T. Apted, J. Kay, “Automatic construction of learning ontologies,” in Proceedings of the 
International Conference on Computers in Education, Vol.2, Dec. 3-6, 2002, pp. 
1563-1564. 
[2] J. L. R. Arjona, M. T. Corchuelo, “A knowledge extraction process specification for today's 
non-semantic Web,” in Proceedings of IEEE International Conference on Web Intelligence, 
Oct. 13-17, 2003, pp. 61-67. 
[3] R. Basili, M. Vindigni, F. M. Zanzotto, “Integrating ontological and linguistic knowledge 
for conceptual information extraction,” in Proceedings of the IEEE International 
Conference on Web Intelligence, Oct. 13-17, 2003. 
[4] G. Bisson, C. Nedellec, D. Canamero, “Designing clustering methods for ontology 
building-The Mo’K workbench,” in Proceedings of the ECAI Ontology Learning Workshop, 
2000. 
[5] P. Clerkin, P. Cunningham, C. Hayes, “Ontology Discovery for the Semantic Web Using 
Hierarchical Clustering,” in Proceedings of the Semantic Web Mining Workshop at 
ECML/PKDD, Sept., 2001 
[6] P. Clough, M. Sanderson, H. Müller, The CLEF Cross Language Image Retrieval Track. In: 
Working Notes of the CLEF 2004 Workshop, 2004, http://ir.shef.ac.uk/imageclef/. 
[7] M. Crampes, S. Ranwez, “Ontology-supported and ontology-driven conceptual navigation 
on the World Wide Web,” in Proceedings of the 11th ACM Conferences on Hypertext and 
Hypermedia, May, 2000, pp. 191-199.  
[8] Y. Ding, S. Foo, “Ontology research and development. Part 1: A review of ontology 
generation,” Journal of Information Science, Vol. 28, No. 2, Feb., 2002, pp. 123-136. 
[9] Y. Ding, S. Foo, “Ontology research and development. Part 2: A review of ontology 
generation,” Journal of Information Science, Vol. 28, No. 5, Feb., 2002, pp. 375-388. 
 11
[32] M. Shamsfard, A. A. Barforoush, “Learning ontologies from natural language texts,” 
International Journal of Human-Computer Studies, Vol. 60, Issue 1, Jan., 2004, pp. 17-63.  
[33] N. Silva, J. Rocha, “Semantic Web complex ontology mapping,” in Proceedings of the 
IEEE International Conference on Web Intelligence, Oct. 13-17, 2003, pp. 82-88. 
[34] H. Stuckenschmidt, F. V. Harmelen, “Ontology-based metadata generation from 
semi-structured information,” in Proceedings of the International Conference on 
Knowledge Capture, Oct. 2001. 
[35] Y. A. Tijerino, D. W. Embley, D. W. Lonsdale, G. Nagy, “Ontology generation from tables,” 
in Proceedings of the 4th International Conference on Web Information Systems 
Engineering, Dec. 10-12, 2003, pp. 242-249. 
[36] Y. Tsuruoka and J. Tsujii, Bidirectional Inference with the Easiest-First Strategy for 
Tagging Sequence Data, in Proceedings of HLT/EMNLP, 2005, pp. 467-474. 
[37] P. Velardi, P. Fabriani, M. Missikoff, “Using text processing techniques to automatically 
enrich a domain ontology,” in Proceedings of the International Conference on Formal 
Ontology in Information Systems, Oct., 2001. 
[38] L. A. Zadeh, “Web intelligence and world knowledge - the concept of Web IQ (WIQ),” in 
Proceedings of the IEEE Annual Meeting of the Fuzzy Information, Vol. 1, June 27-30, 
2004, pp. 1-3. 
[39] L. Zuo, S. Poslad, “Supporting multi-lateral sematic information viewpoints when 
accessing heterogeneous distributed environmental information,” in Proceedings of the 1st 
European Workshop on Multi-Agent Systems, St Catherine’s College Oxford, Dec. 18-19, 
2003 
 
計畫成果自評： 
 
本研究計畫延續前一年度計畫成果，延續發展具有語意階層式概念的本體知識，主要目標
於適當處理無結構化語意資料及半結構化資料的部分。以及如何以人工輔助方式來配合建
構。對整體計畫而言，成果如下： 
1. 探討領域階層概念(Hierarchical concept)為主的知識表示法(Knowledge representation)。 
2. 語意網路中半結構(Semi-structured)網頁文件的處理，與設計如何由具有半結構標記網頁
文件中自動學習與構建領域階層概念知識的方法。 
3. 新舊領域概念知識的映對(Mapping)、驗證(Validation)與領域知識的評估(Evaluation)。 
本計畫中所提出的方法所產生的領域本體知識，經由構建應用於資料擷取問題上的評
估，可以驗證其有效性，達成計畫中所要求的成果。本方法所構建的結果為一分類型的知
識本體架構，相關技術未來除資料擷取外，應可以擴展到不同的應用上。 
目前本計畫學術上的具體成果如下：包括一篇碩士論文之部份內容，另已發表有三篇
Conference papers。其中的第 2 篇亦被邀請至期刊論文中發表，目前正改寫中，另二篇亦正
預計將進一步整理完整版本後，投稿國際期刊。相關技術我們也正應用到其他的應用中，
如網頁知識蒐尋與文件分類等問題，相信後續會有更多的成果呈現。發表著作整理如下： 
 
1.  C. H. Hu, Ontology-based Information Retrieval Using Fuzzy Concept hierarchies, Master 
Thesis, I-Shou University, Kaohsiung, Taiwan. 
2.  B. C. Chien, C. H. Hu and M. Y. Ju, Learning Fuzzy Concept Hierarchy and Measurement with 
Node Labeling, Lecture Notes in Computer Science, LNCS No. 4743, Springer-Verlag, 2007, pp. 
163-172. (EI) 
3.  C. H. Hu, M. Y. Ju and B. C. Chien, Learning Fuzzy Taxonomy for Ontology Construction 
from Semi-Structured Data, in Proceedings of 2006 Multimedia and Networking Systems 
Conference, Kaohsiung, Taiwan, Dec. 16, 2006. 
4.  B. C. Chien, C. H. Hu and M. Y. Ju, Intelligent Information Retrieval Applying Automatic 
Constructed Fuzzy Ontology, in Proceedings of The 6th international Conference on 
Machine Learning and Cybernetics, August, 19-22, 2007, Hong-Kong. (EI) 
the root is a universal concept, whereas the most specific concepts correspond to the 
specific values of attributes in the database. Each internal node in the tree of concept 
hierarchy represents a concept which helps to express knowledge and data relation-
ships. Formally, suppose that a taxonomy H is defined on a set of domains Di, …, Dk, 
we have Hl : {Di × … × Dk} ⇒ Hl-1 ⇒ … ⇒ H0 , where Hl denotes the set of concepts 
at the primitive level, Hl-1 denotes the concepts at one level higher than those at Hl, 
and H0 represents the most general concept on the top level denoted as “ANY”.  
However, concept description is generally vague for human knowledge. Tradi-
tional crisp description of concept usually cannot represent human knowledge com-
pletely and practically. In this paper, we study the characteristics of fuzzy concepts 
including their fuzzy description and fuzzy relationship. At first, fuzzy concept hier-
archy is defined in Section 3. Then, we propose a unsupervised learning method to 
generate fuzzy concept in Section 4 and design a measurement procedure to evaluate 
the effectiveness of fuzzy concept in Section 5. Section 6 describes the experimental 
results and makes comparisons. Conclusion and future work are given in Section 7. 
2   Related Works 
The methods of concept generation are generally divides into three classes. The 
first approach is to specify concept hierarchies by users, experts or data analysts ac-
cording to a partial or total ordering semantics of attributes explicitly at the schema 
level. The second approach is that given a set of attributes, the system then tries to 
generate the partial ordering of attributes automatically so as to construct a meaning-
ful concept hierarchy. The third approach is to define a portion of taxonomy by clus-
tering explicit data in databases. However, it is essentially unrealistic to define an 
entire concept structure by explicit values enumeration manually in a large database. 
Thus, most of methods of the third class focus on automatic generation of concepts by 
grouping data using clustering techniques. 
The related researches for automatic hierarchical concept learning can be divided 
into two main methodologies. The first method is hierarchical clustering that is the 
most popular and efficient method to build a concept hierarchy. The related works 
include COBWEB [4], ClassIT [5], GCF [11] and the Liao’s [2] methods. The other 
method is Formal Concept Analysis (FCA) method [3]. The main objective of FCA is 
to visualize the data in the form of concept lattice to make them more transparent and 
more easily discussed.  
3   Definitions of the Fuzzy Concept Hierarchy 
The hierarchical concept proposed here is described as a fuzzy hierarchical struc-
ture with a subordinate relation used to describe vague information. The definition of 
a fuzzy concept hierarchy is described as follows. 
 
 Figure 1. Representation of a hierarchical fuzzy concept. 
4   Unsupervised Learning of Hierarchical Fuzzy Concept 
For evaluating the importance of attributes on the given dataset X, we design a 
function of fuzzy entropy based on fuzzy set theory and fuzzy entropy measure. Let X 
= {x1, x2, … , xn} be a non-empty finite set of objects with a non-empty finite set of 
attribute A= {A1, … , Am}. µi denotes a fuzzy set of Ai containing a set of m linguistic 
terms {ti1, ti2, … , tim}. We define the fuzzy entropy E(A) as follows. 
 
Definition 4: Let x ∈ X, the fuzzy entropy of information based on a set of attributes 
A is 
( )
ijx AA t
ij
ij
v
nx
n
v
AE
i iij
∑∑∑
Χ∈ ∈ ∈
××=
 
2log)(
µ
µ , 
where )(xijµ  is the membership value of the object x on the j-th linguistic term of i-
th attribute tij and vij is equal to the summation of )(xijµ  for all x ∈ X, that is 
 
( )∑
Χ∈
=
x
ijij xv µ . 
 
As we defined in Definition 4, the fuzzy entropy has monotonic property de-
scribed as follows. 
 
Theorem 1: Let B, B′ ⊆ A. If B′⊆B, then E(B) ≤ E(B′). 
Proof.  The values of fuzzy entropy 
ANY
C l-1i  … C l-1 k2 
Level 0 
(H0) 
Level 1 
 (H1) 
Level l-1 
(Hl-1) 
…
... 
C l1 
Level l  
(Hl) C 
l
2 C l3 
C11  C12
C l-11  
C1k
…………….
C l-12 
C l4
z outlook: {sunny, overcast, rainy}, 
z temperature: numeric 
z humidity: numeric 
z windy: {true, false} 
Table 1. The weather dataset 
data outlook temperature humidity windy 
x1 sunny 85 85 false 
x2 sunny 80 90 true 
x3 overcast 83 86 false 
x4 rainy 70 96 false 
x5 rainy 68 80 false 
x6 rainy 65 70 true 
x7 overcast 64 65 true 
x8 sunny 72 95 false 
x9 sunny 69 70 false 
x10 rainy 75 80 false 
x11 sunny 75 70 true 
x12 overcast 72 90 true 
x13 overcast 81 75 false 
x14 rainy 71 91 true 
Table 2. The fuzzy set of weather dataset 
data outlook temperature humidity windy 
x1 0/O+0/R+1/S 1/H+0/M+0/L 0.65/H+0.35/L 0/T+1/F 
x2 0/O+0/R+1/S 0.5/H+0.5/M+0/L 0.81/H+0.19/L 1/T+0/F 
x3 1/O+0/R+0/S 0.8/H+0.2/M+0/L 0.68/H+0.32/L 0/T+1/F 
x4 0/O+1/R+0/S 0/H+0.55/M+0.45/L 1/H+0/L 0/T+1/F 
x5 0/O+1/R+0/S 0/H+0.36/M+0.64/L 0.48/H+0.52/L 0/T+1/F 
x6 0/O+1/R+0/S 0/H+0.09/M+0.91/L 0.16/H+0.84/L 1/T+0/F 
x7 1/O+0/R+0/S 0/H+0/M+1/L 0/H+1/L 1/T+0/F 
x8 0/O+0/R+1/S 0/H+0.73/M+0.27/L 0.97/H+0.03/L 0/T+1/F 
x9 0/O+0/R+1/S 0/H+0.45/M+0.55/L 0.16/H+0.84/L 0/T+1/F 
x10 0/O+1/R+0/S 0/H+1/M+0/L 0.48/H+0.52/L 0/T+1/F 
x11 0/O+0/R+1/S 0/H+1/M+0/L 0.16/H+0.84/L 1/T+0/F 
x12 1/O+0/R+0/S 0/H+0.73/M+0.27/L 0.81/H+0.19/L 1/T+0/F 
x13 1/O+0/R+0/S 0.6/H+0.4/M+0/L 0.32/H+0.68/L 0/T+1/F 
x14 0/O+1/R+0/S 0/H+0.64/M+0.36/L 0.84/H+0.16/L 1/T+0/F 
 
Input: The dataset as in  Table 1. 
Output: A fuzzy hierarchy H of concepts. 
Step 1: Transform the objects x ∈ X with quantitative attributes into linguistic terms 
using suitable fuzzy membership functions. Assume that the result is shown as 
Table 2. 
Step 2: Initialize the values of m′= |A| = 4. 
Step 3:  Generate the lowest level of concept hierarchy Hm with the set of attributes 
A. Let B= {outlook, temperature, humidity, windy}, H4= 414
4
2
4
1 ... CCC ∪∪∪ , 
such that  
1.  Preprocess: At first, the input dataset will be partitioned into test data and train-
ing data. Fuzzy process will generate membership functions from training data 
and transform the training and testing data into fuzzy sets.  
2.  Concept Learning: The proposed hierarchical clustering algorithm then cluster 
strong concepts from training data and construct a meaningful fuzzy concept hi-
erarchy. 
3.  Evaluation: A node labeling algorithm used for labeling proper class to each 
node in constructed hierarchy. Then, a similarity measure is given to decide the 
best node in fuzzy concept hierarchy for each test datum. 
In unsupervised learning, there is no class information for each cluster. Therefore, 
class information is required for concepts (or clusters) in constructed hierarchy to 
compute predictive accuracy. The labeling algorithm based on fuzzy confidence is 
described as follows. 
Assume database X = {x1, x2, …, xn}, a class vd and attribute set B, the confidence 
value lkC  with the k-th concept of level l is calculated as the following formula:  
( )
( )∑
∑
∈ ∈
∈ ∈
⎟⎠
⎞⎜⎝
⎛
⎟⎠
⎞⎜⎝
⎛ ⎟⎠
⎞⎜⎝
⎛
=
Xx
r
RCr
Xx
vr
RCr
l
k
v
x
xx
CFC
il
ki
dil
ki
d
)(min
)(,)(minmin
)(
µ
µµ
, 
where )( lkr Ciµ  is the membership value of the object x on the linguistic term ir . ir  
is the representative linguistic term of i-th attribute in the representative concept of 
l
kC , 
l
kRC . 
Assume that the hierarchical concepts H is built from training data of database X 
with attribute set A. A contains a decision attribute Ad ∈ A. Let the set of possible 
values in Ad be V = {v1, …, vi}. The node labeling algorithm for H is described in five 
main steps, as follows. 
 
Node Labeling
Preprocess 
Concept Learning 
Evaluation 
Test  
Data
Training
Data
Hierarchical Fuzzy Concept 
Learning Algorithm 
Fuzzy Concept
Test and evaluate
Figure 2. The architecture of evaluation process. 
),( max
lCxS = ( )),(max lk
C
CxS
l
k C∈
. 
If ),( max
lCxS > m, assign Vd to be class of Clmax; otherwise the algorithm stops. 
Step 4: If l ≠ |B| go to Step 4; otherwise the algorithm stops. 
Step 5: Set l = l +1, C’= {Clk’ | Clk’ ∈ Hl and Clk’ ⊆ Clmax }.  
Step 6: If C’ =∅, the algorithm stops; otherwise go to Step 2. 
 
Algorithm 4: Classification by similarity measure and fuzzy confidence 
Input: An unknown class data x and a hierarchy of node labeled concepts H’. 
Output: The assigned class Vd for x 
Step 1: Initially, l = 1, C = {Clk | Clk ∈ Hl}, and m=0. 
Step 2:  Compute ),( lksc CxS  for all Clk ∈ Hl. 
Step 3: Select Clmax such that  
),( max
l
ksc CxS = ( )),(max lksc
C
CxS
l
k C∈
. 
If ),( max
l
ksc CxS > m, assign Vd to be class of C
l
max. 
Step 4:  If l ≠ |B|, then go to Step 5; otherwise the algorithm stops. 
Step 5: Set l = l +1, C’= {Clk’ | Clk’ ∈ Hl and Clk’ ⊆ Clmax }.  
Step 6: If C’ =∅, the algorithm stops; otherwise go to Step 2. 
Table 3. The selected datasets 
Datasets Number of attributes Number of instances Number of Classes 
BCW   9 683 2 
CLEVE 13 296 2 
CRX 15 653 2 
GLASS   9 214 7 
HORSE 12 326 2 
PIMA   8 768 2 
6   Experiments and Evaluation Results 
For illustrating the performance of the proposed method, several datasets are se-
lected. The test datasets including the Breast Cancer dataset of Wisconsin, the Credit 
Approval dataset, the Glass Identification dataset, the Horse Colic dataset and the 
PIMA Indian Diabetes dataset  that are selected from UCI Machine Learning Reposi-
tory [1]. All selected datasets are summarized in Table 3  
In order to evaluate the efficiency of the learning and prediction processes, deci-
sion attribute is used only for testing, but hidden during training. The experimental 
results were obtained by performing 10-fold cross validation. From the experimental 
results, we observed the two scoring methods achieve good results in accuracy of 
different dataset. The threshold of fuzzy confidence α also affects the accuracy 
greatly. As Table 4 illustrates, we evaluate the constructed hierarchy from α is 0.5 to 
7   Conclusion 
The concept hierarchy is an explicit representation of knowledge and can be widely 
used in many applications. Fuzzy concept description is more suitable than crisp 
concept description in representing human knowledge. In this paper, we present the 
fuzzy concept hierarchy and propose an algorithm to generate fuzzy concept hierar-
chy based on fuzzy entropy. The algorithm can construct a meaningful fuzzy concept 
hierarchy automatically and represent fuzzy concepts with representative concepts 
and different grades of subsuming relationships among subordinate fuzzy concepts. 
Our experiments also show that the fuzzy concept can be labeled well and depict 
important concept effectively. This work can be extended to apply to find useful 
concept and knowledge structures in semantic web. The further work on related ap-
plication is to help constructing ontology from database automatically. 
 
References 
1. Blake, C., Keogh E., Merz, C. J.: UCI repository of machine learning database. 
http://www.ics.uci.edu/~mlearn/MLRepository.html, Irvine, University of California, De-
partment of Information and Computer Science (1998) 
2. Chien, B. C., Liao, S. Y.: Mining Categorical Concept Hierarchies in Large Databases.  
Proceedings of 7th World Multiconference on Systemics, Cybernetics and Informatics, Vol. 
2 (2003) 244-249 
3. Burmeister, P.: Formal Concept analysis with ConImp: Introduction to Basic Features. TU 
Darmstdt, Germany, Technical Report, http://www.mathematik.tudarm-
stadt.de/˜burmeister (1996) 
4. Fisher, D. H.: Knowledge Acquisition via Incremental Conceptual Clustering. Machine 
Learning  2(2) (1987) 139-172 
5. Gennari, J. H., Langley, P., Fisher, D.: Models of incremental concept formation. Artificial 
Intelligence 40(1) (1989) 11-61 
6. Han, J., Kamber, M.: Data Mining: Concept and Techniques. Morgan Kaufmann publishers, 
(2001) 
7. Jiang, J., Conrath, D.: Semantic similarity based on corpus statistics and lexical taxonomy.  
Proceedings of International Conference on Research in Computational Linguistics (1997) 
8. Klir, G. J., Yuan, B.: Fuzzy sets and fuzzy logic: theory and applications. Prentice Hall, 
(1995) 
9. Mangasarian, L., Wolberg, W. H.: Cancer Diagnosis via Linear Programming. SIAM News 
23(5) (1990) 1-18 
10. Quinlan, J. R.: Induction of Decision Trees. Machine Learning 1(1) (1986) 81-106 
11. Talavera, L., Bejar, J.: Generality-Based Conceptual Clustering with Probabilistic Con-
cepts. IEEE Transactions on Pattern Analysis and Machine Intelligence 23(2) (2001) 196-
206 
12. Wu, Z., Palmer, M.: Verb Semantics and Lexical Selection. Proceedings of the 32nd An-
nual Meeting of the Association for Computational Linguistics (1994) 
 
most popular and efficient method to build a concept 
taxonomy. The related works including COBWEB [4], 
ClassIT [5], GCF [7] and the Liao’s approach [1]. 
The other approach is Formal Concept Analysis 
(FCA) method [3]. The main objective of FCA is to 
visualize the data in the form of concept lattice to 
make them more transparent and more easily 
discussed. 
 
3  Fuzzy Concept Hierarchy 
The ontology model proposed here is denoted as a 
fuzzy taxonomy structure with a subordinate relation 
to describe vague information. The definition of the 
is described as follows. 
 
Definition 1: A fuzzy concept is defined as C = 
(µ1, …, µi, … , µm) that µi denotes a fuzzy set defined 
on domain Di with m linguistic terms {ti1, …, tij, …, 
tim}. Moreover, µij is the membership values in µi 
corresponding to the fuzzy term tij. The fuzzy set µi 
can be represented as  
µi =∑ ijij t/µ = µi1/ti1 + µi2/ti2 + …. + µim/tim . 
 
The sets of fuzzy terms with strong membership 
grades in the fuzzy sets can be used to represent the 
fuzzy concept of C, called the representative concept, 
RC. 
 
Definition 2: The representative concept RC is 
defined as, ) ,  ... ,,...,( 1 mj rrrRC = , where jr  is the 
linguistic term with the maximum membership value 
)}(max{ Cijµ  of the fuzzy set iµ  on the fuzzy 
concept C. 
 
A subordinate relationship is defined as R(Ck, Ck’) 
to describe the degree of subsuming the concept Ck 
under the concept of Ck’ . 
 
Definition 3: Let H be a set of fuzzy concepts 
defined on the domain Di. For Ck and Ck’ in H, R(Ck, 
Ck’) is a fuzzy relation that Ck is subordinate to Ck’, if 
∀tj ∈ µi, and )()( 'kijkij CC µµ ≤ . The degree of 
subsuming the concept Ck under the concept of Ck’ 
can be defined as ( )( )
B
CC
CCR HA t
kijkij
kk
i iij
∑∏
∈ ∈
−−
= µ
µµ )()(1
),(
'
' , 
such that )( kij Cµ is the membership value of the 
fuzzy concept Ck on the j-th linguistic term of iA  
( i-th attribute ), ijt . B denotes the attribute set. |B| is 
the attribute number of B. 
 
According to the definition of fuzzy partial 
ordering [7], a fuzzy binary relation is a fuzzy partial 
ordering iff it is reflexive, anti-symmetric, and 
transitive. R(Ck, Ck) is reflexive iff R(Ck,Ck)=1, for all 
Ck∈H. R(Ck,Ck’) is anti-symmetric when R(Ck1, Ck2) > 
0 and R(Ck2, Ck1) > 0, implies that Ck1 = Ck2 for Ck1, 
Ck2 in H. R(Ck, Ck’) is transitive (or more specifically, 
max-min transitive) if the following condition is 
satisfied for each pair of < Ck1, Ck3> in H: 
][min  max),( 322131
2
) , C), R( C, CR( CCCR kkkkHCkk k ∈≥ . 
The set of concepts conforming to Definition 1, 
Definition 2, and Definition 3 builds fuzzy taxonomy 
if fuzzy partial ordering exists for all concepts. A 
fuzzy taxonomy H is defined on a set of domains 
{D1, …, Dm} and the set of concepts at the primitive 
level Hl={ lC1 ,
lC2 , … ,
l
nC } with m attributes in 
domains {D1 × … × Dm}. Hl-1 = { 11 −lC , 12−lC , … , 1'−lnC } 
denotes the set of concepts at one level higher than 
those at Hl. H0 represents the most general concept on 
the top level denoted as “ANY”.  
 
4  Framework of Ontology Construction 
 
4.1  General Semantic Ontology  
In order to extract the semantic concept from data 
collection, the knowledge of words and the semantic 
relation are necessary. WordNet [9] is an electronic 
lexical knowledge base. English nouns, verbs, 
adjectives, and adverbs are organized into synonym 
sets. Each synonym set represents one underlying 
lexicalized concept, as shown in Table 1. Various 
semantic relations including synonymy, antonymy, 
hyponymy, meronymy, troponomy and entailment 
link synonym sets. 
WordNet is generally used in computational 
linguistics, text analysis, and many related areas. The 
measurement of word-similarity is another important 
application of WordNet. Many methods were 
proposed for measuring semantic similarity between 
words for text analysis [7][13]. 
 
Semantic 
Relation 
Syntactic 
Category Examples 
Synonymy 
(similar) 
N. , Adj. , 
Adv. , V. 
Sad = unhappy 
Rapidly = Speedily 
Antonymy 
(opposite) 
Adj. , Adv. , 
(V. , N.) 
Wet, Dry 
Friendly, Unfriendly
Hyponymy  
(subordinate) N. 
Maple, Tree 
Tree, Plant 
Meronymy 
(part) N. 
Gin, Martini 
Ship, Fleet 
Troponomy 
(manner) V. 
March, Walk 
Whisper, Talk 
Entailment V. Drive, Ride Divorce, Marry 
Table 1: The semantic relations of WordNet. 
 
4.2  Textual Preprocess 
In textual preprocessing, the main function can be 
divided into three main steps: part-of-speech (P.O.S) 
tagging, stop words pruning, and stemming. 
In P.O.S tagging, a part-of-speech tagger is 
finite set of objects with a non-empty finite set of 
attribute A= {A1, … , Am}. µi denotes a fuzzy set of Ai 
containing a set of m linguistic terms {ti1, ti2, … , tim}. 
We define the fuzzy entropy E(A) as follows: 
Definition 4: Let x ∈ X, the fuzzy entropy of 
information based on a set of attributes A is 
( )
ijx AA t
ij
ij
v
nx
n
v
AE
i iij
∑∑∑
Χ∈ ∈ ∈
××=
 
2log)(
µ
µ , 
where )(xijµ  is the membership value of the object 
x on the j-th linguistic term of i-th attribute tij and vij 
is equal to the summation of )(xijµ  for all x ∈ X, that 
is 
( )∑
Χ∈
=
x
ijij xv µ . 
As we defined in Definition 4, the fuzzy entropy 
has monotonic property described as follows. 
 
Theorem 1: Let B, B′ ⊆ A. If B′⊆B, then E(B) ≤ 
E(B′). 
Proof. The values of fuzzy entropy 
( )
ijx BA t
ij
ij
v
nx
n
v
BE
i iij
∑∑∑
Χ∈ ∈ ∈
××=
µ
µ 2log)( ,  
( )
ijx BA t
ij
ij
v
nx
n
v
BE
i iij
∑∑∑
Χ∈ ∈ ∈
××=
'
2log)'(
µ
µ .  
Since B' ⊆ B, trivially, we have )()'( BEBE < .  
 
The concept is more general while the value of 
fuzzy entropy is smaller. Theorem 1 states the 
property that the fuzzy entropy decreases 
monotonously in accord with the generalization in 
higher levels of a hierarchy. 
The algorithm for clustering meaningful concepts 
with respect to a given database is described in eight 
main steps, as follows: 
 
Algorithm 1: The Taxonomy Clustering Algorithm 
Input: A database X with the set of attributes A. 
Output: A fuzzy hierarchy H of concepts. 
Step 1: Transform the objects x ∈ X with quantitative 
attributes into linguistic terms using the 
defined fuzzy sets iµ  and fuzzy membership 
functions. 
Step 2: Initialize the value m′ = |A|. 
Step 3: Generate the lowest level of hierarchy Hm with 
the set of attributes A. That is, 
Hm= } , ... , ,{ 21
m
n
mm CCC , where mkC = (µ1, 
µ2, … , µm) and µi is the i-th fuzzy set of 
attribute Ai. The representative concepts of Hm 
are ) , ... ,,...,( 1
m
km
m
kj
m
k
m
k rrrRC =  for 1 ≤ k ≤ n. 
The set of reduced attributes B = A, initially. 
Step 4: Let |B| be the number of attributes in the set of 
reduced attributes B, we set m′ = |B| - 1. Find 
Pm′(B), which is the set of all possible subsets 
with m′ attributes in the set of reduced 
attributes B. 
Step 5: Compute the fuzzy entropy E(Bj) for all Bj ∈ 
Pm′(B). 
Step 6: Generate a higher level of hierarchy Hm′ by the 
following sub-steps: 
6.1) Find B′ = Bj′ where j′= { }{ })(minarg jBE
j
, 
for Bj ∈ Pm′(B); then generate new 
representative concepts 'mkRC  in Hm′. 
The 'mkRC  is obtained by merging the 
same representative concepts in the lower 
level Hm′+1. 
6.2) The corresponding fuzzy concept 'mkC = (µ1, 
µ2, … µi, … , µm) is generated from the 
representative concepts 'mkRC , where µi 
is the minimum of membership grade for 
the merged fuzzy concepts with the same 
representative concepts in Step 6.1. 
6.3) The remainder unchanged fuzzy concept 
in Hm′+1 is added into Hm′, directly. 
Step 7: Calculate the degree of subordinate 
relationships R( '1
m
kC ,
1'
2
+m
kC ) between each pair 
of concepts in Hm′ and Hm′+1. 
Step 8: Let the set of reduced attributes B = B′. If B ≠ 
∅, then go to Step 4; otherwise, output all 
levels of the fuzzy concept hierarchy H0, …, 
Hm. 
 
The proposed algorithm constructs a fuzzy 
concept hierarchy, the relationships between too 
concepts are called subordinate relationship. The 
representative concepts of lower levels are selected to 
form new concepts in higher levels. In order words, 
the strongest degree of subordinate relationship. 
The time complexity of the proposed algorithm is 
dependent on the number of attributes m in each 
concept level. The number of attributes is decreased 
after a new level is generated. Hence, the overall time 
complexity of the algorithm is bounded by O (m2n), 
where n is the size of the database. 
We give an example to explain the above 
algorithm more clearly. The example uses the 
weather dataset with 14 objects as shown in Table 4.  
 
data outlook temperature humidity windy
x1 sunny 85 85 false
x2 sunny 80 90 true 
x3 overcast 83 86 false
x4 rainy 70 96 false
x5 rainy 68 80 false
x6 rainy 65 70 true 
x7 overcast 64 65 true 
x8 sunny 72 95 false
x9 sunny 69 70 false
x10 rainy 75 80 false
x11 sunny 75 70 true 
x12 overcast 72 90 true 
x13 overcast 81 75 false
x14 rainy 71 91 true 
Table 4: The weather dataset. 
 i =1 i =2 i =3 i =4 i =5 
3
iRC  (*, H, H, F) (*, M, H, T) (*, M, H, F) (*, L, L, T) (*, L, L, F) 
3
iC  
* 
(0.8/H+0/M+0/L) 
(0.65/H+0.32/L) 
(0/T+1/F) 
* 
(0/H+0.51/M+0/L) 
(0.8/H+0.16/L) 
(1/T+0/F) 
* 
(0/H+0.54/M+0.27/L)
(0.96/H+0/L) 
(0/T+1/F) 
* 
(0/H+0/M+0.9/L) 
(0/H+0.83/L) 
(1/T+0/F) 
* 
(0/H+0.36/M+0.54/L)
(0.16/H+0.51/L) 
(0/T+1/F) 
 i =1 i =2 i =3   
2
iRC  (*, H, *, F) (*, M, *, T) (*, M, *, F)   
2
iC  
* 
(0.6/H+0/M+0/L) 
* 
(0/T+1/F) 
* 
(0/H+0.51/M+0/L) 
* 
(1/T+0/F) 
* 
(0/H+0.54/M+0/L) 
* 
(0/T+1/F) 
  
 i =1 i =2    
1
iRC  (*, L, *, *) (*, M, *, *)    
1
iC  
* 
(0/H+0/M+0.54/L) 
* 
* 
* 
(0/H+0.51/M+0/L) 
* 
* 
   
 i =1     
0
iRC  (*, *, *, *)     
0
iC  
* 
* 
* 
* 
    
Table 6: The generated fuzzy concepts of ontology. 
 
 
Fig. 3: The final fuzzy concept hierarchy. 
 
5.2 Evaluation of Fuzzy Taxonomy 
The induced fuzzy taxonomy reflects the structure 
of the given database should be evaluated. However, 
in unsupervised learning there are no target outputs 
associated with the inputs. A widely used method for 
evaluating unsupervised learning system is to 
compute predictive accuracy as that is done for 
supervised classifiers. The resulting accuracy serves 
as a measure of how well the system has discovered. 
As Fig. 4 illustrates, an evaluation is designed to test 
the constructed taxonomy.  
1.  Preprocess: At first, the input dataset will be 
partitioned into test data and training data. 
Fuzzy process will generate membership 
functions from training data and transform the 
training and testing data into fuzzy sets. The 
membership function generation is modified to 
satisfy unsupervised learning. 
2.  Concept Learning: The proposed hierarchical 
clustering algorithm then cluster strong concepts 
from training data and construct a meaningful 
fuzzy concept hierarchy. 
3.  Evaluation: A node labeling algorithm used for 
labeling proper class to each node in constructed 
H0 
H1 
H2 
H3 
H4 
C01
C42 C41C41C43 C
4
1 C41 C41 C44 C48 C41 C46 C47 C45 C
4
9 
C21 C22 C23
C11 C12 
C33 C34 C35 C31 C32 
 Fig. 5: The result of node labeling algorithm. 
 
In order to find a suitable location for an 
unknown object in the built hierarchy, a similarity 
measure to evaluate the similarity of two concepts 
kC and 'kC  is defined as follows, ( )
||
)()(1 
),(
'
' A
CC
CCf AA t
kijkij
kk
i iij
∑ ∏
∈ ∈
−−
= µ
µµ
 
where )( kij Cµ  is the membership value of the 
fuzzy concept kC  on the j-th linguistic term of i-th 
attribute, tij. |A| is the number of attributes in A. 
 The proposed similarity measure is used to 
predict suitable position for unknown object in 
constructed hierarchy after node labeling. The 
similarity between test data and concepts is measured 
by scanning from the highest level to the lowest level. 
Furthermore, two score methods are used to decide 
the best position of unknown object. The first method 
is scored by the similarity between unknown object 
and each concept,  
),(),( lk
l
k CxfCxS =  
where x denote an unknown object, Clk denotes the 
k-th concept of level l. The second method considers 
more about if the concept is good enough for 
representing all child concepts. Therefore, we 
consider both similarity and fuzzy confidence of each 
concept Clk as the scoring method: 
))((max),(),( lk
v
BVv
l
k
l
ksc CFCCxfCxS i
i ∈
×=  
The most suitable concept having the highest 
score based on these two kinds of score methods is 
chose. The detailed algorithm for evaluating fuzzy 
taxonomy by classification is shown as follows. 
 
Algorithm 3: Classification by similarity measure 
Input: An unknown class data x and taxonomy of 
labeled concepts H’. 
Output: The assigned class Vd for x 
Step 1: Initially, l = 1, C= {Clk | Clk ∈ Hl}, and m=0. 
Step 2: Compute ),( lkCxS  for all C
l
k ∈ Hl. 
Step 3: Select Clmax that ),( max
lCxS = ( )),(max lk
C
CxS
l
k C∈
. 
If ),( max
lCxS > m, assign Vd to be class of 
Clmax; otherwise the algorithm stops. 
Step 4: If l ≠ |B| go to Step 4; otherwise the algorithm 
stops. 
Step 5: Set l = l +1, C’= {Clk’ | Clk’ ∈ Hl and Clk’ ⊆ 
Clmax }.  
Step 6: If C’ =∅, the algorithm stops; otherwise go to 
Step 2. 
 
Example 4: 
Input: An unknown class data x and the hierarchy of 
labeled concepts H’ in Fig. 5. 
Output: The assigned class Vd for x that 
x = {(1/O+0/R+0/S),(0.2/H+0.8/M+0/L), 
(0.55/H + 0.45/L), (0/T+1/F), yes}. 
Step 1: Initially, l = 1, C= {C11, C12}, and m=0. 
Step 2:  Compute the score by similarity measure: 
 77.0),( 11 =CxS  ,  89.0),( 12 =CxS . 
Step 3: Select the maximum, C12. Because 
89.0),( 12 =CxS > m = 0, assign x to be yes 
and m = 0.89. 
Step 4:  l > 3, go to Step 4. 
Step 5:  Set l = l +1=2, C’= {C22, C23}.  
Step 6:  C’ ≠ ∅, go to Step 2. 
Result: The class of x is “yes”. 
 
Algorithm 4: Classification by similarity measure 
and fuzzy confidence 
Input: An unknown class data x and a hierarchy of 
node labeled concepts H’. 
Output: The assigned class Vd for x 
Step 1: Initially, l = 1, C= {Clk | Clk ∈ Hl}, and m=0. 
H0 
H1 
H2 
H3 
H4 
C01
C42 C412 C41C43 C
4
1 C414 C411 C44 C48 C410 C46 C47 C45 C
4
9 
C21 C22 C23
C11 C12 
C33 C34 C35 C31 C32 
yes (0.65) yes (0.7) 
yes (0.58) yes (0.81) 
yes (0.55) yes (0.97) yes (0.61) 
yes (0.5) 
no (0.5) 
yes (0.74) yes (0.58) 
http://www.mathematik.tudarm-stadt.de/˜burme
ister. 
[4] D. H. Fisher, “Knowledge Acquisition via 
Incremental Conceptual Clustering,” Machine 
Learning, Vol. 2, Issue 2, 1987, pp. 139-172. 
[5] J. H. Gennari, P. Langley, D. Fisher, “Models of 
incremental concept formation,” Artificial 
Intelligence, Vol. 40, No. 1, 1989, pp. 11-61. 
[6] J. Han and M. Kamber, Data Mining: Concept 
and Techniques, Morgan Kaufmann publishers, 
2001. 
[7] J. Jiang and D. Conrath, “Semantic similarity 
based on corpus statistics and lexical 
taxonomy,” in Proceedings of International 
Conference on Research in Computational 
Linguistics, Taiwan, 1997. 
[8] G. J. Klir, and B. Yuan, Fuzzy sets and fuzzy 
logic: theory and applications, Prentice Hall, 
1995. 
[9] L. Mangasarian and W. H. Wolberg, “Cancer 
Diagnosis via Linear Programming,” SIAM 
News, Vol. 23, No. 5, 1990, pp. 1-18. 
[10] G. Miller, “WordNet: An on-line lexical 
database," International Journal of 
Lexicography, Vol. 3 No. 4, 1990, pp. 235-312. 
[11] J. R. Quinlan, “Induction of Decision Trees,” 
Machine Learning, Vol. 1, 1986, pp. 81-106. 
[12] L. Talavera and J. Bejar “Generality-Based 
Conceptual Clustering with Probabilistic 
Concepts,” IEEE Transactions on Pattern 
Analysis and Machine Intelligence, Vol. 23, No. 
2, 2001, pp. 196-206. 
[13] Z. Wu and M. Palmer, “Verb Semantics and 
Lexical Selection,” in Proceedings of the 32nd 
Annual Meeting of the Association for 
Computational Linguistics, Las Cruces, New 
Mexico, 1994. 
 
  
 
The related research on ontology-based information 
retrieval system can be distinguished as two aspects. The 
first aspect is to enhance an information retrieval system 
performance by using the manually constructed ontology. 
Many scholars have developed information retrieval system 
used a domain knowledge representation schema in form of 
ontology [3][14]. These systems use queries to retrieve 
semantic concept from the existing ontology or manually 
construed ontology. The relevant concept can be found by 
mapping keywords in the query to the domain ontology. 
Rodger et al. used the manually constructed domain 
ontology in a web query system [10]. The queries are 
extended by mapping terms into the concepts of ontology.  
The second aspect of work is to construct ontology 
automatically through domain corpus. The constructed 
ontology is used to boost the correctness and coverage of 
the system. Wu and Hsu designed a tool to construct 
domain ontology from Chinese corpus [16]. The domain 
ontology is represented by the domain keywords and the 
relationships. They also defined the extracting rules for 
Chinese phrases. They can collect domain keywords and 
find the relationship among domain keywords by these 
rules. Kang proposed a system using the new indexing 
formalism that considers concepts in a document [5]. These 
concepts are extracted by clusters of terms that are 
semantically related. They used these concepts to extract 
semantically exact indices. These indices can be indicated 
to represent the semantic content of a document. 
3. Ontology-based information retrieval model 
As illustrated in Figure 1, the proposed ontology-based 
information retrieval model mainly contains four modules: 
1. Textual Preprocessing: Textual preprocessing is a basic 
step for extracting terms and features. The main 
function of this module is to filter the important 
keywords and eliminate the stop words. 
2. Ontology construction methods: This module tries to 
construct fuzzy taxonomy for representing ontology 
from documents automatically. In order to generate 
ontology efficiently, the constructing process is divided 
into tree steps: term similarity processing, document 
analysis and clustering algorithm. 
3. Matching Method: Matching method is the main 
retrieval mechanism. The relevant documents generally 
are retrieved and ranked using similarity matching. 
4. Ontology Base: Several types of ontology are adopted in 
the proposed model. They are Wordnet, users’ domain 
knowledge constructed manually and automatically 
generated fuzzy taxonomy.  
Generally, documents are processed in two phases for 
an information retrieval system: document processing and 
query processing. In the phase of document processing, 
documents are processed by textual preprocessing to obtain 
important terms and features for representing the 
documents. The terms then are used to construct fuzzy 
taxonomies through the ontology construction methods. 
 Domain 
Knowledge 01
WordNet 
Ontology Base 
Relevant 
Documents
Document 
Analysis 
Term Similarity 
Processing 
Automatic 
Ontology 
Construction 
Ontology Construction Method 
Query 
Textual 
Preprocessing 
 Imported ontology Domain 
Knowledge 02
Documents 
Ontology 
Selection and 
Mapping 
Similarity 
Measure 
Matching Method 
Query processing Document processing
Figure 1. The architecture of ontology-based information. 
i l
  
of the selected ontology H. The sub-ontology mapping 
from the starting concept ljC  is used to build a new 
ontology of query. The following mapping algorithm maps 
the query keyword into the most similar concept from the 
selected ontology: 
Algorithm: Ontology mapping 
Input: )..., ,,,( 321 mkkkkq =r  and a selected ontology H. 
Output: An ontology of query Hq. 
Step 1: Set i = 1. 
Step 2:Set l = 1, MR = 0, where MR is the maximum 
relationships. 
Step 3: Compare the term ki with all l
l
j HC ∈ , find the 
best node with highest relationship ),( lji CkR . If 
),( lji CkRMR <  then ),( lji CkRMR =  and 
l
jCMC = . 
Step 4: l = l + 1 and if HHl ∈  then go to Step 3. 
Step 5: Add the sub-ontology start from MC into the Hq. 
Set i = i + 1 and if ki in q
r
 then got to Step 3. 
Step 6: Output an ontology Hq of query. 
The second step of similarity measure retrieves and 
ranks the relevant documents from the document database. 
At first, the ontology of query selected form the first step  
is used to adjust the weights of documents. The formula of 
computing adjusted weights for Hq is illustrated as follows 
∑ ∑
∈ ∈
×⎟⎟⎠
⎞
⎜⎜⎝
⎛=
id qqdk
di
Hk
dqi wkkRd v
v
,),(max' , 
where wi,d is the weight of document di presented in term kd. 
kq is the terms of Hq. Finally, the similarity measure by the 
following function: 
⎟⎟⎠
⎞
⎜⎜⎝
⎛ ×= ∑
∈Ss
rii
r
sqdCosqdSim ),'(max),'( v
vvv , 
where sr is the weight of selected ontology. ),'( qdCos i
vv  is 
the cosine similarity. 
For example, a query “Fishing vessels in Northern 
Ireland” can be represented as {“fish”, “vessels”, “in”, 
“Northern Ireland”}. The term “northern ireland” is 
mapped into the concept “n ireland” of the ontology 
H“Location” and s”Location”=1. The ontology of query Hq is 
mapped, as Figure 2 illustrated. 
Then, the mapped ontology of query is used in 
similarity measure. If the subordinated relationship R(“sail”, 
“northern ireland”) is 0.3 and R(“sail”, “vessel”) is 1. There 
is a document had the weight 4.81 in the term “sail”. For 
this document, the weight can be considered as 1.44 when 
the concept of query “northern Ireland” is involved. If the 
concept “vessel” is involved for querying, the weight of 
“sail” is 4.81. We choose the maximum weighting of terms 
in same document. Therefore, if a query concerned “vessel” 
and “northern ireland”, the weighting of “sail” should be 
4.81. This modification is then used in the computation of 
similarity by cosine similarity. 
 
 
5. The performance evaluation 
The evaluation of an information retrieval system 
includes document collection, query and relevant document 
of query. We adopted the “Eurovision St. Andrews 
photographic Collections” as the data collection from 
ImageCLEF [2]. The collection consists of 28,133 images, 
all of which have associated text caption. Totally 44,085 
words in the collections and 10,974 words are in WordNet. 
The longest caption contains 316 words and the average is 
48 words. All captions described by British English 
included colloquial expression and historical term. 81% of 
captions completely include seven attributes and 19% of 
captions include null or incomplete values. The years of 
documents centralize in 1930s. Each document has at least 
one and at most nine categories. The average number of 
category is nine. 
The evaluation computed the scores of queries across 
totally all 25 topics. These 25 topics were generated by the 
experts for ImageCLEF. These experts decided on general 
topic areas of St. Andrew collection, and refine them to 
create a representative topic set to test the capabilities. 
Broad categories were obtained from log file analysis, a 
discussion with experts. These topics include broad 
concepts, narrow concept, proper names, compound words, 
abbreviations, morphological variants and idioms. The 
retrieved documents will be identified and marked as 
relevant or not based on the relevant documents (qrels). To 
enable comparison, the UMASS and Lemur versions of the 
standard TREC_eval tool were used to compute the mean 
Query 
“fish” “vessel” “northern ireland”
“co 
down” 
“co 
antrim”
“co 
tyrone”
“boat” “sail”
1
0.3 0.2 
Figure 2. The example fuzzy ontology mapping 
  
Table 1: The comparison of experiments on average precision and MAP of top 1000 results. 
Exp.  
Recall TF-IDF_01 TF-IDF_02 Onto_01 Onto_02 Onto_M1 Onto_M2 Onto_M3
0.0 84.24% 79.68% 85.22% 81.08% 83.88% 83.92% 83.92% 
0.1 79.78% 76.38% 80.58% 76.34% 72.70% 73.91% 73.91% 
0.2 66.98% 61.60% 70.27% 69.40% 67.31% 68.53% 68.53% 
0.3 62.86% 57.47% 64.93% 62.93% 64.30% 64.99% 64.99% 
0.4 58.70% 52.35% 61.53% 59.18% 60.59% 61.98% 61.98% 
0.5 57.38% 51.48% 60.08% 57.71% 58.85% 60.06% 60.06% 
0.6 52.10% 45.39% 54.94% 54.43% 52.66% 55.87% 55.87% 
0.7 47.10% 41.15% 49.83% 50.78% 48.75% 52.23% 52.23% 
0.8 44.55% 38.03% 47.38% 48.64% 45.65% 48.97% 48.97% 
0.9 35.40% 30.37% 38.55% 38.84% 37.20% 41.15% 41.15% 
1.0 20.18% 18.91% 26.25% 24.51% 23.33% 28.25% 28.25% 
Avg. Precision 55.39% 50.25% 58.14% 56.71% 55.93% 58.17% 58.17% 
MAP 52.33% 47.41% 54.91% 55.84% 52.91% 55.04% 55.04% 
 
References 
[1] B. C. Chien, C. H. Hu and S. J. Hsu, “Generating 
Hierarchical Fuzzy Concepts from Large Databases,” 
in Proceedings of IEEE International Conference on 
Systems, Man & Cybernetics, The Hague, Netherland, 
Oct. 10-13, 2004, pp. 3128-3133. 
[2] Cross Language Evaluation Forum, available at 
http://ir.shef.ac.uk/imageclef2004/ 
[3] S. Dumais. “Improving the retrieval of information 
from external sources,” Behavior Research Methods, 
Instruments, and Computers, Vol. 23, No. 2, 1991, pp. 
229-236. 
[4] J. Jiang and D. Conrath, “Semantic similarity based on 
corpus statistics and lexical taxonomy,” in 
Proceedings of International Conference on Research 
in Computational Linguistics, Taiwan, 1997. 
[5] B. Y. Kang, D. W. Kim and S. J. Lee, “Exploiting 
Concept Clusters for Content-based information 
retrieval,” Information Sciences - Informatics and 
Computer Science: An International Journal, Vol. 170, 
2005, pp. 443-462. 
[6] G. Miller, “WordNet: An on-line lexical database,” 
International Journal of Lexicography, Vol. 3 No. 4, 
1990, pp. 235-312. 
[7] R. Neches, et al., “Enabling technology for knowledge 
sharing,” AI Magazine, Vol. 12, No. 3, 1991, pp. 
36-56. 
[8] M. F. Porter, The Porter Stemming Algorithm, 
http://www.tartarus.org/~martin/PorterStemmer, 2006. 
[9] T. T. Quan, S. C. Hui, and T. H. Cao, “FOGA: A 
Fuzzy Ontology Generation Framework for Scholarly 
Semantic Web,” in Proceedings of the 2004 
Knowledge Discovery and Ontologies, Workshop at 
ECML/PKDD 2004, Pisa, Italy, September 2004. 
[10] H. L. Roger, C. E. H. Chua, V. C. Storey, “A smart 
web query method for semantic retrieval of web data,” 
Data and Knowledge Engineering, Vol. 38, No. 1, 
2001, pp. 63-84. 
[11] G. Salton, Introduction to Modern Information 
Retrieval, McGraw-Hill, 1983. 
[12] G. Salton, C. Buckley, “Term-weighting approaches in 
automatic text retrieval,” Information Processing and 
Management: an International Journal, Vol. 24, No.5, 
1988, pp. 513-523. 
[13] Tsujii laboratory, SS Tagger a part-of-speech tagger 
for English, http://www-tsujii.is.s.u-tokyo.ac.jp/ 
tsuruoka/postagger/, University of Tokyo, Department 
of Computer Science, 2005. 
[14] P. Varga, T. Mészáros, Cs. Dezsényi, T. P. 
Dobrowiecki, “An Ontology-based Information 
Retrieval System,” in the proceeding of the 16th 
International Conference on Industrial & Engineering 
Applications of Artificial Intelligence and Expert 
Systems, Loughborough, UK, June 23-26, 2003. 
[15] Z. Wu and M. Palmer, “Verb Semantics and Lexical 
Selection,” in Proceedings of the 32nd Annual 
Meeting of the Association for Computational 
Linguistics, Las Cruces, New Mexico, 1994. 
[16] S. H. Wu, W. L. Hsu, “SOAT: A Semi-Automatic 
Domain Ontology Acquisition Tool From Chinese 
Corpus,” in Proceedings of the 19th International 
Conference on Computational Linguistics, Taipei, 
Taiwan, 2002, pp. 1313-1317. 
的涵蓋範圍包括模糊系統(Fuzzy systems)、類神經網路(Neural network)、演化式計算
(Evolutionary computation)、機器人(Robotics)、人工智慧(Artificial intelligence)等方面的研
究主題。國際進階智慧型系統學術會議(ISIS)是近年來由日本與韓國合辦的一個相當成功
的學術研討會，參與的人數、主題與發表論文數量，都有逐年增加的趨勢，相關領域的學
者近年來的投入相當踴躍，值得繼續觀察其影響力。 
 
參加會議經過： 
本次會議地點位於日本東京東京工業大學(Tokyo Institute of Technology)的大岡山校
區(Campus)，交通尚稱便利，9 月 20 日會議是由下午開始安排人員註冊與歡迎酒會，因此
行程安排直接於 9 月 20 日與同行的賴智錦教授及高雄大學林文揚教授，由高雄清晨搭機
出發前往東京，於 20 日中午左右抵達東京。一行人先搭車至東京下榻飯店放好行李後，
直接搭地鐵前往位於大岡山的東京工業大學大岡山校區參加歡迎會，東工大大岡山校區即
位於大岡山地鐵站出口，交通相當方便，歡迎會由東工大 K. Hirota 教授主持，大會主席
Toshiaki Murofushi 首先致簡單歡迎辭，之後便讓與會學者自由認識與聯誼，我們也趁這個
機會與來自韓國、德國與澳洲學者們相互認識。 
會議正式於 21 日早上揭幕，21 日至 23 日早上 9:00 至 10:00 均有一場邀請演講(Plenary 
talk)，分別由 Prof. Gyei-Karp Park, Prof. Hiroshi Deguchi, Dr. Ah-Hwee Tan 等學者擔任，
相關演講主題如下： 
 
9/21 Expansion on Application Fields of Fuzzy Theory: Intelligent Systems in Marine 
Industry,  
Prof. Gyei-Karp Park, National Maritime University, Korea 
9/22 Agent Based Social Simulation for Social and Organizational Architecture Design by 
SOARS 
Prof. Hiroshi Deguchi, Tokyo Institute of Technology, Japan 
9/23 Advances in Adaptive Resonance Theory 
Dr. Ah-Hwee Tan, Nanyang Technological University, Singapore 
 
由於所發表的論文相當多，其它論文發表分別同時在 10 個場地舉行。個人於本次會
議中所發表論文被安排於第一天下午的 15:00，主題為是 Ontology technology and its 
applications，發表者共有五位，主持人是林文揚教授。個人所發表的論文為利用模糊熵之
相似度評估法由不完整項目資料中學習如何分類的處理方法 (A Similarity Measure 
