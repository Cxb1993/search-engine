The operating model relating the process quality (output
variables) to controllable (manipulated) variables is con-
structed from past operational data using single-target
and multi-target Information Network (IN) algorithms
for induction of oblivious decision-trees. Peng et al. [6]
proposed a semiconductor manufacturing data mining
framework in which Self Organizing Map (SOM) and
Decision Tree were employed to extract empirical rules
for controlling WIP levels given various product mixes
and production conditions.
The decision tree is one of the key data mining tech-
niques that it has been used to yield improvement of dif-
ferent manufacturing processes. In the previous litera-
tures about decision tree, a regression tree is usually used
with the lot-based data as the data source and the lot yield
is considered as the target variable [1,2,7]. However, in
recent years, the production strategy for small-quantity
and diverse products is prevalent in the optoelectronics
industry. In such production environment, yield varia-
tion is changeable and the amount of data is limited; con-
sequently, analyzing with regression tree, a smaller lot
quantity may lead to the deviation of the overall analysis
result due to the difference in lot quantity. To avoid this
problem, we transform each lot-based record by its quan-
tity into glass-based records and use classification tree to
create a yield predication model to improve the yield of
glass sputtering process. Moreover, the problem of using
decision tree to build a yield improvement model is that
the irrelevant values problem may occur in the tree. In
yield improvement model such problem will cause that
critical factors to the glass sputtering process may not be
effectively identified. Therefore, for the performance is-
sue and practical issue, we have to remove irrelevant
conditions from the rules.
Until now, a number of different algorithms have
been proposed to solve the irrelevant values problem
[811]. For example, Fayyad has proposed two algo-
rithms. GID3 and GID3*, to solve the irrelevant values
problem of the decision tree constructed using ID3. How-
ever, the problem of these algorithms is that some bran-
ches in the GID3 and GID3* tree may be longer than that
in ID3 tree. In other words, comprehensibility of the tree
will be detracted. As indicated by [12], another way to
simplify decision tree structure is to translate the tree
structure into a set of rules. Accordingly, Chiang pro-
vided a method to solve the irrelevant values problem of
the decision tree without the problem of GID3 and GID3*
tree [13]. It eliminates irrelevant values in the process of
converting the decision tree to a set of rules by the infor-
mation on the tree with respect to discrete values. How-
ever, the data of glass sputtering process always contains
quantitative data; therefore, we have to modify Chiang’s
previous work such that the revised algorithm can deal
not only discrete data but also quantitative data. The mo-
dified algorithm is presented in section 3.
In this paper, when decision tree is used to build a
model to improve the glass sputtering process, we have
defined a threshold value in advance to find out the posi-
tive and negative rules. When the yield of a rule is higher
than the threshold value, it is set as a positive rule; on the
contrary, if it is lower than the threshold value, it is set as
a negative rule. In this study, we use the positive rules to
adjust the process parameters into optimal range to im-
prove the yield of sputtering process, and the negative
rules to monitor the manufacturing process to avoid to
causing any yield losses. Since we do not integrate thre-
shold value into the decision tree, the original results of
the decision tree cannot be used directly to find out the
positive rules and negative rules. Therefore, we have to
convert the original classification result of each leaf
node to a new class by the predefined threshold value be-
fore of using our modified algorithm to remove irrele-
vant conditions from the rules. In this paper, we will use
a simple example to show this converting process and
demonstrate that our approach can get more concise and
comprehensible rules than the rules which are obtained
from the original tree directly.
In the rest of the article, background knowledge
about glass sputtering processes and the irrelevant va-
lues problem of decision trees are introduced in Section
2. The algorithm for removing irrelevant values of the
decision tree is given in Section 3. The experimental ex-
ample is discussed in Section 4. The conclusion remarks
and future studies are stated in Section 5.
2. Background Knowledge
2.1 Introduction of Glass Sputtering Processes
Tainan Science-based Industrial Park is the main
cluster of Taiwan optoelectronic companies. Despite its
proactive engagement in creating a cluster of optoelec-
tronic plants, it also expects to induce a cluster effect, by
2 Ding-An Chiang et al.
article, we use CART tree to analyze the collected data.
The common decision tree algorithms are compared in Ta-
ble 1. Although different tree-induction algorithms have
been proposed by different authors, without losing gener-
ality, we only consider ID3-like algorithm in this paper.
A decision tree is built up by selecting the best test
attribute as the root of the decision tree. Then, the same
procedure is operated on each branch to induce the re-
maining levels of the decision tree until all examples in a
leaf belong to the same class. However, when an attri-
bute is selected for branching out a node, both sub trees
create a branch for some values of that appearing in the
training data. Since some values of that attribute may not
be relevant to the classification, the resultant rules of the
decision tree may have irrelevant conditions, which de-
mands irrelevant information to be supplied. Actually,
according to the semantics of the irrelevant value, when
a branch value is an irrelevant value of a rule; this value
can be deleted or replaced by any value from the same
domain value without affecting the correctness of the
rule. Moreover, for the decision tree, not all attributes
will be the nodes of some branches in the decision tree;
therefore, when the corresponding values of attributes
are missing in a branch, we say these attributes are also
irrelevant attributes with respect to the branch. We ex-
plain this observation by the following definition.
Let A = {A1, ..., An} be a set of attributes, C = {C1,
..., Cs} be a set of classes and the branch Br of the deci-
sion tree can be represented as the form Br[A1]  ... 
Br[An]  Ck, where Br[Ai] is a set of branch values out
of an attribute Ai in the branch Br, i = 1 … n and 1  k  s.
Definition 1. Let Br[A1]  ...  Br[Aj-1]  Ck be a
branch of the decision tree. Then, the rules with respect
to attributes A1, ..., An implied by Br are:
{Br[A1]  ...  Br[Aj-1] aj ... ans  Ck | ajr, ..., ans
 domain(Aj, ..., An)},
where domain(Aj, ..., An) = domain(Aj)  …  do-
main (An).
Example 1. Let us consider the decision tree in Figure.
4. In this example, we want to identify irrelevant values
of branches Br1 and Br2. By definition 1, we know that
two rules, a3  b1  c1 and a3  b2  c1, are implied
by a3  c1. Since rules a1  b1  c1, a2  b1  c1 and
a3  b1  c1 exist, these three rules can be generalized
by deleting irrelevant values without affecting its accu-
racy; leaving the more appealing rule: b1  c1. That is,
values a1 and a2 are irrelevant values in the rule a1  b1
 c1 and a2  b1  c1, respectively. After eliminating
irrelevant conditions, the resultant rule, b1  c1, is more
concise and comprehensible than the original rules, a1
 b1  c1 and a2  b1  c1. Moreover, since some
rules may be duplicated after eliminating irrelevant va-
lues, the number of the resultant rules may be less than
that of leaves of the tree. The rule without irrelevant
conditions is useful in many applications. For example,
the patient can examine item B first to make sure whe-
ther or not this patient need to take item A. This process
can reduce some burdens (expense, convenience or harm-
ful) to the patient.
3. An Algorithm to Identify the Irrelevant
Values
We introduce some important definitions and theo-
4 Ding-An Chiang et al.
Table 1. Comparison of decision tree algorithms
Data attribute Classification/Regression tree Partitioning rule Pruning rule
ID3 Discrete data Classification tree Information Gain No Pruning
C4.5 Continuous data Regression tree Information Gain Predicted error rate
CHAID Class data Classification tree Chi-Square test No Pruning
CART Continuous data Classification and regression tree (CART) Gini index Entire error rate
Figure 4. A decision tree with irrelevant values.
Br’[A1], a1  Ck2 must be a part of rule implied
by Br’. Therefore, Br must be in conflict with Br’
with respect to A if and only if  a1, a1  Br[A1],
a1  Br’[A1] and Ck1  Ck2.
(2) Let A1 = 
 and Ck1  Ck2. When A1 = 
, it im-
plies that  a, a  Ck2 must be a part of rule im-
plied by Br’, where a  Br[A]. Since Ck1  Ck2,
Br must be in conflict with Br’ with respect to A.
According to theorem 4, when Ck1 = Ck2, branches,
Br and Br’ are never in conflict with each other; there-
fore, to identify all the irrelevant values of a branch Br,
we need only to consider those branches, Br’, whose
leaves are different from Ck1. The corresponding algo-
rithm is shown as Figure 5.
4. Experiments
In order to verify the proposed methodology, this
study adopts the decision tree in IBM DB2 Intelligent
Miner for Data to analyze the sputtering process data. In
this research, the studied plant is an optoelectronic com-
ponent plant in Tainan Science-based Industrial Park.
4.1 Mining Goal
The first step of this research is to set the mining
goal. Yield is an important index to represent the opto-
electronic companies’ value. A high yield indicates high
competitiveness, high production ability, and high qual-
ity. It directly affects production cost, so it is a basic tool
for measuring production performance. Therefore, the
mining goal of this study is to build a yield improvement
model. We will investigate glass products with multi-
layer coatings and aim to improve the yield of first thin-
film sputtering process. Although defects of sputtering
process are caused by process control or human errors, in
this study, the yield improvement goal is set to improve
defects which are caused by process control.
In the model, we use decision trees to analyze pro-
cess data to derive a set of valuable positive and negative
6 Ding-An Chiang et al.
Figure 5. Converting a decision tree to a set of rules without irrelevant conditions.
dition, data directly collected from the system, as shown
in Table 3 and Table 4, are not applicable to the decision
tree analysis. Thus, we have to join Table 3 and Table 4
into one table in the data preprocessing step. The studied
plant mainly focuses on small-quantity and diverse pro-
ducts; therefore, to avoid the problem that a smaller lot
quantity may lead to the deviation of the overall analysis
result, instead of using regression tree, we use classifica-
tion tree to establish the yield improvement model in this
study. Since target variable’s data type of the classifica-
tion tree has to be discrete and the data type of ‘Yield’ at-
tribute is numerical type, we have to transform lot-based
records into glass-based records by the input quantity
and yield of each lot in the data preprocessing step. We
use the following example to explain how to transform
lot-based records into glass-based records.
Example 2. Since the input quantity of lot ‘510005’ has
79 pieces of glass, as shown in Table 3, this lot-based
record is expended to 79 glass-based records, glass
number from ‘55000501’ to ‘55000579’, in Table 5. The
process parameters originally recorded by this lot-based
record are also expanded into the corresponding glass-
based records at the same time. That is, except the va-
lues of attribute ‘yield flag’ and ‘Glass No’, all other
attributes’ values of these 79 glass-based records are
same with each other. Since the yield of this lot is 0.975,
by formula (1), the yield flag’s values of 77 glass-based
records, glass number from ‘55000501’ to ‘55000577’,
are marked by ‘Y’ and that of two other records, glass
number ‘55000578’ and ‘55000579’, are marked by ‘N’,
where the value ‘Y’ and ‘N’ indicate the glass has prac-
tically passed and not passed the process, respectively.
In this table, the attribute ‘yield flag’ is considered as
the target variable of our model. Through the above
transformation, lot-based records are converted into
glass-based records and the target variable’s data type is
changed from numerical to discrete. Therefore, instead
of using regression tree, we can use classification tree
to predict the yield and create a classification model for
yield improvement.
4.3 Interpreting the Mining Results
Considering the derived decision tree presented in
8 Ding-An Chiang et al.
Table 4. Process Production Management Information (2)
Lot No Gas flow M4 Gas flow M6 Gas flow M8 Target 1 KWH Target 2 KWH Target 3 KWH Target 4 KWH
510005 40 100 60/55 1366.3 1034.9 1757 1976.5
510006 40 100 60/55 1283.7 0928.9 .01567.8 1525.0
510007 40 100 60/55 1316.3 1014.9 1657 1876.5
510039 32 080 48/60 1558.4 1286.4 2163 2982.6
510041 32 080 48/60 1598.4 1346.4 2203 3012.6… … … … … … …
Table 5. Transformed data
Glass No Lot No
Work
time
Lot
production
completed
Input
speed
Power
1
Power
2
Power
3
Power
4
Gas
flow
M4
Gas
flow
M6
Gas
flow
M8
Target 1
KWH
Target 2
KWH
Target 3
KWH
Target 4
KWH
Yield
flag
51000501 510005 263 Same Day 3.34 1.5 1.55 3.27 3.3 40 100 60/55 1366.3 1034.9 1757 1976.5 Y
51000502 510005 263 Same Day 3.34 1.5 1.55 3.27 3.3 40 100 60/55 1366.3 1034.9 1757 1976.5 Y
51000503 510005 263 Same Day 3.34 1.5 1.55 3.27 3.3 40 100 60/55 1366.3 1034.9 1757 1976.5 Y
… … … … … … … … … … … … … … … … …
51000543 510005 263 Same Day 3.34 1.5 1.55 3.27 3.3 40 100 60/55 1366.3 1034.9 1757 1976.5 Y
… … … … … … … … … … … … … … … … …
51000577 510005 263 Same Day 3.34 1.5 1.55 3.27 3.3 40 100 60/55 1366.3 1034.9 1757 1976.5 Y
51000578 510005 263 Same Day 3.34 1.5 1.55 3.27 3.3 40 100 60/55 1366.3 1034.9 1757 1976.5 N
51000579 510005 263 Same Day 3.34 1.5 1.55 3.27 3.3 40 100 60/55 1366.3 1034.9 1757 1976.5 N
51000601 510006 263.4 Same Day 3.47 1.5 1.55 3.27 3 40 100 60/55 1283.7 0928.9 1567.8 1525 Y
… … … … … … … … … … … … … … … … …
engineer to adjust the corresponding control parameters
in the process, the sputter coating process of this lot is al-
ready completed. Actually, in the following discussion,
we could find out that the condition ‘Lot production
completed = Same Day’ is an irrelevant condition in this
negative rule. When the condition ‘Lot production com-
pleted = Same Day’ is removed from the rule, this rule is
reduced to If ‘work time >= 333 seconds Then yield flag
= Y (83.6%)’. It becomes a very useful negative rule.
That is why when a decision tree is used to establish a
yield improvement model, it is important that a domain
expert is needed to review the decision tree carefully. In
other words, not only for the performance issue but also
for practical issue, the irrelevant conditions have to be
identified in the tree or when the tree is represented by a
set of rules, the irrelevant conditions have to be removed
from the resultant rules. In the follows, we introduce how
to use the modified algorithm to identify and remove the
irrelevant conditions from the rules.
Since the classes of leaf nodes in the tree, as shown
in Figure 6, are ‘Y’, we cannot identify any irrelevant
condition in the rules by the modified algorithm. How-
ever, as shown in Figure 6, different leaf nodes have dif-
ferent yield values, which may present different meaning
in our research; therefore, to find out and remove irre-
levant conditions from the resultant rules, we have to
convert the original class’s label of each leaf node to a
new class’s label by the predefined threshold value and
yield information of each leaf node. In this paper, when
the yield value of a leaf node is greater than the prede-
fined threshold value, the new value of this leaf node is
set as ‘Po’; otherwise, it is set as ‘Ne’, where ‘Po’ and
‘Ne’ represent ‘Positive rule’ and ‘Negative rule’, re-
spectively. Consequently, the original tree, as shown in
Figure 6, can be converted to the new tree, which is
shown in Figure 7.
Now, let us trace how the proposed algorithm works
in the tree of Figure 7. Since the only irrelevant values
condition is occurred in the branch Br2 of this tree, we
only check whether the condition ‘Lot production com-
pleted = Same Day’ is an irrelevant condition in Br2. By
theorem 3, we do not need to consider the branch Br3 in
this computation. Since the leaf nodes’values of branches
Br1 and Br2 are same with each other, these two branches
are never in conflict with each other by theorem 4. There-
fore, we can conclude that the condition ‘Lot production
completed’= ‘Same Day’ is an irrelevant condition in Br2
by theorem2. Through our modified algorithm, this new
tree can be represented by one positive decision rule and
two negative decision rules as follows:
Positive rule:
If ‘Lot production completed’ = ‘Same Day’
and ‘work time’ < 333 seconds
Then ‘new yield flag’= ‘Po’
Negative rules:
If ‘Lot production completed’ = ‘Next Day’
Then ‘new yield flag’ = ‘Ne’
If ‘work time’ >= 333 seconds
Then ‘new yield flag’= ‘Ne’
10 Ding-An Chiang et al.
Figure 7. Converted to the new tree.
