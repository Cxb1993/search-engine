some models; the others may need specified target neighbors for each sample or rely on the neighbors
defined by the original metric.
On the other hand, Xing et al. [12] formulated the problem of distance metric learning as a con-
vex programming problem with equivalence and inequivalence constraints. The equivalence constraint
forces the pair of semantically similar samples to be close together in the learned metric space. The
inequivalence constraint makes the pair of dissimilar samples not near in that space. Based on graph
embedding, Yan et al. [13] learned a distance metric by solving a semi-definite programming problem
with neighborhood homogeneity constraints. The approach of Weinberger et al. [14], referred as to
LMNN, uses the semi-definite programming for learning distance metrics. The objective of LMNN is to
find a distance metric such that every sample is close to its pre-specified target neighbors and far away
from the samples with different class labels. Jin et al. [15] also applied the semi-definite programming
to learn a distance metric. Wang et al. [16] applied ANMM on metric learning in a semi-supervised
setting. These approaches rely on pre-defined equivalence and inequivalence constraints. However, these
constraints may be difficult to specify when the underlying distribution of the sample of a class exhibits
multi-modal.
Local distance metric (LDM) [17] learns distance metrics by optimizing the compactness of a class
and the separability among classes in a local sense. Since the metric matrix induced by LDM in fact re-
weights the principal component of training samples, the metric matrix induced by LDM is not general.
Neighborhood component analysis (NCA) [18] is based on stochastic nearest neighbors and learns a
Mahalanobis distance metric for kNN classification. Although defining neighborhood relationships in an
adaptive and local sense, NCA is not ensured to be converged. The RELIEF-based approach [19, 20]
learns a weight for each feature based on local and adaptive neighborhood relationships. The RELIEF-
based approach has a nice convergency property; however, it may be inappropriate in the presence of
highly correlated features.
The boosting technique [21–23] is successful in various problems of machine learning, and Ad-
aboost [24, 25] is perhaps the most famous one. Some approaches to learning distance metrics also
adopt the boosting technique but most of them learn nonlinear distance metrics [26–28]. In particular,
Crammer et al. [29] have used the boosting technique with the alignment loss to learn a kernel matrix
in a semi-supervised setting. Their approach can produce a metric matrix in fact. The alignment loss
is based on the principle that two samples which should be similar should have a large inner-product
value; on the contrary, the inner-product value of two samples which should be dissimilar should be
small. Since two samples having a large inner-product value are not necessary to have a short Euclidean
distance between them, the alignment loss may not be suitable for inducing a Mahalanobis distance
metric.
In this study, the class label of a training sample is assumed to be assigned semantically, and no
target neighbors for a training sample are known beforehand. Here, the boosting technique is used to
yield a Mahalanobis distance metric for defining proper distance relationships among samples. Since
having the following two characteristics, the proposed approach could learn distance metrics suitable for
the nearest neighbor (NN) classification.
First, the proposed approach makes use of a hypothesis-margin-based loss function. This loss function
is based on the nearest miss and hit of the sample, and defines an upper bound for the leave-one-out
training error of the NN classification. In the literature, two types of margin, namely, the sample margin
and the hypothesis margin, have been proposed [30, 31]. The sample margin is referred to the distance
2
2.1 The loss function
Let X = {(x1, y1), (x2, y2), . . . , (xN , yN )} be a set of N training samples drawn i.i.d. from an unknown
distribution D over Rn × {1, . . . , c}, where xi denotes the measurement vector, and yi denotes the class
label of xi. Now, denote by h‖·‖M;X (x) the NN classifier with the M-distance and training set X , which
may be the prototype set for the NN classifier. Thus, h‖·‖M;X (x) always assigns to x the class label of
the prototype in X which has the shortest M-distance to x. Denote by C(xi) the set of the training
vector having a class label identical to yi. In addition, denote by Ch(xi) and Cm(xi) the sets of hits and
misses of xi; that is, Ch(xi) = C(xi)−{xi} and Cm(xi) = {x1, . . . ,xN}− C(xi). Thus, the leave-one-out
error of h‖·‖M;X , denoted by eˆrloo(h‖·‖M;X ), can be defined as
eˆrloo(h‖·‖M;X ) =
1
N
N∑
i=1
L0
(
min
x′∈Cm(xi)
‖xi − x′‖2M − min
x′′∈Ch(xi)
‖xi − x′′‖2M
)
,
where the θ-clipped loss function Lθ(z) is defined as
Lθ(z) =

1 z ≤ 0;
1− zθ 0 < z ≤ θ;
0 z > θ,
with θ ≥ 0. That is, eˆrloo(h‖·‖M;X ) calculates the ratio of the training sample of which the nearest
hit is farther than the nearest miss with respect to the M-distance. In addition, eˆrloo(h‖·‖M;X ) can be
bounded above by
eˆrloo(h‖·‖M;X ) ≤
1
N
N∑
i=1
exp
(
min
x′∈Ch(xi)
‖xi − x′‖2M − min
x′′∈Cm(xi)
‖xi − x′′‖2M
)
, (1)
which is related to hypothesis margins for all training samples [30].
In addition, we define function fM(x,S) as
fM(x,S) =
∑
(i,ψ)∈S
ψ‖xi − x‖2M
where S is a set of pairs of the index of a sample and the averaging weight associated with the sample.
Denote by Hkh;‖·‖M(x) andMkm;‖·‖M(x) the sets of sample-index-and-averaging-weight pairs for the kh
nearest hits and km nearest misses of x defined by the M-distance, respectively; that is,
Hkh;‖·‖M(x) =
{(
i,
1
kh
)
|xi ∈ kh nearest hits of x defined by M-distance
}
Mkm;‖·‖M(x) =
{(
i,
1
km
)
|xi ∈ km nearest misses of x defined by M-distance
}
.
4
where
δi;t = fMt(xi,H1;‖·‖Mt (xi))− fMt(xi,M1;‖·‖Mt (xi)), (4)
wi;t−1 = exp(δi;t−1), (5)
di;t =
fQt(xi,M1;‖·‖Qt (xi))− fQt(xi,H1;‖·‖Mt−1 (xi)) if t > 1fQ1(xi,M1;‖·‖Q1 (xi))− fQ1(xi,H1;‖·‖Q1 (xi)) if t = 1 (6)
with M0 = 0.
Proof. See Appendix B.
Accordingly, from Corollary 1, an upper bound for B(Mt) can be obtained as follows:
B(Mt) ≤ 1
N
N∑
i=1
wi;t−1 exp(−αtdi;t) (7)
=
1
N
N∑
i=1
wi;t−1 exp
(
1− di;tηt
2
(ηtαt) +
1 +
di;t
ηt
2
(−ηtαt)
)
(8)
≤ 1
N
N∑
i=1
wi;t−1
(
1− di;tηt
2
exp(ηtαt) +
1 +
di;t
ηt
2
exp(−ηtαt)
)
(9)
, g(αt)
where ηt = maxi=1,...,N |di;t|, and Inequality (9) is derived from Eq. (8) by using Jensen’s inequality [34].
2.2.2 Calculation of αt
With respect to given Qt, αt may be determined in such a way that g(αt) is minimized. Now, by taking
the derivative of g(αt) to zero and using the fact that g(αt) is convex, an analytical formula for αt to
minimize g(αt) can be as follows:
αt =
0
1
2ηt
ln
(
1+γt
1−γt
)
< 0
1
2ηt
ln
(
1+γt
1−γt
)
otherwise,
(10)
where
γt =
1
ηt
N∑
i=1
w′i;t−1di;t (11)
with sample weight w′i;t−1 defined by
w′i;t−1 =
wi;t−1∑N
i=1 wi;t−1
. (12)
For convenience, these sample weights form a sample weight vector w′t−1 = [w
′
i;t−1]. It should be noticed
that the average hypothesis margin with respect to Qt with sample weight vector w
′
t−1 is at least ηtγt.
6
Thus, we can obtain Lt through maximizing the problem P:
P : argmax
Lt
tr
(
LTt X(LAt − LBt)XTLt
)
subject to LTt XLCtX
TLt = I0
where LCt is the Laplacian matrix for weight matrix Ct, and I0 is a diagonal matrix with diagonal
elements zero or one. Weight matrix Ct = [cij;t] is for the nearest-hit relationships among samples
defined by the Qt-distance with sample weight vector w
′
t−1; that is,
cij;t = R(i, j;w
′
t−1,H1;‖·‖Qt (·)). (16)
The constraint on Lt has two purposes: 1. to establish a proper metric system induced by Lt; 2. to put
bounds on the scale of Lt. It should be noticed that Bt is equal to Ct if t = 1, and Bt is independent
to Qt if t > 1.
Directly solving the constrained optimization problem P is not easy because it also includesM1;‖·‖Qt (xi)
and H1;‖·‖Qt (xi), which are defined by the Qt-distance. Here, the framework of the EM algorithm [35]
is adopted for calculating Qt iteratively as follows.
• The E-step. Determine M1;‖·‖Qt (xi) and H1;‖·‖Qt (xi) for every sample xi with respect to Qt
estimated in the previous EM-iteration.
• The M-step. Calculate Lt by Theorem 3, and assign LtLTt to Qt. It should be noticed that
XLCtX
T may need regularization because LCt is only positive semi-definite [36].
At last, the steps for calculating Qt are summarized in Algorithm 1, which is referred as to Base-
Learner.
Theorem 3. The solution of the constrained optimization problem P with fixed LAt , LBt , and LCt can
be obtained by
Lt = ΠΛ, (17)
where Π is an eigenvector matrix of the matrix pair (X(LAt−LBt)XT ,XLCtXT ) with ΠTXLCtXTΠ =
I, and Λ = [λij ] is a diagonal matrix with
λii =
1 if the ith generalized eigenvalue is larger than 0;0 elsewhere.
Proof. This theorem can be proved by the technique described in [33] for proving a similar theorem. 
2.4 The boosting algorithm
Algorithm 2 presents the proposed boosting algorithm, which is called BoostMDM. Here, Base-
Learner is adopted to be the metric-matrix base learner. In order to have a variety of base metric
matrices in a few boosting iterations, Qt will be found in the null space ofMt−1. Without this strategy,
Qt’s learned in successive boosting iterations may be highly linear correlated. Besides, BoostMDM
should be stopped when one of the following three termination conditions occurs.
8
• First, the rank of Qt is zero; in other words, BaseLearner cannot find a useful metric matrix;
• Second, γt is not greater than zero; that is, integrating Qt may not improve the margin;
• Third, Mt has full rank.
Now, the main steps of BoostMDM are illustrated as follows.
• Repeat the following steps until one of the above three termination conditions occurs.
– Line 7 calculates the sample weight vector w′t−1 = [w
′
i;t−1] by Eq. (12).
– Line 10 applies BaseLearner with sample weight vector w′t−1 and sample matrix X˜t−1 to
yield Q˜t, where X˜t−1 is composed of the projections of the training samples onto the null
space of Mt−1.
– Line 15 calculates the metric matrix Qt by Qt = Et−1Q˜tETt−1, where the columns of Et−1
form an orthonormal basis for the null space of Mt−1.
– Line 21 calculates αt and combines Mt−1 with Qt to form Mt.
– Lines 22 to 31 find the null space of Mt, calculate an orthogonal basis for the null space of
Mt, and project the sample matrix onto the null space of Mt for calculating Qt+1.
• The last step decomposesMT into LTLTT such that the columns of LT are ordered by discriminatory
potential.
The rationale of the last step is explained as follows. With MT = ΨΘΨ
T an eigen-decomposition
of MT and Φ an orthogonal matrix, LT = ΨΘ
1
2Φ is a general form for LT such that MT = LTL
T
T . To
order the discriminatory potential of the column of LT , we may define weight matrices A = [aij ] and
C = [cij ] by
aij = R(i, j;
1
N
11×N ,M1;‖·‖MT (·)) and (18)
cij = R(i, j;
1
N
11×N ,H1;‖·‖MT (·)) (19)
to describe the nearest-miss and nearest-hit relationships among the training samples defined by the
MT -distance. Thus, the discriminatory potential of x may be gauged by x
TX(LA − LC)XTx, which
is the difference between the average squared distance to the nearest miss and that to the nearest hit
when the training sample is projected onto x. Since the ith diagonal element of LTTX(LA − LC)XTLT
is the discriminatory potential of the ith column of LT , Φ can be determined by the eigenvector matrix
of Θ
1
2ΨTX(LA − LC)XTΨΘ 12 with corresponding eigenvalues in non-ascending order.
2.5 A bound on the generalization error
For simplicity, we consider the two-class classification problem; that is, the sample is in Z = Rn×{−1, 1}.
Suppose that the training set is the prototype set. Thus, the classifier to be analyzed may be expressed
as
h‖·‖M;X (x) = yp × (‖x− p′‖2M − ‖x− p‖2M), (20)
where p is the prototype having the shortest M-distance to x, yp is the class label of p, and p
′ is the
prototype of class −yp nearest to x. In other words, the sign and magnitude of h‖·‖M;X (x) are the
10
predicted class-label for x, and the prediction confidence, respectively. Thus, the generalization error of
h‖·‖M;X (x) can be defined as
erD(h‖·‖M;X ) = E(x,y)∈ZL0(y × h‖·‖M;X (x)).
Denoting by X \i the sample set X excluding (xi, yi), we may express the empirical leave-one-out error
eˆrloo(h‖·‖M;X ) as
eˆrloo(h‖·‖M;X ) =
1
N
N∑
i=1
L0(yi × h‖·‖M;X\i(xi)).
Furthermore, we also define the following θ-clipped error estimates
erθD(h‖·‖M;X ) = E(x,y)∈ZLθ(y × h‖·‖M;X (x));
eˆrθloo(h‖·‖M;X ) =
1
N
N∑
i=1
Lθ(yi × h‖·‖M;X\i(xi)).
To show a relationship between erD(h‖·‖M;X ) and eˆr
θ
loo(h‖·‖M;X ), two assumptions are made.
• First, the spectral norm of the metric matrix is normalized to one without lost of generality.
• Second, the distribution D is supported by a ball of radius r.
In this study, we adopt a technique for analyzing the generalization error of a graphical learning algorithm
of which some part is stable [37]. Here, the stable part is h‖·‖M;X with given ‖ · ‖M. Define a function
set P(n, l) = {‖x‖2M =
∑l
i=1(u
T
i x)
2} for the squared M-distance where x ∈ Rn and M = LLT with
L = [u1 . . .ul]. A theorem about an upper bound of the generalization error of h‖·‖M;X , which is in
terms of the covering number [38] of P(n, l) and the classification stability [39] of the stable part, will be
provided. First, the covering number of a function set, and the classification stability for a real-valued
learning algorithm are defined as follows.
Definition 1. Let F = {f(x : u)|x ∈ Rn} be a set of real-valued functions parameterized by u. Denote
by V a set of vectors in RN . Given a set of N observations S = {x1, . . . ,xN}, the covering number for F
with respect to these N observations, denoted as N (F , ϵ,S), is the cardinality of the smallest V such that
for every α, there exists v ∈ V satisfying ‖[f(x1|u), . . . , f(xN |u)]T − v‖2 ≤
√
Nϵ. Define N (F , ϵ,N) as
supS N (F , ϵ,S).
Definition 2. A real-valued learning algorithm which maps the training set X to a classifier hX (x) has
classification stability β if the following relationship is satisfied
∀X ∈ ZN ,∀i ∈ {1, . . . , N}, |hX (x)− hX\i(x)| ≤ β,
where hX (x) predicts the label of an arbitrary instance x by the sign of hX (x).
The main result is as follows.
Theorem 4. Let D be an unknown distribution over Z which is supported on a ball of radius r, and
X ∈ ZN be a training set in which every element is drawn i.i.d. from D. Suppose that h‖·‖M;X with any
12
convenience, this extension is called BoostMDM-K. In order not to mislead BoostMDM-K, we have
found that km could be small but kh could not. This is because once hh is small, a large margin between
km nearest misses and kh nearest hits may result from inadequate sampling of nearest hits.
2.6.2 Gaussian-weighted averages of the hits and misses
Based on the stochastic nearest-neighbor model described in [18], the expected squared distances from
a sample x to its nearest hit and miss with respect to the M-distance may be calculated by
∑
y∈Ch(x)
exp(−µ‖y − x‖2M)∑
z∈Ch(x) exp(−µ‖z− x‖2M)
‖y − x‖2M, and
∑
y∈Cm(x)
exp(−µ‖y − x‖2M)∑
z∈Cm(x) exp(−µ‖z− x‖2M)
‖y − x‖2M,
respectively, where µ ≥ 0. Accordingly, the second extension is defined based on the stochastic nearest-
neighbor model as follows:
eˆrloo(h
G
‖·‖M;X ) =
1
N
N∑
i=1
L0(fM(xi,HG;‖·‖M(xi)) > fM(xi,MG;‖·‖M(xi)))
≤ 1
N
N∑
i=1
exp(fM(xi,HG;‖·‖M(xi))− fM(xi,MG;‖·‖M(xi)))
, BG(M),
where
HG;‖·‖M(x) =
{
(i, ψ)|ψ = exp(−µ‖xi − x‖
2
M)∑
z∈Ch(x) exp(−µ‖z− x‖2M)
,xi ∈ Ch(x)
}
,
MG;‖·‖M(x) =
{
(i, ψ)|ψ = exp(−µ‖xi − x‖
2
M)∑
z∈Cm(x) exp(−µ‖z− x‖2M)
,xi ∈ Cm(x)
}
.
By changing H1;•(x) andM1;•(x) into HG;•(x) andMG;•(x), respectively, BaseLearner and Boost-
MDM can also be applied for this extension. This extension is referred as to BoostMDM-G. However,
BoostMDM-G does not have a convergency property as clear as that for BoostMDM-K because this
property is dependent on µ as the illustration in the following.
When µ is equal to zero, we have
HG;‖·‖M(x) = Hcard(Ch(x));‖·‖M(x),
MG;‖·‖M(x) =Mcard(Cm(x));‖·‖M(x);
that is, fM(x,HG;‖·‖M(x)) and fM(x,MG;‖·‖M(x)) actually calculate the average squared M-distances
from x to the hits and misses of x, respectively. It can be checked that Theorem 1 is satisfied here. Thus,
we can conclude that the boosting framework for BG(M) has a convergency property like Theorem 2
when µ is equal to zero.
On the other hand, if the distances from a sample to the others are all distinct, and µ approaches
14
On the other hand, after obtaining M1, BoostMDM-K and BoostMDM-G try to improve M1
by integrating M1 with other metric matrices, which are orthogonal to M1 and to each other. Thus,
BoostMDM-K and BoostMDM-G have the following two different properties.
• First, BoostMDM-K and BoostMDM-G may converge in terms of an upper bound of the
leave-one-out training error.
• Second, BoostMDM-K and BoostMDM-G make use of the information in the null space of
M1.
3.2 Undersampled problems
When the sample vectors are higher-dimensional data, the number of training samples may be less than
the dimensionality of the sample vector (i. e., N < n). In this circumstance, the principal component
analysis (PCA) is first applied to reduce the dimensionality of the sample vector from n to N − 1. It
should be emphasized that no principal components are abandoned at this step. Denote by Ω the unit
eigenvector matrix for the PCA, which is an n × (N − 1) matrix. Then, BoostMDM works on the
reduced sample matrix, ΩTX. Finally, the transformation matrix corresponding to the resultant metric
matrix can be formed by ΩLT . It should be noticed that calculating Lt by Theorem 3 at the M-step of
BaseLearner needs proper regularization. Otherwise, the overfitting problem may occur. Here, for the
undersampled problem, Lt is calculated by Theorem 3 with the matrix pair (X(LAt−LBt)XT ,XLCtXT+
λσ1(XLCtX
T )I), where λσ1(XLCtX
T )I is a regularization term with σ1(XLCtX
T ) the largest singular
value of XLCtX
T and λ an empirically selected parameter.
4 Experimental Results
Table 1 lists seventeen data sets for performance evaluation. These data sets include
• eight data sets from UCI machine learning repository [40]: Spectf, Wisconsin Diagnostic Breast
Cancer (WDBC), Parkinsons, Yeast, Wine, Balance, Ionosphere, and Spambase;
• one data set of handwritten digit images: USPS1;
• three gene expression data sets: COLON [41], Prostate [42] and GCM [43];
• three face data sets: GT [44], YALE [45], and AR [46];
• two synthetical data sets: Crings and Waveform.
Spectf, Breast, Parkinsons, Yeast, Wine, Balance, Ionosphere, Waveform, and Crings are lower-dimensional
small-scale data sets. USPS, and Spambase are larger-scale data sets. COLON, Prostate, GCM, GT,
YALE, and AR are higher-dimensional data sets. The face data set consists of 64 × 80 grayscale face
images, which are manually aligned with respect to the two eyes. The specification of the synthetical
data set Waveform can be found in [47]. The synthetical data set Crings consists of three-dimensional
sample vectors of five classes. The five classes have equal prior probabilities, and are distributed over
five co-centric ring-shaped regions in the first two dimensions. Each ring-shaped region contains a single
1http://www-stat-class.stanford.edu/~tibs/ElemStatLearn/data.html
16
Table 2: The important parameters for the proposed approach.
Algorithm Parameter Value Description
BoostMDM-K kh 5
km 5
BoostMDM-G µ N∑N
i=1 ‖xi−xi;7‖2M
xi;7 denotes the seventh
nearest neighbor of sam-
ple vector xi defined by
the M-distance.
BaseLearner maximum
number of
iterations
10
λ
0.1 regularization parameter
for COLON
0.01 regularization parameter
for Prostate
0.005 regularization parameter
for GCM
10−8 regularization parameter
for GT, YALE, and AR
18
(a) Spectf (b) WDBC
(c) Parkinsons (d) Yeast
Figure 1: 2-D visualization of lower-dimensional and small-scale data sets, and a subset of the first five
classes of USPS.
20
(i) Crings (j) USPS
Figure 1: (Continued) 2-D visualization of lower-dimensional and small-scale data sets, and a subset of
the first five classes of USPS.
22
0 0.5 1
PCA (0.77,  1)
RCA (0.70,  4)
FDA (0.69,  1)
HLDA (0.80,  3)
LFDA (0.78,  4)
DANN (0.70,  7)
MFA (0.71,  8)
NCA (0.70,  2)
LMNN (0.74, 19)
LDM (0.77,  1)
BOOSTKERNEL (0.75,  2)
I−MFA (0.75,  2)
BOOSTMDM−K (0.76,  3)
GI−RELIEF (0.76, 15)
BOOSTMDM−G (0.77, 16)
Spectf
0 0.5 1
PCA (0.92,  5)
RCA (0.91, 30)
FDA (0.58,  1)
HLDA (0.96,  3)
LFDA (0.93, 13)
DANN (0.88,  4)
MFA (0.92, 30)
LMNN (0.95, 29)
BOOSTKERNEL (0.87,  2)
I−MFA (0.95,  8)
BOOSTMDM−K (0.96, 10)
GI−RELIEF (0.96,  2)
BOOSTMDM−G (0.96, 27)
WDBC
0 0.5 1
PCA (0.85,  5)
RCA (0.89, 22)
FDA (0.69,  1)
HLDA (0.90, 18)
LFDA (0.89, 15)
DANN (0.84,  3)
MFA (0.88, 22)
NCA (0.85, 19)
LMNN (0.86, 21)
LDM (0.84, 10)
BOOSTKERNEL (0.75,  2)
I−MFA (0.93,  9)
BOOSTMDM−K (0.93, 21)
GI−RELIEF (0.92,  9)
BOOSTMDM−G (0.92,  9)
Parkinsons
0 0.5 1
PCA (0.52,  7)
RCA (0.52,  8)
FDA (0.51,  8)
HLDA (0.52,  8)
LFDA (0.52,  8)
DANN (0.50,  8)
MFA (0.51,  8)
NCA (0.52,  8)
LMNN (0.53,  8)
LDM (0.51,  7)
BOOSTKERNEL (0.34,  1)
I−MFA (0.46,  6)
BOOSTMDM−K (0.46,  6)
GI−RELIEF (0.51,  7)
BOOSTMDM−G (0.51,  8)
Yeast
0 0.5 1
PCA (0.73,  6)
RCA (0.98, 13)
FDA (0.71,  2)
HLDA (0.99,  2)
LFDA (0.93,  3)
DANN (0.72,  9)
MFA (0.96,  8)
NCA (0.87, 12)
LMNN (0.94, 12)
LDM (0.90, 12)
BOOSTKERNEL (0.58,  1)
I−MFA (0.98, 10)
BOOSTMDM−K (0.98,  8)
GI−RELIEF (0.99,  6)
BOOSTMDM−G (0.99,  6)
Wine
0 0.5 1
PCA (0.79,  4)
RCA (0.89,  2)
FDA (0.88,  1)
HLDA (0.88,  1)
LFDA (0.90,  4)
DANN (0.70,  4)
MFA (0.79,  4)
NCA (0.96,  4)
LMNN (0.83,  2)
LDM (0.94,  4)
BOOSTKERNEL (0.88,  4)
I−MFA (0.90,  3)
BOOSTMDM−K (0.90,  3)
GI−RELIEF (0.96,  3)
BOOSTMDM−G (0.96,  3)
Balance
0 0.5 1
PCA (0.88,  9)
RCA (0.87, 10)
FDA (0.84,  1)
HLDA (0.92,  8)
LFDA (0.86, 10)
DANN (0.90, 10)
MFA (0.88,  9)
NCA (0.89, 12)
LMNN (0.90,  8)
LDM (0.90, 12)
BOOSTKERNEL (0.77,  2)
I−MFA (0.89, 12)
BOOSTMDM−K (0.89, 12)
GI−RELIEF (0.92,  6)
BOOSTMDM−G (0.92,  6)
Ionosphere
0 0.5 1
PCA (0.80,  2)
RCA (0.65, 21)
FDA (0.49,  2)
HLDA (0.80,  2)
LFDA (0.81,  2)
DANN (0.76, 21)
MFA (0.60, 21)
NCA (0.77, 17)
LMNN (0.76, 21)
LDM (0.82, 15)
BOOSTKERNEL (0.51,  2)
I−MFA (0.80,  2)
BOOSTMDM−K (0.80,  2)
GI−RELIEF (0.79,  2)
BOOSTMDM−G (0.79,  2)
Waveform
0 0.5 1
PCA (0.81,  3)
RCA (1.00,  2)
FDA (0.85,  3)
HLDA (1.00,  2)
LFDA (0.75,  3)
DANN (0.81,  3)
MFA (0.90,  3)
NCA (1.00,  2)
LMNN (0.98,  2)
LDM (0.67,  3)
BOOSTKERNEL (0.23,  2)
I−MFA (1.00,  2)
BOOSTMDM−K (1.00,  2)
GI−RELIEF (1.00,  2)
BOOSTMDM−G (1.00,  2)
Crings
Figure 2: Classification rates for the small-scale data set, where the numbers enclosed in the paren-
theses are the best average classification rate and the associated dimensionality of the mapped vector,
respectively.
24
  
Bkh,km
1
Upper bound of Bkh,km
0 20 40
0.6
0.8
1
Spectf
Rank
Lo
ss
0 10 20 30
0.6
0.8
1
WDBC
Rank
Lo
ss
0 10 20
0.6
0.8
1
Parkinsons
Rank
Lo
ss
0 2 4 6 8
0.6
0.8
1
Yeast
Rank
Lo
ss
0 5 10
0.6
0.8
1
Wine
Rank
Lo
ss
0 2 4
0.6
0.8
1
Balance
Rank
Lo
ss
0 10 20 30
0.6
0.8
1
Ionosphere
Rank
Lo
ss
0 10 20
0.6
0.8
1
Waveform
Rank
Lo
ss
0 1 2 3
0.6
0.8
1
Crings
Rank
Lo
ss
0 100 200
0.6
0.8
1
USPS
Rank
Lo
ss
0 20 40
0.6
0.8
1
Spambase
Rank
Lo
ss
0 50 100 150
0.6
0.8
1
GCM
Rank
Lo
ss
0 20 40 60
0.6
0.8
1
COLON
Rank
Lo
ss
0 50 100
0.6
0.8
1
Prostate
Rank
Lo
ss
0 500 1000
0.6
0.8
1
AR
Rank
Lo
ss
0 200 400
0.6
0.8
1
GT
Rank
Lo
ss
0 200 400 600
0.6
0.8
1
YALE
Rank
Lo
ss
Figure 4: Plots of Bkh,km(Mt) with respect to the rank of Mt for BoostMDM-K.
26
0 0.5 1 1.5 2
PCA        
RCA        
FDA        
HLDA       
LFDA       
DANN       
MFA        
LMNN       
BOOSTKERNEL
I−MFA      
BOOSTMDM−K 
GI−RELIEF  
BOOSTMDM−G 
Spectf
 < 0.1
 < 0.1
 < 0.1
 < 0.1
 < 0.1
   0.3
 < 0.1
   0.8
   0.2
   0.1
   0.5
   0.2
   0.6
0 1 2 3 4 5
PCA        
RCA        
FDA        
HLDA       
LFDA       
DANN       
MFA        
LMNN       
BOOSTKERNEL
I−MFA      
BOOSTMDM−K 
GI−RELIEF  
BOOSTMDM−G 
WDBC
 < 0.1
 < 0.1
 < 0.1
 < 0.1
 < 0.1
   0.6
 < 0.1
   1.0
   0.8
   0.5
   1.6
   0.9
   3.7
0 100 200 300 400 500
PCA        
RCA        
FDA        
HLDA       
LFDA       
DANN       
MFA        
NCA        
LMNN       
LDM        
BOOSTKERNEL
I−MFA      
BOOSTMDM−K 
GI−RELIEF  
BOOSTMDM−G 
Parkinsons
 < 0.1
 < 0.1
 < 0.1
 < 0.1
 < 0.1
   0.2
 < 0.1
 342.4
   0.4
  44.4
   0.1
 < 0.1
   0.2
   0.1
   0.2
0 100 200 300 400 500
PCA        
RCA        
FDA        
HLDA       
LFDA       
DANN       
MFA        
NCA        
LMNN       
LDM        
BOOSTKERNEL
I−MFA      
BOOSTMDM−K 
GI−RELIEF  
BOOSTMDM−G 
Yeast
 < 0.1
 < 0.1
 < 0.1
 < 0.1
 < 0.1
   1.9
   0.1
 172.2
 101.2
 375.0
   6.4
   2.9
   3.6
   5.2
   8.2
0 10 20 30
PCA        
RCA        
FDA        
HLDA       
LFDA       
DANN       
MFA        
NCA        
LMNN       
LDM        
BOOSTKERNEL
I−MFA      
BOOSTMDM−K 
GI−RELIEF  
BOOSTMDM−G 
Wine
 < 0.1
 < 0.1
 < 0.1
 < 0.1
 < 0.1
   0.1
 < 0.1
   1.8
   0.3
  24.0
   0.1
 < 0.1
   0.1
   0.1
 < 0.1
0 10 20 30
PCA        
RCA        
FDA        
HLDA       
LFDA       
DANN       
MFA        
NCA        
LMNN       
LDM        
BOOSTKERNEL
I−MFA      
BOOSTMDM−K 
GI−RELIEF  
BOOSTMDM−G 
Balance
 < 0.1
 < 0.1
 < 0.1
 < 0.1
 < 0.1
   0.6
 < 0.1
   0.2
   0.5
  23.3
   0.7
   0.4
   0.4
   0.8
   0.6
0 100 200 300
PCA        
RCA        
FDA        
HLDA       
LFDA       
DANN       
MFA        
NCA        
LMNN       
LDM        
BOOSTKERNEL
I−MFA      
BOOSTMDM−K 
GI−RELIEF  
BOOSTMDM−G 
Ionosphere
 < 0.1
 < 0.1
 < 0.1
 < 0.1
 < 0.1
   0.3
 < 0.1
   0.4
   0.8
 257.9
   0.4
   0.2
   0.4
   0.3
   0.5
0 200 400 600
PCA        
RCA        
FDA        
HLDA       
LFDA       
DANN       
MFA        
NCA        
LMNN       
LDM        
BOOSTKERNEL
I−MFA      
BOOSTMDM−K 
GI−RELIEF  
BOOSTMDM−G 
Waveform
 < 0.1
 < 0.1
 < 0.1
 < 0.1
 < 0.1
   0.9
 < 0.1
  60.9
   0.9
 438.4
   1.6
   0.8
   1.0
   1.6
   1.5
0 10 20 30
PCA        
RCA        
FDA        
HLDA       
LFDA       
DANN       
MFA        
NCA        
LMNN       
LDM        
BOOSTKERNEL
I−MFA      
BOOSTMDM−K 
GI−RELIEF  
BOOSTMDM−G 
Crings
 < 0.1
 < 0.1
 < 0.1
 < 0.1
 < 0.1
   0.6
 < 0.1
  17.2
   0.3
  21.3
   1.0
   0.5
   0.5
   0.9
   0.3
Figure 6: Average computation time for the small-scale data set (in seconds).
28
A A Proof for Theorem 1
According to the definitions of S and Hkh;‖·‖Mt (x), fMt(x,Hkh;‖·‖Mt (x)) calculates the average squared
Mt-distance from x to its kh nearest hits which are defined by theMt-distance, whereas fMt(x,S(x)) cal-
culates the average squaredMt-distance from x to at least kh hits. Thus, we have fMt(x,Hkh;‖·‖Mt (x)) ≤
fMt(x,S(x)). In addition, due to fMt(x,S(x)) = fMt−1(x,S(x)) + αfQt(x,S(x)), we can obtain
fMt(x,Hkh;‖·‖Mt (x)) ≤ fMt−1(x,S(x)) + αfQt(x,S(x)). (21)
Furthermore, we can decompose fMt(x,Mkm;‖·‖Mt (x)) as
fMt(x,Mkm;‖·‖Mt (x)) = fMt−1(x,Mkm;‖·‖Mt (x)) + αfQt(x,Mkm;‖·‖Mt (x)),
and obtain
fMt(x,Mkm;‖·‖Mt (x)) ≥ fMt−1(x,Mkm;‖·‖Mt−1 (x)) + αfQt(x,Mkm;‖·‖Qt (x)), (22)
by inequalities (23) and (24) as follows:
fMt−1(x,Mkm;‖·‖Mt (x)) ≥ fMt−1(x,Mkm;‖·‖Mt−1 (x)) (23)
fQt(x,Mkm;‖·‖Mt (x)) ≥ fQt(x,Mkm;‖·‖Qt (x)). (24)
Inequalities (23) and (24) are established based on the fact that Mkm;‖·‖Mt−1 (x) and Mkm;‖·‖Qt (x) are
composed of the km nearest misses of x defined in terms of metric matrices Mt−1 and Qt, respectively.
Subtracting the left-hand side of Inequality (22) from that of Inequality (21), we can obtain
fMt(x,Hkh;‖·‖Mt (x))− fMt(x,Mkm;‖·‖Mt (x))
≤fMt−1(x,S(x)) + αfQt(x,S(x))−
(fMt−1(x,Mkm;‖·‖Mt−1 (x)) + αfQt(x,Mkm;‖·‖Qt (x)))
(25)
and complete this proof by rearranging the right-hand side of Inequality (25). 
B A Proof for Corollary 1
In case t > 1, we may express δi;t as
δi;t = fMt(xi,H1;‖·‖Mt (xi))− fMt(xi,M1;‖·‖Mt (xi))
≤ fMt−1(xi,H1;‖·‖Mt−1 (xi))− fMt−1(xi,M1;‖·‖Mt−1 (xi))+
αt(fQt(xi,H1;‖·‖Mt−1 (xi))− fQt(xi,M1;‖·‖Qt (xi)))
= δi;t−1 + αt(fQt(xi,H1;‖·‖Mt−1 (xi))− fQt(xi,M1;‖·‖Qt (xi)))
(26)
30
Case 3-2. If the label of r is −y, r must be q. Thus, we have
|h‖·‖M;X (x)− h‖·‖M;X\i(x)| =|y(‖x− q‖2M − ‖x− p‖2M)−
(−y)(‖x− p′‖2M − ‖x− q‖2M)|
=‖x− p′‖2M − ‖x− p‖2M
Accordingly, we can conclude for any i ∈ {1, . . . , N},
|h‖·‖M;X (x)− h‖·‖M;X\i(x)| ≤ max{‖x− p′‖2M − ‖x− p‖2M, ‖x− q′‖2M − ‖x− q‖2M}
and have completed the proof for this lemma. 
Corollary 2. The classification stability of h‖·‖M;X with given ‖ · ‖M is bounded above by 4r2.
Proof. Combining Lemma 1 with the two assumptions made beforehand: the metric matrix M has the
spectral norm of one, and the distribution D is supported by a ball of radius r, we have that |h‖·‖M;X (x)−
h‖·‖M;X\i(x)| is not greater than 4r2. 
The next step is to estimate the covering number of P(n, l). Lemma 2 shows that Lθ is Lipschitz con-
tinuous with respect to ‖·‖M. Corollary 3 presents a covering number of P(n, l) based on Theorem 5 [38],
which is about the covering number for a set of linear functions.
Lemma 2. Suppose that D is supported by a ball of radius r. Then, for any (x, y) ∈ Z, we have
|Lθ(y × h‖·‖M1 ;X (x))− Lθ(y × h‖·‖M2 ;X (x))|
≤ 8r
2
θ
sup
‖y‖2=1
|‖y‖2M1 − ‖y‖2M2 |
=
8r2
θ
‖M1 −M2‖s
where ‖ · ‖s denotes the spectral norm.
Proof. Since Lθ is θ−1-Lipschitz, we have
|Lθ(y × h‖·‖M1 ;X (x))− Lθ(y × h‖·‖M2 ;X (x))| ≤
1
θ
|h‖·‖M1 ;X (x)− h‖·‖M2 ;X (x)|.
Denote by pi the prototype nearest to x with respect to the Mi-distance, and ypi the label of pi. Denote
by p′i the prototype of class −ypi nearest to x with respect to the Mi-distance. Thus, we have
1
θ
|h‖·‖M1 ;X (x)− h‖·‖M2 ;X (x)|
=
1
θ
|‖yp1(‖x− p′1‖2M1 − ‖x− p1‖2M1)− yp2(‖x− p′2‖2M2 − ‖x− p2‖2M2)|.
Next, this lemma may be proven by considering the relationship between yp1 and yp2 .
• In case yp1 is equal to yp2 , we may assume h‖·‖M1 ;X (x) is not less than h‖·‖M2 ;X (x) without lost
32
Now, Theorem 4 can be proven as follows. Due to Lemma 2, we have
sup
‖·‖M∈P(n,l)
erθD(h‖·‖M;X )− eˆrθloo(h‖·‖M;X )
≤ sup
‖·‖M∈ θϵ16r2 -P(n, l)
erθD(h‖·‖M;X )− eˆrθloo(h‖·‖M;X ) + ϵ.
It then follows that
PX
(
sup
‖·‖M∈P(n,l)
erθD(h‖·‖M;X )− eˆrθloo(h‖·‖M;X ) > t
)
≤PX
 sup
‖·‖M∈ θϵ16r2 -P(n, l)
erθD(h‖·‖M;X )− eˆrθloo(h‖·‖M;X ) > t− ϵ

≤N
(
P(n, l), θϵ
16r2
, N
)
sup
‖·‖M∈ θϵ16r2 -P(n, l)
PX
(
erθD(h‖·‖M;X )− eˆrθloo(h‖·‖M;X ) > t− ϵ
)
≤N
(
P(n, l), θϵ
16r2
, N
)
sup
‖·‖M∈P(n,l)
PX
(
erθD(h‖·‖M;X )− eˆrθloo(h‖·‖M;X ) > t− ϵ
)
,
where θϵ16r2 -P(n, l) denotes a θϵ16r2 -cover of P(n, l). Since 0 ≤ Lθ ≤ 1, we may utilize Theorem 12 in [39]:
PX
(
erθD(h‖·‖M;X )− eˆrθloo(h‖·‖M;X ) > ϵ+
β∗
θ
)
≤ exp
(
− 2Nϵ
2
(4Nβ∗/θ + 1)2
)
and obtain
PX
(
sup
‖·‖M∈P(n,l)
erθD(h‖·‖M;X )− eˆrθloo(h‖·‖M;X ) > 2ϵ+
β∗
θ
)
≤ N
(
P(n, l), θϵ
16r2
, N
)
exp
(
− 2Nϵ
2
(4Nβ∗/θ + 1)2
) (27)
by assigning 2ϵ+ β
∗
θ to t. Thus, a PAC style generation error bound can be obtained by
erθD(h‖·‖M;X ) ≤ eˆrθloo(h‖·‖M;X ) +
β∗
θ
+
2
(
4Nβ∗
θ
+ 1
)√
1
2N
(
lnN
(
P(n, l), θϵ
16r2
, N
)
+ ln(1/δ)
)
.
Since erD(h‖·‖M;X ) is not greater than er
θ
D(h‖·‖M;X ), the proof of the first part of Theorem 4 has been
completed.
34
References
[1] L. Yang, R. Jin, Distance metric learning: A comprehensive survey, Tech. Rep. 24, Department of
Computer Science and Engineering, Michigan State University (2006).
[2] R. A. Fisher, The use of multiple measurements in taxonomic problems, Annals of Eugenics 7 (1936)
179–188.
[3] A. Bar-Hillel, T. Hertz, N. Shental, D. Weinshall, Learning a Mahalanobis metric from equivalence
constraints, Journal of Machine Learning Research 6 (2005) 937–965.
[4] M. Loog, R. P. W. Duin, Linear dimensionality reduction via a heteroscedastic extension of LDA:
The Chernoff criterion, IEEE Trans. on Pattern Analysis and Machine Intelligence 26 (6) (2004)
732–739.
[5] D. Xu, S. Yan, D. Tao, S. Lin, H. J. Zhang, Marginal Fisher analysis and its variants for human
gait recognition and content-based image retrieval, IEEE Trans. on Image Processing 16 (11) (2007)
2811–2821.
[6] M. Sugiyama, Dimensionality reduction of multimodal labeled data by local Fisher discriminant
analysis, Journal of Machine Learning Research 8 (2007) 1027–1061.
[7] K. Fukunaga, J. M. Mantock, Nonparametric discriminant analysis, IEEE Trans. on Pattern Anal-
ysis and Machine Intelligence 5 (1983) 671–678.
[8] F. Wang, C. Zhang, Feature extraction by maximizing the average neighborhood margin, in: Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2007, pp.
1–8.
[9] T. Hastie, R. Tibshirani, Discriminant adaptive nearest neighbor classification, IEEE Trans. on
Pattern Analysis and Machine Intelligence 18 (6) (1996) 607–616.
[10] K. Fukunaga, T. E. Flick, An optimal global nearest neighbor metric, IEEE Trans. on Pattern
Analysis and Machine Intelligence PAMI-6 (3) (1984) 314–318.
[11] S. Yan, D. Xu, B. Zhang, H. J. Zhang, Q. Yang, S. Lin, Graph embedding and extensions: A general
framework for dimensionality reduction, IEEE Trans. on Pattern Analysis and Machine Intelligence
29 (1) (2007) 40–51.
[12] E. P. Xing, A. Y. Ng, M. I. Jordan, S. Russell, Distance metric learning with application to clustering
with side-information, in: S. T. S. Becker, K. Obermayer (Eds.), Advances in Neural Information
Processing Systems 15, MIT Press, Cambridge, MA, 2003, pp. 505–512.
[13] S. Yan, J. Liu, X. Tang, T. S. Huang, A parameter-free framework for general supervised subspace
learning, IEEE Trans. on Information Forensics and Security 2 (1) (2007) 69–76.
[14] K. Q. Weinberger, L. K. Saul, Distance metric learning for large margin nearest neighbor classifi-
cation, Journal of Machine Learning Research 10 (2009) 207–244.
[15] R. Jin, S. Wang, Regularized distance metric learning: Theory and algorithm, in: Advance in Neural
Information Processing Systems, Vol. NIPS 23, 2009.
36
[32] R. Kohavi, G. John, Wrappers for feature subset selection, Artificial Intelligence 97 (1997) 273–324.
[33] C. C. Chang, Generalized iterative RELIEF for supervised distance metric learning, Pattern Recog-
nition 43 (2010) 2971–2981.
[34] T. M. Cover, J. A. Thomas, Elements of Information Theory, John Wiley & Sons, Inc., New York,
1991.
[35] A. P. Dempster, N. M. Laird, D. B. Rubin, Maximum likelihood from incomplete data via the EM
algorithm, Journal of the Royal Statistical Society. Series B 39 (1) (1977) 1–38.
[36] F. R. K. Chung, Spectral Graph Theory, Amer Mathematical Society, 1997.
[37] D. Hush, C. Scovel, I. Steinwart, Stability of unstable learning algorithm, Machine Learning 67
(2007) 197–206.
[38] T. Zhang, Covering number bounds of certain regularized linear function classes, Journal of Machine
Learning Research 2 (2002) 527–550.
[39] O. Bousquet, A. Elisseeff, Stability and generalization, Journal of Machine Learning Research 2
(2002) 499–526.
[40] A. Asuncion, D. Newman, UCI machine learning repository (2007).
URL http://www.ics.uci.edu/~mlearn/MLRepository.html
[41] U. Alon, N. Barkai, D. A. Notterman, K. Gish, S. Ybarra, D. Mack, A. J. Levine, Broad patterns of
gene expression revealed by clustering of tumor and normal colon tissues probed by oligonucleotide
arrays, Proc. Nat’l Academy of Science 96 (12) (1999) 6745–6750.
[42] D. Singh, P. G. Febbo, K. Ross, D. G. Jackson, J. Manola, C. Ladd, P. Tamayo, A. A. Renshaw,
A. V. D’Amico, J. P. Richie, E. S. Lander, M. Loda, P. W. Kantoff, T. R. Golub, W. R. Sellers,
Gene expression correlates of clinical prostate cancer behavior, Cancer Cell 1 (2002) 203–209.
[43] S. Ramaswamy, P. Tamayo, R. Rifkin, S. Mukherjee, C. Yeang, M. Angelo, C. Ladd, M. Reich,
E. Latulippe, J. Mesirov, T. Poggio, W. Gerald, M. Loda, E. Lander, R. Golub, Multiclass cancer
diagnosis using tumor gene expression signatures, Proc. Nat’l Academy of Science 98 (26) (2001)
15149–15154.
[44] Georgia Tech Face Database, http://www.anefian.com/face reco.htm.
[45] K. Lee, J. Ho, D. Kriegman, Acquiring linear subspaces for face recognition under variable lighting,
IEEE Trans. Pattern Anal. Mach. Intelligence 27 (5) (2005) 684–698.
[46] A. M. Martinez, R. Benavente, The AR face database, Tech. Rep. 24, CVC (June 1998).
[47] T. Hastie, R. Tibshirani, J. Friedman, The elements of statistical learning: Data mining, Inference,
and Prediction, Springer, 2001.
[48] R. O. Duda, P. E. Hart, D. G. Stork, Pattern Classification, Second Edition, John Wiley & Sons,
Inc., 2001.
[49] K. Fukunaga, Introduction to Statistical Pattern Recognition, second edition Edition, Academic
Press, Boston, 1990.
38
99 年度專題研究計畫研究成果彙整表 
計畫主持人：張欽圳 計畫編號：99-2221-E-019-036- 
計畫名稱：基於最短距離分類法與提昇法之監督式/半監督式學習 Mahalanobis 距離度量演算法 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 1 1 100%  
研究報告/技術報告 1 1 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 2 50%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
