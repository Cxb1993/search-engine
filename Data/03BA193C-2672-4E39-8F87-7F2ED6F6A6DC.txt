我們提出一個全域最佳化的方式來提升關聯結果。在產生物
體特徵時，由於在真實環境底下，沒有一個完美的特徵擷取
方法，無論何種方式都會有一定程度的雜訊干擾，若用傳統
的最近鄰居或是貪婪法，雖然可以確定該關聯結果自己有最
大可能性，但是不能保證從全域的觀點，所有的關聯是最佳
的可能性。我們提出的最佳化方式，是利用最小泥土搬運代
價(earth mover’s distance)的概念，針對每一個可能的配
對，從所有的組合找出最佳解。該方法達到關聯全域最佳
化，且它是在一般距離計算的上一個層級，所以它可以被各
種不同的物體特徵和距離計算方式所採用。實驗結果與傳統
的最近鄰居或是貪婪法相比，整體正確率平均提高 10.5%。
而且本方法計算量極低，在整個系統中是可被忽略，例如當
不同畫面中共有的物體數量平均為 26 個時，以 3GHz 的中央
處理器來模擬，計算速度可達每秒處理 3381 張畫面。 
本論文的第二部份包含利用非參數、參數化方法來實現無控
制器的遊戲平台，和用知識為基礎的遺棄行李的偵測系統。
我們非參數的方法選用以磁磚和移動向量為基礎的特徵，來
讓使用者用全身的動作去模擬排球和足球守門員的動作，進
而控制遊戲中的角色。選用磁磚和移動向量的原因主要是這
個特性被大量視訊壓縮技術所採用，也就是當一個攝影機系
統拍得畫面之後就會取得的資訊。而磁磚化的概念將地區性
的特徵給表現出來，即接近模擬人體四肢在不同的位置所呈
現的特徵。本方法與前人的時空樣板法相比較，平均來說效
果好 13%。時間序列參數法我們採用從粒子濾波器所取得之
人體關節的運動軌跡來當做動作描述子。每條軌跡先被轉換
成符號序列，然後採用兩種軌跡轉換符號方法和兩種表示方
式來進行實驗。最後結果顯示以固定時間長度的符號和符號
統計直方圖的組合，對於這些運動類，短動作的效果最好，
正確率平均達 96.7%，本結果亦比前面的非參數法好 7%。 
遺棄行李本身代表一個潛在的公共安全危機，尤其是炸彈式
的攻擊。要辨識出哪些是行李，哪些是擁有人，確認是否有
行李被遺棄，是遺棄行李的三個主要問題。然而，大多數解
決這類方法都將它表示成物體追縱的問題，而去追畫面當中
所有的前景物體，這造成現實即時應用上有著相當大的困
難。我們提出區域化選擇性追縱的概念，首先利用交集前景
取樣的結果來判斷哪裡可能是靜置物體，然後選擇性追縱該
物體周圍最近的人類。由於在監控環境攝影機是俯角拍攝，
人頭和肩被其他物體遮蔽的可能性最低，我們選擇頭肩輪廓
的特徵來判斷及追縱擁有人。最後結果在公共的測試資料上
均能偵測出行李遺棄的事件，和標準警報時間的差異約在-
2.36~+6.8 秒之間，採絕對值之後，平均差異為 2.7 秒。本
論文整合上述所有用在行為辨識系統上的核心模組，根據他
--- trajectory, and the second part considers the 
model formation of these scenarios. 
英文關鍵詞： Human action recognition, Human activity recognition, 
Feature extraction, Architecture 
 
I 
 
 
計畫名稱:  
智慧型多攝影機監控系統及架構研究 
 
 
以視覺為基礎之人類動作辨識的 
演算法及架構分析 
 
Algorithm and Architecture Analysis of Video-based 
Human Action and Activity Recognition 
 
 
 
 
 
 
 
 
 
III 
 
關節的運動軌跡來當做動作描述子。每條軌跡先被轉換成符號序列，然後採用兩種軌跡轉換符號方法
和兩種表示方式來進行實驗。最後結果顯示以固定時間長度的符號和符號統計直方圖的組合，對於這
些運動類，短動作的效果最好，正確率平均達 96.7%，本結果亦比前面的非參數法好 7%。 
遺棄行李本身代表一個潛在的公共安全危機，尤其是炸彈式的攻擊。要辨識出哪些是行李，哪些
是擁有人，確認是否有行李被遺棄，是遺棄行李的三個主要問題。然而，大多數解決這類方法都將它
表示成物體追縱的問題，而去追畫面當中所有的前景物體，這造成現實即時應用上有著相當大的困難。
我們提出區域化選擇性追縱的概念，首先利用交集前景取樣的結果來判斷哪裡可能是靜置物體，然後
選擇性追縱該物體周圍最近的人類。由於在監控環境攝影機是俯角拍攝，人頭和肩被其他物體遮蔽的
可能性最低，我們選擇頭肩輪廓的特徵來判斷及追縱擁有人。最後結果在公共的測試資料上均能偵測
出行李遺棄的事件，和標準警報時間的差異約在-2.36~+6.8 秒之間，採絕對值之後，平均差異為 2.7 
秒。本論文整合上述所有用在行為辨識系統上的核心模組，根據他們的運算特性及性質，設計高效率
的硬體架構和高準確度的演算法。可供後續研究者參考並延伸其性能和應用。 
 
關鍵字—人體動作辨識，人體活動辨識，特徵擷取，硬體架構 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
V 
 
higher than 87%. 
The third module is an object corresponding method. This work proposes a global-optimization approach 
of spatial object correspondence in distributed surveillance systems. All object correspondence systems need 
to be calibrated. However, since the environment is variable and differences exist between cameras, camera 
calibration may be imprecise and the results of the examination of the similarity between individuals may be 
incorrect. The concept of earth mover's distance (EMD) is employed in an environment under imprecise 
feature measurement. The approach solves the problem of mutually exclusive object correspondence; finds the 
global optimum; allows partial matches, and is able to be used in combination with others' approach of feature 
measurement. Global optimization is achieved by exploring all mutually exclusive match candidates and 
choosing those that generate the global minimum cost value. Applying EMD with a geometry-based feature to 
public surveillance datasets, the precision of the EMD-based method exceeds that of the greedy-based method 
by 4.3% to 20.8%, with an average of 10.5%. 
In the second part, a nonparametric approach and a parametric approach are adopted for controller-free 
gaming applications, and a knowledge-based approach is proposed for abandoned luggage detection systems. 
The nonparametric approach is a tile-based, motion-vector-based approach to provide a function, which 
allows people for using their whole body parts to mimic the real Volleyball/GoalKeeper actions to control the 
role in the game. The idea of introducing motion vector pattern to action recognition is based on the fact that 
video compression is now a common function of camera systems, which is an abundant source of motion 
vectors in the video. Motion vectors represent the dynamics information of an environment. By utilizing 
motion vectors in the region of an object, these dynamics information can be used to analyze actions of the 
object. The performance is compared with that of temporal-template-based approach. Because feature vectors 
of the temporal-template-based approach are generated by the entire foreground of one people, no regional 
information of the foreground is gathered. Therefore, the proposed motion-vector-based approach outperforms 
the temporal-template-based approach. Another approach for controller-free gaming applications is a 
parametric time-series approach using joint trajectories extracted by particle filter as the action descriptors. 
Each trajectory is converted into a symbol sequence. The action recognition is accomplished using all 
combinations of two distance measuring methods and two dictionaries completed by fixed-size or 
adaptive-size segments. 
Abandoned luggage represents a potential threat to public safety. Identifying objects as luggage, 
identifying the owners of such objects, and identifying whether owners have left luggage behind, are the three 
main problems requiring solution. However, in crowded areas, solutions based on identifying what all objects 
are and tracking all objects, based on the possibility of their being abandoned luggage, are computationally 
extremely costly. Accordingly, such methods are difficult to utilize in real-time applications. The 
knowledge-based approach uses two techniques for effectively detecting abandoned luggage. 
"Foreground-mask sampling'' detects luggage with arbitrary appearance and “selective tracking'' locates and 
tracks owners based solely on looking only at the neighborhood of the luggage. A probability model using the 
maximum a posteriori is adopted to generate a confidence score and determine whether luggage has been 
abandoned deliberately. Experimental results demonstrate that once an owner abandons their luggage and 
leaves the scene, the alarm fires within few seconds. The processing speed of the proposed approach is 
approximately 15 to 20 frames per second, which is sufficient for real world applications.  
Keywords-- Human action recognition, Human activity recognition, Feature extraction, Architecture 
3.3.2 Particle Filter Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
3.4 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
3.4.1 Evaluation of Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.4.2 Evaluation of Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
4 Multiple Camera Tracking 46
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
4.1.1 Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
4.1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
4.2 Earth mover’s distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
4.3 Spatial object correspondence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
4.3.1 Homographic location projection . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
4.3.2 Remove outlier objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
4.3.3 EMD-based object correspondence . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.3.4 Greedy-based object correspondence . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.4 Experimental results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
4.5 Summery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
II Human Action Recognition 60
5 Nonparametric Action Recognition 61
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
5.2 Motion-Vector-Based Action Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
5.3 Temporal-Template-Based Action Recognition . . . . . . . . . . . . . . . . . . . . . . . . 63
5.4 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
5.5 Summery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
6 Parametric Action Recognition 68
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
6.2 Joint-Trajectory-Based Action Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . 68
6.2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
6.2.2 Segment Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
6.2.3 Training Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
6.2.4 Matching Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
6.3 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
6.3.1 Testing Similar Actions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
6.3.2 Testing All Actions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
6.4 Summery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
7 Knowledge-based Action Recognition 76
7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
7.1.1 Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
7.1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
7.2 Foreground Mask Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
7.3 Selective Tracking Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
7.3.1 Cr Color Channel with Human Skin . . . . . . . . . . . . . . . . . . . . . . . . . . 82
2
List of Figures
1.1 E. J. Marey and E. Muybridge photographed moving subjects including humans and animals. 9
1.2 Point light displays attached to human body. . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.3 The semantic gap between low level features and high level features. . . . . . . . . . . . . 9
1.4 System architecture of video-based human action and activity recognition. . . . . . . . . 10
1.5 Example of 3D video volume. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.6 Example of the optical ﬂow feature. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.7 Example of the optical ﬂow feature. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
1.8 Example of foreground silhouette after background subtraction. . . . . . . . . . . . . . . 14
1.9 Example of spatio-temporal ﬁlter showing results of detecting spatio-temporal interest
points from the motion of the legs of a walking person. . . . . . . . . . . . . . . . . . . . 15
2.1 Two images have the same traditional histogram, but right one has much more gray com-
ponents in CSD description. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.2 CSD extraction ﬂow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.3 Block diagram of CSD architecture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.4 Pixel scan order of three structuring windows. . . . . . . . . . . . . . . . . . . . . . . . . 22
2.5 Structuring window histogram updating architecture. . . . . . . . . . . . . . . . . . . . . 23
2.6 Folding skill on non-linear quantization. . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.7 CSD Chip layout. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
3.1 Flow of dynamics modeling approach for human action recognition. The “tracking” module
tracks joints and generates trajectories. The “joint trajectory modeling” module extracts
speciﬁc features from trajectories. Finally, the “classiﬁcation engine” decides to which
category of actions each trajectory belongs by measuring the distance between extracted
features and features of all action models. . . . . . . . . . . . . . . . . . . . . . . . . . . 28
3.2 Sampling-importance-resampling particle ﬁlter. . . . . . . . . . . . . . . . . . . . . . . . 28
3.3 Processing ﬂow of particle ﬁlter. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
3.4 Comparison between runtimes of tracking phase and trajectory modeling phase. The
tracking phase is the computational bottleneck of the dynamic modeling approach. . . . . 34
3.5 Runtime proﬁling result. “Histogram accumulation” dominates the runtime of the particle
ﬁlter algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.6 Proposed four-parallel particle observation architecture. . . . . . . . . . . . . . . . . . . . 37
3.7 CAM architecture. “Virtual address” is deﬁned as the original accumulation address.
“Real address” is deﬁned as the actual address used to access the reduced-sized memory. 39
3.8 Hardware architecture scheduling using the ﬁrst class of parallel processing. The x-axis
represents time, and the y-axis represents hardware resources. “SW Gen.” stands for
spatial weight generation. “L1 Dist. Gen.” stands for L1 distance generation. . . . . . . . 40
4
5.4 Classiﬁcation results of the motion-vector-based approach. . . . . . . . . . . . . . . . . . 65
6.1 The trajectory modeling steps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
6.2 Example of smoothing trajectory. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
6.3 Illustrations of ﬁxed-size and adaptive-size segments. . . . . . . . . . . . . . . . . . . . . 70
6.4 The feature extraction and distance measuring steps of the parametric method. . . . . . . 71
6.5 Confusion matrix of all games in game “VB”. . . . . . . . . . . . . . . . . . . . . . . . . 74
7.1 System work ﬂow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
7.2 AVSS 2007 video dataset. Images captured via a typical surveillance camera are looking
down, causing the lower part of objects to appear larger and the upper part to appear
smaller. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
7.3 Foreground-mask sampling. The ﬁrst row shows input frames while the second row shows
corresponding foreground images. After obtaining the intersection result, two ﬁlters are
applied to remove certain static regions which are not the luggage items. One ﬁlter is
the human classiﬁer introduced in section 7.3, while the other removes unreasonably large
regions. Image on the right represents the intersection of the six foreground images sampled
over 30 seconds, and was obtained after removing non-luggage regions. . . . . . . . . . . 81
7.4 Left: input video frame with localized search region indicated by red circle. Right: the Cr
detection result within the search region. . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
7.5 Head-shoulder contour under HT. In template generation, the relative position of (xC , yC)
to (x, y) is recorded in (r, α); in contour matching, the pixel value on the assumed position
of (xC , yC) is incremented by 1 on the detection map. . . . . . . . . . . . . . . . . . . . . 83
7.6 The origin of the two red lines has a ψ angle with degree m, corresponding to the red,
m-th, bin which contains two (r, α) pairs. Solid lines denote correct matches, while dashed
lines represent noise. Diﬀerent points on the head-shoulder contour in the edge image
converge to a local maximum on the detection map on the right. At the bottom is the
180-bin reference table; a Gaussian weighting g is shown below the table, with the center
bin labeled in red and the neighboring ten bins in blue. . . . . . . . . . . . . . . . . . . . 84
7.7 Upper row: (left) input video frame; (middle) the input edge image obtained via 3 × 3
sobel convolution; (right) input frame with HT detection map superimposed on it using the
improved implementation method. Lower row: (left) simple implementation of normalized
correlation between the input edge image and contour template, which yields the most false
positives, as expected. (middle) detection result with original implementation of HT, which
gives more false positives; (right) HT detection result with the improved implementation
method, which yields a single large response at the correct location of the most visible
head-shoulder contour in the input edge image. . . . . . . . . . . . . . . . . . . . . . . . 85
7.8 Sequence 1, 3 and 4: (from left) static luggage detected; owner tracking starts; owner
leaves the scene, alarm triggered. Sequence 2: static luggage detected; owner tracking
starts; owner lost due to occlusion, alarm triggered. Sequence 1 and 2 are from AVSS 2007
dataset. Sequence 3 and 4 are from PETS 2006 dataset. . . . . . . . . . . . . . . . . . . 89
6
Chapter 1
Introduction
In recent years, video-based human action recognition technology provides important applications of com-
puter vision, such as multimedia entertainment, surveillance systems, interactive environments, content-
based video analysis, and behavioral biometrics. These potential applications attract many researchers
from academia and industry to explore the essence of recognizing human actions. The earliest attempt
to explore the nature of human motion was carried out by E. J. Marey and E. Muybridge in the 19th
century. They photographed moving subjects and showed several aspects involved in human and animal
locomotion. Figure 1.1 shows the sample images of their work. In psychology, Johansson studied the
human perception of movement by experimenting with moving light displays (MLD) attached to human
body parts [1]. Figure 1.2 shows an MLD example in [2]. He discovered that people can recognize diﬀerent
actions with only these moving dots without full structural information [3].
The major challenges of action recognition algorithms and systems lie in the semantic gaps between
human cognition systems and speciﬁc computation models, see ﬁg. 1.3. Eﬀective methods to model the
representation mathematically is a key to achieve intelligence of computers. Computer vision approaches
are ﬁrst adopted to cope with these challenges. The other approaches are machine learning and pattern
matching. Speciﬁcally, data mining and information retrieval techniques use the machine learning meth-
ods to identify interesting events given an abundant data or information. To combine the two approaches,
good modeling techniques are need to be discovered, which often involve feature selection and feature
matching.
Before discussing related works of video-based human action and activity recognition, the problem is
deﬁned as follow: Given video clips with an action is performed by a person or an activity performed
by several persons, how to design a system that can recognize what has been performed in these clips.
Is there a methodology to solve the problem eﬀectively? Conceptually, actions and activities has slight
diﬀerence by their literal meaning. That is, activities has higher semantic meaning then actions. An
activity may results from multiple actions. For example, the activity of ‘graﬃti’ may results from actions
‘walking toward the wall’, ‘preparing the paints’, and ‘drawing’. Or, the activity of ‘left luggage’ may
results from actions ‘walking’, ‘not moving’, ‘left bag’, and ‘abandoned bag’. Several works have appeared
over the years. Aggarwal and Cai [4] addressed three problems in a complete action recognition system
— feature extraction of human body from images, tracking across frames, and action recognition. Cedras
and Shah [5] presented a survey on motion-based approaches and claimed that motion is an important
cue for action recognition than the structure of the human body. Gavrila [3] presented a survey focused
mainly on tracking of hands and humans via 2D or 3D models, and a discussion of action recognition
techniques. From these works, the general human action recognition systems can be integrated into
8

	

%

	

&	
 	
%
#'(	
 "
)	
*+
,	
	

-	
	

	

 	

.
%	/	
%	'+	/	0	.
Figure 1.4: System architecture of video-based human action and activity recognition.
the system architecture shown in Fig. 1.4. Basically, each functional block outputs speciﬁc information
and each output information can be adopted solely for one method of human action recognition. The
categories of human action recognition methods will be presented in the next section. Accordingly, the
thesis will include the related core processing modules for extracting useful features for human action
recognition and the several recognition methods adopting diﬀerent features.
1.1 Categories of Human Action/Activity Recognition
Behavior analysis can be classiﬁed into two classes — “action” and “activity” [6]. “Actions” mean simple
motion patterns usually executed by a single person and typically lasting for short durations of time, on the
order of a few seconds. The applications of “action recognition” includes human-computer interfaces, such
as game controllers and the interaction with appliances in intelligent buildings. “Activities” means the
complex sequence of actions performed by several humans and objects that can interact with each other
for longer durations. Because of the deﬁnition of “activities”, the applications of “activity recognition”
mainly focus on surveillance systems and team sport analysis.
Approaches to automatically analyzing “actions” can be classiﬁed into three major classes — non-
parametric, volumetric, and parametric time-series approaches. Nonparametric methods extract features
from each frame of the video. The features are then matched to a stored template. Volumetric approaches
consider a video as an image-time volume of pixel intensities and extend standard image features such
as spatial ﬁlter responses to the 3D volume. Parametric time-series approaches build a model on the
temporal dynamics of the motion. The parameters of actions are then estimated from training data.
Examples of parametric approaches include hidden Markov models (HMMs) and linear dynamical sys-
tems (LDSs). On the other hand, “activity recognition” often adopts approaches of describing relations
between conditions and events, or relies on formal logical rules of domain knowledge to detect activities.
In Part II, a nonparametric and a parametric approaches are proposed for action gaming systems, based
on the features of motion vectors and trajectories accordingly. A knowledge-based approach to detect
abandoned luggage for public security is proposed in Chapter 7. The following subsections collect several
10
Figure 1.5: Example of 3D video volume.
two paths in brain for visual processing [13]. Their system is divided into several stages. The input
video is ﬁrst matched by spatio-temporal ﬁlters. Later, in each stage, a ﬁlter (optical ﬂow, gradient
or orientation) is applied and responses at local maxima points are captured, followed by a template
matching stage. The ﬁnal classiﬁcation uses multi-class SVM as the classiﬁer.
1.1.2 Parametric Methods
Time-Series-Model-Based Approaches
Temporal correspondence is a critical modeling step in human action analysis. Several models such
as Bayesian network, hidden Markov Models (HMM), or Markov networks, can be adopted to build
the temporal correspondence. HMM has been used to model a sequence of symbols and the temporal
relationship in the ﬁeld of speech and audio related researches. The concept can also be used to model
human action by properly ﬁtting features to symbols. Bregler tackled a human dynamics recognition
problem using a probabilistic compositional framework with HMM [14]. He proposed a multilayered
approach. The ﬁrst level is a sequence of input images. The next level consists of blob hypotheses where
each blob is a region of coherent motion. At the third level, blob tracks are grouped temporally. The ﬁnal
level consists of an HMM for representing the complex behavior. Recently, Markov-random-ﬁeld-based
and conditional-random-ﬁeld-based methods have also been proposed. Natarajan and Nevatia combined
the idea of event level and template level [15]. The template level consists of edge templates, and the
event level are modeled by a conditional random ﬁeld.
Dynamics-Modeling-Based Approaches
Another category to modeling human dynamics originates from psychological studies, which is discussed
at the beginning of the chapter. With the aid of human motion capture system, these joint movement data
can be obtained by special optical devices or sensors. Accordingly, joint-movement-modeling approaches
have been proposed. Ali et al. modeled the human movements of six joints by chaotic theories [16].
Assuming the trajectories of the joint can be obtained, the phase space embedding is ﬁrst applied and
chaotic invariants such as maximal Lyapunov exponent, correlation integral and correlation dimension
are adopted to model diﬀerent types of joint movements generated by diﬀerent actions. Raptis et al. also
solved the problem of action recognition in this way [17]. They quantized the continuous trajectories into
discrete labels by the aid of window dictionary, and classiﬁed their results by bagging or string matching
algorithms.
12
(a) Pedestrian ﬂow analysis. (b) 2D human pose tracking.
Figure 1.7: Example of the optical ﬂow feature.
Figure 1.8: Example of foreground silhouette after background subtraction.
1.2.2 Trajectories
Trajectories of moving objects have been used as features to infer the activity of the object, such as the
ﬂow analysis of traﬃc monitoring. The trajectories of joints of living creatures can also be seen as the
extracted feature in Fig. 1.2 for action recognition. Because trajectories are sensitive to translations,
rotations, and scale changes, alternative representations such as trajectory velocities, speeds, spatio-
temporal curvature, relative motion, etc., have been proposed while they are invariant to some of these
variabilities. A good tracking algorithm should be robust under the inﬂuences, such as occlusions, noise,
and background clutter. Figure 1.7 shows two examples of trajectories of pedestrians and human joints.
Figure 1.7(b) from [20] shows a spatio-temporal 2D models framework for 2D-pose tracking.
1.2.3 Foreground Silhouettes
Foreground silhouettes mostly are obtained by background subtraction or frame diﬀerentiation methods
with a static camera. Figure 1.8 is an example of background subtracted image showing a person is going
to block an incoming soccer. The shape of the human silhouette provides useful information in human
action recognition. Several methods using global, boundary, and skeletal descriptors have been proposed
to quantify silhouettes. Global methods use moments of a silhouette; boundary methods describe a
silhouette using its contour; skeletal methods use a set of 1D skeletal curves to represent a silhouette.
14
and chapter 6 presents a joint-trajectory-based action recognition for controller-free gaming applications.
Chapter 7 presents a knowledge-based action recognition focusing on abandoned luggage detection sys-
tems. Finally, chapter 8 concludes this dissertation.
16
Chapter 2
Object Descriptor
2.1 Introduction
With mature digital video technology, inexpensive camcorders gradually enter our life. More and more
multimedia are produced and shared among the world. Original intention of MPEG-7 is to provide a
powerful search engine which helps people easily ﬁnd what they are looking for. Some MPEG-7 toolkits
further integrate useful functionalities for categorizing and organizing their personal collection. However,
some related research [22] showed that most people only categorize their albums at semantic level, but
the recognition technique nowadays is still not able to meet this kind of demand. MPEG-7 descriptors
are good tools for indexing and retrieval but should not be limited to them. MPEG-7 descriptors can
be creatively extended and linked to applications such as rate control in real-time video coding and
movement detection in surveillance systems. In these applications, computational loads of the real-time
implementation for these descriptors will not be a trivial issue.
With statistics derived from MPEG-7 descriptors, good indication of image and video properties can
provide referable adjustment parameters for video pre-processing like auto white balance, RGB gains
tuning, saturation control, auto contrast, and edge enhancement. In video coding, it can assist fast
algorithm of motion estimation, rate control policy, probability distribution model of entropy coding,
and so on. When we use them in surveillance system, the system can notice police to keep an eye on
unusual behavior by analyzing object trajectory. Face descriptor can also provide auto identiﬁcation of
uncertiﬁed people in certain degree.
MPEG-7 visual descriptors record statistics of images and video sequences in color, texture, shape of
objects, and motion. Because the variety of possible applications, we ﬁrst take implementation of color
descriptors as our start point. Color is one of important visual attributes for human vision and image
processing. It is also an expressive visual feature in image and video retrieval. Color descriptions usually
are irrelevant to viewing angle, translation and rotation. This advantage possesses good resistance to
undesired shacking of camera. In MPEG-7, six descriptors are selected to record color statistics of images
and video. Among them, CSD provides best image indexing and retrieval results [23]. The superiority
comes from that CSD considers space distribution of pixel colors by recording appearance of each color
in every structuring element window in its histogram [24]. In this work, we focus on the architecture and
analysis of CSD.
The challenge to realize CSD hardware accelerator for real-time video system is that each pixel in a
frame needs to be scanned 64 times. The vast data bandwidth and then excessive operating frequency
18
	

Figure 2.1: Two images have the same traditional histogram, but right one has much more gray compo-
nents in CSD description.

	


	
	
	
		
	
 	
		
 !"#

$
%&'!(')"#
#
#

*	+
Figure 2.2: CSD extraction ﬂow.
color bins by only adding one, no matter how many same color pixels exist. Figure 2.1 shows that two
images have diﬀerent CSD description with the same traditional histogram [25]. Right image looks more
scattered than left one. Such situation causes gray pixels exist in more SWs and reﬂects on gray bin in
CSD description. This advantage let us easily distinguish those images with similar dispersion.
Figure 2.2 depicts CSD extraction procedure [26]. Our design chose highest number of bins for more
precise CSD description in real-time applications. The top path directs the ﬂow of 256-bin CSD. It starts
with color transformation from RGB to HMMD. Next step is histogram accumulation which is followed
by a decision of number of bins needed. After a nonlinear quantization, CSD description is derived.
2.3 Computational Complexity and Architecture Design
As described in Section 2.1, we focus on real-time applications of MPEG-7 like video coding assistance
and surveillance systems. Besides, generated CSD descriptions still can be used for search of multimedia
contents. And for supporting comparison with descriptions generated by other tools, 256 levels of color
quantization is adopted for down-scale comparison.
Since a sub-sample factor is deﬁned in the standard for large images, we choose 256 × 256 as input
image size. The sub-sample factor, K, is deﬁned as K = max{1, 2log2
√
W·H−7.5}, where W and H are
the width and height of image. For example, K = 2 implies an image is sub-sampled by 2 horizontally
and vertically. Note that the SW size is always 8× 8.
Our CSD block diagram is shown in Fig. 2.3. After color transformation, pixels are sent to correspond-
ing local histogram observing (LHO) blocks and index colors that exist in these windows. Summation
20
Table 2.2: Relationship between parallelism and operating frequency. Zero parallelism means no SW is
buﬀered. The minimum requirement to meet target frequency (27 MHz) is three parallelism.
Parallelism MB/s MHz
0 357 476
1 46 61
2 26 35
3 19 25
4 16 21

0123 012% 012
-
-
-
)
Figure 2.4: Pixel scan order of three structuring windows.
22
++
#
8	
%&
8	
999
999
999
999%:
!(
$

4	5 4"5
Figure 2.6: Folding skill on non-linear quantization.
else HUE = 240 + 60 x (R - G) / DIFF;
}
The most time consuming and area occupied part of this formula is evaluation of hue value. If the
divider were directly mapped into hardware, it would require high precision and set a bottleneck of the
chip. Since dividend and divisor are 8-bit integers and ﬁnal output of hue is quantized into 16 categories,
two drawbacks just mentioned would be eliminated by building a mapping table for hue evaluation. After
synthesizing direct divider architecture and mapping table, the latter saves 36% area.
2.3.5 Non-Linear Quantization
After CSD histogram accumulation is ﬁnished, non-linear histogram quantization is the ﬁnal step. Each
bin should be quantized into 8-bit via 255 comparisons. With binary comparison method and folding
skill, eight comparisons are needed to quantize one bin. This strategy is shown in Fig. 2.6. As shown
in (a), we compare the bin with center value of valid range each time. Since the latency of non-linear
quantization, which is compared with CSD histogram accumulation, is negligible, 255 comparators can
be folded into one. With (b) architecture, 2048 (256×8) cycles and one comparator are needed to achieve
this work.
2.4 Experimental Result
Our indexing and retrieval database contains 526 images in 78 categories. Those images are collected
from Internet and manual categorized. Furthermore, for extending the concepts of these descriptors to
image and video coding, we replace default color spaces with YCbCr domain and the performance drops
slightly.
Here we use a quantitative measure method called query-by-example (QBE) suggested by MPEG-
7 [25]. QBE sorts the distances between description vector of query image and those of images contained
in a database. The smaller average normalized modiﬁed retrieval rank (ANMRR) means the descriptor
provides better indexing and retrieval ability.
Table 2.3 shows the indexing and retrieval results of CSD and scalable color descriptor (SCD) with
designated and YCbCr color spaces. SCD listed here is for comparison. The results with YCbCr are
also acceptable and imply that we can apply the concepts to the ﬁeld of image and video coding which
chooses YCbCr as default color space.
24
Table 2.4: Chip speciﬁcation
Technology UMC 0.18 μm CMOS 1P6M
Core size 1.36950× 1.36584 mm2
Gate count 49865
On-chip single port SRAM 11136 Bits
Max frequency 31.25 MHz
Operating frequency 27 MHz
Processing speed 256× 256, 30 fps@ 27 MHz
Power Consumption 39.53 mW@ 27 MHz, 1.8 V
26
Tracking
Joint Trajectory 
Modeling
Classification
Engine
Video
Frames
Figure 3.1: Flow of dynamics modeling approach for human action recognition. The “tracking” module
tracks joints and generates trajectories. The “joint trajectory modeling” module extracts speciﬁc features
from trajectories. Finally, the “classiﬁcation engine” decides to which category of actions each trajectory
belongs by measuring the distance between extracted features and features of all action models.
	1	
-
 $
"
Figure 3.2: Sampling-importance-resampling particle ﬁlter.
models.
Many tracking algorithms have been proposed to track targets eﬀectively from video frames. Among
these, tracking algorithms based on a Bayesian framework, such as the Kalman ﬁlter [32], the Extended
Kalman ﬁlter, the particle ﬁlter and many others, are popular because of their probabilistic nature.
Probabilistic approaches for tracking can easily escape from local minima, and are more capable of
dealing with environmental clutters [33].
Isard et al. originally developed the particle ﬁlter algorithm [34]. Figure 3.2 depicts a typical sampling-
importance-resampling (SIR) particle ﬁlter. The main concept of the particle ﬁlter is to approximate the
posterior distribution of states using a set of particles and Monte Carlo methods. Since estimating the
posterior distribution directly is commonly impossible, such an approximation not only makes the algo-
rithm tractable but also improves tracking performance. Subsequently, several approaches to targeting
particle-ﬁlter-based tracking have been proposed [33,35–38]. Such tracking systems use various features,
and successfully track objects in many scenarios. Among these features, the color histogram is one of the
most used because of its simplicity [33, 38–40].
Despite its success in vision-based object tracking, the particle ﬁlter algorithm requires high compu-
tation, mainly because of its probabilistic and highly order-dependent nature. Also, as the objects to be
tracked become more complex, the number of particles required to track accurately increases. Therefore,
in the provision of vision-based UI based on dynamics modeling approaches for gaming systems, an ef-
ﬁcient algorithm and an architecture design to run the algorithm in real time are required. This work
focuses on the realization of a particle ﬁlter tracking module.
28
3.2.1 Formal Deﬁnition
The goal of a Bayesian tracking framework is to estimate the state xt of a tracked object at each time t
from current and previous observations, z1:t, and the previous state xt−1. The two models that describe
how a system evolves with time are the dynamic model and the observation model. A dynamic model
xt+1 = f(xt,vt) describes how states evolve, where vt is the noise that is introduced to the model. An
observation model zt = h(xt,nt) describes the relationship between current state xt and observation zt,
where nt is the noise in the observation.
Given a dynamic model and an observation model, the Bayesian tracking framework works as follows.
Following proper initialization of the state variable xt, at each time t, the state variable is initially
predicted by
p(xt, z1:t−1) =
∫
xt−1
p(xt|xt−1)p(xt−1|z1:t−1) (3.1)
where z1:t−1 is the observation from time 1 to time t− 1. After the observation at time t, zt, is obtained,
and the ﬁt of the predicted state with the observation is given by
p(xt|z1:t) = p(zt|xtz1:t−1)p(xt|z1:t−1)
p(zt|z1:t−1) (3.2)
The particle ﬁlter algorithm approximates the posterior distribution with a set of particles:
p(xt|z1:t) ≈
∑
i
w
(i)
t δ(x− x(i)t ) (3.3)
where w
(i)
t and x
(i)
t denote the weight and the state of particle i at time t. The particle weight w
(i)
t
speciﬁes how the estimated state of this particle matches the real state, and can be computed from the
observation model.
3.2.2 Second Order Dynamic Model
The constant velocity assumption that is made by most particle ﬁlters is not always applicable in practice.
Rather than simply estimating the velocity from the position shift, the proposed model additionally
estimates the acceleration of the object. In the following, pt = {x, y}t represents the position; qt =
{vx, vy}t denotes the velocity, and rt = {ax, ay}t stands for the acceleration of the target object at time
t. The original forms of the equal-acceleration equations are{
qt = qo + rt
pt = qot+
1
2
rt2
(3.4)
Equation (3.4) can be transformed into recursive form,{
p˜t = pt−1 + qt−1 + 12r
q˜t = qt−1 + r
(3.5)
where p˜t and q˜t are predicted from pt−1, qt−1, and r.
When the estimated position pt has been obtained by observation, qt and r at the corresponding
frame must be estimated. Solving the dual equation equations (3.5) yields{
qt = [2(pt − pt−1)− qt−1]
r = [2(pt − pt−1)− 2qt−1]
(3.6)
30
weights are known, the state is computed as the weighted sum of all particles.
xt =
∑
i
w
(i)
t x
(i)
t (3.9)
A maximum a posteriori (MAP) method is implemented in hardware architecture to estimate states:
xt = argmaxx(i)t
p(xt|z1:t,x(i)t ) (3.10)
The performance of the MAP method in this case is not evaluated, but this alternative is provided for
system developers. After the state estimation stage, the resampling algorithm [48] is implemented to
reduce the eﬀects of particle degeneracy.
The size and the angles of orientation of the tracked object are updated following the above steps, by
estimating the size of the object using the MAP criterion. The selected nine candidate sizes are given
by [W −ΔW,W,W +ΔW ]× [H −ΔH,H,H +ΔH], where ΔW and ΔH are parameters to specify the
increase of the object size. To estimate the angles of orientation of the object, the angle is quantized into
one of 32 bins that uniformly span [0, π). For each candidate angle, the color histogram is calculated and
the ﬁnal angle that has the minimum distance to the histogram of the object in the previous frame is
selected. After all six steps have been completed, the algorithm proceeds to the next frame.
Notably, in the current implementation, the estimation of the size and the angles of orientation of the
object are separated from that of its position and velocity, because deviation in size and angle does not
inﬂuence the tracking result. Meanwhile, the optimal combination of size and angle for each particle is
found by making 9×32 = 288 estimates for one position, which represents an excessive cost of architecture
design1.
3.3 Hardware Architecture
3.3.1 System Runtime Analysis
A system runtime analysis is conducted to identify the functions of the particle ﬁlter that require a large
computational load and thus need to be accelerated to enable real-time operation.
Our gaming application involves two full-body action games - “GoalKeeper” and “Volleyball”. Gamers’
actions are captured using a video camera at 30 fps with D1 frame size. Table 3.1 presents the runtime
of the tracking module on a PC with an Intel CoreDuo 2.66 GHz CPU and 2 GB memory for the test
sequences in “GoalKeeper.” This table gives the tracking time of a speciﬁc target in a single frame. The
objective is to track three targets (head, left hand, right hand) simultaneously to identify actions. The
runtime can also be measured in fps, and the last column of the table presents the overall fps joining the
three targets. This table reveals that the tracking module can only operate at 5 to 6 fps. Consequently,
the operating speed of the tracking module cannot satisfy real-time requirements even on a CPU with
such a high working frequency.
As well as tracking, joint trajectory modeling is another important component of dynamic modeling.
The purpose of joint trajectory modeling is to model the movement of various body parts to classify
human behaviors. In our system, the body joint trajectories are generated using a particle ﬁlter algorithm.
The method follows trajectory modeling steps presented elsewhere [17]. The concept of joint trajectory
modeling is to transform continuous high dimensional data into discrete symbols (words) using a symbol
1The cost of computing an estimate is similar to that of computing a “particle observation” in Fig. 3.3.
32

 
 






	

	 	!
"#$	%&	!

  




'() '(* '(+ ,)() ,)(-&
Figure 3.4: Comparison between runtimes of tracking phase and trajectory modeling phase. The tracking
phase is the computational bottleneck of the dynamic modeling approach.
system must be hardware-accelerated. In contrast, joint trajectory matching can be performed easily in
real-time using software solutions. Accordingly, a hardware architecture design for the tracking module
is proposed.
3.3.2 Particle Filter Architecture
Most particle ﬁlter algorithms are computationally costly. The software implementation herein can only
support up to 8 fps on a computer with an Intel Core Duo 2.66 GHz CPU and 2 GB RAM. Although
several works have focused on the parallelization of the resample, the proﬁling results, displayed in Fig.
3.5, are consistent with those proposed elsewhere [33, 45], revealing that histogram accumulation and
comparison consume over 90% of the operation time. Therefore, in architecture design, the acceleration
of the histogram accumulation and calculation is emphasized.
Design Challenge
The main challenge of the architecture design is to deliver real-time performance. If M targets to be
tracked, each target requires Np particles, and the video is assumed to run at fs fps, then the processing
time of each particle is 1/(NpMfs) seconds. Also, the number of cycles required to complete the histogram
accumulation is proportional to the size of the object. If the number of cycles needed to accumulate one
histogram exceeds the processing time, calculated above, then real-time performance cannot be achieved.
For example, suppose the requirement is to perform real-time tracking with 30 fps video of six targets
using 256 particles per target. If the cycle length is 5 ns, then each particle has approximately only 4, 340
cycles to process SIR steps. Notably, 5 ns cycle length is a reasonable estimate in current digital signal
processing (DSP) or application-speciﬁc integrated circuit (ASIC) design. Furthermore, in this case, the
sizes of the targets are 128 × 128 at most, requiring 16, 384 cycles to accumulate a single particle. The
needed number of cycles exceeds the cycle limit calculated above, and at least four-parallel operations
are necessary.
34
Table 3.3: Word length of each variable in hardware.
Variable #Bits
Gaussian Input 4.18
Spatial Weight Sum 12.12
Color Variance 3.12
Particle Weight Sum 8.12
Exponential Table Entry 1.12
Normalized Spatial Weight 1.17
Cosine Table Entry 2.14
Normalized Particle Weight 1.26
of major variables that are used in the ﬁnal implementation. The number after the dot is the number of
bits used to represent the fractional parts of the variables.
Particle-level Parallel Processing
To deliver real-time performance, the concept of particle-level parallel processing is proposed and applied
in the hardware architecture. Speciﬁcally, the mathematical operations in Eq. (3.7) and the algorithm
described in Section 3.2 are such that, during the histogram accumulation stage, the operations undergone
by the particles are mutually independent. The parallelization parameter P must ﬁrst be evaluated
from the constraints on number of cycles and processing time. Two parallel processing schemes and
architectures are proposed here.
The ﬁrst parallel processing scheme constructs the color histograms of particles in parallel. In the
sampling stage, the operations of the particles are mutually independent, allowing multiple particles to be
processed simultaneously, as in particle-level parallel processing. After the propagation stage, the color
histogram of each particle can be constructed independently without interference from any other. Figure
3.6(a) displays the hardware architecture of the basic implementation. The current position generator
generates the address of the frame data required for histogram accumulation, in raster scan order and
one pixel at a time. The position ﬁlter retains only those addresses that are actually required for the
accumulation, and fetches the frame data. The spatial weight PE generates spatial weights according
to Eq. (3.7), and the SRAM stores and accumulates color histograms. Figure 3.6(b) shows example
hardware architecture for a four-parallel scheme. Notably, if a P -parallel scheme is required, only P
copies of the “particle observation” have to be used — instead of the whole module.
The second class of parallel schemes is based on pixel-level and particle-level parallel processing. For
example, each current position generator generates two pixel positions. The spatial weights of the two
pixels are calculated at the same time. The dual-port SRAM accumulates two bins of the color histogram
simultaneously. The architecture is like that in Fig. 3.6(a), but dual port SRAMs are adopted instead of
single port SRAMs.
36
Table 3.4: Number of used bins.
Dataset Target Avg. #Bins Peak #Bins
GoalKeeper
Left Hand 19.57 67
Right Hand 18.12 57
Volleyball
Left Hand 13.95 44
Right Hand 12.46 39
Content Addressable Memory Mapping
The comparison of the color histogram of the target appearance model and that of the candidate appear-
ance model requires large memory. The size depends greatly on the number of histogram bins. Although
the proposed prioritized ﬁnite word length analysis can eﬀectively ﬁnd the word length of the memory,
a direct implementation still requires a large on-chip memory. For example, consider a color histogram
consists of a YUV color space that is quantized into 8 × 8 × 8 = 512 bins, and suppose each bin is
12 + 12 = 24-bit. If the parallelization parameter P is 4, then a 48 Kbits memory is required to accu-
mulate the histogram. The utilization of the histogram bins is considered, and the results presented in
Table 3.4, whose third column shows the average number of used bins for all particles, and whose fourth
column shows the peak number of used bins. The results indicate that the actual number of used bins
is well under 512 in the quantized YUV color histogram. The reason for this diﬀerence is that tracked
targets typically have few colors. Therefore, most of the memory is wasted if the direct memory scheme
is adopted for histogram accumulation.
Based on proper understanding of the cause of this waste, CAM is adopted to increase memory
utilization and reduce memory size and chip area [46]. The idea is to use a reduced-size memory to store
the spatial weights, as CAM bridges the reduced-size memory and the original accumulation address. In
the following, the original accumulation address is called the “virtual address”, and the actual address,
used to access the reduced memory, is called the “real address”. Figure 3.7 depicts the architecture of
CAM and the associated terms.
CAM records the access address of the memory as the virtual address; allocates a slot to the real
address, and constructs a map between the new virtual address and the allocated real address. If another
access to the memory using the virtual address is required, then CAM looks up its mapping table to
determine whether the virtual address has been stored. If it has, the virtual address will be translated
into a real address. Otherwise, a new entry is produced, as before. The size of CAM can be chosen
with reference to the empirical data, such as the peak usage or the average usage in Table 3.4. CAM
can greatly reduce the memory requirement. For example, if 128 bins are chosen according to the peak
number of bins from the above examples, then the memory can be reduced to 25% (128/512). Since the
area overhead of the CAM circuit is typically less than 5% of the memory area, the adopted technique
can substantially reduce memory area and reduce chip cost. Notably, that the number of existing bins
may exceed the number of designed bins. This situation is not considered here. Were such a case to arise,
the new bin would be ignored.
38
 	
$*&
 
-	A3	2
2	=


$*&
 	
$*&
 
-	A> 2


$*&

3
=	
=
 	
A3	
3
=
 	
A> 
3
=
-
2
 	
A3	2
 	
A4	2
 	
A5 2
 	
A6 2
-B6


Figure 3.8: Hardware architecture scheduling using the ﬁrst class of parallel processing. The x-axis
represents time, and the y-axis represents hardware resources. “SW Gen.” stands for spatial weight
generation. “L1 Dist. Gen.” stands for L1 distance generation.
3.4.1 Evaluation of Algorithm
To ensure that particle ﬁlter tracks speciﬁc targets correctly, ﬁve body parts - head, hands (two), and
knees (two) - are tracked for each test sequence. Since generating the ground truth of the target positions
of entire dataset or even for only a single test sequence is extremely laborious, and our applications herein
involve action recognition, and do not require highly precise location, the success rate rather than the
positional deviation is employed as an index of the tracking performance. A successful tracking is one
in which the tracker follows a speciﬁc target until the end of a test sequence, meaning that the tracker
still overlaps the target in the last frame. Table 3.5 shows the results. Some actions such as “GK Kick”,
“VB Setting”, and “VB Spike” are not tracked well because the target is occluded for a long period.
The targets in such cases are out of the frame or behind the body parts of the subject for more than
ten successive frames. However, we assert that the overall success rate is suﬃciently high for action
recognition.
Following analysis of the prioritized ﬁnite word length in architecture design, to guarantee that the
ﬁxed-point version does not have diﬀerent tracking results, the cases that were successful with ﬂoating-
point variables are tested again with ﬁxed-point variables. The tracking results remain the same, so the
success rate is as in Table 3.5.
3.4.2 Evaluation of Architecture
To demonstrate the eﬀectiveness of the proposed architecture, the four-parallel scheme is implemented
with single port SRAM. The chip is synthesized at 5 ns by UMC 90 nm process. Table 3.6 presents
the chip speciﬁcations and Fig. 3.10 depicts the layout. In this prototype, 64 bins are implemented in
the CAM mapping. For the four-parallel scheme, a target model, and a resized model, the SRAM uses
64× 24× (4 + 1 + 1) = 9216 bits. A comparison with the original quantized YUV color space with 512
bins indicates that the memory is reduced to 12.5% of its original size.
Figure 3.11 compares the frame rates of the software and hardware implementations of single-target
tracking, with a hardware cycle time of 5 ns. The nfps is the fps normalized to the ratio of object size
to 128 × 128. Since the software frame rate is not normalized, the minimum acceleration ratio between
right-most column and left-most column for each object is approximately 132. The minimum normalized
frame rate also indicates satisfactory real-time performance even when the target is as large as 128×128.
40
Table 3.5: Success rate of tracking using particle ﬁlter
Action Type #Cases #Successes Success Rate
GK Blocking 135 123 91.1%
GK Down 60 53 88.3%
GK Header 160 156 97.5%
GK Kick 145 110 75.9%
GK SingleHand 193 193 99.0%
VB Setting 150 124 82.7%
VB Spike 185 128 69.2%
VB Underhand 160 157 98.1%
Overall 1,190 1,044 87.7%
Table 3.6: Hardware speciﬁcations
Technology UMC 90 nm Logic & Mixed-Mode 1P9M Low-K
Chip Size 1.991× 1.883 mm2
Logic Gate Count 152K
On-Chip Memory 9, 216 bits
Working / Max. Freq. 125 MHz / 200 MHz
Power / Max. Power 185.2 mW / 296.32 mW
42
3 .3
 
.7
 
8 .8
 
9.
2 82
.0
 
89
.5
 
92
.1
 
58
.9
 
60
.2
 
39
.5
 
1,
49
8.
9 
3,
75
8.
8 
4,
33
5.
2 
2,
60
8.
2 
3,
70
7.
3 
6,
22
4.
1 
Software fps/target Hardware nfps/target (5ns) Hardware fps/target (5ns)
11
.3
 
18
.3
 
18
.7
 
14
.8
 
21
.8
 
29
.2
 82
.0
 
89
.5
 
92
.1
 
58
.9
 
60
.2
 
39
.5
 
1,
49
8.
9 
3,
75
8.
8 
4,
33
5.
2 
2,
60
8.
2 
3,
70
7.
3 
6,
22
4.
1 
Head Left hand Right hand Head Left hand Right hand
GKBlockC01 GKHeaderR01
Figure 3.11: Comparison of runtimes of software implementation and hardware implementation.
Table 3.7: Frame rate of software and hardware implementations
Sequence Target SW fps HW fps Gain
GKBlockC01
Head 11.34 1,499 132
Left hand 18.34 3,759 205
Right hand 18.67 4,335 232
GKHeaderR01
Head 14.76 2,608 177
Left hand 21.77 3,707 170
Right hand 29.16 6,224 213
“SW” means “software” and “HW” means “hardware”. The hardware cycle time is 5 ns. This table
indicates the minimum gain between hardware and software implementations is 132.
44
Chapter 4
Multiple Camera Tracking
4.1 Introduction
As demand for intelligent living environments and personal safety market grow, surveillance systems are
becoming larger. Nowadays, a typical distributed surveillance system contains multiple cameras in a
network, several local storage servers that are connected to the cameras, and a control side which ad-
ministrators can access all of the recorded clips. Fig. 4.1presents such a system. Distributed surveillance
systems monitor an area from various views, integrate information from all cameras, and increase the
probability of capturing clear biometric features, such as faces and gaits. Although such a system has
many advantages, the excess of provided information causes diﬃculties with tracking people or searching
for events without automatic association between data from diﬀerent cameras. To associate diﬀerent
views of the same event, object correspondence (also known as consistent labeling [49]) is essential. Fig.
4.2 presents the concept of object correspondence. Object correspondence refers to the fact that if some
foreground instances represent the same object, then they are associated by temporal object correspon-
dence (TOC, which is similar to multiple-object tracking in one camera) at diﬀerent times, and are
associated by spatial object correspondence (SOC) among diﬀerent views.
Object correspondence is a diﬃcult topic because un-calibrated or imprecisely calibrated cameras
commonly capture inaccurate object features, making optimal object correspondence hard to achieve.
Imprecise calibration is caused by diverse intrinsic/extrinsic camera parameters, such as white balance,
lighting condition, color contrast, and viewing angle. The same object captured in diﬀerent views always
exhibits very diﬀerent characteristics. Imprecise calibration is also evident in the geometric projection
between a known world coordinate system and the imaging plane coordinate system in the camera.
The radial distortion of the camera lens, like barrel and pincushion distortion, and the low angle of
depression of the camera, are two sources of such imprecise calibration. Object occlusion is another cause
of degradation of the association quality since discriminating among all objects in a group is diﬃcult.
4.1.1 Related Works
The deployment of the cameras in distributed surveillance systems can be classiﬁed into two classes - one
with an overlapping ﬁeld of view (FOV) and one with a non-overlapping FOV. The non-overlapping FOV
deployment [50] extends the area under surveillance in a cost-eﬃcient way. To detect suspicious strangers,
cameras are mostly installed at important places, such as entrances, corridors and restricted areas. The
46
main purpose of overlapping FOV is to acquire multi-angle views, especially of crowded locations such
as intersections, department stores, and stations. A system with either deployment must perform object
correspondence for object tracking applications.
To build object correspondence for non-overlapping-FOV settings, Kettnaker and Zabih [51] used
color and speed as features of objects and proposed a maximum a posteriori (MAP) solution for an oﬃce
scenario. Kang et al. [52] employed color, edge, speed, and homographic information [53] for tracking in
an environment that is cover by stationary and dynamic pan-tilt-zoom (PTZ) cameras. These features
may diﬀerent substantially between any pair of cameras because of imprecise calibration or variations in
time of the environment. To overcome the calibration issue, Javed et al. [54] proposed a training method
to ﬁnd all brightness transfer functions between cameras.
Overlapping-FOV settings are commonly adopted when cameras are placed in densely or sparsely
populated districts. For the case of a dense population, Nummiaro et al. [55] utilized color and epipolar
constraints to track a single human using pan-tilt cameras. To perform people correspondence in highly
crowded scenes, Khan and Shah [56] utilized homography and projected the foreground of each camera
onto a common view. The locations of the ground point of each individual are located at the intersections
of all projected foreground masks. However, these masks may not overlap if the homographic transfor-
mation matrix is imprecise. Mittal and Davis [57] used color models at diﬀerent heights of the persons
to segment each person in each camera view and match each segmented region by applying epipolar
constraints. For sparse populations, Hu et al. [58] used the principal axis of a human body to locate
his/her ground point, which is the bottom center of an object and is employed in matching foreground
masks. Their approach is ineﬀective at locating ground points of non-human objects with large bottoms.
Calderara et al. [59] adopted the vertical axis of a person as the feature. They applied the concepts of
homography and the epipolar line to improve the robustness in building object correspondence by con-
sidering the degree of the forward/ backward ﬁtness between any pair of cameras. Khan and Shah [49]
also used the boundaries of FOV to determine whether one object appears in various views. Their work
focused on automatically ﬁnding homographic relations among views.
As mentioned above, two main categories of features that are used in object correspondence are
recognition-based and geometry-based. Recognition-based approaches adopt color, texture, or the shape
of masks to perform object correspondence. In object tracking [60–66] using a single camera, where
temporal object correspondence is considered, these features are important and useful. However, the
accuracy of ﬁnding object correspondence across multiple cameras depends on a wide range of intrin-
sic/extrinsic camera parameters, the diﬀerent viewing angles of all of the cameras, and the variations
in the environment with time. An object captured from diﬀerent views always exhibits highly diﬀerent
characteristics, making object correspondence diﬃcult without precise auto-dynamic calibration.
Geometry-based methods are adopted to elucidate the spatial relationship between the environment
and the imaging plane of the camera. One class of geometry-based approaches directly uses the relation-
ship between diﬀerent views. These approaches include disparity, epipolar geometry, and homography
for example. Such methods construct the warping matrix between cameras. Several methods have been
proposed to derive this matrix [67–72]. Another geometric approach is to construct a 3D model of the
environment [73–76]], based on the relationship between a known world coordinate system and the imag-
ing plane of the camera. If object masks in diﬀerent imaging planes in diﬀerent views are projected onto
a world coordinate space, then their correspondence can be obtained. These approaches work well in a
restricted environment in which all camera calibration information and the world coordinates are known.
However, radial distortion of lenses, the setting of cameras at low angles of depression, or the use of
dynamic PTZ cameras may lead to imprecision in calibration.
48
Remove outlier objects
Spatial object correspondence
Evaluate precision/recall rate
Project object locations to global plane
Object list
Figure 4.3: Spatial object correspondence ﬂow. List of objects contains all objects with locations in each
view. Each location address is converted into an address in the global plane. If the converted address of
an object is outside the FOV of another view, then this object is ignored in the SOC stage. SOC builds
the association using EMD-based and greedy-based method. The performances are then expressed in
terms of precision and recall rates.
EMD measures the distance between two distributions and permits partial matches. Therefore, the
essence of SOC is similar to that of EMD. Object correspondence permits that a foreground mask in
one camera can be matched to one foreground mask at most in the other camera, and the optimal
solution is that which minimizes global distance. The approach is similar to the measurement method in
EMD. When the global distance between two distributions is minimal, the system can tolerate a slightly
inaccurate calibration, especially when all measured features deviate from the true values in the same
direction. Not all objects can be captured simultaneously by all cameras, and so partial matching is
necessary.
4.3 Spatial object correspondence
SOC constructs the association between foreground instances of the same object from diﬀerent views. In
the experiment, the homographic approach is adopted to complete SOC, and the EMD-based platform
is compared with the greedy-based platform. To prevent other factors from aﬀecting the experimental
results, the matching ground points used to build the homography matrix are selected manually and the
bounding box of each object is derived from the bounding box annotations of the testing database. Figure
4.3 presents the ﬂow. After the bounding box of each object is obtained, the bottom center of each box
is identiﬁed as the ground point (location) of the object. The address of the bottom center of each box is
projected onto the global plane. Before SOC, obviously outlying objects are removed by checking whether
their projected address is in another view. The features of the remaining objects are then mapped onto
the EMD platform or the greedy-based platform to perform SOC. Finally, the objective performance is
expressed in terms of the precision and recall rates. The following sections describe this method in detail.
50
V1 FOV
V2 FOV
Objects in V2
Objects in V1
H
V1-GP
H
GP-V2
V1 V2
Figure 4.5: Removing outlier objects. The address of a man in V1 is transformed into an address in
the global plane using HV 1−GP . The transformed address is inversely transformed into the address in V2
using HGP−V 2. The objects with white ground points in V2 are absent from V1 and are removed before
SOC.
4.3.3 EMD-based object correspondence
This section describes the mapping of the SOC problem onto the EMDmodel. P andQ are two signatures
that contain the feature and the weight of each object captured by cameras 1 and 2. First, the weights
wpi for all i and wqj for all j are set to the same positive real value. For simplicity, the point weights are
set to 1. This setting is intuitive because an object can not be fragmented into small pieces or paired to
multiple objects. Secondly, the captured features of objects i and j in cameras 1 and 2 form the point
representatives pi and qj. The identiﬁcation of the feature can be recognition-based, geometry-based,
or hybrid. For example, let the object location in the global plane be the point representative. Thirdly,
each distance element dij in D is computed using a user-deﬁned distance metric. This metric can be as
simple as the L1 distance, or as complex as a probability model. In our case, the L2 distance is a natural
selection for specifying the distance between object locations. After EMD, the object i in camera 1 is
associated with object j in camera 2 if fij = 1. For those algorithms in which the cost function is in the
form of a continuous product, such as a product of probabilities, the function can also be converted into
the form of Eq. (4.1) by taking its logarithm.
4.3.4 Greedy-based object correspondence
To understand the extent to which SOC can be improved by adopting the EMD-based platform, a
greedy-based object correspondence method is adopted for comparison. The features and the user-deﬁned
distance metric adopted in the greedy-based method are identical to those adopted in EMD-based object
correspondence. The concept of the greedy method is to select each pair (i, j) with distance dij in order
of increasing distance. Object correspondence depends on a mutually exclusive matching result. Hence,
after the lowest dij out of D is selected, the elements in row i and column j are eliminated. Then, the
52
T
ab
le
4.
1:
S
p
at
ia
l
O
b
je
ct
C
or
re
sp
on
d
en
ce
-
G
ro
u
n
d
T
ru
th
.
S
eq
u
en
ce
P
ai
r/
F
ra
m
e
P
ai
r
O
n
ly
P
a
ir
a
n
d
si
n
g
le
F
P
S
P
re
ci
si
o
n
(%
)
R
ec
a
ll
(%
)
P
re
ci
si
o
n
(%
)
R
ec
a
ll
(%
)
G
re
ed
y
E
M
D
G
re
ed
y
E
M
D
G
re
ed
y
E
M
D
G
re
ed
y
E
M
D
G
re
ed
y
E
M
D
C
A
V
IA
R
-O
n
eS
h
o
p
O
n
eW
ai
t1
1
.2
4
7
3
.0
8
0
.9
7
3
.9
8
3
.1
9
0
.5
9
2
.8
8
8
.9
9
1
.2
4
5
.5
4
5
.2
C
A
V
IA
R
-O
n
eS
to
p
M
ov
eE
n
te
r1
2
.2
5
C
A
V
IA
R
-T
w
oE
n
te
rS
h
o
p
2
1
.4
4
V
iS
O
R
-O
u
td
o
or
U
n
im
o
re
1
4
-1
2
0
.4
4
9
2
.5
9
8
.0
9
1
.8
9
7
.5
9
9
.1
9
9
.6
9
9
.2
9
9
.7
4
5
.6
4
4
.9
V
iS
O
R
-O
u
td
o
or
U
n
im
o
re
1
4
-1
3
0
.2
8
V
iS
O
R
-O
u
td
o
or
U
n
im
o
re
1
4
-2
3
0
.3
5
P
E
T
S
2
0
0
1
2
.2
2
6
8
.7
8
9
.5
6
6
.7
8
8
.8
7
6
.9
9
1
.2
7
8
.2
9
1
.8
4
5
.8
4
5
.3
P
E
T
S
2
0
0
7
S
0
2
-1
2
2
6
.1
1
6
7
.2
8
1
.4
6
8
.3
8
2
.5
8
0
.4
8
8
.6
7
9
.8
8
8
.0
4
0
.3
4
1
.3
P
E
T
S
2
0
0
7
S
0
2
-1
3
5
.1
1
P
E
T
S
2
0
0
7
S
0
2
-1
4
1
3
.1
1
P
E
T
S
2
0
0
7
S
0
2
-2
3
4
.8
9
P
E
T
S
2
0
0
7
S
0
2
-2
4
1
2
.7
8
P
E
T
S
2
0
0
7
S
0
2
-3
4
4
.5
6
P
E
T
S
2
0
0
7
S
0
7
-1
2
1
4
.0
0
8
7
.6
9
1
.9
9
3
.2
9
4
.8
9
2
.6
9
4
.3
9
0
.5
9
2
.3
4
1
.9
4
1
.2
P
E
T
S
2
0
0
7
S
0
7
-1
3
3
.3
3
P
E
T
S
2
0
0
7
S
0
7
-1
4
7
.0
0
P
E
T
S
2
0
0
7
S
0
7
-2
3
2
.4
2
P
E
T
S
2
0
0
7
S
0
7
-2
4
6
.0
8
P
E
T
S
2
0
0
7
S
0
7
-3
4
2
.5
8
A
v
er
a
g
e
5
.8
0
7
7
.8
8
8
.3
7
8
.8
8
9
.3
8
7
.9
9
3
.3
8
7
.3
9
2
.6
4
3
.9
4
3
.6
T
h
e
ta
b
le
sh
ow
s
p
re
ci
si
o
n
,
re
ca
ll
,
a
n
d
F
P
S
o
f
tw
o
ob
je
ct
co
rr
es
p
o
n
d
en
ce
m
et
h
o
d
s
g
iv
en
g
ro
u
n
d
tr
u
th
lo
ca
ti
o
n
.
“
P
a
ir
o
n
ly
”
m
ea
n
s
th
e
ra
te
on
ly
d
ep
en
d
s
o
n
th
e
ob
je
ct
s
ca
p
tu
re
d
in
b
o
th
ca
m
er
a
s
a
t
th
e
sa
m
e
ti
m
e.
“
P
a
ir
a
n
d
si
n
g
le
”
m
ea
n
s
th
e
ra
te
d
ep
en
d
s
o
n
a
ll
ob
je
ct
s
in
th
e
sc
en
e.
54
80
90
100
Pair Only - Precision(%)
GN
50
60
70
CAVIAR VISOR PETS01 PETS07S02PETS07S07
EN
GT
ET
(a)
80
90
100
Pair Only - Recall(%)
GN
50
60
70
CAVIAR VISOR PETS01 PETS07S02PETS07S07
EN
GT
ET
(b)
80
90
100
Pair & Single - Precision(%)
GN
50
60
70
CAVIAR VISOR PETS01 PETS07S02PETS07S07
EN
GT
ET
(c)
80
90
100
Pair & Single - Recall(%)
GN
50
60
70
CAVIAR VISOR PETS01 PETS07S02PETS07S07
EN
GT
ET
(d)
Figure 4.6: Summary of the experimental results.
To evaluate the processing speed of the SOC approach and to analyze the real-time consideration
of surveillance systems, two hypothetical frameworks, shown in Fig. 4.8, are considered. Fig. 4.8(a)
presents a general framework for object correspondence in distributed surveillance. Before the object
correspondence process is implemented, the server must ﬁnish decoding the video; performing video
preprocessing tasks (feature extraction or segmentation), and warping the view. The video preprocessing
tasks prevent object correspondence in real-time in real-world distributed surveillance systems. Fig. 4.9
shows the runtime proﬁling of the preprocessing elements for two-camera object correspondence. As
indicated by the proﬁling, video decoder and segmentation consume over 80% of CPU resources in all
cases. When the frame size of the camera reaches 1280 × 720, the processing speed is less than 1 fps.
Therefore, a real-time object correspondence approach at the server end should reduce the number of
preprocessing tasks as much as possible. Fig. 4.8(b) shows a possible framework. Each smart IP camera
uses a hardware accelerator to extract and segment features. The server receives the compressed bit stream
and the information on the extracted features. The server only uses information on the extracted features
and no further video preprocessing or video decoding steps is required to perform object correspondence.
In the experiment with a general-purpose processor that is running at 3GHz, the EMD function runs at
3,381 fps for PETS07S02-12, whose pair/frame rate is about 26. For PETS07S07-13, the pair/frame rate
is approximately 3.3, and the EMD function runs at 40,100 fps. As a result, the EMD-based method
consumes few resources for object correspondence. Finally, the overall system runs about 43 fps. The
last column in Table 4.1 presents these data.
56
Server end
IP camera 
encoder
Video 
decoder
Feature extract
Object 
corresponding
IP camera 
encoder
Video 
decoder
Feature extract
N
e
t
w
o
r
k
(a)
Server end
Smart IP camera
Feature extract
Video encoder
Smart IP camera
Feature extract
Video encoder
N
e
t
w
o
r
k
Storage
Storage
Object 
corresponding
(b)
Figure 4.8: Two hypothetical frameworks for object correspondence in distributed surveillance systems.
The server in Fig. 4.8(a) should complete all tasks, including video decoding, image preprocessing, and
object correspondence. The server in Fig. 4.8(b) stores the video bit stream and only performs object
correspondence using features captured by smart IP cameras.
58
Part II
Human Action Recognition
60
Figure 5.1: Example of an orientation histogram.
object, the box is partitioned into non-overlapped nw × nh tiles. The concept of the partitioning is to
model the motion of diﬀerent body parts to multiple tiles. The motion vector information in each tile is
then accumulated separately. In the experiment, a 3× 3 tiles is adopted.
The feature vector FVi in a tile Ti comprises the following motion information: a direction mean
(x¯i,y¯i), a magnitude mean mi, a direction variance (vari(x),vari(y)), a magnitude variance vari(m), and
an orientation histogram of the motion vectors. An orientation histogram is formed by the following
steps. Let nmag and nori represent the number of bins used by the magnitude and orientation of motion
vectors respectively. All motion vectors in tile Ti are translated into the form (magnitude, orientation)
followed a quantization along each axis. A bin B(p, q) in the orientation histogram represents the number
of motion vectors that their (magnitude, orientation) are quantized into (p, q). Note that the maximum
of the magnitude is restricted by the size of the search range in ME, and the orientation is in the range
of [0, 2π). Figure 5.1 shows an example of an orientation histogram with nmag = 4 and nori = 6. The
ﬁnal motion-vector-based feature is obtained by concatenating all feature vectors. Figure 5.2 shows an
example on the construction of the motion vector pattern. The red lines are the bounding box of the
performer and the blue lines are the tile boundary lines. The vector at the center of each tile indicates
the mean direction.
Next, a salient feature vector from training and testing action clips is selected for the use of recognizing
behaviors. Currently, in the experiment, such selection is based on sum of all magnitude of the motion
vectors in all tiles, since a larger sum of magnitude indicates more motion occurred in an object. Other
methods combining more than one feature vectors can also be experimented. Once the feature is obtained,
the action label of the testing clip is determined in by
label = argminCidfvi,fvt (5.1)
where i is the training sample index; Ci is the action name of index i; fvi is the feature vector of index i;
fvt is the feature vector of the testing clip; and dfvi,fvt is the distance between the training feature vector
and the testing feature vector. In the implementation, this distance is computed by L1 similarity metric.
Efros et al. also built a feature based on optical ﬂow [82], which can be considered as a dense
motion vector. However, their pattern is formed from entire frame, which may include unrelated motion
information from background. Moreover, they considered the dynamics of the performer as a whole,
62
Figure 5.3: Example of motion energy image and motion history image.
Seven Hu moments [83] are adopted on MEI/MHI to classify actions as in [7]. Speciﬁcally, the mean
and the covariance of the moments of MEI and MHI in each action class are computed from all training
samples in this class. To recognize an unknown action in a clip, the MEI/MHI representation of the clip
and the according moments are calculated. The Mahalanobis distance is then computed, given the vector
of moments of each training class and that of the unknown actions.
5.4 Experimental Results
The motion-vector-based approach is evaluated using the following parameters. The frame size of test
sequences is 720× 480; the block sizes in ME is set to 2× 2, 4× 4, and 8× 8, and the search ranges are
(±8,±8), (±16,±16), and (±32,±32). The following results are based on the setting that the block size
is 4× 4 and the search range is (±16,±16), unless explicitly mentioned.
Table 5.1 shows confusion matrices of the recognition results by the motion-vector-based approach
with the directional discrimination (Right, Left, Center). In most cases, the algorithm performs well.
The worst results are in “VB Spike”. The action “Spike R” and “Spike L” can’t be distinguished from
“Spike C” eﬀectively, even if the block size is 2× 2.
Figure 5.4 displays the overall classiﬁcation result of the motion-vector-based approach; the classes
consist of all VB/GK actions with all three directions. The bars with diﬀerent colors indicate the diﬀerent
sizes of block size used in the experiment. Except the “VB Spike” case discussed before, the correction
rate in most cases is higher than 80%.
Table 5.2 displays the confusion matrices of the classiﬁcation result by the temporal-template-based
approach also with the directional discrimination (Right, Left, Center). In this table, the correction rate
in most cases is higher than 80%. However, the classiﬁcation results of actions performed in frontal view
(C) are better than the actions performed in left view (L) and right view (R). That means the algorithm
has better discriminating ability for the cases performed in frontal view.
Table 5.3 summarizes the result of action classiﬁcation by the temporal-template-based method; the
upper part of this table shows the results of discriminating ability of the actions performed in diﬀerent
views, and the lower part shows the results of discriminating ability of all actions. In the experiments,
64
Table 5.2: Confusion matrices generated by the temporal-template-based approach.
(a) GK Header by MEI
Header R Header L Header C
Header R 8/10 2/10 0/10
Header L 0/10 8/10 2/10
Header C 1/12 0/12 11/12
(b) GK Kick by MEI
Kick R Kick L Kick C
Kick R 8/10 2/10 0/10
Kick L 1/9 7/9 1/9
Kick C 0/10 1/10 9/10
(c) GK Header by MHI
Header R Header L Header C
Header R 8/10 2/10 0/10
Header L 0/10 9/10 1/10
Header C 0/12 0/12 12/12
(d) VB Blocking by MEI
Block R Block L Block C
Block R 7/10 2/10 0/10
Block L 0/8 7/8 1/8
Block C 0/10 0/10 10/10
(e) GK Kick by MHI
Kick R Kick L Kick C
Kick R 9/10 1/10 0/10
Kick L 1/9 7/9 1/9
Kick C 0/10 1/10 9/10
(f) VB Blocking by MHI
Block R Block L Block C
Block R 7/10 2/10 0/10
Block L 1/8 7/8 0/8
Block C 1/10 1/10 8/10
MHI outperforms MEI with an average gain of 1.64%, and the classiﬁcation results deteriorate when the
number of test cases increase. For example, the average classiﬁcation rate of MHI in the upper part of
the table is 85.57%, but drops to 75.29% while including all actions.
Table 5.4 summarizes the overall recognition results by the motion-vector-based and the temporal-
template-based approaches. This table indicates that, with increasing number of actions (performed in
diﬀerent views), the classiﬁcation accuracy decays much more slower in the motion-vector-based approach
than in the temporal-template-based approach. Also, both approaches do not model the body movement
explicitly by joints, the proposed motion-vector-based approach outperforms the temporal-template-based
approach in all cases.
5.5 Summery
This chapter proposes a motion-vector-based approach for recognizing simple and short period actions,
such as actions in controller-free gaming systems or human-computer interfaces. The approach uses the
concept that the movement of diﬀerent body parts may represent a meaningful motion in a speciﬁc action.
Accordingly, the foreground is partitioned into tiles and the motion descriptor of each tile is generated
for action recognition. This approach is compared with the temporal-template-based approach using the
GK/VB physical gaming actions as test cases. Because feature vectors of the temporal-template-based
approach are generated by the entire foreground of one people, no local information of the foreground
is gathered. Therefore, the proposed motion-vector-based approach outperforms the temporal-template-
based approach in all cases.
66
Chapter 6
Parametric Action Recognition
6.1 Introduction
Parametric time-series approaches build a model on the temporal dynamics of motions. The particular
parameters for a class of actions is then estimated from training data. Examples of parametric approaches
include hidden Markov models (HMMs) and linear dynamical systems (LDSs). Parametric approaches
are better suited for more complex actions that are temporally extended. Examples of such actions
include the steps in a ballet dancing video and a music conductor conducting an orchestra using hand
gestures.
This chapter proposes a parametric action recognition approach for the same controller-free gaming
(VB/GK) applications in chapter 5, by using trajectory models of joints of a human, which are generated
by the particle ﬁlter in chapter 3.
6.2 Joint-Trajectory-Based Action Recognition
6.2.1 Overview
In psychology, Johansson studied the human perception of movement by experimenting with well-known
moving light displays (MLD) attached to human body part [1]. He discovered that people can recognize
diﬀerent actions with only these moving dots without full structural information [3]. Accordingly, an
action recognition method using joint trajectories is proposed in this chapter. The purpose of joint-
trajectory modeling is to model the movement of various body parts and use the model to classify human
actions. The joint trajectories are generated by the proposed particle ﬁlter algorithm, which tracks
speciﬁc color markers preset on all joints.
Let nJ be the number of body joints used to model the movement. Because the address of each joint
is expressed in the form of (x, y) in a 2D space, an action is represented by 2nJ 1D trajectories.
Figure 6.1 illustrates the steps to model the joint trajectories and classify actions. The proposed
method mainly follows the trajectory modeling in [17]. The concept of the joint trajectory modeling is
to transform the continuous high dimensional data into discrete symbols (words) from symbol dictionary.
A symbol represents a salient segment in a trajectory, and a symbol dictionary consists of all segments
used to construct the joint trajectories in training databases.
68
(a) Fixed-size segments.
(b) Adaptive-size segments.
Figure 6.3: Illustrations of ﬁxed-size and adaptive-size segments.
to zero before the training stage.
6.2.3 Training Stage
In the training stage, all segments are clustered to build a symbol dictionary. Instead of using K-means
clustering algorithm as in [17], the K-medoids clustering algorithm are applied to keep original shapes of
the segments. In the implementation, the distance metric is the L1 distance, as follows
dist =
n∑
i=1
|tp(i)− tq(i)| (6.1)
where n is the total number of points in each segment, and tp, tq are two compared segments. The ﬁnal
clustered segments form the symbol dictionary.
6.2.4 Matching Stage
In the matching stage, all trajectories are converted into segments and then the symbol sequences.
Speciﬁcally, each segment chooses the closest matching segment in the dictionary, and one trajectory
can be converted into an equivalent symbol sequence. All symbol sequences from all joints are then
concatenated into a single motion descriptor. To classify a descriptor into one action, a histogram-
based method and a sequence-alignment-based method are adopted to train and compare actions. In
the following discussion, the total number of actions are nact, the actions to classify are labeled as ak,
1 ≤ k ≤ nact, and there are nak training samples for action ak.
70
Table 6.1: Testing Similar Actions
Game Name Similar Actions
GK Header (R, L, C)
GK Blocking (R, L, C)
GK Kick (R, L, C)
GK SingleHand (LL, RR, UL, UR)
VB Blocking (R, L, C)
VB Setting (R, L, C)
VB Spike (R, L, C)
VB Underhand (R, L, C)
Table 6.2: Testing All Actions
Game Name Actions
GK Header, Kick, SingleHand, Down, Blocking
VB Blocking, Setting, Spike, Underhand
6.3.1 Testing Similar Actions
As illustrated in subsection 6.2.2 and 6.2.4, there are two methods to build the dictionary — ﬁxed segment
size and adaptive segment size. Also, there are two methods to compare the label sequences — histogram-
based and sequence-alignment-based matching. Therefore, the joint trajectories modeling approach are
tested with four combinations.
Table 6.3 gives the confusion matrices of similar actions by using histogram distance descriptor with
ﬁxed segment size. The table indicates the performance to discriminate actions with diﬀerent direction
are high, only action “Spike” in game “VB” has wrongly recognized actions, which cannot discriminate
the direction left and center eﬀectively. Others actions can be discriminated with diﬀerent directions well.
Table 6.4 lists the action recognition result of the trajectory modeling method for discriminating ac-
tions with diﬀerent directions under four combinations. In general, if the histogram-based distance is
adopted, the method using a ﬁxed-segment-size dictionary outperforms that using an adaptive-segment-
size dictionary with the increase of recognition rate as 14.2%. This table also indicates the performance
of the histogram-based method is better than the sequence-alignment-based method in the implementa-
tion. The reason is that training procedures of these two methods are diﬀerent. The histogram-based
method can eﬀectively merge the training samples in each action by fusing histograms. However, the
sequence-alignment-based method computes the distances between the testing and the training samples
individually. No information merge between the training samples in the same action. Accordingly, an
eﬀective fusion method of training samples by the sequence-alignment-based method needs to be further
explored. If the adaptive-segment-size dictionary is adopted, the sequence-alignment-based method has
smaller variation in recognition rate compared with the histogram-based method.
72
 "	&
	

	

	

		
		
		



	

	

	




	






	






	




	


	


	


	


	


	















	




	




	


7F
47F
67F
;7F
?7F
377F
Figure 6.5: Confusion matrix of all games in game “VB”.
Table 6.5: Recognition accuracy using histogram distance descriptors.
Fixed Dict. Adaptive Dict.
Three views All Three views All
GK 100% 100% 86.89% 59.60%
VB 97.5% 96.67% 82.21% 72.31%
6.3.2 Testing All Actions
Figure 6.5 shows the confusion matrix discriminating all actions in game “VB”, with a ﬁxed-segment-size
dictionary and the histogram-based method. In the test with 12 actions, only action “Spike L” will be
confused by action “Spike C”, with the recognition rate equal to 60%. All other actions in “VB” can be
recognized correctly. The results of recognizing actions in “GK” are all correct. Therefore, the confusion
matrix is not shown.
Table 6.5 and table 6.6 summarizes the recognition results of the proposed joint-trajectory-based
approach. In general, the combination of a ﬁxed-segment-size dictionary and using histogram distance
descriptors achieves best results. However, if the adaptive-segment-size method is adopted, sequence-
alignment-based distance descriptors can provide higher accuracy.
6.4 Summery
This chapter proposes a parametric time-series approach using joint trajectories extracted by particle
ﬁlter as the action descriptors. Each trajectory is converted into a symbol sequence. The action recog-
nition is accomplished using all combinations of two distance measuring methods and two dictionaries
completed by ﬁxed-size or adaptive-size segments. In the experiments, the histogram-based method with
74
Chapter 7
Knowledge-based Action Recognition
7.1 Introduction
Intelligent and automatic security surveillance systems have recently become an active research focus due
to continuously growing public demand for such systems. Terrorist attacks frequently employ bombs,
such as car bombs, suicide bombs, and luggage bombs. Modern technology cannot fully prevent such
attacks, and security oﬃcers can easily miss their targets. However, compared with the two previous
forms, luggage bombs are relatively diﬃcult to hide and furthermore there is generally ample time to
either deal with the bombs or organize an evacuation. Human thus have a better chance to prevent
destruction arising from luggage bombs. Therefore, to achieve early detection of these threats with the
assistance of automatic security systems, the ability to reliably detect suspicious items and identify their
owners is urgently necessary in various venues such as airports and train stations.
Previous studies have given several deﬁnitions of a luggage abandonment event [87–91]. This study
follows three similar but slightly diﬀerent rules [91]: (1) Contextual rule: luggage is considered unattended
after the person who entered the area in possession of that luggage concerned is no longer in close proximity
to it. (2) Spatial rule: luggage is considered unattended when its owner is outside of a small neighborhood
around the luggage. (3) Temporal rule: If the owner of a luggage leaves the area without the luggage, or
if the luggage has been left unattended for more than 30 consecutive seconds, the luggage is considered
abandoned.
7.1.1 Related Works
The task of abandoned luggage detection in surveillance video generally comprises three stages: The ﬁrst
stage localizes candidate abandoned luggage items in the video. The second stage locates and tracks the
luggage owner(s), providing a trajectory for subsequent probabilistic reasoning. The ﬁnal stage assesses a
probability or conﬁdence score for the luggage-abandonment event based on information obtained during
previous stages. The three stages all represent distinct research areas with their own rich literature.
Various existing algorithms may employ diﬀerent methods for diﬀerent stages.
The ﬁrst stage of locating candidate abandoned luggage items within the video frame is performed
using two types of techniques: Those that utilize the technique of background subtraction [92–94], and
those that do not [95, 96]. As is generally acknowledged, object detection and recognition is an instinc-
tive and spontaneous process for human visual system. However, implementing a robust and accurate
76
	@	


	
%

&%	

	"	#	
	,'
)%	%	"	
-	57	
	*'0	
*	


	
%
#	"
#	
	"
	
%
G	57	
"			
H	57	
Figure 7.1: System work ﬂow.
78
Figure 7.2: AVSS 2007 video dataset. Images captured via a typical surveillance camera are looking
down, causing the lower part of objects to appear larger and the upper part to appear smaller.
pixel has a value of 1 and a background pixel a value of 0; let Mk(i, j) represent the k foreground masks,
Mk(i, j) =
{
1, |Fk(i, j)− B(i, j)| ≥ w(i, j) · Std(i, j)
0, |Fk(i, j)− B(i, j)| < w(i, j) · Std(i, j)
These n foreground masks are merged and their intersection taken as the static foreground object mask
S =
⋂n
k=1Mk. Filtering is then conducted on S to remove irrelevant and sporadic noisy pixels, connected
component analysis is subsequently performed. A white (valued 1) block on S indicates a region that has
remained in the foreground of all the n sample frames over the previous 30-second period, and therefore
this region should likely correspond to either a static abandoned luggage item or a stationary human.
The tracking module, which is detailed in the next section, then analyzes the region and further localizes
it if it is determined to be a static luggage item. Figure 7.3 shows an example. S presents candidate
abandoned luggage items. The localized targets provide search regions for subsequent tracking and higher
level event reasoning.
7.3 Selective Tracking Modules
The system presents information on the locations of suspicious items after obtaining S. All static fore-
ground objects are assumed to be either humans or luggage items. Each foreground region in S is checked
to determine whether it is a human via a combination of skin color information and body contours. If
the region is identiﬁed as a human, it is discarded because the object of the search is abandoned luggage
items. If the region is identiﬁed as not a human, it is assumed to be a luggage item. A local search
region is constructed around the detected luggage to see whether its owner is in close proximity in the
present frame at time t. If the owner is found, the region is again discarded because the owner exhibits no
80
Figure 7.4: Left: input video frame with localized search region indicated by red circle. Right: the Cr
detection result within the search region.
7.3.1 Cr Color Channel with Human Skin
Human skin signal response is signiﬁcantly larger in the YCbCr color space than the commonly used
RGB color space. Due to signiﬁcant blood ﬂow, human skin responds strongly to the Cr channel in the
YCbCr space, irrespective of skin color [101]. Accordingly, the Cr channel of skin color is used for human
face localization because in situations involving severe occlusion (crowded scenes with people overlapping
one another), human face is the most visible body part when viewed with a typical surveillance camera
positioned looking downwards from a height.
To ﬁnd the face of the owner, a search region is ﬁrst constructed around the suspicious static luggage
item. Background subtraction is then performed on RGB color space within the region. An RGB
foreground of the region is obtained and then converted to the YCbCr color space, and the Cr channel is
retained, as illustrated in Fig. 7.4. Background subtraction is performed within the search region prior to
conversion to YCbCr color space because Cr is a channel representing the diﬀerence of red color, and thus
the face signal is stronger when background clutter is removed. The Cr channel response is then used to
locate the face of the luggage owner, while simultaneously locating human body contour information as
explained below.
7.3.2 Improved Hough Transform on Body Contour
Cr channel responds to red within the search region, which in some cases may include other reddish
objects besides the face of the owner. Therefore, a new mechanism for reliably detecting the presence
of the luggage owner is employed. The human upper-body contour, which comprises the head-shoulder
silhouette, is then adopted. The head-shoulder contour, as inspired by [95], is used under the Hough
transform (HT) to detect human upper-body within the search region. Figure 7.5 depicts the contour
and the used notations.
HT is a morphological tool which, in its simplest form, maps a straight line in normal space to a
point in parameter space [102]. A generalized version of HT is utilized to localize contour of an arbitrary
shape. The algorithm comprises two stages: template generation and contour matching.
During template generation, given a predeﬁned head-shoulder contour, as in Fig. 7.5, the HT algo-
rithm ﬁrst establishes a center point (xC , yC) of the face for the contour template. The algorithm then
82
  


		
=1
Figure 7.6: The origin of the two red lines has a ψ angle with degree m, corresponding to the red, m-th,
bin which contains two (r, α) pairs. Solid lines denote correct matches, while dashed lines represent noise.
Diﬀerent points on the head-shoulder contour in the edge image converge to a local maximum on the
detection map on the right. At the bottom is the 180-bin reference table; a Gaussian weighting g is shown
below the table, with the center bin labeled in red and the neighboring ten bins in blue.
84
of the owner at time t, and the prediction for time t+ 1 can be formulated as
r(t+ 1) = r(t) + Δr
where Δr is generated recursively via motion prediction and given by
Δrt ← α ·Δrt−1 + β · (r(t)− r(t− 1))
where α+ β = 1, α > 0, β ≥ 0. The fact that Δr is calculated recursively ensures that past information
is considered and past inﬂuences decay exponentially with time. In the implementation, the exponential
smoothing coeﬃcients α and β are empirically determined to be 0.4 and 0.6, respectively.
Three measures are used to calculate the probability score for the trajectory of the luggage owner,
which is then used to obtain a conﬁdence score for the luggage-abandonment event. The three measures
include the diﬀerences between the prediction from the last frame and the detection on the present frame,
in location, size and color histogram of the luggage owner [96]. The probability score increases with
closeness of prediction and detection. Let PTOTAL denote the probability score combining the measures;
let PPOS, PSIZE and PCH denote the scores of the position measure, size measure and color-histogram
measure, respectively; ﬁnally, let r represent the position vector, s the size (in pixel area) and c the color
histogram. The three scores are deﬁned as follows, with subscript P corresponding to prediction and D
to detection.
PPOS(rP, rD) = exp(−(xP − xD)
2
σ2x
) · exp(−(yP − yD)
2
σ2y
)
PSIZE(sP, sD) = exp(−(sP − sD)
2
σ2s
)
PCH(cP, cD) =
1√
2πσ
exp(−D
2
2σ2
)
whereD is the Bhattacharyya distance between the two color histograms, as inD2 = 1−∑256i=1√cP(i)cD(i),
and σ is the standard deviation. The total probability score is calculated by combining the above three
measures, each with a scale factor λ, so they total 1, as follows
PTOTAL = λPOSPPOS + λSIZEPSIZE + λCHPCH (7.1)
The three probabilities serve more as comparative than absolute values. A change in the standard
deviations of these probability calculations would similarly aﬀect all probabilities thus calculated, with the
most probable detection still having the highest probability ranking. Empirical values thus are assigned
to the standard deviations, and parameter selections in the experiment produce insigniﬁcant eﬀects.
7.4 Probabilistic Event Model
The tracking module provides a trajectory and associated probability score. The module calculates the
distance from the feet of the owner to the luggage for each incoming frame, with this distance being used
to determine when the owner leaves the scene or the luggage, and for how long the luggage has been
left unattended. The feet of the owner are assumed to be positioned below the head at a distance of
about ﬁve face-lengths. According to this study, luggage is formally declared abandoned when its owner
leaves the scene without it, or when it has been left unattended for 30 consecutive seconds. Luggage-
abandonment events are deﬁned using a probabilistic framework [87]. Let A denote the event, and O
86
Table 7.1: Alarm Time (Second).
Sequence Ground Truth Owner Break Time Time Alarm
AVSS 2007 Easy 180.00 114.80 119.76
AVSS 2007 Medium 162.00 100.88 102.64
AVSS 2007 Hard 162.00 101.08 102.28
PETS 2006 Seq. 1 113.72 85.88 90.52
PETS 2006 Seq. 2 91.84 61.92 65.04
PETS 2006 Seq. 4 104.08 72.88 76.36
PETS 2006 Seq. 5 110.56 80.28 83.04
PETS 2006 Seq. 6 96.88 68.44 73.96
PETS 2006 Seq. 7 93.96 60.68 90.60
the owner remains continuously with the luggage, and therefore no alarm is raised. In video 7, the owner
wanders before ﬁnally leaving the scene without his luggage; the trajectory of the actively moving owner
contains too many abrupt changes in speed and direction for the present motion prediction algorithm to
successfully follow. The tracker lost the owner 34 seconds after leaving the luggage, while an alarm is
triggered at 30 seconds. Figure 7.8 shows some labeled scene shots. Using diﬀerent system parameter
settings for all tested environments, the processing speed of the proposed approach is approximately 15
to 20 frames per second, which is suﬃcient for real world applications.
7.6 Summery and Future Work
This chapter presents a localized approach for detecting abandoned luggage in surveillance environments.
Through foreground-mask sampling, only the object of interest is localized, while ﬁltering out all irrel-
evant, interfering agents. Tracking thus can be performed in a more selective and localized manner.
An improved implementation of the HT for detecting the contours of the upper-body is also proposed
for use in tandem with skin color detection. A probabilistic framework and the MAP principle are em-
ployed to model the luggage-abandonment event. In the future, the proposed approach is extended to a
multi-camera network in which coordination of various cameras enables cues to be gathered from mul-
tiple perspectives and information to be relayed from one to another camera. Besides, the approach is
generalized to include diﬀerent viewing angles on the human form.
88
Chapter 8
Conclusion
8.1 Principal Contributions
In this dissertation, the analysis of VLSI architecture of feature extraction and related recognition ap-
proaches are studied, which is key modules to human action/activity analysis. The design challenges
include robust feature extraction and eﬀective mathematical action/activity model. Three recognition
categories are chose as case studies to discuss the above design challenges in controller-free gaming in-
terfaces and abandoned luggage detection systems. For diﬀerent application constraints, designers can
adopt proposed techniques to design a suitable system. The major contributions are summarized in the
following subsections.
8.1.1 Color Structure Descriptor
First analysis of dedicated hardware architecture design for MPEG-7 CSD descriptor is proposed. This
design runs at 30fps for real-time multimedia applications, and can be modiﬁed to capture the trajectory
of humans. With the analysis of histogram accumulation, local histogram observing (LHO) is used to
buﬀer local structure window for data reuse, and three parallel LHOs is implemented to support real-time
operations. The chip area is further saved from the color transformation and the non-linear quantization.
The divider in the color transformation is implemented with a lookup table, which area is 36% of that of
original divider. The 255 comparators in non-linear quantization are folded into one.
8.1.2 Tracking Using Particle Filter
A hardware architecture that implements the color-based particle ﬁlter tracking algorithm is proposed.
The design runs at 30 fps on D1 resolution. In essence, the particle ﬁlter is a stochastic algorithm.
Converting from algorithm to hardware architecture requires limiting the bit precision. Prioritized ﬁnite
word length analysis is proposed to analyze the word length requirement. Second, particle-level parallel
processing is developed; it is suitable for accelerating such a highly order-dependent algorithm and can be
easily extended to higher speciﬁcations. Third, content addressable memory is applied to reduce memory
required by the algorithm. The extent of the memory reduction depends on the characteristics of the
90
Third, event detection is formulated as a maximum a posteriori (MAP) problem. The reliability
of the tracked trajectory of the owner is used in evaluating the overall conﬁdence score of the luggage-
abandonment event. An alarm is triggered if the overall conﬁdence score exceeds a given threshold, which
is adjustable by the user to achieve varying degrees of system sensitivity.
8.2 Future Directions for Human Action Recognition
Several important issues still need to be considered for successful action recognition algorithms in terms
of their robustness to real-world conditions and real-time performance. Some of these issues include:
invariances in human action analysis, spatio-temporal action localization, feature extraction in real-world
conditions, and intention reasoning.
8.2.1 Invariances in Human Action Analysis
Invariances in human action analysis contain three main classes: viewpoint invariance, execution rate
invariance, and anthropometry invariance.
Viewpoint invariant issue means that recognition algorithms should recognize actions performed in
diﬀerent views. However, to build statistical models of several views is extremely challenging because
of the variations in motion and structure features induced by camera perspective eﬀects and occlusions.
A direct method is to store templates from several canonical views and interpolate across the stored
views [7, 105]. Another approach assumes that some correspondences across views are available. Rao et
al. [106] preserved extrema in spatio-temporal curvature of trajectories across views. Parameswaran et
al. deﬁned a view-invariant representation of actions based on the theory of 2D and 3D invariants [107].
Weinland et al. extended motion history image to 3D space by combining views from multiple cameras
to generate a 3D binary volume [108]. View-invariant features are extracted by computing circular fast
Fourier transform (FFT) of the volume.
Diﬀerent persons or even the same person may act with diﬀerent execution rates while performing
the same action. Minor changes in execution rates will work ﬁne with state-space approaches. However,
these kind of methods do not explicitly model transformations of the temporal axis and are not truly
rate invariant. The variation in execution rate can be modeled as a warping function of the temporal
scale. To model highly nonlinear warping functions, the most common method is dynamic time warping
(DTW) of the feature sequence [105, 109, 110]. DTW is a promising method because it is independent
of the choice of feature. However, DTW requires the start and end time have to be aligned. Moreover,
DTW is unsuitable for long sequences involving many templates due to the distance computations.
Anthropometry invariant issue illustrates that the same action performed by diﬀerent people must
be reliably categorized into one despite of diﬀerent personal styles. Individual diﬀerences in size, shape,
gender, habit, etc., generate the anthropometric variations. Some methods that normalize the features
to compensate for changes in size, scale, etc., are usually employed.
8.2.2 Spatial and Temporal Action Localization
Action localization issues can be classiﬁed into spatial and temporal action localization. The deﬁnite range
of an action clip is important for some action recognition approaches. Besides, if the range is known,
the feature extraction steps and the following recognition steps can be limited in this data. Accordingly,
using this information can reduce computation complexity. However, to locate start time and end time
92
Bibliography
[1] G. Johansson, “Visual perception of biological motion and a model for its analysis,” Perception
and Psychophysics, vol. 14, no. 2 1973, pp. 201–211, 1973.
[2] Martin A. Giese and Tomaso Poggio, “Neural mechanisms for the recognition of biological move-
ments,” Nature Reviews Neuroscience, vol. 4, pp. 179–192, 2003.
[3] D. M. Gavrila, “The visual analysis of human movement: a survey,” Comput. Vis. Image Underst.,
vol. 73, no. 1, pp. 82–98, 1999.
[4] J.K. Aggarwal and Q. Cai, “Human motion analysis: a review,” in Nonrigid and Articulated Motion
Workshop, 1997. Proceedings., IEEE, Jun 1997, pp. 90–102.
[5] C. Cedras and M. Shah, “Motion-based recognition: A survey,” IVC, vol. 13, no. 2, pp. 129–155,
March 1995.
[6] P. Turaga, R. Chellappa, V.S. Subrahmanian, and O. Udrea, “Machine recognition of human
activities: A survey,” Circuits and Systems for Video Technology, IEEE Transactions on, vol. 18,
no. 11, pp. 1473–1488, Nov. 2008.
[7] A.F. Bobick and J.W. Davis, “The recognition of human movement using temporal templates,”
Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 23, no. 3, pp. 257–267, Mar
2001.
[8] Hongying Meng, N. Pears, and C. Bailey, “A human action recognition system for embedded
computer vision application,” in Computer Vision and Pattern Recognition, 2007. CVPR ’07.
IEEE Conference on, June 2007, pp. 1–6.
[9] Haojie Liu, Shouxun Lin, Yongdong Zhang, and Kun Tao, “Automatic video-based analysis of ath-
lete action,” in Image Analysis and Processing, 2007. ICIAP 2007. 14th International Conference
on, Sept. 2007, pp. 205–210.
[10] E. Shechtman and M. Irani, “Space-time behavior based correlation,” in Computer Vision and
Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, June 2005, vol. 1,
pp. 405–412 vol. 1.
[11] P. Dollar, V. Rabaud, G. Cottrell, and S. Belongie, “Behavior recognition via sparse spatio-temporal
features,” in Visual Surveillance and Performance Evaluation of Tracking and Surveillance, 2005.
2nd Joint IEEE International Workshop on, Oct. 2005, pp. 65–72.
94
[26] L. Cieplinski, M. Kim, J.-R. Ohm, M. Pickering, and A. Yamada, “Iso/iec 15938-3 fcd information
technology - multimedia content description interface - part 3: Visual,” March 2001.
[27] Shao-Yi Chien, Wei-Kai Chan, Der-Chun Cherng, and Jing-Ying Chang, “Human object tracking
algorithm with human color structure descriptor for video surveillance systems,” in Multimedia and
Expo, 2006 IEEE International Conference on, July 2006, pp. 2097–2100.
[28] Nintendo Co. Ltd., “Wii,” http://wii.com/, 2006.
[29] Sony Computer Entertainment, “Playstation3,” http://www.us.playstation.com/, 2006.
[30] Microsoft Co. Ltd., “Project natal,” http://www.xbox.com/en-US/live/projectnatal/, 2009.
[31] Microsoft Co. Ltd., “Xbox 360,” http://www.xbox.com, 2005.
[32] R. E. Kalman, “A new approach to linear ﬁltering and prediction problems,” Transactions of the
ASME Journal of Basic Engineering, , no. 82 (Series D), pp. 35–45, 1960.
[33] P. Pe´rez, C. Hue, J. Vermaak, and M. Gangnet, “Color-based probabilistic tracking,” ECCV ’02:
Proceedings of the 7th European Conference on Computer Vision-Part I, pp. 661–675, 2002.
[34] M. Isard and A. Blake, “Condensation — conditional density propagation for visual tracking,”
IJCV, vol. 29, pp. 5–28, 1998.
[35] X. Li and N. Zheng, “Adaptive target color model updating for visual tracking using particle ﬁlter,”
Systems, Man and Cybernetics, 2004 IEEE International Conference on, vol. 4, pp. 3105–3109 vol.4,
Oct. 2004.
[36] Y. Zhuang, W. Wang, and R. Xing, “Target tracking in colored image sequence using weighted color
histogram based particle ﬁlter,” Robotics and Biomimetics, 2006. ROBIO ’06. IEEE International
Conference on, pp. 1488–1493, Dec. 2006.
[37] G. Liu, C. Fan, and E. Gao, “Visual target tracking based on multiple cues and particle ﬁlter,”
Robotics and Biomimetics, 2006. ROBIO ’06. IEEE International Conference on, pp. 1483–1487,
Dec. 2006.
[38] T. Zhang, S. Fei, X. Li, and H. Lu, “An improved particle ﬁlter for tracking color object,” Intelligent
Computation Technology and Automation (ICICTA), 2008 International Conference on, vol. 2, pp.
109–113, Oct. 2008.
[39] K. Nummiaro, E. B. Koller-Meier, and L. V. Gool, “Object tracking with an adaptive color-based
particle ﬁlter,” Symposium for Pattern Recognition of the DAGM, pp. 355–360, 2002.
[40] J. Czyz, B. Ristic, and B. Macq, “A color-based particle ﬁlter for joint detection and tracking
of multiple objects,” Acoustics, Speech, and Signal Processing, 2005. Proceedings. (ICASSP ’05).
IEEE International Conference on, vol. 2, pp. 217–220, 18-23, 2005.
[41] A.C. Sankaranarayanan, R. Chellappa, and A. Srivastava, “Algorithmic and architectural design
methodology for particle ﬁlters in hardware,” Computer Design: VLSI in Computers and Proces-
sors, 2005. ICCD 2005. Proceedings. 2005 IEEE International Conference on, pp. 275–280, Oct.
2005.
96
[56] Saad M. Khan and Mubarak Shah, “A multiview approach to tracking people in crowded scenes
using a planar homography constraint,” in European Conference on Computer Vision, 2006, pp.
133–146.
[57] Anurag Mittal and Larry S. Davis, “M2tracker: A multi-view approach to segmenting and tracking
people in a cluttered scene,” International Journal of Computer Vision, vol. 51, no. 3, pp. 189–203,
2003.
[58] Weiming Hu, Min Hu, Xue Zhou, Tieniu Tan, Jianguang Lou, and S. Maybank, “Principal axis-
based correspondence between multiple cameras for people tracking,” IEEE Trans. Pattern Anal.
Machine Intell., vol. 28, no. 4, pp. 663–671, 2006.
[59] S. Calderara, R. Cucchiara, and A. Prati, “Bayesian-competitive consistent labeling for people
surveillance,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 30, no. 2,
pp. 354–360, Feb. 2008.
[60] V. Kravtchenko, “Tracking color objects in real time,” M.S. thesis, University of British Columbia,
Vancouver, British Columbia, Nov. 1999.
[61] Y. Deng and B. S. Manjunath, “Unsupervised segmentation of color-texture regions in images and
video,” IEEE Trans. Pattern Anal. Machine Intell., vol. 23, no. 8, pp. 800–810, 2001.
[62] D. Comaniciu, V. Ramesh, and P. Meer, “Kernel-based object tracking,” IEEE Trans. Pattern
Anal. Machine Intell., vol. 25, no. 5, pp. 564–577, 2003.
[63] Tyng-Luh Liu and Hwann-Tzong Chen, “Real-time tracking using trust-region methods,” IEEE
Trans. Pattern Anal. Machine Intell., vol. 26, no. 3, pp. 397–402, 2004.
[64] A. Yilmaz, Li Xin, and M. Shah, “Contour-based object tracking with occlusion handling in video
acquired using mobile cameras,” Transactions on Pattern Analysis and Machine Intelligence, vol.
26, no. 11, pp. 1531–1536, 2004.
[65] Manuel J. Lucena, Jose M. Fuertes, and Nicolas Perez de la Blanca, “Real-time tracking using
multiple target models,” in Pattern Recognition and Image Analysis, pp. 20–27. 2005.
[66] Kazuyuki Morioka, Xuchu Mao, and Hideki Hashimoto, “Global color model based object matching
in the multi-camera environment,” in IEEE/RSJ International Conference on Intelligent Robots
and Systems, 2006, pp. 2644–2649.
[67] C. Schmid and A. Zisserman, “Automatic line matching across views,” in IEEE Computer Society
Conference on Computer Vision and Pattern Recognition, 1997, pp. 666–671.
[68] Zhengyou Zhang, “Determining the epipolar geometry and its uncertainty: A review,” International
Journal of Computer Vision, vol. 27, no. 2, pp. 161–195, 1998.
[69] F. Schaﬀalitzky and A. Zisserman, “Viewpoint invariant texture matching and wide baseline stereo,”
in IEEE International Conference on Computer Vision, 2001, vol. 2, pp. 636–643 vol.2.
[70] Y. Wexler, A. W. Fitzgibbon, and A. Zisserman, “Learning epipolar geometry from image se-
quences,” in IEEE Computer Society Conference on Computer Vision and Pattern Recognition,
2003, vol. 2, pp. II–209–16 vol.2.
98
[87] Y.-L. Tian, R. S. Feris, and A. Hampapur, “Real-time detection of abandoned and removed objects
in complex environments,” in The Eighth International Workshop on Visual Surveillance - VS2008,
2008.
[88] N. Bird, S. Atev, N. Caramelli, R. F. K. Martin, O. Masoud, and N. Papanikolopoulos, “Real time,
online detection of abandoned objects in public areas,” in Robotics and Automation, 2006. ICRA
2006. Proceedings 2006 IEEE International Conference on, 2006, pp. 3775–3780.
[89] S. Ferrando, G. Gera, and C. Regazzoni, “Classiﬁcation of unattended and stolen objects in video-
surveillance system,” in AVSS ’06: Proceedings of the IEEE International Conference on Video
and Signal Based Surveillance, 2006, p. 21.
[90] M. D. Beynon, D. J. Van Hook, M. Seibert, A. Peacock, and D. Dudgeon, “Detecting abandoned
packages in a multi-camera video surveillance system,” Advanced Video and Signal Based Surveil-
lance, IEEE Conference on, vol. 0, pp. 221–228, 2003.
[91] X. Song F. Lv, B. Wu, V. K. Singh, and R. Nevatia, “Left luggage detection using bayesian
inference,” in Proceedings of the 9th IEEE International Workshop on Performance Evaluation in
Tracking and Surveillance (PETS ’06), 2006, pp. 83–90.
[92] J. Martinez del Rincon, J. E. Herrero-Jaraba, J. R. Gomez, and C. Orrite-Urunuela, “Automatic
left luggage detection and tracking using multi-camera ukf,” in Proceedings of the 9th IEEE In-
ternational Workshop on Performance Evaluation in Tracking and Surveillance (PETS ’06), 2006,
pp. 59–66.
[93] L. Li, R. Luo, W. Huang R. Ma, and K. Leman, “Evaluation of an ivs system for abandoned
object detection on pets 2006 datasets,” in Proceedings of the 9th IEEE International Workshop
on Performance Evaluation in Tracking and Surveillance (PETS ’06), 2006, pp. 59–66.
[94] J. Zhou and J. Hoang, “Real time robust human detection and tracking system,” in CVPR
’05: Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition (CVPR’05) - Workshops, 2005, p. 149.
[95] B. Wu and R. Nevatia, “Detection of multiple, partially occluded humans in a single image by
bayesian combination of edgelet part detectors,” Computer Vision, IEEE International Conference
on, vol. 1, pp. 90–97, 2005.
[96] B. Wu and R. Nevatia, “Tracking of multiple, partially occluded humans based on static body
part detection,” in CVPR ’06: Proceedings of the 2006 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition, 2006, pp. 951–958.
[97] E. Auvinet, E. Grossmann, C. Rougier, M. Dahmane, and J. Meunier, “Left-luggage detection
using homographies and simple heuristics,” in Proceedings of the 9th IEEE International Workshop
on Performance Evaluation in Tracking and Surveillance (PETS ’06), 2006, pp. 51–58.
[98] S. Guler, J. A. Silverstein, and I. H. Pushee, “Stationary objects in multiple object tracking,”
in AVSS ’07: Proceedings of the 2007 IEEE Conference on Advanced Video and Signal Based
Surveillance, 2007, pp. 248–253.
[99] K. Smith, P. Quelhas, and D. Gatica-perez, “Detecting abandoned luggage items in a public space,”
in Proceedings of the 9th IEEE International Workshop on Performance Evaluation in Tracking and
Surveillance (PETS ’06), 2006, pp. 75–82.
100
I 
 
計畫相關國際會議論文發表: 
 
1. T.-H. Wang, J.-Y. Chang, and L.-G. Chen, "Algorithm and architecture for object tracking using particle 
filter," in Multimedia and Expo, 2009. ICME '09. IEEE International Conference on, 2009, pp. 1374-1377. 
2. C.-C. Liang, J.-Y. Chang, and L.-G. Chen, "Run-Processing: A Coherence-oriented Processing Method and 
its Hardware Architecture for Real-time Video Object Segmentation," in Robotics,Vision, Signal Processing, 
and Power Applications, 2009. RoViSP '09. International Conference on, 2009. 
3. H.-H. Liao, J.-Y. Chang, and L.-G. Chen, "A Localized Approach to Abandoned Luggage 
Detection with Foreground-Mask Sampling," in Advanced Video and Signal Based Surveillance, 2008. AVSS 
'08. IEEE International Conference on, 2008, pp. 132-139. 
4. J.-Y. Chang, T.-H. Wang, S.-Y. Chien, and L.-G. Chen, "Spatial-temporal consistent labeling for 
multi-camera multi-object surveillance systems," in Circuits and Systems, 2008. ISCAS '08. IEEE 
International Symposium on, 2008, pp. 3530-3533. 
 
3. PARTICLE FILTER TRACKING ALGORITHM
This section gives the formal deﬁnition of the particle ﬁlter algo-
rithm, describes the color feature we used, and illustrates the pro-
posed dynamic model.
3.1. Basic Theory
For Bayesian tracking framework, the goal is to estimate the state
xt in each time t using current and previous observations, z1:t, and
previous state xt−1. There are two models - a motion model and an
observation model, describing how the system evolves along with
time. A motion model xt+1 = ft(xt,vt) describes how states
evolve with time, where vt is the noise introduced to the motion
model. An observation model zt = ht(xt,nt) describes the rela-
tion between current state xt and its observation zt, where nt is the
noise introduced in the observation.
The particle ﬁlter algorithm works by approximating the pos-
terior distribution with a set of particles, namely p(xt|z1:t) ≈P
i
witδ(x− x
i
t) , where wit is the weight of particle i at time t.
3.2. Color-based Particle Filtering
Color histograms are often used as a feature for many vision-based
applications. Here we adopt the spatial-weighted color histogram as
in [1]. Each pixel contributes its color weight according to its norm
to the center of the object as
qˆu = C
nX
i=1
k(||x||2)δk[b(x)− u] (1)
where C is the normalization constant, δk(·) is the Kronecker delta
function, and k(·) is the chosen kernel function. For our case, we
use Gaussian kernel as k(x) = exp(−x2/2) to compute our color
histogram. We use the L1 distance as the similarity metric between
two color histograms as the L1 distance gives good results.
3.3. Second Order Dynamic Model
The constant velocity assumption used in most particle ﬁlters is not
always followed in real cases. Instead of simply estimating the ve-
locity from the position shift, we additionally estimate the accel-
eration of the object. In the following discussion, pt = {x, y}
stands for the position, qt = {vx, vy} stands for the velocity, and
rt = {ax, ay} stands for the acceleration of the target object at time
t. From the recursive form of the equal-acceleration equations we
obtain (
fpt = pt−1 + qt−1 + 12rt−1
eqt = qt−1 + rt−1 (2)
wherefpt, eqt are predicted from previous pt−1, qt−1, and rt−1.
When we obtain the estimated position pt after observation, we
need to estimate qt and rt at this frame. By solving the dual equation
in equation (2), we obtain
(
qt =
2
3
[(pt − pt−1) +
1
2
qt−1]
rt =
2
3
[(pt − pt−1)− qt−1]
(3)
The equation (3) is our second order dynamic model. Note that in
[1], they also proposed a second-order dynamic model. Their model
coefﬁcients are obtained by training or manual initialization, while
in our method they are embedded in the state estimation and auto-
matically learned.
Fig. 2. Main processing diagram.
3.4. The Whole Algorithm
In our current implementation, the state variable consists of xt =
{x, y, vx, vy, ax, ay}. After manual initialization is done, at each
time step, the tracking operations are processed, as shown in ﬁgure
2.
In the propagation stage, each particle is propagated by equation
(2) mentioned above. Then we add some Gaussian noises to perturb
particle states. In the observation stage, each particle constructs its
own color histogram as described in section 3.2, spanning the size
and angle of the current tracked object. Note that we must normalize
the individual weights in each histogram bin before computing the
L1 distance.
In particle weighting stage, we compute the weight of each par-
ticle according to the following formula:
wi = exp(
−d2i
2σ2
) (4)
where i is the particle index, di is theL1 distance calculated between
the color histogram of the target and the color histogram of particle i,
and σ is to model the variance of histograms. To estimate actual state
after all weights are known, we implement the maximum a posteriori
(MAP) and mean estimation. The MAP estimation is given by
xt = argmaxxi
t
p(xt|z1:t,x
i
t) (5)
where xit is the state presented by particle index i at current time t.
In mean estimation, the ﬁnal state is simply the weighted sum of all
particles. We must resample the particles to avoid the degeneracy
problem. Here we implement the resampling algorithm as in [7].
Optionally we update the size and angle of the tracked object
after above steps. To do this, we use another 9 particles to estimate
object’s size by the MAP criterion. The selected 9 candidate sizes
are obtained by [W−ΔW,W,W+ΔW ]×[H−ΔH,H,H+ΔH],
whereΔW andΔH are the parameters to model the increase of the
object size. To estimate object’s angle, we quantize the angle into
32 entries spanning evenly over [0, π). For each angle candidate,
we apply the color histogram calculation and select the ﬁnal angle
with minimum distance. After all these steps are done, the algorithm
proceeds with another frame.
Note that in our current implementation, we separate the estima-
tion of object’s size and angle from its position and velocity. The
reason is that in typical tracking scenario, object’s position is the
most important target to the tracking system.
4. HARDWARE ARCHITECTURE DESIGN
Particle ﬁlter algorithms generally require high computation cost.
In our software implementation, we can only achieve 8 frames per
1375
Authorized licensed use limited to: National Taiwan University. Downloaded on September 18, 2009 at 04:48 from IEEE Xplore.  Restrictions apply. 
(a) Frame 28. (b) Frame 48.
Fig. 4. Example of tracking results.
Table 1. Tracking Accuracy
Category Success Targets Total Targets Success Rate
Category One 11 13 84.6%
Catetory Two 9 10 90.0%
Category Success Frames Total Frames Success Rate
Category One 1022 1277 80.0%
Catetory Two 1536 1727 88.9%
5.2. Architecture Evaluation
We adopt the 4-parallel scheme with single port SRAM as the im-
plementation. To demonstrate the effectiveness of the hardware ar-
chitecture, we implement the architecture in Verilog hardware de-
scription language and synthesize at 5ns using UMC 90nm Logic
SP-RVT Process. The synthesis result is shown in table 2, which
consists of the area and gate count of several modules as in ﬁgure
3(a). Table 3 gives the simulation result of the proposed architec-
ture. The FPS is normalized according to different object sizes in
the sequences. By carefully choosing the parallelization parameter
P and effective pipelining in implementation, real-time performance
can be achieved.
6. CONCLUSION
In this paper, we propose an algorithm and architecture for color-
based particle ﬁlter tracking. The proposed algorithm can estimate
objects’ positions, sizes and angles while tracking. The second or-
der dynamic model gives more accurate results than constant speed
dynamic model in our case, and the tracking accuracy is 85% in av-
erage. The proposed architecture can operate at 31.35 frames per
second in average, achieving real-time performance of color-based
particle ﬁlter. Different parallelization schemes can be chosen ac-
cording to the hardware resource and timing requirement.
Table 2. Synthesis Results
Module Name Cell Area (um2) Gate Count
Current Position Generator 4498 1125
Position Filter 31017 7754
Spatial Weight PE 21704 5426
Spatial Weight Normalizer 20733 5183
L1 Distance Calculator 3277 819
(a) Initialization. (b) Constant speed model.
(c) Proposed model. (d) Frame 131.
Fig. 5. Example of tracking results.
Table 3. Architecture Evaluation Results
Sequence Name Cycles/Frame Normalized FPS
move 67973 30.81
climb 127334 31.43
touch 100051 31.82
7. REFERENCES
[1] Patrick Pe´rez, Carine Hue, Jaco Vermaak, and Michel Gangnet, “Color-
based probabilistic tracking,” ECCV ’02: Proceedings of the 7th Euro-
pean Conference on Computer Vision-Part I, pp. 661–675, 2002.
[2] M. Isard and A. Blake, “Condensation — conditional density propaga-
tion for visual tracking,” IJCV, vol. 29, pp. 5–28, 1998.
[3] A.C. Sankaranarayanan, R. Chellappa, and A. Srivastava, “Algorith-
mic and architectural design methodology for particle ﬁlters in hard-
ware,” Computer Design: VLSI in Computers and Processors, 2005.
ICCD 2005. Proceedings. 2005 IEEE International Conference on, pp.
275–280, Oct. 2005.
[4] Shaohua Hong, Zhiguo Shi, and Kangsheng Chen, “Compact resam-
pling algorithm and hardware architecture for paticle ﬁlters,” Communi-
cations, Circuits and Systems, 2008. ICCCAS 2008. International Con-
ference on, pp. 886–890, May 2008.
[5] J. Alarcon, R. Salvador, F. Moreno, P. Cobos, and I. Lopez, “A new real-
time hardware architecture for road line tracking using a particle ﬁlter,”
IEEE Industrial Electronics, IECON 2006 - 32nd Annual Conference on,
pp. 736–741, Nov. 2006.
[6] H. Medeiros, J. Park, and A. Kak, “A parallel color-based particle ﬁlter
for object tracking,” Computer Vision and Pattern Recognition Work-
shops, 2008. CVPRW ’08. IEEE Computer Society Conference on, pp.
1–8, June 2008.
[7] M.S. Arulampalam, S. Maskell, N. Gordon, and T. Clapp, “A tutorial
on particle ﬁlters for online nonlinear/non-gaussian bayesian tracking,”
Signal Processing, IEEE Transactions on, vol. 50, no. 2, pp. 174–188,
Feb 2002.
1377
Authorized licensed use limited to: National Taiwan University. Downloaded on September 18, 2009 at 04:48 from IEEE Xplore.  Restrictions apply. 
memory actualize high-resolution binary morphology,  the 
computation time is still limited by video resolution due to 
its pixel-by-pixel processing nature, i.e., computation cycle 
proportionally rises with video resolution. For example, 
performing binary morphology on a D1 (720x480) video 
requires about 345,600 cycles, even though the image is 
actually empty. 
Various hardware designs for CCL has been reported.  A 
classical approach is proposed by Rosenfeld and Pfaltz[7], It 
employed a two-path algorithm and a global equivalence 
table, which is later improved by Lumia et al. by storing 
equivalence table of a single-row[21].  Chien et al. 
introduced a sub-word concept which processes four pixels 
at a time to fully-utilize bus-bandwidth[10].   Similar 
multi-pixel solution is also found in [8]. A block of 2x2 
pixels is processed simultaneously and corresponding type 
of possible equivalences are analyzed to ensure its memory 
access reduction scheme.  However, increasing the size of 
block implies more complicate connectivity conditions, 
preventing the solution from higher-resolution extension. 
Moreover, pixel-by-pixel scan restrains further acceleration. 
Considering segmentation as a prior operation of intelligent 
video processing system, a better processing technique 
should have the following features: 
1. It is somehow aware of video content rather than 
blind operation and ensures efficient computation, 
thus reserving the hardware resource for other 
essential processing. 
2. It is memory-efficient and consumes bus-bandwidth 
to a limited extent. 
3. Support numerous general functions in video object 
segmentation. 
In this paper, a run-processing method is proposed to support 
high-resolution real-time video object segmentation.  
Making use of coherence property of binary masks, 
run-processing possesses content-awareness and reduces 
memory usage by 67%. Binary morphological operations, 
connected components labeling, seeded region growing, 
bitwise operation, and random-search operations are 
accelerated. Furthermore, run-processing is suitable for 
hardware implementation. Simulation shows that based on 
the proposed hardware architecture, a shadow-cancellation 
video object segmentation algorithm achieves an average 
speed of 151 frames per second at D1 (720x480) resolution. 
The structure of this paper is as follows: The succeeding 
section illustrates run-coding this paper adopts.  Next, 
Run-processing Functions section describes the methods of 
each operation with run-coding.  Then, the proposed 
architecture are demonstrated. Finally, experiment results 
are shown and conclusion is followed. 
Run-coding 
Coherence Property 
Since foreground (FG) binary mask attempts to store the 
existence of active object, ‘1’ pixels (pixels at where a object 
is found) of a object tends to be spatially connected into a 
closed region. Identical characteristic applies to ‘0’ pixels.  
This is called the coherence property of foreground binary 
masks. 
To achieve both coded-domain computation capability and 
storage reduction, row-based Run-coding is proposed. 
Run-coding Format 
Binary runs are of the form (X,Yb), which represents a series 
of pixel labeled as Yb starting from address X.  Fig.1 shows 
the run coded data of a binary mask.  The rows in a frame are 
then separately encoded into several runs.  The number of 
runs depends on the mask content.  (Nw,0) indicates the ‘end’ 
run of a row.  Where Nw is the length of a row; in this 
example Nw=25.  The encoding processing can be done by 
single raster scan. Moreover, independence between rows 
allows further acceleration through processing multiple rows 
in parallel. 
 
Figure 1 – Example of Run-coding (a)Pixel representation 
(b) Run-coded format 
X is log2Nw bits and Yb is 1 bit. Therefore, each run is 
(log2Nw+1) bits long and the storage requirement of a binary 
mask equals Nrun x (log2Nw+1). Nrun is the number of total 
runs in a frame.  For example, run-coded representation for 
720x480 binary masks of Fig.2(a) takes 1,858 runs. During 
segmentation process, erroneous patterns usually corrupt the 
masks, leading to increased run numbers.  Noisy masks like 
Fig.2(b) contains 4,636 runs, which is equivalent to  50.9Kb 
storage.  Even so, run-coding still achieves 85.3% storage 
reduction compared with 345.6Kb, the pixel-based storage 
size of a D1 image.  Experiments have shown an average of 
78% storage reduction in segmentation processes.  
 
Figure 2 – General binary mask (a)refined binary mask 
(b)noisy binary mask 
isolated ‘1’ runs in (i+1)th row. Go to step 3. 
3. Scan local equivalence table for each non ‘0’ runs in 
(i+1)th row; change run labels Gj into Kj. <Gj,Kj> is 
equivalent units in equivalence table.  If (i+1)th row 
is the last row, go to Bottom-up Process, otherwise i 
= i+1 and return to step 2. 
Bottom-up Process:  
1. Scan the ith and (i+1)th rows. Push <G,K> into 
equivalence table and assign G label to the upper-row 
run if lower-row G label run encounters a upper-row 
K label run; Go to step 2. 
2. Scan local equivalence table for each non ‘0’ runs in 
ith row; change run labels Gj into Kj.  If ith row is the 
first row, end the process, otherwise i = i-1 and return 
to step 1. 
The adopted CCL algorithm also contributes to 
bus-bandwidth reduction, which is later explained in 
Hardware Architecture section. 
Full-frame Bitwise Operations 
Recall that in binary morphology Inter-row process,  a ‘0’ 
run exists only if all of the corresponding B rows is ‘0’ at the 
address.  This is in essence the “OR” operation.  
Programmable input/output label inversion of Inter-row 
process allows all of the bitwise combinations such as 
And/Or/Nand/Nor/(MaskA & MaskB & ~MaskC).  For B 
input binary masks, 2B+1 modes are supported.  This implies 
a reusable hardware module. 
Seeded Region Growing (SRG) 
SRG is done utilizing CCL and seed insertion.  First a binary 
mask to be seeded is labeled by CCL. Then the seed mask is 
inserted into the label image, i.e., runs encountered with seed 
are labeled as ‘seed’ label (a fixed new label number). After 
the insertion, a confined CCL which only propagates ‘seed’ 
label is executed.  Finally unwanted regions are erased by 
label extraction and a binary mask of seeded region is 
obtained. 
Other Functions 
Other functions such as boundary extraction, bounding box 
extraction and random search can be done in similar 
procedures, which is out of the scope of this paper. 
Hardware Architecture of Run Processor 
The architecture of Run Processor is shown in Fig.5.  
Processing Element (PE) is implemented by two 
multi-functional modules because of the highly correlation 
between functions.  Instruction Decoder module decodes the 
input instructions and assigns the process.  External Memory 
Allocation module stores external starting address of data 
frames and currently available address of external memory 
data banks. 
 
 
Figure 5 – Architecture of Run Processor 
Run Data I/O 
Process Control module manages Row-to-PE linker 
according to function requirements.  Each of the 10 Row 
-storage can buffer Npagerun runs. Npagerun is set to 11.  The 
Row-storage serves as temporary buffer for read and write 
commands.  For instance, Inter-row morphology uses one 
row as output buffer and the others as input row buffer.  
After Inter-row morphology completes a row, output buffer 
is written out to internal storage and external storage and the 
next row of the B rows should be read in for the process.  In 
addition, the other rows should be rewind to the first run.  
Process Control module requests a read-in command to the 
new row and sends B-1 rewind commands to the other rows.  
Memory Access Unit reads data from external memory to 
the corresponding internal SRAM bank.  Fig.6 shows the 
internal SRAM and Row-storage structure. Each bank of 
internal SRAM is comprises 4 pages. The number is 
empirically chosen for the reuse scheme.  The first page of 
read data is then transferred to the new row buffer, which 
completes the data preparation for next process.   
If the row consists of more runs than row bank can hold, the 
address to the succeeding runs is kept in External Memory 
Allocation module for the next read-in. Using 10 Row 
Sub-storage, the maximum SE size of a single morphology is 
(Nw x 9). 
Internal Data Reuse Scheme 
Data are reused in two phases—page-reuse phase and 
bank-reuse phase.  During each operation, Process Control 
requests mostly rewind than read-in command because of 
the access pattern of windows-based functions.  If a 
Row-storage did not request a page in previous operation, 
the data in Row-storage is still the first page of the row, thus 
alleviating an internal read-in for rewind command.  
Likewise, page-reuse phase works if the row bank is not 
overwritten by further read-in, rewinding the row can be 
done simply by internal read-in instead of external memory 
access.  Furthermore, temporary data such as local 
equivalence table are not written out to external memory if 
possible.  42% reduction external transmission is reduced by 
the data-reuse scheme. 
binary masks of video object segmentation is proposed.  
Multiple widely-used functions such as binary morphology, 
connected component labeling and seeded region growing is 
supported.  A shadow-cancellation video object 
segmentation algorithm run by the presented hardware 
implementation achieves 151 fps for D1 (720x480) video. 
The presented hardware is shown to be more cost-effective 
than existing architectures. 
References 
[1]  K. I. Diamantaras, S. Y. Kung 1997. A linear systolic 
array for real-time morphological image processingIn 
Journal of VLSI Signal Processing: 43 – 55. 
[2] E. N.  Malamas, A. G. Malamos, T. A.Varvarigou 2000.   
Fast implementation  of  binary  morphological  
operations  on  hardware-efficient  systolic  architectures.  
In Journal of VLSI Signal Processing: 79 – 83. 
[3] S.-Y. Chien, B.-Y.  Hsieh, Y.-W. Huang, S.-Y. Ma, L.-G. 
Chen 2006. Hybrid morphology processing unit 
architecture for moving object segmentation systems. In  
Journal of VLSI Signal Processing: 275-278. 
[4] Hugo Hedberg, Fredrik Kristensen, Viktor Öwall 2008. 
Low-Complexity Binary Morphology Architectures 
With Flat Rectangular Structuring Elements. In IEEE 
Transactions on Circuits and Systems I, 2216-2225. 
[5] S. Fejes, F. Vajda 1994. Data-driven algorithm and 
systolic architecture for image morphology. In IEEE 
International Conference of Image Processing, 550-554. 
[6] J. Velten and A. Kummert 2002. FPGA-based 
implementation of variable sized structuring elements for 
2-D binary morphological operations.  In Proceedings of 
1st IEEE International Workshop Electronic Design, 
Test, Application, 309–312. 
[7] A. Rosenfeld and J. L. Pfaltz. 1966. Sequential 
Operations in Digital Picture Processing. In Journal of 
the ACM (JACM) archive. Volume 13, Issue 4. 
[8] Flatt, H., Blume, S., Hesselbarth, S., Schunemann, T., 
Pirsch, P. 2008. A Parallel Hardware Architecture for 
Connected Component Labeling Based on Fast Label 
Merging. In International Conference on 
Application-Specific Systems, Architectures and 
Processors, 144-149. 
[9] Appiah, K., Hunter, A., Dickinson, P., Owens, J. 2008. A 
Run-Length Based Connected Component Algorithm for 
FPGA Implementation. In International Conference on 
Field-Programmable Technology, 177-184. 
[10] Wei-Kai Chan, Shao-Yi Chien 2006. Subword 
Parallel Architecture for Connected Component 
Labeling and Morphological Operations. In IEEE Asia 
Pacific Conference on Circuits and Systems, 936-939. 
[11] C. J. Nicol 1995. A Systolic Approach for Real 
Time Connected Component Labeling “, In Source, 
Computer Vision and Image Understanding 
archive. ,Volume 61, Issue 1. 
[12] R. M. Haralick 1981. Real time Parallel 
Computing Image Analysis, Mass: Plenum Pub Corp. 
[13] J. Park, C. G. Looney, H. Chen 2000. Fast 
Connected Component Labeling Algorithm Using A 
Divide and Conquer Technique, Technical report. 
[14] A. Rosenfeld and J. Pfaltz. 1966. Sequential 
operations in digital picture processing.  In Journal of the 
ACM, 241–242. 
[15] Yang, X.D. 1988. Design of Fast Connected 
Component hardware. In  Computer Society Conference 
on Computer Vision and Pattern Recognition, 937 – 944. 
[16] Schwartz, J.T., Sharir, M.,  Siegel, A., 1985. An 
Efficient Algorithm for Finding Connected  Components 
in a Binary Image.  Technical Report No.  154, Courant 
Institute, NYU. 
[17] Thomas M. Breuel 2008. Binary Morphology and 
Related Operations on Run-Length Representations. In 
Proceedings VISAPP. 
[18] Lumia, R.,  Shapiro, L.,  Zuniga, O. 1983. A  New  
Connected  Components  Algorithm for  Virtual Memory 
Computers.   In IEEE Transactions on Computer Vision, 
Graphics, and Image Processing, 22, 287-300 
[19] N. Ranganathan,  R. Mehrotra,  S. Subramanian 
1995.  A high speed systolic architecture for labeling 
connected components in an image. In IEEE Trans. Syst., 
vol. 25, 415 – 423. 
[20] Hugo Hedberg,  Petr Dokladal, Viktor Öwall 
2009. Binary Morphology With Spatially Variant 
Structuring Elements: Algorithm and Architecture. In 
IEEE Transactions on Image Processing, 562- 572. 
[21]  Parks, D.H., Fels, S.S. 2008. Evaluation of 
Background Subtraction Algorithms with 
Post-Processing. In International Conference on 
Advanced Video and Signal Based Surveillance, 192 - 
199  
Digital Object Identifier 10.1109/AVSS.2008.19 
[22] Dong Xu, Jianzhuang Liu, Zhengkai Liu, Xiaoou 
2004. Tang Indoor shadow detection for video 
segmentation. In IEEE International Conference on 
Multimedia and Expo, 41-44, Vol.1. 
background subtraction [6], [7] and [8], and those that 
don’t [1] and [2]. Background subtraction works 
reasonably well when the camera is stationary and the 
change in ambient lighting is gradual, if at all. For those 
that do without background subtraction, a set of 
discriminative features of the objects of interest has to be 
learned beforehand through machine learning algorithms 
in order to be able to detect these objects in subsequent 
stages. 
The majority of existing event detection methods 
incorporates tracking algorithm of some form in their 
system, as in [2], [3], [6] and [7]. In most cases tracking is 
performed on all detected moving objects or blobs in the 
foreground. However, due to occlusion and a fixed camera 
angle, this kind of comprehensive tracking often results in 
errors such as identity switch (when two nearing objects 
switch their identities), which is difficult to avoid and can 
be seen in many PETS 2006 demonstration sequences 
such as those in [5]. 
In most cases surveyed, the final stage of determining 
whether an alarm should be issued is done in a 
deterministic fashion. In a deterministic system an event is 
declared to have occurred if some criteria are satisfied. A 
minority employs a probabilistic framework [3] to model 
events, in which case an event is deemed to have occurred 
if its confidence score exceeds a certain threshold. The 
probabilistic approach gives users more flexibility to set 
thresholds and hence system sensitivity, as well as a better 
understanding of how real the situation might actually be. 
1.2. Outline of our approach 
Our proposed approach employs a novel technique, 
which we shall refer to as foreground-mask sampling, to 
localize the candidates of abandoned luggage items in the 
scene. As the first stage of our system, the foreground-
mask sampling technique computes the intersection of a 
number of background-subtracted frames which are 
sampled over a period of time. Abandoned luggage items 
are assumed to be static foreground objects and therefore 
will show up in this intersection. Since our approach 
requires no prior learning of luggage appearance in any 
form, we can successfully localize luggage of all shapes, 
sizes, orientations, viewing angles and colors with no need 
for and no constraints from any training data. Once a 
suspicious luggage item is identified and localized, our 
algorithm attempts to search for its owner within a 
neighborhood around the detected luggage. If the owner is 
found within this neighborhood, the luggage is assumed to 
be attended by its owner and no further processing is 
required. 
However, if no owner is found in proximity to the 
luggage in this present frame at time t, our tracking 
algorithm then goes back in time (for a pre-defined length 
of Δt seconds) to the frame at time t – Δt when the owner 
was still attending the luggage, and it starts tracking the 
owner from then. The tracking algorithm utilizes motion 
prediction in conjunction with (1) skin color information 
and (2) an improved version of generalized Hough 
Transform on human body contour as feature. Rather than 
comprehensively tracking all moving foreground objects, 
which is normally done in most existing event detection 
systems, we track only the owner of the suspicious 
luggage item which has been localized in the first stage; 
other irrelevant moving objects in the foreground are 
simply ignored. We call our method selective tracking, as 
opposed to conventional comprehensive tracking.  
The tracking module provides a trajectory of the 
luggage owner from frame t – Δt to frame t, and this 
information is used for probabilistic reasoning in the third 
stage. For the luggage to become abandoned, its owner has 
to leave the scene without it, or it has to remain unattended 
for at least 30 consecutive seconds. A probability score 
will be given by the tracking algorithm to represent the 
reliability of the owner’s tracked trajectory, and this 
probability score is used in the subsequent evaluation of 
the overall confidence score of the luggage-abandonment 
event. The event detection is formulated as a Maximum A 
Posteriori (MAP) problem. Finally an alarm will be 
Owner left scene or 
more than 30 seconds
Within 30 seconds 
Owner not found in 
present frame 
Owner 
found
Static object is luggage 
Foreground static object is human 
Input 
video 
frames 
Foreground
-mask 
sampling 
Discard track
Search for owner in neighborhood
Discard 
track 
Back-tracking for 
past 60 seconds Discard track 
Luggage abandoned; alarm 
triggered 
Figure 1. Flow chart of our proposed approach 
 
1303
Authorized licensed use limited to: National Taiwan University. Downloaded on September 18, 2009 at 04:50 from IEEE Xplore.  Restrictions apply. 
 
Binarization allows the intersection to be taken through 
simple point-wise multiplication of the 6 foreground 
masks, as indicated by the operator ‘.*’. Filtering 
operation is then carried out on this static foreground 
object mask S to remove irrelevant and sporadic noisy 
pixels, and connected component analysis is performed 
afterwards. A white (valued 1) block on the static 
foreground object mask S indicates a region that has 
remained foreground in all of the 6 sample frames over the 
past 30-second period, and therefore this region should 
very likely correspond to either a static abandoned luggage 
item or a non-moving human being. Our tracking module, 
which is to be detailed in the next section, will then 
analyze the region and further localize it if it is determined 
to be a static luggage item, see Figure 3. 
The static foreground object mask S thus obtained by 
the foreground-mask sampling gives possible candidates 
for abandoned luggage items. The approach is elegant and 
robust in that it manipulates directly such low-level 
features as foreground and background image pixels. This 
provides us with a localized target and allows us to focus 
on a localized search region for later tracking and higher-
level event reasoning.  
3. Selective tracking module 
With the static foreground object mask S obtained, our 
system has localized information on where the suspicious 
objects are in the scene. It should also be pointed out that 
here we assume all static foreground objects to be either 
human or luggage item. For each white region (valued 1) 
in the static foreground object mask S, our algorithm 
checks if it is a human or luggage (i.e. not human) by a 
combination of skin color information and human body 
contour that shall be explained in detail shortly. If the 
region is determined to be a human, it is discarded because 
what we are looking for is abandoned luggage items. If it 
is a luggage item, a local search region is constructed 
around the detected luggage’s neighborhood to see if its 
owner is in close proximity in this present frame at time t. 
If the owner is found, the region is again discarded 
because the owner exhibits no present intention of 
abandoning the luggage. If, however, the owner is not 
found around the luggage in the present frame, our 
algorithm then goes back in time for a pre-defined Δt 
seconds, 60 in this case, to the frame at time t – Δt when 
the owner was still attending the luggage and starts 
tracking the owner from here (at time t – Δt). The tracking 
algorithm again employs skin color information and 
human body contour as features. 
Because suspicious luggage has already been localized 
by foreground-mask sampling in the first place, we are 
able to perform tracking solely and selectively on the 
owner of this static luggage item. This mechanism closely 
mimics the human ability to notice and track only the 
object that is of interest to us even under a highly cluttered 
background, for example humans’ natural ability to 
identify familiar faces in such crowded space as an airport 
pick-up area. Our ability to track only the object we are 
interested in also reduces the risk of identity switch that is 
difficult to avoid if tracking is performed on a 
comprehensive, full-frame scale. 
The details on the implementation of detection and 
tracking using skin color information and human body 
contour are described below, as well as their integration 
into the motion prediction part of the tracking module.   
3.1. Cr color channel with human skin 
Human skin signal response is significantly larger in the 
YCbCr color space than in the commonly used RGB color 
space. Due to a large amount of blood flow, human skin 
gives high response to the Cr channel in YCbCr space, 
irrespective of skin color or race [4]. We propose to utilize 
skin color as given by the Cr channel for human face 
localization because in situations of severe occlusion 
(crowded scenes with people overlapping one another), 
human face is the most visible body part under a typical 
surveillance camera with a tilting angle looking down 
from top. 
A search region is first constructed around the 
Figure 3. Foreground-mask sampling. The first row shows input frames; the second row shows corresponding foreground images. 
Image on right is the intersection of the 6 foreground images sampled over a period of 30 seconds, which contains the abandoned 
luggage item. AVSS 2007 video dataset. 
1325
Authorized licensed use limited to: National Taiwan University. Downloaded on September 18, 2009 at 04:50 from IEEE Xplore.  Restrictions apply. 
(m+1)th bin with a width of Δm = 5
access a total of 11 (=1+2Δm) bins cent
bin with their respective weights giv
distribution g, where g = 1 for the ce
before and 0 < g < 1 for other 10
decreasing with respect to distance fr
these other 10 bins, (xC’, yC’) is also co
α) pair under these bins and the corresp
the detection map is incremented b
weight g (smaller than the center weigh
6 below for an illustration.  
The reason for making this modifica
angle that is computed from an edge 
edge image, there is an inherent e
quantization and angle quantization, an
obtained is at best indicative only o
neighboring angles. We model this ra
Gaussian-weighting system on a range 
range being specified by Δm = 5. A
angle to vary within a limited rang
handle human head-shoulder contours 
of alignment with perfectly frontal imag
Following the above procedure, onc
the input edge image have been trave
center point of the contour of interes
image will correspond to a local maxim
map. See Figure 7 for the detection re
improved version of Hough Transfo
with the original implementation of 
introduced in [10] and a simple norm
Figure 6. The origin point of the two red li
m degrees; it corresponds to the red, 
contains two (r, α) pairs. Solid lines a
while dashed lines indicate noise. As sho
on the head-shoulder contour in the 
converge to a local maximum on the de
right. At the bottom is the 180-bin referen
weighting g is shown below it with the c
red and neighboring 10 bins in blue.  
. Specifically, we 
ered on the (m+1)th 
en by a Gaussian 
nter (m+1)th bin as 
 neighboring bins, 
om center bin. For 
mputed for each (r, 
onding location on 
y the bin’s given 
t of 1). See Figure 
tion is that for a ψ 
point on the input 
rror due to pixel 
d thus the ψ angle 
f a small range of 
nge by applying a 
of ψ-angle bins, the 
lso, by allowing ψ 
e, our system can 
that are slightly out 
e. 
e all edge points on 
rsed, the supposed 
t in the input edge 
um in the detection 
sults given by our 
rm, in comparison 
Hough Transform 
alized correlation 
technique for feature detection
shown to be superior to the l
contour detection operation is s
search region, interference fro
reduced to a minimum. The gen
Transform that we use here eff
contour of our choice (head-s
case) to a large-valued point (or
bright points due to pixel quanti
3.3. Integration into motion
For detection of luggage ow
color information from Cr ch
contour information from our im
algorithm are combined to pin-
the owner. 
To further exploit the temp
successive frames, motion 
Prediction of the owner’s loca
based on its location in the cur
frames with exponentially-deca
if we denote r(t) as the position 
t, the prediction for time t+1 can
 
r(t+1) = r(t) + △r                   
 
where △r is generated recursiv
and is given by 
 
△r = α △r + β( r(t) – r(t-1) )  
 
α + β = 1.                               
 
nes has a ψ angle of 
(m+1)th, bin which 
re correct matches, 
wn, different points 
input edge image 
tection map on the 
ce table; a Gaussian 
enter bin labeled in 
Figure 7. Upper row: (left) input v
edge image by 3x3 sobel convolu
HT detection map superimposed
implementation. Lower row: (left)
result with our improved implemen
response at the correct location
shoulder contour in the input edg
result with original implementatio
false positives; (right) simple im
correlation between the input edge
which gives the most false positive
. Our improved method is 
atter two. And since this 
olely performed within the 
m irrelevant contours is 
eralized version of Hough 
ectively maps an arbitrary 
houlder silhouette in this 
 a small region of scattered 
zation).  
 prediction 
ner in a single frame, the 
annel and the upper-body 
proved Hough Transform 
point the head location of 
oral relationship between 
prediction is employed. 
tion in the next frame is 
rent frame and in the past 
ying weights. Specifically, 
vector of the owner at time 
 be formulated as 
                                   (4) 
ely by motion prediction 
 and                            (5) 
                                   (6) 
ideo frame; (middle) the input 
tion; (right) input frame with 
 on it using our improved 
 Hough Transform detection 
tation, which gives one large 
 of the most visible head-
e image; (middle) detection 
n of HT, which gives more 
plementation of normalized 
 image and contour template, 
s, as expected. 
1347
Authorized licensed use limited to: National Taiwan University. Downloaded on September 18, 2009 at 04:50 from IEEE Xplore.  Restrictions apply. 
sequences recorded with different difficulty levels: easy, 
medium and hard. The easy sequence contains objects of 
larger appearance, activities which are closer to the 
camera and less scene clutter; as the difficulty level rises, 
objects become smaller and clutter more serious. Our 
proposed 3-stage approach has successfully detected the 
abandoned luggage in all three sequences from the AVSS 
2007 dataset. We have been able to track the owner in the 
easy sequence all the way until he leaves the scene without 
the luggage, hence resulting in an alarm event. In the 
medium and hard sequences, however, the owner passes 
behind a large pillar before leaving the scene without the 
luggage, and therefore is occluded for about 1.5 seconds, 
which translates into around 40 frames under a frame rate 
of 25 fps. Our tracking engine has not been able to follow 
the owner through the occlusion and the owner is deemed 
to be lost; therefore alarms are also triggered for these two 
sequences. The PETS 2006 datasets contains seven 
sequences. In video 1, 2, 4, 5 and 6 the luggage owner 
leaves the scene without the luggage, and our method has 
successfully issued an alarm in all these 5 cases while 
tracking the owner all the way until the owner is no longer 
within camera view.  In video 3 the owner stays with the 
luggage all the time, and therefore no alarm is issued. In 
video 7, the owner wanders about for some time before 
finally leaving the scene without luggage; the trajectory of 
the highly-maneuvering owner, however, contains too 
many abrupt changes in speed and direction for our 
present motion prediction algorithm to successfully 
follow. The owner is lost 34 seconds after he leaves the 
luggage, while an alarm is triggered at 30 seconds. In 
Figure 8, some labeled scene shots are provided.  
6. Conclusion and future work 
In this paper, we have proposed a novel approach to 
left-luggage detection in surveillance video. Through the 
use of foreground-mask sampling, we are able to emulate 
the human vision capability of localizing and focusing on 
solely the object of interest to us, while filtering out all 
other irrelevant, interfering agents. We are therefore able 
to apply tracking in a selective, more localized manner. 
We have also proposed an improved implementation of 
the Hough Transform for detecting the human upper-body 
contour from the video frames. And we have incorporated 
a probabilistic framework and employed the MAP 
principle in our modeling of the luggage-abandonment 
event and subsequent reasoning. In the future, we plan to 
extend our proposed approach to a multi-camera network 
where coordination of an array of cameras will allow cues 
to be gathered from multiple views and information to be 
relayed from one to another.   
7. References 
[1] B. Wu and R. Nevatia, “Detection of Multiple, Partially 
Occluded Humans in a Static Image by Bayesian 
Combination of Edgelet Part Detectors”, ICCV 2005, IEEE, 
Vol I: 90-97  
[2] B. Wu and R. Nevatia, “Tracking of Multiple, Partially 
Occluded Humans based on Static Body Part Detection”, 
CVPR 2006, IEEE, Vol I: 951-958  
[3] F. LV, X. Song, B. Wu, V. K. Singh, R. Nevatia, “Left-
Luggage Detection using Bayesian Inference”, 9th PETS, 
CVPR 2006, IEEE, pp. 83-90  
[4] Kumar, C. N. Ravi and Bindu. A, “An Efficient Skin 
Illumination Compensation Model for Efficient Face 
Detection”, IECON 2006, IEEE, pp. 3444-3449 
[5] K. Smith, P. Quelhas and D. Gatica-Perez, “Detecting 
Abandoned Luggage Items in a Public Space”, 9th PETS, 
CVPR 2006, IEEE, pp. 75-82 
[6] J. Martínez-del-Rincón, J. Elías Herrero-Jaraba, J. Raúl 
Gómez, and C. Orrite-Uruñuela, “Automatic Left Luggage 
Detection and Tracking Using Multi-Camera UKF”, 9th  
PETS, CVPR 2006, IEEE, pp. 59-66 
[7] L. Li, R. Luo, R. Ma, W. Huang, K. Leman, “Evaluation of 
An IVS System for Abandoned Object Detection on PETS 
2006 Datasets”, 9th PETS, CVPR 2006, IEEE, pp. 91-98 
[8] J. Zhou, J. Hoang, “Real Time Robust Human Detection 
and Tracking System”, CVPR 2005, IEEE, Vol III: 149-149 
[9] P.V.C Hough Method and Means for Recognizing Complex 
Patterns, US Patent 3,069,653, December 1962 
[10] R. O. Duda, R. E. Hart, Use of the Hough Transform to 
Detect Lines and Curves in Pictures, CACM(15), No. 1, 
January 1972, pp. 11-15 
Figure 8. Sequence 1, 3 and 4: (from left) static luggage detected; owner tracking starts; owner leaves the scene, alarm triggered. 
Sequence 2: static luggage detected; owner tracking starts; owner lost due to occlusion, alarm triggered. Sequence 1 and 2 are 
from AVSS 2007 dataset; Sequence 3 and 4 from PETS 2006 dataset. 
1369
Authorized licensed use limited to: National Taiwan University. Downloaded on September 18, 2009 at 04:50 from IEEE Xplore.  Restrictions apply. 
C
or
re
sp
on
de
nc
e 
R
es
ul
ts
SCL
Homography MapAll video parameters
Object
Segmentation
Connected 
Component 
Labeling
TCLVideo Sequence 0
Object
Segmentation
Connected 
Component 
Labeling
TCLVideo Sequence N
Fig. 1. Spatial-temporal consistent labeling flow.
background object. As indicated in this example, one main
issue for object tracking is how to know where objects are
during merging, splitting, and reappearing.
The main concept of the proposed consistent labeling al-
gorithm is taking each merged mask as a merged object and
recording which single objects are in it. As in Fig. 1, TCL
takes charge of the labeling problem throughout consecutive
frames for each camera. Then SCL solves the labeling problem
across different views with object information from TCL. The
two consistent labeling methods are detailed in the following
sections.
A. TEMPORAL CONSISTENT LABELING
The block diagram of TCL is shown in Fig. 2. Every mask
in the current frame needs to be compared with the objects
saved in a history database. To match a mask with objects
in history, four features are extracted to find their similarity.
Here we use two different pass thresholds for on-merging
masks and other masks. This is because an on-merging mask
should be considered as a new merged object. The mask
should not be recognized as any single object in the database.
These false recognitions happen when one object is too small
compared to the other before they are merged, the bigger mask
will dominate the on-merging mask and then the similarity
checking result. Most of the time, the on-merging mask will
match to one of single objects in history if we do not tighten
the pass threshold. Hence before object matching stage, the
merge/split condition should be detected first to make the
matching result favor whether creating a new merged object
or selecting an existed object. A merge condition is valid if
one mask in the current frame overlaps multiple objects in the
previous frame. On the other hand, a split condition is valid if
one object in the previous frame overlaps multiple masks in
the current frame.
Object matching decision tree is shown in Fig. 3. A mask
is matched to one history object with rules based on the
following features, color histogram distance, overlapped area
size, mask size ratio, and mask centroid distance. Sometimes,
there may be multiple matched candidates, and the object with
the smallest centroid distance is selected. If no candidates
pass all criteria, there would be three kinds of conditions:
one is a new object appears; another is the mask is a new
on-merging object; the other is the mask appearance changes
too fast and fails certain rules. To differentiate which situation
happens, the number of objects appearing in the last frame
which are overlapped by the current mask indicates the result.
If no previous object is overlapped, the mask is identified as
Merge/Split Condition
Object Matching
Merged Object Update
No Longer Existent Object Removal
Object Masks
Updated Object Database
Fig. 2. Temporal consistent labeling block diagram.
Yes No
Yes No 0 Multiple1
Similarity Match
Check
History
Object
Update
Overlapped 
NumberMulti-match?
Minimum 
Distance
Object 
New
Single
Object
New
Merged
Object
Fig. 3. Object matching decision tree.
a new appearing object. If multiple objects are overlapped,
the mask is identified as a new on-merging object. If mask
overlaps only one object, it is considered as an object with
fast changing appearance.
After matching all current masks with history objects, some
single objects will reappear. If the reappearing objects come
from merged objects, the record of which contained objects
in the merged objects should be updated. If the number of
merged objects of a merged object reduces to zero or one,
that means this merged object no longer exists and should
be wiped out. Although one mask may be matched to the
merged object but not the single object before merge, due to
the mask changes a lot during the merging time, this error will
be compensated with the step of removing reappearing single
objects from merged objects. Finally, to keep a database in
a reasonable size, those objects not included in any merged
objects and disappearing for a predefined length of time need
to be removed from database.
B. SPATIAL CONSISTENT LABELING
With the assumption that each view should have common
ground in most surveillance cases, the proposed algorithm for
spatial consistent labeling is based on ground plane homogra-
phy transformation. Homography transforms the coordinates
of a ground point of an object in one view to the coordinates
in the other. An example is shown in Fig. 4. Given some
matched pairs, we can derive the ground plane warping
matrix. The performance of the homography-based consistent
labeling significantly depends on the segmentation result and
the ground point decision. We still assume the bottom center
of a bounding box of an object is its ground point. However,
for a mask of a person, the shadow in the mask may shift the
true ground point, and for a mask of a vehicle, the different
view may have totally different ground points. Two concepts,
3531
Authorized licensed use limited to: National Taiwan University. Downloaded on September 18, 2009 at 04:49 from IEEE Xplore.  Restrictions apply. 
TABLE I
SPATIAL AND TEMPORAL CONSISTENT LABELING RESULTS
Test Sequences View TP OP FIT(FIO) FIP
PETS2001
(Outdoor)
1 99.32% 94.58% 0.0057
2 98.34% 81.25% 0.0500
Avg. 98.81% 87.23% 0.0279 0.2168
Ming-Da Hall
(Indoor)
1 77.43% 78.94% 0.1810
2 87.37% 84.34% 0.0076
Avg. 82.40% 81.64% 0.0943 0.0294
Barry Lam
Hall
(Outdoor)
1 100.00% 90.55% 0.0000
2 100.00% 88.57% 0.1199
3 100.00% 86.47% 0.0000
4 100.00% 96.74% 0.0000
Avg. 100.00% 90.58% 0.0300 0.0763
Fig. 6. Spatial consistent labeling results. The green car contains green and
red tags because the driver goes into the car previously. The merged mask
in the first(left) view contains green, red, and blue tag which represents the
driver, the green car, and the dark blue car in turn. If an object in the first
view is a single object, its transformed ground point with the same color tag
is shown in right view.
are generated from bad foreground masks and the situation
when objects enter the view but occluded by others. Running
with Intel Core 2 Duo processor at 2.8 GHz, the SCL speed
is about 4,638 fps per one pair of CIF size video channels.
The overall speed is about 10.24 fps.
IV. CONCLUSION
An effective TCL and SCL algorithm is proposed in this
paper. The objects within an occluded mask need not to be
separated to make our TCL and SCL work fine. Homography
ground plane warping is used for deriving SCL. EMD is
utilized to do object matching, and the concept of trusting-
former-pairs-more is applied to generate the matching pairs
across temporal and spatial domain correctly and stably. FIP
metric is also defined for an objective judgement on the
performance of the proposed SCL algorithm. As indicated in
the experimental results, the FIP is 0.1075 which means
about 89.25% of pairs are correctly identified. For online
processing applications, the proposed algorithm need not trace
back to the past frames.
REFERENCES
[1] J. Krumm, S. Harris, B. Meyers, B. Brumitt, M. Hale, and S. Shafer,
“Multi-camera multi-person tracking for easyliving,” in IEEE Interna-
tional Workshop on Visual Surveillance, 2000, pp. 3–10.
[2] A. Mittal and L. Davis, “M2tracker: A multi-view approach to segmenting
and tracking people in a cluttered scene using region-based stereo,” in
European Conference on Computer Vision, 2002, pp. 18–36.
t
First view Second view
(a)
t
First view Second view
(b)
Fig. 7. Two pair renew examples. Fig. 7(a) shows that a man is leaving a
car. Fig. 7(b) shows that a car enters left view.
[3] W. Hu, M. Hu, X. Zhou, T. Tan, J. Lou, and S. Maybank, “Principal axis-
based correspondence between multiple cameras for people tracking,”
IEEE Trans. Pattern Anal. Machine Intell., vol. 28, no. 4, pp. 663–671,
2006.
[4] S. Khan and M. Shah, “Consistent labeling of tracked objects in multiple
cameras with overlapping fields of view,” IEEE Trans. Pattern Anal.
Machine Intell., vol. 25, pp. 1355–1360, 2003.
[5] Y. Rubner, C. Tomasi, and L. J. Guibas, “The earth mover’s distance as
a metric for image retrieval,” Stanford, CA, USA, Tech. Rep., 1998.
[6] “Pets2001 dataset,” http://www.cvg.cs.rdg.ac.uk/PETS2001/pets2001-
dataset.html.
[7] K. Smith, D. Gatica-Perez, J. Odobez, and B. Sileye, “Evaluating multi-
object tracking,” in IEEE Computer Society Conference on Computer
Vision and Pattern Recognition, vol. 3, 2005, pp. 36–43.
3533
Authorized licensed use limited to: National Taiwan University. Downloaded on September 18, 2009 at 04:49 from IEEE Xplore.  Restrictions apply. 
 國科會補助計畫衍生研發成果推廣資料表 
日期： 2011  年 10 月 31 日 
國科會補助計畫 
計畫名稱：智慧型多攝影機監控系統及架構研究 
計畫主持人：陳良基         
計畫編號：NSC97-2221-E-002-173-MY3     領域：E1204 
研發成果名稱 
（中文）以視覺為基礎之人類動作辨識的演算法及架構分析 
（英文）Algorithm and Architecture Analysis of Video-based 
Human Action and Activity Recognition 
成果歸屬機構 國立台灣大學 發明人 (創作人) 
陳良基，梁家鈞，張靖
瑩，王子恆 
技術說明 
（中文） 
以視訊為基礎的人類行為辨識技術提供了許多與電腦視覺相關的
重要應用，包含多媒體娛樂、安全監控、互動式環境、視訊分析
與生物行為特徵等等。行為辨識演算法和系統中，最重要的挑戰
來自人類與機器間的語義鴻溝，需要擷取有意義的物體特徵和設
計有效的運動模型，來讓電腦正確理解畫面中所呈現的真正運動
含義。第一個挑戰是物體特徵的選擇，這些特徵不只要能有效的
用來區分各種行為，還必須能在真實環境中，包含雜訊、遮蔽和
陰影的影響之下，都還能被取得並正確表達，因為只要有任何的
誤差，都會導致後面運算的結果錯誤。第二個挑戰是建立人類的
行為模型，這個模型必須精準地描述且能區別不同行為的差異。
此外，該模型描述多少局部和全域特性，模型本身的維度大小的
決定，都要考慮動作本身的性質和測試資料的多寡。本技術以無
控制器的遊戲平台和遺棄行李的偵測系統，來分析如何設計以視
覺為基礎的人類動作辨識。其中包含兩個部分第一部分討論如何
取得大部分行為辨識的共同特徵，即物體的軌跡，而第二部分針
對這兩個應用討論如何設計行為模型。 
 
附件三 
技術移轉可行性及預期
效益 
促進高準度之視覺動作辨識之研究發展與技術開發。此技術可經
由技術轉移至影像感測器、智慧型監控系統及相關動作辨識產業
提供高準度動作辨識演算法及硬體架構平台。 
     註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
是 Does ASR have a PHD, or is it just Piled Higher and Deeper。內容主要在介紹自
動話語音辨識 automatic speech recognition (ASR) 的回顧與未來。他提到過去 
speech recognition 的發展上重要的核心想法，比如說聲音特徵擷取的 MFFC 
PLP， 聲音模型的 HMM 等，大都在 1991 以前就已經建立了基本的理論了。而
一直到 2011 年，這些核心還是沒有改變。暗示了語音領域在 20 年內方法上的突
破是有限的。在這相對「成熟」的領域，未來可能需要新的概念來引導。提出未
來的研究重點可能在於辨識及理解整段話或文章的場景。第四天的主題演講是機
器學習演算法領域的麥可喬丹，UC Berkeley 的 Prof. Michael I. Jordan 教授主
講。題目是 Bayesian Nonparametrics for Speech and Signal Processing。這個主題
研講比較像是一般的 Lecture，介紹 Bayesian Nonparametrics 的一些數學理論和使
用在語音及訊號處理上的一些結果。藉由演算法 non-parametric 的特色，不會受
到事先給定模型參數的限制，在為來極有可能在相關領域扮演重要的角色。 
 
這次大會在每天的中午首創了一個 Penal List 的活動，叫做 Expert Summaries 
of Current Trends。四天中的前三天中午，每天有三到四個研究領域為主題，請
Technical Committees 中的佼佼者，題供開場，隨後讓與會者發問引導，展開討
論。我每個中午都有選擇一個有興趣的場次去參加，但由於時間很短，大部分講
者開場完就沒剩多少時間了，因此討論都不太深入，是稍嫌可惜之處。 
 
ICASSP 向來以盛行的討論風氣所著名，本次大會也不例外。在海報區，聲
音及語音的論文一如往常的都會擠滿人，作者幾乎都不會有空下來的時間。這次
大會特別的是，compressed sensing 相關的題材，因為是最近很熱門的題目，所
以大爆滿。在我們論文所屬的生醫訊號處理方面，雖然人次沒有上面兩個主題那
麼多，但也是很難有閒下來的機會。即使是不同領域的人也都會來詢問了解我們
研究的內容，可見這個會議主辦的用心及參予研究人員好學的風氣。 
 
本次發表的論文為“Design and Implementation of Cubic Spline Interpolation 
有差異，所以一開始準備及演練的時候，就要先設想很多不同的狀況來加以準
備。這次報告由於有充分的準備，所以報告時普遍反應良好。同時不同領域的人
也會給予一些建議，推薦一些不同應用上常用的處理方法，認為應該可以有效的
使用於我們的研究上，可說是收穫不小。 
四、 攜回資料名稱及內容 
 
1. 會議論文集 USB 隨身碟一支，內含所有發表論文； 
2. 會議論文簡介手冊一本 
 
五、 與會照片 
 
大會門口特別的導引指標 
Expert of current trends 演講完後與學生合照。 
 
 
大會 Poster Session 的討論風氣很盛 
 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/23
國科會補助計畫
計畫名稱: 智慧型多攝影機監控系統及架構研究
計畫主持人: 陳良基
計畫編號: 97-2221-E-002-173-MY3 學門領域: 訊號處理
無研發成果推廣資料
'09. International 
Conference on, 2009. 
3. H.-H. Liao, J.-Y. Chang, 
and L.-G. Chen, ’’A 
Localized Approach to 
Abandoned Luggage 
Detection with 
Foreground-Mask 
Sampling,’’ in Advanced 
Video and Signal Based 
Surveillance, 2008. AVSS 
'08. IEEE International 
Conference on, 2008, pp. 
132-139. 
4. J.-Y. Chang, T.-H. Wang, 
S.-Y. Chien, and L.-G. 
Chen, ’’Spatial-temporal 
consistent labeling for 
multi-camera multi-object 
surveillance systems,’’ 
in Circuits and Systems, 
2008. ISCAS '08. IEEE 
International Symposium on, 
2008, pp. 3530-3533. 
 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次
 
其他成果 
(無法以量化表達
之成果如辦理學術
活動、獲得獎項、
重要國際合作、研
究成果國際影響力
及其他協助產業技
術發展之具體效益
事項等，請以文字
敘述填列。) 
本研究成果已有四篇相關論文發表，以及三篇期刊論文送審中，其中包含 ICME 國
際會議論文，TCSVT、TCE 等著名國際期刊。本研究成果整合運動模型、多攝影機
關係模型、物體追蹤等用在行為辨識系統上的核心模組，根據他們的運算特性及性
質，設計高效率的硬體架構和高準確度的演算法。可供後續研究者參考並延伸其性
能和應用。本研究成果可實際應用於監視系統及需要動作行為辨識之人機互動、動
作感知系統，因此可以實際應用於公共安全系統或是消費性電子產業之上，提供產
業界在未來發展的後盾。此外本研究成果提出之硬體架構設計可以應用於相關電子
產業及 IC 設計產業，提供一有效率之動作辨識核心模組。此研究成果更是反應未
來人機互動、動作辨識感知等相關趨勢，未來會有更多相關研究議題因應而生。此
研究成果對於產業界已有進一步影響，其中的智慧型多物體追蹤之多攝影機數位視
訊監控系統模組已經技轉於相關產業。 
 成果項目 量化 名稱或內容性質簡述 
