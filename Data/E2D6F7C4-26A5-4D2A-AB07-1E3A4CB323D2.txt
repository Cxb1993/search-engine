- 2 - 
16%) [13]。以下對於此三篇代表作進行說明： 
Selected Publication #1: OneClick: A Framework for Measuring Network 
Quality of Experience [16] 
在這篇論文中我們提出一個新的方法來量測即時網路應用程式（e.g., VoIP, 視
訊會議, 線上遊戲）提供給使用者的滿意度（QoE, Quality of Experience）。使
用這個方法，使用者毋須經過訓練，在評分時也不用思考，只要在感覺到應
用程式品質不良時按一下按鈕即可。我們的方法就會 summarize在各種不同
網路品質下的 QoE。這個方法與之前的MOS rating完全不同，簡單很多，但
效果更好。 
Selected Publication #2: Tuning the Redundancy Control Algorithm of Skype 
for User Satisfaction [17] 
在這篇論文中我們探討如何自動調校系統參數來達成最佳使用者滿意度。系
統 效 能 指 標 通 常 為 多 維 度  (multi-dimensional) 甚 至 為 高 維 度 
(highly-dimensional)，且不同系統效能指標之間經常有複雜的互動關係 
(interaction)，再加上系統所運作的環境因素時時刻刻皆可能變動，因此如何
自動調校系統參數來提供最佳使用者滿意度是十分困難的控制理論 
(control-theoretic) 問題。此論文以 Voice over IP (網路電話) 為例，基於統計
迴歸方法，提出有效率的多維度系統效能指標調校演算法。在論文中，我們
以實驗證明不論在何種網路狀況下，其所提出的方法皆能自動調整各種系統
參數，提供給使用者最佳的語音經驗。  
Selected Publication #3: A Crowdsourceable QoE Evaluation Framework for 
Multimedia Content [13] 
此論文探討如何有系統有效率地量測使用者滿意度。傳統上，研究者皆使用
MOS (Mean Opinion Score) 來量測使用者對於多媒體內容品質的滿意度；也
就是說，請受試者對於特定的多媒體內容品質來打分數 (例如 1分至 5分) 再
取平均值來做為該多媒體內容的品質。但是，以絕對分數來評量滿意度一件
- 4 - 
be More Satisfying? -- A QoE-Centric Study of the FEC Mechanism in the 
Internet-Scale VoIP System," IEEE Network, volume 24, number 2, March 
2010. 
 
B. 已發表會議論文： 
6. Te-Yuan Huang, Chih-Ming Chen, Kuan-Ta Chen, and Polly Huang, 
"Towards User-Centric Rate Adaptations for VoIP Traffic," Proceedings of ACM 
SIGCOMM 2007 (poster), ACM Press, Kyoto, Japan, August 2007. 
7. Chen-Chi Wu, Kuan-Ta Chen, Yu-Chun Chang, and Chin-Laung Lei, 
"Detecting VoIP Traffic Based on Human Conversation Patterns," Proceedings 
of IPTCOMM 2008, Heidelberg, Germany, 2008. 
8. Kuan-Ta Chen and Jing-Kai Lou, "Toward an Understanding of the Processing 
Delay of Peer-to-Peer Relay Nodes," Proceedings of IEEE/IFIP DSN 2008, 
Anchorage, Alaska, USA, June 2008. 
9. Kuan-Ta Chen and Jing-Kai Lou, "Rapid Detection of Constant-Packet-Rate 
Flows," Proceedings of IEEE ARES 2008, Barcelona, Spain, April 2008. 
10. Chen-Chi Wu, Kuan-Ta Chen, Yu-Chun Chang, and Chin-Laung Lei, 
"Detecting Peer-to-Peer Activity by Signaling Packet," Proceedings of ACM 
SIGCOMM 2008 (poster), August 2008. 
11. Hung-Hsuan Chen, Cheng-Chun Tu, and Kuan-Ta Chen, "A User-Centric 
Framework for Comparing Applications' Network Robustness," Proceedings of 
ACM SIGCOMM 2008 (poster), August 2008. 
12. Cheng-Chun Tu, Kuan-Ta Chen, Yu-Chun Chang, and Chin-Laung Lei, 
"OneClick: A Framework for Capturing Users' Network Experiences," 
Proceedings of ACM SIGCOMM 2008 (poster), August 2008. 
13. Kuan-Ta Chen and Chen-Chi Wu and Yu-Chun Chang and Chin-Laung Lei, 
"A Crowdsourceable QoE Evaluation Framework for Multimedia Content," 
Proceedings of ACM Multimedia 2009, 2009. 
14. Chen-Chi Wu, Kuan-Ta Chen, Chun-Ying Huang, and Chin-Laung Lei, "An 
Empirical Evaluation of VoIP Playout Buffer Dimensioning in Skype, Google 
Talk, and MSN Messenger," Proceedings of ACM NOSSDAV 2009, 2009. 
15. Chen-Chi Wu, Kuan-Ta Chen, Yu-Chun Chang, and Chin-Laung Lei, 
"Peer-to-Peer Application Recognition Based on Signaling Activity," 
IEICE TRANS. COMMUN., VOL.E91–B, NO.5 MAY 2008
1
INVITED PAPER Special Section on Communication Quality
A Generalizable Methodology for Quantifying User
Satisfaction∗
Te-Yuan HUANG†a), Kuan-Ta CHEN††b), Polly HUANG†††,††††c),
and Chin-Laung LEI†††,†††††d), Nonmembers
SUMMARY Quantifying user satisfaction is essential, be-
cause the results can help service providers deliver better services.
In this work, we propose a generalizable methodology, based on
survival analysis, to quantify user satisfaction in terms of session
times, i.e., the length of time users stay with an application. Un-
like subjective human surveys, our methodology is based solely on
passive measurement, which is more cost-efficient and better able
to capture subconscious reactions. Furthermore, by using session
times, rather than a specific performance indicator, such as the
level of distortion of voice signals, the effects of other factors like
loudness and sidetone, can also be captured by the developed
models. Like survival analysis, our methodology is characterized
by low complexity and a simple model-developing process. The
feasibility of our methodology is demonstrated through case stud-
ies of ShenZhou Online, a commercial MMORPG in Taiwan, and
the most prevalent VoIP application in the world, namely Skype.
Through the model development process, we can also identify the
most significant performance factors and their impacts on user
satisfaction and discuss how they can be exploited to improve
user experience and optimize resource allocation.
key words: Human Perception, Internet Measurement, Quality
of Service, Survival Analysis, Network Games, VoIP
1. INTRODUCTION
User satisfaction is a key measure of an application’s
success; hence, service providers need accurate mea-
surement results to help them improve their systems
and technology. However, it is very difficult to quan-
tify the degree of user satisfaction efficiently, especially
for interactive real-time applications. Subjective sur-
veys are expensive and can only capture conscious re-
actions; while most objective methods evaluate user
†The author is with the Department of Electrical Engi-
neering, National Taiwan University, Taipei, Taiwan.
††The author is with the Institute of Information Science,
Academia Sinica, Taipei, Taiwan.
†††The author is with the Faculty of Department of Elec-
trical Engineering, National Taiwan University, Taipei, Tai-
wan.
††††The author is with the Faculty of Graduate Institute
of Networking and Multimedia, National Taiwan University,
Taipei, Taiwan.
a) E-mail: r95921037@ntu.edu.tw
b) E-mail: ktchen@iis.sinica.edu.tw
c) E-mail: phuang@cc.ee.ntu.edu.tw
d) E-mail: lei@cc.ee.ntu.edu.tw
∗This work was supported in part by Taiwan Informa-
tion Security Center (TWISC), National Science Council of
the Republic of China under the grants NSC 96-2219-E-001-
001, NSC 96-2220-E-002-024, and NSC 96-2219-E-011-008.
satisfaction in terms of specific performance indicators,
such as game scores or voice signals, which are limited
and not portable. In this work, we propose a gener-
alizable methodology, based on statistical analysis, to
investigate the real reasons that users decide to stay
with or leave an application. We then quantify user
satisfaction according to the duration of a user’s stay
with the application, i.e., the session time. Although
the session time cannot represent user satisfaction di-
rectly, we believe it is a good and intuitive indicator
of users’ perceptions of system performance. Besides,
in our case studies, which are excerpted from our pre-
vious works [1], [2], the observed correlation between
session times and performance factors, such as network
QoS, strongly supports our assumption that premature
departures from an application are partially caused by
unfavorable experiences.
We base our methodology on survival analysis be-
cause of its ability to handle censored data, i.e., incom-
plete observations. Together with the Cox Proportional
Hazards model [3], we are able to identify the most sig-
nificant performance factors and quantify their impact
on user satisfaction. The feasibility of our methodol-
ogy is demonstrated by our case studies. By model-
ing user satisfaction in ShenZhou Online, a commercial
MMORPG in Taiwan, and Skype, the most popular
VoIP application in the world, we found that system
performance factors, such as network QoS, significantly
affect users’ willingness to continue with an application.
In ShenZhou Online, for example, both network delay
and network loss have a significant impact on user sat-
isfaction; while in Skype, the source rate and its jitter
are the key factors. The proposed model not only quan-
tifies user satisfaction, but also provides useful hints
about optimizing system performance and resource al-
location.
Our contribution in this work is fourfold. 1) Our
methodology provides an objective method for quantify-
ing user satisfaction, and it is generalizable to various
applications. 2) Since the method is based solely on
passive measurements, rather than subjective surveys,
subconscious reactions can also be captured. 3) Un-
like other objective methods, our method is based on
session times, rather than a specific performance indi-
cator, such as game scores or voice signals; factors other
than the designated indicator can also be captured. 4)
HUANG et al.: A GENERALIZABLE METHODOLOGY FOR QUANTIFYING USER SATISFACTION
3
(t1,1)
(t3,0)
(t2,0)
(t4,0)
observation start observation end
Fig. 1 Measurement setups often lead to explicit censoring of
sessions. The four possible censoring scenarios are denoted by (t,
s), where t is the observed duration and s is the censoring status.
Cox Proportional Hazards model [3] in survival analy-
sis.
3.1 Survival Analysis
Survival analysis is often used in the medical field to de-
scribe the relationship between patients’ survival time
and the treatment they receive; and survival time often
refers to the development of a particular symptom. We
found that this approach also provides a good fit for
describing the reactions of users to a system’s perfor-
mance.
Following the conventions of survival analysis, we
denote a user’s departure as an event or a failure. How-
ever, at the end of the observation period, some sub-
jects will not have left the system. On the other hand,
some subjects may start their session before the begin-
ning of the observation period. Thus, the survival times
of those subjects are not completely observed. Such
survival times are termed censored, to indicate that the
session is not completely observed. Censored observa-
tions should be also used because longer sessions are
more likely to be censored than shorter sessions. Sim-
ply disregarding them would lead to underestimation.
An indicator variable, si, which is the censoring status,
is used to indicate whether a session, i, has been cen-
sored: thus si = 1 means an event has occurred (the
observation is not censored) and si = 0 represents a
censored observation, as illustrated in Fig. 1.
A survival function is commonly used to describe
the lifetime pattern of an object or a set of observa-
tions. In our context, the survival function is defined
as follows:
S(t) = Pr(a session that survives longer than time t)
= 1− Pr(a session that fails before,
or is equal to, time t)
= 1− F (t),
where F (t) is the cumulative distribution function
(CDF) of session times. A standard estimator of the
survival function, proposed by Kaplan and Meier [8],
is called the Product-Limit estimator or the Kaplan-
Meier estimator. Suppose there are n distinct ses-
sion times t1, t2, . . . , tn in ascending order such that
t1 < t2 < . . . < tn, and that at time ti there are di
events and Yi active sessions. The estimator is then
defined as follows for all values of t ≤ tn:
Sˆ(t) =
∏
ti≤t
Pr[T > ti|T ≥ ti]
=
{
1 if t < t1,∏
ti≤t
[1− di
Yi
] if t1 ≤ t.
Note that, for time intervals contain censored data, the
probability of surviving is one, since the exact surviv-
ing time is longer than the observed one. Thus, if the
session i is censored, i.e., si is zero; di is zero. On the
other hand, di is one when si is one, i.e., the session i is
not censored. For observations with ties, if the session
times are continuous in essence and later discretized
by measurement, a practical solution is to add a small
amount of “noise” so that all times are unique. After
estimating the survival function, the pth quantile of the
lifetime, tp, can be obtained by
tp = inf{t : Sˆ(t) ≤ 1− p}. (1)
This equation can be used repeatedly to estimate the
median session time as t0.5 for a group of sessions.
In addition to the survival function, hazard func-
tion, or hazard rate, is a frequently used quantity in
survival analysis. It is also known as the conditional
failure rate in reliability engineering, or the intensity
function in stochastic processes. The hazard rate is
defined by
h(t) = lim
∆t→0
Pr[t ≤ T < t+∆t|T ≥ t]
∆t
.
A related quantity is the cumulative hazard function
H(t), which is defined by
H(t) =
∫ t
0
h(u)du = − ln[S(t)].
The hazard function gives the instantaneous rate at
which failures occur for observations that have survived
at time t. The quantity h(t)∆t may therefore be seen
as the approximate probability that a player who has
been in a game for time t will leave the game in the
next ∆t period, given that ∆t is small. The hazard
function plays an important role in the Cox regression
model because the hazard rate of session times h(t) is
taken as the response variable of network QoS factors.
We discuss this point in the next section.
3.2 Regression Modeling
By combining survival analysis with correlation analy-
sis, we may be able to derive the relationship between
a specific QoS factor and session times. However, the
true impact of individual factors remains hidden be-
cause of the collinearity of factors; that is ,two or more
HUANG et al.: A GENERALIZABLE METHODOLOGY FOR QUANTIFYING USER SATISFACTION
5
data to a more generalized Cox model that allows time-
dependent coefficients [9]. In this model, Equation 3 is
extended to
log
h(t|Z)
h0(t)
=
p∑
k=1
β(t)kZk =
p∑
k=1
(βk + θk ln(t))Zk,
where the coefficient vector β(t) is time-dependent.
The null hypothesis, which indicates the conformance
of the proportional hazards assumption, corresponds to
θk ≡ 0, k = 1, . . . , p. In this case, β(t) in the extended
model reduces to β in the standard model. The test is
similar to a standard linear trend test in that it deter-
mines whether a significant non-zero slope exists by an
ordinary least square regression test.
3.2.3 Model Validation
To assess the overall goodness-of-fit of our model, we
can use the Cox and Snell residuals [10]. If the model
is correctly fitted, the random variable ri = Hˆ(ti,Zi)
will have an exponential distribution with a hazard rate
of 1, where Hˆ(ti,Zi) is the estimated cumulative haz-
ard rate for session i with risk vector Zi. Accordingly,
the plot of ri and its Kaplan-Meier estimate of the sur-
vival function Sˆ(r) will be a straight line through the
origin with a slope of 1. We can further validate our
model by prediction; that is, given a performance vec-
tor Z, we can predict the most probable session time as
the median time of the estimated survival curve, i.e.,
inf{t : S(t|Z) ≤ 0.5}; while S(t|Z) = exp(−H(t|Z)) is
the computed survival function for the session with risk
vector Z. By the relation, one can sort and group all
sessions by their risk scores, βtZ, and predict session
times based on the median risk score in each group.
Then, the model can be validated by comparing the
predicted session times with the actual median times
to determine if they are within a certain predicted con-
fidence band.
4. CASE STUDY
In this section, we consider two case studies to demon-
strate the complete procedure for quantifying user sat-
isfaction. We first investigate online gaming, since it is
one of the most profitable businesses on the Internet.
Then, we explore user satisfaction on VoIP, which is
one of the most popular services on the Internet. By
determining user satisfaction, service providers can de-
liver better service quality to users and thereby boost
service subscriptions.
4.1 Online Game
The popularity of online gaming has increased rapidly
in recent years; however, users still experience unfavor-
able network conditions. Numerous complaints about
long or frequent lags are made in game-player forums.
Thus, to understand online gamers’ QoS-sensitivity, we
investigate the relationship between gamers’ playing
times and network QoS factors.
We collected traces from ShenZhou Online [11],
a commercial massively multiplayer online role-playing
game (MMORPG). To play ShenZhou Online, thou-
sands of players pay a monthly subscription fee at a con-
venience store or online. As a typical MMORPG, play-
ers can engage in fights with random creatures, train
themselves to acquire particular skills, partake in com-
merce or take on a quest. Compared to other game
genres, such as FPS (First-Person Shooting) games,
MMORPGs are relatively slow-paced and have less-
stringent service requirements. Therefore, they could
be viewed as a baseline for real-time interactive on-
line games. In other words, if network QoS frustrates
MMORPG players, it should also frustrate players of
other game genres. With the help of the ShenZhou
Online staff, we recorded all inbound/outbound game
traffic of the game servers located in Taipei; a total
of 15,140 game sessions over two days. The observed
players were spread over 13 countries, including China,
India, Hong Kong, and Malaysia, and hundreds of au-
tonomous systems, thus manifesting the heterogeneity
of network-path characteristics and the generality of
the trace.
4.1.1 Performance Factor Identification
According to the theory proposed in [12], when users
are playing a game, if the feeling of involvement in the
virtual world is diminished by network lags, they be-
come more consicous of the real world, which reduces
their sense of time distortion. Therefore, we expect
players’ staying times in MMORPGs will be affected,
to some extent, by the network’s QoS. From the col-
lected trace, we extracted the network performance for
each session based on the sequence number and flags in
the TCP packet header. On average, players stayed for
100 minutes after joining a game. However, the differ-
ence in individual game-playing times was quite large;
for example, the shortest 20% of sessions spanned less
than 40 minutes, but the top 20% of players spent more
than eight hours continuously in the game. Fig. 2 il-
lustrates the difference in the game-playing times of
sessions experiencing different levels of network qual-
ity. The figure depicts the association of game playing
times with network latency, network delay variation,
i.e., the standard deviation of network latency, and the
network loss rate, respectively. All three plots indicate
that the more serious the network impairment experi-
enced by players, the earlier they were likely to leave
the game. The changes in game-playing time are signif-
icant. For instance, gamers who experienced 150 msec
latency played four hours on average, but those expe-
riencing 250 msec latency played for only one hour an
HUANG et al.: A GENERALIZABLE METHODOLOGY FOR QUANTIFYING USER SATISFACTION
7
client-prediction techniques used.
4.1.3 Findings and Discussion
The above results highlight the fact that network delay
variations are less tolerable than absolute delay. There-
fore, while current network games rely primarily on a
“ping time” to select a server for a smooth game play-
ing, delay jitters should also be considered in the server
selection process. We also find that players are more
sensitive to network loss rates than network latency.
However, a study on Unreal Tournament 2003 [13] re-
ported that a typical network loss rate (< 6%) has no
impact on user performance. We believe the difference
is caused by the choice of underlying transport pro-
tocol. That is, while most FPS games transmit mes-
sages via UDP, many MMORPGs, including ShenZhou
Online, use TCP. Since TCP provides in-order deliv-
ery and congestion control, a lost packet will cause the
subsequent packets to be buffered until it is delivered
successfully, thereby reducing TCP’s congestion win-
dow. In contrast, packet loss incurs no overhead in
UDP. In short, for TCP-based online games, packet
loss incurs additional packet delay and delay jitters,
and therefore causes further annoyance to players. For
this reason, and because of TCP’s high communication
overhead [14], we consider that more lightweight proto-
cols would be more appropriate for real-time interactive
network games.
4.2 VoIP
Among the various VoIP services, Skype is by far
the most successful. There are over 200 million
Skype downloads and approximately 85 million users
worldwide. However, fundamental questions, such as
whether VoIP services like Skype are good enough in
terms of user satisfaction, have not been formally ad-
dressed. In this subsection, we quantify Skype user
satisfaction based on the call duration measured from
actual Skype traces, and propose an objective and per-
ceptual index called the User Satisfaction Index (USI).
To collect Skype traffic traces, we set up a packet
sniffer to monitor all traffic entering and leaving a
campus network. In addition, to capture more Skype
traces, a powerful Linux machine was set up to elicit
more relay traffic passing through it during the course
of the trace collection. However, given the huge amount
of monitored traffic and the low proportion of Skype
traffic, we used two-phase filtering to identify Skype
VoIP sessions. In the first stage, we filtered and stored
possible Skype traffic on a disk. Then, in the second
stage, we applied an off-line identification algorithm
to the captured packet traces to extract actual Skype
sessions. Since we could not deduce round-trip times
(RTT) and their jitter simply from packet traces, we
sent out probe packets for each active flow while cap-
10 20 50 100
Bit rate (Kbps)
M
ed
ia
n 
se
ss
io
n 
tim
e 
(m
in)
0.
2
1
10
10
0
50
0
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
− −
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
Fig. 4 Correlation of bit rate with session time
0.2 0.5 1.0 2.0 5.0 10.0
Jitter (Kbps)
M
ed
ia
n 
se
ss
io
n 
tim
e 
(m
in)
0.
2
1
10
10
0
50
0
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
Fig. 5 Correlation of jitter with session time
turing Skype traffic. The trace was collected over two
months in late 2005. We obtained 634 VoIP sessions, of
which 462 sessions were usable because they had more
than five RTT samples. Among the 462 sessions, 253
were directly-established and 209 were relayed.
4.2.1 Performance Factor Identification
Skype uses a wideband codec that adapts to the net-
work environment by adjusting the bandwidth used.
Thus, when we explore the relationship between call
duration and network conditions, we must also con-
sider the source rate, along with network delay and loss.
However, we do not have exact information about the
source rate of remote Skype hosts. Thus, we use the
received data rate as an approximation of the source
rate. For brevity, we use the bit rate to denote the re-
ceived data rate. We illustrate the correlation of the
bit rate and call duration in Fig. 4, where the median
time and their standard errors are plotted. The effect
of the bit rate is clear, as we find that users tend to
have longer conversations when the bit rate is higher.
In fact, the median duration of the top 40% of calls is
HUANG et al.: A GENERALIZABLE METHODOLOGY FOR QUANTIFYING USER SATISFACTION
9
by increasing the packet rate or the degree of data re-
dundancy; thus, users would have better experiences
and be less likely to leave an application prematurely.
Resource allocation could be deliberately biased toward
high-risk sessions. For example, scarce resources, such
as processing power or network bandwidth, could be
allocated more effectively based on session risk scores.
The developed model could also provide useful
hints to resolve design trade-offs. For instance, as the
results in Section 4.1 indicate, players in ShenZhou On-
line are less tolerant of large delay variations than high
latency. Thus, providing a smoothing buffer at the
client side, though incurring additional delay, would
improve overall user experience. Also, the concept of
session time can be used to design an alarm system
for abnormal system conditions. As we know, to pro-
vide continuous high-quality services, providers must
monitor system performance around the clock and de-
tect problems in real time, i.e., before customer com-
plaints flood the customer service center. However,
monitoring a large-scale system in this way would be
prohibitively expensive or even impractical. Instead,
operators can track user session times, which is much
more cost-effective. Since users are more sensitive to
certain system performance factors, a series of unusual
departures over a short period might indicate abnor-
mal system conditions and thus automatically trigger
appropriate remedial action.
6. CONCLUSION
Unlike system-level performance, user satisfaction is in-
tangible and unmeasurable. The key to addressing this
problem is our ability to measure user opinions objec-
tively and efficiently. In this work, we have proposed
a generalizable methodology, based on survival analy-
sis, to quantify user satisfaction from session times, i.e.,
the length of time users stay with an application. The
results of two case studies show that session time is
strongly related to system performance factors, such as
network QoS, and is thus a potential indicator of user
satisfaction. With the derived model, service providers
can further improve user experience and optimize re-
source allocation.
References
[1] K.T. Chen, P. Huang, G.S. Wang, C.Y. Huang, and C.L.
Lei, “On the sensitivity of online game playing time to net-
work QoS,” Proceedings of IEEE INFOCOM’06, Barcelona,
Spain, pp.1–12, April 2006.
[2] K.T. Chen, C.Y. Huang, P. Huang, and C.L. Lei, “Quan-
tifying skype user satisfaction,” Proceedings of ACM SIG-
COMM’06, Pisa, Italy, pp.399–410, Sept. 2006.
[3] D.R. Cox and D. Oakes, Analysis of Survival Data, Chap-
man & Hall/CRC, June 1984.
[4] ITU-T Recommendation P.800, “Methods for subjective de-
termination of transmission quality,” 1996.
[5] U. Jekosch, Voice and Speech Quality Perception Assess-
ment and Evaluation, Springer, 2005.
[6] A. Rix, J. Beerends, M. Hollier, and A. Hekstra, “Percep-
tual evaluation of speech quality (PESQ) - a new method
for speech quality assessment of telephone networks and
codecs,” Proceedings of IEEE International Conference on
Acoustics, Speech, and Signal Processing, pp.73–76, 2001.
[7] ITU-T Recommendation P.862, “Perceptual evaluation of
speech quality (PESQ), an objective method for end-to-end
speech quality assessment of narrow-band telephone net-
works and speech codecs,” Feb 2001.
[8] E.L. Kaplan and P. Meier, “Nonparametric estimation from
incomplete observations,” Journal of the American Statis-
tical Association, vol.53, pp.437–481, 1958.
[9] T.M. Therneau and P.M. Grambsch, Modeling Survival
Data: Extending the Cox Model, 1 ed., Springer, August
2001.
[10] D.R. Cox and E.J. Snell, “A general definition of residuals
(with discussion),” Journal of the Royal Statistical Society,
vol.B 30, pp.248–275, 1968.
[11] “ShenZhou Online.” http://www.ewsoft.com.tw/.
[12] D.M. S. Ila and D. Lam, “Comparing the effect of habit in
the online game play of australian and indonesian gamers.,”
Proceedings of the Australia and New Zealand Marketing
Association Conference, Adelaide, Australia, Dec. 2003.
[13] T. Beigbeder, R. Coughlan, C. Lusher, J. Plunkett, E. Agu,
and M. Claypool, “The effects of loss and latency on user
performance in Unreal Tournament 2003,” NetGames ’04:
Proceedings of the 3nd Workshop on Network and System
Support for Games, pp.144–151, ACM Press, 2004.
[14] K.T. Chen, P. Huang, and C.L. Lei, “Game traffic analysis:
An MMORPG perspective,” Computer Networks, vol.50,
no.16, pp.3002–3023, 2006.
[15] F.E. Harrell, Regression Modeling Strategies, with Appli-
cations to Linear Models, Survival Analysis and Logistic
Regression, Springer, 2001.
Te-Yuan Huang received her B.S.
in Computer Science from National Chiao
Tung University, in 2006. She is currently
a Master’s student in the Department of
Electrical Engineering, National Taiwan
University. Her research interests include
computer networking with a focus on net-
work traffic measurement, analysis and
modeling, as well as quality of service and
performance evaluation.
Kuan-Ta Chen received his B.S.
and M.S. in Computer Science from Na-
tional Tsing-Hua University in 1998 and
2000, respectively. He received his Ph.D.
in Electrical Engineering from National
Taiwan University in 2006. He then
joined the Institute of Information Sci-
ence, Academia Sinica, where he is cur-
rently an assistant research fellow. His
research interests include multimedia net-
working, Internet measurement, network
security, and entertainment networking. Much of his recent work
1Effect of Network Quality on Player Departure
Behavior in Online Games
Kuan-Ta Chen1, Polly Huang2, and Chin-Laung Lei2
ktchen@iis.sinica.edu.tw, {phuang,lei}@cc.ee.ntu.edu.tw
1Institute of Information Science, Academia Sinica
2Department of Electrical Engineering, National Taiwan University
Abstract—Understanding the impact of network conditions on
player satisfaction, which is one of the major concerns of network
game designers, is a popular research topic. Of the various ways
to gauge user satisfaction, in this paper, we focus on how network
quality affects a player’s decision to leave a game prematurely. To
answer this question, we analyze a 1, 356-million-packet trace
from a large commercial MMORPG called ShenZhou Online.
We show that both network delay and network loss signifi-
cantly affect a player’s decision to leave a game prematurely. It
is feasible to predict whether players will quit prematurely based
on the network conditions they experience. The proposed model
can determine the relative impact of different types of network
impairment. For our traces, the degrees of player intolerance of
network delay, delay jitter, client packet loss, and server packet
loss are in the proportion of 1:2:4:3 approximately. The model
can also be used to make system design decisions. Through
simulations, we show that by prioritizing server processing
according to the goodness of network conditions, employing
de-jitter buffers, or replacing TCP with a more lightweight
transport protocol, the probability of premature departure can
be significantly reduced. In this way, we demonstrate how our
model of players’ network experience provides feedback for the
design of online games.
Index Terms—Departure Analysis, Internet Measurement, Lo-
gistic Regression, MMORPG, Quality of Service, User Behavior
I. INTRODUCTION
Of the various research areas related to online games,
assessing the impact of network conditions on user experience
is one of the most popular topics. Many studies, e.g., [3, 4,
7, 8, 16, 18, 25, 27, 28, 33, 37, 39], try to answer questions
like: Are game players sensitive to network conditions? If the
answer is yes, they ask: What level of network QoS (Quality-of-
Service) should be provided to maintain a satisfactory gaming
experience? The answers to the above questions are important
because they could provide useful guidelines for the trade-offs
in network resource planning. For instance, if we can be sure
that players are less tolerant of large delay variations than high
latency, then providing a smoothing buffer at the client side,
which introduces additional latency but smoothes the pace of
game play, would be a plus, as it still improves the overall
gaming experience from the user’s perspective.
Currently, there is no standard way to objectively quantify
the satisfaction that players derive from gaming. Hence, the
effect of network quality is often evaluated in terms of the
users’ performance in a specific context, such as the number
of kills in shooting games, the time taken to complete each lap
in racing games, or the capital accumulated in strategy games.
However, game scores are highly dependent on a player’s
skills, the system design, and the game’s content, so the results
are not comparable and generalizable across different games.
On the other hand, according to flow theory in psychology,
game playing can be described as a pleasurable and exciting
activity that makes players oblivious to time while they are in
the game [21, 26]. The theory suggests that players will be
more conscious of the real world if the feeling of involvement
in the virtual world is diminished by network lags; therefore,
the effect of time distortion will be mitigated. Furthermore,
players may simply decide to quit a game as soon as they
detect unacceptable lags. Thus, we conjecture that the time
players leave a game is affected, to some extent, by the network
quality they experience.
Massively Multiplayer Online Role-Playing Games (MMO-
RPGs) have become immensely popular in recent years, with
several top games reporting millions of subscribers [36]. Our
conjecture is verified by real-life traces from a commercial
MMORPG, ShenZhou Online [35], for two reasons. First,
MMORPGs are deemed to be addictive in that about half of
the players consider themselves addicted [38], so they tend to
stay for a long time once they join a game. For instance, the
statistics of MMORPGs in Japan [1] show that the average
game session time is between 80 and 120 minutes. Most
players stay for more than an hour once they join a game.
If players leave in the first few minutes, it may indicate that
they have an unsatisfactory gaming experience due to poor
QoS. The second reason is that MMORPGs are relatively slow-
paced compared to other popular genres, such as first-person
shooting (FPS) games, which usually require players to make
sub-second decisions. Slow-action games undoubtedly have
less stringent service requirements than fast-action games.
Thus, MMORPGs could be seen as a baseline for real-time
interactive games so that if network QoS frustrates MMORPG
players, it should also affect gamers of other genres.
In this paper, we analyze the player departure patterns in
ShenZhou Online and their relationship to network quality.
We find that both network latency and network loss have a
significant influence on players’ decisions to leave a game
prematurely. We detail our major findings in the following
question-and-answer format.
1) Do game players leave a game prematurely due
to unfavorable network conditions? Yes. Generally
speaking, the worse the network quality, the earlier
3Fig. 1. A screen shot of ShenZhou Online
predictability analysis of player departure events in terms of
network quality; and 3) the representativeness and sampling
methods of network QoS factors. We also explain how the
proposed regression model can be used in making system
design decisions.
While a number of previous works have suggested remark-
able QoS tolerance on the part of game players [4, 16, 33],
our findings based on the ShenZhou Online trace show that
network quality has a significant influence on players’ de-
parture patterns. We believe that the discrepancy is due to
both the nature of the game genre and the design and im-
plementation of each particular game, such as dead reckoning
schemes [2, 29, 34], and transport protocols. For example, TCP
provides in-order delivery, which incurs additional delay and
jitter for each packet loss event. As games employ different
designs and transport protocols, it is inevitable that players
will have diverse levels of QoS-sensitivity in different games,
unless we can separate the effects of network QoS, system
design, and transport protocols on players. This issue remains
to be solved.
III. TRACE COLLECTION
ShenZhou Online is a mid-scale, commercial MMORPG
that is popular in Taiwan [35], where there are thousands
of players online at any one time. To play, the participants
purchase game points from a convenience store or online. A
screen shot of ShenZhou Online is shown in Fig. 1. The
character played by the author is the man under the tree with a
round smiling face above him. He is in a typical market place,
where other players keep stalls. As is normal in MMORPGs,
a player can engage in fights with random creatures, train
himself in special skills, participate in marketplace commerce,
or take on a quest.
With the help of the ShenZhou Online staff, we set up
a traffic monitor beside the game servers. The monitor was
attached to a layer-4 switch upstream of the LAN containing
the game servers (we call it the “game LAN”). The port
forwarding capability of the tapped layer-4 switch was enabled
so that a copy of all inbound/outbound game traffic was for-
warded to our monitor. To minimize the impact of monitoring,
all remote management operations were conducted via an
Internet
Traffic Monitor
L3 switch
L2 switchL4 switch
Game & Database servers
Monitoring
interface
Management
interface
Game Traffic
Fig. 2. Network setup for traffic measurement
additional network path, i.e., the game traffic and management
traffic did not interfere with each other. The network topology
and setup of the game servers and the traffic monitor are shown
in Fig. 2. The traffic monitor was a FreeBSD PC equipped with
1.5 GHz Pentium 4 and 256 MB RAM. We used tcpdump with
the kernel built-in BPF to obtain traffic traces. In each trace,
we randomly chose a subset of game sets, and only packets
belonging to the selected game sets were logged. A game set,
which is logically a “game server” from a player’s viewpoint,
comprises an entry server, several map servers, and a database
server. All game sets are equivalent in content, but isolated.
The reason for providing identical game sets is to distribute the
players over a number of servers with limited game content,
e.g., terrain, missions, and creatures in the virtual world. We
took two packet traces, N1 and N2, which recorded traffic
for two and three game sets, respectively. The two traces,
which spanned 8 and 12 hours, respectively, and contained
more than 1, 356 million packets, are summarized in Table I.
Interested readers may refer to [6] for more details about the
characteristics of game traffic. The full data set is available
for research purposes on request1.
Although the traced game servers are centrally located at
one ISP, players are spread over 13 countries and hundreds
of autonomous systems. More specifically, the average RTTs
experienced by game sessions range from 95 ms to 580
ms, and the loss rates incurred range from zero to 20%
(computed by one percentile and 99 percentile, respectively).
The heterogeneous network path characteristics manifest that
our trace is not specific to a particular configuration.
IV. THE PLAYER DEPARTURE PROCESS AND ITS
SENSITIVITY TO NETWORK QOS
In this section, we first analyze the general departure
process of game players without considering the effect of
network conditions. We then present a correlation analysis
of the relationship between the player departure process and
the network conditions they experience. The purpose of the
correlation analysis is twofold: 1) to confirm the influence of
1Please visit http://mmnet.iis.sinica.edu.tw/download.html to request the
ShenZhou Online traffic traces.
5100 150 200 250
0.
06
0.
10
0.
14
Average RTT
Average RTT (ms)
Pr
em
at
ur
e 
D
ep
ar
tu
re
 R
at
io Corr. coef. = 0.75
500 1000 1500 2000
0.
00
0.
05
0.
10
0.
15
Maximum RTT
Maximum RTT (ms)
Pr
em
at
ur
e 
D
ep
ar
tu
re
 R
at
io Corr. coef. = −0.06
20 40 60 80 100
0.
00
0.
04
0.
08
0.
12
Delay Jitter
RTT std dev (ms)
Pr
em
at
ur
e 
D
ep
ar
tu
re
 R
at
io Corr. coef. = 0.65
80 100 120 140 160
0.
05
0.
15
0.
25
Queueing Delay
Queueing delay (ms)
Pr
em
at
ur
e 
D
ep
ar
tu
re
 R
at
io Corr. coef. = −0.19
0.
00
0.
05
0.
10
0.
15
0.
20
Client Packet Loss
Packet loss rate (%)
Pr
em
at
ur
e 
D
ep
ar
tu
re
 R
at
io
0.01 0.1 1
Corr. coef. = 0.91
0.
00
0.
05
0.
10
0.
15
Server Packet Loss
Packet loss rate (%)
Pr
em
at
ur
e 
D
ep
ar
tu
re
 R
at
io
0.01 0.1 1
Corr. coef. = 0.89
Fig. 4. Correlation of premature departures and network QoS factors
• Maximum RTT: the maximum round-trip transmission
latency of game data packets, which accounts for the most
unpleasant “lag” experience.
• Delay jitter: defined as the standard deviation of packet
round-trip times, which measures the instability of the
game’s response time.
• Average queueing delay: computed as the average round-
trip time minus the minimum round-trip time, which is
an estimate of the average queueing time accumulated
during network transmission.
• Client packet loss rate: the loss ratio of packets sent to the
server by game clients, which accounts for the additional
latency before a player’s command can be processed (by
the server), as loss detection and recovery cost some time.
• Server packet loss rate: the loss ratio of packets sent
from the game server to the client, which accounts for
additional latency before game messages or state updates
can be displayed on the client’s screen (i.e., presented to
the game player).
Our procedure for assessing whether a particular QoS factor
affects the occurrence rate of premature departure events is as
follows. First, the range of each QoS factor is divided into
several equal intervals. Then, we classify all the game sessions
into different groups according to which interval their QoS
factors fall into, and compute the proportion of prematurely
departed sessions within each group. The computed quantity
PDratio(x) is an approximation of the conditional probability
Pr(PD|x), where x denotes the midpoints of the intervals of
a QoS factor. The scatter plots for PD(10), i.e., premature
departures that occurred within 10 minutes of joining a game,
which represent the relationship between PDratio(x) and
x, are shown in Fig. 4. For each plot, we use Kendall’s
rank correlation coefficient τ [24] to quantify the strength
of the relationship between the QoS factors and the ratio of
premature departure events. A lowess smooth curve [9] is also
plotted to facilitate visual detection of the trend.
1) Factor Analysis: In Fig. 4, except for the maximum
RTT and the mean queueing delay, the factors show generally
positive correlations with the rate of premature departures.
This basically confirms our hypothesis that more serious
network impairment annoys players such that they are likely
to leave the game earlier (even though they may come back
later).
Effect of Queueing Delay: The average queueing delay,
however, has negative correlations with premature departures
when it is small, and shows no correlation with premature
departures when it is moderate to high. A detailed analysis
reveals that this is because sessions with short queueing delays
have much higher packet loss rates than those with long
queueing delays. Specifically, the median packet loss rate for
sessions with queueing delays shorter than 50 ms is 0.84%,
but for higher-queueing-delay sessions it is 0.08%, a ratio
of approximately 10:1. The combination of high packet loss
and short queueing delay could be due to certain congested
links that incur a high packet drop rate; however, since the
capacity is high, the queueing time is relatively short (the
queueing time is decided by both the queueing length and
the outgoing link bandwidth). On the other hand, there is
no correlation between moderate to long queueing delays
and premature departures. This suggests that queueing delay
is not a good indicator of network quality, as it does not
directly affect players’ perceptions of game responsiveness
and interactivity. In other words, players cannot distinguish
between specific components of the delay time (i.e., processing
delay, propagation delay, transmission delay, and queueing
delay). Instead, they only care about the total delay time that
they actually experience in the form of game “lags,” “jumps,”
slow responses, or inconsistent states between different peers.
Effect of Maximum RTT: There is no correlation between
maximum RTT and premature departures. This may because
the maximum RTT captures the worst network lags players
experience during the session, instead of the players’ average
experience. Even if the worst lag is intolerable, users may
be patient and wait for conditions to return to normal (as
network quality changes constantly over time). In this case,
the maximum RTT factor cannot capture the true feelings of
players based on premature departures.
Threshold Effect: The trend of the lowess curves in Fig. 4
indicates that the average RTT, delay jitter, and both packet
loss factors have a “threshold” effect, i.e., the impact of a
factor remains unchanged when its magnitude is small. For
example, the threshold of the average RTT is around 180
ms, so the premature departure probability only increases
with the average RTT when the latter is higher than 180
ms. This indicates that players may be insensitive to a
small amount of network impairment. The threshold effect
is commonly seen in measures of physiological reactions to
external substances [11]. For example, human responses to
drugs in terms of enzyme activity, membrane potential, heart
rate, or muscle contraction usually have a threshold effect.
Hence, the threshold effect we identified here could be seen
as evidence that premature departures successfully capture
players’ perceptions of network impairment.
2) Effect of the Observation Period: Fig. 4 shows the effect
of network QoS factors on premature departures with an ob-
servation period of 10 minutes. We now examine whether the
effect of network impairment remains the same with different
definitions of premature departure. To do so, we plot the rank
correlation coefficient between premature departures and QoS
factors with different observation times, as shown in Fig. 5.
750
10
0
15
0
20
0
25
0
Dispersion window size (sec)
Lo
g−
lik
el
ih
oo
d
5 10 30 60 120 300
(a) RTT std dev
0
20
40
60
80
10
0
Window size (sec)
Lo
g−
lik
el
ih
oo
d
5 10 30 60 120 300
(b) Average RTT
10
0
15
0
20
0
25
0
Window size (sec)
Lo
g−
lik
el
ih
oo
d
5 10 30 60 120 300
(a) Client loss rate
Overall
Mean
Min
Max
14
0
18
0
22
0
26
0
Window size (sec)
Lo
g−
lik
el
ih
oo
d
5 10 30 60 120 300
(b) Server loss rate
Fig. 6. Evaluation of the sampling method for network QoS factors
factor in a session, which is analogous to feature vector
extraction in pattern recognition, is the key to determining
how well the model fits the observed departure behavior of
players.
Intuitively, the quantities averaged over the whole session
time should be a good way to obtain a representative feature.
However, (relatively) extreme conditions may have much more
influence on users’ overall perception than other conditions.
For example, users may quit a game immediately because of
serious network lags in a short period, but be unaware of mild
and moderate lags that occur all the time. Moreover, players
might be more sensitive to adverse network conditions than
desirable network quality (i.e., as it is “supposed” to be), or
vice versa. Players might still be happy if the network quality
is satisfactory most of the time, even if it is is intolerable
sometimes, or they may only consider the unsatisfactory
part, and leave as soon as they feel the playing conditions
are intolerable. These behavior patterns are only a few of
numerous possible ways a player might react to network
impairment. As no general and well-established perceptual
and behavioral models exist to describe players’ reactions to
perceived network impairment, we investigate how to derive
the most representative risk vectors.
We propose three measures to account for variations in
network quality over time, namely, the minimum, the average,
and the maximum of a factor, with two-level sampling. That is,
the original time series s is divided into a number of sub-series
of length w, from which network conditions are sampled.
This sub-series approach is intended to confine the measures
of network quality within time spans of length w, thereby
excluding the effect of large-scale variations. The respective
minimum, average, and maximum measures with lengths equal
to |s|/w are computed for each sampled QoS factor. We then
decide which of the three measures is the most representative
for describing a user’s perceived experience during the game.
We evaluate different combinations of measures and win-
dow sizes by fitting the extracted QoS factors into a logistic
model and checking the models’ log-likelihood value, which
is an indicator of goodness-of-fit. As Fig. 6 shows, the client
packet loss rate and server packet loss rate are best sampled
with an overall average in the whole session time, i.e., with
w = |s|. We believe this result is due to the following
reasons: 1) the game packet rate is low (generally less than
10 packets/second), and 2) packet loss is rare. Thus, a large
window would be more appropriate because a short time series
may not contain enough samples to capture the true packet loss
probability along the network path.
On the other hand, the minimum values of the average RTT
and the RTT standard deviation in consecutive windows are the
most representative. That is, we choose the minimum average
RTT and minimum RTT standard deviation and sample both
with a window size of 10 seconds. (For simplicity, we use
delay and delay jitter to refer to the sampled average RTT
and RTT standard deviation variables respectively.) The small
window size implies that players are more sensitive to short-
term, rather than long-term, effects of network quality. This
behavior is reasonable because long-term fluctuations in net-
work quality should have no influence on the real-timeliness
of game playing. The sampling method of the RTT standard
deviation indicates that players are tolerant of infrequent
extreme variations in network latency, and more sensitive to
delay fluctuations that occur in every 10-second period. The
sampling of both RTT-related factors consistently chooses the
value that represents the best (averaged) quality a player
experienced. This interesting finding could be further verified
by cognitive models that explain why good experiences (rather
than bad experiences) have a stronger effect on players’
departure decisions.
C. Model Fitting
Since our two traces were recorded on a weekday and a
weekend respectively, the day of the week effect should be
incorporated into the modeling, if appropriate. For a 10-minute
observation period, the proportion of premature departures on
weekdays and weekends was 6.8% and 7.0% respectively,
which yields an odds ratio of 0.96. This difference between
the two groups of sessions yields a p-value of 0.67 in Fisher’s
test [14], which fails to reject the null hypothesis that their
odds are equal. Furthermore, if we take the binary variable
weekend as the only predictor in the logistic regression, both
the Wald statistic and the likelihood ratio test indicate that
weekend is insignificant with a critical value of 0.2. All
of these tests indicate that the day of the week does not
cause players to leave a game prematurely. Compared to
the discussion in Sec. IV-A, the phenomenon indicates that,
although users generally spend less time playing games on
weekdays, the time constraint on weekdays is not so stressful
that players are forced to quit the game within a short time,
e.g., 10 minutes. Thus, we do not include the weekend
variable in the model.
Like ordinary linear regression models, the logistic model
assumes that the contribution of each risk factor to the
response variable is linear and additive on the logistic scale.
To check whether our QoS factors confirm this assumption, we
fit the data into a generalized addictive model with smoothing
90.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Predicted probability of premature departures
O
bs
er
ve
d 
pr
op
or
tio
n 
of
 p
re
m
at
ur
e 
de
pa
rtu
re
s
Proportion of premature departures
45° straight line through (0,0)
Fig. 9. Examination of the prediction accuracy of the developed model
C-index for our model, 0.87, indicates generally good discrim-
ination compared to a C-index of 0.5, which is equivalent to
a random guess.
To demonstrate the predictive power of our model, we
compare the observed proportion of premature departures and
the predicted probabilities of premature departures, as shown
in Fig. 9. The red crosses mark the proportion of premature
departure events in each group, which is along the 45◦ straight
line through the origin. The figure shows that the predicted
probabilities match the actual probability well, which implies
that the general prediction accuracy of our model is good.
E. Model Cross-Validation
Our model’s prediction accuracy might be due to the fact
that it actually captures the relationship between variables, or
it might be due to overfitting. To confirm that the model does
not overfit the data, we use cross-validation to further verify
its adequacy.
The cross-validation steps are as follows: 1) randomly
divide all the game sessions into two equal-sized groups:
a modeling group and a validation group; 2) fit a logistic
model with the modeling group; 3) predict whether premature
departure events have occurred for the validation group based
on the fitted model, and compute the prediction accuracy; and
4) repeat steps 1–3 one hundred times.
Fig. 10 shows the cross-validation results. The prediction
accuracy differs according to the length of the observation
time. The median correct rates are generally higher than 70%
for observation times shorter than 30 minutes, and higher than
80% for times shorter than 10 minutes. We find that the cor-
rect rates in the worst cases can be quite low in some scenarios,
e.g., with the observation time of 14 minutes. We attribute this
phenomenon to the high variability of game session times.
In addition to the quality of network conditions, there are
many exogenous factors that could affect players’ decisions
to continue with a game or leave it. For example, players
may be tied by quests on hand or social bonds, even when
network conditions are poor and screen updates are jerky. On
the other hand, they may leave a game because of prearranged
events, schedule constraints, or physical conditions. Although
the worst-case prediction performance in the model’s cross-
validation is not good, overall, the median correct rates are
40
50
60
70
80
90
10
0
Observation time (min)
Co
rre
ct
 ra
te
 (%
)
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
Fig. 10. Validation of model fitting with cross-validation
acceptably high, especially when the observation time is
shorter than 10 minutes. This demonstrates that the model’s
accuracy is not a consequence of data overfitting.
F. Model Interpretation
In Table II, we present the estimated coefficients along
with their standard errors and p-values for the fitted logistic
model. All variables are significant at a significance level of
0.1. The coefficients of the model can be interpreted by odds
ratios. Since the magnitude of a coefficient implies a change
in the response (in the logit scale) for a one-unit increase
of the covariate, the odds ratio between two risk vectors
can be obtained by exponentiating their difference in logit
form. For example, assume that two players experience similar
network conditions, except for delay jitter of 20 ms and 10 ms
respectively. The odds ratio of these two sessions can then be
computed by exp((0.02− 0.01)× 86.14) ≈ 2.4, where 86.14
is the coefficient of the covariate jitter. That is, the odds that
player A will leave the game prematurely are 2.4 times higher
than the odds that player B will leave prematurely (i.e., quit
the game within 10 minutes of joining).
VI. MODEL IMPLICATIONS AND APPLICATIONS
In this section, we first discuss the implications of our
analysis results for other game genres. We then present a
predictability analysis of players’ premature departure behav-
ior. Next, we investigate the relative impact of various types
of network impairment on user perception. We conclude the
section by discussing how our model can be used to improve
system design, in terms of server processing scheduling, de-
jitter buffer dimensioning, and the choice of transport proto-
cols.
A. Implications for Other Game Genres
MMORPGs are slow-paced compared to other popular
genres, such as first-person shooting games, which require
players to make sub-second decisions. In addition to a game’s
pace, there is a great deal of difference in how players control
the virtual characters. In fast-action games like shooting,
players instruct characters “what” actions to take and “how” to
11
• Packet loss is much less tolerable than packet delay.
Comparing the overall influence of network latency and
network loss, we obtain a ratio of 3:7. This result is not
consistent with an earlier study of Unreal Tournament
2003 [4], where the authors reported that network latency
< 200 ms and network loss < 6% have a statisti-
cally weak impact on user performance. We believe this
discrepancy is due to the different transport protocols
employed.
While most FPS games use UDP to exchange informa-
tion between game peers, many MMORPGs, including
ShenZhou Online, use TCP. Since TCP provides in-order
delivery and congestion control, a lost packet will cause
subsequent packets to be buffered until it is successfully
delivered, which reduces TCP’s congestion window. On
the other hand, packet loss does not incur any overhead
in UDP. Thus, in TCP-based games, packet loss incurs
additional packet delay and delay jitters, both of which
further degrade players’ gaming experience. We discuss
the effect of transport protocols in the next subsection.
• Client packet loss is slightly less tolerable than server
packet loss. We consider this to be reasonable, since
client packet loss delays the players’ commands to the
server, whereas server packet loss delays responses to
the commands as well as state updates. Current MMOGs
are mostly server-centric, so a player’s command is not
effective until it has been processed by the server. In
addition, to speed up the responses to players’ commands,
game clients may “cheat” by displaying the expected
states in response to players’ own commands on receipt
of players’ inputs before those inputs validated by the
server. Thus, server packet loss only impacts on the
consistency between players’ views of the virtual world,
not the responsiveness to players’ inputs. As a conse-
quence, client packet loss, which may delay the players’
commands, such as attacks and spell casting, is more
annoying than server packet loss, which just delays the
server’s responses and screen updates.
D. Impact of Transport Protocols
Although TCP is generally considered to be unsuitable for
interactive and real-time communications, many MMORPGs
adopt it as their underlying transport protocol. One reason
is that TCP is stream-oriented, so that the message stream
at the sender will be identical to the stream received at the
destination. This property allows game developers to focus
on game development, and leave issues related to network
transmission to TCP. However, TCP can degrade message
transmission efficiency because the stream-oriented feature
is not required for each game message exchange; hence the
protocol’s in-order and reliable delivery might lead to overkill
sometimes.
To quantify the degradation of game message transmission,
we estimate the in-order delivery overhead in terms of the
additional delay jitters incurred. We believe that in-order
delivery is not necessary for all game messages for the
following reasons:
60 80 100 120 140
0.
12
0.
16
0.
20
0.
24
(a) Prioritizing server processing
Target RTT for high−risk sessions (ms)
Es
tim
at
ed
 p
re
m
at
ur
e 
de
pa
rtu
re
 ra
tio
0.0 0.2 0.4 0.6 0.8 1.0
0.
22
0.
24
0.
26
0.
28
0.
30
(b) Using de−jitter buffers
Proportions of de−jitter buffer users
Es
tim
at
ed
 p
re
m
at
ur
e 
de
pa
rtu
re
 ra
tio
Fig. 13. Reducing the probability of players’ premature departure by
providing more resources for high-risk sessions to ensure shorter delays or
less delay jitters.
• Many game messages are accumulative in nature, i.e.,
subsequent messages will override earlier ones. For exam-
ple, state updates, especially position updates, are usually
accumulated so that a missing message does not matter,
unless it is the last in a series of updates. Thus, a series
of accumulated commands, except for the last, could be
delivered in an unreliable and out-of-order manner.
• Some game messages can be processed in any order.
For example, server packets are primarily comprised
of accumulated state updates, dialogue messages, and
responses to queries, such as information about virtual
items. With the exception of dialogue messages, server
packages can usually be processed in any order.
To assess how much additional delay jitters are induced by
enforced packet ordering, we assume an extreme case where
game packets can be processed in any order. For our traces,
the average delay jitters are estimated in two ways: 1) 30
ms if delays induced by retransmitted packets are considered;
or 2) 18 ms if delays induced by retransmitted packets are
not considered. Based on the premature departure prediction
model developed in Section V, we estimate that the odds
of premature departure would be reduced by a factor of
exp((0.030− 0.018)× 86.14) ≈ 2.8 if additional delay jitters
could be eliminated. This corresponds to a 12% decrease in
the premature departure probability (from 20% to 8%) with an
observation time of 30 minutes. In this way, we can estimate
the degree of improvement if we replace TCP with a more
lightweight protocol that only orders packets when necessary.
Also, the result explains why packet loss generates so much
more intolerance among MMORPG players than FPS players
(Section VI-C).
E. Improving the Gaming Experience
In Section IV, we showed that the player departure rate
generally decreases over time; that is, the longer players
remain in a game, the less likely they are to leave the game in
every instant. Furthermore, in Section VI-B, we showed that
the relative influence of network impairment decreases over
time, as extraneous factors, such as social bonds, gradually
outweigh the effect of network QoS on players’ decisions
to continue or leave a game. By combining both properties,
13
much indebted to the following people who helped us gather
the trace: Tsing-San Cheng, Lawrence Ho, Chen-Hsi Li, and
especially to Yen-Shuo Su, who between them made the
datasets available. This work was supported in part by National
Science Council of the Republic of China under the grants
NSC 96-2628-E-001-027 and NSC 97-2221-E-001-009.
REFERENCES
[1] Aetas, Inc., “Gametrics weekly Korea MMORPG
population survey,” 2007. [Online]. Available:
http://www.4gamer.net/specials/gametrics/gametrics.shtml
[2] S. Aggarwal, H. Banavar, A. Khandelwal, S. Mukherjee,
and S. Rangarajan, “Accuracy in dead-reckoning based
distributed multi-player games,” in Proceedings of ACM
SIGCOMM 2004 workshops on NetGames ’04. ACM
Press, 2004, pp. 161–165.
[3] G. Armitage, “An experimental estimation of latency
sensitivity in multiplayer Quake 3,” in 11th IEEE In-
ternational Conference on Networks (ICON), 2003.
[4] T. Beigbeder, R. Coughlan, C. Lusher, J. Plunkett,
E. Agu, and M. Claypool, “The effects of loss and latency
on user performance in Unreal Tournament 2003,” in
Proceedings of NetGames’04. ACM Press, 2004, pp.
144–151.
[5] F. Chang and W. chang Feng, “Modeling player session
times of on-line games,” in NetGames ’03: Proceedings
of the 2nd Workshop on Network and System Support for
Games. ACM Press, 2003, pp. 23–26.
[6] K.-T. Chen, P. Huang, and C.-L. Lei, “Game traffic anal-
ysis: An MMORPG perspective,” Computer Networks,
vol. 50, no. 16, pp. 3002–3023, 2006.
[7] ——, “How sensitive are online gamers to network
quality?” Communications of the ACM, vol. 49, no. 11,
pp. 34–38, Nov 2006.
[8] K.-T. Chen, P. Huang, G.-S. Wang, C.-Y. Huang, and C.-
L. Lei, “On the sensitivity of online game playing time to
network QoS,” in Proceedings of IEEE INFOCOM’06,
Barcelona, Spain, Apr. 2006.
[9] W. S. Cleveland, “LOWESS: a program for smoothing
scatterplots by robust locally weighted regression,” The
American Statistician, vol. 35, no. 54, 1981.
[10] D. R. Cox and E. J. Snell, Analysis of Binary Data,
2nd ed. London: Chapman and Hall, 1989.
[11] A. DeLea´n, P. Munson, and D. Rodbard, “Simultaneous
analysis of families of sigmoidal curves: application
to bioassay, radioligand assay, and physiological dose-
response curves,” Am J Physiol Endocrinol Metab, vol.
235, pp. 97–102, 1978.
[12] J. Egan, Signal Detection Theory and ROC Analysis.
New York: Academic Press, 1975.
[13] W. C. Feng, F. Chang, W. C. Feng, and J. Walpole,
“A traffic characterization of popular on-line games,”
IEEE/ACM Transactions on Networking, vol. 13, no. 3,
pp. 488–500, June 2005.
[14] R. A. Fisher, “On the interpretation of χ2 from contin-
gency tables, and the calculation of p,” Journal of the
Royal Statistical Society, vol. 85, pp. 87–94, 1922.
[15] T. J. Hastie and R. J. Tibshirani, Generalized Additive
Models. London: Chapman and Hall, 1990.
[16] T. Henderson, “Latency and user behaviour on a multi-
player game server,” in Proceedings of the Third Interna-
tional COST Workshop (NGC 2001). Springer-Verlag,
2001, pp. 1–13.
[17] T. Henderson and S. Bhatti, “Modelling user behaviour in
networked games,” in MULTIMEDIA ’01: Proceedings of
the Ninth ACM International Conference on Multimedia.
ACM Press, 2001, pp. 212–220.
[18] ——, “Networked games: a QoS-sensitive application for
QoS-insensitive users?” in RIPQoS’03: Proceedings of
the ACM SIGCOMM Workshop on Revisiting IP QoS.
ACM Press, 2003, pp. 141–147.
[19] Hosmer, S. L. Cessie, and Lemeshow, “A comparison
of goodness of fit tests for the logistic regression model,
statistics in medicine,” Statistics in Medicine, vol. 16, pp.
965–980, 1997.
[20] D. W. J. Hosmer and S. J. Lemeshow, Applied logistic
regression, 2nd ed. New York: Wiley, 2000.
[21] S. Ila, D. Mizerski, and D. Lam, “Comparing the effect
of habit in the online game play of australian and
indonesian gamers,” in Proceedings of the Australia and
New Zealand Marketing Association Conference, 2003.
[22] F. E. H. Jr., R. M. Califf, D. B. Pryor, K. L. Lee, , and
R. A. Rosati, “Evaluating the yield of medical tests,”
Journal of the American Medical Association, vol. 247,
no. 18, pp. 2543–2546, 1982.
[23] E. L. Kaplan and P. Meier, “Nonparametric estimation
from incomplete observations,” Journal of the American
Statistical Association, vol. 53, pp. 437–481, 1958.
[24] M. Kendall, “A new measure of rank correlation,”
Biometrika, vol. 30, pp. 81–93, 1938.
[25] J. Nichols and M. Claypool, “The effects of latency
on online madden NFL football,” in Proceedings of
NOSSDAV’04. ACM Press, 2004, pp. 146–151.
[26] T. P. Novak, D. L. Hoffman, and A. Duhachek, “The
influence of goal-directed and experiential activities on
online flow experiences,” Journal of Consumer Psychol-
ogy, vol. 13, no. 1, pp. 3–16, 2003.
[27] M. Oliveira and T. Henderson, “What online gamers
really think of the internet?” in Proceedings of
NetGames’03. ACM Press, 2003, pp. 185–193.
[28] L. Pantel and L. C. Wolf, “On the impact of delay on
real-time multiplayer games,” in Proceedings of NOSS-
DAV’02. ACM Press, 2002, pp. 23–29.
[29] ——, “On the suitability of dead reckoning schemes
for games,” in NETGAMES ’02: Proceedings of the 1st
workshop on Network and system support for games.
ACM Press, 2002, pp. 79–84.
[30] V. Paxson and M. Allman, “Computing
TCP’s Retransmission Timer,” RFC 2988 (Pro-
posed Standard), Nov. 2000. [Online]. Available:
http://www.ietf.org/rfc/rfc2988.txt
[31] V. Paxson and S. Floyd, “Wide area traffic: the failure
of Poisson modeling,” IEEE/ACM Transactions on Net-
working, vol. 3, no. 3, pp. 226–244, 1995.
[32] P. Quax, P. Monsieurs, W. Lamotte, D. D. Vleeschauwer,
Multimed Tools Appl (2009) 45:7–32
DOI 10.1007/s11042-009-0297-5
On the challenge and design of transport
protocols for MMORPGs
Chen-Chi Wu · Kuan-Ta Chen · Chih-Ming Chen ·
Polly Huang · Chin-Laung Lei
Published online: 13 May 2009
© Springer Science + Business Media, LLC 2009
Abstract Although MMORPGs are becoming increasingly popular as well as a
highly profitable Internet business, there is still a fundamental design question:
Which transport protocol should be used—TCP, UDP, or some other protocol? In this
paper, we first evaluate whether TCP is suitable for MMORPGs, and then propose
some novel transport strategies for this genre of games. Our analysis of a trace
collected from a TCP-based MMORPG called ShenZhou Online indicates that TCP
is unwieldy and inappropriate for MMORPGs. We find that the degraded network
performance problems are due to the following characteristics of MMORPG traffic:
1) tiny packets, 2) a low packet rate, 3) application-limited traffic generation, and
4) bi-directional traffic. Since not all game packets require reliable transmission or
in-order delivery, transmitting all packets with a strict delivery guarantee causes
high delays and delay jitters. Therefore, our proposed transport strategies assign
game packets with appropriate levels of transmission guarantee depending on the
requirements of the packets’ contents. To compare the performance of our approach
with that of existing transport protocols, we conduct network simulations with a
C.-C. Wu · C.-M. Chen · P. Huang · C.-L. Lei
Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan
C.-C. Wu
e-mail: bipa@fractal.ee.ntu.edu.tw
C.-M. Chen
e-mail: cmchen@gmail.com
P. Huang
e-mail: phuang@cc.ee.ntu.edu.tw
C.-L. Lei
e-mail: lei@cc.ee.ntu.edu.tw
K.-T. Chen (B)
Institute of Information Science, Academia Sinica, No. 128, Sec. 2, Academia Rd.,
Nankang District, Taipei 115, Taiwan
e-mail: ktchen@iis.sinica.edu.tw
Multimed Tools Appl (2009) 45:7–32 9
Table 1 The transport
protocols used by popular
MMORPGs
Protocol MMORPGs
TCP World of Warcraft, Angel’s Love, Lineage I/II,
Guild Wars, Ragnarok Online
UDP EverQuest, City of Heroes, Asheron’s Call, Ultima
Online, Final Fantasy XI
TCP/UDP Dark Age of Camelot
3. The congestion control mechanism is ineffectual because game traffic is
application-limited, rather than network-limited. In our trace, 12% of client
packets and 18% of server packets encounter a congestion window reset, which
prevents the packets from being transmitted immediately and induces additional
latencies.
4. The fast-retransmit algorithm is ineffective because game traffic is bi-directional
and packets are generated at a slow rate. We found that more than 99%
of dropped packets sent from servers were not detected until retransmission
timeout occurred. This causes the average latency of lost packets to be much
longer than that of normal (non-lost) packets.
UDP is another widely used transport protocol, but it cannot be applied to
MMORPGs directly due to the lack of reliable and in-order transmission. To deal
with packet loss and the reordering of game messages that need strict reliability and
in-order processing, an excessive amount of effort is required if UDP is adopted.
In practice, the most appropriate protocol for MMORPGs is a hybrid that pro-
vides different levels of transmission guarantee based on the requirements of game
packets, so they can be transmitted efficiently, i.e., the delays and delay jitters are as
low as possible. For example, chat messages should be transmitted reliably and in an
orderly manner, while the loss of certain position updates for avatars is tolerable. To
meet these requirements, we propose content-based transport strategies that provide
each type of packet content with the appropriate level of delivery guarantee. Based on a
trace collected from Angel’s Love, we conducted network simulations to compare our
strategies with some existing transport protocols in terms of network performance.
The results show that our strategies incur much lower end-to-end delay and end-
to-end delay jitter than most of the protocols we evaluated. Finally, based on a
model that describes the relationship between network quality and users’ playing
time [4, 5], we designed the Game Satisfaction Index (GSI) to quantify the gaming
experience of players under different network conditions. We show that the level of
player satisfaction under our transport strategy is 3.57 times better than that of TCP.
Our contribution in this work is three-fold:
1. Based on a realistic game trace, we analyze the performance of TCP and
demonstrate that it is unsuitable for MMORPGs.
2. We propose content-based transport strategies for MMORPGs, and evaluate
their performance improvement over existing protocols.
3. We present the Game Satisfaction Index, which explicitly indicates that the
proposed transport strategies significantly improve player satisfaction in MMO-
RPGs.
Multimed Tools Appl (2009) 45:7–32 11
3 Analysis of TCP performance
TCP is a connection-oriented transport protocol that provides reliable transmission,
in-order delivery, congestion control, and flow control. In this section, we analyze
the performance of TCP in MMORPGs based on real-life game traces. We find that
in-order delivery causes unnecessary transmission latencies and high delay jitters,
and the protocol’s congestion control and loss recovery mechanism are ineffective in
MMORPGs.
3.1 Trace description
First we describe the traces used for evaluating the performance of TCP in MMO-
RPGs. ShenZhou Online is a TCP-based MMORPG that is popular in Taiwan [21],
where there are thousands of players online at any one time. In this game, players
can engage in fights with other players or random creatures, train their avatars to
acquire special skills, participate in commercial activities, or take on a quest. With
the assistance of ShenZhou Online staff, we set up a traffic monitor to capture
packets sent from and received by the game servers. The collected traces are
summarized in Table 2. Although the game servers were located in Taiwan, we
found that the players in the traces were spread over 13 countries and hundreds of
autonomous systems. The wide distribution of clients manifests the heterogeneity of
the network paths’ characteristics and the generality of the traces. Because of space
limitations, we refer interested readers to [3] for details of the game description and
measurement setup.
Next, we investigate the characteristics of the collected traces. For brevity, we
denote packets sent by game clients as client packets, and all traffic sent by clients as
client traffic. The terms server packets and server traffic are defined similarly. Figure 1
shows the cumulative distribution function (CDF) of the payload size. Clearly, client
packets and server packets are drastically different in terms of the payload size. The
reason is that a client packet only contains one player’s commands, whereas a server
packet for a player may contain the actions and states of nearby avatars, and even
system notifications. Client packets are relatively small. In the traces, 98% of the
packets have a payload size smaller than 32 bytes. Specifically, the payload sizes of
23 and 27 bytes comprise 36% and 52% of the packets respectively. This distribution
evidences that user actions are dominated by a few popular commands, such as attack
and walk, so that a large proportion of client packets have certain payload sizes. In
contrast, server packets have a wider distribution and the average payload size is
114 bytes.
Table 2 Summary of game traffic traces
Trace Date Period Dropsa Session Pkt. (in / out) Bytes (in / out)
N1 8/29/04 (Sun.) 8 h 0.003% 7,597 342M / 353M 4.7TB / 27.3TB
N2 8/30/04 (Mon.) 12 h ?b 7,543 325M / 336M 4.7TB / 21.7TB
a This column gives the kernel drop count reported by tcpdump
b The reported kernel drop count was zero, but we found that some packets were actually dropped
at the monitor
Multimed Tools Appl (2009) 45:7–32 13
generated and transmitted over the network. In [2], the author proposed a delayed
ack mechanism that allows the host to only send an ack for alternate data packets
received in a connection, unless the delayed ack timer (usually set at 200 ms) has
expired. In bulk data transfer scenarios, where packets are sent in bursts, the majority
of successive packets arrive at the destination before the delayed ack timer expires.
Therefore, the number of ack packets should be reduced to half the number of data
packets. However, because of the low packet rate in MMORPGs, the subsequent
game packet may not arrive in 200 ms from the time the preceding packet was
received.
In our traces, the number of client ack packets is much more than half the
number of server data packets. This is because approximately 40% of the interarrival
times for server packets are longer than 200 ms (as shown in Fig. 2). Furthermore,
38% of the packets in the traces are pure ack packets. Because of the relatively
large proportion of pure ack packets and the small payload size of game packets,
the network bandwidth occupied by the TCP/IP header constitutes an excessive
overhead. For example, in the traces, TCP/IP headers occupy 46% of the bandwidth
consumed by the game traffic.
3.3 In-order delivery
TCP has an in-order delivery policy that guarantees the receiver will process data
packets in the order generated by the sender. Under this mechanism, a packet must
be buffered at the receiver until all of its preceding packets have been received and
processed. In other words, if a packet is lost in the network, all of its subsequent
packets that have been received already would suffer additional delay. However,
certain game packets do not require in-order processing, so the mechanism may
cause unnecessary delay. For example, if a player attacks an opponent repeatedly
with the same weapon and the same action, the game application will generate a
number of duplicate packets with attack messages and transmit them to the server.
In this case, the packets can be processed out of order at the server without affecting
the outcome of the game. Moreover, position or state updates in MMORPGs are
often designed to be accumulative; that is, subsequent updates override earlier ones.
As a result, an update packet can be processed immediately on arrival, instead of
being buffered until all of its preceding packets arrive at the destination host.
To evaluate how much additional delay is caused by strict in-order delivery, we
consider an extreme case in which all game packets can be processed in an arbitrary
order. We investigate the influence of packet loss, which is the main source of out-
of-order delivery, in terms of transmission latency and delay jitters (i.e., the standard
deviation of the latency). Since the traces were collected at the server side, we define
latency as the time difference between the departure time of a server packet and the
time the corresponding ack packet is received.
In Fig. 3a, we show the average latency of normal (non-lost) packets, lost packets,
and all packets. Before retransmitting a dropped packet, the sender needs extra time
to detect a lost packet. Thus, the average latency of packets that have been lost
(576 ms) is much longer than that of normal packets (186 ms). Figure 3b shows the
expansion ratio of latency due to packet loss for connections that experienced at least
one packet loss. We observe that 33% of them suffered more than 10% additional
Multimed Tools Appl (2009) 45:7–32 15
Fig. 4 The impact of packet
loss on delay jitters (a, b)
(a) Delay jitter (ms)
D
en
si
ty
0 100 200 300 400
0.
00
0.
01
0.
02
0.
03 normal packetslost packets
all packets
0 50 100 150 200 250 300
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
(b) Expansion ratio of delay jitter (%)
Cu
m
ul
at
ive
 d
ist
rb
ut
io
n 
fu
nc
tio
n
The AIMD policy is based on the assumption that data transmission is network-
limited, i.e., the sender always has data to send in the connection. In the case of
bulk transfers, the cwnd inflates when a packet is acknowledged, and shrinks when
packet loss occurs. Hence, the cwnd oscillates between the minimum size and a
value corresponding to the available bandwidth. However, in MMORPGs, data
generation is application-limited, i.e., an application usually has a smaller amount
of data to send compared with the available bandwidth it can use. Thus, packet loss
is rare in this kind of game because the traffic load is usually much smaller than the
network capacity. As a result, the cwnd becomes arbitrarily large and cannot faithfully
reflect the condition of the network path. In our traces, the maximum and average
window sizes in the server to client direction are 1.7 Mbps and 372 Kbps respectively.
Both sizes are too large to reflect the available bandwidth of individual flows. In
addition, we found that 36% of connections experienced no packet loss, which
would never happen in network-limited applications with sufficient data. A potential
problem associated with an inaccurately large window size is that an application with
application-limited traffic may occasionally have a large burst of packets to send in a
short time. This condition results in the release of an overwhelming traffic load due
to the inappropriately large window size; therefore, the network bandwidth will be
exhausted and congestion will occur unnecessarily.
Although the congestion window tends to become arbitrarily large in MMORPGs,
it could be reset incorrectly because of the restart after idle periods policy [1]. In a
connection, if the sender has not released packets in an interval longer than one re-
transmission timeout (RTO), the congestion window should be reset to two packets.
The purpose is to prevent an inappropriate burst of packets being transmitted due
Multimed Tools Appl (2009) 45:7–32 17
Table 3 The reasons why fast
retransmit failed to trigger
Cause Ratio
Insufficient duplicate acks 50.96%
Duplicate ack accumulation was interrupted 49.04%
New data 48.90%
New ack 0.02%
Window size change 0.12%
of the lost packets were not detected by the game servers until RTO occurred.
This surprising result indicates that the fast-retransmit algorithm is ineffective in
MMORPGs. According to our analysis, the mechanism failed for two reasons: 1)
insufficient duplicate acks; and 2) the accumulation of duplicate acks was interrupted.
As shown in Table 3, the occurrence frequency is approximately the same in both
cases. To trigger the fast retransmit mechanism, a sender should receive at least three
duplicate acks within an (RTO − RTT) interval following a dropped packet. In other
words, within that interval, the sender must release at least three additional packets,
each of which will elicit a duplicate ack. However, because the server packet rate
is too low, it seems unlikely that four packets could be released within that short
period. To examine this conjecture, we compare the distribution of (RTO − RTT)
intervals with that of the server packet interarrival times detailed in Fig. 6. The
shaded area indicates that 34% of the server packet interarrival times were probably
longer than their corresponding (RTO − RTT) intervals; that is, there were no
subsequent packets within that interval. Hence, fast retransmit was not triggered
because insufficient duplicate acks were generated.
In the second case, the fast retransmit mechanism failed because the counting
of duplicate acks was interrupted by other packets. According to [1, 19] and the
implementation of 4.4BSD, the definition of a series of duplicate acks is strict in that
each ack packet must not contain data, must have the same receiver window size,
and must have the same acknowledgement sequence number. By this definition, if a
receiving host sends a data packet to a sender that is waiting for more duplicate acks
in that connection, the count of duplicate acks will be reset to zero. Thus, the fast
retransmit mechanism will not be triggered unless the counting of duplicate acks is
Fig. 6 Server packet
interarrival times are
comparable to average
RTO—average RTT
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Time (ms)
Cu
m
ul
at
ive
 d
ist
rib
ut
io
n 
fu
nc
tio
n
0 200 400 600 800 1000
RTO _ RTT
Server packet interarrivals
Multimed Tools Appl (2009) 45:7–32 19
discuss the design of efficient transport strategies for MMORPGs. Then, we propose
three content-based transport strategies that provide an appropriate transmission
guarantee for each type of game message in order to opportunistically improve the
packet delivery performance.
4.1 Prerequisites of MMORPG transport protocol design
The transmission requirements of game packets are diverse because of the intrinsic
characteristics of messages contained in the packets. For example, some messages
require in-order and reliable transmission, while the loss of some messages is
tolerable. For MMORPGs, we generally classify game messages generated by players
into three types: move, attack, and talk messages.
– Move messages report position updates when an avatar moves or goes to a new
area. Since only the latest location in the game play matters, the server simply
discards out-of-date move messages.
– Attack messages correspond to an avatar’s combat actions when it engages in
fights with opponents. Such messages cannot be lost because each action will
have some impact on the target. However, if several successive attack messages
describe the same combat action against the same target, out-of-order arrivals of
these messages can be tolerated, since they can be processed in a different order
without affecting the final outcome.
– Talk messages convey the contents of conversations between players. To display
the complete contents to players in exactly the same order as they were typed,
talk messages must be transmitted in order and reliably.
Intuitively, it is clear that transmitting game messages with short delays and low
delay jitters leads to high interactivity and responsiveness in the game play, as game
messages can be processed more quickly and smoothly by the receiving hosts. Based
on the above descriptions of game messages, we define three levels of transmission
requirements for different types of messages.
– Under the strictest level, messages must be transmitted in order and reliably.
For talk messages, this QoS level must be guaranteed so that the contents of
conversations can be displayed correctly.
– For messages that can be processed out of order, in-order delivery is not essential.
For example, if a player attacks an opponent with the same weapon, a series of
repeat messages are generated. Since these messages are semantically and visu-
ally equivalent, out-of-order processing will not adversely impact the outcomes
of the game play.
– Some types of messages do not require reliable transmission because they can
be lost without affecting the game’s logic. For example, the loss of most move
messages is tolerable because they are designed to be cumulative; that is, each
new message will override the previous ones. However, the last message in a
series of move updates must be delivered to the server because it reports the
avatar’s latest location.
Multimed Tools Appl (2009) 45:7–32 21
5 Performance evaluation
In this section, we first discuss the transport protocols to be evaluated, namely
TCP, UDP, DCCP, SCTP, and the protocols based on our proposed strategies.
We then describe the game traces used in the network simulations and the setup
of the experiment. In the performance evaluation part, we first assess the network
performance of the existing protocols. Then, to demonstrate the efficacy of our
opportunistic content-based transport protocols, we compare their performance with
that of the existing protocols. Finally, we show how the improvement in network
performance affects players’ gaming experience.
In our evaluation, the end-to-end delay of a message is the difference between its
sending time and its receiving time observed at the application level. For brevity, we
denote messages sent by game clients as client messages and messages sent by the
game server as server messages.
5.1 Description of evaluated protocols
– TCP: We have already discussed TCP and analyzed its performance in Section 3.
– UDP: UDP is a connectionless transport protocol that does not provide in-order
delivery, reliable transmission, and congestion control. We expect that UDP will
outperform the compared protocols in terms of end-to-end delay and end-to-end
delay jitter because it does not stipulate loss recovery and packet ordering, both
of which incur additional delays.
– DCCP: DCCP is designed for real-time multimedia applications that need a
reasonable degree of control in the presence of network congestion [14]. It
implements two congestion control mechanisms: a TCP-like congestion control
(TCP-like) [9] and a TCP friendly rate control (TFRC) [10]. However, it is not
reliable and it does not provide in-order delivery. To the best of our knowledge,
no previous studies have evaluated its performance on MMORPGs. We employ
DCCP with a TCP-like congestion control mechanism in our simulation.
– SCTP: SCTP [20] provides optional in-order transmission and multi-streaming. It
can be flexibly configured to satisfy different levels of transmission requirement.
We employ a simple SCTP configuration that only provides one ordered and
reliable stream, which is the most common setting used by game developers. This
configuration is similar to that used in [13].
To evaluate the effect of our three content-based strategies, we use them to
develop the following three content-based transport protocols.
– PMRO implements the MRO strategy, which puts move, attack, and talk messages
into three separate ordered and reliable streams.
– PMR is based on the MR strategy. It transmits move and attack messages via two
unordered and reliable streams individually, while talk messages are put into an
ordered and reliable stream.
– PM employs the M strategy, which transmits move messages via an unordered
and unreliable stream, attack messages via an unordered and reliable stream,
and talk messages via an ordered and reliable stream.
The main difference between our protocols and existing protocols is that the latter
transfer each type of message in the same way, whereas our content-based transport
Multimed Tools Appl (2009) 45:7–32 23
click, whereas a talk message contains several words or letters that require several
keystrokes.
5.3 Experiment setup
We conduct experiments with the ns-2 simulator, which is useful and therefore widely
used in networking researches. As shown in Fig. 8, we deploy m + n + 3 nodes: a
game server and two intermediate nodes that serve as network routers; n nodes are
the game clients, and the remaining m nodes are traffic nodes. The game clients and
traffic nodes are randomly connected to one of the routers. For the links between
the routers and the server, we set the bandwidth at 600 Kbps, and the propagation
delay at 70 ms. For the links between the other nodes and the routers, the bandwidth
varies between 64 Kbps and 128 Kbps, and the propagation delay is set at 70 ms.
To simulate cross traffic on the Internet, 11 pairs of traffic nodes are configured to
generate UDP traffic in an exponential distribution with a sending rate of 500 Kbps.
The cross traffic causes a 4% packet loss rate in the network approximately.
We implement the game server and the game client modules to simulate commu-
nications between the server and clients in MMORPGs. The game client module
is trace-driven, so it can replay the action-based traces in the simulated network.
In other words, the client module regenerates and sends game messages based on
the times and the types of messages recorded in the traces. Meanwhile, the game
server module tracks the locations of avatars by the move messages reported by game
clients. When a client sends the game server a message about the avatar’s next action,
the server responds to the client and, depending on the type of the action, notifies the
target clients related to the action (e.g., two players who are chatting) or other clients
whose avatars are in the same zone (e.g., location updates).
We apply each of the discussed transport protocols to transport game messages.
The number of clients is set between 10 and 150. For each protocol, we run each
setting for 1000 iterations, each of which simulates a different client arrival pattern.
Specifically, the client interarrival time is a uniform distribution with an interval
between 0 and 1 second.
5.4 Comparison of existing protocols
We begin by investigating the mean end-to-end delays of transmitting game mes-
sages using the existing transport protocols. As the number of game clients in the
simulations varies from 10 to 150, we can examine the performance under different
Fig. 8 Experiment network
topology
Multimed Tools Appl (2009) 45:7–32 25
no additional delay will occur in the transport level. In addition, DCCP(TCP-like)
always achieves the second best performance, and SCTP yields the third best. The
performance of SCTP is similar to that of TCP because we employ an ordered and
reliable stream, which provides a transmission guarantee similar to that in TCP.
DCCP(TCP-like) can be viewed as UDP plus a congestion control mechanism, so its
mean delays are very close to those of UDP. Our observation supports the conjecture
that the more functionalities a protocol provides, the worse the end-to-end delay
experienced by packets.
We also consider the mean end-to-end delay jitter incurred by different protocols,
as shown in Fig. 10. The trends in the results and the differences between the results
of these protocols are very similar to those in Fig. 9, since a high average end-to-end
delay usually implies that the end-to-end delay may vary over a wide range.
5.5 Evaluation of content-based transport protocols
We now compare the mean end-to-end delays of our proposed content-based
transport protocols with those of TCP, SCTP, and UDP, as shown in Fig. 11. Overall,
the trends of our protocols as the number of game clients increases are similar to
those of the compared protocols. We observe that the more flexible the strategies
employed by our protocols, the better the performance they achieve. PMRO uses three
TCP streams to transmit different types of messages so that the sequencing delays
between different types of messages can be eliminated. Therefore, in most of the
scenarios, the mean delays under PMRO are lower than those in TCP. For PMR, move
and attack messages are transmitted via two separate streams with optional ordering,
so its performance is better than that of PMRO. PM achieves the best performance
among our protocols because approximately 80% of move messages do not have
to be transmitted reliably and in order. This avoids a large number of unnecessary
retransmissions and reduces sequencing delays for non-critical move updates. As a
result, the mean delay of PM is relatively close to that of UDP. We also analyze the
mean end-to-end delay jitter, as shown in Fig. 12. Again, the results are similar to
those for end-to-end delays, i.e., a high average end-to-end delay implies that the
20 40 60 80 100 120 140
18
0
20
0
22
0
24
0
26
0
28
0
30
0
Client Traffic
Client #
M
ea
n 
de
la
y 
(m
s)
TCP
UDP
SCTP
P_MRO
P_MR
P_M
20 40 60 80 100 120 140
20
0
30
0
40
0
50
0
60
0 Server Traffic
Client #
M
ea
n 
de
la
y 
(m
s)
TCP
UDP
SCTP
P_MRO
P_MR
P_M
Fig. 11 The mean end-to-end delays of TCP, UDP, SCTP, and our transport protocols
Multimed Tools Appl (2009) 45:7–32 27
P_MRO P_MR P_M
Client Traffic
Protocol
Sc
or
e
0
20
40
60
80
10
0
0 1 1 0
69
79
70 Clients
140 Clients
P_MRO P_MR P_M
Server Traffic
Protocol
Sc
or
e
0
20
40
60
80
10
0
1
5 5
28
46
73
70 Clients
140 Clients
Fig. 14 Scores of mean end-to-end delay jitters of our transport protocols
clients. Obviously, PM outperforms the other two protocols. Furthermore, we observe
that our transport protocols achieve a bigger improvement as the number of clients
increases. The normalized scores of the mean delay jitter are shown in Fig. 14. The
results are similar to those in Fig. 13 because reducing the end-to-end delay further
reduces the end-to-end delay jitter effectively.
Finally, to examine the effect of providing different levels of transmission guar-
antee to different types of messages by using PM, we plot the mean end-to-end delay
of each type of message in Fig. 15. For the three types of messages, the magnitudes
of the delays they encountered correspond to the transmission guarantee assigned to
them. Thus, providing an appropriate guarantee for each game packet based on its
content effectively minimizes end-to-end delay and delay jitter.
20 40 60 80 100 120 140
20
0
22
0
24
0
26
0
Client Traffic
Client #
M
ea
n 
de
la
y 
(m
s)
Move msg
Attack msg
Talk msg
20 40 60 80 100 120 140
20
0
25
0
30
0
35
0
Server Traffic
Client #
M
ea
n 
de
la
y 
(m
s)
Move msg
Attack msg
Talk msg
Fig. 15 The mean end-to-end delay of each type of messages in PM
Multimed Tools Appl (2009) 45:7–32 29
the improvement in network performance derived by our strategies can effectively
raise the level of gaming satisfaction among players. To implement our strategies, it
is necessary to identify game packets according to the types of contents. However, as
these strategies are more efficient than non-content-based protocols, this additional
task for game developers is worthwhile because it will bring much better gaming
experiences to players.
Acknowledgements The authors would like to thank the anonymous reviewers for their helpful
comments. This work was supported in part by Taiwan Information Security Center (TWISC),
National Science Council under the grants NSC97-2219-E-001-001 and NSC97-2219-E-011-006. It
was also supported in part by the National Science Council of Taiwan under the grants NSC96-2628-
E-001-027-MY3 and NSC97-2221-E-001-009.
References
1. Allman M, Paxson V, Stevens W (1999) TCP congestion control. RFC 2581
2. Braden R (1989) Requirements for internet hosts—communication layers. RFC 1122
3. Chen K-T, Huang P, Lei C-L (2006) Game traffic analysis: an MMORPG perspective. Comput
Networks 50(16):3002–3023
4. Chen K-T, Huang P, Lei C-L (2006) How sensitive are online gamers to network quality?
Commun ACM 49(11):34–38
5. Chen K-T, Huang P, Lei C-L (2009) Effect of network quality on player departure behavior in
online games. IEEE Trans Parallel Distrib Syst 20(5):593–606
6. Claypool M (2005) The effect of latency on user performance in real-time strategy games. Com-
put Networks 49(1):52–70 (special issue on Networking Issues in Entertainment Computing)
7. Claypool M, Claypool K (2006) Latency and player actions in online games. Commun ACM
49(11):40–45
8. ENet (2008) ENet: an UDP networking layer for the multiplayer first person shooter cube.
http://enet.cubik.org/
9. Floyd S, Kohler E (2006) Profile for datagram congestion control protocol (DCCP) congestion
control ID 2: TCP-like congestion control. RFC 4341
10. Floyd S, Kohler E, Padhye J (2006) Profile for datagram congestion control protocol (DCCP)
congestion control ID 3: Tcp-friendly rate control (TFRC). RFC 4342
11. GameDev.Net (2004) FAQ—multiplayer and network programming. http://www.gamedev.net/
12. Griwodz C, Halvorsen P (2006) The fun of using TCP for an MMORPG. In: NOSSDAV ’06:
proceedings of the 2006 international workshop on Network and operating systems support for
digital audio and video, pp 1–7
13. Harcsik S, Petlund A, Griwodz C, Halvorsen P (2007) Latency evaluation of networking mecha-
nisms for game traffic. In: NetGames ’07: proceedings of the 6th ACM SIGCOMM workshop on
network and system support for games, pp 129–134
14. Kohler E, Handley M, Floyd S (2006) Datagram congestion control protocol (DCCP). RFC 4340
15. Mathis M, Mahdavi J, Floyd S, Romanow A (1996) TCP selective acknowledgement options.
RFC 2018
16. OpenTNL (2004) The Torque network library. GarageGames. http://www.opentnl.org/
17. Pack S, Hong E, Choi Y, Park I, Kim J-S, Ko D (2002) Game transport protocol: lightweight
reliable transport protocol for massive interactive on-line game. In: Proceedings of the SPIE,
vol. 4861. pp 83–94
18. Shirmohammadi S, Georganas ND (2001) An end-to-end communication architecture for col-
laborative virtual environments. Comput Networks 35(2–3):351–367
19. Stevens RW (1995) TCP/IP Illustrated, vol 2: the implementation. Addison-Wesley, Reading
20. Stewart R (2007) Stream control transmission protocol. RFC 4960
21. UserJoy Technology (2007) ShenZhou Online. http://www.ewsoft.com.tw/
22. Woodcock BS (2008) An analysis of MMOG subscription growth—version 23.0. http://
www.mmogchart.com/
Multimed Tools Appl (2009) 45:7–32 31
Chih-Ming Chen received his B.S. in Computer Science from National Chiao Tung University in
2006. He received his M.S. in the department of Electrical Engineering, Nation Taiwan University.
His research interests include internet measurement and traffic identification.
Polly Huang is an associate professor at the Department of Electrical Engineering, the Gradu-
ate Institute of Communication Engineering (joint appointment), and the Graduate Institute of
Networking and Multimedia (courtesy) of National Taiwan University. Dr. Huang received her
Ph.D. (1999) in Computer Science from University of Southern California, and her B.S. (1993) in
Mathematics from National Taiwan University. Prior to joining National Taiwan University, she
spent time in the early stage of her career in AT&T Labs-Research, Swiss Federal Institute of Tech-
nology (ETH) Zurich, and UCLA. Dr. Huang’s research interest spans Internet characterization,
sensor networking, multimedia networking, and performance evaluation. She serves currently on
the editorial board of the Journal of Communications and Networks. She is a member of IEEE and
ACM.
1Quadrant of Euphoria: A Crowdsourcing
Platform for QoE Assessment
Kuan-Ta Chen1, Chi-Jui Chang1, Chen-Chi Wu2, Yu-Chun Chang2, and
Chin-Laung Lei2
1Institute of Information Science, Academia Sinica
2Department of Electrical Engineering, National Taiwan University
Abstract
Existing QoE (Quality of Experience) assessment methods, subjective or objective, suffer from either
or both problems of inaccurate experiment tools and expensive personnel cost. The panacea for them, as
we have come to realize, lies in the joint application of paired comparison and crowdsourcing, the latter
being a Web 2.0 practice of organizations asking ordinary, unspecific Internet users to carry out internal
tasks. We present in this article Quadrant of Euphoria, a user-friendly, web-based platform facilitating
QoE assessments in network and multimedia studies, with features low cost, participant diversity,
meaningful and interpretable QoE scores, subject consistency assurance, and burdenless experiment
process.
Index Terms
Bradley-Terry-Luce Model, Crowdsourcing, Mean Opinion Score (MOS), Paired Comparison, Prob-
abilistic Choice Model, Quality of Experience (QoE)
The everlasting endeavor by network and multimedia researchers to satisfy the end-users’
growing needs has spawned many serious disciplines, of which the study and assessment of
Quality of Experience [1] has proved one of the most challenging. Quality of Experience, or
QoE for short, rivets on the true feelings of end-users from their perspective when they watch
podcasts, listen to digitized music, and leaf through online photo albums, to name a few. It should
32) Scale ordinality [4]. The MOS scale is actually not interval but ordinal, or put otherwise,
a ranked order of five arbitrary numbers. The cognitive distance between Bad (1) and
Poor (2) is usually not the same as that between Good (4) and Excellent (5). It is thus
questionable to calculate the final score by taking the arithmetic mean, which is only
defined on interval scales.
To give an overview of the article, we identify that there are two motivations or problems
to be solved: that subjective QoE tests, in particular MOS, have identifiable intrinsic problems,
and that they incur high personnel and time costs. We propose replacing category rating with
paired comparison to tackle the first (“A Kickoff with Paired Comparison”), and employing
crowdsourcing to address the second (“A QoE Assessment Platform”). Crowdsourcing, however,
brings about another problem (“The Crowd Is Not All Trustworthy”), inspiring the invention of
Transitivity Satisfaction Rate to ensure participant consistency (“Ensuring Subject Consistency”).
The combination of all these endeavors is an unprecedented QoE assessment platform, the titular
Quadrant of Euphoria, whose user interface and operation are described in “Platform Design”.
Finally, we evaluate the platform and and discuss the use of crowdsourcing in general in “Case
Studies and Evaluation”.
A KICKOFF WITH PAIRED COMPARISON
To tackle the problems aforementioned, we propose to assess QoE with paired comparison,
so that the test subject needs only to choose one better stimulus at a time from two based on his
perception. The dichotomous decision is clearly less onerous and less confusing to participants
of QoE experiments than a scaled rating. The features of paired comparison include:
• Applicability to all kinds of network and multimedia systems;
• Elimination of scaled rating and all accompanying problems;
• Quantified assessments of QoE, i.e., QoE scores, on an interval scale through the use of
available probabilistic choice models [5];
and finally,
• The transitive property of preference, which is instrumental in checking the consistency of
unsupervised subjects (described later).
Suppose we have n algorithms for, say, audio compression to rate. One by one we apply them
to a “testbed” audio clip, creating n synchronized stimuli to be paired with each other. There
5Internet crowd. We argue that they are the ideal subjects of QoE experiments for their headcount,
diversity, and (relative) nonchalance to monetary rewards. They often respond to problem-solving
requests solely for kudos, intellectual satisfaction, or a sense of being helpful. It is thus perfectly
reasonable to crowdsource QoE assessing experiments instead of hiring part-timers to carry
them out in the laboratory, since, after all, the crowd is to whom researchers labor to provide
ever-improving service quality.
Crowdsourcing (yet another Web 2.0 neologism) advocates mass collaboration and the wisdom
of the commons. The academia has long embraced it even before the name was coined. NASA-
sponsored Clickworkers1, for instance, began to utilize online volunteers to classify Martian
craters as early as November 2000. Crowdsourcing also has potential in the public sector, most
notably in the area of urban planning [6]. Commercially speaking, crowdsourcing is a further
step from outsourcing in that the task performers are no longer specific, identifiable persons.
Footwear manufacturers like Portland-based RYZ are known to have adopted community-based
designs, thereby cutting costs and cultivating a kinship between themselves and customers.
Conceptually, we picture Quadrant of Euphoria as a fulfilment of network and multimedia
researchers’ need to conduct QoE experiments without any interface programming, thus being
able to focus on their fields of expertise while benefiting from the advantages of paired com-
parison. The idea of a crowdsourcing platform is drawn from websites like InnoCentive [7] and
the Amazon Mechanical Turk (MTurk)2. InnoCentive is a service through which organizations
seek the assistance of a global scientific community for innovative solutions to challenging R&D
problems, and give professional recognition and cash rewards to problem-solvers in return. MTurk
is a popular crowdsourcing marketplace where anyone calling for help from the Internet crowd
can post their tasks. The tasks to be performed can be of any kind, ranging from surveys,
experiments to answering mundane questions.
The Crowd Is Not All Trustworthy
We focus now on the one formidable challenge to crowdsource QoE experiments: Not every
Internet user is good-natured. Since subjects perform experiments without supervision, they
1National Aeronautics and Space Administration, http://clickworkers.arc.nasa.gov/
2Mechanical Turk, http://www.mturk.com/
7(a)
(b) (c)
Fig. 1: Quadrant of Euphoria’s portal and administrative pages. (a) The Portal. The upper pane
directs researchers to administrative pages; the lower pane displays for subjects the catalogue of
open experiments. (b) Profile registration page. (c) Profile update page.
PLATFORM DESIGN
The portal to Quadrant of Euphoria (http://mmnet.iis.sinica.edu.tw/link/qoe/)
is of a role-based layout that serves researchers and participants of their experiments alike
(Figure 1a). From the upper pane researchers are directed to the administrative pages, where
they can register or update their experiment “profiles,” and download results, or logs, for further
9Fig. 2: Flowchart of a researcher conducting an experiment.
fifth second and onwards, giving the participant the illusion that an audio clip is played with
adjustable quality levels. The decision flow that a subject goes through is illustrated in Figure 4.
11
0% 4% 8%
 G722.1
Loss rate
Qo
E 
sc
or
e
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Laboratory
MTurk
Community
0% 4% 8%
 G728
Loss rate
Qo
E 
sc
or
e
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Laboratory
MTurk
Community
Fig. 5: QoE scores of VoIP recordings encoded by two codecs at various packet loss rates.
the POSIX timestamp of the result down to milliseconds, and vcode the verification code unique
to the result. Each line within the log represents a judgment between a comparable pair with
the format
stimulus_A stimulus_B (A|B) time,
where A|B represents a judgment of preferred QoE between stimuli A and B and time denotes
the time spent on making the judgment in seconds.
CASE STUDIES AND EVALUATION
We demonstrate Quadrant of Euphoria with two network-related case studies which, along with
others detailed in [8], were carried out on the platform in three ways: in a physical laboratory,
crowdsourced from MTurk, and crowdsourced from another populous Internet community. Such
an arrangement provides us the stepping stone to evaluate the framework and explore the exciting
possibilities of it.
Effect of Packet Loss on VoIP Quality
A three-minute long uncompressed recording of speech acquired from the Open Speech
Repository was encoded into two voice packet streams by audio codecs G.722.1 and G.728
respectively. To simulate loss events, the streams were put into a Gilbert-Elliott channel [9],
whose two states dictate whether a packet passing through is stripped off or not. The probability
of the channel going from allowing to blocking state is p, and q vice versa. Juggling with the
formulated loss rate, p
p+q
, we were able to drop at will 0%, 4%, or 8% of incoming packets and
decode the streams back into a total of six degraded recordings ready to be paired.
13
TABLE I: Cost and performance summary of laboratory and crowdsourcing strategies. Despite
lower Qualified Rates, with consistency assurance crowdsourcing can still yield results as fine
as laboratory experiments (average TSR ≥ 0.96).
Case Study
Experiment
Strategy
Total
Cost ($)
#
Runs
#
Subjects
Qualified
Rate
Cost/Run
(¢)
Time/Run
(sec)
Avg.
TSR
MP3
Bitrate
Laboratory 50.97 1, 440 10 67% 3.54 16 0.96
MTurk 7.50 750 24 47% 1.00 9 0.96
Community 1.03 1, 470 93 54% 0.07 25 0.96
VoIP
Quality
Laboratory 22.95 675 10 67% 3.40 16 0.98
MTurk 3.00 300 15 74% 1.00 19 0.98
Community 0.40 570 37 86% 0.07 24 0.98
Video
Codec
Laboratory 23.73 1, 500 10 80% 1.58 7 0.98
MTurk 4.95 495 23 65% 1.00 17 0.98
Community 0.95 1, 350 88 71% 0.07 11 0.97
Loss
Concealment
Laboratory 51.93 1, 260 11 69% 4.12 19 0.96
MTurk 5.85 585 21 36% 1.00 25 0.97
Community 0.63 900 59 35% 0.07 21 0.96
Overall 173.88 11, 295 298 59% 1.54 17 0.97
The subjects from different participant sources again show remarkably similar assessments of
the impact of loss concealment schemes on video clips. We remark that laboratory subjects seem
to have more consistent scores (represented by narrower error bars) than the crowd subjects. We
ascribe the fact to the effect of practice: MTurk and community participants on average attend
to 28 and 15 runs respectively, whereas in the laboratory subjects complete more than a hundred
runs, enabling them to discern the subtle difference of the video clips more easily. We will revisit
this argument shortly.
Crowdsourcing Evaluated
In addition to the studies above, we also asked the participants in all three settings (laboratory,
MTurk, and community) to rate MP3 compression levels and various video codecs at different
15
While we argue that crowdsourcing is a viable substitute to laboratory experiments, there are
a few concerns which we cannot ignore without compromising the completeness of this article.
Environment control. Participants in a crowdsourcing setting experience the stimuli under
a wide variety of conditions. Whether or not this is disadvantageous is subject to discussion.
On the one hand, if the experiment is to assess QoE in a specific scenario, then crowdsourcing
might not be the choice. On the other hand, crowdsourcing experiments are carried out in the
real world, where there are subpar equipment, inevitable ambient interference, and no “typical”
user environment.
Type of device. Since in the most case people connect to the Internet with their personal
computers, the crowdsourcing strategy is most suitable for assessing QoE on such platform.
When it comes to televisions, mobile phones, or electronic paper, the feasibility of crowdsourcing
admittedly depends on these alternative devices’ networking and computing capability, which
fortunately is foreseeable in the very near future.
Type of media. For the time being, Quadrant of Euphoria supports only assessments of static
image, audio, and video. While work needs to be done on interactive and streamed multimedia,
a provisional solution is to let experiment participants download “perfect” stimuli and simulate
network-related impairments such as delay or packet loss on the client side given that the
partipants’ machines are powerful enough.
Demography. As we do not physically recognize any one of the crowd, it is difficult to relate
experiment results to gender, age, race, nationality, etc. with confidence, as a certain degree of
virtuality is inherent to the Internet.
PLATFORM OUTLOOK
Quadrant of Euphoria, as its name suggests, strives to bring about an ease of mind to network
and multimedia researchers who wish to be relieved from the annoyances of subjective QoE
experiments. The foundations and evaluation of the platform we do not iterate here, but stress
that it achieves participant diversity at a lower cost, along with enhancements in the theoretical
accuracy and correctness of QoE score inference. In the future, we plan to keep on developing
the platform toward the following directions:
• Streaming support for acoustic and visual QoE assessments;
• QoE assessment for interactive applications, such as video-conferencing and online game;
17
[8] K.-T. Chen, C.-C. Wu, Y.-C. Chang, and C.-L. Lei, “A crowdsourceable QoE evaluation
framework for multimedia content,” in Proceedings of ACM Multimedia 2009, 2009.
[9] E. O. Elliott, “Estimates of error rates for codes on burst-noise channels,” Bell System
Technical Journal, vol. 42, pp. 1977–1997, 1963.
[10] “H.264/AVC reference software JM 15.1,” http://iphome.hhi.de/suehring/tml/.
2between the parties, and (3) how lossy the path the voice data traverses. Skype must be doing something
right while sending the voice bits over the Internet.
This study is motivated by a simple question: How does Skype adapt its voice data to heterogeneous
connection quality and still keep users happy? Let’s flash back our experience designing multimedia
systems over the years. The general tricks are two-fold: (1) in the presence of persistent data loss, i.e.,
the bandwidth along the path the data traverses is limited, a multimedia system adapts the sampling and
compression rate at the codec level to reduce the amount of bits for delivery, while ensuring that the content
is still perceptible; (2) in the presence of sporadic data loss, i.e., links along the path the data traverses
over are lossy or the background traffic contains short-term bursts that occupy a router’s buffer space
temporarily, a multimedia system adapts the amount of redundant data, as well as the scheme for encoding
such data, at the error concealment level. Hereafter, we refer to the redundancy-based error concealment
function of the system as the Forward Error Correction (FEC) mechanism. There are two components in
a general FEC mechanism: a redundancy control algorithm and a redundancy coding scheme. The control
algorithm decides how much redundant FEC data should be added to the voice stream; and the coding
scheme determines how redundant FEC data should be multiplexed and embedded.
For its PC-to-PC voice calls, Skype employs a voice codec that generates variable bit rate (VBR) voice
streams depending on the available network bandwidth [2]. However, this is not the case for the PC-to-
PSTN service, which actually creates revenue for Skype. The PC-to-PSTN service, also called “SkypeOut”,
enables users to make calls to traditional PSTN telephones, including fixed landline and mobile telephones.
In some countries, Skype also allows users to have a local phone number to receive calls from traditional
telephone users. The service is known as “SkypeIn”. In 2008, Skype launched a monthly subscription
program, which attracts more users to switch to Internet telephony completely. The program boosted
Skype’s quarterly revenue to US$170 million in Q2 2009. For much of its commercial success, i.e., the
SkypeIn and SkypeOut calls, Skype falls back to the ITU-defined fixed rate codec (CBR), G.729. The
immediate implication is that Skype’s FEC mechanism is in fact the major design that is holding up
the revenue. On March 31 2009, Skype released an official version on iPhone and iPod Touch. Within
ten days, there are two million downloads from the iTunes App Store. The codec used in the handheld
version is also G.729. As Skype expands services for the wireless and mobile market, the role of the FEC
mechanism becomes increasingly important.
Focusing on Skype’s FEC mechanism, we investigate the relationship between the redundancy ratio and
4available users and resources through its associated super node. The only centralized component in Skype
is the login server, which authenticates users with public key mechanisms. After authentication, all further
signaling is performed in the peer-to-peer network.
B. Skype’s Data Plane
The major reason for Skype’s popularity is its voice quality, which is the result of Skype’s codec
selection and application-level FEC mechanism. A list of the codecs used by Skype and their characteristics
is provided in [3]. Like its proprietary protocol, how Skype selects a particular codec for a voice session is
not publicly known. Even so, through observations, we have found that G.729 is always used in SkypeOut
as well as in voice sessions involving Skype clients on handheld devices, such as iPhone. For voice calls
between PCs, different versions of Skype use different codecs. The default audio codec for version 3.11
is iSAC, a product of Global IP Solutions; however, since version 3.2, Skype has adopted SVOPC [11],
an in-house developed codec, as the default codec. Note that both iSAC and SVOPC are variable bit rate
(VBR) codecs. In early 2009, Skype announced another in-house developed codec called SILK for its
latest version (version 4.0). Skype claims that SILK achieves the same quality as SVOPC, while requiring
only half of the bit rate. Despite this claim, a recent poll of Skype users showed that 46% of the respondent
would rather revert to an earlier version2.
C. Recent Studies
Bonfiglio et al. [2] analyzed how Skype adapts its traffic to different network conditions. They found that
when the available bandwidth decreases, the bit rate and payload size of Skype traffic are also decreased.
On the other hand, when packet loss is detected, Skype mitigates its impact by sending voice packets with
redundancy. The authors also proposed a source traffic model of Skype. Based on the model, Skype’s
traffic is decided by three parameters: 1) the bit rate used by the codec; 2) △T , the framing time of human
speech; and 3) the redundancy factor, which is the number of previous blocks that Skype retransmits with
the current encoded block. Following Bonfiglio et al., in [6], we conducted experiments with the iSAC
codec and found that the first two parameters, i.e., the encoding bit rate and the speech framing time, are
controlled by the codec, while the amount of redundancy is controlled by the Skype program.
1Note that the version number is based on the Windows R© version of Skype.
2http://skypenumerology.blogspot.com/2009/03/satisfaction-of-new-skype-40.html
6frames should be the same. However, from the figure, we can observe that the payload size changes as
the network loss rate increases. When the loss rate is 0%, i.e., between 0 and 180 seconds, the payload
size remains around 30 bytes; however, as the loss rate increases, we find there are more packets with
a payload of about 60 bytes. Furthermore, when the loss rate reaches 10%, i.e., between 1800 and 1980
seconds, the majority of packets have a payload of approximately 60 bytes. Interestingly, when the loss
rate returns to 0% after 1980 seconds, the payload size drops back to around 30 bytes. Meanwhile, the
bit rate of this voice session almost doubles when the payload of the majority of the packets is 60 bytes,
as shown in Fig. 1(d). Thus, the increment in the payload size is not the result of Skype adjusting the
inter-packet gap and including a higher number of voice frames in each packet. Instead, it indicates that
Skype changes the proportion of packets with redundant FEC data based on the network loss rate.
2) iSAC: The iSAC trace exhibits similar behavior. However, the iSAC codec’s bit rate is variable,
so the size of its voice frames fluctuates within a range. When the loss rate is 0%, the payload size is
within the range of (0, 160) bytes, as shown in Fig. 1(b). As the loss rate increases, we find more packets
with a payload in the range of (160, 320) bytes; and when the loss rate reaches 10%, the majority of the
packets have a double-size payload. Moreover, the bit rate almost doubles when the traffic encounters a
significant amount of network loss, as shown in Fig. 1(e).
3) SVOPC: The effect of the network loss rate on the payload size and the bit rate of the SVOPC
trace are shown in Fig. 1(c) and 1(f) respectively. Again, we find that the trace pattern is similar to those
of G.729 and iSAC. Like iSAC, SVOPC has a variable bit rate; thus, the size of its voice frame also
fluctuates within a range. When there is no network loss, the payload size is within the range of (90, 140)
bytes; however, as the network loss rate increases, the number of packets with a double-size payload also
increases along with the bit rate.
In summary, to mitigate the impact of packet loss on PC-to-PSTN (G.729) and PC-to-PC (iSAC and
SVOPC) calls, Skype seems to piggyback redundant FEC data to a number of packets based on the
observed network loss rate.
B. Understanding Skype’s FEC Mechanism
To understand Skype’s FEC mechanism, we attempt to quantify the amount of redundant FEC data
added to Skype voice traffic. We define the redundancy ratio as the percentage of packets that carry
redundant voice data, i.e., packets that have a double-size payload. If all packets carry redundant data, the
8is higher than 4%. In other words, more than 90% of packets carry redundant FEC data when there is a
large amount of packet loss. Furthermore, in the figure, the 95% confidence intervals of the three curves
largely overlap. This suggests that Skype applies the same FEC mechanism for different codecs, even
though it leads to different levels of user perception, as we will show in the next section.
3) Skype’s FEC Mechanism under Bursty Loss: As the network loss over the Internet is often bursty,
it would be interesting to learn how Skype’s FEC mechanism works under different levels of network
loss burstiness. To quantify the burstiness of network loss, we adopt the burst ratio metric, defined in
ITU-T G.107 [7] as the ratio of the average length of consecutive losses under bursty loss to the one under
random loss. By definition, the burst ratio is equal to 1 when packet loss is purely random; and it is greater
than 1 when packet loss is bursty. Specifically, a burst ratio equal to 2 indicates that the average length of
consecutive losses is twice as long as that of purely random losses. Again, we conduct the experiments
by increasing the network loss rate from 0% to 10% in 1% increments every 180 seconds. However, this
time, the packet loss is bursty rather than uniformly distributed. We implemented the Gilbert model [4]
to emulate the burst ratio in dummynet. Details of the implementation can be found in [6].
Fig. 2(b) shows the observed redundancy ratio of SVOPC when its traffic experiences packet loss with
different burst ratios. In the figure, each curve corresponds to a burst ratio setting, and the curves’ 95%
confidence intervals overlap with each other. We have the same observation on the redundancy ratios of
iSAC and G.729 under different levels of burstiness.
In summary, our experiment results suggest that Skype only adjusts redundancy ratios based on the
network loss rate. In other words, Skype does not consider the difference of each codec or network loss
burstiness in its FEC mechanism.
IV. QOE ANALYSIS OF FEC MECHANISMS
Having shown that Skype adjusts its redundancy ratio based on the observed network loss rate only,
we now consider the question: Does the applied redundancy ratio provide good quality of experience? To
address this question, we evaluate the performance of FEC mechanisms in terms of quality of experience
(QoE) in order to derive an optimal FEC design for general VoIP services.
In the following subsections, we first present our methodology for deriving the optimal redundancy ratio
given a desired voice quality. Then, we systematically investigate how the ratio might vary depending on
(1) the audio codec, (2) the loss pattern, and (3) the FEC coding scheme.
10
 
4.3 
 
4.1
 
 
3.9 
 
3.7 
 
3.5
 
0.00 0.02 0.04 0.06 0.08 0.10
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Burst Ratio = 1
Burst Ratio = 1.5
Burst Ratio = 2
 
4.
3 
 
4.
1 
 
3.
9 
 
3.
7 
 
3.
5 
 
4.
3 
 
4.
1 
 
3.
9 
 
3.
7 
 
3.
5 
(a) G.711
 
3.7 
 
3.5 
 
3.3 
 
3.1
 
0.00 0.02 0.04 0.06 0.08 0.10
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Burst Ratio = 1
Burst Ratio = 1.5
Burst Ratio = 2 3.7 
 3.5 
 
3.
3 
 
3.
1 
 
3.
7 
 
3.
5 
 
3.
3 
 
3.
1 
(b) G.729
Fig. 4. The contour plots of audio quality scores under three burst ratio settings.
and 4(b) respectively. On each graph, the contour curve labeled with a number, say 3.5, represents the
combinations of loss rates and redundancy ratios that yield the same MOS score, 3.5. Thus, each contour
curve represents the optimal redundancy ratios for the associated MOS value. We observe that, for a certain
loss rate, higher redundancy ratios yield higher MOS scores. On the other hand, for a fixed redundancy
ratio, higher loss rates lead to lower MOS scores. Comparing the contour plots of the two codecs, we
find that the optimal redundancy ratios required to maintain a certain MOS score for each codec are
very different. For example, assuming the network loss rate is 4% and the desired MOS score is 3.5, the
redundancy ratio should be set at 0.2 for G.711. In contrast, we can achieve the same audio quality by
setting the redundancy ratio at 0.8 if G.729 is used.
C. Optimal Redundancy Ratio for Different Loss Patterns
We repeat the emulations, except that we now infer the optimal redundancy ratios for different burst
ratios. In Fig. 4(a) and Fig. 4(b), the optimal redundancy ratios for three burst ratio settings, 1.0, 1.5 and
2.0, are plotted with black solid lines, red dashed lines, and green dotted lines respectively. The results
show that the redundancy ratio should be increased more aggressively if we wish to maintain the same
audio quality under higher burst ratios. For example, to maintain a consistent level of user perception at
MOS 3.5 with G.711 under network loss rate of 4%, the redundancy ratio should be set to 0.2 when the
burst ratio is 1; however, it should be set to 0.45 and 0.75 when the burst ratios are 1.5 and 2, respectively.
D. Optimal Redundancy Ratio for FEC Coding Schemes
Since the payloads of Skype packets are encrypted, we cannot determine which FEC coding scheme
Skype employs. Nevertheless, based on the observation that the payload size doubles when FEC data is
12
2.6
2.8
3.0
3.2
3.4
3.6
 
3.
7 
 
3.
6 
 
3.
5 
 
3.
3 
 
3.
2 
 
3.
1 
 
2.
9 
 
3.4
 
0.0
0.2
0.4
0.6
0.8
1.0
0.00 0.02 0.04 0.06 0.08 0.10
Loss Rate
R
ed
un
da
nc
y 
Ra
tio
(a) Burst Ratio = 1
95
10
0
10
5
11
0
11
5
Target MOS = 3.4
Loss Rate
Ba
nd
wi
dt
h 
Ut
iliz
at
io
n 
/ Q
ua
lity
 S
co
re
0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1
95
10
0
10
5
11
0
11
5 Quality Score
Bandwidth Utilization
(b) Burst Ratio = 1
Fig. 6. (a) Comparison of Skype’s FEC mechanism vs. the optimal redundancy ratios; (b) quantification of how Skype’s FEC mechanism
deviates from optimal
must be set at 0.6, compared to 0.8 for RS-(2, 1).
Moreover, RS-(3, 2) is also more resilient to bursty loss. For example, if both packet A and B are
lost, A is not recoverable under RS-(2, 1) code. However, under RS-(3, 2), if packets C and D are both
received successfully, packet B can be recovered. Once B is recovered, packet A could also be restored
from B ⊕ (A⊕B). When the burst ratio is 1.5, to maintain a consistent level of user perception at MOS
3.3 under a network loss rate of 4%, the redundancy ratio should be set at 0.6 for RS-(3, 2), as shown in
Fig. 5(b); however, it would need to be at 0.85 for RS-(2, 1).
Under RS-(3, 2) coding, it might be necessary to wait for three subsequent packets to arrive before
the receiver can recover a lost packet. With iSAC and SVOPC, whose framing times could be as high
as 60 ms, waiting for three more packets is equivalent to add an additional 180 ms of FEC delay in the
worst case. This would not be acceptable for real-time interactive voice communication. The FEC delay
for other Reed Solomon codes, such as RS-(4, 3), may be worse, but they are more resilient to network
loss. The choice of FEC coding scheme is a trade-off between the responsiveness and loss resilience.
E. Skype vs. Optimal
Having determined Skype’s FEC mechanism in Section III and derived the optimal algorithm above,
we can now assess whether Skype’s redundancy ratio settings are optimal. In Fig. 6, we overlap the
observed Skype’s redundancy ratios and the optimal redundancy ratios for G.729; each graph corresponds
to a certain burst ratio. By comparing the contour curves and Skype’s redundancy ratio curve, we find
that Skype fails to maintain consistent voice quality.
14
National Science Council of Taiwan under the grant NSC98-2221-E-002-072-MY3, NSC96-2628-E-001-
027-MY3, and Mr. and Mrs. Chun Chiu Stanford Graduate Fellowship.
REFERENCES
[1] S. Baset and H. Schulzrinne, “An Analysis of the Skype Peer-to-Peer Internet Telephony Protocol,”
in Proceedings of IEEE INFOCOM’06, Barcelona, Spain, Apr. 2006.
[2] D. Bonfiglio, M. Mellia, M. Meo, N. Ritacca, and D. Rossi, “Tracking Down Skype Traffic,” in
Proceedings of IEEE INFOCOM’08, Phoenix, AZ, Apr. 2008.
[3] D. Bonfiglio, M. Mellia, M. Meo, and D. Rossi, “Detailed Analysis of Skype Traffic,” IEEE
Transactions on Multimedia, vol. 11, no. 1, pp. 117–126, January 2009.
[4] E. Gilbert, “Capacity of a Burst-Noise Channel,” The Bell System Technical Journal, vol. 39, pp.
1253–1265, SEP 1960.
[5] S. Guha, N. Daswani, and R. Jain, “An Experimental Study of the Skype Peer-to-Peer VoIP System,”
in Proceedings of The 5th International Workshop on Peer-to-Peer Systems (IPTPS’06), Santa
Barbara, CA, Feb. 2006, pp. 1–6.
[6] T.-Y. Huang, K.-T. Chen, and P. Huang, “Tuning Skype’s Redundancy Control Algorithm for User
Satisfaction,” in Proceedings of IEEE INFOCOM’09, Rio de Janeiro, Brazil, Apr. 2009.
[7] ITU-T Recommendation G.107, “The E-model, a computational model for use in transmission
planning,” Mar. 2005.
[8] ITU-T Recommendation P.800, “Methods for subjective determination of transmission quality,” 1996.
[9] ITU-T Recommendation P.862, “Perceptual evaluation of speech quality (PESQ), an objective method
for end-to-end speech quality assessment of narrow-band telephone networks and speech codecs,”
Feb. 2001.
[10] W. Jiang and H. Schulzrinne, “Comparison and optimization of packet loss repair methods on VoIP
perceived quality under bursty loss,” in Proceedings of NOSSDAV’02, Miami Beach, FL, May 2002.
[11] J. Lindblom, “A Sinusoidal Voice Over Packet Coder Tailored for the Frame-Erasure Channel,” IEEE
Trans. Speech Audio Processing, 2004.
[12] L. Rizzo, “Dummynet and Forward Error Correction,” in In Proc. of the 1998 USENIX Annual
Technical Conf. USENIX Association, 1998.
[13] Skype P2P Telephony Explained. [Online]. Available:
http://www.skype.com/help/guides/p2pexplained/
experiments.  The experimental network consists of: (1) a 
source node operating a particular rate adaptation 
mechanism, (2) a destination node receiving data and assist 
in the measurement of receiving rate, receiving rate jitter, 
packet loss rate and round-trip delay, and (3) a 
DummyNet-controlled link between the source and 
destination nodes to emulate different levels of congestion.  
The controlled link is configured to run in the bandwidth of 
80kbps, 65kbps, 50kbps, 35kbps, 20kbps, 35kbps, 50kbps, 
65kbps, and 80kbps for 300 seconds each.  Each 
experiment, therefore, takes 2700 seconds in total. The 
four mechanisms compared are CONST, LOSS, USI-I, and 
USI-II. In the CONST experiment, we use a straight-
forward constant bit rate source. LOSS is our 
implementation of the exact algorithm with exact 
parameter setting proposed in [2].  USI-I and USI-II are 
our proof-of-concept mechanisms.  USI-I and USI-II differ 
in the parameter settings.  They are configured with H=5, 
I=4, and D=3, and H=5, I=3, and D=3 respectively.    
Figure 1 plots the resulting USI for the four mechanisms, 
Figure 2 the packet loss rate, and Figure 3 the round-trip 
delay respectively.   
 Potential of the USI-Based Mechanisms.  Comparing 
CONST, LOSS, and USI-I, we can see that when the 
available bandwidth is abundant, there is no distinct 
difference in the three mechanisms’ performance.   When 
the available bandwidth is mildly sufficient or insufficient, 
we observe that USI-I outperforms the other two in all 
three metrics demonstrating the potential of USI-based 
mechanism for better user satisfaction.  LOSS is not far 
worse and performs significantly better than CONST in the 
mildly sufficient bandwidth case.  In the extreme case, 
however, the sending rate of LOSS, the state of the art 
mechanism, fluctuates and results in a lower level of user 
satisfaction.  
 Sensitivity of the USI-Based Mechanisms to 
Parameter Settings. Although USI-I is shown promising 
providing a higher level of user satisfaction, from the 
results of USI-I and USI-II, we also observe that the 
performance of the USI-based mechanisms are sensitive to 
the parameter settings. A slightly smaller I value in USI-II 
causes the source to increase the sending rate more 
aggressively and, as a result, leads to undesirable sending 
rate fluctuations.  In fact, not just the time scale to adapt 
the sending rate, the granularity of the steps and the rate 
adaptation criteria may also impact the stability of the rate 
adaptation mechanism and ultimately the user experience. 
Systematic exploration of the design space is necessary for 
a robust rate adaptation mechanism.  This is the issue we 
intend to pursue in the future. 
IV. Summary and Outlook 
Voice and best-effort data are different in nature. Sending 
voice data in a TCP-friendly fashion might not lead to 
perceptible quality when voice and best-effort traffic 
content for network bandwidth. We think that adapting the 
VoIP traffic sending rate based on user satisfaction is a 
promising alternative to explore. Taking Skype, one of the 
most popular VoIP services on the Internet, as an example, 
we show with the preliminary results that a user-centric 
congestion control mechanism may result in a higher level 
of user satisfaction and Skype, although runs its own rate 
adaptation mechanism already [6], may potentially be 
improved by a robust rate adaptation mechanism. A more 
thorough investigation on whether existing TCP-friendly 
congestion control mechanisms will result in satisfying 
VoIP service, whether the existing rate adaptation 
mechanism used in Skype is adequate, and whether a 
robust mechanism for consistent user experience can be 
devised is the subject of our future study. 
 
 
 
 
 
 
 
 
 
Figure 1 USI result of the Four mechanisms 
 
 
 
 
 
 
 
 
 
Figure 2   Packet loss rate of the Four mechanisms 
 
 
 
 
 
 
 
 
 
 
Figure 3 RTT of the Four mechanisms 
References 
[1]  S. Floyd, M. Handley, J. Padhye, and J. Widmer. Equation-Based 
Congestion Control for Unicast Applications. In Proceedings of ACM 
SIGCOMM 2000, Stockholm, August 2000. 
[2]  A. Barberis, C. Casetti, J.C. De Martin and M. Meo. A Simulation 
study of adaptive voice communications on IP networks. Computer 
Communications 24 (2001), pages 757-767, 2001. 
[3]  J. Matta, C. Pépin, K. Lashkari and R. Jain. A source and channel rate 
adaptation algorithm for AMR in VoIP using the Emodel. In 
Proceedings of ACM NOSSDAV’03, Montery, California, USA, June, 
2003. 
[4]  Z. Qiao, L. Sun, N. Heilemann and E. Ifeachor. A new method for 
VoIP Quality of Service control use combined adaptive sender rate 
and priority marking. In Proceedings of IEEE International 
Conference on Communication, Paris, France, June, 2004. 
[5]  K.T. Chen, C.Y. Huang, P. Huang and C.L. Lei. Quantifying Skype 
User Satisfaction. In Proceedings of ACM AIGCOMM’06, Pisa, Italy, 
September, 2006. 
[6] L. D. Cicco, S. Mascolo, V. Palmisano. An Experimental 
Investigation of the Congestion Control Used by Skype VoIP. In 
Proceedings of the International Conference on Wired/Wireless 
Internet Communication (WWIC’07), Coimbra, Portugal, May, 2007. 
0
10
20
30
40
50
60
70
80
90
100
30 530 1030 1530 2030 2530time(sec)
PLR
(%)
CONST
USI-I
0
10
20
30
40
50
60
70
80
90
100
30 530 1030 1530 2030 2530time(sec)
PLR
(%)
USI-II
LOSS
4.5
5
5.5
6
6.5
7
7.5
8
30 530 1030 1530 2030 2530time(sec)
USI
CONST
USI-I
LOSS
USI-II
 
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
30 530 1030 1530 2030 2530time(sec)
RTT
(sec)
CONST
USI-I
LOSS
USI-II
Skype, has 246 million registrars and 100-million online users1. Because of the
steady growth of VoIP usage, providing reliable service and satisfactory voice
quality is now a high-priority for Internet and VoIP service providers.
In order to provide a dependable means of voice transmission over the Inter-
net, it is essential that network gateways have the ability to differentiate VoIP
flows from flows generated by other applications. By identifying VoIP flows, net-
work gateways can provide QoS features, such as allocating more bandwidth or
assigning a high priority to identified voice flows. Traffic management is another
important application of flow classification. Enterprises often have to manage
VoIP traffic in line with institutional policies, such as restricting calls to certain
destinations, or blocking calls at certain times. Given these emerging needs, de-
veloping an efficient and accurate identification algorithm for VoIP flows is now
of paramount importance.
To accurately identify VoIP flows in real time, we may face the following
challenges:
1. Non-standard protocols and ports. Different VoIP applications may
use different signaling protocols, such as SIP [1], H.323 [2], and several other
proprietary P2P protocols [3, 4]. Additionally, many VoIP applications are
peer-to-peer based and may use random port numbers rather than a fixed
port number. Thus, it is difficult to detect VoIP flows by analyzing their
signaling protocols.
2. Non-standard codecs. Modern VoIP applications may use different au-
dio codecs to adapt to different network environments, e.g., using a wide-
band codec when a broadband medium is used and a narrowband codec if a
wireless connection is detected. Given the numerous audio codecs available,
detecting VoIP traffic by the signatures of audio codecs is not practical.
3. Payload encryption. VoIP applications now tend to encrypt their packet
payloads to protect privacy. Even if a VoIP application does not encrypt
its packets, the packet payload may still be inaccessible because trying to
obtain such information would be a violation of privacy. Thus, payload-based
identification is ineffective.
4. Silence suppression. A large number of traffic classification methods rely
on traffic patterns, e.g., the mean and variation of packet interarrival times.
However, the pattern of VoIP traffic can vary a great deal over time because
many audio encoders support silence suppression. In other words, they sup-
press a packet stream when speech is absent and resume data delivery when
speech is detected. Therefore, schemes based on traffic patterns are not very
effective for identifying silence-suppression-enabled VoIP flows.
In this paper, we propose a VoIP flow identification scheme based on human
conversation patterns. We consider that the conversation pattern of VoIP traffic
is unique compared to that of other applications. For example, a file transfer
session comprises a group of unidirectional traffic flows that are very different
1 http://seekingalpha.com/article/50328-ebay-watch-59-earnings-growth-skype-
reaches-10-million-concurrent-users
Table 1. Trace Summary
Category # Connections Duration # Packets Packet Rate (1/sec) Bytes
Skype 462 2, 388 (min) 4, 728, 240 33 4, 318 (MB)
TELNET 2, 008 4, 729 (min) 10, 559, 261 37 7, 331 (MB)
WoW 1, 406 1, 537 (min) 2, 528, 359 27 680 (MB)
P2P 15, 845 3, 334 (min) 29, 220, 870 146 30, 500 (MB)
HTTP 2, 224 120 (min) 28, 264, 360 3, 925 59, 097 (MB)
to encrypted traffic. Furthermore, examining payloads raises personal privacy
concerns.
Flow statistics have also been used to classify network traffic. In [5], Moore
et al. use a supervised machine learning technique, the naive Bayesian classifier,
to categorize traffic by application types. In [6, 7], an unsupervised clustering
algorithm is proposed for traffic identification. These works focus on oﬄine traffic
classification for the purposes of traffic trend analysis and network planning.
They do not consider online traffic identification, which is essential for real-time
traffic management.
Another approach used to identify network traffic is based on the specific
signature in packet exchanges between hosts. In [8], Dahmouni et al. modeled
the sequence signature of TCP control packets with a first-order Markov chain.
They first inferred the transition probabilities of a Markov model for each known
application in the learning step, and then identified traffic based on the de-
rived transition probabilities. Our scheme is similar to that in [8] as we also
adopt Markov modeling; however, instead of relying on TCP control packets,
our method is based on the conversation patterns between interacting hosts.
3 Data Description
We evaluated the proposed VoIP detection scheme on real-life Internet traffic
traces obtained from five types of network applications: VoIP, TELNET, HTTP,
P2P, and online games. Skype, a popular VoIP software, was chosen to repre-
sent VoIP applications. The collection procedures for our traces were as follows.
1) Skype traffic was captured according to the procedures detailed in [9]. 2)
TELNET traffic was captured on a gateway router for all TCP flows with port
numbers 22 (SSH) and 23 (telnet); all intra-campus traffic was removed. 3) We
chose World of Warcraft, a popular MMORPG (Massively Multiplayer Online
Role-Playing Game), to represent online games. The traffic was captured on a
gateway router for all TCP flows with port number 3274; either the source or
destination address is within the network 203.66 (where the World of Warcraft
server is located in Taiwan). 4) P2P traffic was captured on a dedicated PC
running BitComet, a variant of BitTorrent client [10]. As the BitTorrent proto-
col does not use a fixed port number, we recorded all the flows that used port
numbers higher than 1024.
Algorithm 1 Speech activity inference from VoIP traffic
for each flow do
observe the size of packets for 1 second
if an idle period in either direction then
infer speech activity based on packet rate
else
infer speech activity based on packet size
end if
end for
4.3 Algorithm
We now present an integrated algorithm that can detect voice activity in
VoIP flows in general, as shown in Algorithm 1. First, we observe the packet
rate in either direction for a specific period, e.g., 1 second, to determine whether
the flow is silence-suppressed. Because double-talk (i.e., both call parties talk at
the same time) is normally short and infrequent, a period of continuous packets
implies that the observed flow is not silence-suppressed. In this case, we infer
voice activity in the VoIP flow based on the packet size. Otherwise, we assume
the flow is generated by an application that employs silence suppression, and
infer the conversation pattern embedded in the flow based on the packet rate.
5 Motivation for the Proposed Scheme
In this section, we explain the intuition behind our approach. We consider that
each type of network application possesses a unique conversation pattern. In ad-
dition, since human interaction is different to the interaction between computer
applications, VoIP traffic can be identified based on the embedded human con-
versation activity. We also provide a graphical illustration of the conversation
patterns of a number of applications to support our argument.
5.1 Application Behavior Analysis
To understand why human conversation patterns differ from the traffic patterns
of other applications, in the following, we present a behavioral analysis of com-
mon applications.
– File transfer applications, such as FTP, are comprised of unidirectional flows.
That is, downloading a file corresponds to a network flow containing only
server-to-client packets, except for a few control messages from the client.
On the other hand, if a client uploads a file, the traffic will contain only
client-to-server packets. Moreover, a file transfer session often continues for
a long period and achieves a stable bit rate in the long-term.
– In web browsing, when a user clicks on a URL, the browser sends a simple
HTTP request message to the web server. The response message from the
BM
talk burst
talk burst
A
A
B
States AM BD
Fig. 1. Voice activity between two speakers and the conversation pattern
D
M
BA
Fig. 2. Conversation model
5.2 Conversation Modeling
In some studies of human speech characteristics, conversation patterns have been
modeled using Markov chains [13, 14]. A 4-state model for generating artificial
conversational speech is proposed in [15]. For two speakers, A and B, engaged
in a conversation, the four states are as follows: state A represents that A is
talking and B is silent; state B represents that A is silent and B is talking; state
D indicates double-talking; and state M denotes mutual silence. We illustrate
the definitions of the four states in Fig. 1, and the transitions between the four
states in Fig. 2. Because of its simplicity, we employ this 4-state Markov chain
for human conversation modeling in this study.
5.3 Conversation Patterns: A Graphical Comparison
To verify our analysis of application behavior, we randomly select 10 flows from
each of our traces and plot the conversation pattern of each flow, as shown in
Fig. 3.
On the graph, each flow is divided into a number of periods and the con-
versation state in each period is determined by the traffic direction. The flows
of HTTP, P2P and TELNET are assigned the state M most of time because
their inter-packet times are normally large. On the other hand, VoIP flows consist
or waiting for the other to speak. To sum up, the VoIP traffic pattern accurately
reflects human conversation patterns, i.e., high interactivity, bi-directionality,
and the interdependency between both directions.
Through graphical comparisons, we show that the VoIP traffic pattern is
very different from that of other applications. In addition, the 4-state model is
effective for representing the differences between the applications’ conversation
patterns.
6 Methodology
In this section, we propose a VoIP flow identification scheme based on the unique
human speech conversation patterns embedded in voice traffic. Our scheme com-
prises two phases: a training phase and an identification phase. In the training
phase, we learn the parameters of a Markov model and train the classifier based
on a set of existing traces. Then, in the identification phase, we use a supervised
classification approach to detect VoIP flows.
6.1 Training Phase
In this phase, we use a 4-state Markov chain to model the human speech con-
versation pattern, and then apply a naive Bayesian classifier to distinguish VoIP
flows from other flows.
Markov Chain: Given a set of known VoIP flows, we compute the initial
probabilities of states Si ∈ {A,B,D,M} based on the proportion of their mean
sojourn times, and compute the transition probabilities based on the empirical
transition frequency between the states. We treat the trained Markov chain as
representative of typical speech conversation patterns. Any conversation pattern
that can be well-modeled by this approach is considered human-like and will be
considered as a VoIP flow in the identification phase.
Naive Bayesian Classifier: To determine how well the model fit indicates
that a traffic pattern is a human conversation, we employ a naive Bayesian
classifier [5], which is a supervised machine learning tool. We use the flows from
all the applications in our data set (Section 3) to train the classifier. For each
training flow, we compute the likelihood of its conversation pattern generated
by the trained Markov chain. Given a state sequence S1, S2, . . . Sn, where Si ∈
{A,B,D,M}, we compute the log-likelihood of the sequence as
log(P1,2 × P2,3 × . . .× P(n−1),n), (1)
where Pi,j is the transition probability of the state sequence SiSj . Since flows
may vary in length, we define the normalized log-likelihood value as
log(P1,2 × P2,3 × . . .× P(n−1),n)/N, (2)
where N is the length of the sequence. For VoIP flows, the computed log-
likelihood tends to be large as the Markov chain represents typical human con-
versation patterns. Because non-VoIP flows normally exhibit non-human-like
Table 4. Transition probabilities of the 1st-order markov chain
A B D M
A 0.9022 0.0028 0.0380 0.0571
B 0.0029 0.9030 0.0391 0.0550
D 0.0607 0.0592 0.8763 0.0038
M 0.0465 0.0439 0.0019 0.9078
Table 5. Transition probabilities of the 2nd-order markov chain
A B D M
AA 0.9067 0.0034 0.0380 0.0519
AB 0.0000 1.0000 0.0000 0.0000
AD 0.0000 0.0781 0.9219 0.0000
AM 0.0000 0.0635 0.0000 0.9366
BA 1.0000 0.0000 0.0000 0.0000
BB 0.0032 0.9115 0.0346 0.0507
BD 0.0775 0.0001 0.9224 0.0000
BM 0.0641 0.0000 0.0000 0.9359
DA 0.9486 0.0000 0.0000 0.0514
DB 0.0000 0.9518 0.0000 0.0482
DD 0.0633 0.0678 0.8651 0.0037
DM 0.0000 0.0000 0.0000 1.0000
MA 0.9586 0.0000 0.0414 0.0000
MB 0.0000 0.9562 0.0438 0.0000
MD 0.0000 0.0005 0.9995 0.0000
MM 0.0549 0.0526 0.0025 0.8901
memoryless process. On the other hand, a second- or higher-order Markov chain
considers the current state as well as the previous states. Finding an appropriate
order for the Markov chain is essential to obtain a good fit of human conversation
patterns and achieve high identification accuracy in our approach.
To examine the impact of the order of the Markov chain, we compare real hu-
man conversation patterns with patterns generated by different Markov models.
Based on real human conversation traces, we first derive the transition probabil-
ities, as shown in Tables 4 and 5. We then generate conversation patterns using
the first- and second-order Markov chains respectively, as shown in Fig. 4. The
real series shows the patterns of empirical conversations in our trace; the 2nd
and 1st patterns are generated by the second- and first-order Markov chains, re-
spectively. In addition, we generate a pattern with the indep. model, where the
states are generated in an independent and identically-distributed (IID) manner.
Compared to the real case, the state alternation of the indep. model is rapid
and disordered. Because of the large difference from true human behavior, the
indep. model is clearly not suitable for describing human conversations. On the
other hand, the patterns of the 1st and 2nd models are similar to those of the
0 5 10 15 20 25 30
0.
70
0.
75
0.
80
0.
85
0.
90
0.
95
1.
00
Detection time (sec.)
Ac
cu
ra
cy
Fig. 5. Influence of the detection time on accuracy
0 5 10 15 20 25 30
0.
6
0.
7
0.
8
0.
9
1.
0
(a) Detection time (sec.)
TP
R
 / 
TN
R
TPR
TNR
0 5 10 15 20 25 30
0.
6
0.
7
0.
8
0.
9
1.
0
(b) Detection time (sec.)
TP
R
 / 
TN
R
TPR
TNR: P2P
TNR: WoW
TNR: TELNET
TNR: HTTP
Fig. 6. Influence of the detection time on true positive rate (TPR) and true negative
rate (TNR)
As shown in Fig. 6(a), the true positive rate is 95% in the first seconds of the
detection time. Specifically, the rate is higher than 97% for a detection time
longer than 2 seconds. On the other hand, the true negative rate is 95% in the
first 4 seconds of the detection time and 97% within 11 seconds. Fig. 6(b) shows
the true negative rate for each non-VoIP application. We find that, among the
non-VoIP applications, the flows of WoW tend to be mis-identified as VoIP. One
possible explanation is that clients may be idle in some sessions so that the traffic
consists mainly of server-to-client packets; therefore, the traffic pattern will be
less disordered and close to that of VoIP. The true negative rate for WoW can
achieve 90% with a detection time longer than 10 seconds.
In Fig. 7, we plot ROC curves for four classifiers that are based on differ-
ent length of the detection time. The performance of a classifier is better than
another while the true positive rate is close to 1 for a small false positive rate
(i.e., TPR is higher and FPR is lower). From these curves, we observe that the
7. Erman, J., Mahanti, A., Arlitt, M., Williamson, C.: Identifying and discriminating
between web and peer-to-peer traffic in the network core. In: Proceedings of the
16th international conference on World Wide Web, Banff, Alberta, Canada (2007)
883–892
8. Dahmouni, H., Vaton, S., Rosse´, D.: A markovian signature-based approach to ip
traffic classification. In: Proceedings of the 3rd annual ACM workshop on Mining
network data, San Diego, California, USA (2007) 29–34
9. Chen, K.T., Huang, C.Y., Huang, P., Lei, C.L.: Quantifying Skype user satisfac-
tion. In: Proceedings of ACM SIGCOMM’06, Pisa, Italy (Sep 2006) 399–410
10. Qiu, D., Srikant, R.: Modeling and performance analysis of bittorrent-like peer-
to-peer networks. In: Proceedings of ACM SIGCOMM’04, Portland, OR, USA
(August 2004) 367–378
11. Hou, F., Ho, P.H., Shen, X.S.: Performance evaluation for unsolicited grant service
flows in 802.16 networks. In: Proceedings of the 2006 international conference
on Wireless communications and mobile computing, Vancouver, British Columbia,
Canada (July 2006) 991–996
12. Baset, S.A., Schulzrinne, H.G.: An analysis of the skype peer-to-peer internet
telephony protocol. In: Proceedings of the IEEE INFOCOM’06, Barcelona, Spain
(2006) 1–11
13. Brady, P.: A model for generating on-off speech patterns in two-way conversation.
The Bell System Technical Journal 48(9) (September 1969) 2445–2472
14. Stern, H.P., Wong, K.K.: A modified on-off model for conversational speech with
short silence gaps. In: Proceedings of the 25th Southeastern Symposium on System
Theory, Tuscaloosa, Alabama, USA (1993) 581–585
15. International Telecommunication Union: Artificial conversational speech. (1993)
ITU-T Recommendation P.59.
2we analyze the sampled processing delays to gain an
understanding of the workload pattern of Internet nodes,
the effect of packet size on relay processing, and the
instability of processing delays.
3) We show that the processing delays incurred at relay
nodes may significantly degrade VoIP quality based on
ITU-T E-model [12] and the delays at relay nodes are
generally unstable.
We consider this paper as a first step in focusing on the
extra processing delays introduced by peer-to-peer relaying.
From our results, we suggest that all real-time peer-to-peer
systems keep track of the workload on relay nodes to prevent
unexpected performance degradation.
The remainder of this paper is organized as follows. Sec-
tion II describes related works. We then propose our process-
ing delay inference methodology in Section III. In Section IV,
we describe the trace collection methodology and discuss the
quality of our collected traces. In Section V, we analyze a
number of characteristics of the collected relay processing
delays, such as typical workload patterns and stability. In
Section VI, we evaluate the impact of the processing delays
of relay nodes on VoIP quality. Finally, in Section VII, we
present our conclusions.
II. RELATED WORK
To the best of our knowledge, this paper is the first to
study the processing delays of peer-to-peer relay nodes and
their impact on the application performance. Thus, there are
no earlier studies directly related to our work. In [14], Liu
and Zimmermann mentioned that the average processing delay
at each overlay node of AudioPeer [29], a commercial voice
chat system, was approximately 30 ms. However, they did not
report how the measurement was conducted and how large and
representative the data set was. One closely related research is
probably that on peer-to-peer relay node selection. A number
of studies [3–5, 11, 15, 21, 27] have been devoted to selecting
relay nodes from a set of candidates to obtain a good quality
network path. However, the selection criteria are mainly based
on the network latency and loss rate, and do not consider
the processing delays introduced by relay nodes. Our work
complements these network-quality-based relay node selection
studies by emphasizing that relay processing delays should
also be considered when selecting the best relay node.
III. PROCESSING DELAY INFERENCE
In this section, we propose a methodology for measuring
the processing delays induced by relaying packets at an
intermediate node. The processing delay is defined as the
time an intermediate node takes to send a data packet to its
destination on behalf of a source host. Delays can be due to
a variety of operations, such as receiving a packet from the
source host, passing it to the relay application (mostly in user
mode) for processing, and forwarding the packet to the target
host.
The processing delay at a relay node is generally unmea-
surable unless we place a sniffer to monitor the incoming
and outgoing traffic of the node. Note that even the relay
application cannot measure the processing delays of relayed
packets directly because it has no information about when
a packet arrives at or departs from the system. To measure
processing delays exactly, an application must exploit a kernel-
mode packet filter, such as BPF [17], or raise its thread priority
to a high value so that it will always be served immediately.
However, either approach is inadequate for a large-scale de-
ployment, as they incur additional resource overhead and may
disturb the execution of the user’s own tasks.
Our methodology is designed to measure the processing
delays experienced by relayed packets at a relay node without
any modification to the existing network infrastructure and
peer-to-peer software. In the following, we first define the
terms used throughout this paper. We then explain the basic
rationale behind our inference methodology, and elucidate the
IPID filtering scheme for improving the estimation accuracy.
Finally we evaluate both the basic and improved methods
through an experiment.
A. Term Definition
In this paper, we assume a two-hop relaying scenario in
which every packet from a source host transits an intermediate
node before reaching the destination host.
• Source/destination: A pair of hosts that communicate
with each other through an intermediate host.
• Relay node (relay host): The intermediate host receives
packets from the source and forwards them to the desti-
nation. The relay node might change the content of the
relayed packet slightly depending on the application’s
context.
• Source packet: A data packet sent from the source host
to the relay node.
• Relay packet: The data packet sent from the relay node
to the destination, which is a copy (verbatim or with slight
changes) of the source packet.
• Ack packet: When a source packet is delivered to the
relay node by TCP, the relay node will acknowledge the
packet by sending a TCP acknowledgement packet back
to the source host. We call this an ack packet.
• Processing delay (PD): The time lapse from the instant
a source packet arrives at a relay node and the instant its
corresponding relay packet leaves the node.
• Data delivery time (DDT): The time lapse from the in-
stant a source packet leaves the source host to the instant
its corresponding relay packet reaches the destination.
• Ack response time (ART): The time lapse from the
instant a source packet leaves the source host to the
instant its corresponding ack packet is received by the
source host.
B. The Basic Method
Our method is based on the following assumptions, which
are generally observable across current peer-to-peer implemen-
tations:
1) The relay node forwards a relay packet to the destination
as soon as it receives a source packet, i.e., no intentional
delays are introduced in relay packet forwarding.
4must have experienced unusual network delays and should be
filtered out.”
As we do not have direct access to every relay node on
the Internet, the following rules are employed to determine
the packet ordering at the node. 1) For packets from the
source host, we detect whether they arrive at the relay node
sequentially by analyzing the IPIDs of their corresponding ack
packets. A smaller IPID in an ack packet indicates that its
corresponding source packet arrived at the relay node earlier,
and vice versa. 2) For packets from the relay node, we detect
the sequence they departed the relay node by their IPIDs.
Fig. 3 illustrates three possible packet-reordering scenarios.
Note that this is not an exhaustive listing of possible reordering
cases. For example, a relay packet for a source packet x may
exchange its order with an ack packet for a source packet y,
which forms another category of reordering. Any occurrence
of packet sequence rearrangement indicates that at least one
packet is experiencing unusual network delays. Suppose a
source packet i that departed from the source host at time
ts,i elicits an ack packet with IPID idack,i and a relay packet
with IPID idr,i, which arrive at their destinations at time tack,i
and tr,i respectively. We detect packets with unusual network
delays as follows:
1) For all source packets, we obtain a sequence {ts, idack}
sorted by ts. We then apply the longest increasing sub-
sequence (LIS) algorithm [24] to the {idack} sequence,
IDack, and obtain its longest increasing subsequence L.
The set {L} denotes the IPIDs of the packets that keep
order with each other in the network. Thus, we remove
the packets with IPIDs in the set {IDack−L}, because
they are the packets with unusual delays that lead to
inaccurate processing delay estimates.
2) We combine {idack, tack} and {idr, tr} as a sequence,
and sort them by the first element. Then, we apply the
LIS algorithm to the sequence formed by the second
element, IDack,r, and obtain the longest increasing
subsequence L. Once again, the packets with IPIDs
in the set {IPIDack,r − L} are those that experience
unusual network delays and should be removed.
After removing packets with inconsistent network delays
(i.e., compared to the remaining packets), the estimated pro-
cessing delays will be all greater than zero, as the DDT must
be longer than the ART for a source packet. Also, the error of a
processing delay estimate will be bounded by the interarrival
times of the packets surrounding pairs comprised of an ack
packet and a relay packet.
D. Accuracy Evaluation
We verify the effectiveness of the proposed processing delay
inference methods with a series of experiments. The network
setup of the experiments is identical to that described in
Section III-B1 and shown in Fig. 1. However, this experiment
described here is much more realistic because it takes account
of network delay variability and packet reordering. We use
a trace-driven approach to emulate the network dynamics.
The network delay of each packet is sequentially assigned
a measured ART from the set of Internet traces described
C
ase 3 Ack pa
cket 5
Source packet 5
Ack packet 5 sent to Sender
C
ase 2
Source packet 4
Relay packet 4
Source packet 3
C
ase 1
Relay packet 2
Relay packet 1
Relay packet 5
Ack packet 2
Ack packet 1
Source packet 2
Ack packet 3
Ack packet 4
Sender
ReceiverRelay Node
Source packet 1
Relay packet 3
Fig. 3. Some common network reordering scenarios
0 100 200 300 400 500 600 700
2
3
4
5
6
(a) Real processing delay (ms)
Av
er
ag
e 
ab
so
lu
te
 e
rro
r (
ms
)
Basic method
IPID filtering
0 100 200 300 400 500 600 700
10
20
30
40
50
70
(b) Real processing delay (ms)
M
ax
im
um
 a
bs
ol
ut
e 
er
ro
r (
ms
)
Basic method
IPID filtering
Fig. 4. The average and maximum absolute errors of processing delay
estimates vs. the real processing delays.
in Section IV. This introduces a great deal of randomness
into the network delay of each packet, thus simply subtracting
ART from DDT might even derive a negative processing delay
estimate.
We run the experiment for 500 flows, each of which lasts for
10 minutes. We first evaluate the performance of both methods
by the average and maximum of the absolute estimation errors,
which are computed by |Real PD − Estimated PD|. Fig. 4
shows the absolute errors of the estimated processing delays
versus the real delays. We observe that the average absolute
errors are only 4 ms and 3 ms for the basic method and the
IPID filtering method respectively, so the difference between
the two methods is not significant. However, the benefit of the
IPID filtering is significant in terms of the maximum absolute
errors, e.g., the error reduces by 20 ms with IPID filtering,
while the real processing delays are around 200 ms.
IV. LARGE-SCALE MEASUREMENT
In this section, we describe how we measure the processing
delays of relayed packets for a large set of Internet relay
hosts, after which we begin with a description of our trace
collection procedures and a summary of the traces. We then
discuss the anomalous behavior patterns identified and their
causes. The section concludes with an accuracy assessment
6TABLE I
SUMMARY OF COLLECTED TRACES
# Calls Time # Pkts/call ART DDT
1, 115 10 min 19, 210 pkts 203 ms 212 ms
TABLE II
SUMMARY OF ESTIMATED PROCESSING DELAYS
# Samp. Samp. dens. PD (Avg / Max / SD)
15, 739 per call 26 samp. / sec 5 ms / 239 ms / 7 ms
• If the PD estimates are accurate, they should be highly
correlated with the IPID differences between each pair
comprised of an ack packet and a relay packet, assuming
that the IPIDs increase at a steady rate on a host. We find
that the correlation between PDs and IPID differences
has a mean of 0.62 and a standard deviation of 0.16
on average. In addition, the large PD samples should
be associated with large IPID differences for accurate
estimation. We find that among the top 5% of PD
estimates, on average, 48% are associated with the top
5% of IPID differences.
• If the PD estimates are accurate, they should be uncor-
related with the ack response times. We find that the
correlation between PDs and ack response times is merely
−0.01 on average, which indicates the large PD estimates
are not simply due to shorter ack response times.
• The IPID filtering method may fail to detect unusual
network delays if there is a low density of packets
surrounding a PD sample. To verify this possibility, we
define the maximum allowed delay time of a PD estimate,
where packet reordering cannot be detected by IPID
filtering, as a “PD error range.” We find that the top 5%
of PD estimates associate with the top 65% of PD error
ranges, and the top 20% of PD estimates associate with
the top 56% of PD error ranges. Also, the correlation
coefficient between the PD estimates and PD error ranges
is only 0.08 on average. These observations strongly
suggest that PD estimates, especially large ones, are not
a consequence of low-density surrounding packets.
We use the above tests to confirm the estimation accuracy of
the processing delays in our large-scale measurement. Having
verified the validity of the estimated processing delays, we
further analyze the characteristics of processing delays at relay
nodes and their impact on VoIP quality in subsequent sections.
V. PROCESSING DELAY CHARACTERIZATION
In this section, we analyze the relay node processing delays
obtained from the Internet measurement (Section IV). We
begin with a classification of the observed processing delays,
and the possible causes of each type of delay, and then
analyze the effect of packet size on processing delays. Finally,
we characterize the stability of the processing delays and
investigate the relationship between delay stability and host
activity.
A. Classification of Processing Delays
Since processing delays at a relay node are closely related
to the workload on the node, we can consider a pattern of
0 5 10 15 20
0.
00
0.
02
0.
04
0.
06
0.
08
0.
10
0.
12
(a) Mean processing delay (ms)
D
en
si
ty
0 100 200 300 400 500
0.
00
0
0.
00
1
0.
00
2
0.
00
3
0.
00
4
(b) Maximum processing delay (ms)
D
en
si
ty
0 50 100 150
0.
00
0
0.
00
4
0.
00
8
0.
01
2
(c) Maximum / mean processing delay
D
en
si
ty
Fig. 7. The distributions of (a) average processing delays (b) maximum
processing delays (c) the ratio of the maximum processing delay versus the
average processing delay in a call.
processing delays as an indication of the workload structure.
We find that the processing delay patterns can be classified
into five categories, as shown in Fig. 14. The categories and
the possible causes of each type of delay are as follows:
• Typical: This type of processing delay is the most com-
mon. The variation of PDs is small and their magnitude
is rarely higher than 20 ms, which implies that the relay
node is lightly-loaded and the computer is not in use.
• Variable: PDs are stable most of time, but they occa-
sionally exhibit very different behavior. For example, the
PDs of Call 561 are relatively high during the 250–450
ms period, which is likely due to a person using the
computer, or an application with a time-varying workload
is running.
• Level-Shifted: The levels of PDs are increased or de-
creased by a significant magnitude, say, larger than 10
ms. This indicates that a heavily loaded task starts or
stops running on the relay node during the call.
• Periodic: Bursts of high PDs occur at regular intervals,
possibly because of the behavior of an application. For
example, the one-minute interval in the processing delays
of Call 214 is likely caused by an email notification
program with a one-minute check interval.
• Loaded: The level of PDs remains large, say 100 ms
or higher. This implies that the relay node is under
a heavy workload generated by computation- or I/O-
intensive applications.
Fig. 7(a) and Fig. 7(b) plot the distributions of the average
and maximum PDs of each call. It can be seen that the average
PDs are generally shorter than 20 ms. The maximum PDs,
however, have a much broader range, which spreads over 0
to 500 ms with a mean around 100 ms. We use the ratio
between the maximum and average PDs to quantify the degree
of maximum PD variation in a call, and plot its distribution
in Fig. 7(c). The median of the ratios is 30, while its 90%
percentile is around 200. The high variability of the ratios
indicates that the variation of PDs can be extremely large,
even within a 10-minute period.
B. Stability Analysis
We now turn to the stability of processing delays, which is
closely related to the workload structure of a relay node. We
begin our analysis with a definition of the metric busy level,
which is designed to capture the level of the workload on the
relay node. The busy level (BL) is defined as the 95% quantile
of processing delays within a window of 10 seconds.
8During each simulation run, we compute the end-to-end
delay and packet loss rate based on a given pair of network
delay and processing delay traces. The end-to-end delay is
decided by the playout buffer size, which also determines the
end-to-end loss rate because a packet is considered lost if it
cannot meet the playout schedule. There are two common
schemes for adjusting the playout buffer size, namely static
and adaptive, both of which are included in our simulations.
For the static buffer strategy, we use a fixed 60-ms buffer after
Skype’s setting [23]. Our adaptive buffer is computed by the
following equation:
pi = di + 4× vi, (1)
di = α× di + (1− α)× ni, (2)
vi = β × vi−1 + (1− α) |di − ni| , (3)
where pi is the buffer size for packet i+1, ni is the network
delay of packet i, and α = β = 1−0.998002 according to [20].
We use the ITU-T E-model [12] to quantify the voice quality
of each simulated call. Essentially, the E-model transforms a
set of transmission impairments into a psychological satisfac-
tion level. The main computation equation of the E-model is
R = Ro − Is − Id − Ie +A,
where Ro represents the fundamental signal-to-noise ratio, Is
represents the impairments occurring simultaneously with the
voice signal, Id represents the impairments related to delays,
and Ie represents the impairments related to information loss.
The advantage factor A is used for compensation when there
are other advantages of access available to the user. Since the
computation of Ro and Ie is codec-dependent, for simplicity,
we assume the voice codec is the widely deployed G.711. The
output of the E-model is the R-score, which represents the
overall voice quality via a 100-pt scale. In our settings, the
R-score reaches a maximum of 93.2 without any end-to-end
delay or loss. Generally an R-score higher than 80 implies
a satisfactory VoIP quality, while an R-score lower than 70
implies a quality that many users find unacceptable [19].
Following Table 1 in [19], we use a reduction of 10-points
in the R-score to denote a significant degradation in VoIP
conversation quality.
B. Performance Degradation
1) Transmission Delay and Loss: For each pair of network
delay and relay processing delay traces, we assess the quality
degradation by evaluating the network performance separately,
i.e., end-to-end delay and loss, as well as voice quality, with
and without relay processing. First, we consider the network
performance degradation shown in Fig. 9. From the figure,
we observe that the average delay increase caused by relay
processing is not large because the playout buffer absorbs
the variability introduced by processing delays. However, a
fraction of calls still experience significant end-to-end delay
increases, say, greater than 50 ms. The reason for the larger
delay increase in the adaptive buffer scheme is that the buffer
size is proportional to the mean deviation of network delays
(Eqn. 3); thus, it becomes larger in the face of highly spiky
processing delays. Moreover, the adaptive buffer does not seem
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
(a) Avarage delay increase (ms)
CD
F
1 5 10 20 50 100
Static buffer
Adaptive buffer
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
(b) Packet loss rate increase (%)
CD
F
0.02 0.1 0.5 1 2
Static buffer
Adaptive buffer
Fig. 9. The distributions of (a) average increase of end-to-end delays, (b)
increase of packet loss rate in each call.
−10 0 10 20 30 40 50
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
(a) Avarage R−score decrease
CD
F
Static buffer
Adaptive buffer
0 20 40 60 80
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
(b) Maximum R−score decrease
CD
F
Static buffer
Adaptive buffer
Fig. 10. The distributions of (a) average R-score increase, (b) maximum
R-score increase in each call.
resilient enough to absorb highly variable processing delays,
as it leads to both longer delays and higher packet loss rates
than the static buffer scheme. In addition, the static buffer
always absorbs delay deviations shorter than 60 ms such that
only 1% of calls have an end-to-end loss rate higher than 1%,
compared to 10% of calls under the adaptive buffer strategy.
2) VoIP Quality: With respect to VoIP quality degradation,
we can see from Fig. 10(a) that the average R-score decrease
caused by relay processing is generally small; most calls
have an average R-score decrease smaller than 10. A few
calls with adaptive buffering do have better voice quality, i.e.,
an increased R-score. Our analysis shows that this counter-
intuitive behavior is due to a lower packet loss rate. The lower
rate is a consequence of a larger playout buffer, which in
turn is caused by a higher degree of delay variation due to
relay processing. At the same time, the adaptive buffer scheme
also leads to poor conversation quality, as about 20% of calls
have an average R-score decrease of more than 10, which we
consider a significant degradation in quality.
The performance degradation is much more obvious if
we check the distribution of maximum R-score decreases in
Fig. 10(b). The adaptive buffer scheme still performs much
worse than the static buffer scheme. However, both schemes
are subject to significant quality degradation due to relay
processing; 40% and 58% of calls experienced considerable
quality degradation for the static and adaptive buffer strategies
respectively.
3) Original Quality vs Degradation: We note that the
impact of processing delay is not equivalent for all calls.
According to the E-model, a better quality call is more prone
to performance degradation [12, 16]. We verify this effect by
10
0 100 200 300 400 500
5
10
15
20
25
30
Typical (Call 228)
Time (sec)
R
D
 (m
s)
0 100 200 300 400 500
0
5
10
15
20
Typical (Call 979)
Time (sec)
R
D
 (m
s)
0 100 200 300 400 500
20
40
60
80
Variable (Call 561)
Time (sec)
R
D
 (m
s)
0 100 200 300 400 500
0
50
10
0
20
0
30
0
Variable (Call 724)
Time (sec)
R
D
 (m
s)
0 100 200 300 400 500
0
10
20
30
40
50
60
Level−Shifted (Call 471)
Time (sec)
R
D
 (m
s)
0 100 200 300 400 500
0
5
10
15
Level−Shifted (Call 475)
Time (sec)
R
D
 (m
s)
0 100 200 300 400 500
0
20
40
60
80
10
0
Periodic (Call 214)
Time (sec)
R
D
 (m
s)
0 100 200 300 400 500
0
50
10
0
15
0
20
0
25
0
30
0 Periodic (Call 444)
Time (sec)
R
D
 (m
s)
0 100 200 300 400 500
0
50
10
0
15
0
20
0
25
0
Loaded (Call 381)
Time (sec)
R
D
 (m
s)
0 100 200 300 400 500
0
50
10
0
15
0
Loaded (Call 727)
Time (sec)
R
D
 (m
s)
Fig. 14. A classification of common processing delay patterns. We list five
categories: typical, variable, level-shifted, periodic, and loaded.
REFERENCES
[1] S. Baset and H. Schulzrinne, “An analysis of the Skype peer-to-
peer internet telephony protocol,” in INFOCOM. IEEE, 2006.
[2] K.-T. Chen, C.-Y. Huang, P. Huang, and C.-L. Lei, “Quantifying
Skype user satisfaction,” in Proceedings of ACM SIGCOMM
2006, Pisa Itlay, Sep 2006.
[3] C.-M. Cheng, Y.-S. Huan, H. T. Kung, and C.-H. Wu, “Path
probing relay routing for achieving high end-to-end per-
formance,” in Global Telecommunications Conference, 2004.
GLOBECOM ’04. IEEE, vol. 3, 2004, pp. 1359–1365.
[4] T. Fei, S. Tao, L. Gao, and R. Gue´rin, “How to select a good
alternate path in large peer-to-peer systems?” in INFOCOM.
IEEE, 2006.
[5] T. Fei, S. Tao, L. Gao, R. Gue´rin, and Z.-L. Zhang, “Light-
weight overlay path selection in a peer-to-peer environment,”
in INFOCOM. IEEE, 2006.
[6] B. Ford, P. Srisuresh, and D. Kegel, “Peer-to-peer communi-
cation across network address translators,” in USENIX Annual
Technical Conference, 2005, pp. 179–192.
[7] Google, Inc., http://www.google.com/talk/.
[8] S. Guha and N. Daswani, “An experimental study of the
Skype peer-to-peer VoIP system,” Cornell University, Tech.
Rep., Dec. 16 2005.
[9] F. Gustafsson, Adaptive Filtering and Change Detection. John
Wiley & Sons, September 2000.
[10] X. Hei, C. Liang, J. Liang, Y. Liu, and K. Ross, “A Mea-
surement Study of a Large-Scale P2P IPTV System,” in IPTV
Workshop, International World Wide Web Conference, 2006.
[11] X. Hei and H. Song, “Stochastic relay routing in peer-to-peer
networks,” in Proceedings 41st IEEE International Conference
on Communications, 2006.
[12] ITU-T Recommandation, “G. 107. The E-Model, a Computa-
tional Model for Use in Transmission Planning,” International
Telecommunication Union, CHGenf, 2002.
[13] X. Liao, H. Jin, Y. Liu, L. M.Ni, and D. Deng, “Anysee: Peer-
to-peer live streaming,” in INFOCOM. IEEE, 2006.
[14] L. Liu and R. Zimmermann, “Adaptive low-latency peer-to-peer
streaming and its application,” Multimedia Systems, vol. 11,
no. 6, pp. 497–512, 2006.
[15] Y. Liu, Y. Gu, H. Zhang, W. Gong, and D. Towsley, “Application
level relay for high-bandwidth data transport,” in The First
Workshop on Networks for Grid Applications (GridNets), San
Jose, October 2004.
[16] A. Markopoulou, F. A. Tobagi, and M. J.Karam, “Assessment
of voIP quality over internet backbones,” in Proceedings of
INFOCOM, 2002.
[17] S. McCanne and V. Jacobson, “The BSD packet filter: A new
architecture for user-level packet capture,” in Proceedings of
USENIX’93, 1993, pp. 259–270.
[18] J. Nagle, “Congestion control in IP/TCP internetworks,” Com-
puter Communication Review, vol. 14, no. 4, pp. 11–17, Oct.
1984.
[19] M. Narbutt, A. Kelly, L. Murphy, and P. Perry, “Adaptive voIP
playout scheduling: Assessing user satisfaction,” IEEE Internet
Computing, vol. 9, no. 4, pp. 28–34, 2005.
[20] R. Ramjee, J. F. Kurose, D. F.Towsley, and H. Schulzrinne,
“Adaptive playout mechanisms for packetized audio applica-
tions in wide-area networks,” in INFOCOM, 1994, pp. 680–688.
[21] S. Ren, L. Guo, and X. Zhang, “ASAP: an AS-aware peer-relay
protocol for high quality voIP,” in Proceedings of ICDCS, 2006,
pp. 70–79.
[22] J. Rosenberg, R. Mahy, and C. Huitema, “Traversal Using
Relay NAT (TURN),” draft-rosenberg-midcom-turn-05 (work in
progress), July, 2004.
[23] B. Sat and B. W. Wah, “Playout scheduling and loss-
concealments in voip for optimizing conversational voice com-
munication quality,” in Proceedings of Multimedia’07. New
York, NY, USA: ACM, 2007, pp. 137–146.
[24] C. Schensted, “Longest increasing and decreasing subse-
quences,” Canad. J. Math., vol. 13, pp. 179–191, 1961.
[25] Skype Limited, http://www.skype.com.
[26] D. A. Solomon and M. Russinovich, Inside Microsoft Windows
2000. Microsoft Press Redmond, WA, USA, 2000.
[27] H. Zhang, L. Tang., and J. Li, “Impact of Overlay Routing on
End-to-End Delay,” in Proceedings of ICCCN, 2006, pp. 435–
440.
[28] Y. Zhang and N. G. Duffield, “On the constancy of internet path
properties,” in Proceedings of Internet Measurement Workshop,
V. Paxson, Ed. San Francisco, California, USA: ACM, Nov
2001, pp. 197–211.
[29] R. Zimmermann, B. Seo, L. Liu, R. Hampole, and B. Nash,
“Audiopeer: A collaborative distributed audio chat system,”
Distributed Multimedia Systems, San Jose, CA, 2004.
2D
en
si
ty
D
en
si
ty
D
en
si
ty
3T2TT
Packet inter-arrival time
(c) IPT distribution with the effect 
of network delay and loss
3T2T
val time
tribution with 
network delay
3T2TT
Packet inter-arrival time
(a) Ideal IPT distribution
T
Packet inter-arri
(b) Ideal IPT dis
the effect of 
Fig. 1. The distribution of IPTs with and without disturbance caused
by network impairment (a) Ideal (b) Delay introduced (c) Delay and loss
introduced
much more serious than in wired networks.
• Network Queueing: A packet may be queued for a while
when it passes through a congested link. The queueing
delay depends on the congestion level of the link. Since
the queueing delays incurred by different packets may
not be the same, the IPTs between packets that have been
queued would be altered.
• Network Packet Loss: A packet may be dropped when
passing through a congested link if the packet queue
is full, which will lead to IPTs of multiples of T . For
example, assume a series of three packets, with two
IPTs (T, T ), are to be transmitted over the Internet. If
the middle packet is dropped by a router, then the IPT
between the remaining two papers will be 2T . Similarly,
the IPT value will be kT if a packet loss burst of length
(k − 1) occurs.
The graph in Fig. 1 illustrates the effect of network delay
and loss on the distribution of IPTs in CPR traffic. As shown
in the figure, even though the packets leave the sender with
exactly equal inter-packet times, the IPTs disturbed by network
delays tend to have a wider distribution compared to the
original distribution. Fig. 1(c) shows that the distribution of
IPTs disturbed by both network delays and network loss would
have multiple clusters centered around T, 2T, 3T, ..., kT , if no
packet loss bursts of length longer than k − 1 have occurred.
Contributions: The contribution of this study is three-fold.
1) We propose using a deviation-based CPR-traffic classifier
to identify real-time interactive applications, specifically VoIP
and online gaming. 2) We show that scale-free variability
measures, such as CoV (the coefficient of variation), are more
appropriate than scale-dependent metrics, such as SD (the
standard deviation), for quantifying the network variability in-
curred by CPR traffic. 3) The proposed classifier is particularly
lightweight in that it requires only a few inter-packet times to
make a decision. Our evaluation shows that by analyzing only
10 successive inter-packet times, we can distinguish between
CPR and non-CPR traffic with approximately 90% accuracy.
The remainder of this paper is organized as follows. Sec-
tion II considers related works. We discuss the measurement
methodology and summarize our traces in Section III. In
Section IV, we describe the design of the deviation estimators
and demonstrate their ability to classify CPR and non-CPR
flows. In Section V, we evaluate the performance of different
deviation estimators in terms of classification accuracy, and
identify the sources of misclassifications. Then, in Section VI,
we present our conclusions.
II. RELATED WORK
Traffic identification was not a serious problem until re-
cently because most Internet flows used well-known TCP/UDP
ports, such as port 80 for HTTP, and port 23 for TELNET
services. Thus, a simple inspection of the TCP/UDP port
number field could identify the application associated with
a certain flow [18]. However, the effectiveness of port-based
classification has been largely reduced due to the prevalence
of P2P applications, many of which use random port num-
bers, or even port numbers that have been assigned to other
applications. For instance, HTTP port numbers (80/443) are
often used by P2P applications to traverse a firewall [16].
Thus, a simple inspection of the port number field may lead
to inaccurate classification.
Payload-based Classification To identify a variety of P2P
applications whose protocols are often proprietary, ill-defined,
and even encrypted, a payload-based classification approach
has been proposed in [11, 23].
Payload-based classification relies on a unique signature
carried by application packets, so the identification accuracy
can be very high. Even so, the approach has some major
drawbacks: 1) The packet payload may be inaccessible in a
carrier network because obtaining such information would be
a violation of privacy. 2) Payload pattern matching usually
relies on the flexibility of regular expressions; however, soft-
ware implementations of regular expression matching incur
a heavy computational load, while hardware implementations
may restrict the expression flexibility and the state number
of the transformed DFA (deterministic finite automata). 3) A
proprietary application’s signature may be subject to change at
any time, so payload-based classifiers require frequent updates
of the signature database to detect newly released software.
Flow-Information-based Classification Another traffic
classification approach is based on a summary of flow statis-
tics, such as the flow duration, number of packets, flow inter-
arrival times, and packet inter-arrival times. Machine learn-
ing [15] or statistical clustering techniques [17, 19, 22] are
usually employed to process flow information for classification
purposes. Among the numerous studies, [15] extracts represen-
tative features from bulk data flows, such as HTTP, FTP, and
SMTP, to create a workload model for network simulators.
In [22], traffic is classified into three classes, namely, bulk
data transfer (e.g., FTP), conversational communications (e.g.,
TELNET), and streaming traffic, to provide different levels of
QoS for each class. Meanwhile, [19] uses a reference pattern
approach, which designates the packet size and inter-packet
time as flow features, to identify VoIP flows for management
purposes and QoS provisioning.
A number of studies do not belong to either of the above
categories. The approach proposed in [12] identifies P2P traffic
based on the transport layer characteristics, as P2P applications
usually exhibit distinct connection patterns; for example, they
create both TCP and UDP connections using the same port
number, and establish several connections simultaneously.
BLINC [13] considers the association between Internet hosts
4times, on a time scale roughly equal to the duration of the
flows. The histograms in Fig. 2 can be interpreted as follows. If
the long-term IPT distribution of an application is heavily con-
centrated within a small range, it is reasonable to expect that
the application’s short-term IPT deviations will also be small.
However, the opposite is not always true. If the long-term IPTs
are widely distributed, then the corresponding short-term IPT
variation might be small, moderate, or large—depending on
how the IPTs are temporally auto-correlated. For example,
if IPTs are positively auto-correlated, indicating that a small
(large) IPT is likely to be followed by a small (large) IPT, then
their small-scale burstiness tends to be small. In contrast, if
IPTs are negatively auto-correlated, which implies that small
and large IPTs are apt to occur alternately, then the short-term
IPT deviations tend to be large.
As one might expect, the IPTs of CPR applications, VoIP
and Counter-Strike in Fig. 2, are heavily clustered with a
mode smaller than 100 ms. Moreover, most of the IPTs are
not far from the cluster’s center. Non-CPR applications—
HTTP, P2P, TELNET, and World of Warcraft—also have a
considerable number of IPTs clustered around a small value
(at scales of 10 ms or smaller). They also have IPTs widely
spread over the range of the x-axis, which clearly manifests
the non-CPR nature of their traffic patterns. Interestingly, the
IPT distributions associated with non-CPR traces are all multi-
modal. We consider that the 200 ms peaks are due to TCP’s
Nagle algorithm and delayed-ack mechanism [24], while the
remaining peaks are probably due to the application’s design.
In particular, we find that the pronounced peaks at 200 ms
and 500 ms in the World of Warcraft trace occur because
the game exchanges data with regular time intervals. This
design is common in network games for maintaining the state
consistency between peers [5].
We also provide the coefficient of variation (CoV) of the
IPTs above each figure. The value of the CoV provides a scale-
independent way to quantify the dispersion of IPTs. Clearly,
the IPT CoV for the VoIP and Counter-Strike traces (< 0.7)
is significantly smaller than those of non-CPR applications
(> 1.5), while the IPT CoV of the World of Warcraft trace
is approximately in-between the two extremes. It seems that
a simple variability measure like the CoV may already be
a useful indicator of whether a flow is CPR or not. Even
so, whether IPT deviations inferred from short-term IPT
series, rather than long-term IPT series, can achieve similar
classification accuracy remains a question. In the rest of this
paper, we attempt to answer this question.
B. Estimating Short-Term IPT Deviation
In Fig. 2, the long-term IPT deviation seems to be a decisive
feature for distinguishing CPR flows from non-CPR flows, as
the former exhibit a much smaller degree of IPT dispersion.
However, in practice, IPT deviations must be computed based
on short-term observation of packet arrivals for the following
reasons:
• Many applications, such as usage accounting and traffic
blocking, require the classifier to make a very quick
decision (e.g., in sub-seconds).
VoIP (CoV = 0.69)
Inter−packet time (ms)
D
en
si
ty
0 200 400 600 800 1000
0.
00
0
0.
01
0
0.
02
0
0.
03
0
Counter−Strike (CoV = 0.60)
Inter−packet time (ms)
D
en
si
ty
0 200 400 600 800 1000
0.
00
0
0.
01
0
0.
02
0
TELNET (CoV = 1.66)
Inter−packet time (ms)
D
en
si
ty
0 200 400 600 800 1000
0.
00
0
0.
01
0
0.
02
0
HTTP (CoV = 1.66)
Inter−packet time (ms)
D
en
si
ty
0 200 400 600 800 1000
0.
00
0
0.
01
0
0.
02
0
0.
03
0 P2P (CoV = 1.59)
Inter−packet time (ms)
D
en
si
ty
0 200 400 600 800 1000
0.
00
0
0.
01
0
0.
02
0
World of Warcraft (CoV = 0.91)
Inter−packet time (ms)
D
en
si
ty
0 200 400 600 800 1000
0.
00
0
0.
00
4
0.
00
8
Fig. 2. IPT distribution of different applications
• The classifier must keep a flow’s state in its memory
before a decision can be made. Thus, the faster the
detection, the less time a flow’s state needs to be kept.
Consequently, more flows can be served simultaneously
with the same amount of memory.
Therefore, even if the dispersion of a long IPT sequence can
be used to accurately identify whether a flow is CPR or not, we
have to limit the length of an IPT series so that we can obtain a
quick decision. The difficulty is that, while long IPT sequences
are primarily governed by an application’s traffic patterns, a
short IPT series is less robust and less reliable for inferring the
“true” traffic patterns (such as the variability) of an application.
This is because the amount of traffic characteristics captured
by short packet sequences depends on the sampling time
and varying network conditions. In other words, short-term
IPT variability, though inherited from the application traffic
variability, might be dominated by sampling randomness and
network dynamics.
Because of the detection time and unavoidable randomness,
we need an estimator for short-term IPT deviations that has
the following properties:
1) It should be able to cope with the randomness introduced
by sampling and network dynamics.
2) It should achieve high discriminability between CPR and
non-CPR traffic.
3) It should incur the lowest possible storage and compu-
tation overhead.
More specifically, the desired estimator should obtain low de-
viation measures for CPR traffic, and high deviation measures
for non-CPR traffic, regardless of the inevitable randomness
introduced in packet arrival sequences.
In the following, we discuss three aspects of designing
an IPT deviation estimator, namely, the deviation metric, the
sample size w, and the smoother size s. We list a number of
possible designs with regard to each aspect, and the reasons
why they deserve to be considered. Hereafter, we use “IPT
deviation(s)” to denote short-term IPT deviation(s).
1) Deviation Metric: In statistics, there are numerous mea-
sures for quantifying the degree of spread, or variability, in
a set of numbers because different metrics usually emphasize
different portions of “variability” in the data. For example,
some metrics, such as the median absolute deviation (MAD),
60 1 2 3 4 5
0
1
2
3
4
VoIP
IPT deviation
D
en
si
ty
w = 3
w = 5
w = 10
w = 20
w = 30
0 1 2 3 4 5
0
1
2
3
4
5
6
Counter−Strike
IPT deviation
D
en
si
ty
0 1 2 3 4 5
0.
0
0.
4
0.
8
TELNET
IPT deviation
D
en
si
ty
0 1 2 3 4 5
0.
0
1.
0
2.
0
3.
0 HTTP
IPT deviation
D
en
si
ty
0 1 2 3 4 5
0.
0
1.
0
2.
0
P2P
IPT deviation
D
en
si
ty
0 1 2 3 4 5
0.
0
0.
2
0.
4
0.
6
0.
8
World of Warcraft
IPT deviation
D
en
si
ty
Fig. 4. IPT deviation distributions computed with various sample sizes
packets per second [4]. With this setting, the flow states have
to be kept in memory for about one second, which may incur
a high overhead for a classifier under a heavy traffic load. For
example, in a scenario where an average of ten thousand flows
are established in one second, each flow requires 100 bytes for
state maintenance at an average rate of 15 packets per second.
Hence, at least a 2-MB SRAM is required to maintain the
states for that have yet to be classsified flows.
In Fig. 4, we plot the distributions of IPT deviations derived
from samples of different size. We make two observations
from the figure. First, the IPT deviations captured by small
samples (e.g., w < 10) are not robust enough and may differ
drastically depending on the particular IPTs observed. This
explains why IPT deviation distributions for small samples
show multiple peaks for non-CPR traces. Second, although
large samples have more consistent deviation estimates, they
generally capture more variability and lead to higher estimates
(identified by modes of larger magnitude). This is because
longer packet sequences are more likely to include irregular
packet arrivals and occasional randomness introduced by net-
work dynamics.
3) Smoother Size: The sample size provides a means of re-
stricting the time scale at which deviation estimates are taken;
however, the granularty of deviations is always measured at
the scale of packet inter-spacing times.
Consider a scenario where the application, host, and net-
work have introduced considerable randomness into a CPR
packet stream. As a result, the stream’s IPT deviations become
large, and thus not easily distinguishable from non-CPR traffic.
In this case, we can apply a smoother to remove short-term
IPT fluctuations before calculating the deviation estimates. For
an IPT sample of size w3, (ipt1, ipt2, ..., iptw) and smoother
size s, the smoothed IPT series of size w/s is computed as
(
∑s
i=1 ipti/s,
∑2s
i=s+1 ipti/s, ...,
∑w
i=w−s+1 ipti/s). With an
appropriate smoother of size s, we can remove IPT fluctuations
in time scales smaller than s × mean(ipt). The deviation
estimates are then derived from this smoothed series.
We apply smoothers with sizes ranging from 2 to 10 IPTs
and compute the IPT deviation estimates using the CoV metric.
As shown in Fig. 5, the larger the smoother size, the smaller
3For simplicity and without loss of generality, we assume that w = k ×
s, k ∈ N.
0 1 2 3 4 5
0
5
10
15
20
VoIP
IPT deviation
D
en
si
ty
s = 1
s = 2
s = 5
s = 10
0 1 2 3 4 5
0
2
4
6
8
12
Counter−Strike
IPT deviation
D
en
si
ty
0 1 2 3 4 5
0.
0
0.
5
1.
0
1.
5
TELNET
IPT deviation
D
en
si
ty
0 1 2 3 4 5
0.
0
0.
4
0.
8
HTTP
IPT deviation
D
en
si
ty
0 1 2 3 4 5
0.
0
0.
5
1.
0
1.
5
P2P
IPT deviation
D
en
si
ty
0 1 2 3 4 5
0.
0
0.
5
1.
0
1.
5
2.
0
World of Warcraft
IPT deviation
D
en
si
ty
Fig. 5. IPT deviation distributions computed with various smoother sizes
TELNET (81%)
CoV of inter−packet time
D
en
si
ty
0.0 0.5 1.0 1.5 2.0 2.5 3.0
0.
00
0.
05
0.
10
0.
15
0.
20
VoIP
TELNET
HTTP (87%)
CoV of inter−packet time
D
en
si
ty
0.0 0.5 1.0 1.5 2.0 2.5 3.0
0.
00
0.
05
0.
10
0.
15
0.
20
VoIP
HTTP
P2P (94%)
CoV of inter−packet time
D
en
si
ty
0.0 0.5 1.0 1.5 2.0 2.5 3.0
0.
00
0.
05
0.
10
0.
15
0.
20
VoIP
P2P
World of Warcraft (80%)
CoV of inter−packet time
D
en
si
ty
0.0 0.5 1.0 1.5 2.0 2.5 3.0
0.
00
0.
05
0.
10
0.
15
0.
20
VoIP
World of Warcraft
Fig. 6. Differences between our VoIP traffic and non-CPR traffic traces based
on deviation estimates (CoV, w = 10 and s = 1)
the deviation estimates we obtain. On the one hand, we expect
the smoothers to remove undesirable fluctuations injected into
CPR packet streams. On the other hand, smoothers that are
too large may unnecessarily remove the intrinsic variability
of non-CPR traffic. Thus, choosing the smoother size is also
an important issue in designing an appropriate IPT deviation
estimator.
C. Graphical Demonstration
Having discussed several aspects of designing an estimator
for short-term deviations of IPTs, we now use a graphical
demonstration to illustrate how the deviation estimates are
used for the classification of CPR and non-CPR traffic.
In Fig. 6, we plot the IPT deviation distributions for VoIP
and each non-CPR traffic trace, where the deviation estimator
used is comprised of the CoV metric, a 10-IPT sample size
(w = 10), and no smoothing (s = 1). We can see from the
graphs that VoIP traffic clearly has very different IPT deviation
distributions to those of non-CPR traffic. The vertical dashed
line marks the threshold value that has the most discriminative
power. The number following each trace name denotes the
discrimination accuracy, which is computed as the number of
82 4 6 8 10
0.
80
0.
82
0.
84
0.
86
0.
88
0.
90
0.
92
0.
94
Smoother size (pkts)
AU
C
w = 4
w = 6
w = 10
w = 20
w = 30
Fig. 9. Flow classification performance with different sample sizes and
smoother sizes
In Fig. 8, we plot the ROC curves that correspond to the
deviation estimators comprised of the CoV metric and samples
of different size to evaluate their classification performance.
In an ROC plot, the top-left area denotes higher true positive
rates and lower false positive rates. We can see that the curves
shift toward the top-left region as the sample size increases,
along with a decrease in marginal utility. Generally, the true
positive rate increases much faster than the false positive rate,
which is an indication of a good classifier. The curves also
reveal that the false positive rates are non-insignificant (10%
for w = 10, and 5% for w = 30) when the true positive
rate reaches 90%, even when the sample size is large. This
indicates that non-CPR traffic may comprise CPR-like packet
sequences that mislead the classifiers. We address this issue
and determine the source of the false positive classifications
in Section V-D.
C. Effect of Smoother Size
We now investigate the effect of the smoother size on
CPR discriminability. Fig. 9 shows the classification efficiency
of deviation estimators comprised of the CoV metric and
smoothers of different size. The CoV metric is chosen for
its generally good discrimination performance.
The result shows that, although smoothers may improve
the classification performance under certain conditions, the
increase is marginal. There is only an improvement when
the sample size is large and the smoother size is not com-
parable to the sample size. One possible explanation of this
phenomenon is that, while a single IPT may be affected by
large disturbances from the application or network, the average
IPTs across a few packets might be better able to capture the
original application’s traffic variability. However, smoothers
reduce the AUC value when the smoother size is comparable
to the sample size. As smoothers are unable to significantly
improve classification efficiency and may even reduce the
accuracy, we conclude that IPT smoothing prior to taking
deviation estimates is not helpful.
D. Error Analysis
Fig. 7 shows that the AUC does not increase unboundedly,
but converges at approximately 0.93 as the sample size in-
5 10 15 20 25 30
0.
75
0.
80
0.
85
0.
90
0.
95
Sample size (pkts)
AU
C
TELNET
HTTP
P2P
World of Warcraft
Fig. 10. Flow classification performance for CPR applications and individual
non-CPR applications
TELNET (75%)
Average inter−packet time (ms)
Fr
eq
ue
nc
y
0
10
00
20
00
30
00
1 10 100 1000
TP samples
FP samples
HTTP (97%)
Average inter−packet time (ms)
Fr
eq
ue
nc
y
0
20
00
40
00
60
00
0.1 1 10 100 1000
TP samples
FP samples
P2P (98%)
Average inter−packet time (ms)
Fr
eq
ue
nc
y
0
40
00
80
00
10 100 1000
TP samples
FP samples
World of Warcraft (69%)
Average inter−packet time (ms)
Fr
eq
ue
nc
y
0
40
00
80
00
0.1 1 10 100 1000
TP samples
FP samples
Fig. 11. Flow classification error analysis for CoV deviation estimator
creases. To determine the sources of misclassifications and
understand why they occur, we evaluate the discrimination
performance of CPR traffic and individual non-CPR traffic
streams, as shown in Fig. 10.
The result for non-CPR applications can be divided into
two groups. TELNET and World of Warcraft traffic is not
easily distinguishable from VoIP traffic (AUC ≈ 0.85 with
w = 10), while HTTP and P2P traffic is much more distinct
from CPR traffic (AUC > 0.95 with w = 10). We note that
the applications with worst classification performance are both
interactive, while the remaining two are both bulk transfer
applications.
What makes TELNET and World of Warcraft traffic less
distinguishable from CPR traffic? To determine whether par-
ticular traffic patterns affect the classification accuracy, the
histograms of the average IPTs of correctly- and mis-classified
samples for each non-CPR trace are plotted in Fig. 11. The
graph reveals that the false positive samples (those mis-
classified as CPR) for TELNET and World of Warcraft traces
have no particular relationship with average IPTs, as the
average IPTs of true-positive and false-positive samples have
approximately identical distributions. One possible explanation
for the poor discrimination for these two traces is the appli-
Detecting Peer-to-Peer Activity by Signaling Packet
Counting∗
Chen-Chi Wu†, Kuan-Ta Chen‡, Yu-Chun Chang†, and Chin-Laung Lei†
†Department of Electrical Engineering, National Taiwan University
‡Institute of Information Science, Academia Sinica
{bipa,congo}@fractal.ee.ntu.edu.tw, ktchen@iis.sinica.edu.tw, lei@cc.ee.ntu.edu.tw
1. MOTIVATION
Peer-to-peer traﬃc now constitutes a substantial propor-
tion of Internet traﬃc. However, managing such traﬃc is a
major challenge for network administrators, because peer-
to-peer applications tend to use dynamic port numbers and
proprietary protocols. There has been a great deal of re-
search in the area of peer-to-peer traﬃc detection, and a
number of approaches have been proposed; among them,
the application-layer approach and the transport-layer ap-
proach are considered the most promising. In the following,
we brieﬂy discuss the strengths and weaknesses of both ap-
proaches.
The application-layer approach [3] identiﬁes a protocol-
speciﬁc signature in the packet payload. This approach
achieves high detection accuracy because false positives do
not occur if the signature is suﬃciently unique; however, it
cannot identify applications with unknown signatures and
cannot be used for encrypted traﬃc. As a result, a peer-to-
peer application might easily avoid detection. Furthermore,
examining user payloads incurs a high computation over-
head and raises privacy concerns. On the other hand, the
transport-layer approach [1, 2] is based on the observation
that a host running peer-to-peer applications usually inter-
acts with a large number of other hosts. This approach is
more lightweight, as no payload examination is required, and
it is more robust against countermeasures of applications.
However, as most peer-to-peer applications share the same
property, i.e., communication with many hosts, it is hard to
recognize a particular peer-to-peer application based on the
network-level information only.
In this poster, we propose an approach that combines the
advantages of payload-based and transport-layer approaches,
and avoids their disadvantages. Speciﬁcally, our approach
can recognize particular peer-to-peer applications running on
the monitored host without checking packet payloads. The
∗This work was supported in part by Taiwan Information
Security Center (TWISC), National Science Council of the
Republic of China under the grants NSC 96-2219-E-001-001,
NSC 96-2219-E-011-008, and NSC 96-2628-E-001-027-MY3.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGCOMM’08, August 17–22, 2008, Seattle, Washington, USA.
Copyright 2008 ACM 978-1-60558-175-0/08/08 ...$5.00.
key to our approach is recognizing the signaling behavior
of a peer-to-peer application. Although a peer-to-peer ap-
plication can easily change its port number, payload, and
even message format, the signaling patterns between peers
are more fundamental and unlikely to change. For exam-
ple, a BitTorrent client needs to regularly exchange the ﬁle
bitmap containing the block status of its ﬁles with neigh-
boring peers. This signaling behavior is essential for the
maintenance of the BitTorrent network; and changing it
would lead to state inconsistency and software incompati-
bility problems. Therefore, it would be diﬃcult for a peer-
to-peer application to change its signaling behavior without
aﬀecting its normal operations.
Our methodology is unique in three respects:
• It does not require any application-layer information,
and only traﬃc associated with the monitored host
is required, i.e., a global view of the network is not
necessary.
• It recognizes particular applications running on the
monitored host, so it does not treat all peer-to-peer
traﬃc in the same way.
• It recognizes peer-to-peer applications based on their
unique signaling behavior ; thus, it is unlikely that an
application will evade recognition.
2. METHODOLOGY
The signaling behavior of each peer-to-peer application
is regulated by its underlying peer-to-peer protocol; there-
fore, each application possesses a distinguishing character-
istic. Based on this concept, we keep track of all the sig-
naling packets sent from and received by a monitored host.
By signaling packets, we mean the small packets exchanged
between peers to maintain network connectivity and other
peers’ states, not packets used to transfer ﬁles. Because we
cannot distinguish signaling packets from ﬁle or media trans-
fer packets, we simply assume that packets smaller than a
certain threshold, e.g., 100 bytes, are signaling packets.
Given a stream of signaling packets from a host, we char-
acterize its signaling behavior on two levels: the host level
and the message level.
• Host level. A host regularly exchanges information
with other hosts that are known to it, and also with
new contacts, i.e., previously unknown hosts. Based on
the number of new or old hosts it communicates with,
we characterize the signaling behavior at the host level
through a number of features, such as the ratio of new
hosts contacted within a certain period.
A User-Centric Framework for Comparing Applications’
Network Robustness∗
Hung-Hsuan Chen
Academia Sinica
sean@iis.sinica.edu.tw
Cheng-Chun Tu
Academia Sinica
williamt@iis.sinica.edu.tw
Kuan-Ta Chen
Academia Sinica
ktchen@iis.sinica.edu.tw
ABSTRACT
In this paper, we compare real-time and interactive network
applications in terms of their ability to handle network er-
rors, i.e., network delay and loss. Our approach is based
on session times, which indicate users’ perceptions of the
network performance. Using a regression model, we sepa-
rate the eﬀect of network error on users’ departure decisions
from an application’s baseline departure rate, and deﬁne an
index to quantify an application’s network robustness. We
demonstrate the eﬀectiveness of our framework on real-life
data traces.
1. MOTIVATION
Real-time data transmission is a constant challenge for
network researchers. In packet-switched networks, such as
the Internet, queuing delay and packet loss are inevitable.
To reduce the impact of various types of delay, researchers
have focused on rate control and the use of smoothing buﬀer;
and to cope with network loss, various error detection and
control mechanisms, such as ARQ and FEC, have been de-
veloped. As each solution involves one or more design de-
cisions, the design of a network application is unavoidably
high-dimensional. For example, an application that is ef-
fective for dealing with network delay may be ineﬀective in
dealing with network loss, whereas another one may show
opposite behavior. Furthermore, it is diﬃcult to determine
and interpret which performance dimension is most impor-
tant to an applications’s users. Therefore, quantifying and
comparing the goodness of network design across diﬀerent
applications remains an open issue.
Because it is often diﬃcult to justify design choices, we
may ﬁnd that diﬀerent designs serve the same purpose. For
example, some VoIP software programs adopt dynamic de-
jitter buﬀers, while some others adopt static buﬀers; and
some online RPG games adopt UDP as the message trans-
port protocol, while others adopt TCP. As the success of an
application depends on many non-technical issues, we can-
∗This work was supported in part by Taiwan Information
Security Center (TWISC), National Science Council of the
Republic of China under the grants NSC 96-2219-E-001-001,
NSC 96-2219-E-011-008, and NSC 96-2628-E-001-027-MY3.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGCOMM’08, August 17–22, 2008, Seattle, Washington, USA.
Copyright 2008 ACM 978-1-60558-175-0/08/08 ...$5.00.
not conclude that one design is better simply because an
application that adopts it is successful in the marketplace.
Thus, we are motivated by the question: How can we evalu-
ate and compare two or more network applications in terms
of their robustness to network errors?
2. METHODOLOGY
We adopt a user-centric approach, as we believe that users
are the best judges of a network application’s performance.
In previous studies [1,2], we showed that users tend to leave
an application early if they experience adverse network con-
ditions. Based on this ﬁnding, we use the Cox model [3] to
establish the relationship between user departure rates and
the quality of the network path. Rather than model session
times directly, we consider the cumulative hazard function,
which denotes the probability of user departure events at
time t. The function is deﬁned as follows:
h(t|X) = h0(t) exp(
p∑
i=1
βixi),
where h0(t) is the baseline hazard function; X = x1, . . . , xp
are the network QoS factors, such as network delay and loss
rate; and βi, . . . , βp denote the respective impact of each p
network factor.
As shown by the equation, the model separates the over-
all cumulative hazard function into two components; one is
application-speciﬁc and the other is not. The baseline haz-
ard function, h0(t), refers to the application-speciﬁc part. It
represents the “ideal” scenario in which there is no network
impairment. The second component, the risk score, repre-
sents the magniﬁer that summarizes the impact of network
impairment, which is comparable across diﬀerent applica-
tions. To calculate the expected hazard function based on
current network QoS, we multiply the score by the application-
speciﬁc baseline hazard function. For example, if the risk
score is 1.1, it means the current QoS setting allows a user
to leave the system 1.1 times faster than in the ideal sce-
nario. As the risk score has an identical meaning in diﬀerent
applications, we take it as an indicator of an application’s
network performance under diﬀerent network conditions.
We deﬁne the Network Robustness Index (NRI) to quan-
tify an application’s overall performance in a range of net-
work scenarios. To ensure that NRI is not signiﬁcantlly af-
fected by speciﬁc network conﬁgurations, we adopt a Gaussian-
mixture model to capture the target network scenarios. We
then compute the NRI as the reciprocal of the integral of
the risk scores over the mixture model. The NRI is deﬁned
as:
(
∫
x1,...,xp
exp(
p∑
i=1
βixi) dx1, ..., xp)
−1,
OneClick: A Framework for Capturing Users’
Network Experiences∗
Cheng-Chun Tu†, Kuan-Ta Chen†, Yu-Chun Chang‡, and Chin-Laung Lei‡
†Institute of Information Science, Academia Sinica
‡Department of Electrical Engineering, National Taiwan University
{williamt,ktchen,congo}@iis.sinica.edu.tw, lei@cc.ee.ntu.edu.tw
ABSTRACT
To learn about users’ experiences in using network applica-
tions, we present a lightweight, non-intrusive, and eﬃcient
framework called OneClick. The framework only requires a
subject to click a dedicated key whenever he/she feels dissat-
isﬁed with the quality of application in use. By aligning the
collected traﬃc trace and the click process, we apply Poisson
regression to analyze the relationship between click events
and network QoS factors. The result shows that OneClick
can be used to assess the eﬀect of network factors on users’
perceptions and compare the users’ experiences with diﬀer-
ent types of design or applications.
1. MOTIVATION
Providing high QoS for users of network applications is
one of the most challenging tasks of network researchers be-
cause of limited resources. As the service requirements of
network applications shift from high throughput to high me-
dia quality, interactivity, and responsiveness, the deﬁnition
of QoS is becoming more complicated and multidimensional.
For example, the QoS requirements of VoIP applications at
least contain sound ﬁdelity, voice loudness, noise level, echo
level, and conversational delay; and the QoS requirements of
online gaming at least contain interactivity, responsiveness,
and consistency. While it is not diﬃcult to measure any sin-
gle dimension of the QoS levels, how to capture users’ per-
ceptions when they are using network applications remains
an open question. For instance, suppose one network sce-
nario provides a QoS setting of (10, 15, 20), which represents
the consistency level, interactivity level, and responsiveness
level respectively, and another network provides a QoS set-
ting of (20, 15, 10). Which network conﬁguration is “better”
from the user’s perspective remains to be investigated.
One common way to learn about users’ network experi-
ences is to ask them to complete a questionnaire after they
use an application. However, this survey method cannot
∗This work was supported in part by Taiwan Information
Security Center (TWISC), National Science Council of the
Republic of China under the grants NSC 96-2219-E-001-001,
NSC 96-2219-E-011-008, and NSC 96-2628-E-001-027-MY3.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGCOMM’08, August 17–22, 2008, Seattle, Washington, USA.
Copyright 2008 ACM 978-1-60558-175-0/08/08 ...$5.00.
capture users’ perceptions in real time as it is performed
afterwards, and people are subject to the recency eﬀect [1],
i.e., the experience in the most recent experience dominates
users’ ratings. In addition, little information is captured be-
cause each subject only contributes one score in each test.
To overcome the above problems, we have developed a
framework to capture users’ perceptions of network applica-
tions. We call the framework OneClick, because users are
asked to click a dedicated button whenever they feel dissat-
isﬁed with the quality of the application in use. OneClick is
particularly eﬀective because it is lightweight, non-intrusive,
and eﬃcient. It is lightweight because it does not need large-
scale or expensive deployment of resources to perform exper-
iments. It is non-intrusive because the user does not need
to pay much attention to the scoring procedure, so his/her
ﬂow experience can be maintained. Finally, it is eﬃcient
because it captures many samples for a subject in each test.
We detail the methodology in Section 2 and present a simple
demonstration in Section 3.
2. METHODOLOGY
The OneClick framework comprises two phases. The ﬁrst
phase involves setting up experiments to collect users’ feed-
back, and the second analyzes the collected raw data to
summarize users’ perceptions under diﬀerent settings.
User experiments. The experiments can be performed
on any computer equipped with a traﬃc monitor and a key
logger. We ask a subject who is using a real-time inter-
active application, e.g., conferencing or gaming, to click a
dedicated button whenever he/she feels dissatisﬁed with the
quality of the application in use. Here, the quality of the ap-
plication refers to any QoS dimension that might make the
subject unhappy. For example, it could be poor voice qual-
ity or conversation smoothness in conferencing, or screen
freezing problem, status inconsistency, or slow responsive-
ness in online gaming. Users do not need to be well-trained
to participate in the experiments as only an intuitive click
action is required.
Data analysis. We apply Poisson regression [2] to model
the relationship between network factors and the click rate
by treating the former as the predictors and the latter as
a dependent variable. The click rate is the average num-
ber of times the subject clicks the button in one second.
Assume the click rate is C(t) and the network factors are
N1(t), N2(t), . . . , Nk(t) at time t. Then, the Poisson regres-
sion equation is
log(C(t)) = α0 + α1N1(t) + . . . + αkNk(t), (1)
where αi denotes the Poisson regression coeﬃcients, which
A Crowdsourceable QoE Evaluation Framework
for Multimedia Content
Kuan-Ta Chen12, Chen-Chi Wu3, Yu-Chun Chang3, and Chin-Laung Lei3
1Institute of Information Science, Academia Sinica
2Research Center for Information Technology Innovation, Academia Sinica
3Department of Electrical Engineering, National Taiwan University
ABSTRACT
Until recently, QoE (Quality of Experience) experiments had
to be conducted in academic laboratories; however, with the
advent of ubiquitous Internet access, it is now possible to ask
an Internet crowd to conduct experiments on their personal
computers. Since such a crowd can be quite large, crowd-
sourcing enables researchers to conduct experiments with a
more diverse set of participants at a lower economic cost
than would be possible under laboratory conditions. How-
ever, because participants carry out experiments without
supervision, they may give erroneous feedback perfuncto-
rily, carelessly, or dishonestly, even if they receive a reward
for each experiment.
In this paper, we propose a crowdsourceable framework
to quantify the QoE of multimedia content. The advantages
of our framework over traditional MOS ratings are: 1) it
enables crowdsourcing because it supports systematic ver-
iﬁcation of participants’ inputs; 2) the rating procedure is
simpler than that of MOS, so there is less burden on par-
ticipants; and 3) it derives interval-scale scores that enable
subsequent quantitative analysis and QoE provisioning. We
conducted four case studies, which demonstrated that, with
our framework, researchers can outsource their QoE evalu-
ation experiments to an Internet crowd without risking the
quality of the results; and at the same time, obtain a higher
level of participant diversity at a lower monetary cost.
Categories and Subject Descriptors
H.5.1 [Information Interfaces and Presentation]: Mul-
timedia Information Systems—Evaluation/methodology ; C.4
[Performance of Systems]: Design studies; H.1.2 [Models
and Principles]: User/Machine Systems—Human factors
General Terms
Experimentation, Human Factors, Performance
Keywords
Bradley-Terry-Luce Model, Crowdsourcing, Mean Opinion
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
MM’09, October 19–24, 2009, Beijing, China.
Copyright 2009 ACM 978-1-60558-608-3/09/10 ...$10.00.
Score (MOS), Paired Comparison, Probabilistic Choice Model,
Quality of Experience (QoE)
1. INTRODUCTION
To provide better service quality for users, multimedia
and network researchers endeavor to improve, respectively,
the presentation quality of multimedia content and network
infrastructure. As the ultimate goal is to provide a satisfy-
ing end-user experience, there is a strong need for a tech-
nique that can measure the quality of multimedia content
eﬃciently and reliably. By “quality” we mean Quality of
Experience (QoE) [22], which indicates the degree of a user’s
subjective satisfaction. It is often confused with the more
commonly used Quality of Service (QoS) concept, which
refers to an objective system performance metric, such as
the bandwidth, delay, and loss rate of a communication net-
work.
One of the most well-known experiment frameworks for
assessing the QoE of multimedia content is called the MOS
(Mean Opinion Score) rating test [19]. In an MOS test,
subjects are asked to give a rating from Bad (the worst) to
Excellent (the best) to grade the quality of a stimulus, and
the overall rating of the stimulus is obtained by averaging
the scores from repeated tests. MOS scoring has been widely
adopted because it is simple and intuitive; however, it is
liable to cause the following problems:
1. The rating standard is somewhat obscure to experi-
ment participants. As the concepts of the ﬁve scales,
i.e., Bad, Poor, Fair, Good, and Excellent, cannot be
concretely deﬁned and explained, subjects may be con-
fused about which scale they should give in each test.
2. Participants may have dissimilar interpretations of the
scales, e.g., Poor and Good. Thus, they may provide
diﬀerent ratings even if they have had similar experi-
ences with the same stimulus. According to a user’s
own idiosyncratic preferences and strategy, he/she may
give higher or lower scales compared to those of other
participants. This is the so-called scale heterogeneity
problem [31].
3. It is assumed that MOS scores are on an interval scale,
but it is only an ordinal scale [36]. In fact, when people
use the MOS scale, their cognitive distance between
Bad (1) and Poor (2) is usually diﬀerent from that
between Good (4) and Excellent (5) [36]. Thus, the
accuracy of MOS scores may be questionable, since
they are obtained by taking arithmetic means, which
does not apply to ordinal-scale measurements.
(i.e., consistency) of a participant’s inputs. This design
encourages participants to ensure that their judgments
in experiments are consistent.
3. To demonstrate the eﬃcacy of our framework, we con-
duct four case studies involving audio and visual mul-
timedia content. The results of laboratory and crowd-
sourced experiments indicate that we can obtain com-
parable evaluation results at lower economic cost and
with wider participant diversity.
Note that our framework is eﬀective even if the experi-
ments are not conducted by an anonymous Internet crowd.
Its simple rating procedure and capability to provide dif-
ferentiated rewards can reduce the burden on participants
and encourage them to give quality inputs. Consequently,
the framework can also be used to maintain the quality of
evaluation results in traditional laboratory and focus-group
studies.
The remainder of this paper is organized as follows. Sec-
tion 2 contains a review of related works. We describe the
proposed framework in Section 3, and, explain how to ap-
ply it to evaluate the QoE of audio and visual content in
Section 4 and Section 5 respectively. We then discuss the
eﬀectiveness of the experiment crowdsourcing strategy in
Section 6. Finally, in Section 7, we present our conclusions.
2. RELATED WORK
2.1 Quality of Experience Assessment
Methods for assessing the QoE of multimedia content can
be classiﬁed as either subjective or objective in nature. Sub-
jective methods ask for human participants’ opinions in the
evaluation process [8, 18]; while objective methods evalu-
ate the QoE of a multimedia clip by analyzing its con-
tent [20, 21], e.g., checking if unnatural noise occurs in a
compressed video segment. Note that the two approaches
are in fact complementary rather than mutually exclusive.
Subjective methods provide factual assessments of users’ ex-
periences; on the other hand, objective methods are more
convenient to use, but they require the results of subjective
experiments for model development and veriﬁcation.
No matter how sophisticated objective assessment meth-
ods may be, intrinsically they cannot capture all the QoE
dimensions that may aﬀect users’ experiences. For example,
PESQ yields inaccurate predictions when used in conjunc-
tion with factors like listening levels, loudness loss, echo,
sidetone, and the eﬀect of delays on conversations [21]. Mean-
while, external factors, such as the quality of the headsets
used in acoustic QoE evaluations, and the distance between
the viewer and the display in optical QoE evaluations, are
not considered by objective methods because they are hard
to measure and quantify. Therefore, subjective experiments
provide the most factual QoE evaluations of multimedia con-
tent, although the cost is usually high.
2.2 Paired Comparison
Paired comparison takes advantage of simple comparative
judgments to prioritize a set of stimuli, and subjects’ pref-
erences for the stimuli can be quantiﬁed via probabilistic
choice modeling [10]. The technique is used in various do-
mains, notably decision making and psychometric testing.
The Analytic Hierarchy Process (AHP) [32], a well-known
application of paired comparison, uses the preference priori-
ties extracted from paired comparison results to construct a
hierarchical framework that can help people make complex
decisions. Paired comparison is also used in the ranking of
universities [11], the rating of celebrities [26], and various
subjective sensation measurements, such as pain [28], sound
quality [9], and the taste of food [29].
2.3 Crowdsourcing
Crowdsourcing is a distributed model that assigns tasks
traditionally undertaken by employees or contractors to an
undeﬁned crowd [5, 16]. It achieves the goal of mass col-
laboration via Web 2.0 technologies. The main diﬀerence
between crowdsourcing and ordinary outsourcing is that a
task is carried out by an unspeciﬁc Internet crowd rather
than a speciﬁc group of people.
Online surveys may be the most popular application of the
crowdsourcing strategy for user studies. Previous works [4,
12,33,37] have shown that online surveys possess a number of
advantages over traditional face-to-face surveys. First, they
are more eﬃcient in terms of time and monetary cost, since
it is relatively easy to collect responses from a large number
of people within a short time frame online. Second, they
do not have the “interviewer eﬀect,” where the interviewer
may inﬂuence how respondents answer the questions. In ad-
dition, online surveys ﬁt in with subjects’ lifestyles; that is,
subjects can respond at their convenience, so they may be
more willing to complete questionnaires. However, online
surveys do have disadvantages. One major issue is that re-
searchers cannot use sampling techniques to select candidate
respondents, as they do for face-to-face surveys [4,12,33,37],
because there is no database that covers all Internet users
and their demographics.
Crowdsourcing services, such as Amazon Mechanical Turk
(MTurk) [25], extend the interactivity of crowdsourcing tasks
by more comprehensive user interfaces and micro-payment
mechanisms. MTurk is a popular crowdsourcing service that
provides a marketplace for a variety of tasks, and anyone
who wishes to seek help from the Internet crowd can post
their tasks on the website. Tasks can involve any kind
of eﬀort, such as participating in surveys, performing ex-
periments, or answering certain specialized questions. Re-
searchers have adopted MTurk to conduct user studies on
image annotation [34], document relevance [2], and docu-
ment evaluation [25]. Because of MTurk’s popularity, we
crowdsourced our QoE evaluation experiments on the web-
site and found that the results were satisfactory. We discuss
the quality of the crowdsourced experiment results in Sec-
tion 6.
2.4 Reward and Punishment Mechanisms
In crowdsourced user studies, it is important to provide
proper incentives so that participants are motivated to give
high quality answers. Reward and punishment mechanisms
have been discussed extensively in works on peer production
systems and reputation systems [30]. In peer production
systems, such as Wikipedia, Yahoo! Answers, and Games
with A Purpose (GWAP), users collaborate in the hope of
achieving a global outcome. A number of studies have used
game theoretic analysis to examine the rationality of incen-
tives in human computation games [14,15] and online Q&A
forums [23]. In reputation systems, a user’s reputation de-
pends on the accumulated ratings given by other users on
tion is shown in Fig. 3. The major diﬀerence between it and
the acoustic interface (Fig. 1) is that the state indicators are
more sophisticated. Since participants need to pay full at-
tention to the video on the screen, we provide an audio eﬀect
so that participants will hear a short, sharp tone whenever
the SPACE key is pressed or released. The pitch is higher
when the SPACE key is released and lower when the SPACE
key is pressed; thus, participants can focus on the quality
diﬀerence of the presented video and rely on the audio eﬀect
to determine the current state. In addition, the background
color of the video playout pane indicates the current state
(red and blue correspond to Pressed and Released respec-
tively), so that a participant can perceive the current state
via his/her peripheral vision. Similar to the acoustic inter-
face, users can watch the video clip repeatedly until they can
determine the quality diﬀerence between the two states, and
then vote by pressing the LEFT key or RIGHT key accord-
ingly. Readers can also experience this experiment design
at http://mmnet.iis.sinica.edu.tw/link/dv.
3.2 Individual Consistency Checking
In each experiment, we collect m =
(
n
2
)
paired comparison
results for n quality levels. Since we encourage crowdsourc-
ing of experiments in which participants may input random
decisions carelessly or intentionally, we need a way to detect
untrustworthy inputs. There are two reasons for this. First,
problematic inputs must be removed as they may cause inac-
curacies in the estimation of QoE scores. Second, we can set
up punishment and reward rules, such as not paying rewards
to participants who provide problematic judgments or giv-
ing more rewards to participants who consistently provide
reliable inputs.
We deﬁne the Transitivity Satisfaction Rate (TSR) to
quantify the consistency of a participant’s judgments over
m rounds in an experiment. The computation of TSR relies
on the transitivity property; that is, if A is preferred over
B, and B is preferred over C, then A should also be pre-
ferred over C by the same participant. Based on this rule,
we compute the TSR as the number of triples that “satisfy”
the transitivity rule divided by the number of triples that
the transitivity rule “may” apply to; thus, the value of the
TSR is always between 0 and 1. The algorithm for comput-
ing the TSR is detailed in Algorithm 1. If a participant’s
judgments are consistent throughout all the rounds of an
experiment, the TSR will be 1; otherwise it will be less than
1. In our experience, if a participant pays full attention to
an experiment, the TSR is usually higher than 0.8.
Algorithm 1 TSR Calculation
m is an n by n matrix, where m[i, j] = 1 indicates that i is
considered better than j; otherwise m[i, j] = 0.
1: n test← 0
2: n pass← 0
3: for all i, j, k, 1 ≤ i, j, k ≤ n, i = j = k do
4: if m[i, j] = 1 and m[j, k] = 1 then
5: n test← n test+ 1
6: if m[i, k] = 1 then
7: n pass← n pass+ 1
8: end if
9: end if
10: end for
11: TSR← n pass/n test
For a crowdsourced experiment, we suggest presenting the
computation rule of the TSR before each experiment and
setting up certain punishment rules for participants who
constantly produce low TSR scores. For example, in our ex-
periments, we only pay a reward if the TSR score is higher
than 0.8. Thus far, we have not received any complaints
about the reward rule, which ensures that resources are not
wasted on untrustworthy experiment results. It also main-
tains the quality of the results, as problematic data is ex-
cluded at the outset.
We believe that there is no systematic way for partici-
pants to cheat our system by inputting “smart” answers.
This is because the presentation order of each pair and the
order within each pair (i.e., which clip corresponds to each
state) are totally random in each experiment; moreover, the
information about the ordering is not available outside the
system. Therefore, the only way for participants to achieve a
high TSR is to pay attention to the diﬀerence in the quality
of states and make judgments that are as consistent as pos-
sible. Of course a participant can still achieve a high TSR
by making consistently “wrong” judgments, i.e., by always
claiming that the state with the lower quality is a better
one; however, such extreme cases can be detected easily by
comparing their choices with those of other participants.
3.3 Overall Consistency Checking
After collecting the paired comparison results from a num-
ber of experiments performed by one or many participants,
we can assess the overall consistency of judgments across dif-
ferent experiments and participants by checking the stochas-
tic transitivity properties or computing Kendall’s u-coeﬃcient.
The stochastic transitivity approach involves checking three
variants of the stochastic transitivity property, namely the
weak (WST), moderate (MST), and strong (SST) stochas-
tic transitivity. Let Pˆij be the empirical probability that the
quality level i is considered better than the quality level j,
the three transitivity variants imply that if Pˆij ≥ 0.5 and
Pˆjk ≥ 0.5, then
Pˆik ≥
⎧⎨
⎩
0.5 (WST),
min{Pˆij , Pˆjk} (MST),
max{Pˆij , Pˆjk} (SST),
for all quality levels Ti, Tj , and Tk. WST is the least restric-
tive of the three properties. Systematic violations of WST
indicate that the paired comparison results from diﬀerent
experiments cannot be integrated into a global preference
ordering. Less severe violations of MST or SST can help
decide whether probabilistic choice modeling is suitable for
analyzing the choice frequencies.
Kendall’s u-coeﬃcient is deﬁned as follows:
u =
2
∑
i=j
(
aij
2
)
(
m
2
)(
n
2
) − 1.
If participants are in complete agreement, there will be
(
n
2
)
elements containing the number m and
(
n
2
)
elements with
zero in the matrix of choice frequencies, so u = 1. As the
number of agreements decreases, u also decreases. The min-
imum agreement occurs when each element is m/2 if m is
even, and (m ± 1)/2 if m is odd; therefore, the minimum
equals −1/(m− 1) if m is even, and −1/m if m is odd.
3.4 Inference of QoE Scores
If the consistency of the collected experiment results is
conﬁrmed, we can proceed to the modeling step and infer
 G722.1  G728
Qo
E 
sc
or
e
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Loss rate
0%
4%
8%
Figure 5: QoE scores of VoIP speeches encoded by diﬀerent
codecs at diﬀerent packet loss rates.
diminishing marginal utility exists; that is, the increase rate
of audio quality declines when the compression bit rate is
higher. This type of utility curve is often seen in the rela-
tionship between a system’s quality and users’ perceptions
because there must be an upper limit, after which increas-
ing the system quality will not enhance users’ experience
any further.
We also ﬁnd that the characteristics of an audio clip may
aﬀect the listening quality even when identical compression
levels are used. It is known that a slower song contains a
larger number of similar pitches than a faster song, so they
tend to have higher compressibility; therefore, an MP3 en-
coder can include more granular information in a ﬁxed-bit-
rate output stream for a slower song. This explains why the
QoE scores of “Garden of Graves” are higher than those of
“Wake Me Up Before You Go Go” at low bit rates. However,
the diﬀerence in the audio quality of the songs is less obvious
at higher bit rates because less information is dropped dur-
ing the compression process, which means that slower songs
no longer beneﬁt from their high compressibility.
4.2 Effect of Packet Loss on VoIP
In this case study, we investigated the eﬀect of the packet
loss rate on VoIP speech quality. Our source clip was a
three-minute speech recording made by concatenating un-
compressed speech segments from the Open Speech Reposi-
tory. We compressed the clip with two speech codecs, G722.1
and G728, into voice packets by using the Intel Integrated
Performance Primitives (IPP) library. Then, we simulated
packet loss events, such as those due to network loss or vari-
able network delays, and decoded the remaining voice pack-
ets into degraded speech recordings. The simulated packet
loss rates were 0%, 4%, and 8%. Because of the combina-
tion of two speech codecs and three loss rates, we obtained
a total of 6 test clips for the QoE evaluation experiments.
A total of 62 participants performed 103 experiments,
which involved 1, 545 paired comparisons. Fig. 5 shows the
QoE scores for the 6 test clips. From the graph, it is clear
that a higher packet loss rate leads to a lower QoE score.
While G722.1 and G728 achieve similar QoE scores when
the loss rate is zero, their robustness to packet loss is sig-
niﬁcantly diﬀerent. Speciﬁcally, the QoE of G722.1 at the
8% loss rate is still much better than that of G728 at the
4% loss rate. The result conforms to our expectation be-
cause G722.1 operates at 32 Kbps, while G728 operates at
16 Kbps. As a result, G722.1 can use a higher encoding bit
rate and a higher sampling rate than G728, and it is also
more robust to packet loss. This case study demonstrates
400 Kbps 800 Kbps
Cheerleaders (Fast)
Qo
E 
sc
or
e
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
H264
WMV3
XVID
400 Kbps 800 Kbps
Mobile Calendar (Slow)
Qo
E 
sc
or
e
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
H264
WMV3
XVID
Figure 6: QoE scores of video clips compressed by diﬀerent
codecs at diﬀerent bit rates.
the eﬀectiveness of our framework in VoIP speech quality
assessment studies.
5. CASE STUDIES: OPTICAL QOE EVAL-
UATION
In this section, we present two case studies based on our
experiment design for optical QoE evaluations (cf. Sec-
tion 3.1). The ﬁrst evaluates the QoE of video clips com-
pressed by diﬀerent codecs with comparable bit rates; and
the second considers the impact of two loss concealment
schemes during playout of IPTV videos. Here, we focus
on the QoE evaluation results, which were inferred from the
combined frequency choices of participants from the three
sources, and their implications in each case study. We ex-
plain how the experiments were crowdsourced and then com-
pare the performances of the laboratory and crowdsourced
experiments in Section 6.
5.1 Video Codec
In this case study, we assess the impact of codecs and com-
pression levels on the QoE of video clips. From the video
database of the Video Quality Experts Group (VQEG), we
selected two 12-second raw video clips, the fast-motion “Cheer-
leaders” and the slow-motion “Mobile Calendar.” We com-
pressed both source clips with three codecs, H.264, WMV3,
and XVID, at two bit rates, 400 Kbps and 800 Kbps respec-
tively. Therefore, for each source clip, we obtained 6 test
clips with diﬀerent codec-and-bit-rate combinations.
A total of 121 participants, both part-timer employees
and Internet volunteers, performed 223 experiments that
involved 3, 345 paired comparisons. Fig. 6 shows the QoE
score of each test clip for both “Cheerleaders” and “Mobile
Calendar.” Generally, the quality of 800-Kbps clips is much
better than that of 400-Kbps clips because more informa-
tion is encoded in the output video clips. If we consider
the “Cheerleaders” video, we ﬁnd that H264 performs bet-
ter than WMV3, which is better than XVID at 400 Kbps;
however, the quality of WMV3 and XVID is comparable at
Table 2: A comparison of laboratory and crowdsourced experiments in terms of their cost and performance
Case Study Participant
Source
Total
Cost
(dollar)
#
Rounds
#
Person
Qualiﬁed
Rate
Cost /
Round
(cent)
Time /
Round
(sec)
Avg.
TSR
WST
Vio-
lation
MST
Vio-
lation
SST
Vio-
lation
Kendall
MP3
Bit Rate
Laboratory 50.97 1, 440 10 67% 3.54 16 0.96 0 0 0.05 0.65
MTurk 7.50 750 24 47% 1.00 9 0.96 0 0.05 0.20 0.58
Community 1.03 1, 470 93 54% 0.07 25 0.96 0 0 0.30 0.58
VoIP
Quality
Laboratory 22.95 675 10 67% 3.40 16 0.98 0 0 0.05 0.78
MTurk 3.00 300 15 74% 1.00 19 0.98 0 0 0.20 0.78
Community 0.40 570 37 86% 0.07 24 0.98 0 0 0.10 0.85
Video Codec
Laboratory 23.73 1, 500 10 80% 1.58 7 0.98 0 0 0.15 0.57
MTurk 4.95 495 23 65% 1.00 17 0.98 0 0 0.20 0.68
Community 0.95 1, 350 88 71% 0.07 11 0.97 0 0 0.25 0.57
Loss
Concealment
Laboratory 51.93 1, 260 11 69% 4.12 19 0.96 0 0 0.30 0.60
MTurk 5.85 585 21 36% 1.00 25 0.97 0 0 0.40 0.60
Community 0.63 900 59 35% 0.07 21 0.96 0 0 0.25 0.52
Overall 173.88 11, 295 298 59% 1.54 17 0.97 0 0.01 0.20 0.65
experiments from the three sources and found that it was
above 0.95. The statistical transitivity checks show that
no WST violations occurred in any of the data sets, and
only 1 in 20 MST checks failed for the MTurk experiments
in one of the case studies. SST violations occurred in all
case studies, but the numbers of such violations were mod-
erate. We remark that the laboratory experiments had the
fewest SST violations. This is reasonable because statis-
tical transitivity checks assess the consistency of judgments
among diﬀerent participants and the laboratory experiments
involved the fewest participants. In all the case studies, the
Kendall-u parameters were higher than 0.5, which indicates
that the judgments provided by all three sources were rea-
sonably consistent.
Cost. The laboratory experiments accounted for 86% of
the total monetary cost. Since the number of experiments
performed by participants in each source was diﬀerent, we
compare the cost of the sources in terms of the cost per
round, as shown in Table 2. The cost per round was not a
constant price in laboratory experiments because the part-
time employees were paid an hourly rate, but the number
of experiments they performed varied. On average, the cost
per round of laboratory experiments was 3 cents; and for the
crowdsourced MTurk and community experiments it was 1
and 0.07 cents respectively, which yields a ratio of 3 : 1 and
43 : 1 respectively.
Participant diversity. The diversity of participants in
QoE evaluation experiments is important. Since the pur-
pose of such experiments is to understand people’s percep-
tions of certain stimuli, such as multimedia content, a more
diverse set of experiment participants enables us to collect
a broader range of opinions. From this perspective, crowd-
sourcing is obviously a more appropriate strategy for QoE
experiments because it increases participant diversity sub-
stantially. Quantitatively, the crowdsourced experiments ac-
counted for only 14% of the total cost, but they accounted
for 289 out of the 298 participants (97%) in our case studies.
Limitations
Although we have demonstrated the advantages of crowd-
sourcing in QoE evaluation experiments, the strategy has
several limitations that may aﬀect its applicability in cer-
tain types of evaluations.
Environment control. In crowdsourced experiments,
the subjects might listen to or view the presented media
content under diﬀerent conditions, such as lighting, screen
size, and the quality of headsets; however, in a laboratory,
the experiments can be conducted under a common, con-
trolled environment. This could be considered as an advan-
tage or a disadvantage. It is an advantage because users’
perceptions can be assessed in real-life scenarios. In prac-
tice, users’ headsets may be not as good as those in labora-
tories, and ambient sound may be unavoidable. While it is
diﬃcult to simulate a “typical” user environment in a lab-
oratory, crowdsourcing allows us to assess people’s real-life
experiences. On the other hand, it can be a disadvantage
if the purpose of experiments is to accurately measure the
quality of multimedia content in a speciﬁc scenario.
Media. Since most people connect to the Internet via
personal computers, the crowdsourcing strategy is most suit-
able for evaluating the media content on personal comput-
ers. It could be a problem if evaluations are conducted on
other input/output devices, such as TV or electronic papers.
Fortunately, those non-PC devices are gradually becoming
Internet-capable, so this problem may be resolved in the
near future.
Demography. As any Internet user can participate in
crowdsourced experiments, it is diﬃcult, if not impossible,
to relate the experiment results to demographic factors, such
as gender and age. For example, we cannot investigate the
eﬀect of age on the perceptions of certain colors in video
clips, because the ages self-reported by the participants may
not be trustworthy.
Even though the crowdsourcing strategy has the above
limitations, it can still be used to evaluate the QoE of mul-
timedia content in a variety of applications. We believe that
it is especially helpful for assessing the eﬀect of techniques
related to coding, processing, and transmission of media con-
tent, such as the case studies we conducted in this work.
Moreover, with the rapid advent of technologies on rich user
interface, such as Flash, the framework can be extended to
assess users’ experiences in interactive applications like com-
puter games, which will also be part of our future work.
7. CONCLUSION AND FUTURE WORK
In this work, we have proposed and evaluated a crowd-
sourceable framework for assessing the QoE of multimedia
content. We have shown that, under our framework, re-
searchers can outsource QoE evaluation experiments to an
An Empirical Evaluation of VoIP Playout Buffer
Dimensioning in Skype, Google Talk, and MSN Messenger ∗
Chen-Chi Wu1, Kuan-Ta Chen2, Chun-Ying Huang3, and Chin-Laung Lei1
1Department of Electrical Engineering, National Taiwan University
2Institute of Information Science, Academia Sinica
3Department of Computer Science and Engineering, National Taiwan Ocean University
ABSTRACT
VoIP playout buffer dimensioning has long been a challeng-
ing optimization problem, as the buffer size must maintain
a balance between conversational interactivity and speech
quality. The conversational quality may be affected by a
number of factors, some of which may change over time.
Although a great deal of research effort has been expended
in trying to solve the problem, how the research results are
applied in practice is unclear.
In this paper, we investigate the playout buffer dimension-
ing algorithms applied in three popular VoIP applications,
namely, Skype, Google Talk, and MSN Messenger. We con-
duct experiments to assess how the applications adjust their
playout buffer sizes. Using an objective QoE (Quality of
Experience) metric, we show that Google Talk and MSN
Messenger do not adjust their respective buffer sizes ap-
propriately, while Skype does not adjust its buffer at all.
In other words, they could provide better QoE to users by
improving their buffer dimensioning algorithms. Moreover,
none of the applications adapts its buffer size to the network
loss rate, which should also be considered to ensure optimal
QoE provisioning.
Categories and Subject Descriptors
C.4 [Performance of Systems]: Measurement techniques;
H.4.3 [Information Systems Applications]: Communi-
cations Applications—Computer conferencing, teleconferenc-
ing, and videoconferencing ; H.1.2 [Models and Princi-
ples]: User/Machine Systems—Human factors
General Terms
Human Factors, Measurement, Performance
∗This work was supported in part by Taiwan Information
Security Center (TWISC), National Science Council under
the grants NSC97-2219-E-001-001 and NSC97-2219-E-011-
006. It was also supported in part by the National Science
Council of Taiwan under the grant NSC96-2628-E-001-027-
MY3 and NSC97-2218-E-019-004-MY2.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
NOSSDAV’09, June 3–5, 2009, Williamsburg, Virginia, USA.
Copyright 2009 ACM 978-1-60558-433-1/09/06 ...$5.00.
Keywords
E-Model, MOS, PESQ, Quality of Experience, User Satis-
faction, VoIP
1. INTRODUCTION
VoIP is becoming an important communication service
both within and between enterprises, while individuals are
relying on it increasingly for daily communications with fam-
ily and friends. There are two reasons for this phenomenon:
the cost of VoIP calls is low and the voice quality is al-
most the same as that of traditional toll telephones. The
trend is exemplified by the fact that Skype, one of the most
widely used VoIP applications, has 405 million registrars
and 15 million online users1. Because of the steady growth
in VoIP usage, providing reliable services with satisfactory
voice quality is now a high priority for Internet and VoIP
service providers.
A number of factors may affect the service quality of VoIP,
e.g., the speech codec, transport protocol, redundancy/error
control, network path selection, and playout buffer dimen-
sioning. In this work, we focus on the playout buffer dimen-
sioning algorithms employed by popular VoIP applications.
Basically, playout buffering2 sacrifices conversational in-
teractivity in exchange for better voice quality. Normally, a
voice packet is transmitted from the speaker’s node to the
listener’s node every 20 ms or 30 ms to maintain continu-
ous and smooth speech conversation. However, in packet-
switched networks, packet queuing delays are variable and
hard to predict, so some packets may arrive at the listener’s
node after long delays and the speech samples in the pack-
ets will be considered lost. This may result in silent pe-
riods, noise, or unclear speech, depending on the loss con-
cealment algorithm adopted by the voice codec. To reduce
the frequency of such occurrences, a playout buffer can be
employed to hold a VoIP packet until its scheduled playout
time. By so doing, packets that experience slightly longer
network delays can still be used as long as they arrive at the
listener’s node ahead of their respective scheduled playout
time.
The most challenging issue raised by VoIP playout buffer-
ing is how to determine the most appropriate buffer size for
current network conditions. Generally, a larger buffer size
leads to better sound quality, but it reduces conversational
1
http://ebayinkblog.com/wp-content/uploads/2009/01/
skype-fast-facts-q4-08.pdf
2
Since we only playout buffer in this paper, we use “playout buffer”
and “buffer” interchangeably hereafter.
Recorder
Listener
Speaker
FreeBSD w/ 
dummynet
Audio output
Audio output
Figure 1: The experiment setup
In this section, we describe the experiment setup and the
procedures for evaluating the playout buffer size of Skype,
Google Talk, and MSN Messenger under various network
scenarios.
3.1 Experiment Setup
To evaluate the playout buffer sizes of the selected VoIP
applications under different network conditions, we set up a
FreeBSD 7.0 machine as a router and controlled the pace
of traffic flows passing through it by dummynet.Two Mi-
crosoft Windows XP PCs installed with Skype (version 3.8),
Google Talk (version 1.0), and MSN Messenger (version
2009, build 14.0) are connected to each other and to the
Internet through the FreeBSD router. In the experiment,
we designate one PC as the speaker, and the other as the
listener. A speech recording is played continuously on the
speaker PC, and via VoIP transmission, the listener PC re-
ceives a degraded copy of the content and outputs it as a
degraded speech segment. To simulate real-life human con-
versations, we use speech recordings obtained from the Open
Speech Repository [3].
During the experiment, we use a PC with an ESI Maya44
recording card to record the audio output of the speaker
(i.e., the original speech segment), and that of the listener
(i.e., the degraded speech segment) in a stereo wave file.
The recording machine stores the speaker’s output in the
left channel, and the listener’s output in the right channel
of the wave file. The setup of the router, call parties, and
recorder is illustrated in Fig. 1.
By configuring dummynet on the router, we can control the
network delay, delay jitter (the standard deviation of the
network delay), and the loss rate between the speaker PC
and the listener PC. As every packet sent from the speaker
must pass through the router to reach the listener, we can
examine how the applications’ playout buffer sizes change
under different network conditions.
In the experiments, we set the network delay and delay
jitter between 0 ms and 200 ms with a 25 ms interval, and
the packet loss rate between 0% and 10% with a 1% interval.
The duration of each VoIP call was 240 seconds, and 10 calls
were made under each network setting. To allow sufficient
time for the VoIP applications to adapt their playout buffer
sizes to the latest network conditions, we started the wave
file recording 60 seconds after a call had been established.
We assume that the delay jitters follow a Gamma distribu-
tion. In addition, as the effect of restricted bandwidth can
be simulated by injecting packet loss, the network band-
width between the two call parties is set to a sufficiently
large value, i.e., 1000 Kbps.
3.2 Buffer Size Estimation
To estimate the size of the playout buffers in the compared
VoIP applications, we need to determine the end-to-end de-
lay between the time the speaker starts speaking and the
time the listener hears the spoken segment. The end-to-
end delay can be estimated by computing the delay between
the speaker’s audio output and the listener’s audio output,
which are stored in the wave files. Therefore, we calculate
the audio delay by searching for the time difference that
yields the largest cross-correlation coefficient between the
speech recordings output by the two parties [1].
In addition to the playout buffer delay, the end-to-end
delay of VoIP transmission comprises network delay, coder
delay, and packetization delay [4]. Since both call parties
are in a LAN, all the network delay components can be con-
trolled. In fact, the propagation delay and transmission de-
lay are very small so they can be neglected. The coder delay
and packetization delay are both application-dependent and
codec-dependent; thus, we do not have exact information
about delays caused by these components. However, a sur-
vey of the typical values used by popular codecs [4] shows
that the sum of the coder delay and packetization delay is
usually around 50 ms. Therefore, we estimate an applica-
tion’s playout buffer size by reducing the measured end-to-
end delay by 50 ms and the average network delay induced
bydummynet. Although the estimate may not be accurate,
our objective is to determine how the compared VoIP ap-
plications adjust their playout buffer sizes under different
network conditions. Thus, the absolute error in estimating
the buffer size does not affect the buffer dimensioning be-
havior we observe or the conclusions we draw.
4. PLAYOUT BUFFER SIZE ADJUSTMENT
IN REAL-LIFE APPLICATIONS
In this section, we discuss how Skype, Google Talk, and
MSN Messenger adjust their respective VoIP playout buffer
sizes under various network conditions.
4.1 Effect of Network Delay and Delay Jitter
The graph in Fig. 2 plots the VoIP playout buffer sizes of
Skype, Google Talk, and MSN Messenger when VoIP pack-
ets experience different levels of network delay and delay
jitter. The vertical bars on the graph represent the 95%
confidence band of the average buffer size. Figures 2(a) and
(c) show that the curves corresponding to different delays
are similar, and the 95% confidence bands merge with each
other. This finding indicates that Skype and MSN Messen-
ger do not adjust their playout buffer sizes to compensate for
the average network delay. On the other hand, the dissimi-
larity between the curves in Fig. 2(b) shows that Google Talk
considers the average network delay and adapts its playout
buffer size accordingly.
We also investigated the impact of delay jitter on the
buffer size. As shown in Fig. 2(a), Skype’s buffer size re-
mains within the range (250, 300) ms regardless of the mag-
nitude of the delay jitter, which suggests that Skype does not
adjust its buffer size in response to network delays. In con-
trast, both Google Talk and MSN Messenger increase their
buffer sizes as the delay jitter increases. This design allows
packets that experience longer queuing delays more oppor-
tunities to arrive and be used before the scheduled playout
0 200 400 600 800
1.
0
2.
0
3.
0
4.
0
Delay 50 ms
Playout buffer size (ms)
M
O
S
x
x
Delay jitter
25 ms
50 ms
0 200 400 600 800
1.
0
2.
0
3.
0
4.
0
Delay 100 ms
Playout buffer size (ms)
M
O
S
x
x
x
x
Delay jitter
25 ms
50 ms
75 ms
100 ms
0 200 400 600 800
1.
0
2.
0
3.
0
4.
0
Delay 150 ms
Playout buffer size (ms)
M
O
S
x
x
x
x
Delay jitter
25 ms
50 ms
100 ms
150 ms
0 200 400 600 800
1.
0
2.
0
3.
0
4.
0
Delay 200 ms
Playout buffer size (ms)
M
O
S x
x
x
x
Delay jitter
50 ms
100 ms
150 ms
200 ms
Figure 4: The simulation result of inferring the op-
timal buffer sizes for different network delays and
delay jitters
vided by a VoIP application. The major advantage of this
model is that it combines the accurate listening quality as-
sessment of PESQ and the interactivity assessment of E-
model. Given an original audio clip and its degraded version,
we compute the MOS score by the following procedures:
1. Apply PESQ to the original and degraded audio clips,
and convert the resulting MOS score to an R score
using the formula in ITU-T G.107, Appendix I [7].
2. Compute the delay impairment Id in the E-Model based
on the network delay. Other parameters of the E-
Model remain unchanged.
3. Subtract the R score derived in step 1 from Id obtained
in step 2, and convert the resulting R score into a MOS
score by Equation 1.
5.2 Optimal Buffer Size Derivation
The optimal playout buffer size yields the highest user
satisfaction in a VoIP call. To derive the optimal buffer size
under a given network condition, we designed a simulator
that evaluates the quality of a VoIP conversation given the
network configuration and the playout buffer size. Based on
the VoIP QoE measurement model, we define the optimal
playout buffer size as the size that yields the highest MOS
score. The steps for determining the optimal buffer size are
as follows:
1. Encode an audio clip into a sequence of VoIP frames
by using the encoder library in the Intel Integrated
Performance Primitives Library [2].
2. Simulate network packet loss with the Gilbert model.
A packet will be dropped if the model is in the “Error”
state; otherwise, it will be retained.
3. Introduce network delay to each packet via a Gamma
distribution. If a packet’s delay is longer than the cur-
rent playout buffer size, it will be dropped; otherwise,
it will be retained.
4. Decode the resulting stream of frames into a degraded
audio clip.
5. Apply the VoIP QoE measurement model with the re-
quired inputs, i.e., the network delay together with the
0 100 200 300 400 500
1.
0
2.
0
3.
0
4.
0
Delay jitter 0 ms
Playout buffer size (ms)
M
O
S
x
x
x
x
x
Loss rate
0
0.01
0.02
0.03
0.04
0 100 200 300 400 500
1.
0
2.
0
3.
0
4.
0
Delay jitter 25 ms
Playout buffer size (ms)
M
O
S
x
x
x
xx
0 100 200 300 400 500
1.
0
2.
0
3.
0
4.
0
Delay jitter 50 ms
Playout buffer size (ms)
M
O
S
x
x
xxx
0 100 200 300 400 500
1.
0
2.
0
3.
0
4.
0
Delay jitter 75 ms
Playout buffer size (ms)
M
O
S x
x
xxx
Figure 5: The simulation result of inferring the op-
timal buffer sizes for different network loss rate
original and degraded audio clips, to derive the MOS
score.
In our simulations, we use G.711, the most widely used
codec in digital speech applications. We also employ the set
of speech recordings [3] that we used to estimate the buffer
sizes in the compared VoIP applications.
Simulations were conducted to observe the impact of dif-
ferent playout buffer sizes on the MOS scores with different
network delays and delay jitters, as shown in Fig. 4. The
MOS score varies significantly as the buffer size increases
from 0 ms to 800 ms, which demonstrates the importance of
using an effective buffer dimensioning algorithm to improve
VoIP conversation quality. We define the buffer size with
the highest MOS score as the optimal buffer size and anno-
tate it with a check mark on the graphs. For example, the
optimal buffer size is 100 ms when the delay and delay jitter
are 50 ms and 25 ms respectively. The figure shows that
as the delay jitter increases, a larger buffer size is normally
required to provide the best QoE to users. However, an un-
reasonably large buffer size may degrade the overall quality
because a long buffering delay will affect the conversational
interactivity.
We also performed simulations to determine the optimal
VoIP buffer sizes with different delay jitters and packet loss
rates. All the simulations were run with the network delay
set to 100 ms. From the results plotted in Fig. 5, we observe
that when the delay jitter is small, network loss rates do not
affect the optimal buffer size significantly. However, when
the delay jitter is large, a higher packet loss rate may lead
to a shorter optimal buffer size. We believe the reason is
that increasing the buffer size does not allow more packets
to arrive and be used when the network loss rate is high.
Instead, by reducing the buffer size in order to increase the
conversation interactivity, the overall QoE can be enhanced.
However, this behavior can be changed if a redundancy con-
trol algorithm is introduced [6], so that a VoIP frame can
be transmitted several times to cope with high packet loss
rates. We will investigate the impact of redundancy control
on the optimal playout buffer in a future work.
5.3 Evaluation of Buffer Dimensioning Algo-
rithms in Real-Life Applications
Peer-to-Peer Application Recognition Based on
Signaling Activity
Chen-Chi Wu†, Kuan-Ta Chen‡, Yu-Chun Chang†, and Chin-Laung Lei†
†Department of Electrical Engineering, National Taiwan University
‡Institute of Information Science, Academia Sinica
{bipa,congo}@fractal.ee.ntu.edu.tw, ktchen@iis.sinica.edu.tw, lei@cc.ee.ntu.edu.tw
Abstract—Because of the enormous growth in the number of
peer-to-peer (P2P) applications in recent years, P2P traffic now
constitutes a substantial proportion of Internet traffic. The ability
to accurately identify different P2P applications from the network
traffic is essential for managing a number of network traffic
issues, such as service differentiation and capacity planning. How-
ever, modern P2P applications often use proprietary protocols,
dynamic port numbers, and packet encryptions, which make
traditional identification approaches like port-based or signature-
based identification less effective.
In this paper, we propose an approach for accurately recog-
nizing P2P applications running on monitored hosts based on
signaling behavior, which is regulated by the underlying P2P
protocol; therefore, each application possesses a distinguishing
characteristic. We consider that the signaling behavior of each
P2P application can serve as a unique signature for application
identification. Our approach is particularly useful for three
reasons: 1) it does not need to access the packet payload;
2) it recognizes applications based purely on their signaling
behavior; and 3) it can identify particular P2P applications. The
performance evaluation shows that 92% of a real-life traffic trace
can be correctly recognized within a 5-minute monitoring period.
Index Terms—Application identification, BitTorrent, Skype,
Support vector machine, Traffic classification
I. INTRODUCTION
Peer-to-peer (P2P) traffic now constitutes a substantial pro-
portion of Internet traffic. The ability to identify different P2P
applications from the network traffic is important for network
management functions, such as service differentiation, capac-
ity planning, and QoS provisioning. For example, network
administrators limit or block P2P traffic that occupies a high
amount of bandwidth to ensure that other applications have
sufficient bandwidth. Another issue is that most content shared
in P2P networks infringes copyright laws.
Managing P2P traffic is a major challenge for network
administrators because P2P applications tend to use dynamic
port numbers and proprietary protocols. In this paper, we
propose a model that can recognize particular P2P appli-
cations running on the monitored host without examining
packet payloads. The key to our approach is recognizing the
signaling behavior of a P2P application. Although a P2P
application can easily change its port number, payload, and
even message format, the signaling patterns between peers
This work was supported in part by Taiwan Information Security Center
(TWISC), National Science Council under the grants NSC97-2219-E-001-001
and NSC97-2219-E-011-006. It was also supported in part by the National
Science Council of Taiwan under the grant NSC96-2628-E-001-027-MY3.
are more fundamental and are therefore unlikely to change.
For example, a BitTorrent client needs to regularly exchange
the file bitmaps containing the block status of its files with
neighboring peers. This signaling behavior is essential for
the maintenance of the BitTorrent network, so changing it
would lead to state inconsistency and software incompatibility
problems. Hence, it would be difficult for a P2P application
to change its signaling behavior without affecting its normal
operations. Moreover, the signaling behavior is regulated by
the underlying P2P protocol, so each application possesses
distinctive features. We consider that the signaling behavior
of each P2P application can serve as a unique signature for
application identification.
The contribution of our approach is threefold:
• It recognizes P2P applications based on their unique
signaling behavior, rather than by examining the packet
payload. Since this behavior is relatively difficult to
change compared to the port numbers or packet format,
it is unlikely that an application will be able to evade
recognition.
• It only needs to examine traffic associated with the
monitored host; in other words, a global view of the
network is not necessary.
• It can recognize particular applications running on the
monitored host, so it does not treat all P2P traffic in the
same way.
The remainder of the paper is organized as follows. In
Section II, we review related works. We describe the data
collection methodology and summarize our traces in Sec-
tion III. In Section IV, we discuss the fundamental concepts
behind our approach. We present a detailed description of our
scheme in Section V and evaluate the scheme’s performance
in Section VI. Then, in Section VII, we summarize our
conclusions.
II. RELATED WORK
In recent years, a number of works have addressed the issue
of P2P traffic identification. Early approaches relied on the
port numbers used by applications [10], but the estimates are
now regarded as misleading because P2P applications may use
dynamic ports or the default ports of other applications (e.g.,
port 80 or 443).
The application-layer approach identifies a protocol-specific
signature by examining the packet payload [2, 9]. This ap-
0 20 40 60 80 100 120
0
20
0
60
0
10
00
(a) BitTorrent
Elapsed time (min.)
N
um
be
r o
f h
os
ts
0 20 40 60 80 100 120
0
50
10
0
15
0
20
0
(b) eMule
Elapsed time (min.)
N
um
be
r o
f h
os
ts
0 20 40 60 80 100 120
0
5
10
15
20
25
30
(c) Skype
Elapsed time (min.)
N
um
be
r o
f h
os
ts
Old hosts
New hosts
Fig. 1. The signaling traffic statistics: the number of old hosts and new hosts contacted by the monitored hosts.
the monitored host contacts a large number of hosts initially,
but the number of hosts decreases dramatically after about 10
minutes and continues to fluctuate over time. Fig. 1(c) shows
that the Skype host contacts a dozen hosts during the login
process and contacts a near constant number of hosts after 60
minutes.
Next, we analyze the ratio of new hosts to all hosts contacted
by the monitored host. Compared to the eMule and Skype
hosts, we observe that the BitTorrent host has a high ratio
of new hosts most of the time. On the other hand, the ratio
of new hosts is only high in the early stage for eMule; and
the Skype host only has a high ratio of new hosts during the
login process. The above preliminary analysis evidences that
BitTorrent clients may continuously seek new hosts, whereas
eMule and Skype clients tend to contact old hosts.
Based on the above analysis, we show that different P2P
applications exhibit very different signaling behavior in terms
of the number of hosts and the ratio of new and old hosts.
B. Signaling Patterns
In Figure 2, we plot the signaling patterns of BitTorrent,
eMule, and Skype in a 1-hour period. We assign numeric
identifiers to hosts that have been contacted by the monitored
host based on the order in which they are observed, and use the
sign of the identifiers to denote the direction of the signaling
traffic. A positive identifier indicates a packet sent from the
monitored host to the peer host, while a negative identifier
indicates a packet sent from the peer host to the monitored
host. Each dot in Figure 2 represents a signaling packet sent
from or received by the monitored host. An intensive exchange
of signaling packets is depicted by a high distribution of dots
or even an area of solid color in this figure. In the following,
we discuss the unique characteristics of each application.
BitTorrent: The high dot density in Fig. 2(a) implies an
intensive exchange of signaling packets between the monitored
BitTorrent client and its peer hosts. Moreover, the near linear
growth of the number of peer hosts shows how a BitTorrent
client progressively discovered new hosts during the monitor-
ing period.
eMule: As shown in Fig. 2(b), the number of signaling
packets sent from and received by the eMule client is much
fewer than those of the BitTorrent client. We also observe
that the number of peer hosts increases rapidly in the first 10
minutes, but it increases slowly thereafter.
0 10 20 30 40 50 60
−
15
00
0
0
10
00
0
(a) BitTorrent
Elapsed time (min.)
H
os
t I
D
0 10 20 30 40 50 60
−
15
00
0
10
00
(b) eMule
Elapsed time (min.)
H
os
t I
D
0 10 20 30 40 50 60
−
10
0
0
50
10
0
(c) Skype
Elapsed time (min.)
H
os
t I
D
Fig. 2. The signaling activity patterns of BitTorrent, eMule, and Skype.
Skype: The sparse distribution of dots in Fig. 2(c) suggests
that most of the signaling packets belonged to probe traffic,
which is a pair of packets comprised of a single probe packet
and the corresponding reply packet [3], i.e., a packet with a
host ID X is coupled with another packet with the host ID
-X .
The above graphical comparisons show that each P2P appli-
cation possesses a number of unique signaling characteristics.
In the next section, we utilize the distinctiveness of each
application’s signaling patterns to develop a P2P application
recognition scheme.
V. THE PROPOSED SCHEME
In this section, we propose a P2P application identification
scheme, which is based on the signaling traffic associated with
the monitored host . First, we explain how we characterize the
signaling behavior and how we derive features from signaling
packet streams. We then exploit the features to design a
classifier for recognizing individual P2P applications.
Overall BT eMule Skype Non−P2P
 
Ac
cu
ra
cy
 / 
Tr
ue
 p
os
itiv
e 
ra
te
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Fig. 5. The overall accuracy and true positive rate of each category for the
traces in Set 2.
our classifier achieves over 96% accuracy in a 5-minute
monitoring period and 98% accuracy in an 8-minute period. It
achieves a true positive rate of 90% for each application with
a monitoring period longer than 6 minutes. We also observe
that the true positive rate of the non-P2P category is nearly
99%, which means our scheme seldom misidentifies a non-P2P
sample as a P2P sample. Furthermore, the false positive rate
of each category is lower than 5%. For an overall evaluation
of the traces in Set 1, we plot the ROC curves for each P2P
application, as shown in Fig. 4(b). The ROC curve of each
application depicts its performance by treating the application
as the positive class and all other applications as the negative
class. We observe that each ROC curve in Fig. 4(b) passes
through the upper left corner of the plot (i.e., the true positive
rate is close to 1 with a small false positive rate). Therefore,
these curves evidence that our proposed scheme can recognize
each application with a high true positive rate and an extremely
low false positive rate.
Next, we use the classification result of an application-layer
classifier as a reference point to evaluate the performance
of our scheme based on the traces in Set 2. Based on
the application-layer classification result, unknown flows are
excluded from the data set. We also perform 10-fold cross
validation and set the monitoring period to 5 minutes. As
shown in Fig 5, our classifier achieves 92% overall accuracy
and the true positive rate of each category is over 90%. The
above identification results for both data sets demonstrate
that our scheme can recognize P2P applications running on
monitored hosts with a high degree of accuracy.
VII. CONCLUSION AND FUTURE WORK
In this paper, we propose a scheme for recognizing P2P
applications based on the signaling behavior of the applica-
tions. Our approach is particularly useful in two respects.
First, it does not need to access the packet payload or rely
on port numbers. It only needs to analyze the signaling traffic
associated with the monitored host. Second, it can recognize
particular P2P applications running on the monitored host.
By analyzing the signaling behavior at the host and message
levels, we show that 92% of a real-life trace can be correctly
recognized in a 5-minute monitoring period.
Although the evaluation results are promising, we will
improve the following two aspects of our scheme in a future
work.
Launching multiple P2P applications on a host. When a
host launches multiple P2P applications, all signaling traffic is
combined. As a result, the proposed scheme cannot recognize
particular applications based purely on observations of all the
signaling traffic of the host. To solve this problem, we are
working on demultiplexing traffic that consists of signaling
packets generated by more than one P2P application. Based on
the port numbers used by each traffic flow, we gather flows that
use related port numbers into a group. We assume that each
group of traffic flows only contains signaling traffic generated
by a single application. With this heuristic, our scheme can
recognize individual applications by analyzing each group of
traffic flows.
Short flows. Since the monitoring period of the proposed
scheme is at least 5 minutes, we are unable to correctly detect
P2P applications with signaling traffic that is shorter than 5
minutes. For example, a user may make a phone call that lasts
less than 5 minutes and sign out immediately after finishing
the call. In this case, we cannot collect enough signaling traffic
to derive the signaling features. In our future work, we will
devise other powerful features to characterize the signaling
behavior of short P2P sessions.
REFERENCES
[1] “L7-filter Supported Protocols,” http://l7-
filter.sourceforge.net/protocols/.
[2] H. Bleul, E. P. Rathgeb, and S. Zilling, “Evaluation of an
efficient measurement concept for p2p multiprotocol traffic
analysis,” in EUROMICRO ’06: Proceedings of the 32nd EU-
ROMICRO Conference on Software Engineering and Advanced
Applications. Cavtat, Croatia: IEEE Computer Society, 2006,
pp. 414–423.
[3] D. Bonfiglio, M. Mellia, M. Meo, N. Ritacca, and D. Rossi,
“Tracking down skype traffic,” in Proceedings of IEEE INFO-
COM’08, Phoenix, AZ, USA, 2008, pp. 15–17.
[4] F. Constantinou and P. Mavrommatis, “Identifying known and
unknown peer-to-peer traffic,” in NCA ’06: Proceedings of the
Fifth IEEE International Symposium on Network Computing
and Applications, Cambridge, MA, USA, 2006, pp. 93–102.
[5] T. Karagiannis, A. Broido, M. Faloutsos, and K. claffy, “Trans-
port layer identification of p2p traffic,” in IMC ’04: Proceedings
of the 4th ACM SIGCOMM conference on Internet measure-
ment, Taormina, Sicily, Italy, 2004, pp. 121–134.
[6] T. Karagiannis, K. Papagiannaki, and M. Faloutsos, “Blinc:
multilevel traffic classification in the dark,” in Proceedings of
ACM SIGCOMM’05, Philadelphia, Pennsylvania, USA, 2005,
pp. 229–240.
[7] X. Lu, H. Duan, and X. Li, “Identification of p2p traffic based
on the content redistribution characteristic,” in ISCIT’07: Pro-
ceedings of the International Symposium on Communications
and Information Technologies, Sydney, Australia, 2007, pp.
596–601.
[8] M. Perenyi, A. G. Trang Dinh Dang, and S. Molnar, “Identifi-
cation and analysis of peer-to-peer traffic,” Journal of Commu-
nication, vol. 1, no. 7, pp. 36–46, 2006.
[9] S. Sen, O. Spatscheck, and D. Wang, “Accurate, scalable in-
network identification of p2p traffic using application signa-
tures,” in WWW ’04: Proceedings of the 13th international
conference on World Wide Web, New York, NY, USA, 2004,
pp. 512–521.
[10] S. Sen and J. Wang, “Analyzing peer-to-peer traffic across large
networks,” IEEE/ACM Transactions on Networking (TON),
vol. 12, no. 2, pp. 219–232, 2004.
2be given immediately. Therefore, as subjects do not need
to pay much attention to the scoring procedure, their
flow experience in using the tested application can be
maintained.
2) It is lightweight because it does not need large-scale
or expensive deployments to perform experiments. In
fact, the only extra equipment needed is the hardware
required to run the tested application. For example, to
evaluate users’ perceptions about video quality, any PC
that is capable of playing video clips can be used to
perform the experiment.
3) It is efficient because it captures a large number of click
events from a subject in each test. Our experiment results
show that a single 5-minute test with one subject is
normally sufficient to reveal his/her perceptions about
audio or video quality over a wide spectrum of network
conditions.
4) It is time-aware because users can provide feedback
any time during a test. Thus, the measurement results
reveal how a subject’s perception changes over time. For
example, our method can be used in video quality tests
to evaluate if a subject tends to adapt to the jerky screen
updates due to network lags over time.
5) It is application-independent because it relies purely
on users’ feedback rather than any application-specific
design. Thus, it can be used to measure users’ per-
ceptions of many types of network applications. In
Section IV and Section V, we show that OneClick
is capable of evaluating the quality of audio streaming
tools, compressed video clips, and interactive online
games.
To demonstrate the efficacy of our approach, we select
the three most well-known instant messaging (IM) appli-
cations, Skype, MSN Messenger, and AIM Messenger, and
use OneClick to evaluate their audio and video quality in
various network configurations. The results show that, from
the user’s perspective, Skype provides the best quality over
a wide range of network loss rates and bandwidth. We also
evaluate user experience in two first-person shooter games,
Unreal Tournament and Halo. The results show that Unreal
Tournament can provide better game play experience than
Halo under conditions of moderate network delay and delay
variations.
The contribution of this paper is three-fold. 1) To quantify
users’ perceptions of network applications, we propose the
OneClick framework which is intuitive, lightweight, effi-
cient, time-aware, and application-independent. 2) We apply
the proposed framework to evaluate the quality of three popu-
lar IM applications and two first-person shooter games at very
low cost. This manifests that OneClick is an application-
independent and effective way of evaluating the quality of net-
work applications. 3) We provide an online implementation of
OneClick at http://mmnet.iis.sinica.edu.tw/oneclick for col-
laborative studies. Any one can contribute his/her perceptions
of the quality of the provided audio clips. The contributions
will be merged to form a composite perception for future QoE
studies.
The remainder of this paper is organized as follows. Sec-
tion II contains a review of related work. In Section III, we
present the basic methodology of the OneClick framework
and a revised methodology based on the implications of pilot
studies. In Section IV, we validate the effectiveness of the
OneClick framework using well-known objective evaluation
methods of audio and video quality. Section V details an
implementation of the proposed framework. Specifically, we
use it to compare the quality of three IM applications and
two first-person shooter games. Then, in Section VII, we
summarize our conclusions.
II. RELATED WORK
Methods for assessing the quality of network applications
can be classified as either objective or subjective. Subjective
methods require users’ input in the evaluation process. Objec-
tive methods do not require such input, but they are often based
on well-validated data derived from subjective evaluations.
Objective evaluations rely on passive monitoring of an
application’s or a user’s behavior. For example, they may
take network-level measurements [2, 4, 7] or media-level
measurements [11, 15] as the input for quality assessment.
Many techniques and standards based on this approach exist
for evaluating the quality of audio content [10, 11], video
content [9], and online games [3]. Objective evaluation meth-
ods are effective when user input is not required; however,
they cannot assess all the QoE dimensions that may affect
users’ experiences in quality summarization. For example, to
ensure that a model remains tractable, the loss of volume
is not considered by the PESQ (Perceptual Evaluation of
Speech Quality) model [11]; the distance between viewers
and the video is not considered by the VQM (Video Quality
Measurement) model [9]; and the higher-order variations,
i.e., the burstiness, of end-to-end delay and loss are not
considered by the E-model, which is designed for assessing
VoIP quality [10].
Subjective quality evaluations, on the other hand, require
users to indicate their feelings in a survey. However, consider-
ing the limited efficiency and high resource overhead of such
schemes, and the fact that people’s feelings are subject to
various effects like personal biases and the memory effect,
it is not always easy to obtain reliable evaluation results.
The Mean Opinion Score (MOS) [8] is probably the most
widely used subjective quality assessment method. Although
it is often used to evaluate user experience in applications, it
has a number of limitations, as described in [12].
In [5], Dinda et al. proposed a scheme that employs
a similar concept to that of the OneClick framework to
evaluate user satisfaction in response to different scheduling
policies and clock frequencies of computer systems. However,
the goals of the two applications are totally different. Our
work expands the idea of “click whenever dissatisfied” into
a complete framework, which includes experiment design,
modeling, and interpretation. In addition, we provide a val-
idation of the proposed framework based on two objective
evaluation methods, PESQ and VQM, and prove its usefulness
by applying it to popular IM applications and online games.
III. METHODOLOGY DEVELOPMENT
In this section, we describe the design of the OneClick
framework and the rationale behind it. We begin with a more
intuitive design and then gradually revise the design based
on the findings in pilot studies. Finally, we summarize the
framework with the data flow chart shown in Fig. 6.
40 1 2 3 4 5
20
0
30
0
40
0
William
(a) Shift (sec)
R
es
id
ua
l d
ev
ia
nc
e Test 1Test 2
0 1 2 3 4 5
10
0
14
0
18
0
Apple
(b) Shift (sec)
R
es
id
ua
l d
ev
ia
nc
e Test 1Test 2
0 1 2 3 4 5
15
0
25
0
35
0
Sean
(c) Shift (sec)
R
es
id
ua
l d
ev
ia
nc
e Test 1Test 2
William Apple Sean
Test 1
Test 2
(d) Subject
Av
g.
 re
sp
on
se
 d
el
ay
 (s
ec
)
0.
0
1.
0
2.
0
3.
0
Fig. 2. (a)(b)(c) The relationship between the time lag added to the click
event process and the residual deviance of the fitted Poisson regression model
for three subjects. (d) The average response delay time in each of the 6 tests
made by the three subjects.
knowing the current network setting. We asked three computer
science students to take the tests so that we could study their
response delays, the consistency and bias of user feedback,
and the effect of test materials.
1) Response Delays: We observe that users may not always
respond quickly; that is, they may unintentionally delay click
actions after they become aware of the degraded application
quality. To compensate for this effect, we assume that the
response delays in the same test are generally close to the
average response delays; a subject may have different response
delays from time to time. Our solution is to shift the click
event process and determine how much delay should be added
to yield the best model fit. We search for the average delay
time davg by fitting the Poisson regression model for network
factors and click event processes with different time lags,
where davg is computed as
argmind{deviance of (1) by replacing C(t) with C(t+d)}.
We conducted a number of experiments to validate the
proposed solution. Figure 2 shows the residual deviance of the
fitted model versus the average delay time for the tests taken
by the three subjects. We can see that the residual deviance
with different average delays is generally concave upwards
with a local minimum around 1–2 seconds. This implies that
1) the Poisson regression model fits our data; 2) the variable
response delays can be accounted for by an average response
delay; and 3) our subjects normally delay their click actions
after hearing music with unsatisfactory quality1. From the
graphs, we observe that a subject’s average response delays
may change in different tests and, not surprisingly, the average
response delays are different for the three subjects.
As we have demonstrated the existence of response delays,
we use the shifted click process (by davg) to replace the
original click process in the rest of analysis and modeling
tasks.
1The response delays may also be due to the application’s processing delay,
such as the time taken in audio decoding and playback; however, it is unrelated
to our processing here.
0.0 0.1 0.2 0.3
0
1
2
3
4
William
Loss rate
Cl
ick
 ra
te
 (s
ec
−
1 )
Test 1
Test 2
0.0 0.1 0.2 0.3
0
1
2
3
4
Sean
Loss rate
Cl
ick
 ra
te
 (s
ec
−
1 )
Test 1
Test 2
Fig. 3. A subject tends to produce similar click rate curves in different tests.
0.0 0.1 0.2 0.3
0
1
2
3
4
Loss rate
Cl
ick
 ra
te
 (s
ec
−
1 )
William
Apple
Sean
Fig. 4. Different subjects may have very different click behavior due to
personal characteristics or preferences even if they share similar perceptions
of an application’s quality of experience.
2) Consistency of User Feedback: One common concern
about subjective quality evaluation methods is the consistency
of users’ feedback, i.e., if the experiment design tends to make
a subject give similar ratings in repeated experiments. Because
of this concern, we asked two subjects to take the same test
twice. We then plotted the relationship between their click
rates and network loss rates, as shown in Fig. 3. The vertical
bars denote the 95% confidence band of the average click
rates with a certain loss rate2. The figure shows that each
subject tended to give similar ratings in two tests because the
average click rates in both tests in most of the scenarios were
statistically equivalent. For this reason, we merge the results in
different tests taken by the same user by averaging the click
rates in each test scenario. This result also suggests that a
5-minute OneClick test is sufficient to reliably measure a
subject’s perceptions of audio quality under a wide spectrum
of network loss rates.
3) Bias of User Feedback: OneClick’s design reduces the
difficulty of decision-making by offering dichotomous choices;
that is, a subject only needs to decide whether or not to
click the button. However, users’ preferences may still have a
significant effect on their click behavior. For example, some
subjects may click the button when the perceived quality is
slightly worse than expected, while others may only click it
when the quality is really unacceptable. The effects of a user’s
preferences and personal characteristics are obvious, especially
2The unequal horizontal spaces between samples are intentional because
the impact of loss rates on audio quality is nonlinear, as shown by PESQ
curve shown in Fig. 7. Thus, we select the loss rates that are expected to
yield the largest discrepancy in click rates. The purpose of this design is
simply to reduce the number of test scenarios and maintain the efficiency
of the experiments. It does not affect the effectiveness of the method or the
accuracy of the assessment results.
60.0 0.1 0.2 0.3 0.4
0.
30
0.
35
0.
40
0.
45
0.
50
Loss rate
PE
SQ
−
1
AIM
MSN
Skype
0.0 0.1 0.2 0.3
0
1
2
3
4
Loss rate
Cl
ick
 ra
te
 (s
ec
−
1 )
AIM
MSN
Skype
Fig. 7. Comparison of PESQ scores and OneClick evaluation results for
the audio quality of IM applications over different loss rates.
objective quality assessment methods, and use it to evaluate
the quality of several real-life applications.
IV. FRAMEWORK VALIDATION
Thus far, we have shown that users’ satisfaction with an
application under different network conditions can be quanti-
fied using the proposed framework (Section III-B). However,
we need to demonstrate that the framework’s evaluation results
are trustworthy and reliable. Therefore, in this section, we val-
idate the OneClick framework by comparing its assessment
results with two objective quality evaluation methods, namely
Perceptual Evaluation of Speech Quality (PESQ) and Video
Quality Measurement (VQM).
We design two experiments for validation purposes. One is
based on PESQ, an objective audio quality assessment tool;
and the other is based on VQM, an objective video quality
assessment tool. Our results show that users’ perceptions
measured using the OneClick framework are consistent with
those derived by the objective methods.
A. PESQ-based Validation
In the first experiment, we use OneClick and PESQ to
evaluate the relative audio quality of three popular instant
messaging (IM) applications: AIM, MSN Messenger, and
Skype. The experiment setup is exactly the same as that
used in the pilot study. However, instead of comparing the
audio quality with different network settings, we focus on
the difference in the audio quality provided by different IM
applications.
Figure 7 compares the results derived by PESQ and
OneClick. Because better audio quality leads to a higher
PESQ score and a lower click rate, the reciprocals of the PESQ
scores are used for comparison. From the graph, we observe
that the relative quality of the three applications is similar
from the perspective of both evaluation methods. Specifically,
Skype’s audio quality is the best under different loss rates, and
MSN Messenger performs slightly better than AIM when the
loss rate is high. In addition, the difference between Skype’s
quality and that of the other two applications is significant.
In the second experiment, we change the network bandwidth
in different test scenarios and keep the loss rate at zero. As
shown in Fig. 8, the evaluation results obtained using PESQ
and OneClick are consistent. From the results, we observe
that AIM performs the best in the non-loss and bandwidth-
limited scenarios, while Skype and MSN Messenger are
20 30 40 50 60 70 80
0.
30
0.
35
0.
40
0.
45
Bandwidth (Kbps)
PE
SQ
−
1
AIM
MSN
Skype
20 40 60 80 100
0
1
2
3
4
Bandwidth (Kbps)
Cl
ick
 ra
te
 (s
ec
−
1 )
AIM
MSN
Skype
Fig. 8. Comparison of PESQ scores and OneClick evaluation results for
the audio quality of IM applications over different network bandwidths.
ranked second and third respectively. The consistency in eval-
uation results demonstrate the efficiency of the OneClick
framework, as a subject only required 15 minutes (5 minutes
for each application) to obtain all the results shown in the right
pane of Fig. 8.
Given that PESQ is effective in assessing the audio quality
of IM applications, one may wonder why we have developed
a framework that relies on the evaluations of human subjects.
Our motivation is that, although PESQ normally provides
accurate assessments of user satisfactions with audio signals,
it can only capture a limited number of QoE dimensions.
Moreover, there are many factors that it cannot measure
or quantify. According to ITU-T P.862 [11], PESQ yields
inaccurate predictions when used in conjunction with factors
like listening levels, loudness loss, effect of delays in conver-
sational tests, talker echo, and sidetone. Taking the listening
level as an example, PESQ assumes a standard level of 79
dB SPL, so it requires the volume of both the original and
degraded audio recordings to be normalized before the quality
estimation procedures. Thus, if different applications incur
different degree of loudness loss in their audio transmissions,
PESQ will not be able to take account of such discrepancies.
In addition, many factors that may affect listeners’ perceptions
are present in the playback stage, but information about them
is not accessible through the audio recordings. For example,
the effect of playback devices (speakers or headphones),
environmental noise, and the distance between the listener
and the speakers (if used) are not considered by PESQ
because they are not part of the audio recordings. An objective
quality evaluation scheme like PESQ or VQM, no matter how
sophisticated it is, cannot capture all factors that may affect
users’ perceptions as some of the factors are unmeasurable.
In contrast, subjective evaluation methods can capture all the
QoE dimensions that humans perceive, as the ratings are
given by the subjects directly. Therefore, we use PESQ for
validation purposes and the OneClick framework in more
complex scenarios, where unmeasurable factors are present.
For example, the framework can be used to evaluate the audio
quality of IM applications in which the environmental noise
levels at the talker side may vary over time.
B. VQM-based Validation
Our second validation experiment is based on VQM, which
is designed to evaluate the quality of a degraded video clip
by comparing it with the original version before transmission
or compression. In this experiment, we evaluate the impact of
820 40 60 80 100 120 140
1.
0
1.
5
2.
0
2.
5
3.
0
3.
5
4.
0 Loss Rate: 0.0
Bandwidth (Kbps)
Cl
ick
 ra
te
 (s
ec
−
1 )
AIM
MSN Messenger
Skype
20 40 60 80 100 120 140
1.
0
1.
5
2.
0
2.
5
3.
0
3.
5
4.
0 Loss Rate: 0.1
Bandwidth (Kbps)
Cl
ick
 ra
te
 (s
ec
−
1 )
AIM
MSN Messenger
Skype
Fig. 10. The expected click rates under different combinations of network
loss rates and bandwidth.
AIM
Bandwidth (Kbps)
Lo
ss
 ra
te
20 40 60 80 100 140
0.
00
0.
05
0.
10
0.
15
0.
20
0.
25
0.
30
MSN Messenger
Bandwidth (Kbps)
Lo
ss
 ra
te
20 40 60 80 100 140
0.
00
0.
05
0.
10
0.
15
0.
20
0.
25
0.
30
Skype
Bandwidth (Kbps)
Lo
ss
 ra
te
20 40 60 80 100 140
0.
00
0.
05
0.
10
0.
15
0.
20
0.
25
0.
30
Fig. 11. The contour plot of expected click rates under different network
settings.
currently no standard method for evaluating gamers’ network
experiences [3]. Some researchers use in-game achievements
as objective indices [1, 14], for example, the number of kills
in shooting games, or the time taken to complete each lap in
racing games. However, game scores are highly dependent on
the players’ skills, the game’s design, and the content, so the
results are not comparable and generalizable across different
games. Most of the remaining studies are based on traditional
subjective evaluation methods, such as MOS. Because of the
cost of such survey methods, the scale of user studies is often
small; hence it is difficult to validate and generalize the results.
Here, we apply the OneClick framework to evaluate the
impact of network conditions on two popular first-person
shooter (FPS) games, namely Unreal Tournament and Halo. In
FPS games, players must make sub-second reactions because
of rapidly changing environmental factors, such as the actions
of companions and opponents; therefore, network delays play
an important role in users’ perceptions of the quality of game
play. In addition, the variations in network delay may affect
the game play if the status updates from other parties are jerky
and the response time of players’ commands constantly vary.
As a result, players may have difficulty to making a turn or
firing on an opponent at an appropriate time. Thus, we change
the network delay and its variation in the test scenarios.
Because game playing is interactive, we cannot use pre-
recorded test materials. Instead, we put a router with dum-
mynet in front of a game client and use it to add an intentional
delay to every packet sent from the client to the game server.
Assuming every game packet spends a relatively constant time
traversing the Internet, we can evaluate the impact of different
combinations of network delay and delay variations on the
game’s quality with OneClick. We define “delay jitter”
as the standard deviation of network delays; and, without
loss of generality, we assume delay jitters follow a Gamma
distribution. The router is configured to automatically change
20 40 60 80 100 120 140
0.
00
0.
05
0.
10
0.
15
0.
20
0.
25
0.
30
Click Rate Threshold: 1.5 sec−1
Bandwidth (Kbps)
Lo
ss
 ra
te
AIM
MSN Messenger
Skype
20 40 60 80 100 120 140
0.
00
0.
05
0.
10
0.
15
0.
20
0.
25
0.
30
Click Rate Threshold: 2.0 sec−1
Bandwidth (Kbps)
Lo
ss
 ra
te
AIM
MSN Messenger
Skype
Fig. 12. The comfort regions of the three IM applications based on the
Poisson regression model.
0.0 0.5 1.0 1.5 2.0
Delay Jitter: 0.0 sec
(a) Network delay (sec)
Cl
ick
 ra
te
 (s
ec
−
1 )
1
2
3
Unreal Tournament
Halo
0.0 0.5 1.0 1.5 2.0
Delay Jitter: 2.0 sec
(b) Network delay (sec)
Cl
ick
 ra
te
 (s
ec
−
1 )
1
2
3
Unreal Tournament
Halo
0.0 0.5 1.0 1.5 2.0
0
1
2
3
4
5
Click Rate Threshold: 1.0 sec−1
(c) Network delay (sec)
D
el
ay
 jit
ter
 (s
ec
) Unreal TournamentHalo
0.0 0.5 1.0 1.5 2.0
0
1
2
3
4
5
Click Rate Threshold: 2.0 sec−1
(d) Network delay (sec)
D
el
ay
 jit
ter
 (s
ec
) Unreal TournamentHalo
Fig. 13. (a)(b) The expected click rates under different combinations of
network delay and delay jitter. (c)(d) The comfort regions of UT and Halo
based on the Poisson regression model.
the network setting every 5 seconds. For each reconfiguration,
it randomly sets the network delay and delay jitter within 2
seconds.
We asked three subjects to participate in the experiments,
each of which lasted for 5 minutes, and then combined the
subjects’ evaluation results. Figures 13(a) and (b) show the
expected click rates with different network delays and delay
jitter. Generally, Unreal Tournament (UT) provides better
quality than Halo under the same network conditions. UT
is slightly less effective than Halo when both the delay and
delay jitter are fairly high (nearly 2 seconds). The comfort
region plots in Fig. 13(c) and (d) also reveal that UT provides
better overall quality than Halo unless the network delay is
extremely large. We cannot understand why UT is more robust
to poor network quality than Halo unless we can analyze the
designs of the game engines, especially the network handling
and screen updating components, of both games. However,
based on the comments of the experiment’s participants,
we suspect that UT implements certain delay-concealment
and dead reckoning mechanisms to deal with network lags.
Therefore, it provides players with better gaming experiences,
even when the network quality deteriorates. As there is no
standard method for comparing players’ network experiences
across games, we plan to further validate our evaluation results
in future work.
1Tuning Skype’s Redundancy Control Algorithm for
User Satisfaction
Te-Yuan Huang†, Kuan-Ta Chen‡, and Polly Huang†
†Department of Electrical Engineering, National Taiwan University
‡Institute of Information Science, Academia Sinica
Abstract—Determining how to transport delay-sensitive voice
data has long been a problem in multimedia networking. The
difficulty arises because voice and best-effort data are different
by nature. It would not be fair to give priority to voice traffic
and starve its best-effort counterpart; however, the voice data
delivered might not be perceptible if each voice call is limited
to the rate of an average TCP flow. To address the problem, we
approach it from a user-centric perspective by tuning the voice
data rate based on user satisfaction.
Our contribution in this work is threefold. First, we investigate
how Skype, the largest and fastest growing VoIP service on the
Internet, adapts its voice data rate (i.e., the redundancy ratio)
to network conditions. Second, by exploiting implementations
of public domain codecs, we discover that Skype’s mechanism
is not really geared to user satisfaction. Third, based on a set
of systematic experiments that quantify user satisfaction under
different levels of packet loss and burstiness, we derive a concise
model that allows user-centric redundancy control. The model
can be easily incorporated into general VoIP services (not only
Skype) to ensure consistent user satisfaction.
Index Terms—MOS, PESQ, Piggyback, QoE (Quality of Ex-
perience), QoS (Quality of Service), VoIP
I. INTRODUCTION
Effective end-to-end transport of delay-sensitive voice data
has long been a problem in multimedia networking. Voice
traffic, by nature, is high in data rate and it is sensitive to
network impairments. With the increase in multimedia traffic
on the Internet, a growing dilemma is that it would not be
fair to give priority to voice traffic and starve its best-effort
counterpart; however, the voice data delivered might not be
perceptible if each voice call is limited to the rate of an average
TCP flow. To address this problem, we approach it from a user-
centric viewpoint by adapting the sending rate of voice calls
based on user satisfaction. Such a user-friendly rate adaptation
mechanism would also be congestion-friendly, although it is
not strictly TCP-friendly [7].
Adapting the voice sending rate is a subtle issue because
users prefer calls with a higher bit rate [5]. However, sending
voice data with an unnecessarily high bit rate could be a
waste of network resources or result in congestion, and that in
turn could compromise the user’s experience. In July 2008,
eBay announced that Skype had 338.2 million registered
users and earned US$136 million in revenue, representing
51% year-over-year growth1. Skype, as one of the largest
and fastest growing VoIP services on the Internet, seems to
note the subtlety and does not indulge its voice data with
1http://www.tgdaily.com/content/view/38431/122/
unlimited network bandwidth. Recently, Skype launched a
very ambitious monthly plan worldwide, which is expected
to attract even more users and voice transmissions from the
traditional telephone services to the Internet. The surge in
demand raises an important question: How should Skype or
competing VoIP services adapt their voice sending rates to
meet customers’ QoS expectations. To address this question,
we investigate three issues: (1) how Skype adapts its voice
rate, (2) whether Skype’s rate adaptation mechanism is geared
to user satisfaction, and (3) how Skype and any other VoIP
services should adapt their voice data rates to ensure consistent
user satisfaction.
Bonfiglio et al. [4] observed that Skype’s voice data rate is
governed by three factors: the bit rate, the framing time, and
redundancy. Among them, the bit rate and framing time are
determined by the codec. Skype uses G.729 as the audio codec
for SkypeOut (PC-to-PSTN) calls; while for PC-to-PC calls,
iSAC, an audio codec developed by Global IP Solutions [9],
is used in most of the calls. In particular, G.729 provides a
constant bit rate (CBR) for voice data. Thus, the rate variation
in SkypeOut calls is the result of adapting redundancy to
network conditions. Furthermore, we found that the bit rate
and framing time adaptation in calls using iSAC, the popular
variable bit rate (VBR) codec, is very likely implemented by
the codec developer [9], instead of Skype. Therefore, the only
parameter tuned by Skype is the redundancy factor.
Focusing on the rate adaptation issue at the redundancy
control level, we present our methodology for automatically
identifying the redundancy ratio, i.e., the percentage of packets
piggyback a previous packet, in general Skype calls, and derive
the relationship between the redundancy ratio and the network
loss rate. The major findings are (1) Skype increases the
redundancy ratio as the network loss rate increases; however,
(2) Skype’s control algorithm does not take the individual
codec and packet loss patterns (burstiness) into consideration.
These findings indicate that, although Skype’s rate adapta-
tion mechanism somehow addresses the subtle relationship
between sending rate and user satisfaction, there are yet
discrepancies towards consistent user satisfaction.
To address the problem, we adopt implementations of public
domain codecs and quantify user satisfaction, i.e., the mean
opinion scores (MOS), for calls under different levels of packet
loss and burstiness. Our results suggest that the adaptation
policy should be codec-specific. To sustain the voice quality at
MOS value 3.3, more redundancy should be added to G.711
voice calls than to those in G.729 calls. More redundancy
320
30
40
50
60
70
Time (second)
Pa
yl
oa
d 
(b
yte
s)
0 180 360 540 720 900 1260 1620 1980
(a) G.729
0
50
10
0
15
0
20
0
25
0
30
0
35
0
Time (second)
Pa
yl
oa
d 
(b
yte
s)
0 180 360 540 720 900 1260 1620 1980
(b) iSAC
Fig. 2. The impact of the network loss rate on the payload size of Skype packets.
For both versions of Skype, the codec G.729 is used for
SkypeOut sessions.
We collect Skype traces on the FreeBSD box with the
program tcpdump [15]. In addition, to avoid the interference
caused by initial setup traffic, we only record the traffic
after the call has been established for 60 seconds. In each
experiment, we increase the network loss rate from 0% to 10%
in 1% increments every 180 seconds. For the iSAC traces, we
filter out the control and signaling packets by inspecting the
“Start of Message” (SoM) field, which contains the message
ID and the function of the packet [2].
B. Observations
Fig. 2(a) shows the scatter plot of the payload sizes of
the packets in the SkypeOut (G.729) trace. From the graph,
we observe that when the loss rate is 0%, i.e., between 0
and 180 seconds, the payload size remains around 30 bytes.
However, as the loss rate increases, we find there are more
packets with a payload size around 60 bytes. When the loss
rate reaches 10%, i.e., between 1800 and 1980 seconds, the
majority of the packets have a payload of around 60 bytes.
However, when the loss rate returns to 0% after 1980 seconds,
the payload size of most packets drops to around 30 bytes.
This phenomenon indicates that Skype changes the proportion
of packets with redundancy information based on the network
loss rate. Note that Skype may also piggyback signaling data
in voice packets. This explains why we can still observe
various payload sizes when no redundant voice information
is introduced, even though G.729 is a constant-bit-rate codec.
The iSAC trace exhibits the same behavior, as shown in
Fig. 2(b). When the loss rate is 0%, the payload size remains
within the range (0, 160) bytes approximately. As the loss rate
increases, we find more packets with a payload size in the
range (160, 320) bytes; and when the loss rate reaches 10%,
the majority of the packets have a payload size in the range
(160, 320) bytes. Note that although iSAC is a codec with
several framing time options, the framing time of the observing
iSAC traffic stays at 30 ms during the whole call; thus, the
variance in the payload sizes is not a consequence of changes
in the speech framing time.
C. Redundancy Ratio Identification
In order to understand the redundancy control algorithm
used by Skype, we attempt to quantify the amount of redun-
dancy added to Skype voice traffic. We define the redundancy
ratio as the percentage of packets that carry redundant voice
data. If all packets carry redundant information, the redun-
dancy ratio is equal to 1. Conversely, if none of the packets
carry redundant information, the redundancy ratio will be 0.
In the following, we present our method for inferring the
redundancy ratio used by Skype based on the traces collected
in the above experiments. We take G.729 and iSAC as ex-
amples, though the method can be extended to other codecs
supported by Skype.
1) G.729: It is easier to deal with the SkypeOut traces,
as the G.729 codec uses a constant bit rate of 8 Kbps and a
constant framing time of 10 ms. For this reason, the codec’s
payload size is more stable than that of iSAC, as shown in
Fig. 2(a). We use a simple threshold method, with the threshold
set at 40 bytes, to determine whether a packet contains a
piggybacked frame. In other words, we assume that a packet
contains redundant information if its payload size is larger than
40 bytes. I.e., if there are 30% of packets with payload size
larger than 40 bytes, then the redundancy ratio will be 0.3.
2) iSAC: It is more difficult to deal with the iSAC traces
because iSAC supports variable bit rate and variable framing
time. We use the the following steps to infer the redundancy
ratio in each of the iSAC traces:
i. First, we determine the framing time of a packet, as
it will affect the payload size of iSAC packets. When
the framing time is longer, the payload size will be
larger, since each packet would carry more information.
5Loss Rate
R
ed
un
da
nc
y 
Ra
tio
0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Burst Ratio = 1
Burst Ratio = 1.5
Burst Ratio = 2
(a) G.729
Loss Rate
R
ed
un
da
nc
y 
Ra
tio
0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Burst Ratio = 1
Burst Ratio = 1.5
Burst Ratio = 2
(b) iSAC
Fig. 5. Comparison of Skype’s redundancy control algorithms for different levels of network loss burstiness for G.729 and iSAC.
iSAC vs. G.729
Loss Rate
R
ed
un
da
nc
y 
Ra
tio
0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
G.729
iSAC
Fig. 4. Comparison of Skype’s redundancy control algorithms over different
network loss rates for G.729 and iSAC.
B. Effect of the Codec
Fig. 4 shows the redundancy ratios for various network loss
rate under G.729 and iSAC. Our objective is to determine
whether Skype uses different redundancy control algorithms
for different codecs. In the figure, the 95% confidence inter-
val of two curves collide with each other. This observation
strongly suggests that Skype applies the same redundancy
control algorithm for different codecs, even though it leads
to different levels of user satisfaction, as we will show in the
next section.
C. Effect of Network Loss Burstiness
To quantify the burstiness of network loss, we adopt the
metric burst ratio defined in ITU-T G.107 [12]:
The average length of observed consecutive losses
The average length of consecutive losses under random losses .
In this definition, the burst ratio is equal to 1 when packet loss
is purely random, and it is larger than 1 when packet loss is
bursty. Specifically, a burst ratio equal to 2 indicates that the
average length of consecutive losses is twice longer than that
of purely random losses.
The experiment are similar to those in Section III, except
that the packet loss is now bursty rather than uniformly dis-
tributed. We implemented the Gilbert model [8] to determine
whether a packet should be dropped in dummynet [17] in
order to simulate different levels of loss burstiness. The Gilbert
model comprises two states, the received state and the loss
state; and two transition probabilities, p and q, where p is the
probability of a transition moving from “received” to “loss”
and q is the probability of a transition moving from “loss” to
“received.” In this model, the packet loss rate is formulated as
p
p+q and the burst ratio is formulated as
1
p+q . Thus, by setting
the values of p and q, we are able to control both the network
loss rate as well as the burst ratio.
Fig. 5(a) shows the observed redundancy ratio of G.729
when its traffic experiences packet losses with different burst
ratios. As shown in the figure, each curve is corresponding to
a burst ratio setting and their 95% confidence intervals overlap
with each other. Similarly, in Fig. 5(b), each curve represents
the redundancy ratios observed from iSAC calls with packet
losses under different burst ratios. Again, we found that the
95% confidence intervals of the curves are also overlapped.
In summary, our experiment results strongly suggest that
Skype adjusts redundancy ratios only based on the network
loss rate; i.e., it does not consider the codec or network loss
busrtiness.
V. DERIVING AN OPTIMAL REDUNDANCY CONTROL
POLICY
In this section, we present a methodology that can derive
the optimal redundancy control policy for a desired VoIP
quality under a certain network condition. We then compare
the inferred Skype redundancy control policy with the optimal
policy to determine whether Skype’s policy is optimal.
A. Methodology
We develop a simulator that can grade the voice quality of
audio clips transmitted using a specific codec with a given
network condition. To evaluate the quality of an audio clip,
we use PESQ [14], which compares a degraded audio clip
with its original version and output a Mean Opinion Score
(MOS) [13].
7G.711
Loss Rate
R
ed
un
da
nc
y 
Ra
tio
0.00 0.02 0.04 0.06 0.08 0.10
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
(a) G.711
G.729
Loss Rate
R
ed
un
da
nc
y 
Ra
tio
0.00 0.02 0.04 0.06 0.08 0.10
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
(b) G.729
Fig. 7. The contour plots of audio quality scores for different combinations of redundancy ratios and network loss rates.
G.711
Loss Rate
R
ed
un
da
nc
y 
Ra
tio
0.00 0.02 0.04 0.06 0.08 0.10
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Burst Ratio = 1
Burst Ratio = 1.5
Burst Ratio = 2
(a) G.711
G.729
Loss Rate
R
ed
un
da
nc
y 
Ra
tio
0.00 0.02 0.04 0.06 0.08 0.10
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Burst Ratio = 1
Burst Ratio = 1.5
Burst Ratio = 2
(b) G.729
Fig. 8. The redundancy ratios needed to sustain voice quality for a MOS score of 3.5 with different loss rates and burst ratios.
ratio derived by our methodology, we can ensure a balance
between bandwidth utilization and voice quality.
We use Fig. 9 to quantify the degree that Skype’s redun-
dancy control algorithm deviates from the policy that achieves
a consistent audio quality under various network conditions.
The graphs are computed based on the assumption that the
desired MOS score is 3.4, as Skype’s audio quality is mostly
around this level in our simulation scenarios. For each network
setting, we plot the bandwidth Skype uses and the MOS
score Skype provides on the respective normalized scales. The
desired MOS score and the bandwidth required to achieve the
desired audio quality are both set to 100. On the graphs, the
left-hand side of the y-axis marks the normalized bandwidth
utilization, and the right-hand side marks the normalized MOS
score3. We observe that the bandwidth utilization and audio
quality of Skype fluctuate under different network settings.
Sometimes, Skype uses too little bandwidth and results in
worse quality scores than the desired score; for example,
the scenario with 2% loss rate in Fig. 9(d), and those with
2% and 3% loss rates in Fig. 9(e) and 9(f) respectively. At
the same time, Skype sometimes injects too much redundant
3Note that the x-axes of Fig. 9(e) and Fig. 9(f) end at 0.03. The reason is
that it is impossible to achieve a MOS score of 3.4 when the burst ratio is
1.5 or 2 with a loss rate higher than 3%.
information and thus achieves a quality level better than the
desired level, e.g., the scenarios with a loss rate higher than
4% and a burst ratio equal to 1, as shown in Fig. 9(d).
Our results show that Skype’s audio quality is not consistent
as it adjusts the redundancy ratio independently of the codec
used and the network loss burstiness. The inconsistency in
voice quality may result in frustration for users or over-
utilization of bandwidth. To balance the needs of users and
ensure network efficiency, a more sophisticated redundancy
control algorithm that considers all the necessary factors is
required.
VI. MODELING OPTIMAL REDUNDANCY RATIOS
We have shown that the redundancy control algorithm
used by Skype is suboptimal. Moreover, we have proposed
computing the optimal redundancy control policy based on
PESQ quality estimation. The policy can be adopted by real-
time audio streaming tools to provide consistent user experi-
ences no matter how network conditions change. However, the
procedures for deriving the optimal redundancy ratios are time
consuming and therefore need to be performed beforehand.
Note that it is not possible to compute an optimal redundancy
ratio for any combination of network factors, as many of
the factors, including the network loss rate, are real-valued.
90
50
10
0
15
0
20
0
25
0
SVOPC
Time (second)
Pa
yl
oa
d 
(b
yte
s)
0 180 360 540 720 900 1260 1620 1980
Fig. 11. The impact of network loss rate on the payload size of SVOPC
packets
incorporating additional network factors. The advantage of
our model is that it is easy to compute, as only simple
arithmetic is needed to calculate the optimal redundancy ratio
given the network factors. Therefore, any VoIP application that
adopts the model can provide consistent service quality with
minimum computation and network overhead.
VII. CONCLUSION
In this paper, we have determined how Skype adapts its
redundancy levels to network loss rate and burstiness; shown
that Skype’s rate adaptation mechanism is not really geared
for user satisfaction; and proposed a general model for various
codecs to tune the redundancy for consistent user satisfaction.
The methodology used to derive the general model can be
extended by Skype developers to facilitate tuning of different
proprietary codecs, such as iSAC and SVOPC.
During our research, Skype has changed to SVOPC for most
PC-to-PC calls. The results of our preliminary experiments
(Fig. 11) show that Skype’s current redundancy control mecha-
nism is much the same as that used in previous releases. This is
consistent with our findings on G.729 and iSAC, and confirms
that Skype’s redundancy control mechanism is probably not
codec specific.
ACKNOWLEDGEMENT
The authors would like to thank anonymous reviewers for
their constructive comments. This work was supported in part
by grants from Intel Education Program, Taiwan Information
Security Center (TWISC) and National Science Council of
Taiwan under Contract NSC 97-2220-E-002-005, NSC 97-
2220-E-002-012, NSC 97-2219-E-001-00, NSC 97-2219-E-
011-006, and NSC 96-2628-E-001-027-MY3.
REFERENCES
[1] S. Baset and H. Schulzrinne, “An analysis of the Skype
peer-to-peer internet telephony protocol,” in Proceedings
of IEEE INFOCOM’06, Barcelona, Spain, Apr. 2006.
[2] P. Biondi and F. Desclaux, “Silver needle in the Skype,”
Amsterdam, the Netherlands, Mar 2006.
[3] D. Bonfiglio, M. Mellia, M. Meo, N. Ritacca, and
D. Rossi, “Tracking down Skype traffic,” in Proceedings
of IEEE INFOCOM’08, Phoenix, AZ, Apr. 2008.
[4] D. Bonfiglio, M. Mellia, M. Meo, D. Rossi, and P. To-
fanelli, “Revealing Skype traffic: when randomness plays
with you,” in Proceedings of ACM SIGCOMM’07, Kyoto,
Japan, Aug. 2007, pp. 37–48.
[5] K.-T. Chen, C.-Y. Huang, P. Huang, and C.-L. Lei,
“Quantifying Skype user satisfaction,” in Proceedings of
ACM SIGCOMM’06, Pisa, Italy, Sep. 2006.
[6] K.-T. Chen, C. C. Tu, and W.-C. Xiao, “OneClick: A
framework for measuring network quality of experience,”
in Proceedings of IEEE INFOCOM 2009, April 2009.
[7] S. Floyd, M. Handley, J. Padhye, and J. Widmer,
“Equation-based congestion control for unicast applica-
tions,” in In Proceedings of ACM SIGCOMM’00, Stock-
holm, Sweden, Aug. 2000.
[8] E. Gilbert, “Capacity of a burst-noise channel,” The Bell
System Technical Journal, vol. 39, pp. 1253–1265, SEP
1960.
[9] http://www.gipscorp.com/, Global IP Solutions.
[10] S. Guha, N. Daswani, and R. Jain, “An experimental
study of the Skype peer-to-peer VoIP system,” in Pro-
ceedings of The 5th International Workshop on Peer-to-
Peer Systems (IPTPS’06), Santa Barbara, CA, Feb. 2006,
pp. 1–6.
[11] http://support.intel.com/support/performancetools/libraries/ipp/,
Intel Integrated Performance Primitives (Intel IPP).
[12] ITU-T Recommendation G.107, “The E-model, a com-
putational model for use in transmission planning,” Mar.
2005.
[13] ITU-T Recommendation P.800, “Methods for subjective
determination of transmission quality,” 1996.
[14] ITU-T Recommendation P.862, “Perceptual evaluation of
speech quality (PESQ), an objective method for end-to-
end speech quality assessment of narrow-band telephone
networks and speech codecs,” Feb. 2001.
[15] V. Jacobson, C. Leres, and S. McCanne, tcpdump,
1989. [Online]. Available: ftp://ftp.ee.lbl.gov
[16] J. Lindblom, “A sinusoidal voice over packet coder tai-
lored for the frame-erasure channel,” IEEE Trans. Speech
Audio Processing, 2004, accepted.
[17] L. Rizzo, “Dummynet and forward error correction,” in
In Proc. of the 1998 USENIX Annual Technical Conf.
USENIX Association, 1998.
[18] http://sox.sourceforge.net/, SoX - Sound eXchange,
Swiss Army knife of sound processing programs.
[19] K. Suh, D. R. Figueiredo, J. Kurose, and D. Towsley,
“An analysis of the Skype peer-to-peer internet tele-
phony protocol,” in Proceedings of IEEE INFOCOM’06,
Barcelona, Spain, Apr. 2006.
[20] http://www.voiptroubleshooter.com/, Open Speech
Repository.
[21] C.-C. Wu, K.-T. Chen, Y.-C. Chang, and C.-L. Lei,
“Peer-to-peer application recognition based on signaling
activity,” in Proceedings of IEEE ICC 2009, June 2009.
2Loss rate
(Unlimited bandwidth)
B
a
n
d
w
id
th
(N
o
 p
a
c
k
e
t 
lo
s
s
)
Delay
(No packet loss)
Loss rate
(No delay)
B
a
n
d
w
id
th
(N
o
 d
e
la
y
)
Delay Loss rate
100 Kbps 100 Kbps
80 Kbps
60 Kbps
40 Kbps
20% 200 ms
50 ms
100 ms
150 ms
Delay
(Unlimited bandwidth)
80 Kbps
60 Kbps
40 Kbps
200 ms
50 ms
100 ms
150 ms
15%
10%
5%
20%
15%
10%
5%
200 ms
200 ms
150 ms
150 ms
50 ms
0 ms
0 ms
50 ms
100 ms
150 ms
200 ms
200 ms
150 ms
100 ms
50 ms
20%
15%
10%
5%
0%
20%
15%
10%
0%
5%
20%
20%
5%
10%
15%
80 Kbps
60 Kbps
40 Kbps
80 Kbps
60 Kbps
40 Kbps
100 Kbps
80 Kbps
60 Kbps
40 Kbps
40 Kbps
100 Kbps
80 Kbps
60 Kbps
Unlimited
No packet loss 
Unlimited bandwidth
No delay
Unlimited bandwidth
No delay
No packet loss
Bandwidth
15%
10%
5 %
100 ms
100 ms
Unlimited
Fig. 1: The layout of our radar chart.
TABLE I: Summary of traces
VoIP Application Calls Host Packets Bit Rate (mean/sd.) Packet Rate (mean/sd.) Packet Size (mean/sd.)
Skype 976 Sender 12, 058, 392 64 / 35 Kbps 48 / 19 pkt/sec 166 / 76 bytesReceiver 11, 200, 088 58 / 30 Kbps 45 / 18 pkt/sec 162 / 75 bytes
MSN Messenger 671 Sender 6, 758, 092 38 / 18 Kbps 40 / 15 pkt/sec 120 / 58 bytesReceiver 6, 089, 949 33 / 15 Kbps 36 / 15 pkt/sec 116 / 57 bytes
Google Talk 793 Sender 3, 632, 410 19 / 16 Kbps 18 / 11 pkt/sec 132 / 65 bytesReceiver 3, 395, 124 17 / 8 Kbps 17 / 4 pkt/sec 124 / 61 bytes
Overall 2, 440 43, 134, 055 40 / 30 Kbps 35 / 19 pkt/sec 145 / 72 bytes
is proportional to the magnitude of the variable relative to
its maximum. Lines are drawn connecting data points on
adjacent axes, thus forming the characteristic polygon for
an observation. A radar chart containing one polygon helps
the researcher identify the dominant variables for a given
observation. A radar chart with multiple polygons compares
the relative strength and weakness of the observations. The
electronic gaming culture makes extensive use of radar charts.
For example, an in-game character’s polygon readily explains
his ability level in every facet (strength, dexterity, constitution,
wisdom, charisma, to name a few), and when superimposed
or juxtaposed with other characters’ it unleashes a telling
comparison between them.
The computer science literature has not seen many ap-
pearances of the radar chart. There is, however, a number
of precursors in other disciplines. The authors of [4], for
example, evaluated the impact of blended learning on a
collegiate course with radar charts, whereas in [5] the radar
chart fused plural parameters to give a verbal description of
environmental comfort level. Buttock pressure distribution of
patients of spinal cord injury was analyzed in [6] with a
hexagonal variant to find the most comfortable wheelchair
cushion and patient posture. Radar charts in [7] summarized
relative concentrations of up to fourteen chemical elements
in order to perform the multielement correlation analyses
for medical diagnosis. Finally, a novel gait analysis method
based on the radar chart was proposed in [8] to facilitate the
treatment of movement disorder in rehabilitation medicine.
III. PROPOSING A RADAR CHART ADAPTATION
The layout of the radar chart we use to map QoE as a
function of QoS metrics is shown in Figure 1. It is divided
into the three sectors of delay, loss rate, and bandwidth, each
with five crossing arcs including the circumference. The arcs
are the isolines for the metric represented by the sector. For
example, the arcs of the bandwidth sector denote, from the
inside out, unlimited bandwidth, 100 Kbps, 80 Kbps, 60 Kbps,
and 40 Kbps, respectively. Each sector is further divided into
two half-sectors, wherein axes intersecting the arcs. In the
bandwidth sector, for example, the half neighboring the delay
4BW 40 Kbps
BW 60 Kbps
BW 80 Kbps
BW 100 Kbps
Loss 5%
Loss 10%
Loss 15%
Loss 20% Delay 200 ms
Delay 150 ms
Delay 100 ms
Delay 50 ms
BW 100 Kbps
BW 80 Kbps
BW 60 Kbps
BW 40 Kbps
Loss 20%
Loss 15%
Loss 10%
Loss 5%Delay 50 ms
Delay 100 ms
Delay 150 ms
Delay 200 ms
Loss 0%,
BW unlimited
Delay 0 ms,
BW unlimited
Delay 0 ms,
Loss 0%
Loss 0%
Loss 5%
Loss 10%
Loss 15%
Loss 20%
Delay 0 ms
Delay 50 ms
Delay 100 ms
Delay 150 ms
Delay 200 ms
BW unlimited
BW 100 Kbps
BW 80 Kbps
BW 60 Kbps
BW 40 Kbps
MOS
2.2
2.5
2.8
3.1
3.4
Fig. 5: Skype’s characteristic radar chart.
 
Delay (ms)
Bi
t r
at
e 
(K
bp
s)
0 50 100 150 200
0
10
20
30
40
50
60
Bandwidth
40 Kbps
60 Kbps
80 Kbps
100 Kbps
Unlimited
 
Loss rate (%)
Bi
t r
at
e 
(K
bp
s)
0 5 10 15 20
0
32
64
96
12
8
16
0
Bandwidth
40 Kbps
60 Kbps
80 Kbps
100 Kbps
Unlimited
Fig. 6: Bitrate of Skype against delay and packet loss rate at
different bandwidths.
A. Single-Application Analysis
We start with the characteristic radar chart of Google Talk,
Figure 3. The polygons shrink rather steadily until the MOS
reaches 3.1, beyond which value Google Talk cannot provide
better QoE even at “perfect” network conditions. A closer look
at the chart reveals that the areas involving loss rate shrink
most rapidly, followed by mild contractions of delay parts
along the bandwidth axes and arcs (compare the polygons at
2.5 and 3.1 in the bandwidth-delay half-sectors), indicating
that the software is most susceptible to packet drops as
compared to other factors. Traditional visualization in Figure 4
confirms our findings, albeit with multiple graphs.
Skype, in contrast, displays a commanding performance in
Figure 5. It is utterly immune to a latency as serious as 200
ms and a packet loss rate as high as 20% if there is more than
60 Kbps of bandwidth available. The bitrates of Skype calls in
channels narrower than that threshold are evidently restrained
in Figure 6, the fact contributing to the software’s inability
to attain expected QoE in such circumstances. The only other
factor that has a say in Skype bitrates is whether or not there
is loss.
BW 40 Kbps
BW 60 Kbps
BW 80 Kbps
BW 100 Kbps
Loss 5%
Loss 10%
Loss 15%
Loss 20% Delay 200 ms
Delay 150 ms
Delay 100 ms
Delay 50 ms
BW 100 Kbps
BW 80 Kbps
BW 60 Kbps
BW 40 Kbps
Loss 20%
Loss 15%
Loss 10%
Loss 5%Delay 50 ms
Delay 100 ms
Delay 150 ms
Delay 200 ms
Loss 0%,
BW unlimited
Delay 0 ms,
BW unlimited
Delay 0 ms,
Loss 0%
Loss 0%
Loss 5%
Loss 10%
Loss 15%
Loss 20%
Delay 0 ms
Delay 50 ms
Delay 100 ms
Delay 150 ms
Delay 200 ms
BW unlimited
BW 100 Kbps
BW 80 Kbps
BW 60 Kbps
BW 40 Kbps
MOS
2.2
2.5
2.8
3.1
3.4
Fig. 7: MSN Messenger’s characteristic radar chart.
The shamrock-like polygons in the radar chart of MSN
Messenger (Figure 7) symbolize the client’s balanced behavior.
Indeed, by skimming through the radar charts of the three
applications, we can quickly capture the fact that in general
Skype performs better than MSN Messenger while they both
outshine Google Talk. The darkest polygons at 3.4 are most
informative in this case, where that of Google Talk has been
reduced to the origin and that of Skype spans half of the chart.
B. Cross-Application Analysis
Figure 8 juxtaposes the radar charts which incorporate the
three VoIP clients’ polygons respectively at lower and higher
values of MOS (2.5 and 3.4). We have discovered in the figure
that:
• With a MOS requirement of 3.4, Skype outperforms
Google Talk and MSN Messenger in every way but for
perhaps a small region where bandwidth ≥ 80 Kbps, loss
rate ≤ 5%, and no delay present. Such glamour does not
come at no price. The Achilles’ heel of Skype seems to
be bandwidth, of which Skype would use as much as 100
Kbps to cope with packet loss, as shown in Figure 9.
• MSN Messenger features the same bitrate strategy as
Skype but with half the usage, resulting in its balanced,
mediocre polygons at higher MOS (see also Figure 7).
• Google Talk can only offer less than acceptable QoE,
especially under “lossy” conditions. It is also observed
that the bitrate of Google Talk are fixed at around 17
Kbps, way below our bandwidth limits. The lack of
adaptiveness or a retransmission mechanism is clearly the
cause of its vulnerability.
VI. APPLICATION
In this section, we propose two applications, application
recommendation and network diagnosis, based on our radar
chart adaption.
6 
 
BW 40 Kbps
BW 60 Kbps
BW 80 Kbps
BW 100 Kbps
 
Loss 5%
Loss 10%
Loss 15%
Loss 20%Delay 200 ms
Delay 150 ms
Delay 100 ms
Delay 50 ms
 
BW 100 Kbps
BW 80 Kbps
BW 60 Kbps
BW 40 Kbps
Loss 20%
Loss 15%
Loss 10%
Loss 5%
 
Delay 50 ms
Delay 100 ms
Delay 150 ms
Delay 200 ms
Loss 0%
Loss 5%
Loss 10%
Loss 15%
Loss 20%
Delay 0 ms
Delay 50 ms
Delay 100 ms
Delay 150 ms
Delay 200 ms
BW unlimited
BW 100 Kbps
BW 80 Kbps
BW 60 Kbps
BW 40 Kbps
Loss 0%,
BW unlimited
Delay 0 ms,
BW unlimited
Delay 0 ms,
Loss 0%
Skype
MSN Messenger
Google Talk
Fig. 10: The meteorological radar chart. A region is occupied
by the VoIP application providing the best QoE therein.
bandwidth axis from less than 50 ms of delay to closing in on
200 ms. In addition to the conclusion that he suffers from a
growing delay, the loss-delay vertex (top right) suggests that
there is a minor increase in the occurrence of packet loss as
well.
VII. CONCLUSION
We realize that the adapted radar chart presented in the
article is not without drawbacks. Readability could be an issue
for laymen or neophytes of the field. Redundancy of infor-
mation, the necessary evil of our current design, is definitely
something we want to get rid of. Nevertheless, we have proved
its versatility and reliability through a sample study of VoIP
clients. By plotting the polygons against varying QoE levels
or across applications of the same nature, one may infer,
both qualitatively and quantitatively, the characteristics and
the relative strength and weakness of a particular application.
We can also construct meteorological radar charts and climate
triangles to recommend a user which application best fits his
network configuration, or give diagnosis should there be any
anomaly in his network connection. In all, we regard the
original visualization tool as an initiative to a broader and
deeper exploration in the multidimensional realm of QoE and
QoS.
REFERENCES
[1] R. Jain, “Quality of experience,” IEEE Multimedia,
vol. 11, no. 1, pp. 96–97, Jan.-March 2004.
[2] ITU-R Recommendation P.800, “Methods for subjective
determination of transmission quality,” 1996.
[3] ITU-T Recommendation P.862, “Perceptual evaluation of
speech quality (PESQ), an objective method for end-to-
end speech quality assessment of narrow-band telephone
networks and speech codecs,” 2001.
BW 40 Kbps
BW 60 Kbps
BW 80 Kbps
BW 100 Kbps
 
Loss 5%
Loss 10%
Loss 15%
Loss 20%Delay 200 ms
Delay 150 ms
Delay 100 ms
Delay 50 ms
 
BW 100 Kbps
BW 80 Kbps
BW 60 Kbps
BW 40 Kbps
Loss 20%
Loss 15%
Loss 10%
Loss 5%
 
Delay 50 ms
Delay 100 ms
Delay 150 ms
Delay 200 ms
Loss 0%,
BW unlimited
Delay 0 ms,
BW unlimited
Delay 0 ms,
Loss 0%
Loss 0%
Loss 5%
Loss 10%
Loss 15%
Loss 20%
Delay 0 ms
Delay 50 ms
Delay 100 ms
Delay 150 ms
Delay 200 ms
BW unlimited
BW 100 Kbps
BW 80 Kbps
BW 60 Kbps
BW 40 Kbps
May 2009
June 2009
July 2009
Fig. 11: An example of diagnosis with radar chart.
[4] A. Harding, D. Kaczynski, and L. Wood, “Evaluation
of blended learning: analysis of qualitative data,” in
Proceedings of the Blended Learning in Science Teaching
and Learning, 2005.
[5] X. Li, W. Hong, J. Wang, J. Song, and J. Kang, “Research
on the radar chart theory applied to the indoor envi-
ronmental comfort level evaluation,” in The Sixth World
Congress on Intelligent Control and Automation, vol. 1,
2006, pp. 5214–5217.
[6] Y. Tanimoto, H. Takechi, H. Nagahata, and H. Ya-
mamoto, “Pressure measurement of air cushions for pa-
tients,” in Proceedings of the 16th IEEE Instrumentation
and Measurement Technology Conference, vol. 2, 1999,
pp. 1245–1250.
[7] T. Hasegawa, K. Inagaki, and H. Haraguchi, “Multiele-
ment correlation analysis of major-to-trace elements in
human blood serum for medical diagnosis as studied
by ICP-AES and ICP-MS,” in Proceedings of IUPAC
International Congress on Analytical Sciences, vol. 17,
2001, pp. 1979–1982.
[8] Y. Zhang, Z. Hao, R. Wang, and D. Jin, “A new
method for the evaluation of gait pathology: a radar chart
approach,” in Proceedings of International Convention
on Rehabilitation Engineering & Assistive Technology.
ACM, 2007, pp. 129–132.
[9] “Open Speech Repository.” [Online]. Available:
http://www.voiptroubleshooter.com/open speech/
[10] L. Ding and R. Goubran, “Assessment of effects of packet
loss on speech quality in VoIP,” in Proceedings of IEEE
International Workshop on Haptic, Audio and Visual
Environments and Their Applications, 2003, pp. 49–54.
[11] ITU-T Recommendation G.107, “The E-model, a com-
putational model for use in transmission planning,” Mar.
2005.
2TABLE I
A COMPARISON OF PAIRED COMPARISON AND COMMONLY USED METHODS IN GAME QOE STUDIES
Generalizable Judgement difficulty Ratio-scale scores Input verifiable
Paired comparison yes low yes yes
MOS ratings yes high no no
Objective performance no N/A no no
inputs since we do not know whether the participants paid
full attention in scoring procedures or they just give ratings
perfunctorily. Paired comparison is another method considered
to evaluate an application’s QoE. In [10], we proposed a
crowdsourceable framework based on paired comparison to
quantify the QoE of multimedia content. We show that paired
comparison is an effective methodology, as it does not have the
problems of the MOS scoring, while keeping its advantages.
In a paired-comparison test, a participant is simply asked
to compare two stimuli at a time, and vote (decide) whose
quality is better based on his/her perception. It can be seen
that the decision is simpler than the MOS method as the five-
scale rating has been reduced to a dichotomous choice. We
summarize the distinct features of pair comparison and other
commonly used evaluation methods in QoE studies, i.e., the
MOS ratings and objective in-game performance method, in
Table I.
In this paper, we propose to use paired comparisons for
evaluating online games’ QoE in various network scenarios.
As a demonstration, we apply the methodology to evaluate
three popular FPS (first-person-shooter) games, namely, Alien
Arena (Alien) [1], Halo [2], and Unreal Tournament (UT) [3],
and investigate their network robustness property. We shall use
the Bradley-Terry-Luce (BTL) model to analyze the paired
comparison results and obtain the ratio-scale magnitudes as
the game’s QoE scores. We defer an overview of the BTL
model to Section III.
Our contribution in this work is two-fold:
1) We propose to jointly use paired comparisons and prob-
abilistic choice models to quantify online games’s QoE
under various network situations. The advantages of our
methodology over the traditional MOS ratings are that
1) the rating procedure is simpler thus less burden is on
experiment participants, 2) it derives ratio-scale scores,
and 3) it enables systematic verification of participants’
inputs.
2) We apply the proposed methodology to evaluate the net-
work robustness of three popular FPS games. The results
manifest that the methodology enables us to summarize
and compare the QoE of the games in different network
scenarios. The analysis results show that the three games
exhibit very different behavior in reaction to network
impairment. In addition, according to the games’ ro-
bustness against delay jitters in either directions, we can
even infer how the time synchronization mechanism is
implemented in each game.
The remainder of this paper is organized as follows. Sec-
tion II describes related works. We present the BTL model,
which is used to extract ratio-scale QoE scores from paired-
comparison results, in Section III. In Section IV, we describe
how we setup the network environments for evaluating the
QoE of FPS games. In Section V, we discuss the effect
of network impairment on the games’ QoE scores and its
implications. Finally, Section VI draws our conclusion.
II. RELATED WORK
A. Game QoE Studies
A number of previous works have been done to assess the
QoE provided by online games in various network situations.
Those studies can be categorized into experimental and obser-
vational studies. Experimental studies estimate games’ QoE
based on users’ perception measures or their game scores
in controlled environments, while observational studies infer
users’ satisfaction degree from real-life traces [9]. In the
following we review the experimental studies as they are
closely related to our work.
Henderson et al. discussed methods of soliciting player
feedback in [14, 15, 19]. Armitage investigated latency
tolerance of players in first-person-shooter games in [4, 5].
Furthermore, experimental studies can be divided into subjec-
tive and objective studies. Subjective studies are mostly based
on MOS scores or descriptive reports about users’ perceptions,
e.g., [7, 22, 25]. For example, Quax et al. evaluated the
influence of small amounts of delay and jitters on Unreal
Tournament 2003 [22]. Objective studies are based on users’
in-game performance, such as the number of kills in shooting
games, the time taken to complete each lap in racing games,
or the capital accumulated in strategy games. For instance,
Beigbeder et al. found that typical ranges of packet loss and
latency do not significantly affect the outcome of the game
Unreal Tournament 2003 [7], while Sheldon et al. concluded
that, overall, high latency has a negligible effect on the
outcome of Warcraft III [25].
B. Studies based on Paired Comparisons
Paired comparison takes advantage of simple comparative
judgements to prioritize a set of stimuli, and is able to quantify
the preferences of the stimuli by adopting probabilistic choice
modeling. Paired comparison is used in various domains,
notably decision making and psychometric testing. Analytic
Hierarchy Process (AHP) [24] is a well-known application
of paired comparison. AHP uses the preference priorities
extracted from paired comparison results to construct a hi-
erarchical framework that can assist people making complex
decisions. Paired comparison is also used in the ranking
of universities [13], rating of celebrities [16], and various
subjective sensation measurement, such as pain [18], sound
quality [11], and taste of food [21].
III. PROBABILISTIC CHOICE MODELING
In the method of paired comparison [12], the basic measure-
ment unit is the comparison of two stimuli. Assume that we
have an experiment composed of t stimuli T1, ..., Tn, thus there
4TABLE III
NETWORK SETTINGS AND THE NUMBER OF TESTS PERFORMED IN OUR
EXPERIMENTS
Settings # Comparisons
Delay 0 ms, 200 ms, 400 ms, uplink 600600 ms, 800 ms, 1000 ms downlink 690
Loss 0%, 10%, 20%, 30% uplink 288downlink 252
Jitter 0 ms, 250 ms, 500 ms uplink 72downlink 78
In our experiments, we use t = 5 seconds to keep the
experiment efficient (in terms of time) and allow sufficient
time for the participants to perceive the game’s smoothness
and interactivity under both network configurations. Note that
we have two participants in a test simply because a minimum
of two players are required to form a deatchmatch game. More
participants can take an experiment at the same time as long
as the game’s design allows (i.e., Alien and Halo both allow
a maximum of 16 players join a game simultaneously).
V. EXPERIMENT RESULTS
In this section, we present our experiment results of the
FPS games’ QoE under different network conditions. First we
summarize the paired comparison results collected from our
experiments. After the results’ consistency is verified, we then
investigate the effect of network delay, loss, and delay jitter
on the QoE of the FPS games.
A. Data Summary
We carry out three sets of experiments. In each of the
experiments, we change network delay, loss rate, and delay
jitter respectively and set the other factors to their respective
ideal settings, i.e., no delay, zero loss rate, and no delay
variations. As network impairment may have different impacts
when it occurs in different links, each set of experiments was
repeated twice with the impairment applied to the uplink and
downlink respectively, where the uplink indicates the network
path from the game client to the server, and the downlink
indicates the path in the opposite direction. We asked a total of
five college students to take the experiments in different time
periods, where each test exactly contains two participants.
A summary of the experiment settings and the number of
tests performed are listed in Table III. The numbers of settings
for the delay, loss, and jitter experiments are 6, 4, and 3,
respectively. These numbers are chosen because we are more
interested in the effect of delay and more settings allow us
to inspect the QoE behavior in more depth. However, more
settings indicate that more tests (i.e., comparisons) are needed
in order to achieve a preference rating with high confidence
(i.e., a narrow confidence band). This is also the reason why
the number of comparisons in the delay experiments is much
higher than that in other experiments.
Before applying the BTL model to analyze the paired com-
parison results, we perform the consistency checks for our data
in order to make sure the participants did not make decisions
arbitrarily. The consistency analysis results are presented in
Table IV. We can see that the numbers of WST, MST, and
SST violations are very small compared with the number of
comparisons. In addition, if we consider both the Kendall’s
TABLE IV
A SUMMARY OF CONSISTENCY CHECK RESULTS OF OUR PAIRED
COMPARISON RESULTS
Game Factor Link WST MST SST Kendall p-BTL
Alien
delay uplink 0 2 7 0.35 0.33downlink 0 0 3 0.62 0.80
loss uplink 0 0 1 0.40 0.12downlink 0 0 0 0.74 0.86
jitter uplink 0 1 1 -0.04 0.26downlink 0 0 0 1.00 0.02
Halo
delay uplink 0 0 6 0.36 0.35downlink 2 2 5 0.53 0.15
loss uplink 0 0 0 0.25 0.98downlink 0 1 3 0.22 0.03
jitter uplink 0 0 0 0.55 0.44downlink 0 1 1 0.32 0.05
UT
delay uplink 0 1 10 0.42 0.01downlink 0 2 5 0.56 0.11
loss uplink 0 1 1 0.28 0.01downlink 0 0 2 0.43 0.17
jitter uplink 0 0 0 1.00 0.02downlink 0 0 0 -0.08 1.00
u-coefficient and the p-value of the goodness-of-fit test for the
BTL model, almost every experiment pass the tests except for
some of the delay jitter experiments. The failure in passing
the consistency check in delay jitter experiments is due to the
participants cannot figure out the difference due to different
jitter settings, which we will explain in Section V-D.
B. Effect of Network Delay
The games’ QoE scores with respect to different network
delays are shown in Fig. 2 in which dot lines stand for 95%
confidence bands of QoE scores. Since the QoE scores are on
a ratio scale, it can be proportionally scaled without losing
the comparative information. To make the plot easier to read,
we normalize the QoE scores on each plot by making the
highest QoE score 1. Therefore, all the QoE scores are within
the range of 0 to 1. Since we include the ideal network
condition in each experiment, without loss of generality, we
can thus assume the best QoE a game can provide is 1, and
inspect how the QoE score degrades due to worse network
conditions. Please note that our methodology does not allow us
to compare the absolute QoE scores across games as we only
have a game’s relative quality in different network scenarios.
Instead, the QoE scores we obtained enable us to observe the
“network robustness” property of a game. That is, how resilient
is a game’s QoE against network impairment. The network
robustness property can be observed from the relationship
between a game’s QoE score and the corresponding network
setting.
Back to Fig. 2, overall, we can see that the effect of delay
is different on the uplink and downlink. From the trend the
uplink delays have a relatively less severe effect on the QoE,
while the downlink delays can easily result in an unacceptable
QoE for all the games. We believe that this discrepancy is due
to the different nature of the data conveyed by uplink and
downlink packets.
Uplink packets primarily contain a player’s inputs to the
server; thus, a longer delay would consistently lead to a longer
command response time and thus a lower interactivity. Since
game clients can provide immediate feedback of a player’s
inputs on his/her screen, the impact of uplink delay can be
somewhat eliminated. However, for the player’s actions that
make changes to the environment or other characters, we
still need to wait for the corresponding responses from the
6downlink jitters. However, considering uplink bandwidth is
usually more restricted than downlink bandwidth in users’
access networks, the robustness against uplink delay variability
seems more important than that against downlink jitters. From
this perspective, Alien’s design is better than the other two
games. Although the design of game architecture is not our
goal in this work, here we demonstrate that our methodology
can be a helpful tool for decision making between design
alternatives.
VI. CONCLUSION AND FUTURE WORK
From our experiment results in Section V, it may be
mistaken that Halo is better than Alien and UT in terms of their
network design and performance. In fact, such conclusions are
difficult, if not impossible, to make because the requirement
for network support of different games can be very different
due to their variety in game design, scene complexity, game
pace, game rules, playing strategy, and so on.
More concretely, according to our experiment participants,
Halo’s game pace is significantly slower than Alien and UT.
Also, the scene complexity and special effects in Alien is far
more sophisticated than those in the other two. The special
effects such as weapon firing and bullet flying are impressive
in Alien. UT also has splendid special effects, while Halo only
provides relatively primitive effects. Therefore, it is reasonable
that Halo exhibits the best robustness to network impairments
since it scene complexity is the lowest and therefore it should
have the least requirement for network data delivery.
If we take the degree of the games’ sophistication into ac-
count, we consider that the robustness of UT against downlink
delays should able be improved compared with that of Alien
and Halo (cf. Fig. 2). Although currently our methodology
does not provide a numeric metric for a game’s network
performance, we have shown in Section V-D that it can be
a helpful tool for making decisions regarding network design
and functionalities, such as how a dead reckoning algorithm
should be designed and where the time synchronization mech-
anism should be implemented.
In the future, we are to continue our studies on the network
robustness of online games. First, we aim to conduct more
experiments and summarize the similarity of and difference
between games of the same genre. Next, we will expand our
study scope and include the comparison of different game
genres into consideration. We will target a goal to understand
the general requirement for network support of different game
genres, and in consequence derive a network requirement
profile for each of the game genres.
REFERENCES
[1] “Alien Arena.” [Online]. Available:
http://icculus.org/alienarena/rpa/aquire.html
[2] “Halo Combat Evolved.” [Online]. Available:
http://halo.wikia.com/wiki/Halo: Combat Evolved
[3] “Unreal Tournament 2004.” [Online]. Available:
http://www.unrealtournament2003.com/ut2004/
[4] G. Armitage, “An experimental estimation of latency sensitivity
in multiplayer quake 3,” in The 11th IEEE International Con-
ference on Networks, 2003, pp. 137–141.
[5] G. Armitage and L. Stewart, “Limitations of using real-world,
public servers to estimate jitter tolerance of first person shooter
games,” in Proceedings of ACM SIGCHI ACE 2004 Conference,
2004, pp. 257–262.
[6] N. E. Baughman and B. N. Levine, “Cheat-proof playout for
centralized and distributed online games,” in Proceedings of
IEEE INFOCOM 2001, Anchorage, AK, Apr. 2001.
[7] T. Beigbeder, R. Coughlan, C. Lusher, J. Plunkett, E. Agu,
and M. Claypool, “The effects of loss and latency on user
performance in Unreal Tournament 2003,” in Proceedings of
NetGames’04. ACM Press, 2004, pp. 144–151.
[8] R. A. Bradley and M. E. Terry, “Rank analysis of incom-
plete block designs: I. the method of paired comparisons,”
Biometrika, vol. 39, no. 3/4, pp. 324–345, 1952.
[9] K.-T. Chen, P. Huang, and C.-L. Lei, “Effect of network
quality on player departure behavior in online games,” IEEE
Transactions on Parallel and Distributed Systems, vol. 20, no. 5,
pp. 593–606, May 2009.
[10] K.-T. Chen, C.-C. Wu, Y.-C. Chang, and C.-L. Lei, “A crowd-
sourceable qoe evaluation framework for multimedia content,”
in Proceedings of ACM Multimedia 2009, 2009.
[11] S. Choisel and F. Wickelmaier, “Evaluation of multichannel re-
produced sound: Scaling auditory attributes underlying listener
preference,” The Journal of the Acoustical Society of America,
vol. 121, no. 1, pp. 388–400, 2007.
[12] H. A. David, The Method of Paired Comparisons. Oxford
University Press, 1988.
[13] R. Dittrich, R. Hatzinger, and W. Katzenbeisser, “Modelling the
effect of subject-specific covariates in paired comparison studies
with an application to university rankings,” Journal of the Royal
Statistical Society (Series C): Applied Statistics, vol. 47, no. 4,
pp. 511–525, 1998.
[14] T. Henderson, “Latency and user behaviour on a multi-
player game server,” in Proceedings of the Third International
COST264 Workshop on Networked Group Communication.
Springer-Verlag, 2001, pp. 1–13.
[15] T. Henderson and S. Bhatti, “Modelling user behaviour in net-
worked games,” in Proceedings of the ninth ACM international
conference on Multimedia. ACM, 2001, pp. 212–220.
[16] C. L. Knott and M. S. James, “An alternate approach to devel-
oping a total celebrity endorser rating model using the analytic
hierarchy process,” International Transactions in Operational
Research, vol. 11, no. 1, pp. 87–95, 2004.
[17] R. D. Luce, Individual Choice Behavior: A Theoretical Analysis.
New York: Wiley, 1959.
[18] J. N. S. Matthews and K. P. Morris, “An application of bradley-
terry-type models to the measurement of pain,” Applied Statis-
tics, vol. 44, pp. 243–255, 1995.
[19] M. Oliveira and T. Henderson, “What online gamers really
think of the internet?” in Proceedings of the 2nd workshop on
Network and system support for games. ACM, 2003, pp. 185–
193.
[20] L. Pantel and L. Wolf, “On the suitability of dead reckoning
schemes for games,” in Proceedings of NetGames’09, 2002, pp.
79–84.
[21] N. L. Powers and R. M. Pangborn, “Paired comparison and
time-intensity measurements of the sensory properties of bever-
ages and gelatins containing sucrose or synthetic sweeteners,”
Journal of Food Science, vol. 43, no. 1, pp. 41–46, 1978.
[22] P. Quax, P. Monsieurs, W. Lamotte, D. D. Vleeschauwer,
and N. Degrande, “Objective and subjective evaluation of the
influence of small amounts of delay and jitter on a recent first
person shooter game,” in Proceedings of ACM SIGCOMM 2004
workshops on NetGames ’04. ACM Press, 2004, pp. 152–156.
[23] P. Rossi, Z. Gilula, and G. Allenby, “Overcoming Scale Usage
Heterogeneity: A Bayesian Hierarchical Approach,” Journal of
the American Statistical Association, vol. 96, no. 453, pp. 20–
31, 2001.
[24] T. L. Saaty, “A scaling method for priorities in hierarchical
structures,” Journal of Mathematical Psychology, vol. 15, no. 3,
pp. 234–281, 1977.
[25] N. Sheldon, E. Girard, S. Borg, M. Claypool, and E. Agu,
“The effect of latency on user performance in Warcraft III,”
in Proceedings of NetGames’03. ACM Press, 2003, pp. 3–14.
  
除了上述子會議，另有十九個 tutorial 和十個 workshop，以及在展示區所展示的通訊界中目前最熱
門的主題與最新的發展。根據大會的會後統計，今年共有 2,020 位來自學界與業界的人士註冊且
參與會議。 
 
    今年的 ICC 選擇在德國東部的德勒斯登 (Dresden) 舉辦，這是有著八百年歷史的城市，在二
次世界大戰中被炸毀的宮殿和教堂陸續重建完成，吸引無數觀光客前來。在龐大資金的挹注之下，
德勒斯登地區已經成為歐洲的半導體產業重鎮，這個新與舊良好並存的城市確實是舉辦國際會議
的最佳地點之一。會議是在易北河畔並且緊鄰古蹟區的 International Congress Center Dresden (ICD) 
進行，這是一個極具設計感且現代化的會議中心，內部相當龐大，除了最大的主演講廳可容納多
達 4,150 人之外，另有五間可容納 300 到 600 人的演講廳，以及十五間中型或小型研討室。大會
的十一個子會議在會場中的不同演講廳或研討室同時進行，其中幾個比較大型的子會議甚至分成
二到三個 session 在不同演講廳同時進行。 
會議第一天在主演講廳開幕，任教於德勒斯登科技大學且服務於 Vodafone 的主持人 Gerhard 
Fettweis 博士開場之後，分別由 T-Mobile International AG 的執行長 Hamid Akhavan 先生和德國薩
克森邦的經濟與勞工部長 Thomas Jurk 先生致詞，緊接著由來自通訊產業的重量級人士進行座談
會，暢談新科技可能帶來的風險，以及新科技為顧客和業者帶來的利益和功效，另外也將話題放
在顧客如何在成本、便利性和複雜度之間尋找平衡點。雖然此座談會的主題比較偏向業界，但是
對於尚未接觸過業界，一直在學校做學術研究的我來說，這是一個了解學界與業界間不同看法的
好機會。此外，我也是第一次在大型國際研討會中參加這類型的座談會，更難得的是座談人士都
是各大通訊或網路公司的技術執行長，因此是一個非常寶貴的經驗。 
 
    座談會結束後，經過三十分鐘的休息就開始論文的口頭報告，因為十一個子會議是同時在不
同演講廳或研討室進行，與會者視自己的興趣或是熱門主題選擇聽不同場次的論文報告，許多與
會者事先勾選對於哪些子會議的哪些 session 有興趣，然後中場休息換場聽講，也有不少人在同一
個 session 的之中，到處聽不同子會議的報告，等於是穿梭全場。在第一個時段裡，我選擇到
Communication Theory Symposium 聽講，這個 session 的主題是 relaying，主要是在無線網路環境
中如何有效經由中繼節點進行通訊，由於中繼節點是額外的訊號源，因此無可避免地增加傳輸路
徑上的干擾訊號，因此這個 session 的論文主要是提出有效的通道編碼方法，解決來源端到中繼節
點及中繼節點到終點端的訊號傳輸錯誤的問題，值得一提的是，這個 session 的其中一篇論文獲得
這個子會議的最佳論文獎。 
 
    下午分成兩個時段，第一個時段我到 Communications QoS, Reliability and Modelling 
Symposium 聽 resource allocation and scheduling 這個 session，論文的主題主要是網路環境中資源的
有效和公平分配及資源的排程，例如在 connection-less 的網路環境上公平的分配頻寬、網路封包
交換器達到差別性服務品質並同時節省電力的方法、以及在多人的網路多媒體傳輸資源分配上的
競爭合作策略。不過這個 session 有多達三位講者是代替論文原作者前來報告，因此在報告後的問
與答時有少數的問題無法提供更詳細的解答，但原則上還是都有解答發問者的問題所在。 
 
    第二個時段我選擇到 Symposium on Selected Areas in Communications 聽講，這個 session 的主
題是 networked service，其中令我印象比較深刻的報告是關於 3D 虛擬實境之 image-based 串流協
    進入第三天的會議，我決定參加在比較大型演講廳舉行的子會議，這些子會議都是關於無線
網路的領域，同時會有兩到三個 session 在不同演講廳進行。雖然我的研究主題不是在這方面，不
過可以藉由聽取報告了解這些領域有什麼樣的發展以及目前熱門的問題。也因為更多聽眾的參
與，講者與聽眾之間的問答更為精采，聽眾的問題直指核心，講者的回答也一針見血。這次會議
的主要議程在這天畫下句點。 
 
2. 與會心得 
這次有機會參與 IEEE ICC 這樣大型的研討會，不管是在國際見聞或者是學術交流的過程，都有極
為豐沛的收穫與心得。由於會議參與者的學術涵養都十分豐富，聆聽與自己學術研究領域相關的
論文發表，更是對日後的學術思考與研究方向的確定都有很大的助益。且此次在會中的口頭報告
所得到的建議與可能的改善方向，提供我在這個研究主題上的延伸議題，對於日後的研究有著正
面的幫助，在外語能力的溝通上更是有長足的進步。這次的會議給我相當大的收穫，不管是研究
主題的多元性與深度，是很難在短時間內在網路上瀏覽大量論文就可以獲得，顯見參與國際研討
會在研究上具有其重要性。 
     
 
式繕打。（Please attach this form to your report；use additional A4 blank pages if needed. ） 
會後報告 Report  

課程結束，我去聽 1#. 的 5.2,62,.5，1#. 是一個特別的 ，
專門收入顛覆傳統想法或是在一般會議較難被接受的 %%，例如，第一組的 % 的
就是一種有趣存取短片的方式，利用點火柴給人印象，把攝影機鑲在火柴棒裡，燒一頭可
以錄影，另一頭則撥放影片，把這樣的火柴棒當作禮物送人，收到的人真的可以從這樣的
互動中 .,，只可惜它只能用一次，不過真的是種全新的感受。 結束，我
透過一位台大教授介紹認識另一位在新加坡大學任教的教授，巧得是我們白天才一起上過
01 的課，他有做跟兒童 ),# 相關的研究，我跟他稍微介紹我們的 %，
他似乎很有興趣。這天晚上許多再會中認識的教授和學生一起參加 #%，周圍有些
與  相關的展覽。
 
 月 7 日 早上聽了  在  領域中關於兒童議題的 %。
+
 開始是我
% 展覽的時間，一路解說到 	 點多。這段時間有很多人來看我們的 %，有些人
提出實作的問題還有提出有參考價值的建議與其它 #。展覽結束我到 *-
, 的 ，剛好趕上最後一個 %，關於 /-(-, 想法，他們還加
入 ##% 的觀念，我覺得很有趣。中午和幾位 8 和台大的教授與學生一起用餐，同
桌的有一位台科大工業設計系的教授，因而了解更多  中的 0- 層面，之後和幾位
台灣來的老師學生們一起去幾個看幾個 , 和 8 的景點。晚上幾乎所有台灣來參
與會議的人都一起去參觀 8 的 2 和 2)。幾位台灣在 8 的生幫我們介紹
這兩個 % 的建築，他們還很熱心的 % 各自 ) 著名或進行中的
%9#。這一天我們在 8 待到很晚才各自回住的地方。

 月  日 早上聽完 :-;%# 後，下午去參觀 (( 的
-(。(( 是一家設計公司，公司的人員很仔細的介紹一些 -##%
與他們的 :*。從餐與會議到參觀設計公司，我發現同樣是在探討 ，工程和設計的角
度有非常大的分歧。離開 ((，晚上大家又聚在一起吃龍蝦大餐，來波士頓必點的
海鮮。吃過飯幾位老師和學生一起到會議中心旁的  (8: 看夜景，一同前來的
還有一位在早稻田唸書的  .10 學生，大家聊了會議裡的 、中日文化和學術環境
的事。

 月  日早上到 ## 會場逛逛後，中午搭巴士前往紐約，晚上在紐約 #/ 和幾
位去年在資科所工作的同事吃飯。隔天搭機回台灣。這趟會議的每一個行程幾乎都是意外
的收穫，不但讓我對  這個領域大開眼界，也更了解自己的特質。
 
 
填表人簽章 signature                         填表日期 Date：                                                           
96年度專題研究計畫研究成果彙整表 
計畫主持人：陳昇瑋 計畫編號：96-2628-E-001-027-MY3 
計畫名稱：Robust Peer-to-Peer Networks and Their Applications on Real-time Multimedia--
高品質的覆疊式多媒體串流傳輸方法研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 1 1 20%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 2 2 80% 
人次 
 
期刊論文 5 5 100%  
研究報告/技術報告 0 0 100%  
研討會論文 14 14 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
