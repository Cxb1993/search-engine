 
where w is the weight vector and b is an intercept, or called the bias. A nonlinear
classifier maps each instance x to a higher dimensional vector φ(x) if data are not
linearly separable. If φ(x) = x (i.e., data points are not mapped), we say (1) is
a linear classifier. Because nonlinear classifiers use more features, generally they
perform better than linear classifiers in terms of prediction accuracy.
For nonlinear classification, evaluating wTφ(x) can be expensive because φ(x)
may be very high dimensional. Kernel methods by Cortes and Vapnik (1995) and
other researchers were introduced to handle such a difficulty. If w is a linear
combination of training data, i.e.,
w ≡
l∑
i=1
αiφ(xi) for some α ∈ Rl, (2)
and the following kernel function can be easily calculated
K(xi,xj) ≡ φ(xi)Tφ(xj),
then the decision function can be calculated by
d(x) ≡
l∑
i=1
αiK(xi,x) + b, (3)
regardless of the dimensionality of φ(x). For example,
K(xi,xj) ≡ (xTi xj + 1)2 (4)
is the degree-2 polynomial kernel with
φ(x) = [1,
√
2x1, . . . ,
√
2xn, . . . x
2
1, . . . x
2
n, (5)√
2x1x2,
√
2x1x3, . . . ,
√
2xn−1xn] ∈ R(n+2)(n+1)/2.
This kernel trick makes methods such as support vector machines (SVM) or kernel
logistic regression practical and popular; however, for large data, the training
and testing processes are still time consuming. For a kernel like (4), the cost of
predicting a testing instance x via (3) can be up to O(ln). In contrast, without
using kernels, w is available in an explicit form, so we can predict an instance by
(1). With φ(x) = x,
wTφ(x) = wTx
costs only O(n). It is also known that training a linear classifier is more efficient.
Therefore, while a linear classifier may give inferior accuracy, it often enjoys faster
training and testing.
2
3.2 Large-scale L1-regularized Linear Classification
Regularized linear classification often takes the following form
min
w
1
2
‖w‖2 + C
l∑
i=1
ξ(w;xi, yi),
where ξ(w;xi, yi) is a non-negative (convex) loss function. The regularization
term ‖w‖2 is used to avoid overfitting the training data. The user-defined param-
eter C > 0 is used to balance the regularization and loss terms.
Recently, L1-regularized classifiers have attracted considerable attention be-
cause they can be used to obtain a sparse model. It solves the following optimiza-
tion problem.
min
w
‖w‖1 + C
l∑
i=1
ξ(w;xi, yi),
In Yuan et al. (2010), we conduct a complete comparison of existing packages for
large-scale L1-regularized classifiers. This paper is so far the most authoritative
comparison in this topic.
In Yuan et al. (2010), we also proposed a a carefully designed coordinate de-
scent implementation CDN and included it in the software LIBLINEAR. We con-
cluded that it is the fastest among state-of-the-art solvers. However, subsequently
we found out that CDN is less competitive on loss functions that are expensive to
compute. In another paper (Yuan et al., 2012a), we showed that CDN for logistic
regression is much slower than CDN for SVM because the logistic loss involves
expensive exp/log operations.
In optimization, Newton methods are known to have fewer iterations although
each iteration costs more. Because solving the Newton sub-problem is indepen-
dent of the loss calculation, this type of methods may surpass CDN under some
circumstances. In L1-regularized classification, GLMNET by Friedman et al. is
already a Newton-type method, but experiments in Yuan et al. (2010) indicated
that the existing GLMNET implementation may face difficulties for some large-
scale problems. In Yuan et al. (2012a), we propose an improved GLMNET to
address some theoretical and implementation issues. In particular, as a Newton-
type method, GLMNET achieves fast local convergence, but may fail to quickly
obtain a useful solution. By a careful design to adjust the effort for each itera-
tion, our method is efficient for both loosely or strictly solving the optimization
problem. Experiments demonstrate that our improved GLMNET is more efficient
than CDN for L1-regularized logistic regression.
4
References
A. L. Berger, V. J. Della Pietra, and S. A. Della Pietra. A maximum entropy
approach to natural language processing. Computational Linguistics, 22(1):
39–71, 1996.
Y.-W. Chang, C.-J. Hsieh, K.-W. Chang, M. Ringgaard, and C.-J. Lin. Training
and testing low-degree polynomial data mappings via linear SVM. Journal of
Machine Learning Research, 11:1471–1490, 2010. URL http://www.csie.ntu.
edu.tw/~cjlin/papers/lowpoly_journal.pdf.
C. Cortes and V. Vapnik. Support-vector network. Machine Learning, 20:273–297,
1995.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLIN-
EAR: A library for large linear classification. Journal of Machine Learning
Research, 9:1871–1874, 2008. URL http://www.csie.ntu.edu.tw/~cjlin/
papers/liblinear.pdf.
J. H. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized
linear models via coordinate descent. Journal of Statistical Software, 33(1):1–22,
2010.
Z. S. Harris. Distributional structure. Word, 10:146–162, 1954.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and S. Sundararajan. A
dual coordinate descent method for large-scale linear SVM. In Proceedings of
the Twenty Fifth International Conference on Machine Learning (ICML), 2008.
URL http://www.csie.ntu.edu.tw/~cjlin/papers/cddual.pdf.
F.-L. Huang, C.-J. Hsieh, K.-W. Chang, and C.-J. Lin. Iterative scaling and
coordinate descent methods for maximum entropy. Journal of Machine Learn-
ing Research, 11:815–848, 2010. URL http://www.csie.ntu.edu.tw/~cjlin/
papers/maxent_journal.pdf.
H.-F. Yu, F.-L. Huang, and C.-J. Lin. Dual coordinate descent methods for logistic
regression and maximum entropy models. Machine Learning, 85(1-2):41–75,
October 2011. URL http://www.csie.ntu.edu.tw/~cjlin/papers/maxent_
dual.pdf.
6
Appendix: G.-X. Yuan, C.-H. Ho, and C.-J. Lin. Recent
advances of large-scale linear classification. Proceedings of
IEEE, 2012. To appear.
8
2is the degree-2 polynomial kernel with
φ(x) = [1,
√
2x1, . . . ,
√
2xn, . . . x
2
1, . . . x
2
n, (5)√
2x1x2,
√
2x1x3, . . . ,
√
2xn−1xn] ∈ R(n+2)(n+1)/2.
This kernel trick makes methods such as SVM or kernel LR
practical and popular; however, for large data, the training and
testing processes are still time consuming. For a kernel like
(4), the cost of predicting a testing instance x via (3) can be
up to O(ln). In contrast, without using kernels, w is available
in an explicit form, so we can predict an instance by (1). With
φ(x) = x,
wTφ(x) = wTx
costs only O(n). It is also known that training a linear classifier
is more efficient. Therefore, while a linear classifier may give
inferior accuracy, it often enjoys faster training and testing.
We conduct an experiment to compare linear SVM and
nonlinear SVM (with the RBF kernel). Table I shows the accu-
racy and training/testing time. Generally, nonlinear SVM has
better accuracy, especially for problems cod-RNA,2 ijcnn1,
covtype, webspam, and MNIST38. This result is consistent
with the theoretical proof that SVM with RBF kernel and
suitable parameters gives at least as good accuracy as linear
kernel [10]. However, for problems with large numbers of
features, i.e. real-sim, rcv1, astro-physic, yahoo-japan, and
news20, the accuracy values of linear and nonlinear SVMs
are similar. Regarding training and testing time, Table I
clearly indicates that linear classifiers are at least an order
of magnitude faster.
In Table I, problems for which linear classifiers yield com-
parable accuracy to nonlinear are all document sets. In the area
of document classification and natural language processing
(NLP), a bag-of-word model is commonly used to generate
feature vectors [11]. Each feature, corresponding to a word,
indicates the existence of the word in a document. Because
the number of features is the same as the number of possible
words, the dimensionality is huge and the data set is often
sparse. For this type of large sparse data, linear classifiers
are very useful because of competitive accuracy and very fast
training and testing.
III. BINARY LINEAR CLASSIFICATION METHODS
To generate a decision function (1), linear classification
involves the following risk minimization problem.
min
w,b
f(w, b) ≡ r(w) + C
l∑
i=1
ξ(w, b;xi, yi), (6)
where r(w) is the regularization term and ξ(w, b;x, y) is the
loss function associated with the observation (y,x). Parameter
C > 0 is user-specified for balancing r(w) and the sum of
losses.
Following the discussion in Section II, linear classification
is often applied to data with many features, so the bias term b
may not be needed in practice. Experiments in [12], [13] on
document data sets showed similar performances with/without
2In this experiment, we scaled cod-RNA feature-wisely to [−1, 1] interval.
−ywTx
ξ(w;x, y)
ξL1
ξL2
ξLR
Fig. 1. Three loss functions: ξL1, ξL2, and ξLR. The x-axis is −ywTx.
the bias term. In the rest of this paper, we omit the bias term
b, so (6) is simplified to
min
w
f(w) ≡ r(w) + C
l∑
i=1
ξ(w;xi, yi) (7)
and the decision function becomes d(x) ≡ wTx.
A. Support Vector Machines and Logistic Regression
In (7), the loss function is used to penalize a wrongly-
classified observation (x, y). There are three common loss
functions considered in the literature of linear classification.
ξL1(w;x, y) ≡ max(0, 1− ywTx), (8)
ξL2(w;x, y) ≡ max(0, 1− ywTx)2, and (9)
ξLR(w;x, y) ≡ log(1 + e−ywTx). (10)
Eqs. (8) and (9) are referred to as L1 and L2 losses, respec-
tively. Problem (7) using (8) and (9) as the loss function is
often called L1-loss and L2-loss SVM, while problem (7)
using (10) is referred to as logistic regression (LR). Both
SVM and LR are popular classification methods. The three
loss functions in (8)–(10) are all convex and non-negative. L1
loss is not differentiable at the point ywTx = 1, while L2 loss
is differentiable, but not twice differentiable [14]. For logistic
loss, it is twice differentiable. Figure 1 shows that these three
losses are increasing functions of −ywTx. They slightly differ
in the amount of penalty imposed.
B. L1 and L2 Regularization
A classifier is used to predict the label y for a hidden
(testing) instance x. Overfitting training data to minimize the
training loss may not imply that the classifier gives the best
testing accuracy. The concept of regularization is introduced
to prevent from overfitting observations. The following L2 and
L1 regularization terms are commonly used.
rL2(w) ≡ 1
2
‖w‖22 =
1
2
n∑
j=1
w2j and (11)
rL1(w) ≡ ‖w‖1 =
n∑
j=1
|wj |. (12)
4• Solving primal or dual problems Problem (7) has n
variables. In some applications, the number of instances
l is much smaller than the number of features n. By
Lagrangian duality, a dual problem of (7) has l variables.
If l  n, solving the dual form may be easier due to the
smaller number of variables. Further, in some situations, the
dual problem possesses nice properties not in the primal
form. For example, the dual problem of the standard SVM
(L2-regularized L1-loss SVM) is the following quadratic
program.3
min
α
fD(α) ≡ 1
2
αTQα− eTα
subject to 0 ≤ αi ≤ C, ∀i = 1, . . . , l,
(16)
where Qij ≡ yiyjxTi xj . Although the primal objective
function is non-differentiable because of the L1 loss, in (16),
the dual objective function is smooth (i.e., derivatives of all
orders are available). Hence, solving the dual problem may
be easier than primal because we can apply differentiable
optimization techniques. Note that the primal optimal w
and the dual optimal α satisfy the relationship (2),4 so
solving primal and dual problems leads to the same decision
function.
Dual problems come with another nice property that each
variable αi corresponds to a training instance (yi,xi). In
contrast, for primal problems, each variable wi corresponds
to a feature. Optimization methods which update some
variables at a time often need to access the corresponding
instances (if solving dual) or the corresponding features
(if solving primal). In practical applications, instance-wise
data storage is more common than feature-wise storage.
Therefore, a dual-based algorithm can directly work on the
input data without any transformation.
Unfortunately, the dual form may not be always easier
to solve. For example, the dual form of L1-regularized
problems involves general linear constraints rather than
bound constraints in (16), so solving primal may be easier.
• Using low-order or high-order information Low-order
methods, such as gradient or sub-gradient methods, have
been widely considered in large-scale training. They charac-
terize low-cost update, low-memory requirement, and slow
convergence. In classification tasks, slow convergence may
not be a serious concern because a loose solution of (7)
may already give similar testing performances to that by an
accurate solution.
High-order methods such as Newton methods often require
the smoothness of the optimization problems. Further, the
cost per step is more expensive; sometimes a linear system
must be solved. However, their convergence rate is superior.
These high-order methods are useful for applications need-
ing an accurate solution of problem (7). Some (e.g., [20])
have tried a hybrid setting by using low-order methods in
3Because the bias term b is not considered, therefore, different from the dual
problem considered in SVM literature, an inequality constraint
∑
yiαi = 0
is absent from (16).
4However, we do not necessarily need the dual problem to get (2). For
example, the reduced SVM [19] directly assumes that w is the linear
combination of a subset of data.
the beginning and switching to higher-order methods in the
end.
• Cost of different types of operations In a real-world
computer, not all types of operations cost equally. For
example, exponential and logarithmic operations are much
more expensive than multiplication and division. For train-
ing large-scale LR, because exp/log operations are required,
the cost of this type of operations may accumulate faster
than that of other types. An optimization method which can
avoid intensive exp/log evaluations is potentially efficient;
see more discussion in, for example, [12], [21], [22].
• Parallelization Most existing training algorithms are inher-
ently sequential, but a parallel algorithm can make good use
of the computational power in a multi-core machine or a dis-
tributed system. However, the communication cost between
different cores or nodes may become a new bottleneck. See
more discussion in Section VII.
Earlier developments of optimization methods for linear
classification tend to focus on data with few features. By taking
this property, they are able to easily train millions of instances
[23]. However, these algorithms may not be suitable for sparse
data with both large numbers of instances and features, for
which we show in Section II that linear classifiers often
give competitive accuracy with nonlinear classifiers. Many
recent studies have proposed algorithms for such data. We
list some of them (and their software name if any) according
to regularization and loss functions used.
• L2-regularized L1-loss SVM: Available approaches in-
clude, for example, cutting plane methods for the primal
form (SVMperf [4], OCAS [24], and BMRM [25]), a
stochastic (sub-)gradient descent method for the primal form
(Pegasos [5] and SGD [26]), and a coordinate descent
method for the dual form (LIBLINEAR [6]).
• L2-regularized L2-loss SVM: Existing methods for the
primal form include a coordinate descent method [21], a
Newton method [27], and a trust region Newton method
(LIBLINEAR [28]). For the dual problem, a coordinate
descent method is in the software LIBLINEAR [6].
• L2-regularized LR: Most unconstrained optimization
methods can be applied to solve the primal problem. An
early comparison on small-scale data is [29]. Existing stud-
ies for large sparse data include iterative scaling methods
[12], [30], [31], a truncated Newton method [32], and a
trust region Newton method (LIBLINEAR [28]). Few works
solve the dual problem. One example is a coordinate descent
method (LIBLINEAR [33]).
• L1-regularized L1-loss SVM: It seems no studies have
applied L1-regularized L1-loss SVM on large sparse data
although some early works for data with either few features
or few instances are available [34]–[36].
• L1-regularized L2-loss SVM: Some proposed methods
include a coordinate descent method (LIBLINEAR [13]) and
a Newton-type method [22].
• L1-regularized LR: Most methods solve the primal form,
for example, an interior-point method (l1 logreg [37]),
(block) coordinate descent methods (BBR [38] and CGD
[39]), a quasi-Newton method (OWL-QN [40]), Newton-
6ALGORITHM 2: TRON for L2-regularized LR and L2-
loss SVM [28]
1. Given w, ∆, and σ0.
2. For k = 1, 2, 3, . . .
(a) Find an approximate solution d of (21) by the
conjugate gradient method.
(b) Check the ratio σ in (22).
(c) If σ > σ0
w ← w + d.
(d) Adjust ∆ according to σ.
D. Example: Solving Dual SVM by Coordinate Descent Meth-
ods (Dual-CD)
Hsieh et al. [6] proposed a coordinate descent method
for the dual L2-regularized linear SVM in (16). We call
this algorithm Dual-CD. Here, we focus on L1-loss SVM,
although the same method has been applied to L2-loss SVM
in [6].
A coordinate descent method sequentially selects one vari-
able for update and fixes others. To update the ith variable,
the following one-variable problem is solved.
min
d
fD(α+ dei)− fD(α)
subject to 0 ≤ αi + d ≤ C,
where f(α) is defined in (16), ei = [0, . . . , 0︸ ︷︷ ︸
i−1
, 1, 0, . . . , 0]T ,
and
fD(α+ dei)− fD(α) = 1
2
Qiid
2 +∇ifD(α)d.
This simple quadratic function can be easily minimized. After
considering the constraint, a simple update rule for αi is
αi ← min(max(αi − ∇if
D(α)
Qii
, 0), C). (24)
From (24), Qii and ∇ifD(α) are our needs. The diagonal
entries of Q, Qii,∀i, are computed only once initially, but
∇ifD(α) = (Qα)i − 1 =
l∑
t=1
(yiytx
T
i xt)αt − 1 (25)
requires O(nl) cost for l inner products xTi xt,∀t = 1, . . . , l.
To make coordinate descent methods viable for large linear
classification, a crucial step is to maintain
u ≡
l∑
t=1
ytαtxt, (26)
so that (25) becomes
∇ifD(α) = (Qα)i − 1 = yiuTxi − 1. (27)
If u is available through the training process, then the cost
O(nl) in (25) is significantly reduced to O(n). The remaining
task is to maintain u. Following (26), if α¯i and αi are values
before and after the update (24), respectively, then we can
easily maintain u by the following O(n) operation.
u← u+ yi(αi − α¯i)xi. (28)
ALGORITHM 3: A coordinate descent method for L2-
regularized L1-loss SVM [6]
1. Given α and the corresponding u =
∑l
i=1 yiαixi.
2. Compute Qii,∀i = 1, . . . , l.
3. For k = 1, 2, 3, . . .
• For i = 1, . . . , l
(a) Compute G = yiuTxi − 1 in (27).
(b) α¯i ← αi.
(c) αi ← min(max(αi −G/Qii, 0), C).
(d) u← u+ yi(αi − α¯i)xi.
Therefore, the total cost for updating an αi is O(n). The
overall procedure of the coordinate descent method is in
Algorithm 3.
The vector u defined in (26) is in the same form as w in
(2). In fact, as α approaches a dual optimal solution, u will
converge to the primal optimal w following the primal-dual
relationship.
The linear convergence of Algorithm 3 is established in
[6] using techniques in [46]. They propose two implemen-
tation tricks to speed up the convergence. First, instead of a
sequential update, they repeatedly permute {1, . . . , l} to decide
the order. Second, similar to the shrinking technique used
in training nonlinear SVM [47], they identify some bounded
variables which may already be optimal and remove them
during the optimization procedure. Experiments in [6] show
that for large sparse data, Algorithm 3 is much faster than
TRON in the early stage. However, it is less competitive if
the parameter C is large.
Algorithm 3 is very related to popular decomposition meth-
ods used in training nonlinear SVM (e.g., [8], [47]). These
decomposition methods also update very few variables at
each step, but use more sophisticated schemes for selecting
variables. The main difference is that for linear SVM, we can
define u in (26) because xi, ∀i are available. For nonlinear
SVM, ∇ifD(w) in (25) needs O(nl) cost for calculating l
kernel elements. This difference between O(n) and O(nl) is
similar to that in the testing phase discussed in Section II.
E. Example: Solving L1-regularized Problems by Combining
Newton and Coordinate Descent Methods (newGLMNET)
GLMNET proposed by Friedman et al. [41] is a Newton
method for L1-regularized minimization. An improved version
newGLMNET [22] is proposed for large-scale training.
Because the 1-norm term is not differentiable, we represent
f(w) as the sum of two terms ‖w‖1 + L(w), where
L(w) ≡ C
l∑
i=1
ξ(w;xi, yi).
At each iteration, newGLMNET considers the second-order
approximation of L(w) and solves the following problem.
min
d
q(d) ≡ ‖w + d‖1 − ‖w‖1 +∇L(w)Td+ 1
2
dTHd,
(29)
8After obtaining all k models, we say an instance x is in the
mth class if the decision value (1) of the mth model is the
largest, i.e.,
class of x ≡ arg max
m=1,...,k
wTmx. (32)
The cost for testing an instance is O(nk).
• One-against-one method One-against-one method [55]
solves k(k − 1)/2 binary problems. Each binary classifier
constructs a model with data from one class as positive
and another class as negative. Since there are k(k − 1)/2
combination of two classes, k(k − 1)/2 weight vectors are
constructed: w1,2, w1,3, . . . , w1,k, w2,3, . . . , w(k−1),k.
There are different methods for testing. One approach is by
voting [56]. For a testing instance x, if model (i, j) predicts
x as in the ith class, then a counter for the ith class is added
by one; otherwise, the counter for the jth class is added.
Then we say x is in the ith class if the ith counter has the
largest value. Other prediction methods are similar though
they differ in how to use the k(k − 1)/2 decision values;
see some examples in [52], [53].
For linear classifiers, one-against-one method is shown
to give better testing accuracy than one-against-rest [57].
However, it requires O(k2n) spaces for storing models and
O(k2n) cost for testing an instance; both are more expensive
than the one-against-rest method. Interestingly, for nonlinear
classifiers via kernels, one-against-one method does not
have such disadvantages [50].
DAGSVM [58] is the same as one-against-one but it at-
tempts to reduce the testing cost. Starting with a candidate
set of all classes, this method sequentially selects a pair of
classes for prediction and removes one of the two. That is,
if a binary classifier of class i and j predicts i, then j is
removed from the candidate set. Alternatively, a prediction
of class j will cause i to be removed. Finally, the only
remained class is the predicted result. For any pair (i, j)
considered, the true class may be neither i nor j. However,
it does not matter which one is removed because all we need
is that if the true class is involved in a binary prediction,
it is the winner. Because classes are sequentially removed,
only k−1 models are used. The testing time complexity of
DAGSVM is thus O(nk).
B. Considering All Data at Once
In contrast to using many binary models, some have pro-
posed solving a single optimization problem for multi-class
classification [59]–[61]. Here we discuss details of Crammer
and Singer’s approach [60]. Assume class labels are 1, . . . , k.
They consider the following optimization problem:
min
w1,...,wk
1
2
k∑
m=1
‖wm‖22 + C
l∑
i=1
ξCS({wm}km=1;xi, yi),
(33)
where
ξCS({wm}km=1;x, y) ≡ max
m 6=y
max(0, 1− (wy −wm)Tx).
(34)
The setting is like to combine all binary models of the one-
against-rest method. There are k weight vectors w1, . . . , wk
for k classes. In the loss function (34), for each m, max(0, 1−
(wyi − wm)Txi) is similar to the L1 loss in (8) for binary
classification. Overall, we hope that the decision value of xi
by the model wyi is at least one larger than the values by
other models. For testing, the decision function is also (32).
Early works of this method focus on the nonlinear (i.e.,
kernel) case [50], [60], [62]. A study for linear classification
is in [63], which applies a coordinate descent method to solve
the dual problem of (33). The idea is similar to the method
in Section IV-D; however, at each step, a larger sub-problem
of k variables is solved. A nice property of this k-variable
sub-problem is that it has a closed-form solution. Experiments
in [63] show that solving (33) gives slightly better accuracy
than one-against-rest, but the training time is competitive. This
result is different from the nonlinear case, where the longer
training time than one-against-rest and one-against-one has
made the approach of solving one single optimization problem
less practical [50]. A careful implementation of the approach
in [63] is given in [7, Appendix E].
C. Maximum Entropy
Maximum Entropy (ME) [64] is a generalization of logistic
regression for multi-class problems6 and a special case of con-
ditional random fields [65] (see Section VIII-A). It is widely
applied by NLP applications. We still assume class labels
1, . . . , k for an easy comparison to (33) in our subsequent
discussion. ME models the following conditional probability
function of label y given data x.
P (y|x) ≡ exp(w
T
y x)∑k
m=1 exp(w
T
mx)
, (35)
where wm,∀m are weight vectors like those in (32) and (33).
This model is also called multinomial logistic regression.
ME minimizes the following regularized negative log-
likelihood.
min
w1,...,wm
1
2
k∑
m=1
‖wk‖2 + C
l∑
i=1
ξME({wm}km=1;xi, yi),
(36)
where
ξME({wm}km=1;x, y) ≡ − logP (y|x).
Clearly, (36) is similar to (33) and ξME(·) can be considered
as a loss function. If wTyixi  wTmxi,∀m 6= yi, then
ξME({wm}km=1;xi, yi) is close to zero (i.e., no loss). On the
other hand, if wTyixi is smaller than other w
T
mxi,m 6= yi,
then P (yi|xi)  1 and the loss is large. For prediction, the
decision function is also (32).
NLP applications often consider a more general ME model
by using a function f(x, y) to generate the feature vector.
P (y|x) ≡ exp(w
Tf(x, y))∑
y′ exp(w
Tf(x, y′))
. (37)
6Details of the connection between logistic regression and maximum
entropy can be found in, for example, [12, Section 5.2].
10
Studies in [76], [77] designed linear classifiers to train
explicit mappings of sequence data, where features correspond
to subsequences. Using the relation between subsequences,
they are able to design efficient training methods for very high
dimensional mappings.
B. Approximation of Kernel Methods via Linear Classification
Methods in Section VI-A train φ(xi),∀i explicitly, so
they obtain the same model as a kernel method using
K(xi,xj) = φ(xi)
Tφ(xj). However, they have limitations
when the dimensionality of φ(x) is very high. To resolve
the slow training/testing of kernel methods, approximation is
sometimes unavoidable. Among the many available methods
to approximate the kernel, some of them lead to training a
linear classifier. Following [78], we categorize these methods
to the following two types.
• Kernel matrix approximation This type of approaches
finds a low-rank matrix Φ¯ ∈ Rd×l with d  l such that
Φ¯T Φ¯ can approximate the kernel matrix Q.
Q¯ = Φ¯T Φ¯ ≈ Q. (39)
Assume Φ¯ ≡ [x¯1, . . . , x¯l]. If we replace Q in (16) with Q¯,
then (16) becomes the dual problem of training a linear
SVM on the new set (yi, x¯i), i = 1, . . . , l. Thus, opti-
mization methods discussed in Section IV can be directly
applied. An advantage of this approach is that we do not
need to know an explicit mapping function corresponding
to a kernel of our interest (see the other type of approaches
discussed below). However, this property causes a compli-
cated testing procedure. That is, the approximation in (39)
does not directly reveal how to adjust the decision function
(3).
Early developments focused on finding a good approxima-
tion matrix Φ¯. Some examples include Nystro¨m mehtod
[79], [80] and incomplete Cholesky factorization [81], [82].
Some works (e.g., [19]) consider approximations other than
(39), but also lead to linear classification problems.
A recent study [78] addresses more on training and testing
linear SVM after obtaining the low-rank approximation. In
particular, details of the testing procedures can be found in
Section 2.4 of [78]. Note that linear SVM problems obtained
after kernel approximations are often dense and have more
instances than features. Thus, training algorithms suitable
for such problems may be different from those for sparse
document data.
• Feature mapping approximation This type of approaches
finds a mapping function φ¯ : Rn → Rd such that
φ¯(x)T φ¯(t) ≈ K(x, t).
Then, linear classifiers can be applied to new data
φ¯(x1), . . . , φ¯(xl). The testing phase is straightforward be-
cause the mapping φ¯(·) is available.
Many mappings have been proposed. Examples include
random Fourier projection [83], random projections [84],
[85], polynomial approximation [86], and hashing [87]–
[90]. They differ in various aspects, which are beyond the
scope of this paper. An issue related to the subsequent linear
classification is that some methods (e.g., [83]) generate
dense φ¯(x) vectors, while others give sparse vectors (e.g.,
[85]). A recent study focusing on the linear classification
after obtaining φ¯(xi), ∀i is in [91].
VII. TRAINING LARGE DATA BEYOND THE MEMORY OR
THE DISK CAPACITY
Recall that we described some binary linear classification
algorithms in Section IV. Those algorithms can work well
under the assumption that the training set is stored in the
computer memory. However, as the training size goes beyond
the memory capacity, traditional algorithms may become very
slow because of frequent disk access. Indeed, even if the
memory is enough, loading data to memory may take more
time than subsequent computation [92]. Therefore, the design
of algorithms for data larger than memory is very different
from that of traditional algorithms.
If the data set is beyond the disk capacity of a single com-
puter, then it must be stored distributively. Internet companies
now routinely handle such large data sets in data centers.
In such a situation, linear classification faces even more
challenges because of expensive communication cost between
different computing nodes. In some recent works [93], [94],
parallel SVM on distributed environments has been studied
but they investigated only kernel SVM. The communication
overhead is less serious because of expensive kernel compu-
tation. For distributed linear classification, the research is still
in its infancy. The current trend is to design algorithms so that
computing nodes access data locally and the communication
between nodes is minimized. The implementation is often
conducted using distributed computing environments such as
Hadoop [95]. In this section, we will discuss some ongoing
research results.
Among the existing developments, some can be easily
categorized as online methods. We describe them in Section
VII-A. Batch methods are discussed in Section VII-B, while
other approaches are in Section VII-C
A. Online Methods
An online learning method receives a sequence of training
samples and processes some instances at a time. Because
training instances may be used only once, not only can online
methods handle data larger than memory, they are also suitable
for streaming data. An online optimization method can also
be applied to a batch setting, where a fixed set of training
instances is given; it iteratively updates the model w using
some instances at a time.
One popular online algorithm is the stochastic gradient
descent method (SGD), which can be traced back to stochas-
tic approximation method [96], [97]. Take the primal L2-
regularized L1-loss SVM in (7) as an example. At each step,
a training instance xi is chosen and w is updated by
w ← w − η∇S(1
2
‖w‖22 + C max(0, 1− yiwTxi)
)
, (40)
12
by the shrinking techniques used in training nonlinear SVM
[8], [47].
For distributed batch learning, all existing parallel optimiza-
tion methods [121] can possibly be applied. However, we
have not seen many practical deployments for training large-
scale data. Recently, [122] considers the ADMM (Alternating
Direction Method of Multiplier) method [123] for distributed
learning. Take SVM as an example and assume data points
are partitioned to m distributively stored sets B1, . . . , Bm.
This method solves the following approximation of the original
optimization problem.
min
w1,...,wm,z
1
2
zTz + C
m∑
j=1
∑
i∈Bj
ξL1(wj ;xi, yi)
+
ρ
2
m∑
j=1
‖wj − z‖2
subject to wj − z = 0,∀j,
where ρ is a pre-specific parameter. It then employs an
optimization method of multipliers by alternatively minimizing
the Lagrangian function over w1, . . . ,wm, minimizing the
Lagrangian over z, and updating dual multipliers. The mini-
mization of Lagrangian over w1, . . . ,wm can be decomposed
to m independent problems. Other steps do not involve data
at all. Therefore, data points are locally accessed and the
communication cost is kept minimum. Examples of using
ADMM for distributed training include [124]. Some known
problems of this approaches are first, the convergence rate is
not very fast, and second, it is unclear how to choose the
parameter ρ.
C. Other Approaches
We briefly discuss some other approaches which cannot be
clearly categorized as batch or online methods.
The most straightforward method to handle large data is
probably to randomly select a subset that can fit in memory.
This approach works well if the data quality is good; however,
sometimes using more data gives higher accuracy. To improve
the performance of using only a subset, some have proposed
techniques to include important data points into the subset. For
example, the approach in [125] selects a subset by reading data
from disk only once. For data in a distributed environment,
sub-sampling can be a complicated operation. Moreover, a
subset fitting the memory of one single computer may be too
small to give good accuracy.
Bagging [126] is a popular classification method to split a
learning task to several easier ones. It selects several random
subsets, trains each of them, and ensembles (e.g., averaging)
the results during testing. This method may be particularly
useful for distributively stored data because we can directly
consider data in each node as a subset. However, if data quality
in each node is not good (e.g., all instances with the same
class label), the model generated by each node may be poor.
Thus, ensuring data quality of each subset is a concern. Some
studies have applied the bagging approach on a distributed
system [127], [128]. For example, in the application of web
advertising, [127] trains a set of individual classifiers in a
distributed way. Then, a final model is obtained by averaging
the separate classifiers. In the linguistic applications, [129]
extends the simple model average to the weighted average and
achieves better performance. An advantage of the bagging-
like approach is the easy implementation using distributed
computing techniques such as MapReduce [130].8
VIII. RELATED TOPICS
In this section, we discuss some other linear models. They
are related to linear classification models discussed in earlier
sections.
A. Structured Learning
In the discussion so far, we assume that the label yi is a
single value. For binary classification, it is +1 or −1, while
for multi-class classification, it is one of the k class labels.
However, in some applications, the label may be a more
sophisticated object. For example, in part-of-speech (POS)
tagging applications, the training instances are sentences and
the labels are sequences of POS tags of words. If there are
l sentences, we can write the training instances as (yi,xi) ∈
Y ni × Xni ,∀i = 1, . . . , l, where xi is the ith sentence, yi
is a sequence of tags, X is a set of unique words in the
context, Y is a set of candidate tags for each word, and ni
is the number of words in the ith sentence. Note that we
may not be able to split the problem to several independent
ones by treating each value yij of yi as the label, because
yij not only depends on the sentence xi but also other tags
(yi1, . . . , yi(j−1), yi(j+1), . . . yini). To handle these problems,
we could use structured learning models like conditional
random fields [65] and structured SVM [131], [132].
• Conditional Random Fields Conditional random fields
(CRF) [65] is a linear structured model commonly used in
NLP. Using notation mentioned above and a feature function
f(x,y) like ME, CRF solves the following problem:
min
w
1
2
‖w‖22 + C
l∑
i=1
ξCRF(w;xi,yi), (44)
where
ξCRF(w;xi,yi) ≡ − logP (yi|xi), and
P (y|x) ≡ exp(w
Tf(x,y))∑
y′ exp(w
Tf(x,y′))
. (45)
If elements in yi are independent to each other, then CRF
reduces to ME.
The optimization of (44) is challenging because in the
probability model (45), the number of possible y’s is
exponentially many. An important property to make CRF
practical is that the gradient of the objective function in
(44) can be efficiently evaluated by dynamic programming
[65]. Some available optimization methods include L-BFGS
(quasi Newton) and conjugate gradient [133], stochastic gra-
dient descent [134], stochastic quasi Newton [103], [135],
8We mentioned earlier the Hadoop system, which includes a MapReduce
implementation.
14
[10] S. S. Keerthi and C.-J. Lin, “Asymptotic behaviors of support vector
machines with Gaussian kernel,” Neural Computation, vol. 15, no. 7,
pp. 1667–1689, 2003.
[11] Z. S. Harris, “Distributional structure,” Word, vol. 10, pp. 146–162,
1954.
[12] F.-L. Huang, C.-J. Hsieh, K.-W. Chang, and C.-J. Lin, “Iterative
scaling and coordinate descent methods for maximum entropy,”
Journal of Machine Learning Research, vol. 11, pp. 815–848, 2010.
[Online]. Available: http://www.csie.ntu.edu.tw/∼cjlin/papers/maxent
journal.pdf
[13] G.-X. Yuan, K.-W. Chang, C.-J. Hsieh, and C.-J. Lin, “A
comparison of optimization methods and software for large-scale
l1-regularized linear classification,” Journal of Machine Learning
Research, vol. 11, pp. 3183–3234, 2010. [Online]. Available:
http://www.csie.ntu.edu.tw/∼cjlin/papers/l1.pdf
[14] O. L. Mangasarian, “A finite Newton method for classification,” Opti-
mization Methods and Software, vol. 17, no. 5, pp. 913–929, 2002.
[15] A. Y. Ng, “Feature selection, L1 vs. L2 regularization, and rotational
invariance,” in Proceedings of the Twenty First International Confer-
ence on Machine Learning (ICML), 2004.
[16] R. Tibshirani, “Regression shrinkage and selection via the lasso,”
Journal of the Royal Statistical Society Series B, vol. 58, pp. 267–
288, 1996.
[17] D. L. Donoho and Y. Tsaig, “Fast solution of l1 minimization problems
when the solution may be sparse,” IEEE Transactions on Information
Theory, vol. 54, pp. 4789–4812, 2008.
[18] H. Zou and T. Hastie, “Regularization and variable selection via
the elastic net,” Journal of the Royal Statistical Society: Series B
(Statistical Methodology), vol. 67, no. 2, pp. 301–320, 2005.
[19] Y.-J. Lee and O. L. Mangasarian, “RSVM: Reduced support vector
machines,” in Proceedings of the First SIAM International Conference
on Data Mining, 2001.
[20] J. Shi, W. Yin, S. Osher, and P. Sajda, “A fast hybrid algorithm for large
scale l1-regularized logistic regression,” Journal of Machine Learning
Research, vol. 11, pp. 713–741, 2010.
[21] K.-W. Chang, C.-J. Hsieh, and C.-J. Lin, “Coordinate descent
method for large-scale L2-loss linear SVM,” Journal of Machine
Learning Research, vol. 9, pp. 1369–1398, 2008. [Online]. Available:
http://www.csie.ntu.edu.tw/∼cjlin/papers/cdl2.pdf
[22] G.-X. Yuan, C.-H. Ho, and C.-J. Lin, “An improved GLMNET
for l1-regularized logistic regression and support vector machines,”
National Taiwan University, Tech. Rep., 2011. [Online]. Available:
http://www.csie.ntu.edu.tw/∼cjlin/papers/long-glmnet.pdf
[23] P. S. Bradley and O. L. Mangasarian, “Massive data discrimination via
linear support vector machines,” Optimization Methods and Software,
vol. 13, no. 1, pp. 1–10, 2000.
[24] V. Franc and S. Sonnenburg, “Optimized cutting plane algorithm for
support vector machines,” in Proceedings of the 25th International
Conference on Machine Learning. ACM, 2008, pp. 320–327.
[25] C. H. Teo, S. Vishwanathan, A. Smola, and Q. V. Le, “Bundle meth-
ods for regularized risk minimization,” Journal of Machine Learning
Research, vol. 11, pp. 311–365, 2010.
[26] L. Bottou, “Stochastic gradient descent examples,” 2007, http://leon.
bottou.org/projects/sgd.
[27] S. S. Keerthi and D. DeCoste, “A modified finite Newton method for
fast solution of large scale linear SVMs,” Journal of Machine Learning
Research, vol. 6, pp. 341–361, 2005.
[28] C.-J. Lin, R. C. Weng, and S. S. Keerthi, “Trust region Newton
method for large-scale logistic regression,” Journal of Machine
Learning Research, vol. 9, pp. 627–650, 2008. [Online]. Available:
http://www.csie.ntu.edu.tw/∼cjlin/papers/logistic.pdf
[29] T. P. Minka, “A comparison of numerical optimizers for logistic
regression,” 2003. [Online]. Available: http://research.microsoft.com/
∼minka/papers/logreg/
[30] J. Goodman, “Sequential conditional generalized iterative scaling,”
in Proceedings of the 40th Annual Meeting of the Association of
Computational Linguistics (ACL), 2002, pp. 9–16.
[31] R. Jin, R. Yan, J. Zhang, and A. G. Hauptmann, “A faster iterative
scaling algorithm for conditional exponential model,” in Proceedings of
the Twentieth International Conference on Machine Learning (ICML),
2003, pp. 282–289.
[32] P. Komarek and A. W. Moore, “Making logistic regression a core
data mining tool: A practical investigation of accuracy, speed, and
simplicity,” Robotics Institute, Carnegie Mellon University, Tech. Rep.
TR-05-27, 2005.
[33] H.-F. Yu, F.-L. Huang, and C.-J. Lin, “Dual coordinate descent
methods for logistic regression and maximum entropy models,”
Machine Learning, vol. 85, no. 1-2, pp. 41–75, October 2011. [Online].
Available: http://www.csie.ntu.edu.tw/∼cjlin/papers/maxent dual.pdf
[34] J. Zhu, S. Rosset, T. Hastie, and R. Tibshirani, “1-norm support vector
machines,” in Advances in Neural Information Processing Systems 16,
S. Thrun, L. Saul, and B. Scho¨lkopf, Eds. Cambridge, MA: MIT
Press, 2004.
[35] G. M. Fung and O. L. Mangasarian, “A feature selection Newton
method for support vector machine classification,” Computational
optimization and applications, vol. 28, pp. 185–202, 2004.
[36] O. L. Mangasarian, “Exact 1-norm support vector machines via un-
constrained convex differentiable minimization,” Journal of Machine
Learning Research, vol. 7, pp. 1517–1530, 2006.
[37] K. Koh, S.-J. Kim, and S. Boyd, “An interior-point method for
large-scale l1-regularized logistic regression,” Journal of Machine
Learning Research, vol. 8, pp. 1519–1555, 2007. [Online]. Available:
http://www.stanford.edu/∼boyd/l1 logistic reg.html
[38] A. Genkin, D. D. Lewis, and D. Madigan, “Large-scale Bayesian
logistic regression for text categorization,” Technometrics, vol. 49,
no. 3, pp. 291–304, 2007.
[39] S. Yun and K.-C. Toh, “A coordinate gradient descent method for l1-
regularized convex minimization,” Computational Optimizations and
Applications, vol. 48, no. 2, pp. 273–307, 2011.
[40] G. Andrew and J. Gao, “Scalable training of L1-regularized log-linear
models,” in Proceedings of the Twenty Fourth International Conference
on Machine Learning (ICML), 2007.
[41] J. H. Friedman, T. Hastie, and R. Tibshirani, “Regularization paths for
generalized linear models via coordinate descent,” Journal of Statistical
Software, vol. 33, no. 1, pp. 1–22, 2010.
[42] J. Liu, J. Chen, and J. Ye, “Large-scale sparse logistic regression,” in
Proceedings of The 15th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2009, pp. 547–556.
[43] R. Tomioka, T. Suzuki, and M. Sugiyama, “Super-linear convergence
of dual augmented Lagrangian algorithm for sparse learning,” Journal
of Machine Learning Research, vol. 12, pp. 1537–1586, 2011.
[44] M. Schmidt, G. Fung, and R. Rosales, “Optimization methods for l1-
regularization,” University of British Columbia, Technical Report TR-
2009-19, 2009.
[45] C.-J. Lin and J. J. More´, “Newton’s method for large-scale bound
constrained problems,” SIAM Journal on Optimization, vol. 9, pp.
1100–1127, 1999.
[46] Z.-Q. Luo and P. Tseng, “On the convergence of coordinate descent
method for convex differentiable minimization,” Journal of Optimiza-
tion Theory and Applications, vol. 72, no. 1, pp. 7–35, 1992.
[47] T. Joachims, “Making large-scale SVM learning practical,” in Advances
in Kernel Methods – Support Vector Learning, B. Scho¨lkopf, C. J. C.
Burges, and A. J. Smola, Eds. Cambridge, MA: MIT Press, 1998, pp.
169–184.
[48] J. H. Friedman, T. Hastie, H. Ho¨fling, and R. Tibshirani, “Pathwise
coordinate optimization,” Annals of Applied Statistics, vol. 1, no. 2,
pp. 302–332, 2007.
[49] S. J. Wright, R. D. Nowak, and M. A. Figueiredo, “Sparse recon-
struction by separable approximation,” IEEE Transactions on Signal
Processing, vol. 57, pp. 2479–2493, 2009.
[50] C.-W. Hsu and C.-J. Lin, “A comparison of methods for multi-class
support vector machines,” IEEE Transactions on Neural Networks,
vol. 13, no. 2, pp. 415–425, 2002.
[51] R. Rifkin and A. Klautau, “In defense of one-vs-all classification,”
Journal of Machine Learning Research, vol. 5, pp. 101–141, 2004.
[52] E. L. Allwein, R. E. Schapire, and Y. Singer, “Reducing multiclass to
binary: a unifying approach for margin classifiers,” Journal of Machine
Learning Research, vol. 1, pp. 113–141, 2001.
[53] T.-K. Huang, R. C. Weng, and C.-J. Lin, “Generalized Bradley-Terry
models and multi-class probability estimates,” Journal of Machine
Learning Research, vol. 7, pp. 85–115, 2006. [Online]. Available:
http://www.csie.ntu.edu.tw/∼cjlin/papers/generalBT.pdf
[54] L. Bottou, C. Cortes, J. Denker, H. Drucker, I. Guyon, L. Jackel, Y. Le-
Cun, U. Muller, E. Sackinger, P. Simard, and V. Vapnik, “Comparison
of classifier methods: a case study in handwriting digit recognition,”
in International Conference on Pattern Recognition. IEEE Computer
Society Press, 1994, pp. 77–87.
[55] S. Knerr, L. Personnaz, and G. Dreyfus, “Single-layer learning revis-
ited: a stepwise procedure for building and training a neural network,”
in Neurocomputing: Algorithms, Architectures and Applications, J. Fo-
gelman, Ed. Springer-Verlag, 1990.
16
Rep., 2010. [Online]. Available: http://www.ucl.be/cps/ucl/doc/core/
documents/coredp2010 2web.pdf
[102] P. Richta´rik and M. Taka´cˇ, “Iteration complexity of randomized block-
coordinate descent methods for minimizing a composite function,”
School of Mathematics, University of Edinburgh, Tech. Rep., 2011.
[103] A. Bordes, L. Bottou, and P. Gallinari, “SGD-QN: Careful quasi-
Newton stochastic gradient descent,” Journal of Machine Learning
Research, vol. 10, pp. 1737–1754, 2009.
[104] A. Bordes, L. Bottou, P. Gallinari, J. Chang, and S. A. Smith, “Erratum:
SGD-QN is less careful than expected,” Journal of Machine Learning
Research, vol. 11, pp. 2229–2240, 2010.
[105] J. Langford, L. Li, and T. Zhang, “Sparse online learning via truncated
gradient,” Journal of Machine Learning Research, vol. 10, pp. 771–
801, 2009.
[106] S. Shalev-Shwartz and A. Tewari, “Stochastic methods for l1-
regularized loss minimization,” Journal of Machine Learning Research,
vol. 12, pp. 1865–1892, 2011.
[107] Y. E. Nesterov, “Primal-dual subgradient methods for convex prob-
lems,” Mathematical Programming, vol. 120, no. 1, pp. 221–259, 2009.
[108] J. Duchi and Y. Singer, “Efficient online and batch learning using
forward backward splitting,” Journal of Machine Learning Research,
vol. 10, pp. 2899–2934, 2009.
[109] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient methods
for online learning and stochastic optimization,” Journal of Machine
Learning Research, vol. 12, pp. 2121–2159, 2011.
[110] L. Xiao, “Dual averaging methods for regularized stochastic learning
and online optimization,” Journal of Machine Learning Research,
vol. 11, pp. 2543–2596, 2010.
[111] J. K. Bradley, A. Kyrola, D. Bickson, and C. Guestrin, “Parallel co-
ordinate descent for l1-regularized loss minimization,” in Proceedings
of the Twenty Eighth International Conference on Machine Learning
(ICML), 2011, pp. 321–328.
[112] J. Langford, L. Li, and A. Strehl, “Vowpal Wabbit,” 2007, https://
github.com/JohnLangford/vowpal wabbit/wiki.
[113] A. Nedic´, D. P. Bertsekas, and V. S. Borkar, “Distributed asynchronous
incremental subgradient methods,” Studies in Computational Mathe-
matics, vol. 8, pp. 381–407, 2001.
[114] J. Langford, A. Smola, and M. Zinkevich, “Slow learners are fast,”
in Advances in Neural Information Processing Systems 22, Y. Bengio,
D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, Eds.,
2009, pp. 2331–2339.
[115] A. Agarwal and J. Duchi, “Distributed delayed stochastic optimization,”
in Advances in Neural Information Processing Systems 24, J. Shawe-
Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Weinberger, Eds., 2011,
pp. 873–881.
[116] O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao, “Optimal
distributed online prediction using mini-batches,” Journal of Machine
Learning Research, vol. 13, pp. 165–202, 2012.
[117] D. Hsu, N. Karampatziakis, and J. Langford, Parallel Online Learning.
Cambridge University Press, 2011, ch. 14.
[118] S. Tong, “Lessons learned developing a practical large scale machine
learning system,” Google Research Blog, 2010, http://googleresearch.
blogspot.com/2010/04/lessons-learned-developing-practical.html.
[119] M. Ferris and T. Munson, “Interior point methods for massive support
vector machines,” SIAM Journal on Optimization, vol. 13, no. 3, pp.
783–804, 2003.
[120] K.-W. Chang and D. Roth, “Selective block minimization for faster
convergence of limited memory large-scale linear models,” in Proceed-
ings of the Seventeenth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2011.
[121] Y. Censor and S. A. Zenios, Parallel Optimization: Theory, Algorithms,
and Applications. Oxford University Press, 1998.
[122] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed
optimization and statistical learning via the alternating direction method
of multipliers,” Foundations and Trends in Machine Learning, vol. 3,
no. 1, pp. 1–122, 2011.
[123] D. Gabay and B. Mercier, “A dual algorithm for the solution of nonlin-
ear variational problems via finite element approximation,” Computers
and Mathematics with Applications, vol. 2, pp. 17–40, 1976.
[124] P. A. Forero, A. Cano, and G. B. Giannakis, “Consensus-based
distributed support vector machines,” Journal of Machine Learning,
vol. 11, pp. 1663–1707, 2010.
[125] H. Yu, J. Yang, and J. Han, “Classifying large data sets using SVMs
with hierarchical clusters,” in Proceedings of the Ninth ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining.
New York, NY, USA: ACM Press, 2003, pp. 306–315.
[126] L. Breiman, “Bagging predictors,” Machine Learning, vol. 24, no. 2,
pp. 123–140, August 1996.
[127] D. Chakrabarti, D. Agarwal, and V. Josifovski, “Contextual advertising
by combining relevance with click feedback,” in Proceeding of the 17th
international conference on World Wide Web, 2008, pp. 417–426.
[128] M. Zinkevich, M. Weimer, A. Smola, and L. Li, “Parallelized stochastic
gradient descent,” in Advances in Neural Information Processing
Systems 23, J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel,
and A. Culotta, Eds., 2010, pp. 2595–2603.
[129] R. McDonald, K. Hall, and G. Mann, “Distributed training strategies for
the structured perceptron,” in Proceedings of the 48th Annual Meeting
of the Association of Computational Linguistics (ACL), 2010, pp. 456–
464.
[130] J. Dean and S. Ghemawat, “MapReduce: simplified data processing
on large clusters,” Communications of the ACM, vol. 51, no. 1, pp.
107–113, 2008.
[131] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun, “Large
margin methods for structured and interdependent output variables,”
Journal of Machine Learning Research, vol. 6, pp. 1453–1484, 2005.
[132] B. Taskar, C. Guestrin, and D. Koller, “Max-margin markov networks,”
in Advances in Neural Information Processing Systems 16. Cambridge,
MA: MIT Press, 2004.
[133] F. Sha and F. C. N. Pereira, “Shallow parsing with conditional random
fields,” in Proceedings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguistics on Human
Language Technology (NAACL), 2003, pp. 134–141.
[134] S. Vishwanathan, N. N. Schraudolph, M. W. Schmidt, and K. Murphy,
“Accelerated training of conditional random fields with stochastic
gradient methods,” in Proceedings of the Twenty Third International
Conference on Machine Learning (ICML), 2006, pp. 969–976.
[135] N. N. Schraudolph, J. Yu, and S. Gunter, “A stochastic quasi-Newton
method for online convex optimization,” in Proceedings of the 11th In-
ternational Conference Artificial Intelligence and Statistics (AISTATS),
2007, pp. 433–440.
[136] P.-J. Chen, “Newton methods for conditional random fields,” Master’s
thesis, Department of Computer Science and Information Engineering,
National Taiwan University, 2009.
[137] T. Joachims, T. Finley, and C.-N. J. Yu, “Cutting-plane training of
structural SVMs,” Machine Learning Journal, 2008.
[138] J. E. Kelley, “The cutting-plane method for solving convex programs,”
Journal of the Society for Industrial and Applied Mathematics, 1960.
[139] N. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich, “(Online) subgradient
methods for structured prediction,” in Proceedings of the Eleventh In-
ternational Conference Artificial Intelligence and Statistics (AISTATS),
vol. 2, 2007, pp. 380–387.
[140] V. Vapnik, Statistical Learning Theory. New York, NY: Wiley, 1998.
[141] I. Daubechies, M. Defrise, and C. De Mol, “An iterative thresholding
algorithm for linear inverse problems with a sparsity constraint,”
Communications on Pure and Applied Mathematics, vol. 57, pp. 1413–
1457, 2004.
[142] M. A. T. Figueiredo, R. Nowak, and S. Wright, “Gradient projection
for sparse reconstruction: Applications to compressed sensing and
other inverse problems,” IEEE Journal on Selected Topics in Signal
Processing, vol. 1, pp. 586–598, 2007.
[143] S.-J. Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinevsky, “An interior
point method for large-scale l1-regularized least squares,” IEEE Journal
on Selected Topics in Signal Processing, vol. 1, pp. 606–617, 2007.
[144] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra, “Efficient
projections onto the L1-ball for learning in high dimensions,” in
Proceedings of the Twenty Fifth International Conference on Machine
Learning (ICML), 2008.
[145] A. Beck and M. Teboulle, “A fast iterative shrinkage-thresholding
algorithm for linear inverse problems,” SIAM Journal on Imaging
Sciences, vol. 2, no. 1, pp. 183–202, 2009.
we were asked do separate songs preferred by some users from other songs. More
than 1,000 teams worldwide attended this competition. Following our past work
to win KDD Cup 2010, at National Taiwan University, we organized a course for
KDD Cup 2011 (with professors SD Lin and HT Lin). Our students and TAs had
done a great job. The NTU team won the first prize on both tracks of KDD Cup
2011. We received the prize in front of all audience at the opening ceremony of
the conference.
Overall, this trip of attending ACM KDD 2011 is very rewarding.
2
98年度專題研究計畫研究成果彙整表 
計畫主持人：林智仁 計畫編號：98-2221-E-002-136-MY3 
計畫名稱：大規模線性分類問題之研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 3 3 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
