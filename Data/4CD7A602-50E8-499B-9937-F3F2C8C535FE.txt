 1
摘  要 
本研究是依據 Intelligent Voice Recognition for Affective Computing in HCI 為主軸。我們透過兩種機制
來實作可以辨識與表達情緒的情感電腦(Affective Computer)，主要是語音的輸入，之後轉化為文字，
經由以下模式來進行情緒辨識：(1) 以本體論與自然語言處理技術為基礎的 Symbolic AI 模式，而採
用的本體論為 OMCSnet。 (2) 使用 SVM 分類器、情緒辭典、語言結構訊息的 Computational 
Intelligence 模式，結合以上的模式，設計出兩套不同方法，有效地提升情緒辨識正確率。並且，我們
將系統所辨識出的情緒，透過我們所修改的 Avatar 適時地反應出使用者的情緒，分別為喜、悲、怒、
恨、驚等。本研究整合數位學習科技、資訊、電機及藝術等跨領域研究，達到多媒體數位藝術，並實
現「前瞻智慧無所不在情緒感知多媒體回饋系統」。 
關鍵詞：情感運算、情緒分析、本體論、情緒辭典、人機互動、SVM 
 
ABSTRACT 
This research project is centered on Intelligent Voice Recognition for Affective Computing in HCI.  By 
using the following two methodologies, we are able to implement an affective computer capable of 
recognizing and expressing emotions: (1) Basing a symbolic Ai on ontology and natural language processing, 
the chosen semantic network is OMCSnet; (2) Applying SVM, emotion lexicons and language structure 
information in a computational intelligence manner. Combining the above methods to design a compound 
solution, it has been proven by empirical means that such a method is effective in increasing the accuracy 
rates for emotion recognition.  Once the emotion has been recognized by our system, a modified avatar 
expresses reaction to the user’s emotion, denoted in the categories of happiness, sadness, anger, hatred, 
surprise and so forth.  This research combines information learning technology, computer science, electrical 
engineering and arts as well as various studies in other realms to form a multimedia digital artwork, as a 
realization of a “prospective intelligent ubiquitous multimedia feedback system for emotion recognition”. 
Keywords：affective computing, affective analyze, Ontology, emotion dictionary, human-computer 
interactive, SVM 
 
前言 
 數位內容的普及，使得個人化需求的技術發展更顯重要。然而，在大量的資訊與環境條件下，如
何主動提供快速的情緒感知回饋給使用者，已引起學者的興趣，但其研究仍僅限於個人情緒模式的分
析與回饋，因此也提供了我們研究的動機，就其最終目的，能提供個人情緒的感知及多媒體回饋，以
達到真正數位生活智慧的特性。  
本研究發展技術採用雙面逼近模式，從理論、系統、技術端立場、總計畫從生活、使用端角度，
各自研發系統產品並提出另一端在接續整合研發上所應注意之事項與理念、技術規範，使整體研發計
畫之發展更為周延。以研發使用者情緒模型為核心，發展相關情緒感知多媒體回饋以及人性化的系統
人機界面，猶如心理醫生般可以給予使用者適當之情緒回饋，使系統以人為主要設計理念並使其更具
人文藝術價值。 
 
1. 研究目的 
本研究配合全程技術發展藍圖，進一步持續發展前瞻智慧無所不在情緒感知多媒體回饋技術，以
實際應用於人性化數位生活。因此，本研究基於有機計算 (Organic Computing) 精神，在現今全球推
動文化創意產業的「美感經濟」體系下，建置具有下列功能之系統：(1) 透過語音嘗試感知使用者的
狀態。(2) 經由情感運算 (Affective Computing) 模組，分析使用者的情緒。而我們判斷情緒的特徵
值，來自語音辨識資訊。 (3) 根據使用者的情緒，提供多媒體回饋。目前回饋的媒材是即時自動產生
具有視覺的互動藝術作品。 
 
2. 文獻探討 
2.1 情感運算 
所謂的「情感運算」(Affective Computing)，係透過各種感應器 (Sensor)，取得由情緒、情感所引
起的表情與生理變化信號，針對這些訊號進行識別，以理解人的感情，並依此做出適當的回應 (李蔡
彥, 2004) (Manovich, 2001)。因此，這形成了一個新興學術領域：研究如何感知情緒、建立適當的情
 3
因此一個巨大詳盡的 Ontology 資料庫是必須的，而本計畫所採用的是 2004 年 MIT media lab 所
製作的 OMCSnet、具有 17 萬餘個 concept 與 28 萬餘 links、且具有 C++實做、具有合理的推論反應時
間。OMCSnet 的本身定義了 link 約十種。各自代表不同 concept 之間關聯的關係、但是本計畫中不採
用 link 種類分類。相反的、我們將每個 link 視為皆是一樣重要的。 
 
圖 3-1 Ontology-based & NLP 系統架構 
 
系統初期純粹藉由關鍵字所提供的情緒強度、不加以任何修改之加總。情緒的強度是藉由不同
concept 之間、在 Ontology graph 當中的距離（定義為 1 node traverse 到另外 1 node 所需之距離、即途
中經過的最短 link 量）視為「兩 concept 之間的關聯強度」、如果將其中的一個 concept 設定為情緒
的喜、怒、哀、恨、驚訝等情緒的話、就可以取得「某個 concept 相對於不同情緒 node 之距離」、那
麼集可是為「某個 concept 對於不同情緒的關聯強度」。因此我們可視為一個關鍵字（concept）所產
生的情緒強度。如果純粹就一個句子當中拳部的關鍵字、進行推論、那麼加總以後、即可視為本句子
所產生的情緒內容。 
 
3.2 量化分析與 NLP 技術之研究 
3.2.1 系統架構 
本系統經由句子去進行一連串的分析到後來的結果輸出，以及最後給予使用者回饋的部份。首先
將詞語進行斷詞，由語料庫內讀取資料，語料庫是經由PMI量化分析取得關鍵字和相對應的情緒編號
以及CO值所組成，並擷取每個句有標記的字串。之後去針對結構先進行分析，分為轉折語意、對等
連接語意、否定語意三大部分並由量化的方法，找到最適合的情緒，最後在根據使用者所表達的情
緒，回應適當對話。 
轉折語意
否定語意
句子 
分析 
刪 
除 
對等語意 
決定情緒
…
斷詞
AVATAR  
圖3-2 系統架構 
3.2.2 量化方法 
本計畫以(Manning and Schütze, 1999)提出的PMI(variation of pointwise mutual information)並以
(Changhua Yang Kevin Hsin-Yih Lin Hsin-Hsi Chen,2007)的方法以及來分析情緒與詞的關聯力co(e,w)。
 5
  if(最高票數不一樣) 
    判斷最高票數的情緒編號S的情緒 
  else if(最高票數一樣) 
    擷取句有最高票數的情緒編號裡有最高PMI值為S的情緒 
 
3.2.3 詞語結構 
詞語量化詞性 
根據(某論文)指出具有代表情緒的詞性的關鍵字，通常以名詞、動詞、形容詞、副詞為主。本論
文經由斷詞器將一段語句過濾掉不必要的詞性，在去做量化的分析，擷取的詞性如下表。 
否定語意 
第一個是否定語意的定義。由於否定語意句有轉換情緒之用途，其結構較為簡單，方便定義。在
本論文的方法中，經由語句分析獲得含有否定語意資訊後，將之後的整段詞語所代表的情緒取相反狀
態。考慮單一句型「我不快樂」，在得知快樂屬正向情緒後，偵測「不」則變更為負向情緒。 
由於本論文採用成大開發的斷詞器做斷詞，將原始資料經由評估符合斷詞的詞語與人工篩選有用
的詞語一併加入引用，因此針對此類否定語句分析如下： 
轉折語意 
第二個是轉折語意的定義。轉折語意具有否定前面所代表的情緒，通常代有轉折語意的那一句
話，往往是表達整段句子的重點，因此對於此類型將以擷取此句當作指標，考慮一複合句型「我很
累，但是我很快樂。」，雖然前一句的分析結果可能屬於負向情緒，但經過轉折語意的判斷之後，整
體代表為正向情緒。 
對等連接語意 
第三個是對等連接語意的定義。對等連接詞又可分為兩種，接句子或者詞，本計畫針對此兩種對
等語意進行不同方法之分析，並借由對等連接語意去判斷左右兩邊詞語的情緒，有兩個優點：一、對
於一方無法判情緒時可以以另一方情緒結果當成整體情緒，二、當雙方都有可判斷情緒，可以當作一
個檢查機制去比較所表達的情緒是否相同。 
 
3.3 Avatar 與 Agent 技術之研究 
Agent (代理人) 在電腦科學上被定義為是一個自主的存在，並且會依照周圍環境的互動進而一步
去完成它所被指定的目標。一個Avatar則是一個視覺化的代表物體或是圖樣，用於在虛擬的世界裏面
代表一個使用者。在本次研究，我們利用了avatar來做為情感運算結果的表現介面，依照情感運算的
結果與使用者進行對應的互動對話。 
 在這次的研究當中，我們使用了”偽春菜”這個程式來進行我們所需要的avatar開發。”偽春菜”本身
是一個原本由日本虛擬人格愛好團體所開發出來的open source開發平台，近年以來也有許多台灣以及
大陸的愛好者各自進行”偽春菜”系統更進一步的開發。 
 “偽春菜”這個平台供給開發者一個可以任意設計所想像的虛擬腳色的環境。在這個平台上構成的
虛擬腳色主要為三個部份: 核心程式，人格以及偽AI。 
首先，核心程式是負責擔任OS與人格之間的橋樑。當使用者企圖與人格互動或是人格需要在畫面上
輸出任何的訊息，皆是由核心程式作為仲介。人格的部份包含了所有定義一個虛擬腳色性格以及反應
和對話的全部設定之料。除此之外，每個人格裡面同時也包含了一組作為在螢幕上顯示時代表該人格
的腳色貼圖。最後，偽AI負責的是進行理解由核心程式所傳過來的指令，並且從啟動中的人格的資料
庫中選出對應的行動。 
 針對此次研究的介面需求，人格的開發主要著重於對使用表達所獲得的情緒分析結果方面和接收
外部輸入。一般而言，對於一個運行中的程式進行內部程式碼或是參數的修改必須要等到該程式重新
啟動後，才能獲得新的資訊。起初開發這個avatar的時候也同樣被這個問題困擾了很久，最後找到了
以檔案I/O的方式進行與外部輸入的銜接。當系統進行情感運算完畢時，分析結果會被寫入到一個資
料檔案，而人格會在定期的時間去檢查這個檔案來獲得最新的分析結果作為選擇互動對話的依據。如
此一來便能避免每一次寫入新的分析資料的時候必須要重新啟動avatar平台的必要。 
 為了使avatar在於視覺上能更豐富及貼切的表達情感分析結果所得到的表情，也請了幾位實驗室
 7
4.2.2 Ontology-based & NLP 實驗結果 
    在 Ontology-based & NLP 實驗中，我們設計了教室學習 Scenario 的教育 Domain。本研究在香港
ICCE2009 中發表論文，並於 TAAI2009 榮獲優良論文獎。發表 Session 皆為 AIED (AI in Edcation)。 
 
表4-1 範例句子 
S1 這次考試真容易 the examination was really easy S11 
我討厭上數學 
I hate math 
S2 我下課後一定要讀書 I have to study after school S12 
同學們考試的時候不要東張西望 
the students do not look around during the 
examination 
S3 這堂課真是有趣 this class is really interesting S13 
舉頭望老師，低頭吃午餐 
raise my eyes to the teacher, bow their heads 
to eat lunch 
S4 上課請專心 please concentrate on school S14 
學生上課應該要專心聽講 
class, the students should concentrate on 
listening and speaking 
S5 知識就是力量 knowledge is power S15 
學習是無止盡的 
learning is endless 
S6 我吃飽了 I am full S16 
學物理對我沒有用 
physics right I did not use 
S7 真想睡覺 really want to go to bed S17 
老師的課讓人想睡覺 
the teacher's class people want to sleep 
S8 要聊天的話，請出去 To chat, please go S18 
上課做筆記會協助記憶 
class and taking notes will assist the memory 
S9 這堂課真是無聊透頂 this class is really boring the extreme S19 
教室有冷氣好睡覺 
Classrooms are air-conditioned sleeping well 
S10 好無聊喔 oh well bored S20 
同學不要趴在桌上睡覺 
Students do not sleep lying on the table 
 
表 4-2 Inference results (Concentration) 
Sample 
Sentence 
Label 
Inference Statistics Concluding State 
Sample 
Sentence 
Label 
Inference Statistics Concluding State 
S1 
concentrate:0.000000 
happy:0.428571 
relax:0.00000 
easy:0.571429 
Not Concentrating S11  
concentrate:0.714286 
happy:0.761905 
relax:0.761905 easy:0.761905 Concentrating 
S2 
concentrate:0.733333 
happy:1.369697 
relax:1.436364 
easy:1.460606 
Concentrating S12  
concentrate:0.000000 
happy:0.428571 
relax:0.000000 
easy:1.571429 
Not Concentrating 
S3 
concentrate:0.733333 
happy:0.800000 
relax:0.733333 
easy:0.733333 
Concentrating S13  
concentrate:0.000000 
happy:2.036364 
relax:1.836364 
easy:2.127273 
Not Concentrating 
S4 
concentrate:1.000000 
happy:0.636364 
relax:0.636364 
easy:0.727273 
Concentrating S14  
concentrate:1.0000 
happy:0.000000 
relax:0.000000 
easy:0.000000 
Concentrating 
S5 
concentrate:1.451128 
happy:1.575188 
relax:1.451128 
easy:1.522556 
Concentrating S15 
concentrate:0.000000 
happy:0.000000 
relax:0.000000 
easy:0.000000 
Not Concentrating 
S6 
concentrate:0.000000 
happy:0.642857 
relax:0.642857 
easy:0.71428 
Not Concentrating S16 
concentrate:0.00000 
happy:0.000000 
relax:0.000000 
easy:0.000000 
Not Concentrating 
S7 
concentrate:0.000000 
happy:0.666667 
relax:0.666667 
easy:0.666667 
Not Concentrating S17 
concentrate:2.125641 
happy:3.005594 
relax:2.929837 
easy:2.938928 
Concentrating 
S8 
concentrate:0.000000 
happy:0.00000 
relax:0.00000 
easy:0.00000 
Not Concentrating S18 
concentrate:0.733333 
happy:0.80000 
relax:0.733333 
easy:0.733333 
Concentrating 
S9 
concentrate:0.733333 
happy:0.800000 
relax:0.733333 
easy:0.733333 
Concentrating S19 
concentrate:0. 714286 
happy:0. 785714 
relax:0. 0.785714 
easy:0. 714286 
Concentrating 
S10 
concentrate:0.000000 
happy:0.00000 
relax:0.00000 
Not Concentrating S20 
concentrate:0.00000 
happy: 1.303030 
relax: 1.393939 
Not concentrating 
 9
4.3 結論 
雖然目前的數位資訊科技快速的蓬勃發展，但是所注重的則是技術的開發，而忽略了科技是服務
人群，有一句廣告台詞說的非常好「科技始終來自於人性」，因此本研究認為以「人」為出發點的研
究才是最重要的，因此本研究將研究主軸放在如何主動提供快速且準確的情緒感知回饋給使用者，而
本研究所採用的回饋方式則是利用智慧型代理人。 
本研究基以Intelligent Voice Recognition for Affective Computing in HCI為主軸並且整合了多項領
域，達到個人化情緒；依據喜、悲、怒、恨、驚五種類別，利用多媒體內容作為情緒回饋，設計一可
根據使用者之情緒而產生之「前瞻智慧無所不在情緒感知多媒體回饋系統」，並發展相關系統，以展
示整體技術之應用與效益。本研究所開發出來的系統是利用語音輸入，轉化成文字，交由以兩種方法
進行情緒的識別，(1) 以本體論與自然語言處理技術為基礎的Symbolic AI模式，而採用的本體論為
OMCSnet。(2) 使用SVM分類器、情緒辭典、語言結構訊息的Computational Intelligence模式，並透過
設計出的機制結合兩種辨識的結果，再將辨識結果輸入至avatar的智慧型代理人中，在語音的部分整
體辨識率達到86%，在情緒辨識的部份使用PMI除去Stop Word可以達到最高的70.18%；Avatar則將適
時的反應出使用者的情緒。 
 
參考文獻 
1. Picard R. W. (1995) Affective Computing, TR-321, MIT, Media Laboratory..  
2. Picard R. W. (2003) Affective Computing: Challenge, International Journal of Human-Computer Studies, 
59 (1-2), July 2003, pp. 55-64. 
3. Manovich, Lev (2001). The Language of New Media, Massachusetts: MIT Press. 
4. TechArt (2005) http://blog.roodo.com/techart/archives/527647.html  
5. Vesterinen, Eerik (2001) Affective Computing, 
http://www.tml.tkk.fi/Opinnot/Tik111.590/2001/paperit/vesterinen.pdf 
5.6.  
6. Paiva, Editor, Springer Books, Affect in Interactions: Towards a New Generation of Interfaces, 
http://gaiva.inesc.pt/i3ws/i3workshop.html (1999) 
7. Asunción Gómez-Pérez and Oscar Corcho, “Ontology Languages for the Semantic Web,” IEEE 
Intelligent Systems, pp.54-60, 2002. 
8. Cowie, R. Douglas-Cowie, E. Tsapatsoulis, N. Votsis, G. Kollias, S. Fellenz, W. Taylor, J.G. , Emotion 
recognition in human-computer interaction, Signal Processing Magazine, IEEE, Volume: 18, Issue: 1, 
page(s): 32-80, 2001. 
9. Dempster, A., Laird, N., and Rubin, D. “Maximum likelihood from incomplete data via the EM 
algorithm.” Journal of the Royal Statistical Society, Series B, 39(1):1–38. , 1977. 
10. Fellbaum, C. (Ed.).: WordNet: An electronic lexical database. MIT Press (1998). 
11. Gentner, D. (1983). Structure-mapping: A theoretical framework for analogy. Cognitive Science, 7, pp 
155-170. 
12. H.-J. Bullinger and J. Ziegler, Editors, Ergonomics and User Interfaces, “Human-Computer Interaction:” 
Volume 1, special session on Affective Computing and HCI, Proceedings of HCI International '99, 
Munich, Lawrence Erlbaum Associates, Mahwah, NJ (August 1999). 
13. Healey, J. and R. W. Picard, “Digital Processing of Affective Signals,” Proceedings of the International 
Conference on Acoustics, Speech, and Signal Processing, Seattle, WA (May 1998). 
14. Klein, J., Y. Moon, and R. W. Picard, “This Computer Responds to User Frustration,” CHI 99 Short 
Papers, Pittsburgh, PA (1999). 
15. Lieberman, H., Liu, H., Singh, P., Barry, B. (2004). Beating Some Common Sense Into Interactive 
Applications. Submitted to AI Magazine. 
16. Lin, Kevin Hsin-Yih, Changhua Yang, and Hsin-Hsi Chen (2008). “Emotion Classification of Online 
News Articles from the Reader’s Perspective.” Proceedings of the 2008 IEEE/WIC/ACM International 
Conference on Web Intelligence (wi 2008), December 9-12, 2008, Sydney, Australia. 
17. Mueller, E. T. (2001): Commonsense in humans. Available: 
http://www.signiform.com/erik/pubs/cshumans.htm 
18. Picard R. W. (1995) Affective Computing, TR-321, MIT, Media Laboratory.. 
19. Picard R. W. (2003) Affective Computing: Challenge, International Journal of Human-Computer Studies, 
59 (1-2), July 2003, pp. 55-64. 
 11
[附錄] 發表於期刊 
 
 
 
 
 
 
 
 
 
 
 
 
I-Hen Tsai, Koong Hao-Chiang Lin, and Rui-Ting Sun, “Affective Computing by Emotion 
Inference and Advanced Semantic Analysis”, JIT, Special Issue on Ambient Intelligence, 
Journal of Internet Technology, pp.15-28, Volume 42, Number 2, Oct. 2010. 
 
SCI-E / EI Journal  (EI Compendex, TSCI, SCIE, IET) 
 2
order to thrive. Thus, imagine that machinery are able to 
recognize, respond to and reproduce human emotions, would it 
not be helpful to our interaction with these systems and further 
allow devices to better interact with the user. The study of 
affective computing[3] brings together such notions to create or 
grant devices the ability to understand and respond, reproduce 
emotion. Recently, affective computing in computer science 
has seen a lot of application in making robots respond to 
human emotions, the earliest project is the Kismet by 
MIT[4][5], with more recent developments such as the WE-
4RII by Waseda University[6]. The emotions we have chosen 
for this research come from five of the most common primal 
emotion from existing theory of emotions[7][8][9]. 
 
2.2 Emotion Recognition 
In order to have a system recognize human emotions, there 
must be some form of input of the emotion to the system. 
Humans are capable of expressing their emotions in a wide 
variety of ways. The way that our facial features are set or the 
posture of our body tells others something about our current 
emotion state. We can also convey emotions through spoken 
form via certain token keywords in our speech and the 
intonation of our speech. Alternatively, what is spoken may 
also be written down in text, and the reader may gain 
information on the emotional state of the writer from the 
textual content[10]. In everyday expression, expressing 
emotion can occur as a mix of the above forms. For the scope 
of this research, we will be focusing on analysis emotions from 
text.  
There are currently two major methodologies for analysis of 
emotions from textual content. One is by statistical analysis, 
where each word in the input string is given a value or token, 
then the value is matched with a corresponding emotion that is 
the interest of the research. Combining the labeling and 
breaking down the each sentence in to key components of 
value with machine learning, a system capable of deriving 
emotions from textual content by analyzing keywords in 
sentences can be devised. The most commonly used tool for 
analysis here is SVM[11][12], though other mathematical 
methods such as CRF[13] and KNN has also been surveyed for 
analyzing text for emotions. 
Our research implements the second method, which is 
generally termed as the ontological approach.  As we know 
each word has its own meaning, as well as synonyms and 
acronyms, a knowledge base is built upon these values to 
associate keywords with their meanings. Emotions themselves 
also have defining and related words; therefore by relating 
words from textual input with the emotions, we can obtain a 
semantic distance between the target keyword and the emotion 
of interest. Taking the sum of distances for each emotion with 
all keywords in the sentence, we have a strength value for each 
of the emotions of interest in regards with the text. Based on 
these resultant values, we may infer the emotion of the text 
input. Current running projects working on lexical analysis 
include WordNet[14], MindNet and OMCSNet[15], which has 
now changed its name to ConceptNet. In our research, we shall 
use a combination of WordNet and OMCSNet to conduct 
inference on given textual input. 
 
 
3. Semantic Analysis 
Our previous inference processing system was conceived using 
only OMCSNet alone. The OMCSNet, whilst providing many 
links between the concepts, is still limited by its known 
keywords [16][17]. Because of this nature, its capability is 
quite limited and incapable of handling too many real world 
sentences [18] (Algorithm: Emotional Inference). For example, 
if a verb was used in past tense form in a sentence, in order to 
construct a relation between that word and a desired concept, it 
would require the predicates of OMCSNet to have an exact 
match of that verb. OMCSNet by itself does not have the 
functionality to process stemming issues, hence limiting 
inference to absolute exact matches. 
Although a link parser can be applied to improve the chance of 
inference on the main verb or adjective, these tokens are still 
highly likely to fall outside of the known concepts of the 
predicates list in the original OMCSNet. That's why we add 
WordNet to our design, which is capable of providing extra 
keywords into the system to cooperate with OMCSNet 
[19][20]( Simplified Algorithm: Sentence Parsing, New 
Algorithm: Emotional Inference). 
WordNet is a lexical database for English, as of WordNet 3.0 it 
has 147278 unique noun, verb, adjective and adverb strings 
[21]. Its purpose is to combine the functions of a dictionary and 
Algorithm: Emotional Inference  
Define I[] = set of tokens 
Define E[]= { happy, sad, angry, hatred, surprise }, 
emotions[sizeof(I)][4]; 
pathSum = 0; 
 
for each I[i] 
for each E[j] 
Let D[i][j] = distance( I[i], E[j] ) 
pathSum += D[i][j] 
end of for 
for each E[j] 
emotions[i][j] = (pathSum - 
D[i][j])/pathSum 
end of for 
pathSum=0 
end of for 
Simplified Algorithm: Sentence Parsing 
Data structure requirement:  
2 dequeues, digestedSymbol & digestedToken 
2 stacks, symbolDequeue & tokenDequeue 
 
For each token from argv[1] to argv[n] 
On [* : push * into symbolDequeue, push an 
empty string into tokenDequeue. 
If currentToken is [NP, skip to the 
corresponding NP] 
On *] : push symbolDequeue.top() into 
“digestedSymbol,” push 
tokenDequeue.top() into 
“digestedToken.” 
On * : Append currentString in 
tokenDequeue.top() 
The way to obtain the series of (object, event) is to 
take the front member and back member in such 
fashion (digestedToken.front(), 
digestedToken.back()), and dump the used member 
every time. Then we can obtain a series of object-
event relation description. 
 4
Currently, the agent has the ability to visual represent the five 
emotions focused in this paper: happy, sad, anger, hatred and 
surprise. 
Here we shall use a visual agent to be the front end of the 
inference system. Based on the outputs of the results of 
emotion inference from the analyzed text, the agent will pop up 
a speech bubble displaying textual reaction to the sentence 
contents.  The agent is capable to be designed to simulate 
different personalities, reflected on its speech output.  In 
addition, at the same time of displaying the textual reaction, it 
can also play a sound file that is corresponding to the context 
of emotion. 
The agent was designed with the idea of a digital secretary in 
mind [25][26][27]. It also has the functions to retrieve your 
electronic mail for you like email clients such as Outlook and 
Thunderbird, as well as update your subscribed RSS feeds. 
Furthermore, it can also be a music player with timed lyrics, 
while allowing you to keep tabs on your schedule and give you 
reminders for events you have set alarms. 
Our current version of the agent is also capable of allowing 
multiple instances of itself to communicate with each other, 
similar to how you would chat with another person (Fig. 3). 
This function is built on a keyword basis followed by decision 
trees and dialogue templates. By constructing a suitable list of 
notable keywords, one could really simulate chatting with the 
agent itself. For the scope of this paper, the agent has been 
limited to the five emotion concepts we are experimenting on. 
 
 
5. Status Classifications 
As we have mentioned before in the previous piece of work, 
emotional analysis of OMCSNet is based on concept features 
in the consumed string of text; hence it is capable of producing 
as many categories as there are related concepts. The analysis 
produces a set of numerical values for chosen focus concepts, 
each value denotes the strength of relation between the inferred 
concept of the given sentence and focus concepts. 
For this experiment, we will be using five emotion concepts.  
The five concepts are as follows: happy, sad, angry, hatred and 
surprise. 
 
 
 
6. Experiment Setup 
Our system is constructed to inference emotions from text in a 
sentence by sentence basis. The corpus of 78 sentences for this 
experiment is hand-picked from news articles related to 
education, with emphasis on each sentence contains keywords 
related to education. Source articles come from the online news 
pages of CNN, Times magazine and the English version of 
China Post [28]. 
Each sentence is fed to the system, and the analysis returns a 
set of five resultant numerals indicating the relation strength of 
the sentence content to the five chosen emotions.  If initial 
analysis with a word within the sentence fails to turn up 
relations for all five emotion concepts, OMCSNet invokes 
WordNet to conduct a search for synonyms of that word.  The 
first found synonym is used as the substitute for a new search, 
if this word also fails to return a relation, the next synonym is 
used. 
 
 
7. Experiment Results 
The following table (Table 1.) shows the totaled occurrences of 
each emotion concept in our set of 77 sentences.  The leftmost 
item, Null, is a tally of the number of sentences that have no 
resultant emotion informatics, even after substituting words for 
synonyms and hypernyms.Most of our emotion inferences are 
centered about the three emotions happy, sad and angry, as 
shown below.  The distributions of the three emotions are 
very close; we take it to be like how each person takes things 
differently. An example from our choice of sentences: “our 
students are now 32nd internationally in math scores, 10th in 
science, 12th in reading.” If you were the teacher with the 
weaker students, you would not be happy; on the flipside, if 
your students ranked above the numbers mentioned you could 
probably be gleeful of your student’ prowess. It would also 
reflect that our selection of sentences come from articles that 
are positive news, as seen in the happy category having the 
greatest number of instances with being the highest value of 
the five emotions. 
Close to a quarter of the tested sentences did not have a final 
emotion inference result.  Looking back at the sample 
sentences, it is quite likely due to the sentences not containing 
words that are associated with high emotion stimulus such as 
“excited” or “funeral”. Moreover, looking up some of the 
keywords in the OMCS predicates list simply show that there 
is no related concept entry, therefore any form to relation could 
not have been constructed with that word. 
We had first started experimenting using simple sentences for 
pretesting. The simple sentences in these formats “I went to 
school today” or “He had attended a funeral” were easy enough 
for the system to recognize. In this simple structure, the main 
clause is independent and easy to decipher because there is 
only a single pair of subject and predicate. It becomes more 
complicated when the sentence has a compound or complex 
structure. The possibility arises that parts of the sentence 
contradict each other and resulting in undesired emotion output, 
especially in the case where the content of a sentence involves 
Table 1. 
Emotion Inference Results (units in occurrences) 
Emotion Concept Happy Sad Angry Hatred Surprise Null 
Occurrences 61 59 61 3 30 18 
Greatest Value 11 3 4 0 1 N/A 
Figure 4. Capture of old agent 
(Chinese Translation: Student, please pay attention) 
 6
Inference Toolkit,  
[16] Asunción Gómez-Pérez and Oscar Corcho, Ontology 
Languages for the Semantic Web, IEEE Intelligent 
Systems, pp.54-60, 2002. 
[17] Fellbaum, C. (Ed.).: WordNet: An electronic lexical 
database. MIT Press, 1998  
[18] Gentner, D., Structure-mapping: A theoretical framework 
for analogy, Cognitive Science Vol. 7, 1983,  pp.155-
170 
[19] Wikipedia contributors, 'WordNet', Wikipedia, The Free 
Encyclopedia, 24 January 2010, 13:25 UTC, 
<http://en.wikipedia.org/w/index.php?title=WordNet&old
id=339719597> [accessed 28 February 2010] 
[20] Wikipedia contributors, 'OMCS', Wikipedia, The Free 
Encyclopedia, 24 January 2010, 13:25 UTC, < 
http://en.wikipedia.org/wiki/Open_Mind_Common_Sense
> [accessed 28 February 2010] 
[21] Miller, George A., WordNet - About Us, 
"http://wordnet.princeton.edu" 
[22] Lieberman, H., Liu, H., Singh, P., Barry, B., Beating 
Some Common Sense Into Interactive Applications, 
Submitted to AI Magazine, 2004 
[23] Mueller, E. T., Commonsense in humans, 
“http://www.signiform.com/erik/pubs/cshumans.htm” 
[24] Paiva, Affect in Interactions: Towards a New Generation 
of Interfaces, Springer Books, 1999 
[25] Wikipedia contributors, '”Ukagaka” (in Japanese Kanji)', 
Wikipedia, The Free Encyclopedia, 21, June 2009, 18:12 
UTC,  [accessed 6, August, 2009] 
[26] Chinese Ukagaka Consortium Forums (中文偽春菜後援
會論壇), http://cuc2.idv.tw/ 
[27] Chinese Ukagaka Consortium, A Brief Intro. To 
Ukagaka(中文偽春菜後援會, 淺談偽春菜), ( 7, August, 
2009), http://cuc2.idv.tw/wiki/index.php?FrontPage 
(visited 1, August, 2009) 
[28] Lin, Kevin Hsin-Yih, Changhua Yang, and Hsin-Hsi 
Chen, Emotion Classification of Online News Articles 
from the Reader’s Perspective, Proc. IEEE/WIC/ACM 
International Conference on Web Intelligence (wi 2008) , 
Sydney, Australia, 9-12 December 2008 
[29] Picard, R. W., Toward computers that recognize and 
respond to user emotion, IBM Systems Journal Col.39, 
No. 3, 2000 
[30] Picard R. W., Affective Computing: Challenge, 
International Journal of Human-Computer Studies Vol. 
59 (1-2), July 2003, pp. 55-64. 
[31] H.-J. Bullinger and J. Ziegler, Ergonomics and User 
Interfaces, Human-Computer Interaction Vol. 1, Special 
Session on Affective Computing and HCI, Proc. HCI 
International, Munich, August 1999  
[32] Klein, J., Y. Moon, and R. W. Picard, This Computer 
Responds to User Frustration, CHI 99 Short Papers, 
Pittsburgh, PA, 1999  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
無研發成果推廣資料 
