 II
中文摘要 
重建老舊影片包含兩個重要步驟：雜訊偵測及雜訊去除。經常發生
於老舊影片中的雜訊有三種：斑點、長線條、或任意等。每一類雜訊各
有不同特性，我們很難只透過一種方法來偵測所有狀況的雜訊，相同的，
對於雜訊移除後的修補也會因週圍影像的影響而增加修補的困難度。因
此，如果只單單利用空間關係想要進行修補似乎不是好的方法，勢必要
加以考慮時間關係(即前後影片)才能有較好的修補結果。本計畫提出兩
個於老舊影片中偵測斑點及長線條雜訊的方法，並提出一個考慮了時間
關係後的影像修補演算法，此演算法可克服掉修補過程中，因先後修補
所造成的錯誤。因此，由實驗結果看出，本計畫最後所得結果確實比他
人結果有較好的視覺效果。 
Abstract 
Reconstruction on aged films contains two important processes: defect detection and defect 
removal. Aged films may contain different types of defects, such as spikes and scratch lines. Each 
type of defect has its different properties. It is relatively hard to precisely detect these defects on 
aged films by using only a single solution. Besides, it is relatively hard to precisely restore aged 
films if the continuation of image property is not considered in the temporal domain. This project 
proposes two techniques to detect spikes and scratch lines on aged films and we also propose a 
technique on restoring aged films based on simultaneously considering the outmost 
patches/pixels which are going to be inpainted. The inpainting order plays an important factor for 
human visualization. The results of defect detection have good accuracy as compared to two 
well-known approaches and by eliminating the inpainting order problem, the results of restored 
films have good quality as compared to earlier approach. Application of our technique can be 
used in the recovery and restoration of aged films. 
 1
研究內容 
1. 前言 
對於年代已久的影片中多少會有雜訊的發生，像是汙點雜訊或直線雜訊等。而每一種
類雜訊都具有不同的特性，所以很難只利用單一方法修復任何類型的雜訊。因此，本研究
提出以動量估測與影像處理之相關技術為基礎，針對於汙點雜訊以及直線雜訊之修復方
法，並應用在實際的例子驗證方法的可行性。 
2. 研究目的 
在一般的老舊(黑白)影片中可能會存在許多缺陷，這些雜訊是會影響到老舊影片品質的
重要關鍵因素。普遍來說，老舊影片的缺陷可以為四種類型。第一種缺陷是影片亮度的不
穩定，這類型雜訊可以透過正規化影片的平均亮度方式來穩定影片亮度性。第二種缺陷主
要是發生在拍攝影片時所產生的晃動情況，對於影片的晃動可以透過光流法與修補技術來
維持畫面的穩定性。第三種缺陷是隨機的汙點雜訊，這種汙點類型的雜訊是以較明亮或較
灰暗的亮度出現在畫面上。汙點雜訊一般是因為空氣中的灰塵或是膠捲變質造成的，因此
出現時間是很短暫的(例如一到二張影格)。由上述特性的描述可知，這一類型雜訊可以利用
時間上的性質(temporal)與像素的屬性來偵測汙點雜訊[4]。最後的缺陷類型是直線雜訊，產
生原因是由於在放映時機器對於膠捲所產生傷害，因此可以看到直線雜訊出現在畫面有數
分鐘的時間，且一般會以很長的直線線段出現在畫面上，甚至會同等畫面的高度一樣長。
對於直線雜訊可以經由空間與時間的連續性分析與偵測。在空間資訊是以在單張影像上所
可取得的資訊進行分析，像是亮度、形狀等。時間資訊主要是直線雜訊在連續影格中的軌
跡來進行分析[6]。在修補方面，本研究提出受損區域上由外部向內部修補，在每一階段的
修補都會參考到周圍可利用的資訊，並會維持與周圍像素的關連性，因此在修補出來的結
果在會看起來很平順、自然。  
本研究主要的貢獻是在本報告的第三部份，在研究方法中將提出雜訊的偵測與修補的
方法。在第四部份，實驗結果是以實際的例子來驗證方法的可行性。最後部份是本研究的
結論。 
3. 研究方法 
參考國內外之相關研究，規劃出雜訊修補之流程(圖 1)。首先，對於輸入的影片會進行”
雜訊偵測”程序，本研究將對於常見之類型”汙點雜訊”與”直線雜訊”進行偵測。接下來在”
雜訊修補”程序，是將從前一個程序所偵測到的不同類型之雜訊進行修補，最後將結果以影
片的方式輸出。 
在公式一中，f和g分別表示marker與mask，ε表示是geodesic erosion函式，在每回合i
中f在g的限制下執行geodesic erosion，直到εg(i) ≣εg(i+1)(f)為止。在本研究中，以MAE
灰階影像做為marker，MAE灰階影像外框做為mask。在經過reconstruction by erosion運算後
的結果(圖 4 (b))，減去原影像(圖 4 (a))再透過顏色之轉換，最後再經由Otsu二值化方法後，
則可得到最後偵測雜訊之結果(圖 4 (c))。 
 
(a) (b) (c) 
圖 4，Reconstruction by Erosion 
¾ 直線雜訊偵測 
直線雜訊一般是以明亮或深暗的垂直線方式出現在影片中，由於雜訊本身是可以歸類
於資料遺失這一類，因此在[2]中提出利用Luminance cross-section方式來偵測直線雜訊。另
外，直線雜訊會在水平方向上，與中心位置上間距 3~10 像素距離方式在影片中移動。針對
上述幾點，本研究提出利用明亮度之投影與watching windows機制進行直線雜訊之偵測。以
下是直線雜訊偵測之流程圖(圖 5)。 
 
圖 5，直線雜訊偵測流程 
首先對於每一影格分割出數個horizontal bands(圖 6)。對於每一個horizontal band，計算
出在x軸每一個位置上的watching windows(在實驗裡windows寬度設為 21 個像素點)之亮度
平均值，減去在每一個x座標位置上之亮度平均。通常直線雜訊的亮度會呈現偏暗或明亮，
相對在x軸上所亮度也會相對周圍計算出的平均亮度來得低或高。因此可以經由計算在每一
個x座標上的亮度與平均亮度，來呈現出雜訊與非雜訊之間的差異(圖 7)。 
 
圖 6，分割影格 
 3
 
(a) (b) 
圖 8，判斷”可能為雜訊”線段例子 
 雜訊修補技術 
在雜訊偵測程序是使用亮度和時間連續之特質。接著在修補程序中，為了能得到滿意
的結果，本研究以地區(local)與時域(temporal)等方式來進行雜訊修補。演算法一開始，對
於在先前步驟中偵測存有雜訊區域Ω之影格I，對於破損區域利用未受損區域Φ做為修補資
訊。在本研究中直線雜訊存在於多張連續影像中，因此Ii = Φk∪Ωi，其中Ii表示在影片中
第幾張影格，Ωi表示在第i張影格中偵測到的所有雜訊。可使用的修補資訊Φk,，包含了連
續的k張影格且Φk∩Ωiψ(空集合)，也就是說非雜訊的區域將會用來做為修補的參考。在圖 
9中，∂Ωi是在屬於Φi且又是圍繞Ωi(灰色區域)的輪廓。∂Ωi是包含n個區塊(patches)，如公
式四所示。 
∂Ωi = {pj|pj(x, y) אΦi, 1≦j≦n}     (公式四) 
把這些區塊(patches)依順時鐘方式排序可得到矩陣 B，BRn*1，R 是表示區塊的值，
且區塊的大小是可以調整或者是單一像素。對於受損的區域從周圍區塊或像素依據轉換矩
陣 A 的比率分配來進行修補，如公式五所示，其中 BRn*n，n = | ∂Ωi |。 
 
圖 9，修補區域 
 5
(a)The 
Original 
Video 
(b)Defect 
Detection 
(c)Inpainting 
Result 
(e)The 
Original 
Video 
(f)Defect 
Detection 
(g)Inpainting 
Result 
(h)The 
Original 
Video 
(i)Defect 
Detection 
(j)Inpainting 
Result 
圖 11，老舊影片雜訊偵測與修補結果 
 7
 9
計畫成自評 
在老舊影片中可能存在著不同類型的雜訊，本研究針對於汙點及直線這兩種類型雜訊
修復提出了新的演算法。對於汙點雜訊利用到動量估測的方法針對在時域上的變化進行估
測後，再以 reconstruction by erosion 等方法。直線雜訊偵測利用亮度差異可分類出”非雜
訊”、”可能的雜訊”與”雜訊”以及權重計算等方法，經由以上的程序可明確的把存在老舊影
片中的雜訊標示出來。在本研究提出不考慮在常見修補方法中注重的優先順序的方法之
下，相對排除掉排列修補優先序的程序來減少整體修補時間。除此之外，在本研究中利用
的影片連續的特性，可以在連續的影格中搜尋最佳的修補資訊來進行破損修補，相對在實
驗結果中也得到不錯的結果。 
 
參加會議經過 
98年 6 月 18 日 
報告人姓名 王駿瑋 就讀系所及年級 淡江大學資訊工程系博三 
參加會議經過： 
此次在杭州參加由 WSEAS 主辦的 2009 國際科學與工程協會(WSEAS)國際會議，我在會議前
一天即搭乘國泰航空的早班飛機前往香港，再轉機到杭州蕭山機場。由於會議在飯店舉行，很
方便。而本次的旅程為 5日 4夜，扣掉出發與離開，在杭州的時間為 3天，正好是會議舉行的
3 天。第 1 天除了領取註冊收據之外，還參加了 2 場 seesion，雖英文聽力沒有很好，但勉強
聽得懂。第 2 天早上專心準備下午的報告，當天也就只參加這麼一場 session，第 3 天，也是
這次杭州之旅的最後一天，我到了西湖走走，西湖景色幽美，令人流連忘返。隔天便搭機回到
台灣。此次參與國際性的研討會，讓我體驗了杭州的國際化;此外發表論文過程順利，並獲得
一些寶貴意見以供未來研究之用。 
註：請以 12 字大小繕打 
2 Related Work 
In most methods of copy-move forgery detection, the 
detected image is divided into overlapping blocks, 
which are then represented as vectors, which are then 
lexicographically sorted for later detection. Suppose a 
detected image of size NN × is divided 
into 2)1( +− bN overlapping blocks of size bb× , which 
are represented as vectors of 2b dimension, and sorted 
in a lexicographical order. Vectors corresponding to 
blocks of similar content would be close to each other in 
the list, so that identical regions could be easily detected. 
The image given in Figure 2(a) was tampered by 
copy-move forgery, as shown in Figure 2(b), in which 
block B1, B2, and block B3 are copies of blocks A1, A2, 
and block A3, respectively, and thus VA1 =VB1, VA2 =VB2, 
and VA3 =VB3, where VX denotes the vector 
corresponding to block X. As shown in Figure 2(c), 
identical vectors are adjacent in the sorted list, from 
which the copy-move regions could be easily detected. 
In the previously mentioned methods, the vectors were 
sorted by the lexicographical sort, which took 
)lg(O kk time to sort on each entry in the vectors, 
where k = 2)1( +− bN . The time complexity of 
lexicographical sorting on theses vectors 
is )lg(O 2 kkb when the vectors are of b2. Farid et al. [6] 
reduced the time complexity to )lg32(O kk× by using 
PCA (principle component analysis). 
 
 
Fig. 2 (a). An original image, (b). Three pairs of identical 
blocks are marked by squares, (c). feature vectors 
corresponding to the divided blocks are sorting in a list. 
 
The time complexity of the method proposed by G. Li 
et al. [1] was reduced to )lg8(O kk×  by using SVD. W. 
Luo et al. [11] defined a feature vector of 7-dimension 
to represent blocks so as the time complexity is reduced 
to )lg7(O kk× . In this paper, we shall propose a further 
efficient method for copy-move forgery detection. 
 
 
3 The Propose Method 
For resistance against various modifications and 
improving the sorting time, we represent each block B 
by a 9-dimensional feature vector vB = (x1, x2, …, x9), 
which is defined as follows. Firstly, the block B is 
divided into four equal-sized sub-blocks, S1, S2, S3, and 
S4, as shown in Fig. 3 and let Ave(.) denote the average 
intensity function. Then as described in (1), f1 denotes 
the average intensity of the block B, the entries f2, f3, f4, 
and f5 denote the ratios of the average intensities of the 
blocks S1, S2, S3, and S4 to f1, respectively, and f6, f7, f8, 
and f9 stand for the differences of the average intensities 
of the blocks S1, S2, S3, and S4 from f1, respectively. 
Finally, entries fi’s are normalized to integers xi’s 
ranging from 0 to 255, as described in (2). Although 
these 9 entities contain duplicated information, they 
together possess higher capability for modification 
resistance, such as JPEG compression and Gaussian 
noise. 
 
 
Fig. 3 A block is divided into four equal-sized sub-blocks. 
 
 
⎪⎩
⎪⎨
⎧
≤≤−=
≤≤+
==
=
−
−
.96 if)()(
)1(,52 if))(4/()(
,1 if)(
5
11
iSAveSAvef
iSAveSAve
iSAvef
f
ii
i
i
i ε     
 
  
⎣ ⎦
⎣ ⎦
{ } { }.min and  max where
,96 if255
)2(,52 if255
,1 if
962961
221
2
iiii
i
i
i
i
fmfm
i
mm
mf
if
if
x
≤≤≤≤
==
⎪⎪
⎪
⎩
⎪⎪
⎪
⎨
⎧
≤≤⎥⎦
⎥⎢⎣
⎢
+−
−×
≤≤×
=
=
ε
 
 
Unlike the matrix constructed by Farid et al. [6], 
which stores floating numbers (the PCA coefficients), 
the feature vectors we extract store integers. As a result, 
we may use the efficient radix sort algorithm to perform 
lexicographical sorting over those vectors. If the given 
image of size NN × is divided into overlapping blocks 
Data sets of 
Copy-move images 
No. of 
images 
Detection rate 
(%) 
without 
midification 
50 98 
JPEG compression 
QF = 100 
50 98 
 
JPEG compression 
QF = 90 
50 98 
JPEG compression 
QF = 80 
50 96 
Gaussian noise 
SNR = 10 
50 98 
Gaussian noise 
SNR = 20 
50 98 
Gaussian noise 
SNR = 35 
50 94 
Table 1 Detection rates for datasets of copy-move images. 
 
 
4 Conclusion 
In this paper, we propose an efficient method for 
copy-move forgery detection. Using of radix sort 
dramatically improves the time complexity and the 
adopted features enhance the capability of resisting of 
various attacks such as JPEG compression and Gaussian 
noise. Both efficiency and high detection rates have 
been demonstrated in our experimental results. However, 
a few small copied regions were not successfully 
detected. In the future, we would like to extend our 
work to video images. 
 
 
References: 
[1] Guohui Li, Qiong Wu, Dan Tu, and Shaojie Sun, “A 
sorted neighborhood approach for detecting 
duplicated regions in image forgeries based on DWT 
and SVD,” in Proceedings of IEEE International 
Conference on Multimedia and Expo, Beijing China, 
2-5 July 2007, pp. 1750-1753. 
[2] Peter Nillius and Jan-Olof Eklundh, “Automatic 
estimation of the projected light source direction,” in 
Proceedings of 2001 IEEE Computer Society 
Conference on Computer Vision and Pattern 
Recognition, Vol. 1, 2001, pp. 1076- 1083. 
[3] Wei Lu, Fu-Lai Chung, and Hongtao Lu, “Blind fake 
image detection scheme using SVD,” IEICE 
Transaction on Communications, Vol. E89-B, No. 5, 
May 2006, pp. 1726-1728. 
[4] J. Lukas, J. Fridich, and M. Goljan, “Detecting 
digital image forgeries Using sensor pattern noise,” 
in Proceedings of the SPIE Conference on Security, 
Steganography, and Watermarking of Multimedia 
Contents, Vol. 6072, January 2006, pp. 362-372. 
[5] J. Fridrich, D. Soukal, and J. Lukas, “Detection of 
copy-move forgery in digital images,” in 
Proceedings of Digital Forensic Research Workshop, 
August 2003. 
[6] A.C. Popescu and H. Farid, “Exposing digital 
forgeries by detecting duplicated image regions,” 
Technical Report, TR2004-515, Dartmouth College, 
Computer Science 2004. 
[7] M.K. Johnson and H. Farid, “exposing digital 
forgeries by detecting inconsistencies in lighting,” in 
Proceedings of ACM Multimedia and Security 
Workshop, New York, 2005, pp. 1-9. 
[8] A.C. Popescu and H. Farid, “Exposing digital 
forgeries by detecting traces of resampling,” IEEE 
Transactions on Signal Processing, Vol. 53, 2005, pp. 
758-767. 
[9] A.C. Popescu and H. Farid, “Exposing digital 
forgeries in color filter array interpolated images,” 
IEEE Transactions on Signal Processing, Vol. 53, 
2005, pp.    3948–3959.  
[10] Kwang-Fu Li, Tung-Shou Chen, and Seng-Cheng 
Wu, “Image tamper detection and Recovery 
System Based on Discrete Wavelet 
Transformation,” Intelligent Conference 
Communications, Computers and Signal 
Processing, Vol. 1, pp. 26-28, 2001. 
[11] Wwiqi Luo, Jiwu Huang, and Guiping Qiu, 
“Robust detection of region- duplication forgery in 
digital image,” in Proceedings of the 18th 
International Conference on Pattern Recognition, 
Vol. 4, 2006, pp. 746-749. 
[12] N. Khanna, A.K. Mikkilineni, G.T.C. Chiu, J.P. 
Allebach, and E.J. Delp, “Scanner identification 
using sensor pattern noise,” in Proceedings of the 
SPIE International Conference on Security, 
Steganography, and Watermarking of Multimedia 
Contents IX, Vol. 6505, No. 1, 2007, pp. 65051K. 
參加會議經過 
98年 6 月 18 日 
報告人姓名 王駿瑋 就讀系所及年級 淡江大學資訊工程系博三 
參加會議經過： 
此次在杭州參加由 WSEAS 主辦的 2009 國際科學與工程協會(WSEAS)國際會議，我在會議前
一天即搭乘國泰航空的早班飛機前往香港，再轉機到杭州蕭山機場。由於會議在飯店舉行，很
方便。而本次的旅程為 5日 4夜，扣掉出發與離開，在杭州的時間為 3天，正好是會議舉行的
3 天。第 1 天除了領取註冊收據之外，還參加了 2 場 seesion，雖英文聽力沒有很好，但勉強
聽得懂。第 2 天早上專心準備下午的報告，當天也就只參加這麼一場 session，第 3 天，也是
這次杭州之旅的最後一天，我到了西湖走走，西湖景色幽美，令人流連忘返。隔天便搭機回到
台灣。此次參與國際性的研討會，讓我體驗了杭州的國際化;此外發表論文過程順利，並獲得
一些寶貴意見以供未來研究之用。 
註：請以 12 字大小繕打 
2 Related Work 
In most methods of copy-move forgery detection, the 
detected image is divided into overlapping blocks, 
which are then represented as vectors, which are then 
lexicographically sorted for later detection. Suppose a 
detected image of size NN × is divided 
into 2)1( +− bN overlapping blocks of size bb× , which 
are represented as vectors of 2b dimension, and sorted 
in a lexicographical order. Vectors corresponding to 
blocks of similar content would be close to each other in 
the list, so that identical regions could be easily detected. 
The image given in Figure 2(a) was tampered by 
copy-move forgery, as shown in Figure 2(b), in which 
block B1, B2, and block B3 are copies of blocks A1, A2, 
and block A3, respectively, and thus VA1 =VB1, VA2 =VB2, 
and VA3 =VB3, where VX denotes the vector 
corresponding to block X. As shown in Figure 2(c), 
identical vectors are adjacent in the sorted list, from 
which the copy-move regions could be easily detected. 
In the previously mentioned methods, the vectors were 
sorted by the lexicographical sort, which took 
)lg(O kk time to sort on each entry in the vectors, 
where k = 2)1( +− bN . The time complexity of 
lexicographical sorting on theses vectors 
is )lg(O 2 kkb when the vectors are of b2. Farid et al. [6] 
reduced the time complexity to )lg32(O kk× by using 
PCA (principle component analysis). 
 
 
Fig. 2 (a). An original image, (b). Three pairs of identical 
blocks are marked by squares, (c). feature vectors 
corresponding to the divided blocks are sorting in a list. 
 
The time complexity of the method proposed by G. Li 
et al. [1] was reduced to )lg8(O kk×  by using SVD. W. 
Luo et al. [11] defined a feature vector of 7-dimension 
to represent blocks so as the time complexity is reduced 
to )lg7(O kk× . In this paper, we shall propose a further 
efficient method for copy-move forgery detection. 
 
 
3 The Propose Method 
For resistance against various modifications and 
improving the sorting time, we represent each block B 
by a 9-dimensional feature vector vB = (x1, x2, …, x9), 
which is defined as follows. Firstly, the block B is 
divided into four equal-sized sub-blocks, S1, S2, S3, and 
S4, as shown in Fig. 3 and let Ave(.) denote the average 
intensity function. Then as described in (1), f1 denotes 
the average intensity of the block B, the entries f2, f3, f4, 
and f5 denote the ratios of the average intensities of the 
blocks S1, S2, S3, and S4 to f1, respectively, and f6, f7, f8, 
and f9 stand for the differences of the average intensities 
of the blocks S1, S2, S3, and S4 from f1, respectively. 
Finally, entries fi’s are normalized to integers xi’s 
ranging from 0 to 255, as described in (2). Although 
these 9 entities contain duplicated information, they 
together possess higher capability for modification 
resistance, such as JPEG compression and Gaussian 
noise. 
 
 
Fig. 3 A block is divided into four equal-sized sub-blocks. 
 
 
⎪⎩
⎪⎨
⎧
≤≤−=
≤≤+
==
=
−
−
.96 if)()(
)1(,52 if))(4/()(
,1 if)(
5
11
iSAveSAvef
iSAveSAve
iSAvef
f
ii
i
i
i ε     
 
  
⎣ ⎦
⎣ ⎦
{ } { }.min and  max where
,96 if255
)2(,52 if255
,1 if
962961
221
2
iiii
i
i
i
i
fmfm
i
mm
mf
if
if
x
≤≤≤≤
==
⎪⎪
⎪
⎩
⎪⎪
⎪
⎨
⎧
≤≤⎥⎦
⎥⎢⎣
⎢
+−
−×
≤≤×
=
=
ε
 
 
Unlike the matrix constructed by Farid et al. [6], 
which stores floating numbers (the PCA coefficients), 
the feature vectors we extract store integers. As a result, 
we may use the efficient radix sort algorithm to perform 
lexicographical sorting over those vectors. If the given 
image of size NN × is divided into overlapping blocks 
