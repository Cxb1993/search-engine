2      Hsin-Hsi Chen and Yih-Chen Chang 
available.  Users often use their familiar languages to annotate images 
and express their information needs.  Cross-language image retrieval 
(CLMR) becomes more practical.  CLMR, which is some sort of cross-
language cross-media information retrieval (CL-CM-IR), allows users 
employ text queries (in one language) and example images (in one 
medium) to access image database with text descriptions (in another 
language/medium).  Two languages, i.e., query language and document 
language, and two media, i.e., text and image, are adopted to express 
the information needs and the data collection.  Both language 
translation and media transformation have to be dealt with. 
Cross-language information retrieval (CLIR) facilitates the uses of 
queries in one language to access documents in another language.  It 
touches on the multilingual aspect only.  Language unification is the 
major issue in CLIR.  Either query translation or document translation 
can be considered.  In the past, dictionary-based, corpus-based and 
hybrid approaches have been proposed [3][11].  Dictionary-based 
approach exploits bilingual machine-readable dictionaries.  Translation 
ambiguity, target polysemy and coverage of dictionaries are several 
important issues to tackle.  Target term selection strategies like select 
all, select N randomly and select best N, and selection level like words 
and phrases have been presented.  Corpus-based approach exploits a 
bilingual parallel corpus, which is a collection of original texts and 
their translations.  Such a corpus may be document-aligned, sentence-
aligned or word-aligned.  Corpus-based approach has been employed to 
set up a bilingual dictionary, or to translate a source query to a target 
one.  Dictionaries and corpora are complementary.  The former 
provides broad and shallow coverage, while the latter provides narrow 
(domain-specific) but deep (more terminology) coverage of the 
language. 
Compared with CLIR, image retrieval touches on medium aspect 
rather than multilingual issue.  Two types of approaches, i.e., content-
based and text-based approaches, are usually adopted in image retrieval 
[8].  Content-based image retrieval (CBIR) uses low-level visual 
features to retrieve images.  In such a way, it is unnecessary to annotate 
images and transform users’ queries.  However, due to the semantic gap 
between image visual features and high-level concepts [7], it is still 
challenging to use a CBIR system to retrieve images with correct 
semantic meanings.  Integrating textual information may help a CBIR 
system to cross the semantic gap and improve retrieval performance. 
4      Hsin-Hsi Chen and Yih-Chen Chang 
 
 
 
Figure 1. A Media-Mapping Approach 
Figure 2 shows a model called 1L1M, i.e., (Q1 Ö Q2) and (Q4 Ö 
Q
ed into a textual 
on
y the textual run and the visual run are 
m
 model called 1L2M 
w
 
5), including one language translation and one media transformation.  
The overall architecture consists of a textual run, an initial visual run, 
and a relevance feedback run.  The original textual query initiates a 
textual run.  Its procedure is the same as a traditional CLIR system.  A 
source textual query is translated into a target textual one, and then the 
target query is submitted to a text-based IR system.  A set of relevant 
text descriptions is reported along with their images. 
In an initial visual run, a visual query is transform
e through the media-mapping approach.  Then, the textual query is 
sent to a text-based IR system to retrieve the relevant text descriptions, 
and, at the same time, the relevant images.  This procedure is similar to 
traditional relevant feedback except that the feedback comes from 
another media, i.e., text, rather than the original media, i.e., image.  
This is because the performance of content-based IR is usually worse 
than that of text-based IR. 
The results generated b
erged together.  The similarity scores of images in the two runs are 
normalized and linearly combined by weighting. 
In Section 4, we will consider an alternative
hich includes one language translation (Q1 Ö Q2) and two media 
6      Hsin-Hsi Chen and Yih-Chen Chang 
Scotland.  The majority of images (82%) in the St. Andrews image 
collection are in black and white.  All images are accompanied by a 
caption written in English by librarians working at St. Andrews 
Library.  The information in a caption ranges from specific date, 
location, and photographer to a more general description of an image.  
Figure 3 shows an example of image and its caption in the St. Andrews 
image collection.  The text descriptions are semi-structured and consist 
of several fields including document number, headline, record id, 
description text, category, and file names of images in a 368×234 large 
version and 120×76 thumbnail version. 
The 2004 and the 2005 test sets contain 25 topics and 28 topics, 
respectively.  Each topic consists of a title (a short sentence or phrase 
describing the search request in a few words), and a narrative (a 
description of what constitutes a relevant or non-relevant image for that 
each request).  In addition to the text description for each topic, one and 
two example images are provided for the 2004 and the 2005 topic sets, 
respectively.  In our experiments, queries are in Chinese.  Figure 4 
illustrates a topic of the 2005 topic set in English and in Chinese along 
with two example images.  
 
 
<DOC> 
<DOCNO>stand03_1029/stand03_5473.txt         
</DOCNO> 
<HEADLINE> Horse and handler. 
</HEADLINE> 
<TEXT> 
<RECORD_ID>LHG-.000010.-.000067 
</RECORD_ID> 
Horse Stable hand holding bridle of heavy 
horse on grass slope, farm buildings, tall 
chimney and trees behind wall beyond. 
ca.1900 Lady Henrietta Gilmour 
Scotland LHG-10-67  
<CATEGORIES> 
[chimneys - industrial],[horses &  
ponies],[Scotland unidentified 
views],[Collection - Lady H Gilmour] 
</CATEGORIES>   </TEXT> 
Figure 3. An Image and Its Description 
8      Hsin-Hsi Chen and Yih-Chen Chang 
Table 1. Performance of monolingual (Q0)/cross-lingual information 
retrieval (Q1ÖQ2) 
Model 2004 topic set 2005 topic set 
Q0 0.6304 0.3952 
Q1ÖQ2  0.4395 0.2399 
 
Next, we consider image query only.  Compared with Table 1, Table 
2 shows that content-based IR is much worse than monolingual IR and 
crosslingual IR.  Because example images (i.e., image queries) are in 
the data set, they are often the top-1 and the top-2 images reported by 
the content-based IR system for the 2004 and the 2005 topic sets, 
respectively.  To evaluate the real performance, we consider two cases: 
the data sets with and without the example images.  It is trivial that the 
MAPs of the former (0.0672 and 0.0725) are better than those of the 
latter (0.0149 and 0.0259). 
Table 2. Performance of content-based information retrieval (CBIR) 
 Keep Example Images Remove Example Images 
 2004  
topic set 
2005  
topic set 
2004  
topic set 
2005  
topic set 
CBIR 0.0672 0.0725 0.0149 0.0259 
 
With the media-mapping approach, we generate a text query Q5 from 
the text description counterparts of retrieved images by using Q4, 
shown in Figure 1.  Table 3 illustrates the performance of Q5 when all 
the words are selected from the text descriptions of the top-n retrieved 
images.  Comparing Tables 2 and 3, media transformation from image 
to text is better than CBIR directly. 
10      Hsin-Hsi Chen and Yih-Chen Chang 
whether the examples are kept in or removed from the test collection, 
the 1L1M model (Q1ÖQ2)∪(Q4ÖQ5) is better than the 1L0M model 
(Q1ÖQ2)∪Q4. 
Table 5. Performance of 1 Language Translation and 1 Media 
Transformation Using ALL 
Top-n Keep Example Images Remove Example Images 
Images↓ 2004  
topic set 
2005  
topic set 
2004  
topic set 
2005  
topic set 
1 0.5220 0.2930 0.4542 0.2440 
2 0.5243 0.3194 0.4476 0.2410 
3 0.5184 0.3207 0.4508 0.2497 
4 0.5097 0.3139 0.4484 0.2526 
5 0.4985 0.3030 0.4519 0.2526 
 
We further introduce the 1L2M model to integrate text and image 
queries.  This model, denoted by (Q1ÖQ2)∪((Q1ÖQ3 selected by 
Q4)ÖQ5), consists of one language translation, and two media 
transformations.  In the first media mapping, the system employs Q1 to 
select 1,000 text descriptions, then Q4 to re-rank the 1,000 image 
counterparts, and finally reports re-ranking results Q3 as retrieval 
results of Q1.  Because Q1 filters out most of the irrelevant images, and 
the search scope is narrowed down, it is more probable to select 
relevant images by using Q4.  In the second media mapping, the re-
ranking result Q3 is considered as new image query Q4’, and 
transformed into Q5.  Textual terms in Q5 are selected from the 
relevant text counterparts of a content-based image retrieval using Q4’.  
Finally, we merge the results of two monolingual text retrieval using 
Q2 and Q5 with weights 0.7 and 0.3.   
Table 6 shows the performance of the new model when all words in 
the text descriptions are selected.  Compared with Table 5, the 
performance is improved when example images are removed from test 
collections.  We employ another alternative – say, Chi-Square, to select 
suitable terms to form query Q5.  Table 7 shows the performance of the 
1L2M model on the test collection without example images.  The best 
MAPs are 0.4740 and 0.2729 for the 2004 and the 2005 topic sets, 
respectively, which are 87.15% and 72.39% of the performance of the 
mono-lingual image retrieval Q0∪Q4 (refer to Table 4).  Compared 
with 1L0M model (Q1ÖQ2)∪Q4, the improvement of 1L2M model 
12      Hsin-Hsi Chen and Yih-Chen Chang 
or “coach” in the reported images are suggested for retrieval.  
Similarly, query “建築物上飄揚的旗子” is translated into 
“building on aflutter flag”.  Image retrieval re-ranks those 
images containing “flags”, and contributes the concept “flying”. 
(2) The translation is correct, but the text description is not 
matched to the translation equivalent.  For example, query “蘇
格蘭的太陽 ” is translated into “Scotland Sun” correctly, 
however, the corresponding concepts in relevant images are 
“sunrise” or “sunset”.  The first media transformation proposes 
those images containing “sun”, and the second media 
transformation suggests the relevant concepts, i.e., “sunrise” or 
“sunset”. 
(3) The translation is correct, and the text description of images is 
exactly matched to the translation equivalent.  For example, 
query “動物雕像” is translated into “animal statue” correctly.  
Here, “statue” is enhanced, so that those images containing the 
concept are re-ranked to the top. 
To sum up, in the two media transformation model, the first media 
mapping (i.e., textÆimage) derives image query to capture extra 
information other than text query.  The second media mapping (i.e., 
image Æ text) generates text query for more reliable text-based 
retrieval other than content-based retrieval.  These two procedures are 
complementary, so that the model results in good performance. 
6. Concluding Remarks 
Cross-lingual IR achieves 69.72% and 60.70% of mono-lingual IR in 
the two topic sets used in this paper.  Content-based IR gets much less 
performance, i.e., it achieves only 2.36% and 6.55% of text-based IR.  
Naïve merging the results of text and image queries gain no benefit.  It 
would be doubt if integrating content-based IR and text-based IR was 
helpful for cross-language image retrieval under such a poor CBIR 
system.   
Compared to content-based IR, the generated text query from the 
given image query using media-mapping approach improves the 
original performance from 0.0149 and 0.0259 to 0.0704 and 0.0582 in 
the best setting.  When both text and image queries are considered, the 
best cross-lingual image retrieval model (i.e., the 1L2M model using χ2 
14      Hsin-Hsi Chen and Yih-Chen Chang 
6. Davis, M.W. and Dunning, T.: A TREC Evaluation of Query 
Translation Methods for Multi-lingual Text Retrieval.  In: 
Proceedings of TREC-4 (1996) 483-498. 
7. Eidenberger, H. and Breiteneder, C.: Semantic Feature Layers in 
Content-based Image Retrieval: Implementation of Human World 
Features.  In: Proceedings of International Conference on Control, 
Automation, Robotic and Vision (2002). 
8. Goodrum, A.A.: Image Information Retrieval: An Overview of 
Current Research.   Information Science 3(2) (2000) 63-66. 
9. Jones, G.J.F., Groves, D., Khasin, A., Lam-Adesina, A., Mellebeek, 
B. and Way, A.:  Dublin City University at CLEF 2004: Experiments 
with the ImageCLEF St. Andrew's Collection.  In: Proceedings of 5th 
Workshop of the Cross-Language Evaluation Forum LNCS 3491 
(2005) 653-663. 
10.Lin, W.C., Chang, Y.C. and Chen, H.H.: Integrating Textual and 
Visual Information for Cross-Language Image Retrieval.  In: 
Proceedings of the Second Asia Information Retrieval Symposium 
LNCS 3689 (2005) 454-466. 
11.Oard, D.W.: Alternative Approaches for Cross-Language Text 
Retrieval.  In: Working Notes of AAAI-97 Spring Symposiums on 
Cross-Language Text and Speech Retrieval (1997) 131-139. 
16      Yih-Chen Chang and Hsin-Hsi ChenT*T 
performance of cross-language image retrieval. The challenging issue is 
the semantic differences among visual and textual information. For 
example, using the visual information “red circle” may retrieve noise 
images containing “red flower”, “red balloon”, “red ball”, and so on, 
rather than the desired ones containing “sun”. The semantic difference 
between visual concept “red circle” and textual symbol “sun” is called 
a semantic gap.  
Some approaches conducted text- and content-based image 
retrieval separately and then merged the results of two runs (Besançon, 
et al., 2005; Jones, et al., 2005; Lin, Chang & Chen, forthcoming). 
Content-based image retrieval may suffer from the semantic gap 
problem and report noise images. That may have negative effects on 
the final performance. Some other approaches (Lin, Chang & Chen, 
forthcoming) learned the relationships between visual and textual 
information and used the relationships for media transformation. The 
final retrieval performance depends on the relationship mining. 
In this paper, we use two intermedia approaches to capture the 
semantic gap. The main characteristic of these approaches is that 
human knowledge is imbedded in the intermedia and can be used to 
compute the semantic similarity of images.  A word-image ontology 
and an annotated image corpus are explored and compared. Section 2 
specifies how to build and use the word-image ontology. Section 3 
deals with the uses of the annotated corpus. Sections 4 and 5 show and 
discuss the official runs in ImageCLEFphoto2006. 
2. An Approach of Using a Word-Image Ontology 
2.1 Building the Ontology 
A word-image ontology is a word ontology aligned with the related 
images on each node. Building such an ontology manually is time 
consuming. In ImageCLEFphoto2006, the image collection has 20,000 
colored images.  There are 15,998 images containing English captions 
in <TITLE> and <DESCRIPTION> fields. The vocabularies include 
more than 8,000 different words, thus an ontology with only hundreds 
words is not enough.  
18      Yih-Chen Chang and Hsin-Hsi ChenT*T 
length, we also consider the weighting of links in a path shown as 
follows. 
(1) When computing the semantic distance of a node A and its 
child B, we consider the number of children of A.  The more children A 
has, the larger the distance between A and B is. In an extreme case, if A 
has only one child B, then the distance between A and B is 0.  Let 
#children(A) denote the number of children of A, and base(A) denote 
the basic distance of A and its children.  We define base(A) to be 
log(#children(A)). 
(2) When computing the semantic distance of a node A and its 
brother, we consider the level it locates. Assume B is a child of A. If A 
and B have the same number of brothers, then the distance between A 
and its brothers is larger than that between B and its brothers.  Let 
level(A) be the depth of node A.  Assume the level of root is 0.  The 
distance between node A and its child, denoted by dist(A), is defined to 
be .  Here C is a constant between 0 and 1. In this paper, 
C is set to 0.9. 
)()( Abasec Alevel ×
If the shortest path between two different nodes N0 and Nm is N0, 
N1, …, NLCA, …, Nm-1, Nm, we define the distance between N0 and Nm to 
be: 
)()(),(
1-
1
0 ∑
=
+=
m
i
iLCAm NdistNdistNNdist  
The larger the distance of two nodes is, the less similar the nodes are. 
2.2.2 Mapping into the Ontology 
Before counting the semantic distance between two given images, we 
need to map the two images into nodes of the ontology. In other words, 
we have to find the fundamental concepts the two images consist of. A 
CBIR system is adopted. It regard an image as a visual query and 
retrieves the top k fundamental images (i.e., fundamental concepts) in 
the word-image ontology. In such a case, we have two sets of nodes 
S1={n11, n12, n13, …, n1k} and S2={n21, n22, n23, …, n2k}, which 
correspond to the two images, respectively. We define the following 
formula to compute the semantic distance: 
 
 kjwherenndistSSstanceSemanticDi j
k
i
i ,...,1)),,(min(),( 2
1
121 ==∑
=
20      Yih-Chen Chang and Hsin-Hsi ChenT*T 
images in the image collection. BM25 formula measures the similarity 
between example images and images in image collection.  
4.  Experiments 
In the formal runs, we submitted 25 cross-lingual runs for eight 
different query languages. All the queries with different source 
languages were translated into target language (i.e., English) using 
SYSTRAN system. We considered several issues, including (1) using 
different intermedia approaches (i.e., the text-image ontology and the 
annotated image corpus), and (2) with/without using visual query. In 
addition, we also submitted 4 monolingual runs which compared (1) the 
annotation in English and in German, and (2) using or not using visual 
query and intermedia. At last, we submitted a run using visual query 
and intermedia only. The details of our runs are described as follows: 
(1) 8 cross-lingual and text query only runs: 
 NTU-PT-EN-AUTO-NOFB-TXT,  
 NTU-RU-EN-AUTO-NOFB-TXT,  
 NTU-ES-EN-AUTO-NOFB-TXT,  
 NTU-ZHT-EN-AUTO-NOFB-TXT,  
 NTU-FR-EN-AUTO-NOFB-TXT,  
 NTU-JA-EN-AUTO-NOFB-TXT,  
 NTU-IT-EN-AUTO-NOFB-TXT, and  
 NTU-ZHS-EN-AUTO-NOFB-TXT.  
These runs are regarded as baselines and are compared with the runs 
using both textual and visual information.  
(2)  2 monolingual and text query only runs:  
 NTU-EN-EN-AUTO-NOFB-TXT, and 
 NTU-DE-DE-AUTO-NOFB-TXT. 
These two runs serve as the baselines to compare with cross-lingual 
runs with text query only, and to compare with the runs using both 
textual and visual information. 
(3)  1 visual query only run with the approach of using an annotated 
image corpus: 
 NTU-AUTO-FB-TXTIMG-WEprf.  
This run will be merged with the runs using textual query only, and 
is also a baseline to compare with the runs using both visual and textual 
queries.  
22      Yih-Chen Chang and Hsin-Hsi ChenT*T 
(i.e., Text Only vs. Text + Annotation and Text + Ontology). In 
addition, we also compare the runs using word-image ontology and the 
runs using annotated image corpus (i.e., Text + Ontology vs. Text + 
Annotation). The runs whose performance is better than that of baseline 
(i.e., Text Only) will be marked in bold.  The results show all runs 
using annotated image corpus are better than the baseline. In contrast, 
only two runs using word-image ontology are better. 
Table 1. Performance of Official Runs (T=text only, A=annotated 
image corpus, O=word-image ontology) 
Query 
Language 
MAP Description Runs 
Portuguese 0.1630 T NTU-PT-EN-AUTO-
NOFB-TXT 
 0.2854 T+A NTU-PT-EN-AUTO-FB-
TXTIMG-T-WEprf 
 0.1580 T+O NTU-PT-EN-AUTO-
NOFB-TXTIMG-T-
IOntology 
Russian 0.1630 T NTU-RU-EN-AUTO-
NOFB-TXT 
 0.2789 T+A NTU-RU-EN-AUTO-FB-
TXTIMG-T-Weprf 
 0.1591 T+O NTU-RU-EN-AUTO-
NOFB-TXTIMG-T-
IOntology 
Spanish 0.1595 T NTU-ES-EN-AUTO-
NOFB-TXT 
 0.2775 T+A NTU-ES-EN-AUTO-FB-
TXTIMG-T-Weprf 
 0.1554 T+O NTU-ES-EN-AUTO-
NOFB-TXTIMG-T-
IOntology 
French 0.1548 T NTU-FR-EN-AUTO-
NOFB-TXT 
 0.2758 T+A NTU-FR-EN-AUTO-FB-
TXTIMG-T-WEprf 
 0.1525 T+O NTU-FR-EN-AUTO-
24      Yih-Chen Chang and Hsin-Hsi ChenT*T 
examine the performance of our intermedia approach. In the 
experiments, we took out the example images from the image 
collection when mapping example images into intermedia. Table 2 
shows the experiment results. Comparing Table 1 and Table 2 we find 
the performance of Table 2 is lower than that of Table 1. It shows the 
performance of CBIR in mapping stage will influence the final result 
and that is very critical. From Table 2, we also find that the approaches 
of annotated image corpus are better than the runs using textual query 
only. It shows even mapping stage have some errors, the annotated 
image corpus can still work well.  
Table 2. MAP of Runs by Removing Example Images from the 
Collection 
Query 
Language 
Text Only Text + Annotated 
image corpus 
Portuguese 0.1630 0.1992 
Russian 0.1630 0.1880 
Spanish 0.1595 0.1928 
French 0.1548 0.1848 
Simplified 
Chinese 
0.1248 0.1779 
Japanese 0.1431 0.1702 
Traditional 
Chinese 
0.1228 0.1757 
Italian 0.1340 0.1694 
 
Table 3 shows the experiment results of monolingual runs. Using 
both textual and visual queries are still better than runs using textual 
query only. The performance of the runs by taking out the example 
images from collection beforehand is still better than the runs use 
textual query only. From this table, we also find the runs using textual 
query only does not perform well even in monolingual runs. This may 
be because the image captions of this year are shorter and we do not 
have enough information when we use textual information only. In 
addition, when image captions are short too, the little differences in 
vocabularies between query and document may influence the results a 
lot. Therefore, German monolingual run and English monolingual run 
perform so different.  
26      Yih-Chen Chang and Hsin-Hsi ChenT*T 
6.  Conclusion 
The experiments show visual query and intermedia approaches are 
useful. Comparing the runs using textual query only with the runs 
merging textual query and visual query, the latter improved 71%~119% 
of performance of the former. Even in the situation which example 
images are removed from the image collection, the performance can 
still be improved about 21%~43%. We find visual query in image 
retrieval is important. The performance of the runs using visual query 
only can be even better than the runs using textual only if we translate 
visual information into textual one correctly. In this year the word-
image ontology built automatically still contain much noise. We will 
investigate how to filter out the noise and explore different methods. 
References 
1. Besançon, R., Hède, P., Moellic, P.A., & Fluhr, C. (2005). Cross-
media feedback strategies: Merging text and image information to 
improve image retrieval.  In Peters, C.; Clough, P.; Gonzalo, J.; 
Jones, G.J.F.; Kluck, M.; Magnini, B. (Eds.), Proceedings of 5th 
Workshop of the Cross-Language Evaluation Forum, LNCS 3491, 
(pp. 709-717). Berlin: Springer.
2. Clough, P., Sanderson, M. & Müller, H. (2005). The CLEF 2004 
cross language image retrieval track.  In Peters, C.; Clough, P.; 
Gonzalo, J.; Jones, G.J.F.; Kluck, M.; Magnini, B. (Eds.), 
Proceedings of 5th Workshop of the Cross-Language Evaluation 
Forum, LNCS 3491, (pp. 597-613). Berlin: Springer. 
3. Clough, P., Müller, H., Deselaers, T., Grubinger, M., Lehmann, 
T.M., Jensen, J., & Hersh, W. (2006). The CLEF 2005 cross-
language image retrieval track, Proceedings of 6th Workshop of the 
Cross Language Evaluation Forum, Lecture Notes in Computer 
Science, 2006. 
4. Jones, G.J.F., Groves, D., Khasin, A., Lam-Adesina, A., Mellebeek, 
B., & Way, A. (2005). Dublin City University at CLEF 2004: 
Experiments with the ImageCLEF St Andrew's Collection.  In 
Peters, C.; Clough, P.; Gonzalo, J.; Jones, G.J.F.; Kluck, M.; 
Magnini, B. (Eds.), Proceedings of 5th Workshop of the Cross-
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                             96 年 5 月 21 日 
報告人姓名  陳信希 
 
服務機構
及職稱 
國立台灣大學 
資訊工程學系 
教授 
     時間 
會議 
     地點 
96 年 5 月 14 日-5 月 19 日 
日本東京 
本會核定
補助文號
NSC 95-2221-E-002-334 
會議 
名稱 
 (中文)  
 (英文) The 6th NTCIR Workshop on Evaluation of Information Access 
Technologies 
發表 
論文 
題目 
(英文)  
(1) Overview of CLIR Task at the Sixth NTCIR Workshop 
(2) Overview of the NTCIR-6 Cross-Lingual Question Answering (CLQA) Task
(3) Overview of Opinion Analysis Pilot Task at NTCIR-6 
(4) Using Opinion Scores of Words for Sentence-Level Opinion Extraction 
附件三
 
表 Y04 
