generation of test cases, safety-critical computing 
system. 
 
 2
中文摘要 
    系統確認測試(validation testing)的目的是在確認軟體有無符合客戶的原始需求(user 
needs)，因此所設計的測試案例應該反應使用者在實際工作條件下的預期行為。現行的系統確認
測試案例多數仍由工程師觀點設計，未能從使用者觀點設計。本研究探討由最原始的客戶需求文
件，自動產生系統確認測試案例的技術。本研究以核能安全領域為研究範疇，建立一套應用領域
的知識本體，用以標註客戶的需求文件，進一步發展自動產生確認測試案例的技術。測試案例的
種類包括單一參數測試、正常使用案例測試、異常狀況測試(即破壞測試)、及大量劇情測試。由
最原始的需求文件產生的系統確認測試案例可更貼近使用者的觀點，並在確認階段補足開發階段
壓力測試、破壞測試可能的不足，故可提昇安全關鍵計算系統的安全性及可靠性。 
關鍵詞: 確認測試、應用領域知識本體、測試案例自動產生、安全關鍵計算系統。 
 
英文摘要 
        System validation tests aim at checking whether the software conforms to the original user 
needs. Therefore, validation test cases should be designed to reflect the expectations of the user under 
the working environment. However, most of the current validation test cases are designed from the 
engineer’s perspective, instead of the user’s perspective.  This research investigated the techniques to 
automatically derive test cases of system validation from the original user needs. This research focused 
on nuclear safety domain; it intended to design a domain-specific ontology first, used it to mark up the 
document of the user’s needs, and then automatically generated validation test cases from the marked 
document.  The types of the test cases generated are single parameter tests, use case tests, abnormal 
condition tests (i.e. break tests), and a large amount of scenario tests. These test cases can better reflect 
the user’s viewpoints; furthermore, they can compensate for the possible insufficiency in break tests and 
stress tests during system development.  Therefore, the proposed approach can enhance the safety and 
reliability of safety-critical computing systems.  
 
Keywords: validation test, domain-specific ontology, automatic generation of test cases, safety-critical 
computing system.   
Validation test case generation based on safety analysis ontology
Chin-Feng Fan a,⇑, Wen-Shing Wang a,b
aDept. of Computer Engineering and Science, Yuan-Ze U., Taiwan, ROC
bNuclear Engineering Division, Institute of Nuclear Energy Research, Taiwan, ROC
a r t i c l e i n f o
Article history:
Received 3 August 2011
Received in revised form 30 January 2012
Accepted 5 February 2012
Available online 22 March 2012
Keywords:
Validation test
Domain-speciﬁc ontology
Automatic test case generation
Safety Analysis Report (SAR)
a b s t r a c t
Validation tests in the current nuclear industry practice are typically performed in an ad hoc fashion. This
study presents a systematic and objective method of generating validation test cases from a Safety Anal-
ysis Report (SAR). A domain-speciﬁc ontology was designed and used to mark up a SAR; relevant infor-
mation was then extracted from the marked-up document for use in automatically generating validation
test cases that satisfy the proposed test coverage criteria; namely, single parameter coverage, use case
coverage, abnormal condition coverage, and scenario coverage. The novelty of this technique is its sys-
tematic rather than ad hoc test case generation from a SAR to achieve high test coverage.
 2012 Elsevier Ltd. All rights reserved.
1. Introduction
Validation tests are important for ensuring that a system meets
the original user needs; thus, validation test cases should reﬂect
user expectations in the working environment. However, the vali-
dation tests currently used in many application domains are de-
signed according to requirement speciﬁcations or system models,
which mainly represent the engineer perspective instead of the user
perspective.
This situation can produce hazardous results in safety–critical
domains. Examples include the failures of Ariane 5 (Dowson,
1997; Jézéquel and Meyer, 1997; Lions, 1996) and Patriot missiles
during the Gulf War (Falatko, 1991).
The root cause of the Ariane 5 explosion (Dowson, 1997;
Jézéquel and Meyer, 1997; Lions, 1996) can be traced to a piece
of unneeded software code, whose operation was prolonged by
the user for 50 s to avoid a hold in countdown. This prolonged
usage triggered an exception message which was later misinter-
preted by the on-board computer as adjustment data, and thus
led to the crash. On the other hand, the root cause of the Patriot
Missile failure (Falatko, 1991), which killed 28 soldiers during
the Gulf War, was a problem in the non-terminating binary expan-
sion of a decimal value (1/10) for time. The US army had been ad-
vised to reset the system within 8 h; whereas on that day the
missile battery had been operated continuously for over 100 con-
secutive hours. Both accidents could have been avoided if valida-
tion tests had tested the actual user needs and habits.
To avoid such failures in safety–critical domains, system valida-
tion tests should be based directly on the original user needs. In the
nuclear power industry, the user safety needs are addressed in a
Safety Analysis Report (SAR). Current digital Instrumentation and
Control (I&C) regulations (BTP 14) prescribe that the testing pro-
cess for a digital I&C system should include unit, integration and
validation phases (NRC, 2007). However, the US Nuclear Regula-
tory Commission (NRC) has only issued a regulatory guide (RG
1.171) for unit testing phase (NRC, 1997). No regulatory guides
are available for integration and validation testing process. Since
validation tests in the nuclear industry are typically derived from
accident analysis results in an ad hoc fashion, test coverage is
uncertain and often inadequate. This study presents a technique
for automatically generating validation test cases according to
the original safety needs of users, which are indicated in a SAR.
After a domain-speciﬁc ontology for the SAR is designed, its asso-
ciated test coverage criteria are then proposed. After the ontology
is used to mark up a Safety Analysis Report, relevant information is
extracted automatically to generate system validation test cases
that meet the coverage criteria. A computerized toolset was also
developed for automating the mark-up and test generation
processes.
Thus, the contributions of this research are twofold:
(1) Compared to the validation tests derived from requirement
speciﬁcations or from system models typically used in many
application domains, the SAR-based validation tests better
meet user needs.
0306-4549/$ - see front matter  2012 Elsevier Ltd. All rights reserved.
doi:10.1016/j.anucene.2012.02.001
⇑ Corresponding author. Address: Dept. of Computer Engineering and Science,
Yuan-Ze U., 135 Far East Road, Chung-Li 320, Taiwan, ROC. Tel.: +886 3
4638800x2360; fax: +886 3 4638850.
E-mail addresses: csfanc@saturn.yzu.edu.tw (C.-F. Fan), wswang@iner.gov.tw
(W.-S. Wang).
Annals of Nuclear Energy 45 (2012) 46–58
Contents lists available at SciVerse ScienceDirect
Annals of Nuclear Energy
journal homepage: www.elsevier .com/locate /anucene
time permitted in the V&V test schedule. A total of 5240 tests were
carried out as random input tests. It took 20 days to perform 14210
(8970 + 5240) tests for the dynamic transient tests and the op-
tional random input tests (Fukumoto et al., 1998).
These facts revealed that the validation testing relied more on
random testing rather than on a systematic approach. This study
proposes an ontology-based approach for deﬁning test coverage
criteria and for systematically generating validation test cases
based on a SAR.
3. Proposed approach
We propose the following six steps to systematically generate
validation test cases from a SAR:
(1) Develop a domain-speciﬁc ontology: First, a domain-speciﬁc
ontology based on relevant regulations must be designed for
explicit representation of the essential concepts and relations
in the examined domain. In our case, the SAR-related regula-
tions are Chapter 15 of NUREG-0800 (Standard Review Plan
SRP) and Regulatory Guide (RG)1.70. A domain-speciﬁc ontol-
ogy for the SAR is ﬁrst developed.
(2) Deﬁne ontology-based test coverage criteria: Test coverage crite-
ria for validation tests should be deﬁned according to the
domain ontology. We propose four types of test coverage cri-
teria based on the SAR ontology; they are: single parameter
coverage, use case coverage, abnormal condition coverage,
and scenario coverage criteria.
(3) Design an XML markup language: A markup language in XML, a
Safety Analysis Markup Language (SAML), representing the
above ontology is then be designed.
(4) Mark up user needs: A case study (an inputted SAR) is marked
up using SAML tags, attributes, and links.
(5) Extract data from the marked-up document: Numerical data and
related information are then extracted from the marked-up
document. These data can be stored in tables along with a
knowledge table for domain speciﬁc ﬁgures, if needed.
(6) Generate test cases: Validation test cases using the above
extracted data are then generated.
These steps and their associated I/O data are shown in Fig. 3.
4. A case study using the proposed method
This section describes the details of the proposed approach,
using Chapter 15 (Accident analysis) of Lungmen PSAR (Taiwan
Power Company, 2005) as a case study. Moreover, a computerized
toolset written in VB has been constructed to aid implementation
of the ontology and to perform test case generation. The procedural
steps are presented sequentially.
4.1. Develop domain ontology
The SAR-related regulations in the nuclear domain include
Chapter 15 of NUREG-0800 (Standard Review Plan; SRP) and Reg-
ulatory Guide (RG)1.70. A SAR describes the safety system reaction,
potential scenarios, and possible impacts when the system
encounters a Postulated Initial Event (PIE). The RG1.70 speciﬁes
that a SAR should include the following data:
(1) Identiﬁcation of causes and frequency classiﬁcation.
(2) Sequence of events and systems operation.
(3) Core and system performance.
(4) Barrier performance.
(5) Radiological consequences.
Therefore, we propose organizing these contents according to
the life cycle of an accident scenario along with its safety analysis.
Speciﬁcally, the stages of the safety analysis in the proposed model
are initial, abnormal, transient, evaluation, and inﬂuence (Fig. 4). The
model indicates that safety analysis for a PIE includes, in sequential
order, the initial system state, the abnormal event occurrence, the
transient state sequence including safety system reaction and pos-
sible scenarios, the evaluation of barrier and radiological conse-
quences, and an analysis of this event inﬂuence on the power
plant. These ﬁve stages comprise the major contents described in
a SAR. Detailed information for each stage is proposed as follows:
(note that terms in italics denote major concepts, and terms in cor-
ner brackets indicate XML tags).
 Initial stage:
(a) Event preconditions or assumptions.
(b) Input parameters and initial conditions (<IPandIC>).
 Abnormal stage for a PIE:
(a) Identiﬁcation of causes.
(b) Frequency classiﬁcation.
 Transient analysis stage:
(a) Sequence of event.
(b) System operation.
(c) Operator action.
(d) Result.
 Evaluation stage:
(a) Barrier performance.
(b) Radiological consequence.
 Event inﬂuence stage:
Event inﬂuences on the power plant.
These contents form the top two levels of the proposed SAR
ontology depicted in Fig. 5. Concepts in the ontology will be repre-
sented in XML. The information from the initial and abnormal
stages are later used as test case inputs while the information from
1.Develop 
domain ontology
2. Define test 
coverage criteria
3. Design markup 
language SAML
4. Mark up a SAR
5. Extract data 
6. Generate test cases
NUREG-0800
RG1.70
Single parameter
Use case
Abnormal condition
Scenario test
XML Tags
Tag attributes
Tag relations
The marked-up PSAR of 
a case study
Predefined knowledge
table
Tag contents
Attribute values
Validation test cases
Fig. 3. Proposed steps and associated I/O data.
48 C.-F. Fan, W.-S. Wang / Annals of Nuclear Energy 45 (2012) 46–58
4.3. Designing a Safety Analysis Markup Language (SAML)
Ontology must be explicitly represented for practical use.
Since the proposed SAR ontology was applied to mark up a doc-
ument, an XML markup language – Safety Analysis Markup Lan-
guage (SAML) was designed to represent the above SAR
ontology. Fig. 6 shows the top-level SAML structure containing
ﬁve DTDs which represent concepts at each of the ﬁve stages,
namely, initial, abnormal, transient, evaluation, and inﬂuence
stages. Note that these ﬁve stages, which may appear more than
once and in any order, are denoted by ‘‘OR’’ and a Kleene Closure
(⁄) (Hopcroft et al., 2000) in the XML DTD. Figs. 7–9 show samples
of the contents of DTDs in the ﬁrst three stages. The concepts rep-
resented by the following XML tags are used in the subsequent
test case generation:
<IPandIC>: Initial Parameter and Initial Condition, which may
involve device states or process variables, such as pump state
and design pressure.
<System>: normal and abnormal system behavior, consisting of
the sub-concepts such as system function (<function>) and
plant parameter (<PlantPara>).
<?xml version="1.0" ?> 
<!ELEMENT AccidentAnalysis (Event+)> 
<!ELEMENT Event (Initial | Abnormal | TransientAnalysis | Evaluation | EventInfluence  | #PCDATA )*> 
<!ENTITY %Initial SYSTEM "Initial.dtd"> 
<!ENTITY %Abnormal SYSTEM " Abnormal.dtd"> 
…. 
<!ATTLIST Event name CDATA #IMPLIED> 
Fig. 6. Top level structure of the SAML.
<?xml version="1.0" ?> 
<!-- Initial.dtd --> 
<!ELEMENT Initial (IPandIC | EventAssume | #PCDATA)*> 
  <!ELEMENT IPandIC (ReactorVesselCoolantLevel | PressurizerCoolantLevel 
FeedwaterFlowRate | DesignPressure | …|#PCDATA)*> 
    <!ELEMENT FeedwaterFlowRate (#PCDATA)> 
....... 
Fig. 7. DTD for the initial stage.
Fig. 8. DTD for the abnormal stage.
50 C.-F. Fan, W.-S. Wang / Annals of Nuclear Energy 45 (2012) 46–58
(2) Test case generation tools.
Fig. 10 shows the main screen of this toolset. The tag design
tool adds, deletes, and saves tags in the markup language
(Fig. 11).
4.4. Mark up user needs
A SAR should then be marked up using the above domain-
speciﬁc tags and attributes so that relevant data can be extracted
for further use in generating test cases. Our case study is Chapter
Fig. 12. Part of the marked-up SAR.
Table 1
Part of the marked-up scenario table in the SAR.
Time (s) Events
<EventTime id = ’’ Runout of One Feedwater Pump’’
index = ’’1’’> 0</EventTime>
<EventTransient id = ’’ Runout of One Feedwater Pump’’ index = ’’1’’>Initiate simulated runout of one feedwater
pump (at system design <EventInputPara id = ’’ Runout of One Feedwater Pump’’, value = ’’7.35’’,
unit = ’’MPaG’’> pressure</EventInputPara> of 7.35 MPaG the pump <EventInputPara id = ’’ Runout of One
Feedwater Pump’’, value = ’’75’’, unit = ’’%’’> runout ﬂow</EventInputPara> is 75% of rated feedwater ﬂow.
</EventTransient>
<EventTime id = ’’ Runout of One Feedwater Pump’’
index = ’’2’’> 0.1</EventTime>
<EventTransient id = ’’ Runout of One Feedwater Pump’’ index = ’’2’’>Feedwater controller starts to reduce the
feedwater ﬂow from the other feedwater pump </EventTransient>. . .
Fig. 13. A tagging tool.
52 C.-F. Fan, W.-S. Wang / Annals of Nuclear Energy 45 (2012) 46–58
The test cases are explained below:
4.6.1. Single parameter tests
First, tests for a single parameter can be generated to test
whether a physical device works or to conﬁrm that a process var-
iable is correctly indicated by its display. For example, if the feed
water ﬂow rate is set to 50% on the control station, the display
should show a feed water ﬂow rate of 50%. These simple tests
are important bases for subsequent tests.
Legal values or ranges for concerned parameters can be ex-
tracted from the marked-up SAR in the previous step. In this case,
test cases can be generated for each of the parameters tagged by
either the <PlantPara> tag or the <IPandIC> tag. They mark up such
parameters as a pump, power level, water level, feed water tem-
perature, ﬂow rate, design pressure, etc. Once a parameter is se-
lected, test cases can be generated using a speciﬁed value
variation; moreover, when the legal ranges are known, test cases
can be further designed using equivalence partitioning approach
(Beizer, 1990). Equivalence portioning is a popular test case design
method identifying a small set of representative input variables
that yield many output conditions as possible so as to increase test
coverage (Desikan and Ramesh, 2006). The set of input values that
generate a single expected output is called an equivalence parti-
tion. Test cases are designed for each equivalence partition. For
example, an input variable may have three partitions: the part in
the legal range, the part that is larger than the upper bound, and
the part that is smaller than the lower bound. All parts should be
tested by representative values. Moreover, the boundary values
around the upper and lower bounds, which are particularly error
prone, should also be checked.
In addition to equivalence partitioning and boundary testing,
multiple test cases are recommended to test the legal range of
the parameter. For example, ‘‘Feedwater ﬂow rate’’ has the legal
range 0–130% and where a 10% interval is used to test the legal
portion, the test cases are 10%, 0%, 10%, 20%, 30% . . .130%, 140%,
as shown in Fig. 15. Fig. 16 shows how the computer tool is used
to generate single parameter test cases based on a selected tag.
Single-parameter tests can verify the correctness of a single de-
vice or a single process variable. These test cases act as bases for
further more complex tests because if a single parameter fails,
the following tests do not work.
4.6.2. Use case tests
Next, standard use cases can be tested to ensure the safety sys-
tem responds as expected when a certain PIE is encountered. A SAR
usually describes a sample operation sequence for major PIEs.
These descriptions can be marked up and used as a template to
generate use case tests. In the initial and abnormal stages, the
SAR describes initial settings and the postulated initial event at
time 0; at the transient stage, the SAR describes the sequences of
system operations at the subsequent time units; the evaluation
and event inﬂuence portions may be needed to judge the conse-
quences of the use case.
For example, the description for ‘‘Runout of one Feedwater
Pump’’ event is marked up in Fig. 12 and in Table 1 above. The
event description can be extracted automatically to form the use
case test given in Fig. 17. The initial settings indicate that design
Fig. 15. Test cases with equivalence partitioning.
Fig. 16. Single parameter test case generation.
54 C.-F. Fan, W.-S. Wang / Annals of Nuclear Energy 45 (2012) 46–58
reveal hazardous situations dynamically and examine how the
system reacts.
4.6.4. Scenario tests
Use case tests are derived from the standard event sequences
described in the SAR. However, non-standard events should also
be tested; they can be formed by varying values of selected param-
eters in the standard use case tests. Thus, numerous scenario tests
can be generated automatically, as shown in Fig. 18. Basically, a
large amount of scenario tests are derived by combining results
from such techniques as equivalence partitioning, boundary test-
ing, and multiple tests within the legal range.
To demonstrate the proposed method, examples in our case
study are described below. In this case study, the parameters
marked by the tags <IPandIC>, <EventAssume>, <PlantPara>, and
<EventInputPara> were selected. For example, the steps to gener-
ate various scenario tests for a ‘‘feedwater controller failure at
maximum demand’’ event are carried out as follows:
Step 1: Select parameters ‘‘power level’’, ‘‘coolant inventory’’,
and ‘‘runout ﬂow’’.
Step 2: Find the legal ranges of these parameters in the data
tables whose information are extracted from such marked-
up text as:
< PlantPara LowLimit ¼ 0:57;UpperLimit ¼ 1:08;Unit ¼m >
coolant inventory < =PlantPara > :
Thus, we get the following ranges:
Coolant inventory: 0.57–1.08 m
Power level: 0–110%
Run out ﬂow rate: 0–130%
Step 3: Assume the variant x = 10% of the total range span.
Thus, we get:
Power level: (110%  0%)  10% = 11%
Coolant inventory: (1.08 m  0.57 m)  10% = 0.05 m
Run out ﬂow rate (130%  0%)  10% = 13%
Boundary values and illegal values should be tested. Thus,
the following variants of each parameter are generated:
Power level (13 cases):
11%, 0% (lower bound), 11%, 22%, 33%, 44%, . . . , 99%
110% (upper bound), 121%
Coolant inventory (14 cases):
0.52 m, 0.57 m (lower bound), 0.62 m, 0.67 m, 0.72 m,
0.77 m, 0.82 m, 0.87 m,
0.92 m, 0.97 m, 1.02 m, 1.07 m, 1.08 m (upper
bound), 1.13 m
Run out ﬂow rate: (13 cases):
13%, 0% (lower bound), 13%, 26%, 39%, 52%, . . . , 117%,
130% (upper bound), 143%
Step 4: Combine the above variant data to form scenario test
case inputs.
Thus, a 2366 (i.e., 13  14  13) scenario tests may be generated
(Table 6). Different variant amount x (Step 3 of Fig. 18) can be cho-
sen so as to control the number of test cases generated. Fig. 19
shows how our toolset is used for generating scenario test cases.
These scenario tests achieve high test coverage efﬁciently because
they use equivalence partitioning approach to test both legal and
illegal portions. Moreover, they also provide comprehensive test-
ing for the legal ranges along with testing of the boundary values,
which are particularly fault-prone. Compared to random tests,
which are commonly performed in the nuclear industry, the pro-
posed scenario test method is more systematic and more
comprehensive.
5. Test coverage
The test cases generated above can be evaluated in a simulator
environment. Generic plant simulators such as PCTran (MST Com-
pany, 2007) may be used for the purpose. For performing these test
cases, a dedicated simulator for the plant being analyzed is prefer-
able. We expect to use a simulator environment to verify the effec-
tiveness of the proposed method in future works.
Because our approach is based on user needs (a SAR), the test
cases generated are black box tests. The conventional test coverage
criteria such as path coverage, state coverage, and transition cover-
age are inapplicable since systemmodels and program code are not
used.
Instead of testing normal system behavior, the generated cases
are used to test transient and accident events addressed in the SAR.
The proposed use case tests can be generated for all PIEs that have
event sequences descriptions in the SAR. Moreover, variations of
parameter values in event sequence are also suggested above;
equivalence partitioning technique is used. Thus, test cases derived
from the approach fully satisfy the proposed test coverage criteria,
namely:
(1) Single parameter coverage: All single parameter mentioned in
the SAR are tested individually.
(2) Use case coverage: All transient or accident sequences men-
tioned in the SAR are tested.
(3) Abnormal condition coverage: Potential failures of hardware
and software in the above use cases are tested.
(4) Scenario coverage: All combinations of partitions of input
parameters are tested using equivalence partitioning and
boundary testing. Multiple tests are also generated within
the legal range.
In summary, the test cases meet the above test coverage crite-
ria. Future works should evaluate the effectiveness of these test
cases in actual case studies.
6. Conclusion
This study introduces a new technique for generating validation
test cases according to speciﬁc user needs. A domain-speciﬁc
Table 6
Sample of Scenario test cases.
Input Expected output
Initial Initial Abnormal Transient Evaluation Inﬂuence
Power level (%) Coolant inventory (m) Run out ﬂow (%)
11 0.52 13 Error Error Error
11 0.57 0 Error Error Error
0 0.57 0 Integrity OK
11 0.57 13 Integrity OK Decrease temperature
11 0.62 13 Integrity OK Decrease temperature
. . .. . . .. . .. . . .. . .. . .. . .. . . .. . .. . ... . . .. . ... . . .. . ...
56 C.-F. Fan, W.-S. Wang / Annals of Nuclear Energy 45 (2012) 46–58
Fan, C., Yih, S., 1999. Safety Markup Language: Concept and Application. In:
Proceedings of Lecture Notes in Computer Science 1698, SafeComp ’99, Toulous,
pp. 177–186.
Goldfarb, C.F., 1990. The SGML Handbook. Oxford University Press.
Hofwebwer, T., 2004. Logic and Ontology. <http://plato.stanford.edu/entries/logic-
ontology/>. (retrieved 03.06.11).
Hopcroft, J., Motwani, R., Ullman, J., 2000. Introduction to Automata Theory,
Languages, and Computation. Addison-Wesley.
Ion, P., Miner, R., 1999. Mathematical Markup Language, W3C working Draft.
<http://www.w3.org/pub/WWW/TR/WD-math>. (retrieved 03.06.11).
Jézéquel, J., Meyer, B., 1997. Design by contract: the lessons of Ariane. IEEE,
Computer Magazine 30 (2), 129–130.
Lions, J., 1996. Ariane 501 Failure: Report by the Inquiry Board. European Space
Agency. <http://esamultimedia.esa.int/docs/esa-x-1819eng.pdf>. (retrieved
03.06.11).
Murray-Rust, P., Rzepa, H., 1995. Chemical Markup Language (CML). <http://
www.xml-cml.org/>. (retrieved 03.06.11).
Nasser, V.H., 2009. Ontology-based Unit Test Generation. Master’s Thesis in
Computer Science. University of New Brunswick.
Offutt, J. et al., 2003. Generating test data from state-based speciﬁcations. The
Journal of Software Testing. Veriﬁcation and Reliability 13 (1), 25–53.
Offutt, J., Abdurazik, A., 2000. Using UML collaboration diagrams for static checking
and test generation. In: Proceedings of 3rd International Conference On UML,
York, UK.
Prasanna, M. et al., 2005. A survey on automatic test case generation. Academic
Open Internet Journal 15, 89–90.
MST Company, 2007. PCTran, PC-based Nuclear Power Plant Simulator. <http://
www.microsimtech.com/abwr/>. (retrieved 19.07.11).
Protégé, 2011. <http://protege.stanford.edu/>. (retrieved 03.06.11).
Taiwan Power Company, 2005. Preliminary Safety Report for Lung-men of Taiwan
Power Company, U10 version.
NRC, NUREG 0800, 2007. Standard Review Plan for the Review of Safety Analysis
Reports for Nuclear Power Plants. <http://www.nrc.gov/reading-rm/doc-
collections/nuregs/staff/sr0800>. (retrieved 19.07.11).
NRC, Regulatory Guide 1.070 (Revision 3), 1978. Standard Format and Content of
Safety Analysis Reports for Nuclear Power Plants. <http://
adamswebsearch2.nrc.gov/idmws/
ViewDocByAccession.asp?AccessionNumber=ML011340122>. (retrieved
19.07.11).
NRC, 1997. Software Test Documentation for Digital Computer Software Used in
Safety Systems of Nuclear Power Plants. <http://www.nrc.gov/about-nrc/
regulatory/research/digital/regs-guidance.html>. (retrieved 19.07.11).
Voas, J., 1997. Fault injection for the masses. Computer 30, 129–130.
Whittaker, J., 2002. How to Break Software: A Practical Guide to Testing. Addison
Wesley.
58 C.-F. Fan, W.-S. Wang / Annals of Nuclear Energy 45 (2012) 46–58
 5
國科會補助專題研究計畫成果報告自評表 
 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明：本研究建立安全關鍵領域由原始的客戶需求文件產生確認測試案例的技術；已完成
預定之工作項目、並已發表一篇期刊論文。 
 
 
 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 □無 
技轉：□已技轉 □洽談中 □無 
其他：（以 100 字為限） 
  
 (見報告內容) 
 
 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
 
本研究建立安全關鍵領域由原始的客戶需求文件(user needs)產生確認測試案例 
(validation tests)的技術，此為創新；因為由需求規格產生確認測試案例的方法眾
多，但鮮有由純文字的客戶需求產生確認測試案例。後者的好處在於更貼使用者的要
求。本研究已獲得初步的結果；研究結果可提供相關主管單位參考。所發展的方法論
具通用性，可應用於其他領域上。 
 
 
 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/09/17
國科會補助計畫
計畫名稱: 以知識本體為基礎之系統確認測試案例自動產生技術
計畫主持人: 范金鳳
計畫編號: 100-2221-E-155-059- 學門領域: 程式語言與軟體工程
無研發成果推廣資料
其他成果 
(無法以量化表達之
成果如辦理學術活
動、獲得獎項、重要
國際合作、研究成果
國際影響力及其他協
助產業技術發展之具
體效益事項等，請以
文字敘述填列。) 
本研究建立安全關鍵領域由原始的客戶需求文件產生確認測試案例的技術；所
發展的方法論具通用性，可應用於其他領域上。下一步可建相關系統之模擬器，
以實際驗證其效用。 
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
