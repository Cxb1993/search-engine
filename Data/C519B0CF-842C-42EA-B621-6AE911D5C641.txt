base-matrix. Luoh et al. [22] proposed a matrix-pattern-based computer algorithm to
solve for max-min and max-product fuzzy relational equations. Bourke and Fisher [23]
studied the max-product fuzzy relational equations and proposed an algorithm to find
all minimal solutions. Leotamonphong and Fang [24] adopted the strategy of reducing
the problem size so that their procedure outperforms Bourke and Fisher’s procedure.
Motivated by Leotamonphong and Fang’s idea of reducing problem size, Guu and
Wu [25] provided a necessary condition for the max-product fuzzy relational equations.
This necessary condition provides that, for a minimal solution, each of its components
is either 0 or the corresponding component’s value of the greatest solution. From the
algorithmic viewpoint, this interesting property enables one to pre-assign values for some
variables. Therefore, the problem size may be reduced during the process of finding
all minimal solutions. Numerical experiments showed that Guu and Wu’s procedure
improved Leotamonphong and Fang’s method.
As pointed out in Guu and Wu, this necessary condition is not universal for a general
fuzzy relational equation. For instance, a similar necessary condition for max-min fuzzy
relational equations becomes much more complicated. Therefore, the class of max-t-norm
fuzzy relational equations is simply too large for it to hold. Wu and Guu [26] extended it
to hold in a smaller subclass with max-strict t-norm composition. (It is known that max-
product is one kind of max-strict t-norm, but max-min is not.) With recent development
on Archimedean t-norm, we shall extend this necessary condition for max-Archimedean
t-norm ([27]) fuzzy relational equations.
One of our main contributions in this paper is to add more theoretical results for
minimal solutions of fuzzy relational equations with max-t-norm composition and max-
Archimedean t-norm composition, respectively. We assume that the solution set is
nonempty. Stamou and Tzafestas [28] provided new criteria for checking the existence
of solutions of the max-t-norm fuzzy relational equations. Perfilieva and Tonis [29] pro-
vided a criterion of solvability of the max-t-norm fuzzy relational equations in terms of
input-output data. Based on the necessary condition, we propose rules to reduce the
problem size. Furthermore, rather than work with the actual equations, we employ a
simple matrix whose elements contain only 1 and 0 and capture all the properties of the
equations to find minimal solutions.
This paper is organized as follows. Section 2 reviews some basic definitions and proper-
2
x = (xi)i∈I such that
max
i∈I
{t(xi, aij)} = bj,∀j ∈ J . (2)
Let x1 = (x1i )1×m, x
2 = (x2i )1×m for all i ∈ I be two vectors. If we define, for any
x1, x2 ∈ X(A, b), x1 ≤ x2 if and only if x1i ≤ x2i for all i ∈ I, then the operator “≤”
forms a partial order relation on X(A, b). A solution x¯ ∈ X(A, b) is called the greatest
solution if x ≤ x¯ for all x ∈ X(A, b). On the other hand, an x ∈ X(A, b) is a minimal
solution if ∀x ∈ X(A, b), x ≤ x implies that x = x. Di Nola et al. [15] showed that for
continuous t-norm if X(A, b) is nonempty, the solution set can be completely determined
by the greatest solution and a finite number of minimal solutions.
It turns out that when the fuzzy relational equations with max-t-norm composition
have a solution, its maximum solution can be computed explicitly. To do so, we need the
“→t” operator.
Definition 1. Let t(α, β) be a t-norm and δ ∈ [0,1], the →t operator is defined by
β →t δ = sup{α ∈ [0, 1]|t(α, β) ≤ δ}.
For a left continuous t-norm, it can be verified that [37]:
(a) t((α→t δ), α) ≤ δ,
(b) t(α, β) ≤ δ if and only if α ≤ β →t δ.
When the solution set X(A, b) is not empty, the greatest solution for fuzzy relational
equations with max t-norm composition can be obtained by the following operation:
x¯ = [min
j∈J
(aij →t bj)]i∈I . (3)
Archimedean t-norm Mostert and Shields [38] proved that the class of continuous
t-norms can be classified into three categories. Precisely, a continuous t-norm is either
a “min” operation or an Archimedean t-norm (to be defined shortly) or the ordinal sum
of a family of properly defined Archimedean t-norms. This classification suggests the
subclass formed by Archimedean t-norms may contain useful information when solving
(2). Indeed, we shall show that a useful result (Theorem 3) can be derived to this
consideration. Before we proceed, we need the following definition.
Definition 2. The Archimedean t-norm t is a t-norm with t(α, α) < α for all 0 < α < 1.
4
Lemma 1. Let t be a t-norm. If there exists the jth equation with aij < bj for each
i ∈ I in (2), then the solution set X(A, b) is empty.
Proof. Due to aij < bj for each i ∈ I in (2), it implies
t(xi, aij) ≤ min(xi, aij) < bj,∀i ∈ I.
This result leads to maxi∈I{t(xi, aij)} < bj and no any variable xi can satisfy the jth
equation in (2).]
From Lemma 1 one can conclude that if X(A, b) is not empty, then, for each j ∈ J ,
t(xi, aij) = bj must hold for some i. Furthermore, if variable xi is a binding variable,
then aij ≥ bj must hold for at least one j ∈ J .
Lemma 2. Let t be a t-norm, x¯ = (x¯i)i∈I be the greatest solution, and x be a solution
of (2). If xi is binding in the jth equation, then x¯i is binding there as well. Moreover, if
x¯i is not a binding variable, then xi is nonbinding as well for any solution x.
Proof. For any solution x = (xi)i∈I ∈ X(A, b), we have
max
i∈I
{t(xi, aij)} = bj,∀j ∈ J .
For variable xi, we have t(xi, aij) ≤ bj, which implies t(x¯i, aij) ≤ bj,∀j ∈ J . If now xi
is binding in the jth equation, we have j ∈ Jˆ(xi) and t(xi, aij) = bj. By monotonically
non-decreasing property of t-norm, the following inequality holds
bj = t(xi, aij) ≤ t(x¯i, aij) ≤ bj.
This implies that t(x¯i, aij) = bj. Hence, x¯i is binding in the jth equation as well.
On the other hand, if x¯i is not binding in any equation, we have t(x¯i, aij) < bj for all
j ∈ J . Since t(xi, aij) ≤ t(x¯i, aij) < bj holds for any solution x, we have t(xi, aij) < bj
for all j ∈ J . In other words, xi is not a binding variable.]
Lemma 3. Let t be a t-norm and X(A, b) 6= ∅. If bj = 0 for some j ∈ J , then for any
solution x, all xi, i ∈ I are binding at the jth equation.
Proof. Trivial.]
Lemma 4. Let t be a t-norm. If xi is a binding variable but only binding at those
equations with bj = 0 in (2), then we have xi ∈ [0, x¯i].
Proof. Since xi is binding at bj = 0 for all j ∈ Jˆ(xi), by Lemma 2, we have t(x¯i, aij) = 0
6
Archimedean t-norm with bounded additive generator (i.e., f(0) <∞) is called a nilpo-
tent t-norm. And a nilpotent t-norm is isomorphic with the Lukasiewicz t-norm. A
continuous Archimedean t-norm with unbounded additive generator (i.e., f(0) = ∞) is
called a strict t-norm. And a strict t-norm is isomorphic with the product operation.
For these results, we refer to [36, 41]. That’s why we consider, among various forms of
continuous Archimedean t-norms, only the product and the Lukasiewicz t-norms as our
examples.
Theorem 2. Let t be a continuous Archimedean t-norm, b be a vector with 0 < bj ≤ 1
for all j ∈ J , X(A, b) of (2) be nonempty, and x¯ be the greatest solution in X(A, b). For
any solution x, if xi is a binding variable, then xi = x¯i.
Proof. For any solution x = (xi)i∈I ∈ X(A, b), we have
max
i∈I
{t(xi, aij)} = bj,∀j ∈ J .
Since xi is a binding variable, we have t(xi, aij) = bj for some j. By Lemma 2, x¯i is
binding at the jth equation as well. Hence, t(x¯i, aij) = bj. Now suppose that xi < x¯i.
Since bj > 0, it follows from Theorem 1 that f(xi) + f(aij) ≤ f(0) and t(xi, aij) =
f−1(f(xi) + f(aij)). Since f is strictly decreasing and xi < x¯i, we have
f(x¯i) + f(aij) ≤ f(0) and t(x¯i, aij) = f−1(f(x¯i) + f(aij)).
Hence, we conclude that
0 < bj = t(xi, aij) < t(x¯i, aij) = bj,
which is impossible. Therefore, we have xi = x¯i.]
Note that so far our analysis for Archimedean t-norm requires the assumption of bj ∈
(0, 1],∀j ∈ J . We now drop this assumption to face the case of bj = 0 for some j ∈ J .
Theorem 3. Let t be a continuous Archimedean t-norm, b be a vector with 0 ≤ bj ≤ 1
for all j ∈ J , and x¯ = (x¯i)i∈I be the greatest solution. If x∗ = (x∗i )i∈I is a minimal
solution of (2), then we have either x∗i = 0 or x
∗
i = x¯i for each i ∈ I.
Proof. Each variable x∗i in a minimal solution is either non-binding or binding. Suppose
that x∗i is not a binding variable and x
∗
i > 0. Then we can construct a solution x
! by
letting x!i = 0 and x
!
k = x
∗
k for all k ∈ I and k 6= i. It follows that x! ≤ x∗. This implies
that x∗ is not a minimal solution. Hence, a non-binding x∗i requires x
∗
i = 0.
8
component of some minimal solutions to be binding at the jth equation. Since each
solution must satisfy all equations, the procedure of finding minimal solutions can be
transformed into the selection of one entry with value 1 in each column of matrix M .
Moreover, the selection should use the least number of binding variables (hence the entries
with value 1 in matrixM) to satisfy all equations. In order to meet this selection, we first
select the variable that satisfies the largest number of equations as a binding variable so
that the fewest number of equations are left for further consideration. We then still select
one of the remaining variables that can satisfy the largest number of the left equations
as a new binding variable. We iterate this process to find a minimal solution until all
equations are satisfied.
To develop a procedure of finding all minimal solutions of (2), we define the following
index sets for matrix M .
Ji(M) := {j ∈ J |mij = 1},∀i ∈ I and Ij(M) := {i ∈ I|mij = 1},∀j ∈ J .
Then we propose the following four rules to find the complete set of minimal solutions
for problem (2) with max-Archimedean t-norm composition.
Rule 1. If there exists a singleton Ij(M) = {i} for some j ∈ J in matrix M , we assign
x¯i to the ith component of any minimal solutions. (This is the rule employed
by Loetamonphong and Fang [9] for max-product composition.) The index set
Ij(M) = {i} identifies that the jth equation only can be satisfied by variable xi.
This implies that the ith component of any minimal solutions (hence, the variable
xi) must be binding in the jth equation. Thanks to xi = x¯i by Theorem 3, we
can delete the jth column of M with j ∈ Ji(M) from further consideration. The
corresponding row of xi in M can be deleted as well. After deletion, we let M¯
denote the reduced matrix, Jˆ(M¯) represent the index set of the remaining columns
of M¯ and Iˆ(M¯) denote the remaining variables which associated with the rows of
M¯ .
Rule 2. In the process of finding minimal solutions, if Jk(M¯) = ∅ for some xk ∈ Iˆ(M¯),
then we can assign the value 0 to the kth component of minimal solutions. Since
Jk(M¯) = ∅, this implies that the kth component of minimal solutions is not a
binding variable to the remaining equations. Hence, we can assign 0 to the kth
10
set of minimal solutions can be summarized as follows.
Step 1 Compute the vector x¯ = (x¯i)i∈I by (3).
Step 2 Check the consistency by verifying whether x¯ ◦ A = b. If inconsistent, then Stop.
(If consistent, then x¯ is the greatest solution.)
Step 3 Compute index sets Ij for all j ∈ J and generate the matrix M .
Step 4 Apply Rules 1-3 to fix as many as possible values of components of minimal solutions
associated with matrix M . If Rule 4 is satisfied, or no remaining rows/variables
are left to be considered in matrix M , then go to Step 12, otherwise let k = 1 (here
k represents the “layer” of assigning binding variable in the process of finding a
minimal solution.)
Step 5 Arrange the rows of the current reduced matrix, say matrix M¯ , according to the
decreasing order of |Ji(M¯)| and denote the arranged matrix by M¯k. Then record
the variables associated with the rows of matrix M¯k in order and denote it by set
I¯k.
Step 6 Let a temporary matrix M¯temp be equal to matrix M¯k. Select the first entry from
set I¯k as a binding variable, say xr, then eliminate binding variable xr from set I¯k
and the corresponding row associated with binding variable xr from matrix M¯k.
Step 7 Eliminate the corresponding rows and columns associated with binding variable xr
from matrix M¯temp. Denote the reduced matrix by M¯ . Then record the variables
associated with the rows of matrix M¯ by set Iˆ(M¯) and the columns by set Jˆ(M¯).
Step 8 Apply Rules 1-3 as many times as possible to fix values of components of minimal
solutions associated with the reduced matrix M¯ . Let k = k + 1.
Step 9 Arrange the rows of the current reduced matrix M¯ , according to the decreasing
order of |Ji(M¯)| and denote the arranged matrix by M¯k. Then record the variables
associated with the rows of matrix M¯k in order and denote it by set I¯k. If Rule 4
can’t be applied, go to Step 6.
12
b =
[
0.48 0.56 0.72 0.56 0.52 0.72 0.42 0.64
]
.
Step 1. Compute the vector
x¯ = [ 0.82 0.82 0.66 0.72 0.76 0.62 0.76 0.88 ].
Step 2. Computing directly shows that x¯ ◦A = b holds. Hence, the problem is solvable
and x¯ is the greatest solution.
Step 3. Compute index sets Ij for all j ∈ J = {1, 2, 3, 4, 5, 6, 7, 8}. They are
I1 = {1, 8}, I2 = {3, 5}, I3 = {2, 5}, I4 = {5, 7},
I5 = {1, 2, 6}, I6 = {2}, I7 = {4, 5, 6}, I8 = {1, 8}.
Note that the problem complexity of this Example is 288(=2× 2× 2× 2× 3× 1× 3× 2).
Then we obtain the matrix M as follows.
equation→ 1 2 3 4 5 6 7 8
M =
(x1)
(x2)
(x3)
(x4)
(x5)
(x6)
(x7)
(x8)

1 0 0 0 1 0 0 1
0 0 1 0 1 1 0 0
0 1 0 0 0 0 0 0
0 0 0 0 0 0 1 0
0 1 1 1 0 0 1 0
0 0 0 0 1 0 1 0
0 0 0 1 0 0 0 0
1 0 0 0 0 0 0 1

.
Step 4. Apply Rules 1-3 to fix as many as possible values of components of minimal
solutions to matrix M . If Rule 4 is satisfied, or no remaining rows/variables are left to
be considered in matrix M , then go to Step 12. Otherwise let k = 1.
For the current matrix M , the index set I6 = {2} identifies that the variable x2 is the
only binding variable in the 6th equation. By Rule 1, we assign x¯2 to the second variable
in any minimal solutions. Note that x2 is also binding in equations 3,5 and 6 (or columns
3,5,6 of M). Hence, these columns and the corresponding row of x2 can be deleted from
matrix M . After deletion, the reduced matrix M becomes
equation→ 1 2 4 7 8
14
Now we have set I¯1 = {x1, x8, x3, x4, x6, x7} and the eliminated matrix M¯1 becomes
equation→ 1 2 4 7 8 equation→ 1 2 4 7 8
M¯temp =
(x5)
(x1)
(x8)
(x3)
(x4)
(x6)
(x7)

0 1 1 1 0
1 0 0 0 1
1 0 0 0 1
0 1 0 0 0
0 0 0 1 0
0 0 0 1 0
0 0 1 0 0

and M¯1 =
(x1)
(x8)
(x3)
(x4)
(x6)
(x7)

1 0 0 0 1
1 0 0 0 1
0 1 0 0 0
0 0 0 1 0
0 0 0 1 0
0 0 1 0 0

.
Step 7. Eliminate the corresponding rows and columns associated with binding variable
x5 from matrix M¯temp. Denote the reduced matrix by M¯ . Then record the variables
associated with the rows of matrix M¯ by set Iˆ(M¯) and the columns by set Jˆ(M¯).
Note that current binding variable x5 is binding in equations 2,4 and 7 (or columns 2,4,7
of M¯temp.) Hence, these columns and first row of matrix M¯temp can be deleted from con-
sideration. After deletion, the remaining variables set is Iˆ(M¯) = {x1, x8, x3, x4, x6, x7},
the remaining equations index set is Jˆ(M¯) = {1, 8} and the reduced matrix denoted by
M¯ becomes
equation→ 1 8
M¯ =
(x1)
(x8)
(x3)
(x4)
(x6)
(x7)

1 1
1 1
0 0
0 0
0 0
0 0

.
Step 8. Apply Rules 1-3 to fix values of components of minimal solutions to the reduced
matrix M¯ until no any rules can be satisfied again. Let k = k + 1.
For matrix M¯ , it is the situation as if we were facing two equations with 6 variables.
Compute the index sets with respect to M¯ for xi ∈ Iˆ(M¯) and yield J3(M¯) = J4(M¯) =
J6(M¯) = J7(M¯) = ∅. Hence, we set variables x3 = x4 = x6 = x7 = 0 by Rule 2. On the
16
Now we have k = 1, M¯ = M¯1 as showed in Step 10. Compute the index set for xi ∈ I¯1
to yield |J1(M¯)| = |J8(M¯)| = 2 > |J4(M¯)| = |J6(M¯)| = 1. Hence, we obtain the arranged
matrix M¯1 as follows.
equation→ 1 7 8
M¯1 =
(x1)
(x8)
(x4)
(x6)

1 0 1
1 0 1
0 1 0
0 1 0
 .
Recording the corresponding variables of matrix M¯1 in order, we have set I¯1 = {x1, x8, x4, x6}.
Step 6. Save a copy of current matrix M¯1 as M¯temp. Select the first entry x1 from set
I¯1 as a binding variable. Then eliminate x1 and the row associated with x1 from from I¯1
and matrix M¯1, respectively.
Now we have set I¯1 = {x8, x4, x6} and, after elimination, the matrix M¯1 becomes
equation→ 1 7 8 equation→ 1 7 8
M¯temp =
(x1)
(x8)
(x4)
(x6)

1 0 1
1 0 1
0 1 0
0 1 0
 and M¯1 =
(x8)
(x4)
(x6)

1 0 1
0 1 0
0 1 0
 .
Step 7. Eliminate the corresponding rows and columns associated with binding variable
x1 from matrix M¯temp. Denote the reduced matrix by M¯ . Then record the variables
associated with the rows of matrix M¯ by set Iˆ(M¯) and the columns by set Jˆ(M¯).
Note that x1 is binding in equations 1 and 7 (or columns 1,7 of M¯temp.) Hence, these
columns and first row of matrix M¯temp can be deleted from consideration. After deletion,
the remaining set is Iˆ(M¯) = {x8, x4, x6} and the index set is Jˆ(M¯) = {7}. The reduced
matrix denoted by M¯ becomes
equation→ 7
M¯ =
(x8)
(x4)
(x6)

0
1
1
 .
18
Step 11. Now the number of binding layer shows k = 1, go to next step.
Step 12. Filter out the nonminimal solution and print the complete set of minimal
solutions.
For this example, the procedure yields 6 minimal solutions and doesn’t generate any
nonminimal solutions. For easy reference, we summarize key information during the
process of finding minimal solutions in Fig. 1.
Fig.1. Searching tree of Example 1.
Example 2. Let x ◦ A = b be a fuzzy relational equation, where t(x, a) = ax is the
algebraic product, and
x = [ x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 ],
A =

0.560 0.100 0.720 0.100 0.100 0.960 0.100 0.100 0.100 0.100 0.100 0.100 0.100 0.100 0.100
0.210 0.100 0.100 0.300 0.320 0.100 0.40 0.100 0.100 0.100 0.100 0.630 0.100 0.700 0.100
0.420 0.480 0.540 0.100 0.100 0.720 0.800 0.100 0.960 1.000 0.100 0.100 0.100 0.100 0.100
0.400 0.100 0.100 0.100 0.100 0.100 0.100 0.800 0.100 0.100 0.100 0.100 0.100 0.100 0.100
0.100 0.300 0.100 0.375 0.400 0.450 0.500 0.525 0.100 0.625 0.700 0.100 0.800 0.100 0.900
0.350 0.400 0.450 0.500 0.100 0.600 0.100 0.100 0.800 0.100 0.100 0.100 0.100 0.100 0.100
0.336 0.384 0.100 0.100 0.100 0.576 0.640 0.672 0.100 0.800 0.896 0.100 0.100 0.100 0.100
0.100 0.375 0.100 0.100 0.500 0.100 0.625 0.100 0.750 0.100 0.100 0.100 1.000 0.100 0.100
0.300 0.100 0.100 0.100 0.100 0.100 0.100 0.600 0.100 0.100 0.800 0.900 0.100 1.000 0.100
0.100 0.100 0.375 0.100 0.100 0.500 0.100 0.100 0.100 0.100 0.100 0.875 0.100 0.100 1.000

,
20
Figure 2 summarizes the key information during the process of finding all 16 minimal
solutions for the following problem.
equation→ 1 2 3 4
M =
(x1)
(x2)
(x3)
(x4)
(x5)
(x6)
(x7)
(x8)

1 0 0 0
1 0 0 0
0 1 0 0
0 1 0 0
0 0 1 0
0 0 1 0
0 0 0 1
0 0 0 1

.
22
for testing problem 1 while Loetamonphong and Fang’s method has 3152 solutions. In-
deed, this testing problem has only 93 minimal solutions. Our Step 12 helps to filter out
the nonminimal/reduant solutions among 113 solutions. Obviously, more computational
efforts will be needed to obtain 93 minimal solutions from the 3152 solutions in Loeta-
monphong and Fang’s method. Our experience also illustrates that the max-product
fuzzy relational equations with large number equations, say n ≥ 20, seem very difficult
to be consistent. That’s why we only report testing problems with n,m < 20.
Table 2. Performance of our procedure and Leotamonphong and Fang’s method
Size of Number of Our procedure Leotamonphong & Fang method
problem Problem minimal No. of cpu time No. of cpu time
No. (m,n) complexity solutions solutions (sec.) solutions (sec.)
1 (15,20) 5.12E+14 93 113 0.2734 3,152 17.8906
2 (15,18) 1.71E+13 85 91 0.2578 2,787 14.2031
3 (15,16) 7.11E+11 90 96 0.2344 1,510 4.9063
4 (15,15) 1.78E+11 100 110 0.2734 1,486 4.7813
5 (15,12) 2.22E+09 84 95 0.1875 544 0.9375
6 (12,20) 3.66E+12 16 17 0.0313 41 0.0469
7 (12,18) 1.46E+11 16 17 0.0234 41 0.0547
8 (12,16) 2.09E+10 27 28 0.0547 370 0.4531
9 (12,15) 6.97E+09 30 33 0.0547 362 0.4531
10 (12,12) 2.58E+08 34 38 0.0469 185 0.1797
11 (10,20) 1.88E+10 6 7 0.0234 17 0.0313
12 (10,18) 1.18E+09 6 7 0.0234 17 0.0234
13 (10,16) 1.96E+08 10 10 0.0234 128 0.1016
14 (10,15) 9.80E+07 12 13 0.0156 123 0.0859
15 (10,12) 1.22E+07 21 22 0.0313 97 0.0703
Remark. We have setup a Visual Basic 6.0 computer program to solve both exam-
ples. The computing environment includes a Pentium III PC with 1000MHZ CPU and
256MB RAM. Our program solves Example 1 in 0.0078125 CPU second and 0.0195 CPU
second for Example 2. For easy reference, flowchart of the program is shown in Fig. 3.
24
5 Conclusions
In the literature, there are many different kinds of fuzzy relational equations, each based
on a specific composition of fuzzy relations, that have played important roles in fuzzy
modeling. In this paper, we confine our study within the “class” of continuous max-t-
norm fuzzy relational equations. The commonly seen max-min and max-product fuzzy
relational equations are typical examples in this class.
Our contribution can be stated as follows. We add more theoretical results to the max-
t-norm fuzzy relational equations. In particular, we show that the greatest solution can
provide useful information in searching for the minimal solutions. To be more precise,
we narrow our study to a subclass of max-t-norm fuzzy relational equations called max-
Archimedean t-norm fuzzy relational equations. We show that each of its components
of minimal solutions is either 0 or the value of the corresponding component of the
greatest solution. This result allows us to preassign values to some components of minimal
solutions, hence the problem size may be reduced. Based on our results, we propose four
Rules to reduce the problem size so that the complete set of minimal solutions can be
yielded efficiently.
Due to Mostert and Shields’ classic result that the class of continuous t-norms can be
classified into three categories. Precisely, a continuous t-norm is either a “min” operation
or an Archimedean t-norm or the ordinal sum of a family of properly defined Archimedean
t-norms. Since the commonly seen max-min fuzzy relational equation has been studied
extensively in the literature. Studying the max-Archimedean t-norm fuzzy relational
equations is not only interesting by itself but also provides useful insight for understanding
the class of continuous max-t-norm fuzzy relational equations.
Markovskii has shown that solving all minimal solutions of the max-product fuzzy
relational equations is NP-hard. Since max-product is a specific composition of max-
Archimedean t-norms, it remains open to show the NP-hardness prevails for every specific
composition in the class of max-Archimedean t-norm fuzzy relational equations.
26
12. Y.-K. Wu and S.-M. Guu, “Minimizing a linear function under a fuzzy max-min
relational equation constraint,” Fuzzy Sets and Systems, Vol. 150, pp. 147-162,
2005.
13. E. Sanchez, “Resolution of composite fuzzy relation equations,”Information and
Control, Vol. 30, pp. 38-48, 1976.
14. W. Pedrycz, “On generalized fuzzy relational equations and their applications,”
Journal of Mathematical Analysis and Applications, Vol. 107, pp. 520-536, 1985.
15. A. Di Nola, S. Sessa, W. Pedrycz and E. Sanchez, Fuzzy Relation Equations and
Their Applications in Knowledge Engineering, Kluwer Academic Press, Dordrecht,
1989.
16. M. M. Gupta and J. Qi, “Theory of T-norms and fuzzy inference,” Fuzzy Sets and
Systems, Vol. 40, pp. 431-450, 1991.
17. M. M. Gupta and J. Qi, “Design of fuzzy logic controllers based on generalized
T-operators,” Fuzzy Sets and Systems, Vol. 40, pp. 473-489, 1991.
18. W. Pedrycz, “s − t Fuzzy relational equations,” Fuzzy Sets and Systems, Vol. 59,
pp. 189-195, 1993.
19. S. Wang, S.-C. Fang, and H. L. W. Nuttle, “Solution Sets of Interval-valued Fuzzy
Relational Equations,” Fuzzy Optimization and Decision Making, Vol. 2, no. 1, pp.
41-60, 2003.
20. A. V. Markovskii, “On the relation between equations with max-product compo-
sition and the covering problem,” Fuzzy Sets and Systems, Vol. 153, pp. 261-273,
2005.
21. L. Chen and P. P. Wang, “Fuzzy relation equations (I): the general and specialized
solving algorithms,” Soft Computing, Vol. 6, pp. 428-435, 2002.
22. L. Luoh, W.-J. Wang and Y.-K. Liaw, “Matrix-pattern-based computer algorithm
for solving fuzzy relation equations,” IEEE Transactions on Fuzzy Systems, Vol.
11, no. 1, pp. 100-108, 2003.
28
34. S. Jenei and J. C. Fodor, “On continuous triangular norms,” Fuzzy Sets and Sys-
tems, Vol. 100, pp. 273-282, 1998.
35. E. P. Klement, R. Mesiar and E. Pap, “Triangular Norms. Position paper III:
continuous t-norms,” Fuzzy Sets and Systems, Vol. 145, pp. 439-454, 2004.
36. E. P. Klement, R. Mesiar and E. Pap, Triangular Norms, Kluwer, Dordrecht, 2000.
37. S. Gottwald, Fuzzy Sets and Fuzzy Logic, Verlag Vieweg, Toulouse, 1993.
38. P. S. Mostert and A. L. Shields, “On the structure of semigroups on a compact
manifold with boundary,” Annals of Math., Vol. 65, pp. 117-143, 1957.
39. Y. D. Yu, “Triangular norms and TNF-sigma-algebras,” Fuzzy Sets and Systems,
Vol. 16, no. 3, pp. 251-264, 1985.
40. S. Weber, “Measures of fuzzy sets and measures of fuzziness,”Fuzzy Sets and Sys-
tems, Vol. 13, no. 3, pp. 247-271, 1983.
41. B. Schweizer and A. Sklar, Probabilistic Metric Spaces, North-Holland, New York,
1983.
30
