 1
 
行政院國家科學委員會補助專題研究計畫成果報告 
（□期中進度報告/▓期末報告） 
 
可調適平行度 VLIW 處理器與功耗最佳化編譯技術 
 
 
計畫類別：▓個別型計畫   □整合型計畫 
計畫編號：NSC 100-2221-E-182-031-MY2 
執行期間： 2012 年 8 月 1 日至 2013 年 10 月 31 日 
 
執行機構及系所：長庚大學 資訊工程系 
 
計畫主持人：馬詠程 
共同主持人：謝萬雲 
計畫參與人員：梁志斌、趙文仕、劉哲安、蔡宗佑 
 
本計畫除繳交成果報告外，另含下列出國報告，共 _0__ 份： 
□執行國際合作與移地研究心得報告 
□出席國際學術會議心得報告 
 
 
  期末報告處理方式： 
1. 公開方式： 
▓非列管計畫亦不具下列情形，立即公開查詢 
      □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
2.「本研究」是否已有嚴重損及公共利益之發現：▓否 □是 
3.「本報告」是否建議提供政府單位施政參考 ▓否 □是，    （請列舉提供
之單位；本會不經審議，依勾選逕予轉送） 
 
中   華   民   國 103 年 1 月 31 日
 3
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性），如已
有嚴重損及公共利益之發現，請簡述可能損及之相關程度（以 500 字為限） 
 
本計畫提出創新的低功耗處理器架構設計方式，使處理器可透過調節指令平
行度以節省耗能。欲透過調整指令平行度已達成省電效益，最大的障礙來自
於 register file 不可變的高耗能。透過本計畫所提出的創新架構與編譯器最佳
化技術，除可有效降低 register file 的耗能外，我們並證明 register file 的耗
能亦可透過調整指令平行度而改變。此創新的設計方式，已為計算機結構領
域最重要的期刊 ACM Transactions on Architecture and Code Optimization 所
接受，即將發表。在低功耗嵌入式系統的運用日益廣泛的今日，我們預期未
來的處理器設計應是廣泛運用 power gating 技術以調變功耗與效能的處理
器。本計畫的研究成果，突破了設計可調變功耗處理器最大的障礙，預計將
成為未來低功耗處理器設計的重要方向。 
 
 
1 Introduction
Digital signal processors (DSPs) with very long instruction word (VLIW) architectures
are widely used in embedded systems and multimedia systems-on-chip (SoCs) [1, 2]. As-
sisted by the compiler to exploit instruction level parallelism, a VLIW DSP has multiple
execution slots to provide high performance arithmetic computing. Many semiconduc-
tor manufacturers provide VLIW DSP cores for embedded SoCs [3, 4]. However, a
wide-issue VLIW architecture increases processor power and has a significant impact on
battery life, system density, cooling cost, and system reliability. On the other hand, in
the era of deep submicron semiconductor fabrication, designers face new challenges such
as how to reduce the exponentially increasing leakage power [5]. This paper investigates
the design of energy-efficient VLIW architectures for deep-submicron processes using
emerging technologies.
Main-stream VLIW DSPs are in some means of distributed register files architecture.
Researchers report that the register file power accounts for a large proportion of the
total processor power [6, 7]. The power dissipated in the global register file of an N -
issue VLIW processor is on the order of O(N3) [8]. Major VLIW DSPs employ some
form of distributed register files architectures to reduce the register file power [3, 9, 4].
Various researchers have also proposed a variety of compiler optimization technologies
for distributed register files architectures [10, 11, 12, 13, 14].
In recent years, energy-proportional computing [15, 16] has been proposed as a means
of improving energy efficiency. An energy-proportional processor has various speed
modes to adapt to applications with diverse performance requirements. Power con-
sumption is reduced while working in a lower speed mode. The philosophy is to run
the processor just fast enough to meet an application’s need. A dynamic voltage scaling
(DVS) [17, 18, 19] processor is a typical energy-proportional processor. However, while
the DVS scheme successfully reduces the dynamic power, designers are facing new chal-
lenges arising from the advancements made in semiconductor manufacturing processes.
One of the challenges arising from the deep sub-micron semiconductor manufactur-
ing process is the leakage power increasing exponentially and becoming comparable to
the switching power [5]. To cope with the rising ratio of the leakage power, multiple
threshold-voltage CMOS (MTCMOS) power-gating technologies are being proposed [20].
Various researchers have also proposed compiler-assisted power-gating control to reduce
the power dissipation of instruction level parallel processors [21, 22, 23, 24]. While power
reduction on functional units are widely discussed, we focus on reducing the power dis-
sipated in register files by means of power-gating.
This paper proposes a novel approach—energy-proportional computing on VLIW
architectures. Utilizing MTCMOS power-gating technologies, the design objective is
to make the processor power scales with adapted parallelism. We propose incorporat-
ing distributed power-gated register files (PGRF) with VLIW to achieve a proposed
PGRF-VLIW architecture that is designed to reduce the power dissipation on func-
tional units as well as register files. The power efficiency of the architecture relies on
compiler instruction scheduling to make efficient use of local registers and reduce power-
hungry data transfers across execution slots. Our major contribution is our proposed
deadline-constrained clustered scheduling (DCCS) algorithm for energy-aware instruc-
1
improve the performance of clustered VLIW architectures [10, 12, 13, 14]. This paper
redefines the instruction scheduling problem from an alternative viewpoint: deadline-
constrained energy optimization for energy-proportional computing.
In the drive to design compilers for architectures with distributed register files, pre-
vious researchers [28, 29] proposed graph clustering schemes for instruction scheduling.
A clustered architecture induces additional latency in inter-cluster data transfer com-
munications. To reduce program execution time, the data dependence graph (DDG) is
partitioned into virtual clusters such that the communication cost is reduced without
affecting parallelism exploitation. The virtual clusters are formed by repeatedly examin-
ing critical paths of the DDG. The virtual clusters are then mapped to physical clusters
with an as soon as possible (ASAP) scheduling policy. This paper revisits the graph
clustering problem from an alternative viewpoint: saving the data transfer energy on a
shared register file.
2.2 Compiler assisted MTCMOS power-gating
Progress in semiconductor manufacturing has given rise to a new challenge: how to
reduce leakage power [5]. MTCMOS circuitry [20] with compiler-assisted power-gating
has subsequently been proposed to overcome this challenge.
2.2.1 MTCMOS circuitry for leakage power reduction
Figure 1 illustrates the idea of MTCMOS power-gating. The operation circuit consists
of low-Vt (threshold voltage) transistors for high-speed computing. To reduce power
dissipation, current switches (Q) using high-Vt transistors are deployed to shut down
the leakage current flowing through the circuit during the idle time.
Figure 1: The MTCMOS power-gating technique.
Realizing MTCMOS power-gating induces overhead [20] and results in constraints
for architecture-level design. The first constraint comes from time and energy overhead
for transition between active and sleep modes. To prevent the circuit being destroyed by
a sudden rush of current, a gradual wake-up scheme has to be applied. To save energy,
power-gating is only enabled when the idle period exceeds a certain threshold. The
second constraint comes from the area overhead. An isolation cell has to be inserted
on each net across power domains to protect the circuit. As a result, power-domain
partitioning has become a key issue in architecture-level design.
3
Figure 2: (a) The PGRF-VLIW processor architecture. (b) The power-gated register
file.
local register file (LRF) that is accessible only by functional units in the execution slot.
Power-gating is applied to execution slots as well as register files. Each execution slot
is an individual power domain. Moreover, as shown in Figure 2(b), each of the local
register files and the shared register file is partitioned into banks with individual power
gating. Register banks not allocated with operands are shut down during execution.
The architecture is designed to extend the overall power scaling range of the pro-
cessor. Each execution slot is an individual power domain and hence power dissipation
on the functional units scales with parallelism. The energy complexity of register files
[8] leads to the use of LRFs: an LRF has fewer access ports and hence consumes less
power. The architecture relies on the compiler to direct most of the operand accesses
to LRFs. Moreover, it is believed that data transfer through SRF increases as paral-
lelism increases. Consequently, we expect power dissipation on both functional units
and register files to scale with changing parallelism.
3.2 Compiler support for the PGRF-VLIW processor
The architecture relies on the compiler to exploit its energy efficiency. Power reduction
is achieved through instruction scheduling and register allocation to allocate operations
and operands onto the power-gated architecture. The optimization problem is to min-
imize the processor power subject to the user-specified program execution deadline.
Instruction scheduling imposes constraints on register allocation and hence plays a key
role in optimization of the power of the register file. This paper focuses on the local
instruction scheduling, which is the theoretical foundation underpinning the success of
the architecture.
3.2.1 Overall compiler optimization flow
Figure 3(a) shows the code optimization flow. The compiler front-end translates the
high-level language source code into an intermediate representation (IR)—a series of
machine-level operations in static single assignment (SSA) form. Operations in IR work
on pseudo data operands called temporaries. A spill stage inserts load/store operations
into the IR such that the shared register file is able to store all register operands. In-
struction scheduling is performed to assign when (cycle) and where (execution slot) each
5
Figure 4: Example of a DDG and two different schedule charts for the example.
We use the DDG in Figure 4(a) to illustrate the guidelines. Suppose that the deadline
constraint is set to four cycles. Figure 4(c) shows a preferred schedule compared to Figure
4(b). The schedule in Figure 4(c) meets the deadline with only two execution slots and
hence consumes less computational energy. Moreover, the schedule in Figure 4(c) has
fewer cross-slot DDG edges and hence directs more temporaries that are accessed through
low-powered LRFs.
3.2.3 Motivating idea underlying the proposed DCCS algorithm
We propose our DCCS algorithm along the above guidelines. The algorithm is motivated
from the observation that most DSP applications consist of equation evaluations and
the DDG can be viewed as an inverted tree. The DCCS algorithm partitions a tree-like
DDG into clusters: each cluster is a tree and all operations in a cluster are scheduled in
the same execution slot. Such a clustering sets most of the edges in the DDG as intra-
slot edges. In addition, we propose an algorithm to obtain a sequential schedule aimed
at minimizing the register pressure within an execution slot. The proposed sequential
scheduling algorithm has been proved to minimize the number of local registers required
when the DDG is a tree. The final schedule for all parallel execution slots is established
by controlling the clustering granularity with the deadline constraint.
4 Sequential Scheduling to Minimize Register Re-
quirements
We first propose the High Register Pressure First (HRPF) algorithm for sequential
scheduling within an execution slot.
4.1 Modeling the sequential scheduling
Figure 5(a) illustrates our terminologies. The example shows a tree-structured DDG T
with accessing temporary tis marked with edges. In this example, each operation (vertex)
has a one-cycle latency. For a path from vertex Ii to Ij, we say Ii is an ancestor of
Ij and Ij is a descendant of Ii. A vertex having no ancestors in T is called a source
node of T , and a vertex having no descendants is called a sink node. For a tree T ,
7
T ′. Notations SSQ(T
′) and RQ(T
′) denote the subsequence and time range, respectively,
for a subtree T ′ in Q. For the example in Figure 5, we have SSQ(T1) = {I2, I1, I5, I6, I8},
and RQ(T1) = [1, 6].
The scheduling determines the register requirement of a program. The live range
of a temporary ti is the time range in which some register has to be occupied by the
value of ti. A schedule defines live ranges for all temporaries. The live range of ti
starts from the time the value is generated by the producer operation and ended at the
time the last operation reading ti is scheduled. In this example, all operations have
one-cycle latency to produce the outcome. The register pressure of a schedule is the
maximum number of simultaneously alive temporaries. With perfect register allocation,
the register pressure is the required number of registers. The lower half of Figure 5(b)
shows the live ranges, for which we have RP (Q) = 3.
We can now define the optimization problem. Given a tree-structured DDG T , we
are to find a topological order Q of T . The objective is to find a feasible sequence
with minimum register pressure. In general, finding an optimal schedule is NP-complete
[32]. Nevertheless, we devise a polynomial-time algorithm that is guaranteed to find an
optimal solution when the DDG is a tree.
4.2 The HRPF sequencing algorithm
Algorithm 1 presents the sequential scheduling algorithm. The algorithm builds a feasi-
ble sequence recursively by decomposing the tree T into subgraphs. The high register-
pressure first (HRPF) policy is applied to cascade sequential schedules for each subgraph.
The recursion terminates when a path graph is reached. Figure 6 shows an example. By
applying the HRPF policy, the temporary t4 carrying the result of subtree T1 does not
increase the register pressure in the final result.
ALGORITHM 1: HRPF Sequencing
Input: Tree-structured DDG T .
Output: A topological order Q of T .
1: if T is a path graph then
2: Let Q be the unique topological order of T ; Return Q;
3: end if
4: Backtracking from root(T ) to find the first vertex u with multiple immediate ancestors;
5: Decompose T as DQ(T ) = ({T1, T2, ..., Tn}, TR) from the split point u;
6: for each branch Tj do
7: Construct sequence Qj for each branch Tj using Algorithm HRPF Sequencing;
8: end for
9: Let QS = {Q1, Q2, ..., Qn}; //exclude QR for TR that contains root(T )
10: while QS is not empty do
11: Remove Qj with the largest RP (Qj) from QS;
12: Q = Q << Qj ;
13: end while
14: Q = Q << QR; Return Q;
9
Figure 7: Pattern associated with the proof of Lemma 2
Proof. Let Q∗ be an optimal sequence for T and consider an arbitrary branch T ′j .
In the case where the appearance order of T ′j is not less than j, Lemma 2 implies that
RP (Q∗) ≥ RP (SSQ∗(T
′
j)) + (j− 1). Consider the case in which the appearance order of
T ′j is less than j. There exists a branch T
′
i , with i < j (and hence RP
∗(T ′i ) ≥ RP
∗(T ′j)),
such that the appearance order of T ′i is not less than j. Lemma 2 implies that RP (Q
∗) ≥
RP (SSQ∗(T
′
i )) + (j − 1). Hence, we have RP
∗(T ) ≥ RP ∗(T ′j) + (j − 1) for each branch
T ′j , and this lemma follows. Q.E.D.
Finally, we prove that the HRPF sequencing algorithm is exact.
Theorem 1 Suppose each operation has one-cycle latency. The HRPF algorithm gen-
erates an optimal sequence for any tree-structured DDG.
Proof. An optimal sequence is obtained for a path graph since there is only one
choice. For a tree T with decomposition DQ(T ) = ({T1, T2, ...Tn}, TR), we prove the
theorem by induction on the number of decompositions to simplify to path graphs.
Suppose that the algorithm obtains optimal sequence Q∗j for each branch Tj. Sort the
parallel branches in non-increasing order of the optimal register pressure and consider
the sequence Q generated by the HRPF algorithm. Note that the sorted order of Tjs is
also the appearance order of all branches in Q. In sequence Q, each branch proceeding
Tj contributes one result-carrying temporary that is alive throughout RQ(Tj). Hence,
we have RP (Q) = max1≤j≤n{RP
∗(Tj) + (j − 1)} and Lemma 3 states the optimality.
Q.E.D.
5 The DCCS Algorithm for Scheduling Parallel Op-
erations
On the basis of the HRPF sequencing algorithm for a single execution slot, we propose
energy-aware instruction scheduling for parallel execution slots. The top-level framework
is shown in Algorithm 2, which executes the DCCS algorithm multiple times to select a
schedule that meets the deadline with minimum parallelism.
The core of the scheduling is the DCCS algorithm for fixed parallelism. Algo-
rithm 3 gives the framework: a clustering-and-allocation approach. A cluster Si is a
11
half-open states. A cluster is open if it is to be further merged with other clusters.
No further merging is performed on a closed cluster. A half-open cluster indicates
the beginning of a new session to search for a tree-structured portion. Only downward
merging is allowed on a half-open cluster. An operation Ii, having multiple out-going
edges, triggers a series of clusters closing (cf. Lines 22 to 24) by setting its ancestors
half-open. A cluster is closed if it cannot be merged with its descendants (cf. Lines 9
and 14).
ALGORITHM 4: Clustering
Input: A DDG G.
Output: Clusters {Si} and SDG H
1: Initialization: Create a cluster Si for each operation Ii and mark Si as open;
2: Assign levels to each operation Ii;
3: for each level m in increasing order do
4: for each operation Ii in level m do
5: /* Step 1: Decisions to merge ancestors with Ii */
6: if all ancestors of Ii are in open clusters then
7: Merge Ii with all ancestor clusters;
8: else if one of Ii’s ancestors is in a closed cluster then
9: /* Ii is an half-open cluster */
10: Keep Ii as an individual cluster;
11: Close all ancestor clusters of Ii;
12: else if one of Ii’s ancestors is in a half-open cluster then
13: Merge Ii with all ancestor clusters which is half-open;
14: Close each ancestor cluster of Ii which is not merged with Ii;
15: end if
16:
17: /* Step 2: Closing decision */
18: if out degree(Ii) = 0 then
19: Close the cluster containing Ii;
20: else if out degree(Ii) = 1 then
21: Keep current cluster of Ii in present state;
22: else if out degree(Ii) > 1 then
23: Close the cluster containing Ii;
24: Mark all descendant clusters of Ii as half-open;
25: end if
26: end for
27: end for
28: Build SDG H for dependencies over clusters and return H;
Figure 8 shows a running example of the clustering stage. In the snapshot depicted
in Figure 8(a), open clusters are merged to form larger trees. For example, the operation
I16 is merged with its open ancestor clusters {Sa, Sb}. In the snapshot depicted in Figure
8(b), a series of closing decisions are triggered by I18, which has more than one out-going
edge. Both I19 and I20 are marked as being in half-open clusters and the cluster S3 is
closed at I18. It is possible to execute I19 and I20 in parallel and the decision is left
13
ALGORITHM 5: Partitioning-to-fit
Input: A SDG and a CPD.
Output: An updated SDG.
1: Find the critical path CP in SDG;
2: while ETCP > CPD and CP is not composed of path graphs do
3: Find Sk: The node with maximum ET (Si) in CP and Sk is not a path graph;
4: Decompose Sk: DQ(Sk) = ({T1, T2, T3, ..., Tn}, TR);
5: Update SDG: Replace Sk with vertices {T1, T2, ..., Tn, TR} and corresponding
dependent edges;
6: Recompute the critical path CP for SDG;
7: end while
8: Return SDG;
5.3 Allocation stage
Finally, we complete the scheduling with the allocation stage. This stage allocates
nodes in SDG to a schedule chart SC, where a node is a tree-structured subgraph that
is allocated in one execution slot. Allocation guidelines are as follows:
1. Build an ASAP schedule that tries to shorten the makespan of the whole SDG,
2. Reduce the amount of cross-slot dependencies that results in operand accesses
through the high-powered SRF, and
3. Reduce the register pressure resulting from the schedule SC.
Algorithm 6 gives an outline of the allocation algorithm. A topological order SS over
nodes in SDG is established by depth-first traversal from sink nodes by treating each
edge in reverse direction [33]. To select the next node to append to SS, we break the tie
using the high-register-pressure-first policy. Each cluster Sj is then allocated to a row
of empty positions in the schedule chart SC following the ASAP guideline: a cluster
starts from the earliest available cycle τ that satisfies all dependence constraints. In
cases where there are multiple empty slots at cycle τ , we break the tie using the affinity
measure AFi, which counts DDG edges connecting the cluster to an execution slot. Note
that the depth-first-style topological order also helps to reduce cross-slot dependencies.
With restriction on hardware parallelism, some tree-structured portion of the SDG will
be allocated in a single execution slot.
Figure 9 illustrates how the partition-to-fit and allocation stages work. After the
clustering at Figure 8, instruction scheduling is now reduced to schedule the SDG in
Figure 9(a). Figures 9(b) and (c) illustrate how the partition-to-fit stage adjusts the
clustering granularity using CPD. Suppose that the SDG fails to satisfy CPD. The
cluster with the longest execution time (S2) is decomposed and parallelized to shorten
the makespan. Figure 9(d) gives a snapshot in the allocation stage. The scheduling
sequence is SS = {S2.1, S2.2, S2.3, S1, S3, S4, S5}. To schedule S5, the dependence from
S3 sets the earliest available cycle τ = m + 2. There are three available slots (Slot2,
Slot3, and Slot4) at cycle τ . The affinity, contributed by edge (S3, S5), leads S5 to Slot4.
15
operation is the time that all source operands are ready, considering the latencies from
all dependencies. The HRPF sequential scheduling also takes the operation latencies to
build the timed schedule. Most of the VLIW-DSP cores have heterogeneous execution
slots. To support heterogeneous slots, we modify the closing rules of the clustering stage
such that a cluster of operations fits into some execution slot types. The allocation stage
then ascertains the feasibility of assigning a clustered subgraph to an execution slot.
The proposed approach can also be applied to clustered hardware architectures [27]
,with addition of LRFs in each execution slot and the deployment of power-gating.
Instruction scheduling to support a clustered architecture has two phases [37]. The
first phase partitions the DDG into subgraphs, where each subgraph is assigned to a
hardware cluster. The second phase schedules each subgraph onto parallel execution
slots connected to a shared register file in a cluster. The DCCS algorithm serves as
the scheduling algorithm for the second phase and can cooperate with various cluster
assignment algorithms [27] in the first phase.
7 Evaluation of the DCCS algorithm
This section discusses the results of evaluations conducted on the power efficiency result-
ing from our approach. The static power consumed by the functional units in proportion
to the parallelism is trivial. Our evaluation focuses on the power dissipated on register
files.
We here compare the performance of the DCCS algorithm to Cai’s algorithm [28].
While our algorithm adjusts the parallelism and clustering granularity by the intended
performance, Cai’s algorithm clusters the DDG to reduce the communication cost with-
out affecting parallelism exploitation. The comparison indicates how we can balance
between parallelism exploitation and power dissipation.
7.1 Evaluation scheme
Figure 10 shows the flow of the evaluation process. We utilized benchmark programs
from the MiBench [25] and DSPstone [26] suites. The LLVM compiler [38] was used to
generate the DDG and the temporaries set. Code optimization worked on the LLVM
IR and loop unrolling performed to exploit parallelism. The DCCS algorithm was im-
plemented to build optimized schedule charts. The schedules were fed to the register
allocator using the Chaitin graph coloring algorithm [39]. The register allocator allo-
cated as many local temporaries as possible on the LRFs. In cases where an LRF did
not have sufficient space, local temporaries with the lowest access counts were sacrificed.
Temporaries not allocated to LRFs were allocated to SRF, also using the Chaitin algo-
rithm. We estimated register file power from the compiler outcome using the CACTI
power model [40] under a 45 nm process.
Although the DCCS algorithm can be cooperated with various global optimization
technologies, the evaluation shows the pure effect coming from the DCCS algorithm.
Each benchmark program is compiled multiple times with various hardware parallelism.
Each time a fixed parallelism is imposed on the whole program and the DCCS algorithm
17
Configuration code Bank size in LRF Bank size in SRF Overhead ratio
(1,1)-Ideal 1 1 -
(4,4) 4 4 16%
(4,8) 4 8 8%
(8,8) 8 8 7%
Table 2: Evaluated configuration and the estimated chip area overhead
We estimate the ratio of isolation cells in the total cell count as the measure of chip area
overhead.
Table 2 shows the evaluated bank-size configurations and the chip area overhead.
Configuration code (m, n) signifies an architecture in which a bank of LRF has m
registers and a bank of SRF has n registers. A special configuration, coded “(1,1)-Ideal,”
indicates the ideal power saving effect achievable if VLSI researchers keep reducing the
power-gating overhead. This configuration represents an architecture in which each
register is power-gated individually without any overhead.
7.1.2 Power model used in the evaluation
We computed power dissipation using the compiler outcome and the CACTI power
model. Table 3 shows all the related parameters. CACTI is an SRAM model and
provides the (1) static power, (2) dynamic energy per access, and (3) access delay of
an SRAM block. The SRAM configuration includes the (1) manufacturing process, (2)
number of access ports, (3) number of storage cells, and (4) width per access (equivalent
to register width). A bank of PGRF is modeled as an SRAM block and power parameters
erw, est, eˆrw, and eˆst are obtained. For the PGRF-VLIW architecture, the power formulas
with the isolation cells overhead considered are as follows:
Pst(m) = LB(m) ∗ est + SB(m) ∗ eˆst + Pst,iso,
Pdyn(m) =
1
T
∗ A ∗ ((1− SR(m)) ∗ erw + SR(m) ∗ eˆrw) + Pdyn,iso(m).
In the formulas, Pst,iso is approximated using the static power of a 1-port SRAM block
with an equal number of storage cells. To obtain Pdyn,iso(m), the energy-per-transfer of
an isolation cell is approximated using a 1-port single-register SRAM block. To compare
with the traditional design, we also computed the power dissipation of the SRF-only
architecture. Owing to space constraints, detailed formulations for Pst,SRF and Pdyn,SRF
are omitted here.
Note that the evaluation overestimates the energy overhead on isolation cells. En-
ergy from decoding logic, which is not consumed by isolation cells, is calculated in our
approximation scheme. Even with the overestimated overhead, our result still shows
significant power savings from the power-gated architecture. Experiment reports on
validating the power model can be found on our website [42].
19
7.2 Code optimization effect from the compiler
To understand the effect the compiler has on power efficiency, we investigated how
register file usage scales with parallelism and cross-referenced it with the speedup.
7.2.1 The Speedup
Figure 12 shows the speedup scaling with parallelism m. The benchmark programs are
classified into three classes: (1) the high-parallelism class (Figure 12(a)) which has almost
linear speedup up to the maximum parallelism, (2) the middle-parallelism class (Figure
12(b)), which gets saturated above m=7, and (3) the low-parallelism class for remaining
benchmark programs. Figure 13 shows the speedup achieved by Cai’s algorithm. Both
algorithms achieve comparable speedup on most of the benchmark programs. Cai’s algo-
rithm achieves a better speedup for a set of benchmark programs in the low-parallelism
class.
1 2 3 4 5 6 7 8
0
1
2
3
4
5
6
7
8
9
(a)
Sp
ee
du
p 
ra
tio
 
 
03_susan_smoothing
04_susan_principle
06_susan_corners
07_susan_edges
11_convolution
12_fir
13_fir2dim
14_iir_biquard_N_sections
16_n_complex_updates
24_adpcm
IPC 1 2 3 4 5 6 7 8
0
1
2
3
4
5
6
7
8
9
(b)
Sp
ee
du
p 
ra
tio
 
 
01_fft_float
02_fht
09_set_key
15_matrix
17_dijkstra
21_aesxam
25_complex_multiply
26_dot_product
27_lms
28_startup
IPC 1 2 3 4 5 6 7 8
0
1
2
3
4
5
6
7
8
9
(c)
Sp
ee
du
p 
ra
tio
 
 
05_median
08_SolveCubic
10_encrypt
18_bitstring
19_patricia
20_crc
22_sha
23_pbmsrch
IPC
Figure 12: Speedup achieved by the DCCS algorithm
1 2 3 4 5 6 7 8
0
1
2
3
4
5
6
7
8
9
(a)
Sp
ee
du
p 
ra
tio
 
 
03_susan_smoothing
04_susan_principle
06_susan_corners
07_susan_edges
11_convolution
12_fir
13_fir2dim
14_iir_biquard_N_sections
16_n_complex_updates
24_adpcm
IPC 1 2 3 4 5 6 7 8
0
1
2
3
4
5
6
7
8
9
(b)
Sp
ee
du
p 
ra
tio
 
 
01_fft_float
02_fht
09_set_key
15_matrix
17_dijkstra
21_aesxam
25_complex_multiply
26_dot_product
27_lms
28_startup
IPC 1 2 3 4 5 6 7 8
0
1
2
3
4
5
6
7
8
9
(c)
Sp
ee
du
p 
ra
tio
 
 
05_median
08_SolveCubic
10_encrypt
18_bitstring
19_patricia
20_crc
22_sha
23_pbmsrch
IPC
Figure 13: Speedup achieved by Cai’s algorithm
7.2.2 SRF access ratio
Figure 14 shows, for the DCCS algorithm, the ratio of total register accesses sent to the
SRF. The DCCS algorithm directs more than 40% of register accesses to low-powered
LRFs in order to conserve power. High- and middle-parallelism benchmark programs
can be divided into two sets: the first set has SA(m) scales up with parallelism and
the second set has low and saturated SA(m). The benchmark program “13 fir2dim”
exemplifies the first set and the behavior matched our expectations: the dynamic power
scaled up with increased parallelism. The second set of benchmark programs has a
different behavior: the ratio to access SRF is low and virtually fixed. The benchmark
“03 susan smoothing” exemplifies this set. Note that each benchmark program in the
high and middle parallelism class obtains speedup increments with increased parallelism.
The saturated SA(m) indicates that the DCCS algorithm has a good clustering effect
21
1 2 3 4 5 6 7 8
0
10
20
30
40
50
(a)
Th
e 
am
ou
nt
 o
f a
ct
iv
e 
SR
F 
re
gi
ste
rs
 
 
03_susan_smoothing
04_susan_principle
06_susan_corners
07_susan_edges
11_convolution
12_fir
13_fir2dim
14_iir_biquard_N_sections
16_n_complex_updates
24_adpcm
IPC 1 2 3 4 5 6 7 8
0
10
20
30
40
50
(b)
Th
e 
am
ou
nt
 o
f a
ct
iv
e 
SR
F 
re
gi
ste
rs
 
 
01_fft_float
02_fht
09_set_key
15_matrix
17_dijkstra
21_aesxam
25_complex_multiply
26_dot_product
27_lms
28_startup
IPC 1 2 3 4 5 6 7 8
0
10
20
30
40
50
(c)
Th
e 
am
ou
nt
 o
f a
ct
iv
e 
SR
F 
re
gi
ste
rs
 
 
05_median
08_SolveCubic
10_encrypt
18_bitstring
19_patricia
20_crc
22_sha
23_pbmsrch
IPC
Figure 16: Number of active SRF registers resulting from the DCCS algorithm
1 2 3 4 5 6 7 8
0
10
20
30
40
50
(a)
Th
e 
am
ou
nt
 o
f a
ct
iv
e 
SR
F 
re
gi
ste
rs
 
 
03_susan_smoothing
04_susan_principle
06_susan_corners
07_susan_edges
11_convolution
12_fir
13_fir2dim
14_iir_biquard_N_sections
16_n_complex_updates
24_adpcm
IPC 1 2 3 4 5 6 7 8
0
10
20
30
40
50
(b)
Th
e 
am
ou
nt
 o
f a
ct
iv
e 
SR
F 
re
gi
ste
rs
 
 
01_fft_float
02_fht
09_set_key
15_matrix
17_dijkstra
21_aesxam
25_complex_multiply
26_dot_product
27_lms
28_startup
IPC 1 2 3 4 5 6 7 8
0
10
20
30
40
50
(c)
Th
e 
am
ou
nt
 o
f a
ct
iv
e 
SR
F 
re
gi
ste
rs
 
 
05_median
08_SolveCubic
10_encrypt
18_bitstring
19_patricia
20_crc
22_sha
23_pbmsrch
IPC
Figure 17: Number of active SRF registers resulting from Cai’s algorithm
As expected, the required number of LRF registers per execution slot scales down as
parallelism increases. With high-parallelism execution, an execution slot needs only four
to eight local registers. Figure 19 shows the results for Cai’s algorithm. For most of the
benchmark programs, the DCCS algorithm has comparable LRF-pressure. The curves
for SRF access and usage imply that the DCCS algorithm allocates more temporaries
on LRFs. The comparable LRF-pressure comes from the effect of HRPF sequencing to
reduce the register pressure.
1 2 3 4 5 6 7 8
0
5
10
15
20
(a)
LR
F 
pr
es
su
re
 
 
03_susan_smoothing
04_susan_principle
06_susan_corners
07_susan_edges
11_convolution
12_fir
13_fir2dim
14_iir_biquard_N_sections
16_n_complex_updates
24_adpcm
IPC 1 2 3 4 5 6 7 8
0
5
10
15
20
(b)
LR
F 
pr
es
su
re
 
 
01_fft_float
02_fht
09_set_key
15_matrix
17_dijkstra
21_aesxam
25_complex_multiply
26_dot_product
27_lms
28_startup
IPC 1 2 3 4 5 6 7 8
0
5
10
15
20
(c)
LR
F 
pr
es
su
re
 
 
05_median
08_SolveCubic
10_encrypt
18_bitstring
19_patricia
20_crc
22_sha
23_pbmsrch
IPC
Figure 18: Scaling of LRF-pressure for the DCCS algorithm
1 2 3 4 5 6 7 8
0
5
10
15
20
(a)
LR
F 
pr
es
su
re
 
 
03_susan_smoothing
04_susan_principle
06_susan_corners
07_susan_edges
11_convolution
12_fir
13_fir2dim
14_iir_biquard_N_sections
16_n_complex_updates
24_adpcm
IPC 1 2 3 4 5 6 7 8
0
5
10
15
20
(b)
LR
F 
pr
es
su
re
 
 
01_fft_float
02_fht
09_set_key
15_matrix
17_dijkstra
21_aesxam
25_complex_multiply
26_dot_product
27_lms
28_startup
IPC 1 2 3 4 5 6 7 8
0
5
10
15
20
(c)
LR
F 
pr
es
su
re
 
 
05_median
08_SolveCubic
10_encrypt
18_bitstring
19_patricia
20_crc
22_sha
23_pbmsrch
IPC
Figure 19: Scaling of LRF-pressure for Cai’s algorithm
7.3 Power efficiency of register files
We also evaluated the power scaling functions on static and dynamic power to justify
the expected effects:
PSLst(m) =
Pst(m)
PSRF,st
, PSLdyn(m) =
Pdyn(m)
PSRF,dyn
.
23
(1,1)−Ideal (4,4) (4,8) (8,8) (1,1)−Ideal_Cai (4,4)_Cai
2 4 6 8
0
0.2
0.4
0.6
0.8
01_fft_float
 
 
PS
L s
t
2 4 6 8
0
0.2
0.4
0.6
0.8
02_fht
 
 
2 4 6 8
0
0.2
0.4
0.6
0.8
09_set_key
 
 
2 4 6 8
0
0.2
0.4
0.6
0.8
15_matrix
 
 
2 4 6 8
0
0.2
0.4
0.6
0.8
17_dijkstra
 
 
IPC
2 4 6 8
0
0.2
0.4
0.6
0.8
21_aesxam
 
 
PS
L s
t
2 4 6 8
0
0.2
0.4
0.6
0.8
25_complex_multiply
 
 
2 4 6 8
0
0.2
0.4
0.6
0.8
26_dot_product
 
 
2 4 6 8
0
0.2
0.4
0.6
0.8
27_lms
 
 
2 4 6 8
0
0.2
0.4
0.6
0.8
28_startup
 
 
IPC
Figure 21: Scaling of static power for middle-parallelism benchmarks
(1,1)−Ideal (4,4) (4,8) (8,8) (1,1)−Ideal_Cai (4,4)_Cai
2 4 6 8
0
0.2
0.4
0.6
0.8
05_median
 
 
PS
L s
t
2 4 6 8
0
0.2
0.4
0.6
0.8
08_SolveCubic
 
 
2 4 6 8
0
0.2
0.4
0.6
0.8
10_encrypt
 
 
2 4 6 8
0
0.2
0.4
0.6
0.8
18_bitstring
 
 
IPC
2 4 6 8
0
0.2
0.4
0.6
0.8
19_patricia
PS
L s
t
 
 
2 4 6 8
0
0.2
0.4
0.6
0.8
20_crc
 
 
2 4 6 8
0
0.2
0.4
0.6
0.8
22_sha
 
 
2 4 6 8
0
0.2
0.4
0.6
0.8
23_pbmsrch
 
 
IPC
Figure 22: Scaling of static power for low-parallelism benchmarks
Cai’s algorithm, the DCCS algorithm consumes less dynamic power. For 18 out of the
28 benchmark programs, the DCCS algorithm saves at least 10% of the dynamic power.
(1,1)−Ideal (4,4) (4,8) (8,8) (1,1)−Ideal_Cai (4,4)_Cai
2 4 6 8
0
0.2
0.4
0.6
0.8
1
03_susan_smoothing
 
 
PS
L d
yn
2 4 6 8
0
0.2
0.4
0.6
0.8
1
04_susan_principle
 
 
2 4 6 8
0
0.2
0.4
0.6
0.8
1
06_susan_corners
 
 
2 4 6 8
0
0.2
0.4
0.6
0.8
1
07_susan_edges
 
 
2 4 6 8
0
0.2
0.4
0.6
0.8
1
11_convolution
 
 
IPC
2 4 6 8
0
0.2
0.4
0.6
0.8
1
12_fir
 
 
PS
L d
yn
2 4 6 8
0
0.2
0.4
0.6
0.8
1
13_fir2dim
 
 
2 4 6 8
0
0.2
0.4
0.6
0.8
1
 
 
14_iir_biquard_N_sections
2 4 6 8
0
0.2
0.4
0.6
0.8
1
 
 
16_n_complex_updates
2 4 6 8
0
0.2
0.4
0.6
0.8
1
24_adpcm
 
 
IPC
Figure 23: Scaling of dynamic power for high-parallelism benchmarks
Finally, we compared the energy saving ratio (ES) at maximum parallelism achieved
25
ES
st
−(4,4) ES
dy
−(4,4)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
En
er
gy
 sa
vi
ng
 ra
tio
 
 
04 susan
principle
06 susan
corners
07 susan
edges
12 fir
 
13 fir2dim
 
14 iir biquard 
N sections
16 n complex
updates
24 adpcm
 
11 convolution
 
03 susan
smoothing
(a)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
En
er
gy
 sa
vi
ng
 ra
tio
 
 
28 startup
 
27 lms
 
25 complex
multiply
26 dot product
 
21 aesxam17 dijkstra
 
15 matrix
 
02 fht
 
09 set key
 
01 fft float
 
(b)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
En
er
gy
 sa
vi
ng
 ra
tio
 
 
05 median
 
08 SolveCubic
 
10 encrypt
 
18 bitstring
 
19 patricia
 
20 crc
 
23 pbmsrch
 
22 sha
 
(c)
Figure 26: Energy saving of the DCCS algorithm compared to Cai’s algorithm
contribution saves the power dissipated on register files. The success relies on the DCCS
algorithm for instruction scheduling to reduce the register pressure and cross-slot data
transfer. The results of evaluations conducted confirm the expected effects as follows:
1. Compared to the traditional SRF-only architecture without power-gating, the
PGRF-VLIW architecture saves approximately 20% to 50% of register file power
even with maximum parallelism,
2. As an overall trend, the register file power scales up with increased parallelism.
The scaling range occupies approximately 20% to 35% of the dynamic power and
35% to 65% of the static power.
The most important finding is that the power dissipated on register files can be saved and
scaled through instruction scheduling with parallelism adaption. The compiler outcome
shows the need to apply power-gating on register files: application programs have diverse
storage and parallelism requirements. While this work establishes the architecture with
theory on local code optimization, future work will enhance the compiler for practical
use. Loop and global code optimization, such as software pipelining, are planned for the
next stage in our drive toward energy-proportional computing.
References
[1] Philips, “Philips nexperiahighly integrated programmable system-on-chip
(http://www.semiconductors.philips.com/products/nexperia),” 2011.
[2] Texas Instruments, “OMAP 5 mobile applications platform,” 2011.
[3] Freescale Semiconductor, “Tuning C code for StarCore-based digital signal proces-
sors,” 2008.
27
[16] K. W. Cameron, “The challenges of energy-proportional computing,” Computer,
vol. 43, pp. 82 –83, May 2010.
[17] I. Corporation, “The Intel xscale microarchitecture,” 2000.
[18] A. M. Devices, “Mobile amd athlon 4 processor model 6 cpga data sheet,” 2001.
[19] M. Fleischmann, “Longrun power managament: dynamic power management for
Crusoe processors,” in AMD Technical Report 24319, January 2001.
[20] Y. Shin, J. Seomun, K.-M. Choi, and T. Sakurai, “Power gating: Circuits, design
methodologies, and best practice for standard-cell VLSI designs,” ACM Trans. Des.
Autom. Electron. Syst., vol. 15, pp. 28:1–28:37, October 2010.
[21] H. S. Kim, N. Vijaykrishnan, M. Kandemir, and M. J. Irwin, “Adapting instruction
level parallelism for optimizing leakage in VLIW architectures,” in LCTES ’03:
Proceedings of the 2003 ACM SIGPLAN conference on Language, compiler, and
tool for embedded systems, (New York, NY, USA), pp. 275–283, ACM, 2003.
[22] M. Wang, Y. Wang, D. Liu, Z. Qin, and Z. Shao, “Compiler-assisted leakage-aware
loop scheduling for embedded VLIW DSP processors,” Journal of Systems and
Software, vol. 83, no. 5, pp. 772 – 785, 2010.
[23] Y.-P. You, C. Lee, and J. K. Lee, “Compilers for leakage power reduction,” ACM
Trans. Des. Autom. Electron. Syst., vol. 11, no. 1, pp. 147–164, 2006.
[24] Y.-P. You, C.-W. Huang, and J. K. Lee, “Compilation for compact power-gating
controls,” ACM Trans. Des. Autom. Electron. Syst., vol. 12, no. 4, p. 51, 2007.
[25] M. Guthaus, J. Ringenberg, D. Ernst, T. Austin, T. Mudge, and R. Brown,
“Mibench: A free, commercially representative embedded benchmark suite,” in
Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on,
pp. 3 – 14, dec. 2001.
[26] V. Zivojnovic, J. M. Velarde, C. Schlager, and H. Meyr, “Dspstone: A dsp-oriented
benchmarking methodology,” in Proceedings of International Conference on Signal
Processing Applications and Technology, vol. 94, 1994.
[27] A. S. Terechko and H. Corporaal, “Inter-cluster communication in VLIW architec-
tures,” ACM Trans. Archit. Code Optim., vol. 4, no. 2, p. 11, 2007.
[28] Q. Cai, J. Codina, J. Gonzalez, and A. Gonzalez, “A software-hardware hybrid
steering mechanism for clustered microarchitectures,” in Parallel and Distributed
Processing, 2008. IPDPS 2008. IEEE International Symposium on, pp. 1–12, 2008.
[29] J. M. Codina, J. Sanchez, and A. Gonzalez, “Virtual cluster scheduling through the
scheduling graph,” in Proceedings of the International Symposium on Code Gen-
eration and Optimization, CGO ’07, (Washington, DC, USA), pp. 89–101, IEEE
Computer Society, 2007.
29
國科會補助計畫衍生研發成果推廣資料表
日期:2014/01/29
國科會補助計畫
計畫名稱: 可調適平行度VLIW 處理器與功耗最佳化編譯技術
計畫主持人: 馬詠程
計畫編號: 100-2221-E-182-031-MY2 學門領域: 計算機結構與計算機系統
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
