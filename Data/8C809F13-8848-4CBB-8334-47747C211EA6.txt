 2 
1. Introduction 
In recent years, 3D technologies have matured due to rapid advancements in hardware performance. Users 
have begun to explore virtual 3D worlds such as those developed by Linden Lab's virtual life and numerous 
on-line games. Gradually, many users have become unsatisfied with using a keyboard as an interface; they 
prefer to become "immersed" in the world of virtual 3D objects through more intimate interaction such as 
hand gestures and head movements. However, there is still a lack of reality even when “immersed” in the 
virtual world. In order to experience a real virtual 3D effect, a technology called augmented reality 
(augmented reality) can be used. augmented reality allows virtual computer-generated imagery to augment a 
live view of a physical, real-world environment.  
This study uses a web cam and the integration of photos to create 3D graphic objects. The main purpose of 
this study is to extract the features of 2D images to identify their corresponding 3D coordinates. However, 
existing augmented reality systems have limitations. The first restriction involves the complex background of 
the natural environment, whether indoor or outdoor. To date, no single system has been able to handle the 
variety and complexity of the background in a natural environment. The next issue involves processing time 
limitations. Although some systems can precisely capture 3D coordinates to track objects, these systems are 
too time-consuming to meet real-time video processing demands. The human eye is aware of processing 
issues if the video speed is less than 30 frames per second. To overcome these two restrictions, researchers 
have begun to look for robust detection methods for feature points and local feature descriptors as well as for 
better tracking methods.  
This study explores the use of a robust local feature descriptor together with tracking methods to create an 
efficient framework for augmented reality systems. The contribution of this study lies first in using binary 
search trees to improve the matching efficiency of corresponding feature points. Within an augmented reality 
system, this new architecture accelerates optical flow tracking to enhance overall system performance. We 
organize all feature patterns into a nearly perfect binary search tree so that the system can more quickly find 
the most similar feature patterns. The proposed system also allows users to flexibly choose the compared 
region by identifying object characteristics. In this way, it significantly decreases the number of matched 
feature points. After exploring and reorganizing the six currently most popular augmented reality systems, a 
common framework for a holistic augmented reality system is proposed. This general framework of 
augmented reality employs a tracking mechanism that uses a robust pyramid Lucas-Kanade optical flow 
algorithm to track feature points. It is thus capable of displaying 3D objects quickly and stably in the context 
of an augmented reality system. 
2. Background 
Under a general augmented reality framework, the first step is to calibrate the camera to extract corresponding 
feature points from images. Camera calibration is used to estimate the relative position of cameras by 
calculating these corresponding feature points. The second step is to track these feature points to predict the 
coordinate of the next object or camera. If the second step fails, the algorithm returns to the first step to again 
estimate the camera’s position. We now discuss these two steps of the augmented reality framework.   
In 2003, Kato et al. [1] implemented the first step by using the augmented reality toolkits tag to obtain the 
initial camera position. In their analysis, in the second step, which has two parts, three kinds of characteristic 
patterns are first solicited from different image resolutions with a neighborhood-based correlation method 
during off-line processing. These characteristic patterns are then stored in a database. During real-time on-line 
processing, the initial position of the camera is obtained in the first step. In the second part of the second step, 
the image plane position in the video screen can be recognized and used to detect the characteristic pattern. 
These authors proposed a mechanism for selecting feature points used in comparing characteristics of an 
existing pattern with feature templates in the database. Until four corresponding feature points are obtained, 
the built-in processes in augmented reality Toolkits are used to find the current camera position.  
In 2005, Bastos et al. [2] proposed two components of augmented reality processing, off-line 
pre-processing and on-line processing. For off-line processing, Shi and Tomasi [3] developed a feature 
detection method to capture characteristic patterns of the same scale and used the normalized correlation 
matching method to eliminate similar patterns (i.e., a correlation coefficient greater than 0.9). They then 
stored these characteristic patterns in a database. During real-time on-line processing, the initial video image 
is obtained by setting the threshold value for binarization and then implementing labeling processing to obtain 
both internal and external contours of the objects. The internal contour is discarded. The Five-Point Pose 
Algorithm is then used to calculate the projection matrix (i.e., homography) with the center and boundary 
points of the external contour. A matching projection matrix composed of the on-line image and the reference 
image is used to adjust the orientation, thus obtaining the relative position of the camera. This method can be 
used to complete the first step of initializing the camera. The second step can be divided into two phases: (1) 
 4 
3 Research Methods 
This proposed research method includes local descriptors, namely, SIFT (i.e., Distinctive Image Features from 
Scale-Invariant Key-points) and ACPI (i.e., Automatic Camera Pose Initialization). The method searches for 
corresponding feature points using the pyramid Lucas-Kanade optical flow algorithm, and it ensures the 
correct corresponding points based on RANdom SAmple Consensus algorithm (RANSAC). Finally, it reduces 
the number of feature templates by using a comparison based on the binary search tree algorithm. An 
augmented reality system inputs a 3D object into a world coordinate system and then calibrates camera 
parameters for this system. A new tracking mechanism is proposed to improve tracking performance. First, the 
new positions of feature points are obtained from the positions of feature points in the previous image frame. 
A parameter estimation algorithm is then used to remove outliers and obtain the relative position of the 
camera. If the number of feature points from the previous image frame is less than four, the feature points are 
detected again to obtain the positions of the feature points. The pyramid Lucas-Kanade optical flow algorithm 
is used as the tracking method. RANSAC is the parameter estimation algorithm used to remove outliers. The 
previous and current positions of the feature points are used to calibrate camera parameters. The entire process 
is shown in Figure 1. 
 
 
Figure 1. The optical flow tracking framework. 
 
To obtain the relative position of the camera, a feature template is extracted as a local descriptor. The most 
similar feature template is then found from the database. RANSAC is used to filter the current position of the 
current feature template and the position of the most similar feature template. The precise correspondence 
relation is obtained from the current feature template and its most similar feature template. If the number of 
corresponding feature points is greater than four, then compute the project matrix of these corresponding 
Grab image frame. 
Start 
Match 
Find local feature 
descriptors. 
RANSAC 
The number of 
characteristic points 
Draw 3D object 
Camera initialization 
Homography 
Pyramid 
Lucas-Kanade optical 
flow 
Grab image frame. 
Homography != NULL 
Track 
 
 
 6 
Table 1 shows that performing SVD is much faster than building a series of binary codes. Indeed, MBST is 
more efficient than ACPI. To compare the overall processing time using MBST with that using ACPI, we 
enlarge the test image Graf to the size of 1000x1200 and extract 756 feature points. There are three kinds of 
methods for comparing overall processing time. The first method is “ACPI + No Cluster”, which does not 
cluster feature points and matches all feature points at each iteration. The second method is “ACPI + Cluster”, 
which performs binary code clustering [24].  
 
Figure 3. The overall processing time for “ACPI + No Cluster”, “ACPI + Cluster” and “MBST” 
 
Figure 3 shows that the overall processing time for “ACPI + No Cluster” is proportional to the number of 
feature templates. For example, it takes 50 seconds when the number of feature templates is 700. It also shows 
that MBST is faster than ACPI + Cluster. 
 
 
Figure 4. The processing time of clustering for “ACPI + Cluster” and MBST 
 
Figure 4 shows that the processing time for clustering under MBST is about 500 msec when the number of 
feature templates is 700. This demonstrates that MBST is faster than binary code clustering [24]. 
 
 8 
 
 
Table 2: The processing time using different local descriptors 
Local Descriptor Frames Per Seconds (FPS) 
640X480 320X240 
SIFT 0.511016 2.17457 
SIFT + Optical flow 5.59771 8.84798 
SURF 2.25218 4.28946 
SURF + Optical flow 9.17638 30.9823 
 
 
5. Conclusions 
 
This research proposes the use of a binary search tree to reduce the iterations needed to match corresponding 
feature points and to flexibly select a matching range to enhance overall system performance. Experimental 
results show that the proposed MBST is more efficient than the binary code clustering method [11]. For a 
real-time tracking mechanism, the pyramid Lucas-Kanade optical flow algorithm was introduced to predict 
the current position of feature points; it significantly improves a system’s tracking ability.  
The contributions of this research can be summarized as follows. 
(1) All feature templates are used to build a nearly balanced MBST. Thus, the proposed system is able to 
efficiently find the most similar feature templates. 
(2) The method allows users to flexibly select the matching range based on features of the recognized object. 
Accordingly, it dramatically reduces the number of iterations needed for matching. 
(3) The method is based on an exploration of the six most popular augmented reality systems, thereby 
resulting in a common augmented reality framework. 
(4) A tracking mechanism together with a robust pyramid Lucas-Kanade optical flow algorithm is used to 
trace feature points, enabling the proposed system to efficiently show 30 objects. 
 
References 
 
1. H. Kato, K. Tachibana, M. Billinghurst, and M. Grafe, “A Registration Method based on Texture 
Tracking using augmented realityToolKit”, The Second IEEE International Augmented Reality Toolkit 
Workshop, 77－85 , 2003. 
2. R. Bastos, “Tracking of Planar Objects in Natural Scenes using Textures”, Master of Science Thesis in 
Computers and Telecommunications Engineering, 2005. 
3. J. Shi, and C. Tomasi, “Good Features to Track”, IEEE Conference on CVPR, 593－600 , 1994. 
4. C. Yuan, “Markerless Pose Tracking for Augmented Reality”, Proceedings of ISVC, 721－730, 2006. 
5. G. Klein, and D. Murray, “Parallel Tracking and Mapping for Small augmented reality Workspaces”, 
International Symposium on Mixed and Augmented Reality - ISMaugmented reality'07, 1－10, 2007. 
6. R. I. Hartley and A. Zisserman, Multiple View Geometry in Computer Vision, 2nd Edition of Cambridge 
University Press, 2004.  
7. E. Rosten and T. Drummond, “Machine learning for high-speed corner detection”, Proc. 9th European 
Conference on Computer Vision (ECCV’06), 430－443 , 2006. 
8. D. Nister, “An efficient solution to the five-point relative pose problem”, PAMI, 756－770, 2004. 
9. K. Xu, K. W. Chia and A. D. Cheok, “Real-time camera tracking for marker-less and unprepared 
augmented reality environments”, Image and Vision Computing, 673－689, 2008. 
10. R. E. Kalman, “A new approach to linear filtering and prediction problems”, Transactions of ASME – 
Journal of Basic Engineering, 35－45, 1960. 
11. R. Bastos and M. S. Dias, “Automatic Camera Pose Initialization, using Scale, Rotation and Luminance 
Invariant Natural Feature Tracking”, Proceedings of WSCG' 2008 - The 16-th International Conference 
in Central Europe on Computer Graphics, Visualization and Computer Vision '2008, 97－104 2008. 
12. J. Shi and C. Tomasi, “Good Features to Track”, IEEE Conference on CVPR, 593－600 , 1994. 
13. Y.I. Abdel-Aziz and H.M. Karara, “Direct Linear Transformation into Object Space Coordinates in 
Close-Range Photogrammetry”, Procedures of Symposium of Close-Range Photogrammetry, 1－18 , 
1971. 
14. J.-Y. Bouguet, “Pyramidal Implementation of the LucasKanade Feature Tracker Description of the 
Algorithm”, Intel Corporation, Microprocessor Research Labs., 1074－1082, 1999. 
 10 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。 
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
   ■達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明： 
 
 
 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：□已發表■未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 □無 
技轉：□已技轉 □洽談中 □無 
其他：（以 100 字為限） 
 
 
 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
 
    本研究廣義的介紹擴充實境系統的類型和追蹤技術，主要的焦點會放在以電腦視覺為主
的無標記符號追蹤技術上，我們將深入探討此種追蹤技術在 2003年到 2008年間六種相關的
擴充實境系統，並歸納出一個共通的擴充實境架構。首先，我們提出一種有彈性的二元搜尋
樹來改善 Bastos 所提出的二元碼分群機制，得以大幅減少特徵樣版比對的次數。再來，我們
將追蹤機制加入到共通的擴充實境架構中，然後選取強健的光流演算法來追蹤特徵點，以加
速整個擴充實境系統。我們所提出的二元搜尋樹方法應用在靜態影像實驗上，在比對 700 個
特徵點時，只需要花費大約 500 毫秒的時間，而在即時視訊影像實驗，我們提出的光流追蹤
架構分別應用在兩種不同的局部特徵描述子 SIFT和 SURF上，實驗的結果都被證明非常有效
率，其中最好的結果還達到每秒三十一個影格的速度。 
 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：黃文楨 計畫編號：99-2221-E-327-040- 
計畫名稱：一個有效率使用二元樹和光流法的擴充實境架構 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 2 2 30%  
研究報告/技術報告 0 0 100%  
研討會論文 3 3 30% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 6 6 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 2 20%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 10% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
