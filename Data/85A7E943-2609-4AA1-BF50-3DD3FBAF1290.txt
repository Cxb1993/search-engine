  
 
Figure 2.  Hardware architecture of our experimental robot. 
 
Fig. 2 shows the hardware architecture of our experimental 
robot. In the ultrasonic sensors system, the BASIC Stamp 2 
microprocessor can process the signals of ultrasonic sensors. 
Once the signals have been processed, we can receive the 
distance data from the PC terminal via RS-232. In the cameras 
system, the images are captured through the USB 2.0 video 
grabber. After processing the captured images and receiving 
the distance data from ultrasonic sensors, the program will                
send commands through RS-232 to the motors system. The 
BASIC Stamp 2 microprocessor can control the Polulu 30A 
high power DC motor driver, and then the left and right 
motors of the front wheels are driven. 
 
III. FACE TRACKING 
On the whole, our detection method can effectively find 
human face regions, but it costs quite a few computations [4]. 
To alleviate the computational load, we exploit a detect-then-
track approach which need not process the detection procedure 
on an entire image at each time step while the face and hands 
have been detected. The goal of tracking is that once the 
system has detected the moving objects, it keeps analyzing 
their movements and trajectories to continuously get the 
locations in real time. During tracking the face, some crucial 
strategies are used in the tracking algorithm. Due to the 
limitation to the length of writing, how to describe and 
discover the moving objects efficiently is not presented in this 
report. 
Herein, we will elaborate how to utilize the particle filter to 
implement face tracking. At first, we introduce our face 
tracker whose system architecture as the right part of Fig. 3 
shows. The face tracker consists of three main procedures: 
preliminary processing, particle filtering, and robot controlling. 
 
 
 
Figure 3.  The flowchart of our face tracker (See the right part). 
 
  
important to set the appropriate number of particles, say totally 
selecting N particles. At the beginning, we draw the particles 
with the weights greater than or equal to a pre-defined 
threshold Tw, and sum up these weights. We should set Tw 
properly when the resulting particles distribute in too many 
repeated regions. The remaining particles with lower weights 
are set to be zero as shown in (7). 
 
( ) ( )
( )       if 
,   1,2, ,
 0         otherwise.
pi pi
pi
 ≥
= =

K
i i
i t t w
t
T i N                        (7) 
 
Then we employ (8) to determine how many particles with the 
same weight ( )itpi  should be duplicated from the i-th particle. 
 
( )
( ) pi 
=  
 
i
i t
t
t
K round N
W
   with ( )
1 .pi== ∑
N i
t tiW                   (8) 
 
By use of a predefined threshold in the re-sampling step, 
our tracking system surmounts a degeneracy problem. That is, 
after a few iterations, all but one particle will have a negligible 
weight. This degeneracy problem implies that a large 
computational effort is devoted to updating particles whose 
contribution is almost zero.  
 
5) Estimate: Assuming that only one target is desired to track 
continuously, we choose the particle with the highest weight to 
be the predicted result. To summarize, we outline an iteration 
procedure of the particle filter as Fig. 4 states. Except the 
initial iteration, N particles with larger weights are 
systematically selected from the previous iteration.  
 
1 Initialization 
Given a particle set 
( ){ }( ) ( )1 1 1, 1,2,...,pi− − −= =i it t tS s i N , a target 
1 1 1( , )pi⊗ ⊗− − −∈t t ts S ,  
and its velocities 
-1  tx
⊗
& and 
-1
⊗
&ty  at time step 
t − 1, perform the following steps: 
2 Propagation 
Fluctuate each particle ( )1
i
ts −  with the weight 
( )
1
i
tpi −  by virtue of a dynamic model to obtain 
the particle set tS . 
3 Observation 
Weigh the particles using both color and 
motion cues as:  
(1) Calculate the distance between each 
particle and the target by 
( ) ( )1= −i it tDis M  , 
where ( )itM  is a linear combination of 
color and motion cues. 
 
(2) Weigh each particle with     
( )2
( )
2exp( ) 22pi piσσ= −
i
i t
t
Dis
 , 
where σ  is the standard deviation of a 
Gaussian distribution. 
 
4 Selection 
Check the weights of particles.  
IF   the particle ( ( )its , ( )itpi ) with ( )itpi  wT≥    
 Duplicate ( )itK  even-weighted 
particles from this particle.  
ELSE 
Ignore the particle. 
ENDIF 
5 Estimate 
Acquire the new target ts⊗  with the largest 
weight as: 
      
*( ) *( )
  for  arg  max{ }pi pi pi⊗ = =j it t tij ,  its 
velocities 1 
⊗ ⊗ ⊗
−
= −&t t tx x x , and 
1.
⊗ ⊗ ⊗
−
= −&t t ty y y  
Figure 4.  An iteration procedure of the particle filter. 
 
 
B. Robot Control 
We can predict the newest possible position of a human 
face by means of the particle filtering technique. According to 
the possible position of a human face, we issue a control 
command to make the robot track the human face 
continuously. Such robot control comprises two parts: PTZ 
camera control and wheel motor control. The following is the 
command set of robot control at time step t  including the PTZ 
camera and wheel motor control. 
 
{ }, , , , , , .=
tCommand t t t t t t tS up down forward stop backward left right    (9) 
 
1) PTZ camera control: The main task is to keep the human 
face appearing in the center of the screen. It prevents the target 
from situating on the high or low side of the screen to leave 
the scope. If ty  is lower than or equal to 50, then we assign 
the command tup  to tilt the PTZ camera upward at time step t; 
if ty  is greater than or equal to 190, then we assign the 
command tdown  to tilt the PTZ camera downward at time 
step t as shown in (10). 
 
              if  50
            if  190
no-operation otherwise,
≤

= ≥


t t
t t t
up y
Command down y                      (10) 
where ty  is the center of the elliptic window in the y-direction 
at time step t. 
 
2) Wheel motor control: The main task is to control the robot 
to track the human face continuously.  We utilize two kinds of  
information  to judge  what  command  is  issued.  The first 
information is the position of the human face appearing on the 
screen.  According to this, we assign the commands as shown 
in (11) to drive the wheel motors to make the robot move 
intentionally. If xt is lower than or equal to 100, then we assign 
the command leftt to drive the wheel motors at time step t such 
that the robot moves leftward; if xt is greater than or equal to 
220, then we assign the command rightt to drive the wheel 
motors at time step t such that the robot moves rightward. 
  
process captured from the image sequence of Video B-1. 
Notice that there are some skin-colored objects in the 
background around the head, but our algorithm can still track 
it correctly because the motion cue is employed in the particle 
filter. 
 
3) Type C (the temporal occlusion): It will increase the 
tracking difficulty if there is an object in front of the target. 
After the object moves away, the particles are usually 
predicted back to the location of a human face. Three video 
samples containing the scenarios of temporal occlusion are 
tested for verifying the effectiveness of our face tracking 
method. TABLE III lists the resulting performance of these 
samples. Fig. 7 demonstrates part of the tracking process 
captured from the image sequence of Video C-1. 
 
TABLE II. THE FACE TRACKING PERFORMANCE OF TYPE B SAMPLES 
Sample 
Measurement 
Average cm (%) 
cf (%) dm (%) Worst Average Best 
Video B-1 2.21 1.50 95.67 96.47 97.26 
Video B-2 1.83 1.68 95.72 96.52 97.32 
Video B-3 2.38 1.76 95.21 95.99 96.77 
 
TABLE III. THE FACE TRACKING PERFORMANCE OF TYPE C SAMPLES 
Sample 
Measurement 
Average cm (%) 
cf (%) dm (%) Worst Average Best 
Video C-1 6.42 3.09 89.57 90.31 91.05 
Video C-2 7.61 3.43 87.74 88.51 89.27 
Video C-3 6.18 5.27 88.63 89.11 89.58 
 
V. CONCLUSIONS 
In this report, we have presented an automatic real-time 
face tracking system installed on a robot, which is able to 
locate human faces in image sequences from the robot situated 
in real environments. Such a system not only allows robots to 
interact with human being properly, but also can make robots 
react more like mankind. The particle filter adopted in our face 
tracking procedure can use fewer samples to maintain multiple 
hypotheses than conventional particle filters do, so the 
computational cost is greatly decreased. When applying the 
particle filter to carry out face tracking, a manner based on the 
fusion of color and motion cues is exploited to reliably 
measure the likelihood of each sample. The skin and hair color 
models are adapted to deal with color variation during the face 
tracking. The purpose of our system is to capture an image 
sequence from a PTZ camera in real time, then track a target 
and guide the robot near to the target continuously. 
Experimental results reveal that the face tracking rate is more 
than 95% in general situations and over 88% when the face 
suffers from temporal occlusion. 
 
        
Frame # 70                                           Frame # 85 
 
         
 Frame # 100                                        Frame # 115 
 
Figure 6.  Part of the tracking results from Video B-1. 
 
         
Frame # 20                                           Frame # 30 
 
        
Frame # 40                                            Frame # 50 
 
Figure 7.  Part of the tracking results from Video C-1. 
  
 
REFERENCES 
[1] G. L. Foresti, C. Micheloni, L. Snidaro, and C. Marchiol, “Face 
detection for visual surveillance,” in Proceedings of the 12th IEEE 
International Conference on Image Analysis and Processing, 2003, 
Mantova, Italy,  pp.115-120. 
[2] H. Zhao and R. Shibasaki, “A real-time system for monitoring 
pedestrians,” Application of Computer Vision, vol. 1, pp. 378-385, 2005. 
[3] K. C. Fan, Y. K. Wang, and B. F. Chen, “Introduction to tracking 
algorithms,”  Image and Recognition, vol. 8, no. 4, pp. 17-30, 2002.   
[4] Y. T. Lin, “Real-time visual face tracking and recognition techniques 
used for the interaction between humans and robots,” Master Thesis, 
Department of Computer Science and Information Engineering, 
National Taiwan University of Science and Technology, Taipei, Taiwan, 
2008.      
[5]   C. S. Fahn, M. J. Kuo, and K. Y. Wang, “Real-time face tracking and 
recognition based on particle filtering and AdaBoosting techniques,” in 
Proceedings of the 13th International Conference on Human-Computer 
Interaction, 2009, San Diego, California, pp.198-207. 
 
 
 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：范欽雄 計畫編號：99-2221-E-011-132- 
計畫名稱：用於人與機器人之間互動的即時視覺人臉追蹤與辨識技術 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 2 2 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 2 100%  
博士生 0 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 2 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 3 100%  專利 已獲得件數 0 0 100% 件  
件數 0 1 100% 件  
技術移轉 
權利金 0 1000 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
