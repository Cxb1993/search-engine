一種結合物件偵測之多階段前景影像抽取方法 2/ 12 
環境的建構」計畫，經過二年多的努力，已累積許多技術和技轉的成果。 
所謂智慧型視覺監控系统就是利用電腦視覺的方法，在不需要人力干預的情况下，對攝
影機拍攝的影像進行自動分析，進而對目標物進行追蹤、識别和行為分析。在分析過程中，不
論最後的目標是進行追蹤或辨識，首先必須將前景物件影像與背景影像先作出區別，之後才能
針對前景物件影像作進一步的處理(如物件辨別和行為分析等)。因此，前景/背景物體影像分離
即成為一項非常關鍵的處理技術。其正確性將密切且絕對的影響後續的分析結果。若被抽取出
的前景物件影像嚴重破碎成好幾部分，則無論接下來是進行物件追蹤或物件辨識，都將因物件
資訊不完整而極易得到錯誤的處理結果。有鑑於此技術的重要性，中華民國影像處理與圖形識
別學會(IPPR)於2006年首度舉辦技術競賽，題目為「運動物體偵測技術」，也就是本研究所探
討的「前景/背景分離技術」。希望藉由此競賽，能增進大家對此議題的認識瞭解與相互交流，
更而鼓勵更多研究力量的投入，以激發出新的解決方法，使得在監控產業蓬勃發展的今天，能
早日開發出實用的智慧監控系統。 
 
二. 研究目的： 
如何在一連串的視訊資料流中，判斷哪些影像點是屬於前景物件而哪些影像點是
屬於背景畫面是一項非常重要且關鍵的技術。其處理結果的好壞將密切的影響視覺監控
系統之視訊分析的正確和實用性。有鑑於現今技術所面臨的技術瓶頸，本計畫提出一種
新穎性的前景物件影像抽取方法，以更豐富的影像資訊來得到更正確與完整的前景物件
影像抽取結果。本計畫技術有望能帶動視覺監控產業相關應用系統的發展。 
本發明將傳統的前景/背景微觀處理與巨觀的物件偵測處理進行巧妙的結合，大大
提升了所抽取的前景影像之正確性和完整性。本計畫技術方法容易實施，具有高度實用
性。 
 
三. 文獻探討： 
一般而言，分離前景物體與背景環境的方法主要可區分成三種類型：背景相減法
(Background Subtraction)[1,5,19,28,29]、時間序列畫面差異法(Temporal Differencing)[30]和光流
場法(Optical Flow)[31,32,33,34]。背景相減法分辨出目前影像與所設定的背景模型之差異，為目
前最普遍被應用於分離前景的方法。背景模型中每個影像點經常採用單一高斯模型或者是混合
式高斯模型(Gaussian Mixture Model)來表達。此方法簡單容易應用，但是卻容易由於一些背景
的變化而無法分離出正確的前景，像是光源的變化或是出現後靜止的物體。時間序列畫面差異
法則是利用前後不同時段所拍攝的影像畫面來進行直接相減，對任何影像點而言，只要此差異
量的絕對值大於所設定的臨截值，則它將被判斷為一前景物件影像點；否則，它將被判斷為一
背景影像點。至於光流場法會對移動的物體之每個影像點計算出在二個影像畫面中的位移量，
當有前景物件進入攝影中的畫面時則會引起光流場的變化。因此，利用光流場的大小即可判斷
移動物體的影像位置。一般而言，背景相減法和時間序列畫面差異法的執行速度相當快速，單
用軟體方式即可以達到即時處理的目的；光流場法則因計算複雜(每個影像點需做區域性的比對
搜尋)，以致若無硬體的配合，則無法滿足即時處理的要求。背景相減法常會因為前景物件影像
和背景影像之顏色相近，造成所抽取的前景物件影像相當破碎。另外，陰影亦常會被判斷成為
一種結合物件偵測之多階段前景影像抽取方法 4/ 12 
[11] L. Lee. R. Romano, G. Stein, “Monitoring Activities from Multiple Video Streams: Establishing a 
Common Coordinate Frames,” IEEE Trans. on PAMI, pp.758-768, Aug. 2000. 
[12] V. Kettnaker, R. Zabih, “Bayesian Multiple Camera Surveillance”, Proceeding of Computer Vision 
and Pattern Recognition, For Collins, CO, pp.253-259, June 1999. 
[13] H. Pasula, et al., “Tracking Many Objects with Many Sensors,” In Proc. IJCAI-99, Stockholm, 1999. 
[14] O. Javed, et al., “Tracking Across Multiple Cameras with Disjoint Views”, Proceedings of the Ninth 
IEEE International Conference on Computer Vision, pp. 1-6, 2003. 
[15] S. Khan, et al., “Human Tracking in Multiple Camera”, Proceedings of the Ninth IEEE International 
Conference on Computer Vision, pp. 1-6, 2002.       
[16] J. Barron, D. Fleet, and S. Beauchemin, “Performance of optical flow techniques”, International 
Journal of Computer Vision, vol. 12, no. 1, pp. 42-77, 1994. 
[17] M. ~Isard and ~A. Blake, “Contour Tracking by Stochastic Propagation of Condition Density”, In Proc. 
Of the 1996 European Conference on Computer Vision, pp. 343-356, 1996. 
[18] Y. Ricquebourg and P. Bouthemy, “Real-Time Tracking Persons by Exploiting Spatio-Temporal 
Image Slices”, IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 22, no. 8, pp. 792-808, 2000 
[19] C. Stauffer and W. E. L. Grimson, “Learning Patterns of Activity Using Real-Time Tracking”, IEEE 
Trans. Pattern Analysis and Machine Intelligence, vol. 22, no. 8, pp. 747-757, 2000 
[20] F. Oberti, A. Teschioni, and C. S. Regazzoni, “ROC curves for performance evaluation of video 
sequences processing systems for surveillance applications”, Proc. 1999, pp. 949-953. 
[21] L. Marcenaro, F. Oberti, and C. S. Regazzoni, “Change detection methods for automatic scene analysis 
by using mobile surveillance cameras”, Proc. 2000, pp. 244-247. 
[22] L. Marcenaro, C. S. Regazzoni and G. Vernazza, “Automatic generation of the statistical model of a 
non-rigid object in a multiple-camera environment”, Proc. 2000, pp. 530-533. 
[23] R. T. Collins, A. J. Lipton, T. Kanade, H. Fujiyoshi, D. Duggins, Y. Tsin.” A System for Video 
Surveillance and Monitoring.” Tech. Rep. The Robotics Institute, Carnegie Mellon University, 2000. 
CMU-RI-TR-00-12. 
[24] I. Haritaoglu, D. Harwood and L. S. Davis.’’W4: Who? When? Where? What? A Real-Time System 
for Detecting and Tracking People.’’ Proc. International Conference on Face and Gesture Recognotion, 
April, pp. 14-16, 1998. 
[25] C. R. Wren, A. Azarbayejani, T. Darrell, and A. P. Pentland, “Pfinder: Real-Time Tracking of the 
Human Body.” IEEE Trans. On Pattern Analysis and Machine Intelligence, Vol. 19, no. 7, pp. 780-785, 
1997. 
[26] C. Stauffer and W. Grimson, “Adaptive Background Mixture Models for Real-Time Tracking”, IEEE 
Computer Society Conference on Computer Vision and Pattern Recognition, vol. 2, pp. 246-252, 1999. 
[27] P. KaewTraKulPong and R. Bowden, “An Improved Adaptive Background Mixture Model for Real-. 
time Tracking with Shadow Detection”, 2nd Eurpoean Workshop on Advanced Video-Based Surveillance 
Systems, 2001.  
[28] Z. Zivkovic and F. Heijden, “Efficient Adaptive Density Estimation per Image Pixel for the Task of 
Kackground Subtraction,” Pattern Recognition Letters, vol. 27, pp773-780, 2006. 
[29] S. C. Cheung and C. Kamath, “Robust Background Subtraction with Foreground Validation for Urban 
Traffic Video,” EURASIP Journal on Applied Signal Processing, vol.14, pp. 2330-2340, 2005. 
[30] C. Anderson, P. Burt, and G. V. D. Wal, “Change Detection and Tracking Using Pyramid 
Transformation Techniques, “ In Proc. of SPIE Intelligent Robics and Computer Vision, Vol. 579, pp. 
72-78, 1985. 
[31] Y. Altunbasak, P. E. Eren and A. M. Tekalp, “Region-Based Parametric Motion Segmentation Using 
Color Information”, Graphical Models and Image Processing: GMIP, 60(1), pp. 13-23, 1998. 
[32] J. G. Choi, S. W. Lee and S.D. Kim, “Spatio-Temporal Video Segmentation Using a Joint Similarity 
Measure”, IEEE Transcations on Circuits and Systems for Video Technology, 7(2), pp. 279-286, 1997. 
[33] F. Dufax, F. Moscheni and A. Lippman, “Spatio-Temporal Segmentation Based on Motion and Static 
Segmentation,” IEEE Conference on Image Processing, pp. 306-309, 1995. 
[34] Y. Tsaig and A. Averbuch, “Automatic Segmentation of Moving Objects in Video Sequences: A 
Region Labeling Approach,” IEEE Transcations on Circuits and Systems for Video Technology, 3(5), pp. 
597-612, 2002. 
[35] G. J. Brostow and R. Cipolla, “Unsupervised Bayesian Detection of Independent Motion in Crowds,” 
IEEE Computer Society Conference on Computer Vision and Pattern Recognition, vol. 1, pp. 594-601, 
2006. 
 
            圖一、一種用來實施本計畫的處理流程架構 
 
A. 畫面式前景影像偵測模組： 
本模組可採用任何一種常用的前景影像偵測演算法(例如：背景相減或是畫面差異法
等)。本模組主要目的是用來指出在整張影像中有哪些部分是屬於較明顯的前景影像，
因此可選用較嚴謹的參數，並且亦可與其它雜訊處理 (如陰影判斷和形態學運算等處理)
搭配，使得大部分的雜訊和陰影都不會被誤判為前景影像點。在此，我們採用最常見的
背景相減法。假設 I(P)代表影像中第 P 點的特徵訊號值(如顏色或灰度值)，B(P)為 P 點
的背景模型特徵訊號值，則前景/背景的判斷方程式 D(P)為 
一種結合物件偵
⎨⎧ >=                                  others. ,0
thresh;|)| when ,1
)(
I(P)-B(P
PD
∑ ∑
≤≤ ≤≤
=
xa yb
bafyxII
1 1
),(),(
.0)
;0)0,(
);,(),1(),(
);,()1,(),(
=
=
+−=
⎩  ...................................(1) 
其中，thresh 為一個預設的差異臨界值。此方程式主要說明當 I(P)和 B(P)的特徵訊號差
異夠大時，影像點 P 會被判斷為前景點(即 D(P)=1)；反之，當 I(P)和 B(P)的特徵訊號差
異不夠大時，影像點 P 則會被判斷為背景點(即 D(P)=0)。當然，當 D(P)為 1 時，可再
用陰影判斷方式來判別點 P 是否屬於陰影狀態，當點 P 被認為是在陰影狀態時，則點 P
則會被更改成為背景點(即 D(P)改設為 0)。 
 
B. 區域掃描模組： 
本模組採用某種特殊形狀的影像區域以由左至右由上到下的方式來逐一掃描整張影
像，當某一區域影像中擁有足夠的前景影像訊號時，則代表此區域影像有可能存在某種
前景物件，因而需要將此區域影像進一步的用物件偵測模組來判斷是否有真正有物件的
存在。當一區域影像中並沒有夠量的前景影像時，則此區域影像會被認為並不存在前景
物件，因此被忽略而再去掃描下一個區域影像。圖二顯示一種可以用來快速計算矩形區
域內所有點之訊號總和的方法，它首先計算此影像的集成影像(Integral Image)。假設 f(x,y)
是一 M×N 影像平面中座標為(x,y)之影像點的訊號值，其中 1≤x≤M 而且 1≤y≤N；II(x,y)
為此影像中以(1,1)點為左上角而點(x,y)為右下角之矩形內所有影像點的訊號總和，也就
是 。利用迭帶式的計算，II(x,y)可以被快速地計算如下： 
,0(
− +=
y
xs
yxsyxIIyxII
yxfyxsyxs
)4( )1(
II
 
在得到集成影像之後，若要計算矩形 D 之訊號總和，則只要先將點 4 之集成影像的對
應值(即Π )加上點 1 之集成影像的對應值(即Π )，然後再減去點 2 和點 3 之集成影
像的對應值(即Π 和 )即可，也就是矩形 D 的訊號總和=
測之多階段前景影像抽取方法 6/ 12 
)2( )3(Π )4(Π + )1(Π - )2(Π - )3(Π 。由
圖二的圖示可清楚的了解上述計算的基本原理。 
 
 
 
 
 
 
 
 
 
 
圖四、人形區域不同位置的影像可對應不同的差異臨界值之示意圖，例如中間白色矩形
之內部影像較此矩形之外部影像可使用較小的差異臨界值。 
 
另外，當已知某一區域內有存在物件時，為了讓所抽取的物件較完整，可選用的差異臨
界值較畫面式前景影像偵測模組所選的值為小。而且，為了執行速度的考量，在具有物
件訊號之區域的影像點中，若是某一影像點已於畫面式前景影像偵測時被判斷是前景影
像，則此點不需要於本模組中再進行前景/背景的判斷。 
 
一種結合物件偵測之多階段前景影像抽取方法 8/ 12 
E. 綜合： 
註解 [A1]:  
本模組是將畫面式前景影像偵測模組和區域式前景影像偵測模組所各自找到的前景影
像點綜合起來產生最終的輸出結果。在此，我們用最簡單的”OR”運算來完成，那就是
在畫面式和區域式前景影像偵測處理時，只要在任何一種偵測模組中被判斷為前景影像
點，則此點即被認定是前景影像點。 
 
F. 背景模型更新模組： 
當選用背景相減法來判斷前景影像點時，如何保持即時更新和有效的背景模型是一個非
常重要的議題。因為不良的背景模型是不可能產生滿意的前景影像偵測結果。由於偵測
過程中可以得到許多方面的判斷資訊(如存在物件的影像區域、前景影像點、背景影像
點等)，因此我們的背景模型更新能更多樣且有效，例如 
⎪⎩
⎪⎨
⎧
+−
+−
+−
=
                                                     P,F(P) B(P) )1(
   PP,F(P) B(P) )1(
PP,F(P)B(P) )1(
B(P)
 
被判斷為前景影像點當
區域內存在於具有物件的影像被判斷為背景點且當
像區域內不存在於具有物件的影被判斷為背景點且當
γγ
ββ
αα
(2) 
其中，B(P)為影像點 P 的背景模型，F(P)為輸入畫面之影像點 P 的特徵值，而且α>β>γ。
有些時候，雖然真正是屬於物件的前景影像點，可能會因其訊號特徵與背景模型相似而
被錯誤地判斷為背景點，我們的背景更新模組就會因為它是存在於具有物件的影像區域
內而能不急著去更新它，這使得我們所建構出的背景模型能與前景影像具有更大分辨的
能力。 
 
若程式所產生的剪影圖片以 O(x,y)表示，人工所產生的剪影圖片以 S(x,y)表示。其中，
剪影部份以”1”表示，背景部份以”0”表示，剪影輪廓邊緣區域以 R 表示，而 I 是整張影
像，則評分方式定義為 
∑
∉∈
⊕=
Ry)(x, and ),(
),(),(*
)( Iyx
yxSyxO
IN
1E  .....................................(3) 
其中，N(I)為影像中所有不屬於 R 的影像點數。所以 E 的值是越小越好。 
 
實驗是在 Pentium Ⅳ 2.0 GHz with 512M RAM 的桌上型電腦上執行。除了 E 值的比較
數據外，執行的速度亦被列出為參考數值。表一為利用背景相減法以及本計畫方法所得
到的結果，其中背景相減法是以方程式(1)來判別前景與背景影像點，而所選用的參數
thresh(=40)是對應到最低的 E 值(0.005657)。 
     
Method E sec/320*240 image sec/160*120 image
Background 
Subtraction 
0.00771 180ms 50ms 
The Proposed 
method 
0.00362 304ms 78ms 
 
      表一、背景相減法以及本計畫方法所得到的前景影像抽取效能比較 
 
圖七即為一視訊影片中三張影像的前景抽取實驗的結果，其中圖七(a)為輸入視訊影像
和自動偵測到的人形矩形外框，圖七(b)為傳統前景影像抽取結果，圖七(c)為本計畫的
前景影像抽取結果。圖八為另一段視訊影片中三張影像的前景抽取實驗的結果。以上之
實驗數據和抽取影像之顯示結果清楚展示本計畫具有抽取較正確與完整的前景影像的
能力。 
一種結合物件偵測之多階段前景影像抽取方法 10/ 12 
 註解 [A2]:  
註解 [A3]:   
 
 
 
 
 
 
 
 
 
 
 
 
      圖七、一視訊影片中三張影像的前景抽取實驗的結果 
一種結合物件偵測之多階段前景影像抽取方法 12/ 12 
一套能更正確和更完整的抽取前景物件影像的方法，已達成當初所預期的計畫
目標。本計畫的研究成果除了已發表在國際性會議論文外，我們將會繼續做更
完整了實驗，希望能將計畫成果往學術期刊發表。 
a set of its selectively neighboring pixels, a texture-like 
feature can be derived which is called the local image 
structure (LIS) of this pixel. Apparently, LIS is a useful 
image feature description. This section will first introduce 
LIS in detail, and an effective matching algorithm based 
on LIS will also be presented.  
 
2.1. Local Image Structure 
 
Let x be an image pixel, I(x), R(x) and L(x) be 
individually the image intensity, the reflectance vector 
and the illumination vector of x. From the imaging optics, 
it exists I(x) = R(x).L(x). This equation describes the 
image intensity is just the product of image reflectance 
and its illumination vectors. For another image pixel y, it 
becomes I(y) = R(y).L(y). Therefore,  
                           ( ) ( ) ( )
( ) ( ) ( )
I x R x L x
I y R y L y
⋅= ⋅ .                        (1) 
Suppose pixels x and y are neighbors, being close to each 
other, so L(x) and L(y) are assumed to be similar or even 
the same. Then 
                                ( ) ( )
( ) ( )
I x R x
I y R y
≈ .                            (2) 
This equation shows that the ratio of two neighboring 
pixels is almost independent of illumination vector, and it 
approximately keeps constant if only the background 
illumination changes and all other factors remain 
unchanged. However, besides illumination there are other 
factors (such as surface normal and noises) which can 
affect the image intensity. Therefore, it is not proper to 
directly take the image ratio as features. Instead the 
contrast relationship (larger than or not larger than) is a 
better and more stable feature description in representing 
the relation among image pixels. For any two pixels x and 
y, a contrast relationship is defined as  
                 (3) 
0, if ;
( )
1, otherwise.            
I(x)  I(y)
I(x), I(y)ζ ≥⎧= ⎨⎩
 
N
The relationship obtains a value 0 if pixel x is not darker 
than pixel y, otherwise it obtains a value 1. Let Φ(x) = {P0, 
P1, …, PN} be an N-element set and each element 
correspond to a chosen neighboring pixel of x. This set is 
called interestedly selected pixel set, which specifies how 
many and what pixels are chosen to derive the local 
image structure. It is not difficult to realize that by 
integrating a set of relationship ( ), 1 nnI(x), I(p )ζ ≤ ≤  
among pixel x and its neighboring pixels (P1, …, and PN) 
a structure-like information can be constructed. In fact, 
the structure-like information semantically represent one 
kind of image texture information. With pixel 
relationships, the local image structure ( )xΓ  is designed 
as 
                          (4) n
0, ( )
( ) 2 ( (x), (p ))
i
N
n
n p x
x Iζ
= ∈Φ
Γ = ×∑ I
Figure 1 is an example of Φ(x) which uses 8 neighbors 
of x to construct its local structure. Obviously, with this 
configuration the corresponding ( )xΓ has in total 256 
possibilities ranging from 0 to 255, and each possibility 
corresponds to one specific image structure. For 
example, when ( )xΓ  is 0, it means that x is the brightest 
pixel among it’s 8 neighbors. For another example, 
when ( )xΓ is 7, it means that among its 8 neighbors 
only P0, P1 and P2 are darker than x. This structure 
representation is very robust to many kinds of variations, 
especially the illumination variation. Figure 2 shows an 
illustrative example of the local structure to the images 
under various illuminations, which is visualized as 
index image where the structure index determines the 
pixel intensity. Figures 2(b)-(e) are generated from 
Figure 2(a) by using an image processing software with 
the following parameters: light value −75, contrast 
value −50, gamma value 2.0, and gamma value 0.4, 
respectively. It reveals that the designed image structure 
is not affected by the illumination variation. 
 
P3 P2 P1 
P4  X P0 
P5 P6 P7 
 
Figure 1: One example of the interestedly selected pixel 
set Φ(X), where 8 neighbors of X, P0, P1, P2, P3, P4, P5, 
P6 and P7, are chosen to construct the local image 
structure of pixel X. 
 
Figure 2: Images with different  illumination are shown in 
the first row, and the corresponding images of derived 
local structures are shown in the second row. 
 
2.2. Extraction by Using Image Intensity and 
Structure  
 
 
Besides image structure information, image color can be 
regarded as the most important image feature. Therefore, 
it is really appropriate to use both color and structure 
information to extract foreground objects. For color, the 
background model can be simply constructed by using a 
mixture Gaussian model (GMM) from a set of collected 
training background images. Let f denote the color value 
of pixel x, λ denote the constructed color background 
model, i.e.  where C is the 
total number of mixture models, and pi, ui and 
{ }, , ,   1, 2,...,i i ip u i Cλ = ∑ =
iΣ  are the 
weight, mean and covariance variance of mixture model i 
individually. The GMM probability ( | )p f λ  that pixel x 
belongs to background becomes  
1
1 22
1
1( | ) exp ( ) ' ( )
2(2 )
C
i
i i iN
u i
pp f f xλ π
−
=
⎧ ⎫= − − Σ⎨ ⎭Σ ⎩∑ μ μ− ⎬
   (8) 
In order to find out the extraction ability of the 
proposed method, two experiments were performed. In 
the first, videos are taken from an office site and in total 
there are 350 image frames. Among them, the first 140 
frames contain only pure background scene. To construct 
the respective background models of image color and 
image structure, the first 120 image frames are used in the 
training stage. For the color model, a single Gaussian 
model is adopted. For the structure model, three 
assignment sets, Φ1, Φ2 and Φ3, are selected to generate 
three local image structures. The used Φ1, Φ2 and Φ3 are 
shown in the Figure 3. Therefore, Φ1, Φ2 and Φ3 contain 
the same number of elements, and each has 8 pixel 
elements. So, each local structure  can be represented 
effectively with just one byte. Because the pixels of Φ1 
are closer to the kernel x than those of Φ2 and Φ3, the 
value of Φ1 will be more representative for image 
structure than those of Φ2 and Φ3. As a result, their 
contribution weights will be set differently. In this 
experiment, w1, w2 and w3 are set to be 0.4, 0.3 and 0.3 
respectively. To speed up the process and reduce the ill 
effect of rarely appeared structures, Equation (6) are used 
to compute  and thresh1 is set to 0.1. Finally, 
the combination weights wc and ws, and the decision 
threshold T are set to 0.4, 0.6 and 0.6 respectively. Figure 
4 shows the extracted results with different foreground 
extraction approaches which contain color-only 
background subtraction, temporal difference, structure-
only background subtraction, and the proposed approach. 
Visually, extraction by temporal difference is usually 
broken and the extracted image mainly focus on both 
object boundary and image edges. Although both color-
only and structure-only background subtraction 
approaches obtain better results, but they still contain a 
few broken areas inside the foreground objects. 
Interestingly, the broken areas of the two approaches are 
seldom coincided with each other. This phenomenon 
displays color and structure information are mutually 
independent so that their extraction results have a 
complementary effect. This results in that to combine 
both of them can produces much complete and correct 
extracted objects as shown in Figure 4(f). Figure 5 shows 
the second experiment with videos taken from outdoor 
hallway. In this video, there is one person who 
approaches the picture taken camera and another person 
who lingers around at a distance. Again, extraction by 
using both color and structure information produces the 
best result. From the above two experiments, the 
effectiveness of the proposed approach has been clearly 
demonstrated. 
miΓ
( , )m mG S tCombining Equations (5) and (8), an integrated likelihood LK(f,T|λ, S1,…, Sm) indicating x to be background 
becomes  
1 1
1
LK(  ) * | ) * 1
M
m m
M M c s m
m m
G(S ,t )f,t , ,t | , S , , S  w p(f w w * ( )
N
λ λ
=
= + −∑L L
(9) 
where   f  is the color value of x, 
   t1,…,tM are the M structure values of x, 
          S1,…,SM are the M sets of structure statistics of x, 
   λ is the color background GMM model, 
   Nm is the element number of Φm, 
   wm is the contribution weight of Φm, 
          wc is the combination weight of color information, 
   ws is the combination weight of structure  
       information. 
 
With a suitable threshold T, the decision of x becomes 
1 10 , when  T;D(x)
1 ,  otherwise.                                             
M mLK(f,t , ,t | λ, S , , S )… ≥⎧= ⎨⎩
L
   
(10) 
When D(x)=0, x is assigned to be a background pixel, and 
when D(x)=1, then x is assigned to be a foreground pixel.  
 
3. Experiments 
出席國際學術會議心得報告 
                                                             
計畫編號 NSC 95-2218-E-216-004 
計畫名稱 物件式前景物影像抽取技術 
出國人員姓名 
服務機關及職稱 
黃雅軒   
中華大學 資訊工程系  副教授 
會議時間地點 2007/7/9~2007/7/12, 美國弗羅里達州的奧蘭多市 
會議名稱 International Conference on Artificial Intelligence and Pattern Recognition) 
發表論文題目 Local Structure Based Foreground Object Extraction 
 
一、參加會議經過 
 
此會議舉辦時間為從 2007/7/9至 2007/7/12，由於 2007/6/25~2007/6/28我在拉斯維加斯參
加另一個會議，因此在洛杉磯停留大約十天後，再於 7/8日前往奧蘭多市。除了聽取國
際其他研究機構在圖形識別領域的研發成果外，我也在 7/10的下午發表我們的論文
「Local Structure Based Foreground Object Extraction」。 
 
 
二、與會心得 
 
AIPR (International Conference on Artificial Intelligence and Pattern Recognition)為電腦視
覺領域一個新的國際性學術會議，著重新穎性的技術和應用的發表，目的為展示此領域技術
的發展趨勢。為了進行跨領域技術的分享和整合，AIPR和其他 3個會議於 7/9至 7/12在美國
弗羅里達州的奧蘭多市(Orlando) 同時舉行，這 3個會議分別為 
1. International Conference on High Performance Computing, Networking and 
Communication Systems 
2. International Conference on Automation, Robotics and Control Systems 
3. International Conference on Bioinformatics, Computational Biology, Genomics and 
Chemoinformatics 
 
第一屆 AIPR論文投稿相當踴躍，論文接受率只有 30%，而我們的論文 Local Structure 
Based Foreground Object Extraction很榮幸的被接受。本論文提出一種新穎性的前景影像抽取
