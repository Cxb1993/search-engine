 1
行政院國家科學委員會補助專題研究計畫成果報告 
在機器學習中有效建構 MCS 以提升分類正確率的多種新方法之研究：樣本、
特徵、類別標記及三者整合等不同考量面向 
Novel Methods of Combining Multiple Classifiers: Instance, Feature, and 
Class Label 
計畫編號： NSC 96-2221-E-022-013 
執行期限： 96 年 8 月 1 日至 97 年 7 月 31 日 
主持人：黃淇竣副教授  國立高雄海洋科技大學資訊管理系 
計畫參與人員：蔡瑞鴻  國立高雄海洋科技大學資訊管理系 
吳家豪  國立高雄海洋科技大學資訊管理系 
謝銘凱  國立高雄海洋科技大學資訊管理系 
王思健  國立高雄海洋科技大學資訊管理系 
 
一、中文摘要 
 
現有的單一的分類方法各有其適合的
特定分類問題，亦擁有不錯的分類效能。
然而，實際上很難找到一個單一的分類方
法，對於所有的分類問題皆可獲得最佳的
分類正確率。組合多個分類器之輸出的作
法，則是嘗試利用多個分類器的不同分類
行為，提升該多分類器組合系統(Multiple 
Classifier System，MCS)的分類正確率及分
類決策判斷的可靠度。實驗研究指出，多
分類器組合系統的分類效能在大部份的情
況下能優於單一分類器的分類效能。 
基於此，本研究旨在提出在機器學習
中有效建構多分類器組合以提升分類正確
率的多種新方法，分別從操作樣本、操作
特徵、類別標記替換等多個面向考慮，有
效建構多種不同類型的多分類器組合，以
提升整體分類正確率。本研究所提出的多
種建構多分類器組合之新方法具有數項優
點，包括可在建構多分類器組合之前先進
行單一分類器的分類正確率或分類效能評
估，以及在建構過程中考量多分類器之間
的多樣性(Diversity)概念，同時亦融入有效
的多分類器合併機制，進而得以有效改善
或提升使用單一分類器所得到的分類正確
率。本計畫之相關研究成果已發表於國際
知名期刊與學術研討會，包括[32-35]等。 
 
關鍵詞：機器學習、多分類器組合、樣本
挑選、特徵挑選、類別標記替換 
 
Abstract 
 
In recent years, methods of combining 
multiple classifiers have gained great 
popularity in machine learning and pattern 
recognition.  The outputs of multiple 
classifiers are combined to deal with the 
problem that each individual classifier might 
be suitable only for some specific real-world 
classification problems.  As a result, the 
overall performance can be improved by 
considering different classification behaviors 
of multiple classifiers.   
In this research project, novel methods 
of combining multiple classifiers are 
proposed and investigated.  Each individual 
classifier in the classifier pool is constructed 
by using different instance selection schemes 
and by using different feature selection 
schemes.  In addition, each individual 
classifier in the classifier pool is constructed 
by switching the class labels of training 
instances.  Meanwhile, a combined 
approach to multiple classifier systems (MCS) 
is proposed by considering the viewpoints of 
instance selection, feature selection and class 
label switching.  In the proposed methods, 
the performance evaluation process of each 
individual classifier is performed (i.e., 
selection of classifiers) by using different 
performance criterions.  Meanwhile, the 
issues of diversities among different 
classifiers in the classifier pool are discussed.  
In addition, some effective classifier 
 3
樣地亦將提高建構 MCS 系統的困難度。 
針對一分類問題 P，MCS 系統中建構
多個不同的單一分類器有下列幾種作法： 
(A) 針對分類問題 P 的原始訓練集合
T，透過抽樣方法(Sampling Methods)或其
他方法從中產生多個不同的訓練集合，用
以建構多個不同的單一分類器。 
(B) 針對分類問題 P，透過不同的訓練
方法 (Training Methods)或訓練初始化步
驟，建構多個不同的單一分類器。 
(C) 針對分類問題 P 的特徵集合 F，
透過抽樣方法(Sampling Methods)或其他方
法產生多個不同的訓練集合，換言之，即
每個訓練集合具有不同的特徵子集合，用
以建構多個不同的單一分類器。 
(D) 合併不同的分類方法之分類決策
判斷，形成 MCS 系統。 
(E) 依據分類問題 P 本身的特性，假
設 P 擁有 n 個不同的資料來源，則這 n 個
資料來源可自然形成一個分類決策組合。 
以方法(A)為基礎的 MCS 系統，主要
有Bagging法[23]、Stacking法[24]、Boosting
法[15]等作法。在方法(C)中，每個單一的
分類器將比方法(A)或(B)更明確地具備獨
立的分類判斷資訊[22]。以此概念為基礎的
MCS 系統，如隨機特徵子空間法(Random 
Subspace Method)[17]。至於前述之方法(B)
及(E)，則分別取決於分類器本身的特性，
及分類問題的資料來源，而有不同的設計
考量。 
曾有專家學者提出針對訓練樣本的類
別標記進行替換(Switching)的動作[30]，其
主要目的在於試圖改變可能為雜訊樣本或
有較高機率產生分類錯誤的訓練樣本，其
值得關注的類別標記，以尋求提升分類正
確率的可能性。已有實驗研究證明[30]，有
效進行訓練樣本其類別標記的替換，確實
有助於提升單一分類器的分類正確率。 
在建構多個不同的單一分類器後，透
過特定的分類器合併機制 (Combination 
Scheme)將這些分類器的分類或決策判斷
整合，以作成最後的分類決策。現有的分
類器合併機制包括(1)平行法(Parallel)。(2)
串聯法(Cascading or Serial Combination)。
(3)階層法(Hierarchical or Tree-Like)[22]。 
在建構 MCS 系統之前，首先必須考量
建構 MCS 的所有單一分類器之分類正確
率。所有單一分類器的分類正確率被期望
為越高越好。另外，所有單一分類器被期
望會產生不同的分類錯誤，以提高分類器
之間的多樣性[21](Diversity)。因此，各單
一分類器之間的差異，將是 MCS 整體分類
決策好壞與否的重要影響因素與設計議題
[26]。也就是說，專注於討論，各單一分類
器之間，是具備了好(Good)的多樣性，或
是不好(Bad)的多樣性[25]。單一分類器之
間多樣性之測量方法主要可區分為兩類
[20]：(1)成對的測量方法(Pairwise diversity 
measures) 、 (2) 非 成 對 的 測 量 方 法
(Non-pairwise diversity measures)。 
綜括來說，本研究主要旨在提出在機
器學習中有效建構 MCS 以提升分類正確
率的多種新方法，其研究目的如下： 
(1) 研究及分析一個新穎且可作為建
構 MCS(以操作訓練樣本作為建構 MCS 的
主要考量面向)之基礎的訓練樣本間關係
結構，並從中研究建立有效的樣本挑選機
制，被挑選的訓練樣本所形成的訓練集合
將用以產生一單一的分類器。 
(2) 完整建構一個新穎的、有效的、
以操作訓練樣本作為主要考量面向的建構
MCS 新方法，進而提升整體分類效能。 
(3) 針對特定分類問題的特徵集合或
特徵空間，研究及分析建立有效的特徵挑
選機制，被挑選的特徵或屬性所形成的特
徵子集合將用以產生一單一的分類器。 
(4) 完整建構一個新穎的、有效的、以
操作樣本特徵作為主要考量面向的建構
MCS 新方法，進而提升整體分類效能。 
(5) 針對分類問題中訓練樣本的類別
標記，研究及分析建立有效的類別標記替
換機制，在進行訓練樣本的類別標記替換
後，所形成之新的訓練集合將用以產生一
單一的分類器。 
(6) 完整建構一個新穎的、有效的、
以進行訓練樣本的類別標記替換作為主要
考量面向的建構 MCS 新方法，進而提升整
體分類效能。 
(7)  提出一個新穎的單一分類器其分
類效能的評估基準，包括分類正確率及與
其他分類器之間的多樣性，以作為該分類
器是否納入建構 MCS 的重要考量因素。 
 5
本，或是容易產生分類錯誤的訓練樣本。 
考量這些不同類型的訓練樣本，得以
訂定多個樣本挑選機制，包括：(1)在灰樣
本空間中處於中心的訓練樣本或是關鍵
的、具代表性且有助於提升分類正確率的
訓練樣本將被挑選，所形成的訓練集合將
用以產生一單一分類器。(2)位處整個灰樣
本空間中邊界或不是任何訓練樣本之最近
鄰居的各個訓練樣本將不列入樣本挑選的
考慮。(3)雜訊樣本、處於灰樣本空間中不
同類別之間邊界的訓練樣本，或是容易產
生分類錯誤的訓練樣本亦將不列入考慮。 
此外，如研究目的所述，本計畫針對
特定分類問題的特徵集合，研究及分析建
立有效的特徵挑選機制，被挑選的特徵或
屬性所形成的特徵子集合將用以產生一單
一分類器，進而作為有效建構 MCS 之用。 
本研究所提出的特徵挑選機制，概念
上是特徵選取方法中的混合模型(Hybrid 
Model)[5]，亦即是混合採用過濾模型(Filter 
Model)[3]與封套模型(Wrapper Model)[4]，
以評估和挑選特徵子集合。換言之，評估
和挑選特徵子集合的過程乃是透過預先選
定好的分類方法，藉由檢視該特徵子集合
對應的分類效能以決定該特徵子集合的優
劣。分類效能較佳的特徵子集合將用以產
生單一的分類器，這些單一的分類器將作
為有效建構 MCS 之用。為此，本研究將搭
配選定一個分類方法，藉由檢視其對應的
分類效能以評估每一個特徵子集合的優
劣。 
相關性基準(Correlation Measure)、一
致性基準(Consistency Measure)、距離基準
(Distance Measure)及資訊基準(Information 
Measure)等特徵子集合評估基準，均與分類
效能無直接關聯。換句話說，利用這些基
準作為評估指標所得到之較佳特徵子集合
在未來實際進行分類的效能表現上往往不
一定是較佳的[1]。相較之下，本研究所提
出的評估基準(即採用基於灰關聯結構之
最近鄰居分類法或以樣本為基礎的學習法
則作為預先選定好用以評估特徵子集合的
分類方法)則是與分類效能直接相關，在進
行建構 MCS 之前，以該分類方法，針對每
一個產生(或候選)的特徵子集合一一作分
類效能評估，分類效能表現較佳的特徵子
集合搭配原始訓練集合將用以產生單一的
分類器，這些多個分類效能較佳的單一分
類器將作為有效建構 MCS 之用。 
此外，以該灰樣本空間為基礎，將可
以有效地決定在特徵空間中最具代表性或
關鍵性的特徵、以及哪些是雜訊特徵、不
相關、重複的特徵或會嚴重影響分類效能
的特徵；進而具體歸納出幾種不同類型的
特徵，這些不同的特徵被預期將有助於選
取特徵以產生較佳的特徵子集合，據以產
生分類效能較佳的單一分類器，這些單一
分類器將作為有效建構 MCS 之用。 
假設在一分類問題中，各個樣本之特
徵集合以 U 表示，即 U={S1, S2, S3,…, 
SN}，其中 Si 代表樣本的第 i 個特徵，而
|U|=N 代表該分類問題中所有特徵的個
數。此外，假設 Ui=U-Si 代表一特徵子集
合，Ui 子集合包含了 U 中所有的特徵或屬
性，唯不包含特徵 Si，換言之，|Ui|=N-1。 
運用前述的基於灰樣本空間之最近鄰
居分類法或以樣本為基礎的學習法則，針
對該特定分類問題中的所有樣本搭配僅僅
選取特徵子集合 Ui，以每次只剔除一個樣
本 的 交 互 驗 證 方 式 (Leave-One-Out 
Cross-Validation[12]，我們可以獲得一個有
關該特徵子集合 Ui 的分類正確率，假設以
Accuracy(Ui)表示。在此，排除選取特徵
Si 後所得到的 Accuracy(Ui)值若較低，代
表該特徵 Si 屬於相對重要、與分類問題直
接相關、排除不選取它會嚴重影響分類正
確率的特徵。相反地，排除選取特徵 Si 後
所得到的 Accuracy(Ui)值若越高，代表該特
徵 Si 極有可能是較不重要、不相關、重複、
雜訊及排除它不會嚴重影響整體分類正確
率的特徵。換言之，藉由 Accuracy(Ui)值的
高低可以區分或辨別每一個特徵 Si 的重要
程度。有關 Accuracy(Ui)值較高或較低的相
對比較基準在此可以使用 Accuracy(U)，其
中 U 代表該分類問題的原始特徵集合。 
這個評估指標 Accuracy(Ui)與分類效
能直接相關，因此可以充份反應每一個特
徵或屬性對於分類或探勘效能的影響程
度。影響程度較高的特徵將優先列入較佳
特徵子集合的優先選擇。 
在作搜尋或產生特徵子集合的動作
前，必須先決定搜尋的起始點[2]，這也主
 7
與原始訓練集合，其所有訓練樣本屬於類
別 Ci 的比率或機率分佈情形將趨於一致。 
如前所述，經由各特定方法將可產生
一單一的分類器。以建構 MCS。在此之前，
必須先針對這些單一的分類器進行分類效
能評估，分類效能表現較佳的分類器方能
作為有效建構 MCS 之用。為此，本研究提
出一個單一分類器其分類效能的評估基
準，包括分類正確率及與其他分類器之間
的多樣性(Diversity)。 
本研究在執行各特定樣本挑選規則挑
選訓練樣本之前，將原始訓練集合 T 隨機
分割為兩大小相等的樣本子集合 Tα與 T
β  ( 此 作 法 可 視 為 2-fold 
Cross-Validation[6]的一種變型)。其中一個
樣本子集合 Tα作為各特定樣本挑選規則
挑選訓練樣本以產生新的訓練樣本集合 Ti
之用，而另一個樣本子集合 Tβ則作為評
估以新的訓練樣本集合 Ti 產生的單一分類
器其分類效能。相對地，亦可將 Tβ作為
各特定樣本挑選規則挑選訓練樣本以產生
新的訓練樣本集合 Ti 之用，而 Tα則作為
評估以新的訓練樣本集合 Ti 產生的單一分
類器其分類效能。 
假設利用新的訓練樣本集合 Ti 可產生
一單一的分類器 CLi，則我們將可計算 CLi
個別的分類效能，記為 Accuracy(CLi, T
β)(或 Accuracy(CLi, Tα))，也就是說，運
用各樣本的類別已知之樣本子集合 Tβ驗
證或評估分類器 CLi 的分類效能。 
本研究擬考量將Accuracy(CLi, Tβ)較
高的分類器CLi納入建構多分類器MCS組
合中，其比較高低的基準可使用原始訓練
集合 T 搭配每次只剔除一個樣本的交互驗
證 方 式 (Leave-One-Out 
Cross-Validation)[12]，所得到對應的分類
正確率記為 Accuracy(CL, T)，其中 CL 代
表利用 T 所產生的單一分類器。 
在有關以操作樣本特徵作為主要考量
面向的單一分類器其分類正確率的研究子
議題方面，本研究主要採下列作法： 
假設利用在灰樣本空間中的原始訓練
集合為 T，被挑選的特徵子集合 FSi 可產生
一單一的分類器 CLi，則將可計算 CLi 個
別的分類效能，記為 Accuracy(CLi, T, 
FSi)。針對該特定分類問題中的所有樣本搭
配僅僅選取特徵子集合 FSi，以每次只剔除
一個樣本的交互驗證方式[12]，可獲得一有
關 FSi 的分類正確率。 
另外，本研究分別採用幾種不同的多
樣性測量方法，以考量所有單一分類器其
分類錯誤之間的關係或關聯性：(1)Q 統計
量 (The Q statistic)(2) 相 關 係 數 ρ
(Correlation Coefficient ρ)(3)雙誤測量法
(The Double-fault Measure)(4)不一致測量
法(Disagreement Measure)(5)廣義的多樣性
(Generalized Diversity)[28](6)Coincident 
Failure Diversity(CFD)[28](7) 熵 測 量 法
(Entropy Measure)E(8)Kohavi-Wolpert 變異
(Kohavi-WolpertVariance)[27]。 
在產生多個分類效能較佳的單一的分
類器之後，即需將各分類器的分類決策判
斷合併，以作成最後的分類決策。本研究
所採用與研究分析比較的決策合併方法包
括：(1)簡單投票法(Simple Voting)、(2)加
權投票法(Weighted Voting)、(3)加權投票
(Weighted Voting)後再進行動態的權重調
整(Dynamic Weight Adjustment)機制 
 
四、結果與討論 
 
針對以操作訓練樣本作為主要考量面
向的建構 MCS 新方法，經由本研究所提出
的訓練樣本挑選機制，由原始訓練集合挑
選可產生較高分類正確率的訓練樣本，將
可獲得不同的新訓練集合，用以產生多個
單一分類器，進一步供作有效建構 MCS 之
用。實驗結果證明，經訓練樣本挑選機制
挑選後所得到的不同新訓練集合，它們將
可提供較佳的分類效能，進而有助於有效
建構 MCS 之用。 
針對以操作樣本特徵作為主要考量面
向的建構 MCS 新方法，經由特徵子集合挑
選機制進行特徵選取，由原始訓練集合挑
選可產生較高分類正確率的樣本特徵，搭
配這些不同、分類效能較佳的特徵子集合
將可獲得不同的新訓練集合，用以產生多
個單一分類器，進一步供作有效建構 MCS
之用。經實驗結果證明，經特徵子集合挑
選機制挑選後所得到的不同特徵子集合組
合，它們將可提供較佳的分類效能，進而
有助於有效建構 MCS 之用。 
 9
向的現有建構 MCS 方法、以操作樣
本特徵作為主要考量面向的現有建
構 MCS 方法、最近鄰居分類方法、
Bagging 法、Stacking 法、及 Boosting
法、分類效能評估、多分類器之間的
多樣性概念及其測量方法、樣本挑選
機制、特徵選取或特徵子集合選取方
法、特徵子集合評估基準、特徵子集
合搜尋方法、分類效能評估、特徵挑
選機制、類別標記替換機制、多分類
器合併機制、灰樣本空間、灰關聯結
構及相似度函數、灰關聯分析方法、
Cross-Validation 以及各項理論之分
類問題領域實務應用等相關重要文
獻，以作為本研究的理論基礎，與建
構 MCS 新方法研究及設計之參考。 
(3) 已完成建立本計畫所提的建構 MCS
新方法的相關實驗環境，以作為後續
相關實驗及與其他 MCS 方法進行分
析比較的基礎。 
(4) 已完成研究、分析與設計本研究所提
出的有效建構 MCS 新方法，與相關
各項測試與驗證。此外，並完成蒐集
整理不同應用領域之分類問題的樣
本集合(Data Set，包括訓練樣本及測
試樣本)，以及將本研究所提有效建構
MCS 新方法所得到的分類結果與人
工處理所得到的結果、單一分類器所
得到的結果，以及其他 MCS 方法進
行各項比較與分析(包括理論分析、分
類效能分析及複雜度分析等)，評估分
類正確率及各項指標。 
(5) 完成發展本研究所提出的有效建構
MCS 新方法，在各項重要問題領域的
可能應用，並驗證其分類正確率及效
能(包括與其他建構 MCS 方法的詳細
比較與分析)。 
(6) 已將研究心得歸納結論，並提出未來
研究方向及建議。 
(7) 完成撰寫研究報告，並以「以操作訓
練樣本、操作樣本特徵、類別標記替
換等三者為主要考量面向的建構
MCS 新方法」為研究主軸，將相關研
究結果發表在國際著名人工智慧與
機器學習領域 SCI 學術期刊。本計畫
之相關研究成果已發表於國際知名
期刊與學術研討會，包括[32-35]等。 
 
本研究之具體成果如下： 
 
(1) 已完成蒐集相關文獻、確定研究方
向、擬定研究目的、方法與步驟。 
(2) 有關理論分析與文獻探討部份，已蒐
集相關重要文獻，以作為本研究的理
論基礎，與建構 MCS 新方法研究及
設計之參考。 
(3) 已完成建構所提的建構 MCS 新方法
的相關實驗環境 
(4) 已完成研究、分析與設計本研究所提
出的有效建構 MCS 新方法，與相關
各項測試與驗證。 
(5) 已將研究心得歸納結論，並提出未來
研究方向及建議。 
(6) 已完成撰寫研究報告，並以「以操作
訓練樣本、操作樣本特徵、類別標記
替換等三者為主要考量面向的建構
MCS 新方法」為研究主軸，將相關研
究結果發表在國際著名人工智慧與
機器學習領域 SCI 學術期刊。本計畫
之相關研究成果已發表於國際知名
期刊與學術研討會，包括[32-35]等。 
 
六、參考文獻 
 
[1] J. Yang and V. Honavar, “Feature Subset 
Selection Using A Genetic Algorithm,” Feature 
Extraction, Construction and Selection:A Data 
Mining Perspective, pp. 117-136, 1998, second 
printing, 2001. 
[2] H. Liu and L. Yu, “Toward Integrating Feature 
Selection Algorithms for Classification and 
Clustering,” IEEE Trans. Knowl. Data Eng. 
17(4), pp. 491-502, 2005. 
[3] M.A. Hall, “Correlation-Based Feature 
Selection for Discrete and Numeric Class 
Machine Learning,” in Proc. 17th Int’l Conf. 
Machine Learning, pp. 359-366, 2000. 
[4] H. Liu and R. Setiono, “Feature Selection and 
Classification-A Probabilistic Wrapper 
Approach,” in Proc. Ninth Int’l Conf. Industrial 
and Eng. Applications of AI and ES, T. Tanaka, 
S. Ohsuga, and M. Ali, eds., pp. 419-424, 1996. 
[5] S. Das, “Filters, Wrappers and a 
Boosting-Based Hybrid for Feature Selection,” 
in Proc. 18th Int’l Conf. Machine Learning, pp. 
74-81, 2001.H. Liu and L. Yu, “Toward 
Integrating Feature Selection Algorithms for 
Classification and Clustering,” IEEE Trans. 
Knowl. Data Eng. 17(4), pp. 491-502, 2005. 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                            2008 年 8 月 5 日 
報告人姓
名 
黃淇竣 
 
 
服務機構
及職稱 
國立高雄海洋科技大學資訊管理系 
副教授 
     時間 
會議 
     地點 
2008 年 7 月 7 日至 
2008 年 7 月 10 日 
美國佛羅里達州奧蘭多市
Imperial Swan Hotel & 
Suites  
國科會專
題研究計
畫編號 
NSC 96-2221-E-022-013 
會議 
名稱 
 (中文) 2008 年人工智慧與圖樣識別國際學術會議 
 (英文) International Conference on Artificial Intelligence and Pattern 
Recognition (AIPR-08) 
發表 
論文 
題目 
 (中文) 一個具特徵反向運算、以灰色理論為基礎的特徵選取新方法 
 (英文) A Novel Grey-based Feature Selection Method with One-Feature 
Reverse Operation 
一、參加會議經過 
 
2008 年人工智慧與圖樣識別國際學術會議(2008 International Conference on 
Artificial Intelligence and Pattern Recognition, AIPR-08)係美國科學與技術研究學
會 (International Society for Research in Science and Technology (ISRST))主辦的
年度大型國際學術會議，為期四天，是國際有關人工智慧與圖樣識別(Artificial 
Intelligence and Pattern Recognition)研究領域的研討會。主辦地點位於美國佛羅里達
州奧蘭多市 Imperial Swan Hotel & Suites (去年亦在美國佛羅里達州奧蘭多市舉行)。今
年吸引來自 29 個國家與地區的領域專家學者投稿，每篇經由至少兩位專家學者審稿人的嚴
格審查，被接受的論文將收錄於會議論文集中，論文接受率僅約 25%。本會議深受該領域專
家學者的重視，參與者來自世界各地，具有相當大的影響與重要性。 
在議程方面，7月 7日主要接受來自世界各地與會者報到與註冊，並領取會議議程及光
碟版論文集一冊。7月 8日至 7月 10 日則安排了接連三天的密集議程。 
    另外，大會邀請來自烏克蘭的 Dr. Roman Bazylevych 及 Dr. Lubov Bazylevych 主講
“Very Large-Scale Intractable Combinatorial Design Automation Problems – Clustering 
Approach for High Quality Solutions”此一重要相關研究主題。 
 
二、與會心得 
 
在 2008 年人工智慧與圖樣識別國際學術會議中，集合了來自世界各地專家學者發表有
關人工智慧與圖樣識別研究領域的最新研究成果，除藉此會議得以相互討論、交換意見及經
驗分享外，並得以瞭解人工智慧與圖樣識別相關的研究歷程、研究現況及未來研究發展趨
向。報告人在此會議中發表了一篇有關人工智慧(Artificial Intelligence)研究領域的文
章，於 7月 7日的議程場次中發表，題目為一個具特徵反向運算、以灰色理論為基礎的特徵
選取新方法(A Novel Grey-based Feature Selection Method with One-Feature Reverse 
Operation)。此篇文章主要目的為提出一個在機器學習中可有效提昇分類正確率
(Classification Accuracy)的特徵選取新方法，此一新方法在特徵選取過程中具備特徵反
A Novel Grey-based Feature Selection Method with One-Feature Reverse 
Operation 
 
Chi-Chun Huang 
Department of Information Management,National Kaohsiung Marine University 
cchuang@mail.nkmu.edu.tw 
 
Hsin-Yun Chang 
Department of Business Administration, Chin-Min Institute of Technology  
Department of Industrial Technology Education, National Kaohsiung Normal University 
ran_hsin@ms.chinmin.edu.tw 
 
 
Abstract 
 
In this paper, a grey-based feature selection method 
with one-feature reverse operation is proposed.  The 
classification effectiveness of each attribute of a 
particular pattern classification problem in the grey-
based learning approach is determined and then each 
attribute can be ranked.  Features with higher 
classification effectiveness are more important and 
relevant for the particular pattern classification 
problem.  The set of these important and relevant 
features is thus considered as the search starting point 
for feature selection.  That is, the search starting point 
for feature selection is determined by somewhere in the 
middle of the search space.  Accordingly, the one-
feature reverse operation is employed on the search 
starting point to generate perturbed candidate feature 
subsets.  Experiments performed on various datasets 
are reported to demonstrate the power of the proposed 
method.  As a result, the overall classification 
performance can be increased when the proposed 
method is performed in advance. 
 
1. Introduction 
 
In pattern classification [1] , feature subset selection 
is generally carried out with four steps [3][4][5][6][7].  
(1)The search starting point in the search space; (2)A 
generation rule with search strategies to generate the 
next candidate feature subset; (3)An evaluation 
function or ranking method to rank or evaluate each 
generated feature subset; (4)A stopping criterion to 
determine when to halt the selection process.  As a 
determinative and principal step for feature subset 
selection, the search starting point in the search space 
is used to decide the direction of the search [5].  
Generally, the feature subset search procedure can start 
with no features (for example, sequential forward 
selection method [8]) or all features (for example, 
sequential backward elimination method [8]).  
Accordingly, features are successively added or 
eliminated (i.e., deterministic heuristic search).  In 
these two cases, local maximums (i.e., sub-optimal 
feature subsets) are often obtained because of 
successive additions or eliminations of features.  In 
another approach, random sampling [5], the feature 
subset search procedure can start with a random subset 
of features.  This method can help the search procedure 
to escape from local maximums [8] (i.e., non-
deterministic heuristic search).  However, inconsistent 
final feature subsets may be derived from different 
runs [9].  As a result, search starting point 
determination plays a vital role here and will 
significantly affect the performance of the 
corresponding feature subset selection method. 
If the search starting point in the search procedure 
for feature subset selection is determined with high 
relevance to the final best feature subset, the final best 
feature subset can then be approached more effectively.  
For example, consider that the best feature subset of a 
specific classification task is {A, D, F, J, K}.  The 
search starting point (or feature subset) {A,D,F,J} is 
helpful in the search procedure since the best feature 
subset {A, D, F, J, K} can be approached by including 
feature K among the search starting point (or feature 
subset) {A,D,F,J} for feature subset selection 
(Restated, the search starting point in the search space, 
as a determinative and principal step for feature subset 
selection, is used to decide the direction of the search.).  
Based on this idea, a grey-based feature selection 
method with one-feature reverse operation is proposed.  
demonstrated to be beneficial when dealing with 
problems containing incomplete, poor or uncertain 
information [11].   
For pattern classification, a so-called grey-based 
instance space (i.e., the relationships among all 
instances) can be formed based on GRA.  That is, each 
instance xi and its nearest neighbor xi* can be identified 
based on the degree of GRG among all instances and 
the principle property of the grey relational analysis 
(i.e., calculating the grey relational coefficient (GRC) 
and the grey relational grade (GRG)).  Obviously, 
⎟⎠
⎞⎜⎝
⎛ *, ii xxGRG  will have the smallest value than others.  
Consequently, a new, unseen instance can be classified 
according to the class label of its nearest instance in 
the grey-based instance space (This learning concept 
originated from the nearest neighbor classification 
principle [14][15]).  
In summary, the grey relational analysis, including 
calculating the grey relational coefficient (GRC) and 
the grey relational grade (GRG), is mainly used to 
determine the relationships or similarities among all 
instances and then identify nearest neighbors of each 
instance for pattern classification in the grey-based 
learning approach.  As pointed out in [10], 
experimental results have shown that the grey-based 
learning approach with the grey relational analysis 
(which is used as the similarity function) yields higher 
performance over other methods that adopt one of the 
two well-known similarity functions or both, i.e., 
Euclidean metric and the Value Difference Metric 
(VDM).    As a result, a grey-based learning approach, 
which can yield excellent performance [10], is 
introduced clearly.  The advantages and the properties 
of the grey-based learning approach are detailed in 
[10].  In addition, more details regarding the concepts 
of grey relational analysis can be found in 
[10][11][12][13]. 
 
3. Grey-based Feature Selection Method 
with One-Feature Reverse Operation 
 
In this section, a grey-based feature selection 
method with one-feature reverse operation is proposed.  
Firstly, the classification effectiveness of each attribute 
of a particular pattern classification problem in the 
above–mentioned grey-based learning approach is 
detailed in this section. 
Let V={v1, v2, …, vm} be a set of m labeled training 
instances in a particular pattern classification problem.  
Each instance has n features (attributes), which are 
denoted as F = (f1, f2, …, fn).  By using the above-
mentioned grey-based learning approach with leave-
one-out cross-validation method [17], an average 
classification accuracy regarding the training set V and 
feature set F (denoted by ACC(V,F)), can be obtained.  
Leave-one-out cross-validation means that each 
instance in V is used as the test instance once and other 
instances in V are used as the corresponding training 
instances in the grey-based learning approach.  That is, 
the learning approach will be carried out m times, with 
respect to m instances in V.  Consequently, ACC(V,F) 
is the baseline classification accuracy of the grey-based 
learning approach for the corresponding pattern 
classification problem. 
Let Fi=F-{fi}.  Similarly, a corresponding 
classification accuracy ACC(V, Fi) can be obtained.  
The value of ACC(V, Fi) specifies the classification 
accuracy obtained by using the grey-based learning 
approach with the feature subset Fi (That is, feature fi 
is not included in the feature space of the grey-based 
learning approach).  The difference between ACC(V,F) 
and ACC(V, Fi), denoted as DIF(fi), indicates the 
classification effectiveness of each feature fi in the 
grey-based learning approach.  Here, feature fi is 
eliminated from the original feature set F and the 
relationships or relevance among all other features in 
the grey-based learning approach are reflected.  If the 
difference between ACC(V,F) and ACC(V, Fi) (i.e., 
DIF(fi)) is big enough, feature fi can then be considered 
as an important and relevant feature for the pattern 
classification problem.  This is because the exclusion 
of feature fi from the original feature set F will 
significantly reduce the overall and baseline 
classification accuracy.  In other words, feature fi 
should be considered a high priority for feature 
selection.  Conversely, feature fi can be viewed as an 
irrelevant and meaningless feature if the difference 
between ACC(V,F) and ACC(V, Fi)(i.e., DIF(fi)) is 
slight.  In other words, feature fi should be considered 
a low priority for feature selection. 
For feature selection, a possible solution in the 
solution space is a specific feature subset that can be 
encoded as a string of n binary digits (or bits).  In the 
proposed feature selection method, each feature is 
represented by a binary digit with values 1 and 0, 
which identify whether the feature is selected or not 
selected in the corresponding feature subset, 
respectively.  This process is called solution encoding.  
For instance, a string of ten binary digits (i.e., a 
solution or a feature subset), say, 0100100010, means 
that features 2, 5, and 9 are selected in the 
corresponding feature subset. 
Based on the concept of classification effectiveness 
of each attribute fi in F (i.e. DIF(fi)), regarding the 
training set V in the grey-based learning approach, a 
method has close connection to the final best feature 
subset {f1, f2, f3, f6, f7, f8}.  For the Voting problem 
domain with sixteen features, all features in the final 
best feature subset {f3, f4, f5, f7, f10, f11} are also 
included in the search starting point {f3, f4, f5, f7, f8, f10, 
f11, f15} obtained by using the proposed method.  Here, 
the exclusion of features f8 and f15 can be easily 
implemented by some other existing feature subset 
selection method, such as sequential backward 
elimination method [8].  For the Primarytumor 
classification task with seventeen features, the search 
starting point {f1, f2, f3, f4, f5, f7, f10, f16, f17} obtained by 
using the proposed method has high relevance to the 
final best feature subset {f1, f2, f3, f4, f5, f7, f10, f13, f15, f16, 
f17}.  Here, the final best feature subset {f1, f2, f3, f4, f5, 
f7, f10, f13, f15, f16, f17} can be approached by including 
features f13 and f15 among the search starting point {f1, 
f2, f3, f4, f5, f7, f10, f16, f17}.  The inclusion of features f13 
and f15 can be easily implemented by some other 
existing feature subset selection method, such as 
sequential forward selection method [8]. 
Let X denote the search starting point (for feature 
subset selection) obtained by using the proposed 
method and Y denote the best feature subset in a 
particular pattern classification problem.  A match 
ratio (MR) regarding X and Y can be introduced as 
follows: 
X
YX
o (MR)match rati
∩=                                 (3) 
where X  and Y  denote the number of features in 
feature sets X and Y, respectively. 
Obviously, a search starting point X with high match 
ratio MR means that nearly all features in X (i.e., nearly 
⎡ ⎤2/n  features in the original feature set) are ‘also’ 
included in the best feature subset Y.  Thus, by using 
the corresponding search starting point X in the search 
procedure for feature subset selection, the final best 
feature subset Y can then be approached more 
effectively. 
Table 1 represents the main characteristics of the 
datasets used for performance comparison regarding 
the match ratios MRs.  Table 2 represents the match 
ratios MRs of the search starting points obtained by 
random sampling [7] and by using the proposed 
method, with respect to the above-mentioned 
classification tasks or datasets.  For these classification 
domains, the average match ratios (MRs) of the search 
starting points obtained by random sampling and by 
using the proposed method are 62.0% and 94.6%, 
respectively.  In other words, as expected, nearly all 
features (94.6%) in the search starting point obtained 
by using the proposed method are also included in the 
final best feature subset of a particular pattern 
classification problem.   
 
Table 1. 
Details of twenty-nine experimental classification tasks
Classification 
task 
Number 
of 
instances
Number 
of 
classes 
Number of 
features and 
their types 
Breastw 699 2 9 (9-C) 
Car 1728 4 6 (6-S) 
Corral 16 2 6 (6-S) 
Echocardiogra
m 
74 2 10 (2-S, 8-
C) 
Echoi 336 8 7 (7-C) 
Glass 214 6 9 (9-C) 
Haberman 306 2 3 (3-C) 
Hayesroth 132 3 4 (4-S) 
Hcleveland 303 5 12 (8-S, 5-
C) 
Iris 150 3 4 (4-C) 
Lenses 24 3 4 (4-S) 
Liver Disorder 345 2 6 (6-C) 
Lymphography 148 4 18 (15-S, 3-
C) 
Monk1 432 2 6 (6-S) 
Monk2 432 2 6 (6-S) 
Monk3 432 2 6 (6-S) 
Nursery 12960 5 8 (8-S) 
Pageblock 5473 5 10 (10-C) 
Pimadiabetes 768 2 8 (8-C) 
Postoperative  90 3 18 (18-S) 
Primarytumor 339 21 17 (17-S) 
Segment 2310 7 18 (18-C) 
Shuttle 43500 7 9 (9-C) 
Solarflare 323 6 12 (12-S) 
Soybeansmall 47 4 21 (15-S, 6-
C) 
Tae 151 3 5 (4-S, 1-C) 
Tictactoe 958 2 9 (9-S) 
Voting 435 2 16 (16-S) 
Vowel 990 11 10 (10-C) 
C: Continuous, S: Symbolic 
 
As shown in Table 3, the statistical analysis, 
including better or worse test (B/W test; for example, a 
better or worse test result of 24/5/0 under the random 
sampling column means that the proposed method 
performs better than the random sampling in 24 cases 
and the same as random sampling in 5 cases) and 
Wilcoxon Signed Ranks test [19] (i.e., the proposed 
method is compared with random sampling) was done.  
Here, Wilcoxon Signed Ranks test was used to test the 
starting point to generate perturbed candidate feature 
subsets.  Experiments performed on various datasets 
are reported to demonstrate the power of the proposed 
method.  As a result, the overall classification 
performance can be increased when the proposed 
method is performed in advance. 
 
Table 4. 
Details of thirty-seven experimental pattern 
classification problems  
Classification 
task 
Number 
of 
instances
Numb
er of 
classes 
Number of 
features and 
their types 
Australian 690 2 14 (8-S, 6-C) 
Autompg 398 3 7 (2-S, 5-C) 
Breastw 699 2 9 (9-C) 
Bridges 105 6 12 (9-S, 3-C) 
Car 1728 4 6 (6-S) 
Corral 16 2 6 (6-S) 
Cpu 209 8 7 (7-C) 
Echocardiogra
m 
74 2 10 (2-S, 8-C) 
Echoi 336 8 7 (7-C) 
Glass 214 6 9 (9-C) 
Haberman 306 2 3 (3-C) 
Hayesroth 132 3 4 (4-S) 
Hcleveland 303 5 13 (8-S, 5-C) 
Hepatitis 155 2 19 (13-S, 6-C)
Hhunggarian 294 2 12 (7-S, 5-C) 
Horsecolic 368 2 22 (15-S, 7-C)
Imagetrain 210 7 19 (19-C) 
Iris 150 3 4 (4-C) 
Lenses 24 3 4 (4-S) 
Liver Disorder 345 2 6 (6-C) 
Lymphography 148 4 18 (15-S, 3-C)
Nursery 12960 5 8 (8-S) 
Pageblocks 5473 5 10 (10-C) 
Pimadiabetes 768 2 8 (8-C) 
Postoperative  90 3 18 (18-S) 
Primarytumor 339 21 17 (17-S) 
Segment 2310 7 18 (18-C) 
Shuttle 43500 7 9 (9-C) 
Solarflare 323 6 12 (12-S) 
Soybeansmall 47 4 21 (15-S, 6-C)
Tae 151 3 5 (4-S, 1-C) 
Tictactoe 958 2 9 (9-S) 
Voting 435 2 16 (16-S) 
Vowel 990 11 10 (10-C) 
Wine 178 3 13 (13-C) 
Yeast 1484 10 8 (8-C) 
Zoo 101 7 16 (16-S) 
C: Continuous, S: Symbolic 
 
 
Table 5. 
The classification abilities or accuracies of the above-
mentioned grey-based learning approach while the 
proposed method for feature selection is performed or 
not. 
Pattern 
classification 
problems 
The proposed 
method is not 
used for 
feature 
selection 
The proposed 
method is 
used for 
feature 
selection 
Australian 81.74  85.51 
Autompg 69.10  87.19 
Breastw 95.85  97.28 
Bridges 87.62  100.00 
Car 73.44 96.18
Corral 81.25 87.50
Cpu 69.38  70.33 
Echocardiogram 98.65  98.65 
Echoi 79.76  80.06 
Glass 73.83  79.91 
Haberman 64.38 73.53
Hayesroth 65.91 71.21
Hcleveland 55.78  59.08 
Hepatitis 80.00  88.39 
Hhunggarian 75.85  80.27 
Horsecolic 76.63  86.41 
Imagetrain 87.62  94.29 
Iris 94.00  97.33 
Lenses 75.00  83.33 
Liver Disorder 62.90 66.96
Lymphography 79.73  89.19 
Nursery 77.60 94.81
Pageblocks 96.02  96.62 
Pimadiabetes 69.14 71.22
Postoperative  52.22 72.22
Primarytumor 32.45 35.40
Segment 97.92 98.35
Shuttle 99.95 99.96
Solarflare 64.71 67.49
Soybeansmall 100.00 100.00
Tae 66.23  66.89 
Tictactoe 48.85 78.71
Voting 92.87  97.24 
Vowel 98.99 99.19
Wine 96.63  100.00 
Yeast 53.64  53.64 
Zoo 96.04  98.02 
Average 77.61  83.85 
 
 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                            2008 年 8 月 5 日 
報告人姓
名 
黃淇竣 
 
 
服務機構
及職稱 
國立高雄海洋科技大學資訊管理系 
副教授 
     時間 
會議 
     地點 
2008 年 7 月 7 日至 
2008 年 7 月 10 日 
美國佛羅里達州奧蘭多市
Imperial Swan Hotel & 
Suites  
國科會專
題研究計
畫編號 
NSC 96-2221-E-022-013 
會議 
名稱 
 (中文) 2008 年人工智慧與圖樣識別國際學術會議 
 (英文) International Conference on Artificial Intelligence and Pattern 
Recognition (AIPR-08) 
發表 
論文 
題目 
 (中文) 一個具特徵反向運算、以灰色理論為基礎的特徵選取新方法 
 (英文) A Novel Grey-based Feature Selection Method with One-Feature 
Reverse Operation 
一、參加會議經過 
 
2008 年人工智慧與圖樣識別國際學術會議(2008 International Conference on 
Artificial Intelligence and Pattern Recognition, AIPR-08)係美國科學與技術研究學
會 (International Society for Research in Science and Technology (ISRST))主辦的
年度大型國際學術會議，為期四天，是國際有關人工智慧與圖樣識別(Artificial 
Intelligence and Pattern Recognition)研究領域的研討會。主辦地點位於美國佛羅里達
州奧蘭多市 Imperial Swan Hotel & Suites (去年亦在美國佛羅里達州奧蘭多市舉行)。今
年吸引來自 29 個國家與地區的領域專家學者投稿，每篇經由至少兩位專家學者審稿人的嚴
格審查，被接受的論文將收錄於會議論文集中，論文接受率僅約 25%。本會議深受該領域專
家學者的重視，參與者來自世界各地，具有相當大的影響與重要性。 
在議程方面，7月 7日主要接受來自世界各地與會者報到與註冊，並領取會議議程及光
碟版論文集一冊。7月 8日至 7月 10 日則安排了接連三天的密集議程。 
    另外，大會邀請來自烏克蘭的 Dr. Roman Bazylevych 及 Dr. Lubov Bazylevych 主講
“Very Large-Scale Intractable Combinatorial Design Automation Problems – Clustering 
Approach for High Quality Solutions”此一重要相關研究主題。 
 
二、與會心得 
 
在 2008 年人工智慧與圖樣識別國際學術會議中，集合了來自世界各地專家學者發表有
關人工智慧與圖樣識別研究領域的最新研究成果，除藉此會議得以相互討論、交換意見及經
驗分享外，並得以瞭解人工智慧與圖樣識別相關的研究歷程、研究現況及未來研究發展趨
向。報告人在此會議中發表了一篇有關人工智慧(Artificial Intelligence)研究領域的文
章，於 7月 7日的議程場次中發表，題目為一個具特徵反向運算、以灰色理論為基礎的特徵
選取新方法(A Novel Grey-based Feature Selection Method with One-Feature Reverse 
Operation)。此篇文章主要目的為提出一個在機器學習中可有效提昇分類正確率
(Classification Accuracy)的特徵選取新方法，此一新方法在特徵選取過程中具備特徵反
A Novel Grey-based Feature Selection Method with One-Feature Reverse 
Operation 
 
Chi-Chun Huang 
Department of Information Management,National Kaohsiung Marine University 
cchuang@mail.nkmu.edu.tw 
 
Hsin-Yun Chang 
Department of Business Administration, Chin-Min Institute of Technology  
Department of Industrial Technology Education, National Kaohsiung Normal University 
ran_hsin@ms.chinmin.edu.tw 
 
 
Abstract 
 
In this paper, a grey-based feature selection method 
with one-feature reverse operation is proposed.  The 
classification effectiveness of each attribute of a 
particular pattern classification problem in the grey-
based learning approach is determined and then each 
attribute can be ranked.  Features with higher 
classification effectiveness are more important and 
relevant for the particular pattern classification 
problem.  The set of these important and relevant 
features is thus considered as the search starting point 
for feature selection.  That is, the search starting point 
for feature selection is determined by somewhere in the 
middle of the search space.  Accordingly, the one-
feature reverse operation is employed on the search 
starting point to generate perturbed candidate feature 
subsets.  Experiments performed on various datasets 
are reported to demonstrate the power of the proposed 
method.  As a result, the overall classification 
performance can be increased when the proposed 
method is performed in advance. 
 
1. Introduction 
 
In pattern classification [1] , feature subset selection 
is generally carried out with four steps [3][4][5][6][7].  
(1)The search starting point in the search space; (2)A 
generation rule with search strategies to generate the 
next candidate feature subset; (3)An evaluation 
function or ranking method to rank or evaluate each 
generated feature subset; (4)A stopping criterion to 
determine when to halt the selection process.  As a 
determinative and principal step for feature subset 
selection, the search starting point in the search space 
is used to decide the direction of the search [5].  
Generally, the feature subset search procedure can start 
with no features (for example, sequential forward 
selection method [8]) or all features (for example, 
sequential backward elimination method [8]).  
Accordingly, features are successively added or 
eliminated (i.e., deterministic heuristic search).  In 
these two cases, local maximums (i.e., sub-optimal 
feature subsets) are often obtained because of 
successive additions or eliminations of features.  In 
another approach, random sampling [5], the feature 
subset search procedure can start with a random subset 
of features.  This method can help the search procedure 
to escape from local maximums [8] (i.e., non-
deterministic heuristic search).  However, inconsistent 
final feature subsets may be derived from different 
runs [9].  As a result, search starting point 
determination plays a vital role here and will 
significantly affect the performance of the 
corresponding feature subset selection method. 
If the search starting point in the search procedure 
for feature subset selection is determined with high 
relevance to the final best feature subset, the final best 
feature subset can then be approached more effectively.  
For example, consider that the best feature subset of a 
specific classification task is {A, D, F, J, K}.  The 
search starting point (or feature subset) {A,D,F,J} is 
helpful in the search procedure since the best feature 
subset {A, D, F, J, K} can be approached by including 
feature K among the search starting point (or feature 
subset) {A,D,F,J} for feature subset selection 
(Restated, the search starting point in the search space, 
as a determinative and principal step for feature subset 
selection, is used to decide the direction of the search.).  
Based on this idea, a grey-based feature selection 
method with one-feature reverse operation is proposed.  
demonstrated to be beneficial when dealing with 
problems containing incomplete, poor or uncertain 
information [11].   
For pattern classification, a so-called grey-based 
instance space (i.e., the relationships among all 
instances) can be formed based on GRA.  That is, each 
instance xi and its nearest neighbor xi* can be identified 
based on the degree of GRG among all instances and 
the principle property of the grey relational analysis 
(i.e., calculating the grey relational coefficient (GRC) 
and the grey relational grade (GRG)).  Obviously, 
⎟⎠
⎞⎜⎝
⎛ *, ii xxGRG  will have the smallest value than others.  
Consequently, a new, unseen instance can be classified 
according to the class label of its nearest instance in 
the grey-based instance space (This learning concept 
originated from the nearest neighbor classification 
principle [14][15]).  
In summary, the grey relational analysis, including 
calculating the grey relational coefficient (GRC) and 
the grey relational grade (GRG), is mainly used to 
determine the relationships or similarities among all 
instances and then identify nearest neighbors of each 
instance for pattern classification in the grey-based 
learning approach.  As pointed out in [10], 
experimental results have shown that the grey-based 
learning approach with the grey relational analysis 
(which is used as the similarity function) yields higher 
performance over other methods that adopt one of the 
two well-known similarity functions or both, i.e., 
Euclidean metric and the Value Difference Metric 
(VDM).    As a result, a grey-based learning approach, 
which can yield excellent performance [10], is 
introduced clearly.  The advantages and the properties 
of the grey-based learning approach are detailed in 
[10].  In addition, more details regarding the concepts 
of grey relational analysis can be found in 
[10][11][12][13]. 
 
3. Grey-based Feature Selection Method 
with One-Feature Reverse Operation 
 
In this section, a grey-based feature selection 
method with one-feature reverse operation is proposed.  
Firstly, the classification effectiveness of each attribute 
of a particular pattern classification problem in the 
above–mentioned grey-based learning approach is 
detailed in this section. 
Let V={v1, v2, …, vm} be a set of m labeled training 
instances in a particular pattern classification problem.  
Each instance has n features (attributes), which are 
denoted as F = (f1, f2, …, fn).  By using the above-
mentioned grey-based learning approach with leave-
one-out cross-validation method [17], an average 
classification accuracy regarding the training set V and 
feature set F (denoted by ACC(V,F)), can be obtained.  
Leave-one-out cross-validation means that each 
instance in V is used as the test instance once and other 
instances in V are used as the corresponding training 
instances in the grey-based learning approach.  That is, 
the learning approach will be carried out m times, with 
respect to m instances in V.  Consequently, ACC(V,F) 
is the baseline classification accuracy of the grey-based 
learning approach for the corresponding pattern 
classification problem. 
Let Fi=F-{fi}.  Similarly, a corresponding 
classification accuracy ACC(V, Fi) can be obtained.  
The value of ACC(V, Fi) specifies the classification 
accuracy obtained by using the grey-based learning 
approach with the feature subset Fi (That is, feature fi 
is not included in the feature space of the grey-based 
learning approach).  The difference between ACC(V,F) 
and ACC(V, Fi), denoted as DIF(fi), indicates the 
classification effectiveness of each feature fi in the 
grey-based learning approach.  Here, feature fi is 
eliminated from the original feature set F and the 
relationships or relevance among all other features in 
the grey-based learning approach are reflected.  If the 
difference between ACC(V,F) and ACC(V, Fi) (i.e., 
DIF(fi)) is big enough, feature fi can then be considered 
as an important and relevant feature for the pattern 
classification problem.  This is because the exclusion 
of feature fi from the original feature set F will 
significantly reduce the overall and baseline 
classification accuracy.  In other words, feature fi 
should be considered a high priority for feature 
selection.  Conversely, feature fi can be viewed as an 
irrelevant and meaningless feature if the difference 
between ACC(V,F) and ACC(V, Fi)(i.e., DIF(fi)) is 
slight.  In other words, feature fi should be considered 
a low priority for feature selection. 
For feature selection, a possible solution in the 
solution space is a specific feature subset that can be 
encoded as a string of n binary digits (or bits).  In the 
proposed feature selection method, each feature is 
represented by a binary digit with values 1 and 0, 
which identify whether the feature is selected or not 
selected in the corresponding feature subset, 
respectively.  This process is called solution encoding.  
For instance, a string of ten binary digits (i.e., a 
solution or a feature subset), say, 0100100010, means 
that features 2, 5, and 9 are selected in the 
corresponding feature subset. 
Based on the concept of classification effectiveness 
of each attribute fi in F (i.e. DIF(fi)), regarding the 
training set V in the grey-based learning approach, a 
method has close connection to the final best feature 
subset {f1, f2, f3, f6, f7, f8}.  For the Voting problem 
domain with sixteen features, all features in the final 
best feature subset {f3, f4, f5, f7, f10, f11} are also 
included in the search starting point {f3, f4, f5, f7, f8, f10, 
f11, f15} obtained by using the proposed method.  Here, 
the exclusion of features f8 and f15 can be easily 
implemented by some other existing feature subset 
selection method, such as sequential backward 
elimination method [8].  For the Primarytumor 
classification task with seventeen features, the search 
starting point {f1, f2, f3, f4, f5, f7, f10, f16, f17} obtained by 
using the proposed method has high relevance to the 
final best feature subset {f1, f2, f3, f4, f5, f7, f10, f13, f15, f16, 
f17}.  Here, the final best feature subset {f1, f2, f3, f4, f5, 
f7, f10, f13, f15, f16, f17} can be approached by including 
features f13 and f15 among the search starting point {f1, 
f2, f3, f4, f5, f7, f10, f16, f17}.  The inclusion of features f13 
and f15 can be easily implemented by some other 
existing feature subset selection method, such as 
sequential forward selection method [8]. 
Let X denote the search starting point (for feature 
subset selection) obtained by using the proposed 
method and Y denote the best feature subset in a 
particular pattern classification problem.  A match 
ratio (MR) regarding X and Y can be introduced as 
follows: 
X
YX
o (MR)match rati
∩=                                 (3) 
where X  and Y  denote the number of features in 
feature sets X and Y, respectively. 
Obviously, a search starting point X with high match 
ratio MR means that nearly all features in X (i.e., nearly 
⎡ ⎤2/n  features in the original feature set) are ‘also’ 
included in the best feature subset Y.  Thus, by using 
the corresponding search starting point X in the search 
procedure for feature subset selection, the final best 
feature subset Y can then be approached more 
effectively. 
Table 1 represents the main characteristics of the 
datasets used for performance comparison regarding 
the match ratios MRs.  Table 2 represents the match 
ratios MRs of the search starting points obtained by 
random sampling [7] and by using the proposed 
method, with respect to the above-mentioned 
classification tasks or datasets.  For these classification 
domains, the average match ratios (MRs) of the search 
starting points obtained by random sampling and by 
using the proposed method are 62.0% and 94.6%, 
respectively.  In other words, as expected, nearly all 
features (94.6%) in the search starting point obtained 
by using the proposed method are also included in the 
final best feature subset of a particular pattern 
classification problem.   
 
Table 1. 
Details of twenty-nine experimental classification tasks
Classification 
task 
Number 
of 
instances
Number 
of 
classes 
Number of 
features and 
their types 
Breastw 699 2 9 (9-C) 
Car 1728 4 6 (6-S) 
Corral 16 2 6 (6-S) 
Echocardiogra
m 
74 2 10 (2-S, 8-
C) 
Echoi 336 8 7 (7-C) 
Glass 214 6 9 (9-C) 
Haberman 306 2 3 (3-C) 
Hayesroth 132 3 4 (4-S) 
Hcleveland 303 5 12 (8-S, 5-
C) 
Iris 150 3 4 (4-C) 
Lenses 24 3 4 (4-S) 
Liver Disorder 345 2 6 (6-C) 
Lymphography 148 4 18 (15-S, 3-
C) 
Monk1 432 2 6 (6-S) 
Monk2 432 2 6 (6-S) 
Monk3 432 2 6 (6-S) 
Nursery 12960 5 8 (8-S) 
Pageblock 5473 5 10 (10-C) 
Pimadiabetes 768 2 8 (8-C) 
Postoperative  90 3 18 (18-S) 
Primarytumor 339 21 17 (17-S) 
Segment 2310 7 18 (18-C) 
Shuttle 43500 7 9 (9-C) 
Solarflare 323 6 12 (12-S) 
Soybeansmall 47 4 21 (15-S, 6-
C) 
Tae 151 3 5 (4-S, 1-C) 
Tictactoe 958 2 9 (9-S) 
Voting 435 2 16 (16-S) 
Vowel 990 11 10 (10-C) 
C: Continuous, S: Symbolic 
 
As shown in Table 3, the statistical analysis, 
including better or worse test (B/W test; for example, a 
better or worse test result of 24/5/0 under the random 
sampling column means that the proposed method 
performs better than the random sampling in 24 cases 
and the same as random sampling in 5 cases) and 
Wilcoxon Signed Ranks test [19] (i.e., the proposed 
method is compared with random sampling) was done.  
Here, Wilcoxon Signed Ranks test was used to test the 
starting point to generate perturbed candidate feature 
subsets.  Experiments performed on various datasets 
are reported to demonstrate the power of the proposed 
method.  As a result, the overall classification 
performance can be increased when the proposed 
method is performed in advance. 
 
Table 4. 
Details of thirty-seven experimental pattern 
classification problems  
Classification 
task 
Number 
of 
instances
Numb
er of 
classes 
Number of 
features and 
their types 
Australian 690 2 14 (8-S, 6-C) 
Autompg 398 3 7 (2-S, 5-C) 
Breastw 699 2 9 (9-C) 
Bridges 105 6 12 (9-S, 3-C) 
Car 1728 4 6 (6-S) 
Corral 16 2 6 (6-S) 
Cpu 209 8 7 (7-C) 
Echocardiogra
m 
74 2 10 (2-S, 8-C) 
Echoi 336 8 7 (7-C) 
Glass 214 6 9 (9-C) 
Haberman 306 2 3 (3-C) 
Hayesroth 132 3 4 (4-S) 
Hcleveland 303 5 13 (8-S, 5-C) 
Hepatitis 155 2 19 (13-S, 6-C)
Hhunggarian 294 2 12 (7-S, 5-C) 
Horsecolic 368 2 22 (15-S, 7-C)
Imagetrain 210 7 19 (19-C) 
Iris 150 3 4 (4-C) 
Lenses 24 3 4 (4-S) 
Liver Disorder 345 2 6 (6-C) 
Lymphography 148 4 18 (15-S, 3-C)
Nursery 12960 5 8 (8-S) 
Pageblocks 5473 5 10 (10-C) 
Pimadiabetes 768 2 8 (8-C) 
Postoperative  90 3 18 (18-S) 
Primarytumor 339 21 17 (17-S) 
Segment 2310 7 18 (18-C) 
Shuttle 43500 7 9 (9-C) 
Solarflare 323 6 12 (12-S) 
Soybeansmall 47 4 21 (15-S, 6-C)
Tae 151 3 5 (4-S, 1-C) 
Tictactoe 958 2 9 (9-S) 
Voting 435 2 16 (16-S) 
Vowel 990 11 10 (10-C) 
Wine 178 3 13 (13-C) 
Yeast 1484 10 8 (8-C) 
Zoo 101 7 16 (16-S) 
C: Continuous, S: Symbolic 
 
 
Table 5. 
The classification abilities or accuracies of the above-
mentioned grey-based learning approach while the 
proposed method for feature selection is performed or 
not. 
Pattern 
classification 
problems 
The proposed 
method is not 
used for 
feature 
selection 
The proposed 
method is 
used for 
feature 
selection 
Australian 81.74  85.51 
Autompg 69.10  87.19 
Breastw 95.85  97.28 
Bridges 87.62  100.00 
Car 73.44 96.18
Corral 81.25 87.50
Cpu 69.38  70.33 
Echocardiogram 98.65  98.65 
Echoi 79.76  80.06 
Glass 73.83  79.91 
Haberman 64.38 73.53
Hayesroth 65.91 71.21
Hcleveland 55.78  59.08 
Hepatitis 80.00  88.39 
Hhunggarian 75.85  80.27 
Horsecolic 76.63  86.41 
Imagetrain 87.62  94.29 
Iris 94.00  97.33 
Lenses 75.00  83.33 
Liver Disorder 62.90 66.96
Lymphography 79.73  89.19 
Nursery 77.60 94.81
Pageblocks 96.02  96.62 
Pimadiabetes 69.14 71.22
Postoperative  52.22 72.22
Primarytumor 32.45 35.40
Segment 97.92 98.35
Shuttle 99.95 99.96
Solarflare 64.71 67.49
Soybeansmall 100.00 100.00
Tae 66.23  66.89 
Tictactoe 48.85 78.71
Voting 92.87  97.24 
Vowel 98.99 99.19
Wine 96.63  100.00 
Yeast 53.64  53.64 
Zoo 96.04  98.02 
Average 77.61  83.85 
 
 
