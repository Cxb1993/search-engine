I 
中文摘要 
 
本計畫係為二年期之計畫， 其主要目的在發展一個整合了音訊分類、音訊分離以及音
訊檢索的系統， 並將之應用於多媒體資料庫等相關分析。對於音訊資料的內容分析中，音
訊分類是最為重要的處理步驟；因此在第一年的計畫中，我們提出一個音訊分類方法，我
們亦將說明此方法之結果如何應用於輔助其他多媒體資料庫檢索。接著在第二年的計畫
中，我們提出一個基於音訊分類的音訊分離演算法。在本階段中我們並將整合視訊資料，
以驗證所提出方法在運用於多媒體資料庫檢索時之效能。最後我們會提出一個基於音訊內
涵為主的音訊資料檢索演算法以及一個整合這兩年所提出相關音訊分析演算法的系統。 
我們所提出的這些音訊分析方法之主要觀念是基於一個事實，即音訊資料的聲紋圖中
蘊含了所有一切音訊資料的相關重要訊息；由於聲紋圖包含了時間域以及頻率域的重要資
訊，因此不同型態的音訊資料，其在聲紋圖上所顯現出來的訊息則有所不同：有些是屬於
有方向性的圖樣，有些則是隨機性的圖樣。基於上述事實，我們將基於聲紋圖上的訊息來
發展相關的音訊分類、音訊分離以及音訊檢索等演算法。 
 
關鍵詞：音訊分類; 音訊分離; 音訊檢索; MPEG-7; 聲紋圖; Fisher Linear Discriminator; 
Gabor wavelets 
III 
目錄 
 
中文摘要...........................................................................................................................................I 
Abstract ........................................................................................................................................... II 
目錄................................................................................................................................................ III 
報告內容.......................................................................................................................................... 1 
1. Introduction ........................................................................................................................ 1 
2. The Proposed Methods ...................................................................................................... 3 
2.1 The Proposed Audio Classification Methods ......................................................... 6 
2.2 The Proposed Audio Segmentation Method ........................................................ 17 
2.3 The Proposed Content-based Audio Retrieval System ....................................... 24 
3. Experimental Results and Discussion............................................................................. 31 
3.1 Audio Classification ............................................................................................... 31 
3.2 Audio Segmentation ............................................................................................... 32 
3.3 Content-based Audio Retrieval System................................................................ 35 
參考文獻........................................................................................................................................ 38 
計畫成果自評................................................................................................................................ 41 
可供推廣之研發成果資料表........................................................................................................ 42 
 
 
2 
However, two-type classification for audio data is not enough in many applications, such as 
content-based video retrieval [11]. Recently, video retrieval has become an important research 
topic. To raise the retrieval speed and precision, a video is usually segmented into several scenes 
[11,14]. In general, neighboring scenes will have different types of audio data. Thus, if we can 
develop a method to classify audio data, the classified results can be used to assist scene 
segmentation. Different kinds of videos will contain different types of audio data. For example, in 
documentaries, commercials or news report, we can usually find the following audio types: 
speech, music, speech with musical or environmental noise background, and song. Thus, besides 
speech and music, it is necessary to take other kinds of sounds into consideration in many 
applications.  
The classifier proposed by Wyse and Smoliar [7] classifies audio signals into “music”, 
“speech”, and “others”. It was developed for the parsing of news stories. In [8], audio signals are 
classified into speech, silence, laughter, and non-speech sounds for the purpose of segmenting 
discussion recordings in meetings. The above-mentioned approaches are developed for specific 
scenarios, only some special audio types are considered and the accuracy of the segmentation 
resulted using this method varies considerably for different types of recording. Besides the 
commonly studied audio types such as speech and music, the research in [12-14] has taken more 
general types of audio data into account, e.g., the speech signal with the music background and 
the singing of a person, which contain more than one basic audio type and usually appear in 
documentaries or commercials. 
In [12], 143 features are first studied for their discrimination capability. Then, the 
cepstral-based features such as Mel-frequency cepstral coefficients (MFCC), linear prediction 
coefficients (LPC), etc., are selected to classify audio signals. The authors concluded that in many 
cases, the selection of features is actually more critical to the classification performance. Zhang 
and Kuo [14] extracted some audio features including the short-time fundamental frequency and 
the spectral tracks by detecting the peaks from the spectrum. The spectrum is generated by 
autoregressive model (AR model) coefficients, which are estimated from the autocorrelation of 
audio signals. Then, the rule-based procedure, which uses many threshold values, is applied to 
classify audio signals into speech, music, song, speech with music background, etc. However, this 
method is complex and time-consuming due to the computation of autocorrelation function. 
Besides, the thresholds used in this approach are empirical, they are improper when the source of 
audio signals is changed. 
As for the audio segmentation, most of the existing approaches for audio segmentation can 
be classified into two major paradigms: temporal segmentation and classification-based 
segmentation. Temporal segmentation is a more primitive process than classification-based 
segmentation since it does not try to interpret the data. By contrast, the classification-based 
segmentation divides an audio sequence into semantic scenes called “audio scene” and to index 
them as different audio classes. That is, the approaches via classification usually adopt 
classification results to achieve segmentation purpose and the performance is dependent on the 
classification result. In this project, based on the proposed classification method, we will present 
4 
to achieve the classification purpose.  
 
Single-Type
Sounds
Hybrid-Type
Sounds
Pure Speech
Music
Speech with Noise Background
Speech with Music Background
SongAudio 
Signal
 
Fig. 1. The hierarchical classification scheme. 
 
The second proposed method is a non-hierarchical audio classifier, which will first divide an 
audio stream into clips, each of which contains one-second audio information. Based on the 
classified clips with smaller length, the proposed method is suitable and can be used to support 
classification-based audio segmentation. 
Generally speaking, the spectrogram is a good representation for an audio signal since it is 
often visually interpretable. By observing a spectrogram, we can find that the energy is not 
uniformly distributed, but tends to cluster to some patterns. All curve-like patterns are called 
tracks. Fig. 2(a) shows that for a music signal, some line tracks corresponding to tones will exist 
on its spectrogram. Fig. 2(b) shows some patterns including clicks (broadband, short time), noise 
burst (energy spread over both time and frequency), and frequency sweeps in a song spectrogram. 
Thus, if we can extract some features from a spectrogram to represent these patterns, the 
classification should be easy. Based on these phenomena, the proposed method will adopt feature 
selection process to explore the features with the highest discriminative ability to achieve 
classification purposes and will be used to do audio segmentation. Smith and Serra [15] proposed 
a method to extract tracks from a STFT spectrogram. Once the tracks are extracted, each track is 
classified. However, tracks are not well suited for describing some kinds of patterns such as 
clicks, noise burst and so on. To treat all kinds of patterns, a richer representation is required. In 
fact, these patterns contain various orientations and spatial scales. For example, each pattern 
formed by lines (see Fig. 2 (a)) will have a particular line direction (corresponding to orientation) 
and width (corresponding to spatial scale) between two adjacent lines; each pattern formed by 
curves (see Fig. 2 (b)) contains multiple line directions and a particular width between two 
neighboring curves. Since Gabor wavelet transform provides an optimal way to extract those 
orientations and scales, in this project, we used the Gabor wavelet functions to extract the  
features to represent those patterns. 
 
6 
phases: time-frequency distribution (TFD) generation, initial feature extraction, feature selection 
and similarity measurement. First, the input audio stream is transformed to a spectrogram and 
divided into clips, each of which contains one-second audio information and will meet the human 
auditory system (HAS) [29]. Second, for each clip with one-second window, a set of initial 
frame-based features are extracted based on the Gabor wavelet filters [27-28]. Third, based on the 
extracted initial features, the Singular Value Decomposition (SVD) [25] is used to perform the 
feature selection and to reduce the feature dimension. Finally, a similarity measuring technique is 
provided to perform pattern matching on the resulting sequences of feature vectors. 
 
2.1 The Proposed Audio Classification Methods 
 
The system diagram of the proposed audio classification method is shown in Fig. 4. It is 
based on the spectrogram and consists of three phases: feature extraction, the coarse-level 
classification and the fine-level classification. First, an input audio clip is transformed to a 
spectrogram by Short Time Fourier Transform and four effective audio features are extracted. Fig. 
5(a)-5(e) show five examples of the spectrograms of music, speech with music background, song, 
pure speech, and speech with environmental noise background, respectively. Then, based on the 
first feature, the coarse-level audio classification is conducted to classify audio signals into two  
 
Speech
with NB
Speech
with M B
Song
M usic
Pure
Speech
Fine-Level
Classification
Phase 1
Phase 2
Audio Signal
Feature  Extraction
Coarse-Level
Classification
Single-Type
Sounds
Hybrid-Type
Sounds
Energy
Distribution
M odel
Horizontal
Profile
(Variance)
Horizontal
Profile
(3rd moment)
Temporal
Interval
(Variance)
 
Fig. 4.  Block diagram of the proposed system, where “MB” and “NB” are the 
abbreviations for “music background” and “noise background”, respectively. 
8 
 
 
(d) 
 
(e) 
Fig. 5.  Five spectrogram examples. (a) Music. (b) Speech with music background. (c) 
Song. (d) Speech. (e) Speech with environmental noise background. (Continued) 
 
2.1.1. Feature Extraction Phase 
 
Four kinds of audio features are used in the proposed method, they are energy distribution 
model, variance and the third moment associated with the horizontal profile of the spectrogram, 
and variance of the differences of temporal intervals (which will be defined later). To get these 
features, the audio spectrogram for an audio signal is constructed first. Based on the spectrogram, 
these four features are extracted and described as follows. 
 
2.1.1.1 The Energy Distribution Model 
 
For the purpose of characterizing single-type and hybrid-type sounds, i.e., with or without 
background components, the energy distribution model is proposed. The histogram of a 
spectrogram is also called the energy distribution of the corresponding audio signal. In our 
10 
Step 4. If 5≤− µp , T = unimodel, go to Step 9. 
Else 
Use µ  to set the search range pℜ  as follows: 
⎩⎨
⎧
>−
<+=ℜ µµσµ
µσµµ
pif
pif
p ,),[
,],(
. 
      End if. 
Step 5. Find the position q  of the highest peak )(qh  within pℜ . 
Step 6. Find the position v  of the lowest valley )(vh  in the range between p  and q . 
Step 7. Set qpdst −= . 
Step 8. Set T = bimodel if the following two conditions are satisfied 
       Condition 1: 
2
σ≥dst . 
       Condition 2: )(
2
1)( phqh ≥  and )(
5
6)( vhqh ≥ . 
       Else T = unimodel.  
Step 9. Output T and assign µ  to T1, µ  + σ  to T2.  
End of Algorithm 1. 
 
Through the model decision algorithm described above, the model type for an audio signal 
can be determined. Note that in the algorithm, except the model type extracted, two parameters, 
T1 and T2, which will be used later, will be also obtained. 
 
2.1.1.2 The Horizontal Profile Analysis 
 
In this section, we will base on two facts to discriminate an audio clip with or without music 
components. One fact is that if an audio clip contains musical components, we can find many 
horizontal long-line like tracks (see Figs. 5(a)-5(c)) in its spectrogram. The other fact is that if an 
audio clip does not contain musical components, most energy in the spectrogram of each frame 
will concentrate on a certain frequency interval (see Figs. 5(d)-5(e)). Based on these two facts, 
two novel features will be derived and used to distinguish music from speech. 
12 
     
(c)                                      (d) 
 
(e) 
Fig. 7.  Five examples of the horizontal profiles. (a)-(e) are the horizontal profiles of Figs. 
5(a)-5(e), respectively. (Continued) 
 
2.1.1.3 The Temporal Intervals 
 
Up to now, we have provided three features. By processing the audio signals through these 
features, all audio signals can be classified successfully except the simultaneous speech and 
music category, which contains two kinds of signals: speech with music background and song. To 
discriminate these, a new feature is provided. One important characteristic to distinguish them is 
the duration of the music-voice.  
The duration of music-voice is defined as the duration of music appearing with human voice 
simultaneously. That is, two successive durations of music-voice is separated by the duration of a 
14 
 
(d) 
Fig. 8.  Two examples of the filtered spectrogram. (a) The spectrogram of song. (b) The 
filtered spectrogram of (a). (c) The spectrogram of speech with music background. (d) The 
filtered spectrogram of (c). (Continued) 
 
By observing the spectrogram in different frequency bands, we can see that music-voice (i.e. 
speech and music appears simultaneously) has more energy in the neighboring middle frequency 
bands, while music without voice will possess more energy in the lower frequency band. These 
phenomena are shown in Fig. 8. 
Based on these phenomena, the property of the duration of each continuous part of the 
simultaneous speech and music in a sound is used to discriminate the speech with music 
background from song. First, a novel feature associated with the temporal interval is derived. The 
temporal interval is defined as the duration of a continuous part of music-voice of a sound. Note 
that the signal between two adjacent temporal intervals will be music without human voice. 
Based on the phenomenon of the energy distribution in different frequency bands described 
previously, an algorithm will be proposed to determine the continuous music-voice parts in a 
sound. Note that some frequency noises usually exist in an audio clip, i.e., these noises will 
contribute to those frequencies with lower energy in spectrogram. In order to avoid the influence 
of frequency noise, a filtering procedure is applied in advance to get rid of those with lower 
energy. The proposed filtering procedure is provided and described as follows. 
 
Filtering Procedure: 
1) Filter out the higher frequency components with lower energy: 
For the spectrogram of each frame τ , ),( ωτS , find the highest frequency hω  with 
2),( TS h >ωτ . Set 0),( =
∧ ωτS , hωω >∀ . 
2) Filter other components: 
16 
2.1.2. Audio Classification 
 
Since there are some similar properties among most of the five classes considered, it is hard 
to find distinguishable features for all of these five classes. To treat this problem, a hierarchical 
system is proposed. It will do coarse-level classification first, then the fine-level classification is 
performed. To meet the aim of on-line classification, features described above are computed on 
the fly with incoming audio data. 
 
2.1.2.1 The Coarse-Level Classification 
 
The aim of coarse-level audio classification is to separate the five classes into two categories 
such that we can find some distinguishable features in each category. Based on the energy 
distribution model, audio signals can be first classified into two categories: single-type and 
hybrid-type, i.e., with or without background components. Single-type sounds contain pure 
speech and music. And hybrid-type sounds contain song, speech with environmental noise 
background and speech with music background. 
 
2.1.2.2 The Fine-Level Classification 
 
The coarse-level classification stage yields a rough classification for audio data. To get the 
finer classification result, the fine-level classifier is conducted. Based on the extracted feature 
vector X, the classifier is designed using a Bayesian approach under the assumption that the 
distribution of the feature vectors in each class kw  is a multidimensional Gaussian distribution 
),( kkk CmN . The Bayesian decision function [15] for class kw , )(Xdk  has the form: 
)()(
2
1ln
2
1)(ln)( 1 kk
T
kkkk mXCmXCwPXd −−−−= −  ,      (1) 
where km  and kC are the mean vector and covariance matrix of X, and )( kwP  is the priori 
probability of class kw . For a piece of sound, if its feature vector X satisfies )()( XdXd ji >  
for all ij ≠ , it is assigned to class iw . 
The fine-level classifier consists of two phases. During the first phase, we take ( idPv , idPm ) 
as the feature vector X and apply Bayesian decision function to each of the two coarse-level 
classes separately. For each audio signal of the single-type class, we can successfully classify it as 
music or pure speech. And the classification is well done without needing any further processing. 
For that of the hybrid-type sounds, which may be speech with environmental noise background, 
speech with music background or song, the same procedure is applied. Speech with 
18 
2.2.1 Initial Feature Extraction 
 
Generally speaking, the spectrogram is a good representation for the audio since it is often 
visually interpretable. By observing a spectrogram, we can find that the energy is not uniformly 
distributed, but tends to cluster to some patterns (see Fig. 11(a), 11(b)). All curve-like patterns are 
called tracks [31]. Fig. 11(a) shows that for a music signal, some line tracks corresponding to 
tones will exist on its spectrogram. Fig. 11(b) shows some patterns including clicks (broadband, 
short time), noise burst (energy spread over both time and frequency), and frequency sweeps in a 
song spectrogram. 
 
      
(a)                              (b) 
Fig. 11.  Two examples to show some possible different kinds of patterns in a spectrogram. 
(a) Line tracks corresponding to tones in a music spectrogram. (b) Clicks, noise burst and 
frequency sweeps in a song spectrogram. 
 
Thus, if we can extract some features from a spectrogram to represent these patterns, the 
classification should be easy. Smith and Serra [32] proposed a method to extract tracks from a 
STFT spectrogram. Once the tracks are extracted, each track is classified. However, tracks are not 
well suited for describing some kinds of patterns such as clicks, noise burst and so on. To treat all 
kinds of patterns, a richer representation is required. In fact, these patterns contain various 
orientations and spatial scales. For example, each pattern formed by lines (see Fig. 11(a)) will 
have a particular line direction (corresponding to orientation) and width (corresponding to spatial 
scale) between two adjacent lines; each pattern formed by curves (see Fig. 11(b)) contains 
multiple line directions and a particular width between two neighboring curves. Since Gabor 
wavelet transform provides an optimal way to extract those orientations and scales [27], in this 
chapter, we will use the Gabor wavelet functions to extract some initial features to represent those 
patterns. The detail will be described in the following section. 
 
Tones 
Noise Burst 
Clicks 
Frequency 
Sweeps 
20 
 
To extract the audio features, each Gabor wavelet filter, ),( yxgmn , is first applied to the 
spectrogram ),( yxI  to get a filtered spectrogram, ),( yxWmn , as 
111111 ),(*),(),( dydxyxgyyxxIyxW mnmn ∫ −−= ,              (8) 
where * indicates the complex conjugate. The above filtering process is executed by FFT (fast 
Fourier Transform). That is 
{ } { }{ }),(),(),( 1 yxIFyxgFFyxW mnmn ⋅= − .                    (9) 
Since peripheral frequency analysis in the ear system roughly follows a logarithmic axis, in 
order to keep with this way, the entire frequency band [0, Fs/2] is divided into six subbands of 
unequal width: F1=[0, Fs/64], F2=[Fs/64, Fs/32], F3=[Fs/32, Fs/16], F4=[Fs/16, Fs/8], F5=[Fs/8, 
Fs/4], and F6=[Fs/4, Fs/2]. In our experiments, high frequency components above Fs/4 (i.e., 
subband [Fs/4, Fs/2]) are discarded to avoid the influence of noise. Then, for each interested 
subband iF , the directional histogram, ),( nmiH , is defined to be 
∑
=
= 5
0
),(
),(
),(
n
i
i
i
nmN
nmN
nmH , 4,,0"=i ,                        (10) 
,
,0
),(,1
),(
⎩⎨
⎧ ∈>=
otherwise
FyandTyxWif
yxW immnimn                   (11) 
                        
,),(),( ∑∑=
x y
i
mni yxWnmN                                  (12) 
where .6,,0"=m  and .5,,0"=n . Note that ),( nmNi  is the number of pixels in the filtered 
spectrogram ),( yxmnW  at subband iF , scale m and direction n with value larger than threshold 
mT . mT  is set as 
mmmT σµ += .                                        (13) 
where 
∑
=
∑∑= 5
0
),(
n
mN
x y
yxmnWmµ , 
2
1
5
0
/2)),(( ⎟⎟⎠
⎞
⎜⎜⎝
⎛
∑
=
∑∑ −=
n x y
mNmyxmnWm µσ , and 
mN  is the number of pixels over all the 6 filtered spectrogram ),( yxmnW  with scale m.  
22 
VSV
VSV
V
w
T
b
T
V
opt maxarg= .                         (18) 
In fact, { }121 ,,, −Cvvv "  is the set of generalized eigenvectors of bS  and wS  
corresponding to the 1−C  largest generalized eigenvalues { }1,,2,1 −= Cii …λ  [17], i.e., 
iwiib vSvS λ= .                               (19) 
Note that in this chapter, two classes and five classes (i.e., 2=C  and 5=C ) are used and 
one-second audio clip is taken as the basic classification unit. 
Based on optV , the initial feature vector for each one-second audio clip in the training data 
and testing data is projected to the space generated by optV  to get a new feature vector 
'f  with 
dimension C-1. 'f  is then used to stand for the audio clip. Before classification, it is important 
to give a good similarity measure. In our experiments, the Euclidean distance worked better than 
others (e.g., Mahalanobis, covariance, etc.). For each test sample, jx  with feature vector 
'
jf , 
the Euclidean distance between the test sample and the class center of each class in the space 
generated by optV  is evaluated. Then the sample is assigned to the class with minimum distance. 
That is, jx  is assigned as class 
'
jC  according to the following criterion: 
CifC ij
i
j ,,2,1,minarg
''' "=−= µ , ,                    (20) 
where 'iµ  is the mean vector of the projected vectors of all test samples in class i. Fig. 12 shows 
an example of using a two-way speech/music discriminator. In the figure, “x” stands for the 
projected result of an music signal, “o” stands for the projected result of a speech signal. From 
this figure, we can see that through FLD, music and speech samples can be easily separated. Fig. 
13 outlines the process of feature selection and classification.  
 
24 
 
2.2.1.4 Segmentation 
 
The segmentation is to divide an audio sequence into semantic scenes called “audio scene” 
and to index them as different audio classes. Due to some classification errors, a reassigning 
algorithm is first provided to rectify these classification errors. For example, if we detect a pattern 
like speech-music-speech, and the music subpattern lasts a very short time, we can conclude that 
the music subpattern should be speech. First, for each one-second audio clip, the similarity 
measure between the audio clip and the center of its class is defined as 
∑ =
−= 5
1
min1
j jdist
distSimilarity , ,minmin jj distdist =                 (21) 
where jdist  is the Euclidean distance between the clip and the 
thj  class center in the feature 
space. If the similarity measure is less than 0.9, mark the clip as ambiguous. Note that ambiguous 
clips often arise in transition periods. For example, if a transition happens when speech stops and 
music starts, then each clip in the transition will contain both speech and music information. Then, 
each ambiguous clip will be reassigned as the class of the nearest unambiguous clip. After the 
reassignment is completed, all neighboring clips with the same class are merged into a segment. 
Finally, for each audio segment, the length is evaluated. If the length is shorter than the threshold 
T (T=3 second), each clip in the segment is reassigned as the class of one of its two neighboring 
audio segments with the least Euclidean distance between the clip and the center of class of the 
selected neighboring segment. 
 
2.3 The Proposed Content-based Audio Retrieval System 
 
The block diagram of the proposed method is shown in Fig. 14. It is based on the 
spectrogram and consists of four phases: time-frequency distribution (TFD) generation, initial 
feature extraction, feature selection and similarity measurement. First, the input audio is 
transformed to a spectrogram, ),( yxI  by the Multi-resolution Short Time Fourier Transform. 
Second, for each clip with one-second window, some Gabor wavelet filters will be applied to the 
resulting spectrogram to extract a set of initial features. Third, based on the extracted initial 
features, the Singular Value Decomposition (SVD) [25] is used to perform the feature selection 
and to reduce the feature dimension. Finally, based on the selected features, a similarity measure 
is provided to measure the similarity of audio data. In what follows, we will describe the details 
of the proposed method. 
 
26 
by lines (see Fig. 15(a)) will have a particular line direction (corresponding to orientation) and 
width (corresponding to spatial scale) between two adjacent lines; each pattern formed by curves 
(see Fig. 15(b)) contains multiple line directions and a particular width between two neighboring 
curves. Since Gabor wavelet transform provides an optimal way to extract those orientations and 
scales [29], in this chapter, we will use the Gabor wavelet functions to extract some initial 
features to represent the needed patterns. The detail will be described in the following sections. 
 
2.3.1.1 Feature Estimation 
 
In this section, we will deal with musical audio signal including musical instrument and 
song. Most of the current works only deal with the monophonic sources, in this section we will 
also consider polyphonic music. Polyphonic music is more common, but it is also more difficult 
to represent. The most meaningful feeling of human perception for the music data is primarily the 
pitch and timbre. Both of them are correlated with the tones. For example, the fundamental tone 
decides the pitch that we hear, and the harmonics decide the timbre. Based on the 
above-observation for the spectrogram (see Fig. 15(a) and 15(b)), we find that some line tracks 
corresponding to tones will exist in the spectrogram. Thus, if we can extract the features about 
tones, the retrieval should be easy. 
Since through our observation, most prominent tracks are near horizontal, we only take one 
orientation that is horizontal. Thus, each Gabor wavelet filter, ),( yxgmn , can be briefly 
represented by ),( yxgm . Note that in our experiments, we set 64
3=lω , 43=hω , 1=K  
and 7=S . To extract the audio features, each Gabor wavelet filter, ),( yxgm , is first applied to 
the spectrogram ),( yxI  to get a filtered spectrogram, the spectrum of which is represented by 
),( vumW  called spectrogram spectrum. That is 
{ } { }),(),(),( yxIFyxgFvuW mm ⋅= ,                      (22) 
where {}⋅F  is a fast Fourier Transform. 
Up to now, there are S spectrogram spectrum with scale m, ),( vumW , to be available. Since, 
in each audio signal, those tracks appear in the corresponding spectrum have a certain scale, not 
all these spectrogram spectrum are used to perform the feature estimation, only the one with the 
maximum contrast (which corresponds to the track scale) is used. To reach this goal, the vertical 
profile of the spectrum, )(uPm  ( Sm ,,2,1 "= ), is constructed as follows: 
∑=
v
mm vuWuP ).,()(                                (23) 
Let PM  be the number of the local peaks ( .,,, 21 PMuuu " ) in )(uPm , )( im uP  
28 
    
(a)                              (b) 
Fig. 16.  An example to show the enhancement process performing in a spectrogram. (a) 
The Gabor-wavelet filtered spectrogram with the maximum contrast. (b) Enhanced spectrogram. 
 
2.3.1.2 Feature Selection and Representation 
 
The initial features are not used directly for similarity measurement since some features give 
poor separability among different objects and inclusion of these features will lower down the 
system performance. In addition, some features are highly correlated so that redundancy will be 
introduced. To remove these disadvantages, in this chapter, the Singular Value Decomposition 
(SVD) [23] is applied to the initial features to find those uncorrected features with the highest 
separability.  
As for the SVD, it is a well-known technique for reducing the dimensionality of data while 
retaining maximum information content. It decomposes the data into a sum of vector outer 
products with vectors representing both the basis function (eigenvectors) and the projected 
features (eigen coefficients). A subset of the complete basis is selected to reduce data 
dimensionality. The loss of information is minimized because the basis functions are ordered by 
statistical salience; thus, functions with low information content are discarded. 
 Based on SVD, the initial feature vector, f , for each one-second audio clip can be 
decomposed into the form [28]: 
,tUSVf =                             (29) 
where S  is a diagonal matrix containing the singular values of f  along its diagonal, and the 
columns of U  and V  are the eigenvectors (the basis function) of tff , and ff t  respectively. 
Then the basis, V , is reduced by retaining only the first k  basis functions. That is 
].,,,[ 21 kk vvvV "=                     (30) 
And the initial feature vector f  is projected to the space generated by kV  to get a new 
feature vector 'f  with the reduced dimension. 'f  is then used to stand for the audio clip as 
follows: 
,][ 21
'
k
t'
M
'' ,,, fVxxxf == "               (31) 
30 
where lj ,,2,1 "=  and ' ,' , ijiq xx −  stands for the Euclidean distance between two vectors: ' ,iqx  
and ' ,ijx . Then for all j , sort jqDist ,  in an increasing order. For the top g  clips, we define 
their grades, jqGd , , as ,,2,1, "−− ggg  and 1, respectively. The clip with the least distance 
will have the highest grade and be considered as the most similar one. In addition, jqGd ,  of all 
other clips are defined as zero. Note that in this chapter, one-second audio clip is taken as the 
basic distance measurement unit. 
 
2.3.2.2 Retrieval 
 
For a query audio sequence, qy , with length p-seconds, it is first divided into p successive 
one-second clips. That is 
].,,,[ 21 pqqqq yyy "=y                                (34) 
Next, for each clip iqy  ( pi ,,2,1 "= ) and a candidate audio sequence cy ,  the similarity 
measure is first performed and the corresponding grades, i jqGd ,  ( pi ,,2,1 "= and lj ,,2,1 "= ), 
are evaluated based on Eq. (4.12). According to these grades, the total grade of the clip iqy , 
i
dq i
TGd ,_ , is defined to be 
p
ipdq
i
dq
i
dq
i
dqidq
i
dq iiiiii
GdGdGdGdGdTGd −+
+
+
−
−+− ++++++= ,1 1,,1 1,1 1,,_ "" ,     (35) 
where i jq
j
i Gdd ,maxarg=  and is the candidate for matching.  
Finally, based on the set of total grades, i dq iTGd ,_ ( pi ,,2,1 "= ), the global similarity is 
defined as 
,
_
_max
1 1
1
,
,
∑−
=
−= p
i
i
cq
i
cq
TDGd
TDGd
Sim                                         (36) 
where .___ 1,,,
+−= i dqi dqi dq iii TGdTGdTDGd ( .1,,2,1 −= pi " ). If the global similarity, Sim , is 
32 
The training is done using 50% of randomly selected samples in each audio type, and the test is 
operated on the remaining 50%. By changing training set several times and evaluating the 
classification rates, we find that the performance of the system is stable and independent on the 
particular test and training sets. Note that the experiments are carried out on a Pentium II 400 
PC/Windows 2000, it needs less than one twentieth of the time required to play the audio clip for 
processing an audio clip. The only computational expensive part is the spectrogram, and the other 
processing is simple by comparison (e.g. variances, peak finding, etc). In order to do comparison, 
we also like to cite the efficiency of the existing system described in [14], which also includes the 
five audio classes considered in our method and uses similar database to ours. The authors of [14] 
report that less than one eighth of the time required to play the audio clip are needed to process an 
audio clip. They also report that their accuracy rates are more than 90%. 
 
Table 1.  COARSE-LEVEL CLASSIFICATION RESULTS. 
Audio Type Number Correct Rates 
Pure Speech 200 100% Single-Type 
Sounds Pure Music 200 100% 
Song 200 100% 
Speech with MB 50 100% 
Hybrid-type 
Sounds 
Speech with NB 50 100% 
 
Table 2.  FINAL CLASSIFICATION RESULTS. 
Audio Type Number Correct Rates 
Pure Speech 200 100% Single-Type 
Sounds Pure Music 200 97.6% 
Song 200 98.53% 
Speech with MB 50 96.5% 
Hybrid-type 
Sounds 
Speech with NB 50 100% 
  
3.2 Audio Segmentation 
 
In order to do comparison, we have collected a set of 700 generic audio pieces as the testing 
database. Note that we take one-second audio signal as a test unit. Besides, we also collected a set 
of 15 longer audio pieces recorded from movies, radio or video programs. These pieces last from 
34 
 
Fig. 17.  Demonstration of audio segmentation and indexing, where “SMB” and “SNB” are 
the abbreviations for “speech with music background” and “speech with noise background”, 
respectively. 
 
 
Fig. 18.  Final results after applying the segmenting algorithm 
 
Listed in Table 5 is the result of the audio segmentation, where miss-rate and over-rate are 
defined as the ratio between the number of miss-segmented ones and the actual number of 
segments, and the ratio between the number of over-segmented ones and the actual number of 
segments in audio streams, respectively. Besides, error rate is defined as the ratio between the 
number of segments indexed in errors and the actual number of segments in audio stream. The 
first column shows the segmentation result without applying the reassignment process to the 
classification result, and the second column shows the segmentation result using the reassignment 
process. 
 
 
 
 
36 
total number of relevant items (i.e. correctly retrieved items and the relevant items that have not 
been retrieved) and K  is the total number of the retrieved items. The recall rate is typically used 
in conjunction with the precision rate, which measures the fraction of the retrieved patterns that is 
relevant. The precision and recall rate can often be traded-off. That is one can achieve high 
precision rate and low recall rate or the other way round. 
Tables 6 and 7 show the results of two experiments presented in this system. 
 
Table 6.  THE AVERAGE RECALL RATES OF THE FIRST EXPERIMENT 
Query Sample Length Basic Function  
Numbers One second Two seconds Three seconds 
5 29% 71% 74% 
10 31% 75% 75% 
15 40% 98% 98% 
 
Table 7.  THE AVERAGE RECALL RATES OF THE SECOND EXPERIMENT 
Query Sample Length Basic Function  
Numbers One second Two seconds Three seconds 
5 31% 71% 72% 
10 31% 71% 74% 
15 38% 94% 94% 
 
In our experiments, the number of retrieved patterns was adjusted to the number of relevant 
patterns, so the precision rate and recall rate are the same. From Table 6, we can see that the 
above-mentioned two factors affect the performance of the proposed approach. The more basis 
functions are used, the higher the recall rate will be. And the longer length of the query sample is 
used, the higher the recall rate will be. Based on the first experiment, we can see that it is best to 
perform retrieval using 15 basis functions and two-second length of query sample. From Table 7, 
we can also see the same phenomena as Table 6 except for the lower recall rate. 
Besides, by examining the occurrence of missing in the experiments based on human 
judgement as the ground truth, we found two major factors. First, for the first experiment, we find 
that some errors occur in those searched clips containing a transition, which is made due to that 
we simply segment an audio object into several one-second clips uniformly against pre-dividing 
the audio object into sequences of audio phrases. As a matter of fact, this kind of errors can be 
reduced by increasing the length of query sequence (i.e., clip number) to get more related 
38 
參考文獻 
 
[1]  S. Pfeiffer, S. Fischer, and W. Effelsberg, “Automatic audio content analysis,” in Proc. ACM 
Multimedia’96, Boston, MA, April 1996, pp. 21-30. 
[2] J. Foote, “An overview of audio information retrieval,” ACM Multimedia Systems, vol. 7, 
no. 1, pp. 2-11, January 1999 . 
[3] E. Scherier and M. Slaney, “Construction and evaluation of a robust multifeature 
speech/music discriminator,” in Proc. Int. Conf. Acoustics, Speech, Signal Processing’97, Munich, 
Germany, April 1997, pp. 1331-1334. 
[4] S. Rossignol, X. Rodet, and J. Soumagne et al., “Feature extraction and temporal 
segmentation of acoustic signals,” in Proc. ICMC 98, Ann Arbor, Michigan, 1998, pp. 199-202. 
[5] J. Saunders, “Real-time discrimination of broadcast speech/music,” in Proc. Int. Conf. 
Acoustics, Speech, Signal Processing’96, vol. 2, Atlanta, GA, May 1996, pp. 993-996. 
[6] I. Fujinaga, “Machine recognition of timbre using steady-state tone of acoustic instruments,” 
in Proc. ICMC 98, Ann Arbor, Michigan, 1998, pp. 207-210. 
[7] L. Wyse and S. Smoliar, “ Toward content-based audio indexing and retrieval and a new 
speaker discrimination technique,” in Proc. ICJAI’95, Singapore, December 1995. 
[8] D. Kimber and L. D. Wilcox, “Acoustic segmentation for audio browsers,” in Proc. Interface 
Conf., Sydney, Australia, July 1996. 
[9] E. Wold, T. Blum, D. Keislar, and J. Wheaton, “Content-based classification, search, and 
retrieval of audio,” IEEE Multimedia Mag., vol. 3, no. 3, pp. 27-36, Fall 1996. 
[10] L. Guojun and T. Hankinson, “A technique towards automatic audio classification and 
retrieval,” in Proc. Int. Conf. Signal Processing’98, vol. 2, 1998, pp. 1142-1145.  
[11] J. S. Boreczky and L. D. Wilcox, “A hidden markov model framework for video 
segmentation using audio and image features,” in Proc. Int. Conf. Acoustics, Speech, Signal 
Processing’98, Seattle, May 1998, pp. 3741-3744. 
[12] D. Li, I. K. Sethi, N. Dimitrova, and T. McGee, “Classification of general audio data for 
content-based retrieval,” Pattern Recognition Letters, vol. 22, no. 5, pp. 533-544, April 2001. 
[13] T. Zhang and C.-C. J. Kuo, “Hierarchical classification of audio data for archiving and 
retrieving,” in Proc. Int. Conf. Acoustics, Speech, Signal Processing’99, vol. 6, 1999, pp. 
3001-3004. 
[14] T. Zhang and C.-C. J. Kuo, “Audio content analysis for online audiovisual data segmentation 
and classification,” IEEE Transactions on Speech and Audio Processing, vol. 9, no. 4, pp. 
441-457, May 2001. 
40 
Inc., 1998. 
[32] J. Smith M. and X. Serra, “An analysis/resynthesis program for non-harmonic sounds based 
on a sinusoidal representation,” in Proc. ICMC 87, Ann Arbor, Michigan, 1987, pp. 290ff. 
[33] N. Peter Belhumeur, and David J. Kriegman, “Eigenfaces vs. fishfaces: recognition using 
class specific linear projection,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 
vol. 19, no. 7, pp. 711-720, July 1997. 
 
 
已發表著作 
 
本計畫已有下列著作於國外期刊發表： 
 
(1) Ruei-Shiang Lin and Ling-Hwei Chen, “A New Approach for Classification of Generic 
Audio Data,” International Journal of Pattern Recognition and Artificial Intelligence, vol. 19, no. 
1, pp. 63-78, Feb. 2005. 
(2) Ruei-Shiang Lin and Ling-Hwei Chen, “A New Approach for Audio Classification and 
Segmentation Using Gabor Wavelet Filtering and Fisher Linear Discriminator,” International 
Journal of Pattern Recognition and Artificial Intelligence, vol. 19, no. 6, pp. 807-822, Sep. 2005. 
(3) Ruei-Shiang Lin and Ling-Hwei Chen, “Content-based Retrieval of Audio Based on Gabor 
Wavelet Filtering,” International Journal of Pattern Recognition and Artificial Intelligence, vol. 
19, no. 6, pp. 823-837, Sep. 2005. 
 
42 
可供推廣之研發成果資料表 
□ 可申請專利  □ 可技術移轉                                      日期：96 年 8 月 30 日 
國科會補助計畫 
計畫名稱：個關於一般音訊資料之音訊分類，音訊分段及音訊檢索
之研究 
計畫主持人：陳玲慧 教授 
計畫編號：NSC 94-2213-E-009-091- 學門領域：影像處理 
技術/創作名稱 二類別音訊分類法及五類別音訊分類法 
發明人/創作人 陳玲慧，林瑞祥 
中文：本技術藉由將音訊轉換為聲紋圖(spectrogram)，並以 Gabor 
Wavelets 分析所轉換之聲紋圖，將音訊分類為語音(Speech)及音樂
(Music)兩個類別，或純語音(Pure speech)、音樂(Music)、歌曲
(Song)、有音樂背景之語音(Speech with music background)及有背景
噪音之語音(Speech with environmental noise background)五個類別。
技術說明 英文：The proposed two-way method classifies audio signals to speech 
or music, and the proposed five-way method classifies audio signals to 
five categories: pure speech, music, song, speech with music 
background, and speech with music background. In order to achieve 
the goal, the classifier converts audio signals to spectrograms. The 
spectrograms are analyzed by Gabor Wavelets to get the classification 
results. 
可利用之產業 
及 
可開發之產品 
音訊資料庫及檢索系統 
技術特點 
將音訊轉換為聲紋圖(spectrogram)，再以 Gabor Wavelets 分析之。
推廣及運用的價值 
本技術可作為音訊資料庫中之分類方法，或可提供音訊檢索系統作
為一音訊描述子(audio descriptor)。 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位研
發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
