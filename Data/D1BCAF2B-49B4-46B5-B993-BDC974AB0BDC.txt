 
 
中文摘要 
本計畫提出一套具備嬰幼兒臉部表情辨識與異物偵測的智慧型數位監護系統；當
嬰幼兒有異物入侵口鼻或是吐奶、嘔吐等危險情形發生時，系統能即時發出警告通知
監護者；也可經由表情來判別嬰幼兒目前是否有身體不適的狀況，來取代現階段全靠
人力監護的方式，減輕監護者的負擔。 
因此本系統可分成兩個子系統：表情辨識和異物偵測。表情辨識的部分，主要辨
識無表情、笑、哭三種表情。首先將影像中嬰幼兒的臉部特徵點擷取出來，藉由特徵
點計算出特徵距離，並將這些特徵距離當成輸入類神經系統的特徵值，即可辨識出嬰
幼兒的表情。異物偵測方面，是針對嬰幼兒發生嘔吐、口鼻被棉被或其他異物遮蓋之
偵測，所以我們可以藉由動態影像前後張在嘴巴附近區域的色彩變化，去偵測目前是
否有異物的入侵。 
實驗結果顯示，使用四核心的 PC（2.66GHz）去測試演算法找出人臉眼睛特徵點
的精確度及其所花費時間，精確度約為 88%、時間約是 45ms 而表情辨識的正確率約
為 80%。 
最後將此嬰幼兒監護系統實現於 ARM926EJ-S CPU 與 Xilinx FPGA 之嵌入式系統
平台上。並基於軟硬體協同設計的概念，抽出演算法裡運算複雜度最高的模組來做成
硬體加速 IP。當 ARM CPU 操作於 266MHz 且系統頻率為 50MHz 時其純 ARM 程式
碼版本的影格率（Frame rate）大約可達每秒 3.03 張，而軟硬體共設計版本的影格率也
可到每秒 1.86 張。 
 
 
 
 
目錄 
計畫摘要 ...................................................................................................................ii 
Abstract .................................................................................................................. iii 
目錄..........................................................................................................................iv 
表目錄 .....................................................................................................................vii 
圖目錄 ....................................................................................................................viii 
第一章  緒論 .........................................................................................................12 
1.1 研究動機與目的 ......................................................................................12 
1.2 相關研究回顧 ..........................................................................................12 
1.2.1 人臉偵測 ......................................................................................12 
1.2.2 特徵點擷取 ..................................................................................13 
1.2.3 表情辨識 ......................................................................................14 
1.3 系統概觀..................................................................................................14 
1.4 計畫架構....................................................................................................6 
第二章  預備知識..................................................................................................16 
2.1 色彩空間轉換(RGB 轉 YCbCr) ..............................................................16 
2.2 平滑濾波(Smooth filter) ........................................................................16 
2.3 邊緣偵測(Edge detection) .....................................................................17 
2.4 自商影像(Self Quotient Image) ..............................................................19 
2.5 直方圖等化(Histogram equalization) ....................................................21 
第三章  先前相關的研究 ......................................................................................24 
3.1 眼睛特徵點偵測 ......................................................................................24 
3.1.1 眼睛特徵點偵測流程...................................................................24 
3.1.2 降取樣(Downscale).....................................................................24 
3.1.3 眼睛濾波器(Eye filter) ...............................................................24 
3.1.4 辨別眼睛 ......................................................................................24 
3.1.5 取出眼睛的區域 ..........................................................................27 
3.1.6 偵測出眼睛特徵點.......................................................................28 
3.2 人臉擷取..................................................................................................28 
3.3 嘴巴特徵點偵測 ......................................................................................28 
3.3.1 嘴巴特徵點偵測流程...................................................................28 
3.3.2 取出嘴巴大概區域.......................................................................28 
3.3.3 偵測出嘴巴特徵點.......................................................................29 
3.4 眉毛特徵點偵測 ......................................................................................29 
3.4.1 眉毛特徵點偵測流程...................................................................29 
3.4.2 直方圖等化 ..................................................................................29 
3.4.3 偵測出眉巴特徵點.......................................................................29 
6.3 FPGA 硬體設計 ......................................................................................66 
6.3.1 IP 與 Wrapper 設計 ....................................................................66 
6.3.2 Cross Filter 設計 .........................................................................68 
6.3.3 Cross Filter IP 合成結果 .............................................................70 
6.4 IP 驅動程式.............................................................................................70 
6.5 系統實現與效能分析 ..............................................................................74 
6.5.1 軟硬體整合之系統架構 ...............................................................74 
6.5.2 實體系統呈現 ..............................................................................75 
6.5.3 軟硬體整合之效能分析 ...............................................................75 
第七章  討論與比較..............................................................................................77 
7.1 實驗環境..................................................................................................77 
7.2 實驗結果..................................................................................................78 
第八章  結論與未來研究方向 ..............................................................................83 
8.1 結論 .........................................................................................................83 
8.2 未來研究方向 ..........................................................................................83 
參考文獻 .................................................................................................................84 
圖目錄 
圖 1-1 系統平台架構圖...........................................................................................15 
圖 1-2 系統流程圖 ..................................................................................................15 
圖 2-1 RGB 彩色影像 .............................................................................................16 
圖 2-2 YCbCr 影像 ..................................................................................................16 
圖 2-3 3x3 遮罩 .......................................................................................................17 
圖 2-4 平均濾波器結果...........................................................................................17 
圖 2-5 Sobel 垂直及水平的濾波遮罩 .....................................................................18 
圖 2-6 Sobel 濾波結果 ............................................................................................18 
圖 2-7 水平與垂直 Sobel 濾波結果 ........................................................................19 
圖 2-8 加權式高斯濾波器.......................................................................................19 
圖 2-9 加權式高斯濾波器設計 ...............................................................................20 
圖 2-10（a）受光不均的原始影像，（b）經過 SQI 處理後的影像 ......................21 
圖 2-11 直方圖等化示意圖 .....................................................................................21 
圖 2-12 機率密度函數分佈圖 .................................................................................22 
圖 2-13 累積分配函數分佈圖 .................................................................................22 
圖 2-14 直方圖等化結果.........................................................................................23 
圖 3-1 眼睛特徵點偵測流程圖 ...............................................................................23 
圖 3-2 十字節判斷區域...........................................................................................25 
圖 3-3 長方形區域大小...........................................................................................25 
圖 3-4 水平 Sobel 運算子結果 ................................................................................25 
圖 3-5 十字節於不同降取樣上還原為原圖大小 ....................................................25 
圖 3-6 以白色區域為搜尋座標，左圖為還原前，右圖為還原原圖大小..............26 
圖 3-7 在水平 Sobel 運算子上搜尋 ........................................................................26 
圖 3-8 十字節判斷為眼睛的結果 ...........................................................................27 
圖 3-9 聯集結果 ......................................................................................................27 
圖 3-10 填滿結果 ....................................................................................................28 
圖 3-11 垂直投影 ....................................................................................................28 
圖 3-12 眼睛上邊界與下邊界 .................................................................................28 
圖 3-13 水平 Sobel 影像的上、下邊界 ..................................................................28 
圖 4-1 左圖為臉上光線分布不均的人臉，中間為水平 Sobel 運算子結果，右邊為眼睛濾波器誤
判的結果 .................................................................................................................30 
圖 4-2 人臉特徵點分佈...........................................................................................30 
圖 4-3 眼睛特徵點偵測流程圖 ...............................................................................31 
圖 4-4 原始灰階影像經過 SQI 處理的結果 ...........................................................32 
圖 4-5 經水平 Sobel 運算結果 ................................................................................32 
圖 4-6 四種降取樣大小的影像 ...............................................................................32 
圖 4-44 垂直投影 ....................................................................................................45 
圖 4-45 水平投影 ....................................................................................................45 
圖 4-46 嘴巴特徵點偵測結果 .................................................................................46 
圖 4-47 眉毛特徵點偵測流程圖 .............................................................................46 
圖 4-48 等化區域 ....................................................................................................47 
圖 4-49 等化前與等化後的直方圖 .........................................................................47 
圖 4-50 等化前與等化後的影像 .............................................................................47 
圖 4-51 灰階值亮度搜尋區域 .................................................................................48 
圖 4-52 搜尋灰階值最大亮度的示意圖..................................................................48 
圖 4-53 二眉毛內側特徵點的水平座標..................................................................49 
圖 4-54 二眉毛內側特徵點的垂直座標示意圖 ......................................................49 
圖 4-55 眉毛特徵點偵測結果 .................................................................................50 
圖 4-56 人臉特徵點偵測結果 .................................................................................46 
圖 5-1 表情辨識流程圖...........................................................................................52 
圖 5-2 無表情、笑、哭的表情線索 .......................................................................53 
圖 5-3 特徵距離 ......................................................................................................49 
圖 5-4 倒傳遞類神經網路架構 ...............................................................................55 
圖 5-5 訓練樣本範本 ..............................................................................................56 
圖 5-6 測試影像範本 ..............................................................................................57 
圖 5-7 異物偵測流程圖...........................................................................................59 
圖 5-8 搜尋範圍示意圖...........................................................................................60 
圖 5-9 計算嘴巴附近區域 RGB 平均值的流程圖 ..................................................60 
圖 5-10 吐奶影片 ....................................................................................................62 
圖 5-11 棉被蓋住口鼻影片 .....................................................................................59 
圖 6-1 CDK 開發平台 .............................................................................................63 
圖 6-2 CDK 開發平台硬體方塊圖 ..........................................................................64 
圖 6-3 Xilinx FPGA 型號與可使用資源 ................................................................64 
圖 6-4 系統概要分析（Profiling）結果 .................................................................65 
圖 6-5 系統 mapping ...............................................................................................66 
圖 6-6 Cross Filter IP ...............................................................................................67 
圖 6-7 Cross Filter 電路 ...........................................................................................69 
圖 6-8 計算眼睛分數流程圖 ...................................................................................70 
圖 6-9 Linux 驅動程式架構 ....................................................................................69 
圖 6-10 配置記憶體 ................................................................................................72 
圖 6-11 定義主、次編號、FPGA 起始位址及 FPGA 大小 ...................................72 
圖 6-12 宣告主編號及次編號 .................................................................................73 
圖 6-13 裝置註冊及配置裝置主次編號..................................................................73 
圖 6-14 註銷裝置註冊與釋放裝置編號..................................................................73 
 12
第一章  緒論 
1.1 研究動機與目的 
 
由於目前的台灣人口結構分佈越趨老化、少子化，因此老人、嬰幼兒的即時照護就更受重
視。對於在家照顧嬰幼兒的母親、保母或在醫院的護士來說，無法時時刻刻都在嬰幼兒身旁。
對於突發狀況如：吐奶、嘔吐或呼吸器官阻塞等情況，有時無法在第一時間得到警訊並做處理。
因此若能發展一套嬰幼兒的即時監護系統，便能隨時監測嬰幼兒的表情與臉部異物偵測，倘若
有任何的不適，可以馬上通知母親或護士，讓嬰幼兒能得到最安全的照顧。 
 
智慧型嬰幼兒監護系統的研究與發展，不但可以對嬰幼兒的狀態做即時偵測，並且能在意
外發生時對監護人員發出警示訊息，進一步的提升嬰幼兒居家照護的安全性，來降低意外的發
生機率。目前有關於即時監護系統的研究方面，大部分研究的對象以病人、老人居多，國內比
較少有針對嬰幼兒的狀態來做偵測監護的專題研究。因此發展智慧型嬰幼兒監護系統，對未來
社會需求是極具重要且深負意義的研究主題。 
 
智慧型嬰幼兒監護系統主要分為二個部份，一個部份為嬰幼兒臉部異物偵測。此部份可即
時偵測出嬰幼兒嘴巴附近是否有異物出現而威脅到嬰幼兒生命安全。另一個部份則是嬰幼兒表
情的辨識。因嬰幼兒身體不適時無法用言語表達只能以哭鬧的方式來傳達其感受，所以我們可
藉由分析臉部表情來判斷嬰幼兒目前是處於何種狀態，如無表情、哭鬧、開心，若呈現哭鬧的
表情則表示嬰幼兒可能有身體不舒服的情況，可提醒看護人員去做妥善的處理。  
 
1.2 相關研究回顧 
 
一般而言，自動化表情辨識系統通常包含人臉偵測（Face detection）、特徵點擷取（Feature 
extraction）及表情辨識（Facial expression recognition）三個部份。以下將針對這三個部份常用
的方法做探討與分析。 
 
1.2.1 人臉偵測 
 
人臉偵測是人臉相關研究中，最先要面對的議題，這是因為無論是身份辨識、人臉追蹤或
表情辨識等，皆需要從影像中偵測並定位出人臉的區域，之後才能再進一步針對取得的人臉影
像進行分析。相關的研究方法很多，大概可區分為以下幾種類別： 
 
◆ 色彩分析方法（Color analysis method） 
因人臉顏色在一般狀況下比較特別，在影像中通常可以和其他顏色區分開來，如此即可減少搜尋人
臉的範圍，加快人臉偵測的速度，所以許多研究都用這種方式，藉由膚色來偵測人臉；但因為 RGB
色彩空間對光線的變化較為敏感，所以很多研究都會先做色彩空間的轉換（如 HSL、YCbCr、
YIQ、YUV…），來降低膚色對週遭光線的依賴程度。如[1][2][3][4][5][6]，此法的缺點在於當
背景有許多與膚色相近的非人物體時，則容易造成誤判，其優點是演算法簡單。 
 
◆ 樣板比對方法（Template matching method） 
 14
Zhang[23]以顏色與嘴唇的邊緣，利用 MRF（Markov random field）架構，將嘴唇特徵擷取
出來。 
 
◆ 小波轉換方法（Wavelet transform method） 
小波轉換法是先將影像轉至頻域，利用臉部肌肉的紋理來擷取特徵點。 
Bashyal[24]等人選擇重要的人臉特徵部位，並根據人眼觀察的方式手動定位特徵點，之後
再利用賈伯轉換（Gabor translation）對特徵點周遭進行紋理的擷取，最後利用學習向量量化網
路（Learning vector quantization network）進行表情分類。 
其他[25][26]方法也是使用小波轉換的方式來擷取特徵點。 
 
1.2.3 表情辨識 
 
將所需的人臉特徵點擷取出來以後，系統要對取出來的特徵做訓練，才能加以分類或辨識
成表情。根據其他研究的整理，現今主要的辨識方向如下： 
 
◆ 樣板比對方法（Template matching method） 
此方法僅適合針對特定的表情來做分類，因臉部肌肉有無限多組的組合方式去產生不同的
表情，不僅樣板不易建立，對系統沒有訓練過的人進行表情判斷也不易得到好的辨識結果。 
 
◆ 臉部動作編碼系統方法（Facial action coding system method） 
Ekman[27]等人利用 FACS 進行表情辨識，其主要的想法是辨識輸入的人臉表情中存在哪些
動作單元（Action unit），之後再根據辨識結果與之前定義的特定表情下動作單元的組成關係，
來決定輸入的影像是什麼表情。 
 
◆ 類神經網路方法（Neural network method） 
 類神經網路透過如同黑盒子般的處理方式，可以由網路的學習及權重值的自我調適，而不
用去面對複雜且不易分析的輸入輸出對應關係，所以此方法廣泛的被大家所採用
[28][29][30][31]。 
 
1.3 系統概觀 
 
 如圖 1-1 所示為智慧型嬰幼兒視訊監護系統平台架構圖，運作方式為：經由 Web Camera
將嬰幼兒的影像擷取下來後，透過 USB 介面傳輸影像至 Socle CDK 開發平台上做演算法處理，
最後將結果輸出到平台上的 LCD Panel。 
 16
 
第二章  預備知識 
2.1 色彩空間轉換(RGB 轉 YCbCr) 
 
因為由 RGB 三原色所構成的色彩空間，容易受到光線的影響，會隨著光線的變化而有很
大的變動，所以我們必須進行色彩空間轉換，找尋對亮度變化靈敏度低的色彩空間，來避免色
彩資訊會隨亮度的變動而有劇烈的變化。所以很多研究改採用對光線強弱較不敏感的色彩空間
來對顏色做分割，如：YCbCr、YIQ、YUV、HSL…等等。 
 
YCbCR 色彩空間是由修改 YUV 色彩空間所得到，其中 Y 代表亮度(Luminance)，Cb 及 Cr 代表
色度(Chrominance) 對亮度的分離性高，方便和色度分開操作，所以適合用於影像處理使用。它同時也
是 MPEG 視訊與 JPEG 影像的標準色彩空間。YCbCR 與 RGB 色彩空間的軟換公式如式(2.1)所示： 
 
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
+
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
−−
−−=
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
128
128
0
08.042.05.0
5.033.017.0
11.059.03.0
B
G
R
Cr
Cb
Y
  (2.1) 
圖 2-1 為一 RGB 色彩空間影像，將此影像利用式(2.1)轉換成 YCbCr 色彩空間影像，圖 2-2
由左至右分別為 Y 的結果、 Cb 的結果、Cr 的結果。 
  
 
圖 2-1 RGB 彩色影像 
 
   
(a) Y 影像    (b) Cb 影像    (c) Cr 影像 
圖 2-2 YCbCr 影像 
2.2 平滑濾波(Smooth filter) 
 
一般 Webcam 所拍攝的影像容易受到雜訊的影響，而這些雜訊會增加影像裡的高頻成份，
 18
如式(2.3)所示： 
 
x y
f ff i j f i f j
x y
∂ ∂∇ = + = +∂ ∂         (2.3) 
其中 xf 代表 x方向(水平)上的梯度分量， yf 代表 y 方向(垂直)上的梯度分量，其梯度向量的大小
如式(2.4)所示： 
 
[ ] 2/122 yx fff +=∇  
   
2/122
⎥⎥⎦
⎤
⎢⎢⎣
⎡
⎟⎟⎠
⎞
⎜⎜⎝
⎛
∂
∂+⎟⎠
⎞⎜⎝
⎛
∂
∂=
y
f
x
f
        (2.4) 
可近似為 
yx fff +≈∇           (2.5) 
而本論文所採用的是 Sobel 邊緣偵測的方式，其中 xf 與 yf 的近似分別由圖 2-5 的 2 個遮罩運算
所獲得： 
 
 
 
 
 
    
 
圖 2-5  Sobel 垂直及水平的濾波遮罩 
圖 2-6 為灰階影像經過 3x3 遮罩的 Sobel 濾波器的結果，圖 2-7 為其水平分量的結果與垂直
分量的結果。 
 
       
     (a) 灰階影像            (b) Sobel 濾波結果(水平+垂直) 
圖 2-6  Sobel 濾波結果 
-1 0 1
-2 0 2
-1 0 1
-1 -2 -1
0 0 0 
1 2 1 
 20
圖 2-9 為加權式高斯濾波器的一個設計範例，我們先計算 ΙΩ 區域中所有像素的平均值 τ，
並利用此平均值 τ 做為臨界值將 ΙΩ 區域影像轉為二值影像 W，之後把得到的 W 與 G 相乘計
算得到 F'，由於 F 需要符合低通濾波器的準則，所以我們需要將 F'做正規化處理。 
 
 
 
圖 2-9 加權式高斯濾波器設計 
接下來說明過濾光照變化部分的步驟，自商影像是利用多個不同的加權式高斯濾波器去過
濾影像，最後將過濾後的結果累加而得到，因此在計算自商影像需要準備很多個高斯濾波器，
具體的步驟如下： 
利用不同的加權式高斯濾波器來過濾影像，如式(2.8)所示。 
nkWG
N
II kk ...,2,1,
1 =⊕=∧       (2.8) 
上式中此處的 I 表示原始影像， kI
∧
表示濾波後的結果，而 kWGN
1 指的就是圖 2-9 的 F，N 表示
圖 2-9 中 F'的數值總合，而 k 表示第 k 個加權式高斯濾波器。 
濾波後的 kI
∧
代表影像中光照變化之部分，根據 Retinex 模型所示，利用原始影像除去光照變化，
如式(2.9)： 
∧=
k
k
I
IQ  , k=1,2,…,n       (2.9) 
其中 kQ 表示除去光照變化部分後的影像，但是在經過上述步驟後，除了光照變化被過濾掉，高
頻部分的雜訊同時也被放大了，因此我們透過一個非線性轉換式將 kQ 轉至 kD 來降低雜訊，如
 22
255...2,1,01)(
0
==∑
=
nxp
n
i
i
255...2,1,0)()(
0
==∑
=
nxpnF
n
i
i
 
機率密度函數是用來描述連續型隨機變數之機率密度分配。在統計灰階影像直方圖中，如
圖 2-12，直方圖的水平軸(X 軸)灰階範圍會落在 0 到 255 之間，垂直軸(Y 軸)則代表某一灰階
值的機率，由 0 到 255 所對應垂直軸的數值代表發生的機率，所以 0 到 255 代表著所有的機率
分佈範圍，而從灰階 0 的機率加到灰階 255 的機率總和為 1，公式如式(2.12)。 
              
圖 2-12 機率密度函數分佈圖 
累積分配函數為隨機變數小於某一特定值的機率，也就是將機率密度函數做累積的計算。在
統計灰階影像直方圖中，如圖 2-13，灰階為 255 時候的機率會為 1，而且曲線會由下往上成長,
也就是將機率密度函數做累加，公式如式(2.13)。 
 
圖 2-13 累積分配函數分佈圖 
 
由以上得到每一個區段的累積分配函數後，再經過以下步驟即可將直方圖等化。首先將累
積分配函數的數值乘以 255，這時候會出現許多重複的部份，將對應原來的影像灰階值，就可
以把原本的直方圖分佈打散，而且直方圖等化的結果也會發現很多地方數值為空心的，因此直
方圖的分佈就會向外推擠，將灰階均勻分佈，如圖 2-14 所示。 
 
(2.12) 
(2.13) 
 24
第三章  先前相關的研究 
 大部分研究人臉特徵點偵測的論文或期刊，第一步都會先將人臉擷取出來，接著才利用人
臉的區域把眼睛、嘴巴、眉毛特徵點偵測出來，在 Hu[34]所提出的方法中，則省略人臉擷取的
部份，而是直接使用眼睛濾波器（Eye filter）將人的眼睛部份偵測出來後，即可得到人臉區域
進而縮小範圍來取得嘴巴及眉毛特徵點。以下是他所用的演算法說明。 
 
3.1 眼睛特徵點偵測 
3.1.1 眼睛特徵點偵測流程 
 
只需輸入灰階影像，即可在複雜背景的影像中將眼睛特徵點偵測出來，如圖 3-1 所示，為眼
睛特徵點偵測的流程圖。一開始先將輸入的灰階影像進行影像模糊，接下來將原影像降取樣
(Downscale)成 4 種比例大小的影像，把每一種降取樣的影像大小經過眼睛濾波器(Eye filter)濾波
後，取可能為眼睛的區域，辨別是否為眼睛，再將為眼睛的部份做水平 Sobel 運算子及設臨界
值二值化，接著做水平投影與垂直投影即可偵測出眼睛特徵點。 
 
圖 3-1 眼睛特徵點偵測流程圖 
 
3.1.2 降取樣(Downscale) 
 
 因為無法事先得知輸入影像中眼睛的大小，所以設計一個固定樣板的眼睛濾波器(Eye 
filter)；先把輸入影像降取樣成四種不同大小的影像，接著透過眼睛濾波器(Eye filter)濾出可能是
眼睛的部份。然而在降取樣前影像必須先經過平均濾波器做模糊化。 
 
3.1.3 眼睛濾波器(Eye filter) 
 
眼睛濾波器主要的目的在於灰階影像經過此濾波器後，只留下可能為眼睛的部份，其他非
眼睛部份則去除掉，因此藉由眼睛濾波器能快速偵測出眼睛的特徵點。 
 
3.1.4 辨別眼睛 
 
灰階影像經過眼睛濾波器的結果還會殘留一些非眼睛部份的雜訊，所以本節利用十字節判
斷條件來辨別眼睛部份，把雜訊部份去除，最後判斷出眼睛的候選區域。 
辨別步驟如下： 
 26
                                 
圖 3-6 以白色區域為搜尋座標，左圖為還原前，右圖為還原原圖大小 
 
圖 3-7 在水平 Sobel 運算子上搜尋 
(5) 判斷為眼睛的十字節須符合以下條件即為眼睛，如式 3.1： 
 
graySgrayS
graySgrayS
avggrayS
hSobelShSobelS
hSobelShSobelS
hSobelShSobelS
hSobelShSobelS
hSobelShSobelS
hSobelShSobelS
_4_0
_3_0
150__0
)__6(*5.1__0
)__5(*5.1__0
__4__0
__3__0
)__2(*3.1__0
)__1(*3.1__0
>
>
<
>
>
>
>
>
>
   (3.1) 
 
其中 hSobelS __0 為水平 Sobel 運算子影像上十字節區塊 0S 的像素值總和，如式 3.2 所式： 
∑=
ji
jihSobelhSobelS
,
,___0     (3.2) 
其他則以此類推 
∑=
ji
jihSobelhSobelS
,
,___1   ∑=
ji
jihSobelhSobelS
,
,___4  
∑=
ji
jihSobelhSobelS
,
,___2   ∑=
ji
jihSobelhSobelS
,
,___5  
∑=
ji
jihSobelhSobelS
,
,___3   ∑=
ji
jihSobelhSobelS
,
,___6  
 28
界與下邊界。如圖 3-12 所示。圖 3-13 為填滿區域的水平 Sobel 運算子影像，標示眼睛上邊界
與下邊界。 
 
      
圖 3-10 填滿結果       圖 3-11 垂直投影 
 
       
圖 3-12 眼睛上邊界與下邊界    圖 3-13 水平 Sobel 影像的上、下邊界 
 
3.1.6 偵測出眼睛特徵點 
 
由上一節已經找出眼睛落在影像中大概位置，接下來將此區域做水平投影與垂直投影，由
各別的投影量分佈，來取得眼睛特徵點的座標，即可準確偵測出眼睛特徵點。 
 
3.2 人臉擷取 
 
 眼睛特徵點已偵測出來，人臉擷取的部份，只要利用眼睛特徵點的相對位置，即可擷取出
人臉的區域。 
 
3.3 嘴巴特徵點偵測 
3.3.1 嘴巴特徵點偵測流程 
 
 由於眼睛特徵點已偵測出來，所以可以利用眼睛與嘴巴的相對位置等等的特性來找出嘴巴
位於人臉的大概區域。接著將這個區域經過水平 Sobel 運算子及二值化後，再使用投影的技巧
來偵測出嘴巴的特徵點。 
 
3.3.2 取出嘴巴大概區域 
 
上邊界 
下邊界 
 30
 
第四章  我們的人臉特徵點偵測方法  
在第三章所討論的眼睛濾波器偵測方法中，但有時會因光源變化或人臉傾斜角度的關係，
使得光線在人臉上分布不均勻而產生陰影（例如鼻子部份），這將會影響水平 Sobel 運算子的結
果，容易造成眼睛濾波器的誤判，如圖 4-1。所以才加入一些前處理演算法及化簡判斷式來提
高眼睛濾波器偵測法的準確度及加快運算速度。 
 
 
圖 4-1 左圖為臉上光線分布不均的人臉，中間為水平 Sobel 運算子結果，右邊為眼睛濾波器誤
判的結果 
 
本計畫的表情辨識需藉由人臉特徵點來判別表情，人臉特徵點共有 14 個，此 14 個特徵點
分別為眼睛的上、下、左、右 8 個特徵點、眉毛內側 2 個特徵點、嘴巴上、下、左、右 4 個特
徵點。以上 14 個特徵點都偵測出來後，利用這些特徵點的特徵向量，即可進行人臉的表情分
析。14 個特徵點分佈如圖 4-2 所示。 
 
 
圖 4-2 人臉特徵點分佈 
 
4.1 眼睛特徵點偵測 
4.1.1 眼睛特徵點偵測流程 
 
如圖 4-3，一開始先將輸入影像的灰階部分擷取出來後，進行影像模糊化(平均濾波器)，接
著把模糊化後的灰階影像做自商影像處理（Self Quotient Image）及降取樣(Downscale)成 4 種比
 32
             
圖 4-4 原始灰階影像經過 SQI 處理的結果 
 
圖 4-5 經水平 Sobel 運算結果 
4.1.3 降取樣(Downscale) 
 
 因為無法事先得知輸入影像中眼睛的大小，所以設計一個固定樣板的眼睛濾波器(Eye 
filter)，先把輸入影像做降取樣的動作，降取樣成四種不同大小的影像，接著透過眼睛濾波器(Eye 
filter)濾出可能是眼睛的部份。然而在降取樣前必須先將影像做模糊化，影像糢糊採用的是平均
濾波器，目的是將不必要的高頻雜訊濾除，對於後面特徵點偵測是有幫助的。 
降取樣(Downscale)的影像大小分為四種，分別為原圖的四分之一，九分之一、十六分之一、
二十五分之一的大小。如圖 4-6 為四種降取樣的大小結果。 
 
 
      (a) 1/4                 (b) 1/9         (c) 1/16    (d) 1/25 
圖 4-6 四種降取樣大小的影像 
SQI 
 34
如圖 4-9 為 4 種大小的降取樣影像經過眼睛濾波器所得到的結果。 
   
   
 
   
   
圖 4-9 通過眼睛濾波器的結果 
 
4.1.5 十字節濾波器（Cross Filter） 
 
灰階影像經過眼睛濾波器的結果還會殘留一些非眼睛部份的雜訊，所以本節利用十字節判
斷條件來辨別這些可能是眼睛的地方，把雜訊部份去除，最後判斷出眼睛的候選區域。 
辨別步驟如下： 
(1) 設計十字節判斷區域 
十字節的區域是由 5 個相同大小的長方形所組成，長方形的大小是依照眼睛濾波器的大
小所設計，如圖 4-10 為十字節區域，圖 4-11 為長方形大小。 
 
 
 
 
 
 
 
 
 
 
圖 4-10 十字節判斷區域    圖 4-11 長方形區域大小 
 S3  
S4 S0 S2 
 S1  L 
W 
L=3*4=12 pixel 
W=8 pixel 
 36
 
}
0
{
}
)__4__3
__2__1(__0
{
))__4__3
__2__1(__0(
=
++
+−=
++
+>
eyescore
else
hSobelShSobelS
hSobelShSobelShSobelSeyescore
hSobelShSobelS
hSobelShSobelShSobelSif
   (4.5) 
 
其中 hSobelS __0 為經水平 Sobel 運算的影像上，十字節區塊 0S 的像素值的總和，如式 4.6 所示： 
∑=
ji
jihSobelhSobelS
,
,___0         (4.6) 
其他則以此類推 
 
∑=
ji
jihSobelhSobelS
,
,___1   ∑=
ji
jihSobelhSobelS
,
,___2  
∑=
ji
jihSobelhSobelS
,
,___3   ∑=
ji
jihSobelhSobelS
,
,___4  
 
(5) 最後每種降取樣的影像只留下前二高分數的十字節結果。4 種降取樣的影像，經過十字節
判斷為眼睛的結果，如圖 4-16 所示 
    
圖 4-16 十字節判斷為眼睛的結果 
 
 
 
 
 
 
 
 38
4.1.7 偵測出眼睛特徵點 
 
由上一節已經找出眼睛落在影像中大概位置，接下來將此區域做水平投影與垂直投影，由
各別的投影量分佈，可取得眼睛特徵點的座標，即可準確偵測出眼睛特徵點。 
 
在水平 Sobel 運算的影像上，只留下眼睛位置的上邊界與下邊界，其他則去除，如圖 4-22
所示，然後設定一個臨界值將此影像二值化，如圖 4-23 所示。 
       
圖 4-22 眼睛區域水平 Sobel 影像     圖 4-23 眼睛區域二值化 
 
接著將二值化的影像做水平投影，以水平軸(X 軸)為基底，像素往水平軸累加，如圖 4-24
為水平投影結果，經過水平投影後，取投影範圍最大的區域，即為眼睛的位置，在眼睛的投影
範圍中分別往右與往左搜尋，即可得眼睛的左、右邊界的水平軸座標，如圖 4-25 所示。 
 
      
        
圖 4-24 水平投影結果 
 
圖 4-25 找出眼睛水平軸座標 
X 軸 
Y 軸 
 40
 
圖 4-28 在紅色框框區域內找另一隻眼睛特徵點 
 
圖 4-29 二個眼睛偵測結果 
經過以上步驟可偵測出眼睛特徵點，如圖 4-30 為偵測出眼睛特徵點結果。 
 
 
 
 
 
圖 4-30 眼睛特徵點偵測結果 
 42
 
 由於眼睛特徵點已偵測出來，所以可以利用二個特性來找出嘴巴位於人臉的大概區域。特
性一為眼睛與嘴巴的相對位置，特性二為嘴巴顏色的 R 色彩濃度會比人臉膚色的 R 色彩濃度
高，由濃度來找出嘴巴位於人臉的大概區域。由人臉的大概區域經過水平 Sobel 運算並設定臨
界值將影像二值化後，再使用水平投影與垂直投影來偵測出嘴巴的特徵點。圖 4-33 為嘴巴特徵
點偵測流程圖。 
 
 
圖 4-33 嘴巴特徵點偵測流程圖 
 
4.3.2 正規化 RGB 
 
在 RGB 色彩空間中的影像，會因為光線亮度的不同，造成影像中同一顏色的地方因受光
線影響呈現的顏色卻不同，所以減少受光線亮度的影響，只要將影像中的 R 與 G 色彩做正規
化處理，就能使 R、G 對光線的靈敏度減少，由於 B 對光線的靈敏度本來就較小，所以可忽略。
R 色彩的正規化公式如式 4.7： 
BGR
Rr ++=         (4.7) 
   
其中 r 為 R 正規化的結果。 
 
4.3.3 取出嘴巴大概區域 
 
利用眼睛跟嘴巴相對位置與 R 色彩濃度的不同，可取出嘴巴大概位於人臉的位置。在二眼
睛內側的特徵點區域當成左邊界與右邊界，由二眼高度較低的眼睛下邊界 L 處開始，往下至人
臉的最下方邊界，以固定大小區塊累加 R 正規化的值 )(r ，比較累加值最大的區塊，為嘴巴的
大概區域。圖 4-34 為搜尋區域，圖 4-35 為 R 正規化累加值最大區塊的位置。 
   
圖 4-34 搜尋區域                圖 4-35 R 正規化累加值最大位置 
 
 44
 
圖 4-37 第一種傾斜情況     圖 4-38 取出嘴巴的大概區域 
 
第二種傾斜情況，右邊眼睛比左邊眼睛高且傾斜角度大於 20 度 
此種情況如圖 4-39 所示。接下來將上一步驟找出的 R 色彩濃度最高的地方，以 2/L 的高度往
上、往下取出嘴巴的上邊界與下邊界，嘴巴的左邊界為左邊眼睛的內側特徵點，嘴巴的右邊界
為右邊眼睛的外側特徵點，如此可取出嘴巴的大概區域，如圖 4-40 所示。 
 
       
 
 
圖 4-39 第二種傾斜情況   圖 4-40 取出嘴巴的大概區域 
 
第三種傾斜情況，無傾斜情況，此種情況如圖 4-41 所示。接下來將上一步驟找出的 R 色彩
濃度最高的地方，以 2/L 的高度往上、往下取嘴巴的上邊界與下邊界，嘴巴的左邊界為左邊眼
睛內外側特徵點的中間位置，嘴巴的右邊界為右邊眼睛內外側特徵點的中間位置，如此可取出
嘴巴的大概區域，如圖 4-42 所示。 
 
        
 
圖 4-41 第三種傾斜情況   圖 4-42 取出嘴巴的大概區域 
 
4.3.4 偵測出嘴巴特徵點 
 
由上一節得到三種人臉傾斜情況的嘴巴大概區域，將此區域的灰階影像經過水平 Sobel 運
算後，再設定臨界值將其影像二值化，如圖 4-43 所示，  
左邊眼睛內側特徵點 右邊眼睛外側特徵點 
左邊眼睛內外側特徵點中點 右邊眼睛內外側特徵點中點 
 46
 
 
 
 
 
 
 
圖 4-46 嘴巴特徵點偵測結果 
4.4 眉毛特徵點偵測 
4.4.1 眉毛特徵點偵測流程 
 
由於眼睛特徵點已偵測出來，所以可以利用眼睛對眉毛的相對位置找出眉毛位於人臉的大
概區域，因為嬰幼兒的眉毛較為稀疏且邊緣不明顯，所以無法使用 Sobel 運算子來偵測出眉毛
的特徵點，為了解決眉毛較稀疏的問題，首先將眼睛上方至額頭的區域做影像等化，可將眉毛
的對比度提高，接下來再找出二個眉毛內側的座標，即可偵測出眉毛特徵點。圖 4-47 為眉毛特
徵點偵測流程圖。 
 
 
圖 4-47 眉毛特徵點偵測流程圖 
4.4.2 影像等化(Image Equalization) 
 
在偵測眉毛特徵點之前，需先將眉毛附近的區域經過直方圖等化，增強眉毛區域的對比來
提高眉毛明顯度，首先由眼睛特徵點來取得眉毛附近要等化的區域，此區域為眼睛上方至額頭
 48
 
圖 4-51 灰階值亮度搜尋區域 
在此區域累加 ⎣ ⎦3/2*L 個像素值為一個單位，由左而右搜尋，找出二眉毛間累加灰階值最
大的地方，即為亮度最亮的地方，如圖 4-52 所示為示意圖。 
 
圖 4-52 搜尋灰階值最大亮度的示意圖 
 
由此區域找出灰階值累加總和最大的值，其座標為亮度最亮的地方，如式 4.9 所示。 
∑
=
=
n
i
iva
1
 ∑
=
=
n
i
ivb
1
 ∑
=
=
n
i
ivc
1
 …….. ∑
=
=
n
i
ivk
1
   ⎣ ⎦3/2*Ln =   
 
maximum },...,,,,max{ kdcba=       (4.9) 
其中 v為像素值， kdcba ,...,,,, 為灰階值累加總和， n為累加高度，maximum 為最大的灰階值累
加總和。 
 
當找出最亮的水平座標後，以水平座標為中心往左與往右搜尋累加灰階值最小的位置，即
為二眉毛內側特徵點的水平座標，如圖 4-53 所示。 
 maximum 
maximum 
 50
 
 
 
 
 
 
 
圖 4-55 眉毛特徵點偵測結果 
4.5 特徵點偵測結果 
圖 4-56 為人臉特徵點偵測的結果。 
 
 
 
 
 52
第五章  表情辨識與異物偵測 
 嬰幼兒的臉部表情較成年人單純，所以本論文表情辨識的部分主要只辨識三種表情，分別
為無表情、笑與哭的表情。首先把人臉上各個特徵點偵測出來，利用這些特徵點計算出特徵距
離，再把這些特徵距離做正規化後當成類神經網路的輸入特徵值，經過事先訓練完成的類神經
網路，即可辨識出嬰幼兒臉部表情。異物偵測的部分主要是去偵測嬰幼兒嘴巴附近是否有異物
入侵或是吐奶、嘔吐等危險情形發生，若有此類危險情況發生時，則在監控端螢幕上顯示警告
標誌，來告知家長和護理人員能夠及時處理上述之危險情形，防止意外發生。 
 
5.1 表情辨識 
 
圖 5-1 為表情辨識流程圖，此部份是使用類神經網路來辨識臉部表情，因訓練(Training)
的部份為離線工作，所以表情辨識的運算量與時間可減少很多。一開始先將已知表情的特徵向
量代入類神經網路訓練，訓練完成後，以後在即時 (Real time)辨識時，可直接將偵測到的這
些特徵點所形成的特徵向量代入已訓練完成的類神經網路系統，即可馬上辨識出嬰幼兒的表
情。 
 
 
 
圖 5-1 表情辨識流程圖 
 
5.1 表情線索  
 
 嬰幼兒的臉部表情較為單純，所以我們主要辨識三種表情，即無表情、笑與哭，由於每種
表情所牽動的臉部肌肉區域並不相同，因此由圖 5-2 這三種表情中我們觀察出情緒與相對應表
情之間的關係，來找出表情的線索，如表 5-1 所示。 
 54
圖 5-3 中的特徵距離計算如式(5.1)。 
 
 
 
 
     (5.1) 
 
其中 n為二眼睛內側特徵點距離， a為二眉毛特徵點距離，b 與 c為眼睛內側特徵點與眉毛
特徵點的距離， d 為嘴巴左右側特徵點距離， xA 為 A點的 x座標(水平座標)， yA 為 A點的 y 座
標(垂直座標)，其他座標 xB 、 yB 、 xC 、 yC 、 xD 、 yD 、 xE 、 yE 、 xF 、 yF 則以此類推。 
由於影像中的人臉大小不一定相同，所以需要使用正規化( Normalize) 的方法，來克服這些
特徵距離不會因為人臉大小不同而受到影響，首先必須在人臉上選擇一個不會變動的距離來當
正規化的數值，在人臉特徵中，二眼睛內側特徵點的距離 )(n 可當作一個不變量，來做正規化
的動作，因此可以得到 4 個經過正規化後的正規化特徵距離，正規化特徵距離如式(5.2)所示。 
ndF
ncF
nbF
naF
d
n
c
n
b
n
a
n
/
/
/
/
=
=
=
=
　　
　　
        (5.2) 
其中 anF 、 bnF 、 cnF 、 dnF 分別為 a、b 、 c、 d 特徵點距離的正規化結果。 
得到以上正規化特徵距離，可將已知表情的正規化特徵距離當成類神經網路的輸入值，代
入類神經網路中訓練，經過訓練完成後會得到權重值；往後只要對未知表情影像計算其正規化
特徵距離，輸入到已訓練完成的類神經網路中，由輸出值即可辨識表情。 
22
22
22
22
22
)()(
)()(
)()(
)()(
)()(
yyxx
yyxx
yyxx
yyxx
yyxx
EFEFd
BDBDc
ACACb
ABABa
CDCDn
−+−=
−+−=
−+−=
−+−=
−+−=
 56
5.4 表情辨識結果 
5.4.1 資料庫（Training database） 
 
本資料庫為人臉表情的訓練樣本，分為無表情、笑、哭三種表情的資料庫，這些已知表情
的樣本判別主要由 4 個人經過投票認定選出來；三種表情的訓練樣本數各為 20 筆，所以總訓
練樣本數為 60 筆，圖 5-5 為無表情、笑、哭的訓練樣本範本。 
 
無表情 笑 哭 
   
   
   
   
   
圖 5-5 訓練樣本範本 
 
 
5.4.2 測試影像（Testing pattern） 
 
測試影像一樣分為無表情、笑、哭三種表情，其中無表情樣本數為 15 筆、笑臉樣本數為
15 筆、哭臉樣本數為 10 筆，所以總測試影像數是 40 筆，圖 5-6 為無表情、笑、哭的測試影像
 58
表 5-2  40 張測試影像的正規化特徵距離結果 
表情 
       輸入
特徵 
第 n 張影像
  
a
nF  bnF  cnF  dnF  
1 0.832976 0.440283 0.649396 1.273612 
2 0.841814 0.480052 0.506019 1.315334 
3 0.724265 0.430070 0.507673 1.379043 
4 0.853945 0.544285 0.475964 1.281441 
5 0.841151 0.581095 0.555175 1.230046 
6 0.813859 0.611646 0.600267 1.202200 
7 0.809294 0.485483 0.462166 1.237744 
8 0.805008 0.497317 0.545220 1.243533 
9 0.833115 0.456070 0.491203 1.305864 
10 0.855409 0.583095 0.609126 1.313064 
11 0.808608 0.567878 0.543928 1.094635 
12 0.699580 0.519049 0.536875 1.131891 
13 0.716106 0.488766 0.501436 0.983130 
14 0.615182 0.569136 0.664967 1.691752 
無表情 
15 0.810693 0.524202 0.526897 1.994681 
16 0.804778 0.548195 0.508987 1.597128 
17 0.783970 0.590436 0.620798 1.503816 
18 0.819601 0.558242 0.640291 1.394737 
19 0.800995 0.574148 0.574148 1.496678 
20 0.797401 0.621168 0.656606 1.689312 
21 0.757743 0.540157 0.533732 1.753824 
22 0.834355 0.646509 0.534141 1.888474 
23 0.806355 0.473573 0.521404 1.780488 
24 0.828671 0.563557 0.534637 1.962962 
25 0.805877 0.506697 0.600094 1.422136 
26 0.790423 0.608018 0.615571 1.641648 
27 0.679531 0.415107 0.556576 1.960155 
28 0.808608 0.366375 0.366375 1.784657 
29 0.851229 0.492568 0.442052 1.510835 
 
 
笑 
30 0.788174 0.573846 0.729044 2.396226 
31 0.600000 0.382370 0.517287 1.411285 
32 0.788922 0.440740 0.394461 1.794441 
33 0.787378 0.431524 0.462080 1.311747 
34 0.814424 0.219858 0.192405 1.781246 
哭 
35 0.815074 0.357611 0.320886 1.990818 
 60
 
圖 5-8 搜尋範圍示意圖 
 
5.5.1 計算嘴巴附近區域的 RGB 平均值 
 
圖 5-9 為計算嘴巴附近區域 RGB 平均值的流程圖，當影像輸入後，我們先用前一章 4.3.3
節的方法把嘴巴的大概位置取出來，接著再去找出用來判斷異物的區域，也就是此範圍內的左
下及右下角的藍色框框區域，最後將此藍色框框區域內的 RGB 平均值記錄下來。 
 
圖 5-9 計算嘴巴附
近區域 RGB 平均值的流程圖 
 
 
5.5.2 比較前後張平均值的差異 
 62
 
圖 5-10 吐奶影片 
 圖 5-11 為棉被蓋住口鼻的影片。剛開始一樣沒有異物入侵，所以就做表情辨識，到第 97
張 Frame 時棉被蓋住了嘴巴，可能造成危險因此發出警告的訊號；第 140 張 Frame 危險解除所
以繼續做表情的分析；最後第 175 張 Frame 時棉被又蓋住嘴巴，因此又再次發出警告訊號。 
 
 
圖 5-11 棉被蓋住口鼻影片 
 
 
 
 
 
 
 
 
 64
 
圖 6-2 CDK 開發平台硬體方塊圖[35] 
 
 
圖 6-3 Xilinx FPGA 型號與可使用資源[36] 
 
下列為我們於此系統平台上，自行開發與使用系統平台現成資源的描述： 
◎ 自行開發 
 ＊ 建置 Embedded Linux Kernel 2.6.19 作業系統 
 ＊ 軟體人臉特徵點偵測、表情辨識及異物偵測演算法 
 ＊ 硬體 Cross filter 電路開發 
 ＊ 硬體系統匯流排（AMBA）之介面電路（Wrapper）開發 
 ＊ 韌體 IP 驅動程式開發 
 
◎ 使用系統平台現成資源 
 ＊ 影像儲存裝置（USB Disk） 
 ＊ 傳輸介面裝置（RS232） 
 ＊ 影像顯示裝置（LCD panel） 
 ＊ CDK 現有驅動程式（USB Disk、RS232、LCD Controller） 
 
 66
6.2.2 系統映像（System mapping） 
 
 在系統概要分析與軟硬體分割後，則將個別模組與整體系統的周邊使用，對應至 Socle CDK
的開發平台。在對應之系統映像（mapping），如圖 6-5 所示，可區分成軟體、韌體、硬體三大
部分。 
 
 
圖 6-5 系統 mapping 
 軟體部分，主要為 ARM 926EJ-S 處理之工作，包含作業系統（Embedded Linux Kernel 2.6.19）
與所發展之應用程式，而所使用到的週邊硬體裝置，包含了 USB Disk、UART，作為我們影像
來源與資料傳輸之裝置；LCD Panel 為影像顯示之裝置；而這些硬體 IP 所對應之驅動程式即為
韌體的部份。在此平台上 Xilinx FPGA 這邊，為我們自行設計之邏輯電路，包括了 Wrapper 及
Cross filter 的設計。 
 
6.3 FPGA 硬體設計 
 
6.3.1 IP 與 Wrapper 設計 
 
 Cross Filter IP 如圖 6-6 所示，可分成兩大部分，分別為 AMBA Bus Wrapper 電路與 Cross 
Filter 電路；其中負責與 AHB Bus 溝通的 Wrapper 我們是採用 AHB Slave 介面的形式，之後利
用這些介面訊號線配合內部有限狀態機（Finite State Machine）來將 AMBA Bus 上的資料寫入
或讀出至 Cross Filter 電路中，AHB Slave 介面訊號如表 6-1 所示。Cross Filter 介面訊號如表 6-2
所示。 
 
 
 
 68
 
6.3.2 Cross Filter 設計 
 
 如第四章 4.1.5 小節所述，十字節濾波器（Cross Filter）主要是用來辨別經過眼睛濾波器後，
這些有可能是眼睛的地方，把雜訊部份去除，最後判斷出眼睛的候選區域。因此我們將辨別是
否為眼睛的十字節判斷條件做成硬體，並計算出眼睛分數（eyescore）後回傳給應用程式端，
即可找出眼睛的候選區域。 
 
 Cross Filter 內部電路如圖 6-7 所示，裡面用到了 3 個加法器（ADD）、1 個 1 對 5 的解多工
器、5 個累加暫存器（ACC0 ~ 4）、4 個比較器（CMP）、4 個減法器（SUB）、4 個 2 對 1 多工
器、4 個存放分數的暫存器（SCORE0 ~ 3）及一個 4 對 1 的多工器；要利用十字節判斷條件去
計算眼睛分數之前，須先得到 hSobelS __0 ，即十字節區塊 S0 的像素值的總和，因此軟體端要
先把 S0 區塊裡的所有像素值以一次傳送 4 個 pixel 的方式，透過 2 層加法器後累加至 ACC0 內，
當累加完所有 S0 區塊的像素值後，軟體端改送十字節 S1 區塊像素值，一樣經累加後放到 ACC1
內，之後透過一減法器與比較器後即可得到眼睛分數，並存放至 SCORE0 內，當此分數等於 0
時則表示此區域不為眼睛的候選區域，所以不繼續把此十字節的其他區塊傳入硬體做處理，而
是去做下一個新的十字節判斷；反之當此分數大於 0 才繼續將此十字節的 S2 區塊像素值傳至
硬體做運算，每次累加完一區塊就與上一次算出的結果做相減及比較，最後當 S4 區塊運算完
後，即可得到一個最終的眼睛分數，再將此分數輸出至軟體端，並準備去做下一個新的十字節
判斷式。圖 6-8 為計算眼睛分數的流程圖。 
 
 
 
 70
 
圖 6-8 計算眼睛分數流程圖 
 
6.3.3 Cross Filter IP 合成結果 
 
 表 6-3 為利用 Xilinx ISE 工具軟體所合成之結果，所使用的 FPGA 型號為 Xilinx 
XC3S4000，此電路的 critical path 為 22.762 ns，電路可執行之頻率為 43.934 MHz，佔用之 slice 
flip flops 數量為 203，而 4 輸入的 LUT 數量為 873，整體電路約等於 9350 個 gate count。 
 
表 6-3  Cross Filter 合成速度與面積 
FPGA type Xilinx XC3S4000 
Estimated Frequency 43.934 MHz 
Estimated period 22.762 ns 
Number of slice flip flops 203 
Number of 4 input LUTs 873 
Equivalent gate count 9350 
 
6.4 IP 驅動程式 
 
 整個嬰幼兒監護系統是建立在 Embedded Linux 核心上，而週邊的這些硬體 IP，則需要對
應的驅動程式才能予以控制。在 CDK 的開發平台上，已附有 3.5 吋 LCD panel、USB、SD card…
 72
◎字元裝置驅動程式（Character Device Driver）：是一種裝置檔（Device File），一般來說需要
實作 open( )、close( )、read( )、write( )這四種系統呼叫。 
 字元裝置驅動程式是一般最常見的驅動程式，而大部分的矽智產（SIP）所採用的驅動程式
也都屬於此類；其運作在輸出入以字元為單位的設備上，以循序存取的方式動作，因此通常不
需要太複雜的緩衝策略或磁碟快取。所以我們的 IP 驅動程式也設計成此種類型。 
 
 要存取這種字元式裝置，必須透過檔案系統。所以驅動程式需要登錄一組檔案作業函式
（file_operations），來提供給字元式裝置使用。表 6-4 為 file_operations 作業函式說明。 
 
表 6-4 為 file_operations 作業函式說明 
函數名稱 說明 
llseek 
llseek 像是使用者空間中所用的函數，它會改變檔案結構
中的位置。 
read 將資料傳回使用者空間 
write 將資料寫入裝置 
ioctl 應用程式可透過此函數去控制裝置的行為或是取得資料 
open 當裝置被應用程式開啟時，便會呼叫此函數 
release 關閉裝置時使用 
 
 任何一個 IP 在驅動程式使用前都必須要先配置一塊記憶體空間來供此 IP 使用，因此我們
使用 request_mem_region( )來做配置，如圖 6-10 所示。 
 
 
圖 6-10 配置記憶體 
 
 完整的驅動程式除了包含裝置註冊外，還要有配置及釋放裝置編號等函式才行。圖 6-11 為
定義主、次編號、FPGA 起始位址及 FPGA 大小。圖 6-12 為宣告主編號及次編號。圖 6-13 為
裝置註冊及配置裝置主次編號。圖 6-14 為註銷裝置註冊與釋放裝置編號。 
 
 
圖 6-11 定義主、次編號、FPGA 起始位址及 FPGA 大小 
 74
 
圖 6-15 應用程式資料寫入 FPGA 的函式 
 
 
圖 6-16 應用程式讀取 FPGA 資料的函式 
 
6.5 系統實現與效能分析 
 
6.5.1 軟硬體整合之系統架構 
 
 圖 6-17 為嬰幼兒監護系統在 Socle CDK 開發平台上實現的軟硬體整合系統架構，其系統之
CPU 為 ARM926EJ-S，其內部具有 Instruction cache、Data cache 及寫入的 Register buffer，並掛
載 Embedded Linux 作業系統，Kernel 版本為 2.6.19，另外還包含了 Boot Loader 開機程序（uboot 
1.1.4）和 Root file system。 
 
 
圖 6-17 軟硬體整合之系統架構 
 
 ARM Core 負責執行我們設計的軟體應用程式，搭配 Embedded Linux 作業系統可分配排程
與做記憶體管理的工作及執行週邊 IO 裝置之驅動程式（Kernel space）。系統主記憶體為 64MB 
 76
成，區塊的大小又分成 24x16、36x24、48x32、60x40；而每次軟體端都須經由 AHB Bus 將十
字節的各區塊傳入 FPGA 硬體加速器來算出該十字節的眼睛分數；當十字節的數目越多時，將
會使得軟硬體間溝通越是頻繁，且 AHB Bus 的傳輸速度並不快，所以造成了軟硬體共同設計
的效能較純軟體版本差了一點。 
 
表 6-5 系統效能比較 
Test sequence：baby01.yuv（320x240） 100Frames 
Type Pure SW HW/SW co-design 
Execution time（sec） 32.98 53.66 
FPS（Frame/sec） 3.03 1.86 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 78
 
     
（a） 
     
（b） 
圖 7-2 （a）正常色溫（b）暖色溫 
     
（a） 
     
（b） 
圖 7-3 （a）光線均勻照射（b）光線不均勻照射 
 
7.2 實驗結果 
 
輸入的嬰幼兒影像背景通常是複雜可變的，會因其所在環境、所穿的衣物…等等而改變，
且嬰幼兒的臥室常處於光線亮度較低、環境色溫偏暖色系的環境，因此演算法也必須要能在這
些不同的環境下正常的運作才行。圖 7-4 為測試在單純及複雜背景下，使用適應性顏色分析、
橢圓樣板邊緣偵測法、眼睛濾波器偵測法及我們提出的演算法去偵測人臉和眼睛特徵點的結
果，左邊影像為單純背景的影像，右邊影像則為複雜背景的影像。 
 
 
   
（a）適應性顏色分析 
 80
   
（b）橢圓樣板邊緣偵測法 
    
（c）眼睛濾波器偵測法[34] 
    
（d）我們的演算法 
圖 7-5 色溫環境測試 
 
如圖 7-6 為測試光線變化下，使用適應性顏色分析、橢圓樣板邊緣偵測法、眼睛濾波器偵
測法及我們提出的演算法去偵測人臉和眼睛特徵點的結果，左邊影像為光線均勻照射的影像，
右邊影像則為光線不均勻照射的影像。 
 
   
（a）適應性顏色分析 
   
（b）橢圓樣板邊緣偵測法 
 82
 
 
表 7-3 四種偵測方法的運算時間 
 
 
 
 
 
 
由表 7-2 正確率與表 7-3 運算時間的比較可發現，我們所提出的演算法不但正確率高，而
且所花費的運算時間也是裡面最少的，所以我們的演算法為這四種偵測法中效果較好的方法。 
  
 
 
 
 
方法 測試影像總數 平均運算時間 
適應性顏色分析 100 60 毫秒 
橢圓樣板邊緣偵測法 100 292 毫秒 
眼睛濾波器偵測法[34] 100 64 毫秒 
我們的演算法 100 45 毫秒 
 84
參考文獻 
[1]  Z. F. Liu, Z. S. You, A. K. Jain, and Y. Q. Wang, “Face detection and facial feature extraction in color 
image”, Fifth International Conference on Computational Intelligence and Multimedia Applications, pp. 
126-130, 2003. 
[2]  Y. Guan, “Robust Eye Detection from Facial Image based on Multi-cue Facial Information,” IEEE 
International Conference on Control and Automation, pp. 1775 – 1778, 2007. 
[3]  R. L. Hsu, A. M. Mohamed, and A. K. Jain, “Face detection in color images,” IEEE Trans. Pattern 
Analysis and Machine Intelligence, Vol. 24, No. 5, pp. 696-706, 2002. 
[4]  O.Ikeda, “Segmentation of faces in video footage using HSV colour for face detection and image 
retrieval,” IEEE International Conference on Image Processing, Vol. 2, pp. 913-916,2003.  
[5]  P. Campadelli, R. Lanzarotti, and G. Lipori, “Face detection in colour images of generic scenes,” IEEE 
International Conference on Computational Intelligence for Homeland Security and Personal Safety, pp. 
97-103, 2004. 
[6] J. Seo, and H. Ko, “Face detection using support vector domain description in color images,” IEEE 
International Conference on Acoustics, Speech, and Signal Processing, vol. 5, pp. 729-732. 
[7] K. Nallaperumal, R. Subban, K. Krishnaveni, L. Fred, and R.K. Selvakumar, “Human face detection in 
color images using skin color and template matching models for multimedia on the Web,” IFIP 
International Conference on Wireless and Optical Communications Networks, pp. 5–5, 2006. 
[8]  R. Pappu, and P. A. Beardsley, “A Qualitative Approach to Classifying Gaze Direction,” Third IEEE 
International Conference on Automatic Face and Gesture Recognition, pp. 160-165, 1998. 
[9]  S. Birchfield, “An Elliptical Head Tracker,” Thirty-First Asilomar Conference on Signals, Systems & 
Computers, vol. 2, pp. 1710-1714, 1997. 
[10] S. Birchfield, “An Elliptical Head Tracking Using Intensity Gradients and Color Histograms,” IEEE 
Computer Society Conference on Computer Vision and Pattern Recognition, pp. 232-237, 1998. 
[11] T. K. Leung, M. C. Burl, and P. Perona, “Finding Faces in Cluttered Scenes Using Random Labeled 
Graph Matching,” Fifth International Conference on Computer Vision, pp. 637-644, 1995. 
[12] C. Lin, and K. C. Fan, “Human face detection using geometric triangle relationship,” 15th International 
Conference on Pattern Recognition, vol. 2, pp. 941-944, 2000. 
[13] J. M. Lee, J. H. Kim, and Y. S. Moon, “Face Extraction Method Using Edge Orientation and Face 
Geometric Features,” International Conference on Convergence Information Technology, pp. 1292 – 
1297, 2007. 
[14] Q. Yuan, W. Gao, and H. Yao, “Robust frontal face detection in complex environment,” 16th International 
Conference on Pattern Recognition, vol. 1, pp. 25-28, 2002.  
[15] C. C. Han, H.Y. Liao, G. J. Yu, and L. H. Chen, “Fast face detection via morphology-based 
pre-processing,” Pattern Recognition, Vol. 33, pp. 1707-1712, 2000. 
[16] H. A. Rowley, S. Baluja, and T. Kanade, “Rotation Invariant Neural Network-Based Face Detection”,  
IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 963 – 963, 1998.  
[17] H. A. Rowley, S. Baluja, and T. Kanade, “Neural network-based face detection”, IEEE Computer Society 
Conference on Computer Vision and Pattern Recognition, pp. 203 – 208, 1996. 
[18] P. Viola and M. J. Jones, “Rapid Object Detection using a Boosted Cascade of Simple Features,” IEEE 
Computer Society International Conference on Computer Vision and Pattern Recognition, vol. 1, pp. 
511-518, 2001. 
[19] C. Garcia, and M. Delakis, “A neural architecture for fast and robust face detection”, 16th International 
Conference on Pattern Recognition, vol. 2, pp. 44-47, 2002. 
[20] C. H. Han, and K. B. Sim, ”Real-time face detection using AdaBoot algorithm”, International Conference 
on Control, Automation and Systems, pp. 1892 – 1895, 2008. 
[21] Z. Xin, X. Yanjun, and D. Limin, “Locating facial features with color information”, Fourth International 
Conference on Signal Processing Proceedings, vol. 2, pp. 889-892, 1998. 
[22] J. Park, J. Seo, D. An, and S. Chung, “Detection of Human Faces Using Skin Color and Eyes,” IEEE 
International Conference on Multimedia and Expo, vol.1, pp. 133-136, 2000. 
[23] X. Zhang and R. M. Mersereau, “Lip feature extraction towards an automatic speechreading system,” 
IEEE International Conference. Image Processing, vol.3, pp. 226-229, 2000. 
 86
 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
5  達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明： 
 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：5已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 5無 
技轉：□已技轉 □洽談中 5無 
其他：（以 100 字為限） 
 
  本子計畫的相關研究成果已發表在會議論文中，而本年度的計畫成果為共發表
2 篇期刊論文與 4 篇會議論文。 
 
附件二 
 88
 
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                             日期： 99  年 12 月 31 日 
 
 
 
 
 
 
計畫編號 NSC 97－2221－E－005－088－MY2 
計畫名稱 智慧型嬰幼兒臉部表情辨識與臉部異物偵測設計與實作 
出國人員
姓名 范志鵬 
服務機構
及職稱 
國立中興大學電機工程系 
副教授 
會議時間 
2009 年 11 月 23
日至 
2009 年 11 月 26 
日 
會議地點 
新加坡 
會議名稱 (中文) 2009 年 國際電機電子學會第 10 區(亞太區)會議,新加坡 
(英文) 2009 IEEE Region 10 Conference ( Tencon 2009 ), Singapore 
發表論文
題目 
1. (中文) 應用於高階 QAM 纜線通訊系統之二階段基於通用多模數與軟決策之快速
盲蔽式等化器 
(英文) Two-Stage Generalized Multilevel Modulus and Soft Decision-Directed Based 
Fast Blind Equalization for High-Order QAM Cable Systems 
2. (中文) 保持邊緣且低複雜度內插的移動偵測適應性解交錯演算法 
(英文) Motion Adaptive De-interlacing Algorithm with Edge-based Low-Complexity 
Interpolation Method 
附件四 
 90
來進行有效的內插.  
 
由研討會的議題觀察，使用智慧型 /低功耗的電路與系統設計已是
一重要的研究與發展趨勢。重要的 Keynote 演講如下，包括智慧型多媒體
人機介面設計的探討，及智慧型供電系統的發展等議題: 
 
Keynote Speech 1 (24 November 2009) 
The Nexus of Science: Key to Spawning New Innovations and Discoveries, 
By Professor Teck Seng LOW, Deputy Managing Director (Research) and Executive, 
Director SERC Agency for Science, Technology & Research (A*STAR) 
 
Keynote Speech 2 (24 November 2009) 
Advances in Evolutionary Optimisation in Modern Power Systems, 
By Professor Kit Po WONG, Chair Professor, Department of Electrical Engineering, 
Hong Kong Polytechnic University 
 
Keynote Speech 3 (25 November 2009) 
Digital Convergence: Challenges and Opportunities, 
By Dr. Sanjoy PAUL, Associate Vice President, Infosys Technologies Limited 
 
Keynote Speech 4 (25 November 2009) 
New Challenges of Advanced Multimedia Processing for Intelligent 
Human-Computer Interface (AMP-IHCI), 
By Professor Jhing-Fa WANG, Chair Professor, Department of Electrical Engineering, 
National Cheng Kung University 
 
而在論文發表期間，國外學者在現場也不吝提出問題給予指教。而除了本身
的論文發表之外，本人也參加了一些相關領域的 sessions，瞭解到了別人的研究
進度與方向，並與參加會議之教授與專家請教，更進一步的瞭解目前學術界與工
業界的研究與需求，以期自己所做的研究能與時代潮流結合。 
 
 
 
 92
3. 參與此次會議的照片集 
 
Tencon 2009 會議的開幕式 
 
Tencon 2009 會場之一 
 
Keynote 的專家演講 
 
 
 
 
 
 
參加會議 
 
論文發表 
 
論文發表後 
 
 
 
 978-1-4244-4547-9/09/$26.00 ©2009 IEEE                                                                                TENCON 2009 
 
Two-Stage Generalized Multilevel Modulus and Soft 
Decision-Directed Based Fast Blind Equalization for 
High-Order QAM Cable Systems
 
Hao-Jun Hu and Chih-Peng Fan* 
Department of Electrical Engineering, National Chung Hsing University, 
250 Kuo-Kuang Road, Tai-chung 402, Taiwan, R.O.C. 
Email: cpfan@dragon.nchu.edu.tw * 
 
 
Abstract— In this paper, a fast blind equalization with a two-
stage generalized multilevel modulus algorithm (GMMA) and 
soft decision-directed (SDD) scheme is proposed for high-order 
QAM (64/256/1024QAM) systems on the downstream wired cable 
channel.  The proposed fast blind equalization algorithm applies 
the two-stage convergence scheme and the SDD part uses the 
scheme with an adaptively selected decision region.  In the first 
convergence stage, the joint GMMA and modified SDD 
equalization is applied for the purpose of the fast convergence.  
When the convergence process reaches the enough steady state, 
the convergence detector changes the first stage equalization to 
the second stage.  In the second stage, the modified SDD scheme 
is applied to reduce the mean square error (MSE) further. On the 
wired cable channel at the 64/256/1024QAM modes, the proposed 
blind algorithm performs faster convergence speed than the 
previous blind methods do; meanwhile, the proposed algorithm 
also achieves smaller MSE than the other methods at the same 
signal-to-noise ratio (SNR). 
Keywords- blind equalizer; fast convergence; high-order QAM  
I.  INTRODUCTION 
In wired cable communication systems [1, 2], the received 
symbol has phase and amplitude distortions, which are caused 
by multi-path channels. Thus, the blind equalization overcomes 
unwanted channel effect and avoids adding training data, and it 
saves the transmitter bandwidth. Two common indices are 
referred to judging the performances of the blind equalization, 
i.e. the convergence speed and the converged mean square 
error (MSE).  The well-known blind equalization algorithm, 
which is called the constant modulus algorithm (CMA), is 
proposed by Godard [3].  The CMA method is one of the 
typical blind equalization methods.  When the CMA method is 
applied to QAM modulations, all received symbols are 
converged toward the same constant modulus value.  The 
typical CMA method can not correct the phase offset of the 
received symbols; therefore, a phase recovery circuit is 
required for the phase correction. In order to solve the problem 
of a little bit phase offset in the CMA method, the multi-
modulus algorithm (MMA) [5, 7, 8, 9], or called the modified 
constant modulus algorithm (MCMA) [4] is proposed to 
provide the ability of the phase recovery. However, since the 
CMA and MMA methods perform slower convergence speed 
and higher MSE, then several hybrid blind algorithms [10, 11] 
are proposed to research how to speed up the convergence 
process of blind equalizations and how to reduce more MSE 
after convergences.  In order to reduce the MSE further, the 
researchers [4, 10, 11] apply the decision-directed (DD) 
scheme in the equalizers.  
According to the GMMA and SDD methods, we develop a 
new two-stage blind equalization scheme, which 
simultaneously performs higher converge speed and lower 
MSE than the previous schemes for high-order QAM cable 
systems, especially at the 1024QAM mode. The rest of this 
paper is organized in the following. In Section 2, the baseband 
model is introduced for simulations.  The related equalization 
algorithms, i.e. GMMA and SDD with an adaptively selected 
decision region, are introduced in Section 3. In Section 4, the 
proposed efficient two-stage fast blind equalization algorithm 
is discussed. The simulation results and performance 
comparisons are shown in Section 5. Finally, we give a 
conclusion at the end of this paper. 
II. BASEBAND SYSTEM MODEL 
The receiver side signal in the baseband cable system can be 
described as follows. 
∑−
=
+−⋅=
1
0
)()()(
L
i
i kwikahkx
 ,                              (1) 
where hi means the channel impulse response (CIR), L is the 
order number of CIR, and a(k) is the complex QAM 
modulation symbol. The w(k) is the additive white Gaussian 
noise. The output symbol after computing the convolution of 
the blind equalization is described as follows. 
       1
0
( ) ( ) ( ) ( ) ( )
m
T
i
i
k c k x k i k k
−
=
= − =∑Z C X ,                   (2) 
where x(k) is the input signal to the blind equalizer, ( )ic k  
denotes the complex coefficients of the equalizer, m means 
the number of the equalizer coefficients, and ( )kZ  is the output 
of the equalizer. Let ˆ( )kZ be defined as the decision output 
after the decision maker. 
III. RELATED EQUALIZATION  ALGORITHMS 
3.1 Generalized Multilevel Modulus Algorithm (GMMA) 
In [7], the multilevel modulus algorithm (MMA) was 
proposed.  Firstly, the cost function of the MMA method is 
described as 
                                       
 
The first term in (8) is equivalent to the cost function of 
the GMMA method.  Then the second term in (8) is similar to 
the cost function of the SDD method.  By adding the second-
term SDD-like cost function, the equalizer can converge not 
only on the modulus values but also on the nearest decision 
symbols.  By applying the stochastic gradient algorithm, the 
error function derived from (8) is described as 
24
i
i
2 2 i
i 24
i
i
z( k ) z
exp( ) ( z z( k ))
2e( k ) 4 ( z( k ) R ) z( k )
z( k ) z
exp( )
2
σ
σ
−
− ⋅ −
= ⋅ − ⋅ +
−
−
∑
∑
    .        (9) 
Since the error function of the GMMA part can be separated 
into the real and imaginary parts, then (9) becomes 
  
        = 
24
i
i
2 2 2 2 i
R R i I I i 24
i
i
GMMA SDD
z(k ) z
exp( ) ( z z(k ))
2e(k ) 4 { z (k ) ( z (k ) R ) z (k ) ( z (k ) R )}
z(k ) z
exp( )
2
e (k ) e (k )
σ
σ
−
− ⋅ −
= ⋅ ⋅ − + ⋅ − +
−
−
+
∑
∑
   ,   
(10) 
where  
2 2 2 2
GMMA R R i I I ie ( k ) 4 { z ( k ) ( z ( k ) R ) z ( k ) ( z ( k ) R )}= ⋅ ⋅ − + ⋅ −   , 
and 
       = 
2 24 4
i i
SDD i
i i
z( k ) z z( k ) z
e ( k ) exp( ) ( z z( k )) exp( )
2 2σ σ
− −
− ⋅ − −∑ ∑   . 
Then the coefficients of the equalizer are updated with (11) as 
     1 ( ) { ( ) ( )) ( )GMMA GMMA SDD SDD(k ) k e k e k k  μ μ ∗+ = + ⋅ + ⋅ ⋅C C X ,    (11) 
where  
GMMAμ and SDDμ  are the step sizes for the GMMA and 
SDD parts, respectively.  The derivations for acquiring (10) 
are similar to those in [10] and [11].  At the beginning, since 
the outputs of the equalizer are scattered on the I-Q 
constellation plain, the values of the decision outputs are not 
accurate, and then the equalized symbol values have several 
decision errors. Thus, the more accurate equalization process 
is necessary to reduce the decision error further.  
 
4.2 Stage II : SDD Mode 
In the equalization process of Stage I, the proposed 
equalization algorithm can perform faster convergence speed 
than the other blind methods. When the proposed equalization 
achieves the enough steady state, the GMMA-part cost 
function in the first term of (10) can not provide the ability to 
reach the optimum points through the stochastic gradient 
algorithm, and then the error function of the GMMA part also 
operates the harmful influence during the equalizations.  To 
reach less MSE, when the equalization process goes to the 
enough steady state, the GMMA part will be discarded. Only 
the cost function of the SDD part is considered to be the 
further equalization. Then the coefficients of the equalizer are 
updated with (12) as 
1 ( ) ( ) ( )SDD SDD(k ) k e k k  μ ∗+ = + ⋅ ⋅C C X  .              (12) 
Based on the two-stage equalization process, the values of 
the error function are further decreased and the convergence 
speed will be raised again.  To detect the convergence for the 
stage change, we apply the convergence detector [11] in the 
proposed algorithm.  As shown in Figure 3, the convergence 
detector is considered when the equalization process is 
switched from the GMMA+SDD mode to the SDD mode for 
the further convergence. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3  Block diagram of the proposed algorithm 
4.3 Variable Step Size without Division 
During the blind equalization, if the step size is fixed, it is 
difficult that the convergence speed is fast, and simultaneously 
the steady MSE is small.   In [6], the scheme of the variable 
step size is used to obtain fast convergence and small steady 
MSE for equalizations, and it is shown in (13).   The concept 
of the variable step size is that the step size can be adaptively 
changed according to the value of the error function. 
      
G M M A
G M M A 2
G M M A
( n )( n 1 )
1 ( n ) e( n )
μμ
λ μ
+ =
+
          .                 (13) 
Then 
λ=0    ;    when  sgn(Re{e(n)})=sgn(Re{e(n-1)}) , 
and   sgn(Im{e(n)})=sgn(Im{e(n-1)} , 
λ=1 ;   other else , 
where “sgn” is the sign function and e(n) is the error function 
of the GMMA equalization.  By using the following equation,  
1,1
1
1 2 <+++=
−
xxx
x
"    . 
The variable step size in (13) can be approximately replaced 
with 
                     
2
GMMA GMMA
GMMA
( n 1 ) ( n ) { 1 }
( n ) ( 1 )
μ μ α α
μ α
+ = ⋅ + + +
≅ ⋅ +
"  ,            (14 ) 
where 2
GMMA( n ) e( n )α λμ= − . It is noted that the computation in 
(14) does not need any division, compared with that in (13).  
The initial step size 
GMMA(0 )μ  is set to a small positive and 
changeable value according to different communication modes.  
V. SIMULATION RESULTS AND COMPARISONS 
To compare the proposed algorithm with the previous 
blind algorithms at the 64, 256, and 1024 QAM modes, the 
downstream wired cable channel model shown in Table I is 
applied for system simulations. To examine the relationship 
between MSE and equalizations, the carrier and the timing 
synchronizations are assumed to be perfect.  
The parameters in our simulations are described as 
follows. The symbol rate is assigned to 5.2 Msymbols/sec at 
the 64, 256, and 1024QAM modes. The structure of the 
equalizer is 1/2 symbol fractional-spaced. The results of the 
equalized constellations and the MSE are simulated under 
30dB, 35dB, and 40dB SNRs at the 64, 256, and 1024QAM 
modes, respectively. The tap numbers of the equalizer are set 
to 16, 16, and 32 at the 64, 256, and 1024QAM modes, 
respectively. In the applied convergence detector [11], L 
equals 10 and refd  is set to 0.8 at the 64 and 256QAM modes, 
and L is 10 and refd  is set to 0.3 at the 1024QAM mode.  
 
)(ˆ kz)(ka )(kz
First stage
GMMA+SDD
Algorithm
Quantizer
Q(·)
CIR  
h(t) EqualizerQAM Modulation
Binary data
)(kw
)(kx
Second stage
SDD
Algorithm
Convergence
Detector
_+
                                       
 
VI. CONCLUSION 
A fast blind equalization with the two-stage GMMA and 
modified SDD scheme is proposed for high-order QAM 
systems on the downstream wired cable channel. The 
proposed fast blind equalization algorithm applies the two-
stage convergence scheme.  In the first stage, the joint GMMA 
and modified SDD equalization is applied for the purpose of 
the fast convergence.  In the second stage, the modified SDD 
is applied to reduce the mean square error further. On the 
wired cable channel at the 64/256/1024QAM modes, the 
proposed blind algorithm achieves faster convergence speed 
and smaller MSE than the previous blind methods. 
ACKNOWLEDGMENT  
This work was supported in part by the National Science 
Council, Taiwan, R.O.C., under Grant NSC97-2221-E-005-
088-MY2. 
REFERENCES 
 [1] Data-Over-Cable Service Interface Specifications (DOCSIS) 2.0 Radio 
Frequency Interface Specification CM-SP-RFIv2.0-I11-060602, June 2, 
2006.  
[2] T. Kurakake, N. Nakamura, and K. Oyamada, “A Blind 1024-QAM 
Demodulator for Cable Television,” Int. Zurich Seminar on 
Communications (IZS), Feb. 18-20, 2004. 
[3] D. Godard, ”Self-Recovering Equalization and Carrier Tracking in Two-
Dimensional Data Communication Systems”, IEEE Transactions on 
Communications, vol. 28, pp. 1867-1875, 1980. 
[4]  K. N. Oh and Y. O. Chin, “New Blind Equalization Techniques Based on 
Constant Modulus Algorithm”, IEEE Conference Global 
Telecommunications, vol. 2, pp 865-869, 1995.  
[5] K. N. Oh, “A Single/Multilevel Modulus Algorithm for Blind Equalization 
of QAM Signals,” IEICE Trans. vol. E80-A, No. 6, pp.1033-1038, June 
1997. 
[6] A. R. Esam, “Blind Adaptive Equalization with Variable Step Size”, 4th 
International Conference on Information & Communications Technology, 
pp.1-1, Dec. 2006. 
[7] J. Yang, J. J. Werner and G. A. Dumont, “The Multimodulus Blind 
Equalization Algorithm,” 13th Int. Conf. Digital Signal Processing, 
Santorini, Greece, July 2-4, 1997. 
[8] J. Yang, J. J. Werner, and G. A. Dumont, “The Multimodulus Blind 
Equalization and Its Generalized Algorithms,” IEEE Journal on Selected 
Areas in Communications, vol. 20, no. 5, pp. 997-1015, June 2002.  
[9] J. T. Yuan and K. D. Tsai, “Analysis of the Multimodulus Blind 
equalization Algorithm in QAM Communication Systems,” IEEE 
Transactions on Communications, vol. 53, no. 9, pp. 1427-1431, September 
2005. 
[10] S. Chen, “Low Complexity Concurrent Constant Modulus Algorithm and 
Soft Decision Directed Scheme for Blind Equalization”, IEE Proceedings 
Image and Signal Processing, vol. 150, pp.312-320, 2003. 
[11] C .P. Fan, W. H. Liang, W. Lee, “Fast Blind Equalization with Two-
Stage Single/Multilevel Modulus and DD Algorithm for High Order QAM 
Cable Systems,” IEEE International Symposium on Circuits and Systems, 
Seattle, USA, May 2008. 
 
                                       
 
The architecture of the whole proposed algorithm is shown in 
Figure 2. The new motion adaptive de-interlacing algorithm is 
described as follows. The proposed de-interlacing method by 
using the pixel-based motion detection is very cost-effective. 
Firstly, we exploit three buffers to store three-field 
information (i.e. backward field, forward field and forward-
forward field).  Then all pixels in buffers and the current field 
are fed into the improved 4-field motion detector. The purpose 
of the improved 4-field motion detection is to detect whether 
the interlaced pixel is a motion, a static, or an uncertain pixel. 
Secondly, the improved 4-field motion detection generates a 
selected signal to the choice line. The multiplexer module 
exploits the choice signal to select one from the three de-
interlacing modes.  If the pixel is a static one, the new motion 
adaptive de-interlacing scheme applies the well-known inter-
field averaging method.  If the pixel is an uncertain one, the 3-
D median LCI method is used.  If the pixel is a motion one, 
the LCI with the expanded edge detection is applied. Finally, 
the progressive frame is produced from the interpolated field 
or the current field according to “the interleave by line” 
module. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2 Block diagram of the proposed de-interlacing algorithm 
 
The following subsections describe the details of the 
improved 4-field motion detection and the new motion 
adaptive de-interlacing scheme. 
 
3.1 Improved 4-field motion detection 
The 4-field motion detection proposed in [3, 9] is applied 
to the motion detection. The motion detection [3, 9] is 
effective. However, owing to the unsuitable detection, it can 
not solve afterimage phenomenon after the de-interlacing 
process when the video object moves fast, especially the fast 
bouncing object.  To identify the motion pixels correctly, we 
propose the improved 4-field motion detection method, and its 
diagram is shown in Figure 3. 
 
E
C
H
A
G
F
X
D
B
n-2 n-1 n n+1  
Figure 3 Diagram of improved 4-field motion detection 
The algorithm of the improved motion detection is described 
as follows. Let 
d1 = abs( C − D )  
d2= abs( E − F )  
d3 = abs( A − B )  
d = max( d1,d2,d3 )                .                      (2) 
d4= abs( D − G ) 
d5= abs( F − H ) 
d6= (d5+d4) / 2 
Then 
if (d ≤ th1 and d6 ≤ th2) 
{ Static pixel } 
else if (d > th1 and d ≤ th3 and d6 ≤ th2)  
{ Uncertain pixel } 
else 
{ Motion pixel }              , 
where X is the interpolation point, and then th1=5, th2=50, and 
th3=40 in our design. The “d1”, “d2”, “d3” and “d” are 
originally used in the previous 4-field motion detection [3]. 
The improved 4-field motion detection adds the “d6” to 
increase the probability which judges motion pixels correctly.  
In the previous methods [3, 9], especially for the fast bouncing 
object, the motion detector usually judges motion pixels to be 
static. Thus, the afterimage phenomenon will be generated. 
 
3.2 New motion adaptive de-interlacing scheme 
  The previous motion adaptive de-interlacing scheme in [3] 
only divides the pixel into two groups, which are static and 
motion pixels. They only use one threshold value to detect 
whether a static or a motion pixel happens. By using the 
improved 4-field motion detection in Section 3.1, we propose 
a new motion adaptive de-interlacing scheme with three 
processing modes, which are shown in Figure 2.  The 
discussions of the proposed LCI with the expanded edge 
detection and the 3-D median LCI methods are described as 
follows. 
 
3.2.1 LCI with the expanded edge detection 
We use the LCI method by expanding the edge directions, 
which are expanded from the 3-point to the 5-point edge 
detection.  By the extension of the edge direction, the 5-point 
LCI shown in Figure 4 can achieve better de-interlacing 
results than the 3-point LCI for de-interlacing the motion 
pixel.  However, when the input interlaced video exists noises, 
the 5-point LCI  can not perfectly detect the correct edge 
direction.  If the detection of the edge is unsuitable, the edge in 
the de-interlacing image will be blurred.  Thus, we expand the 
detection range of edges by considering a 5x9 pixel block to 
detect the correct edge, and the details of the de-interlacing 
method are described as follows. 
 
 
 
 
 
 
 
 
 
 
Figure 4  5-point LCI method for edge detection 
 
Step 1: The edge direction is calculated by using the 5-point 
LCI method and the information of the edge direction is stored 
Improved
4-field
motion
detection
Field s torage
Field s torage
Field s torage
Inter-field
averaging
3D median
LCI
LCI with
expanded edge
detection
M ultiplexer
Interleave by line
Field in Backward f ie ld
Current fie ld
Forward  fie ld
Forward-forward fie ld
Choice line
U ncerta in 
M otion
Static
Interpo lat ion fie ld
Current 
field 
Progressive fram e out 
A
F
B
G
C
H
X
D
I
E
J
x-2 x-1 x x+1 x+2
y-1
y
y+1
Edge 1
A
F
B
G
C
H
X
D
I
E
J
x-2 x-1 x x+1 x+2
y-1
y
y+1
Edge 2
A
F
B
G
C
H
X
D
I
E
J
x-2 x-1 x x+1 x+2
y-1
y
y+1
Edge 5
A
F
B
G
C
H
X
D
I
E
J
x-2 x-1 x x+1 x+2
y-1
y
y+1
Edge 4
A
F
B
G
C
H
X
D
I
E
J
x-2 x-1 x x+1 x+2
y-1
y
y+1
Edge 3
                                       
 
IV. SIMULATION RESULTS AND COMPARISON 
4.1 Objective Comparison 
To obtain the objective performance comparison after we 
use the de-interlacing skills, we use the C language to verify 
different de-interlacing algorithms with 352x288 interlacing 
video sequences. Table 1 shows the PSNR comparison after 
different de-interlacing methods are applied.  For most of the 
test video sequences, our proposed method achieves larger 
PSNR values than the motion adaptive method in [3].  Then 
the proposed method also performs larger average PSNR 
values than the previous non motion-compensated de-
interlacing methods.  Table 2 shows the comparison of the 
memory access, buffer size and computational complexity 
among different de-interlacing methods.  In Table 2, our 
method requires similar memory access, hardware, and 
computational complexities to the method in [3].  It is noted 
that our method can provide the same subjectively de-
interlacing results as the motion compensated de-interlacing 
methods.  However, it requires fewer complexities than the 
motion compensated de-interlacing methods do. Then the 
subjective comparison is depicted in Section 4.2. 
 
Table 1 PSNR comparison between our method and the other non motion-
compensated de-interlacing methods 
Video 
Sequence 
(CIF format) 
ELA[1]  3-D 
ELA[4]  
LCI [6] Motion
Adaptive
E-ELA[3
Prop
Weather 29.144 38.195 29.923 40.098 40.219 
Table Tennis 29.043 27.480 29.823 30.073 30.232
Salesman 31.431 35.992 32.656 36.633 36.627 
Mobile 23.382 24.993 25.316 25.367 25.635
Foreman 31.578 33.042 31.528 32.739 32.404
Tempete 27.035 29.042 29.083 30.642 31.094
Average PSNR 28.6 31.457 29.72  32.59 32.7 
 
Table 2 Memory access, buffer size and complexity comparison 
Method Memory Access Buffer 
Size(bytes) 
Computational 
Complexity Read Write 
Bilinear 
Average 
2n n 2 Small 
ELA [1] / 
 LCI [6] 
2n n 6 Small 
Motion 
Compensated 
> 10n n Frame size Large 
Motion 
Adaptive [3] 
5n n 25 Middle 
Proposed 10n n 33 Middle 
n : the number of the interpolated pixels 
 
4.2 Visually Subjective Comparison 
Figure 7 shows the de-interlaced image sequence with the 
motion adaptive de-interlacing method in [3], and then Figure 
8 shows the de-interlaced image sequence with the proposed 
new motion adaptive de-interlacing method. In Figure 7, the 
serious afterimage phenomenon is revealed in the de-
interlaced video sequence.  However, in Figure 8, the defects 
of the afterimage phenomenon are effectively reduced.    
Figure 9 shows the subjective comparison among different de-
interlacing methods.  In Figure 9, it is noted that the unwanted 
interlaced artifacts are efficiently removed after the proposed 
de-interlacing process and the de-interlaced images also 
preserve clear visualization on shapes and edges. 
 
 
Figure 7 Motion adaptive de-interlacing [3, 9] for “Table tennis” sequence 
(th=20) 
 
 
Figure 8  Proposed new motion adaptive de-interlacing for “Table tennis” 
sequence (th1=5, th2=50, th3=40) 
 
 
 
 
 
 
 
 
 
 
 
Figure 9(a) 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 9(b) 
Figure 9  Comparison of different de-interlacing methods with (a) “Table 
tennis” video (b) “Foreman” video sequences 
V. CONCLUSION 
To achieve the same subjective de-interlacing results as the 
motion compensated based methods but to require smaller 
hardware and computation complexities than these methods, 
the efficient motion adaptive de-interlacing algorithm with the 
edge-oriented LCI based method is developed in this paper. 
The proposed algorithm is composed of the improved 4-field 
motion detection and three de-interlacing modes, which are 
the inter-field averaging, the LCI with expanded edge 
detection, and the 3-D median LCI methods.  Simulation 
results show that our proposed LCI-based method with the 
edge preservation provides better objective and subjective de-
interlacing results than the other non-motion compensated de-
interlacing methods.  In addition, the proposed scheme also 
provides the same subjective de-interlacing results as the 
motion compensated based methods. 
 
Inter-field average                                                   3-D ELA [4]
LCI [6]                                                         Proposed
Inter-field average                                                   3-D ELA [4]
LCI [6]                                                        Proposed
1國立中興大學 范志鵬副教授 出席國際會議報告
2010 年 1 月 5 日
報告人姓名 國立中興大學 電機工程系
范志鵬 副教授
會議期間及地點 2009/11/23 - 2009/11/26
新加坡
會議名稱 (中文) 2009年 國際電機電子學會第10區(亞太區)會議,
新加坡
(英文 ) 2009 IEEE Region 10 Conference ( Tencon 2009 ),
Singapore
發表論文題目
(共 2 篇)
1. (中文) 應用於高階 QAM 纜線通訊系統之二階段基於通用多模數與軟決
策之快速盲蔽式等化器
(英文) Two-Stage Generalized Multilevel Modulus and Soft Decision-Directed
Based Fast Blind Equalization for High-Order QAM Cable Systems
2. (中文) 保持邊緣且低複雜度內插的移動偵測適應性解交錯演算法
( 英 文 ) Motion Adaptive De-interlacing Algorithm with Edge-based
Low-Complexity Interpolation Method
1. 參加會議經過
2. 與會心得
3. 參與此次會議的照片集
4. 在此會議發表的論文內容
3由研討會的議題觀察，使用智慧型 /低功耗的電路與系統設計已是
一重要的研究與發展趨勢。重要的 Keynote 演講如下，包括智慧型多媒體
人機介面設計的探討，及智慧型供電系統的發展等議題:
Keynote Speech 1 (24 November 2009)
The Nexus of Science: Key to Spawning New Innovations and Discoveries,
By Professor Teck Seng LOW, Deputy Managing Director (Research) and Executive,
Director SERC Agency for Science, Technology & Research (A*STAR)
Keynote Speech 2 (24 November 2009)
Advances in Evolutionary Optimisation in Modern Power Systems,
By Professor Kit Po WONG, Chair Professor, Department of Electrical Engineering,
Hong Kong Polytechnic University
Keynote Speech 3 (25 November 2009)
Digital Convergence: Challenges and Opportunities,
By Dr. Sanjoy PAUL, Associate Vice President, Infosys Technologies Limited
Keynote Speech 4 (25 November 2009)
New Challenges of Advanced Multimedia Processing for Intelligent
Human-Computer Interface (AMP-IHCI),
By Professor Jhing-Fa WANG, Chair Professor, Department of Electrical Engineering,
National Cheng Kung University
而在論文發表期間，國外學者在現場也不吝提出問題給予指教。而除了本身
的論文發表之外，本人也參加了一些相關領域的 sessions，瞭解到了別人的研究
進度與方向，並與參加會議之教授與專家請教，更進一步的瞭解目前學術界與工
業界的研究與需求，以期自己所做的研究能與時代潮流結合。
53. 參與此次會議的照片集
Tencon 2009 會議的開幕式
Tencon 2009 會場之一
Keynote 的專家演講
參加會議
論文發表
論文發表後
 978-1-4244-4547-9/09/$26.00 ©2009 IEEE                                                                                TENCON 2009 
 
Two-Stage Generalized Multilevel Modulus and Soft 
Decision-Directed Based Fast Blind Equalization for 
High-Order QAM Cable Systems
 
Hao-Jun Hu and Chih-Peng Fan* 
Department of Electrical Engineering, National Chung Hsing University, 
250 Kuo-Kuang Road, Tai-chung 402, Taiwan, R.O.C. 
Email: cpfan@dragon.nchu.edu.tw * 
 
 
Abstract— In this paper, a fast blind equalization with a two-
stage generalized multilevel modulus algorithm (GMMA) and 
soft decision-directed (SDD) scheme is proposed for high-order 
QAM (64/256/1024QAM) systems on the downstream wired cable 
channel.  The proposed fast blind equalization algorithm applies 
the two-stage convergence scheme and the SDD part uses the 
scheme with an adaptively selected decision region.  In the first 
convergence stage, the joint GMMA and modified SDD 
equalization is applied for the purpose of the fast convergence.  
When the convergence process reaches the enough steady state, 
the convergence detector changes the first stage equalization to 
the second stage.  In the second stage, the modified SDD scheme 
is applied to reduce the mean square error (MSE) further. On the 
wired cable channel at the 64/256/1024QAM modes, the proposed 
blind algorithm performs faster convergence speed than the 
previous blind methods do; meanwhile, the proposed algorithm 
also achieves smaller MSE than the other methods at the same 
signal-to-noise ratio (SNR). 
Keywords- blind equalizer; fast convergence; high-order QAM  
I.  INTRODUCTION 
In wired cable communication systems [1, 2], the received 
symbol has phase and amplitude distortions, which are caused 
by multi-path channels. Thus, the blind equalization overcomes 
unwanted channel effect and avoids adding training data, and it 
saves the transmitter bandwidth. Two common indices are 
referred to judging the performances of the blind equalization, 
i.e. the convergence speed and the converged mean square 
error (MSE).  The well-known blind equalization algorithm, 
which is called the constant modulus algorithm (CMA), is 
proposed by Godard [3].  The CMA method is one of the 
typical blind equalization methods.  When the CMA method is 
applied to QAM modulations, all received symbols are 
converged toward the same constant modulus value.  The 
typical CMA method can not correct the phase offset of the 
received symbols; therefore, a phase recovery circuit is 
required for the phase correction. In order to solve the problem 
of a little bit phase offset in the CMA method, the multi-
modulus algorithm (MMA) [5, 7, 8, 9], or called the modified 
constant modulus algorithm (MCMA) [4] is proposed to 
provide the ability of the phase recovery. However, since the 
CMA and MMA methods perform slower convergence speed 
and higher MSE, then several hybrid blind algorithms [10, 11] 
are proposed to research how to speed up the convergence 
process of blind equalizations and how to reduce more MSE 
after convergences.  In order to reduce the MSE further, the 
researchers [4, 10, 11] apply the decision-directed (DD) 
scheme in the equalizers.  
According to the GMMA and SDD methods, we develop a 
new two-stage blind equalization scheme, which 
simultaneously performs higher converge speed and lower 
MSE than the previous schemes for high-order QAM cable 
systems, especially at the 1024QAM mode. The rest of this 
paper is organized in the following. In Section 2, the baseband 
model is introduced for simulations.  The related equalization 
algorithms, i.e. GMMA and SDD with an adaptively selected 
decision region, are introduced in Section 3. In Section 4, the 
proposed efficient two-stage fast blind equalization algorithm 
is discussed. The simulation results and performance 
comparisons are shown in Section 5. Finally, we give a 
conclusion at the end of this paper. 
II. BASEBAND SYSTEM MODEL 
The receiver side signal in the baseband cable system can be 
described as follows. 
∑−
=
+−⋅=
1
0
)()()(
L
i
i kwikahkx
 ,                              (1) 
where hi means the channel impulse response (CIR), L is the 
order number of CIR, and a(k) is the complex QAM 
modulation symbol. The w(k) is the additive white Gaussian 
noise. The output symbol after computing the convolution of 
the blind equalization is described as follows. 
       1
0
( ) ( ) ( ) ( ) ( )
m
T
i
i
k c k x k i k k
−
=
= − =∑Z C X ,                   (2) 
where x(k) is the input signal to the blind equalizer, ( )ic k  
denotes the complex coefficients of the equalizer, m means 
the number of the equalizer coefficients, and ( )kZ  is the output 
of the equalizer. Let ˆ( )kZ be defined as the decision output 
after the decision maker. 
III. RELATED EQUALIZATION  ALGORITHMS 
3.1 Generalized Multilevel Modulus Algorithm (GMMA) 
In [7], the multilevel modulus algorithm (MMA) was 
proposed.  Firstly, the cost function of the MMA method is 
described as 
                                       
 
The first term in (8) is equivalent to the cost function of 
the GMMA method.  Then the second term in (8) is similar to 
the cost function of the SDD method.  By adding the second-
term SDD-like cost function, the equalizer can converge not 
only on the modulus values but also on the nearest decision 
symbols.  By applying the stochastic gradient algorithm, the 
error function derived from (8) is described as 
24
i
i
2 2 i
i 24
i
i
z( k ) z
exp( ) ( z z( k ))
2e( k ) 4 ( z( k ) R ) z( k )
z( k ) z
exp( )
2
σ
σ
−
− ⋅ −
= ⋅ − ⋅ +
−
−
∑
∑
    .        (9) 
Since the error function of the GMMA part can be separated 
into the real and imaginary parts, then (9) becomes 
  
        = 
24
i
i
2 2 2 2 i
R R i I I i 24
i
i
GMMA SDD
z(k ) z
exp( ) ( z z(k ))
2e(k ) 4 { z (k ) ( z (k ) R ) z (k ) ( z (k ) R )}
z(k ) z
exp( )
2
e (k ) e (k )
σ
σ
−
− ⋅ −
= ⋅ ⋅ − + ⋅ − +
−
−
+
∑
∑
   ,   
(10) 
where  
2 2 2 2
GMMA R R i I I ie ( k ) 4 { z ( k ) ( z ( k ) R ) z ( k ) ( z ( k ) R )}= ⋅ ⋅ − + ⋅ −   , 
and 
       = 
2 24 4
i i
SDD i
i i
z( k ) z z( k ) z
e ( k ) exp( ) ( z z( k )) exp( )
2 2σ σ
− −
− ⋅ − −∑ ∑   . 
Then the coefficients of the equalizer are updated with (11) as 
     1 ( ) { ( ) ( )) ( )GMMA GMMA SDD SDD(k ) k e k e k k  μ μ ∗+ = + ⋅ + ⋅ ⋅C C X ,    (11) 
where  
GMMAμ and SDDμ  are the step sizes for the GMMA and 
SDD parts, respectively.  The derivations for acquiring (10) 
are similar to those in [10] and [11].  At the beginning, since 
the outputs of the equalizer are scattered on the I-Q 
constellation plain, the values of the decision outputs are not 
accurate, and then the equalized symbol values have several 
decision errors. Thus, the more accurate equalization process 
is necessary to reduce the decision error further.  
 
4.2 Stage II : SDD Mode 
In the equalization process of Stage I, the proposed 
equalization algorithm can perform faster convergence speed 
than the other blind methods. When the proposed equalization 
achieves the enough steady state, the GMMA-part cost 
function in the first term of (10) can not provide the ability to 
reach the optimum points through the stochastic gradient 
algorithm, and then the error function of the GMMA part also 
operates the harmful influence during the equalizations.  To 
reach less MSE, when the equalization process goes to the 
enough steady state, the GMMA part will be discarded. Only 
the cost function of the SDD part is considered to be the 
further equalization. Then the coefficients of the equalizer are 
updated with (12) as 
1 ( ) ( ) ( )SDD SDD(k ) k e k k  μ ∗+ = + ⋅ ⋅C C X  .              (12) 
Based on the two-stage equalization process, the values of 
the error function are further decreased and the convergence 
speed will be raised again.  To detect the convergence for the 
stage change, we apply the convergence detector [11] in the 
proposed algorithm.  As shown in Figure 3, the convergence 
detector is considered when the equalization process is 
switched from the GMMA+SDD mode to the SDD mode for 
the further convergence. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3  Block diagram of the proposed algorithm 
4.3 Variable Step Size without Division 
During the blind equalization, if the step size is fixed, it is 
difficult that the convergence speed is fast, and simultaneously 
the steady MSE is small.   In [6], the scheme of the variable 
step size is used to obtain fast convergence and small steady 
MSE for equalizations, and it is shown in (13).   The concept 
of the variable step size is that the step size can be adaptively 
changed according to the value of the error function. 
      
G M M A
G M M A 2
G M M A
( n )( n 1 )
1 ( n ) e( n )
μμ
λ μ
+ =
+
          .                 (13) 
Then 
λ=0    ;    when  sgn(Re{e(n)})=sgn(Re{e(n-1)}) , 
and   sgn(Im{e(n)})=sgn(Im{e(n-1)} , 
λ=1 ;   other else , 
where “sgn” is the sign function and e(n) is the error function 
of the GMMA equalization.  By using the following equation,  
1,1
1
1 2 <+++=
−
xxx
x
"    . 
The variable step size in (13) can be approximately replaced 
with 
                     
2
GMMA GMMA
GMMA
( n 1 ) ( n ) { 1 }
( n ) ( 1 )
μ μ α α
μ α
+ = ⋅ + + +
≅ ⋅ +
"  ,            (14 ) 
where 2
GMMA( n ) e( n )α λμ= − . It is noted that the computation in 
(14) does not need any division, compared with that in (13).  
The initial step size 
GMMA(0 )μ  is set to a small positive and 
changeable value according to different communication modes.  
V. SIMULATION RESULTS AND COMPARISONS 
To compare the proposed algorithm with the previous 
blind algorithms at the 64, 256, and 1024 QAM modes, the 
downstream wired cable channel model shown in Table I is 
applied for system simulations. To examine the relationship 
between MSE and equalizations, the carrier and the timing 
synchronizations are assumed to be perfect.  
The parameters in our simulations are described as 
follows. The symbol rate is assigned to 5.2 Msymbols/sec at 
the 64, 256, and 1024QAM modes. The structure of the 
equalizer is 1/2 symbol fractional-spaced. The results of the 
equalized constellations and the MSE are simulated under 
30dB, 35dB, and 40dB SNRs at the 64, 256, and 1024QAM 
modes, respectively. The tap numbers of the equalizer are set 
to 16, 16, and 32 at the 64, 256, and 1024QAM modes, 
respectively. In the applied convergence detector [11], L 
equals 10 and refd  is set to 0.8 at the 64 and 256QAM modes, 
and L is 10 and refd  is set to 0.3 at the 1024QAM mode.  
 
)(ˆ kz)(ka )(kz
First stage
GMMA+SDD
Algorithm
Quantizer
Q(·)
CIR  
h(t) EqualizerQAM Modulation
Binary data
)(kw
)(kx
Second stage
SDD
Algorithm
Convergence
Detector
_+
                                       
 
VI. CONCLUSION 
A fast blind equalization with the two-stage GMMA and 
modified SDD scheme is proposed for high-order QAM 
systems on the downstream wired cable channel. The 
proposed fast blind equalization algorithm applies the two-
stage convergence scheme.  In the first stage, the joint GMMA 
and modified SDD equalization is applied for the purpose of 
the fast convergence.  In the second stage, the modified SDD 
is applied to reduce the mean square error further. On the 
wired cable channel at the 64/256/1024QAM modes, the 
proposed blind algorithm achieves faster convergence speed 
and smaller MSE than the previous blind methods. 
ACKNOWLEDGMENT  
This work was supported in part by the National Science 
Council, Taiwan, R.O.C., under Grant NSC97-2221-E-005-
088-MY2. 
REFERENCES 
 [1] Data-Over-Cable Service Interface Specifications (DOCSIS) 2.0 Radio 
Frequency Interface Specification CM-SP-RFIv2.0-I11-060602, June 2, 
2006.  
[2] T. Kurakake, N. Nakamura, and K. Oyamada, “A Blind 1024-QAM 
Demodulator for Cable Television,” Int. Zurich Seminar on 
Communications (IZS), Feb. 18-20, 2004. 
[3] D. Godard, ”Self-Recovering Equalization and Carrier Tracking in Two-
Dimensional Data Communication Systems”, IEEE Transactions on 
Communications, vol. 28, pp. 1867-1875, 1980. 
[4]  K. N. Oh and Y. O. Chin, “New Blind Equalization Techniques Based on 
Constant Modulus Algorithm”, IEEE Conference Global 
Telecommunications, vol. 2, pp 865-869, 1995.  
[5] K. N. Oh, “A Single/Multilevel Modulus Algorithm for Blind Equalization 
of QAM Signals,” IEICE Trans. vol. E80-A, No. 6, pp.1033-1038, June 
1997. 
[6] A. R. Esam, “Blind Adaptive Equalization with Variable Step Size”, 4th 
International Conference on Information & Communications Technology, 
pp.1-1, Dec. 2006. 
[7] J. Yang, J. J. Werner and G. A. Dumont, “The Multimodulus Blind 
Equalization Algorithm,” 13th Int. Conf. Digital Signal Processing, 
Santorini, Greece, July 2-4, 1997. 
[8] J. Yang, J. J. Werner, and G. A. Dumont, “The Multimodulus Blind 
Equalization and Its Generalized Algorithms,” IEEE Journal on Selected 
Areas in Communications, vol. 20, no. 5, pp. 997-1015, June 2002.  
[9] J. T. Yuan and K. D. Tsai, “Analysis of the Multimodulus Blind 
equalization Algorithm in QAM Communication Systems,” IEEE 
Transactions on Communications, vol. 53, no. 9, pp. 1427-1431, September 
2005. 
[10] S. Chen, “Low Complexity Concurrent Constant Modulus Algorithm and 
Soft Decision Directed Scheme for Blind Equalization”, IEE Proceedings 
Image and Signal Processing, vol. 150, pp.312-320, 2003. 
[11] C .P. Fan, W. H. Liang, W. Lee, “Fast Blind Equalization with Two-
Stage Single/Multilevel Modulus and DD Algorithm for High Order QAM 
Cable Systems,” IEEE International Symposium on Circuits and Systems, 
Seattle, USA, May 2008. 
 
                                       
 
The architecture of the whole proposed algorithm is shown in 
Figure 2. The new motion adaptive de-interlacing algorithm is 
described as follows. The proposed de-interlacing method by 
using the pixel-based motion detection is very cost-effective. 
Firstly, we exploit three buffers to store three-field 
information (i.e. backward field, forward field and forward-
forward field).  Then all pixels in buffers and the current field 
are fed into the improved 4-field motion detector. The purpose 
of the improved 4-field motion detection is to detect whether 
the interlaced pixel is a motion, a static, or an uncertain pixel. 
Secondly, the improved 4-field motion detection generates a 
selected signal to the choice line. The multiplexer module 
exploits the choice signal to select one from the three de-
interlacing modes.  If the pixel is a static one, the new motion 
adaptive de-interlacing scheme applies the well-known inter-
field averaging method.  If the pixel is an uncertain one, the 3-
D median LCI method is used.  If the pixel is a motion one, 
the LCI with the expanded edge detection is applied. Finally, 
the progressive frame is produced from the interpolated field 
or the current field according to “the interleave by line” 
module. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2 Block diagram of the proposed de-interlacing algorithm 
 
The following subsections describe the details of the 
improved 4-field motion detection and the new motion 
adaptive de-interlacing scheme. 
 
3.1 Improved 4-field motion detection 
The 4-field motion detection proposed in [3, 9] is applied 
to the motion detection. The motion detection [3, 9] is 
effective. However, owing to the unsuitable detection, it can 
not solve afterimage phenomenon after the de-interlacing 
process when the video object moves fast, especially the fast 
bouncing object.  To identify the motion pixels correctly, we 
propose the improved 4-field motion detection method, and its 
diagram is shown in Figure 3. 
 
E
C
H
A
G
F
X
D
B
n-2 n-1 n n+1  
Figure 3 Diagram of improved 4-field motion detection 
The algorithm of the improved motion detection is described 
as follows. Let 
d1 = abs( C − D )  
d2= abs( E − F )  
d3 = abs( A − B )  
d = max( d1,d2,d3 )                .                      (2) 
d4= abs( D − G ) 
d5= abs( F − H ) 
d6= (d5+d4) / 2 
Then 
if (d ≤ th1 and d6 ≤ th2) 
{ Static pixel } 
else if (d > th1 and d ≤ th3 and d6 ≤ th2)  
{ Uncertain pixel } 
else 
{ Motion pixel }              , 
where X is the interpolation point, and then th1=5, th2=50, and 
th3=40 in our design. The “d1”, “d2”, “d3” and “d” are 
originally used in the previous 4-field motion detection [3]. 
The improved 4-field motion detection adds the “d6” to 
increase the probability which judges motion pixels correctly.  
In the previous methods [3, 9], especially for the fast bouncing 
object, the motion detector usually judges motion pixels to be 
static. Thus, the afterimage phenomenon will be generated. 
 
3.2 New motion adaptive de-interlacing scheme 
  The previous motion adaptive de-interlacing scheme in [3] 
only divides the pixel into two groups, which are static and 
motion pixels. They only use one threshold value to detect 
whether a static or a motion pixel happens. By using the 
improved 4-field motion detection in Section 3.1, we propose 
a new motion adaptive de-interlacing scheme with three 
processing modes, which are shown in Figure 2.  The 
discussions of the proposed LCI with the expanded edge 
detection and the 3-D median LCI methods are described as 
follows. 
 
3.2.1 LCI with the expanded edge detection 
We use the LCI method by expanding the edge directions, 
which are expanded from the 3-point to the 5-point edge 
detection.  By the extension of the edge direction, the 5-point 
LCI shown in Figure 4 can achieve better de-interlacing 
results than the 3-point LCI for de-interlacing the motion 
pixel.  However, when the input interlaced video exists noises, 
the 5-point LCI  can not perfectly detect the correct edge 
direction.  If the detection of the edge is unsuitable, the edge in 
the de-interlacing image will be blurred.  Thus, we expand the 
detection range of edges by considering a 5x9 pixel block to 
detect the correct edge, and the details of the de-interlacing 
method are described as follows. 
 
 
 
 
 
 
 
 
 
 
Figure 4  5-point LCI method for edge detection 
 
Step 1: The edge direction is calculated by using the 5-point 
LCI method and the information of the edge direction is stored 
Improved
4-field
motion
detection
Field s torage
Field s torage
Field s torage
Inter-field
averaging
3D median
LCI
LCI with
expanded edge
detection
M ultiplexer
Interleave by line
Field in Backward f ie ld
Current fie ld
Forward  fie ld
Forward-forward fie ld
Choice line
U ncerta in 
M otion
Static
Interpo lat ion fie ld
Current 
field 
Progressive fram e out 
A
F
B
G
C
H
X
D
I
E
J
x-2 x-1 x x+1 x+2
y-1
y
y+1
Edge 1
A
F
B
G
C
H
X
D
I
E
J
x-2 x-1 x x+1 x+2
y-1
y
y+1
Edge 2
A
F
B
G
C
H
X
D
I
E
J
x-2 x-1 x x+1 x+2
y-1
y
y+1
Edge 5
A
F
B
G
C
H
X
D
I
E
J
x-2 x-1 x x+1 x+2
y-1
y
y+1
Edge 4
A
F
B
G
C
H
X
D
I
E
J
x-2 x-1 x x+1 x+2
y-1
y
y+1
Edge 3
                                       
 
IV. SIMULATION RESULTS AND COMPARISON 
4.1 Objective Comparison 
To obtain the objective performance comparison after we 
use the de-interlacing skills, we use the C language to verify 
different de-interlacing algorithms with 352x288 interlacing 
video sequences. Table 1 shows the PSNR comparison after 
different de-interlacing methods are applied.  For most of the 
test video sequences, our proposed method achieves larger 
PSNR values than the motion adaptive method in [3].  Then 
the proposed method also performs larger average PSNR 
values than the previous non motion-compensated de-
interlacing methods.  Table 2 shows the comparison of the 
memory access, buffer size and computational complexity 
among different de-interlacing methods.  In Table 2, our 
method requires similar memory access, hardware, and 
computational complexities to the method in [3].  It is noted 
that our method can provide the same subjectively de-
interlacing results as the motion compensated de-interlacing 
methods.  However, it requires fewer complexities than the 
motion compensated de-interlacing methods do. Then the 
subjective comparison is depicted in Section 4.2. 
 
Table 1 PSNR comparison between our method and the other non motion-
compensated de-interlacing methods 
Video 
Sequence 
(CIF format) 
ELA[1]  3-D 
ELA[4]  
LCI [6] Motion
Adaptive
E-ELA[3
Prop
Weather 29.144 38.195 29.923 40.098 40.219 
Table Tennis 29.043 27.480 29.823 30.073 30.232
Salesman 31.431 35.992 32.656 36.633 36.627 
Mobile 23.382 24.993 25.316 25.367 25.635
Foreman 31.578 33.042 31.528 32.739 32.404
Tempete 27.035 29.042 29.083 30.642 31.094
Average PSNR 28.6 31.457 29.72  32.59 32.7 
 
Table 2 Memory access, buffer size and complexity comparison 
Method Memory Access Buffer 
Size(bytes) 
Computational 
Complexity Read Write 
Bilinear 
Average 
2n n 2 Small 
ELA [1] / 
 LCI [6] 
2n n 6 Small 
Motion 
Compensated 
> 10n n Frame size Large 
Motion 
Adaptive [3] 
5n n 25 Middle 
Proposed 10n n 33 Middle 
n : the number of the interpolated pixels 
 
4.2 Visually Subjective Comparison 
Figure 7 shows the de-interlaced image sequence with the 
motion adaptive de-interlacing method in [3], and then Figure 
8 shows the de-interlaced image sequence with the proposed 
new motion adaptive de-interlacing method. In Figure 7, the 
serious afterimage phenomenon is revealed in the de-
interlaced video sequence.  However, in Figure 8, the defects 
of the afterimage phenomenon are effectively reduced.    
Figure 9 shows the subjective comparison among different de-
interlacing methods.  In Figure 9, it is noted that the unwanted 
interlaced artifacts are efficiently removed after the proposed 
de-interlacing process and the de-interlaced images also 
preserve clear visualization on shapes and edges. 
 
 
Figure 7 Motion adaptive de-interlacing [3, 9] for “Table tennis” sequence 
(th=20) 
 
 
Figure 8  Proposed new motion adaptive de-interlacing for “Table tennis” 
sequence (th1=5, th2=50, th3=40) 
 
 
 
 
 
 
 
 
 
 
 
Figure 9(a) 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 9(b) 
Figure 9  Comparison of different de-interlacing methods with (a) “Table 
tennis” video (b) “Foreman” video sequences 
V. CONCLUSION 
To achieve the same subjective de-interlacing results as the 
motion compensated based methods but to require smaller 
hardware and computation complexities than these methods, 
the efficient motion adaptive de-interlacing algorithm with the 
edge-oriented LCI based method is developed in this paper. 
The proposed algorithm is composed of the improved 4-field 
motion detection and three de-interlacing modes, which are 
the inter-field averaging, the LCI with expanded edge 
detection, and the 3-D median LCI methods.  Simulation 
results show that our proposed LCI-based method with the 
edge preservation provides better objective and subjective de-
interlacing results than the other non-motion compensated de-
interlacing methods.  In addition, the proposed scheme also 
provides the same subjective de-interlacing results as the 
motion compensated based methods. 
 
Inter-field average                                                   3-D ELA [4]
LCI [6]                                                         Proposed
Inter-field average                                                   3-D ELA [4]
LCI [6]                                                        Proposed
無研發成果推廣資料 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
