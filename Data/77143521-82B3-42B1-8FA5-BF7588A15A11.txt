摘要 
由於現今運輸交通便捷，使得經濟發展迅速，然汽車意外事故的發生也趨而頻繁，故
世界先進國家無不致力於發展智慧型運輸系統(Intelligent Transportation System,  ITS)。本
計畫以兩年的時間發展以機器視覺為基之智慧型汽車駕駛輔助系統，主要目的乃藉由障礙
物偵測及車道線搜尋來找出防撞路徑，以提高行車安全性。在第一年計畫中，完成單影像
擷取器之汽車駕駛輔助系統，其中包括：(1)影像擷取器驅動與控制模組，(2)影像灰階化處
理模組，(3)特徵點搜尋模組，(4)車道線判別模組，(5)車道線追蹤模組。第二年則發展雙影
像擷取器之汽車駕駛輔助系統，新增模組包括：(1)三維重建特徵點對應模組，(2)障礙物判
別模組，(3)車距與車寬計算模組，(4)汽車導航輔助模組。最後比較單、雙影像擷取器駕駛
輔助系統之效能，並進行系統功能修正。研究結果顯示本研究所提方法可適用於外界環境
光源變化之道路線偵測，可於有限時間內求出雙影像之對應點並偵測前方物體之距離，以
輔助汽車駕駛。藉由此機器視覺於智慧型汽車駕駛輔助系統之研究，期望能降低交通意外
事故發生率，進而提昇台灣汽車相關產業競爭力。 
關鍵詞：汽車駕駛輔助系統、機器視覺、障礙物偵測、車道線偵測、三維重建 
 
Abstract 
The rapid growing transportation has a critical affect on economic development. However, 
the growing volume of traffic cause large amount of traffic accidents and it requires higher and 
higher level of traffic safety. Recently, the intelligent transportation system (ITS) has attracted 
much attention. Many advanced countries are devoted to the development of ITS. Consequently, 
the research attempts to develop a vision-based intelligent driver assistant system (DAS) in order 
to enhance the traffic safety and raise the competitiveness of the automotive accessory industry. 
Main function of the system is to find the collision avoidance path by obstacle detection and lane 
tracking. In the first year, this project developed an intelligent DAS by utilizing a single camera. 
Several modules are included in the system: (1) sensor driving module, (2) image graying module, 
(3) feature detection module, (4) lane detection module, (5) lane tracking module. The second 
year’s project built a DAS with two cameras. Some new modules will add to the system, 
including (1) features matching module, (2) obstacle detection module, (3) lateral and 
longitudinal distances determining module, (4) guiding module. Experimental results show the 
proposed lane markings detection method can adapt to the environmental change, and the new 
stereo matching algorithm provides an acceptable obstacle detection result. The integrated system 
therefore can provide a collision avoidance vehicle guiding path and reduce the probability of 
traffic accidences. 
Keywords: Driver assistance system (DAS), machine vision, obstacle detection, lane detection, 
3-D reconstruction 
撞部份的應用，在市場上將具有一定的競爭力。 
綜合上述應用機器視覺於道路線與障礙物偵測及防撞方面之討論，為降低系統安裝成
本，提高汽車業者競爭力，並改善交通事故發生率，本計劃以兩年時間發展出一以機器視
覺為基礎之智慧型汽車駕駛輔助系統(Driver Assistance System, DAS)，藉由機器視覺、影像
處理、及最佳化(Optimization)對應演算法等技術之結合，偵測道路線及障礙物，以輔助汽
車駕駛之行車安全，並提供相關業者開發汽車導航資訊系統之參考。研究目的具體陳述如
下： 
¾ 建構用於導航輔助之單影像擷取器及雙影像擷取器之機器視覺系統。 
¾ 發展一影像差異分群之快速搜尋演算法。 
¾ 發展車道線偵測之影像處理方法。 
¾ 發展障礙物偵測之影像處理方法。 
¾ 發展一快速之雙影像特徵對應方法。 
¾ 根據車道線及障礙物判斷發展一導航輔助系統。 
 
2. 文獻探討 
由於本研究乃以車道線偵測、障礙物偵測為研究方向，並預計以機器視覺作為本研究
工具，因此對於相關文獻分為數個主題探討。首先進行機器視覺與影像處理之概略說明，
接著分別對車道線偵測與障礙物偵測之文獻進行評析，最後則是對於利用雙影像進行三維
重建所需的特徵點搜尋與特徵點對應之文獻加以探討。 
2.1 機器視覺與影像處理 
現今電腦運算速度與精準度可謂成熟，故利用機器視覺於檢測系統以彌補人眼因疲勞
或疏忽所導致錯誤或在危險環境中應用。機器視覺系統利用影像擷取器將將光子經光敏二
極體轉為電荷，並依據電荷之數據，給予一梯度數值，梯度數值即可表示亮度，而一般色
彩是由紅、綠、藍所組成，如此便可獲得三色之梯度數值，此三色能建構出大部分可見之
色彩(Gonzalez and Woods, 2001)。最後再將像素(個別梯度數值)給予陣列方式排列，而獲得
彩色圖像，並將這些數值給予運算處理，使其用於辨識檢驗等。 
而在影像處理過程中必存各類雜訊，為抑制此類雜訊，常見法有平均值濾波器
(Equalization Filter)、中間值濾波器(Median Filter)、羅伯特(Roberts)濾波器、拉普拉斯(Laplace)
濾波器…等，再作旋積(Convolution)運算，如式(1)。 
  ∑
=
=
9
1
5
i
ii ZII                 (1) 
其中九宮格中心值 5I 為九宮格灰階梯度數值 iI 各別乘上濾波器權重 iZ 之總和。 
雜訊抑制後，在一般機器視覺處理上，常使用二值化(binary)處理，二值化的目的在於
將影像背景與待測物予以分離，將原本之 256 梯度之數值轉為布林值(boolean)，常見的方
法有平均閾值法、OTSU 等。若二值化後仍無法順利將雜訊隔離，即可透過如：膨脹、侵
表 2. 1、車道偵測相關文獻比較 
(1) Bertozzi and Broggi (1988) 
優點：立體視覺的部份之探討與道路線的條件式的取得。 
缺點：條件式的嚴謹性不足。 
(2) Kluge and Thorpe (1995) 
優點：1.在判別道路線之條件式較以往完整。2.採用之HSI方法可用於降低光線因素影
響。 
缺點：1.在實際的情況中，以條件式的方式，可能遺失非預想內的道路線。2.處理的道
路線資訊較為不完整。  
(3) Broggi et al. (2000) 
優點：可重建被陰影遮蔽之道路線。 
缺點：模糊規則訂定的主觀條件。 
(4) 林重甫 (2001) 
優點：以單影像擷取器建立行進路線，並以二次曲線對道路線做預測，以減少再次搜
尋時間。 
缺點：1.Sobel與二值化閾值之選用皆較無法因應時間光線變遷。2.八鄰近法細部運算並
未說明，運送處理的方式不同，將可導致結果的差異。3.道路並非為二次曲線，
其誤差在任何路況不一定皆能符合。4.假設道路為平坦。 
(5) 余佳任 (2004) 
優點：改變以往直接以Sobel處理的方式，有效改善陰影造成的問題。 
缺點：1.只能針對連續線段，若道路線有所殘缺或無道路線時有可能比對錯誤。2.以顏
色分群，在論文中測試路線為黃線，並未考慮白色道路線。3.在以寬度作比較
是以平坦路面、已經道路線模組為前提，並未考慮其他外在因素。 
                                         
2.3 障礙物偵測 
Gandhi et al. (2000)利用圖面模糊化促使不必要雜訊減少干擾進而搜尋特徵，並利用這
些特徵點關連性作為判斷法則。在搜尋特徵方面 Burie et al. (1995)針對車輛特徵作細部說
明；曾俊元(2003)以搜尋差異處當作特徵點，其利用顏色模糊聚類分析的方式，獲得前方障
礙物的梯度差異，並考量路面路標、斑馬線等的影響，建立了一些條件式，像是車輛邊緣
具有陰影的效應，而成功的將雜訊去除。在距離計算上謝衛中(2002)利用單影像擷取器及單
線掃描的方式，的確能減少處理時間，然卻帶有前方環境資訊不夠充裕的危險性；而劉正
一(2000)採用上下兩攝影機平行擺放，以較快速特徵點對應的方式，建立三維關聯，而在偵
測行人的部份與謝衛中相似，以模板的方式比較。在偵測車輛方面，謝衛中(2002)先行找尋
較為陰暗的部份，橫向結合，再依底部像素判別是否為障礙物；此外也有以邊緣偵測利用
比對的方式來判斷前方物體障礙物，如 Chen and Tsa( 2000) 以數學模式建構出三輪模擬車
所能轉彎其弧度為何，並利用物體於地面或立起時的傾斜角度，比對判斷，不過仍有遮蓋
與無法得知高度的缺點，故有學者提出以雙影像擷取器來補足單影像擷取器不足部分。以
線特徵點往往皆利用道路線特性，然而外在的環境是多變的：光線、時間、陰影、殘缺、
彎曲路線等因素皆會嚴重影響結果，故條件式建立的嚴謹性與彈性化則為判斷辨識成功率
的重大考量。 
而在障礙物偵測上，常見論文主分為單影像與雙影像兩部份。單影像擷取器在機器視
覺應用上，又往往必須建立許多條件式來判別，然而在自然環境底下，是很難找到規則能
涵蓋如此多樣的變異，故本研究最終決定以雙影像擷取器來對此系統作建構。但是雙影像
處理也因兩擷取器之亮度、角度不同，在尋找特徵點及對應點上，有其困難之處。此外，
雖然由文獻可知，兩平行擺放之擷取器搜尋速度較為快速，然實際應用時，會因人員放置
攝影機與行車移動所造成攝影機偏移原有位置的而造成些許誤差。 
 
2.4 特徵點搜尋 
一般而言在機器視覺的應用中，就算只存有一些光線變化，仍會導致結果不同，所以
通常工業檢驗，常利用遮蓋物將待測物控制在某種環境之下，然而在自然環境中，光線的
變化更是劇烈，如何在如此的環境底下找尋出特徵點，是值得研究的問題。Ruichek and 
Postaire (1999)採用的方法為判定邊緣點的部份當做所謂的特徵點，由於邊緣點的梯度差異
較非邊緣點的梯度差異來的變化大，如圖 2.1 內黑色橫線為掃描區段，紅色線條為此黑線
線段之左右像素灰階差異折線圖，故可得知在道路線與道路上之交界處，也就是邊緣所造
成的變化的確較為突兀，因此可利用梯度差異找尋邊緣。然而利用梯度差異值是否大於固
定閾值判別邊緣點的方式雖較 Sobel 佳，但固定閾值法在實際應用時仍有閾值 t 定義問題。 
 
 
圖 2.1、邊緣灰階梯度較大範例圖 
 
蕭賢德(2000)根據 Robinson 運算子找尋出角落邊緣特徵點。在影像強化上是給定一閾
值，此閾值以下將變暗，而以上則將之提升亮度，此種對比擴張技術(contrast stretching)，
其公式如式(2)所示。若車輛色彩偏於平均以下，則將會與道路壓縮至同一區塊，故此方法
較不適合於於自然環境下之應用。 
 
  averageaverageGaS ii +−= )(            (2) 
 其中 ∑∑
= =
= x
yN
x
N
yyx
yxP
NN
average
0 0
),(1 ，代表原始影像的平均灰階度值  
    霍普斐爾-坦克網路是利用李亞普諾夫函數(式 5)的收斂原則， 
  ∑∑ ∑+−=
i j j
jjjij XXWE θ)2
1(             (5) 
其中 iX 為第 i 個狀態，數值為 1 或-1(其值參考式 6)； ijW 等於 jiW 、 iiW 等於零、 jθ 為
權重參數。 
  
⎪⎪
⎪
⎩
⎪⎪
⎪
⎨
⎧
<−
=−
>−
−
=
∑
∑
∑
+
j
i
n
jij
j
i
n
jij
j
i
n
jij
n
i
n
i
XW
XW
XW
if
if
if
XX
0
0
0
1
1
1
θ
θ
θ
           (6) 
    Ruichek and Postair (1999)是以兩攝影機平行擺放，並導入霍普斐爾-坦克網路模式，步
驟說明如下： 
首先為設計限制條件與目標，限制式包括特徵點對應具有唯一性、無多對應點的情況
發生；此外亦有左特徵點之對應點必在右特徵點對應點之左。而目標則為期望對應特徵點
間的差異為最小。 
將上述的限制條件與目標轉為數學形式，唯一性限制式為式(7)(8)， LRE 是左圖(L)與右
圖(R)所對應的值，對應時其值為 1；由於是唯一性，故由 1 去減此行數值最佳配對時應為
0；若不具唯一性對應則式(7)、(8)的值將大於 1。而第二項條件則如式(9)所列，其代表的
意義為當 ''LL XX > 時， 'RR XX > ，換言之若 LX 在 'LX 左邊時，則其在右圖所對應到的特徵
點 RX 必在 'RX 左方。而其目標式則代表其特徵差異值，式(10)則是目標值特徵點間的差異
性，w 為參數。 
 
  ∑ ∑−
L R
LRE
2)1(               (7) 
  ∑ ∑−
R L
LRE
2)1(               (8) 
  ∑∑
LR
RLLRRLRL EEO ''''                  (9) 
  其中 |)()(| '''' RRLLRLRL XXSXXSO −−−= ， 01)( >= aifaS  
  1
1
2)( )('' −+== −wxRLRL eXCC              (10) 
 
 
 
量測等給予測試。 
 
雙影像擷取器架設與驅動
雙影像灰階
計算物體三維座標
供車道、障礙物分割
開始
單影像(左)車道線搜尋
並判斷是否駕駛偏移特徵點搜尋
左右影像特徵點對應
利用單影像成像原理
計算影像擷取器焦距
利用單影像成像原理
計算影像擷取器至道
路距離
利用雙影像成像原理
計算影像擷取器至物
體距離、偏移量
利用影像擷取器至道
路距離、影像擷取器
至物體距離推估物體
高度
測試
測試行車偏移之車道
線偵測
利用單影像成像原理
與實距，測試焦距計
算之可行性
測試雙影像測距
於同平面上，測試比
較單影像與雙影像
較遠距離測試
影像擷取器成像形變
量測
結束
 
圖 3. 1、智慧型汽車駕駛輔助系統架構圖 
 
3.1 單影像擷取器驅動與控制 
將具 USB 傳輸介面之影像擷取器(CCD 或 CMOS)架設於車上以便擷取道路資訊，並
利用 Windows API 驅動影像擷取器再加以控制各項擷取參數。為便於後續研究發展，本研
究中以彩色(因彩色資訊量較灰階多)定焦 CMOS 相機取像，並以 USB 傳輸視訊。在架設部
份，本研究採平行傾斜擺放，目的於此法可減少處理時間與增加精度。由圖 3.2 可知兩平
行擺放之影像擷取器，兩鏡頭(Lx1，Ly1)(Rx1，Ry1)高度相同，兩成像同 Y 座標上高度也
亦相同，故左影像在同高度下之兩點與自身鏡頭所構成的平面(座標(Lx1，Ly1)、(Lx2，Ly2)、
(Lx3，Ly3)所構成平面)，應同於另ㄧ擷取器此 Y 座標下與自身鏡頭所構之平面(座標(Rx1，
Ry1)、(Rx2，Ry2)、(Rx3，Ry3)所構成平面)。假若於左影像(X,Y)座標特徵點找尋右影像之
對應點，相信搜尋右影像同 Y 座標即可，而傾斜擺放，路面於鏡內的成像相對比率較重，
所代表之像素點也較多，故在距離精度的計算上，有其幫助。 
接著對架設之影像擷取器，進行驅動控制，本研究以 Windows API 從影像匯流以每秒
15 張影像取出資料，以時速約 60~80 公里等於高速公路進行拍攝，至於一般道路則以時速
約 25~40 公里進行取像。 
 
 
 
圖 3.3、搜尋車道線道路灰階計算區塊示意圖 
 
(2) 設定搜尋方式：由於車道線具有連續、彎曲、間斷等樣式，故本研究採用圖 3.4 搜尋方
式，此方式能搜尋到間斷線段。而搜尋停止條件定義為當 I(x,y)<初始分割閾值
I(x-1,y)< roadThreshold 、 I(x,y)> roadThreshold 、 I(x+1,y)> roadThreshold 、
I(x+2,y)> roadThreshold 即判定為車道線。 
 
 
圖 3.4、搜尋車道線方向圖 
 
(3) 更新閾值：由於初始閾值只用於初期約略計算車道線，為求更為精準，研究中將搜尋到
之車道線部份，平均其灰度為 eaveragelinGrayLevel ，更新後之閾值 roadThreshold 如下： 
 
roadThreshold = 2
)( eaveragelinaverage GrayLevelGrayLevel +        (16) 
 
(4) 計算車道線角度：在計算車道線角度上，採邊緣三點量測法，所謂三點量測法及利用步
驟(2)停止條件所搜尋到的點 P(x,y)上下各三像素尋找邊緣。對於車道彎曲的處理，縱使
車道線是彎曲的，在極短的距離間仍保持著直線的關係，當由上述方法搜尋到車道線邊
緣如圖 3.5 之 A, B, C 某一點後，先行計算其傾斜角度，並以此傾斜角度做延伸搜尋，
以加速車道線搜尋時間。 
 
3.4 特徵點搜尋 
當利用雙影像擷取器取得影像以便偵測前方障礙物時，本研究採邊緣點為特徵點，邊
緣之特性為梯度差異較大者，若能獲得梯度差異較大即得邊緣。本研究以多次多處 K-Means
特徵點搜尋法，將梯度差異較大之邊緣與梯度較小者予以分隔，此方法可避免光線不同所
造成之閾值差異。由於其搜尋法單次僅能搜尋ㄧ線段內差異較高處，其步驟如下： 
(1) K-Means 特徵點搜尋法輸入值：從一 m×n 影像擷取一 X 座標垂直方向灰階梯度絕對值
為輸入值。 
(2) 設定 K-Means 特徵點搜尋法初始群心：將前述輸入值找最大值 Gmax 為差異大群心，
而差異小群心則以 0 表示 (圖 3.8)。 
(3) 計算 K-Means 特徵點搜尋法分割閾值(Threshold)：分割閾值即分割差異大與小群聚之梯
度差異值。 
(4) 獲得邊緣點：最終若梯度間差異值(K-means 特徵點輸入法閾值) ≥分割閾值(Threshold)，     
則為差異大群聚(邊緣點)。 
 
 
圖 3.8、K-Means 特徵點搜尋初始群心 
 
    其中步驟(3)說明參考圖 3.9：假若最初 K-means 特徵點搜尋法之輸入值獲得最大值
Gmax 為 100，則初始閾值即為(100+0)/2=50。若下ㄧ筆輸入樣本小於分割閾值則歸為差異
小群聚。新的差異小群聚中心為目前差異小群聚中心值乘以小群聚前次累積數量加上新樣
本值再除以本次小群聚內樣本總數量。若新輸入值為 39，39 較閾值 50 為小，故屬差異小
群聚；由於原先在差異小群聚並未放入任何值，故新差異小群聚中心值為 39，新的差異小
群聚累積數量為 1，新的分割閾值為 69.5。以此類推，最後將獲差異大群聚與差異小群聚
分割閾值。 
由上可知，K-Means 特徵點搜尋法乃針對線段搜尋，獲取線段上梯度差異較大點(邊緣
點)，若針對整體影像而言，須多次運算於不同之線段方可獲取整體影像之特徵點，方法如
下： 
(1) 以影像間隔 10 像素取ㄧ垂直段為 K-Means 特徵點搜尋法輸入值(圖 3.10)。 
(2) 將每線段之輸入值分別代入K-Means特徵點搜尋法，其紅色部分即差異較大點(圖3.11)。 
(3) 找尋上下差異較大點後，根據此點上下各一像素分別做橫向搜尋(圖 3.12)，並以此三線
段運算出之最大閾值當作此三左右線段分割閾值，以獲取左右邊緣為研究特徵點(圖
3.13)。對於左影像所有上下梯度(|I(x,y)-I(x,y+1)|)與左右梯度(|I(x,y-I(x+1,y))|)差異較高
點，本研究中只採左右梯度差異較高點，當作特徵點。原因在於兩影像擷取器以平行架
設、平行同橫軸找尋特徵點對應，因此只對左右梯度差之特徵點存有搜尋優勢，此外當
兩擷取器上下仍有兩三像素差時，上下梯度差異高之點似乎難以比對。而先上下掃瞄之
 
圖3.12、橫向掃描 
 
 
圖3.13、最終特徵點 
 
3.5 特徵點對應 
因影像架設方式採雙影像擷取器傾斜平行擺放，故成像特徵點對應比對法則，將只考
慮同 Y 座標下。將左右影像同 Y 座標橫線梯度差陣列代入 K-means 特徵點搜尋法擷取特徵
點，並將特徵點由左至右編號：如左影像在 Y 座標上的特徵點分別編號為 L1、L2、L3…，
而右影像特徵點標號為 R1、R2…。 
為獲得對應關係本研究先計算 L1、L2、L3…R1、R2…等特徵點之特徵值，接著運算
其相似關係。特徵值的取法上：ㄧ般市售影像擷取器(視訊)因硬體製造變異與隨機變異較
大，擷取之影像間灰階差異相形嚴重，無法直接以灰階作為特徵值比對法則，故本研究利
用像素間灰階關聯性作為特徵值，其運算步驟如下(參考圖 3.14)： 
(1) 將左影像由左至右第 i 個特徵點，座標( LiX , LiY )以四周八方向 LiGLC 分別做九宮格旋積
運算(convolution)，個別獲得 X 梯度方向關聯性 LiVx 、Y 梯度方向關聯性 LiVy 。茲將計
算公式中之符號與變數定義說明於下： 
conv()：旋積運算函數 
   ),( YXI ：座標為 P(X,Y)之灰階梯度值 
   Vxconv：x 方向旋積運算遮罩，
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
−
−
−
=
101
101
101
Vxconv  
   Vyconv：y 方向旋積運算遮罩，
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
−−−
=
111
000
111
:Vyconv  
   LiGLC ：欲做旋積之九宮格中心值 
⎥⎥
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎢
⎣
⎡
=
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
−−−
−−−
−−−
)(tan)(tan)(tan
)(tan)(tan)(tan
)(tan)(tan)(tan
:),(
9
91
8
81
7
71
6
61
5
51
4
41
3
31
2
21
1
11
087
654
321
987
654
321
N
M
N
M
N
M
N
M
N
M
N
M
N
M
N
M
N
M
NNN
NNN
NNN
MMM
MMM
MMM
direct  
  
 
圖 3. 14、特徵值運算示意圖 
 
(3) 特徵值運算後，進而計算兩特徵點 Li 、 jR 之相似度 LiRjueSimilarVal ，相似度越高者即
越可能對應，其計算方式定義如式(23)所示。 
 
   LiRjueSimilarVal =MAX( LiRjueSimilarVal )- LiRjDifference        (23) 
其中 LiRjDifference =
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
×
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
×−
1
1
1
)
1
1
1
)(( TRjLi FvalueFvalue  
(4) 計算出所有左右影像特徵點 Li 、 jR 間之相似度後，下ㄧ步驟則是將此代入相似度矩陣
內(式 24)，並根據對應法則求出特徵點對應關係。 
 
圖 3.16、相似矩陣對應法左右對應性示意圖 
 
茲定義以下兩變數分別代表對應唯一性與左右對應性之限制，式(27)目的在計算於相似
矩陣內，若特徵點 Li 與 Rj 對應時，不符合唯一性值；而變數 Error2(式 28)表不符合左右對
應性之值。由於本研究乃利用相似性作為對應之特徵值，並採用矩陣方式加以計算處理，
故將此特徵對應演算法稱之為“相似矩陣相去法”。設迭代次數為 amount，相似矩陣相去法
之特徵對應流程示於圖 3.17。 
 Error1=
)__
__(
1
1
1
1
1
1
∑∑
∑∑
+=
−
=
+=
−
=
++
+
t
iq
LiRj
j
q
LiRq
s
ip
LpRj
i
p
LpRj
valuesimilarvaluesimilar
valuesimilarvaluesimilar
    (27) 
 
  Error2=(∑ ∑−
= +=
1
1 1
_
i
p
t
jq
LpRqvaluesimilar + ∑ ∑
+=
−
=
s
ip
j
q
LpRqvaluesimilar
1
1
1
_ )      (28) 
 
經迭代過程後，在相似矩陣內若相似值 LiRjvaluesimilar _ >0，代表左特徵點 Li 與右影
像特徵點 Rj 是為對應點。 
以往於特徵點對應問題所使用的演算法如遺傳演算法(Genetic Algorithm)、霍普菲爾-
坦克網路等，雖也可在短時間獲取可接受解，但多有跌入區域解、收斂迭代次數不ㄧ等問
題。對於車輛警示系統，除時間的考量外，更須考量每次執行反應效率應大致相同。而本
研究提出之方法能確保在有效迭代次數內即可收歛。 
 
 
 
 
 
3.6 物體距離計算 
影像經擷取、灰階、邊緣點搜尋、特徵值計算與特徵點對應後，接著將對應後的兩點
以相機成像原理，計算真實世界距離、高度等資訊，接下來需計算前方障礙物與駕駛之間
的距離及障礙物寬度，以便後續防撞路徑之計算。本研究將以三角函數關係陸續推導出車
道及障礙物距離等數值。 
由於本研究所使用之影像擷取器，並無標明焦距為何。故需進行實驗自行取得，焦距
推導的運算式如下式(29)~(33(參考自圖 3.18)：在以下焦距的運算式中，利用線段長度、推
算角度，進而利用三角關係計算焦距。之所以利用線段長度，理因角度的取得在硬體的先
天上較為不易，加上角度的稍許誤差，將導致長度計算錯誤的嚴重後果。 
 )(tan
1
1
1 BA
BCCBA −=∠               (29) 
 )(tan
1
1
1 EA
EDEDA −=∠               (30) 
 EDACBADCA o 111 90 ∠−∠−=∠             (31) 
 DCAFGA 11 ∠=∠                (32) 
 
)tan( 1
1 FGA
FGFA ∠=               (33) 
  其中 
       D：影像成像中心與鏡頭形成之直線在某深度之交點 
       1BA ：影像擷取器架設高度 
       BC ：擷取器最短擷取距離 
       1FA ：擷取器焦距 
 
 
圖 3.18、影像擷取器焦距計算示意圖 
 
若前方物體位於兩擷取器之右(如圖 3.20)，則： 
  
22
11 FJFAJA +=                (41) 
  
22
22 FLFALA +=                (42) 
  
HA
HAAA
LMA
JKA
2
221
2
1
)tan(
)tan( +=∠
∠
              (43) 
  
)tan()tan(
)tan(
21
221
2 LMAJKA
LMAAA
HA ∠−∠
∠×=             (44) 
  
)tan( 2
2
LMA
HA
IH ∠=                  (45) 
 
 
圖 3.20、影像擷取器成像示意圖二 
 
若前方物體位於兩擷取器之左(如圖 3.21)，則： 
 
IH
AAHA
LA
LMM)LAtan( 211
2
2
+==∠             (46) 
 
IH
HA
JA
KJK)JAtan( 1
1
1 ==∠                  (47) 
 
1
211
1
2
HA
AAHA
K)JAtan(
M)LAtan( +=∠
∠                 (48) 
 
K)JAtan(-M)LAtan(
K)JAtan(AAHA
12
121
1 ∠∠
∠×=             (49) 
 1BA ：影像擷取器架設高度 
 BC ：擷取器最短擷取距離 
 1FA ：擷取器焦距 
 IA1 ：地面至影像擷取器之歐基里德距離 
 
圖 3.22、影像擷取器成像示意圖四 
 
  )(tan
1
1
1 BA
BCCBA −=∠                         (58) 
  )(tan 111 ED
EA
DEA −=∠                         (59) 
  DEACBACDA O 111 90 ∠−∠−=∠                    (60) 
  )(tan
1
1
1 FA
JFFJA −=∠                         (61) 
  FJADIA 11 ∠=∠                          (62) 
  )tan( 1111 DIACDACBABABI ∠+∠+∠×=               (63) 
  
2
1
2
1 BABIIA +=                          (64) 
 
 
圖 3.24、影像擷取器成像示意圖六 
 
以上為本研究利用三角關係與成像原理，計算出影像擷取器焦距、影像擷取器至物體
真實距離、高度等推導過程，可提供障礙物判斷與外在環境判斷資訊量。 
 
4. 結果與討論 
本研究以 BorlandC++ 5.0 撰寫視窗程式，電腦設備為 MD Athlon 64 Processor 3200+、
1.99GHz,768MB，將由影像擷取器傳遞進來的影像加以處理，以驗證研究方法中所提之車
道線與障礙物偵測、特徵點選取與對應、以及單雙影像距離計算等方法之正確性。 
4.1 車道線偵測 
在車道線偵測的部份，本研究以台南至高雄所經南二高路段所抽取出單純高速公路之
圖樣共 31 件進行偵測(詳附錄 1)，車道線偵測平均運算時間為 0.0164 秒。圖 4.1、4.2 中 (a)
為高速公路影像圖，(b)為偵測出之車道線位置，在 31 個樣本影像中，車道線辨識成功者為
27 件，成功率為 87%。其中附錄 1 中編號 7、18 為搜尋誤判樣本，附錄 1 編號 7 搜尋錯誤
原因於二次搜尋綠色方格內並無任何道路線，而標號 18 則為兩道路線是分叉相連的。本研
究另於一般道路中，分別以右轉道路線 40 張影像、左轉道路 40 張影像、道路線有陰影 40
張影像、以及含其他符號之直線道路線樣本 40 張進行測試，其車道線偵測正確率分別為
97.5%、95%、95%、90%(附錄 2)。研究結果顯示所提車道線偵測方法可適應各種光線與環
境，亦可適用於車道線彎曲或斷線等偵測。圖 4.3 為行車偏移警示介面。 
 
4.2 邊緣偵測 
本研究邊緣偵測採多次多處 K-Means 特徵點搜尋法尋找邊緣，將灰階梯度差異為輸入
值並排除固定閾值設定，而為能依光線變化自動改變閾值(圖 4.4 紅色部分)。表 4.1 為灰階
至邊緣偵測於 320*240 圖像內處理時間，平均為 0.0154 秒。 
 
 
圖 4.4、邊緣偵測示意圖 
 
表 4.1、灰階至邊緣偵測運算時間結果表 
第一次測試 0.0150 (sec) 
第二次測試 0.0160 (sec) 
第三次測試 0.0160 (sec) 
第四次測試 0.0150 (sec) 
第五次測試 0.0150 (sec) 
平均運算時間 0.0154 (sec) 
 
    此外針對 K-Means 特徵點搜尋法取差異大群心與差異小群心中心值，亦或取差異大與
差異小群心相加皆無差異。不同之處僅在特徵點數量多寡將導致處理總時間不同，與後續
對應差異不大。(圖 4.5 為程式處理介面示意，上方為擷取影像下方為部份邊緣偵測結果)。 
 
 
圖 4.5、即時邊緣偵測研究測試撰寫介面 
 
    圖 4.6 為比對樣本示意，上方彩色點為所有特徵點，而下方黃色部份為已配對之對應
點，綠色部分則為目前配對之對應點。由樣本可知左右兩影像，角度亮度高度皆有所差異。 
 
 
圖 4.6、特徵點對應樣本 
 
4.5 影像焦距計算之可行性分析 
本研究中以研究方法中所提出之焦距計算之量測式，先計算出擷取器焦距，再利用單
影像對某深度路面計算距離的方式加以驗證其可行性。由於焦距在計算上，需利用三角函
數關係，在角度的獲得較為不易，故實驗採用線段推估角度。 
圖 4.7 為單影像驅動擷取介面，由圖中可看出測量距離皆位於同高度下之地面，原因
在於單影像測距必須事先獲知計算平面高度。由圖中可看到皮尺置於地面上，此乃為了直
接進行影像量測距離與實際距離之比較。 
 
 
圖 4.7、單影像驅動擷取介面 
 
表 4.6 則為此節測距的硬體設備，從表 4.7 各類樣本中，計算出平均測量誤差為 4.866015
公分。 
 
 
 
表 4.7、焦距驗證於單影像距離量測結果測試表 
真實距離(cm) 計算距離(cm) 
141 144.987411499023 
173 176.034225463867 
217 221.577392578125 
239 244.423721313477 
254 258.619354248047 
274 279.285949707031 
291 297.617553710938 
247 250.32585144043 
211 213.811309814453 
190 193.255340576172 
170 173.946914672852 
157 160.499237060547 
183 187.186798095703 
161 163.220718383789 
174 176.034225463867 
144 148.803558349609 
179 182.576171875 
204 209.387710571289 
232 236.945129394531 
250 256.498474121094 
227 231.608978271484 
200 203.744567871094 
218 223.19319152832 
270 276.838989257813 
277 284.303344726563 
263 269.733154296875 
290 297.617553710938 
303 294.862030029297 
261 267.439788818359 
245 250.32585144043 
 
根據表 4.7 實際距離與計算距離之誤差量可得圖 4.8，其中橫座標為實際量測距離，縱
座標為實距與測距誤差量，可發覺當深度越遠時，計算誤差越大。 
 
 
圖 4.10、焦距計算程式介面 
 
4.6 雙影像測距結果與分析 
利用上節所獲得之焦距，首先根據 3.7 節中所列公式計算單影像與路面斜面距離、雙
影像與物體斜面距離 IH 、再結合算出物體真實距離部份BP。本研究採雙影像擷取器平行
傾斜架設，在路面像素點較高、搜尋同軸之情況下，可增加精度與減少運算時間，不過對
於實驗而言，架設不能完全平行常為計算誤差來源。經測試 40 筆樣本(表)，影像大小
320*240(pixels)，測量範圍 71~150(cm)，平均誤差為 2.468465 公分，測量標準差
1.613617(cm)；測量範圍 151~360(cm)，平均測量誤差為 9.967459 公分，測量標準差 8.349569 
(cm)。 
圖 4.11 為樣本測量範圍 71~150(cm)真實距離與計算距離之結果。縱座標為測量距離，
而橫座標為測距由近至遠的編號，圖中差距皆不大。 
 
0
20
40
60
80
100
120
140
160
1 4 7 10 13 16 19 22 25 28 31 34 37 40
測量距離(cm)
實際距離
測量距離
 
圖 4.11、測量範圍 71~150(cm)真實距離與計算距離折線圖 
 
上述針對雙影像真實距離測量上，堪稱可用，接著以下將針對更遠距離(~3500cm)測試
0
500
1000
1500
2000
2500
3000
3500
4000
1 2 3 4 5 6 7 8 9 10 11 12
距離(cm)
實際距離
單影像測距
結合雙影像測距
 
圖 4. 13、較遠距離量測折線圖 
 
此外，本研究也針對橫向同距物體進行測試，針對高 83.5cm、橫向間距 70cm 物體進
行距離計算，得到表 4.9、圖 4.14 結果： 
 
表 4. 2、橫向測試表 
 
偏移距離
(cm) 
真實距離
(cm) 
單影像測距(cm) 結合雙影像測距
(cm) 
與單影像測距
差異(cm) 
結合雙影像測距
差異(cm) 
-280 1065 1002.931763 810.5609741 62.0682373 254.4390259 
-210 1065 1007.625244 900.9317627 57.37475586 164.0682373 
-140 1065 1007.625244 936.0707397 57.37475586 128.9292603 
0 1065 1072.880981 1164.354004 7.880981445 99.35400391 
70 1065 1072.880981 1093.779663 7.880981445 28.77966309 
140 1065 1067.563721 921.6531372 2.563720703 143.3468628 
210 1065 1105.927246 810.4855347 40.92724609 254.5144653 
280 1065 1117.397949 785.925354 52.39794922 279.074646 
 
圖 4. 16、障礙物偵測警示介面介面 
 
五、結論 
本計劃發展出一以機器視覺為基礎之汽車駕駛輔助系統，藉由道路線及障礙物偵測等
功能，以輔助汽車駕駛達到防撞功能，提高行車安全性。經由分析，本研究所獲得重要發
現與結果說明如下。 
(1) 驅動擷取器之架設：在影像擷取器架設上，本研究採平行傾斜擺放，可用於增加精度與
運算速度。原因在於傾斜擺放於路面所佔像素點數較多，而平行擺放可促使同 y 軸與鏡
頭所構成的面是相同的。此類方法有別於以往在雙影像測距上無考慮傾斜之情況。 
(2) 影像灰階至特徵點搜尋：在 320*240 的影像上，本研究所提 K-means 特徵點搜尋法可
於 0.0154 杪蒐尋出影像特徵點。此步驟主要是建立一演算方式，可根據外在光源的變
化改變特徵點閾值大小。 
(3) 特徵點對應：以往在特徵點對應文獻，常使用如遺傳演算法或是霍普斐爾-坦克網路。
其中遺傳演算法雖能在較短時間獲得較佳值，不過演算迭代收歛次數，卻是非常不固定
的，對於行車駕駛使用上，在計算整體運算時間有其不確定性。而霍普斐爾-坦克網路
雖較演算法在運算時間上更為精簡，不過其四項參數的設定，常嚴重影響後續判斷準
則。而本研究所提相似矩陣消去法演算法則，有運算簡易、迭代次數固定的優點。 
(4) 距離計算：實驗採雙影像結合單影像測距的方式計算真實距離。首先本研究提出一計算
焦距的方式，繼而利用此焦距計算前方物體距離。研究發現若距離越遠，由於能代表之
像數點越少，則所推估之距離將越無法精準。除針對不同直向距離測試獲淂雙影像所計
算方式較單影像於某深度下差異較大此結果外，本實驗更針對橫向測試，發現相機形變
from stereo pairs of linear images, Pattern Recognition Letters, 17(4), 387-398. 
Ruichek, Y. and Postaire, J. G. (1999), A new neural real-time implementation for obstacle using 
linear stereo vision, Real-Time Imaging, 5(2), 141-153. 
Se, S. and Brady, M. (2002), Ground plane estimation, error analysis and applications, Robotics 
and Autonomous Systems, 39(2), 59-71. 
Stiller, C., Hipp, J., Rossig, C. and Ewald, A. (2000), Multisensor obstacle detection and tracking, 
Image and Vision Computing, 18(5), 389-396. 
余佳任(2004)，視覺為主的智慧型自動導航車輛系統的設計與實踐，國立東華大學電機工
程學研究所未出版碩士論文。 
吳成柯、程湘君、戴善榮、雲立實(1993)，數位影像處理，台北：儒林圖書有限公司。 
陳柏安(2003)，視覺作自走車之障礙物定位與環境掃描，國立成功大學工程科學研究所未
出版碩士論文。 
曾俊元(2003)，以視覺感知的智慧型車輛防撞系統之研究，國立東華大學電機工程研究所
未出版碩士論文。 
劉士誠(2003)，以搜尋為基礎的道路線及障礙物偵測，國立中央大學資訊工程研究所未出
版碩士論文。 
劉正一(2000)，以電腦視覺為基礎之汽車行駛障礙物偵測及辨識，國立臺灣大學資訊工程
學研究所未出版碩士論文。 
蕭賢德(2000)，電腦視覺系統應用於自走車系統障礙物檢測之研究，國立成功大學工程科
學研究所未出版碩士論文。 
謝衛中(2002)，以電腦視覺為基礎之道路行駛障礙物警告系統，國立臺灣大學資訊工程學
研究所未出版碩士論文。 
鍾國亮(2002)，影像處理與機器視覺(初版)，台北：台灣東華書局股份有限公司。 
4 
 
5 
 
6 
 
10 
 
11 
 
12 
 
16 
 
17 
 
18 
 
22 
 
23 
 
24 
 
28 
 
29 
 
30 
 
附錄 2、一般公路車道線偵測範例 
右轉偵測:正確率 97.5%  (40 張樣本) 
  
  
  
  
  
 
 
影子偵測:正確率95%  (40張樣本) 

  
 
 
 
 
 
 
出席國際學術會議心得報告 
                                                             
計畫編號  NSC 95-2221-E-214 -057 -MY2 
計畫名稱  以電腦視覺為基礎之智慧型汽車駕駛輔助系統之研發 
出國人員姓名 
服務機關及職稱 
 江育民 
 義守大學工業工程與管理學系 助理教授 
會議時間地點  2007/10/20 ~ 2007/10/23, Alexandra, Egypt 
會議名稱  The 37th International Conference on Computer & Industrial Engineering 
發表論文題目  Vision-based Automatic Guidance of Road Vehicles 
 
一、參加會議經過 
由Computer & Industrial Engineering 國際期刊所主辦之第三十七屆電腦與工業工程國際
會議(The 37th International Conference on Computer & Industrial Engineering) 於 2007 年十月二
十日至二十三日在埃及亞歷山卓(Alexandra)舉行。此次會議共有全球超過 32 個國家之工業工
程與資訊工程領域之學者與業界人士參與，投稿之摘要共 550 篇，最後發表及收錄約三百篇
論文。會議地點為 Hotel Palestine Hotel，會議主題為 Competitive Industrial Engineering 
Education and Research，發表之論文涵蓋了在工業工程、軟體及電腦應用等方面的新方法、
理論與技術，在議程方面除了傳統工業工程領域如製造技術、存貨管理、作業研究、人因工
程外，也包含了如資料探勘、全球工程與經濟等議題，議程場次共四十五場，論文發表時間
由十月二十一日至二十三日共三天，另有論文海報展示及座談會。此次會議規模龐大，提供
全球工業工程與資訊工程從事基本理論研究之學者專家齊聚一堂互相交換研究與實務應用心
得之機會。 
十月二十一日會議在早上九點開始進行，大會安排的專題演講主講人為美國紐澤西州立
大學工業與系統工程系 Elsayed A. Elsayed 教授，講題為“Quality and Reliability Engineering in 
the Global Economy”，主要說明在全球化高度競爭的情況下，企業面臨顧客在品質與可靠度
的高度要求與期待下的因應之道。Elsayed 認為應以產品生命週期成本考量及系統化設計與維
修策略來增進系統可靠度，並以案例說明之。在短暫的休息後，隨即開始進行論文發表，每
個場次時間為一個半小時，約安排六~七篇論文發表，同一時段有四~五個場次同時進行。 
筆者的發表場次(Human Factors, Ergonomics and Safety (1))在下午二點開始。筆者發表的
                                                                                      
二、與會心得 
1. 資料探勘、知識發現及電腦智慧等議題益受重視，相關研究仍有很大發揮空間。 
2. 此次會議規模相當大，包含了諸多工業工程方面最新的研究主題，可藉以了解全球工
業工程產學研相關研究概況。藉由與與會學者討論切磋，令筆者得到相當啟發與肯定。 
3. 此次與會之國內學者相當多，投稿論文數也極可觀，相信與會人士對台灣學者之積極
性與國際觀留下深刻印象。 
 
三、建議 
1. 本次學術會議中有學者提及師資環境的改良方式，如教授可選擇將授課集中於同一
學期，另一學期則可專心於研究。在台灣多數私立大專院校的教師，常有教學與研
究無法完全兼顧的情況。筆者建議教育部未來也許可修正法規，在教師授課方面給
予較大彈性，以利教學品質提升及研究能量強化。 
2. 軟性運算(soft computing)在工業工程領域中，扮演相當重要的角色，可應用於排程、
決策支援、製造等種種領域，各學校宜對研究生安排相關課程之講授。 
3. 由於網路及資訊技術的快速發展，知識管理的議題則備受重視。相信這也是政府及
各大企業亟欲發展的實務課題。學界應可多朝此方面進行研究以與之配合。 
 
 In recent years, substantial research has been carried 
out for developing such driver support systems to 
enhance safety by reducing traffic accidents. These 
systems use a sensor suite mounted in various 
positions of the vehicle. The most common sensors 
are active sensors such as lasers, lidar, or radars (Sun 
et al., 2004). The main advantage is that certain 
quantities (e.g. distance) can be measured directly 
requiring limited computing resources. However, 
active sensors have several drawbacks such as low 
spatial resolution, slow scanning speed, and high cost. 
On the other hand, optical sensors, such as CCD 
cameras, are usually referred to passive sensors. With 
the introduction of inexpensive cameras, we can have 
a wide field of view. Such visual information is very 
important in a number of related applications, such as 
traffic sign recognition (Piccioli et al., 1996; Escalera 
et al., 2002), lane detection (Kluge and Thorpe, 1995; 
Broggi et al., 2000; McCall and Trivedi, 2006), or 
obstacles detection (Gandhi et al., 2000; Stiller et al., 
2000; Labayrade et al., 2005).  
In the past, binocular (or stereo) vision is often used 
for obstacles detection (Ruichek and Postaire, 1996; 
1999). It imitates the depth extraction ability of the 
human visual system with the use of two cameras. 
Stereo vision has the advantage of obtaining a 
detailed three-dimensional (3-D) representation of 
the environment around a vehicle at a relatively low 
sensor cost. The main difficulty is the so-called 
“correspondence problem” which consists in 
matching features extracted from two images that are 
projections of the same object in the 3-D world. The 
depth information can then be perceived depending 
on the disparity between the two images. This 
problem has been previously solved using a 
correlation technique (Burie et al., 1995). Due to the 
high complexity of the correspondence problem, 
many heuristic methods such as Simulated Annealing 
(Starink and Backer ,1995; Pajares, de la Cruz, 2004), 
Hopfield Neural Network (Ruichek and Postaire, 
1996; 1999; Tien and Chang, 1999), and Genetic 
Algorithm (Dipanda et al., 2003; Tien and Tsia, 
2004), have been developed.  
Optical sensors have been widely applied to vehicle 
systems to increase driver’s safety. Nevertheless, 
developing vision-based vehicle support systems is 
quite challenging due to variant illumination, 
complex outdoor environments, and unpredictable 
interactions between traffic participants, and messy 
background. The research presented in this paper is 
about an active safety system based on passive 
optical sensors. We develop a vision-based automatic 
guided system of road vehicles in order to enhance 
the driver’s safety. Main function of the system is to 
find the collision avoidance path by obstacle 
detection and lane tracking. The reminder of this 
paper is organized as follows. Section 2 delineates 
the vision system set-up and the processes of the 
proposed vehicle guidance system; Section 3 details 
the image processing techniques; Section 4 describes 
the 3-D reconstruction algorithm from linear stereo 
vision; Section 5 illustrates how to quickly detect and 
track the lane. The experimental results are shown in 
Section 6. Finally we draw conclusions about our 
research in Section 7. 
2. VISION SYSTEM OVERVIEW 
The purpose of the research is to find a collision-free 
vehicle guiding path to assist drivers keeping safety 
based on a stereo vision system. Two linear colored 
cameras are mounted in front of a vehicle for 
periodically acquiring stereo pairs of linear images as 
the vehicle moves. Their optical axes are parallel and 
at a distance B from each other, as shown in Figure 1. 
The height of the cameras above the ground is h. 
This configuration ensures that objects that lie on the 
road in front of the vehicle will be seen by two 
cameras. The major tasks of the system include lane 
tracking and obstacles detection. A framework for 
the vehicle guiding process is presented in Figure 2. 
Our current implementation of this framework is as 
follows: first, a pair of linear stereo images is 
grabbed. To decrease the image processing time, the 
grabbed color images are transferred into gray 
images. The candidate features for stereo matching 
are then detected and matched in two images. The 3-
D position of an object in front of the vehicle is 
computed by means of triangulation. According to 
the height information of the object, we can separate 
obstacles from road surface. Followed by obstacle 
width calculation and lane tracking, a collision-free 
vehicle guiding path is finally obtained. Details of 
the process are illustrated in the following sections. 
 
Proceedings of the 37th International Conference on Computers and Industrial Engineering, 
October 20-23, 2007, Alexandria, Egypt, edited by M. H. Elwany, A. B. Eltawil
570
 The pixels in the cluster with steep gradient are 
viewed as the vertical edge points. Once the 
threshold for the gradient has determined, we scan 
the vertical edge points along horizontal direction. If 
the gradient of a pixel in horizontal direction is larger 
than GT, it will be viewed as the edge point. To 
reduce the number of feature points, in each edge 
area, we scan from left to right horizontally and 
choose the first edge point as the feature point. This 
process is demonstrated in Figure 3.    
 
4. 3-D RECONSTRUCTION 
This section deals with the stereovision algorithm. 
We will present a new matching algorithm based on 
the similarity matrix of the two images. The 
similarity between two feature points is first defined 
in Section 4.1. After the corresponding points are 
matched, we introduce the camera geometry to 
recover the 3-D information of an object in front of 
the vehicle in Section 4.3. 
4.1. Definition of similarity 
A difficult factor for stereo matching is that the 
intensities of corresponding pixels from the stereo 
images may be different. When raw images are used, 
it is necessary to use an illumination invariant 
similarity measure. A popular approach is the 
convolution of the input images with a Laplacian 
kernel (Gonzalez and Woods, 1992). The paper 
utilizes convolutional filtering to extract the features 
of a feature point for similarity measure. Denote the 
extracted set of features as E. Define the direction of 
a feature point P(x,y) as dir(x,y), and the 
corresponding features set is denoted by E(x, y).  
  1( , ) tan ( )
y
x y
x
G
dir
G
−=           (3) 
where Gx is the gradient in the x direction and Gy is 
the gradient in the y direction. 
For a feature point P(x,y), there are nine elements in 
E(x,y). The nine extracted features are shown in Figure 
4, and E(x, y) = { dir(x-3, y-3), dir(x, y-3), dir(x+3, y-3), dir(x-3, 
y), dir(x, y), dir(x+3, y), dir(x-3, y+3), dir(x, y+3), dir(x+3, y+3) }.    
The value of similarity is defined by the difference 
between the features in two images. Let 
),( yx LL mean the coordinates of a feature point in 
the left image, and ),( yx RR  be the coordinates of a 
feature point in the right image. The measure of 
dissimilarity between the two feature points is 
defined as the following equation.  
( , ) ( , ) ( 3, 3) ( 3, 3)
( , 3) ( , 3) ( 3, 3) ( 3, 3)
( 3, ) ( 3, ) ( , ) ( , )
( 3, ) ( 3, ) ( 3,
Lx Ly Rx Ry Lx Ly Rx Ry
Lx Ly Rx Ry Lx Ly Rx Ry
Lx Ly Rx Ry Lx Ly Rx Ry
Lx Ly Rx Ry Lx Ly
dissimilarity dir dir
dir dir dir dir
dir dir dir dir
dir dir dir
− − − −
− − + − + −
− −
+ + − +
= −
+ − + −
+ − + −
+ − + 3) ( 3, 3)
( , 3) ( , 3) ( 3, 3) ( 3, 3)
Rx Ry
Lx Ly Rx Ry Lx Ly Rx Ry
dir
dir dir dir dir
− +
+ + + + + +
−
+ − + −
  
          (4) 
Let the maximum value of dissimilarity of all feature 
points be M, the similarity measure of the two feature 
points is then defined by Eq. (5). 
( , ) ( , )
( , ) ( , )
Lx Ly Rx Ry
Lx Ly Rx Ry
similarity
M dissimilarity= −            (5) 
 
   
Figure 3 The pixels with red color in the lower-right 
corner are the detected feature points 
Figure 4 Extracted features 
Proceedings of the 37th International Conference on Computers and Industrial Engineering, 
October 20-23, 2007, Alexandria, Egypt, edited by M. H. Elwany, A. B. Eltawil
572
 θ= the angle between the optical axis of the cameras 
and the horizontal 
α= the angle between line OQ and the vertical 
β= the angle between the optical axis and the line 
OQ 
Using the triangulation, the focal length of the lens 
can be found through Eq. (8) to Eq. (11). 
1
mintan ( / )Z hα −=                  (8) 
1
maxtan ( / )Z hθ −=                 (9) 
/ 2β π α θ= − −              (10) 
max / tan( )f IY β=              (11) 
 
The distance between a point P on an object and the 
cameras can be calculated by means of triangulation 
as well after the focal length is obtained. Figure 7 
and Figure 8 indicate different geometric 
relationships between the point P and the two 
cameras. Further parameters shown on Figure 7 and 
Figure 8 are: 
OL and OR: represent the position of the left and the 
right cameras, respectively, 
fL and fR: represent the focal length of the left and the 
right cameras,  
P⊥ = the perpendicular point of P projected on line 
OLOR 
Bi = the distance between P⊥ and Oi, { , }i L R∈  
θi = the angle of PP⊥ and POi, { , }i L R∈  
Di = the distance between the projection of the point 
P and the center of the image on the image 
coordinate system, { , }i L R∈  
DP = the distance between P and P⊥.  
 
For the configuration shown in Figure 7, the distance 
DP can be calculated by the following equations: 
tan( ) / /L L L L pD f B Dθ = =               (12) 
tan( ) / / ( ) /R R R R P L PD f B D B B Dθ = = = −        (13) 
tan( ) /(tan( ) tan( ))L L L RB β θ θ θ= ⋅ +            (14) 
/ tan( ) /P L L L L LD B B f Dθ= = ⋅            (15) 
Note if fL equals to fR, Eq. (14) will simplify to  
 /( )L L RB B D D= +           (16) 
 
For the configuration shown in Figure 8, the distance 
DP can be calculated by the following equations: 
tan( ) / ( ) /L L L R pD f B B Dθ = = +                (17) 
tan( ) / /R R R R PD f B Dθ = =            (18) 
tan( ) /(tan( ) tan( ))R R L RB β θ θ θ= ⋅ −            (19) 
/ tan( ) /P R R R R RD B B f Dθ= = ⋅           (20) 
Note if fL and fR is equal, Eq. (19) will simplify to  
/( )R R L RB B D D D= ⋅ −           (21) 
 
Figure 6 Triangulation for focal length calculation 
    
Figure 7 Stereo sensor modeling (Configuration 1) 
Proceedings of the 37th International Conference on Computers and Industrial Engineering, 
October 20-23, 2007, Alexandria, Egypt, edited by M. H. Elwany, A. B. Eltawil
574
 6. EXPERIMENTAL RESULTS 
 The sensing device described in Section 2 has been 
built with two 320x240 CMOS cameras delivering 
256 values in each of the red, green, and blue channel. 
Their parallel optical axes are separated by a distance 
B = 0.75 m. We implemented the proposed 
algorithms using Borland C++ language and a 
personal computer with Intel Pentium 4 CPU and 512 
MB RAM. This stereo set-up has been first tested in 
a laboratory environment for quantitative 
performance evaluation. Then it has been mounted 
near the front window of a vehicle for field test in 
real traffic conditions. Figure 12 shows the system 
interface. 
6.1. Quantitative performance 
evaluation 
Once the feature points are matched, it is easy to 
compute their 3-D positions. In order to evaluate the 
performance of the proposed edge detection and the 
matching algorithm, we have made some trials in 
indoor conditions. Figure 13 shows the result of edge 
detection in an indoor environment. The true 
positions of the target with respect to the cameras has 
been measured by a measuring tape, and compared 
with the positions computed by the proposed 
procedure. We have totally taken 30 trials with a 
target on different distance in front of the set-up 
vision system. The range of distance is from 140 cm 
to 350 cm. The results are shown in Figure 14. The 
average error of depth is about 2.47 cm and the 
corresponding standard deviation is 1.61 cm.   
 
 
Figure 11 The scanning direction for searching lane 
markings 
        
Figure 12 The interface of the proposed system 
    
Figure 13 The result of edge detection in an indoor 
environment 
Proceedings of the 37th International Conference on Computers and Industrial Engineering, 
October 20-23, 2007, Alexandria, Egypt, edited by M. H. Elwany, A. B. Eltawil
576
