 中文摘要 
 
 有別於一般基於單張影像分析的方法，本計畫所發展出的人臉表情辨識演算法係以處
理影像序列為著眼點。不論是在影像特徵的擷取，分類器的學習與建構，都是建立在分析
影像序列的基礎上。在這個前提下，我們的研究除了探討適用於人臉表情辨識之影像特徵
的基本問題外；同時也深入分析基於 Facial Action Coding System 所建構出的活動單元
（action units），進而推導出一個新的隱藏式條件隨機場（hidden conditional random fields）
的圖形模型（graphical model）來解釋人臉表情辨識的問題。此外，由於人臉表情辨識的電
腦視覺應用大都是即時的，我們亦提出一個適用於處理線上串流影像（online image stream）
的演算法；實作一個即時人臉表情辨識的測試平台，並比較其與基於分析單張影像的現有
人臉表情辨識方法之優劣處。 
 
 
關鍵詞：電腦視覺、人臉表情辨識、圖形模型、條件隨機場 
 
 
 
 
Abstract 
 
The main result of this one-year NSC project is to introduce a novel graphical model approach to 
seamlessly coupling and simultaneously analyzing facial emotions and the action units. Our 
method is based on the hidden conditional random fields (HCRFs) where we link the output class 
label to the underlying emotion of a facial expression sequence, and connect the hidden variables 
to the image frame-wise action units. As HCRFs are formulated with only the clique constraints, 
their labeling for hidden variables often lacks a coherent and meaningful configuration. We 
resolve this matter by introducing a partially-observed HCRF model, and establish an efficient 
scheme via Bethe energy approximation to overcome the resulting difficulties in training. For 
real-time applications, we also propose an on-line implementation to perform incremental 
inference with satisfactory accuracy. 
 
 
 
Keywords: Computer vision, facial expression recognition, graphical model, conditional 
random fields  
topics. In what follows, we briefly describe techniques that deal with facial activity, and then
discuss those focusing on predicting the emotion of a facial expression.
On analyzing facial activity and action units, Donato et al. [6] show that the Gabor wavelet
representation and the independent component analysis are useful for classifying action units.
They conclude with a surmise that combining motion and gray-level information may give the
best facial expression recognition performance. Kapoor et al. [10] have constructed a shape
model of the upper face, and used the model parameters as the inputs to SVMs for recognizing
action units within the upper face. Different from [10], Bartlett et al. [2] consider applying SVMs
to the Gabor wavelet coefficients of a face image. In [17], Tian et al. describe a neural network
approach that facial expressions are analyzed based on a set of permanent facial features (brows,
eyes, mouth) and transient facial features (furrows). For better inferring the action units, Tong
et al. [18] have utilized a dynamic Bayesian network (DBN) to model the spatial and temporal
relationships among the action units.
To classify facial expressions into a set of basic emotion classes, Pantic and Rothkrantz [14]
propose a rule-based system to code the emotions via action units. Zhang and Ji [24] instead
establish a DBN model to correlate the relationships between the emotions and the action units.
Alternatively, there are methods that are formulated to directly recognize the basic emotions
without drawing on action units. Cohen et al. [4] consider the tree-augmented-naive Bayes
(TAN) classifier to learn the dependencies between the facial emotions and the motion units. In
[2], Bartlett et al. report that in their experiments the best results on classifying facial expressions
into basic emotions are achieved by using SVMs with feature selection through AdaBoost [8].
Among the preceding techniques [2], [4], [14], [24] on linking facial expressions to emotions,
the emotion class label is image frame-wise predicted (despite that their formulation may use
temporal information). This may not be reasonable, as the making of a facial expression is a
transition over a sequence of image frames. For example, it would be unrealistic and difficult
to tell whether the facial expression associated with the third image in Figure 1 is joy even
by relating to the second image. Studies, e.g., [1], [3] from a psychology viewpoint have also
supported the sequence-wise analysis can achieve better recognition results.
Yang et al. [21], [22] and Zhao and Pietika¨inen [25] have described techniques to extract
features from a whole image sequence, and to sequence-wise classify the emotion. As the
sequence lengths of a facial expression are generally different, the so-called dynamic features
4
attach the coordinates of an interest point to the Gabor response. Altogether, each interest point
is described by a descriptor vector of dimension 74 (= 8× 9 + 2).
B. Image feature vector
Let {Ji}Mi=1 be the set of (pairwise) labeled images, Ni be the number of interest points
detected from Ji, and N =
∑M
i=1Ni be the total number of interest points. Since not all interest
points are discriminative, and in practice N is too large to work with, we consider AdaBoost for
interest point selection, and build our image representation based on the selected, informative
ones. We carry out the interest point selection in seven different runs in that the training images
are labeled from seven emotion categories. As the procedure in each run is exactly the same, it
suffices to explain how it is done for a particular emotion category.
We begin by dividing the training images into positive and negative ones, and define a “distance
function” d to measure the irrelevance of interest point k to image Ji by
d(k, Ji) = min
ℓ∈{1,...,Ni}
‖gk − gℓi‖, (1)
where gk and gℓi are the interest-point descriptor vectors.
At iteration t of running the AdaBoost algorithm, our criterion for choosing a good interest
point is as follows. Observe that, through (1), an interest point k gives rise to two distributions:
one for the positive images of {Ji} and the other for the negative ones. Treating each interest
point as a weak classifier would produce a weighted misclassification error ǫt(k). It follows that
the selected interest point at iteration t satisfies
k∗t = argmin
k
ǫt(k). (2)
Suppose that upon the completion of AdaBoost, K interest points have be chosen. (K = 60
in all our experiments.) Then, repeatedly performing the interest point selection by setting the
target class to each of the emotion categories in turn would yield a set of n = 7 ×K interest
points, denoted as {k∗1, . . . , k∗n}. To construct a feature vector x (of dimension n) for an arbitrary
image I from its bag-of-features representation, we first detect the interest points of I , and then
define the ith component of x by
xi = d(k
∗
i , I). (3)
6
respectively observed or unknown during training. Analogous to [16], the conditional probability
of class label y and hidden variables h is given by
p(y,ho,hu | s; θ) =
exp {−E(y,ho,hu, s; θ)}∑
y′,h′o,h
′
u
exp {−E(y′,h′o,h
′
u, s; θ)}
(4)
where θ includes the parameters of the probabilistic model, and E is the energy function. It
implies that
p(y,ho | s; θ) =
∑
hu
p(y,ho,hu | s; θ). (5)
And the data log-likelihood of D = {(s(i), y(i),h(i)o )}, with partially-observed hidden states, is
given by
L(θ) =
∑
i
log p(y(i),h(i)o | s
(i); θ)−
‖θ‖2
2σ2
(6)
where we have assumed a zero-mean Gaussian prior on θ. Learning a partially-observed HCRF
model can now be accomplished by solving
θˆ = argmax
θ
L(θ). (7)
In our implementation, scaled conjugate gradient [13] is used to find θˆ in (7). We emphasize that
the partially-observed information about the hidden variables is provided only in training. Hence
probability inference with the proposed model is exactly the same as with a regular HCRF. That
is, given a new test sequence s, we have
p(y | s; θ) =
∑
h
p(y,h | s; θ). (8)
B. Approximation with Bethe free energy
By far it may appear that adding partially-observed hidden variables to the learning of HCRFs
is straightforward. The definition of L(θ) in (6) indicates that solving (7) requires evaluating
p(y,ho | s; θ) for each training sequence. When applying the belief propagation algorithm, the
joint probability whose variables belong to a clique can be directly approximated by the corre-
sponding belief. In our case, the variables y and ho most likely do not form a clique. Evaluating
(5) requires to explicitly compute
∑
hu
exp {−E(y,ho,hu, s; θ)} (9)
8
among the state and the observation nodes, three kinds of feature functions are considered in
the definition of energy function E:
E(y,ho,hu, s; θ) =
∑
j
φ(s, j) · θ1(hj) +
∑
j
θ2(y, hj)
+
∑
(j,k)∈E
θ3(y, hj, hk), (12)
where E is the set of edges linking the hidden nodes, and φ(s, j) represents the observation at
node j. In our formulation, we exploit the temporal information by setting φ(s, j) = [xTj−1,xTj −
xTj−1]
T
.
B. Real-time incremental inference
By incremental inference, we are to design a message-passing scheme such that when pro-
cessing an upcoming image frame It, all the previous inference results before time instant t will
be available for making the new inference. Motivated by this desirable property, we define the
message sent from hidden node ht−1 to ht as
mt(y, ht; θ) =
∑
h1:t−1
exp {−E(y, h1:t,x1:t; θ)} (13)
where h1:t and x1:t stand for h1, . . . , ht and x1, . . . ,xt, respectively. mt(y, ht; θ) can be further
rewritten as
∑
ht−1
exp {−E(y, ht−1:t,xt−1:t; θ)}mt−1(y, ht−1; θ) (14)
where according to (12)
E(y, ht−1:t,xt−1:t; θ) = φ(s, t) · θ1(ht) + θ2(y, ht)
+ θ3(y, ht, ht−1). (15)
From (14), we can derive an implementation for performing incremental inference. Namely,
making inference at time t is done by evaluating
p(y |x1:t; θ) =
∑
ht
mt(y, ht; θ)∑
y′,h′
t
mt(y′, h′t; θ)
. (16)
Indeed the foregoing scheme is not limited to processing on-line streaming data. Owing to the
chain structure (among hidden variables), when the proposed incremental inference is applied to a
whole image sequence s, the resulting probability p(y | s; θ) is the same as that by simultaneously
10
the difference between the values maxy,r;1≤r≤5 p(y |x1:t−r−1; θ) and maxy p(y |x1:t−1; θ). If the
difference is larger then 0.05, we say that a new expression starts and set q = 0.1.
V. EXPERIMENTAL RESULTS AND DISCUSSION
We test our method with three sets of experiments. The first is to compare the sequence-
wise classification outcomes derived by the partially-observed HCRF model (PO-HCRF for
abbreviation) and other implementations. The second is to investigate the effects of using an
extensive set of hidden labels to account for all action unit combinations in the training dataset.
And for the last, we demonstrate that satisfactory real-time recognition performances can be
achieved via the on-line incremental inference.
A. Dataset
The facial expression database used in our experiments is Cohn and Kanade’s DFAT-504
dataset [9]. It contains 486 sequences produced from 97 subjects. There are about 1 to 9 sequences
for each subject. In this dataset, the action unit information is provided only for the last frame
(peak image) of each sequence, and occasionally it may not be sufficient for identifying the
emotion category. We have labeled 392 sequences by referencing Table 2 in Zhang and Ji [24].
B. Sequence-wise classification
In this set of experiments, we are to demonstrate the efficiency of using PO-HCRFs to analyze
sequences of facial expressions. We begin by setting the total possible hidden labels/states to
14, and further consider two cases. For case one, the hidden labels range from 1 to 7 (1 will be
reserved for neutral), and each corresponds to some combination(s) of action units. And for
the other case, the meaningful labels are from 1 to 9. In both cases the remaining labels are not
explicitly defined so that it leaves some degree of freedom for the training process since certain
image frames are hard to be labeled. The purpose of adding two more hidden labels is to enable
our system to distinguish whether (i) a joy face is with mouth open (i.e., AU25), and (ii) a
fear face is with a “shock” (AU2: outer brow raiser or AU5: upper lid raiser). (See Table I
for details.)
Besides comparing the results derived by HCRFs and PO-HCRFs, we have implemented the
method of Bartlett et al. [2] for image sequences. It is done by applying their classifier to each
12
Bartlett et al. [2] Our feature HCRF PO-HCRF7 PO-HCRF9
84.69% 87.50% 86.48% 91.33% 92.86%
TABLE II
ACCURACY RATES FOR SEQUENCE-WISE RECOGNITION.
joy sadness surprise anger fear disgust
joy 98.0% 0.0% 0.0% 0.0% 2.0% 0.0%
sadness 0.0% 97.5% 0.0% 2.5% 0.0% 0.0%
surprise 0.0% 1.4% 98.6% 0.0% 0.0% 0.0%
anger 2.8% 22.2% 0.0% 69.4% 2.8% 2.8%
fear 7.0% 1.8% 1.8% 1.8% 87.7% 0.0%
disgust 2.2% 0.0% 0.0% 6.7% 2.2% 88.9%
TABLE III
CONFUSION MATRIX OF PO-HCRF9.
To verify this claim, we apply PO-HCRF to all the last half sequences, and obtain a slightly
better recognition rate, 93.11%. Two such examples are plotted in Figure 3, where the broken-
line graphs show the probabilities of different emotions for the given (last half) facial expression
sequences.
C. More on the labeling results
Interesting observations can be inferred from the foregoing experiments. First, a facial expres-
sion typically does not progress “linearly.” For example, in a joy sequence as in Figure 4a, it
would be unnatural to use interpolation based on the neutral and peak images to approximate
the middle ones. In this case, the lip corner pulls up first (AU12) and then the mouth opens
gradually (AU25). Our labeling results show a good match for such face actions. Second, a more
dramatic example is given in Figure 4b where hidden label 8 for fear appears in labeling a
joy sequence. This can be explained in terms of action units. In some facial expressions of
fear, the mouth corner is pulled up (AU12), as in Figures 4c and 4d. Furthermore, the basic
set of the action units for fear comprises AU1 (inner brow raiser), AU20 (lip stretcher) and
14
1 2 3
(a)
0 2 4 6 8 10 12 14 16 18 20 22
0
0.2
0.4
0.6
0.8
1
 
 
joy sadness surprise anger fear disgust
1 8 3
1 8 3
(b)
1 8
(c) (d)
Fig. 4. (a) A joy sequence. (b) The plot for the inference probabilities for the six emotion c1asses. Below the plot are the
hidden-node labeling results respectively derived by processing the whole sequence (top) and by incremental inference (bottom).
In both results, despite some of the images have been labeled as fear, the output emotion class label is still correct. (c) and
(d) Some examples of the fear faces with action unit AU12 (mouth corners pull up), which also could occur in a joy facial
expression.
E. Implementation for real-time applications
Five-fold cross-validation is adopted to estimate the on-line recognition accuracy by our real-
time implementation. At each fold, we first run PO-HCRF9 and PO-HCRF100 separately on
each training sequence, and frame-wise derive the inference probabilities for the six emotion
classes (like the plot in Figure 4b). We also compute the entropy and its first derivative at each
time instant. The six probabilities and the two entropy-related quantities form a feature vector
of dimension 8. Suppose now we want to learn a decision function for the emotion of joy via
the Perceptron algorithm. The positive/negative data are those image frames from the joy/non-
joy sequences that the inference probability for joy in their feature vector is the largest. The
remaining cases can be analogously learned.
For testing, we perform incremental inference by treating a test sequence as an image stream.
At each time instant, the decision function corresponding to the emotion class that currently has
the highest inference probability will be applied, and a decision on the emotion label will be made
16
REFERENCES
[1] Z. Ambadar, J. Schooler, and J. Cohn. Deciphering the enigmatic face: The importance of facial dynamics to interpreting
subtle facial expressions. Psychological Science, 2005.
[2] M. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and J. Movellan. Recognizing facial expression: Machine
learning and application to spontaneous behavior. In CVPR, pages II: 568–573, 2005.
[3] J. Bassili. Emotion recognition: The role of facial movement and the relative importance of upper and lower areas of the
face. Journal of Personality and Social Psychology, 37:2049–2058, 1979.
[4] I. Cohen, N. Sebe, A. Garg, L. Chen, and T. Huang. Facial expression recognition from video sequences: Temporal and
static modeling. CVIU, 91(1-2):160–187, July 2003.
[5] F. de la Torre, J. Campoy, Z. Ambadar, and J. Cohn. Temporal segmentation of facial behavior. In ICCV, pages 1–8, 2007.
[6] G. Donato, M. Bartlett, J. Hager, P. Ekman, and T. Sejnowski. Classifying facial actions. PAMI, 21(10):974–989, October
1999.
[7] P. Ekman and W. Friesen. The facial action coding system: A technique for the measurement of facial movement. In
Consulting Psychologists, 1978.
[8] Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. In Proc.
European Conf. Computational Learning Theory, pages 23–37, Barcelona, Spain, 1995.
[9] T. Kanade, J. Cohn, and Y. Tian. Comprehensive database for facial expression analysis. In AFGR, pages 46–53, 2000.
[10] A. Kapoor, Y. Qi, and R. Picard. Fully automatic upper facial action recognition. In International Workshop on Analysis
and Modeling of Faces and Gestures, pages 195–202, 2003.
[11] J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling
sequence data. In ICML, pages 282–289. Morgan Kaufmann, San Francisco, CA, 2001.
[12] D. Lowe. Object recognition from local scale-invariant features. In ICCV, pages 1150–1157, 1999.
[13] I. Nabney. Netlab: Algorithms for Pattern Recognition. Springer-Verlag London Ltd, 2004.
[14] M. Pantic and L. Rothkrantz. An expert system for multiple emotional classification of facial expressions. In Proc. IEEE
Conference of Tools with Artificial Intelligence, pages 113–120, 1999.
[15] M. Pantic and L. Rothkrantz. Automatic analysis of facial expressions: The state of the art. PAMI, 22(12):1424–1445,
December 2000.
[16] A. Quattoni, M. Collins, and T. Darrell. Conditional random fields for object recognition. In L. Saul, Y. Weiss, and
L. Bottou, editors, NIPS 17, pages 1097–1104. MIT Press, Cambridge, MA, 2005.
[17] Y. Tian, T. Kanade, and J. Cohn. Recognizing action units for facial expression analysis. PAMI, 23(2):97–115, February
2001.
[18] Y. Tong, W. Liao, and Q. Ji. Inferring facial action units with causal relations. In CVPR, pages II: 1623–1630, 2006.
[19] Y. Tong, W. Liao, and Q. Ji. Facial action unit recognition by exploiting their dynamic and semantic relationships. PAMI,
29(10):1683–1699, October 2007.
[20] S. Wang, A. Quattoni, L. Morency, D. Demirdjian, and T. Darrell. Hidden conditional random fields for gesture recognition.
In CVPR, pages II: 1521–1527, 2006.
[21] P. Yang, Q. Liu, X. Cui, and D. Metaxas. Facial expression recognition using encoded dynamic features. In CVPR, 2008.
[22] P. Yang, Q. Liu, and D. Metaxas. Boosting coded dynamic features for facial action units and facial expression recognition.
In CVPR, pages 1–6, 2007.
[23] J. Yedidia, W. Freeman, and Y. Weiss. Constructing free-energy approximations and generalized belief propagation
algorithms. IEEE Trans. on Information Theory, 51(7):2282–2312, 2005.
18
無研發成果推廣資料 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
