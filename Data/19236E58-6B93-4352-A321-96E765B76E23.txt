 第 2頁 / 共 49頁 
讀取流入封包之表頭(Header)內的服務類型(Type of 
Service, ToS)欄位，並將封包置於相對應的輸出佇列，
系統根據不同服務之頻寬需求，進行頻寬速率控制，
避免頻寬被單一服務佔據之情況產生，使所有服務皆
能達到服務品質之保證。 
關鍵詞：NetFPGA、網路虛擬化架構、網路服務品質
保證機制、RSVP協定 
Abstract 
Nowadays, the network data flow analysis is prone to 
adopt simulation or benchmark patterns to estimate its 
performance. Considering about applications complexity 
or diversify of network, the actual results of simulation 
would become doubtful to depict the real network 
condition. If the analysis can be conducted on the real 
network equipments, the results will be more precise. 
The analysis of system data flow will be capable for 
simulating the real data flow while testing the new 
algorithms is tested. For this reason, in this research 
NetFPGA hardware platform used to provide the ability 
for capturing real packet and in addition the OpenFlow 
of Stanford University is proposed to provide complete 
control over the network to establish the entire testing 
environment. This project designs and implements an 
adaptive QoS guarantee for network virtualized 
architecture, while it also presents centralized network 
virtualization architecture.  
The subproject (number 3) designs an adaptive QoS 
Engine to provide the network service quality for 
virtualized network architecture. The QoS Engine 
includes three planes: data plane, control plane and 
management plane. The Data Plane is mainly use the 
function of Traffic Monitor and Bandwidth Monitor for 
monitoring network status. The various packet service 
protocols are recorded in the Record Table by the 
Monitor Agent on the Management Plane, in order to 
provide parameters for QoS mechanism required 
information. And the Control Plane provides the decisive 
information through the Management Plane to perform 
the Admission Control, Traffic Control and Congestion 
Control to achieve optimal network performance. The 
Management Plane is defined by a complete 
management process that provides network manager a 
friendly interface to control network. This section defines 
four sub-modules, first is the Policy Pool providing the 
policy repository created by subproject 4. Second, a 
Web-based Management provides the network 
administrator a friendly interface. Third, adaptive QoS 
Strategy provides a strategy to adapt network condition. 
Fourth, a QoS Controller enables the Adaptive QoS 
Strategy and parsing the policy file in the Policy Pool. 
The design of this study architecture is based on the 
project in order to exchange each subproject mechanism. 
Due to the sensitiveness of QoS metric, implementing 
rate limiting QoS design on the real hardware is 
considered to be more accurate, compared to merely 
perform simulation analysis. However, the commercial 
network devices such as router and switch are concealed 
for possible mechanism extension of QoS design. 
Therefore in this study the QoS mechanism is 
implemented using OpenFlow System in NetFPGA 
platform. The modification of wildcard table module in 
OpenFlow user data path allows each packet to be 
budgeted with certain level of rate when it passes the 
output queues. After performing the test-bed 
implementation of service-based QoS mechanism, the 
system is proved to show isolated the traffic and limit the 
rate of traffic, ensure the desired bandwidth allocation. 
Keyword: NetFPGA, Network Virtualized Architecture, 
Quality of Service, Resource Reservation Protocol 
 
 第 4頁 / 共 49頁 
在此架構下，系統的模擬情境分為使用者端及管
理者端。圖 2.3 為使用者端的情境圖，透過 NetFPGA
建置的Router/Switch將他的網路環境建置在一般的網
路中，利用 Arbiter 根據每個 Router/Switch 收集的封
包資訊提供相對應的策略以動態調配目前的網路環境，
以管理每個使用者的網路服務品質。在建置的環境中，
透過乙太網路連線的使用者在使用各種應用服務時，
其系統提供的QoS策略能否根據網路狀況動態的調配
以避免網路服務品質降低的影響。 
 
圖 2.3 使用者端的情境圖 
圖 2.4 為管理者端的情境圖。網路管理者利用回
報得知網路狀況並動態提供相對應的 QoS 策略。本計
畫提供的 QoS 策略會以 XML形式儲存在 Policy Pool
中以因應網路管理者的需要提供各種不同的 QoS 
Policy。網路管理者可以透過 XCAP 的存取協定將每
個QoS Policy截取出來並提供相對應的QoS機制提供
給 QoS Scheduler 執行 Admission Control 以達到
Differentiated Service 的構想。 
 
圖 2.4 管理者端的情境圖 
在 Arbiter 中包含了兩種功能：執行網路 QoS 機
制的 Controller以及排程 QoS 機制的 Scheduler。當使
用者端的網路服務品質降低時，Arbiter 會根據每個
Router/Switch 收集到的封包資訊分析，利用 Scheduler
排程以提供相對應的策略動態調配目前的網路環境，
而 Controller會根據 Scheduler提供的QoS策略以執行
QoS 機制。 
 
 
三、 文獻探討 
本計畫依據所提出的 NetFPGA 網路環境以及達
成服務品質保證與資源管理的目標，分別針對
NetFPGA、OpenFlow、Quality of Service 以及封包辨
識技術進行文獻的蒐集與分析，以利計畫的研究與開
發‧本計畫藉由 NetFPGA硬體開發帄台開放性，達到
截取目前網路封包之功能。因 NetFPGA硬體開發帄台
亦具有高度彈性及線性之特性，可以快速建立網路硬
體模型於教學及研究用途之上 (如修改 Ethernet 
Switches 和 IP Routers)。  
 
3.1 NetFPGA 
 
3.1.1 NetFPGA 概要 
 NetFPGA硬體開發帄台包含三個元件：Hardware、
Gateware和 Software； Software和Gateware(如Verilog 
HDL 原始碼) 透過 Hardware 的幫助，可以實現一高
效能的網路系統；相關原始碼皆可在 NetFPGA 官方
網站上下載。本子計畫係將 IPv4 與網路介面卡根據
NetFPGA 之標準型態，將其建立於硬體開發帄台上，
以作為本計畫研究基礎與開發。 
 
Hardware 
圖 3.1為 NetFPGA(version 2.1)之帄台，此硬體係一
個 PCI卡且具有以下的核心元件[1-5]： 
 Field Programmable Gate Array (FPGA) Logic 
 Xilinx Virtex-II Pro 50 
 53,136 logic cells 
 4,176 Kbit block RAM 
 第 6頁 / 共 49頁 
網路交換機主要的封包處理模組；其模組原理係：首
先，Rx Queues 由 I/O port 取得封包，接著 Input Arbiter
會選擇要服務哪個 Rx Queue，然後 Output Port Lookup
會決定封包儲存在哪個 Output Queue，該模組會接收
封包並儲存資訊直到 Output Port 準備好，最後透過
Tx Queue 將封包由 I/O port送出。 
 
 
圖 3.4 Reference Pipeline[7] 
 
Software 
  包含 NetFPGA driver、CAD Tools、用來模擬的記
憶體模組(Memory Module)、路由器套件(Router Kit)，
Buffer Monitoring 和 NetFPGA 的路由器軟體元件
(SCONE)。在 NetFPGA中，透過這些軟體的使用，截
取實際的 Traffic Flow來模擬 QoS 機制。除此之外，
還有兩個 Router Controller Package 可以填充 IPv4 
Router 之 Forwarding Table。首先是基於 PW-OSPF[8] 
獨立路由軟體封包，在使用者端運作，第二是將 Linux
的 Routing Table 的從原本的 Memory映射到硬體中。
以上可透過標準 Open-Source 路由工具建立一快速的
Full Line-Rate 4Gbps 路由器。 
 
3.1.2 NetFPGA實作 
 本計畫已在網路環境下實現一 NetFPGA 開發帄
台，硬體和軟體之部分架構係參考 NetFPGA官方網頁
中 User Guide。首先安裝 NetFPGA硬體開發帄台至主
機上並確保可以正常運作，接著下載 NetFPGA 
Package(NFP)軟體，其包含 Gateware 原始碼、系統軟
體和回歸測詴，NFP軟提包含 IPv4 Router、4-port NIC、
IPv4 Router with Output Queues Monitoring System、與
IPv4 Router(SCONE)互動之 PW-OSPF 軟體、映射
Routing Table的 Router kit 以及 NetFPGA上從 Linux 
host 到 IPv4 Router 之 ARP Cache。 
 NetFPGA 官方網頁同時也提供範例程式供開發
者使用，包含 NetFPGA 包含軟體和硬體可以透過
Reference NIC 進行通訊、使用網路介面與路由器溝通
和設定 NetFPGA軟體元件(SCONE)參數、監控傳輸系
統緩衝區監控系統以及使用 Java編程介面完成Router 
kit。 
 圖 3.5 為 SCONE 之網頁介面，此程式建立與維
護一個包含靜態與動態路由器之Routing Table和ARP 
Table，當偵測到 Software Table 改變時，SCONE 會複
製此訊息並記錄到 NetFPGA的 Hardware Table 中。圖
3.6為偵測 Traffic 之 Java GUI 介面，此程式包含了由
Java GUI 介面和 Reference Pipeline 資訊所組成之
Router kit。此系統使用到 Java GUI 介面的 Router Kit
在實際的網路中做 Traffic 監控和測詴情形，藉以證明
本計畫所提出 QoS機制的可行性。 
 
 
圖 3.5 SCONE 的網路介面[1] 
 
 第 8頁 / 共 49頁 
表 3.1 A flow entry 
Matching 
Fields 
Counters Instructions 
 
  OpenFlow Switch.由 15 個參數位元所組成，如表
3.2所示。一個 TCP Flow可以藉由十五個欄位詳細說
明，然而一個 IP Flow在定義中，可能不包括傳輸埠。
每個 Matching Field 可以彙整各種 Flow。例如，在特
定 VLAN，Flows 只有在 VLAN ID 在有被定義的情況
下才適用於所有 Traffic 中。 
 
表 3.2 Matching Fields List 
 
 
Matching Field 中 15個位元參數如下所述： 
 交換器輸入 Port 
 Metadata 
 乙太網路來源位址 
 乙太網路目的位址 
 乙太網路型態 
 VLAN ID 
 VLAN 優先權 
 MPLS 標記 
 MPLS Traffic Class 
 IPv4 來源位址 
 IPv4 目的位址 
 IPv4 協定/ARP Opcode 
 IPv4 ToS  
 TCP/UDP/SCTP 來源 Port/ICMP 型態 
 TCP/UDP/SCTP 目的 Port/ICMP Code 
 
 Counter 中包含 Table、Flow、Port及佇列，並藉
由軟體運作方式實現於更多硬體計數器應用中。表 3.3
表示所需的計數器。 
 
表 3.3 計數器統計訊息需求表 
Counter Bits 
Per Table 
Active Entries 32 
Packet Lookups 64 
Packet Matches 64 
Per Flow 
Received Packets 64 
Received Bytes 64 
Duration (seconds) 32 
Duration (nanoseconds) 32 
Per Port 
Received Packets 64 
Transmitted Packets 64 
Received Bytes 64 
Transmitted Bytes 64 
Receive Drops 64 
Transmit Drops 64 
Receive Errors 64 
Transmit Errors 64 
Receive Frame 
Alignment Errors 
64 
Receive Overrun Errors 64 
Receive CRC Errors 64 
Collisions 64 
Per Queue 
Transmitted Packets 64 
Transmitted Bytes 64 
 第 10 頁 / 共 49頁 
ANYs，提供 TCAM-like 以符合 Flow之規範[15]。 
 
3.3 Tunneling─Capsulator 
Capsulator 提供運作在使用者端，基於軟體的
Tunneling 技術，要建立一個 Tunneling 伺服器必須具
有 Open vSwitch 與 Capsulator。 
3.3.1 Open vSwitch 
    Open vSwitch為一 Open Source 軟體，其為多層
虛擬的 Switch，且支援標準管理介面，例如，NetFlow, 
sFlow, RSPAN, ERSPAN 及 CLI 等管理介面。透過有
計畫的擴展，可促使大規模的網路自動化[16,17]。 
Open vSwitch 可作為一個 Soft Switch 運作在
Hypervisor 中，它已經被移植到多種虛擬化帄台及
Switching 晶片中，是 Xen 雲端帄台中的 Switch，也支
援 Xen、XenServer、KVM 及 VirtualBox，並具有易移
植至其他環境之特性，圖 3.9 為 Open vSwitch 架構圖
[18]。 
 
 
圖 3.9 Open vSwitch架構圖 
 
3.3.2 Capsulator 
    Capsulator 是一個 User-Space 的程式，用以連結
不同的網路環境，形成一區域網路。Capsulator 實際
上透過 MAC-in-IP tunnel 連結相同 Tag值的 Ports，表
示乙太網路封包由每個 Border Port 讀取，並傳送至所
有具相同 Tag值 Capsulator 的 Border Port，並利用乙
太網路 IP 封包的 Payload 與其他 Capsulator 溝通。 
    Capsulator 包含 Border Port與 Tunnel Port。Border 
Port 連結至其他具有相同 Tag 值 Capsulator 的 Border 
Port，使用者在啟動 Capsulator 時會指定 Border Port
的 Tag值，可為任一整數；Tunnel Port 則負責與其他
Capsulator 溝通。 
    透過 Capsulator 可以連結數個網路，如圖 3.10所
示，在每個網路中，eth0 作為 Tunnel port 與其他
Capsulator 溝通，同時連結到 Internet。 
    當 Border Port 接收到乙太網路封包後，會將其加
入至 IP封包中，然後傳送給其他 Capsulator；Tunneling 
Port(eth0)接收到 IP封包後，首先將 IP封包標頭移除，
檢查 Tag 值後再將乙太網路封包傳送給所有具相同
Tag 值之 Border Port，作業系統在每個終端重組各個
分段，任意大小(例如，大於 MTU大小的一半)的乙太
網路封包皆可被 Tunnel並與 Capsulator連結進行訊息
之傳遞。 
 
 
圖 3.10 Capsulator 運作圖 
 
圖 3.11為以 Capsulator連結數個 OpenFlow網路
的例子，每個 Capsulator 具有一個 Tunnel Port(eth2)與
兩個 Border Port(eth0 和 eth1)，eth0 為連結到 NOX 
Controller 的 Control Port，eth1 為連結到 OpenFlow 
Switch的 Data Port。 
NOX Controller 可控制網路中所有的 OpenFlow 
Switch，此外，網路 A與網路 B彷彿是直接透過兩個
連結到 eth1Port的 OpenFlow Switch 互相溝通[19]。 
 
 
圖 3.11 OpenFlow Network Example 
 第 12 頁 / 共 49頁 
表 3.5 IntServ 服務 
 
 
3.4.2 DiffServ 
受限於 IntServ 的限制， IETF 提出 DiffServ 
(Differentiated Services) 架構 [21]，圖 3.13 表示
DiffServ 架構。 
 
圖 3.13 DiffServ 架構 
 
在此架構中，使用者可以預先與 DiffServ 進行
SLA協定，DiffServ的網路資源是根據 SLA內容預先
分配，當封包進入 DiffServ 時，邊緣路由器(Router)
會執行 MF Classification、監控 Flow Rate、標示對應
的 DiffServ CodePoint(DSCP)，並根據封包類型採用適
當的 Per-Hop-Behavior (PHB) 。於網路內部中的路由
器\僅提供較簡易的 DSCP 和 PHB 對應機制如表 3.6
所示，基於封包標頭的 DSCP 並根據對應的優先權進
行轉移，於邊緣/邊界節點中保存 Traffic Flow資訊和
控制 Flow。 
 
表 3.6 DiffServ PHBs 
 
 
3.5 特徵識別(Feature Recognition) 
為了提供較佳的網路管理及增進網路傳輸的品質，
因此良好的封包特徵識別是當前重要之議題，目前較
為 常 見 的 特 徵 識 別 方 法 Signature-Based 、
Behavior-Based 和混合型封包特徵識別。 
 
3.5.1 Signature-based 封包特徵識別 
由於多數Web 中Payload的封包傳輸時間會被標
示特殊字串，因此，若能夠辨識網路應用的封包特徵，
便可以避免特定的網路Traffic以維持服務品質。例如，
eDonkey 封包 Payload 必須是 eDonkey 標識的開頭，
其值為 0xe3，4Byte 為封包內容的長度；KaZaA封包
Payload開頭為GET或HTTP，並包含X-KaZaA字串，
其他相似封包識別方式也如上所述[22-24]。 
相較於傳統 Port-based 辨識方法，有較高的正確
率，然而，由於它的標頭(Header)長度通常只有 20Bytes，
但在乙太網路 Payload 却高達 1500Bytes，造成辨識時
間過長及系統資源浪費，在[25]研究中指出，95%的第
一個 Payload 占 400 Bytes，因此調整最大 Byte 數與檢
查的長度相同，儘管會降低正確性，但卻可以提升整
體字串匹配效能；然經過加密處理Payload，如 Skype，
便無法辨識。 
3.5.2 Behavior-based 封包特徵識別 
常應用於 P2P 軟體，例如，Bit-torrent、eMule 或
Skype，為了避免使用傳統單一封包 Payload 方法辨識
P2P 封包或者封包資料於傳輸時被竊取，因此大部分
軟體本身提供加密傳輸，其加密方式可分為傳輸前加
密或者傳輸後加密兩種，使得傳統資訊過濾方式
(Signature-based Filter)於無法應用於 P2P 傳輸上。此
 第 14 頁 / 共 49頁 
式以辨識所接收之封包，若收接受之封包無法辨識則
採用第二種 Port-based 的方法來判定。若此兩種方法
皆無法辨識封包屬於何種應用。此混合型封包之設計
方式是以避免竊取其他 Web 應用而採取標準埠號來
傳送資料，但此種設計方是會減少封包處理速度。由
於 Signature-based 特徵識別會消耗較多時間且可能發
生因輸入暫存器空間已滿，進而造成封包遺失的情形
發生[25]。 
 
3.6 特徵識別軟體 
本計畫除了介紹特徵識別的概念，並闡述相關軟
體之功能及應用範圍。例如，有兩個 signature-based 
Traffic 識別軟體，分別是 IPP2P[29]和 Layer7[30]。
IPP2P及Layer7皆是建立在Netfilter框架提供的Linux
核心當中，將於後面章節中介紹其設計概念。 
 
3.6.1 Netfilter 
Netfilter 採用 Linux 2.4 作業系統[31,32]，此版本
提供封包過濾技術，提供封包過濾機制、封包位址轉
換(網路位址轉換，NAT)和封包損毀功能，圖 3.17 顯
示五個 Hook point 的 Netfilter 封包資料流，
NF_IP_PRE_ROUTING 、 NF_IP_LOCAL_IN 、
NF_IP_FORWARD 、 NF_IP_LOCAL_OUT 和
NF_IP_POST_ROUTING，此種設計方式避免使用者
修改作業系統核心時，避免對系統造成的破壞。 
 
 
圖 3.17五個 hook位置的 Netfilter封包資料流 
 
圖 3.18為 Netfilter 的架構，主要由兩個元件所組
成，第一個部份是在 Linux 核心中進行封包截取和處
理，而第二個部份利用軟體控制 IPtable 設定過濾規則，
由於 IPtable使用 Netlink Socket[35]與 Linux核心進行
連結，並將過濾規則透過 Netlink傳送到 Netfilter，作
為這些過濾封包標準原則的基礎，在封包穿過 Linux
核心的 IP 層時，檢查封包是否需要轉換，准許此封包
由 PRE_ROUTING 進入，並透過 POST_FORWARD
和 POST_ROUTING 進行連續性地傳送，如果封包是
由本土主機(local host)進入，則會藉由 INPUT 處理並
傳送到本土主機，透過 OUTPUT 和 POST_ROUTING
進行連續地傳送，由於 Netfilter 架構存在於 Linux 核
心架構，所以封包的內容不需透過 User Space 進行處
理，使封包過濾可以有較好的處理效能。 
 
 
圖 3.18  Netfilter架構 
 
3.6.2 IPP2P 
IPP2P 用來識別 P2P 封包，據官方網站所提供的
資訊，IPP2P 可支援九種不同的 P2P 軟體，如表 3.7
所示，為使 IPP2P 能夠符合每個 P2P 應用封包資訊，
因此將封包資訊寫入於各種獨立的功能當中；如 UDP 
Packets 寫入於BitTorrent 中，稱之為 udp_search_bit( )。 
因此，若出現新式 P2P 應用，只需透過軟體修改初始
的文件，即可匹配新增之功能。 
 
 
 
 第 16 頁 / 共 49頁 
由於現今許多的網路並無提供相關 MPLS 架構，
這對於網路頻寬管理是一大限制。此外，隨著晶片技
術不斷地發展，使得 Router-Transfer和 Switch-Transfer
之間效能的差異越來越小，MPLS 是藉由 DffiServ 完
成，當執行 MPLS 時，亦會對 DiffServ造成影響。 
 
3.7.2 MPLS-TE&QoS 
傳統的路由器運作方式，是採取最短路徑當作路
由(Routing)，不考慮其他因素。因此，當原本的路徑
壅塞時，傳統的路由器不會將 Traffic Flow改成藉由其
他路徑進行傳輸。例如，傳統路由器選擇從 R1(R8)
到 R5 最短的路徑是 R1(R8)-R2-R3-R4-R5，即使有其
他 替 代 的 路 徑 ， 如 圖 3.21 所 示 ，
R1(R8)-R2-R6-R7-R4-R5 路徑鮮少被傳統路由器所選
擇。 
 
 
圖 3.21傳統路由器(router)的難處 
 
Traffic Engineer (TE)為 Traffic Flow選擇路徑的
過程，TE 帄衡了網路中不同連結路徑、路由器、交換
器中的 Traffic Flow；其目的是取得兩個節點間的路由
器，使此路由(Routing)不違背它的規範。 
由於 MPLS 具 Self-Routing 和個別地傳送工作之
特性，MPLS可用來與 TE組合成MPLS-TE[33]技術，
使用 MPLS-TE 可以提升網路 QoS，主要原因為： 
1. 藉由帄衡多個傳送路徑的負載，使 MPLS-TE 可
以提升 QoS且避免網路壅塞情形發生。 
2. MPLS-TE可以透過RSVP-TE信號建立一個謹慎
的 QoS 及頻寬保證的通道。 
3. 透過備用的 LSP和 FRR(Fast Reroute)，不僅避免
通道擁塞也能相對提升 QoS。 
 
然而，MPLS-TE 也有許多限制，如下所述： 
1. MPLS-TE 必須用於 MPLS 網路，但現今許多網
路並不支援 MPLS 網路架構，造成無法支援
MPLS-TE 情形發生。 
2. 跨領域的 MPLS-TE 應用仍在發展當中，目前的
MPLS-TE 只能在特定領域中執行。 
3. 雖然 MPLS-TE 可以建立一個頻寬保證的通道，
如果使用者同時透過通道傳送多種 traffic，可能
會產生單獨地處理不同優先權的 Traffic 問題發
生。 
 
3.7.3 Ethane 
本計畫所使用的OpenFlow及NOX皆繼承 Ethane
之優點。Ethane[36]是一個新的企業網路架構，允許管
理者自行定義單一網路、並可直接運作執行；Ethane
具備直接連結中央 Controller 之路徑、Flow-based 的乙
太網路交換器以及管理 Entry和 Flow的路由(Routing)
之特性，提供開發者發展 OpenFlow的參考方向。 
OpenFlow 的設計概概念與 Ethane 大致相同，由
中央的 Controller 進行網路控管並決定所有封包的運
作情形。因 Controller 可藉由目前網路拓撲情形並提
供合法 flow 進行存取及路由計算。第二個元件為
Switches，具備簡易及高相容之特性，由單一的 Flow 
Table 及 Controller 的安全通道所構成，Switches 依據
Controller 的指示傳送封包，此方式與 OpenFlow概念
相同，若接收到不在 Flow Table 中的封包時，會將此
封包傳回給 Controller 進行處理；若接受在 Flow Table
中的封包時，則會根據 Controller 的指令進行相對應
之動作。 
Ethane 網路對於 Controller 與 Switches 之間互動
有完整的定義，圖 3.22說明在 Ethane 網路上通訊的五
種行為。 
 
 第 18 頁 / 共 49頁 
圖 3.24 表示 Controller 之元件，Controller 為網
路控制之核心，存在著許多嚴格的操作規範。一般網
路存取方式，是透過認證元件辨識未認證或不在 MAC
內Traffic並核對儲存在註冊資料庫裡使用者與主機使
用的憑證以完成認證機制；Controller 會紀錄所連結
Switch Port以降低 Controller 之複雜度。 
因 Controller 具 Policy功能，可編譯成快速查詢
表；當 Flow 產生時，必須進行相關資訊配對，已決
定是否接收此 flow或透過何種路由(Routing)傳遞，並
藉由 Route Computation 規劃其路徑，因此，拓撲是根
據 Switch Manager所接收到 Switches的最新連結情形
所產生的。 
 
 
圖 3.24 Controller 元件 
 
四、 研究方法 
 
以下介紹本研究所提出的相關模組設計以及可調
式的 QoS 機制演算法。 
 
4.1 系統架構 
本研究提出以ToS-Based限制速率的QoS機制，
運用於 OpenFlow 網路。此機制利用 ToS 的值作為辨
別每個 Flow型態的依據，並依不同 Flow型態限制其
速率。本研究使用史丹佛大學所提出的 NetFPGA硬體
開發帄台實作一 OpenFlow Switch，並利用 LinkSys 無
線路由器實作一 OpenFlow AP。 
圖 4.1 為系統架構，當封包由應用程式伺服器送
出時，會依據其型態由 iptables 給予不同的 ToS 值，
例如，FTP 封包 ToS 值為#1，Http 封包 ToS 值為#2，
而 Video Stream封包 ToS值為#3，其中 ToS 的值用來
辨別不同的 Flow 型態。OpenFlow Switch 中的 Rate 
Limiter 根據 ToS 值限制每個 Flow的速率。 
 
Host Host
eth0 eth0
WLAN
Controller
NetFPGA
Openflow
Rate 
Limiter
LinkSys
Wireless Router
OpenWrt
Openflow
Application 
Server
PCI Bus
NetFPGA
Openflow
Rate 
Limiter
PCI Bus
Flow 1
Flow 2
Flow 3
iptable
iptable
iptable
Openflow Switch A Openflow Switch B Openflow AP
FTP
HTTP
VS
Database
圖 4.1系統架構 
本架構將 OpenFlow網路由有線網路延伸至無線
網路，Rate Limiter 模組實作在基於 NetFPGA硬體開
發帄台上的 OpenFlow，當 QoS 機制啟動時，Rate 
Limiter 會依據封包 ToS 的值限制其速率。此系統可分
為四個部分，如圖 4.2所示。 
 
 
圖 4.2 Proposed System Magnificence 
 
4.2 架構模組設計 
 
4.2.1 OpenFlow Switch 設計 
本計畫利用史丹佛大學所提出的 NetFPGA 硬體
開發帄台實作一 OpenFlow Switch，建立 OpenFlow網
路環境。首先，修改 Linux kernel，使得 NetFPGA硬
體開發帄台可以透過 PCI 介面連結至主機，接著參考
NetFPGA 與 OpenFlow 官 方 網 站 上 的 User 
Guide[23,24]，進行 NetFPGA 與 OpenFlow 封包的安
裝，然後根據硬體規格修改環境變數及路徑，本計畫
 第 20 頁 / 共 49頁 
 
Control Plane 
Rate Limiter模組使用依Token數量而定的Token 
Bucket 演算法機制，若 Token數量足夠，Rate Limiter
會將封包由 Data Flow轉送至佇列，否則，Fliter會將
封包丟棄，因此，決定適當的 Token 數量可調整封包
速率至符合 QoS 要求，封包傳送至佇列前採 First In 
First Out (FIFO)排程方法，圖 4.8為 Rate Limiter 模組
運作，圖 4.9為 Rate Limiter 詳細流程圖。 
 
 
圖 4.8 Rate Limiter 模組運作 
 
 
圖 4.9 頻寬控制流程 
 
圖 4.10為 OpenFlow Switch 預設的速率。圖 4.11
說明當封包 ToS 欄位值與 Flow Table 相符，OpenFlow 
Switch會限制 Flow的速率。 
 
 
圖 4.10 OpenFlow Switch預設速率 
 
 
圖 4.11 OpenFlow Switch 以限制的速率傳送封
包 
 
圖 4.12為 Rate Limiter 運作時的 Scenario，每個
由應用程式伺服器送出的封包，其標頭皆包含 ToS欄
位值，OpenFlow Switch 讀取 ToS 值後，判斷其屬於
哪一種 Flow，將封包依 Flow Table 中所設定的規則以
限制的速率由輸出佇列送出。 
 
 
圖 4.12 Rate Limiter Scenario 
 
4.3 Adaptive QoS Mechanism 
本系統所提出之機制可針對不同 Flow 導入使用
者的 Policy，圖 4.13 為各種封包 Flow 由應用程式伺
服器送至 Edge Router 時，由於封包混雜而造成終端使
用者 QoS 受到影響，因此，封包抵達 Edge Router 後
應依服務類型將 Flow 分類。為達成此目的，將 Edge 
Router 延伸為兩個存取點，一個 Router 位於雲端服務
網路邊緣，一個 Switch 位於終端使用者邊緣。GR與GS
 第 22 頁 / 共 49頁 
位加上設定值，依據封包類型之差異，其值也所有不
同。圖 5.3為利用 Wireshark 確認封包 ToS欄位值與設
定是否相同。 
 
OpenFlow
Switch
OpenFlow
Switch
OpenFlow
AP
eth0 eth0
WLAN
Controller
eth0
Video Stream 
ServerFTP
 Server
192.168.1.10
192.168.1.101
User B
192.168.1.100
User A
192.168.1.59
User C
192.168.1.11x
User D
192.168.1.11x
nf2c0 nf2c0nf2c1 nf2c1nf2c2nf2c3 nf2c3nf2c2
Database
 
圖 5.2 系統模擬環境 
 
 
圖 5.3 透過 Wireshark確認封包標頭 
 
圖 5.4為設定 Policy的流程，當 Server端與Client
端成功建立 Socket 連線後，在 Socket Client 端可設定
資料庫中的 ToS Policy，設定好的 ToS Policy會儲存
至資料庫中的 QoS_Traffic 資料表，根據 Policy，它會
參考資料庫中 QoS_Tos 資料表的 Speed_Value 欄位，
並將此 Policy加入 OpenFlow Table 中，顯示於 Server
端，圖 5.5 為資料庫中所設定的所有 Policy，管理者
可於 Flow Table中察看所設定的規則，如圖 5.6所示。 
 
 
圖 5.4 資料庫中 ToS Policy設定流程 
 
 
圖 5.5 資料庫中所設定的所有 Policy 
 
 
圖 5.6 Flow Table 
 
圖 5.7 與圖 5.8 為無線網路環境下，系統執行
 第 24 頁 / 共 49頁 
基本架構及模組之定義。而為了能夠針對龐大的網路
流量進行統計、分析以及如何透過合理的簡化將目前
的網路狀況摘要性的記錄下來之目的，本計畫已建構
Data Plane、Control Plane和 Management Plane 的相關
功能模組功能，藉以發展可調整的 QoS處理策略，以
達成完備的網路控制。 
 
 第 26 頁 / 共 49頁 
Conference on Computational Intelligence and 
Security, pp.1470-1474, November 2006. 
[24] A. Madhukar and C. Williamson, “A Longitudinal 
Study of P2P Traffic Classification,” Proceedings of 
the 14th IEEE International Symposium on 
Modeling, Analysis, and Simulation of Computer 
and Telecommunication Systems, pp.179-188, 
September 2006.  
[25] 徐于三, “隨機變換通訊埠的網路流量之分析與
影響評估” January 2008. 
[26] T. Karagiannis, A. Broido, N. Brownlee, K.C. Claffy 
and M. Faloutsos, “Is P2P dying or just hiding?” 
Proceedings of the IEEE GLOBECOM 2004 on 
Global Telecommunications Conference, Vol.3, 
pp.1532-1538, December 2004.  
[27] T. Karagiannis, D. Papagiannaki and M. Faloutsos, 
“BLINC: Multilevel Traffic Classification in the 
Dark” Proceedings of the ACM SIGCOMM, 
pp.229-240, August 2005. 
[28] T. Karagiannis, A. Broido, M. Faloutsos and K.C. 
Claffy, “Transport Layer Identification of p2p 
Traffic” Proceedings of the 4th ACM SIGCOMM 
Conference on Internet Measurement, October 
2004. 
[29] IPP2P, http://www.ipp2p.org 
[30] L7-filter, http://l7-filter.sourceforge.net 
[31] Netfilter Project, http://www.netfilter.org. 
[32] Rusty Russell and Harald Welte, Linux Netfilter 
Hacking HOWTO, 
www.netfilter.org/documentation/HOWTO/netfilter
-hacking-HOWTO.htm l, 2002. 
[33] N. Lin and M. Qi, “A QoS Model of Next 
Generation Network based on MPLS,” Proceedings 
of the IFIP International Conference on Network 
and Parallel Computing, pp.18-21, September 
2007. 
[34] D. Zhang and D. Ionescu, “QoS Performance 
Analysis in Deployment of DiffServ-aware MPLS 
Traffic Engineering,” Proceedings of the 8th ACIS 
International Conference on Software Engineering, 
Artificial Intelligence, Networking, and 
Parallel/Distributed Computing, pp.963-967, 
August 2007. 
[35] J. Salim, H. Khosravi, A. Kleen and A. Kuznetsov, 
Linux Netlink as an IP Services Protocol, RFC 3549, 
IETF, July 2003. 
[36] M. Casado, M.J. Freedman, J. Pettit, J. Luo, N. 
McKeown and S. Shenker, “Ethane: Taking Control 
of the Enterprise,” Proceedings of the Conference 
on Applications, Technologies, Architectures, and 
Protocols for Computer Communications, Vol.37, 
October 2007. 
[37] L. Zhang, “Virtual Clock: A New Traffic Control 
Algorithm for Packet Switching Networks,” 
Proceedings of the ACM Symposium on 
Communications Architectures and Protocols, 
Vol.20, pp.19-29, September 1990. 
[38] S. Floyd and K. Fall, “Promoting the Use of 
End-to-end Congestion Control in the Internet,”  
IEEE/ACM Transactions on Networking, Vol.7, 
No.8, August 1999. 
[39] H. Wang, H. Xie, L. Qiu, Y.R. Yang, Y. Zhang and A. 
Greenberg, “Cope: Traffic Engineering in Dynamic 
Networks,” Proceedings of the conference on 
Applications, Technologies, Architectures, and 
Protocols for Computer Communications, Vol.36, 
October 2006. 
[40]  K.K. Yap, M. Kobayashi, R. Sherwood, T.Y. 
Huang, M. Chan, N. Handigol and N. McKeown, 
“OpenRoads: Empowering Research in Mobile 
Networks,” Proceedings of the SIGCOMM 
Computer Communication, Vol.40, January 2010 
[41] A. Bianco, R. Brike, L.Giraudo and M.Palacin, 
“OpenFlow Switching: Data Plane Performance,” 
Proceedings of the IEEE International Conference 
on Communications, pp.1-5, May 2010.  
[42] S. Civanlar, M. Parlakisik, A.M. Tekalp, B. 
Gorkemli, B. Kaytaz and E. Onem, “A 
 第 28 頁 / 共 49頁 
附錄 程式碼 
 
1. adaptive.py 
#!/usr/bin/env python 
# -*- coding: utf-8 -*- 
 
from nox.lib.core     import * 
 
from nox.lib.packet.ethernet     import ethernet 
from nox.lib.packet.packet_utils import mac_to_str, 
mac_to_int 
 
from twisted.python import log 
 
import logging 
from time import time 
from socket import htons 
from struct import unpack 
 
import BeautifulSoup 
 
logger = 
logging.getLogger('nox.coreapps.examples.adaptive') 
 
#a=open('/root/policy.xml').read() 
#policy=BeautifulSoup.BeautifulStoneSoup(a) 
 
 
# Global adaptive instance  
inst = None 
 
condition = 0.84 
counter = 300 
t=0 
 
# Timeout for cached MAC entries 
CACHE_TIMEOUT = 5 
 
# -- 
# Given a packet, learn the source and peg to a 
switch/inport  
# -- 
def do_l2_learning(dpid, inport, packet): 
    global inst  
 
    # learn MAC on incoming port 
    srcaddr = packet.src.tostring() 
 
    if ord(srcaddr[0]) & 1: 
        return 
    if inst.st[dpid].has_key(srcaddr): 
        dst = inst.st[dpid][srcaddr] 
        if dst[0] != inport: 
            log.msg('MAC has moved from 
'+str(dst)+'to'+str(inport), system='adaptive') 
        else: 
            return 
    else: 
        log.msg('learned MAC 
'+mac_to_str(packet.src)+' on %d %d'% (dpid,inport), 
system="adaptive") 
 
    # learn or update timestamp of entry 
    inst.st[dpid][srcaddr] = (inport, time(), packet) 
 
    # Replace any old entry for (switch,mac). 
    mac = mac_to_int(packet.src) 
 
# -- 
# If we've learned the destination MAC set up a flow and 
# send only out of its inport.  Else, flood. 
# -- 
def forward_l2_packet(dpid, inport, packet, buf, bufid):     
    dstaddr = packet.dst.tostring() 
     
    #print packet.dst 
 
    if not ord(dstaddr[0]) & 1 and 
 第 30 頁 / 共 49頁 
 
    # don't forward lldp packets 
    # print ethernet.LLDP_TYPE 
    if packet.type == ethernet.LLDP_TYPE: 
        return CONTINUE 
     
  
    #  don't forward special packets (special computer) 
    if t == counter: 
 counter=counter+30 
 if float(policy.conditions['bu']) >= condition: 
  if policy.actions.allow.string == "Drop" and  
mac_to_str(packet.src) == policy.d1.mac.string: 
          print("************ KILL 
192.168.0.10 *************") 
          return CONTINUE 
 
    #timer 
    t=t+1 
 
    # learn MAC on incoming port 
    do_l2_learning(dpid, inport, packet) 
 
    forward_l2_packet(dpid, inport, packet, packet.arr, 
bufid) 
 
    return CONTINUE 
 
class adaptive(Component): 
 
    def __init__(self, ctxt): 
        global inst, t 
        Component.__init__(self, ctxt) 
        self.st = {} 
        inst = self 
 
    def install(self): 
        
inst.register_for_packet_in(packet_in_callback) 
        
inst.register_for_datapath_leave(datapath_leave_callback
) 
        
inst.register_for_datapath_join(datapath_join_callback) 
        inst.post_callback(1, timer_callback) 
 
    def getInterface(self): 
        return str(adaptive) 
 
def getFactory(): 
    class Factory: 
        def instance(self, ctxt): 
            return adaptive(ctxt) 
 
    return Factory() 
 
2. monitor.py 
# This app just drops to the python interpreter.  Useful 
for debugging 
# 
 
from nox.lib.core     import * 
from nox.coreapps.pyrt.pycomponent import 
Table_stats_in_event, Aggregate_stats_in_event 
from nox.lib.openflow import OFPST_TABLE,  
OFPST_PORT, ofp_match 
from nox.lib.packet.packet_utils import 
longlong_to_octstr 
 
MONITOR_TABLE_PERIOD     = 3 
MONITOR_PORT_PERIOD      = 3 
MONITOR_AGGREGATE_PERIOD = 3 
 
class Monitor(Component): 
 
    def __init__(self, ctxt): 
        Component.__init__(self, ctxt) 
 
 第 32 頁 / 共 49頁 
3. packet_utils.py 
import array 
import struct 
from socket import ntohs 
 
_ethtype_to_str = {} 
_ipproto_to_str = {} 
 
# Map ethernet oui to name 
_ethoui2name = {} 
 
# Map ethernet type to string 
_ethtype_to_str[0x0800] = 'IP' 
_ethtype_to_str[0x0806] = 'ARP' 
_ethtype_to_str[0x8035] = 'RARP' 
_ethtype_to_str[0x8100] = 'VLAN' 
_ethtype_to_str[0x88cc] = 'LLDP' 
 
# IP protocol to string 
_ipproto_to_str[0]  = 'IP' 
_ipproto_to_str[1]  = 'ICMP' 
_ipproto_to_str[2]  = 'IGMP' 
_ipproto_to_str[4]  = 'IPIP' 
_ipproto_to_str[6]  = 'TCP' 
_ipproto_to_str[9]  = 'IGRP' 
_ipproto_to_str[17] = 'UDP' 
_ipproto_to_str[47] = 'GRE' 
_ipproto_to_str[89] = 'OSPF' 
 
def checksum(data, start, skip_word = 0): 
 
    if len(data) % 2 != 0: 
        arr = array.array('H', data[:-1]) 
    else: 
        arr = array.array('H', data) 
 
    if skip_word: 
        for i in range(0, len(arr)): 
            if i == skip_word: 
                continue 
            start +=  arr[i] 
    else: 
        for i in range(0, len(arr)): 
            start +=  arr[i] 
 
    if len(data) % 2 != 0: 
        start += struct.unpack('H', data[-1]+'\0')[0] 
 
    start  = (start >> 16) + (start & 0xffff) 
    start += (start >> 16); 
 
    return ntohs(~start & 0xffff) 
 
 
def ip_to_str(a): 
    return "%d.%d.%d.%d" % ((a >> 24) & 0xff, (a >> 
16) & 0xff, \ 
                            (a >> 8) & 0xff, a & 
0xff) 
 
 
def ipstr_to_int(a): 
    octets = a.split('.') 
    return int(octets[0]) << 24 |\ 
           int(octets[1]) << 16 |\ 
           int(octets[2]) <<  8 |\ 
           int(octets[3]); 
 
def array_to_ipstr(a): 
    return "%d.%d.%d.%d" % (a[0], a[1], a[2], a[3]) 
 
def octstr_to_array(ocstr): 
    a = array.array('B') 
    for item in ocstr.split(':'): 
        a.append(int(item, 16)) 
    return a 
 
def array_to_octstr(arr): 
 第 34 頁 / 共 49頁 
        end.remove('(hex)') 
        oui_name = ' '.join(end) 
        # convert oui to int 
        oui = int(oui_str, 16) 
        _ethoui2name[oui] = oui_name.strip() 
 
load_oui_names() 
 
4. ethernet.py 
import struct 
 
from packet_base import packet_base 
 
from packet_utils import  * 
from array import * 
 
ETHER_ANY            = 
"\x00\x00\x00\x00\x00\x00" 
ETHER_BROADCAST      = "\xff\xff\xff\xff\xff\xff" 
BRIDGE_GROUP_ADDRESS = 
"\x01\x80\xC2\x00\x00\x00" 
LLDP_MULTICAST       = 
"\x01\x80\xc2\x00\x00\x0e" 
PAE_MULTICAST        = 
'\x01\x80\xc2\x00\x00\x03' # 802.1x Port Access Entity 
NDP_MULTICAST        = 
'\x01\x23\x20\x00\x00\x01' # Nicira discovery 
                                                  
# multicast 
 
class ethernet(packet_base): 
    "Ethernet packet struct" 
 
    MIN_LEN = 14 
 
    IP_TYPE   = 0x0800 
    ARP_TYPE  = 0x0806 
    RARP_TYPE = 0x8035 
    VLAN_TYPE = 0x8100 
    LLDP_TYPE = 0x88cc 
    PAE_TYPE  = 0x888e           # 802.1x Port 
Access Entity 
 
    type_parsers = {} 
 
    def __init__(self, arr=None, prev=None): 
 
 
        self.prev = prev 
        if type(arr) == type(''): 
            arr = array('B', arr) 
 
        self.dst  = array('B', ETHER_ANY) 
        self.src  = array('B', ETHER_ANY) 
        self.type = 0 
        self.next = '' 
 
        if arr != None: 
            assert(type(arr) == array) 
            self.arr = arr 
            self.parse() 
 
    def parse(self): 
        alen = len(self.arr) 
        if alen < ethernet.MIN_LEN: 
            self.msg('warning eth packet data too 
short to parse header: data len %u' % alen) 
            return 
 
        self.dst = self.arr[:6] 
        self.src = self.arr[6:12] 
        self.type = struct.unpack('!H', 
self.arr[12:ethernet.MIN_LEN])[0] 
 
        self.hdr_len = ethernet.MIN_LEN 
        self.payload_len = alen - self.hdr_len 
 
        self.parsed = True 
 第 36 頁 / 共 49頁 
 
N_TABLES    = 'n_tables' 
N_BUFFERS   = 'n_bufs' 
CAPABILITES = 'caps' 
ACTIONS     = 'actions' 
PORTS       = 'ports' 
 
PORT_NO     = 'port_no' 
SPEED       = 'speed' 
CONFIG      = 'config' 
STATE       = 'state' 
CURR        = 'curr' 
ADVERTISED  = 'advertised' 
SUPPORTED   = 'supported' 
PEER        = 'peer' 
HW_ADDR     = 'hw_addr' 
 
##############################################
################################## 
# API NOTES: 
# 
# Automatically returns CONTINUE for handlers that do 
not 
# return a value (handlers are supposed to return a 
Disposition) 
# 
# All values should be passed in host byte order.  The 
API changes 
# values to network byte order based on knowledge of 
field.  (NW_SRC 
# --> 32 bit val, TP_SRC --> 16 bit value, etc.).  Other 
than 
# DL_SRC/DST and NW_SRC/DST fields, packet 
header fields should be 
# passed as integers.  DL_SRC, DL_DST fields should 
be passed in as 
# either vigil.netinet.ethernetaddr objects.  They can 
however also be 
# passed in any other type that an ethernetaddr 
constructor has be 
# defined for.  NW_SRC/NW_DST fields meanwhile 
can be passed either 
# ints, ip strings, or vigil.netinet.ipaddr objects. 
##############################################
############################# 
 
class Component: 
    """Abstract class to inherited by all Python 
components.""" 
    def __init__(self, ctxt): 
        self.ctxt = ctxt 
        self.component_names = None 
 
    def configure(self, config): 
        """Configure the component. 
        Once configured, the component has parsed its 
configuration and 
        resolve any references to other components it 
may have. 
        """ 
        pass 
 
    def install(self): 
        """Install the component. 
        Once installed, the component runs and is 
usable by other 
        components. 
        """ 
        pass 
 
    def getInterface(self): 
        """Return the interface (class) component 
provides.  The default 
        implementation returns the class itself.""" 
        return self.__class__ 
 
    def resolve(self, interface): 
        return self.ctxt.resolve(str(interface)) 
 第 38 頁 / 共 49頁 
order 
                a = struct.pack("HHI", 
htons(action[0]), htons(8), iaddr.addr) 
            elif action[0] == 
openflow.OFPAT_SET_TP_SRC \ 
                    or action[0] == 
openflow.OFPAT_SET_TP_DST: 
                a = struct.pack("!HHHH", action[0], 
8, action[1], 0) 
            else: 
                print 'invalid action type', action[0] 
                return None 
 
            action_str = action_str + a 
 
        return action_str 
 
    def send_port_mod(self, dpid, portno, hwaddr, mask, 
config): 
        try: 
            addr = create_eaddr(str(hwaddr)) 
            return self.ctxt.send_port_mod(dpid, 
portno, addr, mask, config) 
        except Exception, e: 
            print e 
            #lg.error("unable to send port 
mod"+str(e)) 
 
    def send_switch_command(self, dpid, command, 
arg_list): 
        return self.ctxt.send_switch_command(dpid, 
command, ",".join(arg_list)) 
 
    def switch_reset(self, dpid): 
        return self.ctxt.switch_reset(dpid) 
 
    def switch_update(self, dpid): 
        return self.ctxt.switch_update(dpid) 
 
 
    def send_openflow_packet(self, dp_id, packet, 
actions, 
                             
inport=openflow.OFPP_CONTROLLER): 
        """ 
        sends an openflow packet to a datapath 
 
        dp_id - datapath to send packet to 
        packet - data to put in openflow packet 
        actions - list of actions or dp port to send out of 
        inport - dp port to mark as source (defaults to 
Controller port) 
        """ 
        if type(packet) == type(array.array('B')): 
            packet = packet.tostring() 
 
        if type(actions) == types.IntType: 
            
self.ctxt.send_openflow_packet_port(dp_id, packet, 
actions, inport) 
        elif type(actions) == types.ListType: 
            oactions = 
self.make_action_array(actions) 
            if oactions == None: 
                raise Exception('Bad action') 
            
self.ctxt.send_openflow_packet_acts(dp_id, packet, 
oactions, inport) 
        else: 
            raise Exception('Bad argument') 
 
    def send_openflow_buffer(self, dp_id, buffer_id, 
actions, 
                             
inport=openflow.OFPP_CONTROLLER): 
        """ 
        Tells a datapath to send out a buffer 
 
 第 40 頁 / 共 49頁 
                 port) 
        """ 
        if buffer_id != None: 
            self.send_openflow_buffer(dp_id, 
buffer_id, actions, inport) 
        else: 
            self.send_openflow_packet(dp_id, packet, 
actions, inport) 
 
    def delete_datapath_flow(self, dp_id, attrs): 
        """ 
        Delete all flow entries matching the passed in 
(potentially 
        wildcarded) flow 
 
        dp_id - datapath to delete the entries from 
        attrs - the flow as a dictionary (described 
above) 
        """ 
        return self.send_flow_command(dp_id, 
openflow.OFPFC_DELETE, attrs) 
 
    def delete_strict_datapath_flow(self, dp_id, attrs, 
                        
priority=openflow.OFP_DEFAULT_PRIORITY): 
        """ 
        Strictly delete the flow entry matching the 
passed in (potentially 
        wildcarded) flow.  i.e. matched flow have 
exactly the same 
        wildcarded fields. 
 
        dp_id - datapath to delete the entries from 
        attrs - the flow as a dictionary (described 
above) 
        priority - the priority of the entry to be deleted 
(only meaningful 
                   for entries with wildcards) 
        """ 
        return self.send_flow_command(dp_id, 
openflow.OFPFC_DELETE_STRICT, 
                                      attrs, 
priority) 
    
##############################################
############################# 
    # The following methods manipulate a flow entry in 
a datapath. 
    # A flow is defined by a dictionary containing 0 or 
more of the 
    # following keys (commented keys have already 
been defined above): 
    # 
 
    # DL_SRC     = "dl_src" 
    # DL_DST     = "dl_dst" 
    # DL_VLAN    = "dl_vlan" 
    # DL_TYPE    = "dl_type" 
    # NW_SRC     = "nw_src" 
    # NW_DST     = "nw_dst" 
    # NW_PROTO   = "nw_proto" 
    # TP_SRC     = "tp_src" 
    # TP_DST     = "tp_dst" 
    # 
    # Absent keys are interpretted as wildcards 
    
##############################################
############################# 
 
    def install_datapath_flow(self, dp_id, attrs, 
idle_timeout, hard_timeout, 
                              actions, 
buffer_id=None, 
                              
priority=openflow.OFP_DEFAULT_PRIORITY, 
                              inport=None, 
packet=None): 
        """ 
 第 42 頁 / 共 49頁 
                              
gen_packet_in_cb(handler)) 
 
    def register_for_flow_expired(self, handler): 
        
self.register_handler(Flow_expired_event.static_get_na
me(), 
                              handler) 
    def register_for_flow_mod(self, handler): 
        
self.register_handler(Flow_mod_event.static_get_name()
, 
                              handler) 
 
    def register_for_bootstrap_complete(self, handler): 
        
self.register_handler(Bootstrap_complete_event.static_g
et_name(), 
                              handler) 
 
    
##############################################
################################## 
    # register a handler to be called on a every 
switch_features event 
    # handler will be called with the following args: 
    # 
    # handler(dp_id, attrs) 
    # 
    # attrs is a dictionary with the following keys: 
 
 
    # the PORTS value is a list of port dictionaries 
where each dictionary 
    # has the keys listed in the register_for_port_status 
message 
    
##############################################
################################## 
    def register_for_datapath_join(self, handler): 
        
self.register_handler(Datapath_join_event.static_get_na
me(), 
                              
gen_dp_join_cb(handler)) 
 
    
##############################################
################################## 
    # register a handler to be called whenever table 
statistics are 
    # returned by a switch. 
    # 
    # handler will be called with the following args: 
    # 
    # handler(dp_id, stats) 
    # 
    # Stats is a dictionary of table stats with the 
following keys: 
    # 
    #   "table_id" 
    #   "name" 
    #   "max_entries" 
    #   "active_count" 
    #   "lookup_count" 
    #   "matched_count" 
    # 
    # XXX 
    # 
    # We should get away from using strings here 
eventually. 
    # 
    
##############################################
################################## 
 
    def register_for_table_stats_in(self, handler): 
        
 第 44 頁 / 共 49頁 
 
    # register a handler to be called whenever 
description 
    # statistics are returned by a switch. 
    # 
    # handler will be called with the following args: 
    # 
    # handler(dp_id, stats) 
    # 
    # Stats is a dictionary of descriptions with the 
following keys: 
    # 
    #   "mfr_desc" 
    #   "hw_desc" 
    #   "sw_desc" 
    #   "serial_num" 
    # 
    
##############################################
################################## 
 
    def register_for_desc_stats_in(self, handler): 
        
self.register_handler(Desc_stats_in_event.static_get_na
me(), 
                              
gen_ds_in_cb(handler)) 
 
    def register_for_datapath_leave(self, handler): 
        """ 
        register a handler to be called on a every 
datapath_leave 
        event handler will be called with the following 
args: 
 
        handler(dp_id) 
        """ 
        
self.register_handler(Datapath_leave_event.static_get_na
me(), 
                              
gen_dp_leave_cb(handler)) 
 
    
##############################################
############################ 
    # register a handler to be called on a every 
port_status event 
    # handler will be called with the following args: 
    # 
    # handler(dp_id, ofp_port_reason, attrs) 
    # 
    # attrs is a dictionary with the following keys: 
 
 
    
##############################################
############################# 
    def register_for_port_status(self, handler): 
        
self.register_handler(Port_status_event.static_get_name(
), 
                              
gen_port_status_cb(handler)) 
 
    
##############################################
############################# 
    # register a handler to be called on every packet_in 
event matching 
    # the passed in expression. 
    # 
    # priority - the priority the installed classifier rule 
should have 
    # expr - a dictionary containing 0 or more of the 
following keys. 
 
 
 第 46 頁 / 共 49頁 
            else: 
                # check for max? 
                if val > UINT32_MAX: 
                    print 'value %u exceeds 
accepted range', val 
                    return False 
                e.set_uint32_field(field, val) 
 
        return 
self.ctxt.register_handler_on_match(gen_packet_in_cb(h
andler), priority, e) 
 
    def register_for_switch_mgr_join(self, handler): 
        """ 
        register a handler to be called on every 
switch_mgr_join 
        event handler will be called with the following 
args: 
 
        handler(mgmt_id) 
        """ 
        
self.register_handler(Switch_mgr_join_event.static_get_
name(), 
                              
gen_switch_mgr_join_cb(handler)) 
 
    def register_for_switch_mgr_leave(self, handler): 
        """ 
        register a handler to be called on every 
switch_mgr_leave 
        event handler will be called with the following 
args: 
        handler(mgmt_id) 
        """ 
        
self.register_handler(Switch_mgr_leave_event.static_get
_name(), 
                              
gen_switch_mgr_leave_cb(handler)) 
 
    def unregister_handler(self, rule_id): 
        """ 
        Unregister a handler for match. 
        """ 
        return self.ctxt.register_handler(event_type, 
event_name, handler) 
 
5.server.c 
 
#include <strings.h> 
#include <stdlib.h>  
#include <stdio.h> 
#include <sys/types.h> 
#include <sys/socket.h> 
#include <netinet/in.h> 
#include <arpa/inet.h> 
#include <unistd.h> 
 
#define PORT 1234 
#define MAXSOCKFD 10 
int main(int argc, char **argv) { 
 
    int sockfd, newsockfd, 
is_connected[MAXSOCKFD], fd; 
    int i; 
    struct sockaddr_in addr; 
    int addr_len = sizeof(struct sockaddr_in); 
    fd_set readfds; 
    char buffer[256]; 
    char msg[] = "Welcome to server!"; 
     
    if ((sockfd = socket(AF_INET,SOCK_STREAM, 0)) 
< 0) { 
        perror("socket"); 
        exit(1); 
    } 
    bzero(&addr,sizeof(addr)); 
 第 48 頁 / 共 49頁 
number); 
                            send(is_connected[i], 
buffer, sizeof(buffer), 0);  
                        } 
                    } 
                } 
            } 
 
    } 
 
    return 0; 
} 
 
6.client.c 
 
#include <strings.h> 
#include <stdlib.h>  
#include <stdio.h> 
#include <sys/stat.h> 
#include <fcntl.h> 
#include <unistd.h> 
#include <sys/types.h> 
#include <sys/socket.h> 
#include <netinet/in.h> 
#include <arpa/inet.h> 
#define PORT 1234 
#define SERVER_IP "127.0.0.1" 
int main(int argc, char **argv) 
{ 
  int s; 
  struct sockaddr_in addr; 
    char buffer[256]; 
    
  if((s = socket(AF_INET,SOCK_STREAM,0))<0){ 
    perror("socket"); 
    exit(1); 
  } 
     
  bzero(&addr,sizeof(addr)); 
  addr.sin_family = AF_INET; 
  if(argc!=3){ 
    printf("Warning! please use ./client SERVER_IP 
PORT\n"); 
    printf("Now use default 
setting,SERVER_IP=%s,PORT=%d\n",SERVER_IP,PO
RT); 
    addr.sin_port=htons(PORT); 
    addr.sin_addr.s_addr = inet_addr(SERVER_IP);   
  } 
  else{ 
    addr.sin_port=htons(atoi(argv[2])); 
    addr.sin_addr.s_addr = inet_addr(argv[1]); 
  } 
  
  if(connect(s,(struct sockaddr 
*)&addr,sizeof(addr))<0){ 
    perror("connect"); 
    exit(1); 
  } 
   
  recv(s,buffer,sizeof(buffer),0); 
  printf("%s\n",buffer); 
 
 
   
  while(1){ 
    bzero(buffer,sizeof(buffer)); 
      
    read(STDIN_FILENO,buffer,sizeof(buffer)); 
      
    if(send(s,buffer,sizeof(buffer),0)<0){ 
      perror("send"); 
      exit(1); 
    } 
    
    if(recv(s,buffer,sizeof(buffer),0)<0){ 
      perror("recv"); 
      exit(1); 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/09/29
國科會補助計畫
計畫名稱: 子計畫三：設計與實作一具有可調式服務品質保證之網路虛擬化架構(2/2)
計畫主持人: 陳俊良
計畫編號: 99-2219-E-011-001- 學門領域: 通訊軟體及平台(網通國家型)
無研發成果推廣資料
Letter)    
 
3. Semantic 
Network Devices: 
A New QoS 
Paradigm? (to be 
submitted to 
IEEE Internet 
computing) 
 
4. Service 
Based-Rate 
Limiter 
Switching 
Mechanism for 
Virtualized IP 
Networks (to be 
submitted to 
IEEE/ACM 
Transactions on 
Networking) 
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
1. Distributed 
Multi-Agent 
Schemes for 
Predictable QoS 
on Heterogeneous 
Wireless 
Networks 
(accepted, will 
be presented in 
17th IEEE 
ICPADS) 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
參與計畫人力 
（外國籍） 
專任助理 1 0 100% 
人次 
Yanuarius 
Teofilus Larosa
目 計畫成果推廣之參與（閱聽）人數 0  
 
