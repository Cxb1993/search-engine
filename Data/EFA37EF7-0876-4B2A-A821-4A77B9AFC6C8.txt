manufacturing or measuring characteristics, the data 
points with the same batch number or measuring group 
might require to be grouped in the same clusters. In 
this research, CML constrained problem is defined as 
performing data clustering on data points with 
complete partition information to search for the 
interesting patterns. 
 
While performing the traditional clustering method 
such as a constrained hierarchical on a CML 
constrained problem, the information missing issue is 
caused by the centroid replacement process where all 
data points within a block are replaced by a 
centroid. This issue affects the clustering accuracy 
because the clustering only concerns on the centriod 
data without considering the characteristic of data 
blocks. In this research, a two-step clustering 
method is proposed to solve CML clustering problem. 
First, in order to alleviate the information missing 
issue, in the similarity matrix, the principal 
component eigenvectors which can represent the 
characteristics of the partition block is added. 
Based on the new similarity matrix, a set of 
candidate clusters can be generated. In the second 
step, an integer linear programming optimization is 
formulated to select the resulting clusters from the 
candidate clusters. The optimization can guarantee 
the resulting clusters to have minimum within-cluster 
distances. 
英文關鍵詞： Constraint Clustering, Principal Component Analysis, 
Integer Linear Programming 
 
 II 
List of Tables 
Table 1 Comparison of clustering algorithms: K-means and agglomerative hierarchical. Both 
traditional and constrained-version of them are listed. .............................................................................. 5 
Table 2. An Illustration of Integer Programming Formulation. Problem with 7 CML partition block and 6 sets 
of candidate clusters. .................................................................................................................................... 9 
Table 3. An Illustration of Candidate Clusters (A Matrix in Equ. 5) ................................................................. 11 
Table 4. An Illustration of within-cluster distances (  vector in Equ. 5) ........................................................ 12 
 
  
 1 
1. Introduction 
Clustering is one of important unsupervised data mining tools which has been applied in different areas. The 
strategy of clustering is to partition the data points into groups (clusters) such that the instances in the same 
cluster have smaller pairwise dissimilarities than those in different clusters (Hastie et al. 2001). Because the 
data labels are not provided, clustering can be considered as a method which tries to find patterns in the data 
and beyond what would be considered pure unstructured noise (Ghahramani 2004). A general clustering 
problem can be defined as a scenario of assigning n items ( 1... nx x ) in a dataset D  into k clusters ( 1... kC C ) to 
form a partition 
D  ( 1 2... ,     D k iC C C C ). 
 
Because of the data driven nature of clustering, it is usually difficult to correctly measure the correctness or 
success of method, especially when the number of clusters is unknown (Hastie et al. 2001; Jain 2008). 
However, in some cases, information about the problem domain such as class labels or true similarity between 
objects is available. The field of semi-supervised or constrained clustering grew out of the need to find ways 
to accommodate this information to improve efficiency and purity of clustering (Davidson and Ravi 2005a; 
Davidson and Ravi 2005b; Wagsta et al. 2001; Wagsta and Cardie 2000). In semi-supervised or constrained 
clustering, the instance-level restrictions – must-link (ML) and cannot-link (CL) constraints can be applied 
based on priori knowledge about the pairs of data points (Wagsta et al. 2001; Wagsta and Cardie 2000). 
Essentially, a ML constraint specifies that two instances must be placed in the same cluster while a CL 
constraint enforces that two instances should not be placed in the same cluster. Fig. 1 illustrates an example of 
using instance-level constraints in clustering. In short, a constrained clustering problem is one in which the 
pre-existing knowledge about desired 
D  is given. 
 
The pre-existing knowledge about desired D  might sometime capture larger building blocks of instances in 
the dataset D  to form a group-level constraint. The group-level constraint can be treated as the union of 
some ML constraints based on the transitive and combinable characteristics of the ML constraint. For instance, 
two ML constraint { 1x , 2x } and { 2x , 3x } implies that an ML constraint { 1x , 3x } exists and they can be 
combined into a group-level constraint { 1x , 2x , 3x } (Law et al. 2004).  
 
Prior information such as labeled data, known data groupings or associations between the data points can be 
available to construct group-level constraints, for example, the nationality of human being or the batch 
number of electronic component. This kind of data feature can provide a complete set of must-link constraints 
which covers all instances in the dataset D . In other words, it is a special case of a constrained clustering 
problem in which the r  pre-existing partitions ( 1... rM M ) among n items is provided. Each item ix  in the 
dataset D  must belong to one of 1... rM M  partition exclusively ( 1 2... ,  D rM M M     ,  jM   
 each ,  
i j
M x  ( ) ( )i j i kM M   x x ). If the clustering is performed upon those pre-determined 
partitions with r k  and no CL constraint is involved, in this research, we named these particular 
restrictions as complete must-link (CML) constraints for clustering. 
 
 3 
 
 
Fig. 2. Illustrative example of information missing issue. M1, M2, and M3 are partition blocks overlapped 
together. The solid points m1, m2, and m3 denote three centroids of M1, M2, and M3 , respectively. Because of 
data points in M1, M2, and M3 will be abandoned by transitive construction, it causes information missing on 
describing original partition blocks. 
In order to apply the CML constraints problem without the information missing issue, a new method is 
required. In this research, a new approach which integrates principal component analysis (PCA) and integer 
programming (IP) is proposed. The PCA which provides the supplemental information describing original 
partition blocks is suggested to be included in the distance matrix of the constrained clustering algorithm. 
Then, the IP method is used to optimize the CML constrained problem by using the new distance matrix.  
2. Objective 
The goal of this research is to develop a new clustering method aimed on solving CML constrained clustering 
problem. CML constrained clustering problem can be treated as a special but not uncommon case of 
constrained clustering problem where the information of complete partition blocks is available. It is 
widespread in data mining analysis. For example, in quality analysis of product/component testing, the batch 
number of component or the testing sequence are available to construct the partition block of data points. The 
clustering method is performed on data points with partition block information to search for the interesting 
patterns about quality measures. Another example can be the clustering analysis in sale analysis of customer 
relationship management while the location information (store branch) of sale occurred is available. 
Clustering sale behavior data under location partition block constraints (assuming the same store branch 
shares similar service quality) can provide insight about customer marketing strategy.  
 
As described in the previous section, the proposed clustering method needs to handle not only the complete 
must-link constraints, but also the information missing issue. The supplementary information about partition 
blocks constructed by CML constraints should be considered in the clustering method, specifically, on the 
distance matrix. Therefore, this research will study the methodology of applying PCA method to improve the 
accuracy of CML constrained clustering.  
IP is missing 
 
 5 
Table 1 Comparison of clustering algorithms: K-means and agglomerative hierarchical. Both traditional and 
constrained-version of them are listed. 
 K-means Agglomerative Hierarchical 
Algorithm Traditional version 
Constrained version 
(CML constraints) 
Traditional version 
Constrained version 
(CML constraints) 
Input 
Number of clusters 
(k) 
Distance matrix, 
size is (n×n) 
Number of clusters 
(k) 
Distance matrix of 
centroids as a 
partition block, size is 
(r×r) 
Distance Matrix, size is 
(n×n) 
Distance matrix of 
centroids as a partition 
block, size is (r×r) 
Output 
k clusters k clusters Dendrogram with all 
data points 
Dendrogram with r 
centroid of connected 
components 
Time 
Complexity* 
O(knl) O(krl) O(n2log n) O(r2log r) 
Space 
Complexity* 
O(k+n) 
O(k+r) 
If k <= r 
O(n2) O(r2) 
Advantages 
Simple and 
ubiquitous  
ML constraints can 
improve efficiency+ 
Consistency of 
clustering result 
Can choose appropriate 
k later by dendrogram  
ML constraints can 
improve efficiency+ 
Disadvantages 
Should decide k in 
advance 
Sensitive to initial 
selection 
Converge to local 
minimum 
Detailed information 
of original data is 
ignored 
More time and space 
complexity 
Detailed information of 
original data is ignored  
r: # of connected components; k: # of clusters; n: # of instances; l: # of iterations 
* More detailed information about this complexity can be found in (Jain et al. 2000; Zhao and Karypis 2005) 
+ More detailed information about this complexity can be found in (Davidson and Ravi 2005a) 
 
As can be seen, K-means algorithm has less time and space complexity than agglomerative hierarchical 
algorithm no matter in traditional or constrained versions. That is the main reason of K-means’ popularity. 
However, K-means is sensitive to initial cluster selection and it only converges to local minimum. Therefore, 
it usually needs to restart several times for choosing the smallest value of the error. On the other hand, 
agglomerative hierarchical algorithm is more versatile because it allows choosing a better number of clusters 
later by reviewing the generated dendrogram. This nature of hierarchical algorithm inevitably costs more 
complexity.  
 7 
[ , ]m' m s  Eq. 2 
: original centroid matrix
: new similarity matrix
: supplementary feature (as a matrix)
m
m'
s
 
In order to describe the shape characteristics of each partition, the eigenvectors of each data partition are 
computed by PCA method. In this research, the coefficients of the eigenvector of first principal component are 
used as the supplementary features in the centroid set of partition. The number of the adding features 
(coefficients of the first eigenvector) is equal to the number of features due to the nature of PCA.  
 
In this research, the constraint-based agglomerative hierarchical clustering is used for data analysis because 
the no randomization is required for hierarchical clustering. Fig. 4 show the algorithm of constraint-based 
CML agglomerative clustering which uses PCA vectors as the adding features. 
 
min max...i
D CML D D
Dendrogram i k k
 ConstrainedCMLAgglomerative_PCA(data set ,must-link constraints ) 
returns ,  =   
 
1 2
1 1

, , . . .,
... , . . ., , x .
 1. Construct the transitive closure of the  constraints resulting in  partition 
     blocks    .
 2. Let    be the r cluster centers of     and replace all  in 
r
r r
CML r
M M M
m m M M CML D
1
1
, . . .,
... '
 3. Compute the eigenvectors P of the first principal component for each partition 
      block   
 4. Append P to corresponding  to form '  
 5. Construct the distance matrix based on 
r
r
M M
m m m
the combined
 
min
l m
k
C C
 6. For each t > 
     (a) Select a pair of clusters  and  according to the specified distance criterion 
 
1
1
l m l tC C C Dendrogram
t t

 
     (b) Merge  into  and remove . (The result is )
     (c) .
 
Fig. 4. Constrained CML agglomerative hierarchical clustering algorithm with PCA  
Please note that the clustering result by Constrained CML agglomerative hierarchical clustering algorithm 
with PCA only consider pair-wise distance of centroid of data partitions and PCA. In the following section, 
the integer programming is introduced to also consider the inner-cluster distance of clustering results. 
4.2 Integer Programming Optimization 
(Mueller and Kramer 2010) proposed an integer linear programming models to formulate a clustering problem 
under the user-defined constraints. In their research, the candidate clusters were constructed by the 
group-level constraints, and the subset of those candidate clusters was determined by maximize the similarity 
measures. Although the similarity measure did not consider the information missing issue mentioned in their 
 9 
Table 2. An Illustration of Integer Programming Formulation. Problem with 7 CML partition block and 6 sets 
of candidate clusters. 
1
2
3
4
5
6
7
1
2
3
4
5
i
j
:=Pi
:=Cj
6
 
 
Two binary vectors are defined to specify the decision variables of clustering. A vector x  is introduced to 
express which clusters are selected from candidate clusters 1C  to nC  (Eq. 4). A vector y  denotes which 
partitions are covered by the selected clusters. If 1iy  , it means ip  is covered by a selected cluster. 
 
1 if cluster  is selected 
0 otherwise
j
j
C
x

 

 Eq. 4 
 
The   indicates the completeness percentage of all of ip  which are included in the final clustering result. 
If all of ip  is covered by the selected clusters, then 1  . In this research, no overlapping clustering (one 
partition block can be only included in one cluster) is considered, and all of partitions should be covered. So, 
1   and y  is a vector whose elements are all equal to 1. The optimization problem constructed by integer 
programming is shown below in Eq. 5. 
 
 
1
Minimize 
subject to (i)     
                (ii)    
                (iiiv) 
                (iv)    0,1
                (v)     0,1




 


T
T
1
1
T
n
m
x
k
Ax y
x k
y m
x
y
 Eq. 5 
 
Although the integer programming problem can be easily solved by any optimization software, several issues 
related to integer programming should be investigated. First, it is impossible to solve the optimization over the 
variable k  (number of clusters) in the objective function. We can treat k  as a constant and resolve the 
optimization problem with a varying values for k . The performance comparison of clustering results under 
different k  values should be conducted. Secondly, the solubility of this IP problem highly depends on how 
A : 1C  2
C
 3
C
 4
C
 5
C
 6
C
 
1m  1 0 0 1 0 0 
2m  1 0 0 0 0 1 
3m  1 0 0 1 0 0 
4m  0 1 0 0 1 0 
5m  0 1 0 1 0 0 
6m  0 0 1 0 1 0 
7m  0 0 1 0 0 1 
  1.33 1.2 2.1 3.7 3.6 4 
 11 
compensate for the information missing issue caused by the CML constraints. 
 
Fig. 7. Comparison of constraint-based agglomerative hierarchical and the proposed clustering 
method which consider PCA vectors 
Based on the clustering results with considering PCA vectors, the candidate clusters were constructed based 
on the dendrogram tree while changing the height from low to high. Table 3 shows the candidate clusters 
generated from the clustering result (with PCA vectors). The binary indicator specifies if cluster jC  contains 
the partition im . As can be seen, different clustering results, 27 clustering candidates in this case, based on 
dendrogram is constructed. Table 4 shows the corresponding within-cluster distances for each candidate 
clusters. 
 
Table 3. An Illustration of Candidate Clusters (A Matrix in Equ. 5) 
X 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 
Partition#1 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 
Partition#2 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 
Partition#3 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 
Partition#4 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 
Partition#5 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 
Partition#6 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 
Partition#7 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 
Partition#8 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 
 
 
 
 13 
In this research, the proposed method considers data partition distribution by PCA and utilizes the IP 
programming to optimal the inner-cluster distance of the clustering result. The optimal number of clusters can 
be evaluated by the total inner-cluster distance. The experimental result shows that the clustering can alleviate 
the information missing issue and search the optimal clustering with lower inner-cluster distance. 
6. Self-evaluation and Contribution 
In terms of academic contribution, this research plans to develop a new clustering method for solving the 
complete must-link (CML) constraints problem which is not uncommon in data mining domain. The 
applications that the developed method can applied on are widespread in product quality analysis, customer 
relationship management, web mining, and etc. The adding features expected to represent the characteristics 
of the data partition can help on alleviating the impact of the information missing issue due to centroid 
replacement process. Therefore, the developed framework of analyzing CML constraints problem can provide 
an alternative method in data mining and multivariate analysis domain.  
 
This research work can also contribute to practical data analysis in several industries. For example, the 
developed method can be utilized on the quality analysis such as defect detection or quality segmentation. 
Also, in customer relationship management, the clustering analysis model is useful on analyzing the 
purchasing behavior based on location or branch partition information. Additionally, the new method by 
integer programming can be solved by any OR software package.  
 
The result of this work will be soon submitted to at least one SCI journal paper in data mining or operation 
research area. Currently, the author targets on European Journal of Operation Research. Depending on the 
availability of the practical dataset, the analysis result can also be published in the domain-relevant 
publication. The case study of quality analysis by the developed method can also be documented as the 
teaching material for education usage.  
 
7. References 
Basu, S., Davidson, I., and Wagstaff, K. (2008). Constrained Clustering: Advances in Algorithms, Theory, and 
Applications. (S. Basu, I. Davidson, and K. Wagstaff), Chapman & Hall/CRC, Boca Raton, FL. 
Davidson, I., and Ravi, S. S. (2005). "Agglomerative Hierarchical Clustering with Constraints: Theory and 
Empirical Resutls." 9th European Conference on Principles and Practice of Knowledge Discovery in 
Databases, PKDD 2005, A. Jorge, L. Torgo, P. Brazdil, R. Camacho, and J. Gama, Birkhäuser, Porto, 
Portugal, 59-70. 
Davidson, I., and Ravi, S. S. (2005). "Clustering With Constraints: Feasibility Issues and the k-Means 
Algorithm." Fifth SIAM International Conference on Data Mining, H. Kargupta, J. Srivastava, C. 
Kamath, and A. Goodman, SIAM, Newport, CA, 138-149. 
Fisher, D. H. (1987). "Knowledge Acquisition Via Incremental Conceptual Clustering." Machine Learning, 2, 
139-172. 
Friedman, D. J., Hansen, M. H., Nair, V. N., and James, D. A. (1997). “Model-Free Estimation of Defect 
 1 
 
國科會補助專題研究計畫出席國際學術會議心得報告 
                                   日期：101年 10 月 31 日 
                                 
一、參加會議經過 
Institute For Operations Research and the Management Sciences (INFORMS) 年度會議，是商管
及工業工程管理領域從事作業研究及管理科學學者一年一度的重要會議。距離上次參加此一會議已是
三年多前。此次承蒙國科會補助，得以參加此次國際會議，在此先向國科會表達感謝之意。 
 
會議地點位於美國亞歷桑那州的鳳凰城，此行是我第二次造訪這個位於美國中西部的沙漠城市。由於
會議時間緊湊，此行大部份時間均在會場中學習，並於久未見面的國際友人及過去同窗聯繫感情。此
行亦和清大工工系張國浩教授及台大工商管理學系的郭佳瑋教授共同分擔住宿的費用。 
 
本人的報告為第四天的上午，並擔任 section的主持人。除了我之外，尚有 Dr.Bin Qin 及 Dr. Soo Ho 
Lee分別針對 Text Matching及 Text Mining Model進行報告。其中來自 SAP的 Dr. Qin介紹了一個
巨量資料庫實務上所面臨的問題，並提出數種簡化資料查詢的方法。此類的問題，是我先前從未思考
的主題。當資料筆數數量達到巨量水準後，原先資料分析的模式將因資料搜尋查找的長時間而有所延
遲，因此新式的資料搜尋方法在巨量資料分析將顯得極為重要。我也與 Dr. Qin交換了研究的意見。 
計畫編號 NSC 100－2218－E－011－026－ 
計畫名稱 整合主成份分析與整數規劃法於完整限制資料分群之研究 
出國人員
姓名 
楊朝龍 
服務機構
及職稱 
國立台灣科技大學 工業管理系 
會議時間 
2012年 10月 14日
至 
2012年 10月 17日 
會議地點 
美國 鳳凰城 (Phoenix, AZ) 
會議名稱 
(中文) 2012 作業研究及管理科學年度會議 
(英文) 2012 Institute For Operations Research and the Management Sciences 
(INFORMS) Annual Meeting 
發表題目 
(中文) 整合主成份分析與整數規劃法於限制資料分群問題 
(英文)  Integration of Principal Component Analysis and Integer 
Programming for Constrained Clustering Problem 
 3 
二、與會心得 
此次會議以「Informatics Rising」作為主軸，闡明在巨量（Big Data）時代，作業研究及管理科學
界將扮演極為重要的角色。由於本身的研究主題是資料探勘，此次與會，對自身研究及知識的拓展有
很大的幫助。首先，巨量資料的分析技術與過往結構式的資料分析方式有相當的不同，如何有效快速
地處理非結構式資料如影音串流或社群網路的文字資料，將是巨量分析上的重要研究方向。此外，如
何透過作業研究方法將資料分析結果有效地與商業運作之間作有系統的連結，亦是商管學界極有潛力
的發展主題。基於本身過去在資料探勘上的研究心得，此次會議更加深了我對巨量分析所需的工具及
分析技術有了更進一步的了解，透過開源的（open source）資訊系統 Hadoop平台，在與商管議題
整合的方法論上，尚有相當大的研發空間，可以供國內學者共同發展。 
 
此次與鳳凰城的年度會議亦遇到不少在普渡大學求學時的舊識，彷彿又回到在美求學的時光。不少過
去的同學也都分別於世界各地取得不錯的研究工作或教職，透過此次會議的交流不但聯繫了感情，也
互相分享了在學術工作上的甘苦。大家也相約與來年再次參加此一會議，並互相鼓勵。總結，此次的
會議不但是一次成果豐碩的學術之旅，同時也與國際學者進行交流及聯繫。希望來年能有更好的研究
成果能於國際上分享，並加深台灣在國際的能見度。 
 
三、發表論文全文或摘要 
我的報告針對一個特別的限制型分群模式進行介紹，並把剛完成的國科會計劃部份內容進行分享。會
議中，也得到一些與會人士的意見及討論，均受益良多。 
 
摘要 
This research aims at developing an integrated method to solve a clustering problem 
constructed by Complete Must-Link (CML) constraints. First, the principal component scores 
representing the characteristics of CML are added in the similarity matrix to generate 
candidate clusters. Then, an integer programming is formulated to select the resulting 
clusters from candidate clusters. The optimization can guarantee minimum within-cluster 
distances without losing partition information. 
 
四、建議 
此次參加會議，席間亦與數位在美國留學的台灣學子見面，並一起分享留學的經驗及研究的心得。從
與同學中的對談也得以了解一些目前美國重要學府在工業工程管理領域的新的發展及經營方向。同時
也體認到國內出國深造特別是博士學位的同學似乎在人數上急速地下滑中，此一現象或多或少也反映
了近年來同學出國念書意願的下滑，著實令人感到憂心。在此想向國科會建議，應思考如何鼓勵或資
助國內同學出國留學，以增加國內學子吸收海外研究的歷鍊，並培養未來的研究人才。 
 
此外，相較於對岸、香港及新加坡龐大的參加人數，我國參加此一大型的國際會議的學者人數越來越
少。或許是因為路途遙遠以致於費用及時間成本較高，以致於國內學者對參加歐美的大型會議的意願
相形較低，此一情形將大大地動搖我國在國際的學術地位。在此，建議國科會應於各領域設定標竿會
國科會補助計畫衍生研發成果推廣資料表
日期:2012/11/02
國科會補助計畫
計畫名稱: 整合主成份分析與整數規劃法於完整限制資料分群之研究
計畫主持人: 楊朝龍
計畫編號: 100-2218-E-011-026- 學門領域: 作業研究
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
本研究所發展之資料分群方法,將可進一步應用於海量資料(BIG DATA)分析上,
目前正積極將發展之演算法實作於海量資料環境,並洽談可能產學合作計劃及
技術轉移。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
