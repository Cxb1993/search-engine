Abstract
The demand of new wireless communication systems with much higher data rates
that allow, e.g., mobile wireless broadband Internet connections inspires a quick
advance in wireless transmission technology. So far most systems rely on an approach
where the channel state is measured with the help of regularly transmitted training
sequences. The detection of the transmitted data is then done under the assumption
of perfect knowledge of the channel state. This approach will not be sufficient
anymore for very high data rate systems since the loss of bandwidth due to the
training sequences is too large. Therefore, the research interest on joint estimation
and detection schemes has been increased considerably.
Apart from potentially higher data rates a further advantage of such a system is
that it allows for a fair analysis of the theoretical upper limit, the so-called channel
capacity. “Fair” is used here in the sense that the capacity analysis does not ignore
the estimation part of the system, i.e., it takes into account the need of the receiver
to gain some knowledge about the channel state without restricting it to assume
some particular form (particularly, this approach does also include the approach
with training sequences!). The capacity of such a joint estimation and detection
scheme is often also known as non-coherent capacity.
Recent studies investigating the non-coherent capacity of fading channels have
shown very unexpected results. In stark contrast to the capacity with perfect channel
knowledge at the receiver, it has been shown that non-coherent fading channels
become very power-inefficient at high signal-to-noise ratios (SNR) in the sense that
increasing the transmission rate by an additional bit requires squaring the necessary
SNR. Since transmission in such a regime will be highly inefficient, it is crucial to
better understand this behavior and to be able to give an estimation as to where the
inefficient regime starts. One parameter that provides a good approximation to such
a border between the power-efficient low-SNR and the power-inefficient high-SNR
regime is the so-called fading number which is defined as the second term in the
high-SNR asymptotic expansion of channel capacity.
The results of this report concern this fading number. We restrict ourselves to
fading channels with multiple antennas at the transmitter, but only one antenna
at the receiver (a multiple-input single-output (MISO) situation), however, we do
allow memory. Furthermore, the fading laws are not restricted to be Gaussian, but
is assumed to be a general regular law with spatial and temporal memory. The
main result of this report are a new upper bound and a new lower bound on the
fading number of this MISO fading channel with memory. It can be seen as a
further step towards the final goal of the fading number of general multiple-inputs
multiple-outputs (MIMO) fading channels with memory.
In case of an isotropically distributed fading vector it is proven that the upper
and lower bound coincide, i.e., the general MISO fading number with memory is
known precisely.
The upper and lower bounds show that a type of beam-forming is asymptotically
optimal.
Keywords: Beam-forming, channel capacity, fading, fading number, flat fading
channel, high SNR, joint estimation and detection, memory, MISO, multiple-antenna,
non-coherent detection.
Acknowledgments
Helpful comments from Amos Lapidoth, Daniel Ho¨sli, Tobias Koch, and Natalia
Miliou are gratefully acknowledged. Specially, I would like to thank Amos who
spent a whole day discussing these results with me and Tobias who gave the right
hint to save a serious bug in an earlier version of the proof of the upper bound.
This work was supported by the National Science Council under NSC 94-2218-
E-009-037.
These results have been presented at the 2006 IEEE International Symposium
on Information Theory (ISIT 2006), Seattle, WA, USA. Some preliminary results to
this work have been presented at the 2006 International Wireless Communications
and Mobile Computing Conference (IWCMC 2006), Vancouver, Canada.
1
Particularly, if the channel is fast changing, the estimates will quickly become poor
and the amount of needed training data will be exuberantly large.
A more promising approach is to design a system that uses the received data
carrying the information at the same time for estimating the channel state. Such
a joint estimation and detection approach will be particularly important for future
systems where the required data rates are considerably larger than the rates provided
by present systems (like, e.g., GSM).
A further advantage of such joint estimation and detection systems is that they
allow fair and realistic approximations to the physically feasible data rates. To elab-
orate more on this point, we need to briefly review some basic facts from Information
Theory: in his famous landmark paper “A Mathematical Theory of Communication”
[1] Claude E. Shannon proved that for every communication channel there exists a
maximal rate—denoted capacity—above which one cannot transmit information re-
liably, i.e., the probability of making decoding errors tends to one. On the other
hand for every rate below the capacity it is theoretically possible to design a system
such that the error probability is as small as one wishes. Of course, depending on
the aimed probability of error, the system design will be rather complex and one
will encounter possibly very long delays between the start of the transmission until
the signal can be decoded. Particularly the latter is a large obstacle in real systems,
because most communication systems cannot afford large delays. Nevertheless, the
capacity shows the ultimate limit of communication rate of the available channel
and is therefore fundamental for the understanding of the channel and also for the
judgment of implemented systems regarding their efficiency.
So far the capacity analysis of above mentioned wireless communication channels
were based on the assumption that the receiver has perfect knowledge of the channel
state due to the training sequences. The capacity was then computed without taking
into account the estimation scheme. Such an approach will definitely lead to an
overly optimistic capacity, because
• even with large amount of training data, the channel knowledge will never be
perfect, but only an estimate; and because
• the data rate that is wasted for the training sequences is completely ignored.
The new approach of joint estimation and detection now allows to incorporate
the estimation into the capacity analysis. As a matter of fact, we don’t even need to
make some assumption about how a particular estimation scheme might work, but
can directly try to derive the ultimate data rate that the theoretically best system
could achieve. The capacity of such a system is also known as the non-coherent
capacity of fading channels.
Unfortunately, the evaluation of the non-coherent channel capacity involves an
optimization that is very difficult—if not infeasible—to evaluate analytically or nu-
merically.1 Therefore, the question arises how one could get knowledge about the
ultimate limit of reliable communication over fading channels without having to
solve this infeasible expression.
A promising and interesting approach to this goal is the study of good upper and
lower bounds to channel capacity. However, one needs to be aware that finding upper
bounds to an expression that itself is a maximization might be rather challenging,
too.
1As a matter of fact, this optimization is infeasible for most channels of interest.
3
Some of these results have been very unexpected. E.g., it has been shown in [2]
that regular fading processes have a capacity that grows only double-logarithmically
in the SNR at high SNR. This means that at high power these channels become ex-
tremely power-inefficient in the sense that for every additional bit capacity the SNR
needs to be squared or, respectively, on a dB-scale the SNR needs to be doubled!
This behavior is independent of the particular law of the fading process, the law of
the noise process, or the number of antennas at the transmitter or receiver. More-
over, the capacity-growth at high SNR is double-logarithmic irrespective whether
there is memory in the fading process or not, and it even remains this slow when
introducing noiseless feedback [19]! This is in stark contrast to the situation of ad-
ditive noise channels and even to the so far known capacity results when assuming
prefect knowledge of the channel state at the receiver: there the capacity grows log-
arithmically in the power and the mentioned factors (like, e.g., number of antennas,
memory, or feedback) have a strong (positive) impact on the capacity. For addi-
tive white Gaussian noise (AWGN) channels, e.g., the number of receiver antennas
multiplies the capacity and is therefore very beneficial!
−10 0 10 20 30 40 50 60
0
1
2
3
4
5
6
7
8
9
10
Output-SNR ρ [dB]
C
[n
at
s
p
er
ch
an
n
el
u
se
]
|d| = 0
|d| = 1
|d| = 2
|d| = 4
|d| = 8
|d| = 16
|d| = 32
H = |d|2 + 1
H ∼ NC(0, 1)
H ∼ NC(d, 1)
Figure 1.1: An upper bound on the capacity of a Rician fading channel for different
values of the specular component d. The dotted line depicts the capacity of a
Gaussian channel of equal output-SNR, namely log(1 + ρ).
Therefore the question arises whether in the case of non-coherent fading channels
multiple antennas or feedback is useful at all. It turns out that although the asymp-
totic growth rate of capacity is unchanged by these parameters, they still do have
a large influence on the systems: the threshold above which the capacity growth
changes from logarithmic to double-logarithmic is highly dependent on them! As
an example Figure 1.1 shows the capacity of non-coherent Rayleigh fading channels
with various numbers of receive antennas.
5
This fading number is achievable by inputs that can be expressed as the product of
a constant unit vector in CnT and a circularly symmetric, scalar, complex random
variable of the same law that achieves the memoryless SISO fading number [7].
Hence, the asymptotic capacity of a MISO fading channel is achieved by beam-
forming where the beam-direction is chosen not to maximize the SNR, but the
fading number.
In [15] and [16] Koch & Lapidoth investigate the fading number of MISO fading
channels with memory where the fading is Gaussian. For the case of a mean-d
Gaussian vector process with memory where {Hk −d} is spatially independent and
identically distributed (IID) and where each component is a zero-mean unit-variance
circularly symmetric complex Gaussian process, the fading number is shown to be4
χGauss, spat. IID({H
T
k}) = −1 + log ‖d‖
2 − Ei
(
−‖d‖2
)
+ log
1
ǫ2
, (1.6)
where ǫ2 denotes the prediction error when predicting one of the components of the
fading vector based on the observation of its past.
Furthermore, Koch & Lapidoth derive an upper bound to the fading number for
the general Gaussian case, i.e., {Hk − d} is a zero-mean circularly symmetric sta-
tionary ergodic complex Gaussian process with matrix-valued spectral distribution
function F(·) and with covariance matrix K. Assuming that the prediction error
covariance matrix Σ is non-singular (regularity assumption) they show that
χGauss({H
T
k}) ≤ −1 + log d
2
∗ − Ei
(
−d2∗
)
+ log
‖K‖
λmin
, (1.7)
where
d∗ = max
‖xˆ‖=1
|E[HTk] xˆ|√
Var
(
HTkxˆ
) ; (1.8)
λmin denotes the smallest eigenvalue of Σ; and where ‖ · ‖ denotes the Euclidean
operator norm of matrices, i.e., the largest singular value.
In this report we extend these results to general (not necessarily Gaussian) fading
channels.
The remaining of this report is structured as follows: after some remarks about
notation and a detailed mathematical definition of the channel model in the following
chapter, we will present the main results, i.e., a new upper and lower bound on the
MISO fading number, in Chapter 3. There also some concepts are introduced that
are important in the analysis of channel capacity, e.g., the concept of distributions
that escape to infinity, and relation between stationarity and capacity achieving
input distributions.
We then specialize these results to the case of isotropically distributed fading
processes in Chapter 4.1 and to Gaussian fading in Chapter 4.2. For isotropically
distributed fading we will show that the upper and lower bound coincide. In the
Gaussian case we shall derive the above mentioned results of Koch & Lapidoth as
special cases of our bounds.
The proof of the main result is found in Chapter 5; and we conclude in Chapter 6.
4Note that all results in this paper are in nats.
7
where we reserve this notation exclusively for unit vectors, i.e., throughout the
paper every vector carrying a hat, vˆ or Vˆ, denotes a (deterministic or random,
respectively) vector of unit length
‖vˆ‖ = ‖Vˆ‖ = 1. (2.4)
To be able to work with such direction vectors we shall need a differential entropy-
like quantity for random vectors that take value on the unit sphere in Cm: let λ
denote the area measure on the unit sphere in Cm. If a random vector Vˆ takes value
in the unit sphere and has the density pλ
Vˆ
(vˆ) with respect to λ, then we shall let
hλ(Vˆ) , −E
[
log pλ
Vˆ
(Vˆ)
]
(2.5)
if the expectation is defined.
We note that just as ordinary differential entropy is invariant under translation,
so is hλ(Vˆ) invariant under rotation. That is, if U is a deterministic unitary matrix,
then
hλ(UVˆ) = hλ(Vˆ). (2.6)
Also note that if Vˆ is uniformly distributed on the unit sphere, then hλ(Vˆ) = log cm,
where cm denotes the surface area of the unit sphere in C
m
cm =
2πm
Γ(m)
. (2.7)
The definition (2.5) can be easily extended to conditional entropies: if W is
some random vector, and if conditional on W = w the random vector Vˆ has density
pλ
Vˆ|W
(vˆ|w) then we can define
hλ
(
Vˆ
∣∣W = w) , −E[ log pλ
Vˆ|W
(Vˆ|W)
∣∣∣ W = w] (2.8)
and we can define hλ
(
Vˆ
∣∣W) as the expectation (with respect to W) of hλ(Vˆ ∣∣W =
w
)
.
Based on these definitions we have the following lemma:
Lemma 2. Let V be a complex random vector taking value in Cm and having dif-
ferential entropy h(V). Let ‖V‖ denote its norm and Vˆ denotes its direction as
defined in (2.3). Then
h(V) = h(‖V‖) + hλ
(
Vˆ
∣∣ ‖V‖)+ (2m− 1)E[log ‖V‖] (2.9)
= hλ
(
Vˆ
)
+ h
(
‖V‖
∣∣ Vˆ)+ (2m− 1)E[log ‖V‖] (2.10)
whenever all the quantities in (2.9) and (2.10), respectively, are defined. Here
h(‖V‖) is the differential entropy of ‖V‖ when viewed as a real (scalar) random
variable.
Proof. Omitted.
We shall write X ∼ NC(µ,K) if X − µ is a circularly symmetric zero-mean
Gaussian random vector of covariance matrix E
[
(X− µ)(X− µ)†
]
= K. By X ∼
U ([a, b]) we denote a random variable that is uniformly distributed on the interval
[a, b].
All rates specified in this paper are in nats per channel use, i.e., log(·) denotes
the natural logarithmic function.
9
Prima facie the fading number depends on whether a peak-power constraint (2.16)
or an average-power constraint (2.17) is imposed on the input. Since a peak-power
constraint is more stringent than an average-power constraint, we will derive the
upper bound using the average-power constraint and the lower bound using the
peak-power constraint. In case of an isotropically distributed fading process we
shall see that both constraints lead to identical fading numbers.
11
Lemma 4. Let the memoryless MISO fading channel be given as in (3.1) and let
W (·|·) denote the corresponding conditional channel law. Let {QE}E≥0 be a family
of input distributions satisfying the power constraint (3.2) and the condition
lim
E↑∞
I(QE ,W )
log log E
= 1. (3.4)
Then {QE}E≥0 escapes to infinity.
Proof. A proof can be found in [7], [2].
Hence, when computing bounds on the fading number (which is part of the
capacity in the limit when E tends to infinity, see (2.19)) we may assume that
Pr
[
‖X‖2 ≤ E0
]
= 0. (3.5)
3.1.2 An Upper Bound on Channel Capacity
In [7], [2] a new approach of finding upper bounds to channel capacity has been
introduced. Since capacity is by definition a maximization of mutual information, it
is implicitly difficult to find upper bounds on it. The new proposed technique bases
on a dual expression of mutual information that leads to an expression of capacity
as a minimization instead of a maximization. This way it becomes much easier to
find upper bounds.
Again, here we only state the upper bound in a form needed in the derivation
of Theorem 7, for a more general form, for more mathematical details, and for all
proofs we refer to [7], [2].
Lemma 5. Consider a memoryless channel1 with input alphabet CnR and output
alphabet C as given in (3.1). Then the mutual information between input and output
of the channel is upper-bounded as follows:
I(X;Y ) = −h(Y |X) + log π + α log β + log Γ
(
α,
ν
β
)
+ (1− α)E
[
log
(
|Y |2 + ν
)]
+
1
β
E
[
|Y |2
]
+
ν
β
(3.6)
where α, β > 0 and ν ≥ 0 are parameters that can be chosen freely.
Proof. A proof can be found in [7], [2].
3.1.3 Capacity Achieving Input Distributions and Stationarity
One of the main assumption about our channel model is that the fading process and
the additive noise are stationary. This assumption is crucial both for the results
as well as the derivation, i.e., we don’t believe the results to still be valid in a
non-stationary setting.
From an intuitive point of view a stationary channel model should have a capacity
achieving input distribution that is stationary. Unfortunately, we are not aware of a
rigorous proof of this claim. However, we are able to prove a less strong statement
which is basically saying that for our channel model we may limit ourselves to joint
input distributions under which the input vectors have the same law for (almost) all
time k:
1Actually, the lemma requires some mathematical conditions on the alphabets and the channel
law to be satisfied. However, all these conditions are satisfied in our context. For more detail see
[7], [2].
13
The derivation of this expression is complicated by the fact that Lemma 6 only
guarantees equal marginals away from the edges, i.e., we need to take care of the
edge effects.
In a next step we now upper-bound the first term on the RHS using Lemma 5.
This leads to a rather complicated looking expression with various terms that depend
on the blocklength n, the power E , the free parameters α, β, and ν, and, of course,
on the input distribution. However, interestingly, there are no terms that depend
on the input direction Xˆk and the input amplitude ‖Xk‖ at the same time. We can
therefore separate the expression in a group of terms that depend on E and another
group of terms that do not depend on E .
By an appropriate choice of the free parameters, and by letting n and E (in this
order) go to infinity, we end up with the following bound:
χ({HTk}) ≤ sup
Q
Xˆ
0
−∞
{
log π + E
[
log |HT0Xˆ0|
2
]
− h
(
HT0Xˆ0
∣∣ {HTℓXˆℓ}−1ℓ=−∞, Xˆ0−κ)
}
(3.13)
≤ sup
xˆ0−∞
{
log π + E
[
log |HT0xˆ0|
2
]
− h
(
HT0xˆ0
∣∣ {HTℓ xˆℓ}−1ℓ=−κ)}. (3.14)
Next we state a lower bound to the fading number of a MISO fading channel:
Theorem 8. Consider a MISO fading channel with memory (2.11) where the sta-
tionary and ergodic fading process {Hk} takes value in C
nT and satisfies h({Hk}) >
−∞ and E
[
‖Hk‖
2
]
<∞. Then the fading number χ
(
{HTk}
)
is lower-bounded by
χ
(
{HTk}
)
≥ sup
xˆ
{
log π + E
[
log |HT0xˆ|
2
]
− h
(
HT0xˆ
∣∣ {HTℓ xˆ}−1ℓ=−∞)} (3.15)
where xˆ , x‖x‖ denotes a vector of unit length.
Moreover, this lower bound is achievable by IID inputs that can be expressed as
the product of a constant unit vector xˆ ∈ CnT and a circularly symmetric, scalar,
complex IID random process {Xk} such that
log |Xk|
2 ∼ U ([log log E , log E ]) . (3.16)
Note that this input satisfies the peak-power constraint (2.16) (and therefore also the
average-power constraint (2.17)).
Proof. We give here only an outline of the proof. The details can be found in
Chapter 5.2.
The lower bound is based on the assumption of a specific input distribution
which is chosen to be of the form
Xk = Xk · xˆ (3.17)
where xˆ is a deterministic unit vector (the beam-direction) and where {Xk} is IID
circularly symmetric with
log |Xk|
2 ∼ U ([log log E , log E ]) . (3.18)
Note that this choice for {Xk} achieves the fading number for the SISO fading
channel
Yk = (H
T
kxˆ) ·Xk + Zk (3.19)
15
Chapter 4
Special Cases
Before we discuss the proofs more in detail, we would like to add some insight by
considering some special cases and specializing Theorem 7 and Theorem 8 to these
situations. Firstly, we will analyze a fading process that is isotropically distributed,
and secondly we investigate the in practice very important special case of Gaussian
fading.
4.1 Isotropically Distributed Fading
Let’s consider the special case of isotropically distributed fading processes, i.e., for
every deterministic unitary nT × nT matrix U
Hk
L
= UHk, (4.1)
where we use “
L
=” to denote equal in law.
In this case we have the following corollary:
Corollary 9. Consider a MISO fading channel with memory (2.11) where the sta-
tionary and ergodic fading process {Hk} takes value in C
nT, satisfies h({Hk}) > −∞
and E
[
‖Hk‖
2
]
< ∞, and is isotropically distributed. Then the upper bound (3.11)
and the lower bound (3.15) coincide and the fading number χiso
(
{HTk}
)
is given by
χiso
(
{HTk}
)
= log π + E
[
log |HT0eˆ|
2
]
− h
(
HT0eˆ
∣∣ {HTℓ eˆ}−1ℓ=−∞) (4.2)
where eˆ is some deterministic unit vector.
Proof. This corollary follows immediately from Theorem 7 and 8 by noting that for
every eˆ
HTkeˆ
L
= HTkU
Teˆ = HTkeˆ
′ (4.3)
where the first equality in law follows from (4.1) and the second equality by defining
a new unit vector eˆ′ , UTeˆ. Note that for the MISO case isotropically distributed is
equivalent to rotation commutative in the generalized sense as defined in [7, Defini-
tion 4.37] or [2, Definition 6.37].
4.2 Gaussian Fading
In this section we assume that the fading process {Hk} is a mean-d Gaussian process
such that {H˜k} = {Hk−d} is a zero-mean, circularly symmetric, stationary, ergodic,
complex Gaussian process with matrix-valued spectral distribution function F(·),
and with covariance matrix K. Furthermore, we assume that the prediction error
covariance matrix Σ is non-singular (regularity assumption).
17
Note that the first inequality in general is not tight, i.e., (1.7) is in general looser
than (4.7) which in turn is in general looser than (3.11).
To compute the second term on the RHS of (4.15), we express the fading H0 as
H0 = H¯0 + H˜0 (4.16)
with H¯0 being the best estimate of H0 based on the past realizations. We note that
H˜0 ∼ NC(0,Σ) where Σ denotes the prediction error covariance matrix. Hence
h
(
HT0xˆ0
∣∣H−1−∞) = log (πexˆ†0Σxˆ0) . (4.17)
The bound (4.11) now follows by the Rayleigh-Ritz Theorem [24, Theorem 4.2.2],
[2, Theorem A.9]
λmin = min
xˆ
xˆ†Σxˆ, (4.18)
the definition of the Euclidean norm of matrices, and the properties of positive
semi-definite matrices:
max
xˆ
xˆ†Kxˆ = max
xˆ
xˆ†STSxˆ = max
xˆ
‖Sxˆ‖2 = ‖S‖2 = ‖K‖. (4.19)
4.2.2 Spatially IID Gaussian Fading
We next specialize the assumptions to the case where {H˜k} = {Hk−d} is a spatially
IID process where each component is a zero-mean unit-variance circularly symmetric
complex Gaussian process of spectral distribution function F(·). For this case we
will now present a new derivation of the result (1.6) based on our new bounds.
Note that we cannot apply Corollary 9 here: even though {H˜k} is isotropically
distributed, {Hk} is not due to its mean vector d.
However, the term I
(
HT0xˆ0; {H
T
ℓ xˆℓ}
−1
ℓ=−∞
)
does not depend on the particular
choice of xˆℓ:
I
(
HT0xˆ0; {H
T
ℓ xˆℓ}
−1
ℓ=−∞
)
= I
(
HT0xˆ0 − d
Txˆ0; {H
T
ℓ xˆℓ − d
Txˆℓ}
−1
ℓ=−∞
)
(4.20)
= I
(
H˜T0xˆ0; {H˜
T
ℓ xˆℓ}
−1
ℓ=−∞
)
(4.21)
= I
(
H˜T0eˆ; {H˜
T
ℓ eˆ}
−1
ℓ=−∞
)
(4.22)
= I
(
H
(1)
0 ; {H
(1)
ℓ }
−1
ℓ=−∞
)
(4.23)
= log
1
ǫ2
. (4.24)
Equation (1.6) now follows from (4.10), Theorem 7, and Theorem 8 by noting that
max
‖xˆ‖=1
|E[HTk] xˆ|√
Var
(
HTkxˆ
) = max
‖xˆ‖=1
|dTxˆ| = ‖d‖, (4.25)
where the maximum is achieved for xˆ = d/‖d‖.
19
Here the first equality follows from the chain rule; the subsequent inequality from
the non-negativity of mutual information; the subsequent equality follows because
we prohibit feedback; the subsequent inequality from the inclusion of the additional
random vectors Hk−11 in the mutual information term; (5.9) follows because, con-
ditional on the past fading and the present input, the past inputs and outputs are
independent of the present output Yk; the subsequent equality follows from the chain
rule; the following three steps are analogous to the first steps; and the last inequal-
ity follows once more from the inclusion of additional random vectors in the mutual
information.
We conclude that
C(E) ≤ lim
n↑∞
1
n
I
(
Xn1 ;Y
n
1
)
+ ǫ (5.15)
= lim
n↑∞
1
n− κ− 3(η − 1)
n−2η+2∑
k=η+κ
I
(
Xn1 ;Yk
∣∣Y k−11 )+ ǫ. (5.16)
This allows us to focus on η + κ ≤ k ≤ n − 2(η − 1) which guarantees that
Xk−κ, . . . ,Xk are each distributed according to QE,ǫ.
We now continue by further upper-bounding I
(
Xn1 ;Yk
∣∣Y k−11 ) for such k:
I
(
Xn1 ;Yk
∣∣Y k−11 )
= I
(
Xn1 , Y
k−1
1 ;Yk
)
− I
(
Yk;Y
k−1
1
)
(5.17)
≤ I
(
Xn1 , Y
k−1
1 ;Yk
)
(5.18)
= I
(
Xk−11 , Y
k−1
1 ,Xk;Yk
)
(5.19)
≤ I
(
Xk−11 , Y
k−1
1 ,H
k−κ−1
1 , {H
T
ℓXℓ}
k−1
ℓ=k−κ,Xk;Yk
)
(5.20)
= I
(
Xk−1k−κ,H
k−κ−1
1 , {H
T
ℓXℓ}
k−1
ℓ=k−κ,Xk;Yk
)
(5.21)
= I(Xk;Yk) + I
(
Xk−1k−κ;Yk
∣∣Xk)︸ ︷︷ ︸
=0
+ I
(
{HTℓXℓ}
k−1
ℓ=k−κ;Yk
∣∣Xkk−κ)
+ I
(
Hk−κ−11 ;Yk
∣∣Xkk−κ, {HTℓXℓ}k−1ℓ=k−κ) (5.22)
= I(Xk;Yk) + I
(
{HTℓXℓ}
k−1
ℓ=k−κ;Yk
∣∣Xkk−κ)
+ I
(
Hk−κ−11 ;Yk
∣∣Xkk−κ, {HTℓXℓ}k−1ℓ=k−κ), (5.23)
where the first three steps are identical to (5.5)–(5.7); (5.20) follows from the inclu-
sion of the additional random vectors Hk−κ−11 and the additional random variables
{HTℓXℓ}
k−1
ℓ=k−κ in the mutual information term; the subsequent equality follows be-
cause, conditional on the past terms Hk−κ−11 , {H
T
ℓXℓ}
k−1
ℓ=k−κ, and on the present and
past inputs Xkk−κ, the past outputs Y
k−1
1 and the past inputs X
k−κ−1
1 are inde-
pendent of the present output Yk; the subsequent equality follows from the chain
rule; and the last equality from the independence of the past inputs and the present
output when conditioning on the present input.
We continue by bounding the last term in (5.23):
I
(
Hk−κ−11 ;Yk
∣∣Xkk−κ, {HTℓXℓ}k−1ℓ=k−κ)
= I
(
Hk−κ−11 ;Yk,Xk
∣∣Xk−1k−κ, {HTℓXℓ}k−1ℓ=k−κ)
− I
(
Hk−κ−11 ;Xk
∣∣Xk−1k−κ, {HTℓXℓ}k−1ℓ=k−κ) (5.24)
≤ I
(
Hk−κ−11 ;Yk,Xk
∣∣Xk−1k−κ, {HTℓXℓ}k−1ℓ=k−κ) (5.25)
≤ I
(
Hk−κ−11 ;Yk,Xk,Hk
∣∣Xk−1k−κ, {HTℓXℓ}k−1ℓ=k−κ) (5.26)
21
+ I
(
HT0Xˆk;H
T
−1Xˆk−1, . . . ,H
T
−κXˆk−κ
∣∣ Xˆkk−κ)
)
+ δ(κ) + ǫ (5.41)
= I(X0;H
T
0X0 + Z0) + δ(κ) + ǫ
+ lim
n↑∞
1
n− κ− 3(η − 1)
n−2η+2∑
k=η+κ
I
(
HT0Xˆk;H
T
−1Xˆk−1, . . . ,H
T
−κXˆk−κ
∣∣ Xˆkk−κ).
(5.42)
Here the first equality follows from the stationarity of {Hk, Zk}; and the subsequent
equality follows from the fact that for all k ∈ {η, . . . , n − 2η + 2} the distribution
of Xk is QE,ǫ given in Lemma 6. Note that we have changed notation here: for
notational convenience we will assume from now on that also X0 ∼ QE,ǫ.
We continue to upper-bound the first term I(X0;Y0) under the constraint that
X0 ∼ QE,ǫ. Note that from Lemma 4 we now that QE,ǫ escapes to infinity, i.e.,
Pr
[
‖X0‖2 ≤ Elow
]
= 0 for some Elow ≥ 0.
I(X0;Y0) ≤ I(X0;H
T
0X0 + Z0, Z0) (5.43)
= I(X0;Z0) + I(X0;H
T
0X0 + Z0|Z0) (5.44)
= I(X0;H
T
0X0|Z0) (5.45)
= I(X0;H
T
0X0). (5.46)
We will now upper-bound this term by the bound given in Lemma 5:
I(X0;H
T
0X0) ≤ −hQE,ǫ
(
HT0X0
∣∣X0)+ log π + α log β + log Γ(α, ν/β)
+ (1− α)EQE,ǫ
[
log
(
|HT0X0|
2 + ν
)]
+
1
β
EQE,ǫ
[
|HT0X0|
2
]
+
ν
β
, (5.47)
where α, β > 0, and ν ≥ 0 can be chosen freely. We fix these parameters and assume
0 < α < 1 such that 1− α > 0. Then define
ǫν , sup
‖x‖2≥Elow
{
E
[
log
(
|HT0x|
2 + ν
)]
− E
[
log |HT0x|
2
] }
. (5.48)
Then
(1− α)EQE,ǫ
[
log
(
|HT0X0|
2 + ν
)]
= (1− α)EQE,ǫ
[
log |HT0X0|
2
]
+ (1− α)EQE,ǫ
[
log
(
|HT0X0|
2 + ν
)]
− (1− α)EQE,ǫ
[
log |H0X0|
2
]
(5.49)
≤ (1− α)EQE,ǫ
[
log |HT0X0|
2
]
+ (1− α) sup
‖x‖2≥Elow
{
E
[
log
(
|HT0x|
2 + ν
)]
− E
[
log |H0x|
2
] }
(5.50)
= (1− α)EQE,ǫ
[
log |HT0X0|
2
]
+ (1− α)ǫν (5.51)
≤ (1− α)EQE,ǫ
[
log |HT0X0|
2
]
+ ǫν . (5.52)
Plugging this into (5.47) yields
I(X0;H
T
0X0)
≤ EQE,ǫ
[
E
[
log |HT0X0|
2
∣∣ X0 = x0]− h(HT0X0∣∣X0 = x0)]
+ log π + log Γ(α, ν/β) + ǫν
+ α
(
log β − EQE,ǫ
[
log |HT0X0|
2
] )
+
1
β
EQE,ǫ
[
|HT0X0|
2
]
+
ν
β
(5.53)
23
MISO fading channels with memory:
χ({HTk})
= lim
E↑∞
{
C(E)− log
(
1 + log
(
1 +
E
σ2
))}
(5.62)
≤ lim
E↑∞
{
E
[
log |HT0Xˆ0|
2
]
− h
(
HT0Xˆ0
∣∣Xˆ0)+ log π + ǫν + δ(κ) + ǫ
+ lim
n↑∞
1
n− κ− 3(η − 1)
n−2η+2∑
k=η+κ
I
(
HT0Xˆk;H
T
−1Xˆk−1, . . . ,H
T
−κXˆk−κ
∣∣ Xˆkk−κ)
+ log Γ(α, ν/β) + α
(
log β − log Elow − ξ
)
+
1
β
sup
‖xˆ‖=1
E
[
|HT0xˆ|
2
]
· E +
ν
β
− log
(
1 + log
(
1 +
E
σ2
))}
(5.63)
= E
[
log |HT0Xˆ0|
2
]
− h
(
HT0Xˆ0
∣∣Xˆ0)+ log π + ǫν + δ(κ) + ǫ
+ lim
n↑∞
1
n− κ− 3(η − 1)
n−2η+2∑
k=η+κ
I
(
HT0Xˆk;H
T
−1Xˆk−1, . . . ,H
T
−κXˆk−κ
∣∣ Xˆkk−κ)
+ lim
E↑∞
{
log Γ(α, ν/β)− log
1
α
+ α
(
log β − log Elow − ξ
)
+
1
β
sup
‖xˆ‖=1
E
[
|HT0xˆ|
2
]
· E +
ν
β
+ log
1
α
− log
(
1 + log
(
1 +
E
σ2
))}
(5.64)
= E
[
log |HT0Xˆ0|
2
]
− h
(
HT0Xˆ0
∣∣Xˆ0)+ log π + ǫν + δ(κ) + ǫ
+ lim
n↑∞
1
n− κ− 3(η − 1)
n−2η+2∑
k=η+κ
I
(
HT0Xˆk;H
T
−1Xˆk−1, . . . ,H
T
−κXˆk−κ
∣∣ Xˆkk−κ)
+ log
(
1− e−ν
)
+ ν − log ν. (5.65)
Here, (5.64) follows because, as mentioned above, we may assume that
{Xˆk} ⊥ {‖Xk‖}, (5.66)
and from combining the mutual information and the differential entropy terms; in
the last equation we have made the following choices on the free parameters α and
β:
α , α(E) =
ν
log E + log sup‖xˆ‖=1 E[|H
T
0xˆ|
2]
; (5.67)
β , β(E) =
1
α(E)
eν/α. (5.68)
For this choice note that
lim
E↑∞
{
log Γ(α, ν/β)− log
1
α
}
= log
(
1− e−ν
)
; (5.69)
lim
E↑∞
α
(
log β − log Elow − ξ
)
= ν; (5.70)
25
= sup
Q
Xˆ
0
−κ
with marg. QE,ǫ
EQ
Xˆ
0
−κ
[
log π + E
[
log |HT0Xˆ0|
2
∣∣∣ Xˆ0 = xˆ0]
− h
(
HT0Xˆ0
∣∣ {HTℓXˆℓ}−1ℓ=−κ, Xˆ0−κ = xˆ0−κ)
]
+ δ(κ) + ǫ
(5.80)
≤ sup
xˆ0−κ
{
log π + E
[
log |HT0xˆ0|
2
]
− h
(
HT0xˆ0
∣∣ {HTℓ xˆℓ}−1ℓ=−κ)}+ δ(κ) + ǫ.(5.81)
Here, (5.78) follows since the first couple of terms only depend on the marginal dis-
tribution QE,ǫ which is kept constant for the maximization; the subsequent equality
follows from the definition of mutual information; and in the last inequality the
expectation is upper-bounded by the supremum.
Finally, we let κ go to infinity. The result now follows because ǫ is arbitrary and
because δ(κ) tends to zero for κ→∞.
5.2 Derivation of the Lower Bound of Theorem 8
To derive a lower bound we choose a specific input distribution which naturally
yields a lower bound to channel capacity. Let {Xk} be of the form
Xk = Xk · xˆ (5.82)
where xˆ is a deterministic unit vector (which is therefore known to both the receiver
and transmitter) and where {Xk} is an IID circularly symmetric random process
with
log |Xk|
2 ∼ U
(
[log x2min, log E ]
)
, (5.83)
where we choose x2min as
x2min = log E . (5.84)
Fix some (large) positive integer κ and use the chain rule and the non-negativity of
mutual information to obtain:
1
n
I
(
Xn1 ;Y
n
1
)
=
1
n
n∑
k=1
I
(
Xk;Y
n
1
∣∣Xk−11 ) (5.85)
≥
1
n
n−κ∑
k=κ+1
I
(
Xk;Y
n
1
∣∣Xk−11 ). (5.86)
Then for every κ+1 ≤ k ≤ n−κ, we can use the fact that {Xk} is IID and circularly
symmetric to lower-bound I
(
Xk;Y
n
1
∣∣Xk−11 ) as follows:
I
(
Xk;Y
n
1
∣∣Xk−11 )
= I
(
Xkxˆ;Y
n
1
∣∣ {Xℓxˆ}k−1ℓ=1 ) (5.87)
= I
(
Xk;Y
n
1
∣∣Xk−11 ) (5.88)
= I
(
Xk;X
k−1
1 , Y
n
1
)
(5.89)
≥ I
(
Xk;X
k−1
k−κ, Y
k−1
k−κ , Yk
)
(5.90)
= I
(
Xk;X
k−1
k−κ, Y
k−1
k−κ , Z
k−1
k−κ, Yk
)
− I
(
Xk;Z
k−1
k−κ
∣∣Xk−1k−κ, Y k−1k−κ , Yk)︸ ︷︷ ︸
≤ǫ(xmin,κ)
(5.91)
27
Chapter 6
Discussion & Conclusion
We have derived a new upper bound and a new lower bound on the fading number
of a MISO fading channel of general law including memory. The fading number is
the second term in the asymptotic expansion of channel capacity, i.e., the fading
number basically determines the capacity in the limit of infinite power.
The bounds are not identical, however, both bounds show the same structure
involving the maximization of a deterministic beam-direction xˆ, which suggests that
beam-forming is optimal at high SNR. Be aware, however, that the beam-direction
is not chosen to maximize the SNR, but to maximize the fading number.
The differences between the upper and lower bound lies in the details of the
maximization: while in the lower bound one single direction unit vector xˆ is chosen
for all time, the upper bound allows for different realizations of xˆk for different times
k.
We are convinced that the lower bound is actually tight: intuition tells that for
our stationary channel model a stationary input should be sufficient for achieving
the capacity. As a matter of fact in the SISO and SIMO case it has been shown
that actually an IID input suffices to achieve capacity at high SNR [7], [2], [3].
Furthermore, in the derivation of the upper bound we use obviously loose bounds
on several places.
In the case of isotropically distributed fading the particular choice of direction
has no influence on the fading process and therefore the upper and lower bounds
coincide. Hence, we are able to specify the fading number of isotropically distributed
MISO fading processes precisely.
In the important special case of Gaussian fading we could show that the bounds
presented in [15] and [16] are special cases of the new bounds presented here, where
the new upper bound (3.11) is in general tighter than (1.7).
Note that in the derivation of the upper bound it is tempting to use the known re-
sults about memoryless MISO fading (e.g., in (5.42)). Unfortunately, this approach
fails because the memoryless MISO fading number involves a supremum over the
memoryless terms which afterward can not be incorporated anymore into a supre-
mum over all terms. Another, so far unsuccessful attempt, has been to reduce the
problem to a memoryless SISO situation. This leads to additional terms that take
into account the direction of the MISO input, however, we have not been able to
manipulate these terms such as to keep the upper bound tight.
We also would like to emphasize the importance of the preliminary results of
Section 3.1, particularly, the concept of distributions that escape to infinity and the
lemma about the stationarity of the capacity achieving input distribution. They
give some additional information about the capacity achieving input distribution
29
Appendix A
Proof of Lemma 6
The proof follows the same lines as the proofs of [3, Lemma 5] and [2, Lemma B.1].
The proof is by a simple shift-and-mix argument. Recalling that
C(E) = lim
n↑∞
1
n
sup I(X1, . . . ,Xn;Y1, . . . , Yn) (A.1)
where the supremum is over all joint distributions on (X1, . . . ,Xn) ∈ C
nT×n under
which
∑n
k=1 E
[
‖Xk‖
2
]
= nE , we conclude that there must exist some integer η ≥ 1
and some joint distribution p∗ ∈ P(CnT×η) such that if (X1, . . . ,Xη) ∼ p
∗ then
1
η
η∑
ℓ=1
E
[
‖Xℓ‖
2
]
= E (A.2)
and
1
η
I(X1, . . . ,Xη;Y1, . . . , Yη) > C(E)−
ǫ
2
. (A.3)
Let Q be the probability law on CnT that is the mixture of the η different
marginals of p∗. That is, for every Borel set B ⊂ CnT
Q(B) =
1
η
η∑
ℓ=1
p∗(Xℓ ∈ B). (A.4)
By (A.2) we have ∫
CnT
‖x‖2 dQ(x) = E . (A.5)
Let n now be given. We shall next describe the required input distribution as
follows. Let
ν =
⌊
n− η + 1
η
⌋
(A.6)
and let the infinite sequence X˜ of random nT-vectors be defined by
X˜ = (0, . . . ,0︸ ︷︷ ︸
η−1
,Ξ
(1)
1 , . . . ,Ξ
(1)
η︸ ︷︷ ︸
η
, . . . , . . . ,Ξ
(ν)
1 , . . . ,Ξ
(ν)
η︸ ︷︷ ︸
η
,0,0, . . . (A.7)
so that
X˜ℓ =


0 if 1 ≤ ℓ ≤ η − 1,
Ξ
⌊ℓ/η⌋
(ℓ mod η)+1 if η ≤ ℓ ≤ (ν + 1)η − 1,
0 if ℓ ≥ (ν + 1)η,
(A.8)
31
Appendix B
Additional Derivation for the
Proof of the Lower Bound
In the derivation of the lower bound to the fading number we need to find the
following upper bound
I
(
Xk;Z
k−1
k−κ
∣∣Xk−1k−κY k−1k−κ , Yk) ≤ ǫ(xmin, κ) (B.1)
and to show that ǫ(xmin, κ) only depends on xmin and κ and tends to zero as xmin
tends to infinity.
To that goal we bound as follows:
I
(
Xk;Z
k−1
k−κ
∣∣Xk−1k−κ, Y k−1k−κ , Yk)
= h
(
Zk−1k−κ
∣∣Xk−1k−κ, Y k−1k−κ , Yk)− h(Zk−1k−κ ∣∣Xk−1k−κ, Y k−1k−κ , Xk, Yk) (B.2)
≤ h
(
Zk−1k−κ
)
− h
(
Zk−1k−κ
∣∣Xk−1k−κ, Y k−1k−κ , Xk, Yk, Zk) (B.3)
= h
(
Zk−1k−κ
)
− h
(
Zk−1k−κ
∣∣Xk−1k−κ, Y k−1k−κ ,HTkxˆ) (B.4)
≤ h
(
Zk−1k−κ
)
− inf
|xk−κ|≥xmin,...,
|xk−1|≥xmin
h
(
Zk−1k−κ
∣∣∣ {HTℓ xˆ · xℓ + Zℓ}k−1ℓ=k−κ,HTkxˆ) (B.5)
= h
(
Zk−1k−κ
)
− h
(
Zk−1k−κ
∣∣ {HTℓ xˆ · xmin + Zℓ}k−1ℓ=k−κ,HTkxˆ) (B.6)
= I
(
Zk−1k−κ;
{
HTℓ xˆ +
Zℓ
xmin
}k−1
ℓ=k−κ
,HTkxˆ
)
(B.7)
= I
(
Zκ1 ;
{
HTℓ xˆ +
Zℓ
xmin
}κ
ℓ=1
,HTκ+1xˆ
)
(B.8)
= I
({
Zℓ
xmin
}κ
ℓ=1
;
{
HTℓ xˆ +
Zℓ
xmin
}κ
ℓ=1
∣∣∣∣ HTκ+1xˆ
)
(B.9)
= h
({
HTℓ xˆ +
Zℓ
xmin
}κ
ℓ=1
∣∣∣∣ HTκ+1xˆ
)
− h
(
{HTℓ xˆ}
κ
ℓ=1
∣∣HTκ+1xˆ) (B.10)
, ǫ(xmin, κ). (B.11)
Here (B.3) follows from conditioning that reduces entropy; in the subsequent equality
we use Xk and Zk in order to extract H
T
kxˆ from Yk, and then we drop (Xk, Yk, Zk)
since given HTkxˆ it is independent of the other random variables; and (B.8) follows
from stationarity.
From [7, Lemma 6.11], [2, Lemma A.19] we conclude that for every realization
33
Bibliography
[1] C. E. Shannon, “A mathematical theory of communication,” Bell System Tech-
nical Journal, vol. 27, pp. 379–423 and 623–656, July and October 1948.
[2] S. M. Moser, “Duality-based bounds on channel capacity,” Ph.D. dissertation,
Swiss Federal Institute of Technology, Zurich, October 2004, Diss. ETH
No. 15769. [Online]. Available: http://moser.cm.nctu.edu.tw/
[3] A. Lapidoth and S. M. Moser, “The fading number of single-input multiple-
output fading channels with memory,” IEEE Transactions on Information The-
ory, vol. 52, no. 2, pp. 437–453, February 2006.
[4] ——, “The fading number of SIMO fading channels with memory,” in Proceed-
ings IEEE International Symposium on Information Theory and its Applica-
tions (ISITA), Parma, Italy, October 10–13, 2004, pp. 287–292.
[5] ——, “Feedback increases neither the fading number nor the pre-log,” in Pro-
ceedings Twenty-Third IEEE Convention of Electrical & Electronics Engineers
in Israel (IEEEI), Herzlia, Israel, September 6–7, 2004, pp. 213–215.
[6] ——, “Bounds on the capacity of the discrete-time Poisson channel,” in Proceed-
ings Forty-First Allerton Conference on Communication, Control and Comput-
ing, Allerton House, Monticello, Illinois, October 1–3, 2003, pp. 201–210.
[7] ——, “Capacity bounds via duality with applications to multiple-antenna sys-
tems on flat fading channels,” IEEE Transactions on Information Theory,
vol. 49, no. 10, pp. 2426–2467, October 2003.
[8] ——, “The asymptotic capacity of the discrete-time Poisson channel,” in Pro-
ceedings Winter School on Coding and Information Theory, Monte Verita`, As-
cona, Switzerland, February 24–27, 2003.
[9] ——, “Capacity bounds via duality with applications to multi-antenna systems
on flat fading channels,” Signal and Information Processing Laboratory, ETH
Zurich, Tech. Rep., June 25, 2002, preprint.
[10] ——, “On the fading number of multi-antenna systems over flat fading channels
with memory and incomplete side information,” in Proceedings IEEE Interna-
tional Symposium on Information Theory (ISIT), Lausanne, Switzerland, June
30 – July 5, 2002, p. 478.
[11] ——, “On the fading number of multi-antenna systems,” in Proceedings IEEE
Information Theory Workshop (ITW), Cairns, Australia, September 2–7, 2001,
pp. 110–111.
35
