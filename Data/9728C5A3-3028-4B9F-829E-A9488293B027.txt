可供推廣之研發成果資料表 
□ 可申請專利  □ 可技術移轉                                      日期：97 年 10 月 15 日 
國科會補助計畫 
計畫名稱：運用漸進式記憶投射法以及目標分析求解附有一般上限
限制式之多維裝袋問題(I) 
計畫主持人： 李際偉助理教授        
計畫編號：NSC 96-2221-E-259-004             
學門領域：工業工程與管理學門(OR/決策科學) 
技術/創作名稱  
發明人/創作人  
中文：此專案發展一個漸進式記憶投射法(adaptive memory 
projection) 來求解附有一般上限限制式之多維袋裝問題
(GUBMKP)，此問題為多維袋裝問題的特殊類型，並且是屬於NP-hard
問題。在此問題中，變數被分派至多重選擇的集合中，且每一個多
重選擇的集合中，最多有一個變數被選擇為 1。GUBMKP 可被應用在
貨運裝載、資源分派與專案分配等很多實際問題上。由於 GUBMKP
是 NP-hard 問題，當求解問題規模較大例子時，使用萬用啟發式演
算法在計算上是較有優勢的。投射法即固定所選擇子集合中的變
數，並同時操控其他的變數；投射法在萬用啟發式演算法中是非常
有用的法則，特別是在大規模的最佳化，此研究結合了啟發式演算
法以及最佳化軟體。在本專案所發展的方法當中，也使用了假切面
(pseudo-cut) 的不等式(pseudo-cut inequality)與多樣化門檻
程序(diversity threshold procedure)來強化和多樣化此演算法。
技術說明 
英文：This project develops an adaptive memory projection 
(AMP) method for the multidimensional knapsack problem with 
generalized upper bound constraints (referred as GUBMKP).The 
GUBMKP can be seen as a special case of the multidimensional 
knapsack problem, a well known NP-hard problem. In the 
GUBMKP, variables are partitioned into several 
multiple-choice sets and at most one variable can be chosen 
from each set. The GUBMKP formulation can be applied to many 
real-world problems such as cargo loading, resource 
allocation and project selection. Because the GUBMKP is also 
NP-hard, approaches of metaheuristics can have an important 
computational advantage for large problem instances. 
Projection methods, which hold a selected subset of variables 
fixed while manipulating others, have a very useful rule in 
metaheuristics, especially in large scale optimization. 
Within the context of the GUBMKP, we show that 
intensification and diversification processes for the AMP 
can be supported in many ways including the use of  
pseudo-cut inequalities and diversity threshold procedures.
可利用之產業 
及 
可開發之產品 
 
附件二 
NSC Project Technical Report
Solving Multidimensional Knapsack Problem
with Generalized Upper Bound Constraints by
the Adaptive Memory Projection Method∗
Vincent C. Li1,† Yun-Chia Liang2,‡ Huan-Fu Chang2,§
1Institute of Global Operations Strategy and Logistics Management
National Dong Hwa University
Hualien, Taiwan
2Department of Industrial Engineering and Management
Yuan Ze University
Chung-Li, Taiwan
Abstract This project develops an adaptive memory projection (AMP) method for the multidi-
mensional knapsack problem with generalized upper bound constraints (referred as GUBMKP).
The GUBMKP can be seen as a special case of the multidimensional knapsack problem, a well-
known NP-hard problem. In the GUBMKP, variables are partitioned into several multiple-choice
sets and at most one variable can be chosen from each set. The GUBMKP formulation can be ap-
plied to many real-world problems such as cargo loading, resource allocation and project selection.
Because the GUBMKP is also NP-hard, approaches of metaheuristics can have an important com-
putational advantage for large problem instances. Projection methods, which hold a selected subset
of variables fixed while manipulating others, have a very useful rule in metaheuristics, especially in
large scale optimization. Within the context of the GUBMKP, we show that intensification and di-
versification processes for the AMP can be supported in many ways including the use of pseudo-cut
inequalities and diversity threshold procedures.
Keywords adaptive memory projection, multidimensional knapsack problem, critical event tabu
search, GUBMKP
1 Introduction
In this research project, we consider the multidimensional knapsack problem with
generalized upper bound (GUB) constraints as stated in Li and Curry (2005) : A set of
n items (N = {1, · · · ,n}) should be packed to maximize the overall profit (item j can
contribute c j) such that the capacity limitation (bi, i ∈ M = {1, · · · ,m}) of each of the m
∗Supported by National Science Council, Taiwan: 96-2221-E-259-004
†vli@mail.ndhu.edu.tw
‡ycliang@saturn.yzu.edu.tw
§s965402@mail.yzu.edu.tw
resulting procedures can be embedded in progressive improvement method as well as
constructive multi-start methods.
In optimization, an old and recurring idea is to generate solutions by iteratively fixing
selected variables while changing the values of others. Simplex method is a straightfor-
ward application where only one variable, denoted as a nonbasic variable, is allowed to
change at a time, to identify a move between adjacent vertices of the feasible region. This
idea also applies to the area of cutting plane methods for integer programming. A com-
mon strategy of lifting of cutting plane methods in integer programming is to successively
assign coefficients of variables that are free, given the values presently assigned to fixed
coefficients, until a complete cutting plane is generated. This underlying concept dwells
in the heart of projection mapping, which are generally used in nonlinear programming
and mixed integer programming.
The scope of metaheuristics furnishes an opportunity to apply this idea in a promising
way. There are a lot of complex optimization problems where exact methods often fail.
How to iteratively choose fixed and free variables, and manage the process of choosing
values for the set of free variables, become very challenging on levels not encountered in
more classical settings.
Three main concepts of tabu search highlights the starting point for AMP methods as
illustrated in Glover (2005) .
1. Strongly determined and consistent variables: The idea of this concept is to keep
track of variables that obtain particular value assignments within some frequency
of intervals over high-quality solutions. This concept also applies to keeping track
of variables that will affect the structure of solutions significantly should their val-
ues change. As expressed in Glover (1977), these variables are likely to receive
their preferred values in high-quality solutions including optimal and near-optimal
solutions. Restricting these variables to their preferred values creates a “combina-
torial implosion” effect that speeds up the solution of the remaining problem. Once
selected variables are constrained to some values, other variables can also become
strongly determined and consistent, which allows them to be exploited in a simi-
lar scheme. It involves the following steps to discover and take advantage of such
variables.
• Segregate sets of good solutions over which the variables and their assign-
ments are recognized.
• Apply measures of frequency to determine which variables qualify as consis-
tent.
• Apply criteria of change to determine which variables qualify as strongly de-
termined.
2. The trade-offs between intensification and diversification: Strongly determined
and consistent variables were originally proposed for application in an intensifi-
cation setting, but the tabu search emphasis on balancing the intensification and
diversification reveals the possibility of augmenting the use of these variables by
periodically putting some constraints to drive the solutions into new regions. In the
constraint programming community, the idea of generating solutions by iteratively
holding some variables fixed while varying others, is also known as “Large Neigh-
borhood Search” (Shaw, 1998). Not too long ago, the theme of identifying and
rule used in the heuristic is to allow a solution with an improved objective function
value.
With this frame work, four subsets of variables are collected described as follows.
1. Maintain a list of α (set to 20) solutions with best objective function values.
After β (set to 50 and 100 in our implementation) iterations (each iteration
includes a constructive phase and a destructive phase), variables with high fre-
quency in this subset are considered as consistent variables1 For convenience,
let’s denote this subset as VC.
2. Keep a record of all the critical solutions. Variables with high frequency in the
set of critical solutions are treated as strongly determined variables (Denote
this subset as VSD1 ). On the other hand, if a variable has ever been added or
dropped in the search process, it can also be considered as a strongly deter-
mined variable (Denote this subset as VSD2 ). However, variables in VSD2 have
a lower priority of being chosen as free variables in comparison to variables
in VSD1 .
3. For each critical-event move, we maintain a list of variables with high evalu-
ation (i.e., top three choices) but not chosen eventually. This set of variables
can be considered as conditionally attractive candidates (denoted as VA1 ). In-
cidentally, we can also maintain a list of variables with high evaluation (i.e.,
top three choices) but not chosen eventually for each move (including critical
events). Variables with high frequency in this subset can be considered as
persistently attractive candidates (denoted as VA2 ). When choosing free vari-
ables, variables in VA2 have a lower priority of being chosen as free variables
in comparison to variables in VA1 .
Step 3 (Referent-Optimization Phase) In Step 2, several subsets of variables are collected
(It is possible some variables will appear in more than one subset). Variables in
these subsets are candidates for free variables of the referent-optimization. There
are many possibilities to exploit these subsets of variables. In short, they are the
following:
1. Choose a set of consistent variables (i.e., VC).
2. Choose a set of strongly determined variables (i.e., VSD1 and VSD2 ). Variables
identified from critical events have a higher priority than variables identified
from ordinary moves (add, drop or swap), that is, VSD1 Â (preferred to) VSD2 .
3. Choose a set of conditionally attractive variables or persistent attractive vari-
ables (i.e., VA1 and VA2 ). Again, variables identified from critical events have
a higher priority than variables identified from ordinary moves, that is, VA1 Â
VA2 .
4. Use hybrid methods as shown in Table 1 to combine the previous three ap-
proaches. Suppose we choose r (set to 60) variables totally as the free vari-
ables. Among these r variables, we choose r1%, r2%, and r3% of the free
variables from approaches 1, 2, and 3, respectively, where r1+ r2+ r3 = 100.
1The frequency will be updated whenever a qualified solution replaces the worst solution (with lowest ob-
jective function value) in the subset
GUB sets. The number of knapsack constraints of Problems 1, 2, and 3 are 10, 20 and
30, respectively. The number of GUB sets of Problems 1, 2, and 3 are 20, 40 and 60,
respectively. The RHS values of knapsack constraints for each of these three problems
are all fixed at 5, which is a pretty tight restriction.
Problem 1 has approximately 4,000 variables. Ten problem instances with 10% to
100% in increments of 10% of the original available variables are varied for each instance
and are referred to by the set
{P1a, P1b, · · · , P1j}
where a stands for 10%, b stands for 20%, etc. Similarly, the ten instances of Problem 2
(originally with approximately 8,000 variables) and Problem 3 (originally with approxi-
mately 15,000 variables) are referred to by the sets
{P2a, P2b, · · · , P2j} and {P3a, P3b, · · · , P3j},
respectively. Variables of these instances are chosen randomly from the original problems
(Problems 1, 2, and 3) according to the desired percentages (10%, 20%,· · · ,100%). The
largest instance of each problem is identical to the original problem. In other words, P1j,
P2j, P3j are the same as Problems 1, 2, and 3, respectively. We use P1a, P1c, P1e, P1g,
P1i for the preliminary run.
For each problem instance, there are 12 different runs. The computational results
are shown in Figure 1. The first column labeled "Run serial number" shows the index
of a different run setting for a particular problem instance. The second column labeled
"Number of iterations in each launch" indicates the number of complete critical event
tabu search iterations for each execution of Step 2 or Step 4. The third column labeled
"Number of relaunches" shows how many relaunches conducted in the run. Column 4
labeled "Free variable type" indicate what the criteria are for selecting free variables. Two
versions are implemented for selecting free variables. The first version collects strongly
determined variables while the second version collects consistent variables. Columns 6
to 10 show the results for each problem instances under each different run serial number.
Some observations based on running Problem Instance P1j are as follows:
• The objective value of each launch may depend on the length of each launch. Fig-
ure 2 shows the best solution found in each of the 50-iteration runs is better than the
one found in each of the 100-iteration counterpart. For example, the best solution
found in the 1st run is better than the one found in the 7th run. The only difference
of these two runs is the number of iterations in each relaunch. It will be intriguing
is we can find a rule that tells how to determine the number of iterations in each
relaunch.
• The choices of free variables may affect the results. Figure 3 shows choosing
strongly determined variables performs better than choosing the consistent vari-
ables when running more iterations in each relaunch. However, it is not sufficient
for us to make a conclusion based on this particular instance. Further experiments
need to be conducted to survey the impact of choices of free variables.
Compare different search iteration times
175000
185000
195000
1 2 3 4 5 6
replication
O
B
J
 
v
a
l
u
e
50
100
 
 
Figure 2: The Best Solution under Different Number of Iterations in Each Relaunch
4 Conclusions and Future Study
In this project, we have designed an adaptive memory projection method within a tabu
search framework for the GUBMKP. Preliminary runs are conducted to test the impacts
by the choice of free variables, the number of iterations in each relaunch, and the number
of relaunches. Different settings are tested for five problem instances based on the data
set in Li and Curry (2005).
As we mentioned in this report, there are other ways of combining different kinds
of variables when selecting free variables. We are going to implement each of them
and make a comparison among different choices. On the other hand, we will implement
diversity threshold procedures as illustrated in Glover (2005) to see if further improvement
can be made. Last but not least, it might be interesting to try different heuristic. Instead of
using critical event tabu search as the heuristic, other methods can be experimented. One
of which is to use swap mechanism to control the progress of heuristic. These will be the
focuses to extend this project in the future.
References
[1] R. K. Ahuja, O. Ergun, J. B. Orlin, and A. P. Punnen. Survey of very large-scale
neighborhood search techniques. Discrete Applied Mathematics, 123(1-3):75–102,
2002.
[2] Emilie Danna, Edward Rothberg, and Claude Le Pape. Exploring relaxation induced
neighborhoodsto improve mip solutions. Mathematical Programming, 2004.
[3] F. Glover. Heuristics for integer programming using surrogate constraints. Decision
Sciences, 8:156–166, 1977.
[4] F. Glover. Multi-start and strategic oscillation methods–principles to exploit adap-
tive memory. In M. Laguna and J. L. Gonzales Velarde, editors, Computing Tools
for Modeling, Optimization and Simulation: Interfaces in Computer Science and
Operations Research, pages 1–24. Kluwer Academic Publishers, Boston, 2000.
[5] F. Glover. Adaptive memory projection methods for integer programming. In
C. Rego and B. Alidaee, editors, Metaheuristic Optimization via Memory and Evolu-
tion: Tabu Search and Scatter Search, pages 425–440. Kluwer Academic Publishers,
Boston, 2005.
[6] F. Glover and G.A. Kochenberger. Critical event tabu search for multidimensional
knapsack problems. In I. H. Osman and J. P. Kelly, editors, Metaheuristics: Theory
and Applications, pages 407–427. Kluwer Academic Publishers, Boston, 1996.
[7] S. Hanafi and A. Freville. An efficient tabu search approach for the 0-1 multidi-
mensional knapsack problem. European Journal of Operational Research, 106(2–
3):659–675, 1998.
[8] C. S. Hiremath and R. R. Hill. New greedy heuristics for the multiple-choice multi-
dimensional knapsack problem. International Journal of Operational Research,
2(4):495–512, 2007.
[9] V.C. Li and G.L. Curry. Solving multidimensional knapsack problems with gen-
eralized upper bound constraints using critical event tabu search. Computers and
Operations Research., 32(4):825–848, 2005.
[10] S. Martello and P. Toth. Knapsack Problems. Wiley, New York, 1990.
[11] P. Shaw. Using constraint programming and local search methods to solve vehicle
routing problems. In M. Maher and J. F. Puget, editors, Proceedings of CP’98, pages
417–431. Springer-Verlag, 1998.
