 i 
 
目錄  
目錄  ................................................................................................... i 
1. 前言  .............................................................................................. 1 
2. 研究目的  ....................................................................................... 2 
3. 文獻探討  ....................................................................................... 2 
3.1 NetFPGA 網路開發平台  ........................................................... 2 
3.1.1 NetFPGA 硬體資源與軟硬體架構  ..................................... 3 
3.1.2 NetFPGA 設計架構  .......................................................... 4 
3.1.3 NetFPGA 資料格式  .......................................................... 6 
3.1.4 NetFPGA 開發流程  .......................................................... 7 
3.2 NetFPGA 專案 - OpenFlow Switch .............................................. 8 
3.2.1 OpenFlow Switch 架構 ...................................................... 8 
3.2.2 OpenFlow Switch 功能定義  ............................................... 9 
3.2.3 OpenFlow Switch 在 NetFPGA 上的實現  ............................ 9 
3.3 FlowVisor ...............................................................................13 
4. 研究方法  ......................................................................................17 
4.1 封包交換 /路由模組 (Switching/Routing Module) ........................18 
4.1.1 Network Virtualizable Switch ...........................................18 
4.1.2 Controller .......................................................................19 
4.2 QoS 佇列模組 (QoS Queue).......................................................23 
4.2.1 流量調整機制  ................................................................23 
4.2.2 流量排程機制  ................................................................25 
4.2.3 流量管理功能的實現  .....................................................25 
4.2.4 流量管理模組功能架構  ..................................................27 
4.2.5 硬體模組的整合  .............................................................29 
5. 結果與討論  ..................................................................................33 
5.1 封包交換 /路由模組效能改進測試  ...........................................33 
5.1.1 相同表頭封包延遲的比較  ...............................................34 
5.1.2 不同封包表頭延遲的比較  ...............................................35 
5.1.3 頻寬的比較  ....................................................................36 
5.2 QoS 佇列模組功能測試  ...........................................................38 
5.2.1 TCP 混和連線流量控制  ...................................................38 
5.2.2 UDP 混和連線流量控制  ..................................................41 
5.2.3 流量控制與優先權佇列驗證  ............................................45 
5.3 結論  ......................................................................................50 
6. 參考文獻  ..................................................................................51 
 2 
 
欄位，依此辨別出不同的連線進而決定其處理動作，但當多個連線共用
處理路徑時，彼此會造成競爭而互相影響，另外在對一般流量的處理速
度上，可能因為所有流量都需經過 Controller 判斷而產生過大延遲，因此
子計畫一改良了 NetFPGA OpenFlow Switch 的硬體封包處理方式，加入
了其對 Forwarding Table 的支援及流量管理模組的實現，利用 NetFPGA
硬體平台以實作一具有網路虛擬化之高速封包處理平台。  
2. 研究目的  
子計畫一：「設計與實作一具有網路虛擬化之高速封包處理平台」，
子計畫一預定於 NetFPGA以硬體描述語言 - Verilog設計一個高速網路封
包處理系統，進行封包擷取 (capture)、過濾 (filter)、分類 (classification)
及轉送 (forwarding)的功能，有效的對不當的網路應用進行監控管理，並
利用 NetFPGA 在現有真實網路上建立一個實驗平台，藉由真實的網路流
量來驗證新的協定及應用，另外為配合子計畫三的 QoS 資源管控也實現
了流量管理模組，提供使用者對相同處理路徑不同連線的資源管控。子
計畫一的重要性在於能夠改善目前網路的效能、扼阻非法應用、以及提
升網路服務品質，並能夠在原有工作網路的基礎上建立實驗網路，而不
需另外建立實驗網路平台，以減少佈建成本，讓新的網路概念或服務能
夠於實際網路上獲得實體的驗證。  
3. 文獻探討  
3.1 NetFPGA網路開發平台  
NetFPGA[1-3]是由美國史丹佛大學研究團隊所提出的一個網路開發
平台，如圖 3.1 所示，其主要目的是讓學生和研究者能在線速 (line-rate)
的環境下，實驗新的封包處理研究，舉例來說，學生可以學習實作一個
支援億兆 (Gigabit)網路通訊埠的網路交換器 [4]，而研究人員也可嘗試實
驗一個加入新功能的網路路由器在 NetFPGA 上 [5]，封包可以依照設計者
的需求隨意的被處理。  
NetFPGA 使用者撰寫 Verilog 程式碼，經過合成、驗證，然後下載到
可程式化的硬體中來運行，由於執行的速度可達到線速 (line-rate)，因此
實作出來的產品可以在實際的環境中使用和測試，目前美國許多大學都
有針對 NetFPGA 開發教授相關課程，全球的使用人數也在逐漸增加中，
新一代的 NetFPGA(支援 10G)也已經被開發出來而開始上市。  
 
 4 
 
NetFPGA 利用 PCI 介面與主機溝通，資料的傳遞和控制命令的下達
分別使用兩種不同的方式，資料部分主要是透過直接記憶體存取 (Direct 
Memory Access, DMA)，藉此讓 NetFPGA和主機端可以快速地交換大量
資料而不需要中央處理單元參與。控制命令則是透過輸入輸出控制 (input 
output control, ioctl)的方式來存取 NetFPGA 上的暫存器，藉此讓上層軟
體得以除錯及設定硬體，其軟硬體架構如圖 3.3 所示。  
 
Linux kernel
NetFPGA driver
User data path
Ethernet
CPU
RxQ
CPU
TxQ
MAC
RxQ
MAC
TxQ
MAC
RxQ
MAC
TxQ
MAC
RxQ
MAC
TxQ
MAC
RxQ
MAC
TxQ
CPU
RxQ
CPU
TxQ
CPU
RxQ
CPU
TxQ
CPU
RxQ
CPU
TxQ
nf2 reg grp
DMA registers
nf2c0 nf2c1 nf2c2 nf2c3 ioctl
Routing protocol
Spanning Tree 
Algorithm
Java GUI
 
圖 3.3 NetFPGA 軟硬體架構  
 
3.1.2 NetFPGA 設計架構  
NetFPGA 採用模組化與管線化 (pipeline)的設計方式 [6]，如圖 3.4 所
示，不同的處理功能分別由不同的模組所負責，而在各級模組間有一同
步訊號線，當第 i+1級模組告知第 i級模組它有空間可以存放處理資料時，
第 i 級模組便會將已完成處理的資料交由第 i+1 級模組做處理，NetFPGA
藉由管線化的處理方式來達到硬體效能的提升，而藉由模組化來達到各
個不同功能模組的再利用性，使得開發人員對新專案的開發負擔程度能
降到最低。  
 
 6 
 
管線化的第一級包含了由 PCI DMA 介面 (CPU RxQ)和乙太網路介面
孔 (MAC RxQ)接收資料的接收佇列，然後 User Data Path 中的 Input 
Arbiter 會藉由輪詢 (Round Robin)來選擇要服務的接收佇列，將資料交給
下一級–  Output Port Lookup 來進行處理，Output Port Lookup 主要會對
資料做處理並決定資料的流向，將資料放到相對應的輸出佇列 (Output 
Queues)，當相對應的傳輸佇列有空間可以接收輸出資料時，資料便由輸
出佇列被取出，進入傳輸佇列 (MAC 或 CPU TxQ)來等待傳送。  
由於 NetFPGA 主要是用來開發網路設備，因此大部分的開發專案皆
適用於此架構，專案設計者可基於此架構於相對應模組中加入自己所實
作的功能，例如 IPv4 Reference Router、OpenFlow Switch 等專案便是在
Output Port Lookup 模組中加入其封包判斷處理的能力來達成設計需求。 
3.1.3 NetFPGA 資料格式  
資料處理格式方面，NetFPGA 將資料處理分成封包串流 (packet bus)
與暫存器串流 (register bus)，封包串流又依來源及目的地的不同區分為
MAC RxQ、CPU RxQ、MAC TxQ 和 CPU TxQ，其中 MAC RxQ 儲存的
是由網路介面孔所收到的封包資料，CPU RxQ 則是由主機端透過 DMA
所傳遞下來的資料，而 MAC TxQ 指的是要由網路介面孔所送出的封包
資料，CPU TxQ 則是由 NetFPGA 要透過 DMA 傳遞到主機的資料，當接
收佇列 (RxQ)接收到封包資料後，封包串流便以 FIFO 的方式在各級間傳
遞資料，傳送佇列 (TxQ)接收到資料後也是以 FIFO 的方式將資料遞送出
去。  
為了實現各個模組間的資料通訊，NetFPGA 在封包串流的格式上使
用了一種特殊的資料格式，包含模組表頭控制區塊和封包的數據區塊兩
部份，如圖 3.6 所示  
 
 
圖 3.6 NetFPGA 模組間封包串流 (packet bus)的資料格式  
 8 
 
3.2 NetFPGA專案 - OpenFlow Switch 
OpenFlow[8, 9]是由美國史丹佛大學研究團隊所提出的一種網路虛擬
化技術，藉此可讓研究人員在一般網路中實驗新的網路協定而不會互相
產生影響，OpenFlow主要被實現在網路交換器上，藉由內部的 Flow Table
區分不同流量並利用標準化的協定來對 Flow Table 新增定義欄位，當封
包經過實現 OpenFlow 功能的交換器時，便能依照 Flow Table 指定的動
作來傳遞，藉此區分一般流量與實驗流量，達成網路虛擬化的功能。  
3.2.1 OpenFlow Switch 架構  
一個 OpenFlow Switch 主要由三個部份所組成，如圖 3.7 所示  
 
 
圖 3.7 OpenFlow Switch 架構  
 
1. Flow Table:對每個進來的封包連線指定其相關動作，交換器藉由
Flow Table 可以得知該如何對一條封包連線做處理，其中每個條目都由
表頭、統計資訊 (Counter)、對應動作 (Action)所組成，如圖 3.8 所示。  
 
Header Fields Counters Actions 
圖 3.8 Flow Table 中的條目組成欄位  
 
 10 
 
Header
Parser
Exact
Match
Lookup
Wildcard
Lookup
Arbiter
Packet
Editor
User Data Path
SRAM DRAM
OpenFlow Output Port Lookup
 
圖 3.9 OpenFlow-NetFPGA-1.0.0 硬體架構  
 
當封包由輸入佇列進入後，會先由 Header Parser 模組把封包的 VLAN
標籤、第二層、第三層及第四層表頭取出，並將這些欄位組成一 256 位
元的字組，然後交由 Eaxct Match Lookup 模組與 Wildcard Lookup 模組做
Flow Table 的欄位比對，其詳細表頭欄位如表 3.1 所示。  
 
表 3.1 OpenFlow-NetFPGA-1.0.0 Flow Table 中用來比對的封包表頭欄位  
欄位  位元數  應用時機  
Ingress port 8 所有封包  
Ethernet source address 48 開啟介面的所有封包  
Ethernet destination 
address 
48 開啟介面的所有封包  
Ethernet type 16 開啟介面的所有封包  
VLAN id 16 所 有 乙 太 網 路 類 型
(Ether Type)為 08100
的封包  
IP source address 32 所有 IP 和 ARP 封包  
IP destination address 32 所有 IP 和 ARP 封包  
IP protocol 8 所有 IP 和 ARP 封包  
IP ToS bits 8 所有 IP 封包  
Transport source port / 
ICMP Type 
16 所 有 TCP 、 UDP 和
ICMP 封包  
 12 
 
 
圖 3.10 OpenFlow-NetFPGA-1.0.0 Flow Table 中的 Action 欄位資料結構  
 
在 OpenFlow Switch 的使用上，使用者只要設定好 Flow Table 中所要
比對的封包表頭欄位，再加上所欲修改的值及處理路徑設定，則此後只
要吻合的封包經過 OpenFlow Switch 便可依照使用者指定的路徑傳輸並
做表頭資料修改。  
另外在 Flow Table的統計資訊方面，資訊依各表格 (Table)、流量 (Flow)、
介面 (Port)及輸出佇列的不同定義了不同的統計資訊，如表 3.3~表 3.6 所
示。  
 
表 3.3 表格相關的統計欄位  
計數器  位元數  
OPENFLOW_LOOKUP_WILDCARD_MISSES_REG 32 
OPENFLOW_LOOKUP_WILDCARD_HITS_REG 32 
OPENFLOW_LOOKUP_EXACT_MISSES_REG 32 
OPENFLOW_LOOKUP_EXACT_HITS_REG 32 
OPENFLOW_LOOKUP_NUM_PKTS_DROPPED_0~7_REG 32 
 
表 3.4 流量相關的統計欄位  
計數器   位元數  
Number of received packets 25 
Packet last seen time stamp 7 
Number of received bytes 32 
 
表 3.5 介面相關的統計欄位  
計數器  位元
數  
MAC_GRP_0~3_RX_QUEUE_NUM_PKTS_STORED_REG 32 
MAC_GRP_0~3_RX_QUEUE_NUM_PKTS_DROPPED_FULL_REG 32 
MAC_GRP_0~3_RX_QUEUE_NUM_PKTS_DROPPED_BAD_REG 32 
MAC_GRP_0~3_RX_QUEUE_NUM_WORDS_PUSHED_REG 32 
 14 
 
 
FlowVisor 是由史丹佛大學研計團隊所提出，最主要的作用是把虛擬
化的工作從 Controller 身上分出來，也就是達到分工的效果：FlowVisor
負責虛擬化，Controller 則專心負責 Control 即可 (如圖 3.12)。  
 
圖 3.12 FlowVisor把虛擬化的工作從 Controller身上分出來 
 
由於 OpenFlow 協定支援第二層、第三層、第四層共十個欄位，因此
可以很有彈性的去切虛擬網路。舉例來說，如果以第三層欄位來切的話
(如圖 3.13)，也就是說某個網域下發出的封包由一 Controller 控制路徑，
可以走某個虛擬網路；另一網域下發出的封包由另一 Controller 控制路徑，
可以走另個虛擬網路，依照各網域所付的費用不同有不同的路徑。  
 
 16 
 
Secure Channel
OpenFlow 
Switch
Controller A
(Control 1.1.1.0/24)
OpenFlow
Protocol
               NetFPGA
SW
HW HW Flow Table
IF IF IF IF
SW Flow Table
FlowVisor
OpenFlow
Protocol
OpenFlow
Protocol
Controller B
(Control 1.1.2.0/24)
Controller A: 1.1.1.0/24
Controller B: 1.1.2.0/24
A(1.1.1.2)  
圖 3.15 OpenFlow Switch將此封包複製一份送給 FlowVisor 
FlowVisor 接著檢查此封包是屬於哪一個虛擬網路，檢查完後將此封
包傳送給所屬之 Controller(如圖 3.16)。  
 
 
圖 3.16 FlowVisor將此封包傳送給所屬之 Controller 
 
Controller 用自己的演算法來決定封包的路徑後，再下指令要求新增
條目；FlowVisor 會協助將此控制訊息轉交給正確的 OpenFlow Switch(如
圖 3.17)；後面同個 Flow 的封包進入 OpenFlow Switch 後就可以直接由
OpenFlow Switch 處理而不再需要將封包傳給 Controller。  
 
 18 
 
擷取封包，再檢查封包的完整性 (CRC 檢查 )及是否為 unicast 封包之後，
將封包往封包分析模組轉送。  
模組二：封包分析模組 (Packet Analysis Module) 
本模組的封包分析可以依照封包表頭第二層、第三層或是第四層的相
關欄位內容來擷取出其他子計畫對此連線封包所需要的資訊，並與 Flow 
Table 中使用者定義的流量做比對，藉此瞭解目前底層流量情況並提供統
計資訊以供流量統計和分析。  
模組三：封包動作路徑處理模組 (Packet Action/Path Determine) 
此模組主要是尋找目前封包在 Flow Table 中有無已建立好的路由處
理資訊，若已有定義相關的處理流程，則此後相符合的封包便依照其定
義的動作做處理 (ex:封包丟棄、由特定介面輸出、導至上層分析等… )，
若無相符合的動作定義，則將此封包交由下一個模組做預設路徑路由處
理。此模組並提供上層管理者對此 Flow Table 做封包動作路徑寫入的動
作，利用此模組來實現上層管理者所欲實現的網路虛擬化路由情況。  
模組四：封包交換 /路由模組 (Switching/Routing Module) 
當封包由動作路徑處理模組處理完後將轉送給本模組，依照上一層模
組的比對狀況會有兩種結果，若上一層中已有預先定義好的命中處理規
則，則此模組會依照其定義規則轉送此封包到對應的 interface queue，讓
其等待 QoS 的處理，若上一模組並無命中的處理規則，則此封包會依照
系統的預設動作處理 (ex:一律丟棄、做 ARP request 並寫入 forwarding 
table 等… )，藉由預先定義好的策略來處理未知封包以維持封包處理的速
度。  
模組五：QoS 佇列模組 (QoS Queue) 
此模組依照各 interface 及 QoS 的需要，實現多個存放封包的佇列，
依封包處理優先權將其置入不同的 QoS Queue 中以便處理，並藉此達到
QoS 及網路流量的管控效果，可依使用者等級或應用的不同，設定不同
的 QoS 處理方式。  
由於美國史丹佛大學研究團隊所實作的 OpenFlow-NetFPGA-1.0.0 中
已對封包擷取 (Packet Capture)、封包分析 (Packet Analysis)、封包動作路
徑處理 (Packet Action/Path Determine)功能有所支援，因此本子計畫主要
基於 OpenFlow-NetFPGA-1.0.0 對其加入封包交換 /路由模組 (Switching/ 
Routing Module)及 QoS 佇列模組 (QoS Queue)的功能擴充。  
4.1 封包交換 /路由模組 (Switching/Routing Module) 
在原始 OpenFlow Switch 中，啟動時 Flow Table 中只有預設的條目，
預設將所有的封包送給 Controller 來決定路徑，此為 OpenFlow 設計之重
點交由 Controller 來掌控流量路徑，然而要實現在現今的網路中可能會導
致網路整體效能不彰，一般情況下，大部分的封包送往 Controller 的這個
 20 
 
00:00 
X X X X X X 01:80:C2:00:
00:00 
X 2 X 送往 if3 
X X X X X X 01:80:C2:00:
00:00 
X 4 X 送往 if5 
X X X X X X 01:80:C2:00:
00:00 
X 6 X 送往 if7 
 
 
 
硬體部分亦大致沿用 OpenFlow Switch所提出在 NetFPGA上的架構，
不同之處在於除了使用者資料路徑中原本的輸入仲裁器模組、VLAN 表
頭移除模組、輸出介面查詢模組、VLAN 表頭加裝模組、輸出佇列模組
外，新增了查詢轉送表 (lookup_forwarding_table)模組 (如圖 4.2)。另外也
修改了輸出介面查詢 (Output_port_lookup)模組，使各模組能互相配合運
作。  
 
圖 4.2 NV Switch加入了查詢轉送表模組 
 
OpenFlow 硬體中的輸出介面查詢模組若精確比對模組和通配符比對
模組都比對失敗，則預設會丟棄封包，本系統將輸出介面查詢模組的預
設動作由丟棄封包改為讓封包走一般封包交換路徑，讓非屬虛擬網路的
封包預設走一般封包交換路徑。  
 
圖 4.4 為整個新增的轉送表模組 (Lookup_forwarding_table)設計流程
圖，以下將一一解釋設計概念：  
我們以丹佛大學研究團隊與 NetFPGA 一併釋出查詢轉送表模組
 22 
 
  
圖 4.4 轉送表模組設計流程圖  
 24 
 
的緩衝區大小，漏水的速度則是封包的輸出速率，如果連線來源端送出
的封包太多使得緩衝區滿了，則多餘的封包會被丟棄，否則放入緩衝區
中等待輸出，Leaky Bucket 藉由固定緩衝區大小及封包輸出速度來達到
對輸出流量的管控。  
Token Bucket基本上是 Leaky Bucket的變形，其加入了 Token的概念，
Token 以固定的速率被產生，產生出來的 Token 存放在 Token Buffer 中，
當封包資料來臨時，若有足夠的 Token 可供使用，則可以輸出封包，否
則封包必須在緩衝區中等待相應數量的 Token 產生，藉由 Token 產生的
速率使得封包輸出速度得到管控。  
在流量管理的設計目標上，本論文希望能達到流量控制的準確度並儘
可能的維持其彈性化，藉此讓使用者能更加貼近需求的對其操作，因此
流量管理模組中對流量調整的機制方面，採用了 Token Bucket 演算法的
方式來進行設計，Token Bucket演算法相對於 Leaky Bucket演算法而言，
增加了對 Token 產生速率及大小設定的彈性，使用者能藉由設定 Token
產生的時間週期及大小來調整封包輸出的延遲及流量，達到管控的準確
度，因此設計出來的流量管理模組中，除了封包輸出速度佇列之外，還
需包含一個讓使用者對 Token 值大小及產生時間週期設定的方式，而
Token Buffer 的大小由於牽涉到可儲存 Token 值容量大小的多寡，所以
也是設計所需考量的重點之一，若設計的太小，則 Token 值容易滿溢而
遺失，造成流量控制準確度的降低，但若設計的太大，則 Token Buffer
中有可能因為存放過多的 Token 值而使輸出流量產生暴衝，因此在這邊
的設計上，Token Buffer 的儲存容量被設為使用者每次補充 Token 數量
的兩倍，藉由保留前一個時間單位所剩餘的 Token 值，來增加對不穩定
性流量的控制能力，並避免造成過大的封包流量暴衝，舉例來說，假設
使用者預定傳輸速率為 10M bytes/s，而將每秒每次補充的 Token 值設為
10M，但網路流量卻因為前端壅塞或其他狀況造成其傳輸速率並不穩定，
在第一個時間單位內有 5M bytes 資料到達，第二個時間單位內則有 15M 
bytes 資料到達，若將 Token Buffer 容量剛好設為 10M bytes，則在第二
個時間單位內便會有 5M bytes 的資料被丟棄 (在這邊假定輸出速度佇列
的容量剛好不夠大 )，造成流量的遺失，如果因此將 Token Buffer 容量設
計的過大，雖然此時不會有流量遺失，但 Token Buffer 中的 Token 值卻
可能會逐漸累積而增加到一個過大數值，此時若前端網路流量突然暴增，
便會對後端網路產生一個大流量的網路暴衝，失去了我們對流量管理的
功能，因此較適當的設計是將 Token Buffer 容量設為每次補充 Token 數
量的兩倍，只保留前一個時間單位所剩下來的 Token 值可以在此時間單
位內被使用，減少因流量不穩定而產生的封包丟棄情況，在提昇流量控
制準確度的同時，也能避免封包流量暴衝的情況產生。  
 26 
 
 
圖 4.1 流量管理功能的實作架構  
 
目前實作的封包輸出速度佇列大小為 18K bytes，而 Token Buffer 則
是利用暫存器的方式來實現，當封包資料放到輸出速度佇列後，會由對
應到此輸出速度佇列的 Token Buffer 及 Clock 值來做輸出資料的管控，
輸出速度佇列輸出資料時，會依照輸出封包資料的大小由 Token Buffer
中減少相同的值，在輸出封包資料大於剩下的 Token 值時，若此時 Token
值仍為正，則依然會讓此封包輸出，但會將超過的 Token 值記為負值並
利用下一個時間單位的 Token 值做補回，舉例來說，假設每時間單位給
予 500 bytes 的 Token 值，而目前 Token Buffer 中的 Token 值只剩下 100 
bytes，若此時仍有封包要輸出且其封包大小佔有 200 bytes，則封包輸出
速度佇列仍會輸出此封包，只是封包輸出後 Token Buffer 中的 Token 值
會被設為 -100 bytes，當下一個時間單位要再補充 Token 值時，其 Token 
Buffer 中的 Token 值會被設為 (-100 bytes) + (500 bytes) = 400 bytes，藉此
將多用掉的 Token 值補回以減少誤差，但若 Token Buffer 中的值已經小
於 0，則封包輸出速度佇列就無法再輸出資料，必須等待 Clock 時間到達
時，Token Buffer 依使用者所設定的 Token 值做補充後，Token Buffer 中
的 Token 值變為正值才可再進行封包資料輸出，如系統設計所述，實作
中將 Token Buffer 的儲存容量設定為使用者每次補充數量的兩倍，藉此
 28 
 
析出來的結果，放到指定的輸出速度佇列來等待輸出，若相對應的輸出
速度佇列已無空間存放封包資料，則將此封包丟棄。  
本流量管理模組目前實作了三個輸出速度佇列可供使用者使用，分別
是輸出速度佇列 0~2，其中號碼越大的輸出速度佇列有越高的輸出優先
權，當各個輸出速度佇列中皆有資料等待輸出時，其輸出速度佇列的輸
出資料順序為 2>1>0，只有當較高優先權的輸出速度佇列無法輸出資料
時，低優先權的輸出速度佇列才可輸出資料，藉此減少對高優先權流量
的輸出延遲。  
為了達到流量管控，各個輸出速度佇列有其輸出預算的限制，上層使
用者可透過暫存器控制模組來對各輸出速度佇列的 Token 值及 Clock 值
做設定，當 Token Buffer 中的值大於 0 時，此輸出速度佇列才可輸出封
包資料，因此 Token Buffer 代表的是此輸出速度佇列在單位時間內所能
傳輸的資料數上限，藉此來對指定連線的輸出速度做限制，除了各個輸
出速度佇列的 Token Buffer 外，為了保留一次對整體網路流量控制的彈
性，模組中也實作了整體流量的 Token Buffer，也就是此連接埠在單位時
間內總共所能傳輸的資料數上限，如此設計的用意是在於當後端網路的
整體網路可負擔流量較小時，可利用此 Token Buffer 來減低過多流量的
輸出，但同時保留對不同連線輸出流量的設定。  
整個流量管理模組的封包資料處理可分為封包輸入及封包輸出兩部
份，處理流程如圖 4.3 及圖 4.4 所示。  
 
 
圖 4.3 流量管理模組的封包資料輸入處理流程  
 
 30 
 
 
圖 4.5 流量管理模組與 OpenFlow-NetFPGA-1.0.0 的硬體整合架構圖  
 
輸出佇列 (Output Queues)模組中共有八個輸出佇列，其中四個對應到
CPU 傳輸佇列，另外四個對應到 MAC 傳輸佇列，只要將本論文所實作
之流量管理模組接到所欲管控的佇列後，即可達到流量管理的功能，例
如 OpenFlow Switch大部分的封包資料都是由 4個MAC傳輸佇列所送出，
所以要達到對經過 OpenFlow Switch 的流量做控制的話，只需要將 4 個
MAC 傳輸佇列的接線與流量管理模組接上即可。  
對於流量管理模組來說，由於必須得知此封包要放入哪個輸出速度佇
列，因此必須對傳輸的封包模組表頭做解析，在原始
OpenFlow-NetFPGA-1.0.0 的使用設定上，會利用 Flow Table 中 Action 欄
位裡的 forward_bitmask 欄位修改吻合連線封包的輸入模組表頭設定，將
其中的 Dst Port One-Hot 欄位修改對應到 forward_bitmask 所指定的輸出
佇列，但由於目前 NetFPGA 的輸出佇列只有 8 個，所以雖然 Flow Table
中 Action 欄位裡的 forward_bitmask 欄位及輸入模組表頭設定的 Dst Port 
One-Hot 欄位皆保留了 16 位元可供使用，但兩者實際上都只使用了後面
的 8 個位元來做表示，為了使底層封包傳輸資料結構的修改降到最低，
流量管理模組利用兩者原本前面未使用到的 8 個位元來當作封包輸出速
度佇列的選擇表示，修改後的 Flow Table Action 欄位與封包輸入模組表
頭設定結構如圖 4.6、4.7 所示。  
 
 32 
 
-ip_src <src_ip> 0.0.0.0~255.255.255.255 來源端 IP 位址  
-type <eth_type> 0~65535 乙太網路類型。  
-eth_dst <eth_dst> xx:xx:xx:xx:xx:xx 目的端 MAC 位
址。  
-eth_src <eth_src> xx:xx:xx:xx:xx:xx 來源端 MAC 位
址。  
-sp <src_port> 8 bit 的 0 或 1，參照下
面的介面設定意義 (表
4.2)。  
此封包進來的介
面  
-vlan <vlan_id> 0~4095 vlan id 
-speed < queue number> 0~2,0default 使用哪個輸出速
度佇列。  
-fbm <forward_bitmask> 8 bit 的 0 或 1，參照下
面介面設定意義。 (表
4.2) 
決定封包的輸出
介面。  
-mask <tuple_mask> 11 bit 的 0 或 1 
最高位元分別由此表的
tr_d 欄位往下對應，
ex:11101111111 代表只
比對 ToS 欄位。  
只有對 wildcard 
table 有效，代表
哪些欄位要
mask 掉 (不看 )。
0要看，1不
看 (mask)。  
 
 
 
表 4.2 介面設定意義 (2 進位 ) 
 bit 7 bit 6 bit 5 bit 4 bit 3 bit 2 bit 1 bit 0 
意義  CPU3 MAC3 CPU2 MAC2 CPU1 MAC1 CPU0 MAC0 
EX: 00000101  代表 MAC0 & MAC1 
 
注意：未設定的參數部份會使用預設值，因此 wildcard table 不用的
部份可以不要設定，但記得利用 mask 參數將相關欄位 mask 掉，exact 
match table 則所有欄位皆需設定才能正常運作。  
EX: ./set_table_rule -a wildcard -tr_s 80 -ip_src 192.168.0.1 -speed 1 
-fbm 01000000 -mask 11111011111 
EX: ./set_table_rule -a connection -tr_d 12 -tr_s 80 -pr 25 -ts 1 -ip_dst 
192.168.0.2 -ip_src 1.1.1.1 -type 500 -eth_dst 00:11:22:33:44:55 -eth_src 
22:22:33:33:44:44 -sp 11111111 -vlan 2 -speed 1 -fbm 01010101  
 
 34 
 
5.1.1 相同表頭封包延遲的比較  
本實驗拓樸如圖 5.1 所示，利用兩台電腦模擬成一般流量的使用者，
再加上 NetFPGA 平台來構成一簡單的區域網路環境做測試，藉由在
NetFPGA上運行 OpenFlow Switch與 Controller Optional OpenFlow Switch
來達到驗證的效果。  
 
圖 5.1 延遲比較實驗環境網路拓樸  
 
本實驗主要是要測量當 OpenFlow Switch 與 Controller 間的連線狀況
不佳而溝通有較大延遲時，對單一網路流量的影響，此乃考量在實際網
路環境中，Controller 與 OpenFlow Switch 間的延遲受中間網路的影響甚
巨，甚至隨著架設環境的考量，可能會有跨縣市或跨國的實驗環境部屬，
因此提出此實驗假設。  
實驗中將 Controller 與 OpenFlow Switch 間的封包延遲加上一固定時
間 (400ms)以模擬壅塞網路情況，主要使用量測工具為 ping，每次送出 100
個相同封包，利用不同的發送速率與 payload 大小來量測其 100 個封包的
平均 Round-Trip Time，實驗結果如圖 5.2、圖 5.3 所示。  
 
圖 5.2 OF Switch RTT(100 same pkt) 
12079.26 12309.04 12337.81 12360.3
56452.42 60305.32 64724.86
60752.53
803447.93 795705.51 763969.87 760203.12
0
100000
200000
300000
400000
500000
600000
700000
800000
900000
64 512 1024 1518
R
T
T
(u
s)
pkt size(Byte)
interval=1s
interval=0.1s
interval=0.01s
 36 
 
 
圖 5.5 CO OpenFlow Switch RTT(100 different pkt) 
 
對於 OpenFlow Switch 來說，由於每個封包都屬於不同的 Flow，因此
每個封包查詢規則皆會失敗，需要等待 Controller 來決定封包走向，造成
每個封包都需要較長來回時間，相較之下 CO OpenFlow Switch 每個封包
皆可用硬體來處理，因此對於網路中各式各樣不同的封包而言，平均封
包來回時間有極佳的表現。  
事實上，在實驗 5.1.1 中發現 OpenFlow Switch 前幾個封包有需要較
長來回時間的現象，而後面封包因為前面的封包讓 Controller 已在 Flow 
Table 中新增了相關條目，因此可全由硬體處理，來回時間相對較短，但
在本實驗中，對任一封包 N 而言，封包 1 至封包 N-1 並不會導致封包 N
所屬的條目被新增，因此封包無法全由硬體處理而使需較長來回時間的
封包數量不因發送速率不同而改變。  
5.1.3 頻寬的比較  
本實驗旨在量測 OpenFlow Switch 架構中，NetFPGA 上新增轉送表
(Forwarding Table)後，在轉送能力上頻寬的提昇，主要使用量測工具為
NetFPGA Packet Generator，NetFPGA Packet Generator[19, 20]是一以硬
體實做的封包產生器，免去了封包在核心中傳遞的時間，完全以硬體產
生以及接收封包。OpenFlow Switch 在本實驗中被視為集線器，Controller
在收到任何封包的時候都會新增條目到 Flow Table：將此封包轉送至除
了進入介面外的所有介面。如此一來就只有前數個封包需要送交給
Controller，而剩餘的封包皆可完全藉由硬體處理。此實驗可以量測出由
於新增條目的延遲所造成的 OpenFlow Switch 效能降低，也能得知 CO 
OpenFlow Switch 是否也有相同的問題。  
本實驗拓樸如圖 5.6 所示，讓 NetFPGA Packet Generator 的介面 0 以
1Gbps 的速度重複傳送一個封包，而介面 1 負責接收封包，藉由量測介
面 1 封包抵達速率即可得知待測頻寬的大小。  
71.11
284.36
299.84 321.86
50.9
274
295
316.25
35.6
274.56
294.92
313.76
0
50
100
150
200
250
300
350
64 512 1024 1518
R
T
T
(u
s)
pkt size(Byte)
interval=1s
interval=0.1s
interval=0.01s
 38 
 
5.2 QoS佇列模組功能測試  
實驗的目的是流量管理模組的功能性驗證，基於資源及實驗需求的考
量，實驗量測的 OpenFlow Switch 只在 nf2c0 與 nf2c1 的 MAC 傳輸佇列
後加入流量管理模組進行實驗量測，如圖 5.8 所示。  
 



 
圖 5.8 實驗量測所啟用的流量管理模組  
 
實驗主要分為三個部份，5.2.1 及 5.2.2 驗證同一路徑不同流量的 TCP
與 UDP 連線的同時流量控制功能，5.2.3 驗證同時利用速率限制及優先
權佇列的封包延遲時間改善。  
由於設計及方便使用的緣故，這邊的 1K代表 1000，1M代表 1,000,000，
後續實驗數據與圖表單位皆同，將不再贅述。  
5.2.1 TCP 混和連線流量控制  
此實驗主要量測實作於 OpenFlow-NetFPGA-1.0.0 的流量管理模組對
TCP 混和連線的流量管理效果，實驗的接線如圖 5.9 所示。  
 
 40 
 
//目標 IP 192.168.0.4 的封包放到 nf2c1 的 mac 佇列，之後放到輸出速度
佇列 2 
 
./set_table_rule -a wildcard -ip_dst 192.168.0.1 -speed 0 -fbm 00000001 
-mask 11110111111 
//目標 IP 192.168.0.1 的封包放到 nf2c0 的 mac 佇列，之後放到輸出速度
佇列 0 
 
 
圖 5.10 TCP 混和連線流量控制的 Flow Table 設定結果  
 
由於資料是由 nf2c1的 mac佇列然後經由不同的輸出速度佇列輸出，
因此依照指定輸出速度的不同，使用表 5.2 的指令來對不同輸出速度佇
列做設定。  
 
表 5.2 TCP 混和連線流量控制的輸出速率控制設定  
./set_table_rule -s 10 100000    
//設定 nf2c1 的 mac 佇列的輸出速度佇列 0 輸出速度為 100KB/s 
 
./set_table_rule -s 11 1000000    
//設定 nf2c1 的 mac 佇列的輸出速度佇列 1 輸出速度為 1MB/s 
 
./set_table_rule -s 12 10000000    
//設定 nf2c1 的 mac 佇列的輸出速度佇列 2 輸出速度為 10MB/s 
 
本實驗於資料接收端使用 ifconfig工具量測網卡所收到的封包資料大
小 (Rx bytes)，並將每秒取樣到的封包接收資料量差值搭配 EXCEL 畫成
圖表，實驗共做 5 次，每次取樣 30 秒，五次傳輸流量平均數據與誤差狀
況如表 5.3 所示。  
 
 42 
 
 
10
M
20M
 
圖 5.11 UDP 混和連線流量控制實驗接線圖  
 
三台主機透過 OpenFlow Switch 的相同連接埠來接收遠端 NetFPGA 
Packet Generator 所產生的 UDP 封包資料，在這邊量測同時對多個 UDP
連線做流量管理的實際傳輸情況。  
假設三個連線的輸出速度分別設為每秒 5M、10M 及 20M bytes，在
開始量測數據前，需對 OpenFlow 的 Flow Table 進行設定，設定的指令
與結果如表 5.4 及圖 5.12 所示。  
 
表 5.3 UDP 混和連線流量控制的 Flow Table 設定  
nf_download -v ~/netfpga/bitfiles/Diffserv_openflow_switch.bit 
//燒錄 bit file 
 
./set_table_rule -a wildcard -tr_s 80 -ip_dst 192.168.0.2 -speed 0 -fbm 
00000001 -mask 11110111111  
//目標 IP 192.168.0.2 的封包放到 nf2c0 的 mac 佇列，之後放到輸出速度
佇列 0 
 
./set_table_rule -a wildcard -tr_s 80 -ip_dst 192.168.0.3 -speed 1 -fbm 
00000001 -mask 11110111111  
//目標 IP 192.168.0.3 的封包放到 nf2c0 的 mac 佇列，之後放到輸出速度
佇列 1 
 
./set_table_rule -a wildcard -tr_s 80 -ip_dst 192.168.0.4 -speed 2 -fbm 
 44 
 
第
一
次  
5,001,374 0.02748% 10,000,092 0.0009% 20,188,218 0.9411% 
第
二
次  
5,000,004 0.00008% 10,000,096 0.001% 20,156,899 0.7845% 
第
三
次  
5,000,002 0.00004% 10,000,096 0.001% 20,183,337 0.9167% 
第
四
次  
5,000,006 0.00012% 10,000,094 0.0009% 20,178,789 0.8939% 
第
五
次  
5,000,007 0.00014% 10,000,100 0.001% 20,185,050 0.9253% 
平
均  
5,000,279 0.00557% 10,000,096 0.001% 20,178,459 0.8923% 
誤
差
範
圍  
0.00004% ~ 
0.02748% 
0.0009% ~ 0.001% 0.7845 ~ 0.9411% 
 
表 5.6 1514 bytes 封包大小輸出速度設為 5MB/s、10MB/s 及 20MB/s 時的
實際傳輸流量數據與誤差範圍  
 5M 
(bytes/s) 
誤差  10M 
(bytes/s) 
誤差  20M 
(bytes/s) 
誤差  
第
一
次  
4,999,993 -0.00014% 10,000,607 0.0061% 20,001,194 0.006% 
第
二
次  
4,999,993 -0.00014% 10,000,552 0.0055% 20,004,262 0.0213% 
第
三
次  
5,000,043 0.00086% 10,000,556 0.0056% 20,147,178 0.7359% 
第
四
次  
4,999,985 -0.0003% 10,000,599 0.006% 20,039,770 0.1989% 
 46 
 
封包延遲時間，其實驗接線圖如圖 5.14 所示。  
 
IP:192.168.0.3
Mask:255.255.255.0
Ping Request & Http File Server 
nf2c0
nf2c1
nf2c2
nf2c3
Http File Client
Data & Ping
Switch
IP:192.168.0.2
Mask:255.255.255.0
IP:192.168.0.4
Mask:255.255.255.0
Ping Reply
OpenFlow-NetFPGA-1.0.0
with local host Controller
 
圖 5.14 OpenFlow-NetFPGA-1.0.0在有其他連線干擾時的封包延遲時間量
測接線圖  
 
第三部份量測在其他連線干擾時，利用流量管理模組的速率控制與優
先權佇列管控下的封包延遲時間，其實驗接線圖如圖 5.15 所示。  
 
 
圖 5.15 使用流量管理及優先權佇列在有其他連線干擾時的封包延遲時
間量測接線圖  
 
其中流量控制的目的主要是為了避免低優先權連線產生過高流量而
對 OpenFlow Switch 產生處理資源的搶佔，在這邊實驗只以流量控制
20MB/s 為例，事實上，對低優先權連線流量控制越嚴格將使得高優先權
連線有更多的處理資源可以使用而使得延遲時間越短。  
在開始量測數據前，需對 OpenFlow進行設定，依照量測部份的不同，
需進行不同的設定，第一部份與第二部份的設定如表 5.7 所示，主要是
建立 OpenFlow Switch與本機端 Controller的連線，讓 Controller能對 Flow 
 48 
 
//由 IP 192.168.0.4 來的封包放到 nf2c0 的 mac 佇列，之後放到輸出速度
佇列 2 
 
./set_table_rule -s 10 20000000 
//設定 nf2c1 的 mac 佇列的輸出速度佇列 0 輸出速度為 20MB/s 
 
 
圖 5.16 流量控制與優先權佇列驗證第三部份量測的 Flow Table 設定結
果  
 
本實驗使用 ping 工具量測封包的來回時間 (Round-Trip Time)，並將每
次來回時間搭配 EXCEL 畫成圖表，每一部分的實驗共做 5 次，每次於
30 秒內每秒發送一個 ping 封包做量測，在第一、二部份的時間量測上，
會由 Flow Table 中已經存在對所有連線的處理方式後開始量測，藉此避
免 OpenFlow Switch 詢問 Controller 所造成的時間延遲，原始
OpenFlow-NetFPGA-1.0.0 第一部份無其他連線干擾時的封包延遲時間量
測如圖 5.17 及表 5.9 所示，第二部份有其他連線干擾時的封包延遲時間
量測如圖 5.18 及表 5.10 所示，最後第三部份在有其他連線干擾時，利用
流量管理模組的速率控制與優先權佇列管控下的封包延遲時間如圖 5.19
及表 5.11 所示。  
 
 
圖 5.17 OpenFlow-NetFPGA-1.0.0在無其他連線干擾時的封包延遲時間量
測  
0
0.5
1
1.5
2
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
R
T
T
(m
s)
ith of the ping packet
平均0.123ms
平均0.123ms
平均0.122ms
平均0.123ms
 50 
 
 
表 5.11 使用流量管理及優先權佇列在有其他連線干擾時的封包延遲時
間量測  
 最小值 (ms) 平均值 (ms) 最大值 (ms) 平均差 (ms) 
第一次  0.12 0.246 0.268 0.028 
第二次  0.076 0.198 0.257 0.07 
第三次  0.115 0.214 0.266 0.065 
第四次  0.078 0.244 0.26 0.042 
第五次  0.084 0.207 0.255 0.066 
 
實驗結果顯示，當相同連線路徑上有不同流量時，其封包處理時間會
彼此互相干擾，造成封包傳輸有較大的延遲及抖動，而利用流量管理模
組所提供的流量控制及優先權佇列後，能有效的將高優先權流量延遲及
抖動情況降低，讓高優先權的流量傳輸更為順暢。  
5.3 結論  
本子計畫所設計之系統改良史丹佛大學研究團隊所提出來 OpenFlow
網路虛擬化技術，並搭配 NetFPGA 硬體平台以實做一可虛擬化的高速網
路交換器。本系統於 OpenFlow Switch 中增加了 Forwarding  Table，使
一般封包不需要經由 Controller決定路徑或學習，而可以直接由 NetFPGA
硬體來處理，加快封包傳送以及縮短封包來回時間，另增加了硬體架構
對 Spanning Tree Protocol 的支援，以避免網路中迴圈風暴的產生。  
實驗結果顯示，Controller Optional OpenFlow Switch 對一般流量的延
遲及頻寬皆有一定程度的改善，然 OpenFlow Switch 亦非不好，應該說
分別有其適用時機，當網路管理者要對網路做全面性管控或絕大部分流
量都屬於虛擬網路的時候，便較適用 OpenFlow Switch 的架構，因為
Controller 可以檢視所有的封包，再決定封包的處理方式，而本論文所提
出的架構則是適用於只想對網路中某些特定流量做管控的情況，因此藉
由 Forwarding  Table 來增進對一般流量的效能處理。  
另外本子計畫也在 NetFPGA OpenFlow Switch 專案上實作了流量管
理模組，實驗結果顯示，流量管理模組對於 UDP 連線的所有輸出速度控
制，無論是 64 bytes 的封包大小或 1514 bytes 的封包大小，準確度皆能
高達 99%以上，而對於 TCP 連線而言，當輸出流量控制在 10M bytes/s
以下時，準確度也能高達 99%以上，設定 20M bytes/s 時，最差也能有
95%以上的準確度，另外搭配上優先權佇列的使用，更能有效的減少高
優先權流量的封包延遲及抖動，因此流量管理模組確實能達到流量資源
管控的效果，也達到對 OpenFlow spec1.0.0 當中所提到的輸入佇列選項
功能一個簡單的實現。  
 52 
 
token bucket model," 1999, pp. 51-62 vol. 1. 
[16] A. Demers, S. Keshav, and S. Shenker, "Analysis and simulat ion of a 
fair queueing algorithm," 1989, pp. 1-12. 
[17] M. F. Homg, W. T. Lee, K. R. Lee, and Y. H. Kuo, "An adaptive 
approach to weighted fair queue with QoS enhanced on IP network," 2001, 
pp. 181-186 vol. 1. 
[18] K. Mezger, D. W. Petr, and T. Kelley, "Weighted Fair Queuing vs. 
Weighted Round Robin: A Comparative Analysis," 1994. 
[19] NetFPGA Packet Generator, 
http://netfpga.org/foswiki/bin/view/NetFPGA/OneGig/PacketGenerator. 
[20] G. A. Covington, G. Gibb, J. W. Lockwood, and N. McKeown, "A 
packet generator on the NetFPGA platform," 2009, pp. 235-238. 
 
 
表 Y04 
北美第二屆 NetFPGA 開發者 2010 研討會報告 
2nd North American NetFPGA Developers Workshop 2010 
 
August 12-13, 2010, Standford University, U.S.A. 
 
楊竹星 
國立成功大學電機工程系 
 
一、參加會議經過 
 
北美第二屆 NetFPGA開發者 2010研討會是 NetFPGA開發相當重要且唯一的
研討會，今年是第二屆，於 2010年 8月 12日及 13日在美國史丹福大學召開。 
 NetFPGA平台是由史丹福大學執行美國國科會支持之計畫，專為網路相關硬
體裝置開發用所設計的一組FPGA開發平台，其目的是希望教育及研究相關的使
用者能夠更輕易且快速地進行硬體的研究或開發，其具有低成本、開發迅速簡
便、以及資源共享等特色，其對網路相關硬體開發的支援度更是令其它FPGA開
發板難以抗衡。NetFPGA具有開放性，史丹福大學提出”Open Source Hardware”
的概念，將NetFPGA工具系統開發在CentOS Linux上，提供開放的底層功能資源，
能讓研究人員直接使用，可節省不必要的時間。 
 感謝國科會經費支援，得以成行參與研討會，在會中發表論文並且與其他研
發團隊討論，分享經驗與成果，收穫良多。 
 
 
二、與會心得 
 
 本次會議為北美第二屆 NetFPGA 開發者研討會，先前 NetFPGA 開發者研討
表 Y04 
NetFPGA平台，參與 summer camp之學生及各教授討論相當熱烈。當天會議除了
論文發表以外，每場發表的時間也包含現場實際 demo，我們所發表的系統由於
設備攜帶不易，沒有在現場正式 demo，但也於會場播放台灣錄製之 demo影片。 
本次會議為本人第一次參與 NetFPGA開發者研討會發表論文，除了分享及
報告系統成果外，也與聽到許多 NetFPGA開發者的論文成果，其中利用休息時
間，與相關開發人員討論 NetFPGA平台之開發經驗收穫良多，也詢問希望未來
也能夠在台灣舉辦 tutorial workshop，甚至 developer workshop，對於國內
有興趣之研究人員提供方便入門之管道。 
。 
 
三、考察參觀活動 
 
無。 
開會地點即是在史丹福大學資訊系 Paul G. Allen Building 內講堂，其為世界
知名大學。 
 
四、建議 
 
參加北美第二屆 NetFPGA 開發者 2010 研討會，與其他國家學者專家討論
NetFPGA 開發之經驗與成果分享，了解到來自加拿大名校多倫多大學資訊系教
授，帶領學生實作 NetFPGA 時，同樣遭遇相當大的門檻與挑戰，希望分享其他
單位訓練成員的經驗。鼓勵我們整合計畫中其他研究團隊成員以及國內其他研
究團隊可以參加如此的系統實作研討會，應可提高我們的系統實作能力與研究
視野。 
 
五、攜回資料名稱與內容 
筆者攜回研討會論文集。 
 
 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：楊竹星 計畫編號：99-2219-E-006-002- 
計畫名稱：網路虛擬化架構之研究與建置--子計畫一：設計與實作一具有網路虛擬化之高速封包處理
平台(2/2) 
量化 
成果項目 
實際已達
成數（被
接受或已
發表）
預期總達
成數(含實
際已達成
數) 
本計畫
實際貢
獻百分
比 
單位
備註（質化說明：如數個計畫
共同成果、成果列為該期刊之
封面故事...等） 
期刊論文 0 0 100%  
研究報告/技術報
告 0 0 100%  
研討會論文 2 2 100% 
篇
Pang-Wei Tsai, Pei-Wen Cheng, 
Mou-Sen Chen, Mon-Yen, Te-Lung 
Liu and Chu-Sing 
Yang, ’’’’’’’’Planning 
and Deployment of OpenFlow 
Networks over 
Internet’’’’’’’’, in 
Proceedings of the Taiwan 
Academic Network Conference 
2010 (TANet 2010), Oct. 27 - 29, 
2010. ； 楊竹星、黃俊嘉、鄭佩汶、
黃 俊 穎 、 蔡 邦
維 , ’ ’ ’ ’ ’ ’ ’ ’ 在
OpenFlow Switch 環 境 下 利 用
NetFPGA 改 善 封 包 傳 送 效
率 ’ ’ ’ ’ ’ ’ ’ ’ , 2011 
Conference on Electronic 
Communication and Applications, 
Kaohsiung, Taiwan, May 20, 2011. 
(皆為與總計畫共同成果) 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 5 5 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國
內 
參與計畫人
力 
（本國籍） 
專任助理 0 0 100% 
人次
 
期刊論文 0 0 100%  
研究報告/技術報
告 0 0 100%  
國
外 
論文著作 
研討會論文 1 1 100% 
篇
Pang-Wei Tsai, Pei-Wen Cheng, 
