行政院國家科學委員會專題研究計畫期末報告 
億閘級積體電路之佈局技術研究 (3/3) 
 
計畫編號：NSC 94-2215-E-007-007 
執行期限：94 年 8 月 1 日至 95 年 7 月 31 日 
計畫主持人：林永隆  國立清華大學資訊工程學系 
 
一、中文摘要 
在本篇報告中，我們實現了一個高效率的
二次規劃法導向之元件擺置架構。該架構是以
透過反覆地解『部份取自於可移動元件所構成
之子集』來實現。此外我們也提出了一個可以
忠實反應出實際元件間連接線的數學模型，並
且可以有效減少所需要解運算之稀疏矩陣大
小，並且以兩階層的桶狀串列資料結構來實做
解矩陣運算。在基礎建設部份，使用者可以很
容易的利用我們所提供的以 Tcl/Tk 為基礎
實作出來的圖形顯示工具，來觀察及評估實驗
的結果。 
 
二、英文摘要 
In this report we realize a high performance 
quadratic-based placement framework through 
solving a sub-set picked out from movable 
objects iteratively. Besides, a reduced star model 
that can disperse the matrix and consider the 
signal direction simultaneously is proposed. In 
our approach, a two level bucket hash data 
structure is used to serve the large sparse matrix 
solver. In our infrastructure, a Tcl/Tk based 
visualization tool is provided to help estimate 
the costs and benefits of solutions 
 
三、計畫緣由與目的 
 The following table is a survey of 
academic placement tools. 
Table. 1 A Survey of Academic Placer 
Methodology Academic Tool 
Simulated Annealing based 
Recursive partition with min-cut 
Multilevel min-cut objective 
Quadratic based 
 
Other Analytic 
TimberWolf, Dragon 
FengShui, Dragon 
mPL, Capo 
FastPlace[3], PROUD[4], mFAR, 
Kraftwerk 
APlace[5] 
 
A good placement methodology should 
relax the tight connectivity as shown below. 
 
Fig. 1 Random Placement with Two Terminal Nets 
(HPWL=19.93) 
B. Formulation 
There are three steps to translate the 
gate-level netlist into sparse matrix, as illustrate 
below. 
 
Fig. 7 Diagonal Part 
 
 
Fig. 8 Triangular Part 
 
 
A                 Bxy  
Fig. 9 Combine with I/O Constraint 
 
C. Sparse Matrix Solver 
In order to handle million gates of design a 
two-level bucket hash data structure is proposed. 
Cells are overlap after performing quadratic 
programming (QP), so an imprecise but fast 
iterative partial QP (PQP) is proposed. (It’s easy 
to apply the Gauss-Seidel with SOR in this 
framework.) 
 
Fig. 10 Two Level Bucket Structure 
D. Iterative Partial QP (PQP) Algorithm 
object pool
P
solve PQP(S)
Randomly pick sub-set
Randomly push back for 
recalculating (overlapping) 
Scalable 
window size
 
Fig. 11 Iterative PQP 
Iterative_PQP ( 
IN  initial placement A;  
OUT placement location L(A); 
) {     
Randomly assign an initial placement if A is empty; 
 
while have improvement do 
     
    Initial object pool P; 
     
    while P is not empty do 
        Randomly pick sub-set S from P; 
        Solve Partial_QP(S); 
    end 
 
end 
} 
 
Partial_QP ( 
IN  selected sub-set S; 
OUT placement location L(S); 
) {     
Initiate L(S), which is to be computed, to zero; 
 
Patch non-pivot columns of S with unselected set S';
  
// Apply elimination with pivot column of S 
foreach selected row R do 
 Fig. 17 PQP Iteration 1 (HPWL=14.11) 
 
Fig. 18 PQP, Iteration 10 (HPWL=5.56) 
 
 
Fig. 19 PQP, Iteration 60 (HPWL=2.22) 
Unfortunately, over 50% objects must be 
legalized in phase II, which indicates that the 
legalized position may differ greatly from the 
solution solved after performing PQP. So, a good 
methodology for spreading cell is very important 
for us. Additional force [3], re-weighting [5] or 
non-linear optimization [7] methodologies can 
be used for spreading cells. Besides additional 
legalization techniques are required to remove 
overlap [8].  
 
Cell with large degree implies that the wire 
length between those nodes connecting to it will 
hard to be improved wherever it is placed. It will 
not be good for any one connecting to it 
wherever it is move, so maybe we can free those 
cells with high-degree or make a clone, as 
shown below. Maybe we can place them 
randomly at the last because the locations solved 
after QP are so crowded. 
表 Y04 
行政院國家科學委員會補助國外差旅費報告 
                                                          95 年 02 月 08 日 
報告人姓名 林永隆 服務機構及職稱 
資工系 
教授 
時間 
 
會議地點 
九十五年一月二十四日至
九十五年一月二十七日 
日本橫濱 
本會核定
補助文號 NSC 94-2215-E-007-029 
會議 
名稱 
 (中文) 2006 亞太設計自動化會議 
 (英文) ASP-DAC 2006  
發表 
論文 
題目 
 (中文)  
 (英文)  
1.    A Near Optimal Deblocking Filter for H.264 Advanced Video Coding 
2.    Introduction to H.264 Advanced Video Coding 
報告內容應包括下列各項： 
一、參加會議經過 
I did the following: 
1. 1/24 Attend a full-day tutorial on system in a package 
2. 1/24 Attend the 4th Asia University Workshop hosted by Kitakyushu City and give a 
10-miniute talk on NTHU’s research activity 
3. 1/25 Present our paper “A near optimal deblocking filter for H.264/AVC” 
4. 1/26 Attend the ASP-DAC Steering Committee’s Advisory Committee meeting 
5. 1/26 Moderate a session 
6. 1/27 Moderate the special session “H.264/AVC Design Challenges and Solutions” and 
present one of the talk on “Introduction to H.264/AVC” 
7. 1/27 Evening: Attend the Joint ASP-DAC Steering Committee and Organizing 
Committee Meeting 
二、與會心得 
After last year in Shanghai, I proposed to the Steering Committee Chair to reform the TPC of 
ASP-DAC. Staring this year, we model after the review process of ICCAD. The results are 
excellent. Out of 424 submissions, 135 have been accepted for presentation. The program 
has been viewed as very high quality. 
 
We have separated the TPC from organization committee. Beginning with 2009 in Japan, 
the conference will have TPC chair from outside country. It has been decided that 
Professor Ren-Song Tsay of NTHU, TAIWAN, will serve as the TPC chair for 2009. It has 
also been decided that ASP-DAC will come to Taiwan in either 2010 or 2011. 
 
The special session on H.264/AVC is particular interesting. I organized it to bring the 
design community to the EDA community. Besides my introduction to the AVC, we have 
Professor H M Hang of NCTU talking about DSP pure software approach, Professor L G 
Chen of NTU talking about pure hardwired approach, and Professor Sunwoo of Ajou 
University, Korea, talking about ASIP approach. After the talks, we have a mini-panel 
discussion for the audience to express their opinion and ask questions. It turns out as one 
of the most popular sessions. 
三、考察參觀活動(無是項活動者省略) 無 
四、建議 
Besides submitting regular papers, our researchers can be more involved by giving 
tutorials, organizing panel, special sessions, etc to achieve greater impact in the 
international community. 
五、攜回資料名稱及內容 
Proceedings, CD. 
- 2 - 
Inputs to the deblocking filter include pixels, boundary 
strength, and threshold values as shown in Fig. 2. The pixels 
of a macroblock are filtered by an edge filter in a specific 
order, and each pixel may be filtered multiple times. After 
the whole picture is filtered, it is ready for display as well as 
being a reference picture. 
Reconstructed Picture 
Unfiltered Macroblock 
Input Pixels 
Coding Information 
Get bS 
Edge Filter 
Filtered Pixels 
Boundary Strength 
QP 
Get Threshold
Į, ȕ, indexA
Fig. 2. Inputs to and outputs from the deblocking filter. 
B. Filter Order 
The deblocking filter process consists of a horizontal 
filtering across all vertical edges and a vertical filtering 
across all horizontal edges. Fig. 3(a) illustrates the filtering 
process for the 16x16 luma component of a macroblock. 
Each small box denotes a pixel, and a dotted one represents 
a pixel from neighboring macroblocks. The top part shows 
the horizontal filtering. Vertical edge 0 is filtered 
horizontally first from top to bottom, followed by edge 1, 
edge 2, and edge 3. For luma filtering, the edge filter takes 
as its inputs eight pixels, p3, p2, p1, p0, q0, q1, q2, and q3. 
At most 6 pixels will be modified by the filter as shown in 
the shadowed part of the figure. Because there are 
overlapping area between the filtering of two adjacent edges, 
some pixels (actually, half of them) may be filtered twice. 
The vertical filtering shown in the bottom part of Fig. 3(a) 
is performed after horizontal filtering in a similar way. Edge 
0 is vertically filtered from left to right, followed by edge 1, 
edge 2, and edge 3.  
The filtering process of chroma components is similar to 
that of luma components as depicted in Fig. 3(b). It is first 
horizontally applied on edge 0 from top to bottom, followed 
by edge 1. After the vertical edges are filtered, the horizontal 
edges are then filtered from edge 0 to edge 1. Note that 
unlike a luma edge which is of length 16, a chroma edge is 
of length 8, and there are only 5 input pixels, p1, p0, q0, q1, 
and q2 with two possible pixel modifications per filtering. 
C. Boundary Strength 
The boundary strength (bS) is derived from the coding 
information [5] of the macroblock. Two adjacent 4x4 blocks 
share a bS value. Its value ranges from 4 to 0, 4 for the 
strongest filtering and 0 for no filtering. Fig. 4 gives a 
flowchart for calculating bS value. If any one of the two 
adjacent 4x4 blocks is coded with intra prediction mode and 
they are on the macroblock edge, the bS is set to 4. If any 
one of them is intra-coded and they are not on the MB edge, 
the bS is 3. If any of them contains non-zero transform 
coefficients, the bS is set to 2. Finally, if different reference 
pictures are used or the difference between two motion 
vectors of the two blocks is greater than or equal to 4 in 
units of quarter pixels, the bS shall be equal to 1. For the 
remaining cases, the bS is set to 0. 
p3 p2 p1 p0 q0 q1 q2 q3 
Edge0   Edge1  Edge2   Edge3 
p3
p2
p1
p0
q0
q1
q2
q3
16 Pixels
16 Pixels
(a) Luma Component 
p1 p0 q0 q1 q2
Edge0   Edge1
8 Pixels
p0 
p1 
q2 
q1 
q0 
Edge0   Edge1   Edge2  Edge3
E
dge0   Edge1
(b) Chroma Component
8 Pixels
Fig. 3. Horizontal filtering and vertical filtering of luma 
component (a) and chroma component (b). 
D. Threshold 
Three threshold variables, Į, ȕ, and indexA, are used to 
prevent true edges from being filtered. Their values depend 
on the quantization parameters as described in [2]. The flag 
filterSamplesFlag is used to decide whether the filtering 
process should be carried out. It is set to true if (1) is true. 
bS!=0 && |p0-q0|<Į && |p1-p0|<ȕ && |q1-q0|<ȕ      (1) 
E. Edge Filter 
The edge filter starts to filter when the input pixels, 
boundary strength, and threshold variables are ready. First, if 
the flag filterSamplesFlag is equal to 1, the current edge is 
very likely to be a blocking artifact instead of a true edge. 
Thus, the filtering process should be applied. If the filtering 
process needs to be performed, there is a branch depending 
- 4 - 
the pipeline. Therefore, we add some forwarding logic to get 
register transferring behavior as illustrated in Fig. 10. 
FrameWidth 
Fram
eH
eight 
Fram
eH
eight/2
reconstruct mem 
96 x 32 bits 
Single-port SRAM …… 
local mem 0 
32 x 32 bits 
Two-port SRAM 
local mem 1 
(1.5xFrameWidth) x 32 bits 
Single-port SRAM …… 
FrameWidth/2 
…
…
…
… … … … …
…
…
…
… … … … … … … … … …
…
…
…
Luma Component : 
Chroma Component (Cb) : Chroma Component (Cr) : 
FrameWidth/2 
Fig. 6. Local memory organization. 
L0 
L1 
L2 
L3 
LT0 LT1 LT2 LT3 
B0
B4
B8
B12
B1
B5
B9
B13
B2
B6
B10 
B14 
B3 
B7 
B11 
B15 
CBL0 
CBL1 
CBT0 CBT1 
B16
B18
B17
B19
CRL0 
CRL1 
CRT0 CRT1
B20
B22
B21
B23
1
2
3
4
6
8
10
12
5
7
9
11 
14
16
18
20
13 
15 
17 
19 
22
24
26
28
21
23
25
27
29
30
31
32
33
34
36
38
35 
37 
39
40
41
42
44
46
43
45
47
48
Fig. 7. Proposed filtering order. 
When we begin to do the first filtering (Line 5) of Step 6, 
we need pixels of Line 1. As described in Sub-Section C, we 
have put Block LT0 into transpose register T0 in Step 4, and 
thus each column of Block LT0 can be read. To get the lower 
part of Line 5, which is still inside the pipe stage, we insert a 
forwarding logic to select pixels marked with a00, a10, a20, 
a30 from different pipeline stages. 
At the next cycle, parameters such as filterSamplesFlag 
have been calculated. Note that it requires 8 pixels, a30, a31, 
a32, a33, b30, b31, b32, and b33, to perform filtering for 
block B0 and B1 in Stage 3. The forwarding pixels are not 
fed back to the edge filter. Instead, it is directly output to the 
transpose register T1 or Write Back Unit in some cases such 
as Step 7. We use the register p3fwd as shown in Fig. 8 to 
keep the required pixel, a30. 
The multiplexers in the data path depicted in Fig. 8
denoted the forwarding paths.  
Stage 3 
(bS4Filter)
Stage 4 
(bS123Filter)
Stage 1 
(RdPixel)
Stage 5
(WrPixel)
r0p3 r0p2 r0p1 r0p0 r0q0 r0q1 r0q2 r0q3
r1p3 r1p2 r1p1 r1p0 r1q0 r1q1 r1q2 r1q3p3fwd
r2p3 r2p2 r2p1 r2p0 r2q0 r2q1 r2q2 r2q3
bS=4 filtering or calculate delta 
r3p3 r3p2 r3p1 r3p0 r3q0 r3q1 r3q2 r3q3
calc. para. 
bS<4 filtering 
Stage 2 
(CalcPara)
Fig. 8. Proposed 5-satge pipelined edge filter. 
b00 
b10 
b20 
b30
b01 
b11 
b21 
b31 
b02 
b12 
b22 
b32 
b03
b13
b23
b33
a00
a10
a20
a30
a01
a11
a21
a31
a02
a12
a22
a32
a03 
a13 
a23 
a33 
c00
c10
c20
c30
c01
c11
c21
c31
c02
c12
c22
c32
c03 
c13 
c23 
c33 
block B0 block B1 
block LT0
Blocks in pipeline
Blocks for next filtering
Line 1
Line 2
Line 3
Line 4
Line 5
Line 6
Line 7
Line8
Fig. 9. Pipeline hazard illustration. 
- 6 - 
TABLE I Comparisons among various deblocking filters
V.Conclusions 
We have proposed a near optimal architecture for 
deblocking filter in H.264/AVC. We implemented the design 
in synthesizable Verilog RTL and verified it with reference 
software [14]. The result shows that the performance of our 
design is near optimal but the usage of local memory is less 
compared with previous work. Besides, with a pipelined 
architecture, our design can achieve higher performance 
with increasing clock frequency. We have integrated the 
hardware accelerator into our H.264 decoder and verify it on 
a FPGA development board. The result shows that our 
design works correctly and the performance for decoding 
greatly increases compared with pure software solution or 
platform-based methodology.In the future, we will work on 
reducing the power consumption of our decoder. Meanwhile, 
we will use the deblocking filter in the development of both 
H.264 encoder and CODEC. 
Acknowledgement 
This research is supported in part by the National Science 
Council of Taiwanʿʳ the Ministry of Economic Affairs of 
Taiwan, and Taiwan Semiconductor Manufacturing 
Company under grant no. NTHU-0416. 
ʳ
References
[1]T. Wiegand, G. J. Sullivan, G. Bjntegaard, and A. Luthra, 
“Overview of the H.264/AVC video coding standard,” IEEE Trans. 
on Circuits and Systems for Video Technology, vol. 13, pp. 560-576, 
2003.
[2]“Draft ITU-T recommendation and final draft international 
standard of joint video specification (ITU-T Rec. H.264 | ISO/IEC 
14496-10 AVC),“ JVT G050, 2003. 
[3]A. Luthra, G. J. Sullivan, and T. Wiegand, “Introduction to the 
special issue on the H.264/AVC video coding standard,” IEEE 
Trans. on Circuits and Systems for Video Technology, vol. 13, pp. 
557-559, 2003. 
[4]P. List, A. Joch, J. Lainema, G. Bjntegaard, and M. Karczewicz, 
“Adaptive deblocking filter,” IEEE Trans. on Circuits and Systems 
for Video Technology, vol. 13, pp. 614-619, 2003. 
[5]Y. W. Huang, T. W. Chen, B. Y. Hsieh, T. C. Wang, T. H. Chang, 
and L. G. Chen, “Architecture design for deblocking filter in 
H.264/JVT/AVC,” IEEE Int’l Conf. on Multimedia and Expo,
2003.
[6]L. Li, S. Goto, and T. Ikenaga, “An efficient deblocking filter 
architecture with 2-dimentional parallel memory for H.264/AVC,” 
Asia South Pacific Design Automation Conf., 2005. 
[7]V. Venkatraman, S. Krishnan, and N. Ling, “Architecture for 
deblocking filter in H.264,” Picture Coding Symposium, 2004. 
[8]S. C. Chang, W. H. Peng, S. H. Wang, and T. Chiang, “A 
platform based bus-interleaved architecture for deblocking filter in 
H.264/MPEG-4 AVC,” IEEE Trans. on Consumer Electronics, vol. 
51, pp. 249-255, 2005. 
[9]M. Sima, Y. Zhou, and W. Zhang, “An efficient architecture for 
adaptive deblocking filter of H.264/AVC video coding,” IEEE
Trans. on Consumer Electronics, vol. 50, pp. 292-296, 2004. 
[10] C. C. Cheng, and T. S. Chang, “An hardware efficient 
deblocking filter for H.264/AVC,” IEEE Int’l Conf. on Consumer 
Electronics, pp. 235–236, 2005. 
[11] B. Sheng, W. Gao, and D. Wu, “An implemented architecture 
of deblocking filter for H.264/AVC,” IEEE Int’l Conf. on Image 
Processing, vol. 1, pp. 665–668, 2004. 
[12] G. Zheng, and L. Yu, “An efficient architecture design for 
deblocking loop filter,” Picture Coding Symposium, 2004. 
[13] T. M. Liu, W. P. Lee, T. A. Lin, and C. Y. Lee, “A 
memory-efficient deblocking filter for H.264/AVC video coding,” 
IEEE Int’l Symposium on Circuit and Systems, 2005. 
[14] JVT H.264/AVC Reference Software JM 8.3 
 [5] [6] [7] [8] [9] [10] [11] [12] [13] Proposed 
Cycles/MB 614 566 N/A 
Max:342 
Min:50
Avg.:86-2413
N/A N/A 446 2386 250 214 or 2469
Filtering 
Cycles/MB 240 192 136 0-342 214
5 336 192 236 250 192 
SRAM for Pixels1 2P 96x32 2P 64x32 8 DP 80x8 
DP 88x32 
DP 72x32 
1P 32x32 
1P 96x32 DP 16x32 1P 80x32 DP 64x32 2 2P 96x32
DP 96x32 
DP 64x32 
1P (2xFW7)x32 
2 1P 96x32 
1P (2xFW7)x32 
1P 96x32 
2P 32x32 
1P (1.5xFW7)x32 
# of 4x4 Arrays 4 0 11 2 6 2 8 9 4 2 
# of Edge Filters 1 1 Pipelined 2 1 1 1 1 1 1 1 Pipelined 
Process(um) .25 .35 N/A .18 N/A .18 .25 .18 .18 .18 
Gate Count2 20.66K 9.35K N/A 11.8K4 N/A 9.16K4 24K 14.5K8 19.64K 20.9K 
1. DP : Dual-port SRAM with two R/W ports; 1P : Single-port SRAM with one R/W port; 2P : Two–port SRAM with one read and one write ports 
2. Gate Count does not include SRAM 
3. The performance is evaluated by QCIF video sequences with 1I+149P 
4. The gate count does not include boundary strength calculation logic and coding information registers 
5. The filtering cycles do not include filtering chroma components 
6. The cycles do not include boundary strength calculation time 
7. FW stands for Frame Width 
8. The gate count does not include coding information registers 
9. It takes 246 cycles to filter one MB at right picture boundary 
- 2 - 
from multiple frames. In an intra-predict (I) mode, the 
reference MB is usually calculated with mathematical 
functions of neighboring pixels of the current MB.  
The difference between the current MB and its 
prediction is called residual error data (residual). It is 
transformed from spatial domain to frequency domain by 
means of discrete cosine transform. Because human visual 
system is more sensitive to low frequency image and less 
sensitive to high frequency ones, quantization is applied 
such that more low frequency information is retained while 
more high frequency information discarded.  
The third and final type of compression is entropy 
coding. A variable-length coding gives shorter codes to more 
probable symbols and longer codes to less probable ones 
such that the total bit count is minimized. After this phase, 
the output bit stream is ready for transmission or storage. 
There is also a decoding path in the encoder. Because in 
the decoder side only the reconstructed frame instead of the 
original frame is available, we have to use a reconstructed 
frame as the reference for prediction. Therefore, in the 
bottom part of Fig. 1, we obtain the restored residual data by 
performing inverse quantization and then inverse 
transformation. Adding the restored residual to the predicted 
MB, we get the reconstructed MB that is then inserted to the 
reconstructed frame f’t. Now, the reconstructed frame can be 
referred to by either the current I-type compression or future 
P-type or B-type prediction. 
In the next section, we explain in more detail each of 
the video coding functions invoked in the pseudo code. 
III. Basic Video Coding Functions
A. Prediction 
Prediction exploits the spatial or the temporal 
redundancy of a video sequence so that only the difference 
between actual and predict instead of the whole image data 
need to be encoded. There are two types of prediction: intra 
prediction for I-type frame and inter prediction for P-type 
(Predictive) and B-type (Bidirectional Predictive) frame. 
Intra Prediction -- There exists high similarity among 
neighboring blocks in a video frame. Consequently, a block 
can be predicted from its neighboring pixels of already 
coded and reconstructed blocks. The prediction is carried out 
by means of a set of mathematical functions.  
In H.264/AVC, an I-type 16x16 4:2:0 MB has its 
luminance component (one 16x16) and chrominance 
components (two 8x8 blocks) separately predicted. There are 
many ways to predict a macroblock as illustrated in Fig. 2. 
The luminance component may be intra-predicted as one 
single INTRA16x16 block or 16 INTRA4x4 blocks. When 
using the INTRA4ͪ4 case, each 4ͪ4 block utilizes one of 
nine prediction modes (one DC prediction mode and eight 
directional prediction modes). When using the INTRA16ͪ16 
case, which is well suited for smooth image area, a uniform 
prediction is performed for the whole luminance component 
of a macroblock. Four prediction modes are defined. Each 
chrominance component is predicted as a single 8x8 block 
using one of four modes.  
Fig. 2. Overview of H.264 intra prediction modes 
Inter Prediction (Motion Estimation) -- High quality 
video sequences usually have high frame rate at 30 or 60 
frames per second (fps). Therefore, two successive frames in 
a video sequence are very likely to be similar. The goal of 
inter prediction is to utilize this temporal redundancy to 
reduce data need to be encoded. In Fig. 3, for example, when 
encoding frame t, we only need to encode the difference 
between frame t-1 and frame t (i.e., the airplane) instead of 
the whole frame t. This is called motion estimated 
inter-frame prediction. 
Fig. 3. Successive video frames 
In most video coding standards, the block-based motion 
estimation (BME) [9] is used to estimate for movement of a 
rectangular block from the current frame. For each M x 
N-pixel current block in the current frame, BME compares it 
with some or all of possible M x N candidate blocks in the 
search area in the reference frame for the best match, as 
shown in Fig. 4. The reference frame may be a previous 
frame or a next frame in P-type coding, or both in B-type 
coding. A popular matching criterion is to measure the 
residual calculated by subtracting the current block from the 
candidate block, so that the candidate block that minimizes 
t-1 t 
- 4 - 
DCT can centralize the coding information. 
The main functionality of quantization is to scale down 
the transformed coefficients and to reduce the coding 
information. Because human visual system is less sensitive 
to high frequency image component, some video and image 
compression standards may use higher scaling-value 
(quantization parameter) for high frequency data.  
The H.264 standard employs a 4x4 integer DCT [12]. 
Fig. 8 illustrates transformation and quantization in H.264 
with an example. 
5
9
1
19 
11 
8
10 
6
8
4
11 
15
10
12 
4
7
140 
-19 
22 
-27 
-1 
-39 
18 
-32 
-6
7
8
-59 
7
-92
31
-21
4x4 pixel value 
Code transform 
W = AXAt
A=
X
1 1 1 1 
2 1 -1 -2 
1 -1 -1 1 
1 -2 2 -1 W
Post-scaling
Quantization
17 
-1 
3
-2 
0
-2 
1
-1 
-1
0
1
-5
0
-5 
2
-1 
Pre-scaling 
Inverse quantization 
544 
-40 
96 
-80 
0
-100 
40 
-50 
-32 
0
32 
-200 
0
-250
80
-50
Z W’ 
4
8
1
18 
13 
8
10 
5
8
4
10 
14
10
12 
3
7
X’
Inverse code transform 
X’ = BW’Bt
B=
1 1 1 1 
1 1/2 -1/2 -1 
1 -1 -1 1 
1/2 -1 1 -1/2 
Fig. 8. Illustrating Transformation and Quantization 
In Fig. 8, X is a 4x4 block of residual data. After 
integer DCT, we get W, a 4x4 coefficient matrix. Its upper 
left portion represents lower frequency components of X 
while its lower right portion gives higher frequency 
components. Z is the quantized version of W. We can see 
that the amount of data is much smaller than that of X, the 
original residual data. Z is the information to be 
entropy-coded and passed to the decoder part. W’ is the 
scale-up (inversely quantized) version of Z. After applying 
inverse integer DCT (IDCT) on W’, we get X’, which is the 
decoded residual. Note that X’ is not exactly identical to X. 
That is, this process is lossy due to the irreversibility of 
quantization. 
D. In-loop filter 
One of the disadvantages of block-based video coding 
is that discontinuity is likely to appear at the block edge. In 
order to reduce this effect, the H.264 standard employs the 
deblocking filter [13] to eliminate blocking artifact and thus 
generate a smooth picture. 
In the encoder side, deblocking filter can reduce the 
difference between the reconstructed block and the original 
block. According to some experiments, it can not only 
improve PSNR, but also achieve up to 9% bit-rate saving. 
Fig. 9 depicts the input and output of deblocking filter. 
p1 p0  q0 q1 q2 
p3 p2 p1 p0  q0 q1 q2 q3 
p3 p2 p1 p0  q0 q1 q2 q3
Reconstructed Picture
Input Luma Pixels
Coding Information 
Get bS 
Edge Filter 
Filtered Pixels 
Boundary Strength 
QP 
Get Threshold
Į, ȕ, indexA
p1 p0  q0 q1 q2
Chroma Pixels 
Fig. 9. Deblocking Filter Illustration 
  The deblocking filter works on one 16x16 MB at a time. 
It filters every boundary defined by 4x4 blocks within the 
MB. The deblocking filter consists of a horizontal filtering 
across all vertical edges and a vertical filtering across all 
horizontal edges. Therefore, for the luma component, it goes 
through 4 vertical boundaries and 4 horizontal boundaries 
with each boundary requiring 16 filtering operations. For 
both chroma components, it goes through 2 vertical 
boundaries and 2 horizontal boundaries with each boundary 
consisting of 8 filtering operations. As depicted in Fig. 9, 
inputs to a filtering operation includes eight luma pixels (p3, 
Current
frame
Previously-decoded 
frames as reference
Fig. 7. Inter Compensation 
